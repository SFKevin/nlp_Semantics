Train: 2018-08-04T23:04:05.297858: step 1, loss 0.657799.
Train: 2018-08-04T23:04:05.641500: step 2, loss 2.26976.
Train: 2018-08-04T23:04:05.953929: step 3, loss 0.680546.
Train: 2018-08-04T23:04:06.250760: step 4, loss 0.778887.
Train: 2018-08-04T23:04:06.578781: step 5, loss 0.653898.
Train: 2018-08-04T23:04:06.891234: step 6, loss 0.718242.
Train: 2018-08-04T23:04:07.188043: step 7, loss 0.568882.
Train: 2018-08-04T23:04:07.516063: step 8, loss 0.609726.
Train: 2018-08-04T23:04:07.828515: step 9, loss 0.836234.
Train: 2018-08-04T23:04:08.125296: step 10, loss 0.539049.
Test: 2018-08-04T23:04:09.703075: step 10, loss 3.81632.
Train: 2018-08-04T23:04:09.984263: step 11, loss 0.608761.
Train: 2018-08-04T23:04:10.249828: step 12, loss 0.563745.
Train: 2018-08-04T23:04:10.515390: step 13, loss 0.519793.
Train: 2018-08-04T23:04:10.796544: step 14, loss 0.608822.
Train: 2018-08-04T23:04:11.077758: step 15, loss 0.620732.
Train: 2018-08-04T23:04:11.358943: step 16, loss 0.553705.
Train: 2018-08-04T23:04:11.624474: step 17, loss 0.534912.
Train: 2018-08-04T23:04:11.905689: step 18, loss 0.616003.
Train: 2018-08-04T23:04:12.186842: step 19, loss 0.500374.
Train: 2018-08-04T23:04:12.468052: step 20, loss 0.550814.
Test: 2018-08-04T23:04:13.702113: step 20, loss 0.762297.
Train: 2018-08-04T23:04:13.983327: step 21, loss 0.598331.
Train: 2018-08-04T23:04:14.264506: step 22, loss 0.53473.
Train: 2018-08-04T23:04:14.530077: step 23, loss 0.586817.
Train: 2018-08-04T23:04:14.795637: step 24, loss 0.620505.
Train: 2018-08-04T23:04:15.076792: step 25, loss 0.60397.
Train: 2018-08-04T23:04:15.342384: step 26, loss 0.551571.
Train: 2018-08-04T23:04:15.623568: step 27, loss 0.566352.
Train: 2018-08-04T23:04:15.904753: step 28, loss 0.551015.
Train: 2018-08-04T23:04:16.170309: step 29, loss 0.546969.
Train: 2018-08-04T23:04:16.435878: step 30, loss 0.662776.
Test: 2018-08-04T23:04:17.685555: step 30, loss 0.655771.
Train: 2018-08-04T23:04:17.951148: step 31, loss 0.558718.
Train: 2018-08-04T23:04:18.232326: step 32, loss 0.600284.
Train: 2018-08-04T23:04:18.497895: step 33, loss 0.541619.
Train: 2018-08-04T23:04:18.779079: step 34, loss 0.617495.
Train: 2018-08-04T23:04:19.044642: step 35, loss 0.610918.
Train: 2018-08-04T23:04:19.294584: step 36, loss 0.563382.
Train: 2018-08-04T23:04:19.575768: step 37, loss 0.580619.
Train: 2018-08-04T23:04:19.841300: step 38, loss 0.484715.
Train: 2018-08-04T23:04:20.122485: step 39, loss 0.539497.
Train: 2018-08-04T23:04:20.372456: step 40, loss 0.53566.
Test: 2018-08-04T23:04:21.622169: step 40, loss 0.638282.
Train: 2018-08-04T23:04:21.887697: step 41, loss 0.492605.
Train: 2018-08-04T23:04:22.168881: step 42, loss 0.636262.
Train: 2018-08-04T23:04:22.450065: step 43, loss 0.547153.
Train: 2018-08-04T23:04:22.731280: step 44, loss 0.5463.
Train: 2018-08-04T23:04:23.012463: step 45, loss 0.535194.
Train: 2018-08-04T23:04:23.293647: step 46, loss 0.599967.
Train: 2018-08-04T23:04:23.559179: step 47, loss 0.520541.
Train: 2018-08-04T23:04:23.840394: step 48, loss 0.51012.
Train: 2018-08-04T23:04:24.121547: step 49, loss 0.647667.
Train: 2018-08-04T23:04:24.387141: step 50, loss 0.555174.
Test: 2018-08-04T23:04:25.621197: step 50, loss 0.619796.
Train: 2018-08-04T23:04:25.886761: step 51, loss 0.506173.
Train: 2018-08-04T23:04:26.152354: step 52, loss 0.593683.
Train: 2018-08-04T23:04:26.433507: step 53, loss 0.553108.
Train: 2018-08-04T23:04:26.699070: step 54, loss 0.576651.
Train: 2018-08-04T23:04:26.980254: step 55, loss 0.64016.
Train: 2018-08-04T23:04:27.261467: step 56, loss 0.5249.
Train: 2018-08-04T23:04:27.542621: step 57, loss 0.591475.
Train: 2018-08-04T23:04:27.823842: step 58, loss 0.553505.
Train: 2018-08-04T23:04:28.105021: step 59, loss 0.594765.
Train: 2018-08-04T23:04:28.386174: step 60, loss 0.517886.
Test: 2018-08-04T23:04:29.620260: step 60, loss 0.603804.
Train: 2018-08-04T23:04:29.885823: step 61, loss 0.605769.
Train: 2018-08-04T23:04:30.167008: step 62, loss 0.597384.
Train: 2018-08-04T23:04:30.432569: step 63, loss 0.566732.
Train: 2018-08-04T23:04:30.713786: step 64, loss 0.553132.
Train: 2018-08-04T23:04:30.994968: step 65, loss 0.5414.
Train: 2018-08-04T23:04:31.276156: step 66, loss 0.583468.
Train: 2018-08-04T23:04:31.557307: step 67, loss 0.55853.
Train: 2018-08-04T23:04:31.822870: step 68, loss 0.635021.
Train: 2018-08-04T23:04:32.104085: step 69, loss 0.590363.
Train: 2018-08-04T23:04:32.385237: step 70, loss 0.520047.
Test: 2018-08-04T23:04:33.634945: step 70, loss 0.59193.
Train: 2018-08-04T23:04:33.900538: step 71, loss 0.619673.
Train: 2018-08-04T23:04:34.181722: step 72, loss 0.549692.
Train: 2018-08-04T23:04:34.462876: step 73, loss 0.559149.
Train: 2018-08-04T23:04:34.744091: step 74, loss 0.597449.
Train: 2018-08-04T23:04:35.009649: step 75, loss 0.648923.
Train: 2018-08-04T23:04:35.275187: step 76, loss 0.618044.
Train: 2018-08-04T23:04:35.540779: step 77, loss 0.585953.
Train: 2018-08-04T23:04:35.821963: step 78, loss 0.517236.
Train: 2018-08-04T23:04:36.103147: step 79, loss 0.550841.
Train: 2018-08-04T23:04:36.353089: step 80, loss 0.545816.
Test: 2018-08-04T23:04:37.602766: step 80, loss 0.58188.
Train: 2018-08-04T23:04:37.883980: step 81, loss 0.590064.
Train: 2018-08-04T23:04:38.165164: step 82, loss 0.597934.
Train: 2018-08-04T23:04:38.446318: step 83, loss 0.570778.
Train: 2018-08-04T23:04:38.711911: step 84, loss 0.542939.
Train: 2018-08-04T23:04:38.977474: step 85, loss 0.559879.
Train: 2018-08-04T23:04:39.258659: step 86, loss 0.504065.
Train: 2018-08-04T23:04:39.539812: step 87, loss 0.626196.
Train: 2018-08-04T23:04:39.821027: step 88, loss 0.612506.
Train: 2018-08-04T23:04:40.102212: step 89, loss 0.574033.
Train: 2018-08-04T23:04:40.383395: step 90, loss 0.566344.
Test: 2018-08-04T23:04:41.617451: step 90, loss 0.574977.
Train: 2018-08-04T23:04:41.883044: step 91, loss 0.607005.
Train: 2018-08-04T23:04:42.164198: step 92, loss 0.555165.
Train: 2018-08-04T23:04:42.445408: step 93, loss 0.615761.
Train: 2018-08-04T23:04:42.726597: step 94, loss 0.661348.
Train: 2018-08-04T23:04:43.007781: step 95, loss 0.550676.
Train: 2018-08-04T23:04:43.288964: step 96, loss 0.576703.
Train: 2018-08-04T23:04:43.585772: step 97, loss 0.582156.
Train: 2018-08-04T23:04:43.866954: step 98, loss 0.550976.
Train: 2018-08-04T23:04:44.148135: step 99, loss 0.585889.
Train: 2018-08-04T23:04:44.429322: step 100, loss 0.55419.
Test: 2018-08-04T23:04:45.694622: step 100, loss 0.565516.
Train: 2018-08-04T23:04:46.600659: step 101, loss 0.5631.
Train: 2018-08-04T23:04:46.928708: step 102, loss 0.453063.
Train: 2018-08-04T23:04:47.194271: step 103, loss 0.556646.
Train: 2018-08-04T23:04:47.459832: step 104, loss 0.569176.
Train: 2018-08-04T23:04:47.741028: step 105, loss 0.52706.
Train: 2018-08-04T23:04:48.037848: step 106, loss 0.552465.
Train: 2018-08-04T23:04:48.319013: step 107, loss 0.57074.
Train: 2018-08-04T23:04:48.615813: step 108, loss 0.527195.
Train: 2018-08-04T23:04:48.896997: step 109, loss 0.672233.
Train: 2018-08-04T23:04:49.178180: step 110, loss 0.564079.
Test: 2018-08-04T23:04:50.443702: step 110, loss 0.564694.
Train: 2018-08-04T23:04:50.709295: step 111, loss 0.564317.
Train: 2018-08-04T23:04:50.990481: step 112, loss 0.598078.
Train: 2018-08-04T23:04:51.287279: step 113, loss 0.550845.
Train: 2018-08-04T23:04:51.574953: step 114, loss 0.639048.
Train: 2018-08-04T23:04:51.856139: step 115, loss 0.585551.
Train: 2018-08-04T23:04:52.137328: step 116, loss 0.572615.
Train: 2018-08-04T23:04:52.418535: step 117, loss 0.610655.
Train: 2018-08-04T23:04:52.699720: step 118, loss 0.575424.
Train: 2018-08-04T23:04:52.980905: step 119, loss 0.631779.
Train: 2018-08-04T23:04:53.293301: step 120, loss 0.548733.
Test: 2018-08-04T23:04:54.558629: step 120, loss 0.565637.
Train: 2018-08-04T23:04:54.839839: step 121, loss 0.5718.
Train: 2018-08-04T23:04:55.121023: step 122, loss 0.626557.
Train: 2018-08-04T23:04:55.402213: step 123, loss 0.512479.
Train: 2018-08-04T23:04:55.683391: step 124, loss 0.580774.
Train: 2018-08-04T23:04:55.980172: step 125, loss 0.567416.
Train: 2018-08-04T23:04:56.245735: step 126, loss 0.510792.
Train: 2018-08-04T23:04:56.528436: step 127, loss 0.608.
Train: 2018-08-04T23:04:56.853768: step 128, loss 0.49553.
Train: 2018-08-04T23:04:57.134951: step 129, loss 0.48773.
Train: 2018-08-04T23:04:57.416139: step 130, loss 0.540149.
Test: 2018-08-04T23:04:58.665819: step 130, loss 0.554075.
Train: 2018-08-04T23:04:58.931413: step 131, loss 0.466861.
Train: 2018-08-04T23:04:59.212597: step 132, loss 0.648574.
Train: 2018-08-04T23:04:59.493780: step 133, loss 0.563595.
Train: 2018-08-04T23:04:59.790586: step 134, loss 0.555187.
Train: 2018-08-04T23:05:00.071769: step 135, loss 0.593824.
Train: 2018-08-04T23:05:00.352924: step 136, loss 0.542847.
Train: 2018-08-04T23:05:00.634108: step 137, loss 0.560639.
Train: 2018-08-04T23:05:00.899702: step 138, loss 0.595211.
Train: 2018-08-04T23:05:01.196502: step 139, loss 0.554584.
Train: 2018-08-04T23:05:01.477661: step 140, loss 0.589182.
Test: 2018-08-04T23:05:02.734959: step 140, loss 0.560409.
Train: 2018-08-04T23:05:03.031765: step 141, loss 0.591974.
Train: 2018-08-04T23:05:03.312982: step 142, loss 0.556455.
Train: 2018-08-04T23:05:03.594132: step 143, loss 0.526994.
Train: 2018-08-04T23:05:03.875317: step 144, loss 0.533153.
Train: 2018-08-04T23:05:04.156533: step 145, loss 0.513871.
Train: 2018-08-04T23:05:04.453306: step 146, loss 0.566284.
Train: 2018-08-04T23:05:04.734491: step 147, loss 0.532351.
Train: 2018-08-04T23:05:05.031321: step 148, loss 0.497629.
Train: 2018-08-04T23:05:05.312510: step 149, loss 0.582645.
Train: 2018-08-04T23:05:05.609300: step 150, loss 0.586826.
Test: 2018-08-04T23:05:06.858993: step 150, loss 0.549975.
Train: 2018-08-04T23:05:07.294034: step 151, loss 0.441766.
Train: 2018-08-04T23:05:07.575246: step 152, loss 0.552501.
Train: 2018-08-04T23:05:07.856427: step 153, loss 0.622222.
Train: 2018-08-04T23:05:08.137615: step 154, loss 0.555343.
Train: 2018-08-04T23:05:08.418806: step 155, loss 0.567733.
Train: 2018-08-04T23:05:08.746581: step 156, loss 0.584581.
Train: 2018-08-04T23:05:09.043388: step 157, loss 0.559821.
Train: 2018-08-04T23:05:09.418299: step 158, loss 0.546663.
Train: 2018-08-04T23:05:09.777590: step 159, loss 0.521726.
Train: 2018-08-04T23:05:10.058803: step 160, loss 0.579224.
Test: 2018-08-04T23:05:11.292859: step 160, loss 0.553413.
Train: 2018-08-04T23:05:11.574043: step 161, loss 0.606898.
Train: 2018-08-04T23:05:11.855241: step 162, loss 0.544284.
Train: 2018-08-04T23:05:12.152063: step 163, loss 0.582785.
Train: 2018-08-04T23:05:12.433247: step 164, loss 0.560469.
Train: 2018-08-04T23:05:12.714431: step 165, loss 0.592982.
Train: 2018-08-04T23:05:12.995585: step 166, loss 0.627166.
Train: 2018-08-04T23:05:13.276794: step 167, loss 0.592234.
Train: 2018-08-04T23:05:13.557953: step 168, loss 0.610054.
Train: 2018-08-04T23:05:13.839138: step 169, loss 0.605448.
Train: 2018-08-04T23:05:14.120321: step 170, loss 0.629626.
Test: 2018-08-04T23:05:15.354408: step 170, loss 0.55751.
Train: 2018-08-04T23:05:15.635592: step 171, loss 0.583197.
Train: 2018-08-04T23:05:15.916807: step 172, loss 0.59581.
Train: 2018-08-04T23:05:16.197992: step 173, loss 0.582746.
Train: 2018-08-04T23:05:16.479176: step 174, loss 0.589899.
Train: 2018-08-04T23:05:16.775950: step 175, loss 0.550817.
Train: 2018-08-04T23:05:17.057134: step 176, loss 0.578214.
Train: 2018-08-04T23:05:17.338318: step 177, loss 0.550484.
Train: 2018-08-04T23:05:17.619533: step 178, loss 0.546225.
Train: 2018-08-04T23:05:17.900711: step 179, loss 0.566967.
Train: 2018-08-04T23:05:18.181901: step 180, loss 0.504242.
Test: 2018-08-04T23:05:19.415957: step 180, loss 0.554421.
Train: 2018-08-04T23:05:19.681550: step 181, loss 0.566456.
Train: 2018-08-04T23:05:19.947108: step 182, loss 0.544607.
Train: 2018-08-04T23:05:20.228267: step 183, loss 0.53578.
Train: 2018-08-04T23:05:20.509482: step 184, loss 0.614262.
Train: 2018-08-04T23:05:20.775044: step 185, loss 0.471826.
Train: 2018-08-04T23:05:21.056228: step 186, loss 0.515966.
Train: 2018-08-04T23:05:21.321761: step 187, loss 0.513723.
Train: 2018-08-04T23:05:21.602975: step 188, loss 0.47208.
Train: 2018-08-04T23:05:21.884128: step 189, loss 0.650718.
Train: 2018-08-04T23:05:22.165344: step 190, loss 0.546723.
Test: 2018-08-04T23:05:23.415021: step 190, loss 0.547894.
Train: 2018-08-04T23:05:23.680608: step 191, loss 0.595496.
Train: 2018-08-04T23:05:23.946176: step 192, loss 0.523973.
Train: 2018-08-04T23:05:24.227363: step 193, loss 0.587459.
Train: 2018-08-04T23:05:24.492893: step 194, loss 0.509989.
Train: 2018-08-04T23:05:24.774079: step 195, loss 0.534561.
Train: 2018-08-04T23:05:25.055292: step 196, loss 0.565028.
Train: 2018-08-04T23:05:25.336446: step 197, loss 0.628422.
Train: 2018-08-04T23:05:25.617660: step 198, loss 0.630699.
Train: 2018-08-04T23:05:25.898847: step 199, loss 0.587277.
Train: 2018-08-04T23:05:26.179998: step 200, loss 0.547036.
Test: 2018-08-04T23:05:27.414084: step 200, loss 0.548676.
Train: 2018-08-04T23:05:28.257667: step 201, loss 0.563183.
Train: 2018-08-04T23:05:28.523199: step 202, loss 0.571002.
Train: 2018-08-04T23:05:28.804414: step 203, loss 0.632104.
Train: 2018-08-04T23:05:29.085568: step 204, loss 0.609797.
Train: 2018-08-04T23:05:29.366782: step 205, loss 0.595386.
Train: 2018-08-04T23:05:29.647966: step 206, loss 0.548397.
Train: 2018-08-04T23:05:29.929121: step 207, loss 0.593315.
Train: 2018-08-04T23:05:30.210304: step 208, loss 0.53352.
Train: 2018-08-04T23:05:30.475892: step 209, loss 0.579337.
Train: 2018-08-04T23:05:30.757081: step 210, loss 0.562829.
Test: 2018-08-04T23:05:32.006758: step 210, loss 0.554256.
Train: 2018-08-04T23:05:32.287973: step 211, loss 0.573382.
Train: 2018-08-04T23:05:32.569156: step 212, loss 0.543771.
Train: 2018-08-04T23:05:32.850340: step 213, loss 0.553022.
Train: 2018-08-04T23:05:33.131525: step 214, loss 0.524439.
Train: 2018-08-04T23:05:33.412709: step 215, loss 0.540967.
Train: 2018-08-04T23:05:33.693863: step 216, loss 0.54872.
Train: 2018-08-04T23:05:33.975048: step 217, loss 0.601179.
Train: 2018-08-04T23:05:34.256232: step 218, loss 0.577988.
Train: 2018-08-04T23:05:34.537446: step 219, loss 0.533706.
Train: 2018-08-04T23:05:34.802979: step 220, loss 0.495053.
Test: 2018-08-04T23:05:36.052686: step 220, loss 0.549192.
Train: 2018-08-04T23:05:36.333871: step 221, loss 0.567574.
Train: 2018-08-04T23:05:36.615089: step 222, loss 0.541654.
Train: 2018-08-04T23:05:36.896269: step 223, loss 0.504868.
Train: 2018-08-04T23:05:37.177422: step 224, loss 0.546407.
Train: 2018-08-04T23:05:37.458637: step 225, loss 0.650288.
Train: 2018-08-04T23:05:37.739821: step 226, loss 0.532423.
Train: 2018-08-04T23:05:38.020976: step 227, loss 0.573914.
Train: 2018-08-04T23:05:38.302189: step 228, loss 0.537858.
Train: 2018-08-04T23:05:38.583374: step 229, loss 0.612368.
Train: 2018-08-04T23:05:38.864533: step 230, loss 0.599955.
Test: 2018-08-04T23:05:40.114235: step 230, loss 0.547834.
Train: 2018-08-04T23:05:40.379828: step 231, loss 0.602547.
Train: 2018-08-04T23:05:40.645361: step 232, loss 0.610776.
Train: 2018-08-04T23:05:40.910954: step 233, loss 0.548501.
Train: 2018-08-04T23:05:41.192108: step 234, loss 0.65582.
Train: 2018-08-04T23:05:41.473324: step 235, loss 0.565442.
Train: 2018-08-04T23:05:41.754506: step 236, loss 0.56772.
Train: 2018-08-04T23:05:42.035690: step 237, loss 0.601896.
Train: 2018-08-04T23:05:42.316875: step 238, loss 0.51038.
Train: 2018-08-04T23:05:42.582437: step 239, loss 0.564125.
Train: 2018-08-04T23:05:42.847970: step 240, loss 0.552394.
Test: 2018-08-04T23:05:44.082055: step 240, loss 0.55035.
Train: 2018-08-04T23:05:44.347648: step 241, loss 0.566752.
Train: 2018-08-04T23:05:44.628833: step 242, loss 0.635725.
Train: 2018-08-04T23:05:44.910017: step 243, loss 0.628536.
Train: 2018-08-04T23:05:45.191171: step 244, loss 0.540312.
Train: 2018-08-04T23:05:45.472385: step 245, loss 0.545812.
Train: 2018-08-04T23:05:45.753569: step 246, loss 0.502837.
Train: 2018-08-04T23:05:46.034749: step 247, loss 0.528281.
Train: 2018-08-04T23:05:46.315908: step 248, loss 0.563952.
Train: 2018-08-04T23:05:46.597116: step 249, loss 0.581195.
Train: 2018-08-04T23:05:46.862662: step 250, loss 0.592928.
Test: 2018-08-04T23:05:48.096741: step 250, loss 0.55025.
Train: 2018-08-04T23:05:48.362333: step 251, loss 0.604257.
Train: 2018-08-04T23:05:48.643487: step 252, loss 0.512837.
Train: 2018-08-04T23:05:48.924702: step 253, loss 0.541516.
Train: 2018-08-04T23:05:49.205886: step 254, loss 0.534259.
Train: 2018-08-04T23:05:49.487070: step 255, loss 0.547882.
Train: 2018-08-04T23:05:49.752633: step 256, loss 0.584091.
Train: 2018-08-04T23:05:50.033817: step 257, loss 0.528205.
Train: 2018-08-04T23:05:50.315001: step 258, loss 0.528827.
Train: 2018-08-04T23:05:50.580558: step 259, loss 0.589626.
Train: 2018-08-04T23:05:50.861719: step 260, loss 0.524981.
Test: 2018-08-04T23:05:52.095805: step 260, loss 0.547829.
Train: 2018-08-04T23:05:52.361397: step 261, loss 0.56746.
Train: 2018-08-04T23:05:52.642552: step 262, loss 0.630606.
Train: 2018-08-04T23:05:52.908144: step 263, loss 0.53393.
Train: 2018-08-04T23:05:53.189328: step 264, loss 0.594172.
Train: 2018-08-04T23:05:53.470512: step 265, loss 0.584025.
Train: 2018-08-04T23:05:53.751697: step 266, loss 0.462514.
Train: 2018-08-04T23:05:54.032881: step 267, loss 0.595374.
Train: 2018-08-04T23:05:54.314065: step 268, loss 0.578455.
Train: 2018-08-04T23:05:54.595249: step 269, loss 0.537621.
Train: 2018-08-04T23:05:54.876402: step 270, loss 0.508016.
Test: 2018-08-04T23:05:56.110489: step 270, loss 0.54948.
Train: 2018-08-04T23:05:56.376051: step 271, loss 0.564247.
Train: 2018-08-04T23:05:56.704130: step 272, loss 0.624388.
Train: 2018-08-04T23:05:56.985314: step 273, loss 0.60606.
Train: 2018-08-04T23:05:57.250877: step 274, loss 0.616723.
Train: 2018-08-04T23:05:57.532061: step 275, loss 0.50351.
Train: 2018-08-04T23:05:57.813245: step 276, loss 0.546311.
Train: 2018-08-04T23:05:58.094430: step 277, loss 0.565841.
Train: 2018-08-04T23:05:58.375614: step 278, loss 0.555268.
Train: 2018-08-04T23:05:58.656767: step 279, loss 0.536777.
Train: 2018-08-04T23:05:58.937952: step 280, loss 0.561665.
Test: 2018-08-04T23:06:00.172039: step 280, loss 0.548569.
Train: 2018-08-04T23:06:00.500116: step 281, loss 0.550918.
Train: 2018-08-04T23:06:00.781300: step 282, loss 0.555769.
Train: 2018-08-04T23:06:01.062485: step 283, loss 0.541566.
Train: 2018-08-04T23:06:01.343669: step 284, loss 0.57834.
Train: 2018-08-04T23:06:01.624852: step 285, loss 0.513846.
Train: 2018-08-04T23:06:01.906037: step 286, loss 0.58006.
Train: 2018-08-04T23:06:02.187221: step 287, loss 0.5248.
Train: 2018-08-04T23:06:02.468407: step 288, loss 0.501461.
Train: 2018-08-04T23:06:02.749559: step 289, loss 0.586761.
Train: 2018-08-04T23:06:03.030774: step 290, loss 0.591921.
Test: 2018-08-04T23:06:04.264829: step 290, loss 0.548699.
Train: 2018-08-04T23:06:04.530422: step 291, loss 0.561509.
Train: 2018-08-04T23:06:04.811606: step 292, loss 0.577414.
Train: 2018-08-04T23:06:05.092760: step 293, loss 0.558932.
Train: 2018-08-04T23:06:05.373976: step 294, loss 0.518409.
Train: 2018-08-04T23:06:05.639533: step 295, loss 0.551666.
Train: 2018-08-04T23:06:05.920692: step 296, loss 0.514875.
Train: 2018-08-04T23:06:06.201907: step 297, loss 0.59589.
Train: 2018-08-04T23:06:06.483090: step 298, loss 0.435382.
Train: 2018-08-04T23:06:06.764245: step 299, loss 0.562721.
Train: 2018-08-04T23:06:07.045453: step 300, loss 0.562966.
Test: 2018-08-04T23:06:08.295136: step 300, loss 0.549313.
Train: 2018-08-04T23:06:09.138718: step 301, loss 0.475602.
Train: 2018-08-04T23:06:09.357411: step 302, loss 0.582103.
Train: 2018-08-04T23:06:09.638601: step 303, loss 0.510128.
Train: 2018-08-04T23:06:09.904135: step 304, loss 0.581127.
Train: 2018-08-04T23:06:10.185348: step 305, loss 0.573895.
Train: 2018-08-04T23:06:10.466526: step 306, loss 0.547111.
Train: 2018-08-04T23:06:10.732095: step 307, loss 0.657084.
Train: 2018-08-04T23:06:11.013248: step 308, loss 0.569971.
Train: 2018-08-04T23:06:11.294463: step 309, loss 0.54427.
Train: 2018-08-04T23:06:11.575617: step 310, loss 0.641282.
Test: 2018-08-04T23:06:12.809704: step 310, loss 0.54753.
Train: 2018-08-04T23:06:13.075266: step 311, loss 0.525169.
Train: 2018-08-04T23:06:13.356451: step 312, loss 0.612723.
Train: 2018-08-04T23:06:13.637665: step 313, loss 0.576313.
Train: 2018-08-04T23:06:13.918851: step 314, loss 0.58843.
Train: 2018-08-04T23:06:14.200033: step 315, loss 0.657062.
Train: 2018-08-04T23:06:14.481186: step 316, loss 0.489942.
Train: 2018-08-04T23:06:14.762371: step 317, loss 0.521367.
Train: 2018-08-04T23:06:15.043585: step 318, loss 0.581763.
Train: 2018-08-04T23:06:15.324770: step 319, loss 0.559432.
Train: 2018-08-04T23:06:15.605954: step 320, loss 0.523322.
Test: 2018-08-04T23:06:16.840010: step 320, loss 0.550007.
Train: 2018-08-04T23:06:17.105603: step 321, loss 0.548319.
Train: 2018-08-04T23:06:17.371160: step 322, loss 0.552799.
Train: 2018-08-04T23:06:17.652350: step 323, loss 0.534249.
Train: 2018-08-04T23:06:17.933504: step 324, loss 0.620916.
Train: 2018-08-04T23:06:18.214718: step 325, loss 0.596775.
Train: 2018-08-04T23:06:18.495903: step 326, loss 0.510388.
Train: 2018-08-04T23:06:18.777086: step 327, loss 0.530315.
Train: 2018-08-04T23:06:19.058270: step 328, loss 0.603585.
Train: 2018-08-04T23:06:19.339454: step 329, loss 0.573379.
Train: 2018-08-04T23:06:19.620638: step 330, loss 0.583722.
Test: 2018-08-04T23:06:20.854694: step 330, loss 0.550282.
Train: 2018-08-04T23:06:21.120287: step 331, loss 0.618066.
Train: 2018-08-04T23:06:21.401471: step 332, loss 0.548581.
Train: 2018-08-04T23:06:21.682656: step 333, loss 0.548666.
Train: 2018-08-04T23:06:21.963840: step 334, loss 0.538173.
Train: 2018-08-04T23:06:22.229403: step 335, loss 0.524889.
Train: 2018-08-04T23:06:22.510557: step 336, loss 0.53498.
Train: 2018-08-04T23:06:22.776150: step 337, loss 0.604358.
Train: 2018-08-04T23:06:23.057334: step 338, loss 0.619945.
Train: 2018-08-04T23:06:23.335417: step 339, loss 0.583758.
Train: 2018-08-04T23:06:23.616601: step 340, loss 0.584706.
Test: 2018-08-04T23:06:24.850686: step 340, loss 0.549761.
Train: 2018-08-04T23:06:25.116250: step 341, loss 0.620389.
Train: 2018-08-04T23:06:25.397463: step 342, loss 0.59863.
Train: 2018-08-04T23:06:25.678648: step 343, loss 0.545521.
Train: 2018-08-04T23:06:25.959832: step 344, loss 0.5627.
Train: 2018-08-04T23:06:26.240987: step 345, loss 0.559132.
Train: 2018-08-04T23:06:26.522170: step 346, loss 0.484733.
Train: 2018-08-04T23:06:26.803384: step 347, loss 0.545025.
Train: 2018-08-04T23:06:27.084538: step 348, loss 0.530059.
Train: 2018-08-04T23:06:27.365754: step 349, loss 0.52292.
Train: 2018-08-04T23:06:27.646906: step 350, loss 0.625225.
Test: 2018-08-04T23:06:28.880994: step 350, loss 0.549667.
Train: 2018-08-04T23:06:29.146579: step 351, loss 0.506213.
Train: 2018-08-04T23:06:29.412120: step 352, loss 0.561163.
Train: 2018-08-04T23:06:29.708953: step 353, loss 0.488627.
Train: 2018-08-04T23:06:29.990138: step 354, loss 0.58321.
Train: 2018-08-04T23:06:30.271291: step 355, loss 0.563721.
Train: 2018-08-04T23:06:30.552476: step 356, loss 0.501009.
Train: 2018-08-04T23:06:30.818069: step 357, loss 0.548324.
Train: 2018-08-04T23:06:31.099253: step 358, loss 0.518342.
Train: 2018-08-04T23:06:31.380438: step 359, loss 0.483646.
Train: 2018-08-04T23:06:31.661622: step 360, loss 0.623282.
Test: 2018-08-04T23:06:32.911298: step 360, loss 0.548386.
Train: 2018-08-04T23:06:33.176893: step 361, loss 0.569732.
Train: 2018-08-04T23:06:33.442424: step 362, loss 0.545345.
Train: 2018-08-04T23:06:33.723608: step 363, loss 0.5003.
Train: 2018-08-04T23:06:33.989171: step 364, loss 0.572522.
Train: 2018-08-04T23:06:34.270385: step 365, loss 0.504579.
Train: 2018-08-04T23:06:34.535920: step 366, loss 0.519368.
Train: 2018-08-04T23:06:34.817127: step 367, loss 0.46578.
Train: 2018-08-04T23:06:35.098321: step 368, loss 0.473933.
Train: 2018-08-04T23:06:35.379501: step 369, loss 0.529123.
Train: 2018-08-04T23:06:35.645034: step 370, loss 0.550159.
Test: 2018-08-04T23:06:36.879120: step 370, loss 0.548239.
Train: 2018-08-04T23:06:37.129091: step 371, loss 0.630577.
Train: 2018-08-04T23:06:37.394624: step 372, loss 0.53145.
Train: 2018-08-04T23:06:37.675808: step 373, loss 0.514041.
Train: 2018-08-04T23:06:37.957023: step 374, loss 0.584237.
Train: 2018-08-04T23:06:38.238207: step 375, loss 0.619615.
Train: 2018-08-04T23:06:38.519361: step 376, loss 0.493895.
Train: 2018-08-04T23:06:38.800575: step 377, loss 0.501138.
Train: 2018-08-04T23:06:39.081759: step 378, loss 0.586976.
Train: 2018-08-04T23:06:39.347322: step 379, loss 0.526844.
Train: 2018-08-04T23:06:39.612854: step 380, loss 0.632879.
Test: 2018-08-04T23:06:40.846942: step 380, loss 0.547551.
Train: 2018-08-04T23:06:41.112533: step 381, loss 0.533897.
Train: 2018-08-04T23:06:41.378096: step 382, loss 0.551897.
Train: 2018-08-04T23:06:41.643658: step 383, loss 0.529688.
Train: 2018-08-04T23:06:41.909222: step 384, loss 0.581413.
Train: 2018-08-04T23:06:42.174755: step 385, loss 0.554825.
Train: 2018-08-04T23:06:42.440318: step 386, loss 0.497321.
Train: 2018-08-04T23:06:42.721503: step 387, loss 0.561227.
Train: 2018-08-04T23:06:42.971473: step 388, loss 0.465662.
Train: 2018-08-04T23:06:43.237030: step 389, loss 0.58223.
Train: 2018-08-04T23:06:43.502599: step 390, loss 0.616931.
Test: 2018-08-04T23:06:44.752277: step 390, loss 0.546957.
Train: 2018-08-04T23:06:45.002242: step 391, loss 0.572162.
Train: 2018-08-04T23:06:45.267811: step 392, loss 0.534007.
Train: 2018-08-04T23:06:45.533375: step 393, loss 0.699775.
Train: 2018-08-04T23:06:45.798937: step 394, loss 0.511756.
Train: 2018-08-04T23:06:46.064468: step 395, loss 0.520176.
Train: 2018-08-04T23:06:46.330064: step 396, loss 0.525988.
Train: 2018-08-04T23:06:46.595596: step 397, loss 0.556717.
Train: 2018-08-04T23:06:46.861183: step 398, loss 0.546813.
Train: 2018-08-04T23:06:47.126749: step 399, loss 0.62738.
Train: 2018-08-04T23:06:47.392283: step 400, loss 0.564272.
Test: 2018-08-04T23:06:48.641990: step 400, loss 0.549123.
Train: 2018-08-04T23:06:49.563679: step 401, loss 0.579715.
Train: 2018-08-04T23:06:49.829212: step 402, loss 0.536852.
Train: 2018-08-04T23:06:50.094810: step 403, loss 0.581929.
Train: 2018-08-04T23:06:50.360337: step 404, loss 0.612118.
Train: 2018-08-04T23:06:50.625930: step 405, loss 0.531228.
Train: 2018-08-04T23:06:50.891467: step 406, loss 0.594925.
Train: 2018-08-04T23:06:51.157056: step 407, loss 0.547229.
Train: 2018-08-04T23:06:51.422619: step 408, loss 0.610856.
Train: 2018-08-04T23:06:51.688151: step 409, loss 0.50686.
Train: 2018-08-04T23:06:51.953744: step 410, loss 0.5445.
Test: 2018-08-04T23:06:53.187801: step 410, loss 0.550796.
Train: 2018-08-04T23:06:53.437773: step 411, loss 0.570009.
Train: 2018-08-04T23:06:53.703306: step 412, loss 0.554673.
Train: 2018-08-04T23:06:53.968897: step 413, loss 0.631421.
Train: 2018-08-04T23:06:54.234458: step 414, loss 0.577422.
Train: 2018-08-04T23:06:54.500024: step 415, loss 0.5628.
Train: 2018-08-04T23:06:54.765586: step 416, loss 0.594267.
Train: 2018-08-04T23:06:55.031150: step 417, loss 0.640094.
Train: 2018-08-04T23:06:55.296712: step 418, loss 0.538032.
Train: 2018-08-04T23:06:55.562275: step 419, loss 0.564288.
Train: 2018-08-04T23:06:55.827832: step 420, loss 0.539484.
Test: 2018-08-04T23:06:57.077514: step 420, loss 0.550161.
Train: 2018-08-04T23:06:57.327457: step 421, loss 0.586763.
Train: 2018-08-04T23:06:57.593053: step 422, loss 0.528114.
Train: 2018-08-04T23:06:57.858612: step 423, loss 0.55093.
Train: 2018-08-04T23:06:58.124144: step 424, loss 0.574804.
Train: 2018-08-04T23:06:58.389708: step 425, loss 0.556812.
Train: 2018-08-04T23:06:58.655300: step 426, loss 0.586667.
Train: 2018-08-04T23:06:58.920833: step 427, loss 0.557735.
Train: 2018-08-04T23:06:59.186426: step 428, loss 0.526695.
Train: 2018-08-04T23:06:59.451983: step 429, loss 0.559428.
Train: 2018-08-04T23:06:59.686300: step 430, loss 0.553556.
Test: 2018-08-04T23:07:00.935987: step 430, loss 0.548614.
Train: 2018-08-04T23:07:01.185928: step 431, loss 0.529911.
Train: 2018-08-04T23:07:01.435870: step 432, loss 0.560532.
Train: 2018-08-04T23:07:01.685819: step 433, loss 0.591926.
Train: 2018-08-04T23:07:01.951374: step 434, loss 0.60742.
Train: 2018-08-04T23:07:02.201345: step 435, loss 0.556773.
Train: 2018-08-04T23:07:02.466877: step 436, loss 0.576757.
Train: 2018-08-04T23:07:02.716820: step 437, loss 0.588897.
Train: 2018-08-04T23:07:02.998003: step 438, loss 0.568766.
Train: 2018-08-04T23:07:03.232355: step 439, loss 0.56004.
Train: 2018-08-04T23:07:03.482295: step 440, loss 0.687571.
Test: 2018-08-04T23:07:04.731972: step 440, loss 0.550579.
Train: 2018-08-04T23:07:04.981945: step 441, loss 0.526174.
Train: 2018-08-04T23:07:05.247506: step 442, loss 0.594558.
Train: 2018-08-04T23:07:05.497418: step 443, loss 0.5613.
Train: 2018-08-04T23:07:05.763011: step 444, loss 0.530843.
Train: 2018-08-04T23:07:06.012922: step 445, loss 0.599517.
Train: 2018-08-04T23:07:06.278486: step 446, loss 0.57655.
Train: 2018-08-04T23:07:06.528457: step 447, loss 0.533877.
Train: 2018-08-04T23:07:06.778368: step 448, loss 0.641276.
Train: 2018-08-04T23:07:07.028310: step 449, loss 0.532766.
Train: 2018-08-04T23:07:07.278265: step 450, loss 0.626784.
Test: 2018-08-04T23:07:08.527958: step 450, loss 0.548798.
Train: 2018-08-04T23:07:08.762283: step 451, loss 0.569514.
Train: 2018-08-04T23:07:09.012258: step 452, loss 0.638649.
Train: 2018-08-04T23:07:09.215327: step 453, loss 0.498988.
Train: 2018-08-04T23:07:09.465269: step 454, loss 0.5474.
Train: 2018-08-04T23:07:09.715205: step 455, loss 0.570907.
Train: 2018-08-04T23:07:09.965152: step 456, loss 0.604573.
Train: 2018-08-04T23:07:10.215094: step 457, loss 0.539972.
Train: 2018-08-04T23:07:10.465005: step 458, loss 0.580946.
Train: 2018-08-04T23:07:10.714977: step 459, loss 0.535307.
Train: 2018-08-04T23:07:10.964918: step 460, loss 0.510055.
Test: 2018-08-04T23:07:12.214595: step 460, loss 0.549798.
Train: 2018-08-04T23:07:12.448949: step 461, loss 0.536643.
Train: 2018-08-04T23:07:12.698888: step 462, loss 0.517078.
Train: 2018-08-04T23:07:12.948829: step 463, loss 0.610429.
Train: 2018-08-04T23:07:13.198772: step 464, loss 0.477983.
Train: 2018-08-04T23:07:13.448709: step 465, loss 0.592641.
Train: 2018-08-04T23:07:13.698653: step 466, loss 0.602046.
Train: 2018-08-04T23:07:13.964221: step 467, loss 0.563012.
Train: 2018-08-04T23:07:14.198536: step 468, loss 0.54517.
Train: 2018-08-04T23:07:14.448476: step 469, loss 0.581586.
Train: 2018-08-04T23:07:14.698419: step 470, loss 0.599577.
Test: 2018-08-04T23:07:15.932474: step 470, loss 0.548241.
Train: 2018-08-04T23:07:16.182453: step 471, loss 0.588955.
Train: 2018-08-04T23:07:16.432388: step 472, loss 0.623379.
Train: 2018-08-04T23:07:16.682329: step 473, loss 0.598401.
Train: 2018-08-04T23:07:16.932271: step 474, loss 0.481451.
Train: 2018-08-04T23:07:17.182183: step 475, loss 0.529928.
Train: 2018-08-04T23:07:17.432154: step 476, loss 0.58996.
Train: 2018-08-04T23:07:17.682095: step 477, loss 0.489613.
Train: 2018-08-04T23:07:17.932036: step 478, loss 0.487414.
Train: 2018-08-04T23:07:18.181978: step 479, loss 0.563015.
Train: 2018-08-04T23:07:18.431920: step 480, loss 0.536298.
Test: 2018-08-04T23:07:19.665975: step 480, loss 0.547775.
Train: 2018-08-04T23:07:19.915918: step 481, loss 0.534317.
Train: 2018-08-04T23:07:20.165858: step 482, loss 0.563867.
Train: 2018-08-04T23:07:20.415832: step 483, loss 0.591137.
Train: 2018-08-04T23:07:20.665771: step 484, loss 0.580097.
Train: 2018-08-04T23:07:20.915708: step 485, loss 0.558001.
Train: 2018-08-04T23:07:21.165656: step 486, loss 0.559111.
Train: 2018-08-04T23:07:21.415566: step 487, loss 0.562964.
Train: 2018-08-04T23:07:21.665538: step 488, loss 0.5565.
Train: 2018-08-04T23:07:21.915479: step 489, loss 0.657976.
Train: 2018-08-04T23:07:22.165390: step 490, loss 0.61343.
Test: 2018-08-04T23:07:23.415096: step 490, loss 0.547872.
Train: 2018-08-04T23:07:23.649454: step 491, loss 0.553975.
Train: 2018-08-04T23:07:23.899361: step 492, loss 0.535321.
Train: 2018-08-04T23:07:24.180545: step 493, loss 0.559927.
Train: 2018-08-04T23:07:24.430515: step 494, loss 0.554297.
Train: 2018-08-04T23:07:24.680426: step 495, loss 0.548078.
Train: 2018-08-04T23:07:24.930393: step 496, loss 0.573218.
Train: 2018-08-04T23:07:25.180339: step 497, loss 0.527558.
Train: 2018-08-04T23:07:25.430281: step 498, loss 0.520138.
Train: 2018-08-04T23:07:25.680192: step 499, loss 0.555898.
Train: 2018-08-04T23:07:25.930163: step 500, loss 0.516818.
Test: 2018-08-04T23:07:27.195461: step 500, loss 0.547897.
Train: 2018-08-04T23:07:28.007803: step 501, loss 0.528647.
Train: 2018-08-04T23:07:28.257739: step 502, loss 0.522628.
Train: 2018-08-04T23:07:28.507685: step 503, loss 0.557959.
Train: 2018-08-04T23:07:28.757626: step 504, loss 0.542512.
Train: 2018-08-04T23:07:29.007537: step 505, loss 0.5934.
Train: 2018-08-04T23:07:29.257509: step 506, loss 0.639287.
Train: 2018-08-04T23:07:29.507451: step 507, loss 0.56682.
Train: 2018-08-04T23:07:29.757393: step 508, loss 0.570562.
Train: 2018-08-04T23:07:30.007304: step 509, loss 0.569188.
Train: 2018-08-04T23:07:30.257278: step 510, loss 0.536094.
Test: 2018-08-04T23:07:31.491331: step 510, loss 0.549404.
Train: 2018-08-04T23:07:31.741302: step 511, loss 0.565563.
Train: 2018-08-04T23:07:31.991244: step 512, loss 0.530188.
Train: 2018-08-04T23:07:32.241187: step 513, loss 0.571975.
Train: 2018-08-04T23:07:32.506734: step 514, loss 0.532927.
Train: 2018-08-04T23:07:32.772282: step 515, loss 0.601596.
Train: 2018-08-04T23:07:33.022231: step 516, loss 0.635406.
Train: 2018-08-04T23:07:33.272202: step 517, loss 0.602525.
Train: 2018-08-04T23:07:33.522106: step 518, loss 0.549277.
Train: 2018-08-04T23:07:33.772047: step 519, loss 0.535999.
Train: 2018-08-04T23:07:34.022019: step 520, loss 0.571001.
Test: 2018-08-04T23:07:35.287316: step 520, loss 0.548561.
Train: 2018-08-04T23:07:35.521664: step 521, loss 0.564293.
Train: 2018-08-04T23:07:35.771608: step 522, loss 0.551915.
Train: 2018-08-04T23:07:36.037141: step 523, loss 0.548858.
Train: 2018-08-04T23:07:36.271462: step 524, loss 0.586464.
Train: 2018-08-04T23:07:36.521404: step 525, loss 0.561908.
Train: 2018-08-04T23:07:36.771344: step 526, loss 0.623096.
Train: 2018-08-04T23:07:37.021316: step 527, loss 0.514715.
Train: 2018-08-04T23:07:37.271254: step 528, loss 0.544739.
Train: 2018-08-04T23:07:37.521169: step 529, loss 0.658173.
Train: 2018-08-04T23:07:37.771140: step 530, loss 0.583536.
Test: 2018-08-04T23:07:39.005196: step 530, loss 0.550125.
Train: 2018-08-04T23:07:39.239516: step 531, loss 0.602676.
Train: 2018-08-04T23:07:39.489459: step 532, loss 0.508972.
Train: 2018-08-04T23:07:39.739430: step 533, loss 0.564904.
Train: 2018-08-04T23:07:39.989371: step 534, loss 0.523444.
Train: 2018-08-04T23:07:40.239282: step 535, loss 0.595876.
Train: 2018-08-04T23:07:40.489254: step 536, loss 0.500028.
Train: 2018-08-04T23:07:40.739196: step 537, loss 0.568734.
Train: 2018-08-04T23:07:40.989108: step 538, loss 0.588029.
Train: 2018-08-04T23:07:41.239078: step 539, loss 0.587363.
Train: 2018-08-04T23:07:41.489021: step 540, loss 0.560945.
Test: 2018-08-04T23:07:42.738697: step 540, loss 0.550454.
Train: 2018-08-04T23:07:42.973018: step 541, loss 0.561081.
Train: 2018-08-04T23:07:43.222989: step 542, loss 0.5533.
Train: 2018-08-04T23:07:43.472925: step 543, loss 0.577522.
Train: 2018-08-04T23:07:43.722867: step 544, loss 0.623395.
Train: 2018-08-04T23:07:43.972814: step 545, loss 0.509048.
Train: 2018-08-04T23:07:44.222749: step 546, loss 0.634087.
Train: 2018-08-04T23:07:44.472667: step 547, loss 0.597217.
Train: 2018-08-04T23:07:44.722639: step 548, loss 0.564416.
Train: 2018-08-04T23:07:44.972579: step 549, loss 0.490327.
Train: 2018-08-04T23:07:45.222521: step 550, loss 0.562527.
Test: 2018-08-04T23:07:46.472198: step 550, loss 0.549926.
Train: 2018-08-04T23:07:46.706546: step 551, loss 0.516841.
Train: 2018-08-04T23:07:46.956489: step 552, loss 0.562154.
Train: 2018-08-04T23:07:47.222050: step 553, loss 0.565389.
Train: 2018-08-04T23:07:47.471994: step 554, loss 0.497689.
Train: 2018-08-04T23:07:47.721936: step 555, loss 0.555021.
Train: 2018-08-04T23:07:47.971872: step 556, loss 0.563898.
Train: 2018-08-04T23:07:48.221819: step 557, loss 0.511746.
Train: 2018-08-04T23:07:48.471760: step 558, loss 0.555106.
Train: 2018-08-04T23:07:48.721703: step 559, loss 0.570602.
Train: 2018-08-04T23:07:48.971644: step 560, loss 0.511958.
Test: 2018-08-04T23:07:50.205699: step 560, loss 0.547745.
Train: 2018-08-04T23:07:50.455670: step 561, loss 0.562326.
Train: 2018-08-04T23:07:50.705616: step 562, loss 0.519639.
Train: 2018-08-04T23:07:50.955522: step 563, loss 0.633257.
Train: 2018-08-04T23:07:51.205500: step 564, loss 0.571299.
Train: 2018-08-04T23:07:51.455436: step 565, loss 0.564606.
Train: 2018-08-04T23:07:51.705347: step 566, loss 0.614844.
Train: 2018-08-04T23:07:51.955319: step 567, loss 0.588012.
Train: 2018-08-04T23:07:52.205256: step 568, loss 0.611655.
Train: 2018-08-04T23:07:52.455202: step 569, loss 0.557629.
Train: 2018-08-04T23:07:52.705149: step 570, loss 0.616203.
Test: 2018-08-04T23:07:53.954820: step 570, loss 0.549863.
Train: 2018-08-04T23:07:54.189167: step 571, loss 0.536054.
Train: 2018-08-04T23:07:54.439084: step 572, loss 0.622277.
Train: 2018-08-04T23:07:54.689054: step 573, loss 0.547339.
Train: 2018-08-04T23:07:54.938966: step 574, loss 0.555673.
Train: 2018-08-04T23:07:55.188935: step 575, loss 0.530908.
Train: 2018-08-04T23:07:55.438877: step 576, loss 0.573088.
Train: 2018-08-04T23:07:55.688814: step 577, loss 0.629959.
Train: 2018-08-04T23:07:55.938759: step 578, loss 0.517042.
Train: 2018-08-04T23:07:56.188701: step 579, loss 0.538056.
Train: 2018-08-04T23:07:56.438615: step 580, loss 0.530951.
Test: 2018-08-04T23:07:57.703942: step 580, loss 0.549902.
Train: 2018-08-04T23:07:57.938291: step 581, loss 0.562957.
Train: 2018-08-04T23:07:58.188234: step 582, loss 0.54709.
Train: 2018-08-04T23:07:58.438176: step 583, loss 0.532084.
Train: 2018-08-04T23:07:58.688118: step 584, loss 0.578031.
Train: 2018-08-04T23:07:58.938030: step 585, loss 0.536429.
Train: 2018-08-04T23:07:59.188001: step 586, loss 0.611751.
Train: 2018-08-04T23:07:59.437942: step 587, loss 0.524357.
Train: 2018-08-04T23:07:59.687886: step 588, loss 0.547177.
Train: 2018-08-04T23:07:59.937823: step 589, loss 0.515835.
Train: 2018-08-04T23:08:00.187736: step 590, loss 0.488921.
Test: 2018-08-04T23:08:01.421821: step 590, loss 0.549087.
Train: 2018-08-04T23:08:01.687386: step 591, loss 0.561944.
Train: 2018-08-04T23:08:01.937362: step 592, loss 0.536846.
Train: 2018-08-04T23:08:02.187298: step 593, loss 0.52134.
Train: 2018-08-04T23:08:02.437240: step 594, loss 0.620288.
Train: 2018-08-04T23:08:02.687182: step 595, loss 0.536216.
Train: 2018-08-04T23:08:02.937122: step 596, loss 0.511524.
Train: 2018-08-04T23:08:03.187064: step 597, loss 0.605228.
Train: 2018-08-04T23:08:03.437006: step 598, loss 0.572456.
Train: 2018-08-04T23:08:03.686941: step 599, loss 0.640614.
Train: 2018-08-04T23:08:03.936892: step 600, loss 0.56977.
Test: 2018-08-04T23:08:05.186598: step 600, loss 0.54789.
Train: 2018-08-04T23:08:06.030148: step 601, loss 0.562198.
Train: 2018-08-04T23:08:06.280090: step 602, loss 0.700657.
Train: 2018-08-04T23:08:06.530033: step 603, loss 0.551116.
Train: 2018-08-04T23:08:06.717487: step 604, loss 0.562165.
Train: 2018-08-04T23:08:06.951807: step 605, loss 0.553892.
Train: 2018-08-04T23:08:07.201748: step 606, loss 0.557227.
Train: 2018-08-04T23:08:07.451660: step 607, loss 0.565185.
Train: 2018-08-04T23:08:07.701627: step 608, loss 0.52984.
Train: 2018-08-04T23:08:07.951578: step 609, loss 0.587768.
Train: 2018-08-04T23:08:08.201484: step 610, loss 0.643827.
Test: 2018-08-04T23:08:09.451192: step 610, loss 0.548956.
Train: 2018-08-04T23:08:09.685514: step 611, loss 0.586919.
Train: 2018-08-04T23:08:09.935484: step 612, loss 0.581842.
Train: 2018-08-04T23:08:10.185395: step 613, loss 0.564056.
Train: 2018-08-04T23:08:10.435336: step 614, loss 0.54891.
Train: 2018-08-04T23:08:10.685308: step 615, loss 0.56411.
Train: 2018-08-04T23:08:10.935222: step 616, loss 0.587096.
Train: 2018-08-04T23:08:11.185191: step 617, loss 0.565033.
Train: 2018-08-04T23:08:11.435102: step 618, loss 0.555482.
Train: 2018-08-04T23:08:11.685074: step 619, loss 0.604329.
Train: 2018-08-04T23:08:11.934986: step 620, loss 0.592481.
Test: 2018-08-04T23:08:13.169072: step 620, loss 0.550931.
Train: 2018-08-04T23:08:13.419043: step 621, loss 0.571265.
Train: 2018-08-04T23:08:13.668984: step 622, loss 0.593979.
Train: 2018-08-04T23:08:13.918895: step 623, loss 0.610659.
Train: 2018-08-04T23:08:14.168837: step 624, loss 0.548738.
Train: 2018-08-04T23:08:14.418809: step 625, loss 0.56535.
Train: 2018-08-04T23:08:14.668750: step 626, loss 0.526859.
Train: 2018-08-04T23:08:14.918692: step 627, loss 0.556619.
Train: 2018-08-04T23:08:15.168604: step 628, loss 0.602017.
Train: 2018-08-04T23:08:15.418545: step 629, loss 0.542372.
Train: 2018-08-04T23:08:15.668516: step 630, loss 0.527457.
Test: 2018-08-04T23:08:16.902572: step 630, loss 0.551065.
Train: 2018-08-04T23:08:17.152513: step 631, loss 0.608533.
Train: 2018-08-04T23:08:17.402455: step 632, loss 0.609714.
Train: 2018-08-04T23:08:17.652427: step 633, loss 0.556607.
Train: 2018-08-04T23:08:17.917990: step 634, loss 0.580866.
Train: 2018-08-04T23:08:18.152310: step 635, loss 0.533873.
Train: 2018-08-04T23:08:18.402246: step 636, loss 0.540683.
Train: 2018-08-04T23:08:18.652193: step 637, loss 0.547283.
Train: 2018-08-04T23:08:18.902104: step 638, loss 0.577861.
Train: 2018-08-04T23:08:19.152075: step 639, loss 0.63264.
Train: 2018-08-04T23:08:19.401986: step 640, loss 0.587676.
Test: 2018-08-04T23:08:20.651693: step 640, loss 0.551111.
Train: 2018-08-04T23:08:20.886046: step 641, loss 0.610812.
Train: 2018-08-04T23:08:21.135986: step 642, loss 0.541047.
Train: 2018-08-04T23:08:21.385927: step 643, loss 0.596239.
Train: 2018-08-04T23:08:21.635869: step 644, loss 0.542254.
Train: 2018-08-04T23:08:21.885780: step 645, loss 0.587297.
Train: 2018-08-04T23:08:22.135757: step 646, loss 0.478728.
Train: 2018-08-04T23:08:22.385694: step 647, loss 0.546903.
Train: 2018-08-04T23:08:22.635635: step 648, loss 0.529274.
Train: 2018-08-04T23:08:22.885576: step 649, loss 0.493668.
Train: 2018-08-04T23:08:23.135520: step 650, loss 0.507087.
Test: 2018-08-04T23:08:24.385194: step 650, loss 0.549076.
Train: 2018-08-04T23:08:24.619517: step 651, loss 0.49863.
Train: 2018-08-04T23:08:24.869456: step 652, loss 0.546361.
Train: 2018-08-04T23:08:25.119429: step 653, loss 0.579611.
Train: 2018-08-04T23:08:25.369370: step 654, loss 0.613815.
Train: 2018-08-04T23:08:25.619282: step 655, loss 0.587914.
Train: 2018-08-04T23:08:25.869248: step 656, loss 0.604627.
Train: 2018-08-04T23:08:26.119195: step 657, loss 0.587197.
Train: 2018-08-04T23:08:26.369136: step 658, loss 0.602628.
Train: 2018-08-04T23:08:26.619077: step 659, loss 0.539308.
Train: 2018-08-04T23:08:26.868989: step 660, loss 0.529828.
Test: 2018-08-04T23:08:28.118695: step 660, loss 0.54802.
Train: 2018-08-04T23:08:28.353016: step 661, loss 0.569673.
Train: 2018-08-04T23:08:28.602987: step 662, loss 0.620803.
Train: 2018-08-04T23:08:28.852930: step 663, loss 0.570783.
Train: 2018-08-04T23:08:29.102871: step 664, loss 0.531965.
Train: 2018-08-04T23:08:29.352812: step 665, loss 0.529702.
Train: 2018-08-04T23:08:29.602754: step 666, loss 0.539164.
Train: 2018-08-04T23:08:29.852694: step 667, loss 0.527463.
Train: 2018-08-04T23:08:30.102636: step 668, loss 0.589729.
Train: 2018-08-04T23:08:30.352578: step 669, loss 0.530553.
Train: 2018-08-04T23:08:30.602519: step 670, loss 0.521659.
Test: 2018-08-04T23:08:31.836575: step 670, loss 0.547672.
Train: 2018-08-04T23:08:32.070895: step 671, loss 0.49456.
Train: 2018-08-04T23:08:32.320871: step 672, loss 0.600743.
Train: 2018-08-04T23:08:32.586400: step 673, loss 0.564345.
Train: 2018-08-04T23:08:32.820755: step 674, loss 0.503686.
Train: 2018-08-04T23:08:33.070661: step 675, loss 0.559491.
Train: 2018-08-04T23:08:33.320628: step 676, loss 0.545084.
Train: 2018-08-04T23:08:33.570574: step 677, loss 0.606676.
Train: 2018-08-04T23:08:33.820511: step 678, loss 0.605528.
Train: 2018-08-04T23:08:34.070457: step 679, loss 0.485913.
Train: 2018-08-04T23:08:34.320398: step 680, loss 0.537826.
Test: 2018-08-04T23:08:35.554454: step 680, loss 0.549445.
Train: 2018-08-04T23:08:35.788776: step 681, loss 0.58015.
Train: 2018-08-04T23:08:36.038746: step 682, loss 0.546038.
Train: 2018-08-04T23:08:36.288687: step 683, loss 0.586727.
Train: 2018-08-04T23:08:36.538625: step 684, loss 0.557516.
Train: 2018-08-04T23:08:36.788570: step 685, loss 0.51188.
Train: 2018-08-04T23:08:37.038518: step 686, loss 0.587019.
Train: 2018-08-04T23:08:37.288423: step 687, loss 0.59608.
Train: 2018-08-04T23:08:37.538395: step 688, loss 0.546462.
Train: 2018-08-04T23:08:37.788337: step 689, loss 0.571176.
Train: 2018-08-04T23:08:38.038273: step 690, loss 0.459295.
Test: 2018-08-04T23:08:39.272334: step 690, loss 0.547798.
Train: 2018-08-04T23:08:39.522276: step 691, loss 0.503365.
Train: 2018-08-04T23:08:39.772246: step 692, loss 0.596634.
Train: 2018-08-04T23:08:40.022188: step 693, loss 0.625924.
Train: 2018-08-04T23:08:40.272099: step 694, loss 0.599802.
Train: 2018-08-04T23:08:40.522071: step 695, loss 0.573081.
Train: 2018-08-04T23:08:40.772013: step 696, loss 0.630618.
Train: 2018-08-04T23:08:41.021925: step 697, loss 0.579091.
Train: 2018-08-04T23:08:41.271866: step 698, loss 0.53883.
Train: 2018-08-04T23:08:41.521807: step 699, loss 0.596482.
Train: 2018-08-04T23:08:41.771779: step 700, loss 0.587798.
Test: 2018-08-04T23:08:43.021455: step 700, loss 0.547782.
Train: 2018-08-04T23:08:43.896282: step 701, loss 0.529862.
Train: 2018-08-04T23:08:44.146225: step 702, loss 0.570542.
Train: 2018-08-04T23:08:44.396164: step 703, loss 0.546704.
Train: 2018-08-04T23:08:44.646108: step 704, loss 0.523634.
Train: 2018-08-04T23:08:44.896017: step 705, loss 0.553845.
Train: 2018-08-04T23:08:45.145988: step 706, loss 0.650705.
Train: 2018-08-04T23:08:45.395930: step 707, loss 0.548996.
Train: 2018-08-04T23:08:45.645872: step 708, loss 0.539039.
Train: 2018-08-04T23:08:45.895807: step 709, loss 0.584917.
Train: 2018-08-04T23:08:46.145755: step 710, loss 0.611651.
Test: 2018-08-04T23:08:47.395430: step 710, loss 0.549906.
Train: 2018-08-04T23:08:47.629753: step 711, loss 0.571161.
Train: 2018-08-04T23:08:47.879706: step 712, loss 0.632474.
Train: 2018-08-04T23:08:48.129667: step 713, loss 0.539691.
Train: 2018-08-04T23:08:48.379627: step 714, loss 0.579411.
Train: 2018-08-04T23:08:48.629547: step 715, loss 0.500823.
Train: 2018-08-04T23:08:48.879489: step 716, loss 0.61121.
Train: 2018-08-04T23:08:49.129431: step 717, loss 0.563232.
Train: 2018-08-04T23:08:49.379371: step 718, loss 0.509526.
Train: 2018-08-04T23:08:49.613662: step 719, loss 0.572028.
Train: 2018-08-04T23:08:49.863633: step 720, loss 0.533676.
Test: 2018-08-04T23:08:51.097690: step 720, loss 0.550119.
Train: 2018-08-04T23:08:51.347630: step 721, loss 0.570719.
Train: 2018-08-04T23:08:51.597603: step 722, loss 0.578148.
Train: 2018-08-04T23:08:51.847544: step 723, loss 0.579432.
Train: 2018-08-04T23:08:52.097485: step 724, loss 0.491336.
Train: 2018-08-04T23:08:52.347399: step 725, loss 0.506409.
Train: 2018-08-04T23:08:52.597370: step 726, loss 0.617817.
Train: 2018-08-04T23:08:52.847279: step 727, loss 0.586967.
Train: 2018-08-04T23:08:53.097251: step 728, loss 0.492278.
Train: 2018-08-04T23:08:53.347189: step 729, loss 0.561721.
Train: 2018-08-04T23:08:53.597134: step 730, loss 0.531252.
Test: 2018-08-04T23:08:54.831190: step 730, loss 0.548659.
Train: 2018-08-04T23:08:55.081172: step 731, loss 0.514155.
Train: 2018-08-04T23:08:55.331098: step 732, loss 0.48867.
Train: 2018-08-04T23:08:55.581015: step 733, loss 0.501243.
Train: 2018-08-04T23:08:55.830986: step 734, loss 0.556038.
Train: 2018-08-04T23:08:56.080897: step 735, loss 0.636216.
Train: 2018-08-04T23:08:56.330870: step 736, loss 0.620755.
Train: 2018-08-04T23:08:56.580811: step 737, loss 0.511119.
Train: 2018-08-04T23:08:56.830752: step 738, loss 0.527911.
Train: 2018-08-04T23:08:57.080694: step 739, loss 0.570692.
Train: 2018-08-04T23:08:57.330636: step 740, loss 0.535786.
Test: 2018-08-04T23:08:58.580312: step 740, loss 0.549522.
Train: 2018-08-04T23:08:58.830284: step 741, loss 0.582592.
Train: 2018-08-04T23:08:59.080225: step 742, loss 0.56174.
Train: 2018-08-04T23:08:59.330136: step 743, loss 0.562417.
Train: 2018-08-04T23:08:59.580079: step 744, loss 0.526491.
Train: 2018-08-04T23:08:59.830019: step 745, loss 0.555094.
Train: 2018-08-04T23:09:00.079987: step 746, loss 0.596886.
Train: 2018-08-04T23:09:00.329932: step 747, loss 0.529811.
Train: 2018-08-04T23:09:00.579843: step 748, loss 0.57023.
Train: 2018-08-04T23:09:00.829786: step 749, loss 0.571945.
Train: 2018-08-04T23:09:01.079764: step 750, loss 0.537646.
Test: 2018-08-04T23:09:02.313813: step 750, loss 0.54772.
Train: 2018-08-04T23:09:02.548162: step 751, loss 0.579944.
Train: 2018-08-04T23:09:02.798074: step 752, loss 0.493998.
Train: 2018-08-04T23:09:03.048017: step 753, loss 0.639995.
Train: 2018-08-04T23:09:03.297990: step 754, loss 0.59684.
Train: 2018-08-04T23:09:03.485414: step 755, loss 0.600803.
Train: 2018-08-04T23:09:03.735386: step 756, loss 0.614412.
Train: 2018-08-04T23:09:03.985327: step 757, loss 0.505846.
Train: 2018-08-04T23:09:04.235268: step 758, loss 0.561513.
Train: 2018-08-04T23:09:04.485208: step 759, loss 0.563999.
Train: 2018-08-04T23:09:04.735122: step 760, loss 0.563777.
Test: 2018-08-04T23:09:05.984828: step 760, loss 0.547931.
Train: 2018-08-04T23:09:06.219148: step 761, loss 0.594517.
Train: 2018-08-04T23:09:06.469120: step 762, loss 0.538216.
Train: 2018-08-04T23:09:06.719057: step 763, loss 0.59516.
Train: 2018-08-04T23:09:06.968999: step 764, loss 0.604345.
Train: 2018-08-04T23:09:07.218944: step 765, loss 0.57989.
Train: 2018-08-04T23:09:07.468856: step 766, loss 0.540233.
Train: 2018-08-04T23:09:07.718828: step 767, loss 0.547221.
Train: 2018-08-04T23:09:07.968770: step 768, loss 0.547689.
Train: 2018-08-04T23:09:08.218680: step 769, loss 0.562937.
Train: 2018-08-04T23:09:08.468631: step 770, loss 0.602036.
Test: 2018-08-04T23:09:09.718329: step 770, loss 0.549563.
Train: 2018-08-04T23:09:09.952674: step 771, loss 0.56236.
Train: 2018-08-04T23:09:10.202616: step 772, loss 0.490693.
Train: 2018-08-04T23:09:10.452563: step 773, loss 0.523454.
Train: 2018-08-04T23:09:10.702503: step 774, loss 0.538028.
Train: 2018-08-04T23:09:10.952414: step 775, loss 0.577589.
Train: 2018-08-04T23:09:11.202387: step 776, loss 0.562707.
Train: 2018-08-04T23:09:11.452297: step 777, loss 0.562743.
Train: 2018-08-04T23:09:11.702269: step 778, loss 0.602361.
Train: 2018-08-04T23:09:11.952211: step 779, loss 0.570485.
Train: 2018-08-04T23:09:12.202152: step 780, loss 0.579789.
Test: 2018-08-04T23:09:13.467450: step 780, loss 0.54925.
Train: 2018-08-04T23:09:13.701774: step 781, loss 0.56318.
Train: 2018-08-04T23:09:13.973341: step 782, loss 0.61175.
Train: 2018-08-04T23:09:14.238911: step 783, loss 0.554395.
Train: 2018-08-04T23:09:14.473249: step 784, loss 0.564245.
Train: 2018-08-04T23:09:14.723166: step 785, loss 0.571402.
Train: 2018-08-04T23:09:14.988754: step 786, loss 0.578345.
Train: 2018-08-04T23:09:15.238701: step 787, loss 0.579748.
Train: 2018-08-04T23:09:15.488639: step 788, loss 0.58464.
Train: 2018-08-04T23:09:15.738584: step 789, loss 0.611984.
Train: 2018-08-04T23:09:16.004140: step 790, loss 0.57866.
Test: 2018-08-04T23:09:17.222581: step 790, loss 0.548732.
Train: 2018-08-04T23:09:17.488144: step 791, loss 0.531047.
Train: 2018-08-04T23:09:17.753736: step 792, loss 0.555506.
Train: 2018-08-04T23:09:18.003672: step 793, loss 0.51521.
Train: 2018-08-04T23:09:18.253619: step 794, loss 0.547084.
Train: 2018-08-04T23:09:18.503556: step 795, loss 0.587243.
Train: 2018-08-04T23:09:18.753502: step 796, loss 0.545944.
Train: 2018-08-04T23:09:19.003413: step 797, loss 0.530957.
Train: 2018-08-04T23:09:19.253379: step 798, loss 0.546323.
Train: 2018-08-04T23:09:19.503322: step 799, loss 0.529987.
Train: 2018-08-04T23:09:19.753239: step 800, loss 0.572261.
Test: 2018-08-04T23:09:20.987324: step 800, loss 0.550165.
Train: 2018-08-04T23:09:21.830905: step 801, loss 0.570608.
Train: 2018-08-04T23:09:22.080851: step 802, loss 0.505929.
Train: 2018-08-04T23:09:22.330790: step 803, loss 0.634281.
Train: 2018-08-04T23:09:22.580705: step 804, loss 0.546474.
Train: 2018-08-04T23:09:22.830669: step 805, loss 0.595001.
Train: 2018-08-04T23:09:23.080620: step 806, loss 0.571432.
Train: 2018-08-04T23:09:23.330554: step 807, loss 0.570382.
Train: 2018-08-04T23:09:23.580494: step 808, loss 0.613361.
Train: 2018-08-04T23:09:23.830440: step 809, loss 0.472994.
Train: 2018-08-04T23:09:24.080380: step 810, loss 0.463467.
Test: 2018-08-04T23:09:25.330056: step 810, loss 0.548992.
Train: 2018-08-04T23:09:25.564404: step 811, loss 0.593633.
Train: 2018-08-04T23:09:25.814343: step 812, loss 0.552964.
Train: 2018-08-04T23:09:26.064260: step 813, loss 0.596403.
Train: 2018-08-04T23:09:26.314232: step 814, loss 0.613267.
Train: 2018-08-04T23:09:26.564144: step 815, loss 0.571395.
Train: 2018-08-04T23:09:26.814115: step 816, loss 0.586624.
Train: 2018-08-04T23:09:27.064027: step 817, loss 0.494783.
Train: 2018-08-04T23:09:27.313968: step 818, loss 0.57161.
Train: 2018-08-04T23:09:27.563908: step 819, loss 0.588311.
Train: 2018-08-04T23:09:27.813851: step 820, loss 0.579416.
Test: 2018-08-04T23:09:29.063556: step 820, loss 0.550021.
Train: 2018-08-04T23:09:29.297909: step 821, loss 0.627954.
Train: 2018-08-04T23:09:29.547819: step 822, loss 0.496334.
Train: 2018-08-04T23:09:29.797760: step 823, loss 0.585821.
Train: 2018-08-04T23:09:30.047733: step 824, loss 0.57219.
Train: 2018-08-04T23:09:30.297674: step 825, loss 0.504144.
Train: 2018-08-04T23:09:30.547587: step 826, loss 0.545876.
Train: 2018-08-04T23:09:30.797527: step 827, loss 0.505516.
Train: 2018-08-04T23:09:31.047493: step 828, loss 0.513663.
Train: 2018-08-04T23:09:31.297440: step 829, loss 0.561964.
Train: 2018-08-04T23:09:31.547352: step 830, loss 0.587403.
Test: 2018-08-04T23:09:32.797059: step 830, loss 0.547614.
Train: 2018-08-04T23:09:33.031409: step 831, loss 0.472526.
Train: 2018-08-04T23:09:33.281344: step 832, loss 0.503022.
Train: 2018-08-04T23:09:33.531292: step 833, loss 0.576364.
Train: 2018-08-04T23:09:33.781228: step 834, loss 0.61266.
Train: 2018-08-04T23:09:34.031183: step 835, loss 0.536894.
Train: 2018-08-04T23:09:34.281117: step 836, loss 0.511126.
Train: 2018-08-04T23:09:34.531058: step 837, loss 0.579924.
Train: 2018-08-04T23:09:34.780999: step 838, loss 0.60604.
Train: 2018-08-04T23:09:35.030941: step 839, loss 0.521674.
Train: 2018-08-04T23:09:35.280882: step 840, loss 0.535613.
Test: 2018-08-04T23:09:36.514938: step 840, loss 0.547211.
Train: 2018-08-04T23:09:36.764904: step 841, loss 0.639092.
Train: 2018-08-04T23:09:36.999231: step 842, loss 0.527109.
Train: 2018-08-04T23:09:37.249142: step 843, loss 0.64969.
Train: 2018-08-04T23:09:37.499113: step 844, loss 0.520007.
Train: 2018-08-04T23:09:37.749054: step 845, loss 0.561865.
Train: 2018-08-04T23:09:37.998996: step 846, loss 0.504182.
Train: 2018-08-04T23:09:38.248906: step 847, loss 0.613483.
Train: 2018-08-04T23:09:38.498879: step 848, loss 0.563813.
Train: 2018-08-04T23:09:38.748814: step 849, loss 0.552936.
Train: 2018-08-04T23:09:38.998731: step 850, loss 0.561953.
Test: 2018-08-04T23:09:40.232817: step 850, loss 0.548505.
Train: 2018-08-04T23:09:40.482759: step 851, loss 0.636843.
Train: 2018-08-04T23:09:40.732730: step 852, loss 0.4882.
Train: 2018-08-04T23:09:40.982672: step 853, loss 0.57149.
Train: 2018-08-04T23:09:41.232613: step 854, loss 0.554103.
Train: 2018-08-04T23:09:41.482555: step 855, loss 0.59465.
Train: 2018-08-04T23:09:41.732492: step 856, loss 0.553662.
Train: 2018-08-04T23:09:41.982438: step 857, loss 0.563826.
Train: 2018-08-04T23:09:42.232379: step 858, loss 0.497191.
Train: 2018-08-04T23:09:42.482322: step 859, loss 0.570361.
Train: 2018-08-04T23:09:42.732267: step 860, loss 0.596015.
Test: 2018-08-04T23:09:43.981940: step 860, loss 0.548901.
Train: 2018-08-04T23:09:44.216261: step 861, loss 0.528986.
Train: 2018-08-04T23:09:44.466201: step 862, loss 0.546194.
Train: 2018-08-04T23:09:44.716176: step 863, loss 0.669939.
Train: 2018-08-04T23:09:44.966116: step 864, loss 0.579957.
Train: 2018-08-04T23:09:45.216025: step 865, loss 0.572084.
Train: 2018-08-04T23:09:45.466000: step 866, loss 0.692136.
Train: 2018-08-04T23:09:45.715908: step 867, loss 0.506506.
Train: 2018-08-04T23:09:45.965850: step 868, loss 0.538139.
Train: 2018-08-04T23:09:46.215824: step 869, loss 0.625801.
Train: 2018-08-04T23:09:46.481385: step 870, loss 0.658429.
Test: 2018-08-04T23:09:47.715442: step 870, loss 0.549564.
Train: 2018-08-04T23:09:47.949761: step 871, loss 0.539232.
Train: 2018-08-04T23:09:48.199730: step 872, loss 0.484067.
Train: 2018-08-04T23:09:48.449643: step 873, loss 0.609501.
Train: 2018-08-04T23:09:48.699618: step 874, loss 0.517412.
Train: 2018-08-04T23:09:48.949556: step 875, loss 0.524126.
Train: 2018-08-04T23:09:49.199467: step 876, loss 0.586853.
Train: 2018-08-04T23:09:49.449443: step 877, loss 0.524473.
Train: 2018-08-04T23:09:49.699351: step 878, loss 0.530801.
Train: 2018-08-04T23:09:49.949321: step 879, loss 0.516327.
Train: 2018-08-04T23:09:50.199264: step 880, loss 0.516231.
Test: 2018-08-04T23:09:51.448940: step 880, loss 0.550777.
Train: 2018-08-04T23:09:51.683294: step 881, loss 0.547107.
Train: 2018-08-04T23:09:51.933232: step 882, loss 0.516784.
Train: 2018-08-04T23:09:52.183174: step 883, loss 0.578802.
Train: 2018-08-04T23:09:52.433085: step 884, loss 0.529893.
Train: 2018-08-04T23:09:52.683057: step 885, loss 0.595231.
Train: 2018-08-04T23:09:52.932969: step 886, loss 0.669754.
Train: 2018-08-04T23:09:53.182935: step 887, loss 0.537785.
Train: 2018-08-04T23:09:53.432882: step 888, loss 0.570576.
Train: 2018-08-04T23:09:53.682823: step 889, loss 0.611829.
Train: 2018-08-04T23:09:53.932765: step 890, loss 0.578115.
Test: 2018-08-04T23:09:55.182441: step 890, loss 0.549084.
Train: 2018-08-04T23:09:55.416792: step 891, loss 0.523217.
Train: 2018-08-04T23:09:55.666736: step 892, loss 0.579585.
Train: 2018-08-04T23:09:55.916675: step 893, loss 0.563498.
Train: 2018-08-04T23:09:56.150996: step 894, loss 0.586378.
Train: 2018-08-04T23:09:56.400936: step 895, loss 0.562184.
Train: 2018-08-04T23:09:56.650847: step 896, loss 0.481336.
Train: 2018-08-04T23:09:56.947653: step 897, loss 0.530814.
Train: 2018-08-04T23:09:57.197594: step 898, loss 0.587176.
Train: 2018-08-04T23:09:57.447566: step 899, loss 0.520405.
Train: 2018-08-04T23:09:57.697507: step 900, loss 0.570999.
Test: 2018-08-04T23:09:58.947184: step 900, loss 0.549119.
Train: 2018-08-04T23:09:59.822008: step 901, loss 0.594591.
Train: 2018-08-04T23:10:00.071926: step 902, loss 0.529355.
Train: 2018-08-04T23:10:00.321893: step 903, loss 0.579691.
Train: 2018-08-04T23:10:00.571804: step 904, loss 0.613271.
Train: 2018-08-04T23:10:00.821773: step 905, loss 0.52847.
Train: 2018-08-04T23:10:01.009232: step 906, loss 0.544968.
Train: 2018-08-04T23:10:01.259169: step 907, loss 0.644788.
Train: 2018-08-04T23:10:01.509115: step 908, loss 0.562637.
Train: 2018-08-04T23:10:01.759026: step 909, loss 0.604957.
Train: 2018-08-04T23:10:02.008968: step 910, loss 0.522685.
Test: 2018-08-04T23:10:03.258675: step 910, loss 0.549445.
Train: 2018-08-04T23:10:03.555510: step 911, loss 0.578889.
Train: 2018-08-04T23:10:03.805453: step 912, loss 0.58032.
Train: 2018-08-04T23:10:04.055395: step 913, loss 0.595303.
Train: 2018-08-04T23:10:04.305306: step 914, loss 0.505488.
Train: 2018-08-04T23:10:04.555247: step 915, loss 0.611557.
Train: 2018-08-04T23:10:04.805218: step 916, loss 0.554636.
Train: 2018-08-04T23:10:05.055161: step 917, loss 0.595374.
Train: 2018-08-04T23:10:05.305101: step 918, loss 0.538896.
Train: 2018-08-04T23:10:05.555014: step 919, loss 0.482844.
Train: 2018-08-04T23:10:05.804978: step 920, loss 0.522423.
Test: 2018-08-04T23:10:07.039039: step 920, loss 0.549282.
Train: 2018-08-04T23:10:07.273360: step 921, loss 0.481618.
Train: 2018-08-04T23:10:07.523303: step 922, loss 0.618442.
Train: 2018-08-04T23:10:07.773273: step 923, loss 0.57078.
Train: 2018-08-04T23:10:08.023215: step 924, loss 0.652583.
Train: 2018-08-04T23:10:08.273150: step 925, loss 0.529124.
Train: 2018-08-04T23:10:08.523098: step 926, loss 0.579372.
Train: 2018-08-04T23:10:08.773038: step 927, loss 0.580085.
Train: 2018-08-04T23:10:09.022980: step 928, loss 0.619814.
Train: 2018-08-04T23:10:09.272922: step 929, loss 0.635649.
Train: 2018-08-04T23:10:09.522864: step 930, loss 0.586218.
Test: 2018-08-04T23:10:10.772540: step 930, loss 0.548613.
Train: 2018-08-04T23:10:11.006891: step 931, loss 0.571059.
Train: 2018-08-04T23:10:11.256827: step 932, loss 0.587152.
Train: 2018-08-04T23:10:11.506744: step 933, loss 0.50659.
Train: 2018-08-04T23:10:11.756716: step 934, loss 0.532047.
Train: 2018-08-04T23:10:12.006657: step 935, loss 0.578542.
Train: 2018-08-04T23:10:12.256598: step 936, loss 0.48439.
Train: 2018-08-04T23:10:12.506540: step 937, loss 0.579105.
Train: 2018-08-04T23:10:12.756481: step 938, loss 0.587.
Train: 2018-08-04T23:10:13.006423: step 939, loss 0.514955.
Train: 2018-08-04T23:10:13.256333: step 940, loss 0.57766.
Test: 2018-08-04T23:10:14.506041: step 940, loss 0.54895.
Train: 2018-08-04T23:10:14.740363: step 941, loss 0.594309.
Train: 2018-08-04T23:10:14.990303: step 942, loss 0.610994.
Train: 2018-08-04T23:10:15.240244: step 943, loss 0.56275.
Train: 2018-08-04T23:10:15.490185: step 944, loss 0.459222.
Train: 2018-08-04T23:10:15.740154: step 945, loss 0.554126.
Train: 2018-08-04T23:10:15.990102: step 946, loss 0.571489.
Train: 2018-08-04T23:10:16.240040: step 947, loss 0.539248.
Train: 2018-08-04T23:10:16.489982: step 948, loss 0.563693.
Train: 2018-08-04T23:10:16.739923: step 949, loss 0.604046.
Train: 2018-08-04T23:10:16.989835: step 950, loss 0.497759.
Test: 2018-08-04T23:10:18.239542: step 950, loss 0.548997.
Train: 2018-08-04T23:10:18.473892: step 951, loss 0.619993.
Train: 2018-08-04T23:10:18.723830: step 952, loss 0.521732.
Train: 2018-08-04T23:10:18.973775: step 953, loss 0.522032.
Train: 2018-08-04T23:10:19.223717: step 954, loss 0.619477.
Train: 2018-08-04T23:10:19.473628: step 955, loss 0.562603.
Train: 2018-08-04T23:10:19.723601: step 956, loss 0.529989.
Train: 2018-08-04T23:10:19.973541: step 957, loss 0.611238.
Train: 2018-08-04T23:10:20.223453: step 958, loss 0.596396.
Train: 2018-08-04T23:10:20.473424: step 959, loss 0.635791.
Train: 2018-08-04T23:10:20.723366: step 960, loss 0.498035.
Test: 2018-08-04T23:10:21.957421: step 960, loss 0.549442.
Train: 2018-08-04T23:10:22.207363: step 961, loss 0.586641.
Train: 2018-08-04T23:10:22.457337: step 962, loss 0.636701.
Train: 2018-08-04T23:10:22.707246: step 963, loss 0.553488.
Train: 2018-08-04T23:10:22.957220: step 964, loss 0.529665.
Train: 2018-08-04T23:10:23.207161: step 965, loss 0.514138.
Train: 2018-08-04T23:10:23.457103: step 966, loss 0.554371.
Train: 2018-08-04T23:10:23.707044: step 967, loss 0.611833.
Train: 2018-08-04T23:10:23.956986: step 968, loss 0.554704.
Train: 2018-08-04T23:10:24.206894: step 969, loss 0.546805.
Train: 2018-08-04T23:10:24.456869: step 970, loss 0.473524.
Test: 2018-08-04T23:10:25.706543: step 970, loss 0.548535.
Train: 2018-08-04T23:10:25.940888: step 971, loss 0.610966.
Train: 2018-08-04T23:10:26.190835: step 972, loss 0.522796.
Train: 2018-08-04T23:10:26.440777: step 973, loss 0.563328.
Train: 2018-08-04T23:10:26.690719: step 974, loss 0.629282.
Train: 2018-08-04T23:10:26.940660: step 975, loss 0.538926.
Train: 2018-08-04T23:10:27.190601: step 976, loss 0.57076.
Train: 2018-08-04T23:10:27.440541: step 977, loss 0.537346.
Train: 2018-08-04T23:10:27.690483: step 978, loss 0.634867.
Train: 2018-08-04T23:10:27.940426: step 979, loss 0.570555.
Train: 2018-08-04T23:10:28.190368: step 980, loss 0.586571.
Test: 2018-08-04T23:10:29.424423: step 980, loss 0.548909.
Train: 2018-08-04T23:10:29.658774: step 981, loss 0.54634.
Train: 2018-08-04T23:10:29.908717: step 982, loss 0.578545.
Train: 2018-08-04T23:10:30.158657: step 983, loss 0.53828.
Train: 2018-08-04T23:10:30.408568: step 984, loss 0.555483.
Train: 2018-08-04T23:10:30.674130: step 985, loss 0.570886.
Train: 2018-08-04T23:10:30.924102: step 986, loss 0.521657.
Train: 2018-08-04T23:10:31.174044: step 987, loss 0.578055.
Train: 2018-08-04T23:10:31.423974: step 988, loss 0.521984.
Train: 2018-08-04T23:10:31.673897: step 989, loss 0.578295.
Train: 2018-08-04T23:10:31.923852: step 990, loss 0.554255.
Test: 2018-08-04T23:10:33.173544: step 990, loss 0.549764.
Train: 2018-08-04T23:10:33.407874: step 991, loss 0.528826.
Train: 2018-08-04T23:10:33.642216: step 992, loss 0.587146.
Train: 2018-08-04T23:10:33.892157: step 993, loss 0.561752.
Train: 2018-08-04T23:10:34.142068: step 994, loss 0.48125.
Train: 2018-08-04T23:10:34.392039: step 995, loss 0.462599.
Train: 2018-08-04T23:10:34.641981: step 996, loss 0.596276.
Train: 2018-08-04T23:10:34.891923: step 997, loss 0.512229.
Train: 2018-08-04T23:10:35.141864: step 998, loss 0.538527.
Train: 2018-08-04T23:10:35.391800: step 999, loss 0.514179.
Train: 2018-08-04T23:10:35.641716: step 1000, loss 0.594575.
Test: 2018-08-04T23:10:36.875802: step 1000, loss 0.548102.
Train: 2018-08-04T23:10:37.750629: step 1001, loss 0.546085.
Train: 2018-08-04T23:10:38.000569: step 1002, loss 0.602848.
Train: 2018-08-04T23:10:38.250511: step 1003, loss 0.476892.
Train: 2018-08-04T23:10:38.500453: step 1004, loss 0.604512.
Train: 2018-08-04T23:10:38.750395: step 1005, loss 0.552135.
Train: 2018-08-04T23:10:39.000335: step 1006, loss 0.528316.
Train: 2018-08-04T23:10:39.250247: step 1007, loss 0.589148.
Train: 2018-08-04T23:10:39.484599: step 1008, loss 0.483922.
Train: 2018-08-04T23:10:39.734510: step 1009, loss 0.554481.
Train: 2018-08-04T23:10:39.984480: step 1010, loss 0.633651.
Test: 2018-08-04T23:10:41.218536: step 1010, loss 0.54758.
Train: 2018-08-04T23:10:41.468509: step 1011, loss 0.553541.
Train: 2018-08-04T23:10:41.718449: step 1012, loss 0.563837.
Train: 2018-08-04T23:10:41.968360: step 1013, loss 0.554456.
Train: 2018-08-04T23:10:42.218333: step 1014, loss 0.563628.
Train: 2018-08-04T23:10:42.468274: step 1015, loss 0.520998.
Train: 2018-08-04T23:10:42.718215: step 1016, loss 0.555455.
Train: 2018-08-04T23:10:42.968157: step 1017, loss 0.546054.
Train: 2018-08-04T23:10:43.218097: step 1018, loss 0.561692.
Train: 2018-08-04T23:10:43.468034: step 1019, loss 0.528494.
Train: 2018-08-04T23:10:43.717951: step 1020, loss 0.632318.
Test: 2018-08-04T23:10:44.967657: step 1020, loss 0.547417.
Train: 2018-08-04T23:10:45.202011: step 1021, loss 0.502303.
Train: 2018-08-04T23:10:45.451952: step 1022, loss 0.648907.
Train: 2018-08-04T23:10:45.701861: step 1023, loss 0.56254.
Train: 2018-08-04T23:10:45.951830: step 1024, loss 0.528082.
Train: 2018-08-04T23:10:46.201777: step 1025, loss 0.580789.
Train: 2018-08-04T23:10:46.451718: step 1026, loss 0.596834.
Train: 2018-08-04T23:10:46.701659: step 1027, loss 0.588111.
Train: 2018-08-04T23:10:46.951601: step 1028, loss 0.594588.
Train: 2018-08-04T23:10:47.201542: step 1029, loss 0.546958.
Train: 2018-08-04T23:10:47.451484: step 1030, loss 0.529566.
Test: 2018-08-04T23:10:48.685538: step 1030, loss 0.5494.
Train: 2018-08-04T23:10:48.935504: step 1031, loss 0.537572.
Train: 2018-08-04T23:10:49.185451: step 1032, loss 0.563329.
Train: 2018-08-04T23:10:49.435361: step 1033, loss 0.530423.
Train: 2018-08-04T23:10:49.685334: step 1034, loss 0.529819.
Train: 2018-08-04T23:10:49.935275: step 1035, loss 0.577991.
Train: 2018-08-04T23:10:50.185216: step 1036, loss 0.577963.
Train: 2018-08-04T23:10:50.435158: step 1037, loss 0.554269.
Train: 2018-08-04T23:10:50.685070: step 1038, loss 0.604168.
Train: 2018-08-04T23:10:50.935041: step 1039, loss 0.57994.
Train: 2018-08-04T23:10:51.184976: step 1040, loss 0.595446.
Test: 2018-08-04T23:10:52.419037: step 1040, loss 0.54916.
Train: 2018-08-04T23:10:52.653388: step 1041, loss 0.603675.
Train: 2018-08-04T23:10:52.903330: step 1042, loss 0.603569.
Train: 2018-08-04T23:10:53.153271: step 1043, loss 0.546997.
Train: 2018-08-04T23:10:53.403214: step 1044, loss 0.523081.
Train: 2018-08-04T23:10:53.653123: step 1045, loss 0.562692.
Train: 2018-08-04T23:10:53.903065: step 1046, loss 0.578961.
Train: 2018-08-04T23:10:54.153007: step 1047, loss 0.507134.
Train: 2018-08-04T23:10:54.402979: step 1048, loss 0.499515.
Train: 2018-08-04T23:10:54.652890: step 1049, loss 0.578893.
Train: 2018-08-04T23:10:54.902831: step 1050, loss 0.571542.
Test: 2018-08-04T23:10:56.152538: step 1050, loss 0.550032.
Train: 2018-08-04T23:10:56.386860: step 1051, loss 0.571287.
Train: 2018-08-04T23:10:56.636830: step 1052, loss 0.602659.
Train: 2018-08-04T23:10:56.886776: step 1053, loss 0.619217.
Train: 2018-08-04T23:10:57.136714: step 1054, loss 0.498551.
Train: 2018-08-04T23:10:57.386626: step 1055, loss 0.594498.
Train: 2018-08-04T23:10:57.636597: step 1056, loss 0.546407.
Train: 2018-08-04T23:10:57.824046: step 1057, loss 0.511498.
Train: 2018-08-04T23:10:58.073996: step 1058, loss 0.570518.
Train: 2018-08-04T23:10:58.323938: step 1059, loss 0.570567.
Train: 2018-08-04T23:10:58.573879: step 1060, loss 0.547412.
Test: 2018-08-04T23:10:59.823553: step 1060, loss 0.547596.
Train: 2018-08-04T23:11:00.057908: step 1061, loss 0.594611.
Train: 2018-08-04T23:11:00.307852: step 1062, loss 0.53894.
Train: 2018-08-04T23:11:00.557785: step 1063, loss 0.579177.
Train: 2018-08-04T23:11:00.807729: step 1064, loss 0.531738.
Train: 2018-08-04T23:11:01.057671: step 1065, loss 0.57068.
Train: 2018-08-04T23:11:01.307581: step 1066, loss 0.579081.
Train: 2018-08-04T23:11:01.557547: step 1067, loss 0.570854.
Train: 2018-08-04T23:11:01.807495: step 1068, loss 0.538736.
Train: 2018-08-04T23:11:02.057437: step 1069, loss 0.496675.
Train: 2018-08-04T23:11:02.307380: step 1070, loss 0.595275.
Test: 2018-08-04T23:11:03.557054: step 1070, loss 0.549693.
Train: 2018-08-04T23:11:03.791406: step 1071, loss 0.57906.
Train: 2018-08-04T23:11:04.041346: step 1072, loss 0.594929.
Train: 2018-08-04T23:11:04.291289: step 1073, loss 0.579218.
Train: 2018-08-04T23:11:04.541229: step 1074, loss 0.4732.
Train: 2018-08-04T23:11:04.791140: step 1075, loss 0.570515.
Train: 2018-08-04T23:11:05.041108: step 1076, loss 0.570635.
Train: 2018-08-04T23:11:05.291049: step 1077, loss 0.504881.
Train: 2018-08-04T23:11:05.540995: step 1078, loss 0.570925.
Train: 2018-08-04T23:11:05.790907: step 1079, loss 0.629345.
Train: 2018-08-04T23:11:06.040879: step 1080, loss 0.546944.
Test: 2018-08-04T23:11:07.274934: step 1080, loss 0.548118.
Train: 2018-08-04T23:11:07.509279: step 1081, loss 0.537838.
Train: 2018-08-04T23:11:07.759227: step 1082, loss 0.56281.
Train: 2018-08-04T23:11:08.009167: step 1083, loss 0.587731.
Train: 2018-08-04T23:11:08.259109: step 1084, loss 0.611493.
Train: 2018-08-04T23:11:08.509050: step 1085, loss 0.513514.
Train: 2018-08-04T23:11:08.758963: step 1086, loss 0.554313.
Train: 2018-08-04T23:11:09.008903: step 1087, loss 0.555324.
Train: 2018-08-04T23:11:09.258875: step 1088, loss 0.513443.
Train: 2018-08-04T23:11:09.508816: step 1089, loss 0.538716.
Train: 2018-08-04T23:11:09.758757: step 1090, loss 0.554164.
Test: 2018-08-04T23:11:11.008434: step 1090, loss 0.547488.
Train: 2018-08-04T23:11:11.242781: step 1091, loss 0.58742.
Train: 2018-08-04T23:11:11.492696: step 1092, loss 0.504057.
Train: 2018-08-04T23:11:11.742639: step 1093, loss 0.520898.
Train: 2018-08-04T23:11:11.992610: step 1094, loss 0.587463.
Train: 2018-08-04T23:11:12.242551: step 1095, loss 0.546076.
Train: 2018-08-04T23:11:12.492492: step 1096, loss 0.604646.
Train: 2018-08-04T23:11:12.742405: step 1097, loss 0.528947.
Train: 2018-08-04T23:11:12.992376: step 1098, loss 0.570848.
Train: 2018-08-04T23:11:13.242317: step 1099, loss 0.562465.
Train: 2018-08-04T23:11:13.492259: step 1100, loss 0.679914.
Test: 2018-08-04T23:11:14.726314: step 1100, loss 0.548657.
Train: 2018-08-04T23:11:15.601139: step 1101, loss 0.519835.
Train: 2018-08-04T23:11:15.851051: step 1102, loss 0.546303.
Train: 2018-08-04T23:11:16.100992: step 1103, loss 0.554816.
Train: 2018-08-04T23:11:16.350964: step 1104, loss 0.628868.
Train: 2018-08-04T23:11:16.600906: step 1105, loss 0.496185.
Train: 2018-08-04T23:11:16.850848: step 1106, loss 0.571543.
Train: 2018-08-04T23:11:17.100789: step 1107, loss 0.579074.
Train: 2018-08-04T23:11:17.335104: step 1108, loss 0.612016.
Train: 2018-08-04T23:11:17.585050: step 1109, loss 0.587071.
Train: 2018-08-04T23:11:17.834992: step 1110, loss 0.603359.
Test: 2018-08-04T23:11:19.069047: step 1110, loss 0.548953.
Train: 2018-08-04T23:11:19.303401: step 1111, loss 0.587933.
Train: 2018-08-04T23:11:19.553339: step 1112, loss 0.54588.
Train: 2018-08-04T23:11:19.803281: step 1113, loss 0.570906.
Train: 2018-08-04T23:11:20.037597: step 1114, loss 0.587029.
Train: 2018-08-04T23:11:20.287543: step 1115, loss 0.570764.
Train: 2018-08-04T23:11:20.537483: step 1116, loss 0.563044.
Train: 2018-08-04T23:11:20.787395: step 1117, loss 0.643014.
Train: 2018-08-04T23:11:21.037366: step 1118, loss 0.586812.
Train: 2018-08-04T23:11:21.287278: step 1119, loss 0.562776.
Train: 2018-08-04T23:11:21.537249: step 1120, loss 0.594624.
Test: 2018-08-04T23:11:22.786926: step 1120, loss 0.550798.
Train: 2018-08-04T23:11:23.021272: step 1121, loss 0.500375.
Train: 2018-08-04T23:11:23.271219: step 1122, loss 0.571285.
Train: 2018-08-04T23:11:23.521131: step 1123, loss 0.539731.
Train: 2018-08-04T23:11:23.771102: step 1124, loss 0.532285.
Train: 2018-08-04T23:11:24.021038: step 1125, loss 0.602191.
Train: 2018-08-04T23:11:24.270980: step 1126, loss 0.547507.
Train: 2018-08-04T23:11:24.520922: step 1127, loss 0.602292.
Train: 2018-08-04T23:11:24.770868: step 1128, loss 0.508496.
Train: 2018-08-04T23:11:25.020778: step 1129, loss 0.547852.
Train: 2018-08-04T23:11:25.270746: step 1130, loss 0.50816.
Test: 2018-08-04T23:11:26.520427: step 1130, loss 0.549263.
Train: 2018-08-04T23:11:26.754778: step 1131, loss 0.492374.
Train: 2018-08-04T23:11:27.004719: step 1132, loss 0.499413.
Train: 2018-08-04T23:11:27.254630: step 1133, loss 0.515252.
Train: 2018-08-04T23:11:27.504605: step 1134, loss 0.514631.
Train: 2018-08-04T23:11:27.754544: step 1135, loss 0.562932.
Train: 2018-08-04T23:11:28.004493: step 1136, loss 0.529972.
Train: 2018-08-04T23:11:28.254426: step 1137, loss 0.603995.
Train: 2018-08-04T23:11:28.504368: step 1138, loss 0.578976.
Train: 2018-08-04T23:11:28.754309: step 1139, loss 0.529295.
Train: 2018-08-04T23:11:29.004251: step 1140, loss 0.537642.
Test: 2018-08-04T23:11:30.253927: step 1140, loss 0.549179.
Train: 2018-08-04T23:11:30.488278: step 1141, loss 0.470363.
Train: 2018-08-04T23:11:30.738216: step 1142, loss 0.596601.
Train: 2018-08-04T23:11:30.972541: step 1143, loss 0.54435.
Train: 2018-08-04T23:11:31.222483: step 1144, loss 0.562377.
Train: 2018-08-04T23:11:31.472394: step 1145, loss 0.494138.
Train: 2018-08-04T23:11:31.722334: step 1146, loss 0.605456.
Train: 2018-08-04T23:11:31.972306: step 1147, loss 0.553785.
Train: 2018-08-04T23:11:32.222251: step 1148, loss 0.570479.
Train: 2018-08-04T23:11:32.472190: step 1149, loss 0.596565.
Train: 2018-08-04T23:11:32.722099: step 1150, loss 0.579794.
Test: 2018-08-04T23:11:33.971807: step 1150, loss 0.550276.
Train: 2018-08-04T23:11:34.206128: step 1151, loss 0.596391.
Train: 2018-08-04T23:11:34.456071: step 1152, loss 0.536521.
Train: 2018-08-04T23:11:34.706041: step 1153, loss 0.68365.
Train: 2018-08-04T23:11:34.955987: step 1154, loss 0.621667.
Train: 2018-08-04T23:11:35.205894: step 1155, loss 0.537211.
Train: 2018-08-04T23:11:35.455870: step 1156, loss 0.630338.
Train: 2018-08-04T23:11:35.705777: step 1157, loss 0.563419.
Train: 2018-08-04T23:11:35.955748: step 1158, loss 0.503686.
Train: 2018-08-04T23:11:36.205661: step 1159, loss 0.512888.
Train: 2018-08-04T23:11:36.455631: step 1160, loss 0.604387.
Test: 2018-08-04T23:11:37.705308: step 1160, loss 0.548373.
Train: 2018-08-04T23:11:37.939662: step 1161, loss 0.587487.
Train: 2018-08-04T23:11:38.189602: step 1162, loss 0.553659.
Train: 2018-08-04T23:11:38.439510: step 1163, loss 0.579301.
Train: 2018-08-04T23:11:38.689486: step 1164, loss 0.587144.
Train: 2018-08-04T23:11:38.939426: step 1165, loss 0.52951.
Train: 2018-08-04T23:11:39.189369: step 1166, loss 0.497051.
Train: 2018-08-04T23:11:39.439310: step 1167, loss 0.521543.
Train: 2018-08-04T23:11:39.689219: step 1168, loss 0.571009.
Train: 2018-08-04T23:11:39.939188: step 1169, loss 0.50556.
Train: 2018-08-04T23:11:40.189102: step 1170, loss 0.60367.
Test: 2018-08-04T23:11:41.438808: step 1170, loss 0.548916.
Train: 2018-08-04T23:11:41.673130: step 1171, loss 0.611668.
Train: 2018-08-04T23:11:41.923101: step 1172, loss 0.562476.
Train: 2018-08-04T23:11:42.173038: step 1173, loss 0.643959.
Train: 2018-08-04T23:11:42.422978: step 1174, loss 0.571207.
Train: 2018-08-04T23:11:42.672925: step 1175, loss 0.530093.
Train: 2018-08-04T23:11:42.938459: step 1176, loss 0.627615.
Train: 2018-08-04T23:11:43.172778: step 1177, loss 0.611654.
Train: 2018-08-04T23:11:43.422749: step 1178, loss 0.514809.
Train: 2018-08-04T23:11:43.672662: step 1179, loss 0.578761.
Train: 2018-08-04T23:11:43.907015: step 1180, loss 0.523023.
Test: 2018-08-04T23:11:45.141067: step 1180, loss 0.549859.
Train: 2018-08-04T23:11:45.375418: step 1181, loss 0.546295.
Train: 2018-08-04T23:11:45.625359: step 1182, loss 0.554652.
Train: 2018-08-04T23:11:45.875301: step 1183, loss 0.586598.
Train: 2018-08-04T23:11:46.125242: step 1184, loss 0.538827.
Train: 2018-08-04T23:11:46.375183: step 1185, loss 0.538802.
Train: 2018-08-04T23:11:46.625125: step 1186, loss 0.60265.
Train: 2018-08-04T23:11:46.875066: step 1187, loss 0.539049.
Train: 2018-08-04T23:11:47.125008: step 1188, loss 0.490911.
Train: 2018-08-04T23:11:47.374949: step 1189, loss 0.547311.
Train: 2018-08-04T23:11:47.624894: step 1190, loss 0.498183.
Test: 2018-08-04T23:11:48.874567: step 1190, loss 0.548437.
Train: 2018-08-04T23:11:49.108918: step 1191, loss 0.514581.
Train: 2018-08-04T23:11:49.358830: step 1192, loss 0.627795.
Train: 2018-08-04T23:11:49.608770: step 1193, loss 0.546369.
Train: 2018-08-04T23:11:49.858713: step 1194, loss 0.554673.
Train: 2018-08-04T23:11:50.108654: step 1195, loss 0.595311.
Train: 2018-08-04T23:11:50.358596: step 1196, loss 0.562325.
Train: 2018-08-04T23:11:50.608562: step 1197, loss 0.570388.
Train: 2018-08-04T23:11:50.858509: step 1198, loss 0.635933.
Train: 2018-08-04T23:11:51.108458: step 1199, loss 0.571064.
Train: 2018-08-04T23:11:51.358392: step 1200, loss 0.562665.
Test: 2018-08-04T23:11:52.592447: step 1200, loss 0.548805.
Train: 2018-08-04T23:11:53.514130: step 1201, loss 0.595328.
Train: 2018-08-04T23:11:53.764047: step 1202, loss 0.56276.
Train: 2018-08-04T23:11:54.014020: step 1203, loss 0.602862.
Train: 2018-08-04T23:11:54.263962: step 1204, loss 0.472898.
Train: 2018-08-04T23:11:54.513877: step 1205, loss 0.505489.
Train: 2018-08-04T23:11:54.763843: step 1206, loss 0.627856.
Train: 2018-08-04T23:11:55.013754: step 1207, loss 0.595425.
Train: 2018-08-04T23:11:55.201242: step 1208, loss 0.545942.
Train: 2018-08-04T23:11:55.451183: step 1209, loss 0.578992.
Train: 2018-08-04T23:11:55.701124: step 1210, loss 0.554305.
Test: 2018-08-04T23:11:56.950801: step 1210, loss 0.54806.
Train: 2018-08-04T23:11:57.185150: step 1211, loss 0.603339.
Train: 2018-08-04T23:11:57.450710: step 1212, loss 0.595684.
Train: 2018-08-04T23:11:57.700650: step 1213, loss 0.538526.
Train: 2018-08-04T23:11:57.950593: step 1214, loss 0.595015.
Train: 2018-08-04T23:11:58.200539: step 1215, loss 0.522434.
Train: 2018-08-04T23:11:58.450451: step 1216, loss 0.554407.
Train: 2018-08-04T23:11:58.700391: step 1217, loss 0.489925.
Train: 2018-08-04T23:11:58.950335: step 1218, loss 0.505879.
Train: 2018-08-04T23:11:59.200305: step 1219, loss 0.513775.
Train: 2018-08-04T23:11:59.450242: step 1220, loss 0.563103.
Test: 2018-08-04T23:12:00.684302: step 1220, loss 0.548183.
Train: 2018-08-04T23:12:00.918649: step 1221, loss 0.603312.
Train: 2018-08-04T23:12:01.168596: step 1222, loss 0.579022.
Train: 2018-08-04T23:12:01.418530: step 1223, loss 0.537696.
Train: 2018-08-04T23:12:01.668447: step 1224, loss 0.587366.
Train: 2018-08-04T23:12:01.918421: step 1225, loss 0.595645.
Train: 2018-08-04T23:12:02.168331: step 1226, loss 0.53765.
Train: 2018-08-04T23:12:02.418300: step 1227, loss 0.595245.
Train: 2018-08-04T23:12:02.668212: step 1228, loss 0.554577.
Train: 2018-08-04T23:12:02.918185: step 1229, loss 0.530162.
Train: 2018-08-04T23:12:03.168123: step 1230, loss 0.546463.
Test: 2018-08-04T23:12:04.402182: step 1230, loss 0.548368.
Train: 2018-08-04T23:12:04.699017: step 1231, loss 0.562918.
Train: 2018-08-04T23:12:04.948959: step 1232, loss 0.562561.
Train: 2018-08-04T23:12:05.198900: step 1233, loss 0.537386.
Train: 2018-08-04T23:12:05.448813: step 1234, loss 0.554078.
Train: 2018-08-04T23:12:05.698753: step 1235, loss 0.603244.
Train: 2018-08-04T23:12:05.948725: step 1236, loss 0.488028.
Train: 2018-08-04T23:12:06.198660: step 1237, loss 0.570399.
Train: 2018-08-04T23:12:06.448607: step 1238, loss 0.504359.
Train: 2018-08-04T23:12:06.698519: step 1239, loss 0.520244.
Train: 2018-08-04T23:12:06.948460: step 1240, loss 0.520043.
Test: 2018-08-04T23:12:08.182546: step 1240, loss 0.550125.
Train: 2018-08-04T23:12:08.432521: step 1241, loss 0.545704.
Train: 2018-08-04T23:12:08.682430: step 1242, loss 0.536736.
Train: 2018-08-04T23:12:08.932403: step 1243, loss 0.579225.
Train: 2018-08-04T23:12:09.182338: step 1244, loss 0.519964.
Train: 2018-08-04T23:12:09.432286: step 1245, loss 0.579605.
Train: 2018-08-04T23:12:09.682228: step 1246, loss 0.570451.
Train: 2018-08-04T23:12:09.947788: step 1247, loss 0.572446.
Train: 2018-08-04T23:12:10.182111: step 1248, loss 0.605105.
Train: 2018-08-04T23:12:10.432057: step 1249, loss 0.528494.
Train: 2018-08-04T23:12:10.681989: step 1250, loss 0.554114.
Test: 2018-08-04T23:12:11.916047: step 1250, loss 0.548035.
Train: 2018-08-04T23:12:12.150401: step 1251, loss 0.570379.
Train: 2018-08-04T23:12:12.400339: step 1252, loss 0.493158.
Train: 2018-08-04T23:12:12.650276: step 1253, loss 0.520321.
Train: 2018-08-04T23:12:12.900223: step 1254, loss 0.536649.
Train: 2018-08-04T23:12:13.150134: step 1255, loss 0.623005.
Train: 2018-08-04T23:12:13.400105: step 1256, loss 0.562517.
Train: 2018-08-04T23:12:13.650046: step 1257, loss 0.578936.
Train: 2018-08-04T23:12:13.899988: step 1258, loss 0.5023.
Train: 2018-08-04T23:12:14.149929: step 1259, loss 0.545438.
Train: 2018-08-04T23:12:14.398815: step 1260, loss 0.587984.
Test: 2018-08-04T23:12:15.648495: step 1260, loss 0.548395.
Train: 2018-08-04T23:12:15.882815: step 1261, loss 0.587619.
Train: 2018-08-04T23:12:16.132791: step 1262, loss 0.535703.
Train: 2018-08-04T23:12:16.382724: step 1263, loss 0.528218.
Train: 2018-08-04T23:12:16.632641: step 1264, loss 0.5798.
Train: 2018-08-04T23:12:16.882615: step 1265, loss 0.519575.
Train: 2018-08-04T23:12:17.132553: step 1266, loss 0.57001.
Train: 2018-08-04T23:12:17.382494: step 1267, loss 0.545349.
Train: 2018-08-04T23:12:17.632407: step 1268, loss 0.580121.
Train: 2018-08-04T23:12:17.882347: step 1269, loss 0.545028.
Train: 2018-08-04T23:12:18.132289: step 1270, loss 0.596851.
Test: 2018-08-04T23:12:19.366376: step 1270, loss 0.548105.
Train: 2018-08-04T23:12:19.600695: step 1271, loss 0.545062.
Train: 2018-08-04T23:12:19.850667: step 1272, loss 0.605101.
Train: 2018-08-04T23:12:20.100607: step 1273, loss 0.587488.
Train: 2018-08-04T23:12:20.350552: step 1274, loss 0.58848.
Train: 2018-08-04T23:12:20.600493: step 1275, loss 0.621336.
Train: 2018-08-04T23:12:20.850432: step 1276, loss 0.595368.
Train: 2018-08-04T23:12:21.100373: step 1277, loss 0.56264.
Train: 2018-08-04T23:12:21.350285: step 1278, loss 0.529468.
Train: 2018-08-04T23:12:21.600256: step 1279, loss 0.586664.
Train: 2018-08-04T23:12:21.850198: step 1280, loss 0.587244.
Test: 2018-08-04T23:12:23.084254: step 1280, loss 0.549668.
Train: 2018-08-04T23:12:23.334220: step 1281, loss 0.587463.
Train: 2018-08-04T23:12:23.584167: step 1282, loss 0.611464.
Train: 2018-08-04T23:12:23.834108: step 1283, loss 0.514136.
Train: 2018-08-04T23:12:24.084050: step 1284, loss 0.538801.
Train: 2018-08-04T23:12:24.333961: step 1285, loss 0.506158.
Train: 2018-08-04T23:12:24.583932: step 1286, loss 0.522679.
Train: 2018-08-04T23:12:24.833874: step 1287, loss 0.571387.
Train: 2018-08-04T23:12:25.083817: step 1288, loss 0.586772.
Train: 2018-08-04T23:12:25.333758: step 1289, loss 0.522423.
Train: 2018-08-04T23:12:25.583669: step 1290, loss 0.571433.
Test: 2018-08-04T23:12:26.817754: step 1290, loss 0.549688.
Train: 2018-08-04T23:12:27.052075: step 1291, loss 0.562388.
Train: 2018-08-04T23:12:27.302041: step 1292, loss 0.522546.
Train: 2018-08-04T23:12:27.551957: step 1293, loss 0.54654.
Train: 2018-08-04T23:12:27.801929: step 1294, loss 0.522316.
Train: 2018-08-04T23:12:28.051871: step 1295, loss 0.522224.
Train: 2018-08-04T23:12:28.301812: step 1296, loss 0.546449.
Train: 2018-08-04T23:12:28.551753: step 1297, loss 0.538247.
Train: 2018-08-04T23:12:28.801665: step 1298, loss 0.587461.
Train: 2018-08-04T23:12:29.051636: step 1299, loss 0.562899.
Train: 2018-08-04T23:12:29.301578: step 1300, loss 0.553815.
Test: 2018-08-04T23:12:30.535634: step 1300, loss 0.550371.
Train: 2018-08-04T23:12:31.394839: step 1301, loss 0.562499.
Train: 2018-08-04T23:12:31.644780: step 1302, loss 0.587755.
Train: 2018-08-04T23:12:31.894690: step 1303, loss 0.520959.
Train: 2018-08-04T23:12:32.144633: step 1304, loss 0.628819.
Train: 2018-08-04T23:12:32.394604: step 1305, loss 0.51194.
Train: 2018-08-04T23:12:32.644515: step 1306, loss 0.595601.
Train: 2018-08-04T23:12:32.894482: step 1307, loss 0.587516.
Train: 2018-08-04T23:12:33.144400: step 1308, loss 0.562426.
Train: 2018-08-04T23:12:33.394339: step 1309, loss 0.579845.
Train: 2018-08-04T23:12:33.659902: step 1310, loss 0.504523.
Test: 2018-08-04T23:12:34.893988: step 1310, loss 0.54819.
Train: 2018-08-04T23:12:35.128343: step 1311, loss 0.588172.
Train: 2018-08-04T23:12:35.378253: step 1312, loss 0.570309.
Train: 2018-08-04T23:12:35.628192: step 1313, loss 0.587323.
Train: 2018-08-04T23:12:35.878133: step 1314, loss 0.604273.
Train: 2018-08-04T23:12:36.128104: step 1315, loss 0.628494.
Train: 2018-08-04T23:12:36.378050: step 1316, loss 0.553858.
Train: 2018-08-04T23:12:36.643609: step 1317, loss 0.571109.
Train: 2018-08-04T23:12:36.893550: step 1318, loss 0.521414.
Train: 2018-08-04T23:12:37.143462: step 1319, loss 0.530158.
Train: 2018-08-04T23:12:37.377812: step 1320, loss 0.578644.
Test: 2018-08-04T23:12:38.643110: step 1320, loss 0.548878.
Train: 2018-08-04T23:12:38.893053: step 1321, loss 0.554359.
Train: 2018-08-04T23:12:39.143024: step 1322, loss 0.595282.
Train: 2018-08-04T23:12:39.392966: step 1323, loss 0.522259.
Train: 2018-08-04T23:12:39.642918: step 1324, loss 0.570697.
Train: 2018-08-04T23:12:39.892850: step 1325, loss 0.587423.
Train: 2018-08-04T23:12:40.142790: step 1326, loss 0.497465.
Train: 2018-08-04T23:12:40.392731: step 1327, loss 0.586908.
Train: 2018-08-04T23:12:40.642672: step 1328, loss 0.554484.
Train: 2018-08-04T23:12:40.892584: step 1329, loss 0.522206.
Train: 2018-08-04T23:12:41.142555: step 1330, loss 0.635765.
Test: 2018-08-04T23:12:42.376611: step 1330, loss 0.549115.
Train: 2018-08-04T23:12:42.626587: step 1331, loss 0.610713.
Train: 2018-08-04T23:12:42.876528: step 1332, loss 0.603118.
Train: 2018-08-04T23:12:43.126437: step 1333, loss 0.562251.
Train: 2018-08-04T23:12:43.376407: step 1334, loss 0.611068.
Train: 2018-08-04T23:12:43.626349: step 1335, loss 0.618967.
Train: 2018-08-04T23:12:43.876259: step 1336, loss 0.610724.
Train: 2018-08-04T23:12:44.126226: step 1337, loss 0.571075.
Train: 2018-08-04T23:12:44.376172: step 1338, loss 0.563255.
Train: 2018-08-04T23:12:44.626114: step 1339, loss 0.555434.
Train: 2018-08-04T23:12:44.876051: step 1340, loss 0.491924.
Test: 2018-08-04T23:12:46.110112: step 1340, loss 0.549081.
Train: 2018-08-04T23:12:46.360085: step 1341, loss 0.594302.
Train: 2018-08-04T23:12:46.610025: step 1342, loss 0.524175.
Train: 2018-08-04T23:12:46.859968: step 1343, loss 0.562844.
Train: 2018-08-04T23:12:47.109878: step 1344, loss 0.602252.
Train: 2018-08-04T23:12:47.359844: step 1345, loss 0.571544.
Train: 2018-08-04T23:12:47.609760: step 1346, loss 0.571104.
Train: 2018-08-04T23:12:47.859702: step 1347, loss 0.587047.
Train: 2018-08-04T23:12:48.109673: step 1348, loss 0.547881.
Train: 2018-08-04T23:12:48.359613: step 1349, loss 0.547468.
Train: 2018-08-04T23:12:48.609557: step 1350, loss 0.61843.
Test: 2018-08-04T23:12:49.859233: step 1350, loss 0.550275.
Train: 2018-08-04T23:12:50.093585: step 1351, loss 0.563507.
Train: 2018-08-04T23:12:50.343526: step 1352, loss 0.516559.
Train: 2018-08-04T23:12:50.593467: step 1353, loss 0.602631.
Train: 2018-08-04T23:12:50.843410: step 1354, loss 0.562923.
Train: 2018-08-04T23:12:51.093320: step 1355, loss 0.532248.
Train: 2018-08-04T23:12:51.343261: step 1356, loss 0.555403.
Train: 2018-08-04T23:12:51.593234: step 1357, loss 0.555582.
Train: 2018-08-04T23:12:51.843175: step 1358, loss 0.579203.
Train: 2018-08-04T23:12:52.030600: step 1359, loss 0.631081.
Train: 2018-08-04T23:12:52.280567: step 1360, loss 0.531354.
Test: 2018-08-04T23:12:53.530249: step 1360, loss 0.54993.
Train: 2018-08-04T23:12:53.764600: step 1361, loss 0.594495.
Train: 2018-08-04T23:12:54.014541: step 1362, loss 0.555223.
Train: 2018-08-04T23:12:54.264484: step 1363, loss 0.554938.
Train: 2018-08-04T23:12:54.514419: step 1364, loss 0.523444.
Train: 2018-08-04T23:12:54.764360: step 1365, loss 0.571128.
Train: 2018-08-04T23:12:55.014307: step 1366, loss 0.563279.
Train: 2018-08-04T23:12:55.264248: step 1367, loss 0.586972.
Train: 2018-08-04T23:12:55.514190: step 1368, loss 0.618634.
Train: 2018-08-04T23:12:55.764131: step 1369, loss 0.626231.
Train: 2018-08-04T23:12:56.014072: step 1370, loss 0.618456.
Test: 2018-08-04T23:12:57.232507: step 1370, loss 0.550256.
Train: 2018-08-04T23:12:57.482479: step 1371, loss 0.602918.
Train: 2018-08-04T23:12:57.732420: step 1372, loss 0.555439.
Train: 2018-08-04T23:12:57.982362: step 1373, loss 0.539701.
Train: 2018-08-04T23:12:58.232303: step 1374, loss 0.500731.
Train: 2018-08-04T23:12:58.482245: step 1375, loss 0.485035.
Train: 2018-08-04T23:12:58.732189: step 1376, loss 0.54775.
Train: 2018-08-04T23:12:58.982127: step 1377, loss 0.618233.
Train: 2018-08-04T23:12:59.232040: step 1378, loss 0.625904.
Train: 2018-08-04T23:12:59.481980: step 1379, loss 0.547462.
Train: 2018-08-04T23:12:59.731950: step 1380, loss 0.547399.
Test: 2018-08-04T23:13:00.966008: step 1380, loss 0.55063.
Train: 2018-08-04T23:13:01.215949: step 1381, loss 0.547441.
Train: 2018-08-04T23:13:01.465921: step 1382, loss 0.602391.
Train: 2018-08-04T23:13:01.715863: step 1383, loss 0.578882.
Train: 2018-08-04T23:13:01.965804: step 1384, loss 0.634248.
Train: 2018-08-04T23:13:02.215745: step 1385, loss 0.507585.
Train: 2018-08-04T23:13:02.465687: step 1386, loss 0.578975.
Train: 2018-08-04T23:13:02.715629: step 1387, loss 0.539415.
Train: 2018-08-04T23:13:02.965570: step 1388, loss 0.56311.
Train: 2018-08-04T23:13:03.215512: step 1389, loss 0.578744.
Train: 2018-08-04T23:13:03.465422: step 1390, loss 0.594583.
Test: 2018-08-04T23:13:04.699508: step 1390, loss 0.551089.
Train: 2018-08-04T23:13:04.949451: step 1391, loss 0.563096.
Train: 2018-08-04T23:13:05.199392: step 1392, loss 0.626142.
Train: 2018-08-04T23:13:05.449363: step 1393, loss 0.539207.
Train: 2018-08-04T23:13:05.699305: step 1394, loss 0.539155.
Train: 2018-08-04T23:13:05.949216: step 1395, loss 0.55508.
Train: 2018-08-04T23:13:06.199188: step 1396, loss 0.547032.
Train: 2018-08-04T23:13:06.449129: step 1397, loss 0.515239.
Train: 2018-08-04T23:13:06.699071: step 1398, loss 0.594534.
Train: 2018-08-04T23:13:06.949012: step 1399, loss 0.547174.
Train: 2018-08-04T23:13:07.198948: step 1400, loss 0.554758.
Test: 2018-08-04T23:13:08.433009: step 1400, loss 0.548784.
Train: 2018-08-04T23:13:09.307836: step 1401, loss 0.53106.
Train: 2018-08-04T23:13:09.557776: step 1402, loss 0.514541.
Train: 2018-08-04T23:13:09.807724: step 1403, loss 0.54685.
Train: 2018-08-04T23:13:10.057659: step 1404, loss 0.554697.
Train: 2018-08-04T23:13:10.307601: step 1405, loss 0.538093.
Train: 2018-08-04T23:13:10.557542: step 1406, loss 0.546331.
Train: 2018-08-04T23:13:10.807483: step 1407, loss 0.488718.
Train: 2018-08-04T23:13:11.057426: step 1408, loss 0.53822.
Train: 2018-08-04T23:13:11.307367: step 1409, loss 0.571167.
Train: 2018-08-04T23:13:11.557277: step 1410, loss 0.579287.
Test: 2018-08-04T23:13:12.806985: step 1410, loss 0.548669.
Train: 2018-08-04T23:13:13.041336: step 1411, loss 0.520803.
Train: 2018-08-04T23:13:13.291271: step 1412, loss 0.47867.
Train: 2018-08-04T23:13:13.556840: step 1413, loss 0.537609.
Train: 2018-08-04T23:13:13.806781: step 1414, loss 0.579113.
Train: 2018-08-04T23:13:14.056694: step 1415, loss 0.520205.
Train: 2018-08-04T23:13:14.306664: step 1416, loss 0.519281.
Train: 2018-08-04T23:13:14.556607: step 1417, loss 0.57949.
Train: 2018-08-04T23:13:14.806516: step 1418, loss 0.536875.
Train: 2018-08-04T23:13:15.056489: step 1419, loss 0.622645.
Train: 2018-08-04T23:13:15.306430: step 1420, loss 0.571529.
Test: 2018-08-04T23:13:16.556107: step 1420, loss 0.549264.
Train: 2018-08-04T23:13:16.790428: step 1421, loss 0.518514.
Train: 2018-08-04T23:13:17.040399: step 1422, loss 0.605815.
Train: 2018-08-04T23:13:17.290341: step 1423, loss 0.562775.
Train: 2018-08-04T23:13:17.540282: step 1424, loss 0.571412.
Train: 2018-08-04T23:13:17.790194: step 1425, loss 0.570799.
Train: 2018-08-04T23:13:18.040165: step 1426, loss 0.535872.
Train: 2018-08-04T23:13:18.290106: step 1427, loss 0.545331.
Train: 2018-08-04T23:13:18.555669: step 1428, loss 0.570491.
Train: 2018-08-04T23:13:18.805610: step 1429, loss 0.5712.
Train: 2018-08-04T23:13:19.055522: step 1430, loss 0.527962.
Test: 2018-08-04T23:13:20.305230: step 1430, loss 0.548362.
Train: 2018-08-04T23:13:20.555170: step 1431, loss 0.459047.
Train: 2018-08-04T23:13:20.805142: step 1432, loss 0.545426.
Train: 2018-08-04T23:13:21.055079: step 1433, loss 0.501597.
Train: 2018-08-04T23:13:21.304995: step 1434, loss 0.544853.
Train: 2018-08-04T23:13:21.554968: step 1435, loss 0.553869.
Train: 2018-08-04T23:13:21.820530: step 1436, loss 0.501762.
Train: 2018-08-04T23:13:22.054851: step 1437, loss 0.606175.
Train: 2018-08-04T23:13:22.304796: step 1438, loss 0.501557.
Train: 2018-08-04T23:13:22.554703: step 1439, loss 0.641053.
Train: 2018-08-04T23:13:22.804645: step 1440, loss 0.47555.
Test: 2018-08-04T23:13:24.054351: step 1440, loss 0.548991.
Train: 2018-08-04T23:13:24.288672: step 1441, loss 0.571095.
Train: 2018-08-04T23:13:24.554264: step 1442, loss 0.615149.
Train: 2018-08-04T23:13:24.804175: step 1443, loss 0.597324.
Train: 2018-08-04T23:13:25.054147: step 1444, loss 0.580024.
Train: 2018-08-04T23:13:25.304089: step 1445, loss 0.57134.
Train: 2018-08-04T23:13:25.554025: step 1446, loss 0.518786.
Train: 2018-08-04T23:13:25.803967: step 1447, loss 0.537375.
Train: 2018-08-04T23:13:26.053884: step 1448, loss 0.588193.
Train: 2018-08-04T23:13:26.303854: step 1449, loss 0.562581.
Train: 2018-08-04T23:13:26.553804: step 1450, loss 0.621971.
Test: 2018-08-04T23:13:27.803473: step 1450, loss 0.546896.
Train: 2018-08-04T23:13:28.037795: step 1451, loss 0.588147.
Train: 2018-08-04T23:13:28.287768: step 1452, loss 0.493925.
Train: 2018-08-04T23:13:28.537707: step 1453, loss 0.613135.
Train: 2018-08-04T23:13:28.787651: step 1454, loss 0.562535.
Train: 2018-08-04T23:13:29.037559: step 1455, loss 0.629749.
Train: 2018-08-04T23:13:29.287525: step 1456, loss 0.554076.
Train: 2018-08-04T23:13:29.537443: step 1457, loss 0.46242.
Train: 2018-08-04T23:13:29.787414: step 1458, loss 0.562616.
Train: 2018-08-04T23:13:30.037326: step 1459, loss 0.612442.
Train: 2018-08-04T23:13:30.287297: step 1460, loss 0.553993.
Test: 2018-08-04T23:13:31.536974: step 1460, loss 0.548865.
Train: 2018-08-04T23:13:31.771319: step 1461, loss 0.562326.
Train: 2018-08-04T23:13:32.021235: step 1462, loss 0.513049.
Train: 2018-08-04T23:13:32.271208: step 1463, loss 0.570871.
Train: 2018-08-04T23:13:32.521149: step 1464, loss 0.546011.
Train: 2018-08-04T23:13:32.771092: step 1465, loss 0.595461.
Train: 2018-08-04T23:13:33.021027: step 1466, loss 0.562502.
Train: 2018-08-04T23:13:33.270944: step 1467, loss 0.546058.
Train: 2018-08-04T23:13:33.520884: step 1468, loss 0.587081.
Train: 2018-08-04T23:13:33.786447: step 1469, loss 0.603527.
Train: 2018-08-04T23:13:34.020769: step 1470, loss 0.538082.
Test: 2018-08-04T23:13:35.270475: step 1470, loss 0.549534.
Train: 2018-08-04T23:13:35.504826: step 1471, loss 0.554562.
Train: 2018-08-04T23:13:35.754766: step 1472, loss 0.546152.
Train: 2018-08-04T23:13:36.004678: step 1473, loss 0.595421.
Train: 2018-08-04T23:13:36.254649: step 1474, loss 0.57885.
Train: 2018-08-04T23:13:36.504591: step 1475, loss 0.652289.
Train: 2018-08-04T23:13:36.754533: step 1476, loss 0.611362.
Train: 2018-08-04T23:13:37.004443: step 1477, loss 0.586897.
Train: 2018-08-04T23:13:37.254415: step 1478, loss 0.56266.
Train: 2018-08-04T23:13:37.504359: step 1479, loss 0.522803.
Train: 2018-08-04T23:13:37.754294: step 1480, loss 0.483168.
Test: 2018-08-04T23:13:39.003974: step 1480, loss 0.548859.
Train: 2018-08-04T23:13:39.238321: step 1481, loss 0.562794.
Train: 2018-08-04T23:13:39.488272: step 1482, loss 0.578912.
Train: 2018-08-04T23:13:39.738215: step 1483, loss 0.65039.
Train: 2018-08-04T23:13:39.988149: step 1484, loss 0.538864.
Train: 2018-08-04T23:13:40.238096: step 1485, loss 0.634423.
Train: 2018-08-04T23:13:40.488032: step 1486, loss 0.523534.
Train: 2018-08-04T23:13:40.737979: step 1487, loss 0.570891.
Train: 2018-08-04T23:13:40.987920: step 1488, loss 0.547029.
Train: 2018-08-04T23:13:41.237827: step 1489, loss 0.657845.
Train: 2018-08-04T23:13:41.487804: step 1490, loss 0.55537.
Test: 2018-08-04T23:13:42.737476: step 1490, loss 0.548746.
Train: 2018-08-04T23:13:42.971827: step 1491, loss 0.563106.
Train: 2018-08-04T23:13:43.221766: step 1492, loss 0.571056.
Train: 2018-08-04T23:13:43.471710: step 1493, loss 0.571154.
Train: 2018-08-04T23:13:43.721651: step 1494, loss 0.633641.
Train: 2018-08-04T23:13:43.971590: step 1495, loss 0.516659.
Train: 2018-08-04T23:13:44.221534: step 1496, loss 0.563308.
Train: 2018-08-04T23:13:44.471475: step 1497, loss 0.508859.
Train: 2018-08-04T23:13:44.721414: step 1498, loss 0.516601.
Train: 2018-08-04T23:13:44.971359: step 1499, loss 0.555387.
Train: 2018-08-04T23:13:45.221294: step 1500, loss 0.578922.
Test: 2018-08-04T23:13:46.455355: step 1500, loss 0.549508.
Train: 2018-08-04T23:13:47.314531: step 1501, loss 0.61033.
Train: 2018-08-04T23:13:47.595714: step 1502, loss 0.539396.
Train: 2018-08-04T23:13:47.830067: step 1503, loss 0.56314.
Train: 2018-08-04T23:13:48.080005: step 1504, loss 0.626003.
Train: 2018-08-04T23:13:48.329947: step 1505, loss 0.492168.
Train: 2018-08-04T23:13:48.579898: step 1506, loss 0.539267.
Train: 2018-08-04T23:13:48.829833: step 1507, loss 0.595092.
Train: 2018-08-04T23:13:49.079771: step 1508, loss 0.539073.
Train: 2018-08-04T23:13:49.329684: step 1509, loss 0.579027.
Train: 2018-08-04T23:13:49.532789: step 1510, loss 0.647455.
Test: 2018-08-04T23:13:50.798113: step 1510, loss 0.549437.
Train: 2018-08-04T23:13:51.032439: step 1511, loss 0.530943.
Train: 2018-08-04T23:13:51.282380: step 1512, loss 0.626472.
Train: 2018-08-04T23:13:51.532293: step 1513, loss 0.483003.
Train: 2018-08-04T23:13:51.782263: step 1514, loss 0.586761.
Train: 2018-08-04T23:13:52.032175: step 1515, loss 0.56276.
Train: 2018-08-04T23:13:52.282116: step 1516, loss 0.59504.
Train: 2018-08-04T23:13:52.532084: step 1517, loss 0.491057.
Train: 2018-08-04T23:13:52.781999: step 1518, loss 0.586889.
Train: 2018-08-04T23:13:53.031970: step 1519, loss 0.546682.
Train: 2018-08-04T23:13:53.281913: step 1520, loss 0.602956.
Test: 2018-08-04T23:13:54.531590: step 1520, loss 0.548089.
Train: 2018-08-04T23:13:54.765910: step 1521, loss 0.595077.
Train: 2018-08-04T23:13:55.015877: step 1522, loss 0.506465.
Train: 2018-08-04T23:13:55.265822: step 1523, loss 0.562402.
Train: 2018-08-04T23:13:55.515764: step 1524, loss 0.691815.
Train: 2018-08-04T23:13:55.765706: step 1525, loss 0.571072.
Train: 2018-08-04T23:13:56.015654: step 1526, loss 0.56286.
Train: 2018-08-04T23:13:56.265584: step 1527, loss 0.578807.
Train: 2018-08-04T23:13:56.515530: step 1528, loss 0.570602.
Train: 2018-08-04T23:13:56.765472: step 1529, loss 0.538889.
Train: 2018-08-04T23:13:57.015413: step 1530, loss 0.570512.
Test: 2018-08-04T23:13:58.280711: step 1530, loss 0.549983.
Train: 2018-08-04T23:13:58.515032: step 1531, loss 0.59453.
Train: 2018-08-04T23:13:58.780593: step 1532, loss 0.539216.
Train: 2018-08-04T23:13:59.030566: step 1533, loss 0.539007.
Train: 2018-08-04T23:13:59.280508: step 1534, loss 0.58662.
Train: 2018-08-04T23:13:59.530449: step 1535, loss 0.618723.
Train: 2018-08-04T23:13:59.764740: step 1536, loss 0.562824.
Train: 2018-08-04T23:14:00.014710: step 1537, loss 0.562642.
Train: 2018-08-04T23:14:00.264656: step 1538, loss 0.491681.
Train: 2018-08-04T23:14:00.514564: step 1539, loss 0.58631.
Train: 2018-08-04T23:14:00.764505: step 1540, loss 0.507258.
Test: 2018-08-04T23:14:02.029833: step 1540, loss 0.549806.
Train: 2018-08-04T23:14:02.264179: step 1541, loss 0.538864.
Train: 2018-08-04T23:14:02.514126: step 1542, loss 0.467225.
Train: 2018-08-04T23:14:02.764066: step 1543, loss 0.555029.
Train: 2018-08-04T23:14:03.014008: step 1544, loss 0.579128.
Train: 2018-08-04T23:14:03.263919: step 1545, loss 0.61969.
Train: 2018-08-04T23:14:03.513892: step 1546, loss 0.586747.
Train: 2018-08-04T23:14:03.763832: step 1547, loss 0.570958.
Train: 2018-08-04T23:14:04.013750: step 1548, loss 0.58773.
Train: 2018-08-04T23:14:04.263716: step 1549, loss 0.529757.
Train: 2018-08-04T23:14:04.513657: step 1550, loss 0.619745.
Test: 2018-08-04T23:14:05.763335: step 1550, loss 0.548655.
Train: 2018-08-04T23:14:06.075791: step 1551, loss 0.578817.
Train: 2018-08-04T23:14:06.325733: step 1552, loss 0.554176.
Train: 2018-08-04T23:14:06.575643: step 1553, loss 0.546318.
Train: 2018-08-04T23:14:06.825616: step 1554, loss 0.619902.
Train: 2018-08-04T23:14:07.075557: step 1555, loss 0.554848.
Train: 2018-08-04T23:14:07.325499: step 1556, loss 0.603627.
Train: 2018-08-04T23:14:07.575440: step 1557, loss 0.546662.
Train: 2018-08-04T23:14:07.825382: step 1558, loss 0.578603.
Train: 2018-08-04T23:14:08.075292: step 1559, loss 0.635449.
Train: 2018-08-04T23:14:08.325234: step 1560, loss 0.554676.
Test: 2018-08-04T23:14:09.559319: step 1560, loss 0.548869.
Train: 2018-08-04T23:14:09.793671: step 1561, loss 0.490973.
Train: 2018-08-04T23:14:10.043611: step 1562, loss 0.546406.
Train: 2018-08-04T23:14:10.293557: step 1563, loss 0.506737.
Train: 2018-08-04T23:14:10.543494: step 1564, loss 0.49888.
Train: 2018-08-04T23:14:10.793407: step 1565, loss 0.514046.
Train: 2018-08-04T23:14:11.043347: step 1566, loss 0.554623.
Train: 2018-08-04T23:14:11.293290: step 1567, loss 0.546197.
Train: 2018-08-04T23:14:11.543260: step 1568, loss 0.554798.
Train: 2018-08-04T23:14:11.793173: step 1569, loss 0.546072.
Train: 2018-08-04T23:14:12.043144: step 1570, loss 0.497051.
Test: 2018-08-04T23:14:13.308442: step 1570, loss 0.547179.
Train: 2018-08-04T23:14:13.542793: step 1571, loss 0.521549.
Train: 2018-08-04T23:14:13.792734: step 1572, loss 0.554046.
Train: 2018-08-04T23:14:14.042646: step 1573, loss 0.520872.
Train: 2018-08-04T23:14:14.292589: step 1574, loss 0.537412.
Train: 2018-08-04T23:14:14.542528: step 1575, loss 0.553504.
Train: 2018-08-04T23:14:14.792500: step 1576, loss 0.588076.
Train: 2018-08-04T23:14:15.042442: step 1577, loss 0.553813.
Train: 2018-08-04T23:14:15.292383: step 1578, loss 0.596332.
Train: 2018-08-04T23:14:15.542325: step 1579, loss 0.587971.
Train: 2018-08-04T23:14:15.792266: step 1580, loss 0.570687.
Test: 2018-08-04T23:14:17.041944: step 1580, loss 0.548454.
Train: 2018-08-04T23:14:17.276263: step 1581, loss 0.536682.
Train: 2018-08-04T23:14:17.526234: step 1582, loss 0.63904.
Train: 2018-08-04T23:14:17.776176: step 1583, loss 0.528164.
Train: 2018-08-04T23:14:18.026118: step 1584, loss 0.545051.
Train: 2018-08-04T23:14:18.276058: step 1585, loss 0.570507.
Train: 2018-08-04T23:14:18.526000: step 1586, loss 0.527739.
Train: 2018-08-04T23:14:18.775942: step 1587, loss 0.604811.
Train: 2018-08-04T23:14:19.025884: step 1588, loss 0.511513.
Train: 2018-08-04T23:14:19.275795: step 1589, loss 0.51112.
Train: 2018-08-04T23:14:19.525767: step 1590, loss 0.554043.
Test: 2018-08-04T23:14:20.775444: step 1590, loss 0.549102.
Train: 2018-08-04T23:14:21.025385: step 1591, loss 0.553746.
Train: 2018-08-04T23:14:21.275356: step 1592, loss 0.56217.
Train: 2018-08-04T23:14:21.525299: step 1593, loss 0.605429.
Train: 2018-08-04T23:14:21.775239: step 1594, loss 0.477066.
Train: 2018-08-04T23:14:22.025181: step 1595, loss 0.613339.
Train: 2018-08-04T23:14:22.275092: step 1596, loss 0.485826.
Train: 2018-08-04T23:14:22.525035: step 1597, loss 0.511152.
Train: 2018-08-04T23:14:22.775006: step 1598, loss 0.570404.
Train: 2018-08-04T23:14:23.024947: step 1599, loss 0.596079.
Train: 2018-08-04T23:14:23.274889: step 1600, loss 0.596855.
Test: 2018-08-04T23:14:24.524565: step 1600, loss 0.548463.
Train: 2018-08-04T23:14:25.368118: step 1601, loss 0.579669.
Train: 2018-08-04T23:14:25.633683: step 1602, loss 0.612806.
Train: 2018-08-04T23:14:25.868002: step 1603, loss 0.528356.
Train: 2018-08-04T23:14:26.133590: step 1604, loss 0.579369.
Train: 2018-08-04T23:14:26.367884: step 1605, loss 0.528312.
Train: 2018-08-04T23:14:26.617825: step 1606, loss 0.511373.
Train: 2018-08-04T23:14:26.867792: step 1607, loss 0.511741.
Train: 2018-08-04T23:14:27.117738: step 1608, loss 0.62178.
Train: 2018-08-04T23:14:27.367680: step 1609, loss 0.612992.
Train: 2018-08-04T23:14:27.617591: step 1610, loss 0.545675.
Test: 2018-08-04T23:14:28.867298: step 1610, loss 0.547935.
Train: 2018-08-04T23:14:29.101649: step 1611, loss 0.561764.
Train: 2018-08-04T23:14:29.351590: step 1612, loss 0.571016.
Train: 2018-08-04T23:14:29.601536: step 1613, loss 0.537484.
Train: 2018-08-04T23:14:29.851473: step 1614, loss 0.57074.
Train: 2018-08-04T23:14:30.101419: step 1615, loss 0.520148.
Train: 2018-08-04T23:14:30.351356: step 1616, loss 0.663196.
Train: 2018-08-04T23:14:30.601269: step 1617, loss 0.596111.
Train: 2018-08-04T23:14:30.851243: step 1618, loss 0.603852.
Train: 2018-08-04T23:14:31.101180: step 1619, loss 0.553918.
Train: 2018-08-04T23:14:31.351122: step 1620, loss 0.644985.
Test: 2018-08-04T23:14:32.600799: step 1620, loss 0.548737.
Train: 2018-08-04T23:14:32.835154: step 1621, loss 0.562587.
Train: 2018-08-04T23:14:33.085091: step 1622, loss 0.55447.
Train: 2018-08-04T23:14:33.335032: step 1623, loss 0.603231.
Train: 2018-08-04T23:14:33.584945: step 1624, loss 0.643616.
Train: 2018-08-04T23:14:33.834910: step 1625, loss 0.498811.
Train: 2018-08-04T23:14:34.084859: step 1626, loss 0.56275.
Train: 2018-08-04T23:14:34.334798: step 1627, loss 0.57039.
Train: 2018-08-04T23:14:34.584740: step 1628, loss 0.563296.
Train: 2018-08-04T23:14:34.834682: step 1629, loss 0.563501.
Train: 2018-08-04T23:14:35.084618: step 1630, loss 0.500149.
Test: 2018-08-04T23:14:36.334300: step 1630, loss 0.55014.
Train: 2018-08-04T23:14:36.568647: step 1631, loss 0.578954.
Train: 2018-08-04T23:14:36.818592: step 1632, loss 0.633616.
Train: 2018-08-04T23:14:37.068503: step 1633, loss 0.531768.
Train: 2018-08-04T23:14:37.318475: step 1634, loss 0.578782.
Train: 2018-08-04T23:14:37.568387: step 1635, loss 0.602486.
Train: 2018-08-04T23:14:37.818358: step 1636, loss 0.602108.
Train: 2018-08-04T23:14:38.068299: step 1637, loss 0.602233.
Train: 2018-08-04T23:14:38.318241: step 1638, loss 0.555636.
Train: 2018-08-04T23:14:38.568154: step 1639, loss 0.547745.
Train: 2018-08-04T23:14:38.818124: step 1640, loss 0.54787.
Test: 2018-08-04T23:14:40.067801: step 1640, loss 0.549001.
Train: 2018-08-04T23:14:40.302146: step 1641, loss 0.547673.
Train: 2018-08-04T23:14:40.552093: step 1642, loss 0.555416.
Train: 2018-08-04T23:14:40.802003: step 1643, loss 0.601907.
Train: 2018-08-04T23:14:41.051976: step 1644, loss 0.478154.
Train: 2018-08-04T23:14:41.301887: step 1645, loss 0.579047.
Train: 2018-08-04T23:14:41.551858: step 1646, loss 0.539659.
Train: 2018-08-04T23:14:41.801770: step 1647, loss 0.594842.
Train: 2018-08-04T23:14:42.051712: step 1648, loss 0.602554.
Train: 2018-08-04T23:14:42.301654: step 1649, loss 0.609784.
Train: 2018-08-04T23:14:42.551595: step 1650, loss 0.500517.
Test: 2018-08-04T23:14:43.801301: step 1650, loss 0.551089.
Train: 2018-08-04T23:14:44.051274: step 1651, loss 0.547828.
Train: 2018-08-04T23:14:44.301215: step 1652, loss 0.539474.
Train: 2018-08-04T23:14:44.551156: step 1653, loss 0.563045.
Train: 2018-08-04T23:14:44.801098: step 1654, loss 0.571239.
Train: 2018-08-04T23:14:45.051010: step 1655, loss 0.539646.
Train: 2018-08-04T23:14:45.300976: step 1656, loss 0.539372.
Train: 2018-08-04T23:14:45.550921: step 1657, loss 0.530813.
Train: 2018-08-04T23:14:45.800833: step 1658, loss 0.563324.
Train: 2018-08-04T23:14:46.050775: step 1659, loss 0.506436.
Train: 2018-08-04T23:14:46.300748: step 1660, loss 0.578482.
Test: 2018-08-04T23:14:47.566045: step 1660, loss 0.549507.
Train: 2018-08-04T23:14:47.753526: step 1661, loss 0.545035.
Train: 2018-08-04T23:14:47.987850: step 1662, loss 0.522095.
Train: 2018-08-04T23:14:48.237793: step 1663, loss 0.554679.
Train: 2018-08-04T23:14:48.487734: step 1664, loss 0.521549.
Train: 2018-08-04T23:14:48.737647: step 1665, loss 0.58707.
Train: 2018-08-04T23:14:48.987618: step 1666, loss 0.587369.
Train: 2018-08-04T23:14:49.237559: step 1667, loss 0.52955.
Train: 2018-08-04T23:14:49.487501: step 1668, loss 0.5287.
Train: 2018-08-04T23:14:49.737412: step 1669, loss 0.536787.
Train: 2018-08-04T23:14:49.987390: step 1670, loss 0.58747.
Test: 2018-08-04T23:14:51.237060: step 1670, loss 0.548528.
Train: 2018-08-04T23:14:51.471416: step 1671, loss 0.528655.
Train: 2018-08-04T23:14:51.721356: step 1672, loss 0.58783.
Train: 2018-08-04T23:14:51.971298: step 1673, loss 0.520333.
Train: 2018-08-04T23:14:52.221204: step 1674, loss 0.562438.
Train: 2018-08-04T23:14:52.455524: step 1675, loss 0.529241.
Train: 2018-08-04T23:14:52.705468: step 1676, loss 0.613161.
Train: 2018-08-04T23:14:52.955438: step 1677, loss 0.571095.
Train: 2018-08-04T23:14:53.205349: step 1678, loss 0.580123.
Train: 2018-08-04T23:14:53.455292: step 1679, loss 0.6128.
Train: 2018-08-04T23:14:53.705231: step 1680, loss 0.587824.
Test: 2018-08-04T23:14:54.970585: step 1680, loss 0.548413.
Train: 2018-08-04T23:14:55.204914: step 1681, loss 0.511804.
Train: 2018-08-04T23:14:55.454853: step 1682, loss 0.554118.
Train: 2018-08-04T23:14:55.704764: step 1683, loss 0.638018.
Train: 2018-08-04T23:14:55.954709: step 1684, loss 0.613038.
Train: 2018-08-04T23:14:56.204647: step 1685, loss 0.537177.
Train: 2018-08-04T23:14:56.454600: step 1686, loss 0.596085.
Train: 2018-08-04T23:14:56.704549: step 1687, loss 0.545892.
Train: 2018-08-04T23:14:56.954506: step 1688, loss 0.595721.
Train: 2018-08-04T23:14:57.204444: step 1689, loss 0.620431.
Train: 2018-08-04T23:14:57.454378: step 1690, loss 0.513311.
Test: 2018-08-04T23:14:58.688440: step 1690, loss 0.549271.
Train: 2018-08-04T23:14:58.938412: step 1691, loss 0.505186.
Train: 2018-08-04T23:14:59.188323: step 1692, loss 0.587435.
Train: 2018-08-04T23:14:59.438295: step 1693, loss 0.521963.
Train: 2018-08-04T23:14:59.688230: step 1694, loss 0.579177.
Train: 2018-08-04T23:14:59.938178: step 1695, loss 0.538422.
Train: 2018-08-04T23:15:00.188119: step 1696, loss 0.546309.
Train: 2018-08-04T23:15:00.438061: step 1697, loss 0.497796.
Train: 2018-08-04T23:15:00.688002: step 1698, loss 0.530004.
Train: 2018-08-04T23:15:00.937944: step 1699, loss 0.554319.
Train: 2018-08-04T23:15:01.187885: step 1700, loss 0.578605.
Test: 2018-08-04T23:15:02.437561: step 1700, loss 0.548129.
Train: 2018-08-04T23:15:03.249898: step 1701, loss 0.538036.
Train: 2018-08-04T23:15:03.499844: step 1702, loss 0.587048.
Train: 2018-08-04T23:15:03.749785: step 1703, loss 0.554537.
Train: 2018-08-04T23:15:03.999727: step 1704, loss 0.55432.
Train: 2018-08-04T23:15:04.249671: step 1705, loss 0.463679.
Train: 2018-08-04T23:15:04.499606: step 1706, loss 0.603448.
Train: 2018-08-04T23:15:04.749551: step 1707, loss 0.521416.
Train: 2018-08-04T23:15:04.999493: step 1708, loss 0.537554.
Train: 2018-08-04T23:15:05.265058: step 1709, loss 0.59573.
Train: 2018-08-04T23:15:05.499345: step 1710, loss 0.595724.
Test: 2018-08-04T23:15:06.733431: step 1710, loss 0.548469.
Train: 2018-08-04T23:15:06.967752: step 1711, loss 0.595872.
Train: 2018-08-04T23:15:07.217723: step 1712, loss 0.57116.
Train: 2018-08-04T23:15:07.467665: step 1713, loss 0.587571.
Train: 2018-08-04T23:15:07.717576: step 1714, loss 0.603961.
Train: 2018-08-04T23:15:07.967517: step 1715, loss 0.579051.
Train: 2018-08-04T23:15:08.217459: step 1716, loss 0.562236.
Train: 2018-08-04T23:15:08.467404: step 1717, loss 0.59591.
Train: 2018-08-04T23:15:08.717370: step 1718, loss 0.488331.
Train: 2018-08-04T23:15:08.967313: step 1719, loss 0.529574.
Train: 2018-08-04T23:15:09.217259: step 1720, loss 0.570727.
Test: 2018-08-04T23:15:10.466931: step 1720, loss 0.548018.
Train: 2018-08-04T23:15:10.701253: step 1721, loss 0.504809.
Train: 2018-08-04T23:15:10.966815: step 1722, loss 0.521184.
Train: 2018-08-04T23:15:11.216170: step 1723, loss 0.521142.
Train: 2018-08-04T23:15:11.450466: step 1724, loss 0.570911.
Train: 2018-08-04T23:15:11.700438: step 1725, loss 0.562524.
Train: 2018-08-04T23:15:11.950376: step 1726, loss 0.570807.
Train: 2018-08-04T23:15:12.200292: step 1727, loss 0.637205.
Train: 2018-08-04T23:15:12.450258: step 1728, loss 0.512486.
Train: 2018-08-04T23:15:12.700203: step 1729, loss 0.62903.
Train: 2018-08-04T23:15:12.950145: step 1730, loss 0.612101.
Test: 2018-08-04T23:15:14.199821: step 1730, loss 0.548634.
Train: 2018-08-04T23:15:14.434143: step 1731, loss 0.603638.
Train: 2018-08-04T23:15:14.684115: step 1732, loss 0.595394.
Train: 2018-08-04T23:15:14.934057: step 1733, loss 0.586999.
Train: 2018-08-04T23:15:15.184004: step 1734, loss 0.587324.
Train: 2018-08-04T23:15:15.433942: step 1735, loss 0.546174.
Train: 2018-08-04T23:15:15.683881: step 1736, loss 0.52231.
Train: 2018-08-04T23:15:15.933821: step 1737, loss 0.546295.
Train: 2018-08-04T23:15:16.183734: step 1738, loss 0.619045.
Train: 2018-08-04T23:15:16.433674: step 1739, loss 0.538553.
Train: 2018-08-04T23:15:16.683646: step 1740, loss 0.595281.
Test: 2018-08-04T23:15:17.917726: step 1740, loss 0.549325.
Train: 2018-08-04T23:15:18.167673: step 1741, loss 0.554537.
Train: 2018-08-04T23:15:18.417586: step 1742, loss 0.538529.
Train: 2018-08-04T23:15:18.667557: step 1743, loss 0.530512.
Train: 2018-08-04T23:15:18.917497: step 1744, loss 0.55504.
Train: 2018-08-04T23:15:19.167439: step 1745, loss 0.578569.
Train: 2018-08-04T23:15:19.417380: step 1746, loss 0.522968.
Train: 2018-08-04T23:15:19.682943: step 1747, loss 0.562977.
Train: 2018-08-04T23:15:19.917265: step 1748, loss 0.450053.
Train: 2018-08-04T23:15:20.167205: step 1749, loss 0.603128.
Train: 2018-08-04T23:15:20.417147: step 1750, loss 0.570919.
Test: 2018-08-04T23:15:21.666823: step 1750, loss 0.54799.
Train: 2018-08-04T23:15:21.901201: step 1751, loss 0.546366.
Train: 2018-08-04T23:15:22.151084: step 1752, loss 0.578883.
Train: 2018-08-04T23:15:22.401027: step 1753, loss 0.562726.
Train: 2018-08-04T23:15:22.650983: step 1754, loss 0.521788.
Train: 2018-08-04T23:15:22.900910: step 1755, loss 0.578658.
Train: 2018-08-04T23:15:23.150850: step 1756, loss 0.562472.
Train: 2018-08-04T23:15:23.400824: step 1757, loss 0.554382.
Train: 2018-08-04T23:15:23.650733: step 1758, loss 0.595312.
Train: 2018-08-04T23:15:23.900705: step 1759, loss 0.505013.
Train: 2018-08-04T23:15:24.150648: step 1760, loss 0.505027.
Test: 2018-08-04T23:15:25.400360: step 1760, loss 0.548687.
Train: 2018-08-04T23:15:25.650296: step 1761, loss 0.562677.
Train: 2018-08-04T23:15:25.900233: step 1762, loss 0.545945.
Train: 2018-08-04T23:15:26.150179: step 1763, loss 0.587802.
Train: 2018-08-04T23:15:26.400091: step 1764, loss 0.620664.
Train: 2018-08-04T23:15:26.650031: step 1765, loss 0.546009.
Train: 2018-08-04T23:15:26.900004: step 1766, loss 0.520894.
Train: 2018-08-04T23:15:27.134296: step 1767, loss 0.62067.
Train: 2018-08-04T23:15:27.384234: step 1768, loss 0.562113.
Train: 2018-08-04T23:15:27.634208: step 1769, loss 0.545638.
Train: 2018-08-04T23:15:27.884149: step 1770, loss 0.579086.
Test: 2018-08-04T23:15:29.133825: step 1770, loss 0.54722.
Train: 2018-08-04T23:15:29.368175: step 1771, loss 0.587385.
Train: 2018-08-04T23:15:29.633736: step 1772, loss 0.554322.
Train: 2018-08-04T23:15:29.883674: step 1773, loss 0.53772.
Train: 2018-08-04T23:15:30.133623: step 1774, loss 0.587301.
Train: 2018-08-04T23:15:30.383557: step 1775, loss 0.604014.
Train: 2018-08-04T23:15:30.633507: step 1776, loss 0.56253.
Train: 2018-08-04T23:15:30.883415: step 1777, loss 0.587299.
Train: 2018-08-04T23:15:31.133357: step 1778, loss 0.554356.
Train: 2018-08-04T23:15:31.383298: step 1779, loss 0.521547.
Train: 2018-08-04T23:15:31.633240: step 1780, loss 0.554191.
Test: 2018-08-04T23:15:32.882946: step 1780, loss 0.548879.
Train: 2018-08-04T23:15:33.117295: step 1781, loss 0.538132.
Train: 2018-08-04T23:15:33.367238: step 1782, loss 0.628118.
Train: 2018-08-04T23:15:33.617183: step 1783, loss 0.636286.
Train: 2018-08-04T23:15:33.867119: step 1784, loss 0.562347.
Train: 2018-08-04T23:15:34.117053: step 1785, loss 0.611355.
Train: 2018-08-04T23:15:34.367005: step 1786, loss 0.546744.
Train: 2018-08-04T23:15:34.616946: step 1787, loss 0.554681.
Train: 2018-08-04T23:15:34.866887: step 1788, loss 0.554784.
Train: 2018-08-04T23:15:35.116831: step 1789, loss 0.56266.
Train: 2018-08-04T23:15:35.366740: step 1790, loss 0.506686.
Test: 2018-08-04T23:15:36.616447: step 1790, loss 0.549415.
Train: 2018-08-04T23:15:36.850798: step 1791, loss 0.634855.
Train: 2018-08-04T23:15:37.100735: step 1792, loss 0.546833.
Train: 2018-08-04T23:15:37.350685: step 1793, loss 0.594788.
Train: 2018-08-04T23:15:37.600625: step 1794, loss 0.594962.
Train: 2018-08-04T23:15:37.850534: step 1795, loss 0.523208.
Train: 2018-08-04T23:15:38.100505: step 1796, loss 0.578817.
Train: 2018-08-04T23:15:38.350443: step 1797, loss 0.467201.
Train: 2018-08-04T23:15:38.600389: step 1798, loss 0.522991.
Train: 2018-08-04T23:15:38.865928: step 1799, loss 0.618967.
Train: 2018-08-04T23:15:39.115862: step 1800, loss 0.514911.
Test: 2018-08-04T23:15:40.349948: step 1800, loss 0.548693.
Train: 2018-08-04T23:15:41.209124: step 1801, loss 0.578752.
Train: 2018-08-04T23:15:41.471113: step 1802, loss 0.546738.
Train: 2018-08-04T23:15:41.721059: step 1803, loss 0.61082.
Train: 2018-08-04T23:15:41.971001: step 1804, loss 0.602954.
Train: 2018-08-04T23:15:42.220943: step 1805, loss 0.546575.
Train: 2018-08-04T23:15:42.470885: step 1806, loss 0.570978.
Train: 2018-08-04T23:15:42.720820: step 1807, loss 0.5304.
Train: 2018-08-04T23:15:42.970767: step 1808, loss 0.595062.
Train: 2018-08-04T23:15:43.220709: step 1809, loss 0.554744.
Train: 2018-08-04T23:15:43.486240: step 1810, loss 0.498282.
Test: 2018-08-04T23:15:44.720327: step 1810, loss 0.549024.
Train: 2018-08-04T23:15:44.954672: step 1811, loss 0.578965.
Train: 2018-08-04T23:15:45.157724: step 1812, loss 0.527981.
Train: 2018-08-04T23:15:45.407700: step 1813, loss 0.595138.
Train: 2018-08-04T23:15:45.657641: step 1814, loss 0.586667.
Train: 2018-08-04T23:15:45.907583: step 1815, loss 0.546571.
Train: 2018-08-04T23:15:46.157525: step 1816, loss 0.554544.
Train: 2018-08-04T23:15:46.407466: step 1817, loss 0.546622.
Train: 2018-08-04T23:15:46.657408: step 1818, loss 0.562637.
Train: 2018-08-04T23:15:46.907349: step 1819, loss 0.587054.
Train: 2018-08-04T23:15:47.157290: step 1820, loss 0.488849.
Test: 2018-08-04T23:15:48.406963: step 1820, loss 0.548714.
Train: 2018-08-04T23:15:48.641312: step 1821, loss 0.652985.
Train: 2018-08-04T23:15:48.891254: step 1822, loss 0.587052.
Train: 2018-08-04T23:15:49.141195: step 1823, loss 0.546053.
Train: 2018-08-04T23:15:49.391137: step 1824, loss 0.562168.
Train: 2018-08-04T23:15:49.641078: step 1825, loss 0.521906.
Train: 2018-08-04T23:15:49.891022: step 1826, loss 0.60324.
Train: 2018-08-04T23:15:50.140962: step 1827, loss 0.480511.
Train: 2018-08-04T23:15:50.390874: step 1828, loss 0.603745.
Train: 2018-08-04T23:15:50.640817: step 1829, loss 0.611229.
Train: 2018-08-04T23:15:50.890785: step 1830, loss 0.488914.
Test: 2018-08-04T23:15:52.140464: step 1830, loss 0.548678.
Train: 2018-08-04T23:15:52.374786: step 1831, loss 0.595099.
Train: 2018-08-04T23:15:52.624754: step 1832, loss 0.578646.
Train: 2018-08-04T23:15:52.874698: step 1833, loss 0.546494.
Train: 2018-08-04T23:15:53.124639: step 1834, loss 0.570945.
Train: 2018-08-04T23:15:53.374581: step 1835, loss 0.595054.
Train: 2018-08-04T23:15:53.624491: step 1836, loss 0.586806.
Train: 2018-08-04T23:15:53.874464: step 1837, loss 0.546396.
Train: 2018-08-04T23:15:54.124406: step 1838, loss 0.587641.
Train: 2018-08-04T23:15:54.374346: step 1839, loss 0.54622.
Train: 2018-08-04T23:15:54.624288: step 1840, loss 0.553751.
Test: 2018-08-04T23:15:55.858343: step 1840, loss 0.549186.
Train: 2018-08-04T23:15:56.092690: step 1841, loss 0.521977.
Train: 2018-08-04T23:15:56.327015: step 1842, loss 0.546259.
Train: 2018-08-04T23:15:56.576956: step 1843, loss 0.619458.
Train: 2018-08-04T23:15:56.826899: step 1844, loss 0.538427.
Train: 2018-08-04T23:15:57.076809: step 1845, loss 0.562251.
Train: 2018-08-04T23:15:57.326780: step 1846, loss 0.537908.
Train: 2018-08-04T23:15:57.576723: step 1847, loss 0.570577.
Train: 2018-08-04T23:15:57.826633: step 1848, loss 0.587463.
Train: 2018-08-04T23:15:58.076606: step 1849, loss 0.595101.
Train: 2018-08-04T23:15:58.326516: step 1850, loss 0.521809.
Test: 2018-08-04T23:15:59.576224: step 1850, loss 0.548589.
Train: 2018-08-04T23:15:59.810569: step 1851, loss 0.546898.
Train: 2018-08-04T23:16:00.076136: step 1852, loss 0.611712.
Train: 2018-08-04T23:16:00.326073: step 1853, loss 0.553936.
Train: 2018-08-04T23:16:00.576020: step 1854, loss 0.554476.
Train: 2018-08-04T23:16:00.825962: step 1855, loss 0.587296.
Train: 2018-08-04T23:16:01.075903: step 1856, loss 0.562609.
Train: 2018-08-04T23:16:01.325813: step 1857, loss 0.61095.
Train: 2018-08-04T23:16:01.575785: step 1858, loss 0.49749.
Train: 2018-08-04T23:16:01.825696: step 1859, loss 0.602926.
Train: 2018-08-04T23:16:02.075670: step 1860, loss 0.546337.
Test: 2018-08-04T23:16:03.325345: step 1860, loss 0.549302.
Train: 2018-08-04T23:16:03.559696: step 1861, loss 0.537989.
Train: 2018-08-04T23:16:03.809634: step 1862, loss 0.571292.
Train: 2018-08-04T23:16:04.059548: step 1863, loss 0.595249.
Train: 2018-08-04T23:16:04.309520: step 1864, loss 0.497625.
Train: 2018-08-04T23:16:04.559432: step 1865, loss 0.586263.
Train: 2018-08-04T23:16:04.809374: step 1866, loss 0.52197.
Train: 2018-08-04T23:16:05.059346: step 1867, loss 0.546096.
Train: 2018-08-04T23:16:05.309259: step 1868, loss 0.498019.
Train: 2018-08-04T23:16:05.559230: step 1869, loss 0.611035.
Train: 2018-08-04T23:16:05.809139: step 1870, loss 0.537603.
Test: 2018-08-04T23:16:07.043225: step 1870, loss 0.548818.
Train: 2018-08-04T23:16:07.355651: step 1871, loss 0.562936.
Train: 2018-08-04T23:16:07.605618: step 1872, loss 0.546273.
Train: 2018-08-04T23:16:07.855535: step 1873, loss 0.569759.
Train: 2018-08-04T23:16:08.105502: step 1874, loss 0.545676.
Train: 2018-08-04T23:16:08.355419: step 1875, loss 0.561608.
Train: 2018-08-04T23:16:08.605359: step 1876, loss 0.587893.
Train: 2018-08-04T23:16:08.855331: step 1877, loss 0.48752.
Train: 2018-08-04T23:16:09.105273: step 1878, loss 0.578812.
Train: 2018-08-04T23:16:09.355217: step 1879, loss 0.487856.
Train: 2018-08-04T23:16:09.605124: step 1880, loss 0.603466.
Test: 2018-08-04T23:16:10.839211: step 1880, loss 0.548379.
Train: 2018-08-04T23:16:11.089152: step 1881, loss 0.603844.
Train: 2018-08-04T23:16:11.339120: step 1882, loss 0.563889.
Train: 2018-08-04T23:16:11.589066: step 1883, loss 0.554359.
Train: 2018-08-04T23:16:11.839008: step 1884, loss 0.611091.
Train: 2018-08-04T23:16:12.088948: step 1885, loss 0.604508.
Train: 2018-08-04T23:16:12.338861: step 1886, loss 0.597451.
Train: 2018-08-04T23:16:12.588831: step 1887, loss 0.587416.
Train: 2018-08-04T23:16:12.838773: step 1888, loss 0.544507.
Train: 2018-08-04T23:16:13.088714: step 1889, loss 0.619482.
Train: 2018-08-04T23:16:13.338655: step 1890, loss 0.519897.
Test: 2018-08-04T23:16:14.603953: step 1890, loss 0.548974.
Train: 2018-08-04T23:16:14.838305: step 1891, loss 0.653429.
Train: 2018-08-04T23:16:15.088247: step 1892, loss 0.522027.
Train: 2018-08-04T23:16:15.338187: step 1893, loss 0.586881.
Train: 2018-08-04T23:16:15.588099: step 1894, loss 0.521896.
Train: 2018-08-04T23:16:15.838041: step 1895, loss 0.546942.
Train: 2018-08-04T23:16:16.088013: step 1896, loss 0.554625.
Train: 2018-08-04T23:16:16.337953: step 1897, loss 0.563167.
Train: 2018-08-04T23:16:16.587896: step 1898, loss 0.54631.
Train: 2018-08-04T23:16:16.853458: step 1899, loss 0.546205.
Train: 2018-08-04T23:16:17.103400: step 1900, loss 0.545515.
Test: 2018-08-04T23:16:18.353076: step 1900, loss 0.547508.
Train: 2018-08-04T23:16:19.181038: step 1901, loss 0.530397.
Train: 2018-08-04T23:16:19.430949: step 1902, loss 0.522911.
Train: 2018-08-04T23:16:19.680891: step 1903, loss 0.563321.
Train: 2018-08-04T23:16:19.930863: step 1904, loss 0.569404.
Train: 2018-08-04T23:16:20.180798: step 1905, loss 0.521425.
Train: 2018-08-04T23:16:20.430740: step 1906, loss 0.578789.
Train: 2018-08-04T23:16:20.680687: step 1907, loss 0.530623.
Train: 2018-08-04T23:16:20.930628: step 1908, loss 0.555479.
Train: 2018-08-04T23:16:21.180569: step 1909, loss 0.580449.
Train: 2018-08-04T23:16:21.430506: step 1910, loss 0.545986.
Test: 2018-08-04T23:16:22.680188: step 1910, loss 0.548477.
Train: 2018-08-04T23:16:22.914534: step 1911, loss 0.563118.
Train: 2018-08-04T23:16:23.164480: step 1912, loss 0.596617.
Train: 2018-08-04T23:16:23.414421: step 1913, loss 0.563194.
Train: 2018-08-04T23:16:23.664363: step 1914, loss 0.553232.
Train: 2018-08-04T23:16:23.914304: step 1915, loss 0.569456.
Train: 2018-08-04T23:16:24.164245: step 1916, loss 0.538438.
Train: 2018-08-04T23:16:24.414184: step 1917, loss 0.587747.
Train: 2018-08-04T23:16:24.648511: step 1918, loss 0.512396.
Train: 2018-08-04T23:16:24.898452: step 1919, loss 0.619793.
Train: 2018-08-04T23:16:25.148394: step 1920, loss 0.645091.
Test: 2018-08-04T23:16:26.398067: step 1920, loss 0.55063.
Train: 2018-08-04T23:16:26.632412: step 1921, loss 0.521177.
Train: 2018-08-04T23:16:26.882361: step 1922, loss 0.578985.
Train: 2018-08-04T23:16:27.132270: step 1923, loss 0.620578.
Train: 2018-08-04T23:16:27.382243: step 1924, loss 0.504983.
Train: 2018-08-04T23:16:27.632154: step 1925, loss 0.538333.
Train: 2018-08-04T23:16:27.882123: step 1926, loss 0.571779.
Train: 2018-08-04T23:16:28.132066: step 1927, loss 0.586321.
Train: 2018-08-04T23:16:28.382006: step 1928, loss 0.530475.
Train: 2018-08-04T23:16:28.647575: step 1929, loss 0.595901.
Train: 2018-08-04T23:16:28.897516: step 1930, loss 0.554134.
Test: 2018-08-04T23:16:30.147189: step 1930, loss 0.549095.
Train: 2018-08-04T23:16:30.381541: step 1931, loss 0.538614.
Train: 2018-08-04T23:16:30.631481: step 1932, loss 0.611894.
Train: 2018-08-04T23:16:30.881423: step 1933, loss 0.587449.
Train: 2018-08-04T23:16:31.131367: step 1934, loss 0.4906.
Train: 2018-08-04T23:16:31.381305: step 1935, loss 0.554963.
Train: 2018-08-04T23:16:31.631248: step 1936, loss 0.513964.
Train: 2018-08-04T23:16:31.881189: step 1937, loss 0.521713.
Train: 2018-08-04T23:16:32.146748: step 1938, loss 0.562839.
Train: 2018-08-04T23:16:32.396662: step 1939, loss 0.635321.
Train: 2018-08-04T23:16:32.646605: step 1940, loss 0.5714.
Test: 2018-08-04T23:16:33.880691: step 1940, loss 0.549423.
Train: 2018-08-04T23:16:34.130631: step 1941, loss 0.480775.
Train: 2018-08-04T23:16:34.396194: step 1942, loss 0.554522.
Train: 2018-08-04T23:16:34.646170: step 1943, loss 0.554263.
Train: 2018-08-04T23:16:34.896102: step 1944, loss 0.546157.
Train: 2018-08-04T23:16:35.146056: step 1945, loss 0.579.
Train: 2018-08-04T23:16:35.395960: step 1946, loss 0.619362.
Train: 2018-08-04T23:16:35.645902: step 1947, loss 0.529898.
Train: 2018-08-04T23:16:35.895877: step 1948, loss 0.522133.
Train: 2018-08-04T23:16:36.145819: step 1949, loss 0.586698.
Train: 2018-08-04T23:16:36.395747: step 1950, loss 0.587778.
Test: 2018-08-04T23:16:37.629813: step 1950, loss 0.548722.
Train: 2018-08-04T23:16:37.879784: step 1951, loss 0.578509.
Train: 2018-08-04T23:16:38.129724: step 1952, loss 0.579007.
Train: 2018-08-04T23:16:38.379667: step 1953, loss 0.553909.
Train: 2018-08-04T23:16:38.629578: step 1954, loss 0.645503.
Train: 2018-08-04T23:16:38.879549: step 1955, loss 0.56252.
Train: 2018-08-04T23:16:39.129489: step 1956, loss 0.505051.
Train: 2018-08-04T23:16:39.379433: step 1957, loss 0.53021.
Train: 2018-08-04T23:16:39.629345: step 1958, loss 0.571298.
Train: 2018-08-04T23:16:39.879314: step 1959, loss 0.505451.
Train: 2018-08-04T23:16:40.129253: step 1960, loss 0.513833.
Test: 2018-08-04T23:16:41.378934: step 1960, loss 0.548837.
Train: 2018-08-04T23:16:41.613287: step 1961, loss 0.595434.
Train: 2018-08-04T23:16:41.863229: step 1962, loss 0.538361.
Train: 2018-08-04T23:16:42.066297: step 1963, loss 0.615531.
Train: 2018-08-04T23:16:42.316242: step 1964, loss 0.579036.
Train: 2018-08-04T23:16:42.566186: step 1965, loss 0.563218.
Train: 2018-08-04T23:16:42.816129: step 1966, loss 0.521505.
Train: 2018-08-04T23:16:43.066069: step 1967, loss 0.553932.
Train: 2018-08-04T23:16:43.316011: step 1968, loss 0.562684.
Train: 2018-08-04T23:16:43.565922: step 1969, loss 0.488327.
Train: 2018-08-04T23:16:43.815864: step 1970, loss 0.529562.
Test: 2018-08-04T23:16:45.065570: step 1970, loss 0.547731.
Train: 2018-08-04T23:16:45.299892: step 1971, loss 0.587179.
Train: 2018-08-04T23:16:45.549834: step 1972, loss 0.546034.
Train: 2018-08-04T23:16:45.799773: step 1973, loss 0.579046.
Train: 2018-08-04T23:16:46.049746: step 1974, loss 0.529267.
Train: 2018-08-04T23:16:46.299690: step 1975, loss 0.504144.
Train: 2018-08-04T23:16:46.549629: step 1976, loss 0.504127.
Train: 2018-08-04T23:16:46.799541: step 1977, loss 0.604308.
Train: 2018-08-04T23:16:47.049507: step 1978, loss 0.486886.
Train: 2018-08-04T23:16:47.299457: step 1979, loss 0.570685.
Train: 2018-08-04T23:16:47.549394: step 1980, loss 0.588233.
Test: 2018-08-04T23:16:48.799071: step 1980, loss 0.548402.
Train: 2018-08-04T23:16:49.033428: step 1981, loss 0.536322.
Train: 2018-08-04T23:16:49.283340: step 1982, loss 0.545178.
Train: 2018-08-04T23:16:49.533310: step 1983, loss 0.545416.
Train: 2018-08-04T23:16:49.783247: step 1984, loss 0.621725.
Train: 2018-08-04T23:16:50.033191: step 1985, loss 0.485542.
Train: 2018-08-04T23:16:50.283131: step 1986, loss 0.562377.
Train: 2018-08-04T23:16:50.533071: step 1987, loss 0.562184.
Train: 2018-08-04T23:16:50.783012: step 1988, loss 0.622101.
Train: 2018-08-04T23:16:51.032954: step 1989, loss 0.571507.
Train: 2018-08-04T23:16:51.267278: step 1990, loss 0.665475.
Test: 2018-08-04T23:16:52.516950: step 1990, loss 0.548905.
Train: 2018-08-04T23:16:52.751273: step 1991, loss 0.579795.
Train: 2018-08-04T23:16:53.001214: step 1992, loss 0.562005.
Train: 2018-08-04T23:16:53.251154: step 1993, loss 0.621427.
Train: 2018-08-04T23:16:53.501095: step 1994, loss 0.562169.
Train: 2018-08-04T23:16:53.751043: step 1995, loss 0.545881.
Train: 2018-08-04T23:16:54.001009: step 1996, loss 0.545838.
Train: 2018-08-04T23:16:54.250951: step 1997, loss 0.562179.
Train: 2018-08-04T23:16:54.500863: step 1998, loss 0.554021.
Train: 2018-08-04T23:16:54.750804: step 1999, loss 0.595441.
Train: 2018-08-04T23:16:55.000773: step 2000, loss 0.620325.
Test: 2018-08-04T23:16:56.250451: step 2000, loss 0.549081.
Train: 2018-08-04T23:16:57.094034: step 2001, loss 0.497255.
Train: 2018-08-04T23:16:57.343976: step 2002, loss 0.578854.
Train: 2018-08-04T23:16:57.593912: step 2003, loss 0.513539.
Train: 2018-08-04T23:16:57.843861: step 2004, loss 0.595276.
Train: 2018-08-04T23:16:58.093770: step 2005, loss 0.586934.
Train: 2018-08-04T23:16:58.343742: step 2006, loss 0.554345.
Train: 2018-08-04T23:16:58.593653: step 2007, loss 0.554738.
Train: 2018-08-04T23:16:58.843594: step 2008, loss 0.497807.
Train: 2018-08-04T23:16:59.093566: step 2009, loss 0.554659.
Train: 2018-08-04T23:16:59.343478: step 2010, loss 0.578865.
Test: 2018-08-04T23:17:00.577566: step 2010, loss 0.548794.
Train: 2018-08-04T23:17:00.827504: step 2011, loss 0.554426.
Train: 2018-08-04T23:17:01.077446: step 2012, loss 0.497778.
Train: 2018-08-04T23:17:01.327419: step 2013, loss 0.595425.
Train: 2018-08-04T23:17:01.577360: step 2014, loss 0.53028.
Train: 2018-08-04T23:17:01.827272: step 2015, loss 0.578825.
Train: 2018-08-04T23:17:02.077243: step 2016, loss 0.611577.
Train: 2018-08-04T23:17:02.327180: step 2017, loss 0.570731.
Train: 2018-08-04T23:17:02.577126: step 2018, loss 0.505528.
Train: 2018-08-04T23:17:02.827063: step 2019, loss 0.578808.
Train: 2018-08-04T23:17:03.076979: step 2020, loss 0.480806.
Test: 2018-08-04T23:17:04.311064: step 2020, loss 0.549457.
Train: 2018-08-04T23:17:04.561035: step 2021, loss 0.53818.
Train: 2018-08-04T23:17:04.810948: step 2022, loss 0.529463.
Train: 2018-08-04T23:17:05.060888: step 2023, loss 0.562384.
Train: 2018-08-04T23:17:05.310861: step 2024, loss 0.480018.
Train: 2018-08-04T23:17:05.560797: step 2025, loss 0.512466.
Train: 2018-08-04T23:17:05.810743: step 2026, loss 0.528822.
Train: 2018-08-04T23:17:06.060680: step 2027, loss 0.578606.
Train: 2018-08-04T23:17:06.310626: step 2028, loss 0.587504.
Train: 2018-08-04T23:17:06.560568: step 2029, loss 0.545023.
Train: 2018-08-04T23:17:06.810511: step 2030, loss 0.621577.
Test: 2018-08-04T23:17:08.044565: step 2030, loss 0.546577.
Train: 2018-08-04T23:17:08.278888: step 2031, loss 0.537353.
Train: 2018-08-04T23:17:08.528851: step 2032, loss 0.520186.
Train: 2018-08-04T23:17:08.778798: step 2033, loss 0.554282.
Train: 2018-08-04T23:17:09.028740: step 2034, loss 0.485444.
Train: 2018-08-04T23:17:09.278682: step 2035, loss 0.527694.
Train: 2018-08-04T23:17:09.528622: step 2036, loss 0.580017.
Train: 2018-08-04T23:17:09.778567: step 2037, loss 0.596843.
Train: 2018-08-04T23:17:10.028506: step 2038, loss 0.510407.
Train: 2018-08-04T23:17:10.278418: step 2039, loss 0.588016.
Train: 2018-08-04T23:17:10.528389: step 2040, loss 0.614595.
Test: 2018-08-04T23:17:11.778065: step 2040, loss 0.547911.
Train: 2018-08-04T23:17:12.012420: step 2041, loss 0.571379.
Train: 2018-08-04T23:17:12.262361: step 2042, loss 0.570815.
Train: 2018-08-04T23:17:12.512270: step 2043, loss 0.563099.
Train: 2018-08-04T23:17:12.762210: step 2044, loss 0.527834.
Train: 2018-08-04T23:17:13.012176: step 2045, loss 0.60567.
Train: 2018-08-04T23:17:13.262127: step 2046, loss 0.588235.
Train: 2018-08-04T23:17:13.512064: step 2047, loss 0.579163.
Train: 2018-08-04T23:17:13.762000: step 2048, loss 0.578943.
Train: 2018-08-04T23:17:14.011951: step 2049, loss 0.537377.
Train: 2018-08-04T23:17:14.261888: step 2050, loss 0.477906.
Test: 2018-08-04T23:17:15.511566: step 2050, loss 0.546728.
Train: 2018-08-04T23:17:15.745888: step 2051, loss 0.545508.
Train: 2018-08-04T23:17:15.995858: step 2052, loss 0.705834.
Train: 2018-08-04T23:17:16.245800: step 2053, loss 0.654542.
Train: 2018-08-04T23:17:16.495741: step 2054, loss 0.620641.
Train: 2018-08-04T23:17:16.745677: step 2055, loss 0.554266.
Train: 2018-08-04T23:17:16.995594: step 2056, loss 0.562781.
Train: 2018-08-04T23:17:17.245560: step 2057, loss 0.636334.
Train: 2018-08-04T23:17:17.495507: step 2058, loss 0.587001.
Train: 2018-08-04T23:17:17.745449: step 2059, loss 0.554456.
Train: 2018-08-04T23:17:17.995388: step 2060, loss 0.546798.
Test: 2018-08-04T23:17:19.245066: step 2060, loss 0.551193.
Train: 2018-08-04T23:17:19.495040: step 2061, loss 0.578827.
Train: 2018-08-04T23:17:19.744950: step 2062, loss 0.547174.
Train: 2018-08-04T23:17:19.994892: step 2063, loss 0.57885.
Train: 2018-08-04T23:17:20.244863: step 2064, loss 0.563017.
Train: 2018-08-04T23:17:20.494774: step 2065, loss 0.594533.
Train: 2018-08-04T23:17:20.744740: step 2066, loss 0.531899.
Train: 2018-08-04T23:17:20.994658: step 2067, loss 0.61024.
Train: 2018-08-04T23:17:21.244630: step 2068, loss 0.570929.
Train: 2018-08-04T23:17:21.494570: step 2069, loss 0.563304.
Train: 2018-08-04T23:17:21.744512: step 2070, loss 0.586629.
Test: 2018-08-04T23:17:22.994188: step 2070, loss 0.549379.
Train: 2018-08-04T23:17:23.228534: step 2071, loss 0.524546.
Train: 2018-08-04T23:17:23.478484: step 2072, loss 0.571156.
Train: 2018-08-04T23:17:23.728418: step 2073, loss 0.547925.
Train: 2018-08-04T23:17:23.978364: step 2074, loss 0.586822.
Train: 2018-08-04T23:17:24.228305: step 2075, loss 0.563385.
Train: 2018-08-04T23:17:24.478246: step 2076, loss 0.586659.
Train: 2018-08-04T23:17:24.728158: step 2077, loss 0.594334.
Train: 2018-08-04T23:17:24.978099: step 2078, loss 0.594352.
Train: 2018-08-04T23:17:25.228065: step 2079, loss 0.547995.
Train: 2018-08-04T23:17:25.462392: step 2080, loss 0.478358.
Test: 2018-08-04T23:17:26.712093: step 2080, loss 0.551001.
Train: 2018-08-04T23:17:26.946419: step 2081, loss 0.609893.
Train: 2018-08-04T23:17:27.196361: step 2082, loss 0.571058.
Train: 2018-08-04T23:17:27.446302: step 2083, loss 0.586651.
Train: 2018-08-04T23:17:27.696243: step 2084, loss 0.547781.
Train: 2018-08-04T23:17:27.946180: step 2085, loss 0.555447.
Train: 2018-08-04T23:17:28.196096: step 2086, loss 0.501051.
Train: 2018-08-04T23:17:28.446067: step 2087, loss 0.657101.
Train: 2018-08-04T23:17:28.696010: step 2088, loss 0.516104.
Train: 2018-08-04T23:17:28.945944: step 2089, loss 0.508397.
Train: 2018-08-04T23:17:29.195863: step 2090, loss 0.507996.
Test: 2018-08-04T23:17:30.461190: step 2090, loss 0.550368.
Train: 2018-08-04T23:17:30.711133: step 2091, loss 0.5076.
Train: 2018-08-04T23:17:30.961104: step 2092, loss 0.594934.
Train: 2018-08-04T23:17:31.211047: step 2093, loss 0.586757.
Train: 2018-08-04T23:17:31.460956: step 2094, loss 0.578744.
Train: 2018-08-04T23:17:31.710922: step 2095, loss 0.506645.
Train: 2018-08-04T23:17:31.960875: step 2096, loss 0.635358.
Train: 2018-08-04T23:17:32.210780: step 2097, loss 0.570952.
Train: 2018-08-04T23:17:32.476345: step 2098, loss 0.57066.
Train: 2018-08-04T23:17:32.710696: step 2099, loss 0.530143.
Train: 2018-08-04T23:17:32.960636: step 2100, loss 0.595355.
Test: 2018-08-04T23:17:34.210312: step 2100, loss 0.54997.
Train: 2018-08-04T23:17:35.069512: step 2101, loss 0.611785.
Train: 2018-08-04T23:17:35.319458: step 2102, loss 0.61961.
Train: 2018-08-04T23:17:35.569399: step 2103, loss 0.58709.
Train: 2018-08-04T23:17:35.819336: step 2104, loss 0.570751.
Train: 2018-08-04T23:17:36.069253: step 2105, loss 0.481887.
Train: 2018-08-04T23:17:36.319218: step 2106, loss 0.562489.
Train: 2018-08-04T23:17:36.569161: step 2107, loss 0.58696.
Train: 2018-08-04T23:17:36.819107: step 2108, loss 0.489992.
Train: 2018-08-04T23:17:37.069046: step 2109, loss 0.578984.
Train: 2018-08-04T23:17:37.318984: step 2110, loss 0.554584.
Test: 2018-08-04T23:17:38.584288: step 2110, loss 0.548923.
Train: 2018-08-04T23:17:38.818640: step 2111, loss 0.514077.
Train: 2018-08-04T23:17:39.068580: step 2112, loss 0.546422.
Train: 2018-08-04T23:17:39.318522: step 2113, loss 0.636281.
Train: 2018-08-04T23:17:39.521599: step 2114, loss 0.580124.
Train: 2018-08-04T23:17:39.771541: step 2115, loss 0.562571.
Train: 2018-08-04T23:17:40.021451: step 2116, loss 0.554388.
Train: 2018-08-04T23:17:40.271424: step 2117, loss 0.603299.
Train: 2018-08-04T23:17:40.521334: step 2118, loss 0.537979.
Train: 2018-08-04T23:17:40.771301: step 2119, loss 0.529996.
Train: 2018-08-04T23:17:41.021250: step 2120, loss 0.562688.
Test: 2018-08-04T23:17:42.270925: step 2120, loss 0.548435.
Train: 2018-08-04T23:17:42.505246: step 2121, loss 0.538097.
Train: 2018-08-04T23:17:42.755220: step 2122, loss 0.513381.
Train: 2018-08-04T23:17:43.005131: step 2123, loss 0.619909.
Train: 2018-08-04T23:17:43.255099: step 2124, loss 0.562532.
Train: 2018-08-04T23:17:43.505051: step 2125, loss 0.51344.
Train: 2018-08-04T23:17:43.754987: step 2126, loss 0.579073.
Train: 2018-08-04T23:17:44.004928: step 2127, loss 0.57077.
Train: 2018-08-04T23:17:44.254837: step 2128, loss 0.562491.
Train: 2018-08-04T23:17:44.504811: step 2129, loss 0.595641.
Train: 2018-08-04T23:17:44.739096: step 2130, loss 0.570682.
Test: 2018-08-04T23:17:45.988804: step 2130, loss 0.548062.
Train: 2018-08-04T23:17:46.238774: step 2131, loss 0.521391.
Train: 2018-08-04T23:17:46.488715: step 2132, loss 0.57893.
Train: 2018-08-04T23:17:46.738659: step 2133, loss 0.521408.
Train: 2018-08-04T23:17:46.988569: step 2134, loss 0.554479.
Train: 2018-08-04T23:17:47.238544: step 2135, loss 0.537751.
Train: 2018-08-04T23:17:47.488478: step 2136, loss 0.521164.
Train: 2018-08-04T23:17:47.738428: step 2137, loss 0.661527.
Train: 2018-08-04T23:17:47.988361: step 2138, loss 0.546015.
Train: 2018-08-04T23:17:48.238278: step 2139, loss 0.603825.
Train: 2018-08-04T23:17:48.488218: step 2140, loss 0.546085.
Test: 2018-08-04T23:17:49.722305: step 2140, loss 0.548733.
Train: 2018-08-04T23:17:49.972246: step 2141, loss 0.570928.
Train: 2018-08-04T23:17:50.222218: step 2142, loss 0.52973.
Train: 2018-08-04T23:17:50.472155: step 2143, loss 0.513174.
Train: 2018-08-04T23:17:50.722101: step 2144, loss 0.645186.
Train: 2018-08-04T23:17:50.972043: step 2145, loss 0.661438.
Train: 2018-08-04T23:17:51.221984: step 2146, loss 0.578988.
Train: 2018-08-04T23:17:51.471895: step 2147, loss 0.587034.
Train: 2018-08-04T23:17:51.721866: step 2148, loss 0.595144.
Train: 2018-08-04T23:17:51.971808: step 2149, loss 0.635757.
Train: 2018-08-04T23:17:52.221752: step 2150, loss 0.546551.
Test: 2018-08-04T23:17:53.471426: step 2150, loss 0.548651.
Train: 2018-08-04T23:17:53.705747: step 2151, loss 0.506504.
Train: 2018-08-04T23:17:53.955688: step 2152, loss 0.651073.
Train: 2018-08-04T23:17:54.205660: step 2153, loss 0.586762.
Train: 2018-08-04T23:17:54.455602: step 2154, loss 0.539253.
Train: 2018-08-04T23:17:54.705539: step 2155, loss 0.650104.
Train: 2018-08-04T23:17:54.955454: step 2156, loss 0.618127.
Train: 2018-08-04T23:17:55.205395: step 2157, loss 0.571081.
Train: 2018-08-04T23:17:55.455337: step 2158, loss 0.602319.
Train: 2018-08-04T23:17:55.705309: step 2159, loss 0.586643.
Train: 2018-08-04T23:17:55.955246: step 2160, loss 0.509627.
Test: 2018-08-04T23:17:57.189306: step 2160, loss 0.551491.
Train: 2018-08-04T23:17:57.439278: step 2161, loss 0.555899.
Train: 2018-08-04T23:17:57.689219: step 2162, loss 0.601933.
Train: 2018-08-04T23:17:57.939156: step 2163, loss 0.525479.
Train: 2018-08-04T23:17:58.189102: step 2164, loss 0.563705.
Train: 2018-08-04T23:17:58.439044: step 2165, loss 0.632407.
Train: 2018-08-04T23:17:58.688986: step 2166, loss 0.563735.
Train: 2018-08-04T23:17:58.938927: step 2167, loss 0.510578.
Train: 2018-08-04T23:17:59.188868: step 2168, loss 0.541019.
Train: 2018-08-04T23:17:59.438810: step 2169, loss 0.662848.
Train: 2018-08-04T23:17:59.688752: step 2170, loss 0.609489.
Test: 2018-08-04T23:18:00.938428: step 2170, loss 0.55027.
Train: 2018-08-04T23:18:01.172779: step 2171, loss 0.548596.
Train: 2018-08-04T23:18:01.469585: step 2172, loss 0.563845.
Train: 2018-08-04T23:18:01.703905: step 2173, loss 0.52588.
Train: 2018-08-04T23:18:01.953815: step 2174, loss 0.563879.
Train: 2018-08-04T23:18:02.203787: step 2175, loss 0.609516.
Train: 2018-08-04T23:18:02.453708: step 2176, loss 0.579134.
Train: 2018-08-04T23:18:02.703641: step 2177, loss 0.518113.
Train: 2018-08-04T23:18:02.953581: step 2178, loss 0.525557.
Train: 2018-08-04T23:18:03.203554: step 2179, loss 0.571368.
Train: 2018-08-04T23:18:03.453494: step 2180, loss 0.532962.
Test: 2018-08-04T23:18:04.703171: step 2180, loss 0.54941.
Train: 2018-08-04T23:18:04.953113: step 2181, loss 0.571234.
Train: 2018-08-04T23:18:05.203086: step 2182, loss 0.509288.
Train: 2018-08-04T23:18:05.468618: step 2183, loss 0.53995.
Train: 2018-08-04T23:18:05.702937: step 2184, loss 0.508533.
Train: 2018-08-04T23:18:05.952879: step 2185, loss 0.570911.
Train: 2018-08-04T23:18:06.202850: step 2186, loss 0.594631.
Train: 2018-08-04T23:18:06.452798: step 2187, loss 0.539235.
Train: 2018-08-04T23:18:06.702733: step 2188, loss 0.586801.
Train: 2018-08-04T23:18:06.952681: step 2189, loss 0.643063.
Train: 2018-08-04T23:18:07.202586: step 2190, loss 0.562801.
Test: 2018-08-04T23:18:08.452293: step 2190, loss 0.548652.
Train: 2018-08-04T23:18:08.749123: step 2191, loss 0.506583.
Train: 2018-08-04T23:18:08.999080: step 2192, loss 0.562715.
Train: 2018-08-04T23:18:09.249016: step 2193, loss 0.562808.
Train: 2018-08-04T23:18:09.498959: step 2194, loss 0.570771.
Train: 2018-08-04T23:18:09.748899: step 2195, loss 0.554672.
Train: 2018-08-04T23:18:09.998807: step 2196, loss 0.546432.
Train: 2018-08-04T23:18:10.248748: step 2197, loss 0.570806.
Train: 2018-08-04T23:18:10.498714: step 2198, loss 0.627889.
Train: 2018-08-04T23:18:10.748665: step 2199, loss 0.570777.
Train: 2018-08-04T23:18:10.998573: step 2200, loss 0.554353.
Test: 2018-08-04T23:18:12.248279: step 2200, loss 0.549379.
Train: 2018-08-04T23:18:13.216833: step 2201, loss 0.529907.
Train: 2018-08-04T23:18:13.466775: step 2202, loss 0.538111.
Train: 2018-08-04T23:18:13.716717: step 2203, loss 0.497191.
Train: 2018-08-04T23:18:13.982294: step 2204, loss 0.570804.
Train: 2018-08-04T23:18:14.216595: step 2205, loss 0.54623.
Train: 2018-08-04T23:18:14.466510: step 2206, loss 0.603776.
Train: 2018-08-04T23:18:14.716483: step 2207, loss 0.529679.
Train: 2018-08-04T23:18:14.966424: step 2208, loss 0.603831.
Train: 2018-08-04T23:18:15.216365: step 2209, loss 0.55423.
Train: 2018-08-04T23:18:15.466302: step 2210, loss 0.570718.
Test: 2018-08-04T23:18:16.700362: step 2210, loss 0.549071.
Train: 2018-08-04T23:18:16.950330: step 2211, loss 0.537748.
Train: 2018-08-04T23:18:17.200275: step 2212, loss 0.587197.
Train: 2018-08-04T23:18:17.450217: step 2213, loss 0.603737.
Train: 2018-08-04T23:18:17.700154: step 2214, loss 0.612068.
Train: 2018-08-04T23:18:17.950101: step 2215, loss 0.595501.
Train: 2018-08-04T23:18:18.200036: step 2216, loss 0.578962.
Train: 2018-08-04T23:18:18.449983: step 2217, loss 0.603541.
Train: 2018-08-04T23:18:18.699924: step 2218, loss 0.587213.
Train: 2018-08-04T23:18:18.949865: step 2219, loss 0.570732.
Train: 2018-08-04T23:18:19.199803: step 2220, loss 0.562723.
Test: 2018-08-04T23:18:20.449484: step 2220, loss 0.549798.
Train: 2018-08-04T23:18:20.683836: step 2221, loss 0.506176.
Train: 2018-08-04T23:18:20.933771: step 2222, loss 0.595002.
Train: 2018-08-04T23:18:21.183712: step 2223, loss 0.562758.
Train: 2018-08-04T23:18:21.433659: step 2224, loss 0.538768.
Train: 2018-08-04T23:18:21.683610: step 2225, loss 0.578915.
Train: 2018-08-04T23:18:21.933542: step 2226, loss 0.570788.
Train: 2018-08-04T23:18:22.183454: step 2227, loss 0.530883.
Train: 2018-08-04T23:18:22.433395: step 2228, loss 0.530805.
Train: 2018-08-04T23:18:22.683336: step 2229, loss 0.514749.
Train: 2018-08-04T23:18:22.933289: step 2230, loss 0.602922.
Test: 2018-08-04T23:18:24.182985: step 2230, loss 0.549207.
Train: 2018-08-04T23:18:24.417336: step 2231, loss 0.570792.
Train: 2018-08-04T23:18:24.667270: step 2232, loss 0.594915.
Train: 2018-08-04T23:18:24.917218: step 2233, loss 0.5307.
Train: 2018-08-04T23:18:25.167160: step 2234, loss 0.570805.
Train: 2018-08-04T23:18:25.417107: step 2235, loss 0.635201.
Train: 2018-08-04T23:18:25.667042: step 2236, loss 0.578923.
Train: 2018-08-04T23:18:25.916984: step 2237, loss 0.538756.
Train: 2018-08-04T23:18:26.166928: step 2238, loss 0.466498.
Train: 2018-08-04T23:18:26.416862: step 2239, loss 0.530674.
Train: 2018-08-04T23:18:26.666809: step 2240, loss 0.546767.
Test: 2018-08-04T23:18:27.916487: step 2240, loss 0.54979.
Train: 2018-08-04T23:18:28.150806: step 2241, loss 0.481981.
Train: 2018-08-04T23:18:28.400782: step 2242, loss 0.546367.
Train: 2018-08-04T23:18:28.650697: step 2243, loss 0.546282.
Train: 2018-08-04T23:18:28.900681: step 2244, loss 0.603417.
Train: 2018-08-04T23:18:29.150603: step 2245, loss 0.587095.
Train: 2018-08-04T23:18:29.400543: step 2246, loss 0.471917.
Train: 2018-08-04T23:18:29.650485: step 2247, loss 0.504657.
Train: 2018-08-04T23:18:29.900427: step 2248, loss 0.479444.
Train: 2018-08-04T23:18:30.150369: step 2249, loss 0.562396.
Train: 2018-08-04T23:18:30.400309: step 2250, loss 0.612714.
Test: 2018-08-04T23:18:31.649987: step 2250, loss 0.548488.
Train: 2018-08-04T23:18:31.884340: step 2251, loss 0.579215.
Train: 2018-08-04T23:18:32.134248: step 2252, loss 0.477931.
Train: 2018-08-04T23:18:32.384189: step 2253, loss 0.460631.
Train: 2018-08-04T23:18:32.634161: step 2254, loss 0.53657.
Train: 2018-08-04T23:18:32.884098: step 2255, loss 0.596573.
Train: 2018-08-04T23:18:33.134044: step 2256, loss 0.579565.
Train: 2018-08-04T23:18:33.383986: step 2257, loss 0.562646.
Train: 2018-08-04T23:18:33.633927: step 2258, loss 0.527666.
Train: 2018-08-04T23:18:33.883869: step 2259, loss 0.562371.
Train: 2018-08-04T23:18:34.133810: step 2260, loss 0.562459.
Test: 2018-08-04T23:18:35.383487: step 2260, loss 0.547617.
Train: 2018-08-04T23:18:35.633431: step 2261, loss 0.553602.
Train: 2018-08-04T23:18:35.883400: step 2262, loss 0.536527.
Train: 2018-08-04T23:18:36.117691: step 2263, loss 0.483838.
Train: 2018-08-04T23:18:36.367632: step 2264, loss 0.553532.
Train: 2018-08-04T23:18:36.570739: step 2265, loss 0.543728.
Train: 2018-08-04T23:18:36.820681: step 2266, loss 0.55356.
Train: 2018-08-04T23:18:37.070622: step 2267, loss 0.518343.
Train: 2018-08-04T23:18:37.320535: step 2268, loss 0.59782.
Train: 2018-08-04T23:18:37.570506: step 2269, loss 0.527049.
Train: 2018-08-04T23:18:37.820417: step 2270, loss 0.615381.
Test: 2018-08-04T23:18:39.054503: step 2270, loss 0.547295.
Train: 2018-08-04T23:18:39.288823: step 2271, loss 0.527062.
Train: 2018-08-04T23:18:39.538794: step 2272, loss 0.509469.
Train: 2018-08-04T23:18:39.788736: step 2273, loss 0.509927.
Train: 2018-08-04T23:18:40.038679: step 2274, loss 0.606631.
Train: 2018-08-04T23:18:40.288619: step 2275, loss 0.579796.
Train: 2018-08-04T23:18:40.538559: step 2276, loss 0.606236.
Train: 2018-08-04T23:18:40.788502: step 2277, loss 0.597304.
Train: 2018-08-04T23:18:41.038445: step 2278, loss 0.571296.
Train: 2018-08-04T23:18:41.288355: step 2279, loss 0.51871.
Train: 2018-08-04T23:18:41.538327: step 2280, loss 0.544854.
Test: 2018-08-04T23:18:42.799515: step 2280, loss 0.549443.
Train: 2018-08-04T23:18:43.033836: step 2281, loss 0.49289.
Train: 2018-08-04T23:18:43.283778: step 2282, loss 0.466968.
Train: 2018-08-04T23:18:43.533720: step 2283, loss 0.519093.
Train: 2018-08-04T23:18:43.783687: step 2284, loss 0.50149.
Train: 2018-08-04T23:18:44.033633: step 2285, loss 0.5623.
Train: 2018-08-04T23:18:44.283552: step 2286, loss 0.536144.
Train: 2018-08-04T23:18:44.533518: step 2287, loss 0.544828.
Train: 2018-08-04T23:18:44.783426: step 2288, loss 0.518557.
Train: 2018-08-04T23:18:45.033398: step 2289, loss 0.588776.
Train: 2018-08-04T23:18:45.283340: step 2290, loss 0.544699.
Test: 2018-08-04T23:18:46.517395: step 2290, loss 0.547915.
Train: 2018-08-04T23:18:46.751741: step 2291, loss 0.606401.
Train: 2018-08-04T23:18:47.001657: step 2292, loss 0.553586.
Train: 2018-08-04T23:18:47.251599: step 2293, loss 0.553484.
Train: 2018-08-04T23:18:47.501571: step 2294, loss 0.52728.
Train: 2018-08-04T23:18:47.751507: step 2295, loss 0.536072.
Train: 2018-08-04T23:18:48.001454: step 2296, loss 0.562189.
Train: 2018-08-04T23:18:48.251365: step 2297, loss 0.614881.
Train: 2018-08-04T23:18:48.501337: step 2298, loss 0.579801.
Train: 2018-08-04T23:18:48.751278: step 2299, loss 0.536174.
Train: 2018-08-04T23:18:49.001188: step 2300, loss 0.562404.
Test: 2018-08-04T23:18:50.250896: step 2300, loss 0.547032.
Train: 2018-08-04T23:18:51.063236: step 2301, loss 0.580048.
Train: 2018-08-04T23:18:51.313179: step 2302, loss 0.60555.
Train: 2018-08-04T23:18:51.563090: step 2303, loss 0.484653.
Train: 2018-08-04T23:18:51.813061: step 2304, loss 0.562113.
Train: 2018-08-04T23:18:52.063001: step 2305, loss 0.570911.
Train: 2018-08-04T23:18:52.312943: step 2306, loss 0.545073.
Train: 2018-08-04T23:18:52.562884: step 2307, loss 0.605021.
Train: 2018-08-04T23:18:52.828441: step 2308, loss 0.630716.
Train: 2018-08-04T23:18:53.062737: step 2309, loss 0.511117.
Train: 2018-08-04T23:18:53.312709: step 2310, loss 0.638727.
Test: 2018-08-04T23:18:54.562385: step 2310, loss 0.548456.
Train: 2018-08-04T23:18:54.796737: step 2311, loss 0.638776.
Train: 2018-08-04T23:18:55.046672: step 2312, loss 0.612672.
Train: 2018-08-04T23:18:55.296621: step 2313, loss 0.562336.
Train: 2018-08-04T23:18:55.546561: step 2314, loss 0.55423.
Train: 2018-08-04T23:18:55.796502: step 2315, loss 0.562316.
Train: 2018-08-04T23:18:56.046447: step 2316, loss 0.513425.
Train: 2018-08-04T23:18:56.296390: step 2317, loss 0.546258.
Train: 2018-08-04T23:18:56.546327: step 2318, loss 0.489353.
Train: 2018-08-04T23:18:56.796269: step 2319, loss 0.554546.
Train: 2018-08-04T23:18:57.046205: step 2320, loss 0.497634.
Test: 2018-08-04T23:18:58.311509: step 2320, loss 0.548282.
Train: 2018-08-04T23:18:58.545859: step 2321, loss 0.562752.
Train: 2018-08-04T23:18:58.795795: step 2322, loss 0.51363.
Train: 2018-08-04T23:18:59.045712: step 2323, loss 0.562587.
Train: 2018-08-04T23:18:59.311275: step 2324, loss 0.505191.
Train: 2018-08-04T23:18:59.545626: step 2325, loss 0.603641.
Train: 2018-08-04T23:18:59.795566: step 2326, loss 0.513104.
Train: 2018-08-04T23:19:00.045509: step 2327, loss 0.513063.
Train: 2018-08-04T23:19:00.295444: step 2328, loss 0.612068.
Train: 2018-08-04T23:19:00.545361: step 2329, loss 0.587606.
Train: 2018-08-04T23:19:00.795333: step 2330, loss 0.5542.
Test: 2018-08-04T23:19:02.045009: step 2330, loss 0.549521.
Train: 2018-08-04T23:19:02.279360: step 2331, loss 0.587259.
Train: 2018-08-04T23:19:02.529301: step 2332, loss 0.595787.
Train: 2018-08-04T23:19:02.779242: step 2333, loss 0.579059.
Train: 2018-08-04T23:19:03.029180: step 2334, loss 0.603817.
Train: 2018-08-04T23:19:03.279126: step 2335, loss 0.52106.
Train: 2018-08-04T23:19:03.529036: step 2336, loss 0.661696.
Train: 2018-08-04T23:19:03.779002: step 2337, loss 0.529597.
Train: 2018-08-04T23:19:04.028919: step 2338, loss 0.603969.
Train: 2018-08-04T23:19:04.278892: step 2339, loss 0.56288.
Train: 2018-08-04T23:19:04.528833: step 2340, loss 0.538129.
Test: 2018-08-04T23:19:05.778509: step 2340, loss 0.548782.
Train: 2018-08-04T23:19:06.028452: step 2341, loss 0.562625.
Train: 2018-08-04T23:19:06.309637: step 2342, loss 0.546289.
Train: 2018-08-04T23:19:06.606441: step 2343, loss 0.578889.
Train: 2018-08-04T23:19:06.856382: step 2344, loss 0.465134.
Train: 2018-08-04T23:19:07.106348: step 2345, loss 0.489376.
Train: 2018-08-04T23:19:07.387535: step 2346, loss 0.546317.
Train: 2018-08-04T23:19:07.637480: step 2347, loss 0.562618.
Train: 2018-08-04T23:19:07.887422: step 2348, loss 0.562613.
Train: 2018-08-04T23:19:08.152954: step 2349, loss 0.587172.
Train: 2018-08-04T23:19:08.402926: step 2350, loss 0.620216.
Test: 2018-08-04T23:19:09.668224: step 2350, loss 0.548979.
Train: 2018-08-04T23:19:09.902568: step 2351, loss 0.463886.
Train: 2018-08-04T23:19:10.168107: step 2352, loss 0.595498.
Train: 2018-08-04T23:19:10.418049: step 2353, loss 0.579016.
Train: 2018-08-04T23:19:10.652397: step 2354, loss 0.570783.
Train: 2018-08-04T23:19:10.902311: step 2355, loss 0.521272.
Train: 2018-08-04T23:19:11.167874: step 2356, loss 0.554247.
Train: 2018-08-04T23:19:11.449058: step 2357, loss 0.570779.
Train: 2018-08-04T23:19:11.714622: step 2358, loss 0.562376.
Train: 2018-08-04T23:19:11.980183: step 2359, loss 0.520938.
Train: 2018-08-04T23:19:12.245746: step 2360, loss 0.595738.
Test: 2018-08-04T23:19:13.542317: step 2360, loss 0.549552.
Train: 2018-08-04T23:19:13.776637: step 2361, loss 0.562511.
Train: 2018-08-04T23:19:14.042201: step 2362, loss 0.529256.
Train: 2018-08-04T23:19:14.323385: step 2363, loss 0.537495.
Train: 2018-08-04T23:19:14.620197: step 2364, loss 0.545841.
Train: 2018-08-04T23:19:14.901375: step 2365, loss 0.579039.
Train: 2018-08-04T23:19:15.166964: step 2366, loss 0.604091.
Train: 2018-08-04T23:19:15.416878: step 2367, loss 0.579061.
Train: 2018-08-04T23:19:15.666847: step 2368, loss 0.645748.
Train: 2018-08-04T23:19:15.916760: step 2369, loss 0.686925.
Train: 2018-08-04T23:19:16.166733: step 2370, loss 0.58713.
Test: 2018-08-04T23:19:17.432031: step 2370, loss 0.54901.
Train: 2018-08-04T23:19:17.666382: step 2371, loss 0.562522.
Train: 2018-08-04T23:19:17.916322: step 2372, loss 0.529845.
Train: 2018-08-04T23:19:18.181881: step 2373, loss 0.53005.
Train: 2018-08-04T23:19:18.416212: step 2374, loss 0.619509.
Train: 2018-08-04T23:19:18.666144: step 2375, loss 0.611275.
Train: 2018-08-04T23:19:18.931705: step 2376, loss 0.595044.
Train: 2018-08-04T23:19:19.166000: step 2377, loss 0.554803.
Train: 2018-08-04T23:19:19.415942: step 2378, loss 0.554855.
Train: 2018-08-04T23:19:19.665883: step 2379, loss 0.49924.
Train: 2018-08-04T23:19:19.915854: step 2380, loss 0.515178.
Test: 2018-08-04T23:19:21.181154: step 2380, loss 0.549122.
Train: 2018-08-04T23:19:21.415498: step 2381, loss 0.555056.
Train: 2018-08-04T23:19:21.665414: step 2382, loss 0.578864.
Train: 2018-08-04T23:19:21.915386: step 2383, loss 0.539053.
Train: 2018-08-04T23:19:22.165324: step 2384, loss 0.530981.
Train: 2018-08-04T23:19:22.415269: step 2385, loss 0.642737.
Train: 2018-08-04T23:19:22.665206: step 2386, loss 0.586802.
Train: 2018-08-04T23:19:22.915124: step 2387, loss 0.626681.
Train: 2018-08-04T23:19:23.196309: step 2388, loss 0.54711.
Train: 2018-08-04T23:19:23.446280: step 2389, loss 0.602625.
Train: 2018-08-04T23:19:23.696215: step 2390, loss 0.57888.
Test: 2018-08-04T23:19:24.977138: step 2390, loss 0.549287.
Train: 2018-08-04T23:19:25.211490: step 2391, loss 0.594626.
Train: 2018-08-04T23:19:25.477023: step 2392, loss 0.523564.
Train: 2018-08-04T23:19:25.711367: step 2393, loss 0.492295.
Train: 2018-08-04T23:19:25.961283: step 2394, loss 0.55506.
Train: 2018-08-04T23:19:26.211224: step 2395, loss 0.62612.
Train: 2018-08-04T23:19:26.476788: step 2396, loss 0.507734.
Train: 2018-08-04T23:19:26.726759: step 2397, loss 0.531435.
Train: 2018-08-04T23:19:26.976701: step 2398, loss 0.515463.
Train: 2018-08-04T23:19:27.226611: step 2399, loss 0.587022.
Train: 2018-08-04T23:19:27.476554: step 2400, loss 0.507113.
Test: 2018-08-04T23:19:28.741883: step 2400, loss 0.549132.
Train: 2018-08-04T23:19:29.616680: step 2401, loss 0.594777.
Train: 2018-08-04T23:19:29.866621: step 2402, loss 0.570983.
Train: 2018-08-04T23:19:30.132182: step 2403, loss 0.619074.
Train: 2018-08-04T23:19:30.382149: step 2404, loss 0.570984.
Train: 2018-08-04T23:19:30.632064: step 2405, loss 0.619093.
Train: 2018-08-04T23:19:30.882031: step 2406, loss 0.554794.
Train: 2018-08-04T23:19:31.147569: step 2407, loss 0.594777.
Train: 2018-08-04T23:19:31.397510: step 2408, loss 0.562783.
Train: 2018-08-04T23:19:31.663100: step 2409, loss 0.586929.
Train: 2018-08-04T23:19:31.913016: step 2410, loss 0.554823.
Test: 2018-08-04T23:19:33.193971: step 2410, loss 0.549859.
Train: 2018-08-04T23:19:33.459529: step 2411, loss 0.58677.
Train: 2018-08-04T23:19:33.709470: step 2412, loss 0.546846.
Train: 2018-08-04T23:19:33.959410: step 2413, loss 0.579175.
Train: 2018-08-04T23:19:34.224998: step 2414, loss 0.586693.
Train: 2018-08-04T23:19:34.474940: step 2415, loss 0.554931.
Train: 2018-08-04T23:19:34.677993: step 2416, loss 0.477915.
Train: 2018-08-04T23:19:34.943580: step 2417, loss 0.59492.
Train: 2018-08-04T23:19:35.193526: step 2418, loss 0.642651.
Train: 2018-08-04T23:19:35.459090: step 2419, loss 0.563009.
Train: 2018-08-04T23:19:35.709027: step 2420, loss 0.523183.
Test: 2018-08-04T23:19:36.958743: step 2420, loss 0.548903.
Train: 2018-08-04T23:19:37.208675: step 2421, loss 0.642528.
Train: 2018-08-04T23:19:37.458590: step 2422, loss 0.570966.
Train: 2018-08-04T23:19:37.708564: step 2423, loss 0.539166.
Train: 2018-08-04T23:19:37.974121: step 2424, loss 0.523314.
Train: 2018-08-04T23:19:38.239688: step 2425, loss 0.618578.
Train: 2018-08-04T23:19:38.474009: step 2426, loss 0.571066.
Train: 2018-08-04T23:19:38.723919: step 2427, loss 0.523414.
Train: 2018-08-04T23:19:38.973897: step 2428, loss 0.642174.
Train: 2018-08-04T23:19:39.239458: step 2429, loss 0.594645.
Train: 2018-08-04T23:19:39.489397: step 2430, loss 0.547282.
Test: 2018-08-04T23:19:40.785936: step 2430, loss 0.548456.
Train: 2018-08-04T23:19:41.020294: step 2431, loss 0.586655.
Train: 2018-08-04T23:19:41.285821: step 2432, loss 0.634019.
Train: 2018-08-04T23:19:41.535761: step 2433, loss 0.523939.
Train: 2018-08-04T23:19:41.801325: step 2434, loss 0.539735.
Train: 2018-08-04T23:19:42.051266: step 2435, loss 0.555395.
Train: 2018-08-04T23:19:42.332450: step 2436, loss 0.555364.
Train: 2018-08-04T23:19:42.566770: step 2437, loss 0.602293.
Train: 2018-08-04T23:19:42.816742: step 2438, loss 0.578842.
Train: 2018-08-04T23:19:43.066687: step 2439, loss 0.555409.
Train: 2018-08-04T23:19:43.316595: step 2440, loss 0.570985.
Test: 2018-08-04T23:19:44.566301: step 2440, loss 0.549952.
Train: 2018-08-04T23:19:44.816273: step 2441, loss 0.4614.
Train: 2018-08-04T23:19:45.066209: step 2442, loss 0.618101.
Train: 2018-08-04T23:19:45.316156: step 2443, loss 0.547486.
Train: 2018-08-04T23:19:45.581689: step 2444, loss 0.468584.
Train: 2018-08-04T23:19:45.816009: step 2445, loss 0.562964.
Train: 2018-08-04T23:19:46.081602: step 2446, loss 0.586693.
Train: 2018-08-04T23:19:46.331543: step 2447, loss 0.523008.
Train: 2018-08-04T23:19:46.581485: step 2448, loss 0.546738.
Train: 2018-08-04T23:19:46.831427: step 2449, loss 0.635169.
Train: 2018-08-04T23:19:47.081339: step 2450, loss 0.594936.
Test: 2018-08-04T23:19:48.377909: step 2450, loss 0.548951.
Train: 2018-08-04T23:19:48.612262: step 2451, loss 0.490492.
Train: 2018-08-04T23:19:48.862201: step 2452, loss 0.570917.
Train: 2018-08-04T23:19:49.112143: step 2453, loss 0.522175.
Train: 2018-08-04T23:19:49.362084: step 2454, loss 0.603006.
Train: 2018-08-04T23:19:49.612027: step 2455, loss 0.554485.
Train: 2018-08-04T23:19:49.861967: step 2456, loss 0.521716.
Train: 2018-08-04T23:19:50.111911: step 2457, loss 0.513445.
Train: 2018-08-04T23:19:50.361850: step 2458, loss 0.562534.
Train: 2018-08-04T23:19:50.611794: step 2459, loss 0.537774.
Train: 2018-08-04T23:19:50.861735: step 2460, loss 0.521289.
Test: 2018-08-04T23:19:52.111410: step 2460, loss 0.549386.
Train: 2018-08-04T23:19:52.361350: step 2461, loss 0.463058.
Train: 2018-08-04T23:19:52.611322: step 2462, loss 0.587078.
Train: 2018-08-04T23:19:52.861260: step 2463, loss 0.587439.
Train: 2018-08-04T23:19:53.111205: step 2464, loss 0.545677.
Train: 2018-08-04T23:19:53.361148: step 2465, loss 0.570731.
Train: 2018-08-04T23:19:53.611089: step 2466, loss 0.630033.
Train: 2018-08-04T23:19:53.860999: step 2467, loss 0.578476.
Train: 2018-08-04T23:19:54.110974: step 2468, loss 0.562555.
Train: 2018-08-04T23:19:54.376531: step 2469, loss 0.604111.
Train: 2018-08-04T23:19:54.626477: step 2470, loss 0.587343.
Test: 2018-08-04T23:19:55.891775: step 2470, loss 0.548658.
Train: 2018-08-04T23:19:56.126126: step 2471, loss 0.553886.
Train: 2018-08-04T23:19:56.376068: step 2472, loss 0.553753.
Train: 2018-08-04T23:19:56.641626: step 2473, loss 0.553689.
Train: 2018-08-04T23:19:56.907162: step 2474, loss 0.588008.
Train: 2018-08-04T23:19:57.141513: step 2475, loss 0.570544.
Train: 2018-08-04T23:19:57.391454: step 2476, loss 0.520738.
Train: 2018-08-04T23:19:57.641395: step 2477, loss 0.587678.
Train: 2018-08-04T23:19:57.891337: step 2478, loss 0.595258.
Train: 2018-08-04T23:19:58.141278: step 2479, loss 0.570723.
Train: 2018-08-04T23:19:58.391221: step 2480, loss 0.586896.
Test: 2018-08-04T23:19:59.640896: step 2480, loss 0.549182.
Train: 2018-08-04T23:19:59.875248: step 2481, loss 0.603487.
Train: 2018-08-04T23:20:00.125183: step 2482, loss 0.586525.
Train: 2018-08-04T23:20:00.375100: step 2483, loss 0.545704.
Train: 2018-08-04T23:20:00.625041: step 2484, loss 0.562336.
Train: 2018-08-04T23:20:00.859391: step 2485, loss 0.512943.
Train: 2018-08-04T23:20:01.109302: step 2486, loss 0.611988.
Train: 2018-08-04T23:20:01.374892: step 2487, loss 0.562037.
Train: 2018-08-04T23:20:01.656079: step 2488, loss 0.619698.
Train: 2018-08-04T23:20:01.906016: step 2489, loss 0.51398.
Train: 2018-08-04T23:20:02.155965: step 2490, loss 0.546782.
Test: 2018-08-04T23:20:03.452504: step 2490, loss 0.548591.
Train: 2018-08-04T23:20:03.718067: step 2491, loss 0.529777.
Train: 2018-08-04T23:20:03.968008: step 2492, loss 0.538288.
Train: 2018-08-04T23:20:04.233597: step 2493, loss 0.578732.
Train: 2018-08-04T23:20:04.467916: step 2494, loss 0.553865.
Train: 2018-08-04T23:20:04.717857: step 2495, loss 0.55463.
Train: 2018-08-04T23:20:04.967799: step 2496, loss 0.562377.
Train: 2018-08-04T23:20:05.217725: step 2497, loss 0.603495.
Train: 2018-08-04T23:20:05.483278: step 2498, loss 0.530521.
Train: 2018-08-04T23:20:05.733221: step 2499, loss 0.523086.
Train: 2018-08-04T23:20:05.983161: step 2500, loss 0.57971.
Test: 2018-08-04T23:20:07.274527: step 2500, loss 0.5473.
Train: 2018-08-04T23:20:08.174166: step 2501, loss 0.496813.
Train: 2018-08-04T23:20:08.430356: step 2502, loss 0.602531.
Train: 2018-08-04T23:20:08.690878: step 2503, loss 0.627273.
Train: 2018-08-04T23:20:08.939897: step 2504, loss 0.545661.
Train: 2018-08-04T23:20:09.205461: step 2505, loss 0.554824.
Train: 2018-08-04T23:20:09.455438: step 2506, loss 0.530128.
Train: 2018-08-04T23:20:09.720967: step 2507, loss 0.587935.
Train: 2018-08-04T23:20:09.986528: step 2508, loss 0.588484.
Train: 2018-08-04T23:20:10.236500: step 2509, loss 0.570343.
Train: 2018-08-04T23:20:10.486441: step 2510, loss 0.464976.
Test: 2018-08-04T23:20:11.782983: step 2510, loss 0.550029.
Train: 2018-08-04T23:20:12.095434: step 2511, loss 0.569765.
Train: 2018-08-04T23:20:12.345352: step 2512, loss 0.588414.
Train: 2018-08-04T23:20:12.595322: step 2513, loss 0.617747.
Train: 2018-08-04T23:20:12.845263: step 2514, loss 0.58014.
Train: 2018-08-04T23:20:13.095205: step 2515, loss 0.604422.
Train: 2018-08-04T23:20:13.360768: step 2516, loss 0.554015.
Train: 2018-08-04T23:20:13.610710: step 2517, loss 0.505202.
Train: 2018-08-04T23:20:13.860645: step 2518, loss 0.499015.
Train: 2018-08-04T23:20:14.110595: step 2519, loss 0.561863.
Train: 2018-08-04T23:20:14.360503: step 2520, loss 0.521699.
Test: 2018-08-04T23:20:15.625831: step 2520, loss 0.548745.
Train: 2018-08-04T23:20:15.860183: step 2521, loss 0.58665.
Train: 2018-08-04T23:20:16.110124: step 2522, loss 0.637164.
Train: 2018-08-04T23:20:16.375657: step 2523, loss 0.544726.
Train: 2018-08-04T23:20:16.625622: step 2524, loss 0.579533.
Train: 2018-08-04T23:20:16.875566: step 2525, loss 0.538964.
Train: 2018-08-04T23:20:17.141102: step 2526, loss 0.594693.
Train: 2018-08-04T23:20:17.406689: step 2527, loss 0.587418.
Train: 2018-08-04T23:20:17.656606: step 2528, loss 0.571345.
Train: 2018-08-04T23:20:17.906548: step 2529, loss 0.571198.
Train: 2018-08-04T23:20:18.156522: step 2530, loss 0.627373.
Test: 2018-08-04T23:20:19.499924: step 2530, loss 0.547683.
Train: 2018-08-04T23:20:19.734276: step 2531, loss 0.522057.
Train: 2018-08-04T23:20:19.984216: step 2532, loss 0.554757.
Train: 2018-08-04T23:20:20.249782: step 2533, loss 0.636252.
Train: 2018-08-04T23:20:20.484095: step 2534, loss 0.546064.
Train: 2018-08-04T23:20:20.734012: step 2535, loss 0.538557.
Train: 2018-08-04T23:20:20.999574: step 2536, loss 0.490069.
Train: 2018-08-04T23:20:21.249517: step 2537, loss 0.578285.
Train: 2018-08-04T23:20:21.515078: step 2538, loss 0.643114.
Train: 2018-08-04T23:20:21.780642: step 2539, loss 0.602432.
Train: 2018-08-04T23:20:22.014987: step 2540, loss 0.562885.
Test: 2018-08-04T23:20:23.280289: step 2540, loss 0.548079.
Train: 2018-08-04T23:20:23.530264: step 2541, loss 0.59396.
Train: 2018-08-04T23:20:23.780205: step 2542, loss 0.523464.
Train: 2018-08-04T23:20:24.030114: step 2543, loss 0.514797.
Train: 2018-08-04T23:20:24.295676: step 2544, loss 0.531497.
Train: 2018-08-04T23:20:24.578771: step 2545, loss 0.530936.
Train: 2018-08-04T23:20:24.828688: step 2546, loss 0.538962.
Train: 2018-08-04T23:20:25.063009: step 2547, loss 0.594493.
Train: 2018-08-04T23:20:25.312949: step 2548, loss 0.53948.
Train: 2018-08-04T23:20:25.562891: step 2549, loss 0.570483.
Train: 2018-08-04T23:20:25.812863: step 2550, loss 0.586595.
Test: 2018-08-04T23:20:27.062539: step 2550, loss 0.549378.
Train: 2018-08-04T23:20:27.296890: step 2551, loss 0.562714.
Train: 2018-08-04T23:20:27.546832: step 2552, loss 0.578741.
Train: 2018-08-04T23:20:27.796773: step 2553, loss 0.554405.
Train: 2018-08-04T23:20:28.046710: step 2554, loss 0.497821.
Train: 2018-08-04T23:20:28.296656: step 2555, loss 0.512914.
Train: 2018-08-04T23:20:28.546597: step 2556, loss 0.563027.
Train: 2018-08-04T23:20:28.796539: step 2557, loss 0.571632.
Train: 2018-08-04T23:20:29.046480: step 2558, loss 0.496828.
Train: 2018-08-04T23:20:29.296423: step 2559, loss 0.570076.
Train: 2018-08-04T23:20:29.546364: step 2560, loss 0.520681.
Test: 2018-08-04T23:20:30.811661: step 2560, loss 0.548342.
Train: 2018-08-04T23:20:31.046013: step 2561, loss 0.537286.
Train: 2018-08-04T23:20:31.295954: step 2562, loss 0.503259.
Train: 2018-08-04T23:20:31.545890: step 2563, loss 0.5363.
Train: 2018-08-04T23:20:31.795806: step 2564, loss 0.536044.
Train: 2018-08-04T23:20:32.061370: step 2565, loss 0.563797.
Train: 2018-08-04T23:20:32.311311: step 2566, loss 0.562589.
Train: 2018-08-04T23:20:32.514388: step 2567, loss 0.581922.
Train: 2018-08-04T23:20:32.764363: step 2568, loss 0.630519.
Train: 2018-08-04T23:20:33.014304: step 2569, loss 0.622697.
Train: 2018-08-04T23:20:33.264243: step 2570, loss 0.612932.
Test: 2018-08-04T23:20:34.513919: step 2570, loss 0.547845.
Train: 2018-08-04T23:20:34.748265: step 2571, loss 0.530289.
Train: 2018-08-04T23:20:35.013834: step 2572, loss 0.569943.
Train: 2018-08-04T23:20:35.263769: step 2573, loss 0.596294.
Train: 2018-08-04T23:20:35.513687: step 2574, loss 0.571407.
Train: 2018-08-04T23:20:35.763657: step 2575, loss 0.528992.
Train: 2018-08-04T23:20:36.013599: step 2576, loss 0.58746.
Train: 2018-08-04T23:20:36.263509: step 2577, loss 0.646554.
Train: 2018-08-04T23:20:36.513452: step 2578, loss 0.504128.
Train: 2018-08-04T23:20:36.763393: step 2579, loss 0.520709.
Train: 2018-08-04T23:20:37.013334: step 2580, loss 0.529526.
Test: 2018-08-04T23:20:38.263042: step 2580, loss 0.54944.
Train: 2018-08-04T23:20:38.497398: step 2581, loss 0.555534.
Train: 2018-08-04T23:20:38.747338: step 2582, loss 0.528371.
Train: 2018-08-04T23:20:38.997280: step 2583, loss 0.644905.
Train: 2018-08-04T23:20:39.247217: step 2584, loss 0.587113.
Train: 2018-08-04T23:20:39.497168: step 2585, loss 0.530413.
Train: 2018-08-04T23:20:39.747104: step 2586, loss 0.530582.
Train: 2018-08-04T23:20:39.997045: step 2587, loss 0.595689.
Train: 2018-08-04T23:20:40.246982: step 2588, loss 0.530824.
Train: 2018-08-04T23:20:40.481272: step 2589, loss 0.464085.
Train: 2018-08-04T23:20:40.731247: step 2590, loss 0.497359.
Test: 2018-08-04T23:20:41.980921: step 2590, loss 0.548478.
Train: 2018-08-04T23:20:42.230887: step 2591, loss 0.537906.
Train: 2018-08-04T23:20:42.480832: step 2592, loss 0.554369.
Train: 2018-08-04T23:20:42.730776: step 2593, loss 0.587651.
Train: 2018-08-04T23:20:42.980717: step 2594, loss 0.587007.
Train: 2018-08-04T23:20:43.230628: step 2595, loss 0.545866.
Train: 2018-08-04T23:20:43.496192: step 2596, loss 0.59562.
Train: 2018-08-04T23:20:43.730540: step 2597, loss 0.546253.
Train: 2018-08-04T23:20:43.980483: step 2598, loss 0.595602.
Train: 2018-08-04T23:20:44.230427: step 2599, loss 0.545393.
Train: 2018-08-04T23:20:44.480340: step 2600, loss 0.537701.
Test: 2018-08-04T23:20:45.730043: step 2600, loss 0.549142.
Train: 2018-08-04T23:20:46.558005: step 2601, loss 0.546404.
Train: 2018-08-04T23:20:46.807946: step 2602, loss 0.54585.
Train: 2018-08-04T23:20:47.057888: step 2603, loss 0.603678.
Train: 2018-08-04T23:20:47.307829: step 2604, loss 0.612441.
Train: 2018-08-04T23:20:47.557771: step 2605, loss 0.645903.
Train: 2018-08-04T23:20:47.807718: step 2606, loss 0.504942.
Train: 2018-08-04T23:20:48.057654: step 2607, loss 0.612219.
Train: 2018-08-04T23:20:48.307595: step 2608, loss 0.571056.
Train: 2018-08-04T23:20:48.557536: step 2609, loss 0.52138.
Train: 2018-08-04T23:20:48.807478: step 2610, loss 0.521453.
Test: 2018-08-04T23:20:50.041535: step 2610, loss 0.550036.
Train: 2018-08-04T23:20:50.291475: step 2611, loss 0.578838.
Train: 2018-08-04T23:20:50.541447: step 2612, loss 0.546133.
Train: 2018-08-04T23:20:50.775737: step 2613, loss 0.488779.
Train: 2018-08-04T23:20:51.025708: step 2614, loss 0.570563.
Train: 2018-08-04T23:20:51.275620: step 2615, loss 0.529707.
Train: 2018-08-04T23:20:51.525591: step 2616, loss 0.512977.
Train: 2018-08-04T23:20:51.775533: step 2617, loss 0.529396.
Train: 2018-08-04T23:20:52.025475: step 2618, loss 0.570956.
Train: 2018-08-04T23:20:52.275418: step 2619, loss 0.645192.
Train: 2018-08-04T23:20:52.525358: step 2620, loss 0.562462.
Test: 2018-08-04T23:20:53.775034: step 2620, loss 0.549742.
Train: 2018-08-04T23:20:54.009354: step 2621, loss 0.496191.
Train: 2018-08-04T23:20:54.259320: step 2622, loss 0.545932.
Train: 2018-08-04T23:20:54.509269: step 2623, loss 0.512484.
Train: 2018-08-04T23:20:54.759179: step 2624, loss 0.587297.
Train: 2018-08-04T23:20:55.009120: step 2625, loss 0.554228.
Train: 2018-08-04T23:20:55.259092: step 2626, loss 0.595422.
Train: 2018-08-04T23:20:55.509030: step 2627, loss 0.604142.
Train: 2018-08-04T23:20:55.758945: step 2628, loss 0.620349.
Train: 2018-08-04T23:20:56.024538: step 2629, loss 0.595871.
Train: 2018-08-04T23:20:56.274479: step 2630, loss 0.537271.
Test: 2018-08-04T23:20:57.555399: step 2630, loss 0.548356.
Train: 2018-08-04T23:20:57.789744: step 2631, loss 0.570694.
Train: 2018-08-04T23:20:58.039686: step 2632, loss 0.487924.
Train: 2018-08-04T23:20:58.289628: step 2633, loss 0.553993.
Train: 2018-08-04T23:20:58.539568: step 2634, loss 0.538121.
Train: 2018-08-04T23:20:58.789515: step 2635, loss 0.594981.
Train: 2018-08-04T23:20:59.055048: step 2636, loss 0.603926.
Train: 2018-08-04T23:20:59.289370: step 2637, loss 0.587278.
Train: 2018-08-04T23:20:59.539309: step 2638, loss 0.587669.
Train: 2018-08-04T23:20:59.789287: step 2639, loss 0.562292.
Train: 2018-08-04T23:21:00.053082: step 2640, loss 0.521276.
Test: 2018-08-04T23:21:01.338841: step 2640, loss 0.548576.
Train: 2018-08-04T23:21:01.580386: step 2641, loss 0.603672.
Train: 2018-08-04T23:21:01.814714: step 2642, loss 0.504368.
Train: 2018-08-04T23:21:02.064652: step 2643, loss 0.530257.
Train: 2018-08-04T23:21:02.314593: step 2644, loss 0.620006.
Train: 2018-08-04T23:21:02.564530: step 2645, loss 0.603554.
Train: 2018-08-04T23:21:02.814472: step 2646, loss 0.61973.
Train: 2018-08-04T23:21:03.064417: step 2647, loss 0.513182.
Train: 2018-08-04T23:21:03.314360: step 2648, loss 0.553821.
Train: 2018-08-04T23:21:03.564301: step 2649, loss 0.610926.
Train: 2018-08-04T23:21:03.814211: step 2650, loss 0.55456.
Test: 2018-08-04T23:21:05.063919: step 2650, loss 0.548629.
Train: 2018-08-04T23:21:05.298264: step 2651, loss 0.56202.
Train: 2018-08-04T23:21:05.548206: step 2652, loss 0.610855.
Train: 2018-08-04T23:21:05.798147: step 2653, loss 0.5309.
Train: 2018-08-04T23:21:06.048094: step 2654, loss 0.603516.
Train: 2018-08-04T23:21:06.298033: step 2655, loss 0.562512.
Train: 2018-08-04T23:21:06.547972: step 2656, loss 0.530006.
Train: 2018-08-04T23:21:06.797887: step 2657, loss 0.499286.
Train: 2018-08-04T23:21:07.047862: step 2658, loss 0.563688.
Train: 2018-08-04T23:21:07.297802: step 2659, loss 0.547251.
Train: 2018-08-04T23:21:07.547713: step 2660, loss 0.513994.
Test: 2018-08-04T23:21:08.797421: step 2660, loss 0.54849.
Train: 2018-08-04T23:21:09.047362: step 2661, loss 0.611577.
Train: 2018-08-04T23:21:09.297304: step 2662, loss 0.619944.
Train: 2018-08-04T23:21:09.547275: step 2663, loss 0.611447.
Train: 2018-08-04T23:21:09.797216: step 2664, loss 0.586372.
Train: 2018-08-04T23:21:10.047128: step 2665, loss 0.554681.
Train: 2018-08-04T23:21:10.297099: step 2666, loss 0.634954.
Train: 2018-08-04T23:21:10.547043: step 2667, loss 0.531454.
Train: 2018-08-04T23:21:10.796951: step 2668, loss 0.58004.
Train: 2018-08-04T23:21:11.046893: step 2669, loss 0.586978.
Train: 2018-08-04T23:21:11.296836: step 2670, loss 0.546881.
Test: 2018-08-04T23:21:12.546542: step 2670, loss 0.549235.
Train: 2018-08-04T23:21:12.780893: step 2671, loss 0.563037.
Train: 2018-08-04T23:21:13.030834: step 2672, loss 0.595459.
Train: 2018-08-04T23:21:13.280771: step 2673, loss 0.523649.
Train: 2018-08-04T23:21:13.530717: step 2674, loss 0.539721.
Train: 2018-08-04T23:21:13.780658: step 2675, loss 0.516011.
Train: 2018-08-04T23:21:14.030600: step 2676, loss 0.595052.
Train: 2018-08-04T23:21:14.280542: step 2677, loss 0.602241.
Train: 2018-08-04T23:21:14.530478: step 2678, loss 0.555096.
Train: 2018-08-04T23:21:14.780424: step 2679, loss 0.579076.
Train: 2018-08-04T23:21:15.030335: step 2680, loss 0.586946.
Test: 2018-08-04T23:21:16.264422: step 2680, loss 0.549891.
Train: 2018-08-04T23:21:16.514363: step 2681, loss 0.602733.
Train: 2018-08-04T23:21:16.764335: step 2682, loss 0.523881.
Train: 2018-08-04T23:21:17.014277: step 2683, loss 0.515455.
Train: 2018-08-04T23:21:17.264187: step 2684, loss 0.554882.
Train: 2018-08-04T23:21:17.514164: step 2685, loss 0.49167.
Train: 2018-08-04T23:21:17.764101: step 2686, loss 0.610621.
Train: 2018-08-04T23:21:18.014042: step 2687, loss 0.626602.
Train: 2018-08-04T23:21:18.263984: step 2688, loss 0.57105.
Train: 2018-08-04T23:21:18.513925: step 2689, loss 0.546994.
Train: 2018-08-04T23:21:18.763866: step 2690, loss 0.451437.
Test: 2018-08-04T23:21:20.013545: step 2690, loss 0.548206.
Train: 2018-08-04T23:21:20.247863: step 2691, loss 0.602905.
Train: 2018-08-04T23:21:20.497835: step 2692, loss 0.570885.
Train: 2018-08-04T23:21:20.747777: step 2693, loss 0.602918.
Train: 2018-08-04T23:21:20.997714: step 2694, loss 0.570704.
Train: 2018-08-04T23:21:21.247630: step 2695, loss 0.587069.
Train: 2018-08-04T23:21:21.497604: step 2696, loss 0.522564.
Train: 2018-08-04T23:21:21.747512: step 2697, loss 0.506026.
Train: 2018-08-04T23:21:21.997453: step 2698, loss 0.514348.
Train: 2018-08-04T23:21:22.247425: step 2699, loss 0.529679.
Train: 2018-08-04T23:21:22.497337: step 2700, loss 0.5621.
Test: 2018-08-04T23:21:23.731423: step 2700, loss 0.548283.
Train: 2018-08-04T23:21:24.590628: step 2701, loss 0.628591.
Train: 2018-08-04T23:21:24.840568: step 2702, loss 0.513086.
Train: 2018-08-04T23:21:25.090509: step 2703, loss 0.588252.
Train: 2018-08-04T23:21:25.340451: step 2704, loss 0.520011.
Train: 2018-08-04T23:21:25.590388: step 2705, loss 0.587834.
Train: 2018-08-04T23:21:25.855951: step 2706, loss 0.610916.
Train: 2018-08-04T23:21:26.105867: step 2707, loss 0.528334.
Train: 2018-08-04T23:21:26.340188: step 2708, loss 0.494973.
Train: 2018-08-04T23:21:26.590153: step 2709, loss 0.535328.
Train: 2018-08-04T23:21:26.840095: step 2710, loss 0.464847.
Test: 2018-08-04T23:21:28.136641: step 2710, loss 0.547885.
Train: 2018-08-04T23:21:28.370992: step 2711, loss 0.502765.
Train: 2018-08-04T23:21:28.620933: step 2712, loss 0.600872.
Train: 2018-08-04T23:21:28.870874: step 2713, loss 0.604375.
Train: 2018-08-04T23:21:29.152030: step 2714, loss 0.536596.
Train: 2018-08-04T23:21:29.386350: step 2715, loss 0.51147.
Train: 2018-08-04T23:21:29.636290: step 2716, loss 0.605868.
Train: 2018-08-04T23:21:29.886263: step 2717, loss 0.604917.
Train: 2018-08-04T23:21:30.073689: step 2718, loss 0.435871.
Train: 2018-08-04T23:21:30.323629: step 2719, loss 0.557722.
Train: 2018-08-04T23:21:30.573601: step 2720, loss 0.585493.
Test: 2018-08-04T23:21:31.823278: step 2720, loss 0.547551.
Train: 2018-08-04T23:21:32.057624: step 2721, loss 0.585257.
Train: 2018-08-04T23:21:32.307570: step 2722, loss 0.518047.
Train: 2018-08-04T23:21:32.573121: step 2723, loss 0.551748.
Train: 2018-08-04T23:21:32.823074: step 2724, loss 0.559233.
Train: 2018-08-04T23:21:33.088608: step 2725, loss 0.627398.
Train: 2018-08-04T23:21:33.338548: step 2726, loss 0.596666.
Train: 2018-08-04T23:21:33.588491: step 2727, loss 0.579227.
Train: 2018-08-04T23:21:33.838462: step 2728, loss 0.57255.
Train: 2018-08-04T23:21:34.088374: step 2729, loss 0.562944.
Train: 2018-08-04T23:21:34.338344: step 2730, loss 0.578799.
Test: 2018-08-04T23:21:35.634885: step 2730, loss 0.549423.
Train: 2018-08-04T23:21:35.869236: step 2731, loss 0.59562.
Train: 2018-08-04T23:21:36.134800: step 2732, loss 0.530556.
Train: 2018-08-04T23:21:36.400331: step 2733, loss 0.610989.
Train: 2018-08-04T23:21:36.681541: step 2734, loss 0.618559.
Train: 2018-08-04T23:21:36.931458: step 2735, loss 0.57886.
Train: 2018-08-04T23:21:37.181424: step 2736, loss 0.665037.
Train: 2018-08-04T23:21:37.431366: step 2737, loss 0.532337.
Train: 2018-08-04T23:21:37.681280: step 2738, loss 0.60973.
Train: 2018-08-04T23:21:37.931249: step 2739, loss 0.555866.
Train: 2018-08-04T23:21:38.196786: step 2740, loss 0.548272.
Test: 2018-08-04T23:21:39.446493: step 2740, loss 0.550809.
Train: 2018-08-04T23:21:39.680838: step 2741, loss 0.525545.
Train: 2018-08-04T23:21:39.930784: step 2742, loss 0.47225.
Train: 2018-08-04T23:21:40.180696: step 2743, loss 0.55601.
Train: 2018-08-04T23:21:40.430662: step 2744, loss 0.525454.
Train: 2018-08-04T23:21:40.696200: step 2745, loss 0.548099.
Train: 2018-08-04T23:21:40.961788: step 2746, loss 0.609835.
Train: 2018-08-04T23:21:41.227351: step 2747, loss 0.571271.
Train: 2018-08-04T23:21:41.477269: step 2748, loss 0.571383.
Train: 2018-08-04T23:21:41.711619: step 2749, loss 0.594508.
Train: 2018-08-04T23:21:41.961531: step 2750, loss 0.602317.
Test: 2018-08-04T23:21:43.238374: step 2750, loss 0.549481.
Train: 2018-08-04T23:21:43.472719: step 2751, loss 0.547805.
Train: 2018-08-04T23:21:43.738278: step 2752, loss 0.649001.
Train: 2018-08-04T23:21:43.988218: step 2753, loss 0.563191.
Train: 2018-08-04T23:21:44.238165: step 2754, loss 0.555659.
Train: 2018-08-04T23:21:44.488101: step 2755, loss 0.555663.
Train: 2018-08-04T23:21:44.753664: step 2756, loss 0.51677.
Train: 2018-08-04T23:21:45.003607: step 2757, loss 0.570966.
Train: 2018-08-04T23:21:45.253522: step 2758, loss 0.586695.
Train: 2018-08-04T23:21:45.503494: step 2759, loss 0.508821.
Train: 2018-08-04T23:21:45.753435: step 2760, loss 0.484888.
Test: 2018-08-04T23:21:47.018733: step 2760, loss 0.550029.
Train: 2018-08-04T23:21:47.253053: step 2761, loss 0.578694.
Train: 2018-08-04T23:21:47.502995: step 2762, loss 0.515782.
Train: 2018-08-04T23:21:47.752967: step 2763, loss 0.642318.
Train: 2018-08-04T23:21:48.002908: step 2764, loss 0.523315.
Train: 2018-08-04T23:21:48.252851: step 2765, loss 0.562788.
Train: 2018-08-04T23:21:48.502792: step 2766, loss 0.514821.
Train: 2018-08-04T23:21:48.768358: step 2767, loss 0.554949.
Train: 2018-08-04T23:21:49.018295: step 2768, loss 0.611015.
Train: 2018-08-04T23:21:49.268237: step 2769, loss 0.530653.
Train: 2018-08-04T23:21:49.518173: step 2770, loss 0.562755.
Test: 2018-08-04T23:21:50.799098: step 2770, loss 0.548705.
Train: 2018-08-04T23:21:51.033449: step 2771, loss 0.513827.
Train: 2018-08-04T23:21:51.283389: step 2772, loss 0.611627.
Train: 2018-08-04T23:21:51.548947: step 2773, loss 0.521913.
Train: 2018-08-04T23:21:51.798910: step 2774, loss 0.578936.
Train: 2018-08-04T23:21:52.048806: step 2775, loss 0.58735.
Train: 2018-08-04T23:21:52.298781: step 2776, loss 0.521437.
Train: 2018-08-04T23:21:52.548690: step 2777, loss 0.521254.
Train: 2018-08-04T23:21:52.798663: step 2778, loss 0.587358.
Train: 2018-08-04T23:21:53.048605: step 2779, loss 0.504661.
Train: 2018-08-04T23:21:53.298547: step 2780, loss 0.487806.
Test: 2018-08-04T23:21:54.645249: step 2780, loss 0.548139.
Train: 2018-08-04T23:21:54.895303: step 2781, loss 0.554132.
Train: 2018-08-04T23:21:55.154566: step 2782, loss 0.570931.
Train: 2018-08-04T23:21:55.385407: step 2783, loss 0.554003.
Train: 2018-08-04T23:21:55.666560: step 2784, loss 0.570846.
Train: 2018-08-04T23:21:55.900880: step 2785, loss 0.54559.
Train: 2018-08-04T23:21:56.166474: step 2786, loss 0.570907.
Train: 2018-08-04T23:21:56.416415: step 2787, loss 0.562459.
Train: 2018-08-04T23:21:56.666351: step 2788, loss 0.613057.
Train: 2018-08-04T23:21:56.916268: step 2789, loss 0.494727.
Train: 2018-08-04T23:21:57.166240: step 2790, loss 0.519982.
Test: 2018-08-04T23:21:58.447159: step 2790, loss 0.547981.
Train: 2018-08-04T23:21:58.681504: step 2791, loss 0.545408.
Train: 2018-08-04T23:21:58.931446: step 2792, loss 0.545251.
Train: 2018-08-04T23:21:59.181363: step 2793, loss 0.528243.
Train: 2018-08-04T23:21:59.431335: step 2794, loss 0.536696.
Train: 2018-08-04T23:21:59.681245: step 2795, loss 0.476795.
Train: 2018-08-04T23:21:59.931188: step 2796, loss 0.49366.
Train: 2018-08-04T23:22:00.181159: step 2797, loss 0.545045.
Train: 2018-08-04T23:22:00.431106: step 2798, loss 0.579616.
Train: 2018-08-04T23:22:00.696632: step 2799, loss 0.562375.
Train: 2018-08-04T23:22:00.946575: step 2800, loss 0.544908.
Test: 2018-08-04T23:22:02.211902: step 2800, loss 0.545797.
Train: 2018-08-04T23:22:03.289806: step 2801, loss 0.605952.
Train: 2018-08-04T23:22:03.586606: step 2802, loss 0.501427.
Train: 2018-08-04T23:22:03.852145: step 2803, loss 0.606076.
Train: 2018-08-04T23:22:04.102122: step 2804, loss 0.588592.
Train: 2018-08-04T23:22:04.352052: step 2805, loss 0.53622.
Train: 2018-08-04T23:22:04.602000: step 2806, loss 0.640923.
Train: 2018-08-04T23:22:04.851935: step 2807, loss 0.59718.
Train: 2018-08-04T23:22:05.101876: step 2808, loss 0.597105.
Train: 2018-08-04T23:22:05.389321: step 2809, loss 0.55369.
Train: 2018-08-04T23:22:05.649625: step 2810, loss 0.50201.
Test: 2018-08-04T23:22:07.141801: step 2810, loss 0.547985.
Train: 2018-08-04T23:22:07.440033: step 2811, loss 0.536526.
Train: 2018-08-04T23:22:07.705323: step 2812, loss 0.613792.
Train: 2018-08-04T23:22:07.979561: step 2813, loss 0.545304.
Train: 2018-08-04T23:22:08.228895: step 2814, loss 0.545226.
Train: 2018-08-04T23:22:08.482219: step 2815, loss 0.613408.
Train: 2018-08-04T23:22:08.721579: step 2816, loss 0.553851.
Train: 2018-08-04T23:22:08.958943: step 2817, loss 0.604692.
Train: 2018-08-04T23:22:09.230219: step 2818, loss 0.46961.
Train: 2018-08-04T23:22:09.499498: step 2819, loss 0.596011.
Train: 2018-08-04T23:22:09.767808: step 2820, loss 0.579197.
Test: 2018-08-04T23:22:11.129142: step 2820, loss 0.548804.
Train: 2018-08-04T23:22:11.399419: step 2821, loss 0.554047.
Train: 2018-08-04T23:22:11.673322: step 2822, loss 0.537407.
Train: 2018-08-04T23:22:11.929637: step 2823, loss 0.604152.
Train: 2018-08-04T23:22:12.193931: step 2824, loss 0.579073.
Train: 2018-08-04T23:22:12.457226: step 2825, loss 0.487774.
Train: 2018-08-04T23:22:12.705565: step 2826, loss 0.612227.
Train: 2018-08-04T23:22:12.948947: step 2827, loss 0.545919.
Train: 2018-08-04T23:22:13.192288: step 2828, loss 0.529428.
Train: 2018-08-04T23:22:13.433642: step 2829, loss 0.628507.
Train: 2018-08-04T23:22:13.675995: step 2830, loss 0.529558.
Test: 2018-08-04T23:22:14.924630: step 2830, loss 0.548032.
Train: 2018-08-04T23:22:15.247767: step 2831, loss 0.57074.
Train: 2018-08-04T23:22:15.493111: step 2832, loss 0.52146.
Train: 2018-08-04T23:22:15.739484: step 2833, loss 0.521435.
Train: 2018-08-04T23:22:15.982828: step 2834, loss 0.578942.
Train: 2018-08-04T23:22:16.234174: step 2835, loss 0.562545.
Train: 2018-08-04T23:22:16.478503: step 2836, loss 0.587242.
Train: 2018-08-04T23:22:16.739779: step 2837, loss 0.595463.
Train: 2018-08-04T23:22:16.991492: step 2838, loss 0.529691.
Train: 2018-08-04T23:22:17.241463: step 2839, loss 0.578941.
Train: 2018-08-04T23:22:17.491407: step 2840, loss 0.587192.
Test: 2018-08-04T23:22:18.772324: step 2840, loss 0.54834.
Train: 2018-08-04T23:22:19.006669: step 2841, loss 0.587124.
Train: 2018-08-04T23:22:19.256619: step 2842, loss 0.546231.
Train: 2018-08-04T23:22:19.506561: step 2843, loss 0.513633.
Train: 2018-08-04T23:22:19.756503: step 2844, loss 0.546256.
Train: 2018-08-04T23:22:20.022063: step 2845, loss 0.619776.
Train: 2018-08-04T23:22:20.271973: step 2846, loss 0.570779.
Train: 2018-08-04T23:22:20.521940: step 2847, loss 0.521907.
Train: 2018-08-04T23:22:20.771890: step 2848, loss 0.546318.
Train: 2018-08-04T23:22:21.021829: step 2849, loss 0.562608.
Train: 2018-08-04T23:22:21.271740: step 2850, loss 0.603399.
Test: 2018-08-04T23:22:22.521446: step 2850, loss 0.548076.
Train: 2018-08-04T23:22:22.755768: step 2851, loss 0.595285.
Train: 2018-08-04T23:22:23.005737: step 2852, loss 0.538273.
Train: 2018-08-04T23:22:23.255674: step 2853, loss 0.530121.
Train: 2018-08-04T23:22:23.521214: step 2854, loss 0.619637.
Train: 2018-08-04T23:22:23.771184: step 2855, loss 0.578954.
Train: 2018-08-04T23:22:24.021095: step 2856, loss 0.554611.
Train: 2018-08-04T23:22:24.271067: step 2857, loss 0.562701.
Train: 2018-08-04T23:22:24.520980: step 2858, loss 0.505992.
Train: 2018-08-04T23:22:24.770951: step 2859, loss 0.595096.
Train: 2018-08-04T23:22:25.020892: step 2860, loss 0.481713.
Test: 2018-08-04T23:22:26.582996: step 2860, loss 0.549499.
Train: 2018-08-04T23:22:26.832967: step 2861, loss 0.611365.
Train: 2018-08-04T23:22:27.082909: step 2862, loss 0.627585.
Train: 2018-08-04T23:22:27.332850: step 2863, loss 0.546492.
Train: 2018-08-04T23:22:27.598408: step 2864, loss 0.570806.
Train: 2018-08-04T23:22:27.848333: step 2865, loss 0.562726.
Train: 2018-08-04T23:22:28.098266: step 2866, loss 0.562713.
Train: 2018-08-04T23:22:28.363856: step 2867, loss 0.611245.
Train: 2018-08-04T23:22:28.613800: step 2868, loss 0.546597.
Train: 2018-08-04T23:22:28.801257: step 2869, loss 0.63159.
Train: 2018-08-04T23:22:29.051193: step 2870, loss 0.514497.
Test: 2018-08-04T23:22:30.316496: step 2870, loss 0.550661.
Train: 2018-08-04T23:22:30.566437: step 2871, loss 0.56279.
Train: 2018-08-04T23:22:30.816409: step 2872, loss 0.474438.
Train: 2018-08-04T23:22:31.066351: step 2873, loss 0.562791.
Train: 2018-08-04T23:22:31.316263: step 2874, loss 0.562782.
Train: 2018-08-04T23:22:31.566234: step 2875, loss 0.530494.
Train: 2018-08-04T23:22:31.816175: step 2876, loss 0.603123.
Train: 2018-08-04T23:22:32.081707: step 2877, loss 0.546536.
Train: 2018-08-04T23:22:32.316028: step 2878, loss 0.522198.
Train: 2018-08-04T23:22:32.565969: step 2879, loss 0.595136.
Train: 2018-08-04T23:22:32.815911: step 2880, loss 0.595169.
Test: 2018-08-04T23:22:34.081239: step 2880, loss 0.548896.
Train: 2018-08-04T23:22:34.315561: step 2881, loss 0.570775.
Train: 2018-08-04T23:22:34.581150: step 2882, loss 0.513885.
Train: 2018-08-04T23:22:34.846711: step 2883, loss 0.578906.
Train: 2018-08-04T23:22:35.096652: step 2884, loss 0.578913.
Train: 2018-08-04T23:22:35.362189: step 2885, loss 0.530044.
Train: 2018-08-04T23:22:35.612159: step 2886, loss 0.529993.
Train: 2018-08-04T23:22:35.862103: step 2887, loss 0.685125.
Train: 2018-08-04T23:22:36.112013: step 2888, loss 0.595258.
Train: 2018-08-04T23:22:36.361977: step 2889, loss 0.603338.
Train: 2018-08-04T23:22:36.611929: step 2890, loss 0.587028.
Test: 2018-08-04T23:22:37.861605: step 2890, loss 0.549371.
Train: 2018-08-04T23:22:38.111547: step 2891, loss 0.5789.
Train: 2018-08-04T23:22:38.361515: step 2892, loss 0.619226.
Train: 2018-08-04T23:22:38.611457: step 2893, loss 0.594948.
Train: 2018-08-04T23:22:38.861371: step 2894, loss 0.530851.
Train: 2018-08-04T23:22:39.111340: step 2895, loss 0.554928.
Train: 2018-08-04T23:22:39.361253: step 2896, loss 0.491311.
Train: 2018-08-04T23:22:39.611223: step 2897, loss 0.547037.
Train: 2018-08-04T23:22:39.861137: step 2898, loss 0.570896.
Train: 2018-08-04T23:22:40.111079: step 2899, loss 0.475392.
Train: 2018-08-04T23:22:40.361047: step 2900, loss 0.554926.
Test: 2018-08-04T23:22:41.610726: step 2900, loss 0.549513.
Train: 2018-08-04T23:22:42.454309: step 2901, loss 0.602849.
Train: 2018-08-04T23:22:42.704251: step 2902, loss 0.530824.
Train: 2018-08-04T23:22:42.954192: step 2903, loss 0.530725.
Train: 2018-08-04T23:22:43.204133: step 2904, loss 0.57887.
Train: 2018-08-04T23:22:43.454075: step 2905, loss 0.562743.
Train: 2018-08-04T23:22:43.704017: step 2906, loss 0.554633.
Train: 2018-08-04T23:22:43.953959: step 2907, loss 0.55463.
Train: 2018-08-04T23:22:44.203899: step 2908, loss 0.53023.
Train: 2018-08-04T23:22:44.453841: step 2909, loss 0.562643.
Train: 2018-08-04T23:22:44.703782: step 2910, loss 0.538159.
Test: 2018-08-04T23:22:45.937838: step 2910, loss 0.549132.
Train: 2018-08-04T23:22:46.203427: step 2911, loss 0.57078.
Train: 2018-08-04T23:22:46.453344: step 2912, loss 0.562583.
Train: 2018-08-04T23:22:46.734527: step 2913, loss 0.570759.
Train: 2018-08-04T23:22:46.984497: step 2914, loss 0.546105.
Train: 2018-08-04T23:22:47.234434: step 2915, loss 0.628388.
Train: 2018-08-04T23:22:47.468755: step 2916, loss 0.611949.
Train: 2018-08-04T23:22:47.718671: step 2917, loss 0.480304.
Train: 2018-08-04T23:22:47.968612: step 2918, loss 0.554301.
Train: 2018-08-04T23:22:48.218584: step 2919, loss 0.529576.
Train: 2018-08-04T23:22:48.468526: step 2920, loss 0.537764.
Test: 2018-08-04T23:22:49.702581: step 2920, loss 0.548309.
Train: 2018-08-04T23:22:49.952522: step 2921, loss 0.537698.
Train: 2018-08-04T23:22:50.202495: step 2922, loss 0.545923.
Train: 2018-08-04T23:22:50.452436: step 2923, loss 0.537549.
Train: 2018-08-04T23:22:50.702379: step 2924, loss 0.570753.
Train: 2018-08-04T23:22:50.952319: step 2925, loss 0.545762.
Train: 2018-08-04T23:22:51.202232: step 2926, loss 0.570773.
Train: 2018-08-04T23:22:51.483415: step 2927, loss 0.554046.
Train: 2018-08-04T23:22:51.795841: step 2928, loss 0.51216.
Train: 2018-08-04T23:22:52.061405: step 2929, loss 0.553996.
Train: 2018-08-04T23:22:52.326967: step 2930, loss 0.579186.
Test: 2018-08-04T23:22:53.576673: step 2930, loss 0.548114.
Train: 2018-08-04T23:22:53.811025: step 2931, loss 0.570787.
Train: 2018-08-04T23:22:54.060966: step 2932, loss 0.62136.
Train: 2018-08-04T23:22:54.342146: step 2933, loss 0.545513.
Train: 2018-08-04T23:22:54.592087: step 2934, loss 0.562365.
Train: 2018-08-04T23:22:54.842003: step 2935, loss 0.663339.
Train: 2018-08-04T23:22:55.091970: step 2936, loss 0.528834.
Train: 2018-08-04T23:22:55.341912: step 2937, loss 0.612637.
Train: 2018-08-04T23:22:55.607477: step 2938, loss 0.537381.
Train: 2018-08-04T23:22:55.841770: step 2939, loss 0.562441.
Train: 2018-08-04T23:22:56.091741: step 2940, loss 0.529193.
Test: 2018-08-04T23:22:57.372660: step 2940, loss 0.547642.
Train: 2018-08-04T23:22:57.607011: step 2941, loss 0.504351.
Train: 2018-08-04T23:22:57.856950: step 2942, loss 0.637154.
Train: 2018-08-04T23:22:58.122486: step 2943, loss 0.512746.
Train: 2018-08-04T23:22:58.372451: step 2944, loss 0.587319.
Train: 2018-08-04T23:22:58.622401: step 2945, loss 0.496306.
Train: 2018-08-04T23:22:58.872754: step 2946, loss 0.603832.
Train: 2018-08-04T23:22:59.122695: step 2947, loss 0.537681.
Train: 2018-08-04T23:22:59.372637: step 2948, loss 0.537684.
Train: 2018-08-04T23:22:59.622579: step 2949, loss 0.562486.
Train: 2018-08-04T23:22:59.872522: step 2950, loss 0.661756.
Test: 2018-08-04T23:23:01.138231: step 2950, loss 0.550415.
Train: 2018-08-04T23:23:01.386285: step 2951, loss 0.53771.
Train: 2018-08-04T23:23:01.636227: step 2952, loss 0.570757.
Train: 2018-08-04T23:23:01.886200: step 2953, loss 0.562521.
Train: 2018-08-04T23:23:02.136141: step 2954, loss 0.587214.
Train: 2018-08-04T23:23:02.386083: step 2955, loss 0.644646.
Train: 2018-08-04T23:23:02.651614: step 2956, loss 0.578966.
Train: 2018-08-04T23:23:02.901557: step 2957, loss 0.554462.
Train: 2018-08-04T23:23:03.151497: step 2958, loss 0.587015.
Train: 2018-08-04T23:23:03.401470: step 2959, loss 0.5789.
Train: 2018-08-04T23:23:03.651407: step 2960, loss 0.4901.
Test: 2018-08-04T23:23:04.932330: step 2960, loss 0.548741.
Train: 2018-08-04T23:23:05.166653: step 2961, loss 0.627274.
Train: 2018-08-04T23:23:05.416623: step 2962, loss 0.522602.
Train: 2018-08-04T23:23:05.682180: step 2963, loss 0.602939.
Train: 2018-08-04T23:23:05.916477: step 2964, loss 0.530798.
Train: 2018-08-04T23:23:06.182039: step 2965, loss 0.554814.
Train: 2018-08-04T23:23:06.416359: step 2966, loss 0.562848.
Train: 2018-08-04T23:23:06.666300: step 2967, loss 0.562866.
Train: 2018-08-04T23:23:06.916272: step 2968, loss 0.570876.
Train: 2018-08-04T23:23:07.166207: step 2969, loss 0.602844.
Train: 2018-08-04T23:23:07.416155: step 2970, loss 0.483132.
Test: 2018-08-04T23:23:08.665856: step 2970, loss 0.548991.
Train: 2018-08-04T23:23:08.900183: step 2971, loss 0.570863.
Train: 2018-08-04T23:23:09.150123: step 2972, loss 0.530893.
Train: 2018-08-04T23:23:09.415655: step 2973, loss 0.538815.
Train: 2018-08-04T23:23:09.649977: step 2974, loss 0.562808.
Train: 2018-08-04T23:23:09.915565: step 2975, loss 0.522558.
Train: 2018-08-04T23:23:10.165511: step 2976, loss 0.52247.
Train: 2018-08-04T23:23:10.415448: step 2977, loss 0.578914.
Train: 2018-08-04T23:23:10.665395: step 2978, loss 0.49775.
Train: 2018-08-04T23:23:10.915305: step 2979, loss 0.5952.
Train: 2018-08-04T23:23:11.165272: step 2980, loss 0.578953.
Test: 2018-08-04T23:23:12.399332: step 2980, loss 0.550589.
Train: 2018-08-04T23:23:12.633653: step 2981, loss 0.562565.
Train: 2018-08-04T23:23:12.883594: step 2982, loss 0.595356.
Train: 2018-08-04T23:23:13.133560: step 2983, loss 0.529689.
Train: 2018-08-04T23:23:13.399098: step 2984, loss 0.521409.
Train: 2018-08-04T23:23:13.649065: step 2985, loss 0.587237.
Train: 2018-08-04T23:23:13.899007: step 2986, loss 0.496436.
Train: 2018-08-04T23:23:14.148946: step 2987, loss 0.479616.
Train: 2018-08-04T23:23:14.398865: step 2988, loss 0.620708.
Train: 2018-08-04T23:23:14.648836: step 2989, loss 0.604094.
Train: 2018-08-04T23:23:14.898772: step 2990, loss 0.62084.
Test: 2018-08-04T23:23:16.164076: step 2990, loss 0.549636.
Train: 2018-08-04T23:23:16.445292: step 2991, loss 0.562411.
Train: 2018-08-04T23:23:16.695201: step 2992, loss 0.512381.
Train: 2018-08-04T23:23:16.929523: step 2993, loss 0.654186.
Train: 2018-08-04T23:23:17.179499: step 2994, loss 0.604029.
Train: 2018-08-04T23:23:17.429405: step 2995, loss 0.512609.
Train: 2018-08-04T23:23:17.694967: step 2996, loss 0.529203.
Train: 2018-08-04T23:23:17.960531: step 2997, loss 0.562436.
Train: 2018-08-04T23:23:18.226117: step 2998, loss 0.545868.
Train: 2018-08-04T23:23:18.491691: step 2999, loss 0.537618.
Train: 2018-08-04T23:23:18.757237: step 3000, loss 0.587337.
Test: 2018-08-04T23:23:19.991304: step 3000, loss 0.548997.
Train: 2018-08-04T23:23:20.834887: step 3001, loss 0.59557.
Train: 2018-08-04T23:23:21.084822: step 3002, loss 0.587264.
Train: 2018-08-04T23:23:21.334769: step 3003, loss 0.512864.
Train: 2018-08-04T23:23:21.584710: step 3004, loss 0.554162.
Train: 2018-08-04T23:23:21.834654: step 3005, loss 0.612061.
Train: 2018-08-04T23:23:22.084591: step 3006, loss 0.529482.
Train: 2018-08-04T23:23:22.334535: step 3007, loss 0.603683.
Train: 2018-08-04T23:23:22.584478: step 3008, loss 0.546044.
Train: 2018-08-04T23:23:22.834418: step 3009, loss 0.587221.
Train: 2018-08-04T23:23:23.084330: step 3010, loss 0.595437.
Test: 2018-08-04T23:23:24.334036: step 3010, loss 0.549067.
Train: 2018-08-04T23:23:24.568357: step 3011, loss 0.513364.
Train: 2018-08-04T23:23:24.818299: step 3012, loss 0.578911.
Train: 2018-08-04T23:23:25.068241: step 3013, loss 0.586994.
Train: 2018-08-04T23:23:25.318183: step 3014, loss 0.619717.
Train: 2018-08-04T23:23:25.568156: step 3015, loss 0.538145.
Train: 2018-08-04T23:23:25.818095: step 3016, loss 0.611376.
Train: 2018-08-04T23:23:26.068030: step 3017, loss 0.538437.
Train: 2018-08-04T23:23:26.317986: step 3018, loss 0.562528.
Train: 2018-08-04T23:23:26.567919: step 3019, loss 0.554653.
Train: 2018-08-04T23:23:26.755345: step 3020, loss 0.562714.
Test: 2018-08-04T23:23:27.989432: step 3020, loss 0.550609.
Train: 2018-08-04T23:23:28.239397: step 3021, loss 0.514361.
Train: 2018-08-04T23:23:28.489339: step 3022, loss 0.506345.
Train: 2018-08-04T23:23:28.739281: step 3023, loss 0.498021.
Train: 2018-08-04T23:23:28.989227: step 3024, loss 0.538302.
Train: 2018-08-04T23:23:29.239169: step 3025, loss 0.562718.
Train: 2018-08-04T23:23:29.489110: step 3026, loss 0.513688.
Train: 2018-08-04T23:23:29.739051: step 3027, loss 0.496942.
Train: 2018-08-04T23:23:29.988993: step 3028, loss 0.56228.
Train: 2018-08-04T23:23:30.238935: step 3029, loss 0.512974.
Train: 2018-08-04T23:23:30.488876: step 3030, loss 0.562044.
Test: 2018-08-04T23:23:31.754174: step 3030, loss 0.546915.
Train: 2018-08-04T23:23:32.019739: step 3031, loss 0.596443.
Train: 2018-08-04T23:23:32.285332: step 3032, loss 0.604742.
Train: 2018-08-04T23:23:32.550888: step 3033, loss 0.630217.
Train: 2018-08-04T23:23:32.785203: step 3034, loss 0.545498.
Train: 2018-08-04T23:23:33.050746: step 3035, loss 0.554166.
Train: 2018-08-04T23:23:33.285066: step 3036, loss 0.578955.
Train: 2018-08-04T23:23:33.535010: step 3037, loss 0.562106.
Train: 2018-08-04T23:23:33.784979: step 3038, loss 0.595564.
Train: 2018-08-04T23:23:34.034890: step 3039, loss 0.537292.
Train: 2018-08-04T23:23:34.284862: step 3040, loss 0.529057.
Test: 2018-08-04T23:23:35.550161: step 3040, loss 0.548439.
Train: 2018-08-04T23:23:35.784511: step 3041, loss 0.612666.
Train: 2018-08-04T23:23:36.050044: step 3042, loss 0.58765.
Train: 2018-08-04T23:23:36.284363: step 3043, loss 0.662318.
Train: 2018-08-04T23:23:36.549957: step 3044, loss 0.54552.
Train: 2018-08-04T23:23:36.799868: step 3045, loss 0.554084.
Train: 2018-08-04T23:23:37.065431: step 3046, loss 0.603507.
Train: 2018-08-04T23:23:37.315372: step 3047, loss 0.489007.
Train: 2018-08-04T23:23:37.565345: step 3048, loss 0.54628.
Train: 2018-08-04T23:23:37.815286: step 3049, loss 0.603622.
Train: 2018-08-04T23:23:38.065227: step 3050, loss 0.562562.
Test: 2018-08-04T23:23:39.330526: step 3050, loss 0.549175.
Train: 2018-08-04T23:23:39.596115: step 3051, loss 0.513941.
Train: 2018-08-04T23:23:39.861651: step 3052, loss 0.562476.
Train: 2018-08-04T23:23:40.111593: step 3053, loss 0.497612.
Train: 2018-08-04T23:23:40.345914: step 3054, loss 0.546511.
Train: 2018-08-04T23:23:40.595888: step 3055, loss 0.554106.
Train: 2018-08-04T23:23:40.845796: step 3056, loss 0.603572.
Train: 2018-08-04T23:23:41.095738: step 3057, loss 0.496981.
Train: 2018-08-04T23:23:41.361301: step 3058, loss 0.562518.
Train: 2018-08-04T23:23:41.595645: step 3059, loss 0.636448.
Train: 2018-08-04T23:23:41.861209: step 3060, loss 0.562554.
Test: 2018-08-04T23:23:43.110890: step 3060, loss 0.549412.
Train: 2018-08-04T23:23:43.345242: step 3061, loss 0.587266.
Train: 2018-08-04T23:23:43.595183: step 3062, loss 0.496745.
Train: 2018-08-04T23:23:43.845118: step 3063, loss 0.554121.
Train: 2018-08-04T23:23:44.095066: step 3064, loss 0.619998.
Train: 2018-08-04T23:23:44.345003: step 3065, loss 0.546166.
Train: 2018-08-04T23:23:44.594950: step 3066, loss 0.595577.
Train: 2018-08-04T23:23:44.844859: step 3067, loss 0.611772.
Train: 2018-08-04T23:23:45.094832: step 3068, loss 0.530003.
Train: 2018-08-04T23:23:45.344773: step 3069, loss 0.49713.
Train: 2018-08-04T23:23:45.594715: step 3070, loss 0.513533.
Test: 2018-08-04T23:23:46.828771: step 3070, loss 0.548333.
Train: 2018-08-04T23:23:47.078748: step 3071, loss 0.57902.
Train: 2018-08-04T23:23:47.328652: step 3072, loss 0.546236.
Train: 2018-08-04T23:23:47.578621: step 3073, loss 0.62812.
Train: 2018-08-04T23:23:47.828562: step 3074, loss 0.562689.
Train: 2018-08-04T23:23:48.078506: step 3075, loss 0.677549.
Train: 2018-08-04T23:23:48.328450: step 3076, loss 0.586802.
Train: 2018-08-04T23:23:48.578392: step 3077, loss 0.538228.
Train: 2018-08-04T23:23:48.828302: step 3078, loss 0.538511.
Train: 2018-08-04T23:23:49.078243: step 3079, loss 0.570642.
Train: 2018-08-04T23:23:49.328183: step 3080, loss 0.586917.
Test: 2018-08-04T23:23:50.577892: step 3080, loss 0.549977.
Train: 2018-08-04T23:23:50.827833: step 3081, loss 0.554636.
Train: 2018-08-04T23:23:51.077807: step 3082, loss 0.554787.
Train: 2018-08-04T23:23:51.327745: step 3083, loss 0.514304.
Train: 2018-08-04T23:23:51.577689: step 3084, loss 0.538419.
Train: 2018-08-04T23:23:51.827626: step 3085, loss 0.522593.
Train: 2018-08-04T23:23:52.077565: step 3086, loss 0.514236.
Train: 2018-08-04T23:23:52.343104: step 3087, loss 0.570728.
Train: 2018-08-04T23:23:52.577456: step 3088, loss 0.530156.
Train: 2018-08-04T23:23:52.827394: step 3089, loss 0.546189.
Train: 2018-08-04T23:23:53.077333: step 3090, loss 0.513571.
Test: 2018-08-04T23:23:54.327013: step 3090, loss 0.547712.
Train: 2018-08-04T23:23:54.576983: step 3091, loss 0.480462.
Train: 2018-08-04T23:23:54.826897: step 3092, loss 0.587035.
Train: 2018-08-04T23:23:55.076870: step 3093, loss 0.554313.
Train: 2018-08-04T23:23:55.326804: step 3094, loss 0.562519.
Train: 2018-08-04T23:23:55.576746: step 3095, loss 0.637188.
Train: 2018-08-04T23:23:55.826690: step 3096, loss 0.520857.
Train: 2018-08-04T23:23:56.076637: step 3097, loss 0.612914.
Train: 2018-08-04T23:23:56.326578: step 3098, loss 0.604248.
Train: 2018-08-04T23:23:56.592133: step 3099, loss 0.512186.
Train: 2018-08-04T23:23:56.826430: step 3100, loss 0.512107.
Test: 2018-08-04T23:23:58.076135: step 3100, loss 0.54838.
Train: 2018-08-04T23:23:59.013415: step 3101, loss 0.512058.
Train: 2018-08-04T23:23:59.263388: step 3102, loss 0.546086.
Train: 2018-08-04T23:23:59.550037: step 3103, loss 0.579208.
Train: 2018-08-04T23:23:59.791796: step 3104, loss 0.528948.
Train: 2018-08-04T23:24:00.057333: step 3105, loss 0.596194.
Train: 2018-08-04T23:24:00.307273: step 3106, loss 0.647254.
Train: 2018-08-04T23:24:00.572868: step 3107, loss 0.578825.
Train: 2018-08-04T23:24:00.838430: step 3108, loss 0.570723.
Train: 2018-08-04T23:24:01.088351: step 3109, loss 0.512177.
Train: 2018-08-04T23:24:01.338313: step 3110, loss 0.520076.
Test: 2018-08-04T23:24:02.619232: step 3110, loss 0.547012.
Train: 2018-08-04T23:24:02.869205: step 3111, loss 0.587428.
Train: 2018-08-04T23:24:03.103526: step 3112, loss 0.587518.
Train: 2018-08-04T23:24:03.369057: step 3113, loss 0.51207.
Train: 2018-08-04T23:24:03.681509: step 3114, loss 0.62105.
Train: 2018-08-04T23:24:03.915835: step 3115, loss 0.453653.
Train: 2018-08-04T23:24:04.165776: step 3116, loss 0.570725.
Train: 2018-08-04T23:24:04.415717: step 3117, loss 0.503642.
Train: 2018-08-04T23:24:04.665628: step 3118, loss 0.595865.
Train: 2018-08-04T23:24:04.918484: step 3119, loss 0.553937.
Train: 2018-08-04T23:24:05.184740: step 3120, loss 0.579114.
Test: 2018-08-04T23:24:06.454085: step 3120, loss 0.547965.
Train: 2018-08-04T23:24:06.688408: step 3121, loss 0.688407.
Train: 2018-08-04T23:24:06.938375: step 3122, loss 0.637501.
Train: 2018-08-04T23:24:07.203943: step 3123, loss 0.587766.
Train: 2018-08-04T23:24:07.469503: step 3124, loss 0.596103.
Train: 2018-08-04T23:24:07.703795: step 3125, loss 0.554514.
Train: 2018-08-04T23:24:07.953740: step 3126, loss 0.529801.
Train: 2018-08-04T23:24:08.203676: step 3127, loss 0.554362.
Train: 2018-08-04T23:24:08.453650: step 3128, loss 0.578692.
Train: 2018-08-04T23:24:08.703560: step 3129, loss 0.521955.
Train: 2018-08-04T23:24:08.969122: step 3130, loss 0.619375.
Test: 2018-08-04T23:24:10.234451: step 3130, loss 0.548449.
Train: 2018-08-04T23:24:10.468796: step 3131, loss 0.586995.
Train: 2018-08-04T23:24:10.718744: step 3132, loss 0.57071.
Train: 2018-08-04T23:24:10.968684: step 3133, loss 0.586856.
Train: 2018-08-04T23:24:11.218596: step 3134, loss 0.530769.
Train: 2018-08-04T23:24:11.468567: step 3135, loss 0.562911.
Train: 2018-08-04T23:24:11.718509: step 3136, loss 0.54688.
Train: 2018-08-04T23:24:11.968421: step 3137, loss 0.554998.
Train: 2018-08-04T23:24:12.218392: step 3138, loss 0.539037.
Train: 2018-08-04T23:24:12.468334: step 3139, loss 0.610679.
Train: 2018-08-04T23:24:12.718270: step 3140, loss 0.538969.
Test: 2018-08-04T23:24:13.967951: step 3140, loss 0.550173.
Train: 2018-08-04T23:24:14.206240: step 3141, loss 0.562923.
Train: 2018-08-04T23:24:14.449255: step 3142, loss 0.555177.
Train: 2018-08-04T23:24:14.699209: step 3143, loss 0.594699.
Train: 2018-08-04T23:24:14.949145: step 3144, loss 0.578922.
Train: 2018-08-04T23:24:15.199062: step 3145, loss 0.523024.
Train: 2018-08-04T23:24:15.480246: step 3146, loss 0.523184.
Train: 2018-08-04T23:24:15.730210: step 3147, loss 0.51502.
Train: 2018-08-04T23:24:15.980127: step 3148, loss 0.562765.
Train: 2018-08-04T23:24:16.230099: step 3149, loss 0.482561.
Train: 2018-08-04T23:24:16.480036: step 3150, loss 0.554702.
Test: 2018-08-04T23:24:17.745340: step 3150, loss 0.549382.
Train: 2018-08-04T23:24:18.042178: step 3151, loss 0.5951.
Train: 2018-08-04T23:24:18.292119: step 3152, loss 0.578754.
Train: 2018-08-04T23:24:18.542064: step 3153, loss 0.497556.
Train: 2018-08-04T23:24:18.791998: step 3154, loss 0.619666.
Train: 2018-08-04T23:24:19.041944: step 3155, loss 0.611662.
Train: 2018-08-04T23:24:19.291886: step 3156, loss 0.545885.
Train: 2018-08-04T23:24:19.541827: step 3157, loss 0.554457.
Train: 2018-08-04T23:24:19.807357: step 3158, loss 0.570804.
Train: 2018-08-04T23:24:20.072950: step 3159, loss 0.578825.
Train: 2018-08-04T23:24:20.322861: step 3160, loss 0.603504.
Test: 2018-08-04T23:24:21.588190: step 3160, loss 0.548652.
Train: 2018-08-04T23:24:21.838155: step 3161, loss 0.595285.
Train: 2018-08-04T23:24:22.088072: step 3162, loss 0.480481.
Train: 2018-08-04T23:24:22.338014: step 3163, loss 0.587222.
Train: 2018-08-04T23:24:22.587955: step 3164, loss 0.587362.
Train: 2018-08-04T23:24:22.837896: step 3165, loss 0.587151.
Train: 2018-08-04T23:24:23.087839: step 3166, loss 0.595255.
Train: 2018-08-04T23:24:23.337780: step 3167, loss 0.587179.
Train: 2018-08-04T23:24:23.587751: step 3168, loss 0.578857.
Train: 2018-08-04T23:24:23.837693: step 3169, loss 0.570804.
Train: 2018-08-04T23:24:24.087634: step 3170, loss 0.578774.
Test: 2018-08-04T23:24:25.337311: step 3170, loss 0.54902.
Train: 2018-08-04T23:24:25.524767: step 3171, loss 0.614441.
Train: 2018-08-04T23:24:25.774733: step 3172, loss 0.619184.
Train: 2018-08-04T23:24:26.024681: step 3173, loss 0.546802.
Train: 2018-08-04T23:24:26.274624: step 3174, loss 0.546837.
Train: 2018-08-04T23:24:26.524557: step 3175, loss 0.491066.
Train: 2018-08-04T23:24:26.774508: step 3176, loss 0.53098.
Train: 2018-08-04T23:24:27.024421: step 3177, loss 0.571237.
Train: 2018-08-04T23:24:27.274388: step 3178, loss 0.65845.
Train: 2018-08-04T23:24:27.524327: step 3179, loss 0.56307.
Train: 2018-08-04T23:24:27.774270: step 3180, loss 0.594769.
Test: 2018-08-04T23:24:29.039568: step 3180, loss 0.549041.
Train: 2018-08-04T23:24:29.273923: step 3181, loss 0.515592.
Train: 2018-08-04T23:24:29.523861: step 3182, loss 0.539148.
Train: 2018-08-04T23:24:29.773805: step 3183, loss 0.571187.
Train: 2018-08-04T23:24:30.023714: step 3184, loss 0.523403.
Train: 2018-08-04T23:24:30.273689: step 3185, loss 0.547225.
Train: 2018-08-04T23:24:30.523627: step 3186, loss 0.499519.
Train: 2018-08-04T23:24:30.773568: step 3187, loss 0.539131.
Train: 2018-08-04T23:24:31.023480: step 3188, loss 0.59493.
Train: 2018-08-04T23:24:31.273452: step 3189, loss 0.586846.
Train: 2018-08-04T23:24:31.523391: step 3190, loss 0.514884.
Test: 2018-08-04T23:24:32.773070: step 3190, loss 0.55043.
Train: 2018-08-04T23:24:33.007421: step 3191, loss 0.546745.
Train: 2018-08-04T23:24:33.257365: step 3192, loss 0.627595.
Train: 2018-08-04T23:24:33.507298: step 3193, loss 0.57088.
Train: 2018-08-04T23:24:33.757245: step 3194, loss 0.538489.
Train: 2018-08-04T23:24:34.007157: step 3195, loss 0.619281.
Train: 2018-08-04T23:24:34.257128: step 3196, loss 0.602985.
Train: 2018-08-04T23:24:34.507070: step 3197, loss 0.578952.
Train: 2018-08-04T23:24:34.757011: step 3198, loss 0.570883.
Train: 2018-08-04T23:24:35.006923: step 3199, loss 0.578793.
Train: 2018-08-04T23:24:35.272518: step 3200, loss 0.602972.
Test: 2018-08-04T23:24:36.522192: step 3200, loss 0.54918.
Train: 2018-08-04T23:24:37.381399: step 3201, loss 0.578975.
Train: 2018-08-04T23:24:37.631338: step 3202, loss 0.57097.
Train: 2018-08-04T23:24:37.881249: step 3203, loss 0.562768.
Train: 2018-08-04T23:24:38.131190: step 3204, loss 0.570885.
Train: 2018-08-04T23:24:38.381133: step 3205, loss 0.554874.
Train: 2018-08-04T23:24:38.631104: step 3206, loss 0.55497.
Train: 2018-08-04T23:24:38.881016: step 3207, loss 0.555072.
Train: 2018-08-04T23:24:39.130987: step 3208, loss 0.539079.
Train: 2018-08-04T23:24:39.380928: step 3209, loss 0.570773.
Train: 2018-08-04T23:24:39.630839: step 3210, loss 0.562892.
Test: 2018-08-04T23:24:40.880547: step 3210, loss 0.550014.
Train: 2018-08-04T23:24:41.114866: step 3211, loss 0.515126.
Train: 2018-08-04T23:24:41.380430: step 3212, loss 0.562822.
Train: 2018-08-04T23:24:41.630371: step 3213, loss 0.594839.
Train: 2018-08-04T23:24:41.864691: step 3214, loss 0.522963.
Train: 2018-08-04T23:24:42.114659: step 3215, loss 0.530812.
Train: 2018-08-04T23:24:42.364605: step 3216, loss 0.570785.
Train: 2018-08-04T23:24:42.614515: step 3217, loss 0.562837.
Train: 2018-08-04T23:24:42.863845: step 3218, loss 0.594885.
Train: 2018-08-04T23:24:43.113812: step 3219, loss 0.55466.
Train: 2018-08-04T23:24:43.363759: step 3220, loss 0.546397.
Test: 2018-08-04T23:24:44.597814: step 3220, loss 0.548437.
Train: 2018-08-04T23:24:44.847756: step 3221, loss 0.578822.
Train: 2018-08-04T23:24:45.082108: step 3222, loss 0.5219.
Train: 2018-08-04T23:24:45.332019: step 3223, loss 0.627752.
Train: 2018-08-04T23:24:45.597579: step 3224, loss 0.578776.
Train: 2018-08-04T23:24:45.863143: step 3225, loss 0.546154.
Train: 2018-08-04T23:24:46.113110: step 3226, loss 0.489498.
Train: 2018-08-04T23:24:46.347405: step 3227, loss 0.521737.
Train: 2018-08-04T23:24:46.597376: step 3228, loss 0.554124.
Train: 2018-08-04T23:24:46.847317: step 3229, loss 0.570759.
Train: 2018-08-04T23:24:47.097260: step 3230, loss 0.537771.
Test: 2018-08-04T23:24:48.346936: step 3230, loss 0.54855.
Train: 2018-08-04T23:24:48.581289: step 3231, loss 0.496557.
Train: 2018-08-04T23:24:48.862442: step 3232, loss 0.513182.
Train: 2018-08-04T23:24:49.143625: step 3233, loss 0.579083.
Train: 2018-08-04T23:24:49.409212: step 3234, loss 0.56229.
Train: 2018-08-04T23:24:49.659129: step 3235, loss 0.562338.
Train: 2018-08-04T23:24:49.909070: step 3236, loss 0.671701.
Train: 2018-08-04T23:24:50.174633: step 3237, loss 0.570904.
Train: 2018-08-04T23:24:50.424575: step 3238, loss 0.562397.
Train: 2018-08-04T23:24:50.674546: step 3239, loss 0.637799.
Train: 2018-08-04T23:24:50.924481: step 3240, loss 0.570533.
Test: 2018-08-04T23:24:52.158543: step 3240, loss 0.549648.
Train: 2018-08-04T23:24:52.392894: step 3241, loss 0.528708.
Train: 2018-08-04T23:24:52.642836: step 3242, loss 0.587495.
Train: 2018-08-04T23:24:52.892746: step 3243, loss 0.546598.
Train: 2018-08-04T23:24:53.142714: step 3244, loss 0.554211.
Train: 2018-08-04T23:24:53.408277: step 3245, loss 0.578914.
Train: 2018-08-04T23:24:53.673814: step 3246, loss 0.670148.
Train: 2018-08-04T23:24:53.908134: step 3247, loss 0.578712.
Train: 2018-08-04T23:24:54.173697: step 3248, loss 0.530086.
Train: 2018-08-04T23:24:54.439261: step 3249, loss 0.546351.
Train: 2018-08-04T23:24:54.689201: step 3250, loss 0.603254.
Test: 2018-08-04T23:24:55.954529: step 3250, loss 0.549488.
Train: 2018-08-04T23:24:56.220118: step 3251, loss 0.660245.
Train: 2018-08-04T23:24:56.454445: step 3252, loss 0.530816.
Train: 2018-08-04T23:24:56.704381: step 3253, loss 0.578537.
Train: 2018-08-04T23:24:56.954295: step 3254, loss 0.547128.
Train: 2018-08-04T23:24:57.204269: step 3255, loss 0.538998.
Train: 2018-08-04T23:24:57.454210: step 3256, loss 0.610635.
Train: 2018-08-04T23:24:57.704119: step 3257, loss 0.54711.
Train: 2018-08-04T23:24:57.954093: step 3258, loss 0.578827.
Train: 2018-08-04T23:24:58.204034: step 3259, loss 0.563021.
Train: 2018-08-04T23:24:58.453944: step 3260, loss 0.555145.
Test: 2018-08-04T23:24:59.703652: step 3260, loss 0.550311.
Train: 2018-08-04T23:24:59.937996: step 3261, loss 0.539318.
Train: 2018-08-04T23:25:00.187941: step 3262, loss 0.531522.
Train: 2018-08-04T23:25:00.437882: step 3263, loss 0.586802.
Train: 2018-08-04T23:25:00.687796: step 3264, loss 0.586766.
Train: 2018-08-04T23:25:00.937768: step 3265, loss 0.57879.
Train: 2018-08-04T23:25:01.187703: step 3266, loss 0.563184.
Train: 2018-08-04T23:25:01.437653: step 3267, loss 0.500077.
Train: 2018-08-04T23:25:01.687592: step 3268, loss 0.586743.
Train: 2018-08-04T23:25:01.937536: step 3269, loss 0.547327.
Train: 2018-08-04T23:25:02.187470: step 3270, loss 0.452242.
Test: 2018-08-04T23:25:03.452774: step 3270, loss 0.549675.
Train: 2018-08-04T23:25:03.718362: step 3271, loss 0.634542.
Train: 2018-08-04T23:25:03.968309: step 3272, loss 0.531051.
Train: 2018-08-04T23:25:04.218219: step 3273, loss 0.51485.
Train: 2018-08-04T23:25:04.468174: step 3274, loss 0.578892.
Train: 2018-08-04T23:25:04.718133: step 3275, loss 0.578886.
Train: 2018-08-04T23:25:04.968043: step 3276, loss 0.570939.
Train: 2018-08-04T23:25:05.233608: step 3277, loss 0.562627.
Train: 2018-08-04T23:25:05.483548: step 3278, loss 0.522249.
Train: 2018-08-04T23:25:05.717895: step 3279, loss 0.505772.
Train: 2018-08-04T23:25:05.967841: step 3280, loss 0.537988.
Test: 2018-08-04T23:25:07.217516: step 3280, loss 0.548139.
Train: 2018-08-04T23:25:07.451870: step 3281, loss 0.521384.
Train: 2018-08-04T23:25:07.701778: step 3282, loss 0.529606.
Train: 2018-08-04T23:25:07.951747: step 3283, loss 0.595545.
Train: 2018-08-04T23:25:08.201689: step 3284, loss 0.587388.
Train: 2018-08-04T23:25:08.451637: step 3285, loss 0.570496.
Train: 2018-08-04T23:25:08.701576: step 3286, loss 0.545935.
Train: 2018-08-04T23:25:08.951511: step 3287, loss 0.512135.
Train: 2018-08-04T23:25:09.201459: step 3288, loss 0.561988.
Train: 2018-08-04T23:25:09.451393: step 3289, loss 0.570685.
Train: 2018-08-04T23:25:09.701342: step 3290, loss 0.587368.
Test: 2018-08-04T23:25:10.951017: step 3290, loss 0.549445.
Train: 2018-08-04T23:25:11.185337: step 3291, loss 0.604223.
Train: 2018-08-04T23:25:11.435307: step 3292, loss 0.612745.
Train: 2018-08-04T23:25:11.685220: step 3293, loss 0.529018.
Train: 2018-08-04T23:25:11.935162: step 3294, loss 0.528384.
Train: 2018-08-04T23:25:12.185134: step 3295, loss 0.562839.
Train: 2018-08-04T23:25:12.435072: step 3296, loss 0.646169.
Train: 2018-08-04T23:25:12.684988: step 3297, loss 0.536909.
Train: 2018-08-04T23:25:12.934961: step 3298, loss 0.562459.
Train: 2018-08-04T23:25:13.184902: step 3299, loss 0.54566.
Train: 2018-08-04T23:25:13.434835: step 3300, loss 0.528952.
Test: 2018-08-04T23:25:14.668897: step 3300, loss 0.548441.
Train: 2018-08-04T23:25:15.606178: step 3301, loss 0.562587.
Train: 2018-08-04T23:25:15.856149: step 3302, loss 0.495506.
Train: 2018-08-04T23:25:16.106085: step 3303, loss 0.604436.
Train: 2018-08-04T23:25:16.356028: step 3304, loss 0.596171.
Train: 2018-08-04T23:25:16.605969: step 3305, loss 0.604974.
Train: 2018-08-04T23:25:16.855916: step 3306, loss 0.529128.
Train: 2018-08-04T23:25:17.105827: step 3307, loss 0.57851.
Train: 2018-08-04T23:25:17.355793: step 3308, loss 0.612065.
Train: 2018-08-04T23:25:17.605710: step 3309, loss 0.579213.
Train: 2018-08-04T23:25:17.855681: step 3310, loss 0.562397.
Test: 2018-08-04T23:25:19.089738: step 3310, loss 0.548212.
Train: 2018-08-04T23:25:19.339709: step 3311, loss 0.59533.
Train: 2018-08-04T23:25:19.589650: step 3312, loss 0.537876.
Train: 2018-08-04T23:25:19.839587: step 3313, loss 0.586555.
Train: 2018-08-04T23:25:20.089533: step 3314, loss 0.529903.
Train: 2018-08-04T23:25:20.339475: step 3315, loss 0.570903.
Train: 2018-08-04T23:25:20.589416: step 3316, loss 0.522041.
Train: 2018-08-04T23:25:20.839327: step 3317, loss 0.538768.
Train: 2018-08-04T23:25:21.104921: step 3318, loss 0.46476.
Train: 2018-08-04T23:25:21.339236: step 3319, loss 0.562707.
Train: 2018-08-04T23:25:21.589178: step 3320, loss 0.537947.
Test: 2018-08-04T23:25:22.838858: step 3320, loss 0.549824.
Train: 2018-08-04T23:25:23.073212: step 3321, loss 0.570703.
Train: 2018-08-04T23:25:23.260665: step 3322, loss 0.598183.
Train: 2018-08-04T23:25:23.510576: step 3323, loss 0.562381.
Train: 2018-08-04T23:25:23.760548: step 3324, loss 0.570701.
Train: 2018-08-04T23:25:24.010489: step 3325, loss 0.49673.
Train: 2018-08-04T23:25:24.260432: step 3326, loss 0.537701.
Train: 2018-08-04T23:25:24.510373: step 3327, loss 0.537531.
Train: 2018-08-04T23:25:24.760285: step 3328, loss 0.645307.
Train: 2018-08-04T23:25:25.010251: step 3329, loss 0.537607.
Train: 2018-08-04T23:25:25.260197: step 3330, loss 0.62059.
Test: 2018-08-04T23:25:26.525495: step 3330, loss 0.548798.
Train: 2018-08-04T23:25:26.759842: step 3331, loss 0.538172.
Train: 2018-08-04T23:25:27.009783: step 3332, loss 0.570249.
Train: 2018-08-04T23:25:27.259729: step 3333, loss 0.504619.
Train: 2018-08-04T23:25:27.509670: step 3334, loss 0.637167.
Train: 2018-08-04T23:25:27.759612: step 3335, loss 0.628703.
Train: 2018-08-04T23:25:28.009548: step 3336, loss 0.554416.
Train: 2018-08-04T23:25:28.259490: step 3337, loss 0.521371.
Train: 2018-08-04T23:25:28.509436: step 3338, loss 0.61959.
Train: 2018-08-04T23:25:28.759378: step 3339, loss 0.619624.
Train: 2018-08-04T23:25:29.009314: step 3340, loss 0.627936.
Test: 2018-08-04T23:25:30.274617: step 3340, loss 0.548287.
Train: 2018-08-04T23:25:30.555835: step 3341, loss 0.554529.
Train: 2018-08-04T23:25:30.821402: step 3342, loss 0.49791.
Train: 2018-08-04T23:25:31.055685: step 3343, loss 0.538106.
Train: 2018-08-04T23:25:31.321248: step 3344, loss 0.586606.
Train: 2018-08-04T23:25:31.571189: step 3345, loss 0.586945.
Train: 2018-08-04T23:25:31.836751: step 3346, loss 0.506585.
Train: 2018-08-04T23:25:32.102341: step 3347, loss 0.538745.
Train: 2018-08-04T23:25:32.352255: step 3348, loss 0.563034.
Train: 2018-08-04T23:25:32.602222: step 3349, loss 0.594956.
Train: 2018-08-04T23:25:32.867760: step 3350, loss 0.586985.
Test: 2018-08-04T23:25:34.117469: step 3350, loss 0.549516.
Train: 2018-08-04T23:25:34.367410: step 3351, loss 0.546435.
Train: 2018-08-04T23:25:34.617381: step 3352, loss 0.490245.
Train: 2018-08-04T23:25:34.867292: step 3353, loss 0.554778.
Train: 2018-08-04T23:25:35.117264: step 3354, loss 0.546596.
Train: 2018-08-04T23:25:35.367205: step 3355, loss 0.546553.
Train: 2018-08-04T23:25:35.617141: step 3356, loss 0.53853.
Train: 2018-08-04T23:25:35.882705: step 3357, loss 0.53788.
Train: 2018-08-04T23:25:36.132651: step 3358, loss 0.538778.
Train: 2018-08-04T23:25:36.382592: step 3359, loss 0.553779.
Train: 2018-08-04T23:25:36.632534: step 3360, loss 0.578764.
Test: 2018-08-04T23:25:37.866589: step 3360, loss 0.548187.
Train: 2018-08-04T23:25:38.116556: step 3361, loss 0.554017.
Train: 2018-08-04T23:25:38.366503: step 3362, loss 0.60463.
Train: 2018-08-04T23:25:38.616445: step 3363, loss 0.57117.
Train: 2018-08-04T23:25:38.866381: step 3364, loss 0.554669.
Train: 2018-08-04T23:25:39.116328: step 3365, loss 0.562577.
Train: 2018-08-04T23:25:39.366269: step 3366, loss 0.57965.
Train: 2018-08-04T23:25:39.616213: step 3367, loss 0.562358.
Train: 2018-08-04T23:25:39.866152: step 3368, loss 0.58696.
Train: 2018-08-04T23:25:40.116093: step 3369, loss 0.669575.
Train: 2018-08-04T23:25:40.366003: step 3370, loss 0.58707.
Test: 2018-08-04T23:25:41.615712: step 3370, loss 0.548691.
Train: 2018-08-04T23:25:41.850032: step 3371, loss 0.571418.
Train: 2018-08-04T23:25:42.100004: step 3372, loss 0.546313.
Train: 2018-08-04T23:25:42.349945: step 3373, loss 0.554564.
Train: 2018-08-04T23:25:42.599886: step 3374, loss 0.58695.
Train: 2018-08-04T23:25:42.849828: step 3375, loss 0.586934.
Train: 2018-08-04T23:25:43.099770: step 3376, loss 0.587128.
Train: 2018-08-04T23:25:43.349711: step 3377, loss 0.579064.
Train: 2018-08-04T23:25:43.599621: step 3378, loss 0.578722.
Train: 2018-08-04T23:25:43.849564: step 3379, loss 0.62691.
Train: 2018-08-04T23:25:44.099506: step 3380, loss 0.507398.
Test: 2018-08-04T23:25:45.364834: step 3380, loss 0.549236.
Train: 2018-08-04T23:25:45.599185: step 3381, loss 0.626231.
Train: 2018-08-04T23:25:45.849129: step 3382, loss 0.55509.
Train: 2018-08-04T23:25:46.099037: step 3383, loss 0.508034.
Train: 2018-08-04T23:25:46.348979: step 3384, loss 0.508143.
Train: 2018-08-04T23:25:46.598944: step 3385, loss 0.531689.
Train: 2018-08-04T23:25:46.848862: step 3386, loss 0.492183.
Train: 2018-08-04T23:25:47.098804: step 3387, loss 0.547126.
Train: 2018-08-04T23:25:47.348770: step 3388, loss 0.57081.
Train: 2018-08-04T23:25:47.598716: step 3389, loss 0.586755.
Train: 2018-08-04T23:25:47.848652: step 3390, loss 0.522996.
Test: 2018-08-04T23:25:49.098335: step 3390, loss 0.549649.
Train: 2018-08-04T23:25:49.348306: step 3391, loss 0.554932.
Train: 2018-08-04T23:25:49.598243: step 3392, loss 0.554759.
Train: 2018-08-04T23:25:49.848189: step 3393, loss 0.627246.
Train: 2018-08-04T23:25:50.098101: step 3394, loss 0.490025.
Train: 2018-08-04T23:25:50.348072: step 3395, loss 0.595122.
Train: 2018-08-04T23:25:50.598009: step 3396, loss 0.578915.
Train: 2018-08-04T23:25:50.847924: step 3397, loss 0.595208.
Train: 2018-08-04T23:25:51.097897: step 3398, loss 0.579058.
Train: 2018-08-04T23:25:51.347838: step 3399, loss 0.562624.
Train: 2018-08-04T23:25:51.597750: step 3400, loss 0.521983.
Test: 2018-08-04T23:25:52.847456: step 3400, loss 0.548664.
Train: 2018-08-04T23:25:53.675420: step 3401, loss 0.586904.
Train: 2018-08-04T23:25:53.925330: step 3402, loss 0.603312.
Train: 2018-08-04T23:25:54.175295: step 3403, loss 0.546296.
Train: 2018-08-04T23:25:54.425242: step 3404, loss 0.603388.
Train: 2018-08-04T23:25:54.675186: step 3405, loss 0.538409.
Train: 2018-08-04T23:25:54.925094: step 3406, loss 0.595214.
Train: 2018-08-04T23:25:55.175066: step 3407, loss 0.530214.
Train: 2018-08-04T23:25:55.425010: step 3408, loss 0.522222.
Train: 2018-08-04T23:25:55.690541: step 3409, loss 0.579029.
Train: 2018-08-04T23:25:55.956103: step 3410, loss 0.546411.
Test: 2018-08-04T23:25:57.205810: step 3410, loss 0.549641.
Train: 2018-08-04T23:25:57.440164: step 3411, loss 0.538229.
Train: 2018-08-04T23:25:57.690104: step 3412, loss 0.587163.
Train: 2018-08-04T23:25:57.940039: step 3413, loss 0.603318.
Train: 2018-08-04T23:25:58.189980: step 3414, loss 0.578964.
Train: 2018-08-04T23:25:58.439929: step 3415, loss 0.619604.
Train: 2018-08-04T23:25:58.689870: step 3416, loss 0.587014.
Train: 2018-08-04T23:25:58.939812: step 3417, loss 0.603129.
Train: 2018-08-04T23:25:59.189748: step 3418, loss 0.595096.
Train: 2018-08-04T23:25:59.439662: step 3419, loss 0.594969.
Train: 2018-08-04T23:25:59.689636: step 3420, loss 0.586865.
Test: 2018-08-04T23:26:00.939313: step 3420, loss 0.548874.
Train: 2018-08-04T23:26:01.173664: step 3421, loss 0.54693.
Train: 2018-08-04T23:26:01.423603: step 3422, loss 0.547108.
Train: 2018-08-04T23:26:01.673545: step 3423, loss 0.547179.
Train: 2018-08-04T23:26:01.923488: step 3424, loss 0.594722.
Train: 2018-08-04T23:26:02.173428: step 3425, loss 0.555198.
Train: 2018-08-04T23:26:02.423369: step 3426, loss 0.570976.
Train: 2018-08-04T23:26:02.673281: step 3427, loss 0.539534.
Train: 2018-08-04T23:26:02.923252: step 3428, loss 0.633962.
Train: 2018-08-04T23:26:03.173196: step 3429, loss 0.547469.
Train: 2018-08-04T23:26:03.423137: step 3430, loss 0.52396.
Test: 2018-08-04T23:26:04.719676: step 3430, loss 0.549535.
Train: 2018-08-04T23:26:04.969642: step 3431, loss 0.555334.
Train: 2018-08-04T23:26:05.219590: step 3432, loss 0.602424.
Train: 2018-08-04T23:26:05.469531: step 3433, loss 0.586701.
Train: 2018-08-04T23:26:05.719473: step 3434, loss 0.578846.
Train: 2018-08-04T23:26:05.969414: step 3435, loss 0.531875.
Train: 2018-08-04T23:26:06.219355: step 3436, loss 0.594564.
Train: 2018-08-04T23:26:06.469298: step 3437, loss 0.571055.
Train: 2018-08-04T23:26:06.719238: step 3438, loss 0.571054.
Train: 2018-08-04T23:26:06.969149: step 3439, loss 0.524062.
Train: 2018-08-04T23:26:07.219117: step 3440, loss 0.54752.
Test: 2018-08-04T23:26:08.453176: step 3440, loss 0.550962.
Train: 2018-08-04T23:26:08.687498: step 3441, loss 0.547477.
Train: 2018-08-04T23:26:08.937471: step 3442, loss 0.523808.
Train: 2018-08-04T23:26:09.187412: step 3443, loss 0.610431.
Train: 2018-08-04T23:26:09.437346: step 3444, loss 0.515638.
Train: 2018-08-04T23:26:09.687295: step 3445, loss 0.531315.
Train: 2018-08-04T23:26:09.937237: step 3446, loss 0.578852.
Train: 2018-08-04T23:26:10.187178: step 3447, loss 0.554933.
Train: 2018-08-04T23:26:10.437119: step 3448, loss 0.514864.
Train: 2018-08-04T23:26:10.687061: step 3449, loss 0.53069.
Train: 2018-08-04T23:26:10.936970: step 3450, loss 0.546621.
Test: 2018-08-04T23:26:12.186677: step 3450, loss 0.549542.
Train: 2018-08-04T23:26:12.421031: step 3451, loss 0.546511.
Train: 2018-08-04T23:26:12.670967: step 3452, loss 0.546378.
Train: 2018-08-04T23:26:12.920910: step 3453, loss 0.529915.
Train: 2018-08-04T23:26:13.170854: step 3454, loss 0.587186.
Train: 2018-08-04T23:26:13.420795: step 3455, loss 0.504864.
Train: 2018-08-04T23:26:13.670733: step 3456, loss 0.612112.
Train: 2018-08-04T23:26:13.920677: step 3457, loss 0.554167.
Train: 2018-08-04T23:26:14.170618: step 3458, loss 0.554149.
Train: 2018-08-04T23:26:14.420560: step 3459, loss 0.537439.
Train: 2018-08-04T23:26:14.670471: step 3460, loss 0.57077.
Test: 2018-08-04T23:26:15.904557: step 3460, loss 0.547256.
Train: 2018-08-04T23:26:16.154529: step 3461, loss 0.537288.
Train: 2018-08-04T23:26:16.420061: step 3462, loss 0.503637.
Train: 2018-08-04T23:26:16.685652: step 3463, loss 0.58767.
Train: 2018-08-04T23:26:16.935568: step 3464, loss 0.486423.
Train: 2018-08-04T23:26:17.169913: step 3465, loss 0.56236.
Train: 2018-08-04T23:26:17.419855: step 3466, loss 0.536865.
Train: 2018-08-04T23:26:17.669802: step 3467, loss 0.519748.
Train: 2018-08-04T23:26:17.919740: step 3468, loss 0.587957.
Train: 2018-08-04T23:26:18.169681: step 3469, loss 0.519516.
Train: 2018-08-04T23:26:18.419621: step 3470, loss 0.519394.
Test: 2018-08-04T23:26:19.684922: step 3470, loss 0.546564.
Train: 2018-08-04T23:26:19.966108: step 3471, loss 0.553732.
Train: 2018-08-04T23:26:20.216048: step 3472, loss 0.519153.
Train: 2018-08-04T23:26:20.403504: step 3473, loss 0.61777.
Train: 2018-08-04T23:26:20.669067: step 3474, loss 0.605672.
Train: 2018-08-04T23:26:20.903413: step 3475, loss 0.579685.
Train: 2018-08-04T23:26:21.168951: step 3476, loss 0.571003.
Train: 2018-08-04T23:26:21.434544: step 3477, loss 0.493101.
Train: 2018-08-04T23:26:21.668860: step 3478, loss 0.579674.
Train: 2018-08-04T23:26:21.918773: step 3479, loss 0.475837.
Train: 2018-08-04T23:26:22.168716: step 3480, loss 0.562356.
Test: 2018-08-04T23:26:23.418422: step 3480, loss 0.549271.
Train: 2018-08-04T23:26:23.652774: step 3481, loss 0.51905.
Train: 2018-08-04T23:26:23.902718: step 3482, loss 0.501643.
Train: 2018-08-04T23:26:24.152656: step 3483, loss 0.536307.
Train: 2018-08-04T23:26:24.402593: step 3484, loss 0.492701.
Train: 2018-08-04T23:26:24.652540: step 3485, loss 0.501224.
Train: 2018-08-04T23:26:24.902451: step 3486, loss 0.562393.
Train: 2018-08-04T23:26:25.152417: step 3487, loss 0.536012.
Train: 2018-08-04T23:26:25.402358: step 3488, loss 0.518362.
Train: 2018-08-04T23:26:25.652305: step 3489, loss 0.642026.
Train: 2018-08-04T23:26:25.902246: step 3490, loss 0.588975.
Test: 2018-08-04T23:26:27.151923: step 3490, loss 0.548484.
Train: 2018-08-04T23:26:27.401866: step 3491, loss 0.535912.
Train: 2018-08-04T23:26:27.651837: step 3492, loss 0.580141.
Train: 2018-08-04T23:26:27.901778: step 3493, loss 0.544762.
Train: 2018-08-04T23:26:28.136100: step 3494, loss 0.562439.
Train: 2018-08-04T23:26:28.386039: step 3495, loss 0.580046.
Train: 2018-08-04T23:26:28.635981: step 3496, loss 0.553595.
Train: 2018-08-04T23:26:28.885923: step 3497, loss 0.579953.
Train: 2018-08-04T23:26:29.135833: step 3498, loss 0.509823.
Train: 2018-08-04T23:26:29.385800: step 3499, loss 0.527394.
Train: 2018-08-04T23:26:29.635747: step 3500, loss 0.483717.
Test: 2018-08-04T23:26:30.885424: step 3500, loss 0.54737.
Train: 2018-08-04T23:26:31.713385: step 3501, loss 0.623544.
Train: 2018-08-04T23:26:31.963327: step 3502, loss 0.579829.
Train: 2018-08-04T23:26:32.213269: step 3503, loss 0.544921.
Train: 2018-08-04T23:26:32.463204: step 3504, loss 0.614565.
Train: 2018-08-04T23:26:32.716244: step 3505, loss 0.510276.
Train: 2018-08-04T23:26:32.960615: step 3506, loss 0.614321.
Train: 2018-08-04T23:26:33.206931: step 3507, loss 0.562326.
Train: 2018-08-04T23:26:33.452300: step 3508, loss 0.493443.
Train: 2018-08-04T23:26:33.696623: step 3509, loss 0.553726.
Train: 2018-08-04T23:26:33.949945: step 3510, loss 0.502239.
Test: 2018-08-04T23:26:35.217557: step 3510, loss 0.547801.
Train: 2018-08-04T23:26:35.464895: step 3511, loss 0.536591.
Train: 2018-08-04T23:26:35.716223: step 3512, loss 0.622405.
Train: 2018-08-04T23:26:35.967552: step 3513, loss 0.588045.
Train: 2018-08-04T23:26:36.207910: step 3514, loss 0.664951.
Train: 2018-08-04T23:26:36.447269: step 3515, loss 0.553804.
Train: 2018-08-04T23:26:36.695606: step 3516, loss 0.579316.
Train: 2018-08-04T23:26:36.943943: step 3517, loss 0.545488.
Train: 2018-08-04T23:26:37.183327: step 3518, loss 0.553977.
Train: 2018-08-04T23:26:37.426651: step 3519, loss 0.612734.
Train: 2018-08-04T23:26:37.670026: step 3520, loss 0.554063.
Test: 2018-08-04T23:26:38.913043: step 3520, loss 0.550075.
Train: 2018-08-04T23:26:39.148443: step 3521, loss 0.570742.
Train: 2018-08-04T23:26:39.387799: step 3522, loss 0.579056.
Train: 2018-08-04T23:26:39.629128: step 3523, loss 0.529469.
Train: 2018-08-04T23:26:39.872477: step 3524, loss 0.554278.
Train: 2018-08-04T23:26:40.117821: step 3525, loss 0.505013.
Train: 2018-08-04T23:26:40.361197: step 3526, loss 0.513283.
Train: 2018-08-04T23:26:40.604549: step 3527, loss 0.51328.
Train: 2018-08-04T23:26:40.853879: step 3528, loss 0.504933.
Train: 2018-08-04T23:26:41.100221: step 3529, loss 0.546003.
Train: 2018-08-04T23:26:41.345539: step 3530, loss 0.554194.
Test: 2018-08-04T23:26:42.590213: step 3530, loss 0.547089.
Train: 2018-08-04T23:26:42.826605: step 3531, loss 0.53758.
Train: 2018-08-04T23:26:43.068957: step 3532, loss 0.50424.
Train: 2018-08-04T23:26:43.309303: step 3533, loss 0.579124.
Train: 2018-08-04T23:26:43.549678: step 3534, loss 0.579136.
Train: 2018-08-04T23:26:43.793030: step 3535, loss 0.570764.
Train: 2018-08-04T23:26:44.046345: step 3536, loss 0.604394.
Train: 2018-08-04T23:26:44.296681: step 3537, loss 0.595977.
Train: 2018-08-04T23:26:44.543989: step 3538, loss 0.545573.
Train: 2018-08-04T23:26:44.792325: step 3539, loss 0.570796.
Train: 2018-08-04T23:26:45.036672: step 3540, loss 0.537226.
Test: 2018-08-04T23:26:46.283339: step 3540, loss 0.548005.
Train: 2018-08-04T23:26:46.522699: step 3541, loss 0.595891.
Train: 2018-08-04T23:26:46.765076: step 3542, loss 0.545649.
Train: 2018-08-04T23:26:47.009429: step 3543, loss 0.545663.
Train: 2018-08-04T23:26:47.253745: step 3544, loss 0.470368.
Train: 2018-08-04T23:26:47.498093: step 3545, loss 0.621082.
Train: 2018-08-04T23:26:47.753409: step 3546, loss 0.59591.
Train: 2018-08-04T23:26:47.998784: step 3547, loss 0.570773.
Train: 2018-08-04T23:26:48.256090: step 3548, loss 0.587491.
Train: 2018-08-04T23:26:48.504402: step 3549, loss 0.570773.
Train: 2018-08-04T23:26:48.749777: step 3550, loss 0.504099.
Test: 2018-08-04T23:26:50.008381: step 3550, loss 0.547376.
Train: 2018-08-04T23:26:50.242783: step 3551, loss 0.545764.
Train: 2018-08-04T23:26:50.485137: step 3552, loss 0.604045.
Train: 2018-08-04T23:26:50.729479: step 3553, loss 0.579091.
Train: 2018-08-04T23:26:50.970840: step 3554, loss 0.537546.
Train: 2018-08-04T23:26:51.218172: step 3555, loss 0.54591.
Train: 2018-08-04T23:26:51.462494: step 3556, loss 0.496139.
Train: 2018-08-04T23:26:51.709864: step 3557, loss 0.603967.
Train: 2018-08-04T23:26:51.955177: step 3558, loss 0.554152.
Train: 2018-08-04T23:26:52.201518: step 3559, loss 0.537589.
Train: 2018-08-04T23:26:52.441899: step 3560, loss 0.620574.
Test: 2018-08-04T23:26:53.685551: step 3560, loss 0.546711.
Train: 2018-08-04T23:26:53.926937: step 3561, loss 0.554164.
Train: 2018-08-04T23:26:54.169285: step 3562, loss 0.545905.
Train: 2018-08-04T23:26:54.418626: step 3563, loss 0.595568.
Train: 2018-08-04T23:26:54.664970: step 3564, loss 0.504621.
Train: 2018-08-04T23:26:54.914293: step 3565, loss 0.603779.
Train: 2018-08-04T23:26:55.162636: step 3566, loss 0.587319.
Train: 2018-08-04T23:26:55.413957: step 3567, loss 0.653259.
Train: 2018-08-04T23:26:55.661294: step 3568, loss 0.595399.
Train: 2018-08-04T23:26:55.908640: step 3569, loss 0.554379.
Train: 2018-08-04T23:26:56.152429: step 3570, loss 0.587079.
Test: 2018-08-04T23:26:57.402105: step 3570, loss 0.547949.
Train: 2018-08-04T23:26:57.636428: step 3571, loss 0.611432.
Train: 2018-08-04T23:26:57.886396: step 3572, loss 0.497916.
Train: 2018-08-04T23:26:58.136338: step 3573, loss 0.514278.
Train: 2018-08-04T23:26:58.386279: step 3574, loss 0.514355.
Train: 2018-08-04T23:26:58.636221: step 3575, loss 0.611124.
Train: 2018-08-04T23:26:58.886162: step 3576, loss 0.586947.
Train: 2018-08-04T23:26:59.151696: step 3577, loss 0.578865.
Train: 2018-08-04T23:26:59.401668: step 3578, loss 0.586883.
Train: 2018-08-04T23:26:59.651610: step 3579, loss 0.578848.
Train: 2018-08-04T23:26:59.901551: step 3580, loss 0.554795.
Test: 2018-08-04T23:27:01.158738: step 3580, loss 0.549129.
Train: 2018-08-04T23:27:01.396105: step 3581, loss 0.522829.
Train: 2018-08-04T23:27:01.644471: step 3582, loss 0.538858.
Train: 2018-08-04T23:27:01.893774: step 3583, loss 0.546813.
Train: 2018-08-04T23:27:02.150088: step 3584, loss 0.64301.
Train: 2018-08-04T23:27:02.395432: step 3585, loss 0.586856.
Train: 2018-08-04T23:27:02.636812: step 3586, loss 0.586867.
Train: 2018-08-04T23:27:02.893133: step 3587, loss 0.554933.
Train: 2018-08-04T23:27:03.139443: step 3588, loss 0.53101.
Train: 2018-08-04T23:27:03.381826: step 3589, loss 0.570794.
Train: 2018-08-04T23:27:03.637114: step 3590, loss 0.586782.
Test: 2018-08-04T23:27:04.877796: step 3590, loss 0.549095.
Train: 2018-08-04T23:27:05.116190: step 3591, loss 0.547051.
Train: 2018-08-04T23:27:05.362500: step 3592, loss 0.586757.
Train: 2018-08-04T23:27:05.608867: step 3593, loss 0.563087.
Train: 2018-08-04T23:27:05.862196: step 3594, loss 0.650523.
Train: 2018-08-04T23:27:06.108515: step 3595, loss 0.570955.
Train: 2018-08-04T23:27:06.354849: step 3596, loss 0.594616.
Train: 2018-08-04T23:27:06.605180: step 3597, loss 0.578953.
Train: 2018-08-04T23:27:06.851521: step 3598, loss 0.602453.
Train: 2018-08-04T23:27:07.104844: step 3599, loss 0.500347.
Train: 2018-08-04T23:27:07.361189: step 3600, loss 0.578852.
Test: 2018-08-04T23:27:08.608823: step 3600, loss 0.550689.
Train: 2018-08-04T23:27:09.461633: step 3601, loss 0.563186.
Train: 2018-08-04T23:27:09.708005: step 3602, loss 0.563209.
Train: 2018-08-04T23:27:09.956342: step 3603, loss 0.586679.
Train: 2018-08-04T23:27:10.203650: step 3604, loss 0.555362.
Train: 2018-08-04T23:27:10.448021: step 3605, loss 0.547554.
Train: 2018-08-04T23:27:10.697331: step 3606, loss 0.563284.
Train: 2018-08-04T23:27:10.942705: step 3607, loss 0.563294.
Train: 2018-08-04T23:27:11.189047: step 3608, loss 0.547634.
Train: 2018-08-04T23:27:11.432366: step 3609, loss 0.563308.
Train: 2018-08-04T23:27:11.676711: step 3610, loss 0.508249.
Test: 2018-08-04T23:27:12.920386: step 3610, loss 0.5498.
Train: 2018-08-04T23:27:13.155758: step 3611, loss 0.555279.
Train: 2018-08-04T23:27:13.403128: step 3612, loss 0.578801.
Train: 2018-08-04T23:27:13.647475: step 3613, loss 0.578765.
Train: 2018-08-04T23:27:13.911736: step 3614, loss 0.578918.
Train: 2018-08-04T23:27:14.159075: step 3615, loss 0.491447.
Train: 2018-08-04T23:27:14.404419: step 3616, loss 0.539095.
Train: 2018-08-04T23:27:14.645776: step 3617, loss 0.634758.
Train: 2018-08-04T23:27:14.898099: step 3618, loss 0.586892.
Train: 2018-08-04T23:27:15.145438: step 3619, loss 0.554883.
Train: 2018-08-04T23:27:15.386793: step 3620, loss 0.570749.
Test: 2018-08-04T23:27:16.627476: step 3620, loss 0.54937.
Train: 2018-08-04T23:27:16.860884: step 3621, loss 0.635049.
Train: 2018-08-04T23:27:17.103232: step 3622, loss 0.514461.
Train: 2018-08-04T23:27:17.354567: step 3623, loss 0.651064.
Train: 2018-08-04T23:27:17.541060: step 3624, loss 0.477186.
Train: 2018-08-04T23:27:17.783418: step 3625, loss 0.506704.
Train: 2018-08-04T23:27:18.030757: step 3626, loss 0.579116.
Train: 2018-08-04T23:27:18.283050: step 3627, loss 0.60312.
Train: 2018-08-04T23:27:18.539368: step 3628, loss 0.57873.
Train: 2018-08-04T23:27:18.780753: step 3629, loss 0.578812.
Train: 2018-08-04T23:27:19.027062: step 3630, loss 0.570688.
Test: 2018-08-04T23:27:20.283703: step 3630, loss 0.550032.
Train: 2018-08-04T23:27:20.517110: step 3631, loss 0.579082.
Train: 2018-08-04T23:27:20.767410: step 3632, loss 0.554879.
Train: 2018-08-04T23:27:21.010760: step 3633, loss 0.602922.
Train: 2018-08-04T23:27:21.259122: step 3634, loss 0.562489.
Train: 2018-08-04T23:27:21.504439: step 3635, loss 0.563156.
Train: 2018-08-04T23:27:21.748829: step 3636, loss 0.49865.
Train: 2018-08-04T23:27:21.992167: step 3637, loss 0.571.
Train: 2018-08-04T23:27:22.237480: step 3638, loss 0.546844.
Train: 2018-08-04T23:27:22.479863: step 3639, loss 0.54662.
Train: 2018-08-04T23:27:22.724212: step 3640, loss 0.603126.
Test: 2018-08-04T23:27:23.967854: step 3640, loss 0.549294.
Train: 2018-08-04T23:27:24.202253: step 3641, loss 0.579108.
Train: 2018-08-04T23:27:24.451562: step 3642, loss 0.538469.
Train: 2018-08-04T23:27:24.702890: step 3643, loss 0.522553.
Train: 2018-08-04T23:27:24.950261: step 3644, loss 0.514267.
Train: 2018-08-04T23:27:25.193579: step 3645, loss 0.562792.
Train: 2018-08-04T23:27:25.445934: step 3646, loss 0.586992.
Train: 2018-08-04T23:27:25.689299: step 3647, loss 0.570781.
Train: 2018-08-04T23:27:25.934609: step 3648, loss 0.521909.
Train: 2018-08-04T23:27:26.169035: step 3649, loss 0.570809.
Train: 2018-08-04T23:27:26.418947: step 3650, loss 0.505583.
Test: 2018-08-04T23:27:27.668654: step 3650, loss 0.548315.
Train: 2018-08-04T23:27:27.902976: step 3651, loss 0.578817.
Train: 2018-08-04T23:27:28.152946: step 3652, loss 0.48039.
Train: 2018-08-04T23:27:28.402883: step 3653, loss 0.50476.
Train: 2018-08-04T23:27:28.652831: step 3654, loss 0.612059.
Train: 2018-08-04T23:27:28.902771: step 3655, loss 0.612344.
Train: 2018-08-04T23:27:29.152712: step 3656, loss 0.587367.
Train: 2018-08-04T23:27:29.402656: step 3657, loss 0.537321.
Train: 2018-08-04T23:27:29.652594: step 3658, loss 0.596017.
Train: 2018-08-04T23:27:29.902536: step 3659, loss 0.570765.
Train: 2018-08-04T23:27:30.152447: step 3660, loss 0.579148.
Test: 2018-08-04T23:27:31.402154: step 3660, loss 0.549287.
Train: 2018-08-04T23:27:31.636476: step 3661, loss 0.579245.
Train: 2018-08-04T23:27:31.886416: step 3662, loss 0.57899.
Train: 2018-08-04T23:27:32.136359: step 3663, loss 0.429404.
Train: 2018-08-04T23:27:32.386330: step 3664, loss 0.603962.
Train: 2018-08-04T23:27:32.636242: step 3665, loss 0.512427.
Train: 2018-08-04T23:27:32.886183: step 3666, loss 0.554114.
Train: 2018-08-04T23:27:33.136123: step 3667, loss 0.528929.
Train: 2018-08-04T23:27:33.386066: step 3668, loss 0.579244.
Train: 2018-08-04T23:27:33.636007: step 3669, loss 0.554005.
Train: 2018-08-04T23:27:33.885950: step 3670, loss 0.621074.
Test: 2018-08-04T23:27:35.135655: step 3670, loss 0.548789.
Train: 2018-08-04T23:27:35.370002: step 3671, loss 0.621099.
Train: 2018-08-04T23:27:35.619919: step 3672, loss 0.612615.
Train: 2018-08-04T23:27:35.869889: step 3673, loss 0.570625.
Train: 2018-08-04T23:27:36.119830: step 3674, loss 0.570887.
Train: 2018-08-04T23:27:36.369767: step 3675, loss 0.603789.
Train: 2018-08-04T23:27:36.635335: step 3676, loss 0.570867.
Train: 2018-08-04T23:27:36.869625: step 3677, loss 0.53803.
Train: 2018-08-04T23:27:37.119565: step 3678, loss 0.513456.
Train: 2018-08-04T23:27:37.369507: step 3679, loss 0.644392.
Train: 2018-08-04T23:27:37.619475: step 3680, loss 0.57075.
Test: 2018-08-04T23:27:38.875324: step 3680, loss 0.548513.
Train: 2018-08-04T23:27:39.108701: step 3681, loss 0.513933.
Train: 2018-08-04T23:27:39.353078: step 3682, loss 0.59502.
Train: 2018-08-04T23:27:39.607368: step 3683, loss 0.586966.
Train: 2018-08-04T23:27:39.847759: step 3684, loss 0.627277.
Train: 2018-08-04T23:27:40.095089: step 3685, loss 0.53058.
Train: 2018-08-04T23:27:40.339411: step 3686, loss 0.546717.
Train: 2018-08-04T23:27:40.587771: step 3687, loss 0.602827.
Train: 2018-08-04T23:27:40.837079: step 3688, loss 0.506915.
Train: 2018-08-04T23:27:41.075443: step 3689, loss 0.57093.
Train: 2018-08-04T23:27:41.320818: step 3690, loss 0.570876.
Test: 2018-08-04T23:27:42.564462: step 3690, loss 0.549236.
Train: 2018-08-04T23:27:42.799482: step 3691, loss 0.554945.
Train: 2018-08-04T23:27:43.042866: step 3692, loss 0.554887.
Train: 2018-08-04T23:27:43.284219: step 3693, loss 0.50708.
Train: 2018-08-04T23:27:43.535541: step 3694, loss 0.602863.
Train: 2018-08-04T23:27:43.780859: step 3695, loss 0.554845.
Train: 2018-08-04T23:27:44.026236: step 3696, loss 0.514921.
Train: 2018-08-04T23:27:44.271546: step 3697, loss 0.602814.
Train: 2018-08-04T23:27:44.519884: step 3698, loss 0.490687.
Train: 2018-08-04T23:27:44.767253: step 3699, loss 0.546782.
Train: 2018-08-04T23:27:45.022538: step 3700, loss 0.5627.
Test: 2018-08-04T23:27:46.284166: step 3700, loss 0.548284.
Train: 2018-08-04T23:27:47.195736: step 3701, loss 0.570647.
Train: 2018-08-04T23:27:47.441079: step 3702, loss 0.570879.
Train: 2018-08-04T23:27:47.687421: step 3703, loss 0.619618.
Train: 2018-08-04T23:27:47.924820: step 3704, loss 0.546342.
Train: 2018-08-04T23:27:48.163150: step 3705, loss 0.546366.
Train: 2018-08-04T23:27:48.417469: step 3706, loss 0.57086.
Train: 2018-08-04T23:27:48.663811: step 3707, loss 0.586927.
Train: 2018-08-04T23:27:48.904203: step 3708, loss 0.578929.
Train: 2018-08-04T23:27:49.149538: step 3709, loss 0.562914.
Train: 2018-08-04T23:27:49.390892: step 3710, loss 0.627644.
Test: 2018-08-04T23:27:50.642522: step 3710, loss 0.5504.
Train: 2018-08-04T23:27:50.882913: step 3711, loss 0.554809.
Train: 2018-08-04T23:27:51.123237: step 3712, loss 0.554372.
Train: 2018-08-04T23:27:51.369578: step 3713, loss 0.554592.
Train: 2018-08-04T23:27:51.612927: step 3714, loss 0.619575.
Train: 2018-08-04T23:27:51.859295: step 3715, loss 0.595154.
Train: 2018-08-04T23:27:52.105610: step 3716, loss 0.498233.
Train: 2018-08-04T23:27:52.353957: step 3717, loss 0.62718.
Train: 2018-08-04T23:27:52.597296: step 3718, loss 0.603083.
Train: 2018-08-04T23:27:52.849622: step 3719, loss 0.450748.
Train: 2018-08-04T23:27:53.091974: step 3720, loss 0.643063.
Test: 2018-08-04T23:27:54.341632: step 3720, loss 0.548963.
Train: 2018-08-04T23:27:54.578031: step 3721, loss 0.554777.
Train: 2018-08-04T23:27:54.821351: step 3722, loss 0.52291.
Train: 2018-08-04T23:27:55.079694: step 3723, loss 0.562842.
Train: 2018-08-04T23:27:55.324006: step 3724, loss 0.562891.
Train: 2018-08-04T23:27:55.569351: step 3725, loss 0.522939.
Train: 2018-08-04T23:27:55.816691: step 3726, loss 0.514836.
Train: 2018-08-04T23:27:56.059405: step 3727, loss 0.578849.
Train: 2018-08-04T23:27:56.293755: step 3728, loss 0.603039.
Train: 2018-08-04T23:27:56.543696: step 3729, loss 0.498522.
Train: 2018-08-04T23:27:56.793639: step 3730, loss 0.603069.
Test: 2018-08-04T23:27:58.043315: step 3730, loss 0.549069.
Train: 2018-08-04T23:27:58.277666: step 3731, loss 0.554633.
Train: 2018-08-04T23:27:58.543229: step 3732, loss 0.554659.
Train: 2018-08-04T23:27:58.793138: step 3733, loss 0.457448.
Train: 2018-08-04T23:27:59.043080: step 3734, loss 0.562588.
Train: 2018-08-04T23:27:59.293052: step 3735, loss 0.59523.
Train: 2018-08-04T23:27:59.542995: step 3736, loss 0.521692.
Train: 2018-08-04T23:27:59.792905: step 3737, loss 0.603565.
Train: 2018-08-04T23:28:00.042847: step 3738, loss 0.603699.
Train: 2018-08-04T23:28:00.292812: step 3739, loss 0.62001.
Train: 2018-08-04T23:28:00.542759: step 3740, loss 0.496814.
Test: 2018-08-04T23:28:01.792437: step 3740, loss 0.548428.
Train: 2018-08-04T23:28:02.026787: step 3741, loss 0.578985.
Train: 2018-08-04T23:28:02.276723: step 3742, loss 0.537929.
Train: 2018-08-04T23:28:02.526639: step 3743, loss 0.529628.
Train: 2018-08-04T23:28:02.776581: step 3744, loss 0.570834.
Train: 2018-08-04T23:28:03.026547: step 3745, loss 0.529544.
Train: 2018-08-04T23:28:03.276463: step 3746, loss 0.562449.
Train: 2018-08-04T23:28:03.526430: step 3747, loss 0.628614.
Train: 2018-08-04T23:28:03.791998: step 3748, loss 0.537745.
Train: 2018-08-04T23:28:04.041910: step 3749, loss 0.521086.
Train: 2018-08-04T23:28:04.291851: step 3750, loss 0.612026.
Test: 2018-08-04T23:28:05.541559: step 3750, loss 0.548845.
Train: 2018-08-04T23:28:05.791500: step 3751, loss 0.570619.
Train: 2018-08-04T23:28:06.041467: step 3752, loss 0.570724.
Train: 2018-08-04T23:28:06.291419: step 3753, loss 0.562555.
Train: 2018-08-04T23:28:06.541355: step 3754, loss 0.603711.
Train: 2018-08-04T23:28:06.791296: step 3755, loss 0.554315.
Train: 2018-08-04T23:28:07.041238: step 3756, loss 0.513089.
Train: 2018-08-04T23:28:07.291149: step 3757, loss 0.562514.
Train: 2018-08-04T23:28:07.541120: step 3758, loss 0.529534.
Train: 2018-08-04T23:28:07.791062: step 3759, loss 0.603759.
Train: 2018-08-04T23:28:08.040999: step 3760, loss 0.554306.
Test: 2018-08-04T23:28:09.306302: step 3760, loss 0.547824.
Train: 2018-08-04T23:28:09.556245: step 3761, loss 0.513289.
Train: 2018-08-04T23:28:09.790588: step 3762, loss 0.562615.
Train: 2018-08-04T23:28:10.040529: step 3763, loss 0.496573.
Train: 2018-08-04T23:28:10.290446: step 3764, loss 0.562457.
Train: 2018-08-04T23:28:10.540422: step 3765, loss 0.570817.
Train: 2018-08-04T23:28:10.790328: step 3766, loss 0.661987.
Train: 2018-08-04T23:28:11.040302: step 3767, loss 0.554236.
Train: 2018-08-04T23:28:11.305859: step 3768, loss 0.53748.
Train: 2018-08-04T23:28:11.587055: step 3769, loss 0.488083.
Train: 2018-08-04T23:28:11.852582: step 3770, loss 0.537681.
Test: 2018-08-04T23:28:13.102288: step 3770, loss 0.547852.
Train: 2018-08-04T23:28:13.352255: step 3771, loss 0.56242.
Train: 2018-08-04T23:28:13.602196: step 3772, loss 0.562476.
Train: 2018-08-04T23:28:13.866041: step 3773, loss 0.562371.
Train: 2018-08-04T23:28:14.109406: step 3774, loss 0.554235.
Train: 2018-08-04T23:28:14.305875: step 3775, loss 0.633432.
Train: 2018-08-04T23:28:14.556212: step 3776, loss 0.637365.
Train: 2018-08-04T23:28:14.805515: step 3777, loss 0.562524.
Train: 2018-08-04T23:28:15.053851: step 3778, loss 0.545921.
Train: 2018-08-04T23:28:15.299195: step 3779, loss 0.562526.
Train: 2018-08-04T23:28:15.548529: step 3780, loss 0.529432.
Test: 2018-08-04T23:28:16.856033: step 3780, loss 0.548559.
Train: 2018-08-04T23:28:17.099413: step 3781, loss 0.562597.
Train: 2018-08-04T23:28:17.354726: step 3782, loss 0.595477.
Train: 2018-08-04T23:28:17.618021: step 3783, loss 0.562543.
Train: 2018-08-04T23:28:17.864338: step 3784, loss 0.50514.
Train: 2018-08-04T23:28:18.111702: step 3785, loss 0.61175.
Train: 2018-08-04T23:28:18.369985: step 3786, loss 0.505209.
Train: 2018-08-04T23:28:18.637270: step 3787, loss 0.570756.
Train: 2018-08-04T23:28:18.877629: step 3788, loss 0.529837.
Train: 2018-08-04T23:28:19.116988: step 3789, loss 0.521452.
Train: 2018-08-04T23:28:19.374301: step 3790, loss 0.529763.
Test: 2018-08-04T23:28:20.636925: step 3790, loss 0.548199.
Train: 2018-08-04T23:28:20.942138: step 3791, loss 0.56257.
Train: 2018-08-04T23:28:21.187453: step 3792, loss 0.54603.
Train: 2018-08-04T23:28:21.434823: step 3793, loss 0.554222.
Train: 2018-08-04T23:28:21.685123: step 3794, loss 0.562481.
Train: 2018-08-04T23:28:21.937479: step 3795, loss 0.562481.
Train: 2018-08-04T23:28:22.195759: step 3796, loss 0.545893.
Train: 2018-08-04T23:28:22.445123: step 3797, loss 0.595759.
Train: 2018-08-04T23:28:22.692461: step 3798, loss 0.487543.
Train: 2018-08-04T23:28:22.935779: step 3799, loss 0.595846.
Train: 2018-08-04T23:28:23.178131: step 3800, loss 0.59584.
Test: 2018-08-04T23:28:24.440757: step 3800, loss 0.549629.
Train: 2018-08-04T23:28:25.311715: step 3801, loss 0.545705.
Train: 2018-08-04T23:28:25.559102: step 3802, loss 0.595832.
Train: 2018-08-04T23:28:25.803400: step 3803, loss 0.545776.
Train: 2018-08-04T23:28:26.044596: step 3804, loss 0.595724.
Train: 2018-08-04T23:28:26.294539: step 3805, loss 0.645696.
Train: 2018-08-04T23:28:26.544475: step 3806, loss 0.570732.
Train: 2018-08-04T23:28:26.794422: step 3807, loss 0.578974.
Train: 2018-08-04T23:28:27.044363: step 3808, loss 0.603778.
Train: 2018-08-04T23:28:27.294302: step 3809, loss 0.463938.
Train: 2018-08-04T23:28:27.544222: step 3810, loss 0.636464.
Test: 2018-08-04T23:28:28.809546: step 3810, loss 0.548725.
Train: 2018-08-04T23:28:29.043896: step 3811, loss 0.554395.
Train: 2018-08-04T23:28:29.293836: step 3812, loss 0.546248.
Train: 2018-08-04T23:28:29.543777: step 3813, loss 0.489275.
Train: 2018-08-04T23:28:29.793718: step 3814, loss 0.538198.
Train: 2018-08-04T23:28:30.043630: step 3815, loss 0.627895.
Train: 2018-08-04T23:28:30.293573: step 3816, loss 0.456727.
Train: 2018-08-04T23:28:30.543514: step 3817, loss 0.562575.
Train: 2018-08-04T23:28:30.793485: step 3818, loss 0.562598.
Train: 2018-08-04T23:28:31.043396: step 3819, loss 0.497118.
Train: 2018-08-04T23:28:31.293368: step 3820, loss 0.529771.
Test: 2018-08-04T23:28:32.543045: step 3820, loss 0.549345.
Train: 2018-08-04T23:28:32.777396: step 3821, loss 0.677806.
Train: 2018-08-04T23:28:33.027336: step 3822, loss 0.554262.
Train: 2018-08-04T23:28:33.277278: step 3823, loss 0.562542.
Train: 2018-08-04T23:28:33.527189: step 3824, loss 0.537741.
Train: 2018-08-04T23:28:33.777162: step 3825, loss 0.570732.
Train: 2018-08-04T23:28:34.027102: step 3826, loss 0.521245.
Train: 2018-08-04T23:28:34.277046: step 3827, loss 0.521248.
Train: 2018-08-04T23:28:34.526985: step 3828, loss 0.570766.
Train: 2018-08-04T23:28:34.776911: step 3829, loss 0.496129.
Train: 2018-08-04T23:28:35.026863: step 3830, loss 0.504319.
Test: 2018-08-04T23:28:36.292167: step 3830, loss 0.548494.
Train: 2018-08-04T23:28:36.526513: step 3831, loss 0.587416.
Train: 2018-08-04T23:28:36.776459: step 3832, loss 0.528988.
Train: 2018-08-04T23:28:37.026369: step 3833, loss 0.545674.
Train: 2018-08-04T23:28:37.276344: step 3834, loss 0.63818.
Train: 2018-08-04T23:28:37.526252: step 3835, loss 0.562283.
Train: 2018-08-04T23:28:37.776220: step 3836, loss 0.579179.
Train: 2018-08-04T23:28:38.026166: step 3837, loss 0.604562.
Train: 2018-08-04T23:28:38.276110: step 3838, loss 0.545602.
Train: 2018-08-04T23:28:38.526049: step 3839, loss 0.629614.
Train: 2018-08-04T23:28:38.775991: step 3840, loss 0.629451.
Test: 2018-08-04T23:28:40.041289: step 3840, loss 0.548651.
Train: 2018-08-04T23:28:40.275610: step 3841, loss 0.570762.
Train: 2018-08-04T23:28:40.541172: step 3842, loss 0.595765.
Train: 2018-08-04T23:28:40.806734: step 3843, loss 0.520977.
Train: 2018-08-04T23:28:41.056676: step 3844, loss 0.537716.
Train: 2018-08-04T23:28:41.322240: step 3845, loss 0.570787.
Train: 2018-08-04T23:28:41.556590: step 3846, loss 0.554338.
Train: 2018-08-04T23:28:41.806532: step 3847, loss 0.48865.
Train: 2018-08-04T23:28:42.056472: step 3848, loss 0.644651.
Train: 2018-08-04T23:28:42.306383: step 3849, loss 0.505215.
Train: 2018-08-04T23:28:42.556325: step 3850, loss 0.578963.
Test: 2018-08-04T23:28:43.822663: step 3850, loss 0.548347.
Train: 2018-08-04T23:28:44.059062: step 3851, loss 0.554421.
Train: 2018-08-04T23:28:44.301414: step 3852, loss 0.578905.
Train: 2018-08-04T23:28:44.551746: step 3853, loss 0.627994.
Train: 2018-08-04T23:28:44.800050: step 3854, loss 0.603348.
Train: 2018-08-04T23:28:45.047423: step 3855, loss 0.619564.
Train: 2018-08-04T23:28:45.289741: step 3856, loss 0.578901.
Train: 2018-08-04T23:28:45.537111: step 3857, loss 0.546661.
Train: 2018-08-04T23:28:45.781428: step 3858, loss 0.586929.
Train: 2018-08-04T23:28:46.027769: step 3859, loss 0.594861.
Train: 2018-08-04T23:28:46.276104: step 3860, loss 0.586843.
Test: 2018-08-04T23:28:47.515790: step 3860, loss 0.550981.
Train: 2018-08-04T23:28:47.753953: step 3861, loss 0.539143.
Train: 2018-08-04T23:28:47.995314: step 3862, loss 0.460022.
Train: 2018-08-04T23:28:48.239660: step 3863, loss 0.634326.
Train: 2018-08-04T23:28:48.487965: step 3864, loss 0.499732.
Train: 2018-08-04T23:28:48.731350: step 3865, loss 0.586773.
Train: 2018-08-04T23:28:48.976659: step 3866, loss 0.54719.
Train: 2018-08-04T23:28:49.218049: step 3867, loss 0.54719.
Train: 2018-08-04T23:28:49.462369: step 3868, loss 0.610561.
Train: 2018-08-04T23:28:49.706708: step 3869, loss 0.570942.
Train: 2018-08-04T23:28:49.953080: step 3870, loss 0.507524.
Test: 2018-08-04T23:28:51.222654: step 3870, loss 0.550278.
Train: 2018-08-04T23:28:51.459024: step 3871, loss 0.523298.
Train: 2018-08-04T23:28:51.704368: step 3872, loss 0.515206.
Train: 2018-08-04T23:28:51.956692: step 3873, loss 0.602799.
Train: 2018-08-04T23:28:52.207024: step 3874, loss 0.514841.
Train: 2018-08-04T23:28:52.449411: step 3875, loss 0.570832.
Train: 2018-08-04T23:28:52.695748: step 3876, loss 0.62715.
Train: 2018-08-04T23:28:52.945050: step 3877, loss 0.570819.
Train: 2018-08-04T23:28:53.195382: step 3878, loss 0.595031.
Train: 2018-08-04T23:28:53.439729: step 3879, loss 0.619213.
Train: 2018-08-04T23:28:53.684100: step 3880, loss 0.562751.
Test: 2018-08-04T23:28:54.957670: step 3880, loss 0.548972.
Train: 2018-08-04T23:28:55.199025: step 3881, loss 0.586895.
Train: 2018-08-04T23:28:55.448385: step 3882, loss 0.578867.
Train: 2018-08-04T23:28:55.696730: step 3883, loss 0.538706.
Train: 2018-08-04T23:28:55.944064: step 3884, loss 0.554838.
Train: 2018-08-04T23:28:56.170459: step 3885, loss 0.562834.
Train: 2018-08-04T23:28:56.436016: step 3886, loss 0.570856.
Train: 2018-08-04T23:28:56.685962: step 3887, loss 0.458725.
Train: 2018-08-04T23:28:56.951521: step 3888, loss 0.554755.
Train: 2018-08-04T23:28:57.185845: step 3889, loss 0.570859.
Train: 2018-08-04T23:28:57.435786: step 3890, loss 0.578877.
Test: 2018-08-04T23:28:58.685462: step 3890, loss 0.549068.
Train: 2018-08-04T23:28:58.919784: step 3891, loss 0.586879.
Train: 2018-08-04T23:28:59.169756: step 3892, loss 0.554628.
Train: 2018-08-04T23:28:59.419692: step 3893, loss 0.546475.
Train: 2018-08-04T23:28:59.669607: step 3894, loss 0.546461.
Train: 2018-08-04T23:28:59.919548: step 3895, loss 0.513958.
Train: 2018-08-04T23:29:00.169522: step 3896, loss 0.644161.
Train: 2018-08-04T23:29:00.419458: step 3897, loss 0.595173.
Train: 2018-08-04T23:29:00.669405: step 3898, loss 0.546355.
Train: 2018-08-04T23:29:00.919346: step 3899, loss 0.464916.
Train: 2018-08-04T23:29:01.169289: step 3900, loss 0.570794.
Test: 2018-08-04T23:29:02.418965: step 3900, loss 0.54894.
Train: 2018-08-04T23:29:03.325032: step 3901, loss 0.57071.
Train: 2018-08-04T23:29:03.574973: step 3902, loss 0.513475.
Train: 2018-08-04T23:29:03.824914: step 3903, loss 0.603506.
Train: 2018-08-04T23:29:04.074856: step 3904, loss 0.578889.
Train: 2018-08-04T23:29:04.324797: step 3905, loss 0.579017.
Train: 2018-08-04T23:29:04.574739: step 3906, loss 0.578958.
Train: 2018-08-04T23:29:04.824681: step 3907, loss 0.603538.
Train: 2018-08-04T23:29:05.074622: step 3908, loss 0.545916.
Train: 2018-08-04T23:29:05.324534: step 3909, loss 0.611938.
Train: 2018-08-04T23:29:05.574500: step 3910, loss 0.579081.
Test: 2018-08-04T23:29:06.824182: step 3910, loss 0.549291.
Train: 2018-08-04T23:29:07.074154: step 3911, loss 0.52166.
Train: 2018-08-04T23:29:07.324064: step 3912, loss 0.521657.
Train: 2018-08-04T23:29:07.574036: step 3913, loss 0.546357.
Train: 2018-08-04T23:29:07.823947: step 3914, loss 0.521811.
Train: 2018-08-04T23:29:08.089535: step 3915, loss 0.488889.
Train: 2018-08-04T23:29:08.339482: step 3916, loss 0.570923.
Train: 2018-08-04T23:29:08.589424: step 3917, loss 0.628292.
Train: 2018-08-04T23:29:08.839365: step 3918, loss 0.529546.
Train: 2018-08-04T23:29:09.089306: step 3919, loss 0.603796.
Train: 2018-08-04T23:29:09.339248: step 3920, loss 0.645185.
Test: 2018-08-04T23:29:10.573305: step 3920, loss 0.549336.
Train: 2018-08-04T23:29:10.823276: step 3921, loss 0.620194.
Train: 2018-08-04T23:29:11.073213: step 3922, loss 0.554224.
Train: 2018-08-04T23:29:11.323159: step 3923, loss 0.497128.
Train: 2018-08-04T23:29:11.573071: step 3924, loss 0.50544.
Train: 2018-08-04T23:29:11.823036: step 3925, loss 0.47259.
Train: 2018-08-04T23:29:12.026113: step 3926, loss 0.614932.
Train: 2018-08-04T23:29:12.276067: step 3927, loss 0.505188.
Train: 2018-08-04T23:29:12.526003: step 3928, loss 0.546048.
Train: 2018-08-04T23:29:12.775914: step 3929, loss 0.504554.
Train: 2018-08-04T23:29:13.025886: step 3930, loss 0.595432.
Test: 2018-08-04T23:29:14.291183: step 3930, loss 0.548407.
Train: 2018-08-04T23:29:14.541152: step 3931, loss 0.595516.
Train: 2018-08-04T23:29:14.791093: step 3932, loss 0.562571.
Train: 2018-08-04T23:29:15.041007: step 3933, loss 0.562442.
Train: 2018-08-04T23:29:15.290981: step 3934, loss 0.587611.
Train: 2018-08-04T23:29:15.540896: step 3935, loss 0.654206.
Train: 2018-08-04T23:29:15.790832: step 3936, loss 0.521054.
Train: 2018-08-04T23:29:16.040805: step 3937, loss 0.562603.
Train: 2018-08-04T23:29:16.290746: step 3938, loss 0.562527.
Train: 2018-08-04T23:29:16.540688: step 3939, loss 0.670135.
Train: 2018-08-04T23:29:16.790598: step 3940, loss 0.661431.
Test: 2018-08-04T23:29:18.040306: step 3940, loss 0.548133.
Train: 2018-08-04T23:29:18.274663: step 3941, loss 0.537967.
Train: 2018-08-04T23:29:18.524599: step 3942, loss 0.603355.
Train: 2018-08-04T23:29:18.774540: step 3943, loss 0.546573.
Train: 2018-08-04T23:29:19.040103: step 3944, loss 0.611135.
Train: 2018-08-04T23:29:19.290045: step 3945, loss 0.586949.
Train: 2018-08-04T23:29:19.539985: step 3946, loss 0.554845.
Train: 2018-08-04T23:29:19.789895: step 3947, loss 0.53905.
Train: 2018-08-04T23:29:20.039868: step 3948, loss 0.594705.
Train: 2018-08-04T23:29:20.289810: step 3949, loss 0.563021.
Train: 2018-08-04T23:29:20.539747: step 3950, loss 0.539433.
Test: 2018-08-04T23:29:21.789427: step 3950, loss 0.551236.
Train: 2018-08-04T23:29:22.023780: step 3951, loss 0.570979.
Train: 2018-08-04T23:29:22.273721: step 3952, loss 0.578857.
Train: 2018-08-04T23:29:22.523663: step 3953, loss 0.594578.
Train: 2018-08-04T23:29:22.773603: step 3954, loss 0.492743.
Train: 2018-08-04T23:29:23.023538: step 3955, loss 0.555385.
Train: 2018-08-04T23:29:23.273454: step 3956, loss 0.618058.
Train: 2018-08-04T23:29:23.523428: step 3957, loss 0.618032.
Train: 2018-08-04T23:29:23.773370: step 3958, loss 0.594519.
Train: 2018-08-04T23:29:24.023308: step 3959, loss 0.493113.
Train: 2018-08-04T23:29:24.273252: step 3960, loss 0.63348.
Test: 2018-08-04T23:29:25.522930: step 3960, loss 0.551396.
Train: 2018-08-04T23:29:25.757248: step 3961, loss 0.539962.
Train: 2018-08-04T23:29:26.007220: step 3962, loss 0.571116.
Train: 2018-08-04T23:29:26.257162: step 3963, loss 0.672253.
Train: 2018-08-04T23:29:26.507072: step 3964, loss 0.571156.
Train: 2018-08-04T23:29:26.757014: step 3965, loss 0.509295.
Train: 2018-08-04T23:29:27.006988: step 3966, loss 0.462952.
Train: 2018-08-04T23:29:27.256927: step 3967, loss 0.594417.
Train: 2018-08-04T23:29:27.506838: step 3968, loss 0.54787.
Train: 2018-08-04T23:29:27.756810: step 3969, loss 0.501132.
Train: 2018-08-04T23:29:28.006747: step 3970, loss 0.500827.
Test: 2018-08-04T23:29:29.256430: step 3970, loss 0.549356.
Train: 2018-08-04T23:29:29.490779: step 3971, loss 0.571033.
Train: 2018-08-04T23:29:29.740727: step 3972, loss 0.55522.
Train: 2018-08-04T23:29:29.990662: step 3973, loss 0.539275.
Train: 2018-08-04T23:29:30.240604: step 3974, loss 0.578864.
Train: 2018-08-04T23:29:30.490545: step 3975, loss 0.546916.
Train: 2018-08-04T23:29:30.740486: step 3976, loss 0.586893.
Train: 2018-08-04T23:29:30.990427: step 3977, loss 0.51452.
Train: 2018-08-04T23:29:31.240370: step 3978, loss 0.538508.
Train: 2018-08-04T23:29:31.490311: step 3979, loss 0.538349.
Train: 2018-08-04T23:29:31.740252: step 3980, loss 0.521893.
Test: 2018-08-04T23:29:32.989929: step 3980, loss 0.547021.
Train: 2018-08-04T23:29:33.224280: step 3981, loss 0.513455.
Train: 2018-08-04T23:29:33.489848: step 3982, loss 0.587217.
Train: 2018-08-04T23:29:33.724163: step 3983, loss 0.529417.
Train: 2018-08-04T23:29:33.974105: step 3984, loss 0.587373.
Train: 2018-08-04T23:29:34.224041: step 3985, loss 0.545766.
Train: 2018-08-04T23:29:34.473987: step 3986, loss 0.528962.
Train: 2018-08-04T23:29:34.723923: step 3987, loss 0.554009.
Train: 2018-08-04T23:29:34.973841: step 3988, loss 0.528683.
Train: 2018-08-04T23:29:35.223782: step 3989, loss 0.570804.
Train: 2018-08-04T23:29:35.473753: step 3990, loss 0.579294.
Test: 2018-08-04T23:29:36.754673: step 3990, loss 0.546218.
Train: 2018-08-04T23:29:36.989019: step 3991, loss 0.596311.
Train: 2018-08-04T23:29:37.238964: step 3992, loss 0.553856.
Train: 2018-08-04T23:29:37.488876: step 3993, loss 0.536816.
Train: 2018-08-04T23:29:37.738817: step 3994, loss 0.536811.
Train: 2018-08-04T23:29:37.988759: step 3995, loss 0.570858.
Train: 2018-08-04T23:29:38.238731: step 3996, loss 0.613481.
Train: 2018-08-04T23:29:38.488673: step 3997, loss 0.613462.
Train: 2018-08-04T23:29:38.738614: step 3998, loss 0.494298.
Train: 2018-08-04T23:29:38.988555: step 3999, loss 0.570859.
Train: 2018-08-04T23:29:39.238496: step 4000, loss 0.562342.
Test: 2018-08-04T23:29:40.488175: step 4000, loss 0.547983.
Train: 2018-08-04T23:29:41.316135: step 4001, loss 0.511428.
Train: 2018-08-04T23:29:41.566076: step 4002, loss 0.485981.
Train: 2018-08-04T23:29:41.816018: step 4003, loss 0.468847.
Train: 2018-08-04T23:29:42.065930: step 4004, loss 0.579379.
Train: 2018-08-04T23:29:42.315900: step 4005, loss 0.476878.
Train: 2018-08-04T23:29:42.565843: step 4006, loss 0.510876.
Train: 2018-08-04T23:29:42.815753: step 4007, loss 0.579617.
Train: 2018-08-04T23:29:43.081316: step 4008, loss 0.553689.
Train: 2018-08-04T23:29:43.331259: step 4009, loss 0.648903.
Train: 2018-08-04T23:29:43.596820: step 4010, loss 0.588306.
Test: 2018-08-04T23:29:44.830908: step 4010, loss 0.548285.
Train: 2018-08-04T23:29:45.096470: step 4011, loss 0.536352.
Train: 2018-08-04T23:29:45.346412: step 4012, loss 0.527723.
Train: 2018-08-04T23:29:45.612005: step 4013, loss 0.52771.
Train: 2018-08-04T23:29:45.846296: step 4014, loss 0.579678.
Train: 2018-08-04T23:29:46.096236: step 4015, loss 0.579669.
Train: 2018-08-04T23:29:46.346201: step 4016, loss 0.536414.
Train: 2018-08-04T23:29:46.596119: step 4017, loss 0.536387.
Train: 2018-08-04T23:29:46.846091: step 4018, loss 0.571043.
Train: 2018-08-04T23:29:47.096033: step 4019, loss 0.51914.
Train: 2018-08-04T23:29:47.345973: step 4020, loss 0.570965.
Test: 2018-08-04T23:29:48.595652: step 4020, loss 0.547915.
Train: 2018-08-04T23:29:48.830004: step 4021, loss 0.579621.
Train: 2018-08-04T23:29:49.079911: step 4022, loss 0.5882.
Train: 2018-08-04T23:29:49.329883: step 4023, loss 0.519296.
Train: 2018-08-04T23:29:49.579825: step 4024, loss 0.588147.
Train: 2018-08-04T23:29:49.829767: step 4025, loss 0.527925.
Train: 2018-08-04T23:29:50.079708: step 4026, loss 0.613806.
Train: 2018-08-04T23:29:50.329649: step 4027, loss 0.536641.
Train: 2018-08-04T23:29:50.579591: step 4028, loss 0.536747.
Train: 2018-08-04T23:29:50.829532: step 4029, loss 0.639204.
Train: 2018-08-04T23:29:51.079474: step 4030, loss 0.587879.
Test: 2018-08-04T23:29:52.329150: step 4030, loss 0.548579.
Train: 2018-08-04T23:29:52.579124: step 4031, loss 0.562329.
Train: 2018-08-04T23:29:52.829034: step 4032, loss 0.528502.
Train: 2018-08-04T23:29:53.079006: step 4033, loss 0.511768.
Train: 2018-08-04T23:29:53.328947: step 4034, loss 0.562384.
Train: 2018-08-04T23:29:53.578888: step 4035, loss 0.553969.
Train: 2018-08-04T23:29:53.828830: step 4036, loss 0.570807.
Train: 2018-08-04T23:29:54.078772: step 4037, loss 0.528856.
Train: 2018-08-04T23:29:54.328713: step 4038, loss 0.570773.
Train: 2018-08-04T23:29:54.578624: step 4039, loss 0.612662.
Train: 2018-08-04T23:29:54.828596: step 4040, loss 0.570804.
Test: 2018-08-04T23:29:56.093894: step 4040, loss 0.547923.
Train: 2018-08-04T23:29:56.359488: step 4041, loss 0.554081.
Train: 2018-08-04T23:29:56.609425: step 4042, loss 0.55412.
Train: 2018-08-04T23:29:56.859341: step 4043, loss 0.612298.
Train: 2018-08-04T23:29:57.109283: step 4044, loss 0.521027.
Train: 2018-08-04T23:29:57.359253: step 4045, loss 0.537643.
Train: 2018-08-04T23:29:57.609195: step 4046, loss 0.62037.
Train: 2018-08-04T23:29:57.859136: step 4047, loss 0.570774.
Train: 2018-08-04T23:29:58.109077: step 4048, loss 0.579007.
Train: 2018-08-04T23:29:58.359019: step 4049, loss 0.529713.
Train: 2018-08-04T23:29:58.608960: step 4050, loss 0.636341.
Test: 2018-08-04T23:29:59.858637: step 4050, loss 0.54895.
Train: 2018-08-04T23:30:00.092989: step 4051, loss 0.521744.
Train: 2018-08-04T23:30:00.342900: step 4052, loss 0.513679.
Train: 2018-08-04T23:30:00.592872: step 4053, loss 0.554475.
Train: 2018-08-04T23:30:00.842812: step 4054, loss 0.627817.
Train: 2018-08-04T23:30:01.092727: step 4055, loss 0.554512.
Train: 2018-08-04T23:30:01.342695: step 4056, loss 0.570789.
Train: 2018-08-04T23:30:01.592637: step 4057, loss 0.554561.
Train: 2018-08-04T23:30:01.842577: step 4058, loss 0.530277.
Train: 2018-08-04T23:30:02.092519: step 4059, loss 0.651823.
Train: 2018-08-04T23:30:02.342431: step 4060, loss 0.530383.
Test: 2018-08-04T23:30:03.592138: step 4060, loss 0.548508.
Train: 2018-08-04T23:30:03.842110: step 4061, loss 0.554663.
Train: 2018-08-04T23:30:04.138917: step 4062, loss 0.643428.
Train: 2018-08-04T23:30:04.388827: step 4063, loss 0.603018.
Train: 2018-08-04T23:30:04.638800: step 4064, loss 0.546787.
Train: 2018-08-04T23:30:04.888711: step 4065, loss 0.570862.
Train: 2018-08-04T23:30:05.138652: step 4066, loss 0.594824.
Train: 2018-08-04T23:30:05.388625: step 4067, loss 0.547019.
Train: 2018-08-04T23:30:05.638559: step 4068, loss 0.650356.
Train: 2018-08-04T23:30:05.888475: step 4069, loss 0.57886.
Train: 2018-08-04T23:30:06.138442: step 4070, loss 0.610405.
Test: 2018-08-04T23:30:07.388124: step 4070, loss 0.549696.
Train: 2018-08-04T23:30:07.622445: step 4071, loss 0.578873.
Train: 2018-08-04T23:30:07.872387: step 4072, loss 0.461596.
Train: 2018-08-04T23:30:08.122327: step 4073, loss 0.532013.
Train: 2018-08-04T23:30:08.372299: step 4074, loss 0.563258.
Train: 2018-08-04T23:30:08.622241: step 4075, loss 0.524164.
Train: 2018-08-04T23:30:08.872151: step 4076, loss 0.477095.
Train: 2018-08-04T23:30:09.059634: step 4077, loss 0.663752.
Train: 2018-08-04T23:30:09.309549: step 4078, loss 0.563132.
Train: 2018-08-04T23:30:09.559521: step 4079, loss 0.578866.
Train: 2018-08-04T23:30:09.825058: step 4080, loss 0.555213.
Test: 2018-08-04T23:30:11.324703: step 4080, loss 0.551349.
Train: 2018-08-04T23:30:11.574671: step 4081, loss 0.555185.
Train: 2018-08-04T23:30:11.824619: step 4082, loss 0.586765.
Train: 2018-08-04T23:30:12.074565: step 4083, loss 0.53931.
Train: 2018-08-04T23:30:12.340121: step 4084, loss 0.555094.
Train: 2018-08-04T23:30:12.590062: step 4085, loss 0.618534.
Train: 2018-08-04T23:30:12.840003: step 4086, loss 0.562984.
Train: 2018-08-04T23:30:13.089945: step 4087, loss 0.523275.
Train: 2018-08-04T23:30:13.339882: step 4088, loss 0.539099.
Train: 2018-08-04T23:30:13.589824: step 4089, loss 0.539018.
Train: 2018-08-04T23:30:13.839774: step 4090, loss 0.498976.
Test: 2018-08-04T23:30:15.105067: step 4090, loss 0.548142.
Train: 2018-08-04T23:30:15.355044: step 4091, loss 0.490648.
Train: 2018-08-04T23:30:15.589360: step 4092, loss 0.54663.
Train: 2018-08-04T23:30:15.839298: step 4093, loss 0.603211.
Train: 2018-08-04T23:30:16.089242: step 4094, loss 0.554501.
Train: 2018-08-04T23:30:16.339183: step 4095, loss 0.554469.
Train: 2018-08-04T23:30:16.604740: step 4096, loss 0.611742.
Train: 2018-08-04T23:30:16.839067: step 4097, loss 0.554347.
Train: 2018-08-04T23:30:17.088979: step 4098, loss 0.652973.
Train: 2018-08-04T23:30:17.354540: step 4099, loss 0.578977.
Train: 2018-08-04T23:30:17.620106: step 4100, loss 0.620026.
Test: 2018-08-04T23:30:18.869810: step 4100, loss 0.548508.
Train: 2018-08-04T23:30:19.775876: step 4101, loss 0.488824.
Train: 2018-08-04T23:30:20.041413: step 4102, loss 0.554379.
Train: 2018-08-04T23:30:20.291383: step 4103, loss 0.603534.
Train: 2018-08-04T23:30:20.572539: step 4104, loss 0.570764.
Train: 2018-08-04T23:30:20.822479: step 4105, loss 0.538068.
Train: 2018-08-04T23:30:21.088072: step 4106, loss 0.546256.
Train: 2018-08-04T23:30:21.353604: step 4107, loss 0.529915.
Train: 2018-08-04T23:30:21.619192: step 4108, loss 0.529891.
Train: 2018-08-04T23:30:21.853489: step 4109, loss 0.570764.
Train: 2018-08-04T23:30:22.103460: step 4110, loss 0.546173.
Test: 2018-08-04T23:30:23.353136: step 4110, loss 0.548658.
Train: 2018-08-04T23:30:23.649942: step 4111, loss 0.529726.
Train: 2018-08-04T23:30:23.899914: step 4112, loss 0.562536.
Train: 2018-08-04T23:30:24.149856: step 4113, loss 0.488399.
Train: 2018-08-04T23:30:24.399797: step 4114, loss 0.612063.
Train: 2018-08-04T23:30:24.649741: step 4115, loss 0.562483.
Train: 2018-08-04T23:30:24.899680: step 4116, loss 0.570757.
Train: 2018-08-04T23:30:25.149621: step 4117, loss 0.570758.
Train: 2018-08-04T23:30:25.399533: step 4118, loss 0.56246.
Train: 2018-08-04T23:30:25.649475: step 4119, loss 0.612271.
Train: 2018-08-04T23:30:25.899445: step 4120, loss 0.512689.
Test: 2018-08-04T23:30:27.149123: step 4120, loss 0.548602.
Train: 2018-08-04T23:30:27.383467: step 4121, loss 0.587354.
Train: 2018-08-04T23:30:27.633415: step 4122, loss 0.512695.
Train: 2018-08-04T23:30:27.883356: step 4123, loss 0.587359.
Train: 2018-08-04T23:30:28.133267: step 4124, loss 0.545857.
Train: 2018-08-04T23:30:28.383235: step 4125, loss 0.529245.
Train: 2018-08-04T23:30:28.633181: step 4126, loss 0.604.
Train: 2018-08-04T23:30:28.883118: step 4127, loss 0.57076.
Train: 2018-08-04T23:30:29.133064: step 4128, loss 0.579063.
Train: 2018-08-04T23:30:29.383005: step 4129, loss 0.529275.
Train: 2018-08-04T23:30:29.632947: step 4130, loss 0.595644.
Test: 2018-08-04T23:30:30.867001: step 4130, loss 0.548439.
Train: 2018-08-04T23:30:31.116944: step 4131, loss 0.587331.
Train: 2018-08-04T23:30:31.366884: step 4132, loss 0.537658.
Train: 2018-08-04T23:30:31.616858: step 4133, loss 0.504617.
Train: 2018-08-04T23:30:31.866766: step 4134, loss 0.504595.
Train: 2018-08-04T23:30:32.116742: step 4135, loss 0.703307.
Train: 2018-08-04T23:30:32.366683: step 4136, loss 0.521138.
Train: 2018-08-04T23:30:32.616624: step 4137, loss 0.521175.
Train: 2018-08-04T23:30:32.866532: step 4138, loss 0.488104.
Train: 2018-08-04T23:30:33.116475: step 4139, loss 0.620437.
Train: 2018-08-04T23:30:33.382075: step 4140, loss 0.504502.
Test: 2018-08-04T23:30:34.616123: step 4140, loss 0.548231.
Train: 2018-08-04T23:30:34.850475: step 4141, loss 0.562464.
Train: 2018-08-04T23:30:35.100411: step 4142, loss 0.537545.
Train: 2018-08-04T23:30:35.350357: step 4143, loss 0.587394.
Train: 2018-08-04T23:30:35.600298: step 4144, loss 0.570762.
Train: 2018-08-04T23:30:35.865830: step 4145, loss 0.604064.
Train: 2018-08-04T23:30:36.131393: step 4146, loss 0.5874.
Train: 2018-08-04T23:30:36.381373: step 4147, loss 0.512599.
Train: 2018-08-04T23:30:36.646929: step 4148, loss 0.562452.
Train: 2018-08-04T23:30:36.881250: step 4149, loss 0.545842.
Train: 2018-08-04T23:30:37.131192: step 4150, loss 0.512615.
Test: 2018-08-04T23:30:38.412110: step 4150, loss 0.546835.
Train: 2018-08-04T23:30:38.662051: step 4151, loss 0.545814.
Train: 2018-08-04T23:30:38.911992: step 4152, loss 0.579089.
Train: 2018-08-04T23:30:39.161964: step 4153, loss 0.579098.
Train: 2018-08-04T23:30:39.427526: step 4154, loss 0.612425.
Train: 2018-08-04T23:30:39.677468: step 4155, loss 0.545794.
Train: 2018-08-04T23:30:39.927411: step 4156, loss 0.537495.
Train: 2018-08-04T23:30:40.177345: step 4157, loss 0.537502.
Train: 2018-08-04T23:30:40.427293: step 4158, loss 0.604029.
Train: 2018-08-04T23:30:40.677236: step 4159, loss 0.579071.
Train: 2018-08-04T23:30:40.927171: step 4160, loss 0.545857.
Test: 2018-08-04T23:30:42.176853: step 4160, loss 0.54823.
Train: 2018-08-04T23:30:42.411173: step 4161, loss 0.645405.
Train: 2018-08-04T23:30:42.661114: step 4162, loss 0.562485.
Train: 2018-08-04T23:30:42.911056: step 4163, loss 0.554253.
Train: 2018-08-04T23:30:43.161024: step 4164, loss 0.554289.
Train: 2018-08-04T23:30:43.410971: step 4165, loss 0.652957.
Train: 2018-08-04T23:30:43.672455: step 4166, loss 0.52163.
Train: 2018-08-04T23:30:43.906776: step 4167, loss 0.562601.
Train: 2018-08-04T23:30:44.156717: step 4168, loss 0.513711.
Train: 2018-08-04T23:30:44.406659: step 4169, loss 0.587068.
Train: 2018-08-04T23:30:44.656569: step 4170, loss 0.635858.
Test: 2018-08-04T23:30:45.890656: step 4170, loss 0.549881.
Train: 2018-08-04T23:30:46.140627: step 4171, loss 0.635681.
Train: 2018-08-04T23:30:46.390540: step 4172, loss 0.578886.
Train: 2018-08-04T23:30:46.640513: step 4173, loss 0.570832.
Train: 2018-08-04T23:30:46.890452: step 4174, loss 0.586876.
Train: 2018-08-04T23:30:47.140364: step 4175, loss 0.538967.
Train: 2018-08-04T23:30:47.390335: step 4176, loss 0.539082.
Train: 2018-08-04T23:30:47.640246: step 4177, loss 0.562979.
Train: 2018-08-04T23:30:47.905809: step 4178, loss 0.610568.
Train: 2018-08-04T23:30:48.155750: step 4179, loss 0.570951.
Train: 2018-08-04T23:30:48.405722: step 4180, loss 0.51574.
Test: 2018-08-04T23:30:49.655399: step 4180, loss 0.550285.
Train: 2018-08-04T23:30:49.905340: step 4181, loss 0.594636.
Train: 2018-08-04T23:30:50.170905: step 4182, loss 0.602479.
Train: 2018-08-04T23:30:50.420876: step 4183, loss 0.570989.
Train: 2018-08-04T23:30:50.670786: step 4184, loss 0.523917.
Train: 2018-08-04T23:30:50.936382: step 4185, loss 0.649535.
Train: 2018-08-04T23:30:51.186292: step 4186, loss 0.594572.
Train: 2018-08-04T23:30:51.436233: step 4187, loss 0.578912.
Train: 2018-08-04T23:30:51.701794: step 4188, loss 0.50104.
Train: 2018-08-04T23:30:51.951776: step 4189, loss 0.555558.
Train: 2018-08-04T23:30:52.217325: step 4190, loss 0.578897.
Test: 2018-08-04T23:30:53.467006: step 4190, loss 0.55017.
Train: 2018-08-04T23:30:53.701361: step 4191, loss 0.547779.
Train: 2018-08-04T23:30:53.951301: step 4192, loss 0.578871.
Train: 2018-08-04T23:30:54.201243: step 4193, loss 0.516573.
Train: 2018-08-04T23:30:54.451151: step 4194, loss 0.508604.
Train: 2018-08-04T23:30:54.701125: step 4195, loss 0.61811.
Train: 2018-08-04T23:30:54.951067: step 4196, loss 0.555126.
Train: 2018-08-04T23:30:55.201009: step 4197, loss 0.554818.
Train: 2018-08-04T23:30:55.450950: step 4198, loss 0.562666.
Train: 2018-08-04T23:30:55.700891: step 4199, loss 0.578263.
Train: 2018-08-04T23:30:55.950833: step 4200, loss 0.629016.
Test: 2018-08-04T23:30:57.184886: step 4200, loss 0.54857.
Train: 2018-08-04T23:30:58.044061: step 4201, loss 0.521395.
Train: 2018-08-04T23:30:58.294002: step 4202, loss 0.629072.
Train: 2018-08-04T23:30:58.543976: step 4203, loss 0.523491.
Train: 2018-08-04T23:30:58.793914: step 4204, loss 0.570005.
Train: 2018-08-04T23:30:59.043852: step 4205, loss 0.555475.
Train: 2018-08-04T23:30:59.293767: step 4206, loss 0.587346.
Train: 2018-08-04T23:30:59.543708: step 4207, loss 0.563175.
Train: 2018-08-04T23:30:59.793683: step 4208, loss 0.531074.
Train: 2018-08-04T23:31:00.043593: step 4209, loss 0.523258.
Train: 2018-08-04T23:31:00.309182: step 4210, loss 0.594575.
Test: 2018-08-04T23:31:01.574483: step 4210, loss 0.550217.
Train: 2018-08-04T23:31:01.808837: step 4211, loss 0.547067.
Train: 2018-08-04T23:31:02.058746: step 4212, loss 0.531064.
Train: 2018-08-04T23:31:02.308716: step 4213, loss 0.578776.
Train: 2018-08-04T23:31:02.558660: step 4214, loss 0.54691.
Train: 2018-08-04T23:31:02.808597: step 4215, loss 0.546802.
Train: 2018-08-04T23:31:03.058535: step 4216, loss 0.579099.
Train: 2018-08-04T23:31:03.308484: step 4217, loss 0.490318.
Train: 2018-08-04T23:31:03.558427: step 4218, loss 0.570562.
Train: 2018-08-04T23:31:03.808367: step 4219, loss 0.579022.
Train: 2018-08-04T23:31:04.058309: step 4220, loss 0.595441.
Test: 2018-08-04T23:31:05.292362: step 4220, loss 0.549823.
Train: 2018-08-04T23:31:05.542338: step 4221, loss 0.538054.
Train: 2018-08-04T23:31:05.792275: step 4222, loss 0.497548.
Train: 2018-08-04T23:31:06.042217: step 4223, loss 0.554646.
Train: 2018-08-04T23:31:06.292127: step 4224, loss 0.537545.
Train: 2018-08-04T23:31:06.542101: step 4225, loss 0.480263.
Train: 2018-08-04T23:31:06.792010: step 4226, loss 0.479842.
Train: 2018-08-04T23:31:07.041983: step 4227, loss 0.637229.
Train: 2018-08-04T23:31:07.229439: step 4228, loss 0.651702.
Train: 2018-08-04T23:31:07.479381: step 4229, loss 0.571159.
Train: 2018-08-04T23:31:07.729322: step 4230, loss 0.562478.
Test: 2018-08-04T23:31:08.978998: step 4230, loss 0.549509.
Train: 2018-08-04T23:31:09.213350: step 4231, loss 0.562388.
Train: 2018-08-04T23:31:09.463291: step 4232, loss 0.61318.
Train: 2018-08-04T23:31:09.713232: step 4233, loss 0.570532.
Train: 2018-08-04T23:31:09.963174: step 4234, loss 0.571173.
Train: 2018-08-04T23:31:10.213111: step 4235, loss 0.504162.
Train: 2018-08-04T23:31:10.463026: step 4236, loss 0.546068.
Train: 2018-08-04T23:31:10.712999: step 4237, loss 0.537492.
Train: 2018-08-04T23:31:10.962940: step 4238, loss 0.628926.
Train: 2018-08-04T23:31:11.212881: step 4239, loss 0.562491.
Train: 2018-08-04T23:31:11.462822: step 4240, loss 0.51276.
Test: 2018-08-04T23:31:12.712500: step 4240, loss 0.550353.
Train: 2018-08-04T23:31:12.946849: step 4241, loss 0.537618.
Train: 2018-08-04T23:31:13.196794: step 4242, loss 0.595625.
Train: 2018-08-04T23:31:13.446731: step 4243, loss 0.554186.
Train: 2018-08-04T23:31:13.696677: step 4244, loss 0.554182.
Train: 2018-08-04T23:31:13.946587: step 4245, loss 0.579043.
Train: 2018-08-04T23:31:14.196527: step 4246, loss 0.587327.
Train: 2018-08-04T23:31:14.446469: step 4247, loss 0.537658.
Train: 2018-08-04T23:31:14.696443: step 4248, loss 0.62863.
Train: 2018-08-04T23:31:14.946352: step 4249, loss 0.52125.
Train: 2018-08-04T23:31:15.196294: step 4250, loss 0.587269.
Test: 2018-08-04T23:31:16.477243: step 4250, loss 0.549344.
Train: 2018-08-04T23:31:16.711595: step 4251, loss 0.58721.
Train: 2018-08-04T23:31:16.977158: step 4252, loss 0.611842.
Train: 2018-08-04T23:31:17.211447: step 4253, loss 0.505183.
Train: 2018-08-04T23:31:17.477008: step 4254, loss 0.529861.
Train: 2018-08-04T23:31:17.726981: step 4255, loss 0.521659.
Train: 2018-08-04T23:31:17.976893: step 4256, loss 0.619876.
Train: 2018-08-04T23:31:18.242456: step 4257, loss 0.546244.
Train: 2018-08-04T23:31:18.476776: step 4258, loss 0.521734.
Train: 2018-08-04T23:31:18.742338: step 4259, loss 0.505344.
Train: 2018-08-04T23:31:18.992279: step 4260, loss 0.537999.
Test: 2018-08-04T23:31:20.257607: step 4260, loss 0.549594.
Train: 2018-08-04T23:31:20.523202: step 4261, loss 0.59538.
Train: 2018-08-04T23:31:20.757491: step 4262, loss 0.554326.
Train: 2018-08-04T23:31:21.007463: step 4263, loss 0.570754.
Train: 2018-08-04T23:31:21.257404: step 4264, loss 0.521348.
Train: 2018-08-04T23:31:21.522936: step 4265, loss 0.570758.
Train: 2018-08-04T23:31:21.788500: step 4266, loss 0.570752.
Train: 2018-08-04T23:31:22.038442: step 4267, loss 0.537681.
Train: 2018-08-04T23:31:22.288417: step 4268, loss 0.570764.
Train: 2018-08-04T23:31:22.538326: step 4269, loss 0.595619.
Train: 2018-08-04T23:31:22.788266: step 4270, loss 0.579033.
Test: 2018-08-04T23:31:24.053593: step 4270, loss 0.547891.
Train: 2018-08-04T23:31:24.287914: step 4271, loss 0.579033.
Train: 2018-08-04T23:31:24.537888: step 4272, loss 0.512855.
Train: 2018-08-04T23:31:24.787798: step 4273, loss 0.545939.
Train: 2018-08-04T23:31:25.037764: step 4274, loss 0.554204.
Train: 2018-08-04T23:31:25.287711: step 4275, loss 0.479661.
Train: 2018-08-04T23:31:25.537652: step 4276, loss 0.579055.
Train: 2018-08-04T23:31:25.803213: step 4277, loss 0.520878.
Train: 2018-08-04T23:31:26.053128: step 4278, loss 0.554101.
Train: 2018-08-04T23:31:26.334336: step 4279, loss 0.570774.
Train: 2018-08-04T23:31:26.599874: step 4280, loss 0.587496.
Test: 2018-08-04T23:31:27.833959: step 4280, loss 0.547463.
Train: 2018-08-04T23:31:28.083925: step 4281, loss 0.562404.
Train: 2018-08-04T23:31:28.333866: step 4282, loss 0.579148.
Train: 2018-08-04T23:31:28.583813: step 4283, loss 0.520532.
Train: 2018-08-04T23:31:28.833724: step 4284, loss 0.562397.
Train: 2018-08-04T23:31:29.083696: step 4285, loss 0.545623.
Train: 2018-08-04T23:31:29.333638: step 4286, loss 0.579174.
Train: 2018-08-04T23:31:29.583579: step 4287, loss 0.58757.
Train: 2018-08-04T23:31:29.833521: step 4288, loss 0.595947.
Train: 2018-08-04T23:31:30.083458: step 4289, loss 0.503765.
Train: 2018-08-04T23:31:30.333403: step 4290, loss 0.495395.
Test: 2018-08-04T23:31:31.567460: step 4290, loss 0.548775.
Train: 2018-08-04T23:31:31.817400: step 4291, loss 0.595939.
Train: 2018-08-04T23:31:32.067375: step 4292, loss 0.520452.
Train: 2018-08-04T23:31:32.317316: step 4293, loss 0.478421.
Train: 2018-08-04T23:31:32.582846: step 4294, loss 0.553958.
Train: 2018-08-04T23:31:32.817201: step 4295, loss 0.629877.
Train: 2018-08-04T23:31:33.067141: step 4296, loss 0.537037.
Train: 2018-08-04T23:31:33.317074: step 4297, loss 0.570812.
Train: 2018-08-04T23:31:33.567025: step 4298, loss 0.494741.
Train: 2018-08-04T23:31:33.816966: step 4299, loss 0.55389.
Train: 2018-08-04T23:31:34.066874: step 4300, loss 0.503002.
Test: 2018-08-04T23:31:35.332203: step 4300, loss 0.547174.
Train: 2018-08-04T23:31:36.160133: step 4301, loss 0.579344.
Train: 2018-08-04T23:31:36.410075: step 4302, loss 0.528294.
Train: 2018-08-04T23:31:36.660049: step 4303, loss 0.562339.
Train: 2018-08-04T23:31:36.909958: step 4304, loss 0.587963.
Train: 2018-08-04T23:31:37.159930: step 4305, loss 0.587975.
Train: 2018-08-04T23:31:37.409842: step 4306, loss 0.605058.
Train: 2018-08-04T23:31:37.659782: step 4307, loss 0.519677.
Train: 2018-08-04T23:31:37.925346: step 4308, loss 0.570867.
Train: 2018-08-04T23:31:38.175317: step 4309, loss 0.468624.
Train: 2018-08-04T23:31:38.425256: step 4310, loss 0.587924.
Test: 2018-08-04T23:31:39.674936: step 4310, loss 0.548488.
Train: 2018-08-04T23:31:39.909289: step 4311, loss 0.545281.
Train: 2018-08-04T23:31:40.159227: step 4312, loss 0.622055.
Train: 2018-08-04T23:31:40.409138: step 4313, loss 0.5879.
Train: 2018-08-04T23:31:40.659104: step 4314, loss 0.579352.
Train: 2018-08-04T23:31:40.909052: step 4315, loss 0.596285.
Train: 2018-08-04T23:31:41.158963: step 4316, loss 0.621565.
Train: 2018-08-04T23:31:41.408937: step 4317, loss 0.520262.
Train: 2018-08-04T23:31:41.658876: step 4318, loss 0.553991.
Train: 2018-08-04T23:31:41.908791: step 4319, loss 0.604274.
Train: 2018-08-04T23:31:42.174352: step 4320, loss 0.504008.
Test: 2018-08-04T23:31:43.424058: step 4320, loss 0.548334.
Train: 2018-08-04T23:31:43.658408: step 4321, loss 0.562435.
Train: 2018-08-04T23:31:43.908352: step 4322, loss 0.545823.
Train: 2018-08-04T23:31:44.158291: step 4323, loss 0.496052.
Train: 2018-08-04T23:31:44.408235: step 4324, loss 0.562457.
Train: 2018-08-04T23:31:44.658145: step 4325, loss 0.587363.
Train: 2018-08-04T23:31:44.908116: step 4326, loss 0.637131.
Train: 2018-08-04T23:31:45.158057: step 4327, loss 0.554202.
Train: 2018-08-04T23:31:45.407968: step 4328, loss 0.529454.
Train: 2018-08-04T23:31:45.657910: step 4329, loss 0.554254.
Train: 2018-08-04T23:31:45.907850: step 4330, loss 0.521293.
Test: 2018-08-04T23:31:47.173181: step 4330, loss 0.546466.
Train: 2018-08-04T23:31:47.438744: step 4331, loss 0.579001.
Train: 2018-08-04T23:31:47.704306: step 4332, loss 0.554273.
Train: 2018-08-04T23:31:47.954277: step 4333, loss 0.578997.
Train: 2018-08-04T23:31:48.204199: step 4334, loss 0.603698.
Train: 2018-08-04T23:31:48.438537: step 4335, loss 0.620094.
Train: 2018-08-04T23:31:48.688483: step 4336, loss 0.578962.
Train: 2018-08-04T23:31:48.938424: step 4337, loss 0.529886.
Train: 2018-08-04T23:31:49.188358: step 4338, loss 0.55445.
Train: 2018-08-04T23:31:49.438307: step 4339, loss 0.562627.
Train: 2018-08-04T23:31:49.688244: step 4340, loss 0.57078.
Test: 2018-08-04T23:31:50.937923: step 4340, loss 0.550583.
Train: 2018-08-04T23:31:51.172272: step 4341, loss 0.538279.
Train: 2018-08-04T23:31:51.422215: step 4342, loss 0.562666.
Train: 2018-08-04T23:31:51.672154: step 4343, loss 0.587022.
Train: 2018-08-04T23:31:51.922096: step 4344, loss 0.587008.
Train: 2018-08-04T23:31:52.172036: step 4345, loss 0.562704.
Train: 2018-08-04T23:31:52.421979: step 4346, loss 0.56272.
Train: 2018-08-04T23:31:52.671922: step 4347, loss 0.546585.
Train: 2018-08-04T23:31:52.921866: step 4348, loss 0.619236.
Train: 2018-08-04T23:31:53.171803: step 4349, loss 0.482212.
Train: 2018-08-04T23:31:53.421750: step 4350, loss 0.554704.
Test: 2018-08-04T23:31:54.702666: step 4350, loss 0.548362.
Train: 2018-08-04T23:31:54.952609: step 4351, loss 0.562755.
Train: 2018-08-04T23:31:55.202575: step 4352, loss 0.546613.
Train: 2018-08-04T23:31:55.452521: step 4353, loss 0.603111.
Train: 2018-08-04T23:31:55.718080: step 4354, loss 0.562736.
Train: 2018-08-04T23:31:55.983642: step 4355, loss 0.635407.
Train: 2018-08-04T23:31:56.233559: step 4356, loss 0.546637.
Train: 2018-08-04T23:31:56.499153: step 4357, loss 0.538616.
Train: 2018-08-04T23:31:56.733478: step 4358, loss 0.562777.
Train: 2018-08-04T23:31:56.983385: step 4359, loss 0.522542.
Train: 2018-08-04T23:31:57.233354: step 4360, loss 0.635259.
Test: 2018-08-04T23:31:58.514274: step 4360, loss 0.549527.
Train: 2018-08-04T23:31:58.748596: step 4361, loss 0.546687.
Train: 2018-08-04T23:31:59.014188: step 4362, loss 0.562786.
Train: 2018-08-04T23:31:59.264123: step 4363, loss 0.594957.
Train: 2018-08-04T23:31:59.514072: step 4364, loss 0.530667.
Train: 2018-08-04T23:31:59.764011: step 4365, loss 0.611008.
Train: 2018-08-04T23:32:00.013948: step 4366, loss 0.554793.
Train: 2018-08-04T23:32:00.263894: step 4367, loss 0.594907.
Train: 2018-08-04T23:32:00.513831: step 4368, loss 0.642937.
Train: 2018-08-04T23:32:00.763778: step 4369, loss 0.530953.
Train: 2018-08-04T23:32:01.013713: step 4370, loss 0.523071.
Test: 2018-08-04T23:32:02.263397: step 4370, loss 0.550177.
Train: 2018-08-04T23:32:02.497748: step 4371, loss 0.570894.
Train: 2018-08-04T23:32:02.747658: step 4372, loss 0.626621.
Train: 2018-08-04T23:32:02.997625: step 4373, loss 0.578858.
Train: 2018-08-04T23:32:03.263164: step 4374, loss 0.515431.
Train: 2018-08-04T23:32:03.528726: step 4375, loss 0.523388.
Train: 2018-08-04T23:32:03.794315: step 4376, loss 0.666087.
Train: 2018-08-04T23:32:04.044230: step 4377, loss 0.547198.
Train: 2018-08-04T23:32:04.341034: step 4378, loss 0.49979.
Train: 2018-08-04T23:32:04.544111: step 4379, loss 0.63056.
Train: 2018-08-04T23:32:04.794053: step 4380, loss 0.539327.
Test: 2018-08-04T23:32:06.028139: step 4380, loss 0.54966.
Train: 2018-08-04T23:32:06.278106: step 4381, loss 0.570954.
Train: 2018-08-04T23:32:06.528024: step 4382, loss 0.507702.
Train: 2018-08-04T23:32:06.777963: step 4383, loss 0.570941.
Train: 2018-08-04T23:32:07.027938: step 4384, loss 0.539213.
Train: 2018-08-04T23:32:07.293498: step 4385, loss 0.570914.
Train: 2018-08-04T23:32:07.527819: step 4386, loss 0.523155.
Train: 2018-08-04T23:32:07.777756: step 4387, loss 0.586839.
Train: 2018-08-04T23:32:08.027696: step 4388, loss 0.538895.
Train: 2018-08-04T23:32:08.277637: step 4389, loss 0.62694.
Train: 2018-08-04T23:32:08.527580: step 4390, loss 0.554814.
Test: 2018-08-04T23:32:09.777262: step 4390, loss 0.549048.
Train: 2018-08-04T23:32:10.011581: step 4391, loss 0.586893.
Train: 2018-08-04T23:32:10.277178: step 4392, loss 0.586896.
Train: 2018-08-04T23:32:10.527112: step 4393, loss 0.522692.
Train: 2018-08-04T23:32:10.777060: step 4394, loss 0.554775.
Train: 2018-08-04T23:32:11.026970: step 4395, loss 0.619074.
Train: 2018-08-04T23:32:11.276944: step 4396, loss 0.570835.
Train: 2018-08-04T23:32:11.526885: step 4397, loss 0.61904.
Train: 2018-08-04T23:32:11.792414: step 4398, loss 0.522726.
Train: 2018-08-04T23:32:12.058006: step 4399, loss 0.562833.
Train: 2018-08-04T23:32:12.323566: step 4400, loss 0.490711.
Test: 2018-08-04T23:32:13.557626: step 4400, loss 0.549411.
Train: 2018-08-04T23:32:14.448074: step 4401, loss 0.522684.
Train: 2018-08-04T23:32:14.698015: step 4402, loss 0.570829.
Train: 2018-08-04T23:32:14.947956: step 4403, loss 0.578881.
Train: 2018-08-04T23:32:15.197892: step 4404, loss 0.562735.
Train: 2018-08-04T23:32:15.447839: step 4405, loss 0.603151.
Train: 2018-08-04T23:32:15.697780: step 4406, loss 0.554623.
Train: 2018-08-04T23:32:15.947717: step 4407, loss 0.578894.
Train: 2018-08-04T23:32:16.197658: step 4408, loss 0.522225.
Train: 2018-08-04T23:32:16.447604: step 4409, loss 0.554585.
Train: 2018-08-04T23:32:16.697546: step 4410, loss 0.595135.
Test: 2018-08-04T23:32:17.962845: step 4410, loss 0.548738.
Train: 2018-08-04T23:32:18.197196: step 4411, loss 0.635734.
Train: 2018-08-04T23:32:18.447107: step 4412, loss 0.611329.
Train: 2018-08-04T23:32:18.697074: step 4413, loss 0.506099.
Train: 2018-08-04T23:32:18.946989: step 4414, loss 0.570806.
Train: 2018-08-04T23:32:19.196961: step 4415, loss 0.554659.
Train: 2018-08-04T23:32:19.446902: step 4416, loss 0.530454.
Train: 2018-08-04T23:32:19.696844: step 4417, loss 0.60311.
Train: 2018-08-04T23:32:19.962404: step 4418, loss 0.619235.
Train: 2018-08-04T23:32:20.212348: step 4419, loss 0.603045.
Train: 2018-08-04T23:32:20.462259: step 4420, loss 0.562801.
Test: 2018-08-04T23:32:21.711966: step 4420, loss 0.548151.
Train: 2018-08-04T23:32:21.946318: step 4421, loss 0.530762.
Train: 2018-08-04T23:32:22.196259: step 4422, loss 0.562848.
Train: 2018-08-04T23:32:22.461817: step 4423, loss 0.530858.
Train: 2018-08-04T23:32:22.696143: step 4424, loss 0.514852.
Train: 2018-08-04T23:32:22.946083: step 4425, loss 0.570854.
Train: 2018-08-04T23:32:23.195995: step 4426, loss 0.578868.
Train: 2018-08-04T23:32:23.445966: step 4427, loss 0.635051.
Train: 2018-08-04T23:32:23.695877: step 4428, loss 0.546797.
Train: 2018-08-04T23:32:23.961440: step 4429, loss 0.659001.
Train: 2018-08-04T23:32:24.227003: step 4430, loss 0.498942.
Test: 2018-08-04T23:32:25.476710: step 4430, loss 0.549184.
Train: 2018-08-04T23:32:25.757895: step 4431, loss 0.634763.
Train: 2018-08-04T23:32:26.007861: step 4432, loss 0.610729.
Train: 2018-08-04T23:32:26.257779: step 4433, loss 0.539146.
Train: 2018-08-04T23:32:26.507751: step 4434, loss 0.610561.
Train: 2018-08-04T23:32:26.757660: step 4435, loss 0.586765.
Train: 2018-08-04T23:32:27.007627: step 4436, loss 0.56311.
Train: 2018-08-04T23:32:27.257574: step 4437, loss 0.571013.
Train: 2018-08-04T23:32:27.507515: step 4438, loss 0.563198.
Train: 2018-08-04T23:32:27.757457: step 4439, loss 0.563233.
Train: 2018-08-04T23:32:28.007367: step 4440, loss 0.5867.
Test: 2018-08-04T23:32:29.272697: step 4440, loss 0.54991.
Train: 2018-08-04T23:32:29.522639: step 4441, loss 0.563295.
Train: 2018-08-04T23:32:29.772613: step 4442, loss 0.610053.
Train: 2018-08-04T23:32:30.038143: step 4443, loss 0.532275.
Train: 2018-08-04T23:32:30.288083: step 4444, loss 0.602205.
Train: 2018-08-04T23:32:30.538058: step 4445, loss 0.555657.
Train: 2018-08-04T23:32:30.803589: step 4446, loss 0.547937.
Train: 2018-08-04T23:32:31.037937: step 4447, loss 0.633138.
Train: 2018-08-04T23:32:31.303470: step 4448, loss 0.509341.
Train: 2018-08-04T23:32:31.553412: step 4449, loss 0.532526.
Train: 2018-08-04T23:32:31.819005: step 4450, loss 0.594411.
Test: 2018-08-04T23:32:33.053062: step 4450, loss 0.551731.
Train: 2018-08-04T23:32:33.303031: step 4451, loss 0.555674.
Train: 2018-08-04T23:32:33.552953: step 4452, loss 0.563402.
Train: 2018-08-04T23:32:33.802918: step 4453, loss 0.555612.
Train: 2018-08-04T23:32:34.052860: step 4454, loss 0.547792.
Train: 2018-08-04T23:32:34.302770: step 4455, loss 0.555514.
Train: 2018-08-04T23:32:34.552709: step 4456, loss 0.500773.
Train: 2018-08-04T23:32:34.818272: step 4457, loss 0.578875.
Train: 2018-08-04T23:32:35.068241: step 4458, loss 0.618206.
Train: 2018-08-04T23:32:35.318180: step 4459, loss 0.523696.
Train: 2018-08-04T23:32:35.583718: step 4460, loss 0.578861.
Test: 2018-08-04T23:32:36.849047: step 4460, loss 0.549426.
Train: 2018-08-04T23:32:37.083399: step 4461, loss 0.5947.
Train: 2018-08-04T23:32:37.333334: step 4462, loss 0.507477.
Train: 2018-08-04T23:32:37.583276: step 4463, loss 0.539092.
Train: 2018-08-04T23:32:37.833216: step 4464, loss 0.570882.
Train: 2018-08-04T23:32:38.083163: step 4465, loss 0.602866.
Train: 2018-08-04T23:32:38.333074: step 4466, loss 0.602908.
Train: 2018-08-04T23:32:38.583015: step 4467, loss 0.506701.
Train: 2018-08-04T23:32:38.832957: step 4468, loss 0.554769.
Train: 2018-08-04T23:32:39.082930: step 4469, loss 0.482269.
Train: 2018-08-04T23:32:39.348492: step 4470, loss 0.465747.
Test: 2018-08-04T23:32:40.598169: step 4470, loss 0.548706.
Train: 2018-08-04T23:32:40.832490: step 4471, loss 0.562655.
Train: 2018-08-04T23:32:41.082461: step 4472, loss 0.546257.
Train: 2018-08-04T23:32:41.332372: step 4473, loss 0.521489.
Train: 2018-08-04T23:32:41.582339: step 4474, loss 0.5625.
Train: 2018-08-04T23:32:41.832282: step 4475, loss 0.487799.
Train: 2018-08-04T23:32:42.082198: step 4476, loss 0.562423.
Train: 2018-08-04T23:32:42.332165: step 4477, loss 0.545618.
Train: 2018-08-04T23:32:42.597704: step 4478, loss 0.587657.
Train: 2018-08-04T23:32:42.863265: step 4479, loss 0.587735.
Train: 2018-08-04T23:32:43.128858: step 4480, loss 0.587792.
Test: 2018-08-04T23:32:44.378533: step 4480, loss 0.548946.
Train: 2018-08-04T23:32:44.644096: step 4481, loss 0.638783.
Train: 2018-08-04T23:32:44.894038: step 4482, loss 0.502929.
Train: 2018-08-04T23:32:45.175247: step 4483, loss 0.587829.
Train: 2018-08-04T23:32:45.425163: step 4484, loss 0.502903.
Train: 2018-08-04T23:32:45.659485: step 4485, loss 0.579345.
Train: 2018-08-04T23:32:45.909426: step 4486, loss 0.647364.
Train: 2018-08-04T23:32:46.159369: step 4487, loss 0.553863.
Train: 2018-08-04T23:32:46.440552: step 4488, loss 0.528465.
Train: 2018-08-04T23:32:46.690493: step 4489, loss 0.503108.
Train: 2018-08-04T23:32:46.940465: step 4490, loss 0.477694.
Test: 2018-08-04T23:32:48.190141: step 4490, loss 0.548966.
Train: 2018-08-04T23:32:48.440109: step 4491, loss 0.570832.
Train: 2018-08-04T23:32:48.690056: step 4492, loss 0.638784.
Train: 2018-08-04T23:32:48.939965: step 4493, loss 0.570836.
Train: 2018-08-04T23:32:49.205534: step 4494, loss 0.57083.
Train: 2018-08-04T23:32:49.439849: step 4495, loss 0.553888.
Train: 2018-08-04T23:32:49.689820: step 4496, loss 0.579274.
Train: 2018-08-04T23:32:49.939762: step 4497, loss 0.646806.
Train: 2018-08-04T23:32:50.189703: step 4498, loss 0.612868.
Train: 2018-08-04T23:32:50.439614: step 4499, loss 0.5624.
Train: 2018-08-04T23:32:50.689581: step 4500, loss 0.545736.
Test: 2018-08-04T23:32:51.939264: step 4500, loss 0.549325.
Train: 2018-08-04T23:32:52.782844: step 4501, loss 0.512557.
Train: 2018-08-04T23:32:53.032788: step 4502, loss 0.496082.
Train: 2018-08-04T23:32:53.282700: step 4503, loss 0.545878.
Train: 2018-08-04T23:32:53.532668: step 4504, loss 0.554173.
Train: 2018-08-04T23:32:53.782612: step 4505, loss 0.554174.
Train: 2018-08-04T23:32:54.032556: step 4506, loss 0.595635.
Train: 2018-08-04T23:32:54.282492: step 4507, loss 0.579043.
Train: 2018-08-04T23:32:54.532435: step 4508, loss 0.53765.
Train: 2018-08-04T23:32:54.782351: step 4509, loss 0.570756.
Train: 2018-08-04T23:32:55.032316: step 4510, loss 0.587288.
Test: 2018-08-04T23:32:56.281996: step 4510, loss 0.549475.
Train: 2018-08-04T23:32:56.516316: step 4511, loss 0.479952.
Train: 2018-08-04T23:32:56.766289: step 4512, loss 0.603797.
Train: 2018-08-04T23:32:57.016199: step 4513, loss 0.579013.
Train: 2018-08-04T23:32:57.266166: step 4514, loss 0.521258.
Train: 2018-08-04T23:32:57.516082: step 4515, loss 0.554255.
Train: 2018-08-04T23:32:57.766053: step 4516, loss 0.545998.
Train: 2018-08-04T23:32:58.015995: step 4517, loss 0.554242.
Train: 2018-08-04T23:32:58.265939: step 4518, loss 0.570756.
Train: 2018-08-04T23:32:58.515881: step 4519, loss 0.595551.
Train: 2018-08-04T23:32:58.765820: step 4520, loss 0.545976.
Test: 2018-08-04T23:33:00.031118: step 4520, loss 0.548705.
Train: 2018-08-04T23:33:00.265476: step 4521, loss 0.60379.
Train: 2018-08-04T23:33:00.515379: step 4522, loss 0.521264.
Train: 2018-08-04T23:33:00.765321: step 4523, loss 0.554262.
Train: 2018-08-04T23:33:01.015293: step 4524, loss 0.50478.
Train: 2018-08-04T23:33:01.265204: step 4525, loss 0.579015.
Train: 2018-08-04T23:33:01.515176: step 4526, loss 0.488123.
Train: 2018-08-04T23:33:01.765117: step 4527, loss 0.562475.
Train: 2018-08-04T23:33:02.015053: step 4528, loss 0.520963.
Train: 2018-08-04T23:33:02.265000: step 4529, loss 0.520846.
Train: 2018-08-04T23:33:02.468078: step 4530, loss 0.651484.
Test: 2018-08-04T23:33:03.717755: step 4530, loss 0.548848.
Train: 2018-08-04T23:33:03.952106: step 4531, loss 0.537346.
Train: 2018-08-04T23:33:04.202047: step 4532, loss 0.637672.
Train: 2018-08-04T23:33:04.451988: step 4533, loss 0.529008.
Train: 2018-08-04T23:33:04.717526: step 4534, loss 0.554066.
Train: 2018-08-04T23:33:04.967493: step 4535, loss 0.503961.
Train: 2018-08-04T23:33:05.217434: step 4536, loss 0.53733.
Train: 2018-08-04T23:33:05.467370: step 4537, loss 0.562403.
Train: 2018-08-04T23:33:05.717317: step 4538, loss 0.579161.
Train: 2018-08-04T23:33:05.967260: step 4539, loss 0.545619.
Train: 2018-08-04T23:33:06.217201: step 4540, loss 0.503637.
Test: 2018-08-04T23:33:07.466877: step 4540, loss 0.547952.
Train: 2018-08-04T23:33:07.701228: step 4541, loss 0.553971.
Train: 2018-08-04T23:33:07.951140: step 4542, loss 0.553954.
Train: 2018-08-04T23:33:08.201080: step 4543, loss 0.528625.
Train: 2018-08-04T23:33:08.451052: step 4544, loss 0.596165.
Train: 2018-08-04T23:33:08.700993: step 4545, loss 0.528529.
Train: 2018-08-04T23:33:08.950935: step 4546, loss 0.528479.
Train: 2018-08-04T23:33:09.200847: step 4547, loss 0.545386.
Train: 2018-08-04T23:33:09.450817: step 4548, loss 0.562346.
Train: 2018-08-04T23:33:09.700759: step 4549, loss 0.604883.
Train: 2018-08-04T23:33:09.950670: step 4550, loss 0.528323.
Test: 2018-08-04T23:33:11.215998: step 4550, loss 0.548324.
Train: 2018-08-04T23:33:11.450350: step 4551, loss 0.502756.
Train: 2018-08-04T23:33:11.700292: step 4552, loss 0.562343.
Train: 2018-08-04T23:33:11.950233: step 4553, loss 0.536729.
Train: 2018-08-04T23:33:12.200173: step 4554, loss 0.545239.
Train: 2018-08-04T23:33:12.450115: step 4555, loss 0.605139.
Train: 2018-08-04T23:33:12.700052: step 4556, loss 0.622265.
Train: 2018-08-04T23:33:12.950000: step 4557, loss 0.570887.
Train: 2018-08-04T23:33:13.199939: step 4558, loss 0.553805.
Train: 2018-08-04T23:33:13.449885: step 4559, loss 0.587904.
Train: 2018-08-04T23:33:13.699818: step 4560, loss 0.587852.
Test: 2018-08-04T23:33:14.949500: step 4560, loss 0.549364.
Train: 2018-08-04T23:33:15.183851: step 4561, loss 0.570833.
Train: 2018-08-04T23:33:15.433761: step 4562, loss 0.621538.
Train: 2018-08-04T23:33:15.683733: step 4563, loss 0.587635.
Train: 2018-08-04T23:33:15.933670: step 4564, loss 0.554015.
Train: 2018-08-04T23:33:16.183616: step 4565, loss 0.587467.
Train: 2018-08-04T23:33:16.433558: step 4566, loss 0.512558.
Train: 2018-08-04T23:33:16.683499: step 4567, loss 0.562462.
Train: 2018-08-04T23:33:16.933440: step 4568, loss 0.529402.
Train: 2018-08-04T23:33:17.183377: step 4569, loss 0.479913.
Train: 2018-08-04T23:33:17.433325: step 4570, loss 0.512887.
Test: 2018-08-04T23:33:18.683000: step 4570, loss 0.548825.
Train: 2018-08-04T23:33:18.917354: step 4571, loss 0.487979.
Train: 2018-08-04T23:33:19.182909: step 4572, loss 0.570847.
Train: 2018-08-04T23:33:19.417237: step 4573, loss 0.629476.
Train: 2018-08-04T23:33:19.667179: step 4574, loss 0.562439.
Train: 2018-08-04T23:33:19.917111: step 4575, loss 0.570801.
Train: 2018-08-04T23:33:20.167028: step 4576, loss 0.653918.
Train: 2018-08-04T23:33:20.416969: step 4577, loss 0.570758.
Train: 2018-08-04T23:33:20.666944: step 4578, loss 0.562478.
Train: 2018-08-04T23:33:20.916853: step 4579, loss 0.545974.
Train: 2018-08-04T23:33:21.166827: step 4580, loss 0.570752.
Test: 2018-08-04T23:33:22.416502: step 4580, loss 0.548959.
Train: 2018-08-04T23:33:22.650855: step 4581, loss 0.529587.
Train: 2018-08-04T23:33:22.900795: step 4582, loss 0.562509.
Train: 2018-08-04T23:33:23.150737: step 4583, loss 0.562569.
Train: 2018-08-04T23:33:23.400676: step 4584, loss 0.636478.
Train: 2018-08-04T23:33:23.650613: step 4585, loss 0.488788.
Train: 2018-08-04T23:33:23.900559: step 4586, loss 0.578922.
Train: 2018-08-04T23:33:24.150471: step 4587, loss 0.578955.
Train: 2018-08-04T23:33:24.400440: step 4588, loss 0.57899.
Train: 2018-08-04T23:33:24.650387: step 4589, loss 0.546274.
Train: 2018-08-04T23:33:24.900328: step 4590, loss 0.513678.
Test: 2018-08-04T23:33:26.150001: step 4590, loss 0.54824.
Train: 2018-08-04T23:33:26.384353: step 4591, loss 0.538165.
Train: 2018-08-04T23:33:26.634297: step 4592, loss 0.546291.
Train: 2018-08-04T23:33:26.884238: step 4593, loss 0.595297.
Train: 2018-08-04T23:33:27.134146: step 4594, loss 0.546252.
Train: 2018-08-04T23:33:27.384118: step 4595, loss 0.546237.
Train: 2018-08-04T23:33:27.634062: step 4596, loss 0.546191.
Train: 2018-08-04T23:33:27.883972: step 4597, loss 0.562579.
Train: 2018-08-04T23:33:28.133912: step 4598, loss 0.59531.
Train: 2018-08-04T23:33:28.383853: step 4599, loss 0.628073.
Train: 2018-08-04T23:33:28.633795: step 4600, loss 0.578907.
Test: 2018-08-04T23:33:29.883502: step 4600, loss 0.549392.
Train: 2018-08-04T23:33:30.758328: step 4601, loss 0.497449.
Train: 2018-08-04T23:33:31.008239: step 4602, loss 0.587102.
Train: 2018-08-04T23:33:31.258206: step 4603, loss 0.578927.
Train: 2018-08-04T23:33:31.508155: step 4604, loss 0.570815.
Train: 2018-08-04T23:33:31.758094: step 4605, loss 0.619609.
Train: 2018-08-04T23:33:32.008035: step 4606, loss 0.586992.
Train: 2018-08-04T23:33:32.257946: step 4607, loss 0.619458.
Train: 2018-08-04T23:33:32.507918: step 4608, loss 0.570628.
Train: 2018-08-04T23:33:32.757859: step 4609, loss 0.602909.
Train: 2018-08-04T23:33:33.007771: step 4610, loss 0.618721.
Test: 2018-08-04T23:33:34.257478: step 4610, loss 0.549759.
Train: 2018-08-04T23:33:34.491830: step 4611, loss 0.499469.
Train: 2018-08-04T23:33:34.741771: step 4612, loss 0.523706.
Train: 2018-08-04T23:33:34.991712: step 4613, loss 0.610279.
Train: 2018-08-04T23:33:35.241654: step 4614, loss 0.610455.
Train: 2018-08-04T23:33:35.491590: step 4615, loss 0.594562.
Train: 2018-08-04T23:33:35.741536: step 4616, loss 0.468867.
Train: 2018-08-04T23:33:35.991477: step 4617, loss 0.516083.
Train: 2018-08-04T23:33:36.241419: step 4618, loss 0.555195.
Train: 2018-08-04T23:33:36.491361: step 4619, loss 0.531422.
Train: 2018-08-04T23:33:36.756919: step 4620, loss 0.539238.
Test: 2018-08-04T23:33:38.006601: step 4620, loss 0.551156.
Train: 2018-08-04T23:33:38.240957: step 4621, loss 0.570913.
Train: 2018-08-04T23:33:38.490893: step 4622, loss 0.539026.
Train: 2018-08-04T23:33:38.740806: step 4623, loss 0.475025.
Train: 2018-08-04T23:33:38.990776: step 4624, loss 0.635041.
Train: 2018-08-04T23:33:39.240712: step 4625, loss 0.627133.
Train: 2018-08-04T23:33:39.490627: step 4626, loss 0.530573.
Train: 2018-08-04T23:33:39.740595: step 4627, loss 0.546627.
Train: 2018-08-04T23:33:40.006133: step 4628, loss 0.538502.
Train: 2018-08-04T23:33:40.271723: step 4629, loss 0.546511.
Train: 2018-08-04T23:33:40.506054: step 4630, loss 0.603272.
Test: 2018-08-04T23:33:41.755722: step 4630, loss 0.548899.
Train: 2018-08-04T23:33:41.990043: step 4631, loss 0.611411.
Train: 2018-08-04T23:33:42.240009: step 4632, loss 0.611414.
Train: 2018-08-04T23:33:42.489956: step 4633, loss 0.554563.
Train: 2018-08-04T23:33:42.739898: step 4634, loss 0.570789.
Train: 2018-08-04T23:33:42.989808: step 4635, loss 0.595104.
Train: 2018-08-04T23:33:43.239780: step 4636, loss 0.562709.
Train: 2018-08-04T23:33:43.489691: step 4637, loss 0.506149.
Train: 2018-08-04T23:33:43.739663: step 4638, loss 0.635474.
Train: 2018-08-04T23:33:43.989604: step 4639, loss 0.595019.
Train: 2018-08-04T23:33:44.235465: step 4640, loss 0.554718.
Test: 2018-08-04T23:33:45.485142: step 4640, loss 0.548803.
Train: 2018-08-04T23:33:45.750704: step 4641, loss 0.522578.
Train: 2018-08-04T23:33:46.000645: step 4642, loss 0.619069.
Train: 2018-08-04T23:33:46.266208: step 4643, loss 0.490582.
Train: 2018-08-04T23:33:46.516180: step 4644, loss 0.554787.
Train: 2018-08-04T23:33:46.781738: step 4645, loss 0.4905.
Train: 2018-08-04T23:33:47.031654: step 4646, loss 0.578877.
Train: 2018-08-04T23:33:47.281596: step 4647, loss 0.611151.
Train: 2018-08-04T23:33:47.562781: step 4648, loss 0.530455.
Train: 2018-08-04T23:33:47.797136: step 4649, loss 0.538477.
Train: 2018-08-04T23:33:48.047067: step 4650, loss 0.562702.
Test: 2018-08-04T23:33:49.312371: step 4650, loss 0.548204.
Train: 2018-08-04T23:33:49.577964: step 4651, loss 0.578904.
Train: 2018-08-04T23:33:49.827913: step 4652, loss 0.611387.
Train: 2018-08-04T23:33:50.093438: step 4653, loss 0.522082.
Train: 2018-08-04T23:33:50.343380: step 4654, loss 0.530169.
Train: 2018-08-04T23:33:50.593348: step 4655, loss 0.635888.
Train: 2018-08-04T23:33:50.858884: step 4656, loss 0.595182.
Train: 2018-08-04T23:33:51.140067: step 4657, loss 0.538294.
Train: 2018-08-04T23:33:51.390010: step 4658, loss 0.546419.
Train: 2018-08-04T23:33:51.639986: step 4659, loss 0.651971.
Train: 2018-08-04T23:33:51.889917: step 4660, loss 0.643763.
Test: 2018-08-04T23:33:53.123978: step 4660, loss 0.548518.
Train: 2018-08-04T23:33:53.358299: step 4661, loss 0.514338.
Train: 2018-08-04T23:33:53.608265: step 4662, loss 0.594969.
Train: 2018-08-04T23:33:53.858184: step 4663, loss 0.490531.
Train: 2018-08-04T23:33:54.108155: step 4664, loss 0.586891.
Train: 2018-08-04T23:33:54.358093: step 4665, loss 0.562813.
Train: 2018-08-04T23:33:54.608005: step 4666, loss 0.578866.
Train: 2018-08-04T23:33:54.857976: step 4667, loss 0.538805.
Train: 2018-08-04T23:33:55.107918: step 4668, loss 0.586876.
Train: 2018-08-04T23:33:55.357829: step 4669, loss 0.594875.
Train: 2018-08-04T23:33:55.607801: step 4670, loss 0.546888.
Test: 2018-08-04T23:33:56.873099: step 4670, loss 0.550831.
Train: 2018-08-04T23:33:57.107455: step 4671, loss 0.578861.
Train: 2018-08-04T23:33:57.357391: step 4672, loss 0.515014.
Train: 2018-08-04T23:33:57.607333: step 4673, loss 0.491032.
Train: 2018-08-04T23:33:57.857244: step 4674, loss 0.538846.
Train: 2018-08-04T23:33:58.107215: step 4675, loss 0.522699.
Train: 2018-08-04T23:33:58.357151: step 4676, loss 0.490302.
Train: 2018-08-04T23:33:58.607093: step 4677, loss 0.60317.
Train: 2018-08-04T23:33:58.857035: step 4678, loss 0.587029.
Train: 2018-08-04T23:33:59.106981: step 4679, loss 0.505649.
Train: 2018-08-04T23:33:59.356894: step 4680, loss 0.587111.
Test: 2018-08-04T23:34:00.606600: step 4680, loss 0.547944.
Train: 2018-08-04T23:34:00.794082: step 4681, loss 0.492656.
Train: 2018-08-04T23:34:01.044028: step 4682, loss 0.537854.
Train: 2018-08-04T23:34:01.293969: step 4683, loss 0.570756.
Train: 2018-08-04T23:34:01.543906: step 4684, loss 0.554181.
Train: 2018-08-04T23:34:01.793854: step 4685, loss 0.529187.
Train: 2018-08-04T23:34:02.043764: step 4686, loss 0.478984.
Train: 2018-08-04T23:34:02.293736: step 4687, loss 0.537241.
Train: 2018-08-04T23:34:02.543677: step 4688, loss 0.553949.
Train: 2018-08-04T23:34:02.793619: step 4689, loss 0.469283.
Train: 2018-08-04T23:34:03.043530: step 4690, loss 0.562343.
Test: 2018-08-04T23:34:04.293238: step 4690, loss 0.548056.
Train: 2018-08-04T23:34:04.527558: step 4691, loss 0.579439.
Train: 2018-08-04T23:34:04.824363: step 4692, loss 0.528002.
Train: 2018-08-04T23:34:05.074337: step 4693, loss 0.570953.
Train: 2018-08-04T23:34:05.324270: step 4694, loss 0.545056.
Train: 2018-08-04T23:34:05.574187: step 4695, loss 0.510348.
Train: 2018-08-04T23:34:05.824159: step 4696, loss 0.597132.
Train: 2018-08-04T23:34:06.074100: step 4697, loss 0.536227.
Train: 2018-08-04T23:34:06.324013: step 4698, loss 0.597277.
Train: 2018-08-04T23:34:06.573954: step 4699, loss 0.544903.
Train: 2018-08-04T23:34:06.823925: step 4700, loss 0.536155.
Test: 2018-08-04T23:34:08.073602: step 4700, loss 0.548575.
Train: 2018-08-04T23:34:08.964046: step 4701, loss 0.544884.
Train: 2018-08-04T23:34:09.213959: step 4702, loss 0.606137.
Train: 2018-08-04T23:34:09.463903: step 4703, loss 0.55363.
Train: 2018-08-04T23:34:09.713873: step 4704, loss 0.606072.
Train: 2018-08-04T23:34:09.963784: step 4705, loss 0.553642.
Train: 2018-08-04T23:34:10.213727: step 4706, loss 0.571066.
Train: 2018-08-04T23:34:10.463668: step 4707, loss 0.518909.
Train: 2018-08-04T23:34:10.713636: step 4708, loss 0.640439.
Train: 2018-08-04T23:34:10.979171: step 4709, loss 0.562342.
Train: 2018-08-04T23:34:11.229143: step 4710, loss 0.570959.
Test: 2018-08-04T23:34:12.478820: step 4710, loss 0.549368.
Train: 2018-08-04T23:34:12.713169: step 4711, loss 0.588117.
Train: 2018-08-04T23:34:12.963107: step 4712, loss 0.647949.
Train: 2018-08-04T23:34:13.213024: step 4713, loss 0.536798.
Train: 2018-08-04T23:34:13.462964: step 4714, loss 0.562353.
Train: 2018-08-04T23:34:13.712906: step 4715, loss 0.511738.
Train: 2018-08-04T23:34:13.962878: step 4716, loss 0.495077.
Train: 2018-08-04T23:34:14.212817: step 4717, loss 0.545583.
Train: 2018-08-04T23:34:14.462758: step 4718, loss 0.579175.
Train: 2018-08-04T23:34:14.712705: step 4719, loss 0.662959.
Train: 2018-08-04T23:34:14.962613: step 4720, loss 0.512324.
Test: 2018-08-04T23:34:16.227942: step 4720, loss 0.548714.
Train: 2018-08-04T23:34:16.462291: step 4721, loss 0.495794.
Train: 2018-08-04T23:34:16.712232: step 4722, loss 0.54579.
Train: 2018-08-04T23:34:16.977792: step 4723, loss 0.604046.
Train: 2018-08-04T23:34:17.227738: step 4724, loss 0.587379.
Train: 2018-08-04T23:34:17.477684: step 4725, loss 0.55417.
Train: 2018-08-04T23:34:17.743237: step 4726, loss 0.579037.
Train: 2018-08-04T23:34:17.977569: step 4727, loss 0.570756.
Train: 2018-08-04T23:34:18.243096: step 4728, loss 0.579005.
Train: 2018-08-04T23:34:18.477446: step 4729, loss 0.578988.
Train: 2018-08-04T23:34:18.727387: step 4730, loss 0.537919.
Test: 2018-08-04T23:34:19.977065: step 4730, loss 0.549065.
Train: 2018-08-04T23:34:20.227036: step 4731, loss 0.496992.
Train: 2018-08-04T23:34:20.476947: step 4732, loss 0.505189.
Train: 2018-08-04T23:34:20.726889: step 4733, loss 0.505101.
Train: 2018-08-04T23:34:20.976830: step 4734, loss 0.587213.
Train: 2018-08-04T23:34:21.226806: step 4735, loss 0.570757.
Train: 2018-08-04T23:34:21.476714: step 4736, loss 0.587257.
Train: 2018-08-04T23:34:21.726685: step 4737, loss 0.587265.
Train: 2018-08-04T23:34:21.976621: step 4738, loss 0.587261.
Train: 2018-08-04T23:34:22.226572: step 4739, loss 0.529529.
Train: 2018-08-04T23:34:22.476509: step 4740, loss 0.562512.
Test: 2018-08-04T23:34:23.726186: step 4740, loss 0.548364.
Train: 2018-08-04T23:34:23.960532: step 4741, loss 0.587245.
Train: 2018-08-04T23:34:24.210448: step 4742, loss 0.554281.
Train: 2018-08-04T23:34:24.460389: step 4743, loss 0.578991.
Train: 2018-08-04T23:34:24.710361: step 4744, loss 0.587212.
Train: 2018-08-04T23:34:24.960303: step 4745, loss 0.546114.
Train: 2018-08-04T23:34:25.210213: step 4746, loss 0.595383.
Train: 2018-08-04T23:34:25.460156: step 4747, loss 0.546181.
Train: 2018-08-04T23:34:25.725719: step 4748, loss 0.546211.
Train: 2018-08-04T23:34:25.960072: step 4749, loss 0.48897.
Train: 2018-08-04T23:34:26.225602: step 4750, loss 0.578953.
Test: 2018-08-04T23:34:27.522175: step 4750, loss 0.547754.
Train: 2018-08-04T23:34:27.834600: step 4751, loss 0.669087.
Train: 2018-08-04T23:34:28.084574: step 4752, loss 0.5953.
Train: 2018-08-04T23:34:28.334483: step 4753, loss 0.595244.
Train: 2018-08-04T23:34:28.584423: step 4754, loss 0.570782.
Train: 2018-08-04T23:34:28.849987: step 4755, loss 0.473495.
Train: 2018-08-04T23:34:29.099958: step 4756, loss 0.595107.
Train: 2018-08-04T23:34:29.365521: step 4757, loss 0.578894.
Train: 2018-08-04T23:34:29.615463: step 4758, loss 0.538474.
Train: 2018-08-04T23:34:29.881020: step 4759, loss 0.522344.
Train: 2018-08-04T23:34:30.115346: step 4760, loss 0.59505.
Test: 2018-08-04T23:34:31.365023: step 4760, loss 0.549054.
Train: 2018-08-04T23:34:31.599343: step 4761, loss 0.546574.
Train: 2018-08-04T23:34:31.849314: step 4762, loss 0.498096.
Train: 2018-08-04T23:34:32.099251: step 4763, loss 0.595079.
Train: 2018-08-04T23:34:32.349167: step 4764, loss 0.530301.
Train: 2018-08-04T23:34:32.599108: step 4765, loss 0.570791.
Train: 2018-08-04T23:34:32.849050: step 4766, loss 0.53018.
Train: 2018-08-04T23:34:33.099022: step 4767, loss 0.521962.
Train: 2018-08-04T23:34:33.348964: step 4768, loss 0.562615.
Train: 2018-08-04T23:34:33.598874: step 4769, loss 0.603476.
Train: 2018-08-04T23:34:33.848846: step 4770, loss 0.529831.
Test: 2018-08-04T23:34:35.114144: step 4770, loss 0.549052.
Train: 2018-08-04T23:34:35.364116: step 4771, loss 0.513352.
Train: 2018-08-04T23:34:35.614059: step 4772, loss 0.578982.
Train: 2018-08-04T23:34:35.863999: step 4773, loss 0.570757.
Train: 2018-08-04T23:34:36.113941: step 4774, loss 0.545997.
Train: 2018-08-04T23:34:36.363853: step 4775, loss 0.488084.
Train: 2018-08-04T23:34:36.613820: step 4776, loss 0.52929.
Train: 2018-08-04T23:34:36.863760: step 4777, loss 0.545794.
Train: 2018-08-04T23:34:37.129299: step 4778, loss 0.604173.
Train: 2018-08-04T23:34:37.379264: step 4779, loss 0.604246.
Train: 2018-08-04T23:34:37.629211: step 4780, loss 0.629395.
Test: 2018-08-04T23:34:38.878888: step 4780, loss 0.548435.
Train: 2018-08-04T23:34:39.113208: step 4781, loss 0.537309.
Train: 2018-08-04T23:34:39.363181: step 4782, loss 0.478785.
Train: 2018-08-04T23:34:39.613121: step 4783, loss 0.654514.
Train: 2018-08-04T23:34:39.863063: step 4784, loss 0.545676.
Train: 2018-08-04T23:34:40.113004: step 4785, loss 0.612574.
Train: 2018-08-04T23:34:40.362945: step 4786, loss 0.504005.
Train: 2018-08-04T23:34:40.612887: step 4787, loss 0.570767.
Train: 2018-08-04T23:34:40.862829: step 4788, loss 0.562429.
Train: 2018-08-04T23:34:41.112739: step 4789, loss 0.595752.
Train: 2018-08-04T23:34:41.362712: step 4790, loss 0.579078.
Test: 2018-08-04T23:34:42.612389: step 4790, loss 0.548593.
Train: 2018-08-04T23:34:42.846735: step 4791, loss 0.562457.
Train: 2018-08-04T23:34:43.096676: step 4792, loss 0.595618.
Train: 2018-08-04T23:34:43.346632: step 4793, loss 0.529422.
Train: 2018-08-04T23:34:43.596533: step 4794, loss 0.52123.
Train: 2018-08-04T23:34:43.846474: step 4795, loss 0.653262.
Train: 2018-08-04T23:34:44.096446: step 4796, loss 0.504922.
Train: 2018-08-04T23:34:44.346383: step 4797, loss 0.620084.
Train: 2018-08-04T23:34:44.596329: step 4798, loss 0.554357.
Train: 2018-08-04T23:34:44.846270: step 4799, loss 0.619884.
Train: 2018-08-04T23:34:45.096212: step 4800, loss 0.56261.
Test: 2018-08-04T23:34:46.361510: step 4800, loss 0.548861.
Train: 2018-08-04T23:34:47.220715: step 4801, loss 0.562641.
Train: 2018-08-04T23:34:47.470656: step 4802, loss 0.595143.
Train: 2018-08-04T23:34:47.720597: step 4803, loss 0.554612.
Train: 2018-08-04T23:34:47.970539: step 4804, loss 0.578885.
Train: 2018-08-04T23:34:48.220481: step 4805, loss 0.53861.
Train: 2018-08-04T23:34:48.470422: step 4806, loss 0.546713.
Train: 2018-08-04T23:34:48.720359: step 4807, loss 0.57887.
Train: 2018-08-04T23:34:48.970275: step 4808, loss 0.594911.
Train: 2018-08-04T23:34:49.220246: step 4809, loss 0.602885.
Train: 2018-08-04T23:34:49.470157: step 4810, loss 0.483021.
Test: 2018-08-04T23:34:50.719866: step 4810, loss 0.549372.
Train: 2018-08-04T23:34:50.969808: step 4811, loss 0.642741.
Train: 2018-08-04T23:34:51.219779: step 4812, loss 0.594797.
Train: 2018-08-04T23:34:51.469719: step 4813, loss 0.570909.
Train: 2018-08-04T23:34:51.719661: step 4814, loss 0.499552.
Train: 2018-08-04T23:34:51.969603: step 4815, loss 0.563003.
Train: 2018-08-04T23:34:52.219549: step 4816, loss 0.602638.
Train: 2018-08-04T23:34:52.469485: step 4817, loss 0.515513.
Train: 2018-08-04T23:34:52.719427: step 4818, loss 0.563016.
Train: 2018-08-04T23:34:52.969364: step 4819, loss 0.586785.
Train: 2018-08-04T23:34:53.219310: step 4820, loss 0.499594.
Test: 2018-08-04T23:34:54.468987: step 4820, loss 0.55063.
Train: 2018-08-04T23:34:54.703338: step 4821, loss 0.570918.
Train: 2018-08-04T23:34:54.953281: step 4822, loss 0.547048.
Train: 2018-08-04T23:34:55.203221: step 4823, loss 0.578859.
Train: 2018-08-04T23:34:55.453162: step 4824, loss 0.554923.
Train: 2018-08-04T23:34:55.703103: step 4825, loss 0.530912.
Train: 2018-08-04T23:34:55.953045: step 4826, loss 0.578865.
Train: 2018-08-04T23:34:56.202988: step 4827, loss 0.610967.
Train: 2018-08-04T23:34:56.452928: step 4828, loss 0.554783.
Train: 2018-08-04T23:34:56.702869: step 4829, loss 0.554767.
Train: 2018-08-04T23:34:56.952780: step 4830, loss 0.619085.
Test: 2018-08-04T23:34:58.186866: step 4830, loss 0.548629.
Train: 2018-08-04T23:34:58.436840: step 4831, loss 0.546717.
Train: 2018-08-04T23:34:58.639916: step 4832, loss 0.648547.
Train: 2018-08-04T23:34:58.889857: step 4833, loss 0.506653.
Train: 2018-08-04T23:34:59.139799: step 4834, loss 0.514695.
Train: 2018-08-04T23:34:59.389740: step 4835, loss 0.562811.
Train: 2018-08-04T23:34:59.639682: step 4836, loss 0.554761.
Train: 2018-08-04T23:34:59.889623: step 4837, loss 0.562782.
Train: 2018-08-04T23:35:00.139536: step 4838, loss 0.546658.
Train: 2018-08-04T23:35:00.389506: step 4839, loss 0.482084.
Train: 2018-08-04T23:35:00.639417: step 4840, loss 0.619358.
Test: 2018-08-04T23:35:01.889160: step 4840, loss 0.548777.
Train: 2018-08-04T23:35:02.139097: step 4841, loss 0.595112.
Train: 2018-08-04T23:35:02.389038: step 4842, loss 0.603238.
Train: 2018-08-04T23:35:02.638948: step 4843, loss 0.522135.
Train: 2018-08-04T23:35:02.888916: step 4844, loss 0.643828.
Train: 2018-08-04T23:35:03.138863: step 4845, loss 0.619428.
Train: 2018-08-04T23:35:03.388799: step 4846, loss 0.586976.
Train: 2018-08-04T23:35:03.638745: step 4847, loss 0.538561.
Train: 2018-08-04T23:35:03.888686: step 4848, loss 0.570827.
Train: 2018-08-04T23:35:04.138597: step 4849, loss 0.562801.
Train: 2018-08-04T23:35:04.388540: step 4850, loss 0.578868.
Test: 2018-08-04T23:35:05.669489: step 4850, loss 0.548731.
Train: 2018-08-04T23:35:05.903841: step 4851, loss 0.506777.
Train: 2018-08-04T23:35:06.153775: step 4852, loss 0.458724.
Train: 2018-08-04T23:35:06.403723: step 4853, loss 0.506601.
Train: 2018-08-04T23:35:06.653659: step 4854, loss 0.506342.
Train: 2018-08-04T23:35:06.903575: step 4855, loss 0.530308.
Train: 2018-08-04T23:35:07.153516: step 4856, loss 0.489391.
Train: 2018-08-04T23:35:07.403489: step 4857, loss 0.63628.
Train: 2018-08-04T23:35:07.653431: step 4858, loss 0.595422.
Train: 2018-08-04T23:35:07.903367: step 4859, loss 0.55427.
Train: 2018-08-04T23:35:08.153282: step 4860, loss 0.562494.
Test: 2018-08-04T23:35:09.402990: step 4860, loss 0.549596.
Train: 2018-08-04T23:35:09.652962: step 4861, loss 0.62045.
Train: 2018-08-04T23:35:09.902899: step 4862, loss 0.529322.
Train: 2018-08-04T23:35:10.152814: step 4863, loss 0.645427.
Train: 2018-08-04T23:35:10.402787: step 4864, loss 0.628782.
Train: 2018-08-04T23:35:10.652725: step 4865, loss 0.587295.
Train: 2018-08-04T23:35:10.902669: step 4866, loss 0.570754.
Train: 2018-08-04T23:35:11.152611: step 4867, loss 0.587208.
Train: 2018-08-04T23:35:11.402521: step 4868, loss 0.611759.
Train: 2018-08-04T23:35:11.668109: step 4869, loss 0.529936.
Train: 2018-08-04T23:35:11.918025: step 4870, loss 0.546345.
Test: 2018-08-04T23:35:13.167735: step 4870, loss 0.549835.
Train: 2018-08-04T23:35:13.402084: step 4871, loss 0.538279.
Train: 2018-08-04T23:35:13.652025: step 4872, loss 0.627597.
Train: 2018-08-04T23:35:13.901967: step 4873, loss 0.578893.
Train: 2018-08-04T23:35:14.151902: step 4874, loss 0.522388.
Train: 2018-08-04T23:35:14.401821: step 4875, loss 0.554703.
Train: 2018-08-04T23:35:14.667384: step 4876, loss 0.514479.
Train: 2018-08-04T23:35:14.917324: step 4877, loss 0.586928.
Train: 2018-08-04T23:35:15.167301: step 4878, loss 0.538631.
Train: 2018-08-04T23:35:15.432854: step 4879, loss 0.522514.
Train: 2018-08-04T23:35:15.682770: step 4880, loss 0.54663.
Test: 2018-08-04T23:35:16.916855: step 4880, loss 0.548503.
Train: 2018-08-04T23:35:17.166822: step 4881, loss 0.538507.
Train: 2018-08-04T23:35:17.416771: step 4882, loss 0.51415.
Train: 2018-08-04T23:35:17.697922: step 4883, loss 0.635731.
Train: 2018-08-04T23:35:17.932243: step 4884, loss 0.578911.
Train: 2018-08-04T23:35:18.182214: step 4885, loss 0.538257.
Train: 2018-08-04T23:35:18.432156: step 4886, loss 0.530077.
Train: 2018-08-04T23:35:18.682098: step 4887, loss 0.562619.
Train: 2018-08-04T23:35:18.932033: step 4888, loss 0.619777.
Train: 2018-08-04T23:35:19.181980: step 4889, loss 0.546263.
Train: 2018-08-04T23:35:19.431922: step 4890, loss 0.603458.
Test: 2018-08-04T23:35:20.681598: step 4890, loss 0.549712.
Train: 2018-08-04T23:35:20.915949: step 4891, loss 0.497257.
Train: 2018-08-04T23:35:21.165885: step 4892, loss 0.538062.
Train: 2018-08-04T23:35:21.415832: step 4893, loss 0.538011.
Train: 2018-08-04T23:35:21.665773: step 4894, loss 0.628186.
Train: 2018-08-04T23:35:21.915715: step 4895, loss 0.529738.
Train: 2018-08-04T23:35:22.165626: step 4896, loss 0.521491.
Train: 2018-08-04T23:35:22.415592: step 4897, loss 0.578983.
Train: 2018-08-04T23:35:22.665540: step 4898, loss 0.562523.
Train: 2018-08-04T23:35:22.915481: step 4899, loss 0.546032.
Train: 2018-08-04T23:35:23.165423: step 4900, loss 0.587258.
Test: 2018-08-04T23:35:24.415099: step 4900, loss 0.548907.
Train: 2018-08-04T23:35:25.274274: step 4901, loss 0.587265.
Train: 2018-08-04T23:35:25.524245: step 4902, loss 0.529495.
Train: 2018-08-04T23:35:25.774187: step 4903, loss 0.570756.
Train: 2018-08-04T23:35:26.024123: step 4904, loss 0.545984.
Train: 2018-08-04T23:35:26.274039: step 4905, loss 0.595539.
Train: 2018-08-04T23:35:26.524010: step 4906, loss 0.620302.
Train: 2018-08-04T23:35:26.773934: step 4907, loss 0.570757.
Train: 2018-08-04T23:35:27.023889: step 4908, loss 0.537844.
Train: 2018-08-04T23:35:27.273830: step 4909, loss 0.56254.
Train: 2018-08-04T23:35:27.523777: step 4910, loss 0.56255.
Test: 2018-08-04T23:35:28.789076: step 4910, loss 0.547916.
Train: 2018-08-04T23:35:29.023429: step 4911, loss 0.554357.
Train: 2018-08-04T23:35:29.273367: step 4912, loss 0.488797.
Train: 2018-08-04T23:35:29.523278: step 4913, loss 0.546146.
Train: 2018-08-04T23:35:29.773250: step 4914, loss 0.570759.
Train: 2018-08-04T23:35:30.023163: step 4915, loss 0.578981.
Train: 2018-08-04T23:35:30.273102: step 4916, loss 0.603662.
Train: 2018-08-04T23:35:30.523069: step 4917, loss 0.620084.
Train: 2018-08-04T23:35:30.773016: step 4918, loss 0.529736.
Train: 2018-08-04T23:35:31.022928: step 4919, loss 0.537976.
Train: 2018-08-04T23:35:31.272893: step 4920, loss 0.603538.
Test: 2018-08-04T23:35:32.538199: step 4920, loss 0.549105.
Train: 2018-08-04T23:35:32.788170: step 4921, loss 0.521668.
Train: 2018-08-04T23:35:33.038110: step 4922, loss 0.619851.
Train: 2018-08-04T23:35:33.288052: step 4923, loss 0.644282.
Train: 2018-08-04T23:35:33.537987: step 4924, loss 0.538215.
Train: 2018-08-04T23:35:33.787935: step 4925, loss 0.513938.
Train: 2018-08-04T23:35:34.037876: step 4926, loss 0.505886.
Train: 2018-08-04T23:35:34.287818: step 4927, loss 0.627606.
Train: 2018-08-04T23:35:34.537753: step 4928, loss 0.595117.
Train: 2018-08-04T23:35:34.787670: step 4929, loss 0.546519.
Train: 2018-08-04T23:35:35.037642: step 4930, loss 0.570805.
Test: 2018-08-04T23:35:36.287320: step 4930, loss 0.548505.
Train: 2018-08-04T23:35:36.537285: step 4931, loss 0.498134.
Train: 2018-08-04T23:35:36.787203: step 4932, loss 0.562728.
Train: 2018-08-04T23:35:37.037169: step 4933, loss 0.546554.
Train: 2018-08-04T23:35:37.302709: step 4934, loss 0.530347.
Train: 2018-08-04T23:35:37.552649: step 4935, loss 0.522171.
Train: 2018-08-04T23:35:37.818239: step 4936, loss 0.546414.
Train: 2018-08-04T23:35:38.052532: step 4937, loss 0.538198.
Train: 2018-08-04T23:35:38.318093: step 4938, loss 0.652454.
Train: 2018-08-04T23:35:38.552439: step 4939, loss 0.554423.
Train: 2018-08-04T23:35:38.802386: step 4940, loss 0.497167.
Test: 2018-08-04T23:35:40.067685: step 4940, loss 0.549826.
Train: 2018-08-04T23:35:40.333249: step 4941, loss 0.578958.
Train: 2018-08-04T23:35:40.567597: step 4942, loss 0.603587.
Train: 2018-08-04T23:35:40.833155: step 4943, loss 0.505086.
Train: 2018-08-04T23:35:41.067476: step 4944, loss 0.554314.
Train: 2018-08-04T23:35:41.317391: step 4945, loss 0.611933.
Train: 2018-08-04T23:35:41.567361: step 4946, loss 0.653128.
Train: 2018-08-04T23:35:41.832897: step 4947, loss 0.45566.
Train: 2018-08-04T23:35:42.067217: step 4948, loss 0.587212.
Train: 2018-08-04T23:35:42.317185: step 4949, loss 0.587212.
Train: 2018-08-04T23:35:42.582720: step 4950, loss 0.611869.
Test: 2018-08-04T23:35:43.848049: step 4950, loss 0.549223.
Train: 2018-08-04T23:35:44.097991: step 4951, loss 0.65284.
Train: 2018-08-04T23:35:44.347932: step 4952, loss 0.51352.
Train: 2018-08-04T23:35:44.582277: step 4953, loss 0.52997.
Train: 2018-08-04T23:35:44.832223: step 4954, loss 0.521871.
Train: 2018-08-04T23:35:45.082165: step 4955, loss 0.595224.
Train: 2018-08-04T23:35:45.332106: step 4956, loss 0.481213.
Train: 2018-08-04T23:35:45.582018: step 4957, loss 0.562624.
Train: 2018-08-04T23:35:45.831989: step 4958, loss 0.587091.
Train: 2018-08-04T23:35:46.081926: step 4959, loss 0.505475.
Train: 2018-08-04T23:35:46.331842: step 4960, loss 0.538067.
Test: 2018-08-04T23:35:47.581550: step 4960, loss 0.548704.
Train: 2018-08-04T23:35:47.815903: step 4961, loss 0.628103.
Train: 2018-08-04T23:35:48.065811: step 4962, loss 0.537989.
Train: 2018-08-04T23:35:48.315785: step 4963, loss 0.562562.
Train: 2018-08-04T23:35:48.565723: step 4964, loss 0.595377.
Train: 2018-08-04T23:35:48.815660: step 4965, loss 0.570761.
Train: 2018-08-04T23:35:49.065577: step 4966, loss 0.546159.
Train: 2018-08-04T23:35:49.315549: step 4967, loss 0.619967.
Train: 2018-08-04T23:35:49.565490: step 4968, loss 0.636283.
Train: 2018-08-04T23:35:49.815401: step 4969, loss 0.521774.
Train: 2018-08-04T23:35:50.065371: step 4970, loss 0.546318.
Test: 2018-08-04T23:35:51.299429: step 4970, loss 0.549223.
Train: 2018-08-04T23:35:51.533780: step 4971, loss 0.54635.
Train: 2018-08-04T23:35:51.783720: step 4972, loss 0.521955.
Train: 2018-08-04T23:35:52.033656: step 4973, loss 0.635902.
Train: 2018-08-04T23:35:52.314841: step 4974, loss 0.546395.
Train: 2018-08-04T23:35:52.564758: step 4975, loss 0.619522.
Train: 2018-08-04T23:35:52.830322: step 4976, loss 0.546476.
Train: 2018-08-04T23:35:53.111535: step 4977, loss 0.522234.
Train: 2018-08-04T23:35:53.377067: step 4978, loss 0.586985.
Train: 2018-08-04T23:35:53.627033: step 4979, loss 0.570805.
Train: 2018-08-04T23:35:53.892602: step 4980, loss 0.530419.
Test: 2018-08-04T23:35:55.126658: step 4980, loss 0.548868.
Train: 2018-08-04T23:35:55.360978: step 4981, loss 0.554652.
Train: 2018-08-04T23:35:55.610949: step 4982, loss 0.562727.
Train: 2018-08-04T23:35:55.813998: step 4983, loss 0.476512.
Train: 2018-08-04T23:35:56.063968: step 4984, loss 0.514089.
Train: 2018-08-04T23:35:56.329502: step 4985, loss 0.546402.
Train: 2018-08-04T23:35:56.579444: step 4986, loss 0.603391.
Train: 2018-08-04T23:35:56.845005: step 4987, loss 0.57894.
Train: 2018-08-04T23:35:57.110568: step 4988, loss 0.587131.
Train: 2018-08-04T23:35:57.376132: step 4989, loss 0.587141.
Train: 2018-08-04T23:35:57.626072: step 4990, loss 0.587142.
Test: 2018-08-04T23:35:58.875779: step 4990, loss 0.549289.
Train: 2018-08-04T23:35:59.125721: step 4991, loss 0.603503.
Train: 2018-08-04T23:35:59.375664: step 4992, loss 0.562596.
Train: 2018-08-04T23:35:59.625634: step 4993, loss 0.5218.
Train: 2018-08-04T23:35:59.875576: step 4994, loss 0.554452.
Train: 2018-08-04T23:36:00.125513: step 4995, loss 0.497335.
Train: 2018-08-04T23:36:00.375453: step 4996, loss 0.644314.
Train: 2018-08-04T23:36:00.625371: step 4997, loss 0.57077.
Train: 2018-08-04T23:36:00.875341: step 4998, loss 0.652384.
Train: 2018-08-04T23:36:01.125284: step 4999, loss 0.587058.
Train: 2018-08-04T23:36:01.375220: step 5000, loss 0.55456.
Test: 2018-08-04T23:36:02.640524: step 5000, loss 0.548442.
Train: 2018-08-04T23:36:03.515349: step 5001, loss 0.595083.
Train: 2018-08-04T23:36:03.765285: step 5002, loss 0.554672.
Train: 2018-08-04T23:36:04.030857: step 5003, loss 0.586927.
Train: 2018-08-04T23:36:04.265174: step 5004, loss 0.50661.
Train: 2018-08-04T23:36:04.530732: step 5005, loss 0.562826.
Train: 2018-08-04T23:36:04.796268: step 5006, loss 0.554825.
Train: 2018-08-04T23:36:05.046248: step 5007, loss 0.610905.
Train: 2018-08-04T23:36:05.311797: step 5008, loss 0.578863.
Train: 2018-08-04T23:36:05.561715: step 5009, loss 0.522966.
Train: 2018-08-04T23:36:05.827302: step 5010, loss 0.515005.
Test: 2018-08-04T23:36:07.092605: step 5010, loss 0.549541.
Train: 2018-08-04T23:36:07.358168: step 5011, loss 0.514948.
Train: 2018-08-04T23:36:07.592514: step 5012, loss 0.522824.
Train: 2018-08-04T23:36:07.842460: step 5013, loss 0.538724.
Train: 2018-08-04T23:36:08.092402: step 5014, loss 0.586933.
Train: 2018-08-04T23:36:08.342313: step 5015, loss 0.52236.
Train: 2018-08-04T23:36:08.592285: step 5016, loss 0.554595.
Train: 2018-08-04T23:36:08.842195: step 5017, loss 0.497654.
Train: 2018-08-04T23:36:09.092167: step 5018, loss 0.603414.
Train: 2018-08-04T23:36:09.342080: step 5019, loss 0.595318.
Train: 2018-08-04T23:36:09.592021: step 5020, loss 0.570762.
Test: 2018-08-04T23:36:10.857349: step 5020, loss 0.549025.
Train: 2018-08-04T23:36:11.122938: step 5021, loss 0.587182.
Train: 2018-08-04T23:36:11.372855: step 5022, loss 0.537893.
Train: 2018-08-04T23:36:11.638416: step 5023, loss 0.562532.
Train: 2018-08-04T23:36:11.888357: step 5024, loss 0.570757.
Train: 2018-08-04T23:36:12.153951: step 5025, loss 0.504841.
Train: 2018-08-04T23:36:12.403862: step 5026, loss 0.537738.
Train: 2018-08-04T23:36:12.653804: step 5027, loss 0.636935.
Train: 2018-08-04T23:36:12.919367: step 5028, loss 0.562483.
Train: 2018-08-04T23:36:13.169333: step 5029, loss 0.50456.
Train: 2018-08-04T23:36:13.419279: step 5030, loss 0.595613.
Test: 2018-08-04T23:36:14.684577: step 5030, loss 0.547482.
Train: 2018-08-04T23:36:14.918928: step 5031, loss 0.554181.
Train: 2018-08-04T23:36:15.168870: step 5032, loss 0.529299.
Train: 2018-08-04T23:36:15.418811: step 5033, loss 0.587359.
Train: 2018-08-04T23:36:15.668752: step 5034, loss 0.545851.
Train: 2018-08-04T23:36:15.918695: step 5035, loss 0.587374.
Train: 2018-08-04T23:36:16.168635: step 5036, loss 0.653818.
Train: 2018-08-04T23:36:16.418548: step 5037, loss 0.529323.
Train: 2018-08-04T23:36:16.668514: step 5038, loss 0.636959.
Train: 2018-08-04T23:36:16.918459: step 5039, loss 0.595508.
Train: 2018-08-04T23:36:17.168402: step 5040, loss 0.57898.
Test: 2018-08-04T23:36:18.418079: step 5040, loss 0.548325.
Train: 2018-08-04T23:36:18.652430: step 5041, loss 0.546188.
Train: 2018-08-04T23:36:18.902341: step 5042, loss 0.546265.
Train: 2018-08-04T23:36:19.152312: step 5043, loss 0.554476.
Train: 2018-08-04T23:36:19.402224: step 5044, loss 0.595186.
Train: 2018-08-04T23:36:19.652195: step 5045, loss 0.554559.
Train: 2018-08-04T23:36:19.902137: step 5046, loss 0.570797.
Train: 2018-08-04T23:36:20.152047: step 5047, loss 0.53847.
Train: 2018-08-04T23:36:20.401988: step 5048, loss 0.530436.
Train: 2018-08-04T23:36:20.667582: step 5049, loss 0.570811.
Train: 2018-08-04T23:36:20.901903: step 5050, loss 0.506246.
Test: 2018-08-04T23:36:22.151579: step 5050, loss 0.549978.
Train: 2018-08-04T23:36:22.401520: step 5051, loss 0.562728.
Train: 2018-08-04T23:36:22.667083: step 5052, loss 0.562715.
Train: 2018-08-04T23:36:22.901434: step 5053, loss 0.538414.
Train: 2018-08-04T23:36:23.151344: step 5054, loss 0.595118.
Train: 2018-08-04T23:36:23.401311: step 5055, loss 0.473431.
Train: 2018-08-04T23:36:23.651255: step 5056, loss 0.562646.
Train: 2018-08-04T23:36:23.901169: step 5057, loss 0.546311.
Train: 2018-08-04T23:36:24.151112: step 5058, loss 0.505367.
Train: 2018-08-04T23:36:24.401082: step 5059, loss 0.546148.
Train: 2018-08-04T23:36:24.651024: step 5060, loss 0.55429.
Test: 2018-08-04T23:36:25.885081: step 5060, loss 0.549459.
Train: 2018-08-04T23:36:26.135051: step 5061, loss 0.537712.
Train: 2018-08-04T23:36:26.384988: step 5062, loss 0.570758.
Train: 2018-08-04T23:36:26.634935: step 5063, loss 0.54582.
Train: 2018-08-04T23:36:26.884876: step 5064, loss 0.545754.
Train: 2018-08-04T23:36:27.134788: step 5065, loss 0.595853.
Train: 2018-08-04T23:36:27.384754: step 5066, loss 0.528905.
Train: 2018-08-04T23:36:27.634703: step 5067, loss 0.528824.
Train: 2018-08-04T23:36:27.884641: step 5068, loss 0.469841.
Train: 2018-08-04T23:36:28.150204: step 5069, loss 0.537023.
Train: 2018-08-04T23:36:28.400146: step 5070, loss 0.579313.
Test: 2018-08-04T23:36:29.649823: step 5070, loss 0.547941.
Train: 2018-08-04T23:36:29.946658: step 5071, loss 0.528316.
Train: 2018-08-04T23:36:30.196600: step 5072, loss 0.579408.
Train: 2018-08-04T23:36:30.446541: step 5073, loss 0.579445.
Train: 2018-08-04T23:36:30.712075: step 5074, loss 0.596605.
Train: 2018-08-04T23:36:30.962046: step 5075, loss 0.519483.
Train: 2018-08-04T23:36:31.211982: step 5076, loss 0.579493.
Train: 2018-08-04T23:36:31.461898: step 5077, loss 0.536591.
Train: 2018-08-04T23:36:31.711871: step 5078, loss 0.467888.
Train: 2018-08-04T23:36:31.961812: step 5079, loss 0.53652.
Train: 2018-08-04T23:36:32.211754: step 5080, loss 0.579587.
Test: 2018-08-04T23:36:33.480228: step 5080, loss 0.547915.
Train: 2018-08-04T23:36:33.716597: step 5081, loss 0.640069.
Train: 2018-08-04T23:36:33.969918: step 5082, loss 0.527814.
Train: 2018-08-04T23:36:34.217261: step 5083, loss 0.605488.
Train: 2018-08-04T23:36:34.459609: step 5084, loss 0.570956.
Train: 2018-08-04T23:36:34.713931: step 5085, loss 0.57094.
Train: 2018-08-04T23:36:34.965259: step 5086, loss 0.613865.
Train: 2018-08-04T23:36:35.219578: step 5087, loss 0.596584.
Train: 2018-08-04T23:36:35.472900: step 5088, loss 0.536749.
Train: 2018-08-04T23:36:35.720239: step 5089, loss 0.562344.
Train: 2018-08-04T23:36:35.978549: step 5090, loss 0.647141.
Test: 2018-08-04T23:36:37.307995: step 5090, loss 0.54769.
Train: 2018-08-04T23:36:37.573312: step 5091, loss 0.52861.
Train: 2018-08-04T23:36:37.811617: step 5092, loss 0.562382.
Train: 2018-08-04T23:36:38.058955: step 5093, loss 0.629431.
Train: 2018-08-04T23:36:38.301338: step 5094, loss 0.620801.
Train: 2018-08-04T23:36:38.545655: step 5095, loss 0.579049.
Train: 2018-08-04T23:36:38.789039: step 5096, loss 0.611982.
Train: 2018-08-04T23:36:39.042325: step 5097, loss 0.603537.
Train: 2018-08-04T23:36:39.286704: step 5098, loss 0.587058.
Train: 2018-08-04T23:36:39.540992: step 5099, loss 0.522291.
Train: 2018-08-04T23:36:39.789328: step 5100, loss 0.538647.
Test: 2018-08-04T23:36:41.058935: step 5100, loss 0.550005.
Train: 2018-08-04T23:36:41.964686: step 5101, loss 0.546808.
Train: 2018-08-04T23:36:42.210020: step 5102, loss 0.538909.
Train: 2018-08-04T23:36:42.456336: step 5103, loss 0.562911.
Train: 2018-08-04T23:36:42.707694: step 5104, loss 0.554976.
Train: 2018-08-04T23:36:42.951013: step 5105, loss 0.547054.
Train: 2018-08-04T23:36:43.210326: step 5106, loss 0.539126.
Train: 2018-08-04T23:36:43.467630: step 5107, loss 0.618599.
Train: 2018-08-04T23:36:43.718990: step 5108, loss 0.586798.
Train: 2018-08-04T23:36:43.960315: step 5109, loss 0.563002.
Train: 2018-08-04T23:36:44.212639: step 5110, loss 0.53926.
Test: 2018-08-04T23:36:45.463296: step 5110, loss 0.549436.
Train: 2018-08-04T23:36:45.700662: step 5111, loss 0.531353.
Train: 2018-08-04T23:36:45.949995: step 5112, loss 0.499625.
Train: 2018-08-04T23:36:46.199361: step 5113, loss 0.578858.
Train: 2018-08-04T23:36:46.444674: step 5114, loss 0.554989.
Train: 2018-08-04T23:36:46.692043: step 5115, loss 0.562915.
Train: 2018-08-04T23:36:46.936389: step 5116, loss 0.594837.
Train: 2018-08-04T23:36:47.181728: step 5117, loss 0.594855.
Train: 2018-08-04T23:36:47.436023: step 5118, loss 0.554869.
Train: 2018-08-04T23:36:47.684385: step 5119, loss 0.594869.
Train: 2018-08-04T23:36:47.927722: step 5120, loss 0.530859.
Test: 2018-08-04T23:36:49.189335: step 5120, loss 0.548375.
Train: 2018-08-04T23:36:49.427698: step 5121, loss 0.586871.
Train: 2018-08-04T23:36:49.675038: step 5122, loss 0.56285.
Train: 2018-08-04T23:36:49.921379: step 5123, loss 0.482754.
Train: 2018-08-04T23:36:50.166749: step 5124, loss 0.610978.
Train: 2018-08-04T23:36:50.414093: step 5125, loss 0.554768.
Train: 2018-08-04T23:36:50.673394: step 5126, loss 0.52257.
Train: 2018-08-04T23:36:50.921736: step 5127, loss 0.5547.
Train: 2018-08-04T23:36:51.171071: step 5128, loss 0.586962.
Train: 2018-08-04T23:36:51.417410: step 5129, loss 0.538455.
Train: 2018-08-04T23:36:51.661758: step 5130, loss 0.595101.
Test: 2018-08-04T23:36:52.931332: step 5130, loss 0.548207.
Train: 2018-08-04T23:36:53.186651: step 5131, loss 0.578901.
Train: 2018-08-04T23:36:53.435983: step 5132, loss 0.570791.
Train: 2018-08-04T23:36:53.679333: step 5133, loss 0.55456.
Train: 2018-08-04T23:36:53.874810: step 5134, loss 0.458755.
Train: 2018-08-04T23:36:54.122150: step 5135, loss 0.513788.
Train: 2018-08-04T23:36:54.374476: step 5136, loss 0.521738.
Train: 2018-08-04T23:36:54.634810: step 5137, loss 0.562554.
Train: 2018-08-04T23:36:54.885110: step 5138, loss 0.59547.
Train: 2018-08-04T23:36:55.128460: step 5139, loss 0.512942.
Train: 2018-08-04T23:36:55.396773: step 5140, loss 0.521034.
Test: 2018-08-04T23:36:56.635273: step 5140, loss 0.548165.
Train: 2018-08-04T23:36:56.900862: step 5141, loss 0.587401.
Train: 2018-08-04T23:36:57.150778: step 5142, loss 0.562424.
Train: 2018-08-04T23:36:57.431988: step 5143, loss 0.545683.
Train: 2018-08-04T23:36:57.681930: step 5144, loss 0.545629.
Train: 2018-08-04T23:36:57.963088: step 5145, loss 0.66323.
Train: 2018-08-04T23:36:58.213030: step 5146, loss 0.553983.
Train: 2018-08-04T23:36:58.463001: step 5147, loss 0.537184.
Train: 2018-08-04T23:36:58.697291: step 5148, loss 0.587594.
Train: 2018-08-04T23:36:58.947234: step 5149, loss 0.62118.
Train: 2018-08-04T23:36:59.212796: step 5150, loss 0.612697.
Test: 2018-08-04T23:37:00.478123: step 5150, loss 0.548453.
Train: 2018-08-04T23:37:00.728066: step 5151, loss 0.595848.
Train: 2018-08-04T23:37:00.978036: step 5152, loss 0.545775.
Train: 2018-08-04T23:37:01.243600: step 5153, loss 0.554149.
Train: 2018-08-04T23:37:01.493542: step 5154, loss 0.53762.
Train: 2018-08-04T23:37:01.743452: step 5155, loss 0.570756.
Train: 2018-08-04T23:37:01.993424: step 5156, loss 0.628532.
Train: 2018-08-04T23:37:02.243361: step 5157, loss 0.587211.
Train: 2018-08-04T23:37:02.508899: step 5158, loss 0.562565.
Train: 2018-08-04T23:37:02.743244: step 5159, loss 0.505402.
Train: 2018-08-04T23:37:02.993190: step 5160, loss 0.513669.
Test: 2018-08-04T23:37:04.258488: step 5160, loss 0.548999.
Train: 2018-08-04T23:37:04.508430: step 5161, loss 0.570774.
Train: 2018-08-04T23:37:04.773993: step 5162, loss 0.530018.
Train: 2018-08-04T23:37:05.008344: step 5163, loss 0.652315.
Train: 2018-08-04T23:37:05.258255: step 5164, loss 0.619616.
Train: 2018-08-04T23:37:05.508225: step 5165, loss 0.505866.
Train: 2018-08-04T23:37:05.758168: step 5166, loss 0.538374.
Train: 2018-08-04T23:37:06.008103: step 5167, loss 0.554595.
Train: 2018-08-04T23:37:06.258021: step 5168, loss 0.514109.
Train: 2018-08-04T23:37:06.507992: step 5169, loss 0.530264.
Train: 2018-08-04T23:37:06.757932: step 5170, loss 0.54643.
Test: 2018-08-04T23:37:08.007610: step 5170, loss 0.5485.
Train: 2018-08-04T23:37:08.241955: step 5171, loss 0.473168.
Train: 2018-08-04T23:37:08.491902: step 5172, loss 0.619762.
Train: 2018-08-04T23:37:08.741814: step 5173, loss 0.570766.
Train: 2018-08-04T23:37:08.991785: step 5174, loss 0.529785.
Train: 2018-08-04T23:37:09.241727: step 5175, loss 0.578973.
Train: 2018-08-04T23:37:09.491637: step 5176, loss 0.620117.
Train: 2018-08-04T23:37:09.741610: step 5177, loss 0.537851.
Train: 2018-08-04T23:37:09.991551: step 5178, loss 0.603683.
Train: 2018-08-04T23:37:10.241463: step 5179, loss 0.570758.
Train: 2018-08-04T23:37:10.507057: step 5180, loss 0.488539.
Test: 2018-08-04T23:37:11.741111: step 5180, loss 0.548402.
Train: 2018-08-04T23:37:11.991083: step 5181, loss 0.636602.
Train: 2018-08-04T23:37:12.241024: step 5182, loss 0.55431.
Train: 2018-08-04T23:37:12.490965: step 5183, loss 0.570759.
Train: 2018-08-04T23:37:12.740902: step 5184, loss 0.595399.
Train: 2018-08-04T23:37:12.990854: step 5185, loss 0.603566.
Train: 2018-08-04T23:37:13.240786: step 5186, loss 0.554401.
Train: 2018-08-04T23:37:13.490740: step 5187, loss 0.587104.
Train: 2018-08-04T23:37:13.740644: step 5188, loss 0.570776.
Train: 2018-08-04T23:37:13.990616: step 5189, loss 0.587043.
Train: 2018-08-04T23:37:14.240556: step 5190, loss 0.538357.
Test: 2018-08-04T23:37:15.521476: step 5190, loss 0.549558.
Train: 2018-08-04T23:37:15.771418: step 5191, loss 0.53842.
Train: 2018-08-04T23:37:16.052633: step 5192, loss 0.530368.
Train: 2018-08-04T23:37:16.318166: step 5193, loss 0.546543.
Train: 2018-08-04T23:37:16.552509: step 5194, loss 0.562712.
Train: 2018-08-04T23:37:16.802456: step 5195, loss 0.473685.
Train: 2018-08-04T23:37:17.052393: step 5196, loss 0.603243.
Train: 2018-08-04T23:37:17.317954: step 5197, loss 0.570786.
Train: 2018-08-04T23:37:17.567896: step 5198, loss 0.554519.
Train: 2018-08-04T23:37:17.802223: step 5199, loss 0.562638.
Train: 2018-08-04T23:37:18.067755: step 5200, loss 0.570776.
Test: 2018-08-04T23:37:19.317462: step 5200, loss 0.549.
Train: 2018-08-04T23:37:20.207903: step 5201, loss 0.570774.
Train: 2018-08-04T23:37:20.457853: step 5202, loss 0.570773.
Train: 2018-08-04T23:37:20.707791: step 5203, loss 0.595253.
Train: 2018-08-04T23:37:20.957733: step 5204, loss 0.529995.
Train: 2018-08-04T23:37:21.207644: step 5205, loss 0.505511.
Train: 2018-08-04T23:37:21.457616: step 5206, loss 0.562599.
Train: 2018-08-04T23:37:21.707557: step 5207, loss 0.554403.
Train: 2018-08-04T23:37:21.957494: step 5208, loss 0.595343.
Train: 2018-08-04T23:37:22.207436: step 5209, loss 0.505184.
Train: 2018-08-04T23:37:22.457377: step 5210, loss 0.546125.
Test: 2018-08-04T23:37:23.707059: step 5210, loss 0.549169.
Train: 2018-08-04T23:37:23.957025: step 5211, loss 0.546076.
Train: 2018-08-04T23:37:24.206942: step 5212, loss 0.546024.
Train: 2018-08-04T23:37:24.456883: step 5213, loss 0.529447.
Train: 2018-08-04T23:37:24.706855: step 5214, loss 0.57904.
Train: 2018-08-04T23:37:24.956796: step 5215, loss 0.520959.
Train: 2018-08-04T23:37:25.206709: step 5216, loss 0.587402.
Train: 2018-08-04T23:37:25.456680: step 5217, loss 0.595757.
Train: 2018-08-04T23:37:25.706621: step 5218, loss 0.587464.
Train: 2018-08-04T23:37:25.956563: step 5219, loss 0.520727.
Train: 2018-08-04T23:37:26.206504: step 5220, loss 0.537371.
Test: 2018-08-04T23:37:27.456181: step 5220, loss 0.548262.
Train: 2018-08-04T23:37:27.706153: step 5221, loss 0.554047.
Train: 2018-08-04T23:37:27.956094: step 5222, loss 0.595948.
Train: 2018-08-04T23:37:28.206037: step 5223, loss 0.554082.
Train: 2018-08-04T23:37:28.455977: step 5224, loss 0.579153.
Train: 2018-08-04T23:37:28.705918: step 5225, loss 0.554053.
Train: 2018-08-04T23:37:28.955830: step 5226, loss 0.620908.
Train: 2018-08-04T23:37:29.205801: step 5227, loss 0.554086.
Train: 2018-08-04T23:37:29.455742: step 5228, loss 0.537423.
Train: 2018-08-04T23:37:29.705684: step 5229, loss 0.520845.
Train: 2018-08-04T23:37:29.955597: step 5230, loss 0.579106.
Test: 2018-08-04T23:37:31.205303: step 5230, loss 0.54969.
Train: 2018-08-04T23:37:31.439649: step 5231, loss 0.579051.
Train: 2018-08-04T23:37:31.689590: step 5232, loss 0.545836.
Train: 2018-08-04T23:37:31.939505: step 5233, loss 0.562468.
Train: 2018-08-04T23:37:32.189449: step 5234, loss 0.529233.
Train: 2018-08-04T23:37:32.439420: step 5235, loss 0.57076.
Train: 2018-08-04T23:37:32.689360: step 5236, loss 0.520966.
Train: 2018-08-04T23:37:32.939302: step 5237, loss 0.57907.
Train: 2018-08-04T23:37:33.189239: step 5238, loss 0.545826.
Train: 2018-08-04T23:37:33.439185: step 5239, loss 0.545808.
Train: 2018-08-04T23:37:33.689127: step 5240, loss 0.545797.
Test: 2018-08-04T23:37:34.938803: step 5240, loss 0.548527.
Train: 2018-08-04T23:37:35.173155: step 5241, loss 0.579099.
Train: 2018-08-04T23:37:35.423096: step 5242, loss 0.479127.
Train: 2018-08-04T23:37:35.673007: step 5243, loss 0.604163.
Train: 2018-08-04T23:37:35.922978: step 5244, loss 0.595824.
Train: 2018-08-04T23:37:36.172891: step 5245, loss 0.63758.
Train: 2018-08-04T23:37:36.422861: step 5246, loss 0.570764.
Train: 2018-08-04T23:37:36.672773: step 5247, loss 0.562444.
Train: 2018-08-04T23:37:36.922714: step 5248, loss 0.58736.
Train: 2018-08-04T23:37:37.172686: step 5249, loss 0.554196.
Train: 2018-08-04T23:37:37.438248: step 5250, loss 0.562493.
Test: 2018-08-04T23:37:38.687927: step 5250, loss 0.548926.
Train: 2018-08-04T23:37:38.922247: step 5251, loss 0.488283.
Train: 2018-08-04T23:37:39.172218: step 5252, loss 0.480038.
Train: 2018-08-04T23:37:39.422154: step 5253, loss 0.521147.
Train: 2018-08-04T23:37:39.672071: step 5254, loss 0.595704.
Train: 2018-08-04T23:37:39.922042: step 5255, loss 0.537562.
Train: 2018-08-04T23:37:40.171984: step 5256, loss 0.529282.
Train: 2018-08-04T23:37:40.421896: step 5257, loss 0.579126.
Train: 2018-08-04T23:37:40.671867: step 5258, loss 0.554099.
Train: 2018-08-04T23:37:40.921808: step 5259, loss 0.537378.
Train: 2018-08-04T23:37:41.171752: step 5260, loss 0.62094.
Test: 2018-08-04T23:37:42.421426: step 5260, loss 0.549222.
Train: 2018-08-04T23:37:42.655781: step 5261, loss 0.637635.
Train: 2018-08-04T23:37:42.905704: step 5262, loss 0.629159.
Train: 2018-08-04T23:37:43.155663: step 5263, loss 0.587337.
Train: 2018-08-04T23:37:43.405600: step 5264, loss 0.637022.
Train: 2018-08-04T23:37:43.655547: step 5265, loss 0.562366.
Train: 2018-08-04T23:37:43.905488: step 5266, loss 0.603129.
Train: 2018-08-04T23:37:44.155429: step 5267, loss 0.538727.
Train: 2018-08-04T23:37:44.405338: step 5268, loss 0.594056.
Train: 2018-08-04T23:37:44.655303: step 5269, loss 0.586841.
Train: 2018-08-04T23:37:44.905221: step 5270, loss 0.531042.
Test: 2018-08-04T23:37:46.154927: step 5270, loss 0.549685.
Train: 2018-08-04T23:37:46.404870: step 5271, loss 0.54662.
Train: 2018-08-04T23:37:46.654844: step 5272, loss 0.515877.
Train: 2018-08-04T23:37:46.904759: step 5273, loss 0.595905.
Train: 2018-08-04T23:37:47.154694: step 5274, loss 0.578582.
Train: 2018-08-04T23:37:47.404669: step 5275, loss 0.507153.
Train: 2018-08-04T23:37:47.654610: step 5276, loss 0.571003.
Train: 2018-08-04T23:37:47.935791: step 5277, loss 0.562926.
Train: 2018-08-04T23:37:48.185729: step 5278, loss 0.610928.
Train: 2018-08-04T23:37:48.435643: step 5279, loss 0.554833.
Train: 2018-08-04T23:37:48.685585: step 5280, loss 0.570858.
Test: 2018-08-04T23:37:49.935292: step 5280, loss 0.549689.
Train: 2018-08-04T23:37:50.200854: step 5281, loss 0.54687.
Train: 2018-08-04T23:37:50.466448: step 5282, loss 0.554868.
Train: 2018-08-04T23:37:50.716390: step 5283, loss 0.570826.
Train: 2018-08-04T23:37:50.981946: step 5284, loss 0.554881.
Train: 2018-08-04T23:37:51.169404: step 5285, loss 0.443725.
Train: 2018-08-04T23:37:51.419350: step 5286, loss 0.578937.
Train: 2018-08-04T23:37:51.684882: step 5287, loss 0.578856.
Train: 2018-08-04T23:37:51.950445: step 5288, loss 0.50625.
Train: 2018-08-04T23:37:52.200417: step 5289, loss 0.57888.
Train: 2018-08-04T23:37:52.450329: step 5290, loss 0.61138.
Test: 2018-08-04T23:37:53.715656: step 5290, loss 0.54794.
Train: 2018-08-04T23:37:53.950007: step 5291, loss 0.546359.
Train: 2018-08-04T23:37:54.199919: step 5292, loss 0.546384.
Train: 2018-08-04T23:37:54.449884: step 5293, loss 0.619755.
Train: 2018-08-04T23:37:54.699827: step 5294, loss 0.546304.
Train: 2018-08-04T23:37:54.949742: step 5295, loss 0.546251.
Train: 2018-08-04T23:37:55.199714: step 5296, loss 0.603485.
Train: 2018-08-04T23:37:55.449656: step 5297, loss 0.52985.
Train: 2018-08-04T23:37:55.699598: step 5298, loss 0.529829.
Train: 2018-08-04T23:37:55.949533: step 5299, loss 0.562565.
Train: 2018-08-04T23:37:56.199481: step 5300, loss 0.578968.
Test: 2018-08-04T23:37:57.449158: step 5300, loss 0.547886.
Train: 2018-08-04T23:37:58.386463: step 5301, loss 0.595398.
Train: 2018-08-04T23:37:58.636410: step 5302, loss 0.537919.
Train: 2018-08-04T23:37:58.886349: step 5303, loss 0.636461.
Train: 2018-08-04T23:37:59.136290: step 5304, loss 0.554356.
Train: 2018-08-04T23:37:59.386203: step 5305, loss 0.578947.
Train: 2018-08-04T23:37:59.636177: step 5306, loss 0.619826.
Train: 2018-08-04T23:37:59.886114: step 5307, loss 0.546369.
Train: 2018-08-04T23:38:00.136060: step 5308, loss 0.619601.
Train: 2018-08-04T23:38:00.385998: step 5309, loss 0.522131.
Train: 2018-08-04T23:38:00.635941: step 5310, loss 0.55461.
Test: 2018-08-04T23:38:01.885618: step 5310, loss 0.54829.
Train: 2018-08-04T23:38:02.135561: step 5311, loss 0.570801.
Train: 2018-08-04T23:38:02.385534: step 5312, loss 0.554624.
Train: 2018-08-04T23:38:02.635471: step 5313, loss 0.586944.
Train: 2018-08-04T23:38:02.885419: step 5314, loss 0.562725.
Train: 2018-08-04T23:38:03.166593: step 5315, loss 0.619056.
Train: 2018-08-04T23:38:03.447754: step 5316, loss 0.602918.
Train: 2018-08-04T23:38:03.713341: step 5317, loss 0.538957.
Train: 2018-08-04T23:38:04.025744: step 5318, loss 0.578871.
Train: 2018-08-04T23:38:04.338170: step 5319, loss 0.523117.
Train: 2018-08-04T23:38:04.588114: step 5320, loss 0.547066.
Test: 2018-08-04T23:38:05.884682: step 5320, loss 0.550034.
Train: 2018-08-04T23:38:06.134624: step 5321, loss 0.562937.
Train: 2018-08-04T23:38:06.384593: step 5322, loss 0.555019.
Train: 2018-08-04T23:38:06.634508: step 5323, loss 0.618662.
Train: 2018-08-04T23:38:06.884480: step 5324, loss 0.555006.
Train: 2018-08-04T23:38:07.134419: step 5325, loss 0.578859.
Train: 2018-08-04T23:38:07.384361: step 5326, loss 0.634444.
Train: 2018-08-04T23:38:07.634273: step 5327, loss 0.523405.
Train: 2018-08-04T23:38:07.884244: step 5328, loss 0.523462.
Train: 2018-08-04T23:38:08.134183: step 5329, loss 0.547193.
Train: 2018-08-04T23:38:08.384125: step 5330, loss 0.563013.
Test: 2018-08-04T23:38:09.633804: step 5330, loss 0.549393.
Train: 2018-08-04T23:38:09.868158: step 5331, loss 0.642295.
Train: 2018-08-04T23:38:10.118066: step 5332, loss 0.610544.
Train: 2018-08-04T23:38:10.368032: step 5333, loss 0.515624.
Train: 2018-08-04T23:38:10.617950: step 5334, loss 0.507748.
Train: 2018-08-04T23:38:10.867890: step 5335, loss 0.618422.
Train: 2018-08-04T23:38:11.117862: step 5336, loss 0.570947.
Train: 2018-08-04T23:38:11.367774: step 5337, loss 0.507687.
Train: 2018-08-04T23:38:11.633336: step 5338, loss 0.491749.
Train: 2018-08-04T23:38:11.898933: step 5339, loss 0.602698.
Train: 2018-08-04T23:38:12.148841: step 5340, loss 0.523137.
Test: 2018-08-04T23:38:13.414169: step 5340, loss 0.548826.
Train: 2018-08-04T23:38:13.679732: step 5341, loss 0.570875.
Train: 2018-08-04T23:38:13.976565: step 5342, loss 0.546847.
Train: 2018-08-04T23:38:14.288964: step 5343, loss 0.530712.
Train: 2018-08-04T23:38:14.601391: step 5344, loss 0.554718.
Train: 2018-08-04T23:38:14.913818: step 5345, loss 0.55465.
Train: 2018-08-04T23:38:15.179381: step 5346, loss 0.530271.
Train: 2018-08-04T23:38:15.429322: step 5347, loss 0.58705.
Train: 2018-08-04T23:38:15.694915: step 5348, loss 0.636023.
Train: 2018-08-04T23:38:15.960481: step 5349, loss 0.570772.
Train: 2018-08-04T23:38:16.210419: step 5350, loss 0.55444.
Test: 2018-08-04T23:38:17.522583: step 5350, loss 0.549143.
Train: 2018-08-04T23:38:17.772526: step 5351, loss 0.587109.
Train: 2018-08-04T23:38:18.053707: step 5352, loss 0.578939.
Train: 2018-08-04T23:38:18.303680: step 5353, loss 0.529942.
Train: 2018-08-04T23:38:18.569212: step 5354, loss 0.57077.
Train: 2018-08-04T23:38:18.834805: step 5355, loss 0.529924.
Train: 2018-08-04T23:38:19.084747: step 5356, loss 0.562592.
Train: 2018-08-04T23:38:19.334688: step 5357, loss 0.488941.
Train: 2018-08-04T23:38:19.584630: step 5358, loss 0.570762.
Train: 2018-08-04T23:38:19.850192: step 5359, loss 0.505016.
Train: 2018-08-04T23:38:20.115725: step 5360, loss 0.611971.
Test: 2018-08-04T23:38:21.365432: step 5360, loss 0.550044.
Train: 2018-08-04T23:38:21.599778: step 5361, loss 0.545989.
Train: 2018-08-04T23:38:21.849724: step 5362, loss 0.562487.
Train: 2018-08-04T23:38:22.099666: step 5363, loss 0.562474.
Train: 2018-08-04T23:38:22.349610: step 5364, loss 0.529297.
Train: 2018-08-04T23:38:22.615165: step 5365, loss 0.612286.
Train: 2018-08-04T23:38:22.865114: step 5366, loss 0.554138.
Train: 2018-08-04T23:38:23.130672: step 5367, loss 0.5957.
Train: 2018-08-04T23:38:23.380586: step 5368, loss 0.529211.
Train: 2018-08-04T23:38:23.630526: step 5369, loss 0.628952.
Train: 2018-08-04T23:38:23.880469: step 5370, loss 0.587352.
Test: 2018-08-04T23:38:25.130176: step 5370, loss 0.549592.
Train: 2018-08-04T23:38:25.395772: step 5371, loss 0.587325.
Train: 2018-08-04T23:38:25.645714: step 5372, loss 0.545972.
Train: 2018-08-04T23:38:25.895654: step 5373, loss 0.587256.
Train: 2018-08-04T23:38:26.145598: step 5374, loss 0.562528.
Train: 2018-08-04T23:38:26.395538: step 5375, loss 0.455807.
Train: 2018-08-04T23:38:26.645471: step 5376, loss 0.603619.
Train: 2018-08-04T23:38:26.911008: step 5377, loss 0.521489.
Train: 2018-08-04T23:38:27.145331: step 5378, loss 0.554327.
Train: 2018-08-04T23:38:27.395305: step 5379, loss 0.636519.
Train: 2018-08-04T23:38:27.645212: step 5380, loss 0.546133.
Test: 2018-08-04T23:38:28.910540: step 5380, loss 0.547536.
Train: 2018-08-04T23:38:29.144894: step 5381, loss 0.611775.
Train: 2018-08-04T23:38:29.394831: step 5382, loss 0.570764.
Train: 2018-08-04T23:38:29.660365: step 5383, loss 0.546251.
Train: 2018-08-04T23:38:29.894709: step 5384, loss 0.546293.
Train: 2018-08-04T23:38:30.144667: step 5385, loss 0.578924.
Train: 2018-08-04T23:38:30.394568: step 5386, loss 0.587076.
Train: 2018-08-04T23:38:30.644564: step 5387, loss 0.505732.
Train: 2018-08-04T23:38:30.894450: step 5388, loss 0.54639.
Train: 2018-08-04T23:38:31.144426: step 5389, loss 0.61958.
Train: 2018-08-04T23:38:31.394368: step 5390, loss 0.505786.
Test: 2018-08-04T23:38:32.644040: step 5390, loss 0.548706.
Train: 2018-08-04T23:38:32.940876: step 5391, loss 0.570783.
Train: 2018-08-04T23:38:33.190819: step 5392, loss 0.530132.
Train: 2018-08-04T23:38:33.456376: step 5393, loss 0.554503.
Train: 2018-08-04T23:38:33.706322: step 5394, loss 0.603365.
Train: 2018-08-04T23:38:33.956265: step 5395, loss 0.521894.
Train: 2018-08-04T23:38:34.206207: step 5396, loss 0.538152.
Train: 2018-08-04T23:38:34.456147: step 5397, loss 0.578937.
Train: 2018-08-04T23:38:34.706089: step 5398, loss 0.529893.
Train: 2018-08-04T23:38:34.971651: step 5399, loss 0.61989.
Train: 2018-08-04T23:38:35.221593: step 5400, loss 0.554389.
Test: 2018-08-04T23:38:36.486890: step 5400, loss 0.548519.
Train: 2018-08-04T23:38:37.346095: step 5401, loss 0.587144.
Train: 2018-08-04T23:38:37.611658: step 5402, loss 0.603514.
Train: 2018-08-04T23:38:37.877191: step 5403, loss 0.578947.
Train: 2018-08-04T23:38:38.127167: step 5404, loss 0.546283.
Train: 2018-08-04T23:38:38.377110: step 5405, loss 0.464775.
Train: 2018-08-04T23:38:38.642636: step 5406, loss 0.578936.
Train: 2018-08-04T23:38:38.892612: step 5407, loss 0.538083.
Train: 2018-08-04T23:38:39.142520: step 5408, loss 0.538036.
Train: 2018-08-04T23:38:39.408112: step 5409, loss 0.480601.
Train: 2018-08-04T23:38:39.658053: step 5410, loss 0.529636.
Test: 2018-08-04T23:38:40.970216: step 5410, loss 0.549092.
Train: 2018-08-04T23:38:41.220182: step 5411, loss 0.529478.
Train: 2018-08-04T23:38:41.470100: step 5412, loss 0.554177.
Train: 2018-08-04T23:38:41.720070: step 5413, loss 0.579087.
Train: 2018-08-04T23:38:41.970007: step 5414, loss 0.529038.
Train: 2018-08-04T23:38:42.219954: step 5415, loss 0.545654.
Train: 2018-08-04T23:38:42.469896: step 5416, loss 0.570785.
Train: 2018-08-04T23:38:42.719807: step 5417, loss 0.537112.
Train: 2018-08-04T23:38:42.985370: step 5418, loss 0.553924.
Train: 2018-08-04T23:38:43.282175: step 5419, loss 0.545421.
Train: 2018-08-04T23:38:43.547738: step 5420, loss 0.562342.
Test: 2018-08-04T23:38:44.781823: step 5420, loss 0.548346.
Train: 2018-08-04T23:38:45.016145: step 5421, loss 0.502861.
Train: 2018-08-04T23:38:45.281731: step 5422, loss 0.570866.
Train: 2018-08-04T23:38:45.531678: step 5423, loss 0.56233.
Train: 2018-08-04T23:38:45.781623: step 5424, loss 0.562355.
Train: 2018-08-04T23:38:46.031560: step 5425, loss 0.528098.
Train: 2018-08-04T23:38:46.281502: step 5426, loss 0.536588.
Train: 2018-08-04T23:38:46.531440: step 5427, loss 0.527967.
Train: 2018-08-04T23:38:46.781383: step 5428, loss 0.536487.
Train: 2018-08-04T23:38:47.031327: step 5429, loss 0.63139.
Train: 2018-08-04T23:38:47.281268: step 5430, loss 0.570967.
Test: 2018-08-04T23:38:48.530945: step 5430, loss 0.547334.
Train: 2018-08-04T23:38:48.780913: step 5431, loss 0.57097.
Train: 2018-08-04T23:38:49.030859: step 5432, loss 0.596823.
Train: 2018-08-04T23:38:49.280800: step 5433, loss 0.57094.
Train: 2018-08-04T23:38:49.530741: step 5434, loss 0.553748.
Train: 2018-08-04T23:38:49.811896: step 5435, loss 0.562335.
Train: 2018-08-04T23:38:50.014974: step 5436, loss 0.635332.
Train: 2018-08-04T23:38:50.264914: step 5437, loss 0.485629.
Train: 2018-08-04T23:38:50.530477: step 5438, loss 0.596377.
Train: 2018-08-04T23:38:50.780444: step 5439, loss 0.519917.
Train: 2018-08-04T23:38:51.045982: step 5440, loss 0.503042.
Test: 2018-08-04T23:38:52.295688: step 5440, loss 0.548208.
Train: 2018-08-04T23:38:52.545630: step 5441, loss 0.553884.
Train: 2018-08-04T23:38:52.826814: step 5442, loss 0.553886.
Train: 2018-08-04T23:38:53.076757: step 5443, loss 0.579287.
Train: 2018-08-04T23:38:53.326728: step 5444, loss 0.621568.
Train: 2018-08-04T23:38:53.576669: step 5445, loss 0.520166.
Train: 2018-08-04T23:38:53.842226: step 5446, loss 0.545511.
Train: 2018-08-04T23:38:54.107764: step 5447, loss 0.553955.
Train: 2018-08-04T23:38:54.357707: step 5448, loss 0.562379.
Train: 2018-08-04T23:38:54.623302: step 5449, loss 0.511938.
Train: 2018-08-04T23:38:54.888866: step 5450, loss 0.596017.
Test: 2018-08-04T23:38:56.122918: step 5450, loss 0.548933.
Train: 2018-08-04T23:38:56.372887: step 5451, loss 0.595993.
Train: 2018-08-04T23:38:56.622801: step 5452, loss 0.621114.
Train: 2018-08-04T23:38:56.872768: step 5453, loss 0.54568.
Train: 2018-08-04T23:38:57.122713: step 5454, loss 0.595802.
Train: 2018-08-04T23:38:57.372626: step 5455, loss 0.587403.
Train: 2018-08-04T23:38:57.622594: step 5456, loss 0.579049.
Train: 2018-08-04T23:38:57.872537: step 5457, loss 0.570755.
Train: 2018-08-04T23:38:58.122475: step 5458, loss 0.554292.
Train: 2018-08-04T23:38:58.372424: step 5459, loss 0.537927.
Train: 2018-08-04T23:38:58.622362: step 5460, loss 0.505239.
Test: 2018-08-04T23:38:59.872039: step 5460, loss 0.548344.
Train: 2018-08-04T23:39:00.106361: step 5461, loss 0.578951.
Train: 2018-08-04T23:39:00.356332: step 5462, loss 0.570767.
Train: 2018-08-04T23:39:00.606276: step 5463, loss 0.5626.
Train: 2018-08-04T23:39:00.871817: step 5464, loss 0.611577.
Train: 2018-08-04T23:39:01.106125: step 5465, loss 0.513767.
Train: 2018-08-04T23:39:01.371717: step 5466, loss 0.513807.
Train: 2018-08-04T23:39:01.606043: step 5467, loss 0.635927.
Train: 2018-08-04T23:39:01.855951: step 5468, loss 0.570782.
Train: 2018-08-04T23:39:02.105891: step 5469, loss 0.538293.
Train: 2018-08-04T23:39:02.355864: step 5470, loss 0.538312.
Test: 2018-08-04T23:39:03.605540: step 5470, loss 0.54892.
Train: 2018-08-04T23:39:03.839893: step 5471, loss 0.538311.
Train: 2018-08-04T23:39:04.089801: step 5472, loss 0.627663.
Train: 2018-08-04T23:39:04.339774: step 5473, loss 0.578905.
Train: 2018-08-04T23:39:04.605307: step 5474, loss 0.578902.
Train: 2018-08-04T23:39:04.870870: step 5475, loss 0.457443.
Train: 2018-08-04T23:39:05.120840: step 5476, loss 0.554582.
Train: 2018-08-04T23:39:05.386373: step 5477, loss 0.522078.
Train: 2018-08-04T23:39:05.636316: step 5478, loss 0.570781.
Train: 2018-08-04T23:39:05.886284: step 5479, loss 0.595227.
Train: 2018-08-04T23:39:06.136232: step 5480, loss 0.587092.
Test: 2018-08-04T23:39:07.385905: step 5480, loss 0.549735.
Train: 2018-08-04T23:39:07.635880: step 5481, loss 0.578931.
Train: 2018-08-04T23:39:07.885822: step 5482, loss 0.546298.
Train: 2018-08-04T23:39:08.135763: step 5483, loss 0.570771.
Train: 2018-08-04T23:39:08.385705: step 5484, loss 0.587094.
Train: 2018-08-04T23:39:08.635637: step 5485, loss 0.595239.
Train: 2018-08-04T23:39:08.885588: step 5486, loss 0.481194.
Train: 2018-08-04T23:39:09.135496: step 5487, loss 0.562625.
Train: 2018-08-04T23:39:09.385438: step 5488, loss 0.59524.
Train: 2018-08-04T23:39:09.635413: step 5489, loss 0.56262.
Train: 2018-08-04T23:39:09.885319: step 5490, loss 0.578927.
Test: 2018-08-04T23:39:11.145182: step 5490, loss 0.548643.
Train: 2018-08-04T23:39:11.386537: step 5491, loss 0.538181.
Train: 2018-08-04T23:39:11.633903: step 5492, loss 0.538176.
Train: 2018-08-04T23:39:11.885205: step 5493, loss 0.619706.
Train: 2018-08-04T23:39:12.128586: step 5494, loss 0.521879.
Train: 2018-08-04T23:39:12.374896: step 5495, loss 0.50556.
Train: 2018-08-04T23:39:12.629215: step 5496, loss 0.55444.
Train: 2018-08-04T23:39:12.880574: step 5497, loss 0.636198.
Train: 2018-08-04T23:39:13.123892: step 5498, loss 0.660711.
Train: 2018-08-04T23:39:13.371265: step 5499, loss 0.529995.
Train: 2018-08-04T23:39:13.626578: step 5500, loss 0.546349.
Test: 2018-08-04T23:39:14.887179: step 5500, loss 0.548311.
Train: 2018-08-04T23:39:15.778486: step 5501, loss 0.63586.
Train: 2018-08-04T23:39:16.036794: step 5502, loss 0.578904.
Train: 2018-08-04T23:39:16.287099: step 5503, loss 0.603166.
Train: 2018-08-04T23:39:16.530450: step 5504, loss 0.55469.
Train: 2018-08-04T23:39:16.772832: step 5505, loss 0.530629.
Train: 2018-08-04T23:39:17.023133: step 5506, loss 0.586896.
Train: 2018-08-04T23:39:17.272466: step 5507, loss 0.546823.
Train: 2018-08-04T23:39:17.515815: step 5508, loss 0.562863.
Train: 2018-08-04T23:39:17.760187: step 5509, loss 0.602835.
Train: 2018-08-04T23:39:18.014513: step 5510, loss 0.586836.
Test: 2018-08-04T23:39:19.273117: step 5510, loss 0.550749.
Train: 2018-08-04T23:39:19.507523: step 5511, loss 0.523149.
Train: 2018-08-04T23:39:19.752861: step 5512, loss 0.531147.
Train: 2018-08-04T23:39:19.996185: step 5513, loss 0.507279.
Train: 2018-08-04T23:39:20.240531: step 5514, loss 0.475292.
Train: 2018-08-04T23:39:20.499869: step 5515, loss 0.546874.
Train: 2018-08-04T23:39:20.743200: step 5516, loss 0.530699.
Train: 2018-08-04T23:39:20.992549: step 5517, loss 0.554691.
Train: 2018-08-04T23:39:21.241854: step 5518, loss 0.522212.
Train: 2018-08-04T23:39:21.490216: step 5519, loss 0.587054.
Train: 2018-08-04T23:39:21.742544: step 5520, loss 0.505439.
Test: 2018-08-04T23:39:22.987189: step 5520, loss 0.549987.
Train: 2018-08-04T23:39:23.221591: step 5521, loss 0.578964.
Train: 2018-08-04T23:39:23.465937: step 5522, loss 0.496627.
Train: 2018-08-04T23:39:23.729233: step 5523, loss 0.570727.
Train: 2018-08-04T23:39:23.976569: step 5524, loss 0.58738.
Train: 2018-08-04T23:39:24.221889: step 5525, loss 0.57076.
Train: 2018-08-04T23:39:24.474247: step 5526, loss 0.520559.
Train: 2018-08-04T23:39:24.719558: step 5527, loss 0.612735.
Train: 2018-08-04T23:39:24.964902: step 5528, loss 0.562262.
Train: 2018-08-04T23:39:25.212267: step 5529, loss 0.596285.
Train: 2018-08-04T23:39:25.456587: step 5530, loss 0.553938.
Test: 2018-08-04T23:39:26.716831: step 5530, loss 0.548534.
Train: 2018-08-04T23:39:26.982419: step 5531, loss 0.553862.
Train: 2018-08-04T23:39:27.232335: step 5532, loss 0.520372.
Train: 2018-08-04T23:39:27.482307: step 5533, loss 0.587527.
Train: 2018-08-04T23:39:27.747840: step 5534, loss 0.604487.
Train: 2018-08-04T23:39:27.982161: step 5535, loss 0.545412.
Train: 2018-08-04T23:39:28.232132: step 5536, loss 0.638269.
Train: 2018-08-04T23:39:28.497691: step 5537, loss 0.596063.
Train: 2018-08-04T23:39:28.747606: step 5538, loss 0.529007.
Train: 2018-08-04T23:39:28.997578: step 5539, loss 0.562445.
Train: 2018-08-04T23:39:29.263136: step 5540, loss 0.504326.
Test: 2018-08-04T23:39:30.512818: step 5540, loss 0.548028.
Train: 2018-08-04T23:39:30.747138: step 5541, loss 0.570757.
Train: 2018-08-04T23:39:30.997110: step 5542, loss 0.496129.
Train: 2018-08-04T23:39:31.262643: step 5543, loss 0.512671.
Train: 2018-08-04T23:39:31.512614: step 5544, loss 0.595701.
Train: 2018-08-04T23:39:31.762549: step 5545, loss 0.604048.
Train: 2018-08-04T23:39:32.059363: step 5546, loss 0.628952.
Train: 2018-08-04T23:39:32.309272: step 5547, loss 0.545868.
Train: 2018-08-04T23:39:32.652942: step 5548, loss 0.587321.
Train: 2018-08-04T23:39:33.137203: step 5549, loss 0.628621.
Train: 2018-08-04T23:39:33.387145: step 5550, loss 0.546053.
Test: 2018-08-04T23:39:34.652473: step 5550, loss 0.549967.
Train: 2018-08-04T23:39:34.886795: step 5551, loss 0.587187.
Train: 2018-08-04T23:39:35.136735: step 5552, loss 0.578945.
Train: 2018-08-04T23:39:35.417949: step 5553, loss 0.529988.
Train: 2018-08-04T23:39:35.652239: step 5554, loss 0.595206.
Train: 2018-08-04T23:39:35.917833: step 5555, loss 0.546428.
Train: 2018-08-04T23:39:36.183396: step 5556, loss 0.522185.
Train: 2018-08-04T23:39:36.433307: step 5557, loss 0.465562.
Train: 2018-08-04T23:39:36.683279: step 5558, loss 0.603229.
Train: 2018-08-04T23:39:36.933219: step 5559, loss 0.65192.
Train: 2018-08-04T23:39:37.183156: step 5560, loss 0.570794.
Test: 2018-08-04T23:39:38.448459: step 5560, loss 0.549216.
Train: 2018-08-04T23:39:38.682810: step 5561, loss 0.570805.
Train: 2018-08-04T23:39:38.932747: step 5562, loss 0.465882.
Train: 2018-08-04T23:39:39.182693: step 5563, loss 0.554632.
Train: 2018-08-04T23:39:39.432634: step 5564, loss 0.546544.
Train: 2018-08-04T23:39:39.698166: step 5565, loss 0.578881.
Train: 2018-08-04T23:39:39.963729: step 5566, loss 0.554584.
Train: 2018-08-04T23:39:40.213671: step 5567, loss 0.554584.
Train: 2018-08-04T23:39:40.479265: step 5568, loss 0.497461.
Train: 2018-08-04T23:39:40.744832: step 5569, loss 0.587027.
Train: 2018-08-04T23:39:40.979118: step 5570, loss 0.529608.
Test: 2018-08-04T23:39:42.244445: step 5570, loss 0.548964.
Train: 2018-08-04T23:39:42.494387: step 5571, loss 0.59576.
Train: 2018-08-04T23:39:42.744360: step 5572, loss 0.562537.
Train: 2018-08-04T23:39:42.994300: step 5573, loss 0.636608.
Train: 2018-08-04T23:39:43.244241: step 5574, loss 0.612314.
Train: 2018-08-04T23:39:43.494183: step 5575, loss 0.644857.
Train: 2018-08-04T23:39:43.744119: step 5576, loss 0.603365.
Train: 2018-08-04T23:39:43.994065: step 5577, loss 0.473516.
Train: 2018-08-04T23:39:44.243977: step 5578, loss 0.538427.
Train: 2018-08-04T23:39:44.493949: step 5579, loss 0.595069.
Train: 2018-08-04T23:39:44.743861: step 5580, loss 0.570811.
Test: 2018-08-04T23:39:46.005078: step 5580, loss 0.549653.
Train: 2018-08-04T23:39:46.239429: step 5581, loss 0.538576.
Train: 2018-08-04T23:39:46.489365: step 5582, loss 0.554691.
Train: 2018-08-04T23:39:46.739311: step 5583, loss 0.611145.
Train: 2018-08-04T23:39:46.989222: step 5584, loss 0.514449.
Train: 2018-08-04T23:39:47.239165: step 5585, loss 0.562772.
Train: 2018-08-04T23:39:47.489136: step 5586, loss 0.514517.
Train: 2018-08-04T23:39:47.692182: step 5587, loss 0.545577.
Train: 2018-08-04T23:39:47.957777: step 5588, loss 0.603065.
Train: 2018-08-04T23:39:48.207721: step 5589, loss 0.587089.
Train: 2018-08-04T23:39:48.457663: step 5590, loss 0.578845.
Test: 2018-08-04T23:39:49.707336: step 5590, loss 0.549626.
Train: 2018-08-04T23:39:49.941684: step 5591, loss 0.546536.
Train: 2018-08-04T23:39:50.191598: step 5592, loss 0.587058.
Train: 2018-08-04T23:39:50.441570: step 5593, loss 0.514432.
Train: 2018-08-04T23:39:50.691514: step 5594, loss 0.530462.
Train: 2018-08-04T23:39:50.941446: step 5595, loss 0.514269.
Train: 2018-08-04T23:39:51.207014: step 5596, loss 0.505955.
Train: 2018-08-04T23:39:51.456960: step 5597, loss 0.51382.
Train: 2018-08-04T23:39:51.706902: step 5598, loss 0.54618.
Train: 2018-08-04T23:39:51.956810: step 5599, loss 0.595591.
Train: 2018-08-04T23:39:52.206779: step 5600, loss 0.587376.
Test: 2018-08-04T23:39:53.456457: step 5600, loss 0.548874.
Train: 2018-08-04T23:39:54.378118: step 5601, loss 0.521092.
Train: 2018-08-04T23:39:54.628089: step 5602, loss 0.554381.
Train: 2018-08-04T23:39:54.878032: step 5603, loss 0.604101.
Train: 2018-08-04T23:39:55.127971: step 5604, loss 0.604024.
Train: 2018-08-04T23:39:55.377913: step 5605, loss 0.537561.
Train: 2018-08-04T23:39:55.627854: step 5606, loss 0.520939.
Train: 2018-08-04T23:39:55.877796: step 5607, loss 0.512555.
Train: 2018-08-04T23:39:56.127738: step 5608, loss 0.629107.
Train: 2018-08-04T23:39:56.377679: step 5609, loss 0.54575.
Train: 2018-08-04T23:39:56.627620: step 5610, loss 0.570771.
Test: 2018-08-04T23:39:57.877297: step 5610, loss 0.549053.
Train: 2018-08-04T23:39:58.111648: step 5611, loss 0.587472.
Train: 2018-08-04T23:39:58.361590: step 5612, loss 0.579144.
Train: 2018-08-04T23:39:58.611500: step 5613, loss 0.537404.
Train: 2018-08-04T23:39:58.861467: step 5614, loss 0.520753.
Train: 2018-08-04T23:39:59.111383: step 5615, loss 0.562454.
Train: 2018-08-04T23:39:59.361355: step 5616, loss 0.512345.
Train: 2018-08-04T23:39:59.611296: step 5617, loss 0.579127.
Train: 2018-08-04T23:39:59.861238: step 5618, loss 0.646004.
Train: 2018-08-04T23:40:00.111179: step 5619, loss 0.595796.
Train: 2018-08-04T23:40:00.376738: step 5620, loss 0.537445.
Test: 2018-08-04T23:40:01.626419: step 5620, loss 0.548353.
Train: 2018-08-04T23:40:01.860771: step 5621, loss 0.60407.
Train: 2018-08-04T23:40:02.110681: step 5622, loss 0.579063.
Train: 2018-08-04T23:40:02.376274: step 5623, loss 0.587327.
Train: 2018-08-04T23:40:02.610564: step 5624, loss 0.521247.
Train: 2018-08-04T23:40:02.860536: step 5625, loss 0.562529.
Train: 2018-08-04T23:40:03.110477: step 5626, loss 0.54609.
Train: 2018-08-04T23:40:03.360419: step 5627, loss 0.521498.
Train: 2018-08-04T23:40:03.610331: step 5628, loss 0.56251.
Train: 2018-08-04T23:40:03.860301: step 5629, loss 0.488531.
Train: 2018-08-04T23:40:04.125864: step 5630, loss 0.611892.
Test: 2018-08-04T23:40:05.375542: step 5630, loss 0.548531.
Train: 2018-08-04T23:40:05.609893: step 5631, loss 0.521246.
Train: 2018-08-04T23:40:05.906697: step 5632, loss 0.604118.
Train: 2018-08-04T23:40:06.156639: step 5633, loss 0.579232.
Train: 2018-08-04T23:40:06.406580: step 5634, loss 0.53764.
Train: 2018-08-04T23:40:06.687734: step 5635, loss 0.578977.
Train: 2018-08-04T23:40:06.922054: step 5636, loss 0.603962.
Train: 2018-08-04T23:40:07.171996: step 5637, loss 0.54617.
Train: 2018-08-04T23:40:07.437584: step 5638, loss 0.603575.
Train: 2018-08-04T23:40:07.687529: step 5639, loss 0.53802.
Train: 2018-08-04T23:40:07.937466: step 5640, loss 0.554412.
Test: 2018-08-04T23:40:09.202770: step 5640, loss 0.547641.
Train: 2018-08-04T23:40:09.437124: step 5641, loss 0.521677.
Train: 2018-08-04T23:40:09.687060: step 5642, loss 0.546294.
Train: 2018-08-04T23:40:09.937003: step 5643, loss 0.52988.
Train: 2018-08-04T23:40:10.186941: step 5644, loss 0.488903.
Train: 2018-08-04T23:40:10.436884: step 5645, loss 0.488638.
Train: 2018-08-04T23:40:10.686823: step 5646, loss 0.58726.
Train: 2018-08-04T23:40:10.936769: step 5647, loss 0.521096.
Train: 2018-08-04T23:40:11.186682: step 5648, loss 0.604009.
Train: 2018-08-04T23:40:11.436652: step 5649, loss 0.612428.
Train: 2018-08-04T23:40:11.686564: step 5650, loss 0.620811.
Test: 2018-08-04T23:40:12.936271: step 5650, loss 0.548326.
Train: 2018-08-04T23:40:13.170628: step 5651, loss 0.512467.
Train: 2018-08-04T23:40:13.420563: step 5652, loss 0.545764.
Train: 2018-08-04T23:40:13.670475: step 5653, loss 0.520692.
Train: 2018-08-04T23:40:13.920417: step 5654, loss 0.587425.
Train: 2018-08-04T23:40:14.185980: step 5655, loss 0.579132.
Train: 2018-08-04T23:40:14.420332: step 5656, loss 0.579103.
Train: 2018-08-04T23:40:14.654620: step 5657, loss 0.60414.
Train: 2018-08-04T23:40:14.904560: step 5658, loss 0.529086.
Train: 2018-08-04T23:40:15.154527: step 5659, loss 0.595757.
Train: 2018-08-04T23:40:15.420065: step 5660, loss 0.579079.
Test: 2018-08-04T23:40:16.701014: step 5660, loss 0.547832.
Train: 2018-08-04T23:40:16.950956: step 5661, loss 0.620564.
Train: 2018-08-04T23:40:17.200898: step 5662, loss 0.529385.
Train: 2018-08-04T23:40:17.466491: step 5663, loss 0.512975.
Train: 2018-08-04T23:40:17.716432: step 5664, loss 0.504768.
Train: 2018-08-04T23:40:17.981968: step 5665, loss 0.579009.
Train: 2018-08-04T23:40:18.231932: step 5666, loss 0.57076.
Train: 2018-08-04T23:40:18.497499: step 5667, loss 0.496473.
Train: 2018-08-04T23:40:18.747411: step 5668, loss 0.603813.
Train: 2018-08-04T23:40:18.997383: step 5669, loss 0.595547.
Train: 2018-08-04T23:40:19.262915: step 5670, loss 0.537723.
Test: 2018-08-04T23:40:20.559486: step 5670, loss 0.548707.
Train: 2018-08-04T23:40:20.809460: step 5671, loss 0.537721.
Train: 2018-08-04T23:40:21.074998: step 5672, loss 0.56249.
Train: 2018-08-04T23:40:21.340553: step 5673, loss 0.537705.
Train: 2018-08-04T23:40:21.590528: step 5674, loss 0.587294.
Train: 2018-08-04T23:40:21.856061: step 5675, loss 0.521142.
Train: 2018-08-04T23:40:22.106029: step 5676, loss 0.537651.
Train: 2018-08-04T23:40:22.355970: step 5677, loss 0.562469.
Train: 2018-08-04T23:40:22.621529: step 5678, loss 0.603943.
Train: 2018-08-04T23:40:22.855854: step 5679, loss 0.570759.
Train: 2018-08-04T23:40:23.105765: step 5680, loss 0.562464.
Test: 2018-08-04T23:40:24.433578: step 5680, loss 0.549196.
Train: 2018-08-04T23:40:24.808492: step 5681, loss 0.487859.
Train: 2018-08-04T23:40:25.387754: step 5682, loss 0.537557.
Train: 2018-08-04T23:40:25.718721: step 5683, loss 0.579074.
Train: 2018-08-04T23:40:25.980583: step 5684, loss 0.554118.
Train: 2018-08-04T23:40:26.236952: step 5685, loss 0.570763.
Train: 2018-08-04T23:40:26.472224: step 5686, loss 0.5791.
Train: 2018-08-04T23:40:26.736071: step 5687, loss 0.56243.
Train: 2018-08-04T23:40:27.004315: step 5688, loss 0.570765.
Train: 2018-08-04T23:40:27.262898: step 5689, loss 0.570764.
Train: 2018-08-04T23:40:27.532715: step 5690, loss 0.587414.
Test: 2018-08-04T23:40:28.809269: step 5690, loss 0.548368.
Train: 2018-08-04T23:40:29.058109: step 5691, loss 0.562446.
Train: 2018-08-04T23:40:29.292403: step 5692, loss 0.537541.
Train: 2018-08-04T23:40:29.561441: step 5693, loss 0.520963.
Train: 2018-08-04T23:40:29.808964: step 5694, loss 0.587361.
Train: 2018-08-04T23:40:30.092515: step 5695, loss 0.520969.
Train: 2018-08-04T23:40:30.342451: step 5696, loss 0.545851.
Train: 2018-08-04T23:40:30.589197: step 5697, loss 0.603994.
Train: 2018-08-04T23:40:30.835765: step 5698, loss 0.512622.
Train: 2018-08-04T23:40:31.089074: step 5699, loss 0.554138.
Train: 2018-08-04T23:40:31.334574: step 5700, loss 0.54581.
Test: 2018-08-04T23:40:32.668810: step 5700, loss 0.548151.
Train: 2018-08-04T23:40:33.605245: step 5701, loss 0.545788.
Train: 2018-08-04T23:40:33.857017: step 5702, loss 0.570765.
Train: 2018-08-04T23:40:34.108725: step 5703, loss 0.562426.
Train: 2018-08-04T23:40:34.358666: step 5704, loss 0.545734.
Train: 2018-08-04T23:40:34.618840: step 5705, loss 0.58747.
Train: 2018-08-04T23:40:34.860387: step 5706, loss 0.570769.
Train: 2018-08-04T23:40:35.119841: step 5707, loss 0.570768.
Train: 2018-08-04T23:40:35.363946: step 5708, loss 0.58745.
Train: 2018-08-04T23:40:35.608997: step 5709, loss 0.52911.
Train: 2018-08-04T23:40:35.859997: step 5710, loss 0.595743.
Test: 2018-08-04T23:40:37.130650: step 5710, loss 0.55028.
Train: 2018-08-04T23:40:37.361102: step 5711, loss 0.520866.
Train: 2018-08-04T23:40:37.625541: step 5712, loss 0.604013.
Train: 2018-08-04T23:40:37.899235: step 5713, loss 0.545854.
Train: 2018-08-04T23:40:38.176412: step 5714, loss 0.579052.
Train: 2018-08-04T23:40:38.407158: step 5715, loss 0.562474.
Train: 2018-08-04T23:40:38.670273: step 5716, loss 0.512848.
Train: 2018-08-04T23:40:38.917305: step 5717, loss 0.587301.
Train: 2018-08-04T23:40:39.178400: step 5718, loss 0.554224.
Train: 2018-08-04T23:40:39.428341: step 5719, loss 0.529447.
Train: 2018-08-04T23:40:39.696263: step 5720, loss 0.562492.
Test: 2018-08-04T23:40:41.023474: step 5720, loss 0.549449.
Train: 2018-08-04T23:40:41.273478: step 5721, loss 0.636877.
Train: 2018-08-04T23:40:41.538358: step 5722, loss 0.603761.
Train: 2018-08-04T23:40:41.784329: step 5723, loss 0.620139.
Train: 2018-08-04T23:40:42.022185: step 5724, loss 0.562562.
Train: 2018-08-04T23:40:42.283362: step 5725, loss 0.578941.
Train: 2018-08-04T23:40:42.535597: step 5726, loss 0.627783.
Train: 2018-08-04T23:40:42.792326: step 5727, loss 0.570794.
Train: 2018-08-04T23:40:43.084489: step 5728, loss 0.570814.
Train: 2018-08-04T23:40:43.390384: step 5729, loss 0.554762.
Train: 2018-08-04T23:40:43.659909: step 5730, loss 0.506786.
Test: 2018-08-04T23:40:45.127592: step 5730, loss 0.55099.
Train: 2018-08-04T23:40:45.404906: step 5731, loss 0.578863.
Train: 2018-08-04T23:40:45.662148: step 5732, loss 0.594824.
Train: 2018-08-04T23:40:45.907128: step 5733, loss 0.578859.
Train: 2018-08-04T23:40:46.161789: step 5734, loss 0.634474.
Train: 2018-08-04T23:40:46.407945: step 5735, loss 0.56303.
Train: 2018-08-04T23:40:46.666396: step 5736, loss 0.547306.
Train: 2018-08-04T23:40:46.918846: step 5737, loss 0.610351.
Train: 2018-08-04T23:40:47.117519: step 5738, loss 0.479489.
Train: 2018-08-04T23:40:47.375038: step 5739, loss 0.500448.
Train: 2018-08-04T23:40:47.658739: step 5740, loss 0.571018.
Test: 2018-08-04T23:40:48.991643: step 5740, loss 0.550731.
Train: 2018-08-04T23:40:49.232268: step 5741, loss 0.539548.
Train: 2018-08-04T23:40:49.483881: step 5742, loss 0.539467.
Train: 2018-08-04T23:40:49.729244: step 5743, loss 0.555162.
Train: 2018-08-04T23:40:49.989574: step 5744, loss 0.547175.
Train: 2018-08-04T23:40:50.274432: step 5745, loss 0.5153.
Train: 2018-08-04T23:40:50.537998: step 5746, loss 0.515046.
Train: 2018-08-04T23:40:50.783780: step 5747, loss 0.546791.
Train: 2018-08-04T23:40:51.069948: step 5748, loss 0.603068.
Train: 2018-08-04T23:40:51.338426: step 5749, loss 0.498009.
Train: 2018-08-04T23:40:51.603422: step 5750, loss 0.530079.
Test: 2018-08-04T23:40:52.910755: step 5750, loss 0.548346.
Train: 2018-08-04T23:40:53.160356: step 5751, loss 0.578856.
Train: 2018-08-04T23:40:53.419559: step 5752, loss 0.562358.
Train: 2018-08-04T23:40:53.674569: step 5753, loss 0.611767.
Train: 2018-08-04T23:40:53.936260: step 5754, loss 0.588166.
Train: 2018-08-04T23:40:54.220605: step 5755, loss 0.630181.
Train: 2018-08-04T23:40:54.468576: step 5756, loss 0.562866.
Train: 2018-08-04T23:40:54.723747: step 5757, loss 0.603867.
Train: 2018-08-04T23:40:54.975969: step 5758, loss 0.579008.
Train: 2018-08-04T23:40:55.232673: step 5759, loss 0.652836.
Train: 2018-08-04T23:40:55.482724: step 5760, loss 0.562601.
Test: 2018-08-04T23:40:56.753147: step 5760, loss 0.548626.
Train: 2018-08-04T23:40:56.993789: step 5761, loss 0.562637.
Train: 2018-08-04T23:40:57.249879: step 5762, loss 0.595203.
Train: 2018-08-04T23:40:57.499669: step 5763, loss 0.570846.
Train: 2018-08-04T23:40:57.762500: step 5764, loss 0.554569.
Train: 2018-08-04T23:40:58.039154: step 5765, loss 0.570811.
Train: 2018-08-04T23:40:58.294490: step 5766, loss 0.595155.
Train: 2018-08-04T23:40:58.551027: step 5767, loss 0.562793.
Train: 2018-08-04T23:40:58.828062: step 5768, loss 0.602824.
Train: 2018-08-04T23:40:59.100403: step 5769, loss 0.578712.
Train: 2018-08-04T23:40:59.354556: step 5770, loss 0.570748.
Test: 2018-08-04T23:41:00.655940: step 5770, loss 0.549425.
Train: 2018-08-04T23:41:00.911486: step 5771, loss 0.562877.
Train: 2018-08-04T23:41:01.153211: step 5772, loss 0.499871.
Train: 2018-08-04T23:41:01.407593: step 5773, loss 0.475904.
Train: 2018-08-04T23:41:01.654406: step 5774, loss 0.55524.
Train: 2018-08-04T23:41:01.926995: step 5775, loss 0.586483.
Train: 2018-08-04T23:41:02.202226: step 5776, loss 0.626449.
Train: 2018-08-04T23:41:02.451618: step 5777, loss 0.491657.
Train: 2018-08-04T23:41:02.706347: step 5778, loss 0.538894.
Train: 2018-08-04T23:41:02.950873: step 5779, loss 0.538769.
Train: 2018-08-04T23:41:03.244987: step 5780, loss 0.554909.
Test: 2018-08-04T23:41:04.575182: step 5780, loss 0.548712.
Train: 2018-08-04T23:41:04.823659: step 5781, loss 0.490956.
Train: 2018-08-04T23:41:05.070619: step 5782, loss 0.578811.
Train: 2018-08-04T23:41:05.320670: step 5783, loss 0.562693.
Train: 2018-08-04T23:41:05.580632: step 5784, loss 0.635263.
Train: 2018-08-04T23:41:05.838569: step 5785, loss 0.570776.
Train: 2018-08-04T23:41:06.112063: step 5786, loss 0.497604.
Train: 2018-08-04T23:41:06.377139: step 5787, loss 0.587131.
Train: 2018-08-04T23:41:06.646425: step 5788, loss 0.570846.
Train: 2018-08-04T23:41:06.895482: step 5789, loss 0.513812.
Train: 2018-08-04T23:41:07.143514: step 5790, loss 0.55427.
Test: 2018-08-04T23:41:08.454578: step 5790, loss 0.548528.
Train: 2018-08-04T23:41:08.748661: step 5791, loss 0.570677.
Train: 2018-08-04T23:41:09.012281: step 5792, loss 0.521573.
Train: 2018-08-04T23:41:09.267371: step 5793, loss 0.513258.
Train: 2018-08-04T23:41:09.520270: step 5794, loss 0.636752.
Train: 2018-08-04T23:41:09.781026: step 5795, loss 0.587363.
Train: 2018-08-04T23:41:10.052883: step 5796, loss 0.570791.
Train: 2018-08-04T23:41:10.366710: step 5797, loss 0.653262.
Train: 2018-08-04T23:41:10.655688: step 5798, loss 0.537813.
Train: 2018-08-04T23:41:10.941661: step 5799, loss 0.529634.
Train: 2018-08-04T23:41:11.205982: step 5800, loss 0.472144.
Test: 2018-08-04T23:41:12.545730: step 5800, loss 0.54764.
Train: 2018-08-04T23:41:13.588115: step 5801, loss 0.521362.
Train: 2018-08-04T23:41:13.854625: step 5802, loss 0.537748.
Train: 2018-08-04T23:41:14.113168: step 5803, loss 0.628672.
Train: 2018-08-04T23:41:14.391204: step 5804, loss 0.504529.
Train: 2018-08-04T23:41:14.651377: step 5805, loss 0.570759.
Train: 2018-08-04T23:41:14.912295: step 5806, loss 0.512596.
Train: 2018-08-04T23:41:15.179037: step 5807, loss 0.554104.
Train: 2018-08-04T23:41:15.452118: step 5808, loss 0.50398.
Train: 2018-08-04T23:41:15.716190: step 5809, loss 0.503748.
Train: 2018-08-04T23:41:15.991953: step 5810, loss 0.562393.
Test: 2018-08-04T23:41:17.276244: step 5810, loss 0.548069.
Train: 2018-08-04T23:41:17.535260: step 5811, loss 0.51171.
Train: 2018-08-04T23:41:17.799189: step 5812, loss 0.570807.
Train: 2018-08-04T23:41:18.045591: step 5813, loss 0.570882.
Train: 2018-08-04T23:41:18.292879: step 5814, loss 0.562311.
Train: 2018-08-04T23:41:18.572848: step 5815, loss 0.579487.
Train: 2018-08-04T23:41:18.836624: step 5816, loss 0.570935.
Train: 2018-08-04T23:41:19.094006: step 5817, loss 0.528118.
Train: 2018-08-04T23:41:19.346386: step 5818, loss 0.519522.
Train: 2018-08-04T23:41:19.609384: step 5819, loss 0.536587.
Train: 2018-08-04T23:41:19.862588: step 5820, loss 0.527946.
Test: 2018-08-04T23:41:21.155022: step 5820, loss 0.548744.
Train: 2018-08-04T23:41:21.397767: step 5821, loss 0.665699.
Train: 2018-08-04T23:41:21.655298: step 5822, loss 0.527905.
Train: 2018-08-04T23:41:21.923492: step 5823, loss 0.579548.
Train: 2018-08-04T23:41:22.174596: step 5824, loss 0.613937.
Train: 2018-08-04T23:41:22.420784: step 5825, loss 0.579498.
Train: 2018-08-04T23:41:22.670503: step 5826, loss 0.54521.
Train: 2018-08-04T23:41:22.951066: step 5827, loss 0.519622.
Train: 2018-08-04T23:41:23.232058: step 5828, loss 0.553808.
Train: 2018-08-04T23:41:23.486200: step 5829, loss 0.622005.
Train: 2018-08-04T23:41:23.733951: step 5830, loss 0.528339.
Test: 2018-08-04T23:41:25.055377: step 5830, loss 0.547395.
Train: 2018-08-04T23:41:25.295735: step 5831, loss 0.604782.
Train: 2018-08-04T23:41:25.564019: step 5832, loss 0.562357.
Train: 2018-08-04T23:41:25.830305: step 5833, loss 0.604571.
Train: 2018-08-04T23:41:26.092603: step 5834, loss 0.596023.
Train: 2018-08-04T23:41:26.358924: step 5835, loss 0.503763.
Train: 2018-08-04T23:41:26.635154: step 5836, loss 0.537347.
Train: 2018-08-04T23:41:26.890473: step 5837, loss 0.645831.
Train: 2018-08-04T23:41:27.160780: step 5838, loss 0.57076.
Train: 2018-08-04T23:41:27.437049: step 5839, loss 0.562477.
Train: 2018-08-04T23:41:27.702302: step 5840, loss 0.504736.
Test: 2018-08-04T23:41:29.040724: step 5840, loss 0.549139.
Train: 2018-08-04T23:41:29.308007: step 5841, loss 0.562518.
Train: 2018-08-04T23:41:29.573330: step 5842, loss 0.529633.
Train: 2018-08-04T23:41:29.829614: step 5843, loss 0.537882.
Train: 2018-08-04T23:41:30.110862: step 5844, loss 0.620068.
Train: 2018-08-04T23:41:30.384131: step 5845, loss 0.578967.
Train: 2018-08-04T23:41:30.653412: step 5846, loss 0.521615.
Train: 2018-08-04T23:41:30.905736: step 5847, loss 0.513462.
Train: 2018-08-04T23:41:31.155071: step 5848, loss 0.52162.
Train: 2018-08-04T23:41:31.399446: step 5849, loss 0.595369.
Train: 2018-08-04T23:41:31.647754: step 5850, loss 0.603587.
Test: 2018-08-04T23:41:32.964234: step 5850, loss 0.548107.
Train: 2018-08-04T23:41:33.208581: step 5851, loss 0.570761.
Train: 2018-08-04T23:41:33.488833: step 5852, loss 0.562567.
Train: 2018-08-04T23:41:33.770081: step 5853, loss 0.587145.
Train: 2018-08-04T23:41:34.041356: step 5854, loss 0.538045.
Train: 2018-08-04T23:41:34.297677: step 5855, loss 0.611648.
Train: 2018-08-04T23:41:34.551990: step 5856, loss 0.570771.
Train: 2018-08-04T23:41:34.808304: step 5857, loss 0.546329.
Train: 2018-08-04T23:41:35.060630: step 5858, loss 0.56264.
Train: 2018-08-04T23:41:35.308995: step 5859, loss 0.660225.
Train: 2018-08-04T23:41:35.562290: step 5860, loss 0.627527.
Test: 2018-08-04T23:41:36.833892: step 5860, loss 0.549085.
Train: 2018-08-04T23:41:37.076242: step 5861, loss 0.619225.
Train: 2018-08-04T23:41:37.329581: step 5862, loss 0.578868.
Train: 2018-08-04T23:41:37.575906: step 5863, loss 0.538941.
Train: 2018-08-04T23:41:37.837208: step 5864, loss 0.570906.
Train: 2018-08-04T23:41:38.090532: step 5865, loss 0.531319.
Train: 2018-08-04T23:41:38.342887: step 5866, loss 0.673716.
Train: 2018-08-04T23:41:38.592189: step 5867, loss 0.523794.
Train: 2018-08-04T23:41:38.849501: step 5868, loss 0.547509.
Train: 2018-08-04T23:41:39.110333: step 5869, loss 0.5241.
Train: 2018-08-04T23:41:39.361209: step 5870, loss 0.516322.
Test: 2018-08-04T23:41:40.629896: step 5870, loss 0.549436.
Train: 2018-08-04T23:41:40.870625: step 5871, loss 0.571055.
Train: 2018-08-04T23:41:41.127319: step 5872, loss 0.555382.
Train: 2018-08-04T23:41:41.384758: step 5873, loss 0.61808.
Train: 2018-08-04T23:41:41.640781: step 5874, loss 0.594555.
Train: 2018-08-04T23:41:41.889826: step 5875, loss 0.55538.
Train: 2018-08-04T23:41:42.146564: step 5876, loss 0.58671.
Train: 2018-08-04T23:41:42.398347: step 5877, loss 0.51628.
Train: 2018-08-04T23:41:42.654737: step 5878, loss 0.53972.
Train: 2018-08-04T23:41:42.908079: step 5879, loss 0.62594.
Train: 2018-08-04T23:41:43.161728: step 5880, loss 0.610252.
Test: 2018-08-04T23:41:44.439552: step 5880, loss 0.549395.
Train: 2018-08-04T23:41:44.676655: step 5881, loss 0.508348.
Train: 2018-08-04T23:41:44.932542: step 5882, loss 0.586717.
Train: 2018-08-04T23:41:45.184692: step 5883, loss 0.531808.
Train: 2018-08-04T23:41:45.433083: step 5884, loss 0.516037.
Train: 2018-08-04T23:41:45.685906: step 5885, loss 0.602488.
Train: 2018-08-04T23:41:45.942537: step 5886, loss 0.539435.
Train: 2018-08-04T23:41:46.195884: step 5887, loss 0.578862.
Train: 2018-08-04T23:41:46.447621: step 5888, loss 0.531367.
Train: 2018-08-04T23:41:46.643409: step 5889, loss 0.562989.
Train: 2018-08-04T23:41:46.896113: step 5890, loss 0.578859.
Test: 2018-08-04T23:41:48.183796: step 5890, loss 0.549432.
Train: 2018-08-04T23:41:48.433927: step 5891, loss 0.642604.
Train: 2018-08-04T23:41:48.694740: step 5892, loss 0.531061.
Train: 2018-08-04T23:41:48.946325: step 5893, loss 0.499138.
Train: 2018-08-04T23:41:49.193009: step 5894, loss 0.498952.
Train: 2018-08-04T23:41:49.438795: step 5895, loss 0.651061.
Train: 2018-08-04T23:41:49.690201: step 5896, loss 0.554774.
Train: 2018-08-04T23:41:49.945706: step 5897, loss 0.594962.
Train: 2018-08-04T23:41:50.192904: step 5898, loss 0.578876.
Train: 2018-08-04T23:41:50.458703: step 5899, loss 0.562775.
Train: 2018-08-04T23:41:50.716114: step 5900, loss 0.562771.
Test: 2018-08-04T23:41:51.967875: step 5900, loss 0.548201.
Train: 2018-08-04T23:41:52.870221: step 5901, loss 0.651377.
Train: 2018-08-04T23:41:53.121982: step 5902, loss 0.578873.
Train: 2018-08-04T23:41:53.367758: step 5903, loss 0.514668.
Train: 2018-08-04T23:41:53.619955: step 5904, loss 0.626994.
Train: 2018-08-04T23:41:53.885641: step 5905, loss 0.578864.
Train: 2018-08-04T23:41:54.140910: step 5906, loss 0.538915.
Train: 2018-08-04T23:41:54.383592: step 5907, loss 0.58684.
Train: 2018-08-04T23:41:54.630480: step 5908, loss 0.49121.
Train: 2018-08-04T23:41:54.880451: step 5909, loss 0.554942.
Train: 2018-08-04T23:41:55.133738: step 5910, loss 0.538966.
Test: 2018-08-04T23:41:56.366064: step 5910, loss 0.548984.
Train: 2018-08-04T23:41:56.600416: step 5911, loss 0.53891.
Train: 2018-08-04T23:41:56.850327: step 5912, loss 0.52282.
Train: 2018-08-04T23:41:57.100268: step 5913, loss 0.586899.
Train: 2018-08-04T23:41:57.350242: step 5914, loss 0.530599.
Train: 2018-08-04T23:41:57.600181: step 5915, loss 0.562747.
Train: 2018-08-04T23:41:57.850118: step 5916, loss 0.586979.
Train: 2018-08-04T23:41:58.100065: step 5917, loss 0.668017.
Train: 2018-08-04T23:41:58.349976: step 5918, loss 0.578894.
Train: 2018-08-04T23:41:58.599916: step 5919, loss 0.578889.
Train: 2018-08-04T23:41:58.860649: step 5920, loss 0.586956.
Test: 2018-08-04T23:42:00.110357: step 5920, loss 0.549864.
Train: 2018-08-04T23:42:00.346270: step 5921, loss 0.530535.
Train: 2018-08-04T23:42:00.596212: step 5922, loss 0.594978.
Train: 2018-08-04T23:42:00.846158: step 5923, loss 0.514561.
Train: 2018-08-04T23:42:01.096094: step 5924, loss 0.562795.
Train: 2018-08-04T23:42:01.361661: step 5925, loss 0.562795.
Train: 2018-08-04T23:42:01.595954: step 5926, loss 0.602991.
Train: 2018-08-04T23:42:01.845894: step 5927, loss 0.60297.
Train: 2018-08-04T23:42:02.111487: step 5928, loss 0.546788.
Train: 2018-08-04T23:42:02.345777: step 5929, loss 0.538807.
Train: 2018-08-04T23:42:02.595718: step 5930, loss 0.546825.
Test: 2018-08-04T23:42:03.845426: step 5930, loss 0.549644.
Train: 2018-08-04T23:42:04.095394: step 5931, loss 0.530791.
Train: 2018-08-04T23:42:04.345339: step 5932, loss 0.570846.
Train: 2018-08-04T23:42:04.610899: step 5933, loss 0.498579.
Train: 2018-08-04T23:42:04.860821: step 5934, loss 0.554729.
Train: 2018-08-04T23:42:05.110754: step 5935, loss 0.554676.
Train: 2018-08-04T23:42:05.360721: step 5936, loss 0.457553.
Train: 2018-08-04T23:42:05.610637: step 5937, loss 0.570784.
Train: 2018-08-04T23:42:05.923090: step 5938, loss 0.554448.
Train: 2018-08-04T23:42:06.173007: step 5939, loss 0.644505.
Train: 2018-08-04T23:42:06.422977: step 5940, loss 0.52973.
Test: 2018-08-04T23:42:07.672654: step 5940, loss 0.54899.
Train: 2018-08-04T23:42:07.906974: step 5941, loss 0.587206.
Train: 2018-08-04T23:42:08.156947: step 5942, loss 0.570757.
Train: 2018-08-04T23:42:08.406884: step 5943, loss 0.513066.
Train: 2018-08-04T23:42:08.656799: step 5944, loss 0.587271.
Train: 2018-08-04T23:42:08.906772: step 5945, loss 0.570756.
Train: 2018-08-04T23:42:09.156712: step 5946, loss 0.554212.
Train: 2018-08-04T23:42:09.406625: step 5947, loss 0.60387.
Train: 2018-08-04T23:42:09.656566: step 5948, loss 0.570756.
Train: 2018-08-04T23:42:09.906536: step 5949, loss 0.545945.
Train: 2018-08-04T23:42:10.156447: step 5950, loss 0.587294.
Test: 2018-08-04T23:42:11.420161: step 5950, loss 0.550599.
Train: 2018-08-04T23:42:11.654483: step 5951, loss 0.562495.
Train: 2018-08-04T23:42:11.904424: step 5952, loss 0.488208.
Train: 2018-08-04T23:42:12.154364: step 5953, loss 0.62033.
Train: 2018-08-04T23:42:12.404305: step 5954, loss 0.562499.
Train: 2018-08-04T23:42:12.654247: step 5955, loss 0.562504.
Train: 2018-08-04T23:42:12.904190: step 5956, loss 0.562509.
Train: 2018-08-04T23:42:13.145975: step 5957, loss 0.562514.
Train: 2018-08-04T23:42:13.395887: step 5958, loss 0.504854.
Train: 2018-08-04T23:42:13.645828: step 5959, loss 0.653197.
Train: 2018-08-04T23:42:13.911391: step 5960, loss 0.521367.
Test: 2018-08-04T23:42:15.178277: step 5960, loss 0.549167.
Train: 2018-08-04T23:42:15.415591: step 5961, loss 0.587214.
Train: 2018-08-04T23:42:15.662874: step 5962, loss 0.620078.
Train: 2018-08-04T23:42:15.906865: step 5963, loss 0.496952.
Train: 2018-08-04T23:42:16.152215: step 5964, loss 0.570763.
Train: 2018-08-04T23:42:16.394441: step 5965, loss 0.488859.
Train: 2018-08-04T23:42:16.644351: step 5966, loss 0.619957.
Train: 2018-08-04T23:42:16.894292: step 5967, loss 0.546176.
Train: 2018-08-04T23:42:17.144235: step 5968, loss 0.636327.
Train: 2018-08-04T23:42:17.394207: step 5969, loss 0.562585.
Train: 2018-08-04T23:42:17.644148: step 5970, loss 0.578937.
Test: 2018-08-04T23:42:18.904166: step 5970, loss 0.547506.
Train: 2018-08-04T23:42:19.169731: step 5971, loss 0.55447.
Train: 2018-08-04T23:42:19.419671: step 5972, loss 0.58706.
Train: 2018-08-04T23:42:19.669642: step 5973, loss 0.554535.
Train: 2018-08-04T23:42:19.919553: step 5974, loss 0.554565.
Train: 2018-08-04T23:42:20.169496: step 5975, loss 0.562692.
Train: 2018-08-04T23:42:20.419436: step 5976, loss 0.554608.
Train: 2018-08-04T23:42:20.669406: step 5977, loss 0.554622.
Train: 2018-08-04T23:42:20.919352: step 5978, loss 0.619326.
Train: 2018-08-04T23:42:21.169285: step 5979, loss 0.578885.
Train: 2018-08-04T23:42:21.419202: step 5980, loss 0.546643.
Test: 2018-08-04T23:42:22.653288: step 5980, loss 0.549334.
Train: 2018-08-04T23:42:22.903263: step 5981, loss 0.603024.
Train: 2018-08-04T23:42:23.153201: step 5982, loss 0.474436.
Train: 2018-08-04T23:42:23.403138: step 5983, loss 0.522613.
Train: 2018-08-04T23:42:23.653085: step 5984, loss 0.611071.
Train: 2018-08-04T23:42:23.903026: step 5985, loss 0.586927.
Train: 2018-08-04T23:42:24.152968: step 5986, loss 0.570828.
Train: 2018-08-04T23:42:24.402878: step 5987, loss 0.554741.
Train: 2018-08-04T23:42:24.652851: step 5988, loss 0.538657.
Train: 2018-08-04T23:42:24.902761: step 5989, loss 0.56278.
Train: 2018-08-04T23:42:25.152703: step 5990, loss 0.627192.
Test: 2018-08-04T23:42:26.402411: step 5990, loss 0.550091.
Train: 2018-08-04T23:42:26.652353: step 5991, loss 0.554741.
Train: 2018-08-04T23:42:26.902293: step 5992, loss 0.554754.
Train: 2018-08-04T23:42:27.152234: step 5993, loss 0.61102.
Train: 2018-08-04T23:42:27.402209: step 5994, loss 0.578869.
Train: 2018-08-04T23:42:27.667767: step 5995, loss 0.52277.
Train: 2018-08-04T23:42:27.902090: step 5996, loss 0.54682.
Train: 2018-08-04T23:42:28.152032: step 5997, loss 0.578866.
Train: 2018-08-04T23:42:28.417564: step 5998, loss 0.610914.
Train: 2018-08-04T23:42:28.651919: step 5999, loss 0.594869.
Train: 2018-08-04T23:42:28.914127: step 6000, loss 0.570873.
Test: 2018-08-04T23:42:30.181593: step 6000, loss 0.549775.
Train: 2018-08-04T23:42:31.167209: step 6001, loss 0.610759.
Train: 2018-08-04T23:42:31.419781: step 6002, loss 0.578859.
Train: 2018-08-04T23:42:31.669732: step 6003, loss 0.547125.
Train: 2018-08-04T23:42:31.919640: step 6004, loss 0.555102.
Train: 2018-08-04T23:42:32.169611: step 6005, loss 0.499769.
Train: 2018-08-04T23:42:32.419524: step 6006, loss 0.563032.
Train: 2018-08-04T23:42:32.669495: step 6007, loss 0.618458.
Train: 2018-08-04T23:42:32.920230: step 6008, loss 0.539285.
Train: 2018-08-04T23:42:33.170145: step 6009, loss 0.523442.
Train: 2018-08-04T23:42:33.420087: step 6010, loss 0.523366.
Test: 2018-08-04T23:42:34.676890: step 6010, loss 0.549878.
Train: 2018-08-04T23:42:34.926862: step 6011, loss 0.562966.
Train: 2018-08-04T23:42:35.176798: step 6012, loss 0.523108.
Train: 2018-08-04T23:42:35.426745: step 6013, loss 0.634789.
Train: 2018-08-04T23:42:35.676686: step 6014, loss 0.618857.
Train: 2018-08-04T23:42:35.926627: step 6015, loss 0.514888.
Train: 2018-08-04T23:42:36.176569: step 6016, loss 0.58687.
Train: 2018-08-04T23:42:36.426513: step 6017, loss 0.538819.
Train: 2018-08-04T23:42:36.676452: step 6018, loss 0.514723.
Train: 2018-08-04T23:42:36.926364: step 6019, loss 0.554763.
Train: 2018-08-04T23:42:37.176336: step 6020, loss 0.530549.
Test: 2018-08-04T23:42:38.441633: step 6020, loss 0.548867.
Train: 2018-08-04T23:42:38.685369: step 6021, loss 0.546573.
Train: 2018-08-04T23:42:38.923036: step 6022, loss 0.595105.
Train: 2018-08-04T23:42:39.172948: step 6023, loss 0.513951.
Train: 2018-08-04T23:42:39.422913: step 6024, loss 0.521915.
Train: 2018-08-04T23:42:39.672831: step 6025, loss 0.562594.
Train: 2018-08-04T23:42:39.922771: step 6026, loss 0.570762.
Train: 2018-08-04T23:42:40.172714: step 6027, loss 0.521422.
Train: 2018-08-04T23:42:40.428608: step 6028, loss 0.546005.
Train: 2018-08-04T23:42:40.672942: step 6029, loss 0.487978.
Train: 2018-08-04T23:42:40.922913: step 6030, loss 0.495914.
Test: 2018-08-04T23:42:42.172592: step 6030, loss 0.548829.
Train: 2018-08-04T23:42:42.422562: step 6031, loss 0.587499.
Train: 2018-08-04T23:42:42.672504: step 6032, loss 0.537193.
Train: 2018-08-04T23:42:42.938062: step 6033, loss 0.570805.
Train: 2018-08-04T23:42:43.186031: step 6034, loss 0.604674.
Train: 2018-08-04T23:42:43.451590: step 6035, loss 0.460591.
Train: 2018-08-04T23:42:43.701504: step 6036, loss 0.579367.
Train: 2018-08-04T23:42:43.951446: step 6037, loss 0.579411.
Train: 2018-08-04T23:42:44.201417: step 6038, loss 0.553784.
Train: 2018-08-04T23:42:44.449158: step 6039, loss 0.57947.
Train: 2018-08-04T23:42:44.652265: step 6040, loss 0.397701.
Test: 2018-08-04T23:42:45.897334: step 6040, loss 0.547165.
Train: 2018-08-04T23:42:46.147302: step 6041, loss 0.613993.
Train: 2018-08-04T23:42:46.397216: step 6042, loss 0.570967.
Train: 2018-08-04T23:42:46.647188: step 6043, loss 0.588269.
Train: 2018-08-04T23:42:46.897098: step 6044, loss 0.570989.
Train: 2018-08-04T23:42:47.147040: step 6045, loss 0.622878.
Train: 2018-08-04T23:42:47.412631: step 6046, loss 0.640053.
Train: 2018-08-04T23:42:47.646955: step 6047, loss 0.6484.
Train: 2018-08-04T23:42:47.896864: step 6048, loss 0.536647.
Train: 2018-08-04T23:42:48.146805: step 6049, loss 0.528235.
Train: 2018-08-04T23:42:48.396778: step 6050, loss 0.579341.
Test: 2018-08-04T23:42:49.662078: step 6050, loss 0.54841.
Train: 2018-08-04T23:42:49.912043: step 6051, loss 0.596224.
Train: 2018-08-04T23:42:50.146339: step 6052, loss 0.511781.
Train: 2018-08-04T23:42:50.396311: step 6053, loss 0.528752.
Train: 2018-08-04T23:42:50.646251: step 6054, loss 0.512046.
Train: 2018-08-04T23:42:50.896163: step 6055, loss 0.528859.
Train: 2018-08-04T23:42:51.146134: step 6056, loss 0.512097.
Train: 2018-08-04T23:42:51.401275: step 6057, loss 0.570783.
Train: 2018-08-04T23:42:51.651217: step 6058, loss 0.570784.
Train: 2018-08-04T23:42:51.901154: step 6059, loss 0.570784.
Train: 2018-08-04T23:42:52.151100: step 6060, loss 0.49525.
Test: 2018-08-04T23:42:53.400777: step 6060, loss 0.547967.
Train: 2018-08-04T23:42:53.635129: step 6061, loss 0.579189.
Train: 2018-08-04T23:42:53.885063: step 6062, loss 0.545572.
Train: 2018-08-04T23:42:54.135005: step 6063, loss 0.486679.
Train: 2018-08-04T23:42:54.384952: step 6064, loss 0.553943.
Train: 2018-08-04T23:42:54.634864: step 6065, loss 0.596144.
Train: 2018-08-04T23:42:54.884835: step 6066, loss 0.55391.
Train: 2018-08-04T23:42:55.134796: step 6067, loss 0.553902.
Train: 2018-08-04T23:42:55.384718: step 6068, loss 0.621598.
Train: 2018-08-04T23:42:55.650275: step 6069, loss 0.57927.
Train: 2018-08-04T23:42:55.884596: step 6070, loss 0.570808.
Test: 2018-08-04T23:42:57.134279: step 6070, loss 0.547128.
Train: 2018-08-04T23:42:57.384244: step 6071, loss 0.520221.
Train: 2018-08-04T23:42:57.634162: step 6072, loss 0.587646.
Train: 2018-08-04T23:42:57.884133: step 6073, loss 0.545554.
Train: 2018-08-04T23:42:58.134074: step 6074, loss 0.612811.
Train: 2018-08-04T23:42:58.384015: step 6075, loss 0.579167.
Train: 2018-08-04T23:42:58.633957: step 6076, loss 0.637697.
Train: 2018-08-04T23:42:58.883899: step 6077, loss 0.54577.
Train: 2018-08-04T23:42:59.133840: step 6078, loss 0.603972.
Train: 2018-08-04T23:42:59.383781: step 6079, loss 0.496335.
Train: 2018-08-04T23:42:59.635695: step 6080, loss 0.537755.
Test: 2018-08-04T23:43:00.893046: step 6080, loss 0.54952.
Train: 2018-08-04T23:43:01.127367: step 6081, loss 0.587232.
Train: 2018-08-04T23:43:01.377306: step 6082, loss 0.529653.
Train: 2018-08-04T23:43:01.627274: step 6083, loss 0.611821.
Train: 2018-08-04T23:43:01.877220: step 6084, loss 0.587152.
Train: 2018-08-04T23:43:02.127132: step 6085, loss 0.554421.
Train: 2018-08-04T23:43:02.377072: step 6086, loss 0.578929.
Train: 2018-08-04T23:43:02.627015: step 6087, loss 0.578917.
Train: 2018-08-04T23:43:02.876986: step 6088, loss 0.522085.
Train: 2018-08-04T23:43:03.126927: step 6089, loss 0.489721.
Train: 2018-08-04T23:43:03.376869: step 6090, loss 0.611354.
Test: 2018-08-04T23:43:04.626547: step 6090, loss 0.548396.
Train: 2018-08-04T23:43:04.860866: step 6091, loss 0.497817.
Train: 2018-08-04T23:43:05.110837: step 6092, loss 0.595141.
Train: 2018-08-04T23:43:05.360776: step 6093, loss 0.562668.
Train: 2018-08-04T23:43:05.610690: step 6094, loss 0.635757.
Train: 2018-08-04T23:43:05.876278: step 6095, loss 0.562683.
Train: 2018-08-04T23:43:06.110574: step 6096, loss 0.506012.
Train: 2018-08-04T23:43:06.360539: step 6097, loss 0.611296.
Train: 2018-08-04T23:43:06.610482: step 6098, loss 0.627446.
Train: 2018-08-04T23:43:06.860428: step 6099, loss 0.562739.
Train: 2018-08-04T23:43:07.110369: step 6100, loss 0.546654.
Test: 2018-08-04T23:43:08.375668: step 6100, loss 0.54972.
Train: 2018-08-04T23:43:09.235576: step 6101, loss 0.57083.
Train: 2018-08-04T23:43:09.485517: step 6102, loss 0.594938.
Train: 2018-08-04T23:43:09.735489: step 6103, loss 0.578867.
Train: 2018-08-04T23:43:09.985400: step 6104, loss 0.570863.
Train: 2018-08-04T23:43:10.235372: step 6105, loss 0.570876.
Train: 2018-08-04T23:43:10.485284: step 6106, loss 0.570889.
Train: 2018-08-04T23:43:10.735396: step 6107, loss 0.554991.
Train: 2018-08-04T23:43:10.985310: step 6108, loss 0.586804.
Train: 2018-08-04T23:43:11.235281: step 6109, loss 0.555059.
Train: 2018-08-04T23:43:11.485193: step 6110, loss 0.586784.
Test: 2018-08-04T23:43:12.734899: step 6110, loss 0.549454.
Train: 2018-08-04T23:43:12.969251: step 6111, loss 0.57886.
Train: 2018-08-04T23:43:13.211141: step 6112, loss 0.523554.
Train: 2018-08-04T23:43:13.461086: step 6113, loss 0.491968.
Train: 2018-08-04T23:43:13.711028: step 6114, loss 0.523466.
Train: 2018-08-04T23:43:13.960704: step 6115, loss 0.555053.
Train: 2018-08-04T23:43:14.210675: step 6116, loss 0.547028.
Train: 2018-08-04T23:43:14.460587: step 6117, loss 0.554915.
Train: 2018-08-04T23:43:14.699319: step 6118, loss 0.538834.
Train: 2018-08-04T23:43:14.949254: step 6119, loss 0.611005.
Train: 2018-08-04T23:43:15.199172: step 6120, loss 0.594975.
Test: 2018-08-04T23:43:16.464501: step 6120, loss 0.548564.
Train: 2018-08-04T23:43:16.698845: step 6121, loss 0.554705.
Train: 2018-08-04T23:43:16.948792: step 6122, loss 0.538544.
Train: 2018-08-04T23:43:17.198733: step 6123, loss 0.611215.
Train: 2018-08-04T23:43:17.448675: step 6124, loss 0.554635.
Train: 2018-08-04T23:43:17.698612: step 6125, loss 0.506081.
Train: 2018-08-04T23:43:17.948558: step 6126, loss 0.5789.
Train: 2018-08-04T23:43:18.198470: step 6127, loss 0.570789.
Train: 2018-08-04T23:43:18.448441: step 6128, loss 0.562658.
Train: 2018-08-04T23:43:18.698378: step 6129, loss 0.473166.
Train: 2018-08-04T23:43:18.945318: step 6130, loss 0.587091.
Test: 2018-08-04T23:43:20.210646: step 6130, loss 0.547993.
Train: 2018-08-04T23:43:20.444968: step 6131, loss 0.505352.
Train: 2018-08-04T23:43:20.694939: step 6132, loss 0.578965.
Train: 2018-08-04T23:43:20.944849: step 6133, loss 0.55431.
Train: 2018-08-04T23:43:21.194792: step 6134, loss 0.587245.
Train: 2018-08-04T23:43:21.444734: step 6135, loss 0.595526.
Train: 2018-08-04T23:43:21.710321: step 6136, loss 0.496406.
Train: 2018-08-04T23:43:21.960268: step 6137, loss 0.628697.
Train: 2018-08-04T23:43:22.210209: step 6138, loss 0.595592.
Train: 2018-08-04T23:43:22.460150: step 6139, loss 0.587302.
Train: 2018-08-04T23:43:22.725684: step 6140, loss 0.570756.
Test: 2018-08-04T23:43:23.991013: step 6140, loss 0.548727.
Train: 2018-08-04T23:43:24.240953: step 6141, loss 0.570756.
Train: 2018-08-04T23:43:24.490928: step 6142, loss 0.513093.
Train: 2018-08-04T23:43:24.740866: step 6143, loss 0.51311.
Train: 2018-08-04T23:43:24.990806: step 6144, loss 0.554272.
Train: 2018-08-04T23:43:25.240749: step 6145, loss 0.653253.
Train: 2018-08-04T23:43:25.490660: step 6146, loss 0.49661.
Train: 2018-08-04T23:43:25.756256: step 6147, loss 0.488351.
Train: 2018-08-04T23:43:26.006164: step 6148, loss 0.587269.
Train: 2018-08-04T23:43:26.256139: step 6149, loss 0.471577.
Train: 2018-08-04T23:43:26.521684: step 6150, loss 0.570758.
Test: 2018-08-04T23:43:27.786998: step 6150, loss 0.548191.
Train: 2018-08-04T23:43:28.021318: step 6151, loss 0.562451.
Train: 2018-08-04T23:43:28.271289: step 6152, loss 0.570763.
Train: 2018-08-04T23:43:28.521227: step 6153, loss 0.52907.
Train: 2018-08-04T23:43:28.786788: step 6154, loss 0.562414.
Train: 2018-08-04T23:43:29.036706: step 6155, loss 0.554034.
Train: 2018-08-04T23:43:29.302298: step 6156, loss 0.579163.
Train: 2018-08-04T23:43:29.552236: step 6157, loss 0.579174.
Train: 2018-08-04T23:43:29.802152: step 6158, loss 0.545603.
Train: 2018-08-04T23:43:30.052123: step 6159, loss 0.55399.
Train: 2018-08-04T23:43:30.302033: step 6160, loss 0.688411.
Test: 2018-08-04T23:43:31.564025: step 6160, loss 0.549367.
Train: 2018-08-04T23:43:31.798371: step 6161, loss 0.520496.
Train: 2018-08-04T23:43:32.048288: step 6162, loss 0.554038.
Train: 2018-08-04T23:43:32.298229: step 6163, loss 0.554056.
Train: 2018-08-04T23:43:32.548171: step 6164, loss 0.595813.
Train: 2018-08-04T23:43:32.798111: step 6165, loss 0.512436.
Train: 2018-08-04T23:43:33.048083: step 6166, loss 0.562436.
Train: 2018-08-04T23:43:33.298025: step 6167, loss 0.537477.
Train: 2018-08-04T23:43:33.547966: step 6168, loss 0.495883.
Train: 2018-08-04T23:43:33.797907: step 6169, loss 0.529112.
Train: 2018-08-04T23:43:34.063470: step 6170, loss 0.537387.
Test: 2018-08-04T23:43:35.313147: step 6170, loss 0.5496.
Train: 2018-08-04T23:43:35.563089: step 6171, loss 0.520599.
Train: 2018-08-04T23:43:35.813060: step 6172, loss 0.51209.
Train: 2018-08-04T23:43:36.063001: step 6173, loss 0.579205.
Train: 2018-08-04T23:43:36.312943: step 6174, loss 0.570803.
Train: 2018-08-04T23:43:36.562855: step 6175, loss 0.528566.
Train: 2018-08-04T23:43:36.812796: step 6176, loss 0.604695.
Train: 2018-08-04T23:43:37.062768: step 6177, loss 0.486073.
Train: 2018-08-04T23:43:37.328331: step 6178, loss 0.562347.
Train: 2018-08-04T23:43:37.562652: step 6179, loss 0.596385.
Train: 2018-08-04T23:43:37.812592: step 6180, loss 0.638988.
Test: 2018-08-04T23:43:39.079354: step 6180, loss 0.547356.
Train: 2018-08-04T23:43:39.313705: step 6181, loss 0.536827.
Train: 2018-08-04T23:43:39.563645: step 6182, loss 0.494355.
Train: 2018-08-04T23:43:39.813557: step 6183, loss 0.545337.
Train: 2018-08-04T23:43:40.063528: step 6184, loss 0.553834.
Train: 2018-08-04T23:43:40.313470: step 6185, loss 0.553828.
Train: 2018-08-04T23:43:40.563412: step 6186, loss 0.647521.
Train: 2018-08-04T23:43:40.813402: step 6187, loss 0.502817.
Train: 2018-08-04T23:43:41.063333: step 6188, loss 0.579347.
Train: 2018-08-04T23:43:41.313285: step 6189, loss 0.60481.
Train: 2018-08-04T23:43:41.563197: step 6190, loss 0.604732.
Test: 2018-08-04T23:43:42.812905: step 6190, loss 0.547275.
Train: 2018-08-04T23:43:43.000384: step 6191, loss 0.580391.
Train: 2018-08-04T23:43:43.251472: step 6192, loss 0.545526.
Train: 2018-08-04T23:43:43.501381: step 6193, loss 0.503575.
Train: 2018-08-04T23:43:43.751348: step 6194, loss 0.537219.
Train: 2018-08-04T23:43:44.001296: step 6195, loss 0.595935.
Train: 2018-08-04T23:43:44.251235: step 6196, loss 0.595891.
Train: 2018-08-04T23:43:44.501179: step 6197, loss 0.570769.
Train: 2018-08-04T23:43:44.761230: step 6198, loss 0.604097.
Train: 2018-08-04T23:43:44.995552: step 6199, loss 0.446162.
Train: 2018-08-04T23:43:45.245522: step 6200, loss 0.579063.
Test: 2018-08-04T23:43:46.495200: step 6200, loss 0.548412.
Train: 2018-08-04T23:43:47.416888: step 6201, loss 0.562461.
Train: 2018-08-04T23:43:47.666799: step 6202, loss 0.579049.
Train: 2018-08-04T23:43:47.916772: step 6203, loss 0.52935.
Train: 2018-08-04T23:43:48.166682: step 6204, loss 0.545922.
Train: 2018-08-04T23:43:48.416654: step 6205, loss 0.587313.
Train: 2018-08-04T23:43:48.666565: step 6206, loss 0.603846.
Train: 2018-08-04T23:43:48.916507: step 6207, loss 0.579015.
Train: 2018-08-04T23:43:49.166448: step 6208, loss 0.529542.
Train: 2018-08-04T23:43:49.416418: step 6209, loss 0.546053.
Train: 2018-08-04T23:43:49.666331: step 6210, loss 0.52961.
Test: 2018-08-04T23:43:50.916038: step 6210, loss 0.547075.
Train: 2018-08-04T23:43:51.150359: step 6211, loss 0.51314.
Train: 2018-08-04T23:43:51.400299: step 6212, loss 0.653171.
Train: 2018-08-04T23:43:51.650272: step 6213, loss 0.587224.
Train: 2018-08-04T23:43:51.900183: step 6214, loss 0.5872.
Train: 2018-08-04T23:43:52.150155: step 6215, loss 0.554353.
Train: 2018-08-04T23:43:52.400096: step 6216, loss 0.652668.
Train: 2018-08-04T23:43:52.650038: step 6217, loss 0.570772.
Train: 2018-08-04T23:43:52.899980: step 6218, loss 0.530119.
Train: 2018-08-04T23:43:53.149921: step 6219, loss 0.603245.
Train: 2018-08-04T23:43:53.399862: step 6220, loss 0.554625.
Test: 2018-08-04T23:43:54.649540: step 6220, loss 0.548899.
Train: 2018-08-04T23:43:54.883890: step 6221, loss 0.570814.
Train: 2018-08-04T23:43:55.133831: step 6222, loss 0.570827.
Train: 2018-08-04T23:43:55.399363: step 6223, loss 0.554778.
Train: 2018-08-04T23:43:55.633684: step 6224, loss 0.554816.
Train: 2018-08-04T23:43:55.883656: step 6225, loss 0.594868.
Train: 2018-08-04T23:43:56.133598: step 6226, loss 0.538891.
Train: 2018-08-04T23:43:56.383508: step 6227, loss 0.578836.
Train: 2018-08-04T23:43:56.633480: step 6228, loss 0.594634.
Train: 2018-08-04T23:43:56.883422: step 6229, loss 0.594751.
Train: 2018-08-04T23:43:57.133333: step 6230, loss 0.579432.
Test: 2018-08-04T23:43:58.383040: step 6230, loss 0.549029.
Train: 2018-08-04T23:43:58.617385: step 6231, loss 0.579175.
Train: 2018-08-04T23:43:58.867301: step 6232, loss 0.523215.
Train: 2018-08-04T23:43:59.117242: step 6233, loss 0.579055.
Train: 2018-08-04T23:43:59.367215: step 6234, loss 0.570986.
Train: 2018-08-04T23:43:59.617157: step 6235, loss 0.602511.
Train: 2018-08-04T23:43:59.867068: step 6236, loss 0.563147.
Train: 2018-08-04T23:44:00.117040: step 6237, loss 0.516064.
Train: 2018-08-04T23:44:00.366981: step 6238, loss 0.625987.
Train: 2018-08-04T23:44:00.616922: step 6239, loss 0.578886.
Train: 2018-08-04T23:44:00.859805: step 6240, loss 0.594552.
Test: 2018-08-04T23:44:02.109484: step 6240, loss 0.54982.
Train: 2018-08-04T23:44:02.359456: step 6241, loss 0.610164.
Train: 2018-08-04T23:44:02.593776: step 6242, loss 0.578868.
Train: 2018-08-04T23:44:02.843717: step 6243, loss 0.524452.
Train: 2018-08-04T23:44:03.093658: step 6244, loss 0.540054.
Train: 2018-08-04T23:44:03.359191: step 6245, loss 0.617789.
Train: 2018-08-04T23:44:03.609131: step 6246, loss 0.578906.
Train: 2018-08-04T23:44:03.859104: step 6247, loss 0.524643.
Train: 2018-08-04T23:44:04.109045: step 6248, loss 0.485849.
Train: 2018-08-04T23:44:04.358987: step 6249, loss 0.563344.
Train: 2018-08-04T23:44:04.624550: step 6250, loss 0.594504.
Test: 2018-08-04T23:44:05.874226: step 6250, loss 0.550063.
Train: 2018-08-04T23:44:06.108548: step 6251, loss 0.53205.
Train: 2018-08-04T23:44:06.374139: step 6252, loss 0.555417.
Train: 2018-08-04T23:44:06.624050: step 6253, loss 0.649482.
Train: 2018-08-04T23:44:06.874023: step 6254, loss 0.578873.
Train: 2018-08-04T23:44:07.123958: step 6255, loss 0.539627.
Train: 2018-08-04T23:44:07.373905: step 6256, loss 0.578871.
Train: 2018-08-04T23:44:07.623816: step 6257, loss 0.594579.
Train: 2018-08-04T23:44:07.873758: step 6258, loss 0.531714.
Train: 2018-08-04T23:44:08.123729: step 6259, loss 0.626034.
Train: 2018-08-04T23:44:08.373674: step 6260, loss 0.50813.
Test: 2018-08-04T23:44:09.659423: step 6260, loss 0.550528.
Train: 2018-08-04T23:44:09.892470: step 6261, loss 0.547408.
Train: 2018-08-04T23:44:10.142439: step 6262, loss 0.602513.
Train: 2018-08-04T23:44:10.392353: step 6263, loss 0.547297.
Train: 2018-08-04T23:44:10.665031: step 6264, loss 0.555138.
Train: 2018-08-04T23:44:10.907098: step 6265, loss 0.5393.
Train: 2018-08-04T23:44:11.141397: step 6266, loss 0.539135.
Train: 2018-08-04T23:44:11.391337: step 6267, loss 0.555052.
Train: 2018-08-04T23:44:11.656929: step 6268, loss 0.523044.
Train: 2018-08-04T23:44:11.892780: step 6269, loss 0.594886.
Train: 2018-08-04T23:44:12.154819: step 6270, loss 0.635328.
Test: 2018-08-04T23:44:13.417569: step 6270, loss 0.550524.
Train: 2018-08-04T23:44:13.667566: step 6271, loss 0.594931.
Train: 2018-08-04T23:44:13.918845: step 6272, loss 0.546803.
Train: 2018-08-04T23:44:14.156042: step 6273, loss 0.554828.
Train: 2018-08-04T23:44:14.398591: step 6274, loss 0.651052.
Train: 2018-08-04T23:44:14.668003: step 6275, loss 0.586875.
Train: 2018-08-04T23:44:14.915340: step 6276, loss 0.538895.
Train: 2018-08-04T23:44:15.158985: step 6277, loss 0.530977.
Train: 2018-08-04T23:44:15.422871: step 6278, loss 0.594825.
Train: 2018-08-04T23:44:15.655601: step 6279, loss 0.546945.
Train: 2018-08-04T23:44:15.909845: step 6280, loss 0.467167.
Test: 2018-08-04T23:44:17.149840: step 6280, loss 0.549323.
Train: 2018-08-04T23:44:17.384186: step 6281, loss 0.610867.
Train: 2018-08-04T23:44:17.634132: step 6282, loss 0.554852.
Train: 2018-08-04T23:44:17.884073: step 6283, loss 0.49068.
Train: 2018-08-04T23:44:18.134014: step 6284, loss 0.554745.
Train: 2018-08-04T23:44:18.383927: step 6285, loss 0.594955.
Train: 2018-08-04T23:44:18.633897: step 6286, loss 0.473849.
Train: 2018-08-04T23:44:18.899432: step 6287, loss 0.514011.
Train: 2018-08-04T23:44:19.149396: step 6288, loss 0.529983.
Train: 2018-08-04T23:44:19.399343: step 6289, loss 0.62812.
Train: 2018-08-04T23:44:19.649284: step 6290, loss 0.529673.
Test: 2018-08-04T23:44:20.914583: step 6290, loss 0.548371.
Train: 2018-08-04T23:44:21.164554: step 6291, loss 0.529606.
Train: 2018-08-04T23:44:21.398870: step 6292, loss 0.570729.
Train: 2018-08-04T23:44:21.648786: step 6293, loss 0.60393.
Train: 2018-08-04T23:44:21.898754: step 6294, loss 0.512611.
Train: 2018-08-04T23:44:22.148670: step 6295, loss 0.587411.
Train: 2018-08-04T23:44:22.398611: step 6296, loss 0.520708.
Train: 2018-08-04T23:44:22.648553: step 6297, loss 0.470444.
Train: 2018-08-04T23:44:22.898525: step 6298, loss 0.604348.
Train: 2018-08-04T23:44:23.148467: step 6299, loss 0.587621.
Train: 2018-08-04T23:44:23.398377: step 6300, loss 0.537097.
Test: 2018-08-04T23:44:24.663292: step 6300, loss 0.548462.
Train: 2018-08-04T23:44:25.507011: step 6301, loss 0.503272.
Train: 2018-08-04T23:44:25.756922: step 6302, loss 0.596212.
Train: 2018-08-04T23:44:26.006894: step 6303, loss 0.570827.
Train: 2018-08-04T23:44:26.272450: step 6304, loss 0.51144.
Train: 2018-08-04T23:44:26.506778: step 6305, loss 0.596345.
Train: 2018-08-04T23:44:26.756719: step 6306, loss 0.528323.
Train: 2018-08-04T23:44:27.006659: step 6307, loss 0.511256.
Train: 2018-08-04T23:44:27.272226: step 6308, loss 0.511157.
Train: 2018-08-04T23:44:27.522164: step 6309, loss 0.647857.
Train: 2018-08-04T23:44:27.772108: step 6310, loss 0.57089.
Test: 2018-08-04T23:44:29.021783: step 6310, loss 0.548644.
Train: 2018-08-04T23:44:29.264291: step 6311, loss 0.519577.
Train: 2018-08-04T23:44:29.514231: step 6312, loss 0.59656.
Train: 2018-08-04T23:44:29.764173: step 6313, loss 0.596543.
Train: 2018-08-04T23:44:30.014083: step 6314, loss 0.54526.
Train: 2018-08-04T23:44:30.279674: step 6315, loss 0.528225.
Train: 2018-08-04T23:44:30.513998: step 6316, loss 0.511195.
Train: 2018-08-04T23:44:30.763939: step 6317, loss 0.553813.
Train: 2018-08-04T23:44:31.014413: step 6318, loss 0.639109.
Train: 2018-08-04T23:44:31.264355: step 6319, loss 0.502727.
Train: 2018-08-04T23:44:31.529935: step 6320, loss 0.579372.
Test: 2018-08-04T23:44:32.779595: step 6320, loss 0.547552.
Train: 2018-08-04T23:44:33.029567: step 6321, loss 0.553841.
Train: 2018-08-04T23:44:33.279514: step 6322, loss 0.587842.
Train: 2018-08-04T23:44:33.529449: step 6323, loss 0.56235.
Train: 2018-08-04T23:44:33.779362: step 6324, loss 0.536937.
Train: 2018-08-04T23:44:34.029301: step 6325, loss 0.520047.
Train: 2018-08-04T23:44:34.279245: step 6326, loss 0.562357.
Train: 2018-08-04T23:44:34.529216: step 6327, loss 0.604647.
Train: 2018-08-04T23:44:34.779156: step 6328, loss 0.503254.
Train: 2018-08-04T23:44:35.029098: step 6329, loss 0.570808.
Train: 2018-08-04T23:44:35.279009: step 6330, loss 0.621434.
Test: 2018-08-04T23:44:36.528716: step 6330, loss 0.548311.
Train: 2018-08-04T23:44:36.825552: step 6331, loss 0.52869.
Train: 2018-08-04T23:44:37.088453: step 6332, loss 0.553969.
Train: 2018-08-04T23:44:37.334323: step 6333, loss 0.545579.
Train: 2018-08-04T23:44:37.584264: step 6334, loss 0.562388.
Train: 2018-08-04T23:44:37.834172: step 6335, loss 0.528834.
Train: 2018-08-04T23:44:38.084146: step 6336, loss 0.554004.
Train: 2018-08-04T23:44:38.334088: step 6337, loss 0.604336.
Train: 2018-08-04T23:44:38.584021: step 6338, loss 0.595915.
Train: 2018-08-04T23:44:38.833971: step 6339, loss 0.545684.
Train: 2018-08-04T23:44:39.083879: step 6340, loss 0.503964.
Test: 2018-08-04T23:44:40.327464: step 6340, loss 0.547322.
Train: 2018-08-04T23:44:40.561785: step 6341, loss 0.54572.
Train: 2018-08-04T23:44:40.764861: step 6342, loss 0.580232.
Train: 2018-08-04T23:44:41.014788: step 6343, loss 0.495644.
Train: 2018-08-04T23:44:41.264730: step 6344, loss 0.554059.
Train: 2018-08-04T23:44:41.514666: step 6345, loss 0.562409.
Train: 2018-08-04T23:44:41.764608: step 6346, loss 0.570775.
Train: 2018-08-04T23:44:42.014554: step 6347, loss 0.570776.
Train: 2018-08-04T23:44:42.265227: step 6348, loss 0.55403.
Train: 2018-08-04T23:44:42.532168: step 6349, loss 0.528909.
Train: 2018-08-04T23:44:42.762784: step 6350, loss 0.537263.
Test: 2018-08-04T23:44:44.015463: step 6350, loss 0.546842.
Train: 2018-08-04T23:44:44.263552: step 6351, loss 0.503682.
Train: 2018-08-04T23:44:44.525370: step 6352, loss 0.629624.
Train: 2018-08-04T23:44:44.769913: step 6353, loss 0.57079.
Train: 2018-08-04T23:44:45.025849: step 6354, loss 0.638034.
Train: 2018-08-04T23:44:45.275791: step 6355, loss 0.570782.
Train: 2018-08-04T23:44:45.518221: step 6356, loss 0.579147.
Train: 2018-08-04T23:44:45.768161: step 6357, loss 0.537365.
Train: 2018-08-04T23:44:46.018070: step 6358, loss 0.645793.
Train: 2018-08-04T23:44:46.268044: step 6359, loss 0.579066.
Train: 2018-08-04T23:44:46.517986: step 6360, loss 0.645241.
Test: 2018-08-04T23:44:47.756598: step 6360, loss 0.548398.
Train: 2018-08-04T23:44:47.990952: step 6361, loss 0.562525.
Train: 2018-08-04T23:44:48.240858: step 6362, loss 0.628109.
Train: 2018-08-04T23:44:48.490830: step 6363, loss 0.652214.
Train: 2018-08-04T23:44:48.740772: step 6364, loss 0.546558.
Train: 2018-08-04T23:44:48.990713: step 6365, loss 0.506589.
Train: 2018-08-04T23:44:49.240654: step 6366, loss 0.546876.
Train: 2018-08-04T23:44:49.490596: step 6367, loss 0.56292.
Train: 2018-08-04T23:44:49.740541: step 6368, loss 0.55502.
Train: 2018-08-04T23:44:49.990480: step 6369, loss 0.547149.
Train: 2018-08-04T23:44:50.240415: step 6370, loss 0.56303.
Test: 2018-08-04T23:44:51.490133: step 6370, loss 0.550393.
Train: 2018-08-04T23:44:51.724443: step 6371, loss 0.578861.
Train: 2018-08-04T23:44:51.974390: step 6372, loss 0.539389.
Train: 2018-08-04T23:44:52.224331: step 6373, loss 0.531516.
Train: 2018-08-04T23:44:52.474272: step 6374, loss 0.578862.
Train: 2018-08-04T23:44:52.724214: step 6375, loss 0.539373.
Train: 2018-08-04T23:44:52.974156: step 6376, loss 0.61839.
Train: 2018-08-04T23:44:53.224097: step 6377, loss 0.570958.
Train: 2018-08-04T23:44:53.474039: step 6378, loss 0.515653.
Train: 2018-08-04T23:44:53.723980: step 6379, loss 0.507674.
Train: 2018-08-04T23:44:53.973922: step 6380, loss 0.578858.
Test: 2018-08-04T23:44:55.223598: step 6380, loss 0.54988.
Train: 2018-08-04T23:44:55.473570: step 6381, loss 0.515293.
Train: 2018-08-04T23:44:55.714247: step 6382, loss 0.499153.
Train: 2018-08-04T23:44:55.973002: step 6383, loss 0.570858.
Train: 2018-08-04T23:44:56.232898: step 6384, loss 0.602989.
Train: 2018-08-04T23:44:56.482839: step 6385, loss 0.562761.
Train: 2018-08-04T23:44:56.732813: step 6386, loss 0.562721.
Train: 2018-08-04T23:44:56.982753: step 6387, loss 0.586992.
Train: 2018-08-04T23:44:57.232693: step 6388, loss 0.546458.
Train: 2018-08-04T23:44:57.506495: step 6389, loss 0.522045.
Train: 2018-08-04T23:44:57.767743: step 6390, loss 0.603351.
Test: 2018-08-04T23:44:59.006799: step 6390, loss 0.54768.
Train: 2018-08-04T23:44:59.241151: step 6391, loss 0.603405.
Train: 2018-08-04T23:44:59.507369: step 6392, loss 0.578911.
Train: 2018-08-04T23:44:59.747727: step 6393, loss 0.513635.
Train: 2018-08-04T23:44:59.997639: step 6394, loss 0.554414.
Train: 2018-08-04T23:45:00.247610: step 6395, loss 0.488948.
Train: 2018-08-04T23:45:00.497551: step 6396, loss 0.570746.
Train: 2018-08-04T23:45:00.754642: step 6397, loss 0.620114.
Train: 2018-08-04T23:45:01.004609: step 6398, loss 0.587425.
Train: 2018-08-04T23:45:01.263040: step 6399, loss 0.579203.
Train: 2018-08-04T23:45:01.512948: step 6400, loss 0.562566.
Test: 2018-08-04T23:45:02.747035: step 6400, loss 0.548253.
Train: 2018-08-04T23:45:03.590617: step 6401, loss 0.521423.
Train: 2018-08-04T23:45:03.840530: step 6402, loss 0.513236.
Train: 2018-08-04T23:45:04.090496: step 6403, loss 0.620162.
Train: 2018-08-04T23:45:04.340442: step 6404, loss 0.537809.
Train: 2018-08-04T23:45:04.590383: step 6405, loss 0.521399.
Train: 2018-08-04T23:45:04.840296: step 6406, loss 0.537801.
Train: 2018-08-04T23:45:05.090262: step 6407, loss 0.529498.
Train: 2018-08-04T23:45:05.340211: step 6408, loss 0.579029.
Train: 2018-08-04T23:45:05.612336: step 6409, loss 0.562459.
Train: 2018-08-04T23:45:05.859679: step 6410, loss 0.595624.
Test: 2018-08-04T23:45:07.134616: step 6410, loss 0.54898.
Train: 2018-08-04T23:45:07.368967: step 6411, loss 0.545826.
Train: 2018-08-04T23:45:07.623534: step 6412, loss 0.537466.
Train: 2018-08-04T23:45:07.873475: step 6413, loss 0.5791.
Train: 2018-08-04T23:45:08.123386: step 6414, loss 0.562437.
Train: 2018-08-04T23:45:08.393069: step 6415, loss 0.579053.
Train: 2018-08-04T23:45:08.627128: step 6416, loss 0.579105.
Train: 2018-08-04T23:45:08.893592: step 6417, loss 0.554142.
Train: 2018-08-04T23:45:09.135065: step 6418, loss 0.562372.
Train: 2018-08-04T23:45:09.389623: step 6419, loss 0.562355.
Train: 2018-08-04T23:45:09.639536: step 6420, loss 0.487638.
Test: 2018-08-04T23:45:10.893668: step 6420, loss 0.547354.
Train: 2018-08-04T23:45:11.128020: step 6421, loss 0.553875.
Train: 2018-08-04T23:45:11.372751: step 6422, loss 0.554212.
Train: 2018-08-04T23:45:11.638314: step 6423, loss 0.620975.
Train: 2018-08-04T23:45:11.903907: step 6424, loss 0.570788.
Train: 2018-08-04T23:45:12.138198: step 6425, loss 0.537346.
Train: 2018-08-04T23:45:12.388163: step 6426, loss 0.637477.
Train: 2018-08-04T23:45:12.638110: step 6427, loss 0.520959.
Train: 2018-08-04T23:45:12.903673: step 6428, loss 0.579324.
Train: 2018-08-04T23:45:13.153584: step 6429, loss 0.562372.
Train: 2018-08-04T23:45:13.403526: step 6430, loss 0.57077.
Test: 2018-08-04T23:45:14.653269: step 6430, loss 0.548333.
Train: 2018-08-04T23:45:14.903206: step 6431, loss 0.587214.
Train: 2018-08-04T23:45:15.164306: step 6432, loss 0.529526.
Train: 2018-08-04T23:45:15.399551: step 6433, loss 0.578913.
Train: 2018-08-04T23:45:15.649497: step 6434, loss 0.49683.
Train: 2018-08-04T23:45:15.899438: step 6435, loss 0.562535.
Train: 2018-08-04T23:45:16.149366: step 6436, loss 0.587207.
Train: 2018-08-04T23:45:16.399322: step 6437, loss 0.562619.
Train: 2018-08-04T23:45:16.649264: step 6438, loss 0.578843.
Train: 2018-08-04T23:45:16.899207: step 6439, loss 0.595227.
Train: 2018-08-04T23:45:17.164768: step 6440, loss 0.570817.
Test: 2018-08-04T23:45:18.414445: step 6440, loss 0.547786.
Train: 2018-08-04T23:45:18.648766: step 6441, loss 0.603554.
Train: 2018-08-04T23:45:18.898731: step 6442, loss 0.652366.
Train: 2018-08-04T23:45:19.148673: step 6443, loss 0.586934.
Train: 2018-08-04T23:45:19.398614: step 6444, loss 0.587002.
Train: 2018-08-04T23:45:19.648561: step 6445, loss 0.522592.
Train: 2018-08-04T23:45:19.898496: step 6446, loss 0.602861.
Train: 2018-08-04T23:45:20.148443: step 6447, loss 0.626728.
Train: 2018-08-04T23:45:20.398355: step 6448, loss 0.594715.
Train: 2018-08-04T23:45:20.648327: step 6449, loss 0.531456.
Train: 2018-08-04T23:45:20.898239: step 6450, loss 0.484382.
Test: 2018-08-04T23:45:22.147946: step 6450, loss 0.549818.
Train: 2018-08-04T23:45:22.397888: step 6451, loss 0.578882.
Train: 2018-08-04T23:45:22.647853: step 6452, loss 0.563154.
Train: 2018-08-04T23:45:22.897770: step 6453, loss 0.571047.
Train: 2018-08-04T23:45:23.147742: step 6454, loss 0.547478.
Train: 2018-08-04T23:45:23.397684: step 6455, loss 0.571026.
Train: 2018-08-04T23:45:23.663216: step 6456, loss 0.602426.
Train: 2018-08-04T23:45:23.897567: step 6457, loss 0.563216.
Train: 2018-08-04T23:45:24.147508: step 6458, loss 0.539725.
Train: 2018-08-04T23:45:24.397418: step 6459, loss 0.5162.
Train: 2018-08-04T23:45:24.647390: step 6460, loss 0.508231.
Test: 2018-08-04T23:45:25.904488: step 6460, loss 0.54997.
Train: 2018-08-04T23:45:26.154459: step 6461, loss 0.610364.
Train: 2018-08-04T23:45:26.404401: step 6462, loss 0.531535.
Train: 2018-08-04T23:45:26.654344: step 6463, loss 0.539322.
Train: 2018-08-04T23:45:26.904284: step 6464, loss 0.49159.
Train: 2018-08-04T23:45:27.154195: step 6465, loss 0.539002.
Train: 2018-08-04T23:45:27.404166: step 6466, loss 0.594893.
Train: 2018-08-04T23:45:27.663709: step 6467, loss 0.522586.
Train: 2018-08-04T23:45:27.901930: step 6468, loss 0.530414.
Train: 2018-08-04T23:45:28.148580: step 6469, loss 0.530217.
Train: 2018-08-04T23:45:28.404434: step 6470, loss 0.554462.
Test: 2018-08-04T23:45:29.654112: step 6470, loss 0.549249.
Train: 2018-08-04T23:45:29.905717: step 6471, loss 0.603556.
Train: 2018-08-04T23:45:30.148915: step 6472, loss 0.587234.
Train: 2018-08-04T23:45:30.409915: step 6473, loss 0.50481.
Train: 2018-08-04T23:45:30.657025: step 6474, loss 0.620388.
Train: 2018-08-04T23:45:30.906992: step 6475, loss 0.54592.
Train: 2018-08-04T23:45:31.156909: step 6476, loss 0.504358.
Train: 2018-08-04T23:45:31.406880: step 6477, loss 0.570767.
Train: 2018-08-04T23:45:31.652516: step 6478, loss 0.554084.
Train: 2018-08-04T23:45:31.902463: step 6479, loss 0.495568.
Train: 2018-08-04T23:45:32.157749: step 6480, loss 0.537218.
Test: 2018-08-04T23:45:33.407691: step 6480, loss 0.548522.
Train: 2018-08-04T23:45:33.657632: step 6481, loss 0.604463.
Train: 2018-08-04T23:45:33.923195: step 6482, loss 0.545491.
Train: 2018-08-04T23:45:34.173166: step 6483, loss 0.579244.
Train: 2018-08-04T23:45:34.407487: step 6484, loss 0.587746.
Train: 2018-08-04T23:45:34.673053: step 6485, loss 0.520014.
Train: 2018-08-04T23:45:34.939959: step 6486, loss 0.613161.
Train: 2018-08-04T23:45:35.182930: step 6487, loss 0.604711.
Train: 2018-08-04T23:45:35.417246: step 6488, loss 0.562297.
Train: 2018-08-04T23:45:35.667189: step 6489, loss 0.579271.
Train: 2018-08-04T23:45:35.917131: step 6490, loss 0.495012.
Test: 2018-08-04T23:45:37.156255: step 6490, loss 0.548121.
Train: 2018-08-04T23:45:37.406197: step 6491, loss 0.587632.
Train: 2018-08-04T23:45:37.652447: step 6492, loss 0.53719.
Train: 2018-08-04T23:45:37.855530: step 6493, loss 0.472732.
Train: 2018-08-04T23:45:38.109796: step 6494, loss 0.579181.
Train: 2018-08-04T23:45:38.380798: step 6495, loss 0.553904.
Train: 2018-08-04T23:45:38.642958: step 6496, loss 0.579255.
Train: 2018-08-04T23:45:38.898308: step 6497, loss 0.579187.
Train: 2018-08-04T23:45:39.148064: step 6498, loss 0.587703.
Train: 2018-08-04T23:45:39.410990: step 6499, loss 0.478212.
Train: 2018-08-04T23:45:39.658196: step 6500, loss 0.495093.
Test: 2018-08-04T23:45:40.902181: step 6500, loss 0.548856.
Train: 2018-08-04T23:45:41.777792: step 6501, loss 0.570874.
Train: 2018-08-04T23:45:42.027733: step 6502, loss 0.562357.
Train: 2018-08-04T23:45:42.289269: step 6503, loss 0.528574.
Train: 2018-08-04T23:45:42.535249: step 6504, loss 0.630068.
Train: 2018-08-04T23:45:42.775708: step 6505, loss 0.630007.
Train: 2018-08-04T23:45:43.025649: step 6506, loss 0.553896.
Train: 2018-08-04T23:45:43.273512: step 6507, loss 0.6382.
Train: 2018-08-04T23:45:43.523482: step 6508, loss 0.528776.
Train: 2018-08-04T23:45:43.773395: step 6509, loss 0.5624.
Train: 2018-08-04T23:45:44.023365: step 6510, loss 0.528983.
Test: 2018-08-04T23:45:45.273042: step 6510, loss 0.549255.
Train: 2018-08-04T23:45:45.523310: step 6511, loss 0.445585.
Train: 2018-08-04T23:45:45.788847: step 6512, loss 0.537326.
Train: 2018-08-04T23:45:46.049860: step 6513, loss 0.52065.
Train: 2018-08-04T23:45:46.299771: step 6514, loss 0.579194.
Train: 2018-08-04T23:45:46.549714: step 6515, loss 0.486798.
Train: 2018-08-04T23:45:46.799679: step 6516, loss 0.671882.
Train: 2018-08-04T23:45:47.049595: step 6517, loss 0.495013.
Train: 2018-08-04T23:45:47.300985: step 6518, loss 0.562384.
Train: 2018-08-04T23:45:47.550924: step 6519, loss 0.587661.
Train: 2018-08-04T23:45:47.800865: step 6520, loss 0.562379.
Test: 2018-08-04T23:45:49.050542: step 6520, loss 0.547301.
Train: 2018-08-04T23:45:49.284892: step 6521, loss 0.613024.
Train: 2018-08-04T23:45:49.534835: step 6522, loss 0.570762.
Train: 2018-08-04T23:45:49.784770: step 6523, loss 0.638063.
Train: 2018-08-04T23:45:50.034685: step 6524, loss 0.587551.
Train: 2018-08-04T23:45:50.284658: step 6525, loss 0.487266.
Train: 2018-08-04T23:45:50.534568: step 6526, loss 0.595771.
Train: 2018-08-04T23:45:50.784541: step 6527, loss 0.57076.
Train: 2018-08-04T23:45:51.034484: step 6528, loss 0.587344.
Train: 2018-08-04T23:45:51.284418: step 6529, loss 0.521138.
Train: 2018-08-04T23:45:51.534365: step 6530, loss 0.554246.
Test: 2018-08-04T23:45:52.784041: step 6530, loss 0.548366.
Train: 2018-08-04T23:45:53.018362: step 6531, loss 0.496565.
Train: 2018-08-04T23:45:53.268334: step 6532, loss 0.570757.
Train: 2018-08-04T23:45:53.518275: step 6533, loss 0.595489.
Train: 2018-08-04T23:45:53.788046: step 6534, loss 0.53781.
Train: 2018-08-04T23:45:54.039205: step 6535, loss 0.513117.
Train: 2018-08-04T23:45:54.287937: step 6536, loss 0.570756.
Train: 2018-08-04T23:45:54.535925: step 6537, loss 0.570756.
Train: 2018-08-04T23:45:54.790484: step 6538, loss 0.546015.
Train: 2018-08-04T23:45:55.029227: step 6539, loss 0.546004.
Train: 2018-08-04T23:45:55.294787: step 6540, loss 0.620301.
Test: 2018-08-04T23:45:56.529137: step 6540, loss 0.549103.
Train: 2018-08-04T23:45:56.779078: step 6541, loss 0.579008.
Train: 2018-08-04T23:45:57.029021: step 6542, loss 0.570757.
Train: 2018-08-04T23:45:57.278197: step 6543, loss 0.611922.
Train: 2018-08-04T23:45:57.530512: step 6544, loss 0.554331.
Train: 2018-08-04T23:45:57.780422: step 6545, loss 0.562563.
Train: 2018-08-04T23:45:58.030394: step 6546, loss 0.64442.
Train: 2018-08-04T23:45:58.280336: step 6547, loss 0.529998.
Train: 2018-08-04T23:45:58.530277: step 6548, loss 0.513834.
Train: 2018-08-04T23:45:58.780218: step 6549, loss 0.481384.
Train: 2018-08-04T23:45:59.030160: step 6550, loss 0.554509.
Test: 2018-08-04T23:46:00.279837: step 6550, loss 0.548279.
Train: 2018-08-04T23:46:00.529809: step 6551, loss 0.619647.
Train: 2018-08-04T23:46:00.779751: step 6552, loss 0.562636.
Train: 2018-08-04T23:46:01.029661: step 6553, loss 0.5952.
Train: 2018-08-04T23:46:01.279633: step 6554, loss 0.473198.
Train: 2018-08-04T23:46:01.529544: step 6555, loss 0.554494.
Train: 2018-08-04T23:46:01.790125: step 6556, loss 0.546317.
Train: 2018-08-04T23:46:02.040066: step 6557, loss 0.627927.
Train: 2018-08-04T23:46:02.290008: step 6558, loss 0.521787.
Train: 2018-08-04T23:46:02.539979: step 6559, loss 0.521744.
Train: 2018-08-04T23:46:02.789891: step 6560, loss 0.57895.
Test: 2018-08-04T23:46:04.062451: step 6560, loss 0.550017.
Train: 2018-08-04T23:46:04.311468: step 6561, loss 0.570763.
Train: 2018-08-04T23:46:04.557966: step 6562, loss 0.570762.
Train: 2018-08-04T23:46:04.807908: step 6563, loss 0.513333.
Train: 2018-08-04T23:46:05.081865: step 6564, loss 0.587193.
Train: 2018-08-04T23:46:05.323889: step 6565, loss 0.554312.
Train: 2018-08-04T23:46:05.573825: step 6566, loss 0.65306.
Train: 2018-08-04T23:46:05.823773: step 6567, loss 0.595415.
Train: 2018-08-04T23:46:06.073682: step 6568, loss 0.628175.
Train: 2018-08-04T23:46:06.323650: step 6569, loss 0.570768.
Train: 2018-08-04T23:46:06.622855: step 6570, loss 0.538191.
Test: 2018-08-04T23:46:07.883504: step 6570, loss 0.549456.
Train: 2018-08-04T23:46:08.133446: step 6571, loss 0.676448.
Train: 2018-08-04T23:46:08.383417: step 6572, loss 0.562714.
Train: 2018-08-04T23:46:08.633359: step 6573, loss 0.554715.
Train: 2018-08-04T23:46:08.891608: step 6574, loss 0.594918.
Train: 2018-08-04T23:46:09.141549: step 6575, loss 0.586854.
Train: 2018-08-04T23:46:09.391492: step 6576, loss 0.554979.
Train: 2018-08-04T23:46:09.641432: step 6577, loss 0.570925.
Train: 2018-08-04T23:46:09.901944: step 6578, loss 0.563042.
Train: 2018-08-04T23:46:10.151887: step 6579, loss 0.626214.
Train: 2018-08-04T23:46:10.401827: step 6580, loss 0.594587.
Test: 2018-08-04T23:46:11.651505: step 6580, loss 0.551238.
Train: 2018-08-04T23:46:11.901610: step 6581, loss 0.524096.
Train: 2018-08-04T23:46:12.151551: step 6582, loss 0.571082.
Train: 2018-08-04T23:46:12.401492: step 6583, loss 0.524361.
Train: 2018-08-04T23:46:12.651463: step 6584, loss 0.571115.
Train: 2018-08-04T23:46:12.901404: step 6585, loss 0.53999.
Train: 2018-08-04T23:46:13.166968: step 6586, loss 0.532192.
Train: 2018-08-04T23:46:13.416909: step 6587, loss 0.571094.
Train: 2018-08-04T23:46:13.666820: step 6588, loss 0.578886.
Train: 2018-08-04T23:46:13.932414: step 6589, loss 0.586694.
Train: 2018-08-04T23:46:14.182354: step 6590, loss 0.52417.
Test: 2018-08-04T23:46:15.447653: step 6590, loss 0.549782.
Train: 2018-08-04T23:46:15.699121: step 6591, loss 0.602358.
Train: 2018-08-04T23:46:15.930071: step 6592, loss 0.594549.
Train: 2018-08-04T23:46:16.180020: step 6593, loss 0.555368.
Train: 2018-08-04T23:46:16.429960: step 6594, loss 0.57104.
Train: 2018-08-04T23:46:16.679903: step 6595, loss 0.594557.
Train: 2018-08-04T23:46:16.952854: step 6596, loss 0.586714.
Train: 2018-08-04T23:46:17.226334: step 6597, loss 0.586712.
Train: 2018-08-04T23:46:17.473735: step 6598, loss 0.54756.
Train: 2018-08-04T23:46:17.723937: step 6599, loss 0.594537.
Train: 2018-08-04T23:46:17.974069: step 6600, loss 0.524126.
Test: 2018-08-04T23:46:19.226006: step 6600, loss 0.549257.
Train: 2018-08-04T23:46:20.085211: step 6601, loss 0.563228.
Train: 2018-08-04T23:46:20.335146: step 6602, loss 0.56322.
Train: 2018-08-04T23:46:20.585087: step 6603, loss 0.571037.
Train: 2018-08-04T23:46:20.835034: step 6604, loss 0.523963.
Train: 2018-08-04T23:46:21.084976: step 6605, loss 0.492416.
Train: 2018-08-04T23:46:21.334917: step 6606, loss 0.594642.
Train: 2018-08-04T23:46:21.584829: step 6607, loss 0.634231.
Train: 2018-08-04T23:46:21.834801: step 6608, loss 0.570943.
Train: 2018-08-04T23:46:22.084711: step 6609, loss 0.563016.
Train: 2018-08-04T23:46:22.334683: step 6610, loss 0.50751.
Test: 2018-08-04T23:46:23.599981: step 6610, loss 0.550063.
Train: 2018-08-04T23:46:23.834303: step 6611, loss 0.55502.
Train: 2018-08-04T23:46:24.084275: step 6612, loss 0.546999.
Train: 2018-08-04T23:46:24.334215: step 6613, loss 0.538932.
Train: 2018-08-04T23:46:24.584156: step 6614, loss 0.498716.
Train: 2018-08-04T23:46:24.834099: step 6615, loss 0.578817.
Train: 2018-08-04T23:46:25.084043: step 6616, loss 0.538419.
Train: 2018-08-04T23:46:25.333981: step 6617, loss 0.51354.
Train: 2018-08-04T23:46:25.588640: step 6618, loss 0.58723.
Train: 2018-08-04T23:46:25.847833: step 6619, loss 0.52042.
Train: 2018-08-04T23:46:26.093840: step 6620, loss 0.596809.
Test: 2018-08-04T23:46:27.337675: step 6620, loss 0.547275.
Train: 2018-08-04T23:46:27.588895: step 6621, loss 0.571122.
Train: 2018-08-04T23:46:27.827039: step 6622, loss 0.620558.
Train: 2018-08-04T23:46:28.076956: step 6623, loss 0.528405.
Train: 2018-08-04T23:46:28.326896: step 6624, loss 0.596422.
Train: 2018-08-04T23:46:28.576867: step 6625, loss 0.595096.
Train: 2018-08-04T23:46:28.826809: step 6626, loss 0.504021.
Train: 2018-08-04T23:46:29.076750: step 6627, loss 0.5294.
Train: 2018-08-04T23:46:29.326692: step 6628, loss 0.51277.
Train: 2018-08-04T23:46:29.592256: step 6629, loss 0.553472.
Train: 2018-08-04T23:46:29.826577: step 6630, loss 0.487634.
Test: 2018-08-04T23:46:31.092534: step 6630, loss 0.548091.
Train: 2018-08-04T23:46:31.326880: step 6631, loss 0.545409.
Train: 2018-08-04T23:46:31.576825: step 6632, loss 0.50269.
Train: 2018-08-04T23:46:31.826768: step 6633, loss 0.535996.
Train: 2018-08-04T23:46:32.076711: step 6634, loss 0.53872.
Train: 2018-08-04T23:46:32.347961: step 6635, loss 0.512549.
Train: 2018-08-04T23:46:32.597927: step 6636, loss 0.570539.
Train: 2018-08-04T23:46:32.847868: step 6637, loss 0.537125.
Train: 2018-08-04T23:46:33.097816: step 6638, loss 0.599897.
Train: 2018-08-04T23:46:33.347756: step 6639, loss 0.624665.
Train: 2018-08-04T23:46:33.597692: step 6640, loss 0.545294.
Test: 2018-08-04T23:46:34.847374: step 6640, loss 0.547019.
Train: 2018-08-04T23:46:35.081726: step 6641, loss 0.519442.
Train: 2018-08-04T23:46:35.331667: step 6642, loss 0.604963.
Train: 2018-08-04T23:46:35.581609: step 6643, loss 0.570881.
Train: 2018-08-04T23:46:35.800302: step 6644, loss 0.562342.
Train: 2018-08-04T23:46:36.060694: step 6645, loss 0.570856.
Train: 2018-08-04T23:46:36.305633: step 6646, loss 0.613344.
Train: 2018-08-04T23:46:36.555574: step 6647, loss 0.604765.
Train: 2018-08-04T23:46:36.805516: step 6648, loss 0.460899.
Train: 2018-08-04T23:46:37.055457: step 6649, loss 0.553991.
Train: 2018-08-04T23:46:37.305368: step 6650, loss 0.562272.
Test: 2018-08-04T23:46:38.582786: step 6650, loss 0.547499.
Train: 2018-08-04T23:46:38.848349: step 6651, loss 0.553865.
Train: 2018-08-04T23:46:39.100528: step 6652, loss 0.537013.
Train: 2018-08-04T23:46:39.350466: step 6653, loss 0.61276.
Train: 2018-08-04T23:46:39.600411: step 6654, loss 0.63792.
Train: 2018-08-04T23:46:39.850347: step 6655, loss 0.495243.
Train: 2018-08-04T23:46:40.103532: step 6656, loss 0.595743.
Train: 2018-08-04T23:46:40.353482: step 6657, loss 0.537121.
Train: 2018-08-04T23:46:40.603425: step 6658, loss 0.520786.
Train: 2018-08-04T23:46:40.853366: step 6659, loss 0.553869.
Train: 2018-08-04T23:46:41.103307: step 6660, loss 0.579142.
Test: 2018-08-04T23:46:42.336559: step 6660, loss 0.547935.
Train: 2018-08-04T23:46:42.586532: step 6661, loss 0.629208.
Train: 2018-08-04T23:46:42.836473: step 6662, loss 0.620667.
Train: 2018-08-04T23:46:43.086414: step 6663, loss 0.496252.
Train: 2018-08-04T23:46:43.343725: step 6664, loss 0.512949.
Train: 2018-08-04T23:46:43.593635: step 6665, loss 0.587358.
Train: 2018-08-04T23:46:43.843577: step 6666, loss 0.570743.
Train: 2018-08-04T23:46:44.109164: step 6667, loss 0.595578.
Train: 2018-08-04T23:46:44.343490: step 6668, loss 0.570885.
Train: 2018-08-04T23:46:44.593432: step 6669, loss 0.546188.
Train: 2018-08-04T23:46:44.843376: step 6670, loss 0.587127.
Test: 2018-08-04T23:46:46.099305: step 6670, loss 0.547674.
Train: 2018-08-04T23:46:46.333657: step 6671, loss 0.570794.
Train: 2018-08-04T23:46:46.583593: step 6672, loss 0.538237.
Train: 2018-08-04T23:46:46.833539: step 6673, loss 0.652104.
Train: 2018-08-04T23:46:47.083451: step 6674, loss 0.562717.
Train: 2018-08-04T23:46:47.333421: step 6675, loss 0.570799.
Train: 2018-08-04T23:46:47.583364: step 6676, loss 0.570801.
Train: 2018-08-04T23:46:47.833304: step 6677, loss 0.57084.
Train: 2018-08-04T23:46:48.083216: step 6678, loss 0.618933.
Train: 2018-08-04T23:46:48.333183: step 6679, loss 0.586832.
Train: 2018-08-04T23:46:48.601670: step 6680, loss 0.594767.
Test: 2018-08-04T23:46:49.860014: step 6680, loss 0.549973.
Train: 2018-08-04T23:46:50.083453: step 6681, loss 0.5551.
Train: 2018-08-04T23:46:50.349008: step 6682, loss 0.547285.
Train: 2018-08-04T23:46:50.613004: step 6683, loss 0.618248.
Train: 2018-08-04T23:46:50.854001: step 6684, loss 0.516073.
Train: 2018-08-04T23:46:51.119228: step 6685, loss 0.563203.
Train: 2018-08-04T23:46:51.358745: step 6686, loss 0.633675.
Train: 2018-08-04T23:46:51.608656: step 6687, loss 0.594501.
Train: 2018-08-04T23:46:51.874218: step 6688, loss 0.532197.
Train: 2018-08-04T23:46:52.139781: step 6689, loss 0.609993.
Train: 2018-08-04T23:46:52.389756: step 6690, loss 0.571166.
Test: 2018-08-04T23:46:53.656705: step 6690, loss 0.549822.
Train: 2018-08-04T23:46:53.891050: step 6691, loss 0.55572.
Train: 2018-08-04T23:46:54.140997: step 6692, loss 0.517126.
Train: 2018-08-04T23:46:54.390942: step 6693, loss 0.486207.
Train: 2018-08-04T23:46:54.640881: step 6694, loss 0.547933.
Train: 2018-08-04T23:46:54.890822: step 6695, loss 0.563358.
Train: 2018-08-04T23:46:55.140763: step 6696, loss 0.610068.
Train: 2018-08-04T23:46:55.390675: step 6697, loss 0.539879.
Train: 2018-08-04T23:46:55.640650: step 6698, loss 0.563253.
Train: 2018-08-04T23:46:55.890557: step 6699, loss 0.523908.
Train: 2018-08-04T23:46:56.140527: step 6700, loss 0.610387.
Test: 2018-08-04T23:46:57.405828: step 6700, loss 0.549913.
Train: 2018-08-04T23:46:58.280653: step 6701, loss 0.53943.
Train: 2018-08-04T23:46:58.525203: step 6702, loss 0.53935.
Train: 2018-08-04T23:46:58.775111: step 6703, loss 0.563013.
Train: 2018-08-04T23:46:59.025078: step 6704, loss 0.491321.
Train: 2018-08-04T23:46:59.275024: step 6705, loss 0.523007.
Train: 2018-08-04T23:46:59.559799: step 6706, loss 0.554807.
Train: 2018-08-04T23:46:59.802611: step 6707, loss 0.481961.
Train: 2018-08-04T23:47:00.072292: step 6708, loss 0.562526.
Train: 2018-08-04T23:47:00.311885: step 6709, loss 0.570727.
Train: 2018-08-04T23:47:00.574606: step 6710, loss 0.537884.
Test: 2018-08-04T23:47:01.824287: step 6710, loss 0.549438.
Train: 2018-08-04T23:47:02.058633: step 6711, loss 0.546047.
Train: 2018-08-04T23:47:02.308582: step 6712, loss 0.5376.
Train: 2018-08-04T23:47:02.576944: step 6713, loss 0.537026.
Train: 2018-08-04T23:47:02.842476: step 6714, loss 0.562973.
Train: 2018-08-04T23:47:03.092447: step 6715, loss 0.520401.
Train: 2018-08-04T23:47:03.349694: step 6716, loss 0.596328.
Train: 2018-08-04T23:47:03.615257: step 6717, loss 0.622112.
Train: 2018-08-04T23:47:03.865192: step 6718, loss 0.5622.
Train: 2018-08-04T23:47:04.117539: step 6719, loss 0.485797.
Train: 2018-08-04T23:47:04.354056: step 6720, loss 0.562401.
Test: 2018-08-04T23:47:05.588118: step 6720, loss 0.548484.
Train: 2018-08-04T23:47:05.838090: step 6721, loss 0.562318.
Train: 2018-08-04T23:47:06.088000: step 6722, loss 0.545372.
Train: 2018-08-04T23:47:06.337973: step 6723, loss 0.519694.
Train: 2018-08-04T23:47:06.587883: step 6724, loss 0.545246.
Train: 2018-08-04T23:47:06.837824: step 6725, loss 0.545193.
Train: 2018-08-04T23:47:07.087797: step 6726, loss 0.648189.
Train: 2018-08-04T23:47:07.335473: step 6727, loss 0.545161.
Train: 2018-08-04T23:47:07.585384: step 6728, loss 0.570938.
Train: 2018-08-04T23:47:07.835359: step 6729, loss 0.588054.
Train: 2018-08-04T23:47:08.085298: step 6730, loss 0.502472.
Test: 2018-08-04T23:47:09.334975: step 6730, loss 0.548459.
Train: 2018-08-04T23:47:09.569320: step 6731, loss 0.511038.
Train: 2018-08-04T23:47:09.819269: step 6732, loss 0.570891.
Train: 2018-08-04T23:47:10.069178: step 6733, loss 0.468261.
Train: 2018-08-04T23:47:10.334742: step 6734, loss 0.562334.
Train: 2018-08-04T23:47:10.568859: step 6735, loss 0.57093.
Train: 2018-08-04T23:47:10.818800: step 6736, loss 0.502211.
Train: 2018-08-04T23:47:11.068742: step 6737, loss 0.639774.
Train: 2018-08-04T23:47:11.318683: step 6738, loss 0.588155.
Train: 2018-08-04T23:47:11.568593: step 6739, loss 0.562346.
Train: 2018-08-04T23:47:11.818566: step 6740, loss 0.510856.
Test: 2018-08-04T23:47:13.066255: step 6740, loss 0.54722.
Train: 2018-08-04T23:47:13.308053: step 6741, loss 0.62238.
Train: 2018-08-04T23:47:13.573644: step 6742, loss 0.502403.
Train: 2018-08-04T23:47:13.807936: step 6743, loss 0.588003.
Train: 2018-08-04T23:47:14.057907: step 6744, loss 0.51107.
Train: 2018-08-04T23:47:14.307848: step 6745, loss 0.613582.
Train: 2018-08-04T23:47:14.573412: step 6746, loss 0.50265.
Train: 2018-08-04T23:47:14.823352: step 6747, loss 0.570865.
Train: 2018-08-04T23:47:15.057641: step 6748, loss 0.536787.
Train: 2018-08-04T23:47:15.307583: step 6749, loss 0.528279.
Train: 2018-08-04T23:47:15.557555: step 6750, loss 0.570856.
Test: 2018-08-04T23:47:16.814222: step 6750, loss 0.54871.
Train: 2018-08-04T23:47:17.048574: step 6751, loss 0.630462.
Train: 2018-08-04T23:47:17.298485: step 6752, loss 0.587843.
Train: 2018-08-04T23:47:17.548451: step 6753, loss 0.579308.
Train: 2018-08-04T23:47:17.798397: step 6754, loss 0.553908.
Train: 2018-08-04T23:47:18.048339: step 6755, loss 0.545506.
Train: 2018-08-04T23:47:18.298281: step 6756, loss 0.486644.
Train: 2018-08-04T23:47:18.563846: step 6757, loss 0.629692.
Train: 2018-08-04T23:47:18.813754: step 6758, loss 0.621143.
Train: 2018-08-04T23:47:19.063696: step 6759, loss 0.562404.
Train: 2018-08-04T23:47:19.313670: step 6760, loss 0.529043.
Test: 2018-08-04T23:47:20.563344: step 6760, loss 0.548337.
Train: 2018-08-04T23:47:20.797691: step 6761, loss 0.595758.
Train: 2018-08-04T23:47:21.047636: step 6762, loss 0.504342.
Train: 2018-08-04T23:47:21.297578: step 6763, loss 0.603922.
Train: 2018-08-04T23:47:21.547520: step 6764, loss 0.521118.
Train: 2018-08-04T23:47:21.797461: step 6765, loss 0.562493.
Train: 2018-08-04T23:47:22.047402: step 6766, loss 0.545991.
Train: 2018-08-04T23:47:22.297313: step 6767, loss 0.504756.
Train: 2018-08-04T23:47:22.547256: step 6768, loss 0.545988.
Train: 2018-08-04T23:47:22.797224: step 6769, loss 0.57902.
Train: 2018-08-04T23:47:23.047168: step 6770, loss 0.537666.
Test: 2018-08-04T23:47:24.296846: step 6770, loss 0.549612.
Train: 2018-08-04T23:47:24.531192: step 6771, loss 0.521101.
Train: 2018-08-04T23:47:24.812378: step 6772, loss 0.57906.
Train: 2018-08-04T23:47:25.046701: step 6773, loss 0.570792.
Train: 2018-08-04T23:47:25.296643: step 6774, loss 0.653768.
Train: 2018-08-04T23:47:25.562204: step 6775, loss 0.587319.
Train: 2018-08-04T23:47:25.812145: step 6776, loss 0.645282.
Train: 2018-08-04T23:47:26.062087: step 6777, loss 0.562507.
Train: 2018-08-04T23:47:26.312028: step 6778, loss 0.546148.
Train: 2018-08-04T23:47:26.561973: step 6779, loss 0.578947.
Train: 2018-08-04T23:47:26.811907: step 6780, loss 0.578936.
Test: 2018-08-04T23:47:28.077209: step 6780, loss 0.549627.
Train: 2018-08-04T23:47:28.311558: step 6781, loss 0.546367.
Train: 2018-08-04T23:47:28.561496: step 6782, loss 0.554571.
Train: 2018-08-04T23:47:28.827034: step 6783, loss 0.53839.
Train: 2018-08-04T23:47:29.061388: step 6784, loss 0.546519.
Train: 2018-08-04T23:47:29.311296: step 6785, loss 0.63552.
Train: 2018-08-04T23:47:29.561267: step 6786, loss 0.627299.
Train: 2018-08-04T23:47:29.811209: step 6787, loss 0.611041.
Train: 2018-08-04T23:47:30.061150: step 6788, loss 0.554837.
Train: 2018-08-04T23:47:30.311092: step 6789, loss 0.546944.
Train: 2018-08-04T23:47:30.561033: step 6790, loss 0.539069.
Test: 2018-08-04T23:47:31.810711: step 6790, loss 0.549706.
Train: 2018-08-04T23:47:32.045031: step 6791, loss 0.562976.
Train: 2018-08-04T23:47:32.295006: step 6792, loss 0.531268.
Train: 2018-08-04T23:47:32.544944: step 6793, loss 0.531265.
Train: 2018-08-04T23:47:32.794886: step 6794, loss 0.555051.
Train: 2018-08-04T23:47:32.991872: step 6795, loss 0.56308.
Train: 2018-08-04T23:47:33.239812: step 6796, loss 0.610659.
Train: 2018-08-04T23:47:33.489753: step 6797, loss 0.499367.
Train: 2018-08-04T23:47:33.739695: step 6798, loss 0.578857.
Train: 2018-08-04T23:47:33.989607: step 6799, loss 0.634638.
Train: 2018-08-04T23:47:34.239548: step 6800, loss 0.515157.
Test: 2018-08-04T23:47:35.483335: step 6800, loss 0.548144.
Train: 2018-08-04T23:47:36.383468: step 6801, loss 0.682455.
Train: 2018-08-04T23:47:36.633415: step 6802, loss 0.547051.
Train: 2018-08-04T23:47:36.867736: step 6803, loss 0.555033.
Train: 2018-08-04T23:47:37.122911: step 6804, loss 0.570928.
Train: 2018-08-04T23:47:37.372852: step 6805, loss 0.547158.
Train: 2018-08-04T23:47:37.622763: step 6806, loss 0.586781.
Train: 2018-08-04T23:47:37.872705: step 6807, loss 0.578856.
Train: 2018-08-04T23:47:38.122648: step 6808, loss 0.507666.
Train: 2018-08-04T23:47:38.372618: step 6809, loss 0.547194.
Train: 2018-08-04T23:47:38.638150: step 6810, loss 0.602639.
Test: 2018-08-04T23:47:39.890168: step 6810, loss 0.549213.
Train: 2018-08-04T23:47:40.124520: step 6811, loss 0.586788.
Train: 2018-08-04T23:47:40.377391: step 6812, loss 0.467886.
Train: 2018-08-04T23:47:40.627334: step 6813, loss 0.578857.
Train: 2018-08-04T23:47:40.876942: step 6814, loss 0.56294.
Train: 2018-08-04T23:47:41.126892: step 6815, loss 0.570884.
Train: 2018-08-04T23:47:41.376833: step 6816, loss 0.594836.
Train: 2018-08-04T23:47:41.625507: step 6817, loss 0.61083.
Train: 2018-08-04T23:47:41.877178: step 6818, loss 0.57886.
Train: 2018-08-04T23:47:42.137995: step 6819, loss 0.562893.
Train: 2018-08-04T23:47:42.383920: step 6820, loss 0.578861.
Test: 2018-08-04T23:47:43.629643: step 6820, loss 0.550881.
Train: 2018-08-04T23:47:43.879584: step 6821, loss 0.523046.
Train: 2018-08-04T23:47:44.129560: step 6822, loss 0.554929.
Train: 2018-08-04T23:47:44.379467: step 6823, loss 0.491051.
Train: 2018-08-04T23:47:44.629438: step 6824, loss 0.578864.
Train: 2018-08-04T23:47:44.879385: step 6825, loss 0.490662.
Train: 2018-08-04T23:47:45.129292: step 6826, loss 0.474254.
Train: 2018-08-04T23:47:45.379263: step 6827, loss 0.59508.
Train: 2018-08-04T23:47:45.629204: step 6828, loss 0.489524.
Train: 2018-08-04T23:47:45.879116: step 6829, loss 0.521747.
Train: 2018-08-04T23:47:46.124549: step 6830, loss 0.562542.
Test: 2018-08-04T23:47:47.386708: step 6830, loss 0.550215.
Train: 2018-08-04T23:47:47.636673: step 6831, loss 0.570765.
Train: 2018-08-04T23:47:47.886619: step 6832, loss 0.579045.
Train: 2018-08-04T23:47:48.134812: step 6833, loss 0.545796.
Train: 2018-08-04T23:47:48.384754: step 6834, loss 0.545697.
Train: 2018-08-04T23:47:48.634664: step 6835, loss 0.595925.
Train: 2018-08-04T23:47:48.884636: step 6836, loss 0.52876.
Train: 2018-08-04T23:47:49.134549: step 6837, loss 0.528717.
Train: 2018-08-04T23:47:49.400116: step 6838, loss 0.579271.
Train: 2018-08-04T23:47:49.648722: step 6839, loss 0.520088.
Train: 2018-08-04T23:47:49.914254: step 6840, loss 0.519937.
Test: 2018-08-04T23:47:51.163962: step 6840, loss 0.549699.
Train: 2018-08-04T23:47:51.405380: step 6841, loss 0.494264.
Train: 2018-08-04T23:47:51.639726: step 6842, loss 0.553835.
Train: 2018-08-04T23:47:51.889671: step 6843, loss 0.570905.
Train: 2018-08-04T23:47:52.139613: step 6844, loss 0.536506.
Train: 2018-08-04T23:47:52.389524: step 6845, loss 0.631255.
Train: 2018-08-04T23:47:52.639496: step 6846, loss 0.553752.
Train: 2018-08-04T23:47:52.889437: step 6847, loss 0.536474.
Train: 2018-08-04T23:47:53.139380: step 6848, loss 0.588268.
Train: 2018-08-04T23:47:53.404942: step 6849, loss 0.579614.
Train: 2018-08-04T23:47:53.654852: step 6850, loss 0.614049.
Test: 2018-08-04T23:47:54.904561: step 6850, loss 0.547576.
Train: 2018-08-04T23:47:55.154532: step 6851, loss 0.596741.
Train: 2018-08-04T23:47:55.431550: step 6852, loss 0.596636.
Train: 2018-08-04T23:47:55.686173: step 6853, loss 0.562346.
Train: 2018-08-04T23:47:55.922407: step 6854, loss 0.545301.
Train: 2018-08-04T23:47:56.165151: step 6855, loss 0.596301.
Train: 2018-08-04T23:47:56.415092: step 6856, loss 0.511633.
Train: 2018-08-04T23:47:56.665034: step 6857, loss 0.621423.
Train: 2018-08-04T23:47:56.915001: step 6858, loss 0.528762.
Train: 2018-08-04T23:47:57.164950: step 6859, loss 0.562397.
Train: 2018-08-04T23:47:57.421590: step 6860, loss 0.59587.
Test: 2018-08-04T23:47:58.686895: step 6860, loss 0.548503.
Train: 2018-08-04T23:47:58.921249: step 6861, loss 0.612455.
Train: 2018-08-04T23:47:59.186778: step 6862, loss 0.562449.
Train: 2018-08-04T23:47:59.420786: step 6863, loss 0.57903.
Train: 2018-08-04T23:47:59.670694: step 6864, loss 0.554278.
Train: 2018-08-04T23:47:59.920665: step 6865, loss 0.521491.
Train: 2018-08-04T23:48:00.170607: step 6866, loss 0.636312.
Train: 2018-08-04T23:48:00.420548: step 6867, loss 0.603425.
Train: 2018-08-04T23:48:00.670490: step 6868, loss 0.64394.
Train: 2018-08-04T23:48:00.912108: step 6869, loss 0.530401.
Train: 2018-08-04T23:48:01.162019: step 6870, loss 0.554738.
Test: 2018-08-04T23:48:02.411728: step 6870, loss 0.548526.
Train: 2018-08-04T23:48:02.661669: step 6871, loss 0.642991.
Train: 2018-08-04T23:48:02.911643: step 6872, loss 0.586833.
Train: 2018-08-04T23:48:03.161581: step 6873, loss 0.570926.
Train: 2018-08-04T23:48:03.405817: step 6874, loss 0.578858.
Train: 2018-08-04T23:48:03.655758: step 6875, loss 0.547434.
Train: 2018-08-04T23:48:03.905667: step 6876, loss 0.649357.
Train: 2018-08-04T23:48:04.155607: step 6877, loss 0.578901.
Train: 2018-08-04T23:48:04.421172: step 6878, loss 0.578928.
Train: 2018-08-04T23:48:04.671112: step 6879, loss 0.586658.
Train: 2018-08-04T23:48:04.921085: step 6880, loss 0.525167.
Test: 2018-08-04T23:48:06.169837: step 6880, loss 0.551204.
Train: 2018-08-04T23:48:06.404476: step 6881, loss 0.486999.
Train: 2018-08-04T23:48:06.701282: step 6882, loss 0.58665.
Train: 2018-08-04T23:48:06.953576: step 6883, loss 0.548342.
Train: 2018-08-04T23:48:07.219168: step 6884, loss 0.58664.
Train: 2018-08-04T23:48:07.494444: step 6885, loss 0.532951.
Train: 2018-08-04T23:48:07.730463: step 6886, loss 0.540541.
Train: 2018-08-04T23:48:08.005694: step 6887, loss 0.555895.
Train: 2018-08-04T23:48:08.251308: step 6888, loss 0.540305.
Train: 2018-08-04T23:48:08.492170: step 6889, loss 0.563327.
Train: 2018-08-04T23:48:08.748477: step 6890, loss 0.540106.
Test: 2018-08-04T23:48:09.998185: step 6890, loss 0.549592.
Train: 2018-08-04T23:48:10.232505: step 6891, loss 0.532017.
Train: 2018-08-04T23:48:10.481414: step 6892, loss 0.578412.
Train: 2018-08-04T23:48:10.731324: step 6893, loss 0.505993.
Train: 2018-08-04T23:48:10.981266: step 6894, loss 0.53885.
Train: 2018-08-04T23:48:11.230892: step 6895, loss 0.570909.
Train: 2018-08-04T23:48:11.480804: step 6896, loss 0.566097.
Train: 2018-08-04T23:48:11.730775: step 6897, loss 0.500997.
Train: 2018-08-04T23:48:11.980716: step 6898, loss 0.527873.
Train: 2018-08-04T23:48:12.263678: step 6899, loss 0.614393.
Train: 2018-08-04T23:48:12.508286: step 6900, loss 0.568373.
Test: 2018-08-04T23:48:13.783387: step 6900, loss 0.546324.
Train: 2018-08-04T23:48:14.642591: step 6901, loss 0.536129.
Train: 2018-08-04T23:48:14.892533: step 6902, loss 0.555578.
Train: 2018-08-04T23:48:15.149101: step 6903, loss 0.56934.
Train: 2018-08-04T23:48:15.414664: step 6904, loss 0.486081.
Train: 2018-08-04T23:48:15.667182: step 6905, loss 0.63872.
Train: 2018-08-04T23:48:15.917155: step 6906, loss 0.554238.
Train: 2018-08-04T23:48:16.167095: step 6907, loss 0.596845.
Train: 2018-08-04T23:48:16.417038: step 6908, loss 0.52062.
Train: 2018-08-04T23:48:16.666979: step 6909, loss 0.613279.
Train: 2018-08-04T23:48:16.916890: step 6910, loss 0.587645.
Test: 2018-08-04T23:48:18.196673: step 6910, loss 0.548863.
Train: 2018-08-04T23:48:18.446614: step 6911, loss 0.578922.
Train: 2018-08-04T23:48:18.722083: step 6912, loss 0.578909.
Train: 2018-08-04T23:48:18.958944: step 6913, loss 0.570844.
Train: 2018-08-04T23:48:19.208855: step 6914, loss 0.530664.
Train: 2018-08-04T23:48:19.474442: step 6915, loss 0.602951.
Train: 2018-08-04T23:48:19.715955: step 6916, loss 0.626937.
Train: 2018-08-04T23:48:19.965863: step 6917, loss 0.506955.
Train: 2018-08-04T23:48:20.215805: step 6918, loss 0.459163.
Train: 2018-08-04T23:48:20.465746: step 6919, loss 0.490999.
Train: 2018-08-04T23:48:20.724839: step 6920, loss 0.506713.
Test: 2018-08-04T23:48:21.974547: step 6920, loss 0.548745.
Train: 2018-08-04T23:48:22.208900: step 6921, loss 0.611005.
Train: 2018-08-04T23:48:22.458840: step 6922, loss 0.546581.
Train: 2018-08-04T23:48:22.708781: step 6923, loss 0.522113.
Train: 2018-08-04T23:48:22.958723: step 6924, loss 0.513792.
Train: 2018-08-04T23:48:23.208664: step 6925, loss 0.587088.
Train: 2018-08-04T23:48:23.458602: step 6926, loss 0.537935.
Train: 2018-08-04T23:48:23.708547: step 6927, loss 0.512908.
Train: 2018-08-04T23:48:23.952622: step 6928, loss 0.562275.
Train: 2018-08-04T23:48:24.202592: step 6929, loss 0.52071.
Train: 2018-08-04T23:48:24.448003: step 6930, loss 0.628886.
Test: 2018-08-04T23:48:25.700076: step 6930, loss 0.548628.
Train: 2018-08-04T23:48:25.934428: step 6931, loss 0.445198.
Train: 2018-08-04T23:48:26.184368: step 6932, loss 0.612899.
Train: 2018-08-04T23:48:26.434313: step 6933, loss 0.553802.
Train: 2018-08-04T23:48:26.684221: step 6934, loss 0.554105.
Train: 2018-08-04T23:48:26.934187: step 6935, loss 0.60481.
Train: 2018-08-04T23:48:27.180398: step 6936, loss 0.520058.
Train: 2018-08-04T23:48:27.430340: step 6937, loss 0.554014.
Train: 2018-08-04T23:48:27.680281: step 6938, loss 0.613387.
Train: 2018-08-04T23:48:27.930222: step 6939, loss 0.588062.
Train: 2018-08-04T23:48:28.180310: step 6940, loss 0.579353.
Test: 2018-08-04T23:48:29.425611: step 6940, loss 0.54762.
Train: 2018-08-04T23:48:29.659931: step 6941, loss 0.53702.
Train: 2018-08-04T23:48:29.925494: step 6942, loss 0.596149.
Train: 2018-08-04T23:48:30.174168: step 6943, loss 0.646772.
Train: 2018-08-04T23:48:30.432777: step 6944, loss 0.537006.
Train: 2018-08-04T23:48:30.667103: step 6945, loss 0.620878.
Train: 2018-08-04T23:48:30.870150: step 6946, loss 0.58026.
Train: 2018-08-04T23:48:31.121146: step 6947, loss 0.603888.
Train: 2018-08-04T23:48:31.371088: step 6948, loss 0.587317.
Train: 2018-08-04T23:48:31.621060: step 6949, loss 0.587218.
Train: 2018-08-04T23:48:31.871002: step 6950, loss 0.595379.
Test: 2018-08-04T23:48:33.121876: step 6950, loss 0.548655.
Train: 2018-08-04T23:48:33.356195: step 6951, loss 0.603487.
Train: 2018-08-04T23:48:33.606167: step 6952, loss 0.522186.
Train: 2018-08-04T23:48:33.849563: step 6953, loss 0.619168.
Train: 2018-08-04T23:48:34.095938: step 6954, loss 0.498709.
Train: 2018-08-04T23:48:34.342781: step 6955, loss 0.546924.
Train: 2018-08-04T23:48:34.602821: step 6956, loss 0.554996.
Train: 2018-08-04T23:48:34.852761: step 6957, loss 0.539039.
Train: 2018-08-04T23:48:35.102672: step 6958, loss 0.586774.
Train: 2018-08-04T23:48:35.352645: step 6959, loss 0.61064.
Train: 2018-08-04T23:48:35.602557: step 6960, loss 0.539219.
Test: 2018-08-04T23:48:36.852264: step 6960, loss 0.550357.
Train: 2018-08-04T23:48:37.086614: step 6961, loss 0.523478.
Train: 2018-08-04T23:48:37.352171: step 6962, loss 0.523465.
Train: 2018-08-04T23:48:37.586466: step 6963, loss 0.563016.
Train: 2018-08-04T23:48:37.852030: step 6964, loss 0.586802.
Train: 2018-08-04T23:48:38.086380: step 6965, loss 0.562958.
Train: 2018-08-04T23:48:38.336321: step 6966, loss 0.594759.
Train: 2018-08-04T23:48:38.601853: step 6967, loss 0.53913.
Train: 2018-08-04T23:48:38.867416: step 6968, loss 0.594791.
Train: 2018-08-04T23:48:39.101769: step 6969, loss 0.570923.
Train: 2018-08-04T23:48:39.351707: step 6970, loss 0.602689.
Test: 2018-08-04T23:48:40.608017: step 6970, loss 0.5497.
Train: 2018-08-04T23:48:40.874015: step 6971, loss 0.547069.
Train: 2018-08-04T23:48:41.123988: step 6972, loss 0.578856.
Train: 2018-08-04T23:48:41.373929: step 6973, loss 0.602697.
Train: 2018-08-04T23:48:41.630478: step 6974, loss 0.531309.
Train: 2018-08-04T23:48:41.880386: step 6975, loss 0.586796.
Train: 2018-08-04T23:48:42.130360: step 6976, loss 0.563017.
Train: 2018-08-04T23:48:42.380643: step 6977, loss 0.56303.
Train: 2018-08-04T23:48:42.630554: step 6978, loss 0.61051.
Train: 2018-08-04T23:48:42.891252: step 6979, loss 0.586763.
Train: 2018-08-04T23:48:43.130801: step 6980, loss 0.507848.
Test: 2018-08-04T23:48:44.391078: step 6980, loss 0.550444.
Train: 2018-08-04T23:48:44.625430: step 6981, loss 0.52362.
Train: 2018-08-04T23:48:44.875372: step 6982, loss 0.547252.
Train: 2018-08-04T23:48:45.125281: step 6983, loss 0.555111.
Train: 2018-08-04T23:48:45.375254: step 6984, loss 0.539201.
Train: 2018-08-04T23:48:45.625190: step 6985, loss 0.51525.
Train: 2018-08-04T23:48:45.875108: step 6986, loss 0.530985.
Train: 2018-08-04T23:48:46.125073: step 6987, loss 0.570854.
Train: 2018-08-04T23:48:46.374990: step 6988, loss 0.594953.
Train: 2018-08-04T23:48:46.620341: step 6989, loss 0.530519.
Train: 2018-08-04T23:48:46.870282: step 6990, loss 0.57889.
Test: 2018-08-04T23:48:48.122655: step 6990, loss 0.549716.
Train: 2018-08-04T23:48:48.356976: step 6991, loss 0.554586.
Train: 2018-08-04T23:48:48.606917: step 6992, loss 0.538292.
Train: 2018-08-04T23:48:48.856858: step 6993, loss 0.513756.
Train: 2018-08-04T23:48:49.122422: step 6994, loss 0.578943.
Train: 2018-08-04T23:48:49.372362: step 6995, loss 0.660946.
Train: 2018-08-04T23:48:49.626442: step 6996, loss 0.628166.
Train: 2018-08-04T23:48:49.876413: step 6997, loss 0.538006.
Train: 2018-08-04T23:48:50.126354: step 6998, loss 0.636238.
Train: 2018-08-04T23:48:50.376295: step 6999, loss 0.521781.
Train: 2018-08-04T23:48:50.626206: step 7000, loss 0.611551.
Test: 2018-08-04T23:48:51.931717: step 7000, loss 0.548865.
Train: 2018-08-04T23:48:52.798103: step 7001, loss 0.489404.
Train: 2018-08-04T23:48:53.048037: step 7002, loss 0.570781.
Train: 2018-08-04T23:48:53.297984: step 7003, loss 0.611444.
Train: 2018-08-04T23:48:53.547927: step 7004, loss 0.554546.
Train: 2018-08-04T23:48:53.797837: step 7005, loss 0.578902.
Train: 2018-08-04T23:48:54.047808: step 7006, loss 0.554597.
Train: 2018-08-04T23:48:54.297722: step 7007, loss 0.586984.
Train: 2018-08-04T23:48:54.547695: step 7008, loss 0.586967.
Train: 2018-08-04T23:48:54.797604: step 7009, loss 0.522426.
Train: 2018-08-04T23:48:55.047575: step 7010, loss 0.54664.
Test: 2018-08-04T23:48:56.297252: step 7010, loss 0.550416.
Train: 2018-08-04T23:48:56.531574: step 7011, loss 0.578879.
Train: 2018-08-04T23:48:56.781514: step 7012, loss 0.586934.
Train: 2018-08-04T23:48:57.031454: step 7013, loss 0.570828.
Train: 2018-08-04T23:48:57.281430: step 7014, loss 0.578873.
Train: 2018-08-04T23:48:57.531338: step 7015, loss 0.546747.
Train: 2018-08-04T23:48:57.781279: step 7016, loss 0.570843.
Train: 2018-08-04T23:48:58.046843: step 7017, loss 0.5147.
Train: 2018-08-04T23:48:58.281164: step 7018, loss 0.554789.
Train: 2018-08-04T23:48:58.526913: step 7019, loss 0.530669.
Train: 2018-08-04T23:48:58.776855: step 7020, loss 0.594969.
Test: 2018-08-04T23:49:00.044758: step 7020, loss 0.550248.
Train: 2018-08-04T23:49:00.279078: step 7021, loss 0.56277.
Train: 2018-08-04T23:49:00.529019: step 7022, loss 0.554701.
Train: 2018-08-04T23:49:00.778961: step 7023, loss 0.554681.
Train: 2018-08-04T23:49:01.028903: step 7024, loss 0.627342.
Train: 2018-08-04T23:49:01.278845: step 7025, loss 0.570812.
Train: 2018-08-04T23:49:01.528816: step 7026, loss 0.538547.
Train: 2018-08-04T23:49:01.778757: step 7027, loss 0.55468.
Train: 2018-08-04T23:49:02.028668: step 7028, loss 0.538531.
Train: 2018-08-04T23:49:02.294231: step 7029, loss 0.56273.
Train: 2018-08-04T23:49:02.544173: step 7030, loss 0.498035.
Test: 2018-08-04T23:49:03.816212: step 7030, loss 0.549527.
Train: 2018-08-04T23:49:04.091874: step 7031, loss 0.554585.
Train: 2018-08-04T23:49:04.324953: step 7032, loss 0.530162.
Train: 2018-08-04T23:49:04.574924: step 7033, loss 0.587074.
Train: 2018-08-04T23:49:04.827081: step 7034, loss 0.53811.
Train: 2018-08-04T23:49:05.076991: step 7035, loss 0.562581.
Train: 2018-08-04T23:49:05.326958: step 7036, loss 0.570763.
Train: 2018-08-04T23:49:05.592522: step 7037, loss 0.603627.
Train: 2018-08-04T23:49:05.849145: step 7038, loss 0.546095.
Train: 2018-08-04T23:49:06.099089: step 7039, loss 0.570759.
Train: 2018-08-04T23:49:06.349028: step 7040, loss 0.570756.
Test: 2018-08-04T23:49:07.598702: step 7040, loss 0.547825.
Train: 2018-08-04T23:49:07.833024: step 7041, loss 0.55429.
Train: 2018-08-04T23:49:08.082995: step 7042, loss 0.570757.
Train: 2018-08-04T23:49:08.332930: step 7043, loss 0.587232.
Train: 2018-08-04T23:49:08.582878: step 7044, loss 0.628383.
Train: 2018-08-04T23:49:08.824275: step 7045, loss 0.529685.
Train: 2018-08-04T23:49:09.074246: step 7046, loss 0.472299.
Train: 2018-08-04T23:49:09.324188: step 7047, loss 0.57076.
Train: 2018-08-04T23:49:09.574123: step 7048, loss 0.578978.
Train: 2018-08-04T23:49:09.824067: step 7049, loss 0.488548.
Train: 2018-08-04T23:49:10.074012: step 7050, loss 0.570757.
Test: 2018-08-04T23:49:11.339310: step 7050, loss 0.548733.
Train: 2018-08-04T23:49:11.573630: step 7051, loss 0.595501.
Train: 2018-08-04T23:49:11.823571: step 7052, loss 0.554252.
Train: 2018-08-04T23:49:12.076064: step 7053, loss 0.579012.
Train: 2018-08-04T23:49:12.326035: step 7054, loss 0.512958.
Train: 2018-08-04T23:49:12.591592: step 7055, loss 0.52116.
Train: 2018-08-04T23:49:12.825887: step 7056, loss 0.628729.
Train: 2018-08-04T23:49:13.075859: step 7057, loss 0.521061.
Train: 2018-08-04T23:49:13.325771: step 7058, loss 0.537595.
Train: 2018-08-04T23:49:13.575712: step 7059, loss 0.628871.
Train: 2018-08-04T23:49:13.825683: step 7060, loss 0.537565.
Test: 2018-08-04T23:49:15.084981: step 7060, loss 0.54879.
Train: 2018-08-04T23:49:15.319303: step 7061, loss 0.562459.
Train: 2018-08-04T23:49:15.569243: step 7062, loss 0.55416.
Train: 2018-08-04T23:49:15.819215: step 7063, loss 0.65376.
Train: 2018-08-04T23:49:16.069152: step 7064, loss 0.537622.
Train: 2018-08-04T23:49:16.319093: step 7065, loss 0.512855.
Train: 2018-08-04T23:49:16.569039: step 7066, loss 0.562487.
Train: 2018-08-04T23:49:16.818981: step 7067, loss 0.587292.
Train: 2018-08-04T23:49:17.068918: step 7068, loss 0.512925.
Train: 2018-08-04T23:49:17.318866: step 7069, loss 0.562496.
Train: 2018-08-04T23:49:17.568805: step 7070, loss 0.537695.
Test: 2018-08-04T23:49:18.816155: step 7070, loss 0.547528.
Train: 2018-08-04T23:49:19.050507: step 7071, loss 0.570749.
Train: 2018-08-04T23:49:19.300416: step 7072, loss 0.46318.
Train: 2018-08-04T23:49:19.550358: step 7073, loss 0.504394.
Train: 2018-08-04T23:49:19.800330: step 7074, loss 0.595754.
Train: 2018-08-04T23:49:20.050271: step 7075, loss 0.537376.
Train: 2018-08-04T23:49:20.315834: step 7076, loss 0.503789.
Train: 2018-08-04T23:49:20.550155: step 7077, loss 0.587563.
Train: 2018-08-04T23:49:20.815687: step 7078, loss 0.579309.
Train: 2018-08-04T23:49:21.065628: step 7079, loss 0.612973.
Train: 2018-08-04T23:49:21.315569: step 7080, loss 0.604564.
Test: 2018-08-04T23:49:22.580900: step 7080, loss 0.548307.
Train: 2018-08-04T23:49:22.815220: step 7081, loss 0.629764.
Train: 2018-08-04T23:49:23.056189: step 7082, loss 0.545585.
Train: 2018-08-04T23:49:23.306130: step 7083, loss 0.604296.
Train: 2018-08-04T23:49:23.556041: step 7084, loss 0.462167.
Train: 2018-08-04T23:49:23.821633: step 7085, loss 0.545721.
Train: 2018-08-04T23:49:24.055955: step 7086, loss 0.537379.
Train: 2018-08-04T23:49:24.321524: step 7087, loss 0.612517.
Train: 2018-08-04T23:49:24.566844: step 7088, loss 0.570766.
Train: 2018-08-04T23:49:24.827776: step 7089, loss 0.5541.
Train: 2018-08-04T23:49:25.064060: step 7090, loss 0.562442.
Test: 2018-08-04T23:49:26.309259: step 7090, loss 0.547797.
Train: 2018-08-04T23:49:26.559225: step 7091, loss 0.604017.
Train: 2018-08-04T23:49:26.809143: step 7092, loss 0.520976.
Train: 2018-08-04T23:49:27.059084: step 7093, loss 0.504442.
Train: 2018-08-04T23:49:27.309055: step 7094, loss 0.529288.
Train: 2018-08-04T23:49:27.558968: step 7095, loss 0.595668.
Train: 2018-08-04T23:49:27.808938: step 7096, loss 0.562455.
Train: 2018-08-04T23:49:28.011986: step 7097, loss 0.491607.
Train: 2018-08-04T23:49:28.275875: step 7098, loss 0.51255.
Train: 2018-08-04T23:49:28.517643: step 7099, loss 0.587435.
Train: 2018-08-04T23:49:28.778950: step 7100, loss 0.570768.
Test: 2018-08-04T23:49:30.051496: step 7100, loss 0.549038.
Train: 2018-08-04T23:49:30.952634: step 7101, loss 0.662681.
Train: 2018-08-04T23:49:31.213096: step 7102, loss 0.554083.
Train: 2018-08-04T23:49:31.454488: step 7103, loss 0.495773.
Train: 2018-08-04T23:49:31.710846: step 7104, loss 0.595771.
Train: 2018-08-04T23:49:31.956672: step 7105, loss 0.537454.
Train: 2018-08-04T23:49:32.212010: step 7106, loss 0.545777.
Train: 2018-08-04T23:49:32.461952: step 7107, loss 0.470809.
Train: 2018-08-04T23:49:32.711893: step 7108, loss 0.604158.
Train: 2018-08-04T23:49:32.961858: step 7109, loss 0.537354.
Train: 2018-08-04T23:49:33.211806: step 7110, loss 0.620955.
Test: 2018-08-04T23:49:34.453154: step 7110, loss 0.547486.
Train: 2018-08-04T23:49:34.703126: step 7111, loss 0.570772.
Train: 2018-08-04T23:49:34.953654: step 7112, loss 0.528994.
Train: 2018-08-04T23:49:35.203594: step 7113, loss 0.562415.
Train: 2018-08-04T23:49:35.453534: step 7114, loss 0.512287.
Train: 2018-08-04T23:49:35.703475: step 7115, loss 0.545685.
Train: 2018-08-04T23:49:35.953386: step 7116, loss 0.587518.
Train: 2018-08-04T23:49:36.203328: step 7117, loss 0.537281.
Train: 2018-08-04T23:49:36.453270: step 7118, loss 0.629434.
Train: 2018-08-04T23:49:36.703212: step 7119, loss 0.545663.
Train: 2018-08-04T23:49:36.953152: step 7120, loss 0.512219.
Test: 2018-08-04T23:49:38.202860: step 7120, loss 0.548623.
Train: 2018-08-04T23:49:38.452801: step 7121, loss 0.562407.
Train: 2018-08-04T23:49:38.718364: step 7122, loss 0.570774.
Train: 2018-08-04T23:49:38.968307: step 7123, loss 0.570774.
Train: 2018-08-04T23:49:39.218247: step 7124, loss 0.554043.
Train: 2018-08-04T23:49:39.483840: step 7125, loss 0.587497.
Train: 2018-08-04T23:49:39.725360: step 7126, loss 0.587478.
Train: 2018-08-04T23:49:39.975302: step 7127, loss 0.512383.
Train: 2018-08-04T23:49:40.225243: step 7128, loss 0.562428.
Train: 2018-08-04T23:49:40.475153: step 7129, loss 0.579098.
Train: 2018-08-04T23:49:40.731049: step 7130, loss 0.595737.
Test: 2018-08-04T23:49:41.983521: step 7130, loss 0.54857.
Train: 2018-08-04T23:49:42.216405: step 7131, loss 0.520895.
Train: 2018-08-04T23:49:42.457293: step 7132, loss 0.545844.
Train: 2018-08-04T23:49:42.714863: step 7133, loss 0.562457.
Train: 2018-08-04T23:49:42.966926: step 7134, loss 0.587356.
Train: 2018-08-04T23:49:43.209572: step 7135, loss 0.570757.
Train: 2018-08-04T23:49:43.469926: step 7136, loss 0.60388.
Train: 2018-08-04T23:49:43.721748: step 7137, loss 0.587283.
Train: 2018-08-04T23:49:43.971664: step 7138, loss 0.496577.
Train: 2018-08-04T23:49:44.220640: step 7139, loss 0.496638.
Train: 2018-08-04T23:49:44.486604: step 7140, loss 0.521307.
Test: 2018-08-04T23:49:45.733905: step 7140, loss 0.548716.
Train: 2018-08-04T23:49:45.968226: step 7141, loss 0.545993.
Train: 2018-08-04T23:49:46.218173: step 7142, loss 0.521144.
Train: 2018-08-04T23:49:46.489270: step 7143, loss 0.645358.
Train: 2018-08-04T23:49:46.739075: step 7144, loss 0.545887.
Train: 2018-08-04T23:49:46.988522: step 7145, loss 0.603935.
Train: 2018-08-04T23:49:47.235932: step 7146, loss 0.54589.
Train: 2018-08-04T23:49:47.473440: step 7147, loss 0.587332.
Train: 2018-08-04T23:49:47.733941: step 7148, loss 0.554197.
Train: 2018-08-04T23:49:47.980917: step 7149, loss 0.529386.
Train: 2018-08-04T23:49:48.235881: step 7150, loss 0.521109.
Test: 2018-08-04T23:49:49.471561: step 7150, loss 0.549214.
Train: 2018-08-04T23:49:49.721528: step 7151, loss 0.562475.
Train: 2018-08-04T23:49:49.968000: step 7152, loss 0.562469.
Train: 2018-08-04T23:49:50.233532: step 7153, loss 0.570758.
Train: 2018-08-04T23:49:50.467883: step 7154, loss 0.537575.
Train: 2018-08-04T23:49:50.717822: step 7155, loss 0.537552.
Train: 2018-08-04T23:49:50.967765: step 7156, loss 0.64556.
Train: 2018-08-04T23:49:51.217706: step 7157, loss 0.512638.
Train: 2018-08-04T23:49:51.467648: step 7158, loss 0.562455.
Train: 2018-08-04T23:49:51.733182: step 7159, loss 0.537541.
Train: 2018-08-04T23:49:51.967533: step 7160, loss 0.653849.
Test: 2018-08-04T23:49:53.219048: step 7160, loss 0.548038.
Train: 2018-08-04T23:49:53.454516: step 7161, loss 0.545874.
Train: 2018-08-04T23:49:53.704466: step 7162, loss 0.504483.
Train: 2018-08-04T23:49:53.954408: step 7163, loss 0.562472.
Train: 2018-08-04T23:49:54.204350: step 7164, loss 0.579042.
Train: 2018-08-04T23:49:54.454292: step 7165, loss 0.512785.
Train: 2018-08-04T23:49:54.704235: step 7166, loss 0.570757.
Train: 2018-08-04T23:49:54.973290: step 7167, loss 0.521019.
Train: 2018-08-04T23:49:55.223231: step 7168, loss 0.529262.
Train: 2018-08-04T23:49:55.483637: step 7169, loss 0.537506.
Train: 2018-08-04T23:49:55.733579: step 7170, loss 0.479131.
Test: 2018-08-04T23:49:56.998878: step 7170, loss 0.54768.
Train: 2018-08-04T23:49:57.233231: step 7171, loss 0.604212.
Train: 2018-08-04T23:49:57.483170: step 7172, loss 0.604289.
Train: 2018-08-04T23:49:57.733111: step 7173, loss 0.545626.
Train: 2018-08-04T23:49:57.983053: step 7174, loss 0.646323.
Train: 2018-08-04T23:49:58.232994: step 7175, loss 0.646233.
Train: 2018-08-04T23:49:58.482935: step 7176, loss 0.587488.
Train: 2018-08-04T23:49:58.732847: step 7177, loss 0.570764.
Train: 2018-08-04T23:49:58.982788: step 7178, loss 0.603964.
Train: 2018-08-04T23:49:59.232760: step 7179, loss 0.570756.
Train: 2018-08-04T23:49:59.482701: step 7180, loss 0.53782.
Test: 2018-08-04T23:50:00.729148: step 7180, loss 0.548275.
Train: 2018-08-04T23:50:00.979090: step 7181, loss 0.587179.
Train: 2018-08-04T23:50:01.229061: step 7182, loss 0.619854.
Train: 2018-08-04T23:50:01.478996: step 7183, loss 0.49747.
Train: 2018-08-04T23:50:01.728913: step 7184, loss 0.635788.
Train: 2018-08-04T23:50:01.978885: step 7185, loss 0.546519.
Train: 2018-08-04T23:50:02.228824: step 7186, loss 0.562746.
Train: 2018-08-04T23:50:02.478737: step 7187, loss 0.603013.
Train: 2018-08-04T23:50:02.728712: step 7188, loss 0.522736.
Train: 2018-08-04T23:50:02.978651: step 7189, loss 0.498836.
Train: 2018-08-04T23:50:03.228562: step 7190, loss 0.530853.
Test: 2018-08-04T23:50:04.478269: step 7190, loss 0.549291.
Train: 2018-08-04T23:50:04.728243: step 7191, loss 0.578865.
Train: 2018-08-04T23:50:04.993808: step 7192, loss 0.538811.
Train: 2018-08-04T23:50:05.243750: step 7193, loss 0.562829.
Train: 2018-08-04T23:50:05.503491: step 7194, loss 0.506623.
Train: 2018-08-04T23:50:05.753401: step 7195, loss 0.61106.
Train: 2018-08-04T23:50:06.018965: step 7196, loss 0.578878.
Train: 2018-08-04T23:50:06.253286: step 7197, loss 0.482167.
Train: 2018-08-04T23:50:06.518880: step 7198, loss 0.643526.
Train: 2018-08-04T23:50:06.831308: step 7199, loss 0.562723.
Train: 2018-08-04T23:50:07.081246: step 7200, loss 0.57889.
Test: 2018-08-04T23:50:08.346544: step 7200, loss 0.549586.
Train: 2018-08-04T23:50:09.221370: step 7201, loss 0.514201.
Train: 2018-08-04T23:50:09.471306: step 7202, loss 0.546511.
Train: 2018-08-04T23:50:09.721253: step 7203, loss 0.570793.
Train: 2018-08-04T23:50:09.971165: step 7204, loss 0.52208.
Train: 2018-08-04T23:50:10.221136: step 7205, loss 0.570781.
Train: 2018-08-04T23:50:10.471072: step 7206, loss 0.554479.
Train: 2018-08-04T23:50:10.721019: step 7207, loss 0.546284.
Train: 2018-08-04T23:50:10.970960: step 7208, loss 0.554411.
Train: 2018-08-04T23:50:11.220871: step 7209, loss 0.652698.
Train: 2018-08-04T23:50:11.470846: step 7210, loss 0.611714.
Test: 2018-08-04T23:50:12.736142: step 7210, loss 0.549498.
Train: 2018-08-04T23:50:12.962330: step 7211, loss 0.472641.
Train: 2018-08-04T23:50:13.208231: step 7212, loss 0.570766.
Train: 2018-08-04T23:50:13.458202: step 7213, loss 0.554393.
Train: 2018-08-04T23:50:13.723735: step 7214, loss 0.554383.
Train: 2018-08-04T23:50:13.958086: step 7215, loss 0.611742.
Train: 2018-08-04T23:50:14.252468: step 7216, loss 0.587146.
Train: 2018-08-04T23:50:14.510607: step 7217, loss 0.578948.
Train: 2018-08-04T23:50:14.744934: step 7218, loss 0.578939.
Train: 2018-08-04T23:50:15.002105: step 7219, loss 0.546303.
Train: 2018-08-04T23:50:15.243944: step 7220, loss 0.595219.
Test: 2018-08-04T23:50:16.493620: step 7220, loss 0.548879.
Train: 2018-08-04T23:50:16.727971: step 7221, loss 0.546383.
Train: 2018-08-04T23:50:16.977882: step 7222, loss 0.619525.
Train: 2018-08-04T23:50:17.227823: step 7223, loss 0.554588.
Train: 2018-08-04T23:50:17.493416: step 7224, loss 0.530365.
Train: 2018-08-04T23:50:17.743327: step 7225, loss 0.603128.
Train: 2018-08-04T23:50:17.998871: step 7226, loss 0.578882.
Train: 2018-08-04T23:50:18.237440: step 7227, loss 0.570826.
Train: 2018-08-04T23:50:18.479771: step 7228, loss 0.530661.
Train: 2018-08-04T23:50:18.729713: step 7229, loss 0.522671.
Train: 2018-08-04T23:50:18.979653: step 7230, loss 0.57887.
Test: 2018-08-04T23:50:20.216995: step 7230, loss 0.548658.
Train: 2018-08-04T23:50:20.482633: step 7231, loss 0.57084.
Train: 2018-08-04T23:50:20.734659: step 7232, loss 0.554781.
Train: 2018-08-04T23:50:20.984567: step 7233, loss 0.602962.
Train: 2018-08-04T23:50:21.234542: step 7234, loss 0.554795.
Train: 2018-08-04T23:50:21.484484: step 7235, loss 0.562825.
Train: 2018-08-04T23:50:21.734391: step 7236, loss 0.626982.
Train: 2018-08-04T23:50:21.984367: step 7237, loss 0.562854.
Train: 2018-08-04T23:50:22.234308: step 7238, loss 0.658797.
Train: 2018-08-04T23:50:22.484250: step 7239, loss 0.578859.
Train: 2018-08-04T23:50:22.749810: step 7240, loss 0.570922.
Test: 2018-08-04T23:50:24.011503: step 7240, loss 0.550007.
Train: 2018-08-04T23:50:24.245854: step 7241, loss 0.642152.
Train: 2018-08-04T23:50:24.495765: step 7242, loss 0.578866.
Train: 2018-08-04T23:50:24.745706: step 7243, loss 0.484811.
Train: 2018-08-04T23:50:24.995677: step 7244, loss 0.438042.
Train: 2018-08-04T23:50:25.245588: step 7245, loss 0.516171.
Train: 2018-08-04T23:50:25.502395: step 7246, loss 0.508115.
Train: 2018-08-04T23:50:25.752309: step 7247, loss 0.507806.
Train: 2018-08-04T23:50:25.955411: step 7248, loss 0.562984.
Train: 2018-08-04T23:50:26.205328: step 7249, loss 0.610767.
Train: 2018-08-04T23:50:26.455299: step 7250, loss 0.626886.
Test: 2018-08-04T23:50:27.704976: step 7250, loss 0.549262.
Train: 2018-08-04T23:50:27.954942: step 7251, loss 0.538786.
Train: 2018-08-04T23:50:28.204890: step 7252, loss 0.546739.
Train: 2018-08-04T23:50:28.470453: step 7253, loss 0.562773.
Train: 2018-08-04T23:50:28.720394: step 7254, loss 0.538538.
Train: 2018-08-04T23:50:28.970305: step 7255, loss 0.562712.
Train: 2018-08-04T23:50:29.235875: step 7256, loss 0.522139.
Train: 2018-08-04T23:50:29.485839: step 7257, loss 0.603318.
Train: 2018-08-04T23:50:29.735781: step 7258, loss 0.562627.
Train: 2018-08-04T23:50:29.985724: step 7259, loss 0.587096.
Train: 2018-08-04T23:50:30.235664: step 7260, loss 0.611619.
Test: 2018-08-04T23:50:31.500963: step 7260, loss 0.548023.
Train: 2018-08-04T23:50:31.735314: step 7261, loss 0.529934.
Train: 2018-08-04T23:50:32.004379: step 7262, loss 0.619794.
Train: 2018-08-04T23:50:32.282149: step 7263, loss 0.578934.
Train: 2018-08-04T23:50:32.531133: step 7264, loss 0.554468.
Train: 2018-08-04T23:50:32.781074: step 7265, loss 0.538195.
Train: 2018-08-04T23:50:33.038714: step 7266, loss 0.505629.
Train: 2018-08-04T23:50:33.312047: step 7267, loss 0.603385.
Train: 2018-08-04T23:50:33.572395: step 7268, loss 0.61154.
Train: 2018-08-04T23:50:33.847875: step 7269, loss 0.554489.
Train: 2018-08-04T23:50:34.114265: step 7270, loss 0.562642.
Test: 2018-08-04T23:50:35.404596: step 7270, loss 0.549069.
Train: 2018-08-04T23:50:35.645506: step 7271, loss 0.570782.
Train: 2018-08-04T23:50:35.914184: step 7272, loss 0.595161.
Train: 2018-08-04T23:50:36.160374: step 7273, loss 0.546451.
Train: 2018-08-04T23:50:36.401575: step 7274, loss 0.546477.
Train: 2018-08-04T23:50:36.658046: step 7275, loss 0.578898.
Train: 2018-08-04T23:50:36.929174: step 7276, loss 0.546509.
Train: 2018-08-04T23:50:37.171569: step 7277, loss 0.611272.
Train: 2018-08-04T23:50:37.405889: step 7278, loss 0.635475.
Train: 2018-08-04T23:50:37.683909: step 7279, loss 0.562759.
Train: 2018-08-04T23:50:37.934089: step 7280, loss 0.570833.
Test: 2018-08-04T23:50:39.226090: step 7280, loss 0.549248.
Train: 2018-08-04T23:50:39.471371: step 7281, loss 0.522726.
Train: 2018-08-04T23:50:39.730818: step 7282, loss 0.578865.
Train: 2018-08-04T23:50:39.984831: step 7283, loss 0.554861.
Train: 2018-08-04T23:50:40.250006: step 7284, loss 0.498927.
Train: 2018-08-04T23:50:40.497561: step 7285, loss 0.570863.
Train: 2018-08-04T23:50:40.745483: step 7286, loss 0.578864.
Train: 2018-08-04T23:50:41.003791: step 7287, loss 0.586874.
Train: 2018-08-04T23:50:41.253765: step 7288, loss 0.506801.
Train: 2018-08-04T23:50:41.503699: step 7289, loss 0.514728.
Train: 2018-08-04T23:50:41.753641: step 7290, loss 0.570835.
Test: 2018-08-04T23:50:43.002272: step 7290, loss 0.550063.
Train: 2018-08-04T23:50:43.290887: step 7291, loss 0.570824.
Train: 2018-08-04T23:50:43.544467: step 7292, loss 0.522413.
Train: 2018-08-04T23:50:43.794378: step 7293, loss 0.481839.
Train: 2018-08-04T23:50:44.044350: step 7294, loss 0.603275.
Train: 2018-08-04T23:50:44.283850: step 7295, loss 0.611506.
Train: 2018-08-04T23:50:44.538017: step 7296, loss 0.51367.
Train: 2018-08-04T23:50:44.787933: step 7297, loss 0.554412.
Train: 2018-08-04T23:50:45.037904: step 7298, loss 0.529778.
Train: 2018-08-04T23:50:45.287847: step 7299, loss 0.521436.
Train: 2018-08-04T23:50:45.537789: step 7300, loss 0.513013.
Test: 2018-08-04T23:50:46.785645: step 7300, loss 0.549401.
Train: 2018-08-04T23:50:47.613607: step 7301, loss 0.554189.
Train: 2018-08-04T23:50:47.863544: step 7302, loss 0.529181.
Train: 2018-08-04T23:50:48.113459: step 7303, loss 0.595819.
Train: 2018-08-04T23:50:48.363432: step 7304, loss 0.562402.
Train: 2018-08-04T23:50:48.613341: step 7305, loss 0.545602.
Train: 2018-08-04T23:50:48.863285: step 7306, loss 0.553965.
Train: 2018-08-04T23:50:49.113257: step 7307, loss 0.545505.
Train: 2018-08-04T23:50:49.363198: step 7308, loss 0.537011.
Train: 2018-08-04T23:50:49.623644: step 7309, loss 0.587764.
Train: 2018-08-04T23:50:49.860042: step 7310, loss 0.562351.
Test: 2018-08-04T23:50:51.125343: step 7310, loss 0.547584.
Train: 2018-08-04T23:50:51.359664: step 7311, loss 0.655734.
Train: 2018-08-04T23:50:51.625257: step 7312, loss 0.562352.
Train: 2018-08-04T23:50:51.859578: step 7313, loss 0.579288.
Train: 2018-08-04T23:50:52.109519: step 7314, loss 0.604615.
Train: 2018-08-04T23:50:52.359428: step 7315, loss 0.621364.
Train: 2018-08-04T23:50:52.609399: step 7316, loss 0.52042.
Train: 2018-08-04T23:50:52.859335: step 7317, loss 0.620999.
Train: 2018-08-04T23:50:53.124901: step 7318, loss 0.579103.
Train: 2018-08-04T23:50:53.359195: step 7319, loss 0.496027.
Train: 2018-08-04T23:50:53.609135: step 7320, loss 0.570757.
Test: 2018-08-04T23:50:54.858843: step 7320, loss 0.547164.
Train: 2018-08-04T23:50:55.108784: step 7321, loss 0.636881.
Train: 2018-08-04T23:50:55.358725: step 7322, loss 0.504887.
Train: 2018-08-04T23:50:55.608697: step 7323, loss 0.570759.
Train: 2018-08-04T23:50:55.858639: step 7324, loss 0.54617.
Train: 2018-08-04T23:50:56.108580: step 7325, loss 0.603501.
Train: 2018-08-04T23:50:56.374146: step 7326, loss 0.513624.
Train: 2018-08-04T23:50:56.624087: step 7327, loss 0.521844.
Train: 2018-08-04T23:50:56.874029: step 7328, loss 0.570774.
Train: 2018-08-04T23:50:57.123936: step 7329, loss 0.636001.
Train: 2018-08-04T23:50:57.373911: step 7330, loss 0.684708.
Test: 2018-08-04T23:50:58.623586: step 7330, loss 0.548422.
Train: 2018-08-04T23:50:58.857906: step 7331, loss 0.570796.
Train: 2018-08-04T23:50:59.123502: step 7332, loss 0.482093.
Train: 2018-08-04T23:50:59.357790: step 7333, loss 0.619129.
Train: 2018-08-04T23:50:59.607760: step 7334, loss 0.586896.
Train: 2018-08-04T23:50:59.857696: step 7335, loss 0.578863.
Train: 2018-08-04T23:51:00.107641: step 7336, loss 0.515057.
Train: 2018-08-04T23:51:00.357585: step 7337, loss 0.515157.
Train: 2018-08-04T23:51:00.607527: step 7338, loss 0.515166.
Train: 2018-08-04T23:51:00.873091: step 7339, loss 0.554946.
Train: 2018-08-04T23:51:01.123001: step 7340, loss 0.522987.
Test: 2018-08-04T23:51:02.388329: step 7340, loss 0.547843.
Train: 2018-08-04T23:51:02.622651: step 7341, loss 0.506858.
Train: 2018-08-04T23:51:02.872624: step 7342, loss 0.586899.
Train: 2018-08-04T23:51:03.122565: step 7343, loss 0.627179.
Train: 2018-08-04T23:51:03.372473: step 7344, loss 0.578879.
Train: 2018-08-04T23:51:03.622440: step 7345, loss 0.57888.
Train: 2018-08-04T23:51:03.872391: step 7346, loss 0.562755.
Train: 2018-08-04T23:51:04.122299: step 7347, loss 0.514366.
Train: 2018-08-04T23:51:04.372273: step 7348, loss 0.538508.
Train: 2018-08-04T23:51:04.622181: step 7349, loss 0.497987.
Train: 2018-08-04T23:51:04.872122: step 7350, loss 0.627607.
Test: 2018-08-04T23:51:06.106209: step 7350, loss 0.549455.
Train: 2018-08-04T23:51:06.356150: step 7351, loss 0.522014.
Train: 2018-08-04T23:51:06.598956: step 7352, loss 0.521898.
Train: 2018-08-04T23:51:06.843688: step 7353, loss 0.562598.
Train: 2018-08-04T23:51:07.110570: step 7354, loss 0.603532.
Train: 2018-08-04T23:51:07.369813: step 7355, loss 0.529743.
Train: 2018-08-04T23:51:07.613415: step 7356, loss 0.513218.
Train: 2018-08-04T23:51:07.863357: step 7357, loss 0.529538.
Train: 2018-08-04T23:51:08.113301: step 7358, loss 0.579027.
Train: 2018-08-04T23:51:08.363240: step 7359, loss 0.587341.
Train: 2018-08-04T23:51:08.611105: step 7360, loss 0.562454.
Test: 2018-08-04T23:51:09.873509: step 7360, loss 0.548557.
Train: 2018-08-04T23:51:10.107831: step 7361, loss 0.545813.
Train: 2018-08-04T23:51:10.357802: step 7362, loss 0.554107.
Train: 2018-08-04T23:51:10.607712: step 7363, loss 0.579106.
Train: 2018-08-04T23:51:10.857684: step 7364, loss 0.520689.
Train: 2018-08-04T23:51:11.107626: step 7365, loss 0.612567.
Train: 2018-08-04T23:51:11.357567: step 7366, loss 0.562412.
Train: 2018-08-04T23:51:11.607509: step 7367, loss 0.587491.
Train: 2018-08-04T23:51:11.857450: step 7368, loss 0.545708.
Train: 2018-08-04T23:51:12.107361: step 7369, loss 0.529015.
Train: 2018-08-04T23:51:12.365289: step 7370, loss 0.562416.
Test: 2018-08-04T23:51:13.622119: step 7370, loss 0.548848.
Train: 2018-08-04T23:51:13.856468: step 7371, loss 0.579124.
Train: 2018-08-04T23:51:14.110060: step 7372, loss 0.554067.
Train: 2018-08-04T23:51:14.375623: step 7373, loss 0.645906.
Train: 2018-08-04T23:51:14.609975: step 7374, loss 0.620744.
Train: 2018-08-04T23:51:14.859916: step 7375, loss 0.462841.
Train: 2018-08-04T23:51:15.109857: step 7376, loss 0.52929.
Train: 2018-08-04T23:51:15.359799: step 7377, loss 0.57905.
Train: 2018-08-04T23:51:15.609709: step 7378, loss 0.529323.
Train: 2018-08-04T23:51:15.875296: step 7379, loss 0.603908.
Train: 2018-08-04T23:51:16.109617: step 7380, loss 0.537637.
Test: 2018-08-04T23:51:17.364055: step 7380, loss 0.548656.
Train: 2018-08-04T23:51:17.598369: step 7381, loss 0.529371.
Train: 2018-08-04T23:51:17.863917: step 7382, loss 0.554197.
Train: 2018-08-04T23:51:18.098252: step 7383, loss 0.587324.
Train: 2018-08-04T23:51:18.348193: step 7384, loss 0.521067.
Train: 2018-08-04T23:51:18.598104: step 7385, loss 0.579044.
Train: 2018-08-04T23:51:18.848046: step 7386, loss 0.595621.
Train: 2018-08-04T23:51:19.098018: step 7387, loss 0.545912.
Train: 2018-08-04T23:51:19.347929: step 7388, loss 0.570757.
Train: 2018-08-04T23:51:19.597870: step 7389, loss 0.562483.
Train: 2018-08-04T23:51:19.847842: step 7390, loss 0.595562.
Test: 2018-08-04T23:51:21.097207: step 7390, loss 0.548329.
Train: 2018-08-04T23:51:21.347173: step 7391, loss 0.504699.
Train: 2018-08-04T23:51:21.581501: step 7392, loss 0.554242.
Train: 2018-08-04T23:51:21.841609: step 7393, loss 0.579015.
Train: 2018-08-04T23:51:22.094891: step 7394, loss 0.5625.
Train: 2018-08-04T23:51:22.332033: step 7395, loss 0.63678.
Train: 2018-08-04T23:51:22.581951: step 7396, loss 0.554286.
Train: 2018-08-04T23:51:22.831920: step 7397, loss 0.513209.
Train: 2018-08-04T23:51:23.087150: step 7398, loss 0.521454.
Train: 2018-08-04T23:51:23.283561: step 7399, loss 0.49238.
Train: 2018-08-04T23:51:23.562976: step 7400, loss 0.636662.
Test: 2018-08-04T23:51:24.842585: step 7400, loss 0.548949.
Train: 2018-08-04T23:51:25.671792: step 7401, loss 0.653143.
Train: 2018-08-04T23:51:25.921767: step 7402, loss 0.52143.
Train: 2018-08-04T23:51:26.171676: step 7403, loss 0.611827.
Train: 2018-08-04T23:51:26.421618: step 7404, loss 0.578959.
Train: 2018-08-04T23:51:26.671591: step 7405, loss 0.497165.
Train: 2018-08-04T23:51:26.921533: step 7406, loss 0.603463.
Train: 2018-08-04T23:51:27.187095: step 7407, loss 0.456504.
Train: 2018-08-04T23:51:27.421409: step 7408, loss 0.603455.
Train: 2018-08-04T23:51:27.671359: step 7409, loss 0.587113.
Train: 2018-08-04T23:51:27.921292: step 7410, loss 0.619779.
Test: 2018-08-04T23:51:29.200168: step 7410, loss 0.549379.
Train: 2018-08-04T23:51:29.468304: step 7411, loss 0.570774.
Train: 2018-08-04T23:51:29.716982: step 7412, loss 0.562641.
Train: 2018-08-04T23:51:29.992073: step 7413, loss 0.513905.
Train: 2018-08-04T23:51:30.263776: step 7414, loss 0.489552.
Train: 2018-08-04T23:51:30.538113: step 7415, loss 0.587052.
Train: 2018-08-04T23:51:30.791953: step 7416, loss 0.554495.
Train: 2018-08-04T23:51:31.060308: step 7417, loss 0.546329.
Train: 2018-08-04T23:51:31.313738: step 7418, loss 0.529981.
Train: 2018-08-04T23:51:31.570202: step 7419, loss 0.652501.
Train: 2018-08-04T23:51:31.811780: step 7420, loss 0.5626.
Test: 2018-08-04T23:51:33.100713: step 7420, loss 0.548215.
Train: 2018-08-04T23:51:33.368949: step 7421, loss 0.57077.
Train: 2018-08-04T23:51:33.637589: step 7422, loss 0.538127.
Train: 2018-08-04T23:51:33.909843: step 7423, loss 0.529964.
Train: 2018-08-04T23:51:34.170530: step 7424, loss 0.480922.
Train: 2018-08-04T23:51:34.442156: step 7425, loss 0.587143.
Train: 2018-08-04T23:51:34.700118: step 7426, loss 0.570761.
Train: 2018-08-04T23:51:34.978700: step 7427, loss 0.611832.
Train: 2018-08-04T23:51:35.214935: step 7428, loss 0.587189.
Train: 2018-08-04T23:51:35.489027: step 7429, loss 0.57076.
Train: 2018-08-04T23:51:35.738790: step 7430, loss 0.595374.
Test: 2018-08-04T23:51:36.988498: step 7430, loss 0.548701.
Train: 2018-08-04T23:51:37.230052: step 7431, loss 0.529802.
Train: 2018-08-04T23:51:37.479995: step 7432, loss 0.538014.
Train: 2018-08-04T23:51:37.729935: step 7433, loss 0.578952.
Train: 2018-08-04T23:51:37.979846: step 7434, loss 0.587135.
Train: 2018-08-04T23:51:38.229818: step 7435, loss 0.472641.
Train: 2018-08-04T23:51:38.486695: step 7436, loss 0.497079.
Train: 2018-08-04T23:51:38.736670: step 7437, loss 0.63643.
Train: 2018-08-04T23:51:38.986579: step 7438, loss 0.562546.
Train: 2018-08-04T23:51:39.252167: step 7439, loss 0.578977.
Train: 2018-08-04T23:51:39.502114: step 7440, loss 0.636504.
Test: 2018-08-04T23:51:40.751790: step 7440, loss 0.549233.
Train: 2018-08-04T23:51:40.986135: step 7441, loss 0.52974.
Train: 2018-08-04T23:51:41.236076: step 7442, loss 0.54617.
Train: 2018-08-04T23:51:41.485994: step 7443, loss 0.595346.
Train: 2018-08-04T23:51:41.735965: step 7444, loss 0.521654.
Train: 2018-08-04T23:51:41.985876: step 7445, loss 0.529843.
Train: 2018-08-04T23:51:42.235817: step 7446, loss 0.554384.
Train: 2018-08-04T23:51:42.505058: step 7447, loss 0.61994.
Train: 2018-08-04T23:51:42.745296: step 7448, loss 0.619908.
Train: 2018-08-04T23:51:42.995207: step 7449, loss 0.546243.
Train: 2018-08-04T23:51:43.245178: step 7450, loss 0.570771.
Test: 2018-08-04T23:51:44.506941: step 7450, loss 0.548634.
Train: 2018-08-04T23:51:44.736640: step 7451, loss 0.59523.
Train: 2018-08-04T23:51:44.986610: step 7452, loss 0.538242.
Train: 2018-08-04T23:51:45.236551: step 7453, loss 0.595159.
Train: 2018-08-04T23:51:45.486489: step 7454, loss 0.514029.
Train: 2018-08-04T23:51:45.736405: step 7455, loss 0.522166.
Train: 2018-08-04T23:51:45.986376: step 7456, loss 0.562684.
Train: 2018-08-04T23:51:46.236318: step 7457, loss 0.530227.
Train: 2018-08-04T23:51:46.501880: step 7458, loss 0.6114.
Train: 2018-08-04T23:51:46.751792: step 7459, loss 0.652011.
Train: 2018-08-04T23:51:46.997654: step 7460, loss 0.522161.
Test: 2018-08-04T23:51:48.248412: step 7460, loss 0.54843.
Train: 2018-08-04T23:51:48.482757: step 7461, loss 0.60319.
Train: 2018-08-04T23:51:48.732674: step 7462, loss 0.570805.
Train: 2018-08-04T23:51:48.982616: step 7463, loss 0.619235.
Train: 2018-08-04T23:51:49.232557: step 7464, loss 0.59497.
Train: 2018-08-04T23:51:49.482529: step 7465, loss 0.602931.
Train: 2018-08-04T23:51:49.732471: step 7466, loss 0.578862.
Train: 2018-08-04T23:51:49.982412: step 7467, loss 0.610701.
Train: 2018-08-04T23:51:50.232353: step 7468, loss 0.531307.
Train: 2018-08-04T23:51:50.497911: step 7469, loss 0.523554.
Train: 2018-08-04T23:51:50.751373: step 7470, loss 0.563087.
Test: 2018-08-04T23:51:52.003307: step 7470, loss 0.548683.
Train: 2018-08-04T23:51:52.237653: step 7471, loss 0.673404.
Train: 2018-08-04T23:51:52.487600: step 7472, loss 0.555326.
Train: 2018-08-04T23:51:52.737510: step 7473, loss 0.531927.
Train: 2018-08-04T23:51:53.003075: step 7474, loss 0.594512.
Train: 2018-08-04T23:51:53.237425: step 7475, loss 0.563301.
Train: 2018-08-04T23:51:53.487365: step 7476, loss 0.532198.
Train: 2018-08-04T23:51:53.737307: step 7477, loss 0.571121.
Train: 2018-08-04T23:51:53.987249: step 7478, loss 0.610014.
Train: 2018-08-04T23:51:54.237158: step 7479, loss 0.540068.
Train: 2018-08-04T23:51:54.487101: step 7480, loss 0.555611.
Test: 2018-08-04T23:51:55.736808: step 7480, loss 0.550758.
Train: 2018-08-04T23:51:55.992569: step 7481, loss 0.586676.
Train: 2018-08-04T23:51:56.248455: step 7482, loss 0.509009.
Train: 2018-08-04T23:51:56.495037: step 7483, loss 0.633353.
Train: 2018-08-04T23:51:56.741146: step 7484, loss 0.547795.
Train: 2018-08-04T23:51:56.986624: step 7485, loss 0.594464.
Train: 2018-08-04T23:51:57.243133: step 7486, loss 0.493318.
Train: 2018-08-04T23:51:57.477454: step 7487, loss 0.563303.
Train: 2018-08-04T23:51:57.727395: step 7488, loss 0.563265.
Train: 2018-08-04T23:51:57.990098: step 7489, loss 0.524094.
Train: 2018-08-04T23:51:58.224444: step 7490, loss 0.547471.
Test: 2018-08-04T23:51:59.496179: step 7490, loss 0.549416.
Train: 2018-08-04T23:51:59.730532: step 7491, loss 0.594618.
Train: 2018-08-04T23:51:59.980442: step 7492, loss 0.507809.
Train: 2018-08-04T23:52:00.246035: step 7493, loss 0.586783.
Train: 2018-08-04T23:52:00.480351: step 7494, loss 0.515289.
Train: 2018-08-04T23:52:00.730297: step 7495, loss 0.530999.
Train: 2018-08-04T23:52:00.980238: step 7496, loss 0.610912.
Train: 2018-08-04T23:52:01.230151: step 7497, loss 0.562803.
Train: 2018-08-04T23:52:01.500041: step 7498, loss 0.506381.
Train: 2018-08-04T23:52:01.748879: step 7499, loss 0.530377.
Train: 2018-08-04T23:52:01.988756: step 7500, loss 0.54643.
Test: 2018-08-04T23:52:03.287776: step 7500, loss 0.548818.
Train: 2018-08-04T23:52:04.186245: step 7501, loss 0.546316.
Train: 2018-08-04T23:52:04.467408: step 7502, loss 0.505274.
Train: 2018-08-04T23:52:04.779834: step 7503, loss 0.570758.
Train: 2018-08-04T23:52:05.139125: step 7504, loss 0.595542.
Train: 2018-08-04T23:52:05.373480: step 7505, loss 0.570757.
Train: 2018-08-04T23:52:05.623416: step 7506, loss 0.537535.
Train: 2018-08-04T23:52:05.873326: step 7507, loss 0.537452.
Train: 2018-08-04T23:52:06.123303: step 7508, loss 0.520661.
Train: 2018-08-04T23:52:06.373240: step 7509, loss 0.503746.
Train: 2018-08-04T23:52:06.623185: step 7510, loss 0.520311.
Test: 2018-08-04T23:52:07.955590: step 7510, loss 0.548053.
Train: 2018-08-04T23:52:08.205562: step 7511, loss 0.570813.
Train: 2018-08-04T23:52:08.455506: step 7512, loss 0.579315.
Train: 2018-08-04T23:52:08.736657: step 7513, loss 0.613375.
Train: 2018-08-04T23:52:09.017844: step 7514, loss 0.596401.
Train: 2018-08-04T23:52:09.283434: step 7515, loss 0.579372.
Train: 2018-08-04T23:52:09.548997: step 7516, loss 0.570854.
Train: 2018-08-04T23:52:09.814555: step 7517, loss 0.613365.
Train: 2018-08-04T23:52:10.064504: step 7518, loss 0.570835.
Train: 2018-08-04T23:52:10.314446: step 7519, loss 0.562356.
Train: 2018-08-04T23:52:10.580005: step 7520, loss 0.570811.
Test: 2018-08-04T23:52:11.845304: step 7520, loss 0.549267.
Train: 2018-08-04T23:52:12.079655: step 7521, loss 0.528661.
Train: 2018-08-04T23:52:12.345187: step 7522, loss 0.604451.
Train: 2018-08-04T23:52:12.579538: step 7523, loss 0.56239.
Train: 2018-08-04T23:52:12.845069: step 7524, loss 0.495417.
Train: 2018-08-04T23:52:13.110657: step 7525, loss 0.595873.
Train: 2018-08-04T23:52:13.355533: step 7526, loss 0.579123.
Train: 2018-08-04T23:52:13.605478: step 7527, loss 0.579102.
Train: 2018-08-04T23:52:13.871042: step 7528, loss 0.579079.
Train: 2018-08-04T23:52:14.120977: step 7529, loss 0.562461.
Train: 2018-08-04T23:52:14.370919: step 7530, loss 0.545923.
Test: 2018-08-04T23:52:15.627242: step 7530, loss 0.550405.
Train: 2018-08-04T23:52:15.877214: step 7531, loss 0.55423.
Train: 2018-08-04T23:52:16.142747: step 7532, loss 0.546005.
Train: 2018-08-04T23:52:16.392688: step 7533, loss 0.554273.
Train: 2018-08-04T23:52:16.642660: step 7534, loss 0.537817.
Train: 2018-08-04T23:52:16.892601: step 7535, loss 0.546058.
Train: 2018-08-04T23:52:17.142543: step 7536, loss 0.496648.
Train: 2018-08-04T23:52:17.392485: step 7537, loss 0.513024.
Train: 2018-08-04T23:52:17.646290: step 7538, loss 0.51288.
Train: 2018-08-04T23:52:17.896260: step 7539, loss 0.579054.
Train: 2018-08-04T23:52:18.146203: step 7540, loss 0.562445.
Test: 2018-08-04T23:52:19.411500: step 7540, loss 0.54832.
Train: 2018-08-04T23:52:19.661442: step 7541, loss 0.495759.
Train: 2018-08-04T23:52:19.911417: step 7542, loss 0.478795.
Train: 2018-08-04T23:52:20.161324: step 7543, loss 0.511981.
Train: 2018-08-04T23:52:20.411300: step 7544, loss 0.570809.
Train: 2018-08-04T23:52:20.661241: step 7545, loss 0.553871.
Train: 2018-08-04T23:52:20.911183: step 7546, loss 0.519778.
Train: 2018-08-04T23:52:21.176712: step 7547, loss 0.664911.
Train: 2018-08-04T23:52:21.438337: step 7548, loss 0.545218.
Train: 2018-08-04T23:52:21.674051: step 7549, loss 0.613752.
Train: 2018-08-04T23:52:21.877128: step 7550, loss 0.489223.
Test: 2018-08-04T23:52:23.126836: step 7550, loss 0.547023.
Train: 2018-08-04T23:52:23.376805: step 7551, loss 0.656686.
Train: 2018-08-04T23:52:23.626717: step 7552, loss 0.570902.
Train: 2018-08-04T23:52:23.876660: step 7553, loss 0.553783.
Train: 2018-08-04T23:52:24.126600: step 7554, loss 0.545255.
Train: 2018-08-04T23:52:24.376575: step 7555, loss 0.562339.
Train: 2018-08-04T23:52:24.626483: step 7556, loss 0.502698.
Train: 2018-08-04T23:52:24.892072: step 7557, loss 0.57938.
Train: 2018-08-04T23:52:25.126399: step 7558, loss 0.621939.
Train: 2018-08-04T23:52:25.376307: step 7559, loss 0.485882.
Train: 2018-08-04T23:52:25.641870: step 7560, loss 0.519887.
Test: 2018-08-04T23:52:26.891578: step 7560, loss 0.546987.
Train: 2018-08-04T23:52:27.125929: step 7561, loss 0.485894.
Train: 2018-08-04T23:52:27.375841: step 7562, loss 0.494269.
Train: 2018-08-04T23:52:27.625781: step 7563, loss 0.545273.
Train: 2018-08-04T23:52:27.875748: step 7564, loss 0.588002.
Train: 2018-08-04T23:52:28.141285: step 7565, loss 0.596611.
Train: 2018-08-04T23:52:28.391227: step 7566, loss 0.579481.
Train: 2018-08-04T23:52:28.641199: step 7567, loss 0.579477.
Train: 2018-08-04T23:52:28.891136: step 7568, loss 0.596593.
Train: 2018-08-04T23:52:29.141090: step 7569, loss 0.519587.
Train: 2018-08-04T23:52:29.391024: step 7570, loss 0.553795.
Test: 2018-08-04T23:52:30.640701: step 7570, loss 0.54769.
Train: 2018-08-04T23:52:30.906267: step 7571, loss 0.562338.
Train: 2018-08-04T23:52:31.140614: step 7572, loss 0.59645.
Train: 2018-08-04T23:52:31.390555: step 7573, loss 0.570855.
Train: 2018-08-04T23:52:31.640467: step 7574, loss 0.562347.
Train: 2018-08-04T23:52:31.890434: step 7575, loss 0.494532.
Train: 2018-08-04T23:52:32.156000: step 7576, loss 0.553881.
Train: 2018-08-04T23:52:32.405942: step 7577, loss 0.613164.
Train: 2018-08-04T23:52:32.655886: step 7578, loss 0.579267.
Train: 2018-08-04T23:52:32.921446: step 7579, loss 0.570804.
Train: 2018-08-04T23:52:33.171358: step 7580, loss 0.53713.
Test: 2018-08-04T23:52:34.421065: step 7580, loss 0.547773.
Train: 2018-08-04T23:52:34.655416: step 7581, loss 0.545581.
Train: 2018-08-04T23:52:34.905356: step 7582, loss 0.587565.
Train: 2018-08-04T23:52:35.155298: step 7583, loss 0.570777.
Train: 2018-08-04T23:52:35.405240: step 7584, loss 0.620925.
Train: 2018-08-04T23:52:35.655182: step 7585, loss 0.579096.
Train: 2018-08-04T23:52:35.905092: step 7586, loss 0.537544.
Train: 2018-08-04T23:52:36.155064: step 7587, loss 0.5045.
Train: 2018-08-04T23:52:36.405006: step 7588, loss 0.579029.
Train: 2018-08-04T23:52:36.654950: step 7589, loss 0.595539.
Train: 2018-08-04T23:52:36.920510: step 7590, loss 0.496567.
Test: 2018-08-04T23:52:38.154565: step 7590, loss 0.549326.
Train: 2018-08-04T23:52:38.388888: step 7591, loss 0.570757.
Train: 2018-08-04T23:52:38.655048: step 7592, loss 0.546055.
Train: 2018-08-04T23:52:38.920611: step 7593, loss 0.603685.
Train: 2018-08-04T23:52:39.154961: step 7594, loss 0.48032.
Train: 2018-08-04T23:52:39.404902: step 7595, loss 0.653035.
Train: 2018-08-04T23:52:39.654843: step 7596, loss 0.570759.
Train: 2018-08-04T23:52:39.904755: step 7597, loss 0.554355.
Train: 2018-08-04T23:52:40.154726: step 7598, loss 0.578956.
Train: 2018-08-04T23:52:40.404668: step 7599, loss 0.521683.
Train: 2018-08-04T23:52:40.666763: step 7600, loss 0.611652.
Test: 2018-08-04T23:52:41.906340: step 7600, loss 0.549723.
Train: 2018-08-04T23:52:42.781137: step 7601, loss 0.554442.
Train: 2018-08-04T23:52:43.062350: step 7602, loss 0.497384.
Train: 2018-08-04T23:52:43.312291: step 7603, loss 0.570773.
Train: 2018-08-04T23:52:43.562231: step 7604, loss 0.497336.
Train: 2018-08-04T23:52:43.827796: step 7605, loss 0.570768.
Train: 2018-08-04T23:52:44.108950: step 7606, loss 0.587134.
Train: 2018-08-04T23:52:44.358919: step 7607, loss 0.587143.
Train: 2018-08-04T23:52:44.640076: step 7608, loss 0.619897.
Train: 2018-08-04T23:52:44.890018: step 7609, loss 0.546235.
Train: 2018-08-04T23:52:45.155610: step 7610, loss 0.497239.
Test: 2018-08-04T23:52:46.485881: step 7610, loss 0.548749.
Train: 2018-08-04T23:52:46.860793: step 7611, loss 0.64435.
Train: 2018-08-04T23:52:47.204463: step 7612, loss 0.578936.
Train: 2018-08-04T23:52:47.470058: step 7613, loss 0.635997.
Train: 2018-08-04T23:52:47.719966: step 7614, loss 0.578912.
Train: 2018-08-04T23:52:47.969941: step 7615, loss 0.514082.
Train: 2018-08-04T23:52:48.238043: step 7616, loss 0.554626.
Train: 2018-08-04T23:52:48.487979: step 7617, loss 0.554653.
Train: 2018-08-04T23:52:48.752703: step 7618, loss 0.498176.
Train: 2018-08-04T23:52:49.002650: step 7619, loss 0.538501.
Train: 2018-08-04T23:52:49.252561: step 7620, loss 0.54654.
Test: 2018-08-04T23:52:50.533511: step 7620, loss 0.548981.
Train: 2018-08-04T23:52:50.767858: step 7621, loss 0.586998.
Train: 2018-08-04T23:52:51.017773: step 7622, loss 0.546469.
Train: 2018-08-04T23:52:51.298958: step 7623, loss 0.56267.
Train: 2018-08-04T23:52:51.548900: step 7624, loss 0.61142.
Train: 2018-08-04T23:52:51.814487: step 7625, loss 0.457017.
Train: 2018-08-04T23:52:52.086012: step 7626, loss 0.481175.
Train: 2018-08-04T23:52:52.327950: step 7627, loss 0.603481.
Train: 2018-08-04T23:52:52.577920: step 7628, loss 0.628165.
Train: 2018-08-04T23:52:52.827831: step 7629, loss 0.562553.
Train: 2018-08-04T23:52:53.093396: step 7630, loss 0.628254.
Test: 2018-08-04T23:52:54.343102: step 7630, loss 0.546961.
Train: 2018-08-04T23:52:54.593073: step 7631, loss 0.505111.
Train: 2018-08-04T23:52:54.843016: step 7632, loss 0.521498.
Train: 2018-08-04T23:52:55.108548: step 7633, loss 0.620087.
Train: 2018-08-04T23:52:55.382655: step 7634, loss 0.603639.
Train: 2018-08-04T23:52:55.646669: step 7635, loss 0.578971.
Train: 2018-08-04T23:52:55.907096: step 7636, loss 0.529765.
Train: 2018-08-04T23:52:56.173356: step 7637, loss 0.537983.
Train: 2018-08-04T23:52:56.434892: step 7638, loss 0.587154.
Train: 2018-08-04T23:52:56.694031: step 7639, loss 0.578955.
Train: 2018-08-04T23:52:56.949376: step 7640, loss 0.521665.
Test: 2018-08-04T23:52:58.244745: step 7640, loss 0.549101.
Train: 2018-08-04T23:52:58.510307: step 7641, loss 0.554396.
Train: 2018-08-04T23:52:58.760253: step 7642, loss 0.538018.
Train: 2018-08-04T23:52:59.010214: step 7643, loss 0.505217.
Train: 2018-08-04T23:52:59.260161: step 7644, loss 0.562551.
Train: 2018-08-04T23:52:59.510071: step 7645, loss 0.587207.
Train: 2018-08-04T23:52:59.760037: step 7646, loss 0.570758.
Train: 2018-08-04T23:53:00.009954: step 7647, loss 0.570757.
Train: 2018-08-04T23:53:00.259926: step 7648, loss 0.521314.
Train: 2018-08-04T23:53:00.509868: step 7649, loss 0.579007.
Train: 2018-08-04T23:53:00.759810: step 7650, loss 0.570756.
Test: 2018-08-04T23:53:02.290683: step 7650, loss 0.547946.
Train: 2018-08-04T23:53:02.556258: step 7651, loss 0.587273.
Train: 2018-08-04T23:53:02.821833: step 7652, loss 0.587267.
Train: 2018-08-04T23:53:03.071739: step 7653, loss 0.554261.
Train: 2018-08-04T23:53:03.431029: step 7654, loss 0.653177.
Train: 2018-08-04T23:53:03.756904: step 7655, loss 0.496785.
Train: 2018-08-04T23:53:04.006846: step 7656, loss 0.554338.
Train: 2018-08-04T23:53:04.256817: step 7657, loss 0.595375.
Train: 2018-08-04T23:53:04.506729: step 7658, loss 0.578956.
Train: 2018-08-04T23:53:04.756701: step 7659, loss 0.619836.
Train: 2018-08-04T23:53:05.006641: step 7660, loss 0.54631.
Test: 2018-08-04T23:53:06.256318: step 7660, loss 0.548682.
Train: 2018-08-04T23:53:06.490669: step 7661, loss 0.603325.
Train: 2018-08-04T23:53:06.740579: step 7662, loss 0.643799.
Train: 2018-08-04T23:53:06.990521: step 7663, loss 0.530435.
Train: 2018-08-04T23:53:07.240493: step 7664, loss 0.490342.
Train: 2018-08-04T23:53:07.490435: step 7665, loss 0.5226.
Train: 2018-08-04T23:53:07.740372: step 7666, loss 0.546718.
Train: 2018-08-04T23:53:07.990317: step 7667, loss 0.603.
Train: 2018-08-04T23:53:08.240262: step 7668, loss 0.578872.
Train: 2018-08-04T23:53:08.490200: step 7669, loss 0.522646.
Train: 2018-08-04T23:53:08.740113: step 7670, loss 0.530662.
Test: 2018-08-04T23:53:09.989819: step 7670, loss 0.549536.
Train: 2018-08-04T23:53:10.239791: step 7671, loss 0.603007.
Train: 2018-08-04T23:53:10.489729: step 7672, loss 0.530602.
Train: 2018-08-04T23:53:10.739669: step 7673, loss 0.594984.
Train: 2018-08-04T23:53:10.989584: step 7674, loss 0.530549.
Train: 2018-08-04T23:53:11.255178: step 7675, loss 0.63532.
Train: 2018-08-04T23:53:11.505089: step 7676, loss 0.578878.
Train: 2018-08-04T23:53:11.755064: step 7677, loss 0.522538.
Train: 2018-08-04T23:53:12.005002: step 7678, loss 0.562778.
Train: 2018-08-04T23:53:12.254913: step 7679, loss 0.554725.
Train: 2018-08-04T23:53:12.504886: step 7680, loss 0.490286.
Test: 2018-08-04T23:53:13.754562: step 7680, loss 0.547962.
Train: 2018-08-04T23:53:14.004506: step 7681, loss 0.611168.
Train: 2018-08-04T23:53:14.254476: step 7682, loss 0.546575.
Train: 2018-08-04T23:53:14.504387: step 7683, loss 0.586978.
Train: 2018-08-04T23:53:14.754328: step 7684, loss 0.595077.
Train: 2018-08-04T23:53:15.004271: step 7685, loss 0.546533.
Train: 2018-08-04T23:53:15.254212: step 7686, loss 0.627442.
Train: 2018-08-04T23:53:15.519798: step 7687, loss 0.530403.
Train: 2018-08-04T23:53:15.769746: step 7688, loss 0.611199.
Train: 2018-08-04T23:53:16.019690: step 7689, loss 0.514351.
Train: 2018-08-04T23:53:16.285250: step 7690, loss 0.595013.
Test: 2018-08-04T23:53:17.534927: step 7690, loss 0.549672.
Train: 2018-08-04T23:53:17.769278: step 7691, loss 0.514404.
Train: 2018-08-04T23:53:18.019213: step 7692, loss 0.506309.
Train: 2018-08-04T23:53:18.269131: step 7693, loss 0.595044.
Train: 2018-08-04T23:53:18.519102: step 7694, loss 0.53846.
Train: 2018-08-04T23:53:18.769043: step 7695, loss 0.5627.
Train: 2018-08-04T23:53:19.018985: step 7696, loss 0.570792.
Train: 2018-08-04T23:53:19.268927: step 7697, loss 0.595141.
Train: 2018-08-04T23:53:19.518868: step 7698, loss 0.465237.
Train: 2018-08-04T23:53:19.768809: step 7699, loss 0.587058.
Train: 2018-08-04T23:53:20.034342: step 7700, loss 0.521855.
Test: 2018-08-04T23:53:21.284049: step 7700, loss 0.547816.
Train: 2018-08-04T23:53:22.080738: step 7701, loss 0.545159.
Train: 2018-08-04T23:53:22.330704: step 7702, loss 0.578958.
Train: 2018-08-04T23:53:22.580651: step 7703, loss 0.587181.
Train: 2018-08-04T23:53:22.830587: step 7704, loss 0.562539.
Train: 2018-08-04T23:53:23.080528: step 7705, loss 0.554303.
Train: 2018-08-04T23:53:23.330445: step 7706, loss 0.537813.
Train: 2018-08-04T23:53:23.580417: step 7707, loss 0.562509.
Train: 2018-08-04T23:53:23.830358: step 7708, loss 0.612046.
Train: 2018-08-04T23:53:24.080295: step 7709, loss 0.521212.
Train: 2018-08-04T23:53:24.330245: step 7710, loss 0.545964.
Test: 2018-08-04T23:53:25.595539: step 7710, loss 0.549049.
Train: 2018-08-04T23:53:25.829891: step 7711, loss 0.537667.
Train: 2018-08-04T23:53:26.079801: step 7712, loss 0.562473.
Train: 2018-08-04T23:53:26.329773: step 7713, loss 0.520995.
Train: 2018-08-04T23:53:26.579714: step 7714, loss 0.595689.
Train: 2018-08-04T23:53:26.829655: step 7715, loss 0.595712.
Train: 2018-08-04T23:53:27.079591: step 7716, loss 0.620661.
Train: 2018-08-04T23:53:27.329508: step 7717, loss 0.496019.
Train: 2018-08-04T23:53:27.579450: step 7718, loss 0.545844.
Train: 2018-08-04T23:53:27.829421: step 7719, loss 0.529219.
Train: 2018-08-04T23:53:28.079364: step 7720, loss 0.612342.
Test: 2018-08-04T23:53:29.329041: step 7720, loss 0.548946.
Train: 2018-08-04T23:53:29.563392: step 7721, loss 0.554134.
Train: 2018-08-04T23:53:29.813333: step 7722, loss 0.52089.
Train: 2018-08-04T23:53:30.063275: step 7723, loss 0.587395.
Train: 2018-08-04T23:53:30.313216: step 7724, loss 0.545812.
Train: 2018-08-04T23:53:30.578778: step 7725, loss 0.562443.
Train: 2018-08-04T23:53:30.828688: step 7726, loss 0.545806.
Train: 2018-08-04T23:53:31.078631: step 7727, loss 0.570762.
Train: 2018-08-04T23:53:31.328573: step 7728, loss 0.579083.
Train: 2018-08-04T23:53:31.578544: step 7729, loss 0.562444.
Train: 2018-08-04T23:53:31.828455: step 7730, loss 0.579074.
Test: 2018-08-04T23:53:33.078162: step 7730, loss 0.548392.
Train: 2018-08-04T23:53:33.312482: step 7731, loss 0.628897.
Train: 2018-08-04T23:53:33.562456: step 7732, loss 0.612183.
Train: 2018-08-04T23:53:33.812365: step 7733, loss 0.612041.
Train: 2018-08-04T23:53:34.062338: step 7734, loss 0.496763.
Train: 2018-08-04T23:53:34.312280: step 7735, loss 0.587166.
Train: 2018-08-04T23:53:34.562222: step 7736, loss 0.562587.
Train: 2018-08-04T23:53:34.812163: step 7737, loss 0.611568.
Train: 2018-08-04T23:53:35.062104: step 7738, loss 0.538257.
Train: 2018-08-04T23:53:35.312014: step 7739, loss 0.554569.
Train: 2018-08-04T23:53:35.561987: step 7740, loss 0.538421.
Test: 2018-08-04T23:53:36.811663: step 7740, loss 0.549962.
Train: 2018-08-04T23:53:37.046008: step 7741, loss 0.586974.
Train: 2018-08-04T23:53:37.295950: step 7742, loss 0.570812.
Train: 2018-08-04T23:53:37.545894: step 7743, loss 0.546644.
Train: 2018-08-04T23:53:37.811430: step 7744, loss 0.594978.
Train: 2018-08-04T23:53:38.061402: step 7745, loss 0.498495.
Train: 2018-08-04T23:53:38.311312: step 7746, loss 0.570834.
Train: 2018-08-04T23:53:38.561285: step 7747, loss 0.635149.
Train: 2018-08-04T23:53:38.811196: step 7748, loss 0.442423.
Train: 2018-08-04T23:53:39.076759: step 7749, loss 0.602988.
Train: 2018-08-04T23:53:39.326729: step 7750, loss 0.594958.
Test: 2018-08-04T23:53:40.576406: step 7750, loss 0.54881.
Train: 2018-08-04T23:53:40.810759: step 7751, loss 0.611032.
Train: 2018-08-04T23:53:41.060669: step 7752, loss 0.57887.
Train: 2018-08-04T23:53:41.310634: step 7753, loss 0.546798.
Train: 2018-08-04T23:53:41.560578: step 7754, loss 0.546825.
Train: 2018-08-04T23:53:41.810524: step 7755, loss 0.586872.
Train: 2018-08-04T23:53:42.060433: step 7756, loss 0.562861.
Train: 2018-08-04T23:53:42.310407: step 7757, loss 0.578863.
Train: 2018-08-04T23:53:42.560349: step 7758, loss 0.546902.
Train: 2018-08-04T23:53:42.810290: step 7759, loss 0.522946.
Train: 2018-08-04T23:53:43.060231: step 7760, loss 0.554878.
Test: 2018-08-04T23:53:44.325528: step 7760, loss 0.548938.
Train: 2018-08-04T23:53:44.575501: step 7761, loss 0.538847.
Train: 2018-08-04T23:53:44.825441: step 7762, loss 0.506719.
Train: 2018-08-04T23:53:45.075379: step 7763, loss 0.586913.
Train: 2018-08-04T23:53:45.325294: step 7764, loss 0.570822.
Train: 2018-08-04T23:53:45.575272: step 7765, loss 0.562742.
Train: 2018-08-04T23:53:45.825208: step 7766, loss 0.562721.
Train: 2018-08-04T23:53:46.075120: step 7767, loss 0.586992.
Train: 2018-08-04T23:53:46.325091: step 7768, loss 0.578899.
Train: 2018-08-04T23:53:46.575032: step 7769, loss 0.505941.
Train: 2018-08-04T23:53:46.840595: step 7770, loss 0.513943.
Test: 2018-08-04T23:53:48.090272: step 7770, loss 0.548474.
Train: 2018-08-04T23:53:48.324625: step 7771, loss 0.521922.
Train: 2018-08-04T23:53:48.574534: step 7772, loss 0.58711.
Train: 2018-08-04T23:53:48.824475: step 7773, loss 0.628099.
Train: 2018-08-04T23:53:49.090038: step 7774, loss 0.546174.
Train: 2018-08-04T23:53:49.324389: step 7775, loss 0.521537.
Train: 2018-08-04T23:53:49.574299: step 7776, loss 0.529665.
Train: 2018-08-04T23:53:49.824272: step 7777, loss 0.546044.
Train: 2018-08-04T23:53:50.074213: step 7778, loss 0.678106.
Train: 2018-08-04T23:53:50.324154: step 7779, loss 0.562503.
Train: 2018-08-04T23:53:50.574095: step 7780, loss 0.603751.
Test: 2018-08-04T23:53:51.823772: step 7780, loss 0.548576.
Train: 2018-08-04T23:53:52.058118: step 7781, loss 0.554284.
Train: 2018-08-04T23:53:52.308034: step 7782, loss 0.554305.
Train: 2018-08-04T23:53:52.558011: step 7783, loss 0.513229.
Train: 2018-08-04T23:53:52.807918: step 7784, loss 0.521436.
Train: 2018-08-04T23:53:53.057890: step 7785, loss 0.537839.
Train: 2018-08-04T23:53:53.307831: step 7786, loss 0.578999.
Train: 2018-08-04T23:53:53.557742: step 7787, loss 0.529506.
Train: 2018-08-04T23:53:53.807683: step 7788, loss 0.562493.
Train: 2018-08-04T23:53:54.057656: step 7789, loss 0.537662.
Train: 2018-08-04T23:53:54.307598: step 7790, loss 0.579045.
Test: 2018-08-04T23:53:55.572894: step 7790, loss 0.550326.
Train: 2018-08-04T23:53:55.807216: step 7791, loss 0.54587.
Train: 2018-08-04T23:53:56.057187: step 7792, loss 0.620597.
Train: 2018-08-04T23:53:56.307100: step 7793, loss 0.545848.
Train: 2018-08-04T23:53:56.557071: step 7794, loss 0.603974.
Train: 2018-08-04T23:53:56.807012: step 7795, loss 0.587349.
Train: 2018-08-04T23:53:57.056953: step 7796, loss 0.521063.
Train: 2018-08-04T23:53:57.306864: step 7797, loss 0.562479.
Train: 2018-08-04T23:53:57.556836: step 7798, loss 0.620394.
Train: 2018-08-04T23:53:57.806747: step 7799, loss 0.562499.
Train: 2018-08-04T23:53:58.056689: step 7800, loss 0.570757.
Test: 2018-08-04T23:53:59.322016: step 7800, loss 0.54936.
Train: 2018-08-04T23:54:00.212463: step 7801, loss 0.51317.
Train: 2018-08-04T23:54:00.478021: step 7802, loss 0.644757.
Train: 2018-08-04T23:54:00.743589: step 7803, loss 0.529753.
Train: 2018-08-04T23:54:00.977880: step 7804, loss 0.587143.
Train: 2018-08-04T23:54:01.227851: step 7805, loss 0.497205.
Train: 2018-08-04T23:54:01.477763: step 7806, loss 0.595284.
Train: 2018-08-04T23:54:01.727733: step 7807, loss 0.521789.
Train: 2018-08-04T23:54:01.977676: step 7808, loss 0.562607.
Train: 2018-08-04T23:54:02.227616: step 7809, loss 0.554442.
Train: 2018-08-04T23:54:02.477558: step 7810, loss 0.554438.
Test: 2018-08-04T23:54:03.727235: step 7810, loss 0.549332.
Train: 2018-08-04T23:54:03.961586: step 7811, loss 0.513583.
Train: 2018-08-04T23:54:04.211527: step 7812, loss 0.546221.
Train: 2018-08-04T23:54:04.461469: step 7813, loss 0.537981.
Train: 2018-08-04T23:54:04.711404: step 7814, loss 0.546122.
Train: 2018-08-04T23:54:04.961351: step 7815, loss 0.546066.
Train: 2018-08-04T23:54:05.211293: step 7816, loss 0.554257.
Train: 2018-08-04T23:54:05.461234: step 7817, loss 0.512882.
Train: 2018-08-04T23:54:05.711176: step 7818, loss 0.587344.
Train: 2018-08-04T23:54:05.961117: step 7819, loss 0.56245.
Train: 2018-08-04T23:54:06.211029: step 7820, loss 0.604062.
Test: 2018-08-04T23:54:07.491979: step 7820, loss 0.547181.
Train: 2018-08-04T23:54:07.726324: step 7821, loss 0.554105.
Train: 2018-08-04T23:54:08.007483: step 7822, loss 0.595767.
Train: 2018-08-04T23:54:08.257454: step 7823, loss 0.512445.
Train: 2018-08-04T23:54:08.507395: step 7824, loss 0.629125.
Train: 2018-08-04T23:54:08.757337: step 7825, loss 0.620737.
Train: 2018-08-04T23:54:09.007249: step 7826, loss 0.537521.
Train: 2018-08-04T23:54:09.257221: step 7827, loss 0.554165.
Train: 2018-08-04T23:54:09.507161: step 7828, loss 0.545902.
Train: 2018-08-04T23:54:09.757103: step 7829, loss 0.587311.
Train: 2018-08-04T23:54:10.007045: step 7830, loss 0.554226.
Test: 2018-08-04T23:54:11.287966: step 7830, loss 0.547194.
Train: 2018-08-04T23:54:11.537908: step 7831, loss 0.603777.
Train: 2018-08-04T23:54:11.803469: step 7832, loss 0.562519.
Train: 2018-08-04T23:54:12.037817: step 7833, loss 0.48032.
Train: 2018-08-04T23:54:12.303352: step 7834, loss 0.611871.
Train: 2018-08-04T23:54:12.553294: step 7835, loss 0.554333.
Train: 2018-08-04T23:54:12.803266: step 7836, loss 0.611794.
Train: 2018-08-04T23:54:13.053208: step 7837, loss 0.570764.
Train: 2018-08-04T23:54:13.303151: step 7838, loss 0.472669.
Train: 2018-08-04T23:54:13.553059: step 7839, loss 0.5953.
Train: 2018-08-04T23:54:13.803001: step 7840, loss 0.570768.
Test: 2018-08-04T23:54:15.052707: step 7840, loss 0.549899.
Train: 2018-08-04T23:54:15.318271: step 7841, loss 0.46458.
Train: 2018-08-04T23:54:15.552592: step 7842, loss 0.562583.
Train: 2018-08-04T23:54:15.818154: step 7843, loss 0.628127.
Train: 2018-08-04T23:54:16.068126: step 7844, loss 0.505217.
Train: 2018-08-04T23:54:16.318038: step 7845, loss 0.537951.
Train: 2018-08-04T23:54:16.552389: step 7846, loss 0.529682.
Train: 2018-08-04T23:54:16.802298: step 7847, loss 0.537826.
Train: 2018-08-04T23:54:17.052271: step 7848, loss 0.537745.
Train: 2018-08-04T23:54:17.302212: step 7849, loss 0.537656.
Train: 2018-08-04T23:54:17.552157: step 7850, loss 0.570759.
Test: 2018-08-04T23:54:18.801830: step 7850, loss 0.547593.
Train: 2018-08-04T23:54:19.051772: step 7851, loss 0.612353.
Train: 2018-08-04T23:54:19.254879: step 7852, loss 0.686766.
Train: 2018-08-04T23:54:19.504820: step 7853, loss 0.512604.
Train: 2018-08-04T23:54:19.754762: step 7854, loss 0.537557.
Train: 2018-08-04T23:54:20.020325: step 7855, loss 0.620545.
Train: 2018-08-04T23:54:20.270266: step 7856, loss 0.612176.
Train: 2018-08-04T23:54:20.520177: step 7857, loss 0.479884.
Train: 2018-08-04T23:54:20.770119: step 7858, loss 0.570756.
Train: 2018-08-04T23:54:21.020093: step 7859, loss 0.537761.
Train: 2018-08-04T23:54:21.270033: step 7860, loss 0.56251.
Test: 2018-08-04T23:54:22.519709: step 7860, loss 0.548554.
Train: 2018-08-04T23:54:22.769681: step 7861, loss 0.603734.
Train: 2018-08-04T23:54:23.019592: step 7862, loss 0.587225.
Train: 2018-08-04T23:54:23.269564: step 7863, loss 0.53788.
Train: 2018-08-04T23:54:23.519505: step 7864, loss 0.562549.
Train: 2018-08-04T23:54:23.769417: step 7865, loss 0.546152.
Train: 2018-08-04T23:54:24.019383: step 7866, loss 0.578961.
Train: 2018-08-04T23:54:24.269299: step 7867, loss 0.578955.
Train: 2018-08-04T23:54:24.519275: step 7868, loss 0.53804.
Train: 2018-08-04T23:54:24.769212: step 7869, loss 0.56259.
Train: 2018-08-04T23:54:25.019125: step 7870, loss 0.562595.
Test: 2018-08-04T23:54:26.268831: step 7870, loss 0.550085.
Train: 2018-08-04T23:54:26.503182: step 7871, loss 0.611616.
Train: 2018-08-04T23:54:26.753123: step 7872, loss 0.562617.
Train: 2018-08-04T23:54:27.003065: step 7873, loss 0.570777.
Train: 2018-08-04T23:54:27.253006: step 7874, loss 0.595181.
Train: 2018-08-04T23:54:27.502948: step 7875, loss 0.522091.
Train: 2018-08-04T23:54:27.752889: step 7876, loss 0.530247.
Train: 2018-08-04T23:54:28.002800: step 7877, loss 0.578901.
Train: 2018-08-04T23:54:28.268363: step 7878, loss 0.603219.
Train: 2018-08-04T23:54:28.518305: step 7879, loss 0.562703.
Train: 2018-08-04T23:54:28.768279: step 7880, loss 0.570803.
Test: 2018-08-04T23:54:30.017954: step 7880, loss 0.549797.
Train: 2018-08-04T23:54:30.267927: step 7881, loss 0.562731.
Train: 2018-08-04T23:54:30.533489: step 7882, loss 0.546603.
Train: 2018-08-04T23:54:30.783431: step 7883, loss 0.530482.
Train: 2018-08-04T23:54:31.033368: step 7884, loss 0.538532.
Train: 2018-08-04T23:54:31.283309: step 7885, loss 0.595044.
Train: 2018-08-04T23:54:31.533255: step 7886, loss 0.570807.
Train: 2018-08-04T23:54:31.783197: step 7887, loss 0.570807.
Train: 2018-08-04T23:54:32.033138: step 7888, loss 0.570808.
Train: 2018-08-04T23:54:32.283082: step 7889, loss 0.586964.
Train: 2018-08-04T23:54:32.533014: step 7890, loss 0.538527.
Test: 2018-08-04T23:54:33.782697: step 7890, loss 0.548891.
Train: 2018-08-04T23:54:34.017047: step 7891, loss 0.530458.
Train: 2018-08-04T23:54:34.269515: step 7892, loss 0.667738.
Train: 2018-08-04T23:54:34.519460: step 7893, loss 0.586944.
Train: 2018-08-04T23:54:34.769431: step 7894, loss 0.514505.
Train: 2018-08-04T23:54:35.019375: step 7895, loss 0.562791.
Train: 2018-08-04T23:54:35.269313: step 7896, loss 0.530653.
Train: 2018-08-04T23:54:35.534876: step 7897, loss 0.570833.
Train: 2018-08-04T23:54:35.800436: step 7898, loss 0.586915.
Train: 2018-08-04T23:54:36.034755: step 7899, loss 0.562795.
Train: 2018-08-04T23:54:36.284709: step 7900, loss 0.546723.
Test: 2018-08-04T23:54:37.534378: step 7900, loss 0.549182.
Train: 2018-08-04T23:54:38.471689: step 7901, loss 0.586912.
Train: 2018-08-04T23:54:38.721630: step 7902, loss 0.627096.
Train: 2018-08-04T23:54:38.971541: step 7903, loss 0.562821.
Train: 2018-08-04T23:54:39.221508: step 7904, loss 0.650972.
Train: 2018-08-04T23:54:39.487076: step 7905, loss 0.522967.
Train: 2018-08-04T23:54:39.737012: step 7906, loss 0.618705.
Train: 2018-08-04T23:54:39.986958: step 7907, loss 0.602694.
Train: 2018-08-04T23:54:40.236871: step 7908, loss 0.515525.
Train: 2018-08-04T23:54:40.486812: step 7909, loss 0.563058.
Train: 2018-08-04T23:54:40.736753: step 7910, loss 0.586753.
Test: 2018-08-04T23:54:41.986460: step 7910, loss 0.549781.
Train: 2018-08-04T23:54:42.220811: step 7911, loss 0.539488.
Train: 2018-08-04T23:54:42.470723: step 7912, loss 0.531654.
Train: 2018-08-04T23:54:42.720663: step 7913, loss 0.563126.
Train: 2018-08-04T23:54:42.970605: step 7914, loss 0.555245.
Train: 2018-08-04T23:54:43.220546: step 7915, loss 0.547347.
Train: 2018-08-04T23:54:43.470518: step 7916, loss 0.618311.
Train: 2018-08-04T23:54:43.720430: step 7917, loss 0.555197.
Train: 2018-08-04T23:54:43.970402: step 7918, loss 0.626209.
Train: 2018-08-04T23:54:44.220344: step 7919, loss 0.618274.
Train: 2018-08-04T23:54:44.470253: step 7920, loss 0.539548.
Test: 2018-08-04T23:54:45.738591: step 7920, loss 0.550226.
Train: 2018-08-04T23:54:45.988564: step 7921, loss 0.52389.
Train: 2018-08-04T23:54:46.238506: step 7922, loss 0.476755.
Train: 2018-08-04T23:54:46.504067: step 7923, loss 0.626113.
Train: 2018-08-04T23:54:46.738382: step 7924, loss 0.500057.
Train: 2018-08-04T23:54:46.988330: step 7925, loss 0.563061.
Train: 2018-08-04T23:54:47.238241: step 7926, loss 0.594698.
Train: 2018-08-04T23:54:47.488213: step 7927, loss 0.626448.
Train: 2018-08-04T23:54:47.738155: step 7928, loss 0.547135.
Train: 2018-08-04T23:54:47.988066: step 7929, loss 0.570924.
Train: 2018-08-04T23:54:48.238037: step 7930, loss 0.618548.
Test: 2018-08-04T23:54:49.494221: step 7930, loss 0.549387.
Train: 2018-08-04T23:54:49.759784: step 7931, loss 0.49162.
Train: 2018-08-04T23:54:50.025346: step 7932, loss 0.555039.
Train: 2018-08-04T23:54:50.259697: step 7933, loss 0.547057.
Train: 2018-08-04T23:54:50.509608: step 7934, loss 0.52311.
Train: 2018-08-04T23:54:50.759580: step 7935, loss 0.546919.
Train: 2018-08-04T23:54:51.009520: step 7936, loss 0.578865.
Train: 2018-08-04T23:54:51.259462: step 7937, loss 0.594923.
Train: 2018-08-04T23:54:51.509404: step 7938, loss 0.578872.
Train: 2018-08-04T23:54:51.774937: step 7939, loss 0.554739.
Train: 2018-08-04T23:54:52.024908: step 7940, loss 0.554715.
Test: 2018-08-04T23:54:53.274585: step 7940, loss 0.549471.
Train: 2018-08-04T23:54:53.524526: step 7941, loss 0.554689.
Train: 2018-08-04T23:54:53.758878: step 7942, loss 0.522361.
Train: 2018-08-04T23:54:54.008819: step 7943, loss 0.530335.
Train: 2018-08-04T23:54:54.274375: step 7944, loss 0.570789.
Train: 2018-08-04T23:54:54.524292: step 7945, loss 0.554511.
Train: 2018-08-04T23:54:54.774265: step 7946, loss 0.587082.
Train: 2018-08-04T23:54:55.024176: step 7947, loss 0.54627.
Train: 2018-08-04T23:54:55.274148: step 7948, loss 0.562585.
Train: 2018-08-04T23:54:55.524089: step 7949, loss 0.521601.
Train: 2018-08-04T23:54:55.774030: step 7950, loss 0.578973.
Test: 2018-08-04T23:54:57.023707: step 7950, loss 0.548603.
Train: 2018-08-04T23:54:57.258059: step 7951, loss 0.644798.
Train: 2018-08-04T23:54:57.508000: step 7952, loss 0.513199.
Train: 2018-08-04T23:54:57.757911: step 7953, loss 0.587215.
Train: 2018-08-04T23:54:58.007876: step 7954, loss 0.562529.
Train: 2018-08-04T23:54:58.257829: step 7955, loss 0.513162.
Train: 2018-08-04T23:54:58.507765: step 7956, loss 0.5131.
Train: 2018-08-04T23:54:58.757707: step 7957, loss 0.562503.
Train: 2018-08-04T23:54:59.007626: step 7958, loss 0.587291.
Train: 2018-08-04T23:54:59.257589: step 7959, loss 0.562484.
Train: 2018-08-04T23:54:59.507532: step 7960, loss 0.545907.
Test: 2018-08-04T23:55:00.757208: step 7960, loss 0.547856.
Train: 2018-08-04T23:55:01.007149: step 7961, loss 0.496134.
Train: 2018-08-04T23:55:01.257091: step 7962, loss 0.520879.
Train: 2018-08-04T23:55:01.507063: step 7963, loss 0.554125.
Train: 2018-08-04T23:55:01.756998: step 7964, loss 0.520469.
Train: 2018-08-04T23:55:02.006946: step 7965, loss 0.52874.
Train: 2018-08-04T23:55:02.256887: step 7966, loss 0.587542.
Train: 2018-08-04T23:55:02.506799: step 7967, loss 0.544918.
Train: 2018-08-04T23:55:02.756770: step 7968, loss 0.552935.
Train: 2018-08-04T23:55:03.006711: step 7969, loss 0.518762.
Train: 2018-08-04T23:55:03.272244: step 7970, loss 0.554761.
Test: 2018-08-04T23:55:04.521951: step 7970, loss 0.547783.
Train: 2018-08-04T23:55:04.771923: step 7971, loss 0.551122.
Train: 2018-08-04T23:55:05.021834: step 7972, loss 0.547286.
Train: 2018-08-04T23:55:05.271806: step 7973, loss 0.53215.
Train: 2018-08-04T23:55:05.521748: step 7974, loss 0.580763.
Train: 2018-08-04T23:55:05.771689: step 7975, loss 0.643795.
Train: 2018-08-04T23:55:06.021599: step 7976, loss 0.614992.
Train: 2018-08-04T23:55:06.271574: step 7977, loss 0.582454.
Train: 2018-08-04T23:55:06.521483: step 7978, loss 0.571071.
Train: 2018-08-04T23:55:06.771424: step 7979, loss 0.596799.
Train: 2018-08-04T23:55:07.021367: step 7980, loss 0.486754.
Test: 2018-08-04T23:55:08.271073: step 7980, loss 0.547645.
Train: 2018-08-04T23:55:08.505420: step 7981, loss 0.562337.
Train: 2018-08-04T23:55:08.755336: step 7982, loss 0.454058.
Train: 2018-08-04T23:55:09.005307: step 7983, loss 0.645746.
Train: 2018-08-04T23:55:09.255248: step 7984, loss 0.554133.
Train: 2018-08-04T23:55:09.505160: step 7985, loss 0.603993.
Train: 2018-08-04T23:55:09.755131: step 7986, loss 0.55423.
Train: 2018-08-04T23:55:10.005073: step 7987, loss 0.521138.
Train: 2018-08-04T23:55:10.254983: step 7988, loss 0.52948.
Train: 2018-08-04T23:55:10.504956: step 7989, loss 0.537654.
Train: 2018-08-04T23:55:10.754868: step 7990, loss 0.554355.
Test: 2018-08-04T23:55:12.020195: step 7990, loss 0.547901.
Train: 2018-08-04T23:55:12.254547: step 7991, loss 0.529446.
Train: 2018-08-04T23:55:12.504487: step 7992, loss 0.595655.
Train: 2018-08-04T23:55:12.754430: step 7993, loss 0.487654.
Train: 2018-08-04T23:55:13.004341: step 7994, loss 0.612125.
Train: 2018-08-04T23:55:13.254312: step 7995, loss 0.562198.
Train: 2018-08-04T23:55:13.504222: step 7996, loss 0.57073.
Train: 2018-08-04T23:55:13.754164: step 7997, loss 0.54577.
Train: 2018-08-04T23:55:14.004106: step 7998, loss 0.570741.
Train: 2018-08-04T23:55:14.254077: step 7999, loss 0.6372.
Train: 2018-08-04T23:55:14.504019: step 8000, loss 0.579267.
Test: 2018-08-04T23:55:15.769317: step 8000, loss 0.549046.
Train: 2018-08-04T23:55:16.612871: step 8001, loss 0.520903.
Train: 2018-08-04T23:55:16.878463: step 8002, loss 0.562479.
Train: 2018-08-04T23:55:17.065890: step 8003, loss 0.492444.
Train: 2018-08-04T23:55:17.315861: step 8004, loss 0.537633.
Train: 2018-08-04T23:55:17.565792: step 8005, loss 0.570717.
Train: 2018-08-04T23:55:17.815744: step 8006, loss 0.612314.
Train: 2018-08-04T23:55:18.081277: step 8007, loss 0.587399.
Train: 2018-08-04T23:55:18.331217: step 8008, loss 0.562664.
Train: 2018-08-04T23:55:18.581183: step 8009, loss 0.562587.
Train: 2018-08-04T23:55:18.831131: step 8010, loss 0.603666.
Test: 2018-08-04T23:55:20.096429: step 8010, loss 0.548097.
Train: 2018-08-04T23:55:20.330780: step 8011, loss 0.546145.
Train: 2018-08-04T23:55:20.580715: step 8012, loss 0.513442.
Train: 2018-08-04T23:55:20.830633: step 8013, loss 0.570802.
Train: 2018-08-04T23:55:21.080575: step 8014, loss 0.562636.
Train: 2018-08-04T23:55:21.330545: step 8015, loss 0.587124.
Train: 2018-08-04T23:55:21.580487: step 8016, loss 0.636139.
Train: 2018-08-04T23:55:21.830428: step 8017, loss 0.554473.
Train: 2018-08-04T23:55:22.080339: step 8018, loss 0.489465.
Train: 2018-08-04T23:55:22.330311: step 8019, loss 0.538267.
Train: 2018-08-04T23:55:22.580253: step 8020, loss 0.554517.
Test: 2018-08-04T23:55:23.829930: step 8020, loss 0.549424.
Train: 2018-08-04T23:55:24.064251: step 8021, loss 0.603334.
Train: 2018-08-04T23:55:24.314223: step 8022, loss 0.513835.
Train: 2018-08-04T23:55:24.564134: step 8023, loss 0.554495.
Train: 2018-08-04T23:55:24.814104: step 8024, loss 0.530029.
Train: 2018-08-04T23:55:25.064017: step 8025, loss 0.529959.
Train: 2018-08-04T23:55:25.313988: step 8026, loss 0.603492.
Train: 2018-08-04T23:55:25.563929: step 8027, loss 0.587138.
Train: 2018-08-04T23:55:25.813870: step 8028, loss 0.52979.
Train: 2018-08-04T23:55:26.063812: step 8029, loss 0.562576.
Train: 2018-08-04T23:55:26.313742: step 8030, loss 0.611798.
Test: 2018-08-04T23:55:27.579052: step 8030, loss 0.547901.
Train: 2018-08-04T23:55:27.813372: step 8031, loss 0.5543.
Train: 2018-08-04T23:55:28.063344: step 8032, loss 0.554342.
Train: 2018-08-04T23:55:28.313285: step 8033, loss 0.554349.
Train: 2018-08-04T23:55:28.563226: step 8034, loss 0.529714.
Train: 2018-08-04T23:55:28.828760: step 8035, loss 0.570763.
Train: 2018-08-04T23:55:29.078701: step 8036, loss 0.521423.
Train: 2018-08-04T23:55:29.328673: step 8037, loss 0.554339.
Train: 2018-08-04T23:55:29.578614: step 8038, loss 0.620384.
Train: 2018-08-04T23:55:29.828526: step 8039, loss 0.570812.
Train: 2018-08-04T23:55:30.094088: step 8040, loss 0.54603.
Test: 2018-08-04T23:55:31.328174: step 8040, loss 0.549132.
Train: 2018-08-04T23:55:31.578146: step 8041, loss 0.570851.
Train: 2018-08-04T23:55:31.828057: step 8042, loss 0.546086.
Train: 2018-08-04T23:55:32.077999: step 8043, loss 0.562481.
Train: 2018-08-04T23:55:32.327970: step 8044, loss 0.513115.
Train: 2018-08-04T23:55:32.577911: step 8045, loss 0.587243.
Train: 2018-08-04T23:55:32.827848: step 8046, loss 0.562514.
Train: 2018-08-04T23:55:33.093394: step 8047, loss 0.612.
Train: 2018-08-04T23:55:33.327739: step 8048, loss 0.570756.
Train: 2018-08-04T23:55:33.577647: step 8049, loss 0.578989.
Train: 2018-08-04T23:55:33.827590: step 8050, loss 0.56254.
Test: 2018-08-04T23:55:35.092918: step 8050, loss 0.549789.
Train: 2018-08-04T23:55:35.327269: step 8051, loss 0.595385.
Train: 2018-08-04T23:55:35.577180: step 8052, loss 0.562572.
Train: 2018-08-04T23:55:35.827122: step 8053, loss 0.529884.
Train: 2018-08-04T23:55:36.077061: step 8054, loss 0.55443.
Train: 2018-08-04T23:55:36.327003: step 8055, loss 0.611592.
Train: 2018-08-04T23:55:36.592567: step 8056, loss 0.570775.
Train: 2018-08-04T23:55:36.826888: step 8057, loss 0.538237.
Train: 2018-08-04T23:55:37.092474: step 8058, loss 0.603296.
Train: 2018-08-04T23:55:37.326800: step 8059, loss 0.619469.
Train: 2018-08-04T23:55:37.576741: step 8060, loss 0.58698.
Test: 2018-08-04T23:55:38.810797: step 8060, loss 0.549662.
Train: 2018-08-04T23:55:39.060738: step 8061, loss 0.506317.
Train: 2018-08-04T23:55:39.310679: step 8062, loss 0.546674.
Train: 2018-08-04T23:55:39.560622: step 8063, loss 0.578872.
Train: 2018-08-04T23:55:39.826185: step 8064, loss 0.490495.
Train: 2018-08-04T23:55:40.076156: step 8065, loss 0.594954.
Train: 2018-08-04T23:55:40.326091: step 8066, loss 0.594955.
Train: 2018-08-04T23:55:40.576033: step 8067, loss 0.522621.
Train: 2018-08-04T23:55:40.825983: step 8068, loss 0.546714.
Train: 2018-08-04T23:55:41.075891: step 8069, loss 0.570828.
Train: 2018-08-04T23:55:41.325832: step 8070, loss 0.538618.
Test: 2018-08-04T23:55:42.575540: step 8070, loss 0.549107.
Train: 2018-08-04T23:55:42.825512: step 8071, loss 0.538571.
Train: 2018-08-04T23:55:43.075454: step 8072, loss 0.586964.
Train: 2018-08-04T23:55:43.325395: step 8073, loss 0.538466.
Train: 2018-08-04T23:55:43.575336: step 8074, loss 0.562701.
Train: 2018-08-04T23:55:43.825247: step 8075, loss 0.627559.
Train: 2018-08-04T23:55:44.075189: step 8076, loss 0.562686.
Train: 2018-08-04T23:55:44.325161: step 8077, loss 0.514051.
Train: 2018-08-04T23:55:44.584767: step 8078, loss 0.57079.
Train: 2018-08-04T23:55:44.843941: step 8079, loss 0.587029.
Train: 2018-08-04T23:55:45.103633: step 8080, loss 0.489566.
Test: 2018-08-04T23:55:46.340078: step 8080, loss 0.548678.
Train: 2018-08-04T23:55:46.590019: step 8081, loss 0.595192.
Train: 2018-08-04T23:55:46.839985: step 8082, loss 0.627792.
Train: 2018-08-04T23:55:47.089932: step 8083, loss 0.546362.
Train: 2018-08-04T23:55:47.339874: step 8084, loss 0.546369.
Train: 2018-08-04T23:55:47.605442: step 8085, loss 0.489393.
Train: 2018-08-04T23:55:47.851990: step 8086, loss 0.587083.
Train: 2018-08-04T23:55:48.115532: step 8087, loss 0.554443.
Train: 2018-08-04T23:55:48.364224: step 8088, loss 0.529899.
Train: 2018-08-04T23:55:48.614165: step 8089, loss 0.546198.
Train: 2018-08-04T23:55:48.864106: step 8090, loss 0.578966.
Test: 2018-08-04T23:55:50.113784: step 8090, loss 0.548062.
Train: 2018-08-04T23:55:50.363749: step 8091, loss 0.570759.
Train: 2018-08-04T23:55:50.613666: step 8092, loss 0.504953.
Train: 2018-08-04T23:55:50.863638: step 8093, loss 0.513049.
Train: 2018-08-04T23:55:51.113550: step 8094, loss 0.512869.
Train: 2018-08-04T23:55:51.363494: step 8095, loss 0.545854.
Train: 2018-08-04T23:55:51.629082: step 8096, loss 0.545766.
Train: 2018-08-04T23:55:51.879025: step 8097, loss 0.595861.
Train: 2018-08-04T23:55:52.144559: step 8098, loss 0.604311.
Train: 2018-08-04T23:55:52.378909: step 8099, loss 0.562392.
Train: 2018-08-04T23:55:52.628849: step 8100, loss 0.553989.
Test: 2018-08-04T23:55:53.894148: step 8100, loss 0.548348.
Train: 2018-08-04T23:55:54.737700: step 8101, loss 0.486746.
Train: 2018-08-04T23:55:54.972053: step 8102, loss 0.537106.
Train: 2018-08-04T23:55:55.237614: step 8103, loss 0.494821.
Train: 2018-08-04T23:55:55.487526: step 8104, loss 0.604715.
Train: 2018-08-04T23:55:55.737497: step 8105, loss 0.596306.
Train: 2018-08-04T23:55:55.987438: step 8106, loss 0.528361.
Train: 2018-08-04T23:55:56.237382: step 8107, loss 0.553836.
Train: 2018-08-04T23:55:56.487321: step 8108, loss 0.604926.
Train: 2018-08-04T23:55:56.737258: step 8109, loss 0.52828.
Train: 2018-08-04T23:55:57.002825: step 8110, loss 0.587899.
Test: 2018-08-04T23:55:58.268123: step 8110, loss 0.547142.
Train: 2018-08-04T23:55:58.502445: step 8111, loss 0.579372.
Train: 2018-08-04T23:55:58.752418: step 8112, loss 0.613381.
Train: 2018-08-04T23:55:59.002355: step 8113, loss 0.5284.
Train: 2018-08-04T23:55:59.252299: step 8114, loss 0.494554.
Train: 2018-08-04T23:55:59.502211: step 8115, loss 0.596252.
Train: 2018-08-04T23:55:59.752179: step 8116, loss 0.587756.
Train: 2018-08-04T23:56:00.002126: step 8117, loss 0.511644.
Train: 2018-08-04T23:56:00.252036: step 8118, loss 0.570811.
Train: 2018-08-04T23:56:00.501976: step 8119, loss 0.537042.
Train: 2018-08-04T23:56:00.751950: step 8120, loss 0.655189.
Test: 2018-08-04T23:56:01.986003: step 8120, loss 0.549096.
Train: 2018-08-04T23:56:02.235975: step 8121, loss 0.545545.
Train: 2018-08-04T23:56:02.501508: step 8122, loss 0.604381.
Train: 2018-08-04T23:56:02.751473: step 8123, loss 0.537282.
Train: 2018-08-04T23:56:02.985770: step 8124, loss 0.595834.
Train: 2018-08-04T23:56:03.235740: step 8125, loss 0.579094.
Train: 2018-08-04T23:56:03.485682: step 8126, loss 0.579063.
Train: 2018-08-04T23:56:03.735623: step 8127, loss 0.570757.
Train: 2018-08-04T23:56:03.985565: step 8128, loss 0.521261.
Train: 2018-08-04T23:56:04.251128: step 8129, loss 0.537828.
Train: 2018-08-04T23:56:04.501072: step 8130, loss 0.504984.
Test: 2018-08-04T23:56:05.766368: step 8130, loss 0.548991.
Train: 2018-08-04T23:56:06.000721: step 8131, loss 0.587211.
Train: 2018-08-04T23:56:06.250659: step 8132, loss 0.496793.
Train: 2018-08-04T23:56:06.500601: step 8133, loss 0.562541.
Train: 2018-08-04T23:56:06.750512: step 8134, loss 0.546053.
Train: 2018-08-04T23:56:07.000482: step 8135, loss 0.562513.
Train: 2018-08-04T23:56:07.250428: step 8136, loss 0.579008.
Train: 2018-08-04T23:56:07.500369: step 8137, loss 0.562501.
Train: 2018-08-04T23:56:07.765899: step 8138, loss 0.545984.
Train: 2018-08-04T23:56:08.062735: step 8139, loss 0.636854.
Train: 2018-08-04T23:56:08.312677: step 8140, loss 0.546001.
Test: 2018-08-04T23:56:09.562353: step 8140, loss 0.549311.
Train: 2018-08-04T23:56:09.796701: step 8141, loss 0.620227.
Train: 2018-08-04T23:56:10.046646: step 8142, loss 0.620122.
Train: 2018-08-04T23:56:10.296589: step 8143, loss 0.587162.
Train: 2018-08-04T23:56:10.546529: step 8144, loss 0.60345.
Train: 2018-08-04T23:56:10.812091: step 8145, loss 0.562641.
Train: 2018-08-04T23:56:11.062033: step 8146, loss 0.514077.
Train: 2018-08-04T23:56:11.311974: step 8147, loss 0.578903.
Train: 2018-08-04T23:56:11.561911: step 8148, loss 0.546614.
Train: 2018-08-04T23:56:11.811852: step 8149, loss 0.522502.
Train: 2018-08-04T23:56:12.061799: step 8150, loss 0.570826.
Test: 2018-08-04T23:56:13.311476: step 8150, loss 0.549164.
Train: 2018-08-04T23:56:13.545797: step 8151, loss 0.594965.
Train: 2018-08-04T23:56:13.795762: step 8152, loss 0.554768.
Train: 2018-08-04T23:56:14.045709: step 8153, loss 0.538727.
Train: 2018-08-04T23:56:14.233159: step 8154, loss 0.579942.
Train: 2018-08-04T23:56:14.483077: step 8155, loss 0.514669.
Train: 2018-08-04T23:56:14.733048: step 8156, loss 0.586903.
Train: 2018-08-04T23:56:14.982990: step 8157, loss 0.602976.
Train: 2018-08-04T23:56:15.232927: step 8158, loss 0.602959.
Train: 2018-08-04T23:56:15.482873: step 8159, loss 0.643006.
Train: 2018-08-04T23:56:15.732815: step 8160, loss 0.594844.
Test: 2018-08-04T23:56:16.998112: step 8160, loss 0.549818.
Train: 2018-08-04T23:56:17.232433: step 8161, loss 0.531082.
Train: 2018-08-04T23:56:17.482405: step 8162, loss 0.578859.
Train: 2018-08-04T23:56:17.747965: step 8163, loss 0.563008.
Train: 2018-08-04T23:56:17.997906: step 8164, loss 0.547219.
Train: 2018-08-04T23:56:18.247847: step 8165, loss 0.578863.
Train: 2018-08-04T23:56:18.497794: step 8166, loss 0.48418.
Train: 2018-08-04T23:56:18.747733: step 8167, loss 0.570962.
Train: 2018-08-04T23:56:19.013298: step 8168, loss 0.563054.
Train: 2018-08-04T23:56:19.263239: step 8169, loss 0.578858.
Train: 2018-08-04T23:56:19.513181: step 8170, loss 0.602612.
Test: 2018-08-04T23:56:20.762856: step 8170, loss 0.549446.
Train: 2018-08-04T23:56:20.997176: step 8171, loss 0.563031.
Train: 2018-08-04T23:56:21.262771: step 8172, loss 0.555117.
Train: 2018-08-04T23:56:21.497061: step 8173, loss 0.531361.
Train: 2018-08-04T23:56:21.747031: step 8174, loss 0.61848.
Train: 2018-08-04T23:56:21.996972: step 8175, loss 0.539231.
Train: 2018-08-04T23:56:22.246916: step 8176, loss 0.563021.
Train: 2018-08-04T23:56:22.496825: step 8177, loss 0.5471.
Train: 2018-08-04T23:56:22.746767: step 8178, loss 0.555009.
Train: 2018-08-04T23:56:22.996735: step 8179, loss 0.570787.
Train: 2018-08-04T23:56:23.246677: step 8180, loss 0.538923.
Test: 2018-08-04T23:56:24.496357: step 8180, loss 0.54898.
Train: 2018-08-04T23:56:24.730678: step 8181, loss 0.610765.
Train: 2018-08-04T23:56:24.980649: step 8182, loss 0.57083.
Train: 2018-08-04T23:56:25.230591: step 8183, loss 0.530051.
Train: 2018-08-04T23:56:25.480532: step 8184, loss 0.570436.
Train: 2018-08-04T23:56:25.730476: step 8185, loss 0.571583.
Train: 2018-08-04T23:56:25.980410: step 8186, loss 0.552283.
Train: 2018-08-04T23:56:26.230325: step 8187, loss 0.537926.
Train: 2018-08-04T23:56:26.480268: step 8188, loss 0.594499.
Train: 2018-08-04T23:56:26.730238: step 8189, loss 0.584231.
Train: 2018-08-04T23:56:26.980180: step 8190, loss 0.498472.
Test: 2018-08-04T23:56:28.229857: step 8190, loss 0.54675.
Train: 2018-08-04T23:56:28.479800: step 8191, loss 0.593609.
Train: 2018-08-04T23:56:28.745390: step 8192, loss 0.54829.
Train: 2018-08-04T23:56:28.979712: step 8193, loss 0.560451.
Train: 2018-08-04T23:56:29.229653: step 8194, loss 0.555765.
Train: 2018-08-04T23:56:29.479595: step 8195, loss 0.600163.
Train: 2018-08-04T23:56:29.729537: step 8196, loss 0.537232.
Train: 2018-08-04T23:56:29.979447: step 8197, loss 0.539191.
Train: 2018-08-04T23:56:30.229390: step 8198, loss 0.579451.
Train: 2018-08-04T23:56:30.479361: step 8199, loss 0.595578.
Train: 2018-08-04T23:56:30.744926: step 8200, loss 0.586166.
Test: 2018-08-04T23:56:31.978979: step 8200, loss 0.549599.
Train: 2018-08-04T23:56:32.838183: step 8201, loss 0.513442.
Train: 2018-08-04T23:56:33.088094: step 8202, loss 0.570042.
Train: 2018-08-04T23:56:33.338067: step 8203, loss 0.547428.
Train: 2018-08-04T23:56:33.588008: step 8204, loss 0.498392.
Train: 2018-08-04T23:56:33.837949: step 8205, loss 0.55456.
Train: 2018-08-04T23:56:34.087891: step 8206, loss 0.57065.
Train: 2018-08-04T23:56:34.337803: step 8207, loss 0.53052.
Train: 2018-08-04T23:56:34.587745: step 8208, loss 0.538334.
Train: 2018-08-04T23:56:34.837686: step 8209, loss 0.490056.
Train: 2018-08-04T23:56:35.087657: step 8210, loss 0.569952.
Test: 2018-08-04T23:56:36.352955: step 8210, loss 0.548831.
Train: 2018-08-04T23:56:36.602897: step 8211, loss 0.561563.
Train: 2018-08-04T23:56:36.837217: step 8212, loss 0.50359.
Train: 2018-08-04T23:56:37.087188: step 8213, loss 0.571204.
Train: 2018-08-04T23:56:37.337130: step 8214, loss 0.553257.
Train: 2018-08-04T23:56:37.587071: step 8215, loss 0.563034.
Train: 2018-08-04T23:56:37.836984: step 8216, loss 0.60541.
Train: 2018-08-04T23:56:38.086925: step 8217, loss 0.546205.
Train: 2018-08-04T23:56:38.336896: step 8218, loss 0.553638.
Train: 2018-08-04T23:56:38.586807: step 8219, loss 0.569938.
Train: 2018-08-04T23:56:38.836779: step 8220, loss 0.502898.
Test: 2018-08-04T23:56:40.102077: step 8220, loss 0.548594.
Train: 2018-08-04T23:56:40.336397: step 8221, loss 0.561354.
Train: 2018-08-04T23:56:40.586340: step 8222, loss 0.545109.
Train: 2018-08-04T23:56:40.836280: step 8223, loss 0.631672.
Train: 2018-08-04T23:56:41.086252: step 8224, loss 0.544697.
Train: 2018-08-04T23:56:41.336193: step 8225, loss 0.544843.
Train: 2018-08-04T23:56:41.586129: step 8226, loss 0.571068.
Train: 2018-08-04T23:56:41.851698: step 8227, loss 0.561588.
Train: 2018-08-04T23:56:42.101639: step 8228, loss 0.580086.
Train: 2018-08-04T23:56:42.351551: step 8229, loss 0.595918.
Train: 2018-08-04T23:56:42.601523: step 8230, loss 0.586774.
Test: 2018-08-04T23:56:43.851200: step 8230, loss 0.548159.
Train: 2018-08-04T23:56:44.085521: step 8231, loss 0.496178.
Train: 2018-08-04T23:56:44.335491: step 8232, loss 0.585437.
Train: 2018-08-04T23:56:44.585404: step 8233, loss 0.477564.
Train: 2018-08-04T23:56:44.835374: step 8234, loss 0.578029.
Train: 2018-08-04T23:56:45.085312: step 8235, loss 0.621564.
Train: 2018-08-04T23:56:45.335228: step 8236, loss 0.612599.
Train: 2018-08-04T23:56:45.616435: step 8237, loss 0.53561.
Train: 2018-08-04T23:56:45.850762: step 8238, loss 0.563.
Train: 2018-08-04T23:56:46.100673: step 8239, loss 0.529683.
Train: 2018-08-04T23:56:46.350647: step 8240, loss 0.562173.
Test: 2018-08-04T23:56:47.615942: step 8240, loss 0.548362.
Train: 2018-08-04T23:56:47.850294: step 8241, loss 0.604463.
Train: 2018-08-04T23:56:48.100235: step 8242, loss 0.479209.
Train: 2018-08-04T23:56:48.350176: step 8243, loss 0.562476.
Train: 2018-08-04T23:56:48.600118: step 8244, loss 0.546577.
Train: 2018-08-04T23:56:48.850060: step 8245, loss 0.570568.
Train: 2018-08-04T23:56:49.100001: step 8246, loss 0.503192.
Train: 2018-08-04T23:56:49.349942: step 8247, loss 0.645532.
Train: 2018-08-04T23:56:49.599853: step 8248, loss 0.535923.
Train: 2018-08-04T23:56:49.849825: step 8249, loss 0.551303.
Train: 2018-08-04T23:56:50.099737: step 8250, loss 0.526801.
Test: 2018-08-04T23:56:51.349443: step 8250, loss 0.548681.
Train: 2018-08-04T23:56:51.646274: step 8251, loss 0.529717.
Train: 2018-08-04T23:56:51.896190: step 8252, loss 0.530305.
Train: 2018-08-04T23:56:52.146131: step 8253, loss 0.556453.
Train: 2018-08-04T23:56:52.396104: step 8254, loss 0.545764.
Train: 2018-08-04T23:56:52.661668: step 8255, loss 0.544846.
Train: 2018-08-04T23:56:52.895958: step 8256, loss 0.560674.
Train: 2018-08-04T23:56:53.145923: step 8257, loss 0.598079.
Train: 2018-08-04T23:56:53.395870: step 8258, loss 0.570804.
Train: 2018-08-04T23:56:53.645811: step 8259, loss 0.494783.
Train: 2018-08-04T23:56:53.895752: step 8260, loss 0.525303.
Test: 2018-08-04T23:56:55.145429: step 8260, loss 0.548958.
Train: 2018-08-04T23:56:55.379782: step 8261, loss 0.606714.
Train: 2018-08-04T23:56:55.645345: step 8262, loss 0.557376.
Train: 2018-08-04T23:56:55.879665: step 8263, loss 0.511546.
Train: 2018-08-04T23:56:56.129574: step 8264, loss 0.642246.
Train: 2018-08-04T23:56:56.379547: step 8265, loss 0.656232.
Train: 2018-08-04T23:56:56.629489: step 8266, loss 0.612183.
Train: 2018-08-04T23:56:56.879431: step 8267, loss 0.546762.
Train: 2018-08-04T23:56:57.129372: step 8268, loss 0.570438.
Train: 2018-08-04T23:56:57.379306: step 8269, loss 0.61177.
Train: 2018-08-04T23:56:57.629250: step 8270, loss 0.546856.
Test: 2018-08-04T23:56:58.894550: step 8270, loss 0.549267.
Train: 2018-08-04T23:56:59.144520: step 8271, loss 0.5388.
Train: 2018-08-04T23:56:59.378815: step 8272, loss 0.538869.
Train: 2018-08-04T23:56:59.628787: step 8273, loss 0.571019.
Train: 2018-08-04T23:56:59.878728: step 8274, loss 0.571102.
Train: 2018-08-04T23:57:00.128637: step 8275, loss 0.507962.
Train: 2018-08-04T23:57:00.378611: step 8276, loss 0.563113.
Train: 2018-08-04T23:57:00.628520: step 8277, loss 0.578848.
Train: 2018-08-04T23:57:00.878490: step 8278, loss 0.555226.
Train: 2018-08-04T23:57:01.128403: step 8279, loss 0.547318.
Train: 2018-08-04T23:57:01.378378: step 8280, loss 0.499907.
Test: 2018-08-04T23:57:02.628052: step 8280, loss 0.549981.
Train: 2018-08-04T23:57:02.878021: step 8281, loss 0.531349.
Train: 2018-08-04T23:57:03.127967: step 8282, loss 0.539124.
Train: 2018-08-04T23:57:03.393528: step 8283, loss 0.626732.
Train: 2018-08-04T23:57:03.643471: step 8284, loss 0.562873.
Train: 2018-08-04T23:57:03.893382: step 8285, loss 0.618921.
Train: 2018-08-04T23:57:04.143353: step 8286, loss 0.554821.
Train: 2018-08-04T23:57:04.393295: step 8287, loss 0.602931.
Train: 2018-08-04T23:57:04.643236: step 8288, loss 0.570857.
Train: 2018-08-04T23:57:04.893146: step 8289, loss 0.602913.
Train: 2018-08-04T23:57:05.143119: step 8290, loss 0.578865.
Test: 2018-08-04T23:57:06.392795: step 8290, loss 0.548971.
Train: 2018-08-04T23:57:06.642763: step 8291, loss 0.610838.
Train: 2018-08-04T23:57:06.892680: step 8292, loss 0.610769.
Train: 2018-08-04T23:57:07.142651: step 8293, loss 0.562958.
Train: 2018-08-04T23:57:07.392592: step 8294, loss 0.563015.
Train: 2018-08-04T23:57:07.642533: step 8295, loss 0.539316.
Train: 2018-08-04T23:57:07.908066: step 8296, loss 0.547273.
Train: 2018-08-04T23:57:08.158037: step 8297, loss 0.61043.
Train: 2018-08-04T23:57:08.423600: step 8298, loss 0.547348.
Train: 2018-08-04T23:57:08.673513: step 8299, loss 0.58674.
Train: 2018-08-04T23:57:08.923484: step 8300, loss 0.54742.
Test: 2018-08-04T23:57:10.173160: step 8300, loss 0.550928.
Train: 2018-08-04T23:57:11.141683: step 8301, loss 0.547429.
Train: 2018-08-04T23:57:11.391655: step 8302, loss 0.555295.
Train: 2018-08-04T23:57:11.641567: step 8303, loss 0.547411.
Train: 2018-08-04T23:57:11.907160: step 8304, loss 0.539494.
Train: 2018-08-04T23:57:12.094617: step 8305, loss 0.529432.
Train: 2018-08-04T23:57:12.344526: step 8306, loss 0.499767.
Train: 2018-08-04T23:57:12.594499: step 8307, loss 0.547087.
Train: 2018-08-04T23:57:12.860032: step 8308, loss 0.546946.
Train: 2018-08-04T23:57:13.109973: step 8309, loss 0.570855.
Train: 2018-08-04T23:57:13.359944: step 8310, loss 0.546691.
Test: 2018-08-04T23:57:14.625243: step 8310, loss 0.549063.
Train: 2018-08-04T23:57:14.859594: step 8311, loss 0.595033.
Train: 2018-08-04T23:57:15.109504: step 8312, loss 0.619386.
Train: 2018-08-04T23:57:15.359476: step 8313, loss 0.59511.
Train: 2018-08-04T23:57:15.609388: step 8314, loss 0.603227.
Train: 2018-08-04T23:57:15.859330: step 8315, loss 0.55459.
Train: 2018-08-04T23:57:16.109303: step 8316, loss 0.595092.
Train: 2018-08-04T23:57:16.359213: step 8317, loss 0.546531.
Train: 2018-08-04T23:57:16.624775: step 8318, loss 0.562716.
Train: 2018-08-04T23:57:16.859125: step 8319, loss 0.635446.
Train: 2018-08-04T23:57:17.109066: step 8320, loss 0.554691.
Test: 2018-08-04T23:57:18.358744: step 8320, loss 0.549707.
Train: 2018-08-04T23:57:18.593064: step 8321, loss 0.570823.
Train: 2018-08-04T23:57:18.858655: step 8322, loss 0.54673.
Train: 2018-08-04T23:57:19.108600: step 8323, loss 0.458471.
Train: 2018-08-04T23:57:19.358539: step 8324, loss 0.490419.
Train: 2018-08-04T23:57:19.608451: step 8325, loss 0.578884.
Train: 2018-08-04T23:57:19.858425: step 8326, loss 0.538437.
Train: 2018-08-04T23:57:20.108366: step 8327, loss 0.4815.
Train: 2018-08-04T23:57:20.358306: step 8328, loss 0.562628.
Train: 2018-08-04T23:57:20.608245: step 8329, loss 0.628124.
Train: 2018-08-04T23:57:20.873812: step 8330, loss 0.570774.
Test: 2018-08-04T23:57:22.123487: step 8330, loss 0.547474.
Train: 2018-08-04T23:57:22.357808: step 8331, loss 0.554309.
Train: 2018-08-04T23:57:22.607750: step 8332, loss 0.554281.
Train: 2018-08-04T23:57:22.857718: step 8333, loss 0.546.
Train: 2018-08-04T23:57:23.107664: step 8334, loss 0.579023.
Train: 2018-08-04T23:57:23.357603: step 8335, loss 0.54592.
Train: 2018-08-04T23:57:23.607545: step 8336, loss 0.504427.
Train: 2018-08-04T23:57:23.873105: step 8337, loss 0.454359.
Train: 2018-08-04T23:57:24.107399: step 8338, loss 0.57077.
Train: 2018-08-04T23:57:24.357363: step 8339, loss 0.537186.
Train: 2018-08-04T23:57:24.607313: step 8340, loss 0.55395.
Test: 2018-08-04T23:57:25.856987: step 8340, loss 0.549339.
Train: 2018-08-04T23:57:26.091339: step 8341, loss 0.553882.
Train: 2018-08-04T23:57:26.341279: step 8342, loss 0.579459.
Train: 2018-08-04T23:57:26.591222: step 8343, loss 0.562355.
Train: 2018-08-04T23:57:26.841162: step 8344, loss 0.468451.
Train: 2018-08-04T23:57:27.091104: step 8345, loss 0.510864.
Train: 2018-08-04T23:57:27.356667: step 8346, loss 0.562149.
Train: 2018-08-04T23:57:27.606610: step 8347, loss 0.623749.
Train: 2018-08-04T23:57:27.856549: step 8348, loss 0.596965.
Train: 2018-08-04T23:57:28.106462: step 8349, loss 0.510455.
Train: 2018-08-04T23:57:28.372024: step 8350, loss 0.562309.
Test: 2018-08-04T23:57:29.606109: step 8350, loss 0.546722.
Train: 2018-08-04T23:57:29.840431: step 8351, loss 0.614183.
Train: 2018-08-04T23:57:30.090401: step 8352, loss 0.579488.
Train: 2018-08-04T23:57:30.340313: step 8353, loss 0.527947.
Train: 2018-08-04T23:57:30.590284: step 8354, loss 0.562335.
Train: 2018-08-04T23:57:30.840197: step 8355, loss 0.528026.
Train: 2018-08-04T23:57:31.090138: step 8356, loss 0.605205.
Train: 2018-08-04T23:57:31.355700: step 8357, loss 0.596572.
Train: 2018-08-04T23:57:31.605674: step 8358, loss 0.673337.
Train: 2018-08-04T23:57:31.855613: step 8359, loss 0.545356.
Train: 2018-08-04T23:57:32.105525: step 8360, loss 0.503137.
Test: 2018-08-04T23:57:33.370853: step 8360, loss 0.547304.
Train: 2018-08-04T23:57:33.605175: step 8361, loss 0.57081.
Train: 2018-08-04T23:57:33.855145: step 8362, loss 0.469814.
Train: 2018-08-04T23:57:34.105086: step 8363, loss 0.562371.
Train: 2018-08-04T23:57:34.370651: step 8364, loss 0.604397.
Train: 2018-08-04T23:57:34.604972: step 8365, loss 0.554003.
Train: 2018-08-04T23:57:34.854881: step 8366, loss 0.570777.
Train: 2018-08-04T23:57:35.104822: step 8367, loss 0.520611.
Train: 2018-08-04T23:57:35.354801: step 8368, loss 0.620919.
Train: 2018-08-04T23:57:35.604735: step 8369, loss 0.512394.
Train: 2018-08-04T23:57:35.854647: step 8370, loss 0.587401.
Test: 2018-08-04T23:57:37.119975: step 8370, loss 0.549703.
Train: 2018-08-04T23:57:37.369917: step 8371, loss 0.537491.
Train: 2018-08-04T23:57:37.635480: step 8372, loss 0.570765.
Train: 2018-08-04T23:57:37.869825: step 8373, loss 0.504363.
Train: 2018-08-04T23:57:38.119774: step 8374, loss 0.562466.
Train: 2018-08-04T23:57:38.385304: step 8375, loss 0.562458.
Train: 2018-08-04T23:57:38.635245: step 8376, loss 0.587367.
Train: 2018-08-04T23:57:38.885186: step 8377, loss 0.595654.
Train: 2018-08-04T23:57:39.135158: step 8378, loss 0.56247.
Train: 2018-08-04T23:57:39.400721: step 8379, loss 0.545932.
Train: 2018-08-04T23:57:39.650663: step 8380, loss 0.603825.
Test: 2018-08-04T23:57:40.900340: step 8380, loss 0.548154.
Train: 2018-08-04T23:57:41.134692: step 8381, loss 0.529498.
Train: 2018-08-04T23:57:41.384631: step 8382, loss 0.570757.
Train: 2018-08-04T23:57:41.634543: step 8383, loss 0.578991.
Train: 2018-08-04T23:57:41.884515: step 8384, loss 0.554318.
Train: 2018-08-04T23:57:42.134425: step 8385, loss 0.595397.
Train: 2018-08-04T23:57:42.384399: step 8386, loss 0.554361.
Train: 2018-08-04T23:57:42.634310: step 8387, loss 0.480727.
Train: 2018-08-04T23:57:42.884281: step 8388, loss 0.611718.
Train: 2018-08-04T23:57:43.134191: step 8389, loss 0.587128.
Train: 2018-08-04T23:57:43.384157: step 8390, loss 0.611631.
Test: 2018-08-04T23:57:44.633842: step 8390, loss 0.549367.
Train: 2018-08-04T23:57:44.868162: step 8391, loss 0.578959.
Train: 2018-08-04T23:57:45.118134: step 8392, loss 0.530101.
Train: 2018-08-04T23:57:45.383665: step 8393, loss 0.538292.
Train: 2018-08-04T23:57:45.633631: step 8394, loss 0.497707.
Train: 2018-08-04T23:57:45.867959: step 8395, loss 0.570788.
Train: 2018-08-04T23:57:46.117900: step 8396, loss 0.562641.
Train: 2018-08-04T23:57:46.367843: step 8397, loss 0.611487.
Train: 2018-08-04T23:57:46.633402: step 8398, loss 0.562646.
Train: 2018-08-04T23:57:46.883344: step 8399, loss 0.587051.
Train: 2018-08-04T23:57:47.133256: step 8400, loss 0.55454.
Test: 2018-08-04T23:57:48.382962: step 8400, loss 0.548562.
Train: 2018-08-04T23:57:49.226516: step 8401, loss 0.505861.
Train: 2018-08-04T23:57:49.476487: step 8402, loss 0.595152.
Train: 2018-08-04T23:57:49.742044: step 8403, loss 0.570801.
Train: 2018-08-04T23:57:49.987881: step 8404, loss 0.513965.
Train: 2018-08-04T23:57:50.237822: step 8405, loss 0.538279.
Train: 2018-08-04T23:57:50.487765: step 8406, loss 0.546357.
Train: 2018-08-04T23:57:50.737705: step 8407, loss 0.546319.
Train: 2018-08-04T23:57:51.003242: step 8408, loss 0.603447.
Train: 2018-08-04T23:57:51.253203: step 8409, loss 0.48085.
Train: 2018-08-04T23:57:51.503150: step 8410, loss 0.587152.
Test: 2018-08-04T23:57:52.752828: step 8410, loss 0.5494.
Train: 2018-08-04T23:57:52.987178: step 8411, loss 0.603619.
Train: 2018-08-04T23:57:53.237119: step 8412, loss 0.472173.
Train: 2018-08-04T23:57:53.487061: step 8413, loss 0.562525.
Train: 2018-08-04T23:57:53.736972: step 8414, loss 0.579014.
Train: 2018-08-04T23:57:53.986914: step 8415, loss 0.554218.
Train: 2018-08-04T23:57:54.236885: step 8416, loss 0.521066.
Train: 2018-08-04T23:57:54.502448: step 8417, loss 0.570759.
Train: 2018-08-04T23:57:54.752360: step 8418, loss 0.55413.
Train: 2018-08-04T23:57:55.002347: step 8419, loss 0.545775.
Train: 2018-08-04T23:57:55.252243: step 8420, loss 0.562423.
Test: 2018-08-04T23:57:56.501949: step 8420, loss 0.548649.
Train: 2018-08-04T23:57:56.751891: step 8421, loss 0.512272.
Train: 2018-08-04T23:57:56.986242: step 8422, loss 0.654546.
Train: 2018-08-04T23:57:57.236183: step 8423, loss 0.621031.
Train: 2018-08-04T23:57:57.486126: step 8424, loss 0.528962.
Train: 2018-08-04T23:57:57.736036: step 8425, loss 0.57077.
Train: 2018-08-04T23:57:57.986008: step 8426, loss 0.587461.
Train: 2018-08-04T23:57:58.235948: step 8427, loss 0.595763.
Train: 2018-08-04T23:57:58.501482: step 8428, loss 0.512569.
Train: 2018-08-04T23:57:58.751453: step 8429, loss 0.662105.
Train: 2018-08-04T23:57:59.001394: step 8430, loss 0.653516.
Test: 2018-08-04T23:58:00.235450: step 8430, loss 0.548208.
Train: 2018-08-04T23:58:00.501013: step 8431, loss 0.562525.
Train: 2018-08-04T23:58:00.735364: step 8432, loss 0.587149.
Train: 2018-08-04T23:58:01.000897: step 8433, loss 0.587079.
Train: 2018-08-04T23:58:01.250837: step 8434, loss 0.587013.
Train: 2018-08-04T23:58:01.500778: step 8435, loss 0.562745.
Train: 2018-08-04T23:58:01.750751: step 8436, loss 0.57887.
Train: 2018-08-04T23:58:02.000686: step 8437, loss 0.546881.
Train: 2018-08-04T23:58:02.250633: step 8438, loss 0.515124.
Train: 2018-08-04T23:58:02.500578: step 8439, loss 0.586811.
Train: 2018-08-04T23:58:02.750487: step 8440, loss 0.555054.
Test: 2018-08-04T23:58:04.000194: step 8440, loss 0.550331.
Train: 2018-08-04T23:58:04.234545: step 8441, loss 0.555093.
Train: 2018-08-04T23:58:04.484486: step 8442, loss 0.563033.
Train: 2018-08-04T23:58:04.734429: step 8443, loss 0.602582.
Train: 2018-08-04T23:58:04.999992: step 8444, loss 0.531495.
Train: 2018-08-04T23:58:05.265554: step 8445, loss 0.539407.
Train: 2018-08-04T23:58:05.515495: step 8446, loss 0.547284.
Train: 2018-08-04T23:58:05.765437: step 8447, loss 0.515646.
Train: 2018-08-04T23:58:06.015378: step 8448, loss 0.5155.
Train: 2018-08-04T23:58:06.265290: step 8449, loss 0.666275.
Train: 2018-08-04T23:58:06.515261: step 8450, loss 0.547051.
Test: 2018-08-04T23:58:07.764937: step 8450, loss 0.550378.
Train: 2018-08-04T23:58:08.014910: step 8451, loss 0.578863.
Train: 2018-08-04T23:58:08.327305: step 8452, loss 0.618687.
Train: 2018-08-04T23:58:08.577277: step 8453, loss 0.594777.
Train: 2018-08-04T23:58:08.827218: step 8454, loss 0.578857.
Train: 2018-08-04T23:58:09.077162: step 8455, loss 0.555046.
Train: 2018-08-04T23:58:09.280231: step 8456, loss 0.579915.
Train: 2018-08-04T23:58:09.530148: step 8457, loss 0.523413.
Train: 2018-08-04T23:58:09.780122: step 8458, loss 0.578859.
Train: 2018-08-04T23:58:10.030062: step 8459, loss 0.547165.
Train: 2018-08-04T23:58:10.279973: step 8460, loss 0.570948.
Test: 2018-08-04T23:58:11.529680: step 8460, loss 0.549756.
Train: 2018-08-04T23:58:11.764000: step 8461, loss 0.5392.
Train: 2018-08-04T23:58:12.013973: step 8462, loss 0.531211.
Train: 2018-08-04T23:58:12.263915: step 8463, loss 0.57091.
Train: 2018-08-04T23:58:12.513824: step 8464, loss 0.554851.
Train: 2018-08-04T23:58:12.763798: step 8465, loss 0.635005.
Train: 2018-08-04T23:58:13.013740: step 8466, loss 0.650978.
Train: 2018-08-04T23:58:13.263681: step 8467, loss 0.547019.
Train: 2018-08-04T23:58:13.513622: step 8468, loss 0.578859.
Train: 2018-08-04T23:58:13.779153: step 8469, loss 0.555047.
Train: 2018-08-04T23:58:14.013499: step 8470, loss 0.618494.
Test: 2018-08-04T23:58:15.263181: step 8470, loss 0.549628.
Train: 2018-08-04T23:58:15.497533: step 8471, loss 0.563039.
Train: 2018-08-04T23:58:15.747475: step 8472, loss 0.626273.
Train: 2018-08-04T23:58:15.997414: step 8473, loss 0.594553.
Train: 2018-08-04T23:58:16.247350: step 8474, loss 0.539594.
Train: 2018-08-04T23:58:16.512913: step 8475, loss 0.531857.
Train: 2018-08-04T23:58:16.747210: step 8476, loss 0.547509.
Train: 2018-08-04T23:58:16.997180: step 8477, loss 0.571074.
Train: 2018-08-04T23:58:17.247122: step 8478, loss 0.532125.
Train: 2018-08-04T23:58:17.497059: step 8479, loss 0.492674.
Train: 2018-08-04T23:58:17.747005: step 8480, loss 0.523896.
Test: 2018-08-04T23:58:18.996682: step 8480, loss 0.550434.
Train: 2018-08-04T23:58:19.246623: step 8481, loss 0.586799.
Train: 2018-08-04T23:58:19.496565: step 8482, loss 0.594714.
Train: 2018-08-04T23:58:19.746507: step 8483, loss 0.610452.
Train: 2018-08-04T23:58:19.996448: step 8484, loss 0.570972.
Train: 2018-08-04T23:58:20.262042: step 8485, loss 0.570947.
Train: 2018-08-04T23:58:20.511984: step 8486, loss 0.578954.
Train: 2018-08-04T23:58:20.761925: step 8487, loss 0.530983.
Train: 2018-08-04T23:58:21.011866: step 8488, loss 0.627135.
Train: 2018-08-04T23:58:21.261807: step 8489, loss 0.555007.
Train: 2018-08-04T23:58:21.511718: step 8490, loss 0.530981.
Test: 2018-08-04T23:58:22.777047: step 8490, loss 0.549262.
Train: 2018-08-04T23:58:23.011399: step 8491, loss 0.674444.
Train: 2018-08-04T23:58:23.261340: step 8492, loss 0.555028.
Train: 2018-08-04T23:58:23.526878: step 8493, loss 0.594726.
Train: 2018-08-04T23:58:23.776837: step 8494, loss 0.594695.
Train: 2018-08-04T23:58:24.026754: step 8495, loss 0.563125.
Train: 2018-08-04T23:58:24.276726: step 8496, loss 0.563165.
Train: 2018-08-04T23:58:24.526638: step 8497, loss 0.571037.
Train: 2018-08-04T23:58:24.792199: step 8498, loss 0.594539.
Train: 2018-08-04T23:58:25.042172: step 8499, loss 0.6336.
Train: 2018-08-04T23:58:25.292114: step 8500, loss 0.571114.
Test: 2018-08-04T23:58:26.541790: step 8500, loss 0.549864.
Train: 2018-08-04T23:58:27.432240: step 8501, loss 0.547843.
Train: 2018-08-04T23:58:27.682178: step 8502, loss 0.578933.
Train: 2018-08-04T23:58:27.932090: step 8503, loss 0.555698.
Train: 2018-08-04T23:58:28.182032: step 8504, loss 0.602118.
Train: 2018-08-04T23:58:28.432004: step 8505, loss 0.62524.
Train: 2018-08-04T23:58:28.697559: step 8506, loss 0.532821.
Train: 2018-08-04T23:58:28.931856: step 8507, loss 0.494496.
Train: 2018-08-04T23:58:29.197419: step 8508, loss 0.578969.
Train: 2018-08-04T23:58:29.431740: step 8509, loss 0.540536.
Train: 2018-08-04T23:58:29.681679: step 8510, loss 0.486548.
Test: 2018-08-04T23:58:30.931388: step 8510, loss 0.550206.
Train: 2018-08-04T23:58:31.165708: step 8511, loss 0.555762.
Train: 2018-08-04T23:58:31.415678: step 8512, loss 0.516835.
Train: 2018-08-04T23:58:31.665591: step 8513, loss 0.547698.
Train: 2018-08-04T23:58:31.915532: step 8514, loss 0.625931.
Train: 2018-08-04T23:58:32.165503: step 8515, loss 0.547386.
Train: 2018-08-04T23:58:32.415440: step 8516, loss 0.602555.
Train: 2018-08-04T23:58:32.665356: step 8517, loss 0.602618.
Train: 2018-08-04T23:58:32.915298: step 8518, loss 0.547215.
Train: 2018-08-04T23:58:33.165257: step 8519, loss 0.539207.
Train: 2018-08-04T23:58:33.415207: step 8520, loss 0.539125.
Test: 2018-08-04T23:58:34.680508: step 8520, loss 0.548884.
Train: 2018-08-04T23:58:34.914831: step 8521, loss 0.562922.
Train: 2018-08-04T23:58:35.164802: step 8522, loss 0.546914.
Train: 2018-08-04T23:58:35.414742: step 8523, loss 0.490753.
Train: 2018-08-04T23:58:35.664653: step 8524, loss 0.54669.
Train: 2018-08-04T23:58:35.914625: step 8525, loss 0.554644.
Train: 2018-08-04T23:58:36.164561: step 8526, loss 0.570789.
Train: 2018-08-04T23:58:36.414479: step 8527, loss 0.554477.
Train: 2018-08-04T23:58:36.664449: step 8528, loss 0.546246.
Train: 2018-08-04T23:58:36.914391: step 8529, loss 0.554353.
Train: 2018-08-04T23:58:37.164303: step 8530, loss 0.480197.
Test: 2018-08-04T23:58:38.414010: step 8530, loss 0.549425.
Train: 2018-08-04T23:58:38.663981: step 8531, loss 0.587323.
Train: 2018-08-04T23:58:38.913917: step 8532, loss 0.570714.
Train: 2018-08-04T23:58:39.163867: step 8533, loss 0.545709.
Train: 2018-08-04T23:58:39.413775: step 8534, loss 0.595814.
Train: 2018-08-04T23:58:39.663718: step 8535, loss 0.579132.
Train: 2018-08-04T23:58:39.929312: step 8536, loss 0.55404.
Train: 2018-08-04T23:58:40.179251: step 8537, loss 0.562222.
Train: 2018-08-04T23:58:40.429193: step 8538, loss 0.59607.
Train: 2018-08-04T23:58:40.679104: step 8539, loss 0.520126.
Train: 2018-08-04T23:58:40.929076: step 8540, loss 0.536857.
Test: 2018-08-04T23:58:42.178753: step 8540, loss 0.54859.
Train: 2018-08-04T23:58:42.428719: step 8541, loss 0.520454.
Train: 2018-08-04T23:58:42.678661: step 8542, loss 0.596752.
Train: 2018-08-04T23:58:42.928608: step 8543, loss 0.562474.
Train: 2018-08-04T23:58:43.178549: step 8544, loss 0.587976.
Train: 2018-08-04T23:58:43.444081: step 8545, loss 0.50329.
Train: 2018-08-04T23:58:43.678432: step 8546, loss 0.553748.
Train: 2018-08-04T23:58:43.928389: step 8547, loss 0.587856.
Train: 2018-08-04T23:58:44.178315: step 8548, loss 0.554016.
Train: 2018-08-04T23:58:44.428252: step 8549, loss 0.596031.
Train: 2018-08-04T23:58:44.678198: step 8550, loss 0.544997.
Test: 2018-08-04T23:58:45.927875: step 8550, loss 0.548839.
Train: 2018-08-04T23:58:46.193438: step 8551, loss 0.621449.
Train: 2018-08-04T23:58:46.427789: step 8552, loss 0.561788.
Train: 2018-08-04T23:58:46.677730: step 8553, loss 0.477889.
Train: 2018-08-04T23:58:46.927673: step 8554, loss 0.545205.
Train: 2018-08-04T23:58:47.177583: step 8555, loss 0.563134.
Train: 2018-08-04T23:58:47.443170: step 8556, loss 0.578819.
Train: 2018-08-04T23:58:47.693117: step 8557, loss 0.5623.
Train: 2018-08-04T23:58:47.943058: step 8558, loss 0.588707.
Train: 2018-08-04T23:58:48.192969: step 8559, loss 0.561111.
Train: 2018-08-04T23:58:48.442942: step 8560, loss 0.580024.
Test: 2018-08-04T23:58:49.692618: step 8560, loss 0.548024.
Train: 2018-08-04T23:58:49.926938: step 8561, loss 0.528533.
Train: 2018-08-04T23:58:50.176905: step 8562, loss 0.512274.
Train: 2018-08-04T23:58:50.426822: step 8563, loss 0.578804.
Train: 2018-08-04T23:58:50.676793: step 8564, loss 0.520534.
Train: 2018-08-04T23:58:50.926705: step 8565, loss 0.529591.
Train: 2018-08-04T23:58:51.176676: step 8566, loss 0.603914.
Train: 2018-08-04T23:58:51.426618: step 8567, loss 0.596176.
Train: 2018-08-04T23:58:51.676559: step 8568, loss 0.562857.
Train: 2018-08-04T23:58:51.926500: step 8569, loss 0.495315.
Train: 2018-08-04T23:58:52.176413: step 8570, loss 0.529124.
Test: 2018-08-04T23:58:53.426119: step 8570, loss 0.547542.
Train: 2018-08-04T23:58:53.691683: step 8571, loss 0.595791.
Train: 2018-08-04T23:58:53.941654: step 8572, loss 0.529786.
Train: 2018-08-04T23:58:54.207210: step 8573, loss 0.520799.
Train: 2018-08-04T23:58:54.457158: step 8574, loss 0.587411.
Train: 2018-08-04T23:58:54.707100: step 8575, loss 0.629345.
Train: 2018-08-04T23:58:54.957041: step 8576, loss 0.537574.
Train: 2018-08-04T23:58:55.206976: step 8577, loss 0.5708.
Train: 2018-08-04T23:58:55.456919: step 8578, loss 0.55435.
Train: 2018-08-04T23:58:55.706865: step 8579, loss 0.496425.
Train: 2018-08-04T23:58:55.956807: step 8580, loss 0.579096.
Test: 2018-08-04T23:58:57.190863: step 8580, loss 0.548901.
Train: 2018-08-04T23:58:57.440828: step 8581, loss 0.521222.
Train: 2018-08-04T23:58:57.690776: step 8582, loss 0.595579.
Train: 2018-08-04T23:58:57.940687: step 8583, loss 0.59555.
Train: 2018-08-04T23:58:58.206251: step 8584, loss 0.603791.
Train: 2018-08-04T23:58:58.456221: step 8585, loss 0.587248.
Train: 2018-08-04T23:58:58.721755: step 8586, loss 0.554301.
Train: 2018-08-04T23:58:58.956105: step 8587, loss 0.570761.
Train: 2018-08-04T23:58:59.206047: step 8588, loss 0.513351.
Train: 2018-08-04T23:58:59.455987: step 8589, loss 0.603543.
Train: 2018-08-04T23:58:59.705929: step 8590, loss 0.538029.
Test: 2018-08-04T23:59:00.955606: step 8590, loss 0.550055.
Train: 2018-08-04T23:59:01.205578: step 8591, loss 0.513497.
Train: 2018-08-04T23:59:01.455519: step 8592, loss 0.587125.
Train: 2018-08-04T23:59:01.705431: step 8593, loss 0.562593.
Train: 2018-08-04T23:59:01.955403: step 8594, loss 0.529849.
Train: 2018-08-04T23:59:02.205338: step 8595, loss 0.529829.
Train: 2018-08-04T23:59:02.455285: step 8596, loss 0.546152.
Train: 2018-08-04T23:59:02.705227: step 8597, loss 0.57895.
Train: 2018-08-04T23:59:02.955168: step 8598, loss 0.554332.
Train: 2018-08-04T23:59:03.205109: step 8599, loss 0.628381.
Train: 2018-08-04T23:59:03.455022: step 8600, loss 0.529618.
Test: 2018-08-04T23:59:04.704728: step 8600, loss 0.547472.
Train: 2018-08-04T23:59:05.595175: step 8601, loss 0.595394.
Train: 2018-08-04T23:59:05.845086: step 8602, loss 0.521472.
Train: 2018-08-04T23:59:06.095057: step 8603, loss 0.504993.
Train: 2018-08-04T23:59:06.344968: step 8604, loss 0.644815.
Train: 2018-08-04T23:59:06.594911: step 8605, loss 0.521395.
Train: 2018-08-04T23:59:06.844883: step 8606, loss 0.537824.
Train: 2018-08-04T23:59:07.047960: step 8607, loss 0.474638.
Train: 2018-08-04T23:59:07.297901: step 8608, loss 0.545973.
Train: 2018-08-04T23:59:07.547812: step 8609, loss 0.579042.
Train: 2018-08-04T23:59:07.797784: step 8610, loss 0.570765.
Test: 2018-08-04T23:59:09.047461: step 8610, loss 0.548357.
Train: 2018-08-04T23:59:09.281812: step 8611, loss 0.570744.
Train: 2018-08-04T23:59:09.531723: step 8612, loss 0.504116.
Train: 2018-08-04T23:59:09.781689: step 8613, loss 0.612493.
Train: 2018-08-04T23:59:10.031630: step 8614, loss 0.512292.
Train: 2018-08-04T23:59:10.281577: step 8615, loss 0.545654.
Train: 2018-08-04T23:59:10.531519: step 8616, loss 0.570764.
Train: 2018-08-04T23:59:10.781460: step 8617, loss 0.579198.
Train: 2018-08-04T23:59:11.031397: step 8618, loss 0.587607.
Train: 2018-08-04T23:59:11.281343: step 8619, loss 0.604412.
Train: 2018-08-04T23:59:11.531285: step 8620, loss 0.604383.
Test: 2018-08-04T23:59:12.780961: step 8620, loss 0.547083.
Train: 2018-08-04T23:59:13.015284: step 8621, loss 0.562412.
Train: 2018-08-04T23:59:13.265257: step 8622, loss 0.595782.
Train: 2018-08-04T23:59:13.515196: step 8623, loss 0.53748.
Train: 2018-08-04T23:59:13.765137: step 8624, loss 0.562455.
Train: 2018-08-04T23:59:14.015079: step 8625, loss 0.570757.
Train: 2018-08-04T23:59:14.265021: step 8626, loss 0.595561.
Train: 2018-08-04T23:59:14.514931: step 8627, loss 0.554267.
Train: 2018-08-04T23:59:14.764904: step 8628, loss 0.570759.
Train: 2018-08-04T23:59:15.014814: step 8629, loss 0.488702.
Train: 2018-08-04T23:59:15.264786: step 8630, loss 0.554356.
Test: 2018-08-04T23:59:16.514462: step 8630, loss 0.54792.
Train: 2018-08-04T23:59:16.748784: step 8631, loss 0.546157.
Train: 2018-08-04T23:59:16.998756: step 8632, loss 0.496936.
Train: 2018-08-04T23:59:17.248697: step 8633, loss 0.603627.
Train: 2018-08-04T23:59:17.498639: step 8634, loss 0.578979.
Train: 2018-08-04T23:59:17.748580: step 8635, loss 0.55431.
Train: 2018-08-04T23:59:17.998521: step 8636, loss 0.611873.
Train: 2018-08-04T23:59:18.248468: step 8637, loss 0.611836.
Train: 2018-08-04T23:59:18.498410: step 8638, loss 0.628145.
Train: 2018-08-04T23:59:18.748315: step 8639, loss 0.538103.
Train: 2018-08-04T23:59:18.998256: step 8640, loss 0.530048.
Test: 2018-08-04T23:59:20.247963: step 8640, loss 0.549253.
Train: 2018-08-04T23:59:20.482317: step 8641, loss 0.505719.
Train: 2018-08-04T23:59:20.732255: step 8642, loss 0.554516.
Train: 2018-08-04T23:59:20.997787: step 8643, loss 0.554513.
Train: 2018-08-04T23:59:21.232135: step 8644, loss 0.530092.
Train: 2018-08-04T23:59:21.482073: step 8645, loss 0.578925.
Train: 2018-08-04T23:59:21.732021: step 8646, loss 0.570774.
Train: 2018-08-04T23:59:21.981932: step 8647, loss 0.578929.
Train: 2018-08-04T23:59:22.231904: step 8648, loss 0.554464.
Train: 2018-08-04T23:59:22.497436: step 8649, loss 0.497371.
Train: 2018-08-04T23:59:22.731788: step 8650, loss 0.619795.
Test: 2018-08-04T23:59:23.981463: step 8650, loss 0.548386.
Train: 2018-08-04T23:59:24.215815: step 8651, loss 0.578945.
Train: 2018-08-04T23:59:24.465758: step 8652, loss 0.595277.
Train: 2018-08-04T23:59:24.715697: step 8653, loss 0.521811.
Train: 2018-08-04T23:59:24.965639: step 8654, loss 0.538131.
Train: 2018-08-04T23:59:25.215580: step 8655, loss 0.619764.
Train: 2018-08-04T23:59:25.465523: step 8656, loss 0.538138.
Train: 2018-08-04T23:59:25.715463: step 8657, loss 0.570773.
Train: 2018-08-04T23:59:25.965375: step 8658, loss 0.513692.
Train: 2018-08-04T23:59:26.215346: step 8659, loss 0.587096.
Train: 2018-08-04T23:59:26.465289: step 8660, loss 0.587098.
Test: 2018-08-04T23:59:27.714965: step 8660, loss 0.548609.
Train: 2018-08-04T23:59:27.964937: step 8661, loss 0.513652.
Train: 2018-08-04T23:59:28.214848: step 8662, loss 0.472772.
Train: 2018-08-04T23:59:28.480411: step 8663, loss 0.50524.
Train: 2018-08-04T23:59:28.730383: step 8664, loss 0.620095.
Train: 2018-08-04T23:59:28.980324: step 8665, loss 0.570758.
Train: 2018-08-04T23:59:29.230265: step 8666, loss 0.529483.
Train: 2018-08-04T23:59:29.480207: step 8667, loss 0.52939.
Train: 2018-08-04T23:59:29.730151: step 8668, loss 0.504403.
Train: 2018-08-04T23:59:29.980089: step 8669, loss 0.545784.
Train: 2018-08-04T23:59:30.230025: step 8670, loss 0.520632.
Test: 2018-08-04T23:59:31.479708: step 8670, loss 0.548185.
Train: 2018-08-04T23:59:31.729650: step 8671, loss 0.629536.
Train: 2018-08-04T23:59:31.964000: step 8672, loss 0.629648.
Train: 2018-08-04T23:59:32.213941: step 8673, loss 0.654858.
Train: 2018-08-04T23:59:32.479474: step 8674, loss 0.570779.
Train: 2018-08-04T23:59:32.729416: step 8675, loss 0.537307.
Train: 2018-08-04T23:59:32.979358: step 8676, loss 0.554064.
Train: 2018-08-04T23:59:33.229329: step 8677, loss 0.554085.
Train: 2018-08-04T23:59:33.479270: step 8678, loss 0.554104.
Train: 2018-08-04T23:59:33.729182: step 8679, loss 0.520832.
Train: 2018-08-04T23:59:33.979157: step 8680, loss 0.57076.
Test: 2018-08-04T23:59:35.228830: step 8680, loss 0.549125.
Train: 2018-08-04T23:59:35.463152: step 8681, loss 0.545808.
Train: 2018-08-04T23:59:35.713121: step 8682, loss 0.57908.
Train: 2018-08-04T23:59:35.963063: step 8683, loss 0.579073.
Train: 2018-08-04T23:59:36.212975: step 8684, loss 0.520922.
Train: 2018-08-04T23:59:36.462916: step 8685, loss 0.612292.
Train: 2018-08-04T23:59:36.712888: step 8686, loss 0.51269.
Train: 2018-08-04T23:59:36.962830: step 8687, loss 0.603937.
Train: 2018-08-04T23:59:37.212771: step 8688, loss 0.529334.
Train: 2018-08-04T23:59:37.462715: step 8689, loss 0.504495.
Train: 2018-08-04T23:59:37.712623: step 8690, loss 0.579045.
Test: 2018-08-04T23:59:38.962330: step 8690, loss 0.548038.
Train: 2018-08-04T23:59:39.196682: step 8691, loss 0.579052.
Train: 2018-08-04T23:59:39.446622: step 8692, loss 0.512702.
Train: 2018-08-04T23:59:39.696564: step 8693, loss 0.587364.
Train: 2018-08-04T23:59:39.962096: step 8694, loss 0.603973.
Train: 2018-08-04T23:59:40.212068: step 8695, loss 0.545871.
Train: 2018-08-04T23:59:40.493222: step 8696, loss 0.58734.
Train: 2018-08-04T23:59:40.727544: step 8697, loss 0.521061.
Train: 2018-08-04T23:59:40.977514: step 8698, loss 0.612164.
Train: 2018-08-04T23:59:41.227425: step 8699, loss 0.620377.
Train: 2018-08-04T23:59:41.477397: step 8700, loss 0.521269.
Test: 2018-08-04T23:59:42.727074: step 8700, loss 0.549146.
Train: 2018-08-04T23:59:43.601899: step 8701, loss 0.570757.
Train: 2018-08-04T23:59:43.851836: step 8702, loss 0.554313.
Train: 2018-08-04T23:59:44.101783: step 8703, loss 0.513276.
Train: 2018-08-04T23:59:44.351694: step 8704, loss 0.521489.
Train: 2018-08-04T23:59:44.601634: step 8705, loss 0.611857.
Train: 2018-08-04T23:59:44.851576: step 8706, loss 0.636491.
Train: 2018-08-04T23:59:45.117171: step 8707, loss 0.554365.
Train: 2018-08-04T23:59:45.351495: step 8708, loss 0.546213.
Train: 2018-08-04T23:59:45.601431: step 8709, loss 0.619812.
Train: 2018-08-04T23:59:45.851344: step 8710, loss 0.521853.
Test: 2018-08-04T23:59:47.101050: step 8710, loss 0.549222.
Train: 2018-08-04T23:59:47.335371: step 8711, loss 0.546348.
Train: 2018-08-04T23:59:47.585311: step 8712, loss 0.57078.
Train: 2018-08-04T23:59:47.835253: step 8713, loss 0.643956.
Train: 2018-08-04T23:59:48.085195: step 8714, loss 0.546468.
Train: 2018-08-04T23:59:48.335168: step 8715, loss 0.611259.
Train: 2018-08-04T23:59:48.600730: step 8716, loss 0.570815.
Train: 2018-08-04T23:59:48.850673: step 8717, loss 0.611051.
Train: 2018-08-04T23:59:49.100615: step 8718, loss 0.514752.
Train: 2018-08-04T23:59:49.350553: step 8719, loss 0.530875.
Train: 2018-08-04T23:59:49.600495: step 8720, loss 0.570871.
Test: 2018-08-04T23:59:50.850172: step 8720, loss 0.549746.
Train: 2018-08-04T23:59:51.100113: step 8721, loss 0.610793.
Train: 2018-08-04T23:59:51.365708: step 8722, loss 0.562924.
Train: 2018-08-04T23:59:51.631240: step 8723, loss 0.523175.
Train: 2018-08-04T23:59:51.881181: step 8724, loss 0.578859.
Train: 2018-08-04T23:59:52.131153: step 8725, loss 0.555014.
Train: 2018-08-04T23:59:52.381094: step 8726, loss 0.586805.
Train: 2018-08-04T23:59:52.631036: step 8727, loss 0.483554.
Train: 2018-08-04T23:59:52.880947: step 8728, loss 0.554993.
Train: 2018-08-04T23:59:53.130889: step 8729, loss 0.586829.
Train: 2018-08-04T23:59:53.380861: step 8730, loss 0.530991.
Test: 2018-08-04T23:59:54.630537: step 8730, loss 0.55081.
Train: 2018-08-04T23:59:54.880509: step 8731, loss 0.586859.
Train: 2018-08-04T23:59:55.130453: step 8732, loss 0.506821.
Train: 2018-08-04T23:59:55.380361: step 8733, loss 0.594916.
Train: 2018-08-04T23:59:55.630333: step 8734, loss 0.602985.
Train: 2018-08-04T23:59:55.880245: step 8735, loss 0.530596.
Train: 2018-08-04T23:59:56.130216: step 8736, loss 0.554703.
Train: 2018-08-04T23:59:56.395749: step 8737, loss 0.570835.
Train: 2018-08-04T23:59:56.630070: step 8738, loss 0.562738.
Train: 2018-08-04T23:59:56.880011: step 8739, loss 0.57079.
Train: 2018-08-04T23:59:57.129952: step 8740, loss 0.595084.
Test: 2018-08-04T23:59:58.379659: step 8740, loss 0.550109.
Train: 2018-08-04T23:59:58.629631: step 8741, loss 0.522165.
Train: 2018-08-04T23:59:58.879543: step 8742, loss 0.530274.
Train: 2018-08-04T23:59:59.129509: step 8743, loss 0.586932.
Train: 2018-08-04T23:59:59.379449: step 8744, loss 0.546344.
Train: 2018-08-04T23:59:59.629397: step 8745, loss 0.595183.
Train: 2018-08-04T23:59:59.879339: step 8746, loss 0.538073.
Train: 2018-08-05T00:00:00.129281: step 8747, loss 0.554423.
Train: 2018-08-05T00:00:00.379221: step 8748, loss 0.52144.
Train: 2018-08-05T00:00:00.629164: step 8749, loss 0.545449.
Train: 2018-08-05T00:00:00.879104: step 8750, loss 0.579542.
Test: 2018-08-05T00:00:02.128781: step 8750, loss 0.549096.
Train: 2018-08-05T00:00:02.378750: step 8751, loss 0.538214.
Train: 2018-08-05T00:00:02.628689: step 8752, loss 0.545744.
Train: 2018-08-05T00:00:02.878606: step 8753, loss 0.52944.
Train: 2018-08-05T00:00:03.144167: step 8754, loss 0.588317.
Train: 2018-08-05T00:00:03.409761: step 8755, loss 0.529394.
Train: 2018-08-05T00:00:03.644082: step 8756, loss 0.604645.
Train: 2018-08-05T00:00:03.893992: step 8757, loss 0.487042.
Train: 2018-08-05T00:00:04.097070: step 8758, loss 0.687082.
Train: 2018-08-05T00:00:04.347042: step 8759, loss 0.546071.
Train: 2018-08-05T00:00:04.596954: step 8760, loss 0.554308.
Test: 2018-08-05T00:00:05.846660: step 8760, loss 0.548819.
Train: 2018-08-05T00:00:06.096626: step 8761, loss 0.587326.
Train: 2018-08-05T00:00:06.346575: step 8762, loss 0.545994.
Train: 2018-08-05T00:00:06.596515: step 8763, loss 0.545998.
Train: 2018-08-05T00:00:06.846425: step 8764, loss 0.521268.
Train: 2018-08-05T00:00:07.096367: step 8765, loss 0.471719.
Train: 2018-08-05T00:00:07.346339: step 8766, loss 0.620413.
Train: 2018-08-05T00:00:07.596251: step 8767, loss 0.579021.
Train: 2018-08-05T00:00:07.846216: step 8768, loss 0.562456.
Train: 2018-08-05T00:00:08.096133: step 8769, loss 0.521004.
Train: 2018-08-05T00:00:08.392969: step 8770, loss 0.579048.
Test: 2018-08-05T00:00:09.642646: step 8770, loss 0.548769.
Train: 2018-08-05T00:00:09.892587: step 8771, loss 0.63722.
Train: 2018-08-05T00:00:10.142559: step 8772, loss 0.54598.
Train: 2018-08-05T00:00:10.392501: step 8773, loss 0.537633.
Train: 2018-08-05T00:00:10.642442: step 8774, loss 0.504429.
Train: 2018-08-05T00:00:10.892354: step 8775, loss 0.620499.
Train: 2018-08-05T00:00:11.142326: step 8776, loss 0.562406.
Train: 2018-08-05T00:00:11.392261: step 8777, loss 0.570715.
Train: 2018-08-05T00:00:11.642178: step 8778, loss 0.537657.
Train: 2018-08-05T00:00:11.892120: step 8779, loss 0.54591.
Train: 2018-08-05T00:00:12.142062: step 8780, loss 0.587363.
Test: 2018-08-05T00:00:13.391767: step 8780, loss 0.547538.
Train: 2018-08-05T00:00:13.641709: step 8781, loss 0.562469.
Train: 2018-08-05T00:00:13.876031: step 8782, loss 0.595523.
Train: 2018-08-05T00:00:14.141623: step 8783, loss 0.620234.
Train: 2018-08-05T00:00:14.391565: step 8784, loss 0.546059.
Train: 2018-08-05T00:00:14.641506: step 8785, loss 0.529714.
Train: 2018-08-05T00:00:14.891447: step 8786, loss 0.587198.
Train: 2018-08-05T00:00:15.156980: step 8787, loss 0.505278.
Train: 2018-08-05T00:00:15.391331: step 8788, loss 0.578946.
Train: 2018-08-05T00:00:15.641271: step 8789, loss 0.546206.
Train: 2018-08-05T00:00:15.891208: step 8790, loss 0.505347.
Test: 2018-08-05T00:00:17.140890: step 8790, loss 0.549845.
Train: 2018-08-05T00:00:17.390832: step 8791, loss 0.578944.
Train: 2018-08-05T00:00:17.640803: step 8792, loss 0.578944.
Train: 2018-08-05T00:00:17.890716: step 8793, loss 0.554379.
Train: 2018-08-05T00:00:18.140686: step 8794, loss 0.521565.
Train: 2018-08-05T00:00:18.390628: step 8795, loss 0.587177.
Train: 2018-08-05T00:00:18.640569: step 8796, loss 0.587179.
Train: 2018-08-05T00:00:18.906132: step 8797, loss 0.578978.
Train: 2018-08-05T00:00:19.156074: step 8798, loss 0.644613.
Train: 2018-08-05T00:00:19.406015: step 8799, loss 0.546205.
Train: 2018-08-05T00:00:19.655927: step 8800, loss 0.619792.
Test: 2018-08-05T00:00:20.905634: step 8800, loss 0.549967.
Train: 2018-08-05T00:00:21.780430: step 8801, loss 0.562632.
Train: 2018-08-05T00:00:22.030395: step 8802, loss 0.578909.
Train: 2018-08-05T00:00:22.280342: step 8803, loss 0.546502.
Train: 2018-08-05T00:00:22.530283: step 8804, loss 0.489999.
Train: 2018-08-05T00:00:22.780225: step 8805, loss 0.627365.
Train: 2018-08-05T00:00:23.030167: step 8806, loss 0.570817.
Train: 2018-08-05T00:00:23.295729: step 8807, loss 0.490285.
Train: 2018-08-05T00:00:23.545673: step 8808, loss 0.546646.
Train: 2018-08-05T00:00:23.795612: step 8809, loss 0.554674.
Train: 2018-08-05T00:00:24.045554: step 8810, loss 0.627363.
Test: 2018-08-05T00:00:25.295230: step 8810, loss 0.548332.
Train: 2018-08-05T00:00:25.529552: step 8811, loss 0.57887.
Train: 2018-08-05T00:00:25.779522: step 8812, loss 0.586987.
Train: 2018-08-05T00:00:26.029464: step 8813, loss 0.506432.
Train: 2018-08-05T00:00:26.279405: step 8814, loss 0.603029.
Train: 2018-08-05T00:00:26.529347: step 8815, loss 0.667349.
Train: 2018-08-05T00:00:26.779288: step 8816, loss 0.570849.
Train: 2018-08-05T00:00:27.029201: step 8817, loss 0.530898.
Train: 2018-08-05T00:00:27.279172: step 8818, loss 0.594823.
Train: 2018-08-05T00:00:27.529083: step 8819, loss 0.531092.
Train: 2018-08-05T00:00:27.779055: step 8820, loss 0.578855.
Test: 2018-08-05T00:00:29.028731: step 8820, loss 0.548975.
Train: 2018-08-05T00:00:29.278699: step 8821, loss 0.602686.
Train: 2018-08-05T00:00:29.528615: step 8822, loss 0.570935.
Train: 2018-08-05T00:00:29.778586: step 8823, loss 0.570954.
Train: 2018-08-05T00:00:30.028497: step 8824, loss 0.531475.
Train: 2018-08-05T00:00:30.278469: step 8825, loss 0.515718.
Train: 2018-08-05T00:00:30.528405: step 8826, loss 0.594661.
Train: 2018-08-05T00:00:30.778322: step 8827, loss 0.586764.
Train: 2018-08-05T00:00:31.028293: step 8828, loss 0.578862.
Train: 2018-08-05T00:00:31.278205: step 8829, loss 0.48413.
Train: 2018-08-05T00:00:31.528176: step 8830, loss 0.618402.
Test: 2018-08-05T00:00:32.762232: step 8830, loss 0.550009.
Train: 2018-08-05T00:00:33.012200: step 8831, loss 0.563039.
Train: 2018-08-05T00:00:33.277737: step 8832, loss 0.618432.
Train: 2018-08-05T00:00:33.512087: step 8833, loss 0.483965.
Train: 2018-08-05T00:00:33.761999: step 8834, loss 0.531341.
Train: 2018-08-05T00:00:34.011970: step 8835, loss 0.57885.
Train: 2018-08-05T00:00:34.261882: step 8836, loss 0.594784.
Train: 2018-08-05T00:00:34.511852: step 8837, loss 0.626608.
Train: 2018-08-05T00:00:34.761794: step 8838, loss 0.562953.
Train: 2018-08-05T00:00:35.011706: step 8839, loss 0.539094.
Train: 2018-08-05T00:00:35.261677: step 8840, loss 0.586809.
Test: 2018-08-05T00:00:36.511354: step 8840, loss 0.550583.
Train: 2018-08-05T00:00:36.745706: step 8841, loss 0.483421.
Train: 2018-08-05T00:00:36.995647: step 8842, loss 0.531061.
Train: 2018-08-05T00:00:37.245558: step 8843, loss 0.562883.
Train: 2018-08-05T00:00:37.495498: step 8844, loss 0.562871.
Train: 2018-08-05T00:00:37.745472: step 8845, loss 0.554768.
Train: 2018-08-05T00:00:37.995382: step 8846, loss 0.586949.
Train: 2018-08-05T00:00:38.245354: step 8847, loss 0.595018.
Train: 2018-08-05T00:00:38.495296: step 8848, loss 0.562759.
Train: 2018-08-05T00:00:38.745206: step 8849, loss 0.570808.
Train: 2018-08-05T00:00:38.995179: step 8850, loss 0.619264.
Test: 2018-08-05T00:00:40.260477: step 8850, loss 0.550005.
Train: 2018-08-05T00:00:40.510419: step 8851, loss 0.635367.
Train: 2018-08-05T00:00:40.760359: step 8852, loss 0.55473.
Train: 2018-08-05T00:00:41.010326: step 8853, loss 0.578888.
Train: 2018-08-05T00:00:41.260243: step 8854, loss 0.570848.
Train: 2018-08-05T00:00:41.525830: step 8855, loss 0.506838.
Train: 2018-08-05T00:00:41.760158: step 8856, loss 0.610856.
Train: 2018-08-05T00:00:42.010097: step 8857, loss 0.522958.
Train: 2018-08-05T00:00:42.260039: step 8858, loss 0.507006.
Train: 2018-08-05T00:00:42.509985: step 8859, loss 0.594843.
Train: 2018-08-05T00:00:42.759922: step 8860, loss 0.498856.
Test: 2018-08-05T00:00:44.009598: step 8860, loss 0.548509.
Train: 2018-08-05T00:00:44.243920: step 8861, loss 0.570852.
Train: 2018-08-05T00:00:44.493884: step 8862, loss 0.54673.
Train: 2018-08-05T00:00:44.743832: step 8863, loss 0.506393.
Train: 2018-08-05T00:00:45.009395: step 8864, loss 0.538437.
Train: 2018-08-05T00:00:45.259336: step 8865, loss 0.570795.
Train: 2018-08-05T00:00:45.509277: step 8866, loss 0.595161.
Train: 2018-08-05T00:00:45.759188: step 8867, loss 0.513617.
Train: 2018-08-05T00:00:46.024751: step 8868, loss 0.587232.
Train: 2018-08-05T00:00:46.274723: step 8869, loss 0.554402.
Train: 2018-08-05T00:00:46.540286: step 8870, loss 0.537758.
Test: 2018-08-05T00:00:47.789962: step 8870, loss 0.54796.
Train: 2018-08-05T00:00:48.024314: step 8871, loss 0.620275.
Train: 2018-08-05T00:00:48.274255: step 8872, loss 0.686195.
Train: 2018-08-05T00:00:48.539812: step 8873, loss 0.529508.
Train: 2018-08-05T00:00:48.789729: step 8874, loss 0.537825.
Train: 2018-08-05T00:00:49.039672: step 8875, loss 0.554369.
Train: 2018-08-05T00:00:49.289612: step 8876, loss 0.464016.
Train: 2018-08-05T00:00:49.539583: step 8877, loss 0.603786.
Train: 2018-08-05T00:00:49.789525: step 8878, loss 0.628297.
Train: 2018-08-05T00:00:50.039467: step 8879, loss 0.611901.
Train: 2018-08-05T00:00:50.289408: step 8880, loss 0.496897.
Test: 2018-08-05T00:00:51.550595: step 8880, loss 0.546583.
Train: 2018-08-05T00:00:51.784916: step 8881, loss 0.537878.
Train: 2018-08-05T00:00:52.034882: step 8882, loss 0.546201.
Train: 2018-08-05T00:00:52.284800: step 8883, loss 0.595424.
Train: 2018-08-05T00:00:52.534770: step 8884, loss 0.61183.
Train: 2018-08-05T00:00:52.784682: step 8885, loss 0.562446.
Train: 2018-08-05T00:00:53.034623: step 8886, loss 0.513322.
Train: 2018-08-05T00:00:53.284595: step 8887, loss 0.480757.
Train: 2018-08-05T00:00:53.534536: step 8888, loss 0.530234.
Train: 2018-08-05T00:00:53.784477: step 8889, loss 0.554296.
Train: 2018-08-05T00:00:54.034419: step 8890, loss 0.603823.
Test: 2018-08-05T00:00:55.299717: step 8890, loss 0.548521.
Train: 2018-08-05T00:00:55.596554: step 8891, loss 0.661873.
Train: 2018-08-05T00:00:55.862115: step 8892, loss 0.488201.
Train: 2018-08-05T00:00:56.127679: step 8893, loss 0.578991.
Train: 2018-08-05T00:00:56.361999: step 8894, loss 0.529648.
Train: 2018-08-05T00:00:56.611940: step 8895, loss 0.537775.
Train: 2018-08-05T00:00:56.861876: step 8896, loss 0.537734.
Train: 2018-08-05T00:00:57.111823: step 8897, loss 0.554368.
Train: 2018-08-05T00:00:57.361735: step 8898, loss 0.545912.
Train: 2018-08-05T00:00:57.611706: step 8899, loss 0.603864.
Train: 2018-08-05T00:00:57.861649: step 8900, loss 0.603852.
Test: 2018-08-05T00:00:59.111325: step 8900, loss 0.549227.
Train: 2018-08-05T00:00:59.954907: step 8901, loss 0.496256.
Train: 2018-08-05T00:01:00.220441: step 8902, loss 0.587315.
Train: 2018-08-05T00:01:00.470412: step 8903, loss 0.570702.
Train: 2018-08-05T00:01:00.720353: step 8904, loss 0.570863.
Train: 2018-08-05T00:01:00.970264: step 8905, loss 0.545914.
Train: 2018-08-05T00:01:01.220236: step 8906, loss 0.554162.
Train: 2018-08-05T00:01:01.470178: step 8907, loss 0.488045.
Train: 2018-08-05T00:01:01.720120: step 8908, loss 0.512775.
Train: 2018-08-05T00:01:01.923197: step 8909, loss 0.420538.
Train: 2018-08-05T00:01:02.173138: step 8910, loss 0.595724.
Test: 2018-08-05T00:01:03.422815: step 8910, loss 0.547575.
Train: 2018-08-05T00:01:03.657161: step 8911, loss 0.570696.
Train: 2018-08-05T00:01:03.907107: step 8912, loss 0.646537.
Train: 2018-08-05T00:01:04.157019: step 8913, loss 0.545631.
Train: 2018-08-05T00:01:04.406984: step 8914, loss 0.612936.
Train: 2018-08-05T00:01:04.656931: step 8915, loss 0.638395.
Train: 2018-08-05T00:01:04.906873: step 8916, loss 0.520234.
Train: 2018-08-05T00:01:05.156815: step 8917, loss 0.545513.
Train: 2018-08-05T00:01:05.406756: step 8918, loss 0.495117.
Train: 2018-08-05T00:01:05.656697: step 8919, loss 0.553766.
Train: 2018-08-05T00:01:05.906608: step 8920, loss 0.570763.
Test: 2018-08-05T00:01:07.156316: step 8920, loss 0.549067.
Train: 2018-08-05T00:01:07.406291: step 8921, loss 0.570565.
Train: 2018-08-05T00:01:07.656230: step 8922, loss 0.595971.
Train: 2018-08-05T00:01:07.921793: step 8923, loss 0.537158.
Train: 2018-08-05T00:01:08.171704: step 8924, loss 0.630038.
Train: 2018-08-05T00:01:08.421674: step 8925, loss 0.604314.
Train: 2018-08-05T00:01:08.671587: step 8926, loss 0.579527.
Train: 2018-08-05T00:01:08.921558: step 8927, loss 0.570653.
Train: 2018-08-05T00:01:09.187120: step 8928, loss 0.570795.
Train: 2018-08-05T00:01:09.437033: step 8929, loss 0.512907.
Train: 2018-08-05T00:01:09.687003: step 8930, loss 0.504844.
Test: 2018-08-05T00:01:10.936681: step 8930, loss 0.548961.
Train: 2018-08-05T00:01:11.171031: step 8931, loss 0.628408.
Train: 2018-08-05T00:01:11.420943: step 8932, loss 0.562593.
Train: 2018-08-05T00:01:11.670914: step 8933, loss 0.562598.
Train: 2018-08-05T00:01:11.920855: step 8934, loss 0.554393.
Train: 2018-08-05T00:01:12.186418: step 8935, loss 0.505418.
Train: 2018-08-05T00:01:12.436329: step 8936, loss 0.521751.
Train: 2018-08-05T00:01:12.686301: step 8937, loss 0.644373.
Train: 2018-08-05T00:01:12.936242: step 8938, loss 0.480893.
Train: 2018-08-05T00:01:13.186184: step 8939, loss 0.554408.
Train: 2018-08-05T00:01:13.451742: step 8940, loss 0.513445.
Test: 2018-08-05T00:01:14.685803: step 8940, loss 0.548283.
Train: 2018-08-05T00:01:14.920155: step 8941, loss 0.480494.
Train: 2018-08-05T00:01:15.170094: step 8942, loss 0.52957.
Train: 2018-08-05T00:01:15.420007: step 8943, loss 0.537667.
Train: 2018-08-05T00:01:15.685568: step 8944, loss 0.645518.
Train: 2018-08-05T00:01:15.919920: step 8945, loss 0.612369.
Train: 2018-08-05T00:01:16.169861: step 8946, loss 0.645682.
Train: 2018-08-05T00:01:16.419803: step 8947, loss 0.545829.
Train: 2018-08-05T00:01:16.669714: step 8948, loss 0.504357.
Train: 2018-08-05T00:01:16.919684: step 8949, loss 0.537555.
Train: 2018-08-05T00:01:17.169626: step 8950, loss 0.545844.
Test: 2018-08-05T00:01:18.403682: step 8950, loss 0.548373.
Train: 2018-08-05T00:01:18.653648: step 8951, loss 0.512571.
Train: 2018-08-05T00:01:18.903566: step 8952, loss 0.587422.
Train: 2018-08-05T00:01:19.153507: step 8953, loss 0.579105.
Train: 2018-08-05T00:01:19.403478: step 8954, loss 0.595786.
Train: 2018-08-05T00:01:19.669041: step 8955, loss 0.554097.
Train: 2018-08-05T00:01:19.903361: step 8956, loss 0.504124.
Train: 2018-08-05T00:01:20.153302: step 8957, loss 0.487401.
Train: 2018-08-05T00:01:20.418835: step 8958, loss 0.545706.
Train: 2018-08-05T00:01:20.668806: step 8959, loss 0.512153.
Train: 2018-08-05T00:01:20.918748: step 8960, loss 0.537181.
Test: 2018-08-05T00:01:22.168425: step 8960, loss 0.547909.
Train: 2018-08-05T00:01:22.418397: step 8961, loss 0.5708.
Train: 2018-08-05T00:01:22.668338: step 8962, loss 0.520118.
Train: 2018-08-05T00:01:22.918281: step 8963, loss 0.503033.
Train: 2018-08-05T00:01:23.168222: step 8964, loss 0.545338.
Train: 2018-08-05T00:01:23.418163: step 8965, loss 0.519658.
Train: 2018-08-05T00:01:23.668104: step 8966, loss 0.536626.
Train: 2018-08-05T00:01:23.918047: step 8967, loss 0.613951.
Train: 2018-08-05T00:01:24.167958: step 8968, loss 0.545098.
Train: 2018-08-05T00:01:24.417928: step 8969, loss 0.570975.
Train: 2018-08-05T00:01:24.683461: step 8970, loss 0.605574.
Test: 2018-08-05T00:01:25.948789: step 8970, loss 0.546508.
Train: 2018-08-05T00:01:26.183141: step 8971, loss 0.562341.
Train: 2018-08-05T00:01:26.433082: step 8972, loss 0.536412.
Train: 2018-08-05T00:01:26.683018: step 8973, loss 0.614194.
Train: 2018-08-05T00:01:26.948556: step 8974, loss 0.562338.
Train: 2018-08-05T00:01:27.182877: step 8975, loss 0.562336.
Train: 2018-08-05T00:01:27.432817: step 8976, loss 0.484931.
Train: 2018-08-05T00:01:27.682760: step 8977, loss 0.545138.
Train: 2018-08-05T00:01:27.932701: step 8978, loss 0.545138.
Train: 2018-08-05T00:01:28.182672: step 8979, loss 0.519341.
Train: 2018-08-05T00:01:28.448205: step 8980, loss 0.545126.
Test: 2018-08-05T00:01:29.682291: step 8980, loss 0.548153.
Train: 2018-08-05T00:01:29.932258: step 8981, loss 0.648448.
Train: 2018-08-05T00:01:30.182204: step 8982, loss 0.605328.
Train: 2018-08-05T00:01:30.432146: step 8983, loss 0.639529.
Train: 2018-08-05T00:01:30.682081: step 8984, loss 0.570879.
Train: 2018-08-05T00:01:30.932030: step 8985, loss 0.519816.
Train: 2018-08-05T00:01:31.181970: step 8986, loss 0.570831.
Train: 2018-08-05T00:01:31.431917: step 8987, loss 0.562361.
Train: 2018-08-05T00:01:31.697443: step 8988, loss 0.579228.
Train: 2018-08-05T00:01:31.947386: step 8989, loss 0.478388.
Train: 2018-08-05T00:01:32.197358: step 8990, loss 0.554004.
Test: 2018-08-05T00:01:33.447034: step 8990, loss 0.547822.
Train: 2018-08-05T00:01:33.681385: step 8991, loss 0.604303.
Train: 2018-08-05T00:01:33.946941: step 8992, loss 0.545682.
Train: 2018-08-05T00:01:34.181268: step 8993, loss 0.629219.
Train: 2018-08-05T00:01:34.431178: step 8994, loss 0.579086.
Train: 2018-08-05T00:01:34.681150: step 8995, loss 0.562462.
Train: 2018-08-05T00:01:34.931092: step 8996, loss 0.579028.
Train: 2018-08-05T00:01:35.181034: step 8997, loss 0.537777.
Train: 2018-08-05T00:01:35.430974: step 8998, loss 0.529631.
Train: 2018-08-05T00:01:35.680911: step 8999, loss 0.63647.
Train: 2018-08-05T00:01:35.930858: step 9000, loss 0.505254.
Test: 2018-08-05T00:01:37.180534: step 9000, loss 0.549875.
Train: 2018-08-05T00:01:38.024088: step 9001, loss 0.50535.
Train: 2018-08-05T00:01:38.274059: step 9002, loss 0.521701.
Train: 2018-08-05T00:01:38.524001: step 9003, loss 0.529835.
Train: 2018-08-05T00:01:38.773942: step 9004, loss 0.529765.
Train: 2018-08-05T00:01:39.023883: step 9005, loss 0.578977.
Train: 2018-08-05T00:01:39.273820: step 9006, loss 0.578986.
Train: 2018-08-05T00:01:39.523737: step 9007, loss 0.537803.
Train: 2018-08-05T00:01:39.773708: step 9008, loss 0.579007.
Train: 2018-08-05T00:01:40.039270: step 9009, loss 0.587269.
Train: 2018-08-05T00:01:40.273563: step 9010, loss 0.579015.
Test: 2018-08-05T00:01:41.523268: step 9010, loss 0.548906.
Train: 2018-08-05T00:01:41.773240: step 9011, loss 0.512974.
Train: 2018-08-05T00:01:42.023182: step 9012, loss 0.496408.
Train: 2018-08-05T00:01:42.288744: step 9013, loss 0.645264.
Train: 2018-08-05T00:01:42.538685: step 9014, loss 0.487976.
Train: 2018-08-05T00:01:42.788596: step 9015, loss 0.645378.
Train: 2018-08-05T00:01:43.038568: step 9016, loss 0.562471.
Train: 2018-08-05T00:01:43.304125: step 9017, loss 0.570757.
Train: 2018-08-05T00:01:43.538452: step 9018, loss 0.570757.
Train: 2018-08-05T00:01:43.788362: step 9019, loss 0.53769.
Train: 2018-08-05T00:01:44.038304: step 9020, loss 0.587283.
Test: 2018-08-05T00:01:45.288011: step 9020, loss 0.549474.
Train: 2018-08-05T00:01:45.553575: step 9021, loss 0.49646.
Train: 2018-08-05T00:01:45.787895: step 9022, loss 0.595536.
Train: 2018-08-05T00:01:46.037836: step 9023, loss 0.636814.
Train: 2018-08-05T00:01:46.287802: step 9024, loss 0.513076.
Train: 2018-08-05T00:01:46.537719: step 9025, loss 0.578991.
Train: 2018-08-05T00:01:46.787690: step 9026, loss 0.570758.
Train: 2018-08-05T00:01:47.037626: step 9027, loss 0.578974.
Train: 2018-08-05T00:01:47.287568: step 9028, loss 0.595366.
Train: 2018-08-05T00:01:47.537510: step 9029, loss 0.603499.
Train: 2018-08-05T00:01:47.787456: step 9030, loss 0.529977.
Test: 2018-08-05T00:01:49.021512: step 9030, loss 0.548097.
Train: 2018-08-05T00:01:49.271479: step 9031, loss 0.578921.
Train: 2018-08-05T00:01:49.537016: step 9032, loss 0.570785.
Train: 2018-08-05T00:01:49.771367: step 9033, loss 0.587011.
Train: 2018-08-05T00:01:50.021308: step 9034, loss 0.546532.
Train: 2018-08-05T00:01:50.271244: step 9035, loss 0.578886.
Train: 2018-08-05T00:01:50.521161: step 9036, loss 0.506338.
Train: 2018-08-05T00:01:50.771128: step 9037, loss 0.522475.
Train: 2018-08-05T00:01:51.021074: step 9038, loss 0.514369.
Train: 2018-08-05T00:01:51.271015: step 9039, loss 0.506172.
Train: 2018-08-05T00:01:51.520957: step 9040, loss 0.522165.
Test: 2018-08-05T00:01:52.786255: step 9040, loss 0.549245.
Train: 2018-08-05T00:01:53.020576: step 9041, loss 0.570781.
Train: 2018-08-05T00:01:53.270518: step 9042, loss 0.538123.
Train: 2018-08-05T00:01:53.520515: step 9043, loss 0.570764.
Train: 2018-08-05T00:01:53.770430: step 9044, loss 0.595402.
Train: 2018-08-05T00:01:54.020372: step 9045, loss 0.595445.
Train: 2018-08-05T00:01:54.270313: step 9046, loss 0.54605.
Train: 2018-08-05T00:01:54.520255: step 9047, loss 0.611979.
Train: 2018-08-05T00:01:54.770196: step 9048, loss 0.562515.
Train: 2018-08-05T00:01:55.020138: step 9049, loss 0.603712.
Train: 2018-08-05T00:01:55.270080: step 9050, loss 0.562525.
Test: 2018-08-05T00:01:56.519756: step 9050, loss 0.548815.
Train: 2018-08-05T00:01:56.769724: step 9051, loss 0.669379.
Train: 2018-08-05T00:01:57.019640: step 9052, loss 0.578961.
Train: 2018-08-05T00:01:57.269582: step 9053, loss 0.570791.
Train: 2018-08-05T00:01:57.519556: step 9054, loss 0.505772.
Train: 2018-08-05T00:01:57.785115: step 9055, loss 0.587018.
Train: 2018-08-05T00:01:58.035056: step 9056, loss 0.570798.
Train: 2018-08-05T00:01:58.284999: step 9057, loss 0.554644.
Train: 2018-08-05T00:01:58.534940: step 9058, loss 0.643436.
Train: 2018-08-05T00:01:58.784885: step 9059, loss 0.56279.
Train: 2018-08-05T00:01:58.987928: step 9060, loss 0.562833.
Test: 2018-08-05T00:02:00.222014: step 9060, loss 0.550069.
Train: 2018-08-05T00:02:00.456335: step 9061, loss 0.602849.
Train: 2018-08-05T00:02:00.706306: step 9062, loss 0.523072.
Train: 2018-08-05T00:02:00.956248: step 9063, loss 0.539088.
Train: 2018-08-05T00:02:01.206189: step 9064, loss 0.610654.
Train: 2018-08-05T00:02:01.456131: step 9065, loss 0.539194.
Train: 2018-08-05T00:02:01.721664: step 9066, loss 0.594709.
Train: 2018-08-05T00:02:01.971629: step 9067, loss 0.594691.
Train: 2018-08-05T00:02:02.221576: step 9068, loss 0.507777.
Train: 2018-08-05T00:02:02.471517: step 9069, loss 0.563068.
Train: 2018-08-05T00:02:02.721430: step 9070, loss 0.547276.
Test: 2018-08-05T00:02:03.971136: step 9070, loss 0.549137.
Train: 2018-08-05T00:02:04.221077: step 9071, loss 0.65787.
Train: 2018-08-05T00:02:04.486673: step 9072, loss 0.531542.
Train: 2018-08-05T00:02:04.736613: step 9073, loss 0.476411.
Train: 2018-08-05T00:02:04.986554: step 9074, loss 0.586763.
Train: 2018-08-05T00:02:05.252117: step 9075, loss 0.55514.
Train: 2018-08-05T00:02:05.486437: step 9076, loss 0.539267.
Train: 2018-08-05T00:02:05.736379: step 9077, loss 0.658208.
Train: 2018-08-05T00:02:05.986319: step 9078, loss 0.475743.
Train: 2018-08-05T00:02:06.236261: step 9079, loss 0.547068.
Train: 2018-08-05T00:02:06.486197: step 9080, loss 0.562927.
Test: 2018-08-05T00:02:07.735880: step 9080, loss 0.551398.
Train: 2018-08-05T00:02:07.970235: step 9081, loss 0.594831.
Train: 2018-08-05T00:02:08.220142: step 9082, loss 0.562875.
Train: 2018-08-05T00:02:08.516947: step 9083, loss 0.490816.
Train: 2018-08-05T00:02:08.766919: step 9084, loss 0.594927.
Train: 2018-08-05T00:02:09.016861: step 9085, loss 0.538652.
Train: 2018-08-05T00:02:09.282392: step 9086, loss 0.530495.
Train: 2018-08-05T00:02:09.516743: step 9087, loss 0.514181.
Train: 2018-08-05T00:02:09.766655: step 9088, loss 0.546425.
Train: 2018-08-05T00:02:10.016626: step 9089, loss 0.58708.
Train: 2018-08-05T00:02:10.266537: step 9090, loss 0.505356.
Test: 2018-08-05T00:02:11.531865: step 9090, loss 0.549598.
Train: 2018-08-05T00:02:11.766188: step 9091, loss 0.595389.
Train: 2018-08-05T00:02:12.031779: step 9092, loss 0.644843.
Train: 2018-08-05T00:02:12.281692: step 9093, loss 0.595466.
Train: 2018-08-05T00:02:12.531631: step 9094, loss 0.496659.
Train: 2018-08-05T00:02:12.781600: step 9095, loss 0.644943.
Train: 2018-08-05T00:02:13.031545: step 9096, loss 0.628406.
Train: 2018-08-05T00:02:13.281487: step 9097, loss 0.587191.
Train: 2018-08-05T00:02:13.531427: step 9098, loss 0.554377.
Train: 2018-08-05T00:02:13.796990: step 9099, loss 0.55442.
Train: 2018-08-05T00:02:14.062554: step 9100, loss 0.570773.
Test: 2018-08-05T00:02:15.296609: step 9100, loss 0.549224.
Train: 2018-08-05T00:02:16.218299: step 9101, loss 0.521922.
Train: 2018-08-05T00:02:16.468239: step 9102, loss 0.530099.
Train: 2018-08-05T00:02:16.733802: step 9103, loss 0.587054.
Train: 2018-08-05T00:02:16.983714: step 9104, loss 0.546383.
Train: 2018-08-05T00:02:17.233685: step 9105, loss 0.627711.
Train: 2018-08-05T00:02:17.483627: step 9106, loss 0.522067.
Train: 2018-08-05T00:02:17.749159: step 9107, loss 0.627605.
Train: 2018-08-05T00:02:17.983510: step 9108, loss 0.627505.
Train: 2018-08-05T00:02:18.249042: step 9109, loss 0.514284.
Train: 2018-08-05T00:02:18.483401: step 9110, loss 0.52245.
Test: 2018-08-05T00:02:19.733070: step 9110, loss 0.550049.
Train: 2018-08-05T00:02:19.967391: step 9111, loss 0.619166.
Train: 2018-08-05T00:02:20.217363: step 9112, loss 0.562787.
Train: 2018-08-05T00:02:20.467303: step 9113, loss 0.538714.
Train: 2018-08-05T00:02:20.717244: step 9114, loss 0.522689.
Train: 2018-08-05T00:02:20.967186: step 9115, loss 0.562811.
Train: 2018-08-05T00:02:21.217128: step 9116, loss 0.474441.
Train: 2018-08-05T00:02:21.467070: step 9117, loss 0.586932.
Train: 2018-08-05T00:02:21.717011: step 9118, loss 0.554676.
Train: 2018-08-05T00:02:21.966953: step 9119, loss 0.522298.
Train: 2018-08-05T00:02:22.232486: step 9120, loss 0.546474.
Test: 2018-08-05T00:02:23.482193: step 9120, loss 0.551134.
Train: 2018-08-05T00:02:23.732134: step 9121, loss 0.611432.
Train: 2018-08-05T00:02:23.997721: step 9122, loss 0.603344.
Train: 2018-08-05T00:02:24.247669: step 9123, loss 0.546345.
Train: 2018-08-05T00:02:24.497610: step 9124, loss 0.635974.
Train: 2018-08-05T00:02:24.747522: step 9125, loss 0.530074.
Train: 2018-08-05T00:02:24.997493: step 9126, loss 0.56264.
Train: 2018-08-05T00:02:25.247435: step 9127, loss 0.627744.
Train: 2018-08-05T00:02:25.497376: step 9128, loss 0.53829.
Train: 2018-08-05T00:02:25.747318: step 9129, loss 0.562672.
Train: 2018-08-05T00:02:25.997259: step 9130, loss 0.570792.
Test: 2018-08-05T00:02:27.246935: step 9130, loss 0.550094.
Train: 2018-08-05T00:02:27.481287: step 9131, loss 0.497873.
Train: 2018-08-05T00:02:27.731222: step 9132, loss 0.603228.
Train: 2018-08-05T00:02:27.981140: step 9133, loss 0.587007.
Train: 2018-08-05T00:02:28.246732: step 9134, loss 0.489791.
Train: 2018-08-05T00:02:28.512297: step 9135, loss 0.505918.
Train: 2018-08-05T00:02:28.762236: step 9136, loss 0.465116.
Train: 2018-08-05T00:02:29.012147: step 9137, loss 0.505458.
Train: 2018-08-05T00:02:29.262119: step 9138, loss 0.546139.
Train: 2018-08-05T00:02:29.512061: step 9139, loss 0.570756.
Train: 2018-08-05T00:02:29.762003: step 9140, loss 0.579041.
Test: 2018-08-05T00:02:30.996058: step 9140, loss 0.548949.
Train: 2018-08-05T00:02:31.246030: step 9141, loss 0.529199.
Train: 2018-08-05T00:02:31.495972: step 9142, loss 0.63751.
Train: 2018-08-05T00:02:31.745883: step 9143, loss 0.49557.
Train: 2018-08-05T00:02:31.995854: step 9144, loss 0.503751.
Train: 2018-08-05T00:02:32.245796: step 9145, loss 0.604425.
Train: 2018-08-05T00:02:32.495706: step 9146, loss 0.469679.
Train: 2018-08-05T00:02:32.745679: step 9147, loss 0.579276.
Train: 2018-08-05T00:02:32.995621: step 9148, loss 0.596281.
Train: 2018-08-05T00:02:33.245562: step 9149, loss 0.613326.
Train: 2018-08-05T00:02:33.495503: step 9150, loss 0.511359.
Test: 2018-08-05T00:02:34.760801: step 9150, loss 0.549116.
Train: 2018-08-05T00:02:34.995158: step 9151, loss 0.570851.
Train: 2018-08-05T00:02:35.245093: step 9152, loss 0.553832.
Train: 2018-08-05T00:02:35.495034: step 9153, loss 0.528282.
Train: 2018-08-05T00:02:35.744946: step 9154, loss 0.545295.
Train: 2018-08-05T00:02:35.994918: step 9155, loss 0.587933.
Train: 2018-08-05T00:02:36.244860: step 9156, loss 0.587935.
Train: 2018-08-05T00:02:36.494770: step 9157, loss 0.56234.
Train: 2018-08-05T00:02:36.744742: step 9158, loss 0.545303.
Train: 2018-08-05T00:02:36.994684: step 9159, loss 0.604912.
Train: 2018-08-05T00:02:37.244631: step 9160, loss 0.536847.
Test: 2018-08-05T00:02:38.494302: step 9160, loss 0.548756.
Train: 2018-08-05T00:02:38.744243: step 9161, loss 0.494431.
Train: 2018-08-05T00:02:38.994215: step 9162, loss 0.587822.
Train: 2018-08-05T00:02:39.228506: step 9163, loss 0.536889.
Train: 2018-08-05T00:02:39.478477: step 9164, loss 0.511438.
Train: 2018-08-05T00:02:39.744040: step 9165, loss 0.604806.
Train: 2018-08-05T00:02:39.993950: step 9166, loss 0.604789.
Train: 2018-08-05T00:02:40.243894: step 9167, loss 0.545403.
Train: 2018-08-05T00:02:40.509481: step 9168, loss 0.570821.
Train: 2018-08-05T00:02:40.775048: step 9169, loss 0.545455.
Train: 2018-08-05T00:02:41.024985: step 9170, loss 0.604582.
Test: 2018-08-05T00:02:42.274667: step 9170, loss 0.549077.
Train: 2018-08-05T00:02:42.524638: step 9171, loss 0.58765.
Train: 2018-08-05T00:02:42.774580: step 9172, loss 0.520366.
Train: 2018-08-05T00:02:43.040142: step 9173, loss 0.554002.
Train: 2018-08-05T00:02:43.305705: step 9174, loss 0.562399.
Train: 2018-08-05T00:02:43.555649: step 9175, loss 0.537306.
Train: 2018-08-05T00:02:43.821180: step 9176, loss 0.537331.
Train: 2018-08-05T00:02:44.071122: step 9177, loss 0.570771.
Train: 2018-08-05T00:02:44.321092: step 9178, loss 0.579122.
Train: 2018-08-05T00:02:44.571034: step 9179, loss 0.537391.
Train: 2018-08-05T00:02:44.836598: step 9180, loss 0.629145.
Test: 2018-08-05T00:02:46.070653: step 9180, loss 0.547582.
Train: 2018-08-05T00:02:46.336246: step 9181, loss 0.587408.
Train: 2018-08-05T00:02:46.570567: step 9182, loss 0.52925.
Train: 2018-08-05T00:02:46.820514: step 9183, loss 0.603912.
Train: 2018-08-05T00:02:47.070448: step 9184, loss 0.570756.
Train: 2018-08-05T00:02:47.320392: step 9185, loss 0.529519.
Train: 2018-08-05T00:02:47.570332: step 9186, loss 0.554288.
Train: 2018-08-05T00:02:47.820273: step 9187, loss 0.587207.
Train: 2018-08-05T00:02:48.070184: step 9188, loss 0.546131.
Train: 2018-08-05T00:02:48.320126: step 9189, loss 0.595361.
Train: 2018-08-05T00:02:48.570067: step 9190, loss 0.562581.
Test: 2018-08-05T00:02:49.835395: step 9190, loss 0.548955.
Train: 2018-08-05T00:02:50.069748: step 9191, loss 0.53809.
Train: 2018-08-05T00:02:50.335304: step 9192, loss 0.61158.
Train: 2018-08-05T00:02:50.569630: step 9193, loss 0.521908.
Train: 2018-08-05T00:02:50.819570: step 9194, loss 0.587055.
Train: 2018-08-05T00:02:51.085133: step 9195, loss 0.538278.
Train: 2018-08-05T00:02:51.335075: step 9196, loss 0.643879.
Train: 2018-08-05T00:02:51.600608: step 9197, loss 0.514088.
Train: 2018-08-05T00:02:51.850550: step 9198, loss 0.570801.
Train: 2018-08-05T00:02:52.100520: step 9199, loss 0.619309.
Train: 2018-08-05T00:02:52.350462: step 9200, loss 0.530489.
Test: 2018-08-05T00:02:53.600139: step 9200, loss 0.548572.
Train: 2018-08-05T00:02:54.490587: step 9201, loss 0.586933.
Train: 2018-08-05T00:02:54.740499: step 9202, loss 0.538662.
Train: 2018-08-05T00:02:54.990440: step 9203, loss 0.538693.
Train: 2018-08-05T00:02:55.240406: step 9204, loss 0.594941.
Train: 2018-08-05T00:02:55.490352: step 9205, loss 0.514639.
Train: 2018-08-05T00:02:55.740294: step 9206, loss 0.578871.
Train: 2018-08-05T00:02:56.005857: step 9207, loss 0.602976.
Train: 2018-08-05T00:02:56.255792: step 9208, loss 0.57887.
Train: 2018-08-05T00:02:56.505709: step 9209, loss 0.602932.
Train: 2018-08-05T00:02:56.755681: step 9210, loss 0.586872.
Test: 2018-08-05T00:02:58.005358: step 9210, loss 0.549352.
Train: 2018-08-05T00:02:58.255324: step 9211, loss 0.528788.
Train: 2018-08-05T00:02:58.505272: step 9212, loss 0.562894.
Train: 2018-08-05T00:02:58.755213: step 9213, loss 0.538973.
Train: 2018-08-05T00:02:59.005153: step 9214, loss 0.538971.
Train: 2018-08-05T00:02:59.255091: step 9215, loss 0.546926.
Train: 2018-08-05T00:02:59.505037: step 9216, loss 0.658794.
Train: 2018-08-05T00:02:59.770569: step 9217, loss 0.538942.
Train: 2018-08-05T00:03:00.020536: step 9218, loss 0.57088.
Train: 2018-08-05T00:03:00.270478: step 9219, loss 0.57886.
Train: 2018-08-05T00:03:00.520427: step 9220, loss 0.554943.
Test: 2018-08-05T00:03:01.770101: step 9220, loss 0.549425.
Train: 2018-08-05T00:03:02.004452: step 9221, loss 0.562919.
Train: 2018-08-05T00:03:02.254394: step 9222, loss 0.531045.
Train: 2018-08-05T00:03:02.504335: step 9223, loss 0.618737.
Train: 2018-08-05T00:03:02.754276: step 9224, loss 0.554946.
Train: 2018-08-05T00:03:03.004218: step 9225, loss 0.602769.
Train: 2018-08-05T00:03:03.254159: step 9226, loss 0.507204.
Train: 2018-08-05T00:03:03.519691: step 9227, loss 0.578859.
Train: 2018-08-05T00:03:03.754013: step 9228, loss 0.515112.
Train: 2018-08-05T00:03:04.003983: step 9229, loss 0.610783.
Train: 2018-08-05T00:03:04.253925: step 9230, loss 0.62676.
Test: 2018-08-05T00:03:05.503601: step 9230, loss 0.549412.
Train: 2018-08-05T00:03:05.737953: step 9231, loss 0.562912.
Train: 2018-08-05T00:03:05.987895: step 9232, loss 0.546994.
Train: 2018-08-05T00:03:06.253456: step 9233, loss 0.51515.
Train: 2018-08-05T00:03:06.503398: step 9234, loss 0.634661.
Train: 2018-08-05T00:03:06.753342: step 9235, loss 0.618688.
Train: 2018-08-05T00:03:07.003281: step 9236, loss 0.531156.
Train: 2018-08-05T00:03:07.253192: step 9237, loss 0.594746.
Train: 2018-08-05T00:03:07.503165: step 9238, loss 0.539194.
Train: 2018-08-05T00:03:07.753075: step 9239, loss 0.547143.
Train: 2018-08-05T00:03:08.018668: step 9240, loss 0.515422.
Test: 2018-08-05T00:03:09.268345: step 9240, loss 0.549898.
Train: 2018-08-05T00:03:09.502667: step 9241, loss 0.547095.
Train: 2018-08-05T00:03:09.752632: step 9242, loss 0.602724.
Train: 2018-08-05T00:03:10.002579: step 9243, loss 0.562936.
Train: 2018-08-05T00:03:10.252515: step 9244, loss 0.586827.
Train: 2018-08-05T00:03:10.502461: step 9245, loss 0.658558.
Train: 2018-08-05T00:03:10.752403: step 9246, loss 0.554997.
Train: 2018-08-05T00:03:11.002345: step 9247, loss 0.586801.
Train: 2018-08-05T00:03:11.252256: step 9248, loss 0.618499.
Train: 2018-08-05T00:03:11.502227: step 9249, loss 0.460277.
Train: 2018-08-05T00:03:11.752169: step 9250, loss 0.570954.
Test: 2018-08-05T00:03:13.001846: step 9250, loss 0.55002.
Train: 2018-08-05T00:03:13.251818: step 9251, loss 0.586769.
Train: 2018-08-05T00:03:13.501759: step 9252, loss 0.56305.
Train: 2018-08-05T00:03:13.767316: step 9253, loss 0.555148.
Train: 2018-08-05T00:03:14.001643: step 9254, loss 0.515609.
Train: 2018-08-05T00:03:14.251553: step 9255, loss 0.57094.
Train: 2018-08-05T00:03:14.501526: step 9256, loss 0.570929.
Train: 2018-08-05T00:03:14.751467: step 9257, loss 0.578858.
Train: 2018-08-05T00:03:15.001411: step 9258, loss 0.578858.
Train: 2018-08-05T00:03:15.251320: step 9259, loss 0.491414.
Train: 2018-08-05T00:03:15.501291: step 9260, loss 0.539019.
Test: 2018-08-05T00:03:16.766589: step 9260, loss 0.548617.
Train: 2018-08-05T00:03:17.000910: step 9261, loss 0.578862.
Train: 2018-08-05T00:03:17.250881: step 9262, loss 0.538824.
Train: 2018-08-05T00:03:17.500792: step 9263, loss 0.57084.
Train: 2018-08-05T00:03:17.750764: step 9264, loss 0.554731.
Train: 2018-08-05T00:03:18.000706: step 9265, loss 0.554681.
Train: 2018-08-05T00:03:18.250618: step 9266, loss 0.611234.
Train: 2018-08-05T00:03:18.500559: step 9267, loss 0.562706.
Train: 2018-08-05T00:03:18.750526: step 9268, loss 0.562695.
Train: 2018-08-05T00:03:19.000472: step 9269, loss 0.587009.
Train: 2018-08-05T00:03:19.250384: step 9270, loss 0.514021.
Test: 2018-08-05T00:03:20.500090: step 9270, loss 0.547419.
Train: 2018-08-05T00:03:20.734411: step 9271, loss 0.570787.
Train: 2018-08-05T00:03:20.999998: step 9272, loss 0.513871.
Train: 2018-08-05T00:03:21.249915: step 9273, loss 0.587072.
Train: 2018-08-05T00:03:21.499886: step 9274, loss 0.464709.
Train: 2018-08-05T00:03:21.765420: step 9275, loss 0.595327.
Train: 2018-08-05T00:03:22.015390: step 9276, loss 0.570761.
Train: 2018-08-05T00:03:22.265301: step 9277, loss 0.50498.
Train: 2018-08-05T00:03:22.515273: step 9278, loss 0.521275.
Train: 2018-08-05T00:03:22.765216: step 9279, loss 0.554204.
Train: 2018-08-05T00:03:23.015156: step 9280, loss 0.562455.
Test: 2018-08-05T00:03:24.280455: step 9280, loss 0.548335.
Train: 2018-08-05T00:03:24.514777: step 9281, loss 0.554107.
Train: 2018-08-05T00:03:24.780339: step 9282, loss 0.579119.
Train: 2018-08-05T00:03:25.045901: step 9283, loss 0.57914.
Train: 2018-08-05T00:03:25.280253: step 9284, loss 0.570777.
Train: 2018-08-05T00:03:25.530192: step 9285, loss 0.503718.
Train: 2018-08-05T00:03:25.780134: step 9286, loss 0.545591.
Train: 2018-08-05T00:03:26.030071: step 9287, loss 0.562379.
Train: 2018-08-05T00:03:26.280018: step 9288, loss 0.537092.
Train: 2018-08-05T00:03:26.529952: step 9289, loss 0.520156.
Train: 2018-08-05T00:03:26.779869: step 9290, loss 0.570819.
Test: 2018-08-05T00:03:28.045198: step 9290, loss 0.548977.
Train: 2018-08-05T00:03:28.279520: step 9291, loss 0.621687.
Train: 2018-08-05T00:03:28.529490: step 9292, loss 0.570829.
Train: 2018-08-05T00:03:28.779402: step 9293, loss 0.587776.
Train: 2018-08-05T00:03:29.029373: step 9294, loss 0.477704.
Train: 2018-08-05T00:03:29.279315: step 9295, loss 0.587768.
Train: 2018-08-05T00:03:29.529256: step 9296, loss 0.452237.
Train: 2018-08-05T00:03:29.779198: step 9297, loss 0.545371.
Train: 2018-08-05T00:03:30.029109: step 9298, loss 0.587863.
Train: 2018-08-05T00:03:30.279051: step 9299, loss 0.536795.
Train: 2018-08-05T00:03:30.529022: step 9300, loss 0.502652.
Test: 2018-08-05T00:03:31.794319: step 9300, loss 0.549245.
Train: 2018-08-05T00:03:32.637903: step 9301, loss 0.536699.
Train: 2018-08-05T00:03:32.887848: step 9302, loss 0.528072.
Train: 2018-08-05T00:03:33.137786: step 9303, loss 0.63104.
Train: 2018-08-05T00:03:33.387698: step 9304, loss 0.588115.
Train: 2018-08-05T00:03:33.637669: step 9305, loss 0.570926.
Train: 2018-08-05T00:03:33.887610: step 9306, loss 0.527995.
Train: 2018-08-05T00:03:34.137551: step 9307, loss 0.570919.
Train: 2018-08-05T00:03:34.403110: step 9308, loss 0.579492.
Train: 2018-08-05T00:03:34.637406: step 9309, loss 0.536629.
Train: 2018-08-05T00:03:34.887374: step 9310, loss 0.596586.
Test: 2018-08-05T00:03:36.137053: step 9310, loss 0.547273.
Train: 2018-08-05T00:03:36.371404: step 9311, loss 0.579434.
Train: 2018-08-05T00:03:36.621340: step 9312, loss 0.528216.
Train: 2018-08-05T00:03:36.871287: step 9313, loss 0.553822.
Train: 2018-08-05T00:03:37.136849: step 9314, loss 0.468751.
Train: 2018-08-05T00:03:37.371140: step 9315, loss 0.596399.
Train: 2018-08-05T00:03:37.621081: step 9316, loss 0.61341.
Train: 2018-08-05T00:03:37.871023: step 9317, loss 0.587838.
Train: 2018-08-05T00:03:38.120995: step 9318, loss 0.604743.
Train: 2018-08-05T00:03:38.370936: step 9319, loss 0.528555.
Train: 2018-08-05T00:03:38.620847: step 9320, loss 0.478047.
Test: 2018-08-05T00:03:39.870554: step 9320, loss 0.548876.
Train: 2018-08-05T00:03:40.120526: step 9321, loss 0.57923.
Train: 2018-08-05T00:03:40.370467: step 9322, loss 0.553953.
Train: 2018-08-05T00:03:40.620410: step 9323, loss 0.562378.
Train: 2018-08-05T00:03:40.870350: step 9324, loss 0.537157.
Train: 2018-08-05T00:03:41.135883: step 9325, loss 0.553978.
Train: 2018-08-05T00:03:41.385854: step 9326, loss 0.553982.
Train: 2018-08-05T00:03:41.635766: step 9327, loss 0.553985.
Train: 2018-08-05T00:03:41.885737: step 9328, loss 0.469991.
Train: 2018-08-05T00:03:42.151300: step 9329, loss 0.638107.
Train: 2018-08-05T00:03:42.401242: step 9330, loss 0.6465.
Test: 2018-08-05T00:03:43.666539: step 9330, loss 0.54818.
Train: 2018-08-05T00:03:43.900891: step 9331, loss 0.570783.
Train: 2018-08-05T00:03:44.150833: step 9332, loss 0.520537.
Train: 2018-08-05T00:03:44.400774: step 9333, loss 0.520601.
Train: 2018-08-05T00:03:44.650711: step 9334, loss 0.629281.
Train: 2018-08-05T00:03:44.900626: step 9335, loss 0.587451.
Train: 2018-08-05T00:03:45.150598: step 9336, loss 0.612369.
Train: 2018-08-05T00:03:45.416161: step 9337, loss 0.603928.
Train: 2018-08-05T00:03:45.681723: step 9338, loss 0.595531.
Train: 2018-08-05T00:03:45.931635: step 9339, loss 0.521437.
Train: 2018-08-05T00:03:46.165980: step 9340, loss 0.578957.
Test: 2018-08-05T00:03:47.415662: step 9340, loss 0.548589.
Train: 2018-08-05T00:03:47.649984: step 9341, loss 0.489104.
Train: 2018-08-05T00:03:47.899954: step 9342, loss 0.644178.
Train: 2018-08-05T00:03:48.149896: step 9343, loss 0.570783.
Train: 2018-08-05T00:03:48.399837: step 9344, loss 0.489723.
Train: 2018-08-05T00:03:48.649778: step 9345, loss 0.554597.
Train: 2018-08-05T00:03:48.899720: step 9346, loss 0.61937.
Train: 2018-08-05T00:03:49.149633: step 9347, loss 0.538487.
Train: 2018-08-05T00:03:49.399603: step 9348, loss 0.586956.
Train: 2018-08-05T00:03:49.649544: step 9349, loss 0.522459.
Train: 2018-08-05T00:03:49.899486: step 9350, loss 0.586937.
Test: 2018-08-05T00:03:51.144551: step 9350, loss 0.549697.
Train: 2018-08-05T00:03:51.394522: step 9351, loss 0.514464.
Train: 2018-08-05T00:03:51.644434: step 9352, loss 0.63527.
Train: 2018-08-05T00:03:51.894406: step 9353, loss 0.578875.
Train: 2018-08-05T00:03:52.144347: step 9354, loss 0.490487.
Train: 2018-08-05T00:03:52.394289: step 9355, loss 0.554754.
Train: 2018-08-05T00:03:52.644199: step 9356, loss 0.498417.
Train: 2018-08-05T00:03:52.909762: step 9357, loss 0.554688.
Train: 2018-08-05T00:03:53.159703: step 9358, loss 0.651637.
Train: 2018-08-05T00:03:53.409645: step 9359, loss 0.619305.
Train: 2018-08-05T00:03:53.675208: step 9360, loss 0.562739.
Test: 2018-08-05T00:03:54.909293: step 9360, loss 0.548358.
Train: 2018-08-05T00:03:55.143645: step 9361, loss 0.554689.
Train: 2018-08-05T00:03:55.346722: step 9362, loss 0.648714.
Train: 2018-08-05T00:03:55.596633: step 9363, loss 0.635126.
Train: 2018-08-05T00:03:55.846604: step 9364, loss 0.530842.
Train: 2018-08-05T00:03:56.096547: step 9365, loss 0.594823.
Train: 2018-08-05T00:03:56.346457: step 9366, loss 0.483391.
Train: 2018-08-05T00:03:56.596400: step 9367, loss 0.586808.
Train: 2018-08-05T00:03:56.846370: step 9368, loss 0.618558.
Train: 2018-08-05T00:03:57.096282: step 9369, loss 0.555094.
Train: 2018-08-05T00:03:57.346224: step 9370, loss 0.499779.
Test: 2018-08-05T00:03:58.595931: step 9370, loss 0.550742.
Train: 2018-08-05T00:03:58.830281: step 9371, loss 0.531403.
Train: 2018-08-05T00:03:59.080222: step 9372, loss 0.53135.
Train: 2018-08-05T00:03:59.330163: step 9373, loss 0.610593.
Train: 2018-08-05T00:03:59.580076: step 9374, loss 0.555041.
Train: 2018-08-05T00:03:59.830048: step 9375, loss 0.578858.
Train: 2018-08-05T00:04:00.079989: step 9376, loss 0.547051.
Train: 2018-08-05T00:04:00.329901: step 9377, loss 0.578859.
Train: 2018-08-05T00:04:00.579841: step 9378, loss 0.594791.
Train: 2018-08-05T00:04:00.845434: step 9379, loss 0.554961.
Train: 2018-08-05T00:04:01.095378: step 9380, loss 0.562923.
Test: 2018-08-05T00:04:02.360674: step 9380, loss 0.549971.
Train: 2018-08-05T00:04:02.595028: step 9381, loss 0.570888.
Train: 2018-08-05T00:04:02.844966: step 9382, loss 0.57886.
Train: 2018-08-05T00:04:03.094907: step 9383, loss 0.515073.
Train: 2018-08-05T00:04:03.344848: step 9384, loss 0.55491.
Train: 2018-08-05T00:04:03.594793: step 9385, loss 0.522897.
Train: 2018-08-05T00:04:03.844701: step 9386, loss 0.602908.
Train: 2018-08-05T00:04:04.110294: step 9387, loss 0.546773.
Train: 2018-08-05T00:04:04.360235: step 9388, loss 0.562798.
Train: 2018-08-05T00:04:04.610177: step 9389, loss 0.498386.
Train: 2018-08-05T00:04:04.860090: step 9390, loss 0.562739.
Test: 2018-08-05T00:04:06.109795: step 9390, loss 0.549558.
Train: 2018-08-05T00:04:06.344150: step 9391, loss 0.55461.
Train: 2018-08-05T00:04:06.594058: step 9392, loss 0.643837.
Train: 2018-08-05T00:04:06.859621: step 9393, loss 0.554546.
Train: 2018-08-05T00:04:07.109563: step 9394, loss 0.578911.
Train: 2018-08-05T00:04:07.375125: step 9395, loss 0.603295.
Train: 2018-08-05T00:04:07.609476: step 9396, loss 0.58703.
Train: 2018-08-05T00:04:07.859416: step 9397, loss 0.465335.
Train: 2018-08-05T00:04:08.109329: step 9398, loss 0.554543.
Train: 2018-08-05T00:04:08.359299: step 9399, loss 0.538251.
Train: 2018-08-05T00:04:08.624862: step 9400, loss 0.505599.
Test: 2018-08-05T00:04:09.890160: step 9400, loss 0.549704.
Train: 2018-08-05T00:04:10.733743: step 9401, loss 0.611624.
Train: 2018-08-05T00:04:10.983684: step 9402, loss 0.578949.
Train: 2018-08-05T00:04:11.233626: step 9403, loss 0.521619.
Train: 2018-08-05T00:04:11.499159: step 9404, loss 0.562556.
Train: 2018-08-05T00:04:11.733510: step 9405, loss 0.537887.
Train: 2018-08-05T00:04:11.983450: step 9406, loss 0.55429.
Train: 2018-08-05T00:04:12.233398: step 9407, loss 0.554258.
Train: 2018-08-05T00:04:12.483304: step 9408, loss 0.545964.
Train: 2018-08-05T00:04:12.733246: step 9409, loss 0.628717.
Train: 2018-08-05T00:04:12.983187: step 9410, loss 0.512787.
Test: 2018-08-05T00:04:14.232894: step 9410, loss 0.548429.
Train: 2018-08-05T00:04:14.482835: step 9411, loss 0.595631.
Train: 2018-08-05T00:04:14.732806: step 9412, loss 0.587344.
Train: 2018-08-05T00:04:14.982719: step 9413, loss 0.5376.
Train: 2018-08-05T00:04:15.232695: step 9414, loss 0.537597.
Train: 2018-08-05T00:04:15.482602: step 9415, loss 0.620526.
Train: 2018-08-05T00:04:15.748164: step 9416, loss 0.579044.
Train: 2018-08-05T00:04:15.998136: step 9417, loss 0.554203.
Train: 2018-08-05T00:04:16.248071: step 9418, loss 0.51288.
Train: 2018-08-05T00:04:16.513635: step 9419, loss 0.579025.
Train: 2018-08-05T00:04:16.747961: step 9420, loss 0.579022.
Test: 2018-08-05T00:04:17.997636: step 9420, loss 0.549272.
Train: 2018-08-05T00:04:18.231959: step 9421, loss 0.545976.
Train: 2018-08-05T00:04:18.481929: step 9422, loss 0.595527.
Train: 2018-08-05T00:04:18.731870: step 9423, loss 0.521271.
Train: 2018-08-05T00:04:18.981811: step 9424, loss 0.570756.
Train: 2018-08-05T00:04:19.231758: step 9425, loss 0.55427.
Train: 2018-08-05T00:04:19.497287: step 9426, loss 0.636689.
Train: 2018-08-05T00:04:19.747228: step 9427, loss 0.521406.
Train: 2018-08-05T00:04:19.981578: step 9428, loss 0.505012.
Train: 2018-08-05T00:04:20.247111: step 9429, loss 0.480309.
Train: 2018-08-05T00:04:20.497052: step 9430, loss 0.521306.
Test: 2018-08-05T00:04:21.746759: step 9430, loss 0.549636.
Train: 2018-08-05T00:04:21.981080: step 9431, loss 0.603821.
Train: 2018-08-05T00:04:22.231051: step 9432, loss 0.554197.
Train: 2018-08-05T00:04:22.480992: step 9433, loss 0.529292.
Train: 2018-08-05T00:04:22.730934: step 9434, loss 0.612313.
Train: 2018-08-05T00:04:22.980875: step 9435, loss 0.52918.
Train: 2018-08-05T00:04:23.230788: step 9436, loss 0.55411.
Train: 2018-08-05T00:04:23.480758: step 9437, loss 0.570765.
Train: 2018-08-05T00:04:23.746291: step 9438, loss 0.554081.
Train: 2018-08-05T00:04:23.996263: step 9439, loss 0.595817.
Train: 2018-08-05T00:04:24.246174: step 9440, loss 0.512332.
Test: 2018-08-05T00:04:25.495881: step 9440, loss 0.548846.
Train: 2018-08-05T00:04:25.745824: step 9441, loss 0.55406.
Train: 2018-08-05T00:04:25.995764: step 9442, loss 0.570772.
Train: 2018-08-05T00:04:26.245737: step 9443, loss 0.537311.
Train: 2018-08-05T00:04:26.495647: step 9444, loss 0.570776.
Train: 2018-08-05T00:04:26.761209: step 9445, loss 0.503771.
Train: 2018-08-05T00:04:27.011182: step 9446, loss 0.64628.
Train: 2018-08-05T00:04:27.261125: step 9447, loss 0.537246.
Train: 2018-08-05T00:04:27.511034: step 9448, loss 0.554015.
Train: 2018-08-05T00:04:27.761006: step 9449, loss 0.537253.
Train: 2018-08-05T00:04:28.010947: step 9450, loss 0.562396.
Test: 2018-08-05T00:04:29.260625: step 9450, loss 0.548776.
Train: 2018-08-05T00:04:29.494975: step 9451, loss 0.604324.
Train: 2018-08-05T00:04:29.744886: step 9452, loss 0.554022.
Train: 2018-08-05T00:04:30.010479: step 9453, loss 0.587518.
Train: 2018-08-05T00:04:30.244800: step 9454, loss 0.595851.
Train: 2018-08-05T00:04:30.494711: step 9455, loss 0.620818.
Train: 2018-08-05T00:04:30.744652: step 9456, loss 0.628958.
Train: 2018-08-05T00:04:30.994593: step 9457, loss 0.512829.
Train: 2018-08-05T00:04:31.244565: step 9458, loss 0.570756.
Train: 2018-08-05T00:04:31.494506: step 9459, loss 0.496741.
Train: 2018-08-05T00:04:31.744448: step 9460, loss 0.537906.
Test: 2018-08-05T00:04:32.994125: step 9460, loss 0.548655.
Train: 2018-08-05T00:04:33.228476: step 9461, loss 0.562552.
Train: 2018-08-05T00:04:33.478421: step 9462, loss 0.578965.
Train: 2018-08-05T00:04:33.743980: step 9463, loss 0.570763.
Train: 2018-08-05T00:04:33.993892: step 9464, loss 0.644443.
Train: 2018-08-05T00:04:34.259484: step 9465, loss 0.538121.
Train: 2018-08-05T00:04:34.509427: step 9466, loss 0.530047.
Train: 2018-08-05T00:04:34.774989: step 9467, loss 0.611466.
Train: 2018-08-05T00:04:35.009310: step 9468, loss 0.578907.
Train: 2018-08-05T00:04:35.259251: step 9469, loss 0.562694.
Train: 2018-08-05T00:04:35.509192: step 9470, loss 0.530379.
Test: 2018-08-05T00:04:36.758869: step 9470, loss 0.549057.
Train: 2018-08-05T00:04:37.008838: step 9471, loss 0.595041.
Train: 2018-08-05T00:04:37.243131: step 9472, loss 0.554689.
Train: 2018-08-05T00:04:37.493102: step 9473, loss 0.594986.
Train: 2018-08-05T00:04:37.743014: step 9474, loss 0.546714.
Train: 2018-08-05T00:04:37.992979: step 9475, loss 0.538719.
Train: 2018-08-05T00:04:38.242897: step 9476, loss 0.49057.
Train: 2018-08-05T00:04:38.492868: step 9477, loss 0.570834.
Train: 2018-08-05T00:04:38.742810: step 9478, loss 0.562778.
Train: 2018-08-05T00:04:38.992751: step 9479, loss 0.546646.
Train: 2018-08-05T00:04:39.242692: step 9480, loss 0.578884.
Test: 2018-08-05T00:04:40.523611: step 9480, loss 0.548494.
Train: 2018-08-05T00:04:40.757963: step 9481, loss 0.554651.
Train: 2018-08-05T00:04:41.023495: step 9482, loss 0.562715.
Train: 2018-08-05T00:04:41.289058: step 9483, loss 0.603185.
Train: 2018-08-05T00:04:41.538999: step 9484, loss 0.562701.
Train: 2018-08-05T00:04:41.804592: step 9485, loss 0.538408.
Train: 2018-08-05T00:04:42.038913: step 9486, loss 0.587001.
Train: 2018-08-05T00:04:42.288824: step 9487, loss 0.578899.
Train: 2018-08-05T00:04:42.554386: step 9488, loss 0.5951.
Train: 2018-08-05T00:04:42.804329: step 9489, loss 0.530336.
Train: 2018-08-05T00:04:43.054300: step 9490, loss 0.619354.
Test: 2018-08-05T00:04:44.303977: step 9490, loss 0.548859.
Train: 2018-08-05T00:04:44.538298: step 9491, loss 0.60313.
Train: 2018-08-05T00:04:44.788268: step 9492, loss 0.546628.
Train: 2018-08-05T00:04:45.038193: step 9493, loss 0.586927.
Train: 2018-08-05T00:04:45.288152: step 9494, loss 0.474409.
Train: 2018-08-05T00:04:45.538062: step 9495, loss 0.530634.
Train: 2018-08-05T00:04:45.788005: step 9496, loss 0.635229.
Train: 2018-08-05T00:04:46.037976: step 9497, loss 0.554736.
Train: 2018-08-05T00:04:46.287917: step 9498, loss 0.546695.
Train: 2018-08-05T00:04:46.553450: step 9499, loss 0.522544.
Train: 2018-08-05T00:04:46.787771: step 9500, loss 0.586937.
Test: 2018-08-05T00:04:48.037479: step 9500, loss 0.549474.
Train: 2018-08-05T00:04:48.912272: step 9501, loss 0.619196.
Train: 2018-08-05T00:04:49.162244: step 9502, loss 0.538592.
Train: 2018-08-05T00:04:49.412186: step 9503, loss 0.578879.
Train: 2018-08-05T00:04:49.677719: step 9504, loss 0.65137.
Train: 2018-08-05T00:04:49.927660: step 9505, loss 0.602979.
Train: 2018-08-05T00:04:50.177632: step 9506, loss 0.586877.
Train: 2018-08-05T00:04:50.411924: step 9507, loss 0.578861.
Train: 2018-08-05T00:04:50.661893: step 9508, loss 0.491301.
Train: 2018-08-05T00:04:50.911829: step 9509, loss 0.539099.
Train: 2018-08-05T00:04:51.161777: step 9510, loss 0.515253.
Test: 2018-08-05T00:04:52.411453: step 9510, loss 0.549096.
Train: 2018-08-05T00:04:52.661427: step 9511, loss 0.570899.
Train: 2018-08-05T00:04:52.911337: step 9512, loss 0.539021.
Train: 2018-08-05T00:04:53.114413: step 9513, loss 0.426702.
Train: 2018-08-05T00:04:53.380007: step 9514, loss 0.562829.
Train: 2018-08-05T00:04:53.645540: step 9515, loss 0.586933.
Train: 2018-08-05T00:04:53.895511: step 9516, loss 0.55464.
Train: 2018-08-05T00:04:54.145456: step 9517, loss 0.595122.
Train: 2018-08-05T00:04:54.395394: step 9518, loss 0.554527.
Train: 2018-08-05T00:04:54.660951: step 9519, loss 0.546337.
Train: 2018-08-05T00:04:54.895277: step 9520, loss 0.611601.
Test: 2018-08-05T00:04:56.144954: step 9520, loss 0.548942.
Train: 2018-08-05T00:04:56.379276: step 9521, loss 0.521722.
Train: 2018-08-05T00:04:56.644838: step 9522, loss 0.587143.
Train: 2018-08-05T00:04:56.894779: step 9523, loss 0.57896.
Train: 2018-08-05T00:04:57.144721: step 9524, loss 0.537951.
Train: 2018-08-05T00:04:57.394692: step 9525, loss 0.48865.
Train: 2018-08-05T00:04:57.644632: step 9526, loss 0.496666.
Train: 2018-08-05T00:04:57.894545: step 9527, loss 0.570756.
Train: 2018-08-05T00:04:58.144516: step 9528, loss 0.620496.
Train: 2018-08-05T00:04:58.394457: step 9529, loss 0.496045.
Train: 2018-08-05T00:04:58.644369: step 9530, loss 0.512495.
Test: 2018-08-05T00:04:59.909697: step 9530, loss 0.54866.
Train: 2018-08-05T00:05:00.206533: step 9531, loss 0.612532.
Train: 2018-08-05T00:05:00.456474: step 9532, loss 0.579142.
Train: 2018-08-05T00:05:00.706417: step 9533, loss 0.554023.
Train: 2018-08-05T00:05:00.956360: step 9534, loss 0.554008.
Train: 2018-08-05T00:05:01.206270: step 9535, loss 0.503625.
Train: 2018-08-05T00:05:01.456210: step 9536, loss 0.587617.
Train: 2018-08-05T00:05:01.721773: step 9537, loss 0.545531.
Train: 2018-08-05T00:05:01.971749: step 9538, loss 0.56237.
Train: 2018-08-05T00:05:02.221680: step 9539, loss 0.528606.
Train: 2018-08-05T00:05:02.471628: step 9540, loss 0.570813.
Test: 2018-08-05T00:05:03.721305: step 9540, loss 0.548425.
Train: 2018-08-05T00:05:03.955625: step 9541, loss 0.52852.
Train: 2018-08-05T00:05:04.205568: step 9542, loss 0.697895.
Train: 2018-08-05T00:05:04.471159: step 9543, loss 0.57927.
Train: 2018-08-05T00:05:04.736722: step 9544, loss 0.596112.
Train: 2018-08-05T00:05:04.986663: step 9545, loss 0.596024.
Train: 2018-08-05T00:05:05.236576: step 9546, loss 0.486969.
Train: 2018-08-05T00:05:05.486547: step 9547, loss 0.520573.
Train: 2018-08-05T00:05:05.736491: step 9548, loss 0.620936.
Train: 2018-08-05T00:05:05.986401: step 9549, loss 0.520707.
Train: 2018-08-05T00:05:06.236354: step 9550, loss 0.637441.
Test: 2018-08-05T00:05:07.501669: step 9550, loss 0.548759.
Train: 2018-08-05T00:05:07.751641: step 9551, loss 0.554137.
Train: 2018-08-05T00:05:08.001583: step 9552, loss 0.512715.
Train: 2018-08-05T00:05:08.267146: step 9553, loss 0.57904.
Train: 2018-08-05T00:05:08.517057: step 9554, loss 0.562486.
Train: 2018-08-05T00:05:08.751402: step 9555, loss 0.512943.
Train: 2018-08-05T00:05:09.016970: step 9556, loss 0.562499.
Train: 2018-08-05T00:05:09.266911: step 9557, loss 0.537734.
Train: 2018-08-05T00:05:09.516824: step 9558, loss 0.545982.
Train: 2018-08-05T00:05:09.766795: step 9559, loss 0.620332.
Train: 2018-08-05T00:05:10.016706: step 9560, loss 0.562501.
Test: 2018-08-05T00:05:11.282034: step 9560, loss 0.548732.
Train: 2018-08-05T00:05:11.516385: step 9561, loss 0.521265.
Train: 2018-08-05T00:05:11.766326: step 9562, loss 0.50476.
Train: 2018-08-05T00:05:12.016238: step 9563, loss 0.595539.
Train: 2018-08-05T00:05:12.266209: step 9564, loss 0.545964.
Train: 2018-08-05T00:05:12.516120: step 9565, loss 0.620375.
Train: 2018-08-05T00:05:12.766062: step 9566, loss 0.537703.
Train: 2018-08-05T00:05:13.016033: step 9567, loss 0.537709.
Train: 2018-08-05T00:05:13.265975: step 9568, loss 0.521171.
Train: 2018-08-05T00:05:13.515916: step 9569, loss 0.636947.
Train: 2018-08-05T00:05:13.765828: step 9570, loss 0.579024.
Test: 2018-08-05T00:05:15.015535: step 9570, loss 0.547945.
Train: 2018-08-05T00:05:15.265477: step 9571, loss 0.595532.
Train: 2018-08-05T00:05:15.499827: step 9572, loss 0.603731.
Train: 2018-08-05T00:05:15.765361: step 9573, loss 0.529649.
Train: 2018-08-05T00:05:16.015302: step 9574, loss 0.537927.
Train: 2018-08-05T00:05:16.265273: step 9575, loss 0.464157.
Train: 2018-08-05T00:05:16.530835: step 9576, loss 0.480432.
Train: 2018-08-05T00:05:16.780777: step 9577, loss 0.56252.
Train: 2018-08-05T00:05:17.030713: step 9578, loss 0.570756.
Train: 2018-08-05T00:05:17.280630: step 9579, loss 0.579034.
Train: 2018-08-05T00:05:17.530602: step 9580, loss 0.587337.
Test: 2018-08-05T00:05:18.780278: step 9580, loss 0.548035.
Train: 2018-08-05T00:05:19.030250: step 9581, loss 0.628829.
Train: 2018-08-05T00:05:19.264560: step 9582, loss 0.562469.
Train: 2018-08-05T00:05:19.514512: step 9583, loss 0.603878.
Train: 2018-08-05T00:05:19.795677: step 9584, loss 0.545961.
Train: 2018-08-05T00:05:20.030016: step 9585, loss 0.48822.
Train: 2018-08-05T00:05:20.279954: step 9586, loss 0.471673.
Train: 2018-08-05T00:05:20.529917: step 9587, loss 0.686626.
Train: 2018-08-05T00:05:20.779810: step 9588, loss 0.554216.
Train: 2018-08-05T00:05:21.029782: step 9589, loss 0.496368.
Train: 2018-08-05T00:05:21.279694: step 9590, loss 0.545938.
Test: 2018-08-05T00:05:22.545022: step 9590, loss 0.54788.
Train: 2018-08-05T00:05:22.779343: step 9591, loss 0.637014.
Train: 2018-08-05T00:05:23.029314: step 9592, loss 0.529377.
Train: 2018-08-05T00:05:23.279255: step 9593, loss 0.562481.
Train: 2018-08-05T00:05:23.529196: step 9594, loss 0.603857.
Train: 2018-08-05T00:05:23.779107: step 9595, loss 0.587289.
Train: 2018-08-05T00:05:24.029080: step 9596, loss 0.545997.
Train: 2018-08-05T00:05:24.279021: step 9597, loss 0.529538.
Train: 2018-08-05T00:05:24.528962: step 9598, loss 0.59548.
Train: 2018-08-05T00:05:24.778873: step 9599, loss 0.562526.
Train: 2018-08-05T00:05:25.028845: step 9600, loss 0.496748.
Test: 2018-08-05T00:05:26.278522: step 9600, loss 0.548789.
Train: 2018-08-05T00:05:27.137697: step 9601, loss 0.595441.
Train: 2018-08-05T00:05:27.403289: step 9602, loss 0.554308.
Train: 2018-08-05T00:05:27.637580: step 9603, loss 0.554311.
Train: 2018-08-05T00:05:27.887551: step 9604, loss 0.587206.
Train: 2018-08-05T00:05:28.153116: step 9605, loss 0.603635.
Train: 2018-08-05T00:05:28.387434: step 9606, loss 0.529728.
Train: 2018-08-05T00:05:28.637375: step 9607, loss 0.537957.
Train: 2018-08-05T00:05:28.887316: step 9608, loss 0.529758.
Train: 2018-08-05T00:05:29.137258: step 9609, loss 0.611792.
Train: 2018-08-05T00:05:29.387169: step 9610, loss 0.529753.
Test: 2018-08-05T00:05:30.636877: step 9610, loss 0.548857.
Train: 2018-08-05T00:05:30.871223: step 9611, loss 0.587169.
Train: 2018-08-05T00:05:31.121168: step 9612, loss 0.496958.
Train: 2018-08-05T00:05:31.371113: step 9613, loss 0.57897.
Train: 2018-08-05T00:05:31.621022: step 9614, loss 0.55433.
Train: 2018-08-05T00:05:31.870987: step 9615, loss 0.521437.
Train: 2018-08-05T00:05:32.120904: step 9616, loss 0.480196.
Train: 2018-08-05T00:05:32.370880: step 9617, loss 0.686384.
Train: 2018-08-05T00:05:32.620789: step 9618, loss 0.529461.
Train: 2018-08-05T00:05:32.870760: step 9619, loss 0.537698.
Train: 2018-08-05T00:05:33.120700: step 9620, loss 0.57903.
Test: 2018-08-05T00:05:34.370377: step 9620, loss 0.548653.
Train: 2018-08-05T00:05:34.620319: step 9621, loss 0.612148.
Train: 2018-08-05T00:05:34.854641: step 9622, loss 0.620391.
Train: 2018-08-05T00:05:35.104580: step 9623, loss 0.554245.
Train: 2018-08-05T00:05:35.354552: step 9624, loss 0.53779.
Train: 2018-08-05T00:05:35.604488: step 9625, loss 0.554291.
Train: 2018-08-05T00:05:35.870057: step 9626, loss 0.578985.
Train: 2018-08-05T00:05:36.104348: step 9627, loss 0.521453.
Train: 2018-08-05T00:05:36.354318: step 9628, loss 0.59541.
Train: 2018-08-05T00:05:36.604260: step 9629, loss 0.554341.
Train: 2018-08-05T00:05:36.869793: step 9630, loss 0.603578.
Test: 2018-08-05T00:05:38.135121: step 9630, loss 0.550212.
Train: 2018-08-05T00:05:38.369472: step 9631, loss 0.554381.
Train: 2018-08-05T00:05:38.619413: step 9632, loss 0.587129.
Train: 2018-08-05T00:05:38.869355: step 9633, loss 0.538101.
Train: 2018-08-05T00:05:39.134886: step 9634, loss 0.60341.
Train: 2018-08-05T00:05:39.384829: step 9635, loss 0.603355.
Train: 2018-08-05T00:05:39.634769: step 9636, loss 0.505799.
Train: 2018-08-05T00:05:39.884742: step 9637, loss 0.611366.
Train: 2018-08-05T00:05:40.134683: step 9638, loss 0.554599.
Train: 2018-08-05T00:05:40.384594: step 9639, loss 0.57889.
Train: 2018-08-05T00:05:40.634567: step 9640, loss 0.530453.
Test: 2018-08-05T00:05:41.884243: step 9640, loss 0.549651.
Train: 2018-08-05T00:05:42.134184: step 9641, loss 0.578882.
Train: 2018-08-05T00:05:42.384156: step 9642, loss 0.586936.
Train: 2018-08-05T00:05:42.634097: step 9643, loss 0.5306.
Train: 2018-08-05T00:05:42.884010: step 9644, loss 0.562789.
Train: 2018-08-05T00:05:43.133981: step 9645, loss 0.554755.
Train: 2018-08-05T00:05:43.383923: step 9646, loss 0.586912.
Train: 2018-08-05T00:05:43.633834: step 9647, loss 0.522629.
Train: 2018-08-05T00:05:43.883774: step 9648, loss 0.570834.
Train: 2018-08-05T00:05:44.133715: step 9649, loss 0.578874.
Train: 2018-08-05T00:05:44.383689: step 9650, loss 0.619085.
Test: 2018-08-05T00:05:45.633365: step 9650, loss 0.549022.
Train: 2018-08-05T00:05:45.883308: step 9651, loss 0.578871.
Train: 2018-08-05T00:05:46.133278: step 9652, loss 0.522723.
Train: 2018-08-05T00:05:46.383215: step 9653, loss 0.586886.
Train: 2018-08-05T00:05:46.633161: step 9654, loss 0.610918.
Train: 2018-08-05T00:05:46.898707: step 9655, loss 0.514868.
Train: 2018-08-05T00:05:47.148666: step 9656, loss 0.53088.
Train: 2018-08-05T00:05:47.398607: step 9657, loss 0.490841.
Train: 2018-08-05T00:05:47.648548: step 9658, loss 0.59491.
Train: 2018-08-05T00:05:47.898459: step 9659, loss 0.538705.
Train: 2018-08-05T00:05:48.148431: step 9660, loss 0.578876.
Test: 2018-08-05T00:05:49.398108: step 9660, loss 0.549297.
Train: 2018-08-05T00:05:49.632461: step 9661, loss 0.514395.
Train: 2018-08-05T00:05:49.882371: step 9662, loss 0.578888.
Train: 2018-08-05T00:05:50.132338: step 9663, loss 0.586992.
Train: 2018-08-05T00:05:50.335390: step 9664, loss 0.614568.
Train: 2018-08-05T00:05:50.585360: step 9665, loss 0.554584.
Train: 2018-08-05T00:05:50.835302: step 9666, loss 0.538369.
Train: 2018-08-05T00:05:51.085214: step 9667, loss 0.570791.
Train: 2018-08-05T00:05:51.335155: step 9668, loss 0.554558.
Train: 2018-08-05T00:05:51.585126: step 9669, loss 0.489577.
Train: 2018-08-05T00:05:51.835067: step 9670, loss 0.54636.
Test: 2018-08-05T00:05:53.084745: step 9670, loss 0.549736.
Train: 2018-08-05T00:05:53.319097: step 9671, loss 0.489173.
Train: 2018-08-05T00:05:53.569039: step 9672, loss 0.570764.
Train: 2018-08-05T00:05:53.818980: step 9673, loss 0.570759.
Train: 2018-08-05T00:05:54.084512: step 9674, loss 0.554274.
Train: 2018-08-05T00:05:54.318833: step 9675, loss 0.545968.
Train: 2018-08-05T00:05:54.568803: step 9676, loss 0.579042.
Train: 2018-08-05T00:05:54.818715: step 9677, loss 0.545858.
Train: 2018-08-05T00:05:55.068687: step 9678, loss 0.562444.
Train: 2018-08-05T00:05:55.318627: step 9679, loss 0.495785.
Train: 2018-08-05T00:05:55.568569: step 9680, loss 0.587481.
Test: 2018-08-05T00:05:56.818245: step 9680, loss 0.549387.
Train: 2018-08-05T00:05:57.052593: step 9681, loss 0.545661.
Train: 2018-08-05T00:05:57.318130: step 9682, loss 0.520455.
Train: 2018-08-05T00:05:57.552449: step 9683, loss 0.537154.
Train: 2018-08-05T00:05:57.802421: step 9684, loss 0.511779.
Train: 2018-08-05T00:05:58.052362: step 9685, loss 0.553898.
Train: 2018-08-05T00:05:58.302306: step 9686, loss 0.579321.
Train: 2018-08-05T00:05:58.552216: step 9687, loss 0.613367.
Train: 2018-08-05T00:05:58.802187: step 9688, loss 0.562343.
Train: 2018-08-05T00:05:59.052128: step 9689, loss 0.579366.
Train: 2018-08-05T00:05:59.302069: step 9690, loss 0.562343.
Test: 2018-08-05T00:06:00.551747: step 9690, loss 0.547748.
Train: 2018-08-05T00:06:00.801720: step 9691, loss 0.545333.
Train: 2018-08-05T00:06:01.036039: step 9692, loss 0.562345.
Train: 2018-08-05T00:06:01.285979: step 9693, loss 0.536848.
Train: 2018-08-05T00:06:01.535922: step 9694, loss 0.53685.
Train: 2018-08-05T00:06:01.785834: step 9695, loss 0.553844.
Train: 2018-08-05T00:06:02.035774: step 9696, loss 0.613364.
Train: 2018-08-05T00:06:02.285746: step 9697, loss 0.579335.
Train: 2018-08-05T00:06:02.535687: step 9698, loss 0.570832.
Train: 2018-08-05T00:06:02.785623: step 9699, loss 0.570821.
Train: 2018-08-05T00:06:03.035570: step 9700, loss 0.528569.
Test: 2018-08-05T00:06:04.269626: step 9700, loss 0.548081.
Train: 2018-08-05T00:06:05.097587: step 9701, loss 0.562367.
Train: 2018-08-05T00:06:05.363150: step 9702, loss 0.612935.
Train: 2018-08-05T00:06:05.613092: step 9703, loss 0.528757.
Train: 2018-08-05T00:06:05.863033: step 9704, loss 0.537212.
Train: 2018-08-05T00:06:06.112945: step 9705, loss 0.579164.
Train: 2018-08-05T00:06:06.362917: step 9706, loss 0.579147.
Train: 2018-08-05T00:06:06.612857: step 9707, loss 0.570771.
Train: 2018-08-05T00:06:06.862799: step 9708, loss 0.554085.
Train: 2018-08-05T00:06:07.112736: step 9709, loss 0.545784.
Train: 2018-08-05T00:06:07.362682: step 9710, loss 0.554129.
Test: 2018-08-05T00:06:08.612359: step 9710, loss 0.54877.
Train: 2018-08-05T00:06:08.846710: step 9711, loss 0.51261.
Train: 2018-08-05T00:06:09.159107: step 9712, loss 0.579068.
Train: 2018-08-05T00:06:09.409078: step 9713, loss 0.53754.
Train: 2018-08-05T00:06:09.658989: step 9714, loss 0.545842.
Train: 2018-08-05T00:06:09.924552: step 9715, loss 0.520906.
Train: 2018-08-05T00:06:10.158873: step 9716, loss 0.5874.
Train: 2018-08-05T00:06:10.408814: step 9717, loss 0.570762.
Train: 2018-08-05T00:06:10.658755: step 9718, loss 0.529147.
Train: 2018-08-05T00:06:10.908727: step 9719, loss 0.529118.
Train: 2018-08-05T00:06:11.158668: step 9720, loss 0.545747.
Test: 2018-08-05T00:06:12.408345: step 9720, loss 0.548085.
Train: 2018-08-05T00:06:12.642696: step 9721, loss 0.554066.
Train: 2018-08-05T00:06:12.892637: step 9722, loss 0.512232.
Train: 2018-08-05T00:06:13.142578: step 9723, loss 0.537253.
Train: 2018-08-05T00:06:13.392520: step 9724, loss 0.612795.
Train: 2018-08-05T00:06:13.642461: step 9725, loss 0.612836.
Train: 2018-08-05T00:06:13.892403: step 9726, loss 0.579194.
Train: 2018-08-05T00:06:14.142315: step 9727, loss 0.537198.
Train: 2018-08-05T00:06:14.392256: step 9728, loss 0.595962.
Train: 2018-08-05T00:06:14.642227: step 9729, loss 0.554015.
Train: 2018-08-05T00:06:14.892169: step 9730, loss 0.520541.
Test: 2018-08-05T00:06:16.141845: step 9730, loss 0.54881.
Train: 2018-08-05T00:06:16.376196: step 9731, loss 0.49544.
Train: 2018-08-05T00:06:16.626137: step 9732, loss 0.554018.
Train: 2018-08-05T00:06:16.876082: step 9733, loss 0.579172.
Train: 2018-08-05T00:06:17.126021: step 9734, loss 0.671507.
Train: 2018-08-05T00:06:17.391578: step 9735, loss 0.595903.
Train: 2018-08-05T00:06:17.641495: step 9736, loss 0.579121.
Train: 2018-08-05T00:06:17.907082: step 9737, loss 0.520805.
Train: 2018-08-05T00:06:18.141408: step 9738, loss 0.587381.
Train: 2018-08-05T00:06:18.391345: step 9739, loss 0.562467.
Train: 2018-08-05T00:06:18.641285: step 9740, loss 0.579028.
Test: 2018-08-05T00:06:19.890968: step 9740, loss 0.548725.
Train: 2018-08-05T00:06:20.140910: step 9741, loss 0.579007.
Train: 2018-08-05T00:06:20.375260: step 9742, loss 0.513156.
Train: 2018-08-05T00:06:20.625201: step 9743, loss 0.505015.
Train: 2018-08-05T00:06:20.875137: step 9744, loss 0.546102.
Train: 2018-08-05T00:06:21.125053: step 9745, loss 0.554314.
Train: 2018-08-05T00:06:21.390656: step 9746, loss 0.51317.
Train: 2018-08-05T00:06:21.640589: step 9747, loss 0.554277.
Train: 2018-08-05T00:06:21.890499: step 9748, loss 0.587262.
Train: 2018-08-05T00:06:22.140471: step 9749, loss 0.537719.
Train: 2018-08-05T00:06:22.390413: step 9750, loss 0.554218.
Test: 2018-08-05T00:06:23.640090: step 9750, loss 0.548079.
Train: 2018-08-05T00:06:23.874442: step 9751, loss 0.595594.
Train: 2018-08-05T00:06:24.124351: step 9752, loss 0.579038.
Train: 2018-08-05T00:06:24.374324: step 9753, loss 0.496247.
Train: 2018-08-05T00:06:24.624234: step 9754, loss 0.579046.
Train: 2018-08-05T00:06:24.874177: step 9755, loss 0.512699.
Train: 2018-08-05T00:06:25.124148: step 9756, loss 0.587375.
Train: 2018-08-05T00:06:25.374089: step 9757, loss 0.545818.
Train: 2018-08-05T00:06:25.624031: step 9758, loss 0.56244.
Train: 2018-08-05T00:06:25.873972: step 9759, loss 0.562435.
Train: 2018-08-05T00:06:26.123883: step 9760, loss 0.437422.
Test: 2018-08-05T00:06:27.357969: step 9760, loss 0.549025.
Train: 2018-08-05T00:06:27.607935: step 9761, loss 0.487161.
Train: 2018-08-05T00:06:27.857854: step 9762, loss 0.679966.
Train: 2018-08-05T00:06:28.107824: step 9763, loss 0.49511.
Train: 2018-08-05T00:06:28.357735: step 9764, loss 0.57923.
Train: 2018-08-05T00:06:28.607676: step 9765, loss 0.646799.
Train: 2018-08-05T00:06:28.857619: step 9766, loss 0.520173.
Train: 2018-08-05T00:06:29.107590: step 9767, loss 0.579248.
Train: 2018-08-05T00:06:29.373152: step 9768, loss 0.579244.
Train: 2018-08-05T00:06:29.607444: step 9769, loss 0.56237.
Train: 2018-08-05T00:06:29.857387: step 9770, loss 0.562374.
Test: 2018-08-05T00:06:31.107091: step 9770, loss 0.548517.
Train: 2018-08-05T00:06:31.341413: step 9771, loss 0.5203.
Train: 2018-08-05T00:06:31.591383: step 9772, loss 0.621281.
Train: 2018-08-05T00:06:31.856916: step 9773, loss 0.511977.
Train: 2018-08-05T00:06:32.091266: step 9774, loss 0.528797.
Train: 2018-08-05T00:06:32.341178: step 9775, loss 0.570786.
Train: 2018-08-05T00:06:32.606740: step 9776, loss 0.595979.
Train: 2018-08-05T00:06:32.856712: step 9777, loss 0.579171.
Train: 2018-08-05T00:06:33.106661: step 9778, loss 0.554024.
Train: 2018-08-05T00:06:33.372185: step 9779, loss 0.545675.
Train: 2018-08-05T00:06:33.622157: step 9780, loss 0.57913.
Test: 2018-08-05T00:06:34.887456: step 9780, loss 0.548479.
Train: 2018-08-05T00:06:35.121776: step 9781, loss 0.503985.
Train: 2018-08-05T00:06:35.371749: step 9782, loss 0.620856.
Train: 2018-08-05T00:06:35.621689: step 9783, loss 0.587437.
Train: 2018-08-05T00:06:35.871631: step 9784, loss 0.512523.
Train: 2018-08-05T00:06:36.121572: step 9785, loss 0.604015.
Train: 2018-08-05T00:06:36.371513: step 9786, loss 0.520962.
Train: 2018-08-05T00:06:36.637076: step 9787, loss 0.612227.
Train: 2018-08-05T00:06:36.871367: step 9788, loss 0.487972.
Train: 2018-08-05T00:06:37.121338: step 9789, loss 0.628704.
Train: 2018-08-05T00:06:37.386871: step 9790, loss 0.570756.
Test: 2018-08-05T00:06:38.636578: step 9790, loss 0.547777.
Train: 2018-08-05T00:06:38.870929: step 9791, loss 0.562506.
Train: 2018-08-05T00:06:39.120870: step 9792, loss 0.595469.
Train: 2018-08-05T00:06:39.370781: step 9793, loss 0.56254.
Train: 2018-08-05T00:06:39.636343: step 9794, loss 0.652778.
Train: 2018-08-05T00:06:39.886286: step 9795, loss 0.57077.
Train: 2018-08-05T00:06:40.136257: step 9796, loss 0.619606.
Train: 2018-08-05T00:06:40.386198: step 9797, loss 0.595094.
Train: 2018-08-05T00:06:40.636140: step 9798, loss 0.554706.
Train: 2018-08-05T00:06:40.901705: step 9799, loss 0.627005.
Train: 2018-08-05T00:06:41.151648: step 9800, loss 0.642698.
Test: 2018-08-05T00:06:42.401322: step 9800, loss 0.548857.
Train: 2018-08-05T00:06:43.244875: step 9801, loss 0.547154.
Train: 2018-08-05T00:06:43.494815: step 9802, loss 0.570982.
Train: 2018-08-05T00:06:43.760378: step 9803, loss 0.547504.
Train: 2018-08-05T00:06:43.994700: step 9804, loss 0.586699.
Train: 2018-08-05T00:06:44.244670: step 9805, loss 0.532215.
Train: 2018-08-05T00:06:44.510208: step 9806, loss 0.586674.
Train: 2018-08-05T00:06:44.744524: step 9807, loss 0.594408.
Train: 2018-08-05T00:06:44.994464: step 9808, loss 0.594379.
Train: 2018-08-05T00:06:45.244406: step 9809, loss 0.663607.
Train: 2018-08-05T00:06:45.494378: step 9810, loss 0.556025.
Test: 2018-08-05T00:06:46.744056: step 9810, loss 0.552796.
Train: 2018-08-05T00:06:46.994030: step 9811, loss 0.579023.
Train: 2018-08-05T00:06:47.243938: step 9812, loss 0.541074.
Train: 2018-08-05T00:06:47.493879: step 9813, loss 0.548763.
Train: 2018-08-05T00:06:47.743851: step 9814, loss 0.594223.
Train: 2018-08-05T00:06:47.946897: step 9815, loss 0.547872.
Train: 2018-08-05T00:06:48.212485: step 9816, loss 0.609312.
Train: 2018-08-05T00:06:48.446781: step 9817, loss 0.556499.
Train: 2018-08-05T00:06:48.696752: step 9818, loss 0.548982.
Train: 2018-08-05T00:06:48.946664: step 9819, loss 0.639434.
Train: 2018-08-05T00:06:49.196635: step 9820, loss 0.579138.
Test: 2018-08-05T00:06:50.461934: step 9820, loss 0.55138.
Train: 2018-08-05T00:06:50.696285: step 9821, loss 0.57915.
Train: 2018-08-05T00:06:50.946196: step 9822, loss 0.601696.
Train: 2018-08-05T00:06:51.196172: step 9823, loss 0.541677.
Train: 2018-08-05T00:06:51.442060: step 9824, loss 0.556687.
Train: 2018-08-05T00:06:51.691970: step 9825, loss 0.549173.
Train: 2018-08-05T00:06:51.941943: step 9826, loss 0.556633.
Train: 2018-08-05T00:06:52.191853: step 9827, loss 0.534004.
Train: 2018-08-05T00:06:52.441826: step 9828, loss 0.54139.
Train: 2018-08-05T00:06:52.691767: step 9829, loss 0.624517.
Train: 2018-08-05T00:06:52.941678: step 9830, loss 0.579064.
Test: 2018-08-05T00:06:54.191384: step 9830, loss 0.550801.
Train: 2018-08-05T00:06:54.425706: step 9831, loss 0.579048.
Train: 2018-08-05T00:06:54.675647: step 9832, loss 0.571422.
Train: 2018-08-05T00:06:54.925617: step 9833, loss 0.586648.
Train: 2018-08-05T00:06:55.175561: step 9834, loss 0.586648.
Train: 2018-08-05T00:06:55.425499: step 9835, loss 0.563727.
Train: 2018-08-05T00:06:55.675412: step 9836, loss 0.52546.
Train: 2018-08-05T00:06:55.925383: step 9837, loss 0.632648.
Train: 2018-08-05T00:06:56.175325: step 9838, loss 0.540624.
Train: 2018-08-05T00:06:56.425237: step 9839, loss 0.563604.
Train: 2018-08-05T00:06:56.675178: step 9840, loss 0.571263.
Test: 2018-08-05T00:06:57.940506: step 9840, loss 0.549775.
Train: 2018-08-05T00:06:58.174857: step 9841, loss 0.524997.
Train: 2018-08-05T00:06:58.424793: step 9842, loss 0.54029.
Train: 2018-08-05T00:06:58.674742: step 9843, loss 0.640955.
Train: 2018-08-05T00:06:58.924650: step 9844, loss 0.571146.
Train: 2018-08-05T00:06:59.174622: step 9845, loss 0.571131.
Train: 2018-08-05T00:06:59.424559: step 9846, loss 0.516636.
Train: 2018-08-05T00:06:59.674475: step 9847, loss 0.602298.
Train: 2018-08-05T00:06:59.924418: step 9848, loss 0.5867.
Train: 2018-08-05T00:07:00.174388: step 9849, loss 0.586704.
Train: 2018-08-05T00:07:00.424299: step 9850, loss 0.484984.
Test: 2018-08-05T00:07:01.674007: step 9850, loss 0.549719.
Train: 2018-08-05T00:07:01.986434: step 9851, loss 0.578874.
Train: 2018-08-05T00:07:02.236376: step 9852, loss 0.508105.
Train: 2018-08-05T00:07:02.486347: step 9853, loss 0.555191.
Train: 2018-08-05T00:07:02.736259: step 9854, loss 0.539266.
Train: 2018-08-05T00:07:02.986228: step 9855, loss 0.562959.
Train: 2018-08-05T00:07:03.236142: step 9856, loss 0.594818.
Train: 2018-08-05T00:07:03.486108: step 9857, loss 0.578863.
Train: 2018-08-05T00:07:03.736054: step 9858, loss 0.562834.
Train: 2018-08-05T00:07:04.001587: step 9859, loss 0.602966.
Train: 2018-08-05T00:07:04.251529: step 9860, loss 0.627105.
Test: 2018-08-05T00:07:05.501235: step 9860, loss 0.548095.
Train: 2018-08-05T00:07:05.735586: step 9861, loss 0.514608.
Train: 2018-08-05T00:07:05.985498: step 9862, loss 0.578872.
Train: 2018-08-05T00:07:06.235439: step 9863, loss 0.594952.
Train: 2018-08-05T00:07:06.485410: step 9864, loss 0.562801.
Train: 2018-08-05T00:07:06.750974: step 9865, loss 0.522644.
Train: 2018-08-05T00:07:06.985296: step 9866, loss 0.578872.
Train: 2018-08-05T00:07:07.235235: step 9867, loss 0.506507.
Train: 2018-08-05T00:07:07.485147: step 9868, loss 0.546659.
Train: 2018-08-05T00:07:07.735118: step 9869, loss 0.562742.
Train: 2018-08-05T00:07:07.985059: step 9870, loss 0.562718.
Test: 2018-08-05T00:07:09.234736: step 9870, loss 0.54861.
Train: 2018-08-05T00:07:09.484678: step 9871, loss 0.562697.
Train: 2018-08-05T00:07:09.734654: step 9872, loss 0.554565.
Train: 2018-08-05T00:07:09.984562: step 9873, loss 0.481399.
Train: 2018-08-05T00:07:10.234532: step 9874, loss 0.489236.
Train: 2018-08-05T00:07:10.484444: step 9875, loss 0.669079.
Train: 2018-08-05T00:07:10.734415: step 9876, loss 0.496893.
Train: 2018-08-05T00:07:10.984351: step 9877, loss 0.562525.
Train: 2018-08-05T00:07:11.234298: step 9878, loss 0.512969.
Train: 2018-08-05T00:07:11.484240: step 9879, loss 0.504482.
Train: 2018-08-05T00:07:11.734151: step 9880, loss 0.562441.
Test: 2018-08-05T00:07:12.983858: step 9880, loss 0.549044.
Train: 2018-08-05T00:07:13.218210: step 9881, loss 0.503948.
Train: 2018-08-05T00:07:13.468150: step 9882, loss 0.495261.
Train: 2018-08-05T00:07:13.718092: step 9883, loss 0.621429.
Train: 2018-08-05T00:07:13.968034: step 9884, loss 0.460759.
Train: 2018-08-05T00:07:14.217945: step 9885, loss 0.587872.
Train: 2018-08-05T00:07:14.467886: step 9886, loss 0.553796.
Train: 2018-08-05T00:07:14.717857: step 9887, loss 0.545194.
Train: 2018-08-05T00:07:14.983390: step 9888, loss 0.441971.
Train: 2018-08-05T00:07:15.217743: step 9889, loss 0.622828.
Train: 2018-08-05T00:07:15.467682: step 9890, loss 0.545012.
Test: 2018-08-05T00:07:16.732980: step 9890, loss 0.547238.
Train: 2018-08-05T00:07:16.967332: step 9891, loss 0.562353.
Train: 2018-08-05T00:07:17.217272: step 9892, loss 0.605907.
Train: 2018-08-05T00:07:17.467214: step 9893, loss 0.527499.
Train: 2018-08-05T00:07:17.717124: step 9894, loss 0.57109.
Train: 2018-08-05T00:07:17.967097: step 9895, loss 0.536183.
Train: 2018-08-05T00:07:18.217033: step 9896, loss 0.553636.
Train: 2018-08-05T00:07:18.466979: step 9897, loss 0.553635.
Train: 2018-08-05T00:07:18.716921: step 9898, loss 0.536158.
Train: 2018-08-05T00:07:18.966832: step 9899, loss 0.536151.
Train: 2018-08-05T00:07:19.216804: step 9900, loss 0.562375.
Test: 2018-08-05T00:07:20.466481: step 9900, loss 0.546561.
Train: 2018-08-05T00:07:21.294414: step 9901, loss 0.606106.
Train: 2018-08-05T00:07:21.544386: step 9902, loss 0.571107.
Train: 2018-08-05T00:07:21.794325: step 9903, loss 0.527475.
Train: 2018-08-05T00:07:22.044237: step 9904, loss 0.562361.
Train: 2018-08-05T00:07:22.294208: step 9905, loss 0.57106.
Train: 2018-08-05T00:07:22.544149: step 9906, loss 0.527597.
Train: 2018-08-05T00:07:22.794091: step 9907, loss 0.562349.
Train: 2018-08-05T00:07:23.044003: step 9908, loss 0.527672.
Train: 2018-08-05T00:07:23.293976: step 9909, loss 0.519033.
Train: 2018-08-05T00:07:23.559518: step 9910, loss 0.50171.
Test: 2018-08-05T00:07:24.793593: step 9910, loss 0.546869.
Train: 2018-08-05T00:07:25.027944: step 9911, loss 0.562347.
Train: 2018-08-05T00:07:25.277894: step 9912, loss 0.62307.
Train: 2018-08-05T00:07:25.527796: step 9913, loss 0.562345.
Train: 2018-08-05T00:07:25.777738: step 9914, loss 0.536381.
Train: 2018-08-05T00:07:26.027709: step 9915, loss 0.545049.
Train: 2018-08-05T00:07:26.277651: step 9916, loss 0.545061.
Train: 2018-08-05T00:07:26.527563: step 9917, loss 0.596876.
Train: 2018-08-05T00:07:26.777533: step 9918, loss 0.605441.
Train: 2018-08-05T00:07:27.027475: step 9919, loss 0.596729.
Train: 2018-08-05T00:07:27.277387: step 9920, loss 0.502346.
Test: 2018-08-05T00:07:28.527093: step 9920, loss 0.54884.
Train: 2018-08-05T00:07:28.761446: step 9921, loss 0.579442.
Train: 2018-08-05T00:07:29.011379: step 9922, loss 0.511144.
Train: 2018-08-05T00:07:29.261327: step 9923, loss 0.545298.
Train: 2018-08-05T00:07:29.511239: step 9924, loss 0.604907.
Train: 2018-08-05T00:07:29.761180: step 9925, loss 0.587833.
Train: 2018-08-05T00:07:30.011152: step 9926, loss 0.587773.
Train: 2018-08-05T00:07:30.261093: step 9927, loss 0.596151.
Train: 2018-08-05T00:07:30.511028: step 9928, loss 0.59604.
Train: 2018-08-05T00:07:30.776570: step 9929, loss 0.520501.
Train: 2018-08-05T00:07:31.010917: step 9930, loss 0.537354.
Test: 2018-08-05T00:07:32.260594: step 9930, loss 0.550238.
Train: 2018-08-05T00:07:32.494946: step 9931, loss 0.520758.
Train: 2018-08-05T00:07:32.744857: step 9932, loss 0.529143.
Train: 2018-08-05T00:07:32.994798: step 9933, loss 0.62068.
Train: 2018-08-05T00:07:33.244769: step 9934, loss 0.587368.
Train: 2018-08-05T00:07:33.494681: step 9935, loss 0.554188.
Train: 2018-08-05T00:07:33.744652: step 9936, loss 0.529416.
Train: 2018-08-05T00:07:33.994594: step 9937, loss 0.727672.
Train: 2018-08-05T00:07:34.244535: step 9938, loss 0.554322.
Train: 2018-08-05T00:07:34.494477: step 9939, loss 0.578949.
Train: 2018-08-05T00:07:34.744418: step 9940, loss 0.570776.
Test: 2018-08-05T00:07:36.009717: step 9940, loss 0.54893.
Train: 2018-08-05T00:07:36.259658: step 9941, loss 0.538323.
Train: 2018-08-05T00:07:36.509630: step 9942, loss 0.570801.
Train: 2018-08-05T00:07:36.743920: step 9943, loss 0.675706.
Train: 2018-08-05T00:07:36.993862: step 9944, loss 0.570843.
Train: 2018-08-05T00:07:37.243833: step 9945, loss 0.546912.
Train: 2018-08-05T00:07:37.493745: step 9946, loss 0.594771.
Train: 2018-08-05T00:07:37.743686: step 9947, loss 0.563013.
Train: 2018-08-05T00:07:37.993628: step 9948, loss 0.570968.
Train: 2018-08-05T00:07:38.243598: step 9949, loss 0.539527.
Train: 2018-08-05T00:07:38.493541: step 9950, loss 0.55532.
Test: 2018-08-05T00:07:39.758838: step 9950, loss 0.550109.
Train: 2018-08-05T00:07:40.008780: step 9951, loss 0.547523.
Train: 2018-08-05T00:07:40.258751: step 9952, loss 0.563215.
Train: 2018-08-05T00:07:40.508693: step 9953, loss 0.547568.
Train: 2018-08-05T00:07:40.758604: step 9954, loss 0.563222.
Train: 2018-08-05T00:07:41.008546: step 9955, loss 0.618038.
Train: 2018-08-05T00:07:41.258487: step 9956, loss 0.508456.
Train: 2018-08-05T00:07:41.524068: step 9957, loss 0.586711.
Train: 2018-08-05T00:07:41.758401: step 9958, loss 0.625885.
Train: 2018-08-05T00:07:42.008342: step 9959, loss 0.555402.
Train: 2018-08-05T00:07:42.258284: step 9960, loss 0.563238.
Test: 2018-08-05T00:07:43.507960: step 9960, loss 0.550543.
Train: 2018-08-05T00:07:43.742281: step 9961, loss 0.602344.
Train: 2018-08-05T00:07:43.992223: step 9962, loss 0.539825.
Train: 2018-08-05T00:07:44.242194: step 9963, loss 0.492957.
Train: 2018-08-05T00:07:44.492135: step 9964, loss 0.586708.
Train: 2018-08-05T00:07:44.742046: step 9965, loss 0.524011.
Train: 2018-08-05T00:07:44.945124: step 9966, loss 0.563156.
Train: 2018-08-05T00:07:45.195095: step 9967, loss 0.578866.
Train: 2018-08-05T00:07:45.460629: step 9968, loss 0.657775.
Train: 2018-08-05T00:07:45.710601: step 9969, loss 0.570976.
Train: 2018-08-05T00:07:45.960541: step 9970, loss 0.57098.
Test: 2018-08-05T00:07:47.210218: step 9970, loss 0.549941.
Train: 2018-08-05T00:07:47.444570: step 9971, loss 0.649794.
Train: 2018-08-05T00:07:47.694480: step 9972, loss 0.571007.
Train: 2018-08-05T00:07:47.960073: step 9973, loss 0.523956.
Train: 2018-08-05T00:07:48.194364: step 9974, loss 0.539677.
Train: 2018-08-05T00:07:48.459956: step 9975, loss 0.586717.
Train: 2018-08-05T00:07:48.709917: step 9976, loss 0.586715.
Train: 2018-08-05T00:07:48.959809: step 9977, loss 0.500537.
Train: 2018-08-05T00:07:49.209781: step 9978, loss 0.555343.
Train: 2018-08-05T00:07:49.459724: step 9979, loss 0.508168.
Train: 2018-08-05T00:07:49.709664: step 9980, loss 0.53159.
Test: 2018-08-05T00:07:50.959340: step 9980, loss 0.550748.
Train: 2018-08-05T00:07:51.193695: step 9981, loss 0.523506.
Train: 2018-08-05T00:07:51.443602: step 9982, loss 0.507375.
Train: 2018-08-05T00:07:51.693544: step 9983, loss 0.538931.
Train: 2018-08-05T00:07:51.943515: step 9984, loss 0.619023.
Train: 2018-08-05T00:07:52.193428: step 9985, loss 0.603061.
Train: 2018-08-05T00:07:52.443369: step 9986, loss 0.586967.
Train: 2018-08-05T00:07:52.693343: step 9987, loss 0.562707.
Train: 2018-08-05T00:07:52.943251: step 9988, loss 0.603216.
Train: 2018-08-05T00:07:53.193194: step 9989, loss 0.627555.
Train: 2018-08-05T00:07:53.443134: step 9990, loss 0.514098.
Test: 2018-08-05T00:07:54.708462: step 9990, loss 0.548792.
Train: 2018-08-05T00:07:54.942784: step 9991, loss 0.554594.
Train: 2018-08-05T00:07:55.192755: step 9992, loss 0.595108.
Train: 2018-08-05T00:07:55.442695: step 9993, loss 0.514087.
Train: 2018-08-05T00:07:55.692637: step 9994, loss 0.546468.
Train: 2018-08-05T00:07:55.942549: step 9995, loss 0.570788.
Train: 2018-08-05T00:07:56.192524: step 9996, loss 0.513912.
Train: 2018-08-05T00:07:56.442433: step 9997, loss 0.635904.
Train: 2018-08-05T00:07:56.692403: step 9998, loss 0.570779.
Train: 2018-08-05T00:07:56.942315: step 9999, loss 0.562639.
Train: 2018-08-05T00:07:57.192286: step 10000, loss 0.513807.
Test: 2018-08-05T00:07:58.441963: step 10000, loss 0.548272.
Train: 2018-08-05T00:07:59.316789: step 10001, loss 0.538186.
Train: 2018-08-05T00:07:59.566730: step 10002, loss 0.627892.
Train: 2018-08-05T00:07:59.816672: step 10003, loss 0.546297.
Train: 2018-08-05T00:08:00.066613: step 10004, loss 0.578933.
Train: 2018-08-05T00:08:00.316525: step 10005, loss 0.59525.
Train: 2018-08-05T00:08:00.566466: step 10006, loss 0.54632.
Train: 2018-08-05T00:08:00.816441: step 10007, loss 0.546333.
Train: 2018-08-05T00:08:01.066379: step 10008, loss 0.538185.
Train: 2018-08-05T00:08:01.316317: step 10009, loss 0.530013.
Train: 2018-08-05T00:08:01.581854: step 10010, loss 0.570771.
Test: 2018-08-05T00:08:02.831561: step 10010, loss 0.54933.
Train: 2018-08-05T00:08:03.065912: step 10011, loss 0.562599.
Train: 2018-08-05T00:08:03.315853: step 10012, loss 0.554413.
Train: 2018-08-05T00:08:03.565763: step 10013, loss 0.59532.
Train: 2018-08-05T00:08:03.815736: step 10014, loss 0.529838.
Train: 2018-08-05T00:08:04.065646: step 10015, loss 0.480653.
Train: 2018-08-05T00:08:04.315618: step 10016, loss 0.595399.
Train: 2018-08-05T00:08:04.565559: step 10017, loss 0.611887.
Train: 2018-08-05T00:08:04.815495: step 10018, loss 0.59544.
Train: 2018-08-05T00:08:05.065443: step 10019, loss 0.562536.
Train: 2018-08-05T00:08:05.315384: step 10020, loss 0.554325.
Test: 2018-08-05T00:08:06.565061: step 10020, loss 0.548639.
Train: 2018-08-05T00:08:06.799383: step 10021, loss 0.496836.
Train: 2018-08-05T00:08:07.049322: step 10022, loss 0.537866.
Train: 2018-08-05T00:08:07.299296: step 10023, loss 0.636642.
Train: 2018-08-05T00:08:07.549238: step 10024, loss 0.587223.
Train: 2018-08-05T00:08:07.799172: step 10025, loss 0.595432.
Train: 2018-08-05T00:08:08.049121: step 10026, loss 0.546127.
Train: 2018-08-05T00:08:08.314675: step 10027, loss 0.595366.
Train: 2018-08-05T00:08:08.564623: step 10028, loss 0.480716.
Train: 2018-08-05T00:08:08.814564: step 10029, loss 0.628082.
Train: 2018-08-05T00:08:09.064477: step 10030, loss 0.578944.
Test: 2018-08-05T00:08:10.376668: step 10030, loss 0.547657.
Train: 2018-08-05T00:08:10.611035: step 10031, loss 0.546278.
Train: 2018-08-05T00:08:10.860930: step 10032, loss 0.546307.
Train: 2018-08-05T00:08:11.126493: step 10033, loss 0.578926.
Train: 2018-08-05T00:08:11.360844: step 10034, loss 0.497488.
Train: 2018-08-05T00:08:11.610785: step 10035, loss 0.619671.
Train: 2018-08-05T00:08:11.860727: step 10036, loss 0.562634.
Train: 2018-08-05T00:08:12.110668: step 10037, loss 0.538225.
Train: 2018-08-05T00:08:12.360609: step 10038, loss 0.546363.
Train: 2018-08-05T00:08:12.610522: step 10039, loss 0.668487.
Train: 2018-08-05T00:08:12.860495: step 10040, loss 0.522033.
Test: 2018-08-05T00:08:14.110169: step 10040, loss 0.549113.
Train: 2018-08-05T00:08:14.344520: step 10041, loss 0.513963.
Train: 2018-08-05T00:08:14.594462: step 10042, loss 0.554546.
Train: 2018-08-05T00:08:14.844402: step 10043, loss 0.635785.
Train: 2018-08-05T00:08:15.094314: step 10044, loss 0.562675.
Train: 2018-08-05T00:08:15.344287: step 10045, loss 0.554581.
Train: 2018-08-05T00:08:15.594197: step 10046, loss 0.522196.
Train: 2018-08-05T00:08:15.844139: step 10047, loss 0.505975.
Train: 2018-08-05T00:08:16.094110: step 10048, loss 0.619485.
Train: 2018-08-05T00:08:16.344022: step 10049, loss 0.603258.
Train: 2018-08-05T00:08:16.593964: step 10050, loss 0.587014.
Test: 2018-08-05T00:08:17.843670: step 10050, loss 0.547674.
Train: 2018-08-05T00:08:18.078016: step 10051, loss 0.6032.
Train: 2018-08-05T00:08:18.327962: step 10052, loss 0.530383.
Train: 2018-08-05T00:08:18.577903: step 10053, loss 0.546582.
Train: 2018-08-05T00:08:18.827814: step 10054, loss 0.522381.
Train: 2018-08-05T00:08:19.093389: step 10055, loss 0.578886.
Train: 2018-08-05T00:08:19.343349: step 10056, loss 0.554653.
Train: 2018-08-05T00:08:19.593291: step 10057, loss 0.52232.
Train: 2018-08-05T00:08:19.858853: step 10058, loss 0.595078.
Train: 2018-08-05T00:08:20.124420: step 10059, loss 0.530317.
Train: 2018-08-05T00:08:20.374327: step 10060, loss 0.595113.
Test: 2018-08-05T00:08:21.624035: step 10060, loss 0.550259.
Train: 2018-08-05T00:08:21.858356: step 10061, loss 0.570792.
Train: 2018-08-05T00:08:22.108327: step 10062, loss 0.570791.
Train: 2018-08-05T00:08:22.358268: step 10063, loss 0.56268.
Train: 2018-08-05T00:08:22.608179: step 10064, loss 0.570791.
Train: 2018-08-05T00:08:22.858145: step 10065, loss 0.554571.
Train: 2018-08-05T00:08:23.108063: step 10066, loss 0.481569.
Train: 2018-08-05T00:08:23.358034: step 10067, loss 0.522018.
Train: 2018-08-05T00:08:23.607976: step 10068, loss 0.554474.
Train: 2018-08-05T00:08:23.857913: step 10069, loss 0.529905.
Train: 2018-08-05T00:08:24.107828: step 10070, loss 0.554365.
Test: 2018-08-05T00:08:25.341914: step 10070, loss 0.54899.
Train: 2018-08-05T00:08:25.591881: step 10071, loss 0.620099.
Train: 2018-08-05T00:08:25.841827: step 10072, loss 0.587226.
Train: 2018-08-05T00:08:26.091739: step 10073, loss 0.52956.
Train: 2018-08-05T00:08:26.341710: step 10074, loss 0.579006.
Train: 2018-08-05T00:08:26.591647: step 10075, loss 0.562501.
Train: 2018-08-05T00:08:26.841564: step 10076, loss 0.603794.
Train: 2018-08-05T00:08:27.091534: step 10077, loss 0.554246.
Train: 2018-08-05T00:08:27.341476: step 10078, loss 0.546.
Train: 2018-08-05T00:08:27.591418: step 10079, loss 0.628522.
Train: 2018-08-05T00:08:27.841328: step 10080, loss 0.570757.
Test: 2018-08-05T00:08:29.106657: step 10080, loss 0.548796.
Train: 2018-08-05T00:08:29.341009: step 10081, loss 0.570758.
Train: 2018-08-05T00:08:29.606571: step 10082, loss 0.562549.
Train: 2018-08-05T00:08:29.856512: step 10083, loss 0.529772.
Train: 2018-08-05T00:08:30.106424: step 10084, loss 0.546187.
Train: 2018-08-05T00:08:30.356394: step 10085, loss 0.554384.
Train: 2018-08-05T00:08:30.606337: step 10086, loss 0.61171.
Train: 2018-08-05T00:08:30.856249: step 10087, loss 0.562588.
Train: 2018-08-05T00:08:31.106190: step 10088, loss 0.627957.
Train: 2018-08-05T00:08:31.356130: step 10089, loss 0.554479.
Train: 2018-08-05T00:08:31.606103: step 10090, loss 0.570782.
Test: 2018-08-05T00:08:32.855779: step 10090, loss 0.548377.
Train: 2018-08-05T00:08:33.090131: step 10091, loss 0.538333.
Train: 2018-08-05T00:08:33.340042: step 10092, loss 0.562691.
Train: 2018-08-05T00:08:33.590014: step 10093, loss 0.619368.
Train: 2018-08-05T00:08:33.839954: step 10094, loss 0.522355.
Train: 2018-08-05T00:08:34.089896: step 10095, loss 0.586949.
Train: 2018-08-05T00:08:34.339837: step 10096, loss 0.538603.
Train: 2018-08-05T00:08:34.589780: step 10097, loss 0.498383.
Train: 2018-08-05T00:08:34.839691: step 10098, loss 0.619165.
Train: 2018-08-05T00:08:35.089632: step 10099, loss 0.5225.
Train: 2018-08-05T00:08:35.339603: step 10100, loss 0.546642.
Test: 2018-08-05T00:08:36.573659: step 10100, loss 0.550944.
Train: 2018-08-05T00:08:37.432863: step 10101, loss 0.570815.
Train: 2018-08-05T00:08:37.682805: step 10102, loss 0.530439.
Train: 2018-08-05T00:08:37.932746: step 10103, loss 0.546543.
Train: 2018-08-05T00:08:38.182687: step 10104, loss 0.505982.
Train: 2018-08-05T00:08:38.432599: step 10105, loss 0.546406.
Train: 2018-08-05T00:08:38.682570: step 10106, loss 0.578927.
Train: 2018-08-05T00:08:38.932481: step 10107, loss 0.603455.
Train: 2018-08-05T00:08:39.182453: step 10108, loss 0.521678.
Train: 2018-08-05T00:08:39.432366: step 10109, loss 0.562564.
Train: 2018-08-05T00:08:39.682307: step 10110, loss 0.554335.
Test: 2018-08-05T00:08:40.932013: step 10110, loss 0.549738.
Train: 2018-08-05T00:08:41.181980: step 10111, loss 0.496715.
Train: 2018-08-05T00:08:41.431927: step 10112, loss 0.512991.
Train: 2018-08-05T00:08:41.697460: step 10113, loss 0.521054.
Train: 2018-08-05T00:08:41.947400: step 10114, loss 0.570762.
Train: 2018-08-05T00:08:42.212964: step 10115, loss 0.503976.
Train: 2018-08-05T00:08:42.462935: step 10116, loss 0.637867.
Train: 2018-08-05T00:08:42.681603: step 10117, loss 0.52653.
Train: 2018-08-05T00:08:42.931546: step 10118, loss 0.503401.
Train: 2018-08-05T00:08:43.181522: step 10119, loss 0.621534.
Train: 2018-08-05T00:08:43.431429: step 10120, loss 0.587756.
Test: 2018-08-05T00:08:44.681135: step 10120, loss 0.549179.
Train: 2018-08-05T00:08:44.931077: step 10121, loss 0.503047.
Train: 2018-08-05T00:08:45.181049: step 10122, loss 0.579323.
Train: 2018-08-05T00:08:45.430960: step 10123, loss 0.604822.
Train: 2018-08-05T00:08:45.680931: step 10124, loss 0.621796.
Train: 2018-08-05T00:08:45.930873: step 10125, loss 0.545396.
Train: 2018-08-05T00:08:46.180815: step 10126, loss 0.528492.
Train: 2018-08-05T00:08:46.430756: step 10127, loss 0.536978.
Train: 2018-08-05T00:08:46.680666: step 10128, loss 0.536986.
Train: 2018-08-05T00:08:46.930638: step 10129, loss 0.520064.
Train: 2018-08-05T00:08:47.180549: step 10130, loss 0.621619.
Test: 2018-08-05T00:08:48.430257: step 10130, loss 0.548619.
Train: 2018-08-05T00:08:48.664578: step 10131, loss 0.503137.
Train: 2018-08-05T00:08:48.914551: step 10132, loss 0.503108.
Train: 2018-08-05T00:08:49.164492: step 10133, loss 0.59626.
Train: 2018-08-05T00:08:49.414433: step 10134, loss 0.596274.
Train: 2018-08-05T00:08:49.679998: step 10135, loss 0.579305.
Train: 2018-08-05T00:08:49.929907: step 10136, loss 0.536954.
Train: 2018-08-05T00:08:50.179878: step 10137, loss 0.621593.
Train: 2018-08-05T00:08:50.429789: step 10138, loss 0.5877.
Train: 2018-08-05T00:08:50.679761: step 10139, loss 0.562374.
Train: 2018-08-05T00:08:50.929702: step 10140, loss 0.570788.
Test: 2018-08-05T00:08:52.179379: step 10140, loss 0.5484.
Train: 2018-08-05T00:08:52.429347: step 10141, loss 0.570779.
Train: 2018-08-05T00:08:52.679293: step 10142, loss 0.528976.
Train: 2018-08-05T00:08:52.929234: step 10143, loss 0.570767.
Train: 2018-08-05T00:08:53.179145: step 10144, loss 0.570763.
Train: 2018-08-05T00:08:53.429117: step 10145, loss 0.612319.
Train: 2018-08-05T00:08:53.679029: step 10146, loss 0.58733.
Train: 2018-08-05T00:08:53.928969: step 10147, loss 0.537723.
Train: 2018-08-05T00:08:54.178912: step 10148, loss 0.496623.
Train: 2018-08-05T00:08:54.428852: step 10149, loss 0.620138.
Train: 2018-08-05T00:08:54.678795: step 10150, loss 0.66931.
Test: 2018-08-05T00:08:55.928501: step 10150, loss 0.549125.
Train: 2018-08-05T00:08:56.162847: step 10151, loss 0.660704.
Train: 2018-08-05T00:08:56.412795: step 10152, loss 0.603283.
Train: 2018-08-05T00:08:56.662736: step 10153, loss 0.50625.
Train: 2018-08-05T00:08:56.912678: step 10154, loss 0.522639.
Train: 2018-08-05T00:08:57.162619: step 10155, loss 0.546833.
Train: 2018-08-05T00:08:57.412530: step 10156, loss 0.578862.
Train: 2018-08-05T00:08:57.678092: step 10157, loss 0.570889.
Train: 2018-08-05T00:08:57.928033: step 10158, loss 0.570906.
Train: 2018-08-05T00:08:58.193626: step 10159, loss 0.499502.
Train: 2018-08-05T00:08:58.459190: step 10160, loss 0.555055.
Test: 2018-08-05T00:08:59.693245: step 10160, loss 0.549002.
Train: 2018-08-05T00:08:59.943212: step 10161, loss 0.539179.
Train: 2018-08-05T00:09:00.193158: step 10162, loss 0.555029.
Train: 2018-08-05T00:09:00.443094: step 10163, loss 0.531144.
Train: 2018-08-05T00:09:00.708656: step 10164, loss 0.531052.
Train: 2018-08-05T00:09:00.942953: step 10165, loss 0.546905.
Train: 2018-08-05T00:09:01.208515: step 10166, loss 0.546817.
Train: 2018-08-05T00:09:01.458490: step 10167, loss 0.562798.
Train: 2018-08-05T00:09:01.708397: step 10168, loss 0.522461.
Train: 2018-08-05T00:09:01.958339: step 10169, loss 0.603157.
Train: 2018-08-05T00:09:02.208312: step 10170, loss 0.5789.
Test: 2018-08-05T00:09:03.473609: step 10170, loss 0.54929.
Train: 2018-08-05T00:09:03.770440: step 10171, loss 0.497699.
Train: 2018-08-05T00:09:04.020389: step 10172, loss 0.627798.
Train: 2018-08-05T00:09:04.270331: step 10173, loss 0.554461.
Train: 2018-08-05T00:09:04.520270: step 10174, loss 0.554437.
Train: 2018-08-05T00:09:04.770181: step 10175, loss 0.603478.
Train: 2018-08-05T00:09:05.020122: step 10176, loss 0.562587.
Train: 2018-08-05T00:09:05.270094: step 10177, loss 0.628036.
Train: 2018-08-05T00:09:05.520035: step 10178, loss 0.538088.
Train: 2018-08-05T00:09:05.769979: step 10179, loss 0.521782.
Train: 2018-08-05T00:09:06.019889: step 10180, loss 0.554434.
Test: 2018-08-05T00:09:07.285216: step 10180, loss 0.548009.
Train: 2018-08-05T00:09:07.519567: step 10181, loss 0.554425.
Train: 2018-08-05T00:09:07.769511: step 10182, loss 0.505352.
Train: 2018-08-05T00:09:08.019450: step 10183, loss 0.546186.
Train: 2018-08-05T00:09:08.269393: step 10184, loss 0.529714.
Train: 2018-08-05T00:09:08.519306: step 10185, loss 0.521375.
Train: 2018-08-05T00:09:08.784893: step 10186, loss 0.603784.
Train: 2018-08-05T00:09:09.034807: step 10187, loss 0.587301.
Train: 2018-08-05T00:09:09.269160: step 10188, loss 0.496228.
Train: 2018-08-05T00:09:09.519101: step 10189, loss 0.603961.
Train: 2018-08-05T00:09:09.769011: step 10190, loss 0.554141.
Test: 2018-08-05T00:09:11.034338: step 10190, loss 0.549125.
Train: 2018-08-05T00:09:11.268690: step 10191, loss 0.587399.
Train: 2018-08-05T00:09:11.518630: step 10192, loss 0.562441.
Train: 2018-08-05T00:09:11.768542: step 10193, loss 0.645664.
Train: 2018-08-05T00:09:12.018483: step 10194, loss 0.579067.
Train: 2018-08-05T00:09:12.284078: step 10195, loss 0.554179.
Train: 2018-08-05T00:09:12.534013: step 10196, loss 0.512836.
Train: 2018-08-05T00:09:12.783961: step 10197, loss 0.554217.
Train: 2018-08-05T00:09:13.049493: step 10198, loss 0.612087.
Train: 2018-08-05T00:09:13.283845: step 10199, loss 0.55425.
Train: 2018-08-05T00:09:13.533786: step 10200, loss 0.562514.
Test: 2018-08-05T00:09:14.783461: step 10200, loss 0.549345.
Train: 2018-08-05T00:09:15.627043: step 10201, loss 0.496665.
Train: 2018-08-05T00:09:15.892606: step 10202, loss 0.463694.
Train: 2018-08-05T00:09:16.142548: step 10203, loss 0.5625.
Train: 2018-08-05T00:09:16.392491: step 10204, loss 0.521105.
Train: 2018-08-05T00:09:16.642430: step 10205, loss 0.620553.
Train: 2018-08-05T00:09:16.892343: step 10206, loss 0.579069.
Train: 2018-08-05T00:09:17.142312: step 10207, loss 0.537506.
Train: 2018-08-05T00:09:17.392226: step 10208, loss 0.612371.
Train: 2018-08-05T00:09:17.642197: step 10209, loss 0.470933.
Train: 2018-08-05T00:09:17.892138: step 10210, loss 0.5291.
Test: 2018-08-05T00:09:19.141815: step 10210, loss 0.54828.
Train: 2018-08-05T00:09:19.376167: step 10211, loss 0.503965.
Train: 2018-08-05T00:09:19.626102: step 10212, loss 0.537272.
Train: 2018-08-05T00:09:19.891641: step 10213, loss 0.562385.
Train: 2018-08-05T00:09:20.125991: step 10214, loss 0.570799.
Train: 2018-08-05T00:09:20.375931: step 10215, loss 0.587696.
Train: 2018-08-05T00:09:20.625843: step 10216, loss 0.562361.
Train: 2018-08-05T00:09:20.875815: step 10217, loss 0.528516.
Train: 2018-08-05T00:09:21.125757: step 10218, loss 0.528467.
Train: 2018-08-05T00:09:21.375696: step 10219, loss 0.53689.
Train: 2018-08-05T00:09:21.625639: step 10220, loss 0.587852.
Test: 2018-08-05T00:09:22.875316: step 10220, loss 0.549501.
Train: 2018-08-05T00:09:23.125289: step 10221, loss 0.579364.
Train: 2018-08-05T00:09:23.375200: step 10222, loss 0.664497.
Train: 2018-08-05T00:09:23.625172: step 10223, loss 0.519875.
Train: 2018-08-05T00:09:23.875112: step 10224, loss 0.570834.
Train: 2018-08-05T00:09:24.125053: step 10225, loss 0.587767.
Train: 2018-08-05T00:09:24.374966: step 10226, loss 0.520096.
Train: 2018-08-05T00:09:24.624937: step 10227, loss 0.537035.
Train: 2018-08-05T00:09:24.874878: step 10228, loss 0.537055.
Train: 2018-08-05T00:09:25.124790: step 10229, loss 0.494882.
Train: 2018-08-05T00:09:25.390353: step 10230, loss 0.553919.
Test: 2018-08-05T00:09:26.624438: step 10230, loss 0.548631.
Train: 2018-08-05T00:09:26.858760: step 10231, loss 0.562361.
Train: 2018-08-05T00:09:27.108730: step 10232, loss 0.613118.
Train: 2018-08-05T00:09:27.358671: step 10233, loss 0.545451.
Train: 2018-08-05T00:09:27.608583: step 10234, loss 0.545459.
Train: 2018-08-05T00:09:27.858555: step 10235, loss 0.587712.
Train: 2018-08-05T00:09:28.108498: step 10236, loss 0.638347.
Train: 2018-08-05T00:09:28.358437: step 10237, loss 0.612896.
Train: 2018-08-05T00:09:28.608379: step 10238, loss 0.562393.
Train: 2018-08-05T00:09:28.858320: step 10239, loss 0.562411.
Train: 2018-08-05T00:09:29.108256: step 10240, loss 0.63743.
Test: 2018-08-05T00:09:30.373560: step 10240, loss 0.54804.
Train: 2018-08-05T00:09:30.607882: step 10241, loss 0.496113.
Train: 2018-08-05T00:09:30.857852: step 10242, loss 0.537675.
Train: 2018-08-05T00:09:31.107764: step 10243, loss 0.620274.
Train: 2018-08-05T00:09:31.357705: step 10244, loss 0.58721.
Train: 2018-08-05T00:09:31.607677: step 10245, loss 0.554369.
Train: 2018-08-05T00:09:31.857617: step 10246, loss 0.627971.
Train: 2018-08-05T00:09:32.107560: step 10247, loss 0.587054.
Train: 2018-08-05T00:09:32.357471: step 10248, loss 0.562696.
Train: 2018-08-05T00:09:32.623063: step 10249, loss 0.554678.
Train: 2018-08-05T00:09:32.873005: step 10250, loss 0.570832.
Test: 2018-08-05T00:09:34.122682: step 10250, loss 0.550002.
Train: 2018-08-05T00:09:34.357004: step 10251, loss 0.530772.
Train: 2018-08-05T00:09:34.606943: step 10252, loss 0.562864.
Train: 2018-08-05T00:09:34.856918: step 10253, loss 0.570875.
Train: 2018-08-05T00:09:35.106827: step 10254, loss 0.538992.
Train: 2018-08-05T00:09:35.356769: step 10255, loss 0.578859.
Train: 2018-08-05T00:09:35.606741: step 10256, loss 0.539058.
Train: 2018-08-05T00:09:35.856682: step 10257, loss 0.515185.
Train: 2018-08-05T00:09:36.106593: step 10258, loss 0.507141.
Train: 2018-08-05T00:09:36.356565: step 10259, loss 0.554894.
Train: 2018-08-05T00:09:36.606505: step 10260, loss 0.650956.
Test: 2018-08-05T00:09:37.856183: step 10260, loss 0.5478.
Train: 2018-08-05T00:09:38.106155: step 10261, loss 0.554829.
Train: 2018-08-05T00:09:38.356096: step 10262, loss 0.578867.
Train: 2018-08-05T00:09:38.606007: step 10263, loss 0.578867.
Train: 2018-08-05T00:09:38.855979: step 10264, loss 0.546798.
Train: 2018-08-05T00:09:39.105891: step 10265, loss 0.602928.
Train: 2018-08-05T00:09:39.355863: step 10266, loss 0.506726.
Train: 2018-08-05T00:09:39.605804: step 10267, loss 0.586891.
Train: 2018-08-05T00:09:39.808876: step 10268, loss 0.494328.
Train: 2018-08-05T00:09:40.058822: step 10269, loss 0.538661.
Train: 2018-08-05T00:09:40.308764: step 10270, loss 0.546629.
Test: 2018-08-05T00:09:41.542820: step 10270, loss 0.550149.
Train: 2018-08-05T00:09:41.808407: step 10271, loss 0.546551.
Train: 2018-08-05T00:09:42.058354: step 10272, loss 0.578901.
Train: 2018-08-05T00:09:42.308293: step 10273, loss 0.554534.
Train: 2018-08-05T00:09:42.558237: step 10274, loss 0.570778.
Train: 2018-08-05T00:09:42.808179: step 10275, loss 0.578931.
Train: 2018-08-05T00:09:43.058120: step 10276, loss 0.595273.
Train: 2018-08-05T00:09:43.308061: step 10277, loss 0.562598.
Train: 2018-08-05T00:09:43.558003: step 10278, loss 0.497213.
Train: 2018-08-05T00:09:43.807945: step 10279, loss 0.578952.
Train: 2018-08-05T00:09:44.057886: step 10280, loss 0.66093.
Test: 2018-08-05T00:09:45.307563: step 10280, loss 0.547775.
Train: 2018-08-05T00:09:45.573125: step 10281, loss 0.636259.
Train: 2018-08-05T00:09:45.807448: step 10282, loss 0.554445.
Train: 2018-08-05T00:09:46.057418: step 10283, loss 0.619639.
Train: 2018-08-05T00:09:46.307359: step 10284, loss 0.627595.
Train: 2018-08-05T00:09:46.557296: step 10285, loss 0.498116.
Train: 2018-08-05T00:09:46.807242: step 10286, loss 0.603045.
Train: 2018-08-05T00:09:47.057178: step 10287, loss 0.554782.
Train: 2018-08-05T00:09:47.307125: step 10288, loss 0.578865.
Train: 2018-08-05T00:09:47.572682: step 10289, loss 0.530946.
Train: 2018-08-05T00:09:47.807009: step 10290, loss 0.523046.
Test: 2018-08-05T00:09:49.056686: step 10290, loss 0.549789.
Train: 2018-08-05T00:09:49.291006: step 10291, loss 0.475237.
Train: 2018-08-05T00:09:49.540946: step 10292, loss 0.570875.
Train: 2018-08-05T00:09:49.806540: step 10293, loss 0.610865.
Train: 2018-08-05T00:09:50.056450: step 10294, loss 0.546848.
Train: 2018-08-05T00:09:50.306392: step 10295, loss 0.530798.
Train: 2018-08-05T00:09:50.556364: step 10296, loss 0.538745.
Train: 2018-08-05T00:09:50.806306: step 10297, loss 0.514535.
Train: 2018-08-05T00:09:51.056247: step 10298, loss 0.506271.
Train: 2018-08-05T00:09:51.306160: step 10299, loss 0.570796.
Train: 2018-08-05T00:09:51.556101: step 10300, loss 0.546386.
Test: 2018-08-05T00:09:52.817319: step 10300, loss 0.548226.
Train: 2018-08-05T00:09:53.660895: step 10301, loss 0.538123.
Train: 2018-08-05T00:09:53.910844: step 10302, loss 0.570763.
Train: 2018-08-05T00:09:54.160784: step 10303, loss 0.570759.
Train: 2018-08-05T00:09:54.410726: step 10304, loss 0.5213.
Train: 2018-08-05T00:09:54.676259: step 10305, loss 0.612107.
Train: 2018-08-05T00:09:54.926200: step 10306, loss 0.570757.
Train: 2018-08-05T00:09:55.191763: step 10307, loss 0.537576.
Train: 2018-08-05T00:09:55.441734: step 10308, loss 0.562451.
Train: 2018-08-05T00:09:55.691676: step 10309, loss 0.570762.
Train: 2018-08-05T00:09:55.941617: step 10310, loss 0.562435.
Test: 2018-08-05T00:09:57.191294: step 10310, loss 0.548893.
Train: 2018-08-05T00:09:57.425615: step 10311, loss 0.504083.
Train: 2018-08-05T00:09:57.675586: step 10312, loss 0.545716.
Train: 2018-08-05T00:09:57.925528: step 10313, loss 0.570774.
Train: 2018-08-05T00:09:58.175469: step 10314, loss 0.562399.
Train: 2018-08-05T00:09:58.425410: step 10315, loss 0.554004.
Train: 2018-08-05T00:09:58.675352: step 10316, loss 0.587581.
Train: 2018-08-05T00:09:58.925265: step 10317, loss 0.537187.
Train: 2018-08-05T00:09:59.190856: step 10318, loss 0.579194.
Train: 2018-08-05T00:09:59.440797: step 10319, loss 0.528761.
Train: 2018-08-05T00:09:59.706354: step 10320, loss 0.587613.
Test: 2018-08-05T00:10:00.956037: step 10320, loss 0.548723.
Train: 2018-08-05T00:10:01.190389: step 10321, loss 0.58761.
Train: 2018-08-05T00:10:01.440329: step 10322, loss 0.587593.
Train: 2018-08-05T00:10:01.690271: step 10323, loss 0.604343.
Train: 2018-08-05T00:10:01.940213: step 10324, loss 0.503816.
Train: 2018-08-05T00:10:02.190124: step 10325, loss 0.528965.
Train: 2018-08-05T00:10:02.440095: step 10326, loss 0.503898.
Train: 2018-08-05T00:10:02.690037: step 10327, loss 0.604242.
Train: 2018-08-05T00:10:02.939978: step 10328, loss 0.554043.
Train: 2018-08-05T00:10:03.189890: step 10329, loss 0.528954.
Train: 2018-08-05T00:10:03.439862: step 10330, loss 0.537302.
Test: 2018-08-05T00:10:04.689538: step 10330, loss 0.548222.
Train: 2018-08-05T00:10:04.923860: step 10331, loss 0.537276.
Train: 2018-08-05T00:10:05.173800: step 10332, loss 0.604322.
Train: 2018-08-05T00:10:05.423742: step 10333, loss 0.646249.
Train: 2018-08-05T00:10:05.689334: step 10334, loss 0.528932.
Train: 2018-08-05T00:10:05.939276: step 10335, loss 0.654356.
Train: 2018-08-05T00:10:06.189217: step 10336, loss 0.562433.
Train: 2018-08-05T00:10:06.439162: step 10337, loss 0.520925.
Train: 2018-08-05T00:10:06.689100: step 10338, loss 0.612206.
Train: 2018-08-05T00:10:06.939012: step 10339, loss 0.562491.
Train: 2018-08-05T00:10:07.188984: step 10340, loss 0.529542.
Test: 2018-08-05T00:10:08.438660: step 10340, loss 0.548788.
Train: 2018-08-05T00:10:08.673012: step 10341, loss 0.603671.
Train: 2018-08-05T00:10:08.922923: step 10342, loss 0.537934.
Train: 2018-08-05T00:10:09.188485: step 10343, loss 0.537994.
Train: 2018-08-05T00:10:09.469669: step 10344, loss 0.554398.
Train: 2018-08-05T00:10:09.719636: step 10345, loss 0.587122.
Train: 2018-08-05T00:10:09.969582: step 10346, loss 0.513604.
Train: 2018-08-05T00:10:10.219494: step 10347, loss 0.529938.
Train: 2018-08-05T00:10:10.485086: step 10348, loss 0.529907.
Train: 2018-08-05T00:10:10.735028: step 10349, loss 0.529845.
Train: 2018-08-05T00:10:10.984970: step 10350, loss 0.578962.
Test: 2018-08-05T00:10:12.234646: step 10350, loss 0.548833.
Train: 2018-08-05T00:10:12.468998: step 10351, loss 0.595396.
Train: 2018-08-05T00:10:12.734559: step 10352, loss 0.546112.
Train: 2018-08-05T00:10:12.984502: step 10353, loss 0.578981.
Train: 2018-08-05T00:10:13.234442: step 10354, loss 0.529637.
Train: 2018-08-05T00:10:13.484384: step 10355, loss 0.57899.
Train: 2018-08-05T00:10:13.734296: step 10356, loss 0.644877.
Train: 2018-08-05T00:10:13.984273: step 10357, loss 0.537868.
Train: 2018-08-05T00:10:14.234208: step 10358, loss 0.595405.
Train: 2018-08-05T00:10:14.484121: step 10359, loss 0.52155.
Train: 2018-08-05T00:10:14.734092: step 10360, loss 0.587158.
Test: 2018-08-05T00:10:15.999389: step 10360, loss 0.548711.
Train: 2018-08-05T00:10:16.233711: step 10361, loss 0.587142.
Train: 2018-08-05T00:10:16.483676: step 10362, loss 0.554415.
Train: 2018-08-05T00:10:16.733623: step 10363, loss 0.513608.
Train: 2018-08-05T00:10:16.983535: step 10364, loss 0.619768.
Train: 2018-08-05T00:10:17.233475: step 10365, loss 0.529996.
Train: 2018-08-05T00:10:17.483447: step 10366, loss 0.530015.
Train: 2018-08-05T00:10:17.733389: step 10367, loss 0.505532.
Train: 2018-08-05T00:10:17.983333: step 10368, loss 0.5626.
Train: 2018-08-05T00:10:18.233272: step 10369, loss 0.554403.
Train: 2018-08-05T00:10:18.483184: step 10370, loss 0.595346.
Test: 2018-08-05T00:10:19.732891: step 10370, loss 0.549059.
Train: 2018-08-05T00:10:19.982862: step 10371, loss 0.578961.
Train: 2018-08-05T00:10:20.232804: step 10372, loss 0.611762.
Train: 2018-08-05T00:10:20.482745: step 10373, loss 0.587146.
Train: 2018-08-05T00:10:20.732687: step 10374, loss 0.587124.
Train: 2018-08-05T00:10:20.982628: step 10375, loss 0.538124.
Train: 2018-08-05T00:10:21.232570: step 10376, loss 0.530016.
Train: 2018-08-05T00:10:21.482512: step 10377, loss 0.570776.
Train: 2018-08-05T00:10:21.732453: step 10378, loss 0.587067.
Train: 2018-08-05T00:10:21.982396: step 10379, loss 0.562644.
Train: 2018-08-05T00:10:22.232336: step 10380, loss 0.513879.
Test: 2018-08-05T00:10:23.513255: step 10380, loss 0.547946.
Train: 2018-08-05T00:10:23.763222: step 10381, loss 0.481329.
Train: 2018-08-05T00:10:24.013163: step 10382, loss 0.530022.
Train: 2018-08-05T00:10:24.263111: step 10383, loss 0.611636.
Train: 2018-08-05T00:10:24.513052: step 10384, loss 0.562581.
Train: 2018-08-05T00:10:24.778583: step 10385, loss 0.587152.
Train: 2018-08-05T00:10:25.012906: step 10386, loss 0.595358.
Train: 2018-08-05T00:10:25.262845: step 10387, loss 0.562567.
Train: 2018-08-05T00:10:25.512817: step 10388, loss 0.570764.
Train: 2018-08-05T00:10:25.762759: step 10389, loss 0.570764.
Train: 2018-08-05T00:10:26.028291: step 10390, loss 0.562583.
Test: 2018-08-05T00:10:27.277998: step 10390, loss 0.548745.
Train: 2018-08-05T00:10:27.512357: step 10391, loss 0.546236.
Train: 2018-08-05T00:10:27.762291: step 10392, loss 0.562592.
Train: 2018-08-05T00:10:28.012202: step 10393, loss 0.521726.
Train: 2018-08-05T00:10:28.262174: step 10394, loss 0.578946.
Train: 2018-08-05T00:10:28.512114: step 10395, loss 0.538036.
Train: 2018-08-05T00:10:28.762044: step 10396, loss 0.578954.
Train: 2018-08-05T00:10:29.012000: step 10397, loss 0.578957.
Train: 2018-08-05T00:10:29.261939: step 10398, loss 0.603529.
Train: 2018-08-05T00:10:29.511881: step 10399, loss 0.603493.
Train: 2018-08-05T00:10:29.777413: step 10400, loss 0.51361.
Test: 2018-08-05T00:10:31.027121: step 10400, loss 0.549358.
Train: 2018-08-05T00:10:31.901945: step 10401, loss 0.521808.
Train: 2018-08-05T00:10:32.167509: step 10402, loss 0.554444.
Train: 2018-08-05T00:10:32.417421: step 10403, loss 0.603445.
Train: 2018-08-05T00:10:32.667392: step 10404, loss 0.529946.
Train: 2018-08-05T00:10:32.917333: step 10405, loss 0.60344.
Train: 2018-08-05T00:10:33.167244: step 10406, loss 0.570772.
Train: 2018-08-05T00:10:33.417187: step 10407, loss 0.546308.
Train: 2018-08-05T00:10:33.667127: step 10408, loss 0.611537.
Train: 2018-08-05T00:10:33.917069: step 10409, loss 0.554498.
Train: 2018-08-05T00:10:34.167011: step 10410, loss 0.538258.
Test: 2018-08-05T00:10:35.432339: step 10410, loss 0.549643.
Train: 2018-08-05T00:10:35.666695: step 10411, loss 0.554528.
Train: 2018-08-05T00:10:35.916600: step 10412, loss 0.578912.
Train: 2018-08-05T00:10:36.166572: step 10413, loss 0.554541.
Train: 2018-08-05T00:10:36.416514: step 10414, loss 0.546424.
Train: 2018-08-05T00:10:36.666455: step 10415, loss 0.546415.
Train: 2018-08-05T00:10:36.916397: step 10416, loss 0.611419.
Train: 2018-08-05T00:10:37.166308: step 10417, loss 0.570783.
Train: 2018-08-05T00:10:37.416280: step 10418, loss 0.554576.
Train: 2018-08-05T00:10:37.619357: step 10419, loss 0.493435.
Train: 2018-08-05T00:10:37.869299: step 10420, loss 0.505765.
Test: 2018-08-05T00:10:39.118976: step 10420, loss 0.54825.
Train: 2018-08-05T00:10:39.353297: step 10421, loss 0.570841.
Train: 2018-08-05T00:10:39.603268: step 10422, loss 0.513569.
Train: 2018-08-05T00:10:39.868801: step 10423, loss 0.636449.
Train: 2018-08-05T00:10:40.103152: step 10424, loss 0.455919.
Train: 2018-08-05T00:10:40.353061: step 10425, loss 0.504935.
Train: 2018-08-05T00:10:40.603033: step 10426, loss 0.578867.
Train: 2018-08-05T00:10:40.852975: step 10427, loss 0.578887.
Train: 2018-08-05T00:10:41.102886: step 10428, loss 0.562618.
Train: 2018-08-05T00:10:41.352858: step 10429, loss 0.595596.
Train: 2018-08-05T00:10:41.602800: step 10430, loss 0.603777.
Test: 2018-08-05T00:10:42.868098: step 10430, loss 0.547506.
Train: 2018-08-05T00:10:43.118039: step 10431, loss 0.578983.
Train: 2018-08-05T00:10:43.368007: step 10432, loss 0.604567.
Train: 2018-08-05T00:10:43.617955: step 10433, loss 0.512327.
Train: 2018-08-05T00:10:43.867863: step 10434, loss 0.495629.
Train: 2018-08-05T00:10:44.117805: step 10435, loss 0.571678.
Train: 2018-08-05T00:10:44.367777: step 10436, loss 0.562325.
Train: 2018-08-05T00:10:44.617718: step 10437, loss 0.478648.
Train: 2018-08-05T00:10:44.867629: step 10438, loss 0.554077.
Train: 2018-08-05T00:10:45.117601: step 10439, loss 0.554032.
Train: 2018-08-05T00:10:45.367543: step 10440, loss 0.46137.
Test: 2018-08-05T00:10:46.617220: step 10440, loss 0.547649.
Train: 2018-08-05T00:10:46.851565: step 10441, loss 0.536831.
Train: 2018-08-05T00:10:47.101512: step 10442, loss 0.587483.
Train: 2018-08-05T00:10:47.351449: step 10443, loss 0.529001.
Train: 2018-08-05T00:10:47.601395: step 10444, loss 0.570772.
Train: 2018-08-05T00:10:47.851336: step 10445, loss 0.493788.
Train: 2018-08-05T00:10:48.101277: step 10446, loss 0.509175.
Train: 2018-08-05T00:10:48.351219: step 10447, loss 0.563064.
Train: 2018-08-05T00:10:48.601160: step 10448, loss 0.545871.
Train: 2018-08-05T00:10:48.851102: step 10449, loss 0.568888.
Train: 2018-08-05T00:10:49.101044: step 10450, loss 0.578749.
Test: 2018-08-05T00:10:50.335099: step 10450, loss 0.549459.
Train: 2018-08-05T00:10:50.585065: step 10451, loss 0.493846.
Train: 2018-08-05T00:10:50.835007: step 10452, loss 0.617904.
Train: 2018-08-05T00:10:51.084954: step 10453, loss 0.544491.
Train: 2018-08-05T00:10:51.334866: step 10454, loss 0.543926.
Train: 2018-08-05T00:10:51.600429: step 10455, loss 0.554191.
Train: 2018-08-05T00:10:51.834780: step 10456, loss 0.510219.
Train: 2018-08-05T00:10:52.084689: step 10457, loss 0.56205.
Train: 2018-08-05T00:10:52.334632: step 10458, loss 0.580119.
Train: 2018-08-05T00:10:52.584602: step 10459, loss 0.53629.
Train: 2018-08-05T00:10:52.850135: step 10460, loss 0.518705.
Test: 2018-08-05T00:10:54.099843: step 10460, loss 0.547652.
Train: 2018-08-05T00:10:54.334164: step 10461, loss 0.597397.
Train: 2018-08-05T00:10:54.584135: step 10462, loss 0.527883.
Train: 2018-08-05T00:10:54.834076: step 10463, loss 0.53628.
Train: 2018-08-05T00:10:55.099608: step 10464, loss 0.63133.
Train: 2018-08-05T00:10:55.349549: step 10465, loss 0.476228.
Train: 2018-08-05T00:10:55.599522: step 10466, loss 0.562266.
Train: 2018-08-05T00:10:55.849473: step 10467, loss 0.527858.
Train: 2018-08-05T00:10:56.099405: step 10468, loss 0.49371.
Train: 2018-08-05T00:10:56.349348: step 10469, loss 0.502078.
Train: 2018-08-05T00:10:56.599287: step 10470, loss 0.639798.
Test: 2018-08-05T00:10:57.864586: step 10470, loss 0.547332.
Train: 2018-08-05T00:10:58.098937: step 10471, loss 0.579619.
Train: 2018-08-05T00:10:58.348873: step 10472, loss 0.596757.
Train: 2018-08-05T00:10:58.598815: step 10473, loss 0.545092.
Train: 2018-08-05T00:10:58.848761: step 10474, loss 0.553671.
Train: 2018-08-05T00:10:59.114294: step 10475, loss 0.605521.
Train: 2018-08-05T00:10:59.348644: step 10476, loss 0.545109.
Train: 2018-08-05T00:10:59.614177: step 10477, loss 0.545271.
Train: 2018-08-05T00:10:59.848498: step 10478, loss 0.536709.
Train: 2018-08-05T00:11:00.098469: step 10479, loss 0.511164.
Train: 2018-08-05T00:11:00.348380: step 10480, loss 0.613612.
Test: 2018-08-05T00:11:01.598087: step 10480, loss 0.547161.
Train: 2018-08-05T00:11:01.832408: step 10481, loss 0.536756.
Train: 2018-08-05T00:11:02.082379: step 10482, loss 0.511367.
Train: 2018-08-05T00:11:02.332321: step 10483, loss 0.613355.
Train: 2018-08-05T00:11:02.582262: step 10484, loss 0.587829.
Train: 2018-08-05T00:11:02.847794: step 10485, loss 0.545434.
Train: 2018-08-05T00:11:03.097766: step 10486, loss 0.66377.
Train: 2018-08-05T00:11:03.347707: step 10487, loss 0.596003.
Train: 2018-08-05T00:11:03.597649: step 10488, loss 0.554.
Train: 2018-08-05T00:11:03.847591: step 10489, loss 0.554071.
Train: 2018-08-05T00:11:04.097532: step 10490, loss 0.562412.
Test: 2018-08-05T00:11:05.347208: step 10490, loss 0.548646.
Train: 2018-08-05T00:11:05.612801: step 10491, loss 0.587308.
Train: 2018-08-05T00:11:05.862714: step 10492, loss 0.595446.
Train: 2018-08-05T00:11:06.112685: step 10493, loss 0.54623.
Train: 2018-08-05T00:11:06.362626: step 10494, loss 0.554386.
Train: 2018-08-05T00:11:06.612570: step 10495, loss 0.522079.
Train: 2018-08-05T00:11:06.878126: step 10496, loss 0.603422.
Train: 2018-08-05T00:11:07.128073: step 10497, loss 0.54635.
Train: 2018-08-05T00:11:07.378014: step 10498, loss 0.513886.
Train: 2018-08-05T00:11:07.643577: step 10499, loss 0.603288.
Train: 2018-08-05T00:11:07.909109: step 10500, loss 0.473428.
Test: 2018-08-05T00:11:09.158816: step 10500, loss 0.54874.
Train: 2018-08-05T00:11:10.033612: step 10501, loss 0.546467.
Train: 2018-08-05T00:11:10.283583: step 10502, loss 0.58707.
Train: 2018-08-05T00:11:10.549146: step 10503, loss 0.595195.
Train: 2018-08-05T00:11:10.783437: step 10504, loss 0.611406.
Train: 2018-08-05T00:11:11.033377: step 10505, loss 0.562614.
Train: 2018-08-05T00:11:11.283349: step 10506, loss 0.595134.
Train: 2018-08-05T00:11:11.533291: step 10507, loss 0.554574.
Train: 2018-08-05T00:11:11.783232: step 10508, loss 0.611228.
Train: 2018-08-05T00:11:12.033145: step 10509, loss 0.619171.
Train: 2018-08-05T00:11:12.298707: step 10510, loss 0.570791.
Test: 2018-08-05T00:11:13.548412: step 10510, loss 0.550202.
Train: 2018-08-05T00:11:13.782733: step 10511, loss 0.522774.
Train: 2018-08-05T00:11:14.048296: step 10512, loss 0.626838.
Train: 2018-08-05T00:11:14.313859: step 10513, loss 0.538998.
Train: 2018-08-05T00:11:14.548210: step 10514, loss 0.586845.
Train: 2018-08-05T00:11:14.798151: step 10515, loss 0.610596.
Train: 2018-08-05T00:11:15.063714: step 10516, loss 0.58677.
Train: 2018-08-05T00:11:15.313655: step 10517, loss 0.578862.
Train: 2018-08-05T00:11:15.563597: step 10518, loss 0.610312.
Train: 2018-08-05T00:11:15.813538: step 10519, loss 0.547556.
Train: 2018-08-05T00:11:16.063450: step 10520, loss 0.532038.
Test: 2018-08-05T00:11:17.328778: step 10520, loss 0.550462.
Train: 2018-08-05T00:11:17.563129: step 10521, loss 0.594486.
Train: 2018-08-05T00:11:17.813040: step 10522, loss 0.586682.
Train: 2018-08-05T00:11:18.063012: step 10523, loss 0.540101.
Train: 2018-08-05T00:11:18.328544: step 10524, loss 0.555652.
Train: 2018-08-05T00:11:18.562865: step 10525, loss 0.516921.
Train: 2018-08-05T00:11:18.828427: step 10526, loss 0.563397.
Train: 2018-08-05T00:11:19.078399: step 10527, loss 0.524529.
Train: 2018-08-05T00:11:19.328340: step 10528, loss 0.57111.
Train: 2018-08-05T00:11:19.578282: step 10529, loss 0.586694.
Train: 2018-08-05T00:11:19.828192: step 10530, loss 0.563251.
Test: 2018-08-05T00:11:21.077900: step 10530, loss 0.549603.
Train: 2018-08-05T00:11:21.312252: step 10531, loss 0.53973.
Train: 2018-08-05T00:11:21.562194: step 10532, loss 0.618114.
Train: 2018-08-05T00:11:21.812134: step 10533, loss 0.571016.
Train: 2018-08-05T00:11:22.062045: step 10534, loss 0.500259.
Train: 2018-08-05T00:11:22.312017: step 10535, loss 0.523698.
Train: 2018-08-05T00:11:22.561952: step 10536, loss 0.515595.
Train: 2018-08-05T00:11:22.811900: step 10537, loss 0.499422.
Train: 2018-08-05T00:11:23.061840: step 10538, loss 0.570873.
Train: 2018-08-05T00:11:23.311752: step 10539, loss 0.538721.
Train: 2018-08-05T00:11:23.561724: step 10540, loss 0.554668.
Test: 2018-08-05T00:11:24.842643: step 10540, loss 0.549319.
Train: 2018-08-05T00:11:25.076995: step 10541, loss 0.595131.
Train: 2018-08-05T00:11:25.326907: step 10542, loss 0.530073.
Train: 2018-08-05T00:11:25.576878: step 10543, loss 0.570769.
Train: 2018-08-05T00:11:25.842442: step 10544, loss 0.562558.
Train: 2018-08-05T00:11:26.092381: step 10545, loss 0.570757.
Train: 2018-08-05T00:11:26.357915: step 10546, loss 0.620238.
Train: 2018-08-05T00:11:26.592265: step 10547, loss 0.57901.
Train: 2018-08-05T00:11:26.842206: step 10548, loss 0.628552.
Train: 2018-08-05T00:11:27.092116: step 10549, loss 0.579001.
Train: 2018-08-05T00:11:27.357706: step 10550, loss 0.570758.
Test: 2018-08-05T00:11:28.591766: step 10550, loss 0.549386.
Train: 2018-08-05T00:11:28.841738: step 10551, loss 0.652932.
Train: 2018-08-05T00:11:29.091649: step 10552, loss 0.619884.
Train: 2018-08-05T00:11:29.341620: step 10553, loss 0.530037.
Train: 2018-08-05T00:11:29.591556: step 10554, loss 0.595146.
Train: 2018-08-05T00:11:29.841503: step 10555, loss 0.554628.
Train: 2018-08-05T00:11:30.107060: step 10556, loss 0.554698.
Train: 2018-08-05T00:11:30.341356: step 10557, loss 0.482409.
Train: 2018-08-05T00:11:30.591327: step 10558, loss 0.619049.
Train: 2018-08-05T00:11:30.841264: step 10559, loss 0.627001.
Train: 2018-08-05T00:11:31.106832: step 10560, loss 0.538874.
Test: 2018-08-05T00:11:32.340887: step 10560, loss 0.549383.
Train: 2018-08-05T00:11:32.606450: step 10561, loss 0.570879.
Train: 2018-08-05T00:11:32.856421: step 10562, loss 0.682425.
Train: 2018-08-05T00:11:33.106364: step 10563, loss 0.547137.
Train: 2018-08-05T00:11:33.371926: step 10564, loss 0.539352.
Train: 2018-08-05T00:11:33.621868: step 10565, loss 0.53157.
Train: 2018-08-05T00:11:33.871809: step 10566, loss 0.570994.
Train: 2018-08-05T00:11:34.121751: step 10567, loss 0.571005.
Train: 2018-08-05T00:11:34.371693: step 10568, loss 0.602438.
Train: 2018-08-05T00:11:34.621630: step 10569, loss 0.52398.
Train: 2018-08-05T00:11:34.824705: step 10570, loss 0.663544.
Test: 2018-08-05T00:11:36.090009: step 10570, loss 0.549829.
Train: 2018-08-05T00:11:36.324361: step 10571, loss 0.563247.
Train: 2018-08-05T00:11:36.574272: step 10572, loss 0.586693.
Train: 2018-08-05T00:11:36.824243: step 10573, loss 0.586684.
Train: 2018-08-05T00:11:37.074185: step 10574, loss 0.532325.
Train: 2018-08-05T00:11:37.324125: step 10575, loss 0.609937.
Train: 2018-08-05T00:11:37.589694: step 10576, loss 0.586667.
Train: 2018-08-05T00:11:37.839601: step 10577, loss 0.602107.
Train: 2018-08-05T00:11:38.089571: step 10578, loss 0.548139.
Train: 2018-08-05T00:11:38.355134: step 10579, loss 0.648177.
Train: 2018-08-05T00:11:38.605076: step 10580, loss 0.563657.
Test: 2018-08-05T00:11:39.854753: step 10580, loss 0.551306.
Train: 2018-08-05T00:11:40.089105: step 10581, loss 0.548433.
Train: 2018-08-05T00:11:40.339045: step 10582, loss 0.517979.
Train: 2018-08-05T00:11:40.588987: step 10583, loss 0.609539.
Train: 2018-08-05T00:11:40.838898: step 10584, loss 0.594272.
Train: 2018-08-05T00:11:41.104491: step 10585, loss 0.55619.
Train: 2018-08-05T00:11:41.338811: step 10586, loss 0.540983.
Train: 2018-08-05T00:11:41.588752: step 10587, loss 0.594264.
Train: 2018-08-05T00:11:41.838694: step 10588, loss 0.540956.
Train: 2018-08-05T00:11:42.104256: step 10589, loss 0.533284.
Train: 2018-08-05T00:11:42.354198: step 10590, loss 0.571369.
Test: 2018-08-05T00:11:43.603875: step 10590, loss 0.551778.
Train: 2018-08-05T00:11:43.838196: step 10591, loss 0.556026.
Train: 2018-08-05T00:11:44.088167: step 10592, loss 0.632694.
Train: 2018-08-05T00:11:44.338108: step 10593, loss 0.532899.
Train: 2018-08-05T00:11:44.588020: step 10594, loss 0.563577.
Train: 2018-08-05T00:11:44.837991: step 10595, loss 0.501883.
Train: 2018-08-05T00:11:45.087904: step 10596, loss 0.555725.
Train: 2018-08-05T00:11:45.337843: step 10597, loss 0.47022.
Train: 2018-08-05T00:11:45.587816: step 10598, loss 0.563266.
Train: 2018-08-05T00:11:45.837758: step 10599, loss 0.539597.
Train: 2018-08-05T00:11:46.103314: step 10600, loss 0.578862.
Test: 2018-08-05T00:11:47.352997: step 10600, loss 0.54935.
Train: 2018-08-05T00:11:48.196580: step 10601, loss 0.562975.
Train: 2018-08-05T00:11:48.446521: step 10602, loss 0.491106.
Train: 2018-08-05T00:11:48.696462: step 10603, loss 0.570848.
Train: 2018-08-05T00:11:48.946404: step 10604, loss 0.506262.
Train: 2018-08-05T00:11:49.196317: step 10605, loss 0.587037.
Train: 2018-08-05T00:11:49.446287: step 10606, loss 0.562624.
Train: 2018-08-05T00:11:49.696199: step 10607, loss 0.537984.
Train: 2018-08-05T00:11:49.946170: step 10608, loss 0.529587.
Train: 2018-08-05T00:11:50.196111: step 10609, loss 0.537668.
Train: 2018-08-05T00:11:50.446053: step 10610, loss 0.570782.
Test: 2018-08-05T00:11:51.695729: step 10610, loss 0.548102.
Train: 2018-08-05T00:11:51.930082: step 10611, loss 0.520689.
Train: 2018-08-05T00:11:52.180023: step 10612, loss 0.570807.
Train: 2018-08-05T00:11:52.429933: step 10613, loss 0.537136.
Train: 2018-08-05T00:11:52.679874: step 10614, loss 0.63835.
Train: 2018-08-05T00:11:52.929847: step 10615, loss 0.537002.
Train: 2018-08-05T00:11:53.179788: step 10616, loss 0.553889.
Train: 2018-08-05T00:11:53.429730: step 10617, loss 0.545394.
Train: 2018-08-05T00:11:53.679641: step 10618, loss 0.528381.
Train: 2018-08-05T00:11:53.929583: step 10619, loss 0.545328.
Train: 2018-08-05T00:11:54.195175: step 10620, loss 0.553817.
Test: 2018-08-05T00:11:55.444852: step 10620, loss 0.548473.
Train: 2018-08-05T00:11:55.694818: step 10621, loss 0.596488.
Train: 2018-08-05T00:11:55.944766: step 10622, loss 0.545254.
Train: 2018-08-05T00:11:56.194707: step 10623, loss 0.570883.
Train: 2018-08-05T00:11:56.444648: step 10624, loss 0.545246.
Train: 2018-08-05T00:11:56.694590: step 10625, loss 0.54524.
Train: 2018-08-05T00:11:56.944501: step 10626, loss 0.579439.
Train: 2018-08-05T00:11:57.194473: step 10627, loss 0.570885.
Train: 2018-08-05T00:11:57.444414: step 10628, loss 0.596508.
Train: 2018-08-05T00:11:57.709977: step 10629, loss 0.604983.
Train: 2018-08-05T00:11:57.959919: step 10630, loss 0.621888.
Test: 2018-08-05T00:11:59.209596: step 10630, loss 0.549373.
Train: 2018-08-05T00:11:59.459562: step 10631, loss 0.536934.
Train: 2018-08-05T00:11:59.709509: step 10632, loss 0.545471.
Train: 2018-08-05T00:11:59.959450: step 10633, loss 0.511829.
Train: 2018-08-05T00:12:00.224983: step 10634, loss 0.528732.
Train: 2018-08-05T00:12:00.490576: step 10635, loss 0.553977.
Train: 2018-08-05T00:12:00.756141: step 10636, loss 0.469972.
Train: 2018-08-05T00:12:01.006080: step 10637, loss 0.671751.
Train: 2018-08-05T00:12:01.256016: step 10638, loss 0.595992.
Train: 2018-08-05T00:12:01.505966: step 10639, loss 0.554012.
Train: 2018-08-05T00:12:01.755904: step 10640, loss 0.612613.
Test: 2018-08-05T00:12:03.005583: step 10640, loss 0.548106.
Train: 2018-08-05T00:12:03.255549: step 10641, loss 0.495678.
Train: 2018-08-05T00:12:03.505494: step 10642, loss 0.570764.
Train: 2018-08-05T00:12:03.755436: step 10643, loss 0.595723.
Train: 2018-08-05T00:12:04.005378: step 10644, loss 0.529246.
Train: 2018-08-05T00:12:04.255320: step 10645, loss 0.545882.
Train: 2018-08-05T00:12:04.505262: step 10646, loss 0.628749.
Train: 2018-08-05T00:12:04.755173: step 10647, loss 0.612082.
Train: 2018-08-05T00:12:05.005114: step 10648, loss 0.504862.
Train: 2018-08-05T00:12:05.255056: step 10649, loss 0.554314.
Train: 2018-08-05T00:12:05.520617: step 10650, loss 0.578971.
Test: 2018-08-05T00:12:06.770325: step 10650, loss 0.548311.
Train: 2018-08-05T00:12:07.004676: step 10651, loss 0.603549.
Train: 2018-08-05T00:12:07.254617: step 10652, loss 0.529889.
Train: 2018-08-05T00:12:07.504558: step 10653, loss 0.570771.
Train: 2018-08-05T00:12:07.754469: step 10654, loss 0.562625.
Train: 2018-08-05T00:12:08.004444: step 10655, loss 0.554502.
Train: 2018-08-05T00:12:08.254385: step 10656, loss 0.505742.
Train: 2018-08-05T00:12:08.504295: step 10657, loss 0.505715.
Train: 2018-08-05T00:12:08.754266: step 10658, loss 0.595219.
Train: 2018-08-05T00:12:09.019833: step 10659, loss 0.570774.
Train: 2018-08-05T00:12:09.269741: step 10660, loss 0.53814.
Test: 2018-08-05T00:12:10.566311: step 10660, loss 0.549152.
Train: 2018-08-05T00:12:10.831873: step 10661, loss 0.61977.
Train: 2018-08-05T00:12:11.097436: step 10662, loss 0.652403.
Train: 2018-08-05T00:12:11.347408: step 10663, loss 0.505636.
Train: 2018-08-05T00:12:11.597350: step 10664, loss 0.578916.
Train: 2018-08-05T00:12:11.847291: step 10665, loss 0.562659.
Train: 2018-08-05T00:12:12.097233: step 10666, loss 0.546437.
Train: 2018-08-05T00:12:12.347174: step 10667, loss 0.514002.
Train: 2018-08-05T00:12:12.597085: step 10668, loss 0.627617.
Train: 2018-08-05T00:12:12.847057: step 10669, loss 0.505902.
Train: 2018-08-05T00:12:13.096994: step 10670, loss 0.538325.
Test: 2018-08-05T00:12:14.346676: step 10670, loss 0.547781.
Train: 2018-08-05T00:12:14.596648: step 10671, loss 0.432658.
Train: 2018-08-05T00:12:14.846558: step 10672, loss 0.578931.
Train: 2018-08-05T00:12:15.096531: step 10673, loss 0.529837.
Train: 2018-08-05T00:12:15.346466: step 10674, loss 0.595408.
Train: 2018-08-05T00:12:15.596413: step 10675, loss 0.529573.
Train: 2018-08-05T00:12:15.846353: step 10676, loss 0.512927.
Train: 2018-08-05T00:12:16.096298: step 10677, loss 0.521003.
Train: 2018-08-05T00:12:16.346238: step 10678, loss 0.604073.
Train: 2018-08-05T00:12:16.596179: step 10679, loss 0.554069.
Train: 2018-08-05T00:12:16.846092: step 10680, loss 0.60426.
Test: 2018-08-05T00:12:18.095798: step 10680, loss 0.548015.
Train: 2018-08-05T00:12:18.345739: step 10681, loss 0.554017.
Train: 2018-08-05T00:12:18.580061: step 10682, loss 0.612734.
Train: 2018-08-05T00:12:18.845652: step 10683, loss 0.545617.
Train: 2018-08-05T00:12:19.079944: step 10684, loss 0.545617.
Train: 2018-08-05T00:12:19.329884: step 10685, loss 0.595952.
Train: 2018-08-05T00:12:19.579856: step 10686, loss 0.562396.
Train: 2018-08-05T00:12:19.829798: step 10687, loss 0.621043.
Train: 2018-08-05T00:12:20.079709: step 10688, loss 0.554052.
Train: 2018-08-05T00:12:20.329681: step 10689, loss 0.595799.
Train: 2018-08-05T00:12:20.579621: step 10690, loss 0.545794.
Test: 2018-08-05T00:12:21.829298: step 10690, loss 0.548201.
Train: 2018-08-05T00:12:22.079241: step 10691, loss 0.529232.
Train: 2018-08-05T00:12:22.313561: step 10692, loss 0.653716.
Train: 2018-08-05T00:12:22.563503: step 10693, loss 0.645177.
Train: 2018-08-05T00:12:22.813475: step 10694, loss 0.587215.
Train: 2018-08-05T00:12:23.063415: step 10695, loss 0.529829.
Train: 2018-08-05T00:12:23.313358: step 10696, loss 0.587085.
Train: 2018-08-05T00:12:23.563298: step 10697, loss 0.562663.
Train: 2018-08-05T00:12:23.813239: step 10698, loss 0.514144.
Train: 2018-08-05T00:12:24.063181: step 10699, loss 0.562732.
Train: 2018-08-05T00:12:24.313122: step 10700, loss 0.57888.
Test: 2018-08-05T00:12:25.578420: step 10700, loss 0.549158.
Train: 2018-08-05T00:12:26.515732: step 10701, loss 0.594968.
Train: 2018-08-05T00:12:26.765673: step 10702, loss 0.586896.
Train: 2018-08-05T00:12:27.015614: step 10703, loss 0.666913.
Train: 2018-08-05T00:12:27.265551: step 10704, loss 0.554964.
Train: 2018-08-05T00:12:27.531089: step 10705, loss 0.523334.
Train: 2018-08-05T00:12:27.781054: step 10706, loss 0.563037.
Train: 2018-08-05T00:12:28.031001: step 10707, loss 0.642016.
Train: 2018-08-05T00:12:28.280943: step 10708, loss 0.531683.
Train: 2018-08-05T00:12:28.530885: step 10709, loss 0.594565.
Train: 2018-08-05T00:12:28.780826: step 10710, loss 0.508471.
Test: 2018-08-05T00:12:30.030503: step 10710, loss 0.549658.
Train: 2018-08-05T00:12:30.264854: step 10711, loss 0.539802.
Train: 2018-08-05T00:12:30.514796: step 10712, loss 0.524167.
Train: 2018-08-05T00:12:30.764736: step 10713, loss 0.531921.
Train: 2018-08-05T00:12:31.014678: step 10714, loss 0.578875.
Train: 2018-08-05T00:12:31.264619: step 10715, loss 0.571014.
Train: 2018-08-05T00:12:31.514561: step 10716, loss 0.610345.
Train: 2018-08-05T00:12:31.764502: step 10717, loss 0.523759.
Train: 2018-08-05T00:12:32.030065: step 10718, loss 0.484249.
Train: 2018-08-05T00:12:32.264356: step 10719, loss 0.523472.
Train: 2018-08-05T00:12:32.514327: step 10720, loss 0.578858.
Test: 2018-08-05T00:12:33.795246: step 10720, loss 0.548671.
Train: 2018-08-05T00:12:33.967081: step 10721, loss 0.596938.
Train: 2018-08-05T00:12:34.217025: step 10722, loss 0.522892.
Train: 2018-08-05T00:12:34.466964: step 10723, loss 0.546778.
Train: 2018-08-05T00:12:34.716937: step 10724, loss 0.603028.
Train: 2018-08-05T00:12:34.966877: step 10725, loss 0.578883.
Train: 2018-08-05T00:12:35.216789: step 10726, loss 0.58697.
Train: 2018-08-05T00:12:35.466761: step 10727, loss 0.554623.
Train: 2018-08-05T00:12:35.732323: step 10728, loss 0.562699.
Train: 2018-08-05T00:12:35.966613: step 10729, loss 0.449194.
Train: 2018-08-05T00:12:36.216585: step 10730, loss 0.497555.
Test: 2018-08-05T00:12:37.466262: step 10730, loss 0.549127.
Train: 2018-08-05T00:12:37.700584: step 10731, loss 0.644346.
Train: 2018-08-05T00:12:37.950524: step 10732, loss 0.595348.
Train: 2018-08-05T00:12:38.200496: step 10733, loss 0.652821.
Train: 2018-08-05T00:12:38.450407: step 10734, loss 0.537968.
Train: 2018-08-05T00:12:38.700379: step 10735, loss 0.587155.
Train: 2018-08-05T00:12:38.950319: step 10736, loss 0.595332.
Train: 2018-08-05T00:12:39.200261: step 10737, loss 0.562591.
Train: 2018-08-05T00:12:39.465824: step 10738, loss 0.595266.
Train: 2018-08-05T00:12:39.700144: step 10739, loss 0.521885.
Train: 2018-08-05T00:12:39.950086: step 10740, loss 0.595203.
Test: 2018-08-05T00:12:41.199763: step 10740, loss 0.548705.
Train: 2018-08-05T00:12:41.434112: step 10741, loss 0.522011.
Train: 2018-08-05T00:12:41.684055: step 10742, loss 0.505782.
Train: 2018-08-05T00:12:41.933966: step 10743, loss 0.562648.
Train: 2018-08-05T00:12:42.183906: step 10744, loss 0.562638.
Train: 2018-08-05T00:12:42.433850: step 10745, loss 0.554482.
Train: 2018-08-05T00:12:42.699415: step 10746, loss 0.636012.
Train: 2018-08-05T00:12:42.949383: step 10747, loss 0.603366.
Train: 2018-08-05T00:12:43.199324: step 10748, loss 0.570782.
Train: 2018-08-05T00:12:43.464887: step 10749, loss 0.538312.
Train: 2018-08-05T00:12:43.714830: step 10750, loss 0.578902.
Test: 2018-08-05T00:12:44.964506: step 10750, loss 0.549726.
Train: 2018-08-05T00:12:45.198857: step 10751, loss 0.554595.
Train: 2018-08-05T00:12:45.448793: step 10752, loss 0.595081.
Train: 2018-08-05T00:12:45.698739: step 10753, loss 0.603131.
Train: 2018-08-05T00:12:45.948651: step 10754, loss 0.506319.
Train: 2018-08-05T00:12:46.198622: step 10755, loss 0.554708.
Train: 2018-08-05T00:12:46.448564: step 10756, loss 0.554715.
Train: 2018-08-05T00:12:46.698509: step 10757, loss 0.586931.
Train: 2018-08-05T00:12:46.948446: step 10758, loss 0.603024.
Train: 2018-08-05T00:12:47.198388: step 10759, loss 0.611024.
Train: 2018-08-05T00:12:47.448330: step 10760, loss 0.578867.
Test: 2018-08-05T00:12:48.698007: step 10760, loss 0.549505.
Train: 2018-08-05T00:12:48.932357: step 10761, loss 0.530867.
Train: 2018-08-05T00:12:49.182299: step 10762, loss 0.530929.
Train: 2018-08-05T00:12:49.432211: step 10763, loss 0.586848.
Train: 2018-08-05T00:12:49.682151: step 10764, loss 0.594821.
Train: 2018-08-05T00:12:49.932093: step 10765, loss 0.515101.
Train: 2018-08-05T00:12:50.182064: step 10766, loss 0.554948.
Train: 2018-08-05T00:12:50.432006: step 10767, loss 0.57886.
Train: 2018-08-05T00:12:50.681948: step 10768, loss 0.626706.
Train: 2018-08-05T00:12:50.931883: step 10769, loss 0.539042.
Train: 2018-08-05T00:12:51.181824: step 10770, loss 0.531104.
Test: 2018-08-05T00:12:52.443017: step 10770, loss 0.550369.
Train: 2018-08-05T00:12:52.677373: step 10771, loss 0.562934.
Train: 2018-08-05T00:12:52.942900: step 10772, loss 0.60276.
Train: 2018-08-05T00:12:53.177222: step 10773, loss 0.586823.
Train: 2018-08-05T00:12:53.427162: step 10774, loss 0.54703.
Train: 2018-08-05T00:12:53.677138: step 10775, loss 0.554992.
Train: 2018-08-05T00:12:53.927079: step 10776, loss 0.547032.
Train: 2018-08-05T00:12:54.176988: step 10777, loss 0.602744.
Train: 2018-08-05T00:12:54.426929: step 10778, loss 0.56294.
Train: 2018-08-05T00:12:54.676904: step 10779, loss 0.547025.
Train: 2018-08-05T00:12:54.926811: step 10780, loss 0.53905.
Test: 2018-08-05T00:12:56.176518: step 10780, loss 0.54924.
Train: 2018-08-05T00:12:56.410873: step 10781, loss 0.570889.
Train: 2018-08-05T00:12:56.660810: step 10782, loss 0.594816.
Train: 2018-08-05T00:12:56.910723: step 10783, loss 0.586839.
Train: 2018-08-05T00:12:57.160693: step 10784, loss 0.570884.
Train: 2018-08-05T00:12:57.410637: step 10785, loss 0.531023.
Train: 2018-08-05T00:12:57.660566: step 10786, loss 0.57886.
Train: 2018-08-05T00:12:57.910518: step 10787, loss 0.57886.
Train: 2018-08-05T00:12:58.160461: step 10788, loss 0.515032.
Train: 2018-08-05T00:12:58.410401: step 10789, loss 0.53892.
Train: 2018-08-05T00:12:58.660342: step 10790, loss 0.546853.
Test: 2018-08-05T00:12:59.894398: step 10790, loss 0.548327.
Train: 2018-08-05T00:13:00.144369: step 10791, loss 0.554808.
Train: 2018-08-05T00:13:00.394306: step 10792, loss 0.586907.
Train: 2018-08-05T00:13:00.644253: step 10793, loss 0.522534.
Train: 2018-08-05T00:13:00.894194: step 10794, loss 0.57888.
Train: 2018-08-05T00:13:01.144135: step 10795, loss 0.514228.
Train: 2018-08-05T00:13:01.394046: step 10796, loss 0.53026.
Train: 2018-08-05T00:13:01.659642: step 10797, loss 0.538258.
Train: 2018-08-05T00:13:01.893964: step 10798, loss 0.546246.
Train: 2018-08-05T00:13:02.143901: step 10799, loss 0.554396.
Train: 2018-08-05T00:13:02.393843: step 10800, loss 0.546109.
Test: 2018-08-05T00:13:03.643520: step 10800, loss 0.549855.
Train: 2018-08-05T00:13:04.518316: step 10801, loss 0.529483.
Train: 2018-08-05T00:13:04.768287: step 10802, loss 0.579165.
Train: 2018-08-05T00:13:05.018228: step 10803, loss 0.554139.
Train: 2018-08-05T00:13:05.283762: step 10804, loss 0.545798.
Train: 2018-08-05T00:13:05.533703: step 10805, loss 0.554054.
Train: 2018-08-05T00:13:05.783676: step 10806, loss 0.579149.
Train: 2018-08-05T00:13:06.033618: step 10807, loss 0.570769.
Train: 2018-08-05T00:13:06.283528: step 10808, loss 0.537205.
Train: 2018-08-05T00:13:06.533469: step 10809, loss 0.587618.
Train: 2018-08-05T00:13:06.799065: step 10810, loss 0.579213.
Test: 2018-08-05T00:13:08.033117: step 10810, loss 0.548725.
Train: 2018-08-05T00:13:08.298713: step 10811, loss 0.545568.
Train: 2018-08-05T00:13:08.548621: step 10812, loss 0.579202.
Train: 2018-08-05T00:13:08.798574: step 10813, loss 0.503529.
Train: 2018-08-05T00:13:09.048505: step 10814, loss 0.646532.
Train: 2018-08-05T00:13:09.298446: step 10815, loss 0.503553.
Train: 2018-08-05T00:13:09.548387: step 10816, loss 0.570788.
Train: 2018-08-05T00:13:09.798363: step 10817, loss 0.612799.
Train: 2018-08-05T00:13:10.048270: step 10818, loss 0.579171.
Train: 2018-08-05T00:13:10.298213: step 10819, loss 0.512164.
Train: 2018-08-05T00:13:10.563774: step 10820, loss 0.595876.
Test: 2018-08-05T00:13:11.813482: step 10820, loss 0.54808.
Train: 2018-08-05T00:13:12.063454: step 10821, loss 0.503944.
Train: 2018-08-05T00:13:12.313365: step 10822, loss 0.595827.
Train: 2018-08-05T00:13:12.563337: step 10823, loss 0.579111.
Train: 2018-08-05T00:13:12.813248: step 10824, loss 0.520777.
Train: 2018-08-05T00:13:13.063219: step 10825, loss 0.587414.
Train: 2018-08-05T00:13:13.313160: step 10826, loss 0.579075.
Train: 2018-08-05T00:13:13.563102: step 10827, loss 0.562454.
Train: 2018-08-05T00:13:13.813044: step 10828, loss 0.496123.
Train: 2018-08-05T00:13:14.062985: step 10829, loss 0.603936.
Train: 2018-08-05T00:13:14.312921: step 10830, loss 0.463034.
Test: 2018-08-05T00:13:15.562603: step 10830, loss 0.549939.
Train: 2018-08-05T00:13:15.796925: step 10831, loss 0.587356.
Train: 2018-08-05T00:13:16.046900: step 10832, loss 0.612272.
Train: 2018-08-05T00:13:16.296841: step 10833, loss 0.595649.
Train: 2018-08-05T00:13:16.562370: step 10834, loss 0.521051.
Train: 2018-08-05T00:13:16.812341: step 10835, loss 0.628719.
Train: 2018-08-05T00:13:17.062283: step 10836, loss 0.570756.
Train: 2018-08-05T00:13:17.312195: step 10837, loss 0.579003.
Train: 2018-08-05T00:13:17.562166: step 10838, loss 0.570758.
Train: 2018-08-05T00:13:17.812107: step 10839, loss 0.505098.
Train: 2018-08-05T00:13:18.062049: step 10840, loss 0.578963.
Test: 2018-08-05T00:13:19.311726: step 10840, loss 0.549079.
Train: 2018-08-05T00:13:19.546081: step 10841, loss 0.578955.
Train: 2018-08-05T00:13:19.795988: step 10842, loss 0.562586.
Train: 2018-08-05T00:13:20.061580: step 10843, loss 0.578939.
Train: 2018-08-05T00:13:20.311493: step 10844, loss 0.521833.
Train: 2018-08-05T00:13:20.561458: step 10845, loss 0.562621.
Train: 2018-08-05T00:13:20.826996: step 10846, loss 0.505581.
Train: 2018-08-05T00:13:21.076969: step 10847, loss 0.57893.
Train: 2018-08-05T00:13:21.326879: step 10848, loss 0.521801.
Train: 2018-08-05T00:13:21.576821: step 10849, loss 0.59529.
Train: 2018-08-05T00:13:21.826793: step 10850, loss 0.562589.
Test: 2018-08-05T00:13:23.076469: step 10850, loss 0.548734.
Train: 2018-08-05T00:13:23.310790: step 10851, loss 0.50532.
Train: 2018-08-05T00:13:23.560732: step 10852, loss 0.562567.
Train: 2018-08-05T00:13:23.841916: step 10853, loss 0.521516.
Train: 2018-08-05T00:13:24.076268: step 10854, loss 0.595436.
Train: 2018-08-05T00:13:24.341829: step 10855, loss 0.628415.
Train: 2018-08-05T00:13:24.591764: step 10856, loss 0.562523.
Train: 2018-08-05T00:13:24.841682: step 10857, loss 0.570758.
Train: 2018-08-05T00:13:25.091652: step 10858, loss 0.529643.
Train: 2018-08-05T00:13:25.341594: step 10859, loss 0.55431.
Train: 2018-08-05T00:13:25.591536: step 10860, loss 0.61189.
Test: 2018-08-05T00:13:26.841213: step 10860, loss 0.548815.
Train: 2018-08-05T00:13:27.075533: step 10861, loss 0.496793.
Train: 2018-08-05T00:13:27.325504: step 10862, loss 0.636548.
Train: 2018-08-05T00:13:27.575446: step 10863, loss 0.546119.
Train: 2018-08-05T00:13:27.825387: step 10864, loss 0.480479.
Train: 2018-08-05T00:13:28.075300: step 10865, loss 0.513237.
Train: 2018-08-05T00:13:28.325266: step 10866, loss 0.521338.
Train: 2018-08-05T00:13:28.575212: step 10867, loss 0.545978.
Train: 2018-08-05T00:13:28.825154: step 10868, loss 0.529329.
Train: 2018-08-05T00:13:29.075097: step 10869, loss 0.620669.
Train: 2018-08-05T00:13:29.356280: step 10870, loss 0.570758.
Test: 2018-08-05T00:13:30.621578: step 10870, loss 0.550431.
Train: 2018-08-05T00:13:30.887170: step 10871, loss 0.595769.
Train: 2018-08-05T00:13:31.090248: step 10872, loss 0.491317.
Train: 2018-08-05T00:13:31.340189: step 10873, loss 0.58746.
Train: 2018-08-05T00:13:31.605752: step 10874, loss 0.554065.
Train: 2018-08-05T00:13:31.855688: step 10875, loss 0.537337.
Train: 2018-08-05T00:13:32.105606: step 10876, loss 0.629347.
Train: 2018-08-05T00:13:32.355576: step 10877, loss 0.56241.
Train: 2018-08-05T00:13:32.605489: step 10878, loss 0.604195.
Train: 2018-08-05T00:13:32.855459: step 10879, loss 0.620816.
Train: 2018-08-05T00:13:33.105396: step 10880, loss 0.579078.
Test: 2018-08-05T00:13:34.370700: step 10880, loss 0.549386.
Train: 2018-08-05T00:13:34.605050: step 10881, loss 0.620498.
Train: 2018-08-05T00:13:34.854991: step 10882, loss 0.59552.
Train: 2018-08-05T00:13:35.104933: step 10883, loss 0.578975.
Train: 2018-08-05T00:13:35.370495: step 10884, loss 0.546236.
Train: 2018-08-05T00:13:35.620438: step 10885, loss 0.521905.
Train: 2018-08-05T00:13:35.870381: step 10886, loss 0.554535.
Train: 2018-08-05T00:13:36.135912: step 10887, loss 0.497817.
Train: 2018-08-05T00:13:36.385883: step 10888, loss 0.570793.
Train: 2018-08-05T00:13:36.635825: step 10889, loss 0.505949.
Train: 2018-08-05T00:13:36.885763: step 10890, loss 0.60329.
Test: 2018-08-05T00:13:38.135442: step 10890, loss 0.549304.
Train: 2018-08-05T00:13:38.385385: step 10891, loss 0.619554.
Train: 2018-08-05T00:13:38.635326: step 10892, loss 0.497895.
Train: 2018-08-05T00:13:38.885297: step 10893, loss 0.514076.
Train: 2018-08-05T00:13:39.150831: step 10894, loss 0.546447.
Train: 2018-08-05T00:13:39.400771: step 10895, loss 0.635826.
Train: 2018-08-05T00:13:39.650744: step 10896, loss 0.505734.
Train: 2018-08-05T00:13:39.900687: step 10897, loss 0.554488.
Train: 2018-08-05T00:13:40.150626: step 10898, loss 0.652295.
Train: 2018-08-05T00:13:40.400537: step 10899, loss 0.644071.
Train: 2018-08-05T00:13:40.650509: step 10900, loss 0.513948.
Test: 2018-08-05T00:13:41.915807: step 10900, loss 0.548943.
Train: 2018-08-05T00:13:42.759361: step 10901, loss 0.570786.
Train: 2018-08-05T00:13:43.009302: step 10902, loss 0.587002.
Train: 2018-08-05T00:13:43.259274: step 10903, loss 0.627403.
Train: 2018-08-05T00:13:43.524805: step 10904, loss 0.570824.
Train: 2018-08-05T00:13:43.759127: step 10905, loss 0.570834.
Train: 2018-08-05T00:13:44.009067: step 10906, loss 0.554826.
Train: 2018-08-05T00:13:44.259040: step 10907, loss 0.570867.
Train: 2018-08-05T00:13:44.508951: step 10908, loss 0.523002.
Train: 2018-08-05T00:13:44.758922: step 10909, loss 0.594845.
Train: 2018-08-05T00:13:45.024485: step 10910, loss 0.570921.
Test: 2018-08-05T00:13:46.258541: step 10910, loss 0.549126.
Train: 2018-08-05T00:13:46.492892: step 10911, loss 0.499338.
Train: 2018-08-05T00:13:46.742809: step 10912, loss 0.586817.
Train: 2018-08-05T00:13:46.992774: step 10913, loss 0.547037.
Train: 2018-08-05T00:13:47.242715: step 10914, loss 0.602739.
Train: 2018-08-05T00:13:47.492657: step 10915, loss 0.499289.
Train: 2018-08-05T00:13:47.742598: step 10916, loss 0.539019.
Train: 2018-08-05T00:13:47.992539: step 10917, loss 0.53894.
Train: 2018-08-05T00:13:48.242481: step 10918, loss 0.562857.
Train: 2018-08-05T00:13:48.492426: step 10919, loss 0.602932.
Train: 2018-08-05T00:13:48.742364: step 10920, loss 0.538712.
Test: 2018-08-05T00:13:49.992041: step 10920, loss 0.548976.
Train: 2018-08-05T00:13:50.226392: step 10921, loss 0.538647.
Train: 2018-08-05T00:13:50.476327: step 10922, loss 0.562749.
Train: 2018-08-05T00:13:50.726274: step 10923, loss 0.522333.
Train: 2018-08-05T00:13:50.976186: step 10924, loss 0.538388.
Train: 2018-08-05T00:13:51.226128: step 10925, loss 0.546391.
Train: 2018-08-05T00:13:51.476099: step 10926, loss 0.554458.
Train: 2018-08-05T00:13:51.741632: step 10927, loss 0.488959.
Train: 2018-08-05T00:13:51.991603: step 10928, loss 0.562539.
Train: 2018-08-05T00:13:52.241516: step 10929, loss 0.56251.
Train: 2018-08-05T00:13:52.491457: step 10930, loss 0.587312.
Test: 2018-08-05T00:13:53.756784: step 10930, loss 0.549933.
Train: 2018-08-05T00:13:53.991131: step 10931, loss 0.562455.
Train: 2018-08-05T00:13:54.241077: step 10932, loss 0.537487.
Train: 2018-08-05T00:13:54.506610: step 10933, loss 0.512392.
Train: 2018-08-05T00:13:54.756575: step 10934, loss 0.528947.
Train: 2018-08-05T00:13:55.006522: step 10935, loss 0.537207.
Train: 2018-08-05T00:13:55.240843: step 10936, loss 0.562367.
Train: 2018-08-05T00:13:55.490785: step 10937, loss 0.520119.
Train: 2018-08-05T00:13:55.740696: step 10938, loss 0.486046.
Train: 2018-08-05T00:13:55.990667: step 10939, loss 0.587898.
Train: 2018-08-05T00:13:56.240578: step 10940, loss 0.570886.
Test: 2018-08-05T00:13:57.505907: step 10940, loss 0.548225.
Train: 2018-08-05T00:13:57.755872: step 10941, loss 0.622304.
Train: 2018-08-05T00:13:58.005820: step 10942, loss 0.562332.
Train: 2018-08-05T00:13:58.255761: step 10943, loss 0.536603.
Train: 2018-08-05T00:13:58.505673: step 10944, loss 0.579501.
Train: 2018-08-05T00:13:58.755639: step 10945, loss 0.605249.
Train: 2018-08-05T00:13:59.005586: step 10946, loss 0.553763.
Train: 2018-08-05T00:13:59.271149: step 10947, loss 0.519522.
Train: 2018-08-05T00:13:59.521093: step 10948, loss 0.519537.
Train: 2018-08-05T00:13:59.771001: step 10949, loss 0.622274.
Train: 2018-08-05T00:14:00.020976: step 10950, loss 0.485364.
Test: 2018-08-05T00:14:01.286271: step 10950, loss 0.548834.
Train: 2018-08-05T00:14:01.520625: step 10951, loss 0.562329.
Train: 2018-08-05T00:14:01.770534: step 10952, loss 0.528095.
Train: 2018-08-05T00:14:02.020507: step 10953, loss 0.605152.
Train: 2018-08-05T00:14:02.270416: step 10954, loss 0.54518.
Train: 2018-08-05T00:14:02.520391: step 10955, loss 0.485333.
Train: 2018-08-05T00:14:02.770300: step 10956, loss 0.545134.
Train: 2018-08-05T00:14:03.020273: step 10957, loss 0.570969.
Train: 2018-08-05T00:14:03.270181: step 10958, loss 0.562398.
Train: 2018-08-05T00:14:03.520154: step 10959, loss 0.571235.
Train: 2018-08-05T00:14:03.770065: step 10960, loss 0.631253.
Test: 2018-08-05T00:14:05.019772: step 10960, loss 0.547457.
Train: 2018-08-05T00:14:05.254093: step 10961, loss 0.53666.
Train: 2018-08-05T00:14:05.504035: step 10962, loss 0.579413.
Train: 2018-08-05T00:14:05.754005: step 10963, loss 0.60495.
Train: 2018-08-05T00:14:06.003947: step 10964, loss 0.65582.
Train: 2018-08-05T00:14:06.253888: step 10965, loss 0.528509.
Train: 2018-08-05T00:14:06.503824: step 10966, loss 0.596095.
Train: 2018-08-05T00:14:06.753775: step 10967, loss 0.570816.
Train: 2018-08-05T00:14:07.003713: step 10968, loss 0.54569.
Train: 2018-08-05T00:14:07.253625: step 10969, loss 0.554193.
Train: 2018-08-05T00:14:07.503596: step 10970, loss 0.512733.
Test: 2018-08-05T00:14:08.753272: step 10970, loss 0.547872.
Train: 2018-08-05T00:14:08.987618: step 10971, loss 0.537581.
Train: 2018-08-05T00:14:09.237565: step 10972, loss 0.637018.
Train: 2018-08-05T00:14:09.487506: step 10973, loss 0.554199.
Train: 2018-08-05T00:14:09.768691: step 10974, loss 0.529605.
Train: 2018-08-05T00:14:10.018632: step 10975, loss 0.620078.
Train: 2018-08-05T00:14:10.268573: step 10976, loss 0.537995.
Train: 2018-08-05T00:14:10.518519: step 10977, loss 0.529894.
Train: 2018-08-05T00:14:10.768426: step 10978, loss 0.513618.
Train: 2018-08-05T00:14:11.018402: step 10979, loss 0.587115.
Train: 2018-08-05T00:14:11.268339: step 10980, loss 0.587096.
Test: 2018-08-05T00:14:12.533637: step 10980, loss 0.549565.
Train: 2018-08-05T00:14:12.767991: step 10981, loss 0.521849.
Train: 2018-08-05T00:14:13.017932: step 10982, loss 0.538153.
Train: 2018-08-05T00:14:13.267870: step 10983, loss 0.6279.
Train: 2018-08-05T00:14:13.517782: step 10984, loss 0.570774.
Train: 2018-08-05T00:14:13.767723: step 10985, loss 0.521901.
Train: 2018-08-05T00:14:14.017698: step 10986, loss 0.611513.
Train: 2018-08-05T00:14:14.267639: step 10987, loss 0.554505.
Train: 2018-08-05T00:14:14.517549: step 10988, loss 0.513848.
Train: 2018-08-05T00:14:14.767520: step 10989, loss 0.578913.
Train: 2018-08-05T00:14:15.017461: step 10990, loss 0.538244.
Test: 2018-08-05T00:14:16.267138: step 10990, loss 0.550337.
Train: 2018-08-05T00:14:16.517080: step 10991, loss 0.570756.
Train: 2018-08-05T00:14:16.767021: step 10992, loss 0.529964.
Train: 2018-08-05T00:14:17.016994: step 10993, loss 0.521855.
Train: 2018-08-05T00:14:17.266905: step 10994, loss 0.603574.
Train: 2018-08-05T00:14:17.532466: step 10995, loss 0.521624.
Train: 2018-08-05T00:14:17.782409: step 10996, loss 0.603364.
Train: 2018-08-05T00:14:18.016759: step 10997, loss 0.595245.
Train: 2018-08-05T00:14:18.266671: step 10998, loss 0.579062.
Train: 2018-08-05T00:14:18.516642: step 10999, loss 0.579121.
Train: 2018-08-05T00:14:18.766584: step 11000, loss 0.545767.
Test: 2018-08-05T00:14:20.016261: step 11000, loss 0.548069.
Train: 2018-08-05T00:14:20.859814: step 11001, loss 0.595322.
Train: 2018-08-05T00:14:21.125405: step 11002, loss 0.595258.
Train: 2018-08-05T00:14:21.375347: step 11003, loss 0.660829.
Train: 2018-08-05T00:14:21.625289: step 11004, loss 0.546216.
Train: 2018-08-05T00:14:21.875231: step 11005, loss 0.554516.
Train: 2018-08-05T00:14:22.125166: step 11006, loss 0.530139.
Train: 2018-08-05T00:14:22.390704: step 11007, loss 0.643176.
Train: 2018-08-05T00:14:22.625026: step 11008, loss 0.554542.
Train: 2018-08-05T00:14:22.874996: step 11009, loss 0.554592.
Train: 2018-08-05T00:14:23.124938: step 11010, loss 0.498123.
Test: 2018-08-05T00:14:24.374615: step 11010, loss 0.54882.
Train: 2018-08-05T00:14:24.608966: step 11011, loss 0.563515.
Train: 2018-08-05T00:14:24.874527: step 11012, loss 0.578926.
Train: 2018-08-05T00:14:25.124469: step 11013, loss 0.554681.
Train: 2018-08-05T00:14:25.374381: step 11014, loss 0.546334.
Train: 2018-08-05T00:14:25.624323: step 11015, loss 0.538084.
Train: 2018-08-05T00:14:25.874294: step 11016, loss 0.553771.
Train: 2018-08-05T00:14:26.124235: step 11017, loss 0.595915.
Train: 2018-08-05T00:14:26.374147: step 11018, loss 0.547169.
Train: 2018-08-05T00:14:26.624118: step 11019, loss 0.545658.
Train: 2018-08-05T00:14:26.874029: step 11020, loss 0.595783.
Test: 2018-08-05T00:14:28.123737: step 11020, loss 0.549155.
Train: 2018-08-05T00:14:28.373679: step 11021, loss 0.506732.
Train: 2018-08-05T00:14:28.623651: step 11022, loss 0.564853.
Train: 2018-08-05T00:14:28.811077: step 11023, loss 0.510212.
Train: 2018-08-05T00:14:29.076669: step 11024, loss 0.586376.
Train: 2018-08-05T00:14:29.326611: step 11025, loss 0.611862.
Train: 2018-08-05T00:14:29.576523: step 11026, loss 0.538777.
Train: 2018-08-05T00:14:29.826463: step 11027, loss 0.538.
Train: 2018-08-05T00:14:30.076435: step 11028, loss 0.595824.
Train: 2018-08-05T00:14:30.326347: step 11029, loss 0.627672.
Train: 2018-08-05T00:14:30.591939: step 11030, loss 0.570792.
Test: 2018-08-05T00:14:31.825995: step 11030, loss 0.548527.
Train: 2018-08-05T00:14:32.091557: step 11031, loss 0.578889.
Train: 2018-08-05T00:14:32.341499: step 11032, loss 0.554855.
Train: 2018-08-05T00:14:32.591442: step 11033, loss 0.562896.
Train: 2018-08-05T00:14:32.841413: step 11034, loss 0.602784.
Train: 2018-08-05T00:14:33.091324: step 11035, loss 0.554965.
Train: 2018-08-05T00:14:33.341295: step 11036, loss 0.602713.
Train: 2018-08-05T00:14:33.591236: step 11037, loss 0.562977.
Train: 2018-08-05T00:14:33.841178: step 11038, loss 0.634359.
Train: 2018-08-05T00:14:34.091120: step 11039, loss 0.523497.
Train: 2018-08-05T00:14:34.356683: step 11040, loss 0.555215.
Test: 2018-08-05T00:14:35.606360: step 11040, loss 0.550095.
Train: 2018-08-05T00:14:35.856332: step 11041, loss 0.547362.
Train: 2018-08-05T00:14:36.106279: step 11042, loss 0.531508.
Train: 2018-08-05T00:14:36.356215: step 11043, loss 0.594685.
Train: 2018-08-05T00:14:36.606125: step 11044, loss 0.484258.
Train: 2018-08-05T00:14:36.856097: step 11045, loss 0.586789.
Train: 2018-08-05T00:14:37.106038: step 11046, loss 0.531359.
Train: 2018-08-05T00:14:37.355949: step 11047, loss 0.531251.
Train: 2018-08-05T00:14:37.605921: step 11048, loss 0.515196.
Train: 2018-08-05T00:14:37.855863: step 11049, loss 0.602823.
Train: 2018-08-05T00:14:38.105805: step 11050, loss 0.546799.
Test: 2018-08-05T00:14:39.371103: step 11050, loss 0.549365.
Train: 2018-08-05T00:14:39.605454: step 11051, loss 0.562796.
Train: 2018-08-05T00:14:39.855395: step 11052, loss 0.490155.
Train: 2018-08-05T00:14:40.105306: step 11053, loss 0.570822.
Train: 2018-08-05T00:14:40.355247: step 11054, loss 0.562626.
Train: 2018-08-05T00:14:40.620840: step 11055, loss 0.595261.
Train: 2018-08-05T00:14:40.855161: step 11056, loss 0.554383.
Train: 2018-08-05T00:14:41.105103: step 11057, loss 0.488912.
Train: 2018-08-05T00:14:41.355043: step 11058, loss 0.56255.
Train: 2018-08-05T00:14:41.604956: step 11059, loss 0.56254.
Train: 2018-08-05T00:14:41.854898: step 11060, loss 0.545975.
Test: 2018-08-05T00:14:43.120225: step 11060, loss 0.547674.
Train: 2018-08-05T00:14:43.385818: step 11061, loss 0.554193.
Train: 2018-08-05T00:14:43.620139: step 11062, loss 0.595704.
Train: 2018-08-05T00:14:43.870080: step 11063, loss 0.537473.
Train: 2018-08-05T00:14:44.120021: step 11064, loss 0.612457.
Train: 2018-08-05T00:14:44.369933: step 11065, loss 0.562433.
Train: 2018-08-05T00:14:44.619905: step 11066, loss 0.595781.
Train: 2018-08-05T00:14:44.869845: step 11067, loss 0.620772.
Train: 2018-08-05T00:14:45.119786: step 11068, loss 0.529171.
Train: 2018-08-05T00:14:45.369698: step 11069, loss 0.53753.
Train: 2018-08-05T00:14:45.619670: step 11070, loss 0.537546.
Test: 2018-08-05T00:14:46.869347: step 11070, loss 0.548017.
Train: 2018-08-05T00:14:47.103669: step 11071, loss 0.595663.
Train: 2018-08-05T00:14:47.353608: step 11072, loss 0.545873.
Train: 2018-08-05T00:14:47.603550: step 11073, loss 0.628791.
Train: 2018-08-05T00:14:47.853492: step 11074, loss 0.537665.
Train: 2018-08-05T00:14:48.103463: step 11075, loss 0.504665.
Train: 2018-08-05T00:14:48.353375: step 11076, loss 0.529448.
Train: 2018-08-05T00:14:48.603346: step 11077, loss 0.537685.
Train: 2018-08-05T00:14:48.853259: step 11078, loss 0.52937.
Train: 2018-08-05T00:14:49.103200: step 11079, loss 0.645383.
Train: 2018-08-05T00:14:49.353171: step 11080, loss 0.595621.
Test: 2018-08-05T00:14:50.602848: step 11080, loss 0.548464.
Train: 2018-08-05T00:14:50.868411: step 11081, loss 0.570756.
Train: 2018-08-05T00:14:51.102761: step 11082, loss 0.496361.
Train: 2018-08-05T00:14:51.368293: step 11083, loss 0.529418.
Train: 2018-08-05T00:14:51.618236: step 11084, loss 0.587306.
Train: 2018-08-05T00:14:51.852586: step 11085, loss 0.521101.
Train: 2018-08-05T00:14:52.102497: step 11086, loss 0.537613.
Train: 2018-08-05T00:14:52.352468: step 11087, loss 0.562454.
Train: 2018-08-05T00:14:52.618031: step 11088, loss 0.554149.
Train: 2018-08-05T00:14:52.852335: step 11089, loss 0.545804.
Train: 2018-08-05T00:14:53.102294: step 11090, loss 0.562444.
Test: 2018-08-05T00:14:54.367591: step 11090, loss 0.547928.
Train: 2018-08-05T00:14:54.633186: step 11091, loss 0.61246.
Train: 2018-08-05T00:14:54.867476: step 11092, loss 0.529091.
Train: 2018-08-05T00:14:55.117449: step 11093, loss 0.579106.
Train: 2018-08-05T00:14:55.367387: step 11094, loss 0.612435.
Train: 2018-08-05T00:14:55.617329: step 11095, loss 0.537467.
Train: 2018-08-05T00:14:55.867271: step 11096, loss 0.52916.
Train: 2018-08-05T00:14:56.117211: step 11097, loss 0.629005.
Train: 2018-08-05T00:14:56.367154: step 11098, loss 0.645471.
Train: 2018-08-05T00:14:56.617064: step 11099, loss 0.554203.
Train: 2018-08-05T00:14:56.867037: step 11100, loss 0.54601.
Test: 2018-08-05T00:14:58.132334: step 11100, loss 0.549926.
Train: 2018-08-05T00:14:59.007159: step 11101, loss 0.562539.
Train: 2018-08-05T00:14:59.257095: step 11102, loss 0.570763.
Train: 2018-08-05T00:14:59.507014: step 11103, loss 0.578951.
Train: 2018-08-05T00:14:59.756979: step 11104, loss 0.529923.
Train: 2018-08-05T00:15:00.006895: step 11105, loss 0.538143.
Train: 2018-08-05T00:15:00.256838: step 11106, loss 0.578932.
Train: 2018-08-05T00:15:00.506809: step 11107, loss 0.570777.
Train: 2018-08-05T00:15:00.756751: step 11108, loss 0.570781.
Train: 2018-08-05T00:15:01.006662: step 11109, loss 0.530165.
Train: 2018-08-05T00:15:01.256602: step 11110, loss 0.595151.
Test: 2018-08-05T00:15:02.521931: step 11110, loss 0.550247.
Train: 2018-08-05T00:15:02.756285: step 11111, loss 0.530223.
Train: 2018-08-05T00:15:03.006226: step 11112, loss 0.522114.
Train: 2018-08-05T00:15:03.256134: step 11113, loss 0.570787.
Train: 2018-08-05T00:15:03.506107: step 11114, loss 0.603287.
Train: 2018-08-05T00:15:03.771673: step 11115, loss 0.578909.
Train: 2018-08-05T00:15:04.021615: step 11116, loss 0.538322.
Train: 2018-08-05T00:15:04.271556: step 11117, loss 0.54644.
Train: 2018-08-05T00:15:04.521498: step 11118, loss 0.497711.
Train: 2018-08-05T00:15:04.771439: step 11119, loss 0.48129.
Train: 2018-08-05T00:15:05.021346: step 11120, loss 0.570771.
Test: 2018-08-05T00:15:06.271054: step 11120, loss 0.548703.
Train: 2018-08-05T00:15:06.505405: step 11121, loss 0.60353.
Train: 2018-08-05T00:15:06.755340: step 11122, loss 0.52973.
Train: 2018-08-05T00:15:07.005258: step 11123, loss 0.537854.
Train: 2018-08-05T00:15:07.255228: step 11124, loss 0.579004.
Train: 2018-08-05T00:15:07.505169: step 11125, loss 0.562493.
Train: 2018-08-05T00:15:07.755106: step 11126, loss 0.570757.
Train: 2018-08-05T00:15:08.005056: step 11127, loss 0.51274.
Train: 2018-08-05T00:15:08.254964: step 11128, loss 0.545841.
Train: 2018-08-05T00:15:08.504907: step 11129, loss 0.562438.
Train: 2018-08-05T00:15:08.770503: step 11130, loss 0.545744.
Test: 2018-08-05T00:15:10.020175: step 11130, loss 0.546724.
Train: 2018-08-05T00:15:10.348254: step 11131, loss 0.570771.
Train: 2018-08-05T00:15:10.598196: step 11132, loss 0.554035.
Train: 2018-08-05T00:15:10.848137: step 11133, loss 0.562398.
Train: 2018-08-05T00:15:11.098080: step 11134, loss 0.562392.
Train: 2018-08-05T00:15:11.363611: step 11135, loss 0.562388.
Train: 2018-08-05T00:15:11.613583: step 11136, loss 0.495166.
Train: 2018-08-05T00:15:11.863524: step 11137, loss 0.579214.
Train: 2018-08-05T00:15:12.113466: step 11138, loss 0.604513.
Train: 2018-08-05T00:15:12.363377: step 11139, loss 0.596083.
Train: 2018-08-05T00:15:12.613349: step 11140, loss 0.570796.
Test: 2018-08-05T00:15:13.863025: step 11140, loss 0.54892.
Train: 2018-08-05T00:15:14.112991: step 11141, loss 0.520339.
Train: 2018-08-05T00:15:14.347313: step 11142, loss 0.537167.
Train: 2018-08-05T00:15:14.597260: step 11143, loss 0.596009.
Train: 2018-08-05T00:15:14.847201: step 11144, loss 0.570786.
Train: 2018-08-05T00:15:15.097142: step 11145, loss 0.654686.
Train: 2018-08-05T00:15:15.347084: step 11146, loss 0.51223.
Train: 2018-08-05T00:15:15.597025: step 11147, loss 0.579115.
Train: 2018-08-05T00:15:15.846966: step 11148, loss 0.529119.
Train: 2018-08-05T00:15:16.096908: step 11149, loss 0.587397.
Train: 2018-08-05T00:15:16.346820: step 11150, loss 0.520948.
Test: 2018-08-05T00:15:17.596526: step 11150, loss 0.549947.
Train: 2018-08-05T00:15:17.830878: step 11151, loss 0.612233.
Train: 2018-08-05T00:15:18.080818: step 11152, loss 0.570757.
Train: 2018-08-05T00:15:18.330731: step 11153, loss 0.612061.
Train: 2018-08-05T00:15:18.580701: step 11154, loss 0.529581.
Train: 2018-08-05T00:15:18.830643: step 11155, loss 0.578977.
Train: 2018-08-05T00:15:19.080584: step 11156, loss 0.58716.
Train: 2018-08-05T00:15:19.330496: step 11157, loss 0.603475.
Train: 2018-08-05T00:15:19.580467: step 11158, loss 0.554478.
Train: 2018-08-05T00:15:19.830409: step 11159, loss 0.546409.
Train: 2018-08-05T00:15:20.080350: step 11160, loss 0.595115.
Test: 2018-08-05T00:15:21.330027: step 11160, loss 0.549404.
Train: 2018-08-05T00:15:21.564379: step 11161, loss 0.57889.
Train: 2018-08-05T00:15:21.814320: step 11162, loss 0.546632.
Train: 2018-08-05T00:15:22.064260: step 11163, loss 0.57083.
Train: 2018-08-05T00:15:22.314204: step 11164, loss 0.602956.
Train: 2018-08-05T00:15:22.564144: step 11165, loss 0.570858.
Train: 2018-08-05T00:15:22.814085: step 11166, loss 0.514973.
Train: 2018-08-05T00:15:23.079618: step 11167, loss 0.562904.
Train: 2018-08-05T00:15:23.329589: step 11168, loss 0.531029.
Train: 2018-08-05T00:15:23.595152: step 11169, loss 0.538993.
Train: 2018-08-05T00:15:23.829472: step 11170, loss 0.602804.
Test: 2018-08-05T00:15:25.079151: step 11170, loss 0.550674.
Train: 2018-08-05T00:15:25.329090: step 11171, loss 0.5629.
Train: 2018-08-05T00:15:25.579032: step 11172, loss 0.530975.
Train: 2018-08-05T00:15:25.828974: step 11173, loss 0.562884.
Train: 2018-08-05T00:15:26.032051: step 11174, loss 0.460512.
Train: 2018-08-05T00:15:26.282023: step 11175, loss 0.578869.
Train: 2018-08-05T00:15:26.531964: step 11176, loss 0.514488.
Train: 2018-08-05T00:15:26.781877: step 11177, loss 0.54657.
Train: 2018-08-05T00:15:27.031847: step 11178, loss 0.643791.
Train: 2018-08-05T00:15:27.281789: step 11179, loss 0.60328.
Train: 2018-08-05T00:15:27.531701: step 11180, loss 0.538276.
Test: 2018-08-05T00:15:28.781407: step 11180, loss 0.548498.
Train: 2018-08-05T00:15:29.031375: step 11181, loss 0.578916.
Train: 2018-08-05T00:15:29.265704: step 11182, loss 0.521944.
Train: 2018-08-05T00:15:29.515644: step 11183, loss 0.627832.
Train: 2018-08-05T00:15:29.765552: step 11184, loss 0.627818.
Train: 2018-08-05T00:15:30.015527: step 11185, loss 0.538245.
Train: 2018-08-05T00:15:30.265469: step 11186, loss 0.570785.
Train: 2018-08-05T00:15:30.515411: step 11187, loss 0.538322.
Train: 2018-08-05T00:15:30.765344: step 11188, loss 0.554563.
Train: 2018-08-05T00:15:31.015293: step 11189, loss 0.554565.
Train: 2018-08-05T00:15:31.265206: step 11190, loss 0.546449.
Test: 2018-08-05T00:15:32.514908: step 11190, loss 0.549113.
Train: 2018-08-05T00:15:32.749230: step 11191, loss 0.505845.
Train: 2018-08-05T00:15:32.999169: step 11192, loss 0.546382.
Train: 2018-08-05T00:15:33.249112: step 11193, loss 0.603378.
Train: 2018-08-05T00:15:33.499086: step 11194, loss 0.529984.
Train: 2018-08-05T00:15:33.749025: step 11195, loss 0.497231.
Train: 2018-08-05T00:15:34.014588: step 11196, loss 0.660912.
Train: 2018-08-05T00:15:34.264499: step 11197, loss 0.603554.
Train: 2018-08-05T00:15:34.514474: step 11198, loss 0.562572.
Train: 2018-08-05T00:15:34.764406: step 11199, loss 0.60351.
Train: 2018-08-05T00:15:35.014323: step 11200, loss 0.52173.
Test: 2018-08-05T00:15:36.264030: step 11200, loss 0.549333.
Train: 2018-08-05T00:15:37.138860: step 11201, loss 0.538093.
Train: 2018-08-05T00:15:37.388801: step 11202, loss 0.521747.
Train: 2018-08-05T00:15:37.638709: step 11203, loss 0.554408.
Train: 2018-08-05T00:15:37.888684: step 11204, loss 0.611708.
Train: 2018-08-05T00:15:38.154212: step 11205, loss 0.570765.
Train: 2018-08-05T00:15:38.404154: step 11206, loss 0.546211.
Train: 2018-08-05T00:15:38.654095: step 11207, loss 0.538024.
Train: 2018-08-05T00:15:38.904067: step 11208, loss 0.497052.
Train: 2018-08-05T00:15:39.154008: step 11209, loss 0.546138.
Train: 2018-08-05T00:15:39.403950: step 11210, loss 0.603662.
Test: 2018-08-05T00:15:40.653627: step 11210, loss 0.54934.
Train: 2018-08-05T00:15:40.887949: step 11211, loss 0.554289.
Train: 2018-08-05T00:15:41.153510: step 11212, loss 0.537786.
Train: 2018-08-05T00:15:41.403452: step 11213, loss 0.554247.
Train: 2018-08-05T00:15:41.653393: step 11214, loss 0.521158.
Train: 2018-08-05T00:15:41.903334: step 11215, loss 0.570757.
Train: 2018-08-05T00:15:42.153277: step 11216, loss 0.545863.
Train: 2018-08-05T00:15:42.403248: step 11217, loss 0.537506.
Train: 2018-08-05T00:15:42.653189: step 11218, loss 0.60409.
Train: 2018-08-05T00:15:42.918724: step 11219, loss 0.637471.
Train: 2018-08-05T00:15:43.168693: step 11220, loss 0.554106.
Test: 2018-08-05T00:15:44.418370: step 11220, loss 0.548927.
Train: 2018-08-05T00:15:44.652721: step 11221, loss 0.604046.
Train: 2018-08-05T00:15:44.902662: step 11222, loss 0.562454.
Train: 2018-08-05T00:15:45.152574: step 11223, loss 0.554177.
Train: 2018-08-05T00:15:45.402546: step 11224, loss 0.554202.
Train: 2018-08-05T00:15:45.652456: step 11225, loss 0.512888.
Train: 2018-08-05T00:15:45.902428: step 11226, loss 0.56249.
Train: 2018-08-05T00:15:46.152370: step 11227, loss 0.579022.
Train: 2018-08-05T00:15:46.402280: step 11228, loss 0.620322.
Train: 2018-08-05T00:15:46.652223: step 11229, loss 0.636716.
Train: 2018-08-05T00:15:46.902194: step 11230, loss 0.480388.
Test: 2018-08-05T00:15:48.151871: step 11230, loss 0.548098.
Train: 2018-08-05T00:15:48.386227: step 11231, loss 0.611785.
Train: 2018-08-05T00:15:48.636133: step 11232, loss 0.554394.
Train: 2018-08-05T00:15:48.886109: step 11233, loss 0.489077.
Train: 2018-08-05T00:15:49.136050: step 11234, loss 0.529924.
Train: 2018-08-05T00:15:49.385958: step 11235, loss 0.611644.
Train: 2018-08-05T00:15:49.635898: step 11236, loss 0.529913.
Train: 2018-08-05T00:15:49.885875: step 11237, loss 0.55442.
Train: 2018-08-05T00:15:50.135782: step 11238, loss 0.611655.
Train: 2018-08-05T00:15:50.385757: step 11239, loss 0.554426.
Train: 2018-08-05T00:15:50.635699: step 11240, loss 0.562603.
Test: 2018-08-05T00:15:51.885372: step 11240, loss 0.548974.
Train: 2018-08-05T00:15:52.119726: step 11241, loss 0.595261.
Train: 2018-08-05T00:15:52.369664: step 11242, loss 0.595235.
Train: 2018-08-05T00:15:52.619606: step 11243, loss 0.57078.
Train: 2018-08-05T00:15:52.865461: step 11244, loss 0.595154.
Train: 2018-08-05T00:15:53.115401: step 11245, loss 0.530282.
Train: 2018-08-05T00:15:53.365344: step 11246, loss 0.514163.
Train: 2018-08-05T00:15:53.630875: step 11247, loss 0.546528.
Train: 2018-08-05T00:15:53.865195: step 11248, loss 0.538422.
Train: 2018-08-05T00:15:54.130782: step 11249, loss 0.546488.
Train: 2018-08-05T00:15:54.365110: step 11250, loss 0.505884.
Test: 2018-08-05T00:15:55.630407: step 11250, loss 0.549248.
Train: 2018-08-05T00:15:55.864728: step 11251, loss 0.627724.
Train: 2018-08-05T00:15:56.114700: step 11252, loss 0.57892.
Train: 2018-08-05T00:15:56.364610: step 11253, loss 0.513774.
Train: 2018-08-05T00:15:56.630172: step 11254, loss 0.513686.
Train: 2018-08-05T00:15:56.864493: step 11255, loss 0.54624.
Train: 2018-08-05T00:15:57.114465: step 11256, loss 0.587157.
Train: 2018-08-05T00:15:57.364375: step 11257, loss 0.496855.
Train: 2018-08-05T00:15:57.614348: step 11258, loss 0.611943.
Train: 2018-08-05T00:15:57.864289: step 11259, loss 0.546007.
Train: 2018-08-05T00:15:58.114231: step 11260, loss 0.587285.
Test: 2018-08-05T00:15:59.363907: step 11260, loss 0.548861.
Train: 2018-08-05T00:15:59.598228: step 11261, loss 0.595571.
Train: 2018-08-05T00:15:59.848200: step 11262, loss 0.471506.
Train: 2018-08-05T00:16:00.113762: step 11263, loss 0.603908.
Train: 2018-08-05T00:16:00.363674: step 11264, loss 0.537583.
Train: 2018-08-05T00:16:00.613645: step 11265, loss 0.554152.
Train: 2018-08-05T00:16:00.863557: step 11266, loss 0.48763.
Train: 2018-08-05T00:16:01.113497: step 11267, loss 0.5791.
Train: 2018-08-05T00:16:01.379061: step 11268, loss 0.512311.
Train: 2018-08-05T00:16:01.629032: step 11269, loss 0.646141.
Train: 2018-08-05T00:16:01.878977: step 11270, loss 0.562399.
Test: 2018-08-05T00:16:03.128651: step 11270, loss 0.549177.
Train: 2018-08-05T00:16:03.378592: step 11271, loss 0.537262.
Train: 2018-08-05T00:16:03.628534: step 11272, loss 0.545627.
Train: 2018-08-05T00:16:03.878507: step 11273, loss 0.520436.
Train: 2018-08-05T00:16:04.128443: step 11274, loss 0.520364.
Train: 2018-08-05T00:16:04.394010: step 11275, loss 0.494998.
Train: 2018-08-05T00:16:04.643924: step 11276, loss 0.511664.
Train: 2018-08-05T00:16:04.893892: step 11277, loss 0.604767.
Train: 2018-08-05T00:16:05.143833: step 11278, loss 0.494321.
Train: 2018-08-05T00:16:05.393775: step 11279, loss 0.553809.
Train: 2018-08-05T00:16:05.643718: step 11280, loss 0.579451.
Test: 2018-08-05T00:16:06.893393: step 11280, loss 0.547026.
Train: 2018-08-05T00:16:07.143336: step 11281, loss 0.536608.
Train: 2018-08-05T00:16:07.408928: step 11282, loss 0.553742.
Train: 2018-08-05T00:16:07.658871: step 11283, loss 0.527896.
Train: 2018-08-05T00:16:07.924427: step 11284, loss 0.562338.
Train: 2018-08-05T00:16:08.158723: step 11285, loss 0.614196.
Train: 2018-08-05T00:16:08.408694: step 11286, loss 0.56234.
Train: 2018-08-05T00:16:08.658635: step 11287, loss 0.545058.
Train: 2018-08-05T00:16:08.908578: step 11288, loss 0.56234.
Train: 2018-08-05T00:16:09.158488: step 11289, loss 0.545067.
Train: 2018-08-05T00:16:09.408460: step 11290, loss 0.570972.
Test: 2018-08-05T00:16:10.673758: step 11290, loss 0.548724.
Train: 2018-08-05T00:16:10.908109: step 11291, loss 0.605474.
Train: 2018-08-05T00:16:11.236158: step 11292, loss 0.639835.
Train: 2018-08-05T00:16:11.486069: step 11293, loss 0.553758.
Train: 2018-08-05T00:16:11.736041: step 11294, loss 0.596537.
Train: 2018-08-05T00:16:11.985953: step 11295, loss 0.51125.
Train: 2018-08-05T00:16:12.251546: step 11296, loss 0.536872.
Train: 2018-08-05T00:16:12.485866: step 11297, loss 0.519983.
Train: 2018-08-05T00:16:12.735777: step 11298, loss 0.553893.
Train: 2018-08-05T00:16:12.985748: step 11299, loss 0.520084.
Train: 2018-08-05T00:16:13.235690: step 11300, loss 0.553909.
Test: 2018-08-05T00:16:14.500987: step 11300, loss 0.548245.
Train: 2018-08-05T00:16:15.422647: step 11301, loss 0.663786.
Train: 2018-08-05T00:16:15.672587: step 11302, loss 0.5708.
Train: 2018-08-05T00:16:15.922561: step 11303, loss 0.562383.
Train: 2018-08-05T00:16:16.172504: step 11304, loss 0.554012.
Train: 2018-08-05T00:16:16.422442: step 11305, loss 0.604236.
Train: 2018-08-05T00:16:16.672384: step 11306, loss 0.562427.
Train: 2018-08-05T00:16:16.922295: step 11307, loss 0.545819.
Train: 2018-08-05T00:16:17.172268: step 11308, loss 0.587345.
Train: 2018-08-05T00:16:17.422209: step 11309, loss 0.562486.
Train: 2018-08-05T00:16:17.672121: step 11310, loss 0.570756.
Test: 2018-08-05T00:16:18.921827: step 11310, loss 0.547657.
Train: 2018-08-05T00:16:19.156149: step 11311, loss 0.513175.
Train: 2018-08-05T00:16:19.406119: step 11312, loss 0.587194.
Train: 2018-08-05T00:16:19.671652: step 11313, loss 0.611771.
Train: 2018-08-05T00:16:19.906002: step 11314, loss 0.578946.
Train: 2018-08-05T00:16:20.155914: step 11315, loss 0.570774.
Train: 2018-08-05T00:16:20.405885: step 11316, loss 0.619568.
Train: 2018-08-05T00:16:20.671447: step 11317, loss 0.578896.
Train: 2018-08-05T00:16:20.921360: step 11318, loss 0.506279.
Train: 2018-08-05T00:16:21.155710: step 11319, loss 0.578876.
Train: 2018-08-05T00:16:21.405651: step 11320, loss 0.514612.
Test: 2018-08-05T00:16:22.655328: step 11320, loss 0.550704.
Train: 2018-08-05T00:16:22.905299: step 11321, loss 0.57887.
Train: 2018-08-05T00:16:23.155210: step 11322, loss 0.562829.
Train: 2018-08-05T00:16:23.405182: step 11323, loss 0.586878.
Train: 2018-08-05T00:16:23.655124: step 11324, loss 0.642889.
Train: 2018-08-05T00:16:23.858171: step 11325, loss 0.630991.
Train: 2018-08-05T00:16:24.108144: step 11326, loss 0.491466.
Train: 2018-08-05T00:16:24.358085: step 11327, loss 0.563.
Train: 2018-08-05T00:16:24.608027: step 11328, loss 0.563026.
Train: 2018-08-05T00:16:24.873559: step 11329, loss 0.53142.
Train: 2018-08-05T00:16:25.123500: step 11330, loss 0.578861.
Test: 2018-08-05T00:16:26.373207: step 11330, loss 0.550404.
Train: 2018-08-05T00:16:26.607528: step 11331, loss 0.563056.
Train: 2018-08-05T00:16:26.873116: step 11332, loss 0.555158.
Train: 2018-08-05T00:16:27.123062: step 11333, loss 0.586764.
Train: 2018-08-05T00:16:27.373004: step 11334, loss 0.602565.
Train: 2018-08-05T00:16:27.622945: step 11335, loss 0.570971.
Train: 2018-08-05T00:16:27.872856: step 11336, loss 0.586749.
Train: 2018-08-05T00:16:28.122828: step 11337, loss 0.539495.
Train: 2018-08-05T00:16:28.372764: step 11338, loss 0.555253.
Train: 2018-08-05T00:16:28.622681: step 11339, loss 0.523765.
Train: 2018-08-05T00:16:28.872623: step 11340, loss 0.570983.
Test: 2018-08-05T00:16:30.137950: step 11340, loss 0.550994.
Train: 2018-08-05T00:16:30.387923: step 11341, loss 0.563082.
Train: 2018-08-05T00:16:30.637864: step 11342, loss 0.523562.
Train: 2018-08-05T00:16:30.887775: step 11343, loss 0.563024.
Train: 2018-08-05T00:16:31.137717: step 11344, loss 0.507442.
Train: 2018-08-05T00:16:31.403280: step 11345, loss 0.586825.
Train: 2018-08-05T00:16:31.668872: step 11346, loss 0.602817.
Train: 2018-08-05T00:16:31.918808: step 11347, loss 0.58686.
Train: 2018-08-05T00:16:32.168750: step 11348, loss 0.578864.
Train: 2018-08-05T00:16:32.434315: step 11349, loss 0.546835.
Train: 2018-08-05T00:16:32.668639: step 11350, loss 0.482685.
Test: 2018-08-05T00:16:33.918315: step 11350, loss 0.548999.
Train: 2018-08-05T00:16:34.168281: step 11351, loss 0.514563.
Train: 2018-08-05T00:16:34.418229: step 11352, loss 0.538532.
Train: 2018-08-05T00:16:34.668171: step 11353, loss 0.570797.
Train: 2018-08-05T00:16:34.918111: step 11354, loss 0.570781.
Train: 2018-08-05T00:16:35.168053: step 11355, loss 0.56262.
Train: 2018-08-05T00:16:35.433616: step 11356, loss 0.497145.
Train: 2018-08-05T00:16:35.667936: step 11357, loss 0.496841.
Train: 2018-08-05T00:16:35.917878: step 11358, loss 0.554231.
Train: 2018-08-05T00:16:36.167819: step 11359, loss 0.529284.
Train: 2018-08-05T00:16:36.417730: step 11360, loss 0.512378.
Test: 2018-08-05T00:16:37.667437: step 11360, loss 0.548372.
Train: 2018-08-05T00:16:37.901791: step 11361, loss 0.56239.
Train: 2018-08-05T00:16:38.151699: step 11362, loss 0.579304.
Train: 2018-08-05T00:16:38.417262: step 11363, loss 0.545415.
Train: 2018-08-05T00:16:38.651583: step 11364, loss 0.536941.
Train: 2018-08-05T00:16:38.901554: step 11365, loss 0.562431.
Train: 2018-08-05T00:16:39.151496: step 11366, loss 0.545344.
Train: 2018-08-05T00:16:39.401407: step 11367, loss 0.536632.
Train: 2018-08-05T00:16:39.651379: step 11368, loss 0.613904.
Train: 2018-08-05T00:16:39.901319: step 11369, loss 0.613927.
Train: 2018-08-05T00:16:40.151256: step 11370, loss 0.519404.
Test: 2018-08-05T00:16:41.400938: step 11370, loss 0.548586.
Train: 2018-08-05T00:16:41.635289: step 11371, loss 0.596672.
Train: 2018-08-05T00:16:41.885230: step 11372, loss 0.579501.
Train: 2018-08-05T00:16:42.135142: step 11373, loss 0.545206.
Train: 2018-08-05T00:16:42.385113: step 11374, loss 0.613751.
Train: 2018-08-05T00:16:42.635054: step 11375, loss 0.622139.
Train: 2018-08-05T00:16:42.884967: step 11376, loss 0.570865.
Train: 2018-08-05T00:16:43.134907: step 11377, loss 0.545397.
Train: 2018-08-05T00:16:43.400472: step 11378, loss 0.537009.
Train: 2018-08-05T00:16:43.634821: step 11379, loss 0.579234.
Train: 2018-08-05T00:16:43.884762: step 11380, loss 0.537154.
Test: 2018-08-05T00:16:45.134439: step 11380, loss 0.549157.
Train: 2018-08-05T00:16:45.384410: step 11381, loss 0.528853.
Train: 2018-08-05T00:16:45.634352: step 11382, loss 0.61266.
Train: 2018-08-05T00:16:45.884293: step 11383, loss 0.579118.
Train: 2018-08-05T00:16:46.134235: step 11384, loss 0.562416.
Train: 2018-08-05T00:16:46.384176: step 11385, loss 0.554133.
Train: 2018-08-05T00:16:46.634089: step 11386, loss 0.512689.
Train: 2018-08-05T00:16:46.884029: step 11387, loss 0.562403.
Train: 2018-08-05T00:16:47.134001: step 11388, loss 0.595629.
Train: 2018-08-05T00:16:47.399533: step 11389, loss 0.603829.
Train: 2018-08-05T00:16:47.649474: step 11390, loss 0.620374.
Test: 2018-08-05T00:16:48.899182: step 11390, loss 0.550689.
Train: 2018-08-05T00:16:49.133533: step 11391, loss 0.595269.
Train: 2018-08-05T00:16:49.383474: step 11392, loss 0.521595.
Train: 2018-08-05T00:16:49.633416: step 11393, loss 0.562533.
Train: 2018-08-05T00:16:49.898948: step 11394, loss 0.546246.
Train: 2018-08-05T00:16:50.133270: step 11395, loss 0.545619.
Train: 2018-08-05T00:16:50.383239: step 11396, loss 0.637804.
Train: 2018-08-05T00:16:50.633182: step 11397, loss 0.570883.
Train: 2018-08-05T00:16:50.883123: step 11398, loss 0.521157.
Train: 2018-08-05T00:16:51.133059: step 11399, loss 0.610972.
Train: 2018-08-05T00:16:51.382976: step 11400, loss 0.554224.
Test: 2018-08-05T00:16:52.632683: step 11400, loss 0.549506.
Train: 2018-08-05T00:16:53.476237: step 11401, loss 0.530183.
Train: 2018-08-05T00:16:53.726208: step 11402, loss 0.587496.
Train: 2018-08-05T00:16:53.976149: step 11403, loss 0.465708.
Train: 2018-08-05T00:16:54.226084: step 11404, loss 0.61249.
Train: 2018-08-05T00:16:54.476033: step 11405, loss 0.594606.
Train: 2018-08-05T00:16:54.725996: step 11406, loss 0.610978.
Train: 2018-08-05T00:16:54.975915: step 11407, loss 0.505775.
Train: 2018-08-05T00:16:55.225857: step 11408, loss 0.628933.
Train: 2018-08-05T00:16:55.491419: step 11409, loss 0.530565.
Train: 2018-08-05T00:16:55.725742: step 11410, loss 0.505991.
Test: 2018-08-05T00:16:56.975415: step 11410, loss 0.550312.
Train: 2018-08-05T00:16:57.225358: step 11411, loss 0.642586.
Train: 2018-08-05T00:16:57.475330: step 11412, loss 0.611147.
Train: 2018-08-05T00:16:57.725240: step 11413, loss 0.522822.
Train: 2018-08-05T00:16:57.975213: step 11414, loss 0.555405.
Train: 2018-08-05T00:16:58.225124: step 11415, loss 0.523067.
Train: 2018-08-05T00:16:58.490712: step 11416, loss 0.554819.
Train: 2018-08-05T00:16:58.725007: step 11417, loss 0.554953.
Train: 2018-08-05T00:16:58.990599: step 11418, loss 0.571195.
Train: 2018-08-05T00:16:59.240540: step 11419, loss 0.586832.
Train: 2018-08-05T00:16:59.490453: step 11420, loss 0.539137.
Test: 2018-08-05T00:17:00.740159: step 11420, loss 0.549099.
Train: 2018-08-05T00:17:00.990102: step 11421, loss 0.563181.
Train: 2018-08-05T00:17:01.240073: step 11422, loss 0.538885.
Train: 2018-08-05T00:17:01.489985: step 11423, loss 0.498699.
Train: 2018-08-05T00:17:01.739930: step 11424, loss 0.538557.
Train: 2018-08-05T00:17:02.005518: step 11425, loss 0.578612.
Train: 2018-08-05T00:17:02.239808: step 11426, loss 0.562401.
Train: 2018-08-05T00:17:02.489781: step 11427, loss 0.570658.
Train: 2018-08-05T00:17:02.739691: step 11428, loss 0.587087.
Train: 2018-08-05T00:17:02.989633: step 11429, loss 0.513102.
Train: 2018-08-05T00:17:03.239573: step 11430, loss 0.620978.
Test: 2018-08-05T00:17:04.489281: step 11430, loss 0.548749.
Train: 2018-08-05T00:17:04.739248: step 11431, loss 0.554267.
Train: 2018-08-05T00:17:04.989190: step 11432, loss 0.537954.
Train: 2018-08-05T00:17:05.239139: step 11433, loss 0.595887.
Train: 2018-08-05T00:17:05.489077: step 11434, loss 0.620019.
Train: 2018-08-05T00:17:05.739019: step 11435, loss 0.554416.
Train: 2018-08-05T00:17:05.988930: step 11436, loss 0.578618.
Train: 2018-08-05T00:17:06.238902: step 11437, loss 0.594913.
Train: 2018-08-05T00:17:06.488843: step 11438, loss 0.538244.
Train: 2018-08-05T00:17:06.738755: step 11439, loss 0.546779.
Train: 2018-08-05T00:17:06.988726: step 11440, loss 0.506029.
Test: 2018-08-05T00:17:08.238403: step 11440, loss 0.550506.
Train: 2018-08-05T00:17:08.472755: step 11441, loss 0.546378.
Train: 2018-08-05T00:17:08.722696: step 11442, loss 0.571004.
Train: 2018-08-05T00:17:08.972640: step 11443, loss 0.562918.
Train: 2018-08-05T00:17:09.222579: step 11444, loss 0.530302.
Train: 2018-08-05T00:17:09.472490: step 11445, loss 0.587057.
Train: 2018-08-05T00:17:09.722462: step 11446, loss 0.603452.
Train: 2018-08-05T00:17:09.972372: step 11447, loss 0.538548.
Train: 2018-08-05T00:17:10.222345: step 11448, loss 0.554641.
Train: 2018-08-05T00:17:10.472256: step 11449, loss 0.57899.
Train: 2018-08-05T00:17:10.722228: step 11450, loss 0.514098.
Test: 2018-08-05T00:17:11.987525: step 11450, loss 0.549138.
Train: 2018-08-05T00:17:12.299983: step 11451, loss 0.53833.
Train: 2018-08-05T00:17:12.549894: step 11452, loss 0.562636.
Train: 2018-08-05T00:17:12.799836: step 11453, loss 0.522012.
Train: 2018-08-05T00:17:13.049777: step 11454, loss 0.562648.
Train: 2018-08-05T00:17:13.299720: step 11455, loss 0.6525.
Train: 2018-08-05T00:17:13.565282: step 11456, loss 0.554427.
Train: 2018-08-05T00:17:13.799633: step 11457, loss 0.521742.
Train: 2018-08-05T00:17:14.049544: step 11458, loss 0.636204.
Train: 2018-08-05T00:17:14.299516: step 11459, loss 0.562596.
Train: 2018-08-05T00:17:14.549425: step 11460, loss 0.554438.
Test: 2018-08-05T00:17:15.814754: step 11460, loss 0.549356.
Train: 2018-08-05T00:17:16.049106: step 11461, loss 0.546289.
Train: 2018-08-05T00:17:16.314638: step 11462, loss 0.570772.
Train: 2018-08-05T00:17:16.548989: step 11463, loss 0.611566.
Train: 2018-08-05T00:17:16.814520: step 11464, loss 0.595217.
Train: 2018-08-05T00:17:17.048871: step 11465, loss 0.538262.
Train: 2018-08-05T00:17:17.298782: step 11466, loss 0.619504.
Train: 2018-08-05T00:17:17.548725: step 11467, loss 0.562703.
Train: 2018-08-05T00:17:17.798696: step 11468, loss 0.522321.
Train: 2018-08-05T00:17:18.048631: step 11469, loss 0.506227.
Train: 2018-08-05T00:17:18.298581: step 11470, loss 0.554652.
Test: 2018-08-05T00:17:19.548254: step 11470, loss 0.548662.
Train: 2018-08-05T00:17:19.798227: step 11471, loss 0.595059.
Train: 2018-08-05T00:17:20.048139: step 11472, loss 0.659724.
Train: 2018-08-05T00:17:20.313731: step 11473, loss 0.522433.
Train: 2018-08-05T00:17:20.548022: step 11474, loss 0.570823.
Train: 2018-08-05T00:17:20.797993: step 11475, loss 0.603016.
Train: 2018-08-05T00:17:21.001070: step 11476, loss 0.528542.
Train: 2018-08-05T00:17:21.250983: step 11477, loss 0.546773.
Train: 2018-08-05T00:17:21.500953: step 11478, loss 0.570848.
Train: 2018-08-05T00:17:21.750894: step 11479, loss 0.538773.
Train: 2018-08-05T00:17:22.000807: step 11480, loss 0.51469.
Test: 2018-08-05T00:17:23.250512: step 11480, loss 0.548271.
Train: 2018-08-05T00:17:23.484863: step 11481, loss 0.54673.
Train: 2018-08-05T00:17:23.734805: step 11482, loss 0.498363.
Train: 2018-08-05T00:17:23.984741: step 11483, loss 0.506178.
Train: 2018-08-05T00:17:24.234687: step 11484, loss 0.538327.
Train: 2018-08-05T00:17:24.484599: step 11485, loss 0.554468.
Train: 2018-08-05T00:17:24.734571: step 11486, loss 0.619906.
Train: 2018-08-05T00:17:24.984512: step 11487, loss 0.595392.
Train: 2018-08-05T00:17:25.234449: step 11488, loss 0.521424.
Train: 2018-08-05T00:17:25.484396: step 11489, loss 0.570758.
Train: 2018-08-05T00:17:25.734337: step 11490, loss 0.545991.
Test: 2018-08-05T00:17:26.999634: step 11490, loss 0.548482.
Train: 2018-08-05T00:17:27.233956: step 11491, loss 0.612112.
Train: 2018-08-05T00:17:27.483927: step 11492, loss 0.628681.
Train: 2018-08-05T00:17:27.749489: step 11493, loss 0.521168.
Train: 2018-08-05T00:17:27.983811: step 11494, loss 0.55423.
Train: 2018-08-05T00:17:28.233746: step 11495, loss 0.579018.
Train: 2018-08-05T00:17:28.483662: step 11496, loss 0.562498.
Train: 2018-08-05T00:17:28.733634: step 11497, loss 0.57901.
Train: 2018-08-05T00:17:28.983576: step 11498, loss 0.570756.
Train: 2018-08-05T00:17:29.233487: step 11499, loss 0.488384.
Train: 2018-08-05T00:17:29.483428: step 11500, loss 0.546027.
Test: 2018-08-05T00:17:30.733136: step 11500, loss 0.549104.
Train: 2018-08-05T00:17:31.623552: step 11501, loss 0.562505.
Train: 2018-08-05T00:17:31.857903: step 11502, loss 0.512948.
Train: 2018-08-05T00:17:32.123436: step 11503, loss 0.537662.
Train: 2018-08-05T00:17:32.357786: step 11504, loss 0.521008.
Train: 2018-08-05T00:17:32.607729: step 11505, loss 0.562445.
Train: 2018-08-05T00:17:32.857669: step 11506, loss 0.570763.
Train: 2018-08-05T00:17:33.107610: step 11507, loss 0.612527.
Train: 2018-08-05T00:17:33.357522: step 11508, loss 0.528996.
Train: 2018-08-05T00:17:33.623109: step 11509, loss 0.503864.
Train: 2018-08-05T00:17:33.873025: step 11510, loss 0.612687.
Test: 2018-08-05T00:17:35.138354: step 11510, loss 0.549739.
Train: 2018-08-05T00:17:35.372705: step 11511, loss 0.604323.
Train: 2018-08-05T00:17:35.622616: step 11512, loss 0.554017.
Train: 2018-08-05T00:17:35.872588: step 11513, loss 0.537263.
Train: 2018-08-05T00:17:36.122498: step 11514, loss 0.612671.
Train: 2018-08-05T00:17:36.372471: step 11515, loss 0.528941.
Train: 2018-08-05T00:17:36.622381: step 11516, loss 0.604222.
Train: 2018-08-05T00:17:36.872353: step 11517, loss 0.545721.
Train: 2018-08-05T00:17:37.122294: step 11518, loss 0.562426.
Train: 2018-08-05T00:17:37.372206: step 11519, loss 0.520785.
Train: 2018-08-05T00:17:37.622178: step 11520, loss 0.529122.
Test: 2018-08-05T00:17:38.871855: step 11520, loss 0.548132.
Train: 2018-08-05T00:17:39.106175: step 11521, loss 0.545767.
Train: 2018-08-05T00:17:39.356147: step 11522, loss 0.60412.
Train: 2018-08-05T00:17:39.606089: step 11523, loss 0.54576.
Train: 2018-08-05T00:17:39.856031: step 11524, loss 0.562431.
Train: 2018-08-05T00:17:40.105971: step 11525, loss 0.579096.
Train: 2018-08-05T00:17:40.355913: step 11526, loss 0.537458.
Train: 2018-08-05T00:17:40.605854: step 11527, loss 0.512489.
Train: 2018-08-05T00:17:40.855795: step 11528, loss 0.529102.
Train: 2018-08-05T00:17:41.105737: step 11529, loss 0.562422.
Train: 2018-08-05T00:17:41.355678: step 11530, loss 0.512286.
Test: 2018-08-05T00:17:42.605356: step 11530, loss 0.549384.
Train: 2018-08-05T00:17:42.855298: step 11531, loss 0.55403.
Train: 2018-08-05T00:17:43.105269: step 11532, loss 0.461723.
Train: 2018-08-05T00:17:43.355181: step 11533, loss 0.587641.
Train: 2018-08-05T00:17:43.620773: step 11534, loss 0.621483.
Train: 2018-08-05T00:17:43.886336: step 11535, loss 0.520092.
Train: 2018-08-05T00:17:44.136278: step 11536, loss 0.621629.
Train: 2018-08-05T00:17:44.386218: step 11537, loss 0.528487.
Train: 2018-08-05T00:17:44.636160: step 11538, loss 0.503055.
Train: 2018-08-05T00:17:44.901694: step 11539, loss 0.579319.
Train: 2018-08-05T00:17:45.136047: step 11540, loss 0.587824.
Test: 2018-08-05T00:17:46.401341: step 11540, loss 0.547579.
Train: 2018-08-05T00:17:46.635693: step 11541, loss 0.596315.
Train: 2018-08-05T00:17:46.885604: step 11542, loss 0.621734.
Train: 2018-08-05T00:17:47.135576: step 11543, loss 0.596208.
Train: 2018-08-05T00:17:47.401108: step 11544, loss 0.503318.
Train: 2018-08-05T00:17:47.635459: step 11545, loss 0.570797.
Train: 2018-08-05T00:17:47.885369: step 11546, loss 0.486732.
Train: 2018-08-05T00:17:48.135342: step 11547, loss 0.570788.
Train: 2018-08-05T00:17:48.385252: step 11548, loss 0.587589.
Train: 2018-08-05T00:17:48.635224: step 11549, loss 0.646303.
Train: 2018-08-05T00:17:48.900786: step 11550, loss 0.587503.
Test: 2018-08-05T00:17:50.150464: step 11550, loss 0.54659.
Train: 2018-08-05T00:17:50.400436: step 11551, loss 0.529088.
Train: 2018-08-05T00:17:50.650347: step 11552, loss 0.595703.
Train: 2018-08-05T00:17:50.900319: step 11553, loss 0.587333.
Train: 2018-08-05T00:17:51.150229: step 11554, loss 0.653343.
Train: 2018-08-05T00:17:51.415823: step 11555, loss 0.570759.
Train: 2018-08-05T00:17:51.665734: step 11556, loss 0.636145.
Train: 2018-08-05T00:17:51.915706: step 11557, loss 0.54643.
Train: 2018-08-05T00:17:52.165648: step 11558, loss 0.578885.
Train: 2018-08-05T00:17:52.415589: step 11559, loss 0.546737.
Train: 2018-08-05T00:17:52.681121: step 11560, loss 0.602861.
Test: 2018-08-05T00:17:53.915207: step 11560, loss 0.548724.
Train: 2018-08-05T00:17:54.180771: step 11561, loss 0.58682.
Train: 2018-08-05T00:17:54.430741: step 11562, loss 0.602629.
Train: 2018-08-05T00:17:54.680653: step 11563, loss 0.578865.
Train: 2018-08-05T00:17:54.930624: step 11564, loss 0.531814.
Train: 2018-08-05T00:17:55.180566: step 11565, loss 0.531984.
Train: 2018-08-05T00:17:55.430508: step 11566, loss 0.547687.
Train: 2018-08-05T00:17:55.680444: step 11567, loss 0.485391.
Train: 2018-08-05T00:17:55.930390: step 11568, loss 0.555485.
Train: 2018-08-05T00:17:56.180327: step 11569, loss 0.547626.
Train: 2018-08-05T00:17:56.430273: step 11570, loss 0.578879.
Test: 2018-08-05T00:17:57.679950: step 11570, loss 0.550987.
Train: 2018-08-05T00:17:57.929893: step 11571, loss 0.492585.
Train: 2018-08-05T00:17:58.179864: step 11572, loss 0.578867.
Train: 2018-08-05T00:17:58.429775: step 11573, loss 0.515681.
Train: 2018-08-05T00:17:58.679717: step 11574, loss 0.547119.
Train: 2018-08-05T00:17:58.929688: step 11575, loss 0.578866.
Train: 2018-08-05T00:17:59.179630: step 11576, loss 0.562943.
Train: 2018-08-05T00:17:59.429541: step 11577, loss 0.602977.
Train: 2018-08-05T00:17:59.695135: step 11578, loss 0.514576.
Train: 2018-08-05T00:17:59.945077: step 11579, loss 0.603068.
Train: 2018-08-05T00:18:00.210608: step 11580, loss 0.643491.
Test: 2018-08-05T00:18:01.460315: step 11580, loss 0.549828.
Train: 2018-08-05T00:18:01.694636: step 11581, loss 0.530472.
Train: 2018-08-05T00:18:01.944602: step 11582, loss 0.53852.
Train: 2018-08-05T00:18:02.194520: step 11583, loss 0.578889.
Train: 2018-08-05T00:18:02.444491: step 11584, loss 0.635513.
Train: 2018-08-05T00:18:02.694433: step 11585, loss 0.538486.
Train: 2018-08-05T00:18:02.944374: step 11586, loss 0.619273.
Train: 2018-08-05T00:18:03.209929: step 11587, loss 0.586991.
Train: 2018-08-05T00:18:03.444226: step 11588, loss 0.514484.
Train: 2018-08-05T00:18:03.694199: step 11589, loss 0.570835.
Train: 2018-08-05T00:18:03.944140: step 11590, loss 0.594948.
Test: 2018-08-05T00:18:05.209437: step 11590, loss 0.54941.
Train: 2018-08-05T00:18:05.443759: step 11591, loss 0.57886.
Train: 2018-08-05T00:18:05.693723: step 11592, loss 0.546805.
Train: 2018-08-05T00:18:05.943672: step 11593, loss 0.482777.
Train: 2018-08-05T00:18:06.193613: step 11594, loss 0.594909.
Train: 2018-08-05T00:18:06.443523: step 11595, loss 0.594913.
Train: 2018-08-05T00:18:06.709110: step 11596, loss 0.546788.
Train: 2018-08-05T00:18:06.943438: step 11597, loss 0.562824.
Train: 2018-08-05T00:18:07.193379: step 11598, loss 0.594916.
Train: 2018-08-05T00:18:07.443321: step 11599, loss 0.538765.
Train: 2018-08-05T00:18:07.693232: step 11600, loss 0.562822.
Test: 2018-08-05T00:18:08.942938: step 11600, loss 0.549414.
Train: 2018-08-05T00:18:09.817733: step 11601, loss 0.570842.
Train: 2018-08-05T00:18:10.067706: step 11602, loss 0.675201.
Train: 2018-08-05T00:18:10.317648: step 11603, loss 0.538828.
Train: 2018-08-05T00:18:10.567589: step 11604, loss 0.578862.
Train: 2018-08-05T00:18:10.817524: step 11605, loss 0.507033.
Train: 2018-08-05T00:18:11.067472: step 11606, loss 0.530981.
Train: 2018-08-05T00:18:11.379867: step 11607, loss 0.578863.
Train: 2018-08-05T00:18:11.629809: step 11608, loss 0.578871.
Train: 2018-08-05T00:18:11.879784: step 11609, loss 0.578861.
Train: 2018-08-05T00:18:12.129691: step 11610, loss 0.57087.
Test: 2018-08-05T00:18:13.379398: step 11610, loss 0.550274.
Train: 2018-08-05T00:18:13.629340: step 11611, loss 0.530922.
Train: 2018-08-05T00:18:13.879312: step 11612, loss 0.5069.
Train: 2018-08-05T00:18:14.144875: step 11613, loss 0.554826.
Train: 2018-08-05T00:18:14.379196: step 11614, loss 0.602963.
Train: 2018-08-05T00:18:14.629132: step 11615, loss 0.538673.
Train: 2018-08-05T00:18:14.879049: step 11616, loss 0.570825.
Train: 2018-08-05T00:18:15.129020: step 11617, loss 0.482113.
Train: 2018-08-05T00:18:15.378961: step 11618, loss 0.643617.
Train: 2018-08-05T00:18:15.628903: step 11619, loss 0.5546.
Train: 2018-08-05T00:18:15.878813: step 11620, loss 0.562686.
Test: 2018-08-05T00:18:17.128521: step 11620, loss 0.549305.
Train: 2018-08-05T00:18:17.378493: step 11621, loss 0.538324.
Train: 2018-08-05T00:18:17.628434: step 11622, loss 0.562655.
Train: 2018-08-05T00:18:17.878379: step 11623, loss 0.5952.
Train: 2018-08-05T00:18:18.128317: step 11624, loss 0.611499.
Train: 2018-08-05T00:18:18.378254: step 11625, loss 0.578918.
Train: 2018-08-05T00:18:18.628201: step 11626, loss 0.522007.
Train: 2018-08-05T00:18:18.831271: step 11627, loss 0.527964.
Train: 2018-08-05T00:18:19.081189: step 11628, loss 0.546367.
Train: 2018-08-05T00:18:19.331131: step 11629, loss 0.562629.
Train: 2018-08-05T00:18:19.581103: step 11630, loss 0.570778.
Test: 2018-08-05T00:18:20.830779: step 11630, loss 0.548411.
Train: 2018-08-05T00:18:21.080721: step 11631, loss 0.570771.
Train: 2018-08-05T00:18:21.330662: step 11632, loss 0.513598.
Train: 2018-08-05T00:18:21.580634: step 11633, loss 0.546225.
Train: 2018-08-05T00:18:21.830546: step 11634, loss 0.537974.
Train: 2018-08-05T00:18:22.080517: step 11635, loss 0.48862.
Train: 2018-08-05T00:18:22.346049: step 11636, loss 0.611981.
Train: 2018-08-05T00:18:22.580402: step 11637, loss 0.612073.
Train: 2018-08-05T00:18:22.830312: step 11638, loss 0.554226.
Train: 2018-08-05T00:18:23.080283: step 11639, loss 0.570752.
Train: 2018-08-05T00:18:23.345846: step 11640, loss 0.554198.
Test: 2018-08-05T00:18:24.611143: step 11640, loss 0.548265.
Train: 2018-08-05T00:18:24.845465: step 11641, loss 0.603871.
Train: 2018-08-05T00:18:25.095406: step 11642, loss 0.570766.
Train: 2018-08-05T00:18:25.345377: step 11643, loss 0.570748.
Train: 2018-08-05T00:18:25.595319: step 11644, loss 0.579018.
Train: 2018-08-05T00:18:25.845229: step 11645, loss 0.60375.
Train: 2018-08-05T00:18:26.095202: step 11646, loss 0.636583.
Train: 2018-08-05T00:18:26.360764: step 11647, loss 0.53798.
Train: 2018-08-05T00:18:26.595085: step 11648, loss 0.603454.
Train: 2018-08-05T00:18:26.860647: step 11649, loss 0.611486.
Train: 2018-08-05T00:18:27.110585: step 11650, loss 0.55459.
Test: 2018-08-05T00:18:28.375887: step 11650, loss 0.549068.
Train: 2018-08-05T00:18:28.610235: step 11651, loss 0.611181.
Train: 2018-08-05T00:18:28.860180: step 11652, loss 0.554761.
Train: 2018-08-05T00:18:29.110090: step 11653, loss 0.594877.
Train: 2018-08-05T00:18:29.360062: step 11654, loss 0.539001.
Train: 2018-08-05T00:18:29.610003: step 11655, loss 0.507335.
Train: 2018-08-05T00:18:29.859945: step 11656, loss 0.602672.
Train: 2018-08-05T00:18:30.109887: step 11657, loss 0.570938.
Train: 2018-08-05T00:18:30.359799: step 11658, loss 0.563046.
Train: 2018-08-05T00:18:30.609740: step 11659, loss 0.61834.
Train: 2018-08-05T00:18:30.859711: step 11660, loss 0.563116.
Test: 2018-08-05T00:18:32.109388: step 11660, loss 0.549668.
Train: 2018-08-05T00:18:32.359354: step 11661, loss 0.563154.
Train: 2018-08-05T00:18:32.609271: step 11662, loss 0.492584.
Train: 2018-08-05T00:18:32.874864: step 11663, loss 0.563176.
Train: 2018-08-05T00:18:33.124805: step 11664, loss 0.586726.
Train: 2018-08-05T00:18:33.390368: step 11665, loss 0.54745.
Train: 2018-08-05T00:18:33.640310: step 11666, loss 0.531702.
Train: 2018-08-05T00:18:33.890251: step 11667, loss 0.563116.
Train: 2018-08-05T00:18:34.140162: step 11668, loss 0.547307.
Train: 2018-08-05T00:18:34.405726: step 11669, loss 0.53933.
Train: 2018-08-05T00:18:34.655691: step 11670, loss 0.586787.
Test: 2018-08-05T00:18:35.905374: step 11670, loss 0.549888.
Train: 2018-08-05T00:18:36.139725: step 11671, loss 0.531195.
Train: 2018-08-05T00:18:36.389636: step 11672, loss 0.554962.
Train: 2018-08-05T00:18:36.639608: step 11673, loss 0.514956.
Train: 2018-08-05T00:18:36.889549: step 11674, loss 0.522738.
Train: 2018-08-05T00:18:37.139459: step 11675, loss 0.570811.
Train: 2018-08-05T00:18:37.389403: step 11676, loss 0.5708.
Train: 2018-08-05T00:18:37.639374: step 11677, loss 0.660078.
Train: 2018-08-05T00:18:37.889285: step 11678, loss 0.538302.
Train: 2018-08-05T00:18:38.139256: step 11679, loss 0.587067.
Train: 2018-08-05T00:18:38.389198: step 11680, loss 0.54638.
Test: 2018-08-05T00:18:39.638874: step 11680, loss 0.548667.
Train: 2018-08-05T00:18:39.873221: step 11681, loss 0.562634.
Train: 2018-08-05T00:18:40.123137: step 11682, loss 0.538193.
Train: 2018-08-05T00:18:40.373108: step 11683, loss 0.489203.
Train: 2018-08-05T00:18:40.623052: step 11684, loss 0.554404.
Train: 2018-08-05T00:18:40.872991: step 11685, loss 0.62819.
Train: 2018-08-05T00:18:41.122935: step 11686, loss 0.537912.
Train: 2018-08-05T00:18:41.372868: step 11687, loss 0.488526.
Train: 2018-08-05T00:18:41.622785: step 11688, loss 0.52127.
Train: 2018-08-05T00:18:41.872757: step 11689, loss 0.521095.
Train: 2018-08-05T00:18:42.122700: step 11690, loss 0.471005.
Test: 2018-08-05T00:18:43.372376: step 11690, loss 0.549017.
Train: 2018-08-05T00:18:43.637939: step 11691, loss 0.579111.
Train: 2018-08-05T00:18:43.872259: step 11692, loss 0.596066.
Train: 2018-08-05T00:18:44.122230: step 11693, loss 0.528656.
Train: 2018-08-05T00:18:44.372172: step 11694, loss 0.570808.
Train: 2018-08-05T00:18:44.622084: step 11695, loss 0.528446.
Train: 2018-08-05T00:18:44.872024: step 11696, loss 0.536826.
Train: 2018-08-05T00:18:45.121997: step 11697, loss 0.570874.
Train: 2018-08-05T00:18:45.371938: step 11698, loss 0.545245.
Train: 2018-08-05T00:18:45.621848: step 11699, loss 0.493796.
Train: 2018-08-05T00:18:45.871815: step 11700, loss 0.631098.
Test: 2018-08-05T00:18:47.121497: step 11700, loss 0.547571.
Train: 2018-08-05T00:18:48.011946: step 11701, loss 0.59674.
Train: 2018-08-05T00:18:48.261857: step 11702, loss 0.562332.
Train: 2018-08-05T00:18:48.511827: step 11703, loss 0.553739.
Train: 2018-08-05T00:18:48.761739: step 11704, loss 0.596722.
Train: 2018-08-05T00:18:49.027301: step 11705, loss 0.588092.
Train: 2018-08-05T00:18:49.261623: step 11706, loss 0.528064.
Train: 2018-08-05T00:18:49.511562: step 11707, loss 0.596561.
Train: 2018-08-05T00:18:49.761506: step 11708, loss 0.587951.
Train: 2018-08-05T00:18:50.011445: step 11709, loss 0.562342.
Train: 2018-08-05T00:18:50.261387: step 11710, loss 0.579329.
Test: 2018-08-05T00:18:51.511095: step 11710, loss 0.549195.
Train: 2018-08-05T00:18:51.761061: step 11711, loss 0.5285.
Train: 2018-08-05T00:18:52.011009: step 11712, loss 0.562368.
Train: 2018-08-05T00:18:52.260949: step 11713, loss 0.520237.
Train: 2018-08-05T00:18:52.510891: step 11714, loss 0.503459.
Train: 2018-08-05T00:18:52.760802: step 11715, loss 0.562376.
Train: 2018-08-05T00:18:53.010774: step 11716, loss 0.629719.
Train: 2018-08-05T00:18:53.256614: step 11717, loss 0.51197.
Train: 2018-08-05T00:18:53.506555: step 11718, loss 0.528798.
Train: 2018-08-05T00:18:53.756466: step 11719, loss 0.671566.
Train: 2018-08-05T00:18:54.006438: step 11720, loss 0.612661.
Test: 2018-08-05T00:18:55.256114: step 11720, loss 0.548482.
Train: 2018-08-05T00:18:55.490436: step 11721, loss 0.545728.
Train: 2018-08-05T00:18:55.756028: step 11722, loss 0.56244.
Train: 2018-08-05T00:18:55.990319: step 11723, loss 0.61225.
Train: 2018-08-05T00:18:56.240260: step 11724, loss 0.488092.
Train: 2018-08-05T00:18:56.490230: step 11725, loss 0.570756.
Train: 2018-08-05T00:18:56.740173: step 11726, loss 0.554285.
Train: 2018-08-05T00:18:56.990115: step 11727, loss 0.562534.
Train: 2018-08-05T00:18:57.240055: step 11728, loss 0.578972.
Train: 2018-08-05T00:18:57.489967: step 11729, loss 0.587159.
Train: 2018-08-05T00:18:57.739934: step 11730, loss 0.628032.
Test: 2018-08-05T00:18:58.989617: step 11730, loss 0.549759.
Train: 2018-08-05T00:18:59.239557: step 11731, loss 0.538167.
Train: 2018-08-05T00:18:59.489529: step 11732, loss 0.587044.
Train: 2018-08-05T00:18:59.739469: step 11733, loss 0.546473.
Train: 2018-08-05T00:18:59.989411: step 11734, loss 0.58698.
Train: 2018-08-05T00:19:00.239353: step 11735, loss 0.570814.
Train: 2018-08-05T00:19:00.489295: step 11736, loss 0.530584.
Train: 2018-08-05T00:19:00.739205: step 11737, loss 0.522608.
Train: 2018-08-05T00:19:00.989177: step 11738, loss 0.546725.
Train: 2018-08-05T00:19:01.254710: step 11739, loss 0.554753.
Train: 2018-08-05T00:19:01.489064: step 11740, loss 0.498426.
Test: 2018-08-05T00:19:02.738738: step 11740, loss 0.548918.
Train: 2018-08-05T00:19:02.988679: step 11741, loss 0.54663.
Train: 2018-08-05T00:19:03.238650: step 11742, loss 0.498061.
Train: 2018-08-05T00:19:03.488563: step 11743, loss 0.578904.
Train: 2018-08-05T00:19:03.738533: step 11744, loss 0.587063.
Train: 2018-08-05T00:19:03.988474: step 11745, loss 0.5463.
Train: 2018-08-05T00:19:04.238416: step 11746, loss 0.578945.
Train: 2018-08-05T00:19:04.488352: step 11747, loss 0.611713.
Train: 2018-08-05T00:19:04.753890: step 11748, loss 0.537992.
Train: 2018-08-05T00:19:05.003862: step 11749, loss 0.60356.
Train: 2018-08-05T00:19:05.253805: step 11750, loss 0.562567.
Test: 2018-08-05T00:19:06.503481: step 11750, loss 0.549446.
Train: 2018-08-05T00:19:06.737802: step 11751, loss 0.554372.
Train: 2018-08-05T00:19:06.987773: step 11752, loss 0.611737.
Train: 2018-08-05T00:19:07.237684: step 11753, loss 0.578951.
Train: 2018-08-05T00:19:07.487656: step 11754, loss 0.57894.
Train: 2018-08-05T00:19:07.753214: step 11755, loss 0.570774.
Train: 2018-08-05T00:19:07.987539: step 11756, loss 0.603347.
Train: 2018-08-05T00:19:08.237480: step 11757, loss 0.546424.
Train: 2018-08-05T00:19:08.487391: step 11758, loss 0.603213.
Train: 2018-08-05T00:19:08.737363: step 11759, loss 0.595053.
Train: 2018-08-05T00:19:08.987275: step 11760, loss 0.627217.
Test: 2018-08-05T00:19:10.236981: step 11760, loss 0.549247.
Train: 2018-08-05T00:19:10.486948: step 11761, loss 0.522723.
Train: 2018-08-05T00:19:10.752516: step 11762, loss 0.546874.
Train: 2018-08-05T00:19:11.002458: step 11763, loss 0.634723.
Train: 2018-08-05T00:19:11.252399: step 11764, loss 0.547054.
Train: 2018-08-05T00:19:11.502340: step 11765, loss 0.555071.
Train: 2018-08-05T00:19:11.752283: step 11766, loss 0.491818.
Train: 2018-08-05T00:19:12.002224: step 11767, loss 0.547202.
Train: 2018-08-05T00:19:12.252166: step 11768, loss 0.515505.
Train: 2018-08-05T00:19:12.502107: step 11769, loss 0.539184.
Train: 2018-08-05T00:19:12.752047: step 11770, loss 0.531126.
Test: 2018-08-05T00:19:14.017346: step 11770, loss 0.549383.
Train: 2018-08-05T00:19:14.345424: step 11771, loss 0.650688.
Train: 2018-08-05T00:19:14.595337: step 11772, loss 0.554897.
Train: 2018-08-05T00:19:14.845308: step 11773, loss 0.4829.
Train: 2018-08-05T00:19:15.095249: step 11774, loss 0.522673.
Train: 2018-08-05T00:19:15.345191: step 11775, loss 0.635413.
Train: 2018-08-05T00:19:15.595132: step 11776, loss 0.530295.
Train: 2018-08-05T00:19:15.845073: step 11777, loss 0.513812.
Train: 2018-08-05T00:19:16.048151: step 11778, loss 0.579115.
Train: 2018-08-05T00:19:16.298092: step 11779, loss 0.562198.
Train: 2018-08-05T00:19:16.548034: step 11780, loss 0.551951.
Test: 2018-08-05T00:19:17.828953: step 11780, loss 0.548189.
Train: 2018-08-05T00:19:18.063303: step 11781, loss 0.526457.
Train: 2018-08-05T00:19:18.313215: step 11782, loss 0.618661.
Train: 2018-08-05T00:19:18.563189: step 11783, loss 0.547055.
Train: 2018-08-05T00:19:18.828751: step 11784, loss 0.567414.
Train: 2018-08-05T00:19:19.078693: step 11785, loss 0.643313.
Train: 2018-08-05T00:19:19.328634: step 11786, loss 0.511189.
Train: 2018-08-05T00:19:19.578544: step 11787, loss 0.520566.
Train: 2018-08-05T00:19:19.828517: step 11788, loss 0.562718.
Train: 2018-08-05T00:19:20.078459: step 11789, loss 0.620247.
Train: 2018-08-05T00:19:20.328400: step 11790, loss 0.48796.
Test: 2018-08-05T00:19:21.593697: step 11790, loss 0.549423.
Train: 2018-08-05T00:19:21.843668: step 11791, loss 0.497749.
Train: 2018-08-05T00:19:22.093580: step 11792, loss 0.562375.
Train: 2018-08-05T00:19:22.343552: step 11793, loss 0.578393.
Train: 2018-08-05T00:19:22.593493: step 11794, loss 0.545829.
Train: 2018-08-05T00:19:22.859027: step 11795, loss 0.547234.
Train: 2018-08-05T00:19:23.108998: step 11796, loss 0.537179.
Train: 2018-08-05T00:19:23.358943: step 11797, loss 0.595035.
Train: 2018-08-05T00:19:23.608882: step 11798, loss 0.645632.
Train: 2018-08-05T00:19:23.858816: step 11799, loss 0.538068.
Train: 2018-08-05T00:19:24.108734: step 11800, loss 0.52992.
Test: 2018-08-05T00:19:25.374061: step 11800, loss 0.548713.
Train: 2018-08-05T00:19:26.217615: step 11801, loss 0.529514.
Train: 2018-08-05T00:19:26.467586: step 11802, loss 0.480668.
Train: 2018-08-05T00:19:26.717527: step 11803, loss 0.545973.
Train: 2018-08-05T00:19:26.967469: step 11804, loss 0.570774.
Train: 2018-08-05T00:19:27.217405: step 11805, loss 0.512918.
Train: 2018-08-05T00:19:27.467352: step 11806, loss 0.579199.
Train: 2018-08-05T00:19:27.717294: step 11807, loss 0.52899.
Train: 2018-08-05T00:19:27.967238: step 11808, loss 0.596007.
Train: 2018-08-05T00:19:28.217176: step 11809, loss 0.570787.
Train: 2018-08-05T00:19:28.467118: step 11810, loss 0.562349.
Test: 2018-08-05T00:19:29.701173: step 11810, loss 0.54916.
Train: 2018-08-05T00:19:29.951147: step 11811, loss 0.570948.
Train: 2018-08-05T00:19:30.216695: step 11812, loss 0.495354.
Train: 2018-08-05T00:19:30.451031: step 11813, loss 0.638496.
Train: 2018-08-05T00:19:30.700971: step 11814, loss 0.495338.
Train: 2018-08-05T00:19:30.950882: step 11815, loss 0.60438.
Train: 2018-08-05T00:19:31.200823: step 11816, loss 0.570723.
Train: 2018-08-05T00:19:31.450796: step 11817, loss 0.50387.
Train: 2018-08-05T00:19:31.700737: step 11818, loss 0.579043.
Train: 2018-08-05T00:19:31.950678: step 11819, loss 0.553949.
Train: 2018-08-05T00:19:32.200620: step 11820, loss 0.545695.
Test: 2018-08-05T00:19:33.450295: step 11820, loss 0.548819.
Train: 2018-08-05T00:19:33.684650: step 11821, loss 0.545704.
Train: 2018-08-05T00:19:33.934587: step 11822, loss 0.570859.
Train: 2018-08-05T00:19:34.184528: step 11823, loss 0.612683.
Train: 2018-08-05T00:19:34.450061: step 11824, loss 0.554061.
Train: 2018-08-05T00:19:34.684412: step 11825, loss 0.512328.
Train: 2018-08-05T00:19:34.934353: step 11826, loss 0.529058.
Train: 2018-08-05T00:19:35.184294: step 11827, loss 0.537432.
Train: 2018-08-05T00:19:35.434238: step 11828, loss 0.503734.
Train: 2018-08-05T00:19:35.684177: step 11829, loss 0.596038.
Train: 2018-08-05T00:19:35.934119: step 11830, loss 0.545631.
Test: 2018-08-05T00:19:37.183796: step 11830, loss 0.549317.
Train: 2018-08-05T00:19:37.433770: step 11831, loss 0.587616.
Train: 2018-08-05T00:19:37.683709: step 11832, loss 0.562397.
Train: 2018-08-05T00:19:37.933651: step 11833, loss 0.528769.
Train: 2018-08-05T00:19:38.199184: step 11834, loss 0.49509.
Train: 2018-08-05T00:19:38.449124: step 11835, loss 0.562333.
Train: 2018-08-05T00:19:38.699067: step 11836, loss 0.570851.
Train: 2018-08-05T00:19:38.949039: step 11837, loss 0.486288.
Train: 2018-08-05T00:19:39.198950: step 11838, loss 0.596291.
Train: 2018-08-05T00:19:39.448922: step 11839, loss 0.587825.
Train: 2018-08-05T00:19:39.714484: step 11840, loss 0.596373.
Test: 2018-08-05T00:19:40.979782: step 11840, loss 0.547979.
Train: 2018-08-05T00:19:41.229754: step 11841, loss 0.468981.
Train: 2018-08-05T00:19:41.495286: step 11842, loss 0.528359.
Train: 2018-08-05T00:19:41.745228: step 11843, loss 0.536783.
Train: 2018-08-05T00:19:41.995193: step 11844, loss 0.562245.
Train: 2018-08-05T00:19:42.245142: step 11845, loss 0.553793.
Train: 2018-08-05T00:19:42.495083: step 11846, loss 0.545201.
Train: 2018-08-05T00:19:42.745025: step 11847, loss 0.579544.
Train: 2018-08-05T00:19:42.994936: step 11848, loss 0.570894.
Train: 2018-08-05T00:19:43.244907: step 11849, loss 0.5194.
Train: 2018-08-05T00:19:43.494818: step 11850, loss 0.605111.
Test: 2018-08-05T00:19:44.744527: step 11850, loss 0.547809.
Train: 2018-08-05T00:19:44.978877: step 11851, loss 0.613812.
Train: 2018-08-05T00:19:45.228787: step 11852, loss 0.519575.
Train: 2018-08-05T00:19:45.494350: step 11853, loss 0.51948.
Train: 2018-08-05T00:19:45.728702: step 11854, loss 0.562394.
Train: 2018-08-05T00:19:45.978643: step 11855, loss 0.665064.
Train: 2018-08-05T00:19:46.228583: step 11856, loss 0.638942.
Train: 2018-08-05T00:19:46.494116: step 11857, loss 0.613166.
Train: 2018-08-05T00:19:46.744088: step 11858, loss 0.553964.
Train: 2018-08-05T00:19:46.994030: step 11859, loss 0.54566.
Train: 2018-08-05T00:19:47.243965: step 11860, loss 0.58746.
Test: 2018-08-05T00:19:48.493647: step 11860, loss 0.548966.
Train: 2018-08-05T00:19:48.727999: step 11861, loss 0.587346.
Train: 2018-08-05T00:19:48.977909: step 11862, loss 0.521172.
Train: 2018-08-05T00:19:49.227881: step 11863, loss 0.620165.
Train: 2018-08-05T00:19:49.477823: step 11864, loss 0.537986.
Train: 2018-08-05T00:19:49.727734: step 11865, loss 0.562597.
Train: 2018-08-05T00:19:49.977676: step 11866, loss 0.554494.
Train: 2018-08-05T00:19:50.227642: step 11867, loss 0.627632.
Train: 2018-08-05T00:19:50.477588: step 11868, loss 0.595068.
Train: 2018-08-05T00:19:50.727525: step 11869, loss 0.56277.
Train: 2018-08-05T00:19:50.977471: step 11870, loss 0.570845.
Test: 2018-08-05T00:19:52.227148: step 11870, loss 0.54934.
Train: 2018-08-05T00:19:52.461469: step 11871, loss 0.586857.
Train: 2018-08-05T00:19:52.711411: step 11872, loss 0.531074.
Train: 2018-08-05T00:19:52.961351: step 11873, loss 0.547078.
Train: 2018-08-05T00:19:53.211324: step 11874, loss 0.531268.
Train: 2018-08-05T00:19:53.461265: step 11875, loss 0.586786.
Train: 2018-08-05T00:19:53.711206: step 11876, loss 0.570938.
Train: 2018-08-05T00:19:53.961148: step 11877, loss 0.555124.
Train: 2018-08-05T00:19:54.211093: step 11878, loss 0.657956.
Train: 2018-08-05T00:19:54.461031: step 11879, loss 0.539421.
Train: 2018-08-05T00:19:54.710973: step 11880, loss 0.570987.
Test: 2018-08-05T00:19:55.960649: step 11880, loss 0.550368.
Train: 2018-08-05T00:19:56.226214: step 11881, loss 0.547409.
Train: 2018-08-05T00:19:56.460563: step 11882, loss 0.578871.
Train: 2018-08-05T00:19:56.710503: step 11883, loss 0.61813.
Train: 2018-08-05T00:19:56.960445: step 11884, loss 0.57888.
Train: 2018-08-05T00:19:57.210381: step 11885, loss 0.594526.
Train: 2018-08-05T00:19:57.460328: step 11886, loss 0.594496.
Train: 2018-08-05T00:19:57.710241: step 11887, loss 0.617801.
Train: 2018-08-05T00:19:57.975802: step 11888, loss 0.57892.
Train: 2018-08-05T00:19:58.225774: step 11889, loss 0.617562.
Train: 2018-08-05T00:19:58.475715: step 11890, loss 0.586657.
Test: 2018-08-05T00:19:59.709771: step 11890, loss 0.549994.
Train: 2018-08-05T00:19:59.959739: step 11891, loss 0.609629.
Train: 2018-08-05T00:20:00.209656: step 11892, loss 0.59427.
Train: 2018-08-05T00:20:00.459597: step 11893, loss 0.54871.
Train: 2018-08-05T00:20:00.725158: step 11894, loss 0.616906.
Train: 2018-08-05T00:20:00.975101: step 11895, loss 0.55654.
Train: 2018-08-05T00:20:01.225072: step 11896, loss 0.541628.
Train: 2018-08-05T00:20:01.475014: step 11897, loss 0.526719.
Train: 2018-08-05T00:20:01.724954: step 11898, loss 0.481741.
Train: 2018-08-05T00:20:01.974896: step 11899, loss 0.52653.
Train: 2018-08-05T00:20:02.224837: step 11900, loss 0.556464.
Test: 2018-08-05T00:20:03.490136: step 11900, loss 0.550547.
Train: 2018-08-05T00:20:04.411825: step 11901, loss 0.556326.
Train: 2018-08-05T00:20:04.661761: step 11902, loss 0.540944.
Train: 2018-08-05T00:20:04.911709: step 11903, loss 0.594296.
Train: 2018-08-05T00:20:05.161650: step 11904, loss 0.540542.
Train: 2018-08-05T00:20:05.411561: step 11905, loss 0.517154.
Train: 2018-08-05T00:20:05.661532: step 11906, loss 0.617694.
Train: 2018-08-05T00:20:05.911469: step 11907, loss 0.594553.
Train: 2018-08-05T00:20:06.161415: step 11908, loss 0.500627.
Train: 2018-08-05T00:20:06.411352: step 11909, loss 0.594582.
Train: 2018-08-05T00:20:06.661299: step 11910, loss 0.563054.
Test: 2018-08-05T00:20:07.910976: step 11910, loss 0.550151.
Train: 2018-08-05T00:20:08.160919: step 11911, loss 0.578804.
Train: 2018-08-05T00:20:08.410859: step 11912, loss 0.618486.
Train: 2018-08-05T00:20:08.660800: step 11913, loss 0.507208.
Train: 2018-08-05T00:20:08.910742: step 11914, loss 0.538867.
Train: 2018-08-05T00:20:09.160713: step 11915, loss 0.554679.
Train: 2018-08-05T00:20:09.410655: step 11916, loss 0.570703.
Train: 2018-08-05T00:20:09.660567: step 11917, loss 0.570858.
Train: 2018-08-05T00:20:09.910538: step 11918, loss 0.57026.
Train: 2018-08-05T00:20:10.160478: step 11919, loss 0.496484.
Train: 2018-08-05T00:20:10.410420: step 11920, loss 0.561971.
Test: 2018-08-05T00:20:11.660098: step 11920, loss 0.548806.
Train: 2018-08-05T00:20:11.894449: step 11921, loss 0.553328.
Train: 2018-08-05T00:20:12.191256: step 11922, loss 0.602759.
Train: 2018-08-05T00:20:12.441197: step 11923, loss 0.606592.
Train: 2018-08-05T00:20:12.691138: step 11924, loss 0.544509.
Train: 2018-08-05T00:20:12.941047: step 11925, loss 0.505816.
Train: 2018-08-05T00:20:13.191021: step 11926, loss 0.501958.
Train: 2018-08-05T00:20:13.440962: step 11927, loss 0.54269.
Train: 2018-08-05T00:20:13.690904: step 11928, loss 0.584446.
Train: 2018-08-05T00:20:13.893980: step 11929, loss 0.619039.
Train: 2018-08-05T00:20:14.159512: step 11930, loss 0.500626.
Test: 2018-08-05T00:20:15.393635: step 11930, loss 0.547226.
Train: 2018-08-05T00:20:15.643570: step 11931, loss 0.593622.
Train: 2018-08-05T00:20:15.893512: step 11932, loss 0.518195.
Train: 2018-08-05T00:20:16.159075: step 11933, loss 0.58099.
Train: 2018-08-05T00:20:16.393395: step 11934, loss 0.491717.
Train: 2018-08-05T00:20:16.643330: step 11935, loss 0.553558.
Train: 2018-08-05T00:20:16.893277: step 11936, loss 0.622595.
Train: 2018-08-05T00:20:17.143219: step 11937, loss 0.518271.
Train: 2018-08-05T00:20:17.393160: step 11938, loss 0.547261.
Train: 2018-08-05T00:20:17.643107: step 11939, loss 0.614782.
Train: 2018-08-05T00:20:17.893043: step 11940, loss 0.554373.
Test: 2018-08-05T00:20:19.142720: step 11940, loss 0.547989.
Train: 2018-08-05T00:20:19.377043: step 11941, loss 0.597171.
Train: 2018-08-05T00:20:19.627012: step 11942, loss 0.547889.
Train: 2018-08-05T00:20:19.892579: step 11943, loss 0.545049.
Train: 2018-08-05T00:20:20.142516: step 11944, loss 0.503929.
Train: 2018-08-05T00:20:20.392458: step 11945, loss 0.545702.
Train: 2018-08-05T00:20:20.642394: step 11946, loss 0.588323.
Train: 2018-08-05T00:20:20.892342: step 11947, loss 0.562761.
Train: 2018-08-05T00:20:21.142253: step 11948, loss 0.56175.
Train: 2018-08-05T00:20:21.392225: step 11949, loss 0.545728.
Train: 2018-08-05T00:20:21.642165: step 11950, loss 0.496819.
Test: 2018-08-05T00:20:22.891843: step 11950, loss 0.548431.
Train: 2018-08-05T00:20:23.126194: step 11951, loss 0.512874.
Train: 2018-08-05T00:20:23.376103: step 11952, loss 0.587666.
Train: 2018-08-05T00:20:23.626076: step 11953, loss 0.579603.
Train: 2018-08-05T00:20:23.875986: step 11954, loss 0.570262.
Train: 2018-08-05T00:20:24.125959: step 11955, loss 0.546251.
Train: 2018-08-05T00:20:24.391491: step 11956, loss 0.512682.
Train: 2018-08-05T00:20:24.625842: step 11957, loss 0.562135.
Train: 2018-08-05T00:20:24.875783: step 11958, loss 0.595592.
Train: 2018-08-05T00:20:25.125695: step 11959, loss 0.612045.
Train: 2018-08-05T00:20:25.391288: step 11960, loss 0.528967.
Test: 2018-08-05T00:20:26.640965: step 11960, loss 0.548168.
Train: 2018-08-05T00:20:26.890936: step 11961, loss 0.54611.
Train: 2018-08-05T00:20:27.140878: step 11962, loss 0.562482.
Train: 2018-08-05T00:20:27.390814: step 11963, loss 0.661726.
Train: 2018-08-05T00:20:27.640761: step 11964, loss 0.637639.
Train: 2018-08-05T00:20:27.890702: step 11965, loss 0.528986.
Train: 2018-08-05T00:20:28.140644: step 11966, loss 0.554456.
Train: 2018-08-05T00:20:28.390585: step 11967, loss 0.521899.
Train: 2018-08-05T00:20:28.640496: step 11968, loss 0.505694.
Train: 2018-08-05T00:20:28.890468: step 11969, loss 0.546802.
Train: 2018-08-05T00:20:29.140410: step 11970, loss 0.611411.
Test: 2018-08-05T00:20:30.405708: step 11970, loss 0.549892.
Train: 2018-08-05T00:20:30.640064: step 11971, loss 0.644426.
Train: 2018-08-05T00:20:30.905591: step 11972, loss 0.562827.
Train: 2018-08-05T00:20:31.139943: step 11973, loss 0.611578.
Train: 2018-08-05T00:20:31.389883: step 11974, loss 0.578893.
Train: 2018-08-05T00:20:31.639793: step 11975, loss 0.514477.
Train: 2018-08-05T00:20:31.889766: step 11976, loss 0.498533.
Train: 2018-08-05T00:20:32.139707: step 11977, loss 0.538817.
Train: 2018-08-05T00:20:32.389618: step 11978, loss 0.538582.
Train: 2018-08-05T00:20:32.639560: step 11979, loss 0.474391.
Train: 2018-08-05T00:20:32.889502: step 11980, loss 0.506287.
Test: 2018-08-05T00:20:34.123587: step 11980, loss 0.549695.
Train: 2018-08-05T00:20:34.373528: step 11981, loss 0.546416.
Train: 2018-08-05T00:20:34.639122: step 11982, loss 0.603373.
Train: 2018-08-05T00:20:34.873413: step 11983, loss 0.660873.
Train: 2018-08-05T00:20:35.123383: step 11984, loss 0.579004.
Train: 2018-08-05T00:20:35.373294: step 11985, loss 0.529872.
Train: 2018-08-05T00:20:35.623261: step 11986, loss 0.538063.
Train: 2018-08-05T00:20:35.873208: step 11987, loss 0.513283.
Train: 2018-08-05T00:20:36.123149: step 11988, loss 0.513062.
Train: 2018-08-05T00:20:36.373093: step 11989, loss 0.571685.
Train: 2018-08-05T00:20:36.623032: step 11990, loss 0.579259.
Test: 2018-08-05T00:20:37.903952: step 11990, loss 0.548273.
Train: 2018-08-05T00:20:38.138303: step 11991, loss 0.536866.
Train: 2018-08-05T00:20:38.388245: step 11992, loss 0.629046.
Train: 2018-08-05T00:20:38.638185: step 11993, loss 0.520473.
Train: 2018-08-05T00:20:38.888126: step 11994, loss 0.613093.
Train: 2018-08-05T00:20:39.138068: step 11995, loss 0.545598.
Train: 2018-08-05T00:20:39.388010: step 11996, loss 0.562486.
Train: 2018-08-05T00:20:39.637951: step 11997, loss 0.570819.
Train: 2018-08-05T00:20:39.887893: step 11998, loss 0.50436.
Train: 2018-08-05T00:20:40.137829: step 11999, loss 0.653855.
Train: 2018-08-05T00:20:40.387776: step 12000, loss 0.570849.
Test: 2018-08-05T00:20:41.653073: step 12000, loss 0.548329.
Train: 2018-08-05T00:20:42.496657: step 12001, loss 0.595565.
Train: 2018-08-05T00:20:42.746598: step 12002, loss 0.57076.
Train: 2018-08-05T00:20:42.996511: step 12003, loss 0.587189.
Train: 2018-08-05T00:20:43.246481: step 12004, loss 0.537973.
Train: 2018-08-05T00:20:43.496423: step 12005, loss 0.562591.
Train: 2018-08-05T00:20:43.761955: step 12006, loss 0.57891.
Train: 2018-08-05T00:20:44.011898: step 12007, loss 0.472821.
Train: 2018-08-05T00:20:44.261869: step 12008, loss 0.562583.
Train: 2018-08-05T00:20:44.511812: step 12009, loss 0.546319.
Train: 2018-08-05T00:20:44.777342: step 12010, loss 0.513424.
Test: 2018-08-05T00:20:46.011429: step 12010, loss 0.549062.
Train: 2018-08-05T00:20:46.245779: step 12011, loss 0.587086.
Train: 2018-08-05T00:20:46.495720: step 12012, loss 0.644653.
Train: 2018-08-05T00:20:46.745662: step 12013, loss 0.562402.
Train: 2018-08-05T00:20:46.995603: step 12014, loss 0.587326.
Train: 2018-08-05T00:20:47.245515: step 12015, loss 0.521491.
Train: 2018-08-05T00:20:47.495483: step 12016, loss 0.603459.
Train: 2018-08-05T00:20:47.745399: step 12017, loss 0.546326.
Train: 2018-08-05T00:20:47.995340: step 12018, loss 0.58754.
Train: 2018-08-05T00:20:48.245280: step 12019, loss 0.521972.
Train: 2018-08-05T00:20:48.495252: step 12020, loss 0.595216.
Test: 2018-08-05T00:20:49.729308: step 12020, loss 0.548503.
Train: 2018-08-05T00:20:49.963630: step 12021, loss 0.578992.
Train: 2018-08-05T00:20:50.213601: step 12022, loss 0.505864.
Train: 2018-08-05T00:20:50.463511: step 12023, loss 0.538356.
Train: 2018-08-05T00:20:50.713452: step 12024, loss 0.530182.
Train: 2018-08-05T00:20:50.963424: step 12025, loss 0.587032.
Train: 2018-08-05T00:20:51.213367: step 12026, loss 0.570903.
Train: 2018-08-05T00:20:51.463307: step 12027, loss 0.554415.
Train: 2018-08-05T00:20:51.713248: step 12028, loss 0.538179.
Train: 2018-08-05T00:20:51.963191: step 12029, loss 0.619809.
Train: 2018-08-05T00:20:52.213102: step 12030, loss 0.530022.
Test: 2018-08-05T00:20:53.478429: step 12030, loss 0.54787.
Train: 2018-08-05T00:20:53.712750: step 12031, loss 0.554421.
Train: 2018-08-05T00:20:53.962722: step 12032, loss 0.505441.
Train: 2018-08-05T00:20:54.212634: step 12033, loss 0.603552.
Train: 2018-08-05T00:20:54.462605: step 12034, loss 0.578894.
Train: 2018-08-05T00:20:54.712547: step 12035, loss 0.644539.
Train: 2018-08-05T00:20:54.962488: step 12036, loss 0.505345.
Train: 2018-08-05T00:20:55.228020: step 12037, loss 0.595352.
Train: 2018-08-05T00:20:55.462365: step 12038, loss 0.611609.
Train: 2018-08-05T00:20:55.712312: step 12039, loss 0.505592.
Train: 2018-08-05T00:20:55.962254: step 12040, loss 0.521906.
Test: 2018-08-05T00:20:57.211930: step 12040, loss 0.548839.
Train: 2018-08-05T00:20:57.446281: step 12041, loss 0.587069.
Train: 2018-08-05T00:20:57.696193: step 12042, loss 0.546335.
Train: 2018-08-05T00:20:57.946168: step 12043, loss 0.562636.
Train: 2018-08-05T00:20:58.196076: step 12044, loss 0.562628.
Train: 2018-08-05T00:20:58.446047: step 12045, loss 0.595217.
Train: 2018-08-05T00:20:58.695988: step 12046, loss 0.513772.
Train: 2018-08-05T00:20:58.945924: step 12047, loss 0.595218.
Train: 2018-08-05T00:20:59.195841: step 12048, loss 0.538175.
Train: 2018-08-05T00:20:59.461404: step 12049, loss 0.570823.
Train: 2018-08-05T00:20:59.711345: step 12050, loss 0.513693.
Test: 2018-08-05T00:21:00.961052: step 12050, loss 0.548208.
Train: 2018-08-05T00:21:01.210995: step 12051, loss 0.570794.
Train: 2018-08-05T00:21:01.460966: step 12052, loss 0.513555.
Train: 2018-08-05T00:21:01.710907: step 12053, loss 0.554373.
Train: 2018-08-05T00:21:01.960818: step 12054, loss 0.57898.
Train: 2018-08-05T00:21:02.210793: step 12055, loss 0.620062.
Train: 2018-08-05T00:21:02.460703: step 12056, loss 0.537869.
Train: 2018-08-05T00:21:02.710673: step 12057, loss 0.579007.
Train: 2018-08-05T00:21:02.960584: step 12058, loss 0.521413.
Train: 2018-08-05T00:21:03.210527: step 12059, loss 0.653083.
Train: 2018-08-05T00:21:03.460497: step 12060, loss 0.611857.
Test: 2018-08-05T00:21:04.710175: step 12060, loss 0.548506.
Train: 2018-08-05T00:21:04.944496: step 12061, loss 0.562542.
Train: 2018-08-05T00:21:05.194466: step 12062, loss 0.562607.
Train: 2018-08-05T00:21:05.444408: step 12063, loss 0.578939.
Train: 2018-08-05T00:21:05.694349: step 12064, loss 0.595198.
Train: 2018-08-05T00:21:05.944260: step 12065, loss 0.554541.
Train: 2018-08-05T00:21:06.194203: step 12066, loss 0.489753.
Train: 2018-08-05T00:21:06.444170: step 12067, loss 0.530281.
Train: 2018-08-05T00:21:06.694086: step 12068, loss 0.578898.
Train: 2018-08-05T00:21:06.944058: step 12069, loss 0.538368.
Train: 2018-08-05T00:21:07.193999: step 12070, loss 0.530234.
Test: 2018-08-05T00:21:08.443675: step 12070, loss 0.548905.
Train: 2018-08-05T00:21:08.693618: step 12071, loss 0.578918.
Train: 2018-08-05T00:21:08.927969: step 12072, loss 0.578916.
Train: 2018-08-05T00:21:09.177879: step 12073, loss 0.530105.
Train: 2018-08-05T00:21:09.427852: step 12074, loss 0.57893.
Train: 2018-08-05T00:21:09.693382: step 12075, loss 0.627825.
Train: 2018-08-05T00:21:09.927703: step 12076, loss 0.546348.
Train: 2018-08-05T00:21:10.177676: step 12077, loss 0.595201.
Train: 2018-08-05T00:21:10.427618: step 12078, loss 0.660218.
Train: 2018-08-05T00:21:10.677559: step 12079, loss 0.562689.
Train: 2018-08-05T00:21:10.880636: step 12080, loss 0.510989.
Test: 2018-08-05T00:21:12.130312: step 12080, loss 0.54907.
Train: 2018-08-05T00:21:12.364663: step 12081, loss 0.48201.
Train: 2018-08-05T00:21:12.614599: step 12082, loss 0.61928.
Train: 2018-08-05T00:21:12.864516: step 12083, loss 0.546588.
Train: 2018-08-05T00:21:13.114457: step 12084, loss 0.514302.
Train: 2018-08-05T00:21:13.364428: step 12085, loss 0.595052.
Train: 2018-08-05T00:21:13.614370: step 12086, loss 0.530387.
Train: 2018-08-05T00:21:13.864312: step 12087, loss 0.611257.
Train: 2018-08-05T00:21:14.114253: step 12088, loss 0.57889.
Train: 2018-08-05T00:21:14.364165: step 12089, loss 0.611233.
Train: 2018-08-05T00:21:14.614135: step 12090, loss 0.530436.
Test: 2018-08-05T00:21:15.863813: step 12090, loss 0.550008.
Train: 2018-08-05T00:21:16.160651: step 12091, loss 0.578881.
Train: 2018-08-05T00:21:16.410589: step 12092, loss 0.635328.
Train: 2018-08-05T00:21:16.660533: step 12093, loss 0.546692.
Train: 2018-08-05T00:21:16.910442: step 12094, loss 0.578867.
Train: 2018-08-05T00:21:17.160414: step 12095, loss 0.506685.
Train: 2018-08-05T00:21:17.410333: step 12096, loss 0.546796.
Train: 2018-08-05T00:21:17.660268: step 12097, loss 0.554783.
Train: 2018-08-05T00:21:17.910246: step 12098, loss 0.602983.
Train: 2018-08-05T00:21:18.160151: step 12099, loss 0.570834.
Train: 2018-08-05T00:21:18.410122: step 12100, loss 0.538788.
Test: 2018-08-05T00:21:19.644178: step 12100, loss 0.548125.
Train: 2018-08-05T00:21:20.503376: step 12101, loss 0.482572.
Train: 2018-08-05T00:21:20.753293: step 12102, loss 0.546677.
Train: 2018-08-05T00:21:21.003235: step 12103, loss 0.50623.
Train: 2018-08-05T00:21:21.253175: step 12104, loss 0.627535.
Train: 2018-08-05T00:21:21.503147: step 12105, loss 0.586946.
Train: 2018-08-05T00:21:21.753089: step 12106, loss 0.603507.
Train: 2018-08-05T00:21:22.003031: step 12107, loss 0.489666.
Train: 2018-08-05T00:21:22.252972: step 12108, loss 0.546302.
Train: 2018-08-05T00:21:22.502914: step 12109, loss 0.619489.
Train: 2018-08-05T00:21:22.752855: step 12110, loss 0.587742.
Test: 2018-08-05T00:21:24.002531: step 12110, loss 0.548736.
Train: 2018-08-05T00:21:24.236884: step 12111, loss 0.513102.
Train: 2018-08-05T00:21:24.486794: step 12112, loss 0.578875.
Train: 2018-08-05T00:21:24.736769: step 12113, loss 0.562035.
Train: 2018-08-05T00:21:24.986707: step 12114, loss 0.562387.
Train: 2018-08-05T00:21:25.236618: step 12115, loss 0.570243.
Train: 2018-08-05T00:21:25.486590: step 12116, loss 0.637298.
Train: 2018-08-05T00:21:25.736532: step 12117, loss 0.469996.
Train: 2018-08-05T00:21:26.002093: step 12118, loss 0.528926.
Train: 2018-08-05T00:21:26.252036: step 12119, loss 0.588794.
Train: 2018-08-05T00:21:26.501978: step 12120, loss 0.571069.
Test: 2018-08-05T00:21:27.751653: step 12120, loss 0.548497.
Train: 2018-08-05T00:21:28.001595: step 12121, loss 0.545798.
Train: 2018-08-05T00:21:28.235946: step 12122, loss 0.563096.
Train: 2018-08-05T00:21:28.485887: step 12123, loss 0.512574.
Train: 2018-08-05T00:21:28.735829: step 12124, loss 0.613572.
Train: 2018-08-05T00:21:28.985770: step 12125, loss 0.61413.
Train: 2018-08-05T00:21:29.235712: step 12126, loss 0.620365.
Train: 2018-08-05T00:21:29.485624: step 12127, loss 0.513584.
Train: 2018-08-05T00:21:29.735565: step 12128, loss 0.529629.
Train: 2018-08-05T00:21:29.985536: step 12129, loss 0.578652.
Train: 2018-08-05T00:21:30.235478: step 12130, loss 0.570771.
Test: 2018-08-05T00:21:31.500776: step 12130, loss 0.548609.
Train: 2018-08-05T00:21:31.735127: step 12131, loss 0.603391.
Train: 2018-08-05T00:21:31.985068: step 12132, loss 0.562653.
Train: 2018-08-05T00:21:32.234980: step 12133, loss 0.538561.
Train: 2018-08-05T00:21:32.484945: step 12134, loss 0.554693.
Train: 2018-08-05T00:21:32.734863: step 12135, loss 0.578861.
Train: 2018-08-05T00:21:32.984834: step 12136, loss 0.562811.
Train: 2018-08-05T00:21:33.234745: step 12137, loss 0.578871.
Train: 2018-08-05T00:21:33.484718: step 12138, loss 0.57087.
Train: 2018-08-05T00:21:33.734628: step 12139, loss 0.546849.
Train: 2018-08-05T00:21:34.000190: step 12140, loss 0.610864.
Test: 2018-08-05T00:21:35.249898: step 12140, loss 0.549537.
Train: 2018-08-05T00:21:35.484220: step 12141, loss 0.49097.
Train: 2018-08-05T00:21:35.734161: step 12142, loss 0.610843.
Train: 2018-08-05T00:21:35.999753: step 12143, loss 0.498946.
Train: 2018-08-05T00:21:36.249665: step 12144, loss 0.458852.
Train: 2018-08-05T00:21:36.499607: step 12145, loss 0.530701.
Train: 2018-08-05T00:21:36.749578: step 12146, loss 0.594998.
Train: 2018-08-05T00:21:36.999489: step 12147, loss 0.586973.
Train: 2018-08-05T00:21:37.265051: step 12148, loss 0.546498.
Train: 2018-08-05T00:21:37.514992: step 12149, loss 0.6195.
Train: 2018-08-05T00:21:37.764935: step 12150, loss 0.58703.
Test: 2018-08-05T00:21:39.014641: step 12150, loss 0.549651.
Train: 2018-08-05T00:21:39.248992: step 12151, loss 0.611398.
Train: 2018-08-05T00:21:39.498934: step 12152, loss 0.578903.
Train: 2018-08-05T00:21:39.748844: step 12153, loss 0.58701.
Train: 2018-08-05T00:21:39.998816: step 12154, loss 0.522219.
Train: 2018-08-05T00:21:40.248758: step 12155, loss 0.578889.
Train: 2018-08-05T00:21:40.498699: step 12156, loss 0.611247.
Train: 2018-08-05T00:21:40.748642: step 12157, loss 0.490071.
Train: 2018-08-05T00:21:40.998584: step 12158, loss 0.611199.
Train: 2018-08-05T00:21:41.248493: step 12159, loss 0.50627.
Train: 2018-08-05T00:21:41.498465: step 12160, loss 0.651525.
Test: 2018-08-05T00:21:42.763763: step 12160, loss 0.548929.
Train: 2018-08-05T00:21:43.013705: step 12161, loss 0.530526.
Train: 2018-08-05T00:21:43.248026: step 12162, loss 0.578875.
Train: 2018-08-05T00:21:43.497968: step 12163, loss 0.47424.
Train: 2018-08-05T00:21:43.747938: step 12164, loss 0.603058.
Train: 2018-08-05T00:21:43.997850: step 12165, loss 0.530511.
Train: 2018-08-05T00:21:44.247792: step 12166, loss 0.498183.
Train: 2018-08-05T00:21:44.497732: step 12167, loss 0.578891.
Train: 2018-08-05T00:21:44.747705: step 12168, loss 0.522165.
Train: 2018-08-05T00:21:44.997646: step 12169, loss 0.595162.
Train: 2018-08-05T00:21:45.247588: step 12170, loss 0.595197.
Test: 2018-08-05T00:21:46.497264: step 12170, loss 0.549779.
Train: 2018-08-05T00:21:46.747205: step 12171, loss 0.562633.
Train: 2018-08-05T00:21:46.997147: step 12172, loss 0.448512.
Train: 2018-08-05T00:21:47.247120: step 12173, loss 0.554415.
Train: 2018-08-05T00:21:47.497060: step 12174, loss 0.529764.
Train: 2018-08-05T00:21:47.747001: step 12175, loss 0.554303.
Train: 2018-08-05T00:21:47.996944: step 12176, loss 0.603763.
Train: 2018-08-05T00:21:48.246885: step 12177, loss 0.579023.
Train: 2018-08-05T00:21:48.496822: step 12178, loss 0.504538.
Train: 2018-08-05T00:21:48.746769: step 12179, loss 0.537575.
Train: 2018-08-05T00:21:48.996709: step 12180, loss 0.579077.
Test: 2018-08-05T00:21:50.246386: step 12180, loss 0.549478.
Train: 2018-08-05T00:21:50.480738: step 12181, loss 0.570764.
Train: 2018-08-05T00:21:50.730648: step 12182, loss 0.579109.
Train: 2018-08-05T00:21:50.980591: step 12183, loss 0.529027.
Train: 2018-08-05T00:21:51.230561: step 12184, loss 0.620923.
Train: 2018-08-05T00:21:51.480473: step 12185, loss 0.604198.
Train: 2018-08-05T00:21:51.730414: step 12186, loss 0.56242.
Train: 2018-08-05T00:21:51.980386: step 12187, loss 0.55409.
Train: 2018-08-05T00:21:52.230298: step 12188, loss 0.595754.
Train: 2018-08-05T00:21:52.480269: step 12189, loss 0.512547.
Train: 2018-08-05T00:21:52.730210: step 12190, loss 0.537513.
Test: 2018-08-05T00:21:53.975274: step 12190, loss 0.547995.
Train: 2018-08-05T00:21:54.225216: step 12191, loss 0.662183.
Train: 2018-08-05T00:21:54.475187: step 12192, loss 0.562465.
Train: 2018-08-05T00:21:54.725099: step 12193, loss 0.529378.
Train: 2018-08-05T00:21:54.975065: step 12194, loss 0.645145.
Train: 2018-08-05T00:21:55.224981: step 12195, loss 0.554274.
Train: 2018-08-05T00:21:55.474924: step 12196, loss 0.504992.
Train: 2018-08-05T00:21:55.724864: step 12197, loss 0.5297.
Train: 2018-08-05T00:21:55.974835: step 12198, loss 0.480457.
Train: 2018-08-05T00:21:56.224748: step 12199, loss 0.529653.
Train: 2018-08-05T00:21:56.474689: step 12200, loss 0.513099.
Test: 2018-08-05T00:21:57.724396: step 12200, loss 0.547943.
Train: 2018-08-05T00:21:58.552326: step 12201, loss 0.595533.
Train: 2018-08-05T00:21:58.802268: step 12202, loss 0.537662.
Train: 2018-08-05T00:21:59.052237: step 12203, loss 0.57905.
Train: 2018-08-05T00:21:59.302150: step 12204, loss 0.529254.
Train: 2018-08-05T00:21:59.552125: step 12205, loss 0.554135.
Train: 2018-08-05T00:21:59.802036: step 12206, loss 0.570767.
Train: 2018-08-05T00:22:00.067597: step 12207, loss 0.66251.
Train: 2018-08-05T00:22:00.317569: step 12208, loss 0.495775.
Train: 2018-08-05T00:22:00.567481: step 12209, loss 0.587437.
Train: 2018-08-05T00:22:00.817452: step 12210, loss 0.570763.
Test: 2018-08-05T00:22:02.051507: step 12210, loss 0.548911.
Train: 2018-08-05T00:22:02.301473: step 12211, loss 0.587419.
Train: 2018-08-05T00:22:02.551391: step 12212, loss 0.512531.
Train: 2018-08-05T00:22:02.801365: step 12213, loss 0.57908.
Train: 2018-08-05T00:22:03.066895: step 12214, loss 0.595703.
Train: 2018-08-05T00:22:03.301247: step 12215, loss 0.537543.
Train: 2018-08-05T00:22:03.551157: step 12216, loss 0.529266.
Train: 2018-08-05T00:22:03.801129: step 12217, loss 0.529266.
Train: 2018-08-05T00:22:04.051068: step 12218, loss 0.612277.
Train: 2018-08-05T00:22:04.301012: step 12219, loss 0.612253.
Train: 2018-08-05T00:22:04.550954: step 12220, loss 0.570757.
Test: 2018-08-05T00:22:05.800629: step 12220, loss 0.5481.
Train: 2018-08-05T00:22:06.034983: step 12221, loss 0.554214.
Train: 2018-08-05T00:22:06.284923: step 12222, loss 0.587275.
Train: 2018-08-05T00:22:06.534832: step 12223, loss 0.579001.
Train: 2018-08-05T00:22:06.800396: step 12224, loss 0.554304.
Train: 2018-08-05T00:22:07.034716: step 12225, loss 0.587185.
Train: 2018-08-05T00:22:07.284657: step 12226, loss 0.587153.
Train: 2018-08-05T00:22:07.534600: step 12227, loss 0.595293.
Train: 2018-08-05T00:22:07.784541: step 12228, loss 0.530019.
Train: 2018-08-05T00:22:08.034481: step 12229, loss 0.546375.
Train: 2018-08-05T00:22:08.284453: step 12230, loss 0.53829.
Test: 2018-08-05T00:22:09.549751: step 12230, loss 0.549299.
Train: 2018-08-05T00:22:09.737239: step 12231, loss 0.56267.
Train: 2018-08-05T00:22:09.987179: step 12232, loss 0.554564.
Train: 2018-08-05T00:22:10.237121: step 12233, loss 0.587012.
Train: 2018-08-05T00:22:10.487031: step 12234, loss 0.46545.
Train: 2018-08-05T00:22:10.736973: step 12235, loss 0.587019.
Train: 2018-08-05T00:22:10.986945: step 12236, loss 0.60327.
Train: 2018-08-05T00:22:11.236882: step 12237, loss 0.578907.
Train: 2018-08-05T00:22:11.486828: step 12238, loss 0.554559.
Train: 2018-08-05T00:22:11.736769: step 12239, loss 0.497771.
Train: 2018-08-05T00:22:11.986682: step 12240, loss 0.595155.
Test: 2018-08-05T00:22:13.252009: step 12240, loss 0.5489.
Train: 2018-08-05T00:22:13.486361: step 12241, loss 0.578911.
Train: 2018-08-05T00:22:13.736295: step 12242, loss 0.562658.
Train: 2018-08-05T00:22:13.986244: step 12243, loss 0.530151.
Train: 2018-08-05T00:22:14.236153: step 12244, loss 0.53825.
Train: 2018-08-05T00:22:14.486095: step 12245, loss 0.554493.
Train: 2018-08-05T00:22:14.736067: step 12246, loss 0.497396.
Train: 2018-08-05T00:22:14.986009: step 12247, loss 0.644332.
Train: 2018-08-05T00:22:15.235952: step 12248, loss 0.554411.
Train: 2018-08-05T00:22:15.485862: step 12249, loss 0.538032.
Train: 2018-08-05T00:22:15.735834: step 12250, loss 0.521611.
Test: 2018-08-05T00:22:16.985510: step 12250, loss 0.547903.
Train: 2018-08-05T00:22:17.235452: step 12251, loss 0.529726.
Train: 2018-08-05T00:22:17.485394: step 12252, loss 0.562532.
Train: 2018-08-05T00:22:17.735365: step 12253, loss 0.521302.
Train: 2018-08-05T00:22:17.985308: step 12254, loss 0.488116.
Train: 2018-08-05T00:22:18.235247: step 12255, loss 0.637124.
Train: 2018-08-05T00:22:18.485189: step 12256, loss 0.595693.
Train: 2018-08-05T00:22:18.735132: step 12257, loss 0.512535.
Train: 2018-08-05T00:22:18.985043: step 12258, loss 0.587429.
Train: 2018-08-05T00:22:19.235014: step 12259, loss 0.595786.
Train: 2018-08-05T00:22:19.484925: step 12260, loss 0.520724.
Test: 2018-08-05T00:22:20.734632: step 12260, loss 0.547136.
Train: 2018-08-05T00:22:20.984599: step 12261, loss 0.604155.
Train: 2018-08-05T00:22:21.234546: step 12262, loss 0.495664.
Train: 2018-08-05T00:22:21.484488: step 12263, loss 0.512297.
Train: 2018-08-05T00:22:21.734399: step 12264, loss 0.570775.
Train: 2018-08-05T00:22:21.984371: step 12265, loss 0.554019.
Train: 2018-08-05T00:22:22.234312: step 12266, loss 0.554001.
Train: 2018-08-05T00:22:22.484223: step 12267, loss 0.553987.
Train: 2018-08-05T00:22:22.734194: step 12268, loss 0.537152.
Train: 2018-08-05T00:22:22.984136: step 12269, loss 0.562373.
Train: 2018-08-05T00:22:23.234077: step 12270, loss 0.55394.
Test: 2018-08-05T00:22:24.483754: step 12270, loss 0.548858.
Train: 2018-08-05T00:22:24.718105: step 12271, loss 0.545487.
Train: 2018-08-05T00:22:24.968046: step 12272, loss 0.503231.
Train: 2018-08-05T00:22:25.217983: step 12273, loss 0.613141.
Train: 2018-08-05T00:22:25.467930: step 12274, loss 0.460734.
Train: 2018-08-05T00:22:25.733462: step 12275, loss 0.545375.
Train: 2018-08-05T00:22:25.967783: step 12276, loss 0.587851.
Train: 2018-08-05T00:22:26.217754: step 12277, loss 0.545307.
Train: 2018-08-05T00:22:26.467696: step 12278, loss 0.579405.
Train: 2018-08-05T00:22:26.717606: step 12279, loss 0.613593.
Train: 2018-08-05T00:22:26.967548: step 12280, loss 0.562347.
Test: 2018-08-05T00:22:28.217255: step 12280, loss 0.547121.
Train: 2018-08-05T00:22:28.451575: step 12281, loss 0.519717.
Train: 2018-08-05T00:22:28.701547: step 12282, loss 0.596444.
Train: 2018-08-05T00:22:28.951459: step 12283, loss 0.545314.
Train: 2018-08-05T00:22:29.201430: step 12284, loss 0.579363.
Train: 2018-08-05T00:22:29.451372: step 12285, loss 0.570846.
Train: 2018-08-05T00:22:29.701313: step 12286, loss 0.502932.
Train: 2018-08-05T00:22:29.951254: step 12287, loss 0.528406.
Train: 2018-08-05T00:22:30.201196: step 12288, loss 0.604788.
Train: 2018-08-05T00:22:30.451139: step 12289, loss 0.638685.
Train: 2018-08-05T00:22:30.701072: step 12290, loss 0.536974.
Test: 2018-08-05T00:22:31.950756: step 12290, loss 0.548842.
Train: 2018-08-05T00:22:32.200728: step 12291, loss 0.596144.
Train: 2018-08-05T00:22:32.450670: step 12292, loss 0.54552.
Train: 2018-08-05T00:22:32.700581: step 12293, loss 0.545573.
Train: 2018-08-05T00:22:32.950552: step 12294, loss 0.528817.
Train: 2018-08-05T00:22:33.200496: step 12295, loss 0.537236.
Train: 2018-08-05T00:22:33.450404: step 12296, loss 0.59592.
Train: 2018-08-05T00:22:33.700377: step 12297, loss 0.595888.
Train: 2018-08-05T00:22:33.950318: step 12298, loss 0.554055.
Train: 2018-08-05T00:22:34.200260: step 12299, loss 0.537405.
Train: 2018-08-05T00:22:34.450201: step 12300, loss 0.570764.
Test: 2018-08-05T00:22:35.699878: step 12300, loss 0.548928.
Train: 2018-08-05T00:22:36.559082: step 12301, loss 0.537481.
Train: 2018-08-05T00:22:36.809023: step 12302, loss 0.545819.
Train: 2018-08-05T00:22:37.058936: step 12303, loss 0.595693.
Train: 2018-08-05T00:22:37.308877: step 12304, loss 0.554157.
Train: 2018-08-05T00:22:37.558847: step 12305, loss 0.520997.
Train: 2018-08-05T00:22:37.808789: step 12306, loss 0.562463.
Train: 2018-08-05T00:22:38.058731: step 12307, loss 0.545881.
Train: 2018-08-05T00:22:38.308672: step 12308, loss 0.512705.
Train: 2018-08-05T00:22:38.558614: step 12309, loss 0.471133.
Train: 2018-08-05T00:22:38.808526: step 12310, loss 0.512486.
Test: 2018-08-05T00:22:40.058232: step 12310, loss 0.548465.
Train: 2018-08-05T00:22:40.292585: step 12311, loss 0.537355.
Train: 2018-08-05T00:22:40.542497: step 12312, loss 0.562397.
Train: 2018-08-05T00:22:40.792467: step 12313, loss 0.503526.
Train: 2018-08-05T00:22:41.042408: step 12314, loss 0.562367.
Train: 2018-08-05T00:22:41.292350: step 12315, loss 0.562356.
Train: 2018-08-05T00:22:41.542292: step 12316, loss 0.545371.
Train: 2018-08-05T00:22:41.792233: step 12317, loss 0.579364.
Train: 2018-08-05T00:22:42.042143: step 12318, loss 0.494134.
Train: 2018-08-05T00:22:42.292084: step 12319, loss 0.562337.
Train: 2018-08-05T00:22:42.542025: step 12320, loss 0.553767.
Test: 2018-08-05T00:22:43.791733: step 12320, loss 0.549776.
Train: 2018-08-05T00:22:44.026054: step 12321, loss 0.631013.
Train: 2018-08-05T00:22:44.276027: step 12322, loss 0.545162.
Train: 2018-08-05T00:22:44.525936: step 12323, loss 0.562335.
Train: 2018-08-05T00:22:44.775879: step 12324, loss 0.639631.
Train: 2018-08-05T00:22:45.041441: step 12325, loss 0.545188.
Train: 2018-08-05T00:22:45.275789: step 12326, loss 0.528092.
Train: 2018-08-05T00:22:45.525701: step 12327, loss 0.605103.
Train: 2018-08-05T00:22:45.775673: step 12328, loss 0.596489.
Train: 2018-08-05T00:22:46.025617: step 12329, loss 0.647501.
Train: 2018-08-05T00:22:46.275556: step 12330, loss 0.621714.
Test: 2018-08-05T00:22:47.525233: step 12330, loss 0.548471.
Train: 2018-08-05T00:22:47.759584: step 12331, loss 0.528619.
Train: 2018-08-05T00:22:48.009496: step 12332, loss 0.545581.
Train: 2018-08-05T00:22:48.259467: step 12333, loss 0.646131.
Train: 2018-08-05T00:22:48.509409: step 12334, loss 0.679066.
Train: 2018-08-05T00:22:48.759320: step 12335, loss 0.554211.
Train: 2018-08-05T00:22:49.009292: step 12336, loss 0.554316.
Train: 2018-08-05T00:22:49.259234: step 12337, loss 0.587119.
Train: 2018-08-05T00:22:49.524766: step 12338, loss 0.54639.
Train: 2018-08-05T00:22:49.790328: step 12339, loss 0.611266.
Train: 2018-08-05T00:22:50.024649: step 12340, loss 0.530571.
Test: 2018-08-05T00:22:51.274355: step 12340, loss 0.550544.
Train: 2018-08-05T00:22:51.508706: step 12341, loss 0.538772.
Train: 2018-08-05T00:22:51.758617: step 12342, loss 0.53089.
Train: 2018-08-05T00:22:52.008589: step 12343, loss 0.507036.
Train: 2018-08-05T00:22:52.274153: step 12344, loss 0.538971.
Train: 2018-08-05T00:22:52.524095: step 12345, loss 0.594822.
Train: 2018-08-05T00:22:52.774037: step 12346, loss 0.570882.
Train: 2018-08-05T00:22:53.023947: step 12347, loss 0.531007.
Train: 2018-08-05T00:22:53.273917: step 12348, loss 0.58684.
Train: 2018-08-05T00:22:53.523861: step 12349, loss 0.538962.
Train: 2018-08-05T00:22:53.773803: step 12350, loss 0.554906.
Test: 2018-08-05T00:22:55.023479: step 12350, loss 0.548795.
Train: 2018-08-05T00:22:55.257800: step 12351, loss 0.578862.
Train: 2018-08-05T00:22:55.507769: step 12352, loss 0.602851.
Train: 2018-08-05T00:22:55.757707: step 12353, loss 0.610837.
Train: 2018-08-05T00:22:56.007653: step 12354, loss 0.522976.
Train: 2018-08-05T00:22:56.257595: step 12355, loss 0.530967.
Train: 2018-08-05T00:22:56.507505: step 12356, loss 0.578861.
Train: 2018-08-05T00:22:56.757448: step 12357, loss 0.594843.
Train: 2018-08-05T00:22:57.007418: step 12358, loss 0.594838.
Train: 2018-08-05T00:22:57.257360: step 12359, loss 0.522992.
Train: 2018-08-05T00:22:57.507271: step 12360, loss 0.546929.
Test: 2018-08-05T00:22:58.741357: step 12360, loss 0.549177.
Train: 2018-08-05T00:22:58.991329: step 12361, loss 0.658741.
Train: 2018-08-05T00:22:59.241240: step 12362, loss 0.586836.
Train: 2018-08-05T00:22:59.491211: step 12363, loss 0.586821.
Train: 2018-08-05T00:22:59.741122: step 12364, loss 0.562966.
Train: 2018-08-05T00:22:59.991095: step 12365, loss 0.578859.
Train: 2018-08-05T00:23:00.256658: step 12366, loss 0.539263.
Train: 2018-08-05T00:23:00.506593: step 12367, loss 0.586772.
Train: 2018-08-05T00:23:00.756541: step 12368, loss 0.523542.
Train: 2018-08-05T00:23:01.006451: step 12369, loss 0.634179.
Train: 2018-08-05T00:23:01.256424: step 12370, loss 0.563082.
Test: 2018-08-05T00:23:02.506100: step 12370, loss 0.549395.
Train: 2018-08-05T00:23:02.771693: step 12371, loss 0.507932.
Train: 2018-08-05T00:23:03.021635: step 12372, loss 0.618286.
Train: 2018-08-05T00:23:03.271576: step 12373, loss 0.610378.
Train: 2018-08-05T00:23:03.521487: step 12374, loss 0.515945.
Train: 2018-08-05T00:23:03.771430: step 12375, loss 0.578869.
Train: 2018-08-05T00:23:04.021403: step 12376, loss 0.555287.
Train: 2018-08-05T00:23:04.271312: step 12377, loss 0.57887.
Train: 2018-08-05T00:23:04.521283: step 12378, loss 0.56315.
Train: 2018-08-05T00:23:04.771196: step 12379, loss 0.539568.
Train: 2018-08-05T00:23:05.021166: step 12380, loss 0.665397.
Test: 2018-08-05T00:23:06.270844: step 12380, loss 0.549503.
Train: 2018-08-05T00:23:06.505195: step 12381, loss 0.578871.
Train: 2018-08-05T00:23:06.692650: step 12382, loss 0.579921.
Train: 2018-08-05T00:23:06.942586: step 12383, loss 0.563219.
Train: 2018-08-05T00:23:07.192503: step 12384, loss 0.539781.
Train: 2018-08-05T00:23:07.442475: step 12385, loss 0.547614.
Train: 2018-08-05T00:23:07.692386: step 12386, loss 0.586703.
Train: 2018-08-05T00:23:07.942329: step 12387, loss 0.578884.
Train: 2018-08-05T00:23:08.192294: step 12388, loss 0.524169.
Train: 2018-08-05T00:23:08.442211: step 12389, loss 0.508471.
Train: 2018-08-05T00:23:08.692177: step 12390, loss 0.586717.
Test: 2018-08-05T00:23:09.926238: step 12390, loss 0.550226.
Train: 2018-08-05T00:23:10.176180: step 12391, loss 0.531745.
Train: 2018-08-05T00:23:10.426121: step 12392, loss 0.578867.
Train: 2018-08-05T00:23:10.676093: step 12393, loss 0.649864.
Train: 2018-08-05T00:23:10.926034: step 12394, loss 0.539421.
Train: 2018-08-05T00:23:11.175946: step 12395, loss 0.586756.
Train: 2018-08-05T00:23:11.425887: step 12396, loss 0.563072.
Train: 2018-08-05T00:23:11.675855: step 12397, loss 0.499881.
Train: 2018-08-05T00:23:11.925799: step 12398, loss 0.602599.
Train: 2018-08-05T00:23:12.175741: step 12399, loss 0.531335.
Train: 2018-08-05T00:23:12.425652: step 12400, loss 0.56299.
Test: 2018-08-05T00:23:13.675360: step 12400, loss 0.549873.
Train: 2018-08-05T00:23:14.550155: step 12401, loss 0.586806.
Train: 2018-08-05T00:23:14.800127: step 12402, loss 0.547034.
Train: 2018-08-05T00:23:15.050068: step 12403, loss 0.554956.
Train: 2018-08-05T00:23:15.315601: step 12404, loss 0.578861.
Train: 2018-08-05T00:23:15.565573: step 12405, loss 0.546901.
Train: 2018-08-05T00:23:15.815485: step 12406, loss 0.578864.
Train: 2018-08-05T00:23:16.065455: step 12407, loss 0.546821.
Train: 2018-08-05T00:23:16.315398: step 12408, loss 0.530733.
Train: 2018-08-05T00:23:16.565339: step 12409, loss 0.570833.
Train: 2018-08-05T00:23:16.815281: step 12410, loss 0.619144.
Test: 2018-08-05T00:23:18.064957: step 12410, loss 0.54931.
Train: 2018-08-05T00:23:18.346142: step 12411, loss 0.530539.
Train: 2018-08-05T00:23:18.596113: step 12412, loss 0.554685.
Train: 2018-08-05T00:23:18.846024: step 12413, loss 0.562735.
Train: 2018-08-05T00:23:19.095995: step 12414, loss 0.51421.
Train: 2018-08-05T00:23:19.345937: step 12415, loss 0.595102.
Train: 2018-08-05T00:23:19.595879: step 12416, loss 0.578903.
Train: 2018-08-05T00:23:19.845820: step 12417, loss 0.538313.
Train: 2018-08-05T00:23:20.095761: step 12418, loss 0.530138.
Train: 2018-08-05T00:23:20.345704: step 12419, loss 0.587066.
Train: 2018-08-05T00:23:20.595614: step 12420, loss 0.578928.
Test: 2018-08-05T00:23:21.845322: step 12420, loss 0.548987.
Train: 2018-08-05T00:23:22.095294: step 12421, loss 0.562613.
Train: 2018-08-05T00:23:22.345235: step 12422, loss 0.595263.
Train: 2018-08-05T00:23:22.595177: step 12423, loss 0.611585.
Train: 2018-08-05T00:23:22.845118: step 12424, loss 0.505551.
Train: 2018-08-05T00:23:23.095059: step 12425, loss 0.578928.
Train: 2018-08-05T00:23:23.345001: step 12426, loss 0.587079.
Train: 2018-08-05T00:23:23.594942: step 12427, loss 0.578923.
Train: 2018-08-05T00:23:23.844884: step 12428, loss 0.619609.
Train: 2018-08-05T00:23:24.094795: step 12429, loss 0.570787.
Train: 2018-08-05T00:23:24.344767: step 12430, loss 0.546482.
Test: 2018-08-05T00:23:25.594444: step 12430, loss 0.549194.
Train: 2018-08-05T00:23:25.844385: step 12431, loss 0.546525.
Train: 2018-08-05T00:23:26.078736: step 12432, loss 0.530383.
Train: 2018-08-05T00:23:26.328677: step 12433, loss 0.522304.
Train: 2018-08-05T00:23:26.578619: step 12434, loss 0.53035.
Train: 2018-08-05T00:23:26.828560: step 12435, loss 0.578898.
Train: 2018-08-05T00:23:27.078504: step 12436, loss 0.546459.
Train: 2018-08-05T00:23:27.328412: step 12437, loss 0.473332.
Train: 2018-08-05T00:23:27.578386: step 12438, loss 0.546337.
Train: 2018-08-05T00:23:27.828296: step 12439, loss 0.578941.
Train: 2018-08-05T00:23:28.078267: step 12440, loss 0.554378.
Test: 2018-08-05T00:23:29.327944: step 12440, loss 0.549589.
Train: 2018-08-05T00:23:29.577916: step 12441, loss 0.521487.
Train: 2018-08-05T00:23:29.827858: step 12442, loss 0.496628.
Train: 2018-08-05T00:23:30.077799: step 12443, loss 0.554217.
Train: 2018-08-05T00:23:30.327711: step 12444, loss 0.529258.
Train: 2018-08-05T00:23:30.577653: step 12445, loss 0.545765.
Train: 2018-08-05T00:23:30.827624: step 12446, loss 0.503859.
Train: 2018-08-05T00:23:31.077565: step 12447, loss 0.604394.
Train: 2018-08-05T00:23:31.343121: step 12448, loss 0.562372.
Train: 2018-08-05T00:23:31.577448: step 12449, loss 0.562362.
Train: 2018-08-05T00:23:31.827359: step 12450, loss 0.562355.
Test: 2018-08-05T00:23:33.092687: step 12450, loss 0.547797.
Train: 2018-08-05T00:23:33.327038: step 12451, loss 0.579313.
Train: 2018-08-05T00:23:33.576950: step 12452, loss 0.587818.
Train: 2018-08-05T00:23:33.826891: step 12453, loss 0.596314.
Train: 2018-08-05T00:23:34.076857: step 12454, loss 0.570834.
Train: 2018-08-05T00:23:34.326804: step 12455, loss 0.528439.
Train: 2018-08-05T00:23:34.576746: step 12456, loss 0.587778.
Train: 2018-08-05T00:23:34.826682: step 12457, loss 0.663955.
Train: 2018-08-05T00:23:35.076629: step 12458, loss 0.587687.
Train: 2018-08-05T00:23:35.326540: step 12459, loss 0.55397.
Train: 2018-08-05T00:23:35.576511: step 12460, loss 0.478551.
Test: 2018-08-05T00:23:36.841810: step 12460, loss 0.548224.
Train: 2018-08-05T00:23:37.076161: step 12461, loss 0.545654.
Train: 2018-08-05T00:23:37.326101: step 12462, loss 0.562407.
Train: 2018-08-05T00:23:37.576013: step 12463, loss 0.528977.
Train: 2018-08-05T00:23:37.825985: step 12464, loss 0.579127.
Train: 2018-08-05T00:23:38.075896: step 12465, loss 0.595821.
Train: 2018-08-05T00:23:38.325867: step 12466, loss 0.562427.
Train: 2018-08-05T00:23:38.575809: step 12467, loss 0.520799.
Train: 2018-08-05T00:23:38.825750: step 12468, loss 0.554116.
Train: 2018-08-05T00:23:39.075692: step 12469, loss 0.579082.
Train: 2018-08-05T00:23:39.325633: step 12470, loss 0.61233.
Test: 2018-08-05T00:23:40.590932: step 12470, loss 0.5486.
Train: 2018-08-05T00:23:40.825253: step 12471, loss 0.603953.
Train: 2018-08-05T00:23:41.075228: step 12472, loss 0.479708.
Train: 2018-08-05T00:23:41.325165: step 12473, loss 0.562485.
Train: 2018-08-05T00:23:41.575110: step 12474, loss 0.545958.
Train: 2018-08-05T00:23:41.825044: step 12475, loss 0.512913.
Train: 2018-08-05T00:23:42.074990: step 12476, loss 0.562487.
Train: 2018-08-05T00:23:42.324931: step 12477, loss 0.504567.
Train: 2018-08-05T00:23:42.574843: step 12478, loss 0.570757.
Train: 2018-08-05T00:23:42.824814: step 12479, loss 0.587353.
Train: 2018-08-05T00:23:43.074726: step 12480, loss 0.562457.
Test: 2018-08-05T00:23:44.340054: step 12480, loss 0.548776.
Train: 2018-08-05T00:23:44.589996: step 12481, loss 0.520931.
Train: 2018-08-05T00:23:44.824346: step 12482, loss 0.545819.
Train: 2018-08-05T00:23:45.074258: step 12483, loss 0.579087.
Train: 2018-08-05T00:23:45.324198: step 12484, loss 0.504124.
Train: 2018-08-05T00:23:45.574170: step 12485, loss 0.470634.
Train: 2018-08-05T00:23:45.839733: step 12486, loss 0.520542.
Train: 2018-08-05T00:23:46.089675: step 12487, loss 0.553981.
Train: 2018-08-05T00:23:46.339616: step 12488, loss 0.587665.
Train: 2018-08-05T00:23:46.589558: step 12489, loss 0.613072.
Train: 2018-08-05T00:23:46.839498: step 12490, loss 0.587737.
Test: 2018-08-05T00:23:48.089176: step 12490, loss 0.547449.
Train: 2018-08-05T00:23:48.323521: step 12491, loss 0.604662.
Train: 2018-08-05T00:23:48.573468: step 12492, loss 0.613082.
Train: 2018-08-05T00:23:48.823380: step 12493, loss 0.570804.
Train: 2018-08-05T00:23:49.073351: step 12494, loss 0.553958.
Train: 2018-08-05T00:23:49.338884: step 12495, loss 0.553982.
Train: 2018-08-05T00:23:49.588856: step 12496, loss 0.570782.
Train: 2018-08-05T00:23:49.838800: step 12497, loss 0.487041.
Train: 2018-08-05T00:23:50.088709: step 12498, loss 0.537289.
Train: 2018-08-05T00:23:50.338650: step 12499, loss 0.621016.
Train: 2018-08-05T00:23:50.588592: step 12500, loss 0.646047.
Test: 2018-08-05T00:23:51.838298: step 12500, loss 0.547538.
Train: 2018-08-05T00:23:52.681881: step 12501, loss 0.604125.
Train: 2018-08-05T00:23:52.931822: step 12502, loss 0.545829.
Train: 2018-08-05T00:23:53.197354: step 12503, loss 0.620471.
Train: 2018-08-05T00:23:53.447296: step 12504, loss 0.55425.
Train: 2018-08-05T00:23:53.697239: step 12505, loss 0.504962.
Train: 2018-08-05T00:23:53.947210: step 12506, loss 0.587178.
Train: 2018-08-05T00:23:54.197121: step 12507, loss 0.562575.
Train: 2018-08-05T00:23:54.447062: step 12508, loss 0.587113.
Train: 2018-08-05T00:23:54.697004: step 12509, loss 0.58708.
Train: 2018-08-05T00:23:54.946975: step 12510, loss 0.50574.
Test: 2018-08-05T00:23:56.196652: step 12510, loss 0.548357.
Train: 2018-08-05T00:23:56.446624: step 12511, loss 0.554546.
Train: 2018-08-05T00:23:56.696566: step 12512, loss 0.57079.
Train: 2018-08-05T00:23:56.946508: step 12513, loss 0.595111.
Train: 2018-08-05T00:23:57.196448: step 12514, loss 0.611266.
Train: 2018-08-05T00:23:57.446361: step 12515, loss 0.603105.
Train: 2018-08-05T00:23:57.696332: step 12516, loss 0.554729.
Train: 2018-08-05T00:23:57.946276: step 12517, loss 0.546754.
Train: 2018-08-05T00:23:58.196185: step 12518, loss 0.642982.
Train: 2018-08-05T00:23:58.446156: step 12519, loss 0.554899.
Train: 2018-08-05T00:23:58.696099: step 12520, loss 0.570893.
Test: 2018-08-05T00:23:59.961396: step 12520, loss 0.549149.
Train: 2018-08-05T00:24:00.195716: step 12521, loss 0.547077.
Train: 2018-08-05T00:24:00.445688: step 12522, loss 0.51541.
Train: 2018-08-05T00:24:00.695629: step 12523, loss 0.610571.
Train: 2018-08-05T00:24:00.945571: step 12524, loss 0.555107.
Train: 2018-08-05T00:24:01.195482: step 12525, loss 0.507664.
Train: 2018-08-05T00:24:01.445424: step 12526, loss 0.563028.
Train: 2018-08-05T00:24:01.695395: step 12527, loss 0.499644.
Train: 2018-08-05T00:24:01.945306: step 12528, loss 0.626494.
Train: 2018-08-05T00:24:02.195281: step 12529, loss 0.562969.
Train: 2018-08-05T00:24:02.445221: step 12530, loss 0.586809.
Test: 2018-08-05T00:24:03.710518: step 12530, loss 0.550037.
Train: 2018-08-05T00:24:03.944838: step 12531, loss 0.586811.
Train: 2018-08-05T00:24:04.194809: step 12532, loss 0.58681.
Train: 2018-08-05T00:24:04.397889: step 12533, loss 0.579918.
Train: 2018-08-05T00:24:04.647830: step 12534, loss 0.531217.
Train: 2018-08-05T00:24:04.897772: step 12535, loss 0.610624.
Train: 2018-08-05T00:24:05.147715: step 12536, loss 0.634402.
Train: 2018-08-05T00:24:05.397655: step 12537, loss 0.539275.
Train: 2018-08-05T00:24:05.647589: step 12538, loss 0.523516.
Train: 2018-08-05T00:24:05.897537: step 12539, loss 0.634197.
Train: 2018-08-05T00:24:06.147479: step 12540, loss 0.547293.
Test: 2018-08-05T00:24:07.397155: step 12540, loss 0.549925.
Train: 2018-08-05T00:24:07.647097: step 12541, loss 0.531554.
Train: 2018-08-05T00:24:07.912690: step 12542, loss 0.570979.
Train: 2018-08-05T00:24:08.162631: step 12543, loss 0.523664.
Train: 2018-08-05T00:24:08.412541: step 12544, loss 0.563074.
Train: 2018-08-05T00:24:08.662484: step 12545, loss 0.555151.
Train: 2018-08-05T00:24:08.912455: step 12546, loss 0.602602.
Train: 2018-08-05T00:24:09.162396: step 12547, loss 0.57886.
Train: 2018-08-05T00:24:09.427959: step 12548, loss 0.523424.
Train: 2018-08-05T00:24:09.677901: step 12549, loss 0.555071.
Train: 2018-08-05T00:24:09.927842: step 12550, loss 0.539156.
Test: 2018-08-05T00:24:11.177519: step 12550, loss 0.549294.
Train: 2018-08-05T00:24:11.427491: step 12551, loss 0.570903.
Train: 2018-08-05T00:24:11.677432: step 12552, loss 0.594798.
Train: 2018-08-05T00:24:11.927374: step 12553, loss 0.538977.
Train: 2018-08-05T00:24:12.177285: step 12554, loss 0.610815.
Train: 2018-08-05T00:24:12.489713: step 12555, loss 0.586853.
Train: 2018-08-05T00:24:12.739655: step 12556, loss 0.522933.
Train: 2018-08-05T00:24:12.989625: step 12557, loss 0.514888.
Train: 2018-08-05T00:24:13.239537: step 12558, loss 0.578866.
Train: 2018-08-05T00:24:13.505131: step 12559, loss 0.474551.
Train: 2018-08-05T00:24:13.755041: step 12560, loss 0.514464.
Test: 2018-08-05T00:24:15.004748: step 12560, loss 0.549031.
Train: 2018-08-05T00:24:15.254727: step 12561, loss 0.6274.
Train: 2018-08-05T00:24:15.504664: step 12562, loss 0.578899.
Train: 2018-08-05T00:24:15.754603: step 12563, loss 0.587025.
Train: 2018-08-05T00:24:16.004515: step 12564, loss 0.530147.
Train: 2018-08-05T00:24:16.254455: step 12565, loss 0.570779.
Train: 2018-08-05T00:24:16.504427: step 12566, loss 0.530013.
Train: 2018-08-05T00:24:16.769979: step 12567, loss 0.636114.
Train: 2018-08-05T00:24:17.019933: step 12568, loss 0.587108.
Train: 2018-08-05T00:24:17.269843: step 12569, loss 0.521775.
Train: 2018-08-05T00:24:17.519816: step 12570, loss 0.546259.
Test: 2018-08-05T00:24:18.769491: step 12570, loss 0.548747.
Train: 2018-08-05T00:24:19.003842: step 12571, loss 0.554414.
Train: 2018-08-05T00:24:19.253783: step 12572, loss 0.595317.
Train: 2018-08-05T00:24:19.503724: step 12573, loss 0.513475.
Train: 2018-08-05T00:24:19.769257: step 12574, loss 0.603537.
Train: 2018-08-05T00:24:20.019231: step 12575, loss 0.619928.
Train: 2018-08-05T00:24:20.269141: step 12576, loss 0.587134.
Train: 2018-08-05T00:24:20.519113: step 12577, loss 0.603454.
Train: 2018-08-05T00:24:20.769024: step 12578, loss 0.538165.
Train: 2018-08-05T00:24:21.018996: step 12579, loss 0.570779.
Train: 2018-08-05T00:24:21.268938: step 12580, loss 0.611417.
Test: 2018-08-05T00:24:22.518613: step 12580, loss 0.549522.
Train: 2018-08-05T00:24:22.752966: step 12581, loss 0.603219.
Train: 2018-08-05T00:24:23.002876: step 12582, loss 0.570807.
Train: 2018-08-05T00:24:23.252849: step 12583, loss 0.603052.
Train: 2018-08-05T00:24:23.502788: step 12584, loss 0.522662.
Train: 2018-08-05T00:24:23.768353: step 12585, loss 0.642963.
Train: 2018-08-05T00:24:24.002674: step 12586, loss 0.507015.
Train: 2018-08-05T00:24:24.252612: step 12587, loss 0.578859.
Train: 2018-08-05T00:24:24.518145: step 12588, loss 0.515248.
Train: 2018-08-05T00:24:24.768119: step 12589, loss 0.531179.
Train: 2018-08-05T00:24:25.018060: step 12590, loss 0.562961.
Test: 2018-08-05T00:24:26.267736: step 12590, loss 0.549674.
Train: 2018-08-05T00:24:26.502056: step 12591, loss 0.515242.
Train: 2018-08-05T00:24:26.752022: step 12592, loss 0.642576.
Train: 2018-08-05T00:24:27.001968: step 12593, loss 0.539047.
Train: 2018-08-05T00:24:27.251880: step 12594, loss 0.523101.
Train: 2018-08-05T00:24:27.501854: step 12595, loss 0.554931.
Train: 2018-08-05T00:24:27.751793: step 12596, loss 0.594839.
Train: 2018-08-05T00:24:28.001737: step 12597, loss 0.594851.
Train: 2018-08-05T00:24:28.251678: step 12598, loss 0.546883.
Train: 2018-08-05T00:24:28.501620: step 12599, loss 0.594861.
Train: 2018-08-05T00:24:28.751566: step 12600, loss 0.562868.
Test: 2018-08-05T00:24:30.016857: step 12600, loss 0.548961.
Train: 2018-08-05T00:24:30.860411: step 12601, loss 0.570866.
Train: 2018-08-05T00:24:31.110376: step 12602, loss 0.522892.
Train: 2018-08-05T00:24:31.360325: step 12603, loss 0.530847.
Train: 2018-08-05T00:24:31.610265: step 12604, loss 0.514742.
Train: 2018-08-05T00:24:31.860206: step 12605, loss 0.522619.
Train: 2018-08-05T00:24:32.110142: step 12606, loss 0.54663.
Train: 2018-08-05T00:24:32.360090: step 12607, loss 0.514175.
Train: 2018-08-05T00:24:32.610031: step 12608, loss 0.55454.
Train: 2018-08-05T00:24:32.859974: step 12609, loss 0.578928.
Train: 2018-08-05T00:24:33.109914: step 12610, loss 0.570766.
Test: 2018-08-05T00:24:34.359590: step 12610, loss 0.549996.
Train: 2018-08-05T00:24:34.593942: step 12611, loss 0.570762.
Train: 2018-08-05T00:24:34.843853: step 12612, loss 0.595416.
Train: 2018-08-05T00:24:35.093793: step 12613, loss 0.537843.
Train: 2018-08-05T00:24:35.343766: step 12614, loss 0.595482.
Train: 2018-08-05T00:24:35.609328: step 12615, loss 0.546016.
Train: 2018-08-05T00:24:35.859269: step 12616, loss 0.587265.
Train: 2018-08-05T00:24:36.109211: step 12617, loss 0.587268.
Train: 2018-08-05T00:24:36.359123: step 12618, loss 0.595514.
Train: 2018-08-05T00:24:36.609094: step 12619, loss 0.60373.
Train: 2018-08-05T00:24:36.859030: step 12620, loss 0.52962.
Test: 2018-08-05T00:24:38.108713: step 12620, loss 0.548625.
Train: 2018-08-05T00:24:38.343064: step 12621, loss 0.611852.
Train: 2018-08-05T00:24:38.592975: step 12622, loss 0.537955.
Train: 2018-08-05T00:24:38.858568: step 12623, loss 0.529812.
Train: 2018-08-05T00:24:39.092889: step 12624, loss 0.505277.
Train: 2018-08-05T00:24:39.342829: step 12625, loss 0.537996.
Train: 2018-08-05T00:24:39.592740: step 12626, loss 0.578963.
Train: 2018-08-05T00:24:39.842713: step 12627, loss 0.595381.
Train: 2018-08-05T00:24:40.092623: step 12628, loss 0.595378.
Train: 2018-08-05T00:24:40.342565: step 12629, loss 0.603557.
Train: 2018-08-05T00:24:40.608154: step 12630, loss 0.56258.
Test: 2018-08-05T00:24:41.857835: step 12630, loss 0.550076.
Train: 2018-08-05T00:24:42.107802: step 12631, loss 0.570768.
Train: 2018-08-05T00:24:42.357748: step 12632, loss 0.619729.
Train: 2018-08-05T00:24:42.623311: step 12633, loss 0.538229.
Train: 2018-08-05T00:24:42.857631: step 12634, loss 0.505805.
Train: 2018-08-05T00:24:43.107543: step 12635, loss 0.595146.
Train: 2018-08-05T00:24:43.373129: step 12636, loss 0.570792.
Train: 2018-08-05T00:24:43.607455: step 12637, loss 0.619406.
Train: 2018-08-05T00:24:43.857398: step 12638, loss 0.562722.
Train: 2018-08-05T00:24:44.107339: step 12639, loss 0.611154.
Train: 2018-08-05T00:24:44.357279: step 12640, loss 0.546693.
Test: 2018-08-05T00:24:45.606957: step 12640, loss 0.54848.
Train: 2018-08-05T00:24:45.856930: step 12641, loss 0.490555.
Train: 2018-08-05T00:24:46.106840: step 12642, loss 0.562815.
Train: 2018-08-05T00:24:46.356812: step 12643, loss 0.514659.
Train: 2018-08-05T00:24:46.606753: step 12644, loss 0.490488.
Train: 2018-08-05T00:24:46.856695: step 12645, loss 0.490261.
Train: 2018-08-05T00:24:47.122259: step 12646, loss 0.578891.
Train: 2018-08-05T00:24:47.372201: step 12647, loss 0.554559.
Train: 2018-08-05T00:24:47.622111: step 12648, loss 0.521933.
Train: 2018-08-05T00:24:47.872084: step 12649, loss 0.546255.
Train: 2018-08-05T00:24:48.122023: step 12650, loss 0.636374.
Test: 2018-08-05T00:24:49.371700: step 12650, loss 0.548067.
Train: 2018-08-05T00:24:49.606051: step 12651, loss 0.554329.
Train: 2018-08-05T00:24:49.855992: step 12652, loss 0.529612.
Train: 2018-08-05T00:24:50.105934: step 12653, loss 0.504782.
Train: 2018-08-05T00:24:50.355875: step 12654, loss 0.603846.
Train: 2018-08-05T00:24:50.605817: step 12655, loss 0.570757.
Train: 2018-08-05T00:24:50.855759: step 12656, loss 0.579058.
Train: 2018-08-05T00:24:51.105700: step 12657, loss 0.512616.
Train: 2018-08-05T00:24:51.371262: step 12658, loss 0.545801.
Train: 2018-08-05T00:24:51.605583: step 12659, loss 0.570765.
Train: 2018-08-05T00:24:51.855523: step 12660, loss 0.620845.
Test: 2018-08-05T00:24:53.105201: step 12660, loss 0.548677.
Train: 2018-08-05T00:24:53.339555: step 12661, loss 0.554077.
Train: 2018-08-05T00:24:53.589493: step 12662, loss 0.579112.
Train: 2018-08-05T00:24:53.839434: step 12663, loss 0.570766.
Train: 2018-08-05T00:24:54.100883: step 12664, loss 0.620771.
Train: 2018-08-05T00:24:54.350827: step 12665, loss 0.545807.
Train: 2018-08-05T00:24:54.600769: step 12666, loss 0.579065.
Train: 2018-08-05T00:24:54.850742: step 12667, loss 0.512726.
Train: 2018-08-05T00:24:55.116272: step 12668, loss 0.570757.
Train: 2018-08-05T00:24:55.350594: step 12669, loss 0.59559.
Train: 2018-08-05T00:24:55.600534: step 12670, loss 0.47984.
Test: 2018-08-05T00:24:56.850242: step 12670, loss 0.54811.
Train: 2018-08-05T00:24:57.100214: step 12671, loss 0.529418.
Train: 2018-08-05T00:24:57.350126: step 12672, loss 0.537656.
Train: 2018-08-05T00:24:57.600097: step 12673, loss 0.529331.
Train: 2018-08-05T00:24:57.850038: step 12674, loss 0.57076.
Train: 2018-08-05T00:24:58.099973: step 12675, loss 0.537516.
Train: 2018-08-05T00:24:58.365513: step 12676, loss 0.554114.
Train: 2018-08-05T00:24:58.615484: step 12677, loss 0.495728.
Train: 2018-08-05T00:24:58.865424: step 12678, loss 0.562412.
Train: 2018-08-05T00:24:59.115337: step 12679, loss 0.570779.
Train: 2018-08-05T00:24:59.365308: step 12680, loss 0.520423.
Test: 2018-08-05T00:25:00.630606: step 12680, loss 0.54833.
Train: 2018-08-05T00:25:00.880548: step 12681, loss 0.596021.
Train: 2018-08-05T00:25:01.130489: step 12682, loss 0.60449.
Train: 2018-08-05T00:25:01.380455: step 12683, loss 0.646608.
Train: 2018-08-05T00:25:01.583509: step 12684, loss 0.454758.
Train: 2018-08-05T00:25:01.849102: step 12685, loss 0.604447.
Train: 2018-08-05T00:25:02.099043: step 12686, loss 0.596001.
Train: 2018-08-05T00:25:02.364575: step 12687, loss 0.470036.
Train: 2018-08-05T00:25:02.614547: step 12688, loss 0.520393.
Train: 2018-08-05T00:25:02.864488: step 12689, loss 0.604421.
Train: 2018-08-05T00:25:03.114401: step 12690, loss 0.486679.
Test: 2018-08-05T00:25:04.379729: step 12690, loss 0.547334.
Train: 2018-08-05T00:25:04.629700: step 12691, loss 0.604493.
Train: 2018-08-05T00:25:04.879611: step 12692, loss 0.562396.
Train: 2018-08-05T00:25:05.129583: step 12693, loss 0.621366.
Train: 2018-08-05T00:25:05.379524: step 12694, loss 0.562382.
Train: 2018-08-05T00:25:05.629436: step 12695, loss 0.495151.
Train: 2018-08-05T00:25:05.895028: step 12696, loss 0.528767.
Train: 2018-08-05T00:25:06.144971: step 12697, loss 0.58761.
Train: 2018-08-05T00:25:06.394912: step 12698, loss 0.596016.
Train: 2018-08-05T00:25:06.644853: step 12699, loss 0.663197.
Train: 2018-08-05T00:25:06.894794: step 12700, loss 0.512147.
Test: 2018-08-05T00:25:08.144472: step 12700, loss 0.548448.
Train: 2018-08-05T00:25:08.988055: step 12701, loss 0.579132.
Train: 2018-08-05T00:25:09.253617: step 12702, loss 0.587453.
Train: 2018-08-05T00:25:09.503559: step 12703, loss 0.520829.
Train: 2018-08-05T00:25:09.753494: step 12704, loss 0.579069.
Train: 2018-08-05T00:25:10.019062: step 12705, loss 0.570759.
Train: 2018-08-05T00:25:10.269005: step 12706, loss 0.579035.
Train: 2018-08-05T00:25:10.518949: step 12707, loss 0.521184.
Train: 2018-08-05T00:25:10.768888: step 12708, loss 0.603768.
Train: 2018-08-05T00:25:11.018829: step 12709, loss 0.546043.
Train: 2018-08-05T00:25:11.284392: step 12710, loss 0.554306.
Test: 2018-08-05T00:25:12.518448: step 12710, loss 0.549389.
Train: 2018-08-05T00:25:12.752799: step 12711, loss 0.595407.
Train: 2018-08-05T00:25:13.018360: step 12712, loss 0.554359.
Train: 2018-08-05T00:25:13.268272: step 12713, loss 0.546196.
Train: 2018-08-05T00:25:13.518244: step 12714, loss 0.513494.
Train: 2018-08-05T00:25:13.768185: step 12715, loss 0.570766.
Train: 2018-08-05T00:25:14.018127: step 12716, loss 0.652591.
Train: 2018-08-05T00:25:14.268068: step 12717, loss 0.497275.
Train: 2018-08-05T00:25:14.518010: step 12718, loss 0.546282.
Train: 2018-08-05T00:25:14.767922: step 12719, loss 0.619748.
Train: 2018-08-05T00:25:15.017893: step 12720, loss 0.554469.
Test: 2018-08-05T00:25:16.267569: step 12720, loss 0.547904.
Train: 2018-08-05T00:25:16.517512: step 12721, loss 0.595212.
Train: 2018-08-05T00:25:16.751833: step 12722, loss 0.570781.
Train: 2018-08-05T00:25:17.001797: step 12723, loss 0.562667.
Train: 2018-08-05T00:25:17.251744: step 12724, loss 0.578898.
Train: 2018-08-05T00:25:17.501656: step 12725, loss 0.562703.
Train: 2018-08-05T00:25:17.751627: step 12726, loss 0.635485.
Train: 2018-08-05T00:25:18.017190: step 12727, loss 0.659502.
Train: 2018-08-05T00:25:18.267101: step 12728, loss 0.562814.
Train: 2018-08-05T00:25:18.517042: step 12729, loss 0.562877.
Train: 2018-08-05T00:25:18.767015: step 12730, loss 0.586825.
Test: 2018-08-05T00:25:20.032312: step 12730, loss 0.54846.
Train: 2018-08-05T00:25:20.297876: step 12731, loss 0.547115.
Train: 2018-08-05T00:25:20.547818: step 12732, loss 0.602595.
Train: 2018-08-05T00:25:20.797789: step 12733, loss 0.665597.
Train: 2018-08-05T00:25:21.047730: step 12734, loss 0.571036.
Train: 2018-08-05T00:25:21.297672: step 12735, loss 0.539838.
Train: 2018-08-05T00:25:21.547613: step 12736, loss 0.602254.
Train: 2018-08-05T00:25:21.797555: step 12737, loss 0.516895.
Train: 2018-08-05T00:25:22.047496: step 12738, loss 0.586665.
Train: 2018-08-05T00:25:22.297407: step 12739, loss 0.501739.
Train: 2018-08-05T00:25:22.547349: step 12740, loss 0.509455.
Test: 2018-08-05T00:25:23.812677: step 12740, loss 0.550189.
Train: 2018-08-05T00:25:24.062619: step 12741, loss 0.50934.
Train: 2018-08-05T00:25:24.296971: step 12742, loss 0.493503.
Train: 2018-08-05T00:25:24.546911: step 12743, loss 0.555793.
Train: 2018-08-05T00:25:24.812476: step 12744, loss 0.500377.
Train: 2018-08-05T00:25:25.062410: step 12745, loss 0.53107.
Train: 2018-08-05T00:25:25.312360: step 12746, loss 0.538804.
Train: 2018-08-05T00:25:25.562298: step 12747, loss 0.636629.
Train: 2018-08-05T00:25:25.812242: step 12748, loss 0.54666.
Train: 2018-08-05T00:25:26.062181: step 12749, loss 0.545917.
Train: 2018-08-05T00:25:26.312117: step 12750, loss 0.571422.
Test: 2018-08-05T00:25:27.546178: step 12750, loss 0.548895.
Train: 2018-08-05T00:25:27.796133: step 12751, loss 0.538537.
Train: 2018-08-05T00:25:28.046086: step 12752, loss 0.521883.
Train: 2018-08-05T00:25:28.296033: step 12753, loss 0.561928.
Train: 2018-08-05T00:25:28.545974: step 12754, loss 0.528827.
Train: 2018-08-05T00:25:28.795916: step 12755, loss 0.554436.
Train: 2018-08-05T00:25:29.045827: step 12756, loss 0.588074.
Train: 2018-08-05T00:25:29.311390: step 12757, loss 0.545163.
Train: 2018-08-05T00:25:29.545741: step 12758, loss 0.544752.
Train: 2018-08-05T00:25:29.811303: step 12759, loss 0.461464.
Train: 2018-08-05T00:25:30.061247: step 12760, loss 0.52788.
Test: 2018-08-05T00:25:31.310921: step 12760, loss 0.548941.
Train: 2018-08-05T00:25:31.545273: step 12761, loss 0.529146.
Train: 2018-08-05T00:25:31.810805: step 12762, loss 0.553839.
Train: 2018-08-05T00:25:32.060747: step 12763, loss 0.528832.
Train: 2018-08-05T00:25:32.310718: step 12764, loss 0.633239.
Train: 2018-08-05T00:25:32.560661: step 12765, loss 0.512365.
Train: 2018-08-05T00:25:32.810578: step 12766, loss 0.500372.
Train: 2018-08-05T00:25:33.076167: step 12767, loss 0.59869.
Train: 2018-08-05T00:25:33.326074: step 12768, loss 0.553865.
Train: 2018-08-05T00:25:33.576050: step 12769, loss 0.589459.
Train: 2018-08-05T00:25:33.825991: step 12770, loss 0.555572.
Test: 2018-08-05T00:25:35.075665: step 12770, loss 0.547564.
Train: 2018-08-05T00:25:35.325607: step 12771, loss 0.656473.
Train: 2018-08-05T00:25:35.575582: step 12772, loss 0.587971.
Train: 2018-08-05T00:25:35.825491: step 12773, loss 0.596284.
Train: 2018-08-05T00:25:36.075464: step 12774, loss 0.511681.
Train: 2018-08-05T00:25:36.325373: step 12775, loss 0.570791.
Train: 2018-08-05T00:25:36.575314: step 12776, loss 0.528814.
Train: 2018-08-05T00:25:36.825256: step 12777, loss 0.503713.
Train: 2018-08-05T00:25:37.075196: step 12778, loss 0.579167.
Train: 2018-08-05T00:25:37.325171: step 12779, loss 0.545615.
Train: 2018-08-05T00:25:37.575113: step 12780, loss 0.545653.
Test: 2018-08-05T00:25:38.840408: step 12780, loss 0.548404.
Train: 2018-08-05T00:25:39.090381: step 12781, loss 0.512078.
Train: 2018-08-05T00:25:39.324672: step 12782, loss 0.545642.
Train: 2018-08-05T00:25:39.574645: step 12783, loss 0.52876.
Train: 2018-08-05T00:25:39.824587: step 12784, loss 0.596038.
Train: 2018-08-05T00:25:40.074528: step 12785, loss 0.65501.
Train: 2018-08-05T00:25:40.324436: step 12786, loss 0.596221.
Train: 2018-08-05T00:25:40.589999: step 12787, loss 0.646595.
Train: 2018-08-05T00:25:40.839970: step 12788, loss 0.620963.
Train: 2018-08-05T00:25:41.089912: step 12789, loss 0.562473.
Train: 2018-08-05T00:25:41.339854: step 12790, loss 0.603838.
Test: 2018-08-05T00:25:42.589530: step 12790, loss 0.547641.
Train: 2018-08-05T00:25:42.823852: step 12791, loss 0.62.
Train: 2018-08-05T00:25:43.073792: step 12792, loss 0.587113.
Train: 2018-08-05T00:25:43.323733: step 12793, loss 0.538215.
Train: 2018-08-05T00:25:43.573708: step 12794, loss 0.465568.
Train: 2018-08-05T00:25:43.823616: step 12795, loss 0.522307.
Train: 2018-08-05T00:25:44.073592: step 12796, loss 0.667668.
Train: 2018-08-05T00:25:44.323533: step 12797, loss 0.530701.
Train: 2018-08-05T00:25:44.589062: step 12798, loss 0.594922.
Train: 2018-08-05T00:25:44.839028: step 12799, loss 0.570912.
Train: 2018-08-05T00:25:45.088945: step 12800, loss 0.54689.
Test: 2018-08-05T00:25:46.338653: step 12800, loss 0.549773.
Train: 2018-08-05T00:25:47.182238: step 12801, loss 0.554931.
Train: 2018-08-05T00:25:47.432177: step 12802, loss 0.515122.
Train: 2018-08-05T00:25:47.682089: step 12803, loss 0.594786.
Train: 2018-08-05T00:25:47.932061: step 12804, loss 0.539049.
Train: 2018-08-05T00:25:48.182004: step 12805, loss 0.610715.
Train: 2018-08-05T00:25:48.431942: step 12806, loss 0.586842.
Train: 2018-08-05T00:25:48.681888: step 12807, loss 0.562957.
Train: 2018-08-05T00:25:48.931796: step 12808, loss 0.586805.
Train: 2018-08-05T00:25:49.181738: step 12809, loss 0.618511.
Train: 2018-08-05T00:25:49.431679: step 12810, loss 0.507648.
Test: 2018-08-05T00:25:50.681386: step 12810, loss 0.549658.
Train: 2018-08-05T00:25:50.931356: step 12811, loss 0.602585.
Train: 2018-08-05T00:25:51.181299: step 12812, loss 0.602551.
Train: 2018-08-05T00:25:51.431243: step 12813, loss 0.563104.
Train: 2018-08-05T00:25:51.681181: step 12814, loss 0.594606.
Train: 2018-08-05T00:25:51.931127: step 12815, loss 0.500323.
Train: 2018-08-05T00:25:52.181065: step 12816, loss 0.657408.
Train: 2018-08-05T00:25:52.431007: step 12817, loss 0.500505.
Train: 2018-08-05T00:25:52.680949: step 12818, loss 0.578878.
Train: 2018-08-05T00:25:52.930892: step 12819, loss 0.516222.
Train: 2018-08-05T00:25:53.196421: step 12820, loss 0.547519.
Test: 2018-08-05T00:25:54.430507: step 12820, loss 0.549701.
Train: 2018-08-05T00:25:54.680449: step 12821, loss 0.516072.
Train: 2018-08-05T00:25:54.930421: step 12822, loss 0.52378.
Train: 2018-08-05T00:25:55.180333: step 12823, loss 0.539384.
Train: 2018-08-05T00:25:55.430308: step 12824, loss 0.570935.
Train: 2018-08-05T00:25:55.680214: step 12825, loss 0.531157.
Train: 2018-08-05T00:25:55.930187: step 12826, loss 0.55492.
Train: 2018-08-05T00:25:56.180123: step 12827, loss 0.54683.
Train: 2018-08-05T00:25:56.430040: step 12828, loss 0.498487.
Train: 2018-08-05T00:25:56.680011: step 12829, loss 0.5385.
Train: 2018-08-05T00:25:56.929924: step 12830, loss 0.595138.
Test: 2018-08-05T00:25:58.179630: step 12830, loss 0.549776.
Train: 2018-08-05T00:25:58.413982: step 12831, loss 0.538192.
Train: 2018-08-05T00:25:58.663916: step 12832, loss 0.595299.
Train: 2018-08-05T00:25:58.913833: step 12833, loss 0.529764.
Train: 2018-08-05T00:25:59.163804: step 12834, loss 0.513178.
Train: 2018-08-05T00:25:59.366882: step 12835, loss 0.59773.
Train: 2018-08-05T00:25:59.616794: step 12836, loss 0.595589.
Train: 2018-08-05T00:25:59.866736: step 12837, loss 0.521012.
Train: 2018-08-05T00:26:00.116677: step 12838, loss 0.554134.
Train: 2018-08-05T00:26:00.366649: step 12839, loss 0.579098.
Train: 2018-08-05T00:26:00.616560: step 12840, loss 0.587439.
Test: 2018-08-05T00:26:01.881888: step 12840, loss 0.548296.
Train: 2018-08-05T00:26:02.116239: step 12841, loss 0.612491.
Train: 2018-08-05T00:26:02.366180: step 12842, loss 0.637472.
Train: 2018-08-05T00:26:02.616121: step 12843, loss 0.454259.
Train: 2018-08-05T00:26:02.866062: step 12844, loss 0.495858.
Train: 2018-08-05T00:26:03.116004: step 12845, loss 0.545765.
Train: 2018-08-05T00:26:03.365946: step 12846, loss 0.61251.
Train: 2018-08-05T00:26:03.615887: step 12847, loss 0.587466.
Train: 2018-08-05T00:26:03.865799: step 12848, loss 0.520679.
Train: 2018-08-05T00:26:04.115739: step 12849, loss 0.562415.
Train: 2018-08-05T00:26:04.365711: step 12850, loss 0.520656.
Test: 2018-08-05T00:26:05.599767: step 12850, loss 0.548641.
Train: 2018-08-05T00:26:05.849735: step 12851, loss 0.620925.
Train: 2018-08-05T00:26:06.099680: step 12852, loss 0.528987.
Train: 2018-08-05T00:26:06.349622: step 12853, loss 0.487186.
Train: 2018-08-05T00:26:06.599564: step 12854, loss 0.570765.
Train: 2018-08-05T00:26:06.849474: step 12855, loss 0.621077.
Train: 2018-08-05T00:26:07.099446: step 12856, loss 0.495338.
Train: 2018-08-05T00:26:07.349388: step 12857, loss 0.461751.
Train: 2018-08-05T00:26:07.599332: step 12858, loss 0.528731.
Train: 2018-08-05T00:26:07.849241: step 12859, loss 0.528581.
Train: 2018-08-05T00:26:08.099212: step 12860, loss 0.570839.
Test: 2018-08-05T00:26:09.348889: step 12860, loss 0.548174.
Train: 2018-08-05T00:26:09.583210: step 12861, loss 0.519951.
Train: 2018-08-05T00:26:09.833180: step 12862, loss 0.502748.
Train: 2018-08-05T00:26:10.083123: step 12863, loss 0.596373.
Train: 2018-08-05T00:26:10.333064: step 12864, loss 0.613856.
Train: 2018-08-05T00:26:10.583006: step 12865, loss 0.528019.
Train: 2018-08-05T00:26:10.848568: step 12866, loss 0.562377.
Train: 2018-08-05T00:26:11.098510: step 12867, loss 0.57949.
Train: 2018-08-05T00:26:11.348452: step 12868, loss 0.562328.
Train: 2018-08-05T00:26:11.598393: step 12869, loss 0.527963.
Train: 2018-08-05T00:26:11.848305: step 12870, loss 0.588085.
Test: 2018-08-05T00:26:13.113632: step 12870, loss 0.547186.
Train: 2018-08-05T00:26:13.347954: step 12871, loss 0.579544.
Train: 2018-08-05T00:26:13.613546: step 12872, loss 0.570888.
Train: 2018-08-05T00:26:13.879109: step 12873, loss 0.622453.
Train: 2018-08-05T00:26:14.129021: step 12874, loss 0.639388.
Train: 2018-08-05T00:26:14.378961: step 12875, loss 0.56235.
Train: 2018-08-05T00:26:14.628934: step 12876, loss 0.536864.
Train: 2018-08-05T00:26:14.878844: step 12877, loss 0.570816.
Train: 2018-08-05T00:26:15.128785: step 12878, loss 0.494951.
Train: 2018-08-05T00:26:15.378758: step 12879, loss 0.503498.
Train: 2018-08-05T00:26:15.628704: step 12880, loss 0.562388.
Test: 2018-08-05T00:26:16.878376: step 12880, loss 0.548154.
Train: 2018-08-05T00:26:17.112727: step 12881, loss 0.621217.
Train: 2018-08-05T00:26:17.362668: step 12882, loss 0.512057.
Train: 2018-08-05T00:26:17.612579: step 12883, loss 0.604311.
Train: 2018-08-05T00:26:17.862551: step 12884, loss 0.554037.
Train: 2018-08-05T00:26:18.112464: step 12885, loss 0.554056.
Train: 2018-08-05T00:26:18.362405: step 12886, loss 0.554074.
Train: 2018-08-05T00:26:18.612376: step 12887, loss 0.512397.
Train: 2018-08-05T00:26:18.862316: step 12888, loss 0.620794.
Train: 2018-08-05T00:26:19.112227: step 12889, loss 0.604072.
Train: 2018-08-05T00:26:19.362174: step 12890, loss 0.545831.
Test: 2018-08-05T00:26:20.611877: step 12890, loss 0.548225.
Train: 2018-08-05T00:26:20.846198: step 12891, loss 0.637127.
Train: 2018-08-05T00:26:21.096169: step 12892, loss 0.587297.
Train: 2018-08-05T00:26:21.346110: step 12893, loss 0.570757.
Train: 2018-08-05T00:26:21.596051: step 12894, loss 0.669346.
Train: 2018-08-05T00:26:21.845992: step 12895, loss 0.521729.
Train: 2018-08-05T00:26:22.095904: step 12896, loss 0.497504.
Train: 2018-08-05T00:26:22.345876: step 12897, loss 0.505789.
Train: 2018-08-05T00:26:22.595818: step 12898, loss 0.595145.
Train: 2018-08-05T00:26:22.845759: step 12899, loss 0.538357.
Train: 2018-08-05T00:26:23.095700: step 12900, loss 0.522169.
Test: 2018-08-05T00:26:24.345378: step 12900, loss 0.549147.
Train: 2018-08-05T00:26:25.204582: step 12901, loss 0.627542.
Train: 2018-08-05T00:26:25.454523: step 12902, loss 0.514108.
Train: 2018-08-05T00:26:25.704464: step 12903, loss 0.586996.
Train: 2018-08-05T00:26:25.954376: step 12904, loss 0.570798.
Train: 2018-08-05T00:26:26.204347: step 12905, loss 0.595078.
Train: 2018-08-05T00:26:26.454289: step 12906, loss 0.522301.
Train: 2018-08-05T00:26:26.704231: step 12907, loss 0.554638.
Train: 2018-08-05T00:26:26.954171: step 12908, loss 0.586974.
Train: 2018-08-05T00:26:27.204113: step 12909, loss 0.538479.
Train: 2018-08-05T00:26:27.454056: step 12910, loss 0.651646.
Test: 2018-08-05T00:26:28.703732: step 12910, loss 0.548706.
Train: 2018-08-05T00:26:28.938086: step 12911, loss 0.562744.
Train: 2018-08-05T00:26:29.188027: step 12912, loss 0.562761.
Train: 2018-08-05T00:26:29.437936: step 12913, loss 0.522529.
Train: 2018-08-05T00:26:29.687909: step 12914, loss 0.578876.
Train: 2018-08-05T00:26:29.937851: step 12915, loss 0.594963.
Train: 2018-08-05T00:26:30.187792: step 12916, loss 0.570836.
Train: 2018-08-05T00:26:30.437702: step 12917, loss 0.627036.
Train: 2018-08-05T00:26:30.687676: step 12918, loss 0.570855.
Train: 2018-08-05T00:26:30.937608: step 12919, loss 0.562877.
Train: 2018-08-05T00:26:31.187558: step 12920, loss 0.546949.
Test: 2018-08-05T00:26:32.437233: step 12920, loss 0.549798.
Train: 2018-08-05T00:26:32.687207: step 12921, loss 0.570891.
Train: 2018-08-05T00:26:32.937149: step 12922, loss 0.586818.
Train: 2018-08-05T00:26:33.187090: step 12923, loss 0.626545.
Train: 2018-08-05T00:26:33.436998: step 12924, loss 0.634349.
Train: 2018-08-05T00:26:33.686973: step 12925, loss 0.452509.
Train: 2018-08-05T00:26:33.936882: step 12926, loss 0.531518.
Train: 2018-08-05T00:26:34.186823: step 12927, loss 0.539402.
Train: 2018-08-05T00:26:34.436798: step 12928, loss 0.594659.
Train: 2018-08-05T00:26:34.686739: step 12929, loss 0.507761.
Train: 2018-08-05T00:26:34.936648: step 12930, loss 0.483912.
Test: 2018-08-05T00:26:36.186355: step 12930, loss 0.549541.
Train: 2018-08-05T00:26:36.436296: step 12931, loss 0.555043.
Train: 2018-08-05T00:26:36.701889: step 12932, loss 0.515145.
Train: 2018-08-05T00:26:36.951831: step 12933, loss 0.466909.
Train: 2018-08-05T00:26:37.201741: step 12934, loss 0.522571.
Train: 2018-08-05T00:26:37.451713: step 12935, loss 0.514152.
Train: 2018-08-05T00:26:37.701655: step 12936, loss 0.611505.
Train: 2018-08-05T00:26:37.951597: step 12937, loss 0.59532.
Train: 2018-08-05T00:26:38.201538: step 12938, loss 0.546115.
Train: 2018-08-05T00:26:38.451485: step 12939, loss 0.546024.
Train: 2018-08-05T00:26:38.717013: step 12940, loss 0.537665.
Test: 2018-08-05T00:26:39.966719: step 12940, loss 0.548591.
Train: 2018-08-05T00:26:40.216661: step 12941, loss 0.504339.
Train: 2018-08-05T00:26:40.466628: step 12942, loss 0.545751.
Train: 2018-08-05T00:26:40.716575: step 12943, loss 0.654496.
Train: 2018-08-05T00:26:40.966516: step 12944, loss 0.59594.
Train: 2018-08-05T00:26:41.216457: step 12945, loss 0.486858.
Train: 2018-08-05T00:26:41.466399: step 12946, loss 0.570791.
Train: 2018-08-05T00:26:41.716340: step 12947, loss 0.570797.
Train: 2018-08-05T00:26:41.966252: step 12948, loss 0.629819.
Train: 2018-08-05T00:26:42.216226: step 12949, loss 0.461252.
Train: 2018-08-05T00:26:42.481786: step 12950, loss 0.494862.
Test: 2018-08-05T00:26:43.731462: step 12950, loss 0.548817.
Train: 2018-08-05T00:26:43.965814: step 12951, loss 0.587733.
Train: 2018-08-05T00:26:44.215755: step 12952, loss 0.545413.
Train: 2018-08-05T00:26:44.465666: step 12953, loss 0.56235.
Train: 2018-08-05T00:26:44.715632: step 12954, loss 0.57084.
Train: 2018-08-05T00:26:44.965579: step 12955, loss 0.638837.
Train: 2018-08-05T00:26:45.215521: step 12956, loss 0.562348.
Train: 2018-08-05T00:26:45.465462: step 12957, loss 0.579313.
Train: 2018-08-05T00:26:45.715404: step 12958, loss 0.570823.
Train: 2018-08-05T00:26:45.965316: step 12959, loss 0.553906.
Train: 2018-08-05T00:26:46.215287: step 12960, loss 0.65522.
Test: 2018-08-05T00:26:47.464963: step 12960, loss 0.548909.
Train: 2018-08-05T00:26:47.714906: step 12961, loss 0.570793.
Train: 2018-08-05T00:26:47.980469: step 12962, loss 0.503696.
Train: 2018-08-05T00:26:48.230440: step 12963, loss 0.595882.
Train: 2018-08-05T00:26:48.480383: step 12964, loss 0.545724.
Train: 2018-08-05T00:26:48.730293: step 12965, loss 0.562433.
Train: 2018-08-05T00:26:48.980235: step 12966, loss 0.562446.
Train: 2018-08-05T00:26:49.230205: step 12967, loss 0.56246.
Train: 2018-08-05T00:26:49.480147: step 12968, loss 0.6536.
Train: 2018-08-05T00:26:49.730083: step 12969, loss 0.529479.
Train: 2018-08-05T00:26:49.980026: step 12970, loss 0.554288.
Test: 2018-08-05T00:26:51.229707: step 12970, loss 0.54882.
Train: 2018-08-05T00:26:51.479679: step 12971, loss 0.578976.
Train: 2018-08-05T00:26:51.729620: step 12972, loss 0.554367.
Train: 2018-08-05T00:26:51.979562: step 12973, loss 0.505314.
Train: 2018-08-05T00:26:52.229503: step 12974, loss 0.529884.
Train: 2018-08-05T00:26:52.479445: step 12975, loss 0.578945.
Train: 2018-08-05T00:26:52.729386: step 12976, loss 0.529889.
Train: 2018-08-05T00:26:52.979327: step 12977, loss 0.636202.
Train: 2018-08-05T00:26:53.229263: step 12978, loss 0.562599.
Train: 2018-08-05T00:26:53.479197: step 12979, loss 0.587095.
Train: 2018-08-05T00:26:53.729122: step 12980, loss 0.603376.
Test: 2018-08-05T00:26:54.978828: step 12980, loss 0.548693.
Train: 2018-08-05T00:26:55.244422: step 12981, loss 0.587047.
Train: 2018-08-05T00:26:55.494365: step 12982, loss 0.595129.
Train: 2018-08-05T00:26:55.744274: step 12983, loss 0.570802.
Train: 2018-08-05T00:26:56.009837: step 12984, loss 0.603086.
Train: 2018-08-05T00:26:56.259810: step 12985, loss 0.514542.
Train: 2018-08-05T00:26:56.462887: step 12986, loss 0.511445.
Train: 2018-08-05T00:26:56.712828: step 12987, loss 0.538754.
Train: 2018-08-05T00:26:56.962770: step 12988, loss 0.538748.
Train: 2018-08-05T00:26:57.212712: step 12989, loss 0.60296.
Train: 2018-08-05T00:26:57.462653: step 12990, loss 0.691274.
Test: 2018-08-05T00:26:58.727951: step 12990, loss 0.549849.
Train: 2018-08-05T00:26:58.962303: step 12991, loss 0.618897.
Train: 2018-08-05T00:26:59.212243: step 12992, loss 0.523019.
Train: 2018-08-05T00:26:59.462187: step 12993, loss 0.554983.
Train: 2018-08-05T00:26:59.712096: step 12994, loss 0.634468.
Train: 2018-08-05T00:26:59.962067: step 12995, loss 0.586779.
Train: 2018-08-05T00:27:00.212009: step 12996, loss 0.626232.
Train: 2018-08-05T00:27:00.461951: step 12997, loss 0.555282.
Train: 2018-08-05T00:27:00.711891: step 12998, loss 0.63373.
Train: 2018-08-05T00:27:00.961833: step 12999, loss 0.586693.
Train: 2018-08-05T00:27:01.211776: step 13000, loss 0.532303.
Test: 2018-08-05T00:27:02.461451: step 13000, loss 0.550143.
Train: 2018-08-05T00:27:03.336276: step 13001, loss 0.478243.
Train: 2018-08-05T00:27:03.601839: step 13002, loss 0.547961.
Train: 2018-08-05T00:27:03.851781: step 13003, loss 0.532471.
Train: 2018-08-05T00:27:04.101692: step 13004, loss 0.493659.
Train: 2018-08-05T00:27:04.351635: step 13005, loss 0.540042.
Train: 2018-08-05T00:27:04.617197: step 13006, loss 0.555497.
Train: 2018-08-05T00:27:04.867169: step 13007, loss 0.578882.
Train: 2018-08-05T00:27:05.117110: step 13008, loss 0.578874.
Train: 2018-08-05T00:27:05.367021: step 13009, loss 0.547406.
Train: 2018-08-05T00:27:05.616962: step 13010, loss 0.539431.
Test: 2018-08-05T00:27:06.866669: step 13010, loss 0.550556.
Train: 2018-08-05T00:27:07.100992: step 13011, loss 0.626325.
Train: 2018-08-05T00:27:07.350962: step 13012, loss 0.555091.
Train: 2018-08-05T00:27:07.616495: step 13013, loss 0.523308.
Train: 2018-08-05T00:27:07.866437: step 13014, loss 0.562948.
Train: 2018-08-05T00:27:08.116379: step 13015, loss 0.554937.
Train: 2018-08-05T00:27:08.366320: step 13016, loss 0.570869.
Train: 2018-08-05T00:27:08.616262: step 13017, loss 0.514788.
Train: 2018-08-05T00:27:08.866233: step 13018, loss 0.586904.
Train: 2018-08-05T00:27:09.116174: step 13019, loss 0.594979.
Train: 2018-08-05T00:27:09.366115: step 13020, loss 0.570818.
Test: 2018-08-05T00:27:10.615793: step 13020, loss 0.550563.
Train: 2018-08-05T00:27:10.850144: step 13021, loss 0.619239.
Train: 2018-08-05T00:27:11.100084: step 13022, loss 0.554674.
Train: 2018-08-05T00:27:11.349995: step 13023, loss 0.595025.
Train: 2018-08-05T00:27:11.599967: step 13024, loss 0.514354.
Train: 2018-08-05T00:27:11.849880: step 13025, loss 0.627307.
Train: 2018-08-05T00:27:12.099819: step 13026, loss 0.578881.
Train: 2018-08-05T00:27:12.349793: step 13027, loss 0.59499.
Train: 2018-08-05T00:27:12.599732: step 13028, loss 0.594961.
Train: 2018-08-05T00:27:12.849645: step 13029, loss 0.610978.
Train: 2018-08-05T00:27:13.099616: step 13030, loss 0.634898.
Test: 2018-08-05T00:27:14.349293: step 13030, loss 0.549415.
Train: 2018-08-05T00:27:14.583644: step 13031, loss 0.546968.
Train: 2018-08-05T00:27:14.833556: step 13032, loss 0.594754.
Train: 2018-08-05T00:27:15.083526: step 13033, loss 0.602622.
Train: 2018-08-05T00:27:15.333468: step 13034, loss 0.531519.
Train: 2018-08-05T00:27:15.583410: step 13035, loss 0.539516.
Train: 2018-08-05T00:27:15.833321: step 13036, loss 0.555299.
Train: 2018-08-05T00:27:16.083292: step 13037, loss 0.50039.
Train: 2018-08-05T00:27:16.333233: step 13038, loss 0.531755.
Train: 2018-08-05T00:27:16.583175: step 13039, loss 0.571005.
Train: 2018-08-05T00:27:16.833116: step 13040, loss 0.578867.
Test: 2018-08-05T00:27:18.082793: step 13040, loss 0.550486.
Train: 2018-08-05T00:27:18.317115: step 13041, loss 0.594626.
Train: 2018-08-05T00:27:18.567086: step 13042, loss 0.594631.
Train: 2018-08-05T00:27:18.816997: step 13043, loss 0.531579.
Train: 2018-08-05T00:27:19.066939: step 13044, loss 0.531549.
Train: 2018-08-05T00:27:19.316880: step 13045, loss 0.563068.
Train: 2018-08-05T00:27:19.566822: step 13046, loss 0.483964.
Train: 2018-08-05T00:27:19.816764: step 13047, loss 0.539193.
Train: 2018-08-05T00:27:20.066704: step 13048, loss 0.610702.
Train: 2018-08-05T00:27:20.316670: step 13049, loss 0.586838.
Train: 2018-08-05T00:27:20.566619: step 13050, loss 0.514937.
Test: 2018-08-05T00:27:21.831915: step 13050, loss 0.549096.
Train: 2018-08-05T00:27:22.128751: step 13051, loss 0.610909.
Train: 2018-08-05T00:27:22.378693: step 13052, loss 0.562825.
Train: 2018-08-05T00:27:22.628634: step 13053, loss 0.594933.
Train: 2018-08-05T00:27:22.878546: step 13054, loss 0.5628.
Train: 2018-08-05T00:27:23.128488: step 13055, loss 0.506513.
Train: 2018-08-05T00:27:23.378462: step 13056, loss 0.514446.
Train: 2018-08-05T00:27:23.628401: step 13057, loss 0.619262.
Train: 2018-08-05T00:27:23.878342: step 13058, loss 0.570805.
Train: 2018-08-05T00:27:24.128283: step 13059, loss 0.530335.
Train: 2018-08-05T00:27:24.378225: step 13060, loss 0.530264.
Test: 2018-08-05T00:27:25.627901: step 13060, loss 0.548909.
Train: 2018-08-05T00:27:25.877874: step 13061, loss 0.562662.
Train: 2018-08-05T00:27:26.127784: step 13062, loss 0.578919.
Train: 2018-08-05T00:27:26.377756: step 13063, loss 0.603378.
Train: 2018-08-05T00:27:26.627667: step 13064, loss 0.521849.
Train: 2018-08-05T00:27:26.877639: step 13065, loss 0.562607.
Train: 2018-08-05T00:27:27.127581: step 13066, loss 0.595288.
Train: 2018-08-05T00:27:27.377522: step 13067, loss 0.497188.
Train: 2018-08-05T00:27:27.627464: step 13068, loss 0.497066.
Train: 2018-08-05T00:27:27.877405: step 13069, loss 0.57076.
Train: 2018-08-05T00:27:28.142938: step 13070, loss 0.521376.
Test: 2018-08-05T00:27:29.392645: step 13070, loss 0.548338.
Train: 2018-08-05T00:27:29.626996: step 13071, loss 0.51298.
Train: 2018-08-05T00:27:29.876908: step 13072, loss 0.587322.
Train: 2018-08-05T00:27:30.126849: step 13073, loss 0.570759.
Train: 2018-08-05T00:27:30.376791: step 13074, loss 0.554119.
Train: 2018-08-05T00:27:30.626762: step 13075, loss 0.562428.
Train: 2018-08-05T00:27:30.876703: step 13076, loss 0.562418.
Train: 2018-08-05T00:27:31.142236: step 13077, loss 0.537323.
Train: 2018-08-05T00:27:31.376557: step 13078, loss 0.579153.
Train: 2018-08-05T00:27:31.626527: step 13079, loss 0.587548.
Train: 2018-08-05T00:27:31.876469: step 13080, loss 0.59594.
Test: 2018-08-05T00:27:33.126146: step 13080, loss 0.548206.
Train: 2018-08-05T00:27:33.376088: step 13081, loss 0.587543.
Train: 2018-08-05T00:27:33.626054: step 13082, loss 0.595893.
Train: 2018-08-05T00:27:33.876001: step 13083, loss 0.545698.
Train: 2018-08-05T00:27:34.125913: step 13084, loss 0.612498.
Train: 2018-08-05T00:27:34.375884: step 13085, loss 0.504152.
Train: 2018-08-05T00:27:34.625825: step 13086, loss 0.637302.
Train: 2018-08-05T00:27:34.875767: step 13087, loss 0.520979.
Train: 2018-08-05T00:27:35.125708: step 13088, loss 0.595609.
Train: 2018-08-05T00:27:35.375650: step 13089, loss 0.587289.
Train: 2018-08-05T00:27:35.625591: step 13090, loss 0.537773.
Test: 2018-08-05T00:27:36.890889: step 13090, loss 0.549727.
Train: 2018-08-05T00:27:37.125241: step 13091, loss 0.562526.
Train: 2018-08-05T00:27:37.375152: step 13092, loss 0.595412.
Train: 2018-08-05T00:27:37.625123: step 13093, loss 0.619959.
Train: 2018-08-05T00:27:37.875065: step 13094, loss 0.529902.
Train: 2018-08-05T00:27:38.124975: step 13095, loss 0.505533.
Train: 2018-08-05T00:27:38.374916: step 13096, loss 0.54633.
Train: 2018-08-05T00:27:38.624858: step 13097, loss 0.578922.
Train: 2018-08-05T00:27:38.874824: step 13098, loss 0.562639.
Train: 2018-08-05T00:27:39.124771: step 13099, loss 0.554511.
Train: 2018-08-05T00:27:39.374684: step 13100, loss 0.505727.
Test: 2018-08-05T00:27:40.624389: step 13100, loss 0.547549.
Train: 2018-08-05T00:27:41.436724: step 13101, loss 0.57078.
Train: 2018-08-05T00:27:41.686673: step 13102, loss 0.59521.
Train: 2018-08-05T00:27:41.936584: step 13103, loss 0.570778.
Train: 2018-08-05T00:27:42.186554: step 13104, loss 0.570779.
Train: 2018-08-05T00:27:42.452088: step 13105, loss 0.587056.
Train: 2018-08-05T00:27:42.686439: step 13106, loss 0.595175.
Train: 2018-08-05T00:27:42.936379: step 13107, loss 0.562669.
Train: 2018-08-05T00:27:43.186320: step 13108, loss 0.538359.
Train: 2018-08-05T00:27:43.436262: step 13109, loss 0.538381.
Train: 2018-08-05T00:27:43.686204: step 13110, loss 0.627519.
Test: 2018-08-05T00:27:44.935880: step 13110, loss 0.548635.
Train: 2018-08-05T00:27:45.185852: step 13111, loss 0.538431.
Train: 2018-08-05T00:27:45.435793: step 13112, loss 0.506109.
Train: 2018-08-05T00:27:45.685729: step 13113, loss 0.546525.
Train: 2018-08-05T00:27:45.935646: step 13114, loss 0.578897.
Train: 2018-08-05T00:27:46.185617: step 13115, loss 0.578899.
Train: 2018-08-05T00:27:46.435559: step 13116, loss 0.489739.
Train: 2018-08-05T00:27:46.685471: step 13117, loss 0.497711.
Train: 2018-08-05T00:27:46.935413: step 13118, loss 0.521917.
Train: 2018-08-05T00:27:47.185384: step 13119, loss 0.554425.
Train: 2018-08-05T00:27:47.435325: step 13120, loss 0.464177.
Test: 2018-08-05T00:27:48.685002: step 13120, loss 0.549515.
Train: 2018-08-05T00:27:48.919322: step 13121, loss 0.644913.
Train: 2018-08-05T00:27:49.169297: step 13122, loss 0.504662.
Train: 2018-08-05T00:27:49.419238: step 13123, loss 0.595631.
Train: 2018-08-05T00:27:49.684802: step 13124, loss 0.545826.
Train: 2018-08-05T00:27:49.934749: step 13125, loss 0.479118.
Train: 2018-08-05T00:27:50.184682: step 13126, loss 0.587498.
Train: 2018-08-05T00:27:50.434622: step 13127, loss 0.587552.
Train: 2018-08-05T00:27:50.684565: step 13128, loss 0.495177.
Train: 2018-08-05T00:27:50.934506: step 13129, loss 0.520249.
Train: 2018-08-05T00:27:51.184447: step 13130, loss 0.621526.
Test: 2018-08-05T00:27:52.418503: step 13130, loss 0.548218.
Train: 2018-08-05T00:27:52.668469: step 13131, loss 0.503098.
Train: 2018-08-05T00:27:52.918386: step 13132, loss 0.562349.
Train: 2018-08-05T00:27:53.168328: step 13133, loss 0.57935.
Train: 2018-08-05T00:27:53.418302: step 13134, loss 0.519776.
Train: 2018-08-05T00:27:53.668243: step 13135, loss 0.596453.
Train: 2018-08-05T00:27:53.918185: step 13136, loss 0.519666.
Train: 2018-08-05T00:27:54.121259: step 13137, loss 0.580567.
Train: 2018-08-05T00:27:54.371196: step 13138, loss 0.613638.
Train: 2018-08-05T00:27:54.617000: step 13139, loss 0.579427.
Train: 2018-08-05T00:27:54.866972: step 13140, loss 0.468453.
Test: 2018-08-05T00:27:56.116650: step 13140, loss 0.548075.
Train: 2018-08-05T00:27:56.351001: step 13141, loss 0.596497.
Train: 2018-08-05T00:27:56.600943: step 13142, loss 0.622097.
Train: 2018-08-05T00:27:56.850852: step 13143, loss 0.511203.
Train: 2018-08-05T00:27:57.100824: step 13144, loss 0.62196.
Train: 2018-08-05T00:27:57.350765: step 13145, loss 0.638848.
Train: 2018-08-05T00:27:57.600707: step 13146, loss 0.511527.
Train: 2018-08-05T00:27:57.850649: step 13147, loss 0.579265.
Train: 2018-08-05T00:27:58.100588: step 13148, loss 0.537079.
Train: 2018-08-05T00:27:58.350531: step 13149, loss 0.596035.
Train: 2018-08-05T00:27:58.600444: step 13150, loss 0.562391.
Test: 2018-08-05T00:27:59.850150: step 13150, loss 0.549.
Train: 2018-08-05T00:28:00.100116: step 13151, loss 0.545659.
Train: 2018-08-05T00:28:00.334445: step 13152, loss 0.537348.
Train: 2018-08-05T00:28:00.584353: step 13153, loss 0.562423.
Train: 2018-08-05T00:28:00.834319: step 13154, loss 0.520771.
Train: 2018-08-05T00:28:01.084237: step 13155, loss 0.520795.
Train: 2018-08-05T00:28:01.334179: step 13156, loss 0.545773.
Train: 2018-08-05T00:28:01.584119: step 13157, loss 0.504089.
Train: 2018-08-05T00:28:01.834060: step 13158, loss 0.470604.
Train: 2018-08-05T00:28:02.084003: step 13159, loss 0.554031.
Train: 2018-08-05T00:28:02.333977: step 13160, loss 0.604368.
Test: 2018-08-05T00:28:03.599272: step 13160, loss 0.546983.
Train: 2018-08-05T00:28:03.833626: step 13161, loss 0.663289.
Train: 2018-08-05T00:28:04.083567: step 13162, loss 0.553982.
Train: 2018-08-05T00:28:04.333476: step 13163, loss 0.562388.
Train: 2018-08-05T00:28:04.583449: step 13164, loss 0.537215.
Train: 2018-08-05T00:28:04.833391: step 13165, loss 0.537223.
Train: 2018-08-05T00:28:05.098951: step 13166, loss 0.537218.
Train: 2018-08-05T00:28:05.364514: step 13167, loss 0.545599.
Train: 2018-08-05T00:28:05.598806: step 13168, loss 0.570787.
Train: 2018-08-05T00:28:05.848746: step 13169, loss 0.612802.
Train: 2018-08-05T00:28:06.098688: step 13170, loss 0.537202.
Test: 2018-08-05T00:28:07.364016: step 13170, loss 0.547989.
Train: 2018-08-05T00:28:07.598378: step 13171, loss 0.537215.
Train: 2018-08-05T00:28:07.848278: step 13172, loss 0.528823.
Train: 2018-08-05T00:28:08.098243: step 13173, loss 0.60437.
Train: 2018-08-05T00:28:08.348160: step 13174, loss 0.570783.
Train: 2018-08-05T00:28:08.598102: step 13175, loss 0.61271.
Train: 2018-08-05T00:28:08.848044: step 13176, loss 0.604259.
Train: 2018-08-05T00:28:09.098015: step 13177, loss 0.512321.
Train: 2018-08-05T00:28:09.347926: step 13178, loss 0.637469.
Train: 2018-08-05T00:28:09.597898: step 13179, loss 0.562447.
Train: 2018-08-05T00:28:09.847809: step 13180, loss 0.570758.
Test: 2018-08-05T00:28:11.113137: step 13180, loss 0.547914.
Train: 2018-08-05T00:28:11.363080: step 13181, loss 0.488059.
Train: 2018-08-05T00:28:11.597430: step 13182, loss 0.612066.
Train: 2018-08-05T00:28:11.847372: step 13183, loss 0.471805.
Train: 2018-08-05T00:28:12.097282: step 13184, loss 0.554264.
Train: 2018-08-05T00:28:12.347223: step 13185, loss 0.603748.
Train: 2018-08-05T00:28:12.597166: step 13186, loss 0.554273.
Train: 2018-08-05T00:28:12.862729: step 13187, loss 0.56252.
Train: 2018-08-05T00:28:13.112700: step 13188, loss 0.554292.
Train: 2018-08-05T00:28:13.362641: step 13189, loss 0.570758.
Train: 2018-08-05T00:28:13.643825: step 13190, loss 0.521406.
Test: 2018-08-05T00:28:14.893502: step 13190, loss 0.548788.
Train: 2018-08-05T00:28:15.143443: step 13191, loss 0.578986.
Train: 2018-08-05T00:28:15.393386: step 13192, loss 0.513167.
Train: 2018-08-05T00:28:15.643357: step 13193, loss 0.570757.
Train: 2018-08-05T00:28:15.893269: step 13194, loss 0.587236.
Train: 2018-08-05T00:28:16.143235: step 13195, loss 0.620192.
Train: 2018-08-05T00:28:16.393176: step 13196, loss 0.504926.
Train: 2018-08-05T00:28:16.643123: step 13197, loss 0.603674.
Train: 2018-08-05T00:28:16.908656: step 13198, loss 0.644755.
Train: 2018-08-05T00:28:17.158597: step 13199, loss 0.603567.
Train: 2018-08-05T00:28:17.408540: step 13200, loss 0.587119.
Test: 2018-08-05T00:28:18.658246: step 13200, loss 0.549769.
Train: 2018-08-05T00:28:19.501828: step 13201, loss 0.587073.
Train: 2018-08-05T00:28:19.751770: step 13202, loss 0.570787.
Train: 2018-08-05T00:28:20.001681: step 13203, loss 0.554615.
Train: 2018-08-05T00:28:20.251653: step 13204, loss 0.595023.
Train: 2018-08-05T00:28:20.501565: step 13205, loss 0.57083.
Train: 2018-08-05T00:28:20.751505: step 13206, loss 0.554806.
Train: 2018-08-05T00:28:21.001448: step 13207, loss 0.618868.
Train: 2018-08-05T00:28:21.267037: step 13208, loss 0.538988.
Train: 2018-08-05T00:28:21.501361: step 13209, loss 0.586814.
Train: 2018-08-05T00:28:21.751302: step 13210, loss 0.531247.
Test: 2018-08-05T00:28:23.016600: step 13210, loss 0.549779.
Train: 2018-08-05T00:28:23.250947: step 13211, loss 0.71356.
Train: 2018-08-05T00:28:23.516484: step 13212, loss 0.523647.
Train: 2018-08-05T00:28:23.766449: step 13213, loss 0.476635.
Train: 2018-08-05T00:28:24.032012: step 13214, loss 0.547434.
Train: 2018-08-05T00:28:24.281959: step 13215, loss 0.57887.
Train: 2018-08-05T00:28:24.531870: step 13216, loss 0.508158.
Train: 2018-08-05T00:28:24.781813: step 13217, loss 0.578868.
Train: 2018-08-05T00:28:25.031784: step 13218, loss 0.523753.
Train: 2018-08-05T00:28:25.281725: step 13219, loss 0.507873.
Train: 2018-08-05T00:28:25.547287: step 13220, loss 0.594684.
Test: 2018-08-05T00:28:26.796965: step 13220, loss 0.549577.
Train: 2018-08-05T00:28:27.031316: step 13221, loss 0.586788.
Train: 2018-08-05T00:28:27.281256: step 13222, loss 0.531212.
Train: 2018-08-05T00:28:27.531198: step 13223, loss 0.531107.
Train: 2018-08-05T00:28:27.796763: step 13224, loss 0.538957.
Train: 2018-08-05T00:28:28.046702: step 13225, loss 0.546842.
Train: 2018-08-05T00:28:28.312235: step 13226, loss 0.546744.
Train: 2018-08-05T00:28:28.562176: step 13227, loss 0.546647.
Train: 2018-08-05T00:28:28.812148: step 13228, loss 0.546549.
Train: 2018-08-05T00:28:29.077681: step 13229, loss 0.538339.
Train: 2018-08-05T00:28:29.327655: step 13230, loss 0.595203.
Test: 2018-08-05T00:28:30.592951: step 13230, loss 0.548225.
Train: 2018-08-05T00:28:30.827296: step 13231, loss 0.521796.
Train: 2018-08-05T00:28:31.077214: step 13232, loss 0.56258.
Train: 2018-08-05T00:28:31.342806: step 13233, loss 0.537916.
Train: 2018-08-05T00:28:31.592747: step 13234, loss 0.546053.
Train: 2018-08-05T00:28:31.842660: step 13235, loss 0.53772.
Train: 2018-08-05T00:28:32.092599: step 13236, loss 0.603898.
Train: 2018-08-05T00:28:32.342541: step 13237, loss 0.570759.
Train: 2018-08-05T00:28:32.592513: step 13238, loss 0.545823.
Train: 2018-08-05T00:28:32.842455: step 13239, loss 0.562438.
Train: 2018-08-05T00:28:33.092366: step 13240, loss 0.595772.
Test: 2018-08-05T00:28:34.342073: step 13240, loss 0.548116.
Train: 2018-08-05T00:28:34.592015: step 13241, loss 0.53741.
Train: 2018-08-05T00:28:34.841986: step 13242, loss 0.570767.
Train: 2018-08-05T00:28:35.091898: step 13243, loss 0.579117.
Train: 2018-08-05T00:28:35.341869: step 13244, loss 0.620857.
Train: 2018-08-05T00:28:35.591781: step 13245, loss 0.537417.
Train: 2018-08-05T00:28:35.841755: step 13246, loss 0.512451.
Train: 2018-08-05T00:28:36.091694: step 13247, loss 0.570764.
Train: 2018-08-05T00:28:36.341635: step 13248, loss 0.587426.
Train: 2018-08-05T00:28:36.591547: step 13249, loss 0.562438.
Train: 2018-08-05T00:28:36.841518: step 13250, loss 0.570762.
Test: 2018-08-05T00:28:38.091195: step 13250, loss 0.547037.
Train: 2018-08-05T00:28:38.325540: step 13251, loss 0.57076.
Train: 2018-08-05T00:28:38.575489: step 13252, loss 0.595668.
Train: 2018-08-05T00:28:38.825431: step 13253, loss 0.537603.
Train: 2018-08-05T00:28:39.075340: step 13254, loss 0.562477.
Train: 2018-08-05T00:28:39.325314: step 13255, loss 0.529404.
Train: 2018-08-05T00:28:39.575256: step 13256, loss 0.669967.
Train: 2018-08-05T00:28:39.825197: step 13257, loss 0.554264.
Train: 2018-08-05T00:28:40.075106: step 13258, loss 0.578986.
Train: 2018-08-05T00:28:40.325080: step 13259, loss 0.603596.
Train: 2018-08-05T00:28:40.574988: step 13260, loss 0.529844.
Test: 2018-08-05T00:28:41.809074: step 13260, loss 0.54915.
Train: 2018-08-05T00:28:42.043395: step 13261, loss 0.57077.
Train: 2018-08-05T00:28:42.293367: step 13262, loss 0.554474.
Train: 2018-08-05T00:28:42.543278: step 13263, loss 0.530093.
Train: 2018-08-05T00:28:42.793249: step 13264, loss 0.538259.
Train: 2018-08-05T00:28:43.043190: step 13265, loss 0.546396.
Train: 2018-08-05T00:28:43.293133: step 13266, loss 0.603305.
Train: 2018-08-05T00:28:43.543074: step 13267, loss 0.53016.
Train: 2018-08-05T00:28:43.793015: step 13268, loss 0.595162.
Train: 2018-08-05T00:28:44.042958: step 13269, loss 0.530182.
Train: 2018-08-05T00:28:44.292898: step 13270, loss 0.554542.
Test: 2018-08-05T00:28:45.542575: step 13270, loss 0.547781.
Train: 2018-08-05T00:28:45.808138: step 13271, loss 0.652036.
Train: 2018-08-05T00:28:46.042490: step 13272, loss 0.554566.
Train: 2018-08-05T00:28:46.292430: step 13273, loss 0.587001.
Train: 2018-08-05T00:28:46.542341: step 13274, loss 0.562712.
Train: 2018-08-05T00:28:46.792312: step 13275, loss 0.570808.
Train: 2018-08-05T00:28:47.042255: step 13276, loss 0.490138.
Train: 2018-08-05T00:28:47.292191: step 13277, loss 0.52239.
Train: 2018-08-05T00:28:47.542139: step 13278, loss 0.514244.
Train: 2018-08-05T00:28:47.792049: step 13279, loss 0.530307.
Train: 2018-08-05T00:28:48.041990: step 13280, loss 0.538309.
Test: 2018-08-05T00:28:49.307318: step 13280, loss 0.547909.
Train: 2018-08-05T00:28:49.541640: step 13281, loss 0.562634.
Train: 2018-08-05T00:28:49.807202: step 13282, loss 0.587101.
Train: 2018-08-05T00:28:50.057144: step 13283, loss 0.595307.
Train: 2018-08-05T00:28:50.307114: step 13284, loss 0.529824.
Train: 2018-08-05T00:28:50.557059: step 13285, loss 0.570762.
Train: 2018-08-05T00:28:50.806999: step 13286, loss 0.56255.
Train: 2018-08-05T00:28:51.056939: step 13287, loss 0.513228.
Train: 2018-08-05T00:28:51.260021: step 13288, loss 0.562523.
Train: 2018-08-05T00:28:51.509958: step 13289, loss 0.595501.
Train: 2018-08-05T00:28:51.759899: step 13290, loss 0.620283.
Test: 2018-08-05T00:28:53.009576: step 13290, loss 0.549488.
Train: 2018-08-05T00:28:53.243928: step 13291, loss 0.579006.
Train: 2018-08-05T00:28:53.509490: step 13292, loss 0.587242.
Train: 2018-08-05T00:28:53.759432: step 13293, loss 0.496673.
Train: 2018-08-05T00:28:54.024963: step 13294, loss 0.529595.
Train: 2018-08-05T00:28:54.259315: step 13295, loss 0.562519.
Train: 2018-08-05T00:28:54.509226: step 13296, loss 0.603729.
Train: 2018-08-05T00:28:54.759200: step 13297, loss 0.595478.
Train: 2018-08-05T00:28:55.009139: step 13298, loss 0.603684.
Train: 2018-08-05T00:28:55.259075: step 13299, loss 0.620059.
Train: 2018-08-05T00:28:55.524646: step 13300, loss 0.619922.
Test: 2018-08-05T00:28:56.774320: step 13300, loss 0.548979.
Train: 2018-08-05T00:28:57.602281: step 13301, loss 0.538124.
Train: 2018-08-05T00:28:57.867844: step 13302, loss 0.595194.
Train: 2018-08-05T00:28:58.117786: step 13303, loss 0.570791.
Train: 2018-08-05T00:28:58.367727: step 13304, loss 0.554633.
Train: 2018-08-05T00:28:58.617669: step 13305, loss 0.651461.
Train: 2018-08-05T00:28:58.867610: step 13306, loss 0.554779.
Train: 2018-08-05T00:28:59.117551: step 13307, loss 0.56286.
Train: 2018-08-05T00:28:59.367494: step 13308, loss 0.530999.
Train: 2018-08-05T00:28:59.617436: step 13309, loss 0.578859.
Train: 2018-08-05T00:28:59.867380: step 13310, loss 0.602691.
Test: 2018-08-05T00:29:01.117052: step 13310, loss 0.549779.
Train: 2018-08-05T00:29:01.351398: step 13311, loss 0.578859.
Train: 2018-08-05T00:29:01.601345: step 13312, loss 0.555152.
Train: 2018-08-05T00:29:01.851286: step 13313, loss 0.507879.
Train: 2018-08-05T00:29:02.116818: step 13314, loss 0.523677.
Train: 2018-08-05T00:29:02.366760: step 13315, loss 0.515749.
Train: 2018-08-05T00:29:02.616701: step 13316, loss 0.523536.
Train: 2018-08-05T00:29:02.866674: step 13317, loss 0.56301.
Train: 2018-08-05T00:29:03.116615: step 13318, loss 0.531191.
Train: 2018-08-05T00:29:03.366527: step 13319, loss 0.562921.
Train: 2018-08-05T00:29:03.616498: step 13320, loss 0.538902.
Test: 2018-08-05T00:29:04.866175: step 13320, loss 0.549073.
Train: 2018-08-05T00:29:05.116117: step 13321, loss 0.60292.
Train: 2018-08-05T00:29:05.366089: step 13322, loss 0.643145.
Train: 2018-08-05T00:29:05.616025: step 13323, loss 0.594944.
Train: 2018-08-05T00:29:05.865966: step 13324, loss 0.530676.
Train: 2018-08-05T00:29:06.115884: step 13325, loss 0.586907.
Train: 2018-08-05T00:29:06.365825: step 13326, loss 0.59494.
Train: 2018-08-05T00:29:06.631411: step 13327, loss 0.52267.
Train: 2018-08-05T00:29:06.896979: step 13328, loss 0.530686.
Train: 2018-08-05T00:29:07.131271: step 13329, loss 0.546717.
Train: 2018-08-05T00:29:07.381244: step 13330, loss 0.611074.
Test: 2018-08-05T00:29:08.646540: step 13330, loss 0.549698.
Train: 2018-08-05T00:29:08.896481: step 13331, loss 0.619133.
Train: 2018-08-05T00:29:09.146453: step 13332, loss 0.538656.
Train: 2018-08-05T00:29:09.396414: step 13333, loss 0.538667.
Train: 2018-08-05T00:29:09.646307: step 13334, loss 0.586918.
Train: 2018-08-05T00:29:09.896277: step 13335, loss 0.530617.
Train: 2018-08-05T00:29:10.146189: step 13336, loss 0.611066.
Train: 2018-08-05T00:29:10.396160: step 13337, loss 0.586918.
Train: 2018-08-05T00:29:10.646072: step 13338, loss 0.530647.
Train: 2018-08-05T00:29:10.896043: step 13339, loss 0.522609.
Train: 2018-08-05T00:29:11.145984: step 13340, loss 0.514513.
Test: 2018-08-05T00:29:12.395662: step 13340, loss 0.549667.
Train: 2018-08-05T00:29:12.630012: step 13341, loss 0.546637.
Train: 2018-08-05T00:29:12.879924: step 13342, loss 0.546574.
Train: 2018-08-05T00:29:13.129865: step 13343, loss 0.595089.
Train: 2018-08-05T00:29:13.379837: step 13344, loss 0.530251.
Train: 2018-08-05T00:29:13.629779: step 13345, loss 0.611408.
Train: 2018-08-05T00:29:13.895341: step 13346, loss 0.54639.
Train: 2018-08-05T00:29:14.145284: step 13347, loss 0.5545.
Train: 2018-08-05T00:29:14.410841: step 13348, loss 0.570776.
Train: 2018-08-05T00:29:14.645137: step 13349, loss 0.636018.
Train: 2018-08-05T00:29:14.895107: step 13350, loss 0.587074.
Test: 2018-08-05T00:29:16.144784: step 13350, loss 0.549983.
Train: 2018-08-05T00:29:16.379135: step 13351, loss 0.635894.
Train: 2018-08-05T00:29:16.629079: step 13352, loss 0.611378.
Train: 2018-08-05T00:29:16.879016: step 13353, loss 0.578892.
Train: 2018-08-05T00:29:17.128928: step 13354, loss 0.578881.
Train: 2018-08-05T00:29:17.378871: step 13355, loss 0.554762.
Train: 2018-08-05T00:29:17.628811: step 13356, loss 0.554823.
Train: 2018-08-05T00:29:17.878783: step 13357, loss 0.554873.
Train: 2018-08-05T00:29:18.128725: step 13358, loss 0.570878.
Train: 2018-08-05T00:29:18.378636: step 13359, loss 0.443385.
Train: 2018-08-05T00:29:18.628607: step 13360, loss 0.562902.
Test: 2018-08-05T00:29:19.878285: step 13360, loss 0.549359.
Train: 2018-08-05T00:29:20.112606: step 13361, loss 0.554896.
Train: 2018-08-05T00:29:20.362576: step 13362, loss 0.578863.
Train: 2018-08-05T00:29:20.612521: step 13363, loss 0.530821.
Train: 2018-08-05T00:29:20.862460: step 13364, loss 0.554806.
Train: 2018-08-05T00:29:21.112371: step 13365, loss 0.651181.
Train: 2018-08-05T00:29:21.362343: step 13366, loss 0.546742.
Train: 2018-08-05T00:29:21.612284: step 13367, loss 0.48247.
Train: 2018-08-05T00:29:21.877840: step 13368, loss 0.53863.
Train: 2018-08-05T00:29:22.127757: step 13369, loss 0.619222.
Train: 2018-08-05T00:29:22.377730: step 13370, loss 0.57081.
Test: 2018-08-05T00:29:23.643028: step 13370, loss 0.549788.
Train: 2018-08-05T00:29:23.939866: step 13371, loss 0.627375.
Train: 2018-08-05T00:29:24.189775: step 13372, loss 0.595037.
Train: 2018-08-05T00:29:24.439749: step 13373, loss 0.570816.
Train: 2018-08-05T00:29:24.689691: step 13374, loss 0.611098.
Train: 2018-08-05T00:29:24.955251: step 13375, loss 0.562796.
Train: 2018-08-05T00:29:25.205164: step 13376, loss 0.586891.
Train: 2018-08-05T00:29:25.455103: step 13377, loss 0.514815.
Train: 2018-08-05T00:29:25.705046: step 13378, loss 0.506863.
Train: 2018-08-05T00:29:25.970638: step 13379, loss 0.530835.
Train: 2018-08-05T00:29:26.204960: step 13380, loss 0.538789.
Test: 2018-08-05T00:29:27.454635: step 13380, loss 0.549584.
Train: 2018-08-05T00:29:27.688957: step 13381, loss 0.56281.
Train: 2018-08-05T00:29:27.938898: step 13382, loss 0.611048.
Train: 2018-08-05T00:29:28.188871: step 13383, loss 0.627162.
Train: 2018-08-05T00:29:28.438779: step 13384, loss 0.594955.
Train: 2018-08-05T00:29:28.688722: step 13385, loss 0.59493.
Train: 2018-08-05T00:29:28.938663: step 13386, loss 0.506731.
Train: 2018-08-05T00:29:29.188605: step 13387, loss 0.498746.
Train: 2018-08-05T00:29:29.438545: step 13388, loss 0.562826.
Train: 2018-08-05T00:29:29.688487: step 13389, loss 0.546751.
Train: 2018-08-05T00:29:29.938462: step 13390, loss 0.498465.
Test: 2018-08-05T00:29:31.203757: step 13390, loss 0.548362.
Train: 2018-08-05T00:29:31.438111: step 13391, loss 0.490191.
Train: 2018-08-05T00:29:31.688044: step 13392, loss 0.51413.
Train: 2018-08-05T00:29:31.937960: step 13393, loss 0.587057.
Train: 2018-08-05T00:29:32.187926: step 13394, loss 0.448304.
Train: 2018-08-05T00:29:32.437876: step 13395, loss 0.546148.
Train: 2018-08-05T00:29:32.687818: step 13396, loss 0.579069.
Train: 2018-08-05T00:29:32.937727: step 13397, loss 0.529302.
Train: 2018-08-05T00:29:33.187701: step 13398, loss 0.462463.
Train: 2018-08-05T00:29:33.437609: step 13399, loss 0.553995.
Train: 2018-08-05T00:29:33.687584: step 13400, loss 0.579202.
Test: 2018-08-05T00:29:34.952879: step 13400, loss 0.548197.
Train: 2018-08-05T00:29:35.796431: step 13401, loss 0.562352.
Train: 2018-08-05T00:29:36.046374: step 13402, loss 0.570992.
Train: 2018-08-05T00:29:36.296345: step 13403, loss 0.528141.
Train: 2018-08-05T00:29:36.546287: step 13404, loss 0.622073.
Train: 2018-08-05T00:29:36.796199: step 13405, loss 0.630953.
Train: 2018-08-05T00:29:37.046140: step 13406, loss 0.631019.
Train: 2018-08-05T00:29:37.296082: step 13407, loss 0.545248.
Train: 2018-08-05T00:29:37.561676: step 13408, loss 0.502663.
Train: 2018-08-05T00:29:37.811618: step 13409, loss 0.596432.
Train: 2018-08-05T00:29:38.061560: step 13410, loss 0.579372.
Test: 2018-08-05T00:29:39.311233: step 13410, loss 0.546776.
Train: 2018-08-05T00:29:39.545584: step 13411, loss 0.545369.
Train: 2018-08-05T00:29:39.795525: step 13412, loss 0.502875.
Train: 2018-08-05T00:29:40.061058: step 13413, loss 0.5709.
Train: 2018-08-05T00:29:40.311000: step 13414, loss 0.613293.
Train: 2018-08-05T00:29:40.560974: step 13415, loss 0.536927.
Train: 2018-08-05T00:29:40.810882: step 13416, loss 0.536956.
Train: 2018-08-05T00:29:41.060857: step 13417, loss 0.553922.
Train: 2018-08-05T00:29:41.310798: step 13418, loss 0.579262.
Train: 2018-08-05T00:29:41.560708: step 13419, loss 0.545463.
Train: 2018-08-05T00:29:41.810648: step 13420, loss 0.562354.
Test: 2018-08-05T00:29:43.075976: step 13420, loss 0.548878.
Train: 2018-08-05T00:29:43.325949: step 13421, loss 0.56237.
Train: 2018-08-05T00:29:43.575891: step 13422, loss 0.612884.
Train: 2018-08-05T00:29:43.825832: step 13423, loss 0.503553.
Train: 2018-08-05T00:29:44.075742: step 13424, loss 0.520406.
Train: 2018-08-05T00:29:44.325714: step 13425, loss 0.587576.
Train: 2018-08-05T00:29:44.575657: step 13426, loss 0.520448.
Train: 2018-08-05T00:29:44.825568: step 13427, loss 0.553999.
Train: 2018-08-05T00:29:45.075539: step 13428, loss 0.478464.
Train: 2018-08-05T00:29:45.341097: step 13429, loss 0.562383.
Train: 2018-08-05T00:29:45.606663: step 13430, loss 0.520287.
Test: 2018-08-05T00:29:46.856341: step 13430, loss 0.547507.
Train: 2018-08-05T00:29:47.090693: step 13431, loss 0.562369.
Train: 2018-08-05T00:29:47.340605: step 13432, loss 0.51168.
Train: 2018-08-05T00:29:47.590546: step 13433, loss 0.545424.
Train: 2018-08-05T00:29:47.840487: step 13434, loss 0.52841.
Train: 2018-08-05T00:29:48.090461: step 13435, loss 0.553844.
Train: 2018-08-05T00:29:48.340400: step 13436, loss 0.647573.
Train: 2018-08-05T00:29:48.590312: step 13437, loss 0.49415.
Train: 2018-08-05T00:29:48.855873: step 13438, loss 0.630615.
Train: 2018-08-05T00:29:49.043330: step 13439, loss 0.562339.
Train: 2018-08-05T00:29:49.293302: step 13440, loss 0.51971.
Test: 2018-08-05T00:29:50.527357: step 13440, loss 0.547904.
Train: 2018-08-05T00:29:50.777298: step 13441, loss 0.545288.
Train: 2018-08-05T00:29:51.027270: step 13442, loss 0.630567.
Train: 2018-08-05T00:29:51.277220: step 13443, loss 0.570859.
Train: 2018-08-05T00:29:51.527153: step 13444, loss 0.494315.
Train: 2018-08-05T00:29:51.777095: step 13445, loss 0.613353.
Train: 2018-08-05T00:29:52.027037: step 13446, loss 0.502923.
Train: 2018-08-05T00:29:52.276947: step 13447, loss 0.579323.
Train: 2018-08-05T00:29:52.526890: step 13448, loss 0.528431.
Train: 2018-08-05T00:29:52.776860: step 13449, loss 0.545395.
Train: 2018-08-05T00:29:53.026772: step 13450, loss 0.52844.
Test: 2018-08-05T00:29:54.276479: step 13450, loss 0.547015.
Train: 2018-08-05T00:29:54.510830: step 13451, loss 0.579315.
Train: 2018-08-05T00:29:54.760771: step 13452, loss 0.613235.
Train: 2018-08-05T00:29:55.010683: step 13453, loss 0.613174.
Train: 2018-08-05T00:29:55.260654: step 13454, loss 0.562361.
Train: 2018-08-05T00:29:55.526217: step 13455, loss 0.46962.
Train: 2018-08-05T00:29:55.776129: step 13456, loss 0.612945.
Train: 2018-08-05T00:29:56.041691: step 13457, loss 0.537127.
Train: 2018-08-05T00:29:56.276013: step 13458, loss 0.545564.
Train: 2018-08-05T00:29:56.525979: step 13459, loss 0.562385.
Train: 2018-08-05T00:29:56.775926: step 13460, loss 0.537198.
Test: 2018-08-05T00:29:58.025601: step 13460, loss 0.549143.
Train: 2018-08-05T00:29:58.259923: step 13461, loss 0.612756.
Train: 2018-08-05T00:29:58.525516: step 13462, loss 0.562397.
Train: 2018-08-05T00:29:58.775456: step 13463, loss 0.637744.
Train: 2018-08-05T00:29:59.025397: step 13464, loss 0.562421.
Train: 2018-08-05T00:29:59.275339: step 13465, loss 0.520813.
Train: 2018-08-05T00:29:59.525281: step 13466, loss 0.554138.
Train: 2018-08-05T00:29:59.775193: step 13467, loss 0.603955.
Train: 2018-08-05T00:30:00.025160: step 13468, loss 0.554195.
Train: 2018-08-05T00:30:00.275105: step 13469, loss 0.587286.
Train: 2018-08-05T00:30:00.525017: step 13470, loss 0.463553.
Test: 2018-08-05T00:30:01.774724: step 13470, loss 0.54798.
Train: 2018-08-05T00:30:02.024689: step 13471, loss 0.587248.
Train: 2018-08-05T00:30:02.274636: step 13472, loss 0.521313.
Train: 2018-08-05T00:30:02.524578: step 13473, loss 0.54603.
Train: 2018-08-05T00:30:02.774490: step 13474, loss 0.611988.
Train: 2018-08-05T00:30:03.024461: step 13475, loss 0.562517.
Train: 2018-08-05T00:30:03.274405: step 13476, loss 0.587232.
Train: 2018-08-05T00:30:03.539935: step 13477, loss 0.611894.
Train: 2018-08-05T00:30:03.774288: step 13478, loss 0.496865.
Train: 2018-08-05T00:30:04.024197: step 13479, loss 0.570761.
Train: 2018-08-05T00:30:04.274139: step 13480, loss 0.611765.
Test: 2018-08-05T00:30:05.523845: step 13480, loss 0.550412.
Train: 2018-08-05T00:30:05.758166: step 13481, loss 0.546203.
Train: 2018-08-05T00:30:06.008138: step 13482, loss 0.570767.
Train: 2018-08-05T00:30:06.258079: step 13483, loss 0.529931.
Train: 2018-08-05T00:30:06.508015: step 13484, loss 0.619756.
Train: 2018-08-05T00:30:06.757956: step 13485, loss 0.497413.
Train: 2018-08-05T00:30:07.007874: step 13486, loss 0.562624.
Train: 2018-08-05T00:30:07.257845: step 13487, loss 0.530024.
Train: 2018-08-05T00:30:07.507757: step 13488, loss 0.562618.
Train: 2018-08-05T00:30:07.757698: step 13489, loss 0.562612.
Train: 2018-08-05T00:30:08.007639: step 13490, loss 0.595263.
Test: 2018-08-05T00:30:09.272967: step 13490, loss 0.548602.
Train: 2018-08-05T00:30:09.507289: step 13491, loss 0.529961.
Train: 2018-08-05T00:30:09.757260: step 13492, loss 0.538106.
Train: 2018-08-05T00:30:10.007170: step 13493, loss 0.595289.
Train: 2018-08-05T00:30:10.257143: step 13494, loss 0.603466.
Train: 2018-08-05T00:30:10.522705: step 13495, loss 0.562601.
Train: 2018-08-05T00:30:10.772647: step 13496, loss 0.521792.
Train: 2018-08-05T00:30:11.022559: step 13497, loss 0.505449.
Train: 2018-08-05T00:30:11.272500: step 13498, loss 0.529885.
Train: 2018-08-05T00:30:11.522471: step 13499, loss 0.537994.
Train: 2018-08-05T00:30:11.772414: step 13500, loss 0.488656.
Test: 2018-08-05T00:30:13.053332: step 13500, loss 0.548948.
Train: 2018-08-05T00:30:13.975021: step 13501, loss 0.611952.
Train: 2018-08-05T00:30:14.256206: step 13502, loss 0.554244.
Train: 2018-08-05T00:30:14.506148: step 13503, loss 0.587301.
Train: 2018-08-05T00:30:14.771688: step 13504, loss 0.595603.
Train: 2018-08-05T00:30:15.006001: step 13505, loss 0.637035.
Train: 2018-08-05T00:30:15.255973: step 13506, loss 0.53766.
Train: 2018-08-05T00:30:15.505913: step 13507, loss 0.529415.
Train: 2018-08-05T00:30:15.755855: step 13508, loss 0.645167.
Train: 2018-08-05T00:30:16.021388: step 13509, loss 0.645036.
Train: 2018-08-05T00:30:16.255739: step 13510, loss 0.513177.
Test: 2018-08-05T00:30:17.505415: step 13510, loss 0.548275.
Train: 2018-08-05T00:30:17.739735: step 13511, loss 0.554342.
Train: 2018-08-05T00:30:17.989706: step 13512, loss 0.546177.
Train: 2018-08-05T00:30:18.239648: step 13513, loss 0.677173.
Train: 2018-08-05T00:30:18.489590: step 13514, loss 0.595244.
Train: 2018-08-05T00:30:18.739502: step 13515, loss 0.570785.
Train: 2018-08-05T00:30:18.989475: step 13516, loss 0.578895.
Train: 2018-08-05T00:30:19.239414: step 13517, loss 0.562747.
Train: 2018-08-05T00:30:19.489355: step 13518, loss 0.586916.
Train: 2018-08-05T00:30:19.739297: step 13519, loss 0.618945.
Train: 2018-08-05T00:30:19.989238: step 13520, loss 0.562895.
Test: 2018-08-05T00:30:21.238915: step 13520, loss 0.548935.
Train: 2018-08-05T00:30:21.473266: step 13521, loss 0.531136.
Train: 2018-08-05T00:30:21.723208: step 13522, loss 0.594727.
Train: 2018-08-05T00:30:21.973148: step 13523, loss 0.563036.
Train: 2018-08-05T00:30:22.223061: step 13524, loss 0.602542.
Train: 2018-08-05T00:30:22.473031: step 13525, loss 0.539514.
Train: 2018-08-05T00:30:22.722944: step 13526, loss 0.539593.
Train: 2018-08-05T00:30:22.972885: step 13527, loss 0.563178.
Train: 2018-08-05T00:30:23.222856: step 13528, loss 0.563191.
Train: 2018-08-05T00:30:23.472768: step 13529, loss 0.524008.
Train: 2018-08-05T00:30:23.722739: step 13530, loss 0.571031.
Test: 2018-08-05T00:30:24.988038: step 13530, loss 0.550612.
Train: 2018-08-05T00:30:25.222358: step 13531, loss 0.555329.
Train: 2018-08-05T00:30:25.472329: step 13532, loss 0.618146.
Train: 2018-08-05T00:30:25.722240: step 13533, loss 0.54746.
Train: 2018-08-05T00:30:25.972213: step 13534, loss 0.594582.
Train: 2018-08-05T00:30:26.222154: step 13535, loss 0.523899.
Train: 2018-08-05T00:30:26.472096: step 13536, loss 0.531712.
Train: 2018-08-05T00:30:26.722037: step 13537, loss 0.563122.
Train: 2018-08-05T00:30:26.971978: step 13538, loss 0.523668.
Train: 2018-08-05T00:30:27.221920: step 13539, loss 0.515622.
Train: 2018-08-05T00:30:27.471832: step 13540, loss 0.658181.
Test: 2018-08-05T00:30:28.721538: step 13540, loss 0.549532.
Train: 2018-08-05T00:30:28.955890: step 13541, loss 0.555036.
Train: 2018-08-05T00:30:29.205825: step 13542, loss 0.562956.
Train: 2018-08-05T00:30:29.455742: step 13543, loss 0.570897.
Train: 2018-08-05T00:30:29.705682: step 13544, loss 0.562919.
Train: 2018-08-05T00:30:29.955624: step 13545, loss 0.507051.
Train: 2018-08-05T00:30:30.205567: step 13546, loss 0.506889.
Train: 2018-08-05T00:30:30.455507: step 13547, loss 0.538749.
Train: 2018-08-05T00:30:30.705449: step 13548, loss 0.538613.
Train: 2018-08-05T00:30:30.955420: step 13549, loss 0.595056.
Train: 2018-08-05T00:30:31.205331: step 13550, loss 0.489734.
Test: 2018-08-05T00:30:32.455039: step 13550, loss 0.549798.
Train: 2018-08-05T00:30:32.704982: step 13551, loss 0.578918.
Train: 2018-08-05T00:30:32.954952: step 13552, loss 0.603433.
Train: 2018-08-05T00:30:33.204865: step 13553, loss 0.529852.
Train: 2018-08-05T00:30:33.454835: step 13554, loss 0.472317.
Train: 2018-08-05T00:30:33.704777: step 13555, loss 0.570757.
Train: 2018-08-05T00:30:33.954713: step 13556, loss 0.620353.
Train: 2018-08-05T00:30:34.204630: step 13557, loss 0.446533.
Train: 2018-08-05T00:30:34.454572: step 13558, loss 0.495933.
Train: 2018-08-05T00:30:34.704542: step 13559, loss 0.495585.
Train: 2018-08-05T00:30:34.954485: step 13560, loss 0.587586.
Test: 2018-08-05T00:30:36.204161: step 13560, loss 0.547504.
Train: 2018-08-05T00:30:36.438483: step 13561, loss 0.503319.
Train: 2018-08-05T00:30:36.688453: step 13562, loss 0.553876.
Train: 2018-08-05T00:30:36.938394: step 13563, loss 0.502753.
Train: 2018-08-05T00:30:37.188335: step 13564, loss 0.63932.
Train: 2018-08-05T00:30:37.438272: step 13565, loss 0.648099.
Train: 2018-08-05T00:30:37.688219: step 13566, loss 0.536592.
Train: 2018-08-05T00:30:37.938160: step 13567, loss 0.51081.
Train: 2018-08-05T00:30:38.188071: step 13568, loss 0.613932.
Train: 2018-08-05T00:30:38.438043: step 13569, loss 0.562335.
Train: 2018-08-05T00:30:38.687985: step 13570, loss 0.510743.
Test: 2018-08-05T00:30:39.937662: step 13570, loss 0.550343.
Train: 2018-08-05T00:30:40.172015: step 13571, loss 0.553732.
Train: 2018-08-05T00:30:40.421923: step 13572, loss 0.553728.
Train: 2018-08-05T00:30:40.671864: step 13573, loss 0.545116.
Train: 2018-08-05T00:30:40.921806: step 13574, loss 0.614016.
Train: 2018-08-05T00:30:41.187393: step 13575, loss 0.605365.
Train: 2018-08-05T00:30:41.421690: step 13576, loss 0.570925.
Train: 2018-08-05T00:30:41.671632: step 13577, loss 0.528044.
Train: 2018-08-05T00:30:41.921605: step 13578, loss 0.60514.
Train: 2018-08-05T00:30:42.171547: step 13579, loss 0.613585.
Train: 2018-08-05T00:30:42.437106: step 13580, loss 0.528288.
Test: 2018-08-05T00:30:43.686784: step 13580, loss 0.546991.
Train: 2018-08-05T00:30:43.936755: step 13581, loss 0.587826.
Train: 2018-08-05T00:30:44.186668: step 13582, loss 0.545419.
Train: 2018-08-05T00:30:44.436608: step 13583, loss 0.604601.
Train: 2018-08-05T00:30:44.702170: step 13584, loss 0.545534.
Train: 2018-08-05T00:30:44.936522: step 13585, loss 0.495204.
Train: 2018-08-05T00:30:45.186432: step 13586, loss 0.579169.
Train: 2018-08-05T00:30:45.436374: step 13587, loss 0.587525.
Train: 2018-08-05T00:30:45.686346: step 13588, loss 0.570771.
Train: 2018-08-05T00:30:45.936290: step 13589, loss 0.604124.
Train: 2018-08-05T00:30:46.123738: step 13590, loss 0.580186.
Test: 2018-08-05T00:30:47.357799: step 13590, loss 0.547284.
Train: 2018-08-05T00:30:47.607771: step 13591, loss 0.545884.
Train: 2018-08-05T00:30:47.857712: step 13592, loss 0.537673.
Train: 2018-08-05T00:30:48.107654: step 13593, loss 0.678085.
Train: 2018-08-05T00:30:48.357595: step 13594, loss 0.537866.
Train: 2018-08-05T00:30:48.607506: step 13595, loss 0.57896.
Train: 2018-08-05T00:30:48.857478: step 13596, loss 0.562597.
Train: 2018-08-05T00:30:49.107389: step 13597, loss 0.595219.
Train: 2018-08-05T00:30:49.357361: step 13598, loss 0.570787.
Train: 2018-08-05T00:30:49.607303: step 13599, loss 0.489842.
Train: 2018-08-05T00:30:49.857244: step 13600, loss 0.50612.
Test: 2018-08-05T00:30:51.106920: step 13600, loss 0.550143.
Train: 2018-08-05T00:30:51.997367: step 13601, loss 0.546545.
Train: 2018-08-05T00:30:52.247310: step 13602, loss 0.635523.
Train: 2018-08-05T00:30:52.497221: step 13603, loss 0.578888.
Train: 2018-08-05T00:30:52.762783: step 13604, loss 0.578884.
Train: 2018-08-05T00:30:52.997135: step 13605, loss 0.54664.
Train: 2018-08-05T00:30:53.247075: step 13606, loss 0.562772.
Train: 2018-08-05T00:30:53.497020: step 13607, loss 0.586921.
Train: 2018-08-05T00:30:53.746929: step 13608, loss 0.546722.
Train: 2018-08-05T00:30:53.996899: step 13609, loss 0.578871.
Train: 2018-08-05T00:30:54.246842: step 13610, loss 0.578869.
Test: 2018-08-05T00:30:55.491906: step 13610, loss 0.54981.
Train: 2018-08-05T00:30:55.741848: step 13611, loss 0.594903.
Train: 2018-08-05T00:30:55.991818: step 13612, loss 0.594876.
Train: 2018-08-05T00:30:56.257386: step 13613, loss 0.546902.
Train: 2018-08-05T00:30:56.507326: step 13614, loss 0.546944.
Train: 2018-08-05T00:30:56.757234: step 13615, loss 0.57886.
Train: 2018-08-05T00:30:57.022796: step 13616, loss 0.570893.
Train: 2018-08-05T00:30:57.272769: step 13617, loss 0.586817.
Train: 2018-08-05T00:30:57.522681: step 13618, loss 0.547063.
Train: 2018-08-05T00:30:57.772651: step 13619, loss 0.547083.
Train: 2018-08-05T00:30:58.038216: step 13620, loss 0.555029.
Test: 2018-08-05T00:30:59.287891: step 13620, loss 0.549335.
Train: 2018-08-05T00:30:59.522243: step 13621, loss 0.531191.
Train: 2018-08-05T00:30:59.772154: step 13622, loss 0.531143.
Train: 2018-08-05T00:31:00.022099: step 13623, loss 0.507163.
Train: 2018-08-05T00:31:00.272066: step 13624, loss 0.578862.
Train: 2018-08-05T00:31:00.522009: step 13625, loss 0.634917.
Train: 2018-08-05T00:31:00.771951: step 13626, loss 0.602903.
Train: 2018-08-05T00:31:01.021891: step 13627, loss 0.538812.
Train: 2018-08-05T00:31:01.271832: step 13628, loss 0.546811.
Train: 2018-08-05T00:31:01.521773: step 13629, loss 0.578867.
Train: 2018-08-05T00:31:01.787336: step 13630, loss 0.458516.
Test: 2018-08-05T00:31:03.037013: step 13630, loss 0.550089.
Train: 2018-08-05T00:31:03.271335: step 13631, loss 0.627145.
Train: 2018-08-05T00:31:03.521305: step 13632, loss 0.578877.
Train: 2018-08-05T00:31:03.771247: step 13633, loss 0.578879.
Train: 2018-08-05T00:31:04.021188: step 13634, loss 0.530505.
Train: 2018-08-05T00:31:04.271100: step 13635, loss 0.506244.
Train: 2018-08-05T00:31:04.521041: step 13636, loss 0.530359.
Train: 2018-08-05T00:31:04.770982: step 13637, loss 0.684339.
Train: 2018-08-05T00:31:05.020954: step 13638, loss 0.497806.
Train: 2018-08-05T00:31:05.270866: step 13639, loss 0.587027.
Train: 2018-08-05T00:31:05.552082: step 13640, loss 0.595161.
Test: 2018-08-05T00:31:06.801757: step 13640, loss 0.547783.
Train: 2018-08-05T00:31:07.051731: step 13641, loss 0.570785.
Train: 2018-08-05T00:31:07.301672: step 13642, loss 0.570786.
Train: 2018-08-05T00:31:07.551614: step 13643, loss 0.489585.
Train: 2018-08-05T00:31:07.801555: step 13644, loss 0.497611.
Train: 2018-08-05T00:31:08.051497: step 13645, loss 0.603377.
Train: 2018-08-05T00:31:08.317028: step 13646, loss 0.497318.
Train: 2018-08-05T00:31:08.566998: step 13647, loss 0.578948.
Train: 2018-08-05T00:31:08.816909: step 13648, loss 0.546166.
Train: 2018-08-05T00:31:09.066881: step 13649, loss 0.587191.
Train: 2018-08-05T00:31:09.316794: step 13650, loss 0.595438.
Test: 2018-08-05T00:31:10.550879: step 13650, loss 0.549918.
Train: 2018-08-05T00:31:10.800821: step 13651, loss 0.546066.
Train: 2018-08-05T00:31:11.050794: step 13652, loss 0.570757.
Train: 2018-08-05T00:31:11.300736: step 13653, loss 0.587237.
Train: 2018-08-05T00:31:11.550670: step 13654, loss 0.562518.
Train: 2018-08-05T00:31:11.800619: step 13655, loss 0.578995.
Train: 2018-08-05T00:31:12.050560: step 13656, loss 0.546059.
Train: 2018-08-05T00:31:12.300502: step 13657, loss 0.554295.
Train: 2018-08-05T00:31:12.550410: step 13658, loss 0.455526.
Train: 2018-08-05T00:31:12.800384: step 13659, loss 0.562509.
Train: 2018-08-05T00:31:13.050326: step 13660, loss 0.529442.
Test: 2018-08-05T00:31:14.315622: step 13660, loss 0.548645.
Train: 2018-08-05T00:31:14.549975: step 13661, loss 0.504506.
Train: 2018-08-05T00:31:14.799885: step 13662, loss 0.612295.
Train: 2018-08-05T00:31:15.049826: step 13663, loss 0.529156.
Train: 2018-08-05T00:31:15.299767: step 13664, loss 0.54575.
Train: 2018-08-05T00:31:15.549709: step 13665, loss 0.528989.
Train: 2018-08-05T00:31:15.799682: step 13666, loss 0.570777.
Train: 2018-08-05T00:31:16.049591: step 13667, loss 0.62114.
Train: 2018-08-05T00:31:16.299565: step 13668, loss 0.545595.
Train: 2018-08-05T00:31:16.549474: step 13669, loss 0.545583.
Train: 2018-08-05T00:31:16.799448: step 13670, loss 0.562382.
Test: 2018-08-05T00:31:18.049123: step 13670, loss 0.549493.
Train: 2018-08-05T00:31:18.299064: step 13671, loss 0.537144.
Train: 2018-08-05T00:31:18.549039: step 13672, loss 0.570795.
Train: 2018-08-05T00:31:18.798980: step 13673, loss 0.503417.
Train: 2018-08-05T00:31:19.048914: step 13674, loss 0.60454.
Train: 2018-08-05T00:31:19.298863: step 13675, loss 0.511748.
Train: 2018-08-05T00:31:19.548804: step 13676, loss 0.545473.
Train: 2018-08-05T00:31:19.798745: step 13677, loss 0.655369.
Train: 2018-08-05T00:31:20.048688: step 13678, loss 0.596153.
Train: 2018-08-05T00:31:20.298597: step 13679, loss 0.579237.
Train: 2018-08-05T00:31:20.548537: step 13680, loss 0.570794.
Test: 2018-08-05T00:31:21.813866: step 13680, loss 0.549325.
Train: 2018-08-05T00:31:22.063808: step 13681, loss 0.579186.
Train: 2018-08-05T00:31:22.329370: step 13682, loss 0.587539.
Train: 2018-08-05T00:31:22.579312: step 13683, loss 0.537341.
Train: 2018-08-05T00:31:22.829283: step 13684, loss 0.587446.
Train: 2018-08-05T00:31:23.079226: step 13685, loss 0.545803.
Train: 2018-08-05T00:31:23.329161: step 13686, loss 0.537547.
Train: 2018-08-05T00:31:23.594729: step 13687, loss 0.653674.
Train: 2018-08-05T00:31:23.829051: step 13688, loss 0.512897.
Train: 2018-08-05T00:31:24.078985: step 13689, loss 0.645006.
Train: 2018-08-05T00:31:24.328933: step 13690, loss 0.546093.
Test: 2018-08-05T00:31:25.578610: step 13690, loss 0.548493.
Train: 2018-08-05T00:31:25.906688: step 13691, loss 0.529768.
Train: 2018-08-05T00:31:26.156629: step 13692, loss 0.464382.
Train: 2018-08-05T00:31:26.406574: step 13693, loss 0.554393.
Train: 2018-08-05T00:31:26.656513: step 13694, loss 0.587143.
Train: 2018-08-05T00:31:26.922077: step 13695, loss 0.595329.
Train: 2018-08-05T00:31:27.171987: step 13696, loss 0.628037.
Train: 2018-08-05T00:31:27.437575: step 13697, loss 0.529949.
Train: 2018-08-05T00:31:27.687522: step 13698, loss 0.481075.
Train: 2018-08-05T00:31:27.937433: step 13699, loss 0.660523.
Train: 2018-08-05T00:31:28.187373: step 13700, loss 0.611512.
Test: 2018-08-05T00:31:29.437081: step 13700, loss 0.548146.
Train: 2018-08-05T00:31:30.265013: step 13701, loss 0.530144.
Train: 2018-08-05T00:31:30.514984: step 13702, loss 0.570789.
Train: 2018-08-05T00:31:30.764895: step 13703, loss 0.603213.
Train: 2018-08-05T00:31:31.014867: step 13704, loss 0.546541.
Train: 2018-08-05T00:31:31.264809: step 13705, loss 0.546585.
Train: 2018-08-05T00:31:31.514745: step 13706, loss 0.611151.
Train: 2018-08-05T00:31:31.764692: step 13707, loss 0.570825.
Train: 2018-08-05T00:31:32.014633: step 13708, loss 0.506534.
Train: 2018-08-05T00:31:32.264575: step 13709, loss 0.514594.
Train: 2018-08-05T00:31:32.514519: step 13710, loss 0.562791.
Test: 2018-08-05T00:31:33.764193: step 13710, loss 0.548787.
Train: 2018-08-05T00:31:33.998513: step 13711, loss 0.611063.
Train: 2018-08-05T00:31:34.248484: step 13712, loss 0.586919.
Train: 2018-08-05T00:31:34.498426: step 13713, loss 0.578873.
Train: 2018-08-05T00:31:34.748367: step 13714, loss 0.610997.
Train: 2018-08-05T00:31:34.998309: step 13715, loss 0.546797.
Train: 2018-08-05T00:31:35.248251: step 13716, loss 0.530817.
Train: 2018-08-05T00:31:35.498192: step 13717, loss 0.578864.
Train: 2018-08-05T00:31:35.748127: step 13718, loss 0.594866.
Train: 2018-08-05T00:31:35.998046: step 13719, loss 0.530905.
Train: 2018-08-05T00:31:36.248016: step 13720, loss 0.586853.
Test: 2018-08-05T00:31:37.497693: step 13720, loss 0.550285.
Train: 2018-08-05T00:31:37.747636: step 13721, loss 0.538928.
Train: 2018-08-05T00:31:37.997576: step 13722, loss 0.530938.
Train: 2018-08-05T00:31:38.247549: step 13723, loss 0.55488.
Train: 2018-08-05T00:31:38.497459: step 13724, loss 0.482835.
Train: 2018-08-05T00:31:38.747432: step 13725, loss 0.546772.
Train: 2018-08-05T00:31:38.997343: step 13726, loss 0.562781.
Train: 2018-08-05T00:31:39.247314: step 13727, loss 0.530473.
Train: 2018-08-05T00:31:39.497255: step 13728, loss 0.54652.
Train: 2018-08-05T00:31:39.747197: step 13729, loss 0.562669.
Train: 2018-08-05T00:31:40.012755: step 13730, loss 0.530069.
Test: 2018-08-05T00:31:41.246816: step 13730, loss 0.547456.
Train: 2018-08-05T00:31:41.496788: step 13731, loss 0.611611.
Train: 2018-08-05T00:31:41.746729: step 13732, loss 0.587134.
Train: 2018-08-05T00:31:41.996670: step 13733, loss 0.48883.
Train: 2018-08-05T00:31:42.246609: step 13734, loss 0.521475.
Train: 2018-08-05T00:31:42.496524: step 13735, loss 0.546038.
Train: 2018-08-05T00:31:42.746495: step 13736, loss 0.537698.
Train: 2018-08-05T00:31:42.996436: step 13737, loss 0.58734.
Train: 2018-08-05T00:31:43.246372: step 13738, loss 0.595689.
Train: 2018-08-05T00:31:43.496322: step 13739, loss 0.595724.
Train: 2018-08-05T00:31:43.746261: step 13740, loss 0.579087.
Test: 2018-08-05T00:31:44.995938: step 13740, loss 0.549879.
Train: 2018-08-05T00:31:45.183393: step 13741, loss 0.580196.
Train: 2018-08-05T00:31:45.433336: step 13742, loss 0.529164.
Train: 2018-08-05T00:31:45.698899: step 13743, loss 0.554121.
Train: 2018-08-05T00:31:45.933249: step 13744, loss 0.570762.
Train: 2018-08-05T00:31:46.183192: step 13745, loss 0.56244.
Train: 2018-08-05T00:31:46.448752: step 13746, loss 0.570762.
Train: 2018-08-05T00:31:46.698664: step 13747, loss 0.537494.
Train: 2018-08-05T00:31:46.964228: step 13748, loss 0.595713.
Train: 2018-08-05T00:31:47.214169: step 13749, loss 0.562449.
Train: 2018-08-05T00:31:47.479756: step 13750, loss 0.570759.
Test: 2018-08-05T00:31:48.729438: step 13750, loss 0.547839.
Train: 2018-08-05T00:31:48.963784: step 13751, loss 0.545866.
Train: 2018-08-05T00:31:49.213730: step 13752, loss 0.545879.
Train: 2018-08-05T00:31:49.463641: step 13753, loss 0.570758.
Train: 2018-08-05T00:31:49.713613: step 13754, loss 0.554183.
Train: 2018-08-05T00:31:49.963524: step 13755, loss 0.529336.
Train: 2018-08-05T00:31:50.213467: step 13756, loss 0.620477.
Train: 2018-08-05T00:31:50.463438: step 13757, loss 0.545921.
Train: 2018-08-05T00:31:50.713373: step 13758, loss 0.57903.
Train: 2018-08-05T00:31:50.963291: step 13759, loss 0.612085.
Train: 2018-08-05T00:31:51.213262: step 13760, loss 0.504758.
Test: 2018-08-05T00:31:52.462939: step 13760, loss 0.548173.
Train: 2018-08-05T00:31:52.712882: step 13761, loss 0.537778.
Train: 2018-08-05T00:31:52.978477: step 13762, loss 0.579001.
Train: 2018-08-05T00:31:53.212794: step 13763, loss 0.570757.
Train: 2018-08-05T00:31:53.462735: step 13764, loss 0.587227.
Train: 2018-08-05T00:31:53.712676: step 13765, loss 0.529631.
Train: 2018-08-05T00:31:53.962618: step 13766, loss 0.521425.
Train: 2018-08-05T00:31:54.212559: step 13767, loss 0.661245.
Train: 2018-08-05T00:31:54.462501: step 13768, loss 0.644672.
Train: 2018-08-05T00:31:54.712443: step 13769, loss 0.546206.
Train: 2018-08-05T00:31:54.978005: step 13770, loss 0.562605.
Test: 2018-08-05T00:31:56.212061: step 13770, loss 0.549213.
Train: 2018-08-05T00:31:56.462004: step 13771, loss 0.530045.
Train: 2018-08-05T00:31:56.711975: step 13772, loss 0.505703.
Train: 2018-08-05T00:31:56.977537: step 13773, loss 0.562647.
Train: 2018-08-05T00:31:57.211829: step 13774, loss 0.587049.
Train: 2018-08-05T00:31:57.461799: step 13775, loss 0.49762.
Train: 2018-08-05T00:31:57.711742: step 13776, loss 0.587053.
Train: 2018-08-05T00:31:57.961651: step 13777, loss 0.611474.
Train: 2018-08-05T00:31:58.211623: step 13778, loss 0.627711.
Train: 2018-08-05T00:31:58.461564: step 13779, loss 0.587022.
Train: 2018-08-05T00:31:58.711475: step 13780, loss 0.522213.
Test: 2018-08-05T00:31:59.961183: step 13780, loss 0.547533.
Train: 2018-08-05T00:32:00.195503: step 13781, loss 0.554628.
Train: 2018-08-05T00:32:00.445487: step 13782, loss 0.619292.
Train: 2018-08-05T00:32:00.695417: step 13783, loss 0.611142.
Train: 2018-08-05T00:32:00.960980: step 13784, loss 0.506486.
Train: 2018-08-05T00:32:01.195300: step 13785, loss 0.522636.
Train: 2018-08-05T00:32:01.445241: step 13786, loss 0.514612.
Train: 2018-08-05T00:32:01.695182: step 13787, loss 0.570833.
Train: 2018-08-05T00:32:01.960715: step 13788, loss 0.570829.
Train: 2018-08-05T00:32:02.210657: step 13789, loss 0.578878.
Train: 2018-08-05T00:32:02.460628: step 13790, loss 0.538614.
Test: 2018-08-05T00:32:03.710305: step 13790, loss 0.548374.
Train: 2018-08-05T00:32:03.944656: step 13791, loss 0.619175.
Train: 2018-08-05T00:32:04.194567: step 13792, loss 0.594989.
Train: 2018-08-05T00:32:04.444508: step 13793, loss 0.586922.
Train: 2018-08-05T00:32:04.694481: step 13794, loss 0.522619.
Train: 2018-08-05T00:32:04.944421: step 13795, loss 0.635106.
Train: 2018-08-05T00:32:05.194364: step 13796, loss 0.506692.
Train: 2018-08-05T00:32:05.444304: step 13797, loss 0.634986.
Train: 2018-08-05T00:32:05.694246: step 13798, loss 0.538849.
Train: 2018-08-05T00:32:05.944188: step 13799, loss 0.530891.
Train: 2018-08-05T00:32:06.194129: step 13800, loss 0.514906.
Test: 2018-08-05T00:32:07.459427: step 13800, loss 0.549309.
Train: 2018-08-05T00:32:08.287358: step 13801, loss 0.53885.
Train: 2018-08-05T00:32:08.537300: step 13802, loss 0.55482.
Train: 2018-08-05T00:32:08.787274: step 13803, loss 0.522672.
Train: 2018-08-05T00:32:09.052838: step 13804, loss 0.530589.
Train: 2018-08-05T00:32:09.287157: step 13805, loss 0.643451.
Train: 2018-08-05T00:32:09.537067: step 13806, loss 0.586965.
Train: 2018-08-05T00:32:09.787007: step 13807, loss 0.53848.
Train: 2018-08-05T00:32:10.036981: step 13808, loss 0.570803.
Train: 2018-08-05T00:32:10.286891: step 13809, loss 0.538423.
Train: 2018-08-05T00:32:10.536864: step 13810, loss 0.578898.
Test: 2018-08-05T00:32:11.786539: step 13810, loss 0.54914.
Train: 2018-08-05T00:32:12.052102: step 13811, loss 0.554574.
Train: 2018-08-05T00:32:12.286455: step 13812, loss 0.489628.
Train: 2018-08-05T00:32:12.536396: step 13813, loss 0.578916.
Train: 2018-08-05T00:32:12.786337: step 13814, loss 0.562627.
Train: 2018-08-05T00:32:13.036279: step 13815, loss 0.652391.
Train: 2018-08-05T00:32:13.286220: step 13816, loss 0.529982.
Train: 2018-08-05T00:32:13.536162: step 13817, loss 0.53813.
Train: 2018-08-05T00:32:13.801722: step 13818, loss 0.562604.
Train: 2018-08-05T00:32:14.067255: step 13819, loss 0.538081.
Train: 2018-08-05T00:32:14.379712: step 13820, loss 0.546224.
Test: 2018-08-05T00:32:15.629389: step 13820, loss 0.548138.
Train: 2018-08-05T00:32:15.894952: step 13821, loss 0.578955.
Train: 2018-08-05T00:32:16.160548: step 13822, loss 0.472385.
Train: 2018-08-05T00:32:16.426079: step 13823, loss 0.578978.
Train: 2018-08-05T00:32:16.691670: step 13824, loss 0.562522.
Train: 2018-08-05T00:32:16.957203: step 13825, loss 0.661494.
Train: 2018-08-05T00:32:17.222767: step 13826, loss 0.562512.
Train: 2018-08-05T00:32:17.488359: step 13827, loss 0.521317.
Train: 2018-08-05T00:32:17.753921: step 13828, loss 0.521301.
Train: 2018-08-05T00:32:18.019454: step 13829, loss 0.546006.
Train: 2018-08-05T00:32:18.285017: step 13830, loss 0.562486.
Test: 2018-08-05T00:32:19.534725: step 13830, loss 0.5481.
Train: 2018-08-05T00:32:19.800318: step 13831, loss 0.537682.
Train: 2018-08-05T00:32:20.065880: step 13832, loss 0.587328.
Train: 2018-08-05T00:32:20.331412: step 13833, loss 0.587332.
Train: 2018-08-05T00:32:20.597009: step 13834, loss 0.529314.
Train: 2018-08-05T00:32:20.862569: step 13835, loss 0.579051.
Train: 2018-08-05T00:32:21.128135: step 13836, loss 0.562457.
Train: 2018-08-05T00:32:21.409285: step 13837, loss 0.529281.
Train: 2018-08-05T00:32:21.659227: step 13838, loss 0.637184.
Train: 2018-08-05T00:32:21.924820: step 13839, loss 0.520996.
Train: 2018-08-05T00:32:22.190352: step 13840, loss 0.55417.
Test: 2018-08-05T00:32:23.440060: step 13840, loss 0.548811.
Train: 2018-08-05T00:32:23.690033: step 13841, loss 0.587336.
Train: 2018-08-05T00:32:23.955564: step 13842, loss 0.521039.
Train: 2018-08-05T00:32:24.221126: step 13843, loss 0.537607.
Train: 2018-08-05T00:32:24.486720: step 13844, loss 0.520978.
Train: 2018-08-05T00:32:24.752252: step 13845, loss 0.504272.
Train: 2018-08-05T00:32:25.017848: step 13846, loss 0.570685.
Train: 2018-08-05T00:32:25.283411: step 13847, loss 0.537235.
Train: 2018-08-05T00:32:25.548974: step 13848, loss 0.554157.
Train: 2018-08-05T00:32:25.814534: step 13849, loss 0.529032.
Train: 2018-08-05T00:32:26.080067: step 13850, loss 0.663622.
Test: 2018-08-05T00:32:27.329774: step 13850, loss 0.548459.
Train: 2018-08-05T00:32:27.595337: step 13851, loss 0.562493.
Train: 2018-08-05T00:32:27.845309: step 13852, loss 0.486842.
Train: 2018-08-05T00:32:28.110871: step 13853, loss 0.562325.
Train: 2018-08-05T00:32:28.376435: step 13854, loss 0.5455.
Train: 2018-08-05T00:32:28.657621: step 13855, loss 0.579199.
Train: 2018-08-05T00:32:28.923181: step 13856, loss 0.579245.
Train: 2018-08-05T00:32:29.173123: step 13857, loss 0.587603.
Train: 2018-08-05T00:32:29.438656: step 13858, loss 0.646361.
Train: 2018-08-05T00:32:29.719864: step 13859, loss 0.554027.
Train: 2018-08-05T00:32:29.969811: step 13860, loss 0.554054.
Test: 2018-08-05T00:32:31.235109: step 13860, loss 0.547534.
Train: 2018-08-05T00:32:31.485081: step 13861, loss 0.587443.
Train: 2018-08-05T00:32:31.750644: step 13862, loss 0.470878.
Train: 2018-08-05T00:32:32.031830: step 13863, loss 0.570764.
Train: 2018-08-05T00:32:32.297391: step 13864, loss 0.579066.
Train: 2018-08-05T00:32:32.547332: step 13865, loss 0.587395.
Train: 2018-08-05T00:32:32.812865: step 13866, loss 0.529216.
Train: 2018-08-05T00:32:33.078458: step 13867, loss 0.562521.
Train: 2018-08-05T00:32:33.343991: step 13868, loss 0.529332.
Train: 2018-08-05T00:32:33.625176: step 13869, loss 0.554197.
Train: 2018-08-05T00:32:33.875147: step 13870, loss 0.60389.
Test: 2018-08-05T00:32:35.124824: step 13870, loss 0.548079.
Train: 2018-08-05T00:32:35.374796: step 13871, loss 0.487967.
Train: 2018-08-05T00:32:35.640353: step 13872, loss 0.521081.
Train: 2018-08-05T00:32:35.905923: step 13873, loss 0.661996.
Train: 2018-08-05T00:32:36.171484: step 13874, loss 0.603897.
Train: 2018-08-05T00:32:36.437046: step 13875, loss 0.537683.
Train: 2018-08-05T00:32:36.702609: step 13876, loss 0.562493.
Train: 2018-08-05T00:32:36.968142: step 13877, loss 0.603754.
Train: 2018-08-05T00:32:37.233735: step 13878, loss 0.570754.
Train: 2018-08-05T00:32:37.514919: step 13879, loss 0.611865.
Train: 2018-08-05T00:32:37.780476: step 13880, loss 0.587159.
Test: 2018-08-05T00:32:39.014538: step 13880, loss 0.549507.
Train: 2018-08-05T00:32:39.280129: step 13881, loss 0.562593.
Train: 2018-08-05T00:32:39.545693: step 13882, loss 0.587078.
Train: 2018-08-05T00:32:39.826881: step 13883, loss 0.554523.
Train: 2018-08-05T00:32:40.076820: step 13884, loss 0.55459.
Train: 2018-08-05T00:32:40.342352: step 13885, loss 0.562707.
Train: 2018-08-05T00:32:40.607915: step 13886, loss 0.562728.
Train: 2018-08-05T00:32:40.873479: step 13887, loss 0.538563.
Train: 2018-08-05T00:32:41.139070: step 13888, loss 0.554712.
Train: 2018-08-05T00:32:41.420224: step 13889, loss 0.554718.
Train: 2018-08-05T00:32:41.670197: step 13890, loss 0.530561.
Test: 2018-08-05T00:32:42.919874: step 13890, loss 0.548933.
Train: 2018-08-05T00:32:43.169845: step 13891, loss 0.514398.
Train: 2018-08-05T00:32:43.388514: step 13892, loss 0.579975.
Train: 2018-08-05T00:32:43.654109: step 13893, loss 0.611239.
Train: 2018-08-05T00:32:43.935261: step 13894, loss 0.55464.
Train: 2018-08-05T00:32:44.200853: step 13895, loss 0.635442.
Train: 2018-08-05T00:32:44.466417: step 13896, loss 0.522402.
Train: 2018-08-05T00:32:44.731948: step 13897, loss 0.506303.
Train: 2018-08-05T00:32:44.997542: step 13898, loss 0.611189.
Train: 2018-08-05T00:32:45.263104: step 13899, loss 0.59504.
Train: 2018-08-05T00:32:45.559911: step 13900, loss 0.619198.
Test: 2018-08-05T00:32:46.825208: step 13900, loss 0.549149.
Train: 2018-08-05T00:32:47.668762: step 13901, loss 0.514483.
Train: 2018-08-05T00:32:47.934354: step 13902, loss 0.53865.
Train: 2018-08-05T00:32:48.199917: step 13903, loss 0.522554.
Train: 2018-08-05T00:32:48.481101: step 13904, loss 0.578876.
Train: 2018-08-05T00:32:48.746664: step 13905, loss 0.57082.
Train: 2018-08-05T00:32:49.027850: step 13906, loss 0.48212.
Train: 2018-08-05T00:32:49.293411: step 13907, loss 0.562741.
Train: 2018-08-05T00:32:49.574595: step 13908, loss 0.562714.
Train: 2018-08-05T00:32:49.824537: step 13909, loss 0.570793.
Train: 2018-08-05T00:32:50.105691: step 13910, loss 0.546415.
Test: 2018-08-05T00:32:51.339777: step 13910, loss 0.54849.
Train: 2018-08-05T00:32:51.605364: step 13911, loss 0.578921.
Train: 2018-08-05T00:32:51.870933: step 13912, loss 0.464868.
Train: 2018-08-05T00:32:52.136489: step 13913, loss 0.538082.
Train: 2018-08-05T00:32:52.417674: step 13914, loss 0.693738.
Train: 2018-08-05T00:32:52.698866: step 13915, loss 0.619967.
Train: 2018-08-05T00:32:52.980048: step 13916, loss 0.619924.
Train: 2018-08-05T00:32:53.229959: step 13917, loss 0.546245.
Train: 2018-08-05T00:32:53.511142: step 13918, loss 0.595259.
Train: 2018-08-05T00:32:53.792352: step 13919, loss 0.619644.
Train: 2018-08-05T00:32:54.042299: step 13920, loss 0.562667.
Test: 2018-08-05T00:32:55.291976: step 13920, loss 0.550669.
Train: 2018-08-05T00:32:55.541949: step 13921, loss 0.546507.
Train: 2018-08-05T00:32:55.823132: step 13922, loss 0.595047.
Train: 2018-08-05T00:32:56.088694: step 13923, loss 0.538583.
Train: 2018-08-05T00:32:56.354257: step 13924, loss 0.603013.
Train: 2018-08-05T00:32:56.619820: step 13925, loss 0.538735.
Train: 2018-08-05T00:32:56.885385: step 13926, loss 0.586884.
Train: 2018-08-05T00:32:57.166537: step 13927, loss 0.610866.
Train: 2018-08-05T00:32:57.416510: step 13928, loss 0.499069.
Train: 2018-08-05T00:32:57.682071: step 13929, loss 0.538998.
Train: 2018-08-05T00:32:57.947635: step 13930, loss 0.64263.
Test: 2018-08-05T00:32:59.197312: step 13930, loss 0.549289.
Train: 2018-08-05T00:32:59.447278: step 13931, loss 0.570902.
Train: 2018-08-05T00:32:59.728467: step 13932, loss 0.586802.
Train: 2018-08-05T00:32:59.994030: step 13933, loss 0.602647.
Train: 2018-08-05T00:33:00.259587: step 13934, loss 0.578861.
Train: 2018-08-05T00:33:00.540771: step 13935, loss 0.5473.
Train: 2018-08-05T00:33:00.806340: step 13936, loss 0.602499.
Train: 2018-08-05T00:33:01.071903: step 13937, loss 0.547429.
Train: 2018-08-05T00:33:01.337435: step 13938, loss 0.618116.
Train: 2018-08-05T00:33:01.603028: step 13939, loss 0.547559.
Train: 2018-08-05T00:33:01.868591: step 13940, loss 0.62579.
Test: 2018-08-05T00:33:03.133890: step 13940, loss 0.549381.
Train: 2018-08-05T00:33:03.399485: step 13941, loss 0.524314.
Train: 2018-08-05T00:33:03.680672: step 13942, loss 0.563322.
Train: 2018-08-05T00:33:03.930609: step 13943, loss 0.571125.
Train: 2018-08-05T00:33:04.211793: step 13944, loss 0.524482.
Train: 2018-08-05T00:33:04.477356: step 13945, loss 0.555578.
Train: 2018-08-05T00:33:04.758539: step 13946, loss 0.547786.
Train: 2018-08-05T00:33:05.039717: step 13947, loss 0.571094.
Train: 2018-08-05T00:33:05.305286: step 13948, loss 0.524251.
Train: 2018-08-05T00:33:05.555228: step 13949, loss 0.555427.
Train: 2018-08-05T00:33:05.852003: step 13950, loss 0.539654.
Test: 2018-08-05T00:33:07.101711: step 13950, loss 0.550881.
Train: 2018-08-05T00:33:07.351677: step 13951, loss 0.602433.
Train: 2018-08-05T00:33:07.632869: step 13952, loss 0.539429.
Train: 2018-08-05T00:33:07.914052: step 13953, loss 0.539358.
Train: 2018-08-05T00:33:08.195235: step 13954, loss 0.594998.
Train: 2018-08-05T00:33:08.445176: step 13955, loss 0.515246.
Train: 2018-08-05T00:33:08.726330: step 13956, loss 0.522857.
Train: 2018-08-05T00:33:08.991923: step 13957, loss 0.546837.
Train: 2018-08-05T00:33:09.273077: step 13958, loss 0.562933.
Train: 2018-08-05T00:33:09.523051: step 13959, loss 0.529731.
Train: 2018-08-05T00:33:09.788612: step 13960, loss 0.522128.
Test: 2018-08-05T00:33:11.053909: step 13960, loss 0.547761.
Train: 2018-08-05T00:33:11.319473: step 13961, loss 0.480193.
Train: 2018-08-05T00:33:11.600687: step 13962, loss 0.621901.
Train: 2018-08-05T00:33:11.866250: step 13963, loss 0.503596.
Train: 2018-08-05T00:33:12.131813: step 13964, loss 0.553993.
Train: 2018-08-05T00:33:12.412997: step 13965, loss 0.546642.
Train: 2018-08-05T00:33:12.694181: step 13966, loss 0.554985.
Train: 2018-08-05T00:33:12.944093: step 13967, loss 0.623246.
Train: 2018-08-05T00:33:13.225276: step 13968, loss 0.562528.
Train: 2018-08-05T00:33:13.506460: step 13969, loss 0.578102.
Train: 2018-08-05T00:33:13.772049: step 13970, loss 0.58719.
Test: 2018-08-05T00:33:15.021731: step 13970, loss 0.550273.
Train: 2018-08-05T00:33:15.302945: step 13971, loss 0.578905.
Train: 2018-08-05T00:33:15.568509: step 13972, loss 0.511781.
Train: 2018-08-05T00:33:15.849693: step 13973, loss 0.553913.
Train: 2018-08-05T00:33:16.099634: step 13974, loss 0.562512.
Train: 2018-08-05T00:33:16.365196: step 13975, loss 0.629256.
Train: 2018-08-05T00:33:16.630731: step 13976, loss 0.512222.
Train: 2018-08-05T00:33:16.911938: step 13977, loss 0.520826.
Train: 2018-08-05T00:33:17.177507: step 13978, loss 0.620765.
Train: 2018-08-05T00:33:17.443039: step 13979, loss 0.645342.
Train: 2018-08-05T00:33:17.708632: step 13980, loss 0.562458.
Test: 2018-08-05T00:33:18.958309: step 13980, loss 0.549226.
Train: 2018-08-05T00:33:19.239494: step 13981, loss 0.545909.
Train: 2018-08-05T00:33:19.505055: step 13982, loss 0.603889.
Train: 2018-08-05T00:33:19.770651: step 13983, loss 0.537724.
Train: 2018-08-05T00:33:20.051834: step 13984, loss 0.521456.
Train: 2018-08-05T00:33:20.333017: step 13985, loss 0.628191.
Train: 2018-08-05T00:33:20.598550: step 13986, loss 0.562631.
Train: 2018-08-05T00:33:20.864143: step 13987, loss 0.619633.
Train: 2018-08-05T00:33:21.129706: step 13988, loss 0.546312.
Train: 2018-08-05T00:33:21.395270: step 13989, loss 0.603305.
Train: 2018-08-05T00:33:21.676453: step 13990, loss 0.49784.
Test: 2018-08-05T00:33:22.910510: step 13990, loss 0.548835.
Train: 2018-08-05T00:33:23.176097: step 13991, loss 0.506045.
Train: 2018-08-05T00:33:23.457285: step 13992, loss 0.570781.
Train: 2018-08-05T00:33:23.722843: step 13993, loss 0.538513.
Train: 2018-08-05T00:33:24.004032: step 13994, loss 0.554543.
Train: 2018-08-05T00:33:24.285211: step 13995, loss 0.562641.
Train: 2018-08-05T00:33:24.550749: step 13996, loss 0.538284.
Train: 2018-08-05T00:33:24.831958: step 13997, loss 0.530245.
Train: 2018-08-05T00:33:25.113148: step 13998, loss 0.530099.
Train: 2018-08-05T00:33:25.394326: step 13999, loss 0.586993.
Train: 2018-08-05T00:33:25.659865: step 14000, loss 0.562295.
Test: 2018-08-05T00:33:26.909572: step 14000, loss 0.549029.
Train: 2018-08-05T00:33:27.768746: step 14001, loss 0.612036.
Train: 2018-08-05T00:33:28.034339: step 14002, loss 0.496586.
Train: 2018-08-05T00:33:28.299872: step 14003, loss 0.59563.
Train: 2018-08-05T00:33:28.565464: step 14004, loss 0.54603.
Train: 2018-08-05T00:33:28.830996: step 14005, loss 0.545774.
Train: 2018-08-05T00:33:29.112186: step 14006, loss 0.570894.
Train: 2018-08-05T00:33:29.377744: step 14007, loss 0.594992.
Train: 2018-08-05T00:33:29.643308: step 14008, loss 0.60416.
Train: 2018-08-05T00:33:29.908869: step 14009, loss 0.553802.
Train: 2018-08-05T00:33:30.174432: step 14010, loss 0.570329.
Test: 2018-08-05T00:33:31.424140: step 14010, loss 0.548293.
Train: 2018-08-05T00:33:31.674082: step 14011, loss 0.579648.
Train: 2018-08-05T00:33:31.939645: step 14012, loss 0.578734.
Train: 2018-08-05T00:33:32.220853: step 14013, loss 0.570583.
Train: 2018-08-05T00:33:32.486421: step 14014, loss 0.562508.
Train: 2018-08-05T00:33:32.767577: step 14015, loss 0.644596.
Train: 2018-08-05T00:33:33.048758: step 14016, loss 0.546052.
Train: 2018-08-05T00:33:33.314352: step 14017, loss 0.595259.
Train: 2018-08-05T00:33:33.579885: step 14018, loss 0.50548.
Train: 2018-08-05T00:33:33.845473: step 14019, loss 0.587587.
Train: 2018-08-05T00:33:34.111011: step 14020, loss 0.522281.
Test: 2018-08-05T00:33:35.376339: step 14020, loss 0.548687.
Train: 2018-08-05T00:33:35.626310: step 14021, loss 0.594856.
Train: 2018-08-05T00:33:35.891876: step 14022, loss 0.554422.
Train: 2018-08-05T00:33:36.157436: step 14023, loss 0.578769.
Train: 2018-08-05T00:33:36.423000: step 14024, loss 0.56266.
Train: 2018-08-05T00:33:36.688533: step 14025, loss 0.506469.
Train: 2018-08-05T00:33:36.954119: step 14026, loss 0.530227.
Train: 2018-08-05T00:33:37.219688: step 14027, loss 0.570309.
Train: 2018-08-05T00:33:37.485251: step 14028, loss 0.570897.
Train: 2018-08-05T00:33:37.750813: step 14029, loss 0.514276.
Train: 2018-08-05T00:33:38.031968: step 14030, loss 0.465623.
Test: 2018-08-05T00:33:39.281675: step 14030, loss 0.550223.
Train: 2018-08-05T00:33:39.562858: step 14031, loss 0.57889.
Train: 2018-08-05T00:33:39.828454: step 14032, loss 0.52903.
Train: 2018-08-05T00:33:40.109607: step 14033, loss 0.471389.
Train: 2018-08-05T00:33:40.375170: step 14034, loss 0.527859.
Train: 2018-08-05T00:33:40.640764: step 14035, loss 0.560079.
Train: 2018-08-05T00:33:40.921916: step 14036, loss 0.603608.
Train: 2018-08-05T00:33:41.171857: step 14037, loss 0.483765.
Train: 2018-08-05T00:33:41.437452: step 14038, loss 0.553171.
Train: 2018-08-05T00:33:41.703013: step 14039, loss 0.567325.
Train: 2018-08-05T00:33:41.968576: step 14040, loss 0.555113.
Test: 2018-08-05T00:33:43.233874: step 14040, loss 0.546095.
Train: 2018-08-05T00:33:43.483846: step 14041, loss 0.53834.
Train: 2018-08-05T00:33:43.749378: step 14042, loss 0.563473.
Train: 2018-08-05T00:33:43.968108: step 14043, loss 0.583625.
Train: 2018-08-05T00:33:44.233667: step 14044, loss 0.579675.
Train: 2018-08-05T00:33:44.514856: step 14045, loss 0.541326.
Train: 2018-08-05T00:33:44.780412: step 14046, loss 0.501683.
Train: 2018-08-05T00:33:45.045980: step 14047, loss 0.555076.
Train: 2018-08-05T00:33:45.327135: step 14048, loss 0.52717.
Train: 2018-08-05T00:33:45.592722: step 14049, loss 0.545281.
Train: 2018-08-05T00:33:45.858259: step 14050, loss 0.563241.
Test: 2018-08-05T00:33:47.107968: step 14050, loss 0.548391.
Train: 2018-08-05T00:33:47.373562: step 14051, loss 0.570371.
Train: 2018-08-05T00:33:47.654715: step 14052, loss 0.536786.
Train: 2018-08-05T00:33:47.904686: step 14053, loss 0.52823.
Train: 2018-08-05T00:33:48.185844: step 14054, loss 0.579325.
Train: 2018-08-05T00:33:48.451432: step 14055, loss 0.563227.
Train: 2018-08-05T00:33:48.732586: step 14056, loss 0.59537.
Train: 2018-08-05T00:33:48.998150: step 14057, loss 0.553935.
Train: 2018-08-05T00:33:49.263713: step 14058, loss 0.563435.
Train: 2018-08-05T00:33:49.529276: step 14059, loss 0.555017.
Train: 2018-08-05T00:33:49.810490: step 14060, loss 0.597594.
Test: 2018-08-05T00:33:51.060166: step 14060, loss 0.549651.
Train: 2018-08-05T00:33:51.325759: step 14061, loss 0.503487.
Train: 2018-08-05T00:33:51.591322: step 14062, loss 0.553776.
Train: 2018-08-05T00:33:51.872506: step 14063, loss 0.529194.
Train: 2018-08-05T00:33:52.122419: step 14064, loss 0.487038.
Train: 2018-08-05T00:33:52.387981: step 14065, loss 0.529024.
Train: 2018-08-05T00:33:52.653573: step 14066, loss 0.520714.
Train: 2018-08-05T00:33:52.934738: step 14067, loss 0.545897.
Train: 2018-08-05T00:33:53.184700: step 14068, loss 0.646411.
Train: 2018-08-05T00:33:53.450262: step 14069, loss 0.629598.
Train: 2018-08-05T00:33:53.715819: step 14070, loss 0.595685.
Test: 2018-08-05T00:33:54.981124: step 14070, loss 0.548465.
Train: 2018-08-05T00:33:55.231096: step 14071, loss 0.478822.
Train: 2018-08-05T00:33:55.508229: step 14072, loss 0.503863.
Train: 2018-08-05T00:33:55.773794: step 14073, loss 0.587387.
Train: 2018-08-05T00:33:56.039355: step 14074, loss 0.528823.
Train: 2018-08-05T00:33:56.304888: step 14075, loss 0.57083.
Train: 2018-08-05T00:33:56.601694: step 14076, loss 0.503746.
Train: 2018-08-05T00:33:56.882908: step 14077, loss 0.58775.
Train: 2018-08-05T00:33:57.148471: step 14078, loss 0.57931.
Train: 2018-08-05T00:33:57.414033: step 14079, loss 0.570692.
Train: 2018-08-05T00:33:57.679596: step 14080, loss 0.48668.
Test: 2018-08-05T00:33:58.944894: step 14080, loss 0.547733.
Train: 2018-08-05T00:33:59.194867: step 14081, loss 0.545512.
Train: 2018-08-05T00:33:59.476045: step 14082, loss 0.545291.
Train: 2018-08-05T00:33:59.741583: step 14083, loss 0.613305.
Train: 2018-08-05T00:34:00.022799: step 14084, loss 0.579188.
Train: 2018-08-05T00:34:00.288360: step 14085, loss 0.629965.
Train: 2018-08-05T00:34:00.569538: step 14086, loss 0.553933.
Train: 2018-08-05T00:34:00.819486: step 14087, loss 0.562413.
Train: 2018-08-05T00:34:01.085048: step 14088, loss 0.553837.
Train: 2018-08-05T00:34:01.350606: step 14089, loss 0.621158.
Train: 2018-08-05T00:34:01.631795: step 14090, loss 0.61246.
Test: 2018-08-05T00:34:02.881472: step 14090, loss 0.548382.
Train: 2018-08-05T00:34:03.131439: step 14091, loss 0.579025.
Train: 2018-08-05T00:34:03.412599: step 14092, loss 0.56249.
Train: 2018-08-05T00:34:03.662541: step 14093, loss 0.587299.
Train: 2018-08-05T00:34:03.943754: step 14094, loss 0.505054.
Train: 2018-08-05T00:34:04.193696: step 14095, loss 0.611748.
Train: 2018-08-05T00:34:04.459254: step 14096, loss 0.538096.
Train: 2018-08-05T00:34:04.724821: step 14097, loss 0.530019.
Train: 2018-08-05T00:34:05.005976: step 14098, loss 0.587045.
Train: 2018-08-05T00:34:05.287159: step 14099, loss 0.546401.
Train: 2018-08-05T00:34:05.552722: step 14100, loss 0.570797.
Test: 2018-08-05T00:34:06.802430: step 14100, loss 0.549321.
Train: 2018-08-05T00:34:07.708467: step 14101, loss 0.530232.
Train: 2018-08-05T00:34:07.989652: step 14102, loss 0.554568.
Train: 2018-08-05T00:34:08.239594: step 14103, loss 0.570778.
Train: 2018-08-05T00:34:08.520808: step 14104, loss 0.562694.
Train: 2018-08-05T00:34:08.801991: step 14105, loss 0.530253.
Train: 2018-08-05T00:34:09.067554: step 14106, loss 0.5464.
Train: 2018-08-05T00:34:09.333118: step 14107, loss 0.578826.
Train: 2018-08-05T00:34:09.598684: step 14108, loss 0.57067.
Train: 2018-08-05T00:34:09.864247: step 14109, loss 0.530124.
Train: 2018-08-05T00:34:10.129775: step 14110, loss 0.57858.
Test: 2018-08-05T00:34:11.379483: step 14110, loss 0.547977.
Train: 2018-08-05T00:34:11.660697: step 14111, loss 0.587284.
Train: 2018-08-05T00:34:11.910640: step 14112, loss 0.554676.
Train: 2018-08-05T00:34:12.176201: step 14113, loss 0.604249.
Train: 2018-08-05T00:34:12.441764: step 14114, loss 0.619481.
Train: 2018-08-05T00:34:12.707327: step 14115, loss 0.554484.
Train: 2018-08-05T00:34:12.972861: step 14116, loss 0.554372.
Train: 2018-08-05T00:34:13.238422: step 14117, loss 0.570775.
Train: 2018-08-05T00:34:13.519637: step 14118, loss 0.578873.
Train: 2018-08-05T00:34:13.785200: step 14119, loss 0.570691.
Train: 2018-08-05T00:34:14.050733: step 14120, loss 0.611404.
Test: 2018-08-05T00:34:15.300440: step 14120, loss 0.548781.
Train: 2018-08-05T00:34:15.566002: step 14121, loss 0.651216.
Train: 2018-08-05T00:34:15.878429: step 14122, loss 0.570749.
Train: 2018-08-05T00:34:16.159644: step 14123, loss 0.578825.
Train: 2018-08-05T00:34:16.440831: step 14124, loss 0.483469.
Train: 2018-08-05T00:34:16.722020: step 14125, loss 0.666203.
Train: 2018-08-05T00:34:16.971924: step 14126, loss 0.539297.
Train: 2018-08-05T00:34:17.253108: step 14127, loss 0.507833.
Train: 2018-08-05T00:34:17.534318: step 14128, loss 0.563092.
Train: 2018-08-05T00:34:17.799857: step 14129, loss 0.673451.
Train: 2018-08-05T00:34:18.081063: step 14130, loss 0.555296.
Test: 2018-08-05T00:34:19.330745: step 14130, loss 0.54847.
Train: 2018-08-05T00:34:19.580718: step 14131, loss 0.523979.
Train: 2018-08-05T00:34:19.846281: step 14132, loss 0.524029.
Train: 2018-08-05T00:34:20.111837: step 14133, loss 0.571039.
Train: 2018-08-05T00:34:20.392997: step 14134, loss 0.54752.
Train: 2018-08-05T00:34:20.658590: step 14135, loss 0.531797.
Train: 2018-08-05T00:34:20.924124: step 14136, loss 0.500286.
Train: 2018-08-05T00:34:21.189718: step 14137, loss 0.500035.
Train: 2018-08-05T00:34:21.455278: step 14138, loss 0.547197.
Train: 2018-08-05T00:34:21.720841: step 14139, loss 0.547053.
Train: 2018-08-05T00:34:22.002026: step 14140, loss 0.578879.
Test: 2018-08-05T00:34:23.251703: step 14140, loss 0.549816.
Train: 2018-08-05T00:34:23.501674: step 14141, loss 0.554819.
Train: 2018-08-05T00:34:23.782859: step 14142, loss 0.522549.
Train: 2018-08-05T00:34:24.032771: step 14143, loss 0.530423.
Train: 2018-08-05T00:34:24.313955: step 14144, loss 0.60324.
Train: 2018-08-05T00:34:24.579516: step 14145, loss 0.578935.
Train: 2018-08-05T00:34:24.845110: step 14146, loss 0.562607.
Train: 2018-08-05T00:34:25.110642: step 14147, loss 0.570794.
Train: 2018-08-05T00:34:25.391857: step 14148, loss 0.546179.
Train: 2018-08-05T00:34:25.657419: step 14149, loss 0.554348.
Train: 2018-08-05T00:34:25.922982: step 14150, loss 0.595429.
Test: 2018-08-05T00:34:27.172660: step 14150, loss 0.548591.
Train: 2018-08-05T00:34:27.453874: step 14151, loss 0.595449.
Train: 2018-08-05T00:34:27.703786: step 14152, loss 0.653077.
Train: 2018-08-05T00:34:27.969382: step 14153, loss 0.529681.
Train: 2018-08-05T00:34:28.250563: step 14154, loss 0.578968.
Train: 2018-08-05T00:34:28.516125: step 14155, loss 0.562564.
Train: 2018-08-05T00:34:28.797280: step 14156, loss 0.587141.
Train: 2018-08-05T00:34:29.047221: step 14157, loss 0.554416.
Train: 2018-08-05T00:34:29.328429: step 14158, loss 0.513612.
Train: 2018-08-05T00:34:29.593967: step 14159, loss 0.578935.
Train: 2018-08-05T00:34:29.875152: step 14160, loss 0.513644.
Test: 2018-08-05T00:34:31.124859: step 14160, loss 0.549153.
Train: 2018-08-05T00:34:31.390453: step 14161, loss 0.521772.
Train: 2018-08-05T00:34:31.656015: step 14162, loss 0.570767.
Train: 2018-08-05T00:34:31.921548: step 14163, loss 0.578951.
Train: 2018-08-05T00:34:32.187141: step 14164, loss 0.578955.
Train: 2018-08-05T00:34:32.452705: step 14165, loss 0.546182.
Train: 2018-08-05T00:34:32.718266: step 14166, loss 0.554368.
Train: 2018-08-05T00:34:32.983832: step 14167, loss 0.636381.
Train: 2018-08-05T00:34:33.249362: step 14168, loss 0.537988.
Train: 2018-08-05T00:34:33.514954: step 14169, loss 0.570765.
Train: 2018-08-05T00:34:33.796110: step 14170, loss 0.57895.
Test: 2018-08-05T00:34:35.045816: step 14170, loss 0.548554.
Train: 2018-08-05T00:34:35.295787: step 14171, loss 0.513521.
Train: 2018-08-05T00:34:35.561351: step 14172, loss 0.562587.
Train: 2018-08-05T00:34:35.826913: step 14173, loss 0.546221.
Train: 2018-08-05T00:34:36.092446: step 14174, loss 0.497097.
Train: 2018-08-05T00:34:36.358042: step 14175, loss 0.562562.
Train: 2018-08-05T00:34:36.623571: step 14176, loss 0.628251.
Train: 2018-08-05T00:34:36.889164: step 14177, loss 0.529694.
Train: 2018-08-05T00:34:37.154697: step 14178, loss 0.578978.
Train: 2018-08-05T00:34:37.420259: step 14179, loss 0.546097.
Train: 2018-08-05T00:34:37.685853: step 14180, loss 0.694127.
Test: 2018-08-05T00:34:38.935530: step 14180, loss 0.548474.
Train: 2018-08-05T00:34:39.185501: step 14181, loss 0.578966.
Train: 2018-08-05T00:34:39.451064: step 14182, loss 0.538024.
Train: 2018-08-05T00:34:39.716627: step 14183, loss 0.652483.
Train: 2018-08-05T00:34:39.982194: step 14184, loss 0.538204.
Train: 2018-08-05T00:34:40.232101: step 14185, loss 0.570786.
Train: 2018-08-05T00:34:40.482044: step 14186, loss 0.530285.
Train: 2018-08-05T00:34:40.747605: step 14187, loss 0.570802.
Train: 2018-08-05T00:34:41.013198: step 14188, loss 0.514265.
Train: 2018-08-05T00:34:41.278732: step 14189, loss 0.57081.
Train: 2018-08-05T00:34:41.544325: step 14190, loss 0.578884.
Test: 2018-08-05T00:34:42.794001: step 14190, loss 0.549832.
Train: 2018-08-05T00:34:43.043974: step 14191, loss 0.546614.
Train: 2018-08-05T00:34:43.309536: step 14192, loss 0.546619.
Train: 2018-08-05T00:34:43.575069: step 14193, loss 0.538544.
Train: 2018-08-05T00:34:43.762525: step 14194, loss 0.545513.
Train: 2018-08-05T00:34:44.028112: step 14195, loss 0.586972.
Train: 2018-08-05T00:34:44.278059: step 14196, loss 0.578891.
Train: 2018-08-05T00:34:44.543622: step 14197, loss 0.538448.
Train: 2018-08-05T00:34:44.809154: step 14198, loss 0.651743.
Train: 2018-08-05T00:34:45.059097: step 14199, loss 0.554636.
Train: 2018-08-05T00:34:45.324659: step 14200, loss 0.554655.
Test: 2018-08-05T00:34:46.589988: step 14200, loss 0.548887.
Train: 2018-08-05T00:34:47.449186: step 14201, loss 0.538523.
Train: 2018-08-05T00:34:47.699133: step 14202, loss 0.546594.
Train: 2018-08-05T00:34:47.949044: step 14203, loss 0.61119.
Train: 2018-08-05T00:34:48.214641: step 14204, loss 0.554671.
Train: 2018-08-05T00:34:48.464548: step 14205, loss 0.635362.
Train: 2018-08-05T00:34:48.730111: step 14206, loss 0.538608.
Train: 2018-08-05T00:34:48.980087: step 14207, loss 0.643238.
Train: 2018-08-05T00:34:49.229994: step 14208, loss 0.554796.
Train: 2018-08-05T00:34:49.479970: step 14209, loss 0.578865.
Train: 2018-08-05T00:34:49.729902: step 14210, loss 0.554892.
Test: 2018-08-05T00:34:50.979585: step 14210, loss 0.549403.
Train: 2018-08-05T00:34:51.229560: step 14211, loss 0.546955.
Train: 2018-08-05T00:34:51.479502: step 14212, loss 0.53902.
Train: 2018-08-05T00:34:51.729443: step 14213, loss 0.570894.
Train: 2018-08-05T00:34:51.979384: step 14214, loss 0.586821.
Train: 2018-08-05T00:34:52.229326: step 14215, loss 0.53112.
Train: 2018-08-05T00:34:52.479235: step 14216, loss 0.531113.
Train: 2018-08-05T00:34:52.729174: step 14217, loss 0.570894.
Train: 2018-08-05T00:34:52.979150: step 14218, loss 0.570888.
Train: 2018-08-05T00:34:53.229092: step 14219, loss 0.538976.
Train: 2018-08-05T00:34:53.494620: step 14220, loss 0.514967.
Test: 2018-08-05T00:34:54.728706: step 14220, loss 0.548564.
Train: 2018-08-05T00:34:54.963028: step 14221, loss 0.514828.
Train: 2018-08-05T00:34:55.212969: step 14222, loss 0.522659.
Train: 2018-08-05T00:34:55.462940: step 14223, loss 0.554695.
Train: 2018-08-05T00:34:55.712882: step 14224, loss 0.50606.
Train: 2018-08-05T00:34:55.994066: step 14225, loss 0.59519.
Train: 2018-08-05T00:34:56.244007: step 14226, loss 0.595239.
Train: 2018-08-05T00:34:56.493949: step 14227, loss 0.538009.
Train: 2018-08-05T00:34:56.743859: step 14228, loss 0.513346.
Train: 2018-08-05T00:34:56.993831: step 14229, loss 0.480264.
Train: 2018-08-05T00:34:57.243743: step 14230, loss 0.54589.
Test: 2018-08-05T00:34:58.493449: step 14230, loss 0.54924.
Train: 2018-08-05T00:34:58.743422: step 14231, loss 0.478939.
Train: 2018-08-05T00:34:58.993363: step 14232, loss 0.528805.
Train: 2018-08-05T00:34:59.243305: step 14233, loss 0.638405.
Train: 2018-08-05T00:34:59.493249: step 14234, loss 0.553572.
Train: 2018-08-05T00:34:59.743188: step 14235, loss 0.563439.
Train: 2018-08-05T00:34:59.993129: step 14236, loss 0.526818.
Train: 2018-08-05T00:35:00.243041: step 14237, loss 0.535971.
Train: 2018-08-05T00:35:00.493012: step 14238, loss 0.4932.
Train: 2018-08-05T00:35:00.742954: step 14239, loss 0.598805.
Train: 2018-08-05T00:35:00.992896: step 14240, loss 0.545959.
Test: 2018-08-05T00:35:02.242572: step 14240, loss 0.547543.
Train: 2018-08-05T00:35:02.476927: step 14241, loss 0.553815.
Train: 2018-08-05T00:35:02.726868: step 14242, loss 0.555152.
Train: 2018-08-05T00:35:02.976775: step 14243, loss 0.501.
Train: 2018-08-05T00:35:03.226716: step 14244, loss 0.553729.
Train: 2018-08-05T00:35:03.476692: step 14245, loss 0.623009.
Train: 2018-08-05T00:35:03.726601: step 14246, loss 0.57958.
Train: 2018-08-05T00:35:03.976576: step 14247, loss 0.622556.
Train: 2018-08-05T00:35:04.226516: step 14248, loss 0.562288.
Train: 2018-08-05T00:35:04.476425: step 14249, loss 0.570877.
Train: 2018-08-05T00:35:04.726400: step 14250, loss 0.579512.
Test: 2018-08-05T00:35:05.976073: step 14250, loss 0.548691.
Train: 2018-08-05T00:35:06.210394: step 14251, loss 0.562372.
Train: 2018-08-05T00:35:06.460367: step 14252, loss 0.562358.
Train: 2018-08-05T00:35:06.710276: step 14253, loss 0.613234.
Train: 2018-08-05T00:35:06.960249: step 14254, loss 0.553915.
Train: 2018-08-05T00:35:07.210159: step 14255, loss 0.604482.
Train: 2018-08-05T00:35:07.460133: step 14256, loss 0.545603.
Train: 2018-08-05T00:35:07.710074: step 14257, loss 0.50387.
Train: 2018-08-05T00:35:07.959984: step 14258, loss 0.554068.
Train: 2018-08-05T00:35:08.209926: step 14259, loss 0.512383.
Train: 2018-08-05T00:35:08.459867: step 14260, loss 0.637436.
Test: 2018-08-05T00:35:09.709574: step 14260, loss 0.548739.
Train: 2018-08-05T00:35:09.943925: step 14261, loss 0.520867.
Train: 2018-08-05T00:35:10.193865: step 14262, loss 0.504275.
Train: 2018-08-05T00:35:10.443807: step 14263, loss 0.603995.
Train: 2018-08-05T00:35:10.693749: step 14264, loss 0.579071.
Train: 2018-08-05T00:35:10.943690: step 14265, loss 0.529245.
Train: 2018-08-05T00:35:11.193634: step 14266, loss 0.587317.
Train: 2018-08-05T00:35:11.443573: step 14267, loss 0.595613.
Train: 2018-08-05T00:35:11.693514: step 14268, loss 0.53768.
Train: 2018-08-05T00:35:11.943456: step 14269, loss 0.62857.
Train: 2018-08-05T00:35:12.193397: step 14270, loss 0.546044.
Test: 2018-08-05T00:35:13.443074: step 14270, loss 0.54842.
Train: 2018-08-05T00:35:13.677426: step 14271, loss 0.611858.
Train: 2018-08-05T00:35:13.927366: step 14272, loss 0.570807.
Train: 2018-08-05T00:35:14.177308: step 14273, loss 0.488973.
Train: 2018-08-05T00:35:14.427249: step 14274, loss 0.521753.
Train: 2018-08-05T00:35:14.677191: step 14275, loss 0.603449.
Train: 2018-08-05T00:35:14.927132: step 14276, loss 0.57894.
Train: 2018-08-05T00:35:15.177074: step 14277, loss 0.562611.
Train: 2018-08-05T00:35:15.427018: step 14278, loss 0.644104.
Train: 2018-08-05T00:35:15.676927: step 14279, loss 0.546407.
Train: 2018-08-05T00:35:15.926867: step 14280, loss 0.611342.
Test: 2018-08-05T00:35:17.176574: step 14280, loss 0.548281.
Train: 2018-08-05T00:35:17.410895: step 14281, loss 0.498026.
Train: 2018-08-05T00:35:17.660867: step 14282, loss 0.570808.
Train: 2018-08-05T00:35:17.910808: step 14283, loss 0.595018.
Train: 2018-08-05T00:35:18.176371: step 14284, loss 0.546657.
Train: 2018-08-05T00:35:18.426283: step 14285, loss 0.538643.
Train: 2018-08-05T00:35:18.676255: step 14286, loss 0.603005.
Train: 2018-08-05T00:35:18.926196: step 14287, loss 0.602973.
Train: 2018-08-05T00:35:19.191758: step 14288, loss 0.562829.
Train: 2018-08-05T00:35:19.426079: step 14289, loss 0.490798.
Train: 2018-08-05T00:35:19.676020: step 14290, loss 0.546834.
Test: 2018-08-05T00:35:20.925697: step 14290, loss 0.549826.
Train: 2018-08-05T00:35:21.160048: step 14291, loss 0.56284.
Train: 2018-08-05T00:35:21.409960: step 14292, loss 0.570849.
Train: 2018-08-05T00:35:21.675521: step 14293, loss 0.530733.
Train: 2018-08-05T00:35:21.941115: step 14294, loss 0.562807.
Train: 2018-08-05T00:35:22.191027: step 14295, loss 0.586919.
Train: 2018-08-05T00:35:22.440997: step 14296, loss 0.562777.
Train: 2018-08-05T00:35:22.690939: step 14297, loss 0.530567.
Train: 2018-08-05T00:35:22.940880: step 14298, loss 0.595003.
Train: 2018-08-05T00:35:23.206443: step 14299, loss 0.58695.
Train: 2018-08-05T00:35:23.471976: step 14300, loss 0.546605.
Test: 2018-08-05T00:35:24.721683: step 14300, loss 0.548523.
Train: 2018-08-05T00:35:25.612100: step 14301, loss 0.506251.
Train: 2018-08-05T00:35:25.862071: step 14302, loss 0.611216.
Train: 2018-08-05T00:35:26.111984: step 14303, loss 0.538447.
Train: 2018-08-05T00:35:26.361954: step 14304, loss 0.497948.
Train: 2018-08-05T00:35:26.611890: step 14305, loss 0.562702.
Train: 2018-08-05T00:35:26.861837: step 14306, loss 0.513872.
Train: 2018-08-05T00:35:27.111780: step 14307, loss 0.562632.
Train: 2018-08-05T00:35:27.361690: step 14308, loss 0.546193.
Train: 2018-08-05T00:35:27.611663: step 14309, loss 0.521553.
Train: 2018-08-05T00:35:27.861603: step 14310, loss 0.504922.
Test: 2018-08-05T00:35:29.126902: step 14310, loss 0.548879.
Train: 2018-08-05T00:35:29.423708: step 14311, loss 0.53767.
Train: 2018-08-05T00:35:29.673680: step 14312, loss 0.603961.
Train: 2018-08-05T00:35:29.923621: step 14313, loss 0.612333.
Train: 2018-08-05T00:35:30.173561: step 14314, loss 0.52893.
Train: 2018-08-05T00:35:30.423499: step 14315, loss 0.612435.
Train: 2018-08-05T00:35:30.673415: step 14316, loss 0.604282.
Train: 2018-08-05T00:35:30.923387: step 14317, loss 0.587531.
Train: 2018-08-05T00:35:31.173327: step 14318, loss 0.554061.
Train: 2018-08-05T00:35:31.423238: step 14319, loss 0.570831.
Train: 2018-08-05T00:35:31.673211: step 14320, loss 0.520711.
Test: 2018-08-05T00:35:32.922888: step 14320, loss 0.546784.
Train: 2018-08-05T00:35:33.157238: step 14321, loss 0.587358.
Train: 2018-08-05T00:35:33.407182: step 14322, loss 0.595682.
Train: 2018-08-05T00:35:33.657090: step 14323, loss 0.612372.
Train: 2018-08-05T00:35:33.907062: step 14324, loss 0.545876.
Train: 2018-08-05T00:35:34.157008: step 14325, loss 0.546082.
Train: 2018-08-05T00:35:34.406946: step 14326, loss 0.587128.
Train: 2018-08-05T00:35:34.656886: step 14327, loss 0.521209.
Train: 2018-08-05T00:35:34.906798: step 14328, loss 0.504874.
Train: 2018-08-05T00:35:35.156770: step 14329, loss 0.546039.
Train: 2018-08-05T00:35:35.406681: step 14330, loss 0.595589.
Test: 2018-08-05T00:35:36.656389: step 14330, loss 0.548548.
Train: 2018-08-05T00:35:36.906355: step 14331, loss 0.579051.
Train: 2018-08-05T00:35:37.156271: step 14332, loss 0.636834.
Train: 2018-08-05T00:35:37.406243: step 14333, loss 0.537744.
Train: 2018-08-05T00:35:37.656154: step 14334, loss 0.537978.
Train: 2018-08-05T00:35:37.906095: step 14335, loss 0.554393.
Train: 2018-08-05T00:35:38.156068: step 14336, loss 0.562574.
Train: 2018-08-05T00:35:38.405980: step 14337, loss 0.570699.
Train: 2018-08-05T00:35:38.655921: step 14338, loss 0.562713.
Train: 2018-08-05T00:35:38.905892: step 14339, loss 0.562581.
Train: 2018-08-05T00:35:39.155833: step 14340, loss 0.562619.
Test: 2018-08-05T00:35:40.421131: step 14340, loss 0.549924.
Train: 2018-08-05T00:35:40.655452: step 14341, loss 0.554423.
Train: 2018-08-05T00:35:40.905418: step 14342, loss 0.595168.
Train: 2018-08-05T00:35:41.155365: step 14343, loss 0.562623.
Train: 2018-08-05T00:35:41.405300: step 14344, loss 0.587023.
Train: 2018-08-05T00:35:41.608378: step 14345, loss 0.701372.
Train: 2018-08-05T00:35:41.858326: step 14346, loss 0.627467.
Train: 2018-08-05T00:35:42.108268: step 14347, loss 0.603018.
Train: 2018-08-05T00:35:42.358208: step 14348, loss 0.618925.
Train: 2018-08-05T00:35:42.608151: step 14349, loss 0.578834.
Train: 2018-08-05T00:35:42.858092: step 14350, loss 0.523458.
Test: 2018-08-05T00:35:44.123389: step 14350, loss 0.550483.
Train: 2018-08-05T00:35:44.357741: step 14351, loss 0.484297.
Train: 2018-08-05T00:35:44.607682: step 14352, loss 0.539537.
Train: 2018-08-05T00:35:44.857623: step 14353, loss 0.563127.
Train: 2018-08-05T00:35:45.107534: step 14354, loss 0.555298.
Train: 2018-08-05T00:35:45.357501: step 14355, loss 0.641724.
Train: 2018-08-05T00:35:45.607418: step 14356, loss 0.56321.
Train: 2018-08-05T00:35:45.872982: step 14357, loss 0.492773.
Train: 2018-08-05T00:35:46.122952: step 14358, loss 0.563224.
Train: 2018-08-05T00:35:46.388514: step 14359, loss 0.476996.
Train: 2018-08-05T00:35:46.638426: step 14360, loss 0.586734.
Test: 2018-08-05T00:35:47.888133: step 14360, loss 0.54978.
Train: 2018-08-05T00:35:48.122455: step 14361, loss 0.586723.
Train: 2018-08-05T00:35:48.372425: step 14362, loss 0.50787.
Train: 2018-08-05T00:35:48.622361: step 14363, loss 0.586785.
Train: 2018-08-05T00:35:48.872277: step 14364, loss 0.491672.
Train: 2018-08-05T00:35:49.122249: step 14365, loss 0.547048.
Train: 2018-08-05T00:35:49.372162: step 14366, loss 0.602847.
Train: 2018-08-05T00:35:49.637724: step 14367, loss 0.586876.
Train: 2018-08-05T00:35:49.872075: step 14368, loss 0.554791.
Train: 2018-08-05T00:35:50.122015: step 14369, loss 0.635094.
Train: 2018-08-05T00:35:50.371928: step 14370, loss 0.611019.
Test: 2018-08-05T00:35:51.621634: step 14370, loss 0.549765.
Train: 2018-08-05T00:35:51.871605: step 14371, loss 0.554766.
Train: 2018-08-05T00:35:52.121548: step 14372, loss 0.570825.
Train: 2018-08-05T00:35:52.371458: step 14373, loss 0.610966.
Train: 2018-08-05T00:35:52.621430: step 14374, loss 0.594904.
Train: 2018-08-05T00:35:52.871372: step 14375, loss 0.490911.
Train: 2018-08-05T00:35:53.121313: step 14376, loss 0.626839.
Train: 2018-08-05T00:35:53.386876: step 14377, loss 0.507008.
Train: 2018-08-05T00:35:53.636787: step 14378, loss 0.594849.
Train: 2018-08-05T00:35:53.886759: step 14379, loss 0.562893.
Train: 2018-08-05T00:35:54.136701: step 14380, loss 0.554926.
Test: 2018-08-05T00:35:55.386377: step 14380, loss 0.549586.
Train: 2018-08-05T00:35:55.636349: step 14381, loss 0.538982.
Train: 2018-08-05T00:35:55.886261: step 14382, loss 0.570881.
Train: 2018-08-05T00:35:56.136232: step 14383, loss 0.538936.
Train: 2018-08-05T00:35:56.386144: step 14384, loss 0.554876.
Train: 2018-08-05T00:35:56.636085: step 14385, loss 0.610868.
Train: 2018-08-05T00:35:56.886051: step 14386, loss 0.594875.
Train: 2018-08-05T00:35:57.135998: step 14387, loss 0.474866.
Train: 2018-08-05T00:35:57.385939: step 14388, loss 0.586881.
Train: 2018-08-05T00:35:57.651502: step 14389, loss 0.634995.
Train: 2018-08-05T00:35:57.901439: step 14390, loss 0.55482.
Test: 2018-08-05T00:35:59.151121: step 14390, loss 0.549827.
Train: 2018-08-05T00:35:59.385466: step 14391, loss 0.57887.
Train: 2018-08-05T00:35:59.635413: step 14392, loss 0.586866.
Train: 2018-08-05T00:35:59.885323: step 14393, loss 0.554845.
Train: 2018-08-05T00:36:00.135290: step 14394, loss 0.530887.
Train: 2018-08-05T00:36:00.385237: step 14395, loss 0.546878.
Train: 2018-08-05T00:36:00.635182: step 14396, loss 0.602891.
Train: 2018-08-05T00:36:00.885090: step 14397, loss 0.562843.
Train: 2018-08-05T00:36:01.135061: step 14398, loss 0.530843.
Train: 2018-08-05T00:36:01.385003: step 14399, loss 0.562834.
Train: 2018-08-05T00:36:01.634944: step 14400, loss 0.602956.
Test: 2018-08-05T00:36:02.884622: step 14400, loss 0.548712.
Train: 2018-08-05T00:36:03.743825: step 14401, loss 0.562825.
Train: 2018-08-05T00:36:03.993767: step 14402, loss 0.538792.
Train: 2018-08-05T00:36:04.243679: step 14403, loss 0.538796.
Train: 2018-08-05T00:36:04.493644: step 14404, loss 0.506585.
Train: 2018-08-05T00:36:04.743591: step 14405, loss 0.562759.
Train: 2018-08-05T00:36:05.009154: step 14406, loss 0.578889.
Train: 2018-08-05T00:36:05.259096: step 14407, loss 0.554657.
Train: 2018-08-05T00:36:05.509006: step 14408, loss 0.546489.
Train: 2018-08-05T00:36:05.758948: step 14409, loss 0.595226.
Train: 2018-08-05T00:36:06.008890: step 14410, loss 0.627736.
Test: 2018-08-05T00:36:07.258598: step 14410, loss 0.547784.
Train: 2018-08-05T00:36:07.508569: step 14411, loss 0.481373.
Train: 2018-08-05T00:36:07.758479: step 14412, loss 0.505648.
Train: 2018-08-05T00:36:08.008452: step 14413, loss 0.554427.
Train: 2018-08-05T00:36:08.258393: step 14414, loss 0.56276.
Train: 2018-08-05T00:36:08.508304: step 14415, loss 0.628044.
Train: 2018-08-05T00:36:08.758276: step 14416, loss 0.579064.
Train: 2018-08-05T00:36:09.008217: step 14417, loss 0.521627.
Train: 2018-08-05T00:36:09.258160: step 14418, loss 0.570615.
Train: 2018-08-05T00:36:09.508096: step 14419, loss 0.579002.
Train: 2018-08-05T00:36:09.773675: step 14420, loss 0.521711.
Test: 2018-08-05T00:36:11.007719: step 14420, loss 0.549946.
Train: 2018-08-05T00:36:11.242070: step 14421, loss 0.521441.
Train: 2018-08-05T00:36:11.491982: step 14422, loss 0.578991.
Train: 2018-08-05T00:36:11.741923: step 14423, loss 0.5296.
Train: 2018-08-05T00:36:11.991894: step 14424, loss 0.479911.
Train: 2018-08-05T00:36:12.241806: step 14425, loss 0.504397.
Train: 2018-08-05T00:36:12.491746: step 14426, loss 0.562477.
Train: 2018-08-05T00:36:12.741719: step 14427, loss 0.570816.
Train: 2018-08-05T00:36:12.991660: step 14428, loss 0.579012.
Train: 2018-08-05T00:36:13.241571: step 14429, loss 0.529291.
Train: 2018-08-05T00:36:13.507168: step 14430, loss 0.520313.
Test: 2018-08-05T00:36:14.772462: step 14430, loss 0.548609.
Train: 2018-08-05T00:36:15.006813: step 14431, loss 0.604308.
Train: 2018-08-05T00:36:15.256755: step 14432, loss 0.494584.
Train: 2018-08-05T00:36:15.506665: step 14433, loss 0.554084.
Train: 2018-08-05T00:36:15.756619: step 14434, loss 0.579279.
Train: 2018-08-05T00:36:16.037792: step 14435, loss 0.579339.
Train: 2018-08-05T00:36:16.287763: step 14436, loss 0.545525.
Train: 2018-08-05T00:36:16.537704: step 14437, loss 0.596917.
Train: 2018-08-05T00:36:16.787646: step 14438, loss 0.613512.
Train: 2018-08-05T00:36:17.037587: step 14439, loss 0.621897.
Train: 2018-08-05T00:36:17.287529: step 14440, loss 0.545164.
Test: 2018-08-05T00:36:18.537205: step 14440, loss 0.547474.
Train: 2018-08-05T00:36:18.771527: step 14441, loss 0.494607.
Train: 2018-08-05T00:36:19.021498: step 14442, loss 0.562348.
Train: 2018-08-05T00:36:19.271440: step 14443, loss 0.587804.
Train: 2018-08-05T00:36:19.521351: step 14444, loss 0.612974.
Train: 2018-08-05T00:36:19.771322: step 14445, loss 0.495289.
Train: 2018-08-05T00:36:20.021264: step 14446, loss 0.537255.
Train: 2018-08-05T00:36:20.286796: step 14447, loss 0.579159.
Train: 2018-08-05T00:36:20.536773: step 14448, loss 0.570766.
Train: 2018-08-05T00:36:20.786709: step 14449, loss 0.587473.
Train: 2018-08-05T00:36:21.036651: step 14450, loss 0.587442.
Test: 2018-08-05T00:36:22.286328: step 14450, loss 0.54873.
Train: 2018-08-05T00:36:22.536271: step 14451, loss 0.562442.
Train: 2018-08-05T00:36:22.786241: step 14452, loss 0.603974.
Train: 2018-08-05T00:36:23.036182: step 14453, loss 0.512777.
Train: 2018-08-05T00:36:23.286124: step 14454, loss 0.512858.
Train: 2018-08-05T00:36:23.536035: step 14455, loss 0.562497.
Train: 2018-08-05T00:36:23.801628: step 14456, loss 0.471579.
Train: 2018-08-05T00:36:24.051540: step 14457, loss 0.628685.
Train: 2018-08-05T00:36:24.301511: step 14458, loss 0.612126.
Train: 2018-08-05T00:36:24.551453: step 14459, loss 0.59555.
Train: 2018-08-05T00:36:24.801394: step 14460, loss 0.537753.
Test: 2018-08-05T00:36:26.051072: step 14460, loss 0.547807.
Train: 2018-08-05T00:36:26.301037: step 14461, loss 0.5378.
Train: 2018-08-05T00:36:26.550989: step 14462, loss 0.488408.
Train: 2018-08-05T00:36:26.800926: step 14463, loss 0.537785.
Train: 2018-08-05T00:36:27.050837: step 14464, loss 0.512986.
Train: 2018-08-05T00:36:27.300779: step 14465, loss 0.529401.
Train: 2018-08-05T00:36:27.550750: step 14466, loss 0.504422.
Train: 2018-08-05T00:36:27.800692: step 14467, loss 0.57908.
Train: 2018-08-05T00:36:28.050629: step 14468, loss 0.570765.
Train: 2018-08-05T00:36:28.300570: step 14469, loss 0.679436.
Train: 2018-08-05T00:36:28.550517: step 14470, loss 0.537357.
Test: 2018-08-05T00:36:29.800194: step 14470, loss 0.547699.
Train: 2018-08-05T00:36:30.050165: step 14471, loss 0.52901.
Train: 2018-08-05T00:36:30.315729: step 14472, loss 0.579121.
Train: 2018-08-05T00:36:30.550049: step 14473, loss 0.612547.
Train: 2018-08-05T00:36:30.799989: step 14474, loss 0.570771.
Train: 2018-08-05T00:36:31.049932: step 14475, loss 0.612435.
Train: 2018-08-05T00:36:31.299843: step 14476, loss 0.554133.
Train: 2018-08-05T00:36:31.549815: step 14477, loss 0.595652.
Train: 2018-08-05T00:36:31.799755: step 14478, loss 0.628691.
Train: 2018-08-05T00:36:32.049697: step 14479, loss 0.579002.
Train: 2018-08-05T00:36:32.299638: step 14480, loss 0.578975.
Test: 2018-08-05T00:36:33.549316: step 14480, loss 0.549668.
Train: 2018-08-05T00:36:33.814879: step 14481, loss 0.53803.
Train: 2018-08-05T00:36:34.064850: step 14482, loss 0.538131.
Train: 2018-08-05T00:36:34.314791: step 14483, loss 0.587064.
Train: 2018-08-05T00:36:34.564733: step 14484, loss 0.554539.
Train: 2018-08-05T00:36:34.814675: step 14485, loss 0.570797.
Train: 2018-08-05T00:36:35.064616: step 14486, loss 0.481805.
Train: 2018-08-05T00:36:35.314552: step 14487, loss 0.514163.
Train: 2018-08-05T00:36:35.580124: step 14488, loss 0.514101.
Train: 2018-08-05T00:36:35.830065: step 14489, loss 0.587027.
Train: 2018-08-05T00:36:36.079972: step 14490, loss 0.603294.
Test: 2018-08-05T00:36:37.329679: step 14490, loss 0.549636.
Train: 2018-08-05T00:36:37.564033: step 14491, loss 0.587045.
Train: 2018-08-05T00:36:37.813972: step 14492, loss 0.570783.
Train: 2018-08-05T00:36:38.063916: step 14493, loss 0.562659.
Train: 2018-08-05T00:36:38.313859: step 14494, loss 0.603279.
Train: 2018-08-05T00:36:38.563796: step 14495, loss 0.562676.
Train: 2018-08-05T00:36:38.766873: step 14496, loss 0.562687.
Train: 2018-08-05T00:36:39.016816: step 14497, loss 0.562697.
Train: 2018-08-05T00:36:39.266726: step 14498, loss 0.554612.
Train: 2018-08-05T00:36:39.516668: step 14499, loss 0.514167.
Train: 2018-08-05T00:36:39.766609: step 14500, loss 0.530317.
Test: 2018-08-05T00:36:41.016317: step 14500, loss 0.548026.
Train: 2018-08-05T00:36:41.859900: step 14501, loss 0.55458.
Train: 2018-08-05T00:36:42.109844: step 14502, loss 0.570786.
Train: 2018-08-05T00:36:42.359783: step 14503, loss 0.611442.
Train: 2018-08-05T00:36:42.609693: step 14504, loss 0.562657.
Train: 2018-08-05T00:36:42.859665: step 14505, loss 0.595165.
Train: 2018-08-05T00:36:43.109608: step 14506, loss 0.546421.
Train: 2018-08-05T00:36:43.359518: step 14507, loss 0.587025.
Train: 2018-08-05T00:36:43.609490: step 14508, loss 0.505883.
Train: 2018-08-05T00:36:43.859431: step 14509, loss 0.538317.
Train: 2018-08-05T00:36:44.109373: step 14510, loss 0.57891.
Test: 2018-08-05T00:36:45.359050: step 14510, loss 0.549631.
Train: 2018-08-05T00:36:45.593401: step 14511, loss 0.538253.
Train: 2018-08-05T00:36:45.843342: step 14512, loss 0.481226.
Train: 2018-08-05T00:36:46.093283: step 14513, loss 0.587099.
Train: 2018-08-05T00:36:46.343222: step 14514, loss 0.497161.
Train: 2018-08-05T00:36:46.593160: step 14515, loss 0.58718.
Train: 2018-08-05T00:36:46.858701: step 14516, loss 0.504975.
Train: 2018-08-05T00:36:47.108640: step 14517, loss 0.612007.
Train: 2018-08-05T00:36:47.358612: step 14518, loss 0.554226.
Train: 2018-08-05T00:36:47.608553: step 14519, loss 0.603869.
Train: 2018-08-05T00:36:47.874086: step 14520, loss 0.570755.
Test: 2018-08-05T00:36:49.108172: step 14520, loss 0.548829.
Train: 2018-08-05T00:36:49.373735: step 14521, loss 0.545906.
Train: 2018-08-05T00:36:49.608086: step 14522, loss 0.637063.
Train: 2018-08-05T00:36:49.858027: step 14523, loss 0.554199.
Train: 2018-08-05T00:36:50.107968: step 14524, loss 0.579026.
Train: 2018-08-05T00:36:50.357909: step 14525, loss 0.645104.
Train: 2018-08-05T00:36:50.607851: step 14526, loss 0.521334.
Train: 2018-08-05T00:36:50.857793: step 14527, loss 0.537868.
Train: 2018-08-05T00:36:51.107734: step 14528, loss 0.578973.
Train: 2018-08-05T00:36:51.357676: step 14529, loss 0.578964.
Train: 2018-08-05T00:36:51.607618: step 14530, loss 0.538011.
Test: 2018-08-05T00:36:52.841673: step 14530, loss 0.549489.
Train: 2018-08-05T00:36:53.091614: step 14531, loss 0.595308.
Train: 2018-08-05T00:36:53.341586: step 14532, loss 0.554434.
Train: 2018-08-05T00:36:53.607143: step 14533, loss 0.636029.
Train: 2018-08-05T00:36:53.841465: step 14534, loss 0.521969.
Train: 2018-08-05T00:36:54.091410: step 14535, loss 0.554542.
Train: 2018-08-05T00:36:54.341347: step 14536, loss 0.603244.
Train: 2018-08-05T00:36:54.606915: step 14537, loss 0.570794.
Train: 2018-08-05T00:36:54.856856: step 14538, loss 0.514233.
Train: 2018-08-05T00:36:55.106797: step 14539, loss 0.595037.
Train: 2018-08-05T00:36:55.356739: step 14540, loss 0.586943.
Test: 2018-08-05T00:36:56.605802: step 14540, loss 0.549125.
Train: 2018-08-05T00:36:56.840153: step 14541, loss 0.619157.
Train: 2018-08-05T00:36:57.090094: step 14542, loss 0.611014.
Train: 2018-08-05T00:36:57.340037: step 14543, loss 0.498777.
Train: 2018-08-05T00:36:57.589978: step 14544, loss 0.578863.
Train: 2018-08-05T00:36:57.839918: step 14545, loss 0.506994.
Train: 2018-08-05T00:36:58.089860: step 14546, loss 0.610835.
Train: 2018-08-05T00:36:58.339802: step 14547, loss 0.626717.
Train: 2018-08-05T00:36:58.589744: step 14548, loss 0.618633.
Train: 2018-08-05T00:36:58.870931: step 14549, loss 0.58679.
Train: 2018-08-05T00:36:59.105218: step 14550, loss 0.570957.
Test: 2018-08-05T00:37:00.370546: step 14550, loss 0.550304.
Train: 2018-08-05T00:37:00.604899: step 14551, loss 0.602509.
Train: 2018-08-05T00:37:00.854838: step 14552, loss 0.563162.
Train: 2018-08-05T00:37:01.104779: step 14553, loss 0.618043.
Train: 2018-08-05T00:37:01.354723: step 14554, loss 0.500857.
Train: 2018-08-05T00:37:01.604631: step 14555, loss 0.508785.
Train: 2018-08-05T00:37:01.854604: step 14556, loss 0.53215.
Train: 2018-08-05T00:37:02.104545: step 14557, loss 0.563305.
Train: 2018-08-05T00:37:02.354489: step 14558, loss 0.539864.
Train: 2018-08-05T00:37:02.604397: step 14559, loss 0.516323.
Train: 2018-08-05T00:37:02.854369: step 14560, loss 0.586717.
Test: 2018-08-05T00:37:04.104046: step 14560, loss 0.55003.
Train: 2018-08-05T00:37:04.354017: step 14561, loss 0.555302.
Train: 2018-08-05T00:37:04.603959: step 14562, loss 0.531614.
Train: 2018-08-05T00:37:04.853901: step 14563, loss 0.570955.
Train: 2018-08-05T00:37:05.119468: step 14564, loss 0.531332.
Train: 2018-08-05T00:37:05.369375: step 14565, loss 0.483509.
Train: 2018-08-05T00:37:05.619318: step 14566, loss 0.57886.
Train: 2018-08-05T00:37:05.869292: step 14567, loss 0.634973.
Train: 2018-08-05T00:37:06.119234: step 14568, loss 0.554774.
Train: 2018-08-05T00:37:06.369175: step 14569, loss 0.450098.
Train: 2018-08-05T00:37:06.619083: step 14570, loss 0.643557.
Test: 2018-08-05T00:37:07.884411: step 14570, loss 0.550096.
Train: 2018-08-05T00:37:08.134354: step 14571, loss 0.49788.
Train: 2018-08-05T00:37:08.384295: step 14572, loss 0.603296.
Train: 2018-08-05T00:37:08.634265: step 14573, loss 0.595216.
Train: 2018-08-05T00:37:08.899829: step 14574, loss 0.562619.
Train: 2018-08-05T00:37:09.149741: step 14575, loss 0.529932.
Train: 2018-08-05T00:37:09.399712: step 14576, loss 0.554405.
Train: 2018-08-05T00:37:09.649653: step 14577, loss 0.636326.
Train: 2018-08-05T00:37:09.899595: step 14578, loss 0.537982.
Train: 2018-08-05T00:37:10.149507: step 14579, loss 0.578961.
Train: 2018-08-05T00:37:10.399448: step 14580, loss 0.521564.
Test: 2018-08-05T00:37:11.649154: step 14580, loss 0.54828.
Train: 2018-08-05T00:37:11.899127: step 14581, loss 0.513311.
Train: 2018-08-05T00:37:12.149068: step 14582, loss 0.578986.
Train: 2018-08-05T00:37:12.399009: step 14583, loss 0.537828.
Train: 2018-08-05T00:37:12.648951: step 14584, loss 0.471814.
Train: 2018-08-05T00:37:12.898886: step 14585, loss 0.653477.
Train: 2018-08-05T00:37:13.148833: step 14586, loss 0.579031.
Train: 2018-08-05T00:37:13.398770: step 14587, loss 0.570778.
Train: 2018-08-05T00:37:13.648717: step 14588, loss 0.496197.
Train: 2018-08-05T00:37:13.898634: step 14589, loss 0.60395.
Train: 2018-08-05T00:37:14.148599: step 14590, loss 0.587356.
Test: 2018-08-05T00:37:15.398277: step 14590, loss 0.548223.
Train: 2018-08-05T00:37:15.632598: step 14591, loss 0.496086.
Train: 2018-08-05T00:37:15.882538: step 14592, loss 0.579065.
Train: 2018-08-05T00:37:16.132481: step 14593, loss 0.570754.
Train: 2018-08-05T00:37:16.398051: step 14594, loss 0.529199.
Train: 2018-08-05T00:37:16.648014: step 14595, loss 0.678909.
Train: 2018-08-05T00:37:16.897956: step 14596, loss 0.504325.
Train: 2018-08-05T00:37:17.147868: step 14597, loss 0.653763.
Train: 2018-08-05T00:37:17.397809: step 14598, loss 0.595598.
Train: 2018-08-05T00:37:17.647780: step 14599, loss 0.562494.
Train: 2018-08-05T00:37:17.897691: step 14600, loss 0.521351.
Test: 2018-08-05T00:37:19.147399: step 14600, loss 0.548806.
Train: 2018-08-05T00:37:19.990952: step 14601, loss 0.52965.
Train: 2018-08-05T00:37:20.240893: step 14602, loss 0.546114.
Train: 2018-08-05T00:37:20.490868: step 14603, loss 0.578973.
Train: 2018-08-05T00:37:20.740775: step 14604, loss 0.521529.
Train: 2018-08-05T00:37:21.006338: step 14605, loss 0.578967.
Train: 2018-08-05T00:37:21.271932: step 14606, loss 0.554352.
Train: 2018-08-05T00:37:21.521873: step 14607, loss 0.546149.
Train: 2018-08-05T00:37:21.771814: step 14608, loss 0.513319.
Train: 2018-08-05T00:37:22.021757: step 14609, loss 0.595406.
Train: 2018-08-05T00:37:22.287289: step 14610, loss 0.529662.
Test: 2018-08-05T00:37:23.536995: step 14610, loss 0.547653.
Train: 2018-08-05T00:37:23.771343: step 14611, loss 0.570758.
Train: 2018-08-05T00:37:24.021288: step 14612, loss 0.578995.
Train: 2018-08-05T00:37:24.271198: step 14613, loss 0.570752.
Train: 2018-08-05T00:37:24.521170: step 14614, loss 0.636636.
Train: 2018-08-05T00:37:24.771081: step 14615, loss 0.521428.
Train: 2018-08-05T00:37:25.021055: step 14616, loss 0.570766.
Train: 2018-08-05T00:37:25.270964: step 14617, loss 0.513279.
Train: 2018-08-05T00:37:25.520936: step 14618, loss 0.513268.
Train: 2018-08-05T00:37:25.770848: step 14619, loss 0.521417.
Train: 2018-08-05T00:37:26.020819: step 14620, loss 0.488353.
Test: 2018-08-05T00:37:27.270496: step 14620, loss 0.547725.
Train: 2018-08-05T00:37:27.504842: step 14621, loss 0.554209.
Train: 2018-08-05T00:37:27.754758: step 14622, loss 0.554162.
Train: 2018-08-05T00:37:28.004730: step 14623, loss 0.545762.
Train: 2018-08-05T00:37:28.254674: step 14624, loss 0.537221.
Train: 2018-08-05T00:37:28.504582: step 14625, loss 0.545538.
Train: 2018-08-05T00:37:28.754525: step 14626, loss 0.672004.
Train: 2018-08-05T00:37:29.020092: step 14627, loss 0.536568.
Train: 2018-08-05T00:37:29.254438: step 14628, loss 0.587526.
Train: 2018-08-05T00:37:29.504348: step 14629, loss 0.562914.
Train: 2018-08-05T00:37:29.754320: step 14630, loss 0.604687.
Test: 2018-08-05T00:37:31.019617: step 14630, loss 0.547898.
Train: 2018-08-05T00:37:31.316457: step 14631, loss 0.545602.
Train: 2018-08-05T00:37:31.566396: step 14632, loss 0.528615.
Train: 2018-08-05T00:37:31.816307: step 14633, loss 0.520204.
Train: 2018-08-05T00:37:32.066279: step 14634, loss 0.545528.
Train: 2018-08-05T00:37:32.316220: step 14635, loss 0.528599.
Train: 2018-08-05T00:37:32.566156: step 14636, loss 0.562066.
Train: 2018-08-05T00:37:32.816097: step 14637, loss 0.578363.
Train: 2018-08-05T00:37:33.066014: step 14638, loss 0.495197.
Train: 2018-08-05T00:37:33.315989: step 14639, loss 0.527891.
Train: 2018-08-05T00:37:33.565898: step 14640, loss 0.492952.
Test: 2018-08-05T00:37:34.815604: step 14640, loss 0.548181.
Train: 2018-08-05T00:37:35.065571: step 14641, loss 0.609213.
Train: 2018-08-05T00:37:35.331110: step 14642, loss 0.484514.
Train: 2018-08-05T00:37:35.581052: step 14643, loss 0.639055.
Train: 2018-08-05T00:37:35.830993: step 14644, loss 0.626441.
Train: 2018-08-05T00:37:36.080934: step 14645, loss 0.666723.
Train: 2018-08-05T00:37:36.330905: step 14646, loss 0.588329.
Train: 2018-08-05T00:37:36.518331: step 14647, loss 0.65392.
Train: 2018-08-05T00:37:36.783920: step 14648, loss 0.604483.
Train: 2018-08-05T00:37:37.018244: step 14649, loss 0.579173.
Train: 2018-08-05T00:37:37.268155: step 14650, loss 0.50483.
Test: 2018-08-05T00:37:38.517863: step 14650, loss 0.548278.
Train: 2018-08-05T00:37:38.752208: step 14651, loss 0.546139.
Train: 2018-08-05T00:37:39.002125: step 14652, loss 0.603528.
Train: 2018-08-05T00:37:39.252090: step 14653, loss 0.53811.
Train: 2018-08-05T00:37:39.502008: step 14654, loss 0.611473.
Train: 2018-08-05T00:37:39.751979: step 14655, loss 0.595144.
Train: 2018-08-05T00:37:40.001920: step 14656, loss 0.595268.
Train: 2018-08-05T00:37:40.267453: step 14657, loss 0.522314.
Train: 2018-08-05T00:37:40.501807: step 14658, loss 0.603063.
Train: 2018-08-05T00:37:40.751740: step 14659, loss 0.498443.
Train: 2018-08-05T00:37:41.001686: step 14660, loss 0.554921.
Test: 2018-08-05T00:37:42.251364: step 14660, loss 0.549138.
Train: 2018-08-05T00:37:42.485718: step 14661, loss 0.578909.
Train: 2018-08-05T00:37:42.735650: step 14662, loss 0.546809.
Train: 2018-08-05T00:37:43.001218: step 14663, loss 0.547333.
Train: 2018-08-05T00:37:43.251159: step 14664, loss 0.666551.
Train: 2018-08-05T00:37:43.501100: step 14665, loss 0.563277.
Train: 2018-08-05T00:37:43.751042: step 14666, loss 0.562747.
Train: 2018-08-05T00:37:44.000984: step 14667, loss 0.523131.
Train: 2018-08-05T00:37:44.250926: step 14668, loss 0.547198.
Train: 2018-08-05T00:37:44.500867: step 14669, loss 0.602645.
Train: 2018-08-05T00:37:44.750803: step 14670, loss 0.563019.
Test: 2018-08-05T00:37:46.016106: step 14670, loss 0.548153.
Train: 2018-08-05T00:37:46.250428: step 14671, loss 0.475812.
Train: 2018-08-05T00:37:46.500369: step 14672, loss 0.626909.
Train: 2018-08-05T00:37:46.750340: step 14673, loss 0.587214.
Train: 2018-08-05T00:37:47.015881: step 14674, loss 0.563149.
Train: 2018-08-05T00:37:47.281436: step 14675, loss 0.459802.
Train: 2018-08-05T00:37:47.515757: step 14676, loss 0.515252.
Train: 2018-08-05T00:37:47.765727: step 14677, loss 0.618613.
Train: 2018-08-05T00:37:48.015639: step 14678, loss 0.594958.
Train: 2018-08-05T00:37:48.265610: step 14679, loss 0.506848.
Train: 2018-08-05T00:37:48.515552: step 14680, loss 0.554894.
Test: 2018-08-05T00:37:49.765228: step 14680, loss 0.549459.
Train: 2018-08-05T00:37:49.999581: step 14681, loss 0.546685.
Train: 2018-08-05T00:37:50.249490: step 14682, loss 0.602931.
Train: 2018-08-05T00:37:50.499462: step 14683, loss 0.610944.
Train: 2018-08-05T00:37:50.749403: step 14684, loss 0.602828.
Train: 2018-08-05T00:37:50.999345: step 14685, loss 0.611045.
Train: 2018-08-05T00:37:51.249281: step 14686, loss 0.546818.
Train: 2018-08-05T00:37:51.499228: step 14687, loss 0.514709.
Train: 2018-08-05T00:37:51.749170: step 14688, loss 0.603021.
Train: 2018-08-05T00:37:51.999105: step 14689, loss 0.594701.
Train: 2018-08-05T00:37:52.249052: step 14690, loss 0.650958.
Test: 2018-08-05T00:37:53.514350: step 14690, loss 0.54903.
Train: 2018-08-05T00:37:53.748702: step 14691, loss 0.562886.
Train: 2018-08-05T00:37:53.998643: step 14692, loss 0.658454.
Train: 2018-08-05T00:37:54.248584: step 14693, loss 0.531426.
Train: 2018-08-05T00:37:54.514147: step 14694, loss 0.563091.
Train: 2018-08-05T00:37:54.764088: step 14695, loss 0.531525.
Train: 2018-08-05T00:37:55.013999: step 14696, loss 0.555272.
Train: 2018-08-05T00:37:55.263971: step 14697, loss 0.586745.
Train: 2018-08-05T00:37:55.513883: step 14698, loss 0.508236.
Train: 2018-08-05T00:37:55.763854: step 14699, loss 0.578884.
Train: 2018-08-05T00:37:56.013767: step 14700, loss 0.610301.
Test: 2018-08-05T00:37:57.263473: step 14700, loss 0.550618.
Train: 2018-08-05T00:37:58.122677: step 14701, loss 0.641628.
Train: 2018-08-05T00:37:58.372589: step 14702, loss 0.531906.
Train: 2018-08-05T00:37:58.638151: step 14703, loss 0.594535.
Train: 2018-08-05T00:37:58.872502: step 14704, loss 0.516432.
Train: 2018-08-05T00:37:59.138063: step 14705, loss 0.578892.
Train: 2018-08-05T00:37:59.387975: step 14706, loss 0.53207.
Train: 2018-08-05T00:37:59.637947: step 14707, loss 0.547647.
Train: 2018-08-05T00:37:59.887888: step 14708, loss 0.516338.
Train: 2018-08-05T00:38:00.137831: step 14709, loss 0.476997.
Train: 2018-08-05T00:38:00.387742: step 14710, loss 0.547389.
Test: 2018-08-05T00:38:01.653071: step 14710, loss 0.549495.
Train: 2018-08-05T00:38:01.887422: step 14711, loss 0.57886.
Train: 2018-08-05T00:38:02.137362: step 14712, loss 0.594719.
Train: 2018-08-05T00:38:02.387273: step 14713, loss 0.539097.
Train: 2018-08-05T00:38:02.637244: step 14714, loss 0.602788.
Train: 2018-08-05T00:38:02.887180: step 14715, loss 0.57087.
Train: 2018-08-05T00:38:03.137122: step 14716, loss 0.554847.
Train: 2018-08-05T00:38:03.387070: step 14717, loss 0.570848.
Train: 2018-08-05T00:38:03.637011: step 14718, loss 0.562807.
Train: 2018-08-05T00:38:03.886952: step 14719, loss 0.570831.
Train: 2018-08-05T00:38:04.136894: step 14720, loss 0.570825.
Test: 2018-08-05T00:38:05.386570: step 14720, loss 0.549669.
Train: 2018-08-05T00:38:05.620922: step 14721, loss 0.506338.
Train: 2018-08-05T00:38:05.870833: step 14722, loss 0.562732.
Train: 2018-08-05T00:38:06.120804: step 14723, loss 0.562708.
Train: 2018-08-05T00:38:06.370746: step 14724, loss 0.5789.
Train: 2018-08-05T00:38:06.620687: step 14725, loss 0.570789.
Train: 2018-08-05T00:38:06.870629: step 14726, loss 0.603286.
Train: 2018-08-05T00:38:07.120569: step 14727, loss 0.562659.
Train: 2018-08-05T00:38:07.386102: step 14728, loss 0.497649.
Train: 2018-08-05T00:38:07.636074: step 14729, loss 0.554504.
Train: 2018-08-05T00:38:07.886010: step 14730, loss 0.505577.
Test: 2018-08-05T00:38:09.135692: step 14730, loss 0.548014.
Train: 2018-08-05T00:38:09.385664: step 14731, loss 0.61162.
Train: 2018-08-05T00:38:09.635607: step 14732, loss 0.480783.
Train: 2018-08-05T00:38:09.885547: step 14733, loss 0.636385.
Train: 2018-08-05T00:38:10.135489: step 14734, loss 0.521496.
Train: 2018-08-05T00:38:10.385400: step 14735, loss 0.570758.
Train: 2018-08-05T00:38:10.635372: step 14736, loss 0.578992.
Train: 2018-08-05T00:38:10.885284: step 14737, loss 0.529551.
Train: 2018-08-05T00:38:11.135224: step 14738, loss 0.529496.
Train: 2018-08-05T00:38:11.385197: step 14739, loss 0.620357.
Train: 2018-08-05T00:38:11.635137: step 14740, loss 0.496325.
Test: 2018-08-05T00:38:12.900436: step 14740, loss 0.549402.
Train: 2018-08-05T00:38:13.134788: step 14741, loss 0.670159.
Train: 2018-08-05T00:38:13.384697: step 14742, loss 0.579034.
Train: 2018-08-05T00:38:13.634669: step 14743, loss 0.488077.
Train: 2018-08-05T00:38:13.884580: step 14744, loss 0.5873.
Train: 2018-08-05T00:38:14.134552: step 14745, loss 0.545945.
Train: 2018-08-05T00:38:14.384494: step 14746, loss 0.612113.
Train: 2018-08-05T00:38:14.634435: step 14747, loss 0.570756.
Train: 2018-08-05T00:38:14.884376: step 14748, loss 0.636792.
Train: 2018-08-05T00:38:15.134318: step 14749, loss 0.546056.
Train: 2018-08-05T00:38:15.384262: step 14750, loss 0.53789.
Test: 2018-08-05T00:38:16.633937: step 14750, loss 0.54866.
Train: 2018-08-05T00:38:16.868256: step 14751, loss 0.537935.
Train: 2018-08-05T00:38:17.165093: step 14752, loss 0.595363.
Train: 2018-08-05T00:38:17.415003: step 14753, loss 0.521628.
Train: 2018-08-05T00:38:17.664976: step 14754, loss 0.619882.
Train: 2018-08-05T00:38:17.914886: step 14755, loss 0.562594.
Train: 2018-08-05T00:38:18.164829: step 14756, loss 0.513638.
Train: 2018-08-05T00:38:18.414794: step 14757, loss 0.619731.
Train: 2018-08-05T00:38:18.664741: step 14758, loss 0.521886.
Train: 2018-08-05T00:38:18.914683: step 14759, loss 0.554488.
Train: 2018-08-05T00:38:19.164624: step 14760, loss 0.660351.
Test: 2018-08-05T00:38:20.429923: step 14760, loss 0.549277.
Train: 2018-08-05T00:38:20.664274: step 14761, loss 0.603285.
Train: 2018-08-05T00:38:20.914215: step 14762, loss 0.530284.
Train: 2018-08-05T00:38:21.164126: step 14763, loss 0.538454.
Train: 2018-08-05T00:38:21.414068: step 14764, loss 0.554652.
Train: 2018-08-05T00:38:21.664039: step 14765, loss 0.546596.
Train: 2018-08-05T00:38:21.913952: step 14766, loss 0.603092.
Train: 2018-08-05T00:38:22.163892: step 14767, loss 0.570819.
Train: 2018-08-05T00:38:22.413864: step 14768, loss 0.578877.
Train: 2018-08-05T00:38:22.663805: step 14769, loss 0.546707.
Train: 2018-08-05T00:38:22.913717: step 14770, loss 0.546729.
Test: 2018-08-05T00:38:24.163423: step 14770, loss 0.549388.
Train: 2018-08-05T00:38:24.397776: step 14771, loss 0.570837.
Train: 2018-08-05T00:38:24.663336: step 14772, loss 0.538717.
Train: 2018-08-05T00:38:24.913280: step 14773, loss 0.578871.
Train: 2018-08-05T00:38:25.163220: step 14774, loss 0.586903.
Train: 2018-08-05T00:38:25.413161: step 14775, loss 0.538725.
Train: 2018-08-05T00:38:25.663103: step 14776, loss 0.530689.
Train: 2018-08-05T00:38:25.913014: step 14777, loss 0.546722.
Train: 2018-08-05T00:38:26.162986: step 14778, loss 0.562781.
Train: 2018-08-05T00:38:26.412927: step 14779, loss 0.562765.
Train: 2018-08-05T00:38:26.678460: step 14780, loss 0.554685.
Test: 2018-08-05T00:38:27.928167: step 14780, loss 0.549621.
Train: 2018-08-05T00:38:28.162487: step 14781, loss 0.522359.
Train: 2018-08-05T00:38:28.412430: step 14782, loss 0.546528.
Train: 2018-08-05T00:38:28.662400: step 14783, loss 0.522141.
Train: 2018-08-05T00:38:28.912311: step 14784, loss 0.55452.
Train: 2018-08-05T00:38:29.162283: step 14785, loss 0.627848.
Train: 2018-08-05T00:38:29.412195: step 14786, loss 0.578934.
Train: 2018-08-05T00:38:29.662166: step 14787, loss 0.464594.
Train: 2018-08-05T00:38:29.912107: step 14788, loss 0.529823.
Train: 2018-08-05T00:38:30.162020: step 14789, loss 0.480421.
Train: 2018-08-05T00:38:30.411968: step 14790, loss 0.570756.
Test: 2018-08-05T00:38:31.661668: step 14790, loss 0.548083.
Train: 2018-08-05T00:38:31.895989: step 14791, loss 0.512812.
Train: 2018-08-05T00:38:32.145959: step 14792, loss 0.587385.
Train: 2018-08-05T00:38:32.395901: step 14793, loss 0.520734.
Train: 2018-08-05T00:38:32.661434: step 14794, loss 0.503827.
Train: 2018-08-05T00:38:32.895787: step 14795, loss 0.562384.
Train: 2018-08-05T00:38:33.145725: step 14796, loss 0.579238.
Train: 2018-08-05T00:38:33.395638: step 14797, loss 0.536984.
Train: 2018-08-05T00:38:33.583117: step 14798, loss 0.508064.
Train: 2018-08-05T00:38:33.833068: step 14799, loss 0.511275.
Train: 2018-08-05T00:38:34.083009: step 14800, loss 0.536705.
Test: 2018-08-05T00:38:35.348304: step 14800, loss 0.548999.
Train: 2018-08-05T00:38:36.238751: step 14801, loss 0.528033.
Train: 2018-08-05T00:38:36.488692: step 14802, loss 0.605375.
Train: 2018-08-05T00:38:36.738634: step 14803, loss 0.570969.
Train: 2018-08-05T00:38:36.988576: step 14804, loss 0.579624.
Train: 2018-08-05T00:38:37.254109: step 14805, loss 0.614253.
Train: 2018-08-05T00:38:37.488430: step 14806, loss 0.553693.
Train: 2018-08-05T00:38:37.738370: step 14807, loss 0.553697.
Train: 2018-08-05T00:38:37.988341: step 14808, loss 0.588258.
Train: 2018-08-05T00:38:38.238283: step 14809, loss 0.53645.
Train: 2018-08-05T00:38:38.488224: step 14810, loss 0.570959.
Test: 2018-08-05T00:38:39.737902: step 14810, loss 0.547952.
Train: 2018-08-05T00:38:39.987844: step 14811, loss 0.545111.
Train: 2018-08-05T00:38:40.237784: step 14812, loss 0.639777.
Train: 2018-08-05T00:38:40.487756: step 14813, loss 0.605248.
Train: 2018-08-05T00:38:40.737697: step 14814, loss 0.562336.
Train: 2018-08-05T00:38:40.987639: step 14815, loss 0.613502.
Train: 2018-08-05T00:38:41.237551: step 14816, loss 0.53687.
Train: 2018-08-05T00:38:41.487492: step 14817, loss 0.553893.
Train: 2018-08-05T00:38:41.737463: step 14818, loss 0.5033.
Train: 2018-08-05T00:38:41.987405: step 14819, loss 0.621338.
Train: 2018-08-05T00:38:42.237347: step 14820, loss 0.646378.
Test: 2018-08-05T00:38:43.487023: step 14820, loss 0.548829.
Train: 2018-08-05T00:38:43.721344: step 14821, loss 0.537322.
Train: 2018-08-05T00:38:43.971285: step 14822, loss 0.570764.
Train: 2018-08-05T00:38:44.221227: step 14823, loss 0.570759.
Train: 2018-08-05T00:38:44.471202: step 14824, loss 0.595584.
Train: 2018-08-05T00:38:44.721110: step 14825, loss 0.554268.
Train: 2018-08-05T00:38:44.971050: step 14826, loss 0.578976.
Train: 2018-08-05T00:38:45.221027: step 14827, loss 0.595333.
Train: 2018-08-05T00:38:45.470935: step 14828, loss 0.603409.
Train: 2018-08-05T00:38:45.720905: step 14829, loss 0.530161.
Train: 2018-08-05T00:38:45.970851: step 14830, loss 0.595096.
Test: 2018-08-05T00:38:47.236145: step 14830, loss 0.549633.
Train: 2018-08-05T00:38:47.486088: step 14831, loss 0.570813.
Train: 2018-08-05T00:38:47.736030: step 14832, loss 0.58692.
Train: 2018-08-05T00:38:47.985970: step 14833, loss 0.506706.
Train: 2018-08-05T00:38:48.235942: step 14834, loss 0.570861.
Train: 2018-08-05T00:38:48.485883: step 14835, loss 0.506951.
Train: 2018-08-05T00:38:48.735825: step 14836, loss 0.610815.
Train: 2018-08-05T00:38:48.985766: step 14837, loss 0.467151.
Train: 2018-08-05T00:38:49.235708: step 14838, loss 0.490989.
Train: 2018-08-05T00:38:49.485649: step 14839, loss 0.578865.
Train: 2018-08-05T00:38:49.735590: step 14840, loss 0.538735.
Test: 2018-08-05T00:38:50.985267: step 14840, loss 0.549157.
Train: 2018-08-05T00:38:51.219613: step 14841, loss 0.530594.
Train: 2018-08-05T00:38:51.469529: step 14842, loss 0.619239.
Train: 2018-08-05T00:38:51.719500: step 14843, loss 0.514222.
Train: 2018-08-05T00:38:51.985033: step 14844, loss 0.538383.
Train: 2018-08-05T00:38:52.234974: step 14845, loss 0.570785.
Train: 2018-08-05T00:38:52.500538: step 14846, loss 0.530059.
Train: 2018-08-05T00:38:52.750510: step 14847, loss 0.554437.
Train: 2018-08-05T00:38:53.016043: step 14848, loss 0.554389.
Train: 2018-08-05T00:38:53.266008: step 14849, loss 0.644634.
Train: 2018-08-05T00:38:53.515955: step 14850, loss 0.644673.
Test: 2018-08-05T00:38:54.765632: step 14850, loss 0.549615.
Train: 2018-08-05T00:38:55.015598: step 14851, loss 0.562559.
Train: 2018-08-05T00:38:55.249925: step 14852, loss 0.603537.
Train: 2018-08-05T00:38:55.499866: step 14853, loss 0.636192.
Train: 2018-08-05T00:38:55.749808: step 14854, loss 0.562623.
Train: 2018-08-05T00:38:55.999748: step 14855, loss 0.554527.
Train: 2018-08-05T00:38:56.249690: step 14856, loss 0.530249.
Train: 2018-08-05T00:38:56.499602: step 14857, loss 0.586992.
Train: 2018-08-05T00:38:56.749573: step 14858, loss 0.546561.
Train: 2018-08-05T00:38:56.999485: step 14859, loss 0.514311.
Train: 2018-08-05T00:38:57.265077: step 14860, loss 0.611168.
Test: 2018-08-05T00:38:58.530375: step 14860, loss 0.548919.
Train: 2018-08-05T00:38:58.764696: step 14861, loss 0.54663.
Train: 2018-08-05T00:38:59.014670: step 14862, loss 0.603054.
Train: 2018-08-05T00:38:59.264609: step 14863, loss 0.546683.
Train: 2018-08-05T00:38:59.514550: step 14864, loss 0.546706.
Train: 2018-08-05T00:38:59.764492: step 14865, loss 0.554754.
Train: 2018-08-05T00:39:00.014433: step 14866, loss 0.586912.
Train: 2018-08-05T00:39:00.264370: step 14867, loss 0.60298.
Train: 2018-08-05T00:39:00.514316: step 14868, loss 0.57887.
Train: 2018-08-05T00:39:00.764258: step 14869, loss 0.594897.
Train: 2018-08-05T00:39:01.014200: step 14870, loss 0.570862.
Test: 2018-08-05T00:39:02.263876: step 14870, loss 0.549549.
Train: 2018-08-05T00:39:02.498229: step 14871, loss 0.666717.
Train: 2018-08-05T00:39:02.748138: step 14872, loss 0.499286.
Train: 2018-08-05T00:39:03.013701: step 14873, loss 0.515318.
Train: 2018-08-05T00:39:03.263673: step 14874, loss 0.618551.
Train: 2018-08-05T00:39:03.513583: step 14875, loss 0.539227.
Train: 2018-08-05T00:39:03.763556: step 14876, loss 0.563018.
Train: 2018-08-05T00:39:04.029118: step 14877, loss 0.523446.
Train: 2018-08-05T00:39:04.279060: step 14878, loss 0.578859.
Train: 2018-08-05T00:39:04.528971: step 14879, loss 0.539246.
Train: 2018-08-05T00:39:04.778943: step 14880, loss 0.586788.
Test: 2018-08-05T00:39:06.044240: step 14880, loss 0.55084.
Train: 2018-08-05T00:39:06.278594: step 14881, loss 0.55506.
Train: 2018-08-05T00:39:06.528504: step 14882, loss 0.562982.
Train: 2018-08-05T00:39:06.778476: step 14883, loss 0.547082.
Train: 2018-08-05T00:39:07.044037: step 14884, loss 0.539094.
Train: 2018-08-05T00:39:07.278359: step 14885, loss 0.570893.
Train: 2018-08-05T00:39:07.528269: step 14886, loss 0.538974.
Train: 2018-08-05T00:39:07.778240: step 14887, loss 0.626816.
Train: 2018-08-05T00:39:08.028182: step 14888, loss 0.586858.
Train: 2018-08-05T00:39:08.278123: step 14889, loss 0.594853.
Train: 2018-08-05T00:39:08.528034: step 14890, loss 0.474991.
Test: 2018-08-05T00:39:09.777741: step 14890, loss 0.549314.
Train: 2018-08-05T00:39:10.012093: step 14891, loss 0.522854.
Train: 2018-08-05T00:39:10.262029: step 14892, loss 0.546791.
Train: 2018-08-05T00:39:10.511975: step 14893, loss 0.538681.
Train: 2018-08-05T00:39:10.761916: step 14894, loss 0.530518.
Train: 2018-08-05T00:39:11.011859: step 14895, loss 0.570804.
Train: 2018-08-05T00:39:11.261799: step 14896, loss 0.49782.
Train: 2018-08-05T00:39:11.511741: step 14897, loss 0.546362.
Train: 2018-08-05T00:39:11.761682: step 14898, loss 0.603449.
Train: 2018-08-05T00:39:12.011595: step 14899, loss 0.513428.
Train: 2018-08-05T00:39:12.261536: step 14900, loss 0.554324.
Test: 2018-08-05T00:39:13.526864: step 14900, loss 0.547609.
Train: 2018-08-05T00:39:14.370447: step 14901, loss 0.587242.
Train: 2018-08-05T00:39:14.620359: step 14902, loss 0.570756.
Train: 2018-08-05T00:39:14.870330: step 14903, loss 0.628685.
Train: 2018-08-05T00:39:15.120272: step 14904, loss 0.512818.
Train: 2018-08-05T00:39:15.370213: step 14905, loss 0.537614.
Train: 2018-08-05T00:39:15.620154: step 14906, loss 0.562461.
Train: 2018-08-05T00:39:15.870095: step 14907, loss 0.504301.
Train: 2018-08-05T00:39:16.120006: step 14908, loss 0.479186.
Train: 2018-08-05T00:39:16.369980: step 14909, loss 0.453812.
Train: 2018-08-05T00:39:16.635511: step 14910, loss 0.495212.
Test: 2018-08-05T00:39:17.885219: step 14910, loss 0.548454.
Train: 2018-08-05T00:39:18.119569: step 14911, loss 0.545473.
Train: 2018-08-05T00:39:18.369510: step 14912, loss 0.613291.
Train: 2018-08-05T00:39:18.635075: step 14913, loss 0.545298.
Train: 2018-08-05T00:39:18.885014: step 14914, loss 0.60509.
Train: 2018-08-05T00:39:19.134925: step 14915, loss 0.622308.
Train: 2018-08-05T00:39:19.384867: step 14916, loss 0.553764.
Train: 2018-08-05T00:39:19.634839: step 14917, loss 0.682382.
Train: 2018-08-05T00:39:19.884780: step 14918, loss 0.570893.
Train: 2018-08-05T00:39:20.134716: step 14919, loss 0.545264.
Train: 2018-08-05T00:39:20.384658: step 14920, loss 0.485656.
Test: 2018-08-05T00:39:21.634340: step 14920, loss 0.547335.
Train: 2018-08-05T00:39:21.868661: step 14921, loss 0.511244.
Train: 2018-08-05T00:39:22.118632: step 14922, loss 0.553822.
Train: 2018-08-05T00:39:22.368573: step 14923, loss 0.553819.
Train: 2018-08-05T00:39:22.618516: step 14924, loss 0.604956.
Train: 2018-08-05T00:39:22.868457: step 14925, loss 0.519761.
Train: 2018-08-05T00:39:23.118398: step 14926, loss 0.613432.
Train: 2018-08-05T00:39:23.383961: step 14927, loss 0.562345.
Train: 2018-08-05T00:39:23.633897: step 14928, loss 0.579333.
Train: 2018-08-05T00:39:23.868223: step 14929, loss 0.528437.
Train: 2018-08-05T00:39:24.118164: step 14930, loss 0.545415.
Test: 2018-08-05T00:39:25.367841: step 14930, loss 0.548613.
Train: 2018-08-05T00:39:25.602192: step 14931, loss 0.545431.
Train: 2018-08-05T00:39:25.852134: step 14932, loss 0.545442.
Train: 2018-08-05T00:39:26.102075: step 14933, loss 0.520084.
Train: 2018-08-05T00:39:26.352015: step 14934, loss 0.60465.
Train: 2018-08-05T00:39:26.617579: step 14935, loss 0.520098.
Train: 2018-08-05T00:39:26.851901: step 14936, loss 0.596173.
Train: 2018-08-05T00:39:27.101811: step 14937, loss 0.57081.
Train: 2018-08-05T00:39:27.367403: step 14938, loss 0.59612.
Train: 2018-08-05T00:39:27.617343: step 14939, loss 0.537103.
Train: 2018-08-05T00:39:27.867286: step 14940, loss 0.553965.
Test: 2018-08-05T00:39:29.116963: step 14940, loss 0.548541.
Train: 2018-08-05T00:39:29.366906: step 14941, loss 0.587598.
Train: 2018-08-05T00:39:29.616876: step 14942, loss 0.612741.
Train: 2018-08-05T00:39:29.866818: step 14943, loss 0.537294.
Train: 2018-08-05T00:39:30.132380: step 14944, loss 0.554061.
Train: 2018-08-05T00:39:30.382322: step 14945, loss 0.495702.
Train: 2018-08-05T00:39:30.647885: step 14946, loss 0.529075.
Train: 2018-08-05T00:39:30.897797: step 14947, loss 0.562426.
Train: 2018-08-05T00:39:31.147739: step 14948, loss 0.512372.
Train: 2018-08-05T00:39:31.350839: step 14949, loss 0.687123.
Train: 2018-08-05T00:39:31.600787: step 14950, loss 0.570765.
Test: 2018-08-05T00:39:32.866085: step 14950, loss 0.548157.
Train: 2018-08-05T00:39:33.178542: step 14951, loss 0.629022.
Train: 2018-08-05T00:39:33.444074: step 14952, loss 0.554162.
Train: 2018-08-05T00:39:33.694047: step 14953, loss 0.48799.
Train: 2018-08-05T00:39:33.943991: step 14954, loss 0.570756.
Train: 2018-08-05T00:39:34.193930: step 14955, loss 0.529454.
Train: 2018-08-05T00:39:34.443871: step 14956, loss 0.504696.
Train: 2018-08-05T00:39:34.693812: step 14957, loss 0.49638.
Train: 2018-08-05T00:39:34.959345: step 14958, loss 0.562476.
Train: 2018-08-05T00:39:35.209321: step 14959, loss 0.53758.
Train: 2018-08-05T00:39:35.474850: step 14960, loss 0.579071.
Test: 2018-08-05T00:39:36.724556: step 14960, loss 0.549308.
Train: 2018-08-05T00:39:36.958878: step 14961, loss 0.554118.
Train: 2018-08-05T00:39:37.208819: step 14962, loss 0.612426.
Train: 2018-08-05T00:39:37.458759: step 14963, loss 0.595763.
Train: 2018-08-05T00:39:37.708701: step 14964, loss 0.512473.
Train: 2018-08-05T00:39:37.958644: step 14965, loss 0.579093.
Train: 2018-08-05T00:39:38.208618: step 14966, loss 0.537452.
Train: 2018-08-05T00:39:38.458525: step 14967, loss 0.537444.
Train: 2018-08-05T00:39:38.708501: step 14968, loss 0.479081.
Train: 2018-08-05T00:39:38.958443: step 14969, loss 0.554065.
Train: 2018-08-05T00:39:39.223996: step 14970, loss 0.503827.
Test: 2018-08-05T00:39:40.473679: step 14970, loss 0.547797.
Train: 2018-08-05T00:39:40.708000: step 14971, loss 0.554.
Train: 2018-08-05T00:39:40.957975: step 14972, loss 0.537142.
Train: 2018-08-05T00:39:41.207916: step 14973, loss 0.553934.
Train: 2018-08-05T00:39:41.457858: step 14974, loss 0.528542.
Train: 2018-08-05T00:39:41.707766: step 14975, loss 0.579306.
Train: 2018-08-05T00:39:41.957740: step 14976, loss 0.553855.
Train: 2018-08-05T00:39:42.207647: step 14977, loss 0.613381.
Train: 2018-08-05T00:39:42.457590: step 14978, loss 0.51129.
Train: 2018-08-05T00:39:42.707565: step 14979, loss 0.52827.
Train: 2018-08-05T00:39:42.957506: step 14980, loss 0.579399.
Test: 2018-08-05T00:39:44.207179: step 14980, loss 0.548082.
Train: 2018-08-05T00:39:44.457155: step 14981, loss 0.553802.
Train: 2018-08-05T00:39:44.707096: step 14982, loss 0.579421.
Train: 2018-08-05T00:39:44.957038: step 14983, loss 0.596506.
Train: 2018-08-05T00:39:45.206945: step 14984, loss 0.587944.
Train: 2018-08-05T00:39:45.472508: step 14985, loss 0.647571.
Train: 2018-08-05T00:39:45.722480: step 14986, loss 0.477378.
Train: 2018-08-05T00:39:45.988043: step 14987, loss 0.604777.
Train: 2018-08-05T00:39:46.237985: step 14988, loss 0.570823.
Train: 2018-08-05T00:39:46.487920: step 14989, loss 0.528574.
Train: 2018-08-05T00:39:46.737868: step 14990, loss 0.528634.
Test: 2018-08-05T00:39:48.003166: step 14990, loss 0.548883.
Train: 2018-08-05T00:39:48.237517: step 14991, loss 0.511817.
Train: 2018-08-05T00:39:48.487457: step 14992, loss 0.579225.
Train: 2018-08-05T00:39:48.737398: step 14993, loss 0.528687.
Train: 2018-08-05T00:39:48.987310: step 14994, loss 0.621332.
Train: 2018-08-05T00:39:49.237252: step 14995, loss 0.520319.
Train: 2018-08-05T00:39:49.487224: step 14996, loss 0.503521.
Train: 2018-08-05T00:39:49.737165: step 14997, loss 0.520309.
Train: 2018-08-05T00:39:49.987105: step 14998, loss 0.596071.
Train: 2018-08-05T00:39:50.237017: step 14999, loss 0.621362.
Train: 2018-08-05T00:39:50.502581: step 15000, loss 0.545539.
Test: 2018-08-05T00:39:51.752288: step 15000, loss 0.547749.
Train: 2018-08-05T00:39:52.580253: step 15001, loss 0.663331.
Train: 2018-08-05T00:39:52.830161: step 15002, loss 0.579172.
Train: 2018-08-05T00:39:53.080136: step 15003, loss 0.595869.
Train: 2018-08-05T00:39:53.330072: step 15004, loss 0.612449.
Train: 2018-08-05T00:39:53.580019: step 15005, loss 0.545853.
Train: 2018-08-05T00:39:53.845573: step 15006, loss 0.554211.
Train: 2018-08-05T00:39:54.079902: step 15007, loss 0.595499.
Train: 2018-08-05T00:39:54.329810: step 15008, loss 0.488571.
Train: 2018-08-05T00:39:54.579784: step 15009, loss 0.521532.
Train: 2018-08-05T00:39:54.829726: step 15010, loss 0.513369.
Test: 2018-08-05T00:39:56.095020: step 15010, loss 0.549051.
Train: 2018-08-05T00:39:56.344963: step 15011, loss 0.546156.
Train: 2018-08-05T00:39:56.590829: step 15012, loss 0.529727.
Train: 2018-08-05T00:39:56.840770: step 15013, loss 0.52146.
Train: 2018-08-05T00:39:57.106303: step 15014, loss 0.53783.
Train: 2018-08-05T00:39:57.356277: step 15015, loss 0.612003.
Train: 2018-08-05T00:39:57.606211: step 15016, loss 0.53773.
Train: 2018-08-05T00:39:57.871774: step 15017, loss 0.636889.
Train: 2018-08-05T00:39:58.121714: step 15018, loss 0.562493.
Train: 2018-08-05T00:39:58.371661: step 15019, loss 0.595533.
Train: 2018-08-05T00:39:58.621573: step 15020, loss 0.570756.
Test: 2018-08-05T00:39:59.855660: step 15020, loss 0.549327.
Train: 2018-08-05T00:40:00.105602: step 15021, loss 0.54604.
Train: 2018-08-05T00:40:00.355543: step 15022, loss 0.570758.
Train: 2018-08-05T00:40:00.605516: step 15023, loss 0.587207.
Train: 2018-08-05T00:40:00.855424: step 15024, loss 0.53791.
Train: 2018-08-05T00:40:01.105399: step 15025, loss 0.587173.
Train: 2018-08-05T00:40:01.355308: step 15026, loss 0.611741.
Train: 2018-08-05T00:40:01.620896: step 15027, loss 0.603479.
Train: 2018-08-05T00:40:01.855223: step 15028, loss 0.587084.
Train: 2018-08-05T00:40:02.105132: step 15029, loss 0.538261.
Train: 2018-08-05T00:40:02.355106: step 15030, loss 0.505892.
Test: 2018-08-05T00:40:03.604781: step 15030, loss 0.548777.
Train: 2018-08-05T00:40:03.839103: step 15031, loss 0.49784.
Train: 2018-08-05T00:40:04.089073: step 15032, loss 0.627571.
Train: 2018-08-05T00:40:04.338985: step 15033, loss 0.643744.
Train: 2018-08-05T00:40:04.588956: step 15034, loss 0.546544.
Train: 2018-08-05T00:40:04.838867: step 15035, loss 0.490083.
Train: 2018-08-05T00:40:05.104430: step 15036, loss 0.490079.
Train: 2018-08-05T00:40:05.354404: step 15037, loss 0.522285.
Train: 2018-08-05T00:40:05.604345: step 15038, loss 0.578895.
Train: 2018-08-05T00:40:05.854287: step 15039, loss 0.538302.
Train: 2018-08-05T00:40:06.104221: step 15040, loss 0.595172.
Test: 2018-08-05T00:40:07.353903: step 15040, loss 0.54863.
Train: 2018-08-05T00:40:07.588256: step 15041, loss 0.595259.
Train: 2018-08-05T00:40:07.838195: step 15042, loss 0.578958.
Train: 2018-08-05T00:40:08.088139: step 15043, loss 0.529979.
Train: 2018-08-05T00:40:08.338048: step 15044, loss 0.570764.
Train: 2018-08-05T00:40:08.588019: step 15045, loss 0.505444.
Train: 2018-08-05T00:40:08.837961: step 15046, loss 0.619828.
Train: 2018-08-05T00:40:09.087905: step 15047, loss 0.587137.
Train: 2018-08-05T00:40:09.337815: step 15048, loss 0.562587.
Train: 2018-08-05T00:40:09.587756: step 15049, loss 0.546261.
Train: 2018-08-05T00:40:09.837727: step 15050, loss 0.6281.
Test: 2018-08-05T00:40:11.087404: step 15050, loss 0.548977.
Train: 2018-08-05T00:40:11.321749: step 15051, loss 0.529952.
Train: 2018-08-05T00:40:11.571665: step 15052, loss 0.538147.
Train: 2018-08-05T00:40:11.821637: step 15053, loss 0.538147.
Train: 2018-08-05T00:40:12.087200: step 15054, loss 0.587094.
Train: 2018-08-05T00:40:12.337141: step 15055, loss 0.587099.
Train: 2018-08-05T00:40:12.602673: step 15056, loss 0.61155.
Train: 2018-08-05T00:40:12.852616: step 15057, loss 0.578924.
Train: 2018-08-05T00:40:13.102589: step 15058, loss 0.578914.
Train: 2018-08-05T00:40:13.368151: step 15059, loss 0.587028.
Train: 2018-08-05T00:40:13.618062: step 15060, loss 0.603206.
Test: 2018-08-05T00:40:14.867806: step 15060, loss 0.549239.
Train: 2018-08-05T00:40:15.102122: step 15061, loss 0.554645.
Train: 2018-08-05T00:40:15.352063: step 15062, loss 0.554698.
Train: 2018-08-05T00:40:15.602004: step 15063, loss 0.522567.
Train: 2018-08-05T00:40:15.851945: step 15064, loss 0.635136.
Train: 2018-08-05T00:40:16.101887: step 15065, loss 0.546775.
Train: 2018-08-05T00:40:16.351821: step 15066, loss 0.538819.
Train: 2018-08-05T00:40:16.601770: step 15067, loss 0.554857.
Train: 2018-08-05T00:40:16.851712: step 15068, loss 0.594863.
Train: 2018-08-05T00:40:17.101653: step 15069, loss 0.602837.
Train: 2018-08-05T00:40:17.382835: step 15070, loss 0.515032.
Test: 2018-08-05T00:40:18.632512: step 15070, loss 0.548303.
Train: 2018-08-05T00:40:18.866865: step 15071, loss 0.634691.
Train: 2018-08-05T00:40:19.116804: step 15072, loss 0.539051.
Train: 2018-08-05T00:40:19.366747: step 15073, loss 0.594767.
Train: 2018-08-05T00:40:19.616686: step 15074, loss 0.531202.
Train: 2018-08-05T00:40:19.866623: step 15075, loss 0.602676.
Train: 2018-08-05T00:40:20.116570: step 15076, loss 0.57093.
Train: 2018-08-05T00:40:20.366511: step 15077, loss 0.594707.
Train: 2018-08-05T00:40:20.616453: step 15078, loss 0.594681.
Train: 2018-08-05T00:40:20.866393: step 15079, loss 0.539396.
Train: 2018-08-05T00:40:21.116336: step 15080, loss 0.618286.
Test: 2018-08-05T00:40:22.366012: step 15080, loss 0.54999.
Train: 2018-08-05T00:40:22.600335: step 15081, loss 0.508051.
Train: 2018-08-05T00:40:22.850275: step 15082, loss 0.578866.
Train: 2018-08-05T00:40:23.100240: step 15083, loss 0.563147.
Train: 2018-08-05T00:40:23.350189: step 15084, loss 0.563154.
Train: 2018-08-05T00:40:23.600131: step 15085, loss 0.555286.
Train: 2018-08-05T00:40:23.850078: step 15086, loss 0.570988.
Train: 2018-08-05T00:40:24.100014: step 15087, loss 0.508122.
Train: 2018-08-05T00:40:24.349955: step 15088, loss 0.555292.
Train: 2018-08-05T00:40:24.599865: step 15089, loss 0.54731.
Train: 2018-08-05T00:40:24.849838: step 15090, loss 0.555135.
Test: 2018-08-05T00:40:26.099514: step 15090, loss 0.548824.
Train: 2018-08-05T00:40:26.333867: step 15091, loss 0.539295.
Train: 2018-08-05T00:40:26.583805: step 15092, loss 0.531056.
Train: 2018-08-05T00:40:26.833749: step 15093, loss 0.546945.
Train: 2018-08-05T00:40:27.083657: step 15094, loss 0.603063.
Train: 2018-08-05T00:40:27.333630: step 15095, loss 0.546515.
Train: 2018-08-05T00:40:27.583541: step 15096, loss 0.61912.
Train: 2018-08-05T00:40:27.849136: step 15097, loss 0.530521.
Train: 2018-08-05T00:40:28.099045: step 15098, loss 0.530408.
Train: 2018-08-05T00:40:28.349019: step 15099, loss 0.562669.
Train: 2018-08-05T00:40:28.552094: step 15100, loss 0.649104.
Test: 2018-08-05T00:40:29.817392: step 15100, loss 0.548482.
