Train: 2018-07-31T09:06:26.224652: step 1, loss 1.33878.
Train: 2018-07-31T09:06:26.427727: step 2, loss 1.38959.
Train: 2018-07-31T09:06:26.630805: step 3, loss 1.04646.
Train: 2018-07-31T09:06:26.818232: step 4, loss 1.05465.
Train: 2018-07-31T09:06:27.005687: step 5, loss 0.918046.
Train: 2018-07-31T09:06:27.193173: step 6, loss 0.881224.
Train: 2018-07-31T09:06:27.380630: step 7, loss 0.784157.
Train: 2018-07-31T09:06:27.552465: step 8, loss 0.784776.
Train: 2018-07-31T09:06:27.739920: step 9, loss 0.763903.
Train: 2018-07-31T09:06:27.942998: step 10, loss 0.689028.
Test: 2018-07-31T09:06:28.364746: step 10, loss 0.695593.
Train: 2018-07-31T09:06:28.552200: step 11, loss 0.76816.
Train: 2018-07-31T09:06:28.739686: step 12, loss 0.675102.
Train: 2018-07-31T09:06:28.911521: step 13, loss 0.666673.
Train: 2018-07-31T09:06:29.098946: step 14, loss 0.685763.
Train: 2018-07-31T09:06:29.286433: step 15, loss 0.683003.
Train: 2018-07-31T09:06:29.458269: step 16, loss 0.701903.
Train: 2018-07-31T09:06:29.645693: step 17, loss 0.613543.
Train: 2018-07-31T09:06:29.833150: step 18, loss 0.659789.
Train: 2018-07-31T09:06:30.005015: step 19, loss 0.572216.
Train: 2018-07-31T09:06:30.192471: step 20, loss 0.65928.
Test: 2018-07-31T09:06:30.426762: step 20, loss 0.617882.
Train: 2018-07-31T09:06:30.598627: step 21, loss 0.669609.
Train: 2018-07-31T09:06:30.786082: step 22, loss 0.654589.
Train: 2018-07-31T09:06:30.973509: step 23, loss 0.646402.
Train: 2018-07-31T09:06:31.160994: step 24, loss 0.646432.
Train: 2018-07-31T09:06:31.348451: step 25, loss 0.657957.
Train: 2018-07-31T09:06:31.520285: step 26, loss 0.603411.
Train: 2018-07-31T09:06:31.707743: step 27, loss 0.635123.
Train: 2018-07-31T09:06:31.910789: step 28, loss 0.617683.
Train: 2018-07-31T09:06:32.260374: step 29, loss 0.604066.
Train: 2018-07-31T09:06:32.510344: step 30, loss 0.699837.
Test: 2018-07-31T09:06:32.744644: step 30, loss 0.603173.
Train: 2018-07-31T09:06:32.932128: step 31, loss 0.633648.
Train: 2018-07-31T09:06:33.119583: step 32, loss 0.64758.
Train: 2018-07-31T09:06:33.307032: step 33, loss 0.594494.
Train: 2018-07-31T09:06:33.478873: step 34, loss 0.683549.
Train: 2018-07-31T09:06:33.666299: step 35, loss 0.660264.
Train: 2018-07-31T09:06:33.853785: step 36, loss 0.61773.
Train: 2018-07-31T09:06:34.025620: step 37, loss 0.660148.
Train: 2018-07-31T09:06:34.213075: step 38, loss 0.556719.
Train: 2018-07-31T09:06:34.400526: step 39, loss 0.600621.
Train: 2018-07-31T09:06:34.587957: step 40, loss 0.574155.
Test: 2018-07-31T09:06:34.806656: step 40, loss 0.59144.
Train: 2018-07-31T09:06:34.994113: step 41, loss 0.591717.
Train: 2018-07-31T09:06:35.181598: step 42, loss 0.594697.
Train: 2018-07-31T09:06:35.369056: step 43, loss 0.600831.
Train: 2018-07-31T09:06:35.556480: step 44, loss 0.58697.
Train: 2018-07-31T09:06:35.743969: step 45, loss 0.502351.
Train: 2018-07-31T09:06:35.931418: step 46, loss 0.608301.
Train: 2018-07-31T09:06:36.103228: step 47, loss 0.564574.
Train: 2018-07-31T09:06:36.290714: step 48, loss 0.533941.
Train: 2018-07-31T09:06:36.493763: step 49, loss 0.678068.
Train: 2018-07-31T09:06:36.696840: step 50, loss 0.618107.
Test: 2018-07-31T09:06:36.946832: step 50, loss 0.582365.
Train: 2018-07-31T09:06:37.149858: step 51, loss 0.557779.
Train: 2018-07-31T09:06:37.337324: step 52, loss 0.600641.
Train: 2018-07-31T09:06:37.556044: step 53, loss 0.562253.
Train: 2018-07-31T09:06:37.759127: step 54, loss 0.596817.
Train: 2018-07-31T09:06:37.977791: step 55, loss 0.668676.
Train: 2018-07-31T09:06:38.165278: step 56, loss 0.520792.
Train: 2018-07-31T09:06:38.368355: step 57, loss 0.621595.
Train: 2018-07-31T09:06:38.587054: step 58, loss 0.54918.
Train: 2018-07-31T09:06:38.790130: step 59, loss 0.61544.
Train: 2018-07-31T09:06:39.008832: step 60, loss 0.537523.
Test: 2018-07-31T09:06:39.258771: step 60, loss 0.577566.
Train: 2018-07-31T09:06:39.461850: step 61, loss 0.634837.
Train: 2018-07-31T09:06:39.680549: step 62, loss 0.605698.
Train: 2018-07-31T09:06:39.883594: step 63, loss 0.606807.
Train: 2018-07-31T09:06:40.071094: step 64, loss 0.593509.
Train: 2018-07-31T09:06:40.289778: step 65, loss 0.575448.
Train: 2018-07-31T09:06:40.508447: step 66, loss 0.613992.
Train: 2018-07-31T09:06:40.742769: step 67, loss 0.617701.
Train: 2018-07-31T09:06:40.945871: step 68, loss 0.661755.
Train: 2018-07-31T09:06:41.133325: step 69, loss 0.596243.
Train: 2018-07-31T09:06:41.305134: step 70, loss 0.537254.
Test: 2018-07-31T09:06:41.539486: step 70, loss 0.580248.
Train: 2018-07-31T09:06:41.742551: step 71, loss 0.662748.
Train: 2018-07-31T09:06:41.930018: step 72, loss 0.600332.
Train: 2018-07-31T09:06:42.117446: step 73, loss 0.591367.
Train: 2018-07-31T09:06:42.289279: step 74, loss 0.64423.
Train: 2018-07-31T09:06:42.476765: step 75, loss 0.668153.
Train: 2018-07-31T09:06:42.664221: step 76, loss 0.623426.
Train: 2018-07-31T09:06:42.851672: step 77, loss 0.628658.
Train: 2018-07-31T09:06:43.039104: step 78, loss 0.529901.
Train: 2018-07-31T09:06:43.226563: step 79, loss 0.575092.
Train: 2018-07-31T09:06:43.538988: step 80, loss 0.535769.
Test: 2018-07-31T09:06:43.788929: step 80, loss 0.571961.
Train: 2018-07-31T09:06:44.054492: step 81, loss 0.628817.
Train: 2018-07-31T09:06:44.398163: step 82, loss 0.628553.
Train: 2018-07-31T09:06:44.726209: step 83, loss 0.614715.
Train: 2018-07-31T09:06:44.913665: step 84, loss 0.575505.
Train: 2018-07-31T09:06:45.116751: step 85, loss 0.581717.
Train: 2018-07-31T09:06:45.319850: step 86, loss 0.554026.
Train: 2018-07-31T09:06:45.491687: step 87, loss 0.645771.
Train: 2018-07-31T09:06:45.679135: step 88, loss 0.600621.
Train: 2018-07-31T09:06:45.866567: step 89, loss 0.567926.
Train: 2018-07-31T09:06:46.054023: step 90, loss 0.580552.
Test: 2018-07-31T09:06:46.288373: step 90, loss 0.574108.
Train: 2018-07-31T09:06:46.491452: step 91, loss 0.639258.
Train: 2018-07-31T09:06:46.663285: step 92, loss 0.559748.
Train: 2018-07-31T09:06:46.866333: step 93, loss 0.634143.
Train: 2018-07-31T09:06:47.053818: step 94, loss 0.704603.
Train: 2018-07-31T09:06:47.241276: step 95, loss 0.556283.
Train: 2018-07-31T09:06:47.444347: step 96, loss 0.594544.
Train: 2018-07-31T09:06:47.631778: step 97, loss 0.593701.
Train: 2018-07-31T09:06:47.819266: step 98, loss 0.573766.
Train: 2018-07-31T09:06:48.022312: step 99, loss 0.603454.
Train: 2018-07-31T09:06:48.209769: step 100, loss 0.581178.
Test: 2018-07-31T09:06:48.444088: step 100, loss 0.569906.
Train: 2018-07-31T09:06:49.396990: step 101, loss 0.594379.
Train: 2018-07-31T09:06:49.600068: step 102, loss 0.5125.
Train: 2018-07-31T09:06:49.818766: step 103, loss 0.596874.
Train: 2018-07-31T09:06:50.037466: step 104, loss 0.584684.
Train: 2018-07-31T09:06:50.240568: step 105, loss 0.570766.
Train: 2018-07-31T09:06:50.459251: step 106, loss 0.578236.
Train: 2018-07-31T09:06:50.662320: step 107, loss 0.590027.
Train: 2018-07-31T09:06:50.881018: step 108, loss 0.589101.
Train: 2018-07-31T09:06:51.099716: step 109, loss 0.654096.
Train: 2018-07-31T09:06:51.318422: step 110, loss 0.597775.
Test: 2018-07-31T09:06:51.552742: step 110, loss 0.570263.
Train: 2018-07-31T09:06:51.740224: step 111, loss 0.604466.
Train: 2018-07-31T09:06:51.927646: step 112, loss 0.620377.
Train: 2018-07-31T09:06:52.115128: step 113, loss 0.577066.
Train: 2018-07-31T09:06:52.318180: step 114, loss 0.648499.
Train: 2018-07-31T09:06:52.490015: step 115, loss 0.613746.
Train: 2018-07-31T09:06:52.677472: step 116, loss 0.595719.
Train: 2018-07-31T09:06:52.880574: step 117, loss 0.629091.
Train: 2018-07-31T09:06:53.052409: step 118, loss 0.608755.
Train: 2018-07-31T09:06:53.239873: step 119, loss 0.645461.
Train: 2018-07-31T09:06:53.427321: step 120, loss 0.567256.
Test: 2018-07-31T09:06:53.661648: step 120, loss 0.573145.
Train: 2018-07-31T09:06:53.849097: step 121, loss 0.587007.
Train: 2018-07-31T09:06:54.052150: step 122, loss 0.642111.
Train: 2018-07-31T09:06:54.255229: step 123, loss 0.536465.
Train: 2018-07-31T09:06:54.458304: step 124, loss 0.600009.
Train: 2018-07-31T09:06:54.645786: step 125, loss 0.565074.
Train: 2018-07-31T09:06:54.817622: step 126, loss 0.511984.
Train: 2018-07-31T09:06:55.005077: step 127, loss 0.652077.
Train: 2018-07-31T09:06:55.208130: step 128, loss 0.512451.
Train: 2018-07-31T09:06:55.395612: step 129, loss 0.485859.
Train: 2018-07-31T09:06:55.583066: step 130, loss 0.569683.
Test: 2018-07-31T09:06:55.817361: step 130, loss 0.565354.
Train: 2018-07-31T09:06:56.004845: step 131, loss 0.525109.
Train: 2018-07-31T09:06:56.192273: step 132, loss 0.67014.
Train: 2018-07-31T09:06:56.364108: step 133, loss 0.603759.
Train: 2018-07-31T09:06:56.567211: step 134, loss 0.572964.
Train: 2018-07-31T09:06:56.754642: step 135, loss 0.630903.
Train: 2018-07-31T09:06:56.926477: step 136, loss 0.594936.
Train: 2018-07-31T09:06:57.113957: step 137, loss 0.579257.
Train: 2018-07-31T09:06:57.301389: step 138, loss 0.602899.
Train: 2018-07-31T09:06:57.488871: step 139, loss 0.584007.
Train: 2018-07-31T09:06:57.676302: step 140, loss 0.608039.
Test: 2018-07-31T09:06:57.910652: step 140, loss 0.574897.
Train: 2018-07-31T09:06:58.113723: step 141, loss 0.622942.
Train: 2018-07-31T09:06:58.301156: step 142, loss 0.58026.
Train: 2018-07-31T09:06:58.504258: step 143, loss 0.559411.
Train: 2018-07-31T09:06:58.691688: step 144, loss 0.552189.
Train: 2018-07-31T09:06:58.879170: step 145, loss 0.531891.
Train: 2018-07-31T09:06:59.066626: step 146, loss 0.593231.
Train: 2018-07-31T09:06:59.269677: step 147, loss 0.547727.
Train: 2018-07-31T09:06:59.457134: step 148, loss 0.514143.
Train: 2018-07-31T09:06:59.644616: step 149, loss 0.616129.
Train: 2018-07-31T09:06:59.832047: step 150, loss 0.624148.
Test: 2018-07-31T09:07:00.066366: step 150, loss 0.561315.
Train: 2018-07-31T09:07:00.253848: step 151, loss 0.484073.
Train: 2018-07-31T09:07:00.441278: step 152, loss 0.590329.
Train: 2018-07-31T09:07:00.628734: step 153, loss 0.560305.
Train: 2018-07-31T09:07:00.816190: step 154, loss 0.596676.
Train: 2018-07-31T09:07:01.003646: step 155, loss 0.664352.
Train: 2018-07-31T09:07:01.191103: step 156, loss 0.620223.
Train: 2018-07-31T09:07:01.378559: step 157, loss 0.641033.
Train: 2018-07-31T09:07:01.566015: step 158, loss 0.543376.
Train: 2018-07-31T09:07:01.753506: step 159, loss 0.589262.
Train: 2018-07-31T09:07:01.925331: step 160, loss 0.568955.
Test: 2018-07-31T09:07:02.159628: step 160, loss 0.569379.
Train: 2018-07-31T09:07:02.347083: step 161, loss 0.592702.
Train: 2018-07-31T09:07:02.534538: step 162, loss 0.568663.
Train: 2018-07-31T09:07:02.722026: step 163, loss 0.569399.
Train: 2018-07-31T09:07:02.909476: step 164, loss 0.631292.
Train: 2018-07-31T09:07:03.096907: step 165, loss 0.577259.
Train: 2018-07-31T09:07:03.284388: step 166, loss 0.627744.
Train: 2018-07-31T09:07:03.471844: step 167, loss 0.623492.
Train: 2018-07-31T09:07:03.643653: step 168, loss 0.549871.
Train: 2018-07-31T09:07:03.831110: step 169, loss 0.636674.
Train: 2018-07-31T09:07:04.018565: step 170, loss 0.631511.
Test: 2018-07-31T09:07:04.268539: step 170, loss 0.563345.
Train: 2018-07-31T09:07:04.455989: step 171, loss 0.532794.
Train: 2018-07-31T09:07:04.643420: step 172, loss 0.604177.
Train: 2018-07-31T09:07:04.830876: step 173, loss 0.561192.
Train: 2018-07-31T09:07:05.018358: step 174, loss 0.579605.
Train: 2018-07-31T09:07:05.205813: step 175, loss 0.617918.
Train: 2018-07-31T09:07:05.393269: step 176, loss 0.609946.
Train: 2018-07-31T09:07:05.596321: step 177, loss 0.561923.
Train: 2018-07-31T09:07:05.783802: step 178, loss 0.57302.
Train: 2018-07-31T09:07:05.986857: step 179, loss 0.595038.
Train: 2018-07-31T09:07:06.158716: step 180, loss 0.618895.
Test: 2018-07-31T09:07:06.393010: step 180, loss 0.562047.
Train: 2018-07-31T09:07:06.596088: step 181, loss 0.484025.
Train: 2018-07-31T09:07:06.783571: step 182, loss 0.660634.
Train: 2018-07-31T09:07:06.971000: step 183, loss 0.550123.
Train: 2018-07-31T09:07:07.158456: step 184, loss 0.599882.
Train: 2018-07-31T09:07:07.330290: step 185, loss 0.521637.
Train: 2018-07-31T09:07:07.533374: step 186, loss 0.578193.
Train: 2018-07-31T09:07:07.705203: step 187, loss 0.565699.
Train: 2018-07-31T09:07:07.892660: step 188, loss 0.479275.
Train: 2018-07-31T09:07:08.095762: step 189, loss 0.51148.
Train: 2018-07-31T09:07:08.283217: step 190, loss 0.575679.
Test: 2018-07-31T09:07:08.517512: step 190, loss 0.559894.
Train: 2018-07-31T09:07:08.704993: step 191, loss 0.610205.
Train: 2018-07-31T09:07:08.892450: step 192, loss 0.615378.
Train: 2018-07-31T09:07:09.064285: step 193, loss 0.695211.
Train: 2018-07-31T09:07:09.267362: step 194, loss 0.585098.
Train: 2018-07-31T09:07:09.454819: step 195, loss 0.607516.
Train: 2018-07-31T09:07:09.642275: step 196, loss 0.610975.
Train: 2018-07-31T09:07:09.829731: step 197, loss 0.528193.
Train: 2018-07-31T09:07:10.001572: step 198, loss 0.630387.
Train: 2018-07-31T09:07:10.189021: step 199, loss 0.534359.
Train: 2018-07-31T09:07:10.376477: step 200, loss 0.571736.
Test: 2018-07-31T09:07:10.610797: step 200, loss 0.570494.
Train: 2018-07-31T09:07:11.298141: step 201, loss 0.582449.
Train: 2018-07-31T09:07:11.485601: step 202, loss 0.60473.
Train: 2018-07-31T09:07:11.673053: step 203, loss 0.558971.
Train: 2018-07-31T09:07:11.860510: step 204, loss 0.577932.
Train: 2018-07-31T09:07:12.047966: step 205, loss 0.532416.
Train: 2018-07-31T09:07:12.235423: step 206, loss 0.592643.
Train: 2018-07-31T09:07:12.407256: step 207, loss 0.577802.
Train: 2018-07-31T09:07:12.594713: step 208, loss 0.619605.
Train: 2018-07-31T09:07:12.782169: step 209, loss 0.621135.
Train: 2018-07-31T09:07:12.969624: step 210, loss 0.599502.
Test: 2018-07-31T09:07:13.203915: step 210, loss 0.559979.
Train: 2018-07-31T09:07:13.375749: step 211, loss 0.552859.
Train: 2018-07-31T09:07:13.563236: step 212, loss 0.523365.
Train: 2018-07-31T09:07:13.750662: step 213, loss 0.556275.
Train: 2018-07-31T09:07:13.953763: step 214, loss 0.641107.
Train: 2018-07-31T09:07:14.141226: step 215, loss 0.585375.
Train: 2018-07-31T09:07:14.328677: step 216, loss 0.609891.
Train: 2018-07-31T09:07:14.516138: step 217, loss 0.594383.
Train: 2018-07-31T09:07:14.703594: step 218, loss 0.623501.
Train: 2018-07-31T09:07:14.891050: step 219, loss 0.605375.
Train: 2018-07-31T09:07:15.078508: step 220, loss 0.537668.
Test: 2018-07-31T09:07:15.312821: step 220, loss 0.558227.
Train: 2018-07-31T09:07:15.500286: step 221, loss 0.545797.
Train: 2018-07-31T09:07:15.687738: step 222, loss 0.57078.
Train: 2018-07-31T09:07:15.875195: step 223, loss 0.635371.
Train: 2018-07-31T09:07:16.062651: step 224, loss 0.586313.
Train: 2018-07-31T09:07:16.234487: step 225, loss 0.573502.
Train: 2018-07-31T09:07:16.421941: step 226, loss 0.538495.
Train: 2018-07-31T09:07:16.609367: step 227, loss 0.597984.
Train: 2018-07-31T09:07:16.796824: step 228, loss 0.518264.
Train: 2018-07-31T09:07:16.968689: step 229, loss 0.575409.
Train: 2018-07-31T09:07:17.156113: step 230, loss 0.591518.
Test: 2018-07-31T09:07:17.390434: step 230, loss 0.558203.
Train: 2018-07-31T09:07:17.577916: step 231, loss 0.585983.
Train: 2018-07-31T09:07:17.765371: step 232, loss 0.527492.
Train: 2018-07-31T09:07:17.952803: step 233, loss 0.552557.
Train: 2018-07-31T09:07:18.140289: step 234, loss 0.589438.
Train: 2018-07-31T09:07:18.327714: step 235, loss 0.592421.
Train: 2018-07-31T09:07:18.499581: step 236, loss 0.580229.
Train: 2018-07-31T09:07:18.687036: step 237, loss 0.546386.
Train: 2018-07-31T09:07:18.874493: step 238, loss 0.594504.
Train: 2018-07-31T09:07:19.061948: step 239, loss 0.628787.
Train: 2018-07-31T09:07:19.249374: step 240, loss 0.52459.
Test: 2018-07-31T09:07:19.468072: step 240, loss 0.555929.
Train: 2018-07-31T09:07:19.655554: step 241, loss 0.61594.
Train: 2018-07-31T09:07:19.843010: step 242, loss 0.573182.
Train: 2018-07-31T09:07:20.014850: step 243, loss 0.513841.
Train: 2018-07-31T09:07:20.202276: step 244, loss 0.598821.
Train: 2018-07-31T09:07:20.389763: step 245, loss 0.598081.
Train: 2018-07-31T09:07:20.577188: step 246, loss 0.639954.
Train: 2018-07-31T09:07:20.764669: step 247, loss 0.581788.
Train: 2018-07-31T09:07:20.952130: step 248, loss 0.598836.
Train: 2018-07-31T09:07:21.123936: step 249, loss 0.563214.
Train: 2018-07-31T09:07:21.311421: step 250, loss 0.583566.
Test: 2018-07-31T09:07:21.545737: step 250, loss 0.55879.
Train: 2018-07-31T09:07:21.733192: step 251, loss 0.583796.
Train: 2018-07-31T09:07:21.920653: step 252, loss 0.536621.
Train: 2018-07-31T09:07:22.108104: step 253, loss 0.544302.
Train: 2018-07-31T09:07:22.279915: step 254, loss 0.533729.
Train: 2018-07-31T09:07:22.467371: step 255, loss 0.595098.
Train: 2018-07-31T09:07:22.639235: step 256, loss 0.51016.
Train: 2018-07-31T09:07:22.811070: step 257, loss 0.494674.
Train: 2018-07-31T09:07:22.998527: step 258, loss 0.595306.
Train: 2018-07-31T09:07:23.170331: step 259, loss 0.518745.
Train: 2018-07-31T09:07:23.357818: step 260, loss 0.612793.
Test: 2018-07-31T09:07:23.592140: step 260, loss 0.55529.
Train: 2018-07-31T09:07:23.779593: step 261, loss 0.556567.
Train: 2018-07-31T09:07:23.951398: step 262, loss 0.496433.
Train: 2018-07-31T09:07:24.138854: step 263, loss 0.528587.
Train: 2018-07-31T09:07:24.310719: step 264, loss 0.610191.
Train: 2018-07-31T09:07:24.498145: step 265, loss 0.5504.
Train: 2018-07-31T09:07:24.670009: step 266, loss 0.630863.
Train: 2018-07-31T09:07:24.857436: step 267, loss 0.537401.
Train: 2018-07-31T09:07:25.029299: step 268, loss 0.532452.
Train: 2018-07-31T09:07:25.201136: step 269, loss 0.570881.
Train: 2018-07-31T09:07:25.388592: step 270, loss 0.56869.
Test: 2018-07-31T09:07:25.622883: step 270, loss 0.555034.
Train: 2018-07-31T09:07:25.794717: step 271, loss 0.591479.
Train: 2018-07-31T09:07:25.966581: step 272, loss 0.572404.
Train: 2018-07-31T09:07:26.154040: step 273, loss 0.501194.
Train: 2018-07-31T09:07:26.325842: step 274, loss 0.575682.
Train: 2018-07-31T09:07:26.497677: step 275, loss 0.587058.
Train: 2018-07-31T09:07:26.669512: step 276, loss 0.519596.
Train: 2018-07-31T09:07:26.841347: step 277, loss 0.566722.
Train: 2018-07-31T09:07:26.997584: step 278, loss 0.578187.
Train: 2018-07-31T09:07:27.169394: step 279, loss 0.554436.
Train: 2018-07-31T09:07:27.341259: step 280, loss 0.5664.
Test: 2018-07-31T09:07:27.575551: step 280, loss 0.555263.
Train: 2018-07-31T09:07:27.747415: step 281, loss 0.54873.
Train: 2018-07-31T09:07:27.919249: step 282, loss 0.579344.
Train: 2018-07-31T09:07:28.075463: step 283, loss 0.551974.
Train: 2018-07-31T09:07:28.247299: step 284, loss 0.501621.
Train: 2018-07-31T09:07:28.419126: step 285, loss 0.564354.
Train: 2018-07-31T09:07:28.590961: step 286, loss 0.504859.
Train: 2018-07-31T09:07:28.762771: step 287, loss 0.59931.
Train: 2018-07-31T09:07:28.934641: step 288, loss 0.466738.
Train: 2018-07-31T09:07:29.106472: step 289, loss 0.601737.
Train: 2018-07-31T09:07:29.262684: step 290, loss 0.579217.
Test: 2018-07-31T09:07:29.497006: step 290, loss 0.554138.
Train: 2018-07-31T09:07:29.668839: step 291, loss 0.563342.
Train: 2018-07-31T09:07:29.825048: step 292, loss 0.576631.
Train: 2018-07-31T09:07:29.996888: step 293, loss 0.579142.
Train: 2018-07-31T09:07:30.153103: step 294, loss 0.613767.
Train: 2018-07-31T09:07:30.309314: step 295, loss 0.536067.
Train: 2018-07-31T09:07:30.481118: step 296, loss 0.530005.
Train: 2018-07-31T09:07:30.637362: step 297, loss 0.526224.
Train: 2018-07-31T09:07:30.809167: step 298, loss 0.554138.
Train: 2018-07-31T09:07:30.965380: step 299, loss 0.541776.
Train: 2018-07-31T09:07:31.137214: step 300, loss 0.561966.
Test: 2018-07-31T09:07:31.371537: step 300, loss 0.553883.
Train: 2018-07-31T09:07:32.090142: step 301, loss 0.53822.
Train: 2018-07-31T09:07:32.246331: step 302, loss 0.541222.
Train: 2018-07-31T09:07:32.418165: step 303, loss 0.571129.
Train: 2018-07-31T09:07:32.574378: step 304, loss 0.685105.
Train: 2018-07-31T09:07:32.746214: step 305, loss 0.556935.
Train: 2018-07-31T09:07:32.902426: step 306, loss 0.560211.
Train: 2018-07-31T09:07:33.058670: step 307, loss 0.644701.
Train: 2018-07-31T09:07:33.230506: step 308, loss 0.545592.
Train: 2018-07-31T09:07:33.386719: step 309, loss 0.545174.
Train: 2018-07-31T09:07:33.542932: step 310, loss 0.494798.
Test: 2018-07-31T09:07:33.777253: step 310, loss 0.555526.
Train: 2018-07-31T09:07:33.949057: step 311, loss 0.657477.
Train: 2018-07-31T09:07:34.105302: step 312, loss 0.553848.
Train: 2018-07-31T09:07:34.261513: step 313, loss 0.557671.
Train: 2018-07-31T09:07:34.417696: step 314, loss 0.551626.
Train: 2018-07-31T09:07:34.573910: step 315, loss 0.531447.
Train: 2018-07-31T09:07:34.730148: step 316, loss 0.562226.
Train: 2018-07-31T09:07:34.886338: step 317, loss 0.554919.
Train: 2018-07-31T09:07:35.042580: step 318, loss 0.56042.
Train: 2018-07-31T09:07:35.214416: step 319, loss 0.658332.
Train: 2018-07-31T09:07:35.370630: step 320, loss 0.526384.
Test: 2018-07-31T09:07:35.604921: step 320, loss 0.553611.
Train: 2018-07-31T09:07:35.761133: step 321, loss 0.601371.
Train: 2018-07-31T09:07:35.932997: step 322, loss 0.554189.
Train: 2018-07-31T09:07:36.089208: step 323, loss 0.609397.
Train: 2018-07-31T09:07:36.245394: step 324, loss 0.502289.
Train: 2018-07-31T09:07:36.401607: step 325, loss 0.499303.
Train: 2018-07-31T09:07:36.557851: step 326, loss 0.547574.
Train: 2018-07-31T09:07:36.714064: step 327, loss 0.557595.
Train: 2018-07-31T09:07:36.885869: step 328, loss 0.597799.
Train: 2018-07-31T09:07:37.042113: step 329, loss 0.610692.
Train: 2018-07-31T09:07:37.198326: step 330, loss 0.575407.
Test: 2018-07-31T09:07:37.432618: step 330, loss 0.552906.
Train: 2018-07-31T09:07:37.573208: step 331, loss 0.613345.
Train: 2018-07-31T09:07:37.729453: step 332, loss 0.526616.
Train: 2018-07-31T09:07:37.885665: step 333, loss 0.583435.
Train: 2018-07-31T09:07:38.041873: step 334, loss 0.596068.
Train: 2018-07-31T09:07:38.198091: step 335, loss 0.570822.
Train: 2018-07-31T09:07:38.354307: step 336, loss 0.577643.
Train: 2018-07-31T09:07:38.510518: step 337, loss 0.479826.
Train: 2018-07-31T09:07:38.651111: step 338, loss 0.509655.
Train: 2018-07-31T09:07:38.807295: step 339, loss 0.578087.
Train: 2018-07-31T09:07:38.979160: step 340, loss 0.579365.
Test: 2018-07-31T09:07:39.213483: step 340, loss 0.55304.
Train: 2018-07-31T09:07:39.369662: step 341, loss 0.532863.
Train: 2018-07-31T09:07:39.525876: step 342, loss 0.50532.
Train: 2018-07-31T09:07:39.682125: step 343, loss 0.564427.
Train: 2018-07-31T09:07:39.838333: step 344, loss 0.582501.
Train: 2018-07-31T09:07:39.994547: step 345, loss 0.580314.
Train: 2018-07-31T09:07:40.150731: step 346, loss 0.576981.
Train: 2018-07-31T09:07:40.322589: step 347, loss 0.571768.
Train: 2018-07-31T09:07:40.494424: step 348, loss 0.530789.
Train: 2018-07-31T09:07:40.650613: step 349, loss 0.551771.
Train: 2018-07-31T09:07:40.806857: step 350, loss 0.515847.
Test: 2018-07-31T09:07:41.041146: step 350, loss 0.552461.
Train: 2018-07-31T09:07:41.197383: step 351, loss 0.547816.
Train: 2018-07-31T09:07:41.353604: step 352, loss 0.562767.
Train: 2018-07-31T09:07:41.494165: step 353, loss 0.579314.
Train: 2018-07-31T09:07:41.650408: step 354, loss 0.565866.
Train: 2018-07-31T09:07:41.822238: step 355, loss 0.574131.
Train: 2018-07-31T09:07:41.978458: step 356, loss 0.491843.
Train: 2018-07-31T09:07:42.134665: step 357, loss 0.526846.
Train: 2018-07-31T09:07:42.290854: step 358, loss 0.553648.
Train: 2018-07-31T09:07:42.447091: step 359, loss 0.549157.
Train: 2018-07-31T09:07:42.603280: step 360, loss 0.531435.
Test: 2018-07-31T09:07:42.837634: step 360, loss 0.552143.
Train: 2018-07-31T09:07:42.993814: step 361, loss 0.610505.
Train: 2018-07-31T09:07:43.150057: step 362, loss 0.59722.
Train: 2018-07-31T09:07:43.306242: step 363, loss 0.612511.
Train: 2018-07-31T09:07:43.462487: step 364, loss 0.609632.
Train: 2018-07-31T09:07:43.618699: step 365, loss 0.566899.
Train: 2018-07-31T09:07:43.774880: step 366, loss 0.593655.
Train: 2018-07-31T09:07:43.915497: step 367, loss 0.561515.
Train: 2018-07-31T09:07:44.071687: step 368, loss 0.575748.
Train: 2018-07-31T09:07:44.227924: step 369, loss 0.603227.
Train: 2018-07-31T09:07:44.384145: step 370, loss 0.587869.
Test: 2018-07-31T09:07:44.634079: step 370, loss 0.553236.
Train: 2018-07-31T09:07:44.790293: step 371, loss 0.58978.
Train: 2018-07-31T09:07:44.946512: step 372, loss 0.552963.
Train: 2018-07-31T09:07:45.118340: step 373, loss 0.541771.
Train: 2018-07-31T09:07:45.274530: step 374, loss 0.573343.
Train: 2018-07-31T09:07:45.430745: step 375, loss 0.541491.
Train: 2018-07-31T09:07:45.586987: step 376, loss 0.523289.
Train: 2018-07-31T09:07:45.743201: step 377, loss 0.566458.
Train: 2018-07-31T09:07:45.899384: step 378, loss 0.627394.
Train: 2018-07-31T09:07:46.055627: step 379, loss 0.554483.
Train: 2018-07-31T09:07:46.211810: step 380, loss 0.615303.
Test: 2018-07-31T09:07:46.446133: step 380, loss 0.553485.
Train: 2018-07-31T09:07:46.602343: step 381, loss 0.529922.
Train: 2018-07-31T09:07:46.758557: step 382, loss 0.64827.
Train: 2018-07-31T09:07:46.914801: step 383, loss 0.552244.
Train: 2018-07-31T09:07:47.071014: step 384, loss 0.572843.
Train: 2018-07-31T09:07:47.227222: step 385, loss 0.534679.
Train: 2018-07-31T09:07:47.383436: step 386, loss 0.539039.
Train: 2018-07-31T09:07:47.539624: step 387, loss 0.576138.
Train: 2018-07-31T09:07:47.680246: step 388, loss 0.596014.
Train: 2018-07-31T09:07:47.836468: step 389, loss 0.672417.
Train: 2018-07-31T09:07:47.992643: step 390, loss 0.564318.
Test: 2018-07-31T09:07:48.226996: step 390, loss 0.552922.
Train: 2018-07-31T09:07:48.383207: step 391, loss 0.570399.
Train: 2018-07-31T09:07:48.555012: step 392, loss 0.588877.
Train: 2018-07-31T09:07:48.711226: step 393, loss 0.5679.
Train: 2018-07-31T09:07:48.851818: step 394, loss 0.53093.
Train: 2018-07-31T09:07:49.008031: step 395, loss 0.533165.
Train: 2018-07-31T09:07:49.164243: step 396, loss 0.564578.
Train: 2018-07-31T09:07:49.320457: step 397, loss 0.586842.
Train: 2018-07-31T09:07:49.476695: step 398, loss 0.572676.
Train: 2018-07-31T09:07:49.632915: step 399, loss 0.568439.
Train: 2018-07-31T09:07:49.789099: step 400, loss 0.623326.
Test: 2018-07-31T09:07:50.023449: step 400, loss 0.552386.
Train: 2018-07-31T09:07:50.710757: step 401, loss 0.506238.
Train: 2018-07-31T09:07:50.867001: step 402, loss 0.564545.
Train: 2018-07-31T09:07:51.007562: step 403, loss 0.586566.
Train: 2018-07-31T09:07:51.179432: step 404, loss 0.5869.
Train: 2018-07-31T09:07:51.335635: step 405, loss 0.565105.
Train: 2018-07-31T09:07:51.491824: step 406, loss 0.492668.
Train: 2018-07-31T09:07:51.648037: step 407, loss 0.534915.
Train: 2018-07-31T09:07:51.804252: step 408, loss 0.60566.
Train: 2018-07-31T09:07:51.960488: step 409, loss 0.547754.
Train: 2018-07-31T09:07:52.116709: step 410, loss 0.556643.
Test: 2018-07-31T09:07:52.351030: step 410, loss 0.551229.
Train: 2018-07-31T09:07:52.507211: step 411, loss 0.544659.
Train: 2018-07-31T09:07:52.663424: step 412, loss 0.515337.
Train: 2018-07-31T09:07:52.819639: step 413, loss 0.573383.
Train: 2018-07-31T09:07:52.991476: step 414, loss 0.514578.
Train: 2018-07-31T09:07:53.132065: step 415, loss 0.632163.
Train: 2018-07-31T09:07:53.303899: step 416, loss 0.54896.
Train: 2018-07-31T09:07:53.444522: step 417, loss 0.62491.
Train: 2018-07-31T09:07:53.616327: step 418, loss 0.669415.
Train: 2018-07-31T09:07:53.772539: step 419, loss 0.539822.
Train: 2018-07-31T09:07:53.913157: step 420, loss 0.502706.
Test: 2018-07-31T09:07:54.147483: step 420, loss 0.551273.
Train: 2018-07-31T09:07:54.303701: step 421, loss 0.589131.
Train: 2018-07-31T09:07:54.475531: step 422, loss 0.608546.
Train: 2018-07-31T09:07:54.616123: step 423, loss 0.565344.
Train: 2018-07-31T09:07:54.787957: step 424, loss 0.55289.
Train: 2018-07-31T09:07:54.944171: step 425, loss 0.595842.
Train: 2018-07-31T09:07:55.100354: step 426, loss 0.528586.
Train: 2018-07-31T09:07:55.240949: step 427, loss 0.552331.
Train: 2018-07-31T09:07:55.397160: step 428, loss 0.549119.
Train: 2018-07-31T09:07:55.553373: step 429, loss 0.556408.
Train: 2018-07-31T09:07:55.709586: step 430, loss 0.644272.
Test: 2018-07-31T09:07:55.943908: step 430, loss 0.552738.
Train: 2018-07-31T09:07:56.100121: step 431, loss 0.586337.
Train: 2018-07-31T09:07:56.271984: step 432, loss 0.560325.
Train: 2018-07-31T09:07:56.428167: step 433, loss 0.508449.
Train: 2018-07-31T09:07:56.584381: step 434, loss 0.583219.
Train: 2018-07-31T09:07:56.740594: step 435, loss 0.540988.
Train: 2018-07-31T09:07:56.896809: step 436, loss 0.653947.
Train: 2018-07-31T09:07:57.053047: step 437, loss 0.601839.
Train: 2018-07-31T09:07:57.209234: step 438, loss 0.490422.
Train: 2018-07-31T09:07:57.365480: step 439, loss 0.552438.
Train: 2018-07-31T09:07:57.521692: step 440, loss 0.626072.
Test: 2018-07-31T09:07:57.756012: step 440, loss 0.552252.
Train: 2018-07-31T09:07:57.912220: step 441, loss 0.559978.
Train: 2018-07-31T09:07:58.068408: step 442, loss 0.623347.
Train: 2018-07-31T09:07:58.224654: step 443, loss 0.582844.
Train: 2018-07-31T09:07:58.380867: step 444, loss 0.519165.
Train: 2018-07-31T09:07:58.537080: step 445, loss 0.55482.
Train: 2018-07-31T09:07:58.693263: step 446, loss 0.547363.
Train: 2018-07-31T09:07:58.849507: step 447, loss 0.578274.
Train: 2018-07-31T09:07:59.005714: step 448, loss 0.545111.
Train: 2018-07-31T09:07:59.161928: step 449, loss 0.572356.
Train: 2018-07-31T09:07:59.318118: step 450, loss 0.485961.
Test: 2018-07-31T09:07:59.552468: step 450, loss 0.551492.
Train: 2018-07-31T09:07:59.708680: step 451, loss 0.563309.
Train: 2018-07-31T09:07:59.864894: step 452, loss 0.577552.
Train: 2018-07-31T09:08:00.021076: step 453, loss 0.653478.
Train: 2018-07-31T09:08:00.177315: step 454, loss 0.516959.
Train: 2018-07-31T09:08:00.333504: step 455, loss 0.530433.
Train: 2018-07-31T09:08:00.505362: step 456, loss 0.582109.
Train: 2018-07-31T09:08:00.661581: step 457, loss 0.547137.
Train: 2018-07-31T09:08:00.817797: step 458, loss 0.658488.
Train: 2018-07-31T09:08:00.958387: step 459, loss 0.531457.
Train: 2018-07-31T09:08:01.130222: step 460, loss 0.609343.
Test: 2018-07-31T09:08:01.364512: step 460, loss 0.550752.
Train: 2018-07-31T09:08:01.520756: step 461, loss 0.554544.
Train: 2018-07-31T09:08:01.676969: step 462, loss 0.508234.
Train: 2018-07-31T09:08:01.833182: step 463, loss 0.547973.
Train: 2018-07-31T09:08:01.989365: step 464, loss 0.553756.
Train: 2018-07-31T09:08:02.145579: step 465, loss 0.646337.
Train: 2018-07-31T09:08:02.301793: step 466, loss 0.533137.
Train: 2018-07-31T09:08:02.473626: step 467, loss 0.54041.
Train: 2018-07-31T09:08:02.614244: step 468, loss 0.486281.
Train: 2018-07-31T09:08:02.786054: step 469, loss 0.555758.
Train: 2018-07-31T09:08:02.942297: step 470, loss 0.559765.
Test: 2018-07-31T09:08:03.176587: step 470, loss 0.550533.
Train: 2018-07-31T09:08:03.332835: step 471, loss 0.56001.
Train: 2018-07-31T09:08:03.489045: step 472, loss 0.54362.
Train: 2018-07-31T09:08:03.645229: step 473, loss 0.529321.
Train: 2018-07-31T09:08:03.817087: step 474, loss 0.562434.
Train: 2018-07-31T09:08:03.973308: step 475, loss 0.583458.
Train: 2018-07-31T09:08:04.129490: step 476, loss 0.55225.
Train: 2018-07-31T09:08:04.285733: step 477, loss 0.560295.
Train: 2018-07-31T09:08:04.441947: step 478, loss 0.57332.
Train: 2018-07-31T09:08:04.613752: step 479, loss 0.51156.
Train: 2018-07-31T09:08:04.769965: step 480, loss 0.466503.
Test: 2018-07-31T09:08:05.004313: step 480, loss 0.55019.
Train: 2018-07-31T09:08:05.160498: step 481, loss 0.509864.
Train: 2018-07-31T09:08:05.332333: step 482, loss 0.502113.
Train: 2018-07-31T09:08:05.488547: step 483, loss 0.536295.
Train: 2018-07-31T09:08:05.644760: step 484, loss 0.597043.
Train: 2018-07-31T09:08:05.800998: step 485, loss 0.580146.
Train: 2018-07-31T09:08:05.957216: step 486, loss 0.574742.
Train: 2018-07-31T09:08:06.113399: step 487, loss 0.555682.
Train: 2018-07-31T09:08:06.269614: step 488, loss 0.559249.
Train: 2018-07-31T09:08:06.425827: step 489, loss 0.678696.
Train: 2018-07-31T09:08:06.582040: step 490, loss 0.556605.
Test: 2018-07-31T09:08:06.816362: step 490, loss 0.550117.
Train: 2018-07-31T09:08:06.972605: step 491, loss 0.576802.
Train: 2018-07-31T09:08:07.128788: step 492, loss 0.586554.
Train: 2018-07-31T09:08:07.285001: step 493, loss 0.547607.
Train: 2018-07-31T09:08:07.425592: step 494, loss 0.550158.
Train: 2018-07-31T09:08:07.597428: step 495, loss 0.60562.
Train: 2018-07-31T09:08:07.753641: step 496, loss 0.518734.
Train: 2018-07-31T09:08:07.909854: step 497, loss 0.644097.
Train: 2018-07-31T09:08:08.050446: step 498, loss 0.555377.
Train: 2018-07-31T09:08:08.206661: step 499, loss 0.539386.
Train: 2018-07-31T09:08:08.362904: step 500, loss 0.544069.
Test: 2018-07-31T09:08:08.597225: step 500, loss 0.551244.
Train: 2018-07-31T09:08:09.315805: step 501, loss 0.535512.
Train: 2018-07-31T09:08:09.471988: step 502, loss 0.613291.
Train: 2018-07-31T09:08:09.643848: step 503, loss 0.615567.
Train: 2018-07-31T09:08:09.800066: step 504, loss 0.559357.
Train: 2018-07-31T09:08:09.956251: step 505, loss 0.572543.
Train: 2018-07-31T09:08:10.112463: step 506, loss 0.590047.
Train: 2018-07-31T09:08:10.268676: step 507, loss 0.587276.
Train: 2018-07-31T09:08:10.424890: step 508, loss 0.584252.
Train: 2018-07-31T09:08:10.581104: step 509, loss 0.613067.
Train: 2018-07-31T09:08:10.737348: step 510, loss 0.566938.
Test: 2018-07-31T09:08:10.971639: step 510, loss 0.552624.
Train: 2018-07-31T09:08:11.127876: step 511, loss 0.544857.
Train: 2018-07-31T09:08:11.284095: step 512, loss 0.5528.
Train: 2018-07-31T09:08:11.440302: step 513, loss 0.509179.
Train: 2018-07-31T09:08:11.596515: step 514, loss 0.573174.
Train: 2018-07-31T09:08:11.768326: step 515, loss 0.540184.
Train: 2018-07-31T09:08:11.940161: step 516, loss 0.487266.
Train: 2018-07-31T09:08:12.096405: step 517, loss 0.516463.
Train: 2018-07-31T09:08:12.236999: step 518, loss 0.589564.
Train: 2018-07-31T09:08:12.408801: step 519, loss 0.523504.
Train: 2018-07-31T09:08:12.565014: step 520, loss 0.547464.
Test: 2018-07-31T09:08:12.799360: step 520, loss 0.550207.
Train: 2018-07-31T09:08:12.955578: step 521, loss 0.550513.
Train: 2018-07-31T09:08:13.111762: step 522, loss 0.517012.
Train: 2018-07-31T09:08:13.267975: step 523, loss 0.609213.
Train: 2018-07-31T09:08:13.424189: step 524, loss 0.490916.
Train: 2018-07-31T09:08:13.580402: step 525, loss 0.593504.
Train: 2018-07-31T09:08:13.845964: step 526, loss 0.575883.
Train: 2018-07-31T09:08:14.127602: step 527, loss 0.471335.
Train: 2018-07-31T09:08:14.330700: step 528, loss 0.553518.
Train: 2018-07-31T09:08:14.518126: step 529, loss 0.615873.
Train: 2018-07-31T09:08:14.658747: step 530, loss 0.635761.
Test: 2018-07-31T09:08:14.908692: step 530, loss 0.549842.
Train: 2018-07-31T09:08:15.064873: step 531, loss 0.506402.
Train: 2018-07-31T09:08:15.221115: step 532, loss 0.554278.
Train: 2018-07-31T09:08:15.377300: step 533, loss 0.541451.
Train: 2018-07-31T09:08:15.533543: step 534, loss 0.546825.
Train: 2018-07-31T09:08:15.705347: step 535, loss 0.513137.
Train: 2018-07-31T09:08:15.845939: step 536, loss 0.649395.
Train: 2018-07-31T09:08:16.002188: step 537, loss 0.582082.
Train: 2018-07-31T09:08:16.158365: step 538, loss 0.522088.
Train: 2018-07-31T09:08:16.314610: step 539, loss 0.629757.
Train: 2018-07-31T09:08:16.470824: step 540, loss 0.595583.
Test: 2018-07-31T09:08:16.705115: step 540, loss 0.549723.
Train: 2018-07-31T09:08:16.861325: step 541, loss 0.535018.
Train: 2018-07-31T09:08:17.017539: step 542, loss 0.510222.
Train: 2018-07-31T09:08:17.189398: step 543, loss 0.512721.
Train: 2018-07-31T09:08:17.345619: step 544, loss 0.553148.
Train: 2018-07-31T09:08:17.501832: step 545, loss 0.615023.
Train: 2018-07-31T09:08:17.658045: step 546, loss 0.581705.
Train: 2018-07-31T09:08:17.814227: step 547, loss 0.620161.
Train: 2018-07-31T09:08:17.970467: step 548, loss 0.571151.
Train: 2018-07-31T09:08:18.126656: step 549, loss 0.598167.
Train: 2018-07-31T09:08:18.282893: step 550, loss 0.604136.
Test: 2018-07-31T09:08:18.517219: step 550, loss 0.550569.
Train: 2018-07-31T09:08:18.689024: step 551, loss 0.487071.
Train: 2018-07-31T09:08:18.845238: step 552, loss 0.617458.
Train: 2018-07-31T09:08:19.001450: step 553, loss 0.53277.
Train: 2018-07-31T09:08:19.157663: step 554, loss 0.58076.
Train: 2018-07-31T09:08:19.360741: step 555, loss 0.575808.
Train: 2018-07-31T09:08:19.516984: step 556, loss 0.611935.
Train: 2018-07-31T09:08:19.673170: step 557, loss 0.544599.
Train: 2018-07-31T09:08:19.829411: step 558, loss 0.584685.
Train: 2018-07-31T09:08:19.985595: step 559, loss 0.588674.
Train: 2018-07-31T09:08:20.141811: step 560, loss 0.600538.
Test: 2018-07-31T09:08:20.376162: step 560, loss 0.551325.
Train: 2018-07-31T09:08:20.626099: step 561, loss 0.582349.
Train: 2018-07-31T09:08:20.782321: step 562, loss 0.548831.
Train: 2018-07-31T09:08:20.938496: step 563, loss 0.513672.
Train: 2018-07-31T09:08:21.094710: step 564, loss 0.555811.
Train: 2018-07-31T09:08:21.266545: step 565, loss 0.596783.
Train: 2018-07-31T09:08:21.422788: step 566, loss 0.556001.
Train: 2018-07-31T09:08:21.578970: step 567, loss 0.596038.
Train: 2018-07-31T09:08:21.735185: step 568, loss 0.599327.
Train: 2018-07-31T09:08:21.907019: step 569, loss 0.566509.
Train: 2018-07-31T09:08:22.047643: step 570, loss 0.53777.
Test: 2018-07-31T09:08:22.297552: step 570, loss 0.550929.
Train: 2018-07-31T09:08:22.453796: step 571, loss 0.60846.
Train: 2018-07-31T09:08:22.594388: step 572, loss 0.539408.
Train: 2018-07-31T09:08:22.750605: step 573, loss 0.576556.
Train: 2018-07-31T09:08:22.906816: step 574, loss 0.548056.
Train: 2018-07-31T09:08:23.063030: step 575, loss 0.593561.
Train: 2018-07-31T09:08:23.219245: step 576, loss 0.508802.
Train: 2018-07-31T09:08:23.375456: step 577, loss 0.536007.
Train: 2018-07-31T09:08:23.531669: step 578, loss 0.569403.
Train: 2018-07-31T09:08:23.687884: step 579, loss 0.575225.
Train: 2018-07-31T09:08:23.844097: step 580, loss 0.505625.
Test: 2018-07-31T09:08:24.094038: step 580, loss 0.549916.
Train: 2018-07-31T09:08:24.250252: step 581, loss 0.546553.
Train: 2018-07-31T09:08:24.406435: step 582, loss 0.503219.
Train: 2018-07-31T09:08:24.562648: step 583, loss 0.581288.
Train: 2018-07-31T09:08:24.718861: step 584, loss 0.507567.
Train: 2018-07-31T09:08:24.875075: step 585, loss 0.533007.
Train: 2018-07-31T09:08:25.031312: step 586, loss 0.627349.
Train: 2018-07-31T09:08:25.187532: step 587, loss 0.564667.
Train: 2018-07-31T09:08:25.343714: step 588, loss 0.589382.
Train: 2018-07-31T09:08:25.499929: step 589, loss 0.64188.
Train: 2018-07-31T09:08:25.656141: step 590, loss 0.52021.
Test: 2018-07-31T09:08:25.906107: step 590, loss 0.549249.
Train: 2018-07-31T09:08:26.062327: step 591, loss 0.608814.
Train: 2018-07-31T09:08:26.218540: step 592, loss 0.60092.
Train: 2018-07-31T09:08:26.374748: step 593, loss 0.626261.
Train: 2018-07-31T09:08:26.530969: step 594, loss 0.600947.
Train: 2018-07-31T09:08:26.687181: step 595, loss 0.555994.
Train: 2018-07-31T09:08:26.858985: step 596, loss 0.580644.
Train: 2018-07-31T09:08:27.015223: step 597, loss 0.547054.
Train: 2018-07-31T09:08:27.171436: step 598, loss 0.664762.
Train: 2018-07-31T09:08:27.327649: step 599, loss 0.605397.
Train: 2018-07-31T09:08:27.483863: step 600, loss 0.598473.
Test: 2018-07-31T09:08:27.718159: step 600, loss 0.551006.
Train: 2018-07-31T09:08:28.421143: step 601, loss 0.509111.
Train: 2018-07-31T09:08:28.592953: step 602, loss 0.586077.
Train: 2018-07-31T09:08:28.733576: step 603, loss 0.549892.
Train: 2018-07-31T09:08:28.889793: step 604, loss 0.598259.
Train: 2018-07-31T09:08:29.046004: step 605, loss 0.625614.
Train: 2018-07-31T09:08:29.202217: step 606, loss 0.587374.
Train: 2018-07-31T09:08:29.374020: step 607, loss 0.581234.
Train: 2018-07-31T09:08:29.530258: step 608, loss 0.542355.
Train: 2018-07-31T09:08:29.686480: step 609, loss 0.611012.
Train: 2018-07-31T09:08:29.842691: step 610, loss 0.527354.
Test: 2018-07-31T09:08:30.076982: step 610, loss 0.552923.
Train: 2018-07-31T09:08:30.233194: step 611, loss 0.595605.
Train: 2018-07-31T09:08:30.389438: step 612, loss 0.613065.
Train: 2018-07-31T09:08:30.545622: step 613, loss 0.602004.
Train: 2018-07-31T09:08:30.701865: step 614, loss 0.595343.
Train: 2018-07-31T09:08:30.858079: step 615, loss 0.565156.
Train: 2018-07-31T09:08:31.029884: step 616, loss 0.573091.
Train: 2018-07-31T09:08:31.186097: step 617, loss 0.610346.
Train: 2018-07-31T09:08:31.342340: step 618, loss 0.612255.
Train: 2018-07-31T09:08:31.482903: step 619, loss 0.545954.
Train: 2018-07-31T09:08:31.639146: step 620, loss 0.502018.
Test: 2018-07-31T09:08:31.873467: step 620, loss 0.552854.
Train: 2018-07-31T09:08:32.045271: step 621, loss 0.514203.
Train: 2018-07-31T09:08:32.201515: step 622, loss 0.583025.
Train: 2018-07-31T09:08:32.357697: step 623, loss 0.549112.
Train: 2018-07-31T09:08:32.498320: step 624, loss 0.615771.
Train: 2018-07-31T09:08:32.654532: step 625, loss 0.571461.
Train: 2018-07-31T09:08:32.810746: step 626, loss 0.525218.
Train: 2018-07-31T09:08:32.966930: step 627, loss 0.567275.
Train: 2018-07-31T09:08:33.123142: step 628, loss 0.538124.
Train: 2018-07-31T09:08:33.279386: step 629, loss 0.524812.
Train: 2018-07-31T09:08:33.435594: step 630, loss 0.545844.
Test: 2018-07-31T09:08:33.669921: step 630, loss 0.549986.
Train: 2018-07-31T09:08:33.826137: step 631, loss 0.547065.
Train: 2018-07-31T09:08:33.997939: step 632, loss 0.588918.
Train: 2018-07-31T09:08:34.154151: step 633, loss 0.561961.
Train: 2018-07-31T09:08:34.310365: step 634, loss 0.54736.
Train: 2018-07-31T09:08:34.466579: step 635, loss 0.558167.
Train: 2018-07-31T09:08:34.622791: step 636, loss 0.577214.
Train: 2018-07-31T09:08:34.763383: step 637, loss 0.45166.
Train: 2018-07-31T09:08:34.919633: step 638, loss 0.455107.
Train: 2018-07-31T09:08:35.075810: step 639, loss 0.439834.
Train: 2018-07-31T09:08:35.232025: step 640, loss 0.503536.
Test: 2018-07-31T09:08:35.466344: step 640, loss 0.54908.
Train: 2018-07-31T09:08:35.622557: step 641, loss 0.655475.
Train: 2018-07-31T09:08:35.778772: step 642, loss 0.579983.
Train: 2018-07-31T09:08:35.934985: step 643, loss 0.549231.
Train: 2018-07-31T09:08:36.091228: step 644, loss 0.558386.
Train: 2018-07-31T09:08:36.247436: step 645, loss 0.619889.
Train: 2018-07-31T09:08:36.403655: step 646, loss 0.50697.
Train: 2018-07-31T09:08:36.559869: step 647, loss 0.539681.
Train: 2018-07-31T09:08:36.716078: step 648, loss 0.552734.
Train: 2018-07-31T09:08:36.872265: step 649, loss 0.539739.
Train: 2018-07-31T09:08:37.028509: step 650, loss 0.691949.
Test: 2018-07-31T09:08:37.262800: step 650, loss 0.549191.
Train: 2018-07-31T09:08:37.419012: step 651, loss 0.537292.
Train: 2018-07-31T09:08:37.575227: step 652, loss 0.544694.
Train: 2018-07-31T09:08:37.731438: step 653, loss 0.612251.
Train: 2018-07-31T09:08:37.887684: step 654, loss 0.616082.
Train: 2018-07-31T09:08:38.043890: step 655, loss 0.574257.
Train: 2018-07-31T09:08:38.200109: step 656, loss 0.600942.
Train: 2018-07-31T09:08:38.356293: step 657, loss 0.519447.
Train: 2018-07-31T09:08:38.512542: step 658, loss 0.505557.
Train: 2018-07-31T09:08:38.668750: step 659, loss 0.615277.
Train: 2018-07-31T09:08:38.824932: step 660, loss 0.539989.
Test: 2018-07-31T09:08:39.059254: step 660, loss 0.549474.
Train: 2018-07-31T09:08:39.215490: step 661, loss 0.622935.
Train: 2018-07-31T09:08:39.371680: step 662, loss 0.512754.
Train: 2018-07-31T09:08:39.527893: step 663, loss 0.541678.
Train: 2018-07-31T09:08:39.684107: step 664, loss 0.485866.
Train: 2018-07-31T09:08:39.840353: step 665, loss 0.596741.
Train: 2018-07-31T09:08:39.996563: step 666, loss 0.478152.
Train: 2018-07-31T09:08:40.152748: step 667, loss 0.571559.
Train: 2018-07-31T09:08:40.308992: step 668, loss 0.556111.
Train: 2018-07-31T09:08:40.465204: step 669, loss 0.529086.
Train: 2018-07-31T09:08:40.621418: step 670, loss 0.711101.
Test: 2018-07-31T09:08:40.871353: step 670, loss 0.549478.
Train: 2018-07-31T09:08:41.027544: step 671, loss 0.603388.
Train: 2018-07-31T09:08:41.199401: step 672, loss 0.587054.
Train: 2018-07-31T09:08:41.340005: step 673, loss 0.548618.
Train: 2018-07-31T09:08:41.496207: step 674, loss 0.547628.
Train: 2018-07-31T09:08:41.652426: step 675, loss 0.599231.
Train: 2018-07-31T09:08:41.824261: step 676, loss 0.515446.
Train: 2018-07-31T09:08:41.964824: step 677, loss 0.547331.
Train: 2018-07-31T09:08:42.121066: step 678, loss 0.625445.
Train: 2018-07-31T09:08:42.277279: step 679, loss 0.53996.
Train: 2018-07-31T09:08:42.433488: step 680, loss 0.565061.
Test: 2018-07-31T09:08:42.683403: step 680, loss 0.549752.
Train: 2018-07-31T09:08:42.839648: step 681, loss 0.531227.
Train: 2018-07-31T09:08:42.995861: step 682, loss 0.563623.
Train: 2018-07-31T09:08:43.152069: step 683, loss 0.506705.
Train: 2018-07-31T09:08:43.308294: step 684, loss 0.514103.
Train: 2018-07-31T09:08:43.464502: step 685, loss 0.533154.
Train: 2018-07-31T09:08:43.620684: step 686, loss 0.536087.
Train: 2018-07-31T09:08:43.761306: step 687, loss 0.613262.
Train: 2018-07-31T09:08:43.917490: step 688, loss 0.559759.
Train: 2018-07-31T09:08:44.089326: step 689, loss 0.562878.
Train: 2018-07-31T09:08:44.245539: step 690, loss 0.613894.
Test: 2018-07-31T09:08:44.479860: step 690, loss 0.548989.
Train: 2018-07-31T09:08:44.636097: step 691, loss 0.48553.
Train: 2018-07-31T09:08:44.792317: step 692, loss 0.523353.
Train: 2018-07-31T09:08:44.948529: step 693, loss 0.556053.
Train: 2018-07-31T09:08:45.104712: step 694, loss 0.580067.
Train: 2018-07-31T09:08:45.276571: step 695, loss 0.557993.
Train: 2018-07-31T09:08:45.432791: step 696, loss 0.537257.
Train: 2018-07-31T09:08:45.604620: step 697, loss 0.598377.
Train: 2018-07-31T09:08:45.760809: step 698, loss 0.541993.
Train: 2018-07-31T09:08:45.932668: step 699, loss 0.574133.
Train: 2018-07-31T09:08:46.088887: step 700, loss 0.553389.
Test: 2018-07-31T09:08:46.323179: step 700, loss 0.548783.
Train: 2018-07-31T09:08:47.073001: step 701, loss 0.608276.
Train: 2018-07-31T09:08:47.229245: step 702, loss 0.555721.
Train: 2018-07-31T09:08:47.385427: step 703, loss 0.606194.
Train: 2018-07-31T09:08:47.541666: step 704, loss 0.531454.
Train: 2018-07-31T09:08:47.697855: step 705, loss 0.554933.
Train: 2018-07-31T09:08:47.854098: step 706, loss 0.527884.
Train: 2018-07-31T09:08:47.994690: step 707, loss 0.550986.
Train: 2018-07-31T09:08:48.150874: step 708, loss 0.476556.
Train: 2018-07-31T09:08:48.307090: step 709, loss 0.599816.
Train: 2018-07-31T09:08:48.463301: step 710, loss 0.608606.
Test: 2018-07-31T09:08:48.697655: step 710, loss 0.548898.
Train: 2018-07-31T09:08:48.853864: step 711, loss 0.489604.
Train: 2018-07-31T09:08:49.010047: step 712, loss 0.547781.
Train: 2018-07-31T09:08:49.166297: step 713, loss 0.546889.
Train: 2018-07-31T09:08:49.322505: step 714, loss 0.652489.
Train: 2018-07-31T09:08:49.478718: step 715, loss 0.571635.
Train: 2018-07-31T09:08:49.634932: step 716, loss 0.58999.
Train: 2018-07-31T09:08:49.791146: step 717, loss 0.537622.
Train: 2018-07-31T09:08:49.947329: step 718, loss 0.632683.
Train: 2018-07-31T09:08:50.103542: step 719, loss 0.59552.
Train: 2018-07-31T09:08:50.259791: step 720, loss 0.522441.
Test: 2018-07-31T09:08:50.494107: step 720, loss 0.549211.
Train: 2018-07-31T09:08:50.665910: step 721, loss 0.520638.
Train: 2018-07-31T09:08:50.806533: step 722, loss 0.580171.
Train: 2018-07-31T09:08:50.962715: step 723, loss 0.614831.
Train: 2018-07-31T09:08:51.134550: step 724, loss 0.524797.
Train: 2018-07-31T09:08:51.290764: step 725, loss 0.52237.
Train: 2018-07-31T09:08:51.447007: step 726, loss 0.600052.
Train: 2018-07-31T09:08:51.603221: step 727, loss 0.621677.
Train: 2018-07-31T09:08:51.759435: step 728, loss 0.513315.
Train: 2018-07-31T09:08:51.931239: step 729, loss 0.62141.
Train: 2018-07-31T09:08:52.087484: step 730, loss 0.521291.
Test: 2018-07-31T09:08:52.321772: step 730, loss 0.549573.
Train: 2018-07-31T09:08:52.477985: step 731, loss 0.585901.
Train: 2018-07-31T09:08:52.634198: step 732, loss 0.637385.
Train: 2018-07-31T09:08:52.790444: step 733, loss 0.554897.
Train: 2018-07-31T09:08:52.946627: step 734, loss 0.582842.
Train: 2018-07-31T09:08:53.102840: step 735, loss 0.547841.
Train: 2018-07-31T09:08:53.243462: step 736, loss 0.563818.
Train: 2018-07-31T09:08:53.399645: step 737, loss 0.620275.
Train: 2018-07-31T09:08:53.571479: step 738, loss 0.629752.
Train: 2018-07-31T09:08:53.727717: step 739, loss 0.57155.
Train: 2018-07-31T09:08:53.883937: step 740, loss 0.576763.
Test: 2018-07-31T09:08:54.118251: step 740, loss 0.550301.
Train: 2018-07-31T09:08:54.274464: step 741, loss 0.555628.
Train: 2018-07-31T09:08:54.446274: step 742, loss 0.532023.
Train: 2018-07-31T09:08:54.602518: step 743, loss 0.563408.
Train: 2018-07-31T09:08:54.758701: step 744, loss 0.537547.
Train: 2018-07-31T09:08:54.914948: step 745, loss 0.556845.
Train: 2018-07-31T09:08:55.071158: step 746, loss 0.590755.
Train: 2018-07-31T09:08:55.211720: step 747, loss 0.594215.
Train: 2018-07-31T09:08:55.383585: step 748, loss 0.581598.
Train: 2018-07-31T09:08:55.539769: step 749, loss 0.573127.
Train: 2018-07-31T09:08:55.696012: step 750, loss 0.456134.
Test: 2018-07-31T09:08:55.930304: step 750, loss 0.549889.
Train: 2018-07-31T09:08:56.086519: step 751, loss 0.609124.
Train: 2018-07-31T09:08:56.242728: step 752, loss 0.557188.
Train: 2018-07-31T09:08:56.398943: step 753, loss 0.593898.
Train: 2018-07-31T09:08:56.570776: step 754, loss 0.555494.
Train: 2018-07-31T09:08:56.711368: step 755, loss 0.497311.
Train: 2018-07-31T09:08:56.867614: step 756, loss 0.55602.
Train: 2018-07-31T09:08:57.023799: step 757, loss 0.508658.
Train: 2018-07-31T09:08:57.180033: step 758, loss 0.60764.
Train: 2018-07-31T09:08:57.336247: step 759, loss 0.504735.
Train: 2018-07-31T09:08:57.492466: step 760, loss 0.622587.
Test: 2018-07-31T09:08:57.726758: step 760, loss 0.549.
Train: 2018-07-31T09:08:57.898615: step 761, loss 0.590503.
Train: 2018-07-31T09:08:58.054835: step 762, loss 0.595193.
Train: 2018-07-31T09:08:58.211054: step 763, loss 0.572495.
Train: 2018-07-31T09:08:58.367256: step 764, loss 0.555456.
Train: 2018-07-31T09:08:58.523469: step 765, loss 0.570435.
Train: 2018-07-31T09:08:58.664069: step 766, loss 0.564593.
Train: 2018-07-31T09:08:58.820284: step 767, loss 0.529928.
Train: 2018-07-31T09:08:58.976494: step 768, loss 0.64856.
Train: 2018-07-31T09:08:59.132707: step 769, loss 0.51134.
Train: 2018-07-31T09:08:59.288930: step 770, loss 0.510025.
Test: 2018-07-31T09:08:59.523212: step 770, loss 0.548939.
Train: 2018-07-31T09:08:59.679425: step 771, loss 0.501637.
Train: 2018-07-31T09:08:59.838220: step 772, loss 0.544616.
Train: 2018-07-31T09:09:00.010060: step 773, loss 0.614349.
Train: 2018-07-31T09:09:00.166244: step 774, loss 0.519241.
Train: 2018-07-31T09:09:00.322457: step 775, loss 0.570973.
Train: 2018-07-31T09:09:00.478671: step 776, loss 0.534757.
Train: 2018-07-31T09:09:00.634908: step 777, loss 0.58783.
Train: 2018-07-31T09:09:00.791097: step 778, loss 0.583032.
Train: 2018-07-31T09:09:00.947335: step 779, loss 0.613817.
Train: 2018-07-31T09:09:01.119145: step 780, loss 0.51294.
Test: 2018-07-31T09:09:01.353497: step 780, loss 0.548544.
Train: 2018-07-31T09:09:01.509703: step 781, loss 0.581907.
Train: 2018-07-31T09:09:01.665923: step 782, loss 0.554064.
Train: 2018-07-31T09:09:01.822136: step 783, loss 0.55574.
Train: 2018-07-31T09:09:01.978345: step 784, loss 0.486033.
Train: 2018-07-31T09:09:02.150155: step 785, loss 0.636835.
Train: 2018-07-31T09:09:02.306367: step 786, loss 0.60845.
Train: 2018-07-31T09:09:02.462611: step 787, loss 0.449025.
Train: 2018-07-31T09:09:02.618793: step 788, loss 0.62628.
Train: 2018-07-31T09:09:02.775007: step 789, loss 0.556918.
Train: 2018-07-31T09:09:02.931220: step 790, loss 0.559344.
Test: 2018-07-31T09:09:03.165541: step 790, loss 0.548627.
Train: 2018-07-31T09:09:03.321755: step 791, loss 0.574245.
Train: 2018-07-31T09:09:03.477996: step 792, loss 0.483786.
Train: 2018-07-31T09:09:03.634181: step 793, loss 0.546559.
Train: 2018-07-31T09:09:03.790394: step 794, loss 0.509768.
Train: 2018-07-31T09:09:03.946640: step 795, loss 0.573426.
Train: 2018-07-31T09:09:04.102852: step 796, loss 0.501829.
Train: 2018-07-31T09:09:04.259065: step 797, loss 0.606073.
Train: 2018-07-31T09:09:04.415279: step 798, loss 0.590484.
Train: 2018-07-31T09:09:04.571463: step 799, loss 0.615624.
Train: 2018-07-31T09:09:04.727706: step 800, loss 0.602317.
Test: 2018-07-31T09:09:04.961997: step 800, loss 0.548568.
Train: 2018-07-31T09:09:05.696228: step 801, loss 0.544162.
Train: 2018-07-31T09:09:05.868064: step 802, loss 0.61764.
Train: 2018-07-31T09:09:06.024272: step 803, loss 0.536472.
Train: 2018-07-31T09:09:06.180461: step 804, loss 0.553564.
Train: 2018-07-31T09:09:06.336704: step 805, loss 0.606789.
Train: 2018-07-31T09:09:06.508545: step 806, loss 0.520642.
Train: 2018-07-31T09:09:06.664747: step 807, loss 0.503645.
Train: 2018-07-31T09:09:06.820971: step 808, loss 0.529108.
Train: 2018-07-31T09:09:06.977179: step 809, loss 0.48584.
Train: 2018-07-31T09:09:07.133393: step 810, loss 0.535968.
Test: 2018-07-31T09:09:07.383331: step 810, loss 0.548675.
Train: 2018-07-31T09:09:07.539549: step 811, loss 0.591849.
Train: 2018-07-31T09:09:07.680109: step 812, loss 0.6678.
Train: 2018-07-31T09:09:07.851975: step 813, loss 0.562584.
Train: 2018-07-31T09:09:07.992536: step 814, loss 0.536282.
Train: 2018-07-31T09:09:08.164371: step 815, loss 0.573316.
Train: 2018-07-31T09:09:08.320615: step 816, loss 0.50481.
Train: 2018-07-31T09:09:08.476827: step 817, loss 0.650009.
Train: 2018-07-31T09:09:08.633011: step 818, loss 0.555583.
Train: 2018-07-31T09:09:08.789223: step 819, loss 0.596079.
Train: 2018-07-31T09:09:08.945437: step 820, loss 0.562494.
Test: 2018-07-31T09:09:09.179760: step 820, loss 0.548788.
Train: 2018-07-31T09:09:09.335995: step 821, loss 0.529092.
Train: 2018-07-31T09:09:09.492185: step 822, loss 0.607265.
Train: 2018-07-31T09:09:09.648398: step 823, loss 0.562993.
Train: 2018-07-31T09:09:09.804645: step 824, loss 0.510187.
Train: 2018-07-31T09:09:09.960824: step 825, loss 0.555038.
Train: 2018-07-31T09:09:10.117038: step 826, loss 0.574034.
Train: 2018-07-31T09:09:10.273252: step 827, loss 0.530471.
Train: 2018-07-31T09:09:10.427354: step 828, loss 0.531746.
Train: 2018-07-31T09:09:10.583568: step 829, loss 0.553504.
Train: 2018-07-31T09:09:10.739813: step 830, loss 0.537888.
Test: 2018-07-31T09:09:10.974103: step 830, loss 0.548697.
Train: 2018-07-31T09:09:11.145967: step 831, loss 0.534691.
Train: 2018-07-31T09:09:11.286558: step 832, loss 0.547062.
Train: 2018-07-31T09:09:11.442772: step 833, loss 0.624591.
Train: 2018-07-31T09:09:11.598956: step 834, loss 0.553232.
Train: 2018-07-31T09:09:11.755169: step 835, loss 0.560436.
Train: 2018-07-31T09:09:11.911383: step 836, loss 0.651002.
Train: 2018-07-31T09:09:12.067594: step 837, loss 0.555322.
Train: 2018-07-31T09:09:12.223808: step 838, loss 0.561694.
Train: 2018-07-31T09:09:12.380021: step 839, loss 0.503484.
Train: 2018-07-31T09:09:12.536235: step 840, loss 0.546817.
Test: 2018-07-31T09:09:12.770589: step 840, loss 0.548595.
Train: 2018-07-31T09:09:12.926768: step 841, loss 0.552705.
Train: 2018-07-31T09:09:13.098603: step 842, loss 0.51217.
Train: 2018-07-31T09:09:13.254847: step 843, loss 0.537018.
Train: 2018-07-31T09:09:13.411030: step 844, loss 0.625589.
Train: 2018-07-31T09:09:13.567268: step 845, loss 0.523384.
Train: 2018-07-31T09:09:13.723489: step 846, loss 0.623021.
Train: 2018-07-31T09:09:13.879670: step 847, loss 0.566061.
Train: 2018-07-31T09:09:14.035885: step 848, loss 0.527498.
Train: 2018-07-31T09:09:14.192098: step 849, loss 0.580784.
Train: 2018-07-31T09:09:14.348311: step 850, loss 0.598718.
Test: 2018-07-31T09:09:14.582662: step 850, loss 0.54855.
Train: 2018-07-31T09:09:14.754496: step 851, loss 0.546685.
Train: 2018-07-31T09:09:14.910679: step 852, loss 0.553553.
Train: 2018-07-31T09:09:15.068901: step 853, loss 0.655345.
Train: 2018-07-31T09:09:15.240734: step 854, loss 0.59951.
Train: 2018-07-31T09:09:15.396949: step 855, loss 0.548215.
Train: 2018-07-31T09:09:15.553191: step 856, loss 0.545915.
Train: 2018-07-31T09:09:15.709404: step 857, loss 0.529907.
Train: 2018-07-31T09:09:15.865618: step 858, loss 0.545616.
Train: 2018-07-31T09:09:16.021832: step 859, loss 0.519183.
Train: 2018-07-31T09:09:16.178014: step 860, loss 0.542664.
Test: 2018-07-31T09:09:16.412366: step 860, loss 0.548894.
Train: 2018-07-31T09:09:16.568573: step 861, loss 0.578819.
Train: 2018-07-31T09:09:16.724793: step 862, loss 0.555579.
Train: 2018-07-31T09:09:16.880975: step 863, loss 0.53621.
Train: 2018-07-31T09:09:17.037188: step 864, loss 0.538097.
Train: 2018-07-31T09:09:17.193402: step 865, loss 0.598426.
Train: 2018-07-31T09:09:17.349616: step 866, loss 0.475797.
Train: 2018-07-31T09:09:17.505883: step 867, loss 0.537476.
Train: 2018-07-31T09:09:17.662072: step 868, loss 0.624332.
Train: 2018-07-31T09:09:17.818255: step 869, loss 0.589449.
Train: 2018-07-31T09:09:17.974500: step 870, loss 0.641651.
Test: 2018-07-31T09:09:18.208789: step 870, loss 0.548485.
Train: 2018-07-31T09:09:18.365033: step 871, loss 0.563601.
Train: 2018-07-31T09:09:18.521247: step 872, loss 0.553675.
Train: 2018-07-31T09:09:18.677429: step 873, loss 0.588932.
Train: 2018-07-31T09:09:18.833673: step 874, loss 0.558627.
Train: 2018-07-31T09:09:18.989856: step 875, loss 0.528271.
Train: 2018-07-31T09:09:19.146101: step 876, loss 0.560636.
Train: 2018-07-31T09:09:19.286661: step 877, loss 0.644323.
Train: 2018-07-31T09:09:19.458496: step 878, loss 0.617115.
Train: 2018-07-31T09:09:19.599115: step 879, loss 0.537662.
Train: 2018-07-31T09:09:19.755332: step 880, loss 0.581128.
Test: 2018-07-31T09:09:20.005243: step 880, loss 0.548988.
Train: 2018-07-31T09:09:20.161458: step 881, loss 0.584557.
Train: 2018-07-31T09:09:20.317702: step 882, loss 0.561789.
Train: 2018-07-31T09:09:20.473914: step 883, loss 0.544929.
Train: 2018-07-31T09:09:20.630097: step 884, loss 0.545117.
Train: 2018-07-31T09:09:20.786342: step 885, loss 0.615187.
Train: 2018-07-31T09:09:20.942524: step 886, loss 0.4818.
Train: 2018-07-31T09:09:21.098768: step 887, loss 0.603423.
Train: 2018-07-31T09:09:21.239329: step 888, loss 0.52218.
Train: 2018-07-31T09:09:21.395543: step 889, loss 0.523245.
Train: 2018-07-31T09:09:21.567403: step 890, loss 0.47117.
Test: 2018-07-31T09:09:21.801700: step 890, loss 0.549065.
Train: 2018-07-31T09:09:21.957941: step 891, loss 0.572047.
Train: 2018-07-31T09:09:22.114155: step 892, loss 0.658987.
Train: 2018-07-31T09:09:22.285991: step 893, loss 0.572472.
Train: 2018-07-31T09:09:22.442209: step 894, loss 0.551481.
Train: 2018-07-31T09:09:22.582766: step 895, loss 0.545917.
Train: 2018-07-31T09:09:22.738978: step 896, loss 0.614616.
Train: 2018-07-31T09:09:22.895192: step 897, loss 0.594973.
Train: 2018-07-31T09:09:23.051435: step 898, loss 0.535862.
Train: 2018-07-31T09:09:23.207619: step 899, loss 0.604356.
Train: 2018-07-31T09:09:23.363863: step 900, loss 0.65054.
Test: 2018-07-31T09:09:23.598183: step 900, loss 0.548795.
Train: 2018-07-31T09:09:24.316764: step 901, loss 0.530208.
Train: 2018-07-31T09:09:24.472978: step 902, loss 0.539527.
Train: 2018-07-31T09:09:24.629161: step 903, loss 0.587739.
Train: 2018-07-31T09:09:24.785399: step 904, loss 0.573485.
Train: 2018-07-31T09:09:24.941618: step 905, loss 0.579859.
Train: 2018-07-31T09:09:25.097825: step 906, loss 0.422386.
Train: 2018-07-31T09:09:25.254045: step 907, loss 0.546041.
Train: 2018-07-31T09:09:25.410259: step 908, loss 0.569845.
Train: 2018-07-31T09:09:25.566465: step 909, loss 0.608638.
Train: 2018-07-31T09:09:25.722654: step 910, loss 0.587898.
Test: 2018-07-31T09:09:25.957006: step 910, loss 0.548769.
Train: 2018-07-31T09:09:26.113219: step 911, loss 0.503393.
Train: 2018-07-31T09:09:26.269402: step 912, loss 0.563439.
Train: 2018-07-31T09:09:26.425650: step 913, loss 0.544729.
Train: 2018-07-31T09:09:26.581859: step 914, loss 0.654419.
Train: 2018-07-31T09:09:26.738066: step 915, loss 0.57232.
Train: 2018-07-31T09:09:26.909908: step 916, loss 0.536076.
Train: 2018-07-31T09:09:27.066120: step 917, loss 0.502257.
Train: 2018-07-31T09:09:27.222303: step 918, loss 0.580722.
Train: 2018-07-31T09:09:27.362896: step 919, loss 0.538999.
Train: 2018-07-31T09:09:27.534755: step 920, loss 0.502457.
Test: 2018-07-31T09:09:27.769085: step 920, loss 0.548451.
Train: 2018-07-31T09:09:27.925264: step 921, loss 0.562604.
Train: 2018-07-31T09:09:28.081507: step 922, loss 0.605787.
Train: 2018-07-31T09:09:28.237721: step 923, loss 0.503019.
Train: 2018-07-31T09:09:28.393934: step 924, loss 0.58197.
Train: 2018-07-31T09:09:28.550118: step 925, loss 0.50779.
Train: 2018-07-31T09:09:28.706361: step 926, loss 0.606375.
Train: 2018-07-31T09:09:28.846954: step 927, loss 0.587483.
Train: 2018-07-31T09:09:29.003161: step 928, loss 0.537588.
Train: 2018-07-31T09:09:29.159380: step 929, loss 0.539039.
Train: 2018-07-31T09:09:29.315594: step 930, loss 0.501817.
Test: 2018-07-31T09:09:29.565505: step 930, loss 0.548197.
Train: 2018-07-31T09:09:29.721719: step 931, loss 0.589229.
Train: 2018-07-31T09:09:29.877932: step 932, loss 0.606363.
Train: 2018-07-31T09:09:30.034145: step 933, loss 0.491863.
Train: 2018-07-31T09:09:30.190388: step 934, loss 0.552597.
Train: 2018-07-31T09:09:30.346596: step 935, loss 0.541359.
Train: 2018-07-31T09:09:30.502785: step 936, loss 0.516292.
Train: 2018-07-31T09:09:30.643408: step 937, loss 0.53952.
Train: 2018-07-31T09:09:30.815213: step 938, loss 0.517896.
Train: 2018-07-31T09:09:30.971462: step 939, loss 0.652999.
Train: 2018-07-31T09:09:31.127669: step 940, loss 0.611459.
Test: 2018-07-31T09:09:31.361959: step 940, loss 0.548096.
Train: 2018-07-31T09:09:31.518172: step 941, loss 0.556665.
Train: 2018-07-31T09:09:31.674387: step 942, loss 0.538841.
Train: 2018-07-31T09:09:31.830599: step 943, loss 0.669025.
Train: 2018-07-31T09:09:31.986838: step 944, loss 0.582759.
Train: 2018-07-31T09:09:32.143056: step 945, loss 0.569752.
Train: 2018-07-31T09:09:32.314887: step 946, loss 0.545178.
Train: 2018-07-31T09:09:32.471074: step 947, loss 0.590625.
Train: 2018-07-31T09:09:32.627318: step 948, loss 0.666842.
Train: 2018-07-31T09:09:32.783537: step 949, loss 0.545272.
Train: 2018-07-31T09:09:32.939715: step 950, loss 0.613766.
Test: 2018-07-31T09:09:33.174066: step 950, loss 0.548896.
Train: 2018-07-31T09:09:33.330249: step 951, loss 0.496733.
Train: 2018-07-31T09:09:33.486492: step 952, loss 0.512539.
Train: 2018-07-31T09:09:33.642705: step 953, loss 0.538112.
Train: 2018-07-31T09:09:33.798919: step 954, loss 0.586655.
Train: 2018-07-31T09:09:33.955127: step 955, loss 0.548468.
Train: 2018-07-31T09:09:34.111346: step 956, loss 0.587852.
Train: 2018-07-31T09:09:34.267562: step 957, loss 0.522665.
Train: 2018-07-31T09:09:34.423742: step 958, loss 0.603378.
Train: 2018-07-31T09:09:34.579955: step 959, loss 0.572568.
Train: 2018-07-31T09:09:34.736199: step 960, loss 0.496828.
Test: 2018-07-31T09:09:34.970520: step 960, loss 0.549235.
Train: 2018-07-31T09:09:35.142323: step 961, loss 0.588704.
Train: 2018-07-31T09:09:35.298562: step 962, loss 0.497976.
Train: 2018-07-31T09:09:35.454784: step 963, loss 0.606828.
Train: 2018-07-31T09:09:35.610964: step 964, loss 0.521841.
Train: 2018-07-31T09:09:35.767177: step 965, loss 0.612312.
Train: 2018-07-31T09:09:35.923421: step 966, loss 0.638158.
Train: 2018-07-31T09:09:36.095253: step 967, loss 0.554326.
Train: 2018-07-31T09:09:36.235819: step 968, loss 0.5948.
Train: 2018-07-31T09:09:36.392056: step 969, loss 0.504891.
Train: 2018-07-31T09:09:36.563890: step 970, loss 0.579415.
Test: 2018-07-31T09:09:36.798218: step 970, loss 0.54901.
Train: 2018-07-31T09:09:36.954424: step 971, loss 0.580744.
Train: 2018-07-31T09:09:37.110644: step 972, loss 0.514813.
Train: 2018-07-31T09:09:37.266826: step 973, loss 0.528517.
Train: 2018-07-31T09:09:37.423070: step 974, loss 0.585866.
Train: 2018-07-31T09:09:37.579254: step 975, loss 0.538417.
Train: 2018-07-31T09:09:37.735466: step 976, loss 0.563017.
Train: 2018-07-31T09:09:37.891710: step 977, loss 0.547752.
Train: 2018-07-31T09:09:38.047924: step 978, loss 0.569976.
Train: 2018-07-31T09:09:38.204137: step 979, loss 0.511362.
Train: 2018-07-31T09:09:38.360350: step 980, loss 0.589486.
Test: 2018-07-31T09:09:38.594673: step 980, loss 0.548441.
Train: 2018-07-31T09:09:38.750853: step 981, loss 0.572257.
Train: 2018-07-31T09:09:38.907097: step 982, loss 0.503099.
Train: 2018-07-31T09:09:39.063311: step 983, loss 0.547428.
Train: 2018-07-31T09:09:39.219525: step 984, loss 0.501515.
Train: 2018-07-31T09:09:39.375738: step 985, loss 0.643665.
Train: 2018-07-31T09:09:39.531952: step 986, loss 0.625241.
Train: 2018-07-31T09:09:39.688135: step 987, loss 0.589362.
Train: 2018-07-31T09:09:39.844378: step 988, loss 0.528135.
Train: 2018-07-31T09:09:40.000591: step 989, loss 0.503869.
Train: 2018-07-31T09:09:40.156799: step 990, loss 0.650686.
Test: 2018-07-31T09:09:40.391096: step 990, loss 0.548256.
Train: 2018-07-31T09:09:40.547308: step 991, loss 0.596703.
Train: 2018-07-31T09:09:40.703552: step 992, loss 0.564151.
Train: 2018-07-31T09:09:40.859766: step 993, loss 0.544398.
Train: 2018-07-31T09:09:41.015984: step 994, loss 0.572196.
Train: 2018-07-31T09:09:41.172161: step 995, loss 0.534355.
Train: 2018-07-31T09:09:41.328375: step 996, loss 0.544467.
Train: 2018-07-31T09:09:41.484589: step 997, loss 0.478852.
Train: 2018-07-31T09:09:41.640833: step 998, loss 0.52864.
Train: 2018-07-31T09:09:41.797046: step 999, loss 0.544638.
Train: 2018-07-31T09:09:41.953230: step 1000, loss 0.605906.
Test: 2018-07-31T09:09:42.171958: step 1000, loss 0.548276.
Train: 2018-07-31T09:09:42.890539: step 1001, loss 0.605824.
Train: 2018-07-31T09:09:43.046753: step 1002, loss 0.570855.
Train: 2018-07-31T09:09:43.202970: step 1003, loss 0.564151.
Train: 2018-07-31T09:09:43.374801: step 1004, loss 0.553294.
Train: 2018-07-31T09:09:43.531014: step 1005, loss 0.667865.
Train: 2018-07-31T09:09:43.687222: step 1006, loss 0.55502.
Train: 2018-07-31T09:09:43.827790: step 1007, loss 0.491174.
Train: 2018-07-31T09:09:43.984028: step 1008, loss 0.571503.
Train: 2018-07-31T09:09:44.155839: step 1009, loss 0.519781.
Train: 2018-07-31T09:09:44.312088: step 1010, loss 0.493981.
Test: 2018-07-31T09:09:44.546372: step 1010, loss 0.548361.
Train: 2018-07-31T09:09:44.702586: step 1011, loss 0.570788.
Train: 2018-07-31T09:09:44.874420: step 1012, loss 0.537195.
Train: 2018-07-31T09:09:45.030633: step 1013, loss 0.528501.
Train: 2018-07-31T09:09:45.186846: step 1014, loss 0.553392.
Train: 2018-07-31T09:09:45.343091: step 1015, loss 0.493185.
Train: 2018-07-31T09:09:45.499298: step 1016, loss 0.551946.
Train: 2018-07-31T09:09:45.655517: step 1017, loss 0.52872.
Train: 2018-07-31T09:09:45.811701: step 1018, loss 0.635437.
Train: 2018-07-31T09:09:45.967944: step 1019, loss 0.547563.
Train: 2018-07-31T09:09:46.124151: step 1020, loss 0.481604.
Test: 2018-07-31T09:09:46.358449: step 1020, loss 0.548013.
Train: 2018-07-31T09:09:46.514661: step 1021, loss 0.610568.
Train: 2018-07-31T09:09:46.670904: step 1022, loss 0.660187.
Train: 2018-07-31T09:09:46.842739: step 1023, loss 0.569691.
Train: 2018-07-31T09:09:46.983301: step 1024, loss 0.527857.
Train: 2018-07-31T09:09:47.139544: step 1025, loss 0.552414.
Train: 2018-07-31T09:09:47.295753: step 1026, loss 0.56439.
Train: 2018-07-31T09:09:47.451972: step 1027, loss 0.498805.
Train: 2018-07-31T09:09:47.623806: step 1028, loss 0.550219.
Train: 2018-07-31T09:09:47.780015: step 1029, loss 0.555288.
Train: 2018-07-31T09:09:47.936202: step 1030, loss 0.616133.
Test: 2018-07-31T09:09:48.170549: step 1030, loss 0.548027.
Train: 2018-07-31T09:09:48.342388: step 1031, loss 0.555317.
Train: 2018-07-31T09:09:48.482981: step 1032, loss 0.563824.
Train: 2018-07-31T09:09:48.639187: step 1033, loss 0.563524.
Train: 2018-07-31T09:09:48.795377: step 1034, loss 0.588452.
Train: 2018-07-31T09:09:48.935999: step 1035, loss 0.605797.
Train: 2018-07-31T09:09:49.107804: step 1036, loss 0.596445.
Train: 2018-07-31T09:09:49.248421: step 1037, loss 0.553025.
Train: 2018-07-31T09:09:49.404640: step 1038, loss 0.466767.
Train: 2018-07-31T09:09:49.560823: step 1039, loss 0.630011.
Train: 2018-07-31T09:09:49.717066: step 1040, loss 0.59727.
Test: 2018-07-31T09:09:49.951355: step 1040, loss 0.548299.
Train: 2018-07-31T09:09:50.107600: step 1041, loss 0.58932.
Train: 2018-07-31T09:09:50.263813: step 1042, loss 0.520102.
Train: 2018-07-31T09:09:50.419997: step 1043, loss 0.595803.
Train: 2018-07-31T09:09:50.576240: step 1044, loss 0.570755.
Train: 2018-07-31T09:09:50.732453: step 1045, loss 0.552705.
Train: 2018-07-31T09:09:50.888636: step 1046, loss 0.58186.
Train: 2018-07-31T09:09:51.044849: step 1047, loss 0.51132.
Train: 2018-07-31T09:09:51.201094: step 1048, loss 0.597107.
Train: 2018-07-31T09:09:51.341680: step 1049, loss 0.605392.
Train: 2018-07-31T09:09:51.497869: step 1050, loss 0.615741.
Test: 2018-07-31T09:09:51.732189: step 1050, loss 0.54888.
Train: 2018-07-31T09:09:51.904055: step 1051, loss 0.556213.
Train: 2018-07-31T09:09:52.060238: step 1052, loss 0.562393.
Train: 2018-07-31T09:09:52.216450: step 1053, loss 0.571672.
Train: 2018-07-31T09:09:52.372663: step 1054, loss 0.57255.
Train: 2018-07-31T09:09:52.528878: step 1055, loss 0.606447.
Train: 2018-07-31T09:09:52.685092: step 1056, loss 0.56361.
Train: 2018-07-31T09:09:52.841335: step 1057, loss 0.493644.
Train: 2018-07-31T09:09:52.997548: step 1058, loss 0.539957.
Train: 2018-07-31T09:09:53.153731: step 1059, loss 0.573425.
Train: 2018-07-31T09:09:53.325566: step 1060, loss 0.538478.
Test: 2018-07-31T09:09:53.559886: step 1060, loss 0.549188.
Train: 2018-07-31T09:09:53.716099: step 1061, loss 0.618444.
Train: 2018-07-31T09:09:53.872337: step 1062, loss 0.513065.
Train: 2018-07-31T09:09:54.012935: step 1063, loss 0.514258.
Train: 2018-07-31T09:09:54.169117: step 1064, loss 0.537634.
Train: 2018-07-31T09:09:54.340989: step 1065, loss 0.529653.
Train: 2018-07-31T09:09:54.497202: step 1066, loss 0.544861.
Train: 2018-07-31T09:09:54.637794: step 1067, loss 0.521033.
Train: 2018-07-31T09:09:54.793973: step 1068, loss 0.521558.
Train: 2018-07-31T09:09:54.950215: step 1069, loss 0.50196.
Train: 2018-07-31T09:09:55.106430: step 1070, loss 0.579278.
Test: 2018-07-31T09:09:55.340750: step 1070, loss 0.548202.
Train: 2018-07-31T09:09:55.496933: step 1071, loss 0.518978.
Train: 2018-07-31T09:09:55.653176: step 1072, loss 0.657928.
Train: 2018-07-31T09:09:55.809360: step 1073, loss 0.65442.
Train: 2018-07-31T09:09:55.965573: step 1074, loss 0.632602.
Train: 2018-07-31T09:09:56.121816: step 1075, loss 0.536733.
Train: 2018-07-31T09:09:56.278030: step 1076, loss 0.555477.
Train: 2018-07-31T09:09:56.434239: step 1077, loss 0.513324.
Train: 2018-07-31T09:09:56.590426: step 1078, loss 0.607111.
Train: 2018-07-31T09:09:56.746640: step 1079, loss 0.509694.
Train: 2018-07-31T09:09:56.902884: step 1080, loss 0.510058.
Test: 2018-07-31T09:09:57.137204: step 1080, loss 0.548133.
Train: 2018-07-31T09:09:57.293387: step 1081, loss 0.60534.
Train: 2018-07-31T09:09:57.449631: step 1082, loss 0.555588.
Train: 2018-07-31T09:09:57.621435: step 1083, loss 0.563497.
Train: 2018-07-31T09:09:57.777649: step 1084, loss 0.569905.
Train: 2018-07-31T09:09:57.918241: step 1085, loss 0.685525.
Train: 2018-07-31T09:09:58.074454: step 1086, loss 0.527989.
Train: 2018-07-31T09:09:58.230667: step 1087, loss 0.589413.
Train: 2018-07-31T09:09:58.386911: step 1088, loss 0.51182.
Train: 2018-07-31T09:09:58.543118: step 1089, loss 0.538925.
Train: 2018-07-31T09:09:58.699307: step 1090, loss 0.616226.
Test: 2018-07-31T09:09:58.933628: step 1090, loss 0.548367.
Train: 2018-07-31T09:09:59.089872: step 1091, loss 0.537258.
Train: 2018-07-31T09:09:59.261676: step 1092, loss 0.486058.
Train: 2018-07-31T09:09:59.402304: step 1093, loss 0.511299.
Train: 2018-07-31T09:09:59.558506: step 1094, loss 0.502854.
Train: 2018-07-31T09:09:59.714725: step 1095, loss 0.606484.
Train: 2018-07-31T09:09:59.886559: step 1096, loss 0.500828.
Train: 2018-07-31T09:10:00.042744: step 1097, loss 0.608194.
Train: 2018-07-31T09:10:00.198957: step 1098, loss 0.597886.
Train: 2018-07-31T09:10:00.355207: step 1099, loss 0.60675.
Train: 2018-07-31T09:10:00.511408: step 1100, loss 0.536904.
Test: 2018-07-31T09:10:00.745705: step 1100, loss 0.548205.
Train: 2018-07-31T09:10:01.495527: step 1101, loss 0.562953.
Train: 2018-07-31T09:10:01.651772: step 1102, loss 0.555097.
Train: 2018-07-31T09:10:01.807962: step 1103, loss 0.682558.
Train: 2018-07-31T09:10:01.964168: step 1104, loss 0.624681.
Train: 2018-07-31T09:10:02.120414: step 1105, loss 0.605789.
Train: 2018-07-31T09:10:02.276620: step 1106, loss 0.596812.
Train: 2018-07-31T09:10:02.432840: step 1107, loss 0.537912.
Train: 2018-07-31T09:10:02.573430: step 1108, loss 0.538273.
Train: 2018-07-31T09:10:02.729639: step 1109, loss 0.585863.
Train: 2018-07-31T09:10:02.885860: step 1110, loss 0.586211.
Test: 2018-07-31T09:10:03.120179: step 1110, loss 0.549054.
Train: 2018-07-31T09:10:03.291982: step 1111, loss 0.547255.
Train: 2018-07-31T09:10:03.448229: step 1112, loss 0.563026.
Train: 2018-07-31T09:10:03.604408: step 1113, loss 0.545579.
Train: 2018-07-31T09:10:03.745000: step 1114, loss 0.565073.
Train: 2018-07-31T09:10:03.901214: step 1115, loss 0.63829.
Train: 2018-07-31T09:10:04.057452: step 1116, loss 0.504196.
Train: 2018-07-31T09:10:04.229263: step 1117, loss 0.6212.
Train: 2018-07-31T09:10:04.385506: step 1118, loss 0.554904.
Train: 2018-07-31T09:10:04.526098: step 1119, loss 0.539252.
Train: 2018-07-31T09:10:04.697936: step 1120, loss 0.554326.
Test: 2018-07-31T09:10:04.932225: step 1120, loss 0.549279.
Train: 2018-07-31T09:10:05.088436: step 1121, loss 0.580055.
Train: 2018-07-31T09:10:05.244650: step 1122, loss 0.521715.
Train: 2018-07-31T09:10:05.400887: step 1123, loss 0.554671.
Train: 2018-07-31T09:10:05.557112: step 1124, loss 0.588623.
Train: 2018-07-31T09:10:05.713321: step 1125, loss 0.579258.
Train: 2018-07-31T09:10:05.869503: step 1126, loss 0.562958.
Train: 2018-07-31T09:10:06.025752: step 1127, loss 0.563919.
Train: 2018-07-31T09:10:06.166339: step 1128, loss 0.622335.
Train: 2018-07-31T09:10:06.322522: step 1129, loss 0.57173.
Train: 2018-07-31T09:10:06.478735: step 1130, loss 0.629072.
Test: 2018-07-31T09:10:06.713088: step 1130, loss 0.548959.
Train: 2018-07-31T09:10:06.884921: step 1131, loss 0.55458.
Train: 2018-07-31T09:10:07.041104: step 1132, loss 0.512737.
Train: 2018-07-31T09:10:07.181737: step 1133, loss 0.620868.
Train: 2018-07-31T09:10:07.337946: step 1134, loss 0.546603.
Train: 2018-07-31T09:10:07.494159: step 1135, loss 0.513101.
Train: 2018-07-31T09:10:07.650366: step 1136, loss 0.603792.
Train: 2018-07-31T09:10:07.806549: step 1137, loss 0.646407.
Train: 2018-07-31T09:10:07.962762: step 1138, loss 0.578166.
Train: 2018-07-31T09:10:08.119001: step 1139, loss 0.555729.
Train: 2018-07-31T09:10:08.275189: step 1140, loss 0.561329.
Test: 2018-07-31T09:10:08.509512: step 1140, loss 0.549121.
Train: 2018-07-31T09:10:08.665749: step 1141, loss 0.628285.
Train: 2018-07-31T09:10:08.821967: step 1142, loss 0.602692.
Train: 2018-07-31T09:10:08.978151: step 1143, loss 0.465307.
Train: 2018-07-31T09:10:09.150009: step 1144, loss 0.50803.
Train: 2018-07-31T09:10:09.306199: step 1145, loss 0.54602.
Train: 2018-07-31T09:10:09.462412: step 1146, loss 0.579424.
Train: 2018-07-31T09:10:09.618626: step 1147, loss 0.579268.
Train: 2018-07-31T09:10:09.759243: step 1148, loss 0.546113.
Train: 2018-07-31T09:10:09.915464: step 1149, loss 0.635611.
Train: 2018-07-31T09:10:10.087302: step 1150, loss 0.496231.
Test: 2018-07-31T09:10:10.321610: step 1150, loss 0.548889.
Train: 2018-07-31T09:10:10.477801: step 1151, loss 0.571542.
Train: 2018-07-31T09:10:10.634043: step 1152, loss 0.588171.
Train: 2018-07-31T09:10:10.805877: step 1153, loss 0.571044.
Train: 2018-07-31T09:10:10.962092: step 1154, loss 0.584791.
Train: 2018-07-31T09:10:11.118275: step 1155, loss 0.653317.
Train: 2018-07-31T09:10:11.258896: step 1156, loss 0.571601.
Train: 2018-07-31T09:10:11.430701: step 1157, loss 0.539387.
Train: 2018-07-31T09:10:11.586914: step 1158, loss 0.555013.
Train: 2018-07-31T09:10:11.743129: step 1159, loss 0.53013.
Train: 2018-07-31T09:10:11.899366: step 1160, loss 0.547807.
Test: 2018-07-31T09:10:12.133663: step 1160, loss 0.548811.
Train: 2018-07-31T09:10:12.289899: step 1161, loss 0.638653.
Train: 2018-07-31T09:10:12.446122: step 1162, loss 0.547606.
Train: 2018-07-31T09:10:12.602302: step 1163, loss 0.553327.
Train: 2018-07-31T09:10:12.758516: step 1164, loss 0.512537.
Train: 2018-07-31T09:10:12.899138: step 1165, loss 0.545909.
Train: 2018-07-31T09:10:13.055351: step 1166, loss 0.528736.
Train: 2018-07-31T09:10:13.227156: step 1167, loss 0.554358.
Train: 2018-07-31T09:10:13.383393: step 1168, loss 0.529063.
Train: 2018-07-31T09:10:13.539583: step 1169, loss 0.487284.
Train: 2018-07-31T09:10:13.695795: step 1170, loss 0.630429.
Test: 2018-07-31T09:10:13.930115: step 1170, loss 0.548318.
Train: 2018-07-31T09:10:14.086360: step 1171, loss 0.579387.
Train: 2018-07-31T09:10:14.242542: step 1172, loss 0.648053.
Train: 2018-07-31T09:10:14.414377: step 1173, loss 0.528946.
Train: 2018-07-31T09:10:14.554969: step 1174, loss 0.527584.
Train: 2018-07-31T09:10:14.726804: step 1175, loss 0.553678.
Train: 2018-07-31T09:10:14.883043: step 1176, loss 0.538253.
Train: 2018-07-31T09:10:15.039261: step 1177, loss 0.527873.
Train: 2018-07-31T09:10:15.195475: step 1178, loss 0.555399.
Train: 2018-07-31T09:10:15.351689: step 1179, loss 0.589702.
Train: 2018-07-31T09:10:15.507871: step 1180, loss 0.572984.
Test: 2018-07-31T09:10:15.757843: step 1180, loss 0.548063.
Train: 2018-07-31T09:10:15.914051: step 1181, loss 0.54331.
Train: 2018-07-31T09:10:16.070239: step 1182, loss 0.555006.
Train: 2018-07-31T09:10:16.226483: step 1183, loss 0.51799.
Train: 2018-07-31T09:10:16.382667: step 1184, loss 0.597359.
Train: 2018-07-31T09:10:16.538905: step 1185, loss 0.606096.
Train: 2018-07-31T09:10:16.695094: step 1186, loss 0.511189.
Train: 2018-07-31T09:10:16.835716: step 1187, loss 0.543828.
Train: 2018-07-31T09:10:17.007521: step 1188, loss 0.582198.
Train: 2018-07-31T09:10:17.163733: step 1189, loss 0.508688.
Train: 2018-07-31T09:10:17.319972: step 1190, loss 0.543903.
Test: 2018-07-31T09:10:17.554297: step 1190, loss 0.547957.
Train: 2018-07-31T09:10:17.710511: step 1191, loss 0.526863.
Train: 2018-07-31T09:10:17.866730: step 1192, loss 0.554161.
Train: 2018-07-31T09:10:18.022932: step 1193, loss 0.527739.
Train: 2018-07-31T09:10:18.179120: step 1194, loss 0.589436.
Train: 2018-07-31T09:10:18.335365: step 1195, loss 0.563878.
Train: 2018-07-31T09:10:18.491548: step 1196, loss 0.473849.
Train: 2018-07-31T09:10:18.647791: step 1197, loss 0.617069.
Train: 2018-07-31T09:10:18.804004: step 1198, loss 0.518505.
Train: 2018-07-31T09:10:18.960219: step 1199, loss 0.579096.
Train: 2018-07-31T09:10:19.116432: step 1200, loss 0.579981.
Test: 2018-07-31T09:10:19.350723: step 1200, loss 0.547842.
Train: 2018-07-31T09:10:20.100545: step 1201, loss 0.528791.
Train: 2018-07-31T09:10:20.272412: step 1202, loss 0.592105.
Train: 2018-07-31T09:10:20.428625: step 1203, loss 0.508321.
Train: 2018-07-31T09:10:20.600459: step 1204, loss 0.573209.
Train: 2018-07-31T09:10:20.741020: step 1205, loss 0.53568.
Train: 2018-07-31T09:10:20.912856: step 1206, loss 0.536627.
Train: 2018-07-31T09:10:21.069068: step 1207, loss 0.598507.
Train: 2018-07-31T09:10:21.209662: step 1208, loss 0.468025.
Train: 2018-07-31T09:10:21.365875: step 1209, loss 0.537202.
Train: 2018-07-31T09:10:21.522119: step 1210, loss 0.625445.
Test: 2018-07-31T09:10:21.756409: step 1210, loss 0.547823.
Train: 2018-07-31T09:10:22.006350: step 1211, loss 0.525982.
Train: 2018-07-31T09:10:22.162593: step 1212, loss 0.59824.
Train: 2018-07-31T09:10:22.334429: step 1213, loss 0.528263.
Train: 2018-07-31T09:10:22.490610: step 1214, loss 0.537994.
Train: 2018-07-31T09:10:22.646857: step 1215, loss 0.555744.
Train: 2018-07-31T09:10:22.803064: step 1216, loss 0.508982.
Train: 2018-07-31T09:10:22.959282: step 1217, loss 0.571275.
Train: 2018-07-31T09:10:23.115489: step 1218, loss 0.573049.
Train: 2018-07-31T09:10:23.271708: step 1219, loss 0.610174.
Train: 2018-07-31T09:10:23.427921: step 1220, loss 0.599336.
Test: 2018-07-31T09:10:23.662212: step 1220, loss 0.547846.
Train: 2018-07-31T09:10:23.818426: step 1221, loss 0.58187.
Train: 2018-07-31T09:10:23.974669: step 1222, loss 0.543407.
Train: 2018-07-31T09:10:24.130882: step 1223, loss 0.508868.
Train: 2018-07-31T09:10:24.287066: step 1224, loss 0.562941.
Train: 2018-07-31T09:10:24.443278: step 1225, loss 0.624105.
Train: 2018-07-31T09:10:24.599523: step 1226, loss 0.607297.
Train: 2018-07-31T09:10:24.771352: step 1227, loss 0.597633.
Train: 2018-07-31T09:10:24.927541: step 1228, loss 0.536612.
Train: 2018-07-31T09:10:25.068133: step 1229, loss 0.588034.
Train: 2018-07-31T09:10:25.224376: step 1230, loss 0.511287.
Test: 2018-07-31T09:10:25.458691: step 1230, loss 0.548174.
Train: 2018-07-31T09:10:25.614879: step 1231, loss 0.605978.
Train: 2018-07-31T09:10:25.771124: step 1232, loss 0.468259.
Train: 2018-07-31T09:10:25.927332: step 1233, loss 0.54548.
Train: 2018-07-31T09:10:26.083520: step 1234, loss 0.59628.
Train: 2018-07-31T09:10:26.255354: step 1235, loss 0.510431.
Train: 2018-07-31T09:10:26.395947: step 1236, loss 0.57257.
Train: 2018-07-31T09:10:26.552184: step 1237, loss 0.441836.
Train: 2018-07-31T09:10:26.708404: step 1238, loss 0.519832.
Train: 2018-07-31T09:10:26.864611: step 1239, loss 0.518837.
Train: 2018-07-31T09:10:27.020800: step 1240, loss 0.544078.
Test: 2018-07-31T09:10:27.270774: step 1240, loss 0.548004.
Train: 2018-07-31T09:10:27.426985: step 1241, loss 0.553087.
Train: 2018-07-31T09:10:27.583200: step 1242, loss 0.614869.
Train: 2018-07-31T09:10:27.723791: step 1243, loss 0.455546.
Train: 2018-07-31T09:10:27.880004: step 1244, loss 0.617047.
Train: 2018-07-31T09:10:28.036219: step 1245, loss 0.526496.
Train: 2018-07-31T09:10:28.192400: step 1246, loss 0.537188.
Train: 2018-07-31T09:10:28.348613: step 1247, loss 0.580342.
Train: 2018-07-31T09:10:28.504827: step 1248, loss 0.526397.
Train: 2018-07-31T09:10:28.661066: step 1249, loss 0.616307.
Train: 2018-07-31T09:10:28.817255: step 1250, loss 0.48007.
Test: 2018-07-31T09:10:29.051575: step 1250, loss 0.547797.
Train: 2018-07-31T09:10:29.207813: step 1251, loss 0.588804.
Train: 2018-07-31T09:10:29.364003: step 1252, loss 0.608392.
Train: 2018-07-31T09:10:29.520245: step 1253, loss 0.527187.
Train: 2018-07-31T09:10:29.676459: step 1254, loss 0.517984.
Train: 2018-07-31T09:10:29.832643: step 1255, loss 0.59199.
Train: 2018-07-31T09:10:29.973234: step 1256, loss 0.563222.
Train: 2018-07-31T09:10:30.145099: step 1257, loss 0.528338.
Train: 2018-07-31T09:10:30.301282: step 1258, loss 0.589257.
Train: 2018-07-31T09:10:30.457520: step 1259, loss 0.627939.
Train: 2018-07-31T09:10:30.613709: step 1260, loss 0.626359.
Test: 2018-07-31T09:10:30.848056: step 1260, loss 0.547816.
Train: 2018-07-31T09:10:31.004243: step 1261, loss 0.508146.
Train: 2018-07-31T09:10:31.160457: step 1262, loss 0.625092.
Train: 2018-07-31T09:10:31.332291: step 1263, loss 0.626438.
Train: 2018-07-31T09:10:31.488503: step 1264, loss 0.543979.
Train: 2018-07-31T09:10:31.644748: step 1265, loss 0.536082.
Train: 2018-07-31T09:10:31.800956: step 1266, loss 0.563274.
Train: 2018-07-31T09:10:31.957144: step 1267, loss 0.561647.
Train: 2018-07-31T09:10:32.113357: step 1268, loss 0.59881.
Train: 2018-07-31T09:10:32.269601: step 1269, loss 0.587964.
Train: 2018-07-31T09:10:32.425815: step 1270, loss 0.494936.
Test: 2018-07-31T09:10:32.660136: step 1270, loss 0.548247.
Train: 2018-07-31T09:10:32.816348: step 1271, loss 0.546566.
Train: 2018-07-31T09:10:32.972532: step 1272, loss 0.612071.
Train: 2018-07-31T09:10:33.128745: step 1273, loss 0.563633.
Train: 2018-07-31T09:10:33.300610: step 1274, loss 0.477051.
Train: 2018-07-31T09:10:33.456824: step 1275, loss 0.596735.
Train: 2018-07-31T09:10:33.613038: step 1276, loss 0.537122.
Train: 2018-07-31T09:10:33.769219: step 1277, loss 0.554221.
Train: 2018-07-31T09:10:33.925434: step 1278, loss 0.519261.
Train: 2018-07-31T09:10:34.081677: step 1279, loss 0.502874.
Train: 2018-07-31T09:10:34.237890: step 1280, loss 0.502291.
Test: 2018-07-31T09:10:34.472207: step 1280, loss 0.548221.
Train: 2018-07-31T09:10:34.628395: step 1281, loss 0.527327.
Train: 2018-07-31T09:10:34.784608: step 1282, loss 0.614126.
Train: 2018-07-31T09:10:34.940820: step 1283, loss 0.563258.
Train: 2018-07-31T09:10:35.097034: step 1284, loss 0.536475.
Train: 2018-07-31T09:10:35.253248: step 1285, loss 0.587959.
Train: 2018-07-31T09:10:35.409460: step 1286, loss 0.65019.
Train: 2018-07-31T09:10:35.550054: step 1287, loss 0.580501.
Train: 2018-07-31T09:10:35.706297: step 1288, loss 0.55397.
Train: 2018-07-31T09:10:35.862480: step 1289, loss 0.579467.
Train: 2018-07-31T09:10:36.018723: step 1290, loss 0.66659.
Test: 2018-07-31T09:10:36.253013: step 1290, loss 0.548114.
Train: 2018-07-31T09:10:36.424879: step 1291, loss 0.580631.
Train: 2018-07-31T09:10:36.581086: step 1292, loss 0.55173.
Train: 2018-07-31T09:10:36.737301: step 1293, loss 0.613451.
Train: 2018-07-31T09:10:36.893489: step 1294, loss 0.55389.
Train: 2018-07-31T09:10:37.049702: step 1295, loss 0.528192.
Train: 2018-07-31T09:10:37.205917: step 1296, loss 0.596252.
Train: 2018-07-31T09:10:37.362129: step 1297, loss 0.572049.
Train: 2018-07-31T09:10:37.518372: step 1298, loss 0.571625.
Train: 2018-07-31T09:10:37.674556: step 1299, loss 0.605929.
Train: 2018-07-31T09:10:37.830769: step 1300, loss 0.562107.
Test: 2018-07-31T09:10:38.065090: step 1300, loss 0.548727.
Train: 2018-07-31T09:10:38.814913: step 1301, loss 0.570909.
Train: 2018-07-31T09:10:38.971158: step 1302, loss 0.52969.
Train: 2018-07-31T09:10:39.127371: step 1303, loss 0.488152.
Train: 2018-07-31T09:10:39.283589: step 1304, loss 0.595959.
Train: 2018-07-31T09:10:39.439766: step 1305, loss 0.605351.
Train: 2018-07-31T09:10:39.596011: step 1306, loss 0.588535.
Train: 2018-07-31T09:10:39.752225: step 1307, loss 0.554066.
Train: 2018-07-31T09:10:39.908438: step 1308, loss 0.587545.
Train: 2018-07-31T09:10:40.064621: step 1309, loss 0.571583.
Train: 2018-07-31T09:10:40.205213: step 1310, loss 0.613587.
Test: 2018-07-31T09:10:40.455155: step 1310, loss 0.548993.
Train: 2018-07-31T09:10:40.595747: step 1311, loss 0.546174.
Train: 2018-07-31T09:10:40.751985: step 1312, loss 0.505839.
Train: 2018-07-31T09:10:40.908203: step 1313, loss 0.529941.
Train: 2018-07-31T09:10:41.064386: step 1314, loss 0.522135.
Train: 2018-07-31T09:10:41.220630: step 1315, loss 0.580314.
Train: 2018-07-31T09:10:41.376814: step 1316, loss 0.471646.
Train: 2018-07-31T09:10:41.533026: step 1317, loss 0.546262.
Train: 2018-07-31T09:10:41.689274: step 1318, loss 0.587002.
Train: 2018-07-31T09:10:41.845485: step 1319, loss 0.570735.
Train: 2018-07-31T09:10:42.001698: step 1320, loss 0.588856.
Test: 2018-07-31T09:10:42.236018: step 1320, loss 0.548438.
Train: 2018-07-31T09:10:42.392225: step 1321, loss 0.528676.
Train: 2018-07-31T09:10:42.548450: step 1322, loss 0.50353.
Train: 2018-07-31T09:10:42.704653: step 1323, loss 0.571326.
Train: 2018-07-31T09:10:42.860873: step 1324, loss 0.60591.
Train: 2018-07-31T09:10:43.017055: step 1325, loss 0.510652.
Train: 2018-07-31T09:10:43.173267: step 1326, loss 0.640687.
Train: 2018-07-31T09:10:43.329512: step 1327, loss 0.667024.
Train: 2018-07-31T09:10:43.485725: step 1328, loss 0.570865.
Train: 2018-07-31T09:10:43.641933: step 1329, loss 0.561531.
Train: 2018-07-31T09:10:43.798122: step 1330, loss 0.546578.
Test: 2018-07-31T09:10:44.032443: step 1330, loss 0.548238.
Train: 2018-07-31T09:10:44.204307: step 1331, loss 0.562152.
Train: 2018-07-31T09:10:44.344869: step 1332, loss 0.476271.
Train: 2018-07-31T09:10:44.501112: step 1333, loss 0.579878.
Train: 2018-07-31T09:10:44.657296: step 1334, loss 0.605096.
Train: 2018-07-31T09:10:44.813539: step 1335, loss 0.621836.
Train: 2018-07-31T09:10:44.985368: step 1336, loss 0.536597.
Train: 2018-07-31T09:10:45.125966: step 1337, loss 0.561255.
Train: 2018-07-31T09:10:45.282148: step 1338, loss 0.570523.
Train: 2018-07-31T09:10:45.438396: step 1339, loss 0.528027.
Train: 2018-07-31T09:10:45.594575: step 1340, loss 0.588133.
Test: 2018-07-31T09:10:45.828916: step 1340, loss 0.548271.
Train: 2018-07-31T09:10:45.985109: step 1341, loss 0.579802.
Train: 2018-07-31T09:10:46.141347: step 1342, loss 0.537665.
Train: 2018-07-31T09:10:46.297566: step 1343, loss 0.537048.
Train: 2018-07-31T09:10:46.453781: step 1344, loss 0.623198.
Train: 2018-07-31T09:10:46.609993: step 1345, loss 0.605682.
Train: 2018-07-31T09:10:46.766177: step 1346, loss 0.588692.
Train: 2018-07-31T09:10:46.922390: step 1347, loss 0.570821.
Train: 2018-07-31T09:10:47.078634: step 1348, loss 0.545279.
Train: 2018-07-31T09:10:47.234817: step 1349, loss 0.545905.
Train: 2018-07-31T09:10:47.391066: step 1350, loss 0.604373.
Test: 2018-07-31T09:10:47.625382: step 1350, loss 0.548488.
Train: 2018-07-31T09:10:47.797184: step 1351, loss 0.470235.
Train: 2018-07-31T09:10:47.953429: step 1352, loss 0.511772.
Train: 2018-07-31T09:10:48.109611: step 1353, loss 0.511273.
Train: 2018-07-31T09:10:48.281447: step 1354, loss 0.528549.
Train: 2018-07-31T09:10:48.422038: step 1355, loss 0.545959.
Train: 2018-07-31T09:10:48.593904: step 1356, loss 0.613813.
Train: 2018-07-31T09:10:48.750117: step 1357, loss 0.571708.
Train: 2018-07-31T09:10:48.906330: step 1358, loss 0.589185.
Train: 2018-07-31T09:10:49.046922: step 1359, loss 0.580767.
Train: 2018-07-31T09:10:49.203106: step 1360, loss 0.536947.
Test: 2018-07-31T09:10:49.453078: step 1360, loss 0.548113.
Train: 2018-07-31T09:10:49.609260: step 1361, loss 0.5539.
Train: 2018-07-31T09:10:49.765473: step 1362, loss 0.501383.
Train: 2018-07-31T09:10:49.921687: step 1363, loss 0.63182.
Train: 2018-07-31T09:10:50.077931: step 1364, loss 0.536881.
Train: 2018-07-31T09:10:50.234144: step 1365, loss 0.527583.
Train: 2018-07-31T09:10:50.390328: step 1366, loss 0.589118.
Train: 2018-07-31T09:10:50.546571: step 1367, loss 0.500502.
Train: 2018-07-31T09:10:50.702754: step 1368, loss 0.632608.
Train: 2018-07-31T09:10:50.874619: step 1369, loss 0.544914.
Train: 2018-07-31T09:10:51.030802: step 1370, loss 0.483235.
Test: 2018-07-31T09:10:51.265153: step 1370, loss 0.547926.
Train: 2018-07-31T09:10:51.421336: step 1371, loss 0.518243.
Train: 2018-07-31T09:10:51.577551: step 1372, loss 0.578606.
Train: 2018-07-31T09:10:51.733787: step 1373, loss 0.554511.
Train: 2018-07-31T09:10:51.889976: step 1374, loss 0.545576.
Train: 2018-07-31T09:10:52.046189: step 1375, loss 0.501271.
Train: 2018-07-31T09:10:52.202434: step 1376, loss 0.580541.
Train: 2018-07-31T09:10:52.358647: step 1377, loss 0.545213.
Train: 2018-07-31T09:10:52.514860: step 1378, loss 0.509194.
Train: 2018-07-31T09:10:52.671074: step 1379, loss 0.525893.
Train: 2018-07-31T09:10:52.827288: step 1380, loss 0.581853.
Test: 2018-07-31T09:10:53.061608: step 1380, loss 0.547724.
Train: 2018-07-31T09:10:53.217822: step 1381, loss 0.580544.
Train: 2018-07-31T09:10:53.374004: step 1382, loss 0.518676.
Train: 2018-07-31T09:10:53.514596: step 1383, loss 0.581493.
Train: 2018-07-31T09:10:53.686466: step 1384, loss 0.490521.
Train: 2018-07-31T09:10:53.842645: step 1385, loss 0.644603.
Train: 2018-07-31T09:10:53.998857: step 1386, loss 0.481351.
Train: 2018-07-31T09:10:54.155101: step 1387, loss 0.499296.
Train: 2018-07-31T09:10:54.311318: step 1388, loss 0.593374.
Train: 2018-07-31T09:10:54.467528: step 1389, loss 0.471398.
Train: 2018-07-31T09:10:54.623712: step 1390, loss 0.627456.
Test: 2018-07-31T09:10:54.858064: step 1390, loss 0.547731.
Train: 2018-07-31T09:10:55.014275: step 1391, loss 0.590207.
Train: 2018-07-31T09:10:55.154838: step 1392, loss 0.572739.
Train: 2018-07-31T09:10:55.311075: step 1393, loss 0.553518.
Train: 2018-07-31T09:10:55.467264: step 1394, loss 0.525464.
Train: 2018-07-31T09:10:55.623477: step 1395, loss 0.592326.
Train: 2018-07-31T09:10:55.779720: step 1396, loss 0.561358.
Train: 2018-07-31T09:10:55.935934: step 1397, loss 0.626988.
Train: 2018-07-31T09:10:56.092117: step 1398, loss 0.471681.
Train: 2018-07-31T09:10:56.248356: step 1399, loss 0.681282.
Train: 2018-07-31T09:10:56.404544: step 1400, loss 0.579453.
Test: 2018-07-31T09:10:56.638898: step 1400, loss 0.547738.
Train: 2018-07-31T09:10:57.419965: step 1401, loss 0.56184.
Train: 2018-07-31T09:10:57.560554: step 1402, loss 0.571423.
Train: 2018-07-31T09:10:57.716767: step 1403, loss 0.580271.
Train: 2018-07-31T09:10:57.872981: step 1404, loss 0.536478.
Train: 2018-07-31T09:10:58.029165: step 1405, loss 0.58934.
Train: 2018-07-31T09:10:58.200998: step 1406, loss 0.659758.
Train: 2018-07-31T09:10:58.357242: step 1407, loss 0.553303.
Train: 2018-07-31T09:10:58.497803: step 1408, loss 0.623101.
Train: 2018-07-31T09:10:58.654048: step 1409, loss 0.511359.
Train: 2018-07-31T09:10:58.810255: step 1410, loss 0.562931.
Test: 2018-07-31T09:10:59.060178: step 1410, loss 0.548334.
Train: 2018-07-31T09:10:59.216385: step 1411, loss 0.537128.
Train: 2018-07-31T09:10:59.372629: step 1412, loss 0.630524.
Train: 2018-07-31T09:10:59.528843: step 1413, loss 0.562918.
Train: 2018-07-31T09:10:59.685057: step 1414, loss 0.529422.
Train: 2018-07-31T09:10:59.841270: step 1415, loss 0.5878.
Train: 2018-07-31T09:10:59.981862: step 1416, loss 0.562157.
Train: 2018-07-31T09:11:00.138045: step 1417, loss 0.563676.
Train: 2018-07-31T09:11:00.309910: step 1418, loss 0.546084.
Train: 2018-07-31T09:11:00.466124: step 1419, loss 0.553889.
Train: 2018-07-31T09:11:00.622337: step 1420, loss 0.620196.
Test: 2018-07-31T09:11:00.872278: step 1420, loss 0.548917.
Train: 2018-07-31T09:11:01.028463: step 1421, loss 0.537377.
Train: 2018-07-31T09:11:01.184676: step 1422, loss 0.521332.
Train: 2018-07-31T09:11:01.340913: step 1423, loss 0.562483.
Train: 2018-07-31T09:11:01.497126: step 1424, loss 0.603715.
Train: 2018-07-31T09:11:01.637724: step 1425, loss 0.578508.
Train: 2018-07-31T09:11:01.793937: step 1426, loss 0.546006.
Train: 2018-07-31T09:11:01.950151: step 1427, loss 0.554705.
Train: 2018-07-31T09:11:02.106333: step 1428, loss 0.661918.
Train: 2018-07-31T09:11:02.262547: step 1429, loss 0.488606.
Train: 2018-07-31T09:11:02.403140: step 1430, loss 0.579481.
Test: 2018-07-31T09:11:02.637460: step 1430, loss 0.54885.
Train: 2018-07-31T09:11:02.809294: step 1431, loss 0.587947.
Train: 2018-07-31T09:11:02.949886: step 1432, loss 0.529788.
Train: 2018-07-31T09:11:03.106101: step 1433, loss 0.620229.
Train: 2018-07-31T09:11:03.262343: step 1434, loss 0.530003.
Train: 2018-07-31T09:11:03.418526: step 1435, loss 0.570422.
Train: 2018-07-31T09:11:03.574741: step 1436, loss 0.570141.
Train: 2018-07-31T09:11:03.730954: step 1437, loss 0.562749.
Train: 2018-07-31T09:11:03.887168: step 1438, loss 0.628771.
Train: 2018-07-31T09:11:04.043381: step 1439, loss 0.620462.
Train: 2018-07-31T09:11:04.199594: step 1440, loss 0.57938.
Test: 2018-07-31T09:11:04.433945: step 1440, loss 0.548927.
Train: 2018-07-31T09:11:04.590128: step 1441, loss 0.553619.
Train: 2018-07-31T09:11:04.746341: step 1442, loss 0.521277.
Train: 2018-07-31T09:11:04.902579: step 1443, loss 0.621252.
Train: 2018-07-31T09:11:05.058793: step 1444, loss 0.53002.
Train: 2018-07-31T09:11:05.215013: step 1445, loss 0.60445.
Train: 2018-07-31T09:11:05.371227: step 1446, loss 0.546838.
Train: 2018-07-31T09:11:05.527438: step 1447, loss 0.571476.
Train: 2018-07-31T09:11:05.683622: step 1448, loss 0.570048.
Train: 2018-07-31T09:11:05.824214: step 1449, loss 0.545593.
Train: 2018-07-31T09:11:05.980427: step 1450, loss 0.537687.
Test: 2018-07-31T09:11:06.230368: step 1450, loss 0.548964.
Train: 2018-07-31T09:11:06.370985: step 1451, loss 0.620051.
Train: 2018-07-31T09:11:06.527204: step 1452, loss 0.586227.
Train: 2018-07-31T09:11:06.683387: step 1453, loss 0.588154.
Train: 2018-07-31T09:11:06.839601: step 1454, loss 0.562568.
Train: 2018-07-31T09:11:06.995844: step 1455, loss 0.570477.
Train: 2018-07-31T09:11:07.167648: step 1456, loss 0.544991.
Train: 2018-07-31T09:11:07.323862: step 1457, loss 0.54553.
Train: 2018-07-31T09:11:07.480111: step 1458, loss 0.520631.
Train: 2018-07-31T09:11:07.620667: step 1459, loss 0.538485.
Train: 2018-07-31T09:11:07.792527: step 1460, loss 0.537329.
Test: 2018-07-31T09:11:08.026853: step 1460, loss 0.548643.
Train: 2018-07-31T09:11:08.183061: step 1461, loss 0.495928.
Train: 2018-07-31T09:11:08.339274: step 1462, loss 0.536326.
Train: 2018-07-31T09:11:08.479843: step 1463, loss 0.538658.
Train: 2018-07-31T09:11:08.636056: step 1464, loss 0.528431.
Train: 2018-07-31T09:11:08.792269: step 1465, loss 0.460568.
Train: 2018-07-31T09:11:08.948512: step 1466, loss 0.519741.
Train: 2018-07-31T09:11:09.104725: step 1467, loss 0.562167.
Train: 2018-07-31T09:11:09.260908: step 1468, loss 0.551902.
Train: 2018-07-31T09:11:09.417152: step 1469, loss 0.526576.
Train: 2018-07-31T09:11:09.573336: step 1470, loss 0.543988.
Test: 2018-07-31T09:11:09.807689: step 1470, loss 0.547701.
Train: 2018-07-31T09:11:09.963893: step 1471, loss 0.553109.
Train: 2018-07-31T09:11:10.120083: step 1472, loss 0.564002.
Train: 2018-07-31T09:11:10.276328: step 1473, loss 0.571646.
Train: 2018-07-31T09:11:10.432510: step 1474, loss 0.561118.
Train: 2018-07-31T09:11:10.573132: step 1475, loss 0.581431.
Train: 2018-07-31T09:11:10.729316: step 1476, loss 0.517878.
Train: 2018-07-31T09:11:10.885527: step 1477, loss 0.515415.
Train: 2018-07-31T09:11:11.041771: step 1478, loss 0.515147.
Train: 2018-07-31T09:11:11.197955: step 1479, loss 0.533317.
Train: 2018-07-31T09:11:11.354198: step 1480, loss 0.583303.
Test: 2018-07-31T09:11:11.588488: step 1480, loss 0.54783.
Train: 2018-07-31T09:11:11.760353: step 1481, loss 0.577316.
Train: 2018-07-31T09:11:11.916537: step 1482, loss 0.582244.
Train: 2018-07-31T09:11:12.072781: step 1483, loss 0.611766.
Train: 2018-07-31T09:11:12.228963: step 1484, loss 0.563001.
Train: 2018-07-31T09:11:12.369555: step 1485, loss 0.536935.
Train: 2018-07-31T09:11:12.525799: step 1486, loss 0.545206.
Train: 2018-07-31T09:11:12.681983: step 1487, loss 0.602687.
Train: 2018-07-31T09:11:12.838196: step 1488, loss 0.572447.
Train: 2018-07-31T09:11:12.994439: step 1489, loss 0.581705.
Train: 2018-07-31T09:11:13.150653: step 1490, loss 0.562344.
Test: 2018-07-31T09:11:13.384974: step 1490, loss 0.547693.
Train: 2018-07-31T09:11:13.541187: step 1491, loss 0.506916.
Train: 2018-07-31T09:11:13.697400: step 1492, loss 0.578929.
Train: 2018-07-31T09:11:13.853614: step 1493, loss 0.589653.
Train: 2018-07-31T09:11:14.009822: step 1494, loss 0.527426.
Train: 2018-07-31T09:11:14.166046: step 1495, loss 0.543769.
Train: 2018-07-31T09:11:14.337875: step 1496, loss 0.570694.
Train: 2018-07-31T09:11:14.494059: step 1497, loss 0.634008.
Train: 2018-07-31T09:11:14.650302: step 1498, loss 0.527101.
Train: 2018-07-31T09:11:14.806516: step 1499, loss 0.553851.
Train: 2018-07-31T09:11:14.962698: step 1500, loss 0.605115.
Test: 2018-07-31T09:11:15.197019: step 1500, loss 0.54792.
Train: 2018-07-31T09:11:15.931221: step 1501, loss 0.536136.
Train: 2018-07-31T09:11:16.071813: step 1502, loss 0.589582.
Train: 2018-07-31T09:11:16.228057: step 1503, loss 0.666295.
Train: 2018-07-31T09:11:16.384270: step 1504, loss 0.622802.
Train: 2018-07-31T09:11:16.540484: step 1505, loss 0.546315.
Train: 2018-07-31T09:11:16.696698: step 1506, loss 0.528551.
Train: 2018-07-31T09:11:16.852915: step 1507, loss 0.554957.
Train: 2018-07-31T09:11:17.009124: step 1508, loss 0.588289.
Train: 2018-07-31T09:11:17.165338: step 1509, loss 0.629267.
Train: 2018-07-31T09:11:17.321520: step 1510, loss 0.596734.
Test: 2018-07-31T09:11:17.555872: step 1510, loss 0.548756.
Train: 2018-07-31T09:11:17.712079: step 1511, loss 0.588077.
Train: 2018-07-31T09:11:17.868268: step 1512, loss 0.52929.
Train: 2018-07-31T09:11:18.024482: step 1513, loss 0.56265.
Train: 2018-07-31T09:11:18.180695: step 1514, loss 0.513814.
Train: 2018-07-31T09:11:18.336908: step 1515, loss 0.546376.
Train: 2018-07-31T09:11:18.493121: step 1516, loss 0.57122.
Train: 2018-07-31T09:11:18.649335: step 1517, loss 0.578317.
Train: 2018-07-31T09:11:18.805548: step 1518, loss 0.644466.
Train: 2018-07-31T09:11:18.961761: step 1519, loss 0.537997.
Train: 2018-07-31T09:11:19.117975: step 1520, loss 0.60303.
Test: 2018-07-31T09:11:19.352326: step 1520, loss 0.54924.
Train: 2018-07-31T09:11:19.508540: step 1521, loss 0.596113.
Train: 2018-07-31T09:11:19.649101: step 1522, loss 0.540715.
Train: 2018-07-31T09:11:19.805344: step 1523, loss 0.513431.
Train: 2018-07-31T09:11:19.961552: step 1524, loss 0.514232.
Train: 2018-07-31T09:11:20.117771: step 1525, loss 0.513723.
Train: 2018-07-31T09:11:20.273979: step 1526, loss 0.561604.
Train: 2018-07-31T09:11:20.430195: step 1527, loss 0.611317.
Train: 2018-07-31T09:11:20.586412: step 1528, loss 0.522735.
Train: 2018-07-31T09:11:20.742594: step 1529, loss 0.579476.
Train: 2018-07-31T09:11:20.898812: step 1530, loss 0.529044.
Test: 2018-07-31T09:11:21.133131: step 1530, loss 0.548718.
Train: 2018-07-31T09:11:21.289343: step 1531, loss 0.57107.
Train: 2018-07-31T09:11:21.461208: step 1532, loss 0.589318.
Train: 2018-07-31T09:11:21.617391: step 1533, loss 0.612779.
Train: 2018-07-31T09:11:21.773628: step 1534, loss 0.538515.
Train: 2018-07-31T09:11:21.929844: step 1535, loss 0.59608.
Train: 2018-07-31T09:11:22.086030: step 1536, loss 0.537366.
Train: 2018-07-31T09:11:22.226654: step 1537, loss 0.588603.
Train: 2018-07-31T09:11:22.398458: step 1538, loss 0.588307.
Train: 2018-07-31T09:11:22.554704: step 1539, loss 0.54392.
Train: 2018-07-31T09:11:22.710883: step 1540, loss 0.604112.
Test: 2018-07-31T09:11:22.945231: step 1540, loss 0.548507.
Train: 2018-07-31T09:11:23.101417: step 1541, loss 0.570889.
Train: 2018-07-31T09:11:23.242010: step 1542, loss 0.536383.
Train: 2018-07-31T09:11:23.398224: step 1543, loss 0.546621.
Train: 2018-07-31T09:11:23.554437: step 1544, loss 0.52908.
Train: 2018-07-31T09:11:23.710680: step 1545, loss 0.556275.
Train: 2018-07-31T09:11:23.866893: step 1546, loss 0.587657.
Train: 2018-07-31T09:11:24.023077: step 1547, loss 0.562413.
Train: 2018-07-31T09:11:24.179291: step 1548, loss 0.572814.
Train: 2018-07-31T09:11:24.335534: step 1549, loss 0.629793.
Train: 2018-07-31T09:11:24.491718: step 1550, loss 0.503046.
Test: 2018-07-31T09:11:24.726062: step 1550, loss 0.548371.
Train: 2018-07-31T09:11:24.882250: step 1551, loss 0.554073.
Train: 2018-07-31T09:11:25.038464: step 1552, loss 0.528587.
Train: 2018-07-31T09:11:25.194678: step 1553, loss 0.503035.
Train: 2018-07-31T09:11:25.335300: step 1554, loss 0.519696.
Train: 2018-07-31T09:11:25.491513: step 1555, loss 0.562526.
Train: 2018-07-31T09:11:25.663317: step 1556, loss 0.491893.
Train: 2018-07-31T09:11:25.819559: step 1557, loss 0.553962.
Train: 2018-07-31T09:11:25.960154: step 1558, loss 0.536544.
Train: 2018-07-31T09:11:26.116361: step 1559, loss 0.58058.
Train: 2018-07-31T09:11:26.272581: step 1560, loss 0.598301.
Test: 2018-07-31T09:11:26.506870: step 1560, loss 0.547835.
Train: 2018-07-31T09:11:26.663114: step 1561, loss 0.581467.
Train: 2018-07-31T09:11:26.819297: step 1562, loss 0.580225.
Train: 2018-07-31T09:11:26.959919: step 1563, loss 0.537283.
Train: 2018-07-31T09:11:27.116102: step 1564, loss 0.5354.
Train: 2018-07-31T09:11:27.287962: step 1565, loss 0.580701.
Train: 2018-07-31T09:11:27.444151: step 1566, loss 0.607971.
Train: 2018-07-31T09:11:27.600365: step 1567, loss 0.590217.
Train: 2018-07-31T09:11:27.756607: step 1568, loss 0.494018.
Train: 2018-07-31T09:11:27.897169: step 1569, loss 0.518438.
Train: 2018-07-31T09:11:28.053407: step 1570, loss 0.516607.
Test: 2018-07-31T09:11:28.303362: step 1570, loss 0.547793.
Train: 2018-07-31T09:11:28.459538: step 1571, loss 0.545086.
Train: 2018-07-31T09:11:28.615752: step 1572, loss 0.62535.
Train: 2018-07-31T09:11:28.756344: step 1573, loss 0.508728.
Train: 2018-07-31T09:11:28.912557: step 1574, loss 0.632972.
Train: 2018-07-31T09:11:29.068801: step 1575, loss 0.56319.
Train: 2018-07-31T09:11:29.224984: step 1576, loss 0.5799.
Train: 2018-07-31T09:11:29.381227: step 1577, loss 0.605976.
Train: 2018-07-31T09:11:29.537441: step 1578, loss 0.455764.
Train: 2018-07-31T09:11:29.678027: step 1579, loss 0.580429.
Train: 2018-07-31T09:11:29.834250: step 1580, loss 0.571275.
Test: 2018-07-31T09:11:30.084189: step 1580, loss 0.547841.
Train: 2018-07-31T09:11:30.240371: step 1581, loss 0.545077.
Train: 2018-07-31T09:11:30.380993: step 1582, loss 0.509644.
Train: 2018-07-31T09:11:30.537207: step 1583, loss 0.571212.
Train: 2018-07-31T09:11:30.693389: step 1584, loss 0.561921.
Train: 2018-07-31T09:11:30.849633: step 1585, loss 0.562547.
Train: 2018-07-31T09:11:31.005846: step 1586, loss 0.642925.
Train: 2018-07-31T09:11:31.162060: step 1587, loss 0.545611.
Train: 2018-07-31T09:11:31.318268: step 1588, loss 0.554639.
Train: 2018-07-31T09:11:31.474457: step 1589, loss 0.545073.
Train: 2018-07-31T09:11:31.630700: step 1590, loss 0.528264.
Test: 2018-07-31T09:11:31.864991: step 1590, loss 0.547901.
Train: 2018-07-31T09:11:32.021203: step 1591, loss 0.588434.
Train: 2018-07-31T09:11:32.177417: step 1592, loss 0.589001.
Train: 2018-07-31T09:11:32.333661: step 1593, loss 0.519059.
Train: 2018-07-31T09:11:32.489874: step 1594, loss 0.545131.
Train: 2018-07-31T09:11:32.646058: step 1595, loss 0.552707.
Train: 2018-07-31T09:11:32.802301: step 1596, loss 0.640243.
Train: 2018-07-31T09:11:32.974106: step 1597, loss 0.561946.
Train: 2018-07-31T09:11:33.114733: step 1598, loss 0.622234.
Train: 2018-07-31T09:11:33.270911: step 1599, loss 0.639762.
Train: 2018-07-31T09:11:33.427124: step 1600, loss 0.485134.
Test: 2018-07-31T09:11:33.661445: step 1600, loss 0.548123.
Train: 2018-07-31T09:11:34.364435: step 1601, loss 0.537772.
Train: 2018-07-31T09:11:34.520648: step 1602, loss 0.502612.
Train: 2018-07-31T09:11:34.676862: step 1603, loss 0.639443.
Train: 2018-07-31T09:11:34.833079: step 1604, loss 0.537849.
Train: 2018-07-31T09:11:34.989288: step 1605, loss 0.578801.
Train: 2018-07-31T09:11:35.145502: step 1606, loss 0.536406.
Train: 2018-07-31T09:11:35.301716: step 1607, loss 0.529733.
Train: 2018-07-31T09:11:35.457900: step 1608, loss 0.468774.
Train: 2018-07-31T09:11:35.614113: step 1609, loss 0.485635.
Train: 2018-07-31T09:11:35.770356: step 1610, loss 0.56332.
Test: 2018-07-31T09:11:36.004677: step 1610, loss 0.548039.
Train: 2018-07-31T09:11:36.160889: step 1611, loss 0.546098.
Train: 2018-07-31T09:11:36.317103: step 1612, loss 0.607167.
Train: 2018-07-31T09:11:36.473316: step 1613, loss 0.519662.
Train: 2018-07-31T09:11:36.629500: step 1614, loss 0.569557.
Train: 2018-07-31T09:11:36.785744: step 1615, loss 0.578989.
Train: 2018-07-31T09:11:36.941952: step 1616, loss 0.492297.
Train: 2018-07-31T09:11:37.082549: step 1617, loss 0.536458.
Train: 2018-07-31T09:11:37.238733: step 1618, loss 0.617221.
Train: 2018-07-31T09:11:37.394945: step 1619, loss 0.606157.
Train: 2018-07-31T09:11:37.551189: step 1620, loss 0.527364.
Test: 2018-07-31T09:11:37.785479: step 1620, loss 0.547795.
Train: 2018-07-31T09:11:37.941692: step 1621, loss 0.500462.
Train: 2018-07-31T09:11:38.113557: step 1622, loss 0.572414.
Train: 2018-07-31T09:11:38.254119: step 1623, loss 0.605805.
Train: 2018-07-31T09:11:38.425955: step 1624, loss 0.518272.
Train: 2018-07-31T09:11:38.566546: step 1625, loss 0.598477.
Train: 2018-07-31T09:11:38.722790: step 1626, loss 0.464887.
Train: 2018-07-31T09:11:38.894624: step 1627, loss 0.528543.
Train: 2018-07-31T09:11:39.050838: step 1628, loss 0.597909.
Train: 2018-07-31T09:11:39.207020: step 1629, loss 0.545305.
Train: 2018-07-31T09:11:39.363265: step 1630, loss 0.519538.
Test: 2018-07-31T09:11:39.597586: step 1630, loss 0.547708.
Train: 2018-07-31T09:11:39.769414: step 1631, loss 0.606267.
Train: 2018-07-31T09:11:39.925603: step 1632, loss 0.527915.
Train: 2018-07-31T09:11:40.081841: step 1633, loss 0.573035.
Train: 2018-07-31T09:11:40.222445: step 1634, loss 0.553347.
Train: 2018-07-31T09:11:40.378648: step 1635, loss 0.57189.
Train: 2018-07-31T09:11:40.534835: step 1636, loss 0.537064.
Train: 2018-07-31T09:11:40.675428: step 1637, loss 0.624818.
Train: 2018-07-31T09:11:40.847292: step 1638, loss 0.526152.
Train: 2018-07-31T09:11:40.987855: step 1639, loss 0.643438.
Train: 2018-07-31T09:11:41.144068: step 1640, loss 0.572035.
Test: 2018-07-31T09:11:41.378418: step 1640, loss 0.547756.
Train: 2018-07-31T09:11:41.534636: step 1641, loss 0.589654.
Train: 2018-07-31T09:11:41.706437: step 1642, loss 0.526697.
Train: 2018-07-31T09:11:41.847063: step 1643, loss 0.596959.
Train: 2018-07-31T09:11:42.003271: step 1644, loss 0.544443.
Train: 2018-07-31T09:11:42.159455: step 1645, loss 0.562946.
Train: 2018-07-31T09:11:42.300077: step 1646, loss 0.596616.
Train: 2018-07-31T09:11:42.456261: step 1647, loss 0.587744.
Train: 2018-07-31T09:11:42.612504: step 1648, loss 0.562778.
Train: 2018-07-31T09:11:42.784347: step 1649, loss 0.631268.
Train: 2018-07-31T09:11:42.940553: step 1650, loss 0.580191.
Test: 2018-07-31T09:11:43.174873: step 1650, loss 0.548132.
Train: 2018-07-31T09:11:43.331088: step 1651, loss 0.613273.
Train: 2018-07-31T09:11:43.487305: step 1652, loss 0.554454.
Train: 2018-07-31T09:11:43.643512: step 1653, loss 0.613403.
Train: 2018-07-31T09:11:43.799726: step 1654, loss 0.545244.
Train: 2018-07-31T09:11:43.955911: step 1655, loss 0.503772.
Train: 2018-07-31T09:11:44.112152: step 1656, loss 0.553373.
Train: 2018-07-31T09:11:44.268372: step 1657, loss 0.595061.
Train: 2018-07-31T09:11:44.424548: step 1658, loss 0.570204.
Train: 2018-07-31T09:11:44.580795: step 1659, loss 0.63671.
Train: 2018-07-31T09:11:44.737001: step 1660, loss 0.61993.
Test: 2018-07-31T09:11:44.971296: step 1660, loss 0.548794.
Train: 2018-07-31T09:11:45.127509: step 1661, loss 0.581909.
Train: 2018-07-31T09:11:45.283724: step 1662, loss 0.661987.
Train: 2018-07-31T09:11:45.424353: step 1663, loss 0.603864.
Train: 2018-07-31T09:11:45.580559: step 1664, loss 0.530208.
Train: 2018-07-31T09:11:45.736772: step 1665, loss 0.529786.
Train: 2018-07-31T09:11:45.892955: step 1666, loss 0.618887.
Train: 2018-07-31T09:11:46.049168: step 1667, loss 0.475074.
Train: 2018-07-31T09:11:46.205382: step 1668, loss 0.58738.
Train: 2018-07-31T09:11:46.361632: step 1669, loss 0.546225.
Train: 2018-07-31T09:11:46.517809: step 1670, loss 0.579499.
Test: 2018-07-31T09:11:46.752160: step 1670, loss 0.549557.
Train: 2018-07-31T09:11:46.908343: step 1671, loss 0.587241.
Train: 2018-07-31T09:11:47.064587: step 1672, loss 0.611338.
Train: 2018-07-31T09:11:47.220770: step 1673, loss 0.54705.
Train: 2018-07-31T09:11:47.361362: step 1674, loss 0.555.
Train: 2018-07-31T09:11:47.517605: step 1675, loss 0.466494.
Train: 2018-07-31T09:11:47.673819: step 1676, loss 0.570523.
Train: 2018-07-31T09:11:47.830032: step 1677, loss 0.595016.
Train: 2018-07-31T09:11:47.986246: step 1678, loss 0.57904.
Train: 2018-07-31T09:11:48.142459: step 1679, loss 0.555011.
Train: 2018-07-31T09:11:48.298643: step 1680, loss 0.562903.
Test: 2018-07-31T09:11:48.548584: step 1680, loss 0.549154.
Train: 2018-07-31T09:11:48.704798: step 1681, loss 0.513781.
Train: 2018-07-31T09:11:48.861041: step 1682, loss 0.570695.
Train: 2018-07-31T09:11:49.017249: step 1683, loss 0.587419.
Train: 2018-07-31T09:11:49.173437: step 1684, loss 0.538594.
Train: 2018-07-31T09:11:49.329681: step 1685, loss 0.530117.
Train: 2018-07-31T09:11:49.485894: step 1686, loss 0.571021.
Train: 2018-07-31T09:11:49.642077: step 1687, loss 0.645777.
Train: 2018-07-31T09:11:49.798315: step 1688, loss 0.553973.
Train: 2018-07-31T09:11:49.954505: step 1689, loss 0.546576.
Train: 2018-07-31T09:11:50.110742: step 1690, loss 0.587003.
Test: 2018-07-31T09:11:50.345068: step 1690, loss 0.548517.
Train: 2018-07-31T09:11:50.501281: step 1691, loss 0.546452.
Train: 2018-07-31T09:11:50.657496: step 1692, loss 0.503708.
Train: 2018-07-31T09:11:50.813679: step 1693, loss 0.478572.
Train: 2018-07-31T09:11:50.969922: step 1694, loss 0.512029.
Train: 2018-07-31T09:11:51.126105: step 1695, loss 0.639485.
Train: 2018-07-31T09:11:51.282348: step 1696, loss 0.579246.
Train: 2018-07-31T09:11:51.438532: step 1697, loss 0.52039.
Train: 2018-07-31T09:11:51.594775: step 1698, loss 0.416636.
Train: 2018-07-31T09:11:51.750959: step 1699, loss 0.58011.
Train: 2018-07-31T09:11:51.907173: step 1700, loss 0.649581.
Test: 2018-07-31T09:11:52.141492: step 1700, loss 0.547887.
Train: 2018-07-31T09:11:52.875696: step 1701, loss 0.509848.
Train: 2018-07-31T09:11:53.016288: step 1702, loss 0.545463.
Train: 2018-07-31T09:11:53.188123: step 1703, loss 0.544161.
Train: 2018-07-31T09:11:53.328745: step 1704, loss 0.588428.
Train: 2018-07-31T09:11:53.484957: step 1705, loss 0.561461.
Train: 2018-07-31T09:11:53.641173: step 1706, loss 0.599166.
Train: 2018-07-31T09:11:53.812976: step 1707, loss 0.599007.
Train: 2018-07-31T09:11:53.969219: step 1708, loss 0.615799.
Train: 2018-07-31T09:11:54.125403: step 1709, loss 0.570471.
Train: 2018-07-31T09:11:54.281617: step 1710, loss 0.535588.
Test: 2018-07-31T09:11:54.531582: step 1710, loss 0.547784.
Train: 2018-07-31T09:11:54.687801: step 1711, loss 0.571359.
Train: 2018-07-31T09:11:54.844015: step 1712, loss 0.554097.
Train: 2018-07-31T09:11:55.000228: step 1713, loss 0.615474.
Train: 2018-07-31T09:11:55.156443: step 1714, loss 0.519169.
Train: 2018-07-31T09:11:55.312655: step 1715, loss 0.535888.
Train: 2018-07-31T09:11:55.468868: step 1716, loss 0.561747.
Train: 2018-07-31T09:11:55.625081: step 1717, loss 0.554443.
Train: 2018-07-31T09:11:55.781298: step 1718, loss 0.501812.
Train: 2018-07-31T09:11:55.937479: step 1719, loss 0.580527.
Train: 2018-07-31T09:11:56.093721: step 1720, loss 0.596971.
Test: 2018-07-31T09:11:56.328044: step 1720, loss 0.547867.
Train: 2018-07-31T09:11:56.484256: step 1721, loss 0.53757.
Train: 2018-07-31T09:11:56.640438: step 1722, loss 0.561876.
Train: 2018-07-31T09:11:56.781062: step 1723, loss 0.605976.
Train: 2018-07-31T09:11:56.952866: step 1724, loss 0.52775.
Train: 2018-07-31T09:11:57.093483: step 1725, loss 0.493341.
Train: 2018-07-31T09:11:57.265291: step 1726, loss 0.632668.
Train: 2018-07-31T09:11:57.421531: step 1727, loss 0.528588.
Train: 2018-07-31T09:11:57.562134: step 1728, loss 0.589514.
Train: 2018-07-31T09:11:57.718341: step 1729, loss 0.49332.
Train: 2018-07-31T09:11:57.874563: step 1730, loss 0.596434.
Test: 2018-07-31T09:11:58.108845: step 1730, loss 0.547902.
Train: 2018-07-31T09:11:58.280678: step 1731, loss 0.563464.
Train: 2018-07-31T09:11:58.421301: step 1732, loss 0.614721.
Train: 2018-07-31T09:11:58.593144: step 1733, loss 0.632006.
Train: 2018-07-31T09:11:58.749319: step 1734, loss 0.528322.
Train: 2018-07-31T09:11:58.889913: step 1735, loss 0.631119.
Train: 2018-07-31T09:11:59.046156: step 1736, loss 0.510408.
Train: 2018-07-31T09:11:59.186718: step 1737, loss 0.588371.
Train: 2018-07-31T09:11:59.358552: step 1738, loss 0.554184.
Train: 2018-07-31T09:11:59.499174: step 1739, loss 0.554365.
Train: 2018-07-31T09:11:59.655358: step 1740, loss 0.587394.
Test: 2018-07-31T09:11:59.889709: step 1740, loss 0.548221.
Train: 2018-07-31T09:12:00.045921: step 1741, loss 0.604868.
Train: 2018-07-31T09:12:00.202135: step 1742, loss 0.502858.
Train: 2018-07-31T09:12:00.358319: step 1743, loss 0.571415.
Train: 2018-07-31T09:12:00.514562: step 1744, loss 0.596015.
Train: 2018-07-31T09:12:00.670778: step 1745, loss 0.647195.
Train: 2018-07-31T09:12:00.826959: step 1746, loss 0.553606.
Train: 2018-07-31T09:12:00.983171: step 1747, loss 0.529349.
Train: 2018-07-31T09:12:01.139384: step 1748, loss 0.562876.
Train: 2018-07-31T09:12:01.311251: step 1749, loss 0.654743.
Train: 2018-07-31T09:12:01.467434: step 1750, loss 0.687624.
Test: 2018-07-31T09:12:01.701785: step 1750, loss 0.548712.
Train: 2018-07-31T09:12:01.857992: step 1751, loss 0.46326.
Train: 2018-07-31T09:12:01.998589: step 1752, loss 0.60366.
Train: 2018-07-31T09:12:02.154803: step 1753, loss 0.528838.
Train: 2018-07-31T09:12:02.310985: step 1754, loss 0.553992.
Train: 2018-07-31T09:12:02.482820: step 1755, loss 0.554287.
Train: 2018-07-31T09:12:02.623413: step 1756, loss 0.513422.
Train: 2018-07-31T09:12:02.779656: step 1757, loss 0.604204.
Train: 2018-07-31T09:12:02.935870: step 1758, loss 0.513457.
Train: 2018-07-31T09:12:03.092082: step 1759, loss 0.537969.
Train: 2018-07-31T09:12:03.248296: step 1760, loss 0.57897.
Test: 2018-07-31T09:12:03.482588: step 1760, loss 0.548765.
Train: 2018-07-31T09:12:03.638799: step 1761, loss 0.545656.
Train: 2018-07-31T09:12:03.795013: step 1762, loss 0.488812.
Train: 2018-07-31T09:12:03.951257: step 1763, loss 0.487908.
Train: 2018-07-31T09:12:04.107439: step 1764, loss 0.629336.
Train: 2018-07-31T09:12:04.248033: step 1765, loss 0.579821.
Train: 2018-07-31T09:12:04.404272: step 1766, loss 0.561921.
Train: 2018-07-31T09:12:04.560490: step 1767, loss 0.537467.
Train: 2018-07-31T09:12:04.716703: step 1768, loss 0.570834.
Train: 2018-07-31T09:12:04.872917: step 1769, loss 0.58751.
Train: 2018-07-31T09:12:05.029130: step 1770, loss 0.52839.
Test: 2018-07-31T09:12:05.263450: step 1770, loss 0.548161.
Train: 2018-07-31T09:12:05.419663: step 1771, loss 0.554354.
Train: 2018-07-31T09:12:05.591468: step 1772, loss 0.570468.
Train: 2018-07-31T09:12:05.747710: step 1773, loss 0.631026.
Train: 2018-07-31T09:12:05.903894: step 1774, loss 0.596846.
Train: 2018-07-31T09:12:06.060139: step 1775, loss 0.494731.
Train: 2018-07-31T09:12:06.216352: step 1776, loss 0.511118.
Train: 2018-07-31T09:12:06.388186: step 1777, loss 0.588835.
Train: 2018-07-31T09:12:06.544370: step 1778, loss 0.579288.
Train: 2018-07-31T09:12:06.700613: step 1779, loss 0.579242.
Train: 2018-07-31T09:12:06.856797: step 1780, loss 0.501715.
Test: 2018-07-31T09:12:07.091118: step 1780, loss 0.547991.
Train: 2018-07-31T09:12:07.247329: step 1781, loss 0.6318.
Train: 2018-07-31T09:12:07.419165: step 1782, loss 0.519023.
Train: 2018-07-31T09:12:07.575377: step 1783, loss 0.622461.
Train: 2018-07-31T09:12:07.731621: step 1784, loss 0.588378.
Train: 2018-07-31T09:12:07.887834: step 1785, loss 0.527972.
Train: 2018-07-31T09:12:08.044050: step 1786, loss 0.570876.
Train: 2018-07-31T09:12:08.200232: step 1787, loss 0.493181.
Train: 2018-07-31T09:12:08.356475: step 1788, loss 0.52794.
Train: 2018-07-31T09:12:08.512659: step 1789, loss 0.553745.
Train: 2018-07-31T09:12:08.668908: step 1790, loss 0.589074.
Test: 2018-07-31T09:12:08.903223: step 1790, loss 0.547911.
Train: 2018-07-31T09:12:09.059405: step 1791, loss 0.535883.
Train: 2018-07-31T09:12:09.215649: step 1792, loss 0.562771.
Train: 2018-07-31T09:12:09.371862: step 1793, loss 0.598343.
Train: 2018-07-31T09:12:09.528076: step 1794, loss 0.622934.
Train: 2018-07-31T09:12:09.684290: step 1795, loss 0.552537.
Train: 2018-07-31T09:12:09.840502: step 1796, loss 0.57096.
Train: 2018-07-31T09:12:09.996686: step 1797, loss 0.554263.
Train: 2018-07-31T09:12:10.152900: step 1798, loss 0.544824.
Train: 2018-07-31T09:12:10.309143: step 1799, loss 0.536784.
Train: 2018-07-31T09:12:10.465356: step 1800, loss 0.623359.
Test: 2018-07-31T09:12:10.699646: step 1800, loss 0.547934.
Train: 2018-07-31T09:12:11.433880: step 1801, loss 0.570401.
Train: 2018-07-31T09:12:11.590062: step 1802, loss 0.588603.
Train: 2018-07-31T09:12:11.746277: step 1803, loss 0.493366.
Train: 2018-07-31T09:12:11.902520: step 1804, loss 0.51042.
Train: 2018-07-31T09:12:12.058733: step 1805, loss 0.492798.
Train: 2018-07-31T09:12:12.214916: step 1806, loss 0.545872.
Train: 2018-07-31T09:12:12.355509: step 1807, loss 0.641123.
Train: 2018-07-31T09:12:12.511746: step 1808, loss 0.519158.
Train: 2018-07-31T09:12:12.667935: step 1809, loss 0.553995.
Train: 2018-07-31T09:12:12.824179: step 1810, loss 0.518263.
Test: 2018-07-31T09:12:13.058496: step 1810, loss 0.547827.
Train: 2018-07-31T09:12:13.214717: step 1811, loss 0.562209.
Train: 2018-07-31T09:12:13.370926: step 1812, loss 0.560051.
Train: 2018-07-31T09:12:13.527110: step 1813, loss 0.587254.
Train: 2018-07-31T09:12:13.683322: step 1814, loss 0.474501.
Train: 2018-07-31T09:12:13.839566: step 1815, loss 0.47258.
Train: 2018-07-31T09:12:13.980159: step 1816, loss 0.571015.
Train: 2018-07-31T09:12:14.136366: step 1817, loss 0.562857.
Train: 2018-07-31T09:12:14.292554: step 1818, loss 0.500318.
Train: 2018-07-31T09:12:14.448768: step 1819, loss 0.590183.
Train: 2018-07-31T09:12:14.605012: step 1820, loss 0.527492.
Test: 2018-07-31T09:12:14.839302: step 1820, loss 0.547616.
Train: 2018-07-31T09:12:14.995515: step 1821, loss 0.573919.
Train: 2018-07-31T09:12:15.151728: step 1822, loss 0.574389.
Train: 2018-07-31T09:12:15.307973: step 1823, loss 0.553926.
Train: 2018-07-31T09:12:15.460131: step 1824, loss 0.56394.
Train: 2018-07-31T09:12:15.616314: step 1825, loss 0.636682.
Train: 2018-07-31T09:12:15.772528: step 1826, loss 0.598973.
Train: 2018-07-31T09:12:15.944363: step 1827, loss 0.565271.
Train: 2018-07-31T09:12:16.084985: step 1828, loss 0.545995.
Train: 2018-07-31T09:12:16.256814: step 1829, loss 0.598215.
Train: 2018-07-31T09:12:16.413033: step 1830, loss 0.526612.
Test: 2018-07-31T09:12:16.647323: step 1830, loss 0.547708.
Train: 2018-07-31T09:12:16.803537: step 1831, loss 0.580618.
Train: 2018-07-31T09:12:16.959749: step 1832, loss 0.607164.
Train: 2018-07-31T09:12:17.115964: step 1833, loss 0.510242.
Train: 2018-07-31T09:12:17.272176: step 1834, loss 0.597934.
Train: 2018-07-31T09:12:17.428389: step 1835, loss 0.580316.
Train: 2018-07-31T09:12:17.584639: step 1836, loss 0.553815.
Train: 2018-07-31T09:12:17.725226: step 1837, loss 0.571221.
Train: 2018-07-31T09:12:17.897055: step 1838, loss 0.597618.
Train: 2018-07-31T09:12:18.037653: step 1839, loss 0.562767.
Train: 2018-07-31T09:12:18.193867: step 1840, loss 0.640108.
Test: 2018-07-31T09:12:18.428156: step 1840, loss 0.548209.
Train: 2018-07-31T09:12:18.584370: step 1841, loss 0.511111.
Train: 2018-07-31T09:12:18.740613: step 1842, loss 0.536829.
Train: 2018-07-31T09:12:18.912447: step 1843, loss 0.562122.
Train: 2018-07-31T09:12:19.068663: step 1844, loss 0.58739.
Train: 2018-07-31T09:12:19.224874: step 1845, loss 0.56195.
Train: 2018-07-31T09:12:19.365467: step 1846, loss 0.570905.
Train: 2018-07-31T09:12:19.521680: step 1847, loss 0.562529.
Train: 2018-07-31T09:12:19.677864: step 1848, loss 0.595959.
Train: 2018-07-31T09:12:19.834077: step 1849, loss 0.56246.
Train: 2018-07-31T09:12:19.990320: step 1850, loss 0.487294.
Test: 2018-07-31T09:12:20.224641: step 1850, loss 0.548518.
Train: 2018-07-31T09:12:20.380823: step 1851, loss 0.587434.
Train: 2018-07-31T09:12:20.599522: step 1852, loss 0.570553.
Train: 2018-07-31T09:12:20.755766: step 1853, loss 0.503742.
Train: 2018-07-31T09:12:20.911950: step 1854, loss 0.571253.
Train: 2018-07-31T09:12:21.068194: step 1855, loss 0.55411.
Train: 2018-07-31T09:12:21.224400: step 1856, loss 0.61292.
Train: 2018-07-31T09:12:21.380589: step 1857, loss 0.596322.
Train: 2018-07-31T09:12:21.536803: step 1858, loss 0.570016.
Train: 2018-07-31T09:12:21.693016: step 1859, loss 0.596342.
Train: 2018-07-31T09:12:21.849230: step 1860, loss 0.57111.
Test: 2018-07-31T09:12:22.083575: step 1860, loss 0.548482.
Train: 2018-07-31T09:12:22.302278: step 1861, loss 0.587544.
Train: 2018-07-31T09:12:22.458462: step 1862, loss 0.563503.
Train: 2018-07-31T09:12:22.614705: step 1863, loss 0.570462.
Train: 2018-07-31T09:12:22.770918: step 1864, loss 0.52947.
Train: 2018-07-31T09:12:22.927134: step 1865, loss 0.612301.
Train: 2018-07-31T09:12:23.083345: step 1866, loss 0.536959.
Train: 2018-07-31T09:12:23.239559: step 1867, loss 0.495792.
Train: 2018-07-31T09:12:23.395743: step 1868, loss 0.537672.
Train: 2018-07-31T09:12:23.551980: step 1869, loss 0.571112.
Train: 2018-07-31T09:12:23.708199: step 1870, loss 0.537466.
Test: 2018-07-31T09:12:23.942523: step 1870, loss 0.548435.
Train: 2018-07-31T09:12:24.098733: step 1871, loss 0.587951.
Train: 2018-07-31T09:12:24.254946: step 1872, loss 0.537298.
Train: 2018-07-31T09:12:24.411129: step 1873, loss 0.570931.
Train: 2018-07-31T09:12:24.567375: step 1874, loss 0.536885.
Train: 2018-07-31T09:12:24.707936: step 1875, loss 0.55448.
Train: 2018-07-31T09:12:24.864149: step 1876, loss 0.536601.
Train: 2018-07-31T09:12:25.035983: step 1877, loss 0.511889.
Train: 2018-07-31T09:12:25.176576: step 1878, loss 0.55352.
Train: 2018-07-31T09:12:25.332788: step 1879, loss 0.502711.
Train: 2018-07-31T09:12:25.489032: step 1880, loss 0.53678.
Test: 2018-07-31T09:12:25.723354: step 1880, loss 0.547959.
Train: 2018-07-31T09:12:25.879535: step 1881, loss 0.580474.
Train: 2018-07-31T09:12:26.035750: step 1882, loss 0.570443.
Train: 2018-07-31T09:12:26.191962: step 1883, loss 0.510633.
Train: 2018-07-31T09:12:26.348177: step 1884, loss 0.536428.
Train: 2018-07-31T09:12:26.504389: step 1885, loss 0.526999.
Train: 2018-07-31T09:12:26.660633: step 1886, loss 0.491095.
Train: 2018-07-31T09:12:26.816815: step 1887, loss 0.579374.
Train: 2018-07-31T09:12:26.973060: step 1888, loss 0.535752.
Train: 2018-07-31T09:12:27.129274: step 1889, loss 0.589677.
Train: 2018-07-31T09:12:27.285487: step 1890, loss 0.562835.
Test: 2018-07-31T09:12:27.519778: step 1890, loss 0.547638.
Train: 2018-07-31T09:12:27.676020: step 1891, loss 0.48098.
Train: 2018-07-31T09:12:27.832234: step 1892, loss 0.580999.
Train: 2018-07-31T09:12:27.988447: step 1893, loss 0.607954.
Train: 2018-07-31T09:12:28.160252: step 1894, loss 0.489243.
Train: 2018-07-31T09:12:28.316497: step 1895, loss 0.672007.
Train: 2018-07-31T09:12:28.488330: step 1896, loss 0.518044.
Train: 2018-07-31T09:12:28.644514: step 1897, loss 0.51682.
Train: 2018-07-31T09:12:28.800759: step 1898, loss 0.52612.
Train: 2018-07-31T09:12:28.956970: step 1899, loss 0.56414.
Train: 2018-07-31T09:12:29.113154: step 1900, loss 0.626255.
Test: 2018-07-31T09:12:29.347473: step 1900, loss 0.547612.
Train: 2018-07-31T09:12:30.097328: step 1901, loss 0.52515.
Train: 2018-07-31T09:12:30.253541: step 1902, loss 0.644068.
Train: 2018-07-31T09:12:30.409725: step 1903, loss 0.607563.
Train: 2018-07-31T09:12:30.565938: step 1904, loss 0.600117.
Train: 2018-07-31T09:12:30.706530: step 1905, loss 0.544938.
Train: 2018-07-31T09:12:30.862744: step 1906, loss 0.527139.
Train: 2018-07-31T09:12:31.018987: step 1907, loss 0.617013.
Train: 2018-07-31T09:12:31.175170: step 1908, loss 0.554398.
Train: 2018-07-31T09:12:31.331414: step 1909, loss 0.580831.
Train: 2018-07-31T09:12:31.471976: step 1910, loss 0.5973.
Test: 2018-07-31T09:12:31.721917: step 1910, loss 0.547771.
Train: 2018-07-31T09:12:31.878131: step 1911, loss 0.570602.
Train: 2018-07-31T09:12:32.034378: step 1912, loss 0.562956.
Train: 2018-07-31T09:12:32.190559: step 1913, loss 0.500646.
Train: 2018-07-31T09:12:32.346771: step 1914, loss 0.589316.
Train: 2018-07-31T09:12:32.502985: step 1915, loss 0.544875.
Train: 2018-07-31T09:12:32.659197: step 1916, loss 0.579636.
Train: 2018-07-31T09:12:32.815444: step 1917, loss 0.632158.
Train: 2018-07-31T09:12:32.971626: step 1918, loss 0.493168.
Train: 2018-07-31T09:12:33.127862: step 1919, loss 0.553455.
Train: 2018-07-31T09:12:33.284083: step 1920, loss 0.595962.
Test: 2018-07-31T09:12:33.518403: step 1920, loss 0.548105.
Train: 2018-07-31T09:12:33.674610: step 1921, loss 0.58839.
Train: 2018-07-31T09:12:33.830798: step 1922, loss 0.553729.
Train: 2018-07-31T09:12:34.002663: step 1923, loss 0.520336.
Train: 2018-07-31T09:12:34.158871: step 1924, loss 0.511011.
Train: 2018-07-31T09:12:34.315092: step 1925, loss 0.57174.
Train: 2018-07-31T09:12:34.455682: step 1926, loss 0.562754.
Train: 2018-07-31T09:12:34.611897: step 1927, loss 0.604783.
Train: 2018-07-31T09:12:34.768080: step 1928, loss 0.553742.
Train: 2018-07-31T09:12:34.924325: step 1929, loss 0.535985.
Train: 2018-07-31T09:12:35.080507: step 1930, loss 0.596258.
Test: 2018-07-31T09:12:35.330449: step 1930, loss 0.548164.
Train: 2018-07-31T09:12:35.471039: step 1931, loss 0.537161.
Train: 2018-07-31T09:12:35.627254: step 1932, loss 0.553957.
Train: 2018-07-31T09:12:35.783496: step 1933, loss 0.597047.
Train: 2018-07-31T09:12:35.939712: step 1934, loss 0.562064.
Train: 2018-07-31T09:12:36.095926: step 1935, loss 0.579347.
Train: 2018-07-31T09:12:36.252137: step 1936, loss 0.588789.
Train: 2018-07-31T09:12:36.408319: step 1937, loss 0.536961.
Train: 2018-07-31T09:12:36.564566: step 1938, loss 0.570241.
Train: 2018-07-31T09:12:36.720747: step 1939, loss 0.630767.
Train: 2018-07-31T09:12:36.861369: step 1940, loss 0.52066.
Test: 2018-07-31T09:12:37.111311: step 1940, loss 0.548264.
Train: 2018-07-31T09:12:37.267530: step 1941, loss 0.588127.
Train: 2018-07-31T09:12:37.423737: step 1942, loss 0.545377.
Train: 2018-07-31T09:12:37.564332: step 1943, loss 0.511385.
Train: 2018-07-31T09:12:37.720543: step 1944, loss 0.63932.
Train: 2018-07-31T09:12:37.876757: step 1945, loss 0.570389.
Train: 2018-07-31T09:12:38.032940: step 1946, loss 0.596807.
Train: 2018-07-31T09:12:38.173531: step 1947, loss 0.570499.
Train: 2018-07-31T09:12:38.329745: step 1948, loss 0.579547.
Train: 2018-07-31T09:12:38.485990: step 1949, loss 0.554566.
Train: 2018-07-31T09:12:38.642202: step 1950, loss 0.521064.
Test: 2018-07-31T09:12:38.876494: step 1950, loss 0.548492.
Train: 2018-07-31T09:12:39.032735: step 1951, loss 0.538002.
Train: 2018-07-31T09:12:39.188918: step 1952, loss 0.512532.
Train: 2018-07-31T09:12:39.345163: step 1953, loss 0.504094.
Train: 2018-07-31T09:12:39.501376: step 1954, loss 0.596187.
Train: 2018-07-31T09:12:39.657590: step 1955, loss 0.61335.
Train: 2018-07-31T09:12:39.813797: step 1956, loss 0.52816.
Train: 2018-07-31T09:12:39.969987: step 1957, loss 0.55344.
Train: 2018-07-31T09:12:40.126231: step 1958, loss 0.629717.
Train: 2018-07-31T09:12:40.298034: step 1959, loss 0.529002.
Train: 2018-07-31T09:12:40.454248: step 1960, loss 0.587113.
Test: 2018-07-31T09:12:40.688596: step 1960, loss 0.54825.
Train: 2018-07-31T09:12:40.844805: step 1961, loss 0.553385.
Train: 2018-07-31T09:12:41.001025: step 1962, loss 0.545831.
Train: 2018-07-31T09:12:41.141617: step 1963, loss 0.634807.
Train: 2018-07-31T09:12:41.313421: step 1964, loss 0.503725.
Train: 2018-07-31T09:12:41.469635: step 1965, loss 0.587498.
Train: 2018-07-31T09:12:41.610258: step 1966, loss 0.553673.
Train: 2018-07-31T09:12:41.766441: step 1967, loss 0.54577.
Train: 2018-07-31T09:12:41.922686: step 1968, loss 0.570471.
Train: 2018-07-31T09:12:42.078897: step 1969, loss 0.510645.
Train: 2018-07-31T09:12:42.235110: step 1970, loss 0.588376.
Test: 2018-07-31T09:12:42.469401: step 1970, loss 0.548131.
Train: 2018-07-31T09:12:42.641265: step 1971, loss 0.596652.
Train: 2018-07-31T09:12:42.781858: step 1972, loss 0.545001.
Train: 2018-07-31T09:12:42.938042: step 1973, loss 0.579402.
Train: 2018-07-31T09:12:43.094285: step 1974, loss 0.537003.
Train: 2018-07-31T09:12:43.250499: step 1975, loss 0.580591.
Train: 2018-07-31T09:12:43.406712: step 1976, loss 0.528198.
Train: 2018-07-31T09:12:43.562920: step 1977, loss 0.553496.
Train: 2018-07-31T09:12:43.719108: step 1978, loss 0.554434.
Train: 2018-07-31T09:12:43.875322: step 1979, loss 0.570977.
Train: 2018-07-31T09:12:44.031566: step 1980, loss 0.528055.
Test: 2018-07-31T09:12:44.265887: step 1980, loss 0.548035.
Train: 2018-07-31T09:12:44.422068: step 1981, loss 0.537145.
Train: 2018-07-31T09:12:44.578282: step 1982, loss 0.467607.
Train: 2018-07-31T09:12:44.734496: step 1983, loss 0.554409.
Train: 2018-07-31T09:12:44.906360: step 1984, loss 0.606513.
Train: 2018-07-31T09:12:45.062543: step 1985, loss 0.545332.
Train: 2018-07-31T09:12:45.218757: step 1986, loss 0.571422.
Train: 2018-07-31T09:12:45.374970: step 1987, loss 0.641955.
Train: 2018-07-31T09:12:45.531209: step 1988, loss 0.527583.
Train: 2018-07-31T09:12:45.671777: step 1989, loss 0.605768.
Train: 2018-07-31T09:12:45.827989: step 1990, loss 0.597332.
Test: 2018-07-31T09:12:46.077962: step 1990, loss 0.547856.
Train: 2018-07-31T09:12:46.234169: step 1991, loss 0.605286.
Train: 2018-07-31T09:12:46.390388: step 1992, loss 0.527456.
Train: 2018-07-31T09:12:46.546570: step 1993, loss 0.562774.
Train: 2018-07-31T09:12:46.702809: step 1994, loss 0.614425.
Train: 2018-07-31T09:12:46.843407: step 1995, loss 0.571078.
Train: 2018-07-31T09:12:46.999621: step 1996, loss 0.53651.
Train: 2018-07-31T09:12:47.155828: step 1997, loss 0.536643.
Train: 2018-07-31T09:12:47.327668: step 1998, loss 0.562931.
Train: 2018-07-31T09:12:47.483851: step 1999, loss 0.570955.
Train: 2018-07-31T09:12:47.624444: step 2000, loss 0.53675.
Test: 2018-07-31T09:12:47.858764: step 2000, loss 0.548108.
Train: 2018-07-31T09:12:48.592997: step 2001, loss 0.510668.
Train: 2018-07-31T09:12:48.749211: step 2002, loss 0.528576.
Train: 2018-07-31T09:12:48.905418: step 2003, loss 0.571936.
Train: 2018-07-31T09:12:49.061608: step 2004, loss 0.562397.
Train: 2018-07-31T09:12:49.217851: step 2005, loss 0.553951.
Train: 2018-07-31T09:12:49.374034: step 2006, loss 0.622626.
Train: 2018-07-31T09:12:49.530248: step 2007, loss 0.527576.
Train: 2018-07-31T09:12:49.686493: step 2008, loss 0.554023.
Train: 2018-07-31T09:12:49.842674: step 2009, loss 0.553062.
Train: 2018-07-31T09:12:49.998918: step 2010, loss 0.536027.
Test: 2018-07-31T09:12:50.233239: step 2010, loss 0.547953.
Train: 2018-07-31T09:12:50.389422: step 2011, loss 0.56218.
Train: 2018-07-31T09:12:50.561281: step 2012, loss 0.623079.
Train: 2018-07-31T09:12:50.717469: step 2013, loss 0.545532.
Train: 2018-07-31T09:12:50.873708: step 2014, loss 0.561737.
Train: 2018-07-31T09:12:51.029895: step 2015, loss 0.614873.
Train: 2018-07-31T09:12:51.186111: step 2016, loss 0.544864.
Train: 2018-07-31T09:12:51.342348: step 2017, loss 0.544993.
Train: 2018-07-31T09:12:51.482915: step 2018, loss 0.501897.
Train: 2018-07-31T09:12:51.639159: step 2019, loss 0.54527.
Train: 2018-07-31T09:12:51.795372: step 2020, loss 0.640287.
Test: 2018-07-31T09:12:52.029687: step 2020, loss 0.548003.
Train: 2018-07-31T09:12:52.185907: step 2021, loss 0.623273.
Train: 2018-07-31T09:12:52.342119: step 2022, loss 0.536578.
Train: 2018-07-31T09:12:52.498302: step 2023, loss 0.519558.
Train: 2018-07-31T09:12:52.654554: step 2024, loss 0.59665.
Train: 2018-07-31T09:12:52.826387: step 2025, loss 0.571722.
Train: 2018-07-31T09:12:52.982595: step 2026, loss 0.630743.
Train: 2018-07-31T09:12:53.138808: step 2027, loss 0.53732.
Train: 2018-07-31T09:12:53.295020: step 2028, loss 0.469965.
Train: 2018-07-31T09:12:53.451234: step 2029, loss 0.470136.
Train: 2018-07-31T09:12:53.607417: step 2030, loss 0.570727.
Test: 2018-07-31T09:12:53.841738: step 2030, loss 0.548136.
Train: 2018-07-31T09:12:53.997976: step 2031, loss 0.518447.
Train: 2018-07-31T09:12:54.154164: step 2032, loss 0.519008.
Train: 2018-07-31T09:12:54.310378: step 2033, loss 0.580225.
Train: 2018-07-31T09:12:54.482212: step 2034, loss 0.510022.
Train: 2018-07-31T09:12:54.638426: step 2035, loss 0.563063.
Train: 2018-07-31T09:12:54.794642: step 2036, loss 0.536254.
Train: 2018-07-31T09:12:54.950853: step 2037, loss 0.535904.
Train: 2018-07-31T09:12:55.091475: step 2038, loss 0.546251.
Train: 2018-07-31T09:12:55.247689: step 2039, loss 0.563212.
Train: 2018-07-31T09:12:55.403897: step 2040, loss 0.491653.
Test: 2018-07-31T09:12:55.653813: step 2040, loss 0.547673.
Train: 2018-07-31T09:12:55.810028: step 2041, loss 0.580436.
Train: 2018-07-31T09:12:55.966270: step 2042, loss 0.544581.
Train: 2018-07-31T09:12:56.122455: step 2043, loss 0.580169.
Train: 2018-07-31T09:12:56.278666: step 2044, loss 0.482195.
Train: 2018-07-31T09:12:56.434882: step 2045, loss 0.508682.
Train: 2018-07-31T09:12:56.591093: step 2046, loss 0.553447.
Train: 2018-07-31T09:12:56.747337: step 2047, loss 0.535333.
Train: 2018-07-31T09:12:56.903520: step 2048, loss 0.608775.
Train: 2018-07-31T09:12:57.044145: step 2049, loss 0.554042.
Train: 2018-07-31T09:12:57.200357: step 2050, loss 0.553857.
Test: 2018-07-31T09:12:57.434679: step 2050, loss 0.5476.
Train: 2018-07-31T09:12:57.590859: step 2051, loss 0.65504.
Train: 2018-07-31T09:12:57.747103: step 2052, loss 0.581305.
Train: 2018-07-31T09:12:57.903287: step 2053, loss 0.636029.
Train: 2018-07-31T09:12:58.059499: step 2054, loss 0.60841.
Train: 2018-07-31T09:12:58.215745: step 2055, loss 0.580593.
Train: 2018-07-31T09:12:58.371926: step 2056, loss 0.581086.
Train: 2018-07-31T09:12:58.528171: step 2057, loss 0.724646.
Train: 2018-07-31T09:12:58.684353: step 2058, loss 0.615812.
Train: 2018-07-31T09:12:58.840566: step 2059, loss 0.580189.
Train: 2018-07-31T09:12:58.996811: step 2060, loss 0.632527.
Test: 2018-07-31T09:12:59.231102: step 2060, loss 0.547899.
Train: 2018-07-31T09:12:59.387345: step 2061, loss 0.527854.
Train: 2018-07-31T09:12:59.559180: step 2062, loss 0.519369.
Train: 2018-07-31T09:12:59.715393: step 2063, loss 0.605439.
Train: 2018-07-31T09:12:59.871576: step 2064, loss 0.553979.
Train: 2018-07-31T09:13:00.027814: step 2065, loss 0.613308.
Train: 2018-07-31T09:13:00.168381: step 2066, loss 0.520487.
Train: 2018-07-31T09:13:00.340246: step 2067, loss 0.537368.
Train: 2018-07-31T09:13:00.496459: step 2068, loss 0.537505.
Train: 2018-07-31T09:13:00.652667: step 2069, loss 0.479256.
Train: 2018-07-31T09:13:00.808888: step 2070, loss 0.520984.
Test: 2018-07-31T09:13:01.043209: step 2070, loss 0.548551.
Train: 2018-07-31T09:13:01.199415: step 2071, loss 0.587552.
Train: 2018-07-31T09:13:01.355634: step 2072, loss 0.579067.
Train: 2018-07-31T09:13:01.511847: step 2073, loss 0.554087.
Train: 2018-07-31T09:13:01.668055: step 2074, loss 0.596201.
Train: 2018-07-31T09:13:01.824244: step 2075, loss 0.604322.
Train: 2018-07-31T09:13:01.980457: step 2076, loss 0.604042.
Train: 2018-07-31T09:13:02.136701: step 2077, loss 0.620948.
Train: 2018-07-31T09:13:02.292916: step 2078, loss 0.554301.
Train: 2018-07-31T09:13:02.449127: step 2079, loss 0.595717.
Train: 2018-07-31T09:13:02.605335: step 2080, loss 0.529615.
Test: 2018-07-31T09:13:02.839633: step 2080, loss 0.548746.
Train: 2018-07-31T09:13:02.995844: step 2081, loss 0.636948.
Train: 2018-07-31T09:13:03.152088: step 2082, loss 0.471918.
Train: 2018-07-31T09:13:03.308270: step 2083, loss 0.521477.
Train: 2018-07-31T09:13:03.464517: step 2084, loss 0.537788.
Train: 2018-07-31T09:13:03.620728: step 2085, loss 0.562596.
Train: 2018-07-31T09:13:03.761321: step 2086, loss 0.579228.
Train: 2018-07-31T09:13:03.917533: step 2087, loss 0.587279.
Train: 2018-07-31T09:13:04.073747: step 2088, loss 0.579098.
Train: 2018-07-31T09:13:04.229931: step 2089, loss 0.512624.
Train: 2018-07-31T09:13:04.386174: step 2090, loss 0.595824.
Test: 2018-07-31T09:13:04.620488: step 2090, loss 0.548582.
Train: 2018-07-31T09:13:04.776707: step 2091, loss 0.587695.
Train: 2018-07-31T09:13:04.932922: step 2092, loss 0.579133.
Train: 2018-07-31T09:13:05.089105: step 2093, loss 0.470982.
Train: 2018-07-31T09:13:05.245350: step 2094, loss 0.61278.
Train: 2018-07-31T09:13:05.401555: step 2095, loss 0.545877.
Train: 2018-07-31T09:13:05.557776: step 2096, loss 0.553927.
Train: 2018-07-31T09:13:05.713957: step 2097, loss 0.570953.
Train: 2018-07-31T09:13:05.870201: step 2098, loss 0.54566.
Train: 2018-07-31T09:13:06.026415: step 2099, loss 0.562442.
Train: 2018-07-31T09:13:06.182630: step 2100, loss 0.604888.
Test: 2018-07-31T09:13:06.416918: step 2100, loss 0.548326.
Train: 2018-07-31T09:13:07.119878: step 2101, loss 0.545573.
Train: 2018-07-31T09:13:07.291713: step 2102, loss 0.638531.
Train: 2018-07-31T09:13:07.447926: step 2103, loss 0.537315.
Train: 2018-07-31T09:13:07.604165: step 2104, loss 0.537329.
Train: 2018-07-31T09:13:07.760354: step 2105, loss 0.587696.
Train: 2018-07-31T09:13:07.916566: step 2106, loss 0.494812.
Train: 2018-07-31T09:13:08.072810: step 2107, loss 0.630031.
Train: 2018-07-31T09:13:08.229025: step 2108, loss 0.460985.
Train: 2018-07-31T09:13:08.385237: step 2109, loss 0.528534.
Train: 2018-07-31T09:13:08.525798: step 2110, loss 0.571016.
Test: 2018-07-31T09:13:08.760151: step 2110, loss 0.54814.
Train: 2018-07-31T09:13:08.916357: step 2111, loss 0.587758.
Train: 2018-07-31T09:13:09.072577: step 2112, loss 0.554158.
Train: 2018-07-31T09:13:09.228760: step 2113, loss 0.588039.
Train: 2018-07-31T09:13:09.384972: step 2114, loss 0.599161.
Train: 2018-07-31T09:13:09.525565: step 2115, loss 0.527956.
Train: 2018-07-31T09:13:09.681779: step 2116, loss 0.528224.
Train: 2018-07-31T09:13:09.837992: step 2117, loss 0.527923.
Train: 2018-07-31T09:13:09.994206: step 2118, loss 0.553577.
Train: 2018-07-31T09:13:10.134797: step 2119, loss 0.519376.
Train: 2018-07-31T09:13:10.291040: step 2120, loss 0.5366.
Test: 2018-07-31T09:13:10.525331: step 2120, loss 0.547894.
Train: 2018-07-31T09:13:10.681544: step 2121, loss 0.510413.
Train: 2018-07-31T09:13:10.837788: step 2122, loss 0.562341.
Train: 2018-07-31T09:13:10.993972: step 2123, loss 0.623702.
Train: 2018-07-31T09:13:11.150185: step 2124, loss 0.49221.
Train: 2018-07-31T09:13:11.306430: step 2125, loss 0.597241.
Train: 2018-07-31T09:13:11.462641: step 2126, loss 0.553754.
Train: 2018-07-31T09:13:11.618824: step 2127, loss 0.571065.
Train: 2018-07-31T09:13:11.775038: step 2128, loss 0.518246.
Train: 2018-07-31T09:13:11.931251: step 2129, loss 0.553519.
Train: 2018-07-31T09:13:12.087490: step 2130, loss 0.588968.
Test: 2018-07-31T09:13:12.321787: step 2130, loss 0.547692.
Train: 2018-07-31T09:13:12.478023: step 2131, loss 0.589142.
Train: 2018-07-31T09:13:12.634212: step 2132, loss 0.589101.
Train: 2018-07-31T09:13:12.790458: step 2133, loss 0.588874.
Train: 2018-07-31T09:13:12.946638: step 2134, loss 0.518272.
Train: 2018-07-31T09:13:13.102852: step 2135, loss 0.58931.
Train: 2018-07-31T09:13:13.259096: step 2136, loss 0.562422.
Train: 2018-07-31T09:13:13.415312: step 2137, loss 0.641679.
Train: 2018-07-31T09:13:13.571523: step 2138, loss 0.553657.
Train: 2018-07-31T09:13:13.727736: step 2139, loss 0.53596.
Train: 2018-07-31T09:13:13.883950: step 2140, loss 0.63199.
Test: 2018-07-31T09:13:14.118240: step 2140, loss 0.547825.
Train: 2018-07-31T09:13:14.274452: step 2141, loss 0.562519.
Train: 2018-07-31T09:13:14.430666: step 2142, loss 0.571101.
Train: 2018-07-31T09:13:14.586910: step 2143, loss 0.501837.
Train: 2018-07-31T09:13:14.727502: step 2144, loss 0.614155.
Train: 2018-07-31T09:13:14.899331: step 2145, loss 0.493044.
Train: 2018-07-31T09:13:15.055552: step 2146, loss 0.493363.
Train: 2018-07-31T09:13:15.211733: step 2147, loss 0.588735.
Train: 2018-07-31T09:13:15.367977: step 2148, loss 0.518943.
Train: 2018-07-31T09:13:15.524190: step 2149, loss 0.562289.
Train: 2018-07-31T09:13:15.680411: step 2150, loss 0.597397.
Test: 2018-07-31T09:13:15.914722: step 2150, loss 0.547907.
Train: 2018-07-31T09:13:16.070933: step 2151, loss 0.544807.
Train: 2018-07-31T09:13:16.227120: step 2152, loss 0.588511.
Train: 2018-07-31T09:13:16.383365: step 2153, loss 0.527538.
Train: 2018-07-31T09:13:16.539580: step 2154, loss 0.588564.
Train: 2018-07-31T09:13:16.695785: step 2155, loss 0.553913.
Train: 2018-07-31T09:13:16.851975: step 2156, loss 0.57098.
Train: 2018-07-31T09:13:17.008218: step 2157, loss 0.545394.
Train: 2018-07-31T09:13:17.164431: step 2158, loss 0.596644.
Train: 2018-07-31T09:13:17.320639: step 2159, loss 0.545422.
Train: 2018-07-31T09:13:17.476858: step 2160, loss 0.562252.
Test: 2018-07-31T09:13:17.711148: step 2160, loss 0.547931.
Train: 2018-07-31T09:13:17.867392: step 2161, loss 0.493719.
Train: 2018-07-31T09:13:18.023605: step 2162, loss 0.510096.
Train: 2018-07-31T09:13:18.179820: step 2163, loss 0.606076.
Train: 2018-07-31T09:13:18.351648: step 2164, loss 0.571097.
Train: 2018-07-31T09:13:18.507836: step 2165, loss 0.536596.
Train: 2018-07-31T09:13:18.648458: step 2166, loss 0.684222.
Train: 2018-07-31T09:13:18.804674: step 2167, loss 0.640776.
Train: 2018-07-31T09:13:18.976502: step 2168, loss 0.544895.
Train: 2018-07-31T09:13:19.132715: step 2169, loss 0.596885.
Train: 2018-07-31T09:13:19.288936: step 2170, loss 0.61406.
Test: 2018-07-31T09:13:19.523250: step 2170, loss 0.548078.
Train: 2018-07-31T09:13:19.679469: step 2171, loss 0.536953.
Train: 2018-07-31T09:13:19.835676: step 2172, loss 0.544925.
Train: 2018-07-31T09:13:19.991895: step 2173, loss 0.630236.
Train: 2018-07-31T09:13:20.148108: step 2174, loss 0.596692.
Train: 2018-07-31T09:13:20.304290: step 2175, loss 0.494986.
Train: 2018-07-31T09:13:20.460504: step 2176, loss 0.528291.
Train: 2018-07-31T09:13:20.601096: step 2177, loss 0.528549.
Train: 2018-07-31T09:13:20.757311: step 2178, loss 0.545734.
Train: 2018-07-31T09:13:20.913523: step 2179, loss 0.587832.
Train: 2018-07-31T09:13:21.069768: step 2180, loss 0.537157.
Test: 2018-07-31T09:13:21.304089: step 2180, loss 0.548327.
Train: 2018-07-31T09:13:21.475916: step 2181, loss 0.596031.
Train: 2018-07-31T09:13:21.632106: step 2182, loss 0.545669.
Train: 2018-07-31T09:13:21.788350: step 2183, loss 0.587326.
Train: 2018-07-31T09:13:21.944532: step 2184, loss 0.520475.
Train: 2018-07-31T09:13:22.100776: step 2185, loss 0.638274.
Train: 2018-07-31T09:13:22.256989: step 2186, loss 0.520444.
Train: 2018-07-31T09:13:22.413204: step 2187, loss 0.494866.
Train: 2018-07-31T09:13:22.569421: step 2188, loss 0.587966.
Train: 2018-07-31T09:13:22.725630: step 2189, loss 0.570727.
Train: 2018-07-31T09:13:22.881843: step 2190, loss 0.545508.
Test: 2018-07-31T09:13:23.131784: step 2190, loss 0.548219.
Train: 2018-07-31T09:13:23.287968: step 2191, loss 0.553925.
Train: 2018-07-31T09:13:23.444181: step 2192, loss 0.477269.
Train: 2018-07-31T09:13:23.600395: step 2193, loss 0.502492.
Train: 2018-07-31T09:13:23.756608: step 2194, loss 0.613611.
Train: 2018-07-31T09:13:23.912821: step 2195, loss 0.50238.
Train: 2018-07-31T09:13:24.069035: step 2196, loss 0.597181.
Train: 2018-07-31T09:13:24.225278: step 2197, loss 0.528072.
Train: 2018-07-31T09:13:24.381497: step 2198, loss 0.509781.
Train: 2018-07-31T09:13:24.537707: step 2199, loss 0.62321.
Train: 2018-07-31T09:13:24.693888: step 2200, loss 0.579704.
Test: 2018-07-31T09:13:24.928241: step 2200, loss 0.547819.
Train: 2018-07-31T09:13:25.662410: step 2201, loss 0.527588.
Train: 2018-07-31T09:13:25.818654: step 2202, loss 0.526833.
Train: 2018-07-31T09:13:25.974868: step 2203, loss 0.589165.
Train: 2018-07-31T09:13:26.131082: step 2204, loss 0.580231.
Train: 2018-07-31T09:13:26.287297: step 2205, loss 0.623996.
Train: 2018-07-31T09:13:26.443513: step 2206, loss 0.553459.
Train: 2018-07-31T09:13:26.599691: step 2207, loss 0.544487.
Train: 2018-07-31T09:13:26.755936: step 2208, loss 0.615155.
Train: 2018-07-31T09:13:26.912118: step 2209, loss 0.535887.
Train: 2018-07-31T09:13:27.068362: step 2210, loss 0.563012.
Test: 2018-07-31T09:13:27.318305: step 2210, loss 0.547793.
Train: 2018-07-31T09:13:27.474486: step 2211, loss 0.579947.
Train: 2018-07-31T09:13:27.630731: step 2212, loss 0.475068.
Train: 2018-07-31T09:13:27.771293: step 2213, loss 0.518828.
Train: 2018-07-31T09:13:27.927506: step 2214, loss 0.562318.
Train: 2018-07-31T09:13:28.083720: step 2215, loss 0.474807.
Train: 2018-07-31T09:13:28.239963: step 2216, loss 0.58921.
Train: 2018-07-31T09:13:28.396176: step 2217, loss 0.501341.
Train: 2018-07-31T09:13:28.536768: step 2218, loss 0.536523.
Train: 2018-07-31T09:13:28.708574: step 2219, loss 0.615503.
Train: 2018-07-31T09:13:28.849164: step 2220, loss 0.571364.
Test: 2018-07-31T09:13:29.099136: step 2220, loss 0.547676.
Train: 2018-07-31T09:13:29.239728: step 2221, loss 0.580444.
Train: 2018-07-31T09:13:29.411533: step 2222, loss 0.52738.
Train: 2018-07-31T09:13:29.567747: step 2223, loss 0.500475.
Train: 2018-07-31T09:13:29.723985: step 2224, loss 0.633612.
Train: 2018-07-31T09:13:29.880204: step 2225, loss 0.500185.
Train: 2018-07-31T09:13:30.052008: step 2226, loss 0.518177.
Train: 2018-07-31T09:13:30.192601: step 2227, loss 0.571561.
Train: 2018-07-31T09:13:30.364435: step 2228, loss 0.544885.
Train: 2018-07-31T09:13:30.505051: step 2229, loss 0.553672.
Train: 2018-07-31T09:13:30.661270: step 2230, loss 0.508777.
Test: 2018-07-31T09:13:30.895594: step 2230, loss 0.547622.
Train: 2018-07-31T09:13:31.067396: step 2231, loss 0.616099.
Train: 2018-07-31T09:13:31.223639: step 2232, loss 0.652687.
Train: 2018-07-31T09:13:31.364231: step 2233, loss 0.517835.
Train: 2018-07-31T09:13:31.520445: step 2234, loss 0.589542.
Train: 2018-07-31T09:13:31.676659: step 2235, loss 0.571687.
Train: 2018-07-31T09:13:31.832842: step 2236, loss 0.500225.
Train: 2018-07-31T09:13:31.989085: step 2237, loss 0.607108.
Train: 2018-07-31T09:13:32.145299: step 2238, loss 0.535859.
Train: 2018-07-31T09:13:32.301513: step 2239, loss 0.562234.
Train: 2018-07-31T09:13:32.442103: step 2240, loss 0.580274.
Test: 2018-07-31T09:13:32.692039: step 2240, loss 0.547691.
Train: 2018-07-31T09:13:32.848229: step 2241, loss 0.570914.
Train: 2018-07-31T09:13:32.988846: step 2242, loss 0.589143.
Train: 2018-07-31T09:13:33.145033: step 2243, loss 0.633691.
Train: 2018-07-31T09:13:33.301279: step 2244, loss 0.579754.
Train: 2018-07-31T09:13:33.457491: step 2245, loss 0.571401.
Train: 2018-07-31T09:13:33.598053: step 2246, loss 0.571136.
Train: 2018-07-31T09:13:33.769889: step 2247, loss 0.554019.
Train: 2018-07-31T09:13:33.926133: step 2248, loss 0.579718.
Train: 2018-07-31T09:13:34.082344: step 2249, loss 0.571261.
Train: 2018-07-31T09:13:34.238529: step 2250, loss 0.562373.
Test: 2018-07-31T09:13:34.472873: step 2250, loss 0.548041.
Train: 2018-07-31T09:13:34.629062: step 2251, loss 0.597122.
Train: 2018-07-31T09:13:34.785276: step 2252, loss 0.553641.
Train: 2018-07-31T09:13:34.957140: step 2253, loss 0.579356.
Train: 2018-07-31T09:13:35.113323: step 2254, loss 0.56228.
Train: 2018-07-31T09:13:35.269536: step 2255, loss 0.647289.
Train: 2018-07-31T09:13:35.410130: step 2256, loss 0.604707.
Train: 2018-07-31T09:13:35.566343: step 2257, loss 0.621405.
Train: 2018-07-31T09:13:35.722585: step 2258, loss 0.495552.
Train: 2018-07-31T09:13:35.878793: step 2259, loss 0.545834.
Train: 2018-07-31T09:13:36.035014: step 2260, loss 0.587583.
Test: 2018-07-31T09:13:36.269336: step 2260, loss 0.548678.
Train: 2018-07-31T09:13:36.425515: step 2261, loss 0.562654.
Train: 2018-07-31T09:13:36.581728: step 2262, loss 0.595653.
Train: 2018-07-31T09:13:36.737942: step 2263, loss 0.554538.
Train: 2018-07-31T09:13:36.894155: step 2264, loss 0.562422.
Train: 2018-07-31T09:13:37.034748: step 2265, loss 0.527467.
Train: 2018-07-31T09:13:37.190961: step 2266, loss 0.546158.
Train: 2018-07-31T09:13:37.347205: step 2267, loss 0.562808.
Train: 2018-07-31T09:13:37.503389: step 2268, loss 0.628587.
Train: 2018-07-31T09:13:37.659602: step 2269, loss 0.562548.
Train: 2018-07-31T09:13:37.815815: step 2270, loss 0.50503.
Test: 2018-07-31T09:13:38.050137: step 2270, loss 0.548863.
Train: 2018-07-31T09:13:38.206378: step 2271, loss 0.603789.
Train: 2018-07-31T09:13:38.362562: step 2272, loss 0.546382.
Train: 2018-07-31T09:13:38.503154: step 2273, loss 0.529904.
Train: 2018-07-31T09:13:38.659400: step 2274, loss 0.537995.
Train: 2018-07-31T09:13:38.815611: step 2275, loss 0.546062.
Train: 2018-07-31T09:13:38.971825: step 2276, loss 0.57084.
Train: 2018-07-31T09:13:39.128008: step 2277, loss 0.529298.
Train: 2018-07-31T09:13:39.284253: step 2278, loss 0.595591.
Train: 2018-07-31T09:13:39.440434: step 2279, loss 0.520747.
Train: 2018-07-31T09:13:39.596678: step 2280, loss 0.562414.
Test: 2018-07-31T09:13:39.830970: step 2280, loss 0.548418.
Train: 2018-07-31T09:13:39.987183: step 2281, loss 0.570956.
Train: 2018-07-31T09:13:40.143395: step 2282, loss 0.596016.
Train: 2018-07-31T09:13:40.299609: step 2283, loss 0.528713.
Train: 2018-07-31T09:13:40.455853: step 2284, loss 0.587874.
Train: 2018-07-31T09:13:40.612060: step 2285, loss 0.579526.
Train: 2018-07-31T09:13:40.752658: step 2286, loss 0.545635.
Train: 2018-07-31T09:13:40.908871: step 2287, loss 0.596292.
Train: 2018-07-31T09:13:41.065054: step 2288, loss 0.511486.
Train: 2018-07-31T09:13:41.236920: step 2289, loss 0.494726.
Train: 2018-07-31T09:13:41.377514: step 2290, loss 0.562462.
Test: 2018-07-31T09:13:41.627453: step 2290, loss 0.5481.
Train: 2018-07-31T09:13:41.783635: step 2291, loss 0.65652.
Train: 2018-07-31T09:13:41.924260: step 2292, loss 0.511473.
Train: 2018-07-31T09:13:42.080471: step 2293, loss 0.562465.
Train: 2018-07-31T09:13:42.236686: step 2294, loss 0.56256.
Train: 2018-07-31T09:13:42.392868: step 2295, loss 0.493721.
Train: 2018-07-31T09:13:42.564733: step 2296, loss 0.622562.
Train: 2018-07-31T09:13:42.720917: step 2297, loss 0.519223.
Train: 2018-07-31T09:13:42.861538: step 2298, loss 0.493455.
Train: 2018-07-31T09:13:43.017723: step 2299, loss 0.579812.
Train: 2018-07-31T09:13:43.173967: step 2300, loss 0.510355.
Test: 2018-07-31T09:13:43.408255: step 2300, loss 0.547855.
Train: 2018-07-31T09:13:44.142460: step 2301, loss 0.588491.
Train: 2018-07-31T09:13:44.298671: step 2302, loss 0.684722.
Train: 2018-07-31T09:13:44.454921: step 2303, loss 0.597148.
Train: 2018-07-31T09:13:44.611129: step 2304, loss 0.492769.
Train: 2018-07-31T09:13:44.751691: step 2305, loss 0.527462.
Train: 2018-07-31T09:13:44.907905: step 2306, loss 0.597295.
Train: 2018-07-31T09:13:45.064148: step 2307, loss 0.501529.
Train: 2018-07-31T09:13:45.235953: step 2308, loss 0.59737.
Train: 2018-07-31T09:13:45.392196: step 2309, loss 0.553783.
Train: 2018-07-31T09:13:45.548416: step 2310, loss 0.536211.
Test: 2018-07-31T09:13:45.782732: step 2310, loss 0.54782.
Train: 2018-07-31T09:13:45.954563: step 2311, loss 0.536156.
Train: 2018-07-31T09:13:46.110783: step 2312, loss 0.466502.
Train: 2018-07-31T09:13:46.266962: step 2313, loss 0.492581.
Train: 2018-07-31T09:13:46.423174: step 2314, loss 0.54487.
Train: 2018-07-31T09:13:46.563791: step 2315, loss 0.553706.
Train: 2018-07-31T09:13:46.720010: step 2316, loss 0.553726.
Train: 2018-07-31T09:13:46.891815: step 2317, loss 0.553718.
Train: 2018-07-31T09:13:47.048028: step 2318, loss 0.616029.
Train: 2018-07-31T09:13:47.204242: step 2319, loss 0.669419.
Train: 2018-07-31T09:13:47.360479: step 2320, loss 0.553663.
Test: 2018-07-31T09:13:47.594775: step 2320, loss 0.547654.
Train: 2018-07-31T09:13:47.750988: step 2321, loss 0.571471.
Train: 2018-07-31T09:13:47.907202: step 2322, loss 0.535829.
Train: 2018-07-31T09:13:48.047793: step 2323, loss 0.616165.
Train: 2018-07-31T09:13:48.219629: step 2324, loss 0.518212.
Train: 2018-07-31T09:13:48.360221: step 2325, loss 0.562585.
Train: 2018-07-31T09:13:48.516467: step 2326, loss 0.571458.
Train: 2018-07-31T09:13:48.672679: step 2327, loss 0.509606.
Train: 2018-07-31T09:13:48.828862: step 2328, loss 0.491872.
Train: 2018-07-31T09:13:48.985104: step 2329, loss 0.474182.
Train: 2018-07-31T09:13:49.141289: step 2330, loss 0.562477.
Test: 2018-07-31T09:13:49.375639: step 2330, loss 0.547671.
Train: 2018-07-31T09:13:49.531821: step 2331, loss 0.562519.
Train: 2018-07-31T09:13:49.688034: step 2332, loss 0.607124.
Train: 2018-07-31T09:13:49.828657: step 2333, loss 0.571311.
Train: 2018-07-31T09:13:49.984842: step 2334, loss 0.500327.
Train: 2018-07-31T09:13:50.141085: step 2335, loss 0.508969.
Train: 2018-07-31T09:13:50.297268: step 2336, loss 0.508993.
Train: 2018-07-31T09:13:50.453481: step 2337, loss 0.544904.
Train: 2018-07-31T09:13:50.609695: step 2338, loss 0.481741.
Train: 2018-07-31T09:13:50.750311: step 2339, loss 0.5626.
Train: 2018-07-31T09:13:50.906532: step 2340, loss 0.553751.
Test: 2018-07-31T09:13:51.140820: step 2340, loss 0.547586.
Train: 2018-07-31T09:13:51.312685: step 2341, loss 0.590193.
Train: 2018-07-31T09:13:51.468868: step 2342, loss 0.608039.
Train: 2018-07-31T09:13:51.625112: step 2343, loss 0.580684.
Train: 2018-07-31T09:13:51.781296: step 2344, loss 0.571989.
Train: 2018-07-31T09:13:51.937507: step 2345, loss 0.626379.
Train: 2018-07-31T09:13:52.093751: step 2346, loss 0.625953.
Train: 2018-07-31T09:13:52.249961: step 2347, loss 0.562637.
Train: 2018-07-31T09:13:52.421769: step 2348, loss 0.598751.
Train: 2018-07-31T09:13:52.577983: step 2349, loss 0.517641.
Train: 2018-07-31T09:13:52.734227: step 2350, loss 0.642963.
Test: 2018-07-31T09:13:52.968542: step 2350, loss 0.54767.
Train: 2018-07-31T09:13:53.124730: step 2351, loss 0.562446.
Train: 2018-07-31T09:13:53.296564: step 2352, loss 0.562463.
Train: 2018-07-31T09:13:53.452814: step 2353, loss 0.580165.
Train: 2018-07-31T09:13:53.609022: step 2354, loss 0.571291.
Train: 2018-07-31T09:13:53.765205: step 2355, loss 0.431232.
Train: 2018-07-31T09:13:53.905827: step 2356, loss 0.588481.
Train: 2018-07-31T09:13:54.062010: step 2357, loss 0.562494.
Train: 2018-07-31T09:13:54.218225: step 2358, loss 0.588496.
Train: 2018-07-31T09:13:54.390089: step 2359, loss 0.623039.
Train: 2018-07-31T09:13:54.546274: step 2360, loss 0.536473.
Test: 2018-07-31T09:13:54.780592: step 2360, loss 0.547928.
Train: 2018-07-31T09:13:54.936836: step 2361, loss 0.562272.
Train: 2018-07-31T09:13:55.093049: step 2362, loss 0.501923.
Train: 2018-07-31T09:13:55.249269: step 2363, loss 0.588205.
Train: 2018-07-31T09:13:55.405479: step 2364, loss 0.501992.
Train: 2018-07-31T09:13:55.561689: step 2365, loss 0.55364.
Train: 2018-07-31T09:13:55.717904: step 2366, loss 0.502221.
Train: 2018-07-31T09:13:55.874087: step 2367, loss 0.67459.
Train: 2018-07-31T09:13:56.030332: step 2368, loss 0.622732.
Train: 2018-07-31T09:13:56.186513: step 2369, loss 0.571028.
Train: 2018-07-31T09:13:56.342726: step 2370, loss 0.588329.
Test: 2018-07-31T09:13:56.577078: step 2370, loss 0.548046.
Train: 2018-07-31T09:13:56.748912: step 2371, loss 0.52807.
Train: 2018-07-31T09:13:56.905127: step 2372, loss 0.571046.
Train: 2018-07-31T09:13:57.045719: step 2373, loss 0.622045.
Train: 2018-07-31T09:13:57.201900: step 2374, loss 0.502704.
Train: 2018-07-31T09:13:57.358144: step 2375, loss 0.622144.
Train: 2018-07-31T09:13:57.529973: step 2376, loss 0.562307.
Train: 2018-07-31T09:13:57.686161: step 2377, loss 0.579221.
Train: 2018-07-31T09:13:57.842376: step 2378, loss 0.680748.
Train: 2018-07-31T09:13:57.998619: step 2379, loss 0.562338.
Train: 2018-07-31T09:13:58.154834: step 2380, loss 0.529159.
Test: 2018-07-31T09:13:58.389123: step 2380, loss 0.548453.
Train: 2018-07-31T09:13:58.545336: step 2381, loss 0.579274.
Train: 2018-07-31T09:13:58.701550: step 2382, loss 0.587459.
Train: 2018-07-31T09:13:58.857763: step 2383, loss 0.596019.
Train: 2018-07-31T09:13:59.014006: step 2384, loss 0.545701.
Train: 2018-07-31T09:13:59.170190: step 2385, loss 0.587403.
Train: 2018-07-31T09:13:59.326434: step 2386, loss 0.554386.
Train: 2018-07-31T09:13:59.498268: step 2387, loss 0.587241.
Train: 2018-07-31T09:13:59.654481: step 2388, loss 0.546093.
Train: 2018-07-31T09:13:59.810695: step 2389, loss 0.521424.
Train: 2018-07-31T09:13:59.966904: step 2390, loss 0.52979.
Test: 2018-07-31T09:14:00.216844: step 2390, loss 0.548825.
Train: 2018-07-31T09:14:00.373063: step 2391, loss 0.562581.
Train: 2018-07-31T09:14:00.529274: step 2392, loss 0.579024.
Train: 2018-07-31T09:14:00.701081: step 2393, loss 0.595831.
Train: 2018-07-31T09:14:00.857294: step 2394, loss 0.529689.
Train: 2018-07-31T09:14:00.997916: step 2395, loss 0.521321.
Train: 2018-07-31T09:14:01.169729: step 2396, loss 0.628695.
Train: 2018-07-31T09:14:01.325935: step 2397, loss 0.612273.
Train: 2018-07-31T09:14:01.482148: step 2398, loss 0.488137.
Train: 2018-07-31T09:14:01.622741: step 2399, loss 0.579256.
Train: 2018-07-31T09:14:01.794575: step 2400, loss 0.579485.
Test: 2018-07-31T09:14:02.028923: step 2400, loss 0.54863.
Train: 2018-07-31T09:14:02.778749: step 2401, loss 0.545775.
Train: 2018-07-31T09:14:02.934932: step 2402, loss 0.578967.
Train: 2018-07-31T09:14:03.091176: step 2403, loss 0.629324.
Train: 2018-07-31T09:14:03.231774: step 2404, loss 0.545765.
Train: 2018-07-31T09:14:03.403572: step 2405, loss 0.620891.
Train: 2018-07-31T09:14:03.559787: step 2406, loss 0.546012.
Train: 2018-07-31T09:14:03.716001: step 2407, loss 0.562516.
Train: 2018-07-31T09:14:03.872214: step 2408, loss 0.620681.
Train: 2018-07-31T09:14:04.028428: step 2409, loss 0.587206.
Train: 2018-07-31T09:14:04.184671: step 2410, loss 0.59566.
Test: 2018-07-31T09:14:04.418993: step 2410, loss 0.548755.
Train: 2018-07-31T09:14:04.575204: step 2411, loss 0.537833.
Train: 2018-07-31T09:14:04.731388: step 2412, loss 0.496749.
Train: 2018-07-31T09:14:04.887600: step 2413, loss 0.529626.
Train: 2018-07-31T09:14:05.028193: step 2414, loss 0.537884.
Train: 2018-07-31T09:14:05.184406: step 2415, loss 0.578926.
Train: 2018-07-31T09:14:05.356240: step 2416, loss 0.49171.
Train: 2018-07-31T09:14:05.512453: step 2417, loss 0.529181.
Train: 2018-07-31T09:14:05.668668: step 2418, loss 0.58743.
Train: 2018-07-31T09:14:05.824911: step 2419, loss 0.570844.
Train: 2018-07-31T09:14:05.981125: step 2420, loss 0.638186.
Test: 2018-07-31T09:14:06.215414: step 2420, loss 0.548365.
Train: 2018-07-31T09:14:06.371628: step 2421, loss 0.638392.
Train: 2018-07-31T09:14:06.527871: step 2422, loss 0.58763.
Train: 2018-07-31T09:14:06.684085: step 2423, loss 0.503649.
Train: 2018-07-31T09:14:06.840269: step 2424, loss 0.587593.
Train: 2018-07-31T09:14:06.996512: step 2425, loss 0.587573.
Train: 2018-07-31T09:14:07.152695: step 2426, loss 0.596156.
Train: 2018-07-31T09:14:07.308939: step 2427, loss 0.528954.
Train: 2018-07-31T09:14:07.465121: step 2428, loss 0.553956.
Train: 2018-07-31T09:14:07.621337: step 2429, loss 0.64613.
Train: 2018-07-31T09:14:07.777549: step 2430, loss 0.612772.
Test: 2018-07-31T09:14:08.011895: step 2430, loss 0.548517.
Train: 2018-07-31T09:14:08.168082: step 2431, loss 0.537628.
Train: 2018-07-31T09:14:08.324327: step 2432, loss 0.59587.
Train: 2018-07-31T09:14:08.480508: step 2433, loss 0.470982.
Train: 2018-07-31T09:14:08.621102: step 2434, loss 0.63736.
Train: 2018-07-31T09:14:08.777315: step 2435, loss 0.587634.
Train: 2018-07-31T09:14:08.933559: step 2436, loss 0.53757.
Train: 2018-07-31T09:14:09.089772: step 2437, loss 0.545873.
Train: 2018-07-31T09:14:09.245985: step 2438, loss 0.645655.
Train: 2018-07-31T09:14:09.402169: step 2439, loss 0.653626.
Train: 2018-07-31T09:14:09.542760: step 2440, loss 0.537747.
Test: 2018-07-31T09:14:09.792735: step 2440, loss 0.548815.
Train: 2018-07-31T09:14:09.948915: step 2441, loss 0.529752.
Train: 2018-07-31T09:14:10.105159: step 2442, loss 0.537951.
Train: 2018-07-31T09:14:10.261374: step 2443, loss 0.661205.
Train: 2018-07-31T09:14:10.417556: step 2444, loss 0.472367.
Train: 2018-07-31T09:14:10.573799: step 2445, loss 0.611708.
Train: 2018-07-31T09:14:10.730012: step 2446, loss 0.55448.
Train: 2018-07-31T09:14:10.886195: step 2447, loss 0.570849.
Train: 2018-07-31T09:14:11.042441: step 2448, loss 0.562703.
Train: 2018-07-31T09:14:11.198656: step 2449, loss 0.52169.
Train: 2018-07-31T09:14:11.354866: step 2450, loss 0.505243.
Test: 2018-07-31T09:14:11.589156: step 2450, loss 0.548825.
Train: 2018-07-31T09:14:11.745401: step 2451, loss 0.480442.
Train: 2018-07-31T09:14:11.917236: step 2452, loss 0.529529.
Train: 2018-07-31T09:14:12.073449: step 2453, loss 0.529266.
Train: 2018-07-31T09:14:12.229631: step 2454, loss 0.570705.
Train: 2018-07-31T09:14:12.385875: step 2455, loss 0.470171.
Train: 2018-07-31T09:14:12.542082: step 2456, loss 0.587717.
Train: 2018-07-31T09:14:12.698272: step 2457, loss 0.579565.
Train: 2018-07-31T09:14:12.854514: step 2458, loss 0.528325.
Train: 2018-07-31T09:14:13.010729: step 2459, loss 0.605285.
Train: 2018-07-31T09:14:13.166944: step 2460, loss 0.528224.
Test: 2018-07-31T09:14:13.401232: step 2460, loss 0.547968.
Train: 2018-07-31T09:14:13.541825: step 2461, loss 0.536422.
Train: 2018-07-31T09:14:13.713659: step 2462, loss 0.493288.
Train: 2018-07-31T09:14:13.869903: step 2463, loss 0.527998.
Train: 2018-07-31T09:14:14.026118: step 2464, loss 0.553716.
Train: 2018-07-31T09:14:14.182330: step 2465, loss 0.623733.
Train: 2018-07-31T09:14:14.338543: step 2466, loss 0.518543.
Train: 2018-07-31T09:14:14.494725: step 2467, loss 0.536074.
Train: 2018-07-31T09:14:14.650940: step 2468, loss 0.518062.
Train: 2018-07-31T09:14:14.807153: step 2469, loss 0.544689.
Train: 2018-07-31T09:14:14.947746: step 2470, loss 0.473256.
Test: 2018-07-31T09:14:15.182066: step 2470, loss 0.547618.
Train: 2018-07-31T09:14:15.353926: step 2471, loss 0.571629.
Train: 2018-07-31T09:14:15.510143: step 2472, loss 0.526821.
Train: 2018-07-31T09:14:15.650737: step 2473, loss 0.490256.
Train: 2018-07-31T09:14:15.806919: step 2474, loss 0.580898.
Train: 2018-07-31T09:14:15.978753: step 2475, loss 0.517339.
Train: 2018-07-31T09:14:16.134966: step 2476, loss 0.553711.
Train: 2018-07-31T09:14:16.275560: step 2477, loss 0.471312.
Train: 2018-07-31T09:14:16.447393: step 2478, loss 0.535284.
Train: 2018-07-31T09:14:16.588016: step 2479, loss 0.609239.
Train: 2018-07-31T09:14:16.744229: step 2480, loss 0.525679.
Test: 2018-07-31T09:14:16.994140: step 2480, loss 0.547618.
Train: 2018-07-31T09:14:17.150353: step 2481, loss 0.600261.
Train: 2018-07-31T09:14:17.306597: step 2482, loss 0.591463.
Train: 2018-07-31T09:14:17.462780: step 2483, loss 0.665618.
Train: 2018-07-31T09:14:17.618994: step 2484, loss 0.516625.
Train: 2018-07-31T09:14:17.759586: step 2485, loss 0.544856.
Train: 2018-07-31T09:14:17.915831: step 2486, loss 0.526207.
Train: 2018-07-31T09:14:18.072014: step 2487, loss 0.562999.
Train: 2018-07-31T09:14:18.228258: step 2488, loss 0.553481.
Train: 2018-07-31T09:14:18.384470: step 2489, loss 0.544807.
Train: 2018-07-31T09:14:18.540685: step 2490, loss 0.590704.
Test: 2018-07-31T09:14:18.775002: step 2490, loss 0.547586.
Train: 2018-07-31T09:14:18.946842: step 2491, loss 0.56271.
Train: 2018-07-31T09:14:19.103022: step 2492, loss 0.6088.
Train: 2018-07-31T09:14:19.259235: step 2493, loss 0.498944.
Train: 2018-07-31T09:14:19.415449: step 2494, loss 0.617759.
Train: 2018-07-31T09:14:19.571686: step 2495, loss 0.544683.
Train: 2018-07-31T09:14:19.712254: step 2496, loss 0.590333.
Train: 2018-07-31T09:14:19.884119: step 2497, loss 0.590131.
Train: 2018-07-31T09:14:20.040303: step 2498, loss 0.526706.
Train: 2018-07-31T09:14:20.196516: step 2499, loss 0.526489.
Train: 2018-07-31T09:14:20.352759: step 2500, loss 0.544728.
Test: 2018-07-31T09:14:20.587080: step 2500, loss 0.547608.
Train: 2018-07-31T09:14:21.321283: step 2501, loss 0.598464.
Train: 2018-07-31T09:14:21.493087: step 2502, loss 0.553527.
Train: 2018-07-31T09:14:21.649300: step 2503, loss 0.580723.
Train: 2018-07-31T09:14:21.805544: step 2504, loss 0.500278.
Train: 2018-07-31T09:14:21.977348: step 2505, loss 0.633859.
Train: 2018-07-31T09:14:22.133592: step 2506, loss 0.571276.
Train: 2018-07-31T09:14:22.289807: step 2507, loss 0.500552.
Train: 2018-07-31T09:14:22.445988: step 2508, loss 0.64221.
Train: 2018-07-31T09:14:22.602202: step 2509, loss 0.588766.
Train: 2018-07-31T09:14:22.758442: step 2510, loss 0.580141.
Test: 2018-07-31T09:14:22.992736: step 2510, loss 0.547787.
Train: 2018-07-31T09:14:23.195844: step 2511, loss 0.606136.
Train: 2018-07-31T09:14:23.367648: step 2512, loss 0.510038.
Train: 2018-07-31T09:14:23.508240: step 2513, loss 0.562309.
Train: 2018-07-31T09:14:23.664486: step 2514, loss 0.536764.
Train: 2018-07-31T09:14:23.820697: step 2515, loss 0.588444.
Train: 2018-07-31T09:14:23.992532: step 2516, loss 0.50179.
Train: 2018-07-31T09:14:24.148714: step 2517, loss 0.527944.
Train: 2018-07-31T09:14:24.304929: step 2518, loss 0.562406.
Train: 2018-07-31T09:14:24.461141: step 2519, loss 0.579721.
Train: 2018-07-31T09:14:24.617355: step 2520, loss 0.579645.
Test: 2018-07-31T09:14:24.851706: step 2520, loss 0.54797.
Train: 2018-07-31T09:14:25.007888: step 2521, loss 0.536615.
Train: 2018-07-31T09:14:25.164134: step 2522, loss 0.501967.
Train: 2018-07-31T09:14:25.320346: step 2523, loss 0.622922.
Train: 2018-07-31T09:14:25.476530: step 2524, loss 0.605345.
Train: 2018-07-31T09:14:25.632743: step 2525, loss 0.510774.
Train: 2018-07-31T09:14:25.788956: step 2526, loss 0.571144.
Train: 2018-07-31T09:14:25.945170: step 2527, loss 0.614078.
Train: 2018-07-31T09:14:26.101414: step 2528, loss 0.53674.
Train: 2018-07-31T09:14:26.241976: step 2529, loss 0.639547.
Train: 2018-07-31T09:14:26.413810: step 2530, loss 0.502335.
Test: 2018-07-31T09:14:26.648163: step 2530, loss 0.548053.
Train: 2018-07-31T09:14:26.804368: step 2531, loss 0.562502.
Train: 2018-07-31T09:14:26.960587: step 2532, loss 0.49389.
Train: 2018-07-31T09:14:27.116771: step 2533, loss 0.596824.
Train: 2018-07-31T09:14:27.272984: step 2534, loss 0.622394.
Train: 2018-07-31T09:14:27.429196: step 2535, loss 0.630875.
Train: 2018-07-31T09:14:27.585410: step 2536, loss 0.519573.
Train: 2018-07-31T09:14:27.741623: step 2537, loss 0.545311.
Train: 2018-07-31T09:14:27.897837: step 2538, loss 0.553879.
Train: 2018-07-31T09:14:28.054080: step 2539, loss 0.570836.
Train: 2018-07-31T09:14:28.210264: step 2540, loss 0.57097.
Test: 2018-07-31T09:14:28.444615: step 2540, loss 0.548168.
Train: 2018-07-31T09:14:28.600828: step 2541, loss 0.528268.
Train: 2018-07-31T09:14:28.757012: step 2542, loss 0.604872.
Train: 2018-07-31T09:14:28.913256: step 2543, loss 0.579432.
Train: 2018-07-31T09:14:29.069437: step 2544, loss 0.579352.
Train: 2018-07-31T09:14:29.225651: step 2545, loss 0.604691.
Train: 2018-07-31T09:14:29.366244: step 2546, loss 0.494599.
Train: 2018-07-31T09:14:29.522457: step 2547, loss 0.545578.
Train: 2018-07-31T09:14:29.678670: step 2548, loss 0.562584.
Train: 2018-07-31T09:14:29.834916: step 2549, loss 0.680981.
Train: 2018-07-31T09:14:29.991127: step 2550, loss 0.638418.
Test: 2018-07-31T09:14:30.225449: step 2550, loss 0.548341.
Train: 2018-07-31T09:14:30.397254: step 2551, loss 0.604526.
Train: 2018-07-31T09:14:30.553465: step 2552, loss 0.621076.
Train: 2018-07-31T09:14:30.709709: step 2553, loss 0.562527.
Train: 2018-07-31T09:14:30.865922: step 2554, loss 0.479286.
Train: 2018-07-31T09:14:31.006484: step 2555, loss 0.57079.
Train: 2018-07-31T09:14:31.162729: step 2556, loss 0.562489.
Train: 2018-07-31T09:14:31.318936: step 2557, loss 0.52133.
Train: 2018-07-31T09:14:31.475155: step 2558, loss 0.545891.
Train: 2018-07-31T09:14:31.631337: step 2559, loss 0.570904.
Train: 2018-07-31T09:14:31.787552: step 2560, loss 0.537854.
Test: 2018-07-31T09:14:32.021873: step 2560, loss 0.548659.
Train: 2018-07-31T09:14:32.178085: step 2561, loss 0.579155.
Train: 2018-07-31T09:14:32.334299: step 2562, loss 0.579296.
Train: 2018-07-31T09:14:32.490512: step 2563, loss 0.512769.
Train: 2018-07-31T09:14:32.631104: step 2564, loss 0.570909.
Train: 2018-07-31T09:14:32.787349: step 2565, loss 0.487546.
Train: 2018-07-31T09:14:32.959182: step 2566, loss 0.579113.
Train: 2018-07-31T09:14:33.099774: step 2567, loss 0.616078.
Train: 2018-07-31T09:14:33.271580: step 2568, loss 0.537302.
Train: 2018-07-31T09:14:33.412202: step 2569, loss 0.469927.
Train: 2018-07-31T09:14:33.568414: step 2570, loss 0.62141.
Test: 2018-07-31T09:14:33.802717: step 2570, loss 0.548284.
Train: 2018-07-31T09:14:33.958942: step 2571, loss 0.613231.
Train: 2018-07-31T09:14:34.115162: step 2572, loss 0.587645.
Train: 2018-07-31T09:14:34.271375: step 2573, loss 0.537191.
Train: 2018-07-31T09:14:34.427584: step 2574, loss 0.587662.
Train: 2018-07-31T09:14:34.583801: step 2575, loss 0.604746.
Train: 2018-07-31T09:14:34.739984: step 2576, loss 0.579408.
Train: 2018-07-31T09:14:34.896231: step 2577, loss 0.537051.
Train: 2018-07-31T09:14:35.052442: step 2578, loss 0.46946.
Train: 2018-07-31T09:14:35.208659: step 2579, loss 0.545573.
Train: 2018-07-31T09:14:35.364839: step 2580, loss 0.553965.
Test: 2018-07-31T09:14:35.614810: step 2580, loss 0.548154.
Train: 2018-07-31T09:14:35.771026: step 2581, loss 0.587802.
Train: 2018-07-31T09:14:35.927209: step 2582, loss 0.562516.
Train: 2018-07-31T09:14:36.083420: step 2583, loss 0.502766.
Train: 2018-07-31T09:14:36.239664: step 2584, loss 0.545265.
Train: 2018-07-31T09:14:36.395880: step 2585, loss 0.648354.
Train: 2018-07-31T09:14:36.552061: step 2586, loss 0.536496.
Train: 2018-07-31T09:14:36.708304: step 2587, loss 0.579508.
Train: 2018-07-31T09:14:36.864512: step 2588, loss 0.545175.
Train: 2018-07-31T09:14:37.020701: step 2589, loss 0.536661.
Train: 2018-07-31T09:14:37.176914: step 2590, loss 0.519193.
Test: 2018-07-31T09:14:37.411234: step 2590, loss 0.547946.
Train: 2018-07-31T09:14:37.567473: step 2591, loss 0.519216.
Train: 2018-07-31T09:14:37.723691: step 2592, loss 0.570861.
Train: 2018-07-31T09:14:37.879905: step 2593, loss 0.623047.
Train: 2018-07-31T09:14:38.036089: step 2594, loss 0.570985.
Train: 2018-07-31T09:14:38.192302: step 2595, loss 0.510402.
Train: 2018-07-31T09:14:38.348545: step 2596, loss 0.597125.
Train: 2018-07-31T09:14:38.504758: step 2597, loss 0.518843.
Train: 2018-07-31T09:14:38.660974: step 2598, loss 0.51888.
Train: 2018-07-31T09:14:38.817155: step 2599, loss 0.56262.
Train: 2018-07-31T09:14:38.973399: step 2600, loss 0.466313.
Test: 2018-07-31T09:14:39.207689: step 2600, loss 0.547764.
Train: 2018-07-31T09:14:39.910681: step 2601, loss 0.571303.
Train: 2018-07-31T09:14:40.066863: step 2602, loss 0.536311.
Train: 2018-07-31T09:14:40.223076: step 2603, loss 0.580175.
Train: 2018-07-31T09:14:40.379320: step 2604, loss 0.580207.
Train: 2018-07-31T09:14:40.535502: step 2605, loss 0.571151.
Train: 2018-07-31T09:14:40.707337: step 2606, loss 0.642584.
Train: 2018-07-31T09:14:40.847961: step 2607, loss 0.633227.
Train: 2018-07-31T09:14:41.004143: step 2608, loss 0.518241.
Train: 2018-07-31T09:14:41.160357: step 2609, loss 0.439288.
Train: 2018-07-31T09:14:41.316570: step 2610, loss 0.509424.
Test: 2018-07-31T09:14:41.550915: step 2610, loss 0.547698.
Train: 2018-07-31T09:14:41.722750: step 2611, loss 0.544816.
Train: 2018-07-31T09:14:41.878939: step 2612, loss 0.535752.
Train: 2018-07-31T09:14:42.035183: step 2613, loss 0.588992.
Train: 2018-07-31T09:14:42.191395: step 2614, loss 0.642408.
Train: 2018-07-31T09:14:42.331988: step 2615, loss 0.473687.
Train: 2018-07-31T09:14:42.503822: step 2616, loss 0.562525.
Train: 2018-07-31T09:14:42.644384: step 2617, loss 0.678516.
Train: 2018-07-31T09:14:42.816244: step 2618, loss 0.624788.
Train: 2018-07-31T09:14:42.956842: step 2619, loss 0.509341.
Train: 2018-07-31T09:14:43.113025: step 2620, loss 0.518315.
Test: 2018-07-31T09:14:43.362992: step 2620, loss 0.547707.
Train: 2018-07-31T09:14:43.519180: step 2621, loss 0.597935.
Train: 2018-07-31T09:14:43.675423: step 2622, loss 0.571388.
Train: 2018-07-31T09:14:43.831605: step 2623, loss 0.571349.
Train: 2018-07-31T09:14:43.987850: step 2624, loss 0.544906.
Train: 2018-07-31T09:14:44.144065: step 2625, loss 0.509742.
Train: 2018-07-31T09:14:44.300247: step 2626, loss 0.562519.
Train: 2018-07-31T09:14:44.456495: step 2627, loss 0.51856.
Train: 2018-07-31T09:14:44.612707: step 2628, loss 0.527528.
Train: 2018-07-31T09:14:44.768887: step 2629, loss 0.492241.
Train: 2018-07-31T09:14:44.925129: step 2630, loss 0.588889.
Test: 2018-07-31T09:14:45.159420: step 2630, loss 0.547733.
Train: 2018-07-31T09:14:45.315663: step 2631, loss 0.685696.
Train: 2018-07-31T09:14:45.471880: step 2632, loss 0.580102.
Train: 2018-07-31T09:14:45.628084: step 2633, loss 0.544846.
Train: 2018-07-31T09:14:45.784306: step 2634, loss 0.579874.
Train: 2018-07-31T09:14:45.940517: step 2635, loss 0.536244.
Train: 2018-07-31T09:14:46.096731: step 2636, loss 0.562392.
Train: 2018-07-31T09:14:46.252944: step 2637, loss 0.545052.
Train: 2018-07-31T09:14:46.409159: step 2638, loss 0.614637.
Train: 2018-07-31T09:14:46.565342: step 2639, loss 0.562457.
Train: 2018-07-31T09:14:46.721584: step 2640, loss 0.553652.
Test: 2018-07-31T09:14:46.955905: step 2640, loss 0.547871.
Train: 2018-07-31T09:14:47.112118: step 2641, loss 0.597381.
Train: 2018-07-31T09:14:47.283922: step 2642, loss 0.536494.
Train: 2018-07-31T09:14:47.440135: step 2643, loss 0.527725.
Train: 2018-07-31T09:14:47.596350: step 2644, loss 0.48463.
Train: 2018-07-31T09:14:47.752563: step 2645, loss 0.536256.
Train: 2018-07-31T09:14:47.908776: step 2646, loss 0.666473.
Train: 2018-07-31T09:14:48.049368: step 2647, loss 0.545262.
Train: 2018-07-31T09:14:48.205618: step 2648, loss 0.597205.
Train: 2018-07-31T09:14:48.361796: step 2649, loss 0.545119.
Train: 2018-07-31T09:14:48.518038: step 2650, loss 0.536443.
Test: 2018-07-31T09:14:48.752362: step 2650, loss 0.547931.
Train: 2018-07-31T09:14:48.908567: step 2651, loss 0.605592.
Train: 2018-07-31T09:14:49.064780: step 2652, loss 0.501972.
Train: 2018-07-31T09:14:49.220999: step 2653, loss 0.562469.
Train: 2018-07-31T09:14:49.377183: step 2654, loss 0.57117.
Train: 2018-07-31T09:14:49.533396: step 2655, loss 0.553737.
Train: 2018-07-31T09:14:49.689639: step 2656, loss 0.536441.
Train: 2018-07-31T09:14:49.845823: step 2657, loss 0.596921.
Train: 2018-07-31T09:14:49.986445: step 2658, loss 0.493252.
Train: 2018-07-31T09:14:50.142627: step 2659, loss 0.519207.
Train: 2018-07-31T09:14:50.298841: step 2660, loss 0.623059.
Test: 2018-07-31T09:14:50.533195: step 2660, loss 0.547892.
Train: 2018-07-31T09:14:50.704997: step 2661, loss 0.518934.
Train: 2018-07-31T09:14:50.861234: step 2662, loss 0.553707.
Train: 2018-07-31T09:14:51.017455: step 2663, loss 0.536463.
Train: 2018-07-31T09:14:51.173672: step 2664, loss 0.51886.
Train: 2018-07-31T09:14:51.314229: step 2665, loss 0.588504.
Train: 2018-07-31T09:14:51.470472: step 2666, loss 0.614782.
Train: 2018-07-31T09:14:51.626685: step 2667, loss 0.59736.
Train: 2018-07-31T09:14:51.782868: step 2668, loss 0.518799.
Train: 2018-07-31T09:14:51.939083: step 2669, loss 0.614914.
Train: 2018-07-31T09:14:52.095326: step 2670, loss 0.518712.
Test: 2018-07-31T09:14:52.329616: step 2670, loss 0.547819.
Train: 2018-07-31T09:14:52.485829: step 2671, loss 0.58852.
Train: 2018-07-31T09:14:52.657663: step 2672, loss 0.510052.
Train: 2018-07-31T09:14:52.798287: step 2673, loss 0.562522.
Train: 2018-07-31T09:14:52.954499: step 2674, loss 0.56235.
Train: 2018-07-31T09:14:53.110713: step 2675, loss 0.562542.
Train: 2018-07-31T09:14:53.266926: step 2676, loss 0.536298.
Train: 2018-07-31T09:14:53.423135: step 2677, loss 0.536239.
Train: 2018-07-31T09:14:53.579324: step 2678, loss 0.544996.
Train: 2018-07-31T09:14:53.735536: step 2679, loss 0.588764.
Train: 2018-07-31T09:14:53.891783: step 2680, loss 0.571238.
Test: 2018-07-31T09:14:54.126072: step 2680, loss 0.547786.
Train: 2018-07-31T09:14:54.282286: step 2681, loss 0.632669.
Train: 2018-07-31T09:14:54.438497: step 2682, loss 0.518765.
Train: 2018-07-31T09:14:54.594741: step 2683, loss 0.588747.
Train: 2018-07-31T09:14:54.750957: step 2684, loss 0.579938.
Train: 2018-07-31T09:14:54.907169: step 2685, loss 0.614688.
Train: 2018-07-31T09:14:55.047759: step 2686, loss 0.562456.
Train: 2018-07-31T09:14:55.203973: step 2687, loss 0.492988.
Train: 2018-07-31T09:14:55.360157: step 2688, loss 0.544924.
Train: 2018-07-31T09:14:55.516399: step 2689, loss 0.605872.
Train: 2018-07-31T09:14:55.672584: step 2690, loss 0.6143.
Test: 2018-07-31T09:14:55.922555: step 2690, loss 0.547946.
Train: 2018-07-31T09:14:56.078768: step 2691, loss 0.588192.
Train: 2018-07-31T09:14:56.234981: step 2692, loss 0.596869.
Train: 2018-07-31T09:14:56.391164: step 2693, loss 0.553798.
Train: 2018-07-31T09:14:56.547408: step 2694, loss 0.51098.
Train: 2018-07-31T09:14:56.703591: step 2695, loss 0.596708.
Train: 2018-07-31T09:14:56.875426: step 2696, loss 0.596465.
Train: 2018-07-31T09:14:57.031639: step 2697, loss 0.58804.
Train: 2018-07-31T09:14:57.187884: step 2698, loss 0.587939.
Train: 2018-07-31T09:14:57.344096: step 2699, loss 0.58776.
Train: 2018-07-31T09:14:57.500310: step 2700, loss 0.562431.
Test: 2018-07-31T09:14:57.734631: step 2700, loss 0.548333.
Train: 2018-07-31T09:14:58.453182: step 2701, loss 0.579174.
Train: 2018-07-31T09:14:58.609395: step 2702, loss 0.587676.
Train: 2018-07-31T09:14:58.765610: step 2703, loss 0.612659.
Train: 2018-07-31T09:14:58.921821: step 2704, loss 0.587577.
Train: 2018-07-31T09:14:59.062414: step 2705, loss 0.587473.
Train: 2018-07-31T09:14:59.218628: step 2706, loss 0.521059.
Train: 2018-07-31T09:14:59.374842: step 2707, loss 0.554341.
Train: 2018-07-31T09:14:59.531054: step 2708, loss 0.579112.
Train: 2018-07-31T09:14:59.687267: step 2709, loss 0.570812.
Train: 2018-07-31T09:14:59.843512: step 2710, loss 0.554366.
Test: 2018-07-31T09:15:00.077802: step 2710, loss 0.548844.
Train: 2018-07-31T09:15:00.234040: step 2711, loss 0.49689.
Train: 2018-07-31T09:15:00.390260: step 2712, loss 0.546169.
Train: 2018-07-31T09:15:00.546472: step 2713, loss 0.620151.
Train: 2018-07-31T09:15:00.702655: step 2714, loss 0.611915.
Train: 2018-07-31T09:15:00.858899: step 2715, loss 0.529701.
Train: 2018-07-31T09:15:01.015082: step 2716, loss 0.595484.
Train: 2018-07-31T09:15:01.171326: step 2717, loss 0.554346.
Train: 2018-07-31T09:15:01.311923: step 2718, loss 0.422257.
Train: 2018-07-31T09:15:01.468102: step 2719, loss 0.521265.
Train: 2018-07-31T09:15:01.624344: step 2720, loss 0.662015.
Test: 2018-07-31T09:15:01.858671: step 2720, loss 0.548608.
Train: 2018-07-31T09:15:02.014848: step 2721, loss 0.479506.
Train: 2018-07-31T09:15:02.171060: step 2722, loss 0.537505.
Train: 2018-07-31T09:15:02.327275: step 2723, loss 0.528849.
Train: 2018-07-31T09:15:02.483518: step 2724, loss 0.612934.
Train: 2018-07-31T09:15:02.639734: step 2725, loss 0.570855.
Train: 2018-07-31T09:15:02.795916: step 2726, loss 0.587691.
Train: 2018-07-31T09:15:02.952158: step 2727, loss 0.486236.
Train: 2018-07-31T09:15:03.108366: step 2728, loss 0.486017.
Train: 2018-07-31T09:15:03.264555: step 2729, loss 0.562426.
Train: 2018-07-31T09:15:03.405179: step 2730, loss 0.45111.
Test: 2018-07-31T09:15:03.655090: step 2730, loss 0.54797.
Train: 2018-07-31T09:15:03.795682: step 2731, loss 0.553682.
Train: 2018-07-31T09:15:03.951924: step 2732, loss 0.484382.
Train: 2018-07-31T09:15:04.123730: step 2733, loss 0.536276.
Train: 2018-07-31T09:15:04.264351: step 2734, loss 0.57989.
Train: 2018-07-31T09:15:04.420565: step 2735, loss 0.597759.
Train: 2018-07-31T09:15:04.576747: step 2736, loss 0.562597.
Train: 2018-07-31T09:15:04.732961: step 2737, loss 0.518183.
Train: 2018-07-31T09:15:04.889174: step 2738, loss 0.56272.
Train: 2018-07-31T09:15:05.045418: step 2739, loss 0.544622.
Train: 2018-07-31T09:15:05.201631: step 2740, loss 0.607543.
Test: 2018-07-31T09:15:05.435922: step 2740, loss 0.547604.
Train: 2018-07-31T09:15:05.576513: step 2741, loss 0.553712.
Train: 2018-07-31T09:15:05.748378: step 2742, loss 0.56272.
Train: 2018-07-31T09:15:05.904561: step 2743, loss 0.517637.
Train: 2018-07-31T09:15:06.060775: step 2744, loss 0.553914.
Train: 2018-07-31T09:15:06.201368: step 2745, loss 0.581047.
Train: 2018-07-31T09:15:06.373202: step 2746, loss 0.508407.
Train: 2018-07-31T09:15:06.529415: step 2747, loss 0.535646.
Train: 2018-07-31T09:15:06.685628: step 2748, loss 0.580624.
Train: 2018-07-31T09:15:06.841842: step 2749, loss 0.589846.
Train: 2018-07-31T09:15:06.982465: step 2750, loss 0.535563.
Test: 2018-07-31T09:15:07.232401: step 2750, loss 0.547578.
Train: 2018-07-31T09:15:07.388590: step 2751, loss 0.526465.
Train: 2018-07-31T09:15:07.544833: step 2752, loss 0.490342.
Train: 2018-07-31T09:15:07.701016: step 2753, loss 0.562786.
Train: 2018-07-31T09:15:07.857230: step 2754, loss 0.608397.
Train: 2018-07-31T09:15:07.997821: step 2755, loss 0.581078.
Train: 2018-07-31T09:15:08.154037: step 2756, loss 0.489977.
Train: 2018-07-31T09:15:08.310249: step 2757, loss 0.499169.
Train: 2018-07-31T09:15:08.466493: step 2758, loss 0.626545.
Train: 2018-07-31T09:15:08.638297: step 2759, loss 0.508381.
Train: 2018-07-31T09:15:08.794509: step 2760, loss 0.489779.
Test: 2018-07-31T09:15:09.028863: step 2760, loss 0.547571.
Train: 2018-07-31T09:15:09.200667: step 2761, loss 0.644996.
Train: 2018-07-31T09:15:09.356904: step 2762, loss 0.626749.
Train: 2018-07-31T09:15:09.513092: step 2763, loss 0.599379.
Train: 2018-07-31T09:15:09.669306: step 2764, loss 0.508276.
Train: 2018-07-31T09:15:09.825549: step 2765, loss 0.499264.
Train: 2018-07-31T09:15:09.981762: step 2766, loss 0.553532.
Train: 2018-07-31T09:15:10.137946: step 2767, loss 0.517439.
Train: 2018-07-31T09:15:10.294189: step 2768, loss 0.535589.
Train: 2018-07-31T09:15:10.450373: step 2769, loss 0.589982.
Train: 2018-07-31T09:15:10.606617: step 2770, loss 0.617137.
Test: 2018-07-31T09:15:10.840937: step 2770, loss 0.547582.
Train: 2018-07-31T09:15:10.997151: step 2771, loss 0.571678.
Train: 2018-07-31T09:15:11.153332: step 2772, loss 0.508502.
Train: 2018-07-31T09:15:11.309546: step 2773, loss 0.616816.
Train: 2018-07-31T09:15:11.465759: step 2774, loss 0.598746.
Train: 2018-07-31T09:15:11.621974: step 2775, loss 0.571662.
Train: 2018-07-31T09:15:11.778211: step 2776, loss 0.473089.
Train: 2018-07-31T09:15:11.918809: step 2777, loss 0.670025.
Train: 2018-07-31T09:15:12.075022: step 2778, loss 0.589322.
Train: 2018-07-31T09:15:12.231206: step 2779, loss 0.527075.
Train: 2018-07-31T09:15:12.387451: step 2780, loss 0.606698.
Test: 2018-07-31T09:15:12.621770: step 2780, loss 0.547712.
Train: 2018-07-31T09:15:12.777952: step 2781, loss 0.536075.
Train: 2018-07-31T09:15:12.934166: step 2782, loss 0.536176.
Train: 2018-07-31T09:15:13.090409: step 2783, loss 0.562482.
Train: 2018-07-31T09:15:13.246631: step 2784, loss 0.571349.
Train: 2018-07-31T09:15:13.402831: step 2785, loss 0.588649.
Train: 2018-07-31T09:15:13.559049: step 2786, loss 0.606079.
Train: 2018-07-31T09:15:13.715263: step 2787, loss 0.536353.
Train: 2018-07-31T09:15:13.871478: step 2788, loss 0.571061.
Train: 2018-07-31T09:15:14.027692: step 2789, loss 0.545062.
Train: 2018-07-31T09:15:14.183874: step 2790, loss 0.536416.
Test: 2018-07-31T09:15:14.418195: step 2790, loss 0.547952.
Train: 2018-07-31T09:15:14.574406: step 2791, loss 0.570997.
Train: 2018-07-31T09:15:14.730651: step 2792, loss 0.527945.
Train: 2018-07-31T09:15:14.886866: step 2793, loss 0.605486.
Train: 2018-07-31T09:15:15.043047: step 2794, loss 0.58822.
Train: 2018-07-31T09:15:15.199286: step 2795, loss 0.579532.
Train: 2018-07-31T09:15:15.371120: step 2796, loss 0.630978.
Train: 2018-07-31T09:15:15.527308: step 2797, loss 0.579492.
Train: 2018-07-31T09:15:15.683521: step 2798, loss 0.587928.
Train: 2018-07-31T09:15:15.839765: step 2799, loss 0.562417.
Train: 2018-07-31T09:15:15.995338: step 2800, loss 0.528584.
Test: 2018-07-31T09:15:16.229689: step 2800, loss 0.54828.
Train: 2018-07-31T09:15:16.979512: step 2801, loss 0.596222.
Train: 2018-07-31T09:15:17.135696: step 2802, loss 0.545548.
Train: 2018-07-31T09:15:17.291910: step 2803, loss 0.528762.
Train: 2018-07-31T09:15:17.463774: step 2804, loss 0.528826.
Train: 2018-07-31T09:15:17.619988: step 2805, loss 0.596141.
Train: 2018-07-31T09:15:17.776195: step 2806, loss 0.436363.
Train: 2018-07-31T09:15:17.932384: step 2807, loss 0.570837.
Train: 2018-07-31T09:15:18.088628: step 2808, loss 0.562479.
Train: 2018-07-31T09:15:18.260463: step 2809, loss 0.554001.
Train: 2018-07-31T09:15:18.416672: step 2810, loss 0.596286.
Test: 2018-07-31T09:15:18.650997: step 2810, loss 0.548205.
Train: 2018-07-31T09:15:18.807204: step 2811, loss 0.460626.
Train: 2018-07-31T09:15:18.963394: step 2812, loss 0.630475.
Train: 2018-07-31T09:15:19.119606: step 2813, loss 0.605003.
Train: 2018-07-31T09:15:19.275851: step 2814, loss 0.613595.
Train: 2018-07-31T09:15:19.432064: step 2815, loss 0.605063.
Train: 2018-07-31T09:15:19.588276: step 2816, loss 0.596495.
Train: 2018-07-31T09:15:19.744461: step 2817, loss 0.587891.
Train: 2018-07-31T09:15:19.916320: step 2818, loss 0.570895.
Train: 2018-07-31T09:15:20.072533: step 2819, loss 0.56245.
Train: 2018-07-31T09:15:20.228752: step 2820, loss 0.638427.
Test: 2018-07-31T09:15:20.463073: step 2820, loss 0.548355.
Train: 2018-07-31T09:15:20.619280: step 2821, loss 0.570902.
Train: 2018-07-31T09:15:20.775499: step 2822, loss 0.579167.
Train: 2018-07-31T09:15:20.931683: step 2823, loss 0.579139.
Train: 2018-07-31T09:15:21.087920: step 2824, loss 0.487485.
Train: 2018-07-31T09:15:21.244139: step 2825, loss 0.587453.
Train: 2018-07-31T09:15:21.400352: step 2826, loss 0.604106.
Train: 2018-07-31T09:15:21.556560: step 2827, loss 0.529326.
Train: 2018-07-31T09:15:21.712780: step 2828, loss 0.529323.
Train: 2018-07-31T09:15:21.868992: step 2829, loss 0.595743.
Train: 2018-07-31T09:15:22.025206: step 2830, loss 0.579114.
Test: 2018-07-31T09:15:22.259497: step 2830, loss 0.548642.
Train: 2018-07-31T09:15:22.415710: step 2831, loss 0.570803.
Train: 2018-07-31T09:15:22.571924: step 2832, loss 0.512784.
Train: 2018-07-31T09:15:22.728137: step 2833, loss 0.595696.
Train: 2018-07-31T09:15:22.884350: step 2834, loss 0.554208.
Train: 2018-07-31T09:15:23.040596: step 2835, loss 0.512726.
Train: 2018-07-31T09:15:23.196777: step 2836, loss 0.512666.
Train: 2018-07-31T09:15:23.352990: step 2837, loss 0.637488.
Train: 2018-07-31T09:15:23.493613: step 2838, loss 0.520757.
Train: 2018-07-31T09:15:23.649826: step 2839, loss 0.570833.
Train: 2018-07-31T09:15:23.806039: step 2840, loss 0.545742.
Test: 2018-07-31T09:15:24.055952: step 2840, loss 0.548402.
Train: 2018-07-31T09:15:24.212194: step 2841, loss 0.520515.
Train: 2018-07-31T09:15:24.352756: step 2842, loss 0.562458.
Train: 2018-07-31T09:15:24.524621: step 2843, loss 0.579283.
Train: 2018-07-31T09:15:24.665213: step 2844, loss 0.570867.
Train: 2018-07-31T09:15:24.821396: step 2845, loss 0.486217.
Train: 2018-07-31T09:15:24.977610: step 2846, loss 0.528459.
Train: 2018-07-31T09:15:25.133823: step 2847, loss 0.545358.
Train: 2018-07-31T09:15:25.290069: step 2848, loss 0.588105.
Train: 2018-07-31T09:15:25.446280: step 2849, loss 0.579526.
Train: 2018-07-31T09:15:25.602493: step 2850, loss 0.57102.
Test: 2018-07-31T09:15:25.836784: step 2850, loss 0.547975.
Train: 2018-07-31T09:15:25.993027: step 2851, loss 0.536516.
Train: 2018-07-31T09:15:26.149240: step 2852, loss 0.5624.
Train: 2018-07-31T09:15:26.305423: step 2853, loss 0.588359.
Train: 2018-07-31T09:15:26.461637: step 2854, loss 0.579729.
Train: 2018-07-31T09:15:26.617851: step 2855, loss 0.631691.
Train: 2018-07-31T09:15:26.774064: step 2856, loss 0.51912.
Train: 2018-07-31T09:15:26.930277: step 2857, loss 0.545149.
Train: 2018-07-31T09:15:27.102142: step 2858, loss 0.571053.
Train: 2018-07-31T09:15:27.258325: step 2859, loss 0.631654.
Train: 2018-07-31T09:15:27.398947: step 2860, loss 0.571075.
Test: 2018-07-31T09:15:27.633272: step 2860, loss 0.547949.
Train: 2018-07-31T09:15:27.789483: step 2861, loss 0.571073.
Train: 2018-07-31T09:15:27.945695: step 2862, loss 0.536542.
Train: 2018-07-31T09:15:28.101908: step 2863, loss 0.588192.
Train: 2018-07-31T09:15:28.258124: step 2864, loss 0.553799.
Train: 2018-07-31T09:15:28.414329: step 2865, loss 0.528013.
Train: 2018-07-31T09:15:28.570517: step 2866, loss 0.631108.
Train: 2018-07-31T09:15:28.726732: step 2867, loss 0.613767.
Train: 2018-07-31T09:15:28.882979: step 2868, loss 0.65642.
Train: 2018-07-31T09:15:29.039192: step 2869, loss 0.544376.
Train: 2018-07-31T09:15:29.195402: step 2870, loss 0.5709.
Test: 2018-07-31T09:15:29.429722: step 2870, loss 0.548305.
Train: 2018-07-31T09:15:29.585936: step 2871, loss 0.579202.
Train: 2018-07-31T09:15:29.742119: step 2872, loss 0.621076.
Train: 2018-07-31T09:15:29.898332: step 2873, loss 0.504007.
Train: 2018-07-31T09:15:30.054575: step 2874, loss 0.563147.
Train: 2018-07-31T09:15:30.210789: step 2875, loss 0.546488.
Train: 2018-07-31T09:15:30.367002: step 2876, loss 0.612406.
Train: 2018-07-31T09:15:30.523216: step 2877, loss 0.570704.
Train: 2018-07-31T09:15:30.663809: step 2878, loss 0.546087.
Train: 2018-07-31T09:15:30.820022: step 2879, loss 0.545876.
Train: 2018-07-31T09:15:30.976235: step 2880, loss 0.562489.
Test: 2018-07-31T09:15:31.226177: step 2880, loss 0.548599.
Train: 2018-07-31T09:15:31.382368: step 2881, loss 0.496072.
Train: 2018-07-31T09:15:31.538605: step 2882, loss 0.562506.
Train: 2018-07-31T09:15:31.710408: step 2883, loss 0.554168.
Train: 2018-07-31T09:15:31.866621: step 2884, loss 0.554131.
Train: 2018-07-31T09:15:32.007243: step 2885, loss 0.529034.
Train: 2018-07-31T09:15:32.179078: step 2886, loss 0.49543.
Train: 2018-07-31T09:15:32.335286: step 2887, loss 0.537222.
Train: 2018-07-31T09:15:32.491505: step 2888, loss 0.613047.
Train: 2018-07-31T09:15:32.647689: step 2889, loss 0.570852.
Train: 2018-07-31T09:15:32.803902: step 2890, loss 0.503033.
Test: 2018-07-31T09:15:33.038247: step 2890, loss 0.548162.
Train: 2018-07-31T09:15:33.194435: step 2891, loss 0.579404.
Train: 2018-07-31T09:15:33.350679: step 2892, loss 0.536809.
Train: 2018-07-31T09:15:33.506893: step 2893, loss 0.528287.
Train: 2018-07-31T09:15:33.663109: step 2894, loss 0.588105.
Train: 2018-07-31T09:15:33.819289: step 2895, loss 0.54531.
Train: 2018-07-31T09:15:33.975527: step 2896, loss 0.536532.
Train: 2018-07-31T09:15:34.131746: step 2897, loss 0.571075.
Train: 2018-07-31T09:15:34.287954: step 2898, loss 0.519018.
Train: 2018-07-31T09:15:34.444174: step 2899, loss 0.492929.
Train: 2018-07-31T09:15:34.615978: step 2900, loss 0.597449.
Test: 2018-07-31T09:15:34.850299: step 2900, loss 0.547789.
Train: 2018-07-31T09:15:35.584500: step 2901, loss 0.518712.
Train: 2018-07-31T09:15:35.725122: step 2902, loss 0.53614.
Train: 2018-07-31T09:15:35.896958: step 2903, loss 0.562514.
Train: 2018-07-31T09:15:36.037520: step 2904, loss 0.536048.
Train: 2018-07-31T09:15:36.193766: step 2905, loss 0.606834.
Train: 2018-07-31T09:15:36.349976: step 2906, loss 0.491479.
Train: 2018-07-31T09:15:36.506184: step 2907, loss 0.571479.
Train: 2018-07-31T09:15:36.662398: step 2908, loss 0.526837.
Train: 2018-07-31T09:15:36.818618: step 2909, loss 0.58055.
Train: 2018-07-31T09:15:36.974801: step 2910, loss 0.526727.
Test: 2018-07-31T09:15:37.209152: step 2910, loss 0.547593.
Train: 2018-07-31T09:15:37.365333: step 2911, loss 0.472676.
Train: 2018-07-31T09:15:37.521547: step 2912, loss 0.526908.
Train: 2018-07-31T09:15:37.677760: step 2913, loss 0.599109.
Train: 2018-07-31T09:15:37.833998: step 2914, loss 0.572091.
Train: 2018-07-31T09:15:37.990218: step 2915, loss 0.608293.
Train: 2018-07-31T09:15:38.146425: step 2916, loss 0.608196.
Train: 2018-07-31T09:15:38.302644: step 2917, loss 0.581016.
Train: 2018-07-31T09:15:38.458858: step 2918, loss 0.607932.
Train: 2018-07-31T09:15:38.615072: step 2919, loss 0.571744.
Train: 2018-07-31T09:15:38.771295: step 2920, loss 0.499692.
Test: 2018-07-31T09:15:39.005574: step 2920, loss 0.547601.
Train: 2018-07-31T09:15:39.177409: step 2921, loss 0.589613.
Train: 2018-07-31T09:15:39.333652: step 2922, loss 0.616461.
Train: 2018-07-31T09:15:39.489837: step 2923, loss 0.598377.
Train: 2018-07-31T09:15:39.630458: step 2924, loss 0.509089.
Train: 2018-07-31T09:15:39.786642: step 2925, loss 0.500273.
Train: 2018-07-31T09:15:39.942856: step 2926, loss 0.535851.
Train: 2018-07-31T09:15:40.099093: step 2927, loss 0.589187.
Train: 2018-07-31T09:15:40.255312: step 2928, loss 0.553632.
Train: 2018-07-31T09:15:40.411495: step 2929, loss 0.544803.
Train: 2018-07-31T09:15:40.567708: step 2930, loss 0.536005.
Test: 2018-07-31T09:15:40.802029: step 2930, loss 0.547685.
Train: 2018-07-31T09:15:40.958273: step 2931, loss 0.562466.
Train: 2018-07-31T09:15:41.114455: step 2932, loss 0.553621.
Train: 2018-07-31T09:15:41.270699: step 2933, loss 0.571262.
Train: 2018-07-31T09:15:41.426882: step 2934, loss 0.59793.
Train: 2018-07-31T09:15:41.583126: step 2935, loss 0.562486.
Train: 2018-07-31T09:15:41.739340: step 2936, loss 0.642019.
Train: 2018-07-31T09:15:41.895523: step 2937, loss 0.641521.
Train: 2018-07-31T09:15:42.051737: step 2938, loss 0.536113.
Train: 2018-07-31T09:15:42.223570: step 2939, loss 0.597277.
Train: 2018-07-31T09:15:42.364163: step 2940, loss 0.562424.
Test: 2018-07-31T09:15:42.614129: step 2940, loss 0.547909.
Train: 2018-07-31T09:15:42.770318: step 2941, loss 0.588375.
Train: 2018-07-31T09:15:42.926566: step 2942, loss 0.571054.
Train: 2018-07-31T09:15:43.098396: step 2943, loss 0.562343.
Train: 2018-07-31T09:15:43.254610: step 2944, loss 0.613594.
Train: 2018-07-31T09:15:43.410825: step 2945, loss 0.596668.
Train: 2018-07-31T09:15:43.567006: step 2946, loss 0.536862.
Train: 2018-07-31T09:15:43.723246: step 2947, loss 0.596203.
Train: 2018-07-31T09:15:43.879434: step 2948, loss 0.5792.
Train: 2018-07-31T09:15:44.035680: step 2949, loss 0.545667.
Train: 2018-07-31T09:15:44.191893: step 2950, loss 0.56251.
Test: 2018-07-31T09:15:44.441826: step 2950, loss 0.548474.
Train: 2018-07-31T09:15:44.582424: step 2951, loss 0.512236.
Train: 2018-07-31T09:15:44.754259: step 2952, loss 0.554067.
Train: 2018-07-31T09:15:44.894845: step 2953, loss 0.529096.
Train: 2018-07-31T09:15:45.066655: step 2954, loss 0.512197.
Train: 2018-07-31T09:15:45.222899: step 2955, loss 0.512196.
Train: 2018-07-31T09:15:45.379112: step 2956, loss 0.579023.
Train: 2018-07-31T09:15:45.535295: step 2957, loss 0.570472.
Train: 2018-07-31T09:15:45.707130: step 2958, loss 0.587814.
Train: 2018-07-31T09:15:45.863373: step 2959, loss 0.629833.
Train: 2018-07-31T09:15:46.019557: step 2960, loss 0.579521.
Test: 2018-07-31T09:15:46.253879: step 2960, loss 0.548257.
Train: 2018-07-31T09:15:46.410091: step 2961, loss 0.529178.
Train: 2018-07-31T09:15:46.566305: step 2962, loss 0.587871.
Train: 2018-07-31T09:15:46.722518: step 2963, loss 0.536865.
Train: 2018-07-31T09:15:46.878731: step 2964, loss 0.545681.
Train: 2018-07-31T09:15:47.034944: step 2965, loss 0.604418.
Train: 2018-07-31T09:15:47.175566: step 2966, loss 0.562512.
Train: 2018-07-31T09:15:47.331780: step 2967, loss 0.502742.
Train: 2018-07-31T09:15:47.487993: step 2968, loss 0.605289.
Train: 2018-07-31T09:15:47.644176: step 2969, loss 0.562175.
Train: 2018-07-31T09:15:47.800425: step 2970, loss 0.519984.
Test: 2018-07-31T09:15:48.034711: step 2970, loss 0.548159.
Train: 2018-07-31T09:15:48.190948: step 2971, loss 0.494427.
Train: 2018-07-31T09:15:48.331516: step 2972, loss 0.570771.
Train: 2018-07-31T09:15:48.503350: step 2973, loss 0.605356.
Train: 2018-07-31T09:15:48.659597: step 2974, loss 0.536475.
Train: 2018-07-31T09:15:48.800156: step 2975, loss 0.596718.
Train: 2018-07-31T09:15:48.956370: step 2976, loss 0.579021.
Train: 2018-07-31T09:15:49.112583: step 2977, loss 0.579017.
Train: 2018-07-31T09:15:49.268827: step 2978, loss 0.519243.
Train: 2018-07-31T09:15:49.425039: step 2979, loss 0.527318.
Train: 2018-07-31T09:15:49.581253: step 2980, loss 0.440709.
Test: 2018-07-31T09:15:49.815572: step 2980, loss 0.547879.
Train: 2018-07-31T09:15:49.971756: step 2981, loss 0.60592.
Train: 2018-07-31T09:15:50.143629: step 2982, loss 0.570601.
Train: 2018-07-31T09:15:50.299805: step 2983, loss 0.45707.
Train: 2018-07-31T09:15:50.456025: step 2984, loss 0.555237.
Train: 2018-07-31T09:15:50.596610: step 2985, loss 0.597429.
Train: 2018-07-31T09:15:50.768444: step 2986, loss 0.579447.
Train: 2018-07-31T09:15:50.909067: step 2987, loss 0.64212.
Train: 2018-07-31T09:15:51.065280: step 2988, loss 0.616248.
Train: 2018-07-31T09:15:51.221494: step 2989, loss 0.544747.
Train: 2018-07-31T09:15:51.377678: step 2990, loss 0.591661.
Test: 2018-07-31T09:15:51.612033: step 2990, loss 0.547707.
Train: 2018-07-31T09:15:51.768210: step 2991, loss 0.500379.
Train: 2018-07-31T09:15:51.924425: step 2992, loss 0.527431.
Train: 2018-07-31T09:15:52.065017: step 2993, loss 0.614756.
Train: 2018-07-31T09:15:52.236851: step 2994, loss 0.606223.
Train: 2018-07-31T09:15:52.377443: step 2995, loss 0.536254.
Train: 2018-07-31T09:15:52.533656: step 2996, loss 0.60641.
Train: 2018-07-31T09:15:52.689870: step 2997, loss 0.510397.
Train: 2018-07-31T09:15:52.846108: step 2998, loss 0.607069.
Train: 2018-07-31T09:15:53.002297: step 2999, loss 0.528759.
Train: 2018-07-31T09:15:53.174157: step 3000, loss 0.545357.
Test: 2018-07-31T09:15:53.408453: step 3000, loss 0.547906.
Train: 2018-07-31T09:15:54.111412: step 3001, loss 0.527726.
Train: 2018-07-31T09:15:54.283277: step 3002, loss 0.570321.
Train: 2018-07-31T09:15:54.439460: step 3003, loss 0.606199.
Train: 2018-07-31T09:15:54.595674: step 3004, loss 0.57959.
Train: 2018-07-31T09:15:54.736296: step 3005, loss 0.570201.
Train: 2018-07-31T09:15:54.892517: step 3006, loss 0.545408.
Train: 2018-07-31T09:15:55.048692: step 3007, loss 0.527567.
Train: 2018-07-31T09:15:55.220527: step 3008, loss 0.6229.
Train: 2018-07-31T09:15:55.376771: step 3009, loss 0.553314.
Train: 2018-07-31T09:15:55.548576: step 3010, loss 0.631025.
Test: 2018-07-31T09:15:55.782921: step 3010, loss 0.54811.
Train: 2018-07-31T09:15:55.939134: step 3011, loss 0.553533.
Train: 2018-07-31T09:15:56.095352: step 3012, loss 0.647419.
Train: 2018-07-31T09:15:56.251566: step 3013, loss 0.579279.
Train: 2018-07-31T09:15:56.407750: step 3014, loss 0.647305.
Train: 2018-07-31T09:15:56.563963: step 3015, loss 0.571249.
Train: 2018-07-31T09:15:56.720176: step 3016, loss 0.546375.
Train: 2018-07-31T09:15:56.876415: step 3017, loss 0.529364.
Train: 2018-07-31T09:15:57.032636: step 3018, loss 0.570832.
Train: 2018-07-31T09:15:57.188847: step 3019, loss 0.554055.
Train: 2018-07-31T09:15:57.345030: step 3020, loss 0.526984.
Test: 2018-07-31T09:15:57.579350: step 3020, loss 0.548741.
Train: 2018-07-31T09:15:57.751185: step 3021, loss 0.512797.
Train: 2018-07-31T09:15:57.891807: step 3022, loss 0.570375.
Train: 2018-07-31T09:15:58.047991: step 3023, loss 0.578751.
Train: 2018-07-31T09:15:58.204234: step 3024, loss 0.513221.
Train: 2018-07-31T09:15:58.360416: step 3025, loss 0.545946.
Train: 2018-07-31T09:15:58.516630: step 3026, loss 0.603786.
Train: 2018-07-31T09:15:58.672878: step 3027, loss 0.520979.
Train: 2018-07-31T09:15:58.829057: step 3028, loss 0.554032.
Train: 2018-07-31T09:15:58.985270: step 3029, loss 0.620794.
Train: 2018-07-31T09:15:59.141514: step 3030, loss 0.55421.
Test: 2018-07-31T09:15:59.375829: step 3030, loss 0.548415.
Train: 2018-07-31T09:15:59.532043: step 3031, loss 0.65457.
Train: 2018-07-31T09:15:59.688231: step 3032, loss 0.528968.
Train: 2018-07-31T09:15:59.844445: step 3033, loss 0.645858.
Train: 2018-07-31T09:16:00.016304: step 3034, loss 0.462271.
Train: 2018-07-31T09:16:00.172522: step 3035, loss 0.59554.
Train: 2018-07-31T09:16:00.328737: step 3036, loss 0.596107.
Train: 2018-07-31T09:16:00.484920: step 3037, loss 0.528818.
Train: 2018-07-31T09:16:00.641163: step 3038, loss 0.512248.
Train: 2018-07-31T09:16:00.797347: step 3039, loss 0.528993.
Train: 2018-07-31T09:16:00.937940: step 3040, loss 0.587861.
Test: 2018-07-31T09:16:01.172259: step 3040, loss 0.548316.
Train: 2018-07-31T09:16:01.328473: step 3041, loss 0.570553.
Train: 2018-07-31T09:16:01.484716: step 3042, loss 0.536859.
Train: 2018-07-31T09:16:01.640898: step 3043, loss 0.562396.
Train: 2018-07-31T09:16:01.797112: step 3044, loss 0.612439.
Train: 2018-07-31T09:16:01.953327: step 3045, loss 0.596474.
Train: 2018-07-31T09:16:02.125186: step 3046, loss 0.554384.
Train: 2018-07-31T09:16:02.265782: step 3047, loss 0.587562.
Train: 2018-07-31T09:16:02.421996: step 3048, loss 0.552985.
Train: 2018-07-31T09:16:02.593834: step 3049, loss 0.536172.
Train: 2018-07-31T09:16:02.750015: step 3050, loss 0.553433.
Test: 2018-07-31T09:16:02.984360: step 3050, loss 0.548152.
Train: 2018-07-31T09:16:03.140573: step 3051, loss 0.656443.
Train: 2018-07-31T09:16:03.296761: step 3052, loss 0.587584.
Train: 2018-07-31T09:16:03.453005: step 3053, loss 0.604077.
Train: 2018-07-31T09:16:03.609218: step 3054, loss 0.621822.
Train: 2018-07-31T09:16:03.765402: step 3055, loss 0.503352.
Train: 2018-07-31T09:16:03.921615: step 3056, loss 0.596384.
Train: 2018-07-31T09:16:04.077858: step 3057, loss 0.536452.
Train: 2018-07-31T09:16:04.218450: step 3058, loss 0.52914.
Train: 2018-07-31T09:16:04.374664: step 3059, loss 0.511365.
Train: 2018-07-31T09:16:04.530872: step 3060, loss 0.563346.
Test: 2018-07-31T09:16:04.780789: step 3060, loss 0.548319.
Train: 2018-07-31T09:16:04.937027: step 3061, loss 0.537031.
Train: 2018-07-31T09:16:05.077624: step 3062, loss 0.579605.
Train: 2018-07-31T09:16:05.233808: step 3063, loss 0.512193.
Train: 2018-07-31T09:16:05.390051: step 3064, loss 0.586444.
Train: 2018-07-31T09:16:05.546275: step 3065, loss 0.588466.
Train: 2018-07-31T09:16:05.718094: step 3066, loss 0.546136.
Train: 2018-07-31T09:16:05.858661: step 3067, loss 0.519866.
Train: 2018-07-31T09:16:06.014874: step 3068, loss 0.597055.
Train: 2018-07-31T09:16:06.171116: step 3069, loss 0.536006.
Train: 2018-07-31T09:16:06.327326: step 3070, loss 0.631142.
Test: 2018-07-31T09:16:06.561622: step 3070, loss 0.548069.
Train: 2018-07-31T09:16:06.717865: step 3071, loss 0.554886.
Train: 2018-07-31T09:16:06.874078: step 3072, loss 0.545517.
Train: 2018-07-31T09:16:07.030287: step 3073, loss 0.571275.
Train: 2018-07-31T09:16:07.170884: step 3074, loss 0.579907.
Train: 2018-07-31T09:16:07.327067: step 3075, loss 0.554477.
Train: 2018-07-31T09:16:07.483281: step 3076, loss 0.509827.
Train: 2018-07-31T09:16:07.639525: step 3077, loss 0.511362.
Train: 2018-07-31T09:16:07.795738: step 3078, loss 0.553386.
Train: 2018-07-31T09:16:07.951921: step 3079, loss 0.588068.
Train: 2018-07-31T09:16:08.108165: step 3080, loss 0.580325.
Test: 2018-07-31T09:16:08.342457: step 3080, loss 0.547979.
Train: 2018-07-31T09:16:08.498668: step 3081, loss 0.519046.
Train: 2018-07-31T09:16:08.654911: step 3082, loss 0.536227.
Train: 2018-07-31T09:16:08.811094: step 3083, loss 0.518444.
Train: 2018-07-31T09:16:08.967308: step 3084, loss 0.50267.
Train: 2018-07-31T09:16:09.123547: step 3085, loss 0.597346.
Train: 2018-07-31T09:16:09.279735: step 3086, loss 0.615511.
Train: 2018-07-31T09:16:09.451571: step 3087, loss 0.518863.
Train: 2018-07-31T09:16:09.592192: step 3088, loss 0.492775.
Train: 2018-07-31T09:16:09.748375: step 3089, loss 0.55084.
Train: 2018-07-31T09:16:09.904588: step 3090, loss 0.449022.
Test: 2018-07-31T09:16:10.154530: step 3090, loss 0.54768.
Train: 2018-07-31T09:16:10.310774: step 3091, loss 0.545043.
Train: 2018-07-31T09:16:10.466987: step 3092, loss 0.571676.
Train: 2018-07-31T09:16:10.623171: step 3093, loss 0.653547.
Train: 2018-07-31T09:16:10.779385: step 3094, loss 0.536512.
Train: 2018-07-31T09:16:10.935627: step 3095, loss 0.517434.
Train: 2018-07-31T09:16:11.091841: step 3096, loss 0.561126.
Train: 2018-07-31T09:16:11.248055: step 3097, loss 0.55565.
Train: 2018-07-31T09:16:11.404270: step 3098, loss 0.560615.
Train: 2018-07-31T09:16:11.544830: step 3099, loss 0.525901.
Train: 2018-07-31T09:16:11.701073: step 3100, loss 0.527454.
Test: 2018-07-31T09:16:11.950986: step 3100, loss 0.547589.
Train: 2018-07-31T09:16:12.685217: step 3101, loss 0.544496.
Train: 2018-07-31T09:16:12.841431: step 3102, loss 0.518464.
Train: 2018-07-31T09:16:12.997644: step 3103, loss 0.526174.
Train: 2018-07-31T09:16:13.153861: step 3104, loss 0.535685.
Train: 2018-07-31T09:16:13.310072: step 3105, loss 0.560149.
Train: 2018-07-31T09:16:13.466254: step 3106, loss 0.5545.
Train: 2018-07-31T09:16:13.606877: step 3107, loss 0.518165.
Train: 2018-07-31T09:16:13.763095: step 3108, loss 0.581095.
Train: 2018-07-31T09:16:13.919304: step 3109, loss 0.580356.
Train: 2018-07-31T09:16:14.075487: step 3110, loss 0.59125.
Test: 2018-07-31T09:16:14.309807: step 3110, loss 0.547587.
Train: 2018-07-31T09:16:14.481673: step 3111, loss 0.59244.
Train: 2018-07-31T09:16:14.637856: step 3112, loss 0.628578.
Train: 2018-07-31T09:16:14.794104: step 3113, loss 0.525971.
Train: 2018-07-31T09:16:14.950283: step 3114, loss 0.535362.
Train: 2018-07-31T09:16:15.106526: step 3115, loss 0.508308.
Train: 2018-07-31T09:16:15.247089: step 3116, loss 0.554647.
Train: 2018-07-31T09:16:15.418949: step 3117, loss 0.509522.
Train: 2018-07-31T09:16:15.559545: step 3118, loss 0.490283.
Train: 2018-07-31T09:16:15.731351: step 3119, loss 0.600039.
Train: 2018-07-31T09:16:15.887563: step 3120, loss 0.651715.
Test: 2018-07-31T09:16:16.121884: step 3120, loss 0.547596.
Train: 2018-07-31T09:16:16.278127: step 3121, loss 0.535745.
Train: 2018-07-31T09:16:16.434340: step 3122, loss 0.562102.
Train: 2018-07-31T09:16:16.590556: step 3123, loss 0.526631.
Train: 2018-07-31T09:16:16.746762: step 3124, loss 0.562123.
Train: 2018-07-31T09:16:16.902980: step 3125, loss 0.625292.
Train: 2018-07-31T09:16:17.059194: step 3126, loss 0.624518.
Train: 2018-07-31T09:16:17.215401: step 3127, loss 0.561503.
Train: 2018-07-31T09:16:17.371590: step 3128, loss 0.587773.
Train: 2018-07-31T09:16:17.527829: step 3129, loss 0.587838.
Train: 2018-07-31T09:16:17.684041: step 3130, loss 0.500699.
Test: 2018-07-31T09:16:17.918337: step 3130, loss 0.547759.
Train: 2018-07-31T09:16:18.090171: step 3131, loss 0.62319.
Train: 2018-07-31T09:16:18.246385: step 3132, loss 0.570897.
Train: 2018-07-31T09:16:18.386977: step 3133, loss 0.615538.
Train: 2018-07-31T09:16:18.543192: step 3134, loss 0.535991.
Train: 2018-07-31T09:16:18.699405: step 3135, loss 0.553848.
Train: 2018-07-31T09:16:18.855648: step 3136, loss 0.58735.
Train: 2018-07-31T09:16:19.011833: step 3137, loss 0.510536.
Train: 2018-07-31T09:16:19.168045: step 3138, loss 0.545989.
Train: 2018-07-31T09:16:19.324289: step 3139, loss 0.605952.
Train: 2018-07-31T09:16:19.480502: step 3140, loss 0.561754.
Test: 2018-07-31T09:16:19.714793: step 3140, loss 0.548091.
Train: 2018-07-31T09:16:19.871035: step 3141, loss 0.570919.
Train: 2018-07-31T09:16:20.027251: step 3142, loss 0.605221.
Train: 2018-07-31T09:16:20.183456: step 3143, loss 0.562188.
Train: 2018-07-31T09:16:20.339675: step 3144, loss 0.578888.
Train: 2018-07-31T09:16:20.495883: step 3145, loss 0.579968.
Train: 2018-07-31T09:16:20.652072: step 3146, loss 0.605061.
Train: 2018-07-31T09:16:20.808315: step 3147, loss 0.478093.
Train: 2018-07-31T09:16:20.964529: step 3148, loss 0.520438.
Train: 2018-07-31T09:16:21.136374: step 3149, loss 0.537244.
Train: 2018-07-31T09:16:21.292549: step 3150, loss 0.554974.
Test: 2018-07-31T09:16:21.526900: step 3150, loss 0.548323.
Train: 2018-07-31T09:16:21.698702: step 3151, loss 0.554223.
Train: 2018-07-31T09:16:21.901779: step 3152, loss 0.562276.
Train: 2018-07-31T09:16:22.058026: step 3153, loss 0.554135.
Train: 2018-07-31T09:16:22.214236: step 3154, loss 0.511931.
Train: 2018-07-31T09:16:22.354799: step 3155, loss 0.6816.
Train: 2018-07-31T09:16:22.511012: step 3156, loss 0.587673.
Train: 2018-07-31T09:16:22.667255: step 3157, loss 0.629329.
Train: 2018-07-31T09:16:22.823469: step 3158, loss 0.57888.
Train: 2018-07-31T09:16:22.979682: step 3159, loss 0.570884.
Train: 2018-07-31T09:16:23.135895: step 3160, loss 0.621705.
Test: 2018-07-31T09:16:23.385807: step 3160, loss 0.54845.
Train: 2018-07-31T09:16:23.588884: step 3161, loss 0.503173.
Train: 2018-07-31T09:16:23.729507: step 3162, loss 0.579354.
Train: 2018-07-31T09:16:23.885690: step 3163, loss 0.57894.
Train: 2018-07-31T09:16:24.041903: step 3164, loss 0.529403.
Train: 2018-07-31T09:16:24.198117: step 3165, loss 0.537749.
Train: 2018-07-31T09:16:24.369958: step 3166, loss 0.60331.
Train: 2018-07-31T09:16:24.526165: step 3167, loss 0.579969.
Train: 2018-07-31T09:16:24.666758: step 3168, loss 0.587122.
Train: 2018-07-31T09:16:24.822971: step 3169, loss 0.537712.
Train: 2018-07-31T09:16:24.979184: step 3170, loss 0.570596.
Test: 2018-07-31T09:16:25.213534: step 3170, loss 0.54862.
Train: 2018-07-31T09:16:25.369717: step 3171, loss 0.61472.
Train: 2018-07-31T09:16:25.525931: step 3172, loss 0.595959.
Train: 2018-07-31T09:16:25.682144: step 3173, loss 0.520896.
Train: 2018-07-31T09:16:25.838394: step 3174, loss 0.619786.
Train: 2018-07-31T09:16:25.994572: step 3175, loss 0.496112.
Train: 2018-07-31T09:16:26.150785: step 3176, loss 0.554183.
Train: 2018-07-31T09:16:26.307029: step 3177, loss 0.60422.
Train: 2018-07-31T09:16:26.447591: step 3178, loss 0.587552.
Train: 2018-07-31T09:16:26.603804: step 3179, loss 0.488871.
Train: 2018-07-31T09:16:26.760046: step 3180, loss 0.546555.
Test: 2018-07-31T09:16:26.994338: step 3180, loss 0.548616.
Train: 2018-07-31T09:16:27.166207: step 3181, loss 0.512345.
Train: 2018-07-31T09:16:27.322384: step 3182, loss 0.587732.
Train: 2018-07-31T09:16:27.463010: step 3183, loss 0.529463.
Train: 2018-07-31T09:16:27.619220: step 3184, loss 0.588194.
Train: 2018-07-31T09:16:27.775405: step 3185, loss 0.545582.
Train: 2018-07-31T09:16:27.931647: step 3186, loss 0.519949.
Train: 2018-07-31T09:16:28.087861: step 3187, loss 0.528455.
Train: 2018-07-31T09:16:28.244045: step 3188, loss 0.605005.
Train: 2018-07-31T09:16:28.400287: step 3189, loss 0.469039.
Train: 2018-07-31T09:16:28.540884: step 3190, loss 0.562692.
Test: 2018-07-31T09:16:28.790793: step 3190, loss 0.548048.
Train: 2018-07-31T09:16:28.962650: step 3191, loss 0.615061.
Train: 2018-07-31T09:16:29.103248: step 3192, loss 0.59818.
Train: 2018-07-31T09:16:29.275053: step 3193, loss 0.511543.
Train: 2018-07-31T09:16:29.431266: step 3194, loss 0.647627.
Train: 2018-07-31T09:16:29.571858: step 3195, loss 0.536773.
Train: 2018-07-31T09:16:29.728071: step 3196, loss 0.630703.
Train: 2018-07-31T09:16:29.884315: step 3197, loss 0.519632.
Train: 2018-07-31T09:16:30.040499: step 3198, loss 0.476307.
Train: 2018-07-31T09:16:30.196711: step 3199, loss 0.623603.
Train: 2018-07-31T09:16:30.352926: step 3200, loss 0.519074.
Test: 2018-07-31T09:16:30.587276: step 3200, loss 0.547957.
Train: 2018-07-31T09:16:31.399556: step 3201, loss 0.546144.
Train: 2018-07-31T09:16:31.540177: step 3202, loss 0.571203.
Train: 2018-07-31T09:16:31.696391: step 3203, loss 0.665897.
Train: 2018-07-31T09:16:31.852604: step 3204, loss 0.544706.
Train: 2018-07-31T09:16:32.024441: step 3205, loss 0.493864.
Train: 2018-07-31T09:16:32.180623: step 3206, loss 0.596815.
Train: 2018-07-31T09:16:32.336837: step 3207, loss 0.605634.
Train: 2018-07-31T09:16:32.493082: step 3208, loss 0.588274.
Train: 2018-07-31T09:16:32.649262: step 3209, loss 0.544572.
Train: 2018-07-31T09:16:32.805475: step 3210, loss 0.587712.
Test: 2018-07-31T09:16:33.039826: step 3210, loss 0.548047.
Train: 2018-07-31T09:16:33.196010: step 3211, loss 0.61412.
Train: 2018-07-31T09:16:33.352254: step 3212, loss 0.528254.
Train: 2018-07-31T09:16:33.508466: step 3213, loss 0.450833.
Train: 2018-07-31T09:16:33.664680: step 3214, loss 0.520041.
Train: 2018-07-31T09:16:33.820895: step 3215, loss 0.596883.
Train: 2018-07-31T09:16:33.977108: step 3216, loss 0.553837.
Train: 2018-07-31T09:16:34.133291: step 3217, loss 0.510734.
Train: 2018-07-31T09:16:34.289533: step 3218, loss 0.622138.
Train: 2018-07-31T09:16:34.445717: step 3219, loss 0.519077.
Train: 2018-07-31T09:16:34.601960: step 3220, loss 0.58016.
Test: 2018-07-31T09:16:34.836281: step 3220, loss 0.54795.
Train: 2018-07-31T09:16:34.992464: step 3221, loss 0.58842.
Train: 2018-07-31T09:16:35.148678: step 3222, loss 0.545459.
Train: 2018-07-31T09:16:35.304915: step 3223, loss 0.622843.
Train: 2018-07-31T09:16:35.461105: step 3224, loss 0.53672.
Train: 2018-07-31T09:16:35.617317: step 3225, loss 0.571528.
Train: 2018-07-31T09:16:35.757940: step 3226, loss 0.527924.
Train: 2018-07-31T09:16:35.929745: step 3227, loss 0.501981.
Train: 2018-07-31T09:16:36.070337: step 3228, loss 0.545427.
Train: 2018-07-31T09:16:36.226575: step 3229, loss 0.545403.
Train: 2018-07-31T09:16:36.382794: step 3230, loss 0.544915.
Test: 2018-07-31T09:16:36.632729: step 3230, loss 0.547877.
Train: 2018-07-31T09:16:36.773327: step 3231, loss 0.545241.
Train: 2018-07-31T09:16:36.929540: step 3232, loss 0.640716.
Train: 2018-07-31T09:16:37.101385: step 3233, loss 0.57153.
Train: 2018-07-31T09:16:37.257589: step 3234, loss 0.518431.
Train: 2018-07-31T09:16:37.413796: step 3235, loss 0.571255.
Train: 2018-07-31T09:16:37.585606: step 3236, loss 0.527487.
Train: 2018-07-31T09:16:37.741855: step 3237, loss 0.658439.
Train: 2018-07-31T09:16:37.898066: step 3238, loss 0.5191.
Train: 2018-07-31T09:16:38.054277: step 3239, loss 0.553939.
Train: 2018-07-31T09:16:38.210490: step 3240, loss 0.535979.
Test: 2018-07-31T09:16:38.444780: step 3240, loss 0.54785.
Train: 2018-07-31T09:16:38.616616: step 3241, loss 0.62333.
Train: 2018-07-31T09:16:38.772858: step 3242, loss 0.45779.
Train: 2018-07-31T09:16:38.929043: step 3243, loss 0.554578.
Train: 2018-07-31T09:16:39.085280: step 3244, loss 0.553831.
Train: 2018-07-31T09:16:39.241469: step 3245, loss 0.562352.
Train: 2018-07-31T09:16:39.382061: step 3246, loss 0.597305.
Train: 2018-07-31T09:16:39.538304: step 3247, loss 0.58812.
Train: 2018-07-31T09:16:39.694518: step 3248, loss 0.56268.
Train: 2018-07-31T09:16:39.850702: step 3249, loss 0.588646.
Train: 2018-07-31T09:16:40.006945: step 3250, loss 0.536581.
Test: 2018-07-31T09:16:40.241235: step 3250, loss 0.547846.
Train: 2018-07-31T09:16:40.413099: step 3251, loss 0.545414.
Train: 2018-07-31T09:16:40.569313: step 3252, loss 0.54493.
Train: 2018-07-31T09:16:40.725496: step 3253, loss 0.588337.
Train: 2018-07-31T09:16:40.881709: step 3254, loss 0.536316.
Train: 2018-07-31T09:16:41.037923: step 3255, loss 0.553684.
Train: 2018-07-31T09:16:41.194136: step 3256, loss 0.484564.
Train: 2018-07-31T09:16:41.350350: step 3257, loss 0.597455.
Train: 2018-07-31T09:16:41.506594: step 3258, loss 0.606415.
Train: 2018-07-31T09:16:41.662777: step 3259, loss 0.553974.
Train: 2018-07-31T09:16:41.818991: step 3260, loss 0.553663.
Test: 2018-07-31T09:16:42.053341: step 3260, loss 0.547847.
Train: 2018-07-31T09:16:42.209524: step 3261, loss 0.614801.
Train: 2018-07-31T09:16:42.365738: step 3262, loss 0.553861.
Train: 2018-07-31T09:16:42.521951: step 3263, loss 0.596779.
Train: 2018-07-31T09:16:42.678194: step 3264, loss 0.519325.
Train: 2018-07-31T09:16:42.834402: step 3265, loss 0.527905.
Train: 2018-07-31T09:16:42.990615: step 3266, loss 0.518969.
Train: 2018-07-31T09:16:43.131209: step 3267, loss 0.492897.
Train: 2018-07-31T09:16:43.287396: step 3268, loss 0.597249.
Train: 2018-07-31T09:16:43.443609: step 3269, loss 0.579893.
Train: 2018-07-31T09:16:43.599855: step 3270, loss 0.510237.
Test: 2018-07-31T09:16:43.834143: step 3270, loss 0.547833.
Train: 2018-07-31T09:16:43.990358: step 3271, loss 0.510221.
Train: 2018-07-31T09:16:44.146571: step 3272, loss 0.571226.
Train: 2018-07-31T09:16:44.302784: step 3273, loss 0.571239.
Train: 2018-07-31T09:16:44.458998: step 3274, loss 0.527559.
Train: 2018-07-31T09:16:44.615241: step 3275, loss 0.483461.
Train: 2018-07-31T09:16:44.771425: step 3276, loss 0.57147.
Train: 2018-07-31T09:16:44.927668: step 3277, loss 0.589079.
Train: 2018-07-31T09:16:45.099501: step 3278, loss 0.598012.
Train: 2018-07-31T09:16:45.255711: step 3279, loss 0.518147.
Train: 2018-07-31T09:16:45.396278: step 3280, loss 0.562405.
Test: 2018-07-31T09:16:45.630631: step 3280, loss 0.547674.
Train: 2018-07-31T09:16:45.802433: step 3281, loss 0.562471.
Train: 2018-07-31T09:16:45.958676: step 3282, loss 0.544673.
Train: 2018-07-31T09:16:46.114890: step 3283, loss 0.580138.
Train: 2018-07-31T09:16:46.271097: step 3284, loss 0.642656.
Train: 2018-07-31T09:16:46.427286: step 3285, loss 0.598014.
Train: 2018-07-31T09:16:46.583500: step 3286, loss 0.553553.
Train: 2018-07-31T09:16:46.739743: step 3287, loss 0.527245.
Train: 2018-07-31T09:16:46.895927: step 3288, loss 0.580165.
Train: 2018-07-31T09:16:47.052139: step 3289, loss 0.580121.
Train: 2018-07-31T09:16:47.208354: step 3290, loss 0.606403.
Test: 2018-07-31T09:16:47.442699: step 3290, loss 0.547767.
Train: 2018-07-31T09:16:47.598917: step 3291, loss 0.579938.
Train: 2018-07-31T09:16:47.755100: step 3292, loss 0.510025.
Train: 2018-07-31T09:16:47.911314: step 3293, loss 0.61476.
Train: 2018-07-31T09:16:48.067526: step 3294, loss 0.605913.
Train: 2018-07-31T09:16:48.223770: step 3295, loss 0.571011.
Train: 2018-07-31T09:16:48.379986: step 3296, loss 0.510476.
Train: 2018-07-31T09:16:48.536167: step 3297, loss 0.493308.
Train: 2018-07-31T09:16:48.692411: step 3298, loss 0.545.
Train: 2018-07-31T09:16:48.848594: step 3299, loss 0.639978.
Train: 2018-07-31T09:16:49.020430: step 3300, loss 0.553768.
Test: 2018-07-31T09:16:49.254779: step 3300, loss 0.547983.
Train: 2018-07-31T09:16:49.973330: step 3301, loss 0.588214.
Train: 2018-07-31T09:16:50.145196: step 3302, loss 0.562512.
Train: 2018-07-31T09:16:50.301409: step 3303, loss 0.545252.
Train: 2018-07-31T09:16:50.457592: step 3304, loss 0.502384.
Train: 2018-07-31T09:16:50.613805: step 3305, loss 0.605222.
Train: 2018-07-31T09:16:50.770018: step 3306, loss 0.605357.
Train: 2018-07-31T09:16:50.926263: step 3307, loss 0.699362.
Train: 2018-07-31T09:16:51.082470: step 3308, loss 0.630527.
Train: 2018-07-31T09:16:51.238689: step 3309, loss 0.545475.
Train: 2018-07-31T09:16:51.394874: step 3310, loss 0.570858.
Test: 2018-07-31T09:16:51.629219: step 3310, loss 0.548364.
Train: 2018-07-31T09:16:51.785406: step 3311, loss 0.587674.
Train: 2018-07-31T09:16:51.941620: step 3312, loss 0.512244.
Train: 2018-07-31T09:16:52.082243: step 3313, loss 0.579229.
Train: 2018-07-31T09:16:52.238456: step 3314, loss 0.529154.
Train: 2018-07-31T09:16:52.394639: step 3315, loss 0.579129.
Train: 2018-07-31T09:16:52.535232: step 3316, loss 0.603995.
Train: 2018-07-31T09:16:52.691444: step 3317, loss 0.570746.
Train: 2018-07-31T09:16:52.847658: step 3318, loss 0.529364.
Train: 2018-07-31T09:16:53.003871: step 3319, loss 0.587365.
Train: 2018-07-31T09:16:53.160085: step 3320, loss 0.545942.
Test: 2018-07-31T09:16:53.394438: step 3320, loss 0.548724.
Train: 2018-07-31T09:16:53.550617: step 3321, loss 0.562563.
Train: 2018-07-31T09:16:53.706831: step 3322, loss 0.597775.
Train: 2018-07-31T09:16:53.863074: step 3323, loss 0.496625.
Train: 2018-07-31T09:16:54.019291: step 3324, loss 0.554434.
Train: 2018-07-31T09:16:54.175502: step 3325, loss 0.479785.
Train: 2018-07-31T09:16:54.331715: step 3326, loss 0.529409.
Train: 2018-07-31T09:16:54.472307: step 3327, loss 0.5958.
Train: 2018-07-31T09:16:54.644144: step 3328, loss 0.570954.
Train: 2018-07-31T09:16:54.784734: step 3329, loss 0.595904.
Train: 2018-07-31T09:16:54.956538: step 3330, loss 0.495336.
Test: 2018-07-31T09:16:55.190889: step 3330, loss 0.548355.
Train: 2018-07-31T09:16:55.347097: step 3331, loss 0.528819.
Train: 2018-07-31T09:16:55.503286: step 3332, loss 0.570935.
Train: 2018-07-31T09:16:55.659529: step 3333, loss 0.57945.
Train: 2018-07-31T09:16:55.815750: step 3334, loss 0.681084.
Train: 2018-07-31T09:16:55.971926: step 3335, loss 0.545518.
Train: 2018-07-31T09:16:56.128170: step 3336, loss 0.579411.
Train: 2018-07-31T09:16:56.268761: step 3337, loss 0.587847.
Train: 2018-07-31T09:16:56.440597: step 3338, loss 0.579343.
Train: 2018-07-31T09:16:56.596779: step 3339, loss 0.604707.
Train: 2018-07-31T09:16:56.737371: step 3340, loss 0.579344.
Test: 2018-07-31T09:16:56.987315: step 3340, loss 0.548293.
Train: 2018-07-31T09:16:57.143557: step 3341, loss 0.49496.
Train: 2018-07-31T09:16:57.299771: step 3342, loss 0.596074.
Train: 2018-07-31T09:16:57.455984: step 3343, loss 0.579333.
Train: 2018-07-31T09:16:57.612167: step 3344, loss 0.638291.
Train: 2018-07-31T09:16:57.784031: step 3345, loss 0.554036.
Train: 2018-07-31T09:16:57.940214: step 3346, loss 0.554071.
Train: 2018-07-31T09:16:58.096428: step 3347, loss 0.512251.
Train: 2018-07-31T09:16:58.252642: step 3348, loss 0.570742.
Train: 2018-07-31T09:16:58.408885: step 3349, loss 0.596018.
Train: 2018-07-31T09:16:58.565099: step 3350, loss 0.554127.
Test: 2018-07-31T09:16:58.799391: step 3350, loss 0.548457.
Train: 2018-07-31T09:16:58.955636: step 3351, loss 0.620993.
Train: 2018-07-31T09:16:59.111815: step 3352, loss 0.537394.
Train: 2018-07-31T09:16:59.268062: step 3353, loss 0.512376.
Train: 2018-07-31T09:16:59.424273: step 3354, loss 0.612567.
Train: 2018-07-31T09:16:59.580456: step 3355, loss 0.495684.
Train: 2018-07-31T09:16:59.752322: step 3356, loss 0.604283.
Train: 2018-07-31T09:16:59.908505: step 3357, loss 0.579169.
Train: 2018-07-31T09:17:00.080338: step 3358, loss 0.612651.
Train: 2018-07-31T09:17:00.236583: step 3359, loss 0.612573.
Train: 2018-07-31T09:17:00.392795: step 3360, loss 0.52074.
Test: 2018-07-31T09:17:00.627086: step 3360, loss 0.548527.
Train: 2018-07-31T09:17:00.783300: step 3361, loss 0.529102.
Train: 2018-07-31T09:17:00.939512: step 3362, loss 0.529192.
Train: 2018-07-31T09:17:01.111348: step 3363, loss 0.545715.
Train: 2018-07-31T09:17:01.251940: step 3364, loss 0.528982.
Train: 2018-07-31T09:17:01.408183: step 3365, loss 0.595934.
Train: 2018-07-31T09:17:01.580013: step 3366, loss 0.545649.
Train: 2018-07-31T09:17:01.736201: step 3367, loss 0.58765.
Train: 2018-07-31T09:17:01.876793: step 3368, loss 0.537128.
Train: 2018-07-31T09:17:02.033006: step 3369, loss 0.520215.
Train: 2018-07-31T09:17:02.189251: step 3370, loss 0.52864.
Test: 2018-07-31T09:17:02.423540: step 3370, loss 0.548206.
Train: 2018-07-31T09:17:02.579778: step 3371, loss 0.58796.
Train: 2018-07-31T09:17:02.735992: step 3372, loss 0.613495.
Train: 2018-07-31T09:17:02.892180: step 3373, loss 0.545287.
Train: 2018-07-31T09:17:03.048424: step 3374, loss 0.570984.
Train: 2018-07-31T09:17:03.204608: step 3375, loss 0.57942.
Train: 2018-07-31T09:17:03.360851: step 3376, loss 0.528337.
Train: 2018-07-31T09:17:03.517064: step 3377, loss 0.579385.
Train: 2018-07-31T09:17:03.673272: step 3378, loss 0.656411.
Train: 2018-07-31T09:17:03.829461: step 3379, loss 0.596457.
Train: 2018-07-31T09:17:03.985699: step 3380, loss 0.485611.
Test: 2018-07-31T09:17:04.220025: step 3380, loss 0.548125.
Train: 2018-07-31T09:17:04.376234: step 3381, loss 0.553778.
Train: 2018-07-31T09:17:04.516800: step 3382, loss 0.536933.
Train: 2018-07-31T09:17:04.673014: step 3383, loss 0.528318.
Train: 2018-07-31T09:17:04.829227: step 3384, loss 0.604985.
Train: 2018-07-31T09:17:04.985464: step 3385, loss 0.511032.
Train: 2018-07-31T09:17:05.157276: step 3386, loss 0.451141.
Train: 2018-07-31T09:17:05.313518: step 3387, loss 0.527981.
Train: 2018-07-31T09:17:05.469701: step 3388, loss 0.605603.
Train: 2018-07-31T09:17:05.610324: step 3389, loss 0.536236.
Train: 2018-07-31T09:17:05.782129: step 3390, loss 0.562509.
Test: 2018-07-31T09:17:06.016477: step 3390, loss 0.547844.
Train: 2018-07-31T09:17:06.172692: step 3391, loss 0.57116.
Train: 2018-07-31T09:17:06.328906: step 3392, loss 0.545168.
Train: 2018-07-31T09:17:06.485119: step 3393, loss 0.475177.
Train: 2018-07-31T09:17:06.641332: step 3394, loss 0.509838.
Train: 2018-07-31T09:17:06.797546: step 3395, loss 0.544879.
Train: 2018-07-31T09:17:06.969380: step 3396, loss 0.588966.
Train: 2018-07-31T09:17:07.109973: step 3397, loss 0.580239.
Train: 2018-07-31T09:17:07.281777: step 3398, loss 0.544649.
Train: 2018-07-31T09:17:07.422369: step 3399, loss 0.598341.
Train: 2018-07-31T09:17:07.578619: step 3400, loss 0.562877.
Test: 2018-07-31T09:17:07.812904: step 3400, loss 0.547635.
Train: 2018-07-31T09:17:08.578379: step 3401, loss 0.51801.
Train: 2018-07-31T09:17:08.750185: step 3402, loss 0.482236.
Train: 2018-07-31T09:17:08.906397: step 3403, loss 0.49086.
Train: 2018-07-31T09:17:09.062610: step 3404, loss 0.607544.
Train: 2018-07-31T09:17:09.218854: step 3405, loss 0.562708.
Train: 2018-07-31T09:17:09.375067: step 3406, loss 0.526592.
Train: 2018-07-31T09:17:09.531251: step 3407, loss 0.589782.
Train: 2018-07-31T09:17:09.687465: step 3408, loss 0.589803.
Train: 2018-07-31T09:17:09.843707: step 3409, loss 0.571645.
Train: 2018-07-31T09:17:09.999921: step 3410, loss 0.499396.
Test: 2018-07-31T09:17:10.234243: step 3410, loss 0.547579.
Train: 2018-07-31T09:17:10.406045: step 3411, loss 0.599156.
Train: 2018-07-31T09:17:10.546668: step 3412, loss 0.526559.
Train: 2018-07-31T09:17:10.702852: step 3413, loss 0.562585.
Train: 2018-07-31T09:17:10.874687: step 3414, loss 0.49924.
Train: 2018-07-31T09:17:11.030929: step 3415, loss 0.463124.
Train: 2018-07-31T09:17:11.187113: step 3416, loss 0.617521.
Train: 2018-07-31T09:17:11.343327: step 3417, loss 0.608113.
Train: 2018-07-31T09:17:11.499540: step 3418, loss 0.535719.
Train: 2018-07-31T09:17:11.655778: step 3419, loss 0.554013.
Train: 2018-07-31T09:17:11.827588: step 3420, loss 0.553837.
Test: 2018-07-31T09:17:12.061909: step 3420, loss 0.547574.
Train: 2018-07-31T09:17:12.218152: step 3421, loss 0.607727.
Train: 2018-07-31T09:17:12.374365: step 3422, loss 0.608035.
Train: 2018-07-31T09:17:12.530583: step 3423, loss 0.617155.
Train: 2018-07-31T09:17:12.686792: step 3424, loss 0.643711.
Train: 2018-07-31T09:17:12.843011: step 3425, loss 0.58977.
Train: 2018-07-31T09:17:12.983598: step 3426, loss 0.491062.
Train: 2018-07-31T09:17:13.139810: step 3427, loss 0.526953.
Train: 2018-07-31T09:17:13.295994: step 3428, loss 0.580285.
Train: 2018-07-31T09:17:13.452238: step 3429, loss 0.589049.
Train: 2018-07-31T09:17:13.608455: step 3430, loss 0.597608.
Test: 2018-07-31T09:17:13.842741: step 3430, loss 0.547753.
Train: 2018-07-31T09:17:13.998979: step 3431, loss 0.553717.
Train: 2018-07-31T09:17:14.155203: step 3432, loss 0.54475.
Train: 2018-07-31T09:17:14.311382: step 3433, loss 0.527586.
Train: 2018-07-31T09:17:14.467595: step 3434, loss 0.588599.
Train: 2018-07-31T09:17:14.623838: step 3435, loss 0.588536.
Train: 2018-07-31T09:17:14.780051: step 3436, loss 0.545035.
Train: 2018-07-31T09:17:14.936235: step 3437, loss 0.588386.
Train: 2018-07-31T09:17:15.092448: step 3438, loss 0.519385.
Train: 2018-07-31T09:17:15.248692: step 3439, loss 0.536498.
Train: 2018-07-31T09:17:15.404899: step 3440, loss 0.613968.
Test: 2018-07-31T09:17:15.639230: step 3440, loss 0.547997.
Train: 2018-07-31T09:17:15.795439: step 3441, loss 0.596767.
Train: 2018-07-31T09:17:15.951623: step 3442, loss 0.588258.
Train: 2018-07-31T09:17:16.107860: step 3443, loss 0.587982.
Train: 2018-07-31T09:17:16.264049: step 3444, loss 0.553875.
Train: 2018-07-31T09:17:16.420262: step 3445, loss 0.587905.
Train: 2018-07-31T09:17:16.560885: step 3446, loss 0.613357.
Train: 2018-07-31T09:17:16.732689: step 3447, loss 0.537047.
Train: 2018-07-31T09:17:16.888903: step 3448, loss 0.62157.
Train: 2018-07-31T09:17:17.045151: step 3449, loss 0.537235.
Train: 2018-07-31T09:17:17.201360: step 3450, loss 0.512078.
Test: 2018-07-31T09:17:17.451296: step 3450, loss 0.548428.
Train: 2018-07-31T09:17:17.607514: step 3451, loss 0.554084.
Train: 2018-07-31T09:17:17.763699: step 3452, loss 0.537376.
Train: 2018-07-31T09:17:17.919941: step 3453, loss 0.604339.
Train: 2018-07-31T09:17:18.076149: step 3454, loss 0.528977.
Train: 2018-07-31T09:17:18.232339: step 3455, loss 0.620989.
Train: 2018-07-31T09:17:18.388551: step 3456, loss 0.587493.
Train: 2018-07-31T09:17:18.560410: step 3457, loss 0.487256.
Train: 2018-07-31T09:17:18.716624: step 3458, loss 0.629374.
Train: 2018-07-31T09:17:18.857223: step 3459, loss 0.587502.
Train: 2018-07-31T09:17:19.029061: step 3460, loss 0.5959.
Test: 2018-07-31T09:17:19.263380: step 3460, loss 0.54855.
Train: 2018-07-31T09:17:19.419560: step 3461, loss 0.529222.
Train: 2018-07-31T09:17:19.575774: step 3462, loss 0.662322.
Train: 2018-07-31T09:17:19.731987: step 3463, loss 0.55429.
Train: 2018-07-31T09:17:19.888226: step 3464, loss 0.562468.
Train: 2018-07-31T09:17:20.044445: step 3465, loss 0.562484.
Train: 2018-07-31T09:17:20.200628: step 3466, loss 0.504746.
Train: 2018-07-31T09:17:20.356872: step 3467, loss 0.570758.
Train: 2018-07-31T09:17:20.513087: step 3468, loss 0.513001.
Train: 2018-07-31T09:17:20.669268: step 3469, loss 0.637056.
Train: 2018-07-31T09:17:20.841133: step 3470, loss 0.51291.
Test: 2018-07-31T09:17:21.075422: step 3470, loss 0.54865.
Train: 2018-07-31T09:17:21.247281: step 3471, loss 0.554244.
Train: 2018-07-31T09:17:21.403500: step 3472, loss 0.521011.
Train: 2018-07-31T09:17:21.544063: step 3473, loss 0.562533.
Train: 2018-07-31T09:17:21.700306: step 3474, loss 0.637616.
Train: 2018-07-31T09:17:21.856519: step 3475, loss 0.57916.
Train: 2018-07-31T09:17:22.012703: step 3476, loss 0.504017.
Train: 2018-07-31T09:17:22.168947: step 3477, loss 0.545756.
Train: 2018-07-31T09:17:22.325130: step 3478, loss 0.570852.
Train: 2018-07-31T09:17:22.481342: step 3479, loss 0.5709.
Train: 2018-07-31T09:17:22.637589: step 3480, loss 0.545675.
Test: 2018-07-31T09:17:22.856291: step 3480, loss 0.548343.
Train: 2018-07-31T09:17:23.028115: step 3481, loss 0.537241.
Train: 2018-07-31T09:17:23.184333: step 3482, loss 0.511811.
Train: 2018-07-31T09:17:23.340517: step 3483, loss 0.570823.
Train: 2018-07-31T09:17:23.496760: step 3484, loss 0.460618.
Train: 2018-07-31T09:17:23.652978: step 3485, loss 0.596421.
Train: 2018-07-31T09:17:23.809187: step 3486, loss 0.5709.
Train: 2018-07-31T09:17:23.965400: step 3487, loss 0.570992.
Train: 2018-07-31T09:17:24.121584: step 3488, loss 0.510842.
Train: 2018-07-31T09:17:24.293418: step 3489, loss 0.648759.
Train: 2018-07-31T09:17:24.449633: step 3490, loss 0.605538.
Test: 2018-07-31T09:17:24.683952: step 3490, loss 0.547929.
Train: 2018-07-31T09:17:24.840196: step 3491, loss 0.571041.
Train: 2018-07-31T09:17:24.996409: step 3492, loss 0.493296.
Train: 2018-07-31T09:17:25.152593: step 3493, loss 0.588366.
Train: 2018-07-31T09:17:25.308830: step 3494, loss 0.553747.
Train: 2018-07-31T09:17:25.449428: step 3495, loss 0.545006.
Train: 2018-07-31T09:17:25.605642: step 3496, loss 0.536486.
Train: 2018-07-31T09:17:25.761858: step 3497, loss 0.510293.
Train: 2018-07-31T09:17:25.933694: step 3498, loss 0.597211.
Train: 2018-07-31T09:17:26.089904: step 3499, loss 0.527615.
Train: 2018-07-31T09:17:26.246120: step 3500, loss 0.571171.
Test: 2018-07-31T09:17:26.480437: step 3500, loss 0.547801.
Train: 2018-07-31T09:17:27.198988: step 3501, loss 0.510141.
Train: 2018-07-31T09:17:27.355202: step 3502, loss 0.562366.
Train: 2018-07-31T09:17:27.495795: step 3503, loss 0.52739.
Train: 2018-07-31T09:17:27.667659: step 3504, loss 0.527319.
Train: 2018-07-31T09:17:27.823841: step 3505, loss 0.509631.
Train: 2018-07-31T09:17:27.980086: step 3506, loss 0.597794.
Train: 2018-07-31T09:17:28.136299: step 3507, loss 0.589081.
Train: 2018-07-31T09:17:28.276891: step 3508, loss 0.598024.
Train: 2018-07-31T09:17:28.433100: step 3509, loss 0.651152.
Train: 2018-07-31T09:17:28.604939: step 3510, loss 0.598076.
Test: 2018-07-31T09:17:28.839255: step 3510, loss 0.547698.
Train: 2018-07-31T09:17:28.995442: step 3511, loss 0.571427.
Train: 2018-07-31T09:17:29.151686: step 3512, loss 0.518448.
Train: 2018-07-31T09:17:29.292249: step 3513, loss 0.518464.
Train: 2018-07-31T09:17:29.464113: step 3514, loss 0.571167.
Train: 2018-07-31T09:17:29.620296: step 3515, loss 0.659113.
Train: 2018-07-31T09:17:29.776510: step 3516, loss 0.483568.
Train: 2018-07-31T09:17:29.932724: step 3517, loss 0.457381.
Train: 2018-07-31T09:17:30.088966: step 3518, loss 0.55361.
Train: 2018-07-31T09:17:30.245149: step 3519, loss 0.571265.
Train: 2018-07-31T09:17:30.401363: step 3520, loss 0.536159.
Test: 2018-07-31T09:17:30.635684: step 3520, loss 0.547744.
Train: 2018-07-31T09:17:30.807519: step 3521, loss 0.606379.
Train: 2018-07-31T09:17:30.963762: step 3522, loss 0.518518.
Train: 2018-07-31T09:17:31.119945: step 3523, loss 0.597608.
Train: 2018-07-31T09:17:31.276194: step 3524, loss 0.588938.
Train: 2018-07-31T09:17:31.432402: step 3525, loss 0.518529.
Train: 2018-07-31T09:17:31.588617: step 3526, loss 0.562506.
Train: 2018-07-31T09:17:31.744829: step 3527, loss 0.562522.
Train: 2018-07-31T09:17:31.901013: step 3528, loss 0.623992.
Train: 2018-07-31T09:17:32.057225: step 3529, loss 0.492313.
Train: 2018-07-31T09:17:32.213439: step 3530, loss 0.527347.
Test: 2018-07-31T09:17:32.447790: step 3530, loss 0.54776.
Train: 2018-07-31T09:17:32.603973: step 3531, loss 0.580173.
Train: 2018-07-31T09:17:32.760219: step 3532, loss 0.536177.
Train: 2018-07-31T09:17:32.916430: step 3533, loss 0.580045.
Train: 2018-07-31T09:17:33.056992: step 3534, loss 0.553595.
Train: 2018-07-31T09:17:33.213236: step 3535, loss 0.597591.
Train: 2018-07-31T09:17:33.369449: step 3536, loss 0.562432.
Train: 2018-07-31T09:17:33.525631: step 3537, loss 0.641323.
Train: 2018-07-31T09:17:33.681879: step 3538, loss 0.606185.
Train: 2018-07-31T09:17:33.838060: step 3539, loss 0.45786.
Train: 2018-07-31T09:17:33.978681: step 3540, loss 0.562387.
Test: 2018-07-31T09:17:34.212971: step 3540, loss 0.54785.
Train: 2018-07-31T09:17:34.369209: step 3541, loss 0.597147.
Train: 2018-07-31T09:17:34.525399: step 3542, loss 0.536349.
Train: 2018-07-31T09:17:34.681612: step 3543, loss 0.484301.
Train: 2018-07-31T09:17:34.837855: step 3544, loss 0.545014.
Train: 2018-07-31T09:17:34.994068: step 3545, loss 0.571103.
Train: 2018-07-31T09:17:35.150286: step 3546, loss 0.544985.
Train: 2018-07-31T09:17:35.306464: step 3547, loss 0.5102.
Train: 2018-07-31T09:17:35.462708: step 3548, loss 0.545016.
Train: 2018-07-31T09:17:35.618892: step 3549, loss 0.579914.
Train: 2018-07-31T09:17:35.775133: step 3550, loss 0.483654.
Test: 2018-07-31T09:17:36.009454: step 3550, loss 0.547761.
Train: 2018-07-31T09:17:36.181261: step 3551, loss 0.492312.
Train: 2018-07-31T09:17:36.337473: step 3552, loss 0.64168.
Train: 2018-07-31T09:17:36.493717: step 3553, loss 0.580029.
Train: 2018-07-31T09:17:36.649924: step 3554, loss 0.580209.
Train: 2018-07-31T09:17:36.806145: step 3555, loss 0.650774.
Train: 2018-07-31T09:17:36.962328: step 3556, loss 0.553759.
Train: 2018-07-31T09:17:37.118571: step 3557, loss 0.606457.
Train: 2018-07-31T09:17:37.274784: step 3558, loss 0.606331.
Train: 2018-07-31T09:17:37.430998: step 3559, loss 0.536175.
Train: 2018-07-31T09:17:37.587211: step 3560, loss 0.614888.
Test: 2018-07-31T09:17:37.821502: step 3560, loss 0.547833.
Train: 2018-07-31T09:17:37.977748: step 3561, loss 0.623359.
Train: 2018-07-31T09:17:38.133928: step 3562, loss 0.527764.
Train: 2018-07-31T09:17:38.290140: step 3563, loss 0.536459.
Train: 2018-07-31T09:17:38.446355: step 3564, loss 0.571041.
Train: 2018-07-31T09:17:38.602568: step 3565, loss 0.493557.
Train: 2018-07-31T09:17:38.758782: step 3566, loss 0.553768.
Train: 2018-07-31T09:17:38.915025: step 3567, loss 0.648386.
Train: 2018-07-31T09:17:39.071239: step 3568, loss 0.588155.
Train: 2018-07-31T09:17:39.227451: step 3569, loss 0.570957.
Train: 2018-07-31T09:17:39.383635: step 3570, loss 0.605051.
Test: 2018-07-31T09:17:39.617981: step 3570, loss 0.54815.
Train: 2018-07-31T09:17:39.774203: step 3571, loss 0.570914.
Train: 2018-07-31T09:17:39.930383: step 3572, loss 0.596358.
Train: 2018-07-31T09:17:40.086596: step 3573, loss 0.503223.
Train: 2018-07-31T09:17:40.242809: step 3574, loss 0.562442.
Train: 2018-07-31T09:17:40.399057: step 3575, loss 0.663637.
Train: 2018-07-31T09:17:40.555270: step 3576, loss 0.604473.
Train: 2018-07-31T09:17:40.711479: step 3577, loss 0.554102.
Train: 2018-07-31T09:17:40.867663: step 3578, loss 0.60419.
Train: 2018-07-31T09:17:41.023876: step 3579, loss 0.537579.
Train: 2018-07-31T09:17:41.180089: step 3580, loss 0.487901.
Test: 2018-07-31T09:17:41.430036: step 3580, loss 0.548651.
Train: 2018-07-31T09:17:41.570623: step 3581, loss 0.55425.
Train: 2018-07-31T09:17:41.726836: step 3582, loss 0.603926.
Train: 2018-07-31T09:17:41.883050: step 3583, loss 0.554251.
Train: 2018-07-31T09:17:42.039263: step 3584, loss 0.454963.
Train: 2018-07-31T09:17:42.195477: step 3585, loss 0.562522.
Train: 2018-07-31T09:17:42.351720: step 3586, loss 0.554177.
Train: 2018-07-31T09:17:42.507903: step 3587, loss 0.537514.
Train: 2018-07-31T09:17:42.664147: step 3588, loss 0.562499.
Train: 2018-07-31T09:17:42.820330: step 3589, loss 0.595945.
Train: 2018-07-31T09:17:42.976543: step 3590, loss 0.503756.
Test: 2018-07-31T09:17:43.195274: step 3590, loss 0.548349.
Train: 2018-07-31T09:17:43.351480: step 3591, loss 0.537195.
Train: 2018-07-31T09:17:43.507669: step 3592, loss 0.579335.
Train: 2018-07-31T09:17:43.663913: step 3593, loss 0.553953.
Train: 2018-07-31T09:17:43.820126: step 3594, loss 0.613311.
Train: 2018-07-31T09:17:43.976340: step 3595, loss 0.630403.
Train: 2018-07-31T09:17:44.132554: step 3596, loss 0.579367.
Train: 2018-07-31T09:17:44.288767: step 3597, loss 0.562429.
Train: 2018-07-31T09:17:44.444980: step 3598, loss 0.562434.
Train: 2018-07-31T09:17:44.601164: step 3599, loss 0.604802.
Train: 2018-07-31T09:17:44.757377: step 3600, loss 0.494658.
Test: 2018-07-31T09:17:44.991721: step 3600, loss 0.548216.
Train: 2018-07-31T09:17:45.694687: step 3601, loss 0.579341.
Train: 2018-07-31T09:17:45.850870: step 3602, loss 0.46069.
Train: 2018-07-31T09:17:46.007117: step 3603, loss 0.502917.
Train: 2018-07-31T09:17:46.147706: step 3604, loss 0.587967.
Train: 2018-07-31T09:17:46.303925: step 3605, loss 0.494018.
Train: 2018-07-31T09:17:46.475754: step 3606, loss 0.639612.
Train: 2018-07-31T09:17:46.631937: step 3607, loss 0.588175.
Train: 2018-07-31T09:17:46.788181: step 3608, loss 0.553791.
Train: 2018-07-31T09:17:46.928773: step 3609, loss 0.648551.
Train: 2018-07-31T09:17:47.100608: step 3610, loss 0.536571.
Test: 2018-07-31T09:17:47.334900: step 3610, loss 0.547988.
Train: 2018-07-31T09:17:47.491141: step 3611, loss 0.536575.
Train: 2018-07-31T09:17:47.647355: step 3612, loss 0.527984.
Train: 2018-07-31T09:17:47.803570: step 3613, loss 0.605475.
Train: 2018-07-31T09:17:47.959782: step 3614, loss 0.6055.
Train: 2018-07-31T09:17:48.115995: step 3615, loss 0.54518.
Train: 2018-07-31T09:17:48.272179: step 3616, loss 0.579587.
Train: 2018-07-31T09:17:48.428422: step 3617, loss 0.485045.
Train: 2018-07-31T09:17:48.584605: step 3618, loss 0.570998.
Train: 2018-07-31T09:17:48.725198: step 3619, loss 0.605405.
Train: 2018-07-31T09:17:48.881441: step 3620, loss 0.605427.
Test: 2018-07-31T09:17:49.115731: step 3620, loss 0.548004.
Train: 2018-07-31T09:17:49.271945: step 3621, loss 0.588152.
Train: 2018-07-31T09:17:49.428188: step 3622, loss 0.562393.
Train: 2018-07-31T09:17:49.584402: step 3623, loss 0.511014.
Train: 2018-07-31T09:17:49.740615: step 3624, loss 0.580678.
Train: 2018-07-31T09:17:49.896828: step 3625, loss 0.528174.
Train: 2018-07-31T09:17:50.053012: step 3626, loss 0.519583.
Train: 2018-07-31T09:17:50.209225: step 3627, loss 0.596662.
Train: 2018-07-31T09:17:50.365439: step 3628, loss 0.562395.
Train: 2018-07-31T09:17:50.521652: step 3629, loss 0.528092.
Train: 2018-07-31T09:17:50.677896: step 3630, loss 0.536651.
Test: 2018-07-31T09:17:50.912216: step 3630, loss 0.548006.
Train: 2018-07-31T09:17:51.068398: step 3631, loss 0.570987.
Train: 2018-07-31T09:17:51.224642: step 3632, loss 0.545196.
Train: 2018-07-31T09:17:51.380825: step 3633, loss 0.502131.
Train: 2018-07-31T09:17:51.521419: step 3634, loss 0.657351.
Train: 2018-07-31T09:17:51.677630: step 3635, loss 0.56238.
Train: 2018-07-31T09:17:51.833875: step 3636, loss 0.588297.
Train: 2018-07-31T09:17:51.990083: step 3637, loss 0.571026.
Train: 2018-07-31T09:17:52.146296: step 3638, loss 0.605506.
Train: 2018-07-31T09:17:52.302515: step 3639, loss 0.66576.
Train: 2018-07-31T09:17:52.458697: step 3640, loss 0.510884.
Test: 2018-07-31T09:17:52.708639: step 3640, loss 0.548051.
Train: 2018-07-31T09:17:52.864883: step 3641, loss 0.570941.
Train: 2018-07-31T09:17:53.021067: step 3642, loss 0.519672.
Train: 2018-07-31T09:17:53.177281: step 3643, loss 0.605103.
Train: 2018-07-31T09:17:53.333524: step 3644, loss 0.494203.
Train: 2018-07-31T09:17:53.505361: step 3645, loss 0.579412.
Train: 2018-07-31T09:17:53.661542: step 3646, loss 0.460132.
Train: 2018-07-31T09:17:53.802134: step 3647, loss 0.570958.
Train: 2018-07-31T09:17:53.958348: step 3648, loss 0.511064.
Train: 2018-07-31T09:17:54.114561: step 3649, loss 0.519537.
Train: 2018-07-31T09:17:54.270809: step 3650, loss 0.553828.
Test: 2018-07-31T09:17:54.505127: step 3650, loss 0.547959.
Train: 2018-07-31T09:17:54.661308: step 3651, loss 0.553812.
Train: 2018-07-31T09:17:54.817521: step 3652, loss 0.58835.
Train: 2018-07-31T09:17:54.973767: step 3653, loss 0.553734.
Train: 2018-07-31T09:17:55.129977: step 3654, loss 0.571086.
Train: 2018-07-31T09:17:55.286191: step 3655, loss 0.623241.
Train: 2018-07-31T09:17:55.442405: step 3656, loss 0.579853.
Train: 2018-07-31T09:17:55.614245: step 3657, loss 0.588479.
Train: 2018-07-31T09:17:55.754832: step 3658, loss 0.536374.
Train: 2018-07-31T09:17:55.895394: step 3659, loss 0.605767.
Train: 2018-07-31T09:17:56.067259: step 3660, loss 0.57107.
Test: 2018-07-31T09:17:56.301579: step 3660, loss 0.547913.
Train: 2018-07-31T09:17:56.457762: step 3661, loss 0.562391.
Train: 2018-07-31T09:17:56.613975: step 3662, loss 0.57102.
Train: 2018-07-31T09:17:56.770189: step 3663, loss 0.536534.
Train: 2018-07-31T09:17:56.926432: step 3664, loss 0.484811.
Train: 2018-07-31T09:17:57.082646: step 3665, loss 0.605591.
Train: 2018-07-31T09:17:57.238859: step 3666, loss 0.596895.
Train: 2018-07-31T09:17:57.395043: step 3667, loss 0.553786.
Train: 2018-07-31T09:17:57.551256: step 3668, loss 0.536565.
Train: 2018-07-31T09:17:57.691878: step 3669, loss 0.605489.
Train: 2018-07-31T09:17:57.863683: step 3670, loss 0.605418.
Test: 2018-07-31T09:17:58.098003: step 3670, loss 0.548002.
Train: 2018-07-31T09:17:58.254246: step 3671, loss 0.596751.
Train: 2018-07-31T09:17:58.410429: step 3672, loss 0.562396.
Train: 2018-07-31T09:17:58.566674: step 3673, loss 0.553817.
Train: 2018-07-31T09:17:58.722856: step 3674, loss 0.553886.
Train: 2018-07-31T09:17:58.879070: step 3675, loss 0.545335.
Train: 2018-07-31T09:17:59.050906: step 3676, loss 0.50272.
Train: 2018-07-31T09:17:59.207117: step 3677, loss 0.545366.
Train: 2018-07-31T09:17:59.363331: step 3678, loss 0.588006.
Train: 2018-07-31T09:17:59.535190: step 3679, loss 0.57093.
Train: 2018-07-31T09:17:59.675782: step 3680, loss 0.579488.
Test: 2018-07-31T09:17:59.925701: step 3680, loss 0.548098.
Train: 2018-07-31T09:18:00.081912: step 3681, loss 0.570939.
Train: 2018-07-31T09:18:00.238151: step 3682, loss 0.570934.
Train: 2018-07-31T09:18:00.394370: step 3683, loss 0.528232.
Train: 2018-07-31T09:18:00.550577: step 3684, loss 0.588011.
Train: 2018-07-31T09:18:00.706768: step 3685, loss 0.519767.
Train: 2018-07-31T09:18:00.863010: step 3686, loss 0.553865.
Train: 2018-07-31T09:18:01.019223: step 3687, loss 0.545297.
Train: 2018-07-31T09:18:01.191059: step 3688, loss 0.459748.
Train: 2018-07-31T09:18:01.347266: step 3689, loss 0.570996.
Train: 2018-07-31T09:18:01.503456: step 3690, loss 0.56238.
Test: 2018-07-31T09:18:01.737809: step 3690, loss 0.547954.
Train: 2018-07-31T09:18:01.894018: step 3691, loss 0.605519.
Train: 2018-07-31T09:18:02.065823: step 3692, loss 0.553773.
Train: 2018-07-31T09:18:02.206446: step 3693, loss 0.493239.
Train: 2018-07-31T09:18:02.378251: step 3694, loss 0.562379.
Train: 2018-07-31T09:18:02.518872: step 3695, loss 0.605883.
Train: 2018-07-31T09:18:02.690678: step 3696, loss 0.553724.
Train: 2018-07-31T09:18:02.846891: step 3697, loss 0.623309.
Train: 2018-07-31T09:18:03.003136: step 3698, loss 0.640735.
Train: 2018-07-31T09:18:03.159318: step 3699, loss 0.501601.
Train: 2018-07-31T09:18:03.315561: step 3700, loss 0.536358.
Test: 2018-07-31T09:18:03.549851: step 3700, loss 0.547875.
Train: 2018-07-31T09:18:04.268463: step 3701, loss 0.588487.
Train: 2018-07-31T09:18:04.424646: step 3702, loss 0.597103.
Train: 2018-07-31T09:18:04.580884: step 3703, loss 0.571053.
Train: 2018-07-31T09:18:04.737108: step 3704, loss 0.475891.
Train: 2018-07-31T09:18:04.893286: step 3705, loss 0.579721.
Train: 2018-07-31T09:18:05.049500: step 3706, loss 0.596995.
Train: 2018-07-31T09:18:05.205713: step 3707, loss 0.597011.
Train: 2018-07-31T09:18:05.361956: step 3708, loss 0.588297.
Train: 2018-07-31T09:18:05.502519: step 3709, loss 0.631343.
Train: 2018-07-31T09:18:05.658732: step 3710, loss 0.545206.
Test: 2018-07-31T09:18:05.893088: step 3710, loss 0.548041.
Train: 2018-07-31T09:18:06.049290: step 3711, loss 0.588102.
Train: 2018-07-31T09:18:06.205478: step 3712, loss 0.587999.
Train: 2018-07-31T09:18:06.361693: step 3713, loss 0.673192.
Train: 2018-07-31T09:18:06.517906: step 3714, loss 0.647198.
Train: 2018-07-31T09:18:06.674150: step 3715, loss 0.545567.
Train: 2018-07-31T09:18:06.830363: step 3716, loss 0.545699.
Train: 2018-07-31T09:18:06.986576: step 3717, loss 0.537435.
Train: 2018-07-31T09:18:07.142789: step 3718, loss 0.545886.
Train: 2018-07-31T09:18:07.299004: step 3719, loss 0.579113.
Train: 2018-07-31T09:18:07.470832: step 3720, loss 0.54597.
Test: 2018-07-31T09:18:07.705129: step 3720, loss 0.5487.
Train: 2018-07-31T09:18:07.861342: step 3721, loss 0.545997.
Train: 2018-07-31T09:18:08.017589: step 3722, loss 0.521249.
Train: 2018-07-31T09:18:08.173769: step 3723, loss 0.521212.
Train: 2018-07-31T09:18:08.329982: step 3724, loss 0.545993.
Train: 2018-07-31T09:18:08.486194: step 3725, loss 0.579114.
Train: 2018-07-31T09:18:08.642438: step 3726, loss 0.620677.
Train: 2018-07-31T09:18:08.798653: step 3727, loss 0.537595.
Train: 2018-07-31T09:18:08.970456: step 3728, loss 0.520948.
Train: 2018-07-31T09:18:09.126670: step 3729, loss 0.512542.
Train: 2018-07-31T09:18:09.282883: step 3730, loss 0.595873.
Test: 2018-07-31T09:18:09.517205: step 3730, loss 0.548453.
Train: 2018-07-31T09:18:09.673417: step 3731, loss 0.545736.
Train: 2018-07-31T09:18:09.829667: step 3732, loss 0.59597.
Train: 2018-07-31T09:18:09.985874: step 3733, loss 0.520491.
Train: 2018-07-31T09:18:10.142087: step 3734, loss 0.554021.
Train: 2018-07-31T09:18:10.298301: step 3735, loss 0.545543.
Train: 2018-07-31T09:18:10.454514: step 3736, loss 0.545433.
Train: 2018-07-31T09:18:10.626318: step 3737, loss 0.570906.
Train: 2018-07-31T09:18:10.782531: step 3738, loss 0.638961.
Train: 2018-07-31T09:18:10.938746: step 3739, loss 0.579421.
Train: 2018-07-31T09:18:11.094989: step 3740, loss 0.562387.
Test: 2018-07-31T09:18:11.329315: step 3740, loss 0.548155.
Train: 2018-07-31T09:18:11.485526: step 3741, loss 0.545415.
Train: 2018-07-31T09:18:11.641741: step 3742, loss 0.536898.
Train: 2018-07-31T09:18:11.797949: step 3743, loss 0.4687.
Train: 2018-07-31T09:18:11.954163: step 3744, loss 0.5795.
Train: 2018-07-31T09:18:12.110377: step 3745, loss 0.570981.
Train: 2018-07-31T09:18:12.282212: step 3746, loss 0.588109.
Train: 2018-07-31T09:18:12.438395: step 3747, loss 0.648314.
Train: 2018-07-31T09:18:12.594639: step 3748, loss 0.553855.
Train: 2018-07-31T09:18:12.735238: step 3749, loss 0.545196.
Train: 2018-07-31T09:18:12.891444: step 3750, loss 0.562379.
Test: 2018-07-31T09:18:13.141380: step 3750, loss 0.548026.
Train: 2018-07-31T09:18:13.297568: step 3751, loss 0.493813.
Train: 2018-07-31T09:18:13.453815: step 3752, loss 0.588143.
Train: 2018-07-31T09:18:13.594375: step 3753, loss 0.545178.
Train: 2018-07-31T09:18:13.766209: step 3754, loss 0.510769.
Train: 2018-07-31T09:18:13.922423: step 3755, loss 0.588301.
Train: 2018-07-31T09:18:14.078666: step 3756, loss 0.571061.
Train: 2018-07-31T09:18:14.234848: step 3757, loss 0.553807.
Train: 2018-07-31T09:18:14.391092: step 3758, loss 0.501828.
Train: 2018-07-31T09:18:14.547275: step 3759, loss 0.536413.
Train: 2018-07-31T09:18:14.687869: step 3760, loss 0.484249.
Test: 2018-07-31T09:18:14.937810: step 3760, loss 0.547828.
Train: 2018-07-31T09:18:15.094023: step 3761, loss 0.571152.
Train: 2018-07-31T09:18:15.234615: step 3762, loss 0.54501.
Train: 2018-07-31T09:18:15.390862: step 3763, loss 0.597551.
Train: 2018-07-31T09:18:15.547066: step 3764, loss 0.544872.
Train: 2018-07-31T09:18:15.703255: step 3765, loss 0.615302.
Train: 2018-07-31T09:18:15.859494: step 3766, loss 0.562472.
Train: 2018-07-31T09:18:16.015712: step 3767, loss 0.527216.
Train: 2018-07-31T09:18:16.187516: step 3768, loss 0.562486.
Train: 2018-07-31T09:18:16.343760: step 3769, loss 0.63303.
Train: 2018-07-31T09:18:16.495381: step 3770, loss 0.553674.
Test: 2018-07-31T09:18:16.729703: step 3770, loss 0.547727.
Train: 2018-07-31T09:18:16.885884: step 3771, loss 0.465629.
Train: 2018-07-31T09:18:17.057720: step 3772, loss 0.588891.
Train: 2018-07-31T09:18:17.213957: step 3773, loss 0.580128.
Train: 2018-07-31T09:18:17.370179: step 3774, loss 0.518378.
Train: 2018-07-31T09:18:17.510739: step 3775, loss 0.637712.
Train: 2018-07-31T09:18:17.666982: step 3776, loss 0.588914.
Train: 2018-07-31T09:18:17.838786: step 3777, loss 0.553648.
Train: 2018-07-31T09:18:17.995025: step 3778, loss 0.571295.
Train: 2018-07-31T09:18:18.151214: step 3779, loss 0.536145.
Train: 2018-07-31T09:18:18.307427: step 3780, loss 0.544914.
Test: 2018-07-31T09:18:18.541749: step 3780, loss 0.547777.
Train: 2018-07-31T09:18:18.697961: step 3781, loss 0.597484.
Train: 2018-07-31T09:18:18.854205: step 3782, loss 0.61493.
Train: 2018-07-31T09:18:19.010388: step 3783, loss 0.588603.
Train: 2018-07-31T09:18:19.166602: step 3784, loss 0.579804.
Train: 2018-07-31T09:18:19.322839: step 3785, loss 0.57976.
Train: 2018-07-31T09:18:19.479028: step 3786, loss 0.605639.
Train: 2018-07-31T09:18:19.635271: step 3787, loss 0.605494.
Train: 2018-07-31T09:18:19.791484: step 3788, loss 0.605282.
Train: 2018-07-31T09:18:19.947698: step 3789, loss 0.639252.
Train: 2018-07-31T09:18:20.103911: step 3790, loss 0.596389.
Test: 2018-07-31T09:18:20.338202: step 3790, loss 0.548292.
Train: 2018-07-31T09:18:20.494440: step 3791, loss 0.511795.
Train: 2018-07-31T09:18:20.650628: step 3792, loss 0.587664.
Train: 2018-07-31T09:18:20.822497: step 3793, loss 0.595922.
Train: 2018-07-31T09:18:20.963055: step 3794, loss 0.604118.
Train: 2018-07-31T09:18:21.119300: step 3795, loss 0.554224.
Train: 2018-07-31T09:18:21.275512: step 3796, loss 0.471758.
Train: 2018-07-31T09:18:21.431695: step 3797, loss 0.49663.
Train: 2018-07-31T09:18:21.587909: step 3798, loss 0.587314.
Train: 2018-07-31T09:18:21.744152: step 3799, loss 0.595565.
Train: 2018-07-31T09:18:21.931579: step 3800, loss 0.529603.
Test: 2018-07-31T09:18:22.181525: step 3800, loss 0.548756.
Train: 2018-07-31T09:18:22.884480: step 3801, loss 0.471847.
Train: 2018-07-31T09:18:23.040724: step 3802, loss 0.587348.
Train: 2018-07-31T09:18:23.196941: step 3803, loss 0.529388.
Train: 2018-07-31T09:18:23.353151: step 3804, loss 0.520964.
Train: 2018-07-31T09:18:23.509364: step 3805, loss 0.587494.
Train: 2018-07-31T09:18:23.665548: step 3806, loss 0.495592.
Train: 2018-07-31T09:18:23.821761: step 3807, loss 0.503696.
Train: 2018-07-31T09:18:23.978002: step 3808, loss 0.553989.
Train: 2018-07-31T09:18:24.134189: step 3809, loss 0.630196.
Train: 2018-07-31T09:18:24.290425: step 3810, loss 0.553877.
Test: 2018-07-31T09:18:24.540342: step 3810, loss 0.548125.
Train: 2018-07-31T09:18:24.743420: step 3811, loss 0.587966.
Train: 2018-07-31T09:18:24.899632: step 3812, loss 0.502625.
Train: 2018-07-31T09:18:25.055877: step 3813, loss 0.476714.
Train: 2018-07-31T09:18:25.212060: step 3814, loss 0.553764.
Train: 2018-07-31T09:18:25.368310: step 3815, loss 0.588338.
Train: 2018-07-31T09:18:25.524517: step 3816, loss 0.510354.
Train: 2018-07-31T09:18:25.680731: step 3817, loss 0.501525.
Train: 2018-07-31T09:18:25.836954: step 3818, loss 0.536191.
Train: 2018-07-31T09:18:25.993152: step 3819, loss 0.615168.
Train: 2018-07-31T09:18:26.149370: step 3820, loss 0.500703.
Test: 2018-07-31T09:18:26.383691: step 3820, loss 0.547689.
Train: 2018-07-31T09:18:26.539899: step 3821, loss 0.527146.
Train: 2018-07-31T09:18:26.696113: step 3822, loss 0.49148.
Train: 2018-07-31T09:18:26.852331: step 3823, loss 0.562576.
Train: 2018-07-31T09:18:27.008540: step 3824, loss 0.544666.
Train: 2018-07-31T09:18:27.164758: step 3825, loss 0.598699.
Train: 2018-07-31T09:18:27.320942: step 3826, loss 0.571777.
Train: 2018-07-31T09:18:27.477155: step 3827, loss 0.571685.
Train: 2018-07-31T09:18:27.633398: step 3828, loss 0.544643.
Train: 2018-07-31T09:18:27.789582: step 3829, loss 0.517407.
Train: 2018-07-31T09:18:27.945830: step 3830, loss 0.535525.
Test: 2018-07-31T09:18:28.180146: step 3830, loss 0.54757.
Train: 2018-07-31T09:18:28.336358: step 3831, loss 0.544675.
Train: 2018-07-31T09:18:28.508199: step 3832, loss 0.535414.
Train: 2018-07-31T09:18:28.664376: step 3833, loss 0.544713.
Train: 2018-07-31T09:18:28.820623: step 3834, loss 0.553759.
Train: 2018-07-31T09:18:28.976828: step 3835, loss 0.52633.
Train: 2018-07-31T09:18:29.133042: step 3836, loss 0.553696.
Train: 2018-07-31T09:18:29.289261: step 3837, loss 0.51696.
Train: 2018-07-31T09:18:29.445477: step 3838, loss 0.599805.
Train: 2018-07-31T09:18:29.586066: step 3839, loss 0.590599.
Train: 2018-07-31T09:18:29.742250: step 3840, loss 0.563086.
Test: 2018-07-31T09:18:29.992191: step 3840, loss 0.547572.
Train: 2018-07-31T09:18:30.148405: step 3841, loss 0.562963.
Train: 2018-07-31T09:18:30.304642: step 3842, loss 0.608828.
Train: 2018-07-31T09:18:30.460831: step 3843, loss 0.581283.
Train: 2018-07-31T09:18:30.617075: step 3844, loss 0.626874.
Train: 2018-07-31T09:18:30.773258: step 3845, loss 0.544569.
Train: 2018-07-31T09:18:30.929502: step 3846, loss 0.535539.
Train: 2018-07-31T09:18:31.085720: step 3847, loss 0.490325.
Train: 2018-07-31T09:18:31.241928: step 3848, loss 0.653213.
Train: 2018-07-31T09:18:31.398142: step 3849, loss 0.490564.
Train: 2018-07-31T09:18:31.569970: step 3850, loss 0.562643.
Test: 2018-07-31T09:18:31.804266: step 3850, loss 0.547601.
Train: 2018-07-31T09:18:31.960509: step 3851, loss 0.598578.
Train: 2018-07-31T09:18:32.116724: step 3852, loss 0.598474.
Train: 2018-07-31T09:18:32.272937: step 3853, loss 0.464283.
Train: 2018-07-31T09:18:32.429121: step 3854, loss 0.642826.
Train: 2018-07-31T09:18:32.585358: step 3855, loss 0.571461.
Train: 2018-07-31T09:18:32.741577: step 3856, loss 0.49148.
Train: 2018-07-31T09:18:32.897760: step 3857, loss 0.606805.
Train: 2018-07-31T09:18:33.053999: step 3858, loss 0.553652.
Train: 2018-07-31T09:18:33.194599: step 3859, loss 0.527144.
Train: 2018-07-31T09:18:33.350809: step 3860, loss 0.571288.
Test: 2018-07-31T09:18:33.585124: step 3860, loss 0.547726.
Train: 2018-07-31T09:18:33.756934: step 3861, loss 0.580071.
Train: 2018-07-31T09:18:33.913147: step 3862, loss 0.5449.
Train: 2018-07-31T09:18:34.069362: step 3863, loss 0.61512.
Train: 2018-07-31T09:18:34.225574: step 3864, loss 0.597471.
Train: 2018-07-31T09:18:34.381818: step 3865, loss 0.492621.
Train: 2018-07-31T09:18:34.553653: step 3866, loss 0.597298.
Train: 2018-07-31T09:18:34.709860: step 3867, loss 0.579888.
Train: 2018-07-31T09:18:34.866074: step 3868, loss 0.562358.
Train: 2018-07-31T09:18:35.022293: step 3869, loss 0.5277.
Train: 2018-07-31T09:18:35.178477: step 3870, loss 0.579713.
Test: 2018-07-31T09:18:35.412800: step 3870, loss 0.547919.
Train: 2018-07-31T09:18:35.569011: step 3871, loss 0.527816.
Train: 2018-07-31T09:18:35.725253: step 3872, loss 0.545149.
Train: 2018-07-31T09:18:35.881467: step 3873, loss 0.553711.
Train: 2018-07-31T09:18:36.037681: step 3874, loss 0.56245.
Train: 2018-07-31T09:18:36.209509: step 3875, loss 0.657433.
Train: 2018-07-31T09:18:36.365729: step 3876, loss 0.545129.
Train: 2018-07-31T09:18:36.537534: step 3877, loss 0.493498.
Train: 2018-07-31T09:18:36.678126: step 3878, loss 0.570986.
Train: 2018-07-31T09:18:36.834368: step 3879, loss 0.622673.
Train: 2018-07-31T09:18:36.990552: step 3880, loss 0.631128.
Test: 2018-07-31T09:18:37.240527: step 3880, loss 0.548041.
Train: 2018-07-31T09:18:37.396731: step 3881, loss 0.468099.
Train: 2018-07-31T09:18:37.552951: step 3882, loss 0.528133.
Train: 2018-07-31T09:18:37.709168: step 3883, loss 0.596691.
Train: 2018-07-31T09:18:37.865377: step 3884, loss 0.510958.
Train: 2018-07-31T09:18:38.021591: step 3885, loss 0.53674.
Train: 2018-07-31T09:18:38.177805: step 3886, loss 0.528044.
Train: 2018-07-31T09:18:38.334017: step 3887, loss 0.536581.
Train: 2018-07-31T09:18:38.490231: step 3888, loss 0.605451.
Train: 2018-07-31T09:18:38.646445: step 3889, loss 0.622748.
Train: 2018-07-31T09:18:38.802627: step 3890, loss 0.631393.
Test: 2018-07-31T09:18:39.036982: step 3890, loss 0.547985.
Train: 2018-07-31T09:18:39.193186: step 3891, loss 0.622635.
Train: 2018-07-31T09:18:39.349405: step 3892, loss 0.596739.
Train: 2018-07-31T09:18:39.505618: step 3893, loss 0.630891.
Train: 2018-07-31T09:18:39.661826: step 3894, loss 0.528354.
Train: 2018-07-31T09:18:39.833661: step 3895, loss 0.587929.
Train: 2018-07-31T09:18:39.989880: step 3896, loss 0.562419.
Train: 2018-07-31T09:18:40.146087: step 3897, loss 0.562454.
Train: 2018-07-31T09:18:40.317923: step 3898, loss 0.562463.
Train: 2018-07-31T09:18:40.458490: step 3899, loss 0.612758.
Train: 2018-07-31T09:18:40.614733: step 3900, loss 0.512262.
Test: 2018-07-31T09:18:40.864644: step 3900, loss 0.548496.
Train: 2018-07-31T09:18:41.598849: step 3901, loss 0.554101.
Train: 2018-07-31T09:18:41.755091: step 3902, loss 0.50409.
Train: 2018-07-31T09:18:41.911305: step 3903, loss 0.504056.
Train: 2018-07-31T09:18:42.067513: step 3904, loss 0.529038.
Train: 2018-07-31T09:18:42.223732: step 3905, loss 0.604346.
Train: 2018-07-31T09:18:42.379914: step 3906, loss 0.579246.
Train: 2018-07-31T09:18:42.551750: step 3907, loss 0.570843.
Train: 2018-07-31T09:18:42.707964: step 3908, loss 0.545644.
Train: 2018-07-31T09:18:42.864206: step 3909, loss 0.60457.
Train: 2018-07-31T09:18:43.020421: step 3910, loss 0.545583.
Test: 2018-07-31T09:18:43.254712: step 3910, loss 0.548319.
Train: 2018-07-31T09:18:43.410948: step 3911, loss 0.688848.
Train: 2018-07-31T09:18:43.582758: step 3912, loss 0.587657.
Train: 2018-07-31T09:18:43.723380: step 3913, loss 0.545683.
Train: 2018-07-31T09:18:43.879594: step 3914, loss 0.587563.
Train: 2018-07-31T09:18:44.035778: step 3915, loss 0.503904.
Train: 2018-07-31T09:18:44.192023: step 3916, loss 0.545788.
Train: 2018-07-31T09:18:44.348234: step 3917, loss 0.529033.
Train: 2018-07-31T09:18:44.504448: step 3918, loss 0.528968.
Train: 2018-07-31T09:18:44.660655: step 3919, loss 0.503756.
Train: 2018-07-31T09:18:44.816877: step 3920, loss 0.722283.
Test: 2018-07-31T09:18:45.051198: step 3920, loss 0.548364.
Train: 2018-07-31T09:18:45.207403: step 3921, loss 0.570854.
Train: 2018-07-31T09:18:45.363592: step 3922, loss 0.587627.
Train: 2018-07-31T09:18:45.535425: step 3923, loss 0.60443.
Train: 2018-07-31T09:18:45.691666: step 3924, loss 0.487074.
Train: 2018-07-31T09:18:45.847883: step 3925, loss 0.528973.
Train: 2018-07-31T09:18:45.988446: step 3926, loss 0.562467.
Train: 2018-07-31T09:18:46.144688: step 3927, loss 0.604411.
Train: 2018-07-31T09:18:46.300902: step 3928, loss 0.545652.
Train: 2018-07-31T09:18:46.457115: step 3929, loss 0.570855.
Train: 2018-07-31T09:18:46.613328: step 3930, loss 0.621279.
Test: 2018-07-31T09:18:46.847650: step 3930, loss 0.548385.
Train: 2018-07-31T09:18:47.003864: step 3931, loss 0.537264.
Train: 2018-07-31T09:18:47.160045: step 3932, loss 0.638001.
Train: 2018-07-31T09:18:47.300668: step 3933, loss 0.579229.
Train: 2018-07-31T09:18:47.472473: step 3934, loss 0.570817.
Train: 2018-07-31T09:18:47.628715: step 3935, loss 0.512344.
Train: 2018-07-31T09:18:47.784898: step 3936, loss 0.637656.
Train: 2018-07-31T09:18:47.941142: step 3937, loss 0.562473.
Train: 2018-07-31T09:18:48.097358: step 3938, loss 0.562501.
Train: 2018-07-31T09:18:48.253570: step 3939, loss 0.570826.
Train: 2018-07-31T09:18:48.409784: step 3940, loss 0.487718.
Test: 2018-07-31T09:18:48.644104: step 3940, loss 0.548575.
Train: 2018-07-31T09:18:48.800317: step 3941, loss 0.537558.
Train: 2018-07-31T09:18:48.956534: step 3942, loss 0.48754.
Train: 2018-07-31T09:18:49.112744: step 3943, loss 0.56246.
Train: 2018-07-31T09:18:49.284547: step 3944, loss 0.587572.
Train: 2018-07-31T09:18:49.440762: step 3945, loss 0.545656.
Train: 2018-07-31T09:18:49.581354: step 3946, loss 0.604522.
Train: 2018-07-31T09:18:49.737597: step 3947, loss 0.587709.
Train: 2018-07-31T09:18:49.909401: step 3948, loss 0.57085.
Train: 2018-07-31T09:18:50.050025: step 3949, loss 0.511787.
Train: 2018-07-31T09:18:50.221829: step 3950, loss 0.596244.
Test: 2018-07-31T09:18:50.456179: step 3950, loss 0.548244.
Train: 2018-07-31T09:18:50.612363: step 3951, loss 0.553956.
Train: 2018-07-31T09:18:50.768606: step 3952, loss 0.630201.
Train: 2018-07-31T09:18:50.924816: step 3953, loss 0.52008.
Train: 2018-07-31T09:18:51.081032: step 3954, loss 0.587838.
Train: 2018-07-31T09:18:51.252873: step 3955, loss 0.570854.
Train: 2018-07-31T09:18:51.409086: step 3956, loss 0.511611.
Train: 2018-07-31T09:18:51.549673: step 3957, loss 0.51154.
Train: 2018-07-31T09:18:51.705855: step 3958, loss 0.604882.
Train: 2018-07-31T09:18:51.862070: step 3959, loss 0.587918.
Train: 2018-07-31T09:18:52.018284: step 3960, loss 0.511364.
Test: 2018-07-31T09:18:52.268225: step 3960, loss 0.548128.
Train: 2018-07-31T09:18:52.424468: step 3961, loss 0.579459.
Train: 2018-07-31T09:18:52.580686: step 3962, loss 0.596502.
Train: 2018-07-31T09:18:52.721243: step 3963, loss 0.570911.
Train: 2018-07-31T09:18:52.893079: step 3964, loss 0.545344.
Train: 2018-07-31T09:18:53.049321: step 3965, loss 0.588006.
Train: 2018-07-31T09:18:53.189916: step 3966, loss 0.46853.
Train: 2018-07-31T09:18:53.361748: step 3967, loss 0.528174.
Train: 2018-07-31T09:18:53.517931: step 3968, loss 0.519542.
Train: 2018-07-31T09:18:53.674175: step 3969, loss 0.579587.
Train: 2018-07-31T09:18:53.830393: step 3970, loss 0.579656.
Test: 2018-07-31T09:18:54.064707: step 3970, loss 0.547942.
Train: 2018-07-31T09:18:54.220923: step 3971, loss 0.605562.
Train: 2018-07-31T09:18:54.392740: step 3972, loss 0.536511.
Train: 2018-07-31T09:18:54.548940: step 3973, loss 0.519161.
Train: 2018-07-31T09:18:54.689532: step 3974, loss 0.579761.
Train: 2018-07-31T09:18:54.845777: step 3975, loss 0.536385.
Train: 2018-07-31T09:18:55.001989: step 3976, loss 0.545029.
Train: 2018-07-31T09:18:55.158173: step 3977, loss 0.588506.
Train: 2018-07-31T09:18:55.330008: step 3978, loss 0.640823.
Train: 2018-07-31T09:18:55.470630: step 3979, loss 0.510213.
Train: 2018-07-31T09:18:55.642465: step 3980, loss 0.56243.
Test: 2018-07-31T09:18:55.876779: step 3980, loss 0.547836.
Train: 2018-07-31T09:18:56.032968: step 3981, loss 0.527592.
Train: 2018-07-31T09:18:56.189182: step 3982, loss 0.588572.
Train: 2018-07-31T09:18:56.345425: step 3983, loss 0.562445.
Train: 2018-07-31T09:18:56.501638: step 3984, loss 0.553703.
Train: 2018-07-31T09:18:56.673474: step 3985, loss 0.623444.
Train: 2018-07-31T09:18:56.814035: step 3986, loss 0.605966.
Train: 2018-07-31T09:18:56.970248: step 3987, loss 0.58848.
Train: 2018-07-31T09:18:57.142084: step 3988, loss 0.510408.
Train: 2018-07-31T09:18:57.298327: step 3989, loss 0.562396.
Train: 2018-07-31T09:18:57.470132: step 3990, loss 0.579692.
Test: 2018-07-31T09:18:57.704451: step 3990, loss 0.547936.
Train: 2018-07-31T09:18:57.860698: step 3991, loss 0.519213.
Train: 2018-07-31T09:18:58.016910: step 3992, loss 0.519232.
Train: 2018-07-31T09:18:58.173122: step 3993, loss 0.614229.
Train: 2018-07-31T09:18:58.329334: step 3994, loss 0.605572.
Train: 2018-07-31T09:18:58.485549: step 3995, loss 0.596873.
Train: 2018-07-31T09:18:58.641762: step 3996, loss 0.622593.
Train: 2018-07-31T09:18:58.797970: step 3997, loss 0.553818.
Train: 2018-07-31T09:18:58.954159: step 3998, loss 0.545296.
Train: 2018-07-31T09:18:59.110404: step 3999, loss 0.545326.
Train: 2018-07-31T09:18:59.266585: step 4000, loss 0.579447.
Test: 2018-07-31T09:18:59.500931: step 4000, loss 0.548153.
Train: 2018-07-31T09:19:00.203896: step 4001, loss 0.62196.
Train: 2018-07-31T09:19:00.360110: step 4002, loss 0.545427.
Train: 2018-07-31T09:19:00.516324: step 4003, loss 0.613234.
Train: 2018-07-31T09:19:00.688157: step 4004, loss 0.46954.
Train: 2018-07-31T09:19:00.828750: step 4005, loss 0.520213.
Train: 2018-07-31T09:19:01.000556: step 4006, loss 0.54553.
Train: 2018-07-31T09:19:01.156793: step 4007, loss 0.58778.
Train: 2018-07-31T09:19:01.313006: step 4008, loss 0.646977.
Train: 2018-07-31T09:19:01.469196: step 4009, loss 0.553977.
Train: 2018-07-31T09:19:01.625410: step 4010, loss 0.56242.
Test: 2018-07-31T09:19:01.859759: step 4010, loss 0.548315.
Train: 2018-07-31T09:19:02.015966: step 4011, loss 0.545584.
Train: 2018-07-31T09:19:02.187809: step 4012, loss 0.528749.
Train: 2018-07-31T09:19:02.344022: step 4013, loss 0.545568.
Train: 2018-07-31T09:19:02.500234: step 4014, loss 0.596181.
Train: 2018-07-31T09:19:02.672068: step 4015, loss 0.613074.
Train: 2018-07-31T09:19:02.812631: step 4016, loss 0.58774.
Train: 2018-07-31T09:19:02.968873: step 4017, loss 0.478213.
Train: 2018-07-31T09:19:03.140679: step 4018, loss 0.587725.
Train: 2018-07-31T09:19:03.312514: step 4019, loss 0.57928.
Train: 2018-07-31T09:19:03.468751: step 4020, loss 0.613009.
Test: 2018-07-31T09:19:03.703082: step 4020, loss 0.54833.
Train: 2018-07-31T09:19:03.859261: step 4021, loss 0.621385.
Train: 2018-07-31T09:19:03.999888: step 4022, loss 0.503629.
Train: 2018-07-31T09:19:04.171688: step 4023, loss 0.537248.
Train: 2018-07-31T09:19:04.312319: step 4024, loss 0.579246.
Train: 2018-07-31T09:19:04.484144: step 4025, loss 0.562446.
Train: 2018-07-31T09:19:04.640358: step 4026, loss 0.528848.
Train: 2018-07-31T09:19:04.780920: step 4027, loss 0.537226.
Train: 2018-07-31T09:19:04.937158: step 4028, loss 0.495102.
Train: 2018-07-31T09:19:05.093377: step 4029, loss 0.57931.
Train: 2018-07-31T09:19:05.265181: step 4030, loss 0.553959.
Test: 2018-07-31T09:19:05.515123: step 4030, loss 0.548212.
Train: 2018-07-31T09:19:05.655745: step 4031, loss 0.503056.
Train: 2018-07-31T09:19:05.811929: step 4032, loss 0.587932.
Train: 2018-07-31T09:19:05.968171: step 4033, loss 0.485654.
Train: 2018-07-31T09:19:06.124355: step 4034, loss 0.605202.
Train: 2018-07-31T09:19:06.296220: step 4035, loss 0.545234.
Train: 2018-07-31T09:19:06.452432: step 4036, loss 0.52797.
Train: 2018-07-31T09:19:06.592996: step 4037, loss 0.605562.
Train: 2018-07-31T09:19:06.749239: step 4038, loss 0.527808.
Train: 2018-07-31T09:19:06.905421: step 4039, loss 0.588413.
Train: 2018-07-31T09:19:07.061635: step 4040, loss 0.579772.
Test: 2018-07-31T09:19:07.295957: step 4040, loss 0.547861.
Train: 2018-07-31T09:19:07.467790: step 4041, loss 0.614561.
Train: 2018-07-31T09:19:07.624028: step 4042, loss 0.605853.
Train: 2018-07-31T09:19:07.780216: step 4043, loss 0.51902.
Train: 2018-07-31T09:19:07.936431: step 4044, loss 0.519029.
Train: 2018-07-31T09:19:08.092674: step 4045, loss 0.605819.
Train: 2018-07-31T09:19:08.248887: step 4046, loss 0.614481.
Train: 2018-07-31T09:19:08.405072: step 4047, loss 0.597067.
Train: 2018-07-31T09:19:08.561318: step 4048, loss 0.553757.
Train: 2018-07-31T09:19:08.717527: step 4049, loss 0.527867.
Train: 2018-07-31T09:19:08.873711: step 4050, loss 0.502018.
Test: 2018-07-31T09:19:09.108056: step 4050, loss 0.547948.
Train: 2018-07-31T09:19:09.264269: step 4051, loss 0.579657.
Train: 2018-07-31T09:19:09.420487: step 4052, loss 0.579654.
Train: 2018-07-31T09:19:09.576706: step 4053, loss 0.614158.
Train: 2018-07-31T09:19:09.732915: step 4054, loss 0.622694.
Train: 2018-07-31T09:19:09.889131: step 4055, loss 0.519439.
Train: 2018-07-31T09:19:10.045312: step 4056, loss 0.476598.
Train: 2018-07-31T09:19:10.201554: step 4057, loss 0.545226.
Train: 2018-07-31T09:19:10.357738: step 4058, loss 0.588167.
Train: 2018-07-31T09:19:10.513952: step 4059, loss 0.562404.
Train: 2018-07-31T09:19:10.670196: step 4060, loss 0.510828.
Test: 2018-07-31T09:19:10.904522: step 4060, loss 0.547986.
Train: 2018-07-31T09:19:11.045078: step 4061, loss 0.510775.
Train: 2018-07-31T09:19:11.201326: step 4062, loss 0.605512.
Train: 2018-07-31T09:19:11.357535: step 4063, loss 0.527873.
Train: 2018-07-31T09:19:11.513718: step 4064, loss 0.519181.
Train: 2018-07-31T09:19:11.669961: step 4065, loss 0.657724.
Train: 2018-07-31T09:19:11.826179: step 4066, loss 0.527752.
Train: 2018-07-31T09:19:11.982358: step 4067, loss 0.562413.
Train: 2018-07-31T09:19:12.138571: step 4068, loss 0.545061.
Train: 2018-07-31T09:19:12.294784: step 4069, loss 0.545045.
Train: 2018-07-31T09:19:12.451028: step 4070, loss 0.501569.
Test: 2018-07-31T09:19:12.685347: step 4070, loss 0.547834.
Train: 2018-07-31T09:19:12.857152: step 4071, loss 0.536295.
Train: 2018-07-31T09:19:13.013396: step 4072, loss 0.571166.
Train: 2018-07-31T09:19:13.169604: step 4073, loss 0.527454.
Train: 2018-07-31T09:19:13.325828: step 4074, loss 0.509849.
Train: 2018-07-31T09:19:13.482032: step 4075, loss 0.536086.
Train: 2018-07-31T09:19:13.638251: step 4076, loss 0.562487.
Train: 2018-07-31T09:19:13.794463: step 4077, loss 0.63797.
Train: 2018-07-31T09:19:13.950647: step 4078, loss 0.535958.
Train: 2018-07-31T09:19:14.106890: step 4079, loss 0.527077.
Train: 2018-07-31T09:19:14.263098: step 4080, loss 0.606893.
Test: 2018-07-31T09:19:14.497396: step 4080, loss 0.547666.
Train: 2018-07-31T09:19:14.653631: step 4081, loss 0.553658.
Train: 2018-07-31T09:19:14.809853: step 4082, loss 0.58029.
Train: 2018-07-31T09:19:14.966065: step 4083, loss 0.509274.
Train: 2018-07-31T09:19:15.122278: step 4084, loss 0.544772.
Train: 2018-07-31T09:19:15.278491: step 4085, loss 0.571435.
Train: 2018-07-31T09:19:15.434675: step 4086, loss 0.526977.
Train: 2018-07-31T09:19:15.590888: step 4087, loss 0.589253.
Train: 2018-07-31T09:19:15.747132: step 4088, loss 0.589256.
Train: 2018-07-31T09:19:15.918937: step 4089, loss 0.580339.
Train: 2018-07-31T09:19:16.059561: step 4090, loss 0.651403.
Test: 2018-07-31T09:19:16.309469: step 4090, loss 0.547678.
Train: 2018-07-31T09:19:16.465682: step 4091, loss 0.518214.
Train: 2018-07-31T09:19:16.606275: step 4092, loss 0.474058.
Train: 2018-07-31T09:19:16.762519: step 4093, loss 0.580191.
Train: 2018-07-31T09:19:16.918703: step 4094, loss 0.650892.
Train: 2018-07-31T09:19:17.074916: step 4095, loss 0.527211.
Train: 2018-07-31T09:19:17.246781: step 4096, loss 0.53606.
Train: 2018-07-31T09:19:17.402965: step 4097, loss 0.588855.
Train: 2018-07-31T09:19:17.559177: step 4098, loss 0.641497.
Train: 2018-07-31T09:19:17.731038: step 4099, loss 0.614974.
Train: 2018-07-31T09:19:17.887225: step 4100, loss 0.562428.
Test: 2018-07-31T09:19:18.105955: step 4100, loss 0.547862.
Train: 2018-07-31T09:19:18.808914: step 4101, loss 0.597172.
Train: 2018-07-31T09:19:18.965128: step 4102, loss 0.597022.
Train: 2018-07-31T09:19:19.121336: step 4103, loss 0.493468.
Train: 2018-07-31T09:19:19.277555: step 4104, loss 0.562397.
Train: 2018-07-31T09:19:19.449383: step 4105, loss 0.468079.
Train: 2018-07-31T09:19:19.605574: step 4106, loss 0.605261.
Train: 2018-07-31T09:19:19.761816: step 4107, loss 0.519574.
Train: 2018-07-31T09:19:19.918030: step 4108, loss 0.562396.
Train: 2018-07-31T09:19:20.089865: step 4109, loss 0.596653.
Train: 2018-07-31T09:19:20.246048: step 4110, loss 0.588071.
Test: 2018-07-31T09:19:20.480399: step 4110, loss 0.548081.
Train: 2018-07-31T09:19:20.636606: step 4111, loss 0.59659.
Train: 2018-07-31T09:19:20.792833: step 4112, loss 0.6136.
Train: 2018-07-31T09:19:20.949038: step 4113, loss 0.54538.
Train: 2018-07-31T09:19:21.105246: step 4114, loss 0.621867.
Train: 2018-07-31T09:19:21.261435: step 4115, loss 0.570883.
Train: 2018-07-31T09:19:21.417648: step 4116, loss 0.587758.
Train: 2018-07-31T09:19:21.589484: step 4117, loss 0.604533.
Train: 2018-07-31T09:19:21.745697: step 4118, loss 0.520502.
Train: 2018-07-31T09:19:21.901940: step 4119, loss 0.470375.
Train: 2018-07-31T09:19:22.042533: step 4120, loss 0.562461.
Test: 2018-07-31T09:19:22.292443: step 4120, loss 0.548433.
Train: 2018-07-31T09:19:22.448656: step 4121, loss 0.579209.
Train: 2018-07-31T09:19:22.604902: step 4122, loss 0.51221.
Train: 2018-07-31T09:19:22.761108: step 4123, loss 0.604378.
Train: 2018-07-31T09:19:22.917327: step 4124, loss 0.562452.
Train: 2018-07-31T09:19:23.089156: step 4125, loss 0.54567.
Train: 2018-07-31T09:19:23.245376: step 4126, loss 0.562446.
Train: 2018-07-31T09:19:23.401589: step 4127, loss 0.545636.
Train: 2018-07-31T09:19:23.557805: step 4128, loss 0.503542.
Train: 2018-07-31T09:19:23.713986: step 4129, loss 0.537125.
Train: 2018-07-31T09:19:23.870229: step 4130, loss 0.621623.
Test: 2018-07-31T09:19:24.120171: step 4130, loss 0.54823.
Train: 2018-07-31T09:19:24.276383: step 4131, loss 0.562414.
Train: 2018-07-31T09:19:24.432601: step 4132, loss 0.536972.
Train: 2018-07-31T09:19:24.588811: step 4133, loss 0.587889.
Train: 2018-07-31T09:19:24.760643: step 4134, loss 0.587914.
Train: 2018-07-31T09:19:24.916830: step 4135, loss 0.587924.
Train: 2018-07-31T09:19:25.073042: step 4136, loss 0.604935.
Train: 2018-07-31T09:19:25.229256: step 4137, loss 0.511417.
Train: 2018-07-31T09:19:25.369849: step 4138, loss 0.545404.
Train: 2018-07-31T09:19:25.526062: step 4139, loss 0.553898.
Train: 2018-07-31T09:19:25.682275: step 4140, loss 0.570915.
Test: 2018-07-31T09:19:25.916595: step 4140, loss 0.548136.
Train: 2018-07-31T09:19:26.088460: step 4141, loss 0.596472.
Train: 2018-07-31T09:19:26.244642: step 4142, loss 0.596467.
Train: 2018-07-31T09:19:26.400856: step 4143, loss 0.553894.
Train: 2018-07-31T09:19:26.557102: step 4144, loss 0.528385.
Train: 2018-07-31T09:19:26.713313: step 4145, loss 0.579418.
Train: 2018-07-31T09:19:26.869497: step 4146, loss 0.54539.
Train: 2018-07-31T09:19:27.025740: step 4147, loss 0.468799.
Train: 2018-07-31T09:19:27.181954: step 4148, loss 0.5624.
Train: 2018-07-31T09:19:27.338136: step 4149, loss 0.502566.
Train: 2018-07-31T09:19:27.494381: step 4150, loss 0.553821.
Test: 2018-07-31T09:19:27.728707: step 4150, loss 0.547993.
Train: 2018-07-31T09:19:27.884914: step 4151, loss 0.588198.
Train: 2018-07-31T09:19:28.041127: step 4152, loss 0.571017.
Train: 2018-07-31T09:19:28.197311: step 4153, loss 0.588303.
Train: 2018-07-31T09:19:28.353554: step 4154, loss 0.596974.
Train: 2018-07-31T09:19:28.509771: step 4155, loss 0.545114.
Train: 2018-07-31T09:19:28.665981: step 4156, loss 0.614293.
Train: 2018-07-31T09:19:28.837787: step 4157, loss 0.536475.
Train: 2018-07-31T09:19:29.009645: step 4158, loss 0.562401.
Train: 2018-07-31T09:19:29.165865: step 4159, loss 0.553762.
Train: 2018-07-31T09:19:29.306425: step 4160, loss 0.588322.
Test: 2018-07-31T09:19:29.556367: step 4160, loss 0.547938.
Train: 2018-07-31T09:19:29.712611: step 4161, loss 0.519223.
Train: 2018-07-31T09:19:29.868797: step 4162, loss 0.519208.
Train: 2018-07-31T09:19:30.025038: step 4163, loss 0.519159.
Train: 2018-07-31T09:19:30.196843: step 4164, loss 0.562407.
Train: 2018-07-31T09:19:30.353057: step 4165, loss 0.536372.
Train: 2018-07-31T09:19:30.509300: step 4166, loss 0.571115.
Train: 2018-07-31T09:19:30.665483: step 4167, loss 0.605975.
Train: 2018-07-31T09:19:30.821726: step 4168, loss 0.579852.
Train: 2018-07-31T09:19:30.977909: step 4169, loss 0.632134.
Train: 2018-07-31T09:19:31.134153: step 4170, loss 0.553718.
Test: 2018-07-31T09:19:31.368474: step 4170, loss 0.547862.
Train: 2018-07-31T09:19:31.540277: step 4171, loss 0.484209.
Train: 2018-07-31T09:19:31.696520: step 4172, loss 0.649345.
Train: 2018-07-31T09:19:31.852735: step 4173, loss 0.510331.
Train: 2018-07-31T09:19:32.008919: step 4174, loss 0.50167.
Train: 2018-07-31T09:19:32.165132: step 4175, loss 0.562413.
Train: 2018-07-31T09:19:32.321375: step 4176, loss 0.553724.
Train: 2018-07-31T09:19:32.477588: step 4177, loss 0.484132.
Train: 2018-07-31T09:19:32.633803: step 4178, loss 0.562426.
Train: 2018-07-31T09:19:32.790016: step 4179, loss 0.588637.
Train: 2018-07-31T09:19:32.946199: step 4180, loss 0.579927.
Test: 2018-07-31T09:19:33.180520: step 4180, loss 0.547787.
Train: 2018-07-31T09:19:33.352354: step 4181, loss 0.588689.
Train: 2018-07-31T09:19:33.492946: step 4182, loss 0.527446.
Train: 2018-07-31T09:19:33.649158: step 4183, loss 0.606207.
Train: 2018-07-31T09:19:33.820993: step 4184, loss 0.553691.
Train: 2018-07-31T09:19:33.977207: step 4185, loss 0.518712.
Train: 2018-07-31T09:19:34.133420: step 4186, loss 0.457449.
Train: 2018-07-31T09:19:34.289664: step 4187, loss 0.623845.
Train: 2018-07-31T09:19:34.445882: step 4188, loss 0.58879.
Train: 2018-07-31T09:19:34.602091: step 4189, loss 0.544899.
Train: 2018-07-31T09:19:34.758301: step 4190, loss 0.457073.
Test: 2018-07-31T09:19:35.008215: step 4190, loss 0.54773.
Train: 2018-07-31T09:19:35.164454: step 4191, loss 0.55367.
Train: 2018-07-31T09:19:35.320643: step 4192, loss 0.500738.
Train: 2018-07-31T09:19:35.476857: step 4193, loss 0.482877.
Train: 2018-07-31T09:19:35.633100: step 4194, loss 0.571422.
Train: 2018-07-31T09:19:35.789283: step 4195, loss 0.54474.
Train: 2018-07-31T09:19:35.945537: step 4196, loss 0.607307.
Train: 2018-07-31T09:19:36.101741: step 4197, loss 0.562615.
Train: 2018-07-31T09:19:36.273575: step 4198, loss 0.589543.
Train: 2018-07-31T09:19:36.429791: step 4199, loss 0.562637.
Train: 2018-07-31T09:19:36.586004: step 4200, loss 0.589576.
Test: 2018-07-31T09:19:36.820322: step 4200, loss 0.547605.
Train: 2018-07-31T09:19:37.585739: step 4201, loss 0.589559.
Train: 2018-07-31T09:19:37.741981: step 4202, loss 0.58055.
Train: 2018-07-31T09:19:37.898199: step 4203, loss 0.589458.
Train: 2018-07-31T09:19:38.054402: step 4204, loss 0.508997.
Train: 2018-07-31T09:19:38.226212: step 4205, loss 0.535811.
Train: 2018-07-31T09:19:38.382426: step 4206, loss 0.607153.
Train: 2018-07-31T09:19:38.538664: step 4207, loss 0.526947.
Train: 2018-07-31T09:19:38.694887: step 4208, loss 0.633701.
Train: 2018-07-31T09:19:38.851066: step 4209, loss 0.642385.
Train: 2018-07-31T09:19:39.007279: step 4210, loss 0.562499.
Test: 2018-07-31T09:19:39.241602: step 4210, loss 0.547726.
Train: 2018-07-31T09:19:39.397843: step 4211, loss 0.483215.
Train: 2018-07-31T09:19:39.554027: step 4212, loss 0.641581.
Train: 2018-07-31T09:19:39.710240: step 4213, loss 0.509878.
Train: 2018-07-31T09:19:39.866483: step 4214, loss 0.536207.
Train: 2018-07-31T09:19:40.022666: step 4215, loss 0.544969.
Train: 2018-07-31T09:19:40.178879: step 4216, loss 0.579872.
Train: 2018-07-31T09:19:40.335093: step 4217, loss 0.53629.
Train: 2018-07-31T09:19:40.491332: step 4218, loss 0.510194.
Train: 2018-07-31T09:19:40.631929: step 4219, loss 0.562421.
Train: 2018-07-31T09:19:40.788142: step 4220, loss 0.649513.
Test: 2018-07-31T09:19:41.038053: step 4220, loss 0.547855.
Train: 2018-07-31T09:19:41.194292: step 4221, loss 0.571112.
Train: 2018-07-31T09:19:41.350480: step 4222, loss 0.519012.
Train: 2018-07-31T09:19:41.506724: step 4223, loss 0.501694.
Train: 2018-07-31T09:19:41.662908: step 4224, loss 0.57109.
Train: 2018-07-31T09:19:41.819121: step 4225, loss 0.571092.
Train: 2018-07-31T09:19:41.975359: step 4226, loss 0.605814.
Train: 2018-07-31T09:19:42.131548: step 4227, loss 0.623116.
Train: 2018-07-31T09:19:42.287762: step 4228, loss 0.580866.
Train: 2018-07-31T09:19:42.443974: step 4229, loss 0.579667.
Train: 2018-07-31T09:19:42.600219: step 4230, loss 0.579621.
Test: 2018-07-31T09:19:42.834539: step 4230, loss 0.548011.
Train: 2018-07-31T09:19:42.990746: step 4231, loss 0.485093.
Train: 2018-07-31T09:19:43.146965: step 4232, loss 0.519483.
Train: 2018-07-31T09:19:43.318769: step 4233, loss 0.519474.
Train: 2018-07-31T09:19:43.475014: step 4234, loss 0.528022.
Train: 2018-07-31T09:19:43.631227: step 4235, loss 0.614043.
Train: 2018-07-31T09:19:43.787409: step 4236, loss 0.51073.
Train: 2018-07-31T09:19:43.943623: step 4237, loss 0.614139.
Train: 2018-07-31T09:19:44.099836: step 4238, loss 0.605524.
Train: 2018-07-31T09:19:44.256051: step 4239, loss 0.510687.
Train: 2018-07-31T09:19:44.427909: step 4240, loss 0.502046.
Test: 2018-07-31T09:19:44.662207: step 4240, loss 0.547939.
Train: 2018-07-31T09:19:44.818448: step 4241, loss 0.553765.
Train: 2018-07-31T09:19:44.974662: step 4242, loss 0.571052.
Train: 2018-07-31T09:19:45.130846: step 4243, loss 0.53643.
Train: 2018-07-31T09:19:45.287089: step 4244, loss 0.562409.
Train: 2018-07-31T09:19:45.443307: step 4245, loss 0.518995.
Train: 2018-07-31T09:19:45.615142: step 4246, loss 0.562419.
Train: 2018-07-31T09:19:45.771350: step 4247, loss 0.527559.
Train: 2018-07-31T09:19:45.927567: step 4248, loss 0.571169.
Train: 2018-07-31T09:19:46.083747: step 4249, loss 0.614945.
Train: 2018-07-31T09:19:46.239961: step 4250, loss 0.606214.
Test: 2018-07-31T09:19:46.474281: step 4250, loss 0.547788.
Train: 2018-07-31T09:19:46.630494: step 4251, loss 0.588687.
Train: 2018-07-31T09:19:46.786708: step 4252, loss 0.579912.
Train: 2018-07-31T09:19:46.942945: step 4253, loss 0.571155.
Train: 2018-07-31T09:19:47.099171: step 4254, loss 0.632116.
Train: 2018-07-31T09:19:47.255372: step 4255, loss 0.510296.
Train: 2018-07-31T09:19:47.411561: step 4256, loss 0.579753.
Train: 2018-07-31T09:19:47.567804: step 4257, loss 0.536438.
Train: 2018-07-31T09:19:47.708397: step 4258, loss 0.553757.
Train: 2018-07-31T09:19:47.864610: step 4259, loss 0.545127.
Train: 2018-07-31T09:19:48.036439: step 4260, loss 0.614195.
Test: 2018-07-31T09:19:48.270766: step 4260, loss 0.547964.
Train: 2018-07-31T09:19:48.426978: step 4261, loss 0.493447.
Train: 2018-07-31T09:19:48.583163: step 4262, loss 0.605492.
Train: 2018-07-31T09:19:48.739400: step 4263, loss 0.502122.
Train: 2018-07-31T09:19:48.895619: step 4264, loss 0.579626.
Train: 2018-07-31T09:19:49.051834: step 4265, loss 0.571012.
Train: 2018-07-31T09:19:49.223667: step 4266, loss 0.596846.
Train: 2018-07-31T09:19:49.379851: step 4267, loss 0.571001.
Train: 2018-07-31T09:19:49.520472: step 4268, loss 0.528015.
Train: 2018-07-31T09:19:49.676656: step 4269, loss 0.613956.
Train: 2018-07-31T09:19:49.832869: step 4270, loss 0.596726.
Test: 2018-07-31T09:19:50.067189: step 4270, loss 0.548049.
Train: 2018-07-31T09:19:50.223433: step 4271, loss 0.562396.
Train: 2018-07-31T09:19:50.379646: step 4272, loss 0.562397.
Train: 2018-07-31T09:19:50.535860: step 4273, loss 0.511163.
Train: 2018-07-31T09:19:50.692043: step 4274, loss 0.579472.
Train: 2018-07-31T09:19:50.863908: step 4275, loss 0.596527.
Train: 2018-07-31T09:19:51.020092: step 4276, loss 0.511269.
Train: 2018-07-31T09:19:51.176335: step 4277, loss 0.587967.
Train: 2018-07-31T09:19:51.316927: step 4278, loss 0.562401.
Train: 2018-07-31T09:19:51.488761: step 4279, loss 0.639033.
Train: 2018-07-31T09:19:51.644981: step 4280, loss 0.553908.
Test: 2018-07-31T09:19:51.879296: step 4280, loss 0.548199.
Train: 2018-07-31T09:19:52.035503: step 4281, loss 0.570894.
Train: 2018-07-31T09:19:52.191691: step 4282, loss 0.570885.
Train: 2018-07-31T09:19:52.347935: step 4283, loss 0.570877.
Train: 2018-07-31T09:19:52.519771: step 4284, loss 0.494846.
Train: 2018-07-31T09:19:52.675983: step 4285, loss 0.553972.
Train: 2018-07-31T09:19:52.832197: step 4286, loss 0.537062.
Train: 2018-07-31T09:19:52.988380: step 4287, loss 0.537032.
Train: 2018-07-31T09:19:53.144594: step 4288, loss 0.587836.
Train: 2018-07-31T09:19:53.300839: step 4289, loss 0.52.
Train: 2018-07-31T09:19:53.472678: step 4290, loss 0.519921.
Test: 2018-07-31T09:19:53.706992: step 4290, loss 0.548135.
Train: 2018-07-31T09:19:53.863176: step 4291, loss 0.562401.
Train: 2018-07-31T09:19:54.019419: step 4292, loss 0.519706.
Train: 2018-07-31T09:19:54.191248: step 4293, loss 0.52814.
Train: 2018-07-31T09:19:54.347467: step 4294, loss 0.528022.
Train: 2018-07-31T09:19:54.503682: step 4295, loss 0.588277.
Train: 2018-07-31T09:19:54.659865: step 4296, loss 0.614306.
Train: 2018-07-31T09:19:54.816077: step 4297, loss 0.501767.
Train: 2018-07-31T09:19:54.972320: step 4298, loss 0.571097.
Train: 2018-07-31T09:19:55.128535: step 4299, loss 0.588524.
Train: 2018-07-31T09:19:55.284718: step 4300, loss 0.501438.
Test: 2018-07-31T09:19:55.519069: step 4300, loss 0.547808.
Train: 2018-07-31T09:19:56.253240: step 4301, loss 0.623553.
Train: 2018-07-31T09:19:56.409484: step 4302, loss 0.606123.
Train: 2018-07-31T09:19:56.565668: step 4303, loss 0.51876.
Train: 2018-07-31T09:19:56.721911: step 4304, loss 0.553696.
Train: 2018-07-31T09:19:56.878124: step 4305, loss 0.588668.
Train: 2018-07-31T09:19:57.034308: step 4306, loss 0.518725.
Train: 2018-07-31T09:19:57.190551: step 4307, loss 0.492455.
Train: 2018-07-31T09:19:57.346765: step 4308, loss 0.544919.
Train: 2018-07-31T09:19:57.502949: step 4309, loss 0.483419.
Train: 2018-07-31T09:19:57.659162: step 4310, loss 0.536046.
Test: 2018-07-31T09:19:57.909103: step 4310, loss 0.547695.
Train: 2018-07-31T09:19:58.065346: step 4311, loss 0.55366.
Train: 2018-07-31T09:19:58.221530: step 4312, loss 0.589122.
Train: 2018-07-31T09:19:58.362122: step 4313, loss 0.491462.
Train: 2018-07-31T09:19:58.533956: step 4314, loss 0.580389.
Train: 2018-07-31T09:19:58.690170: step 4315, loss 0.58938.
Train: 2018-07-31T09:19:58.846383: step 4316, loss 0.59837.
Train: 2018-07-31T09:19:59.002598: step 4317, loss 0.517875.
Train: 2018-07-31T09:19:59.158811: step 4318, loss 0.517843.
Train: 2018-07-31T09:19:59.315053: step 4319, loss 0.517791.
Train: 2018-07-31T09:19:59.471267: step 4320, loss 0.553662.
Test: 2018-07-31T09:19:59.705588: step 4320, loss 0.547594.
Train: 2018-07-31T09:19:59.861771: step 4321, loss 0.652672.
Train: 2018-07-31T09:20:00.017984: step 4322, loss 0.607649.
Train: 2018-07-31T09:20:00.189854: step 4323, loss 0.544678.
Train: 2018-07-31T09:20:00.330441: step 4324, loss 0.661354.
Train: 2018-07-31T09:20:00.502246: step 4325, loss 0.571548.
Train: 2018-07-31T09:20:00.642838: step 4326, loss 0.61608.
Train: 2018-07-31T09:20:00.799052: step 4327, loss 0.580299.
Train: 2018-07-31T09:20:00.955265: step 4328, loss 0.589037.
Train: 2018-07-31T09:20:01.111509: step 4329, loss 0.632913.
Train: 2018-07-31T09:20:01.267691: step 4330, loss 0.544932.
Test: 2018-07-31T09:20:01.502043: step 4330, loss 0.54783.
Train: 2018-07-31T09:20:01.658224: step 4331, loss 0.510139.
Train: 2018-07-31T09:20:01.814469: step 4332, loss 0.588469.
Train: 2018-07-31T09:20:01.970652: step 4333, loss 0.579713.
Train: 2018-07-31T09:20:02.126895: step 4334, loss 0.519286.
Train: 2018-07-31T09:20:02.298700: step 4335, loss 0.5796.
Train: 2018-07-31T09:20:02.454943: step 4336, loss 0.536657.
Train: 2018-07-31T09:20:02.611127: step 4337, loss 0.502445.
Train: 2018-07-31T09:20:02.767365: step 4338, loss 0.579519.
Train: 2018-07-31T09:20:02.923554: step 4339, loss 0.630845.
Train: 2018-07-31T09:20:03.095389: step 4340, loss 0.528242.
Test: 2018-07-31T09:20:03.329739: step 4340, loss 0.548114.
Train: 2018-07-31T09:20:03.485948: step 4341, loss 0.468572.
Train: 2018-07-31T09:20:03.642135: step 4342, loss 0.519704.
Train: 2018-07-31T09:20:03.798349: step 4343, loss 0.528178.
Train: 2018-07-31T09:20:03.938976: step 4344, loss 0.562396.
Train: 2018-07-31T09:20:04.110808: step 4345, loss 0.553802.
Train: 2018-07-31T09:20:04.267013: step 4346, loss 0.553785.
Train: 2018-07-31T09:20:04.423236: step 4347, loss 0.52788.
Train: 2018-07-31T09:20:04.579415: step 4348, loss 0.631619.
Train: 2018-07-31T09:20:04.735630: step 4349, loss 0.605701.
Train: 2018-07-31T09:20:04.891879: step 4350, loss 0.562405.
Test: 2018-07-31T09:20:05.126163: step 4350, loss 0.547909.
Train: 2018-07-31T09:20:05.282407: step 4351, loss 0.579715.
Train: 2018-07-31T09:20:05.454241: step 4352, loss 0.622954.
Train: 2018-07-31T09:20:05.626076: step 4353, loss 0.640109.
Train: 2018-07-31T09:20:05.782289: step 4354, loss 0.571003.
Train: 2018-07-31T09:20:05.922881: step 4355, loss 0.519502.
Train: 2018-07-31T09:20:06.094716: step 4356, loss 0.53671.
Train: 2018-07-31T09:20:06.250930: step 4357, loss 0.545294.
Train: 2018-07-31T09:20:06.407113: step 4358, loss 0.528216.
Train: 2018-07-31T09:20:06.563326: step 4359, loss 0.553851.
Train: 2018-07-31T09:20:06.719539: step 4360, loss 0.60514.
Test: 2018-07-31T09:20:06.953891: step 4360, loss 0.54809.
Train: 2018-07-31T09:20:07.110104: step 4361, loss 0.588027.
Train: 2018-07-31T09:20:07.281919: step 4362, loss 0.622136.
Train: 2018-07-31T09:20:07.422501: step 4363, loss 0.587947.
Train: 2018-07-31T09:20:07.594334: step 4364, loss 0.621861.
Train: 2018-07-31T09:20:07.750549: step 4365, loss 0.528561.
Train: 2018-07-31T09:20:07.906787: step 4366, loss 0.579311.
Train: 2018-07-31T09:20:08.062975: step 4367, loss 0.554009.
Train: 2018-07-31T09:20:08.219221: step 4368, loss 0.520405.
Train: 2018-07-31T09:20:08.375401: step 4369, loss 0.646456.
Train: 2018-07-31T09:20:08.547237: step 4370, loss 0.570837.
Test: 2018-07-31T09:20:08.781595: step 4370, loss 0.548462.
Train: 2018-07-31T09:20:08.937771: step 4371, loss 0.579193.
Train: 2018-07-31T09:20:09.094014: step 4372, loss 0.554137.
Train: 2018-07-31T09:20:09.250227: step 4373, loss 0.529172.
Train: 2018-07-31T09:20:09.406411: step 4374, loss 0.604117.
Train: 2018-07-31T09:20:09.562625: step 4375, loss 0.537566.
Train: 2018-07-31T09:20:09.718873: step 4376, loss 0.537583.
Train: 2018-07-31T09:20:09.875082: step 4377, loss 0.504333.
Train: 2018-07-31T09:20:10.031264: step 4378, loss 0.579144.
Train: 2018-07-31T09:20:10.171888: step 4379, loss 0.615831.
Train: 2018-07-31T09:20:10.328100: step 4380, loss 0.537479.
Test: 2018-07-31T09:20:10.562390: step 4380, loss 0.54851.
Train: 2018-07-31T09:20:10.718628: step 4381, loss 0.554139.
Train: 2018-07-31T09:20:10.874847: step 4382, loss 0.504018.
Train: 2018-07-31T09:20:11.046682: step 4383, loss 0.52898.
Train: 2018-07-31T09:20:11.187243: step 4384, loss 0.570843.
Train: 2018-07-31T09:20:11.343458: step 4385, loss 0.587687.
Train: 2018-07-31T09:20:11.499700: step 4386, loss 0.520264.
Train: 2018-07-31T09:20:11.655915: step 4387, loss 0.511687.
Train: 2018-07-31T09:20:11.812098: step 4388, loss 0.596351.
Train: 2018-07-31T09:20:11.968310: step 4389, loss 0.579417.
Train: 2018-07-31T09:20:12.124524: step 4390, loss 0.443073.
Test: 2018-07-31T09:20:12.374467: step 4390, loss 0.54806.
Train: 2018-07-31T09:20:12.530679: step 4391, loss 0.553836.
Train: 2018-07-31T09:20:12.671272: step 4392, loss 0.450665.
Train: 2018-07-31T09:20:12.827485: step 4393, loss 0.519174.
Train: 2018-07-31T09:20:12.999319: step 4394, loss 0.527622.
Train: 2018-07-31T09:20:13.155532: step 4395, loss 0.684976.
Train: 2018-07-31T09:20:13.311746: step 4396, loss 0.518566.
Train: 2018-07-31T09:20:13.452368: step 4397, loss 0.580092.
Train: 2018-07-31T09:20:13.624203: step 4398, loss 0.588986.
Train: 2018-07-31T09:20:13.780387: step 4399, loss 0.63327.
Train: 2018-07-31T09:20:13.936634: step 4400, loss 0.518278.
Test: 2018-07-31T09:20:14.170922: step 4400, loss 0.547685.
Train: 2018-07-31T09:20:14.920745: step 4401, loss 0.535955.
Train: 2018-07-31T09:20:15.076958: step 4402, loss 0.589098.
Train: 2018-07-31T09:20:15.233201: step 4403, loss 0.491619.
Train: 2018-07-31T09:20:15.389414: step 4404, loss 0.606906.
Train: 2018-07-31T09:20:15.545628: step 4405, loss 0.598043.
Train: 2018-07-31T09:20:15.701841: step 4406, loss 0.60689.
Train: 2018-07-31T09:20:15.842433: step 4407, loss 0.544798.
Train: 2018-07-31T09:20:16.014239: step 4408, loss 0.527115.
Train: 2018-07-31T09:20:16.186108: step 4409, loss 0.624405.
Train: 2018-07-31T09:20:16.342287: step 4410, loss 0.509529.
Test: 2018-07-31T09:20:16.576637: step 4410, loss 0.547713.
Train: 2018-07-31T09:20:16.732850: step 4411, loss 0.633048.
Train: 2018-07-31T09:20:16.889069: step 4412, loss 0.571273.
Train: 2018-07-31T09:20:17.045246: step 4413, loss 0.57124.
Train: 2018-07-31T09:20:17.201461: step 4414, loss 0.536164.
Train: 2018-07-31T09:20:17.357709: step 4415, loss 0.579931.
Train: 2018-07-31T09:20:17.513911: step 4416, loss 0.597346.
Train: 2018-07-31T09:20:17.670101: step 4417, loss 0.588541.
Train: 2018-07-31T09:20:17.826349: step 4418, loss 0.623183.
Train: 2018-07-31T09:20:17.982528: step 4419, loss 0.596994.
Train: 2018-07-31T09:20:18.138742: step 4420, loss 0.562397.
Test: 2018-07-31T09:20:18.373092: step 4420, loss 0.548031.
Train: 2018-07-31T09:20:18.529305: step 4421, loss 0.553818.
Train: 2018-07-31T09:20:18.685488: step 4422, loss 0.596592.
Train: 2018-07-31T09:20:18.841725: step 4423, loss 0.485748.
Train: 2018-07-31T09:20:18.997915: step 4424, loss 0.562405.
Train: 2018-07-31T09:20:19.154155: step 4425, loss 0.519958.
Train: 2018-07-31T09:20:19.310341: step 4426, loss 0.528462.
Train: 2018-07-31T09:20:19.466590: step 4427, loss 0.494485.
Train: 2018-07-31T09:20:19.622804: step 4428, loss 0.596429.
Train: 2018-07-31T09:20:19.778982: step 4429, loss 0.570917.
Train: 2018-07-31T09:20:19.935194: step 4430, loss 0.545358.
Test: 2018-07-31T09:20:20.169516: step 4430, loss 0.548113.
Train: 2018-07-31T09:20:20.325759: step 4431, loss 0.562399.
Train: 2018-07-31T09:20:20.481972: step 4432, loss 0.45993.
Train: 2018-07-31T09:20:20.638155: step 4433, loss 0.622352.
Train: 2018-07-31T09:20:20.794399: step 4434, loss 0.510927.
Train: 2018-07-31T09:20:20.950582: step 4435, loss 0.519402.
Train: 2018-07-31T09:20:21.106795: step 4436, loss 0.493397.
Train: 2018-07-31T09:20:21.278631: step 4437, loss 0.553745.
Train: 2018-07-31T09:20:21.434844: step 4438, loss 0.536334.
Train: 2018-07-31T09:20:21.591056: step 4439, loss 0.606071.
Train: 2018-07-31T09:20:21.747271: step 4440, loss 0.588692.
Test: 2018-07-31T09:20:22.028456: step 4440, loss 0.54777.
Train: 2018-07-31T09:20:22.184698: step 4441, loss 0.527389.
Train: 2018-07-31T09:20:22.340913: step 4442, loss 0.650294.
Train: 2018-07-31T09:20:22.512716: step 4443, loss 0.500974.
Train: 2018-07-31T09:20:22.653309: step 4444, loss 0.536085.
Train: 2018-07-31T09:20:22.809557: step 4445, loss 0.536057.
Train: 2018-07-31T09:20:22.965736: step 4446, loss 0.456632.
Train: 2018-07-31T09:20:23.121949: step 4447, loss 0.5094.
Train: 2018-07-31T09:20:23.278188: step 4448, loss 0.59809.
Train: 2018-07-31T09:20:23.434376: step 4449, loss 0.616033.
Train: 2018-07-31T09:20:23.590619: step 4450, loss 0.589344.
Test: 2018-07-31T09:20:23.824940: step 4450, loss 0.547631.
Train: 2018-07-31T09:20:23.981122: step 4451, loss 0.500095.
Train: 2018-07-31T09:20:24.152987: step 4452, loss 0.589411.
Train: 2018-07-31T09:20:24.309171: step 4453, loss 0.580487.
Train: 2018-07-31T09:20:24.465408: step 4454, loss 0.499993.
Train: 2018-07-31T09:20:24.621598: step 4455, loss 0.616325.
Train: 2018-07-31T09:20:24.809055: step 4456, loss 0.670001.
Train: 2018-07-31T09:20:24.965270: step 4457, loss 0.571508.
Train: 2018-07-31T09:20:25.121481: step 4458, loss 0.580364.
Train: 2018-07-31T09:20:25.277694: step 4459, loss 0.589165.
Train: 2018-07-31T09:20:25.433908: step 4460, loss 0.624446.
Test: 2018-07-31T09:20:25.668229: step 4460, loss 0.547723.
Train: 2018-07-31T09:20:25.871306: step 4461, loss 0.562478.
Train: 2018-07-31T09:20:26.027549: step 4462, loss 0.606329.
Train: 2018-07-31T09:20:26.183757: step 4463, loss 0.571168.
Train: 2018-07-31T09:20:26.339964: step 4464, loss 0.571113.
Train: 2018-07-31T09:20:26.496189: step 4465, loss 0.597042.
Train: 2018-07-31T09:20:26.668018: step 4466, loss 0.596876.
Train: 2018-07-31T09:20:26.824231: step 4467, loss 0.631015.
Train: 2018-07-31T09:20:26.964798: step 4468, loss 0.613564.
Train: 2018-07-31T09:20:27.121042: step 4469, loss 0.545465.
Train: 2018-07-31T09:20:27.277226: step 4470, loss 0.570859.
Test: 2018-07-31T09:20:27.527167: step 4470, loss 0.548405.
Train: 2018-07-31T09:20:27.683415: step 4471, loss 0.520515.
Train: 2018-07-31T09:20:27.839624: step 4472, loss 0.554112.
Train: 2018-07-31T09:20:27.995837: step 4473, loss 0.612499.
Train: 2018-07-31T09:20:28.152051: step 4474, loss 0.512671.
Train: 2018-07-31T09:20:28.308264: step 4475, loss 0.570814.
Train: 2018-07-31T09:20:28.464448: step 4476, loss 0.587367.
Train: 2018-07-31T09:20:28.620697: step 4477, loss 0.529508.
Train: 2018-07-31T09:20:28.776905: step 4478, loss 0.54605.
Train: 2018-07-31T09:20:28.933123: step 4479, loss 0.504791.
Train: 2018-07-31T09:20:29.089326: step 4480, loss 0.512967.
Test: 2018-07-31T09:20:29.323622: step 4480, loss 0.548656.
Train: 2018-07-31T09:20:29.479835: step 4481, loss 0.56253.
Train: 2018-07-31T09:20:29.636079: step 4482, loss 0.57912.
Train: 2018-07-31T09:20:29.792261: step 4483, loss 0.570819.
Train: 2018-07-31T09:20:29.948475: step 4484, loss 0.545819.
Train: 2018-07-31T09:20:30.104719: step 4485, loss 0.570826.
Train: 2018-07-31T09:20:30.260901: step 4486, loss 0.529001.
Train: 2018-07-31T09:20:30.417146: step 4487, loss 0.537293.
Train: 2018-07-31T09:20:30.588980: step 4488, loss 0.545618.
Train: 2018-07-31T09:20:30.745188: step 4489, loss 0.579299.
Train: 2018-07-31T09:20:30.901377: step 4490, loss 0.655439.
Test: 2018-07-31T09:20:31.135728: step 4490, loss 0.548252.
Train: 2018-07-31T09:20:31.291943: step 4491, loss 0.537042.
Train: 2018-07-31T09:20:31.448154: step 4492, loss 0.570881.
Train: 2018-07-31T09:20:31.604367: step 4493, loss 0.570884.
Train: 2018-07-31T09:20:31.760550: step 4494, loss 0.511563.
Train: 2018-07-31T09:20:31.916797: step 4495, loss 0.545432.
Train: 2018-07-31T09:20:32.073008: step 4496, loss 0.562404.
Train: 2018-07-31T09:20:32.229192: step 4497, loss 0.553884.
Train: 2018-07-31T09:20:32.385404: step 4498, loss 0.502675.
Train: 2018-07-31T09:20:32.526021: step 4499, loss 0.59662.
Train: 2018-07-31T09:20:32.682244: step 4500, loss 0.570967.
Test: 2018-07-31T09:20:32.916530: step 4500, loss 0.548019.
Train: 2018-07-31T09:20:33.681977: step 4501, loss 0.605317.
Train: 2018-07-31T09:20:33.838219: step 4502, loss 0.485106.
Train: 2018-07-31T09:20:33.994432: step 4503, loss 0.527978.
Train: 2018-07-31T09:20:34.150647: step 4504, loss 0.545147.
Train: 2018-07-31T09:20:34.322482: step 4505, loss 0.562403.
Train: 2018-07-31T09:20:34.478665: step 4506, loss 0.562408.
Train: 2018-07-31T09:20:34.650498: step 4507, loss 0.614521.
Train: 2018-07-31T09:20:34.806712: step 4508, loss 0.588486.
Train: 2018-07-31T09:20:34.947335: step 4509, loss 0.510272.
Train: 2018-07-31T09:20:35.119140: step 4510, loss 0.571117.
Test: 2018-07-31T09:20:35.353485: step 4510, loss 0.547842.
Train: 2018-07-31T09:20:35.509697: step 4511, loss 0.527601.
Train: 2018-07-31T09:20:35.665919: step 4512, loss 0.597286.
Train: 2018-07-31T09:20:35.822130: step 4513, loss 0.623454.
Train: 2018-07-31T09:20:35.978314: step 4514, loss 0.562422.
Train: 2018-07-31T09:20:36.134551: step 4515, loss 0.57112.
Train: 2018-07-31T09:20:36.290773: step 4516, loss 0.536342.
Train: 2018-07-31T09:20:36.446978: step 4517, loss 0.527668.
Train: 2018-07-31T09:20:36.603197: step 4518, loss 0.562414.
Train: 2018-07-31T09:20:36.759389: step 4519, loss 0.623232.
Train: 2018-07-31T09:20:36.899973: step 4520, loss 0.562411.
Test: 2018-07-31T09:20:37.134292: step 4520, loss 0.547892.
Train: 2018-07-31T09:20:37.290536: step 4521, loss 0.562408.
Train: 2018-07-31T09:20:37.446750: step 4522, loss 0.597038.
Train: 2018-07-31T09:20:37.602957: step 4523, loss 0.562402.
Train: 2018-07-31T09:20:37.759147: step 4524, loss 0.519253.
Train: 2018-07-31T09:20:37.915392: step 4525, loss 0.60552.
Train: 2018-07-31T09:20:38.087219: step 4526, loss 0.605456.
Train: 2018-07-31T09:20:38.227811: step 4527, loss 0.562396.
Train: 2018-07-31T09:20:38.384033: step 4528, loss 0.588122.
Train: 2018-07-31T09:20:38.555860: step 4529, loss 0.588063.
Train: 2018-07-31T09:20:38.696452: step 4530, loss 0.580602.
Test: 2018-07-31T09:20:38.930747: step 4530, loss 0.548152.
Train: 2018-07-31T09:20:39.102582: step 4531, loss 0.528365.
Train: 2018-07-31T09:20:39.258825: step 4532, loss 0.630364.
Train: 2018-07-31T09:20:39.415008: step 4533, loss 0.579353.
Train: 2018-07-31T09:20:39.571221: step 4534, loss 0.494867.
Train: 2018-07-31T09:20:39.727435: step 4535, loss 0.570862.
Train: 2018-07-31T09:20:39.883679: step 4536, loss 0.486603.
Train: 2018-07-31T09:20:40.055484: step 4537, loss 0.537135.
Train: 2018-07-31T09:20:40.211728: step 4538, loss 0.494888.
Train: 2018-07-31T09:20:40.367911: step 4539, loss 0.562416.
Train: 2018-07-31T09:20:40.524154: step 4540, loss 0.545438.
Test: 2018-07-31T09:20:40.758443: step 4540, loss 0.548155.
Train: 2018-07-31T09:20:40.914688: step 4541, loss 0.596435.
Train: 2018-07-31T09:20:41.070901: step 4542, loss 0.553878.
Train: 2018-07-31T09:20:41.227083: step 4543, loss 0.596545.
Train: 2018-07-31T09:20:41.383328: step 4544, loss 0.579485.
Train: 2018-07-31T09:20:41.539543: step 4545, loss 0.596588.
Train: 2018-07-31T09:20:41.695749: step 4546, loss 0.553853.
Train: 2018-07-31T09:20:41.851938: step 4547, loss 0.553853.
Train: 2018-07-31T09:20:42.023802: step 4548, loss 0.639302.
Train: 2018-07-31T09:20:42.180016: step 4549, loss 0.570931.
Train: 2018-07-31T09:20:42.336223: step 4550, loss 0.587959.
Test: 2018-07-31T09:20:42.570544: step 4550, loss 0.548163.
Train: 2018-07-31T09:20:42.726763: step 4551, loss 0.579411.
Train: 2018-07-31T09:20:42.882976: step 4552, loss 0.494519.
Train: 2018-07-31T09:20:43.039193: step 4553, loss 0.536959.
Train: 2018-07-31T09:20:43.195374: step 4554, loss 0.587868.
Train: 2018-07-31T09:20:43.351587: step 4555, loss 0.553925.
Train: 2018-07-31T09:20:43.507830: step 4556, loss 0.579379.
Train: 2018-07-31T09:20:43.664046: step 4557, loss 0.545444.
Train: 2018-07-31T09:20:43.835849: step 4558, loss 0.519987.
Train: 2018-07-31T09:20:43.992092: step 4559, loss 0.545419.
Train: 2018-07-31T09:20:44.148275: step 4560, loss 0.61344.
Test: 2018-07-31T09:20:44.382596: step 4560, loss 0.548154.
Train: 2018-07-31T09:20:44.538809: step 4561, loss 0.621959.
Train: 2018-07-31T09:20:44.695053: step 4562, loss 0.587903.
Train: 2018-07-31T09:20:44.866887: step 4563, loss 0.638792.
Train: 2018-07-31T09:20:45.023071: step 4564, loss 0.545491.
Train: 2018-07-31T09:20:45.179318: step 4565, loss 0.528646.
Train: 2018-07-31T09:20:45.335496: step 4566, loss 0.570862.
Train: 2018-07-31T09:20:45.491741: step 4567, loss 0.604558.
Train: 2018-07-31T09:20:45.647954: step 4568, loss 0.520391.
Train: 2018-07-31T09:20:45.804167: step 4569, loss 0.545634.
Train: 2018-07-31T09:20:45.960375: step 4570, loss 0.570846.
Test: 2018-07-31T09:20:46.194670: step 4570, loss 0.548373.
Train: 2018-07-31T09:20:46.350885: step 4571, loss 0.512035.
Train: 2018-07-31T09:20:46.507128: step 4572, loss 0.629721.
Train: 2018-07-31T09:20:46.663341: step 4573, loss 0.654912.
Train: 2018-07-31T09:20:46.819555: step 4574, loss 0.587613.
Train: 2018-07-31T09:20:46.991390: step 4575, loss 0.587562.
Train: 2018-07-31T09:20:47.147573: step 4576, loss 0.537453.
Train: 2018-07-31T09:20:47.303816: step 4577, loss 0.520853.
Train: 2018-07-31T09:20:47.460029: step 4578, loss 0.620761.
Train: 2018-07-31T09:20:47.616248: step 4579, loss 0.545887.
Train: 2018-07-31T09:20:47.788081: step 4580, loss 0.570816.
Test: 2018-07-31T09:20:48.022393: step 4580, loss 0.548627.
Train: 2018-07-31T09:20:48.178611: step 4581, loss 0.570815.
Train: 2018-07-31T09:20:48.334819: step 4582, loss 0.579102.
Train: 2018-07-31T09:20:48.491038: step 4583, loss 0.570813.
Train: 2018-07-31T09:20:48.647252: step 4584, loss 0.562539.
Train: 2018-07-31T09:20:48.803464: step 4585, loss 0.579081.
Train: 2018-07-31T09:20:48.944027: step 4586, loss 0.554287.
Train: 2018-07-31T09:20:49.100240: step 4587, loss 0.521251.
Train: 2018-07-31T09:20:49.256490: step 4588, loss 0.471604.
Train: 2018-07-31T09:20:49.412667: step 4589, loss 0.545938.
Train: 2018-07-31T09:20:49.568881: step 4590, loss 0.545861.
Test: 2018-07-31T09:20:49.818847: step 4590, loss 0.548497.
Train: 2018-07-31T09:20:49.975036: step 4591, loss 0.545782.
Train: 2018-07-31T09:20:50.131248: step 4592, loss 0.570835.
Train: 2018-07-31T09:20:50.303108: step 4593, loss 0.562442.
Train: 2018-07-31T09:20:50.443677: step 4594, loss 0.587716.
Train: 2018-07-31T09:20:50.599890: step 4595, loss 0.545529.
Train: 2018-07-31T09:20:50.756132: step 4596, loss 0.520081.
Train: 2018-07-31T09:20:50.912341: step 4597, loss 0.5709.
Train: 2018-07-31T09:20:51.068559: step 4598, loss 0.519823.
Train: 2018-07-31T09:20:51.224773: step 4599, loss 0.536765.
Train: 2018-07-31T09:20:51.380986: step 4600, loss 0.485225.
Test: 2018-07-31T09:20:51.615277: step 4600, loss 0.547968.
Train: 2018-07-31T09:20:52.333888: step 4601, loss 0.596861.
Train: 2018-07-31T09:20:52.505723: step 4602, loss 0.51917.
Train: 2018-07-31T09:20:52.661939: step 4603, loss 0.527686.
Train: 2018-07-31T09:20:52.818150: step 4604, loss 0.597302.
Train: 2018-07-31T09:20:52.974365: step 4605, loss 0.51871.
Train: 2018-07-31T09:20:53.130546: step 4606, loss 0.553679.
Train: 2018-07-31T09:20:53.286759: step 4607, loss 0.58889.
Train: 2018-07-31T09:20:53.442998: step 4608, loss 0.606615.
Train: 2018-07-31T09:20:53.599217: step 4609, loss 0.57133.
Train: 2018-07-31T09:20:53.755431: step 4610, loss 0.518302.
Test: 2018-07-31T09:20:53.989751: step 4610, loss 0.547686.
Train: 2018-07-31T09:20:54.145965: step 4611, loss 0.57136.
Train: 2018-07-31T09:20:54.317769: step 4612, loss 0.597949.
Train: 2018-07-31T09:20:54.458360: step 4613, loss 0.61566.
Train: 2018-07-31T09:20:54.614604: step 4614, loss 0.668658.
Train: 2018-07-31T09:20:54.770817: step 4615, loss 0.571298.
Train: 2018-07-31T09:20:54.927030: step 4616, loss 0.536103.
Train: 2018-07-31T09:20:55.083244: step 4617, loss 0.553685.
Train: 2018-07-31T09:20:55.239428: step 4618, loss 0.553694.
Train: 2018-07-31T09:20:55.395642: step 4619, loss 0.492625.
Train: 2018-07-31T09:20:55.551854: step 4620, loss 0.57987.
Test: 2018-07-31T09:20:55.801796: step 4620, loss 0.54783.
Train: 2018-07-31T09:20:55.958039: step 4621, loss 0.544995.
Train: 2018-07-31T09:20:56.114222: step 4622, loss 0.614683.
Train: 2018-07-31T09:20:56.254845: step 4623, loss 0.518935.
Train: 2018-07-31T09:20:56.426649: step 4624, loss 0.545033.
Train: 2018-07-31T09:20:56.582863: step 4625, loss 0.510279.
Train: 2018-07-31T09:20:56.739078: step 4626, loss 0.579809.
Train: 2018-07-31T09:20:56.910944: step 4627, loss 0.553719.
Train: 2018-07-31T09:20:57.067156: step 4628, loss 0.614632.
Train: 2018-07-31T09:20:57.223370: step 4629, loss 0.597198.
Train: 2018-07-31T09:20:57.379582: step 4630, loss 0.605824.
Test: 2018-07-31T09:20:57.613903: step 4630, loss 0.547899.
Train: 2018-07-31T09:20:57.785736: step 4631, loss 0.553744.
Train: 2018-07-31T09:20:57.941920: step 4632, loss 0.545111.
Train: 2018-07-31T09:20:58.098163: step 4633, loss 0.631466.
Train: 2018-07-31T09:20:58.254347: step 4634, loss 0.596835.
Train: 2018-07-31T09:20:58.410561: step 4635, loss 0.545232.
Train: 2018-07-31T09:20:58.582395: step 4636, loss 0.588075.
Train: 2018-07-31T09:20:58.738609: step 4637, loss 0.562398.
Train: 2018-07-31T09:20:58.894822: step 4638, loss 0.57943.
Train: 2018-07-31T09:20:59.051065: step 4639, loss 0.494475.
Train: 2018-07-31T09:20:59.207248: step 4640, loss 0.562409.
Test: 2018-07-31T09:20:59.441600: step 4640, loss 0.548212.
Train: 2018-07-31T09:20:59.597812: step 4641, loss 0.579367.
Train: 2018-07-31T09:20:59.769643: step 4642, loss 0.503123.
Train: 2018-07-31T09:20:59.925865: step 4643, loss 0.587835.
Train: 2018-07-31T09:21:00.082074: step 4644, loss 0.562413.
Train: 2018-07-31T09:21:00.238258: step 4645, loss 0.520043.
Train: 2018-07-31T09:21:00.394501: step 4646, loss 0.681162.
Train: 2018-07-31T09:21:00.550718: step 4647, loss 0.553946.
Train: 2018-07-31T09:21:00.706927: step 4648, loss 0.545501.
Train: 2018-07-31T09:21:00.863136: step 4649, loss 0.579325.
Train: 2018-07-31T09:21:01.019323: step 4650, loss 0.638429.
Test: 2018-07-31T09:21:01.253645: step 4650, loss 0.548322.
Train: 2018-07-31T09:21:01.409887: step 4651, loss 0.520307.
Train: 2018-07-31T09:21:01.566101: step 4652, loss 0.570852.
Train: 2018-07-31T09:21:01.722285: step 4653, loss 0.604472.
Train: 2018-07-31T09:21:01.878498: step 4654, loss 0.612794.
Train: 2018-07-31T09:21:02.034744: step 4655, loss 0.520622.
Train: 2018-07-31T09:21:02.190924: step 4656, loss 0.579185.
Train: 2018-07-31T09:21:02.347164: step 4657, loss 0.554135.
Train: 2018-07-31T09:21:02.518997: step 4658, loss 0.579159.
Train: 2018-07-31T09:21:02.659595: step 4659, loss 0.545839.
Train: 2018-07-31T09:21:02.815779: step 4660, loss 0.495913.
Test: 2018-07-31T09:21:03.065720: step 4660, loss 0.548533.
Train: 2018-07-31T09:21:03.221959: step 4661, loss 0.545823.
Train: 2018-07-31T09:21:03.378147: step 4662, loss 0.562479.
Train: 2018-07-31T09:21:03.518769: step 4663, loss 0.570829.
Train: 2018-07-31T09:21:03.674987: step 4664, loss 0.587573.
Train: 2018-07-31T09:21:03.831165: step 4665, loss 0.621096.
Train: 2018-07-31T09:21:03.987380: step 4666, loss 0.487107.
Train: 2018-07-31T09:21:04.143602: step 4667, loss 0.537302.
Train: 2018-07-31T09:21:04.315452: step 4668, loss 0.554045.
Train: 2018-07-31T09:21:04.471672: step 4669, loss 0.537188.
Train: 2018-07-31T09:21:04.627853: step 4670, loss 0.520244.
Test: 2018-07-31T09:21:04.862205: step 4670, loss 0.548242.
Train: 2018-07-31T09:21:05.034041: step 4671, loss 0.545489.
Train: 2018-07-31T09:21:05.190252: step 4672, loss 0.553916.
Train: 2018-07-31T09:21:05.362057: step 4673, loss 0.528329.
Train: 2018-07-31T09:21:05.518271: step 4674, loss 0.53675.
Train: 2018-07-31T09:21:05.674485: step 4675, loss 0.52807.
Train: 2018-07-31T09:21:05.815076: step 4676, loss 0.588249.
Train: 2018-07-31T09:21:05.971289: step 4677, loss 0.510538.
Train: 2018-07-31T09:21:06.127502: step 4678, loss 0.553733.
Train: 2018-07-31T09:21:06.283716: step 4679, loss 0.579839.
Train: 2018-07-31T09:21:06.455552: step 4680, loss 0.5275.
Test: 2018-07-31T09:21:06.689902: step 4680, loss 0.547775.
Train: 2018-07-31T09:21:06.830494: step 4681, loss 0.469003.
Train: 2018-07-31T09:21:06.986677: step 4682, loss 0.580073.
Train: 2018-07-31T09:21:07.142923: step 4683, loss 0.650823.
Train: 2018-07-31T09:21:07.299133: step 4684, loss 0.62442.
Train: 2018-07-31T09:21:07.455342: step 4685, loss 0.571347.
Train: 2018-07-31T09:21:07.611555: step 4686, loss 0.54482.
Train: 2018-07-31T09:21:07.767743: step 4687, loss 0.553661.
Train: 2018-07-31T09:21:07.923987: step 4688, loss 0.500638.
Train: 2018-07-31T09:21:08.064575: step 4689, loss 0.518276.
Train: 2018-07-31T09:21:08.236408: step 4690, loss 0.535937.
Test: 2018-07-31T09:21:08.470705: step 4690, loss 0.547666.
Train: 2018-07-31T09:21:08.626917: step 4691, loss 0.642417.
Train: 2018-07-31T09:21:08.767509: step 4692, loss 0.56253.
Train: 2018-07-31T09:21:08.923757: step 4693, loss 0.562528.
Train: 2018-07-31T09:21:09.079936: step 4694, loss 0.598005.
Train: 2018-07-31T09:21:09.236180: step 4695, loss 0.562516.
Train: 2018-07-31T09:21:09.392365: step 4696, loss 0.553659.
Train: 2018-07-31T09:21:09.548576: step 4697, loss 0.580177.
Train: 2018-07-31T09:21:09.704820: step 4698, loss 0.580141.
Train: 2018-07-31T09:21:09.861028: step 4699, loss 0.544858.
Train: 2018-07-31T09:21:10.017247: step 4700, loss 0.509684.
Test: 2018-07-31T09:21:10.251568: step 4700, loss 0.547738.
Train: 2018-07-31T09:21:10.954528: step 4701, loss 0.500904.
Train: 2018-07-31T09:21:11.110712: step 4702, loss 0.55367.
Train: 2018-07-31T09:21:11.266925: step 4703, loss 0.492003.
Train: 2018-07-31T09:21:11.407546: step 4704, loss 0.536009.
Train: 2018-07-31T09:21:11.563729: step 4705, loss 0.518273.
Train: 2018-07-31T09:21:11.719973: step 4706, loss 0.562526.
Train: 2018-07-31T09:21:11.891808: step 4707, loss 0.544763.
Train: 2018-07-31T09:21:12.048027: step 4708, loss 0.580386.
Train: 2018-07-31T09:21:12.204235: step 4709, loss 0.526882.
Train: 2018-07-31T09:21:12.360449: step 4710, loss 0.571536.
Test: 2018-07-31T09:21:12.594738: step 4710, loss 0.547616.
Train: 2018-07-31T09:21:12.750951: step 4711, loss 0.571561.
Train: 2018-07-31T09:21:12.907166: step 4712, loss 0.580535.
Train: 2018-07-31T09:21:13.079001: step 4713, loss 0.58054.
Train: 2018-07-31T09:21:13.219593: step 4714, loss 0.607399.
Train: 2018-07-31T09:21:13.391452: step 4715, loss 0.553656.
Train: 2018-07-31T09:21:13.547670: step 4716, loss 0.589384.
Train: 2018-07-31T09:21:13.703889: step 4717, loss 0.589315.
Train: 2018-07-31T09:21:13.860097: step 4718, loss 0.562548.
Train: 2018-07-31T09:21:14.016311: step 4719, loss 0.527037.
Train: 2018-07-31T09:21:14.172524: step 4720, loss 0.562516.
Test: 2018-07-31T09:21:14.406815: step 4720, loss 0.547691.
Train: 2018-07-31T09:21:14.563058: step 4721, loss 0.642108.
Train: 2018-07-31T09:21:14.719241: step 4722, loss 0.527215.
Train: 2018-07-31T09:21:14.875454: step 4723, loss 0.650439.
Train: 2018-07-31T09:21:15.031702: step 4724, loss 0.53616.
Train: 2018-07-31T09:21:15.187911: step 4725, loss 0.65852.
Train: 2018-07-31T09:21:15.344094: step 4726, loss 0.536338.
Train: 2018-07-31T09:21:15.500339: step 4727, loss 0.510454.
Train: 2018-07-31T09:21:15.656549: step 4728, loss 0.614222.
Train: 2018-07-31T09:21:15.812736: step 4729, loss 0.553789.
Train: 2018-07-31T09:21:15.984569: step 4730, loss 0.570979.
Test: 2018-07-31T09:21:16.218891: step 4730, loss 0.548062.
Train: 2018-07-31T09:21:16.375140: step 4731, loss 0.52816.
Train: 2018-07-31T09:21:16.531349: step 4732, loss 0.579485.
Train: 2018-07-31T09:21:16.703187: step 4733, loss 0.553873.
Train: 2018-07-31T09:21:16.855251: step 4734, loss 0.519833.
Train: 2018-07-31T09:21:17.011466: step 4735, loss 0.613464.
Train: 2018-07-31T09:21:17.167680: step 4736, loss 0.604894.
Train: 2018-07-31T09:21:17.323924: step 4737, loss 0.630244.
Train: 2018-07-31T09:21:17.480137: step 4738, loss 0.503268.
Train: 2018-07-31T09:21:17.651942: step 4739, loss 0.621479.
Train: 2018-07-31T09:21:17.808185: step 4740, loss 0.570851.
Test: 2018-07-31T09:21:18.042501: step 4740, loss 0.548396.
Train: 2018-07-31T09:21:18.198713: step 4741, loss 0.629581.
Train: 2018-07-31T09:21:18.354903: step 4742, loss 0.554109.
Train: 2018-07-31T09:21:18.526737: step 4743, loss 0.495806.
Train: 2018-07-31T09:21:18.682981: step 4744, loss 0.537516.
Train: 2018-07-31T09:21:18.839198: step 4745, loss 0.512553.
Train: 2018-07-31T09:21:18.995408: step 4746, loss 0.512495.
Train: 2018-07-31T09:21:19.151621: step 4747, loss 0.562476.
Train: 2018-07-31T09:21:19.307805: step 4748, loss 0.570832.
Train: 2018-07-31T09:21:19.464042: step 4749, loss 0.570838.
Train: 2018-07-31T09:21:19.620232: step 4750, loss 0.554051.
Test: 2018-07-31T09:21:19.854551: step 4750, loss 0.548354.
Train: 2018-07-31T09:21:20.010765: step 4751, loss 0.604491.
Train: 2018-07-31T09:21:20.166978: step 4752, loss 0.48669.
Train: 2018-07-31T09:21:20.323222: step 4753, loss 0.613044.
Train: 2018-07-31T09:21:20.479436: step 4754, loss 0.58776.
Train: 2018-07-31T09:21:20.635648: step 4755, loss 0.545521.
Train: 2018-07-31T09:21:20.791834: step 4756, loss 0.55396.
Train: 2018-07-31T09:21:20.948045: step 4757, loss 0.579348.
Train: 2018-07-31T09:21:21.104289: step 4758, loss 0.570885.
Train: 2018-07-31T09:21:21.260502: step 4759, loss 0.613269.
Train: 2018-07-31T09:21:21.416685: step 4760, loss 0.570884.
Test: 2018-07-31T09:21:21.666633: step 4760, loss 0.548238.
Train: 2018-07-31T09:21:21.822874: step 4761, loss 0.587811.
Train: 2018-07-31T09:21:21.979084: step 4762, loss 0.562419.
Train: 2018-07-31T09:21:22.135266: step 4763, loss 0.511735.
Train: 2018-07-31T09:21:22.291480: step 4764, loss 0.596224.
Train: 2018-07-31T09:21:22.447693: step 4765, loss 0.545526.
Train: 2018-07-31T09:21:22.603942: step 4766, loss 0.57087.
Train: 2018-07-31T09:21:22.760150: step 4767, loss 0.520177.
Train: 2018-07-31T09:21:22.931980: step 4768, loss 0.537046.
Train: 2018-07-31T09:21:23.088168: step 4769, loss 0.545473.
Train: 2018-07-31T09:21:23.244407: step 4770, loss 0.545438.
Test: 2018-07-31T09:21:23.463116: step 4770, loss 0.548164.
Train: 2018-07-31T09:21:23.619319: step 4771, loss 0.528393.
Train: 2018-07-31T09:21:23.775509: step 4772, loss 0.570925.
Train: 2018-07-31T09:21:23.947369: step 4773, loss 0.519679.
Train: 2018-07-31T09:21:24.103556: step 4774, loss 0.59667.
Train: 2018-07-31T09:21:24.259769: step 4775, loss 0.613904.
Train: 2018-07-31T09:21:24.416013: step 4776, loss 0.562396.
Train: 2018-07-31T09:21:24.572229: step 4777, loss 0.562396.
Train: 2018-07-31T09:21:24.728410: step 4778, loss 0.467815.
Train: 2018-07-31T09:21:24.900245: step 4779, loss 0.510684.
Train: 2018-07-31T09:21:25.056457: step 4780, loss 0.527815.
Test: 2018-07-31T09:21:25.290804: step 4780, loss 0.547877.
Train: 2018-07-31T09:21:25.446992: step 4781, loss 0.597122.
Train: 2018-07-31T09:21:25.603205: step 4782, loss 0.562418.
Train: 2018-07-31T09:21:25.759419: step 4783, loss 0.606013.
Train: 2018-07-31T09:21:25.915662: step 4784, loss 0.510075.
Train: 2018-07-31T09:21:26.056254: step 4785, loss 0.606144.
Train: 2018-07-31T09:21:26.212471: step 4786, loss 0.536197.
Train: 2018-07-31T09:21:26.384271: step 4787, loss 0.614983.
Train: 2018-07-31T09:21:26.540486: step 4788, loss 0.466145.
Train: 2018-07-31T09:21:26.681093: step 4789, loss 0.518607.
Train: 2018-07-31T09:21:26.852913: step 4790, loss 0.580041.
Test: 2018-07-31T09:21:27.087267: step 4790, loss 0.54773.
Train: 2018-07-31T09:21:27.243475: step 4791, loss 0.588881.
Train: 2018-07-31T09:21:27.399689: step 4792, loss 0.580096.
Train: 2018-07-31T09:21:27.555902: step 4793, loss 0.509611.
Train: 2018-07-31T09:21:27.712120: step 4794, loss 0.633055.
Train: 2018-07-31T09:21:27.868330: step 4795, loss 0.536033.
Train: 2018-07-31T09:21:28.040134: step 4796, loss 0.562482.
Train: 2018-07-31T09:21:28.196378: step 4797, loss 0.518412.
Train: 2018-07-31T09:21:28.352591: step 4798, loss 0.562484.
Train: 2018-07-31T09:21:28.508775: step 4799, loss 0.597776.
Train: 2018-07-31T09:21:28.664988: step 4800, loss 0.527212.
Test: 2018-07-31T09:21:28.899338: step 4800, loss 0.547714.
Train: 2018-07-31T09:21:29.617921: step 4801, loss 0.518389.
Train: 2018-07-31T09:21:29.789724: step 4802, loss 0.527183.
Train: 2018-07-31T09:21:29.945970: step 4803, loss 0.544822.
Train: 2018-07-31T09:21:30.102181: step 4804, loss 0.61562.
Train: 2018-07-31T09:21:30.258364: step 4805, loss 0.509395.
Train: 2018-07-31T09:21:30.414608: step 4806, loss 0.571379.
Train: 2018-07-31T09:21:30.570824: step 4807, loss 0.571389.
Train: 2018-07-31T09:21:30.727035: step 4808, loss 0.589127.
Train: 2018-07-31T09:21:30.883219: step 4809, loss 0.56252.
Train: 2018-07-31T09:21:31.039433: step 4810, loss 0.535942.
Test: 2018-07-31T09:21:31.273783: step 4810, loss 0.547682.
Train: 2018-07-31T09:21:31.445587: step 4811, loss 0.535946.
Train: 2018-07-31T09:21:31.601830: step 4812, loss 0.589088.
Train: 2018-07-31T09:21:31.758044: step 4813, loss 0.615632.
Train: 2018-07-31T09:21:31.914257: step 4814, loss 0.544822.
Train: 2018-07-31T09:21:32.070470: step 4815, loss 0.544836.
Train: 2018-07-31T09:21:32.226654: step 4816, loss 0.518388.
Train: 2018-07-31T09:21:32.382897: step 4817, loss 0.571303.
Train: 2018-07-31T09:21:32.539114: step 4818, loss 0.624188.
Train: 2018-07-31T09:21:32.710915: step 4819, loss 0.544871.
Train: 2018-07-31T09:21:32.851507: step 4820, loss 0.492155.
Test: 2018-07-31T09:21:33.101479: step 4820, loss 0.547743.
Train: 2018-07-31T09:21:33.257663: step 4821, loss 0.562464.
Train: 2018-07-31T09:21:33.429498: step 4822, loss 0.580043.
Train: 2018-07-31T09:21:33.585710: step 4823, loss 0.54489.
Train: 2018-07-31T09:21:33.741925: step 4824, loss 0.544892.
Train: 2018-07-31T09:21:33.913758: step 4825, loss 0.553676.
Train: 2018-07-31T09:21:34.054381: step 4826, loss 0.536104.
Train: 2018-07-31T09:21:34.226221: step 4827, loss 0.606419.
Train: 2018-07-31T09:21:34.382424: step 4828, loss 0.562462.
Train: 2018-07-31T09:21:34.538612: step 4829, loss 0.650271.
Train: 2018-07-31T09:21:34.694825: step 4830, loss 0.509886.
Test: 2018-07-31T09:21:34.929148: step 4830, loss 0.547787.
Train: 2018-07-31T09:21:35.085393: step 4831, loss 0.544942.
Train: 2018-07-31T09:21:35.241604: step 4832, loss 0.674337.
Train: 2018-07-31T09:21:35.397816: step 4833, loss 0.53628.
Train: 2018-07-31T09:21:35.554029: step 4834, loss 0.501556.
Train: 2018-07-31T09:21:35.710238: step 4835, loss 0.5711.
Train: 2018-07-31T09:21:35.866427: step 4836, loss 0.466964.
Train: 2018-07-31T09:21:36.022641: step 4837, loss 0.657951.
Train: 2018-07-31T09:21:36.178854: step 4838, loss 0.51036.
Train: 2018-07-31T09:21:36.335100: step 4839, loss 0.571085.
Train: 2018-07-31T09:21:36.491310: step 4840, loss 0.501701.
Test: 2018-07-31T09:21:36.725631: step 4840, loss 0.547873.
Train: 2018-07-31T09:21:36.881814: step 4841, loss 0.605817.
Train: 2018-07-31T09:21:37.038028: step 4842, loss 0.51901.
Train: 2018-07-31T09:21:37.194241: step 4843, loss 0.562414.
Train: 2018-07-31T09:21:37.350454: step 4844, loss 0.527646.
Train: 2018-07-31T09:21:37.506667: step 4845, loss 0.571123.
Train: 2018-07-31T09:21:37.678540: step 4846, loss 0.588554.
Train: 2018-07-31T09:21:37.834716: step 4847, loss 0.562423.
Train: 2018-07-31T09:21:37.990959: step 4848, loss 0.562423.
Train: 2018-07-31T09:21:38.147172: step 4849, loss 0.597273.
Train: 2018-07-31T09:21:38.303355: step 4850, loss 0.545009.
Test: 2018-07-31T09:21:38.537676: step 4850, loss 0.547846.
Train: 2018-07-31T09:21:38.693913: step 4851, loss 0.640734.
Train: 2018-07-31T09:21:38.865723: step 4852, loss 0.527678.
Train: 2018-07-31T09:21:39.021937: step 4853, loss 0.605775.
Train: 2018-07-31T09:21:39.178152: step 4854, loss 0.527783.
Train: 2018-07-31T09:21:39.334398: step 4855, loss 0.545112.
Train: 2018-07-31T09:21:39.490613: step 4856, loss 0.562401.
Train: 2018-07-31T09:21:39.662413: step 4857, loss 0.510602.
Train: 2018-07-31T09:21:39.803005: step 4858, loss 0.622859.
Train: 2018-07-31T09:21:39.959219: step 4859, loss 0.579658.
Train: 2018-07-31T09:21:40.115461: step 4860, loss 0.536542.
Test: 2018-07-31T09:21:40.365372: step 4860, loss 0.547972.
Train: 2018-07-31T09:21:40.521616: step 4861, loss 0.631307.
Train: 2018-07-31T09:21:40.677824: step 4862, loss 0.570993.
Train: 2018-07-31T09:21:40.818422: step 4863, loss 0.519498.
Train: 2018-07-31T09:21:40.974635: step 4864, loss 0.528108.
Train: 2018-07-31T09:21:41.130818: step 4865, loss 0.570967.
Train: 2018-07-31T09:21:41.287033: step 4866, loss 0.502408.
Train: 2018-07-31T09:21:41.443278: step 4867, loss 0.510922.
Train: 2018-07-31T09:21:41.599490: step 4868, loss 0.588186.
Train: 2018-07-31T09:21:41.755671: step 4869, loss 0.571005.
Train: 2018-07-31T09:21:41.927532: step 4870, loss 0.571014.
Test: 2018-07-31T09:21:42.161858: step 4870, loss 0.547959.
Train: 2018-07-31T09:21:42.318042: step 4871, loss 0.579642.
Train: 2018-07-31T09:21:42.489875: step 4872, loss 0.571023.
Train: 2018-07-31T09:21:42.646089: step 4873, loss 0.519281.
Train: 2018-07-31T09:21:42.802302: step 4874, loss 0.545138.
Train: 2018-07-31T09:21:42.942925: step 4875, loss 0.596965.
Train: 2018-07-31T09:21:43.114753: step 4876, loss 0.579688.
Train: 2018-07-31T09:21:43.270942: step 4877, loss 0.657457.
Train: 2018-07-31T09:21:43.427180: step 4878, loss 0.571021.
Train: 2018-07-31T09:21:43.583403: step 4879, loss 0.553793.
Train: 2018-07-31T09:21:43.739614: step 4880, loss 0.519456.
Test: 2018-07-31T09:21:43.989525: step 4880, loss 0.548023.
Train: 2018-07-31T09:21:44.145738: step 4881, loss 0.528068.
Train: 2018-07-31T09:21:44.301951: step 4882, loss 0.553813.
Train: 2018-07-31T09:21:44.458164: step 4883, loss 0.510887.
Train: 2018-07-31T09:21:44.614409: step 4884, loss 0.562396.
Train: 2018-07-31T09:21:44.770621: step 4885, loss 0.510759.
Train: 2018-07-31T09:21:44.926835: step 4886, loss 0.502026.
Train: 2018-07-31T09:21:45.083042: step 4887, loss 0.562404.
Train: 2018-07-31T09:21:45.239262: step 4888, loss 0.579763.
Train: 2018-07-31T09:21:45.395446: step 4889, loss 0.536333.
Train: 2018-07-31T09:21:45.567310: step 4890, loss 0.57114.
Test: 2018-07-31T09:21:45.801627: step 4890, loss 0.547809.
Train: 2018-07-31T09:21:45.957814: step 4891, loss 0.579894.
Train: 2018-07-31T09:21:46.114057: step 4892, loss 0.632375.
Train: 2018-07-31T09:21:46.270240: step 4893, loss 0.676036.
Train: 2018-07-31T09:21:46.426484: step 4894, loss 0.483997.
Train: 2018-07-31T09:21:46.598319: step 4895, loss 0.527598.
Train: 2018-07-31T09:21:46.754531: step 4896, loss 0.579827.
Train: 2018-07-31T09:21:46.910715: step 4897, loss 0.55372.
Train: 2018-07-31T09:21:47.066958: step 4898, loss 0.623274.
Train: 2018-07-31T09:21:47.223141: step 4899, loss 0.57109.
Train: 2018-07-31T09:21:47.379355: step 4900, loss 0.588396.
Test: 2018-07-31T09:21:47.613677: step 4900, loss 0.547926.
Train: 2018-07-31T09:21:48.332291: step 4901, loss 0.553758.
Train: 2018-07-31T09:21:48.488503: step 4902, loss 0.510632.
Train: 2018-07-31T09:21:48.644714: step 4903, loss 0.596889.
Train: 2018-07-31T09:21:48.800927: step 4904, loss 0.605454.
Train: 2018-07-31T09:21:48.957111: step 4905, loss 0.562396.
Train: 2018-07-31T09:21:49.113355: step 4906, loss 0.562396.
Train: 2018-07-31T09:21:49.269569: step 4907, loss 0.605207.
Train: 2018-07-31T09:21:49.425781: step 4908, loss 0.596562.
Train: 2018-07-31T09:21:49.597616: step 4909, loss 0.579434.
Train: 2018-07-31T09:21:49.738208: step 4910, loss 0.545426.
Test: 2018-07-31T09:21:49.988121: step 4910, loss 0.548226.
Train: 2018-07-31T09:21:50.144334: step 4911, loss 0.553942.
Train: 2018-07-31T09:21:50.300570: step 4912, loss 0.579331.
Train: 2018-07-31T09:21:50.456760: step 4913, loss 0.545546.
Train: 2018-07-31T09:21:50.612973: step 4914, loss 0.554001.
Train: 2018-07-31T09:21:50.769187: step 4915, loss 0.461377.
Train: 2018-07-31T09:21:50.925401: step 4916, loss 0.511822.
Train: 2018-07-31T09:21:51.081647: step 4917, loss 0.528595.
Train: 2018-07-31T09:21:51.237827: step 4918, loss 0.570892.
Train: 2018-07-31T09:21:51.394071: step 4919, loss 0.587919.
Train: 2018-07-31T09:21:51.534657: step 4920, loss 0.477189.
Test: 2018-07-31T09:21:51.784574: step 4920, loss 0.548075.
Train: 2018-07-31T09:21:51.940817: step 4921, loss 0.562397.
Train: 2018-07-31T09:21:52.097000: step 4922, loss 0.562396.
Train: 2018-07-31T09:21:52.253244: step 4923, loss 0.536585.
Train: 2018-07-31T09:21:52.409458: step 4924, loss 0.614184.
Train: 2018-07-31T09:21:52.565640: step 4925, loss 0.553757.
Train: 2018-07-31T09:21:52.706258: step 4926, loss 0.571065.
Train: 2018-07-31T09:21:52.862476: step 4927, loss 0.58842.
Train: 2018-07-31T09:21:53.018660: step 4928, loss 0.536385.
Train: 2018-07-31T09:21:53.174903: step 4929, loss 0.571096.
Train: 2018-07-31T09:21:53.331086: step 4930, loss 0.545036.
Test: 2018-07-31T09:21:53.581062: step 4930, loss 0.547852.
Train: 2018-07-31T09:21:53.737272: step 4931, loss 0.579811.
Train: 2018-07-31T09:21:53.893455: step 4932, loss 0.597218.
Train: 2018-07-31T09:21:54.065290: step 4933, loss 0.562417.
Train: 2018-07-31T09:21:54.221502: step 4934, loss 0.562415.
Train: 2018-07-31T09:21:54.377716: step 4935, loss 0.536352.
Train: 2018-07-31T09:21:54.533929: step 4936, loss 0.527664.
Train: 2018-07-31T09:21:54.690143: step 4937, loss 0.536335.
Train: 2018-07-31T09:21:54.846387: step 4938, loss 0.545013.
Train: 2018-07-31T09:21:55.002606: step 4939, loss 0.527567.
Train: 2018-07-31T09:21:55.158814: step 4940, loss 0.606081.
Test: 2018-07-31T09:21:55.393134: step 4940, loss 0.547803.
Train: 2018-07-31T09:21:55.564939: step 4941, loss 0.606112.
Train: 2018-07-31T09:21:55.721182: step 4942, loss 0.5537.
Train: 2018-07-31T09:21:55.877396: step 4943, loss 0.518783.
Train: 2018-07-31T09:21:56.033579: step 4944, loss 0.562433.
Train: 2018-07-31T09:21:56.189792: step 4945, loss 0.562435.
Train: 2018-07-31T09:21:56.346036: step 4946, loss 0.553695.
Train: 2018-07-31T09:21:56.517840: step 4947, loss 0.649876.
Train: 2018-07-31T09:21:56.674084: step 4948, loss 0.588621.
Train: 2018-07-31T09:21:56.814646: step 4949, loss 0.597272.
Train: 2018-07-31T09:21:56.986511: step 4950, loss 0.553725.
Test: 2018-07-31T09:21:57.220801: step 4950, loss 0.547889.
Train: 2018-07-31T09:21:57.377038: step 4951, loss 0.553739.
Train: 2018-07-31T09:21:57.533258: step 4952, loss 0.545099.
Train: 2018-07-31T09:21:57.689466: step 4953, loss 0.484637.
Train: 2018-07-31T09:21:57.845684: step 4954, loss 0.545115.
Train: 2018-07-31T09:21:58.001892: step 4955, loss 0.571052.
Train: 2018-07-31T09:21:58.158111: step 4956, loss 0.484536.
Train: 2018-07-31T09:21:58.314321: step 4957, loss 0.545071.
Train: 2018-07-31T09:21:58.486159: step 4958, loss 0.605845.
Train: 2018-07-31T09:21:58.642344: step 4959, loss 0.553722.
Train: 2018-07-31T09:21:58.798587: step 4960, loss 0.588524.
Test: 2018-07-31T09:21:59.032879: step 4960, loss 0.547844.
Train: 2018-07-31T09:21:59.189089: step 4961, loss 0.597233.
Train: 2018-07-31T09:21:59.345333: step 4962, loss 0.605906.
Train: 2018-07-31T09:21:59.501517: step 4963, loss 0.588466.
Train: 2018-07-31T09:21:59.673351: step 4964, loss 0.588409.
Train: 2018-07-31T09:21:59.829599: step 4965, loss 0.596988.
Train: 2018-07-31T09:21:59.985779: step 4966, loss 0.605503.
Train: 2018-07-31T09:22:00.141992: step 4967, loss 0.570986.
Train: 2018-07-31T09:22:00.298229: step 4968, loss 0.596638.
Train: 2018-07-31T09:22:00.454453: step 4969, loss 0.536817.
Train: 2018-07-31T09:22:00.610662: step 4970, loss 0.638932.
Test: 2018-07-31T09:22:00.844983: step 4970, loss 0.548234.
Train: 2018-07-31T09:22:01.001166: step 4971, loss 0.503145.
Train: 2018-07-31T09:22:01.157409: step 4972, loss 0.579315.
Train: 2018-07-31T09:22:01.313623: step 4973, loss 0.528731.
Train: 2018-07-31T09:22:01.469837: step 4974, loss 0.570851.
Train: 2018-07-31T09:22:01.626049: step 4975, loss 0.49523.
Train: 2018-07-31T09:22:01.782232: step 4976, loss 0.545632.
Train: 2018-07-31T09:22:01.938478: step 4977, loss 0.57085.
Train: 2018-07-31T09:22:02.094690: step 4978, loss 0.520351.
Train: 2018-07-31T09:22:02.250872: step 4979, loss 0.587721.
Train: 2018-07-31T09:22:02.407116: step 4980, loss 0.596181.
Test: 2018-07-31T09:22:02.641431: step 4980, loss 0.548289.
Train: 2018-07-31T09:22:02.797645: step 4981, loss 0.553984.
Train: 2018-07-31T09:22:02.953873: step 4982, loss 0.579312.
Train: 2018-07-31T09:22:03.110048: step 4983, loss 0.580439.
Train: 2018-07-31T09:22:03.266261: step 4984, loss 0.587753.
Train: 2018-07-31T09:22:03.422504: step 4985, loss 0.587738.
Train: 2018-07-31T09:22:03.578721: step 4986, loss 0.537146.
Train: 2018-07-31T09:22:03.734901: step 4987, loss 0.503455.
Train: 2018-07-31T09:22:03.906737: step 4988, loss 0.520255.
Train: 2018-07-31T09:22:04.047357: step 4989, loss 0.570872.
Train: 2018-07-31T09:22:04.203541: step 4990, loss 0.553949.
Test: 2018-07-31T09:22:04.437890: step 4990, loss 0.548207.
Train: 2018-07-31T09:22:04.594104: step 4991, loss 0.579372.
Train: 2018-07-31T09:22:04.750318: step 4992, loss 0.68129.
Train: 2018-07-31T09:22:04.906531: step 4993, loss 0.486094.
Train: 2018-07-31T09:22:05.062748: step 4994, loss 0.621794.
Train: 2018-07-31T09:22:05.234550: step 4995, loss 0.545459.
Train: 2018-07-31T09:22:05.390792: step 4996, loss 0.604781.
Train: 2018-07-31T09:22:05.546975: step 4997, loss 0.621663.
Train: 2018-07-31T09:22:05.718810: step 4998, loss 0.545534.
Train: 2018-07-31T09:22:05.875054: step 4999, loss 0.57086.
Train: 2018-07-31T09:22:06.031268: step 5000, loss 0.5456.
Test: 2018-07-31T09:22:06.265558: step 5000, loss 0.548355.
Train: 2018-07-31T09:22:06.968520: step 5001, loss 0.604489.
Train: 2018-07-31T09:22:07.124732: step 5002, loss 0.579238.
Train: 2018-07-31T09:22:07.280976: step 5003, loss 0.528934.
Train: 2018-07-31T09:22:07.437191: step 5004, loss 0.56246.
Train: 2018-07-31T09:22:07.593402: step 5005, loss 0.562463.
Train: 2018-07-31T09:22:07.749585: step 5006, loss 0.562465.
Train: 2018-07-31T09:22:07.905799: step 5007, loss 0.587561.
Train: 2018-07-31T09:22:08.062012: step 5008, loss 0.495593.
Train: 2018-07-31T09:22:08.233847: step 5009, loss 0.503884.
Train: 2018-07-31T09:22:08.374439: step 5010, loss 0.512119.
Test: 2018-07-31T09:22:08.624380: step 5010, loss 0.548338.
Train: 2018-07-31T09:22:08.780593: step 5011, loss 0.570853.
Train: 2018-07-31T09:22:08.936808: step 5012, loss 0.579309.
Train: 2018-07-31T09:22:09.093050: step 5013, loss 0.520105.
Train: 2018-07-31T09:22:09.249234: step 5014, loss 0.528453.
Train: 2018-07-31T09:22:09.405447: step 5015, loss 0.562401.
Train: 2018-07-31T09:22:09.577282: step 5016, loss 0.49402.
Train: 2018-07-31T09:22:09.733495: step 5017, loss 0.656831.
Train: 2018-07-31T09:22:09.889708: step 5018, loss 0.570998.
Train: 2018-07-31T09:22:10.045923: step 5019, loss 0.571013.
Train: 2018-07-31T09:22:10.202137: step 5020, loss 0.484774.
Test: 2018-07-31T09:22:10.436456: step 5020, loss 0.547919.
Train: 2018-07-31T09:22:10.592670: step 5021, loss 0.545107.
Train: 2018-07-31T09:22:10.748884: step 5022, loss 0.640456.
Train: 2018-07-31T09:22:10.905097: step 5023, loss 0.527697.
Train: 2018-07-31T09:22:11.061310: step 5024, loss 0.518964.
Train: 2018-07-31T09:22:11.217523: step 5025, loss 0.57113.
Train: 2018-07-31T09:22:11.389358: step 5026, loss 0.606038.
Train: 2018-07-31T09:22:11.545571: step 5027, loss 0.536251.
Train: 2018-07-31T09:22:11.701784: step 5028, loss 0.527498.
Train: 2018-07-31T09:22:11.858029: step 5029, loss 0.44874.
Train: 2018-07-31T09:22:12.029863: step 5030, loss 0.553679.
Test: 2018-07-31T09:22:12.264178: step 5030, loss 0.547728.
Train: 2018-07-31T09:22:12.420397: step 5031, loss 0.650528.
Train: 2018-07-31T09:22:12.576614: step 5032, loss 0.518411.
Train: 2018-07-31T09:22:12.732824: step 5033, loss 0.57132.
Train: 2018-07-31T09:22:12.889037: step 5034, loss 0.562499.
Train: 2018-07-31T09:22:13.060841: step 5035, loss 0.491731.
Train: 2018-07-31T09:22:13.217085: step 5036, loss 0.544791.
Train: 2018-07-31T09:22:13.373298: step 5037, loss 0.571422.
Train: 2018-07-31T09:22:13.529512: step 5038, loss 0.553654.
Train: 2018-07-31T09:22:13.685694: step 5039, loss 0.518015.
Train: 2018-07-31T09:22:13.841938: step 5040, loss 0.562581.
Test: 2018-07-31T09:22:14.076264: step 5040, loss 0.547622.
Train: 2018-07-31T09:22:14.232476: step 5041, loss 0.571537.
Train: 2018-07-31T09:22:14.388685: step 5042, loss 0.598408.
Train: 2018-07-31T09:22:14.544899: step 5043, loss 0.661063.
Train: 2018-07-31T09:22:14.701116: step 5044, loss 0.535791.
Train: 2018-07-31T09:22:14.857326: step 5045, loss 0.607159.
Train: 2018-07-31T09:22:15.013510: step 5046, loss 0.624817.
Train: 2018-07-31T09:22:15.169724: step 5047, loss 0.53593.
Train: 2018-07-31T09:22:15.325960: step 5048, loss 0.465289.
Train: 2018-07-31T09:22:15.482150: step 5049, loss 0.580153.
Train: 2018-07-31T09:22:15.654014: step 5050, loss 0.536027.
Test: 2018-07-31T09:22:15.888306: step 5050, loss 0.54772.
Train: 2018-07-31T09:22:16.044518: step 5051, loss 0.571293.
Train: 2018-07-31T09:22:16.200731: step 5052, loss 0.659327.
Train: 2018-07-31T09:22:16.356975: step 5053, loss 0.553678.
Train: 2018-07-31T09:22:16.528815: step 5054, loss 0.536175.
Train: 2018-07-31T09:22:16.700648: step 5055, loss 0.544956.
Train: 2018-07-31T09:22:16.872448: step 5056, loss 0.544975.
Train: 2018-07-31T09:22:17.028696: step 5057, loss 0.544989.
Train: 2018-07-31T09:22:17.184875: step 5058, loss 0.544997.
Train: 2018-07-31T09:22:17.341113: step 5059, loss 0.571133.
Train: 2018-07-31T09:22:17.497338: step 5060, loss 0.58854.
Test: 2018-07-31T09:22:17.747246: step 5060, loss 0.547852.
Train: 2018-07-31T09:22:17.903457: step 5061, loss 0.510235.
Train: 2018-07-31T09:22:18.059671: step 5062, loss 0.640696.
Train: 2018-07-31T09:22:18.215885: step 5063, loss 0.640563.
Train: 2018-07-31T09:22:18.372099: step 5064, loss 0.536437.
Train: 2018-07-31T09:22:18.528341: step 5065, loss 0.493317.
Train: 2018-07-31T09:22:18.668903: step 5066, loss 0.5624.
Train: 2018-07-31T09:22:18.825117: step 5067, loss 0.484785.
Train: 2018-07-31T09:22:18.981331: step 5068, loss 0.571033.
Train: 2018-07-31T09:22:19.137543: step 5069, loss 0.605594.
Train: 2018-07-31T09:22:19.293756: step 5070, loss 0.562401.
Test: 2018-07-31T09:22:19.528108: step 5070, loss 0.547942.
Train: 2018-07-31T09:22:19.699945: step 5071, loss 0.657366.
Train: 2018-07-31T09:22:19.856125: step 5072, loss 0.579624.
Train: 2018-07-31T09:22:20.012339: step 5073, loss 0.562396.
Train: 2018-07-31T09:22:20.152961: step 5074, loss 0.536677.
Train: 2018-07-31T09:22:20.309172: step 5075, loss 0.519595.
Train: 2018-07-31T09:22:20.481009: step 5076, loss 0.528169.
Train: 2018-07-31T09:22:20.637192: step 5077, loss 0.613758.
Train: 2018-07-31T09:22:20.793406: step 5078, loss 0.545289.
Train: 2018-07-31T09:22:20.949653: step 5079, loss 0.605155.
Train: 2018-07-31T09:22:21.105833: step 5080, loss 0.536773.
Test: 2018-07-31T09:22:21.324531: step 5080, loss 0.5481.
Train: 2018-07-31T09:22:21.496391: step 5081, loss 0.622162.
Train: 2018-07-31T09:22:21.636988: step 5082, loss 0.639109.
Train: 2018-07-31T09:22:21.808794: step 5083, loss 0.562406.
Train: 2018-07-31T09:22:21.996278: step 5084, loss 0.55394.
Train: 2018-07-31T09:22:22.152493: step 5085, loss 0.604686.
Train: 2018-07-31T09:22:22.308701: step 5086, loss 0.545573.
Train: 2018-07-31T09:22:22.480524: step 5087, loss 0.57085.
Train: 2018-07-31T09:22:22.636724: step 5088, loss 0.579235.
Train: 2018-07-31T09:22:22.792938: step 5089, loss 0.637837.
Train: 2018-07-31T09:22:22.949151: step 5090, loss 0.53744.
Test: 2018-07-31T09:22:23.183500: step 5090, loss 0.54855.
Train: 2018-07-31T09:22:23.339715: step 5091, loss 0.620775.
Train: 2018-07-31T09:22:23.495898: step 5092, loss 0.562517.
Train: 2018-07-31T09:22:23.652112: step 5093, loss 0.603908.
Train: 2018-07-31T09:22:23.808325: step 5094, loss 0.554321.
Train: 2018-07-31T09:22:23.964566: step 5095, loss 0.628381.
Train: 2018-07-31T09:22:24.136403: step 5096, loss 0.521659.
Train: 2018-07-31T09:22:24.292616: step 5097, loss 0.595349.
Train: 2018-07-31T09:22:24.464451: step 5098, loss 0.538208.
Train: 2018-07-31T09:22:24.620664: step 5099, loss 0.611551.
Train: 2018-07-31T09:22:24.761256: step 5100, loss 0.538335.
Test: 2018-07-31T09:22:25.011171: step 5100, loss 0.549132.
Train: 2018-07-31T09:22:25.745401: step 5101, loss 0.522141.
Train: 2018-07-31T09:22:25.901585: step 5102, loss 0.50589.
Train: 2018-07-31T09:22:26.057798: step 5103, loss 0.587104.
Train: 2018-07-31T09:22:26.214042: step 5104, loss 0.587122.
Train: 2018-07-31T09:22:26.370224: step 5105, loss 0.554524.
Train: 2018-07-31T09:22:26.526468: step 5106, loss 0.513686.
Train: 2018-07-31T09:22:26.682684: step 5107, loss 0.570821.
Train: 2018-07-31T09:22:26.838895: step 5108, loss 0.513398.
Train: 2018-07-31T09:22:27.010724: step 5109, loss 0.529658.
Train: 2018-07-31T09:22:27.166943: step 5110, loss 0.537754.
Test: 2018-07-31T09:22:27.401264: step 5110, loss 0.548612.
Train: 2018-07-31T09:22:27.619961: step 5111, loss 0.512708.
Train: 2018-07-31T09:22:27.776181: step 5112, loss 0.495723.
Train: 2018-07-31T09:22:27.932392: step 5113, loss 0.629627.
Train: 2018-07-31T09:22:28.088572: step 5114, loss 0.520261.
Train: 2018-07-31T09:22:28.244786: step 5115, loss 0.613258.
Train: 2018-07-31T09:22:28.401024: step 5116, loss 0.519894.
Train: 2018-07-31T09:22:28.557213: step 5117, loss 0.570934.
Train: 2018-07-31T09:22:28.713427: step 5118, loss 0.52814.
Train: 2018-07-31T09:22:28.869640: step 5119, loss 0.579588.
Train: 2018-07-31T09:22:29.025881: step 5120, loss 0.46756.
Test: 2018-07-31T09:22:29.260204: step 5120, loss 0.5479.
Train: 2018-07-31T09:22:29.432032: step 5121, loss 0.562406.
Train: 2018-07-31T09:22:29.588251: step 5122, loss 0.571115.
Train: 2018-07-31T09:22:29.744435: step 5123, loss 0.623522.
Train: 2018-07-31T09:22:29.900647: step 5124, loss 0.553694.
Train: 2018-07-31T09:22:30.056894: step 5125, loss 0.536173.
Train: 2018-07-31T09:22:30.213100: step 5126, loss 0.571229.
Train: 2018-07-31T09:22:30.369319: step 5127, loss 0.571249.
Train: 2018-07-31T09:22:30.525502: step 5128, loss 0.571264.
Train: 2018-07-31T09:22:30.681749: step 5129, loss 0.544869.
Train: 2018-07-31T09:22:30.837958: step 5130, loss 0.553668.
Test: 2018-07-31T09:22:31.072248: step 5130, loss 0.547718.
Train: 2018-07-31T09:22:31.244083: step 5131, loss 0.518406.
Train: 2018-07-31T09:22:31.400327: step 5132, loss 0.597802.
Train: 2018-07-31T09:22:31.540920: step 5133, loss 0.588989.
Train: 2018-07-31T09:22:31.697136: step 5134, loss 0.505983.
Train: 2018-07-31T09:22:31.868968: step 5135, loss 0.482971.
Train: 2018-07-31T09:22:32.025181: step 5136, loss 0.518237.
Train: 2018-07-31T09:22:32.181394: step 5137, loss 0.58917.
Train: 2018-07-31T09:22:32.337578: step 5138, loss 0.535866.
Train: 2018-07-31T09:22:32.493821: step 5139, loss 0.526921.
Train: 2018-07-31T09:22:32.650005: step 5140, loss 0.678684.
Test: 2018-07-31T09:22:32.868734: step 5140, loss 0.547631.
Train: 2018-07-31T09:22:33.024949: step 5141, loss 0.553654.
Train: 2018-07-31T09:22:33.165539: step 5142, loss 0.509041.
Train: 2018-07-31T09:22:33.321756: step 5143, loss 0.517947.
Train: 2018-07-31T09:22:33.477960: step 5144, loss 0.580465.
Train: 2018-07-31T09:22:33.634184: step 5145, loss 0.607301.
Train: 2018-07-31T09:22:33.790361: step 5146, loss 0.59833.
Train: 2018-07-31T09:22:33.946575: step 5147, loss 0.607185.
Train: 2018-07-31T09:22:34.102819: step 5148, loss 0.518052.
Train: 2018-07-31T09:22:34.259003: step 5149, loss 0.589204.
Train: 2018-07-31T09:22:34.415246: step 5150, loss 0.518177.
Test: 2018-07-31T09:22:34.665163: step 5150, loss 0.547678.
Train: 2018-07-31T09:22:34.805774: step 5151, loss 0.571377.
Train: 2018-07-31T09:22:34.977589: step 5152, loss 0.580204.
Train: 2018-07-31T09:22:35.118206: step 5153, loss 0.571329.
Train: 2018-07-31T09:22:35.274420: step 5154, loss 0.615387.
Train: 2018-07-31T09:22:35.446249: step 5155, loss 0.562466.
Train: 2018-07-31T09:22:35.602474: step 5156, loss 0.536145.
Train: 2018-07-31T09:22:35.758676: step 5157, loss 0.606197.
Train: 2018-07-31T09:22:35.914895: step 5158, loss 0.571156.
Train: 2018-07-31T09:22:36.071102: step 5159, loss 0.536312.
Train: 2018-07-31T09:22:36.227292: step 5160, loss 0.63189.
Test: 2018-07-31T09:22:36.461643: step 5160, loss 0.547909.
Train: 2018-07-31T09:22:36.617826: step 5161, loss 0.519127.
Train: 2018-07-31T09:22:36.774070: step 5162, loss 0.579675.
Train: 2018-07-31T09:22:36.930253: step 5163, loss 0.536548.
Train: 2018-07-31T09:22:37.086466: step 5164, loss 0.596809.
Train: 2018-07-31T09:22:37.242709: step 5165, loss 0.528058.
Train: 2018-07-31T09:22:37.398922: step 5166, loss 0.588117.
Train: 2018-07-31T09:22:37.555135: step 5167, loss 0.588075.
Train: 2018-07-31T09:22:37.711320: step 5168, loss 0.519687.
Train: 2018-07-31T09:22:37.867562: step 5169, loss 0.62214.
Train: 2018-07-31T09:22:38.023776: step 5170, loss 0.570918.
Test: 2018-07-31T09:22:38.258100: step 5170, loss 0.548171.
Train: 2018-07-31T09:22:38.429901: step 5171, loss 0.545406.
Train: 2018-07-31T09:22:38.570494: step 5172, loss 0.553921.
Train: 2018-07-31T09:22:38.742352: step 5173, loss 0.621763.
Train: 2018-07-31T09:22:38.898541: step 5174, loss 0.587798.
Train: 2018-07-31T09:22:39.054784: step 5175, loss 0.503348.
Train: 2018-07-31T09:22:39.210968: step 5176, loss 0.587726.
Train: 2018-07-31T09:22:39.367213: step 5177, loss 0.537166.
Train: 2018-07-31T09:22:39.507774: step 5178, loss 0.486667.
Train: 2018-07-31T09:22:39.664018: step 5179, loss 0.528708.
Train: 2018-07-31T09:22:39.820200: step 5180, loss 0.553974.
Test: 2018-07-31T09:22:40.070166: step 5180, loss 0.548237.
Train: 2018-07-31T09:22:40.226379: step 5181, loss 0.528553.
Train: 2018-07-31T09:22:40.382599: step 5182, loss 0.545431.
Train: 2018-07-31T09:22:40.538813: step 5183, loss 0.502813.
Train: 2018-07-31T09:22:40.679404: step 5184, loss 0.570943.
Train: 2018-07-31T09:22:40.851209: step 5185, loss 0.622417.
Train: 2018-07-31T09:22:41.007421: step 5186, loss 0.579574.
Train: 2018-07-31T09:22:41.163636: step 5187, loss 0.485005.
Train: 2018-07-31T09:22:41.319873: step 5188, loss 0.502045.
Train: 2018-07-31T09:22:41.476092: step 5189, loss 0.545097.
Train: 2018-07-31T09:22:41.632275: step 5190, loss 0.605836.
Test: 2018-07-31T09:22:41.882231: step 5190, loss 0.547842.
Train: 2018-07-31T09:22:42.038457: step 5191, loss 0.518898.
Train: 2018-07-31T09:22:42.194645: step 5192, loss 0.588618.
Train: 2018-07-31T09:22:42.350888: step 5193, loss 0.588678.
Train: 2018-07-31T09:22:42.522723: step 5194, loss 0.55369.
Train: 2018-07-31T09:22:42.678906: step 5195, loss 0.492325.
Train: 2018-07-31T09:22:42.835144: step 5196, loss 0.580035.
Train: 2018-07-31T09:22:42.991362: step 5197, loss 0.56247.
Train: 2018-07-31T09:22:43.147545: step 5198, loss 0.483175.
Train: 2018-07-31T09:22:43.303792: step 5199, loss 0.57133.
Train: 2018-07-31T09:22:43.459973: step 5200, loss 0.553659.
Test: 2018-07-31T09:22:43.694328: step 5200, loss 0.54767.
Train: 2018-07-31T09:22:44.428496: step 5201, loss 0.491565.
Train: 2018-07-31T09:22:44.584739: step 5202, loss 0.500273.
Train: 2018-07-31T09:22:44.740953: step 5203, loss 0.509003.
Train: 2018-07-31T09:22:44.897136: step 5204, loss 0.562625.
Train: 2018-07-31T09:22:45.053380: step 5205, loss 0.688681.
Train: 2018-07-31T09:22:45.209562: step 5206, loss 0.544661.
Train: 2018-07-31T09:22:45.365807: step 5207, loss 0.508619.
Train: 2018-07-31T09:22:45.521989: step 5208, loss 0.526611.
Train: 2018-07-31T09:22:45.662612: step 5209, loss 0.580786.
Train: 2018-07-31T09:22:45.818796: step 5210, loss 0.580817.
Test: 2018-07-31T09:22:46.068771: step 5210, loss 0.547578.
Train: 2018-07-31T09:22:46.224949: step 5211, loss 0.644154.
Train: 2018-07-31T09:22:46.381194: step 5212, loss 0.662073.
Train: 2018-07-31T09:22:46.537408: step 5213, loss 0.553667.
Train: 2018-07-31T09:22:46.709213: step 5214, loss 0.589542.
Train: 2018-07-31T09:22:46.865425: step 5215, loss 0.607279.
Train: 2018-07-31T09:22:47.021638: step 5216, loss 0.580348.
Train: 2018-07-31T09:22:47.177852: step 5217, loss 0.562516.
Train: 2018-07-31T09:22:47.334066: step 5218, loss 0.50074.
Train: 2018-07-31T09:22:47.505931: step 5219, loss 0.553671.
Train: 2018-07-31T09:22:47.662113: step 5220, loss 0.509799.
Test: 2018-07-31T09:22:47.896435: step 5220, loss 0.547769.
Train: 2018-07-31T09:22:48.052647: step 5221, loss 0.623808.
Train: 2018-07-31T09:22:48.224482: step 5222, loss 0.509967.
Train: 2018-07-31T09:22:48.380695: step 5223, loss 0.553695.
Train: 2018-07-31T09:22:48.552555: step 5224, loss 0.623507.
Train: 2018-07-31T09:22:48.708773: step 5225, loss 0.588543.
Train: 2018-07-31T09:22:48.864988: step 5226, loss 0.562406.
Train: 2018-07-31T09:22:49.021201: step 5227, loss 0.536427.
Train: 2018-07-31T09:22:49.177417: step 5228, loss 0.536467.
Train: 2018-07-31T09:22:49.333597: step 5229, loss 0.631524.
Train: 2018-07-31T09:22:49.505433: step 5230, loss 0.562387.
Test: 2018-07-31T09:22:49.739754: step 5230, loss 0.547993.
Train: 2018-07-31T09:22:49.895996: step 5231, loss 0.571019.
Train: 2018-07-31T09:22:50.052180: step 5232, loss 0.545228.
Train: 2018-07-31T09:22:50.208392: step 5233, loss 0.545266.
Train: 2018-07-31T09:22:50.364606: step 5234, loss 0.536717.
Train: 2018-07-31T09:22:50.520820: step 5235, loss 0.570951.
Train: 2018-07-31T09:22:50.677033: step 5236, loss 0.579502.
Train: 2018-07-31T09:22:50.833276: step 5237, loss 0.502595.
Train: 2018-07-31T09:22:50.989490: step 5238, loss 0.545305.
Train: 2018-07-31T09:22:51.145674: step 5239, loss 0.605165.
Train: 2018-07-31T09:22:51.301920: step 5240, loss 0.656512.
Test: 2018-07-31T09:22:51.551860: step 5240, loss 0.548101.
Train: 2018-07-31T09:22:51.708071: step 5241, loss 0.553869.
Train: 2018-07-31T09:22:51.864254: step 5242, loss 0.468661.
Train: 2018-07-31T09:22:52.020467: step 5243, loss 0.511245.
Train: 2018-07-31T09:22:52.176714: step 5244, loss 0.519706.
Train: 2018-07-31T09:22:52.332925: step 5245, loss 0.588081.
Train: 2018-07-31T09:22:52.489139: step 5246, loss 0.665294.
Train: 2018-07-31T09:22:52.660973: step 5247, loss 0.528121.
Train: 2018-07-31T09:22:52.817196: step 5248, loss 0.553834.
Train: 2018-07-31T09:22:52.988992: step 5249, loss 0.536685.
Train: 2018-07-31T09:22:53.145236: step 5250, loss 0.528088.
Test: 2018-07-31T09:22:53.379556: step 5250, loss 0.548012.
Train: 2018-07-31T09:22:53.551390: step 5251, loss 0.579575.
Train: 2018-07-31T09:22:53.707574: step 5252, loss 0.605379.
Train: 2018-07-31T09:22:53.863817: step 5253, loss 0.545207.
Train: 2018-07-31T09:22:54.020031: step 5254, loss 0.476422.
Train: 2018-07-31T09:22:54.176244: step 5255, loss 0.553782.
Train: 2018-07-31T09:22:54.332457: step 5256, loss 0.596928.
Train: 2018-07-31T09:22:54.488669: step 5257, loss 0.519196.
Train: 2018-07-31T09:22:54.644890: step 5258, loss 0.614345.
Train: 2018-07-31T09:22:54.801097: step 5259, loss 0.562405.
Train: 2018-07-31T09:22:54.957280: step 5260, loss 0.501754.
Test: 2018-07-31T09:22:55.191635: step 5260, loss 0.547877.
Train: 2018-07-31T09:22:55.363460: step 5261, loss 0.605801.
Train: 2018-07-31T09:22:55.504057: step 5262, loss 0.588461.
Train: 2018-07-31T09:22:55.675863: step 5263, loss 0.597135.
Train: 2018-07-31T09:22:55.832106: step 5264, loss 0.501701.
Train: 2018-07-31T09:22:55.988288: step 5265, loss 0.588435.
Train: 2018-07-31T09:22:56.144533: step 5266, loss 0.553735.
Train: 2018-07-31T09:22:56.300717: step 5267, loss 0.562407.
Train: 2018-07-31T09:22:56.456929: step 5268, loss 0.536397.
Train: 2018-07-31T09:22:56.613173: step 5269, loss 0.562409.
Train: 2018-07-31T09:22:56.769386: step 5270, loss 0.536383.
Test: 2018-07-31T09:22:57.003709: step 5270, loss 0.547871.
Train: 2018-07-31T09:22:57.175511: step 5271, loss 0.553728.
Train: 2018-07-31T09:22:57.316104: step 5272, loss 0.562415.
Train: 2018-07-31T09:22:57.472316: step 5273, loss 0.631977.
Train: 2018-07-31T09:22:57.628560: step 5274, loss 0.597165.
Train: 2018-07-31T09:22:57.784776: step 5275, loss 0.579758.
Train: 2018-07-31T09:22:57.956579: step 5276, loss 0.527776.
Train: 2018-07-31T09:22:58.112791: step 5277, loss 0.596998.
Train: 2018-07-31T09:22:58.269035: step 5278, loss 0.614199.
Train: 2018-07-31T09:22:58.425218: step 5279, loss 0.614059.
Train: 2018-07-31T09:22:58.581431: step 5280, loss 0.553815.
Test: 2018-07-31T09:22:58.815787: step 5280, loss 0.54807.
Train: 2018-07-31T09:22:58.971995: step 5281, loss 0.536734.
Train: 2018-07-31T09:22:59.128178: step 5282, loss 0.579471.
Train: 2018-07-31T09:22:59.284392: step 5283, loss 0.622021.
Train: 2018-07-31T09:22:59.440645: step 5284, loss 0.519962.
Train: 2018-07-31T09:22:59.596850: step 5285, loss 0.544338.
Train: 2018-07-31T09:22:59.753068: step 5286, loss 0.494728.
Train: 2018-07-31T09:22:59.909276: step 5287, loss 0.604746.
Train: 2018-07-31T09:23:00.065460: step 5288, loss 0.52857.
Train: 2018-07-31T09:23:00.221673: step 5289, loss 0.443906.
Train: 2018-07-31T09:23:00.377910: step 5290, loss 0.528399.
Test: 2018-07-31T09:23:00.612232: step 5290, loss 0.548102.
Train: 2018-07-31T09:23:00.768450: step 5291, loss 0.664668.
Train: 2018-07-31T09:23:00.924632: step 5292, loss 0.630996.
Train: 2018-07-31T09:23:01.080876: step 5293, loss 0.536925.
Train: 2018-07-31T09:23:01.252711: step 5294, loss 0.536786.
Train: 2018-07-31T09:23:01.408894: step 5295, loss 0.55382.
Train: 2018-07-31T09:23:01.565107: step 5296, loss 0.57096.
Train: 2018-07-31T09:23:01.721357: step 5297, loss 0.605072.
Train: 2018-07-31T09:23:01.877565: step 5298, loss 0.562433.
Train: 2018-07-31T09:23:02.049370: step 5299, loss 0.613562.
Train: 2018-07-31T09:23:02.189962: step 5300, loss 0.536864.
Test: 2018-07-31T09:23:02.439937: step 5300, loss 0.548156.
Train: 2018-07-31T09:23:03.189762: step 5301, loss 0.579417.
Train: 2018-07-31T09:23:03.345971: step 5302, loss 0.596401.
Train: 2018-07-31T09:23:03.502186: step 5303, loss 0.545436.
Train: 2018-07-31T09:23:03.674021: step 5304, loss 0.520004.
Train: 2018-07-31T09:23:03.830233: step 5305, loss 0.545452.
Train: 2018-07-31T09:23:03.986449: step 5306, loss 0.553918.
Train: 2018-07-31T09:23:04.158281: step 5307, loss 0.562413.
Train: 2018-07-31T09:23:04.314498: step 5308, loss 0.621873.
Train: 2018-07-31T09:23:04.470708: step 5309, loss 0.613414.
Train: 2018-07-31T09:23:04.626891: step 5310, loss 0.579299.
Test: 2018-07-31T09:23:04.845589: step 5310, loss 0.548231.
Train: 2018-07-31T09:23:05.001837: step 5311, loss 0.54549.
Train: 2018-07-31T09:23:05.173638: step 5312, loss 0.621651.
Train: 2018-07-31T09:23:05.314230: step 5313, loss 0.579332.
Train: 2018-07-31T09:23:05.470444: step 5314, loss 0.579316.
Train: 2018-07-31T09:23:05.626658: step 5315, loss 0.596045.
Train: 2018-07-31T09:23:05.798492: step 5316, loss 0.520548.
Train: 2018-07-31T09:23:05.954730: step 5317, loss 0.587535.
Train: 2018-07-31T09:23:06.095298: step 5318, loss 0.520745.
Train: 2018-07-31T09:23:06.267132: step 5319, loss 0.562526.
Train: 2018-07-31T09:23:06.423345: step 5320, loss 0.562495.
Test: 2018-07-31T09:23:06.657702: step 5320, loss 0.548517.
Train: 2018-07-31T09:23:06.829527: step 5321, loss 0.479044.
Train: 2018-07-31T09:23:06.970123: step 5322, loss 0.55409.
Train: 2018-07-31T09:23:07.141928: step 5323, loss 0.579301.
Train: 2018-07-31T09:23:07.298173: step 5324, loss 0.612785.
Train: 2018-07-31T09:23:07.454354: step 5325, loss 0.562397.
Train: 2018-07-31T09:23:07.626220: step 5326, loss 0.53718.
Train: 2018-07-31T09:23:07.782432: step 5327, loss 0.54563.
Train: 2018-07-31T09:23:07.938615: step 5328, loss 0.570942.
Train: 2018-07-31T09:23:08.094859: step 5329, loss 0.545433.
Train: 2018-07-31T09:23:08.266688: step 5330, loss 0.50329.
Test: 2018-07-31T09:23:08.500986: step 5330, loss 0.548226.
Train: 2018-07-31T09:23:08.657197: step 5331, loss 0.553867.
Train: 2018-07-31T09:23:08.829033: step 5332, loss 0.50299.
Train: 2018-07-31T09:23:08.969666: step 5333, loss 0.511063.
Train: 2018-07-31T09:23:09.141489: step 5334, loss 0.622534.
Train: 2018-07-31T09:23:09.297673: step 5335, loss 0.545118.
Train: 2018-07-31T09:23:09.453915: step 5336, loss 0.57984.
Train: 2018-07-31T09:23:09.610130: step 5337, loss 0.605363.
Train: 2018-07-31T09:23:09.766313: step 5338, loss 0.631474.
Train: 2018-07-31T09:23:09.922526: step 5339, loss 0.536511.
Train: 2018-07-31T09:23:10.094391: step 5340, loss 0.544862.
Test: 2018-07-31T09:23:10.328707: step 5340, loss 0.547875.
Train: 2018-07-31T09:23:10.484895: step 5341, loss 0.536027.
Train: 2018-07-31T09:23:10.641137: step 5342, loss 0.683922.
Train: 2018-07-31T09:23:10.797351: step 5343, loss 0.527997.
Train: 2018-07-31T09:23:10.953535: step 5344, loss 0.640511.
Train: 2018-07-31T09:23:11.109748: step 5345, loss 0.527888.
Train: 2018-07-31T09:23:11.265961: step 5346, loss 0.536731.
Train: 2018-07-31T09:23:11.422205: step 5347, loss 0.553705.
Train: 2018-07-31T09:23:11.578389: step 5348, loss 0.579797.
Train: 2018-07-31T09:23:11.750248: step 5349, loss 0.553554.
Train: 2018-07-31T09:23:11.906466: step 5350, loss 0.459651.
Test: 2018-07-31T09:23:12.140757: step 5350, loss 0.547998.
Train: 2018-07-31T09:23:12.296971: step 5351, loss 0.596958.
Train: 2018-07-31T09:23:12.453214: step 5352, loss 0.536437.
Train: 2018-07-31T09:23:12.609427: step 5353, loss 0.519614.
Train: 2018-07-31T09:23:12.765610: step 5354, loss 0.588467.
Train: 2018-07-31T09:23:12.921823: step 5355, loss 0.545725.
Train: 2018-07-31T09:23:13.078037: step 5356, loss 0.519156.
Train: 2018-07-31T09:23:13.234251: step 5357, loss 0.57939.
Train: 2018-07-31T09:23:13.390493: step 5358, loss 0.579753.
Train: 2018-07-31T09:23:13.546676: step 5359, loss 0.553873.
Train: 2018-07-31T09:23:13.702890: step 5360, loss 0.536514.
Test: 2018-07-31T09:23:13.937212: step 5360, loss 0.54785.
Train: 2018-07-31T09:23:14.093424: step 5361, loss 0.58896.
Train: 2018-07-31T09:23:14.249668: step 5362, loss 0.65814.
Train: 2018-07-31T09:23:14.405851: step 5363, loss 0.553799.
Train: 2018-07-31T09:23:14.562098: step 5364, loss 0.545108.
Train: 2018-07-31T09:23:14.718307: step 5365, loss 0.570907.
Train: 2018-07-31T09:23:14.890143: step 5366, loss 0.553847.
Train: 2018-07-31T09:23:15.046356: step 5367, loss 0.502003.
Train: 2018-07-31T09:23:15.202570: step 5368, loss 0.545251.
Train: 2018-07-31T09:23:15.358783: step 5369, loss 0.553811.
Train: 2018-07-31T09:23:15.514998: step 5370, loss 0.570995.
Test: 2018-07-31T09:23:15.749317: step 5370, loss 0.547928.
Train: 2018-07-31T09:23:15.921122: step 5371, loss 0.570997.
Train: 2018-07-31T09:23:16.077335: step 5372, loss 0.59696.
Train: 2018-07-31T09:23:16.233547: step 5373, loss 0.614157.
Train: 2018-07-31T09:23:16.389762: step 5374, loss 0.527876.
Train: 2018-07-31T09:23:16.545974: step 5375, loss 0.527867.
Train: 2018-07-31T09:23:16.702188: step 5376, loss 0.58828.
Train: 2018-07-31T09:23:16.874023: step 5377, loss 0.545143.
Train: 2018-07-31T09:23:17.030237: step 5378, loss 0.51938.
Train: 2018-07-31T09:23:17.186484: step 5379, loss 0.605412.
Train: 2018-07-31T09:23:17.342693: step 5380, loss 0.545184.
Test: 2018-07-31T09:23:17.576985: step 5380, loss 0.547957.
Train: 2018-07-31T09:23:17.733196: step 5381, loss 0.562337.
Train: 2018-07-31T09:23:17.889441: step 5382, loss 0.510614.
Train: 2018-07-31T09:23:18.045624: step 5383, loss 0.46725.
Train: 2018-07-31T09:23:18.201867: step 5384, loss 0.588405.
Train: 2018-07-31T09:23:18.373671: step 5385, loss 0.623364.
Train: 2018-07-31T09:23:18.529910: step 5386, loss 0.52763.
Train: 2018-07-31T09:23:18.686133: step 5387, loss 0.553784.
Train: 2018-07-31T09:23:18.842313: step 5388, loss 0.466549.
Train: 2018-07-31T09:23:18.998562: step 5389, loss 0.562631.
Train: 2018-07-31T09:23:19.170391: step 5390, loss 0.500963.
Test: 2018-07-31T09:23:19.404707: step 5390, loss 0.547724.
Train: 2018-07-31T09:23:19.560893: step 5391, loss 0.579945.
Train: 2018-07-31T09:23:19.717138: step 5392, loss 0.588699.
Train: 2018-07-31T09:23:19.873351: step 5393, loss 0.536213.
Train: 2018-07-31T09:23:20.029564: step 5394, loss 0.615969.
Train: 2018-07-31T09:23:20.201400: step 5395, loss 0.526899.
Train: 2018-07-31T09:23:20.357581: step 5396, loss 0.562326.
Train: 2018-07-31T09:23:20.513795: step 5397, loss 0.544635.
Train: 2018-07-31T09:23:20.670008: step 5398, loss 0.624984.
Train: 2018-07-31T09:23:20.826253: step 5399, loss 0.535674.
Train: 2018-07-31T09:23:20.982466: step 5400, loss 0.517897.
Test: 2018-07-31T09:23:21.216787: step 5400, loss 0.547644.
Train: 2018-07-31T09:23:21.919717: step 5401, loss 0.642822.
Train: 2018-07-31T09:23:22.075960: step 5402, loss 0.588884.
Train: 2018-07-31T09:23:22.232173: step 5403, loss 0.571285.
Train: 2018-07-31T09:23:22.388386: step 5404, loss 0.580528.
Train: 2018-07-31T09:23:22.544600: step 5405, loss 0.553964.
Train: 2018-07-31T09:23:22.700784: step 5406, loss 0.527188.
Train: 2018-07-31T09:23:22.872619: step 5407, loss 0.580057.
Train: 2018-07-31T09:23:23.028865: step 5408, loss 0.536235.
Train: 2018-07-31T09:23:23.185046: step 5409, loss 0.527417.
Train: 2018-07-31T09:23:23.341288: step 5410, loss 0.606107.
Test: 2018-07-31T09:23:23.575580: step 5410, loss 0.547791.
Train: 2018-07-31T09:23:23.731792: step 5411, loss 0.553807.
Train: 2018-07-31T09:23:23.888005: step 5412, loss 0.579744.
Train: 2018-07-31T09:23:24.044249: step 5413, loss 0.518922.
Train: 2018-07-31T09:23:24.200463: step 5414, loss 0.614685.
Train: 2018-07-31T09:23:24.356646: step 5415, loss 0.588677.
Train: 2018-07-31T09:23:24.512860: step 5416, loss 0.536305.
Train: 2018-07-31T09:23:24.669073: step 5417, loss 0.60572.
Train: 2018-07-31T09:23:24.825317: step 5418, loss 0.588349.
Train: 2018-07-31T09:23:24.981499: step 5419, loss 0.605516.
Train: 2018-07-31T09:23:25.137713: step 5420, loss 0.571017.
Test: 2018-07-31T09:23:25.372064: step 5420, loss 0.548072.
Train: 2018-07-31T09:23:25.528276: step 5421, loss 0.554029.
Train: 2018-07-31T09:23:25.684461: step 5422, loss 0.570921.
Train: 2018-07-31T09:23:25.840704: step 5423, loss 0.596432.
Train: 2018-07-31T09:23:25.996886: step 5424, loss 0.486119.
Train: 2018-07-31T09:23:26.153100: step 5425, loss 0.570871.
Train: 2018-07-31T09:23:26.309314: step 5426, loss 0.520131.
Train: 2018-07-31T09:23:26.465557: step 5427, loss 0.520141.
Train: 2018-07-31T09:23:26.637362: step 5428, loss 0.587831.
Train: 2018-07-31T09:23:26.793574: step 5429, loss 0.596307.
Train: 2018-07-31T09:23:26.949788: step 5430, loss 0.562427.
Test: 2018-07-31T09:23:27.184143: step 5430, loss 0.548238.
Train: 2018-07-31T09:23:27.355968: step 5431, loss 0.587819.
Train: 2018-07-31T09:23:27.512181: step 5432, loss 0.587801.
Train: 2018-07-31T09:23:27.668395: step 5433, loss 0.621577.
Train: 2018-07-31T09:23:27.808998: step 5434, loss 0.51184.
Train: 2018-07-31T09:23:27.980798: step 5435, loss 0.511883.
Train: 2018-07-31T09:23:28.121419: step 5436, loss 0.526448.
Train: 2018-07-31T09:23:28.277603: step 5437, loss 0.58775.
Train: 2018-07-31T09:23:28.433850: step 5438, loss 0.562418.
Train: 2018-07-31T09:23:28.590060: step 5439, loss 0.545499.
Train: 2018-07-31T09:23:28.746274: step 5440, loss 0.511613.
Test: 2018-07-31T09:23:28.996185: step 5440, loss 0.548197.
Train: 2018-07-31T09:23:29.152398: step 5441, loss 0.553926.
Train: 2018-07-31T09:23:29.308645: step 5442, loss 0.553885.
Train: 2018-07-31T09:23:29.464855: step 5443, loss 0.562428.
Train: 2018-07-31T09:23:29.621039: step 5444, loss 0.545347.
Train: 2018-07-31T09:23:29.777282: step 5445, loss 0.648026.
Train: 2018-07-31T09:23:29.933489: step 5446, loss 0.596648.
Train: 2018-07-31T09:23:30.089679: step 5447, loss 0.493902.
Train: 2018-07-31T09:23:30.245892: step 5448, loss 0.536712.
Train: 2018-07-31T09:23:30.402129: step 5449, loss 0.510975.
Train: 2018-07-31T09:23:30.558352: step 5450, loss 0.588216.
Test: 2018-07-31T09:23:30.792672: step 5450, loss 0.547974.
Train: 2018-07-31T09:23:30.948852: step 5451, loss 0.5194.
Train: 2018-07-31T09:23:31.105096: step 5452, loss 0.536501.
Train: 2018-07-31T09:23:31.261280: step 5453, loss 0.614336.
Train: 2018-07-31T09:23:31.433138: step 5454, loss 0.545062.
Train: 2018-07-31T09:23:31.589328: step 5455, loss 0.562451.
Train: 2018-07-31T09:23:31.745574: step 5456, loss 0.562411.
Train: 2018-07-31T09:23:31.901754: step 5457, loss 0.59724.
Train: 2018-07-31T09:23:32.042347: step 5458, loss 0.492833.
Train: 2018-07-31T09:23:32.198590: step 5459, loss 0.536304.
Train: 2018-07-31T09:23:32.370418: step 5460, loss 0.588623.
Test: 2018-07-31T09:23:32.604745: step 5460, loss 0.547805.
Train: 2018-07-31T09:23:32.760927: step 5461, loss 0.606119.
Train: 2018-07-31T09:23:32.917171: step 5462, loss 0.518777.
Train: 2018-07-31T09:23:33.073384: step 5463, loss 0.562398.
Train: 2018-07-31T09:23:33.229598: step 5464, loss 0.58865.
Train: 2018-07-31T09:23:33.385811: step 5465, loss 0.64117.
Train: 2018-07-31T09:23:33.542027: step 5466, loss 0.623517.
Train: 2018-07-31T09:23:33.698244: step 5467, loss 0.666789.
Train: 2018-07-31T09:23:33.870043: step 5468, loss 0.614361.
Train: 2018-07-31T09:23:34.026294: step 5469, loss 0.622648.
Train: 2018-07-31T09:23:34.182469: step 5470, loss 0.588056.
Test: 2018-07-31T09:23:34.416823: step 5470, loss 0.548192.
Train: 2018-07-31T09:23:34.573034: step 5471, loss 0.519952.
Train: 2018-07-31T09:23:34.729218: step 5472, loss 0.629968.
Train: 2018-07-31T09:23:34.885431: step 5473, loss 0.512105.
Train: 2018-07-31T09:23:35.041644: step 5474, loss 0.520598.
Train: 2018-07-31T09:23:35.197882: step 5475, loss 0.545827.
Train: 2018-07-31T09:23:35.369693: step 5476, loss 0.604209.
Train: 2018-07-31T09:23:35.541559: step 5477, loss 0.579183.
Train: 2018-07-31T09:23:35.697770: step 5478, loss 0.529167.
Train: 2018-07-31T09:23:35.853953: step 5479, loss 0.470604.
Train: 2018-07-31T09:23:36.010167: step 5480, loss 0.546685.
Test: 2018-07-31T09:23:36.244518: step 5480, loss 0.548335.
Train: 2018-07-31T09:23:36.400732: step 5481, loss 0.495186.
Train: 2018-07-31T09:23:36.556915: step 5482, loss 0.571938.
Train: 2018-07-31T09:23:36.728779: step 5483, loss 0.604398.
Train: 2018-07-31T09:23:36.884986: step 5484, loss 0.578402.
Train: 2018-07-31T09:23:37.041200: step 5485, loss 0.58957.
Train: 2018-07-31T09:23:37.213035: step 5486, loss 0.614694.
Train: 2018-07-31T09:23:37.369257: step 5487, loss 0.527738.
Train: 2018-07-31T09:23:37.525437: step 5488, loss 0.62148.
Train: 2018-07-31T09:23:37.681650: step 5489, loss 0.571061.
Train: 2018-07-31T09:23:37.837895: step 5490, loss 0.603915.
Test: 2018-07-31T09:23:38.087806: step 5490, loss 0.548403.
Train: 2018-07-31T09:23:38.244049: step 5491, loss 0.487223.
Train: 2018-07-31T09:23:38.400262: step 5492, loss 0.578866.
Train: 2018-07-31T09:23:38.556480: step 5493, loss 0.562264.
Train: 2018-07-31T09:23:38.712684: step 5494, loss 0.562648.
Train: 2018-07-31T09:23:38.868872: step 5495, loss 0.520745.
Train: 2018-07-31T09:23:39.025086: step 5496, loss 0.587692.
Train: 2018-07-31T09:23:39.196950: step 5497, loss 0.528822.
Train: 2018-07-31T09:23:39.353165: step 5498, loss 0.562274.
Train: 2018-07-31T09:23:39.509347: step 5499, loss 0.536624.
Train: 2018-07-31T09:23:39.665591: step 5500, loss 0.563612.
Test: 2018-07-31T09:23:39.899911: step 5500, loss 0.548205.
Train: 2018-07-31T09:23:40.665327: step 5501, loss 0.537343.
Train: 2018-07-31T09:23:40.821539: step 5502, loss 0.51899.
Train: 2018-07-31T09:23:40.977754: step 5503, loss 0.587664.
Train: 2018-07-31T09:23:41.133997: step 5504, loss 0.632162.
Train: 2018-07-31T09:23:41.290210: step 5505, loss 0.570766.
Train: 2018-07-31T09:23:41.446394: step 5506, loss 0.527835.
Train: 2018-07-31T09:23:41.602608: step 5507, loss 0.510546.
Train: 2018-07-31T09:23:41.758851: step 5508, loss 0.510432.
Train: 2018-07-31T09:23:41.915074: step 5509, loss 0.581687.
Train: 2018-07-31T09:23:42.086899: step 5510, loss 0.526978.
Test: 2018-07-31T09:23:42.321220: step 5510, loss 0.54784.
Train: 2018-07-31T09:23:42.477432: step 5511, loss 0.545144.
Train: 2018-07-31T09:23:42.633615: step 5512, loss 0.493299.
Train: 2018-07-31T09:23:42.789829: step 5513, loss 0.51747.
Train: 2018-07-31T09:23:42.946073: step 5514, loss 0.535679.
Train: 2018-07-31T09:23:43.102256: step 5515, loss 0.570678.
Train: 2018-07-31T09:23:43.258494: step 5516, loss 0.599419.
Train: 2018-07-31T09:23:43.414684: step 5517, loss 0.635821.
Train: 2018-07-31T09:23:43.570926: step 5518, loss 0.589281.
Train: 2018-07-31T09:23:43.727110: step 5519, loss 0.545461.
Train: 2018-07-31T09:23:43.883324: step 5520, loss 0.55448.
Test: 2018-07-31T09:23:44.117674: step 5520, loss 0.547638.
Train: 2018-07-31T09:23:44.289502: step 5521, loss 0.544535.
Train: 2018-07-31T09:23:44.445722: step 5522, loss 0.669872.
Train: 2018-07-31T09:23:44.617563: step 5523, loss 0.482794.
Train: 2018-07-31T09:23:44.773740: step 5524, loss 0.579696.
Train: 2018-07-31T09:23:44.929987: step 5525, loss 0.597536.
Train: 2018-07-31T09:23:45.086202: step 5526, loss 0.588138.
Train: 2018-07-31T09:23:45.242410: step 5527, loss 0.56235.
Train: 2018-07-31T09:23:45.398624: step 5528, loss 0.623072.
Train: 2018-07-31T09:23:45.554807: step 5529, loss 0.493406.
Train: 2018-07-31T09:23:45.711019: step 5530, loss 0.59677.
Test: 2018-07-31T09:23:45.945365: step 5530, loss 0.548048.
Train: 2018-07-31T09:23:46.101554: step 5531, loss 0.570788.
Train: 2018-07-31T09:23:46.257797: step 5532, loss 0.545265.
Train: 2018-07-31T09:23:46.414013: step 5533, loss 0.545431.
Train: 2018-07-31T09:23:46.570195: step 5534, loss 0.562409.
Train: 2018-07-31T09:23:46.726438: step 5535, loss 0.536811.
Train: 2018-07-31T09:23:46.882621: step 5536, loss 0.519883.
Train: 2018-07-31T09:23:47.038834: step 5537, loss 0.502813.
Train: 2018-07-31T09:23:47.195077: step 5538, loss 0.536819.
Train: 2018-07-31T09:23:47.351291: step 5539, loss 0.536799.
Train: 2018-07-31T09:23:47.507475: step 5540, loss 0.562335.
Test: 2018-07-31T09:23:47.741820: step 5540, loss 0.547995.
Train: 2018-07-31T09:23:47.898007: step 5541, loss 0.66557.
Train: 2018-07-31T09:23:48.054222: step 5542, loss 0.596815.
Train: 2018-07-31T09:23:48.210434: step 5543, loss 0.605382.
Train: 2018-07-31T09:23:48.366678: step 5544, loss 0.605304.
Train: 2018-07-31T09:23:48.522862: step 5545, loss 0.536706.
Train: 2018-07-31T09:23:48.679076: step 5546, loss 0.53682.
Train: 2018-07-31T09:23:48.835313: step 5547, loss 0.511199.
Train: 2018-07-31T09:23:48.991503: step 5548, loss 0.570931.
Train: 2018-07-31T09:23:49.163361: step 5549, loss 0.528271.
Train: 2018-07-31T09:23:49.303963: step 5550, loss 0.579491.
Test: 2018-07-31T09:23:49.538277: step 5550, loss 0.548085.
Train: 2018-07-31T09:23:49.710109: step 5551, loss 0.519624.
Train: 2018-07-31T09:23:49.850676: step 5552, loss 0.570983.
Train: 2018-07-31T09:23:50.022541: step 5553, loss 0.519523.
Train: 2018-07-31T09:23:50.178724: step 5554, loss 0.519467.
Train: 2018-07-31T09:23:50.334967: step 5555, loss 0.527981.
Train: 2018-07-31T09:23:50.491183: step 5556, loss 0.536514.
Train: 2018-07-31T09:23:50.647364: step 5557, loss 0.510427.
Train: 2018-07-31T09:23:50.803612: step 5558, loss 0.492889.
Train: 2018-07-31T09:23:50.959791: step 5559, loss 0.510074.
Train: 2018-07-31T09:23:51.116004: step 5560, loss 0.527306.
Test: 2018-07-31T09:23:51.350350: step 5560, loss 0.547701.
Train: 2018-07-31T09:23:51.506568: step 5561, loss 0.624311.
Train: 2018-07-31T09:23:51.662783: step 5562, loss 0.598023.
Train: 2018-07-31T09:23:51.818989: step 5563, loss 0.509217.
Train: 2018-07-31T09:23:51.975178: step 5564, loss 0.616035.
Train: 2018-07-31T09:23:52.131391: step 5565, loss 0.580394.
Train: 2018-07-31T09:23:52.287633: step 5566, loss 0.616159.
Train: 2018-07-31T09:23:52.443849: step 5567, loss 0.53581.
Train: 2018-07-31T09:23:52.600062: step 5568, loss 0.580412.
Train: 2018-07-31T09:23:52.756245: step 5569, loss 0.571504.
Train: 2018-07-31T09:23:52.912491: step 5570, loss 0.544732.
Test: 2018-07-31T09:23:53.162428: step 5570, loss 0.547649.
Train: 2018-07-31T09:23:53.318644: step 5571, loss 0.642669.
Train: 2018-07-31T09:23:53.474857: step 5572, loss 0.509307.
Train: 2018-07-31T09:23:53.631070: step 5573, loss 0.589117.
Train: 2018-07-31T09:23:53.802875: step 5574, loss 0.615602.
Train: 2018-07-31T09:23:53.959125: step 5575, loss 0.544885.
Train: 2018-07-31T09:23:54.115332: step 5576, loss 0.606476.
Train: 2018-07-31T09:23:54.271515: step 5577, loss 0.527354.
Train: 2018-07-31T09:23:54.443374: step 5578, loss 0.562441.
Train: 2018-07-31T09:23:54.599596: step 5579, loss 0.579932.
Train: 2018-07-31T09:23:54.755778: step 5580, loss 0.579871.
Test: 2018-07-31T09:23:54.990128: step 5580, loss 0.547862.
Train: 2018-07-31T09:23:55.161964: step 5581, loss 0.518925.
Train: 2018-07-31T09:23:55.318177: step 5582, loss 0.553722.
Train: 2018-07-31T09:23:55.474389: step 5583, loss 0.570985.
Train: 2018-07-31T09:23:55.630602: step 5584, loss 0.588401.
Train: 2018-07-31T09:23:55.786816: step 5585, loss 0.571122.
Train: 2018-07-31T09:23:55.958653: step 5586, loss 0.5279.
Train: 2018-07-31T09:23:56.114859: step 5587, loss 0.635887.
Train: 2018-07-31T09:23:56.271077: step 5588, loss 0.553813.
Train: 2018-07-31T09:23:56.442881: step 5589, loss 0.57085.
Train: 2018-07-31T09:23:56.599131: step 5590, loss 0.485643.
Test: 2018-07-31T09:23:56.833447: step 5590, loss 0.548074.
Train: 2018-07-31T09:23:57.005249: step 5591, loss 0.596657.
Train: 2018-07-31T09:23:57.161494: step 5592, loss 0.579469.
Train: 2018-07-31T09:23:57.333328: step 5593, loss 0.545293.
Train: 2018-07-31T09:23:57.489513: step 5594, loss 0.519792.
Train: 2018-07-31T09:23:57.645725: step 5595, loss 0.485557.
Train: 2018-07-31T09:23:57.801938: step 5596, loss 0.588048.
Train: 2018-07-31T09:23:57.958182: step 5597, loss 0.631038.
Train: 2018-07-31T09:23:58.130011: step 5598, loss 0.545295.
Train: 2018-07-31T09:23:58.286201: step 5599, loss 0.570947.
Train: 2018-07-31T09:23:58.442444: step 5600, loss 0.562395.
Test: 2018-07-31T09:23:58.661112: step 5600, loss 0.548041.
Train: 2018-07-31T09:23:59.395316: step 5601, loss 0.553812.
Train: 2018-07-31T09:23:59.551563: step 5602, loss 0.519532.
Train: 2018-07-31T09:23:59.707772: step 5603, loss 0.622469.
Train: 2018-07-31T09:23:59.879608: step 5604, loss 0.519496.
Train: 2018-07-31T09:24:00.035790: step 5605, loss 0.596732.
Train: 2018-07-31T09:24:00.192005: step 5606, loss 0.570985.
Train: 2018-07-31T09:24:00.363838: step 5607, loss 0.588147.
Train: 2018-07-31T09:24:00.520052: step 5608, loss 0.528115.
Train: 2018-07-31T09:24:00.676265: step 5609, loss 0.536676.
Train: 2018-07-31T09:24:00.816858: step 5610, loss 0.570974.
Test: 2018-07-31T09:24:01.066830: step 5610, loss 0.548026.
Train: 2018-07-31T09:24:01.223012: step 5611, loss 0.528077.
Train: 2018-07-31T09:24:01.379256: step 5612, loss 0.579572.
Train: 2018-07-31T09:24:01.519848: step 5613, loss 0.502246.
Train: 2018-07-31T09:24:01.691653: step 5614, loss 0.562398.
Train: 2018-07-31T09:24:01.847865: step 5615, loss 0.50205.
Train: 2018-07-31T09:24:02.004081: step 5616, loss 0.510538.
Train: 2018-07-31T09:24:02.160318: step 5617, loss 0.458328.
Train: 2018-07-31T09:24:02.316536: step 5618, loss 0.518834.
Train: 2018-07-31T09:24:02.472750: step 5619, loss 0.597515.
Train: 2018-07-31T09:24:02.628934: step 5620, loss 0.606482.
Test: 2018-07-31T09:24:02.878875: step 5620, loss 0.547709.
Train: 2018-07-31T09:24:03.035089: step 5621, loss 0.597792.
Train: 2018-07-31T09:24:03.191302: step 5622, loss 0.562496.
Train: 2018-07-31T09:24:03.347516: step 5623, loss 0.544809.
Train: 2018-07-31T09:24:03.503759: step 5624, loss 0.553662.
Train: 2018-07-31T09:24:03.659941: step 5625, loss 0.53592.
Train: 2018-07-31T09:24:03.831808: step 5626, loss 0.553651.
Train: 2018-07-31T09:24:03.987991: step 5627, loss 0.544757.
Train: 2018-07-31T09:24:04.144204: step 5628, loss 0.535838.
Train: 2018-07-31T09:24:04.300416: step 5629, loss 0.526888.
Train: 2018-07-31T09:24:04.456630: step 5630, loss 0.616244.
Test: 2018-07-31T09:24:04.690950: step 5630, loss 0.54762.
Train: 2018-07-31T09:24:04.862786: step 5631, loss 0.63416.
Train: 2018-07-31T09:24:05.019029: step 5632, loss 0.491111.
Train: 2018-07-31T09:24:05.175212: step 5633, loss 0.562591.
Train: 2018-07-31T09:24:05.347047: step 5634, loss 0.508979.
Train: 2018-07-31T09:24:05.503260: step 5635, loss 0.544709.
Train: 2018-07-31T09:24:05.659507: step 5636, loss 0.526803.
Train: 2018-07-31T09:24:05.815686: step 5637, loss 0.472987.
Train: 2018-07-31T09:24:05.971931: step 5638, loss 0.697477.
Train: 2018-07-31T09:24:06.143765: step 5639, loss 0.580614.
Train: 2018-07-31T09:24:06.299949: step 5640, loss 0.643421.
Test: 2018-07-31T09:24:06.534269: step 5640, loss 0.547616.
Train: 2018-07-31T09:24:06.690481: step 5641, loss 0.553657.
Train: 2018-07-31T09:24:06.846696: step 5642, loss 0.571517.
Train: 2018-07-31T09:24:07.002939: step 5643, loss 0.526929.
Train: 2018-07-31T09:24:07.174778: step 5644, loss 0.509188.
Train: 2018-07-31T09:24:07.330958: step 5645, loss 0.571428.
Train: 2018-07-31T09:24:07.487195: step 5646, loss 0.624686.
Train: 2018-07-31T09:24:07.659006: step 5647, loss 0.580233.
Train: 2018-07-31T09:24:07.815218: step 5648, loss 0.589009.
Train: 2018-07-31T09:24:07.971432: step 5649, loss 0.562475.
Train: 2018-07-31T09:24:08.127676: step 5650, loss 0.536106.
Test: 2018-07-31T09:24:08.361998: step 5650, loss 0.547768.
Train: 2018-07-31T09:24:08.533801: step 5651, loss 0.571218.
Train: 2018-07-31T09:24:08.690041: step 5652, loss 0.56244.
Train: 2018-07-31T09:24:08.846252: step 5653, loss 0.527508.
Train: 2018-07-31T09:24:09.002471: step 5654, loss 0.510117.
Train: 2018-07-31T09:24:09.158655: step 5655, loss 0.623451.
Train: 2018-07-31T09:24:09.330514: step 5656, loss 0.597241.
Train: 2018-07-31T09:24:09.486702: step 5657, loss 0.545037.
Train: 2018-07-31T09:24:09.642915: step 5658, loss 0.545063.
Train: 2018-07-31T09:24:09.799163: step 5659, loss 0.545079.
Train: 2018-07-31T09:24:09.955366: step 5660, loss 0.562402.
Test: 2018-07-31T09:24:10.189694: step 5660, loss 0.547914.
Train: 2018-07-31T09:24:10.345907: step 5661, loss 0.571054.
Train: 2018-07-31T09:24:10.502089: step 5662, loss 0.579692.
Train: 2018-07-31T09:24:10.658303: step 5663, loss 0.510591.
Train: 2018-07-31T09:24:10.814516: step 5664, loss 0.484683.
Train: 2018-07-31T09:24:10.970761: step 5665, loss 0.527809.
Train: 2018-07-31T09:24:11.126974: step 5666, loss 0.55374.
Train: 2018-07-31T09:24:11.283187: step 5667, loss 0.571101.
Train: 2018-07-31T09:24:11.439370: step 5668, loss 0.510218.
Train: 2018-07-31T09:24:11.595614: step 5669, loss 0.544984.
Train: 2018-07-31T09:24:11.751827: step 5670, loss 0.492485.
Test: 2018-07-31T09:24:11.986118: step 5670, loss 0.547758.
Train: 2018-07-31T09:24:12.157952: step 5671, loss 0.580007.
Train: 2018-07-31T09:24:12.314165: step 5672, loss 0.553671.
Train: 2018-07-31T09:24:12.470409: step 5673, loss 0.562485.
Train: 2018-07-31T09:24:12.642213: step 5674, loss 0.606708.
Train: 2018-07-31T09:24:12.798428: step 5675, loss 0.553657.
Train: 2018-07-31T09:24:12.954665: step 5676, loss 0.544803.
Train: 2018-07-31T09:24:13.110853: step 5677, loss 0.615712.
Train: 2018-07-31T09:24:13.282721: step 5678, loss 0.518209.
Train: 2018-07-31T09:24:13.438937: step 5679, loss 0.713233.
Train: 2018-07-31T09:24:13.595147: step 5680, loss 0.5625.
Test: 2018-07-31T09:24:13.845057: step 5680, loss 0.547716.
Train: 2018-07-31T09:24:14.001300: step 5681, loss 0.650653.
Train: 2018-07-31T09:24:14.157516: step 5682, loss 0.536121.
Train: 2018-07-31T09:24:14.313728: step 5683, loss 0.562439.
Train: 2018-07-31T09:24:14.469941: step 5684, loss 0.518825.
Train: 2018-07-31T09:24:14.626125: step 5685, loss 0.640737.
Train: 2018-07-31T09:24:14.797958: step 5686, loss 0.545067.
Train: 2018-07-31T09:24:14.954171: step 5687, loss 0.605629.
Train: 2018-07-31T09:24:15.110386: step 5688, loss 0.562399.
Train: 2018-07-31T09:24:15.266624: step 5689, loss 0.562397.
Train: 2018-07-31T09:24:15.422814: step 5690, loss 0.570959.
Test: 2018-07-31T09:24:15.657166: step 5690, loss 0.548101.
Train: 2018-07-31T09:24:15.813376: step 5691, loss 0.579474.
Train: 2018-07-31T09:24:15.985211: step 5692, loss 0.545378.
Train: 2018-07-31T09:24:16.141418: step 5693, loss 0.519944.
Train: 2018-07-31T09:24:16.297608: step 5694, loss 0.587866.
Train: 2018-07-31T09:24:16.453851: step 5695, loss 0.613247.
Train: 2018-07-31T09:24:16.610035: step 5696, loss 0.520156.
Train: 2018-07-31T09:24:16.781869: step 5697, loss 0.562424.
Train: 2018-07-31T09:24:16.938113: step 5698, loss 0.545555.
Train: 2018-07-31T09:24:17.094297: step 5699, loss 0.520265.
Train: 2018-07-31T09:24:17.266161: step 5700, loss 0.613056.
Test: 2018-07-31T09:24:17.495875: step 5700, loss 0.548301.
Train: 2018-07-31T09:24:18.230062: step 5701, loss 0.528687.
Train: 2018-07-31T09:24:18.386255: step 5702, loss 0.528673.
Train: 2018-07-31T09:24:18.542468: step 5703, loss 0.621564.
Train: 2018-07-31T09:24:18.698707: step 5704, loss 0.604662.
Train: 2018-07-31T09:24:18.854896: step 5705, loss 0.528667.
Train: 2018-07-31T09:24:19.011139: step 5706, loss 0.570865.
Train: 2018-07-31T09:24:19.182977: step 5707, loss 0.553988.
Train: 2018-07-31T09:24:19.339156: step 5708, loss 0.604622.
Train: 2018-07-31T09:24:19.495400: step 5709, loss 0.587727.
Train: 2018-07-31T09:24:19.667235: step 5710, loss 0.537162.
Test: 2018-07-31T09:24:19.901557: step 5710, loss 0.548332.
Train: 2018-07-31T09:24:20.057738: step 5711, loss 0.579275.
Train: 2018-07-31T09:24:20.213983: step 5712, loss 0.562437.
Train: 2018-07-31T09:24:20.370195: step 5713, loss 0.604495.
Train: 2018-07-31T09:24:20.542000: step 5714, loss 0.680053.
Train: 2018-07-31T09:24:20.682592: step 5715, loss 0.570832.
Train: 2018-07-31T09:24:20.838842: step 5716, loss 0.495747.
Train: 2018-07-31T09:24:21.010641: step 5717, loss 0.629133.
Train: 2018-07-31T09:24:21.166887: step 5718, loss 0.520965.
Train: 2018-07-31T09:24:21.323097: step 5719, loss 0.479536.
Train: 2018-07-31T09:24:21.479311: step 5720, loss 0.687097.
Test: 2018-07-31T09:24:21.713628: step 5720, loss 0.548639.
Train: 2018-07-31T09:24:21.869815: step 5721, loss 0.554233.
Train: 2018-07-31T09:24:22.088544: step 5722, loss 0.562533.
Train: 2018-07-31T09:24:22.244728: step 5723, loss 0.612171.
Train: 2018-07-31T09:24:22.400939: step 5724, loss 0.562556.
Train: 2018-07-31T09:24:22.557186: step 5725, loss 0.546081.
Train: 2018-07-31T09:24:22.713397: step 5726, loss 0.5461.
Train: 2018-07-31T09:24:22.869610: step 5727, loss 0.562576.
Train: 2018-07-31T09:24:23.025823: step 5728, loss 0.612002.
Train: 2018-07-31T09:24:23.182008: step 5729, loss 0.587275.
Train: 2018-07-31T09:24:23.338251: step 5730, loss 0.570815.
Test: 2018-07-31T09:24:23.572572: step 5730, loss 0.548846.
Train: 2018-07-31T09:24:23.744375: step 5731, loss 0.570815.
Train: 2018-07-31T09:24:23.900620: step 5732, loss 0.513371.
Train: 2018-07-31T09:24:24.056802: step 5733, loss 0.554393.
Train: 2018-07-31T09:24:24.213015: step 5734, loss 0.546158.
Train: 2018-07-31T09:24:24.369228: step 5735, loss 0.570813.
Train: 2018-07-31T09:24:24.525473: step 5736, loss 0.521364.
Train: 2018-07-31T09:24:24.681680: step 5737, loss 0.587334.
Train: 2018-07-31T09:24:24.837899: step 5738, loss 0.50958.
Train: 2018-07-31T09:24:24.994083: step 5739, loss 0.529321.
Train: 2018-07-31T09:24:25.150326: step 5740, loss 0.537508.
Test: 2018-07-31T09:24:25.384616: step 5740, loss 0.548467.
Train: 2018-07-31T09:24:25.556452: step 5741, loss 0.570829.
Train: 2018-07-31T09:24:25.712689: step 5742, loss 0.57084.
Train: 2018-07-31T09:24:25.868878: step 5743, loss 0.537193.
Train: 2018-07-31T09:24:26.025092: step 5744, loss 0.545538.
Train: 2018-07-31T09:24:26.181329: step 5745, loss 0.596299.
Train: 2018-07-31T09:24:26.353139: step 5746, loss 0.579395.
Train: 2018-07-31T09:24:26.509377: step 5747, loss 0.511353.
Train: 2018-07-31T09:24:26.665591: step 5748, loss 0.605059.
Train: 2018-07-31T09:24:26.821813: step 5749, loss 0.545308.
Train: 2018-07-31T09:24:26.993614: step 5750, loss 0.588083.
Test: 2018-07-31T09:24:27.227960: step 5750, loss 0.548041.
Train: 2018-07-31T09:24:27.384147: step 5751, loss 0.476687.
Train: 2018-07-31T09:24:27.540361: step 5752, loss 0.536606.
Train: 2018-07-31T09:24:27.680986: step 5753, loss 0.536536.
Train: 2018-07-31T09:24:27.837198: step 5754, loss 0.510499.
Train: 2018-07-31T09:24:27.993381: step 5755, loss 0.56241.
Train: 2018-07-31T09:24:28.165215: step 5756, loss 0.553705.
Train: 2018-07-31T09:24:28.321458: step 5757, loss 0.588664.
Train: 2018-07-31T09:24:28.477672: step 5758, loss 0.606275.
Train: 2018-07-31T09:24:28.649477: step 5759, loss 0.571231.
Train: 2018-07-31T09:24:28.805724: step 5760, loss 0.623919.
Test: 2018-07-31T09:24:29.040035: step 5760, loss 0.54776.
Train: 2018-07-31T09:24:29.289952: step 5761, loss 0.50981.
Train: 2018-07-31T09:24:29.446165: step 5762, loss 0.615122.
Train: 2018-07-31T09:24:29.602379: step 5763, loss 0.553681.
Train: 2018-07-31T09:24:29.774214: step 5764, loss 0.597493.
Train: 2018-07-31T09:24:29.930452: step 5765, loss 0.64992.
Train: 2018-07-31T09:24:30.086640: step 5766, loss 0.553708.
Train: 2018-07-31T09:24:30.242853: step 5767, loss 0.466777.
Train: 2018-07-31T09:24:30.399091: step 5768, loss 0.545036.
Train: 2018-07-31T09:24:30.555313: step 5769, loss 0.545038.
Train: 2018-07-31T09:24:30.711525: step 5770, loss 0.545042.
Test: 2018-07-31T09:24:30.945844: step 5770, loss 0.54786.
Train: 2018-07-31T09:24:31.102052: step 5771, loss 0.562417.
Train: 2018-07-31T09:24:31.258241: step 5772, loss 0.640651.
Train: 2018-07-31T09:24:31.430075: step 5773, loss 0.53637.
Train: 2018-07-31T09:24:31.570668: step 5774, loss 0.571081.
Train: 2018-07-31T09:24:31.726911: step 5775, loss 0.545079.
Train: 2018-07-31T09:24:31.898746: step 5776, loss 0.588382.
Train: 2018-07-31T09:24:32.054962: step 5777, loss 0.501853.
Train: 2018-07-31T09:24:32.211172: step 5778, loss 0.597012.
Train: 2018-07-31T09:24:32.367357: step 5779, loss 0.57105.
Train: 2018-07-31T09:24:32.539190: step 5780, loss 0.588324.
Test: 2018-07-31T09:24:32.773511: step 5780, loss 0.547946.
Train: 2018-07-31T09:24:32.929724: step 5781, loss 0.562398.
Train: 2018-07-31T09:24:33.085968: step 5782, loss 0.622744.
Train: 2018-07-31T09:24:33.242179: step 5783, loss 0.5968.
Train: 2018-07-31T09:24:33.413986: step 5784, loss 0.519513.
Train: 2018-07-31T09:24:33.554611: step 5785, loss 0.519583.
Train: 2018-07-31T09:24:33.710821: step 5786, loss 0.562395.
Train: 2018-07-31T09:24:33.867036: step 5787, loss 0.588056.
Train: 2018-07-31T09:24:34.023219: step 5788, loss 0.528215.
Train: 2018-07-31T09:24:34.179464: step 5789, loss 0.570943.
Train: 2018-07-31T09:24:34.335675: step 5790, loss 0.588026.
Test: 2018-07-31T09:24:34.569965: step 5790, loss 0.548104.
Train: 2018-07-31T09:24:34.741800: step 5791, loss 0.570932.
Train: 2018-07-31T09:24:34.898044: step 5792, loss 0.5624.
Train: 2018-07-31T09:24:35.054265: step 5793, loss 0.54536.
Train: 2018-07-31T09:24:35.210470: step 5794, loss 0.536845.
Train: 2018-07-31T09:24:35.366684: step 5795, loss 0.587963.
Train: 2018-07-31T09:24:35.522868: step 5796, loss 0.519797.
Train: 2018-07-31T09:24:35.679081: step 5797, loss 0.502714.
Train: 2018-07-31T09:24:35.835293: step 5798, loss 0.519677.
Train: 2018-07-31T09:24:35.991537: step 5799, loss 0.588102.
Train: 2018-07-31T09:24:36.147721: step 5800, loss 0.48514.
Test: 2018-07-31T09:24:36.382048: step 5800, loss 0.547972.
Train: 2018-07-31T09:24:37.100623: step 5801, loss 0.519323.
Train: 2018-07-31T09:24:37.256866: step 5802, loss 0.519167.
Train: 2018-07-31T09:24:37.413080: step 5803, loss 0.605837.
Train: 2018-07-31T09:24:37.569293: step 5804, loss 0.57986.
Train: 2018-07-31T09:24:37.725476: step 5805, loss 0.553708.
Train: 2018-07-31T09:24:37.897312: step 5806, loss 0.518687.
Train: 2018-07-31T09:24:38.069147: step 5807, loss 0.615077.
Train: 2018-07-31T09:24:38.225359: step 5808, loss 0.500974.
Train: 2018-07-31T09:24:38.381597: step 5809, loss 0.615296.
Train: 2018-07-31T09:24:38.537787: step 5810, loss 0.527234.
Test: 2018-07-31T09:24:38.772131: step 5810, loss 0.547711.
Train: 2018-07-31T09:24:38.928353: step 5811, loss 0.553661.
Train: 2018-07-31T09:24:39.100154: step 5812, loss 0.571324.
Train: 2018-07-31T09:24:39.256368: step 5813, loss 0.527137.
Train: 2018-07-31T09:24:39.412581: step 5814, loss 0.482821.
Train: 2018-07-31T09:24:39.568819: step 5815, loss 0.562491.
Train: 2018-07-31T09:24:39.725009: step 5816, loss 0.589217.
Train: 2018-07-31T09:24:39.881222: step 5817, loss 0.553685.
Train: 2018-07-31T09:24:40.037436: step 5818, loss 0.500147.
Train: 2018-07-31T09:24:40.193648: step 5819, loss 0.589426.
Train: 2018-07-31T09:24:40.349862: step 5820, loss 0.598385.
Test: 2018-07-31T09:24:40.584213: step 5820, loss 0.547618.
Train: 2018-07-31T09:24:40.740396: step 5821, loss 0.580521.
Train: 2018-07-31T09:24:40.896639: step 5822, loss 0.553705.
Train: 2018-07-31T09:24:41.052855: step 5823, loss 0.580478.
Train: 2018-07-31T09:24:41.224657: step 5824, loss 0.5715.
Train: 2018-07-31T09:24:41.380870: step 5825, loss 0.562587.
Train: 2018-07-31T09:24:41.537117: step 5826, loss 0.535794.
Train: 2018-07-31T09:24:41.677714: step 5827, loss 0.553661.
Train: 2018-07-31T09:24:41.833889: step 5828, loss 0.571418.
Train: 2018-07-31T09:24:41.990133: step 5829, loss 0.51807.
Train: 2018-07-31T09:24:42.146349: step 5830, loss 0.624757.
Test: 2018-07-31T09:24:42.380663: step 5830, loss 0.547669.
Train: 2018-07-31T09:24:42.536849: step 5831, loss 0.535872.
Train: 2018-07-31T09:24:42.693094: step 5832, loss 0.615724.
Train: 2018-07-31T09:24:42.864922: step 5833, loss 0.518314.
Train: 2018-07-31T09:24:43.021141: step 5834, loss 0.509486.
Train: 2018-07-31T09:24:43.177357: step 5835, loss 0.615511.
Train: 2018-07-31T09:24:43.317949: step 5836, loss 0.589001.
Train: 2018-07-31T09:24:43.489782: step 5837, loss 0.685693.
Train: 2018-07-31T09:24:43.645966: step 5838, loss 0.466044.
Train: 2018-07-31T09:24:43.802178: step 5839, loss 0.536211.
Train: 2018-07-31T09:24:43.958393: step 5840, loss 0.562432.
Test: 2018-07-31T09:24:44.192742: step 5840, loss 0.547821.
Train: 2018-07-31T09:24:44.364547: step 5841, loss 0.527533.
Train: 2018-07-31T09:24:44.520760: step 5842, loss 0.544991.
Train: 2018-07-31T09:24:44.677006: step 5843, loss 0.518861.
Train: 2018-07-31T09:24:44.833223: step 5844, loss 0.553699.
Train: 2018-07-31T09:24:44.989424: step 5845, loss 0.536241.
Train: 2018-07-31T09:24:45.145614: step 5846, loss 0.536221.
Train: 2018-07-31T09:24:45.317486: step 5847, loss 0.623677.
Train: 2018-07-31T09:24:45.473692: step 5848, loss 0.62366.
Train: 2018-07-31T09:24:45.629905: step 5849, loss 0.501277.
Train: 2018-07-31T09:24:45.786118: step 5850, loss 0.553688.
Test: 2018-07-31T09:24:46.004818: step 5850, loss 0.547806.
Train: 2018-07-31T09:24:46.176648: step 5851, loss 0.544944.
Train: 2018-07-31T09:24:46.332872: step 5852, loss 0.527495.
Train: 2018-07-31T09:24:46.489048: step 5853, loss 0.588667.
Train: 2018-07-31T09:24:46.645263: step 5854, loss 0.606184.
Train: 2018-07-31T09:24:46.801509: step 5855, loss 0.58865.
Train: 2018-07-31T09:24:46.973311: step 5856, loss 0.614785.
Train: 2018-07-31T09:24:47.129555: step 5857, loss 0.579809.
Train: 2018-07-31T09:24:47.285767: step 5858, loss 0.562403.
Train: 2018-07-31T09:24:47.441951: step 5859, loss 0.562403.
Train: 2018-07-31T09:24:47.613786: step 5860, loss 0.571066.
Test: 2018-07-31T09:24:47.848137: step 5860, loss 0.547958.
Train: 2018-07-31T09:24:48.004319: step 5861, loss 0.510642.
Train: 2018-07-31T09:24:48.160566: step 5862, loss 0.622684.
Train: 2018-07-31T09:24:48.316771: step 5863, loss 0.639776.
Train: 2018-07-31T09:24:48.488581: step 5864, loss 0.5624.
Train: 2018-07-31T09:24:48.644825: step 5865, loss 0.588029.
Train: 2018-07-31T09:24:48.801041: step 5866, loss 0.579439.
Train: 2018-07-31T09:24:48.957257: step 5867, loss 0.621769.
Train: 2018-07-31T09:24:49.113466: step 5868, loss 0.562426.
Train: 2018-07-31T09:24:49.269678: step 5869, loss 0.528823.
Train: 2018-07-31T09:24:49.425861: step 5870, loss 0.545692.
Test: 2018-07-31T09:24:49.675831: step 5870, loss 0.548451.
Train: 2018-07-31T09:24:49.832017: step 5871, loss 0.520621.
Train: 2018-07-31T09:24:49.988255: step 5872, loss 0.579193.
Train: 2018-07-31T09:24:50.144443: step 5873, loss 0.579181.
Train: 2018-07-31T09:24:50.316309: step 5874, loss 0.554133.
Train: 2018-07-31T09:24:50.472492: step 5875, loss 0.562483.
Train: 2018-07-31T09:24:50.644356: step 5876, loss 0.545808.
Train: 2018-07-31T09:24:50.800572: step 5877, loss 0.587504.
Train: 2018-07-31T09:24:50.956789: step 5878, loss 0.595835.
Train: 2018-07-31T09:24:51.112996: step 5879, loss 0.620804.
Train: 2018-07-31T09:24:51.269210: step 5880, loss 0.620703.
Test: 2018-07-31T09:24:51.503500: step 5880, loss 0.548641.
Train: 2018-07-31T09:24:51.659713: step 5881, loss 0.570814.
Train: 2018-07-31T09:24:51.815957: step 5882, loss 0.51294.
Train: 2018-07-31T09:24:51.972170: step 5883, loss 0.595588.
Train: 2018-07-31T09:24:52.128384: step 5884, loss 0.504847.
Train: 2018-07-31T09:24:52.284591: step 5885, loss 0.546073.
Train: 2018-07-31T09:24:52.440811: step 5886, loss 0.562557.
Train: 2018-07-31T09:24:52.597024: step 5887, loss 0.521253.
Train: 2018-07-31T09:24:52.753237: step 5888, loss 0.579089.
Train: 2018-07-31T09:24:52.909453: step 5889, loss 0.580207.
Train: 2018-07-31T09:24:53.081287: step 5890, loss 0.504431.
Test: 2018-07-31T09:24:53.315575: step 5890, loss 0.548565.
Train: 2018-07-31T09:24:53.471819: step 5891, loss 0.579149.
Train: 2018-07-31T09:24:53.628032: step 5892, loss 0.512451.
Train: 2018-07-31T09:24:53.799838: step 5893, loss 0.503944.
Train: 2018-07-31T09:24:53.956051: step 5894, loss 0.621258.
Train: 2018-07-31T09:24:54.096672: step 5895, loss 0.621419.
Train: 2018-07-31T09:24:54.268508: step 5896, loss 0.613031.
Train: 2018-07-31T09:24:54.424715: step 5897, loss 0.579283.
Train: 2018-07-31T09:24:54.580940: step 5898, loss 0.562433.
Train: 2018-07-31T09:24:54.737146: step 5899, loss 0.587697.
Train: 2018-07-31T09:24:54.893361: step 5900, loss 0.570853.
Test: 2018-07-31T09:24:55.127651: step 5900, loss 0.548356.
Train: 2018-07-31T09:24:55.830611: step 5901, loss 0.596073.
Train: 2018-07-31T09:24:55.986855: step 5902, loss 0.528842.
Train: 2018-07-31T09:24:56.143039: step 5903, loss 0.596018.
Train: 2018-07-31T09:24:56.299285: step 5904, loss 0.461827.
Train: 2018-07-31T09:24:56.471117: step 5905, loss 0.545664.
Train: 2018-07-31T09:24:56.627336: step 5906, loss 0.612927.
Train: 2018-07-31T09:24:56.783546: step 5907, loss 0.579255.
Train: 2018-07-31T09:24:56.939727: step 5908, loss 0.53717.
Train: 2018-07-31T09:24:57.111586: step 5909, loss 0.528719.
Train: 2018-07-31T09:24:57.267775: step 5910, loss 0.604721.
Test: 2018-07-31T09:24:57.502126: step 5910, loss 0.548261.
Train: 2018-07-31T09:24:57.658309: step 5911, loss 0.52016.
Train: 2018-07-31T09:24:57.814521: step 5912, loss 0.562468.
Train: 2018-07-31T09:24:57.970735: step 5913, loss 0.528492.
Train: 2018-07-31T09:24:58.126979: step 5914, loss 0.56246.
Train: 2018-07-31T09:24:58.283162: step 5915, loss 0.536839.
Train: 2018-07-31T09:24:58.439375: step 5916, loss 0.519709.
Train: 2018-07-31T09:24:58.595591: step 5917, loss 0.562347.
Train: 2018-07-31T09:24:58.751802: step 5918, loss 0.54522.
Train: 2018-07-31T09:24:58.923637: step 5919, loss 0.527883.
Train: 2018-07-31T09:24:59.079881: step 5920, loss 0.579736.
Test: 2018-07-31T09:24:59.314172: step 5920, loss 0.547899.
Train: 2018-07-31T09:24:59.470383: step 5921, loss 0.5624.
Train: 2018-07-31T09:24:59.626628: step 5922, loss 0.51037.
Train: 2018-07-31T09:24:59.798471: step 5923, loss 0.553783.
Train: 2018-07-31T09:24:59.954645: step 5924, loss 0.571154.
Train: 2018-07-31T09:25:00.110890: step 5925, loss 0.544896.
Train: 2018-07-31T09:25:00.267102: step 5926, loss 0.544877.
Train: 2018-07-31T09:25:00.423286: step 5927, loss 0.562567.
Train: 2018-07-31T09:25:00.595120: step 5928, loss 0.536055.
Train: 2018-07-31T09:25:00.751365: step 5929, loss 0.562522.
Train: 2018-07-31T09:25:00.907577: step 5930, loss 0.518278.
Test: 2018-07-31T09:25:01.141868: step 5930, loss 0.547673.
Train: 2018-07-31T09:25:01.298114: step 5931, loss 0.553692.
Train: 2018-07-31T09:25:01.454325: step 5932, loss 0.562562.
Train: 2018-07-31T09:25:01.594887: step 5933, loss 0.571479.
Train: 2018-07-31T09:25:01.766722: step 5934, loss 0.571513.
Train: 2018-07-31T09:25:01.922968: step 5935, loss 0.580378.
Train: 2018-07-31T09:25:02.079179: step 5936, loss 0.526901.
Train: 2018-07-31T09:25:02.235392: step 5937, loss 0.616114.
Train: 2018-07-31T09:25:02.391605: step 5938, loss 0.526982.
Train: 2018-07-31T09:25:02.547822: step 5939, loss 0.660597.
Train: 2018-07-31T09:25:02.704003: step 5940, loss 0.562543.
Test: 2018-07-31T09:25:02.938324: step 5940, loss 0.54767.
Train: 2018-07-31T09:25:03.094535: step 5941, loss 0.527005.
Train: 2018-07-31T09:25:03.266400: step 5942, loss 0.527063.
Train: 2018-07-31T09:25:03.422584: step 5943, loss 0.580215.
Train: 2018-07-31T09:25:03.578798: step 5944, loss 0.535953.
Train: 2018-07-31T09:25:03.735036: step 5945, loss 0.518383.
Train: 2018-07-31T09:25:03.891243: step 5946, loss 0.509443.
Train: 2018-07-31T09:25:04.031849: step 5947, loss 0.58016.
Train: 2018-07-31T09:25:04.203676: step 5948, loss 0.562553.
Train: 2018-07-31T09:25:04.359863: step 5949, loss 0.615519.
Train: 2018-07-31T09:25:04.500457: step 5950, loss 0.5802.
Test: 2018-07-31T09:25:04.750398: step 5950, loss 0.547709.
Train: 2018-07-31T09:25:04.906612: step 5951, loss 0.509605.
Train: 2018-07-31T09:25:05.062855: step 5952, loss 0.535922.
Train: 2018-07-31T09:25:05.219069: step 5953, loss 0.509484.
Train: 2018-07-31T09:25:05.375282: step 5954, loss 0.553765.
Train: 2018-07-31T09:25:05.531465: step 5955, loss 0.562475.
Train: 2018-07-31T09:25:05.703330: step 5956, loss 0.624521.
Train: 2018-07-31T09:25:05.859514: step 5957, loss 0.598012.
Train: 2018-07-31T09:25:06.015727: step 5958, loss 0.597783.
Train: 2018-07-31T09:25:06.187560: step 5959, loss 0.500967.
Train: 2018-07-31T09:25:06.343808: step 5960, loss 0.615223.
Test: 2018-07-31T09:25:06.578120: step 5960, loss 0.54775.
Train: 2018-07-31T09:25:06.734309: step 5961, loss 0.597545.
Train: 2018-07-31T09:25:06.890552: step 5962, loss 0.606332.
Train: 2018-07-31T09:25:07.062387: step 5963, loss 0.527729.
Train: 2018-07-31T09:25:07.218600: step 5964, loss 0.54514.
Train: 2018-07-31T09:25:07.374783: step 5965, loss 0.47535.
Train: 2018-07-31T09:25:07.530996: step 5966, loss 0.614609.
Train: 2018-07-31T09:25:07.671622: step 5967, loss 0.527548.
Train: 2018-07-31T09:25:07.843456: step 5968, loss 0.631933.
Train: 2018-07-31T09:25:07.984015: step 5969, loss 0.553743.
Train: 2018-07-31T09:25:08.155851: step 5970, loss 0.501877.
Test: 2018-07-31T09:25:08.390170: step 5970, loss 0.547904.
Train: 2018-07-31T09:25:08.546417: step 5971, loss 0.562438.
Train: 2018-07-31T09:25:08.718218: step 5972, loss 0.527777.
Train: 2018-07-31T09:25:08.874462: step 5973, loss 0.553706.
Train: 2018-07-31T09:25:09.030646: step 5974, loss 0.536384.
Train: 2018-07-31T09:25:09.202480: step 5975, loss 0.579748.
Train: 2018-07-31T09:25:09.358724: step 5976, loss 0.614529.
Train: 2018-07-31T09:25:09.514908: step 5977, loss 0.510336.
Train: 2018-07-31T09:25:09.671153: step 5978, loss 0.614523.
Train: 2018-07-31T09:25:09.827364: step 5979, loss 0.597145.
Train: 2018-07-31T09:25:09.983548: step 5980, loss 0.519067.
Test: 2018-07-31T09:25:10.217899: step 5980, loss 0.547901.
Train: 2018-07-31T09:25:10.389701: step 5981, loss 0.571025.
Train: 2018-07-31T09:25:10.545940: step 5982, loss 0.623006.
Train: 2018-07-31T09:25:10.702129: step 5983, loss 0.545101.
Train: 2018-07-31T09:25:10.858343: step 5984, loss 0.605537.
Train: 2018-07-31T09:25:11.014556: step 5985, loss 0.562431.
Train: 2018-07-31T09:25:11.170802: step 5986, loss 0.571014.
Train: 2018-07-31T09:25:11.326982: step 5987, loss 0.588084.
Train: 2018-07-31T09:25:11.483195: step 5988, loss 0.630818.
Train: 2018-07-31T09:25:11.639440: step 5989, loss 0.579459.
Train: 2018-07-31T09:25:11.795656: step 5990, loss 0.621917.
Test: 2018-07-31T09:25:12.029968: step 5990, loss 0.548274.
Train: 2018-07-31T09:25:12.186181: step 5991, loss 0.562445.
Train: 2018-07-31T09:25:12.358027: step 5992, loss 0.562497.
Train: 2018-07-31T09:25:12.514204: step 5993, loss 0.545784.
Train: 2018-07-31T09:25:12.670418: step 5994, loss 0.579181.
Train: 2018-07-31T09:25:12.811041: step 5995, loss 0.529076.
Train: 2018-07-31T09:25:12.982875: step 5996, loss 0.529187.
Train: 2018-07-31T09:25:13.139089: step 5997, loss 0.595819.
Train: 2018-07-31T09:25:13.295305: step 5998, loss 0.595822.
Train: 2018-07-31T09:25:13.451515: step 5999, loss 0.579121.
Train: 2018-07-31T09:25:13.607730: step 6000, loss 0.587415.
Test: 2018-07-31T09:25:13.842050: step 6000, loss 0.548664.
Train: 2018-07-31T09:25:14.669983: step 6001, loss 0.637068.
Train: 2018-07-31T09:25:14.826188: step 6002, loss 0.562558.
Train: 2018-07-31T09:25:14.982407: step 6003, loss 0.587274.
Train: 2018-07-31T09:25:15.154245: step 6004, loss 0.447683.
Train: 2018-07-31T09:25:15.310424: step 6005, loss 0.546179.
Train: 2018-07-31T09:25:15.482290: step 6006, loss 0.6037.
Train: 2018-07-31T09:25:15.638497: step 6007, loss 0.455712.
Train: 2018-07-31T09:25:15.794687: step 6008, loss 0.562571.
Train: 2018-07-31T09:25:15.950935: step 6009, loss 0.529473.
Train: 2018-07-31T09:25:16.122765: step 6010, loss 0.504448.
Test: 2018-07-31T09:25:16.357085: step 6010, loss 0.548531.
Train: 2018-07-31T09:25:16.513298: step 6011, loss 0.554154.
Train: 2018-07-31T09:25:16.669515: step 6012, loss 0.570842.
Train: 2018-07-31T09:25:16.825695: step 6013, loss 0.629678.
Train: 2018-07-31T09:25:16.981908: step 6014, loss 0.520314.
Train: 2018-07-31T09:25:17.138147: step 6015, loss 0.570869.
Train: 2018-07-31T09:25:17.309956: step 6016, loss 0.579342.
Train: 2018-07-31T09:25:17.481816: step 6017, loss 0.579393.
Train: 2018-07-31T09:25:17.638034: step 6018, loss 0.570885.
Train: 2018-07-31T09:25:17.794257: step 6019, loss 0.53687.
Train: 2018-07-31T09:25:17.950433: step 6020, loss 0.613565.
Test: 2018-07-31T09:25:18.184777: step 6020, loss 0.548119.
Train: 2018-07-31T09:25:18.356611: step 6021, loss 0.536791.
Train: 2018-07-31T09:25:18.497179: step 6022, loss 0.55384.
Train: 2018-07-31T09:25:18.669013: step 6023, loss 0.511179.
Train: 2018-07-31T09:25:18.825227: step 6024, loss 0.570974.
Train: 2018-07-31T09:25:18.965843: step 6025, loss 0.630846.
Train: 2018-07-31T09:25:19.137678: step 6026, loss 0.588003.
Train: 2018-07-31T09:25:19.293900: step 6027, loss 0.519314.
Train: 2018-07-31T09:25:19.450111: step 6028, loss 0.596803.
Train: 2018-07-31T09:25:19.606294: step 6029, loss 0.562547.
Train: 2018-07-31T09:25:19.762507: step 6030, loss 0.563276.
Test: 2018-07-31T09:25:20.012450: step 6030, loss 0.547895.
Train: 2018-07-31T09:25:20.168695: step 6031, loss 0.606894.
Train: 2018-07-31T09:25:20.309285: step 6032, loss 0.527539.
Train: 2018-07-31T09:25:20.465469: step 6033, loss 0.579689.
Train: 2018-07-31T09:25:20.621712: step 6034, loss 0.545304.
Train: 2018-07-31T09:25:20.793516: step 6035, loss 0.553784.
Train: 2018-07-31T09:25:20.949729: step 6036, loss 0.553866.
Train: 2018-07-31T09:25:21.105968: step 6037, loss 0.553849.
Train: 2018-07-31T09:25:21.246568: step 6038, loss 0.562391.
Train: 2018-07-31T09:25:21.402748: step 6039, loss 0.545338.
Train: 2018-07-31T09:25:21.558992: step 6040, loss 0.544186.
Test: 2018-07-31T09:25:21.793307: step 6040, loss 0.54808.
Train: 2018-07-31T09:25:21.949495: step 6041, loss 0.605139.
Train: 2018-07-31T09:25:22.105709: step 6042, loss 0.528204.
Train: 2018-07-31T09:25:22.277544: step 6043, loss 0.502521.
Train: 2018-07-31T09:25:22.433787: step 6044, loss 0.553825.
Train: 2018-07-31T09:25:22.590000: step 6045, loss 0.553813.
Train: 2018-07-31T09:25:22.746184: step 6046, loss 0.527973.
Train: 2018-07-31T09:25:22.918043: step 6047, loss 0.579658.
Train: 2018-07-31T09:25:23.074264: step 6048, loss 0.527826.
Train: 2018-07-31T09:25:23.230446: step 6049, loss 0.571071.
Train: 2018-07-31T09:25:23.386690: step 6050, loss 0.510327.
Test: 2018-07-31T09:25:23.621003: step 6050, loss 0.547842.
Train: 2018-07-31T09:25:23.777193: step 6051, loss 0.562419.
Train: 2018-07-31T09:25:23.933436: step 6052, loss 0.597336.
Train: 2018-07-31T09:25:24.089619: step 6053, loss 0.597385.
Train: 2018-07-31T09:25:24.245858: step 6054, loss 0.509992.
Train: 2018-07-31T09:25:24.402076: step 6055, loss 0.588702.
Train: 2018-07-31T09:25:24.573880: step 6056, loss 0.579962.
Train: 2018-07-31T09:25:24.714499: step 6057, loss 0.483628.
Train: 2018-07-31T09:25:24.870717: step 6058, loss 0.527368.
Train: 2018-07-31T09:25:25.026929: step 6059, loss 0.527296.
Train: 2018-07-31T09:25:25.198735: step 6060, loss 0.544854.
Test: 2018-07-31T09:25:25.433085: step 6060, loss 0.547701.
Train: 2018-07-31T09:25:25.589298: step 6061, loss 0.544822.
Train: 2018-07-31T09:25:25.745482: step 6062, loss 0.589093.
Train: 2018-07-31T09:25:25.901695: step 6063, loss 0.562513.
Train: 2018-07-31T09:25:26.057942: step 6064, loss 0.544759.
Train: 2018-07-31T09:25:26.214152: step 6065, loss 0.598114.
Train: 2018-07-31T09:25:26.385986: step 6066, loss 0.615908.
Train: 2018-07-31T09:25:26.542194: step 6067, loss 0.544772.
Train: 2018-07-31T09:25:26.682792: step 6068, loss 0.553679.
Train: 2018-07-31T09:25:26.838977: step 6069, loss 0.5714.
Train: 2018-07-31T09:25:26.995188: step 6070, loss 0.571362.
Test: 2018-07-31T09:25:27.245129: step 6070, loss 0.547686.
Train: 2018-07-31T09:25:27.401373: step 6071, loss 0.615596.
Train: 2018-07-31T09:25:27.557557: step 6072, loss 0.597832.
Train: 2018-07-31T09:25:27.713800: step 6073, loss 0.606495.
Train: 2018-07-31T09:25:27.870013: step 6074, loss 0.579977.
Train: 2018-07-31T09:25:28.026230: step 6075, loss 0.588593.
Train: 2018-07-31T09:25:28.182411: step 6076, loss 0.527651.
Train: 2018-07-31T09:25:28.338655: step 6077, loss 0.553513.
Train: 2018-07-31T09:25:28.510489: step 6078, loss 0.510881.
Train: 2018-07-31T09:25:28.666672: step 6079, loss 0.588064.
Train: 2018-07-31T09:25:28.822885: step 6080, loss 0.519738.
Test: 2018-07-31T09:25:29.057207: step 6080, loss 0.54805.
Train: 2018-07-31T09:25:29.213449: step 6081, loss 0.528123.
Train: 2018-07-31T09:25:29.369632: step 6082, loss 0.528658.
Train: 2018-07-31T09:25:29.525847: step 6083, loss 0.579683.
Train: 2018-07-31T09:25:29.682059: step 6084, loss 0.519236.
Train: 2018-07-31T09:25:29.838272: step 6085, loss 0.588383.
Train: 2018-07-31T09:25:29.994518: step 6086, loss 0.510511.
Train: 2018-07-31T09:25:30.166346: step 6087, loss 0.545078.
Train: 2018-07-31T09:25:30.322559: step 6088, loss 0.527706.
Train: 2018-07-31T09:25:30.478778: step 6089, loss 0.56242.
Train: 2018-07-31T09:25:30.634999: step 6090, loss 0.597443.
Test: 2018-07-31T09:25:30.869315: step 6090, loss 0.547831.
Train: 2018-07-31T09:25:31.041150: step 6091, loss 0.571017.
Train: 2018-07-31T09:25:31.197354: step 6092, loss 0.588546.
Train: 2018-07-31T09:25:31.353573: step 6093, loss 0.536368.
Train: 2018-07-31T09:25:31.509787: step 6094, loss 0.597117.
Train: 2018-07-31T09:25:31.681616: step 6095, loss 0.58835.
Train: 2018-07-31T09:25:31.822213: step 6096, loss 0.571108.
Train: 2018-07-31T09:25:31.994048: step 6097, loss 0.536163.
Train: 2018-07-31T09:25:32.150264: step 6098, loss 0.553954.
Train: 2018-07-31T09:25:32.306495: step 6099, loss 0.519272.
Train: 2018-07-31T09:25:32.447067: step 6100, loss 0.475683.
Test: 2018-07-31T09:25:32.696978: step 6100, loss 0.547903.
Train: 2018-07-31T09:25:33.415593: step 6101, loss 0.588585.
Train: 2018-07-31T09:25:33.571803: step 6102, loss 0.605579.
Train: 2018-07-31T09:25:33.727987: step 6103, loss 0.622829.
Train: 2018-07-31T09:25:33.884231: step 6104, loss 0.493399.
Train: 2018-07-31T09:25:34.040414: step 6105, loss 0.622853.
Train: 2018-07-31T09:25:34.196627: step 6106, loss 0.579465.
Train: 2018-07-31T09:25:34.352870: step 6107, loss 0.605478.
Train: 2018-07-31T09:25:34.524702: step 6108, loss 0.520142.
Train: 2018-07-31T09:25:34.680888: step 6109, loss 0.484921.
Train: 2018-07-31T09:25:34.837102: step 6110, loss 0.621563.
Test: 2018-07-31T09:25:35.071454: step 6110, loss 0.548028.
Train: 2018-07-31T09:25:35.227666: step 6111, loss 0.519279.
Train: 2018-07-31T09:25:35.383850: step 6112, loss 0.510466.
Train: 2018-07-31T09:25:35.540065: step 6113, loss 0.5711.
Train: 2018-07-31T09:25:35.696275: step 6114, loss 0.638928.
Train: 2018-07-31T09:25:35.852520: step 6115, loss 0.579169.
Train: 2018-07-31T09:25:36.008733: step 6116, loss 0.5706.
Train: 2018-07-31T09:25:36.164949: step 6117, loss 0.554657.
Train: 2018-07-31T09:25:36.321160: step 6118, loss 0.511777.
Train: 2018-07-31T09:25:36.477373: step 6119, loss 0.639953.
Train: 2018-07-31T09:25:36.633587: step 6120, loss 0.552924.
Test: 2018-07-31T09:25:36.867878: step 6120, loss 0.54817.
Train: 2018-07-31T09:25:37.024115: step 6121, loss 0.552556.
Train: 2018-07-31T09:25:37.180334: step 6122, loss 0.604532.
Train: 2018-07-31T09:25:37.336517: step 6123, loss 0.545034.
Train: 2018-07-31T09:25:37.492731: step 6124, loss 0.590081.
Train: 2018-07-31T09:25:37.648943: step 6125, loss 0.578213.
Train: 2018-07-31T09:25:37.820809: step 6126, loss 0.552787.
Train: 2018-07-31T09:25:37.976993: step 6127, loss 0.544339.
Train: 2018-07-31T09:25:38.133236: step 6128, loss 0.580468.
Train: 2018-07-31T09:25:38.289452: step 6129, loss 0.605512.
Train: 2018-07-31T09:25:38.445663: step 6130, loss 0.597529.
Test: 2018-07-31T09:25:38.679984: step 6130, loss 0.548592.
Train: 2018-07-31T09:25:38.836165: step 6131, loss 0.594865.
Train: 2018-07-31T09:25:39.008031: step 6132, loss 0.578344.
Train: 2018-07-31T09:25:39.164247: step 6133, loss 0.554367.
Train: 2018-07-31T09:25:39.320427: step 6134, loss 0.547886.
Train: 2018-07-31T09:25:39.476640: step 6135, loss 0.529133.
Train: 2018-07-31T09:25:39.632854: step 6136, loss 0.562741.
Train: 2018-07-31T09:25:39.789095: step 6137, loss 0.583895.
Train: 2018-07-31T09:25:39.945282: step 6138, loss 0.582503.
Train: 2018-07-31T09:25:40.101519: step 6139, loss 0.643316.
Train: 2018-07-31T09:25:40.257708: step 6140, loss 0.595755.
Test: 2018-07-31T09:25:40.492028: step 6140, loss 0.548917.
Train: 2018-07-31T09:25:40.648242: step 6141, loss 0.555371.
Train: 2018-07-31T09:25:40.804485: step 6142, loss 0.562084.
Train: 2018-07-31T09:25:40.960698: step 6143, loss 0.564739.
Train: 2018-07-31T09:25:41.116912: step 6144, loss 0.587927.
Train: 2018-07-31T09:25:41.273094: step 6145, loss 0.569617.
Train: 2018-07-31T09:25:41.444960: step 6146, loss 0.553044.
Train: 2018-07-31T09:25:41.601173: step 6147, loss 0.528348.
Train: 2018-07-31T09:25:41.757357: step 6148, loss 0.537095.
Train: 2018-07-31T09:25:41.913603: step 6149, loss 0.571511.
Train: 2018-07-31T09:25:42.069814: step 6150, loss 0.626908.
Test: 2018-07-31T09:25:42.304135: step 6150, loss 0.548883.
Train: 2018-07-31T09:25:42.460347: step 6151, loss 0.497929.
Train: 2018-07-31T09:25:42.616531: step 6152, loss 0.512906.
Train: 2018-07-31T09:25:42.772743: step 6153, loss 0.586132.
Train: 2018-07-31T09:25:42.928988: step 6154, loss 0.527871.
Train: 2018-07-31T09:25:43.085201: step 6155, loss 0.561049.
Train: 2018-07-31T09:25:43.225793: step 6156, loss 0.581158.
Train: 2018-07-31T09:25:43.382007: step 6157, loss 0.528848.
Train: 2018-07-31T09:25:43.538222: step 6158, loss 0.580599.
Train: 2018-07-31T09:25:43.694402: step 6159, loss 0.585733.
Train: 2018-07-31T09:25:43.850646: step 6160, loss 0.56191.
Test: 2018-07-31T09:25:44.084967: step 6160, loss 0.548218.
Train: 2018-07-31T09:25:44.256771: step 6161, loss 0.538541.
Train: 2018-07-31T09:25:44.412985: step 6162, loss 0.571616.
Train: 2018-07-31T09:25:44.569228: step 6163, loss 0.510591.
Train: 2018-07-31T09:25:44.725443: step 6164, loss 0.579921.
Train: 2018-07-31T09:25:44.881655: step 6165, loss 0.545041.
Train: 2018-07-31T09:25:45.053490: step 6166, loss 0.434234.
Train: 2018-07-31T09:25:45.209672: step 6167, loss 0.587096.
Train: 2018-07-31T09:25:45.365886: step 6168, loss 0.529193.
Train: 2018-07-31T09:25:45.522133: step 6169, loss 0.483373.
Train: 2018-07-31T09:25:45.678314: step 6170, loss 0.509267.
Test: 2018-07-31T09:25:45.928255: step 6170, loss 0.547641.
Train: 2018-07-31T09:25:46.084469: step 6171, loss 0.54633.
Train: 2018-07-31T09:25:46.240681: step 6172, loss 0.536207.
Train: 2018-07-31T09:25:46.396926: step 6173, loss 0.599935.
Train: 2018-07-31T09:25:46.568729: step 6174, loss 0.626598.
Train: 2018-07-31T09:25:46.724968: step 6175, loss 0.581115.
Train: 2018-07-31T09:25:46.881188: step 6176, loss 0.554502.
Train: 2018-07-31T09:25:47.053021: step 6177, loss 0.562567.
Train: 2018-07-31T09:25:47.224827: step 6178, loss 0.644089.
Train: 2018-07-31T09:25:47.381040: step 6179, loss 0.572141.
Train: 2018-07-31T09:25:47.537286: step 6180, loss 0.544424.
Test: 2018-07-31T09:25:47.787194: step 6180, loss 0.547597.
Train: 2018-07-31T09:25:47.943409: step 6181, loss 0.552705.
Train: 2018-07-31T09:25:48.099622: step 6182, loss 0.52674.
Train: 2018-07-31T09:25:48.255865: step 6183, loss 0.500121.
Train: 2018-07-31T09:25:48.412081: step 6184, loss 0.464212.
Train: 2018-07-31T09:25:48.568292: step 6185, loss 0.562326.
Train: 2018-07-31T09:25:48.724476: step 6186, loss 0.56227.
Train: 2018-07-31T09:25:48.896311: step 6187, loss 0.616699.
Train: 2018-07-31T09:25:49.036903: step 6188, loss 0.572576.
Train: 2018-07-31T09:25:49.193145: step 6189, loss 0.608265.
Train: 2018-07-31T09:25:49.349328: step 6190, loss 0.670838.
Test: 2018-07-31T09:25:49.583680: step 6190, loss 0.547615.
Train: 2018-07-31T09:25:49.739863: step 6191, loss 0.638688.
Train: 2018-07-31T09:25:49.896077: step 6192, loss 0.597897.
Train: 2018-07-31T09:25:50.052319: step 6193, loss 0.53566.
Train: 2018-07-31T09:25:50.208533: step 6194, loss 0.562669.
Train: 2018-07-31T09:25:50.380368: step 6195, loss 0.623045.
Train: 2018-07-31T09:25:50.536590: step 6196, loss 0.570307.
Train: 2018-07-31T09:25:50.692788: step 6197, loss 0.571284.
Train: 2018-07-31T09:25:50.849007: step 6198, loss 0.571202.
Train: 2018-07-31T09:25:51.020846: step 6199, loss 0.536612.
Train: 2018-07-31T09:25:51.177027: step 6200, loss 0.537319.
Test: 2018-07-31T09:25:51.411379: step 6200, loss 0.548381.
Train: 2018-07-31T09:25:52.208064: step 6201, loss 0.588034.
Train: 2018-07-31T09:25:52.364249: step 6202, loss 0.54586.
Train: 2018-07-31T09:25:52.520490: step 6203, loss 0.654369.
Train: 2018-07-31T09:25:52.692296: step 6204, loss 0.496253.
Train: 2018-07-31T09:25:52.848510: step 6205, loss 0.5125.
Train: 2018-07-31T09:25:53.004753: step 6206, loss 0.645851.
Train: 2018-07-31T09:25:53.160969: step 6207, loss 0.596544.
Train: 2018-07-31T09:25:53.317150: step 6208, loss 0.553818.
Train: 2018-07-31T09:25:53.473393: step 6209, loss 0.595598.
Train: 2018-07-31T09:25:53.629607: step 6210, loss 0.620472.
Test: 2018-07-31T09:25:53.863928: step 6210, loss 0.548965.
Train: 2018-07-31T09:25:54.020140: step 6211, loss 0.578079.
Train: 2018-07-31T09:25:54.176323: step 6212, loss 0.612409.
Train: 2018-07-31T09:25:54.332567: step 6213, loss 0.611196.
Train: 2018-07-31T09:25:54.488780: step 6214, loss 0.563282.
Train: 2018-07-31T09:25:54.644993: step 6215, loss 0.516594.
Train: 2018-07-31T09:25:54.801207: step 6216, loss 0.554872.
Train: 2018-07-31T09:25:54.957420: step 6217, loss 0.562944.
Train: 2018-07-31T09:25:55.113604: step 6218, loss 0.529395.
Train: 2018-07-31T09:25:55.269842: step 6219, loss 0.45917.
Train: 2018-07-31T09:25:55.426032: step 6220, loss 0.603459.
Test: 2018-07-31T09:25:55.660384: step 6220, loss 0.549205.
Train: 2018-07-31T09:25:55.816565: step 6221, loss 0.603693.
Train: 2018-07-31T09:25:55.972808: step 6222, loss 0.554683.
Train: 2018-07-31T09:25:56.129021: step 6223, loss 0.670074.
Train: 2018-07-31T09:25:56.285205: step 6224, loss 0.53061.
Train: 2018-07-31T09:25:56.441419: step 6225, loss 0.544925.
Train: 2018-07-31T09:25:56.597662: step 6226, loss 0.613112.
Train: 2018-07-31T09:25:56.753845: step 6227, loss 0.521891.
Train: 2018-07-31T09:25:56.925710: step 6228, loss 0.538071.
Train: 2018-07-31T09:25:57.081917: step 6229, loss 0.588663.
Train: 2018-07-31T09:25:57.238106: step 6230, loss 0.612624.
Test: 2018-07-31T09:25:57.488048: step 6230, loss 0.548699.
Train: 2018-07-31T09:25:57.644291: step 6231, loss 0.513004.
Train: 2018-07-31T09:25:57.800505: step 6232, loss 0.479752.
Train: 2018-07-31T09:25:57.956689: step 6233, loss 0.595866.
Train: 2018-07-31T09:25:58.112932: step 6234, loss 0.579234.
Train: 2018-07-31T09:25:58.269115: step 6235, loss 0.571034.
Train: 2018-07-31T09:25:58.425358: step 6236, loss 0.553424.
Train: 2018-07-31T09:25:58.597164: step 6237, loss 0.528589.
Train: 2018-07-31T09:25:58.753409: step 6238, loss 0.579419.
Train: 2018-07-31T09:25:58.909590: step 6239, loss 0.545811.
Train: 2018-07-31T09:25:59.065828: step 6240, loss 0.545397.
Test: 2018-07-31T09:25:59.315774: step 6240, loss 0.548032.
Train: 2018-07-31T09:25:59.471989: step 6241, loss 0.502403.
Train: 2018-07-31T09:25:59.628202: step 6242, loss 0.56243.
Train: 2018-07-31T09:25:59.784385: step 6243, loss 0.519147.
Train: 2018-07-31T09:25:59.925008: step 6244, loss 0.527838.
Train: 2018-07-31T09:26:00.096843: step 6245, loss 0.562475.
Train: 2018-07-31T09:26:00.253026: step 6246, loss 0.51859.
Train: 2018-07-31T09:26:00.409272: step 6247, loss 0.518508.
Train: 2018-07-31T09:26:00.565452: step 6248, loss 0.562995.
Train: 2018-07-31T09:26:00.721696: step 6249, loss 0.571432.
Train: 2018-07-31T09:26:00.877879: step 6250, loss 0.633649.
Test: 2018-07-31T09:26:01.112230: step 6250, loss 0.54762.
Train: 2018-07-31T09:26:01.284058: step 6251, loss 0.535389.
Train: 2018-07-31T09:26:01.440283: step 6252, loss 0.553878.
Train: 2018-07-31T09:26:01.596491: step 6253, loss 0.562038.
Train: 2018-07-31T09:26:01.752712: step 6254, loss 0.5083.
Train: 2018-07-31T09:26:01.893300: step 6255, loss 0.526797.
Train: 2018-07-31T09:26:02.065137: step 6256, loss 0.563097.
Train: 2018-07-31T09:26:02.221315: step 6257, loss 0.472089.
Train: 2018-07-31T09:26:02.377529: step 6258, loss 0.517541.
Train: 2018-07-31T09:26:02.533774: step 6259, loss 0.608231.
Train: 2018-07-31T09:26:02.689986: step 6260, loss 0.562152.
Test: 2018-07-31T09:26:02.924306: step 6260, loss 0.547587.
Train: 2018-07-31T09:26:03.096109: step 6261, loss 0.60914.
Train: 2018-07-31T09:26:03.252323: step 6262, loss 0.61856.
Train: 2018-07-31T09:26:03.408570: step 6263, loss 0.627281.
Train: 2018-07-31T09:26:03.580401: step 6264, loss 0.589849.
Train: 2018-07-31T09:26:03.736585: step 6265, loss 0.545037.
Train: 2018-07-31T09:26:03.892828: step 6266, loss 0.493803.
Train: 2018-07-31T09:26:04.049012: step 6267, loss 0.597768.
Train: 2018-07-31T09:26:04.205224: step 6268, loss 0.562095.
Train: 2018-07-31T09:26:04.361439: step 6269, loss 0.502872.
Train: 2018-07-31T09:26:04.517676: step 6270, loss 0.545128.
Test: 2018-07-31T09:26:04.751974: step 6270, loss 0.547917.
Train: 2018-07-31T09:26:04.908185: step 6271, loss 0.621558.
Train: 2018-07-31T09:26:05.064399: step 6272, loss 0.544251.
Train: 2018-07-31T09:26:05.220642: step 6273, loss 0.624307.
Train: 2018-07-31T09:26:05.376863: step 6274, loss 0.544545.
Train: 2018-07-31T09:26:05.533040: step 6275, loss 0.461062.
Train: 2018-07-31T09:26:05.689282: step 6276, loss 0.528287.
Train: 2018-07-31T09:26:05.845501: step 6277, loss 0.552176.
Train: 2018-07-31T09:26:06.001679: step 6278, loss 0.579636.
Train: 2018-07-31T09:26:06.173514: step 6279, loss 0.53599.
Train: 2018-07-31T09:26:06.329752: step 6280, loss 0.500787.
Test: 2018-07-31T09:26:06.564079: step 6280, loss 0.547624.
Train: 2018-07-31T09:26:06.720261: step 6281, loss 0.536143.
Train: 2018-07-31T09:26:06.876508: step 6282, loss 0.436482.
Train: 2018-07-31T09:26:07.032720: step 6283, loss 0.61706.
Train: 2018-07-31T09:26:07.204552: step 6284, loss 0.581566.
Train: 2018-07-31T09:26:07.360761: step 6285, loss 0.621891.
Train: 2018-07-31T09:26:07.516950: step 6286, loss 0.488307.
Train: 2018-07-31T09:26:07.688815: step 6287, loss 0.573707.
Train: 2018-07-31T09:26:07.829406: step 6288, loss 0.526757.
Train: 2018-07-31T09:26:07.985619: step 6289, loss 0.608298.
Train: 2018-07-31T09:26:08.157457: step 6290, loss 0.544049.
Test: 2018-07-31T09:26:08.391745: step 6290, loss 0.54758.
Train: 2018-07-31T09:26:08.547958: step 6291, loss 0.58095.
Train: 2018-07-31T09:26:08.704202: step 6292, loss 0.57142.
Train: 2018-07-31T09:26:08.860385: step 6293, loss 0.526807.
Train: 2018-07-31T09:26:09.016599: step 6294, loss 0.55309.
Train: 2018-07-31T09:26:09.188433: step 6295, loss 0.607147.
Train: 2018-07-31T09:26:09.344677: step 6296, loss 0.562421.
Train: 2018-07-31T09:26:09.500890: step 6297, loss 0.517578.
Train: 2018-07-31T09:26:09.657106: step 6298, loss 0.606679.
Train: 2018-07-31T09:26:09.828909: step 6299, loss 0.508517.
Train: 2018-07-31T09:26:09.969530: step 6300, loss 0.571627.
Test: 2018-07-31T09:26:10.203822: step 6300, loss 0.54767.
Train: 2018-07-31T09:26:10.906814: step 6301, loss 0.47534.
Train: 2018-07-31T09:26:11.047397: step 6302, loss 0.616349.
Train: 2018-07-31T09:26:11.203611: step 6303, loss 0.481951.
Train: 2018-07-31T09:26:11.359829: step 6304, loss 0.616166.
Train: 2018-07-31T09:26:11.516043: step 6305, loss 0.518543.
Train: 2018-07-31T09:26:11.687878: step 6306, loss 0.571913.
Train: 2018-07-31T09:26:11.844061: step 6307, loss 0.554613.
Train: 2018-07-31T09:26:12.000305: step 6308, loss 0.562963.
Train: 2018-07-31T09:26:12.156489: step 6309, loss 0.652987.
Train: 2018-07-31T09:26:12.328347: step 6310, loss 0.616342.
Test: 2018-07-31T09:26:12.562643: step 6310, loss 0.547681.
Train: 2018-07-31T09:26:12.718856: step 6311, loss 0.535353.
Train: 2018-07-31T09:26:12.875100: step 6312, loss 0.569763.
Train: 2018-07-31T09:26:13.031316: step 6313, loss 0.544635.
Train: 2018-07-31T09:26:13.187528: step 6314, loss 0.537082.
Train: 2018-07-31T09:26:13.343740: step 6315, loss 0.736168.
Train: 2018-07-31T09:26:13.499953: step 6316, loss 0.534855.
Train: 2018-07-31T09:26:13.656170: step 6317, loss 0.536526.
Train: 2018-07-31T09:26:13.812350: step 6318, loss 0.604157.
Train: 2018-07-31T09:26:13.968594: step 6319, loss 0.544601.
Train: 2018-07-31T09:26:14.124807: step 6320, loss 0.546176.
Test: 2018-07-31T09:26:14.374718: step 6320, loss 0.54862.
Train: 2018-07-31T09:26:14.530965: step 6321, loss 0.62547.
Train: 2018-07-31T09:26:14.687172: step 6322, loss 0.555573.
Train: 2018-07-31T09:26:14.843360: step 6323, loss 0.583341.
Train: 2018-07-31T09:26:14.999572: step 6324, loss 0.58699.
Train: 2018-07-31T09:26:15.155810: step 6325, loss 0.643048.
Train: 2018-07-31T09:26:15.312029: step 6326, loss 0.550665.
Train: 2018-07-31T09:26:15.468213: step 6327, loss 0.604436.
Train: 2018-07-31T09:26:15.640048: step 6328, loss 0.579775.
Train: 2018-07-31T09:26:15.780638: step 6329, loss 0.54019.
Train: 2018-07-31T09:26:15.936883: step 6330, loss 0.49849.
Test: 2018-07-31T09:26:16.171204: step 6330, loss 0.549515.
Train: 2018-07-31T09:26:16.327411: step 6331, loss 0.529967.
Train: 2018-07-31T09:26:16.499252: step 6332, loss 0.529277.
Train: 2018-07-31T09:26:16.655473: step 6333, loss 0.603375.
Train: 2018-07-31T09:26:16.811647: step 6334, loss 0.587577.
Train: 2018-07-31T09:26:16.967892: step 6335, loss 0.587166.
Train: 2018-07-31T09:26:17.139729: step 6336, loss 0.636059.
Train: 2018-07-31T09:26:17.295909: step 6337, loss 0.520678.
Train: 2018-07-31T09:26:17.467756: step 6338, loss 0.512713.
Train: 2018-07-31T09:26:17.623958: step 6339, loss 0.495149.
Train: 2018-07-31T09:26:17.780204: step 6340, loss 0.528928.
Test: 2018-07-31T09:26:18.014524: step 6340, loss 0.548322.
Train: 2018-07-31T09:26:18.170729: step 6341, loss 0.595655.
Train: 2018-07-31T09:26:18.326919: step 6342, loss 0.544973.
Train: 2018-07-31T09:26:18.498783: step 6343, loss 0.545251.
Train: 2018-07-31T09:26:18.654967: step 6344, loss 0.570811.
Train: 2018-07-31T09:26:18.811180: step 6345, loss 0.535892.
Train: 2018-07-31T09:26:18.967426: step 6346, loss 0.580297.
Train: 2018-07-31T09:26:19.123636: step 6347, loss 0.579158.
Train: 2018-07-31T09:26:19.295442: step 6348, loss 0.614105.
Train: 2018-07-31T09:26:19.451655: step 6349, loss 0.492803.
Train: 2018-07-31T09:26:19.607869: step 6350, loss 0.510772.
Test: 2018-07-31T09:26:19.842219: step 6350, loss 0.547782.
Train: 2018-07-31T09:26:20.014024: step 6351, loss 0.571182.
Train: 2018-07-31T09:26:20.170272: step 6352, loss 0.570686.
Train: 2018-07-31T09:26:20.326450: step 6353, loss 0.474378.
Train: 2018-07-31T09:26:20.482693: step 6354, loss 0.625233.
Train: 2018-07-31T09:26:20.638907: step 6355, loss 0.606386.
Train: 2018-07-31T09:26:20.795091: step 6356, loss 0.500577.
Train: 2018-07-31T09:26:20.951303: step 6357, loss 0.545478.
Train: 2018-07-31T09:26:21.107548: step 6358, loss 0.518917.
Train: 2018-07-31T09:26:21.279384: step 6359, loss 0.589278.
Train: 2018-07-31T09:26:21.435589: step 6360, loss 0.473674.
Test: 2018-07-31T09:26:21.654298: step 6360, loss 0.547632.
Train: 2018-07-31T09:26:21.810509: step 6361, loss 0.553175.
Train: 2018-07-31T09:26:21.966721: step 6362, loss 0.553929.
Train: 2018-07-31T09:26:22.185419: step 6363, loss 0.616746.
Train: 2018-07-31T09:26:22.341633: step 6364, loss 0.562756.
Train: 2018-07-31T09:26:22.497847: step 6365, loss 0.481694.
Train: 2018-07-31T09:26:22.654063: step 6366, loss 0.680945.
Train: 2018-07-31T09:26:22.810273: step 6367, loss 0.590745.
Train: 2018-07-31T09:26:22.966486: step 6368, loss 0.535756.
Train: 2018-07-31T09:26:23.122700: step 6369, loss 0.581194.
Train: 2018-07-31T09:26:23.278916: step 6370, loss 0.616122.
Test: 2018-07-31T09:26:23.513203: step 6370, loss 0.547686.
Train: 2018-07-31T09:26:23.669416: step 6371, loss 0.580164.
Train: 2018-07-31T09:26:23.825660: step 6372, loss 0.598033.
Train: 2018-07-31T09:26:23.981868: step 6373, loss 0.597358.
Train: 2018-07-31T09:26:24.138090: step 6374, loss 0.484261.
Train: 2018-07-31T09:26:24.294270: step 6375, loss 0.58014.
Train: 2018-07-31T09:26:24.450509: step 6376, loss 0.536387.
Train: 2018-07-31T09:26:24.606730: step 6377, loss 0.56244.
Train: 2018-07-31T09:26:24.762944: step 6378, loss 0.501914.
Train: 2018-07-31T09:26:24.919156: step 6379, loss 0.588457.
Train: 2018-07-31T09:26:25.075368: step 6380, loss 0.588224.
Test: 2018-07-31T09:26:25.325310: step 6380, loss 0.547949.
Train: 2018-07-31T09:26:25.481492: step 6381, loss 0.527624.
Train: 2018-07-31T09:26:25.637737: step 6382, loss 0.545056.
Train: 2018-07-31T09:26:25.793949: step 6383, loss 0.519279.
Train: 2018-07-31T09:26:25.950163: step 6384, loss 0.484838.
Train: 2018-07-31T09:26:26.106378: step 6385, loss 0.683918.
Train: 2018-07-31T09:26:26.278214: step 6386, loss 0.536487.
Train: 2018-07-31T09:26:26.418804: step 6387, loss 0.562849.
Train: 2018-07-31T09:26:26.590638: step 6388, loss 0.59682.
Train: 2018-07-31T09:26:26.746820: step 6389, loss 0.579531.
Train: 2018-07-31T09:26:26.903034: step 6390, loss 0.553847.
Test: 2018-07-31T09:26:27.121764: step 6390, loss 0.547932.
Train: 2018-07-31T09:26:27.277980: step 6391, loss 0.597211.
Train: 2018-07-31T09:26:27.434160: step 6392, loss 0.56238.
Train: 2018-07-31T09:26:27.590404: step 6393, loss 0.588183.
Train: 2018-07-31T09:26:27.746588: step 6394, loss 0.579721.
Train: 2018-07-31T09:26:27.918422: step 6395, loss 0.502165.
Train: 2018-07-31T09:26:28.074668: step 6396, loss 0.553468.
Train: 2018-07-31T09:26:28.230879: step 6397, loss 0.562227.
Train: 2018-07-31T09:26:28.387063: step 6398, loss 0.554129.
Train: 2018-07-31T09:26:28.558927: step 6399, loss 0.51966.
Train: 2018-07-31T09:26:28.715141: step 6400, loss 0.545392.
Test: 2018-07-31T09:26:28.965082: step 6400, loss 0.548051.
Train: 2018-07-31T09:26:29.730528: step 6401, loss 0.536625.
Train: 2018-07-31T09:26:29.902357: step 6402, loss 0.519603.
Train: 2018-07-31T09:26:30.042955: step 6403, loss 0.596882.
Train: 2018-07-31T09:26:30.214758: step 6404, loss 0.545411.
Train: 2018-07-31T09:26:30.371002: step 6405, loss 0.54537.
Train: 2018-07-31T09:26:30.527219: step 6406, loss 0.527669.
Train: 2018-07-31T09:26:30.683430: step 6407, loss 0.588574.
Train: 2018-07-31T09:26:30.839612: step 6408, loss 0.553699.
Train: 2018-07-31T09:26:30.995856: step 6409, loss 0.562586.
Train: 2018-07-31T09:26:31.152073: step 6410, loss 0.57144.
Test: 2018-07-31T09:26:31.386393: step 6410, loss 0.547807.
Train: 2018-07-31T09:26:31.589467: step 6411, loss 0.605814.
Train: 2018-07-31T09:26:31.745681: step 6412, loss 0.562374.
Train: 2018-07-31T09:26:31.901897: step 6413, loss 0.623351.
Train: 2018-07-31T09:26:32.073698: step 6414, loss 0.562526.
Train: 2018-07-31T09:26:32.229913: step 6415, loss 0.536284.
Train: 2018-07-31T09:26:32.386126: step 6416, loss 0.544967.
Train: 2018-07-31T09:26:32.542369: step 6417, loss 0.570819.
Train: 2018-07-31T09:26:32.714173: step 6418, loss 0.527835.
Train: 2018-07-31T09:26:32.870417: step 6419, loss 0.52763.
Train: 2018-07-31T09:26:33.026634: step 6420, loss 0.588801.
Test: 2018-07-31T09:26:33.260954: step 6420, loss 0.547868.
Train: 2018-07-31T09:26:33.417135: step 6421, loss 0.588532.
Train: 2018-07-31T09:26:33.573377: step 6422, loss 0.60559.
Train: 2018-07-31T09:26:33.745207: step 6423, loss 0.597217.
Train: 2018-07-31T09:26:33.901395: step 6424, loss 0.536645.
Train: 2018-07-31T09:26:34.073232: step 6425, loss 0.49347.
Train: 2018-07-31T09:26:34.229474: step 6426, loss 0.53631.
Train: 2018-07-31T09:26:34.385658: step 6427, loss 0.57083.
Train: 2018-07-31T09:26:34.541872: step 6428, loss 0.622827.
Train: 2018-07-31T09:26:34.698114: step 6429, loss 0.58808.
Train: 2018-07-31T09:26:34.869919: step 6430, loss 0.562908.
Test: 2018-07-31T09:26:35.104241: step 6430, loss 0.547973.
Train: 2018-07-31T09:26:35.260477: step 6431, loss 0.519609.
Train: 2018-07-31T09:26:35.416697: step 6432, loss 0.545104.
Train: 2018-07-31T09:26:35.572909: step 6433, loss 0.536445.
Train: 2018-07-31T09:26:35.729123: step 6434, loss 0.614262.
Train: 2018-07-31T09:26:35.885340: step 6435, loss 0.510706.
Train: 2018-07-31T09:26:36.041550: step 6436, loss 0.596662.
Train: 2018-07-31T09:26:36.197764: step 6437, loss 0.553843.
Train: 2018-07-31T09:26:36.353972: step 6438, loss 0.605485.
Train: 2018-07-31T09:26:36.510159: step 6439, loss 0.605442.
Train: 2018-07-31T09:26:36.666404: step 6440, loss 0.579636.
Test: 2018-07-31T09:26:36.900727: step 6440, loss 0.548061.
Train: 2018-07-31T09:26:37.056906: step 6441, loss 0.562535.
Train: 2018-07-31T09:26:37.213150: step 6442, loss 0.553897.
Train: 2018-07-31T09:26:37.369364: step 6443, loss 0.55364.
Train: 2018-07-31T09:26:37.525581: step 6444, loss 0.537067.
Train: 2018-07-31T09:26:37.681760: step 6445, loss 0.545374.
Train: 2018-07-31T09:26:37.837974: step 6446, loss 0.604962.
Train: 2018-07-31T09:26:37.978566: step 6447, loss 0.57088.
Train: 2018-07-31T09:26:38.150401: step 6448, loss 0.570966.
Train: 2018-07-31T09:26:38.291023: step 6449, loss 0.613172.
Train: 2018-07-31T09:26:38.447207: step 6450, loss 0.477609.
Test: 2018-07-31T09:26:38.681557: step 6450, loss 0.548236.
Train: 2018-07-31T09:26:38.837770: step 6451, loss 0.519973.
Train: 2018-07-31T09:26:38.993954: step 6452, loss 0.579198.
Train: 2018-07-31T09:26:39.165818: step 6453, loss 0.562539.
Train: 2018-07-31T09:26:39.322031: step 6454, loss 0.536881.
Train: 2018-07-31T09:26:39.478250: step 6455, loss 0.486192.
Train: 2018-07-31T09:26:39.634428: step 6456, loss 0.485767.
Train: 2018-07-31T09:26:39.790672: step 6457, loss 0.613804.
Train: 2018-07-31T09:26:39.946886: step 6458, loss 0.65704.
Train: 2018-07-31T09:26:40.103069: step 6459, loss 0.605742.
Train: 2018-07-31T09:26:40.259282: step 6460, loss 0.53647.
Test: 2018-07-31T09:26:40.493636: step 6460, loss 0.547984.
Train: 2018-07-31T09:26:40.649849: step 6461, loss 0.665828.
Train: 2018-07-31T09:26:40.821650: step 6462, loss 0.5109.
Train: 2018-07-31T09:26:40.962273: step 6463, loss 0.605286.
Train: 2018-07-31T09:26:41.118456: step 6464, loss 0.476538.
Train: 2018-07-31T09:26:41.290291: step 6465, loss 0.536673.
Train: 2018-07-31T09:26:41.446534: step 6466, loss 0.553793.
Train: 2018-07-31T09:26:41.587096: step 6467, loss 0.553871.
Train: 2018-07-31T09:26:41.743340: step 6468, loss 0.579397.
Train: 2018-07-31T09:26:41.899557: step 6469, loss 0.553948.
Train: 2018-07-31T09:26:42.055766: step 6470, loss 0.571181.
Test: 2018-07-31T09:26:42.305679: step 6470, loss 0.547957.
Train: 2018-07-31T09:26:42.461916: step 6471, loss 0.596781.
Train: 2018-07-31T09:26:42.618134: step 6472, loss 0.570911.
Train: 2018-07-31T09:26:42.774351: step 6473, loss 0.631258.
Train: 2018-07-31T09:26:42.930531: step 6474, loss 0.55378.
Train: 2018-07-31T09:26:43.086775: step 6475, loss 0.536636.
Train: 2018-07-31T09:26:43.242959: step 6476, loss 0.622393.
Train: 2018-07-31T09:26:43.414792: step 6477, loss 0.588155.
Train: 2018-07-31T09:26:43.571036: step 6478, loss 0.520105.
Train: 2018-07-31T09:26:43.727220: step 6479, loss 0.605043.
Train: 2018-07-31T09:26:43.883434: step 6480, loss 0.511422.
Test: 2018-07-31T09:26:44.117787: step 6480, loss 0.548206.
Train: 2018-07-31T09:26:44.273967: step 6481, loss 0.587812.
Train: 2018-07-31T09:26:44.430211: step 6482, loss 0.511639.
Train: 2018-07-31T09:26:44.602015: step 6483, loss 0.520029.
Train: 2018-07-31T09:26:44.758228: step 6484, loss 0.554043.
Train: 2018-07-31T09:26:44.914441: step 6485, loss 0.604875.
Train: 2018-07-31T09:26:45.070656: step 6486, loss 0.537072.
Train: 2018-07-31T09:26:45.242490: step 6487, loss 0.553783.
Train: 2018-07-31T09:26:45.398704: step 6488, loss 0.536732.
Train: 2018-07-31T09:26:45.554917: step 6489, loss 0.562493.
Train: 2018-07-31T09:26:45.726752: step 6490, loss 0.613523.
Test: 2018-07-31T09:26:45.945481: step 6490, loss 0.548099.
Train: 2018-07-31T09:26:46.117315: step 6491, loss 0.57108.
Train: 2018-07-31T09:26:46.273499: step 6492, loss 0.519769.
Train: 2018-07-31T09:26:46.429743: step 6493, loss 0.580682.
Train: 2018-07-31T09:26:46.585955: step 6494, loss 0.579645.
Train: 2018-07-31T09:26:46.742140: step 6495, loss 0.528197.
Train: 2018-07-31T09:26:46.898351: step 6496, loss 0.562472.
Train: 2018-07-31T09:26:47.054566: step 6497, loss 0.630986.
Train: 2018-07-31T09:26:47.210780: step 6498, loss 0.51956.
Train: 2018-07-31T09:26:47.367023: step 6499, loss 0.528182.
Train: 2018-07-31T09:26:47.523205: step 6500, loss 0.502371.
Test: 2018-07-31T09:26:47.757559: step 6500, loss 0.548022.
Train: 2018-07-31T09:26:48.491759: step 6501, loss 0.528039.
Train: 2018-07-31T09:26:48.647942: step 6502, loss 0.545301.
Train: 2018-07-31T09:26:48.804156: step 6503, loss 0.519264.
Train: 2018-07-31T09:26:48.960369: step 6504, loss 0.527699.
Train: 2018-07-31T09:26:49.116615: step 6505, loss 0.545012.
Train: 2018-07-31T09:26:49.272829: step 6506, loss 0.509926.
Train: 2018-07-31T09:26:49.413388: step 6507, loss 0.536193.
Train: 2018-07-31T09:26:49.569602: step 6508, loss 0.562593.
Train: 2018-07-31T09:26:49.725815: step 6509, loss 0.553923.
Train: 2018-07-31T09:26:49.882061: step 6510, loss 0.544826.
Test: 2018-07-31T09:26:50.116383: step 6510, loss 0.547645.
Train: 2018-07-31T09:26:50.288208: step 6511, loss 0.642547.
Train: 2018-07-31T09:26:50.444427: step 6512, loss 0.571452.
Train: 2018-07-31T09:26:50.600641: step 6513, loss 0.553681.
Train: 2018-07-31T09:26:50.756848: step 6514, loss 0.589481.
Train: 2018-07-31T09:26:50.913067: step 6515, loss 0.580372.
Train: 2018-07-31T09:26:51.053629: step 6516, loss 0.491247.
Train: 2018-07-31T09:26:51.209842: step 6517, loss 0.526712.
Train: 2018-07-31T09:26:51.366055: step 6518, loss 0.571575.
Train: 2018-07-31T09:26:51.522271: step 6519, loss 0.607325.
Train: 2018-07-31T09:26:51.678483: step 6520, loss 0.562595.
Test: 2018-07-31T09:26:51.912804: step 6520, loss 0.547637.
Train: 2018-07-31T09:26:52.069047: step 6521, loss 0.625108.
Train: 2018-07-31T09:26:52.225230: step 6522, loss 0.544703.
Train: 2018-07-31T09:26:52.381443: step 6523, loss 0.562475.
Train: 2018-07-31T09:26:52.553278: step 6524, loss 0.598104.
Train: 2018-07-31T09:26:52.709522: step 6525, loss 0.589004.
Train: 2018-07-31T09:26:52.865735: step 6526, loss 0.509589.
Train: 2018-07-31T09:26:53.021951: step 6527, loss 0.544777.
Train: 2018-07-31T09:26:53.178161: step 6528, loss 0.597589.
Train: 2018-07-31T09:26:53.334346: step 6529, loss 0.562581.
Train: 2018-07-31T09:26:53.490561: step 6530, loss 0.562558.
Test: 2018-07-31T09:26:53.724909: step 6530, loss 0.547798.
Train: 2018-07-31T09:26:53.881092: step 6531, loss 0.641131.
Train: 2018-07-31T09:26:54.037305: step 6532, loss 0.492741.
Train: 2018-07-31T09:26:54.193520: step 6533, loss 0.640588.
Train: 2018-07-31T09:26:54.349733: step 6534, loss 0.545161.
Train: 2018-07-31T09:26:54.505975: step 6535, loss 0.56239.
Train: 2018-07-31T09:26:54.662160: step 6536, loss 0.588239.
Train: 2018-07-31T09:26:54.818372: step 6537, loss 0.6226.
Train: 2018-07-31T09:26:54.974616: step 6538, loss 0.553819.
Train: 2018-07-31T09:26:55.130800: step 6539, loss 0.519637.
Train: 2018-07-31T09:26:55.287012: step 6540, loss 0.545319.
Test: 2018-07-31T09:26:55.521335: step 6540, loss 0.548148.
Train: 2018-07-31T09:26:55.693168: step 6541, loss 0.562426.
Train: 2018-07-31T09:26:55.833760: step 6542, loss 0.562434.
Train: 2018-07-31T09:26:56.005594: step 6543, loss 0.528383.
Train: 2018-07-31T09:26:56.161838: step 6544, loss 0.579405.
Train: 2018-07-31T09:26:56.318021: step 6545, loss 0.596442.
Train: 2018-07-31T09:26:56.458613: step 6546, loss 0.570924.
Train: 2018-07-31T09:26:56.614857: step 6547, loss 0.587772.
Train: 2018-07-31T09:26:56.771040: step 6548, loss 0.579299.
Train: 2018-07-31T09:26:56.927285: step 6549, loss 0.587833.
Train: 2018-07-31T09:26:57.083497: step 6550, loss 0.503417.
Test: 2018-07-31T09:26:57.317819: step 6550, loss 0.548328.
Train: 2018-07-31T09:26:57.489623: step 6551, loss 0.545503.
Train: 2018-07-31T09:26:57.645869: step 6552, loss 0.520318.
Train: 2018-07-31T09:26:57.802079: step 6553, loss 0.528683.
Train: 2018-07-31T09:26:57.958292: step 6554, loss 0.562441.
Train: 2018-07-31T09:26:58.114506: step 6555, loss 0.621658.
Train: 2018-07-31T09:26:58.286340: step 6556, loss 0.520039.
Train: 2018-07-31T09:26:58.442523: step 6557, loss 0.528474.
Train: 2018-07-31T09:26:58.598761: step 6558, loss 0.519923.
Train: 2018-07-31T09:26:58.754975: step 6559, loss 0.587972.
Train: 2018-07-31T09:26:58.911164: step 6560, loss 0.562368.
Test: 2018-07-31T09:26:59.145518: step 6560, loss 0.548065.
Train: 2018-07-31T09:26:59.317319: step 6561, loss 0.588023.
Train: 2018-07-31T09:26:59.473532: step 6562, loss 0.605254.
Train: 2018-07-31T09:26:59.629746: step 6563, loss 0.536755.
Train: 2018-07-31T09:26:59.785990: step 6564, loss 0.588082.
Train: 2018-07-31T09:26:59.942172: step 6565, loss 0.570915.
Train: 2018-07-31T09:27:00.098387: step 6566, loss 0.622444.
Train: 2018-07-31T09:27:00.254632: step 6567, loss 0.562364.
Train: 2018-07-31T09:27:00.410814: step 6568, loss 0.562352.
Train: 2018-07-31T09:27:00.567057: step 6569, loss 0.528241.
Train: 2018-07-31T09:27:00.723240: step 6570, loss 0.596403.
Test: 2018-07-31T09:27:00.957560: step 6570, loss 0.548175.
Train: 2018-07-31T09:27:01.113807: step 6571, loss 0.621891.
Train: 2018-07-31T09:27:01.285644: step 6572, loss 0.587779.
Train: 2018-07-31T09:27:01.441852: step 6573, loss 0.578609.
Train: 2018-07-31T09:27:01.598036: step 6574, loss 0.553675.
Train: 2018-07-31T09:27:01.754248: step 6575, loss 0.595769.
Train: 2018-07-31T09:27:01.926092: step 6576, loss 0.534846.
Train: 2018-07-31T09:27:02.082327: step 6577, loss 0.576987.
Train: 2018-07-31T09:27:02.238541: step 6578, loss 0.571642.
Train: 2018-07-31T09:27:02.394756: step 6579, loss 0.560942.
Train: 2018-07-31T09:27:02.550936: step 6580, loss 0.443468.
Test: 2018-07-31T09:27:02.785257: step 6580, loss 0.550239.
Train: 2018-07-31T09:27:02.941501: step 6581, loss 0.518335.
Train: 2018-07-31T09:27:03.097684: step 6582, loss 0.589144.
Train: 2018-07-31T09:27:03.253930: step 6583, loss 0.518094.
Train: 2018-07-31T09:27:03.410111: step 6584, loss 0.556042.
Train: 2018-07-31T09:27:03.566360: step 6585, loss 0.529919.
Train: 2018-07-31T09:27:03.722568: step 6586, loss 0.622573.
Train: 2018-07-31T09:27:03.878751: step 6587, loss 0.561699.
Train: 2018-07-31T09:27:04.034988: step 6588, loss 0.483759.
Train: 2018-07-31T09:27:04.191213: step 6589, loss 0.562473.
Train: 2018-07-31T09:27:04.347416: step 6590, loss 0.554607.
Test: 2018-07-31T09:27:04.581712: step 6590, loss 0.547605.
Train: 2018-07-31T09:27:04.737924: step 6591, loss 0.588598.
Train: 2018-07-31T09:27:04.894138: step 6592, loss 0.542866.
Train: 2018-07-31T09:27:05.050352: step 6593, loss 0.5907.
Train: 2018-07-31T09:27:05.206595: step 6594, loss 0.579217.
Train: 2018-07-31T09:27:05.362779: step 6595, loss 0.562491.
Train: 2018-07-31T09:27:05.519025: step 6596, loss 0.592076.
Train: 2018-07-31T09:27:05.675206: step 6597, loss 0.516887.
Train: 2018-07-31T09:27:05.831418: step 6598, loss 0.4632.
Train: 2018-07-31T09:27:05.987632: step 6599, loss 0.601815.
Train: 2018-07-31T09:27:06.143879: step 6600, loss 0.543545.
Test: 2018-07-31T09:27:06.378167: step 6600, loss 0.547585.
Train: 2018-07-31T09:27:07.096777: step 6601, loss 0.572903.
Train: 2018-07-31T09:27:07.268582: step 6602, loss 0.537372.
Train: 2018-07-31T09:27:07.424826: step 6603, loss 0.533729.
Train: 2018-07-31T09:27:07.581039: step 6604, loss 0.479419.
Train: 2018-07-31T09:27:07.737253: step 6605, loss 0.607768.
Train: 2018-07-31T09:27:07.893462: step 6606, loss 0.518589.
Train: 2018-07-31T09:27:08.065301: step 6607, loss 0.500459.
Train: 2018-07-31T09:27:08.221514: step 6608, loss 0.570524.
Train: 2018-07-31T09:27:08.377731: step 6609, loss 0.507267.
Train: 2018-07-31T09:27:08.549562: step 6610, loss 0.62828.
Test: 2018-07-31T09:27:08.783854: step 6610, loss 0.547585.
Train: 2018-07-31T09:27:08.940066: step 6611, loss 0.498708.
Train: 2018-07-31T09:27:09.096279: step 6612, loss 0.499201.
Train: 2018-07-31T09:27:09.268113: step 6613, loss 0.590494.
Train: 2018-07-31T09:27:09.424328: step 6614, loss 0.588579.
Train: 2018-07-31T09:27:09.564949: step 6615, loss 0.617159.
Train: 2018-07-31T09:27:09.736778: step 6616, loss 0.573999.
Train: 2018-07-31T09:27:09.893001: step 6617, loss 0.589354.
Train: 2018-07-31T09:27:10.049214: step 6618, loss 0.551262.
Train: 2018-07-31T09:27:10.205418: step 6619, loss 0.635704.
Train: 2018-07-31T09:27:10.377230: step 6620, loss 0.652349.
Test: 2018-07-31T09:27:10.611581: step 6620, loss 0.547654.
Train: 2018-07-31T09:27:10.767796: step 6621, loss 0.517763.
Train: 2018-07-31T09:27:10.924006: step 6622, loss 0.597582.
Train: 2018-07-31T09:27:11.080220: step 6623, loss 0.615487.
Train: 2018-07-31T09:27:11.236403: step 6624, loss 0.570269.
Train: 2018-07-31T09:27:11.392615: step 6625, loss 0.623453.
Train: 2018-07-31T09:27:11.548860: step 6626, loss 0.554481.
Train: 2018-07-31T09:27:11.705043: step 6627, loss 0.570812.
Train: 2018-07-31T09:27:11.861287: step 6628, loss 0.613221.
Train: 2018-07-31T09:27:12.017503: step 6629, loss 0.544913.
Train: 2018-07-31T09:27:12.173684: step 6630, loss 0.503832.
Test: 2018-07-31T09:27:12.423625: step 6630, loss 0.548557.
Train: 2018-07-31T09:27:12.579862: step 6631, loss 0.562544.
Train: 2018-07-31T09:27:12.736082: step 6632, loss 0.570335.
Train: 2018-07-31T09:27:12.907886: step 6633, loss 0.588274.
Train: 2018-07-31T09:27:13.064124: step 6634, loss 0.503142.
Train: 2018-07-31T09:27:13.220313: step 6635, loss 0.570329.
Train: 2018-07-31T09:27:13.376552: step 6636, loss 0.504824.
Train: 2018-07-31T09:27:13.532765: step 6637, loss 0.504442.
Train: 2018-07-31T09:27:13.688953: step 6638, loss 0.570478.
Train: 2018-07-31T09:27:13.845192: step 6639, loss 0.545838.
Train: 2018-07-31T09:27:14.001412: step 6640, loss 0.571169.
Test: 2018-07-31T09:27:14.235731: step 6640, loss 0.548543.
Train: 2018-07-31T09:27:14.391947: step 6641, loss 0.562664.
Train: 2018-07-31T09:27:14.563779: step 6642, loss 0.644109.
Train: 2018-07-31T09:27:14.719993: step 6643, loss 0.597154.
Train: 2018-07-31T09:27:14.860584: step 6644, loss 0.613072.
Train: 2018-07-31T09:27:15.016801: step 6645, loss 0.553609.
Train: 2018-07-31T09:27:15.188603: step 6646, loss 0.545773.
Train: 2018-07-31T09:27:15.344816: step 6647, loss 0.513334.
Train: 2018-07-31T09:27:15.501030: step 6648, loss 0.50498.
Train: 2018-07-31T09:27:15.657273: step 6649, loss 0.594747.
Train: 2018-07-31T09:27:15.813486: step 6650, loss 0.629673.
Test: 2018-07-31T09:27:16.047807: step 6650, loss 0.548576.
Train: 2018-07-31T09:27:16.204020: step 6651, loss 0.528821.
Train: 2018-07-31T09:27:16.360234: step 6652, loss 0.595077.
Train: 2018-07-31T09:27:16.516450: step 6653, loss 0.569168.
Train: 2018-07-31T09:27:16.672660: step 6654, loss 0.604077.
Train: 2018-07-31T09:27:16.828844: step 6655, loss 0.545403.
Train: 2018-07-31T09:27:16.985057: step 6656, loss 0.595412.
Train: 2018-07-31T09:27:17.141270: step 6657, loss 0.569801.
Train: 2018-07-31T09:27:17.313106: step 6658, loss 0.643719.
Train: 2018-07-31T09:27:17.469348: step 6659, loss 0.561837.
Train: 2018-07-31T09:27:17.641155: step 6660, loss 0.563641.
Test: 2018-07-31T09:27:17.871393: step 6660, loss 0.548928.
Train: 2018-07-31T09:27:18.027632: step 6661, loss 0.545546.
Train: 2018-07-31T09:27:18.199467: step 6662, loss 0.572111.
Train: 2018-07-31T09:27:18.355680: step 6663, loss 0.60266.
Train: 2018-07-31T09:27:18.511869: step 6664, loss 0.62808.
Train: 2018-07-31T09:27:18.668112: step 6665, loss 0.531389.
Train: 2018-07-31T09:27:18.824320: step 6666, loss 0.581912.
Train: 2018-07-31T09:27:18.996160: step 6667, loss 0.6068.
Train: 2018-07-31T09:27:19.152374: step 6668, loss 0.538542.
Train: 2018-07-31T09:27:19.308556: step 6669, loss 0.54551.
Train: 2018-07-31T09:27:19.480393: step 6670, loss 0.556185.
Test: 2018-07-31T09:27:19.714737: step 6670, loss 0.549161.
Train: 2018-07-31T09:27:19.870955: step 6671, loss 0.529774.
Train: 2018-07-31T09:27:20.027169: step 6672, loss 0.53658.
Train: 2018-07-31T09:27:20.183382: step 6673, loss 0.572517.
Train: 2018-07-31T09:27:20.339598: step 6674, loss 0.547782.
Train: 2018-07-31T09:27:20.511431: step 6675, loss 0.480363.
Train: 2018-07-31T09:27:20.667642: step 6676, loss 0.553467.
Train: 2018-07-31T09:27:20.823860: step 6677, loss 0.586419.
Train: 2018-07-31T09:27:20.964452: step 6678, loss 0.504251.
Train: 2018-07-31T09:27:21.120663: step 6679, loss 0.555086.
Train: 2018-07-31T09:27:21.292492: step 6680, loss 0.555496.
Test: 2018-07-31T09:27:21.526819: step 6680, loss 0.548154.
Train: 2018-07-31T09:27:21.683031: step 6681, loss 0.57977.
Train: 2018-07-31T09:27:21.839215: step 6682, loss 0.544931.
Train: 2018-07-31T09:27:21.995428: step 6683, loss 0.612076.
Train: 2018-07-31T09:27:22.151641: step 6684, loss 0.579534.
Train: 2018-07-31T09:27:22.323476: step 6685, loss 0.598069.
Train: 2018-07-31T09:27:22.479689: step 6686, loss 0.553652.
Train: 2018-07-31T09:27:22.635927: step 6687, loss 0.537206.
Train: 2018-07-31T09:27:22.792146: step 6688, loss 0.588943.
Train: 2018-07-31T09:27:22.948360: step 6689, loss 0.570557.
Train: 2018-07-31T09:27:23.104568: step 6690, loss 0.615765.
Test: 2018-07-31T09:27:23.338897: step 6690, loss 0.547919.
Train: 2018-07-31T09:27:23.495102: step 6691, loss 0.605211.
Train: 2018-07-31T09:27:23.651320: step 6692, loss 0.512243.
Train: 2018-07-31T09:27:23.807533: step 6693, loss 0.570946.
Train: 2018-07-31T09:27:23.963750: step 6694, loss 0.502404.
Train: 2018-07-31T09:27:24.119960: step 6695, loss 0.57901.
Train: 2018-07-31T09:27:24.276174: step 6696, loss 0.613915.
Train: 2018-07-31T09:27:24.432387: step 6697, loss 0.544384.
Train: 2018-07-31T09:27:24.588604: step 6698, loss 0.502679.
Train: 2018-07-31T09:27:24.744785: step 6699, loss 0.605997.
Train: 2018-07-31T09:27:24.901028: step 6700, loss 0.510886.
Test: 2018-07-31T09:27:25.135348: step 6700, loss 0.547936.
Train: 2018-07-31T09:27:25.853929: step 6701, loss 0.596578.
Train: 2018-07-31T09:27:26.010143: step 6702, loss 0.553451.
Train: 2018-07-31T09:27:26.166325: step 6703, loss 0.562304.
Train: 2018-07-31T09:27:26.322570: step 6704, loss 0.580228.
Train: 2018-07-31T09:27:26.478754: step 6705, loss 0.606686.
Train: 2018-07-31T09:27:26.650588: step 6706, loss 0.59706.
Train: 2018-07-31T09:27:26.806800: step 6707, loss 0.588078.
Train: 2018-07-31T09:27:26.963047: step 6708, loss 0.587776.
Train: 2018-07-31T09:27:27.119258: step 6709, loss 0.638865.
Train: 2018-07-31T09:27:27.275443: step 6710, loss 0.54569.
Test: 2018-07-31T09:27:27.509792: step 6710, loss 0.548203.
Train: 2018-07-31T09:27:27.666006: step 6711, loss 0.604945.
Train: 2018-07-31T09:27:27.822221: step 6712, loss 0.554414.
Train: 2018-07-31T09:27:27.978437: step 6713, loss 0.555363.
Train: 2018-07-31T09:27:28.134645: step 6714, loss 0.578809.
Train: 2018-07-31T09:27:28.290859: step 6715, loss 0.520597.
Train: 2018-07-31T09:27:28.462664: step 6716, loss 0.512445.
Train: 2018-07-31T09:27:28.618877: step 6717, loss 0.52102.
Train: 2018-07-31T09:27:28.775120: step 6718, loss 0.529212.
Train: 2018-07-31T09:27:28.931334: step 6719, loss 0.520244.
Train: 2018-07-31T09:27:29.087550: step 6720, loss 0.537155.
Test: 2018-07-31T09:27:29.321837: step 6720, loss 0.548184.
Train: 2018-07-31T09:27:29.478051: step 6721, loss 0.554176.
Train: 2018-07-31T09:27:29.649885: step 6722, loss 0.579672.
Train: 2018-07-31T09:27:29.806099: step 6723, loss 0.545118.
Train: 2018-07-31T09:27:29.962336: step 6724, loss 0.580169.
Train: 2018-07-31T09:27:30.118555: step 6725, loss 0.649714.
Train: 2018-07-31T09:27:30.274739: step 6726, loss 0.587581.
Train: 2018-07-31T09:27:30.430952: step 6727, loss 0.554192.
Train: 2018-07-31T09:27:30.587165: step 6728, loss 0.544434.
Train: 2018-07-31T09:27:30.743380: step 6729, loss 0.588452.
Train: 2018-07-31T09:27:30.884001: step 6730, loss 0.597393.
Test: 2018-07-31T09:27:31.133912: step 6730, loss 0.548088.
Train: 2018-07-31T09:27:31.290156: step 6731, loss 0.665431.
Train: 2018-07-31T09:27:31.446339: step 6732, loss 0.545566.
Train: 2018-07-31T09:27:31.602584: step 6733, loss 0.605138.
Train: 2018-07-31T09:27:31.758796: step 6734, loss 0.512144.
Train: 2018-07-31T09:27:31.915010: step 6735, loss 0.570885.
Train: 2018-07-31T09:27:32.086847: step 6736, loss 0.587633.
Train: 2018-07-31T09:27:32.227437: step 6737, loss 0.613487.
Train: 2018-07-31T09:27:32.399272: step 6738, loss 0.595979.
Train: 2018-07-31T09:27:32.555456: step 6739, loss 0.512782.
Train: 2018-07-31T09:27:32.711696: step 6740, loss 0.512504.
Test: 2018-07-31T09:27:32.945988: step 6740, loss 0.54856.
Train: 2018-07-31T09:27:33.117823: step 6741, loss 0.537066.
Train: 2018-07-31T09:27:33.274037: step 6742, loss 0.521005.
Train: 2018-07-31T09:27:33.430250: step 6743, loss 0.562106.
Train: 2018-07-31T09:27:33.570875: step 6744, loss 0.537363.
Train: 2018-07-31T09:27:33.727056: step 6745, loss 0.554037.
Train: 2018-07-31T09:27:33.883269: step 6746, loss 0.528751.
Train: 2018-07-31T09:27:34.055128: step 6747, loss 0.502843.
Train: 2018-07-31T09:27:34.211350: step 6748, loss 0.605134.
Train: 2018-07-31T09:27:34.367556: step 6749, loss 0.562788.
Train: 2018-07-31T09:27:34.523774: step 6750, loss 0.596291.
Test: 2018-07-31T09:27:34.758064: step 6750, loss 0.54781.
Train: 2018-07-31T09:27:34.929900: step 6751, loss 0.485071.
Train: 2018-07-31T09:27:35.086144: step 6752, loss 0.660454.
Train: 2018-07-31T09:27:35.242326: step 6753, loss 0.562303.
Train: 2018-07-31T09:27:35.398538: step 6754, loss 0.527173.
Train: 2018-07-31T09:27:35.554782: step 6755, loss 0.526964.
Train: 2018-07-31T09:27:35.710998: step 6756, loss 0.552301.
Train: 2018-07-31T09:27:35.867209: step 6757, loss 0.580144.
Train: 2018-07-31T09:27:36.023394: step 6758, loss 0.61621.
Train: 2018-07-31T09:27:36.179636: step 6759, loss 0.529708.
Train: 2018-07-31T09:27:36.335852: step 6760, loss 0.606321.
Test: 2018-07-31T09:27:36.585785: step 6760, loss 0.547852.
Train: 2018-07-31T09:27:36.741975: step 6761, loss 0.543851.
Train: 2018-07-31T09:27:36.898212: step 6762, loss 0.510146.
Train: 2018-07-31T09:27:37.054402: step 6763, loss 0.553693.
Train: 2018-07-31T09:27:37.210639: step 6764, loss 0.571151.
Train: 2018-07-31T09:27:37.366858: step 6765, loss 0.562026.
Train: 2018-07-31T09:27:37.538663: step 6766, loss 0.579656.
Train: 2018-07-31T09:27:37.694906: step 6767, loss 0.553985.
Train: 2018-07-31T09:27:37.866742: step 6768, loss 0.527807.
Train: 2018-07-31T09:27:38.022955: step 6769, loss 0.553632.
Train: 2018-07-31T09:27:38.179138: step 6770, loss 0.588369.
Test: 2018-07-31T09:27:38.413458: step 6770, loss 0.547883.
Train: 2018-07-31T09:27:38.585293: step 6771, loss 0.545609.
Train: 2018-07-31T09:27:38.741542: step 6772, loss 0.535577.
Train: 2018-07-31T09:27:38.897750: step 6773, loss 0.563812.
Train: 2018-07-31T09:27:39.053963: step 6774, loss 0.51846.
Train: 2018-07-31T09:27:39.210179: step 6775, loss 0.492267.
Train: 2018-07-31T09:27:39.366391: step 6776, loss 0.527706.
Train: 2018-07-31T09:27:39.538219: step 6777, loss 0.597561.
Train: 2018-07-31T09:27:39.678786: step 6778, loss 0.54429.
Train: 2018-07-31T09:27:39.850652: step 6779, loss 0.55381.
Train: 2018-07-31T09:27:40.006835: step 6780, loss 0.588754.
Test: 2018-07-31T09:27:40.241186: step 6780, loss 0.547626.
Train: 2018-07-31T09:27:40.397370: step 6781, loss 0.500875.
Train: 2018-07-31T09:27:40.553583: step 6782, loss 0.552427.
Train: 2018-07-31T09:27:40.709828: step 6783, loss 0.555844.
Train: 2018-07-31T09:27:40.866039: step 6784, loss 0.590845.
Train: 2018-07-31T09:27:41.022252: step 6785, loss 0.609119.
Train: 2018-07-31T09:27:41.178435: step 6786, loss 0.579572.
Train: 2018-07-31T09:27:41.334650: step 6787, loss 0.53716.
Train: 2018-07-31T09:27:41.490863: step 6788, loss 0.517628.
Train: 2018-07-31T09:27:41.647106: step 6789, loss 0.544468.
Train: 2018-07-31T09:27:41.803321: step 6790, loss 0.500769.
Test: 2018-07-31T09:27:42.053230: step 6790, loss 0.547725.
Train: 2018-07-31T09:27:42.209444: step 6791, loss 0.554811.
Train: 2018-07-31T09:27:42.365688: step 6792, loss 0.553704.
Train: 2018-07-31T09:27:42.521901: step 6793, loss 0.552397.
Train: 2018-07-31T09:27:42.678084: step 6794, loss 0.545096.
Train: 2018-07-31T09:27:42.834297: step 6795, loss 0.561796.
Train: 2018-07-31T09:27:42.990541: step 6796, loss 0.545125.
Train: 2018-07-31T09:27:43.146755: step 6797, loss 0.554869.
Train: 2018-07-31T09:27:43.302968: step 6798, loss 0.616467.
Train: 2018-07-31T09:27:43.490424: step 6799, loss 0.607231.
Train: 2018-07-31T09:27:43.646608: step 6800, loss 0.632874.
Test: 2018-07-31T09:27:43.880953: step 6800, loss 0.547733.
Train: 2018-07-31T09:27:44.615161: step 6801, loss 0.589989.
Train: 2018-07-31T09:27:44.771345: step 6802, loss 0.536482.
Train: 2018-07-31T09:27:44.927588: step 6803, loss 0.553327.
Train: 2018-07-31T09:27:45.083804: step 6804, loss 0.43311.
Train: 2018-07-31T09:27:45.255607: step 6805, loss 0.519504.
Train: 2018-07-31T09:27:45.396198: step 6806, loss 0.57051.
Train: 2018-07-31T09:27:45.552442: step 6807, loss 0.588612.
Train: 2018-07-31T09:27:45.708657: step 6808, loss 0.589152.
Train: 2018-07-31T09:27:45.864838: step 6809, loss 0.561458.
Train: 2018-07-31T09:27:46.036672: step 6810, loss 0.616594.
Test: 2018-07-31T09:27:46.270993: step 6810, loss 0.547758.
Train: 2018-07-31T09:27:46.427207: step 6811, loss 0.562095.
Train: 2018-07-31T09:27:46.583452: step 6812, loss 0.607676.
Train: 2018-07-31T09:27:46.739634: step 6813, loss 0.580864.
Train: 2018-07-31T09:27:46.895848: step 6814, loss 0.501565.
Train: 2018-07-31T09:27:47.052090: step 6815, loss 0.519072.
Train: 2018-07-31T09:27:47.208298: step 6816, loss 0.562448.
Train: 2018-07-31T09:27:47.380108: step 6817, loss 0.562106.
Train: 2018-07-31T09:27:47.520725: step 6818, loss 0.571712.
Train: 2018-07-31T09:27:47.676913: step 6819, loss 0.615496.
Train: 2018-07-31T09:27:47.833160: step 6820, loss 0.596542.
Test: 2018-07-31T09:27:48.067476: step 6820, loss 0.547891.
Train: 2018-07-31T09:27:48.239313: step 6821, loss 0.60527.
Train: 2018-07-31T09:27:48.395520: step 6822, loss 0.502377.
Train: 2018-07-31T09:27:48.551737: step 6823, loss 0.60563.
Train: 2018-07-31T09:27:48.707955: step 6824, loss 0.493737.
Train: 2018-07-31T09:27:48.879787: step 6825, loss 0.536709.
Train: 2018-07-31T09:27:49.036006: step 6826, loss 0.54566.
Train: 2018-07-31T09:27:49.192209: step 6827, loss 0.553553.
Train: 2018-07-31T09:27:49.348428: step 6828, loss 0.614189.
Train: 2018-07-31T09:27:49.504647: step 6829, loss 0.519603.
Train: 2018-07-31T09:27:49.660825: step 6830, loss 0.579506.
Test: 2018-07-31T09:27:49.895147: step 6830, loss 0.548022.
Train: 2018-07-31T09:27:50.067009: step 6831, loss 0.561556.
Train: 2018-07-31T09:27:50.223193: step 6832, loss 0.639765.
Train: 2018-07-31T09:27:50.379406: step 6833, loss 0.502214.
Train: 2018-07-31T09:27:50.535650: step 6834, loss 0.579402.
Train: 2018-07-31T09:27:50.691833: step 6835, loss 0.528061.
Train: 2018-07-31T09:27:50.848077: step 6836, loss 0.553611.
Train: 2018-07-31T09:27:51.004290: step 6837, loss 0.545893.
Train: 2018-07-31T09:27:51.160503: step 6838, loss 0.519881.
Train: 2018-07-31T09:27:51.316687: step 6839, loss 0.509939.
Train: 2018-07-31T09:27:51.472929: step 6840, loss 0.467565.
Test: 2018-07-31T09:27:51.707254: step 6840, loss 0.547862.
Train: 2018-07-31T09:27:51.863464: step 6841, loss 0.623122.
Train: 2018-07-31T09:27:52.035299: step 6842, loss 0.563181.
Train: 2018-07-31T09:27:52.191482: step 6843, loss 0.544592.
Train: 2018-07-31T09:27:52.363347: step 6844, loss 0.580217.
Train: 2018-07-31T09:27:52.519530: step 6845, loss 0.589226.
Train: 2018-07-31T09:27:52.675773: step 6846, loss 0.545025.
Train: 2018-07-31T09:27:52.831989: step 6847, loss 0.642678.
Train: 2018-07-31T09:27:52.988170: step 6848, loss 0.509347.
Train: 2018-07-31T09:27:53.144414: step 6849, loss 0.597649.
Train: 2018-07-31T09:27:53.300597: step 6850, loss 0.544921.
Test: 2018-07-31T09:27:53.534943: step 6850, loss 0.547783.
Train: 2018-07-31T09:27:53.691160: step 6851, loss 0.483239.
Train: 2018-07-31T09:27:53.847374: step 6852, loss 0.527411.
Train: 2018-07-31T09:27:54.003558: step 6853, loss 0.562903.
Train: 2018-07-31T09:27:54.175417: step 6854, loss 0.579958.
Train: 2018-07-31T09:27:54.331638: step 6855, loss 0.553872.
Train: 2018-07-31T09:27:54.487819: step 6856, loss 0.50922.
Train: 2018-07-31T09:27:54.644063: step 6857, loss 0.615655.
Train: 2018-07-31T09:27:54.800246: step 6858, loss 0.659742.
Train: 2018-07-31T09:27:54.956459: step 6859, loss 0.545449.
Train: 2018-07-31T09:27:55.112673: step 6860, loss 0.606224.
Test: 2018-07-31T09:27:55.346993: step 6860, loss 0.547772.
Train: 2018-07-31T09:27:55.518828: step 6861, loss 0.518447.
Train: 2018-07-31T09:27:55.659420: step 6862, loss 0.588609.
Train: 2018-07-31T09:27:55.815663: step 6863, loss 0.605965.
Train: 2018-07-31T09:27:55.987498: step 6864, loss 0.553943.
Train: 2018-07-31T09:27:56.143711: step 6865, loss 0.587945.
Train: 2018-07-31T09:27:56.299895: step 6866, loss 0.502022.
Train: 2018-07-31T09:27:56.456109: step 6867, loss 0.493831.
Train: 2018-07-31T09:27:56.596730: step 6868, loss 0.562692.
Train: 2018-07-31T09:27:56.752945: step 6869, loss 0.579522.
Train: 2018-07-31T09:27:56.909157: step 6870, loss 0.57122.
Test: 2018-07-31T09:27:57.143478: step 6870, loss 0.547964.
Train: 2018-07-31T09:27:57.315312: step 6871, loss 0.587791.
Train: 2018-07-31T09:27:57.455907: step 6872, loss 0.545108.
Train: 2018-07-31T09:27:57.627708: step 6873, loss 0.571153.
Train: 2018-07-31T09:27:57.783952: step 6874, loss 0.579693.
Train: 2018-07-31T09:27:57.940164: step 6875, loss 0.467985.
Train: 2018-07-31T09:27:58.096379: step 6876, loss 0.502021.
Train: 2018-07-31T09:27:58.252587: step 6877, loss 0.579419.
Train: 2018-07-31T09:27:58.408776: step 6878, loss 0.588252.
Train: 2018-07-31T09:27:58.565021: step 6879, loss 0.536407.
Train: 2018-07-31T09:27:58.721228: step 6880, loss 0.571154.
Test: 2018-07-31T09:27:58.955524: step 6880, loss 0.547864.
Train: 2018-07-31T09:27:59.111766: step 6881, loss 0.500931.
Train: 2018-07-31T09:27:59.283602: step 6882, loss 0.432146.
Train: 2018-07-31T09:27:59.439814: step 6883, loss 0.580352.
Train: 2018-07-31T09:27:59.596027: step 6884, loss 0.527019.
Train: 2018-07-31T09:27:59.752241: step 6885, loss 0.490421.
Train: 2018-07-31T09:27:59.892804: step 6886, loss 0.651965.
Train: 2018-07-31T09:28:00.064671: step 6887, loss 0.527591.
Train: 2018-07-31T09:28:00.220852: step 6888, loss 0.609045.
Train: 2018-07-31T09:28:00.377095: step 6889, loss 0.553833.
Train: 2018-07-31T09:28:00.533279: step 6890, loss 0.553537.
Test: 2018-07-31T09:28:00.767600: step 6890, loss 0.547598.
Train: 2018-07-31T09:28:00.939433: step 6891, loss 0.563427.
Train: 2018-07-31T09:28:01.095677: step 6892, loss 0.607298.
Train: 2018-07-31T09:28:01.251896: step 6893, loss 0.562975.
Train: 2018-07-31T09:28:01.408074: step 6894, loss 0.553547.
Train: 2018-07-31T09:28:01.579940: step 6895, loss 0.572134.
Train: 2018-07-31T09:28:01.736152: step 6896, loss 0.589285.
Train: 2018-07-31T09:28:01.876743: step 6897, loss 0.527064.
Train: 2018-07-31T09:28:02.032928: step 6898, loss 0.500309.
Train: 2018-07-31T09:28:02.204761: step 6899, loss 0.616231.
Train: 2018-07-31T09:28:02.360975: step 6900, loss 0.491496.
Test: 2018-07-31T09:28:02.595326: step 6900, loss 0.547675.
Train: 2018-07-31T09:28:03.376386: step 6901, loss 0.536294.
Train: 2018-07-31T09:28:03.548215: step 6902, loss 0.686925.
Train: 2018-07-31T09:28:03.704443: step 6903, loss 0.580143.
Train: 2018-07-31T09:28:03.876275: step 6904, loss 0.641491.
Train: 2018-07-31T09:28:04.032460: step 6905, loss 0.475144.
Train: 2018-07-31T09:28:04.188673: step 6906, loss 0.510414.
Train: 2018-07-31T09:28:04.344916: step 6907, loss 0.588917.
Train: 2018-07-31T09:28:04.501124: step 6908, loss 0.55357.
Train: 2018-07-31T09:28:04.657343: step 6909, loss 0.5711.
Train: 2018-07-31T09:28:04.829173: step 6910, loss 0.57068.
Test: 2018-07-31T09:28:05.063501: step 6910, loss 0.547971.
Train: 2018-07-31T09:28:05.219712: step 6911, loss 0.544232.
Train: 2018-07-31T09:28:05.391515: step 6912, loss 0.647962.
Train: 2018-07-31T09:28:05.547759: step 6913, loss 0.585197.
Train: 2018-07-31T09:28:05.703975: step 6914, loss 0.562671.
Train: 2018-07-31T09:28:05.860191: step 6915, loss 0.587885.
Train: 2018-07-31T09:28:06.016398: step 6916, loss 0.482036.
Train: 2018-07-31T09:28:06.172583: step 6917, loss 0.538967.
Train: 2018-07-31T09:28:06.328828: step 6918, loss 0.540433.
Train: 2018-07-31T09:28:06.485009: step 6919, loss 0.570536.
Train: 2018-07-31T09:28:06.641253: step 6920, loss 0.622041.
Test: 2018-07-31T09:28:06.891195: step 6920, loss 0.548277.
Train: 2018-07-31T09:28:07.047378: step 6921, loss 0.538167.
Train: 2018-07-31T09:28:07.203592: step 6922, loss 0.554264.
Train: 2018-07-31T09:28:07.359834: step 6923, loss 0.588794.
Train: 2018-07-31T09:28:07.531670: step 6924, loss 0.510412.
Train: 2018-07-31T09:28:07.687852: step 6925, loss 0.562238.
Train: 2018-07-31T09:28:07.844097: step 6926, loss 0.483983.
Train: 2018-07-31T09:28:08.000310: step 6927, loss 0.545121.
Train: 2018-07-31T09:28:08.172145: step 6928, loss 0.562553.
Train: 2018-07-31T09:28:08.328328: step 6929, loss 0.501206.
Train: 2018-07-31T09:28:08.484572: step 6930, loss 0.589141.
Test: 2018-07-31T09:28:08.718904: step 6930, loss 0.547711.
Train: 2018-07-31T09:28:08.890721: step 6931, loss 0.597634.
Train: 2018-07-31T09:28:09.046910: step 6932, loss 0.588743.
Train: 2018-07-31T09:28:09.203148: step 6933, loss 0.650954.
Train: 2018-07-31T09:28:09.374982: step 6934, loss 0.535701.
Train: 2018-07-31T09:28:09.531172: step 6935, loss 0.606961.
Train: 2018-07-31T09:28:09.703039: step 6936, loss 0.579957.
Train: 2018-07-31T09:28:09.859249: step 6937, loss 0.571274.
Train: 2018-07-31T09:28:10.015463: step 6938, loss 0.571336.
Train: 2018-07-31T09:28:10.171683: step 6939, loss 0.606143.
Train: 2018-07-31T09:28:10.327860: step 6940, loss 0.640427.
Test: 2018-07-31T09:28:10.577834: step 6940, loss 0.548012.
Train: 2018-07-31T09:28:10.734046: step 6941, loss 0.536411.
Train: 2018-07-31T09:28:10.890228: step 6942, loss 0.63072.
Train: 2018-07-31T09:28:11.046480: step 6943, loss 0.570996.
Train: 2018-07-31T09:28:11.218312: step 6944, loss 0.571052.
Train: 2018-07-31T09:28:11.374490: step 6945, loss 0.536847.
Train: 2018-07-31T09:28:11.515081: step 6946, loss 0.508034.
Train: 2018-07-31T09:28:11.686946: step 6947, loss 0.587786.
Train: 2018-07-31T09:28:11.843160: step 6948, loss 0.595936.
Train: 2018-07-31T09:28:11.999373: step 6949, loss 0.56175.
Train: 2018-07-31T09:28:12.155556: step 6950, loss 0.505603.
Test: 2018-07-31T09:28:12.389877: step 6950, loss 0.548784.
Train: 2018-07-31T09:28:12.546128: step 6951, loss 0.586915.
Train: 2018-07-31T09:28:12.702330: step 6952, loss 0.520833.
Train: 2018-07-31T09:28:12.858547: step 6953, loss 0.537943.
Train: 2018-07-31T09:28:13.014761: step 6954, loss 0.603875.
Train: 2018-07-31T09:28:13.170974: step 6955, loss 0.587295.
Train: 2018-07-31T09:28:13.327158: step 6956, loss 0.553234.
Train: 2018-07-31T09:28:13.483372: step 6957, loss 0.577563.
Train: 2018-07-31T09:28:13.639585: step 6958, loss 0.652009.
Train: 2018-07-31T09:28:13.795827: step 6959, loss 0.502455.
Train: 2018-07-31T09:28:13.952011: step 6960, loss 0.537438.
Test: 2018-07-31T09:28:14.186360: step 6960, loss 0.549168.
Train: 2018-07-31T09:28:14.342575: step 6961, loss 0.579261.
Train: 2018-07-31T09:28:14.498758: step 6962, loss 0.568151.
Train: 2018-07-31T09:28:14.670592: step 6963, loss 0.570092.
Train: 2018-07-31T09:28:14.811215: step 6964, loss 0.606366.
Train: 2018-07-31T09:28:14.983044: step 6965, loss 0.591686.
Train: 2018-07-31T09:28:15.139263: step 6966, loss 0.520253.
Train: 2018-07-31T09:28:15.295477: step 6967, loss 0.619992.
Train: 2018-07-31T09:28:15.451692: step 6968, loss 0.509046.
Train: 2018-07-31T09:28:15.607874: step 6969, loss 0.564083.
Train: 2018-07-31T09:28:15.764086: step 6970, loss 0.526576.
Test: 2018-07-31T09:28:15.998408: step 6970, loss 0.548984.
Train: 2018-07-31T09:28:16.170242: step 6971, loss 0.544359.
Train: 2018-07-31T09:28:16.326455: step 6972, loss 0.526789.
Train: 2018-07-31T09:28:16.482668: step 6973, loss 0.580373.
Train: 2018-07-31T09:28:16.638906: step 6974, loss 0.560939.
Train: 2018-07-31T09:28:16.810746: step 6975, loss 0.546008.
Train: 2018-07-31T09:28:16.966960: step 6976, loss 0.57835.
Train: 2018-07-31T09:28:17.123174: step 6977, loss 0.556619.
Train: 2018-07-31T09:28:17.279357: step 6978, loss 0.611867.
Train: 2018-07-31T09:28:17.451191: step 6979, loss 0.510929.
Train: 2018-07-31T09:28:17.607429: step 6980, loss 0.605206.
Test: 2018-07-31T09:28:17.841725: step 6980, loss 0.548528.
Train: 2018-07-31T09:28:17.982318: step 6981, loss 0.5639.
Train: 2018-07-31T09:28:18.138531: step 6982, loss 0.566113.
Train: 2018-07-31T09:28:18.294774: step 6983, loss 0.541853.
Train: 2018-07-31T09:28:18.450990: step 6984, loss 0.604316.
Train: 2018-07-31T09:28:18.607200: step 6985, loss 0.543976.
Train: 2018-07-31T09:28:18.779037: step 6986, loss 0.535447.
Train: 2018-07-31T09:28:18.935218: step 6987, loss 0.58157.
Train: 2018-07-31T09:28:19.091463: step 6988, loss 0.616568.
Train: 2018-07-31T09:28:19.247676: step 6989, loss 0.561778.
Train: 2018-07-31T09:28:19.403884: step 6990, loss 0.622375.
Test: 2018-07-31T09:28:19.653830: step 6990, loss 0.548087.
Train: 2018-07-31T09:28:19.810044: step 6991, loss 0.554744.
Train: 2018-07-31T09:28:19.950639: step 6992, loss 0.53645.
Train: 2018-07-31T09:28:20.122471: step 6993, loss 0.598517.
Train: 2018-07-31T09:28:20.278685: step 6994, loss 0.586433.
Train: 2018-07-31T09:28:20.434868: step 6995, loss 0.562129.
Train: 2018-07-31T09:28:20.606733: step 6996, loss 0.614229.
Train: 2018-07-31T09:28:20.762940: step 6997, loss 0.597304.
Train: 2018-07-31T09:28:20.919153: step 6998, loss 0.612194.
Train: 2018-07-31T09:28:21.075375: step 6999, loss 0.547127.
Train: 2018-07-31T09:28:21.231556: step 7000, loss 0.560815.
Test: 2018-07-31T09:28:21.465907: step 7000, loss 0.548775.
Train: 2018-07-31T09:28:22.153246: step 7001, loss 0.488952.
Train: 2018-07-31T09:28:22.340672: step 7002, loss 0.553257.
Train: 2018-07-31T09:28:22.496885: step 7003, loss 0.603638.
Train: 2018-07-31T09:28:22.653099: step 7004, loss 0.580955.
Train: 2018-07-31T09:28:22.809312: step 7005, loss 0.561446.
Train: 2018-07-31T09:28:22.965526: step 7006, loss 0.488757.
Train: 2018-07-31T09:28:23.137384: step 7007, loss 0.521113.
Train: 2018-07-31T09:28:23.277982: step 7008, loss 0.563032.
Train: 2018-07-31T09:28:23.449786: step 7009, loss 0.571568.
Train: 2018-07-31T09:28:23.606036: step 7010, loss 0.537725.
Test: 2018-07-31T09:28:23.840320: step 7010, loss 0.54827.
Train: 2018-07-31T09:28:23.996564: step 7011, loss 0.492248.
Train: 2018-07-31T09:28:24.152778: step 7012, loss 0.56186.
Train: 2018-07-31T09:28:24.308993: step 7013, loss 0.535541.
Train: 2018-07-31T09:28:24.465205: step 7014, loss 0.571439.
Train: 2018-07-31T09:28:24.637023: step 7015, loss 0.53504.
Train: 2018-07-31T09:28:24.793247: step 7016, loss 0.619172.
Train: 2018-07-31T09:28:24.949469: step 7017, loss 0.578741.
Train: 2018-07-31T09:28:25.121290: step 7018, loss 0.537474.
Train: 2018-07-31T09:28:25.261892: step 7019, loss 0.601434.
Train: 2018-07-31T09:28:25.433721: step 7020, loss 0.528084.
Test: 2018-07-31T09:28:25.668048: step 7020, loss 0.547783.
Train: 2018-07-31T09:28:25.824231: step 7021, loss 0.482275.
Train: 2018-07-31T09:28:25.980480: step 7022, loss 0.509273.
Train: 2018-07-31T09:28:26.136688: step 7023, loss 0.589766.
Train: 2018-07-31T09:28:26.308492: step 7024, loss 0.552294.
Train: 2018-07-31T09:28:26.464735: step 7025, loss 0.669949.
Train: 2018-07-31T09:28:26.620950: step 7026, loss 0.53466.
Train: 2018-07-31T09:28:26.777133: step 7027, loss 0.633122.
Train: 2018-07-31T09:28:26.933346: step 7028, loss 0.553613.
Train: 2018-07-31T09:28:27.105212: step 7029, loss 0.562047.
Train: 2018-07-31T09:28:27.261395: step 7030, loss 0.570567.
Test: 2018-07-31T09:28:27.480093: step 7030, loss 0.54816.
Train: 2018-07-31T09:28:27.636306: step 7031, loss 0.597501.
Train: 2018-07-31T09:28:27.792520: step 7032, loss 0.530546.
Train: 2018-07-31T09:28:27.948734: step 7033, loss 0.606178.
Train: 2018-07-31T09:28:28.104977: step 7034, loss 0.511445.
Train: 2018-07-31T09:28:28.261161: step 7035, loss 0.478783.
Train: 2018-07-31T09:28:28.433027: step 7036, loss 0.570411.
Train: 2018-07-31T09:28:28.589239: step 7037, loss 0.554376.
Train: 2018-07-31T09:28:28.745446: step 7038, loss 0.572411.
Train: 2018-07-31T09:28:28.917286: step 7039, loss 0.659562.
Train: 2018-07-31T09:28:29.073502: step 7040, loss 0.570641.
Test: 2018-07-31T09:28:29.307823: step 7040, loss 0.548164.
Train: 2018-07-31T09:28:29.464028: step 7041, loss 0.519204.
Train: 2018-07-31T09:28:29.620247: step 7042, loss 0.519896.
Train: 2018-07-31T09:28:29.776460: step 7043, loss 0.571795.
Train: 2018-07-31T09:28:29.932643: step 7044, loss 0.562904.
Train: 2018-07-31T09:28:30.088858: step 7045, loss 0.561248.
Train: 2018-07-31T09:28:30.245070: step 7046, loss 0.631703.
Train: 2018-07-31T09:28:30.401314: step 7047, loss 0.537476.
Train: 2018-07-31T09:28:30.557530: step 7048, loss 0.536598.
Train: 2018-07-31T09:28:30.713711: step 7049, loss 0.519808.
Train: 2018-07-31T09:28:30.869955: step 7050, loss 0.588356.
Test: 2018-07-31T09:28:31.119891: step 7050, loss 0.548137.
Train: 2018-07-31T09:28:31.276103: step 7051, loss 0.55325.
Train: 2018-07-31T09:28:31.416671: step 7052, loss 0.528715.
Train: 2018-07-31T09:28:31.572917: step 7053, loss 0.579799.
Train: 2018-07-31T09:28:31.744718: step 7054, loss 0.554229.
Train: 2018-07-31T09:28:31.900963: step 7055, loss 0.545736.
Train: 2018-07-31T09:28:32.057179: step 7056, loss 0.537315.
Train: 2018-07-31T09:28:32.213389: step 7057, loss 0.63215.
Train: 2018-07-31T09:28:32.369603: step 7058, loss 0.588638.
Train: 2018-07-31T09:28:32.525811: step 7059, loss 0.545207.
Train: 2018-07-31T09:28:32.682029: step 7060, loss 0.63101.
Test: 2018-07-31T09:28:32.916353: step 7060, loss 0.548174.
Train: 2018-07-31T09:28:33.244368: step 7061, loss 0.571574.
Train: 2018-07-31T09:28:33.416203: step 7062, loss 0.535309.
Train: 2018-07-31T09:28:33.572441: step 7063, loss 0.503114.
Train: 2018-07-31T09:28:33.728629: step 7064, loss 0.563763.
Train: 2018-07-31T09:28:33.884842: step 7065, loss 0.621768.
Train: 2018-07-31T09:28:34.041056: step 7066, loss 0.579942.
Train: 2018-07-31T09:28:34.197296: step 7067, loss 0.536608.
Train: 2018-07-31T09:28:34.353483: step 7068, loss 0.553195.
Train: 2018-07-31T09:28:34.509697: step 7069, loss 0.604303.
Train: 2018-07-31T09:28:34.665910: step 7070, loss 0.554457.
Test: 2018-07-31T09:28:34.900261: step 7070, loss 0.548369.
Train: 2018-07-31T09:28:35.056477: step 7071, loss 0.571137.
Train: 2018-07-31T09:28:35.212659: step 7072, loss 0.588622.
Train: 2018-07-31T09:28:35.368871: step 7073, loss 0.58883.
Train: 2018-07-31T09:28:35.525114: step 7074, loss 0.579212.
Train: 2018-07-31T09:28:35.681330: step 7075, loss 0.570123.
Train: 2018-07-31T09:28:35.837541: step 7076, loss 0.546404.
Train: 2018-07-31T09:28:35.993754: step 7077, loss 0.529482.
Train: 2018-07-31T09:28:36.149937: step 7078, loss 0.570896.
Train: 2018-07-31T09:28:36.306181: step 7079, loss 0.571177.
Train: 2018-07-31T09:28:36.477986: step 7080, loss 0.497871.
Test: 2018-07-31T09:28:36.712306: step 7080, loss 0.548572.
Train: 2018-07-31T09:28:36.884140: step 7081, loss 0.503808.
Train: 2018-07-31T09:28:37.040353: step 7082, loss 0.503732.
Train: 2018-07-31T09:28:37.196567: step 7083, loss 0.578854.
Train: 2018-07-31T09:28:37.352812: step 7084, loss 0.562131.
Train: 2018-07-31T09:28:37.509024: step 7085, loss 0.52033.
Train: 2018-07-31T09:28:37.680829: step 7086, loss 0.554399.
Train: 2018-07-31T09:28:37.837042: step 7087, loss 0.535921.
Train: 2018-07-31T09:28:37.993255: step 7088, loss 0.449891.
Train: 2018-07-31T09:28:38.149479: step 7089, loss 0.614023.
Train: 2018-07-31T09:28:38.305715: step 7090, loss 0.561594.
Test: 2018-07-31T09:28:38.555626: step 7090, loss 0.547795.
Train: 2018-07-31T09:28:38.711838: step 7091, loss 0.544618.
Train: 2018-07-31T09:28:38.868052: step 7092, loss 0.571713.
Train: 2018-07-31T09:28:39.024264: step 7093, loss 0.607713.
Train: 2018-07-31T09:28:39.180508: step 7094, loss 0.562347.
Train: 2018-07-31T09:28:39.336722: step 7095, loss 0.527487.
Train: 2018-07-31T09:28:39.492906: step 7096, loss 0.508936.
Train: 2018-07-31T09:28:39.649118: step 7097, loss 0.599207.
Train: 2018-07-31T09:28:39.820957: step 7098, loss 0.570921.
Train: 2018-07-31T09:28:39.977167: step 7099, loss 0.527107.
Train: 2018-07-31T09:28:40.133410: step 7100, loss 0.553293.
Test: 2018-07-31T09:28:40.367701: step 7100, loss 0.547748.
Train: 2018-07-31T09:28:41.101904: step 7101, loss 0.64291.
Train: 2018-07-31T09:28:41.258117: step 7102, loss 0.606868.
Train: 2018-07-31T09:28:41.414329: step 7103, loss 0.562227.
Train: 2018-07-31T09:28:41.586190: step 7104, loss 0.590266.
Train: 2018-07-31T09:28:41.726787: step 7105, loss 0.622871.
Train: 2018-07-31T09:28:41.882971: step 7106, loss 0.564198.
Train: 2018-07-31T09:28:42.054838: step 7107, loss 0.54562.
Train: 2018-07-31T09:28:42.211018: step 7108, loss 0.546373.
Train: 2018-07-31T09:28:42.367262: step 7109, loss 0.55988.
Train: 2018-07-31T09:28:42.539067: step 7110, loss 0.57145.
Test: 2018-07-31T09:28:42.773418: step 7110, loss 0.548259.
Train: 2018-07-31T09:28:42.929599: step 7111, loss 0.545129.
Train: 2018-07-31T09:28:43.101459: step 7112, loss 0.54684.
Train: 2018-07-31T09:28:43.257679: step 7113, loss 0.552961.
Train: 2018-07-31T09:28:43.398241: step 7114, loss 0.528722.
Train: 2018-07-31T09:28:43.554484: step 7115, loss 0.537855.
Train: 2018-07-31T09:28:43.726318: step 7116, loss 0.494118.
Train: 2018-07-31T09:28:43.882532: step 7117, loss 0.601951.
Train: 2018-07-31T09:28:44.038715: step 7118, loss 0.645745.
Train: 2018-07-31T09:28:44.194928: step 7119, loss 0.624511.
Train: 2018-07-31T09:28:44.351172: step 7120, loss 0.571543.
Test: 2018-07-31T09:28:44.601085: step 7120, loss 0.548225.
Train: 2018-07-31T09:28:44.757296: step 7121, loss 0.555038.
Train: 2018-07-31T09:28:44.913510: step 7122, loss 0.537015.
Train: 2018-07-31T09:28:45.069723: step 7123, loss 0.579911.
Train: 2018-07-31T09:28:45.241558: step 7124, loss 0.553111.
Train: 2018-07-31T09:28:45.397772: step 7125, loss 0.610959.
Train: 2018-07-31T09:28:45.553986: step 7126, loss 0.504025.
Train: 2018-07-31T09:28:45.710229: step 7127, loss 0.595058.
Train: 2018-07-31T09:28:45.882064: step 7128, loss 0.604204.
Train: 2018-07-31T09:28:46.038247: step 7129, loss 0.601185.
Train: 2018-07-31T09:28:46.194493: step 7130, loss 0.587803.
Test: 2018-07-31T09:28:46.428806: step 7130, loss 0.550543.
Train: 2018-07-31T09:28:46.585024: step 7131, loss 0.575721.
Train: 2018-07-31T09:28:46.756858: step 7132, loss 0.494027.
Train: 2018-07-31T09:28:46.913061: step 7133, loss 0.62581.
Train: 2018-07-31T09:28:47.069288: step 7134, loss 0.626096.
Train: 2018-07-31T09:28:47.225470: step 7135, loss 0.543266.
Train: 2018-07-31T09:28:47.397334: step 7136, loss 0.549918.
Train: 2018-07-31T09:28:47.553541: step 7137, loss 0.639883.
Train: 2018-07-31T09:28:47.709761: step 7138, loss 0.595431.
Train: 2018-07-31T09:28:47.865974: step 7139, loss 0.523455.
Train: 2018-07-31T09:28:48.037809: step 7140, loss 0.5409.
Test: 2018-07-31T09:28:48.272099: step 7140, loss 0.549222.
Train: 2018-07-31T09:28:48.428337: step 7141, loss 0.534099.
Train: 2018-07-31T09:28:48.584527: step 7142, loss 0.574468.
Train: 2018-07-31T09:28:48.756362: step 7143, loss 0.548347.
Train: 2018-07-31T09:28:48.912574: step 7144, loss 0.548262.
Train: 2018-07-31T09:28:49.068788: step 7145, loss 0.597636.
Train: 2018-07-31T09:28:49.225031: step 7146, loss 0.609782.
Train: 2018-07-31T09:28:49.381214: step 7147, loss 0.589079.
Train: 2018-07-31T09:28:49.537428: step 7148, loss 0.56471.
Train: 2018-07-31T09:28:49.693642: step 7149, loss 0.607089.
Train: 2018-07-31T09:28:49.849854: step 7150, loss 0.529544.
Test: 2018-07-31T09:28:50.099821: step 7150, loss 0.548971.
Train: 2018-07-31T09:28:50.256035: step 7151, loss 0.512678.
Train: 2018-07-31T09:28:50.412253: step 7152, loss 0.580587.
Train: 2018-07-31T09:28:50.568435: step 7153, loss 0.537848.
Train: 2018-07-31T09:28:50.724679: step 7154, loss 0.492683.
Train: 2018-07-31T09:28:50.896485: step 7155, loss 0.603357.
Train: 2018-07-31T09:28:51.052698: step 7156, loss 0.537795.
Train: 2018-07-31T09:28:51.208942: step 7157, loss 0.606171.
Train: 2018-07-31T09:28:51.365124: step 7158, loss 0.595507.
Train: 2018-07-31T09:28:51.521338: step 7159, loss 0.555277.
Train: 2018-07-31T09:28:51.677584: step 7160, loss 0.52051.
Test: 2018-07-31T09:28:51.927522: step 7160, loss 0.549157.
Train: 2018-07-31T09:28:52.083731: step 7161, loss 0.554373.
Train: 2018-07-31T09:28:52.239951: step 7162, loss 0.545518.
Train: 2018-07-31T09:28:52.396133: step 7163, loss 0.54573.
Train: 2018-07-31T09:28:52.552347: step 7164, loss 0.553819.
Train: 2018-07-31T09:28:52.724211: step 7165, loss 0.564949.
Train: 2018-07-31T09:28:52.880425: step 7166, loss 0.563703.
Train: 2018-07-31T09:28:53.036638: step 7167, loss 0.547749.
Train: 2018-07-31T09:28:53.192822: step 7168, loss 0.610249.
Train: 2018-07-31T09:28:53.349065: step 7169, loss 0.492689.
Train: 2018-07-31T09:28:53.505279: step 7170, loss 0.599664.
Test: 2018-07-31T09:28:53.739569: step 7170, loss 0.549482.
Train: 2018-07-31T09:28:53.911427: step 7171, loss 0.624152.
Train: 2018-07-31T09:28:54.067655: step 7172, loss 0.632174.
Train: 2018-07-31T09:28:54.223829: step 7173, loss 0.573529.
Train: 2018-07-31T09:28:54.380068: step 7174, loss 0.572589.
Train: 2018-07-31T09:28:54.520665: step 7175, loss 0.569772.
Train: 2018-07-31T09:28:54.676873: step 7176, loss 0.575031.
Train: 2018-07-31T09:28:54.833093: step 7177, loss 0.545647.
Train: 2018-07-31T09:28:54.989301: step 7178, loss 0.626655.
Train: 2018-07-31T09:28:55.145525: step 7179, loss 0.590592.
Train: 2018-07-31T09:28:55.301736: step 7180, loss 0.504124.
Test: 2018-07-31T09:28:55.551644: step 7180, loss 0.550262.
Train: 2018-07-31T09:28:55.707858: step 7181, loss 0.52121.
Train: 2018-07-31T09:28:55.864071: step 7182, loss 0.555865.
Train: 2018-07-31T09:28:56.020308: step 7183, loss 0.52567.
Train: 2018-07-31T09:28:56.176528: step 7184, loss 0.528085.
Train: 2018-07-31T09:28:56.332741: step 7185, loss 0.674972.
Train: 2018-07-31T09:28:56.488925: step 7186, loss 0.62422.
Train: 2018-07-31T09:28:56.645138: step 7187, loss 0.549023.
Train: 2018-07-31T09:28:56.801382: step 7188, loss 0.545746.
Train: 2018-07-31T09:28:56.973187: step 7189, loss 0.548562.
Train: 2018-07-31T09:28:57.129400: step 7190, loss 0.575613.
Test: 2018-07-31T09:28:57.363751: step 7190, loss 0.551853.
Train: 2018-07-31T09:28:57.535580: step 7191, loss 0.591438.
Train: 2018-07-31T09:28:57.691800: step 7192, loss 0.5469.
Train: 2018-07-31T09:28:57.863602: step 7193, loss 0.489989.
Train: 2018-07-31T09:28:58.019852: step 7194, loss 0.532453.
Train: 2018-07-31T09:28:58.191684: step 7195, loss 0.679364.
Train: 2018-07-31T09:28:58.347865: step 7196, loss 0.486382.
Train: 2018-07-31T09:28:58.504078: step 7197, loss 0.500199.
Train: 2018-07-31T09:28:58.660322: step 7198, loss 0.59725.
Train: 2018-07-31T09:28:58.832127: step 7199, loss 0.555833.
Train: 2018-07-31T09:28:58.988339: step 7200, loss 0.572582.
Test: 2018-07-31T09:28:59.222661: step 7200, loss 0.556096.
Train: 2018-07-31T09:28:59.956892: step 7201, loss 0.552249.
Train: 2018-07-31T09:29:00.113106: step 7202, loss 0.503741.
Train: 2018-07-31T09:29:00.284942: step 7203, loss 0.655091.
Train: 2018-07-31T09:29:00.441157: step 7204, loss 0.60567.
Train: 2018-07-31T09:29:00.612989: step 7205, loss 0.522851.
Train: 2018-07-31T09:29:00.769172: step 7206, loss 0.517984.
Train: 2018-07-31T09:29:00.941040: step 7207, loss 0.579702.
Train: 2018-07-31T09:29:01.112866: step 7208, loss 0.57334.
Train: 2018-07-31T09:29:01.269085: step 7209, loss 0.592185.
Train: 2018-07-31T09:29:01.440891: step 7210, loss 0.564516.
Test: 2018-07-31T09:29:01.690833: step 7210, loss 0.560529.
Train: 2018-07-31T09:29:01.862666: step 7211, loss 0.568173.
Train: 2018-07-31T09:29:02.034526: step 7212, loss 0.619935.
Train: 2018-07-31T09:29:02.206360: step 7213, loss 0.598462.
Train: 2018-07-31T09:29:02.378195: step 7214, loss 0.557713.
Train: 2018-07-31T09:29:02.534412: step 7215, loss 0.514516.
Train: 2018-07-31T09:29:02.706249: step 7216, loss 0.587463.
Train: 2018-07-31T09:29:02.878084: step 7217, loss 0.558658.
Train: 2018-07-31T09:29:03.049922: step 7218, loss 0.580131.
Train: 2018-07-31T09:29:03.221755: step 7219, loss 0.583088.
Train: 2018-07-31T09:29:03.393588: step 7220, loss 0.662998.
Test: 2018-07-31T09:29:03.627878: step 7220, loss 0.585863.
Train: 2018-07-31T09:29:03.815334: step 7221, loss 0.589942.
Train: 2018-07-31T09:29:03.987193: step 7222, loss 0.585952.
Train: 2018-07-31T09:29:04.159034: step 7223, loss 0.573706.
Train: 2018-07-31T09:29:04.346459: step 7224, loss 1.56842.
Train: 2018-07-31T09:29:04.518324: step 7225, loss 0.647775.
Train: 2018-07-31T09:29:04.690161: step 7226, loss 0.607674.
Train: 2018-07-31T09:29:04.861963: step 7227, loss 0.696059.
Train: 2018-07-31T09:29:05.049445: step 7228, loss 0.784437.
Train: 2018-07-31T09:29:05.221285: step 7229, loss 0.769863.
Train: 2018-07-31T09:29:05.393089: step 7230, loss 0.742221.
Test: 2018-07-31T09:29:05.627410: step 7230, loss 0.790655.
Train: 2018-07-31T09:29:05.799275: step 7231, loss 0.84537.
Train: 2018-07-31T09:29:05.986732: step 7232, loss 0.856817.
Train: 2018-07-31T09:29:06.158536: step 7233, loss 0.89479.
Train: 2018-07-31T09:29:06.346022: step 7234, loss 0.951886.
Train: 2018-07-31T09:29:06.517858: step 7235, loss 0.934806.
Train: 2018-07-31T09:29:06.689696: step 7236, loss 0.876199.
Train: 2018-07-31T09:29:06.861526: step 7237, loss 0.986351.
Train: 2018-07-31T09:29:07.033362: step 7238, loss 0.881424.
Train: 2018-07-31T09:29:07.220787: step 7239, loss 0.863351.
Train: 2018-07-31T09:29:07.392654: step 7240, loss 0.937503.
Test: 2018-07-31T09:29:07.626973: step 7240, loss 0.8801.
Train: 2018-07-31T09:29:07.814430: step 7241, loss 0.863841.
Train: 2018-07-31T09:29:07.986262: step 7242, loss 1.11912.
Train: 2018-07-31T09:29:08.173689: step 7243, loss 0.872984.
Train: 2018-07-31T09:29:08.329934: step 7244, loss 0.956798.
Train: 2018-07-31T09:29:08.501736: step 7245, loss 0.831185.
Train: 2018-07-31T09:29:08.689227: step 7246, loss 0.910701.
Train: 2018-07-31T09:29:08.861058: step 7247, loss 0.917707.
Train: 2018-07-31T09:29:09.032863: step 7248, loss 1.05822.
Train: 2018-07-31T09:29:09.204727: step 7249, loss 1.03883.
Train: 2018-07-31T09:29:09.376563: step 7250, loss 0.85045.
Test: 2018-07-31T09:29:09.626497: step 7250, loss 0.885642.
Train: 2018-07-31T09:29:09.798339: step 7251, loss 0.962103.
Train: 2018-07-31T09:29:09.970173: step 7252, loss 0.965661.
Train: 2018-07-31T09:29:10.141977: step 7253, loss 0.924803.
Train: 2018-07-31T09:29:10.329434: step 7254, loss 0.881736.
Train: 2018-07-31T09:29:10.501269: step 7255, loss 0.890152.
Train: 2018-07-31T09:29:10.673103: step 7256, loss 0.796551.
Train: 2018-07-31T09:29:10.860560: step 7257, loss 0.782285.
Train: 2018-07-31T09:29:11.032395: step 7258, loss 0.911335.
Train: 2018-07-31T09:29:11.219850: step 7259, loss 0.840373.
Train: 2018-07-31T09:29:11.391715: step 7260, loss 0.841133.
Test: 2018-07-31T09:29:11.626005: step 7260, loss 0.823823.
Train: 2018-07-31T09:29:11.813460: step 7261, loss 0.895852.
Train: 2018-07-31T09:29:11.985320: step 7262, loss 0.839876.
Train: 2018-07-31T09:29:12.172785: step 7263, loss 0.850897.
Train: 2018-07-31T09:29:12.344587: step 7264, loss 0.738441.
Train: 2018-07-31T09:29:12.532076: step 7265, loss 0.767598.
Train: 2018-07-31T09:29:12.719500: step 7266, loss 0.824577.
Train: 2018-07-31T09:29:12.891334: step 7267, loss 0.872621.
Train: 2018-07-31T09:29:13.063193: step 7268, loss 0.753054.
Train: 2018-07-31T09:29:13.235028: step 7269, loss 0.730467.
Train: 2018-07-31T09:29:13.422492: step 7270, loss 0.792843.
Test: 2018-07-31T09:29:13.656811: step 7270, loss 0.769459.
Train: 2018-07-31T09:29:13.844266: step 7271, loss 0.854139.
Train: 2018-07-31T09:29:14.016071: step 7272, loss 0.814101.
Train: 2018-07-31T09:29:14.187905: step 7273, loss 0.822755.
Train: 2018-07-31T09:29:14.359740: step 7274, loss 0.764042.
Train: 2018-07-31T09:29:14.531574: step 7275, loss 0.735212.
Train: 2018-07-31T09:29:14.703409: step 7276, loss 0.904994.
Train: 2018-07-31T09:29:14.890865: step 7277, loss 0.806052.
Train: 2018-07-31T09:29:15.062733: step 7278, loss 0.741377.
Train: 2018-07-31T09:29:15.234565: step 7279, loss 0.748667.
Train: 2018-07-31T09:29:15.406400: step 7280, loss 0.730445.
Test: 2018-07-31T09:29:15.656337: step 7280, loss 0.731035.
Train: 2018-07-31T09:29:15.828176: step 7281, loss 0.758497.
Train: 2018-07-31T09:29:15.999980: step 7282, loss 0.719933.
Train: 2018-07-31T09:29:16.187470: step 7283, loss 0.774547.
Train: 2018-07-31T09:29:16.359296: step 7284, loss 0.735036.
Train: 2018-07-31T09:29:16.546727: step 7285, loss 0.764695.
Train: 2018-07-31T09:29:16.718595: step 7286, loss 0.716459.
Train: 2018-07-31T09:29:16.906019: step 7287, loss 0.697236.
Train: 2018-07-31T09:29:17.077854: step 7288, loss 0.771015.
Train: 2018-07-31T09:29:17.249712: step 7289, loss 0.741567.
Train: 2018-07-31T09:29:17.437182: step 7290, loss 0.744211.
Test: 2018-07-31T09:29:17.671465: step 7290, loss 0.698251.
Train: 2018-07-31T09:29:17.843323: step 7291, loss 0.748161.
Train: 2018-07-31T09:29:18.046376: step 7292, loss 0.757673.
Train: 2018-07-31T09:29:18.218212: step 7293, loss 0.680159.
Train: 2018-07-31T09:29:18.390076: step 7294, loss 0.657519.
Train: 2018-07-31T09:29:18.561913: step 7295, loss 0.697904.
Train: 2018-07-31T09:29:18.749337: step 7296, loss 0.627139.
Train: 2018-07-31T09:29:18.921201: step 7297, loss 0.794217.
Train: 2018-07-31T09:29:19.108628: step 7298, loss 0.652097.
Train: 2018-07-31T09:29:19.280487: step 7299, loss 0.676842.
Train: 2018-07-31T09:29:19.452327: step 7300, loss 0.719377.
Test: 2018-07-31T09:29:19.686619: step 7300, loss 0.680875.
Train: 2018-07-31T09:29:20.405229: step 7301, loss 0.704613.
Train: 2018-07-31T09:29:20.577033: step 7302, loss 0.694607.
Train: 2018-07-31T09:29:20.748869: step 7303, loss 0.697479.
Train: 2018-07-31T09:29:20.936325: step 7304, loss 0.713516.
Train: 2018-07-31T09:29:21.108190: step 7305, loss 0.697739.
Train: 2018-07-31T09:29:21.280018: step 7306, loss 0.702815.
Train: 2018-07-31T09:29:21.451829: step 7307, loss 0.644973.
Train: 2018-07-31T09:29:21.623688: step 7308, loss 0.717154.
Train: 2018-07-31T09:29:21.795499: step 7309, loss 0.660326.
Train: 2018-07-31T09:29:21.982955: step 7310, loss 0.666138.
Test: 2018-07-31T09:29:22.217306: step 7310, loss 0.666633.
Train: 2018-07-31T09:29:22.389139: step 7311, loss 0.737383.
Train: 2018-07-31T09:29:22.560969: step 7312, loss 0.67947.
Train: 2018-07-31T09:29:22.732780: step 7313, loss 0.626502.
Train: 2018-07-31T09:29:22.904614: step 7314, loss 0.648823.
Train: 2018-07-31T09:29:23.092069: step 7315, loss 0.668038.
Train: 2018-07-31T09:29:23.263935: step 7316, loss 0.78004.
Train: 2018-07-31T09:29:23.435772: step 7317, loss 0.663009.
Train: 2018-07-31T09:29:23.607599: step 7318, loss 0.676146.
Train: 2018-07-31T09:29:23.795063: step 7319, loss 0.664814.
Train: 2018-07-31T09:29:23.966895: step 7320, loss 0.643927.
Test: 2018-07-31T09:29:24.201217: step 7320, loss 0.646539.
Train: 2018-07-31T09:29:24.373021: step 7321, loss 0.669194.
Train: 2018-07-31T09:29:24.560509: step 7322, loss 0.617962.
Train: 2018-07-31T09:29:24.732341: step 7323, loss 0.597007.
Train: 2018-07-31T09:29:24.904146: step 7324, loss 0.666952.
Train: 2018-07-31T09:29:25.076005: step 7325, loss 0.68272.
Train: 2018-07-31T09:29:25.247845: step 7326, loss 0.652938.
Train: 2018-07-31T09:29:25.435304: step 7327, loss 0.630614.
Train: 2018-07-31T09:29:25.607137: step 7328, loss 0.658219.
Train: 2018-07-31T09:29:25.778971: step 7329, loss 0.632329.
Train: 2018-07-31T09:29:25.950775: step 7330, loss 0.660303.
Test: 2018-07-31T09:29:26.200747: step 7330, loss 0.634011.
Train: 2018-07-31T09:29:26.372582: step 7331, loss 0.658371.
Train: 2018-07-31T09:29:26.544414: step 7332, loss 0.590086.
Train: 2018-07-31T09:29:26.716222: step 7333, loss 0.698357.
Train: 2018-07-31T09:29:26.888088: step 7334, loss 0.695716.
Train: 2018-07-31T09:29:27.075537: step 7335, loss 0.669739.
Train: 2018-07-31T09:29:27.247377: step 7336, loss 0.613689.
Train: 2018-07-31T09:29:27.419182: step 7337, loss 0.639214.
Train: 2018-07-31T09:29:27.606662: step 7338, loss 0.655243.
Train: 2018-07-31T09:29:27.778503: step 7339, loss 0.687598.
Train: 2018-07-31T09:29:27.950338: step 7340, loss 0.665084.
Test: 2018-07-31T09:29:28.200249: step 7340, loss 0.624429.
Train: 2018-07-31T09:29:28.372113: step 7341, loss 0.638142.
Train: 2018-07-31T09:29:28.543948: step 7342, loss 0.669142.
Train: 2018-07-31T09:29:28.715777: step 7343, loss 0.651105.
Train: 2018-07-31T09:29:28.903208: step 7344, loss 0.658258.
Train: 2018-07-31T09:29:29.075074: step 7345, loss 0.608479.
Train: 2018-07-31T09:29:29.246878: step 7346, loss 0.597155.
Train: 2018-07-31T09:29:29.418746: step 7347, loss 0.591174.
Train: 2018-07-31T09:29:29.590579: step 7348, loss 0.610867.
Train: 2018-07-31T09:29:29.778036: step 7349, loss 0.530786.
Train: 2018-07-31T09:29:29.949838: step 7350, loss 0.651923.
Test: 2018-07-31T09:29:30.199780: step 7350, loss 0.612117.
Train: 2018-07-31T09:29:30.371615: step 7351, loss 0.674352.
Train: 2018-07-31T09:29:30.543450: step 7352, loss 0.703497.
Train: 2018-07-31T09:29:30.730906: step 7353, loss 0.592486.
Train: 2018-07-31T09:29:30.887120: step 7354, loss 0.635463.
Train: 2018-07-31T09:29:31.074576: step 7355, loss 0.624266.
Train: 2018-07-31T09:29:31.246435: step 7356, loss 0.63699.
Train: 2018-07-31T09:29:31.418244: step 7357, loss 0.662698.
Train: 2018-07-31T09:29:31.590080: step 7358, loss 0.58787.
Train: 2018-07-31T09:29:31.777537: step 7359, loss 0.617702.
Train: 2018-07-31T09:29:31.933749: step 7360, loss 0.62296.
Test: 2018-07-31T09:29:32.183716: step 7360, loss 0.605933.
Train: 2018-07-31T09:29:32.355558: step 7361, loss 0.660337.
Train: 2018-07-31T09:29:32.542982: step 7362, loss 0.613774.
Train: 2018-07-31T09:29:32.714841: step 7363, loss 0.533063.
Train: 2018-07-31T09:29:32.886681: step 7364, loss 0.651983.
Train: 2018-07-31T09:29:33.058486: step 7365, loss 0.605729.
Train: 2018-07-31T09:29:33.230346: step 7366, loss 0.68775.
Train: 2018-07-31T09:29:33.402185: step 7367, loss 0.598787.
Train: 2018-07-31T09:29:33.574021: step 7368, loss 0.620797.
Train: 2018-07-31T09:29:33.745855: step 7369, loss 0.638423.
Train: 2018-07-31T09:29:33.917660: step 7370, loss 0.655287.
Test: 2018-07-31T09:29:34.167635: step 7370, loss 0.601015.
Train: 2018-07-31T09:29:34.339436: step 7371, loss 0.668947.
Train: 2018-07-31T09:29:34.526922: step 7372, loss 0.641563.
Train: 2018-07-31T09:29:34.683105: step 7373, loss 0.603478.
Train: 2018-07-31T09:29:34.870563: step 7374, loss 0.586535.
Train: 2018-07-31T09:29:35.042397: step 7375, loss 0.627055.
Train: 2018-07-31T09:29:35.214262: step 7376, loss 0.590452.
Train: 2018-07-31T09:29:35.386066: step 7377, loss 0.577288.
Train: 2018-07-31T09:29:35.557933: step 7378, loss 0.621867.
Train: 2018-07-31T09:29:35.729736: step 7379, loss 0.605087.
Train: 2018-07-31T09:29:35.917216: step 7380, loss 0.602159.
Test: 2018-07-31T09:29:36.151512: step 7380, loss 0.594267.
Train: 2018-07-31T09:29:36.323347: step 7381, loss 0.669089.
Train: 2018-07-31T09:29:36.510827: step 7382, loss 0.576304.
Train: 2018-07-31T09:29:36.682670: step 7383, loss 0.566437.
Train: 2018-07-31T09:29:36.854471: step 7384, loss 0.637844.
Train: 2018-07-31T09:29:37.026307: step 7385, loss 0.58102.
Train: 2018-07-31T09:29:37.198172: step 7386, loss 0.726606.
Train: 2018-07-31T09:29:37.385598: step 7387, loss 0.665681.
Train: 2018-07-31T09:29:37.557466: step 7388, loss 0.709482.
Train: 2018-07-31T09:29:37.729267: step 7389, loss 0.622996.
Train: 2018-07-31T09:29:37.901132: step 7390, loss 0.654268.
Test: 2018-07-31T09:29:38.135454: step 7390, loss 0.589846.
Train: 2018-07-31T09:29:38.307258: step 7391, loss 0.506473.
Train: 2018-07-31T09:29:38.479121: step 7392, loss 0.622624.
Train: 2018-07-31T09:29:38.650926: step 7393, loss 0.597729.
Train: 2018-07-31T09:29:38.822761: step 7394, loss 0.567706.
Train: 2018-07-31T09:29:38.994597: step 7395, loss 0.612101.
Train: 2018-07-31T09:29:39.166460: step 7396, loss 0.570929.
Train: 2018-07-31T09:29:39.338266: step 7397, loss 0.607736.
Train: 2018-07-31T09:29:39.510101: step 7398, loss 0.615914.
Train: 2018-07-31T09:29:39.681968: step 7399, loss 0.599921.
Train: 2018-07-31T09:29:39.869391: step 7400, loss 0.613508.
Test: 2018-07-31T09:29:40.103713: step 7400, loss 0.58505.
Train: 2018-07-31T09:29:40.806704: step 7401, loss 0.570584.
Train: 2018-07-31T09:29:40.994153: step 7402, loss 0.532197.
Train: 2018-07-31T09:29:41.165992: step 7403, loss 0.560145.
Train: 2018-07-31T09:29:41.337833: step 7404, loss 0.639608.
Train: 2018-07-31T09:29:41.509631: step 7405, loss 0.553235.
Train: 2018-07-31T09:29:41.681467: step 7406, loss 0.614326.
Train: 2018-07-31T09:29:41.853331: step 7407, loss 0.5347.
Train: 2018-07-31T09:29:42.025137: step 7408, loss 0.578967.
Train: 2018-07-31T09:29:42.212593: step 7409, loss 0.615529.
Train: 2018-07-31T09:29:42.384451: step 7410, loss 0.55352.
Test: 2018-07-31T09:29:42.618778: step 7410, loss 0.580272.
Train: 2018-07-31T09:29:42.790583: step 7411, loss 0.613629.
Train: 2018-07-31T09:29:42.962417: step 7412, loss 0.578282.
Train: 2018-07-31T09:29:43.149873: step 7413, loss 0.497091.
Train: 2018-07-31T09:29:43.306119: step 7414, loss 0.578137.
Train: 2018-07-31T09:29:43.477920: step 7415, loss 0.634267.
Train: 2018-07-31T09:29:43.649755: step 7416, loss 0.604196.
Train: 2018-07-31T09:29:43.821591: step 7417, loss 0.546534.
Train: 2018-07-31T09:29:43.993450: step 7418, loss 0.526007.
Train: 2018-07-31T09:29:44.165290: step 7419, loss 0.544509.
Train: 2018-07-31T09:29:44.337119: step 7420, loss 0.611744.
Test: 2018-07-31T09:29:44.571415: step 7420, loss 0.577525.
Train: 2018-07-31T09:29:44.743250: step 7421, loss 0.577665.
Train: 2018-07-31T09:29:44.915117: step 7422, loss 0.602685.
Train: 2018-07-31T09:29:45.086949: step 7423, loss 0.603714.
Train: 2018-07-31T09:29:45.258755: step 7424, loss 0.586248.
Train: 2018-07-31T09:29:45.446235: step 7425, loss 0.575392.
Train: 2018-07-31T09:29:45.618075: step 7426, loss 0.537077.
Train: 2018-07-31T09:29:45.789880: step 7427, loss 0.538777.
Train: 2018-07-31T09:29:45.946123: step 7428, loss 0.600303.
Train: 2018-07-31T09:29:46.117928: step 7429, loss 0.534947.
Train: 2018-07-31T09:29:46.289763: step 7430, loss 0.628033.
Test: 2018-07-31T09:29:46.524085: step 7430, loss 0.575071.
Train: 2018-07-31T09:29:46.695948: step 7431, loss 0.553817.
Train: 2018-07-31T09:29:46.867783: step 7432, loss 0.562052.
Train: 2018-07-31T09:29:47.039586: step 7433, loss 0.536889.
Train: 2018-07-31T09:29:47.211446: step 7434, loss 0.710699.
Train: 2018-07-31T09:29:47.367635: step 7435, loss 0.609461.
Train: 2018-07-31T09:29:47.539500: step 7436, loss 0.635339.
Train: 2018-07-31T09:29:47.711336: step 7437, loss 0.571535.
Train: 2018-07-31T09:29:47.883139: step 7438, loss 0.562179.
Train: 2018-07-31T09:29:48.039386: step 7439, loss 0.614786.
Train: 2018-07-31T09:29:48.211212: step 7440, loss 0.615157.
Test: 2018-07-31T09:29:48.461131: step 7440, loss 0.573053.
Train: 2018-07-31T09:29:48.632964: step 7441, loss 0.50912.
Train: 2018-07-31T09:29:48.789207: step 7442, loss 0.620416.
Train: 2018-07-31T09:29:48.961012: step 7443, loss 0.578721.
Train: 2018-07-31T09:29:49.132847: step 7444, loss 0.613597.
Train: 2018-07-31T09:29:49.304706: step 7445, loss 0.603856.
Train: 2018-07-31T09:29:49.476551: step 7446, loss 0.621271.
Train: 2018-07-31T09:29:49.648352: step 7447, loss 0.587651.
Train: 2018-07-31T09:29:49.804597: step 7448, loss 0.560446.
Train: 2018-07-31T09:29:49.976399: step 7449, loss 0.552654.
Train: 2018-07-31T09:29:50.148236: step 7450, loss 0.542937.
Test: 2018-07-31T09:29:50.382556: step 7450, loss 0.571462.
Train: 2018-07-31T09:29:50.554421: step 7451, loss 0.635265.
Train: 2018-07-31T09:29:50.710603: step 7452, loss 0.593033.
Train: 2018-07-31T09:29:50.882437: step 7453, loss 0.569972.
Train: 2018-07-31T09:29:51.054271: step 7454, loss 0.636756.
Train: 2018-07-31T09:29:51.226137: step 7455, loss 0.617921.
Train: 2018-07-31T09:29:51.397941: step 7456, loss 0.61023.
Train: 2018-07-31T09:29:51.569807: step 7457, loss 0.561566.
Train: 2018-07-31T09:29:51.725990: step 7458, loss 0.58415.
Train: 2018-07-31T09:29:51.882203: step 7459, loss 0.599148.
Train: 2018-07-31T09:29:52.054071: step 7460, loss 0.550823.
Test: 2018-07-31T09:29:52.288386: step 7460, loss 0.570396.
Train: 2018-07-31T09:29:52.460192: step 7461, loss 0.576467.
Train: 2018-07-31T09:29:52.616436: step 7462, loss 0.615985.
Train: 2018-07-31T09:29:52.788274: step 7463, loss 0.665033.
Train: 2018-07-31T09:29:52.944479: step 7464, loss 0.587983.
Train: 2018-07-31T09:29:53.116289: step 7465, loss 0.537079.
Train: 2018-07-31T09:29:53.272529: step 7466, loss 0.567076.
Train: 2018-07-31T09:29:53.444368: step 7467, loss 0.626174.
Train: 2018-07-31T09:29:53.616171: step 7468, loss 0.575631.
Train: 2018-07-31T09:29:53.772410: step 7469, loss 0.559638.
Train: 2018-07-31T09:29:53.944251: step 7470, loss 0.5602.
Test: 2018-07-31T09:29:54.178575: step 7470, loss 0.570842.
Train: 2018-07-31T09:29:54.350376: step 7471, loss 0.552662.
Train: 2018-07-31T09:29:54.506620: step 7472, loss 0.565651.
Train: 2018-07-31T09:29:54.678456: step 7473, loss 0.541626.
Train: 2018-07-31T09:29:54.850262: step 7474, loss 0.569183.
Train: 2018-07-31T09:29:55.006502: step 7475, loss 0.608119.
Train: 2018-07-31T09:29:55.178331: step 7476, loss 0.554104.
Train: 2018-07-31T09:29:55.334520: step 7477, loss 0.612844.
Train: 2018-07-31T09:29:55.506355: step 7478, loss 0.585795.
Train: 2018-07-31T09:29:55.678188: step 7479, loss 0.524716.
Train: 2018-07-31T09:29:55.834404: step 7480, loss 0.588216.
Test: 2018-07-31T09:29:56.084343: step 7480, loss 0.572247.
Train: 2018-07-31T09:29:56.240582: step 7481, loss 0.630761.
Train: 2018-07-31T09:29:56.412392: step 7482, loss 0.543866.
Train: 2018-07-31T09:29:56.568605: step 7483, loss 0.629623.
Train: 2018-07-31T09:29:56.740470: step 7484, loss 0.560572.
Train: 2018-07-31T09:29:56.896654: step 7485, loss 0.604371.
Train: 2018-07-31T09:29:57.068488: step 7486, loss 0.587611.
Train: 2018-07-31T09:29:57.224701: step 7487, loss 0.5336.
Train: 2018-07-31T09:29:57.396562: step 7488, loss 0.55804.
Train: 2018-07-31T09:29:57.552783: step 7489, loss 0.609462.
Train: 2018-07-31T09:29:57.724609: step 7490, loss 0.657939.
Test: 2018-07-31T09:29:57.958936: step 7490, loss 0.570336.
Train: 2018-07-31T09:29:58.130765: step 7491, loss 0.618635.
Train: 2018-07-31T09:29:58.286955: step 7492, loss 0.62051.
Train: 2018-07-31T09:29:58.458789: step 7493, loss 0.526664.
Train: 2018-07-31T09:29:58.615031: step 7494, loss 0.562826.
Train: 2018-07-31T09:29:58.802469: step 7495, loss 0.60537.
Train: 2018-07-31T09:29:58.958696: step 7496, loss 0.546809.
Train: 2018-07-31T09:29:59.114915: step 7497, loss 0.665144.
Train: 2018-07-31T09:29:59.286720: step 7498, loss 0.546476.
Train: 2018-07-31T09:29:59.458554: step 7499, loss 0.554824.
Train: 2018-07-31T09:29:59.614797: step 7500, loss 0.589524.
Test: 2018-07-31T09:29:59.864738: step 7500, loss 0.574567.
Train: 2018-07-31T09:30:00.598942: step 7501, loss 0.578918.
Train: 2018-07-31T09:30:00.770746: step 7502, loss 0.563258.
Train: 2018-07-31T09:30:00.942611: step 7503, loss 0.672886.
Train: 2018-07-31T09:30:01.098795: step 7504, loss 0.647857.
Train: 2018-07-31T09:30:01.255038: step 7505, loss 0.508598.
Train: 2018-07-31T09:30:01.426842: step 7506, loss 0.62817.
Train: 2018-07-31T09:30:01.583056: step 7507, loss 0.576918.
Train: 2018-07-31T09:30:01.754921: step 7508, loss 0.601954.
Train: 2018-07-31T09:30:01.911105: step 7509, loss 0.59049.
Train: 2018-07-31T09:30:02.082940: step 7510, loss 0.587031.
Test: 2018-07-31T09:30:02.317260: step 7510, loss 0.570451.
Train: 2018-07-31T09:30:02.473472: step 7511, loss 0.636296.
Train: 2018-07-31T09:30:02.645307: step 7512, loss 0.644064.
Train: 2018-07-31T09:30:02.801554: step 7513, loss 0.592907.
Train: 2018-07-31T09:30:02.973356: step 7514, loss 0.524461.
Train: 2018-07-31T09:30:03.129594: step 7515, loss 0.608135.
Train: 2018-07-31T09:30:03.301405: step 7516, loss 0.657323.
Train: 2018-07-31T09:30:03.457618: step 7517, loss 0.591183.
Train: 2018-07-31T09:30:03.629453: step 7518, loss 0.584937.
Train: 2018-07-31T09:30:03.785666: step 7519, loss 0.524422.
Train: 2018-07-31T09:30:03.941904: step 7520, loss 0.580375.
Test: 2018-07-31T09:30:04.176199: step 7520, loss 0.56742.
Train: 2018-07-31T09:30:04.348064: step 7521, loss 0.681498.
Train: 2018-07-31T09:30:04.504272: step 7522, loss 0.621208.
Train: 2018-07-31T09:30:04.676083: step 7523, loss 0.550058.
Train: 2018-07-31T09:30:04.847947: step 7524, loss 0.556281.
Train: 2018-07-31T09:30:05.004131: step 7525, loss 0.555812.
Train: 2018-07-31T09:30:05.160374: step 7526, loss 0.579611.
Train: 2018-07-31T09:30:05.332208: step 7527, loss 0.622095.
Train: 2018-07-31T09:30:05.488422: step 7528, loss 0.572599.
Train: 2018-07-31T09:30:05.644630: step 7529, loss 0.604303.
Train: 2018-07-31T09:30:05.800819: step 7530, loss 0.653124.
Test: 2018-07-31T09:30:06.050793: step 7530, loss 0.565074.
Train: 2018-07-31T09:30:06.207003: step 7531, loss 0.619579.
Train: 2018-07-31T09:30:06.378838: step 7532, loss 0.571092.
Train: 2018-07-31T09:30:06.550676: step 7533, loss 0.545182.
Train: 2018-07-31T09:30:06.706881: step 7534, loss 0.669121.
Train: 2018-07-31T09:30:06.878722: step 7535, loss 0.601367.
Train: 2018-07-31T09:30:07.034906: step 7536, loss 0.576671.
Train: 2018-07-31T09:30:07.206740: step 7537, loss 0.622077.
Train: 2018-07-31T09:30:07.378599: step 7538, loss 0.647916.
Train: 2018-07-31T09:30:07.550442: step 7539, loss 0.552187.
Train: 2018-07-31T09:30:07.722274: step 7540, loss 0.57149.
Test: 2018-07-31T09:30:07.956566: step 7540, loss 0.568511.
Train: 2018-07-31T09:30:08.112810: step 7541, loss 0.523381.
Train: 2018-07-31T09:30:08.284645: step 7542, loss 0.577635.
Train: 2018-07-31T09:30:08.456447: step 7543, loss 0.659063.
Train: 2018-07-31T09:30:08.628311: step 7544, loss 0.597725.
Train: 2018-07-31T09:30:08.784494: step 7545, loss 0.544635.
Train: 2018-07-31T09:30:08.956359: step 7546, loss 0.616328.
Train: 2018-07-31T09:30:09.112573: step 7547, loss 0.555977.
Train: 2018-07-31T09:30:09.284378: step 7548, loss 0.579317.
Train: 2018-07-31T09:30:09.456243: step 7549, loss 0.566961.
Train: 2018-07-31T09:30:09.612456: step 7550, loss 0.546672.
Test: 2018-07-31T09:30:09.862398: step 7550, loss 0.583866.
Train: 2018-07-31T09:30:10.034202: step 7551, loss 0.564153.
Train: 2018-07-31T09:30:10.190441: step 7552, loss 0.581302.
Train: 2018-07-31T09:30:10.362280: step 7553, loss 0.550094.
Train: 2018-07-31T09:30:10.518488: step 7554, loss 0.547093.
Train: 2018-07-31T09:30:10.690329: step 7555, loss 0.654635.
Train: 2018-07-31T09:30:10.862165: step 7556, loss 0.602473.
Train: 2018-07-31T09:30:11.034000: step 7557, loss 0.555378.
Train: 2018-07-31T09:30:11.190211: step 7558, loss 0.571378.
Train: 2018-07-31T09:30:11.362016: step 7559, loss 0.60622.
Train: 2018-07-31T09:30:11.518260: step 7560, loss 0.611784.
Test: 2018-07-31T09:30:11.768173: step 7560, loss 0.578189.
Train: 2018-07-31T09:30:11.924409: step 7561, loss 0.576265.
Train: 2018-07-31T09:30:12.096250: step 7562, loss 0.557197.
Train: 2018-07-31T09:30:12.268087: step 7563, loss 0.614306.
Train: 2018-07-31T09:30:12.424300: step 7564, loss 0.589574.
Train: 2018-07-31T09:30:12.580511: step 7565, loss 0.61414.
Train: 2018-07-31T09:30:12.752315: step 7566, loss 0.543911.
Train: 2018-07-31T09:30:12.908562: step 7567, loss 0.557519.
Train: 2018-07-31T09:30:13.080364: step 7568, loss 0.58563.
Train: 2018-07-31T09:30:13.252229: step 7569, loss 0.603702.
Train: 2018-07-31T09:30:13.408412: step 7570, loss 0.589402.
Test: 2018-07-31T09:30:13.658353: step 7570, loss 0.573793.
Train: 2018-07-31T09:30:13.814567: step 7571, loss 0.610626.
Train: 2018-07-31T09:30:13.986401: step 7572, loss 0.636702.
Train: 2018-07-31T09:30:14.142639: step 7573, loss 0.627611.
Train: 2018-07-31T09:30:14.314480: step 7574, loss 0.602679.
Train: 2018-07-31T09:30:14.486285: step 7575, loss 0.607249.
Train: 2018-07-31T09:30:14.658120: step 7576, loss 0.598454.
Train: 2018-07-31T09:30:14.814363: step 7577, loss 0.605112.
Train: 2018-07-31T09:30:14.986168: step 7578, loss 0.596218.
Train: 2018-07-31T09:30:15.158035: step 7579, loss 0.575944.
Train: 2018-07-31T09:30:15.314215: step 7580, loss 0.511995.
Test: 2018-07-31T09:30:15.548570: step 7580, loss 0.573122.
Train: 2018-07-31T09:30:15.720396: step 7581, loss 0.564156.
Train: 2018-07-31T09:30:15.876618: step 7582, loss 0.566551.
Train: 2018-07-31T09:30:16.048419: step 7583, loss 0.665465.
Train: 2018-07-31T09:30:16.220254: step 7584, loss 0.55169.
Train: 2018-07-31T09:30:16.392119: step 7585, loss 0.602829.
Train: 2018-07-31T09:30:16.548334: step 7586, loss 0.652586.
Train: 2018-07-31T09:30:16.720162: step 7587, loss 0.539084.
Train: 2018-07-31T09:30:16.876349: step 7588, loss 0.570982.
Train: 2018-07-31T09:30:17.048186: step 7589, loss 0.713916.
Train: 2018-07-31T09:30:17.220020: step 7590, loss 0.628093.
Test: 2018-07-31T09:30:17.454366: step 7590, loss 0.569784.
Train: 2018-07-31T09:30:17.626204: step 7591, loss 0.591406.
Train: 2018-07-31T09:30:17.782415: step 7592, loss 0.541442.
Train: 2018-07-31T09:30:17.954254: step 7593, loss 0.658651.
Train: 2018-07-31T09:30:18.126090: step 7594, loss 0.525234.
Train: 2018-07-31T09:30:18.293315: step 7595, loss 0.598755.
Train: 2018-07-31T09:30:18.449528: step 7596, loss 0.534924.
Train: 2018-07-31T09:30:18.621364: step 7597, loss 0.588999.
Train: 2018-07-31T09:30:18.777577: step 7598, loss 0.58326.
Train: 2018-07-31T09:30:18.933791: step 7599, loss 0.540754.
Train: 2018-07-31T09:30:19.105625: step 7600, loss 0.646956.
Test: 2018-07-31T09:30:19.339947: step 7600, loss 0.567119.
Train: 2018-07-31T09:30:20.105391: step 7601, loss 0.482268.
Train: 2018-07-31T09:30:20.277257: step 7602, loss 0.547494.
Train: 2018-07-31T09:30:20.449094: step 7603, loss 0.581075.
Train: 2018-07-31T09:30:20.620895: step 7604, loss 0.62231.
Train: 2018-07-31T09:30:20.777139: step 7605, loss 0.554096.
Train: 2018-07-31T09:30:20.933356: step 7606, loss 0.612707.
Train: 2018-07-31T09:30:21.105187: step 7607, loss 0.544621.
Train: 2018-07-31T09:30:21.261401: step 7608, loss 0.604959.
Train: 2018-07-31T09:30:21.433236: step 7609, loss 0.533346.
Train: 2018-07-31T09:30:21.589449: step 7610, loss 0.585886.
Test: 2018-07-31T09:30:21.823773: step 7610, loss 0.563506.
Train: 2018-07-31T09:30:21.995573: step 7611, loss 0.578879.
Train: 2018-07-31T09:30:22.167433: step 7612, loss 0.595226.
Train: 2018-07-31T09:30:22.323622: step 7613, loss 0.525851.
Train: 2018-07-31T09:30:22.526729: step 7614, loss 0.597884.
Train: 2018-07-31T09:30:22.682937: step 7615, loss 0.590329.
Train: 2018-07-31T09:30:22.839126: step 7616, loss 0.517814.
Train: 2018-07-31T09:30:23.010991: step 7617, loss 0.633437.
Train: 2018-07-31T09:30:23.182829: step 7618, loss 0.58667.
Train: 2018-07-31T09:30:23.339010: step 7619, loss 0.637091.
Train: 2018-07-31T09:30:23.495253: step 7620, loss 0.545468.
Test: 2018-07-31T09:30:23.745189: step 7620, loss 0.562359.
Train: 2018-07-31T09:30:23.901413: step 7621, loss 0.54386.
Train: 2018-07-31T09:30:24.073252: step 7622, loss 0.610049.
Train: 2018-07-31T09:30:24.229428: step 7623, loss 0.592385.
Train: 2018-07-31T09:30:24.385664: step 7624, loss 0.566795.
Train: 2018-07-31T09:30:24.541877: step 7625, loss 0.602432.
Train: 2018-07-31T09:30:24.713687: step 7626, loss 0.581566.
Train: 2018-07-31T09:30:24.885547: step 7627, loss 0.559998.
Train: 2018-07-31T09:30:25.041736: step 7628, loss 0.552114.
Train: 2018-07-31T09:30:25.213601: step 7629, loss 0.545079.
Train: 2018-07-31T09:30:25.369808: step 7630, loss 0.633216.
Test: 2018-07-31T09:30:25.604105: step 7630, loss 0.561837.
Train: 2018-07-31T09:30:25.775939: step 7631, loss 0.615369.
Train: 2018-07-31T09:30:25.932152: step 7632, loss 0.618246.
Train: 2018-07-31T09:30:26.103987: step 7633, loss 0.625407.
Train: 2018-07-31T09:30:26.260201: step 7634, loss 0.534576.
Train: 2018-07-31T09:30:26.432068: step 7635, loss 0.618206.
Train: 2018-07-31T09:30:26.588248: step 7636, loss 0.593144.
Train: 2018-07-31T09:30:26.744486: step 7637, loss 0.538912.
Train: 2018-07-31T09:30:26.900705: step 7638, loss 0.538009.
Train: 2018-07-31T09:30:27.072511: step 7639, loss 0.562629.
Train: 2018-07-31T09:30:27.228753: step 7640, loss 0.559575.
Test: 2018-07-31T09:30:27.478673: step 7640, loss 0.561003.
Train: 2018-07-31T09:30:27.634909: step 7641, loss 0.548636.
Train: 2018-07-31T09:30:27.791091: step 7642, loss 0.581024.
Train: 2018-07-31T09:30:27.962956: step 7643, loss 0.590962.
Train: 2018-07-31T09:30:28.134791: step 7644, loss 0.556289.
Train: 2018-07-31T09:30:28.290975: step 7645, loss 0.543777.
Train: 2018-07-31T09:30:28.447221: step 7646, loss 0.547547.
Train: 2018-07-31T09:30:28.619023: step 7647, loss 0.635539.
Train: 2018-07-31T09:30:28.775236: step 7648, loss 0.502934.
Train: 2018-07-31T09:30:28.947104: step 7649, loss 0.589286.
Train: 2018-07-31T09:30:29.118907: step 7650, loss 0.556343.
Test: 2018-07-31T09:30:29.353228: step 7650, loss 0.559102.
Train: 2018-07-31T09:30:29.509439: step 7651, loss 0.572389.
Train: 2018-07-31T09:30:29.665682: step 7652, loss 0.521021.
Train: 2018-07-31T09:30:29.837512: step 7653, loss 0.580364.
Train: 2018-07-31T09:30:29.993731: step 7654, loss 0.633495.
Train: 2018-07-31T09:30:30.165565: step 7655, loss 0.59744.
Train: 2018-07-31T09:30:30.321782: step 7656, loss 0.51062.
Train: 2018-07-31T09:30:30.477962: step 7657, loss 0.578801.
Train: 2018-07-31T09:30:30.634208: step 7658, loss 0.638616.
Train: 2018-07-31T09:30:30.790390: step 7659, loss 0.557502.
Train: 2018-07-31T09:30:30.962224: step 7660, loss 0.533605.
Test: 2018-07-31T09:30:31.196546: step 7660, loss 0.558269.
Train: 2018-07-31T09:30:31.368378: step 7661, loss 0.644977.
Train: 2018-07-31T09:30:31.524592: step 7662, loss 0.557119.
Train: 2018-07-31T09:30:31.680830: step 7663, loss 0.553092.
Train: 2018-07-31T09:30:31.837020: step 7664, loss 0.535553.
Train: 2018-07-31T09:30:32.008884: step 7665, loss 0.579528.
Train: 2018-07-31T09:30:32.165068: step 7666, loss 0.548529.
Train: 2018-07-31T09:30:32.336902: step 7667, loss 0.567204.
Train: 2018-07-31T09:30:32.493142: step 7668, loss 0.599857.
Train: 2018-07-31T09:30:32.664974: step 7669, loss 0.54702.
Train: 2018-07-31T09:30:32.821197: step 7670, loss 0.584196.
Test: 2018-07-31T09:30:33.071139: step 7670, loss 0.558786.
Train: 2018-07-31T09:30:33.274182: step 7671, loss 0.545005.
Train: 2018-07-31T09:30:33.446051: step 7672, loss 0.60915.
Train: 2018-07-31T09:30:33.602231: step 7673, loss 0.520993.
Train: 2018-07-31T09:30:33.774095: step 7674, loss 0.608877.
Train: 2018-07-31T09:30:33.930307: step 7675, loss 0.555292.
Train: 2018-07-31T09:30:34.086523: step 7676, loss 0.612715.
Train: 2018-07-31T09:30:34.258357: step 7677, loss 0.522187.
Train: 2018-07-31T09:30:34.414541: step 7678, loss 0.537546.
Train: 2018-07-31T09:30:34.586405: step 7679, loss 0.619369.
Train: 2018-07-31T09:30:34.742619: step 7680, loss 0.590754.
Test: 2018-07-31T09:30:34.976941: step 7680, loss 0.557696.
Train: 2018-07-31T09:30:35.148743: step 7681, loss 0.558378.
Train: 2018-07-31T09:30:35.320584: step 7682, loss 0.607508.
Train: 2018-07-31T09:30:35.476817: step 7683, loss 0.63894.
Train: 2018-07-31T09:30:35.633035: step 7684, loss 0.638817.
Train: 2018-07-31T09:30:35.789252: step 7685, loss 0.581051.
Train: 2018-07-31T09:30:35.961079: step 7686, loss 0.620647.
Train: 2018-07-31T09:30:36.117297: step 7687, loss 0.538729.
Train: 2018-07-31T09:30:36.289102: step 7688, loss 0.56318.
Train: 2018-07-31T09:30:36.445315: step 7689, loss 0.600441.
Train: 2018-07-31T09:30:36.601528: step 7690, loss 0.562768.
Test: 2018-07-31T09:30:36.851470: step 7690, loss 0.559597.
Train: 2018-07-31T09:30:37.007683: step 7691, loss 0.566981.
Train: 2018-07-31T09:30:37.179548: step 7692, loss 0.566743.
Train: 2018-07-31T09:30:37.335757: step 7693, loss 0.568278.
Train: 2018-07-31T09:30:37.491945: step 7694, loss 0.527056.
Train: 2018-07-31T09:30:37.663804: step 7695, loss 0.527614.
Train: 2018-07-31T09:30:37.820026: step 7696, loss 0.524037.
Train: 2018-07-31T09:30:37.991858: step 7697, loss 0.530383.
Train: 2018-07-31T09:30:38.148066: step 7698, loss 0.511593.
Train: 2018-07-31T09:30:38.319877: step 7699, loss 0.561098.
Train: 2018-07-31T09:30:38.476114: step 7700, loss 0.59723.
Test: 2018-07-31T09:30:38.726032: step 7700, loss 0.556671.
Train: 2018-07-31T09:30:39.444646: step 7701, loss 0.677172.
Train: 2018-07-31T09:30:39.616447: step 7702, loss 0.537321.
Train: 2018-07-31T09:30:39.772660: step 7703, loss 0.571504.
Train: 2018-07-31T09:30:39.944496: step 7704, loss 0.501489.
Train: 2018-07-31T09:30:40.100741: step 7705, loss 0.57964.
Train: 2018-07-31T09:30:40.256953: step 7706, loss 0.597338.
Train: 2018-07-31T09:30:40.428788: step 7707, loss 0.588415.
Train: 2018-07-31T09:30:40.585001: step 7708, loss 0.596323.
Train: 2018-07-31T09:30:40.756805: step 7709, loss 0.6577.
Train: 2018-07-31T09:30:40.913020: step 7710, loss 0.597739.
Test: 2018-07-31T09:30:41.162960: step 7710, loss 0.555833.
Train: 2018-07-31T09:30:41.319174: step 7711, loss 0.516596.
Train: 2018-07-31T09:30:41.491008: step 7712, loss 0.542723.
Train: 2018-07-31T09:30:41.647221: step 7713, loss 0.614579.
Train: 2018-07-31T09:30:41.803435: step 7714, loss 0.579138.
Train: 2018-07-31T09:30:41.959680: step 7715, loss 0.57809.
Train: 2018-07-31T09:30:42.131513: step 7716, loss 0.588757.
Train: 2018-07-31T09:30:42.287727: step 7717, loss 0.502645.
Train: 2018-07-31T09:30:42.459531: step 7718, loss 0.604878.
Train: 2018-07-31T09:30:42.615775: step 7719, loss 0.639055.
Train: 2018-07-31T09:30:42.771989: step 7720, loss 0.587293.
Test: 2018-07-31T09:30:43.006311: step 7720, loss 0.555488.
Train: 2018-07-31T09:30:43.162516: step 7721, loss 0.58695.
Train: 2018-07-31T09:30:43.334356: step 7722, loss 0.571031.
Train: 2018-07-31T09:30:43.490541: step 7723, loss 0.543263.
Train: 2018-07-31T09:30:43.662405: step 7724, loss 0.517467.
Train: 2018-07-31T09:30:43.818622: step 7725, loss 0.604029.
Train: 2018-07-31T09:30:43.990453: step 7726, loss 0.526575.
Train: 2018-07-31T09:30:44.146661: step 7727, loss 0.611231.
Train: 2018-07-31T09:30:44.302883: step 7728, loss 0.52662.
Train: 2018-07-31T09:30:44.474684: step 7729, loss 0.543.
Train: 2018-07-31T09:30:44.630929: step 7730, loss 0.475395.
Test: 2018-07-31T09:30:44.865218: step 7730, loss 0.555023.
Train: 2018-07-31T09:30:45.037083: step 7731, loss 0.629838.
Train: 2018-07-31T09:30:45.193300: step 7732, loss 0.571201.
Train: 2018-07-31T09:30:45.365126: step 7733, loss 0.534486.
Train: 2018-07-31T09:30:45.521345: step 7734, loss 0.510826.
Train: 2018-07-31T09:30:45.693183: step 7735, loss 0.492045.
Train: 2018-07-31T09:30:45.865014: step 7736, loss 0.612902.
Train: 2018-07-31T09:30:46.021228: step 7737, loss 0.603631.
Train: 2018-07-31T09:30:46.193065: step 7738, loss 0.543654.
Train: 2018-07-31T09:30:46.349246: step 7739, loss 0.595294.
Train: 2018-07-31T09:30:46.521111: step 7740, loss 0.509133.
Test: 2018-07-31T09:30:46.755401: step 7740, loss 0.554208.
Train: 2018-07-31T09:30:46.927236: step 7741, loss 0.587184.
Train: 2018-07-31T09:30:47.083449: step 7742, loss 0.586608.
Train: 2018-07-31T09:30:47.255284: step 7743, loss 0.549924.
Train: 2018-07-31T09:30:47.427149: step 7744, loss 0.604153.
Train: 2018-07-31T09:30:47.583333: step 7745, loss 0.613937.
Train: 2018-07-31T09:30:47.755167: step 7746, loss 0.49846.
Train: 2018-07-31T09:30:47.911411: step 7747, loss 0.542326.
Train: 2018-07-31T09:30:48.067627: step 7748, loss 0.550024.
Train: 2018-07-31T09:30:48.239458: step 7749, loss 0.623281.
Train: 2018-07-31T09:30:48.395642: step 7750, loss 0.541641.
Test: 2018-07-31T09:30:48.645583: step 7750, loss 0.553812.
Train: 2018-07-31T09:30:48.801830: step 7751, loss 0.515833.
Train: 2018-07-31T09:30:48.973632: step 7752, loss 0.569407.
Train: 2018-07-31T09:30:49.129844: step 7753, loss 0.604042.
Train: 2018-07-31T09:30:49.286082: step 7754, loss 0.55035.
Train: 2018-07-31T09:30:49.457893: step 7755, loss 0.586027.
Train: 2018-07-31T09:30:49.614107: step 7756, loss 0.567256.
Train: 2018-07-31T09:30:49.785941: step 7757, loss 0.550107.
Train: 2018-07-31T09:30:49.942155: step 7758, loss 0.629808.
Train: 2018-07-31T09:30:50.098368: step 7759, loss 0.578885.
Train: 2018-07-31T09:30:50.254581: step 7760, loss 0.57813.
Test: 2018-07-31T09:30:50.488903: step 7760, loss 0.553568.
Train: 2018-07-31T09:30:50.660737: step 7761, loss 0.525205.
Train: 2018-07-31T09:30:50.816983: step 7762, loss 0.594897.
Train: 2018-07-31T09:30:50.988814: step 7763, loss 0.533492.
Train: 2018-07-31T09:30:51.144998: step 7764, loss 0.524405.
Train: 2018-07-31T09:30:51.316833: step 7765, loss 0.550731.
Train: 2018-07-31T09:30:51.473046: step 7766, loss 0.5249.
Train: 2018-07-31T09:30:51.644882: step 7767, loss 0.586487.
Train: 2018-07-31T09:30:51.801094: step 7768, loss 0.568349.
Train: 2018-07-31T09:30:51.972929: step 7769, loss 0.603499.
Train: 2018-07-31T09:30:52.129172: step 7770, loss 0.506274.
Test: 2018-07-31T09:30:52.379084: step 7770, loss 0.553234.
Train: 2018-07-31T09:30:52.535327: step 7771, loss 0.576407.
Train: 2018-07-31T09:30:52.707131: step 7772, loss 0.558053.
Train: 2018-07-31T09:30:52.863371: step 7773, loss 0.593976.
Train: 2018-07-31T09:30:53.019589: step 7774, loss 0.567268.
Train: 2018-07-31T09:30:53.191394: step 7775, loss 0.532681.
Train: 2018-07-31T09:30:53.363253: step 7776, loss 0.523736.
Train: 2018-07-31T09:30:53.519441: step 7777, loss 0.698819.
Train: 2018-07-31T09:30:53.691277: step 7778, loss 0.541878.
Train: 2018-07-31T09:30:53.847514: step 7779, loss 0.611099.
Train: 2018-07-31T09:30:54.019324: step 7780, loss 0.499213.
Test: 2018-07-31T09:30:54.253670: step 7780, loss 0.552994.
Train: 2018-07-31T09:30:54.425479: step 7781, loss 0.549904.
Train: 2018-07-31T09:30:54.581723: step 7782, loss 0.515185.
Train: 2018-07-31T09:30:54.753559: step 7783, loss 0.523921.
Train: 2018-07-31T09:30:54.909740: step 7784, loss 0.481071.
Train: 2018-07-31T09:30:55.065955: step 7785, loss 0.602377.
Train: 2018-07-31T09:30:55.237790: step 7786, loss 0.60295.
Train: 2018-07-31T09:30:55.394003: step 7787, loss 0.59482.
Train: 2018-07-31T09:30:55.565870: step 7788, loss 0.549811.
Train: 2018-07-31T09:30:55.737673: step 7789, loss 0.60301.
Train: 2018-07-31T09:30:55.909507: step 7790, loss 0.584358.
Test: 2018-07-31T09:30:56.143858: step 7790, loss 0.552611.
Train: 2018-07-31T09:30:56.315695: step 7791, loss 0.576898.
Train: 2018-07-31T09:30:56.471900: step 7792, loss 0.540203.
Train: 2018-07-31T09:30:56.643709: step 7793, loss 0.646283.
Train: 2018-07-31T09:30:56.815578: step 7794, loss 0.558673.
Train: 2018-07-31T09:30:56.971759: step 7795, loss 0.567255.
Train: 2018-07-31T09:30:57.128002: step 7796, loss 0.575435.
Train: 2018-07-31T09:30:57.299806: step 7797, loss 0.6106.
Train: 2018-07-31T09:30:57.456020: step 7798, loss 0.584408.
Train: 2018-07-31T09:30:57.612234: step 7799, loss 0.567683.
Train: 2018-07-31T09:30:57.784069: step 7800, loss 0.637035.
Test: 2018-07-31T09:30:58.018419: step 7800, loss 0.552713.
Train: 2018-07-31T09:30:58.768243: step 7801, loss 0.549964.
Train: 2018-07-31T09:30:58.940047: step 7802, loss 0.575449.
Train: 2018-07-31T09:30:59.096261: step 7803, loss 0.584321.
Train: 2018-07-31T09:30:59.252474: step 7804, loss 0.557702.
Train: 2018-07-31T09:30:59.408718: step 7805, loss 0.56036.
Train: 2018-07-31T09:30:59.580552: step 7806, loss 0.55931.
Train: 2018-07-31T09:30:59.736766: step 7807, loss 0.651383.
Train: 2018-07-31T09:30:59.892974: step 7808, loss 0.533383.
Train: 2018-07-31T09:31:00.064783: step 7809, loss 0.575603.
Train: 2018-07-31T09:31:00.220997: step 7810, loss 0.624959.
Test: 2018-07-31T09:31:00.455319: step 7810, loss 0.553093.
Train: 2018-07-31T09:31:00.642774: step 7811, loss 0.558262.
Train: 2018-07-31T09:31:00.799011: step 7812, loss 0.558345.
Train: 2018-07-31T09:31:00.970852: step 7813, loss 0.567915.
Train: 2018-07-31T09:31:01.142657: step 7814, loss 0.526786.
Train: 2018-07-31T09:31:01.298870: step 7815, loss 0.575494.
Train: 2018-07-31T09:31:01.455084: step 7816, loss 0.592827.
Train: 2018-07-31T09:31:01.626918: step 7817, loss 0.550858.
Train: 2018-07-31T09:31:01.783156: step 7818, loss 0.583251.
Train: 2018-07-31T09:31:01.939344: step 7819, loss 0.641135.
Train: 2018-07-31T09:31:02.111205: step 7820, loss 0.615862.
Test: 2018-07-31T09:31:02.345500: step 7820, loss 0.553068.
Train: 2018-07-31T09:31:02.517335: step 7821, loss 0.477075.
Train: 2018-07-31T09:31:02.673547: step 7822, loss 0.566974.
Train: 2018-07-31T09:31:02.829793: step 7823, loss 0.53365.
Train: 2018-07-31T09:31:03.001597: step 7824, loss 0.591481.
Train: 2018-07-31T09:31:03.173430: step 7825, loss 0.50012.
Train: 2018-07-31T09:31:03.329690: step 7826, loss 0.607453.
Train: 2018-07-31T09:31:03.501509: step 7827, loss 0.566297.
Train: 2018-07-31T09:31:03.657692: step 7828, loss 0.58334.
Train: 2018-07-31T09:31:03.813939: step 7829, loss 0.566733.
Train: 2018-07-31T09:31:03.985740: step 7830, loss 0.541655.
Test: 2018-07-31T09:31:04.220087: step 7830, loss 0.552563.
Train: 2018-07-31T09:31:04.391926: step 7831, loss 0.500277.
Train: 2018-07-31T09:31:04.563763: step 7832, loss 0.56711.
Train: 2018-07-31T09:31:04.719969: step 7833, loss 0.549221.
Train: 2018-07-31T09:31:04.891779: step 7834, loss 0.515688.
Train: 2018-07-31T09:31:05.063614: step 7835, loss 0.541461.
Train: 2018-07-31T09:31:05.219857: step 7836, loss 0.608475.
Train: 2018-07-31T09:31:05.391661: step 7837, loss 0.584237.
Train: 2018-07-31T09:31:05.563521: step 7838, loss 0.635302.
Train: 2018-07-31T09:31:05.719710: step 7839, loss 0.54013.
Train: 2018-07-31T09:31:05.891573: step 7840, loss 0.583044.
Test: 2018-07-31T09:31:06.110274: step 7840, loss 0.551873.
Train: 2018-07-31T09:31:06.282108: step 7841, loss 0.531713.
Train: 2018-07-31T09:31:06.438325: step 7842, loss 0.497373.
Train: 2018-07-31T09:31:06.610157: step 7843, loss 0.539754.
Train: 2018-07-31T09:31:06.766340: step 7844, loss 0.566233.
Train: 2018-07-31T09:31:06.938208: step 7845, loss 0.566267.
Train: 2018-07-31T09:31:07.094387: step 7846, loss 0.549177.
Train: 2018-07-31T09:31:07.266222: step 7847, loss 0.592601.
Train: 2018-07-31T09:31:07.422469: step 7848, loss 0.618681.
Train: 2018-07-31T09:31:07.594296: step 7849, loss 0.522071.
Train: 2018-07-31T09:31:07.766135: step 7850, loss 0.592399.
Test: 2018-07-31T09:31:08.000457: step 7850, loss 0.551487.
Train: 2018-07-31T09:31:08.156638: step 7851, loss 0.574189.
Train: 2018-07-31T09:31:08.312852: step 7852, loss 0.565792.
Train: 2018-07-31T09:31:08.484688: step 7853, loss 0.557342.
Train: 2018-07-31T09:31:08.640927: step 7854, loss 0.548489.
Train: 2018-07-31T09:31:08.812735: step 7855, loss 0.496408.
Train: 2018-07-31T09:31:08.984600: step 7856, loss 0.557358.
Train: 2018-07-31T09:31:09.140814: step 7857, loss 0.513147.
Train: 2018-07-31T09:31:09.312652: step 7858, loss 0.619627.
Train: 2018-07-31T09:31:09.484454: step 7859, loss 0.549105.
Train: 2018-07-31T09:31:09.640697: step 7860, loss 0.58418.
Test: 2018-07-31T09:31:09.874988: step 7860, loss 0.551267.
Train: 2018-07-31T09:31:10.046854: step 7861, loss 0.51262.
Train: 2018-07-31T09:31:10.203035: step 7862, loss 0.548323.
Train: 2018-07-31T09:31:10.374870: step 7863, loss 0.539089.
Train: 2018-07-31T09:31:10.531083: step 7864, loss 0.530906.
Train: 2018-07-31T09:31:10.687297: step 7865, loss 0.547983.
Train: 2018-07-31T09:31:10.859131: step 7866, loss 0.584059.
Train: 2018-07-31T09:31:11.015369: step 7867, loss 0.557802.
Train: 2018-07-31T09:31:11.187213: step 7868, loss 0.530621.
Train: 2018-07-31T09:31:11.359014: step 7869, loss 0.673731.
Train: 2018-07-31T09:31:11.515258: step 7870, loss 0.574829.
Test: 2018-07-31T09:31:11.749547: step 7870, loss 0.551053.
Train: 2018-07-31T09:31:11.921383: step 7871, loss 0.637693.
Train: 2018-07-31T09:31:12.093218: step 7872, loss 0.592885.
Train: 2018-07-31T09:31:12.249456: step 7873, loss 0.575305.
Train: 2018-07-31T09:31:12.405674: step 7874, loss 0.539353.
Train: 2018-07-31T09:31:12.577503: step 7875, loss 0.522189.
Train: 2018-07-31T09:31:12.733692: step 7876, loss 0.609588.
Train: 2018-07-31T09:31:12.889936: step 7877, loss 0.504195.
Train: 2018-07-31T09:31:13.061740: step 7878, loss 0.574139.
Train: 2018-07-31T09:31:13.233575: step 7879, loss 0.547997.
Train: 2018-07-31T09:31:13.389789: step 7880, loss 0.566049.
Test: 2018-07-31T09:31:13.624140: step 7880, loss 0.551121.
Train: 2018-07-31T09:31:13.795968: step 7881, loss 0.547894.
Train: 2018-07-31T09:31:13.952187: step 7882, loss 0.652759.
Train: 2018-07-31T09:31:14.123991: step 7883, loss 0.513553.
Train: 2018-07-31T09:31:14.280243: step 7884, loss 0.608954.
Train: 2018-07-31T09:31:14.452075: step 7885, loss 0.539651.
Train: 2018-07-31T09:31:14.623899: step 7886, loss 0.539022.
Train: 2018-07-31T09:31:14.780087: step 7887, loss 0.660285.
Train: 2018-07-31T09:31:14.951923: step 7888, loss 0.616817.
Train: 2018-07-31T09:31:15.123788: step 7889, loss 0.634386.
Train: 2018-07-31T09:31:15.295625: step 7890, loss 0.540369.
Test: 2018-07-31T09:31:15.514323: step 7890, loss 0.551367.
Train: 2018-07-31T09:31:15.686150: step 7891, loss 0.615911.
Train: 2018-07-31T09:31:15.857960: step 7892, loss 0.523852.
Train: 2018-07-31T09:31:16.029794: step 7893, loss 0.566054.
Train: 2018-07-31T09:31:16.186009: step 7894, loss 0.52374.
Train: 2018-07-31T09:31:16.357868: step 7895, loss 0.607068.
Train: 2018-07-31T09:31:16.514087: step 7896, loss 0.624059.
Train: 2018-07-31T09:31:16.685891: step 7897, loss 0.531939.
Train: 2018-07-31T09:31:16.842129: step 7898, loss 0.515374.
Train: 2018-07-31T09:31:17.013939: step 7899, loss 0.59015.
Train: 2018-07-31T09:31:17.170178: step 7900, loss 0.573757.
Test: 2018-07-31T09:31:17.404474: step 7900, loss 0.551723.
Train: 2018-07-31T09:31:18.169919: step 7901, loss 0.591.
Train: 2018-07-31T09:31:18.357376: step 7902, loss 0.681523.
Train: 2018-07-31T09:31:18.513615: step 7903, loss 0.56584.
Train: 2018-07-31T09:31:18.685457: step 7904, loss 0.500408.
Train: 2018-07-31T09:31:18.841638: step 7905, loss 0.532499.
Train: 2018-07-31T09:31:19.013473: step 7906, loss 0.566133.
Train: 2018-07-31T09:31:19.169685: step 7907, loss 0.581823.
Train: 2018-07-31T09:31:19.341520: step 7908, loss 0.574417.
Train: 2018-07-31T09:31:19.513354: step 7909, loss 0.589918.
Train: 2018-07-31T09:31:19.669569: step 7910, loss 0.565586.
Test: 2018-07-31T09:31:19.919511: step 7910, loss 0.551737.
Train: 2018-07-31T09:31:20.075723: step 7911, loss 0.500033.
Train: 2018-07-31T09:31:20.247588: step 7912, loss 0.590377.
Train: 2018-07-31T09:31:20.403801: step 7913, loss 0.532332.
Train: 2018-07-31T09:31:20.575606: step 7914, loss 0.549307.
Train: 2018-07-31T09:31:20.747470: step 7915, loss 0.565426.
Train: 2018-07-31T09:31:20.903687: step 7916, loss 0.59043.
Train: 2018-07-31T09:31:21.075488: step 7917, loss 0.473518.
Train: 2018-07-31T09:31:21.247348: step 7918, loss 0.607361.
Train: 2018-07-31T09:31:21.403567: step 7919, loss 0.540597.
Train: 2018-07-31T09:31:21.559775: step 7920, loss 0.539942.
Test: 2018-07-31T09:31:21.809691: step 7920, loss 0.551016.
Train: 2018-07-31T09:31:21.965904: step 7921, loss 0.599426.
Train: 2018-07-31T09:31:22.122118: step 7922, loss 0.547767.
Train: 2018-07-31T09:31:22.293953: step 7923, loss 0.547785.
Train: 2018-07-31T09:31:22.450201: step 7924, loss 0.624926.
Train: 2018-07-31T09:31:22.622001: step 7925, loss 0.514142.
Train: 2018-07-31T09:31:22.778245: step 7926, loss 0.565486.
Train: 2018-07-31T09:31:22.934462: step 7927, loss 0.573883.
Train: 2018-07-31T09:31:23.090672: step 7928, loss 0.591304.
Train: 2018-07-31T09:31:23.262506: step 7929, loss 0.504524.
Train: 2018-07-31T09:31:23.418723: step 7930, loss 0.60911.
Test: 2018-07-31T09:31:23.653042: step 7930, loss 0.550603.
Train: 2018-07-31T09:31:23.824845: step 7931, loss 0.530534.
Train: 2018-07-31T09:31:23.981089: step 7932, loss 0.556606.
Train: 2018-07-31T09:31:24.137272: step 7933, loss 0.574408.
Train: 2018-07-31T09:31:24.309139: step 7934, loss 0.582223.
Train: 2018-07-31T09:31:24.480965: step 7935, loss 0.66029.
Train: 2018-07-31T09:31:24.637179: step 7936, loss 0.600052.
Train: 2018-07-31T09:31:24.808989: step 7937, loss 0.616899.
Train: 2018-07-31T09:31:24.965204: step 7938, loss 0.521758.
Train: 2018-07-31T09:31:25.137062: step 7939, loss 0.591663.
Train: 2018-07-31T09:31:25.293251: step 7940, loss 0.54733.
Test: 2018-07-31T09:31:25.543226: step 7940, loss 0.550652.
Train: 2018-07-31T09:31:25.699436: step 7941, loss 0.556613.
Train: 2018-07-31T09:31:25.871265: step 7942, loss 0.574031.
Train: 2018-07-31T09:31:26.027485: step 7943, loss 0.531248.
Train: 2018-07-31T09:31:26.199323: step 7944, loss 0.565457.
Train: 2018-07-31T09:31:26.371148: step 7945, loss 0.547818.
Train: 2018-07-31T09:31:26.527338: step 7946, loss 0.581462.
Train: 2018-07-31T09:31:26.699171: step 7947, loss 0.564867.
Train: 2018-07-31T09:31:26.855385: step 7948, loss 0.496663.
Train: 2018-07-31T09:31:27.027220: step 7949, loss 0.555969.
Train: 2018-07-31T09:31:27.199085: step 7950, loss 0.513568.
Test: 2018-07-31T09:31:27.433408: step 7950, loss 0.550553.
Train: 2018-07-31T09:31:27.605234: step 7951, loss 0.632861.
Train: 2018-07-31T09:31:27.761422: step 7952, loss 0.548209.
Train: 2018-07-31T09:31:27.917637: step 7953, loss 0.598981.
Train: 2018-07-31T09:31:28.089501: step 7954, loss 0.530345.
Train: 2018-07-31T09:31:28.245714: step 7955, loss 0.565294.
Train: 2018-07-31T09:31:28.417519: step 7956, loss 0.642184.
Train: 2018-07-31T09:31:28.589380: step 7957, loss 0.487401.
Train: 2018-07-31T09:31:28.745598: step 7958, loss 0.651626.
Train: 2018-07-31T09:31:28.901781: step 7959, loss 0.616557.
Train: 2018-07-31T09:31:29.073646: step 7960, loss 0.59124.
Test: 2018-07-31T09:31:29.323588: step 7960, loss 0.550509.
Train: 2018-07-31T09:31:29.479795: step 7961, loss 0.547614.
Train: 2018-07-31T09:31:29.651636: step 7962, loss 0.547708.
Train: 2018-07-31T09:31:29.823470: step 7963, loss 0.573404.
Train: 2018-07-31T09:31:29.979684: step 7964, loss 0.531125.
Train: 2018-07-31T09:31:30.151514: step 7965, loss 0.539473.
Train: 2018-07-31T09:31:30.307735: step 7966, loss 0.624548.
Train: 2018-07-31T09:31:30.463940: step 7967, loss 0.539732.
Train: 2018-07-31T09:31:30.635780: step 7968, loss 0.488291.
Train: 2018-07-31T09:31:30.791996: step 7969, loss 0.513774.
Train: 2018-07-31T09:31:30.963831: step 7970, loss 0.599073.
Test: 2018-07-31T09:31:31.198118: step 7970, loss 0.550439.
Train: 2018-07-31T09:31:31.369954: step 7971, loss 0.539275.
Train: 2018-07-31T09:31:31.541788: step 7972, loss 0.478894.
Train: 2018-07-31T09:31:31.698025: step 7973, loss 0.564976.
Train: 2018-07-31T09:31:31.869866: step 7974, loss 0.512896.
Train: 2018-07-31T09:31:32.041671: step 7975, loss 0.538694.
Train: 2018-07-31T09:31:32.197883: step 7976, loss 0.573749.
Train: 2018-07-31T09:31:32.354122: step 7977, loss 0.520923.
Train: 2018-07-31T09:31:32.510312: step 7978, loss 0.573802.
Train: 2018-07-31T09:31:32.682147: step 7979, loss 0.58239.
Train: 2018-07-31T09:31:32.838385: step 7980, loss 0.538532.
Test: 2018-07-31T09:31:33.088300: step 7980, loss 0.549951.
Train: 2018-07-31T09:31:33.244538: step 7981, loss 0.57386.
Train: 2018-07-31T09:31:33.416374: step 7982, loss 0.671237.
Train: 2018-07-31T09:31:33.572592: step 7983, loss 0.546869.
Train: 2018-07-31T09:31:33.744422: step 7984, loss 0.555638.
Train: 2018-07-31T09:31:33.900640: step 7985, loss 0.564973.
Train: 2018-07-31T09:31:34.056825: step 7986, loss 0.573336.
Train: 2018-07-31T09:31:34.213067: step 7987, loss 0.564978.
Train: 2018-07-31T09:31:34.384873: step 7988, loss 0.556062.
Train: 2018-07-31T09:31:34.541118: step 7989, loss 0.582497.
Train: 2018-07-31T09:31:34.712950: step 7990, loss 0.520687.
Test: 2018-07-31T09:31:34.947243: step 7990, loss 0.549907.
Train: 2018-07-31T09:31:35.119100: step 7991, loss 0.564696.
Train: 2018-07-31T09:31:35.275319: step 7992, loss 0.564706.
Train: 2018-07-31T09:31:35.447124: step 7993, loss 0.538076.
Train: 2018-07-31T09:31:35.618958: step 7994, loss 0.511649.
Train: 2018-07-31T09:31:35.775172: step 7995, loss 0.635542.
Train: 2018-07-31T09:31:35.947037: step 7996, loss 0.564286.
Train: 2018-07-31T09:31:36.103220: step 7997, loss 0.502984.
Train: 2018-07-31T09:31:36.275054: step 7998, loss 0.582328.
Train: 2018-07-31T09:31:36.431267: step 7999, loss 0.547326.
Train: 2018-07-31T09:31:36.603102: step 8000, loss 0.626764.
Test: 2018-07-31T09:31:36.837453: step 8000, loss 0.54986.
Train: 2018-07-31T09:31:37.556005: step 8001, loss 0.564795.
Train: 2018-07-31T09:31:37.712248: step 8002, loss 0.538341.
Train: 2018-07-31T09:31:37.868431: step 8003, loss 0.545804.
Train: 2018-07-31T09:31:38.055920: step 8004, loss 0.538171.
Train: 2018-07-31T09:31:38.212100: step 8005, loss 0.467936.
Train: 2018-07-31T09:31:38.368344: step 8006, loss 0.520731.
Train: 2018-07-31T09:31:38.524557: step 8007, loss 0.512111.
Train: 2018-07-31T09:31:38.696387: step 8008, loss 0.573369.
Train: 2018-07-31T09:31:38.852605: step 8009, loss 0.565018.
Train: 2018-07-31T09:31:39.024436: step 8010, loss 0.591319.
Test: 2018-07-31T09:31:39.243110: step 8010, loss 0.549694.
Train: 2018-07-31T09:31:39.414974: step 8011, loss 0.609684.
Train: 2018-07-31T09:31:39.571157: step 8012, loss 0.520285.
Train: 2018-07-31T09:31:39.742993: step 8013, loss 0.448797.
Train: 2018-07-31T09:31:39.914857: step 8014, loss 0.511434.
Train: 2018-07-31T09:31:40.071040: step 8015, loss 0.564747.
Train: 2018-07-31T09:31:40.242875: step 8016, loss 0.546869.
Train: 2018-07-31T09:31:40.399089: step 8017, loss 0.510238.
Train: 2018-07-31T09:31:40.570924: step 8018, loss 0.582918.
Train: 2018-07-31T09:31:40.727161: step 8019, loss 0.583271.
Train: 2018-07-31T09:31:40.899002: step 8020, loss 0.546992.
Test: 2018-07-31T09:31:41.148914: step 8020, loss 0.549551.
Train: 2018-07-31T09:31:41.305160: step 8021, loss 0.564679.
Train: 2018-07-31T09:31:41.476986: step 8022, loss 0.555799.
Train: 2018-07-31T09:31:41.633205: step 8023, loss 0.628903.
Train: 2018-07-31T09:31:41.789422: step 8024, loss 0.537191.
Train: 2018-07-31T09:31:41.961223: step 8025, loss 0.555826.
Train: 2018-07-31T09:31:42.133087: step 8026, loss 0.665123.
Train: 2018-07-31T09:31:42.304916: step 8027, loss 0.564881.
Train: 2018-07-31T09:31:42.461131: step 8028, loss 0.573697.
Train: 2018-07-31T09:31:42.632970: step 8029, loss 0.582536.
Train: 2018-07-31T09:31:42.804808: step 8030, loss 0.537841.
Test: 2018-07-31T09:31:43.039130: step 8030, loss 0.549532.
Train: 2018-07-31T09:31:43.210931: step 8031, loss 0.546214.
Train: 2018-07-31T09:31:43.367174: step 8032, loss 0.591253.
Train: 2018-07-31T09:31:43.539003: step 8033, loss 0.564552.
Train: 2018-07-31T09:31:43.695192: step 8034, loss 0.626476.
Train: 2018-07-31T09:31:43.867026: step 8035, loss 0.608185.
Train: 2018-07-31T09:31:44.023240: step 8036, loss 0.705365.
Train: 2018-07-31T09:31:44.195074: step 8037, loss 0.54683.
Train: 2018-07-31T09:31:44.366940: step 8038, loss 0.599069.
Train: 2018-07-31T09:31:44.538745: step 8039, loss 0.598741.
Train: 2018-07-31T09:31:44.694988: step 8040, loss 0.513109.
Test: 2018-07-31T09:31:44.929279: step 8040, loss 0.550026.
Train: 2018-07-31T09:31:45.101137: step 8041, loss 0.598438.
Train: 2018-07-31T09:31:45.272948: step 8042, loss 0.632072.
Train: 2018-07-31T09:31:45.429194: step 8043, loss 0.564383.
Train: 2018-07-31T09:31:45.585399: step 8044, loss 0.597777.
Train: 2018-07-31T09:31:45.757209: step 8045, loss 0.514359.
Train: 2018-07-31T09:31:45.913422: step 8046, loss 0.531296.
Train: 2018-07-31T09:31:46.085288: step 8047, loss 0.564617.
Train: 2018-07-31T09:31:46.257122: step 8048, loss 0.51522.
Train: 2018-07-31T09:31:46.413304: step 8049, loss 0.588983.
Train: 2018-07-31T09:31:46.585140: step 8050, loss 0.622082.
Test: 2018-07-31T09:31:46.819460: step 8050, loss 0.550677.
Train: 2018-07-31T09:31:46.991325: step 8051, loss 0.629875.
Train: 2018-07-31T09:31:47.147541: step 8052, loss 0.588527.
Train: 2018-07-31T09:31:47.319344: step 8053, loss 0.515987.
Train: 2018-07-31T09:31:47.475556: step 8054, loss 0.540011.
Train: 2018-07-31T09:31:47.647391: step 8055, loss 0.564548.
Train: 2018-07-31T09:31:47.819259: step 8056, loss 0.56451.
Train: 2018-07-31T09:31:47.975470: step 8057, loss 0.572858.
Train: 2018-07-31T09:31:48.147274: step 8058, loss 0.531971.
Train: 2018-07-31T09:31:48.303488: step 8059, loss 0.556372.
Train: 2018-07-31T09:31:48.475348: step 8060, loss 0.613657.
Test: 2018-07-31T09:31:48.709674: step 8060, loss 0.550676.
Train: 2018-07-31T09:31:48.881508: step 8061, loss 0.589118.
Train: 2018-07-31T09:31:49.037691: step 8062, loss 0.548118.
Train: 2018-07-31T09:31:49.209550: step 8063, loss 0.498748.
Train: 2018-07-31T09:31:49.365739: step 8064, loss 0.531734.
Train: 2018-07-31T09:31:49.521952: step 8065, loss 0.572615.
Train: 2018-07-31T09:31:49.693787: step 8066, loss 0.531027.
Train: 2018-07-31T09:31:49.850031: step 8067, loss 0.530943.
Train: 2018-07-31T09:31:50.006245: step 8068, loss 0.555833.
Train: 2018-07-31T09:31:50.178048: step 8069, loss 0.589285.
Train: 2018-07-31T09:31:50.334263: step 8070, loss 0.60626.
Test: 2018-07-31T09:31:50.568582: step 8070, loss 0.550009.
Train: 2018-07-31T09:31:50.740441: step 8071, loss 0.56395.
Train: 2018-07-31T09:31:50.912252: step 8072, loss 0.538551.
Train: 2018-07-31T09:31:51.068466: step 8073, loss 0.538754.
Train: 2018-07-31T09:31:51.240299: step 8074, loss 0.555542.
Train: 2018-07-31T09:31:51.396543: step 8075, loss 0.589717.
Train: 2018-07-31T09:31:51.568373: step 8076, loss 0.598314.
Train: 2018-07-31T09:31:51.724592: step 8077, loss 0.555581.
Train: 2018-07-31T09:31:51.880775: step 8078, loss 0.504142.
Train: 2018-07-31T09:31:52.052643: step 8079, loss 0.581106.
Train: 2018-07-31T09:31:52.208847: step 8080, loss 0.529719.
Test: 2018-07-31T09:31:52.458766: step 8080, loss 0.549622.
Train: 2018-07-31T09:31:52.614978: step 8081, loss 0.529502.
Train: 2018-07-31T09:31:52.786845: step 8082, loss 0.538295.
Train: 2018-07-31T09:31:52.943062: step 8083, loss 0.607077.
Train: 2018-07-31T09:31:53.114891: step 8084, loss 0.590119.
Train: 2018-07-31T09:31:53.271107: step 8085, loss 0.555366.
Train: 2018-07-31T09:31:53.442933: step 8086, loss 0.51172.
Train: 2018-07-31T09:31:53.614770: step 8087, loss 0.598739.
Train: 2018-07-31T09:31:53.770982: step 8088, loss 0.642068.
Train: 2018-07-31T09:31:53.942793: step 8089, loss 0.520694.
Train: 2018-07-31T09:31:54.099042: step 8090, loss 0.624938.
Test: 2018-07-31T09:31:54.348973: step 8090, loss 0.54944.
Train: 2018-07-31T09:31:54.505191: step 8091, loss 0.511975.
Train: 2018-07-31T09:31:54.661398: step 8092, loss 0.52036.
Train: 2018-07-31T09:31:54.833239: step 8093, loss 0.529431.
Train: 2018-07-31T09:31:54.989452: step 8094, loss 0.511749.
Train: 2018-07-31T09:31:55.161287: step 8095, loss 0.52032.
Train: 2018-07-31T09:31:55.317470: step 8096, loss 0.564008.
Train: 2018-07-31T09:31:55.473683: step 8097, loss 0.608152.
Train: 2018-07-31T09:31:55.645557: step 8098, loss 0.616382.
Train: 2018-07-31T09:31:55.801757: step 8099, loss 0.564477.
Train: 2018-07-31T09:31:55.957945: step 8100, loss 0.485006.
Test: 2018-07-31T09:31:56.207888: step 8100, loss 0.549272.
Train: 2018-07-31T09:31:56.973362: step 8101, loss 0.555049.
Train: 2018-07-31T09:31:57.145168: step 8102, loss 0.608483.
Train: 2018-07-31T09:31:57.301414: step 8103, loss 0.59033.
Train: 2018-07-31T09:31:57.473245: step 8104, loss 0.546325.
Train: 2018-07-31T09:31:57.645075: step 8105, loss 0.528778.
Train: 2018-07-31T09:31:57.801264: step 8106, loss 0.581348.
Train: 2018-07-31T09:31:57.973099: step 8107, loss 0.607934.
Train: 2018-07-31T09:31:58.129345: step 8108, loss 0.616781.
Train: 2018-07-31T09:31:58.301146: step 8109, loss 0.458791.
Train: 2018-07-31T09:31:58.457390: step 8110, loss 0.519981.
Test: 2018-07-31T09:31:58.707302: step 8110, loss 0.549252.
Train: 2018-07-31T09:31:58.863515: step 8111, loss 0.590394.
Train: 2018-07-31T09:31:59.035350: step 8112, loss 0.52892.
Train: 2018-07-31T09:31:59.191562: step 8113, loss 0.608184.
Train: 2018-07-31T09:31:59.347800: step 8114, loss 0.555124.
Train: 2018-07-31T09:31:59.519612: step 8115, loss 0.537831.
Train: 2018-07-31T09:31:59.691447: step 8116, loss 0.572612.
Train: 2018-07-31T09:31:59.847690: step 8117, loss 0.546199.
Train: 2018-07-31T09:32:00.003872: step 8118, loss 0.572473.
Train: 2018-07-31T09:32:00.175740: step 8119, loss 0.555172.
Train: 2018-07-31T09:32:00.331951: step 8120, loss 0.519961.
Test: 2018-07-31T09:32:00.581863: step 8120, loss 0.549203.
Train: 2018-07-31T09:32:00.738100: step 8121, loss 0.511219.
Train: 2018-07-31T09:32:00.909935: step 8122, loss 0.590244.
Train: 2018-07-31T09:32:01.066123: step 8123, loss 0.546261.
Train: 2018-07-31T09:32:01.237959: step 8124, loss 0.519771.
Train: 2018-07-31T09:32:01.394172: step 8125, loss 0.660668.
Train: 2018-07-31T09:32:01.550386: step 8126, loss 0.643499.
Train: 2018-07-31T09:32:01.753463: step 8127, loss 0.528868.
Train: 2018-07-31T09:32:01.909707: step 8128, loss 0.563889.
Train: 2018-07-31T09:32:02.081511: step 8129, loss 0.581579.
Train: 2018-07-31T09:32:02.237749: step 8130, loss 0.642789.
Test: 2018-07-31T09:32:02.472046: step 8130, loss 0.549229.
Train: 2018-07-31T09:32:02.643904: step 8131, loss 0.572343.
Train: 2018-07-31T09:32:02.815714: step 8132, loss 0.598338.
Train: 2018-07-31T09:32:02.971957: step 8133, loss 0.554943.
Train: 2018-07-31T09:32:03.143795: step 8134, loss 0.511969.
Train: 2018-07-31T09:32:03.300009: step 8135, loss 0.589663.
Train: 2018-07-31T09:32:03.456214: step 8136, loss 0.538034.
Train: 2018-07-31T09:32:03.628023: step 8137, loss 0.572213.
Train: 2018-07-31T09:32:03.799859: step 8138, loss 0.529513.
Train: 2018-07-31T09:32:03.956102: step 8139, loss 0.504106.
Train: 2018-07-31T09:32:04.127932: step 8140, loss 0.563614.
Test: 2018-07-31T09:32:04.362227: step 8140, loss 0.549413.
Train: 2018-07-31T09:32:04.534062: step 8141, loss 0.529287.
Train: 2018-07-31T09:32:04.705909: step 8142, loss 0.572281.
Train: 2018-07-31T09:32:04.862141: step 8143, loss 0.55523.
Train: 2018-07-31T09:32:05.033945: step 8144, loss 0.529357.
Train: 2018-07-31T09:32:05.190158: step 8145, loss 0.589476.
Train: 2018-07-31T09:32:05.362023: step 8146, loss 0.649823.
Train: 2018-07-31T09:32:05.518236: step 8147, loss 0.606678.
Train: 2018-07-31T09:32:05.690069: step 8148, loss 0.555363.
Train: 2018-07-31T09:32:05.846254: step 8149, loss 0.589214.
Train: 2018-07-31T09:32:06.002468: step 8150, loss 0.563705.
Test: 2018-07-31T09:32:06.252439: step 8150, loss 0.549394.
Train: 2018-07-31T09:32:06.408648: step 8151, loss 0.55515.
Train: 2018-07-31T09:32:06.580483: step 8152, loss 0.512337.
Train: 2018-07-31T09:32:06.736702: step 8153, loss 0.581061.
Train: 2018-07-31T09:32:06.908530: step 8154, loss 0.636671.
Train: 2018-07-31T09:32:07.064750: step 8155, loss 0.60655.
Train: 2018-07-31T09:32:07.220933: step 8156, loss 0.546424.
Train: 2018-07-31T09:32:07.392797: step 8157, loss 0.538387.
Train: 2018-07-31T09:32:07.549014: step 8158, loss 0.580578.
Train: 2018-07-31T09:32:07.720846: step 8159, loss 0.546677.
Train: 2018-07-31T09:32:07.877053: step 8160, loss 0.487353.
Test: 2018-07-31T09:32:08.111381: step 8160, loss 0.549536.
Train: 2018-07-31T09:32:08.283208: step 8161, loss 0.495994.
Train: 2018-07-31T09:32:08.439427: step 8162, loss 0.563353.
Train: 2018-07-31T09:32:08.595641: step 8163, loss 0.554889.
Train: 2018-07-31T09:32:08.767481: step 8164, loss 0.572508.
Train: 2018-07-31T09:32:08.923660: step 8165, loss 0.478219.
Train: 2018-07-31T09:32:09.079903: step 8166, loss 0.563952.
Train: 2018-07-31T09:32:09.251737: step 8167, loss 0.589452.
Train: 2018-07-31T09:32:09.423541: step 8168, loss 0.468669.
Train: 2018-07-31T09:32:09.579756: step 8169, loss 0.57241.
Train: 2018-07-31T09:32:09.751620: step 8170, loss 0.537321.
Test: 2018-07-31T09:32:09.985912: step 8170, loss 0.549056.
Train: 2018-07-31T09:32:10.157745: step 8171, loss 0.57207.
Train: 2018-07-31T09:32:10.313958: step 8172, loss 0.510899.
Train: 2018-07-31T09:32:10.485794: step 8173, loss 0.563742.
Train: 2018-07-31T09:32:10.642006: step 8174, loss 0.590387.
Train: 2018-07-31T09:32:10.813872: step 8175, loss 0.475356.
Train: 2018-07-31T09:32:10.985676: step 8176, loss 0.53725.
Train: 2018-07-31T09:32:11.141889: step 8177, loss 0.545711.
Train: 2018-07-31T09:32:11.313755: step 8178, loss 0.62628.
Train: 2018-07-31T09:32:11.469968: step 8179, loss 0.626881.
Train: 2018-07-31T09:32:11.641802: step 8180, loss 0.581976.
Test: 2018-07-31T09:32:11.876093: step 8180, loss 0.548827.
Train: 2018-07-31T09:32:12.032306: step 8181, loss 0.582147.
Train: 2018-07-31T09:32:12.204166: step 8182, loss 0.519332.
Train: 2018-07-31T09:32:12.375975: step 8183, loss 0.536842.
Train: 2018-07-31T09:32:12.532219: step 8184, loss 0.644207.
Train: 2018-07-31T09:32:12.704054: step 8185, loss 0.492203.
Train: 2018-07-31T09:32:12.875889: step 8186, loss 0.599213.
Train: 2018-07-31T09:32:13.032102: step 8187, loss 0.528114.
Train: 2018-07-31T09:32:13.203940: step 8188, loss 0.537177.
Train: 2018-07-31T09:32:13.360150: step 8189, loss 0.53689.
Train: 2018-07-31T09:32:13.516333: step 8190, loss 0.572737.
Test: 2018-07-31T09:32:13.766305: step 8190, loss 0.548821.
Train: 2018-07-31T09:32:13.922489: step 8191, loss 0.564009.
Train: 2018-07-31T09:32:14.078702: step 8192, loss 0.554659.
Train: 2018-07-31T09:32:14.250536: step 8193, loss 0.554722.
Train: 2018-07-31T09:32:14.406783: step 8194, loss 0.527957.
Train: 2018-07-31T09:32:14.578585: step 8195, loss 0.546127.
Train: 2018-07-31T09:32:14.734798: step 8196, loss 0.617242.
Train: 2018-07-31T09:32:14.906633: step 8197, loss 0.510402.
Train: 2018-07-31T09:32:15.062846: step 8198, loss 0.670637.
Train: 2018-07-31T09:32:15.234682: step 8199, loss 0.554764.
Train: 2018-07-31T09:32:15.390919: step 8200, loss 0.53708.
Test: 2018-07-31T09:32:15.625217: step 8200, loss 0.548833.
Train: 2018-07-31T09:32:16.359417: step 8201, loss 0.652327.
Train: 2018-07-31T09:32:16.531252: step 8202, loss 0.484111.
Train: 2018-07-31T09:32:16.703117: step 8203, loss 0.634047.
Train: 2018-07-31T09:32:16.859302: step 8204, loss 0.502363.
Train: 2018-07-31T09:32:17.031166: step 8205, loss 0.57238.
Train: 2018-07-31T09:32:17.187348: step 8206, loss 0.59851.
Train: 2018-07-31T09:32:17.343593: step 8207, loss 0.607143.
Train: 2018-07-31T09:32:17.515427: step 8208, loss 0.58963.
Train: 2018-07-31T09:32:17.671643: step 8209, loss 0.589632.
Train: 2018-07-31T09:32:17.827854: step 8210, loss 0.502834.
Test: 2018-07-31T09:32:18.077796: step 8210, loss 0.54908.
Train: 2018-07-31T09:32:18.249631: step 8211, loss 0.520618.
Train: 2018-07-31T09:32:18.421435: step 8212, loss 0.580457.
Train: 2018-07-31T09:32:18.593270: step 8213, loss 0.701126.
Train: 2018-07-31T09:32:18.765135: step 8214, loss 0.55448.
Train: 2018-07-31T09:32:18.936938: step 8215, loss 0.546685.
Train: 2018-07-31T09:32:19.093153: step 8216, loss 0.622817.
Train: 2018-07-31T09:32:19.264988: step 8217, loss 0.572117.
Train: 2018-07-31T09:32:19.421225: step 8218, loss 0.546551.
Train: 2018-07-31T09:32:19.593035: step 8219, loss 0.563637.
Train: 2018-07-31T09:32:19.749279: step 8220, loss 0.571834.
Test: 2018-07-31T09:32:19.983571: step 8220, loss 0.549517.
Train: 2018-07-31T09:32:20.155404: step 8221, loss 0.538316.
Train: 2018-07-31T09:32:20.311618: step 8222, loss 0.580232.
Train: 2018-07-31T09:32:20.483452: step 8223, loss 0.563448.
Train: 2018-07-31T09:32:20.639666: step 8224, loss 0.530033.
Train: 2018-07-31T09:32:20.795880: step 8225, loss 0.538293.
Train: 2018-07-31T09:32:20.967744: step 8226, loss 0.529853.
Train: 2018-07-31T09:32:21.139549: step 8227, loss 0.571564.
Train: 2018-07-31T09:32:21.311384: step 8228, loss 0.521696.
Train: 2018-07-31T09:32:21.467621: step 8229, loss 0.504862.
Train: 2018-07-31T09:32:21.623846: step 8230, loss 0.504619.
Test: 2018-07-31T09:32:21.873751: step 8230, loss 0.549307.
Train: 2018-07-31T09:32:22.045587: step 8231, loss 0.512752.
Train: 2018-07-31T09:32:22.201800: step 8232, loss 0.554843.
Train: 2018-07-31T09:32:22.373665: step 8233, loss 0.580995.
Train: 2018-07-31T09:32:22.561091: step 8234, loss 0.597654.
Train: 2018-07-31T09:32:22.732925: step 8235, loss 0.554612.
Train: 2018-07-31T09:32:22.889139: step 8236, loss 0.554936.
Train: 2018-07-31T09:32:23.061003: step 8237, loss 0.650369.
Train: 2018-07-31T09:32:23.217211: step 8238, loss 0.571696.
Train: 2018-07-31T09:32:23.373401: step 8239, loss 0.554688.
Train: 2018-07-31T09:32:23.545268: step 8240, loss 0.56343.
Test: 2018-07-31T09:32:23.779583: step 8240, loss 0.548916.
Train: 2018-07-31T09:32:23.951419: step 8241, loss 0.641476.
Train: 2018-07-31T09:32:24.107604: step 8242, loss 0.493906.
Train: 2018-07-31T09:32:24.279438: step 8243, loss 0.580745.
Train: 2018-07-31T09:32:24.435682: step 8244, loss 0.554756.
Train: 2018-07-31T09:32:24.607510: step 8245, loss 0.502908.
Train: 2018-07-31T09:32:24.779346: step 8246, loss 0.571982.
Train: 2018-07-31T09:32:24.935564: step 8247, loss 0.528835.
Train: 2018-07-31T09:32:25.091772: step 8248, loss 0.494058.
Train: 2018-07-31T09:32:25.263583: step 8249, loss 0.650321.
Train: 2018-07-31T09:32:25.419820: step 8250, loss 0.528677.
Test: 2018-07-31T09:32:25.654149: step 8250, loss 0.548826.
Train: 2018-07-31T09:32:25.825981: step 8251, loss 0.528215.
Train: 2018-07-31T09:32:25.982165: step 8252, loss 0.545943.
Train: 2018-07-31T09:32:26.138377: step 8253, loss 0.563454.
Train: 2018-07-31T09:32:26.310212: step 8254, loss 0.554332.
Train: 2018-07-31T09:32:26.466456: step 8255, loss 0.554485.
Train: 2018-07-31T09:32:26.638291: step 8256, loss 0.616044.
Train: 2018-07-31T09:32:26.794474: step 8257, loss 0.519625.
Train: 2018-07-31T09:32:26.966339: step 8258, loss 0.607307.
Train: 2018-07-31T09:32:27.122547: step 8259, loss 0.563417.
Train: 2018-07-31T09:32:27.294385: step 8260, loss 0.554725.
Test: 2018-07-31T09:32:27.528702: step 8260, loss 0.548725.
Train: 2018-07-31T09:32:27.700511: step 8261, loss 0.607384.
Train: 2018-07-31T09:32:27.872347: step 8262, loss 0.563513.
Train: 2018-07-31T09:32:28.028592: step 8263, loss 0.607232.
Train: 2018-07-31T09:32:28.200419: step 8264, loss 0.563585.
Train: 2018-07-31T09:32:28.372230: step 8265, loss 0.5805.
Train: 2018-07-31T09:32:28.528476: step 8266, loss 0.563323.
Train: 2018-07-31T09:32:28.684656: step 8267, loss 0.597892.
Train: 2018-07-31T09:32:28.856492: step 8268, loss 0.59811.
Train: 2018-07-31T09:32:29.012705: step 8269, loss 0.598153.
Train: 2018-07-31T09:32:29.184569: step 8270, loss 0.546041.
Test: 2018-07-31T09:32:29.418893: step 8270, loss 0.54901.
Train: 2018-07-31T09:32:29.575072: step 8271, loss 0.571899.
Train: 2018-07-31T09:32:29.746938: step 8272, loss 0.623078.
Train: 2018-07-31T09:32:29.903120: step 8273, loss 0.597211.
Train: 2018-07-31T09:32:30.074955: step 8274, loss 0.521024.
Train: 2018-07-31T09:32:30.246790: step 8275, loss 0.622338.
Train: 2018-07-31T09:32:30.403037: step 8276, loss 0.596714.
Train: 2018-07-31T09:32:30.574869: step 8277, loss 0.563397.
Train: 2018-07-31T09:32:30.731053: step 8278, loss 0.538382.
Train: 2018-07-31T09:32:30.887296: step 8279, loss 0.580052.
Train: 2018-07-31T09:32:31.059100: step 8280, loss 0.546966.
Test: 2018-07-31T09:32:31.293422: step 8280, loss 0.549618.
Train: 2018-07-31T09:32:31.465279: step 8281, loss 0.488892.
Train: 2018-07-31T09:32:31.621499: step 8282, loss 0.580285.
Train: 2018-07-31T09:32:31.793304: step 8283, loss 0.521993.
Train: 2018-07-31T09:32:31.949517: step 8284, loss 0.588427.
Train: 2018-07-31T09:32:32.121381: step 8285, loss 0.530751.
Train: 2018-07-31T09:32:32.277597: step 8286, loss 0.61331.
Train: 2018-07-31T09:32:32.449430: step 8287, loss 0.596862.
Train: 2018-07-31T09:32:32.621265: step 8288, loss 0.58018.
Train: 2018-07-31T09:32:32.793070: step 8289, loss 0.605255.
Train: 2018-07-31T09:32:32.949283: step 8290, loss 0.613246.
Test: 2018-07-31T09:32:33.183634: step 8290, loss 0.549556.
Train: 2018-07-31T09:32:33.371084: step 8291, loss 0.488995.
Train: 2018-07-31T09:32:33.542893: step 8292, loss 0.580253.
Train: 2018-07-31T09:32:33.699107: step 8293, loss 0.57137.
Train: 2018-07-31T09:32:33.870971: step 8294, loss 0.505473.
Train: 2018-07-31T09:32:34.027180: step 8295, loss 0.55486.
Train: 2018-07-31T09:32:34.199026: step 8296, loss 0.605041.
Train: 2018-07-31T09:32:34.355234: step 8297, loss 0.538421.
Train: 2018-07-31T09:32:34.527039: step 8298, loss 0.630039.
Train: 2018-07-31T09:32:34.698874: step 8299, loss 0.563523.
Train: 2018-07-31T09:32:34.870708: step 8300, loss 0.604635.
Test: 2018-07-31T09:32:35.105028: step 8300, loss 0.549427.
Train: 2018-07-31T09:32:35.854852: step 8301, loss 0.57161.
Train: 2018-07-31T09:32:36.011096: step 8302, loss 0.529927.
Train: 2018-07-31T09:32:36.182900: step 8303, loss 0.596627.
Train: 2018-07-31T09:32:36.339114: step 8304, loss 0.471579.
Train: 2018-07-31T09:32:36.495327: step 8305, loss 0.545611.
Train: 2018-07-31T09:32:36.667161: step 8306, loss 0.613231.
Train: 2018-07-31T09:32:36.839021: step 8307, loss 0.512907.
Train: 2018-07-31T09:32:36.995241: step 8308, loss 0.563229.
Train: 2018-07-31T09:32:37.167078: step 8309, loss 0.530067.
Train: 2018-07-31T09:32:37.323259: step 8310, loss 0.571677.
Test: 2018-07-31T09:32:37.557580: step 8310, loss 0.549126.
Train: 2018-07-31T09:32:37.729443: step 8311, loss 0.571912.
Train: 2018-07-31T09:32:37.885662: step 8312, loss 0.529579.
Train: 2018-07-31T09:32:38.057492: step 8313, loss 0.648286.
Train: 2018-07-31T09:32:38.229297: step 8314, loss 0.580012.
Train: 2018-07-31T09:32:38.385511: step 8315, loss 0.537607.
Train: 2018-07-31T09:32:38.541724: step 8316, loss 0.546608.
Train: 2018-07-31T09:32:38.713558: step 8317, loss 0.512187.
Train: 2018-07-31T09:32:38.869772: step 8318, loss 0.588937.
Train: 2018-07-31T09:32:39.041639: step 8319, loss 0.597369.
Train: 2018-07-31T09:32:39.197819: step 8320, loss 0.503471.
Test: 2018-07-31T09:32:39.432177: step 8320, loss 0.548873.
Train: 2018-07-31T09:32:39.603975: step 8321, loss 0.554711.
Train: 2018-07-31T09:32:39.760187: step 8322, loss 0.572303.
Train: 2018-07-31T09:32:39.932022: step 8323, loss 0.572084.
Train: 2018-07-31T09:32:40.088261: step 8324, loss 0.563046.
Train: 2018-07-31T09:32:40.260097: step 8325, loss 0.554428.
Train: 2018-07-31T09:32:40.416285: step 8326, loss 0.563242.
Train: 2018-07-31T09:32:40.588149: step 8327, loss 0.520277.
Train: 2018-07-31T09:32:40.759980: step 8328, loss 0.519962.
Train: 2018-07-31T09:32:40.916200: step 8329, loss 0.485071.
Train: 2018-07-31T09:32:41.088026: step 8330, loss 0.571937.
Test: 2018-07-31T09:32:41.322353: step 8330, loss 0.548608.
Train: 2018-07-31T09:32:41.494187: step 8331, loss 0.502111.
Train: 2018-07-31T09:32:41.666025: step 8332, loss 0.615729.
Train: 2018-07-31T09:32:41.837857: step 8333, loss 0.519135.
Train: 2018-07-31T09:32:41.994069: step 8334, loss 0.528138.
Train: 2018-07-31T09:32:42.165874: step 8335, loss 0.563416.
Train: 2018-07-31T09:32:42.322087: step 8336, loss 0.581186.
Train: 2018-07-31T09:32:42.493954: step 8337, loss 0.545691.
Train: 2018-07-31T09:32:42.665757: step 8338, loss 0.590218.
Train: 2018-07-31T09:32:42.837622: step 8339, loss 0.626051.
Train: 2018-07-31T09:32:43.009457: step 8340, loss 0.536712.
Test: 2018-07-31T09:32:43.243773: step 8340, loss 0.548423.
Train: 2018-07-31T09:32:43.415581: step 8341, loss 0.500686.
Train: 2018-07-31T09:32:43.587417: step 8342, loss 0.590091.
Train: 2018-07-31T09:32:43.743665: step 8343, loss 0.608533.
Train: 2018-07-31T09:32:43.915490: step 8344, loss 0.643387.
Train: 2018-07-31T09:32:44.087329: step 8345, loss 0.536438.
Train: 2018-07-31T09:32:44.243544: step 8346, loss 0.580756.
Train: 2018-07-31T09:32:44.415386: step 8347, loss 0.474782.
Train: 2018-07-31T09:32:44.587183: step 8348, loss 0.580791.
Train: 2018-07-31T09:32:44.759048: step 8349, loss 0.527912.
Train: 2018-07-31T09:32:44.915264: step 8350, loss 0.474768.
Test: 2018-07-31T09:32:45.165173: step 8350, loss 0.548452.
Train: 2018-07-31T09:32:45.321415: step 8351, loss 0.474695.
Train: 2018-07-31T09:32:45.493251: step 8352, loss 0.625325.
Train: 2018-07-31T09:32:45.649459: step 8353, loss 0.536856.
Train: 2018-07-31T09:32:45.805672: step 8354, loss 0.589929.
Train: 2018-07-31T09:32:45.977509: step 8355, loss 0.598825.
Train: 2018-07-31T09:32:46.133725: step 8356, loss 0.581447.
Train: 2018-07-31T09:32:46.305561: step 8357, loss 0.598988.
Train: 2018-07-31T09:32:46.461743: step 8358, loss 0.563468.
Train: 2018-07-31T09:32:46.633578: step 8359, loss 0.634205.
Train: 2018-07-31T09:32:46.789816: step 8360, loss 0.589954.
Test: 2018-07-31T09:32:47.039733: step 8360, loss 0.548476.
Train: 2018-07-31T09:32:47.195971: step 8361, loss 0.642391.
Train: 2018-07-31T09:32:47.367814: step 8362, loss 0.571932.
Train: 2018-07-31T09:32:47.523995: step 8363, loss 0.641905.
Train: 2018-07-31T09:32:47.695860: step 8364, loss 0.632194.
Train: 2018-07-31T09:32:47.867694: step 8365, loss 0.589036.
Train: 2018-07-31T09:32:48.023877: step 8366, loss 0.588472.
Train: 2018-07-31T09:32:48.195742: step 8367, loss 0.487063.
Train: 2018-07-31T09:32:48.351956: step 8368, loss 0.503877.
Train: 2018-07-31T09:32:48.523794: step 8369, loss 0.605246.
Train: 2018-07-31T09:32:48.695625: step 8370, loss 0.56316.
Test: 2018-07-31T09:32:48.929947: step 8370, loss 0.549203.
Train: 2018-07-31T09:32:49.101751: step 8371, loss 0.58802.
Train: 2018-07-31T09:32:49.257988: step 8372, loss 0.62168.
Train: 2018-07-31T09:32:49.429831: step 8373, loss 0.579732.
Train: 2018-07-31T09:32:49.586011: step 8374, loss 0.522103.
Train: 2018-07-31T09:32:49.757846: step 8375, loss 0.56355.
Train: 2018-07-31T09:32:49.914093: step 8376, loss 0.530466.
Train: 2018-07-31T09:32:50.085894: step 8377, loss 0.604441.
Train: 2018-07-31T09:32:50.242108: step 8378, loss 0.59612.
Train: 2018-07-31T09:32:50.413968: step 8379, loss 0.579841.
Train: 2018-07-31T09:32:50.570190: step 8380, loss 0.571531.
Test: 2018-07-31T09:32:50.804476: step 8380, loss 0.549677.
Train: 2018-07-31T09:32:50.976312: step 8381, loss 0.546951.
Train: 2018-07-31T09:32:51.132555: step 8382, loss 0.604081.
Train: 2018-07-31T09:32:51.304390: step 8383, loss 0.604335.
Train: 2018-07-31T09:32:51.460573: step 8384, loss 0.612354.
Train: 2018-07-31T09:32:51.632438: step 8385, loss 0.604088.
Train: 2018-07-31T09:32:51.788654: step 8386, loss 0.539243.
Train: 2018-07-31T09:32:51.960486: step 8387, loss 0.49094.
Train: 2018-07-31T09:32:52.116700: step 8388, loss 0.571315.
Train: 2018-07-31T09:32:52.272882: step 8389, loss 0.595881.
Train: 2018-07-31T09:32:52.444718: step 8390, loss 0.531338.
Test: 2018-07-31T09:32:52.679038: step 8390, loss 0.549916.
Train: 2018-07-31T09:32:52.850897: step 8391, loss 0.531225.
Train: 2018-07-31T09:32:53.022733: step 8392, loss 0.555303.
Train: 2018-07-31T09:32:53.178951: step 8393, loss 0.539019.
Train: 2018-07-31T09:32:53.350785: step 8394, loss 0.603945.
Train: 2018-07-31T09:32:53.506969: step 8395, loss 0.505979.
Train: 2018-07-31T09:32:53.678804: step 8396, loss 0.555024.
Train: 2018-07-31T09:32:53.835047: step 8397, loss 0.530349.
Train: 2018-07-31T09:32:54.022502: step 8398, loss 0.555032.
Train: 2018-07-31T09:32:54.178687: step 8399, loss 0.571961.
Train: 2018-07-31T09:32:54.350551: step 8400, loss 0.49642.
Test: 2018-07-31T09:32:54.584872: step 8400, loss 0.549129.
Train: 2018-07-31T09:32:55.319045: step 8401, loss 0.546712.
Train: 2018-07-31T09:32:55.475288: step 8402, loss 0.529398.
Train: 2018-07-31T09:32:55.647125: step 8403, loss 0.605097.
Train: 2018-07-31T09:32:55.818929: step 8404, loss 0.529399.
Train: 2018-07-31T09:32:55.975171: step 8405, loss 0.56282.
Train: 2018-07-31T09:32:56.146999: step 8406, loss 0.520288.
Train: 2018-07-31T09:32:56.303189: step 8407, loss 0.580294.
Train: 2018-07-31T09:32:56.475057: step 8408, loss 0.657892.
Train: 2018-07-31T09:32:56.631236: step 8409, loss 0.588999.
Train: 2018-07-31T09:32:56.803071: step 8410, loss 0.554398.
Test: 2018-07-31T09:32:57.037426: step 8410, loss 0.548581.
Train: 2018-07-31T09:32:57.209257: step 8411, loss 0.476744.
Train: 2018-07-31T09:32:57.381062: step 8412, loss 0.571848.
Train: 2018-07-31T09:32:57.552926: step 8413, loss 0.563291.
Train: 2018-07-31T09:32:57.724761: step 8414, loss 0.537167.
Train: 2018-07-31T09:32:57.896566: step 8415, loss 0.624236.
Train: 2018-07-31T09:32:58.052780: step 8416, loss 0.554289.
Train: 2018-07-31T09:32:58.224638: step 8417, loss 0.554312.
Train: 2018-07-31T09:32:58.396449: step 8418, loss 0.580534.
Train: 2018-07-31T09:32:58.568283: step 8419, loss 0.659303.
Train: 2018-07-31T09:32:58.724522: step 8420, loss 0.563186.
Test: 2018-07-31T09:32:58.974470: step 8420, loss 0.548504.
Train: 2018-07-31T09:32:59.130682: step 8421, loss 0.554459.
Train: 2018-07-31T09:32:59.302487: step 8422, loss 0.528225.
Train: 2018-07-31T09:32:59.474345: step 8423, loss 0.493692.
Train: 2018-07-31T09:32:59.646189: step 8424, loss 0.588801.
Train: 2018-07-31T09:32:59.802400: step 8425, loss 0.545763.
Train: 2018-07-31T09:32:59.974229: step 8426, loss 0.580499.
Train: 2018-07-31T09:33:00.146039: step 8427, loss 0.554579.
Train: 2018-07-31T09:33:00.302285: step 8428, loss 0.554279.
Train: 2018-07-31T09:33:00.474087: step 8429, loss 0.597684.
Train: 2018-07-31T09:33:00.630301: step 8430, loss 0.519732.
Test: 2018-07-31T09:33:00.864651: step 8430, loss 0.548512.
Train: 2018-07-31T09:33:01.036455: step 8431, loss 0.614867.
Train: 2018-07-31T09:33:01.192699: step 8432, loss 0.571585.
Train: 2018-07-31T09:33:01.364528: step 8433, loss 0.528529.
Train: 2018-07-31T09:33:01.536371: step 8434, loss 0.537095.
Train: 2018-07-31T09:33:01.692552: step 8435, loss 0.58012.
Train: 2018-07-31T09:33:01.864386: step 8436, loss 0.632083.
Train: 2018-07-31T09:33:02.036221: step 8437, loss 0.562901.
Train: 2018-07-31T09:33:02.208055: step 8438, loss 0.537223.
Train: 2018-07-31T09:33:02.364294: step 8439, loss 0.580092.
Train: 2018-07-31T09:33:02.536105: step 8440, loss 0.597632.
Test: 2018-07-31T09:33:02.770457: step 8440, loss 0.548625.
Train: 2018-07-31T09:33:02.942289: step 8441, loss 0.477103.
Train: 2018-07-31T09:33:03.114094: step 8442, loss 0.6229.
Train: 2018-07-31T09:33:03.285962: step 8443, loss 0.494313.
Train: 2018-07-31T09:33:03.442142: step 8444, loss 0.545944.
Train: 2018-07-31T09:33:03.613977: step 8445, loss 0.468388.
Train: 2018-07-31T09:33:03.770219: step 8446, loss 0.554808.
Train: 2018-07-31T09:33:03.942055: step 8447, loss 0.554179.
Train: 2018-07-31T09:33:04.113890: step 8448, loss 0.580253.
Train: 2018-07-31T09:33:04.270106: step 8449, loss 0.563064.
Train: 2018-07-31T09:33:04.441938: step 8450, loss 0.589208.
Test: 2018-07-31T09:33:04.676259: step 8450, loss 0.548445.
Train: 2018-07-31T09:33:04.832441: step 8451, loss 0.528191.
Train: 2018-07-31T09:33:05.004277: step 8452, loss 0.449684.
Train: 2018-07-31T09:33:05.176135: step 8453, loss 0.562889.
Train: 2018-07-31T09:33:05.347946: step 8454, loss 0.589293.
Train: 2018-07-31T09:33:05.504190: step 8455, loss 0.571767.
Train: 2018-07-31T09:33:05.660405: step 8456, loss 0.619219.
Train: 2018-07-31T09:33:05.832208: step 8457, loss 0.48375.
Train: 2018-07-31T09:33:05.988420: step 8458, loss 0.545809.
Train: 2018-07-31T09:33:06.160289: step 8459, loss 0.598259.
Train: 2018-07-31T09:33:06.316469: step 8460, loss 0.49235.
Test: 2018-07-31T09:33:06.550791: step 8460, loss 0.548258.
Train: 2018-07-31T09:33:06.722649: step 8461, loss 0.562869.
Train: 2018-07-31T09:33:06.894487: step 8462, loss 0.500774.
Train: 2018-07-31T09:33:07.066294: step 8463, loss 0.590014.
Train: 2018-07-31T09:33:07.238128: step 8464, loss 0.536644.
Train: 2018-07-31T09:33:07.394341: step 8465, loss 0.509615.
Train: 2018-07-31T09:33:07.566176: step 8466, loss 0.581046.
Train: 2018-07-31T09:33:07.722390: step 8467, loss 0.544973.
Train: 2018-07-31T09:33:07.894225: step 8468, loss 0.617274.
Train: 2018-07-31T09:33:08.050463: step 8469, loss 0.581101.
Train: 2018-07-31T09:33:08.222273: step 8470, loss 0.51836.
Test: 2018-07-31T09:33:08.456618: step 8470, loss 0.548172.
Train: 2018-07-31T09:33:08.628427: step 8471, loss 0.545313.
Train: 2018-07-31T09:33:08.784641: step 8472, loss 0.55437.
Train: 2018-07-31T09:33:08.956476: step 8473, loss 0.581314.
Train: 2018-07-31T09:33:09.128341: step 8474, loss 0.608408.
Train: 2018-07-31T09:33:09.284524: step 8475, loss 0.625856.
Train: 2018-07-31T09:33:09.456388: step 8476, loss 0.518383.
Train: 2018-07-31T09:33:09.612602: step 8477, loss 0.56306.
Train: 2018-07-31T09:33:09.784440: step 8478, loss 0.509677.
Train: 2018-07-31T09:33:09.956272: step 8479, loss 0.581144.
Train: 2018-07-31T09:33:10.112485: step 8480, loss 0.625243.
Test: 2018-07-31T09:33:10.346809: step 8480, loss 0.548226.
Train: 2018-07-31T09:33:10.518610: step 8481, loss 0.518567.
Train: 2018-07-31T09:33:10.690444: step 8482, loss 0.554239.
Train: 2018-07-31T09:33:10.846689: step 8483, loss 0.669089.
Train: 2018-07-31T09:33:11.018520: step 8484, loss 0.56287.
Train: 2018-07-31T09:33:11.190357: step 8485, loss 0.633487.
Train: 2018-07-31T09:33:11.346571: step 8486, loss 0.580321.
Train: 2018-07-31T09:33:11.502755: step 8487, loss 0.49323.
Train: 2018-07-31T09:33:11.674589: step 8488, loss 0.493623.
Train: 2018-07-31T09:33:11.830802: step 8489, loss 0.641034.
Train: 2018-07-31T09:33:12.002662: step 8490, loss 0.571377.
Test: 2018-07-31T09:33:12.236988: step 8490, loss 0.548506.
Train: 2018-07-31T09:33:12.408817: step 8491, loss 0.554236.
Train: 2018-07-31T09:33:12.565036: step 8492, loss 0.614479.
Train: 2018-07-31T09:33:12.736871: step 8493, loss 0.485715.
Train: 2018-07-31T09:33:12.893053: step 8494, loss 0.588705.
Train: 2018-07-31T09:33:13.064889: step 8495, loss 0.631247.
Train: 2018-07-31T09:33:13.221131: step 8496, loss 0.596961.
Train: 2018-07-31T09:33:13.392937: step 8497, loss 0.58859.
Train: 2018-07-31T09:33:13.564802: step 8498, loss 0.554446.
Train: 2018-07-31T09:33:13.721017: step 8499, loss 0.512283.
Train: 2018-07-31T09:33:13.892847: step 8500, loss 0.546052.
Test: 2018-07-31T09:33:14.127140: step 8500, loss 0.548865.
Train: 2018-07-31T09:33:14.830101: step 8501, loss 0.495639.
Train: 2018-07-31T09:33:14.986313: step 8502, loss 0.571466.
Train: 2018-07-31T09:33:15.158181: step 8503, loss 0.537424.
Train: 2018-07-31T09:33:15.314361: step 8504, loss 0.5122.
Train: 2018-07-31T09:33:15.486196: step 8505, loss 0.562877.
Train: 2018-07-31T09:33:15.658031: step 8506, loss 0.54585.
Train: 2018-07-31T09:33:15.814275: step 8507, loss 0.495053.
Train: 2018-07-31T09:33:15.986109: step 8508, loss 0.545594.
Train: 2018-07-31T09:33:16.142318: step 8509, loss 0.596951.
Train: 2018-07-31T09:33:16.314152: step 8510, loss 0.494168.
Test: 2018-07-31T09:33:16.532855: step 8510, loss 0.548466.
Train: 2018-07-31T09:33:16.704686: step 8511, loss 0.64049.
Train: 2018-07-31T09:33:16.876526: step 8512, loss 0.528496.
Train: 2018-07-31T09:33:17.032710: step 8513, loss 0.510753.
Train: 2018-07-31T09:33:17.204544: step 8514, loss 0.606514.
Train: 2018-07-31T09:33:17.360782: step 8515, loss 0.571243.
Train: 2018-07-31T09:33:17.532617: step 8516, loss 0.528283.
Train: 2018-07-31T09:33:17.688807: step 8517, loss 0.580259.
Train: 2018-07-31T09:33:17.860671: step 8518, loss 0.597975.
Train: 2018-07-31T09:33:18.016854: step 8519, loss 0.49267.
Train: 2018-07-31T09:33:18.188688: step 8520, loss 0.475112.
Test: 2018-07-31T09:33:18.423011: step 8520, loss 0.548244.
Train: 2018-07-31T09:33:18.594874: step 8521, loss 0.598501.
Train: 2018-07-31T09:33:18.778189: step 8522, loss 0.598146.
Train: 2018-07-31T09:33:18.934435: step 8523, loss 0.527951.
Train: 2018-07-31T09:33:19.106238: step 8524, loss 0.53659.
Train: 2018-07-31T09:33:19.278073: step 8525, loss 0.563013.
Train: 2018-07-31T09:33:19.434285: step 8526, loss 0.598458.
Train: 2018-07-31T09:33:19.590530: step 8527, loss 0.580623.
Train: 2018-07-31T09:33:19.762365: step 8528, loss 0.589487.
Train: 2018-07-31T09:33:19.918572: step 8529, loss 0.571935.
Train: 2018-07-31T09:33:20.090412: step 8530, loss 0.54531.
Test: 2018-07-31T09:33:20.324738: step 8530, loss 0.548191.
Train: 2018-07-31T09:33:20.496538: step 8531, loss 0.598344.
Train: 2018-07-31T09:33:20.668371: step 8532, loss 0.510258.
Train: 2018-07-31T09:33:20.824617: step 8533, loss 0.633171.
Train: 2018-07-31T09:33:20.996420: step 8534, loss 0.536791.
Train: 2018-07-31T09:33:21.168285: step 8535, loss 0.553898.
Train: 2018-07-31T09:33:21.324468: step 8536, loss 0.615547.
Train: 2018-07-31T09:33:21.496327: step 8537, loss 0.5018.
Train: 2018-07-31T09:33:21.668138: step 8538, loss 0.518986.
Train: 2018-07-31T09:33:21.824351: step 8539, loss 0.580421.
Train: 2018-07-31T09:33:21.996186: step 8540, loss 0.572004.
Test: 2018-07-31T09:33:22.230507: step 8540, loss 0.548285.
Train: 2018-07-31T09:33:22.386751: step 8541, loss 0.51914.
Train: 2018-07-31T09:33:22.558581: step 8542, loss 0.501809.
Train: 2018-07-31T09:33:22.730422: step 8543, loss 0.518854.
Train: 2018-07-31T09:33:22.886632: step 8544, loss 0.597611.
Train: 2018-07-31T09:33:23.058437: step 8545, loss 0.589053.
Train: 2018-07-31T09:33:23.214651: step 8546, loss 0.519018.
Train: 2018-07-31T09:33:23.386509: step 8547, loss 0.589186.
Train: 2018-07-31T09:33:23.542698: step 8548, loss 0.510193.
Train: 2018-07-31T09:33:23.714564: step 8549, loss 0.562426.
Train: 2018-07-31T09:33:23.886398: step 8550, loss 0.562801.
Test: 2018-07-31T09:33:24.120689: step 8550, loss 0.548186.
Train: 2018-07-31T09:33:24.292524: step 8551, loss 0.659777.
Train: 2018-07-31T09:33:24.448737: step 8552, loss 0.55381.
Train: 2018-07-31T09:33:24.620571: step 8553, loss 0.668618.
Train: 2018-07-31T09:33:24.776815: step 8554, loss 0.641641.
Train: 2018-07-31T09:33:24.948620: step 8555, loss 0.580472.
Train: 2018-07-31T09:33:25.104832: step 8556, loss 0.571606.
Train: 2018-07-31T09:33:25.261046: step 8557, loss 0.476369.
Train: 2018-07-31T09:33:25.432882: step 8558, loss 0.511149.
Train: 2018-07-31T09:33:25.589125: step 8559, loss 0.467981.
Train: 2018-07-31T09:33:25.760930: step 8560, loss 0.571497.
Test: 2018-07-31T09:33:25.995281: step 8560, loss 0.548413.
Train: 2018-07-31T09:33:26.167114: step 8561, loss 0.502741.
Train: 2018-07-31T09:33:26.338919: step 8562, loss 0.614907.
Train: 2018-07-31T09:33:26.495163: step 8563, loss 0.588876.
Train: 2018-07-31T09:33:26.666998: step 8564, loss 0.580434.
Train: 2018-07-31T09:33:26.823212: step 8565, loss 0.666729.
Train: 2018-07-31T09:33:26.995046: step 8566, loss 0.649358.
Train: 2018-07-31T09:33:27.166851: step 8567, loss 0.588741.
Train: 2018-07-31T09:33:27.323095: step 8568, loss 0.606199.
Train: 2018-07-31T09:33:27.494924: step 8569, loss 0.545787.
Train: 2018-07-31T09:33:27.666765: step 8570, loss 0.647678.
Test: 2018-07-31T09:33:27.901054: step 8570, loss 0.548785.
Train: 2018-07-31T09:33:28.072887: step 8571, loss 0.546141.
Train: 2018-07-31T09:33:28.244753: step 8572, loss 0.537584.
Train: 2018-07-31T09:33:28.400937: step 8573, loss 0.646698.
Train: 2018-07-31T09:33:28.572770: step 8574, loss 0.571473.
Train: 2018-07-31T09:33:28.744606: step 8575, loss 0.537999.
Train: 2018-07-31T09:33:28.900820: step 8576, loss 0.62901.
Train: 2018-07-31T09:33:29.072687: step 8577, loss 0.595424.
Train: 2018-07-31T09:33:29.244519: step 8578, loss 0.489414.
Train: 2018-07-31T09:33:29.416324: step 8579, loss 0.555012.
Train: 2018-07-31T09:33:29.572536: step 8580, loss 0.497816.
Test: 2018-07-31T09:33:29.822510: step 8580, loss 0.54947.
Train: 2018-07-31T09:33:29.978722: step 8581, loss 0.57935.
Train: 2018-07-31T09:33:30.150551: step 8582, loss 0.612069.
Train: 2018-07-31T09:33:30.322361: step 8583, loss 0.579637.
Train: 2018-07-31T09:33:30.494226: step 8584, loss 0.554967.
Train: 2018-07-31T09:33:30.650409: step 8585, loss 0.538682.
Train: 2018-07-31T09:33:30.822245: step 8586, loss 0.579358.
Train: 2018-07-31T09:33:30.994079: step 8587, loss 0.530441.
Train: 2018-07-31T09:33:31.150293: step 8588, loss 0.538588.
Train: 2018-07-31T09:33:31.306506: step 8589, loss 0.53854.
Train: 2018-07-31T09:33:31.478371: step 8590, loss 0.587652.
Test: 2018-07-31T09:33:31.712662: step 8590, loss 0.549234.
Train: 2018-07-31T09:33:31.884495: step 8591, loss 0.596093.
Train: 2018-07-31T09:33:32.040739: step 8592, loss 0.505472.
Train: 2018-07-31T09:33:32.212574: step 8593, loss 0.571231.
Train: 2018-07-31T09:33:32.368787: step 8594, loss 0.554644.
Train: 2018-07-31T09:33:32.540621: step 8595, loss 0.63773.
Train: 2018-07-31T09:33:32.712426: step 8596, loss 0.554588.
Train: 2018-07-31T09:33:32.884261: step 8597, loss 0.571353.
Train: 2018-07-31T09:33:33.040505: step 8598, loss 0.604526.
Train: 2018-07-31T09:33:33.212340: step 8599, loss 0.537895.
Train: 2018-07-31T09:33:33.384169: step 8600, loss 0.546476.
Test: 2018-07-31T09:33:33.618491: step 8600, loss 0.548922.
Train: 2018-07-31T09:33:34.352691: step 8601, loss 0.546008.
Train: 2018-07-31T09:33:34.524532: step 8602, loss 0.554422.
Train: 2018-07-31T09:33:34.696368: step 8603, loss 0.546143.
Train: 2018-07-31T09:33:34.868202: step 8604, loss 0.562976.
Train: 2018-07-31T09:33:35.024386: step 8605, loss 0.436868.
Train: 2018-07-31T09:33:35.196251: step 8606, loss 0.545748.
Train: 2018-07-31T09:33:35.352464: step 8607, loss 0.617239.
Train: 2018-07-31T09:33:35.524268: step 8608, loss 0.588302.
Train: 2018-07-31T09:33:35.680482: step 8609, loss 0.545848.
Train: 2018-07-31T09:33:35.852316: step 8610, loss 0.57987.
Test: 2018-07-31T09:33:36.086636: step 8610, loss 0.548455.
Train: 2018-07-31T09:33:36.258502: step 8611, loss 0.520338.
Train: 2018-07-31T09:33:36.430338: step 8612, loss 0.605818.
Train: 2018-07-31T09:33:36.586519: step 8613, loss 0.571247.
Train: 2018-07-31T09:33:36.758382: step 8614, loss 0.631386.
Train: 2018-07-31T09:33:36.914599: step 8615, loss 0.64032.
Train: 2018-07-31T09:33:37.086427: step 8616, loss 0.537161.
Train: 2018-07-31T09:33:37.242640: step 8617, loss 0.537183.
Train: 2018-07-31T09:33:37.414451: step 8618, loss 0.545931.
Train: 2018-07-31T09:33:37.570663: step 8619, loss 0.605768.
Train: 2018-07-31T09:33:37.742523: step 8620, loss 0.571473.
Test: 2018-07-31T09:33:37.976819: step 8620, loss 0.548495.
Train: 2018-07-31T09:33:38.148653: step 8621, loss 0.588398.
Train: 2018-07-31T09:33:38.304899: step 8622, loss 0.579768.
Train: 2018-07-31T09:33:38.461113: step 8623, loss 0.5971.
Train: 2018-07-31T09:33:38.632950: step 8624, loss 0.579737.
Train: 2018-07-31T09:33:38.789129: step 8625, loss 0.528768.
Train: 2018-07-31T09:33:38.960988: step 8626, loss 0.545757.
Train: 2018-07-31T09:33:39.117207: step 8627, loss 0.545805.
Train: 2018-07-31T09:33:39.289011: step 8628, loss 0.545959.
Train: 2018-07-31T09:33:39.445224: step 8629, loss 0.571188.
Train: 2018-07-31T09:33:39.617060: step 8630, loss 0.537636.
Test: 2018-07-31T09:33:39.851411: step 8630, loss 0.548662.
Train: 2018-07-31T09:33:40.023215: step 8631, loss 0.545976.
Train: 2018-07-31T09:33:40.179428: step 8632, loss 0.579853.
Train: 2018-07-31T09:33:40.351262: step 8633, loss 0.545862.
Train: 2018-07-31T09:33:40.507476: step 8634, loss 0.596918.
Train: 2018-07-31T09:33:40.679310: step 8635, loss 0.537204.
Train: 2018-07-31T09:33:40.835525: step 8636, loss 0.55431.
Train: 2018-07-31T09:33:41.007360: step 8637, loss 0.562744.
Train: 2018-07-31T09:33:41.163572: step 8638, loss 0.579799.
Train: 2018-07-31T09:33:41.335406: step 8639, loss 0.528529.
Train: 2018-07-31T09:33:41.507243: step 8640, loss 0.562885.
Test: 2018-07-31T09:33:41.741594: step 8640, loss 0.548508.
Train: 2018-07-31T09:33:41.913397: step 8641, loss 0.528693.
Train: 2018-07-31T09:33:42.069642: step 8642, loss 0.537046.
Train: 2018-07-31T09:33:42.241446: step 8643, loss 0.58004.
Train: 2018-07-31T09:33:42.397683: step 8644, loss 0.511477.
Train: 2018-07-31T09:33:42.569525: step 8645, loss 0.60592.
Train: 2018-07-31T09:33:42.725737: step 8646, loss 0.545699.
Train: 2018-07-31T09:33:42.881919: step 8647, loss 0.597453.
Train: 2018-07-31T09:33:43.053787: step 8648, loss 0.545545.
Train: 2018-07-31T09:33:43.209968: step 8649, loss 0.597413.
Train: 2018-07-31T09:33:43.381803: step 8650, loss 0.614366.
Test: 2018-07-31T09:33:43.616123: step 8650, loss 0.548329.
Train: 2018-07-31T09:33:43.787958: step 8651, loss 0.537043.
Train: 2018-07-31T09:33:43.959793: step 8652, loss 0.5713.
Train: 2018-07-31T09:33:44.116005: step 8653, loss 0.57125.
Train: 2018-07-31T09:33:44.287871: step 8654, loss 0.545489.
Train: 2018-07-31T09:33:44.459676: step 8655, loss 0.493972.
Train: 2018-07-31T09:33:44.615919: step 8656, loss 0.571642.
Train: 2018-07-31T09:33:44.772133: step 8657, loss 0.511091.
Train: 2018-07-31T09:33:44.943937: step 8658, loss 0.571317.
Train: 2018-07-31T09:33:45.115772: step 8659, loss 0.441635.
Train: 2018-07-31T09:33:45.272016: step 8660, loss 0.579952.
Test: 2018-07-31T09:33:45.506336: step 8660, loss 0.548197.
Train: 2018-07-31T09:33:45.678140: step 8661, loss 0.510495.
Train: 2018-07-31T09:33:45.849975: step 8662, loss 0.641366.
Train: 2018-07-31T09:33:46.006189: step 8663, loss 0.54522.
Train: 2018-07-31T09:33:46.178023: step 8664, loss 0.562993.
Train: 2018-07-31T09:33:46.334267: step 8665, loss 0.589077.
Train: 2018-07-31T09:33:46.506101: step 8666, loss 0.59791.
Train: 2018-07-31T09:33:46.662284: step 8667, loss 0.509819.
Train: 2018-07-31T09:33:46.834149: step 8668, loss 0.483838.
Train: 2018-07-31T09:33:47.005979: step 8669, loss 0.589341.
Train: 2018-07-31T09:33:47.162168: step 8670, loss 0.580517.
Test: 2018-07-31T09:33:47.396488: step 8670, loss 0.548058.
Train: 2018-07-31T09:33:47.568322: step 8671, loss 0.589534.
Train: 2018-07-31T09:33:47.724537: step 8672, loss 0.536337.
Train: 2018-07-31T09:33:47.896401: step 8673, loss 0.55409.
Train: 2018-07-31T09:33:48.068237: step 8674, loss 0.518694.
Train: 2018-07-31T09:33:48.240065: step 8675, loss 0.562782.
Train: 2018-07-31T09:33:48.396254: step 8676, loss 0.438775.
Train: 2018-07-31T09:33:48.552492: step 8677, loss 0.580625.
Train: 2018-07-31T09:33:48.724326: step 8678, loss 0.634203.
Train: 2018-07-31T09:33:48.896136: step 8679, loss 0.509434.
Train: 2018-07-31T09:33:49.067972: step 8680, loss 0.616568.
Test: 2018-07-31T09:33:49.302292: step 8680, loss 0.547986.
Train: 2018-07-31T09:33:49.458535: step 8681, loss 0.634262.
Train: 2018-07-31T09:33:49.630339: step 8682, loss 0.536137.
Train: 2018-07-31T09:33:49.802174: step 8683, loss 0.607502.
Train: 2018-07-31T09:33:49.958389: step 8684, loss 0.616065.
Train: 2018-07-31T09:33:50.130223: step 8685, loss 0.518659.
Train: 2018-07-31T09:33:50.286474: step 8686, loss 0.510112.
Train: 2018-07-31T09:33:50.458301: step 8687, loss 0.668447.
Train: 2018-07-31T09:33:50.614484: step 8688, loss 0.571341.
Train: 2018-07-31T09:33:50.786352: step 8689, loss 0.650481.
Train: 2018-07-31T09:33:50.958154: step 8690, loss 0.510231.
Test: 2018-07-31T09:33:51.192475: step 8690, loss 0.548202.
Train: 2018-07-31T09:33:51.348718: step 8691, loss 0.588634.
Train: 2018-07-31T09:33:51.520552: step 8692, loss 0.553997.
Train: 2018-07-31T09:33:51.692388: step 8693, loss 0.553914.
Train: 2018-07-31T09:33:51.848571: step 8694, loss 0.562746.
Train: 2018-07-31T09:33:52.020404: step 8695, loss 0.588594.
Train: 2018-07-31T09:33:52.192239: step 8696, loss 0.597057.
Train: 2018-07-31T09:33:52.364105: step 8697, loss 0.511463.
Train: 2018-07-31T09:33:52.520288: step 8698, loss 0.49448.
Train: 2018-07-31T09:33:52.692154: step 8699, loss 0.554186.
Train: 2018-07-31T09:33:52.848366: step 8700, loss 0.537022.
Test: 2018-07-31T09:33:53.082687: step 8700, loss 0.54845.
Train: 2018-07-31T09:33:53.801238: step 8701, loss 0.528677.
Train: 2018-07-31T09:33:53.973072: step 8702, loss 0.554161.
Train: 2018-07-31T09:33:54.144932: step 8703, loss 0.597116.
Train: 2018-07-31T09:33:54.316744: step 8704, loss 0.537147.
Train: 2018-07-31T09:33:54.488578: step 8705, loss 0.545694.
Train: 2018-07-31T09:33:54.660443: step 8706, loss 0.571288.
Train: 2018-07-31T09:33:54.832246: step 8707, loss 0.614286.
Train: 2018-07-31T09:33:55.004106: step 8708, loss 0.562696.
Train: 2018-07-31T09:33:55.175945: step 8709, loss 0.562911.
Train: 2018-07-31T09:33:55.347776: step 8710, loss 0.588183.
Test: 2018-07-31T09:33:55.582073: step 8710, loss 0.548358.
Train: 2018-07-31T09:33:55.753930: step 8711, loss 0.579729.
Train: 2018-07-31T09:33:55.925741: step 8712, loss 0.52809.
Train: 2018-07-31T09:33:56.081953: step 8713, loss 0.579714.
Train: 2018-07-31T09:33:56.253819: step 8714, loss 0.622601.
Train: 2018-07-31T09:33:56.410002: step 8715, loss 0.554104.
Train: 2018-07-31T09:33:56.566215: step 8716, loss 0.502744.
Train: 2018-07-31T09:33:56.738050: step 8717, loss 0.545442.
Train: 2018-07-31T09:33:56.894263: step 8718, loss 0.554317.
Train: 2018-07-31T09:33:57.066125: step 8719, loss 0.477333.
Train: 2018-07-31T09:33:57.237933: step 8720, loss 0.631282.
Test: 2018-07-31T09:33:57.472284: step 8720, loss 0.548361.
Train: 2018-07-31T09:33:57.644087: step 8721, loss 0.545514.
Train: 2018-07-31T09:33:57.815922: step 8722, loss 0.562592.
Train: 2018-07-31T09:33:57.972167: step 8723, loss 0.554168.
Train: 2018-07-31T09:33:58.144001: step 8724, loss 0.562569.
Train: 2018-07-31T09:33:58.300216: step 8725, loss 0.562671.
Train: 2018-07-31T09:33:58.472019: step 8726, loss 0.536836.
Train: 2018-07-31T09:33:58.643854: step 8727, loss 0.519589.
Train: 2018-07-31T09:33:58.800067: step 8728, loss 0.580047.
Train: 2018-07-31T09:33:58.971928: step 8729, loss 0.537101.
Train: 2018-07-31T09:33:59.128145: step 8730, loss 0.545344.
Test: 2018-07-31T09:33:59.362467: step 8730, loss 0.548177.
Train: 2018-07-31T09:33:59.534300: step 8731, loss 0.528263.
Train: 2018-07-31T09:33:59.690484: step 8732, loss 0.536483.
Train: 2018-07-31T09:33:59.862318: step 8733, loss 0.580399.
Train: 2018-07-31T09:34:00.018533: step 8734, loss 0.606587.
Train: 2018-07-31T09:34:00.190391: step 8735, loss 0.509996.
Train: 2018-07-31T09:34:00.346581: step 8736, loss 0.536825.
Train: 2018-07-31T09:34:00.518415: step 8737, loss 0.554003.
Train: 2018-07-31T09:34:00.690282: step 8738, loss 0.59789.
Train: 2018-07-31T09:34:00.846493: step 8739, loss 0.633303.
Train: 2018-07-31T09:34:01.018298: step 8740, loss 0.615397.
Test: 2018-07-31T09:34:01.252644: step 8740, loss 0.548061.
Train: 2018-07-31T09:34:01.408832: step 8741, loss 0.580355.
Train: 2018-07-31T09:34:01.580667: step 8742, loss 0.510269.
Train: 2018-07-31T09:34:01.736880: step 8743, loss 0.545475.
Train: 2018-07-31T09:34:01.908747: step 8744, loss 0.545367.
Train: 2018-07-31T09:34:02.064928: step 8745, loss 0.580363.
Train: 2018-07-31T09:34:02.236762: step 8746, loss 0.519184.
Train: 2018-07-31T09:34:02.408628: step 8747, loss 0.606472.
Train: 2018-07-31T09:34:02.564811: step 8748, loss 0.536457.
Train: 2018-07-31T09:34:02.736645: step 8749, loss 0.545143.
Train: 2018-07-31T09:34:02.892889: step 8750, loss 0.563046.
Test: 2018-07-31T09:34:03.142831: step 8750, loss 0.548124.
Train: 2018-07-31T09:34:03.314636: step 8751, loss 0.580396.
Train: 2018-07-31T09:34:03.470849: step 8752, loss 0.588866.
Train: 2018-07-31T09:34:03.642683: step 8753, loss 0.528101.
Train: 2018-07-31T09:34:03.814550: step 8754, loss 0.614922.
Train: 2018-07-31T09:34:03.970731: step 8755, loss 0.614952.
Train: 2018-07-31T09:34:04.142566: step 8756, loss 0.554225.
Train: 2018-07-31T09:34:04.298780: step 8757, loss 0.545299.
Train: 2018-07-31T09:34:04.454994: step 8758, loss 0.544624.
Train: 2018-07-31T09:34:04.626828: step 8759, loss 0.562673.
Train: 2018-07-31T09:34:04.783041: step 8760, loss 0.545404.
Test: 2018-07-31T09:34:05.033013: step 8760, loss 0.548271.
Train: 2018-07-31T09:34:05.204847: step 8761, loss 0.571159.
Train: 2018-07-31T09:34:05.376652: step 8762, loss 0.545678.
Train: 2018-07-31T09:34:05.532896: step 8763, loss 0.545316.
Train: 2018-07-31T09:34:05.689110: step 8764, loss 0.597174.
Train: 2018-07-31T09:34:05.860914: step 8765, loss 0.562618.
Train: 2018-07-31T09:34:06.017158: step 8766, loss 0.5541.
Train: 2018-07-31T09:34:06.188993: step 8767, loss 0.605641.
Train: 2018-07-31T09:34:06.345176: step 8768, loss 0.554159.
Train: 2018-07-31T09:34:06.501389: step 8769, loss 0.56261.
Train: 2018-07-31T09:34:06.673223: step 8770, loss 0.605533.
Test: 2018-07-31T09:34:06.907569: step 8770, loss 0.548372.
Train: 2018-07-31T09:34:07.079403: step 8771, loss 0.588353.
Train: 2018-07-31T09:34:07.251244: step 8772, loss 0.639483.
Train: 2018-07-31T09:34:07.407458: step 8773, loss 0.664897.
Train: 2018-07-31T09:34:07.579292: step 8774, loss 0.613874.
Train: 2018-07-31T09:34:07.735500: step 8775, loss 0.537471.
Train: 2018-07-31T09:34:07.907334: step 8776, loss 0.512638.
Train: 2018-07-31T09:34:08.079175: step 8777, loss 0.496215.
Train: 2018-07-31T09:34:08.235391: step 8778, loss 0.537857.
Train: 2018-07-31T09:34:08.407192: step 8779, loss 0.529614.
Train: 2018-07-31T09:34:08.563406: step 8780, loss 0.629562.
Test: 2018-07-31T09:34:08.797728: step 8780, loss 0.548843.
Train: 2018-07-31T09:34:08.969560: step 8781, loss 0.612678.
Train: 2018-07-31T09:34:09.141426: step 8782, loss 0.537745.
Train: 2018-07-31T09:34:09.297610: step 8783, loss 0.537883.
Train: 2018-07-31T09:34:09.469444: step 8784, loss 0.646008.
Train: 2018-07-31T09:34:09.641278: step 8785, loss 0.546277.
Train: 2018-07-31T09:34:09.813113: step 8786, loss 0.587409.
Train: 2018-07-31T09:34:09.969327: step 8787, loss 0.562815.
Train: 2018-07-31T09:34:10.141161: step 8788, loss 0.628919.
Train: 2018-07-31T09:34:10.297375: step 8789, loss 0.579414.
Train: 2018-07-31T09:34:10.469240: step 8790, loss 0.587602.
Test: 2018-07-31T09:34:10.703563: step 8790, loss 0.549145.
Train: 2018-07-31T09:34:10.875365: step 8791, loss 0.562911.
Train: 2018-07-31T09:34:11.031608: step 8792, loss 0.538457.
Train: 2018-07-31T09:34:11.203437: step 8793, loss 0.579266.
Train: 2018-07-31T09:34:11.375278: step 8794, loss 0.513943.
Train: 2018-07-31T09:34:11.531491: step 8795, loss 0.603941.
Train: 2018-07-31T09:34:11.703326: step 8796, loss 0.587553.
Train: 2018-07-31T09:34:11.859539: step 8797, loss 0.505697.
Train: 2018-07-31T09:34:12.031369: step 8798, loss 0.546411.
Train: 2018-07-31T09:34:12.187589: step 8799, loss 0.554618.
Train: 2018-07-31T09:34:12.359393: step 8800, loss 0.521791.
Test: 2018-07-31T09:34:12.593744: step 8800, loss 0.549055.
Train: 2018-07-31T09:34:13.343567: step 8801, loss 0.505093.
Train: 2018-07-31T09:34:13.515402: step 8802, loss 0.513178.
Train: 2018-07-31T09:34:13.687207: step 8803, loss 0.612843.
Train: 2018-07-31T09:34:13.843450: step 8804, loss 0.512981.
Train: 2018-07-31T09:34:13.999633: step 8805, loss 0.596189.
Train: 2018-07-31T09:34:14.171500: step 8806, loss 0.529116.
Train: 2018-07-31T09:34:14.343303: step 8807, loss 0.512163.
Train: 2018-07-31T09:34:14.515166: step 8808, loss 0.545636.
Train: 2018-07-31T09:34:14.686973: step 8809, loss 0.562759.
Train: 2018-07-31T09:34:14.858832: step 8810, loss 0.511268.
Test: 2018-07-31T09:34:15.093158: step 8810, loss 0.548261.
Train: 2018-07-31T09:34:15.249367: step 8811, loss 0.494065.
Train: 2018-07-31T09:34:15.421176: step 8812, loss 0.605766.
Train: 2018-07-31T09:34:15.577419: step 8813, loss 0.553996.
Train: 2018-07-31T09:34:15.749254: step 8814, loss 0.571492.
Train: 2018-07-31T09:34:15.905467: step 8815, loss 0.545143.
Train: 2018-07-31T09:34:16.077272: step 8816, loss 0.580379.
Train: 2018-07-31T09:34:16.233485: step 8817, loss 0.545167.
Train: 2018-07-31T09:34:16.405352: step 8818, loss 0.571627.
Train: 2018-07-31T09:34:16.577154: step 8819, loss 0.52746.
Train: 2018-07-31T09:34:16.749019: step 8820, loss 0.518612.
Test: 2018-07-31T09:34:16.983340: step 8820, loss 0.547923.
Train: 2018-07-31T09:34:17.155169: step 8821, loss 0.616088.
Train: 2018-07-31T09:34:17.311389: step 8822, loss 0.545039.
Train: 2018-07-31T09:34:17.483224: step 8823, loss 0.607247.
Train: 2018-07-31T09:34:17.639407: step 8824, loss 0.509613.
Train: 2018-07-31T09:34:17.811272: step 8825, loss 0.687606.
Train: 2018-07-31T09:34:17.983100: step 8826, loss 0.580497.
Train: 2018-07-31T09:34:18.154940: step 8827, loss 0.438345.
Train: 2018-07-31T09:34:18.311155: step 8828, loss 0.607238.
Train: 2018-07-31T09:34:18.482984: step 8829, loss 0.465239.
Train: 2018-07-31T09:34:18.654793: step 8830, loss 0.580623.
Test: 2018-07-31T09:34:18.889114: step 8830, loss 0.547912.
Train: 2018-07-31T09:34:19.045351: step 8831, loss 0.544998.
Train: 2018-07-31T09:34:19.217191: step 8832, loss 0.598374.
Train: 2018-07-31T09:34:19.373406: step 8833, loss 0.536006.
Train: 2018-07-31T09:34:19.545242: step 8834, loss 0.536131.
Train: 2018-07-31T09:34:19.717074: step 8835, loss 0.545114.
Train: 2018-07-31T09:34:19.888910: step 8836, loss 0.580433.
Train: 2018-07-31T09:34:20.045124: step 8837, loss 0.500395.
Train: 2018-07-31T09:34:20.216928: step 8838, loss 0.651757.
Train: 2018-07-31T09:34:20.373170: step 8839, loss 0.518359.
Train: 2018-07-31T09:34:20.544976: step 8840, loss 0.607392.
Test: 2018-07-31T09:34:20.779329: step 8840, loss 0.547906.
Train: 2018-07-31T09:34:20.935540: step 8841, loss 0.51829.
Train: 2018-07-31T09:34:21.107368: step 8842, loss 0.642726.
Train: 2018-07-31T09:34:21.279211: step 8843, loss 0.55378.
Train: 2018-07-31T09:34:21.451014: step 8844, loss 0.571634.
Train: 2018-07-31T09:34:21.607251: step 8845, loss 0.571705.
Train: 2018-07-31T09:34:21.779062: step 8846, loss 0.606634.
Train: 2018-07-31T09:34:21.935306: step 8847, loss 0.483617.
Train: 2018-07-31T09:34:22.107134: step 8848, loss 0.527541.
Train: 2018-07-31T09:34:22.278975: step 8849, loss 0.615525.
Train: 2018-07-31T09:34:22.435189: step 8850, loss 0.510267.
Test: 2018-07-31T09:34:22.747585: step 8850, loss 0.548028.
Train: 2018-07-31T09:34:22.919451: step 8851, loss 0.580415.
Train: 2018-07-31T09:34:23.075634: step 8852, loss 0.536513.
Train: 2018-07-31T09:34:23.247498: step 8853, loss 0.5013.
Train: 2018-07-31T09:34:23.403681: step 8854, loss 0.571348.
Train: 2018-07-31T09:34:23.559931: step 8855, loss 0.545245.
Train: 2018-07-31T09:34:23.731759: step 8856, loss 0.589157.
Train: 2018-07-31T09:34:23.903563: step 8857, loss 0.571524.
Train: 2018-07-31T09:34:24.075399: step 8858, loss 0.51899.
Train: 2018-07-31T09:34:24.231613: step 8859, loss 0.545231.
Train: 2018-07-31T09:34:24.403447: step 8860, loss 0.632805.
Test: 2018-07-31T09:34:24.637767: step 8860, loss 0.548013.
Train: 2018-07-31T09:34:24.809601: step 8861, loss 0.562751.
Train: 2018-07-31T09:34:24.981466: step 8862, loss 0.676238.
Train: 2018-07-31T09:34:25.153304: step 8863, loss 0.606254.
Train: 2018-07-31T09:34:25.309516: step 8864, loss 0.510527.
Train: 2018-07-31T09:34:25.481344: step 8865, loss 0.527751.
Train: 2018-07-31T09:34:25.653185: step 8866, loss 0.588623.
Train: 2018-07-31T09:34:25.809399: step 8867, loss 0.510856.
Train: 2018-07-31T09:34:25.981233: step 8868, loss 0.510838.
Train: 2018-07-31T09:34:26.137417: step 8869, loss 0.571199.
Train: 2018-07-31T09:34:26.309251: step 8870, loss 0.484932.
Test: 2018-07-31T09:34:26.559192: step 8870, loss 0.548154.
Train: 2018-07-31T09:34:26.715405: step 8871, loss 0.510814.
Train: 2018-07-31T09:34:26.887270: step 8872, loss 0.588618.
Train: 2018-07-31T09:34:27.059075: step 8873, loss 0.545201.
Train: 2018-07-31T09:34:27.215289: step 8874, loss 0.606149.
Train: 2018-07-31T09:34:27.387153: step 8875, loss 0.588937.
Train: 2018-07-31T09:34:27.558989: step 8876, loss 0.536355.
Train: 2018-07-31T09:34:27.730826: step 8877, loss 0.588904.
Train: 2018-07-31T09:34:27.887031: step 8878, loss 0.562745.
Train: 2018-07-31T09:34:28.058872: step 8879, loss 0.606195.
Train: 2018-07-31T09:34:28.230706: step 8880, loss 0.527929.
Test: 2018-07-31T09:34:28.480617: step 8880, loss 0.548083.
Train: 2018-07-31T09:34:28.636860: step 8881, loss 0.579995.
Train: 2018-07-31T09:34:28.793076: step 8882, loss 0.54531.
Train: 2018-07-31T09:34:28.964909: step 8883, loss 0.553975.
Train: 2018-07-31T09:34:29.121092: step 8884, loss 0.501983.
Train: 2018-07-31T09:34:29.292954: step 8885, loss 0.527885.
Train: 2018-07-31T09:34:29.464788: step 8886, loss 0.605987.
Train: 2018-07-31T09:34:29.620975: step 8887, loss 0.580114.
Train: 2018-07-31T09:34:29.792809: step 8888, loss 0.527808.
Train: 2018-07-31T09:34:29.949053: step 8889, loss 0.588676.
Train: 2018-07-31T09:34:30.105237: step 8890, loss 0.510386.
Test: 2018-07-31T09:34:30.339556: step 8890, loss 0.548046.
Train: 2018-07-31T09:34:30.511422: step 8891, loss 0.484084.
Train: 2018-07-31T09:34:30.667632: step 8892, loss 0.588761.
Train: 2018-07-31T09:34:30.823857: step 8893, loss 0.545115.
Train: 2018-07-31T09:34:30.995684: step 8894, loss 0.615236.
Train: 2018-07-31T09:34:31.151897: step 8895, loss 0.589075.
Train: 2018-07-31T09:34:31.323702: step 8896, loss 0.650575.
Train: 2018-07-31T09:34:31.495566: step 8897, loss 0.545124.
Train: 2018-07-31T09:34:31.651750: step 8898, loss 0.553898.
Train: 2018-07-31T09:34:31.823585: step 8899, loss 0.580026.
Train: 2018-07-31T09:34:31.979797: step 8900, loss 0.545179.
Test: 2018-07-31T09:34:32.214119: step 8900, loss 0.548075.
Train: 2018-07-31T09:34:32.979594: step 8901, loss 0.640673.
Train: 2018-07-31T09:34:33.151428: step 8902, loss 0.571418.
Train: 2018-07-31T09:34:33.307612: step 8903, loss 0.536704.
Train: 2018-07-31T09:34:33.479479: step 8904, loss 0.554048.
Train: 2018-07-31T09:34:33.651282: step 8905, loss 0.674482.
Train: 2018-07-31T09:34:33.807495: step 8906, loss 0.511135.
Train: 2018-07-31T09:34:33.979329: step 8907, loss 0.545528.
Train: 2018-07-31T09:34:34.135542: step 8908, loss 0.536877.
Train: 2018-07-31T09:34:34.291756: step 8909, loss 0.526237.
Train: 2018-07-31T09:34:34.463590: step 8910, loss 0.579764.
Test: 2018-07-31T09:34:34.697942: step 8910, loss 0.548337.
Train: 2018-07-31T09:34:34.901021: step 8911, loss 0.571278.
Train: 2018-07-31T09:34:35.057234: step 8912, loss 0.605163.
Train: 2018-07-31T09:34:35.229037: step 8913, loss 0.562477.
Train: 2018-07-31T09:34:35.400902: step 8914, loss 0.52859.
Train: 2018-07-31T09:34:35.572705: step 8915, loss 0.596503.
Train: 2018-07-31T09:34:35.728950: step 8916, loss 0.545632.
Train: 2018-07-31T09:34:35.900784: step 8917, loss 0.52863.
Train: 2018-07-31T09:34:36.056999: step 8918, loss 0.596594.
Train: 2018-07-31T09:34:36.213181: step 8919, loss 0.571148.
Train: 2018-07-31T09:34:36.385041: step 8920, loss 0.630529.
Test: 2018-07-31T09:34:36.619362: step 8920, loss 0.548453.
Train: 2018-07-31T09:34:36.791170: step 8921, loss 0.554134.
Train: 2018-07-31T09:34:36.947415: step 8922, loss 0.587953.
Train: 2018-07-31T09:34:37.119219: step 8923, loss 0.613381.
Train: 2018-07-31T09:34:37.291078: step 8924, loss 0.579385.
Train: 2018-07-31T09:34:37.447268: step 8925, loss 0.587864.
Train: 2018-07-31T09:34:37.619133: step 8926, loss 0.512483.
Train: 2018-07-31T09:34:37.775316: step 8927, loss 0.579341.
Train: 2018-07-31T09:34:37.947150: step 8928, loss 0.604419.
Train: 2018-07-31T09:34:38.103399: step 8929, loss 0.587658.
Train: 2018-07-31T09:34:38.275198: step 8930, loss 0.512787.
Test: 2018-07-31T09:34:38.509518: step 8930, loss 0.548824.
Train: 2018-07-31T09:34:38.665759: step 8931, loss 0.629092.
Train: 2018-07-31T09:34:38.837566: step 8932, loss 0.55442.
Train: 2018-07-31T09:34:38.993806: step 8933, loss 0.51313.
Train: 2018-07-31T09:34:39.165647: step 8934, loss 0.554451.
Train: 2018-07-31T09:34:39.321858: step 8935, loss 0.579134.
Train: 2018-07-31T09:34:39.493694: step 8936, loss 0.55455.
Train: 2018-07-31T09:34:39.649876: step 8937, loss 0.595876.
Train: 2018-07-31T09:34:39.821741: step 8938, loss 0.53796.
Train: 2018-07-31T09:34:39.977955: step 8939, loss 0.538038.
Train: 2018-07-31T09:34:40.149760: step 8940, loss 0.629043.
Test: 2018-07-31T09:34:40.384111: step 8940, loss 0.548841.
Train: 2018-07-31T09:34:40.555945: step 8941, loss 0.529522.
Train: 2018-07-31T09:34:40.727773: step 8942, loss 0.587624.
Train: 2018-07-31T09:34:40.883962: step 8943, loss 0.579275.
Train: 2018-07-31T09:34:41.055829: step 8944, loss 0.537944.
Train: 2018-07-31T09:34:41.212041: step 8945, loss 0.595875.
Train: 2018-07-31T09:34:41.383845: step 8946, loss 0.612586.
Train: 2018-07-31T09:34:41.540058: step 8947, loss 0.620834.
Train: 2018-07-31T09:34:41.711924: step 8948, loss 0.579458.
Train: 2018-07-31T09:34:41.883728: step 8949, loss 0.562762.
Train: 2018-07-31T09:34:42.039974: step 8950, loss 0.579275.
Test: 2018-07-31T09:34:42.274264: step 8950, loss 0.548977.
Train: 2018-07-31T09:34:42.446097: step 8951, loss 0.620643.
Train: 2018-07-31T09:34:42.617932: step 8952, loss 0.57937.
Train: 2018-07-31T09:34:42.789766: step 8953, loss 0.579246.
Train: 2018-07-31T09:34:42.946010: step 8954, loss 0.587383.
Train: 2018-07-31T09:34:43.117845: step 8955, loss 0.611645.
Train: 2018-07-31T09:34:43.289681: step 8956, loss 0.53046.
Train: 2018-07-31T09:34:43.461514: step 8957, loss 0.562818.
Train: 2018-07-31T09:34:43.633349: step 8958, loss 0.52246.
Train: 2018-07-31T09:34:43.789563: step 8959, loss 0.579153.
Train: 2018-07-31T09:34:43.961366: step 8960, loss 0.498296.
Test: 2018-07-31T09:34:44.195717: step 8960, loss 0.549351.
Train: 2018-07-31T09:34:44.367546: step 8961, loss 0.498044.
Train: 2018-07-31T09:34:44.523736: step 8962, loss 0.571058.
Train: 2018-07-31T09:34:44.695570: step 8963, loss 0.546595.
Train: 2018-07-31T09:34:44.867405: step 8964, loss 0.579244.
Train: 2018-07-31T09:34:45.039239: step 8965, loss 0.595626.
Train: 2018-07-31T09:34:45.195477: step 8966, loss 0.521457.
Train: 2018-07-31T09:34:45.367289: step 8967, loss 0.529544.
Train: 2018-07-31T09:34:45.523500: step 8968, loss 0.546136.
Train: 2018-07-31T09:34:45.695336: step 8969, loss 0.587774.
Train: 2018-07-31T09:34:45.867201: step 8970, loss 0.604445.
Test: 2018-07-31T09:34:46.101490: step 8970, loss 0.548633.
Train: 2018-07-31T09:34:46.273357: step 8971, loss 0.621245.
Train: 2018-07-31T09:34:46.429539: step 8972, loss 0.554265.
Train: 2018-07-31T09:34:46.601373: step 8973, loss 0.545827.
Train: 2018-07-31T09:34:46.757617: step 8974, loss 0.596211.
Train: 2018-07-31T09:34:46.929453: step 8975, loss 0.613117.
Train: 2018-07-31T09:34:47.085636: step 8976, loss 0.554234.
Train: 2018-07-31T09:34:47.257500: step 8977, loss 0.554177.
Train: 2018-07-31T09:34:47.429336: step 8978, loss 0.478659.
Train: 2018-07-31T09:34:47.601164: step 8979, loss 0.545875.
Train: 2018-07-31T09:34:47.773005: step 8980, loss 0.596347.
Test: 2018-07-31T09:34:48.007294: step 8980, loss 0.548482.
Train: 2018-07-31T09:34:48.179161: step 8981, loss 0.562582.
Train: 2018-07-31T09:34:48.350964: step 8982, loss 0.554245.
Train: 2018-07-31T09:34:48.507201: step 8983, loss 0.545687.
Train: 2018-07-31T09:34:48.679043: step 8984, loss 0.579347.
Train: 2018-07-31T09:34:48.835259: step 8985, loss 0.53707.
Train: 2018-07-31T09:34:49.007091: step 8986, loss 0.56261.
Train: 2018-07-31T09:34:49.163298: step 8987, loss 0.519999.
Train: 2018-07-31T09:34:49.335139: step 8988, loss 0.579646.
Train: 2018-07-31T09:34:49.506944: step 8989, loss 0.554083.
Train: 2018-07-31T09:34:49.678802: step 8990, loss 0.639682.
Test: 2018-07-31T09:34:49.913098: step 8990, loss 0.548226.
Train: 2018-07-31T09:34:50.084933: step 8991, loss 0.511023.
Train: 2018-07-31T09:34:50.241177: step 8992, loss 0.511057.
Train: 2018-07-31T09:34:50.397390: step 8993, loss 0.545449.
Train: 2018-07-31T09:34:50.569218: step 8994, loss 0.588536.
Train: 2018-07-31T09:34:50.756681: step 8995, loss 0.545306.
Train: 2018-07-31T09:34:50.912896: step 8996, loss 0.649293.
Train: 2018-07-31T09:34:51.084729: step 8997, loss 0.622922.
Train: 2018-07-31T09:34:51.240937: step 8998, loss 0.553815.
Train: 2018-07-31T09:34:51.412779: step 8999, loss 0.579775.
Train: 2018-07-31T09:34:51.568991: step 9000, loss 0.545459.
Test: 2018-07-31T09:34:51.818933: step 9000, loss 0.548189.
Train: 2018-07-31T09:34:52.568757: step 9001, loss 0.562774.
Train: 2018-07-31T09:34:52.756183: step 9002, loss 0.502675.
Train: 2018-07-31T09:34:52.912396: step 9003, loss 0.579925.
Train: 2018-07-31T09:34:53.084231: step 9004, loss 0.588303.
Train: 2018-07-31T09:34:53.256096: step 9005, loss 0.605457.
Train: 2018-07-31T09:34:53.427931: step 9006, loss 0.53687.
Train: 2018-07-31T09:34:53.584144: step 9007, loss 0.571.
Train: 2018-07-31T09:34:53.755949: step 9008, loss 0.579717.
Train: 2018-07-31T09:34:53.927814: step 9009, loss 0.502921.
Train: 2018-07-31T09:34:54.099654: step 9010, loss 0.605308.
Test: 2018-07-31T09:34:54.333938: step 9010, loss 0.548274.
Train: 2018-07-31T09:34:54.505772: step 9011, loss 0.562569.
Train: 2018-07-31T09:34:54.662018: step 9012, loss 0.59679.
Train: 2018-07-31T09:34:54.833845: step 9013, loss 0.545582.
Train: 2018-07-31T09:34:54.990059: step 9014, loss 0.571013.
Train: 2018-07-31T09:34:55.161869: step 9015, loss 0.588184.
Train: 2018-07-31T09:34:55.318113: step 9016, loss 0.554097.
Train: 2018-07-31T09:34:55.489916: step 9017, loss 0.545577.
Train: 2018-07-31T09:34:55.661779: step 9018, loss 0.58807.
Train: 2018-07-31T09:34:55.817995: step 9019, loss 0.579623.
Train: 2018-07-31T09:34:55.989800: step 9020, loss 0.537303.
Test: 2018-07-31T09:34:56.224120: step 9020, loss 0.548405.
Train: 2018-07-31T09:34:56.395985: step 9021, loss 0.621846.
Train: 2018-07-31T09:34:56.552200: step 9022, loss 0.587969.
Train: 2018-07-31T09:34:56.724034: step 9023, loss 0.520482.
Train: 2018-07-31T09:34:56.880247: step 9024, loss 0.520402.
Train: 2018-07-31T09:34:57.052051: step 9025, loss 0.503478.
Train: 2018-07-31T09:34:57.208295: step 9026, loss 0.520245.
Train: 2018-07-31T09:34:57.364515: step 9027, loss 0.503149.
Train: 2018-07-31T09:34:57.536345: step 9028, loss 0.545447.
Train: 2018-07-31T09:34:57.692550: step 9029, loss 0.545532.
Train: 2018-07-31T09:34:57.864391: step 9030, loss 0.468165.
Test: 2018-07-31T09:34:58.098712: step 9030, loss 0.548127.
Train: 2018-07-31T09:34:58.270541: step 9031, loss 0.56255.
Train: 2018-07-31T09:34:58.442351: step 9032, loss 0.47568.
Train: 2018-07-31T09:34:58.598595: step 9033, loss 0.553795.
Train: 2018-07-31T09:34:58.770429: step 9034, loss 0.536264.
Train: 2018-07-31T09:34:58.926644: step 9035, loss 0.544823.
Train: 2018-07-31T09:34:59.098448: step 9036, loss 0.535957.
Train: 2018-07-31T09:34:59.270312: step 9037, loss 0.562814.
Train: 2018-07-31T09:34:59.426495: step 9038, loss 0.562723.
Train: 2018-07-31T09:34:59.598361: step 9039, loss 0.48186.
Train: 2018-07-31T09:34:59.754544: step 9040, loss 0.508756.
Test: 2018-07-31T09:35:00.004484: step 9040, loss 0.547742.
Train: 2018-07-31T09:35:00.160730: step 9041, loss 0.517359.
Train: 2018-07-31T09:35:00.332533: step 9042, loss 0.572425.
Train: 2018-07-31T09:35:00.504409: step 9043, loss 0.489875.
Train: 2018-07-31T09:35:00.660606: step 9044, loss 0.683389.
Train: 2018-07-31T09:35:00.832416: step 9045, loss 0.581375.
Train: 2018-07-31T09:35:01.004251: step 9046, loss 0.544738.
Train: 2018-07-31T09:35:01.160489: step 9047, loss 0.618687.
Train: 2018-07-31T09:35:01.332300: step 9048, loss 0.618734.
Train: 2018-07-31T09:35:01.504164: step 9049, loss 0.553842.
Train: 2018-07-31T09:35:01.676000: step 9050, loss 0.535362.
Test: 2018-07-31T09:35:01.910289: step 9050, loss 0.547741.
Train: 2018-07-31T09:35:02.082148: step 9051, loss 0.55395.
Train: 2018-07-31T09:35:02.253988: step 9052, loss 0.554024.
Train: 2018-07-31T09:35:02.425794: step 9053, loss 0.5261.
Train: 2018-07-31T09:35:02.597627: step 9054, loss 0.599927.
Train: 2018-07-31T09:35:02.769462: step 9055, loss 0.553929.
Train: 2018-07-31T09:35:02.925676: step 9056, loss 0.481365.
Train: 2018-07-31T09:35:03.097512: step 9057, loss 0.544857.
Train: 2018-07-31T09:35:03.269347: step 9058, loss 0.54477.
Train: 2018-07-31T09:35:03.441210: step 9059, loss 0.50832.
Train: 2018-07-31T09:35:03.597394: step 9060, loss 0.504588.
Test: 2018-07-31T09:35:03.831714: step 9060, loss 0.547736.
Train: 2018-07-31T09:35:04.003578: step 9061, loss 0.535663.
Train: 2018-07-31T09:35:04.175383: step 9062, loss 0.58149.
Train: 2018-07-31T09:35:04.331596: step 9063, loss 0.53561.
Train: 2018-07-31T09:35:04.503462: step 9064, loss 0.526584.
Train: 2018-07-31T09:35:04.675266: step 9065, loss 0.544769.
Train: 2018-07-31T09:35:04.831480: step 9066, loss 0.544911.
Train: 2018-07-31T09:35:04.987693: step 9067, loss 0.517272.
Train: 2018-07-31T09:35:05.159560: step 9068, loss 0.489747.
Train: 2018-07-31T09:35:05.315740: step 9069, loss 0.526366.
Train: 2018-07-31T09:35:05.487575: step 9070, loss 0.60017.
Test: 2018-07-31T09:35:05.721898: step 9070, loss 0.547747.
Train: 2018-07-31T09:35:05.893762: step 9071, loss 0.554129.
Train: 2018-07-31T09:35:06.049976: step 9072, loss 0.590878.
Train: 2018-07-31T09:35:06.221809: step 9073, loss 0.572496.
Train: 2018-07-31T09:35:06.393614: step 9074, loss 0.609405.
Train: 2018-07-31T09:35:06.549828: step 9075, loss 0.498728.
Train: 2018-07-31T09:35:06.721662: step 9076, loss 0.517016.
Train: 2018-07-31T09:35:06.877875: step 9077, loss 0.507757.
Train: 2018-07-31T09:35:07.049742: step 9078, loss 0.609421.
Train: 2018-07-31T09:35:07.221545: step 9079, loss 0.563468.
Train: 2018-07-31T09:35:07.393379: step 9080, loss 0.517035.
Test: 2018-07-31T09:35:07.627700: step 9080, loss 0.547736.
Train: 2018-07-31T09:35:07.799564: step 9081, loss 0.554026.
Train: 2018-07-31T09:35:07.955772: step 9082, loss 0.526459.
Train: 2018-07-31T09:35:08.127613: step 9083, loss 0.636837.
Train: 2018-07-31T09:35:08.283828: step 9084, loss 0.645436.
Train: 2018-07-31T09:35:08.455660: step 9085, loss 0.654498.
Train: 2018-07-31T09:35:08.627467: step 9086, loss 0.563069.
Train: 2018-07-31T09:35:08.783711: step 9087, loss 0.499525.
Train: 2018-07-31T09:35:08.955544: step 9088, loss 0.526898.
Train: 2018-07-31T09:35:09.111728: step 9089, loss 0.553768.
Train: 2018-07-31T09:35:09.283594: step 9090, loss 0.562769.
Test: 2018-07-31T09:35:09.517917: step 9090, loss 0.547773.
Train: 2018-07-31T09:35:09.689716: step 9091, loss 0.616347.
Train: 2018-07-31T09:35:09.861551: step 9092, loss 0.616344.
Train: 2018-07-31T09:35:10.017796: step 9093, loss 0.562619.
Train: 2018-07-31T09:35:10.189600: step 9094, loss 0.536205.
Train: 2018-07-31T09:35:10.345845: step 9095, loss 0.606511.
Train: 2018-07-31T09:35:10.533294: step 9096, loss 0.544985.
Train: 2018-07-31T09:35:10.689513: step 9097, loss 0.588743.
Train: 2018-07-31T09:35:10.845726: step 9098, loss 0.56267.
Train: 2018-07-31T09:35:11.017561: step 9099, loss 0.545265.
Train: 2018-07-31T09:35:11.173744: step 9100, loss 0.527867.
Test: 2018-07-31T09:35:11.423686: step 9100, loss 0.548084.
Train: 2018-07-31T09:35:12.126677: step 9101, loss 0.605749.
Train: 2018-07-31T09:35:12.298512: step 9102, loss 0.5626.
Train: 2018-07-31T09:35:12.454695: step 9103, loss 0.579667.
Train: 2018-07-31T09:35:12.642151: step 9104, loss 0.639681.
Train: 2018-07-31T09:35:12.798364: step 9105, loss 0.571169.
Train: 2018-07-31T09:35:12.970228: step 9106, loss 0.562547.
Train: 2018-07-31T09:35:13.142072: step 9107, loss 0.554093.
Train: 2018-07-31T09:35:13.313899: step 9108, loss 0.596327.
Train: 2018-07-31T09:35:13.485733: step 9109, loss 0.554207.
Train: 2018-07-31T09:35:13.657538: step 9110, loss 0.545753.
Test: 2018-07-31T09:35:13.907512: step 9110, loss 0.548553.
Train: 2018-07-31T09:35:14.063693: step 9111, loss 0.545851.
Train: 2018-07-31T09:35:14.235527: step 9112, loss 0.604403.
Train: 2018-07-31T09:35:14.407362: step 9113, loss 0.570963.
Train: 2018-07-31T09:35:14.563600: step 9114, loss 0.579147.
Train: 2018-07-31T09:35:14.735440: step 9115, loss 0.529303.
Train: 2018-07-31T09:35:14.907244: step 9116, loss 0.596061.
Train: 2018-07-31T09:35:15.079079: step 9117, loss 0.58774.
Train: 2018-07-31T09:35:15.235294: step 9118, loss 0.562667.
Train: 2018-07-31T09:35:15.407129: step 9119, loss 0.570904.
Train: 2018-07-31T09:35:15.578993: step 9120, loss 0.604095.
Test: 2018-07-31T09:35:15.813285: step 9120, loss 0.548856.
Train: 2018-07-31T09:35:15.985147: step 9121, loss 0.513129.
Train: 2018-07-31T09:35:16.141361: step 9122, loss 0.521441.
Train: 2018-07-31T09:35:16.313166: step 9123, loss 0.53791.
Train: 2018-07-31T09:35:16.485031: step 9124, loss 0.579397.
Train: 2018-07-31T09:35:16.641245: step 9125, loss 0.57927.
Train: 2018-07-31T09:35:16.813079: step 9126, loss 0.570833.
Train: 2018-07-31T09:35:16.969262: step 9127, loss 0.537769.
Train: 2018-07-31T09:35:17.141097: step 9128, loss 0.537502.
Train: 2018-07-31T09:35:17.297310: step 9129, loss 0.529291.
Train: 2018-07-31T09:35:17.484792: step 9130, loss 0.562642.
Test: 2018-07-31T09:35:17.719087: step 9130, loss 0.548564.
Train: 2018-07-31T09:35:17.890947: step 9131, loss 0.537408.
Train: 2018-07-31T09:35:18.047166: step 9132, loss 0.613022.
Train: 2018-07-31T09:35:18.219000: step 9133, loss 0.562528.
Train: 2018-07-31T09:35:18.390834: step 9134, loss 0.55404.
Train: 2018-07-31T09:35:18.562640: step 9135, loss 0.59632.
Train: 2018-07-31T09:35:18.734499: step 9136, loss 0.52033.
Train: 2018-07-31T09:35:18.890717: step 9137, loss 0.520261.
Train: 2018-07-31T09:35:19.078144: step 9138, loss 0.477722.
Train: 2018-07-31T09:35:19.234356: step 9139, loss 0.579528.
Train: 2018-07-31T09:35:19.406223: step 9140, loss 0.562532.
Test: 2018-07-31T09:35:19.640545: step 9140, loss 0.548152.
Train: 2018-07-31T09:35:19.812371: step 9141, loss 0.51954.
Train: 2018-07-31T09:35:19.984182: step 9142, loss 0.657296.
Train: 2018-07-31T09:35:20.140425: step 9143, loss 0.579821.
Train: 2018-07-31T09:35:20.312230: step 9144, loss 0.528064.
Train: 2018-07-31T09:35:20.484071: step 9145, loss 0.554023.
Train: 2018-07-31T09:35:20.640308: step 9146, loss 0.571118.
Train: 2018-07-31T09:35:20.812142: step 9147, loss 0.562714.
Train: 2018-07-31T09:35:20.983948: step 9148, loss 0.536595.
Train: 2018-07-31T09:35:21.140160: step 9149, loss 0.536497.
Train: 2018-07-31T09:35:21.296374: step 9150, loss 0.536425.
Test: 2018-07-31T09:35:21.546315: step 9150, loss 0.547959.
Train: 2018-07-31T09:35:21.718150: step 9151, loss 0.553689.
Train: 2018-07-31T09:35:21.874394: step 9152, loss 0.527487.
Train: 2018-07-31T09:35:22.046197: step 9153, loss 0.544951.
Train: 2018-07-31T09:35:22.218064: step 9154, loss 0.580274.
Train: 2018-07-31T09:35:22.389892: step 9155, loss 0.597713.
Train: 2018-07-31T09:35:22.546081: step 9156, loss 0.615076.
Train: 2018-07-31T09:35:22.717946: step 9157, loss 0.474774.
Train: 2018-07-31T09:35:22.889751: step 9158, loss 0.580352.
Train: 2018-07-31T09:35:23.061586: step 9159, loss 0.598114.
Train: 2018-07-31T09:35:23.217799: step 9160, loss 0.527263.
Test: 2018-07-31T09:35:23.467771: step 9160, loss 0.547862.
Train: 2018-07-31T09:35:23.639576: step 9161, loss 0.615482.
Train: 2018-07-31T09:35:23.795790: step 9162, loss 0.659525.
Train: 2018-07-31T09:35:23.967622: step 9163, loss 0.571455.
Train: 2018-07-31T09:35:24.123867: step 9164, loss 0.553689.
Train: 2018-07-31T09:35:24.295703: step 9165, loss 0.545219.
Train: 2018-07-31T09:35:24.467536: step 9166, loss 0.597415.
Train: 2018-07-31T09:35:24.639342: step 9167, loss 0.519202.
Train: 2018-07-31T09:35:24.795555: step 9168, loss 0.597103.
Train: 2018-07-31T09:35:24.967415: step 9169, loss 0.536669.
Train: 2018-07-31T09:35:25.139224: step 9170, loss 0.588308.
Test: 2018-07-31T09:35:25.373576: step 9170, loss 0.548106.
Train: 2018-07-31T09:35:25.529758: step 9171, loss 0.493637.
Train: 2018-07-31T09:35:25.701623: step 9172, loss 0.56258.
Train: 2018-07-31T09:35:25.873457: step 9173, loss 0.553947.
Train: 2018-07-31T09:35:26.029672: step 9174, loss 0.519394.
Train: 2018-07-31T09:35:26.201505: step 9175, loss 0.665904.
Train: 2018-07-31T09:35:26.357718: step 9176, loss 0.545254.
Train: 2018-07-31T09:35:26.529555: step 9177, loss 0.605441.
Train: 2018-07-31T09:35:26.701389: step 9178, loss 0.596853.
Train: 2018-07-31T09:35:26.857602: step 9179, loss 0.47703.
Train: 2018-07-31T09:35:27.045027: step 9180, loss 0.579493.
Test: 2018-07-31T09:35:27.279381: step 9180, loss 0.548209.
Train: 2018-07-31T09:35:27.435591: step 9181, loss 0.545407.
Train: 2018-07-31T09:35:27.607427: step 9182, loss 0.536916.
Train: 2018-07-31T09:35:27.779262: step 9183, loss 0.648065.
Train: 2018-07-31T09:35:27.935474: step 9184, loss 0.511278.
Train: 2018-07-31T09:35:28.107310: step 9185, loss 0.579679.
Train: 2018-07-31T09:35:28.279145: step 9186, loss 0.571167.
Train: 2018-07-31T09:35:28.450978: step 9187, loss 0.511335.
Train: 2018-07-31T09:35:28.622814: step 9188, loss 0.605196.
Train: 2018-07-31T09:35:28.778997: step 9189, loss 0.511464.
Train: 2018-07-31T09:35:28.950831: step 9190, loss 0.579415.
Test: 2018-07-31T09:35:29.200803: step 9190, loss 0.548224.
Train: 2018-07-31T09:35:29.372638: step 9191, loss 0.596571.
Train: 2018-07-31T09:35:29.544468: step 9192, loss 0.536921.
Train: 2018-07-31T09:35:29.700687: step 9193, loss 0.562568.
Train: 2018-07-31T09:35:29.872491: step 9194, loss 0.468622.
Train: 2018-07-31T09:35:30.028704: step 9195, loss 0.639536.
Train: 2018-07-31T09:35:30.200569: step 9196, loss 0.511076.
Train: 2018-07-31T09:35:30.356751: step 9197, loss 0.545409.
Train: 2018-07-31T09:35:30.528624: step 9198, loss 0.53677.
Train: 2018-07-31T09:35:30.684825: step 9199, loss 0.571265.
Train: 2018-07-31T09:35:30.856635: step 9200, loss 0.502032.
Test: 2018-07-31T09:35:31.106586: step 9200, loss 0.548044.
Train: 2018-07-31T09:35:31.825189: step 9201, loss 0.597224.
Train: 2018-07-31T09:35:31.996992: step 9202, loss 0.562421.
Train: 2018-07-31T09:35:32.168859: step 9203, loss 0.56244.
Train: 2018-07-31T09:35:32.340693: step 9204, loss 0.519114.
Train: 2018-07-31T09:35:32.496910: step 9205, loss 0.605857.
Train: 2018-07-31T09:35:32.653122: step 9206, loss 0.571423.
Train: 2018-07-31T09:35:32.824924: step 9207, loss 0.553822.
Train: 2018-07-31T09:35:32.996758: step 9208, loss 0.614774.
Train: 2018-07-31T09:35:33.153004: step 9209, loss 0.51021.
Train: 2018-07-31T09:35:33.324838: step 9210, loss 0.579976.
Test: 2018-07-31T09:35:33.543538: step 9210, loss 0.547948.
Train: 2018-07-31T09:35:33.715371: step 9211, loss 0.636861.
Train: 2018-07-31T09:35:33.871585: step 9212, loss 0.466711.
Train: 2018-07-31T09:35:34.043389: step 9213, loss 0.518963.
Train: 2018-07-31T09:35:34.215254: step 9214, loss 0.597593.
Train: 2018-07-31T09:35:34.371436: step 9215, loss 0.527627.
Train: 2018-07-31T09:35:34.543303: step 9216, loss 0.632439.
Train: 2018-07-31T09:35:34.715107: step 9217, loss 0.579821.
Train: 2018-07-31T09:35:34.886943: step 9218, loss 0.58852.
Train: 2018-07-31T09:35:35.043154: step 9219, loss 0.597302.
Train: 2018-07-31T09:35:35.215019: step 9220, loss 0.536644.
Test: 2018-07-31T09:35:35.449310: step 9220, loss 0.548021.
Train: 2018-07-31T09:35:35.621175: step 9221, loss 0.519101.
Train: 2018-07-31T09:35:35.777387: step 9222, loss 0.545232.
Train: 2018-07-31T09:35:35.949222: step 9223, loss 0.536523.
Train: 2018-07-31T09:35:36.121027: step 9224, loss 0.519142.
Train: 2018-07-31T09:35:36.277272: step 9225, loss 0.554029.
Train: 2018-07-31T09:35:36.433484: step 9226, loss 0.553901.
Train: 2018-07-31T09:35:36.605319: step 9227, loss 0.493001.
Train: 2018-07-31T09:35:36.777123: step 9228, loss 0.632536.
Train: 2018-07-31T09:35:36.948988: step 9229, loss 0.597555.
Train: 2018-07-31T09:35:37.120818: step 9230, loss 0.484096.
Test: 2018-07-31T09:35:37.370765: step 9230, loss 0.54793.
Train: 2018-07-31T09:35:37.542601: step 9231, loss 0.615026.
Train: 2018-07-31T09:35:37.698808: step 9232, loss 0.536352.
Train: 2018-07-31T09:35:37.870618: step 9233, loss 0.553912.
Train: 2018-07-31T09:35:38.042453: step 9234, loss 0.562523.
Train: 2018-07-31T09:35:38.198665: step 9235, loss 0.553876.
Train: 2018-07-31T09:35:38.370531: step 9236, loss 0.693788.
Train: 2018-07-31T09:35:38.542336: step 9237, loss 0.518966.
Train: 2018-07-31T09:35:38.698549: step 9238, loss 0.553927.
Train: 2018-07-31T09:35:38.870413: step 9239, loss 0.579886.
Train: 2018-07-31T09:35:39.026596: step 9240, loss 0.562475.
Test: 2018-07-31T09:35:39.276566: step 9240, loss 0.547998.
Train: 2018-07-31T09:35:39.432783: step 9241, loss 0.562544.
Train: 2018-07-31T09:35:39.604587: step 9242, loss 0.545259.
Train: 2018-07-31T09:35:39.776421: step 9243, loss 0.553786.
Train: 2018-07-31T09:35:39.932666: step 9244, loss 0.597052.
Train: 2018-07-31T09:35:40.104500: step 9245, loss 0.510609.
Train: 2018-07-31T09:35:40.260713: step 9246, loss 0.510797.
Train: 2018-07-31T09:35:40.432549: step 9247, loss 0.571083.
Train: 2018-07-31T09:35:40.604355: step 9248, loss 0.605736.
Train: 2018-07-31T09:35:40.776186: step 9249, loss 0.527866.
Train: 2018-07-31T09:35:40.948021: step 9250, loss 0.545157.
Test: 2018-07-31T09:35:41.182343: step 9250, loss 0.548022.
Train: 2018-07-31T09:35:41.354201: step 9251, loss 0.519149.
Train: 2018-07-31T09:35:41.510421: step 9252, loss 0.605868.
Train: 2018-07-31T09:35:41.682224: step 9253, loss 0.562681.
Train: 2018-07-31T09:35:41.854090: step 9254, loss 0.562527.
Train: 2018-07-31T09:35:42.010303: step 9255, loss 0.61445.
Train: 2018-07-31T09:35:42.182108: step 9256, loss 0.501837.
Train: 2018-07-31T09:35:42.338357: step 9257, loss 0.510633.
Train: 2018-07-31T09:35:42.510157: step 9258, loss 0.536461.
Train: 2018-07-31T09:35:42.682020: step 9259, loss 0.52762.
Train: 2018-07-31T09:35:42.838204: step 9260, loss 0.571321.
Test: 2018-07-31T09:35:43.072524: step 9260, loss 0.547921.
Train: 2018-07-31T09:35:43.244360: step 9261, loss 0.475192.
Train: 2018-07-31T09:35:43.416193: step 9262, loss 0.606373.
Train: 2018-07-31T09:35:43.588029: step 9263, loss 0.536236.
Train: 2018-07-31T09:35:43.744242: step 9264, loss 0.56262.
Train: 2018-07-31T09:35:43.916077: step 9265, loss 0.580228.
Train: 2018-07-31T09:35:44.072291: step 9266, loss 0.509588.
Train: 2018-07-31T09:35:44.244125: step 9267, loss 0.509694.
Train: 2018-07-31T09:35:44.415960: step 9268, loss 0.580448.
Train: 2018-07-31T09:35:44.572203: step 9269, loss 0.562513.
Train: 2018-07-31T09:35:44.744009: step 9270, loss 0.606897.
Test: 2018-07-31T09:35:44.978328: step 9270, loss 0.54776.
Train: 2018-07-31T09:35:45.150164: step 9271, loss 0.518074.
Train: 2018-07-31T09:35:45.322028: step 9272, loss 0.669777.
Train: 2018-07-31T09:35:45.493833: step 9273, loss 0.535872.
Train: 2018-07-31T09:35:45.665698: step 9274, loss 0.642823.
Train: 2018-07-31T09:35:45.837526: step 9275, loss 0.527079.
Train: 2018-07-31T09:35:45.993715: step 9276, loss 0.65114.
Train: 2018-07-31T09:35:46.165551: step 9277, loss 0.544759.
Train: 2018-07-31T09:35:46.337385: step 9278, loss 0.624363.
Train: 2018-07-31T09:35:46.509220: step 9279, loss 0.579998.
Train: 2018-07-31T09:35:46.665433: step 9280, loss 0.510275.
Test: 2018-07-31T09:35:46.915399: step 9280, loss 0.547967.
Train: 2018-07-31T09:35:47.071618: step 9281, loss 0.623312.
Train: 2018-07-31T09:35:47.243424: step 9282, loss 0.58852.
Train: 2018-07-31T09:35:47.415257: step 9283, loss 0.553939.
Train: 2018-07-31T09:35:47.571470: step 9284, loss 0.571164.
Train: 2018-07-31T09:35:47.743305: step 9285, loss 0.477041.
Train: 2018-07-31T09:35:47.899550: step 9286, loss 0.502554.
Train: 2018-07-31T09:35:48.071384: step 9287, loss 0.536852.
Train: 2018-07-31T09:35:48.243219: step 9288, loss 0.579682.
Train: 2018-07-31T09:35:48.399434: step 9289, loss 0.596711.
Train: 2018-07-31T09:35:48.571236: step 9290, loss 0.571072.
Test: 2018-07-31T09:35:48.805589: step 9290, loss 0.548173.
Train: 2018-07-31T09:35:48.977417: step 9291, loss 0.511256.
Train: 2018-07-31T09:35:49.133604: step 9292, loss 0.528239.
Train: 2018-07-31T09:35:49.305465: step 9293, loss 0.528051.
Train: 2018-07-31T09:35:49.477274: step 9294, loss 0.605528.
Train: 2018-07-31T09:35:49.633518: step 9295, loss 0.596945.
Train: 2018-07-31T09:35:49.805353: step 9296, loss 0.614104.
Train: 2018-07-31T09:35:49.977158: step 9297, loss 0.656795.
Train: 2018-07-31T09:35:50.133372: step 9298, loss 0.553981.
Train: 2018-07-31T09:35:50.289616: step 9299, loss 0.536928.
Train: 2018-07-31T09:35:50.461449: step 9300, loss 0.494168.
Test: 2018-07-31T09:35:50.695765: step 9300, loss 0.548231.
Train: 2018-07-31T09:35:51.445564: step 9301, loss 0.502654.
Train: 2018-07-31T09:35:51.617398: step 9302, loss 0.536917.
Train: 2018-07-31T09:35:51.789233: step 9303, loss 0.605236.
Train: 2018-07-31T09:35:51.945477: step 9304, loss 0.545214.
Train: 2018-07-31T09:35:52.117313: step 9305, loss 0.545312.
Train: 2018-07-31T09:35:52.304767: step 9306, loss 0.511066.
Train: 2018-07-31T09:35:52.476573: step 9307, loss 0.536633.
Train: 2018-07-31T09:35:52.648408: step 9308, loss 0.588177.
Train: 2018-07-31T09:35:52.804651: step 9309, loss 0.519075.
Train: 2018-07-31T09:35:52.976483: step 9310, loss 0.614541.
Test: 2018-07-31T09:35:53.226423: step 9310, loss 0.547997.
Train: 2018-07-31T09:35:53.398263: step 9311, loss 0.527929.
Train: 2018-07-31T09:35:53.570096: step 9312, loss 0.640863.
Train: 2018-07-31T09:35:53.726309: step 9313, loss 0.605728.
Train: 2018-07-31T09:35:53.898146: step 9314, loss 0.588515.
Train: 2018-07-31T09:35:54.069948: step 9315, loss 0.622963.
Train: 2018-07-31T09:35:54.241808: step 9316, loss 0.622819.
Train: 2018-07-31T09:35:54.397998: step 9317, loss 0.485088.
Train: 2018-07-31T09:35:54.569863: step 9318, loss 0.536658.
Train: 2018-07-31T09:35:54.741697: step 9319, loss 0.536611.
Train: 2018-07-31T09:35:54.913533: step 9320, loss 0.622369.
Test: 2018-07-31T09:35:55.147850: step 9320, loss 0.548175.
Train: 2018-07-31T09:35:55.319656: step 9321, loss 0.562514.
Train: 2018-07-31T09:35:55.491491: step 9322, loss 0.622159.
Train: 2018-07-31T09:35:55.647704: step 9323, loss 0.596396.
Train: 2018-07-31T09:35:55.819570: step 9324, loss 0.571135.
Train: 2018-07-31T09:35:55.975752: step 9325, loss 0.570961.
Train: 2018-07-31T09:35:56.131997: step 9326, loss 0.579247.
Train: 2018-07-31T09:35:56.303831: step 9327, loss 0.520686.
Train: 2018-07-31T09:35:56.475635: step 9328, loss 0.56238.
Train: 2018-07-31T09:35:56.647500: step 9329, loss 0.58768.
Train: 2018-07-31T09:35:56.819335: step 9330, loss 0.52082.
Test: 2018-07-31T09:35:57.053625: step 9330, loss 0.548521.
Train: 2018-07-31T09:35:57.225461: step 9331, loss 0.62961.
Train: 2018-07-31T09:35:57.397295: step 9332, loss 0.570713.
Train: 2018-07-31T09:35:57.569129: step 9333, loss 0.579433.
Train: 2018-07-31T09:35:57.725343: step 9334, loss 0.504362.
Train: 2018-07-31T09:35:57.897210: step 9335, loss 0.529304.
Train: 2018-07-31T09:35:58.069013: step 9336, loss 0.579267.
Train: 2018-07-31T09:35:58.240848: step 9337, loss 0.554234.
Train: 2018-07-31T09:35:58.397062: step 9338, loss 0.537662.
Train: 2018-07-31T09:35:58.568896: step 9339, loss 0.529119.
Train: 2018-07-31T09:35:58.740730: step 9340, loss 0.520555.
Test: 2018-07-31T09:35:58.975082: step 9340, loss 0.548448.
Train: 2018-07-31T09:35:59.146886: step 9341, loss 0.612835.
Train: 2018-07-31T09:35:59.303129: step 9342, loss 0.579309.
Train: 2018-07-31T09:35:59.474964: step 9343, loss 0.562297.
Train: 2018-07-31T09:35:59.631147: step 9344, loss 0.562569.
Train: 2018-07-31T09:35:59.802981: step 9345, loss 0.57091.
Train: 2018-07-31T09:35:59.974816: step 9346, loss 0.503239.
Train: 2018-07-31T09:36:00.131054: step 9347, loss 0.562444.
Train: 2018-07-31T09:36:00.302900: step 9348, loss 0.656127.
Train: 2018-07-31T09:36:00.474730: step 9349, loss 0.596446.
Train: 2018-07-31T09:36:00.646534: step 9350, loss 0.579491.
Test: 2018-07-31T09:36:00.880885: step 9350, loss 0.548295.
Train: 2018-07-31T09:36:01.037068: step 9351, loss 0.554289.
Train: 2018-07-31T09:36:01.208932: step 9352, loss 0.579913.
Train: 2018-07-31T09:36:01.380768: step 9353, loss 0.528947.
Train: 2018-07-31T09:36:01.536982: step 9354, loss 0.545794.
Train: 2018-07-31T09:36:01.708784: step 9355, loss 0.528566.
Train: 2018-07-31T09:36:01.880650: step 9356, loss 0.545448.
Train: 2018-07-31T09:36:02.052454: step 9357, loss 0.570486.
Train: 2018-07-31T09:36:02.208698: step 9358, loss 0.579499.
Train: 2018-07-31T09:36:02.380533: step 9359, loss 0.570948.
Train: 2018-07-31T09:36:02.552368: step 9360, loss 0.519839.
Test: 2018-07-31T09:36:02.786690: step 9360, loss 0.548221.
Train: 2018-07-31T09:36:02.958523: step 9361, loss 0.537049.
Train: 2018-07-31T09:36:03.130358: step 9362, loss 0.653897.
Train: 2018-07-31T09:36:03.302193: step 9363, loss 0.519913.
Train: 2018-07-31T09:36:03.458406: step 9364, loss 0.562637.
Train: 2018-07-31T09:36:03.630211: step 9365, loss 0.605527.
Train: 2018-07-31T09:36:03.802076: step 9366, loss 0.57964.
Train: 2018-07-31T09:36:03.958258: step 9367, loss 0.605303.
Train: 2018-07-31T09:36:04.130127: step 9368, loss 0.630757.
Train: 2018-07-31T09:36:04.301958: step 9369, loss 0.613505.
Train: 2018-07-31T09:36:04.473762: step 9370, loss 0.545561.
Test: 2018-07-31T09:36:04.708114: step 9370, loss 0.548384.
Train: 2018-07-31T09:36:04.879943: step 9371, loss 0.528755.
Train: 2018-07-31T09:36:05.051783: step 9372, loss 0.638233.
Train: 2018-07-31T09:36:05.207995: step 9373, loss 0.520588.
Train: 2018-07-31T09:36:05.379801: step 9374, loss 0.579357.
Train: 2018-07-31T09:36:05.536015: step 9375, loss 0.545729.
Train: 2018-07-31T09:36:05.707878: step 9376, loss 0.554381.
Train: 2018-07-31T09:36:05.879685: step 9377, loss 0.56263.
Train: 2018-07-31T09:36:06.051544: step 9378, loss 0.537734.
Train: 2018-07-31T09:36:06.207732: step 9379, loss 0.587532.
Train: 2018-07-31T09:36:06.379566: step 9380, loss 0.562507.
Test: 2018-07-31T09:36:06.613889: step 9380, loss 0.548608.
Train: 2018-07-31T09:36:06.785753: step 9381, loss 0.495743.
Train: 2018-07-31T09:36:06.957582: step 9382, loss 0.570839.
Train: 2018-07-31T09:36:07.113800: step 9383, loss 0.545825.
Train: 2018-07-31T09:36:07.285636: step 9384, loss 0.570803.
Train: 2018-07-31T09:36:07.457470: step 9385, loss 0.545886.
Train: 2018-07-31T09:36:07.613652: step 9386, loss 0.520322.
Train: 2018-07-31T09:36:07.785519: step 9387, loss 0.570825.
Train: 2018-07-31T09:36:07.957353: step 9388, loss 0.604775.
Train: 2018-07-31T09:36:08.113566: step 9389, loss 0.579494.
Train: 2018-07-31T09:36:08.285394: step 9390, loss 0.537219.
Test: 2018-07-31T09:36:08.519723: step 9390, loss 0.548296.
Train: 2018-07-31T09:36:08.691525: step 9391, loss 0.537155.
Train: 2018-07-31T09:36:08.863385: step 9392, loss 0.630553.
Train: 2018-07-31T09:36:09.019605: step 9393, loss 0.511579.
Train: 2018-07-31T09:36:09.191438: step 9394, loss 0.56256.
Train: 2018-07-31T09:36:09.363273: step 9395, loss 0.613712.
Train: 2018-07-31T09:36:09.519456: step 9396, loss 0.579697.
Train: 2018-07-31T09:36:09.691321: step 9397, loss 0.57972.
Train: 2018-07-31T09:36:09.863127: step 9398, loss 0.57971.
Train: 2018-07-31T09:36:10.019371: step 9399, loss 0.537085.
Train: 2018-07-31T09:36:10.191204: step 9400, loss 0.605192.
Test: 2018-07-31T09:36:10.441115: step 9400, loss 0.548278.
Train: 2018-07-31T09:36:11.175349: step 9401, loss 0.553887.
Train: 2018-07-31T09:36:11.331557: step 9402, loss 0.536966.
Train: 2018-07-31T09:36:11.503366: step 9403, loss 0.528596.
Train: 2018-07-31T09:36:11.675201: step 9404, loss 0.520188.
Train: 2018-07-31T09:36:11.831447: step 9405, loss 0.605063.
Train: 2018-07-31T09:36:12.003249: step 9406, loss 0.519918.
Train: 2018-07-31T09:36:12.159487: step 9407, loss 0.511271.
Train: 2018-07-31T09:36:12.331297: step 9408, loss 0.571121.
Train: 2018-07-31T09:36:12.503166: step 9409, loss 0.553888.
Train: 2018-07-31T09:36:12.659377: step 9410, loss 0.57104.
Test: 2018-07-31T09:36:12.909312: step 9410, loss 0.548087.
Train: 2018-07-31T09:36:13.065501: step 9411, loss 0.554142.
Train: 2018-07-31T09:36:13.237335: step 9412, loss 0.622903.
Train: 2018-07-31T09:36:13.409170: step 9413, loss 0.502233.
Train: 2018-07-31T09:36:13.581006: step 9414, loss 0.545377.
Train: 2018-07-31T09:36:13.752874: step 9415, loss 0.554055.
Train: 2018-07-31T09:36:13.924704: step 9416, loss 0.631738.
Train: 2018-07-31T09:36:14.096539: step 9417, loss 0.536453.
Train: 2018-07-31T09:36:14.252753: step 9418, loss 0.527843.
Train: 2018-07-31T09:36:14.424587: step 9419, loss 0.536624.
Train: 2018-07-31T09:36:14.580770: step 9420, loss 0.545209.
Test: 2018-07-31T09:36:14.815116: step 9420, loss 0.547951.
Train: 2018-07-31T09:36:14.986926: step 9421, loss 0.571094.
Train: 2018-07-31T09:36:15.158761: step 9422, loss 0.475439.
Train: 2018-07-31T09:36:15.330620: step 9423, loss 0.580061.
Train: 2018-07-31T09:36:15.502431: step 9424, loss 0.518749.
Train: 2018-07-31T09:36:15.658643: step 9425, loss 0.571601.
Train: 2018-07-31T09:36:15.830479: step 9426, loss 0.492362.
Train: 2018-07-31T09:36:16.002343: step 9427, loss 0.456733.
Train: 2018-07-31T09:36:16.174148: step 9428, loss 0.651278.
Train: 2018-07-31T09:36:16.345983: step 9429, loss 0.544706.
Train: 2018-07-31T09:36:16.502197: step 9430, loss 0.51794.
Test: 2018-07-31T09:36:16.736516: step 9430, loss 0.547709.
Train: 2018-07-31T09:36:16.908350: step 9431, loss 0.517956.
Train: 2018-07-31T09:36:17.064565: step 9432, loss 0.598557.
Train: 2018-07-31T09:36:17.236400: step 9433, loss 0.589738.
Train: 2018-07-31T09:36:17.408265: step 9434, loss 0.61698.
Train: 2018-07-31T09:36:17.580099: step 9435, loss 0.562736.
Train: 2018-07-31T09:36:17.751928: step 9436, loss 0.544941.
Train: 2018-07-31T09:36:17.923768: step 9437, loss 0.463712.
Train: 2018-07-31T09:36:18.095574: step 9438, loss 0.562698.
Train: 2018-07-31T09:36:18.251816: step 9439, loss 0.51779.
Train: 2018-07-31T09:36:18.423652: step 9440, loss 0.526558.
Test: 2018-07-31T09:36:18.657942: step 9440, loss 0.547665.
Train: 2018-07-31T09:36:18.829777: step 9441, loss 0.571781.
Train: 2018-07-31T09:36:19.001610: step 9442, loss 0.553852.
Train: 2018-07-31T09:36:19.168832: step 9443, loss 0.544947.
Train: 2018-07-31T09:36:19.356319: step 9444, loss 0.544503.
Train: 2018-07-31T09:36:19.512505: step 9445, loss 0.52642.
Train: 2018-07-31T09:36:19.684370: step 9446, loss 0.544723.
Train: 2018-07-31T09:36:19.840583: step 9447, loss 0.581189.
Train: 2018-07-31T09:36:20.012418: step 9448, loss 0.62687.
Train: 2018-07-31T09:36:20.184222: step 9449, loss 0.572055.
Train: 2018-07-31T09:36:20.340436: step 9450, loss 0.47188.
Test: 2018-07-31T09:36:20.590377: step 9450, loss 0.547656.
Train: 2018-07-31T09:36:20.746621: step 9451, loss 0.54465.
Train: 2018-07-31T09:36:20.918426: step 9452, loss 0.481127.
Train: 2018-07-31T09:36:21.090260: step 9453, loss 0.663182.
Train: 2018-07-31T09:36:21.262127: step 9454, loss 0.617513.
Train: 2018-07-31T09:36:21.433929: step 9455, loss 0.599297.
Train: 2018-07-31T09:36:21.605764: step 9456, loss 0.508572.
Train: 2018-07-31T09:36:21.777633: step 9457, loss 0.544711.
Train: 2018-07-31T09:36:21.949464: step 9458, loss 0.580862.
Train: 2018-07-31T09:36:22.121270: step 9459, loss 0.553717.
Train: 2018-07-31T09:36:22.277482: step 9460, loss 0.544795.
Test: 2018-07-31T09:36:22.527448: step 9460, loss 0.547691.
Train: 2018-07-31T09:36:22.699282: step 9461, loss 0.634532.
Train: 2018-07-31T09:36:22.917988: step 9462, loss 0.535858.
Train: 2018-07-31T09:36:23.074201: step 9463, loss 0.625121.
Train: 2018-07-31T09:36:23.246006: step 9464, loss 0.527157.
Train: 2018-07-31T09:36:23.402218: step 9465, loss 0.606745.
Train: 2018-07-31T09:36:23.574084: step 9466, loss 0.518572.
Train: 2018-07-31T09:36:23.745919: step 9467, loss 0.571404.
Train: 2018-07-31T09:36:23.917753: step 9468, loss 0.527418.
Train: 2018-07-31T09:36:24.073961: step 9469, loss 0.589044.
Train: 2018-07-31T09:36:24.245770: step 9470, loss 0.641418.
Test: 2018-07-31T09:36:24.495746: step 9470, loss 0.547897.
Train: 2018-07-31T09:36:24.651960: step 9471, loss 0.527757.
Train: 2018-07-31T09:36:24.823761: step 9472, loss 0.562532.
Train: 2018-07-31T09:36:24.995626: step 9473, loss 0.536408.
Train: 2018-07-31T09:36:25.167431: step 9474, loss 0.545184.
Train: 2018-07-31T09:36:25.323674: step 9475, loss 0.536513.
Train: 2018-07-31T09:36:25.495516: step 9476, loss 0.631689.
Train: 2018-07-31T09:36:25.651692: step 9477, loss 0.579776.
Train: 2018-07-31T09:36:25.823557: step 9478, loss 0.631283.
Train: 2018-07-31T09:36:25.995393: step 9479, loss 0.5196.
Train: 2018-07-31T09:36:26.167226: step 9480, loss 0.579608.
Test: 2018-07-31T09:36:26.401551: step 9480, loss 0.54818.
Train: 2018-07-31T09:36:26.573381: step 9481, loss 0.477058.
Train: 2018-07-31T09:36:26.729564: step 9482, loss 0.596595.
Train: 2018-07-31T09:36:26.901399: step 9483, loss 0.647808.
Train: 2018-07-31T09:36:27.057613: step 9484, loss 0.502946.
Train: 2018-07-31T09:36:27.229447: step 9485, loss 0.562508.
Train: 2018-07-31T09:36:27.385693: step 9486, loss 0.58793.
Train: 2018-07-31T09:36:27.557495: step 9487, loss 0.570969.
Train: 2018-07-31T09:36:27.729356: step 9488, loss 0.520186.
Train: 2018-07-31T09:36:27.885544: step 9489, loss 0.630218.
Train: 2018-07-31T09:36:28.057378: step 9490, loss 0.596279.
Test: 2018-07-31T09:36:28.291700: step 9490, loss 0.548402.
Train: 2018-07-31T09:36:28.463557: step 9491, loss 0.570993.
Train: 2018-07-31T09:36:28.635393: step 9492, loss 0.604596.
Train: 2018-07-31T09:36:28.791582: step 9493, loss 0.629593.
Train: 2018-07-31T09:36:28.963416: step 9494, loss 0.545942.
Train: 2018-07-31T09:36:29.119660: step 9495, loss 0.579213.
Train: 2018-07-31T09:36:29.291495: step 9496, loss 0.570913.
Train: 2018-07-31T09:36:29.447677: step 9497, loss 0.570892.
Train: 2018-07-31T09:36:29.619542: step 9498, loss 0.620465.
Train: 2018-07-31T09:36:29.791377: step 9499, loss 0.562693.
Train: 2018-07-31T09:36:29.947591: step 9500, loss 0.505245.
Test: 2018-07-31T09:36:30.197527: step 9500, loss 0.54895.
Train: 2018-07-31T09:36:30.947327: step 9501, loss 0.579088.
Train: 2018-07-31T09:36:31.103566: step 9502, loss 0.538143.
Train: 2018-07-31T09:36:31.275409: step 9503, loss 0.546352.
Train: 2018-07-31T09:36:31.447240: step 9504, loss 0.611913.
Train: 2018-07-31T09:36:31.619075: step 9505, loss 0.562715.
Train: 2018-07-31T09:36:31.790878: step 9506, loss 0.538094.
Train: 2018-07-31T09:36:31.962745: step 9507, loss 0.587297.
Train: 2018-07-31T09:36:32.118928: step 9508, loss 0.652934.
Train: 2018-07-31T09:36:32.290793: step 9509, loss 0.456228.
Train: 2018-07-31T09:36:32.462627: step 9510, loss 0.595472.
Test: 2018-07-31T09:36:32.712538: step 9510, loss 0.548943.
Train: 2018-07-31T09:36:32.868782: step 9511, loss 0.587333.
Train: 2018-07-31T09:36:33.040586: step 9512, loss 0.455995.
Train: 2018-07-31T09:36:33.196801: step 9513, loss 0.52755.
Train: 2018-07-31T09:36:33.368665: step 9514, loss 0.504805.
Train: 2018-07-31T09:36:33.524848: step 9515, loss 0.579168.
Train: 2018-07-31T09:36:33.696714: step 9516, loss 0.612545.
Train: 2018-07-31T09:36:33.868548: step 9517, loss 0.562576.
Train: 2018-07-31T09:36:34.040352: step 9518, loss 0.579299.
Train: 2018-07-31T09:36:34.196596: step 9519, loss 0.486999.
Train: 2018-07-31T09:36:34.368400: step 9520, loss 0.511983.
Test: 2018-07-31T09:36:34.618373: step 9520, loss 0.548322.
Train: 2018-07-31T09:36:34.774589: step 9521, loss 0.59634.
Train: 2018-07-31T09:36:34.946421: step 9522, loss 0.545468.
Train: 2018-07-31T09:36:35.118226: step 9523, loss 0.511363.
Train: 2018-07-31T09:36:35.274472: step 9524, loss 0.570999.
Train: 2018-07-31T09:36:35.446273: step 9525, loss 0.579589.
Train: 2018-07-31T09:36:35.602517: step 9526, loss 0.571043.
Train: 2018-07-31T09:36:35.774356: step 9527, loss 0.553807.
Train: 2018-07-31T09:36:35.946156: step 9528, loss 0.484611.
Train: 2018-07-31T09:36:36.102370: step 9529, loss 0.527785.
Train: 2018-07-31T09:36:36.274204: step 9530, loss 0.579971.
Test: 2018-07-31T09:36:36.508525: step 9530, loss 0.547868.
Train: 2018-07-31T09:36:36.727254: step 9531, loss 0.571283.
Train: 2018-07-31T09:36:36.899092: step 9532, loss 0.667756.
Train: 2018-07-31T09:36:37.055301: step 9533, loss 0.518771.
Train: 2018-07-31T09:36:37.227137: step 9534, loss 0.55372.
Train: 2018-07-31T09:36:37.398975: step 9535, loss 0.527428.
Train: 2018-07-31T09:36:37.570806: step 9536, loss 0.588856.
Train: 2018-07-31T09:36:37.727020: step 9537, loss 0.588873.
Train: 2018-07-31T09:36:37.898858: step 9538, loss 0.553746.
Train: 2018-07-31T09:36:38.070689: step 9539, loss 0.544889.
Train: 2018-07-31T09:36:38.242493: step 9540, loss 0.60648.
Test: 2018-07-31T09:36:38.476844: step 9540, loss 0.547822.
Train: 2018-07-31T09:36:38.648649: step 9541, loss 0.580103.
Train: 2018-07-31T09:36:38.820483: step 9542, loss 0.606358.
Train: 2018-07-31T09:36:38.976726: step 9543, loss 0.57123.
Train: 2018-07-31T09:36:39.148565: step 9544, loss 0.579966.
Train: 2018-07-31T09:36:39.320397: step 9545, loss 0.492817.
Train: 2018-07-31T09:36:39.492231: step 9546, loss 0.553786.
Train: 2018-07-31T09:36:39.648413: step 9547, loss 0.562499.
Train: 2018-07-31T09:36:39.820279: step 9548, loss 0.527684.
Train: 2018-07-31T09:36:39.992083: step 9549, loss 0.579896.
Train: 2018-07-31T09:36:40.148331: step 9550, loss 0.588592.
Test: 2018-07-31T09:36:40.382651: step 9550, loss 0.547942.
Train: 2018-07-31T09:36:40.554452: step 9551, loss 0.510396.
Train: 2018-07-31T09:36:40.726286: step 9552, loss 0.57115.
Train: 2018-07-31T09:36:40.898122: step 9553, loss 0.571188.
Train: 2018-07-31T09:36:41.054336: step 9554, loss 0.510403.
Train: 2018-07-31T09:36:41.226200: step 9555, loss 0.519086.
Train: 2018-07-31T09:36:41.398039: step 9556, loss 0.579829.
Train: 2018-07-31T09:36:41.554219: step 9557, loss 0.623454.
Train: 2018-07-31T09:36:41.726052: step 9558, loss 0.492812.
Train: 2018-07-31T09:36:41.897887: step 9559, loss 0.614745.
Train: 2018-07-31T09:36:42.054131: step 9560, loss 0.518943.
Test: 2018-07-31T09:36:42.304073: step 9560, loss 0.547897.
Train: 2018-07-31T09:36:42.460255: step 9561, loss 0.640943.
Train: 2018-07-31T09:36:42.632123: step 9562, loss 0.527663.
Train: 2018-07-31T09:36:42.803925: step 9563, loss 0.60601.
Train: 2018-07-31T09:36:42.975791: step 9564, loss 0.501601.
Train: 2018-07-31T09:36:43.147628: step 9565, loss 0.597297.
Train: 2018-07-31T09:36:43.319429: step 9566, loss 0.536451.
Train: 2018-07-31T09:36:43.475643: step 9567, loss 0.605929.
Train: 2018-07-31T09:36:43.647511: step 9568, loss 0.588536.
Train: 2018-07-31T09:36:43.803716: step 9569, loss 0.51917.
Train: 2018-07-31T09:36:43.975551: step 9570, loss 0.484651.
Test: 2018-07-31T09:36:44.209877: step 9570, loss 0.547973.
Train: 2018-07-31T09:36:44.381710: step 9571, loss 0.545166.
Train: 2018-07-31T09:36:44.537924: step 9572, loss 0.501774.
Train: 2018-07-31T09:36:44.709759: step 9573, loss 0.527686.
Train: 2018-07-31T09:36:44.881588: step 9574, loss 0.606018.
Train: 2018-07-31T09:36:45.037778: step 9575, loss 0.63228.
Train: 2018-07-31T09:36:45.209644: step 9576, loss 0.510098.
Train: 2018-07-31T09:36:45.381446: step 9577, loss 0.544994.
Train: 2018-07-31T09:36:45.553281: step 9578, loss 0.501335.
Train: 2018-07-31T09:36:45.709525: step 9579, loss 0.63262.
Train: 2018-07-31T09:36:45.881329: step 9580, loss 0.483573.
Test: 2018-07-31T09:36:46.115675: step 9580, loss 0.547818.
Train: 2018-07-31T09:36:46.287511: step 9581, loss 0.562527.
Train: 2018-07-31T09:36:46.459350: step 9582, loss 0.580096.
Train: 2018-07-31T09:36:46.615532: step 9583, loss 0.606554.
Train: 2018-07-31T09:36:46.787367: step 9584, loss 0.553663.
Train: 2018-07-31T09:36:46.959229: step 9585, loss 0.597768.
Train: 2018-07-31T09:36:47.131070: step 9586, loss 0.588909.
Train: 2018-07-31T09:36:47.302895: step 9587, loss 0.606445.
Train: 2018-07-31T09:36:47.459084: step 9588, loss 0.580051.
Train: 2018-07-31T09:36:47.630953: step 9589, loss 0.571171.
Train: 2018-07-31T09:36:47.787134: step 9590, loss 0.632249.
Test: 2018-07-31T09:36:48.037105: step 9590, loss 0.547941.
Train: 2018-07-31T09:36:48.208940: step 9591, loss 0.666647.
Train: 2018-07-31T09:36:48.365153: step 9592, loss 0.588348.
Train: 2018-07-31T09:36:48.536988: step 9593, loss 0.519544.
Train: 2018-07-31T09:36:48.708792: step 9594, loss 0.553955.
Train: 2018-07-31T09:36:48.880660: step 9595, loss 0.596428.
Train: 2018-07-31T09:36:49.052492: step 9596, loss 0.545558.
Train: 2018-07-31T09:36:49.208701: step 9597, loss 0.52017.
Train: 2018-07-31T09:36:49.380510: step 9598, loss 0.59626.
Train: 2018-07-31T09:36:49.552375: step 9599, loss 0.486833.
Train: 2018-07-31T09:36:49.724210: step 9600, loss 0.545667.
Test: 2018-07-31T09:36:49.958500: step 9600, loss 0.548422.
Train: 2018-07-31T09:36:50.708354: step 9601, loss 0.528837.
Train: 2018-07-31T09:36:50.864537: step 9602, loss 0.570846.
Train: 2018-07-31T09:36:51.036403: step 9603, loss 0.54565.
Train: 2018-07-31T09:36:51.208243: step 9604, loss 0.570925.
Train: 2018-07-31T09:36:51.380074: step 9605, loss 0.621623.
Train: 2018-07-31T09:36:51.536255: step 9606, loss 0.562498.
Train: 2018-07-31T09:36:51.708120: step 9607, loss 0.579464.
Train: 2018-07-31T09:36:51.864304: step 9608, loss 0.587784.
Train: 2018-07-31T09:36:52.036169: step 9609, loss 0.56248.
Train: 2018-07-31T09:36:52.207972: step 9610, loss 0.604581.
Test: 2018-07-31T09:36:52.442294: step 9610, loss 0.54843.
Train: 2018-07-31T09:36:52.629773: step 9611, loss 0.596071.
Train: 2018-07-31T09:36:52.801614: step 9612, loss 0.621256.
Train: 2018-07-31T09:36:52.973420: step 9613, loss 0.520748.
Train: 2018-07-31T09:36:53.129633: step 9614, loss 0.529154.
Train: 2018-07-31T09:36:53.301466: step 9615, loss 0.52914.
Train: 2018-07-31T09:36:53.457680: step 9616, loss 0.612665.
Train: 2018-07-31T09:36:53.629515: step 9617, loss 0.604222.
Train: 2018-07-31T09:36:53.801380: step 9618, loss 0.55427.
Train: 2018-07-31T09:36:53.957564: step 9619, loss 0.570879.
Train: 2018-07-31T09:36:54.129398: step 9620, loss 0.545961.
Test: 2018-07-31T09:36:54.363744: step 9620, loss 0.548655.
Train: 2018-07-31T09:36:54.535583: step 9621, loss 0.554302.
Train: 2018-07-31T09:36:54.691796: step 9622, loss 0.629068.
Train: 2018-07-31T09:36:54.863601: step 9623, loss 0.512815.
Train: 2018-07-31T09:36:55.035436: step 9624, loss 0.521059.
Train: 2018-07-31T09:36:55.191679: step 9625, loss 0.629039.
Train: 2018-07-31T09:36:55.363483: step 9626, loss 0.55419.
Train: 2018-07-31T09:36:55.519697: step 9627, loss 0.545965.
Train: 2018-07-31T09:36:55.691558: step 9628, loss 0.5626.
Train: 2018-07-31T09:36:55.863400: step 9629, loss 0.595811.
Train: 2018-07-31T09:36:56.019615: step 9630, loss 0.595892.
Test: 2018-07-31T09:36:56.269559: step 9630, loss 0.548633.
Train: 2018-07-31T09:36:56.425735: step 9631, loss 0.512669.
Train: 2018-07-31T09:36:56.597569: step 9632, loss 0.462679.
Train: 2018-07-31T09:36:56.769405: step 9633, loss 0.595884.
Train: 2018-07-31T09:36:56.925649: step 9634, loss 0.579267.
Train: 2018-07-31T09:36:57.097454: step 9635, loss 0.579356.
Train: 2018-07-31T09:36:57.269288: step 9636, loss 0.47862.
Train: 2018-07-31T09:36:57.441153: step 9637, loss 0.545722.
Train: 2018-07-31T09:36:57.597366: step 9638, loss 0.596302.
Train: 2018-07-31T09:36:57.769205: step 9639, loss 0.630222.
Train: 2018-07-31T09:36:57.925383: step 9640, loss 0.545532.
Test: 2018-07-31T09:36:58.175325: step 9640, loss 0.548267.
Train: 2018-07-31T09:36:58.347161: step 9641, loss 0.553982.
Train: 2018-07-31T09:36:58.519029: step 9642, loss 0.562521.
Train: 2018-07-31T09:36:58.690829: step 9643, loss 0.571043.
Train: 2018-07-31T09:36:58.847043: step 9644, loss 0.605043.
Train: 2018-07-31T09:36:59.018912: step 9645, loss 0.502934.
Train: 2018-07-31T09:36:59.175092: step 9646, loss 0.562508.
Train: 2018-07-31T09:36:59.346957: step 9647, loss 0.545426.
Train: 2018-07-31T09:36:59.503164: step 9648, loss 0.562489.
Train: 2018-07-31T09:36:59.675004: step 9649, loss 0.588098.
Train: 2018-07-31T09:36:59.846809: step 9650, loss 0.519692.
Test: 2018-07-31T09:37:00.096781: step 9650, loss 0.548114.
Train: 2018-07-31T09:37:00.268585: step 9651, loss 0.588135.
Train: 2018-07-31T09:37:00.424829: step 9652, loss 0.502423.
Train: 2018-07-31T09:37:00.596634: step 9653, loss 0.596743.
Train: 2018-07-31T09:37:00.752877: step 9654, loss 0.59679.
Train: 2018-07-31T09:37:00.924681: step 9655, loss 0.49364.
Train: 2018-07-31T09:37:01.096547: step 9656, loss 0.571053.
Train: 2018-07-31T09:37:01.268390: step 9657, loss 0.579744.
Train: 2018-07-31T09:37:01.440187: step 9658, loss 0.519279.
Train: 2018-07-31T09:37:01.612021: step 9659, loss 0.597059.
Train: 2018-07-31T09:37:01.768234: step 9660, loss 0.562496.
Test: 2018-07-31T09:37:02.002588: step 9660, loss 0.54797.
Train: 2018-07-31T09:37:02.174419: step 9661, loss 0.51055.
Train: 2018-07-31T09:37:02.346224: step 9662, loss 0.571144.
Train: 2018-07-31T09:37:02.518059: step 9663, loss 0.51047.
Train: 2018-07-31T09:37:02.674302: step 9664, loss 0.506897.
Train: 2018-07-31T09:37:02.846108: step 9665, loss 0.614798.
Train: 2018-07-31T09:37:03.002353: step 9666, loss 0.527512.
Train: 2018-07-31T09:37:03.174185: step 9667, loss 0.571252.
Train: 2018-07-31T09:37:03.346019: step 9668, loss 0.536265.
Train: 2018-07-31T09:37:03.517825: step 9669, loss 0.580039.
Train: 2018-07-31T09:37:03.689689: step 9670, loss 0.553819.
Test: 2018-07-31T09:37:03.924010: step 9670, loss 0.547791.
Train: 2018-07-31T09:37:04.095814: step 9671, loss 0.500967.
Train: 2018-07-31T09:37:04.267650: step 9672, loss 0.57141.
Train: 2018-07-31T09:37:04.439515: step 9673, loss 0.553672.
Train: 2018-07-31T09:37:04.595727: step 9674, loss 0.589058.
Train: 2018-07-31T09:37:04.767531: step 9675, loss 0.535989.
Train: 2018-07-31T09:37:04.939397: step 9676, loss 0.580289.
Train: 2018-07-31T09:37:05.111202: step 9677, loss 0.544857.
Train: 2018-07-31T09:37:05.283037: step 9678, loss 0.562527.
Train: 2018-07-31T09:37:05.454901: step 9679, loss 0.589239.
Train: 2018-07-31T09:37:05.626706: step 9680, loss 0.606893.
Test: 2018-07-31T09:37:05.845435: step 9680, loss 0.547742.
Train: 2018-07-31T09:37:06.017239: step 9681, loss 0.491685.
Train: 2018-07-31T09:37:06.189073: step 9682, loss 0.473985.
Train: 2018-07-31T09:37:06.360944: step 9683, loss 0.562588.
Train: 2018-07-31T09:37:06.532743: step 9684, loss 0.624722.
Train: 2018-07-31T09:37:06.688957: step 9685, loss 0.518139.
Train: 2018-07-31T09:37:06.860825: step 9686, loss 0.580465.
Train: 2018-07-31T09:37:07.032657: step 9687, loss 0.500406.
Train: 2018-07-31T09:37:07.188870: step 9688, loss 0.527063.
Train: 2018-07-31T09:37:07.360705: step 9689, loss 0.518086.
Train: 2018-07-31T09:37:07.532539: step 9690, loss 0.500198.
Test: 2018-07-31T09:37:07.766855: step 9690, loss 0.547675.
Train: 2018-07-31T09:37:07.938664: step 9691, loss 0.571634.
Train: 2018-07-31T09:37:08.094908: step 9692, loss 0.517706.
Train: 2018-07-31T09:37:08.266746: step 9693, loss 0.517772.
Train: 2018-07-31T09:37:08.438577: step 9694, loss 0.544696.
Train: 2018-07-31T09:37:08.610383: step 9695, loss 0.535678.
Train: 2018-07-31T09:37:08.766596: step 9696, loss 0.48114.
Train: 2018-07-31T09:37:08.938463: step 9697, loss 0.599348.
Train: 2018-07-31T09:37:09.110298: step 9698, loss 0.562971.
Train: 2018-07-31T09:37:09.266503: step 9699, loss 0.517081.
Train: 2018-07-31T09:37:09.453935: step 9700, loss 0.581476.
Test: 2018-07-31T09:37:09.688257: step 9700, loss 0.547631.
Train: 2018-07-31T09:37:10.422458: step 9701, loss 0.471047.
Train: 2018-07-31T09:37:10.594318: step 9702, loss 0.517024.
Train: 2018-07-31T09:37:10.766161: step 9703, loss 0.507718.
Train: 2018-07-31T09:37:10.937961: step 9704, loss 0.572405.
Train: 2018-07-31T09:37:11.094200: step 9705, loss 0.581688.
Train: 2018-07-31T09:37:11.281632: step 9706, loss 0.572528.
Train: 2018-07-31T09:37:11.453465: step 9707, loss 0.591198.
Train: 2018-07-31T09:37:11.625300: step 9708, loss 0.572611.
Train: 2018-07-31T09:37:11.781515: step 9709, loss 0.553939.
Train: 2018-07-31T09:37:11.953349: step 9710, loss 0.553876.
Test: 2018-07-31T09:37:12.187684: step 9710, loss 0.547664.
Train: 2018-07-31T09:37:12.359503: step 9711, loss 0.553981.
Train: 2018-07-31T09:37:12.531368: step 9712, loss 0.572416.
Train: 2018-07-31T09:37:12.687584: step 9713, loss 0.479888.
Train: 2018-07-31T09:37:12.859412: step 9714, loss 0.66509.
Train: 2018-07-31T09:37:13.031252: step 9715, loss 0.461428.
Train: 2018-07-31T09:37:13.187435: step 9716, loss 0.600166.
Train: 2018-07-31T09:37:13.359269: step 9717, loss 0.553906.
Train: 2018-07-31T09:37:13.515483: step 9718, loss 0.535366.
Train: 2018-07-31T09:37:13.687348: step 9719, loss 0.54473.
Train: 2018-07-31T09:37:13.859152: step 9720, loss 0.608882.
Test: 2018-07-31T09:37:14.093475: step 9720, loss 0.547626.
Train: 2018-07-31T09:37:14.265308: step 9721, loss 0.507933.
Train: 2018-07-31T09:37:14.437142: step 9722, loss 0.590419.
Train: 2018-07-31T09:37:14.608976: step 9723, loss 0.526325.
Train: 2018-07-31T09:37:14.780836: step 9724, loss 0.499006.
Train: 2018-07-31T09:37:14.937025: step 9725, loss 0.544575.
Train: 2018-07-31T09:37:15.108859: step 9726, loss 0.508259.
Train: 2018-07-31T09:37:15.280695: step 9727, loss 0.590216.
Train: 2018-07-31T09:37:15.436909: step 9728, loss 0.480782.
Train: 2018-07-31T09:37:15.608774: step 9729, loss 0.52644.
Train: 2018-07-31T09:37:15.764956: step 9730, loss 0.636078.
Test: 2018-07-31T09:37:16.014899: step 9730, loss 0.547624.
Train: 2018-07-31T09:37:16.171136: step 9731, loss 0.581135.
Train: 2018-07-31T09:37:16.342976: step 9732, loss 0.626759.
Train: 2018-07-31T09:37:16.514782: step 9733, loss 0.5902.
Train: 2018-07-31T09:37:16.686615: step 9734, loss 0.626341.
Train: 2018-07-31T09:37:16.858481: step 9735, loss 0.598948.
Train: 2018-07-31T09:37:17.030286: step 9736, loss 0.616661.
Train: 2018-07-31T09:37:17.202150: step 9737, loss 0.517894.
Train: 2018-07-31T09:37:17.373954: step 9738, loss 0.553762.
Train: 2018-07-31T09:37:17.545790: step 9739, loss 0.571386.
Train: 2018-07-31T09:37:17.717624: step 9740, loss 0.562686.
Test: 2018-07-31T09:37:17.951946: step 9740, loss 0.547785.
Train: 2018-07-31T09:37:18.123778: step 9741, loss 0.536072.
Train: 2018-07-31T09:37:18.295614: step 9742, loss 0.579982.
Train: 2018-07-31T09:37:18.467479: step 9743, loss 0.518741.
Train: 2018-07-31T09:37:18.639284: step 9744, loss 0.536219.
Train: 2018-07-31T09:37:18.795497: step 9745, loss 0.579892.
Train: 2018-07-31T09:37:18.967332: step 9746, loss 0.536204.
Train: 2018-07-31T09:37:19.154812: step 9747, loss 0.562416.
Train: 2018-07-31T09:37:19.311031: step 9748, loss 0.48434.
Train: 2018-07-31T09:37:19.498458: step 9749, loss 0.571017.
Train: 2018-07-31T09:37:19.670293: step 9750, loss 0.658075.
Test: 2018-07-31T09:37:19.920264: step 9750, loss 0.54792.
Train: 2018-07-31T09:37:20.092069: step 9751, loss 0.658013.
Train: 2018-07-31T09:37:20.263936: step 9752, loss 0.571163.
Train: 2018-07-31T09:37:20.435763: step 9753, loss 0.666088.
Train: 2018-07-31T09:37:20.607573: step 9754, loss 0.502376.
Train: 2018-07-31T09:37:20.779438: step 9755, loss 0.57096.
Train: 2018-07-31T09:37:20.935622: step 9756, loss 0.604977.
Train: 2018-07-31T09:37:21.107480: step 9757, loss 0.621912.
Train: 2018-07-31T09:37:21.279290: step 9758, loss 0.537198.
Train: 2018-07-31T09:37:21.435503: step 9759, loss 0.570835.
Train: 2018-07-31T09:37:21.607338: step 9760, loss 0.570946.
Test: 2018-07-31T09:37:21.857313: step 9760, loss 0.548536.
Train: 2018-07-31T09:37:22.013493: step 9761, loss 0.587718.
Train: 2018-07-31T09:37:22.185358: step 9762, loss 0.554234.
Train: 2018-07-31T09:37:22.341541: step 9763, loss 0.587497.
Train: 2018-07-31T09:37:22.513410: step 9764, loss 0.537741.
Train: 2018-07-31T09:37:22.685212: step 9765, loss 0.60384.
Train: 2018-07-31T09:37:22.857045: step 9766, loss 0.505012.
Train: 2018-07-31T09:37:23.013260: step 9767, loss 0.554308.
Train: 2018-07-31T09:37:23.185093: step 9768, loss 0.579144.
Train: 2018-07-31T09:37:23.356929: step 9769, loss 0.603856.
Train: 2018-07-31T09:37:23.513142: step 9770, loss 0.603758.
Test: 2018-07-31T09:37:23.763083: step 9770, loss 0.548906.
Train: 2018-07-31T09:37:23.934948: step 9771, loss 0.579094.
Train: 2018-07-31T09:37:24.091163: step 9772, loss 0.579087.
Train: 2018-07-31T09:37:24.263006: step 9773, loss 0.636299.
Train: 2018-07-31T09:37:24.434826: step 9774, loss 0.570825.
Train: 2018-07-31T09:37:24.591015: step 9775, loss 0.611643.
Train: 2018-07-31T09:37:24.762884: step 9776, loss 0.554612.
Train: 2018-07-31T09:37:24.934714: step 9777, loss 0.538499.
Train: 2018-07-31T09:37:25.090928: step 9778, loss 0.546711.
Train: 2018-07-31T09:37:25.262734: step 9779, loss 0.522513.
Train: 2018-07-31T09:37:25.434566: step 9780, loss 0.611254.
Test: 2018-07-31T09:37:25.668918: step 9780, loss 0.549308.
Train: 2018-07-31T09:37:25.856368: step 9781, loss 0.538662.
Train: 2018-07-31T09:37:26.028203: step 9782, loss 0.570847.
Train: 2018-07-31T09:37:26.184391: step 9783, loss 0.554651.
Train: 2018-07-31T09:37:26.356253: step 9784, loss 0.514134.
Train: 2018-07-31T09:37:26.528060: step 9785, loss 0.595314.
Train: 2018-07-31T09:37:26.684304: step 9786, loss 0.603379.
Train: 2018-07-31T09:37:26.856142: step 9787, loss 0.603385.
Train: 2018-07-31T09:37:27.027945: step 9788, loss 0.595372.
Train: 2018-07-31T09:37:27.184157: step 9789, loss 0.570919.
Train: 2018-07-31T09:37:27.355993: step 9790, loss 0.578989.
Test: 2018-07-31T09:37:27.590314: step 9790, loss 0.549118.
Train: 2018-07-31T09:37:27.762147: step 9791, loss 0.546484.
Train: 2018-07-31T09:37:27.933981: step 9792, loss 0.562709.
Train: 2018-07-31T09:37:28.090219: step 9793, loss 0.611725.
Train: 2018-07-31T09:37:28.262064: step 9794, loss 0.611527.
Train: 2018-07-31T09:37:28.418244: step 9795, loss 0.538335.
Train: 2018-07-31T09:37:28.590108: step 9796, loss 0.538371.
Train: 2018-07-31T09:37:28.761938: step 9797, loss 0.522034.
Train: 2018-07-31T09:37:28.933748: step 9798, loss 0.521985.
Train: 2018-07-31T09:37:29.105582: step 9799, loss 0.513708.
Train: 2018-07-31T09:37:29.261830: step 9800, loss 0.579189.
Test: 2018-07-31T09:37:29.511737: step 9800, loss 0.548841.
Train: 2018-07-31T09:37:30.245971: step 9801, loss 0.562674.
Train: 2018-07-31T09:37:30.402153: step 9802, loss 0.57922.
Train: 2018-07-31T09:37:30.573988: step 9803, loss 0.562543.
Train: 2018-07-31T09:37:30.745856: step 9804, loss 0.570882.
Train: 2018-07-31T09:37:30.917689: step 9805, loss 0.562592.
Train: 2018-07-31T09:37:31.073871: step 9806, loss 0.604278.
Train: 2018-07-31T09:37:31.245740: step 9807, loss 0.629477.
Train: 2018-07-31T09:37:31.417542: step 9808, loss 0.470638.
Train: 2018-07-31T09:37:31.589406: step 9809, loss 0.637795.
Train: 2018-07-31T09:37:31.761245: step 9810, loss 0.554073.
Test: 2018-07-31T09:37:32.011151: step 9810, loss 0.548496.
Train: 2018-07-31T09:37:32.167365: step 9811, loss 0.579225.
Train: 2018-07-31T09:37:32.339231: step 9812, loss 0.537508.
Train: 2018-07-31T09:37:32.511036: step 9813, loss 0.579251.
Train: 2018-07-31T09:37:32.667248: step 9814, loss 0.537367.
Train: 2018-07-31T09:37:32.823461: step 9815, loss 0.526844.
Train: 2018-07-31T09:37:32.995296: step 9816, loss 0.570992.
Train: 2018-07-31T09:37:33.167162: step 9817, loss 0.537253.
Train: 2018-07-31T09:37:33.323346: step 9818, loss 0.545565.
Train: 2018-07-31T09:37:33.510801: step 9819, loss 0.537112.
Train: 2018-07-31T09:37:33.667044: step 9820, loss 0.587953.
Test: 2018-07-31T09:37:33.901365: step 9820, loss 0.548222.
Train: 2018-07-31T09:37:34.073168: step 9821, loss 0.613405.
Train: 2018-07-31T09:37:34.245004: step 9822, loss 0.57093.
Train: 2018-07-31T09:37:34.416869: step 9823, loss 0.570927.
Train: 2018-07-31T09:37:34.588673: step 9824, loss 0.570958.
Train: 2018-07-31T09:37:34.760508: step 9825, loss 0.622117.
Train: 2018-07-31T09:37:34.932372: step 9826, loss 0.51997.
Train: 2018-07-31T09:37:35.104210: step 9827, loss 0.553986.
Train: 2018-07-31T09:37:35.276012: step 9828, loss 0.605078.
Train: 2018-07-31T09:37:35.432226: step 9829, loss 0.486026.
Train: 2018-07-31T09:37:35.604091: step 9830, loss 0.48599.
Test: 2018-07-31T09:37:35.838412: step 9830, loss 0.54817.
Train: 2018-07-31T09:37:36.010250: step 9831, loss 0.656238.
Train: 2018-07-31T09:37:36.166459: step 9832, loss 0.545366.
Train: 2018-07-31T09:37:36.338295: step 9833, loss 0.528337.
Train: 2018-07-31T09:37:36.510133: step 9834, loss 0.579527.
Train: 2018-07-31T09:37:36.666342: step 9835, loss 0.528149.
Train: 2018-07-31T09:37:36.838148: step 9836, loss 0.579541.
Train: 2018-07-31T09:37:37.009982: step 9837, loss 0.58811.
Train: 2018-07-31T09:37:37.166194: step 9838, loss 0.588141.
Train: 2018-07-31T09:37:37.338060: step 9839, loss 0.502368.
Train: 2018-07-31T09:37:37.509865: step 9840, loss 0.570998.
Test: 2018-07-31T09:37:37.744185: step 9840, loss 0.548046.
Train: 2018-07-31T09:37:37.916049: step 9841, loss 0.596839.
Train: 2018-07-31T09:37:38.087885: step 9842, loss 0.536633.
Train: 2018-07-31T09:37:38.259723: step 9843, loss 0.545289.
Train: 2018-07-31T09:37:38.431523: step 9844, loss 0.579674.
Train: 2018-07-31T09:37:38.603358: step 9845, loss 0.596905.
Train: 2018-07-31T09:37:38.775193: step 9846, loss 0.571074.
Train: 2018-07-31T09:37:38.947058: step 9847, loss 0.502212.
Train: 2018-07-31T09:37:39.103272: step 9848, loss 0.579582.
Train: 2018-07-31T09:37:39.275075: step 9849, loss 0.596927.
Train: 2018-07-31T09:37:39.446935: step 9850, loss 0.54525.
Test: 2018-07-31T09:37:39.681231: step 9850, loss 0.54803.
Train: 2018-07-31T09:37:39.868687: step 9851, loss 0.579655.
Train: 2018-07-31T09:37:40.040522: step 9852, loss 0.588237.
Train: 2018-07-31T09:37:40.196736: step 9853, loss 0.545328.
Train: 2018-07-31T09:37:40.368600: step 9854, loss 0.5539.
Train: 2018-07-31T09:37:40.524784: step 9855, loss 0.596815.
Train: 2018-07-31T09:37:40.696617: step 9856, loss 0.553896.
Train: 2018-07-31T09:37:40.852863: step 9857, loss 0.57102.
Train: 2018-07-31T09:37:41.024667: step 9858, loss 0.519605.
Train: 2018-07-31T09:37:41.196531: step 9859, loss 0.588116.
Train: 2018-07-31T09:37:41.368335: step 9860, loss 0.571041.
Test: 2018-07-31T09:37:41.602687: step 9860, loss 0.548114.
Train: 2018-07-31T09:37:41.774490: step 9861, loss 0.570978.
Train: 2018-07-31T09:37:41.946325: step 9862, loss 0.605241.
Train: 2018-07-31T09:37:42.102564: step 9863, loss 0.511225.
Train: 2018-07-31T09:37:42.289995: step 9864, loss 0.57951.
Train: 2018-07-31T09:37:42.461829: step 9865, loss 0.588062.
Train: 2018-07-31T09:37:42.633665: step 9866, loss 0.58796.
Train: 2018-07-31T09:37:42.789878: step 9867, loss 0.502934.
Train: 2018-07-31T09:37:42.961712: step 9868, loss 0.545499.
Train: 2018-07-31T09:37:43.133582: step 9869, loss 0.579476.
Train: 2018-07-31T09:37:43.305384: step 9870, loss 0.579483.
Test: 2018-07-31T09:37:43.539702: step 9870, loss 0.548199.
Train: 2018-07-31T09:37:43.711567: step 9871, loss 0.656105.
Train: 2018-07-31T09:37:43.867750: step 9872, loss 0.60488.
Train: 2018-07-31T09:37:44.039616: step 9873, loss 0.60477.
Train: 2018-07-31T09:37:44.211450: step 9874, loss 0.56245.
Train: 2018-07-31T09:37:44.383258: step 9875, loss 0.503732.
Train: 2018-07-31T09:37:44.539499: step 9876, loss 0.545723.
Train: 2018-07-31T09:37:44.711327: step 9877, loss 0.579246.
Train: 2018-07-31T09:37:44.883138: step 9878, loss 0.537457.
Train: 2018-07-31T09:37:45.054972: step 9879, loss 0.570879.
Train: 2018-07-31T09:37:45.226807: step 9880, loss 0.579282.
Test: 2018-07-31T09:37:45.476786: step 9880, loss 0.548497.
Train: 2018-07-31T09:37:45.648584: step 9881, loss 0.570869.
Train: 2018-07-31T09:37:45.820418: step 9882, loss 0.629345.
Train: 2018-07-31T09:37:45.992252: step 9883, loss 0.529126.
Train: 2018-07-31T09:37:46.164118: step 9884, loss 0.595853.
Train: 2018-07-31T09:37:46.320326: step 9885, loss 0.537573.
Train: 2018-07-31T09:37:46.492160: step 9886, loss 0.495967.
Train: 2018-07-31T09:37:46.663995: step 9887, loss 0.54597.
Train: 2018-07-31T09:37:46.820184: step 9888, loss 0.512404.
Train: 2018-07-31T09:37:46.992053: step 9889, loss 0.570931.
Train: 2018-07-31T09:37:47.163879: step 9890, loss 0.503646.
Test: 2018-07-31T09:37:47.398205: step 9890, loss 0.548347.
Train: 2018-07-31T09:37:47.570033: step 9891, loss 0.50347.
Train: 2018-07-31T09:37:47.741876: step 9892, loss 0.562449.
Train: 2018-07-31T09:37:47.898087: step 9893, loss 0.605002.
Train: 2018-07-31T09:37:48.085512: step 9894, loss 0.528333.
Train: 2018-07-31T09:37:48.241726: step 9895, loss 0.459601.
Train: 2018-07-31T09:37:48.413591: step 9896, loss 0.536589.
Train: 2018-07-31T09:37:48.585395: step 9897, loss 0.519167.
Train: 2018-07-31T09:37:48.741609: step 9898, loss 0.58868.
Train: 2018-07-31T09:37:48.913443: step 9899, loss 0.518775.
Train: 2018-07-31T09:37:49.085316: step 9900, loss 0.492185.
Test: 2018-07-31T09:37:49.335219: step 9900, loss 0.547741.
Train: 2018-07-31T09:37:50.116287: step 9901, loss 0.465327.
Train: 2018-07-31T09:37:50.288122: step 9902, loss 0.660441.
Train: 2018-07-31T09:37:50.459957: step 9903, loss 0.589463.
Train: 2018-07-31T09:37:50.616204: step 9904, loss 0.508922.
Train: 2018-07-31T09:37:50.788035: step 9905, loss 0.580691.
Train: 2018-07-31T09:37:50.959871: step 9906, loss 0.535714.
Train: 2018-07-31T09:37:51.131674: step 9907, loss 0.562755.
Train: 2018-07-31T09:37:51.303509: step 9908, loss 0.635198.
Train: 2018-07-31T09:37:51.475344: step 9909, loss 0.517518.
Train: 2018-07-31T09:37:51.631558: step 9910, loss 0.671593.
Test: 2018-07-31T09:37:51.881533: step 9910, loss 0.547622.
Train: 2018-07-31T09:37:52.037712: step 9911, loss 0.553744.
Train: 2018-07-31T09:37:52.209548: step 9912, loss 0.427339.
Train: 2018-07-31T09:37:52.381416: step 9913, loss 0.589903.
Train: 2018-07-31T09:37:52.553247: step 9914, loss 0.626087.
Train: 2018-07-31T09:37:52.725082: step 9915, loss 0.562699.
Train: 2018-07-31T09:37:52.881265: step 9916, loss 0.535663.
Train: 2018-07-31T09:37:53.053129: step 9917, loss 0.58066.
Train: 2018-07-31T09:37:53.209343: step 9918, loss 0.472801.
Train: 2018-07-31T09:37:53.381182: step 9919, loss 0.589692.
Train: 2018-07-31T09:37:53.553008: step 9920, loss 0.571681.
Test: 2018-07-31T09:37:53.802924: step 9920, loss 0.547647.
Train: 2018-07-31T09:37:53.974783: step 9921, loss 0.589578.
Train: 2018-07-31T09:37:54.146594: step 9922, loss 0.535788.
Train: 2018-07-31T09:37:54.318428: step 9923, loss 0.535805.
Train: 2018-07-31T09:37:54.490264: step 9924, loss 0.544668.
Train: 2018-07-31T09:37:54.662098: step 9925, loss 0.580573.
Train: 2018-07-31T09:37:54.818312: step 9926, loss 0.509039.
Train: 2018-07-31T09:37:54.990179: step 9927, loss 0.57161.
Train: 2018-07-31T09:37:55.162011: step 9928, loss 0.553616.
Train: 2018-07-31T09:37:55.333816: step 9929, loss 0.616217.
Train: 2018-07-31T09:37:55.505651: step 9930, loss 0.633892.
Test: 2018-07-31T09:37:55.739970: step 9930, loss 0.5477.
Train: 2018-07-31T09:37:55.911806: step 9931, loss 0.562551.
Train: 2018-07-31T09:37:56.083640: step 9932, loss 0.535958.
Train: 2018-07-31T09:37:56.239853: step 9933, loss 0.535959.
Train: 2018-07-31T09:37:56.411689: step 9934, loss 0.580184.
Train: 2018-07-31T09:37:56.583547: step 9935, loss 0.571376.
Train: 2018-07-31T09:37:56.755392: step 9936, loss 0.641582.
Train: 2018-07-31T09:37:56.911571: step 9937, loss 0.51875.
Train: 2018-07-31T09:37:57.083436: step 9938, loss 0.483832.
Train: 2018-07-31T09:37:57.239649: step 9939, loss 0.571155.
Train: 2018-07-31T09:37:57.411479: step 9940, loss 0.510108.
Test: 2018-07-31T09:37:57.645808: step 9940, loss 0.547859.
Train: 2018-07-31T09:37:57.817640: step 9941, loss 0.544985.
Train: 2018-07-31T09:37:57.989477: step 9942, loss 0.553723.
Train: 2018-07-31T09:37:58.161309: step 9943, loss 0.658575.
Train: 2018-07-31T09:37:58.317522: step 9944, loss 0.544969.
Train: 2018-07-31T09:37:58.504982: step 9945, loss 0.640863.
Train: 2018-07-31T09:37:58.676783: step 9946, loss 0.571155.
Train: 2018-07-31T09:37:58.848648: step 9947, loss 0.553777.
Train: 2018-07-31T09:37:59.020482: step 9948, loss 0.553797.
Train: 2018-07-31T09:37:59.192317: step 9949, loss 0.588282.
Train: 2018-07-31T09:37:59.364122: step 9950, loss 0.58817.
Test: 2018-07-31T09:37:59.598467: step 9950, loss 0.548071.
Train: 2018-07-31T09:37:59.770301: step 9951, loss 0.528061.
Train: 2018-07-31T09:37:59.942142: step 9952, loss 0.528207.
Train: 2018-07-31T09:38:00.098325: step 9953, loss 0.61382.
Train: 2018-07-31T09:38:00.270159: step 9954, loss 0.511199.
Train: 2018-07-31T09:38:00.441994: step 9955, loss 0.587996.
Train: 2018-07-31T09:38:00.613829: step 9956, loss 0.579505.
Train: 2018-07-31T09:38:00.770073: step 9957, loss 0.536947.
Train: 2018-07-31T09:38:00.941877: step 9958, loss 0.528332.
Train: 2018-07-31T09:38:01.113712: step 9959, loss 0.545357.
Train: 2018-07-31T09:38:01.285577: step 9960, loss 0.52843.
Test: 2018-07-31T09:38:01.519899: step 9960, loss 0.548147.
Train: 2018-07-31T09:38:01.691732: step 9961, loss 0.596533.
Train: 2018-07-31T09:38:01.863536: step 9962, loss 0.588121.
Train: 2018-07-31T09:38:02.019775: step 9963, loss 0.562502.
Train: 2018-07-31T09:38:02.191615: step 9964, loss 0.562473.
Train: 2018-07-31T09:38:02.379074: step 9965, loss 0.60517.
Train: 2018-07-31T09:38:02.535284: step 9966, loss 0.635212.
Train: 2018-07-31T09:38:02.722740: step 9967, loss 0.570993.
Train: 2018-07-31T09:38:02.894575: step 9968, loss 0.562376.
Train: 2018-07-31T09:38:03.050789: step 9969, loss 0.621693.
Train: 2018-07-31T09:38:03.222594: step 9970, loss 0.511993.
Test: 2018-07-31T09:38:03.456915: step 9970, loss 0.548373.
Train: 2018-07-31T09:38:03.628747: step 9971, loss 0.604577.
Train: 2018-07-31T09:38:03.800614: step 9972, loss 0.621193.
Train: 2018-07-31T09:38:03.956826: step 9973, loss 0.52071.
Train: 2018-07-31T09:38:04.128665: step 9974, loss 0.570653.
Train: 2018-07-31T09:38:04.284874: step 9975, loss 0.562376.
Train: 2018-07-31T09:38:04.456679: step 9976, loss 0.554227.
Train: 2018-07-31T09:38:04.628514: step 9977, loss 0.596323.
Train: 2018-07-31T09:38:04.800379: step 9978, loss 0.521314.
Train: 2018-07-31T09:38:04.956562: step 9979, loss 0.529587.
Train: 2018-07-31T09:38:05.128397: step 9980, loss 0.620827.
Test: 2018-07-31T09:38:05.378338: step 9980, loss 0.548639.
Train: 2018-07-31T09:38:05.550172: step 9981, loss 0.61247.
Train: 2018-07-31T09:38:05.722008: step 9982, loss 0.529438.
Train: 2018-07-31T09:38:05.893844: step 9983, loss 0.537774.
Train: 2018-07-31T09:38:06.065708: step 9984, loss 0.529283.
Train: 2018-07-31T09:38:06.221890: step 9985, loss 0.529349.
Train: 2018-07-31T09:38:06.393755: step 9986, loss 0.620854.
Train: 2018-07-31T09:38:06.565590: step 9987, loss 0.537462.
Train: 2018-07-31T09:38:06.737425: step 9988, loss 0.562514.
Train: 2018-07-31T09:38:06.909231: step 9989, loss 0.537345.
Train: 2018-07-31T09:38:07.081095: step 9990, loss 0.587693.
Test: 2018-07-31T09:38:07.315416: step 9990, loss 0.54841.
Train: 2018-07-31T09:38:07.487220: step 9991, loss 0.554091.
Train: 2018-07-31T09:38:07.659079: step 9992, loss 0.520429.
Train: 2018-07-31T09:38:07.830890: step 9993, loss 0.511862.
Train: 2018-07-31T09:38:08.002758: step 9994, loss 0.545544.
Train: 2018-07-31T09:38:08.174589: step 9995, loss 0.630333.
Train: 2018-07-31T09:38:08.330772: step 9996, loss 0.519977.
Train: 2018-07-31T09:38:08.502641: step 9997, loss 0.485712.
Train: 2018-07-31T09:38:08.674441: step 9998, loss 0.54535.
Train: 2018-07-31T09:38:08.830654: step 9999, loss 0.571036.
Train: 2018-07-31T09:38:09.002489: step 10000, loss 0.484868.
Test: 2018-07-31T09:38:09.252431: step 10000, loss 0.547943.
Train: 2018-07-31T09:38:09.955392: step 10001, loss 0.640397.
Train: 2018-07-31T09:38:10.127260: step 10002, loss 0.545099.
Train: 2018-07-31T09:38:10.299062: step 10003, loss 0.562425.
Train: 2018-07-31T09:38:10.486516: step 10004, loss 0.632265.
Train: 2018-07-31T09:38:10.642761: step 10005, loss 0.632249.
Train: 2018-07-31T09:38:10.814595: step 10006, loss 0.501504.
Train: 2018-07-31T09:38:10.986400: step 10007, loss 0.553726.
Train: 2018-07-31T09:38:11.158268: step 10008, loss 0.571162.
Train: 2018-07-31T09:38:11.330070: step 10009, loss 0.571201.
Train: 2018-07-31T09:38:11.486311: step 10010, loss 0.56242.
Test: 2018-07-31T09:38:11.736224: step 10010, loss 0.547876.
Train: 2018-07-31T09:38:11.908059: step 10011, loss 0.545117.
Train: 2018-07-31T09:38:12.079894: step 10012, loss 0.527602.
Train: 2018-07-31T09:38:12.251763: step 10013, loss 0.606006.
Train: 2018-07-31T09:38:12.407973: step 10014, loss 0.518871.
Train: 2018-07-31T09:38:12.579776: step 10015, loss 0.518882.
Train: 2018-07-31T09:38:12.751612: step 10016, loss 0.510102.
Train: 2018-07-31T09:38:12.923477: step 10017, loss 0.562441.
Train: 2018-07-31T09:38:13.095311: step 10018, loss 0.597559.
Train: 2018-07-31T09:38:13.267146: step 10019, loss 0.580014.
Train: 2018-07-31T09:38:13.454602: step 10020, loss 0.518634.
Test: 2018-07-31T09:38:13.688924: step 10020, loss 0.547795.
Train: 2018-07-31T09:38:13.860726: step 10021, loss 0.588818.
Train: 2018-07-31T09:38:14.032592: step 10022, loss 0.527427.
Train: 2018-07-31T09:38:14.188805: step 10023, loss 0.527327.
Train: 2018-07-31T09:38:14.360638: step 10024, loss 0.518468.
Train: 2018-07-31T09:38:14.532475: step 10025, loss 0.580093.
Train: 2018-07-31T09:38:14.704304: step 10026, loss 0.57129.
Train: 2018-07-31T09:38:14.876148: step 10027, loss 0.642072.
Train: 2018-07-31T09:38:15.047980: step 10028, loss 0.527183.
Train: 2018-07-31T09:38:15.235433: step 10029, loss 0.518369.
Train: 2018-07-31T09:38:15.391618: step 10030, loss 0.518366.
Test: 2018-07-31T09:38:15.641560: step 10030, loss 0.547733.
Train: 2018-07-31T09:38:15.813395: step 10031, loss 0.544968.
Train: 2018-07-31T09:38:15.985260: step 10032, loss 0.571431.
Train: 2018-07-31T09:38:16.157089: step 10033, loss 0.562538.
Train: 2018-07-31T09:38:16.313277: step 10034, loss 0.500522.
Train: 2018-07-31T09:38:16.485142: step 10035, loss 0.615808.
Train: 2018-07-31T09:38:16.656978: step 10036, loss 0.642451.
Train: 2018-07-31T09:38:16.828812: step 10037, loss 0.615786.
Train: 2018-07-31T09:38:17.000617: step 10038, loss 0.474063.
Train: 2018-07-31T09:38:17.172481: step 10039, loss 0.642125.
Train: 2018-07-31T09:38:17.328695: step 10040, loss 0.589005.
Test: 2018-07-31T09:38:17.578605: step 10040, loss 0.547779.
Train: 2018-07-31T09:38:17.734819: step 10041, loss 0.580113.
Train: 2018-07-31T09:38:17.906654: step 10042, loss 0.553734.
Train: 2018-07-31T09:38:18.078520: step 10043, loss 0.545007.
Train: 2018-07-31T09:38:18.250324: step 10044, loss 0.54505.
Train: 2018-07-31T09:38:18.422183: step 10045, loss 0.501536.
Train: 2018-07-31T09:38:18.594024: step 10046, loss 0.614665.
Train: 2018-07-31T09:38:18.750206: step 10047, loss 0.562458.
Train: 2018-07-31T09:38:18.922066: step 10048, loss 0.623184.
Train: 2018-07-31T09:38:19.093903: step 10049, loss 0.579754.
Train: 2018-07-31T09:38:19.265712: step 10050, loss 0.6316.
Test: 2018-07-31T09:38:19.515653: step 10050, loss 0.548031.
Train: 2018-07-31T09:38:19.687488: step 10051, loss 0.442056.
Train: 2018-07-31T09:38:19.859356: step 10052, loss 0.596814.
Train: 2018-07-31T09:38:20.015565: step 10053, loss 0.622398.
Train: 2018-07-31T09:38:20.187400: step 10054, loss 0.476859.
Train: 2018-07-31T09:38:20.343613: step 10055, loss 0.656486.
Train: 2018-07-31T09:38:20.515451: step 10056, loss 0.596479.
Train: 2018-07-31T09:38:20.687253: step 10057, loss 0.528457.
Train: 2018-07-31T09:38:20.859088: step 10058, loss 0.570904.
Train: 2018-07-31T09:38:21.015301: step 10059, loss 0.48625.
Train: 2018-07-31T09:38:21.187136: step 10060, loss 0.57091.
Test: 2018-07-31T09:38:21.421483: step 10060, loss 0.548276.
Train: 2018-07-31T09:38:21.593315: step 10061, loss 0.647204.
Train: 2018-07-31T09:38:21.765150: step 10062, loss 0.570856.
Train: 2018-07-31T09:38:21.936991: step 10063, loss 0.528723.
Train: 2018-07-31T09:38:22.108795: step 10064, loss 0.503535.
Train: 2018-07-31T09:38:22.280630: step 10065, loss 0.613092.
Train: 2018-07-31T09:38:22.436843: step 10066, loss 0.554111.
Train: 2018-07-31T09:38:22.608678: step 10067, loss 0.646712.
Train: 2018-07-31T09:38:22.796135: step 10068, loss 0.54566.
Train: 2018-07-31T09:38:23.014863: step 10069, loss 0.528941.
Train: 2018-07-31T09:38:23.202320: step 10070, loss 0.612848.
Test: 2018-07-31T09:38:23.436609: step 10070, loss 0.548437.
Train: 2018-07-31T09:38:23.608445: step 10071, loss 0.503718.
Train: 2018-07-31T09:38:23.780279: step 10072, loss 0.495377.
Train: 2018-07-31T09:38:23.952145: step 10073, loss 0.562429.
Train: 2018-07-31T09:38:24.123978: step 10074, loss 0.613022.
Train: 2018-07-31T09:38:24.295783: step 10075, loss 0.520336.
Train: 2018-07-31T09:38:24.467649: step 10076, loss 0.613183.
Train: 2018-07-31T09:38:24.623831: step 10077, loss 0.613104.
Train: 2018-07-31T09:38:24.811318: step 10078, loss 0.613006.
Train: 2018-07-31T09:38:24.967531: step 10079, loss 0.579276.
Train: 2018-07-31T09:38:25.139366: step 10080, loss 0.512043.
Test: 2018-07-31T09:38:25.389279: step 10080, loss 0.548404.
Train: 2018-07-31T09:38:25.561113: step 10081, loss 0.596133.
Train: 2018-07-31T09:38:25.732947: step 10082, loss 0.528823.
Train: 2018-07-31T09:38:25.904781: step 10083, loss 0.562527.
Train: 2018-07-31T09:38:26.061025: step 10084, loss 0.554102.
Train: 2018-07-31T09:38:26.232829: step 10085, loss 0.520512.
Train: 2018-07-31T09:38:26.389044: step 10086, loss 0.520453.
Train: 2018-07-31T09:38:26.560878: step 10087, loss 0.520309.
Train: 2018-07-31T09:38:26.717122: step 10088, loss 0.579366.
Train: 2018-07-31T09:38:26.888926: step 10089, loss 0.452278.
Train: 2018-07-31T09:38:27.060761: step 10090, loss 0.562361.
Test: 2018-07-31T09:38:27.295083: step 10090, loss 0.548113.
Train: 2018-07-31T09:38:27.466946: step 10091, loss 0.570994.
Train: 2018-07-31T09:38:27.638780: step 10092, loss 0.588205.
Train: 2018-07-31T09:38:27.794994: step 10093, loss 0.67427.
Train: 2018-07-31T09:38:27.966829: step 10094, loss 0.519391.
Train: 2018-07-31T09:38:28.138664: step 10095, loss 0.519474.
Train: 2018-07-31T09:38:28.310467: step 10096, loss 0.527863.
Train: 2018-07-31T09:38:28.466682: step 10097, loss 0.519192.
Train: 2018-07-31T09:38:28.638547: step 10098, loss 0.571095.
Train: 2018-07-31T09:38:28.810350: step 10099, loss 0.605913.
Train: 2018-07-31T09:38:28.982185: step 10100, loss 0.553761.
Test: 2018-07-31T09:38:29.216540: step 10100, loss 0.547866.
Train: 2018-07-31T09:38:29.966364: step 10101, loss 0.518851.
Train: 2018-07-31T09:38:30.138196: step 10102, loss 0.518821.
Train: 2018-07-31T09:38:30.310031: step 10103, loss 0.553742.
Train: 2018-07-31T09:38:30.481863: step 10104, loss 0.527384.
Train: 2018-07-31T09:38:30.653669: step 10105, loss 0.632935.
Train: 2018-07-31T09:38:30.809883: step 10106, loss 0.527312.
Train: 2018-07-31T09:38:30.981717: step 10107, loss 0.562506.
Train: 2018-07-31T09:38:31.153553: step 10108, loss 0.571346.
Train: 2018-07-31T09:38:31.325388: step 10109, loss 0.59785.
Train: 2018-07-31T09:38:31.481631: step 10110, loss 0.56254.
Test: 2018-07-31T09:38:31.715953: step 10110, loss 0.547741.
Train: 2018-07-31T09:38:31.903407: step 10111, loss 0.544893.
Train: 2018-07-31T09:38:32.075241: step 10112, loss 0.580179.
Train: 2018-07-31T09:38:32.247071: step 10113, loss 0.544919.
Train: 2018-07-31T09:38:32.418882: step 10114, loss 0.606595.
Train: 2018-07-31T09:38:32.590746: step 10115, loss 0.624145.
Train: 2018-07-31T09:38:32.762550: step 10116, loss 0.527331.
Train: 2018-07-31T09:38:32.918764: step 10117, loss 0.674599.
Train: 2018-07-31T09:38:33.090628: step 10118, loss 0.649782.
Train: 2018-07-31T09:38:33.246837: step 10119, loss 0.571127.
Train: 2018-07-31T09:38:33.449890: step 10120, loss 0.631555.
Test: 2018-07-31T09:38:33.684210: step 10120, loss 0.548046.
Train: 2018-07-31T09:38:33.856075: step 10121, loss 0.528073.
Train: 2018-07-31T09:38:34.027911: step 10122, loss 0.476971.
Train: 2018-07-31T09:38:34.199738: step 10123, loss 0.579412.
Train: 2018-07-31T09:38:34.371549: step 10124, loss 0.53694.
Train: 2018-07-31T09:38:34.543383: step 10125, loss 0.562414.
Train: 2018-07-31T09:38:34.715218: step 10126, loss 0.579409.
Train: 2018-07-31T09:38:34.871431: step 10127, loss 0.613244.
Train: 2018-07-31T09:38:35.043291: step 10128, loss 0.537071.
Train: 2018-07-31T09:38:35.215132: step 10129, loss 0.562538.
Train: 2018-07-31T09:38:35.371349: step 10130, loss 0.604545.
Test: 2018-07-31T09:38:35.605636: step 10130, loss 0.548431.
Train: 2018-07-31T09:38:35.777470: step 10131, loss 0.537293.
Train: 2018-07-31T09:38:35.949335: step 10132, loss 0.554147.
Train: 2018-07-31T09:38:36.121139: step 10133, loss 0.570935.
Train: 2018-07-31T09:38:36.293004: step 10134, loss 0.528994.
Train: 2018-07-31T09:38:36.449218: step 10135, loss 0.595928.
Train: 2018-07-31T09:38:36.621056: step 10136, loss 0.537392.
Train: 2018-07-31T09:38:36.792857: step 10137, loss 0.529043.
Train: 2018-07-31T09:38:36.964721: step 10138, loss 0.579295.
Train: 2018-07-31T09:38:37.136551: step 10139, loss 0.554078.
Train: 2018-07-31T09:38:37.292770: step 10140, loss 0.528908.
Test: 2018-07-31T09:38:37.542708: step 10140, loss 0.548401.
Train: 2018-07-31T09:38:37.777032: step 10141, loss 0.554071.
Train: 2018-07-31T09:38:37.948867: step 10142, loss 0.511982.
Train: 2018-07-31T09:38:38.120671: step 10143, loss 0.587818.
Train: 2018-07-31T09:38:38.292505: step 10144, loss 0.647084.
Train: 2018-07-31T09:38:38.479987: step 10145, loss 0.503217.
Train: 2018-07-31T09:38:38.651828: step 10146, loss 0.58784.
Train: 2018-07-31T09:38:38.823632: step 10147, loss 0.579409.
Train: 2018-07-31T09:38:38.995500: step 10148, loss 0.536968.
Train: 2018-07-31T09:38:39.167325: step 10149, loss 0.55391.
Train: 2018-07-31T09:38:39.339135: step 10150, loss 0.588012.
Test: 2018-07-31T09:38:39.573455: step 10150, loss 0.548189.
Train: 2018-07-31T09:38:39.745325: step 10151, loss 0.59644.
Train: 2018-07-31T09:38:39.901534: step 10152, loss 0.690003.
Train: 2018-07-31T09:38:40.073369: step 10153, loss 0.54552.
Train: 2018-07-31T09:38:40.260824: step 10154, loss 0.59636.
Train: 2018-07-31T09:38:40.432659: step 10155, loss 0.570884.
Train: 2018-07-31T09:38:40.604488: step 10156, loss 0.562401.
Train: 2018-07-31T09:38:40.760708: step 10157, loss 0.596054.
Train: 2018-07-31T09:38:40.948164: step 10158, loss 0.554124.
Train: 2018-07-31T09:38:41.104377: step 10159, loss 0.612642.
Train: 2018-07-31T09:38:41.276212: step 10160, loss 0.504264.
Test: 2018-07-31T09:38:41.526125: step 10160, loss 0.548587.
Train: 2018-07-31T09:38:41.697958: step 10161, loss 0.570846.
Train: 2018-07-31T09:38:41.854171: step 10162, loss 0.570862.
Train: 2018-07-31T09:38:42.026007: step 10163, loss 0.587506.
Train: 2018-07-31T09:38:42.197872: step 10164, loss 0.562513.
Train: 2018-07-31T09:38:42.369675: step 10165, loss 0.545891.
Train: 2018-07-31T09:38:42.541511: step 10166, loss 0.554269.
Train: 2018-07-31T09:38:42.713376: step 10167, loss 0.587493.
Train: 2018-07-31T09:38:42.869559: step 10168, loss 0.488039.
Train: 2018-07-31T09:38:43.041393: step 10169, loss 0.595762.
Train: 2018-07-31T09:38:43.213259: step 10170, loss 0.512776.
Test: 2018-07-31T09:38:43.447548: step 10170, loss 0.548596.
Train: 2018-07-31T09:38:43.619383: step 10171, loss 0.537656.
Train: 2018-07-31T09:38:43.791218: step 10172, loss 0.620894.
Train: 2018-07-31T09:38:43.963083: step 10173, loss 0.587593.
Train: 2018-07-31T09:38:44.134888: step 10174, loss 0.587606.
Train: 2018-07-31T09:38:44.291131: step 10175, loss 0.478897.
Train: 2018-07-31T09:38:44.462935: step 10176, loss 0.612809.
Train: 2018-07-31T09:38:44.634801: step 10177, loss 0.520578.
Train: 2018-07-31T09:38:44.806605: step 10178, loss 0.570859.
Train: 2018-07-31T09:38:44.978441: step 10179, loss 0.55408.
Train: 2018-07-31T09:38:45.150305: step 10180, loss 0.528795.
Test: 2018-07-31T09:38:45.384625: step 10180, loss 0.548314.
Train: 2018-07-31T09:38:45.556430: step 10181, loss 0.486441.
Train: 2018-07-31T09:38:45.728265: step 10182, loss 0.596338.
Train: 2018-07-31T09:38:45.900099: step 10183, loss 0.502955.
Train: 2018-07-31T09:38:46.056313: step 10184, loss 0.485646.
Train: 2018-07-31T09:38:46.228148: step 10185, loss 0.588158.
Train: 2018-07-31T09:38:46.399981: step 10186, loss 0.588269.
Train: 2018-07-31T09:38:46.556226: step 10187, loss 0.54508.
Train: 2018-07-31T09:38:46.728031: step 10188, loss 0.63175.
Train: 2018-07-31T09:38:46.899866: step 10189, loss 0.605866.
Train: 2018-07-31T09:38:47.071730: step 10190, loss 0.65791.
Test: 2018-07-31T09:38:47.306022: step 10190, loss 0.547933.
Train: 2018-07-31T09:38:47.477885: step 10191, loss 0.571088.
Train: 2018-07-31T09:38:47.649690: step 10192, loss 0.571048.
Train: 2018-07-31T09:38:47.821555: step 10193, loss 0.596978.
Train: 2018-07-31T09:38:47.993393: step 10194, loss 0.545239.
Train: 2018-07-31T09:38:48.149597: step 10195, loss 0.622553.
Train: 2018-07-31T09:38:48.321408: step 10196, loss 0.630937.
Train: 2018-07-31T09:38:48.493243: step 10197, loss 0.519798.
Train: 2018-07-31T09:38:48.680699: step 10198, loss 0.570928.
Train: 2018-07-31T09:38:48.836911: step 10199, loss 0.520041.
Train: 2018-07-31T09:38:49.024392: step 10200, loss 0.494729.
Test: 2018-07-31T09:38:49.258690: step 10200, loss 0.548265.
Train: 2018-07-31T09:38:50.008512: step 10201, loss 0.596331.
Train: 2018-07-31T09:38:50.180371: step 10202, loss 0.5878.
Train: 2018-07-31T09:38:50.352181: step 10203, loss 0.562482.
Train: 2018-07-31T09:38:50.524016: step 10204, loss 0.570871.
Train: 2018-07-31T09:38:50.695882: step 10205, loss 0.604715.
Train: 2018-07-31T09:38:50.883337: step 10206, loss 0.562447.
Train: 2018-07-31T09:38:51.055142: step 10207, loss 0.503545.
Train: 2018-07-31T09:38:51.226976: step 10208, loss 0.545629.
Train: 2018-07-31T09:38:51.383220: step 10209, loss 0.495029.
Train: 2018-07-31T09:38:51.555055: step 10210, loss 0.528676.
Test: 2018-07-31T09:38:51.804992: step 10210, loss 0.548264.
Train: 2018-07-31T09:38:51.976802: step 10211, loss 0.528583.
Train: 2018-07-31T09:38:52.148666: step 10212, loss 0.502961.
Train: 2018-07-31T09:38:52.320501: step 10213, loss 0.570956.
Train: 2018-07-31T09:38:52.492340: step 10214, loss 0.536759.
Train: 2018-07-31T09:38:52.648519: step 10215, loss 0.485073.
Train: 2018-07-31T09:38:52.836006: step 10216, loss 0.510541.
Train: 2018-07-31T09:38:53.007810: step 10217, loss 0.631909.
Train: 2018-07-31T09:38:53.179675: step 10218, loss 0.640904.
Train: 2018-07-31T09:38:53.351507: step 10219, loss 0.52753.
Train: 2018-07-31T09:38:53.507692: step 10220, loss 0.527502.
Test: 2018-07-31T09:38:53.757634: step 10220, loss 0.547794.
Train: 2018-07-31T09:38:53.929469: step 10221, loss 0.632648.
Train: 2018-07-31T09:38:54.101304: step 10222, loss 0.553732.
Train: 2018-07-31T09:38:54.273139: step 10223, loss 0.527372.
Train: 2018-07-31T09:38:54.444973: step 10224, loss 0.527329.
Train: 2018-07-31T09:38:54.601187: step 10225, loss 0.562542.
Train: 2018-07-31T09:38:54.773021: step 10226, loss 0.55368.
Train: 2018-07-31T09:38:54.944856: step 10227, loss 0.571357.
Train: 2018-07-31T09:38:55.116690: step 10228, loss 0.633204.
Train: 2018-07-31T09:38:55.288525: step 10229, loss 0.597824.
Train: 2018-07-31T09:38:55.460385: step 10230, loss 0.48314.
Test: 2018-07-31T09:38:55.694682: step 10230, loss 0.54775.
Train: 2018-07-31T09:38:55.866549: step 10231, loss 0.536065.
Train: 2018-07-31T09:38:56.038349: step 10232, loss 0.553673.
Train: 2018-07-31T09:38:56.210215: step 10233, loss 0.518406.
Train: 2018-07-31T09:38:56.382051: step 10234, loss 0.606628.
Train: 2018-07-31T09:38:56.553854: step 10235, loss 0.553708.
Train: 2018-07-31T09:38:56.725690: step 10236, loss 0.562516.
Train: 2018-07-31T09:38:56.897523: step 10237, loss 0.553674.
Train: 2018-07-31T09:38:57.069359: step 10238, loss 0.553698.
Train: 2018-07-31T09:38:57.241194: step 10239, loss 0.491911.
Train: 2018-07-31T09:38:57.413047: step 10240, loss 0.518269.
Test: 2018-07-31T09:38:57.647348: step 10240, loss 0.547709.
Train: 2018-07-31T09:38:57.819184: step 10241, loss 0.668861.
Train: 2018-07-31T09:38:57.975427: step 10242, loss 0.51827.
Train: 2018-07-31T09:38:58.147261: step 10243, loss 0.482871.
Train: 2018-07-31T09:38:58.334718: step 10244, loss 0.571458.
Train: 2018-07-31T09:38:58.506572: step 10245, loss 0.535884.
Train: 2018-07-31T09:38:58.662766: step 10246, loss 0.5893.
Train: 2018-07-31T09:38:58.834601: step 10247, loss 0.509184.
Train: 2018-07-31T09:38:59.006435: step 10248, loss 0.562577.
Train: 2018-07-31T09:38:59.162648: step 10249, loss 0.562664.
Train: 2018-07-31T09:38:59.350107: step 10250, loss 0.544785.
Test: 2018-07-31T09:38:59.584428: step 10250, loss 0.547656.
Train: 2018-07-31T09:38:59.756230: step 10251, loss 0.580478.
Train: 2018-07-31T09:38:59.928065: step 10252, loss 0.526838.
Train: 2018-07-31T09:39:00.099932: step 10253, loss 0.553673.
Train: 2018-07-31T09:39:00.271770: step 10254, loss 0.517884.
Train: 2018-07-31T09:39:00.443598: step 10255, loss 0.544714.
Train: 2018-07-31T09:39:00.615403: step 10256, loss 0.625406.
Train: 2018-07-31T09:39:00.771616: step 10257, loss 0.607513.
Train: 2018-07-31T09:39:00.943452: step 10258, loss 0.580568.
Train: 2018-07-31T09:39:01.115286: step 10259, loss 0.562633.
Train: 2018-07-31T09:39:01.287120: step 10260, loss 0.509113.
Test: 2018-07-31T09:39:01.521472: step 10260, loss 0.547666.
Train: 2018-07-31T09:39:01.693306: step 10261, loss 0.562583.
Train: 2018-07-31T09:39:01.865145: step 10262, loss 0.598167.
Train: 2018-07-31T09:39:02.036946: step 10263, loss 0.571459.
Train: 2018-07-31T09:39:02.193159: step 10264, loss 0.598095.
Train: 2018-07-31T09:39:02.365027: step 10265, loss 0.571436.
Train: 2018-07-31T09:39:02.536828: step 10266, loss 0.527254.
Train: 2018-07-31T09:39:02.708693: step 10267, loss 0.624171.
Train: 2018-07-31T09:39:02.864901: step 10268, loss 0.487568.
Train: 2018-07-31T09:39:03.036712: step 10269, loss 0.562496.
Train: 2018-07-31T09:39:03.208576: step 10270, loss 0.509891.
Test: 2018-07-31T09:39:03.458512: step 10270, loss 0.547799.
Train: 2018-07-31T09:39:03.630323: step 10271, loss 0.580008.
Train: 2018-07-31T09:39:03.786536: step 10272, loss 0.53618.
Train: 2018-07-31T09:39:03.973991: step 10273, loss 0.597509.
Train: 2018-07-31T09:39:04.130235: step 10274, loss 0.536223.
Train: 2018-07-31T09:39:04.302070: step 10275, loss 0.588738.
Train: 2018-07-31T09:39:04.489496: step 10276, loss 0.579942.
Train: 2018-07-31T09:39:04.661355: step 10277, loss 0.579914.
Train: 2018-07-31T09:39:04.833165: step 10278, loss 0.536288.
Train: 2018-07-31T09:39:05.005031: step 10279, loss 0.536327.
Train: 2018-07-31T09:39:05.176835: step 10280, loss 0.597283.
Test: 2018-07-31T09:39:05.411155: step 10280, loss 0.54789.
Train: 2018-07-31T09:39:05.598612: step 10281, loss 0.527689.
Train: 2018-07-31T09:39:05.770471: step 10282, loss 0.649361.
Train: 2018-07-31T09:39:05.926690: step 10283, loss 0.640453.
Train: 2018-07-31T09:39:06.098523: step 10284, loss 0.53654.
Train: 2018-07-31T09:39:06.270360: step 10285, loss 0.493518.
Train: 2018-07-31T09:39:06.442163: step 10286, loss 0.528011.
Train: 2018-07-31T09:39:06.629651: step 10287, loss 0.536644.
Train: 2018-07-31T09:39:06.801454: step 10288, loss 0.485079.
Train: 2018-07-31T09:39:06.957668: step 10289, loss 0.545214.
Train: 2018-07-31T09:39:07.129502: step 10290, loss 0.519245.
Test: 2018-07-31T09:39:07.379456: step 10290, loss 0.547933.
Train: 2018-07-31T09:39:07.551278: step 10291, loss 0.579708.
Train: 2018-07-31T09:39:07.723146: step 10292, loss 0.579771.
Train: 2018-07-31T09:39:07.879357: step 10293, loss 0.579821.
Train: 2018-07-31T09:39:08.051192: step 10294, loss 0.562479.
Train: 2018-07-31T09:39:08.222997: step 10295, loss 0.536361.
Train: 2018-07-31T09:39:08.394832: step 10296, loss 0.57991.
Train: 2018-07-31T09:39:08.551070: step 10297, loss 0.606004.
Train: 2018-07-31T09:39:08.722879: step 10298, loss 0.562407.
Train: 2018-07-31T09:39:08.910336: step 10299, loss 0.562474.
Train: 2018-07-31T09:39:09.066549: step 10300, loss 0.510248.
Test: 2018-07-31T09:39:09.316491: step 10300, loss 0.547866.
Train: 2018-07-31T09:39:10.097590: step 10301, loss 0.562431.
Train: 2018-07-31T09:39:10.269424: step 10302, loss 0.579867.
Train: 2018-07-31T09:39:10.441227: step 10303, loss 0.606011.
Train: 2018-07-31T09:39:10.613087: step 10304, loss 0.52765.
Train: 2018-07-31T09:39:10.784928: step 10305, loss 0.527656.
Train: 2018-07-31T09:39:10.956732: step 10306, loss 0.588518.
Train: 2018-07-31T09:39:11.128597: step 10307, loss 0.536369.
Train: 2018-07-31T09:39:11.300401: step 10308, loss 0.605952.
Train: 2018-07-31T09:39:11.472236: step 10309, loss 0.527649.
Train: 2018-07-31T09:39:11.659716: step 10310, loss 0.484158.
Test: 2018-07-31T09:39:11.894014: step 10310, loss 0.547862.
Train: 2018-07-31T09:39:12.065871: step 10311, loss 0.518913.
Train: 2018-07-31T09:39:12.237681: step 10312, loss 0.562458.
Train: 2018-07-31T09:39:12.409517: step 10313, loss 0.527509.
Train: 2018-07-31T09:39:12.581383: step 10314, loss 0.544926.
Train: 2018-07-31T09:39:12.753210: step 10315, loss 0.58882.
Train: 2018-07-31T09:39:12.925020: step 10316, loss 0.536091.
Train: 2018-07-31T09:39:13.096856: step 10317, loss 0.536046.
Train: 2018-07-31T09:39:13.268714: step 10318, loss 0.58902.
Train: 2018-07-31T09:39:13.440556: step 10319, loss 0.509477.
Train: 2018-07-31T09:39:13.612384: step 10320, loss 0.562542.
Test: 2018-07-31T09:39:13.846715: step 10320, loss 0.547693.
Train: 2018-07-31T09:39:14.018514: step 10321, loss 0.518192.
Train: 2018-07-31T09:39:14.190349: step 10322, loss 0.527026.
Train: 2018-07-31T09:39:14.346562: step 10323, loss 0.544761.
Train: 2018-07-31T09:39:14.518398: step 10324, loss 0.464336.
Train: 2018-07-31T09:39:14.690262: step 10325, loss 0.616498.
Train: 2018-07-31T09:39:14.862068: step 10326, loss 0.517725.
Train: 2018-07-31T09:39:15.033903: step 10327, loss 0.742976.
Train: 2018-07-31T09:39:15.205737: step 10328, loss 0.490675.
Train: 2018-07-31T09:39:15.377601: step 10329, loss 0.436651.
Train: 2018-07-31T09:39:15.549436: step 10330, loss 0.598803.
Test: 2018-07-31T09:39:15.783752: step 10330, loss 0.547608.
Train: 2018-07-31T09:39:15.955591: step 10331, loss 0.544676.
Train: 2018-07-31T09:39:16.127396: step 10332, loss 0.517542.
Train: 2018-07-31T09:39:16.299230: step 10333, loss 0.499396.
Train: 2018-07-31T09:39:16.471098: step 10334, loss 0.562821.
Train: 2018-07-31T09:39:16.642930: step 10335, loss 0.535552.
Train: 2018-07-31T09:39:16.814735: step 10336, loss 0.63575.
Train: 2018-07-31T09:39:16.986569: step 10337, loss 0.581041.
Train: 2018-07-31T09:39:17.142814: step 10338, loss 0.653895.
Train: 2018-07-31T09:39:17.314618: step 10339, loss 0.53555.
Train: 2018-07-31T09:39:17.486487: step 10340, loss 0.562779.
Test: 2018-07-31T09:39:17.736394: step 10340, loss 0.547605.
Train: 2018-07-31T09:39:17.908259: step 10341, loss 0.589892.
Train: 2018-07-31T09:39:18.064473: step 10342, loss 0.553638.
Train: 2018-07-31T09:39:18.236276: step 10343, loss 0.553668.
Train: 2018-07-31T09:39:18.408111: step 10344, loss 0.544727.
Train: 2018-07-31T09:39:18.579977: step 10345, loss 0.58061.
Train: 2018-07-31T09:39:18.751781: step 10346, loss 0.455306.
Train: 2018-07-31T09:39:18.923647: step 10347, loss 0.66086.
Train: 2018-07-31T09:39:19.079860: step 10348, loss 0.50908.
Train: 2018-07-31T09:39:19.251663: step 10349, loss 0.607078.
Train: 2018-07-31T09:39:19.439120: step 10350, loss 0.571366.
Test: 2018-07-31T09:39:19.685001: step 10350, loss 0.547705.
Train: 2018-07-31T09:39:19.872451: step 10351, loss 0.606966.
Train: 2018-07-31T09:39:20.028638: step 10352, loss 0.50066.
Train: 2018-07-31T09:39:20.200474: step 10353, loss 0.571458.
Train: 2018-07-31T09:39:20.372339: step 10354, loss 0.597652.
Train: 2018-07-31T09:39:20.544173: step 10355, loss 0.553747.
Train: 2018-07-31T09:39:20.715979: step 10356, loss 0.553706.
Train: 2018-07-31T09:39:20.872222: step 10357, loss 0.606415.
Train: 2018-07-31T09:39:21.044056: step 10358, loss 0.710662.
Train: 2018-07-31T09:39:21.215891: step 10359, loss 0.519147.
Train: 2018-07-31T09:39:21.387695: step 10360, loss 0.579852.
Test: 2018-07-31T09:39:21.622016: step 10360, loss 0.548032.
Train: 2018-07-31T09:39:21.809496: step 10361, loss 0.579525.
Train: 2018-07-31T09:39:21.965720: step 10362, loss 0.622543.
Train: 2018-07-31T09:39:22.137551: step 10363, loss 0.638983.
Train: 2018-07-31T09:39:22.309386: step 10364, loss 0.553959.
Train: 2018-07-31T09:39:22.481220: step 10365, loss 0.545675.
Train: 2018-07-31T09:39:22.653055: step 10366, loss 0.629506.
Train: 2018-07-31T09:39:22.824861: step 10367, loss 0.579154.
Train: 2018-07-31T09:39:22.981103: step 10368, loss 0.488144.
Train: 2018-07-31T09:39:23.168529: step 10369, loss 0.578682.
Train: 2018-07-31T09:39:23.324743: step 10370, loss 0.603739.
Test: 2018-07-31T09:39:23.574717: step 10370, loss 0.54889.
Train: 2018-07-31T09:39:23.746519: step 10371, loss 0.505746.
Train: 2018-07-31T09:39:23.918353: step 10372, loss 0.505236.
Train: 2018-07-31T09:39:24.090213: step 10373, loss 0.611918.
Train: 2018-07-31T09:39:24.246402: step 10374, loss 0.611964.
Train: 2018-07-31T09:39:24.418236: step 10375, loss 0.538159.
Train: 2018-07-31T09:39:24.590071: step 10376, loss 0.570706.
Train: 2018-07-31T09:39:24.761939: step 10377, loss 0.619884.
Train: 2018-07-31T09:39:24.933741: step 10378, loss 0.562812.
Train: 2018-07-31T09:39:25.105575: step 10379, loss 0.5791.
Train: 2018-07-31T09:39:25.277441: step 10380, loss 0.497032.
Test: 2018-07-31T09:39:25.511732: step 10380, loss 0.54898.
Train: 2018-07-31T09:39:25.683565: step 10381, loss 0.546348.
Train: 2018-07-31T09:39:25.855430: step 10382, loss 0.570619.
Train: 2018-07-31T09:39:26.027235: step 10383, loss 0.54611.
Train: 2018-07-31T09:39:26.199094: step 10384, loss 0.620165.
Train: 2018-07-31T09:39:26.370904: step 10385, loss 0.579117.
Train: 2018-07-31T09:39:26.542769: step 10386, loss 0.562552.
Train: 2018-07-31T09:39:26.714604: step 10387, loss 0.538132.
Train: 2018-07-31T09:39:26.870819: step 10388, loss 0.570969.
Train: 2018-07-31T09:39:27.042646: step 10389, loss 0.570866.
Train: 2018-07-31T09:39:27.214487: step 10390, loss 0.579056.
Test: 2018-07-31T09:39:27.464400: step 10390, loss 0.54875.
Train: 2018-07-31T09:39:27.636263: step 10391, loss 0.546101.
Train: 2018-07-31T09:39:27.808092: step 10392, loss 0.661872.
Train: 2018-07-31T09:39:27.979933: step 10393, loss 0.595653.
Train: 2018-07-31T09:39:28.151767: step 10394, loss 0.562762.
Train: 2018-07-31T09:39:28.323604: step 10395, loss 0.603741.
Train: 2018-07-31T09:39:28.479816: step 10396, loss 0.587353.
Train: 2018-07-31T09:39:28.651621: step 10397, loss 0.570846.
Train: 2018-07-31T09:39:28.823487: step 10398, loss 0.472428.
Train: 2018-07-31T09:39:28.995320: step 10399, loss 0.57903.
Train: 2018-07-31T09:39:29.167123: step 10400, loss 0.51325.
Test: 2018-07-31T09:39:29.417096: step 10400, loss 0.548838.
Train: 2018-07-31T09:39:30.151299: step 10401, loss 0.496857.
Train: 2018-07-31T09:39:30.307512: step 10402, loss 0.603802.
Train: 2018-07-31T09:39:30.479316: step 10403, loss 0.58744.
Train: 2018-07-31T09:39:30.651182: step 10404, loss 0.529404.
Train: 2018-07-31T09:39:30.822986: step 10405, loss 0.579272.
Train: 2018-07-31T09:39:30.994821: step 10406, loss 0.545861.
Train: 2018-07-31T09:39:31.166656: step 10407, loss 0.504067.
Train: 2018-07-31T09:39:31.338521: step 10408, loss 0.545685.
Train: 2018-07-31T09:39:31.510325: step 10409, loss 0.511985.
Train: 2018-07-31T09:39:31.682160: step 10410, loss 0.646998.
Test: 2018-07-31T09:39:31.916514: step 10410, loss 0.548261.
Train: 2018-07-31T09:39:32.088346: step 10411, loss 0.613229.
Train: 2018-07-31T09:39:32.260149: step 10412, loss 0.537075.
Train: 2018-07-31T09:39:32.432015: step 10413, loss 0.511526.
Train: 2018-07-31T09:39:32.603849: step 10414, loss 0.639031.
Train: 2018-07-31T09:39:32.775684: step 10415, loss 0.588009.
Train: 2018-07-31T09:39:32.931893: step 10416, loss 0.579475.
Train: 2018-07-31T09:39:33.103732: step 10417, loss 0.502842.
Train: 2018-07-31T09:39:33.275567: step 10418, loss 0.562414.
Train: 2018-07-31T09:39:33.447384: step 10419, loss 0.416832.
Train: 2018-07-31T09:39:33.603616: step 10420, loss 0.562467.
Test: 2018-07-31T09:39:33.853557: step 10420, loss 0.548026.
Train: 2018-07-31T09:39:34.025391: step 10421, loss 0.519503.
Train: 2018-07-31T09:39:34.197195: step 10422, loss 0.502011.
Train: 2018-07-31T09:39:34.369032: step 10423, loss 0.588421.
Train: 2018-07-31T09:39:34.540865: step 10424, loss 0.666938.
Train: 2018-07-31T09:39:34.712731: step 10425, loss 0.47529.
Train: 2018-07-31T09:39:34.884536: step 10426, loss 0.571175.
Train: 2018-07-31T09:39:35.040779: step 10427, loss 0.623775.
Train: 2018-07-31T09:39:35.212584: step 10428, loss 0.61503.
Train: 2018-07-31T09:39:35.384448: step 10429, loss 0.553699.
Train: 2018-07-31T09:39:35.571908: step 10430, loss 0.61502.
Test: 2018-07-31T09:39:35.806226: step 10430, loss 0.547817.
Train: 2018-07-31T09:39:35.978028: step 10431, loss 0.588686.
Train: 2018-07-31T09:39:36.149894: step 10432, loss 0.527544.
Train: 2018-07-31T09:39:36.321729: step 10433, loss 0.579819.
Train: 2018-07-31T09:39:36.493534: step 10434, loss 0.579864.
Train: 2018-07-31T09:39:36.665368: step 10435, loss 0.518994.
Train: 2018-07-31T09:39:36.837203: step 10436, loss 0.571124.
Train: 2018-07-31T09:39:36.993416: step 10437, loss 0.562439.
Train: 2018-07-31T09:39:37.165282: step 10438, loss 0.675095.
Train: 2018-07-31T09:39:37.337116: step 10439, loss 0.588282.
Train: 2018-07-31T09:39:37.524573: step 10440, loss 0.545147.
Test: 2018-07-31T09:39:37.758863: step 10440, loss 0.548039.
Train: 2018-07-31T09:39:37.930727: step 10441, loss 0.545239.
Train: 2018-07-31T09:39:38.102532: step 10442, loss 0.528175.
Train: 2018-07-31T09:39:38.274397: step 10443, loss 0.528184.
Train: 2018-07-31T09:39:38.446201: step 10444, loss 0.56244.
Train: 2018-07-31T09:39:38.618035: step 10445, loss 0.579548.
Train: 2018-07-31T09:39:38.774250: step 10446, loss 0.630786.
Train: 2018-07-31T09:39:38.946085: step 10447, loss 0.570951.
Train: 2018-07-31T09:39:39.133540: step 10448, loss 0.588037.
Train: 2018-07-31T09:39:39.305405: step 10449, loss 0.613368.
Train: 2018-07-31T09:39:39.461589: step 10450, loss 0.681004.
Test: 2018-07-31T09:39:39.711530: step 10450, loss 0.548355.
Train: 2018-07-31T09:39:39.883389: step 10451, loss 0.537165.
Train: 2018-07-31T09:39:40.055224: step 10452, loss 0.478614.
Train: 2018-07-31T09:39:40.211443: step 10453, loss 0.503847.
Train: 2018-07-31T09:39:40.398899: step 10454, loss 0.562542.
Train: 2018-07-31T09:39:40.555113: step 10455, loss 0.554089.
Train: 2018-07-31T09:39:40.726918: step 10456, loss 0.579234.
Train: 2018-07-31T09:39:40.898753: step 10457, loss 0.562432.
Train: 2018-07-31T09:39:41.070625: step 10458, loss 0.554191.
Train: 2018-07-31T09:39:41.242452: step 10459, loss 0.520612.
Train: 2018-07-31T09:39:41.414286: step 10460, loss 0.629549.
Test: 2018-07-31T09:39:41.664222: step 10460, loss 0.54844.
Train: 2018-07-31T09:39:41.820436: step 10461, loss 0.5206.
Train: 2018-07-31T09:39:41.992276: step 10462, loss 0.621229.
Train: 2018-07-31T09:39:42.164111: step 10463, loss 0.562496.
Train: 2018-07-31T09:39:42.335915: step 10464, loss 0.596015.
Train: 2018-07-31T09:39:42.492159: step 10465, loss 0.512187.
Train: 2018-07-31T09:39:42.663964: step 10466, loss 0.545711.
Train: 2018-07-31T09:39:42.835798: step 10467, loss 0.486944.
Train: 2018-07-31T09:39:43.007633: step 10468, loss 0.54563.
Train: 2018-07-31T09:39:43.163846: step 10469, loss 0.621557.
Train: 2018-07-31T09:39:43.335706: step 10470, loss 0.570892.
Test: 2018-07-31T09:39:43.585622: step 10470, loss 0.548269.
Train: 2018-07-31T09:39:43.757457: step 10471, loss 0.553925.
Train: 2018-07-31T09:39:43.929292: step 10472, loss 0.579382.
Train: 2018-07-31T09:39:44.101128: step 10473, loss 0.579379.
Train: 2018-07-31T09:39:44.272962: step 10474, loss 0.604853.
Train: 2018-07-31T09:39:44.444797: step 10475, loss 0.477654.
Train: 2018-07-31T09:39:44.616632: step 10476, loss 0.604834.
Train: 2018-07-31T09:39:44.772844: step 10477, loss 0.545413.
Train: 2018-07-31T09:39:44.944680: step 10478, loss 0.56245.
Train: 2018-07-31T09:39:45.116544: step 10479, loss 0.53686.
Train: 2018-07-31T09:39:45.288379: step 10480, loss 0.587974.
Test: 2018-07-31T09:39:45.522700: step 10480, loss 0.548151.
Train: 2018-07-31T09:39:45.694534: step 10481, loss 0.528325.
Train: 2018-07-31T09:39:45.881959: step 10482, loss 0.519781.
Train: 2018-07-31T09:39:46.053795: step 10483, loss 0.545312.
Train: 2018-07-31T09:39:46.225654: step 10484, loss 0.562389.
Train: 2018-07-31T09:39:46.397465: step 10485, loss 0.562435.
Train: 2018-07-31T09:39:46.569329: step 10486, loss 0.52802.
Train: 2018-07-31T09:39:46.741164: step 10487, loss 0.53648.
Train: 2018-07-31T09:39:46.912999: step 10488, loss 0.553787.
Train: 2018-07-31T09:39:47.084834: step 10489, loss 0.571107.
Train: 2018-07-31T09:39:47.241047: step 10490, loss 0.484286.
Test: 2018-07-31T09:39:47.475338: step 10490, loss 0.547848.
Train: 2018-07-31T09:39:47.647202: step 10491, loss 0.484017.
Train: 2018-07-31T09:39:47.819036: step 10492, loss 0.536212.
Train: 2018-07-31T09:39:47.990871: step 10493, loss 0.580036.
Train: 2018-07-31T09:39:48.162677: step 10494, loss 0.606652.
Train: 2018-07-31T09:39:48.334541: step 10495, loss 0.624404.
Train: 2018-07-31T09:39:48.506370: step 10496, loss 0.562528.
Train: 2018-07-31T09:39:48.662584: step 10497, loss 0.589125.
Train: 2018-07-31T09:39:48.834394: step 10498, loss 0.615594.
Train: 2018-07-31T09:39:49.006259: step 10499, loss 0.544807.
Train: 2018-07-31T09:39:49.178093: step 10500, loss 0.51844.
Test: 2018-07-31T09:39:49.412415: step 10500, loss 0.54773.
Train: 2018-07-31T09:39:50.130996: step 10501, loss 0.58902.
Train: 2018-07-31T09:39:50.302799: step 10502, loss 0.500814.
Train: 2018-07-31T09:39:50.459012: step 10503, loss 0.624224.
Train: 2018-07-31T09:39:50.630872: step 10504, loss 0.491986.
Train: 2018-07-31T09:39:50.802716: step 10505, loss 0.58889.
Train: 2018-07-31T09:39:50.958896: step 10506, loss 0.580102.
Train: 2018-07-31T09:39:51.130731: step 10507, loss 0.5625.
Train: 2018-07-31T09:39:51.302596: step 10508, loss 0.624016.
Train: 2018-07-31T09:39:51.474430: step 10509, loss 0.606281.
Train: 2018-07-31T09:39:51.646236: step 10510, loss 0.47495.
Test: 2018-07-31T09:39:51.880557: step 10510, loss 0.547821.
Train: 2018-07-31T09:39:52.052389: step 10511, loss 0.562454.
Train: 2018-07-31T09:39:52.224255: step 10512, loss 0.588656.
Train: 2018-07-31T09:39:52.396090: step 10513, loss 0.597266.
Train: 2018-07-31T09:39:52.567921: step 10514, loss 0.571159.
Train: 2018-07-31T09:39:52.739759: step 10515, loss 0.50167.
Train: 2018-07-31T09:39:52.911563: step 10516, loss 0.553804.
Train: 2018-07-31T09:39:53.083429: step 10517, loss 0.605794.
Train: 2018-07-31T09:39:53.239643: step 10518, loss 0.545122.
Train: 2018-07-31T09:39:53.411477: step 10519, loss 0.588395.
Train: 2018-07-31T09:39:53.583282: step 10520, loss 0.545101.
Test: 2018-07-31T09:39:53.817632: step 10520, loss 0.547972.
Train: 2018-07-31T09:39:53.989436: step 10521, loss 0.545174.
Train: 2018-07-31T09:39:54.161301: step 10522, loss 0.519387.
Train: 2018-07-31T09:39:54.333136: step 10523, loss 0.476179.
Train: 2018-07-31T09:39:54.489320: step 10524, loss 0.614225.
Train: 2018-07-31T09:39:54.661184: step 10525, loss 0.597075.
Train: 2018-07-31T09:39:54.832990: step 10526, loss 0.571025.
Train: 2018-07-31T09:39:55.004824: step 10527, loss 0.605729.
Train: 2018-07-31T09:39:55.161067: step 10528, loss 0.484681.
Train: 2018-07-31T09:39:55.332902: step 10529, loss 0.553719.
Train: 2018-07-31T09:39:55.504737: step 10530, loss 0.553803.
Test: 2018-07-31T09:39:55.754649: step 10530, loss 0.547932.
Train: 2018-07-31T09:39:55.926484: step 10531, loss 0.579716.
Train: 2018-07-31T09:39:56.098348: step 10532, loss 0.579748.
Train: 2018-07-31T09:39:56.270183: step 10533, loss 0.475848.
Train: 2018-07-31T09:39:56.441986: step 10534, loss 0.519093.
Train: 2018-07-31T09:39:56.598231: step 10535, loss 0.527696.
Train: 2018-07-31T09:39:56.770066: step 10536, loss 0.562463.
Train: 2018-07-31T09:39:56.941903: step 10537, loss 0.544999.
Train: 2018-07-31T09:39:57.113704: step 10538, loss 0.588797.
Train: 2018-07-31T09:39:57.285564: step 10539, loss 0.527395.
Train: 2018-07-31T09:39:57.457405: step 10540, loss 0.597609.
Test: 2018-07-31T09:39:57.691728: step 10540, loss 0.547758.
Train: 2018-07-31T09:39:57.863560: step 10541, loss 0.544824.
Train: 2018-07-31T09:39:58.035388: step 10542, loss 0.606476.
Train: 2018-07-31T09:39:58.207199: step 10543, loss 0.527356.
Train: 2018-07-31T09:39:58.363437: step 10544, loss 0.6066.
Train: 2018-07-31T09:39:58.535278: step 10545, loss 0.597744.
Train: 2018-07-31T09:39:58.722703: step 10546, loss 0.562491.
Train: 2018-07-31T09:39:58.878916: step 10547, loss 0.527351.
Train: 2018-07-31T09:39:59.050781: step 10548, loss 0.544918.
Train: 2018-07-31T09:39:59.206964: step 10549, loss 0.492314.
Train: 2018-07-31T09:39:59.378830: step 10550, loss 0.536112.
Test: 2018-07-31T09:39:59.628766: step 10550, loss 0.547756.
Train: 2018-07-31T09:39:59.800576: step 10551, loss 0.55375.
Train: 2018-07-31T09:39:59.972441: step 10552, loss 0.571331.
Train: 2018-07-31T09:40:00.144244: step 10553, loss 0.474298.
Train: 2018-07-31T09:40:00.300458: step 10554, loss 0.535986.
Train: 2018-07-31T09:40:00.472323: step 10555, loss 0.589119.
Train: 2018-07-31T09:40:00.644127: step 10556, loss 0.553594.
Train: 2018-07-31T09:40:00.815996: step 10557, loss 0.562525.
Train: 2018-07-31T09:40:00.987798: step 10558, loss 0.535802.
Train: 2018-07-31T09:40:01.159632: step 10559, loss 0.544753.
Train: 2018-07-31T09:40:01.331497: step 10560, loss 0.625132.
Test: 2018-07-31T09:40:01.565789: step 10560, loss 0.547649.
Train: 2018-07-31T09:40:01.737623: step 10561, loss 0.625066.
Train: 2018-07-31T09:40:01.909456: step 10562, loss 0.562536.
Train: 2018-07-31T09:40:02.081292: step 10563, loss 0.598183.
Train: 2018-07-31T09:40:02.253127: step 10564, loss 0.633509.
Train: 2018-07-31T09:40:02.409339: step 10565, loss 0.598017.
Train: 2018-07-31T09:40:02.581205: step 10566, loss 0.597631.
Train: 2018-07-31T09:40:02.753009: step 10567, loss 0.509931.
Train: 2018-07-31T09:40:02.909222: step 10568, loss 0.553607.
Train: 2018-07-31T09:40:03.081088: step 10569, loss 0.50138.
Train: 2018-07-31T09:40:03.252922: step 10570, loss 0.581052.
Test: 2018-07-31T09:40:03.487214: step 10570, loss 0.547873.
Train: 2018-07-31T09:40:03.659071: step 10571, loss 0.57982.
Train: 2018-07-31T09:40:03.830883: step 10572, loss 0.623245.
Train: 2018-07-31T09:40:04.002717: step 10573, loss 0.579694.
Train: 2018-07-31T09:40:04.174551: step 10574, loss 0.493354.
Train: 2018-07-31T09:40:04.330765: step 10575, loss 0.553777.
Train: 2018-07-31T09:40:04.502600: step 10576, loss 0.484986.
Train: 2018-07-31T09:40:04.674434: step 10577, loss 0.562442.
Train: 2018-07-31T09:40:04.830678: step 10578, loss 0.579628.
Train: 2018-07-31T09:40:05.002483: step 10579, loss 0.467583.
Train: 2018-07-31T09:40:05.174318: step 10580, loss 0.536526.
Test: 2018-07-31T09:40:05.408672: step 10580, loss 0.547923.
Train: 2018-07-31T09:40:05.580471: step 10581, loss 0.553614.
Train: 2018-07-31T09:40:05.752337: step 10582, loss 0.510423.
Train: 2018-07-31T09:40:05.908521: step 10583, loss 0.492794.
Train: 2018-07-31T09:40:06.080355: step 10584, loss 0.527587.
Train: 2018-07-31T09:40:06.252219: step 10585, loss 0.544713.
Train: 2018-07-31T09:40:06.424055: step 10586, loss 0.589146.
Train: 2018-07-31T09:40:06.580268: step 10587, loss 0.571659.
Train: 2018-07-31T09:40:06.752097: step 10588, loss 0.571606.
Train: 2018-07-31T09:40:06.923909: step 10589, loss 0.580065.
Train: 2018-07-31T09:40:07.095773: step 10590, loss 0.553879.
Test: 2018-07-31T09:40:07.330062: step 10590, loss 0.547658.
Train: 2018-07-31T09:40:07.501927: step 10591, loss 0.509125.
Train: 2018-07-31T09:40:07.689354: step 10592, loss 0.518417.
Train: 2018-07-31T09:40:07.861188: step 10593, loss 0.589671.
Train: 2018-07-31T09:40:08.033053: step 10594, loss 0.553525.
Train: 2018-07-31T09:40:08.204888: step 10595, loss 0.634215.
Train: 2018-07-31T09:40:08.376718: step 10596, loss 0.526839.
Train: 2018-07-31T09:40:08.548557: step 10597, loss 0.526627.
Train: 2018-07-31T09:40:08.720362: step 10598, loss 0.643127.
Train: 2018-07-31T09:40:08.892227: step 10599, loss 0.67033.
Train: 2018-07-31T09:40:09.064056: step 10600, loss 0.553712.
Test: 2018-07-31T09:40:09.314003: step 10600, loss 0.547682.
Train: 2018-07-31T09:40:10.032580: step 10601, loss 0.500389.
Train: 2018-07-31T09:40:10.204389: step 10602, loss 0.509456.
Train: 2018-07-31T09:40:10.376255: step 10603, loss 0.580204.
Train: 2018-07-31T09:40:10.548089: step 10604, loss 0.633192.
Train: 2018-07-31T09:40:10.719893: step 10605, loss 0.483119.
Train: 2018-07-31T09:40:10.891728: step 10606, loss 0.58017.
Train: 2018-07-31T09:40:11.063587: step 10607, loss 0.606505.
Train: 2018-07-31T09:40:11.219807: step 10608, loss 0.571339.
Train: 2018-07-31T09:40:11.391612: step 10609, loss 0.553691.
Train: 2018-07-31T09:40:11.547854: step 10610, loss 0.553691.
Test: 2018-07-31T09:40:11.797767: step 10610, loss 0.547836.
Train: 2018-07-31T09:40:11.969631: step 10611, loss 0.518777.
Train: 2018-07-31T09:40:12.157059: step 10612, loss 0.518869.
Train: 2018-07-31T09:40:12.313270: step 10613, loss 0.623482.
Train: 2018-07-31T09:40:12.485135: step 10614, loss 0.614736.
Train: 2018-07-31T09:40:12.672562: step 10615, loss 0.562431.
Train: 2018-07-31T09:40:12.828775: step 10616, loss 0.562433.
Train: 2018-07-31T09:40:13.016231: step 10617, loss 0.614347.
Train: 2018-07-31T09:40:13.188066: step 10618, loss 0.553779.
Train: 2018-07-31T09:40:13.359900: step 10619, loss 0.614094.
Train: 2018-07-31T09:40:13.516114: step 10620, loss 0.648248.
Test: 2018-07-31T09:40:13.766057: step 10620, loss 0.548106.
Train: 2018-07-31T09:40:13.937919: step 10621, loss 0.545332.
Train: 2018-07-31T09:40:14.094103: step 10622, loss 0.587963.
Train: 2018-07-31T09:40:14.265974: step 10623, loss 0.579388.
Train: 2018-07-31T09:40:14.437806: step 10624, loss 0.511788.
Train: 2018-07-31T09:40:14.609638: step 10625, loss 0.604564.
Train: 2018-07-31T09:40:14.765851: step 10626, loss 0.503676.
Train: 2018-07-31T09:40:14.937655: step 10627, loss 0.545708.
Train: 2018-07-31T09:40:15.109491: step 10628, loss 0.604405.
Train: 2018-07-31T09:40:15.265735: step 10629, loss 0.520632.
Train: 2018-07-31T09:40:15.437572: step 10630, loss 0.554109.
Test: 2018-07-31T09:40:15.671890: step 10630, loss 0.548465.
Train: 2018-07-31T09:40:15.843693: step 10631, loss 0.545741.
Train: 2018-07-31T09:40:16.015529: step 10632, loss 0.654593.
Train: 2018-07-31T09:40:16.203015: step 10633, loss 0.60429.
Train: 2018-07-31T09:40:16.374850: step 10634, loss 0.587531.
Train: 2018-07-31T09:40:16.531063: step 10635, loss 0.479247.
Train: 2018-07-31T09:40:16.702903: step 10636, loss 0.612459.
Train: 2018-07-31T09:40:16.874703: step 10637, loss 0.620733.
Train: 2018-07-31T09:40:17.046567: step 10638, loss 0.587428.
Train: 2018-07-31T09:40:17.218402: step 10639, loss 0.554268.
Train: 2018-07-31T09:40:17.390207: step 10640, loss 0.546057.
Test: 2018-07-31T09:40:17.624527: step 10640, loss 0.54874.
Train: 2018-07-31T09:40:17.796361: step 10641, loss 0.55431.
Train: 2018-07-31T09:40:17.968197: step 10642, loss 0.54606.
Train: 2018-07-31T09:40:18.124439: step 10643, loss 0.603853.
Train: 2018-07-31T09:40:18.296269: step 10644, loss 0.570857.
Train: 2018-07-31T09:40:18.468112: step 10645, loss 0.521371.
Train: 2018-07-31T09:40:18.639944: step 10646, loss 0.554315.
Train: 2018-07-31T09:40:18.811779: step 10647, loss 0.603858.
Train: 2018-07-31T09:40:18.983584: step 10648, loss 0.53779.
Train: 2018-07-31T09:40:19.139828: step 10649, loss 0.488175.
Train: 2018-07-31T09:40:19.311664: step 10650, loss 0.529396.
Test: 2018-07-31T09:40:19.561574: step 10650, loss 0.548596.
Train: 2018-07-31T09:40:19.733407: step 10651, loss 0.537575.
Train: 2018-07-31T09:40:19.905244: step 10652, loss 0.504077.
Train: 2018-07-31T09:40:20.061484: step 10653, loss 0.5457.
Train: 2018-07-31T09:40:20.233322: step 10654, loss 0.528762.
Train: 2018-07-31T09:40:20.405126: step 10655, loss 0.553966.
Train: 2018-07-31T09:40:20.561369: step 10656, loss 0.604942.
Train: 2018-07-31T09:40:20.733198: step 10657, loss 0.545339.
Train: 2018-07-31T09:40:20.905039: step 10658, loss 0.605215.
Train: 2018-07-31T09:40:21.076844: step 10659, loss 0.510933.
Train: 2018-07-31T09:40:21.248678: step 10660, loss 0.631258.
Test: 2018-07-31T09:40:21.482998: step 10660, loss 0.547986.
Train: 2018-07-31T09:40:21.654833: step 10661, loss 0.648569.
Train: 2018-07-31T09:40:21.826698: step 10662, loss 0.596852.
Train: 2018-07-31T09:40:21.982882: step 10663, loss 0.536618.
Train: 2018-07-31T09:40:22.154746: step 10664, loss 0.562424.
Train: 2018-07-31T09:40:22.326581: step 10665, loss 0.613935.
Train: 2018-07-31T09:40:22.482794: step 10666, loss 0.562399.
Train: 2018-07-31T09:40:22.654629: step 10667, loss 0.519611.
Train: 2018-07-31T09:40:22.826434: step 10668, loss 0.56242.
Train: 2018-07-31T09:40:23.013921: step 10669, loss 0.570958.
Train: 2018-07-31T09:40:23.185755: step 10670, loss 0.528216.
Test: 2018-07-31T09:40:23.435691: step 10670, loss 0.548091.
Train: 2018-07-31T09:40:23.591903: step 10671, loss 0.588068.
Train: 2018-07-31T09:40:23.763744: step 10672, loss 0.605161.
Train: 2018-07-31T09:40:23.951201: step 10673, loss 0.528274.
Train: 2018-07-31T09:40:24.123036: step 10674, loss 0.494117.
Train: 2018-07-31T09:40:24.294870: step 10675, loss 0.570947.
Train: 2018-07-31T09:40:24.466705: step 10676, loss 0.519626.
Train: 2018-07-31T09:40:24.622918: step 10677, loss 0.536695.
Train: 2018-07-31T09:40:24.794753: step 10678, loss 0.59676.
Train: 2018-07-31T09:40:24.966587: step 10679, loss 0.562407.
Train: 2018-07-31T09:40:25.122770: step 10680, loss 0.605452.
Test: 2018-07-31T09:40:25.357091: step 10680, loss 0.547997.
Train: 2018-07-31T09:40:25.528952: step 10681, loss 0.588248.
Train: 2018-07-31T09:40:25.700790: step 10682, loss 0.562405.
Train: 2018-07-31T09:40:25.872596: step 10683, loss 0.614015.
Train: 2018-07-31T09:40:26.044460: step 10684, loss 0.570992.
Train: 2018-07-31T09:40:26.200644: step 10685, loss 0.588138.
Train: 2018-07-31T09:40:26.372479: step 10686, loss 0.468301.
Train: 2018-07-31T09:40:26.544343: step 10687, loss 0.528173.
Train: 2018-07-31T09:40:26.716172: step 10688, loss 0.528147.
Train: 2018-07-31T09:40:26.888008: step 10689, loss 0.519513.
Train: 2018-07-31T09:40:27.059817: step 10690, loss 0.536606.
Test: 2018-07-31T09:40:27.309778: step 10690, loss 0.547975.
Train: 2018-07-31T09:40:27.481611: step 10691, loss 0.57965.
Train: 2018-07-31T09:40:27.637838: step 10692, loss 0.588328.
Train: 2018-07-31T09:40:27.809670: step 10693, loss 0.527816.
Train: 2018-07-31T09:40:27.981477: step 10694, loss 0.553768.
Train: 2018-07-31T09:40:28.137689: step 10695, loss 0.597142.
Train: 2018-07-31T09:40:28.325176: step 10696, loss 0.61455.
Train: 2018-07-31T09:40:28.496983: step 10697, loss 0.562432.
Train: 2018-07-31T09:40:28.653193: step 10698, loss 0.57109.
Train: 2018-07-31T09:40:28.825028: step 10699, loss 0.553756.
Train: 2018-07-31T09:40:28.996864: step 10700, loss 0.536414.
Test: 2018-07-31T09:40:29.231215: step 10700, loss 0.547906.
Train: 2018-07-31T09:40:29.981008: step 10701, loss 0.493072.
Train: 2018-07-31T09:40:30.152873: step 10702, loss 0.57979.
Train: 2018-07-31T09:40:30.324677: step 10703, loss 0.527671.
Train: 2018-07-31T09:40:30.496543: step 10704, loss 0.484118.
Train: 2018-07-31T09:40:30.668348: step 10705, loss 0.579892.
Train: 2018-07-31T09:40:30.840181: step 10706, loss 0.536211.
Train: 2018-07-31T09:40:31.012017: step 10707, loss 0.474806.
Train: 2018-07-31T09:40:31.183882: step 10708, loss 0.54489.
Train: 2018-07-31T09:40:31.355687: step 10709, loss 0.580168.
Train: 2018-07-31T09:40:31.511930: step 10710, loss 0.571394.
Test: 2018-07-31T09:40:31.761871: step 10710, loss 0.547685.
Train: 2018-07-31T09:40:31.933706: step 10711, loss 0.642409.
Train: 2018-07-31T09:40:32.105511: step 10712, loss 0.660183.
Train: 2018-07-31T09:40:32.277345: step 10713, loss 0.553684.
Train: 2018-07-31T09:40:32.433589: step 10714, loss 0.544836.
Train: 2018-07-31T09:40:32.605395: step 10715, loss 0.589001.
Train: 2018-07-31T09:40:32.792874: step 10716, loss 0.562507.
Train: 2018-07-31T09:40:32.964716: step 10717, loss 0.571286.
Train: 2018-07-31T09:40:33.120929: step 10718, loss 0.588843.
Train: 2018-07-31T09:40:33.292732: step 10719, loss 0.544931.
Train: 2018-07-31T09:40:33.464598: step 10720, loss 0.562435.
Test: 2018-07-31T09:40:33.714510: step 10720, loss 0.547827.
Train: 2018-07-31T09:40:33.870723: step 10721, loss 0.599682.
Train: 2018-07-31T09:40:34.042587: step 10722, loss 0.536311.
Train: 2018-07-31T09:40:34.214392: step 10723, loss 0.553756.
Train: 2018-07-31T09:40:34.386257: step 10724, loss 0.59717.
Train: 2018-07-31T09:40:34.558095: step 10725, loss 0.631742.
Train: 2018-07-31T09:40:34.729927: step 10726, loss 0.648789.
Train: 2018-07-31T09:40:34.886139: step 10727, loss 0.588193.
Train: 2018-07-31T09:40:35.057977: step 10728, loss 0.562419.
Train: 2018-07-31T09:40:35.229779: step 10729, loss 0.4858.
Train: 2018-07-31T09:40:35.401613: step 10730, loss 0.52846.
Test: 2018-07-31T09:40:35.635965: step 10730, loss 0.548221.
Train: 2018-07-31T09:40:35.807793: step 10731, loss 0.528489.
Train: 2018-07-31T09:40:35.979633: step 10732, loss 0.469154.
Train: 2018-07-31T09:40:36.135847: step 10733, loss 0.596391.
Train: 2018-07-31T09:40:36.307652: step 10734, loss 0.596418.
Train: 2018-07-31T09:40:36.479516: step 10735, loss 0.48593.
Train: 2018-07-31T09:40:36.635732: step 10736, loss 0.545393.
Train: 2018-07-31T09:40:36.807567: step 10737, loss 0.528284.
Train: 2018-07-31T09:40:36.979405: step 10738, loss 0.502549.
Train: 2018-07-31T09:40:37.151235: step 10739, loss 0.545249.
Train: 2018-07-31T09:40:37.323063: step 10740, loss 0.571028.
Test: 2018-07-31T09:40:37.557392: step 10740, loss 0.54795.
Train: 2018-07-31T09:40:37.776058: step 10741, loss 0.562423.
Train: 2018-07-31T09:40:37.963544: step 10742, loss 0.562418.
Train: 2018-07-31T09:40:38.119728: step 10743, loss 0.588472.
Train: 2018-07-31T09:40:38.291592: step 10744, loss 0.588501.
Train: 2018-07-31T09:40:38.463427: step 10745, loss 0.55373.
Train: 2018-07-31T09:40:38.635262: step 10746, loss 0.571142.
Train: 2018-07-31T09:40:38.807065: step 10747, loss 0.536307.
Train: 2018-07-31T09:40:38.963279: step 10748, loss 0.536289.
Train: 2018-07-31T09:40:39.135115: step 10749, loss 0.483885.
Train: 2018-07-31T09:40:39.306950: step 10750, loss 0.562457.
Test: 2018-07-31T09:40:39.541269: step 10750, loss 0.547779.
Train: 2018-07-31T09:40:39.713128: step 10751, loss 0.588793.
Train: 2018-07-31T09:40:39.884956: step 10752, loss 0.623959.
Train: 2018-07-31T09:40:40.056774: step 10753, loss 0.623964.
Train: 2018-07-31T09:40:40.228633: step 10754, loss 0.553692.
Train: 2018-07-31T09:40:40.400444: step 10755, loss 0.536179.
Train: 2018-07-31T09:40:40.556656: step 10756, loss 0.544949.
Train: 2018-07-31T09:40:40.728491: step 10757, loss 0.562457.
Train: 2018-07-31T09:40:40.900326: step 10758, loss 0.49247.
Train: 2018-07-31T09:40:41.072160: step 10759, loss 0.536188.
Train: 2018-07-31T09:40:41.243996: step 10760, loss 0.615073.
Test: 2018-07-31T09:40:41.478346: step 10760, loss 0.547782.
Train: 2018-07-31T09:40:41.650150: step 10761, loss 0.509857.
Train: 2018-07-31T09:40:41.822011: step 10762, loss 0.509811.
Train: 2018-07-31T09:40:41.993856: step 10763, loss 0.580073.
Train: 2018-07-31T09:40:42.165655: step 10764, loss 0.553686.
Train: 2018-07-31T09:40:42.321892: step 10765, loss 0.597744.
Train: 2018-07-31T09:40:42.493734: step 10766, loss 0.544869.
Train: 2018-07-31T09:40:42.665563: step 10767, loss 0.562495.
Train: 2018-07-31T09:40:42.821750: step 10768, loss 0.571322.
Train: 2018-07-31T09:40:42.993586: step 10769, loss 0.509592.
Train: 2018-07-31T09:40:43.165420: step 10770, loss 0.51838.
Test: 2018-07-31T09:40:43.399740: step 10770, loss 0.547713.
Train: 2018-07-31T09:40:43.571575: step 10771, loss 0.527157.
Train: 2018-07-31T09:40:43.743411: step 10772, loss 0.580235.
Train: 2018-07-31T09:40:43.915245: step 10773, loss 0.571401.
Train: 2018-07-31T09:40:44.087110: step 10774, loss 0.615779.
Train: 2018-07-31T09:40:44.258914: step 10775, loss 0.615746.
Train: 2018-07-31T09:40:44.415127: step 10776, loss 0.535965.
Train: 2018-07-31T09:40:44.586993: step 10777, loss 0.527151.
Train: 2018-07-31T09:40:44.758797: step 10778, loss 0.527159.
Train: 2018-07-31T09:40:44.930633: step 10779, loss 0.571352.
Train: 2018-07-31T09:40:45.086846: step 10780, loss 0.527165.
Test: 2018-07-31T09:40:45.336818: step 10780, loss 0.547709.
Train: 2018-07-31T09:40:45.508621: step 10781, loss 0.535993.
Train: 2018-07-31T09:40:45.664836: step 10782, loss 0.518284.
Train: 2018-07-31T09:40:45.836670: step 10783, loss 0.72202.
Train: 2018-07-31T09:40:46.008535: step 10784, loss 0.606736.
Train: 2018-07-31T09:40:46.180369: step 10785, loss 0.606592.
Train: 2018-07-31T09:40:46.336584: step 10786, loss 0.52733.
Train: 2018-07-31T09:40:46.508388: step 10787, loss 0.562463.
Train: 2018-07-31T09:40:46.680248: step 10788, loss 0.606169.
Train: 2018-07-31T09:40:46.852087: step 10789, loss 0.53629.
Train: 2018-07-31T09:40:47.008301: step 10790, loss 0.588508.
Test: 2018-07-31T09:40:47.258212: step 10790, loss 0.547906.
Train: 2018-07-31T09:40:47.430047: step 10791, loss 0.484407.
Train: 2018-07-31T09:40:47.586260: step 10792, loss 0.510458.
Train: 2018-07-31T09:40:47.758095: step 10793, loss 0.571082.
Train: 2018-07-31T09:40:47.929930: step 10794, loss 0.475812.
Train: 2018-07-31T09:40:48.101765: step 10795, loss 0.553743.
Train: 2018-07-31T09:40:48.273630: step 10796, loss 0.54506.
Train: 2018-07-31T09:40:48.429846: step 10797, loss 0.545029.
Train: 2018-07-31T09:40:48.601678: step 10798, loss 0.579878.
Train: 2018-07-31T09:40:48.773483: step 10799, loss 0.606095.
Train: 2018-07-31T09:40:48.945347: step 10800, loss 0.544983.
Test: 2018-07-31T09:40:49.195259: step 10800, loss 0.547819.
Train: 2018-07-31T09:40:49.913870: step 10801, loss 0.579915.
Train: 2018-07-31T09:40:50.085675: step 10802, loss 0.588655.
Train: 2018-07-31T09:40:50.257510: step 10803, loss 0.571177.
Train: 2018-07-31T09:40:50.429378: step 10804, loss 0.553723.
Train: 2018-07-31T09:40:50.601210: step 10805, loss 0.51886.
Train: 2018-07-31T09:40:50.773014: step 10806, loss 0.492704.
Train: 2018-07-31T09:40:50.944849: step 10807, loss 0.658478.
Train: 2018-07-31T09:40:51.116714: step 10808, loss 0.536263.
Train: 2018-07-31T09:40:51.272927: step 10809, loss 0.571161.
Train: 2018-07-31T09:40:51.444731: step 10810, loss 0.579878.
Test: 2018-07-31T09:40:51.679054: step 10810, loss 0.547847.
Train: 2018-07-31T09:40:51.850917: step 10811, loss 0.536301.
Train: 2018-07-31T09:40:52.022751: step 10812, loss 0.605977.
Train: 2018-07-31T09:40:52.194587: step 10813, loss 0.553742.
Train: 2018-07-31T09:40:52.350769: step 10814, loss 0.640661.
Train: 2018-07-31T09:40:52.522604: step 10815, loss 0.562433.
Train: 2018-07-31T09:40:52.678817: step 10816, loss 0.536467.
Train: 2018-07-31T09:40:52.850683: step 10817, loss 0.519243.
Train: 2018-07-31T09:40:53.006897: step 10818, loss 0.61421.
Train: 2018-07-31T09:40:53.178734: step 10819, loss 0.519315.
Train: 2018-07-31T09:40:53.350535: step 10820, loss 0.553791.
Test: 2018-07-31T09:40:53.584856: step 10820, loss 0.547994.
Train: 2018-07-31T09:40:53.756690: step 10821, loss 0.605467.
Train: 2018-07-31T09:40:53.928525: step 10822, loss 0.571017.
Train: 2018-07-31T09:40:54.084739: step 10823, loss 0.545226.
Train: 2018-07-31T09:40:54.272195: step 10824, loss 0.596734.
Train: 2018-07-31T09:40:54.444029: step 10825, loss 0.502456.
Train: 2018-07-31T09:40:54.600242: step 10826, loss 0.528146.
Train: 2018-07-31T09:40:54.772108: step 10827, loss 0.502403.
Train: 2018-07-31T09:40:54.943943: step 10828, loss 0.622535.
Train: 2018-07-31T09:40:55.115777: step 10829, loss 0.571005.
Train: 2018-07-31T09:40:55.287583: step 10830, loss 0.536624.
Test: 2018-07-31T09:40:55.521933: step 10830, loss 0.548007.
Train: 2018-07-31T09:40:55.693738: step 10831, loss 0.528012.
Train: 2018-07-31T09:40:55.865602: step 10832, loss 0.502139.
Train: 2018-07-31T09:40:56.037436: step 10833, loss 0.545145.
Train: 2018-07-31T09:40:56.209272: step 10834, loss 0.614343.
Train: 2018-07-31T09:40:56.381075: step 10835, loss 0.527766.
Train: 2018-07-31T09:40:56.537326: step 10836, loss 0.605821.
Train: 2018-07-31T09:40:56.709124: step 10837, loss 0.571108.
Train: 2018-07-31T09:40:56.880958: step 10838, loss 0.640612.
Train: 2018-07-31T09:40:57.068415: step 10839, loss 0.553751.
Train: 2018-07-31T09:40:57.224658: step 10840, loss 0.588413.
Test: 2018-07-31T09:40:57.474569: step 10840, loss 0.54793.
Train: 2018-07-31T09:40:57.646435: step 10841, loss 0.50186.
Train: 2018-07-31T09:40:57.818270: step 10842, loss 0.501873.
Train: 2018-07-31T09:40:57.990073: step 10843, loss 0.605711.
Train: 2018-07-31T09:40:58.146317: step 10844, loss 0.579733.
Train: 2018-07-31T09:40:58.318153: step 10845, loss 0.597035.
Train: 2018-07-31T09:40:58.489987: step 10846, loss 0.674785.
Train: 2018-07-31T09:40:58.661792: step 10847, loss 0.596868.
Train: 2018-07-31T09:40:58.833627: step 10848, loss 0.528077.
Train: 2018-07-31T09:40:58.989870: step 10849, loss 0.545296.
Train: 2018-07-31T09:40:59.177326: step 10850, loss 0.553866.
Test: 2018-07-31T09:40:59.411649: step 10850, loss 0.548134.
Train: 2018-07-31T09:40:59.583481: step 10851, loss 0.562416.
Train: 2018-07-31T09:40:59.755285: step 10852, loss 0.596474.
Train: 2018-07-31T09:40:59.927152: step 10853, loss 0.528433.
Train: 2018-07-31T09:41:00.098985: step 10854, loss 0.562429.
Train: 2018-07-31T09:41:00.270820: step 10855, loss 0.53699.
Train: 2018-07-31T09:41:00.427037: step 10856, loss 0.587861.
Train: 2018-07-31T09:41:00.598868: step 10857, loss 0.587853.
Train: 2018-07-31T09:41:00.770703: step 10858, loss 0.596287.
Train: 2018-07-31T09:41:00.942538: step 10859, loss 0.537084.
Train: 2018-07-31T09:41:01.098722: step 10860, loss 0.528664.
Test: 2018-07-31T09:41:01.348662: step 10860, loss 0.548299.
Train: 2018-07-31T09:41:01.520527: step 10861, loss 0.570887.
Train: 2018-07-31T09:41:01.692363: step 10862, loss 0.613085.
Train: 2018-07-31T09:41:01.864166: step 10863, loss 0.520283.
Train: 2018-07-31T09:41:02.020380: step 10864, loss 0.54557.
Train: 2018-07-31T09:41:02.192245: step 10865, loss 0.596191.
Train: 2018-07-31T09:41:02.364080: step 10866, loss 0.596185.
Train: 2018-07-31T09:41:02.535914: step 10867, loss 0.638283.
Train: 2018-07-31T09:41:02.692128: step 10868, loss 0.503607.
Train: 2018-07-31T09:41:02.863963: step 10869, loss 0.596074.
Train: 2018-07-31T09:41:03.035768: step 10870, loss 0.596016.
Test: 2018-07-31T09:41:03.270088: step 10870, loss 0.548448.
Train: 2018-07-31T09:41:03.441923: step 10871, loss 0.537345.
Train: 2018-07-31T09:41:03.613788: step 10872, loss 0.616034.
Train: 2018-07-31T09:41:03.785623: step 10873, loss 0.545788.
Train: 2018-07-31T09:41:03.957427: step 10874, loss 0.59585.
Train: 2018-07-31T09:41:04.129292: step 10875, loss 0.562509.
Train: 2018-07-31T09:41:04.285505: step 10876, loss 0.629019.
Train: 2018-07-31T09:41:04.457311: step 10877, loss 0.554241.
Train: 2018-07-31T09:41:04.629144: step 10878, loss 0.587376.
Train: 2018-07-31T09:41:04.801010: step 10879, loss 0.58735.
Train: 2018-07-31T09:41:04.972814: step 10880, loss 0.620238.
Test: 2018-07-31T09:41:05.207134: step 10880, loss 0.548873.
Train: 2018-07-31T09:41:05.378999: step 10881, loss 0.603709.
Train: 2018-07-31T09:41:05.535182: step 10882, loss 0.562639.
Train: 2018-07-31T09:41:05.707073: step 10883, loss 0.513783.
Train: 2018-07-31T09:41:05.878882: step 10884, loss 0.554549.
Train: 2018-07-31T09:41:06.050716: step 10885, loss 0.538271.
Train: 2018-07-31T09:41:06.222551: step 10886, loss 0.562687.
Train: 2018-07-31T09:41:06.394387: step 10887, loss 0.554578.
Train: 2018-07-31T09:41:06.566221: step 10888, loss 0.546374.
Train: 2018-07-31T09:41:06.738026: step 10889, loss 0.562719.
Train: 2018-07-31T09:41:06.909891: step 10890, loss 0.595343.
Test: 2018-07-31T09:41:07.144211: step 10890, loss 0.548971.
Train: 2018-07-31T09:41:07.331636: step 10891, loss 0.52176.
Train: 2018-07-31T09:41:07.487880: step 10892, loss 0.529866.
Train: 2018-07-31T09:41:07.659684: step 10893, loss 0.603735.
Train: 2018-07-31T09:41:07.831549: step 10894, loss 0.505009.
Train: 2018-07-31T09:41:08.003355: step 10895, loss 0.562576.
Train: 2018-07-31T09:41:08.190810: step 10896, loss 0.587401.
Train: 2018-07-31T09:41:08.362675: step 10897, loss 0.58743.
Train: 2018-07-31T09:41:08.518859: step 10898, loss 0.545891.
Train: 2018-07-31T09:41:08.690724: step 10899, loss 0.529185.
Train: 2018-07-31T09:41:08.862528: step 10900, loss 0.637666.
Test: 2018-07-31T09:41:09.096850: step 10900, loss 0.548483.
Train: 2018-07-31T09:41:09.877946: step 10901, loss 0.545765.
Train: 2018-07-31T09:41:10.049751: step 10902, loss 0.587593.
Train: 2018-07-31T09:41:10.221615: step 10903, loss 0.637855.
Train: 2018-07-31T09:41:10.393444: step 10904, loss 0.545739.
Train: 2018-07-31T09:41:10.565254: step 10905, loss 0.604309.
Train: 2018-07-31T09:41:10.737091: step 10906, loss 0.629336.
Train: 2018-07-31T09:41:10.924544: step 10907, loss 0.620854.
Train: 2018-07-31T09:41:11.080758: step 10908, loss 0.587448.
Train: 2018-07-31T09:41:11.252624: step 10909, loss 0.479731.
Train: 2018-07-31T09:41:11.424428: step 10910, loss 0.570826.
Test: 2018-07-31T09:41:11.674394: step 10910, loss 0.548719.
Train: 2018-07-31T09:41:11.846229: step 10911, loss 0.537767.
Train: 2018-07-31T09:41:12.002448: step 10912, loss 0.50471.
Train: 2018-07-31T09:41:12.174277: step 10913, loss 0.529447.
Train: 2018-07-31T09:41:12.346118: step 10914, loss 0.529356.
Train: 2018-07-31T09:41:12.517922: step 10915, loss 0.604104.
Train: 2018-07-31T09:41:12.689758: step 10916, loss 0.554165.
Train: 2018-07-31T09:41:12.861622: step 10917, loss 0.562491.
Train: 2018-07-31T09:41:13.017835: step 10918, loss 0.51229.
Train: 2018-07-31T09:41:13.189639: step 10919, loss 0.520517.
Train: 2018-07-31T09:41:13.361505: step 10920, loss 0.461403.
Test: 2018-07-31T09:41:13.595826: step 10920, loss 0.548248.
Train: 2018-07-31T09:41:13.783280: step 10921, loss 0.537027.
Train: 2018-07-31T09:41:13.939495: step 10922, loss 0.570931.
Train: 2018-07-31T09:41:14.111299: step 10923, loss 0.51963.
Train: 2018-07-31T09:41:14.298788: step 10924, loss 0.485002.
Train: 2018-07-31T09:41:14.470614: step 10925, loss 0.519145.
Train: 2018-07-31T09:41:14.626827: step 10926, loss 0.50146.
Train: 2018-07-31T09:41:14.814290: step 10927, loss 0.51861.
Train: 2018-07-31T09:41:14.986130: step 10928, loss 0.633177.
Train: 2018-07-31T09:41:15.157928: step 10929, loss 0.571416.
Train: 2018-07-31T09:41:15.314178: step 10930, loss 0.544757.
Test: 2018-07-31T09:41:15.564114: step 10930, loss 0.547635.
Train: 2018-07-31T09:41:15.720322: step 10931, loss 0.589437.
Train: 2018-07-31T09:41:15.892132: step 10932, loss 0.580567.
Train: 2018-07-31T09:41:16.063967: step 10933, loss 0.526733.
Train: 2018-07-31T09:41:16.220210: step 10934, loss 0.553681.
Train: 2018-07-31T09:41:16.392016: step 10935, loss 0.64384.
Train: 2018-07-31T09:41:16.563874: step 10936, loss 0.517629.
Train: 2018-07-31T09:41:16.735684: step 10937, loss 0.571724.
Train: 2018-07-31T09:41:16.891897: step 10938, loss 0.553685.
Train: 2018-07-31T09:41:17.063757: step 10939, loss 0.535646.
Train: 2018-07-31T09:41:17.219946: step 10940, loss 0.526621.
Test: 2018-07-31T09:41:17.469918: step 10940, loss 0.547597.
Train: 2018-07-31T09:41:17.626131: step 10941, loss 0.544661.
Train: 2018-07-31T09:41:17.797968: step 10942, loss 0.544658.
Train: 2018-07-31T09:41:17.969800: step 10943, loss 0.571776.
Train: 2018-07-31T09:41:18.141635: step 10944, loss 0.526563.
Train: 2018-07-31T09:41:18.297850: step 10945, loss 0.472245.
Train: 2018-07-31T09:41:18.469685: step 10946, loss 0.571843.
Train: 2018-07-31T09:41:18.641489: step 10947, loss 0.599126.
Train: 2018-07-31T09:41:18.813353: step 10948, loss 0.590058.
Train: 2018-07-31T09:41:18.985189: step 10949, loss 0.680856.
Train: 2018-07-31T09:41:19.157024: step 10950, loss 0.617092.
Test: 2018-07-31T09:41:19.391343: step 10950, loss 0.547599.
Train: 2018-07-31T09:41:19.563147: step 10951, loss 0.553687.
Train: 2018-07-31T09:41:19.750604: step 10952, loss 0.562668.
Train: 2018-07-31T09:41:19.922469: step 10953, loss 0.517836.
Train: 2018-07-31T09:41:20.109894: step 10954, loss 0.508982.
Train: 2018-07-31T09:41:20.281729: step 10955, loss 0.616143.
Train: 2018-07-31T09:41:20.437972: step 10956, loss 0.589281.
Train: 2018-07-31T09:41:20.609777: step 10957, loss 0.589179.
Train: 2018-07-31T09:41:20.781642: step 10958, loss 0.659861.
Train: 2018-07-31T09:41:20.969069: step 10959, loss 0.60651.
Train: 2018-07-31T09:41:21.140935: step 10960, loss 0.606237.
Test: 2018-07-31T09:41:21.375254: step 10960, loss 0.547856.
Train: 2018-07-31T09:41:21.547057: step 10961, loss 0.492802.
Train: 2018-07-31T09:41:21.718928: step 10962, loss 0.484405.
Train: 2018-07-31T09:41:21.875105: step 10963, loss 0.562411.
Train: 2018-07-31T09:41:22.046966: step 10964, loss 0.553783.
Train: 2018-07-31T09:41:22.218805: step 10965, loss 0.553791.
Train: 2018-07-31T09:41:22.390640: step 10966, loss 0.614053.
Train: 2018-07-31T09:41:22.562444: step 10967, loss 0.493693.
Train: 2018-07-31T09:41:22.718658: step 10968, loss 0.528071.
Train: 2018-07-31T09:41:22.890523: step 10969, loss 0.62251.
Train: 2018-07-31T09:41:23.062328: step 10970, loss 0.605291.
Test: 2018-07-31T09:41:23.296686: step 10970, loss 0.548074.
Train: 2018-07-31T09:41:23.468482: step 10971, loss 0.596646.
Train: 2018-07-31T09:41:23.640317: step 10972, loss 0.570949.
Train: 2018-07-31T09:41:23.812183: step 10973, loss 0.528343.
Train: 2018-07-31T09:41:23.983987: step 10974, loss 0.536888.
Train: 2018-07-31T09:41:24.155852: step 10975, loss 0.468901.
Train: 2018-07-31T09:41:24.327681: step 10976, loss 0.494305.
Train: 2018-07-31T09:41:24.499491: step 10977, loss 0.54533.
Train: 2018-07-31T09:41:24.671327: step 10978, loss 0.613779.
Train: 2018-07-31T09:41:24.843192: step 10979, loss 0.545259.
Train: 2018-07-31T09:41:25.014995: step 10980, loss 0.631131.
Test: 2018-07-31T09:41:25.249342: step 10980, loss 0.54802.
Train: 2018-07-31T09:41:25.421180: step 10981, loss 0.61396.
Train: 2018-07-31T09:41:25.593016: step 10982, loss 0.605323.
Train: 2018-07-31T09:41:25.764844: step 10983, loss 0.570973.
Train: 2018-07-31T09:41:25.936685: step 10984, loss 0.579511.
Train: 2018-07-31T09:41:26.108490: step 10985, loss 0.545335.
Train: 2018-07-31T09:41:26.264703: step 10986, loss 0.536835.
Train: 2018-07-31T09:41:26.436537: step 10987, loss 0.605005.
Train: 2018-07-31T09:41:26.592751: step 10988, loss 0.562406.
Train: 2018-07-31T09:41:26.780237: step 10989, loss 0.451991.
Train: 2018-07-31T09:41:26.936451: step 10990, loss 0.613455.
Test: 2018-07-31T09:41:27.186398: step 10990, loss 0.548171.
Train: 2018-07-31T09:41:27.342606: step 10991, loss 0.545408.
Train: 2018-07-31T09:41:27.514440: step 10992, loss 0.553904.
Train: 2018-07-31T09:41:27.686276: step 10993, loss 0.528352.
Train: 2018-07-31T09:41:27.858080: step 10994, loss 0.605044.
Train: 2018-07-31T09:41:28.029914: step 10995, loss 0.596526.
Train: 2018-07-31T09:41:28.201749: step 10996, loss 0.553885.
Train: 2018-07-31T09:41:28.373585: step 10997, loss 0.690284.
Train: 2018-07-31T09:41:28.529797: step 10998, loss 0.528425.
Train: 2018-07-31T09:41:28.701632: step 10999, loss 0.604858.
Train: 2018-07-31T09:41:28.873467: step 11000, loss 0.57088.
Test: 2018-07-31T09:41:29.107819: step 11000, loss 0.548291.
Train: 2018-07-31T09:41:29.826399: step 11001, loss 0.520207.
Train: 2018-07-31T09:41:29.998203: step 11002, loss 0.562447.
Train: 2018-07-31T09:41:30.185661: step 11003, loss 0.596159.
Train: 2018-07-31T09:41:30.357524: step 11004, loss 0.562444.
Train: 2018-07-31T09:41:30.529329: step 11005, loss 0.51202.
Train: 2018-07-31T09:41:30.701194: step 11006, loss 0.596073.
Train: 2018-07-31T09:41:30.873024: step 11007, loss 0.579271.
Train: 2018-07-31T09:41:31.044864: step 11008, loss 0.621228.
Train: 2018-07-31T09:41:31.216699: step 11009, loss 0.570855.
Train: 2018-07-31T09:41:31.388534: step 11010, loss 0.554117.
Test: 2018-07-31T09:41:31.622854: step 11010, loss 0.548495.
Train: 2018-07-31T09:41:31.794688: step 11011, loss 0.570843.
Train: 2018-07-31T09:41:31.950902: step 11012, loss 0.462374.
Train: 2018-07-31T09:41:32.122706: step 11013, loss 0.587542.
Train: 2018-07-31T09:41:32.310196: step 11014, loss 0.545761.
Train: 2018-07-31T09:41:32.466405: step 11015, loss 0.604314.
Train: 2018-07-31T09:41:32.638211: step 11016, loss 0.562464.
Train: 2018-07-31T09:41:32.825697: step 11017, loss 0.461989.
Train: 2018-07-31T09:41:32.997526: step 11018, loss 0.646413.
Train: 2018-07-31T09:41:33.169366: step 11019, loss 0.537255.
Train: 2018-07-31T09:41:33.341201: step 11020, loss 0.47835.
Test: 2018-07-31T09:41:33.575517: step 11020, loss 0.548312.
Train: 2018-07-31T09:41:33.747359: step 11021, loss 0.511831.
Train: 2018-07-31T09:41:33.919161: step 11022, loss 0.528555.
Train: 2018-07-31T09:41:34.075374: step 11023, loss 0.580563.
Train: 2018-07-31T09:41:34.247234: step 11024, loss 0.562411.
Train: 2018-07-31T09:41:34.419068: step 11025, loss 0.562405.
Train: 2018-07-31T09:41:34.590903: step 11026, loss 0.67399.
Train: 2018-07-31T09:41:34.762712: step 11027, loss 0.553828.
Train: 2018-07-31T09:41:34.934573: step 11028, loss 0.467947.
Train: 2018-07-31T09:41:35.090761: step 11029, loss 0.502159.
Train: 2018-07-31T09:41:35.262596: step 11030, loss 0.588318.
Test: 2018-07-31T09:41:35.512537: step 11030, loss 0.547921.
Train: 2018-07-31T09:41:35.684402: step 11031, loss 0.545104.
Train: 2018-07-31T09:41:35.856206: step 11032, loss 0.588448.
Train: 2018-07-31T09:41:36.028042: step 11033, loss 0.518974.
Train: 2018-07-31T09:41:36.199876: step 11034, loss 0.597274.
Train: 2018-07-31T09:41:36.371712: step 11035, loss 0.59732.
Train: 2018-07-31T09:41:36.543545: step 11036, loss 0.579889.
Train: 2018-07-31T09:41:36.715380: step 11037, loss 0.440319.
Train: 2018-07-31T09:41:36.871625: step 11038, loss 0.54496.
Train: 2018-07-31T09:41:37.043459: step 11039, loss 0.641327.
Train: 2018-07-31T09:41:37.215294: step 11040, loss 0.65013.
Test: 2018-07-31T09:41:37.465204: step 11040, loss 0.547795.
Train: 2018-07-31T09:41:37.637039: step 11041, loss 0.544946.
Train: 2018-07-31T09:41:37.793277: step 11042, loss 0.649888.
Train: 2018-07-31T09:41:37.965088: step 11043, loss 0.545001.
Train: 2018-07-31T09:41:38.121331: step 11044, loss 0.527627.
Train: 2018-07-31T09:41:38.293136: step 11045, loss 0.527671.
Train: 2018-07-31T09:41:38.465001: step 11046, loss 0.536375.
Train: 2018-07-31T09:41:38.636837: step 11047, loss 0.51901.
Train: 2018-07-31T09:41:38.808640: step 11048, loss 0.440786.
Train: 2018-07-31T09:41:38.980506: step 11049, loss 0.562434.
Train: 2018-07-31T09:41:39.152340: step 11050, loss 0.606136.
Test: 2018-07-31T09:41:39.386630: step 11050, loss 0.547798.
Train: 2018-07-31T09:41:39.558466: step 11051, loss 0.597457.
Train: 2018-07-31T09:41:39.730330: step 11052, loss 0.571211.
Train: 2018-07-31T09:41:39.902164: step 11053, loss 0.57997.
Train: 2018-07-31T09:41:40.058349: step 11054, loss 0.54495.
Train: 2018-07-31T09:41:40.230183: step 11055, loss 0.518681.
Train: 2018-07-31T09:41:40.402017: step 11056, loss 0.562463.
Train: 2018-07-31T09:41:40.573883: step 11057, loss 0.632596.
Train: 2018-07-31T09:41:40.745686: step 11058, loss 0.571218.
Train: 2018-07-31T09:41:40.917554: step 11059, loss 0.623694.
Train: 2018-07-31T09:41:41.089386: step 11060, loss 0.51007.
Test: 2018-07-31T09:41:41.308091: step 11060, loss 0.547835.
Train: 2018-07-31T09:41:41.495512: step 11061, loss 0.588595.
Train: 2018-07-31T09:41:41.667379: step 11062, loss 0.614666.
Train: 2018-07-31T09:41:41.839212: step 11063, loss 0.510327.
Train: 2018-07-31T09:41:42.011016: step 11064, loss 0.467033.
Train: 2018-07-31T09:41:42.182874: step 11065, loss 0.631842.
Train: 2018-07-31T09:41:42.354715: step 11066, loss 0.631784.
Train: 2018-07-31T09:41:42.526550: step 11067, loss 0.579719.
Train: 2018-07-31T09:41:42.698385: step 11068, loss 0.562413.
Train: 2018-07-31T09:41:42.870220: step 11069, loss 0.545182.
Train: 2018-07-31T09:41:43.042024: step 11070, loss 0.49362.
Test: 2018-07-31T09:41:43.276376: step 11070, loss 0.548008.
Train: 2018-07-31T09:41:43.463831: step 11071, loss 0.588204.
Train: 2018-07-31T09:41:43.620013: step 11072, loss 0.510851.
Train: 2018-07-31T09:41:43.791879: step 11073, loss 0.536616.
Train: 2018-07-31T09:41:43.963709: step 11074, loss 0.579621.
Train: 2018-07-31T09:41:44.135542: step 11075, loss 0.519356.
Train: 2018-07-31T09:41:44.322974: step 11076, loss 0.56241.
Train: 2018-07-31T09:41:44.479217: step 11077, loss 0.562411.
Train: 2018-07-31T09:41:44.651053: step 11078, loss 0.527852.
Train: 2018-07-31T09:41:44.822888: step 11079, loss 0.579721.
Train: 2018-07-31T09:41:44.994722: step 11080, loss 0.510441.
Test: 2018-07-31T09:41:45.229043: step 11080, loss 0.547887.
Train: 2018-07-31T09:41:45.400877: step 11081, loss 0.631858.
Train: 2018-07-31T09:41:45.572713: step 11082, loss 0.501651.
Train: 2018-07-31T09:41:45.744517: step 11083, loss 0.54504.
Train: 2018-07-31T09:41:45.916351: step 11084, loss 0.510184.
Train: 2018-07-31T09:41:46.088217: step 11085, loss 0.544985.
Train: 2018-07-31T09:41:46.260051: step 11086, loss 0.553703.
Train: 2018-07-31T09:41:46.431868: step 11087, loss 0.606308.
Train: 2018-07-31T09:41:46.603720: step 11088, loss 0.536134.
Train: 2018-07-31T09:41:46.775555: step 11089, loss 0.597635.
Train: 2018-07-31T09:41:46.947360: step 11090, loss 0.562478.
Test: 2018-07-31T09:41:47.181705: step 11090, loss 0.547748.
Train: 2018-07-31T09:41:47.353515: step 11091, loss 0.56248.
Train: 2018-07-31T09:41:47.509758: step 11092, loss 0.544886.
Train: 2018-07-31T09:41:47.681562: step 11093, loss 0.536081.
Train: 2018-07-31T09:41:47.853397: step 11094, loss 0.553681.
Train: 2018-07-31T09:41:48.025257: step 11095, loss 0.606553.
Train: 2018-07-31T09:41:48.197067: step 11096, loss 0.509634.
Train: 2018-07-31T09:41:48.368902: step 11097, loss 0.571308.
Train: 2018-07-31T09:41:48.540736: step 11098, loss 0.633023.
Train: 2018-07-31T09:41:48.696975: step 11099, loss 0.624119.
Train: 2018-07-31T09:41:48.868785: step 11100, loss 0.571255.
Test: 2018-07-31T09:41:49.103107: step 11100, loss 0.547786.
Train: 2018-07-31T09:41:49.884172: step 11101, loss 0.509891.
Train: 2018-07-31T09:41:50.056007: step 11102, loss 0.553702.
Train: 2018-07-31T09:41:50.227872: step 11103, loss 0.536229.
Train: 2018-07-31T09:41:50.399707: step 11104, loss 0.614851.
Train: 2018-07-31T09:41:50.571541: step 11105, loss 0.571156.
Train: 2018-07-31T09:41:50.743345: step 11106, loss 0.562431.
Train: 2018-07-31T09:41:50.899590: step 11107, loss 0.571115.
Train: 2018-07-31T09:41:51.087045: step 11108, loss 0.588449.
Train: 2018-07-31T09:41:51.258880: step 11109, loss 0.545102.
Train: 2018-07-31T09:41:51.430715: step 11110, loss 0.579702.
Test: 2018-07-31T09:41:51.665038: step 11110, loss 0.547961.
Train: 2018-07-31T09:41:51.836871: step 11111, loss 0.588294.
Train: 2018-07-31T09:41:52.008704: step 11112, loss 0.553798.
Train: 2018-07-31T09:41:52.196161: step 11113, loss 0.613966.
Train: 2018-07-31T09:41:52.352375: step 11114, loss 0.493848.
Train: 2018-07-31T09:41:52.524203: step 11115, loss 0.622335.
Train: 2018-07-31T09:41:52.696043: step 11116, loss 0.528238.
Train: 2018-07-31T09:41:52.867879: step 11117, loss 0.6136.
Train: 2018-07-31T09:41:53.039684: step 11118, loss 0.528359.
Train: 2018-07-31T09:41:53.211518: step 11119, loss 0.545407.
Train: 2018-07-31T09:41:53.367730: step 11120, loss 0.553916.
Test: 2018-07-31T09:41:53.602051: step 11120, loss 0.54819.
Train: 2018-07-31T09:41:53.773916: step 11121, loss 0.536933.
Train: 2018-07-31T09:41:53.945751: step 11122, loss 0.553921.
Train: 2018-07-31T09:41:54.117591: step 11123, loss 0.562416.
Train: 2018-07-31T09:41:54.289391: step 11124, loss 0.579421.
Train: 2018-07-31T09:41:54.461250: step 11125, loss 0.485869.
Train: 2018-07-31T09:41:54.648712: step 11126, loss 0.553892.
Train: 2018-07-31T09:41:54.820548: step 11127, loss 0.545338.
Train: 2018-07-31T09:41:54.976753: step 11128, loss 0.502547.
Train: 2018-07-31T09:41:55.148597: step 11129, loss 0.596715.
Train: 2018-07-31T09:41:55.320398: step 11130, loss 0.588191.
Test: 2018-07-31T09:41:55.554719: step 11130, loss 0.547996.
Train: 2018-07-31T09:41:55.726554: step 11131, loss 0.545199.
Train: 2018-07-31T09:41:55.898422: step 11132, loss 0.51932.
Train: 2018-07-31T09:41:56.070223: step 11133, loss 0.562412.
Train: 2018-07-31T09:41:56.257704: step 11134, loss 0.545109.
Train: 2018-07-31T09:41:56.429545: step 11135, loss 0.597108.
Train: 2018-07-31T09:41:56.585728: step 11136, loss 0.54506.
Train: 2018-07-31T09:41:56.757593: step 11137, loss 0.588502.
Train: 2018-07-31T09:41:56.929427: step 11138, loss 0.623303.
Train: 2018-07-31T09:41:57.101262: step 11139, loss 0.492908.
Train: 2018-07-31T09:41:57.273067: step 11140, loss 0.588512.
Test: 2018-07-31T09:41:57.523010: step 11140, loss 0.547866.
Train: 2018-07-31T09:41:57.694843: step 11141, loss 0.545039.
Train: 2018-07-31T09:41:57.866678: step 11142, loss 0.56243.
Train: 2018-07-31T09:41:58.038545: step 11143, loss 0.536333.
Train: 2018-07-31T09:41:58.194755: step 11144, loss 0.571138.
Train: 2018-07-31T09:41:58.366560: step 11145, loss 0.571138.
Train: 2018-07-31T09:41:58.538429: step 11146, loss 0.553725.
Train: 2018-07-31T09:41:58.710231: step 11147, loss 0.545016.
Train: 2018-07-31T09:41:58.882065: step 11148, loss 0.579858.
Train: 2018-07-31T09:41:59.038278: step 11149, loss 0.527587.
Train: 2018-07-31T09:41:59.210144: step 11150, loss 0.545003.
Test: 2018-07-31T09:41:59.460054: step 11150, loss 0.547829.
Train: 2018-07-31T09:41:59.631888: step 11151, loss 0.518819.
Train: 2018-07-31T09:41:59.788103: step 11152, loss 0.562445.
Train: 2018-07-31T09:41:59.959968: step 11153, loss 0.614941.
Train: 2018-07-31T09:42:00.147424: step 11154, loss 0.614941.
Train: 2018-07-31T09:42:00.319258: step 11155, loss 0.492538.
Train: 2018-07-31T09:42:00.475466: step 11156, loss 0.562446.
Train: 2018-07-31T09:42:00.647276: step 11157, loss 0.562448.
Train: 2018-07-31T09:42:00.819144: step 11158, loss 0.649882.
Train: 2018-07-31T09:42:00.990976: step 11159, loss 0.614808.
Train: 2018-07-31T09:42:01.162781: step 11160, loss 0.536319.
Test: 2018-07-31T09:42:01.397132: step 11160, loss 0.547876.
Train: 2018-07-31T09:42:01.568966: step 11161, loss 0.562425.
Train: 2018-07-31T09:42:01.740771: step 11162, loss 0.510396.
Train: 2018-07-31T09:42:01.896984: step 11163, loss 0.631735.
Train: 2018-07-31T09:42:02.068848: step 11164, loss 0.54512.
Train: 2018-07-31T09:42:02.240678: step 11165, loss 0.640112.
Train: 2018-07-31T09:42:02.412489: step 11166, loss 0.605445.
Train: 2018-07-31T09:42:02.584353: step 11167, loss 0.528101.
Train: 2018-07-31T09:42:02.756157: step 11168, loss 0.630843.
Train: 2018-07-31T09:42:02.912402: step 11169, loss 0.443094.
Train: 2018-07-31T09:42:03.084237: step 11170, loss 0.604991.
Test: 2018-07-31T09:42:03.334147: step 11170, loss 0.548176.
Train: 2018-07-31T09:42:03.506012: step 11171, loss 0.502901.
Train: 2018-07-31T09:42:03.677817: step 11172, loss 0.61342.
Train: 2018-07-31T09:42:03.849651: step 11173, loss 0.545436.
Train: 2018-07-31T09:42:04.021487: step 11174, loss 0.56242.
Train: 2018-07-31T09:42:04.193320: step 11175, loss 0.587863.
Train: 2018-07-31T09:42:04.365156: step 11176, loss 0.579369.
Train: 2018-07-31T09:42:04.537021: step 11177, loss 0.562427.
Train: 2018-07-31T09:42:04.708855: step 11178, loss 0.537066.
Train: 2018-07-31T09:42:04.880659: step 11179, loss 0.545526.
Train: 2018-07-31T09:42:05.052525: step 11180, loss 0.680783.
Test: 2018-07-31T09:42:05.286840: step 11180, loss 0.548312.
Train: 2018-07-31T09:42:05.474301: step 11181, loss 0.520266.
Train: 2018-07-31T09:42:05.646136: step 11182, loss 0.570869.
Train: 2018-07-31T09:42:05.817940: step 11183, loss 0.562448.
Train: 2018-07-31T09:42:05.974153: step 11184, loss 0.604494.
Train: 2018-07-31T09:42:06.146019: step 11185, loss 0.570853.
Train: 2018-07-31T09:42:06.317824: step 11186, loss 0.637909.
Train: 2018-07-31T09:42:06.489691: step 11187, loss 0.529046.
Train: 2018-07-31T09:42:06.645871: step 11188, loss 0.570835.
Train: 2018-07-31T09:42:06.817707: step 11189, loss 0.51252.
Train: 2018-07-31T09:42:06.989571: step 11190, loss 0.545847.
Test: 2018-07-31T09:42:07.223889: step 11190, loss 0.548549.
Train: 2018-07-31T09:42:07.395727: step 11191, loss 0.662469.
Train: 2018-07-31T09:42:07.567555: step 11192, loss 0.545879.
Train: 2018-07-31T09:42:07.739366: step 11193, loss 0.554213.
Train: 2018-07-31T09:42:07.911201: step 11194, loss 0.612331.
Train: 2018-07-31T09:42:08.083065: step 11195, loss 0.554247.
Train: 2018-07-31T09:42:08.254901: step 11196, loss 0.579104.
Train: 2018-07-31T09:42:08.411116: step 11197, loss 0.529469.
Train: 2018-07-31T09:42:08.582918: step 11198, loss 0.587361.
Train: 2018-07-31T09:42:08.754753: step 11199, loss 0.587353.
Train: 2018-07-31T09:42:08.926624: step 11200, loss 0.579083.
Test: 2018-07-31T09:42:09.160908: step 11200, loss 0.548751.
Train: 2018-07-31T09:42:09.926354: step 11201, loss 0.513061.
Train: 2018-07-31T09:42:10.098188: step 11202, loss 0.51303.
Train: 2018-07-31T09:42:10.270023: step 11203, loss 0.56255.
Train: 2018-07-31T09:42:10.441888: step 11204, loss 0.570825.
Train: 2018-07-31T09:42:10.613723: step 11205, loss 0.537638.
Train: 2018-07-31T09:42:10.769936: step 11206, loss 0.545883.
Train: 2018-07-31T09:42:10.957392: step 11207, loss 0.545829.
Train: 2018-07-31T09:42:11.113605: step 11208, loss 0.503986.
Train: 2018-07-31T09:42:11.285410: step 11209, loss 0.562463.
Train: 2018-07-31T09:42:11.457275: step 11210, loss 0.511942.
Test: 2018-07-31T09:42:11.707218: step 11210, loss 0.548272.
Train: 2018-07-31T09:42:11.879021: step 11211, loss 0.545523.
Train: 2018-07-31T09:42:12.050855: step 11212, loss 0.545437.
Train: 2018-07-31T09:42:12.222692: step 11213, loss 0.596513.
Train: 2018-07-31T09:42:12.394557: step 11214, loss 0.588066.
Train: 2018-07-31T09:42:12.566384: step 11215, loss 0.605269.
Train: 2018-07-31T09:42:12.722604: step 11216, loss 0.459417.
Train: 2018-07-31T09:42:12.894439: step 11217, loss 0.605456.
Train: 2018-07-31T09:42:13.066244: step 11218, loss 0.527903.
Train: 2018-07-31T09:42:13.222487: step 11219, loss 0.53647.
Train: 2018-07-31T09:42:13.394292: step 11220, loss 0.562419.
Test: 2018-07-31T09:42:13.628612: step 11220, loss 0.54787.
Train: 2018-07-31T09:42:13.800446: step 11221, loss 0.492897.
Train: 2018-07-31T09:42:13.972281: step 11222, loss 0.544996.
Train: 2018-07-31T09:42:14.144148: step 11223, loss 0.553699.
Train: 2018-07-31T09:42:14.315981: step 11224, loss 0.544912.
Train: 2018-07-31T09:42:14.487815: step 11225, loss 0.641723.
Train: 2018-07-31T09:42:14.659619: step 11226, loss 0.50961.
Train: 2018-07-31T09:42:14.831485: step 11227, loss 0.491865.
Train: 2018-07-31T09:42:15.003289: step 11228, loss 0.509392.
Train: 2018-07-31T09:42:15.175124: step 11229, loss 0.562551.
Train: 2018-07-31T09:42:15.346984: step 11230, loss 0.589314.
Test: 2018-07-31T09:42:15.581310: step 11230, loss 0.547638.
Train: 2018-07-31T09:42:15.753145: step 11231, loss 0.598316.
Train: 2018-07-31T09:42:15.924950: step 11232, loss 0.553664.
Train: 2018-07-31T09:42:16.096808: step 11233, loss 0.553665.
Train: 2018-07-31T09:42:16.268617: step 11234, loss 0.517858.
Train: 2018-07-31T09:42:16.440484: step 11235, loss 0.553668.
Train: 2018-07-31T09:42:16.612287: step 11236, loss 0.643409.
Train: 2018-07-31T09:42:16.784155: step 11237, loss 0.580571.
Train: 2018-07-31T09:42:16.955988: step 11238, loss 0.580538.
Train: 2018-07-31T09:42:17.127822: step 11239, loss 0.526838.
Train: 2018-07-31T09:42:17.299657: step 11240, loss 0.607268.
Test: 2018-07-31T09:42:17.549568: step 11240, loss 0.547647.
Train: 2018-07-31T09:42:17.721402: step 11241, loss 0.580415.
Train: 2018-07-31T09:42:17.893268: step 11242, loss 0.535869.
Train: 2018-07-31T09:42:18.065072: step 11243, loss 0.615842.
Train: 2018-07-31T09:42:18.221316: step 11244, loss 0.562525.
Train: 2018-07-31T09:42:18.393121: step 11245, loss 0.624357.
Train: 2018-07-31T09:42:18.564955: step 11246, loss 0.483252.
Train: 2018-07-31T09:42:18.736791: step 11247, loss 0.641554.
Train: 2018-07-31T09:42:18.908665: step 11248, loss 0.597485.
Train: 2018-07-31T09:42:19.064838: step 11249, loss 0.518819.
Train: 2018-07-31T09:42:19.236703: step 11250, loss 0.553728.
Test: 2018-07-31T09:42:19.470994: step 11250, loss 0.547883.
Train: 2018-07-31T09:42:19.642859: step 11251, loss 0.562422.
Train: 2018-07-31T09:42:19.830284: step 11252, loss 0.553755.
Train: 2018-07-31T09:42:20.002143: step 11253, loss 0.527824.
Train: 2018-07-31T09:42:20.169879: step 11254, loss 0.553772.
Train: 2018-07-31T09:42:20.357302: step 11255, loss 0.588309.
Train: 2018-07-31T09:42:20.544758: step 11256, loss 0.571032.
Train: 2018-07-31T09:42:20.700971: step 11257, loss 0.66575.
Train: 2018-07-31T09:42:20.872836: step 11258, loss 0.613907.
Train: 2018-07-31T09:42:21.029021: step 11259, loss 0.622248.
Train: 2018-07-31T09:42:21.200886: step 11260, loss 0.57092.
Test: 2018-07-31T09:42:21.450828: step 11260, loss 0.548241.
Train: 2018-07-31T09:42:21.622631: step 11261, loss 0.570891.
Train: 2018-07-31T09:42:21.778875: step 11262, loss 0.562438.
Train: 2018-07-31T09:42:21.950679: step 11263, loss 0.545652.
Train: 2018-07-31T09:42:22.122515: step 11264, loss 0.562468.
Train: 2018-07-31T09:42:22.294379: step 11265, loss 0.579192.
Train: 2018-07-31T09:42:22.466183: step 11266, loss 0.562497.
Train: 2018-07-31T09:42:22.638048: step 11267, loss 0.595776.
Train: 2018-07-31T09:42:22.809852: step 11268, loss 0.570825.
Train: 2018-07-31T09:42:22.966097: step 11269, loss 0.587376.
Train: 2018-07-31T09:42:23.184796: step 11270, loss 0.513031.
Test: 2018-07-31T09:42:23.419117: step 11270, loss 0.548759.
Train: 2018-07-31T09:42:23.590951: step 11271, loss 0.537829.
Train: 2018-07-31T09:42:23.762754: step 11272, loss 0.595566.
Train: 2018-07-31T09:42:23.934620: step 11273, loss 0.56258.
Train: 2018-07-31T09:42:24.106454: step 11274, loss 0.579062.
Train: 2018-07-31T09:42:24.278289: step 11275, loss 0.562587.
Train: 2018-07-31T09:42:24.450094: step 11276, loss 0.611991.
Train: 2018-07-31T09:42:24.621959: step 11277, loss 0.595496.
Train: 2018-07-31T09:42:24.778172: step 11278, loss 0.480503.
Train: 2018-07-31T09:42:24.950008: step 11279, loss 0.587259.
Train: 2018-07-31T09:42:25.121843: step 11280, loss 0.579042.
Test: 2018-07-31T09:42:25.356134: step 11280, loss 0.54884.
Train: 2018-07-31T09:42:25.527967: step 11281, loss 0.529729.
Train: 2018-07-31T09:42:25.699801: step 11282, loss 0.537914.
Train: 2018-07-31T09:42:25.871666: step 11283, loss 0.603787.
Train: 2018-07-31T09:42:26.043501: step 11284, loss 0.595563.
Train: 2018-07-31T09:42:26.215306: step 11285, loss 0.554326.
Train: 2018-07-31T09:42:26.387141: step 11286, loss 0.471793.
Train: 2018-07-31T09:42:26.558975: step 11287, loss 0.545999.
Train: 2018-07-31T09:42:26.715188: step 11288, loss 0.496136.
Train: 2018-07-31T09:42:26.887023: step 11289, loss 0.462486.
Train: 2018-07-31T09:42:27.058858: step 11290, loss 0.570848.
Test: 2018-07-31T09:42:27.293204: step 11290, loss 0.548319.
Train: 2018-07-31T09:42:27.465044: step 11291, loss 0.5793.
Train: 2018-07-31T09:42:27.636848: step 11292, loss 0.664059.
Train: 2018-07-31T09:42:27.808716: step 11293, loss 0.528472.
Train: 2018-07-31T09:42:27.980518: step 11294, loss 0.587937.
Train: 2018-07-31T09:42:28.152382: step 11295, loss 0.468647.
Train: 2018-07-31T09:42:28.324188: step 11296, loss 0.570961.
Train: 2018-07-31T09:42:28.496022: step 11297, loss 0.553824.
Train: 2018-07-31T09:42:28.667857: step 11298, loss 0.493556.
Train: 2018-07-31T09:42:28.839722: step 11299, loss 0.562412.
Train: 2018-07-31T09:42:29.011526: step 11300, loss 0.467026.
Test: 2018-07-31T09:42:29.245877: step 11300, loss 0.547837.
Train: 2018-07-31T09:42:30.026943: step 11301, loss 0.579866.
Train: 2018-07-31T09:42:30.198749: step 11302, loss 0.518688.
Train: 2018-07-31T09:42:30.354962: step 11303, loss 0.588854.
Train: 2018-07-31T09:42:30.542449: step 11304, loss 0.536035.
Train: 2018-07-31T09:42:30.714253: step 11305, loss 0.544819.
Train: 2018-07-31T09:42:30.886117: step 11306, loss 0.527024.
Train: 2018-07-31T09:42:31.057952: step 11307, loss 0.526922.
Train: 2018-07-31T09:42:31.229756: step 11308, loss 0.589449.
Train: 2018-07-31T09:42:31.401591: step 11309, loss 0.580578.
Train: 2018-07-31T09:42:31.573425: step 11310, loss 0.535711.
Test: 2018-07-31T09:42:31.807780: step 11310, loss 0.547603.
Train: 2018-07-31T09:42:31.979581: step 11311, loss 0.508681.
Train: 2018-07-31T09:42:32.151416: step 11312, loss 0.59879.
Train: 2018-07-31T09:42:32.307659: step 11313, loss 0.517557.
Train: 2018-07-31T09:42:32.479464: step 11314, loss 0.517496.
Train: 2018-07-31T09:42:32.651329: step 11315, loss 0.617187.
Train: 2018-07-31T09:42:32.807542: step 11316, loss 0.635387.
Train: 2018-07-31T09:42:32.979378: step 11317, loss 0.626238.
Train: 2018-07-31T09:42:33.151212: step 11318, loss 0.553694.
Train: 2018-07-31T09:42:33.323053: step 11319, loss 0.571738.
Train: 2018-07-31T09:42:33.494851: step 11320, loss 0.544671.
Test: 2018-07-31T09:42:33.729171: step 11320, loss 0.547607.
Train: 2018-07-31T09:42:33.901005: step 11321, loss 0.625583.
Train: 2018-07-31T09:42:34.072840: step 11322, loss 0.526786.
Train: 2018-07-31T09:42:34.229085: step 11323, loss 0.50003.
Train: 2018-07-31T09:42:34.400920: step 11324, loss 0.553663.
Train: 2018-07-31T09:42:34.572724: step 11325, loss 0.695784.
Train: 2018-07-31T09:42:34.744583: step 11326, loss 0.56255.
Train: 2018-07-31T09:42:34.900773: step 11327, loss 0.482813.
Train: 2018-07-31T09:42:35.072638: step 11328, loss 0.562511.
Train: 2018-07-31T09:42:35.260062: step 11329, loss 0.491887.
Train: 2018-07-31T09:42:35.431928: step 11330, loss 0.571321.
Test: 2018-07-31T09:42:35.666248: step 11330, loss 0.547723.
Train: 2018-07-31T09:42:35.838077: step 11331, loss 0.58895.
Train: 2018-07-31T09:42:36.025539: step 11332, loss 0.615341.
Train: 2018-07-31T09:42:36.181723: step 11333, loss 0.492152.
Train: 2018-07-31T09:42:36.369179: step 11334, loss 0.536117.
Train: 2018-07-31T09:42:36.541014: step 11335, loss 0.553688.
Train: 2018-07-31T09:42:36.697251: step 11336, loss 0.588811.
Train: 2018-07-31T09:42:36.869091: step 11337, loss 0.55369.
Train: 2018-07-31T09:42:37.040926: step 11338, loss 0.448463.
Train: 2018-07-31T09:42:37.212761: step 11339, loss 0.562469.
Train: 2018-07-31T09:42:37.384566: step 11340, loss 0.62404.
Test: 2018-07-31T09:42:37.618886: step 11340, loss 0.547748.
Train: 2018-07-31T09:42:37.821963: step 11341, loss 0.492125.
Train: 2018-07-31T09:42:37.993798: step 11342, loss 0.562483.
Train: 2018-07-31T09:42:38.150041: step 11343, loss 0.597736.
Train: 2018-07-31T09:42:38.321877: step 11344, loss 0.553675.
Train: 2018-07-31T09:42:38.493680: step 11345, loss 0.562489.
Train: 2018-07-31T09:42:38.665516: step 11346, loss 0.668239.
Train: 2018-07-31T09:42:38.821760: step 11347, loss 0.597644.
Train: 2018-07-31T09:42:38.993563: step 11348, loss 0.553693.
Train: 2018-07-31T09:42:39.149777: step 11349, loss 0.614924.
Train: 2018-07-31T09:42:39.321643: step 11350, loss 0.527568.
Test: 2018-07-31T09:42:39.555967: step 11350, loss 0.547862.
Train: 2018-07-31T09:42:39.727766: step 11351, loss 0.579817.
Train: 2018-07-31T09:42:39.899631: step 11352, loss 0.536399.
Train: 2018-07-31T09:42:40.071436: step 11353, loss 0.536443.
Train: 2018-07-31T09:42:40.243301: step 11354, loss 0.519178.
Train: 2018-07-31T09:42:40.415135: step 11355, loss 0.596989.
Train: 2018-07-31T09:42:40.586941: step 11356, loss 0.545138.
Train: 2018-07-31T09:42:40.758805: step 11357, loss 0.579672.
Train: 2018-07-31T09:42:40.930611: step 11358, loss 0.536535.
Train: 2018-07-31T09:42:41.102475: step 11359, loss 0.631382.
Train: 2018-07-31T09:42:41.274309: step 11360, loss 0.510761.
Test: 2018-07-31T09:42:41.524245: step 11360, loss 0.547997.
Train: 2018-07-31T09:42:41.680435: step 11361, loss 0.553801.
Train: 2018-07-31T09:42:41.852269: step 11362, loss 0.5194.
Train: 2018-07-31T09:42:42.024129: step 11363, loss 0.596832.
Train: 2018-07-31T09:42:42.195963: step 11364, loss 0.545195.
Train: 2018-07-31T09:42:42.367774: step 11365, loss 0.527978.
Train: 2018-07-31T09:42:42.539638: step 11366, loss 0.579635.
Train: 2018-07-31T09:42:42.711468: step 11367, loss 0.648587.
Train: 2018-07-31T09:42:42.867657: step 11368, loss 0.562406.
Train: 2018-07-31T09:42:43.039522: step 11369, loss 0.562403.
Train: 2018-07-31T09:42:43.211326: step 11370, loss 0.528068.
Test: 2018-07-31T09:42:43.445677: step 11370, loss 0.548032.
Train: 2018-07-31T09:42:43.617481: step 11371, loss 0.54524.
Train: 2018-07-31T09:42:43.789315: step 11372, loss 0.656808.
Train: 2018-07-31T09:42:43.945560: step 11373, loss 0.579539.
Train: 2018-07-31T09:42:44.117394: step 11374, loss 0.5795.
Train: 2018-07-31T09:42:44.289223: step 11375, loss 0.545351.
Train: 2018-07-31T09:42:44.461063: step 11376, loss 0.46023.
Train: 2018-07-31T09:42:44.632868: step 11377, loss 0.55389.
Train: 2018-07-31T09:42:44.789112: step 11378, loss 0.536823.
Train: 2018-07-31T09:42:44.960916: step 11379, loss 0.51117.
Train: 2018-07-31T09:42:45.132751: step 11380, loss 0.545287.
Test: 2018-07-31T09:42:45.367102: step 11380, loss 0.548034.
Train: 2018-07-31T09:42:45.538936: step 11381, loss 0.562405.
Train: 2018-07-31T09:42:45.710771: step 11382, loss 0.545207.
Train: 2018-07-31T09:42:45.882608: step 11383, loss 0.536551.
Train: 2018-07-31T09:42:46.054411: step 11384, loss 0.501924.
Train: 2018-07-31T09:42:46.226274: step 11385, loss 0.527734.
Train: 2018-07-31T09:42:46.382459: step 11386, loss 0.536319.
Train: 2018-07-31T09:42:46.554327: step 11387, loss 0.606132.
Train: 2018-07-31T09:42:46.726128: step 11388, loss 0.518659.
Train: 2018-07-31T09:42:46.897993: step 11389, loss 0.509759.
Train: 2018-07-31T09:42:47.054177: step 11390, loss 0.536042.
Test: 2018-07-31T09:42:47.304150: step 11390, loss 0.547696.
Train: 2018-07-31T09:42:47.475979: step 11391, loss 0.50942.
Train: 2018-07-31T09:42:47.647817: step 11392, loss 0.544778.
Train: 2018-07-31T09:42:47.819653: step 11393, loss 0.544741.
Train: 2018-07-31T09:42:47.991456: step 11394, loss 0.643204.
Train: 2018-07-31T09:42:48.163290: step 11395, loss 0.553667.
Train: 2018-07-31T09:42:48.319504: step 11396, loss 0.526731.
Train: 2018-07-31T09:42:48.491370: step 11397, loss 0.553675.
Train: 2018-07-31T09:42:48.663174: step 11398, loss 0.526653.
Train: 2018-07-31T09:42:48.835008: step 11399, loss 0.499531.
Train: 2018-07-31T09:42:49.006875: step 11400, loss 0.535594.
Test: 2018-07-31T09:42:49.241194: step 11400, loss 0.547581.
Train: 2018-07-31T09:42:49.990989: step 11401, loss 0.562772.
Train: 2018-07-31T09:42:50.162847: step 11402, loss 0.617347.
Train: 2018-07-31T09:42:50.334688: step 11403, loss 0.644672.
Train: 2018-07-31T09:42:50.506492: step 11404, loss 0.59913.
Train: 2018-07-31T09:42:50.678357: step 11405, loss 0.571831.
Train: 2018-07-31T09:42:50.834541: step 11406, loss 0.580827.
Train: 2018-07-31T09:42:51.006375: step 11407, loss 0.508574.
Train: 2018-07-31T09:42:51.178240: step 11408, loss 0.643749.
Train: 2018-07-31T09:42:51.350076: step 11409, loss 0.508774.
Train: 2018-07-31T09:42:51.521910: step 11410, loss 0.562627.
Test: 2018-07-31T09:42:51.756231: step 11410, loss 0.54763.
Train: 2018-07-31T09:42:51.928034: step 11411, loss 0.616261.
Train: 2018-07-31T09:42:52.099869: step 11412, loss 0.571493.
Train: 2018-07-31T09:42:52.271734: step 11413, loss 0.598101.
Train: 2018-07-31T09:42:52.443570: step 11414, loss 0.571377.
Train: 2018-07-31T09:42:52.615373: step 11415, loss 0.509554.
Train: 2018-07-31T09:42:52.787238: step 11416, loss 0.580086.
Train: 2018-07-31T09:42:52.943446: step 11417, loss 0.536126.
Train: 2018-07-31T09:42:53.115257: step 11418, loss 0.509876.
Train: 2018-07-31T09:42:53.287091: step 11419, loss 0.614992.
Train: 2018-07-31T09:42:53.458925: step 11420, loss 0.68481.
Test: 2018-07-31T09:42:53.693248: step 11420, loss 0.547851.
Train: 2018-07-31T09:42:53.880703: step 11421, loss 0.614651.
Train: 2018-07-31T09:42:54.036946: step 11422, loss 0.5884.
Train: 2018-07-31T09:42:54.208750: step 11423, loss 0.571025.
Train: 2018-07-31T09:42:54.380616: step 11424, loss 0.57956.
Train: 2018-07-31T09:42:54.552444: step 11425, loss 0.570944.
Train: 2018-07-31T09:42:54.708659: step 11426, loss 0.587917.
Train: 2018-07-31T09:42:54.880501: step 11427, loss 0.59628.
Train: 2018-07-31T09:42:55.052338: step 11428, loss 0.554015.
Train: 2018-07-31T09:42:55.224137: step 11429, loss 0.6212.
Train: 2018-07-31T09:42:55.396003: step 11430, loss 0.570835.
Test: 2018-07-31T09:42:55.630294: step 11430, loss 0.548582.
Train: 2018-07-31T09:42:55.802128: step 11431, loss 0.595775.
Train: 2018-07-31T09:42:55.973962: step 11432, loss 0.521147.
Train: 2018-07-31T09:42:56.145827: step 11433, loss 0.562566.
Train: 2018-07-31T09:42:56.317631: step 11434, loss 0.570822.
Train: 2018-07-31T09:42:56.489467: step 11435, loss 0.579039.
Train: 2018-07-31T09:42:56.661300: step 11436, loss 0.58722.
Train: 2018-07-31T09:42:56.833136: step 11437, loss 0.538115.
Train: 2018-07-31T09:42:56.989380: step 11438, loss 0.62801.
Train: 2018-07-31T09:42:57.176806: step 11439, loss 0.595285.
Train: 2018-07-31T09:42:57.348672: step 11440, loss 0.530214.
Test: 2018-07-31T09:42:57.582962: step 11440, loss 0.549145.
Train: 2018-07-31T09:42:57.754826: step 11441, loss 0.514045.
Train: 2018-07-31T09:42:57.926630: step 11442, loss 0.61955.
Train: 2018-07-31T09:42:58.098464: step 11443, loss 0.538418.
Train: 2018-07-31T09:42:58.270300: step 11444, loss 0.611399.
Train: 2018-07-31T09:42:58.426543: step 11445, loss 0.570857.
Train: 2018-07-31T09:42:58.598378: step 11446, loss 0.546571.
Train: 2018-07-31T09:42:58.770182: step 11447, loss 0.51418.
Train: 2018-07-31T09:42:58.942017: step 11448, loss 0.603289.
Train: 2018-07-31T09:42:59.113882: step 11449, loss 0.554624.
Train: 2018-07-31T09:42:59.285687: step 11450, loss 0.538357.
Test: 2018-07-31T09:42:59.520007: step 11450, loss 0.549077.
Train: 2018-07-31T09:42:59.691873: step 11451, loss 0.595253.
Train: 2018-07-31T09:42:59.863706: step 11452, loss 0.587134.
Train: 2018-07-31T09:43:00.035536: step 11453, loss 0.505626.
Train: 2018-07-31T09:43:00.207376: step 11454, loss 0.521814.
Train: 2018-07-31T09:43:00.379210: step 11455, loss 0.562632.
Train: 2018-07-31T09:43:00.551015: step 11456, loss 0.587256.
Train: 2018-07-31T09:43:00.707228: step 11457, loss 0.521362.
Train: 2018-07-31T09:43:00.879094: step 11458, loss 0.579115.
Train: 2018-07-31T09:43:01.050898: step 11459, loss 0.521077.
Train: 2018-07-31T09:43:01.222733: step 11460, loss 0.470975.
Test: 2018-07-31T09:43:01.457055: step 11460, loss 0.548451.
Train: 2018-07-31T09:43:01.628924: step 11461, loss 0.537313.
Train: 2018-07-31T09:43:01.800723: step 11462, loss 0.553861.
Train: 2018-07-31T09:43:01.956936: step 11463, loss 0.604569.
Train: 2018-07-31T09:43:02.128807: step 11464, loss 0.579318.
Train: 2018-07-31T09:43:02.300606: step 11465, loss 0.484882.
Train: 2018-07-31T09:43:02.488092: step 11466, loss 0.56251.
Train: 2018-07-31T09:43:02.659896: step 11467, loss 0.595199.
Train: 2018-07-31T09:43:02.831730: step 11468, loss 0.580962.
Train: 2018-07-31T09:43:03.003566: step 11469, loss 0.579926.
Train: 2018-07-31T09:43:03.159779: step 11470, loss 0.535287.
Test: 2018-07-31T09:43:03.409745: step 11470, loss 0.547597.
Train: 2018-07-31T09:43:03.581556: step 11471, loss 0.581679.
Train: 2018-07-31T09:43:03.753421: step 11472, loss 0.608435.
Train: 2018-07-31T09:43:03.925225: step 11473, loss 0.519811.
Train: 2018-07-31T09:43:04.097059: step 11474, loss 0.588915.
Train: 2018-07-31T09:43:04.253304: step 11475, loss 0.589583.
Train: 2018-07-31T09:43:04.425146: step 11476, loss 0.581601.
Train: 2018-07-31T09:43:04.596973: step 11477, loss 0.519244.
Train: 2018-07-31T09:43:04.768778: step 11478, loss 0.57096.
Train: 2018-07-31T09:43:04.940613: step 11479, loss 0.605396.
Train: 2018-07-31T09:43:05.112471: step 11480, loss 0.545135.
Test: 2018-07-31T09:43:05.346767: step 11480, loss 0.548034.
Train: 2018-07-31T09:43:05.518601: step 11481, loss 0.528081.
Train: 2018-07-31T09:43:05.690468: step 11482, loss 0.553831.
Train: 2018-07-31T09:43:05.862301: step 11483, loss 0.553873.
Train: 2018-07-31T09:43:06.034136: step 11484, loss 0.588117.
Train: 2018-07-31T09:43:06.190321: step 11485, loss 0.622374.
Train: 2018-07-31T09:43:06.362154: step 11486, loss 0.56241.
Train: 2018-07-31T09:43:06.534020: step 11487, loss 0.519723.
Train: 2018-07-31T09:43:06.705855: step 11488, loss 0.494098.
Train: 2018-07-31T09:43:06.877688: step 11489, loss 0.579499.
Train: 2018-07-31T09:43:07.049523: step 11490, loss 0.596645.
Test: 2018-07-31T09:43:07.283845: step 11490, loss 0.548075.
Train: 2018-07-31T09:43:07.455679: step 11491, loss 0.562415.
Train: 2018-07-31T09:43:07.627519: step 11492, loss 0.562387.
Train: 2018-07-31T09:43:07.799318: step 11493, loss 0.579511.
Train: 2018-07-31T09:43:07.971152: step 11494, loss 0.493989.
Train: 2018-07-31T09:43:08.127365: step 11495, loss 0.57094.
Train: 2018-07-31T09:43:08.299233: step 11496, loss 0.536707.
Train: 2018-07-31T09:43:08.471066: step 11497, loss 0.579638.
Train: 2018-07-31T09:43:08.642871: step 11498, loss 0.596791.
Train: 2018-07-31T09:43:08.814736: step 11499, loss 0.536644.
Train: 2018-07-31T09:43:08.986570: step 11500, loss 0.467854.
Test: 2018-07-31T09:43:09.220891: step 11500, loss 0.547979.
Train: 2018-07-31T09:43:09.970714: step 11501, loss 0.545226.
Train: 2018-07-31T09:43:10.142520: step 11502, loss 0.510606.
Train: 2018-07-31T09:43:10.314385: step 11503, loss 0.51042.
Train: 2018-07-31T09:43:10.486189: step 11504, loss 0.545055.
Train: 2018-07-31T09:43:10.642403: step 11505, loss 0.52747.
Train: 2018-07-31T09:43:10.814268: step 11506, loss 0.544943.
Train: 2018-07-31T09:43:10.986103: step 11507, loss 0.606542.
Train: 2018-07-31T09:43:11.157937: step 11508, loss 0.606646.
Train: 2018-07-31T09:43:11.329742: step 11509, loss 0.527242.
Train: 2018-07-31T09:43:11.485985: step 11510, loss 0.597875.
Test: 2018-07-31T09:43:11.735898: step 11510, loss 0.54769.
Train: 2018-07-31T09:43:11.907730: step 11511, loss 0.562544.
Train: 2018-07-31T09:43:12.079565: step 11512, loss 0.562541.
Train: 2018-07-31T09:43:12.251400: step 11513, loss 0.527082.
Train: 2018-07-31T09:43:12.423268: step 11514, loss 0.58025.
Train: 2018-07-31T09:43:12.579479: step 11515, loss 0.553695.
Train: 2018-07-31T09:43:12.766905: step 11516, loss 0.58024.
Train: 2018-07-31T09:43:12.923148: step 11517, loss 0.642337.
Train: 2018-07-31T09:43:13.094952: step 11518, loss 0.651032.
Train: 2018-07-31T09:43:13.266813: step 11519, loss 0.509576.
Train: 2018-07-31T09:43:13.423001: step 11520, loss 0.588892.
Test: 2018-07-31T09:43:13.672973: step 11520, loss 0.547772.
Train: 2018-07-31T09:43:13.829156: step 11521, loss 0.518628.
Train: 2018-07-31T09:43:14.000991: step 11522, loss 0.562482.
Train: 2018-07-31T09:43:14.172826: step 11523, loss 0.728525.
Train: 2018-07-31T09:43:14.344661: step 11524, loss 0.588515.
Train: 2018-07-31T09:43:14.516525: step 11525, loss 0.614293.
Train: 2018-07-31T09:43:14.672709: step 11526, loss 0.55378.
Train: 2018-07-31T09:43:14.860164: step 11527, loss 0.502495.
Train: 2018-07-31T09:43:15.016378: step 11528, loss 0.579467.
Train: 2018-07-31T09:43:15.203846: step 11529, loss 0.519883.
Train: 2018-07-31T09:43:15.360072: step 11530, loss 0.562415.
Test: 2018-07-31T09:43:15.610014: step 11530, loss 0.548233.
Train: 2018-07-31T09:43:15.781824: step 11531, loss 0.579361.
Train: 2018-07-31T09:43:15.953658: step 11532, loss 0.596254.
Train: 2018-07-31T09:43:16.125524: step 11533, loss 0.61304.
Train: 2018-07-31T09:43:16.297328: step 11534, loss 0.62132.
Train: 2018-07-31T09:43:16.469193: step 11535, loss 0.512213.
Train: 2018-07-31T09:43:16.641028: step 11536, loss 0.570834.
Train: 2018-07-31T09:43:16.797235: step 11537, loss 0.54582.
Train: 2018-07-31T09:43:16.969076: step 11538, loss 0.587481.
Train: 2018-07-31T09:43:17.140910: step 11539, loss 0.595755.
Train: 2018-07-31T09:43:17.312746: step 11540, loss 0.554241.
Test: 2018-07-31T09:43:17.562656: step 11540, loss 0.548677.
Train: 2018-07-31T09:43:17.734522: step 11541, loss 0.554263.
Train: 2018-07-31T09:43:17.906326: step 11542, loss 0.537743.
Train: 2018-07-31T09:43:18.062565: step 11543, loss 0.50468.
Train: 2018-07-31T09:43:18.234405: step 11544, loss 0.545989.
Train: 2018-07-31T09:43:18.406208: step 11545, loss 0.55424.
Train: 2018-07-31T09:43:18.578045: step 11546, loss 0.479449.
Train: 2018-07-31T09:43:18.734257: step 11547, loss 0.562493.
Train: 2018-07-31T09:43:18.906122: step 11548, loss 0.512282.
Train: 2018-07-31T09:43:19.093578: step 11549, loss 0.596058.
Train: 2018-07-31T09:43:19.249762: step 11550, loss 0.528728.
Test: 2018-07-31T09:43:19.499703: step 11550, loss 0.548258.
Train: 2018-07-31T09:43:19.671568: step 11551, loss 0.587798.
Train: 2018-07-31T09:43:19.843373: step 11552, loss 0.5709.
Train: 2018-07-31T09:43:20.015207: step 11553, loss 0.519897.
Train: 2018-07-31T09:43:20.187042: step 11554, loss 0.553876.
Train: 2018-07-31T09:43:20.358876: step 11555, loss 0.511072.
Train: 2018-07-31T09:43:20.546358: step 11556, loss 0.605343.
Train: 2018-07-31T09:43:20.718198: step 11557, loss 0.614068.
Train: 2018-07-31T09:43:20.890003: step 11558, loss 0.5107.
Train: 2018-07-31T09:43:21.061868: step 11559, loss 0.588324.
Train: 2018-07-31T09:43:21.218081: step 11560, loss 0.571062.
Test: 2018-07-31T09:43:21.452373: step 11560, loss 0.547916.
Train: 2018-07-31T09:43:21.624206: step 11561, loss 0.553756.
Train: 2018-07-31T09:43:21.796070: step 11562, loss 0.527755.
Train: 2018-07-31T09:43:21.967876: step 11563, loss 0.571092.
Train: 2018-07-31T09:43:22.139710: step 11564, loss 0.571108.
Train: 2018-07-31T09:43:22.295924: step 11565, loss 0.553735.
Train: 2018-07-31T09:43:22.467788: step 11566, loss 0.579829.
Train: 2018-07-31T09:43:22.639622: step 11567, loss 0.562433.
Train: 2018-07-31T09:43:22.811427: step 11568, loss 0.649498.
Train: 2018-07-31T09:43:22.983262: step 11569, loss 0.614585.
Train: 2018-07-31T09:43:23.139505: step 11570, loss 0.510394.
Test: 2018-07-31T09:43:23.389417: step 11570, loss 0.547911.
Train: 2018-07-31T09:43:23.561276: step 11571, loss 0.553754.
Train: 2018-07-31T09:43:23.733087: step 11572, loss 0.545111.
Train: 2018-07-31T09:43:23.904921: step 11573, loss 0.493252.
Train: 2018-07-31T09:43:24.076755: step 11574, loss 0.571063.
Train: 2018-07-31T09:43:24.248619: step 11575, loss 0.562415.
Train: 2018-07-31T09:43:24.420456: step 11576, loss 0.571073.
Train: 2018-07-31T09:43:24.576669: step 11577, loss 0.510447.
Train: 2018-07-31T09:43:24.764127: step 11578, loss 0.57109.
Train: 2018-07-31T09:43:24.935931: step 11579, loss 0.501673.
Train: 2018-07-31T09:43:25.123417: step 11580, loss 0.605896.
Test: 2018-07-31T09:43:25.357737: step 11580, loss 0.547854.
Train: 2018-07-31T09:43:25.529540: step 11581, loss 0.562425.
Train: 2018-07-31T09:43:25.701407: step 11582, loss 0.579842.
Train: 2018-07-31T09:43:25.873210: step 11583, loss 0.55372.
Train: 2018-07-31T09:43:26.045076: step 11584, loss 0.562429.
Train: 2018-07-31T09:43:26.201259: step 11585, loss 0.597276.
Train: 2018-07-31T09:43:26.373124: step 11586, loss 0.640778.
Train: 2018-07-31T09:43:26.544959: step 11587, loss 0.536362.
Train: 2018-07-31T09:43:26.716763: step 11588, loss 0.623125.
Train: 2018-07-31T09:43:26.888628: step 11589, loss 0.536463.
Train: 2018-07-31T09:43:27.044841: step 11590, loss 0.640101.
Test: 2018-07-31T09:43:27.294786: step 11590, loss 0.547995.
Train: 2018-07-31T09:43:27.466617: step 11591, loss 0.588214.
Train: 2018-07-31T09:43:27.638452: step 11592, loss 0.570975.
Train: 2018-07-31T09:43:27.810288: step 11593, loss 0.519688.
Train: 2018-07-31T09:43:27.982092: step 11594, loss 0.511259.
Train: 2018-07-31T09:43:28.153926: step 11595, loss 0.477235.
Train: 2018-07-31T09:43:28.310140: step 11596, loss 0.587986.
Train: 2018-07-31T09:43:28.481974: step 11597, loss 0.570936.
Train: 2018-07-31T09:43:28.653840: step 11598, loss 0.553873.
Train: 2018-07-31T09:43:28.825677: step 11599, loss 0.605086.
Train: 2018-07-31T09:43:28.981857: step 11600, loss 0.579467.
Test: 2018-07-31T09:43:29.231818: step 11600, loss 0.548129.
Train: 2018-07-31T09:43:29.966003: step 11601, loss 0.536828.
Train: 2018-07-31T09:43:30.122245: step 11602, loss 0.596506.
Train: 2018-07-31T09:43:30.294081: step 11603, loss 0.579452.
Train: 2018-07-31T09:43:30.465915: step 11604, loss 0.613454.
Train: 2018-07-31T09:43:30.637755: step 11605, loss 0.579397.
Train: 2018-07-31T09:43:30.809585: step 11606, loss 0.503107.
Train: 2018-07-31T09:43:30.981419: step 11607, loss 0.570889.
Train: 2018-07-31T09:43:31.153254: step 11608, loss 0.579347.
Train: 2018-07-31T09:43:31.325089: step 11609, loss 0.528601.
Train: 2018-07-31T09:43:31.496923: step 11610, loss 0.537061.
Test: 2018-07-31T09:43:31.731215: step 11610, loss 0.548254.
Train: 2018-07-31T09:43:31.903079: step 11611, loss 0.579346.
Train: 2018-07-31T09:43:32.074882: step 11612, loss 0.570882.
Train: 2018-07-31T09:43:32.246750: step 11613, loss 0.54549.
Train: 2018-07-31T09:43:32.418553: step 11614, loss 0.570891.
Train: 2018-07-31T09:43:32.590418: step 11615, loss 0.528526.
Train: 2018-07-31T09:43:32.762252: step 11616, loss 0.536964.
Train: 2018-07-31T09:43:32.934056: step 11617, loss 0.562407.
Train: 2018-07-31T09:43:33.105922: step 11618, loss 0.562427.
Train: 2018-07-31T09:43:33.262138: step 11619, loss 0.528334.
Train: 2018-07-31T09:43:33.449562: step 11620, loss 0.536797.
Test: 2018-07-31T09:43:33.683913: step 11620, loss 0.548075.
Train: 2018-07-31T09:43:33.855746: step 11621, loss 0.588064.
Train: 2018-07-31T09:43:34.027581: step 11622, loss 0.622388.
Train: 2018-07-31T09:43:34.199416: step 11623, loss 0.570969.
Train: 2018-07-31T09:43:34.371220: step 11624, loss 0.536688.
Train: 2018-07-31T09:43:34.527434: step 11625, loss 0.562393.
Train: 2018-07-31T09:43:34.699268: step 11626, loss 0.493777.
Train: 2018-07-31T09:43:34.871134: step 11627, loss 0.562426.
Train: 2018-07-31T09:43:35.042939: step 11628, loss 0.553803.
Train: 2018-07-31T09:43:35.214773: step 11629, loss 0.519278.
Train: 2018-07-31T09:43:35.386638: step 11630, loss 0.545121.
Test: 2018-07-31T09:43:35.620953: step 11630, loss 0.547902.
Train: 2018-07-31T09:43:35.792787: step 11631, loss 0.51042.
Train: 2018-07-31T09:43:35.964627: step 11632, loss 0.571121.
Train: 2018-07-31T09:43:36.120811: step 11633, loss 0.53628.
Train: 2018-07-31T09:43:36.292646: step 11634, loss 0.544975.
Train: 2018-07-31T09:43:36.464510: step 11635, loss 0.509867.
Train: 2018-07-31T09:43:36.636314: step 11636, loss 0.588871.
Train: 2018-07-31T09:43:36.792559: step 11637, loss 0.597762.
Train: 2018-07-31T09:43:36.980017: step 11638, loss 0.650807.
Train: 2018-07-31T09:43:37.136228: step 11639, loss 0.509541.
Train: 2018-07-31T09:43:37.308058: step 11640, loss 0.553669.
Test: 2018-07-31T09:43:37.558005: step 11640, loss 0.54771.
Train: 2018-07-31T09:43:37.714188: step 11641, loss 0.527174.
Train: 2018-07-31T09:43:37.886052: step 11642, loss 0.633222.
Train: 2018-07-31T09:43:38.057857: step 11643, loss 0.518337.
Train: 2018-07-31T09:43:38.229722: step 11644, loss 0.527171.
Train: 2018-07-31T09:43:38.401557: step 11645, loss 0.535995.
Train: 2018-07-31T09:43:38.573361: step 11646, loss 0.54482.
Train: 2018-07-31T09:43:38.745196: step 11647, loss 0.527093.
Train: 2018-07-31T09:43:38.917030: step 11648, loss 0.491571.
Train: 2018-07-31T09:43:39.088865: step 11649, loss 0.580339.
Train: 2018-07-31T09:43:39.245105: step 11650, loss 0.589298.
Test: 2018-07-31T09:43:39.479401: step 11650, loss 0.547643.
Train: 2018-07-31T09:43:39.651234: step 11651, loss 0.55366.
Train: 2018-07-31T09:43:39.823068: step 11652, loss 0.642912.
Train: 2018-07-31T09:43:39.994904: step 11653, loss 0.544744.
Train: 2018-07-31T09:43:40.166768: step 11654, loss 0.553659.
Train: 2018-07-31T09:43:40.322952: step 11655, loss 0.544757.
Train: 2018-07-31T09:43:40.494818: step 11656, loss 0.55366.
Train: 2018-07-31T09:43:40.666651: step 11657, loss 0.580352.
Train: 2018-07-31T09:43:40.838455: step 11658, loss 0.589217.
Train: 2018-07-31T09:43:41.010290: step 11659, loss 0.580291.
Train: 2018-07-31T09:43:41.182157: step 11660, loss 0.518221.
Test: 2018-07-31T09:43:41.416446: step 11660, loss 0.547692.
Train: 2018-07-31T09:43:41.588280: step 11661, loss 0.624481.
Train: 2018-07-31T09:43:41.760115: step 11662, loss 0.571333.
Train: 2018-07-31T09:43:41.931981: step 11663, loss 0.606548.
Train: 2018-07-31T09:43:42.103814: step 11664, loss 0.597609.
Train: 2018-07-31T09:43:42.275620: step 11665, loss 0.544943.
Train: 2018-07-31T09:43:42.447485: step 11666, loss 0.54498.
Train: 2018-07-31T09:43:42.603699: step 11667, loss 0.571135.
Train: 2018-07-31T09:43:42.775532: step 11668, loss 0.562419.
Train: 2018-07-31T09:43:42.947338: step 11669, loss 0.58841.
Train: 2018-07-31T09:43:43.119171: step 11670, loss 0.596976.
Test: 2018-07-31T09:43:43.353523: step 11670, loss 0.547975.
Train: 2018-07-31T09:43:43.525357: step 11671, loss 0.571019.
Train: 2018-07-31T09:43:43.697161: step 11672, loss 0.588169.
Train: 2018-07-31T09:43:43.868996: step 11673, loss 0.519597.
Train: 2018-07-31T09:43:44.040832: step 11674, loss 0.596583.
Train: 2018-07-31T09:43:44.197044: step 11675, loss 0.570937.
Train: 2018-07-31T09:43:44.368880: step 11676, loss 0.536903.
Train: 2018-07-31T09:43:44.540745: step 11677, loss 0.562416.
Train: 2018-07-31T09:43:44.712579: step 11678, loss 0.562423.
Train: 2018-07-31T09:43:44.884413: step 11679, loss 0.53703.
Train: 2018-07-31T09:43:45.056250: step 11680, loss 0.553958.
Test: 2018-07-31T09:43:45.290540: step 11680, loss 0.548256.
Train: 2018-07-31T09:43:45.462374: step 11681, loss 0.570881.
Train: 2018-07-31T09:43:45.634238: step 11682, loss 0.587794.
Train: 2018-07-31T09:43:45.806074: step 11683, loss 0.570882.
Train: 2018-07-31T09:43:45.977907: step 11684, loss 0.528652.
Train: 2018-07-31T09:43:46.134121: step 11685, loss 0.570878.
Train: 2018-07-31T09:43:46.305956: step 11686, loss 0.545536.
Train: 2018-07-31T09:43:46.477760: step 11687, loss 0.520182.
Train: 2018-07-31T09:43:46.649625: step 11688, loss 0.579345.
Train: 2018-07-31T09:43:46.821455: step 11689, loss 0.520075.
Train: 2018-07-31T09:43:46.993295: step 11690, loss 0.53696.
Test: 2018-07-31T09:43:47.227585: step 11690, loss 0.548168.
Train: 2018-07-31T09:43:47.399419: step 11691, loss 0.48587.
Train: 2018-07-31T09:43:47.571285: step 11692, loss 0.553872.
Train: 2018-07-31T09:43:47.743090: step 11693, loss 0.5624.
Train: 2018-07-31T09:43:47.914954: step 11694, loss 0.588186.
Train: 2018-07-31T09:43:48.086789: step 11695, loss 0.571018.
Train: 2018-07-31T09:43:48.258594: step 11696, loss 0.605553.
Train: 2018-07-31T09:43:48.430428: step 11697, loss 0.622862.
Train: 2018-07-31T09:43:48.602293: step 11698, loss 0.553773.
Train: 2018-07-31T09:43:48.758477: step 11699, loss 0.571031.
Train: 2018-07-31T09:43:48.930310: step 11700, loss 0.571024.
Test: 2018-07-31T09:43:49.180278: step 11700, loss 0.547975.
Train: 2018-07-31T09:43:49.914486: step 11701, loss 0.58825.
Train: 2018-07-31T09:43:50.086291: step 11702, loss 0.588219.
Train: 2018-07-31T09:43:50.258156: step 11703, loss 0.579585.
Train: 2018-07-31T09:43:50.429991: step 11704, loss 0.519524.
Train: 2018-07-31T09:43:50.601794: step 11705, loss 0.545265.
Train: 2018-07-31T09:43:50.773629: step 11706, loss 0.65662.
Train: 2018-07-31T09:43:50.945495: step 11707, loss 0.511131.
Train: 2018-07-31T09:43:51.117330: step 11708, loss 0.511181.
Train: 2018-07-31T09:43:51.289164: step 11709, loss 0.579481.
Train: 2018-07-31T09:43:51.460968: step 11710, loss 0.545329.
Test: 2018-07-31T09:43:51.695290: step 11710, loss 0.5481.
Train: 2018-07-31T09:43:51.867153: step 11711, loss 0.562399.
Train: 2018-07-31T09:43:52.038958: step 11712, loss 0.562407.
Train: 2018-07-31T09:43:52.210792: step 11713, loss 0.528217.
Train: 2018-07-31T09:43:52.367007: step 11714, loss 0.502521.
Train: 2018-07-31T09:43:52.538842: step 11715, loss 0.519534.
Train: 2018-07-31T09:43:52.710675: step 11716, loss 0.545204.
Train: 2018-07-31T09:43:52.882541: step 11717, loss 0.596902.
Train: 2018-07-31T09:43:53.054370: step 11718, loss 0.519208.
Train: 2018-07-31T09:43:53.226210: step 11719, loss 0.54509.
Train: 2018-07-31T09:43:53.398045: step 11720, loss 0.614531.
Test: 2018-07-31T09:43:53.632365: step 11720, loss 0.547861.
Train: 2018-07-31T09:43:53.804196: step 11721, loss 0.605895.
Train: 2018-07-31T09:43:53.976006: step 11722, loss 0.553726.
Train: 2018-07-31T09:43:54.147870: step 11723, loss 0.527633.
Train: 2018-07-31T09:43:54.319674: step 11724, loss 0.588544.
Train: 2018-07-31T09:43:54.491539: step 11725, loss 0.597266.
Train: 2018-07-31T09:43:54.663374: step 11726, loss 0.597226.
Train: 2018-07-31T09:43:54.819588: step 11727, loss 0.597178.
Train: 2018-07-31T09:43:54.991392: step 11728, loss 0.553745.
Train: 2018-07-31T09:43:55.163256: step 11729, loss 0.614356.
Train: 2018-07-31T09:43:55.350710: step 11730, loss 0.579674.
Test: 2018-07-31T09:43:55.585034: step 11730, loss 0.547984.
Train: 2018-07-31T09:43:55.756837: step 11731, loss 0.562408.
Train: 2018-07-31T09:43:55.928672: step 11732, loss 0.536632.
Train: 2018-07-31T09:43:56.100537: step 11733, loss 0.579546.
Train: 2018-07-31T09:43:56.272341: step 11734, loss 0.519625.
Train: 2018-07-31T09:43:56.428555: step 11735, loss 0.511103.
Train: 2018-07-31T09:43:56.600420: step 11736, loss 0.596617.
Train: 2018-07-31T09:43:56.772225: step 11737, loss 0.57095.
Train: 2018-07-31T09:43:56.944059: step 11738, loss 0.545318.
Train: 2018-07-31T09:43:57.100273: step 11739, loss 0.570951.
Train: 2018-07-31T09:43:57.272137: step 11740, loss 0.570945.
Test: 2018-07-31T09:43:57.506428: step 11740, loss 0.548102.
Train: 2018-07-31T09:43:57.678262: step 11741, loss 0.588024.
Train: 2018-07-31T09:43:57.850097: step 11742, loss 0.48562.
Train: 2018-07-31T09:43:58.006340: step 11743, loss 0.459928.
Train: 2018-07-31T09:43:58.178146: step 11744, loss 0.536703.
Train: 2018-07-31T09:43:58.349980: step 11745, loss 0.588184.
Train: 2018-07-31T09:43:58.521845: step 11746, loss 0.588255.
Train: 2018-07-31T09:43:58.693649: step 11747, loss 0.536522.
Train: 2018-07-31T09:43:58.849897: step 11748, loss 0.61428.
Train: 2018-07-31T09:43:59.021699: step 11749, loss 0.519157.
Train: 2018-07-31T09:43:59.193563: step 11750, loss 0.458459.
Test: 2018-07-31T09:43:59.427853: step 11750, loss 0.547864.
Train: 2018-07-31T09:43:59.599712: step 11751, loss 0.562423.
Train: 2018-07-31T09:43:59.787144: step 11752, loss 0.597304.
Train: 2018-07-31T09:43:59.943388: step 11753, loss 0.562439.
Train: 2018-07-31T09:44:00.115222: step 11754, loss 0.623687.
Train: 2018-07-31T09:44:00.287057: step 11755, loss 0.544948.
Train: 2018-07-31T09:44:00.458862: step 11756, loss 0.571202.
Train: 2018-07-31T09:44:00.630697: step 11757, loss 0.536189.
Train: 2018-07-31T09:44:00.802562: step 11758, loss 0.536179.
Train: 2018-07-31T09:44:00.958746: step 11759, loss 0.562455.
Train: 2018-07-31T09:44:01.130609: step 11760, loss 0.492292.
Test: 2018-07-31T09:44:01.364930: step 11760, loss 0.547751.
Train: 2018-07-31T09:44:01.552356: step 11761, loss 0.59762.
Train: 2018-07-31T09:44:01.724215: step 11762, loss 0.553679.
Train: 2018-07-31T09:44:01.896055: step 11763, loss 0.544872.
Train: 2018-07-31T09:44:02.067860: step 11764, loss 0.624174.
Train: 2018-07-31T09:44:02.239719: step 11765, loss 0.5801.
Train: 2018-07-31T09:44:02.411560: step 11766, loss 0.518473.
Train: 2018-07-31T09:44:02.583364: step 11767, loss 0.562477.
Train: 2018-07-31T09:44:02.739607: step 11768, loss 0.544878.
Train: 2018-07-31T09:44:02.911445: step 11769, loss 0.571277.
Train: 2018-07-31T09:44:03.098899: step 11770, loss 0.544879.
Test: 2018-07-31T09:44:03.333219: step 11770, loss 0.54774.
Train: 2018-07-31T09:44:03.505023: step 11771, loss 0.641665.
Train: 2018-07-31T09:44:03.676857: step 11772, loss 0.641522.
Train: 2018-07-31T09:44:03.833071: step 11773, loss 0.632495.
Train: 2018-07-31T09:44:04.004906: step 11774, loss 0.466548.
Train: 2018-07-31T09:44:04.176740: step 11775, loss 0.545027.
Train: 2018-07-31T09:44:04.364196: step 11776, loss 0.60584.
Train: 2018-07-31T09:44:04.520411: step 11777, loss 0.571076.
Train: 2018-07-31T09:44:04.676624: step 11778, loss 0.673029.
Train: 2018-07-31T09:44:04.848458: step 11779, loss 0.622626.
Train: 2018-07-31T09:44:05.020292: step 11780, loss 0.562403.
Test: 2018-07-31T09:44:05.254645: step 11780, loss 0.548146.
Train: 2018-07-31T09:44:05.426479: step 11781, loss 0.570924.
Train: 2018-07-31T09:44:05.598313: step 11782, loss 0.579371.
Train: 2018-07-31T09:44:05.770118: step 11783, loss 0.537113.
Train: 2018-07-31T09:44:05.926355: step 11784, loss 0.587679.
Train: 2018-07-31T09:44:06.098167: step 11785, loss 0.537314.
Train: 2018-07-31T09:44:06.270001: step 11786, loss 0.512306.
Train: 2018-07-31T09:44:06.441866: step 11787, loss 0.504008.
Train: 2018-07-31T09:44:06.613670: step 11788, loss 0.51233.
Train: 2018-07-31T09:44:06.769914: step 11789, loss 0.512234.
Train: 2018-07-31T09:44:06.941718: step 11790, loss 0.612827.
Test: 2018-07-31T09:44:07.191679: step 11790, loss 0.548364.
Train: 2018-07-31T09:44:07.363524: step 11791, loss 0.562446.
Train: 2018-07-31T09:44:07.535359: step 11792, loss 0.587703.
Train: 2018-07-31T09:44:07.707163: step 11793, loss 0.596147.
Train: 2018-07-31T09:44:07.879029: step 11794, loss 0.655149.
Train: 2018-07-31T09:44:08.050858: step 11795, loss 0.528793.
Train: 2018-07-31T09:44:08.207047: step 11796, loss 0.570853.
Train: 2018-07-31T09:44:08.394503: step 11797, loss 0.646437.
Train: 2018-07-31T09:44:08.550748: step 11798, loss 0.579219.
Train: 2018-07-31T09:44:08.722552: step 11799, loss 0.554122.
Train: 2018-07-31T09:44:08.894410: step 11800, loss 0.52913.
Test: 2018-07-31T09:44:09.144327: step 11800, loss 0.54854.
Train: 2018-07-31T09:44:09.878561: step 11801, loss 0.529164.
Train: 2018-07-31T09:44:10.050396: step 11802, loss 0.595828.
Train: 2018-07-31T09:44:10.222224: step 11803, loss 0.520853.
Train: 2018-07-31T09:44:10.394060: step 11804, loss 0.654169.
Train: 2018-07-31T09:44:10.565869: step 11805, loss 0.61244.
Train: 2018-07-31T09:44:10.737705: step 11806, loss 0.504388.
Train: 2018-07-31T09:44:10.925161: step 11807, loss 0.529322.
Train: 2018-07-31T09:44:11.081404: step 11808, loss 0.579126.
Train: 2018-07-31T09:44:11.253208: step 11809, loss 0.587433.
Train: 2018-07-31T09:44:11.425074: step 11810, loss 0.637249.
Test: 2018-07-31T09:44:11.674997: step 11810, loss 0.548649.
Train: 2018-07-31T09:44:11.846849: step 11811, loss 0.603974.
Train: 2018-07-31T09:44:12.003064: step 11812, loss 0.579088.
Train: 2018-07-31T09:44:12.174867: step 11813, loss 0.504817.
Train: 2018-07-31T09:44:12.346735: step 11814, loss 0.562575.
Train: 2018-07-31T09:44:12.518538: step 11815, loss 0.554332.
Train: 2018-07-31T09:44:12.690402: step 11816, loss 0.59555.
Train: 2018-07-31T09:44:12.846586: step 11817, loss 0.59554.
Train: 2018-07-31T09:44:13.018446: step 11818, loss 0.554354.
Train: 2018-07-31T09:44:13.190286: step 11819, loss 0.611959.
Train: 2018-07-31T09:44:13.362120: step 11820, loss 0.529737.
Test: 2018-07-31T09:44:13.596410: step 11820, loss 0.548849.
Train: 2018-07-31T09:44:13.783890: step 11821, loss 0.49689.
Train: 2018-07-31T09:44:13.955701: step 11822, loss 0.496779.
Train: 2018-07-31T09:44:14.111914: step 11823, loss 0.54606.
Train: 2018-07-31T09:44:14.283779: step 11824, loss 0.57082.
Train: 2018-07-31T09:44:14.455584: step 11825, loss 0.54591.
Train: 2018-07-31T09:44:14.627449: step 11826, loss 0.512514.
Train: 2018-07-31T09:44:14.799284: step 11827, loss 0.562472.
Train: 2018-07-31T09:44:14.971113: step 11828, loss 0.470084.
Train: 2018-07-31T09:44:15.142923: step 11829, loss 0.537099.
Train: 2018-07-31T09:44:15.314758: step 11830, loss 0.570904.
Test: 2018-07-31T09:44:15.549104: step 11830, loss 0.548114.
Train: 2018-07-31T09:44:15.720942: step 11831, loss 0.622134.
Train: 2018-07-31T09:44:15.892746: step 11832, loss 0.59664.
Train: 2018-07-31T09:44:16.064583: step 11833, loss 0.53667.
Train: 2018-07-31T09:44:16.236417: step 11834, loss 0.570999.
Train: 2018-07-31T09:44:16.392631: step 11835, loss 0.588249.
Train: 2018-07-31T09:44:16.564465: step 11836, loss 0.527903.
Train: 2018-07-31T09:44:16.736330: step 11837, loss 0.571048.
Train: 2018-07-31T09:44:16.892543: step 11838, loss 0.449917.
Train: 2018-07-31T09:44:17.064348: step 11839, loss 0.518999.
Train: 2018-07-31T09:44:17.236213: step 11840, loss 0.57115.
Test: 2018-07-31T09:44:17.470503: step 11840, loss 0.547795.
Train: 2018-07-31T09:44:17.642337: step 11841, loss 0.518706.
Train: 2018-07-31T09:44:17.829826: step 11842, loss 0.588808.
Train: 2018-07-31T09:44:17.986007: step 11843, loss 0.597703.
Train: 2018-07-31T09:44:18.157842: step 11844, loss 0.52721.
Train: 2018-07-31T09:44:18.329701: step 11845, loss 0.527152.
Train: 2018-07-31T09:44:18.501511: step 11846, loss 0.509364.
Train: 2018-07-31T09:44:18.673377: step 11847, loss 0.544773.
Train: 2018-07-31T09:44:18.845211: step 11848, loss 0.544746.
Train: 2018-07-31T09:44:19.017045: step 11849, loss 0.571537.
Train: 2018-07-31T09:44:19.188850: step 11850, loss 0.589493.
Test: 2018-07-31T09:44:19.438792: step 11850, loss 0.547613.
Train: 2018-07-31T09:44:19.595005: step 11851, loss 0.481917.
Train: 2018-07-31T09:44:19.782461: step 11852, loss 0.598622.
Train: 2018-07-31T09:44:19.954296: step 11853, loss 0.562675.
Train: 2018-07-31T09:44:20.126156: step 11854, loss 0.580707.
Train: 2018-07-31T09:44:20.313586: step 11855, loss 0.607755.
Train: 2018-07-31T09:44:20.485422: step 11856, loss 0.499641.
Train: 2018-07-31T09:44:20.672877: step 11857, loss 0.571689.
Train: 2018-07-31T09:44:20.844714: step 11858, loss 0.571685.
Train: 2018-07-31T09:44:21.032169: step 11859, loss 0.51767.
Train: 2018-07-31T09:44:21.188413: step 11860, loss 0.517666.
Test: 2018-07-31T09:44:21.438325: step 11860, loss 0.547596.
Train: 2018-07-31T09:44:21.610158: step 11861, loss 0.463589.
Train: 2018-07-31T09:44:21.781993: step 11862, loss 0.553682.
Train: 2018-07-31T09:44:21.953827: step 11863, loss 0.553689.
Train: 2018-07-31T09:44:22.110072: step 11864, loss 0.626211.
Train: 2018-07-31T09:44:22.281914: step 11865, loss 0.562762.
Train: 2018-07-31T09:44:22.453741: step 11866, loss 0.508374.
Train: 2018-07-31T09:44:22.625576: step 11867, loss 0.517417.
Train: 2018-07-31T09:44:22.797381: step 11868, loss 0.580949.
Train: 2018-07-31T09:44:22.984867: step 11869, loss 0.580964.
Train: 2018-07-31T09:44:23.203536: step 11870, loss 0.56279.
Test: 2018-07-31T09:44:23.437887: step 11870, loss 0.547577.
Train: 2018-07-31T09:44:23.609689: step 11871, loss 0.635424.
Train: 2018-07-31T09:44:23.781556: step 11872, loss 0.508386.
Train: 2018-07-31T09:44:23.953360: step 11873, loss 0.5899.
Train: 2018-07-31T09:44:24.125225: step 11874, loss 0.544647.
Train: 2018-07-31T09:44:24.297060: step 11875, loss 0.526608.
Train: 2018-07-31T09:44:24.468894: step 11876, loss 0.508594.
Train: 2018-07-31T09:44:24.640732: step 11877, loss 0.60778.
Train: 2018-07-31T09:44:24.812533: step 11878, loss 0.670774.
Train: 2018-07-31T09:44:24.984398: step 11879, loss 0.580602.
Train: 2018-07-31T09:44:25.156233: step 11880, loss 0.571556.
Test: 2018-07-31T09:44:25.406145: step 11880, loss 0.547643.
Train: 2018-07-31T09:44:25.578010: step 11881, loss 0.607155.
Train: 2018-07-31T09:44:25.749845: step 11882, loss 0.544782.
Train: 2018-07-31T09:44:25.906061: step 11883, loss 0.571357.
Train: 2018-07-31T09:44:26.077861: step 11884, loss 0.588929.
Train: 2018-07-31T09:44:26.265348: step 11885, loss 0.615142.
Train: 2018-07-31T09:44:26.437154: step 11886, loss 0.518748.
Train: 2018-07-31T09:44:26.608987: step 11887, loss 0.588552.
Train: 2018-07-31T09:44:26.796443: step 11888, loss 0.588444.
Train: 2018-07-31T09:44:26.968278: step 11889, loss 0.545124.
Train: 2018-07-31T09:44:27.124522: step 11890, loss 0.545176.
Test: 2018-07-31T09:44:27.374464: step 11890, loss 0.548013.
Train: 2018-07-31T09:44:27.546268: step 11891, loss 0.596766.
Train: 2018-07-31T09:44:27.718104: step 11892, loss 0.48532.
Train: 2018-07-31T09:44:27.874316: step 11893, loss 0.519626.
Train: 2018-07-31T09:44:28.046182: step 11894, loss 0.647944.
Train: 2018-07-31T09:44:28.217986: step 11895, loss 0.562403.
Train: 2018-07-31T09:44:28.389821: step 11896, loss 0.562406.
Train: 2018-07-31T09:44:28.561689: step 11897, loss 0.553897.
Train: 2018-07-31T09:44:28.733491: step 11898, loss 0.5029.
Train: 2018-07-31T09:44:28.905324: step 11899, loss 0.638947.
Train: 2018-07-31T09:44:29.061569: step 11900, loss 0.511454.
Test: 2018-07-31T09:44:29.311480: step 11900, loss 0.548189.
Train: 2018-07-31T09:44:30.076926: step 11901, loss 0.570906.
Train: 2018-07-31T09:44:30.248786: step 11902, loss 0.519957.
Train: 2018-07-31T09:44:30.420596: step 11903, loss 0.596403.
Train: 2018-07-31T09:44:30.592460: step 11904, loss 0.536915.
Train: 2018-07-31T09:44:30.779885: step 11905, loss 0.579417.
Train: 2018-07-31T09:44:30.951751: step 11906, loss 0.579421.
Train: 2018-07-31T09:44:31.123586: step 11907, loss 0.528389.
Train: 2018-07-31T09:44:31.295421: step 11908, loss 0.630495.
Train: 2018-07-31T09:44:31.467226: step 11909, loss 0.528393.
Train: 2018-07-31T09:44:31.639090: step 11910, loss 0.621939.
Test: 2018-07-31T09:44:31.873411: step 11910, loss 0.548186.
Train: 2018-07-31T09:44:32.045246: step 11911, loss 0.502954.
Train: 2018-07-31T09:44:32.217050: step 11912, loss 0.528427.
Train: 2018-07-31T09:44:32.388885: step 11913, loss 0.587923.
Train: 2018-07-31T09:44:32.576340: step 11914, loss 0.579425.
Train: 2018-07-31T09:44:32.732553: step 11915, loss 0.613458.
Train: 2018-07-31T09:44:32.904413: step 11916, loss 0.519915.
Train: 2018-07-31T09:44:33.076257: step 11917, loss 0.596408.
Train: 2018-07-31T09:44:33.248059: step 11918, loss 0.528436.
Train: 2018-07-31T09:44:33.419924: step 11919, loss 0.536924.
Train: 2018-07-31T09:44:33.576106: step 11920, loss 0.570912.
Test: 2018-07-31T09:44:33.826079: step 11920, loss 0.548162.
Train: 2018-07-31T09:44:33.982260: step 11921, loss 0.579422.
Train: 2018-07-31T09:44:34.154127: step 11922, loss 0.596441.
Train: 2018-07-31T09:44:34.325960: step 11923, loss 0.545403.
Train: 2018-07-31T09:44:34.497796: step 11924, loss 0.621928.
Train: 2018-07-31T09:44:34.669631: step 11925, loss 0.562412.
Train: 2018-07-31T09:44:34.841437: step 11926, loss 0.553934.
Train: 2018-07-31T09:44:35.013270: step 11927, loss 0.638693.
Train: 2018-07-31T09:44:35.185135: step 11928, loss 0.49478.
Train: 2018-07-31T09:44:35.341318: step 11929, loss 0.598483.
Train: 2018-07-31T09:44:35.513184: step 11930, loss 0.604635.
Test: 2018-07-31T09:44:35.747473: step 11930, loss 0.548328.
Train: 2018-07-31T09:44:35.934929: step 11931, loss 0.562437.
Train: 2018-07-31T09:44:36.091142: step 11932, loss 0.511979.
Train: 2018-07-31T09:44:36.278625: step 11933, loss 0.59608.
Train: 2018-07-31T09:44:36.450463: step 11934, loss 0.596053.
Train: 2018-07-31T09:44:36.606676: step 11935, loss 0.604401.
Train: 2018-07-31T09:44:36.794133: step 11936, loss 0.554097.
Train: 2018-07-31T09:44:36.965968: step 11937, loss 0.54576.
Train: 2018-07-31T09:44:37.122181: step 11938, loss 0.63763.
Train: 2018-07-31T09:44:37.294016: step 11939, loss 0.579155.
Train: 2018-07-31T09:44:37.465821: step 11940, loss 0.587442.
Test: 2018-07-31T09:44:37.700142: step 11940, loss 0.548651.
Train: 2018-07-31T09:44:37.934492: step 11941, loss 0.562529.
Train: 2018-07-31T09:44:38.106325: step 11942, loss 0.529468.
Train: 2018-07-31T09:44:38.278129: step 11943, loss 0.479937.
Train: 2018-07-31T09:44:38.449964: step 11944, loss 0.587361.
Train: 2018-07-31T09:44:38.621799: step 11945, loss 0.570817.
Train: 2018-07-31T09:44:38.778012: step 11946, loss 0.545973.
Train: 2018-07-31T09:44:38.949878: step 11947, loss 0.537656.
Train: 2018-07-31T09:44:39.121683: step 11948, loss 0.545907.
Train: 2018-07-31T09:44:39.309169: step 11949, loss 0.562501.
Train: 2018-07-31T09:44:39.481004: step 11950, loss 0.520796.
Test: 2018-07-31T09:44:39.715324: step 11950, loss 0.548467.
Train: 2018-07-31T09:44:39.887128: step 11951, loss 0.545745.
Train: 2018-07-31T09:44:40.058962: step 11952, loss 0.48696.
Train: 2018-07-31T09:44:40.230797: step 11953, loss 0.596142.
Train: 2018-07-31T09:44:40.402657: step 11954, loss 0.621612.
Train: 2018-07-31T09:44:40.558847: step 11955, loss 0.596303.
Train: 2018-07-31T09:44:40.730711: step 11956, loss 0.536978.
Train: 2018-07-31T09:44:40.902545: step 11957, loss 0.536937.
Train: 2018-07-31T09:44:41.074382: step 11958, loss 0.502847.
Train: 2018-07-31T09:44:41.246219: step 11959, loss 0.502662.
Train: 2018-07-31T09:44:41.418056: step 11960, loss 0.536693.
Test: 2018-07-31T09:44:41.652373: step 11960, loss 0.54799.
Train: 2018-07-31T09:44:41.824199: step 11961, loss 0.605428.
Train: 2018-07-31T09:44:41.996040: step 11962, loss 0.510626.
Train: 2018-07-31T09:44:42.167845: step 11963, loss 0.553751.
Train: 2018-07-31T09:44:42.339704: step 11964, loss 0.56242.
Train: 2018-07-31T09:44:42.495922: step 11965, loss 0.606007.
Train: 2018-07-31T09:44:42.667758: step 11966, loss 0.475135.
Train: 2018-07-31T09:44:42.839592: step 11967, loss 0.606241.
Train: 2018-07-31T09:44:43.011397: step 11968, loss 0.544911.
Train: 2018-07-31T09:44:43.183255: step 11969, loss 0.536095.
Train: 2018-07-31T09:44:43.355096: step 11970, loss 0.641782.
Test: 2018-07-31T09:44:43.589387: step 11970, loss 0.547725.
Train: 2018-07-31T09:44:43.761251: step 11971, loss 0.544859.
Train: 2018-07-31T09:44:43.933057: step 11972, loss 0.588938.
Train: 2018-07-31T09:44:44.104891: step 11973, loss 0.553671.
Train: 2018-07-31T09:44:44.276725: step 11974, loss 0.597735.
Train: 2018-07-31T09:44:44.448561: step 11975, loss 0.54487.
Train: 2018-07-31T09:44:44.620425: step 11976, loss 0.58007.
Train: 2018-07-31T09:44:44.776608: step 11977, loss 0.615197.
Train: 2018-07-31T09:44:44.948473: step 11978, loss 0.553688.
Train: 2018-07-31T09:44:45.135930: step 11979, loss 0.5712.
Train: 2018-07-31T09:44:45.292113: step 11980, loss 0.553703.
Test: 2018-07-31T09:44:45.542079: step 11980, loss 0.547826.
Train: 2018-07-31T09:44:45.713919: step 11981, loss 0.54499.
Train: 2018-07-31T09:44:45.885724: step 11982, loss 0.527586.
Train: 2018-07-31T09:44:46.057589: step 11983, loss 0.55372.
Train: 2018-07-31T09:44:46.229423: step 11984, loss 0.545019.
Train: 2018-07-31T09:44:46.401227: step 11985, loss 0.553721.
Train: 2018-07-31T09:44:46.573062: step 11986, loss 0.614654.
Train: 2018-07-31T09:44:46.760518: step 11987, loss 0.527641.
Train: 2018-07-31T09:44:46.932354: step 11988, loss 0.588499.
Train: 2018-07-31T09:44:47.104188: step 11989, loss 0.562418.
Train: 2018-07-31T09:44:47.276053: step 11990, loss 0.519024.
Test: 2018-07-31T09:44:47.510380: step 11990, loss 0.54788.
Train: 2018-07-31T09:44:47.682202: step 11991, loss 0.597134.
Train: 2018-07-31T09:44:47.854013: step 11992, loss 0.510375.
Train: 2018-07-31T09:44:48.025847: step 11993, loss 0.597123.
Train: 2018-07-31T09:44:48.197683: step 11994, loss 0.536394.
Train: 2018-07-31T09:44:48.353926: step 11995, loss 0.623136.
Train: 2018-07-31T09:44:48.525730: step 11996, loss 0.57974.
Train: 2018-07-31T09:44:48.697596: step 11997, loss 0.597013.
Train: 2018-07-31T09:44:48.869430: step 11998, loss 0.51061.
Train: 2018-07-31T09:44:49.041266: step 11999, loss 0.596905.
Train: 2018-07-31T09:44:49.213071: step 12000, loss 0.59685.
Test: 2018-07-31T09:44:49.447390: step 12000, loss 0.548008.
Train: 2018-07-31T09:44:50.166002: step 12001, loss 0.562401.
Train: 2018-07-31T09:44:50.337830: step 12002, loss 0.519505.
Train: 2018-07-31T09:44:50.509671: step 12003, loss 0.545254.
Train: 2018-07-31T09:44:50.681506: step 12004, loss 0.55383.
Train: 2018-07-31T09:44:50.853310: step 12005, loss 0.48528.
Train: 2018-07-31T09:44:51.009523: step 12006, loss 0.613883.
Train: 2018-07-31T09:44:51.181389: step 12007, loss 0.553817.
Train: 2018-07-31T09:44:51.353224: step 12008, loss 0.519468.
Train: 2018-07-31T09:44:51.525061: step 12009, loss 0.570996.
Train: 2018-07-31T09:44:51.696894: step 12010, loss 0.648437.
Test: 2018-07-31T09:44:51.931183: step 12010, loss 0.548003.
Train: 2018-07-31T09:44:52.103042: step 12011, loss 0.570996.
Train: 2018-07-31T09:44:52.274882: step 12012, loss 0.536636.
Train: 2018-07-31T09:44:52.446686: step 12013, loss 0.553814.
Train: 2018-07-31T09:44:52.602931: step 12014, loss 0.553815.
Train: 2018-07-31T09:44:52.774735: step 12015, loss 0.502316.
Train: 2018-07-31T09:44:52.946569: step 12016, loss 0.622552.
Train: 2018-07-31T09:44:53.102784: step 12017, loss 0.596767.
Train: 2018-07-31T09:44:53.274618: step 12018, loss 0.485131.
Train: 2018-07-31T09:44:53.446483: step 12019, loss 0.553807.
Train: 2018-07-31T09:44:53.602667: step 12020, loss 0.545201.
Test: 2018-07-31T09:44:53.852639: step 12020, loss 0.547982.
Train: 2018-07-31T09:44:54.024442: step 12021, loss 0.596837.
Train: 2018-07-31T09:44:54.196307: step 12022, loss 0.527955.
Train: 2018-07-31T09:44:54.352491: step 12023, loss 0.536543.
Train: 2018-07-31T09:44:54.524356: step 12024, loss 0.562403.
Train: 2018-07-31T09:44:54.696160: step 12025, loss 0.588326.
Train: 2018-07-31T09:44:54.868019: step 12026, loss 0.519181.
Train: 2018-07-31T09:44:55.039830: step 12027, loss 0.562408.
Train: 2018-07-31T09:44:55.196077: step 12028, loss 0.631729.
Train: 2018-07-31T09:44:55.367909: step 12029, loss 0.527763.
Train: 2018-07-31T09:44:55.539712: step 12030, loss 0.484435.
Test: 2018-07-31T09:44:55.774033: step 12030, loss 0.54788.
Train: 2018-07-31T09:44:55.945892: step 12031, loss 0.562414.
Train: 2018-07-31T09:44:56.117733: step 12032, loss 0.536346.
Train: 2018-07-31T09:44:56.273946: step 12033, loss 0.579836.
Train: 2018-07-31T09:44:56.445781: step 12034, loss 0.562428.
Train: 2018-07-31T09:44:56.617616: step 12035, loss 0.658402.
Train: 2018-07-31T09:44:56.773828: step 12036, loss 0.53628.
Train: 2018-07-31T09:44:56.945634: step 12037, loss 0.53629.
Train: 2018-07-31T09:44:57.101847: step 12038, loss 0.588563.
Train: 2018-07-31T09:44:57.273682: step 12039, loss 0.562424.
Train: 2018-07-31T09:44:57.445516: step 12040, loss 0.605935.
Test: 2018-07-31T09:44:57.679837: step 12040, loss 0.547863.
Train: 2018-07-31T09:44:57.851697: step 12041, loss 0.640633.
Train: 2018-07-31T09:44:58.023536: step 12042, loss 0.536411.
Train: 2018-07-31T09:44:58.179751: step 12043, loss 0.519162.
Train: 2018-07-31T09:44:58.351555: step 12044, loss 0.536485.
Train: 2018-07-31T09:44:58.523390: step 12045, loss 0.605581.
Train: 2018-07-31T09:44:58.679602: step 12046, loss 0.502032.
Train: 2018-07-31T09:44:58.835846: step 12047, loss 0.53653.
Train: 2018-07-31T09:44:59.007650: step 12048, loss 0.579658.
Train: 2018-07-31T09:44:59.163888: step 12049, loss 0.605542.
Train: 2018-07-31T09:44:59.335700: step 12050, loss 0.54516.
Test: 2018-07-31T09:44:59.570045: step 12050, loss 0.547969.
Train: 2018-07-31T09:44:59.726262: step 12051, loss 0.519315.
Train: 2018-07-31T09:44:59.898068: step 12052, loss 0.65723.
Train: 2018-07-31T09:45:00.069932: step 12053, loss 0.553792.
Train: 2018-07-31T09:45:00.226115: step 12054, loss 0.579596.
Train: 2018-07-31T09:45:00.397980: step 12055, loss 0.545226.
Train: 2018-07-31T09:45:00.554164: step 12056, loss 0.562399.
Train: 2018-07-31T09:45:00.725998: step 12057, loss 0.51097.
Train: 2018-07-31T09:45:00.897833: step 12058, loss 0.502384.
Train: 2018-07-31T09:45:01.054077: step 12059, loss 0.510886.
Train: 2018-07-31T09:45:01.225907: step 12060, loss 0.553795.
Test: 2018-07-31T09:45:01.460204: step 12060, loss 0.54796.
Train: 2018-07-31T09:45:01.632036: step 12061, loss 0.571025.
Train: 2018-07-31T09:45:01.788250: step 12062, loss 0.62287.
Train: 2018-07-31T09:45:01.960109: step 12063, loss 0.605612.
Train: 2018-07-31T09:45:02.116328: step 12064, loss 0.501945.
Train: 2018-07-31T09:45:02.288157: step 12065, loss 0.536477.
Train: 2018-07-31T09:45:02.444382: step 12066, loss 0.553755.
Train: 2018-07-31T09:45:02.616211: step 12067, loss 0.579732.
Train: 2018-07-31T09:45:02.772393: step 12068, loss 0.61441.
Train: 2018-07-31T09:45:02.928644: step 12069, loss 0.467124.
Train: 2018-07-31T09:45:03.084822: step 12070, loss 0.597107.
Test: 2018-07-31T09:45:03.334787: step 12070, loss 0.54788.
Train: 2018-07-31T09:45:03.491006: step 12071, loss 0.562414.
Train: 2018-07-31T09:45:03.662811: step 12072, loss 0.605821.
Train: 2018-07-31T09:45:03.819025: step 12073, loss 0.553737.
Train: 2018-07-31T09:45:03.990883: step 12074, loss 0.57976.
Train: 2018-07-31T09:45:04.147110: step 12075, loss 0.614418.
Train: 2018-07-31T09:45:04.318907: step 12076, loss 0.553754.
Train: 2018-07-31T09:45:04.490772: step 12077, loss 0.545121.
Train: 2018-07-31T09:45:04.646956: step 12078, loss 0.545136.
Train: 2018-07-31T09:45:04.803199: step 12079, loss 0.58829.
Train: 2018-07-31T09:45:04.959413: step 12080, loss 0.635963.
Test: 2018-07-31T09:45:05.193733: step 12080, loss 0.547996.
Train: 2018-07-31T09:45:05.365562: step 12081, loss 0.5624.
Train: 2018-07-31T09:45:05.521781: step 12082, loss 0.631057.
Train: 2018-07-31T09:45:05.693586: step 12083, loss 0.553845.
Train: 2018-07-31T09:45:05.849828: step 12084, loss 0.502683.
Train: 2018-07-31T09:45:06.006013: step 12085, loss 0.579444.
Train: 2018-07-31T09:45:06.177847: step 12086, loss 0.528374.
Train: 2018-07-31T09:45:06.334090: step 12087, loss 0.553904.
Train: 2018-07-31T09:45:06.490274: step 12088, loss 0.553908.
Train: 2018-07-31T09:45:06.662139: step 12089, loss 0.604903.
Train: 2018-07-31T09:45:06.818351: step 12090, loss 0.536936.
Test: 2018-07-31T09:45:07.052673: step 12090, loss 0.548193.
Train: 2018-07-31T09:45:07.224477: step 12091, loss 0.545432.
Train: 2018-07-31T09:45:07.380721: step 12092, loss 0.5709.
Train: 2018-07-31T09:45:07.536904: step 12093, loss 0.519957.
Train: 2018-07-31T09:45:07.693148: step 12094, loss 0.55391.
Train: 2018-07-31T09:45:07.864977: step 12095, loss 0.477338.
Train: 2018-07-31T09:45:08.021165: step 12096, loss 0.545344.
Train: 2018-07-31T09:45:08.177409: step 12097, loss 0.519639.
Train: 2018-07-31T09:45:08.349243: step 12098, loss 0.631038.
Train: 2018-07-31T09:45:08.505457: step 12099, loss 0.51943.
Train: 2018-07-31T09:45:08.661640: step 12100, loss 0.605466.
Test: 2018-07-31T09:45:08.911612: step 12100, loss 0.547959.
Train: 2018-07-31T09:45:09.661405: step 12101, loss 0.605519.
Train: 2018-07-31T09:45:09.817649: step 12102, loss 0.553776.
Train: 2018-07-31T09:45:09.989484: step 12103, loss 0.605544.
Train: 2018-07-31T09:45:10.145667: step 12104, loss 0.484788.
Train: 2018-07-31T09:45:10.317532: step 12105, loss 0.51061.
Train: 2018-07-31T09:45:10.473715: step 12106, loss 0.631589.
Train: 2018-07-31T09:45:10.629928: step 12107, loss 0.67486.
Train: 2018-07-31T09:45:10.801764: step 12108, loss 0.553769.
Train: 2018-07-31T09:45:10.957977: step 12109, loss 0.527918.
Train: 2018-07-31T09:45:11.114222: step 12110, loss 0.605469.
Test: 2018-07-31T09:45:11.364132: step 12110, loss 0.547995.
Train: 2018-07-31T09:45:11.520379: step 12111, loss 0.682808.
Train: 2018-07-31T09:45:11.692181: step 12112, loss 0.605243.
Train: 2018-07-31T09:45:11.848424: step 12113, loss 0.502669.
Train: 2018-07-31T09:45:12.020228: step 12114, loss 0.596455.
Train: 2018-07-31T09:45:12.176442: step 12115, loss 0.604847.
Train: 2018-07-31T09:45:12.332686: step 12116, loss 0.562424.
Train: 2018-07-31T09:45:12.488869: step 12117, loss 0.553997.
Train: 2018-07-31T09:45:12.645112: step 12118, loss 0.604495.
Train: 2018-07-31T09:45:12.801327: step 12119, loss 0.562458.
Train: 2018-07-31T09:45:12.957508: step 12120, loss 0.554108.
Test: 2018-07-31T09:45:13.207450: step 12120, loss 0.548505.
Train: 2018-07-31T09:45:13.363694: step 12121, loss 0.554136.
Train: 2018-07-31T09:45:13.519902: step 12122, loss 0.595826.
Train: 2018-07-31T09:45:13.676121: step 12123, loss 0.537557.
Train: 2018-07-31T09:45:13.847926: step 12124, loss 0.570821.
Train: 2018-07-31T09:45:14.004164: step 12125, loss 0.512732.
Train: 2018-07-31T09:45:14.160353: step 12126, loss 0.570818.
Train: 2018-07-31T09:45:14.316566: step 12127, loss 0.545917.
Train: 2018-07-31T09:45:14.488431: step 12128, loss 0.504367.
Train: 2018-07-31T09:45:14.644638: step 12129, loss 0.504243.
Train: 2018-07-31T09:45:14.800827: step 12130, loss 0.562481.
Test: 2018-07-31T09:45:15.050787: step 12130, loss 0.548444.
Train: 2018-07-31T09:45:15.207007: step 12131, loss 0.554094.
Train: 2018-07-31T09:45:15.363196: step 12132, loss 0.56245.
Train: 2018-07-31T09:45:15.519442: step 12133, loss 0.570855.
Train: 2018-07-31T09:45:15.675623: step 12134, loss 0.587738.
Train: 2018-07-31T09:45:15.847458: step 12135, loss 0.562423.
Train: 2018-07-31T09:45:16.003671: step 12136, loss 0.520104.
Train: 2018-07-31T09:45:16.159885: step 12137, loss 0.545448.
Train: 2018-07-31T09:45:16.300512: step 12138, loss 0.562407.
Train: 2018-07-31T09:45:16.472310: step 12139, loss 0.587968.
Train: 2018-07-31T09:45:16.628524: step 12140, loss 0.502662.
Test: 2018-07-31T09:45:16.862869: step 12140, loss 0.54807.
Train: 2018-07-31T09:45:17.019088: step 12141, loss 0.579511.
Train: 2018-07-31T09:45:17.190925: step 12142, loss 0.553826.
Train: 2018-07-31T09:45:17.347136: step 12143, loss 0.562399.
Train: 2018-07-31T09:45:17.503347: step 12144, loss 0.519378.
Train: 2018-07-31T09:45:17.659564: step 12145, loss 0.640022.
Train: 2018-07-31T09:45:17.800156: step 12146, loss 0.510623.
Train: 2018-07-31T09:45:17.971990: step 12147, loss 0.579691.
Train: 2018-07-31T09:45:18.128204: step 12148, loss 0.536452.
Train: 2018-07-31T09:45:18.284418: step 12149, loss 0.579736.
Train: 2018-07-31T09:45:18.440630: step 12150, loss 0.519058.
Test: 2018-07-31T09:45:18.674921: step 12150, loss 0.547872.
Train: 2018-07-31T09:45:18.831134: step 12151, loss 0.492943.
Train: 2018-07-31T09:45:19.002999: step 12152, loss 0.553717.
Train: 2018-07-31T09:45:19.143561: step 12153, loss 0.536248.
Train: 2018-07-31T09:45:19.315395: step 12154, loss 0.52744.
Train: 2018-07-31T09:45:19.471608: step 12155, loss 0.553682.
Train: 2018-07-31T09:45:19.627853: step 12156, loss 0.500873.
Train: 2018-07-31T09:45:19.784066: step 12157, loss 0.553665.
Train: 2018-07-31T09:45:19.940280: step 12158, loss 0.562517.
Train: 2018-07-31T09:45:20.096463: step 12159, loss 0.544776.
Train: 2018-07-31T09:45:20.252706: step 12160, loss 0.464618.
Test: 2018-07-31T09:45:20.502641: step 12160, loss 0.547626.
Train: 2018-07-31T09:45:20.654751: step 12161, loss 0.571534.
Train: 2018-07-31T09:45:20.826556: step 12162, loss 0.508829.
Train: 2018-07-31T09:45:20.982799: step 12163, loss 0.598662.
Train: 2018-07-31T09:45:21.139012: step 12164, loss 0.544655.
Train: 2018-07-31T09:45:21.310817: step 12165, loss 0.571764.
Train: 2018-07-31T09:45:21.467055: step 12166, loss 0.517468.
Train: 2018-07-31T09:45:21.623275: step 12167, loss 0.60814.
Train: 2018-07-31T09:45:21.779459: step 12168, loss 0.571863.
Train: 2018-07-31T09:45:21.935703: step 12169, loss 0.562787.
Train: 2018-07-31T09:45:22.107536: step 12170, loss 0.608211.
Test: 2018-07-31T09:45:22.341858: step 12170, loss 0.547575.
Train: 2018-07-31T09:45:22.498065: step 12171, loss 0.544623.
Train: 2018-07-31T09:45:22.654282: step 12172, loss 0.526489.
Train: 2018-07-31T09:45:22.810467: step 12173, loss 0.51743.
Train: 2018-07-31T09:45:22.966680: step 12174, loss 0.481149.
Train: 2018-07-31T09:45:23.122923: step 12175, loss 0.544621.
Train: 2018-07-31T09:45:23.294752: step 12176, loss 0.517335.
Train: 2018-07-31T09:45:23.450975: step 12177, loss 0.571936.
Train: 2018-07-31T09:45:23.607154: step 12178, loss 0.535484.
Train: 2018-07-31T09:45:23.763398: step 12179, loss 0.590257.
Train: 2018-07-31T09:45:23.919605: step 12180, loss 0.489789.
Test: 2018-07-31T09:45:24.153932: step 12180, loss 0.54757.
Train: 2018-07-31T09:45:24.310146: step 12181, loss 0.544595.
Train: 2018-07-31T09:45:24.466329: step 12182, loss 0.599545.
Train: 2018-07-31T09:45:24.606951: step 12183, loss 0.517109.
Train: 2018-07-31T09:45:24.778785: step 12184, loss 0.590428.
Train: 2018-07-31T09:45:24.934996: step 12185, loss 0.617923.
Train: 2018-07-31T09:45:25.091183: step 12186, loss 0.544593.
Train: 2018-07-31T09:45:25.247425: step 12187, loss 0.55374.
Train: 2018-07-31T09:45:25.388019: step 12188, loss 0.581136.
Train: 2018-07-31T09:45:25.544201: step 12189, loss 0.599323.
Train: 2018-07-31T09:45:25.700415: step 12190, loss 0.608304.
Test: 2018-07-31T09:45:25.934734: step 12190, loss 0.547576.
Train: 2018-07-31T09:45:26.090947: step 12191, loss 0.544626.
Train: 2018-07-31T09:45:26.247161: step 12192, loss 0.562731.
Train: 2018-07-31T09:45:26.403405: step 12193, loss 0.571719.
Train: 2018-07-31T09:45:26.559587: step 12194, loss 0.571661.
Train: 2018-07-31T09:45:26.715831: step 12195, loss 0.580573.
Train: 2018-07-31T09:45:26.872014: step 12196, loss 0.607313.
Train: 2018-07-31T09:45:27.028261: step 12197, loss 0.616017.
Train: 2018-07-31T09:45:27.184442: step 12198, loss 0.633474.
Train: 2018-07-31T09:45:27.325034: step 12199, loss 0.606585.
Train: 2018-07-31T09:45:27.481281: step 12200, loss 0.60629.
Test: 2018-07-31T09:45:27.715599: step 12200, loss 0.547834.
Train: 2018-07-31T09:45:28.449804: step 12201, loss 0.614701.
Train: 2018-07-31T09:45:28.606014: step 12202, loss 0.553753.
Train: 2018-07-31T09:45:28.777819: step 12203, loss 0.553796.
Train: 2018-07-31T09:45:28.934057: step 12204, loss 0.545284.
Train: 2018-07-31T09:45:29.090246: step 12205, loss 0.605003.
Train: 2018-07-31T09:45:29.246459: step 12206, loss 0.536976.
Train: 2018-07-31T09:45:29.387051: step 12207, loss 0.511742.
Train: 2018-07-31T09:45:29.543295: step 12208, loss 0.528723.
Train: 2018-07-31T09:45:29.699477: step 12209, loss 0.604519.
Train: 2018-07-31T09:45:29.855691: step 12210, loss 0.512051.
Test: 2018-07-31T09:45:30.090036: step 12210, loss 0.548393.
Train: 2018-07-31T09:45:30.246256: step 12211, loss 0.579238.
Train: 2018-07-31T09:45:30.402469: step 12212, loss 0.587615.
Train: 2018-07-31T09:45:30.558682: step 12213, loss 0.554084.
Train: 2018-07-31T09:45:30.730517: step 12214, loss 0.604317.
Train: 2018-07-31T09:45:30.886700: step 12215, loss 0.554115.
Train: 2018-07-31T09:45:31.042943: step 12216, loss 0.50403.
Train: 2018-07-31T09:45:31.199126: step 12217, loss 0.54577.
Train: 2018-07-31T09:45:31.339718: step 12218, loss 0.604272.
Train: 2018-07-31T09:45:31.495932: step 12219, loss 0.604272.
Train: 2018-07-31T09:45:31.652177: step 12220, loss 0.587538.
Test: 2018-07-31T09:45:31.886496: step 12220, loss 0.548506.
Train: 2018-07-31T09:45:32.058300: step 12221, loss 0.670968.
Train: 2018-07-31T09:45:32.214513: step 12222, loss 0.562501.
Train: 2018-07-31T09:45:32.370757: step 12223, loss 0.612308.
Train: 2018-07-31T09:45:32.526942: step 12224, loss 0.546002.
Train: 2018-07-31T09:45:32.683184: step 12225, loss 0.636825.
Train: 2018-07-31T09:45:32.839397: step 12226, loss 0.480375.
Train: 2018-07-31T09:45:32.979960: step 12227, loss 0.579029.
Train: 2018-07-31T09:45:33.136173: step 12228, loss 0.521616.
Train: 2018-07-31T09:45:33.308040: step 12229, loss 0.55442.
Train: 2018-07-31T09:45:33.479844: step 12230, loss 0.57082.
Test: 2018-07-31T09:45:33.714189: step 12230, loss 0.548874.
Train: 2018-07-31T09:45:33.870407: step 12231, loss 0.527609.
Train: 2018-07-31T09:45:34.010993: step 12232, loss 0.562603.
Train: 2018-07-31T09:45:34.182828: step 12233, loss 0.587266.
Train: 2018-07-31T09:45:34.323395: step 12234, loss 0.513196.
Train: 2018-07-31T09:45:34.479608: step 12235, loss 0.570815.
Train: 2018-07-31T09:45:34.635821: step 12236, loss 0.562553.
Train: 2018-07-31T09:45:34.792034: step 12237, loss 0.579092.
Train: 2018-07-31T09:45:34.948248: step 12238, loss 0.512802.
Train: 2018-07-31T09:45:35.088871: step 12239, loss 0.537584.
Train: 2018-07-31T09:45:35.245084: step 12240, loss 0.512502.
Test: 2018-07-31T09:45:35.479399: step 12240, loss 0.548464.
Train: 2018-07-31T09:45:35.635618: step 12241, loss 0.545747.
Train: 2018-07-31T09:45:35.791831: step 12242, loss 0.495295.
Train: 2018-07-31T09:45:35.948040: step 12243, loss 0.553997.
Train: 2018-07-31T09:45:36.104228: step 12244, loss 0.570894.
Train: 2018-07-31T09:45:36.260472: step 12245, loss 0.553905.
Train: 2018-07-31T09:45:36.401066: step 12246, loss 0.570944.
Train: 2018-07-31T09:45:36.557279: step 12247, loss 0.630932.
Train: 2018-07-31T09:45:36.713491: step 12248, loss 0.562402.
Train: 2018-07-31T09:45:36.869703: step 12249, loss 0.596771.
Train: 2018-07-31T09:45:37.025888: step 12250, loss 0.545203.
Test: 2018-07-31T09:45:37.275828: step 12250, loss 0.547988.
Train: 2018-07-31T09:45:37.416451: step 12251, loss 0.510769.
Train: 2018-07-31T09:45:37.572635: step 12252, loss 0.614122.
Train: 2018-07-31T09:45:37.744468: step 12253, loss 0.527901.
Train: 2018-07-31T09:45:37.885061: step 12254, loss 0.553768.
Train: 2018-07-31T09:45:38.041275: step 12255, loss 0.579695.
Train: 2018-07-31T09:45:38.197488: step 12256, loss 0.536453.
Train: 2018-07-31T09:45:38.353702: step 12257, loss 0.58839.
Train: 2018-07-31T09:45:38.509916: step 12258, loss 0.614393.
Train: 2018-07-31T09:45:38.666159: step 12259, loss 0.605701.
Train: 2018-07-31T09:45:38.806753: step 12260, loss 0.52782.
Test: 2018-07-31T09:45:39.041042: step 12260, loss 0.547933.
Train: 2018-07-31T09:45:39.197254: step 12261, loss 0.527841.
Train: 2018-07-31T09:45:39.353474: step 12262, loss 0.588327.
Train: 2018-07-31T09:45:39.509711: step 12263, loss 0.588311.
Train: 2018-07-31T09:45:39.650273: step 12264, loss 0.622798.
Train: 2018-07-31T09:45:39.806486: step 12265, loss 0.648503.
Train: 2018-07-31T09:45:39.962730: step 12266, loss 0.519502.
Train: 2018-07-31T09:45:40.118913: step 12267, loss 0.536717.
Train: 2018-07-31T09:45:40.275127: step 12268, loss 0.562397.
Train: 2018-07-31T09:45:40.431373: step 12269, loss 0.5112.
Train: 2018-07-31T09:45:40.587584: step 12270, loss 0.52828.
Test: 2018-07-31T09:45:40.821908: step 12270, loss 0.548107.
Train: 2018-07-31T09:45:40.978117: step 12271, loss 0.519728.
Train: 2018-07-31T09:45:41.134330: step 12272, loss 0.536762.
Train: 2018-07-31T09:45:41.290545: step 12273, loss 0.562402.
Train: 2018-07-31T09:45:41.446728: step 12274, loss 0.579542.
Train: 2018-07-31T09:45:41.618586: step 12275, loss 0.562395.
Train: 2018-07-31T09:45:41.774805: step 12276, loss 0.562397.
Train: 2018-07-31T09:45:41.930990: step 12277, loss 0.588182.
Train: 2018-07-31T09:45:42.071614: step 12278, loss 0.485019.
Train: 2018-07-31T09:45:42.227815: step 12279, loss 0.562399.
Train: 2018-07-31T09:45:42.384038: step 12280, loss 0.61415.
Test: 2018-07-31T09:45:42.633951: step 12280, loss 0.547952.
Train: 2018-07-31T09:45:42.790193: step 12281, loss 0.579658.
Train: 2018-07-31T09:45:42.946410: step 12282, loss 0.553767.
Train: 2018-07-31T09:45:43.102619: step 12283, loss 0.605545.
Train: 2018-07-31T09:45:43.258802: step 12284, loss 0.579637.
Train: 2018-07-31T09:45:43.415015: step 12285, loss 0.57101.
Train: 2018-07-31T09:45:43.571229: step 12286, loss 0.605423.
Train: 2018-07-31T09:45:43.727443: step 12287, loss 0.57098.
Train: 2018-07-31T09:45:43.883657: step 12288, loss 0.588115.
Train: 2018-07-31T09:45:44.024279: step 12289, loss 0.588053.
Train: 2018-07-31T09:45:44.180494: step 12290, loss 0.536816.
Test: 2018-07-31T09:45:44.430436: step 12290, loss 0.548139.
Train: 2018-07-31T09:45:44.586616: step 12291, loss 0.579435.
Train: 2018-07-31T09:45:44.742830: step 12292, loss 0.596411.
Train: 2018-07-31T09:45:44.899045: step 12293, loss 0.630262.
Train: 2018-07-31T09:45:45.039635: step 12294, loss 0.511698.
Train: 2018-07-31T09:45:45.195849: step 12295, loss 0.520244.
Train: 2018-07-31T09:45:45.352092: step 12296, loss 0.528701.
Train: 2018-07-31T09:45:45.508306: step 12297, loss 0.503421.
Train: 2018-07-31T09:45:45.664519: step 12298, loss 0.545545.
Train: 2018-07-31T09:45:45.805112: step 12299, loss 0.52015.
Train: 2018-07-31T09:45:45.961326: step 12300, loss 0.536999.
Test: 2018-07-31T09:45:46.211266: step 12300, loss 0.548178.
Train: 2018-07-31T09:45:46.976683: step 12301, loss 0.562413.
Train: 2018-07-31T09:45:47.148517: step 12302, loss 0.596474.
Train: 2018-07-31T09:45:47.289139: step 12303, loss 0.596508.
Train: 2018-07-31T09:45:47.445322: step 12304, loss 0.536792.
Train: 2018-07-31T09:45:47.601560: step 12305, loss 0.622232.
Train: 2018-07-31T09:45:47.757774: step 12306, loss 0.545312.
Train: 2018-07-31T09:45:47.913993: step 12307, loss 0.553857.
Train: 2018-07-31T09:45:48.070176: step 12308, loss 0.562402.
Train: 2018-07-31T09:45:48.226389: step 12309, loss 0.639403.
Train: 2018-07-31T09:45:48.382602: step 12310, loss 0.519675.
Test: 2018-07-31T09:45:48.616954: step 12310, loss 0.548092.
Train: 2018-07-31T09:45:48.773166: step 12311, loss 0.605118.
Train: 2018-07-31T09:45:48.929382: step 12312, loss 0.596535.
Train: 2018-07-31T09:45:49.085593: step 12313, loss 0.630569.
Train: 2018-07-31T09:45:49.241809: step 12314, loss 0.553906.
Train: 2018-07-31T09:45:49.397990: step 12315, loss 0.570897.
Train: 2018-07-31T09:45:49.554236: step 12316, loss 0.520113.
Train: 2018-07-31T09:45:49.710452: step 12317, loss 0.604681.
Train: 2018-07-31T09:45:49.851039: step 12318, loss 0.537111.
Train: 2018-07-31T09:45:50.007223: step 12319, loss 0.537133.
Train: 2018-07-31T09:45:50.179089: step 12320, loss 0.638289.
Test: 2018-07-31T09:45:50.413402: step 12320, loss 0.548348.
Train: 2018-07-31T09:45:50.569624: step 12321, loss 0.570851.
Train: 2018-07-31T09:45:50.725834: step 12322, loss 0.554046.
Train: 2018-07-31T09:45:50.882017: step 12323, loss 0.570851.
Train: 2018-07-31T09:45:51.038231: step 12324, loss 0.612744.
Train: 2018-07-31T09:45:51.194445: step 12325, loss 0.512282.
Train: 2018-07-31T09:45:51.350658: step 12326, loss 0.503952.
Train: 2018-07-31T09:45:51.506897: step 12327, loss 0.562466.
Train: 2018-07-31T09:45:51.647494: step 12328, loss 0.57084.
Train: 2018-07-31T09:45:51.803677: step 12329, loss 0.537327.
Train: 2018-07-31T09:45:51.959920: step 12330, loss 0.537286.
Test: 2018-07-31T09:45:52.209831: step 12330, loss 0.548373.
Train: 2018-07-31T09:45:52.366045: step 12331, loss 0.537231.
Train: 2018-07-31T09:45:52.522289: step 12332, loss 0.596117.
Train: 2018-07-31T09:45:52.678471: step 12333, loss 0.587729.
Train: 2018-07-31T09:45:52.834715: step 12334, loss 0.59618.
Train: 2018-07-31T09:45:52.990899: step 12335, loss 0.613063.
Train: 2018-07-31T09:45:53.147112: step 12336, loss 0.511839.
Train: 2018-07-31T09:45:53.303358: step 12337, loss 0.570859.
Train: 2018-07-31T09:45:53.459575: step 12338, loss 0.511811.
Train: 2018-07-31T09:45:53.615782: step 12339, loss 0.638447.
Train: 2018-07-31T09:45:53.771990: step 12340, loss 0.672196.
Test: 2018-07-31T09:45:54.006286: step 12340, loss 0.548329.
Train: 2018-07-31T09:45:54.162500: step 12341, loss 0.537161.
Train: 2018-07-31T09:45:54.318712: step 12342, loss 0.520395.
Train: 2018-07-31T09:45:54.474926: step 12343, loss 0.579255.
Train: 2018-07-31T09:45:54.631139: step 12344, loss 0.64644.
Train: 2018-07-31T09:45:54.787383: step 12345, loss 0.595976.
Train: 2018-07-31T09:45:54.943565: step 12346, loss 0.612625.
Train: 2018-07-31T09:45:55.099811: step 12347, loss 0.562491.
Train: 2018-07-31T09:45:55.255993: step 12348, loss 0.56251.
Train: 2018-07-31T09:45:55.412206: step 12349, loss 0.570818.
Train: 2018-07-31T09:45:55.568419: step 12350, loss 0.50465.
Test: 2018-07-31T09:45:55.802740: step 12350, loss 0.548708.
Train: 2018-07-31T09:45:55.958954: step 12351, loss 0.488164.
Train: 2018-07-31T09:45:56.115197: step 12352, loss 0.554263.
Train: 2018-07-31T09:45:56.271412: step 12353, loss 0.612248.
Train: 2018-07-31T09:45:56.427627: step 12354, loss 0.587401.
Train: 2018-07-31T09:45:56.583838: step 12355, loss 0.504495.
Train: 2018-07-31T09:45:56.755673: step 12356, loss 0.587424.
Train: 2018-07-31T09:45:56.911880: step 12357, loss 0.595742.
Train: 2018-07-31T09:45:57.068108: step 12358, loss 0.520955.
Train: 2018-07-31T09:45:57.224313: step 12359, loss 0.512573.
Train: 2018-07-31T09:45:57.380496: step 12360, loss 0.612523.
Test: 2018-07-31T09:45:57.614817: step 12360, loss 0.548498.
Train: 2018-07-31T09:45:57.786681: step 12361, loss 0.604214.
Train: 2018-07-31T09:45:57.942898: step 12362, loss 0.520725.
Train: 2018-07-31T09:45:58.099108: step 12363, loss 0.503953.
Train: 2018-07-31T09:45:58.255290: step 12364, loss 0.587612.
Train: 2018-07-31T09:45:58.427158: step 12365, loss 0.545658.
Train: 2018-07-31T09:45:58.583370: step 12366, loss 0.495142.
Train: 2018-07-31T09:45:58.739553: step 12367, loss 0.511776.
Train: 2018-07-31T09:45:58.895796: step 12368, loss 0.570891.
Train: 2018-07-31T09:45:59.051980: step 12369, loss 0.468853.
Train: 2018-07-31T09:45:59.208218: step 12370, loss 0.622194.
Test: 2018-07-31T09:45:59.442548: step 12370, loss 0.548037.
Train: 2018-07-31T09:45:59.598726: step 12371, loss 0.553853.
Train: 2018-07-31T09:45:59.754964: step 12372, loss 0.57961.
Train: 2018-07-31T09:45:59.926807: step 12373, loss 0.5279.
Train: 2018-07-31T09:46:00.083018: step 12374, loss 0.571029.
Train: 2018-07-31T09:46:00.254852: step 12375, loss 0.536424.
Train: 2018-07-31T09:46:00.411036: step 12376, loss 0.562423.
Train: 2018-07-31T09:46:00.567250: step 12377, loss 0.544986.
Train: 2018-07-31T09:46:00.723463: step 12378, loss 0.518723.
Train: 2018-07-31T09:46:00.879707: step 12379, loss 0.501147.
Train: 2018-07-31T09:46:01.035920: step 12380, loss 0.536084.
Test: 2018-07-31T09:46:01.270240: step 12380, loss 0.547715.
Train: 2018-07-31T09:46:01.442044: step 12381, loss 0.518367.
Train: 2018-07-31T09:46:01.582636: step 12382, loss 0.524705.
Train: 2018-07-31T09:46:01.754497: step 12383, loss 0.544675.
Train: 2018-07-31T09:46:01.910715: step 12384, loss 0.535894.
Train: 2018-07-31T09:46:02.066932: step 12385, loss 0.517665.
Train: 2018-07-31T09:46:02.223111: step 12386, loss 0.544636.
Train: 2018-07-31T09:46:02.379361: step 12387, loss 0.526539.
Train: 2018-07-31T09:46:02.535569: step 12388, loss 0.526484.
Train: 2018-07-31T09:46:02.707374: step 12389, loss 0.562942.
Train: 2018-07-31T09:46:02.863623: step 12390, loss 0.617615.
Test: 2018-07-31T09:46:03.097908: step 12390, loss 0.547573.
Train: 2018-07-31T09:46:03.269742: step 12391, loss 0.508077.
Train: 2018-07-31T09:46:03.425955: step 12392, loss 0.618123.
Train: 2018-07-31T09:46:03.582199: step 12393, loss 0.572242.
Train: 2018-07-31T09:46:03.738412: step 12394, loss 0.590212.
Train: 2018-07-31T09:46:03.894595: step 12395, loss 0.59057.
Train: 2018-07-31T09:46:04.050808: step 12396, loss 0.636268.
Train: 2018-07-31T09:46:04.222645: step 12397, loss 0.544545.
Train: 2018-07-31T09:46:04.378887: step 12398, loss 0.526386.
Train: 2018-07-31T09:46:04.535071: step 12399, loss 0.608218.
Train: 2018-07-31T09:46:04.691317: step 12400, loss 0.562646.
Test: 2018-07-31T09:46:04.925634: step 12400, loss 0.547584.
Train: 2018-07-31T09:46:05.612973: step 12401, loss 0.508453.
Train: 2018-07-31T09:46:05.784809: step 12402, loss 0.589879.
Train: 2018-07-31T09:46:05.941024: step 12403, loss 0.535631.
Train: 2018-07-31T09:46:06.097205: step 12404, loss 0.517772.
Train: 2018-07-31T09:46:06.269059: step 12405, loss 0.535728.
Train: 2018-07-31T09:46:06.425285: step 12406, loss 0.562745.
Train: 2018-07-31T09:46:06.581466: step 12407, loss 0.526711.
Train: 2018-07-31T09:46:06.737710: step 12408, loss 0.616397.
Train: 2018-07-31T09:46:06.893893: step 12409, loss 0.508891.
Train: 2018-07-31T09:46:07.050140: step 12410, loss 0.526859.
Test: 2018-07-31T09:46:07.284427: step 12410, loss 0.547622.
Train: 2018-07-31T09:46:07.456291: step 12411, loss 0.571647.
Train: 2018-07-31T09:46:07.612498: step 12412, loss 0.517906.
Train: 2018-07-31T09:46:07.768724: step 12413, loss 0.589498.
Train: 2018-07-31T09:46:07.924933: step 12414, loss 0.553667.
Train: 2018-07-31T09:46:08.081114: step 12415, loss 0.598339.
Train: 2018-07-31T09:46:08.237359: step 12416, loss 0.651857.
Train: 2018-07-31T09:46:08.409193: step 12417, loss 0.535847.
Train: 2018-07-31T09:46:08.565410: step 12418, loss 0.624755.
Train: 2018-07-31T09:46:08.721590: step 12419, loss 0.553679.
Train: 2018-07-31T09:46:08.893449: step 12420, loss 0.536007.
Test: 2018-07-31T09:46:09.127776: step 12420, loss 0.547726.
Train: 2018-07-31T09:46:09.283959: step 12421, loss 0.536055.
Train: 2018-07-31T09:46:09.455823: step 12422, loss 0.580065.
Train: 2018-07-31T09:46:09.596418: step 12423, loss 0.527344.
Train: 2018-07-31T09:46:09.768220: step 12424, loss 0.553689.
Train: 2018-07-31T09:46:09.924466: step 12425, loss 0.536172.
Train: 2018-07-31T09:46:10.080647: step 12426, loss 0.536182.
Train: 2018-07-31T09:46:10.236859: step 12427, loss 0.544938.
Train: 2018-07-31T09:46:10.393104: step 12428, loss 0.685024.
Train: 2018-07-31T09:46:10.549319: step 12429, loss 0.571173.
Train: 2018-07-31T09:46:10.705531: step 12430, loss 0.632163.
Test: 2018-07-31T09:46:10.939855: step 12430, loss 0.547867.
Train: 2018-07-31T09:46:11.111656: step 12431, loss 0.614541.
Train: 2018-07-31T09:46:11.267868: step 12432, loss 0.553755.
Train: 2018-07-31T09:46:11.424081: step 12433, loss 0.553781.
Train: 2018-07-31T09:46:11.580295: step 12434, loss 0.579587.
Train: 2018-07-31T09:46:11.736509: step 12435, loss 0.596671.
Train: 2018-07-31T09:46:11.892722: step 12436, loss 0.570938.
Train: 2018-07-31T09:46:12.048966: step 12437, loss 0.528366.
Train: 2018-07-31T09:46:12.205149: step 12438, loss 0.63034.
Train: 2018-07-31T09:46:12.377014: step 12439, loss 0.604732.
Train: 2018-07-31T09:46:12.533224: step 12440, loss 0.528715.
Test: 2018-07-31T09:46:12.767548: step 12440, loss 0.548363.
Train: 2018-07-31T09:46:12.923764: step 12441, loss 0.57085.
Train: 2018-07-31T09:46:13.079944: step 12442, loss 0.570841.
Train: 2018-07-31T09:46:13.236188: step 12443, loss 0.512269.
Train: 2018-07-31T09:46:13.392401: step 12444, loss 0.579191.
Train: 2018-07-31T09:46:13.548614: step 12445, loss 0.620925.
Train: 2018-07-31T09:46:13.720444: step 12446, loss 0.562494.
Train: 2018-07-31T09:46:13.876632: step 12447, loss 0.49596.
Train: 2018-07-31T09:46:14.032846: step 12448, loss 0.579144.
Train: 2018-07-31T09:46:14.204711: step 12449, loss 0.454402.
Train: 2018-07-31T09:46:14.360895: step 12450, loss 0.595818.
Test: 2018-07-31T09:46:14.610848: step 12450, loss 0.548505.
Train: 2018-07-31T09:46:14.767048: step 12451, loss 0.579178.
Train: 2018-07-31T09:46:14.923262: step 12452, loss 0.612603.
Train: 2018-07-31T09:46:15.063888: step 12453, loss 0.637673.
Train: 2018-07-31T09:46:15.220069: step 12454, loss 0.495752.
Train: 2018-07-31T09:46:15.391930: step 12455, loss 0.595859.
Train: 2018-07-31T09:46:15.548149: step 12456, loss 0.570826.
Train: 2018-07-31T09:46:15.719952: step 12457, loss 0.504129.
Train: 2018-07-31T09:46:15.876200: step 12458, loss 0.512411.
Train: 2018-07-31T09:46:16.032408: step 12459, loss 0.621006.
Train: 2018-07-31T09:46:16.188625: step 12460, loss 0.562466.
Test: 2018-07-31T09:46:16.422913: step 12460, loss 0.548435.
Train: 2018-07-31T09:46:16.579158: step 12461, loss 0.579212.
Train: 2018-07-31T09:46:16.750991: step 12462, loss 0.545701.
Train: 2018-07-31T09:46:16.907203: step 12463, loss 0.604388.
Train: 2018-07-31T09:46:17.063420: step 12464, loss 0.57923.
Train: 2018-07-31T09:46:17.235251: step 12465, loss 0.587612.
Train: 2018-07-31T09:46:17.391466: step 12466, loss 0.545694.
Train: 2018-07-31T09:46:17.547649: step 12467, loss 0.554079.
Train: 2018-07-31T09:46:17.719513: step 12468, loss 0.604362.
Train: 2018-07-31T09:46:17.875696: step 12469, loss 0.570838.
Train: 2018-07-31T09:46:18.031910: step 12470, loss 0.587582.
Test: 2018-07-31T09:46:18.266262: step 12470, loss 0.548463.
Train: 2018-07-31T09:46:18.438064: step 12471, loss 0.562469.
Train: 2018-07-31T09:46:18.594308: step 12472, loss 0.470548.
Train: 2018-07-31T09:46:18.750492: step 12473, loss 0.587576.
Train: 2018-07-31T09:46:18.922327: step 12474, loss 0.604336.
Train: 2018-07-31T09:46:19.078570: step 12475, loss 0.503831.
Train: 2018-07-31T09:46:19.234783: step 12476, loss 0.520519.
Train: 2018-07-31T09:46:19.406618: step 12477, loss 0.579245.
Train: 2018-07-31T09:46:19.562802: step 12478, loss 0.579272.
Train: 2018-07-31T09:46:19.719014: step 12479, loss 0.503436.
Train: 2018-07-31T09:46:19.875252: step 12480, loss 0.587762.
Test: 2018-07-31T09:46:20.125201: step 12480, loss 0.548249.
Train: 2018-07-31T09:46:20.281407: step 12481, loss 0.570886.
Train: 2018-07-31T09:46:20.437597: step 12482, loss 0.596287.
Train: 2018-07-31T09:46:20.625077: step 12483, loss 0.553943.
Train: 2018-07-31T09:46:20.781298: step 12484, loss 0.511527.
Train: 2018-07-31T09:46:20.953125: step 12485, loss 0.562411.
Train: 2018-07-31T09:46:21.109340: step 12486, loss 0.545375.
Train: 2018-07-31T09:46:21.281180: step 12487, loss 0.553913.
Train: 2018-07-31T09:46:21.437363: step 12488, loss 0.519675.
Train: 2018-07-31T09:46:21.593576: step 12489, loss 0.579523.
Train: 2018-07-31T09:46:21.765436: step 12490, loss 0.570969.
Test: 2018-07-31T09:46:21.999732: step 12490, loss 0.548.
Train: 2018-07-31T09:46:22.155970: step 12491, loss 0.55381.
Train: 2018-07-31T09:46:22.327809: step 12492, loss 0.579626.
Train: 2018-07-31T09:46:22.484017: step 12493, loss 0.553794.
Train: 2018-07-31T09:46:22.640205: step 12494, loss 0.519252.
Train: 2018-07-31T09:46:22.812040: step 12495, loss 0.571045.
Train: 2018-07-31T09:46:22.968253: step 12496, loss 0.605705.
Train: 2018-07-31T09:46:23.124497: step 12497, loss 0.579737.
Train: 2018-07-31T09:46:23.311957: step 12498, loss 0.545088.
Train: 2018-07-31T09:46:23.468168: step 12499, loss 0.510426.
Train: 2018-07-31T09:46:23.640002: step 12500, loss 0.553731.
Test: 2018-07-31T09:46:23.874293: step 12500, loss 0.54787.
Train: 2018-07-31T09:46:24.592903: step 12501, loss 0.571092.
Train: 2018-07-31T09:46:24.749116: step 12502, loss 0.536341.
Train: 2018-07-31T09:46:24.920949: step 12503, loss 0.518905.
Train: 2018-07-31T09:46:25.077165: step 12504, loss 0.632186.
Train: 2018-07-31T09:46:25.249000: step 12505, loss 0.649646.
Train: 2018-07-31T09:46:25.405214: step 12506, loss 0.510176.
Train: 2018-07-31T09:46:25.577018: step 12507, loss 0.484078.
Train: 2018-07-31T09:46:25.733262: step 12508, loss 0.640853.
Train: 2018-07-31T09:46:25.905097: step 12509, loss 0.553717.
Train: 2018-07-31T09:46:26.061280: step 12510, loss 0.518899.
Test: 2018-07-31T09:46:26.311246: step 12510, loss 0.54784.
Train: 2018-07-31T09:46:26.467465: step 12511, loss 0.579837.
Train: 2018-07-31T09:46:26.623678: step 12512, loss 0.562428.
Train: 2018-07-31T09:46:26.795515: step 12513, loss 0.536312.
Train: 2018-07-31T09:46:26.951695: step 12514, loss 0.553714.
Train: 2018-07-31T09:46:27.123530: step 12515, loss 0.597281.
Train: 2018-07-31T09:46:27.279744: step 12516, loss 0.614676.
Train: 2018-07-31T09:46:27.435983: step 12517, loss 0.527633.
Train: 2018-07-31T09:46:27.607816: step 12518, loss 0.518965.
Train: 2018-07-31T09:46:27.764036: step 12519, loss 0.501576.
Train: 2018-07-31T09:46:27.920219: step 12520, loss 0.562418.
Test: 2018-07-31T09:46:28.170160: step 12520, loss 0.547835.
Train: 2018-07-31T09:46:28.326404: step 12521, loss 0.553713.
Train: 2018-07-31T09:46:28.498239: step 12522, loss 0.562429.
Train: 2018-07-31T09:46:28.670044: step 12523, loss 0.562432.
Train: 2018-07-31T09:46:28.841879: step 12524, loss 0.536235.
Train: 2018-07-31T09:46:28.998121: step 12525, loss 0.59742.
Train: 2018-07-31T09:46:29.154306: step 12526, loss 0.614916.
Train: 2018-07-31T09:46:29.326139: step 12527, loss 0.527491.
Train: 2018-07-31T09:46:29.482353: step 12528, loss 0.52749.
Train: 2018-07-31T09:46:29.654218: step 12529, loss 0.509987.
Train: 2018-07-31T09:46:29.826053: step 12530, loss 0.588696.
Test: 2018-07-31T09:46:30.060378: step 12530, loss 0.547783.
Train: 2018-07-31T09:46:30.232178: step 12531, loss 0.553693.
Train: 2018-07-31T09:46:30.404013: step 12532, loss 0.553694.
Train: 2018-07-31T09:46:30.560226: step 12533, loss 0.487656.
Train: 2018-07-31T09:46:30.716469: step 12534, loss 0.632722.
Train: 2018-07-31T09:46:30.872683: step 12535, loss 0.55368.
Train: 2018-07-31T09:46:31.044488: step 12536, loss 0.483391.
Train: 2018-07-31T09:46:31.216353: step 12537, loss 0.518467.
Train: 2018-07-31T09:46:31.388182: step 12538, loss 0.527212.
Train: 2018-07-31T09:46:31.544403: step 12539, loss 0.562502.
Train: 2018-07-31T09:46:31.716236: step 12540, loss 0.695413.
Test: 2018-07-31T09:46:31.950525: step 12540, loss 0.547688.
Train: 2018-07-31T09:46:32.122384: step 12541, loss 0.606782.
Train: 2018-07-31T09:46:32.294196: step 12542, loss 0.500629.
Train: 2018-07-31T09:46:32.450407: step 12543, loss 0.500653.
Train: 2018-07-31T09:46:32.622273: step 12544, loss 0.553671.
Train: 2018-07-31T09:46:32.778486: step 12545, loss 0.482886.
Train: 2018-07-31T09:46:32.950321: step 12546, loss 0.562519.
Train: 2018-07-31T09:46:33.106505: step 12547, loss 0.544784.
Train: 2018-07-31T09:46:33.278370: step 12548, loss 0.580332.
Train: 2018-07-31T09:46:33.450173: step 12549, loss 0.562559.
Train: 2018-07-31T09:46:33.606387: step 12550, loss 0.518045.
Test: 2018-07-31T09:46:33.856328: step 12550, loss 0.547641.
Train: 2018-07-31T09:46:34.012575: step 12551, loss 0.598231.
Train: 2018-07-31T09:46:34.200029: step 12552, loss 0.669592.
Train: 2018-07-31T09:46:34.356212: step 12553, loss 0.544748.
Train: 2018-07-31T09:46:34.528086: step 12554, loss 0.589206.
Train: 2018-07-31T09:46:34.684259: step 12555, loss 0.580279.
Train: 2018-07-31T09:46:34.856120: step 12556, loss 0.580219.
Train: 2018-07-31T09:46:35.012339: step 12557, loss 0.553671.
Train: 2018-07-31T09:46:35.184142: step 12558, loss 0.580101.
Train: 2018-07-31T09:46:35.355978: step 12559, loss 0.588839.
Train: 2018-07-31T09:46:35.527843: step 12560, loss 0.474806.
Test: 2018-07-31T09:46:35.762164: step 12560, loss 0.547782.
Train: 2018-07-31T09:46:35.933967: step 12561, loss 0.527421.
Train: 2018-07-31T09:46:36.105833: step 12562, loss 0.58871.
Train: 2018-07-31T09:46:36.277637: step 12563, loss 0.588678.
Train: 2018-07-31T09:46:36.433884: step 12564, loss 0.562435.
Train: 2018-07-31T09:46:36.605684: step 12565, loss 0.527533.
Train: 2018-07-31T09:46:36.761929: step 12566, loss 0.510116.
Train: 2018-07-31T09:46:36.933767: step 12567, loss 0.588598.
Train: 2018-07-31T09:46:37.089977: step 12568, loss 0.53627.
Train: 2018-07-31T09:46:37.261811: step 12569, loss 0.544992.
Train: 2018-07-31T09:46:37.418023: step 12570, loss 0.553707.
Test: 2018-07-31T09:46:37.667961: step 12570, loss 0.547813.
Train: 2018-07-31T09:46:37.824149: step 12571, loss 0.518777.
Train: 2018-07-31T09:46:37.995984: step 12572, loss 0.614887.
Train: 2018-07-31T09:46:38.152197: step 12573, loss 0.492508.
Train: 2018-07-31T09:46:38.324063: step 12574, loss 0.571195.
Train: 2018-07-31T09:46:38.495897: step 12575, loss 0.50989.
Train: 2018-07-31T09:46:38.652081: step 12576, loss 0.580005.
Train: 2018-07-31T09:46:38.823945: step 12577, loss 0.518543.
Train: 2018-07-31T09:46:38.980130: step 12578, loss 0.615261.
Train: 2018-07-31T09:46:39.151963: step 12579, loss 0.536074.
Train: 2018-07-31T09:46:39.323799: step 12580, loss 0.624148.
Test: 2018-07-31T09:46:39.558119: step 12580, loss 0.547732.
Train: 2018-07-31T09:46:39.776848: step 12581, loss 0.588889.
Train: 2018-07-31T09:46:39.948683: step 12582, loss 0.544882.
Train: 2018-07-31T09:46:40.120487: step 12583, loss 0.632764.
Train: 2018-07-31T09:46:40.292355: step 12584, loss 0.650142.
Train: 2018-07-31T09:46:40.448565: step 12585, loss 0.562439.
Train: 2018-07-31T09:46:40.620397: step 12586, loss 0.658207.
Train: 2018-07-31T09:46:40.776584: step 12587, loss 0.527752.
Train: 2018-07-31T09:46:40.948419: step 12588, loss 0.571035.
Train: 2018-07-31T09:46:41.120252: step 12589, loss 0.527996.
Train: 2018-07-31T09:46:41.292113: step 12590, loss 0.536664.
Test: 2018-07-31T09:46:41.526440: step 12590, loss 0.548061.
Train: 2018-07-31T09:46:41.698273: step 12591, loss 0.5624.
Train: 2018-07-31T09:46:41.870078: step 12592, loss 0.528209.
Train: 2018-07-31T09:46:42.026291: step 12593, loss 0.579483.
Train: 2018-07-31T09:46:42.198125: step 12594, loss 0.605058.
Train: 2018-07-31T09:46:42.369961: step 12595, loss 0.477245.
Train: 2018-07-31T09:46:42.541796: step 12596, loss 0.545372.
Train: 2018-07-31T09:46:42.698009: step 12597, loss 0.545364.
Train: 2018-07-31T09:46:42.869843: step 12598, loss 0.613565.
Train: 2018-07-31T09:46:43.041702: step 12599, loss 0.570927.
Train: 2018-07-31T09:46:43.197891: step 12600, loss 0.587966.
Test: 2018-07-31T09:46:43.447833: step 12600, loss 0.548145.
Train: 2018-07-31T09:46:44.197687: step 12601, loss 0.511317.
Train: 2018-07-31T09:46:44.353871: step 12602, loss 0.58796.
Train: 2018-07-31T09:46:44.525706: step 12603, loss 0.553891.
Train: 2018-07-31T09:46:44.681943: step 12604, loss 0.570917.
Train: 2018-07-31T09:46:44.853784: step 12605, loss 0.622007.
Train: 2018-07-31T09:46:45.025589: step 12606, loss 0.587921.
Train: 2018-07-31T09:46:45.181835: step 12607, loss 0.511472.
Train: 2018-07-31T09:46:45.338015: step 12608, loss 0.528464.
Train: 2018-07-31T09:46:45.509850: step 12609, loss 0.630336.
Train: 2018-07-31T09:46:45.681718: step 12610, loss 0.553934.
Test: 2018-07-31T09:46:45.916034: step 12610, loss 0.548218.
Train: 2018-07-31T09:46:46.087869: step 12611, loss 0.511553.
Train: 2018-07-31T09:46:46.244083: step 12612, loss 0.553934.
Train: 2018-07-31T09:46:46.415918: step 12613, loss 0.528467.
Train: 2018-07-31T09:46:46.587723: step 12614, loss 0.528416.
Train: 2018-07-31T09:46:46.743961: step 12615, loss 0.596463.
Train: 2018-07-31T09:46:46.900149: step 12616, loss 0.536833.
Train: 2018-07-31T09:46:47.071983: step 12617, loss 0.52826.
Train: 2018-07-31T09:46:47.243850: step 12618, loss 0.545294.
Train: 2018-07-31T09:46:47.400063: step 12619, loss 0.528112.
Train: 2018-07-31T09:46:47.571892: step 12620, loss 0.570993.
Test: 2018-07-31T09:46:47.806189: step 12620, loss 0.547976.
Train: 2018-07-31T09:46:47.978021: step 12621, loss 0.605466.
Train: 2018-07-31T09:46:48.149889: step 12622, loss 0.571024.
Train: 2018-07-31T09:46:48.306104: step 12623, loss 0.493369.
Train: 2018-07-31T09:46:48.477906: step 12624, loss 0.510525.
Train: 2018-07-31T09:46:48.649770: step 12625, loss 0.5364.
Train: 2018-07-31T09:46:48.805978: step 12626, loss 0.623294.
Train: 2018-07-31T09:46:48.977788: step 12627, loss 0.588551.
Train: 2018-07-31T09:46:49.134032: step 12628, loss 0.606003.
Train: 2018-07-31T09:46:49.305870: step 12629, loss 0.527574.
Train: 2018-07-31T09:46:49.477672: step 12630, loss 0.588577.
Test: 2018-07-31T09:46:49.712015: step 12630, loss 0.547833.
Train: 2018-07-31T09:46:49.883861: step 12631, loss 0.58857.
Train: 2018-07-31T09:46:50.040040: step 12632, loss 0.605963.
Train: 2018-07-31T09:46:50.211904: step 12633, loss 0.510256.
Train: 2018-07-31T09:46:50.383708: step 12634, loss 0.623246.
Train: 2018-07-31T09:46:50.539955: step 12635, loss 0.510361.
Train: 2018-07-31T09:46:50.711790: step 12636, loss 0.562412.
Train: 2018-07-31T09:46:50.883591: step 12637, loss 0.579743.
Train: 2018-07-31T09:46:51.039805: step 12638, loss 0.579725.
Train: 2018-07-31T09:46:51.211640: step 12639, loss 0.493218.
Train: 2018-07-31T09:46:51.367883: step 12640, loss 0.553756.
Test: 2018-07-31T09:46:51.617826: step 12640, loss 0.547914.
Train: 2018-07-31T09:46:51.774008: step 12641, loss 0.519138.
Train: 2018-07-31T09:46:51.945843: step 12642, loss 0.579737.
Train: 2018-07-31T09:46:52.102087: step 12643, loss 0.631763.
Train: 2018-07-31T09:46:52.273924: step 12644, loss 0.484452.
Train: 2018-07-31T09:46:52.445755: step 12645, loss 0.597084.
Train: 2018-07-31T09:46:52.617590: step 12646, loss 0.588414.
Train: 2018-07-31T09:46:52.789426: step 12647, loss 0.519097.
Train: 2018-07-31T09:46:52.961231: step 12648, loss 0.588405.
Train: 2018-07-31T09:46:53.117443: step 12649, loss 0.588396.
Train: 2018-07-31T09:46:53.289278: step 12650, loss 0.562408.
Test: 2018-07-31T09:46:53.539220: step 12650, loss 0.547923.
Train: 2018-07-31T09:46:53.695432: step 12651, loss 0.588351.
Train: 2018-07-31T09:46:53.867268: step 12652, loss 0.527854.
Train: 2018-07-31T09:46:54.039103: step 12653, loss 0.588304.
Train: 2018-07-31T09:46:54.210963: step 12654, loss 0.588278.
Train: 2018-07-31T09:46:54.382802: step 12655, loss 0.622693.
Train: 2018-07-31T09:46:54.554641: step 12656, loss 0.502253.
Train: 2018-07-31T09:46:54.726441: step 12657, loss 0.562399.
Train: 2018-07-31T09:46:54.898307: step 12658, loss 0.588125.
Train: 2018-07-31T09:46:55.070142: step 12659, loss 0.579527.
Train: 2018-07-31T09:46:55.241975: step 12660, loss 0.579502.
Test: 2018-07-31T09:46:55.491889: step 12660, loss 0.548105.
Train: 2018-07-31T09:46:55.663755: step 12661, loss 0.51972.
Train: 2018-07-31T09:46:55.835587: step 12662, loss 0.494159.
Train: 2018-07-31T09:46:56.007391: step 12663, loss 0.588012.
Train: 2018-07-31T09:46:56.163604: step 12664, loss 0.553863.
Train: 2018-07-31T09:46:56.335439: step 12665, loss 0.570943.
Train: 2018-07-31T09:46:56.491653: step 12666, loss 0.570944.
Train: 2018-07-31T09:46:56.663488: step 12667, loss 0.570944.
Train: 2018-07-31T09:46:56.835352: step 12668, loss 0.528231.
Train: 2018-07-31T09:46:57.007189: step 12669, loss 0.570948.
Train: 2018-07-31T09:46:57.179026: step 12670, loss 0.519648.
Test: 2018-07-31T09:46:57.413347: step 12670, loss 0.548063.
Train: 2018-07-31T09:46:57.585171: step 12671, loss 0.57096.
Train: 2018-07-31T09:46:57.757012: step 12672, loss 0.553831.
Train: 2018-07-31T09:46:57.928850: step 12673, loss 0.553822.
Train: 2018-07-31T09:46:58.085060: step 12674, loss 0.605327.
Train: 2018-07-31T09:46:58.256900: step 12675, loss 0.493707.
Train: 2018-07-31T09:46:58.428700: step 12676, loss 0.596794.
Train: 2018-07-31T09:46:58.600535: step 12677, loss 0.614024.
Train: 2018-07-31T09:46:58.772368: step 12678, loss 0.570999.
Train: 2018-07-31T09:46:58.928611: step 12679, loss 0.536615.
Train: 2018-07-31T09:46:59.100418: step 12680, loss 0.54521.
Test: 2018-07-31T09:46:59.350360: step 12680, loss 0.548002.
Train: 2018-07-31T09:46:59.506602: step 12681, loss 0.5624.
Train: 2018-07-31T09:46:59.678441: step 12682, loss 0.528004.
Train: 2018-07-31T09:46:59.850241: step 12683, loss 0.605432.
Train: 2018-07-31T09:47:00.022102: step 12684, loss 0.525682.
Train: 2018-07-31T09:47:00.178290: step 12685, loss 0.605458.
Train: 2018-07-31T09:47:00.365779: step 12686, loss 0.510745.
Train: 2018-07-31T09:47:00.521990: step 12687, loss 0.527939.
Train: 2018-07-31T09:47:00.693824: step 12688, loss 0.579656.
Train: 2018-07-31T09:47:00.865654: step 12689, loss 0.579672.
Train: 2018-07-31T09:47:01.037465: step 12690, loss 0.588315.
Test: 2018-07-31T09:47:01.271822: step 12690, loss 0.547941.
Train: 2018-07-31T09:47:01.443648: step 12691, loss 0.553768.
Train: 2018-07-31T09:47:01.599832: step 12692, loss 0.579674.
Train: 2018-07-31T09:47:01.771667: step 12693, loss 0.553771.
Train: 2018-07-31T09:47:01.943541: step 12694, loss 0.536513.
Train: 2018-07-31T09:47:02.115369: step 12695, loss 0.596933.
Train: 2018-07-31T09:47:02.271550: step 12696, loss 0.665947.
Train: 2018-07-31T09:47:02.443384: step 12697, loss 0.519361.
Train: 2018-07-31T09:47:02.630840: step 12698, loss 0.648359.
Train: 2018-07-31T09:47:02.787087: step 12699, loss 0.630966.
Train: 2018-07-31T09:47:02.958919: step 12700, loss 0.528254.
Test: 2018-07-31T09:47:03.193240: step 12700, loss 0.548149.
Train: 2018-07-31T09:47:03.958685: step 12701, loss 0.570918.
Train: 2018-07-31T09:47:04.114898: step 12702, loss 0.638813.
Train: 2018-07-31T09:47:04.286734: step 12703, loss 0.570878.
Train: 2018-07-31T09:47:04.458568: step 12704, loss 0.47819.
Train: 2018-07-31T09:47:04.630372: step 12705, loss 0.528795.
Train: 2018-07-31T09:47:04.802207: step 12706, loss 0.554038.
Train: 2018-07-31T09:47:04.974072: step 12707, loss 0.579254.
Train: 2018-07-31T09:47:05.145908: step 12708, loss 0.570848.
Train: 2018-07-31T09:47:05.302090: step 12709, loss 0.56245.
Train: 2018-07-31T09:47:05.473924: step 12710, loss 0.55406.
Test: 2018-07-31T09:47:05.708276: step 12710, loss 0.548399.
Train: 2018-07-31T09:47:05.880110: step 12711, loss 0.587627.
Train: 2018-07-31T09:47:06.051948: step 12712, loss 0.554068.
Train: 2018-07-31T09:47:06.223749: step 12713, loss 0.545684.
Train: 2018-07-31T09:47:06.395583: step 12714, loss 0.57923.
Train: 2018-07-31T09:47:06.567452: step 12715, loss 0.503733.
Train: 2018-07-31T09:47:06.723657: step 12716, loss 0.562448.
Train: 2018-07-31T09:47:06.895491: step 12717, loss 0.621317.
Train: 2018-07-31T09:47:07.067333: step 12718, loss 0.554031.
Train: 2018-07-31T09:47:07.239136: step 12719, loss 0.604508.
Train: 2018-07-31T09:47:07.411001: step 12720, loss 0.528805.
Test: 2018-07-31T09:47:07.645318: step 12720, loss 0.548353.
Train: 2018-07-31T09:47:07.801538: step 12721, loss 0.562442.
Train: 2018-07-31T09:47:07.973370: step 12722, loss 0.528778.
Train: 2018-07-31T09:47:08.145205: step 12723, loss 0.596134.
Train: 2018-07-31T09:47:08.317008: step 12724, loss 0.62143.
Train: 2018-07-31T09:47:08.488844: step 12725, loss 0.621389.
Train: 2018-07-31T09:47:08.660678: step 12726, loss 0.537225.
Train: 2018-07-31T09:47:08.832513: step 12727, loss 0.604438.
Train: 2018-07-31T09:47:09.004349: step 12728, loss 0.612759.
Train: 2018-07-31T09:47:09.176216: step 12729, loss 0.520652.
Train: 2018-07-31T09:47:09.348017: step 12730, loss 0.537416.
Test: 2018-07-31T09:47:09.582370: step 12730, loss 0.548497.
Train: 2018-07-31T09:47:09.754173: step 12731, loss 0.595877.
Train: 2018-07-31T09:47:09.926041: step 12732, loss 0.52912.
Train: 2018-07-31T09:47:10.097841: step 12733, loss 0.537466.
Train: 2018-07-31T09:47:10.269676: step 12734, loss 0.520755.
Train: 2018-07-31T09:47:10.441511: step 12735, loss 0.587551.
Train: 2018-07-31T09:47:10.613346: step 12736, loss 0.595934.
Train: 2018-07-31T09:47:10.785214: step 12737, loss 0.528995.
Train: 2018-07-31T09:47:10.957047: step 12738, loss 0.554086.
Train: 2018-07-31T09:47:11.128850: step 12739, loss 0.596008.
Train: 2018-07-31T09:47:11.316307: step 12740, loss 0.503709.
Test: 2018-07-31T09:47:11.550652: step 12740, loss 0.548365.
Train: 2018-07-31T09:47:11.722462: step 12741, loss 0.587664.
Train: 2018-07-31T09:47:11.878675: step 12742, loss 0.537189.
Train: 2018-07-31T09:47:12.050543: step 12743, loss 0.613015.
Train: 2018-07-31T09:47:12.222345: step 12744, loss 0.528691.
Train: 2018-07-31T09:47:12.394178: step 12745, loss 0.579318.
Train: 2018-07-31T09:47:12.566044: step 12746, loss 0.528612.
Train: 2018-07-31T09:47:12.737849: step 12747, loss 0.553954.
Train: 2018-07-31T09:47:12.894063: step 12748, loss 0.503063.
Train: 2018-07-31T09:47:13.065928: step 12749, loss 0.502898.
Train: 2018-07-31T09:47:13.237756: step 12750, loss 0.596531.
Test: 2018-07-31T09:47:13.472083: step 12750, loss 0.548073.
Train: 2018-07-31T09:47:13.643888: step 12751, loss 0.519629.
Train: 2018-07-31T09:47:13.815721: step 12752, loss 0.57098.
Train: 2018-07-31T09:47:13.987556: step 12753, loss 0.502168.
Train: 2018-07-31T09:47:14.159391: step 12754, loss 0.493322.
Train: 2018-07-31T09:47:14.315635: step 12755, loss 0.59711.
Train: 2018-07-31T09:47:14.487439: step 12756, loss 0.52761.
Train: 2018-07-31T09:47:14.674929: step 12757, loss 0.544967.
Train: 2018-07-31T09:47:14.831109: step 12758, loss 0.597509.
Train: 2018-07-31T09:47:15.018565: step 12759, loss 0.483398.
Train: 2018-07-31T09:47:15.190399: step 12760, loss 0.5713.
Test: 2018-07-31T09:47:15.424720: step 12760, loss 0.547698.
Train: 2018-07-31T09:47:15.596555: step 12761, loss 0.642071.
Train: 2018-07-31T09:47:15.768390: step 12762, loss 0.535963.
Train: 2018-07-31T09:47:15.940224: step 12763, loss 0.60682.
Train: 2018-07-31T09:47:16.112059: step 12764, loss 0.535937.
Train: 2018-07-31T09:47:16.283893: step 12765, loss 0.580255.
Train: 2018-07-31T09:47:16.440137: step 12766, loss 0.580253.
Train: 2018-07-31T09:47:16.611972: step 12767, loss 0.562525.
Train: 2018-07-31T09:47:16.783809: step 12768, loss 0.535952.
Train: 2018-07-31T09:47:16.955612: step 12769, loss 0.500544.
Train: 2018-07-31T09:47:17.127447: step 12770, loss 0.527078.
Test: 2018-07-31T09:47:17.361797: step 12770, loss 0.547672.
Train: 2018-07-31T09:47:17.533633: step 12771, loss 0.544787.
Train: 2018-07-31T09:47:17.705435: step 12772, loss 0.562542.
Train: 2018-07-31T09:47:17.877270: step 12773, loss 0.482512.
Train: 2018-07-31T09:47:18.049106: step 12774, loss 0.535831.
Train: 2018-07-31T09:47:18.205319: step 12775, loss 0.562592.
Train: 2018-07-31T09:47:18.392799: step 12776, loss 0.517856.
Train: 2018-07-31T09:47:18.549022: step 12777, loss 0.481889.
Train: 2018-07-31T09:47:18.720847: step 12778, loss 0.517663.
Train: 2018-07-31T09:47:18.892691: step 12779, loss 0.607887.
Train: 2018-07-31T09:47:19.064522: step 12780, loss 0.535581.
Test: 2018-07-31T09:47:19.298847: step 12780, loss 0.547575.
Train: 2018-07-31T09:47:19.470677: step 12781, loss 0.544624.
Train: 2018-07-31T09:47:19.642482: step 12782, loss 0.58099.
Train: 2018-07-31T09:47:19.814347: step 12783, loss 0.581032.
Train: 2018-07-31T09:47:19.986151: step 12784, loss 0.526385.
Train: 2018-07-31T09:47:20.157986: step 12785, loss 0.608443.
Train: 2018-07-31T09:47:20.329851: step 12786, loss 0.544607.
Train: 2018-07-31T09:47:20.501656: step 12787, loss 0.553722.
Train: 2018-07-31T09:47:20.673490: step 12788, loss 0.553722.
Train: 2018-07-31T09:47:20.860971: step 12789, loss 0.626613.
Train: 2018-07-31T09:47:21.032781: step 12790, loss 0.644675.
Test: 2018-07-31T09:47:21.267132: step 12790, loss 0.547577.
Train: 2018-07-31T09:47:21.438966: step 12791, loss 0.481151.
Train: 2018-07-31T09:47:21.610802: step 12792, loss 0.553689.
Train: 2018-07-31T09:47:21.782639: step 12793, loss 0.553685.
Train: 2018-07-31T09:47:21.954441: step 12794, loss 0.553676.
Train: 2018-07-31T09:47:22.126306: step 12795, loss 0.589727.
Train: 2018-07-31T09:47:22.298111: step 12796, loss 0.553668.
Train: 2018-07-31T09:47:22.454355: step 12797, loss 0.571622.
Train: 2018-07-31T09:47:22.641810: step 12798, loss 0.580547.
Train: 2018-07-31T09:47:22.813645: step 12799, loss 0.517894.
Train: 2018-07-31T09:47:22.969827: step 12800, loss 0.553655.
Test: 2018-07-31T09:47:23.204179: step 12800, loss 0.54764.
Train: 2018-07-31T09:47:23.953996: step 12801, loss 0.544743.
Train: 2018-07-31T09:47:24.125806: step 12802, loss 0.544746.
Train: 2018-07-31T09:47:24.297641: step 12803, loss 0.562561.
Train: 2018-07-31T09:47:24.485098: step 12804, loss 0.509193.
Train: 2018-07-31T09:47:24.656967: step 12805, loss 0.60702.
Train: 2018-07-31T09:47:24.828793: step 12806, loss 0.535885.
Train: 2018-07-31T09:47:25.000638: step 12807, loss 0.615829.
Train: 2018-07-31T09:47:25.172470: step 12808, loss 0.597997.
Train: 2018-07-31T09:47:25.344272: step 12809, loss 0.527122.
Train: 2018-07-31T09:47:25.516137: step 12810, loss 0.571336.
Test: 2018-07-31T09:47:25.766049: step 12810, loss 0.547717.
Train: 2018-07-31T09:47:25.937883: step 12811, loss 0.580124.
Train: 2018-07-31T09:47:26.094095: step 12812, loss 0.571285.
Train: 2018-07-31T09:47:26.265930: step 12813, loss 0.544869.
Train: 2018-07-31T09:47:26.437796: step 12814, loss 0.61509.
Train: 2018-07-31T09:47:26.609600: step 12815, loss 0.544998.
Train: 2018-07-31T09:47:26.765844: step 12816, loss 0.571153.
Train: 2018-07-31T09:47:26.937649: step 12817, loss 0.588569.
Train: 2018-07-31T09:47:27.109514: step 12818, loss 0.605902.
Train: 2018-07-31T09:47:27.281319: step 12819, loss 0.519068.
Train: 2018-07-31T09:47:27.453153: step 12820, loss 0.536504.
Test: 2018-07-31T09:47:27.687473: step 12820, loss 0.547944.
Train: 2018-07-31T09:47:27.859307: step 12821, loss 0.553771.
Train: 2018-07-31T09:47:28.031143: step 12822, loss 0.56241.
Train: 2018-07-31T09:47:28.203007: step 12823, loss 0.58824.
Train: 2018-07-31T09:47:28.374843: step 12824, loss 0.570991.
Train: 2018-07-31T09:47:28.531027: step 12825, loss 0.570976.
Train: 2018-07-31T09:47:28.702891: step 12826, loss 0.562372.
Train: 2018-07-31T09:47:28.874725: step 12827, loss 0.545249.
Train: 2018-07-31T09:47:29.046531: step 12828, loss 0.596696.
Train: 2018-07-31T09:47:29.218365: step 12829, loss 0.570941.
Train: 2018-07-31T09:47:29.374608: step 12830, loss 0.639121.
Test: 2018-07-31T09:47:29.624521: step 12830, loss 0.548189.
Train: 2018-07-31T09:47:29.780762: step 12831, loss 0.596499.
Train: 2018-07-31T09:47:29.952600: step 12832, loss 0.537157.
Train: 2018-07-31T09:47:30.124432: step 12833, loss 0.545454.
Train: 2018-07-31T09:47:30.296271: step 12834, loss 0.56254.
Train: 2018-07-31T09:47:30.468103: step 12835, loss 0.544361.
Train: 2018-07-31T09:47:30.639907: step 12836, loss 0.528702.
Train: 2018-07-31T09:47:30.811772: step 12837, loss 0.528747.
Train: 2018-07-31T09:47:30.983576: step 12838, loss 0.554009.
Train: 2018-07-31T09:47:31.155443: step 12839, loss 0.537118.
Train: 2018-07-31T09:47:31.327246: step 12840, loss 0.528583.
Test: 2018-07-31T09:47:31.561596: step 12840, loss 0.548213.
Train: 2018-07-31T09:47:31.733431: step 12841, loss 0.596343.
Train: 2018-07-31T09:47:31.920860: step 12842, loss 0.596367.
Train: 2018-07-31T09:47:32.092716: step 12843, loss 0.485972.
Train: 2018-07-31T09:47:32.264526: step 12844, loss 0.664506.
Train: 2018-07-31T09:47:32.420773: step 12845, loss 0.553914.
Train: 2018-07-31T09:47:32.592573: step 12846, loss 0.511389.
Train: 2018-07-31T09:47:32.764439: step 12847, loss 0.51981.
Train: 2018-07-31T09:47:32.936274: step 12848, loss 0.570924.
Train: 2018-07-31T09:47:33.108078: step 12849, loss 0.553844.
Train: 2018-07-31T09:47:33.279945: step 12850, loss 0.579535.
Test: 2018-07-31T09:47:33.514259: step 12850, loss 0.548061.
Train: 2018-07-31T09:47:33.701689: step 12851, loss 0.528188.
Train: 2018-07-31T09:47:33.873524: step 12852, loss 0.562519.
Train: 2018-07-31T09:47:34.045359: step 12853, loss 0.536643.
Train: 2018-07-31T09:47:34.217218: step 12854, loss 0.579679.
Train: 2018-07-31T09:47:34.389059: step 12855, loss 0.553771.
Train: 2018-07-31T09:47:34.560864: step 12856, loss 0.640058.
Train: 2018-07-31T09:47:34.732699: step 12857, loss 0.588295.
Train: 2018-07-31T09:47:34.904565: step 12858, loss 0.65727.
Train: 2018-07-31T09:47:35.076398: step 12859, loss 0.579607.
Train: 2018-07-31T09:47:35.248232: step 12860, loss 0.545264.
Test: 2018-07-31T09:47:35.482523: step 12860, loss 0.548073.
Train: 2018-07-31T09:47:35.654358: step 12861, loss 0.528208.
Train: 2018-07-31T09:47:35.826223: step 12862, loss 0.588014.
Train: 2018-07-31T09:47:35.998027: step 12863, loss 0.562393.
Train: 2018-07-31T09:47:36.169862: step 12864, loss 0.536812.
Train: 2018-07-31T09:47:36.341697: step 12865, loss 0.460265.
Train: 2018-07-31T09:47:36.513561: step 12866, loss 0.605021.
Train: 2018-07-31T09:47:36.701017: step 12867, loss 0.511206.
Train: 2018-07-31T09:47:36.872852: step 12868, loss 0.570945.
Train: 2018-07-31T09:47:37.029068: step 12869, loss 0.588073.
Train: 2018-07-31T09:47:37.200895: step 12870, loss 0.579504.
Test: 2018-07-31T09:47:37.435225: step 12870, loss 0.548066.
Train: 2018-07-31T09:47:37.622646: step 12871, loss 0.511065.
Train: 2018-07-31T09:47:37.794515: step 12872, loss 0.588083.
Train: 2018-07-31T09:47:37.950725: step 12873, loss 0.579554.
Train: 2018-07-31T09:47:38.122530: step 12874, loss 0.562403.
Train: 2018-07-31T09:47:38.294364: step 12875, loss 0.579563.
Train: 2018-07-31T09:47:38.466198: step 12876, loss 0.6653.
Train: 2018-07-31T09:47:38.638063: step 12877, loss 0.528213.
Train: 2018-07-31T09:47:38.809894: step 12878, loss 0.553882.
Train: 2018-07-31T09:47:38.981733: step 12879, loss 0.570932.
Train: 2018-07-31T09:47:39.153538: step 12880, loss 0.587957.
Test: 2018-07-31T09:47:39.387884: step 12880, loss 0.548145.
Train: 2018-07-31T09:47:39.559693: step 12881, loss 0.647541.
Train: 2018-07-31T09:47:39.731557: step 12882, loss 0.536973.
Train: 2018-07-31T09:47:39.903386: step 12883, loss 0.579343.
Train: 2018-07-31T09:47:40.090850: step 12884, loss 0.528593.
Train: 2018-07-31T09:47:40.247062: step 12885, loss 0.537121.
Train: 2018-07-31T09:47:40.418900: step 12886, loss 0.503414.
Train: 2018-07-31T09:47:40.590731: step 12887, loss 0.553968.
Train: 2018-07-31T09:47:40.762566: step 12888, loss 0.570904.
Train: 2018-07-31T09:47:40.934372: step 12889, loss 0.613295.
Train: 2018-07-31T09:47:41.106206: step 12890, loss 0.630164.
Test: 2018-07-31T09:47:41.356149: step 12890, loss 0.548269.
Train: 2018-07-31T09:47:41.527982: step 12891, loss 0.537026.
Train: 2018-07-31T09:47:41.699846: step 12892, loss 0.461053.
Train: 2018-07-31T09:47:41.871652: step 12893, loss 0.494711.
Train: 2018-07-31T09:47:42.043520: step 12894, loss 0.62199.
Train: 2018-07-31T09:47:42.215351: step 12895, loss 0.519808.
Train: 2018-07-31T09:47:42.387156: step 12896, loss 0.553727.
Train: 2018-07-31T09:47:42.558990: step 12897, loss 0.536444.
Train: 2018-07-31T09:47:42.730825: step 12898, loss 0.545624.
Train: 2018-07-31T09:47:42.902660: step 12899, loss 0.640592.
Train: 2018-07-31T09:47:43.074495: step 12900, loss 0.562456.
Test: 2018-07-31T09:47:43.308815: step 12900, loss 0.547945.
Train: 2018-07-31T09:47:44.089906: step 12901, loss 0.502013.
Train: 2018-07-31T09:47:44.261747: step 12902, loss 0.50241.
Train: 2018-07-31T09:47:44.433577: step 12903, loss 0.597163.
Train: 2018-07-31T09:47:44.589795: step 12904, loss 0.519355.
Train: 2018-07-31T09:47:44.761629: step 12905, loss 0.614798.
Train: 2018-07-31T09:47:44.949056: step 12906, loss 0.527583.
Train: 2018-07-31T09:47:45.120921: step 12907, loss 0.527735.
Train: 2018-07-31T09:47:45.292759: step 12908, loss 0.588517.
Train: 2018-07-31T09:47:45.448964: step 12909, loss 0.60596.
Train: 2018-07-31T09:47:45.636395: step 12910, loss 0.501578.
Test: 2018-07-31T09:47:45.870716: step 12910, loss 0.547851.
Train: 2018-07-31T09:47:46.042550: step 12911, loss 0.605922.
Train: 2018-07-31T09:47:46.214415: step 12912, loss 0.536292.
Train: 2018-07-31T09:47:46.386219: step 12913, loss 0.597239.
Train: 2018-07-31T09:47:46.573705: step 12914, loss 0.562406.
Train: 2018-07-31T09:47:46.745540: step 12915, loss 0.623291.
Train: 2018-07-31T09:47:46.917380: step 12916, loss 0.527632.
Train: 2018-07-31T09:47:47.089180: step 12917, loss 0.579784.
Train: 2018-07-31T09:47:47.261014: step 12918, loss 0.501814.
Train: 2018-07-31T09:47:47.432879: step 12919, loss 0.51037.
Train: 2018-07-31T09:47:47.620336: step 12920, loss 0.544957.
Test: 2018-07-31T09:47:47.854656: step 12920, loss 0.547868.
Train: 2018-07-31T09:47:48.026460: step 12921, loss 0.519105.
Train: 2018-07-31T09:47:48.198295: step 12922, loss 0.605889.
Train: 2018-07-31T09:47:48.370130: step 12923, loss 0.579824.
Train: 2018-07-31T09:47:48.541998: step 12924, loss 0.545106.
Train: 2018-07-31T09:47:48.713824: step 12925, loss 0.571091.
Train: 2018-07-31T09:47:48.885664: step 12926, loss 0.588624.
Train: 2018-07-31T09:47:49.057468: step 12927, loss 0.588685.
Train: 2018-07-31T09:47:49.229334: step 12928, loss 0.54515.
Train: 2018-07-31T09:47:49.401139: step 12929, loss 0.527535.
Train: 2018-07-31T09:47:49.572973: step 12930, loss 0.501414.
Test: 2018-07-31T09:47:49.807324: step 12930, loss 0.547825.
Train: 2018-07-31T09:47:49.994780: step 12931, loss 0.52761.
Train: 2018-07-31T09:47:50.150995: step 12932, loss 0.536166.
Train: 2018-07-31T09:47:50.338419: step 12933, loss 0.632612.
Train: 2018-07-31T09:47:50.510278: step 12934, loss 0.457168.
Train: 2018-07-31T09:47:50.682089: step 12935, loss 0.65921.
Train: 2018-07-31T09:47:50.838332: step 12936, loss 0.553677.
Train: 2018-07-31T09:47:51.010166: step 12937, loss 0.562466.
Train: 2018-07-31T09:47:51.181970: step 12938, loss 0.571259.
Train: 2018-07-31T09:47:51.353836: step 12939, loss 0.597649.
Train: 2018-07-31T09:47:51.525671: step 12940, loss 0.509757.
Test: 2018-07-31T09:47:51.759992: step 12940, loss 0.547756.
Train: 2018-07-31T09:47:51.931797: step 12941, loss 0.544938.
Train: 2018-07-31T09:47:52.103631: step 12942, loss 0.536104.
Train: 2018-07-31T09:47:52.275490: step 12943, loss 0.588858.
Train: 2018-07-31T09:47:52.462922: step 12944, loss 0.571284.
Train: 2018-07-31T09:47:52.619134: step 12945, loss 0.527321.
Train: 2018-07-31T09:47:52.790969: step 12946, loss 0.527333.
Train: 2018-07-31T09:47:52.962803: step 12947, loss 0.571249.
Train: 2018-07-31T09:47:53.150290: step 12948, loss 0.63287.
Train: 2018-07-31T09:47:53.322095: step 12949, loss 0.615197.
Train: 2018-07-31T09:47:53.478337: step 12950, loss 0.553682.
Test: 2018-07-31T09:47:53.728280: step 12950, loss 0.547785.
Train: 2018-07-31T09:47:53.900119: step 12951, loss 0.553703.
Train: 2018-07-31T09:47:54.071949: step 12952, loss 0.55369.
Train: 2018-07-31T09:47:54.243785: step 12953, loss 0.632202.
Train: 2018-07-31T09:47:54.415623: step 12954, loss 0.640727.
Train: 2018-07-31T09:47:54.587424: step 12955, loss 0.519118.
Train: 2018-07-31T09:47:54.759258: step 12956, loss 0.502083.
Train: 2018-07-31T09:47:54.915505: step 12957, loss 0.562376.
Train: 2018-07-31T09:47:55.087307: step 12958, loss 0.493513.
Train: 2018-07-31T09:47:55.259171: step 12959, loss 0.614232.
Train: 2018-07-31T09:47:55.431007: step 12960, loss 0.66554.
Test: 2018-07-31T09:47:55.665331: step 12960, loss 0.548046.
Train: 2018-07-31T09:47:55.852782: step 12961, loss 0.630843.
Train: 2018-07-31T09:47:56.024586: step 12962, loss 0.536838.
Train: 2018-07-31T09:47:56.196423: step 12963, loss 0.536816.
Train: 2018-07-31T09:47:56.352667: step 12964, loss 0.613493.
Train: 2018-07-31T09:47:56.524470: step 12965, loss 0.520161.
Train: 2018-07-31T09:47:56.696329: step 12966, loss 0.563009.
Train: 2018-07-31T09:47:56.868170: step 12967, loss 0.570747.
Train: 2018-07-31T09:47:57.040008: step 12968, loss 0.51991.
Train: 2018-07-31T09:47:57.211810: step 12969, loss 0.528787.
Train: 2018-07-31T09:47:57.368023: step 12970, loss 0.537005.
Test: 2018-07-31T09:47:57.617995: step 12970, loss 0.548366.
Train: 2018-07-31T09:47:57.789799: step 12971, loss 0.562151.
Train: 2018-07-31T09:47:57.961633: step 12972, loss 0.60464.
Train: 2018-07-31T09:47:58.117880: step 12973, loss 0.596255.
Train: 2018-07-31T09:47:58.289715: step 12974, loss 0.512029.
Train: 2018-07-31T09:47:58.461547: step 12975, loss 0.528452.
Train: 2018-07-31T09:47:58.633376: step 12976, loss 0.621271.
Train: 2018-07-31T09:47:58.805187: step 12977, loss 0.536568.
Train: 2018-07-31T09:47:58.977037: step 12978, loss 0.629448.
Train: 2018-07-31T09:47:59.133264: step 12979, loss 0.544539.
Train: 2018-07-31T09:47:59.305101: step 12980, loss 0.5302.
Test: 2018-07-31T09:47:59.555041: step 12980, loss 0.548534.
Train: 2018-07-31T09:47:59.726845: step 12981, loss 0.612955.
Train: 2018-07-31T09:47:59.898680: step 12982, loss 0.570473.
Train: 2018-07-31T09:48:00.070545: step 12983, loss 0.530197.
Train: 2018-07-31T09:48:00.242379: step 12984, loss 0.553099.
Train: 2018-07-31T09:48:00.414218: step 12985, loss 0.528233.
Train: 2018-07-31T09:48:00.570428: step 12986, loss 0.599212.
Train: 2018-07-31T09:48:00.742233: step 12987, loss 0.59524.
Train: 2018-07-31T09:48:00.914106: step 12988, loss 0.569611.
Train: 2018-07-31T09:48:01.070280: step 12989, loss 0.56914.
Train: 2018-07-31T09:48:01.242145: step 12990, loss 0.581702.
Test: 2018-07-31T09:48:01.476435: step 12990, loss 0.54872.
Train: 2018-07-31T09:48:01.648295: step 12991, loss 0.56268.
Train: 2018-07-31T09:48:01.804484: step 12992, loss 0.563602.
Train: 2018-07-31T09:48:01.976349: step 12993, loss 0.594102.
Train: 2018-07-31T09:48:02.148152: step 12994, loss 0.613715.
Train: 2018-07-31T09:48:02.320013: step 12995, loss 0.621613.
Train: 2018-07-31T09:48:02.476226: step 12996, loss 0.531442.
Train: 2018-07-31T09:48:02.663657: step 12997, loss 0.671308.
Train: 2018-07-31T09:48:02.819901: step 12998, loss 0.432169.
Train: 2018-07-31T09:48:02.991706: step 12999, loss 0.522231.
Train: 2018-07-31T09:48:03.163574: step 13000, loss 0.570988.
Test: 2018-07-31T09:48:03.397862: step 13000, loss 0.54843.
Train: 2018-07-31T09:48:04.132094: step 13001, loss 0.476876.
Train: 2018-07-31T09:48:04.303930: step 13002, loss 0.562337.
Train: 2018-07-31T09:48:04.460142: step 13003, loss 0.553576.
Train: 2018-07-31T09:48:04.647600: step 13004, loss 0.61356.
Train: 2018-07-31T09:48:04.803781: step 13005, loss 0.468212.
Train: 2018-07-31T09:48:04.975646: step 13006, loss 0.665407.
Train: 2018-07-31T09:48:05.147452: step 13007, loss 0.622819.
Train: 2018-07-31T09:48:05.319316: step 13008, loss 0.631187.
Train: 2018-07-31T09:48:05.491153: step 13009, loss 0.53655.
Train: 2018-07-31T09:48:05.662955: step 13010, loss 0.49375.
Test: 2018-07-31T09:48:05.897277: step 13010, loss 0.548019.
Train: 2018-07-31T09:48:06.069140: step 13011, loss 0.613824.
Train: 2018-07-31T09:48:06.225353: step 13012, loss 0.631003.
Train: 2018-07-31T09:48:06.397158: step 13013, loss 0.605299.
Train: 2018-07-31T09:48:06.569023: step 13014, loss 0.579421.
Train: 2018-07-31T09:48:06.725237: step 13015, loss 0.528166.
Train: 2018-07-31T09:48:06.897041: step 13016, loss 0.485958.
Train: 2018-07-31T09:48:07.068907: step 13017, loss 0.52019.
Train: 2018-07-31T09:48:07.225119: step 13018, loss 0.562386.
Train: 2018-07-31T09:48:07.396957: step 13019, loss 0.579522.
Train: 2018-07-31T09:48:07.568759: step 13020, loss 0.596315.
Test: 2018-07-31T09:48:07.803079: step 13020, loss 0.548144.
Train: 2018-07-31T09:48:07.974938: step 13021, loss 0.587708.
Train: 2018-07-31T09:48:08.131128: step 13022, loss 0.562502.
Train: 2018-07-31T09:48:08.302994: step 13023, loss 0.545179.
Train: 2018-07-31T09:48:08.474827: step 13024, loss 0.477436.
Train: 2018-07-31T09:48:08.646664: step 13025, loss 0.519825.
Train: 2018-07-31T09:48:08.818499: step 13026, loss 0.579571.
Train: 2018-07-31T09:48:08.974710: step 13027, loss 0.579509.
Train: 2018-07-31T09:48:09.146514: step 13028, loss 0.562574.
Train: 2018-07-31T09:48:09.318348: step 13029, loss 0.485235.
Train: 2018-07-31T09:48:09.474593: step 13030, loss 0.614054.
Test: 2018-07-31T09:48:09.708914: step 13030, loss 0.547976.
Train: 2018-07-31T09:48:09.880717: step 13031, loss 0.459155.
Train: 2018-07-31T09:48:10.052582: step 13032, loss 0.562368.
Train: 2018-07-31T09:48:10.224419: step 13033, loss 0.562453.
Train: 2018-07-31T09:48:10.396252: step 13034, loss 0.571105.
Train: 2018-07-31T09:48:10.568086: step 13035, loss 0.553758.
Train: 2018-07-31T09:48:10.724300: step 13036, loss 0.641033.
Train: 2018-07-31T09:48:10.896134: step 13037, loss 0.536257.
Train: 2018-07-31T09:48:11.067969: step 13038, loss 0.527443.
Train: 2018-07-31T09:48:11.224153: step 13039, loss 0.579944.
Train: 2018-07-31T09:48:11.395988: step 13040, loss 0.641375.
Test: 2018-07-31T09:48:11.630309: step 13040, loss 0.547788.
Train: 2018-07-31T09:48:11.786554: step 13041, loss 0.527401.
Train: 2018-07-31T09:48:11.958386: step 13042, loss 0.536188.
Train: 2018-07-31T09:48:12.114600: step 13043, loss 0.57116.
Train: 2018-07-31T09:48:12.286403: step 13044, loss 0.509919.
Train: 2018-07-31T09:48:12.458270: step 13045, loss 0.527426.
Train: 2018-07-31T09:48:12.614452: step 13046, loss 0.597445.
Train: 2018-07-31T09:48:12.786287: step 13047, loss 0.562368.
Train: 2018-07-31T09:48:12.942527: step 13048, loss 0.632713.
Train: 2018-07-31T09:48:13.114359: step 13049, loss 0.588758.
Train: 2018-07-31T09:48:13.270581: step 13050, loss 0.54483.
Test: 2018-07-31T09:48:13.504869: step 13050, loss 0.547796.
Train: 2018-07-31T09:48:13.676704: step 13051, loss 0.579969.
Train: 2018-07-31T09:48:13.848568: step 13052, loss 0.606105.
Train: 2018-07-31T09:48:14.020397: step 13053, loss 0.623379.
Train: 2018-07-31T09:48:14.192239: step 13054, loss 0.553722.
Train: 2018-07-31T09:48:14.348422: step 13055, loss 0.527702.
Train: 2018-07-31T09:48:14.520288: step 13056, loss 0.605578.
Train: 2018-07-31T09:48:14.676470: step 13057, loss 0.596818.
Train: 2018-07-31T09:48:14.848304: step 13058, loss 0.596754.
Train: 2018-07-31T09:48:15.004548: step 13059, loss 0.511096.
Train: 2018-07-31T09:48:15.176352: step 13060, loss 0.502689.
Test: 2018-07-31T09:48:15.410707: step 13060, loss 0.54812.
Train: 2018-07-31T09:48:15.582532: step 13061, loss 0.494145.
Train: 2018-07-31T09:48:15.738751: step 13062, loss 0.596634.
Train: 2018-07-31T09:48:15.910589: step 13063, loss 0.579446.
Train: 2018-07-31T09:48:16.066793: step 13064, loss 0.545298.
Train: 2018-07-31T09:48:16.238634: step 13065, loss 0.528277.
Train: 2018-07-31T09:48:16.394849: step 13066, loss 0.545314.
Train: 2018-07-31T09:48:16.566688: step 13067, loss 0.451246.
Train: 2018-07-31T09:48:16.738518: step 13068, loss 0.528057.
Train: 2018-07-31T09:48:16.894724: step 13069, loss 0.579733.
Train: 2018-07-31T09:48:17.066535: step 13070, loss 0.588416.
Test: 2018-07-31T09:48:17.300885: step 13070, loss 0.547907.
Train: 2018-07-31T09:48:17.472690: step 13071, loss 0.588461.
Train: 2018-07-31T09:48:17.628933: step 13072, loss 0.579875.
Train: 2018-07-31T09:48:17.785116: step 13073, loss 0.501817.
Train: 2018-07-31T09:48:17.941329: step 13074, loss 0.449549.
Train: 2018-07-31T09:48:18.113164: step 13075, loss 0.553854.
Train: 2018-07-31T09:48:18.269411: step 13076, loss 0.658819.
Train: 2018-07-31T09:48:18.425591: step 13077, loss 0.606289.
Train: 2018-07-31T09:48:18.597457: step 13078, loss 0.501228.
Train: 2018-07-31T09:48:18.753640: step 13079, loss 0.536176.
Train: 2018-07-31T09:48:18.909886: step 13080, loss 0.518634.
Test: 2018-07-31T09:48:19.159828: step 13080, loss 0.547752.
Train: 2018-07-31T09:48:19.316007: step 13081, loss 0.51855.
Train: 2018-07-31T09:48:19.472252: step 13082, loss 0.553669.
Train: 2018-07-31T09:48:19.644084: step 13083, loss 0.571312.
Train: 2018-07-31T09:48:19.815922: step 13084, loss 0.633232.
Train: 2018-07-31T09:48:19.972135: step 13085, loss 0.580241.
Train: 2018-07-31T09:48:20.143939: step 13086, loss 0.606753.
Train: 2018-07-31T09:48:20.300153: step 13087, loss 0.50949.
Train: 2018-07-31T09:48:20.456365: step 13088, loss 0.562436.
Train: 2018-07-31T09:48:20.628201: step 13089, loss 0.544811.
Train: 2018-07-31T09:48:20.784453: step 13090, loss 0.58902.
Test: 2018-07-31T09:48:21.030275: step 13090, loss 0.547709.
Train: 2018-07-31T09:48:21.186489: step 13091, loss 0.544789.
Train: 2018-07-31T09:48:21.373971: step 13092, loss 0.571384.
Train: 2018-07-31T09:48:21.530189: step 13093, loss 0.544805.
Train: 2018-07-31T09:48:21.717643: step 13094, loss 0.633083.
Train: 2018-07-31T09:48:21.889449: step 13095, loss 0.544879.
Train: 2018-07-31T09:48:22.045663: step 13096, loss 0.544889.
Train: 2018-07-31T09:48:22.217498: step 13097, loss 0.500963.
Train: 2018-07-31T09:48:22.373711: step 13098, loss 0.536174.
Train: 2018-07-31T09:48:22.545577: step 13099, loss 0.544949.
Train: 2018-07-31T09:48:22.701791: step 13100, loss 0.562415.
Test: 2018-07-31T09:48:22.936079: step 13100, loss 0.54774.
Train: 2018-07-31T09:48:23.670284: step 13101, loss 0.544801.
Train: 2018-07-31T09:48:23.857739: step 13102, loss 0.659273.
Train: 2018-07-31T09:48:24.029604: step 13103, loss 0.553778.
Train: 2018-07-31T09:48:24.201408: step 13104, loss 0.580013.
Train: 2018-07-31T09:48:24.357655: step 13105, loss 0.545008.
Train: 2018-07-31T09:48:24.513865: step 13106, loss 0.544918.
Train: 2018-07-31T09:48:24.685700: step 13107, loss 0.588615.
Train: 2018-07-31T09:48:24.857538: step 13108, loss 0.562362.
Train: 2018-07-31T09:48:25.013719: step 13109, loss 0.579878.
Train: 2018-07-31T09:48:25.185583: step 13110, loss 0.536278.
Test: 2018-07-31T09:48:25.419904: step 13110, loss 0.54784.
Train: 2018-07-31T09:48:25.591741: step 13111, loss 0.588511.
Train: 2018-07-31T09:48:25.747921: step 13112, loss 0.484193.
Train: 2018-07-31T09:48:25.904166: step 13113, loss 0.536377.
Train: 2018-07-31T09:48:26.076001: step 13114, loss 0.597263.
Train: 2018-07-31T09:48:26.232216: step 13115, loss 0.484142.
Train: 2018-07-31T09:48:26.404046: step 13116, loss 0.571113.
Train: 2018-07-31T09:48:26.560262: step 13117, loss 0.501357.
Train: 2018-07-31T09:48:26.716477: step 13118, loss 0.527512.
Train: 2018-07-31T09:48:26.888304: step 13119, loss 0.518672.
Train: 2018-07-31T09:48:27.044494: step 13120, loss 0.544998.
Test: 2018-07-31T09:48:27.278848: step 13120, loss 0.547729.
Train: 2018-07-31T09:48:27.450648: step 13121, loss 0.518494.
Train: 2018-07-31T09:48:27.606861: step 13122, loss 0.677328.
Train: 2018-07-31T09:48:27.763075: step 13123, loss 0.544757.
Train: 2018-07-31T09:48:27.919288: step 13124, loss 0.562474.
Train: 2018-07-31T09:48:28.075501: step 13125, loss 0.606735.
Train: 2018-07-31T09:48:28.247336: step 13126, loss 0.580199.
Train: 2018-07-31T09:48:28.403549: step 13127, loss 0.527221.
Train: 2018-07-31T09:48:28.559793: step 13128, loss 0.615494.
Train: 2018-07-31T09:48:28.716010: step 13129, loss 0.62421.
Train: 2018-07-31T09:48:28.887842: step 13130, loss 0.553671.
Test: 2018-07-31T09:48:29.122162: step 13130, loss 0.547758.
Train: 2018-07-31T09:48:29.293996: step 13131, loss 0.553653.
Train: 2018-07-31T09:48:29.450210: step 13132, loss 0.544888.
Train: 2018-07-31T09:48:29.606393: step 13133, loss 0.614924.
Train: 2018-07-31T09:48:29.762607: step 13134, loss 0.562412.
Train: 2018-07-31T09:48:29.934471: step 13135, loss 0.614665.
Train: 2018-07-31T09:48:30.106309: step 13136, loss 0.571065.
Train: 2018-07-31T09:48:30.246898: step 13137, loss 0.543931.
Train: 2018-07-31T09:48:30.418727: step 13138, loss 0.510623.
Train: 2018-07-31T09:48:30.574947: step 13139, loss 0.579641.
Train: 2018-07-31T09:48:30.731163: step 13140, loss 0.493534.
Test: 2018-07-31T09:48:30.965451: step 13140, loss 0.547977.
Train: 2018-07-31T09:48:31.137315: step 13141, loss 0.579602.
Train: 2018-07-31T09:48:31.293524: step 13142, loss 0.571025.
Train: 2018-07-31T09:48:31.465333: step 13143, loss 0.614019.
Train: 2018-07-31T09:48:31.605957: step 13144, loss 0.545223.
Train: 2018-07-31T09:48:31.777790: step 13145, loss 0.613903.
Train: 2018-07-31T09:48:31.934003: step 13146, loss 0.613796.
Train: 2018-07-31T09:48:32.090186: step 13147, loss 0.588034.
Train: 2018-07-31T09:48:32.246430: step 13148, loss 0.630557.
Train: 2018-07-31T09:48:32.402613: step 13149, loss 0.587867.
Train: 2018-07-31T09:48:32.558860: step 13150, loss 0.494824.
Test: 2018-07-31T09:48:32.808769: step 13150, loss 0.548307.
Train: 2018-07-31T09:48:32.965014: step 13151, loss 0.469668.
Train: 2018-07-31T09:48:33.121194: step 13152, loss 0.621445.
Train: 2018-07-31T09:48:33.293029: step 13153, loss 0.495054.
Train: 2018-07-31T09:48:33.449273: step 13154, loss 0.537157.
Train: 2018-07-31T09:48:33.605456: step 13155, loss 0.604596.
Train: 2018-07-31T09:48:33.761704: step 13156, loss 0.629895.
Train: 2018-07-31T09:48:33.917919: step 13157, loss 0.579279.
Train: 2018-07-31T09:48:34.074097: step 13158, loss 0.562439.
Train: 2018-07-31T09:48:34.245931: step 13159, loss 0.57925.
Train: 2018-07-31T09:48:34.402175: step 13160, loss 0.562449.
Test: 2018-07-31T09:48:34.636496: step 13160, loss 0.548415.
Train: 2018-07-31T09:48:34.792708: step 13161, loss 0.545682.
Train: 2018-07-31T09:48:34.948922: step 13162, loss 0.604359.
Train: 2018-07-31T09:48:35.105106: step 13163, loss 0.621052.
Train: 2018-07-31T09:48:35.261318: step 13164, loss 0.579176.
Train: 2018-07-31T09:48:35.433183: step 13165, loss 0.579155.
Train: 2018-07-31T09:48:35.589400: step 13166, loss 0.504294.
Train: 2018-07-31T09:48:35.745606: step 13167, loss 0.579138.
Train: 2018-07-31T09:48:35.901793: step 13168, loss 0.595728.
Train: 2018-07-31T09:48:36.058007: step 13169, loss 0.579108.
Train: 2018-07-31T09:48:36.229876: step 13170, loss 0.587384.
Test: 2018-07-31T09:48:36.464190: step 13170, loss 0.548691.
Train: 2018-07-31T09:48:36.620405: step 13171, loss 0.512917.
Train: 2018-07-31T09:48:36.776619: step 13172, loss 0.546015.
Train: 2018-07-31T09:48:36.932803: step 13173, loss 0.570812.
Train: 2018-07-31T09:48:37.089043: step 13174, loss 0.537716.
Train: 2018-07-31T09:48:37.260876: step 13175, loss 0.496273.
Train: 2018-07-31T09:48:37.417063: step 13176, loss 0.554209.
Train: 2018-07-31T09:48:37.573278: step 13177, loss 0.545854.
Train: 2018-07-31T09:48:37.745142: step 13178, loss 0.570828.
Train: 2018-07-31T09:48:37.901325: step 13179, loss 0.537371.
Train: 2018-07-31T09:48:38.057540: step 13180, loss 0.545677.
Test: 2018-07-31T09:48:38.307482: step 13180, loss 0.548347.
Train: 2018-07-31T09:48:38.463694: step 13181, loss 0.520369.
Train: 2018-07-31T09:48:38.619937: step 13182, loss 0.537087.
Train: 2018-07-31T09:48:38.776150: step 13183, loss 0.596315.
Train: 2018-07-31T09:48:38.932333: step 13184, loss 0.562401.
Train: 2018-07-31T09:48:39.088580: step 13185, loss 0.494228.
Train: 2018-07-31T09:48:39.244792: step 13186, loss 0.613695.
Train: 2018-07-31T09:48:39.416595: step 13187, loss 0.588112.
Train: 2018-07-31T09:48:39.572839: step 13188, loss 0.545231.
Train: 2018-07-31T09:48:39.729055: step 13189, loss 0.59681.
Train: 2018-07-31T09:48:39.885266: step 13190, loss 0.562419.
Test: 2018-07-31T09:48:40.135179: step 13190, loss 0.547955.
Train: 2018-07-31T09:48:40.322634: step 13191, loss 0.553775.
Train: 2018-07-31T09:48:40.478847: step 13192, loss 0.501984.
Train: 2018-07-31T09:48:40.619439: step 13193, loss 0.527809.
Train: 2018-07-31T09:48:40.791274: step 13194, loss 0.501581.
Train: 2018-07-31T09:48:40.947520: step 13195, loss 0.509751.
Train: 2018-07-31T09:48:41.103733: step 13196, loss 0.622804.
Train: 2018-07-31T09:48:41.259913: step 13197, loss 0.570005.
Train: 2018-07-31T09:48:41.416128: step 13198, loss 0.542687.
Train: 2018-07-31T09:48:41.572340: step 13199, loss 0.561438.
Train: 2018-07-31T09:48:41.728554: step 13200, loss 0.570488.
Test: 2018-07-31T09:48:41.978497: step 13200, loss 0.547728.
Train: 2018-07-31T09:48:42.712732: step 13201, loss 0.586261.
Train: 2018-07-31T09:48:42.884566: step 13202, loss 0.544291.
Train: 2018-07-31T09:48:43.040747: step 13203, loss 0.553578.
Train: 2018-07-31T09:48:43.196993: step 13204, loss 0.641416.
Train: 2018-07-31T09:48:43.353206: step 13205, loss 0.517176.
Train: 2018-07-31T09:48:43.525039: step 13206, loss 0.610794.
Train: 2018-07-31T09:48:43.681247: step 13207, loss 0.554594.
Train: 2018-07-31T09:48:43.837435: step 13208, loss 0.55508.
Train: 2018-07-31T09:48:43.993679: step 13209, loss 0.500677.
Train: 2018-07-31T09:48:44.149861: step 13210, loss 0.580711.
Test: 2018-07-31T09:48:44.384184: step 13210, loss 0.547761.
Train: 2018-07-31T09:48:44.540420: step 13211, loss 0.509837.
Train: 2018-07-31T09:48:44.696610: step 13212, loss 0.597391.
Train: 2018-07-31T09:48:44.868444: step 13213, loss 0.588252.
Train: 2018-07-31T09:48:45.024660: step 13214, loss 0.545289.
Train: 2018-07-31T09:48:45.180901: step 13215, loss 0.588266.
Train: 2018-07-31T09:48:45.337085: step 13216, loss 0.579606.
Train: 2018-07-31T09:48:45.493297: step 13217, loss 0.588045.
Train: 2018-07-31T09:48:45.649541: step 13218, loss 0.596601.
Train: 2018-07-31T09:48:45.805748: step 13219, loss 0.528495.
Train: 2018-07-31T09:48:45.961938: step 13220, loss 0.528385.
Test: 2018-07-31T09:48:46.196293: step 13220, loss 0.548166.
Train: 2018-07-31T09:48:46.352504: step 13221, loss 0.570776.
Train: 2018-07-31T09:48:46.508714: step 13222, loss 0.587988.
Train: 2018-07-31T09:48:46.664928: step 13223, loss 0.452166.
Train: 2018-07-31T09:48:46.821136: step 13224, loss 0.54541.
Train: 2018-07-31T09:48:46.992946: step 13225, loss 0.545388.
Train: 2018-07-31T09:48:47.149190: step 13226, loss 0.536735.
Train: 2018-07-31T09:48:47.305404: step 13227, loss 0.51959.
Train: 2018-07-31T09:48:47.477241: step 13228, loss 0.639759.
Train: 2018-07-31T09:48:47.633451: step 13229, loss 0.545157.
Train: 2018-07-31T09:48:47.789636: step 13230, loss 0.614169.
Test: 2018-07-31T09:48:48.023985: step 13230, loss 0.547958.
Train: 2018-07-31T09:48:48.180169: step 13231, loss 0.519224.
Train: 2018-07-31T09:48:48.336382: step 13232, loss 0.545232.
Train: 2018-07-31T09:48:48.492595: step 13233, loss 0.562389.
Train: 2018-07-31T09:48:48.648838: step 13234, loss 0.545102.
Train: 2018-07-31T09:48:48.820644: step 13235, loss 0.553852.
Train: 2018-07-31T09:48:48.976890: step 13236, loss 0.527663.
Train: 2018-07-31T09:48:49.148721: step 13237, loss 0.597219.
Train: 2018-07-31T09:48:49.304935: step 13238, loss 0.544871.
Train: 2018-07-31T09:48:49.461119: step 13239, loss 0.54488.
Train: 2018-07-31T09:48:49.632953: step 13240, loss 0.614729.
Test: 2018-07-31T09:48:49.867275: step 13240, loss 0.54783.
Train: 2018-07-31T09:48:50.023518: step 13241, loss 0.580079.
Train: 2018-07-31T09:48:50.179730: step 13242, loss 0.545163.
Train: 2018-07-31T09:48:50.335913: step 13243, loss 0.579857.
Train: 2018-07-31T09:48:50.492151: step 13244, loss 0.571279.
Train: 2018-07-31T09:48:50.648341: step 13245, loss 0.553759.
Train: 2018-07-31T09:48:50.820200: step 13246, loss 0.588673.
Train: 2018-07-31T09:48:50.976390: step 13247, loss 0.519229.
Train: 2018-07-31T09:48:51.132602: step 13248, loss 0.605759.
Train: 2018-07-31T09:48:51.288815: step 13249, loss 0.562359.
Train: 2018-07-31T09:48:51.445060: step 13250, loss 0.562477.
Test: 2018-07-31T09:48:51.679349: step 13250, loss 0.547911.
Train: 2018-07-31T09:48:51.835595: step 13251, loss 0.631588.
Train: 2018-07-31T09:48:51.991806: step 13252, loss 0.597086.
Train: 2018-07-31T09:48:52.148019: step 13253, loss 0.579658.
Train: 2018-07-31T09:48:52.304202: step 13254, loss 0.51078.
Train: 2018-07-31T09:48:52.460449: step 13255, loss 0.519609.
Train: 2018-07-31T09:48:52.616661: step 13256, loss 0.570965.
Train: 2018-07-31T09:48:52.788464: step 13257, loss 0.639441.
Train: 2018-07-31T09:48:52.944679: step 13258, loss 0.502686.
Train: 2018-07-31T09:48:53.100924: step 13259, loss 0.588017.
Train: 2018-07-31T09:48:53.257136: step 13260, loss 0.553867.
Test: 2018-07-31T09:48:53.507070: step 13260, loss 0.548131.
Train: 2018-07-31T09:48:53.663259: step 13261, loss 0.613643.
Train: 2018-07-31T09:48:53.819473: step 13262, loss 0.571.
Train: 2018-07-31T09:48:53.991308: step 13263, loss 0.638901.
Train: 2018-07-31T09:48:54.147551: step 13264, loss 0.486264.
Train: 2018-07-31T09:48:54.303764: step 13265, loss 0.57084.
Train: 2018-07-31T09:48:54.459948: step 13266, loss 0.553968.
Train: 2018-07-31T09:48:54.616186: step 13267, loss 0.520263.
Train: 2018-07-31T09:48:54.788026: step 13268, loss 0.58775.
Train: 2018-07-31T09:48:54.944210: step 13269, loss 0.562376.
Train: 2018-07-31T09:48:55.100423: step 13270, loss 0.520181.
Test: 2018-07-31T09:48:55.334776: step 13270, loss 0.548285.
Train: 2018-07-31T09:48:55.506578: step 13271, loss 0.621565.
Train: 2018-07-31T09:48:55.662822: step 13272, loss 0.528663.
Train: 2018-07-31T09:48:55.819035: step 13273, loss 0.511775.
Train: 2018-07-31T09:48:55.975218: step 13274, loss 0.621642.
Train: 2018-07-31T09:48:56.131463: step 13275, loss 0.520173.
Train: 2018-07-31T09:48:56.287675: step 13276, loss 0.613201.
Train: 2018-07-31T09:48:56.459504: step 13277, loss 0.587804.
Train: 2018-07-31T09:48:56.615723: step 13278, loss 0.545473.
Train: 2018-07-31T09:48:56.771937: step 13279, loss 0.562405.
Train: 2018-07-31T09:48:56.928120: step 13280, loss 0.570855.
Test: 2018-07-31T09:48:57.178097: step 13280, loss 0.548246.
Train: 2018-07-31T09:48:57.334275: step 13281, loss 0.570857.
Train: 2018-07-31T09:48:57.490522: step 13282, loss 0.579336.
Train: 2018-07-31T09:48:57.646731: step 13283, loss 0.50319.
Train: 2018-07-31T09:48:57.802915: step 13284, loss 0.528569.
Train: 2018-07-31T09:48:57.959128: step 13285, loss 0.570893.
Train: 2018-07-31T09:48:58.130988: step 13286, loss 0.53692.
Train: 2018-07-31T09:48:58.287206: step 13287, loss 0.494432.
Train: 2018-07-31T09:48:58.443420: step 13288, loss 0.617064.
Train: 2018-07-31T09:48:58.599604: step 13289, loss 0.570952.
Train: 2018-07-31T09:48:58.771468: step 13290, loss 0.570987.
Test: 2018-07-31T09:48:59.005758: step 13290, loss 0.548066.
Train: 2018-07-31T09:48:59.161971: step 13291, loss 0.673673.
Train: 2018-07-31T09:48:59.333805: step 13292, loss 0.562344.
Train: 2018-07-31T09:48:59.490050: step 13293, loss 0.536779.
Train: 2018-07-31T09:48:59.646234: step 13294, loss 0.579528.
Train: 2018-07-31T09:48:59.818067: step 13295, loss 0.502695.
Train: 2018-07-31T09:48:59.974281: step 13296, loss 0.588003.
Train: 2018-07-31T09:49:00.161739: step 13297, loss 0.553877.
Train: 2018-07-31T09:49:00.317951: step 13298, loss 0.630673.
Train: 2018-07-31T09:49:00.474203: step 13299, loss 0.588012.
Train: 2018-07-31T09:49:00.645999: step 13300, loss 0.494388.
Test: 2018-07-31T09:49:00.880319: step 13300, loss 0.54816.
Train: 2018-07-31T09:49:01.645765: step 13301, loss 0.511379.
Train: 2018-07-31T09:49:01.802009: step 13302, loss 0.664501.
Train: 2018-07-31T09:49:01.973838: step 13303, loss 0.604877.
Train: 2018-07-31T09:49:02.130057: step 13304, loss 0.545445.
Train: 2018-07-31T09:49:02.301861: step 13305, loss 0.477603.
Train: 2018-07-31T09:49:02.458075: step 13306, loss 0.469048.
Train: 2018-07-31T09:49:02.629910: step 13307, loss 0.596475.
Train: 2018-07-31T09:49:02.786123: step 13308, loss 0.60508.
Train: 2018-07-31T09:49:02.942369: step 13309, loss 0.494161.
Train: 2018-07-31T09:49:03.098549: step 13310, loss 0.579494.
Test: 2018-07-31T09:49:03.348516: step 13310, loss 0.548067.
Train: 2018-07-31T09:49:03.504735: step 13311, loss 0.570971.
Train: 2018-07-31T09:49:03.660918: step 13312, loss 0.49386.
Train: 2018-07-31T09:49:03.817169: step 13313, loss 0.58816.
Train: 2018-07-31T09:49:03.973344: step 13314, loss 0.493556.
Train: 2018-07-31T09:49:04.129595: step 13315, loss 0.536574.
Train: 2018-07-31T09:49:04.285807: step 13316, loss 0.545104.
Train: 2018-07-31T09:49:04.457639: step 13317, loss 0.545042.
Train: 2018-07-31T09:49:04.613819: step 13318, loss 0.588542.
Train: 2018-07-31T09:49:04.770034: step 13319, loss 0.536269.
Train: 2018-07-31T09:49:04.926284: step 13320, loss 0.571235.
Test: 2018-07-31T09:49:05.176213: step 13320, loss 0.547781.
Train: 2018-07-31T09:49:05.332434: step 13321, loss 0.536165.
Train: 2018-07-31T09:49:05.488645: step 13322, loss 0.597584.
Train: 2018-07-31T09:49:05.660449: step 13323, loss 0.571288.
Train: 2018-07-31T09:49:05.816693: step 13324, loss 0.580022.
Train: 2018-07-31T09:49:05.972910: step 13325, loss 0.571247.
Train: 2018-07-31T09:49:06.129120: step 13326, loss 0.53609.
Train: 2018-07-31T09:49:06.285303: step 13327, loss 0.52728.
Train: 2018-07-31T09:49:06.457167: step 13328, loss 0.597629.
Train: 2018-07-31T09:49:06.613378: step 13329, loss 0.562453.
Train: 2018-07-31T09:49:06.769564: step 13330, loss 0.580074.
Test: 2018-07-31T09:49:07.003885: step 13330, loss 0.547741.
Train: 2018-07-31T09:49:07.175750: step 13331, loss 0.536066.
Train: 2018-07-31T09:49:07.331934: step 13332, loss 0.597626.
Train: 2018-07-31T09:49:07.503793: step 13333, loss 0.509745.
Train: 2018-07-31T09:49:07.659982: step 13334, loss 0.492179.
Train: 2018-07-31T09:49:07.816228: step 13335, loss 0.562472.
Train: 2018-07-31T09:49:07.972409: step 13336, loss 0.553676.
Train: 2018-07-31T09:49:08.144243: step 13337, loss 0.544791.
Train: 2018-07-31T09:49:08.316077: step 13338, loss 0.544883.
Train: 2018-07-31T09:49:08.472291: step 13339, loss 0.509478.
Train: 2018-07-31T09:49:08.628529: step 13340, loss 0.597931.
Test: 2018-07-31T09:49:08.878446: step 13340, loss 0.547675.
Train: 2018-07-31T09:49:09.034692: step 13341, loss 0.606859.
Train: 2018-07-31T09:49:09.190903: step 13342, loss 0.518257.
Train: 2018-07-31T09:49:09.347120: step 13343, loss 0.553624.
Train: 2018-07-31T09:49:09.518952: step 13344, loss 0.562501.
Train: 2018-07-31T09:49:09.675135: step 13345, loss 0.526985.
Train: 2018-07-31T09:49:09.831381: step 13346, loss 0.544802.
Train: 2018-07-31T09:49:09.987561: step 13347, loss 0.562647.
Train: 2018-07-31T09:49:10.159397: step 13348, loss 0.526992.
Train: 2018-07-31T09:49:10.315610: step 13349, loss 0.544795.
Train: 2018-07-31T09:49:10.487445: step 13350, loss 0.624982.
Test: 2018-07-31T09:49:10.721789: step 13350, loss 0.547642.
Train: 2018-07-31T09:49:10.893599: step 13351, loss 0.589264.
Train: 2018-07-31T09:49:11.049843: step 13352, loss 0.607114.
Train: 2018-07-31T09:49:11.206026: step 13353, loss 0.624812.
Train: 2018-07-31T09:49:11.377861: step 13354, loss 0.615664.
Train: 2018-07-31T09:49:11.534075: step 13355, loss 0.562521.
Train: 2018-07-31T09:49:11.705909: step 13356, loss 0.536079.
Train: 2018-07-31T09:49:11.862153: step 13357, loss 0.57996.
Train: 2018-07-31T09:49:12.033957: step 13358, loss 0.571155.
Train: 2018-07-31T09:49:12.190171: step 13359, loss 0.501391.
Train: 2018-07-31T09:49:12.346417: step 13360, loss 0.614581.
Test: 2018-07-31T09:49:12.596374: step 13360, loss 0.547897.
Train: 2018-07-31T09:49:12.752539: step 13361, loss 0.6317.
Train: 2018-07-31T09:49:12.908783: step 13362, loss 0.527751.
Train: 2018-07-31T09:49:13.080617: step 13363, loss 0.579756.
Train: 2018-07-31T09:49:13.236830: step 13364, loss 0.596868.
Train: 2018-07-31T09:49:13.393013: step 13365, loss 0.596157.
Train: 2018-07-31T09:49:13.564879: step 13366, loss 0.588108.
Train: 2018-07-31T09:49:13.736683: step 13367, loss 0.562209.
Train: 2018-07-31T09:49:13.908548: step 13368, loss 0.56959.
Train: 2018-07-31T09:49:14.064733: step 13369, loss 0.553379.
Train: 2018-07-31T09:49:14.220978: step 13370, loss 0.611457.
Test: 2018-07-31T09:49:14.470886: step 13370, loss 0.549404.
Train: 2018-07-31T09:49:14.627124: step 13371, loss 0.546032.
Train: 2018-07-31T09:49:14.798965: step 13372, loss 0.563701.
Train: 2018-07-31T09:49:14.955181: step 13373, loss 0.611569.
Train: 2018-07-31T09:49:15.111392: step 13374, loss 0.580724.
Train: 2018-07-31T09:49:15.267605: step 13375, loss 0.52486.
Train: 2018-07-31T09:49:15.423788: step 13376, loss 0.530773.
Train: 2018-07-31T09:49:15.595622: step 13377, loss 0.497252.
Train: 2018-07-31T09:49:15.751861: step 13378, loss 0.609746.
Train: 2018-07-31T09:49:15.923702: step 13379, loss 0.610469.
Train: 2018-07-31T09:49:16.095539: step 13380, loss 0.547233.
Test: 2018-07-31T09:49:16.329827: step 13380, loss 0.548796.
Train: 2018-07-31T09:49:16.501691: step 13381, loss 0.555214.
Train: 2018-07-31T09:49:16.673531: step 13382, loss 0.570395.
Train: 2018-07-31T09:49:16.829709: step 13383, loss 0.579926.
Train: 2018-07-31T09:49:16.985954: step 13384, loss 0.612936.
Train: 2018-07-31T09:49:17.142166: step 13385, loss 0.629162.
Train: 2018-07-31T09:49:17.314011: step 13386, loss 0.487556.
Train: 2018-07-31T09:49:17.470184: step 13387, loss 0.521143.
Train: 2018-07-31T09:49:17.626422: step 13388, loss 0.59617.
Train: 2018-07-31T09:49:17.798233: step 13389, loss 0.494889.
Train: 2018-07-31T09:49:17.954474: step 13390, loss 0.537011.
Test: 2018-07-31T09:49:18.188797: step 13390, loss 0.548163.
Train: 2018-07-31T09:49:18.360631: step 13391, loss 0.596438.
Train: 2018-07-31T09:49:18.516850: step 13392, loss 0.622183.
Train: 2018-07-31T09:49:18.688681: step 13393, loss 0.647742.
Train: 2018-07-31T09:49:18.844863: step 13394, loss 0.545361.
Train: 2018-07-31T09:49:19.016727: step 13395, loss 0.536867.
Train: 2018-07-31T09:49:19.172940: step 13396, loss 0.494459.
Train: 2018-07-31T09:49:19.344778: step 13397, loss 0.562417.
Train: 2018-07-31T09:49:19.516610: step 13398, loss 0.536772.
Train: 2018-07-31T09:49:19.672824: step 13399, loss 0.554039.
Train: 2018-07-31T09:49:19.844628: step 13400, loss 0.61357.
Test: 2018-07-31T09:49:20.078978: step 13400, loss 0.54809.
Train: 2018-07-31T09:49:20.797556: step 13401, loss 0.588031.
Train: 2018-07-31T09:49:20.969364: step 13402, loss 0.579428.
Train: 2018-07-31T09:49:21.141224: step 13403, loss 0.545274.
Train: 2018-07-31T09:49:21.313035: step 13404, loss 0.647845.
Train: 2018-07-31T09:49:21.484870: step 13405, loss 0.613508.
Train: 2018-07-31T09:49:21.656730: step 13406, loss 0.502933.
Train: 2018-07-31T09:49:21.812918: step 13407, loss 0.520063.
Train: 2018-07-31T09:49:21.984782: step 13408, loss 0.562459.
Train: 2018-07-31T09:49:22.140966: step 13409, loss 0.570905.
Train: 2018-07-31T09:49:22.312830: step 13410, loss 0.528503.
Test: 2018-07-31T09:49:22.547151: step 13410, loss 0.548209.
Train: 2018-07-31T09:49:22.718956: step 13411, loss 0.520021.
Train: 2018-07-31T09:49:22.890790: step 13412, loss 0.587899.
Train: 2018-07-31T09:49:23.047034: step 13413, loss 0.519927.
Train: 2018-07-31T09:49:23.218872: step 13414, loss 0.604966.
Train: 2018-07-31T09:49:23.375052: step 13415, loss 0.477238.
Train: 2018-07-31T09:49:23.546916: step 13416, loss 0.570946.
Train: 2018-07-31T09:49:23.703100: step 13417, loss 0.502526.
Train: 2018-07-31T09:49:23.874935: step 13418, loss 0.562407.
Train: 2018-07-31T09:49:24.031148: step 13419, loss 0.493578.
Train: 2018-07-31T09:49:24.202983: step 13420, loss 0.571035.
Test: 2018-07-31T09:49:24.437329: step 13420, loss 0.547899.
Train: 2018-07-31T09:49:24.609138: step 13421, loss 0.545064.
Train: 2018-07-31T09:49:24.781002: step 13422, loss 0.545054.
Train: 2018-07-31T09:49:24.937219: step 13423, loss 0.63212.
Train: 2018-07-31T09:49:25.109021: step 13424, loss 0.571135.
Train: 2018-07-31T09:49:25.265258: step 13425, loss 0.606218.
Train: 2018-07-31T09:49:25.437093: step 13426, loss 0.544974.
Train: 2018-07-31T09:49:25.608933: step 13427, loss 0.579881.
Train: 2018-07-31T09:49:25.765147: step 13428, loss 0.553716.
Train: 2018-07-31T09:49:25.936951: step 13429, loss 0.571156.
Train: 2018-07-31T09:49:26.108787: step 13430, loss 0.649716.
Test: 2018-07-31T09:49:26.343140: step 13430, loss 0.547836.
Train: 2018-07-31T09:49:26.514967: step 13431, loss 0.544977.
Train: 2018-07-31T09:49:26.686781: step 13432, loss 0.571098.
Train: 2018-07-31T09:49:26.843023: step 13433, loss 0.597089.
Train: 2018-07-31T09:49:27.014824: step 13434, loss 0.450022.
Train: 2018-07-31T09:49:27.171037: step 13435, loss 0.48444.
Train: 2018-07-31T09:49:27.342905: step 13436, loss 0.536456.
Train: 2018-07-31T09:49:27.499085: step 13437, loss 0.571065.
Train: 2018-07-31T09:49:27.670921: step 13438, loss 0.501494.
Train: 2018-07-31T09:49:27.827164: step 13439, loss 0.562375.
Train: 2018-07-31T09:49:27.983377: step 13440, loss 0.562394.
Test: 2018-07-31T09:49:28.217701: step 13440, loss 0.547758.
Train: 2018-07-31T09:49:28.389503: step 13441, loss 0.597682.
Train: 2018-07-31T09:49:28.545746: step 13442, loss 0.606346.
Train: 2018-07-31T09:49:28.717550: step 13443, loss 0.65032.
Train: 2018-07-31T09:49:28.873795: step 13444, loss 0.579876.
Train: 2018-07-31T09:49:29.045598: step 13445, loss 0.606251.
Train: 2018-07-31T09:49:29.201844: step 13446, loss 0.614742.
Train: 2018-07-31T09:49:29.373647: step 13447, loss 0.57107.
Train: 2018-07-31T09:49:29.545481: step 13448, loss 0.536473.
Train: 2018-07-31T09:49:29.701728: step 13449, loss 0.553779.
Train: 2018-07-31T09:49:29.873530: step 13450, loss 0.571057.
Test: 2018-07-31T09:49:30.107851: step 13450, loss 0.54804.
Train: 2018-07-31T09:49:30.279685: step 13451, loss 0.596702.
Train: 2018-07-31T09:49:30.451553: step 13452, loss 0.562367.
Train: 2018-07-31T09:49:30.623384: step 13453, loss 0.52835.
Train: 2018-07-31T09:49:30.795220: step 13454, loss 0.596417.
Train: 2018-07-31T09:49:30.967056: step 13455, loss 0.528459.
Train: 2018-07-31T09:49:31.123268: step 13456, loss 0.596215.
Train: 2018-07-31T09:49:31.295073: step 13457, loss 0.53693.
Train: 2018-07-31T09:49:31.466906: step 13458, loss 0.537092.
Train: 2018-07-31T09:49:31.638742: step 13459, loss 0.604601.
Train: 2018-07-31T09:49:31.794954: step 13460, loss 0.537152.
Test: 2018-07-31T09:49:32.044897: step 13460, loss 0.548298.
Train: 2018-07-31T09:49:32.201110: step 13461, loss 0.579237.
Train: 2018-07-31T09:49:32.372969: step 13462, loss 0.56243.
Train: 2018-07-31T09:49:32.544809: step 13463, loss 0.570931.
Train: 2018-07-31T09:49:32.716614: step 13464, loss 0.621596.
Train: 2018-07-31T09:49:32.888479: step 13465, loss 0.537173.
Train: 2018-07-31T09:49:33.060321: step 13466, loss 0.587462.
Train: 2018-07-31T09:49:33.216521: step 13467, loss 0.486718.
Train: 2018-07-31T09:49:33.388332: step 13468, loss 0.536685.
Train: 2018-07-31T09:49:33.560196: step 13469, loss 0.604219.
Train: 2018-07-31T09:49:33.716380: step 13470, loss 0.562292.
Test: 2018-07-31T09:49:33.950728: step 13470, loss 0.548013.
Train: 2018-07-31T09:49:34.122561: step 13471, loss 0.561429.
Train: 2018-07-31T09:49:34.294400: step 13472, loss 0.502579.
Train: 2018-07-31T09:49:34.466205: step 13473, loss 0.542448.
Train: 2018-07-31T09:49:34.638075: step 13474, loss 0.553343.
Train: 2018-07-31T09:49:34.794284: step 13475, loss 0.595504.
Train: 2018-07-31T09:49:34.966120: step 13476, loss 0.565071.
Train: 2018-07-31T09:49:35.137952: step 13477, loss 0.549739.
Train: 2018-07-31T09:49:35.309790: step 13478, loss 0.580998.
Train: 2018-07-31T09:49:35.466003: step 13479, loss 0.56439.
Train: 2018-07-31T09:49:35.637804: step 13480, loss 0.606709.
Test: 2018-07-31T09:49:35.872156: step 13480, loss 0.547704.
Train: 2018-07-31T09:49:36.043990: step 13481, loss 0.572406.
Train: 2018-07-31T09:49:36.215828: step 13482, loss 0.563111.
Train: 2018-07-31T09:49:36.387629: step 13483, loss 0.502473.
Train: 2018-07-31T09:49:36.543873: step 13484, loss 0.547742.
Train: 2018-07-31T09:49:36.715706: step 13485, loss 0.512644.
Train: 2018-07-31T09:49:36.887538: step 13486, loss 0.56261.
Train: 2018-07-31T09:49:37.059385: step 13487, loss 0.655579.
Train: 2018-07-31T09:49:37.215560: step 13488, loss 0.562537.
Train: 2018-07-31T09:49:37.387425: step 13489, loss 0.462398.
Train: 2018-07-31T09:49:37.559260: step 13490, loss 0.587126.
Test: 2018-07-31T09:49:37.793550: step 13490, loss 0.548457.
Train: 2018-07-31T09:49:37.965386: step 13491, loss 0.537308.
Train: 2018-07-31T09:49:38.137250: step 13492, loss 0.512104.
Train: 2018-07-31T09:49:38.293463: step 13493, loss 0.537155.
Train: 2018-07-31T09:49:38.465267: step 13494, loss 0.59629.
Train: 2018-07-31T09:49:38.652754: step 13495, loss 0.537034.
Train: 2018-07-31T09:49:38.808938: step 13496, loss 0.57078.
Train: 2018-07-31T09:49:38.980802: step 13497, loss 0.528356.
Train: 2018-07-31T09:49:39.152608: step 13498, loss 0.519593.
Train: 2018-07-31T09:49:39.340096: step 13499, loss 0.605302.
Train: 2018-07-31T09:49:39.496307: step 13500, loss 0.596752.
Test: 2018-07-31T09:49:39.746248: step 13500, loss 0.547976.
Train: 2018-07-31T09:49:40.496042: step 13501, loss 0.571111.
Train: 2018-07-31T09:49:40.667907: step 13502, loss 0.536448.
Train: 2018-07-31T09:49:40.839712: step 13503, loss 0.562411.
Train: 2018-07-31T09:49:41.011577: step 13504, loss 0.536562.
Train: 2018-07-31T09:49:41.183411: step 13505, loss 0.51902.
Train: 2018-07-31T09:49:41.355216: step 13506, loss 0.545146.
Train: 2018-07-31T09:49:41.511459: step 13507, loss 0.579756.
Train: 2018-07-31T09:49:41.683296: step 13508, loss 0.536152.
Train: 2018-07-31T09:49:41.855098: step 13509, loss 0.632212.
Train: 2018-07-31T09:49:42.026970: step 13510, loss 0.501281.
Test: 2018-07-31T09:49:42.261254: step 13510, loss 0.54779.
Train: 2018-07-31T09:49:42.433089: step 13511, loss 0.614921.
Train: 2018-07-31T09:49:42.604954: step 13512, loss 0.53624.
Train: 2018-07-31T09:49:42.776759: step 13513, loss 0.501133.
Train: 2018-07-31T09:49:42.932971: step 13514, loss 0.553765.
Train: 2018-07-31T09:49:43.104807: step 13515, loss 0.544805.
Train: 2018-07-31T09:49:43.276671: step 13516, loss 0.518364.
Train: 2018-07-31T09:49:43.448508: step 13517, loss 0.553611.
Train: 2018-07-31T09:49:43.635932: step 13518, loss 0.544784.
Train: 2018-07-31T09:49:43.807766: step 13519, loss 0.642425.
Train: 2018-07-31T09:49:43.979602: step 13520, loss 0.508943.
Test: 2018-07-31T09:49:44.213955: step 13520, loss 0.547677.
Train: 2018-07-31T09:49:44.385786: step 13521, loss 0.553579.
Train: 2018-07-31T09:49:44.557621: step 13522, loss 0.482589.
Train: 2018-07-31T09:49:44.713837: step 13523, loss 0.51824.
Train: 2018-07-31T09:49:44.885669: step 13524, loss 0.500362.
Train: 2018-07-31T09:49:45.057474: step 13525, loss 0.562618.
Train: 2018-07-31T09:49:45.229339: step 13526, loss 0.499823.
Train: 2018-07-31T09:49:45.401174: step 13527, loss 0.562376.
Train: 2018-07-31T09:49:45.573009: step 13528, loss 0.535963.
Train: 2018-07-31T09:49:45.744843: step 13529, loss 0.616862.
Train: 2018-07-31T09:49:45.916678: step 13530, loss 0.680773.
Test: 2018-07-31T09:49:46.150968: step 13530, loss 0.547578.
Train: 2018-07-31T09:49:46.322827: step 13531, loss 0.562706.
Train: 2018-07-31T09:49:46.494668: step 13532, loss 0.563015.
Train: 2018-07-31T09:49:46.666503: step 13533, loss 0.526468.
Train: 2018-07-31T09:49:46.838307: step 13534, loss 0.499217.
Train: 2018-07-31T09:49:47.010141: step 13535, loss 0.571893.
Train: 2018-07-31T09:49:47.182006: step 13536, loss 0.508603.
Train: 2018-07-31T09:49:47.353842: step 13537, loss 0.571799.
Train: 2018-07-31T09:49:47.525676: step 13538, loss 0.599122.
Train: 2018-07-31T09:49:47.697480: step 13539, loss 0.535466.
Train: 2018-07-31T09:49:47.869315: step 13540, loss 0.55344.
Test: 2018-07-31T09:49:48.103666: step 13540, loss 0.547588.
Train: 2018-07-31T09:49:48.275500: step 13541, loss 0.643736.
Train: 2018-07-31T09:49:48.447305: step 13542, loss 0.571459.
Train: 2018-07-31T09:49:48.619139: step 13543, loss 0.661502.
Train: 2018-07-31T09:49:48.791005: step 13544, loss 0.55379.
Train: 2018-07-31T09:49:48.962842: step 13545, loss 0.589109.
Train: 2018-07-31T09:49:49.134644: step 13546, loss 0.588904.
Train: 2018-07-31T09:49:49.306509: step 13547, loss 0.526997.
Train: 2018-07-31T09:49:49.478338: step 13548, loss 0.509659.
Train: 2018-07-31T09:49:49.634527: step 13549, loss 0.588725.
Train: 2018-07-31T09:49:49.806392: step 13550, loss 0.536391.
Test: 2018-07-31T09:49:50.040707: step 13550, loss 0.547778.
Train: 2018-07-31T09:49:50.212518: step 13551, loss 0.614781.
Train: 2018-07-31T09:49:50.384352: step 13552, loss 0.562239.
Train: 2018-07-31T09:49:50.556216: step 13553, loss 0.527752.
Train: 2018-07-31T09:49:50.728022: step 13554, loss 0.571099.
Train: 2018-07-31T09:49:50.899856: step 13555, loss 0.527764.
Train: 2018-07-31T09:49:51.071721: step 13556, loss 0.571752.
Train: 2018-07-31T09:49:51.259177: step 13557, loss 0.493387.
Train: 2018-07-31T09:49:51.431006: step 13558, loss 0.536271.
Train: 2018-07-31T09:49:51.602817: step 13559, loss 0.527761.
Train: 2018-07-31T09:49:51.774681: step 13560, loss 0.553929.
Test: 2018-07-31T09:49:52.009003: step 13560, loss 0.547854.
Train: 2018-07-31T09:49:52.180836: step 13561, loss 0.597598.
Train: 2018-07-31T09:49:52.352640: step 13562, loss 0.52721.
Train: 2018-07-31T09:49:52.524475: step 13563, loss 0.59734.
Train: 2018-07-31T09:49:52.696341: step 13564, loss 0.509864.
Train: 2018-07-31T09:49:52.868145: step 13565, loss 0.605655.
Train: 2018-07-31T09:49:53.040010: step 13566, loss 0.58014.
Train: 2018-07-31T09:49:53.211815: step 13567, loss 0.536202.
Train: 2018-07-31T09:49:53.383679: step 13568, loss 0.545172.
Train: 2018-07-31T09:49:53.571139: step 13569, loss 0.632112.
Train: 2018-07-31T09:49:53.742941: step 13570, loss 0.536319.
Test: 2018-07-31T09:49:53.977292: step 13570, loss 0.547849.
Train: 2018-07-31T09:49:54.149120: step 13571, loss 0.588239.
Train: 2018-07-31T09:49:54.320931: step 13572, loss 0.545008.
Train: 2018-07-31T09:49:54.492795: step 13573, loss 0.553875.
Train: 2018-07-31T09:49:54.664600: step 13574, loss 0.510532.
Train: 2018-07-31T09:49:54.820837: step 13575, loss 0.536393.
Train: 2018-07-31T09:49:54.992648: step 13576, loss 0.54502.
Train: 2018-07-31T09:49:55.164514: step 13577, loss 0.632314.
Train: 2018-07-31T09:49:55.336318: step 13578, loss 0.571076.
Train: 2018-07-31T09:49:55.492563: step 13579, loss 0.580155.
Train: 2018-07-31T09:49:55.679986: step 13580, loss 0.588264.
Test: 2018-07-31T09:49:55.914338: step 13580, loss 0.547894.
Train: 2018-07-31T09:49:56.086175: step 13581, loss 0.631704.
Train: 2018-07-31T09:49:56.258009: step 13582, loss 0.510666.
Train: 2018-07-31T09:49:56.429835: step 13583, loss 0.553782.
Train: 2018-07-31T09:49:56.601646: step 13584, loss 0.588482.
Train: 2018-07-31T09:49:56.773481: step 13585, loss 0.570878.
Train: 2018-07-31T09:49:56.929724: step 13586, loss 0.536844.
Train: 2018-07-31T09:49:57.101528: step 13587, loss 0.613816.
Train: 2018-07-31T09:49:57.273364: step 13588, loss 0.596644.
Train: 2018-07-31T09:49:57.445199: step 13589, loss 0.587957.
Train: 2018-07-31T09:49:57.601442: step 13590, loss 0.562318.
Test: 2018-07-31T09:49:57.851353: step 13590, loss 0.548195.
Train: 2018-07-31T09:49:58.023187: step 13591, loss 0.596598.
Train: 2018-07-31T09:49:58.195022: step 13592, loss 0.562382.
Train: 2018-07-31T09:49:58.351236: step 13593, loss 0.562641.
Train: 2018-07-31T09:49:58.523100: step 13594, loss 0.596117.
Train: 2018-07-31T09:49:58.694931: step 13595, loss 0.570712.
Train: 2018-07-31T09:49:58.851149: step 13596, loss 0.670446.
Train: 2018-07-31T09:49:59.022953: step 13597, loss 0.513207.
Train: 2018-07-31T09:49:59.194789: step 13598, loss 0.57834.
Train: 2018-07-31T09:49:59.366624: step 13599, loss 0.636238.
Train: 2018-07-31T09:49:59.538487: step 13600, loss 0.627387.
Test: 2018-07-31T09:49:59.772808: step 13600, loss 0.549707.
Train: 2018-07-31T09:50:00.522633: step 13601, loss 0.546593.
Train: 2018-07-31T09:50:00.694469: step 13602, loss 0.570943.
Train: 2018-07-31T09:50:00.866272: step 13603, loss 0.508219.
Train: 2018-07-31T09:50:01.038106: step 13604, loss 0.504805.
Train: 2018-07-31T09:50:01.209941: step 13605, loss 0.570723.
Train: 2018-07-31T09:50:01.381801: step 13606, loss 0.514212.
Train: 2018-07-31T09:50:01.553611: step 13607, loss 0.596005.
Train: 2018-07-31T09:50:01.725446: step 13608, loss 0.571456.
Train: 2018-07-31T09:50:01.897312: step 13609, loss 0.612031.
Train: 2018-07-31T09:50:02.069146: step 13610, loss 0.620175.
Test: 2018-07-31T09:50:02.319058: step 13610, loss 0.548861.
Train: 2018-07-31T09:50:02.490922: step 13611, loss 0.512934.
Train: 2018-07-31T09:50:02.662756: step 13612, loss 0.480423.
Train: 2018-07-31T09:50:02.834594: step 13613, loss 0.595797.
Train: 2018-07-31T09:50:03.006426: step 13614, loss 0.545943.
Train: 2018-07-31T09:50:03.178264: step 13615, loss 0.620801.
Train: 2018-07-31T09:50:03.350066: step 13616, loss 0.60413.
Train: 2018-07-31T09:50:03.521900: step 13617, loss 0.529038.
Train: 2018-07-31T09:50:03.693765: step 13618, loss 0.520704.
Train: 2018-07-31T09:50:03.865569: step 13619, loss 0.545744.
Train: 2018-07-31T09:50:04.037404: step 13620, loss 0.587587.
Test: 2018-07-31T09:50:04.271755: step 13620, loss 0.548425.
Train: 2018-07-31T09:50:04.443589: step 13621, loss 0.49542.
Train: 2018-07-31T09:50:04.615394: step 13622, loss 0.604479.
Train: 2018-07-31T09:50:04.787228: step 13623, loss 0.579277.
Train: 2018-07-31T09:50:04.959096: step 13624, loss 0.562658.
Train: 2018-07-31T09:50:05.130928: step 13625, loss 0.62142.
Train: 2018-07-31T09:50:05.302734: step 13626, loss 0.587636.
Train: 2018-07-31T09:50:05.458980: step 13627, loss 0.596157.
Train: 2018-07-31T09:50:05.646433: step 13628, loss 0.604407.
Train: 2018-07-31T09:50:05.818270: step 13629, loss 0.570882.
Train: 2018-07-31T09:50:05.990073: step 13630, loss 0.537187.
Test: 2018-07-31T09:50:06.224394: step 13630, loss 0.548402.
Train: 2018-07-31T09:50:06.396259: step 13631, loss 0.503641.
Train: 2018-07-31T09:50:06.568062: step 13632, loss 0.570778.
Train: 2018-07-31T09:50:06.739927: step 13633, loss 0.570751.
Train: 2018-07-31T09:50:06.911762: step 13634, loss 0.512076.
Train: 2018-07-31T09:50:07.083599: step 13635, loss 0.596144.
Train: 2018-07-31T09:50:07.255431: step 13636, loss 0.528764.
Train: 2018-07-31T09:50:07.427266: step 13637, loss 0.511935.
Train: 2018-07-31T09:50:07.599071: step 13638, loss 0.571089.
Train: 2018-07-31T09:50:07.770905: step 13639, loss 0.570894.
Train: 2018-07-31T09:50:07.942767: step 13640, loss 0.519915.
Test: 2018-07-31T09:50:08.177060: step 13640, loss 0.548174.
Train: 2018-07-31T09:50:08.348894: step 13641, loss 0.58796.
Train: 2018-07-31T09:50:08.520730: step 13642, loss 0.468757.
Train: 2018-07-31T09:50:08.692597: step 13643, loss 0.536616.
Train: 2018-07-31T09:50:08.864429: step 13644, loss 0.596435.
Train: 2018-07-31T09:50:09.020613: step 13645, loss 0.570935.
Train: 2018-07-31T09:50:09.192447: step 13646, loss 0.605414.
Train: 2018-07-31T09:50:09.364312: step 13647, loss 0.570954.
Train: 2018-07-31T09:50:09.536147: step 13648, loss 0.55397.
Train: 2018-07-31T09:50:09.707976: step 13649, loss 0.614392.
Train: 2018-07-31T09:50:09.879787: step 13650, loss 0.5969.
Test: 2018-07-31T09:50:10.114106: step 13650, loss 0.547954.
Train: 2018-07-31T09:50:10.285971: step 13651, loss 0.657629.
Train: 2018-07-31T09:50:10.457809: step 13652, loss 0.596838.
Train: 2018-07-31T09:50:10.629611: step 13653, loss 0.493934.
Train: 2018-07-31T09:50:10.817091: step 13654, loss 0.519399.
Train: 2018-07-31T09:50:10.988901: step 13655, loss 0.588026.
Train: 2018-07-31T09:50:11.160766: step 13656, loss 0.605164.
Train: 2018-07-31T09:50:11.332604: step 13657, loss 0.459982.
Train: 2018-07-31T09:50:11.504406: step 13658, loss 0.54541.
Train: 2018-07-31T09:50:11.676272: step 13659, loss 0.5882.
Train: 2018-07-31T09:50:11.863727: step 13660, loss 0.553817.
Test: 2018-07-31T09:50:12.098048: step 13660, loss 0.548122.
Train: 2018-07-31T09:50:12.269852: step 13661, loss 0.571191.
Train: 2018-07-31T09:50:12.441686: step 13662, loss 0.536917.
Train: 2018-07-31T09:50:12.613551: step 13663, loss 0.604971.
Train: 2018-07-31T09:50:12.785386: step 13664, loss 0.48568.
Train: 2018-07-31T09:50:12.957190: step 13665, loss 0.545635.
Train: 2018-07-31T09:50:13.129055: step 13666, loss 0.605286.
Train: 2018-07-31T09:50:13.300861: step 13667, loss 0.553773.
Train: 2018-07-31T09:50:13.472694: step 13668, loss 0.648311.
Train: 2018-07-31T09:50:13.660151: step 13669, loss 0.631232.
Train: 2018-07-31T09:50:13.832010: step 13670, loss 0.596562.
Test: 2018-07-31T09:50:14.066340: step 13670, loss 0.548089.
Train: 2018-07-31T09:50:14.238140: step 13671, loss 0.49403.
Train: 2018-07-31T09:50:14.409975: step 13672, loss 0.553898.
Train: 2018-07-31T09:50:14.566222: step 13673, loss 0.485684.
Train: 2018-07-31T09:50:14.738062: step 13674, loss 0.545378.
Train: 2018-07-31T09:50:14.925479: step 13675, loss 0.536805.
Train: 2018-07-31T09:50:15.097314: step 13676, loss 0.60537.
Train: 2018-07-31T09:50:15.269174: step 13677, loss 0.553989.
Train: 2018-07-31T09:50:15.425393: step 13678, loss 0.553963.
Train: 2018-07-31T09:50:15.597198: step 13679, loss 0.596601.
Train: 2018-07-31T09:50:15.769032: step 13680, loss 0.545286.
Test: 2018-07-31T09:50:16.018998: step 13680, loss 0.548031.
Train: 2018-07-31T09:50:16.190807: step 13681, loss 0.493832.
Train: 2018-07-31T09:50:16.378289: step 13682, loss 0.61393.
Train: 2018-07-31T09:50:16.534479: step 13683, loss 0.493665.
Train: 2018-07-31T09:50:16.706313: step 13684, loss 0.588227.
Train: 2018-07-31T09:50:16.878147: step 13685, loss 0.622851.
Train: 2018-07-31T09:50:17.049983: step 13686, loss 0.614231.
Train: 2018-07-31T09:50:17.237439: step 13687, loss 0.50223.
Train: 2018-07-31T09:50:17.393682: step 13688, loss 0.57974.
Train: 2018-07-31T09:50:17.565487: step 13689, loss 0.613919.
Train: 2018-07-31T09:50:17.737322: step 13690, loss 0.545172.
Test: 2018-07-31T09:50:17.971642: step 13690, loss 0.548016.
Train: 2018-07-31T09:50:18.143507: step 13691, loss 0.536592.
Train: 2018-07-31T09:50:18.330933: step 13692, loss 0.528076.
Train: 2018-07-31T09:50:18.502768: step 13693, loss 0.553815.
Train: 2018-07-31T09:50:18.674602: step 13694, loss 0.579427.
Train: 2018-07-31T09:50:18.846467: step 13695, loss 0.553814.
Train: 2018-07-31T09:50:19.018297: step 13696, loss 0.536537.
Train: 2018-07-31T09:50:19.205761: step 13697, loss 0.536623.
Train: 2018-07-31T09:50:19.361940: step 13698, loss 0.545209.
Train: 2018-07-31T09:50:19.549427: step 13699, loss 0.571026.
Train: 2018-07-31T09:50:19.721232: step 13700, loss 0.544988.
Test: 2018-07-31T09:50:19.955552: step 13700, loss 0.547928.
Train: 2018-07-31T09:50:20.674134: step 13701, loss 0.536416.
Train: 2018-07-31T09:50:20.845999: step 13702, loss 0.519159.
Train: 2018-07-31T09:50:21.033455: step 13703, loss 0.571001.
Train: 2018-07-31T09:50:21.189668: step 13704, loss 0.562432.
Train: 2018-07-31T09:50:21.377124: step 13705, loss 0.649431.
Train: 2018-07-31T09:50:21.548929: step 13706, loss 0.519014.
Train: 2018-07-31T09:50:21.736385: step 13707, loss 0.536267.
Train: 2018-07-31T09:50:21.908250: step 13708, loss 0.475314.
Train: 2018-07-31T09:50:22.080054: step 13709, loss 0.588613.
Train: 2018-07-31T09:50:22.251888: step 13710, loss 0.535997.
Test: 2018-07-31T09:50:22.486240: step 13710, loss 0.547774.
Train: 2018-07-31T09:50:22.673691: step 13711, loss 0.588622.
Train: 2018-07-31T09:50:22.845530: step 13712, loss 0.588811.
Train: 2018-07-31T09:50:23.017335: step 13713, loss 0.562419.
Train: 2018-07-31T09:50:23.189170: step 13714, loss 0.49205.
Train: 2018-07-31T09:50:23.361005: step 13715, loss 0.545071.
Train: 2018-07-31T09:50:23.532871: step 13716, loss 0.562605.
Train: 2018-07-31T09:50:23.720326: step 13717, loss 0.544989.
Train: 2018-07-31T09:50:23.954648: step 13718, loss 0.553332.
Train: 2018-07-31T09:50:24.126480: step 13719, loss 0.668434.
Train: 2018-07-31T09:50:24.298315: step 13720, loss 0.544655.
Test: 2018-07-31T09:50:24.548227: step 13720, loss 0.547718.
Train: 2018-07-31T09:50:24.720086: step 13721, loss 0.544744.
Train: 2018-07-31T09:50:24.891926: step 13722, loss 0.518234.
Train: 2018-07-31T09:50:25.063764: step 13723, loss 0.544793.
Train: 2018-07-31T09:50:25.235566: step 13724, loss 0.589038.
Train: 2018-07-31T09:50:25.407400: step 13725, loss 0.483212.
Train: 2018-07-31T09:50:25.579268: step 13726, loss 0.606606.
Train: 2018-07-31T09:50:25.751070: step 13727, loss 0.642124.
Train: 2018-07-31T09:50:25.922905: step 13728, loss 0.56258.
Train: 2018-07-31T09:50:26.094739: step 13729, loss 0.518134.
Train: 2018-07-31T09:50:26.266573: step 13730, loss 0.544704.
Test: 2018-07-31T09:50:26.500924: step 13730, loss 0.54775.
Train: 2018-07-31T09:50:26.688350: step 13731, loss 0.535926.
Train: 2018-07-31T09:50:26.860215: step 13732, loss 0.553685.
Train: 2018-07-31T09:50:27.032050: step 13733, loss 0.562458.
Train: 2018-07-31T09:50:27.203880: step 13734, loss 0.553807.
Train: 2018-07-31T09:50:27.375689: step 13735, loss 0.536401.
Train: 2018-07-31T09:50:27.547523: step 13736, loss 0.571488.
Train: 2018-07-31T09:50:27.735006: step 13737, loss 0.615346.
Train: 2018-07-31T09:50:27.906845: step 13738, loss 0.553694.
Train: 2018-07-31T09:50:28.078682: step 13739, loss 0.553971.
Train: 2018-07-31T09:50:28.266136: step 13740, loss 0.509739.
Test: 2018-07-31T09:50:28.500457: step 13740, loss 0.547766.
Train: 2018-07-31T09:50:28.672285: step 13741, loss 0.487553.
Train: 2018-07-31T09:50:28.844095: step 13742, loss 0.580271.
Train: 2018-07-31T09:50:29.015960: step 13743, loss 0.615323.
Train: 2018-07-31T09:50:29.187795: step 13744, loss 0.509693.
Train: 2018-07-31T09:50:29.359600: step 13745, loss 0.632778.
Train: 2018-07-31T09:50:29.531465: step 13746, loss 0.597575.
Train: 2018-07-31T09:50:29.703302: step 13747, loss 0.562137.
Train: 2018-07-31T09:50:29.875133: step 13748, loss 0.571143.
Train: 2018-07-31T09:50:30.062592: step 13749, loss 0.509893.
Train: 2018-07-31T09:50:30.234394: step 13750, loss 0.571111.
Test: 2018-07-31T09:50:30.468746: step 13750, loss 0.547796.
Train: 2018-07-31T09:50:30.640580: step 13751, loss 0.588703.
Train: 2018-07-31T09:50:30.812385: step 13752, loss 0.649736.
Train: 2018-07-31T09:50:30.968598: step 13753, loss 0.536251.
Train: 2018-07-31T09:50:31.140463: step 13754, loss 0.536451.
Train: 2018-07-31T09:50:31.327927: step 13755, loss 0.544949.
Train: 2018-07-31T09:50:31.484103: step 13756, loss 0.501745.
Train: 2018-07-31T09:50:31.655938: step 13757, loss 0.56223.
Train: 2018-07-31T09:50:31.827804: step 13758, loss 0.545103.
Train: 2018-07-31T09:50:31.999637: step 13759, loss 0.562499.
Train: 2018-07-31T09:50:32.155820: step 13760, loss 0.57961.
Test: 2018-07-31T09:50:32.405761: step 13760, loss 0.547886.
Train: 2018-07-31T09:50:32.577620: step 13761, loss 0.536354.
Train: 2018-07-31T09:50:32.749456: step 13762, loss 0.536324.
Train: 2018-07-31T09:50:32.921296: step 13763, loss 0.614438.
Train: 2018-07-31T09:50:33.093131: step 13764, loss 0.519367.
Train: 2018-07-31T09:50:33.264936: step 13765, loss 0.5537.
Train: 2018-07-31T09:50:33.436769: step 13766, loss 0.518871.
Train: 2018-07-31T09:50:33.608630: step 13767, loss 0.519127.
Train: 2018-07-31T09:50:33.780470: step 13768, loss 0.623516.
Train: 2018-07-31T09:50:33.936685: step 13769, loss 0.605889.
Train: 2018-07-31T09:50:34.108488: step 13770, loss 0.518947.
Test: 2018-07-31T09:50:34.342838: step 13770, loss 0.547823.
Train: 2018-07-31T09:50:34.514672: step 13771, loss 0.562413.
Train: 2018-07-31T09:50:34.686511: step 13772, loss 0.544914.
Train: 2018-07-31T09:50:34.858311: step 13773, loss 0.553486.
Train: 2018-07-31T09:50:35.030177: step 13774, loss 0.614918.
Train: 2018-07-31T09:50:35.202014: step 13775, loss 0.544557.
Train: 2018-07-31T09:50:35.373846: step 13776, loss 0.527811.
Train: 2018-07-31T09:50:35.545681: step 13777, loss 0.457558.
Train: 2018-07-31T09:50:35.717517: step 13778, loss 0.475142.
Train: 2018-07-31T09:50:35.889351: step 13779, loss 0.579876.
Train: 2018-07-31T09:50:36.045567: step 13780, loss 0.535897.
Test: 2018-07-31T09:50:36.279855: step 13780, loss 0.547708.
Train: 2018-07-31T09:50:36.451722: step 13781, loss 0.509497.
Train: 2018-07-31T09:50:36.623524: step 13782, loss 0.527128.
Train: 2018-07-31T09:50:36.795389: step 13783, loss 0.535662.
Train: 2018-07-31T09:50:36.967194: step 13784, loss 0.517659.
Train: 2018-07-31T09:50:37.139052: step 13785, loss 0.517761.
Train: 2018-07-31T09:50:37.310895: step 13786, loss 0.535689.
Train: 2018-07-31T09:50:37.467077: step 13787, loss 0.51746.
Train: 2018-07-31T09:50:37.638911: step 13788, loss 0.562723.
Train: 2018-07-31T09:50:37.810778: step 13789, loss 0.581101.
Train: 2018-07-31T09:50:37.982580: step 13790, loss 0.572075.
Test: 2018-07-31T09:50:38.216901: step 13790, loss 0.547574.
Train: 2018-07-31T09:50:38.388734: step 13791, loss 0.544429.
Train: 2018-07-31T09:50:38.560570: step 13792, loss 0.544645.
Train: 2018-07-31T09:50:38.732435: step 13793, loss 0.526022.
Train: 2018-07-31T09:50:38.904270: step 13794, loss 0.599753.
Train: 2018-07-31T09:50:39.076075: step 13795, loss 0.600232.
Train: 2018-07-31T09:50:39.247935: step 13796, loss 0.535496.
Train: 2018-07-31T09:50:39.419774: step 13797, loss 0.553902.
Train: 2018-07-31T09:50:39.591605: step 13798, loss 0.526098.
Train: 2018-07-31T09:50:39.747821: step 13799, loss 0.591246.
Train: 2018-07-31T09:50:39.919626: step 13800, loss 0.581655.
Test: 2018-07-31T09:50:40.169569: step 13800, loss 0.547578.
Train: 2018-07-31T09:50:40.935046: step 13801, loss 0.53515.
Train: 2018-07-31T09:50:41.106879: step 13802, loss 0.516903.
Train: 2018-07-31T09:50:41.263092: step 13803, loss 0.535269.
Train: 2018-07-31T09:50:41.434924: step 13804, loss 0.59947.
Train: 2018-07-31T09:50:41.606756: step 13805, loss 0.580876.
Train: 2018-07-31T09:50:41.778597: step 13806, loss 0.544485.
Train: 2018-07-31T09:50:41.934779: step 13807, loss 0.499057.
Train: 2018-07-31T09:50:42.106640: step 13808, loss 0.526791.
Train: 2018-07-31T09:50:42.278480: step 13809, loss 0.590444.
Train: 2018-07-31T09:50:42.450284: step 13810, loss 0.562746.
Test: 2018-07-31T09:50:42.684638: step 13810, loss 0.547572.
Train: 2018-07-31T09:50:42.840842: step 13811, loss 0.562476.
Train: 2018-07-31T09:50:43.028304: step 13812, loss 0.599415.
Train: 2018-07-31T09:50:43.200109: step 13813, loss 0.581038.
Train: 2018-07-31T09:50:43.356323: step 13814, loss 0.508436.
Train: 2018-07-31T09:50:43.528187: step 13815, loss 0.517711.
Train: 2018-07-31T09:50:43.699992: step 13816, loss 0.616854.
Train: 2018-07-31T09:50:43.871857: step 13817, loss 0.60743.
Train: 2018-07-31T09:50:44.028064: step 13818, loss 0.643824.
Train: 2018-07-31T09:50:44.199874: step 13819, loss 0.562437.
Train: 2018-07-31T09:50:44.371711: step 13820, loss 0.616147.
Test: 2018-07-31T09:50:44.606056: step 13820, loss 0.547675.
Train: 2018-07-31T09:50:44.777864: step 13821, loss 0.518431.
Train: 2018-07-31T09:50:44.949699: step 13822, loss 0.562732.
Train: 2018-07-31T09:50:45.105913: step 13823, loss 0.597604.
Train: 2018-07-31T09:50:45.277778: step 13824, loss 0.544865.
Train: 2018-07-31T09:50:45.449606: step 13825, loss 0.64964.
Train: 2018-07-31T09:50:45.605825: step 13826, loss 0.571428.
Train: 2018-07-31T09:50:45.777631: step 13827, loss 0.527916.
Train: 2018-07-31T09:50:45.933844: step 13828, loss 0.519418.
Train: 2018-07-31T09:50:46.105679: step 13829, loss 0.545015.
Train: 2018-07-31T09:50:46.261892: step 13830, loss 0.570978.
Test: 2018-07-31T09:50:46.511835: step 13830, loss 0.548073.
Train: 2018-07-31T09:50:46.668076: step 13831, loss 0.622433.
Train: 2018-07-31T09:50:46.839911: step 13832, loss 0.60457.
Train: 2018-07-31T09:50:47.011716: step 13833, loss 0.486489.
Train: 2018-07-31T09:50:47.167960: step 13834, loss 0.562738.
Train: 2018-07-31T09:50:47.339794: step 13835, loss 0.511489.
Train: 2018-07-31T09:50:47.511598: step 13836, loss 0.562682.
Train: 2018-07-31T09:50:47.667843: step 13837, loss 0.554058.
Train: 2018-07-31T09:50:47.839677: step 13838, loss 0.57949.
Train: 2018-07-31T09:50:47.995861: step 13839, loss 0.553911.
Train: 2018-07-31T09:50:48.167726: step 13840, loss 0.536163.
Test: 2018-07-31T09:50:48.402047: step 13840, loss 0.548183.
Train: 2018-07-31T09:50:48.573883: step 13841, loss 0.621762.
Train: 2018-07-31T09:50:48.745686: step 13842, loss 0.553959.
Train: 2018-07-31T09:50:48.917544: step 13843, loss 0.536653.
Train: 2018-07-31T09:50:49.073766: step 13844, loss 0.579666.
Train: 2018-07-31T09:50:49.245603: step 13845, loss 0.579876.
Train: 2018-07-31T09:50:49.417403: step 13846, loss 0.545656.
Train: 2018-07-31T09:50:49.573616: step 13847, loss 0.605163.
Train: 2018-07-31T09:50:49.745482: step 13848, loss 0.604811.
Train: 2018-07-31T09:50:49.917321: step 13849, loss 0.545575.
Train: 2018-07-31T09:50:50.089121: step 13850, loss 0.520146.
Test: 2018-07-31T09:50:50.323440: step 13850, loss 0.548236.
Train: 2018-07-31T09:50:50.495274: step 13851, loss 0.570741.
Train: 2018-07-31T09:50:50.667110: step 13852, loss 0.570897.
Train: 2018-07-31T09:50:50.823348: step 13853, loss 0.570853.
Train: 2018-07-31T09:50:50.995188: step 13854, loss 0.56256.
Train: 2018-07-31T09:50:51.151423: step 13855, loss 0.561913.
Train: 2018-07-31T09:50:51.323240: step 13856, loss 0.570858.
Train: 2018-07-31T09:50:51.495071: step 13857, loss 0.596381.
Train: 2018-07-31T09:50:51.651285: step 13858, loss 0.528478.
Train: 2018-07-31T09:50:51.823089: step 13859, loss 0.545401.
Train: 2018-07-31T09:50:51.994954: step 13860, loss 0.579256.
Test: 2018-07-31T09:50:52.229275: step 13860, loss 0.54827.
Train: 2018-07-31T09:50:52.401079: step 13861, loss 0.596262.
Train: 2018-07-31T09:50:52.557292: step 13862, loss 0.612993.
Train: 2018-07-31T09:50:52.729158: step 13863, loss 0.537322.
Train: 2018-07-31T09:50:52.885365: step 13864, loss 0.56268.
Train: 2018-07-31T09:50:53.057175: step 13865, loss 0.570328.
Train: 2018-07-31T09:50:53.213388: step 13866, loss 0.553986.
Train: 2018-07-31T09:50:53.385253: step 13867, loss 0.478263.
Train: 2018-07-31T09:50:53.557090: step 13868, loss 0.561817.
Train: 2018-07-31T09:50:53.728893: step 13869, loss 0.536876.
Train: 2018-07-31T09:50:53.885137: step 13870, loss 0.495064.
Test: 2018-07-31T09:50:54.119428: step 13870, loss 0.548269.
Train: 2018-07-31T09:50:54.291292: step 13871, loss 0.587702.
Train: 2018-07-31T09:50:54.463126: step 13872, loss 0.630166.
Train: 2018-07-31T09:50:54.619340: step 13873, loss 0.613572.
Train: 2018-07-31T09:50:54.822420: step 13874, loss 0.528508.
Train: 2018-07-31T09:50:54.994221: step 13875, loss 0.68965.
Train: 2018-07-31T09:50:55.166056: step 13876, loss 0.553654.
Train: 2018-07-31T09:50:55.322270: step 13877, loss 0.536473.
Train: 2018-07-31T09:50:55.478513: step 13878, loss 0.504039.
Train: 2018-07-31T09:50:55.650349: step 13879, loss 0.638171.
Train: 2018-07-31T09:50:55.822153: step 13880, loss 0.57866.
Test: 2018-07-31T09:50:56.056502: step 13880, loss 0.548408.
Train: 2018-07-31T09:50:56.228307: step 13881, loss 0.54643.
Train: 2018-07-31T09:50:56.384557: step 13882, loss 0.604652.
Train: 2018-07-31T09:50:56.556355: step 13883, loss 0.562199.
Train: 2018-07-31T09:50:56.712569: step 13884, loss 0.629356.
Train: 2018-07-31T09:50:56.884434: step 13885, loss 0.604864.
Train: 2018-07-31T09:50:57.056239: step 13886, loss 0.537804.
Train: 2018-07-31T09:50:57.212452: step 13887, loss 0.562124.
Train: 2018-07-31T09:50:57.384287: step 13888, loss 0.546362.
Train: 2018-07-31T09:50:57.540500: step 13889, loss 0.596438.
Train: 2018-07-31T09:50:57.696747: step 13890, loss 0.537575.
Test: 2018-07-31T09:50:57.946654: step 13890, loss 0.548792.
Train: 2018-07-31T09:50:58.118489: step 13891, loss 0.570486.
Train: 2018-07-31T09:50:58.274734: step 13892, loss 0.595453.
Train: 2018-07-31T09:50:58.446538: step 13893, loss 0.571036.
Train: 2018-07-31T09:50:58.602781: step 13894, loss 0.522447.
Train: 2018-07-31T09:50:58.774586: step 13895, loss 0.570826.
Train: 2018-07-31T09:50:58.930824: step 13896, loss 0.568131.
Train: 2018-07-31T09:50:59.087013: step 13897, loss 0.513526.
Train: 2018-07-31T09:50:59.258878: step 13898, loss 0.611564.
Train: 2018-07-31T09:50:59.415062: step 13899, loss 0.536973.
Train: 2018-07-31T09:50:59.586926: step 13900, loss 0.610866.
Test: 2018-07-31T09:50:59.821251: step 13900, loss 0.548945.
Train: 2018-07-31T09:51:00.555434: step 13901, loss 0.562367.
Train: 2018-07-31T09:51:00.711664: step 13902, loss 0.512126.
Train: 2018-07-31T09:51:00.883467: step 13903, loss 0.586932.
Train: 2018-07-31T09:51:01.039681: step 13904, loss 0.545452.
Train: 2018-07-31T09:51:01.211546: step 13905, loss 0.564153.
Train: 2018-07-31T09:51:01.367759: step 13906, loss 0.577109.
Train: 2018-07-31T09:51:01.539565: step 13907, loss 0.582417.
Train: 2018-07-31T09:51:01.711398: step 13908, loss 0.569717.
Train: 2018-07-31T09:51:01.867642: step 13909, loss 0.546678.
Train: 2018-07-31T09:51:02.039447: step 13910, loss 0.49801.
Test: 2018-07-31T09:51:02.273775: step 13910, loss 0.54874.
Train: 2018-07-31T09:51:02.445634: step 13911, loss 0.637318.
Train: 2018-07-31T09:51:02.601845: step 13912, loss 0.530581.
Train: 2018-07-31T09:51:02.773649: step 13913, loss 0.56193.
Train: 2018-07-31T09:51:02.929863: step 13914, loss 0.596183.
Train: 2018-07-31T09:51:03.086076: step 13915, loss 0.536552.
Train: 2018-07-31T09:51:03.257911: step 13916, loss 0.50323.
Train: 2018-07-31T09:51:03.414126: step 13917, loss 0.563364.
Train: 2018-07-31T09:51:03.570371: step 13918, loss 0.605338.
Train: 2018-07-31T09:51:03.742203: step 13919, loss 0.527548.
Train: 2018-07-31T09:51:03.898417: step 13920, loss 0.570795.
Test: 2018-07-31T09:51:04.132707: step 13920, loss 0.548086.
Train: 2018-07-31T09:51:04.304566: step 13921, loss 0.578326.
Train: 2018-07-31T09:51:04.460755: step 13922, loss 0.596637.
Train: 2018-07-31T09:51:04.616967: step 13923, loss 0.57986.
Train: 2018-07-31T09:51:04.773211: step 13924, loss 0.586744.
Train: 2018-07-31T09:51:04.945017: step 13925, loss 0.595817.
Train: 2018-07-31T09:51:05.101260: step 13926, loss 0.657091.
Train: 2018-07-31T09:51:05.273095: step 13927, loss 0.529644.
Train: 2018-07-31T09:51:05.429308: step 13928, loss 0.605009.
Train: 2018-07-31T09:51:05.601143: step 13929, loss 0.611929.
Train: 2018-07-31T09:51:05.757357: step 13930, loss 0.49424.
Test: 2018-07-31T09:51:05.991646: step 13930, loss 0.548415.
Train: 2018-07-31T09:51:06.163506: step 13931, loss 0.521845.
Train: 2018-07-31T09:51:06.319695: step 13932, loss 0.530031.
Train: 2018-07-31T09:51:06.491530: step 13933, loss 0.529509.
Train: 2018-07-31T09:51:06.663363: step 13934, loss 0.525849.
Train: 2018-07-31T09:51:06.835225: step 13935, loss 0.544751.
Train: 2018-07-31T09:51:06.991438: step 13936, loss 0.552646.
Train: 2018-07-31T09:51:07.163271: step 13937, loss 0.571876.
Train: 2018-07-31T09:51:07.335112: step 13938, loss 0.520466.
Train: 2018-07-31T09:51:07.491326: step 13939, loss 0.570076.
Train: 2018-07-31T09:51:07.647509: step 13940, loss 0.511847.
Test: 2018-07-31T09:51:07.897481: step 13940, loss 0.547976.
Train: 2018-07-31T09:51:08.053664: step 13941, loss 0.622635.
Train: 2018-07-31T09:51:08.225497: step 13942, loss 0.596959.
Train: 2018-07-31T09:51:08.381743: step 13943, loss 0.526882.
Train: 2018-07-31T09:51:08.553574: step 13944, loss 0.597554.
Train: 2018-07-31T09:51:08.709760: step 13945, loss 0.544994.
Train: 2018-07-31T09:51:08.866003: step 13946, loss 0.527395.
Train: 2018-07-31T09:51:09.037838: step 13947, loss 0.563697.
Train: 2018-07-31T09:51:09.194022: step 13948, loss 0.572029.
Train: 2018-07-31T09:51:09.365856: step 13949, loss 0.573085.
Train: 2018-07-31T09:51:09.522100: step 13950, loss 0.640354.
Test: 2018-07-31T09:51:09.756421: step 13950, loss 0.547846.
Train: 2018-07-31T09:51:09.928225: step 13951, loss 0.535944.
Train: 2018-07-31T09:51:10.084438: step 13952, loss 0.578872.
Train: 2018-07-31T09:51:10.240651: step 13953, loss 0.467187.
Train: 2018-07-31T09:51:10.412498: step 13954, loss 0.562782.
Train: 2018-07-31T09:51:10.568726: step 13955, loss 0.536745.
Train: 2018-07-31T09:51:10.724913: step 13956, loss 0.545204.
Train: 2018-07-31T09:51:10.896778: step 13957, loss 0.639845.
Train: 2018-07-31T09:51:11.052994: step 13958, loss 0.62259.
Train: 2018-07-31T09:51:11.224826: step 13959, loss 0.605037.
Train: 2018-07-31T09:51:11.381010: step 13960, loss 0.589554.
Test: 2018-07-31T09:51:11.615332: step 13960, loss 0.547934.
Train: 2018-07-31T09:51:11.771543: step 13961, loss 0.511096.
Train: 2018-07-31T09:51:11.943378: step 13962, loss 0.545377.
Train: 2018-07-31T09:51:12.099592: step 13963, loss 0.664596.
Train: 2018-07-31T09:51:12.271450: step 13964, loss 0.528506.
Train: 2018-07-31T09:51:12.427639: step 13965, loss 0.681426.
Train: 2018-07-31T09:51:12.599505: step 13966, loss 0.537851.
Train: 2018-07-31T09:51:12.755688: step 13967, loss 0.554251.
Train: 2018-07-31T09:51:12.911931: step 13968, loss 0.596299.
Train: 2018-07-31T09:51:13.083736: step 13969, loss 0.546255.
Train: 2018-07-31T09:51:13.239980: step 13970, loss 0.528361.
Test: 2018-07-31T09:51:13.474300: step 13970, loss 0.548519.
Train: 2018-07-31T09:51:13.646103: step 13971, loss 0.54689.
Train: 2018-07-31T09:51:13.802348: step 13972, loss 0.586933.
Train: 2018-07-31T09:51:13.974179: step 13973, loss 0.647692.
Train: 2018-07-31T09:51:14.130395: step 13974, loss 0.588419.
Train: 2018-07-31T09:51:14.286609: step 13975, loss 0.521489.
Train: 2018-07-31T09:51:14.442793: step 13976, loss 0.65466.
Train: 2018-07-31T09:51:14.599005: step 13977, loss 0.572073.
Train: 2018-07-31T09:51:14.755220: step 13978, loss 0.611742.
Train: 2018-07-31T09:51:14.927053: step 13979, loss 0.586325.
Train: 2018-07-31T09:51:15.083298: step 13980, loss 0.546255.
Test: 2018-07-31T09:51:15.317589: step 13980, loss 0.549095.
Train: 2018-07-31T09:51:15.489453: step 13981, loss 0.572862.
Train: 2018-07-31T09:51:15.661258: step 13982, loss 0.627404.
Train: 2018-07-31T09:51:15.833122: step 13983, loss 0.5803.
Train: 2018-07-31T09:51:15.989336: step 13984, loss 0.547058.
Train: 2018-07-31T09:51:16.145518: step 13985, loss 0.602749.
Train: 2018-07-31T09:51:16.301762: step 13986, loss 0.53853.
Train: 2018-07-31T09:51:16.473567: step 13987, loss 0.522829.
Train: 2018-07-31T09:51:16.629780: step 13988, loss 0.555022.
Train: 2018-07-31T09:51:16.785993: step 13989, loss 0.577651.
Train: 2018-07-31T09:51:16.942240: step 13990, loss 0.569862.
Test: 2018-07-31T09:51:17.192149: step 13990, loss 0.549292.
Train: 2018-07-31T09:51:17.348387: step 13991, loss 0.449736.
Train: 2018-07-31T09:51:17.504606: step 13992, loss 0.552928.
Train: 2018-07-31T09:51:17.676409: step 13993, loss 0.570982.
Train: 2018-07-31T09:51:17.832648: step 13994, loss 0.577949.
Train: 2018-07-31T09:51:18.004458: step 13995, loss 0.522052.
Train: 2018-07-31T09:51:18.160671: step 13996, loss 0.612546.
Train: 2018-07-31T09:51:18.316918: step 13997, loss 0.545602.
Train: 2018-07-31T09:51:18.473129: step 13998, loss 0.486911.
Train: 2018-07-31T09:51:18.644933: step 13999, loss 0.56247.
Train: 2018-07-31T09:51:18.801180: step 14000, loss 0.562892.
Test: 2018-07-31T09:51:19.035468: step 14000, loss 0.548298.
Train: 2018-07-31T09:51:19.785327: step 14001, loss 0.579265.
Train: 2018-07-31T09:51:19.957125: step 14002, loss 0.485624.
Train: 2018-07-31T09:51:20.113375: step 14003, loss 0.503027.
Train: 2018-07-31T09:51:20.269583: step 14004, loss 0.528898.
Train: 2018-07-31T09:51:20.425796: step 14005, loss 0.554548.
Train: 2018-07-31T09:51:20.581981: step 14006, loss 0.588886.
Train: 2018-07-31T09:51:20.753845: step 14007, loss 0.553541.
Train: 2018-07-31T09:51:20.910029: step 14008, loss 0.579948.
Train: 2018-07-31T09:51:21.066275: step 14009, loss 0.579994.
Train: 2018-07-31T09:51:21.238106: step 14010, loss 0.570686.
Test: 2018-07-31T09:51:21.468313: step 14010, loss 0.547741.
Train: 2018-07-31T09:51:21.640176: step 14011, loss 0.544325.
Train: 2018-07-31T09:51:21.796385: step 14012, loss 0.482243.
Train: 2018-07-31T09:51:21.952603: step 14013, loss 0.553991.
Train: 2018-07-31T09:51:22.108793: step 14014, loss 0.509363.
Train: 2018-07-31T09:51:22.265006: step 14015, loss 0.56255.
Train: 2018-07-31T09:51:22.436840: step 14016, loss 0.571783.
Train: 2018-07-31T09:51:22.593053: step 14017, loss 0.499781.
Train: 2018-07-31T09:51:22.749297: step 14018, loss 0.535329.
Train: 2018-07-31T09:51:22.905481: step 14019, loss 0.590294.
Train: 2018-07-31T09:51:23.061725: step 14020, loss 0.535282.
Test: 2018-07-31T09:51:23.296044: step 14020, loss 0.547586.
Train: 2018-07-31T09:51:23.467879: step 14021, loss 0.544707.
Train: 2018-07-31T09:51:23.624061: step 14022, loss 0.553588.
Train: 2018-07-31T09:51:23.780309: step 14023, loss 0.599268.
Train: 2018-07-31T09:51:23.936488: step 14024, loss 0.535869.
Train: 2018-07-31T09:51:24.108323: step 14025, loss 0.635288.
Train: 2018-07-31T09:51:24.264569: step 14026, loss 0.590248.
Train: 2018-07-31T09:51:24.420780: step 14027, loss 0.59928.
Train: 2018-07-31T09:51:24.576994: step 14028, loss 0.581791.
Train: 2018-07-31T09:51:24.748826: step 14029, loss 0.589845.
Train: 2018-07-31T09:51:24.905042: step 14030, loss 0.499605.
Test: 2018-07-31T09:51:25.139365: step 14030, loss 0.547609.
Train: 2018-07-31T09:51:25.311167: step 14031, loss 0.50865.
Train: 2018-07-31T09:51:25.467410: step 14032, loss 0.526947.
Train: 2018-07-31T09:51:25.623626: step 14033, loss 0.554025.
Train: 2018-07-31T09:51:25.795458: step 14034, loss 0.607381.
Train: 2018-07-31T09:51:25.951642: step 14035, loss 0.625091.
Train: 2018-07-31T09:51:26.107855: step 14036, loss 0.50945.
Train: 2018-07-31T09:51:26.279691: step 14037, loss 0.562647.
Train: 2018-07-31T09:51:26.435905: step 14038, loss 0.535743.
Train: 2018-07-31T09:51:26.592147: step 14039, loss 0.553941.
Train: 2018-07-31T09:51:26.748363: step 14040, loss 0.491604.
Test: 2018-07-31T09:51:26.982684: step 14040, loss 0.547673.
Train: 2018-07-31T09:51:27.138863: step 14041, loss 0.509122.
Train: 2018-07-31T09:51:27.310734: step 14042, loss 0.53581.
Train: 2018-07-31T09:51:27.466942: step 14043, loss 0.600316.
Train: 2018-07-31T09:51:27.623126: step 14044, loss 0.553409.
Train: 2018-07-31T09:51:27.794960: step 14045, loss 0.562203.
Train: 2018-07-31T09:51:27.951198: step 14046, loss 0.589254.
Train: 2018-07-31T09:51:28.107412: step 14047, loss 0.482404.
Train: 2018-07-31T09:51:28.263633: step 14048, loss 0.526659.
Train: 2018-07-31T09:51:28.419813: step 14049, loss 0.553328.
Train: 2018-07-31T09:51:28.591649: step 14050, loss 0.553537.
Test: 2018-07-31T09:51:28.826000: step 14050, loss 0.547628.
Train: 2018-07-31T09:51:28.982183: step 14051, loss 0.55374.
Train: 2018-07-31T09:51:29.138428: step 14052, loss 0.56285.
Train: 2018-07-31T09:51:29.294608: step 14053, loss 0.535675.
Train: 2018-07-31T09:51:29.450853: step 14054, loss 0.580898.
Train: 2018-07-31T09:51:29.622690: step 14055, loss 0.535561.
Train: 2018-07-31T09:51:29.778871: step 14056, loss 0.517678.
Train: 2018-07-31T09:51:29.950735: step 14057, loss 0.625606.
Train: 2018-07-31T09:51:30.106920: step 14058, loss 0.607742.
Train: 2018-07-31T09:51:30.263157: step 14059, loss 0.535735.
Train: 2018-07-31T09:51:30.419375: step 14060, loss 0.50028.
Test: 2018-07-31T09:51:30.653666: step 14060, loss 0.547615.
Train: 2018-07-31T09:51:30.809909: step 14061, loss 0.598699.
Train: 2018-07-31T09:51:30.981744: step 14062, loss 0.633936.
Train: 2018-07-31T09:51:31.153573: step 14063, loss 0.552991.
Train: 2018-07-31T09:51:31.309786: step 14064, loss 0.500032.
Train: 2018-07-31T09:51:31.465976: step 14065, loss 0.553598.
Train: 2018-07-31T09:51:31.622190: step 14066, loss 0.571758.
Train: 2018-07-31T09:51:31.794054: step 14067, loss 0.598551.
Train: 2018-07-31T09:51:31.950268: step 14068, loss 0.464481.
Train: 2018-07-31T09:51:32.106475: step 14069, loss 0.633627.
Train: 2018-07-31T09:51:32.262688: step 14070, loss 0.633285.
Test: 2018-07-31T09:51:32.512606: step 14070, loss 0.547686.
Train: 2018-07-31T09:51:32.668819: step 14071, loss 0.651045.
Train: 2018-07-31T09:51:32.825068: step 14072, loss 0.518264.
Train: 2018-07-31T09:51:32.996868: step 14073, loss 0.553937.
Train: 2018-07-31T09:51:33.153111: step 14074, loss 0.579885.
Train: 2018-07-31T09:51:33.324941: step 14075, loss 0.562372.
Train: 2018-07-31T09:51:33.481128: step 14076, loss 0.579503.
Train: 2018-07-31T09:51:33.637343: step 14077, loss 0.579918.
Train: 2018-07-31T09:51:33.809176: step 14078, loss 0.483952.
Train: 2018-07-31T09:51:33.965390: step 14079, loss 0.604981.
Train: 2018-07-31T09:51:34.121637: step 14080, loss 0.501863.
Test: 2018-07-31T09:51:34.355958: step 14080, loss 0.547974.
Train: 2018-07-31T09:51:34.527758: step 14081, loss 0.562331.
Train: 2018-07-31T09:51:34.684003: step 14082, loss 0.580094.
Train: 2018-07-31T09:51:34.855833: step 14083, loss 0.604873.
Train: 2018-07-31T09:51:35.012052: step 14084, loss 0.501883.
Train: 2018-07-31T09:51:35.183855: step 14085, loss 0.570665.
Train: 2018-07-31T09:51:35.340099: step 14086, loss 0.579576.
Train: 2018-07-31T09:51:35.496281: step 14087, loss 0.519244.
Train: 2018-07-31T09:51:35.668142: step 14088, loss 0.553119.
Train: 2018-07-31T09:51:35.824360: step 14089, loss 0.510849.
Train: 2018-07-31T09:51:35.996197: step 14090, loss 0.544873.
Test: 2018-07-31T09:51:36.230485: step 14090, loss 0.547992.
Train: 2018-07-31T09:51:36.386731: step 14091, loss 0.57964.
Train: 2018-07-31T09:51:36.558564: step 14092, loss 0.519613.
Train: 2018-07-31T09:51:36.714747: step 14093, loss 0.632137.
Train: 2018-07-31T09:51:36.886582: step 14094, loss 0.657615.
Train: 2018-07-31T09:51:37.042794: step 14095, loss 0.552616.
Train: 2018-07-31T09:51:37.214629: step 14096, loss 0.544724.
Train: 2018-07-31T09:51:37.370881: step 14097, loss 0.501541.
Train: 2018-07-31T09:51:37.542679: step 14098, loss 0.554274.
Train: 2018-07-31T09:51:37.698916: step 14099, loss 0.587407.
Train: 2018-07-31T09:51:37.870758: step 14100, loss 0.544562.
Test: 2018-07-31T09:51:38.105079: step 14100, loss 0.54806.
Train: 2018-07-31T09:51:38.839250: step 14101, loss 0.554557.
Train: 2018-07-31T09:51:39.011083: step 14102, loss 0.572684.
Train: 2018-07-31T09:51:39.167327: step 14103, loss 0.603828.
Train: 2018-07-31T09:51:39.323510: step 14104, loss 0.546164.
Train: 2018-07-31T09:51:39.495345: step 14105, loss 0.562267.
Train: 2018-07-31T09:51:39.651560: step 14106, loss 0.56271.
Train: 2018-07-31T09:51:39.823420: step 14107, loss 0.562016.
Train: 2018-07-31T09:51:39.995229: step 14108, loss 0.511526.
Train: 2018-07-31T09:51:40.151472: step 14109, loss 0.511514.
Train: 2018-07-31T09:51:40.323276: step 14110, loss 0.587911.
Test: 2018-07-31T09:51:40.557597: step 14110, loss 0.548031.
Train: 2018-07-31T09:51:40.713841: step 14111, loss 0.596929.
Train: 2018-07-31T09:51:40.901291: step 14112, loss 0.622382.
Train: 2018-07-31T09:51:41.057509: step 14113, loss 0.52819.
Train: 2018-07-31T09:51:41.213693: step 14114, loss 0.581032.
Train: 2018-07-31T09:51:41.385527: step 14115, loss 0.502501.
Train: 2018-07-31T09:51:41.541742: step 14116, loss 0.594772.
Train: 2018-07-31T09:51:41.713575: step 14117, loss 0.518927.
Train: 2018-07-31T09:51:41.885411: step 14118, loss 0.631465.
Train: 2018-07-31T09:51:42.041649: step 14119, loss 0.6138.
Train: 2018-07-31T09:51:42.197868: step 14120, loss 0.5628.
Test: 2018-07-31T09:51:42.447809: step 14120, loss 0.548086.
Train: 2018-07-31T09:51:42.604017: step 14121, loss 0.562644.
Train: 2018-07-31T09:51:42.760238: step 14122, loss 0.579644.
Train: 2018-07-31T09:51:42.932070: step 14123, loss 0.604469.
Train: 2018-07-31T09:51:43.088254: step 14124, loss 0.57933.
Train: 2018-07-31T09:51:43.260089: step 14125, loss 0.545365.
Train: 2018-07-31T09:51:43.416332: step 14126, loss 0.469471.
Train: 2018-07-31T09:51:43.588137: step 14127, loss 0.528554.
Train: 2018-07-31T09:51:43.759972: step 14128, loss 0.536795.
Train: 2018-07-31T09:51:43.916209: step 14129, loss 0.595176.
Train: 2018-07-31T09:51:44.088050: step 14130, loss 0.570773.
Test: 2018-07-31T09:51:44.322371: step 14130, loss 0.548231.
Train: 2018-07-31T09:51:44.478583: step 14131, loss 0.553186.
Train: 2018-07-31T09:51:44.650388: step 14132, loss 0.640009.
Train: 2018-07-31T09:51:44.806602: step 14133, loss 0.545773.
Train: 2018-07-31T09:51:44.978438: step 14134, loss 0.537416.
Train: 2018-07-31T09:51:45.134682: step 14135, loss 0.554594.
Train: 2018-07-31T09:51:45.306485: step 14136, loss 0.545956.
Train: 2018-07-31T09:51:45.478332: step 14137, loss 0.537839.
Train: 2018-07-31T09:51:45.634533: step 14138, loss 0.527886.
Train: 2018-07-31T09:51:45.790770: step 14139, loss 0.570254.
Train: 2018-07-31T09:51:45.962612: step 14140, loss 0.554657.
Test: 2018-07-31T09:51:46.196927: step 14140, loss 0.548003.
Train: 2018-07-31T09:51:46.368766: step 14141, loss 0.589146.
Train: 2018-07-31T09:51:46.524980: step 14142, loss 0.579975.
Train: 2018-07-31T09:51:46.696815: step 14143, loss 0.527503.
Train: 2018-07-31T09:51:46.868651: step 14144, loss 0.571335.
Train: 2018-07-31T09:51:47.024832: step 14145, loss 0.484874.
Train: 2018-07-31T09:51:47.196666: step 14146, loss 0.509791.
Train: 2018-07-31T09:51:47.368534: step 14147, loss 0.527324.
Train: 2018-07-31T09:51:47.524715: step 14148, loss 0.597188.
Train: 2018-07-31T09:51:47.680929: step 14149, loss 0.56301.
Train: 2018-07-31T09:51:47.852764: step 14150, loss 0.52688.
Test: 2018-07-31T09:51:48.087085: step 14150, loss 0.547686.
Train: 2018-07-31T09:51:48.243296: step 14151, loss 0.535311.
Train: 2018-07-31T09:51:48.415165: step 14152, loss 0.499759.
Train: 2018-07-31T09:51:48.571344: step 14153, loss 0.508861.
Train: 2018-07-31T09:51:48.743212: step 14154, loss 0.553361.
Train: 2018-07-31T09:51:48.915045: step 14155, loss 0.59957.
Train: 2018-07-31T09:51:49.086850: step 14156, loss 0.544816.
Train: 2018-07-31T09:51:49.243095: step 14157, loss 0.544636.
Train: 2018-07-31T09:51:49.414897: step 14158, loss 0.525754.
Train: 2018-07-31T09:51:49.571141: step 14159, loss 0.598966.
Train: 2018-07-31T09:51:49.742945: step 14160, loss 0.553618.
Test: 2018-07-31T09:51:49.977297: step 14160, loss 0.54758.
Train: 2018-07-31T09:51:50.149125: step 14161, loss 0.628316.
Train: 2018-07-31T09:51:50.305343: step 14162, loss 0.608544.
Train: 2018-07-31T09:51:50.461563: step 14163, loss 0.656005.
Train: 2018-07-31T09:51:50.633394: step 14164, loss 0.562469.
Train: 2018-07-31T09:51:50.805227: step 14165, loss 0.662369.
Train: 2018-07-31T09:51:50.961441: step 14166, loss 0.508168.
Train: 2018-07-31T09:51:51.133277: step 14167, loss 0.57033.
Train: 2018-07-31T09:51:51.305109: step 14168, loss 0.545215.
Train: 2018-07-31T09:51:51.461323: step 14169, loss 0.562618.
Train: 2018-07-31T09:51:51.633160: step 14170, loss 0.553155.
Test: 2018-07-31T09:51:51.867482: step 14170, loss 0.548052.
Train: 2018-07-31T09:51:52.039313: step 14171, loss 0.460773.
Train: 2018-07-31T09:51:52.195527: step 14172, loss 0.520689.
Train: 2018-07-31T09:51:52.351740: step 14173, loss 0.606375.
Train: 2018-07-31T09:51:52.523574: step 14174, loss 0.563527.
Train: 2018-07-31T09:51:52.695378: step 14175, loss 0.561959.
Train: 2018-07-31T09:51:52.851623: step 14176, loss 0.640691.
Train: 2018-07-31T09:51:53.023428: step 14177, loss 0.553707.
Train: 2018-07-31T09:51:53.195262: step 14178, loss 0.545464.
Train: 2018-07-31T09:51:53.367125: step 14179, loss 0.588186.
Train: 2018-07-31T09:51:53.523311: step 14180, loss 0.570887.
Test: 2018-07-31T09:51:53.757664: step 14180, loss 0.548021.
Train: 2018-07-31T09:51:53.929495: step 14181, loss 0.639563.
Train: 2018-07-31T09:51:54.101336: step 14182, loss 0.511534.
Train: 2018-07-31T09:51:54.273166: step 14183, loss 0.604709.
Train: 2018-07-31T09:51:54.429347: step 14184, loss 0.528534.
Train: 2018-07-31T09:51:54.601213: step 14185, loss 0.545294.
Train: 2018-07-31T09:51:54.773048: step 14186, loss 0.570584.
Train: 2018-07-31T09:51:54.929261: step 14187, loss 0.537742.
Train: 2018-07-31T09:51:55.101066: step 14188, loss 0.554071.
Train: 2018-07-31T09:51:55.257312: step 14189, loss 0.571408.
Train: 2018-07-31T09:51:55.429139: step 14190, loss 0.689031.
Test: 2018-07-31T09:51:55.679056: step 14190, loss 0.548342.
Train: 2018-07-31T09:51:55.835299: step 14191, loss 0.520321.
Train: 2018-07-31T09:51:56.007136: step 14192, loss 0.579153.
Train: 2018-07-31T09:51:56.178968: step 14193, loss 0.553981.
Train: 2018-07-31T09:51:56.350774: step 14194, loss 0.561486.
Train: 2018-07-31T09:51:56.522638: step 14195, loss 0.570784.
Train: 2018-07-31T09:51:56.678822: step 14196, loss 0.621068.
Train: 2018-07-31T09:51:56.850656: step 14197, loss 0.636849.
Train: 2018-07-31T09:51:57.022522: step 14198, loss 0.529298.
Train: 2018-07-31T09:51:57.178734: step 14199, loss 0.563192.
Train: 2018-07-31T09:51:57.350538: step 14200, loss 0.621935.
Test: 2018-07-31T09:51:57.584890: step 14200, loss 0.548868.
Train: 2018-07-31T09:51:58.381575: step 14201, loss 0.513777.
Train: 2018-07-31T09:51:58.553413: step 14202, loss 0.529193.
Train: 2018-07-31T09:51:58.725250: step 14203, loss 0.512738.
Train: 2018-07-31T09:51:58.897082: step 14204, loss 0.653767.
Train: 2018-07-31T09:51:59.068917: step 14205, loss 0.61178.
Train: 2018-07-31T09:51:59.225101: step 14206, loss 0.563424.
Train: 2018-07-31T09:51:59.396965: step 14207, loss 0.472456.
Train: 2018-07-31T09:51:59.568769: step 14208, loss 0.5958.
Train: 2018-07-31T09:51:59.740629: step 14209, loss 0.612589.
Train: 2018-07-31T09:51:59.912438: step 14210, loss 0.561684.
Test: 2018-07-31T09:52:00.146794: step 14210, loss 0.548798.
Train: 2018-07-31T09:52:00.318624: step 14211, loss 0.569934.
Train: 2018-07-31T09:52:00.490430: step 14212, loss 0.570662.
Train: 2018-07-31T09:52:00.662265: step 14213, loss 0.578804.
Train: 2018-07-31T09:52:00.818507: step 14214, loss 0.587626.
Train: 2018-07-31T09:52:00.990336: step 14215, loss 0.50552.
Train: 2018-07-31T09:52:01.162176: step 14216, loss 0.544794.
Train: 2018-07-31T09:52:01.334011: step 14217, loss 0.52995.
Train: 2018-07-31T09:52:01.505840: step 14218, loss 0.505057.
Train: 2018-07-31T09:52:01.662030: step 14219, loss 0.52982.
Train: 2018-07-31T09:52:01.833897: step 14220, loss 0.603296.
Test: 2018-07-31T09:52:02.068184: step 14220, loss 0.548453.
Train: 2018-07-31T09:52:02.240044: step 14221, loss 0.528896.
Train: 2018-07-31T09:52:02.411854: step 14222, loss 0.545639.
Train: 2018-07-31T09:52:02.568092: step 14223, loss 0.605131.
Train: 2018-07-31T09:52:02.739902: step 14224, loss 0.638214.
Train: 2018-07-31T09:52:02.911737: step 14225, loss 0.620887.
Train: 2018-07-31T09:52:03.083572: step 14226, loss 0.529044.
Train: 2018-07-31T09:52:03.239815: step 14227, loss 0.528349.
Train: 2018-07-31T09:52:03.411650: step 14228, loss 0.562966.
Train: 2018-07-31T09:52:03.583455: step 14229, loss 0.647408.
Train: 2018-07-31T09:52:03.755321: step 14230, loss 0.579484.
Test: 2018-07-31T09:52:03.989609: step 14230, loss 0.548271.
Train: 2018-07-31T09:52:04.161474: step 14231, loss 0.553741.
Train: 2018-07-31T09:52:04.333309: step 14232, loss 0.553557.
Train: 2018-07-31T09:52:04.505138: step 14233, loss 0.538252.
Train: 2018-07-31T09:52:04.661328: step 14234, loss 0.580046.
Train: 2018-07-31T09:52:04.833192: step 14235, loss 0.630421.
Train: 2018-07-31T09:52:05.005029: step 14236, loss 0.537053.
Train: 2018-07-31T09:52:05.176831: step 14237, loss 0.605635.
Train: 2018-07-31T09:52:05.333075: step 14238, loss 0.638509.
Train: 2018-07-31T09:52:05.504880: step 14239, loss 0.578951.
Train: 2018-07-31T09:52:05.676715: step 14240, loss 0.545164.
Test: 2018-07-31T09:52:05.911036: step 14240, loss 0.548488.
Train: 2018-07-31T09:52:06.082899: step 14241, loss 0.637566.
Train: 2018-07-31T09:52:06.254704: step 14242, loss 0.605016.
Train: 2018-07-31T09:52:06.426569: step 14243, loss 0.579568.
Train: 2018-07-31T09:52:06.598411: step 14244, loss 0.554111.
Train: 2018-07-31T09:52:06.754586: step 14245, loss 0.587129.
Train: 2018-07-31T09:52:06.926421: step 14246, loss 0.54684.
Train: 2018-07-31T09:52:07.082665: step 14247, loss 0.522693.
Train: 2018-07-31T09:52:07.254471: step 14248, loss 0.562208.
Train: 2018-07-31T09:52:07.426305: step 14249, loss 0.555306.
Train: 2018-07-31T09:52:07.598140: step 14250, loss 0.563231.
Test: 2018-07-31T09:52:07.832461: step 14250, loss 0.548882.
Train: 2018-07-31T09:52:08.004327: step 14251, loss 0.670072.
Train: 2018-07-31T09:52:08.176160: step 14252, loss 0.59596.
Train: 2018-07-31T09:52:08.347994: step 14253, loss 0.578578.
Train: 2018-07-31T09:52:08.519829: step 14254, loss 0.58825.
Train: 2018-07-31T09:52:08.691634: step 14255, loss 0.612028.
Train: 2018-07-31T09:52:08.863469: step 14256, loss 0.546561.
Train: 2018-07-31T09:52:09.035333: step 14257, loss 0.611789.
Train: 2018-07-31T09:52:09.207167: step 14258, loss 0.586825.
Train: 2018-07-31T09:52:09.378973: step 14259, loss 0.530847.
Train: 2018-07-31T09:52:09.550837: step 14260, loss 0.490244.
Test: 2018-07-31T09:52:09.785129: step 14260, loss 0.549278.
Train: 2018-07-31T09:52:09.956992: step 14261, loss 0.570901.
Train: 2018-07-31T09:52:10.128796: step 14262, loss 0.578596.
Train: 2018-07-31T09:52:10.285041: step 14263, loss 0.465663.
Train: 2018-07-31T09:52:10.441224: step 14264, loss 0.53041.
Train: 2018-07-31T09:52:10.613091: step 14265, loss 0.529891.
Train: 2018-07-31T09:52:10.784893: step 14266, loss 0.513477.
Train: 2018-07-31T09:52:10.941106: step 14267, loss 0.587379.
Train: 2018-07-31T09:52:11.112940: step 14268, loss 0.488002.
Train: 2018-07-31T09:52:11.284806: step 14269, loss 0.487392.
Train: 2018-07-31T09:52:11.456641: step 14270, loss 0.537186.
Test: 2018-07-31T09:52:11.690962: step 14270, loss 0.54823.
Train: 2018-07-31T09:52:11.862790: step 14271, loss 0.511381.
Train: 2018-07-31T09:52:12.034625: step 14272, loss 0.553847.
Train: 2018-07-31T09:52:12.190844: step 14273, loss 0.545151.
Train: 2018-07-31T09:52:12.362681: step 14274, loss 0.562517.
Train: 2018-07-31T09:52:12.534513: step 14275, loss 0.61473.
Train: 2018-07-31T09:52:12.706318: step 14276, loss 0.553794.
Train: 2018-07-31T09:52:12.862556: step 14277, loss 0.623983.
Train: 2018-07-31T09:52:13.034397: step 14278, loss 0.544815.
Train: 2018-07-31T09:52:13.206231: step 14279, loss 0.518623.
Train: 2018-07-31T09:52:13.378035: step 14280, loss 0.447992.
Test: 2018-07-31T09:52:13.612381: step 14280, loss 0.547698.
Train: 2018-07-31T09:52:13.799843: step 14281, loss 0.562596.
Train: 2018-07-31T09:52:13.971647: step 14282, loss 0.562561.
Train: 2018-07-31T09:52:14.127861: step 14283, loss 0.51802.
Train: 2018-07-31T09:52:14.299725: step 14284, loss 0.598556.
Train: 2018-07-31T09:52:14.471560: step 14285, loss 0.571627.
Train: 2018-07-31T09:52:14.643390: step 14286, loss 0.571778.
Train: 2018-07-31T09:52:14.799608: step 14287, loss 0.526773.
Train: 2018-07-31T09:52:14.971437: step 14288, loss 0.571731.
Train: 2018-07-31T09:52:15.158869: step 14289, loss 0.535579.
Train: 2018-07-31T09:52:15.315112: step 14290, loss 0.58971.
Test: 2018-07-31T09:52:15.549402: step 14290, loss 0.547592.
Train: 2018-07-31T09:52:15.736889: step 14291, loss 0.58064.
Train: 2018-07-31T09:52:15.893102: step 14292, loss 0.526509.
Train: 2018-07-31T09:52:16.080529: step 14293, loss 0.64384.
Train: 2018-07-31T09:52:16.236741: step 14294, loss 0.589608.
Train: 2018-07-31T09:52:16.408576: step 14295, loss 0.580751.
Train: 2018-07-31T09:52:16.580410: step 14296, loss 0.571504.
Train: 2018-07-31T09:52:16.752278: step 14297, loss 0.544535.
Train: 2018-07-31T09:52:16.924081: step 14298, loss 0.589266.
Train: 2018-07-31T09:52:17.095945: step 14299, loss 0.544917.
Train: 2018-07-31T09:52:17.283401: step 14300, loss 0.553526.
Test: 2018-07-31T09:52:17.502069: step 14300, loss 0.547694.
Train: 2018-07-31T09:52:18.251895: step 14301, loss 0.447408.
Train: 2018-07-31T09:52:18.423730: step 14302, loss 0.535782.
Train: 2018-07-31T09:52:18.595594: step 14303, loss 0.589207.
Train: 2018-07-31T09:52:18.767429: step 14304, loss 0.553989.
Train: 2018-07-31T09:52:18.939234: step 14305, loss 0.5358.
Train: 2018-07-31T09:52:19.111101: step 14306, loss 0.518524.
Train: 2018-07-31T09:52:19.282933: step 14307, loss 0.473592.
Train: 2018-07-31T09:52:19.454764: step 14308, loss 0.544642.
Train: 2018-07-31T09:52:19.626605: step 14309, loss 0.589329.
Train: 2018-07-31T09:52:19.798438: step 14310, loss 0.544798.
Test: 2018-07-31T09:52:20.032758: step 14310, loss 0.547599.
Train: 2018-07-31T09:52:20.220184: step 14311, loss 0.589355.
Train: 2018-07-31T09:52:20.376398: step 14312, loss 0.544641.
Train: 2018-07-31T09:52:20.563877: step 14313, loss 0.526614.
Train: 2018-07-31T09:52:20.720067: step 14314, loss 0.49972.
Train: 2018-07-31T09:52:20.891931: step 14315, loss 0.517112.
Train: 2018-07-31T09:52:21.063736: step 14316, loss 0.553381.
Train: 2018-07-31T09:52:21.235601: step 14317, loss 0.562741.
Train: 2018-07-31T09:52:21.407405: step 14318, loss 0.61817.
Train: 2018-07-31T09:52:21.579242: step 14319, loss 0.616531.
Train: 2018-07-31T09:52:21.751074: step 14320, loss 0.471995.
Test: 2018-07-31T09:52:22.001042: step 14320, loss 0.547576.
Train: 2018-07-31T09:52:22.172877: step 14321, loss 0.563878.
Train: 2018-07-31T09:52:22.344711: step 14322, loss 0.627259.
Train: 2018-07-31T09:52:22.516551: step 14323, loss 0.5356.
Train: 2018-07-31T09:52:22.703977: step 14324, loss 0.534867.
Train: 2018-07-31T09:52:22.860191: step 14325, loss 0.571584.
Train: 2018-07-31T09:52:23.032025: step 14326, loss 0.661053.
Train: 2018-07-31T09:52:23.203890: step 14327, loss 0.670105.
Train: 2018-07-31T09:52:23.375727: step 14328, loss 0.536099.
Train: 2018-07-31T09:52:23.547529: step 14329, loss 0.500629.
Train: 2018-07-31T09:52:23.719364: step 14330, loss 0.535913.
Test: 2018-07-31T09:52:23.953685: step 14330, loss 0.547836.
Train: 2018-07-31T09:52:24.141171: step 14331, loss 0.561793.
Train: 2018-07-31T09:52:24.344250: step 14332, loss 0.544282.
Train: 2018-07-31T09:52:24.516083: step 14333, loss 0.528959.
Train: 2018-07-31T09:52:24.687888: step 14334, loss 0.553583.
Train: 2018-07-31T09:52:24.859755: step 14335, loss 0.57024.
Train: 2018-07-31T09:52:25.031557: step 14336, loss 0.604028.
Train: 2018-07-31T09:52:25.203391: step 14337, loss 0.51134.
Train: 2018-07-31T09:52:25.375226: step 14338, loss 0.589803.
Train: 2018-07-31T09:52:25.562707: step 14339, loss 0.5182.
Train: 2018-07-31T09:52:25.734542: step 14340, loss 0.562592.
Test: 2018-07-31T09:52:25.968871: step 14340, loss 0.548068.
Train: 2018-07-31T09:52:26.140672: step 14341, loss 0.537801.
Train: 2018-07-31T09:52:26.312531: step 14342, loss 0.560747.
Train: 2018-07-31T09:52:26.484342: step 14343, loss 0.534486.
Train: 2018-07-31T09:52:26.656207: step 14344, loss 0.59716.
Train: 2018-07-31T09:52:26.828042: step 14345, loss 0.577482.
Train: 2018-07-31T09:52:26.999878: step 14346, loss 0.520946.
Train: 2018-07-31T09:52:27.171711: step 14347, loss 0.570897.
Train: 2018-07-31T09:52:27.343515: step 14348, loss 0.510851.
Train: 2018-07-31T09:52:27.515350: step 14349, loss 0.569965.
Train: 2018-07-31T09:52:27.671594: step 14350, loss 0.545401.
Test: 2018-07-31T09:52:27.905884: step 14350, loss 0.548048.
Train: 2018-07-31T09:52:28.093340: step 14351, loss 0.62423.
Train: 2018-07-31T09:52:28.265205: step 14352, loss 0.522029.
Train: 2018-07-31T09:52:28.437010: step 14353, loss 0.605744.
Train: 2018-07-31T09:52:28.593253: step 14354, loss 0.514961.
Train: 2018-07-31T09:52:28.765058: step 14355, loss 0.604417.
Train: 2018-07-31T09:52:28.936923: step 14356, loss 0.468084.
Train: 2018-07-31T09:52:29.108727: step 14357, loss 0.5445.
Train: 2018-07-31T09:52:29.280562: step 14358, loss 0.570926.
Train: 2018-07-31T09:52:29.452397: step 14359, loss 0.510516.
Train: 2018-07-31T09:52:29.624232: step 14360, loss 0.573974.
Test: 2018-07-31T09:52:29.842932: step 14360, loss 0.547852.
Train: 2018-07-31T09:52:30.014765: step 14361, loss 0.588482.
Train: 2018-07-31T09:52:30.186629: step 14362, loss 0.502832.
Train: 2018-07-31T09:52:30.358435: step 14363, loss 0.526032.
Train: 2018-07-31T09:52:30.530269: step 14364, loss 0.59754.
Train: 2018-07-31T09:52:30.702103: step 14365, loss 0.520623.
Train: 2018-07-31T09:52:30.873966: step 14366, loss 0.61239.
Train: 2018-07-31T09:52:31.045774: step 14367, loss 0.608941.
Train: 2018-07-31T09:52:31.217638: step 14368, loss 0.527123.
Train: 2018-07-31T09:52:31.389474: step 14369, loss 0.563583.
Train: 2018-07-31T09:52:31.561309: step 14370, loss 0.608934.
Test: 2018-07-31T09:52:31.795598: step 14370, loss 0.547678.
Train: 2018-07-31T09:52:31.967432: step 14371, loss 0.581834.
Train: 2018-07-31T09:52:32.154919: step 14372, loss 0.526912.
Train: 2018-07-31T09:52:32.326755: step 14373, loss 0.526606.
Train: 2018-07-31T09:52:32.498559: step 14374, loss 0.526529.
Train: 2018-07-31T09:52:32.670392: step 14375, loss 0.509487.
Train: 2018-07-31T09:52:32.842257: step 14376, loss 0.615872.
Train: 2018-07-31T09:52:33.014093: step 14377, loss 0.562869.
Train: 2018-07-31T09:52:33.185898: step 14378, loss 0.587946.
Train: 2018-07-31T09:52:33.373386: step 14379, loss 0.55252.
Train: 2018-07-31T09:52:33.545218: step 14380, loss 0.589034.
Test: 2018-07-31T09:52:33.779542: step 14380, loss 0.547675.
Train: 2018-07-31T09:52:33.951343: step 14381, loss 0.580738.
Train: 2018-07-31T09:52:34.138830: step 14382, loss 0.508999.
Train: 2018-07-31T09:52:34.310635: step 14383, loss 0.615659.
Train: 2018-07-31T09:52:34.482498: step 14384, loss 0.572016.
Train: 2018-07-31T09:52:34.654303: step 14385, loss 0.554661.
Train: 2018-07-31T09:52:34.826139: step 14386, loss 0.492621.
Train: 2018-07-31T09:52:34.998005: step 14387, loss 0.612935.
Train: 2018-07-31T09:52:35.169838: step 14388, loss 0.518921.
Train: 2018-07-31T09:52:35.357297: step 14389, loss 0.491995.
Train: 2018-07-31T09:52:35.513476: step 14390, loss 0.597822.
Test: 2018-07-31T09:52:35.763418: step 14390, loss 0.547746.
Train: 2018-07-31T09:52:35.935285: step 14391, loss 0.517163.
Train: 2018-07-31T09:52:36.107121: step 14392, loss 0.563424.
Train: 2018-07-31T09:52:36.278923: step 14393, loss 0.518359.
Train: 2018-07-31T09:52:36.450788: step 14394, loss 0.527203.
Train: 2018-07-31T09:52:36.622625: step 14395, loss 0.563213.
Train: 2018-07-31T09:52:36.794458: step 14396, loss 0.482286.
Train: 2018-07-31T09:52:36.966292: step 14397, loss 0.635132.
Train: 2018-07-31T09:52:37.138097: step 14398, loss 0.579835.
Train: 2018-07-31T09:52:37.309962: step 14399, loss 0.535558.
Train: 2018-07-31T09:52:37.481767: step 14400, loss 0.534726.
Test: 2018-07-31T09:52:37.716086: step 14400, loss 0.547625.
Train: 2018-07-31T09:52:38.450289: step 14401, loss 0.624552.
Train: 2018-07-31T09:52:38.637747: step 14402, loss 0.553733.
Train: 2018-07-31T09:52:38.809610: step 14403, loss 0.554589.
Train: 2018-07-31T09:52:38.981416: step 14404, loss 0.571563.
Train: 2018-07-31T09:52:39.153250: step 14405, loss 0.571083.
Train: 2018-07-31T09:52:39.325085: step 14406, loss 0.491735.
Train: 2018-07-31T09:52:39.496951: step 14407, loss 0.572341.
Train: 2018-07-31T09:52:39.668779: step 14408, loss 0.52739.
Train: 2018-07-31T09:52:39.840619: step 14409, loss 0.536339.
Train: 2018-07-31T09:52:40.012455: step 14410, loss 0.553027.
Test: 2018-07-31T09:52:40.262390: step 14410, loss 0.54763.
Train: 2018-07-31T09:52:40.465473: step 14411, loss 0.53593.
Train: 2018-07-31T09:52:40.637278: step 14412, loss 0.536615.
Train: 2018-07-31T09:52:40.809114: step 14413, loss 0.59038.
Train: 2018-07-31T09:52:40.980946: step 14414, loss 0.499027.
Train: 2018-07-31T09:52:41.152781: step 14415, loss 0.543859.
Train: 2018-07-31T09:52:41.308995: step 14416, loss 0.599104.
Train: 2018-07-31T09:52:41.480830: step 14417, loss 0.508465.
Train: 2018-07-31T09:52:41.652665: step 14418, loss 0.509379.
Train: 2018-07-31T09:52:41.824529: step 14419, loss 0.534651.
Train: 2018-07-31T09:52:42.011956: step 14420, loss 0.672251.
Test: 2018-07-31T09:52:42.246309: step 14420, loss 0.547589.
Train: 2018-07-31T09:52:42.433762: step 14421, loss 0.516822.
Train: 2018-07-31T09:52:42.605597: step 14422, loss 0.608053.
Train: 2018-07-31T09:52:42.777435: step 14423, loss 0.590198.
Train: 2018-07-31T09:52:42.949267: step 14424, loss 0.599861.
Train: 2018-07-31T09:52:43.121071: step 14425, loss 0.572179.
Train: 2018-07-31T09:52:43.292935: step 14426, loss 0.626255.
Train: 2018-07-31T09:52:43.464770: step 14427, loss 0.59782.
Train: 2018-07-31T09:52:43.652227: step 14428, loss 0.517794.
Train: 2018-07-31T09:52:43.824061: step 14429, loss 0.615935.
Train: 2018-07-31T09:52:43.995866: step 14430, loss 0.526748.
Test: 2018-07-31T09:52:44.230220: step 14430, loss 0.547791.
Train: 2018-07-31T09:52:44.402047: step 14431, loss 0.570738.
Train: 2018-07-31T09:52:44.573886: step 14432, loss 0.60609.
Train: 2018-07-31T09:52:44.745691: step 14433, loss 0.579625.
Train: 2018-07-31T09:52:44.917525: step 14434, loss 0.536412.
Train: 2018-07-31T09:52:45.089391: step 14435, loss 0.57024.
Train: 2018-07-31T09:52:45.261228: step 14436, loss 0.545883.
Train: 2018-07-31T09:52:45.433054: step 14437, loss 0.570935.
Train: 2018-07-31T09:52:45.620518: step 14438, loss 0.527904.
Train: 2018-07-31T09:52:45.776729: step 14439, loss 0.546295.
Train: 2018-07-31T09:52:45.948534: step 14440, loss 0.553986.
Test: 2018-07-31T09:52:46.198474: step 14440, loss 0.548165.
Train: 2018-07-31T09:52:46.370338: step 14441, loss 0.563138.
Train: 2018-07-31T09:52:46.542145: step 14442, loss 0.562029.
Train: 2018-07-31T09:52:46.714012: step 14443, loss 0.604628.
Train: 2018-07-31T09:52:46.901436: step 14444, loss 0.511935.
Train: 2018-07-31T09:52:47.073270: step 14445, loss 0.485282.
Train: 2018-07-31T09:52:47.245135: step 14446, loss 0.536301.
Train: 2018-07-31T09:52:47.416970: step 14447, loss 0.613025.
Train: 2018-07-31T09:52:47.588775: step 14448, loss 0.614513.
Train: 2018-07-31T09:52:47.776231: step 14449, loss 0.459968.
Train: 2018-07-31T09:52:47.948065: step 14450, loss 0.639834.
Test: 2018-07-31T09:52:48.182386: step 14450, loss 0.548019.
Train: 2018-07-31T09:52:48.354221: step 14451, loss 0.5706.
Train: 2018-07-31T09:52:48.526085: step 14452, loss 0.58864.
Train: 2018-07-31T09:52:48.697921: step 14453, loss 0.570079.
Train: 2018-07-31T09:52:48.869757: step 14454, loss 0.614449.
Train: 2018-07-31T09:52:49.041589: step 14455, loss 0.513235.
Train: 2018-07-31T09:52:49.213395: step 14456, loss 0.494566.
Train: 2018-07-31T09:52:49.385228: step 14457, loss 0.52978.
Train: 2018-07-31T09:52:49.557094: step 14458, loss 0.604795.
Train: 2018-07-31T09:52:49.728929: step 14459, loss 0.545124.
Train: 2018-07-31T09:52:49.900733: step 14460, loss 0.570794.
Test: 2018-07-31T09:52:50.135084: step 14460, loss 0.548019.
Train: 2018-07-31T09:52:50.322510: step 14461, loss 0.579584.
Train: 2018-07-31T09:52:50.494377: step 14462, loss 0.518999.
Train: 2018-07-31T09:52:50.666212: step 14463, loss 0.578994.
Train: 2018-07-31T09:52:50.838014: step 14464, loss 0.63187.
Train: 2018-07-31T09:52:51.009873: step 14465, loss 0.587415.
Train: 2018-07-31T09:52:51.181683: step 14466, loss 0.553903.
Train: 2018-07-31T09:52:51.353519: step 14467, loss 0.657458.
Train: 2018-07-31T09:52:51.525384: step 14468, loss 0.528688.
Train: 2018-07-31T09:52:51.697217: step 14469, loss 0.527444.
Train: 2018-07-31T09:52:51.869023: step 14470, loss 0.606475.
Test: 2018-07-31T09:52:52.103343: step 14470, loss 0.547977.
Train: 2018-07-31T09:52:52.290828: step 14471, loss 0.561067.
Train: 2018-07-31T09:52:52.447011: step 14472, loss 0.571911.
Train: 2018-07-31T09:52:52.634498: step 14473, loss 0.562323.
Train: 2018-07-31T09:52:52.806333: step 14474, loss 0.554261.
Train: 2018-07-31T09:52:52.978168: step 14475, loss 0.58831.
Train: 2018-07-31T09:52:53.149971: step 14476, loss 0.528289.
Train: 2018-07-31T09:52:53.321807: step 14477, loss 0.621947.
Train: 2018-07-31T09:52:53.478020: step 14478, loss 0.510864.
Train: 2018-07-31T09:52:53.665477: step 14479, loss 0.519565.
Train: 2018-07-31T09:52:53.821690: step 14480, loss 0.623227.
Test: 2018-07-31T09:52:54.071632: step 14480, loss 0.548164.
Train: 2018-07-31T09:52:54.243467: step 14481, loss 0.495158.
Train: 2018-07-31T09:52:54.415301: step 14482, loss 0.570586.
Train: 2018-07-31T09:52:54.587166: step 14483, loss 0.579766.
Train: 2018-07-31T09:52:54.758997: step 14484, loss 0.63077.
Train: 2018-07-31T09:52:54.930805: step 14485, loss 0.579161.
Train: 2018-07-31T09:52:55.102665: step 14486, loss 0.579526.
Train: 2018-07-31T09:52:55.274505: step 14487, loss 0.519417.
Train: 2018-07-31T09:52:55.446339: step 14488, loss 0.579194.
Train: 2018-07-31T09:52:55.618177: step 14489, loss 0.579713.
Train: 2018-07-31T09:52:55.790009: step 14490, loss 0.570246.
Test: 2018-07-31T09:52:56.024324: step 14490, loss 0.548267.
Train: 2018-07-31T09:52:56.196158: step 14491, loss 0.621877.
Train: 2018-07-31T09:52:56.367969: step 14492, loss 0.57842.
Train: 2018-07-31T09:52:56.539804: step 14493, loss 0.536903.
Train: 2018-07-31T09:52:56.711669: step 14494, loss 0.679892.
Train: 2018-07-31T09:52:56.883472: step 14495, loss 0.537684.
Train: 2018-07-31T09:52:57.039712: step 14496, loss 0.579507.
Train: 2018-07-31T09:52:57.227143: step 14497, loss 0.521227.
Train: 2018-07-31T09:52:57.399007: step 14498, loss 0.587427.
Train: 2018-07-31T09:52:57.570842: step 14499, loss 0.578419.
Train: 2018-07-31T09:52:57.742646: step 14500, loss 0.58856.
Test: 2018-07-31T09:52:57.976969: step 14500, loss 0.548936.
Train: 2018-07-31T09:52:58.726824: step 14501, loss 0.528906.
Train: 2018-07-31T09:52:58.898656: step 14502, loss 0.514194.
Train: 2018-07-31T09:52:59.070491: step 14503, loss 0.570687.
Train: 2018-07-31T09:52:59.257947: step 14504, loss 0.54638.
Train: 2018-07-31T09:52:59.429782: step 14505, loss 0.580217.
Train: 2018-07-31T09:52:59.601619: step 14506, loss 0.612433.
Train: 2018-07-31T09:52:59.773422: step 14507, loss 0.570903.
Train: 2018-07-31T09:52:59.929664: step 14508, loss 0.60324.
Train: 2018-07-31T09:53:00.101502: step 14509, loss 0.554744.
Train: 2018-07-31T09:53:00.288950: step 14510, loss 0.479107.
Test: 2018-07-31T09:53:00.523246: step 14510, loss 0.548665.
Train: 2018-07-31T09:53:00.695110: step 14511, loss 0.587326.
Train: 2018-07-31T09:53:00.851293: step 14512, loss 0.57882.
Train: 2018-07-31T09:53:01.023128: step 14513, loss 0.544866.
Train: 2018-07-31T09:53:01.194993: step 14514, loss 0.521021.
Train: 2018-07-31T09:53:01.366798: step 14515, loss 0.570891.
Train: 2018-07-31T09:53:01.538663: step 14516, loss 0.638066.
Train: 2018-07-31T09:53:01.710498: step 14517, loss 0.529942.
Train: 2018-07-31T09:53:01.882332: step 14518, loss 0.613625.
Train: 2018-07-31T09:53:02.038546: step 14519, loss 0.537999.
Train: 2018-07-31T09:53:02.210350: step 14520, loss 0.511804.
Test: 2018-07-31T09:53:02.444671: step 14520, loss 0.548702.
Train: 2018-07-31T09:53:02.632127: step 14521, loss 0.603404.
Train: 2018-07-31T09:53:02.803962: step 14522, loss 0.578712.
Train: 2018-07-31T09:53:02.975796: step 14523, loss 0.596108.
Train: 2018-07-31T09:53:03.132040: step 14524, loss 0.512903.
Train: 2018-07-31T09:53:03.319467: step 14525, loss 0.579079.
Train: 2018-07-31T09:53:03.475712: step 14526, loss 0.547296.
Train: 2018-07-31T09:53:03.647513: step 14527, loss 0.569671.
Train: 2018-07-31T09:53:03.819348: step 14528, loss 0.579346.
Train: 2018-07-31T09:53:03.991216: step 14529, loss 0.497614.
Train: 2018-07-31T09:53:04.147396: step 14530, loss 0.604778.
Test: 2018-07-31T09:53:04.381748: step 14530, loss 0.548474.
Train: 2018-07-31T09:53:04.553584: step 14531, loss 0.59723.
Train: 2018-07-31T09:53:04.725387: step 14532, loss 0.529572.
Train: 2018-07-31T09:53:04.897252: step 14533, loss 0.570573.
Train: 2018-07-31T09:53:05.069055: step 14534, loss 0.503676.
Train: 2018-07-31T09:53:05.240923: step 14535, loss 0.536384.
Train: 2018-07-31T09:53:05.412756: step 14536, loss 0.536768.
Train: 2018-07-31T09:53:05.584561: step 14537, loss 0.536501.
Train: 2018-07-31T09:53:05.756426: step 14538, loss 0.528657.
Train: 2018-07-31T09:53:05.928229: step 14539, loss 0.553223.
Train: 2018-07-31T09:53:06.084443: step 14540, loss 0.509785.
Test: 2018-07-31T09:53:06.318763: step 14540, loss 0.547703.
Train: 2018-07-31T09:53:06.490598: step 14541, loss 0.535988.
Train: 2018-07-31T09:53:06.662433: step 14542, loss 0.561573.
Train: 2018-07-31T09:53:06.834268: step 14543, loss 0.509708.
Train: 2018-07-31T09:53:06.990480: step 14544, loss 0.553869.
Train: 2018-07-31T09:53:07.162316: step 14545, loss 0.609035.
Train: 2018-07-31T09:53:07.334150: step 14546, loss 0.562353.
Train: 2018-07-31T09:53:07.490397: step 14547, loss 0.543822.
Train: 2018-07-31T09:53:07.662198: step 14548, loss 0.553987.
Train: 2018-07-31T09:53:07.834064: step 14549, loss 0.635397.
Train: 2018-07-31T09:53:08.005899: step 14550, loss 0.589186.
Test: 2018-07-31T09:53:08.240222: step 14550, loss 0.547603.
Train: 2018-07-31T09:53:08.412053: step 14551, loss 0.544887.
Train: 2018-07-31T09:53:08.583859: step 14552, loss 0.607746.
Train: 2018-07-31T09:53:08.755693: step 14553, loss 0.607095.
Train: 2018-07-31T09:53:08.927527: step 14554, loss 0.508511.
Train: 2018-07-31T09:53:09.099361: step 14555, loss 0.53681.
Train: 2018-07-31T09:53:09.255575: step 14556, loss 0.553473.
Train: 2018-07-31T09:53:09.427412: step 14557, loss 0.606941.
Train: 2018-07-31T09:53:09.599275: step 14558, loss 0.561511.
Train: 2018-07-31T09:53:09.771080: step 14559, loss 0.510067.
Train: 2018-07-31T09:53:09.927324: step 14560, loss 0.545689.
Test: 2018-07-31T09:53:10.161644: step 14560, loss 0.54776.
Train: 2018-07-31T09:53:10.333474: step 14561, loss 0.465596.
Train: 2018-07-31T09:53:10.489694: step 14562, loss 0.597993.
Train: 2018-07-31T09:53:10.661527: step 14563, loss 0.632056.
Train: 2018-07-31T09:53:10.833332: step 14564, loss 0.598038.
Train: 2018-07-31T09:53:11.005197: step 14565, loss 0.545409.
Train: 2018-07-31T09:53:11.161411: step 14566, loss 0.571144.
Train: 2018-07-31T09:53:11.333245: step 14567, loss 0.535555.
Train: 2018-07-31T09:53:11.505080: step 14568, loss 0.562634.
Train: 2018-07-31T09:53:11.676883: step 14569, loss 0.579268.
Train: 2018-07-31T09:53:11.833128: step 14570, loss 0.555031.
Test: 2018-07-31T09:53:12.067418: step 14570, loss 0.547878.
Train: 2018-07-31T09:53:12.254897: step 14571, loss 0.60592.
Train: 2018-07-31T09:53:12.411087: step 14572, loss 0.55318.
Train: 2018-07-31T09:53:12.582921: step 14573, loss 0.597171.
Train: 2018-07-31T09:53:12.754787: step 14574, loss 0.588309.
Train: 2018-07-31T09:53:12.911000: step 14575, loss 0.536334.
Train: 2018-07-31T09:53:13.082805: step 14576, loss 0.561971.
Train: 2018-07-31T09:53:13.239043: step 14577, loss 0.537148.
Train: 2018-07-31T09:53:13.410883: step 14578, loss 0.587545.
Train: 2018-07-31T09:53:13.567066: step 14579, loss 0.647041.
Train: 2018-07-31T09:53:13.738933: step 14580, loss 0.605993.
Test: 2018-07-31T09:53:13.973250: step 14580, loss 0.54833.
Train: 2018-07-31T09:53:14.160676: step 14581, loss 0.504411.
Train: 2018-07-31T09:53:14.316920: step 14582, loss 0.469289.
Train: 2018-07-31T09:53:14.488758: step 14583, loss 0.545586.
Train: 2018-07-31T09:53:14.644969: step 14584, loss 0.561569.
Train: 2018-07-31T09:53:14.816773: step 14585, loss 0.638624.
Train: 2018-07-31T09:53:14.973019: step 14586, loss 0.502803.
Train: 2018-07-31T09:53:15.144822: step 14587, loss 0.544924.
Train: 2018-07-31T09:53:15.301059: step 14588, loss 0.519352.
Train: 2018-07-31T09:53:15.472903: step 14589, loss 0.555.
Train: 2018-07-31T09:53:15.629113: step 14590, loss 0.510876.
Test: 2018-07-31T09:53:15.879026: step 14590, loss 0.547862.
Train: 2018-07-31T09:53:16.035237: step 14591, loss 0.54478.
Train: 2018-07-31T09:53:16.207072: step 14592, loss 0.528554.
Train: 2018-07-31T09:53:16.363286: step 14593, loss 0.535546.
Train: 2018-07-31T09:53:16.535121: step 14594, loss 0.563905.
Train: 2018-07-31T09:53:16.691365: step 14595, loss 0.618145.
Train: 2018-07-31T09:53:16.863201: step 14596, loss 0.58838.
Train: 2018-07-31T09:53:17.019382: step 14597, loss 0.570327.
Train: 2018-07-31T09:53:17.206869: step 14598, loss 0.509946.
Train: 2018-07-31T09:53:17.363080: step 14599, loss 0.56135.
Train: 2018-07-31T09:53:17.534886: step 14600, loss 0.562274.
Test: 2018-07-31T09:53:17.769238: step 14600, loss 0.547694.
Train: 2018-07-31T09:53:18.487821: step 14601, loss 0.606869.
Train: 2018-07-31T09:53:18.659654: step 14602, loss 0.546011.
Train: 2018-07-31T09:53:18.815838: step 14603, loss 0.562459.
Train: 2018-07-31T09:53:18.987672: step 14604, loss 0.55525.
Train: 2018-07-31T09:53:19.159507: step 14605, loss 0.509644.
Train: 2018-07-31T09:53:19.315750: step 14606, loss 0.623447.
Train: 2018-07-31T09:53:19.471932: step 14607, loss 0.605862.
Train: 2018-07-31T09:53:19.643795: step 14608, loss 0.596614.
Train: 2018-07-31T09:53:19.815628: step 14609, loss 0.545017.
Train: 2018-07-31T09:53:19.987437: step 14610, loss 0.692772.
Test: 2018-07-31T09:53:20.221758: step 14610, loss 0.547997.
Train: 2018-07-31T09:53:20.377995: step 14611, loss 0.554014.
Train: 2018-07-31T09:53:20.549836: step 14612, loss 0.546021.
Train: 2018-07-31T09:53:20.721640: step 14613, loss 0.639013.
Train: 2018-07-31T09:53:20.877878: step 14614, loss 0.57075.
Train: 2018-07-31T09:53:21.049714: step 14615, loss 0.562421.
Train: 2018-07-31T09:53:21.221524: step 14616, loss 0.604151.
Train: 2018-07-31T09:53:21.377737: step 14617, loss 0.629304.
Train: 2018-07-31T09:53:21.533951: step 14618, loss 0.587296.
Train: 2018-07-31T09:53:21.705786: step 14619, loss 0.538429.
Train: 2018-07-31T09:53:21.861998: step 14620, loss 0.488822.
Test: 2018-07-31T09:53:22.096347: step 14620, loss 0.548873.
Train: 2018-07-31T09:53:22.268153: step 14621, loss 0.587749.
Train: 2018-07-31T09:53:22.439988: step 14622, loss 0.529425.
Train: 2018-07-31T09:53:22.596234: step 14623, loss 0.571184.
Train: 2018-07-31T09:53:22.768037: step 14624, loss 0.537732.
Train: 2018-07-31T09:53:22.924249: step 14625, loss 0.54535.
Train: 2018-07-31T09:53:23.080493: step 14626, loss 0.59534.
Train: 2018-07-31T09:53:23.252297: step 14627, loss 0.571313.
Train: 2018-07-31T09:53:23.408511: step 14628, loss 0.545676.
Train: 2018-07-31T09:53:23.580346: step 14629, loss 0.519199.
Train: 2018-07-31T09:53:23.736584: step 14630, loss 0.504279.
Test: 2018-07-31T09:53:23.970914: step 14630, loss 0.547769.
Train: 2018-07-31T09:53:24.142739: step 14631, loss 0.588557.
Train: 2018-07-31T09:53:24.314580: step 14632, loss 0.51593.
Train: 2018-07-31T09:53:24.470763: step 14633, loss 0.579427.
Train: 2018-07-31T09:53:24.642598: step 14634, loss 0.518094.
Train: 2018-07-31T09:53:24.814463: step 14635, loss 0.522858.
Train: 2018-07-31T09:53:24.986267: step 14636, loss 0.507131.
Train: 2018-07-31T09:53:25.142481: step 14637, loss 0.549131.
Train: 2018-07-31T09:53:25.314340: step 14638, loss 0.554393.
Train: 2018-07-31T09:53:25.486151: step 14639, loss 0.617487.
Train: 2018-07-31T09:53:25.642394: step 14640, loss 0.563749.
Test: 2018-07-31T09:53:25.876715: step 14640, loss 0.54777.
Train: 2018-07-31T09:53:26.048548: step 14641, loss 0.533607.
Train: 2018-07-31T09:53:26.204762: step 14642, loss 0.519785.
Train: 2018-07-31T09:53:26.376596: step 14643, loss 0.515136.
Train: 2018-07-31T09:53:26.532810: step 14644, loss 0.563847.
Train: 2018-07-31T09:53:26.704639: step 14645, loss 0.66814.
Train: 2018-07-31T09:53:26.860860: step 14646, loss 0.628146.
Train: 2018-07-31T09:53:27.017072: step 14647, loss 0.660178.
Train: 2018-07-31T09:53:27.173285: step 14648, loss 0.520145.
Train: 2018-07-31T09:53:27.345089: step 14649, loss 0.561511.
Train: 2018-07-31T09:53:27.501304: step 14650, loss 0.498503.
Test: 2018-07-31T09:53:27.735623: step 14650, loss 0.549502.
Train: 2018-07-31T09:53:27.907488: step 14651, loss 0.554731.
Train: 2018-07-31T09:53:28.063702: step 14652, loss 0.61859.
Train: 2018-07-31T09:53:28.219885: step 14653, loss 0.507248.
Train: 2018-07-31T09:53:28.391751: step 14654, loss 0.539892.
Train: 2018-07-31T09:53:28.563554: step 14655, loss 0.51428.
Train: 2018-07-31T09:53:28.719792: step 14656, loss 0.595527.
Train: 2018-07-31T09:53:28.876012: step 14657, loss 0.562938.
Train: 2018-07-31T09:53:29.047816: step 14658, loss 0.521445.
Train: 2018-07-31T09:53:29.219683: step 14659, loss 0.562403.
Train: 2018-07-31T09:53:29.375864: step 14660, loss 0.562721.
Test: 2018-07-31T09:53:29.594564: step 14660, loss 0.548426.
Train: 2018-07-31T09:53:29.766429: step 14661, loss 0.613053.
Train: 2018-07-31T09:53:29.922647: step 14662, loss 0.554191.
Train: 2018-07-31T09:53:30.094479: step 14663, loss 0.596326.
Train: 2018-07-31T09:53:30.250689: step 14664, loss 0.620752.
Train: 2018-07-31T09:53:30.406873: step 14665, loss 0.504046.
Train: 2018-07-31T09:53:30.578738: step 14666, loss 0.562055.
Train: 2018-07-31T09:53:30.734922: step 14667, loss 0.562129.
Train: 2018-07-31T09:53:30.906786: step 14668, loss 0.579183.
Train: 2018-07-31T09:53:31.062970: step 14669, loss 0.621464.
Train: 2018-07-31T09:53:31.219215: step 14670, loss 0.553206.
Test: 2018-07-31T09:53:31.469125: step 14670, loss 0.548471.
Train: 2018-07-31T09:53:31.625368: step 14671, loss 0.545808.
Train: 2018-07-31T09:53:31.781552: step 14672, loss 0.595787.
Train: 2018-07-31T09:53:31.953386: step 14673, loss 0.486637.
Train: 2018-07-31T09:53:32.109631: step 14674, loss 0.518664.
Train: 2018-07-31T09:53:32.281464: step 14675, loss 0.588548.
Train: 2018-07-31T09:53:32.437647: step 14676, loss 0.622591.
Train: 2018-07-31T09:53:32.609482: step 14677, loss 0.579786.
Train: 2018-07-31T09:53:32.765720: step 14678, loss 0.561001.
Train: 2018-07-31T09:53:32.921939: step 14679, loss 0.476154.
Train: 2018-07-31T09:53:33.078123: step 14680, loss 0.529536.
Test: 2018-07-31T09:53:33.328094: step 14680, loss 0.547871.
Train: 2018-07-31T09:53:33.484277: step 14681, loss 0.491577.
Train: 2018-07-31T09:53:33.640521: step 14682, loss 0.554903.
Train: 2018-07-31T09:53:33.796728: step 14683, loss 0.466188.
Train: 2018-07-31T09:53:33.968568: step 14684, loss 0.60724.
Train: 2018-07-31T09:53:34.140374: step 14685, loss 0.526267.
Train: 2018-07-31T09:53:34.296617: step 14686, loss 0.630487.
Train: 2018-07-31T09:53:34.452830: step 14687, loss 0.63715.
Train: 2018-07-31T09:53:34.624665: step 14688, loss 0.544318.
Train: 2018-07-31T09:53:34.780879: step 14689, loss 0.5334.
Train: 2018-07-31T09:53:34.937061: step 14690, loss 0.613763.
Test: 2018-07-31T09:53:35.171382: step 14690, loss 0.547815.
Train: 2018-07-31T09:53:35.327626: step 14691, loss 0.543751.
Train: 2018-07-31T09:53:35.499460: step 14692, loss 0.58011.
Train: 2018-07-31T09:53:35.655643: step 14693, loss 0.579269.
Train: 2018-07-31T09:53:35.827478: step 14694, loss 0.510567.
Train: 2018-07-31T09:53:35.983724: step 14695, loss 0.493632.
Train: 2018-07-31T09:53:36.155528: step 14696, loss 0.62252.
Train: 2018-07-31T09:53:36.327391: step 14697, loss 0.604106.
Train: 2018-07-31T09:53:36.483606: step 14698, loss 0.629877.
Train: 2018-07-31T09:53:36.655409: step 14699, loss 0.58636.
Train: 2018-07-31T09:53:36.811647: step 14700, loss 0.512697.
Test: 2018-07-31T09:53:37.045975: step 14700, loss 0.548478.
Train: 2018-07-31T09:53:37.780176: step 14701, loss 0.563291.
Train: 2018-07-31T09:53:37.951980: step 14702, loss 0.528715.
Train: 2018-07-31T09:53:38.108195: step 14703, loss 0.613282.
Train: 2018-07-31T09:53:38.280060: step 14704, loss 0.502096.
Train: 2018-07-31T09:53:38.436242: step 14705, loss 0.597084.
Train: 2018-07-31T09:53:38.608110: step 14706, loss 0.572308.
Train: 2018-07-31T09:53:38.764321: step 14707, loss 0.554321.
Train: 2018-07-31T09:53:38.936155: step 14708, loss 0.569149.
Train: 2018-07-31T09:53:39.092372: step 14709, loss 0.564961.
Train: 2018-07-31T09:53:39.264204: step 14710, loss 0.654629.
Test: 2018-07-31T09:53:39.498519: step 14710, loss 0.548735.
Train: 2018-07-31T09:53:39.654740: step 14711, loss 0.596595.
Train: 2018-07-31T09:53:39.826541: step 14712, loss 0.495885.
Train: 2018-07-31T09:53:39.998376: step 14713, loss 0.5972.
Train: 2018-07-31T09:53:40.154590: step 14714, loss 0.53915.
Train: 2018-07-31T09:53:40.310834: step 14715, loss 0.605422.
Train: 2018-07-31T09:53:40.482639: step 14716, loss 0.563415.
Train: 2018-07-31T09:53:40.638882: step 14717, loss 0.530288.
Train: 2018-07-31T09:53:40.795066: step 14718, loss 0.555068.
Train: 2018-07-31T09:53:40.966899: step 14719, loss 0.561993.
Train: 2018-07-31T09:53:41.138735: step 14720, loss 0.552526.
Test: 2018-07-31T09:53:41.373054: step 14720, loss 0.548456.
Train: 2018-07-31T09:53:41.529298: step 14721, loss 0.578524.
Train: 2018-07-31T09:53:41.701133: step 14722, loss 0.612984.
Train: 2018-07-31T09:53:41.872969: step 14723, loss 0.588774.
Train: 2018-07-31T09:53:42.029150: step 14724, loss 0.544963.
Train: 2018-07-31T09:53:42.201018: step 14725, loss 0.544139.
Train: 2018-07-31T09:53:42.357232: step 14726, loss 0.486693.
Train: 2018-07-31T09:53:42.513412: step 14727, loss 0.537737.
Train: 2018-07-31T09:53:42.685247: step 14728, loss 0.587745.
Train: 2018-07-31T09:53:42.841461: step 14729, loss 0.546764.
Train: 2018-07-31T09:53:43.013326: step 14730, loss 0.605523.
Test: 2018-07-31T09:53:43.247646: step 14730, loss 0.548138.
Train: 2018-07-31T09:53:43.419493: step 14731, loss 0.562159.
Train: 2018-07-31T09:53:43.575695: step 14732, loss 0.48685.
Train: 2018-07-31T09:53:43.731878: step 14733, loss 0.59536.
Train: 2018-07-31T09:53:43.903713: step 14734, loss 0.47533.
Train: 2018-07-31T09:53:44.059956: step 14735, loss 0.52784.
Train: 2018-07-31T09:53:44.216138: step 14736, loss 0.563025.
Train: 2018-07-31T09:53:44.388007: step 14737, loss 0.572541.
Train: 2018-07-31T09:53:44.559839: step 14738, loss 0.579954.
Train: 2018-07-31T09:53:44.731675: step 14739, loss 0.596523.
Train: 2018-07-31T09:53:44.887887: step 14740, loss 0.543753.
Test: 2018-07-31T09:53:45.137799: step 14740, loss 0.54773.
Train: 2018-07-31T09:53:45.294041: step 14741, loss 0.527113.
Train: 2018-07-31T09:53:45.465846: step 14742, loss 0.563102.
Train: 2018-07-31T09:53:45.622060: step 14743, loss 0.545667.
Train: 2018-07-31T09:53:45.778310: step 14744, loss 0.490455.
Train: 2018-07-31T09:53:45.934511: step 14745, loss 0.517723.
Train: 2018-07-31T09:53:46.106321: step 14746, loss 0.521219.
Train: 2018-07-31T09:53:46.262565: step 14747, loss 0.585526.
Train: 2018-07-31T09:53:46.434394: step 14748, loss 0.52739.
Train: 2018-07-31T09:53:46.590583: step 14749, loss 0.564068.
Train: 2018-07-31T09:53:46.746797: step 14750, loss 0.578371.
Test: 2018-07-31T09:53:46.996769: step 14750, loss 0.547665.
Train: 2018-07-31T09:53:47.152982: step 14751, loss 0.570872.
Train: 2018-07-31T09:53:47.309197: step 14752, loss 0.600083.
Train: 2018-07-31T09:53:47.480999: step 14753, loss 0.55342.
Train: 2018-07-31T09:53:47.637213: step 14754, loss 0.598629.
Train: 2018-07-31T09:53:47.809072: step 14755, loss 0.508424.
Train: 2018-07-31T09:53:47.965260: step 14756, loss 0.697504.
Train: 2018-07-31T09:53:48.121475: step 14757, loss 0.589492.
Train: 2018-07-31T09:53:48.277722: step 14758, loss 0.527784.
Train: 2018-07-31T09:53:48.449553: step 14759, loss 0.605128.
Train: 2018-07-31T09:53:48.605735: step 14760, loss 0.537144.
Test: 2018-07-31T09:53:48.840081: step 14760, loss 0.548091.
Train: 2018-07-31T09:53:48.996300: step 14761, loss 0.518849.
Train: 2018-07-31T09:53:49.152512: step 14762, loss 0.562821.
Train: 2018-07-31T09:53:49.308726: step 14763, loss 0.638844.
Train: 2018-07-31T09:53:49.480564: step 14764, loss 0.502363.
Train: 2018-07-31T09:53:49.652395: step 14765, loss 0.562533.
Train: 2018-07-31T09:53:49.808578: step 14766, loss 0.553368.
Train: 2018-07-31T09:53:49.980414: step 14767, loss 0.536515.
Train: 2018-07-31T09:53:50.136627: step 14768, loss 0.562808.
Train: 2018-07-31T09:53:50.292872: step 14769, loss 0.537843.
Train: 2018-07-31T09:53:50.464676: step 14770, loss 0.538222.
Test: 2018-07-31T09:53:50.699028: step 14770, loss 0.548654.
Train: 2018-07-31T09:53:50.870856: step 14771, loss 0.564272.
Train: 2018-07-31T09:53:51.027074: step 14772, loss 0.534707.
Train: 2018-07-31T09:53:51.198909: step 14773, loss 0.580003.
Train: 2018-07-31T09:53:51.355125: step 14774, loss 0.546156.
Train: 2018-07-31T09:53:51.526957: step 14775, loss 0.511787.
Train: 2018-07-31T09:53:51.683165: step 14776, loss 0.536096.
Train: 2018-07-31T09:53:51.855003: step 14777, loss 0.614388.
Train: 2018-07-31T09:53:52.026809: step 14778, loss 0.61217.
Train: 2018-07-31T09:53:52.183024: step 14779, loss 0.564002.
Train: 2018-07-31T09:53:52.354890: step 14780, loss 0.648896.
Test: 2018-07-31T09:53:52.589203: step 14780, loss 0.548294.
Train: 2018-07-31T09:53:52.761012: step 14781, loss 0.527453.
Train: 2018-07-31T09:53:52.917228: step 14782, loss 0.486854.
Train: 2018-07-31T09:53:53.089062: step 14783, loss 0.594328.
Train: 2018-07-31T09:53:53.245310: step 14784, loss 0.553113.
Train: 2018-07-31T09:53:53.417140: step 14785, loss 0.543791.
Train: 2018-07-31T09:53:53.573352: step 14786, loss 0.561006.
Train: 2018-07-31T09:53:53.745188: step 14787, loss 0.616666.
Train: 2018-07-31T09:53:53.901372: step 14788, loss 0.584431.
Train: 2018-07-31T09:53:54.073205: step 14789, loss 0.576057.
Train: 2018-07-31T09:53:54.229419: step 14790, loss 0.584528.
Test: 2018-07-31T09:53:54.479360: step 14790, loss 0.553373.
Train: 2018-07-31T09:53:54.635573: step 14791, loss 0.589902.
Train: 2018-07-31T09:53:54.791819: step 14792, loss 0.650818.
Train: 2018-07-31T09:53:54.963655: step 14793, loss 0.589823.
Train: 2018-07-31T09:53:55.135487: step 14794, loss 0.560085.
Train: 2018-07-31T09:53:55.291701: step 14795, loss 0.617762.
Train: 2018-07-31T09:53:55.463531: step 14796, loss 0.518296.
Train: 2018-07-31T09:53:55.619748: step 14797, loss 0.524557.
Train: 2018-07-31T09:53:55.791583: step 14798, loss 0.642352.
Train: 2018-07-31T09:53:55.947797: step 14799, loss 0.547655.
Train: 2018-07-31T09:53:56.104013: step 14800, loss 0.577576.
Test: 2018-07-31T09:53:56.353921: step 14800, loss 0.548163.
Train: 2018-07-31T09:53:57.072509: step 14801, loss 0.597069.
Train: 2018-07-31T09:53:57.228716: step 14802, loss 0.545659.
Train: 2018-07-31T09:53:57.400552: step 14803, loss 0.535849.
Train: 2018-07-31T09:53:57.572387: step 14804, loss 0.493785.
Train: 2018-07-31T09:53:57.728599: step 14805, loss 0.537557.
Train: 2018-07-31T09:53:57.900434: step 14806, loss 0.606133.
Train: 2018-07-31T09:53:58.072299: step 14807, loss 0.501742.
Train: 2018-07-31T09:53:58.228483: step 14808, loss 0.545642.
Train: 2018-07-31T09:53:58.400317: step 14809, loss 0.597799.
Train: 2018-07-31T09:53:58.572151: step 14810, loss 0.545709.
Test: 2018-07-31T09:53:58.806473: step 14810, loss 0.547968.
Train: 2018-07-31T09:53:58.978339: step 14811, loss 0.562544.
Train: 2018-07-31T09:53:59.134520: step 14812, loss 0.492418.
Train: 2018-07-31T09:53:59.306354: step 14813, loss 0.543837.
Train: 2018-07-31T09:53:59.478191: step 14814, loss 0.506047.
Train: 2018-07-31T09:53:59.650055: step 14815, loss 0.547982.
Train: 2018-07-31T09:53:59.821859: step 14816, loss 0.598119.
Train: 2018-07-31T09:53:59.978105: step 14817, loss 0.573583.
Train: 2018-07-31T09:54:00.149908: step 14818, loss 0.544731.
Train: 2018-07-31T09:54:00.306122: step 14819, loss 0.482084.
Train: 2018-07-31T09:54:00.477957: step 14820, loss 0.606597.
Test: 2018-07-31T09:54:00.712309: step 14820, loss 0.548058.
Train: 2018-07-31T09:54:00.884110: step 14821, loss 0.518447.
Train: 2018-07-31T09:54:01.055945: step 14822, loss 0.555784.
Train: 2018-07-31T09:54:01.212159: step 14823, loss 0.550062.
Train: 2018-07-31T09:54:01.384024: step 14824, loss 0.442644.
Train: 2018-07-31T09:54:01.555858: step 14825, loss 0.631082.
Train: 2018-07-31T09:54:01.712072: step 14826, loss 0.542609.
Train: 2018-07-31T09:54:01.883876: step 14827, loss 0.519168.
Train: 2018-07-31T09:54:02.055742: step 14828, loss 0.553033.
Train: 2018-07-31T09:54:02.227578: step 14829, loss 0.576104.
Train: 2018-07-31T09:54:02.383760: step 14830, loss 0.535252.
Test: 2018-07-31T09:54:02.618111: step 14830, loss 0.548488.
Train: 2018-07-31T09:54:02.789915: step 14831, loss 0.568326.
Train: 2018-07-31T09:54:02.961782: step 14832, loss 0.525293.
Train: 2018-07-31T09:54:03.133585: step 14833, loss 0.532281.
Train: 2018-07-31T09:54:03.305419: step 14834, loss 0.590074.
Train: 2018-07-31T09:54:03.477253: step 14835, loss 0.491933.
Train: 2018-07-31T09:54:03.649089: step 14836, loss 0.490671.
Train: 2018-07-31T09:54:03.805331: step 14837, loss 0.493237.
Train: 2018-07-31T09:54:03.977169: step 14838, loss 0.609631.
Train: 2018-07-31T09:54:04.149007: step 14839, loss 0.564062.
Train: 2018-07-31T09:54:04.320836: step 14840, loss 0.654768.
Test: 2018-07-31T09:54:04.570748: step 14840, loss 0.548888.
Train: 2018-07-31T09:54:04.726960: step 14841, loss 0.647879.
Train: 2018-07-31T09:54:04.898796: step 14842, loss 0.528166.
Train: 2018-07-31T09:54:05.055040: step 14843, loss 0.520905.
Train: 2018-07-31T09:54:05.226876: step 14844, loss 0.545869.
Train: 2018-07-31T09:54:05.398679: step 14845, loss 0.557687.
Train: 2018-07-31T09:54:05.570513: step 14846, loss 0.62031.
Train: 2018-07-31T09:54:05.726727: step 14847, loss 0.553753.
Train: 2018-07-31T09:54:05.898561: step 14848, loss 0.582947.
Train: 2018-07-31T09:54:06.070397: step 14849, loss 0.615678.
Train: 2018-07-31T09:54:06.226642: step 14850, loss 0.598668.
Test: 2018-07-31T09:54:06.476550: step 14850, loss 0.553684.
Train: 2018-07-31T09:54:06.632798: step 14851, loss 0.63025.
Train: 2018-07-31T09:54:06.804598: step 14852, loss 0.486837.
Train: 2018-07-31T09:54:06.976466: step 14853, loss 0.578167.
Train: 2018-07-31T09:54:07.132677: step 14854, loss 0.596021.
Train: 2018-07-31T09:54:07.304481: step 14855, loss 0.645809.
Train: 2018-07-31T09:54:07.476317: step 14856, loss 0.612538.
Train: 2018-07-31T09:54:07.632531: step 14857, loss 0.5924.
Train: 2018-07-31T09:54:07.804395: step 14858, loss 0.542065.
Train: 2018-07-31T09:54:07.976199: step 14859, loss 0.593753.
Train: 2018-07-31T09:54:08.148059: step 14860, loss 0.597557.
Test: 2018-07-31T09:54:08.382380: step 14860, loss 0.554379.
Train: 2018-07-31T09:54:08.554214: step 14861, loss 0.661797.
Train: 2018-07-31T09:54:08.710402: step 14862, loss 0.573583.
Train: 2018-07-31T09:54:08.882238: step 14863, loss 0.585807.
Train: 2018-07-31T09:54:09.054073: step 14864, loss 0.601055.
Train: 2018-07-31T09:54:09.225940: step 14865, loss 0.628357.
Train: 2018-07-31T09:54:09.397770: step 14866, loss 0.548488.
Train: 2018-07-31T09:54:09.569578: step 14867, loss 0.567462.
Train: 2018-07-31T09:54:09.725815: step 14868, loss 0.488148.
Train: 2018-07-31T09:54:09.913271: step 14869, loss 0.570983.
Train: 2018-07-31T09:54:10.085111: step 14870, loss 0.599354.
Test: 2018-07-31T09:54:10.319437: step 14870, loss 0.560007.
Train: 2018-07-31T09:54:10.491236: step 14871, loss 0.572137.
Train: 2018-07-31T09:54:10.663096: step 14872, loss 0.548402.
Train: 2018-07-31T09:54:10.834906: step 14873, loss 0.675062.
Train: 2018-07-31T09:54:11.006770: step 14874, loss 0.648739.
Train: 2018-07-31T09:54:11.194196: step 14875, loss 0.641611.
Train: 2018-07-31T09:54:11.366031: step 14876, loss 0.689009.
Train: 2018-07-31T09:54:11.537896: step 14877, loss 0.620131.
Train: 2018-07-31T09:54:11.725321: step 14878, loss 0.615359.
Train: 2018-07-31T09:54:11.912778: step 14879, loss 0.71842.
Train: 2018-07-31T09:54:12.084613: step 14880, loss 0.58574.
Test: 2018-07-31T09:54:12.318933: step 14880, loss 0.627346.
Train: 2018-07-31T09:54:12.506413: step 14881, loss 0.744856.
Train: 2018-07-31T09:54:12.693871: step 14882, loss 0.75472.
Train: 2018-07-31T09:54:12.865710: step 14883, loss 0.758866.
Train: 2018-07-31T09:54:13.053135: step 14884, loss 0.692499.
Train: 2018-07-31T09:54:13.225004: step 14885, loss 0.785525.
Train: 2018-07-31T09:54:13.412427: step 14886, loss 0.738422.
Train: 2018-07-31T09:54:13.584294: step 14887, loss 0.7387.
Train: 2018-07-31T09:54:13.771746: step 14888, loss 0.725483.
Train: 2018-07-31T09:54:13.943583: step 14889, loss 0.667678.
Train: 2018-07-31T09:54:14.115415: step 14890, loss 0.675047.
Test: 2018-07-31T09:54:14.365359: step 14890, loss 0.686526.
Train: 2018-07-31T09:54:14.537163: step 14891, loss 0.723546.
Train: 2018-07-31T09:54:14.709028: step 14892, loss 1.10859.
Train: 2018-07-31T09:54:14.896479: step 14893, loss 0.703206.
Train: 2018-07-31T09:54:15.083935: step 14894, loss 0.693228.
Train: 2018-07-31T09:54:15.255775: step 14895, loss 0.648044.
Train: 2018-07-31T09:54:15.443233: step 14896, loss 0.915461.
Train: 2018-07-31T09:54:15.630657: step 14897, loss 0.758427.
Train: 2018-07-31T09:54:15.802492: step 14898, loss 0.791805.
Train: 2018-07-31T09:54:15.989973: step 14899, loss 0.785004.
Train: 2018-07-31T09:54:16.177405: step 14900, loss 0.783025.
Test: 2018-07-31T09:54:16.411726: step 14900, loss 0.752948.
Train: 2018-07-31T09:54:17.145927: step 14901, loss 0.738807.
Train: 2018-07-31T09:54:17.317762: step 14902, loss 0.858496.
Train: 2018-07-31T09:54:17.505245: step 14903, loss 0.734425.
Train: 2018-07-31T09:54:17.677083: step 14904, loss 0.748949.
Train: 2018-07-31T09:54:17.864539: step 14905, loss 0.799906.
Train: 2018-07-31T09:54:18.036374: step 14906, loss 0.748883.
Train: 2018-07-31T09:54:18.223832: step 14907, loss 0.719895.
Train: 2018-07-31T09:54:18.395636: step 14908, loss 0.688893.
Train: 2018-07-31T09:54:18.567499: step 14909, loss 0.801645.
Train: 2018-07-31T09:54:18.754926: step 14910, loss 0.772118.
Test: 2018-07-31T09:54:18.989246: step 14910, loss 0.725806.
Train: 2018-07-31T09:54:19.176728: step 14911, loss 0.816664.
Train: 2018-07-31T09:54:19.364182: step 14912, loss 0.823651.
Train: 2018-07-31T09:54:19.536017: step 14913, loss 0.729134.
Train: 2018-07-31T09:54:19.723449: step 14914, loss 0.776277.
Train: 2018-07-31T09:54:19.895284: step 14915, loss 0.75381.
Train: 2018-07-31T09:54:20.067119: step 14916, loss 0.741232.
Train: 2018-07-31T09:54:20.254599: step 14917, loss 0.784577.
Train: 2018-07-31T09:54:20.426410: step 14918, loss 0.728645.
Train: 2018-07-31T09:54:20.598277: step 14919, loss 0.682837.
Train: 2018-07-31T09:54:20.785725: step 14920, loss 0.706344.
Test: 2018-07-31T09:54:21.020055: step 14920, loss 0.701653.
Train: 2018-07-31T09:54:21.191886: step 14921, loss 0.685618.
Train: 2018-07-31T09:54:21.379341: step 14922, loss 0.701249.
Train: 2018-07-31T09:54:21.566791: step 14923, loss 0.723093.
Train: 2018-07-31T09:54:21.738626: step 14924, loss 0.659846.
Train: 2018-07-31T09:54:21.926060: step 14925, loss 0.699382.
Train: 2018-07-31T09:54:22.108905: step 14926, loss 0.734466.
Train: 2018-07-31T09:54:22.296371: step 14927, loss 0.712197.
Train: 2018-07-31T09:54:22.468224: step 14928, loss 0.672841.
Train: 2018-07-31T09:54:22.640060: step 14929, loss 0.623433.
Train: 2018-07-31T09:54:22.827486: step 14930, loss 0.720096.
Test: 2018-07-31T09:54:23.061837: step 14930, loss 0.666275.
Train: 2018-07-31T09:54:23.249287: step 14931, loss 0.653389.
Train: 2018-07-31T09:54:23.421097: step 14932, loss 0.663841.
Train: 2018-07-31T09:54:23.608553: step 14933, loss 0.652746.
Train: 2018-07-31T09:54:23.796039: step 14934, loss 0.656609.
Train: 2018-07-31T09:54:23.983509: step 14935, loss 0.629272.
Train: 2018-07-31T09:54:24.155325: step 14936, loss 0.695777.
Train: 2018-07-31T09:54:24.389651: step 14937, loss 0.621658.
Train: 2018-07-31T09:54:24.561485: step 14938, loss 0.676098.
Train: 2018-07-31T09:54:24.748941: step 14939, loss 0.693924.
Train: 2018-07-31T09:54:24.936368: step 14940, loss 0.728914.
Test: 2018-07-31T09:54:25.170718: step 14940, loss 0.658503.
Train: 2018-07-31T09:54:25.342556: step 14941, loss 0.660063.
Train: 2018-07-31T09:54:25.530009: step 14942, loss 0.652901.
Train: 2018-07-31T09:54:25.701814: step 14943, loss 0.550331.
Train: 2018-07-31T09:54:25.889299: step 14944, loss 0.585106.
Train: 2018-07-31T09:54:26.061104: step 14945, loss 0.7954.
Train: 2018-07-31T09:54:26.248591: step 14946, loss 0.71725.
Train: 2018-07-31T09:54:26.436046: step 14947, loss 0.583341.
Train: 2018-07-31T09:54:26.607881: step 14948, loss 0.713239.
Train: 2018-07-31T09:54:26.779716: step 14949, loss 0.594158.
Train: 2018-07-31T09:54:26.967176: step 14950, loss 0.714449.
Test: 2018-07-31T09:54:27.201487: step 14950, loss 0.638629.
Train: 2018-07-31T09:54:27.388948: step 14951, loss 0.630196.
Train: 2018-07-31T09:54:27.576375: step 14952, loss 0.61774.
Train: 2018-07-31T09:54:27.763861: step 14953, loss 0.680132.
Train: 2018-07-31T09:54:27.935695: step 14954, loss 0.62164.
Train: 2018-07-31T09:54:28.107527: step 14955, loss 0.713359.
Train: 2018-07-31T09:54:28.294956: step 14956, loss 0.638681.
Train: 2018-07-31T09:54:28.466820: step 14957, loss 0.686474.
Train: 2018-07-31T09:54:28.654271: step 14958, loss 0.63874.
Train: 2018-07-31T09:54:28.826082: step 14959, loss 0.664101.
Train: 2018-07-31T09:54:29.013568: step 14960, loss 0.643483.
Test: 2018-07-31T09:54:29.263510: step 14960, loss 0.623921.
Train: 2018-07-31T09:54:29.435339: step 14961, loss 0.627457.
Train: 2018-07-31T09:54:29.622798: step 14962, loss 0.693784.
Train: 2018-07-31T09:54:29.794629: step 14963, loss 0.543278.
Train: 2018-07-31T09:54:29.982091: step 14964, loss 0.673458.
Train: 2018-07-31T09:54:30.153926: step 14965, loss 0.641313.
Train: 2018-07-31T09:54:30.341386: step 14966, loss 0.645932.
Train: 2018-07-31T09:54:30.513186: step 14967, loss 0.649067.
Train: 2018-07-31T09:54:30.700642: step 14968, loss 0.645561.
Train: 2018-07-31T09:54:30.888098: step 14969, loss 0.705112.
Train: 2018-07-31T09:54:31.075555: step 14970, loss 0.751586.
Test: 2018-07-31T09:54:31.309875: step 14970, loss 0.62602.
Train: 2018-07-31T09:54:31.481710: step 14971, loss 0.679379.
Train: 2018-07-31T09:54:31.669196: step 14972, loss 0.590371.
Train: 2018-07-31T09:54:31.841034: step 14973, loss 0.595995.
Train: 2018-07-31T09:54:32.028482: step 14974, loss 0.639186.
Train: 2018-07-31T09:54:32.200291: step 14975, loss 0.648026.
Train: 2018-07-31T09:54:32.372151: step 14976, loss 0.628482.
Train: 2018-07-31T09:54:32.559582: step 14977, loss 0.59283.
Train: 2018-07-31T09:54:32.731449: step 14978, loss 0.581454.
Train: 2018-07-31T09:54:32.918903: step 14979, loss 0.624071.
Train: 2018-07-31T09:54:33.090708: step 14980, loss 0.596545.
Test: 2018-07-31T09:54:33.325028: step 14980, loss 0.614677.
Train: 2018-07-31T09:54:33.512485: step 14981, loss 0.566798.
Train: 2018-07-31T09:54:33.684320: step 14982, loss 0.573717.
Train: 2018-07-31T09:54:33.856185: step 14983, loss 0.68437.
Train: 2018-07-31T09:54:34.043610: step 14984, loss 0.688354.
Train: 2018-07-31T09:54:34.215478: step 14985, loss 0.626758.
Train: 2018-07-31T09:54:34.402931: step 14986, loss 0.641304.
Train: 2018-07-31T09:54:34.574768: step 14987, loss 0.554689.
Train: 2018-07-31T09:54:34.746570: step 14988, loss 0.62878.
Train: 2018-07-31T09:54:34.934026: step 14989, loss 0.608573.
Train: 2018-07-31T09:54:35.105887: step 14990, loss 0.751214.
Test: 2018-07-31T09:54:35.340211: step 14990, loss 0.602123.
Train: 2018-07-31T09:54:35.527667: step 14991, loss 0.594309.
Train: 2018-07-31T09:54:35.715093: step 14992, loss 0.629716.
Train: 2018-07-31T09:54:35.886928: step 14993, loss 0.593702.
Train: 2018-07-31T09:54:36.058763: step 14994, loss 0.622524.
Train: 2018-07-31T09:54:36.246219: step 14995, loss 0.606261.
Train: 2018-07-31T09:54:36.418086: step 14996, loss 0.598936.
Train: 2018-07-31T09:54:36.589889: step 14997, loss 0.659472.
Train: 2018-07-31T09:54:36.777375: step 14998, loss 0.576148.
Train: 2018-07-31T09:54:36.949212: step 14999, loss 0.532845.
Train: 2018-07-31T09:54:37.121013: step 15000, loss 0.585081.
Test: 2018-07-31T09:54:37.355336: step 15000, loss 0.597257.
Train: 2018-07-31T09:54:38.136401: step 15001, loss 0.63061.
Train: 2018-07-31T09:54:38.308261: step 15002, loss 0.61446.
Train: 2018-07-31T09:54:38.480101: step 15003, loss 0.603682.
Train: 2018-07-31T09:54:38.667527: step 15004, loss 0.595654.
Train: 2018-07-31T09:54:38.854982: step 15005, loss 0.629078.
Train: 2018-07-31T09:54:39.026817: step 15006, loss 0.676467.
Train: 2018-07-31T09:54:39.214275: step 15007, loss 0.602529.
Train: 2018-07-31T09:54:39.386139: step 15008, loss 0.611017.
Train: 2018-07-31T09:54:39.557973: step 15009, loss 0.611233.
Train: 2018-07-31T09:54:39.745430: step 15010, loss 0.553844.
Test: 2018-07-31T09:54:39.979751: step 15010, loss 0.593505.
Train: 2018-07-31T09:54:40.167199: step 15011, loss 0.599405.
Train: 2018-07-31T09:54:40.339044: step 15012, loss 0.636441.
Train: 2018-07-31T09:54:40.526497: step 15013, loss 0.597824.
Train: 2018-07-31T09:54:40.698302: step 15014, loss 0.642564.
Train: 2018-07-31T09:54:40.885759: step 15015, loss 0.555081.
Train: 2018-07-31T09:54:41.057593: step 15016, loss 0.543848.
Train: 2018-07-31T09:54:41.229457: step 15017, loss 0.571127.
Train: 2018-07-31T09:54:41.416907: step 15018, loss 0.576618.
Train: 2018-07-31T09:54:41.588717: step 15019, loss 0.578595.
Train: 2018-07-31T09:54:41.760583: step 15020, loss 0.489754.
Test: 2018-07-31T09:54:42.010494: step 15020, loss 0.592681.
Train: 2018-07-31T09:54:42.229223: step 15021, loss 0.692606.
Train: 2018-07-31T09:54:42.401052: step 15022, loss 0.630421.
Train: 2018-07-31T09:54:42.588513: step 15023, loss 0.617076.
Train: 2018-07-31T09:54:42.760344: step 15024, loss 0.618959.
Train: 2018-07-31T09:54:42.947809: step 15025, loss 0.627177.
Train: 2018-07-31T09:54:43.135261: step 15026, loss 0.609935.
Train: 2018-07-31T09:54:43.307065: step 15027, loss 0.543482.
Train: 2018-07-31T09:54:43.478900: step 15028, loss 0.661197.
Train: 2018-07-31T09:54:43.666355: step 15029, loss 0.626381.
Train: 2018-07-31T09:54:43.838191: step 15030, loss 0.653623.
Test: 2018-07-31T09:54:44.088134: step 15030, loss 0.594674.
Train: 2018-07-31T09:54:44.259968: step 15031, loss 0.626372.
Train: 2018-07-31T09:54:44.447423: step 15032, loss 0.63427.
Train: 2018-07-31T09:54:44.619257: step 15033, loss 0.619545.
Train: 2018-07-31T09:54:44.806714: step 15034, loss 0.588813.
Train: 2018-07-31T09:54:44.978550: step 15035, loss 0.676218.
Train: 2018-07-31T09:54:45.166005: step 15036, loss 0.653615.
Train: 2018-07-31T09:54:45.337874: step 15037, loss 0.585685.
Train: 2018-07-31T09:54:45.525326: step 15038, loss 0.612204.
Train: 2018-07-31T09:54:45.697163: step 15039, loss 0.678741.
Train: 2018-07-31T09:54:45.884587: step 15040, loss 0.628311.
Test: 2018-07-31T09:54:46.118939: step 15040, loss 0.591216.
Train: 2018-07-31T09:54:46.306393: step 15041, loss 0.578891.
Train: 2018-07-31T09:54:46.478197: step 15042, loss 0.599559.
Train: 2018-07-31T09:54:46.665653: step 15043, loss 0.591772.
Train: 2018-07-31T09:54:46.868757: step 15044, loss 0.626418.
Train: 2018-07-31T09:54:47.056212: step 15045, loss 0.554516.
Train: 2018-07-31T09:54:47.228052: step 15046, loss 0.726909.
Train: 2018-07-31T09:54:47.415509: step 15047, loss 0.526263.
Train: 2018-07-31T09:54:47.602959: step 15048, loss 0.647536.
Train: 2018-07-31T09:54:47.774799: step 15049, loss 0.561689.
Train: 2018-07-31T09:54:47.962259: step 15050, loss 0.562287.
Test: 2018-07-31T09:54:48.196575: step 15050, loss 0.580614.
Train: 2018-07-31T09:54:48.384002: step 15051, loss 0.578399.
Train: 2018-07-31T09:54:48.571457: step 15052, loss 0.626028.
Train: 2018-07-31T09:54:48.743293: step 15053, loss 0.523883.
Train: 2018-07-31T09:54:48.915157: step 15054, loss 0.598551.
Train: 2018-07-31T09:54:49.102613: step 15055, loss 0.638093.
Train: 2018-07-31T09:54:49.274450: step 15056, loss 0.597865.
Train: 2018-07-31T09:54:49.446280: step 15057, loss 0.57351.
Train: 2018-07-31T09:54:49.633708: step 15058, loss 0.580804.
Train: 2018-07-31T09:54:49.805568: step 15059, loss 0.518538.
Train: 2018-07-31T09:54:49.977404: step 15060, loss 0.596171.
Test: 2018-07-31T09:54:50.211698: step 15060, loss 0.576263.
Train: 2018-07-31T09:54:50.399154: step 15061, loss 0.522904.
Train: 2018-07-31T09:54:50.570989: step 15062, loss 0.528781.
Train: 2018-07-31T09:54:50.742854: step 15063, loss 0.581196.
Train: 2018-07-31T09:54:50.930310: step 15064, loss 0.570808.
Train: 2018-07-31T09:54:51.102145: step 15065, loss 0.511934.
Train: 2018-07-31T09:54:51.273979: step 15066, loss 0.682967.
Train: 2018-07-31T09:54:51.461406: step 15067, loss 0.484497.
Train: 2018-07-31T09:54:51.633266: step 15068, loss 0.647553.
Train: 2018-07-31T09:54:51.805113: step 15069, loss 0.564908.
Train: 2018-07-31T09:54:51.976911: step 15070, loss 0.643891.
Test: 2018-07-31T09:54:52.211232: step 15070, loss 0.57599.
Train: 2018-07-31T09:54:52.398687: step 15071, loss 0.666382.
Train: 2018-07-31T09:54:52.570551: step 15072, loss 0.622641.
Train: 2018-07-31T09:54:52.742356: step 15073, loss 0.549448.
Train: 2018-07-31T09:54:52.929813: step 15074, loss 0.564965.
Train: 2018-07-31T09:54:53.101677: step 15075, loss 0.537894.
Train: 2018-07-31T09:54:53.273482: step 15076, loss 0.599337.
Train: 2018-07-31T09:54:53.445349: step 15077, loss 0.643623.
Train: 2018-07-31T09:54:53.617152: step 15078, loss 0.573374.
Train: 2018-07-31T09:54:53.789016: step 15079, loss 0.613794.
Train: 2018-07-31T09:54:53.976472: step 15080, loss 0.514532.
Test: 2018-07-31T09:54:54.210762: step 15080, loss 0.572527.
Train: 2018-07-31T09:54:54.398218: step 15081, loss 0.617577.
Train: 2018-07-31T09:54:54.570083: step 15082, loss 0.579864.
Train: 2018-07-31T09:54:54.741888: step 15083, loss 0.519607.
Train: 2018-07-31T09:54:54.913723: step 15084, loss 0.534537.
Train: 2018-07-31T09:54:55.085557: step 15085, loss 0.600156.
Train: 2018-07-31T09:54:55.257392: step 15086, loss 0.505565.
Train: 2018-07-31T09:54:55.429257: step 15087, loss 0.663755.
Train: 2018-07-31T09:54:55.601093: step 15088, loss 0.633033.
Train: 2018-07-31T09:54:55.788548: step 15089, loss 0.605752.
Train: 2018-07-31T09:54:55.960387: step 15090, loss 0.559088.
Test: 2018-07-31T09:54:56.194707: step 15090, loss 0.570812.
Train: 2018-07-31T09:54:56.382128: step 15091, loss 0.597491.
Train: 2018-07-31T09:54:56.553963: step 15092, loss 0.558768.
Train: 2018-07-31T09:54:56.725831: step 15093, loss 0.62397.
Train: 2018-07-31T09:54:56.897669: step 15094, loss 0.479098.
Train: 2018-07-31T09:54:57.069467: step 15095, loss 0.628428.
Train: 2018-07-31T09:54:57.256954: step 15096, loss 0.606966.
Train: 2018-07-31T09:54:57.428759: step 15097, loss 0.636962.
Train: 2018-07-31T09:54:57.600623: step 15098, loss 0.629195.
Train: 2018-07-31T09:54:57.788080: step 15099, loss 0.644593.
Train: 2018-07-31T09:54:57.959918: step 15100, loss 0.655182.
Test: 2018-07-31T09:54:58.194232: step 15100, loss 0.57202.
