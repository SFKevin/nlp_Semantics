Train: 2018-08-05T23:20:15.504492: step 1, loss 1.27963.
Train: 2018-08-05T23:20:15.723189: step 2, loss 3.05681.
Train: 2018-08-05T23:20:15.926267: step 3, loss 1.12062.
Train: 2018-08-05T23:20:16.113692: step 4, loss 1.06654.
Train: 2018-08-05T23:20:16.316769: step 5, loss 1.01617.
Train: 2018-08-05T23:20:16.504225: step 6, loss 0.970137.
Train: 2018-08-05T23:20:16.707303: step 7, loss 0.931619.
Train: 2018-08-05T23:20:16.894758: step 8, loss 0.898614.
Train: 2018-08-05T23:20:17.082215: step 9, loss 0.870643.
Train: 2018-08-05T23:20:17.269671: step 10, loss 0.848691.
Test: 2018-08-05T23:20:17.707100: step 10, loss 0.833467.
Train: 2018-08-05T23:20:17.894553: step 11, loss 0.830354.
Train: 2018-08-05T23:20:18.082012: step 12, loss 0.829594.
Train: 2018-08-05T23:20:18.269437: step 13, loss 0.805378.
Train: 2018-08-05T23:20:18.441300: step 14, loss 0.795105.
Train: 2018-08-05T23:20:18.628750: step 15, loss 0.77589.
Train: 2018-08-05T23:20:18.816212: step 16, loss 0.778389.
Train: 2018-08-05T23:20:18.988214: step 17, loss 0.769058.
Train: 2018-08-05T23:20:19.175701: step 18, loss 0.769195.
Train: 2018-08-05T23:20:19.363136: step 19, loss 0.740476.
Train: 2018-08-05T23:20:19.534991: step 20, loss 0.771802.
Test: 2018-08-05T23:20:19.784901: step 20, loss 0.761156.
Train: 2018-08-05T23:20:19.956767: step 21, loss 0.848276.
Train: 2018-08-05T23:20:20.128571: step 22, loss 0.783568.
Train: 2018-08-05T23:20:20.316027: step 23, loss 0.774911.
Train: 2018-08-05T23:20:20.503514: step 24, loss 0.735209.
Train: 2018-08-05T23:20:20.675317: step 25, loss 0.760458.
Train: 2018-08-05T23:20:20.862803: step 26, loss 0.735946.
Train: 2018-08-05T23:20:21.034638: step 27, loss 0.751939.
Train: 2018-08-05T23:20:21.222088: step 28, loss 0.771826.
Train: 2018-08-05T23:20:21.393928: step 29, loss 0.744209.
Train: 2018-08-05T23:20:21.581354: step 30, loss 0.741998.
Test: 2018-08-05T23:20:21.815705: step 30, loss 0.748721.
Train: 2018-08-05T23:20:22.003161: step 31, loss 0.750283.
Train: 2018-08-05T23:20:22.190615: step 32, loss 0.751724.
Train: 2018-08-05T23:20:22.393687: step 33, loss 0.732379.
Train: 2018-08-05T23:20:22.565521: step 34, loss 0.742902.
Train: 2018-08-05T23:20:22.752977: step 35, loss 0.731916.
Train: 2018-08-05T23:20:22.940439: step 36, loss 0.712065.
Train: 2018-08-05T23:20:23.127897: step 37, loss 0.731099.
Train: 2018-08-05T23:20:23.299730: step 38, loss 0.715293.
Train: 2018-08-05T23:20:23.487180: step 39, loss 0.696511.
Train: 2018-08-05T23:20:23.674642: step 40, loss 0.601794.
Test: 2018-08-05T23:20:23.908930: step 40, loss 0.596268.
Train: 2018-08-05T23:20:24.080790: step 41, loss 0.639104.
Train: 2018-08-05T23:20:24.268252: step 42, loss 0.691437.
Train: 2018-08-05T23:20:24.455708: step 43, loss 0.616949.
Train: 2018-08-05T23:20:24.627544: step 44, loss 0.631467.
Train: 2018-08-05T23:20:24.814998: step 45, loss 0.538169.
Train: 2018-08-05T23:20:24.986832: step 46, loss 0.651953.
Train: 2018-08-05T23:20:25.174258: step 47, loss 0.596169.
Train: 2018-08-05T23:20:25.346123: step 48, loss 0.573151.
Train: 2018-08-05T23:20:25.533548: step 49, loss 0.646992.
Train: 2018-08-05T23:20:25.705383: step 50, loss 0.678441.
Test: 2018-08-05T23:20:25.955351: step 50, loss 0.597544.
Train: 2018-08-05T23:20:26.127191: step 51, loss 0.621073.
Train: 2018-08-05T23:20:26.314645: step 52, loss 0.608799.
Train: 2018-08-05T23:20:26.502095: step 53, loss 0.581502.
Train: 2018-08-05T23:20:26.700249: step 54, loss 0.613681.
Train: 2018-08-05T23:20:26.872102: step 55, loss 0.710871.
Train: 2018-08-05T23:20:27.059555: step 56, loss 0.542731.
Train: 2018-08-05T23:20:27.231390: step 57, loss 0.661501.
Train: 2018-08-05T23:20:27.418816: step 58, loss 0.557068.
Train: 2018-08-05T23:20:27.606302: step 59, loss 0.666161.
Train: 2018-08-05T23:20:27.793759: step 60, loss 0.529995.
Test: 2018-08-05T23:20:28.028079: step 60, loss 0.588258.
Train: 2018-08-05T23:20:28.215505: step 61, loss 0.663548.
Train: 2018-08-05T23:20:28.387371: step 62, loss 0.671132.
Train: 2018-08-05T23:20:28.574794: step 63, loss 0.595176.
Train: 2018-08-05T23:20:28.762283: step 64, loss 0.599651.
Train: 2018-08-05T23:20:28.934114: step 65, loss 0.60351.
Train: 2018-08-05T23:20:29.121570: step 66, loss 0.659474.
Train: 2018-08-05T23:20:29.308996: step 67, loss 0.615139.
Train: 2018-08-05T23:20:29.496482: step 68, loss 0.671203.
Train: 2018-08-05T23:20:29.668316: step 69, loss 0.589273.
Train: 2018-08-05T23:20:29.855766: step 70, loss 0.562699.
Test: 2018-08-05T23:20:30.090093: step 70, loss 0.586975.
Train: 2018-08-05T23:20:30.277548: step 71, loss 0.652058.
Train: 2018-08-05T23:20:30.465004: step 72, loss 0.604588.
Train: 2018-08-05T23:20:30.652429: step 73, loss 0.592621.
Train: 2018-08-05T23:20:30.839917: step 74, loss 0.666898.
Train: 2018-08-05T23:20:31.027344: step 75, loss 0.70837.
Train: 2018-08-05T23:20:31.214798: step 76, loss 0.634019.
Train: 2018-08-05T23:20:31.386666: step 77, loss 0.625622.
Train: 2018-08-05T23:20:31.574118: step 78, loss 0.539008.
Train: 2018-08-05T23:20:31.745956: step 79, loss 0.593693.
Train: 2018-08-05T23:20:31.933405: step 80, loss 0.569735.
Test: 2018-08-05T23:20:32.167698: step 80, loss 0.58104.
Train: 2018-08-05T23:20:32.355184: step 81, loss 0.683789.
Train: 2018-08-05T23:20:32.542611: step 82, loss 0.652836.
Train: 2018-08-05T23:20:32.714475: step 83, loss 0.647783.
Train: 2018-08-05T23:20:32.901931: step 84, loss 0.60639.
Train: 2018-08-05T23:20:33.089356: step 85, loss 0.596669.
Train: 2018-08-05T23:20:33.261191: step 86, loss 0.575312.
Train: 2018-08-05T23:20:33.448677: step 87, loss 0.632265.
Train: 2018-08-05T23:20:33.636128: step 88, loss 0.625473.
Train: 2018-08-05T23:20:33.807967: step 89, loss 0.592899.
Train: 2018-08-05T23:20:33.995427: step 90, loss 0.609356.
Test: 2018-08-05T23:20:34.229746: step 90, loss 0.581427.
Train: 2018-08-05T23:20:34.417199: step 91, loss 0.689609.
Train: 2018-08-05T23:20:34.604655: step 92, loss 0.581959.
Train: 2018-08-05T23:20:34.792105: step 93, loss 0.633308.
Train: 2018-08-05T23:20:34.979567: step 94, loss 0.691381.
Train: 2018-08-05T23:20:35.151401: step 95, loss 0.541211.
Train: 2018-08-05T23:20:35.338857: step 96, loss 0.599399.
Train: 2018-08-05T23:20:35.510695: step 97, loss 0.620788.
Train: 2018-08-05T23:20:35.698118: step 98, loss 0.602649.
Train: 2018-08-05T23:20:35.885573: step 99, loss 0.605509.
Train: 2018-08-05T23:20:36.073060: step 100, loss 0.597547.
Test: 2018-08-05T23:20:36.307380: step 100, loss 0.577961.
Train: 2018-08-05T23:20:37.072827: step 101, loss 0.590514.
Train: 2018-08-05T23:20:37.260250: step 102, loss 0.508278.
Train: 2018-08-05T23:20:37.432117: step 103, loss 0.592454.
Train: 2018-08-05T23:20:37.619571: step 104, loss 0.616497.
Train: 2018-08-05T23:20:37.807027: step 105, loss 0.581662.
Train: 2018-08-05T23:20:37.994485: step 106, loss 0.594884.
Train: 2018-08-05T23:20:38.181938: step 107, loss 0.575317.
Train: 2018-08-05T23:20:38.353773: step 108, loss 0.597833.
Train: 2018-08-05T23:20:38.541229: step 109, loss 0.714007.
Train: 2018-08-05T23:20:38.728702: step 110, loss 0.595401.
Test: 2018-08-05T23:20:38.963006: step 110, loss 0.584324.
Train: 2018-08-05T23:20:39.150430: step 111, loss 0.609326.
Train: 2018-08-05T23:20:39.337886: step 112, loss 0.623799.
Train: 2018-08-05T23:20:39.509745: step 113, loss 0.60671.
Train: 2018-08-05T23:20:39.697202: step 114, loss 0.676608.
Train: 2018-08-05T23:20:39.884666: step 115, loss 0.623232.
Train: 2018-08-05T23:20:40.072119: step 116, loss 0.61099.
Train: 2018-08-05T23:20:40.259579: step 117, loss 0.645172.
Train: 2018-08-05T23:20:40.447031: step 118, loss 0.616466.
Train: 2018-08-05T23:20:40.618865: step 119, loss 0.655925.
Train: 2018-08-05T23:20:40.806321: step 120, loss 0.569253.
Test: 2018-08-05T23:20:41.040641: step 120, loss 0.583362.
Train: 2018-08-05T23:20:41.228097: step 121, loss 0.602689.
Train: 2018-08-05T23:20:41.399901: step 122, loss 0.644706.
Train: 2018-08-05T23:20:41.587357: step 123, loss 0.56308.
Train: 2018-08-05T23:20:41.777464: step 124, loss 0.604908.
Train: 2018-08-05T23:20:41.949267: step 125, loss 0.569273.
Train: 2018-08-05T23:20:42.136751: step 126, loss 0.532893.
Train: 2018-08-05T23:20:42.324208: step 127, loss 0.676027.
Train: 2018-08-05T23:20:42.511663: step 128, loss 0.52826.
Train: 2018-08-05T23:20:42.683468: step 129, loss 0.511086.
Train: 2018-08-05T23:20:42.886545: step 130, loss 0.57283.
Test: 2018-08-05T23:20:43.120883: step 130, loss 0.571772.
Train: 2018-08-05T23:20:43.323943: step 131, loss 0.538471.
Train: 2018-08-05T23:20:43.511398: step 132, loss 0.630583.
Train: 2018-08-05T23:20:43.704279: step 133, loss 0.603429.
Train: 2018-08-05T23:20:43.879216: step 134, loss 0.606679.
Train: 2018-08-05T23:20:44.066672: step 135, loss 0.627932.
Train: 2018-08-05T23:20:44.254128: step 136, loss 0.586987.
Train: 2018-08-05T23:20:44.425957: step 137, loss 0.581842.
Train: 2018-08-05T23:20:44.613418: step 138, loss 0.631509.
Train: 2018-08-05T23:20:44.800874: step 139, loss 0.580151.
Train: 2018-08-05T23:20:44.972709: step 140, loss 0.612261.
Test: 2018-08-05T23:20:45.207032: step 140, loss 0.576087.
Train: 2018-08-05T23:20:45.394485: step 141, loss 0.612708.
Train: 2018-08-05T23:20:45.581944: step 142, loss 0.563165.
Train: 2018-08-05T23:20:45.769391: step 143, loss 0.556062.
Train: 2018-08-05T23:20:45.956823: step 144, loss 0.539214.
Train: 2018-08-05T23:20:46.128687: step 145, loss 0.534317.
Train: 2018-08-05T23:20:46.316142: step 146, loss 0.615188.
Train: 2018-08-05T23:20:46.503598: step 147, loss 0.566095.
Train: 2018-08-05T23:20:46.691054: step 148, loss 0.527432.
Train: 2018-08-05T23:20:46.878480: step 149, loss 0.612397.
Train: 2018-08-05T23:20:47.065937: step 150, loss 0.604043.
Test: 2018-08-05T23:20:47.300289: step 150, loss 0.570218.
Train: 2018-08-05T23:20:47.487713: step 151, loss 0.506958.
Train: 2018-08-05T23:20:47.690819: step 152, loss 0.601574.
Train: 2018-08-05T23:20:47.878286: step 153, loss 0.565281.
Train: 2018-08-05T23:20:48.065702: step 154, loss 0.625331.
Train: 2018-08-05T23:20:48.253188: step 155, loss 0.671488.
Train: 2018-08-05T23:20:48.456267: step 156, loss 0.619523.
Train: 2018-08-05T23:20:48.643720: step 157, loss 0.63343.
Train: 2018-08-05T23:20:48.831179: step 158, loss 0.570662.
Train: 2018-08-05T23:20:49.018632: step 159, loss 0.612876.
Train: 2018-08-05T23:20:49.206090: step 160, loss 0.588385.
Test: 2018-08-05T23:20:49.440378: step 160, loss 0.576451.
Train: 2018-08-05T23:20:49.627864: step 161, loss 0.591367.
Train: 2018-08-05T23:20:49.815319: step 162, loss 0.587334.
Train: 2018-08-05T23:20:49.994040: step 163, loss 0.598608.
Train: 2018-08-05T23:20:50.196148: step 164, loss 0.653862.
Train: 2018-08-05T23:20:50.383574: step 165, loss 0.59075.
Train: 2018-08-05T23:20:50.586678: step 166, loss 0.640348.
Train: 2018-08-05T23:20:50.758486: step 167, loss 0.618269.
Train: 2018-08-05T23:20:50.961563: step 168, loss 0.549402.
Train: 2018-08-05T23:20:51.149050: step 169, loss 0.645119.
Train: 2018-08-05T23:20:51.336505: step 170, loss 0.63756.
Test: 2018-08-05T23:20:51.586417: step 170, loss 0.573946.
Train: 2018-08-05T23:20:51.773873: step 171, loss 0.544874.
Train: 2018-08-05T23:20:51.961328: step 172, loss 0.612298.
Train: 2018-08-05T23:20:52.133193: step 173, loss 0.559888.
Train: 2018-08-05T23:20:52.336266: step 174, loss 0.58623.
Train: 2018-08-05T23:20:52.523721: step 175, loss 0.613008.
Train: 2018-08-05T23:20:52.711181: step 176, loss 0.626296.
Train: 2018-08-05T23:20:52.898638: step 177, loss 0.556849.
Train: 2018-08-05T23:20:53.086093: step 178, loss 0.598675.
Train: 2018-08-05T23:20:53.273520: step 179, loss 0.604082.
Train: 2018-08-05T23:20:53.476626: step 180, loss 0.622493.
Test: 2018-08-05T23:20:53.710947: step 180, loss 0.570484.
Train: 2018-08-05T23:20:53.913386: step 181, loss 0.487112.
Train: 2018-08-05T23:20:54.100867: step 182, loss 0.663752.
Train: 2018-08-05T23:20:54.288328: step 183, loss 0.565585.
Train: 2018-08-05T23:20:54.486447: step 184, loss 0.621784.
Train: 2018-08-05T23:20:54.673870: step 185, loss 0.524081.
Train: 2018-08-05T23:20:54.861356: step 186, loss 0.584074.
Train: 2018-08-05T23:20:55.048783: step 187, loss 0.569677.
Train: 2018-08-05T23:20:55.251861: step 188, loss 0.478143.
Train: 2018-08-05T23:20:55.439316: step 189, loss 0.517636.
Train: 2018-08-05T23:20:55.626773: step 190, loss 0.633222.
Test: 2018-08-05T23:20:55.866211: step 190, loss 0.567991.
Train: 2018-08-05T23:20:56.050392: step 191, loss 0.637504.
Train: 2018-08-05T23:20:56.237879: step 192, loss 0.602333.
Train: 2018-08-05T23:20:56.425328: step 193, loss 0.691799.
Train: 2018-08-05T23:20:56.612789: step 194, loss 0.571441.
Train: 2018-08-05T23:20:56.800246: step 195, loss 0.620717.
Train: 2018-08-05T23:20:56.987682: step 196, loss 0.623311.
Train: 2018-08-05T23:20:57.175161: step 197, loss 0.544047.
Train: 2018-08-05T23:20:57.362614: step 198, loss 0.635845.
Train: 2018-08-05T23:20:57.550069: step 199, loss 0.549609.
Train: 2018-08-05T23:20:57.745689: step 200, loss 0.578679.
Test: 2018-08-05T23:20:57.980039: step 200, loss 0.576842.
Train: 2018-08-05T23:20:58.714235: step 201, loss 0.583159.
Train: 2018-08-05T23:20:58.917318: step 202, loss 0.61736.
Train: 2018-08-05T23:20:59.120397: step 203, loss 0.566457.
Train: 2018-08-05T23:20:59.292199: step 204, loss 0.58335.
Train: 2018-08-05T23:20:59.479656: step 205, loss 0.537287.
Train: 2018-08-05T23:20:59.667141: step 206, loss 0.601684.
Train: 2018-08-05T23:20:59.854567: step 207, loss 0.584715.
Train: 2018-08-05T23:21:00.042023: step 208, loss 0.639471.
Train: 2018-08-05T23:21:00.229509: step 209, loss 0.628105.
Train: 2018-08-05T23:21:00.432586: step 210, loss 0.601145.
Test: 2018-08-05T23:21:00.666906: step 210, loss 0.56738.
Train: 2018-08-05T23:21:00.854363: step 211, loss 0.566727.
Train: 2018-08-05T23:21:01.057410: step 212, loss 0.522517.
Train: 2018-08-05T23:21:01.244896: step 213, loss 0.565849.
Train: 2018-08-05T23:21:01.432321: step 214, loss 0.663929.
Train: 2018-08-05T23:21:01.604185: step 215, loss 0.623831.
Train: 2018-08-05T23:21:01.807235: step 216, loss 0.62287.
Train: 2018-08-05T23:21:01.994688: step 217, loss 0.577195.
Train: 2018-08-05T23:21:02.182145: step 218, loss 0.61628.
Train: 2018-08-05T23:21:02.385257: step 219, loss 0.615789.
Train: 2018-08-05T23:21:02.557086: step 220, loss 0.560781.
Test: 2018-08-05T23:21:02.807022: step 220, loss 0.568381.
Train: 2018-08-05T23:21:02.994477: step 221, loss 0.550538.
Train: 2018-08-05T23:21:03.181909: step 222, loss 0.576465.
Train: 2018-08-05T23:21:03.369365: step 223, loss 0.616928.
Train: 2018-08-05T23:21:03.556852: step 224, loss 0.608113.
Train: 2018-08-05T23:21:03.728685: step 225, loss 0.583727.
Train: 2018-08-05T23:21:03.917542: step 226, loss 0.545321.
Train: 2018-08-05T23:21:04.136271: step 227, loss 0.615446.
Train: 2018-08-05T23:21:04.323699: step 228, loss 0.531652.
Train: 2018-08-05T23:21:04.511177: step 229, loss 0.579342.
Train: 2018-08-05T23:21:04.698643: step 230, loss 0.586966.
Test: 2018-08-05T23:21:04.932961: step 230, loss 0.564594.
Train: 2018-08-05T23:21:05.120415: step 231, loss 0.584275.
Train: 2018-08-05T23:21:05.292249: step 232, loss 0.539927.
Train: 2018-08-05T23:21:05.479705: step 233, loss 0.575976.
Train: 2018-08-05T23:21:05.651539: step 234, loss 0.613671.
Train: 2018-08-05T23:21:05.838999: step 235, loss 0.601254.
Train: 2018-08-05T23:21:06.026454: step 236, loss 0.58929.
Train: 2018-08-05T23:21:06.213911: step 237, loss 0.546021.
Train: 2018-08-05T23:21:06.401333: step 238, loss 0.57707.
Train: 2018-08-05T23:21:06.573198: step 239, loss 0.627883.
Train: 2018-08-05T23:21:06.760624: step 240, loss 0.567722.
Test: 2018-08-05T23:21:07.010595: step 240, loss 0.562128.
Train: 2018-08-05T23:21:07.198051: step 241, loss 0.619364.
Train: 2018-08-05T23:21:07.385507: step 242, loss 0.571437.
Train: 2018-08-05T23:21:07.572963: step 243, loss 0.527916.
Train: 2018-08-05T23:21:07.744768: step 244, loss 0.604522.
Train: 2018-08-05T23:21:07.932253: step 245, loss 0.615365.
Train: 2018-08-05T23:21:08.119709: step 246, loss 0.650399.
Train: 2018-08-05T23:21:08.291544: step 247, loss 0.593327.
Train: 2018-08-05T23:21:08.478994: step 248, loss 0.619071.
Train: 2018-08-05T23:21:08.650829: step 249, loss 0.564348.
Train: 2018-08-05T23:21:08.838293: step 250, loss 0.604811.
Test: 2018-08-05T23:21:09.072608: step 250, loss 0.573697.
Train: 2018-08-05T23:21:09.260035: step 251, loss 0.604592.
Train: 2018-08-05T23:21:09.447516: step 252, loss 0.55122.
Train: 2018-08-05T23:21:09.619351: step 253, loss 0.548226.
Train: 2018-08-05T23:21:09.791191: step 254, loss 0.556764.
Train: 2018-08-05T23:21:09.978642: step 255, loss 0.607911.
Train: 2018-08-05T23:21:10.166072: step 256, loss 0.522251.
Train: 2018-08-05T23:21:10.337940: step 257, loss 0.507242.
Train: 2018-08-05T23:21:10.509741: step 258, loss 0.599988.
Train: 2018-08-05T23:21:10.697198: step 259, loss 0.544989.
Train: 2018-08-05T23:21:10.869063: step 260, loss 0.634361.
Test: 2018-08-05T23:21:11.103383: step 260, loss 0.571903.
Train: 2018-08-05T23:21:11.290809: step 261, loss 0.581429.
Train: 2018-08-05T23:21:11.462676: step 262, loss 0.528549.
Train: 2018-08-05T23:21:11.650099: step 263, loss 0.536739.
Train: 2018-08-05T23:21:11.821934: step 264, loss 0.603559.
Train: 2018-08-05T23:21:11.993767: step 265, loss 0.569911.
Train: 2018-08-05T23:21:12.181253: step 266, loss 0.634439.
Train: 2018-08-05T23:21:12.353088: step 267, loss 0.550437.
Train: 2018-08-05T23:21:12.524893: step 268, loss 0.542344.
Train: 2018-08-05T23:21:12.712382: step 269, loss 0.59275.
Train: 2018-08-05T23:21:12.899835: step 270, loss 0.576191.
Test: 2018-08-05T23:21:13.134124: step 270, loss 0.569511.
Train: 2018-08-05T23:21:13.324672: step 271, loss 0.621916.
Train: 2018-08-05T23:21:13.500893: step 272, loss 0.602683.
Train: 2018-08-05T23:21:13.672760: step 273, loss 0.499562.
Train: 2018-08-05T23:21:13.844592: step 274, loss 0.595127.
Train: 2018-08-05T23:21:14.016426: step 275, loss 0.612646.
Train: 2018-08-05T23:21:14.188264: step 276, loss 0.53295.
Train: 2018-08-05T23:21:14.375717: step 277, loss 0.569267.
Train: 2018-08-05T23:21:14.547552: step 278, loss 0.586446.
Train: 2018-08-05T23:21:14.735511: step 279, loss 0.540702.
Train: 2018-08-05T23:21:14.908528: step 280, loss 0.574575.
Test: 2018-08-05T23:21:15.142859: step 280, loss 0.566605.
Train: 2018-08-05T23:21:15.330279: step 281, loss 0.558312.
Train: 2018-08-05T23:21:15.502114: step 282, loss 0.590642.
Train: 2018-08-05T23:21:15.673984: step 283, loss 0.57049.
Train: 2018-08-05T23:21:15.845815: step 284, loss 0.515615.
Train: 2018-08-05T23:21:16.017647: step 285, loss 0.571735.
Train: 2018-08-05T23:21:16.205075: step 286, loss 0.519927.
Train: 2018-08-05T23:21:16.376939: step 287, loss 0.611554.
Train: 2018-08-05T23:21:16.548767: step 288, loss 0.483386.
Train: 2018-08-05T23:21:16.720607: step 289, loss 0.615939.
Train: 2018-08-05T23:21:16.892442: step 290, loss 0.564524.
Test: 2018-08-05T23:21:17.142746: step 290, loss 0.563579.
Train: 2018-08-05T23:21:17.314606: step 291, loss 0.567858.
Train: 2018-08-05T23:21:17.470820: step 292, loss 0.575201.
Train: 2018-08-05T23:21:17.658281: step 293, loss 0.599761.
Train: 2018-08-05T23:21:17.814495: step 294, loss 0.631104.
Train: 2018-08-05T23:21:17.986329: step 295, loss 0.546695.
Train: 2018-08-05T23:21:18.158164: step 296, loss 0.537552.
Train: 2018-08-05T23:21:18.330000: step 297, loss 0.53826.
Train: 2018-08-05T23:21:18.486182: step 298, loss 0.567917.
Train: 2018-08-05T23:21:18.658017: step 299, loss 0.556653.
Train: 2018-08-05T23:21:18.829852: step 300, loss 0.568634.
Test: 2018-08-05T23:21:19.079793: step 300, loss 0.562487.
Train: 2018-08-05T23:21:19.829617: step 301, loss 0.546195.
Train: 2018-08-05T23:21:19.985859: step 302, loss 0.547067.
Train: 2018-08-05T23:21:20.157693: step 303, loss 0.575014.
Train: 2018-08-05T23:21:20.313909: step 304, loss 0.687974.
Train: 2018-08-05T23:21:20.470119: step 305, loss 0.560202.
Train: 2018-08-05T23:21:20.626304: step 306, loss 0.574029.
Train: 2018-08-05T23:21:20.798138: step 307, loss 0.650066.
Train: 2018-08-05T23:21:20.960139: step 308, loss 0.550173.
Train: 2018-08-05T23:21:21.116381: step 309, loss 0.552716.
Train: 2018-08-05T23:21:21.272595: step 310, loss 0.488943.
Test: 2018-08-05T23:21:21.512981: step 310, loss 0.561298.
Train: 2018-08-05T23:21:21.669180: step 311, loss 0.662776.
Train: 2018-08-05T23:21:21.825422: step 312, loss 0.552532.
Train: 2018-08-05T23:21:21.997258: step 313, loss 0.558498.
Train: 2018-08-05T23:21:22.153470: step 314, loss 0.551389.
Train: 2018-08-05T23:21:22.309653: step 315, loss 0.53048.
Train: 2018-08-05T23:21:22.465866: step 316, loss 0.56881.
Train: 2018-08-05T23:21:22.622079: step 317, loss 0.575381.
Train: 2018-08-05T23:21:22.778293: step 318, loss 0.565913.
Train: 2018-08-05T23:21:22.934506: step 319, loss 0.672446.
Train: 2018-08-05T23:21:23.090747: step 320, loss 0.536469.
Test: 2018-08-05T23:21:23.340691: step 320, loss 0.559941.
Train: 2018-08-05T23:21:23.496907: step 321, loss 0.606604.
Train: 2018-08-05T23:21:23.653087: step 322, loss 0.564875.
Train: 2018-08-05T23:21:23.824955: step 323, loss 0.613526.
Train: 2018-08-05T23:21:23.981136: step 324, loss 0.507548.
Train: 2018-08-05T23:21:24.137378: step 325, loss 0.5116.
Train: 2018-08-05T23:21:24.293591: step 326, loss 0.549004.
Train: 2018-08-05T23:21:24.449780: step 327, loss 0.563814.
Train: 2018-08-05T23:21:24.605987: step 328, loss 0.601788.
Train: 2018-08-05T23:21:24.762201: step 329, loss 0.611067.
Train: 2018-08-05T23:21:24.902793: step 330, loss 0.577568.
Test: 2018-08-05T23:21:25.152751: step 330, loss 0.559056.
Train: 2018-08-05T23:21:25.308978: step 331, loss 0.618823.
Train: 2018-08-05T23:21:25.465160: step 332, loss 0.527671.
Train: 2018-08-05T23:21:25.621374: step 333, loss 0.58541.
Train: 2018-08-05T23:21:25.777588: step 334, loss 0.594739.
Train: 2018-08-05T23:21:25.949421: step 335, loss 0.571325.
Train: 2018-08-05T23:21:26.090044: step 336, loss 0.577128.
Train: 2018-08-05T23:21:26.246258: step 337, loss 0.488345.
Train: 2018-08-05T23:21:26.402440: step 338, loss 0.516805.
Train: 2018-08-05T23:21:26.574274: step 339, loss 0.581516.
Train: 2018-08-05T23:21:26.730488: step 340, loss 0.584337.
Test: 2018-08-05T23:21:26.964808: step 340, loss 0.558149.
Train: 2018-08-05T23:21:27.121022: step 341, loss 0.531811.
Train: 2018-08-05T23:21:27.292856: step 342, loss 0.510436.
Train: 2018-08-05T23:21:27.449094: step 343, loss 0.565105.
Train: 2018-08-05T23:21:27.605314: step 344, loss 0.590615.
Train: 2018-08-05T23:21:27.761528: step 345, loss 0.583184.
Train: 2018-08-05T23:21:27.917742: step 346, loss 0.585534.
Train: 2018-08-05T23:21:28.089543: step 347, loss 0.574856.
Train: 2018-08-05T23:21:28.245757: step 348, loss 0.533128.
Train: 2018-08-05T23:21:28.417591: step 349, loss 0.552815.
Train: 2018-08-05T23:21:28.573811: step 350, loss 0.519666.
Test: 2018-08-05T23:21:28.805174: step 350, loss 0.557253.
Train: 2018-08-05T23:21:28.976975: step 351, loss 0.553385.
Train: 2018-08-05T23:21:29.133217: step 352, loss 0.562381.
Train: 2018-08-05T23:21:29.289425: step 353, loss 0.582723.
Train: 2018-08-05T23:21:29.445644: step 354, loss 0.576317.
Train: 2018-08-05T23:21:29.617448: step 355, loss 0.584921.
Train: 2018-08-05T23:21:29.773662: step 356, loss 0.499303.
Train: 2018-08-05T23:21:29.929908: step 357, loss 0.531252.
Train: 2018-08-05T23:21:30.101709: step 358, loss 0.553135.
Train: 2018-08-05T23:21:30.242331: step 359, loss 0.551014.
Train: 2018-08-05T23:21:30.414135: step 360, loss 0.526689.
Test: 2018-08-05T23:21:30.648489: step 360, loss 0.556819.
Train: 2018-08-05T23:21:30.804669: step 361, loss 0.624229.
Train: 2018-08-05T23:21:30.960913: step 362, loss 0.598534.
Train: 2018-08-05T23:21:31.132716: step 363, loss 0.61429.
Train: 2018-08-05T23:21:31.288963: step 364, loss 0.618335.
Train: 2018-08-05T23:21:31.445144: step 365, loss 0.570893.
Train: 2018-08-05T23:21:31.617013: step 366, loss 0.595562.
Train: 2018-08-05T23:21:31.773191: step 367, loss 0.564005.
Train: 2018-08-05T23:21:31.929438: step 368, loss 0.581077.
Train: 2018-08-05T23:21:32.101269: step 369, loss 0.618692.
Train: 2018-08-05T23:21:32.273074: step 370, loss 0.58475.
Test: 2018-08-05T23:21:32.507393: step 370, loss 0.557897.
Train: 2018-08-05T23:21:32.663606: step 371, loss 0.595443.
Train: 2018-08-05T23:21:32.819819: step 372, loss 0.559704.
Train: 2018-08-05T23:21:32.976033: step 373, loss 0.549032.
Train: 2018-08-05T23:21:33.132246: step 374, loss 0.58619.
Train: 2018-08-05T23:21:33.288459: step 375, loss 0.548198.
Train: 2018-08-05T23:21:33.460294: step 376, loss 0.531032.
Train: 2018-08-05T23:21:33.616531: step 377, loss 0.575778.
Train: 2018-08-05T23:21:33.772720: step 378, loss 0.632309.
Train: 2018-08-05T23:21:33.928958: step 379, loss 0.555332.
Train: 2018-08-05T23:21:34.085147: step 380, loss 0.620646.
Test: 2018-08-05T23:21:34.335099: step 380, loss 0.55758.
Train: 2018-08-05T23:21:34.491332: step 381, loss 0.534775.
Train: 2018-08-05T23:21:34.647545: step 382, loss 0.650718.
Train: 2018-08-05T23:21:34.803752: step 383, loss 0.554619.
Train: 2018-08-05T23:21:34.959942: step 384, loss 0.579726.
Train: 2018-08-05T23:21:35.131775: step 385, loss 0.53188.
Train: 2018-08-05T23:21:35.287989: step 386, loss 0.54846.
Train: 2018-08-05T23:21:35.444202: step 387, loss 0.582521.
Train: 2018-08-05T23:21:35.600416: step 388, loss 0.602117.
Train: 2018-08-05T23:21:35.756628: step 389, loss 0.664541.
Train: 2018-08-05T23:21:35.928487: step 390, loss 0.562312.
Test: 2018-08-05T23:21:36.162816: step 390, loss 0.557242.
Train: 2018-08-05T23:21:36.319026: step 391, loss 0.58518.
Train: 2018-08-05T23:21:36.475210: step 392, loss 0.592588.
Train: 2018-08-05T23:21:36.647044: step 393, loss 0.568862.
Train: 2018-08-05T23:21:36.803258: step 394, loss 0.538274.
Train: 2018-08-05T23:21:36.975132: step 395, loss 0.535939.
Train: 2018-08-05T23:21:37.131306: step 396, loss 0.575177.
Train: 2018-08-05T23:21:37.287548: step 397, loss 0.594443.
Train: 2018-08-05T23:21:37.443763: step 398, loss 0.579433.
Train: 2018-08-05T23:21:37.615597: step 399, loss 0.575348.
Train: 2018-08-05T23:21:37.771779: step 400, loss 0.630534.
Test: 2018-08-05T23:21:38.006131: step 400, loss 0.556793.
Train: 2018-08-05T23:21:38.724711: step 401, loss 0.511733.
Train: 2018-08-05T23:21:38.896545: step 402, loss 0.566882.
Train: 2018-08-05T23:21:39.052767: step 403, loss 0.591909.
Train: 2018-08-05T23:21:39.208942: step 404, loss 0.586212.
Train: 2018-08-05T23:21:39.365156: step 405, loss 0.5775.
Train: 2018-08-05T23:21:39.521399: step 406, loss 0.492675.
Train: 2018-08-05T23:21:39.677581: step 407, loss 0.534611.
Train: 2018-08-05T23:21:39.833819: step 408, loss 0.612615.
Train: 2018-08-05T23:21:40.005659: step 409, loss 0.550723.
Train: 2018-08-05T23:21:40.161843: step 410, loss 0.560933.
Test: 2018-08-05T23:21:40.396196: step 410, loss 0.555139.
Train: 2018-08-05T23:21:40.552375: step 411, loss 0.550625.
Train: 2018-08-05T23:21:40.724215: step 412, loss 0.514672.
Train: 2018-08-05T23:21:40.880423: step 413, loss 0.586537.
Train: 2018-08-05T23:21:41.036636: step 414, loss 0.518056.
Train: 2018-08-05T23:21:41.192849: step 415, loss 0.632406.
Train: 2018-08-05T23:21:41.349063: step 416, loss 0.552746.
Train: 2018-08-05T23:21:41.505277: step 417, loss 0.621609.
Train: 2018-08-05T23:21:41.661489: step 418, loss 0.671263.
Train: 2018-08-05T23:21:41.817702: step 419, loss 0.540795.
Train: 2018-08-05T23:21:41.989537: step 420, loss 0.502057.
Test: 2018-08-05T23:21:42.223888: step 420, loss 0.554686.
Train: 2018-08-05T23:21:42.380070: step 421, loss 0.590575.
Train: 2018-08-05T23:21:42.551939: step 422, loss 0.611915.
Train: 2018-08-05T23:21:42.708118: step 423, loss 0.567302.
Train: 2018-08-05T23:21:42.864331: step 424, loss 0.551436.
Train: 2018-08-05T23:21:43.020545: step 425, loss 0.598611.
Train: 2018-08-05T23:21:43.176757: step 426, loss 0.536022.
Train: 2018-08-05T23:21:43.332971: step 427, loss 0.554073.
Train: 2018-08-05T23:21:43.489215: step 428, loss 0.555012.
Train: 2018-08-05T23:21:43.629777: step 429, loss 0.561241.
Train: 2018-08-05T23:21:43.801644: step 430, loss 0.65417.
Test: 2018-08-05T23:21:44.035931: step 430, loss 0.555041.
Train: 2018-08-05T23:21:44.207790: step 431, loss 0.592591.
Train: 2018-08-05T23:21:44.363978: step 432, loss 0.562206.
Train: 2018-08-05T23:21:44.535843: step 433, loss 0.506788.
Train: 2018-08-05T23:21:44.692057: step 434, loss 0.583787.
Train: 2018-08-05T23:21:44.848271: step 435, loss 0.536149.
Train: 2018-08-05T23:21:45.020104: step 436, loss 0.654716.
Train: 2018-08-05T23:21:45.174462: step 437, loss 0.60596.
Train: 2018-08-05T23:21:45.330676: step 438, loss 0.486494.
Train: 2018-08-05T23:21:45.486892: step 439, loss 0.547971.
Train: 2018-08-05T23:21:45.658719: step 440, loss 0.628512.
Test: 2018-08-05T23:21:45.891380: step 440, loss 0.555296.
Train: 2018-08-05T23:21:46.047595: step 441, loss 0.560359.
Train: 2018-08-05T23:21:46.219433: step 442, loss 0.625782.
Train: 2018-08-05T23:21:46.391257: step 443, loss 0.585501.
Train: 2018-08-05T23:21:46.547446: step 444, loss 0.518148.
Train: 2018-08-05T23:21:46.703689: step 445, loss 0.551255.
Train: 2018-08-05T23:21:46.859872: step 446, loss 0.550737.
Train: 2018-08-05T23:21:47.031706: step 447, loss 0.587413.
Train: 2018-08-05T23:21:47.172299: step 448, loss 0.540811.
Train: 2018-08-05T23:21:47.328511: step 449, loss 0.577743.
Train: 2018-08-05T23:21:47.500347: step 450, loss 0.484416.
Test: 2018-08-05T23:21:47.734669: step 450, loss 0.554078.
Train: 2018-08-05T23:21:47.906502: step 451, loss 0.569968.
Train: 2018-08-05T23:21:48.062714: step 452, loss 0.577996.
Train: 2018-08-05T23:21:48.218927: step 453, loss 0.673696.
Train: 2018-08-05T23:21:48.375171: step 454, loss 0.513304.
Train: 2018-08-05T23:21:48.547011: step 455, loss 0.531156.
Train: 2018-08-05T23:21:48.703212: step 456, loss 0.583943.
Train: 2018-08-05T23:21:48.859428: step 457, loss 0.552794.
Train: 2018-08-05T23:21:48.999993: step 458, loss 0.656655.
Train: 2018-08-05T23:21:49.171828: step 459, loss 0.529474.
Train: 2018-08-05T23:21:49.328040: step 460, loss 0.616154.
Test: 2018-08-05T23:21:49.562392: step 460, loss 0.553792.
Train: 2018-08-05T23:21:49.734196: step 461, loss 0.560735.
Train: 2018-08-05T23:21:49.906061: step 462, loss 0.506744.
Train: 2018-08-05T23:21:50.046622: step 463, loss 0.544863.
Train: 2018-08-05T23:21:50.218456: step 464, loss 0.555702.
Train: 2018-08-05T23:21:50.390321: step 465, loss 0.651412.
Train: 2018-08-05T23:21:50.546535: step 466, loss 0.538395.
Train: 2018-08-05T23:21:50.718372: step 467, loss 0.541765.
Train: 2018-08-05T23:21:50.874582: step 468, loss 0.494532.
Train: 2018-08-05T23:21:51.030796: step 469, loss 0.547175.
Train: 2018-08-05T23:21:51.186979: step 470, loss 0.571004.
Test: 2018-08-05T23:21:51.421298: step 470, loss 0.55369.
Train: 2018-08-05T23:21:51.593179: step 471, loss 0.561676.
Train: 2018-08-05T23:21:51.749348: step 472, loss 0.545623.
Train: 2018-08-05T23:21:51.921181: step 473, loss 0.537363.
Train: 2018-08-05T23:21:52.065396: step 474, loss 0.570171.
Train: 2018-08-05T23:21:52.221610: step 475, loss 0.587958.
Train: 2018-08-05T23:21:52.377854: step 476, loss 0.558803.
Train: 2018-08-05T23:21:52.534065: step 477, loss 0.558972.
Train: 2018-08-05T23:21:52.699709: step 478, loss 0.581764.
Train: 2018-08-05T23:21:52.843408: step 479, loss 0.51985.
Train: 2018-08-05T23:21:52.999592: step 480, loss 0.468854.
Test: 2018-08-05T23:21:53.249534: step 480, loss 0.553201.
Train: 2018-08-05T23:21:53.405747: step 481, loss 0.510081.
Train: 2018-08-05T23:21:53.577611: step 482, loss 0.506015.
Train: 2018-08-05T23:21:53.749416: step 483, loss 0.543222.
Train: 2018-08-05T23:21:53.936871: step 484, loss 0.60699.
Train: 2018-08-05T23:21:54.093085: step 485, loss 0.593466.
Train: 2018-08-05T23:21:54.249324: step 486, loss 0.570719.
Train: 2018-08-05T23:21:54.421136: step 487, loss 0.55357.
Train: 2018-08-05T23:21:54.577376: step 488, loss 0.555796.
Train: 2018-08-05T23:21:54.733588: step 489, loss 0.676356.
Train: 2018-08-05T23:21:54.889772: step 490, loss 0.564939.
Test: 2018-08-05T23:21:55.124125: step 490, loss 0.553062.
Train: 2018-08-05T23:21:55.280337: step 491, loss 0.574428.
Train: 2018-08-05T23:21:55.436518: step 492, loss 0.583857.
Train: 2018-08-05T23:21:55.592756: step 493, loss 0.550861.
Train: 2018-08-05T23:21:55.764566: step 494, loss 0.55941.
Train: 2018-08-05T23:21:55.920810: step 495, loss 0.610728.
Train: 2018-08-05T23:21:56.077017: step 496, loss 0.523744.
Train: 2018-08-05T23:21:56.233236: step 497, loss 0.652116.
Train: 2018-08-05T23:21:56.389452: step 498, loss 0.56581.
Train: 2018-08-05T23:21:56.561254: step 499, loss 0.547022.
Train: 2018-08-05T23:21:56.717491: step 500, loss 0.54739.
Test: 2018-08-05T23:21:56.951817: step 500, loss 0.554921.
Train: 2018-08-05T23:21:57.670397: step 501, loss 0.537477.
Train: 2018-08-05T23:21:57.842204: step 502, loss 0.611699.
Train: 2018-08-05T23:21:57.998416: step 503, loss 0.614869.
Train: 2018-08-05T23:21:58.154630: step 504, loss 0.559884.
Train: 2018-08-05T23:21:58.326464: step 505, loss 0.578783.
Train: 2018-08-05T23:21:58.482677: step 506, loss 0.588321.
Train: 2018-08-05T23:21:58.654511: step 507, loss 0.59327.
Train: 2018-08-05T23:21:58.810725: step 508, loss 0.584384.
Train: 2018-08-05T23:21:58.966968: step 509, loss 0.617817.
Train: 2018-08-05T23:21:59.123181: step 510, loss 0.565972.
Test: 2018-08-05T23:21:59.357473: step 510, loss 0.55517.
Train: 2018-08-05T23:21:59.513709: step 511, loss 0.542025.
Train: 2018-08-05T23:21:59.669928: step 512, loss 0.553044.
Train: 2018-08-05T23:21:59.832661: step 513, loss 0.514466.
Train: 2018-08-05T23:21:59.988849: step 514, loss 0.576333.
Train: 2018-08-05T23:22:00.145062: step 515, loss 0.54649.
Train: 2018-08-05T23:22:00.301277: step 516, loss 0.484456.
Train: 2018-08-05T23:22:00.457490: step 517, loss 0.517822.
Train: 2018-08-05T23:22:00.613704: step 518, loss 0.589878.
Train: 2018-08-05T23:22:00.769946: step 519, loss 0.523492.
Train: 2018-08-05T23:22:00.926130: step 520, loss 0.548812.
Test: 2018-08-05T23:22:01.160484: step 520, loss 0.553151.
Train: 2018-08-05T23:22:01.316663: step 521, loss 0.548586.
Train: 2018-08-05T23:22:01.488498: step 522, loss 0.520744.
Train: 2018-08-05T23:22:01.644715: step 523, loss 0.611229.
Train: 2018-08-05T23:22:01.816570: step 524, loss 0.498774.
Train: 2018-08-05T23:22:01.972758: step 525, loss 0.598791.
Train: 2018-08-05T23:22:02.144593: step 526, loss 0.577753.
Train: 2018-08-05T23:22:02.300806: step 527, loss 0.475442.
Train: 2018-08-05T23:22:02.457020: step 528, loss 0.547626.
Train: 2018-08-05T23:22:02.613259: step 529, loss 0.617294.
Train: 2018-08-05T23:22:02.785101: step 530, loss 0.640076.
Test: 2018-08-05T23:22:03.019415: step 530, loss 0.552409.
Train: 2018-08-05T23:22:03.175630: step 531, loss 0.501295.
Train: 2018-08-05T23:22:03.347459: step 532, loss 0.552581.
Train: 2018-08-05T23:22:03.488061: step 533, loss 0.547474.
Train: 2018-08-05T23:22:03.659895: step 534, loss 0.548691.
Train: 2018-08-05T23:22:03.816105: step 535, loss 0.521881.
Train: 2018-08-05T23:22:03.972318: step 536, loss 0.650742.
Train: 2018-08-05T23:22:04.128539: step 537, loss 0.583617.
Train: 2018-08-05T23:22:04.300366: step 538, loss 0.520402.
Train: 2018-08-05T23:22:04.440927: step 539, loss 0.629221.
Train: 2018-08-05T23:22:04.612762: step 540, loss 0.600702.
Test: 2018-08-05T23:22:04.847082: step 540, loss 0.552228.
Train: 2018-08-05T23:22:05.003295: step 541, loss 0.543878.
Train: 2018-08-05T23:22:05.159508: step 542, loss 0.514501.
Train: 2018-08-05T23:22:05.315722: step 543, loss 0.515253.
Train: 2018-08-05T23:22:05.471934: step 544, loss 0.554646.
Train: 2018-08-05T23:22:05.628148: step 545, loss 0.620435.
Train: 2018-08-05T23:22:05.800009: step 546, loss 0.586649.
Train: 2018-08-05T23:22:05.940574: step 547, loss 0.625018.
Train: 2018-08-05T23:22:06.096788: step 548, loss 0.575148.
Train: 2018-08-05T23:22:06.253025: step 549, loss 0.603901.
Train: 2018-08-05T23:22:06.409248: step 550, loss 0.603737.
Test: 2018-08-05T23:22:06.643534: step 550, loss 0.55277.
Train: 2018-08-05T23:22:06.799747: step 551, loss 0.494322.
Train: 2018-08-05T23:22:06.955960: step 552, loss 0.619211.
Train: 2018-08-05T23:22:07.112174: step 553, loss 0.532182.
Train: 2018-08-05T23:22:07.268387: step 554, loss 0.583666.
Train: 2018-08-05T23:22:07.424600: step 555, loss 0.575955.
Train: 2018-08-05T23:22:07.580839: step 556, loss 0.617209.
Train: 2018-08-05T23:22:07.737058: step 557, loss 0.555552.
Train: 2018-08-05T23:22:07.893275: step 558, loss 0.585433.
Train: 2018-08-05T23:22:08.049484: step 559, loss 0.590777.
Train: 2018-08-05T23:22:08.221312: step 560, loss 0.609435.
Test: 2018-08-05T23:22:08.439986: step 560, loss 0.553796.
Train: 2018-08-05T23:22:08.596200: step 561, loss 0.586507.
Train: 2018-08-05T23:22:08.768064: step 562, loss 0.548351.
Train: 2018-08-05T23:22:08.924272: step 563, loss 0.512393.
Train: 2018-08-05T23:22:09.080491: step 564, loss 0.559279.
Train: 2018-08-05T23:22:09.252326: step 565, loss 0.598187.
Train: 2018-08-05T23:22:09.408508: step 566, loss 0.557536.
Train: 2018-08-05T23:22:09.564722: step 567, loss 0.599707.
Train: 2018-08-05T23:22:09.720966: step 568, loss 0.601949.
Train: 2018-08-05T23:22:09.877178: step 569, loss 0.565634.
Train: 2018-08-05T23:22:10.033362: step 570, loss 0.543795.
Test: 2018-08-05T23:22:10.267710: step 570, loss 0.553626.
Train: 2018-08-05T23:22:10.502001: step 571, loss 0.609124.
Train: 2018-08-05T23:22:10.658214: step 572, loss 0.542871.
Train: 2018-08-05T23:22:10.814428: step 573, loss 0.583964.
Train: 2018-08-05T23:22:10.986263: step 574, loss 0.551771.
Train: 2018-08-05T23:22:11.142475: step 575, loss 0.593556.
Train: 2018-08-05T23:22:11.298688: step 576, loss 0.510008.
Train: 2018-08-05T23:22:11.450289: step 577, loss 0.54131.
Train: 2018-08-05T23:22:11.606527: step 578, loss 0.573918.
Train: 2018-08-05T23:22:11.762715: step 579, loss 0.573949.
Train: 2018-08-05T23:22:11.918928: step 580, loss 0.508941.
Test: 2018-08-05T23:22:12.168905: step 580, loss 0.55242.
Train: 2018-08-05T23:22:12.325113: step 581, loss 0.547817.
Train: 2018-08-05T23:22:12.481327: step 582, loss 0.507172.
Train: 2018-08-05T23:22:12.637510: step 583, loss 0.583882.
Train: 2018-08-05T23:22:12.809346: step 584, loss 0.515082.
Train: 2018-08-05T23:22:12.965845: step 585, loss 0.5388.
Train: 2018-08-05T23:22:13.144337: step 586, loss 0.626727.
Train: 2018-08-05T23:22:13.290658: step 587, loss 0.565795.
Train: 2018-08-05T23:22:13.446866: step 588, loss 0.595879.
Train: 2018-08-05T23:22:13.603085: step 589, loss 0.640095.
Train: 2018-08-05T23:22:13.759298: step 590, loss 0.521933.
Test: 2018-08-05T23:22:14.009210: step 590, loss 0.551561.
Train: 2018-08-05T23:22:14.165423: step 591, loss 0.615611.
Train: 2018-08-05T23:22:14.321636: step 592, loss 0.601951.
Train: 2018-08-05T23:22:14.477879: step 593, loss 0.627469.
Train: 2018-08-05T23:22:14.634061: step 594, loss 0.600549.
Train: 2018-08-05T23:22:14.805897: step 595, loss 0.558106.
Train: 2018-08-05T23:22:14.962135: step 596, loss 0.581217.
Train: 2018-08-05T23:22:15.165189: step 597, loss 0.553481.
Train: 2018-08-05T23:22:15.368265: step 598, loss 0.6679.
Train: 2018-08-05T23:22:15.571342: step 599, loss 0.605195.
Train: 2018-08-05T23:22:15.836907: step 600, loss 0.60051.
Test: 2018-08-05T23:22:16.086846: step 600, loss 0.552754.
Train: 2018-08-05T23:22:16.805446: step 601, loss 0.507254.
Train: 2018-08-05T23:22:16.964923: step 602, loss 0.591233.
Train: 2018-08-05T23:22:17.121142: step 603, loss 0.548184.
Train: 2018-08-05T23:22:17.261728: step 604, loss 0.599125.
Train: 2018-08-05T23:22:17.417941: step 605, loss 0.633622.
Train: 2018-08-05T23:22:17.574154: step 606, loss 0.591253.
Train: 2018-08-05T23:22:17.730338: step 607, loss 0.580998.
Train: 2018-08-05T23:22:17.886580: step 608, loss 0.54211.
Train: 2018-08-05T23:22:18.042763: step 609, loss 0.613815.
Train: 2018-08-05T23:22:18.198977: step 610, loss 0.527329.
Test: 2018-08-05T23:22:18.433297: step 610, loss 0.554156.
Train: 2018-08-05T23:22:18.605157: step 611, loss 0.597823.
Train: 2018-08-05T23:22:18.761375: step 612, loss 0.612491.
Train: 2018-08-05T23:22:18.917589: step 613, loss 0.603331.
Train: 2018-08-05T23:22:19.073801: step 614, loss 0.59849.
Train: 2018-08-05T23:22:19.230015: step 615, loss 0.565962.
Train: 2018-08-05T23:22:19.386197: step 616, loss 0.574771.
Train: 2018-08-05T23:22:19.558062: step 617, loss 0.611564.
Train: 2018-08-05T23:22:19.714246: step 618, loss 0.615596.
Train: 2018-08-05T23:22:19.870459: step 619, loss 0.545352.
Train: 2018-08-05T23:22:20.026698: step 620, loss 0.498091.
Test: 2018-08-05T23:22:20.260994: step 620, loss 0.554206.
Train: 2018-08-05T23:22:20.417230: step 621, loss 0.515122.
Train: 2018-08-05T23:22:20.573443: step 622, loss 0.582718.
Train: 2018-08-05T23:22:20.729657: step 623, loss 0.552364.
Train: 2018-08-05T23:22:20.885875: step 624, loss 0.622009.
Train: 2018-08-05T23:22:21.042084: step 625, loss 0.57413.
Train: 2018-08-05T23:22:21.198296: step 626, loss 0.525809.
Train: 2018-08-05T23:22:21.354515: step 627, loss 0.566461.
Train: 2018-08-05T23:22:21.510700: step 628, loss 0.540175.
Train: 2018-08-05T23:22:21.666911: step 629, loss 0.522604.
Train: 2018-08-05T23:22:21.823149: step 630, loss 0.550023.
Test: 2018-08-05T23:22:22.073066: step 630, loss 0.551862.
Train: 2018-08-05T23:22:22.229283: step 631, loss 0.547756.
Train: 2018-08-05T23:22:22.385493: step 632, loss 0.587439.
Train: 2018-08-05T23:22:22.572949: step 633, loss 0.568611.
Train: 2018-08-05T23:22:22.729161: step 634, loss 0.544072.
Train: 2018-08-05T23:22:22.885410: step 635, loss 0.553362.
Train: 2018-08-05T23:22:23.057234: step 636, loss 0.573087.
Train: 2018-08-05T23:22:23.216090: step 637, loss 0.448971.
Train: 2018-08-05T23:22:23.372298: step 638, loss 0.452895.
Train: 2018-08-05T23:22:23.528517: step 639, loss 0.437206.
Train: 2018-08-05T23:22:23.684724: step 640, loss 0.513841.
Test: 2018-08-05T23:22:23.919020: step 640, loss 0.551543.
Train: 2018-08-05T23:22:24.078831: step 641, loss 0.670857.
Train: 2018-08-05T23:22:24.250633: step 642, loss 0.591021.
Train: 2018-08-05T23:22:24.406877: step 643, loss 0.550617.
Train: 2018-08-05T23:22:24.563090: step 644, loss 0.55969.
Train: 2018-08-05T23:22:24.719304: step 645, loss 0.632721.
Train: 2018-08-05T23:22:24.875487: step 646, loss 0.510107.
Train: 2018-08-05T23:22:25.031734: step 647, loss 0.537603.
Train: 2018-08-05T23:22:25.203565: step 648, loss 0.559254.
Train: 2018-08-05T23:22:25.344157: step 649, loss 0.535519.
Train: 2018-08-05T23:22:25.500370: step 650, loss 0.686109.
Test: 2018-08-05T23:22:25.750286: step 650, loss 0.551051.
Train: 2018-08-05T23:22:25.906519: step 651, loss 0.540055.
Train: 2018-08-05T23:22:26.062736: step 652, loss 0.543008.
Train: 2018-08-05T23:22:26.203301: step 653, loss 0.611581.
Train: 2018-08-05T23:22:26.359547: step 654, loss 0.611317.
Train: 2018-08-05T23:22:26.515750: step 655, loss 0.577559.
Train: 2018-08-05T23:22:26.671940: step 656, loss 0.600159.
Train: 2018-08-05T23:22:26.828183: step 657, loss 0.525426.
Train: 2018-08-05T23:22:26.984396: step 658, loss 0.505863.
Train: 2018-08-05T23:22:27.140579: step 659, loss 0.616935.
Train: 2018-08-05T23:22:27.296825: step 660, loss 0.541023.
Test: 2018-08-05T23:22:27.546748: step 660, loss 0.551515.
Train: 2018-08-05T23:22:27.687326: step 661, loss 0.624909.
Train: 2018-08-05T23:22:27.859190: step 662, loss 0.515852.
Train: 2018-08-05T23:22:28.015403: step 663, loss 0.539341.
Train: 2018-08-05T23:22:28.171587: step 664, loss 0.492981.
Train: 2018-08-05T23:22:28.343422: step 665, loss 0.600863.
Train: 2018-08-05T23:22:28.499634: step 666, loss 0.480486.
Train: 2018-08-05T23:22:28.655847: step 667, loss 0.574466.
Train: 2018-08-05T23:22:28.812095: step 668, loss 0.556278.
Train: 2018-08-05T23:22:28.968298: step 669, loss 0.532612.
Train: 2018-08-05T23:22:29.124519: step 670, loss 0.70802.
Test: 2018-08-05T23:22:29.374428: step 670, loss 0.551489.
Train: 2018-08-05T23:22:29.530641: step 671, loss 0.612763.
Train: 2018-08-05T23:22:29.686854: step 672, loss 0.591099.
Train: 2018-08-05T23:22:29.843068: step 673, loss 0.549198.
Train: 2018-08-05T23:22:29.999282: step 674, loss 0.549803.
Train: 2018-08-05T23:22:30.171117: step 675, loss 0.598004.
Train: 2018-08-05T23:22:30.327354: step 676, loss 0.516447.
Train: 2018-08-05T23:22:30.483543: step 677, loss 0.550383.
Train: 2018-08-05T23:22:30.639787: step 678, loss 0.632521.
Train: 2018-08-05T23:22:30.795970: step 679, loss 0.540586.
Train: 2018-08-05T23:22:30.954558: step 680, loss 0.564311.
Test: 2018-08-05T23:22:31.188872: step 680, loss 0.551758.
Train: 2018-08-05T23:22:31.360704: step 681, loss 0.532593.
Train: 2018-08-05T23:22:31.516892: step 682, loss 0.565736.
Train: 2018-08-05T23:22:31.673106: step 683, loss 0.505058.
Train: 2018-08-05T23:22:31.844974: step 684, loss 0.515846.
Train: 2018-08-05T23:22:32.001191: step 685, loss 0.52964.
Train: 2018-08-05T23:22:32.157393: step 686, loss 0.538498.
Train: 2018-08-05T23:22:32.313604: step 687, loss 0.622007.
Train: 2018-08-05T23:22:32.469817: step 688, loss 0.557581.
Train: 2018-08-05T23:22:32.626036: step 689, loss 0.566471.
Train: 2018-08-05T23:22:32.782249: step 690, loss 0.620661.
Test: 2018-08-05T23:22:33.016570: step 690, loss 0.550945.
Train: 2018-08-05T23:22:33.172782: step 691, loss 0.488191.
Train: 2018-08-05T23:22:33.328999: step 692, loss 0.521224.
Train: 2018-08-05T23:22:33.485179: step 693, loss 0.55716.
Train: 2018-08-05T23:22:33.641392: step 694, loss 0.580341.
Train: 2018-08-05T23:22:33.797605: step 695, loss 0.56112.
Train: 2018-08-05T23:22:33.953818: step 696, loss 0.538122.
Train: 2018-08-05T23:22:34.125653: step 697, loss 0.601685.
Train: 2018-08-05T23:22:34.266246: step 698, loss 0.537252.
Train: 2018-08-05T23:22:34.422460: step 699, loss 0.573955.
Train: 2018-08-05T23:22:34.594323: step 700, loss 0.556265.
Test: 2018-08-05T23:22:34.828614: step 700, loss 0.550581.
Train: 2018-08-05T23:22:35.547224: step 701, loss 0.611984.
Train: 2018-08-05T23:22:35.703431: step 702, loss 0.556799.
Train: 2018-08-05T23:22:35.859621: step 703, loss 0.608019.
Train: 2018-08-05T23:22:36.015834: step 704, loss 0.529203.
Train: 2018-08-05T23:22:36.156456: step 705, loss 0.555762.
Train: 2018-08-05T23:22:36.312671: step 706, loss 0.531624.
Train: 2018-08-05T23:22:36.468882: step 707, loss 0.557774.
Train: 2018-08-05T23:22:36.625096: step 708, loss 0.478976.
Train: 2018-08-05T23:22:36.781280: step 709, loss 0.60326.
Train: 2018-08-05T23:22:36.953113: step 710, loss 0.611755.
Test: 2018-08-05T23:22:37.187433: step 710, loss 0.550573.
Train: 2018-08-05T23:22:37.343645: step 711, loss 0.48679.
Train: 2018-08-05T23:22:37.499893: step 712, loss 0.549916.
Train: 2018-08-05T23:22:37.656103: step 713, loss 0.548341.
Train: 2018-08-05T23:22:37.812286: step 714, loss 0.655383.
Train: 2018-08-05T23:22:37.968523: step 715, loss 0.575022.
Train: 2018-08-05T23:22:38.124743: step 716, loss 0.591666.
Train: 2018-08-05T23:22:38.280926: step 717, loss 0.540877.
Train: 2018-08-05T23:22:38.437138: step 718, loss 0.637039.
Train: 2018-08-05T23:22:38.577731: step 719, loss 0.599791.
Train: 2018-08-05T23:22:38.733969: step 720, loss 0.524049.
Test: 2018-08-05T23:22:38.968295: step 720, loss 0.550835.
Train: 2018-08-05T23:22:39.124508: step 721, loss 0.522064.
Train: 2018-08-05T23:22:39.280721: step 722, loss 0.580521.
Train: 2018-08-05T23:22:39.436937: step 723, loss 0.614614.
Train: 2018-08-05T23:22:39.593147: step 724, loss 0.522598.
Train: 2018-08-05T23:22:39.749355: step 725, loss 0.524399.
Train: 2018-08-05T23:22:39.905574: step 726, loss 0.600493.
Train: 2018-08-05T23:22:40.061787: step 727, loss 0.62672.
Train: 2018-08-05T23:22:40.217970: step 728, loss 0.515345.
Train: 2018-08-05T23:22:40.374213: step 729, loss 0.625185.
Train: 2018-08-05T23:22:40.530432: step 730, loss 0.523486.
Test: 2018-08-05T23:22:40.764748: step 730, loss 0.551101.
Train: 2018-08-05T23:22:40.920930: step 731, loss 0.591403.
Train: 2018-08-05T23:22:41.077176: step 732, loss 0.642096.
Train: 2018-08-05T23:22:41.217765: step 733, loss 0.557164.
Train: 2018-08-05T23:22:41.373981: step 734, loss 0.579477.
Train: 2018-08-05T23:22:41.530191: step 735, loss 0.549453.
Train: 2018-08-05T23:22:41.686408: step 736, loss 0.565017.
Train: 2018-08-05T23:22:41.842622: step 737, loss 0.623556.
Train: 2018-08-05T23:22:41.998832: step 738, loss 0.631052.
Train: 2018-08-05T23:22:42.155044: step 739, loss 0.573616.
Train: 2018-08-05T23:22:42.311257: step 740, loss 0.583899.
Test: 2018-08-05T23:22:42.545581: step 740, loss 0.551877.
Train: 2018-08-05T23:22:42.717412: step 741, loss 0.556362.
Train: 2018-08-05T23:22:42.873594: step 742, loss 0.533318.
Train: 2018-08-05T23:22:43.029808: step 743, loss 0.564694.
Train: 2018-08-05T23:22:43.186021: step 744, loss 0.541515.
Train: 2018-08-05T23:22:43.342234: step 745, loss 0.557686.
Train: 2018-08-05T23:22:43.498447: step 746, loss 0.588856.
Train: 2018-08-05T23:22:43.654661: step 747, loss 0.597922.
Train: 2018-08-05T23:22:43.810875: step 748, loss 0.580819.
Train: 2018-08-05T23:22:43.967119: step 749, loss 0.572502.
Train: 2018-08-05T23:22:44.123326: step 750, loss 0.460048.
Test: 2018-08-05T23:22:44.357653: step 750, loss 0.551727.
Train: 2018-08-05T23:22:44.513865: step 751, loss 0.603851.
Train: 2018-08-05T23:22:44.670077: step 752, loss 0.55684.
Train: 2018-08-05T23:22:44.826294: step 753, loss 0.599927.
Train: 2018-08-05T23:22:44.982504: step 754, loss 0.556104.
Train: 2018-08-05T23:22:45.138718: step 755, loss 0.492728.
Train: 2018-08-05T23:22:45.294931: step 756, loss 0.554437.
Train: 2018-08-05T23:22:45.451114: step 757, loss 0.50668.
Train: 2018-08-05T23:22:45.607326: step 758, loss 0.609208.
Train: 2018-08-05T23:22:45.763566: step 759, loss 0.505783.
Train: 2018-08-05T23:22:45.919784: step 760, loss 0.625074.
Test: 2018-08-05T23:22:46.169725: step 760, loss 0.550556.
Train: 2018-08-05T23:22:46.325932: step 761, loss 0.589319.
Train: 2018-08-05T23:22:46.482155: step 762, loss 0.597423.
Train: 2018-08-05T23:22:46.638365: step 763, loss 0.57502.
Train: 2018-08-05T23:22:46.794586: step 764, loss 0.557144.
Train: 2018-08-05T23:22:46.950762: step 765, loss 0.573058.
Train: 2018-08-05T23:22:47.106998: step 766, loss 0.565405.
Train: 2018-08-05T23:22:47.263217: step 767, loss 0.530142.
Train: 2018-08-05T23:22:47.419431: step 768, loss 0.654645.
Train: 2018-08-05T23:22:47.575648: step 769, loss 0.513106.
Train: 2018-08-05T23:22:47.731857: step 770, loss 0.513664.
Test: 2018-08-05T23:22:47.966180: step 770, loss 0.550383.
Train: 2018-08-05T23:22:48.130204: step 771, loss 0.503576.
Train: 2018-08-05T23:22:48.286413: step 772, loss 0.547473.
Train: 2018-08-05T23:22:48.442634: step 773, loss 0.61823.
Train: 2018-08-05T23:22:48.598814: step 774, loss 0.518623.
Train: 2018-08-05T23:22:48.749961: step 775, loss 0.573517.
Train: 2018-08-05T23:22:48.906207: step 776, loss 0.536862.
Train: 2018-08-05T23:22:49.062417: step 777, loss 0.591225.
Train: 2018-08-05T23:22:49.218631: step 778, loss 0.579826.
Train: 2018-08-05T23:22:49.374844: step 779, loss 0.619281.
Train: 2018-08-05T23:22:49.531059: step 780, loss 0.512747.
Test: 2018-08-05T23:22:49.765374: step 780, loss 0.550117.
Train: 2018-08-05T23:22:49.921584: step 781, loss 0.583752.
Train: 2018-08-05T23:22:50.077803: step 782, loss 0.55508.
Train: 2018-08-05T23:22:50.233986: step 783, loss 0.556484.
Train: 2018-08-05T23:22:50.390230: step 784, loss 0.482827.
Train: 2018-08-05T23:22:50.546412: step 785, loss 0.63409.
Train: 2018-08-05T23:22:50.702657: step 786, loss 0.609654.
Train: 2018-08-05T23:22:50.858869: step 787, loss 0.452231.
Train: 2018-08-05T23:22:51.015053: step 788, loss 0.626044.
Train: 2018-08-05T23:22:51.171296: step 789, loss 0.55528.
Train: 2018-08-05T23:22:51.327510: step 790, loss 0.553686.
Test: 2018-08-05T23:22:51.577438: step 790, loss 0.550047.
Train: 2018-08-05T23:22:51.733636: step 791, loss 0.575094.
Train: 2018-08-05T23:22:51.889879: step 792, loss 0.488226.
Train: 2018-08-05T23:22:52.046090: step 793, loss 0.546207.
Train: 2018-08-05T23:22:52.202304: step 794, loss 0.510362.
Train: 2018-08-05T23:22:52.358510: step 795, loss 0.571793.
Train: 2018-08-05T23:22:52.514724: step 796, loss 0.501338.
Train: 2018-08-05T23:22:52.655324: step 797, loss 0.610574.
Train: 2018-08-05T23:22:52.827159: step 798, loss 0.596501.
Train: 2018-08-05T23:22:52.983340: step 799, loss 0.618347.
Train: 2018-08-05T23:22:53.155174: step 800, loss 0.600998.
Test: 2018-08-05T23:22:53.389495: step 800, loss 0.549929.
Train: 2018-08-05T23:22:54.139350: step 801, loss 0.55079.
Train: 2018-08-05T23:22:54.302260: step 802, loss 0.619445.
Train: 2018-08-05T23:22:54.444088: step 803, loss 0.539454.
Train: 2018-08-05T23:22:54.600307: step 804, loss 0.558079.
Train: 2018-08-05T23:22:54.756517: step 805, loss 0.605865.
Train: 2018-08-05T23:22:54.912735: step 806, loss 0.521922.
Train: 2018-08-05T23:22:55.068948: step 807, loss 0.502142.
Train: 2018-08-05T23:22:55.225155: step 808, loss 0.529052.
Train: 2018-08-05T23:22:55.388027: step 809, loss 0.486454.
Train: 2018-08-05T23:22:55.531763: step 810, loss 0.540665.
Test: 2018-08-05T23:22:55.781674: step 810, loss 0.550137.
Train: 2018-08-05T23:22:55.937887: step 811, loss 0.587649.
Train: 2018-08-05T23:22:56.094131: step 812, loss 0.668927.
Train: 2018-08-05T23:22:56.250314: step 813, loss 0.565993.
Train: 2018-08-05T23:22:56.406557: step 814, loss 0.539484.
Train: 2018-08-05T23:22:56.562741: step 815, loss 0.575093.
Train: 2018-08-05T23:22:56.718985: step 816, loss 0.507367.
Train: 2018-08-05T23:22:56.875202: step 817, loss 0.65242.
Train: 2018-08-05T23:22:57.031410: step 818, loss 0.554714.
Train: 2018-08-05T23:22:57.172002: step 819, loss 0.598145.
Train: 2018-08-05T23:22:57.328217: step 820, loss 0.565791.
Test: 2018-08-05T23:22:57.562539: step 820, loss 0.550309.
Train: 2018-08-05T23:22:57.734339: step 821, loss 0.530952.
Train: 2018-08-05T23:22:57.890583: step 822, loss 0.606989.
Train: 2018-08-05T23:22:58.031169: step 823, loss 0.562842.
Train: 2018-08-05T23:22:58.202979: step 824, loss 0.513608.
Train: 2018-08-05T23:22:58.343571: step 825, loss 0.556895.
Train: 2018-08-05T23:22:58.515405: step 826, loss 0.572397.
Train: 2018-08-05T23:22:58.656029: step 827, loss 0.533079.
Train: 2018-08-05T23:22:58.812212: step 828, loss 0.532318.
Train: 2018-08-05T23:22:58.968454: step 829, loss 0.556719.
Train: 2018-08-05T23:22:59.124668: step 830, loss 0.537757.
Test: 2018-08-05T23:22:59.358991: step 830, loss 0.550218.
Train: 2018-08-05T23:22:59.530822: step 831, loss 0.540888.
Train: 2018-08-05T23:22:59.687005: step 832, loss 0.54503.
Train: 2018-08-05T23:22:59.843218: step 833, loss 0.626542.
Train: 2018-08-05T23:22:59.999432: step 834, loss 0.555082.
Train: 2018-08-05T23:23:00.155644: step 835, loss 0.565637.
Train: 2018-08-05T23:23:00.311858: step 836, loss 0.653282.
Train: 2018-08-05T23:23:00.452489: step 837, loss 0.555387.
Train: 2018-08-05T23:23:00.608663: step 838, loss 0.566023.
Train: 2018-08-05T23:23:00.764878: step 839, loss 0.50545.
Train: 2018-08-05T23:23:00.921121: step 840, loss 0.546563.
Test: 2018-08-05T23:23:01.155443: step 840, loss 0.550092.
Train: 2018-08-05T23:23:01.327244: step 841, loss 0.555144.
Train: 2018-08-05T23:23:01.483488: step 842, loss 0.514189.
Train: 2018-08-05T23:23:01.639701: step 843, loss 0.53999.
Train: 2018-08-05T23:23:01.795908: step 844, loss 0.62512.
Train: 2018-08-05T23:23:01.936509: step 845, loss 0.522165.
Train: 2018-08-05T23:23:02.101346: step 846, loss 0.625534.
Train: 2018-08-05T23:23:02.257565: step 847, loss 0.563792.
Train: 2018-08-05T23:23:02.413778: step 848, loss 0.531035.
Train: 2018-08-05T23:23:02.569962: step 849, loss 0.582677.
Train: 2018-08-05T23:23:02.726174: step 850, loss 0.601137.
Test: 2018-08-05T23:23:02.960525: step 850, loss 0.54999.
Train: 2018-08-05T23:23:03.132328: step 851, loss 0.545959.
Train: 2018-08-05T23:23:03.288542: step 852, loss 0.558533.
Train: 2018-08-05T23:23:03.444755: step 853, loss 0.658105.
Train: 2018-08-05T23:23:03.600968: step 854, loss 0.598402.
Train: 2018-08-05T23:23:03.757183: step 855, loss 0.547869.
Train: 2018-08-05T23:23:03.913425: step 856, loss 0.546482.
Train: 2018-08-05T23:23:04.069610: step 857, loss 0.53013.
Train: 2018-08-05T23:23:04.225852: step 858, loss 0.548024.
Train: 2018-08-05T23:23:04.397656: step 859, loss 0.519802.
Train: 2018-08-05T23:23:04.553869: step 860, loss 0.549408.
Test: 2018-08-05T23:23:04.788223: step 860, loss 0.550198.
Train: 2018-08-05T23:23:04.944433: step 861, loss 0.581736.
Train: 2018-08-05T23:23:05.100647: step 862, loss 0.55565.
Train: 2018-08-05T23:23:05.256831: step 863, loss 0.540011.
Train: 2018-08-05T23:23:05.428664: step 864, loss 0.539147.
Train: 2018-08-05T23:23:05.584877: step 865, loss 0.596535.
Train: 2018-08-05T23:23:05.741090: step 866, loss 0.478821.
Train: 2018-08-05T23:23:05.881681: step 867, loss 0.540122.
Train: 2018-08-05T23:23:06.053546: step 868, loss 0.625346.
Train: 2018-08-05T23:23:06.209730: step 869, loss 0.589949.
Train: 2018-08-05T23:23:06.365943: step 870, loss 0.641796.
Test: 2018-08-05T23:23:06.600264: step 870, loss 0.549875.
Train: 2018-08-05T23:23:06.756507: step 871, loss 0.562716.
Train: 2018-08-05T23:23:06.912689: step 872, loss 0.553479.
Train: 2018-08-05T23:23:07.068903: step 873, loss 0.591417.
Train: 2018-08-05T23:23:07.225115: step 874, loss 0.555714.
Train: 2018-08-05T23:23:07.381355: step 875, loss 0.530594.
Train: 2018-08-05T23:23:07.537542: step 876, loss 0.565101.
Train: 2018-08-05T23:23:07.693780: step 877, loss 0.643063.
Train: 2018-08-05T23:23:07.834347: step 878, loss 0.614706.
Train: 2018-08-05T23:23:07.990592: step 879, loss 0.540127.
Train: 2018-08-05T23:23:08.146804: step 880, loss 0.58054.
Test: 2018-08-05T23:23:08.381094: step 880, loss 0.550174.
Train: 2018-08-05T23:23:08.552929: step 881, loss 0.590229.
Train: 2018-08-05T23:23:08.709142: step 882, loss 0.565693.
Train: 2018-08-05T23:23:08.849764: step 883, loss 0.547812.
Train: 2018-08-05T23:23:09.005948: step 884, loss 0.549724.
Train: 2018-08-05T23:23:09.162160: step 885, loss 0.616523.
Train: 2018-08-05T23:23:09.318373: step 886, loss 0.481739.
Train: 2018-08-05T23:23:09.474586: step 887, loss 0.606608.
Train: 2018-08-05T23:23:09.630800: step 888, loss 0.52101.
Train: 2018-08-05T23:23:09.787013: step 889, loss 0.522681.
Train: 2018-08-05T23:23:09.943227: step 890, loss 0.473404.
Test: 2018-08-05T23:23:10.177577: step 890, loss 0.550252.
Train: 2018-08-05T23:23:10.349412: step 891, loss 0.572753.
Train: 2018-08-05T23:23:10.505625: step 892, loss 0.66462.
Train: 2018-08-05T23:23:10.661808: step 893, loss 0.57171.
Train: 2018-08-05T23:23:10.833672: step 894, loss 0.555505.
Train: 2018-08-05T23:23:10.989885: step 895, loss 0.546594.
Train: 2018-08-05T23:23:11.146096: step 896, loss 0.613662.
Train: 2018-08-05T23:23:11.302312: step 897, loss 0.600889.
Train: 2018-08-05T23:23:11.458526: step 898, loss 0.538496.
Train: 2018-08-05T23:23:11.614740: step 899, loss 0.608216.
Train: 2018-08-05T23:23:11.770952: step 900, loss 0.649841.
Test: 2018-08-05T23:23:12.005242: step 900, loss 0.550162.
Train: 2018-08-05T23:23:12.755096: step 901, loss 0.530717.
Train: 2018-08-05T23:23:12.926900: step 902, loss 0.539728.
Train: 2018-08-05T23:23:13.083143: step 903, loss 0.588666.
Train: 2018-08-05T23:23:13.239357: step 904, loss 0.57282.
Train: 2018-08-05T23:23:13.395569: step 905, loss 0.582533.
Train: 2018-08-05T23:23:13.551783: step 906, loss 0.421125.
Train: 2018-08-05T23:23:13.707990: step 907, loss 0.547401.
Train: 2018-08-05T23:23:13.864230: step 908, loss 0.571085.
Train: 2018-08-05T23:23:14.020425: step 909, loss 0.6058.
Train: 2018-08-05T23:23:14.176606: step 910, loss 0.588531.
Test: 2018-08-05T23:23:14.410956: step 910, loss 0.550037.
Train: 2018-08-05T23:23:14.567164: step 911, loss 0.505685.
Train: 2018-08-05T23:23:14.723353: step 912, loss 0.562812.
Train: 2018-08-05T23:23:14.879596: step 913, loss 0.547278.
Train: 2018-08-05T23:23:15.051431: step 914, loss 0.659507.
Train: 2018-08-05T23:23:15.207613: step 915, loss 0.571271.
Train: 2018-08-05T23:23:15.363851: step 916, loss 0.538827.
Train: 2018-08-05T23:23:15.520065: step 917, loss 0.505762.
Train: 2018-08-05T23:23:15.676278: step 918, loss 0.584927.
Train: 2018-08-05T23:23:15.848092: step 919, loss 0.538183.
Train: 2018-08-05T23:23:16.019924: step 920, loss 0.504543.
Test: 2018-08-05T23:23:16.269864: step 920, loss 0.549718.
Train: 2018-08-05T23:23:16.441699: step 921, loss 0.563629.
Train: 2018-08-05T23:23:16.582290: step 922, loss 0.607952.
Train: 2018-08-05T23:23:16.754125: step 923, loss 0.502895.
Train: 2018-08-05T23:23:16.894716: step 924, loss 0.581966.
Train: 2018-08-05T23:23:17.050930: step 925, loss 0.513253.
Train: 2018-08-05T23:23:17.222779: step 926, loss 0.609078.
Train: 2018-08-05T23:23:17.379002: step 927, loss 0.590876.
Train: 2018-08-05T23:23:17.535216: step 928, loss 0.538122.
Train: 2018-08-05T23:23:17.691404: step 929, loss 0.537541.
Train: 2018-08-05T23:23:17.847648: step 930, loss 0.501875.
Test: 2018-08-05T23:23:18.081939: step 930, loss 0.549466.
Train: 2018-08-05T23:23:18.253771: step 931, loss 0.591008.
Train: 2018-08-05T23:23:18.394363: step 932, loss 0.609742.
Train: 2018-08-05T23:23:18.550607: step 933, loss 0.49275.
Train: 2018-08-05T23:23:18.722442: step 934, loss 0.555595.
Train: 2018-08-05T23:23:18.878656: step 935, loss 0.546491.
Train: 2018-08-05T23:23:19.034862: step 936, loss 0.519243.
Train: 2018-08-05T23:23:19.216804: step 937, loss 0.537659.
Train: 2018-08-05T23:23:19.372660: step 938, loss 0.518752.
Train: 2018-08-05T23:23:19.528844: step 939, loss 0.654432.
Train: 2018-08-05T23:23:19.685056: step 940, loss 0.608545.
Test: 2018-08-05T23:23:19.935029: step 940, loss 0.549295.
Train: 2018-08-05T23:23:20.091211: step 941, loss 0.555105.
Train: 2018-08-05T23:23:20.263058: step 942, loss 0.538185.
Train: 2018-08-05T23:23:20.419285: step 943, loss 0.671241.
Train: 2018-08-05T23:23:20.589216: step 944, loss 0.581869.
Train: 2018-08-05T23:23:20.776660: step 945, loss 0.562181.
Train: 2018-08-05T23:23:20.932846: step 946, loss 0.545729.
Train: 2018-08-05T23:23:21.104682: step 947, loss 0.591272.
Train: 2018-08-05T23:23:21.276546: step 948, loss 0.669058.
Train: 2018-08-05T23:23:21.448380: step 949, loss 0.54665.
Train: 2018-08-05T23:23:21.604563: step 950, loss 0.615338.
Test: 2018-08-05T23:23:21.854505: step 950, loss 0.549671.
Train: 2018-08-05T23:23:22.010747: step 951, loss 0.496315.
Train: 2018-08-05T23:23:22.166966: step 952, loss 0.512936.
Train: 2018-08-05T23:23:22.323144: step 953, loss 0.538493.
Train: 2018-08-05T23:23:22.479389: step 954, loss 0.589634.
Train: 2018-08-05T23:23:22.651221: step 955, loss 0.546505.
Train: 2018-08-05T23:23:22.807430: step 956, loss 0.590341.
Train: 2018-08-05T23:23:22.963648: step 957, loss 0.521994.
Train: 2018-08-05T23:23:23.119863: step 958, loss 0.606586.
Train: 2018-08-05T23:23:23.276076: step 959, loss 0.572949.
Train: 2018-08-05T23:23:23.432260: step 960, loss 0.495852.
Test: 2018-08-05T23:23:23.682199: step 960, loss 0.549864.
Train: 2018-08-05T23:23:23.854034: step 961, loss 0.590152.
Train: 2018-08-05T23:23:24.025899: step 962, loss 0.495706.
Train: 2018-08-05T23:23:24.197703: step 963, loss 0.606642.
Train: 2018-08-05T23:23:24.353949: step 964, loss 0.520087.
Train: 2018-08-05T23:23:24.525775: step 965, loss 0.615105.
Train: 2018-08-05T23:23:24.681994: step 966, loss 0.64114.
Train: 2018-08-05T23:23:24.822586: step 967, loss 0.556266.
Train: 2018-08-05T23:23:24.994414: step 968, loss 0.597144.
Train: 2018-08-05T23:23:25.150603: step 969, loss 0.503916.
Train: 2018-08-05T23:23:25.306816: step 970, loss 0.580604.
Test: 2018-08-05T23:23:25.545928: step 970, loss 0.549814.
Train: 2018-08-05T23:23:25.702137: step 971, loss 0.581031.
Train: 2018-08-05T23:23:25.874001: step 972, loss 0.513816.
Train: 2018-08-05T23:23:26.045805: step 973, loss 0.530195.
Train: 2018-08-05T23:23:26.202020: step 974, loss 0.589603.
Train: 2018-08-05T23:23:26.358232: step 975, loss 0.53794.
Train: 2018-08-05T23:23:26.514475: step 976, loss 0.564804.
Train: 2018-08-05T23:23:26.670659: step 977, loss 0.547405.
Train: 2018-08-05T23:23:26.842066: step 978, loss 0.571383.
Train: 2018-08-05T23:23:26.998309: step 979, loss 0.513363.
Train: 2018-08-05T23:23:27.154491: step 980, loss 0.590376.
Test: 2018-08-05T23:23:27.388842: step 980, loss 0.549533.
Train: 2018-08-05T23:23:27.560649: step 981, loss 0.571718.
Train: 2018-08-05T23:23:27.716858: step 982, loss 0.502683.
Train: 2018-08-05T23:23:27.873103: step 983, loss 0.546866.
Train: 2018-08-05T23:23:28.029286: step 984, loss 0.50357.
Train: 2018-08-05T23:23:28.201120: step 985, loss 0.643859.
Train: 2018-08-05T23:23:28.357333: step 986, loss 0.626051.
Train: 2018-08-05T23:23:28.513546: step 987, loss 0.593354.
Train: 2018-08-05T23:23:28.685410: step 988, loss 0.529858.
Train: 2018-08-05T23:23:28.857216: step 989, loss 0.504266.
Train: 2018-08-05T23:23:28.997809: step 990, loss 0.650335.
Test: 2018-08-05T23:23:29.247749: step 990, loss 0.549377.
Train: 2018-08-05T23:23:29.403963: step 991, loss 0.599396.
Train: 2018-08-05T23:23:29.560205: step 992, loss 0.563436.
Train: 2018-08-05T23:23:29.716420: step 993, loss 0.547004.
Train: 2018-08-05T23:23:29.872632: step 994, loss 0.573213.
Train: 2018-08-05T23:23:30.028821: step 995, loss 0.538443.
Train: 2018-08-05T23:23:30.200684: step 996, loss 0.54492.
Train: 2018-08-05T23:23:30.356862: step 997, loss 0.478018.
Train: 2018-08-05T23:23:30.513077: step 998, loss 0.52868.
Train: 2018-08-05T23:23:30.669289: step 999, loss 0.544981.
Train: 2018-08-05T23:23:30.825528: step 1000, loss 0.606553.
Test: 2018-08-05T23:23:31.059853: step 1000, loss 0.549333.
Train: 2018-08-05T23:23:31.856540: step 1001, loss 0.607965.
Train: 2018-08-05T23:23:32.012747: step 1002, loss 0.572551.
Train: 2018-08-05T23:23:32.168968: step 1003, loss 0.564135.
Train: 2018-08-05T23:23:32.325151: step 1004, loss 0.555139.
Train: 2018-08-05T23:23:32.481363: step 1005, loss 0.667045.
Train: 2018-08-05T23:23:32.637576: step 1006, loss 0.556319.
Train: 2018-08-05T23:23:32.778199: step 1007, loss 0.495433.
Train: 2018-08-05T23:23:32.934412: step 1008, loss 0.572673.
Train: 2018-08-05T23:23:33.090619: step 1009, loss 0.520643.
Train: 2018-08-05T23:23:33.249475: step 1010, loss 0.49429.
Test: 2018-08-05T23:23:33.483789: step 1010, loss 0.549343.
Train: 2018-08-05T23:23:33.640001: step 1011, loss 0.572526.
Train: 2018-08-05T23:23:33.796215: step 1012, loss 0.537174.
Train: 2018-08-05T23:23:33.936806: step 1013, loss 0.529167.
Train: 2018-08-05T23:23:34.093020: step 1014, loss 0.554923.
Train: 2018-08-05T23:23:34.249204: step 1015, loss 0.493563.
Train: 2018-08-05T23:23:34.405417: step 1016, loss 0.554103.
Train: 2018-08-05T23:23:34.561661: step 1017, loss 0.527158.
Train: 2018-08-05T23:23:34.717843: step 1018, loss 0.634958.
Train: 2018-08-05T23:23:34.874086: step 1019, loss 0.545802.
Train: 2018-08-05T23:23:35.045904: step 1020, loss 0.483733.
Test: 2018-08-05T23:23:35.280210: step 1020, loss 0.54903.
Train: 2018-08-05T23:23:35.436448: step 1021, loss 0.610706.
Train: 2018-08-05T23:23:35.592666: step 1022, loss 0.661621.
Train: 2018-08-05T23:23:35.764470: step 1023, loss 0.57469.
Train: 2018-08-05T23:23:35.920684: step 1024, loss 0.527853.
Train: 2018-08-05T23:23:36.076897: step 1025, loss 0.556511.
Train: 2018-08-05T23:23:36.233110: step 1026, loss 0.561946.
Train: 2018-08-05T23:23:36.389354: step 1027, loss 0.501607.
Train: 2018-08-05T23:23:36.545567: step 1028, loss 0.556072.
Train: 2018-08-05T23:23:36.701750: step 1029, loss 0.555523.
Train: 2018-08-05T23:23:36.842342: step 1030, loss 0.619675.
Test: 2018-08-05T23:23:37.076693: step 1030, loss 0.549053.
Train: 2018-08-05T23:23:37.232901: step 1031, loss 0.557019.
Train: 2018-08-05T23:23:37.404711: step 1032, loss 0.565227.
Train: 2018-08-05T23:23:37.560957: step 1033, loss 0.562688.
Train: 2018-08-05T23:23:37.717137: step 1034, loss 0.589635.
Train: 2018-08-05T23:23:37.873351: step 1035, loss 0.608194.
Train: 2018-08-05T23:23:38.029564: step 1036, loss 0.59964.
Train: 2018-08-05T23:23:38.201399: step 1037, loss 0.556749.
Train: 2018-08-05T23:23:38.357614: step 1038, loss 0.469679.
Train: 2018-08-05T23:23:38.513824: step 1039, loss 0.632911.
Train: 2018-08-05T23:23:38.670073: step 1040, loss 0.599755.
Test: 2018-08-05T23:23:38.904388: step 1040, loss 0.549403.
Train: 2018-08-05T23:23:39.076223: step 1041, loss 0.588934.
Train: 2018-08-05T23:23:39.232437: step 1042, loss 0.521483.
Train: 2018-08-05T23:23:39.404241: step 1043, loss 0.598423.
Train: 2018-08-05T23:23:39.560483: step 1044, loss 0.572372.
Train: 2018-08-05T23:23:39.716698: step 1045, loss 0.555912.
Train: 2018-08-05T23:23:39.872913: step 1046, loss 0.582521.
Train: 2018-08-05T23:23:40.029094: step 1047, loss 0.51269.
Train: 2018-08-05T23:23:40.185336: step 1048, loss 0.598193.
Train: 2018-08-05T23:23:40.357166: step 1049, loss 0.60631.
Train: 2018-08-05T23:23:40.513378: step 1050, loss 0.61453.
Test: 2018-08-05T23:23:40.747707: step 1050, loss 0.550004.
Train: 2018-08-05T23:23:40.903887: step 1051, loss 0.555426.
Train: 2018-08-05T23:23:41.060100: step 1052, loss 0.563213.
Train: 2018-08-05T23:23:41.231936: step 1053, loss 0.572947.
Train: 2018-08-05T23:23:41.388148: step 1054, loss 0.572272.
Train: 2018-08-05T23:23:41.544362: step 1055, loss 0.604699.
Train: 2018-08-05T23:23:41.700605: step 1056, loss 0.563501.
Train: 2018-08-05T23:23:41.841201: step 1057, loss 0.495436.
Train: 2018-08-05T23:23:41.997413: step 1058, loss 0.53986.
Train: 2018-08-05T23:23:42.169215: step 1059, loss 0.573451.
Train: 2018-08-05T23:23:42.325428: step 1060, loss 0.540135.
Test: 2018-08-05T23:23:42.559778: step 1060, loss 0.550306.
Train: 2018-08-05T23:23:42.731581: step 1061, loss 0.622337.
Train: 2018-08-05T23:23:42.887825: step 1062, loss 0.513195.
Train: 2018-08-05T23:23:43.044038: step 1063, loss 0.51428.
Train: 2018-08-05T23:23:43.200248: step 1064, loss 0.540093.
Train: 2018-08-05T23:23:43.409812: step 1065, loss 0.529767.
Train: 2018-08-05T23:23:43.644184: step 1066, loss 0.546807.
Train: 2018-08-05T23:23:43.815107: step 1067, loss 0.520896.
Train: 2018-08-05T23:23:43.986937: step 1068, loss 0.522016.
Train: 2018-08-05T23:23:44.143150: step 1069, loss 0.503605.
Train: 2018-08-05T23:23:44.299394: step 1070, loss 0.581183.
Test: 2018-08-05T23:23:44.549305: step 1070, loss 0.549117.
Train: 2018-08-05T23:23:44.705547: step 1071, loss 0.520428.
Train: 2018-08-05T23:23:44.877382: step 1072, loss 0.661811.
Train: 2018-08-05T23:23:45.033565: step 1073, loss 0.652263.
Train: 2018-08-05T23:23:45.189778: step 1074, loss 0.634365.
Train: 2018-08-05T23:23:45.346026: step 1075, loss 0.53745.
Train: 2018-08-05T23:23:45.501398: step 1076, loss 0.555548.
Train: 2018-08-05T23:23:45.673233: step 1077, loss 0.510952.
Train: 2018-08-05T23:23:45.829445: step 1078, loss 0.609275.
Train: 2018-08-05T23:23:46.001306: step 1079, loss 0.510567.
Train: 2018-08-05T23:23:46.157494: step 1080, loss 0.510131.
Test: 2018-08-05T23:23:46.391844: step 1080, loss 0.549013.
Train: 2018-08-05T23:23:46.548057: step 1081, loss 0.607849.
Train: 2018-08-05T23:23:46.719861: step 1082, loss 0.556985.
Train: 2018-08-05T23:23:46.876075: step 1083, loss 0.56364.
Train: 2018-08-05T23:23:47.047909: step 1084, loss 0.572919.
Train: 2018-08-05T23:23:47.204122: step 1085, loss 0.688005.
Train: 2018-08-05T23:23:47.375957: step 1086, loss 0.528949.
Train: 2018-08-05T23:23:47.516579: step 1087, loss 0.589901.
Train: 2018-08-05T23:23:47.672792: step 1088, loss 0.512258.
Train: 2018-08-05T23:23:47.828976: step 1089, loss 0.538289.
Train: 2018-08-05T23:23:47.985218: step 1090, loss 0.616313.
Test: 2018-08-05T23:23:48.235131: step 1090, loss 0.549219.
Train: 2018-08-05T23:23:48.375753: step 1091, loss 0.538116.
Train: 2018-08-05T23:23:48.547556: step 1092, loss 0.486158.
Train: 2018-08-05T23:23:48.703770: step 1093, loss 0.512006.
Train: 2018-08-05T23:23:48.860013: step 1094, loss 0.504175.
Train: 2018-08-05T23:23:49.016226: step 1095, loss 0.607534.
Train: 2018-08-05T23:23:49.172439: step 1096, loss 0.502236.
Train: 2018-08-05T23:23:49.328623: step 1097, loss 0.606161.
Train: 2018-08-05T23:23:49.500487: step 1098, loss 0.598247.
Train: 2018-08-05T23:23:49.641050: step 1099, loss 0.60786.
Train: 2018-08-05T23:23:49.812883: step 1100, loss 0.537481.
Test: 2018-08-05T23:23:50.047234: step 1100, loss 0.549082.
Train: 2018-08-05T23:23:50.769604: step 1101, loss 0.563323.
Train: 2018-08-05T23:23:50.925788: step 1102, loss 0.554289.
Train: 2018-08-05T23:23:51.082013: step 1103, loss 0.682861.
Train: 2018-08-05T23:23:51.238215: step 1104, loss 0.62383.
Train: 2018-08-05T23:23:51.394428: step 1105, loss 0.60655.
Train: 2018-08-05T23:23:51.566289: step 1106, loss 0.597433.
Train: 2018-08-05T23:23:51.722500: step 1107, loss 0.537725.
Train: 2018-08-05T23:23:51.909932: step 1108, loss 0.537936.
Train: 2018-08-05T23:23:52.144253: step 1109, loss 0.589631.
Train: 2018-08-05T23:23:52.349384: step 1110, loss 0.589213.
Test: 2018-08-05T23:23:52.630565: step 1110, loss 0.549709.
Train: 2018-08-05T23:23:52.818021: step 1111, loss 0.546706.
Train: 2018-08-05T23:23:52.958616: step 1112, loss 0.563196.
Train: 2018-08-05T23:23:53.114827: step 1113, loss 0.547047.
Train: 2018-08-05T23:23:53.271070: step 1114, loss 0.563489.
Train: 2018-08-05T23:23:53.427284: step 1115, loss 0.636812.
Train: 2018-08-05T23:23:53.583499: step 1116, loss 0.507147.
Train: 2018-08-05T23:23:53.739680: step 1117, loss 0.621576.
Train: 2018-08-05T23:23:53.895923: step 1118, loss 0.555316.
Train: 2018-08-05T23:23:54.052107: step 1119, loss 0.5381.
Train: 2018-08-05T23:23:54.208352: step 1120, loss 0.554378.
Test: 2018-08-05T23:23:54.442656: step 1120, loss 0.549966.
Train: 2018-08-05T23:23:54.614504: step 1121, loss 0.580716.
Train: 2018-08-05T23:23:54.755096: step 1122, loss 0.524109.
Train: 2018-08-05T23:23:54.926901: step 1123, loss 0.555017.
Train: 2018-08-05T23:23:55.067523: step 1124, loss 0.588277.
Train: 2018-08-05T23:23:55.223738: step 1125, loss 0.578605.
Train: 2018-08-05T23:23:55.379949: step 1126, loss 0.564105.
Train: 2018-08-05T23:23:55.536132: step 1127, loss 0.564178.
Train: 2018-08-05T23:23:55.692345: step 1128, loss 0.620595.
Train: 2018-08-05T23:23:55.848591: step 1129, loss 0.571675.
Train: 2018-08-05T23:23:56.004802: step 1130, loss 0.630757.
Test: 2018-08-05T23:23:56.239123: step 1130, loss 0.549892.
Train: 2018-08-05T23:23:56.410927: step 1131, loss 0.555355.
Train: 2018-08-05T23:23:56.551549: step 1132, loss 0.514427.
Train: 2018-08-05T23:23:56.708649: step 1133, loss 0.620209.
Train: 2018-08-05T23:23:56.864861: step 1134, loss 0.546731.
Train: 2018-08-05T23:23:57.021108: step 1135, loss 0.514811.
Train: 2018-08-05T23:23:57.177319: step 1136, loss 0.606029.
Train: 2018-08-05T23:23:57.333532: step 1137, loss 0.646849.
Train: 2018-08-05T23:23:57.489745: step 1138, loss 0.579848.
Train: 2018-08-05T23:23:57.645960: step 1139, loss 0.554551.
Train: 2018-08-05T23:23:57.802172: step 1140, loss 0.56459.
Test: 2018-08-05T23:23:58.036495: step 1140, loss 0.550239.
Train: 2018-08-05T23:23:58.197382: step 1141, loss 0.627246.
Train: 2018-08-05T23:23:58.353596: step 1142, loss 0.604538.
Train: 2018-08-05T23:23:58.509841: step 1143, loss 0.467767.
Train: 2018-08-05T23:23:58.681645: step 1144, loss 0.506787.
Train: 2018-08-05T23:23:58.837887: step 1145, loss 0.548121.
Train: 2018-08-05T23:23:58.994110: step 1146, loss 0.57969.
Train: 2018-08-05T23:23:59.150313: step 1147, loss 0.580636.
Train: 2018-08-05T23:23:59.306532: step 1148, loss 0.546561.
Train: 2018-08-05T23:23:59.447125: step 1149, loss 0.635555.
Train: 2018-08-05T23:23:59.603332: step 1150, loss 0.497407.
Test: 2018-08-05T23:23:59.837648: step 1150, loss 0.549845.
Train: 2018-08-05T23:24:00.009456: step 1151, loss 0.571018.
Train: 2018-08-05T23:24:00.150079: step 1152, loss 0.589315.
Train: 2018-08-05T23:24:00.306292: step 1153, loss 0.571535.
Train: 2018-08-05T23:24:00.462506: step 1154, loss 0.589267.
Train: 2018-08-05T23:24:00.618722: step 1155, loss 0.653981.
Train: 2018-08-05T23:24:00.774932: step 1156, loss 0.571749.
Train: 2018-08-05T23:24:00.931145: step 1157, loss 0.54088.
Train: 2018-08-05T23:24:01.087361: step 1158, loss 0.555807.
Train: 2018-08-05T23:24:01.243542: step 1159, loss 0.53202.
Train: 2018-08-05T23:24:01.399754: step 1160, loss 0.547634.
Test: 2018-08-05T23:24:01.634106: step 1160, loss 0.549712.
Train: 2018-08-05T23:24:01.790318: step 1161, loss 0.637352.
Train: 2018-08-05T23:24:01.977743: step 1162, loss 0.548281.
Train: 2018-08-05T23:24:02.133957: step 1163, loss 0.555002.
Train: 2018-08-05T23:24:02.274584: step 1164, loss 0.514307.
Train: 2018-08-05T23:24:02.430761: step 1165, loss 0.546743.
Train: 2018-08-05T23:24:02.587000: step 1166, loss 0.53017.
Train: 2018-08-05T23:24:02.743188: step 1167, loss 0.554362.
Train: 2018-08-05T23:24:02.899427: step 1168, loss 0.530703.
Train: 2018-08-05T23:24:03.055615: step 1169, loss 0.488012.
Train: 2018-08-05T23:24:03.211861: step 1170, loss 0.633681.
Test: 2018-08-05T23:24:03.446179: step 1170, loss 0.548985.
Train: 2018-08-05T23:24:03.602391: step 1171, loss 0.580545.
Train: 2018-08-05T23:24:03.758574: step 1172, loss 0.649955.
Train: 2018-08-05T23:24:03.914817: step 1173, loss 0.52917.
Train: 2018-08-05T23:24:04.086622: step 1174, loss 0.529234.
Train: 2018-08-05T23:24:04.227239: step 1175, loss 0.555919.
Train: 2018-08-05T23:24:04.395568: step 1176, loss 0.537786.
Train: 2018-08-05T23:24:04.537743: step 1177, loss 0.527787.
Train: 2018-08-05T23:24:04.693963: step 1178, loss 0.556267.
Train: 2018-08-05T23:24:04.850173: step 1179, loss 0.589427.
Train: 2018-08-05T23:24:05.006389: step 1180, loss 0.573648.
Test: 2018-08-05T23:24:05.256300: step 1180, loss 0.548763.
Train: 2018-08-05T23:24:05.396923: step 1181, loss 0.545432.
Train: 2018-08-05T23:24:05.568727: step 1182, loss 0.55466.
Train: 2018-08-05T23:24:05.709348: step 1183, loss 0.52186.
Train: 2018-08-05T23:24:05.881183: step 1184, loss 0.601348.
Train: 2018-08-05T23:24:06.037365: step 1185, loss 0.607761.
Train: 2018-08-05T23:24:06.193578: step 1186, loss 0.510016.
Train: 2018-08-05T23:24:06.349822: step 1187, loss 0.545943.
Train: 2018-08-05T23:24:06.506036: step 1188, loss 0.580687.
Train: 2018-08-05T23:24:06.662219: step 1189, loss 0.509908.
Train: 2018-08-05T23:24:06.818433: step 1190, loss 0.544757.
Test: 2018-08-05T23:24:07.068373: step 1190, loss 0.548722.
Train: 2018-08-05T23:24:07.224612: step 1191, loss 0.528987.
Train: 2018-08-05T23:24:07.380830: step 1192, loss 0.555005.
Train: 2018-08-05T23:24:07.537037: step 1193, loss 0.527615.
Train: 2018-08-05T23:24:07.693257: step 1194, loss 0.590219.
Train: 2018-08-05T23:24:07.849478: step 1195, loss 0.562407.
Train: 2018-08-05T23:24:08.005677: step 1196, loss 0.475235.
Train: 2018-08-05T23:24:08.161865: step 1197, loss 0.616846.
Train: 2018-08-05T23:24:08.318110: step 1198, loss 0.520779.
Train: 2018-08-05T23:24:08.474324: step 1199, loss 0.58154.
Train: 2018-08-05T23:24:08.630505: step 1200, loss 0.582063.
Test: 2018-08-05T23:24:08.942933: step 1200, loss 0.548616.
Train: 2018-08-05T23:24:09.677165: step 1201, loss 0.527799.
Train: 2018-08-05T23:24:09.833378: step 1202, loss 0.590328.
Train: 2018-08-05T23:24:09.989561: step 1203, loss 0.509306.
Train: 2018-08-05T23:24:10.145774: step 1204, loss 0.574066.
Train: 2018-08-05T23:24:10.317639: step 1205, loss 0.537729.
Train: 2018-08-05T23:24:10.473822: step 1206, loss 0.537351.
Train: 2018-08-05T23:24:10.630036: step 1207, loss 0.598989.
Train: 2018-08-05T23:24:10.770652: step 1208, loss 0.466769.
Train: 2018-08-05T23:24:10.926840: step 1209, loss 0.536715.
Train: 2018-08-05T23:24:11.083053: step 1210, loss 0.627338.
Test: 2018-08-05T23:24:11.317376: step 1210, loss 0.548579.
Train: 2018-08-05T23:24:11.520481: step 1211, loss 0.528479.
Train: 2018-08-05T23:24:11.676695: step 1212, loss 0.599787.
Train: 2018-08-05T23:24:11.832905: step 1213, loss 0.527677.
Train: 2018-08-05T23:24:12.004736: step 1214, loss 0.537403.
Train: 2018-08-05T23:24:12.160955: step 1215, loss 0.554277.
Train: 2018-08-05T23:24:12.317139: step 1216, loss 0.509314.
Train: 2018-08-05T23:24:12.473351: step 1217, loss 0.572617.
Train: 2018-08-05T23:24:12.629590: step 1218, loss 0.573885.
Train: 2018-08-05T23:24:12.785814: step 1219, loss 0.608716.
Train: 2018-08-05T23:24:12.941991: step 1220, loss 0.600678.
Test: 2018-08-05T23:24:13.191938: step 1220, loss 0.548577.
Train: 2018-08-05T23:24:13.332526: step 1221, loss 0.582757.
Train: 2018-08-05T23:24:13.488769: step 1222, loss 0.54407.
Train: 2018-08-05T23:24:13.644982: step 1223, loss 0.509958.
Train: 2018-08-05T23:24:13.801164: step 1224, loss 0.563453.
Train: 2018-08-05T23:24:13.957407: step 1225, loss 0.625625.
Train: 2018-08-05T23:24:14.098001: step 1226, loss 0.609014.
Train: 2018-08-05T23:24:14.254213: step 1227, loss 0.600015.
Train: 2018-08-05T23:24:14.410427: step 1228, loss 0.537276.
Train: 2018-08-05T23:24:14.566610: step 1229, loss 0.589582.
Train: 2018-08-05T23:24:14.722823: step 1230, loss 0.511006.
Test: 2018-08-05T23:24:14.957191: step 1230, loss 0.548836.
Train: 2018-08-05T23:24:15.113386: step 1231, loss 0.60692.
Train: 2018-08-05T23:24:15.269599: step 1232, loss 0.468144.
Train: 2018-08-05T23:24:15.441428: step 1233, loss 0.545416.
Train: 2018-08-05T23:24:15.597647: step 1234, loss 0.597094.
Train: 2018-08-05T23:24:15.753860: step 1235, loss 0.512318.
Train: 2018-08-05T23:24:15.894452: step 1236, loss 0.572036.
Train: 2018-08-05T23:24:16.066281: step 1237, loss 0.442103.
Train: 2018-08-05T23:24:16.222469: step 1238, loss 0.520444.
Train: 2018-08-05T23:24:16.378708: step 1239, loss 0.520254.
Train: 2018-08-05T23:24:16.534931: step 1240, loss 0.545979.
Test: 2018-08-05T23:24:16.784864: step 1240, loss 0.548705.
Train: 2018-08-05T23:24:16.925460: step 1241, loss 0.554499.
Train: 2018-08-05T23:24:17.097299: step 1242, loss 0.615939.
Train: 2018-08-05T23:24:17.253513: step 1243, loss 0.457967.
Train: 2018-08-05T23:24:17.409720: step 1244, loss 0.617106.
Train: 2018-08-05T23:24:17.565904: step 1245, loss 0.528439.
Train: 2018-08-05T23:24:17.722117: step 1246, loss 0.537739.
Train: 2018-08-05T23:24:17.878361: step 1247, loss 0.581559.
Train: 2018-08-05T23:24:18.034574: step 1248, loss 0.527648.
Train: 2018-08-05T23:24:18.206378: step 1249, loss 0.617836.
Train: 2018-08-05T23:24:18.347000: step 1250, loss 0.482632.
Test: 2018-08-05T23:24:18.596913: step 1250, loss 0.548508.
Train: 2018-08-05T23:24:18.753124: step 1251, loss 0.590741.
Train: 2018-08-05T23:24:18.909368: step 1252, loss 0.609277.
Train: 2018-08-05T23:24:19.065578: step 1253, loss 0.52784.
Train: 2018-08-05T23:24:19.221798: step 1254, loss 0.517987.
Train: 2018-08-05T23:24:19.378008: step 1255, loss 0.590088.
Train: 2018-08-05T23:24:19.534190: step 1256, loss 0.562639.
Train: 2018-08-05T23:24:19.674782: step 1257, loss 0.527819.
Train: 2018-08-05T23:24:19.846647: step 1258, loss 0.590242.
Train: 2018-08-05T23:24:20.002831: step 1259, loss 0.626785.
Train: 2018-08-05T23:24:20.159073: step 1260, loss 0.626112.
Test: 2018-08-05T23:24:20.393395: step 1260, loss 0.54852.
Train: 2018-08-05T23:24:20.549601: step 1261, loss 0.509964.
Train: 2018-08-05T23:24:20.705790: step 1262, loss 0.625482.
Train: 2018-08-05T23:24:20.862003: step 1263, loss 0.625755.
Train: 2018-08-05T23:24:21.018241: step 1264, loss 0.545359.
Train: 2018-08-05T23:24:21.190076: step 1265, loss 0.537094.
Train: 2018-08-05T23:24:21.330673: step 1266, loss 0.563865.
Train: 2018-08-05T23:24:21.486892: step 1267, loss 0.564389.
Train: 2018-08-05T23:24:21.659214: step 1268, loss 0.598327.
Train: 2018-08-05T23:24:21.806479: step 1269, loss 0.589944.
Train: 2018-08-05T23:24:21.962723: step 1270, loss 0.494081.
Test: 2018-08-05T23:24:22.197013: step 1270, loss 0.548937.
Train: 2018-08-05T23:24:22.368872: step 1271, loss 0.546071.
Train: 2018-08-05T23:24:22.525093: step 1272, loss 0.614815.
Train: 2018-08-05T23:24:22.681273: step 1273, loss 0.564036.
Train: 2018-08-05T23:24:22.837488: step 1274, loss 0.478882.
Train: 2018-08-05T23:24:22.978080: step 1275, loss 0.596815.
Train: 2018-08-05T23:24:23.134323: step 1276, loss 0.537313.
Train: 2018-08-05T23:24:23.290506: step 1277, loss 0.554256.
Train: 2018-08-05T23:24:23.446719: step 1278, loss 0.520421.
Train: 2018-08-05T23:24:23.602932: step 1279, loss 0.504348.
Train: 2018-08-05T23:24:23.774796: step 1280, loss 0.50346.
Test: 2018-08-05T23:24:24.009088: step 1280, loss 0.548856.
Train: 2018-08-05T23:24:24.165324: step 1281, loss 0.528918.
Train: 2018-08-05T23:24:24.325071: step 1282, loss 0.61403.
Train: 2018-08-05T23:24:24.481313: step 1283, loss 0.563914.
Train: 2018-08-05T23:24:24.637497: step 1284, loss 0.53645.
Train: 2018-08-05T23:24:24.793740: step 1285, loss 0.589092.
Train: 2018-08-05T23:24:24.949923: step 1286, loss 0.65045.
Train: 2018-08-05T23:24:25.090515: step 1287, loss 0.579168.
Train: 2018-08-05T23:24:25.246729: step 1288, loss 0.554267.
Train: 2018-08-05T23:24:25.402942: step 1289, loss 0.580347.
Train: 2018-08-05T23:24:25.559156: step 1290, loss 0.665148.
Test: 2018-08-05T23:24:25.809096: step 1290, loss 0.549236.
Train: 2018-08-05T23:24:25.965310: step 1291, loss 0.581751.
Train: 2018-08-05T23:24:26.105931: step 1292, loss 0.553897.
Train: 2018-08-05T23:24:26.277760: step 1293, loss 0.616228.
Train: 2018-08-05T23:24:26.433979: step 1294, loss 0.555495.
Train: 2018-08-05T23:24:26.590163: step 1295, loss 0.532903.
Train: 2018-08-05T23:24:26.746376: step 1296, loss 0.599469.
Train: 2018-08-05T23:24:26.902613: step 1297, loss 0.569492.
Train: 2018-08-05T23:24:27.058801: step 1298, loss 0.572519.
Train: 2018-08-05T23:24:27.215047: step 1299, loss 0.602937.
Train: 2018-08-05T23:24:27.371263: step 1300, loss 0.562409.
Test: 2018-08-05T23:24:27.621195: step 1300, loss 0.55011.
Train: 2018-08-05T23:24:28.324637: step 1301, loss 0.573786.
Train: 2018-08-05T23:24:28.480851: step 1302, loss 0.524398.
Train: 2018-08-05T23:24:28.621475: step 1303, loss 0.492635.
Train: 2018-08-05T23:24:28.793278: step 1304, loss 0.598205.
Train: 2018-08-05T23:24:28.949490: step 1305, loss 0.606396.
Train: 2018-08-05T23:24:29.105703: step 1306, loss 0.588878.
Train: 2018-08-05T23:24:29.261917: step 1307, loss 0.55542.
Train: 2018-08-05T23:24:29.418130: step 1308, loss 0.587803.
Train: 2018-08-05T23:24:29.574374: step 1309, loss 0.571972.
Train: 2018-08-05T23:24:29.738277: step 1310, loss 0.61264.
Test: 2018-08-05T23:24:29.972606: step 1310, loss 0.549433.
Train: 2018-08-05T23:24:30.128817: step 1311, loss 0.545037.
Train: 2018-08-05T23:24:30.285029: step 1312, loss 0.506575.
Train: 2018-08-05T23:24:30.441250: step 1313, loss 0.528713.
Train: 2018-08-05T23:24:30.597458: step 1314, loss 0.522837.
Train: 2018-08-05T23:24:30.753638: step 1315, loss 0.580979.
Train: 2018-08-05T23:24:30.909851: step 1316, loss 0.469317.
Train: 2018-08-05T23:24:31.050443: step 1317, loss 0.545508.
Train: 2018-08-05T23:24:31.206689: step 1318, loss 0.590015.
Train: 2018-08-05T23:24:31.362870: step 1319, loss 0.572325.
Train: 2018-08-05T23:24:31.519083: step 1320, loss 0.589371.
Test: 2018-08-05T23:24:31.753405: step 1320, loss 0.548603.
Train: 2018-08-05T23:24:31.909646: step 1321, loss 0.527181.
Train: 2018-08-05T23:24:32.065829: step 1322, loss 0.501787.
Train: 2018-08-05T23:24:32.237689: step 1323, loss 0.572167.
Train: 2018-08-05T23:24:32.393877: step 1324, loss 0.607496.
Train: 2018-08-05T23:24:32.534469: step 1325, loss 0.509923.
Train: 2018-08-05T23:24:32.690682: step 1326, loss 0.643729.
Train: 2018-08-05T23:24:32.846928: step 1327, loss 0.670395.
Train: 2018-08-05T23:24:33.003109: step 1328, loss 0.572437.
Train: 2018-08-05T23:24:33.159322: step 1329, loss 0.563519.
Train: 2018-08-05T23:24:33.315536: step 1330, loss 0.545542.
Test: 2018-08-05T23:24:33.549855: step 1330, loss 0.548558.
Train: 2018-08-05T23:24:33.721722: step 1331, loss 0.564153.
Train: 2018-08-05T23:24:33.862312: step 1332, loss 0.475796.
Train: 2018-08-05T23:24:34.018525: step 1333, loss 0.579571.
Train: 2018-08-05T23:24:34.174738: step 1334, loss 0.607368.
Train: 2018-08-05T23:24:34.330951: step 1335, loss 0.624018.
Train: 2018-08-05T23:24:34.487136: step 1336, loss 0.537413.
Train: 2018-08-05T23:24:34.643378: step 1337, loss 0.562923.
Train: 2018-08-05T23:24:34.799562: step 1338, loss 0.571565.
Train: 2018-08-05T23:24:34.955805: step 1339, loss 0.528468.
Train: 2018-08-05T23:24:35.111988: step 1340, loss 0.588516.
Test: 2018-08-05T23:24:35.346341: step 1340, loss 0.548723.
Train: 2018-08-05T23:24:35.502551: step 1341, loss 0.579769.
Train: 2018-08-05T23:24:35.668378: step 1342, loss 0.5366.
Train: 2018-08-05T23:24:35.824566: step 1343, loss 0.537513.
Train: 2018-08-05T23:24:35.980809: step 1344, loss 0.624247.
Train: 2018-08-05T23:24:36.121372: step 1345, loss 0.606266.
Train: 2018-08-05T23:24:36.277586: step 1346, loss 0.590067.
Train: 2018-08-05T23:24:36.433828: step 1347, loss 0.571643.
Train: 2018-08-05T23:24:36.590041: step 1348, loss 0.547148.
Train: 2018-08-05T23:24:36.746254: step 1349, loss 0.546147.
Train: 2018-08-05T23:24:36.902467: step 1350, loss 0.605269.
Test: 2018-08-05T23:24:37.152379: step 1350, loss 0.549052.
Train: 2018-08-05T23:24:37.308616: step 1351, loss 0.470203.
Train: 2018-08-05T23:24:37.464805: step 1352, loss 0.512562.
Train: 2018-08-05T23:24:37.605398: step 1353, loss 0.512458.
Train: 2018-08-05T23:24:37.777231: step 1354, loss 0.529288.
Train: 2018-08-05T23:24:37.933444: step 1355, loss 0.546515.
Train: 2018-08-05T23:24:38.089658: step 1356, loss 0.614279.
Train: 2018-08-05T23:24:38.245870: step 1357, loss 0.571141.
Train: 2018-08-05T23:24:38.402084: step 1358, loss 0.58874.
Train: 2018-08-05T23:24:38.542707: step 1359, loss 0.581688.
Train: 2018-08-05T23:24:38.698889: step 1360, loss 0.537294.
Test: 2018-08-05T23:24:38.948861: step 1360, loss 0.548772.
Train: 2018-08-05T23:24:39.105075: step 1361, loss 0.554467.
Train: 2018-08-05T23:24:39.261287: step 1362, loss 0.503179.
Train: 2018-08-05T23:24:39.417501: step 1363, loss 0.631016.
Train: 2018-08-05T23:24:39.573714: step 1364, loss 0.536862.
Train: 2018-08-05T23:24:39.729927: step 1365, loss 0.528661.
Train: 2018-08-05T23:24:39.886111: step 1366, loss 0.588438.
Train: 2018-08-05T23:24:40.026703: step 1367, loss 0.502463.
Train: 2018-08-05T23:24:40.198561: step 1368, loss 0.632697.
Train: 2018-08-05T23:24:40.339153: step 1369, loss 0.545662.
Train: 2018-08-05T23:24:40.495343: step 1370, loss 0.485327.
Test: 2018-08-05T23:24:40.729682: step 1370, loss 0.548589.
Train: 2018-08-05T23:24:40.885905: step 1371, loss 0.519421.
Train: 2018-08-05T23:24:41.042119: step 1372, loss 0.58059.
Train: 2018-08-05T23:24:41.198335: step 1373, loss 0.555353.
Train: 2018-08-05T23:24:41.354545: step 1374, loss 0.545495.
Train: 2018-08-05T23:24:41.510758: step 1375, loss 0.501838.
Train: 2018-08-05T23:24:41.666972: step 1376, loss 0.581149.
Train: 2018-08-05T23:24:41.807563: step 1377, loss 0.544967.
Train: 2018-08-05T23:24:41.963747: step 1378, loss 0.510165.
Train: 2018-08-05T23:24:42.135581: step 1379, loss 0.528344.
Train: 2018-08-05T23:24:42.291795: step 1380, loss 0.582123.
Test: 2018-08-05T23:24:42.526116: step 1380, loss 0.548336.
Train: 2018-08-05T23:24:42.682328: step 1381, loss 0.580794.
Train: 2018-08-05T23:24:42.838571: step 1382, loss 0.518632.
Train: 2018-08-05T23:24:42.994753: step 1383, loss 0.581212.
Train: 2018-08-05T23:24:43.150997: step 1384, loss 0.49133.
Train: 2018-08-05T23:24:43.307210: step 1385, loss 0.644528.
Train: 2018-08-05T23:24:43.447803: step 1386, loss 0.48244.
Train: 2018-08-05T23:24:43.619606: step 1387, loss 0.499602.
Train: 2018-08-05T23:24:43.775853: step 1388, loss 0.589747.
Train: 2018-08-05T23:24:43.932033: step 1389, loss 0.472152.
Train: 2018-08-05T23:24:44.088277: step 1390, loss 0.627495.
Test: 2018-08-05T23:24:44.322568: step 1390, loss 0.548284.
Train: 2018-08-05T23:24:44.478780: step 1391, loss 0.59114.
Train: 2018-08-05T23:24:44.635024: step 1392, loss 0.572501.
Train: 2018-08-05T23:24:44.791207: step 1393, loss 0.553628.
Train: 2018-08-05T23:24:44.947444: step 1394, loss 0.527631.
Train: 2018-08-05T23:24:45.103632: step 1395, loss 0.590831.
Train: 2018-08-05T23:24:45.259845: step 1396, loss 0.564276.
Train: 2018-08-05T23:24:45.400438: step 1397, loss 0.627176.
Train: 2018-08-05T23:24:45.572302: step 1398, loss 0.472992.
Train: 2018-08-05T23:24:45.712864: step 1399, loss 0.681349.
Train: 2018-08-05T23:24:45.869077: step 1400, loss 0.580145.
Test: 2018-08-05T23:24:46.119019: step 1400, loss 0.5483.
Train: 2018-08-05T23:24:46.822009: step 1401, loss 0.562657.
Train: 2018-08-05T23:24:46.978223: step 1402, loss 0.572234.
Train: 2018-08-05T23:24:47.134430: step 1403, loss 0.581272.
Train: 2018-08-05T23:24:47.290643: step 1404, loss 0.537129.
Train: 2018-08-05T23:24:47.446862: step 1405, loss 0.589191.
Train: 2018-08-05T23:24:47.618690: step 1406, loss 0.66027.
Train: 2018-08-05T23:24:47.759289: step 1407, loss 0.554602.
Train: 2018-08-05T23:24:47.915502: step 1408, loss 0.6228.
Train: 2018-08-05T23:24:48.071714: step 1409, loss 0.511531.
Train: 2018-08-05T23:24:48.227922: step 1410, loss 0.562985.
Test: 2018-08-05T23:24:48.462218: step 1410, loss 0.548753.
Train: 2018-08-05T23:24:48.618430: step 1411, loss 0.537902.
Train: 2018-08-05T23:24:48.774644: step 1412, loss 0.630843.
Train: 2018-08-05T23:24:48.930857: step 1413, loss 0.562737.
Train: 2018-08-05T23:24:49.087070: step 1414, loss 0.529331.
Train: 2018-08-05T23:24:49.243283: step 1415, loss 0.588954.
Train: 2018-08-05T23:24:49.399543: step 1416, loss 0.562773.
Train: 2018-08-05T23:24:49.555710: step 1417, loss 0.563286.
Train: 2018-08-05T23:24:49.711923: step 1418, loss 0.545744.
Train: 2018-08-05T23:24:49.868136: step 1419, loss 0.555681.
Train: 2018-08-05T23:24:50.024350: step 1420, loss 0.622338.
Test: 2018-08-05T23:24:50.258701: step 1420, loss 0.549228.
Train: 2018-08-05T23:24:50.414914: step 1421, loss 0.538373.
Train: 2018-08-05T23:24:50.571121: step 1422, loss 0.522033.
Train: 2018-08-05T23:24:50.727340: step 1423, loss 0.56267.
Train: 2018-08-05T23:24:50.883553: step 1424, loss 0.604515.
Train: 2018-08-05T23:24:51.039735: step 1425, loss 0.579625.
Train: 2018-08-05T23:24:51.195949: step 1426, loss 0.546333.
Train: 2018-08-05T23:24:51.336573: step 1427, loss 0.554359.
Train: 2018-08-05T23:24:51.492755: step 1428, loss 0.662339.
Train: 2018-08-05T23:24:51.649001: step 1429, loss 0.488263.
Train: 2018-08-05T23:24:51.805211: step 1430, loss 0.579653.
Test: 2018-08-05T23:24:52.039501: step 1430, loss 0.549337.
Train: 2018-08-05T23:24:52.195738: step 1431, loss 0.588896.
Train: 2018-08-05T23:24:52.367589: step 1432, loss 0.530667.
Train: 2018-08-05T23:24:52.508165: step 1433, loss 0.621146.
Train: 2018-08-05T23:24:52.664385: step 1434, loss 0.530579.
Train: 2018-08-05T23:24:52.817516: step 1435, loss 0.570999.
Train: 2018-08-05T23:24:52.982410: step 1436, loss 0.572019.
Train: 2018-08-05T23:24:53.138624: step 1437, loss 0.563961.
Train: 2018-08-05T23:24:53.294807: step 1438, loss 0.630205.
Train: 2018-08-05T23:24:53.451020: step 1439, loss 0.620282.
Train: 2018-08-05T23:24:53.607234: step 1440, loss 0.579895.
Test: 2018-08-05T23:24:53.841585: step 1440, loss 0.549463.
Train: 2018-08-05T23:24:53.997767: step 1441, loss 0.554753.
Train: 2018-08-05T23:24:54.154010: step 1442, loss 0.521765.
Train: 2018-08-05T23:24:54.294603: step 1443, loss 0.621235.
Train: 2018-08-05T23:24:54.450815: step 1444, loss 0.530579.
Train: 2018-08-05T23:24:54.622654: step 1445, loss 0.604333.
Train: 2018-08-05T23:24:54.778833: step 1446, loss 0.546584.
Train: 2018-08-05T23:24:54.935074: step 1447, loss 0.57222.
Train: 2018-08-05T23:24:55.075668: step 1448, loss 0.572314.
Train: 2018-08-05T23:24:55.231882: step 1449, loss 0.546631.
Train: 2018-08-05T23:24:55.388095: step 1450, loss 0.538773.
Test: 2018-08-05T23:24:55.622418: step 1450, loss 0.549463.
Train: 2018-08-05T23:24:55.794248: step 1451, loss 0.620781.
Train: 2018-08-05T23:24:55.950433: step 1452, loss 0.588329.
Train: 2018-08-05T23:24:56.110261: step 1453, loss 0.587752.
Train: 2018-08-05T23:24:56.266498: step 1454, loss 0.562491.
Train: 2018-08-05T23:24:56.422718: step 1455, loss 0.571813.
Train: 2018-08-05T23:24:56.578900: step 1456, loss 0.547091.
Train: 2018-08-05T23:24:56.719492: step 1457, loss 0.547504.
Train: 2018-08-05T23:24:56.875736: step 1458, loss 0.521641.
Train: 2018-08-05T23:24:57.031949: step 1459, loss 0.538354.
Train: 2018-08-05T23:24:57.188163: step 1460, loss 0.537962.
Test: 2018-08-05T23:24:57.438074: step 1460, loss 0.549233.
Train: 2018-08-05T23:24:57.594288: step 1461, loss 0.496133.
Train: 2018-08-05T23:24:57.750531: step 1462, loss 0.538071.
Train: 2018-08-05T23:24:57.891124: step 1463, loss 0.537967.
Train: 2018-08-05T23:24:58.047341: step 1464, loss 0.528861.
Train: 2018-08-05T23:24:58.203519: step 1465, loss 0.460703.
Train: 2018-08-05T23:24:58.359763: step 1466, loss 0.520554.
Train: 2018-08-05T23:24:58.515944: step 1467, loss 0.563453.
Train: 2018-08-05T23:24:58.687779: step 1468, loss 0.554384.
Train: 2018-08-05T23:24:58.828402: step 1469, loss 0.527371.
Train: 2018-08-05T23:24:58.984621: step 1470, loss 0.545691.
Test: 2018-08-05T23:24:59.234948: step 1470, loss 0.548286.
Train: 2018-08-05T23:24:59.375542: step 1471, loss 0.555322.
Train: 2018-08-05T23:24:59.531752: step 1472, loss 0.562926.
Train: 2018-08-05T23:24:59.687960: step 1473, loss 0.572864.
Train: 2018-08-05T23:24:59.844149: step 1474, loss 0.563025.
Train: 2018-08-05T23:25:00.000391: step 1475, loss 0.581775.
Train: 2018-08-05T23:25:00.172196: step 1476, loss 0.517949.
Train: 2018-08-05T23:25:00.328440: step 1477, loss 0.517577.
Train: 2018-08-05T23:25:00.484647: step 1478, loss 0.517585.
Train: 2018-08-05T23:25:00.640844: step 1479, loss 0.536248.
Train: 2018-08-05T23:25:00.797049: step 1480, loss 0.579914.
Test: 2018-08-05T23:25:01.031399: step 1480, loss 0.548184.
Train: 2018-08-05T23:25:01.198428: step 1481, loss 0.571682.
Train: 2018-08-05T23:25:01.354611: step 1482, loss 0.582789.
Train: 2018-08-05T23:25:01.510824: step 1483, loss 0.610276.
Train: 2018-08-05T23:25:01.667037: step 1484, loss 0.563668.
Train: 2018-08-05T23:25:01.823251: step 1485, loss 0.535435.
Train: 2018-08-05T23:25:01.979464: step 1486, loss 0.545004.
Train: 2018-08-05T23:25:02.135677: step 1487, loss 0.600544.
Train: 2018-08-05T23:25:02.291890: step 1488, loss 0.573543.
Train: 2018-08-05T23:25:02.432482: step 1489, loss 0.582554.
Train: 2018-08-05T23:25:02.588695: step 1490, loss 0.563152.
Test: 2018-08-05T23:25:02.838666: step 1490, loss 0.548167.
Train: 2018-08-05T23:25:02.994875: step 1491, loss 0.508621.
Train: 2018-08-05T23:25:03.151094: step 1492, loss 0.580753.
Train: 2018-08-05T23:25:03.307301: step 1493, loss 0.591229.
Train: 2018-08-05T23:25:03.463520: step 1494, loss 0.527193.
Train: 2018-08-05T23:25:03.619702: step 1495, loss 0.545143.
Train: 2018-08-05T23:25:03.775916: step 1496, loss 0.57168.
Train: 2018-08-05T23:25:03.932130: step 1497, loss 0.634209.
Train: 2018-08-05T23:25:04.088342: step 1498, loss 0.526781.
Train: 2018-08-05T23:25:04.228965: step 1499, loss 0.554714.
Train: 2018-08-05T23:25:04.385149: step 1500, loss 0.607801.
Test: 2018-08-05T23:25:04.635090: step 1500, loss 0.548271.
Train: 2018-08-05T23:25:05.384943: step 1501, loss 0.536694.
Train: 2018-08-05T23:25:05.556777: step 1502, loss 0.589391.
Train: 2018-08-05T23:25:05.712990: step 1503, loss 0.668812.
Train: 2018-08-05T23:25:05.869204: step 1504, loss 0.624153.
Train: 2018-08-05T23:25:06.025417: step 1505, loss 0.544814.
Train: 2018-08-05T23:25:06.181624: step 1506, loss 0.528206.
Train: 2018-08-05T23:25:06.337847: step 1507, loss 0.55342.
Train: 2018-08-05T23:25:06.478436: step 1508, loss 0.588818.
Train: 2018-08-05T23:25:06.634650: step 1509, loss 0.629647.
Train: 2018-08-05T23:25:06.790832: step 1510, loss 0.599826.
Test: 2018-08-05T23:25:07.031794: step 1510, loss 0.54885.
Train: 2018-08-05T23:25:07.188034: step 1511, loss 0.587306.
Train: 2018-08-05T23:25:07.344256: step 1512, loss 0.529195.
Train: 2018-08-05T23:25:07.500463: step 1513, loss 0.56252.
Train: 2018-08-05T23:25:07.641027: step 1514, loss 0.512486.
Train: 2018-08-05T23:25:07.797270: step 1515, loss 0.546791.
Train: 2018-08-05T23:25:07.969073: step 1516, loss 0.571615.
Train: 2018-08-05T23:25:08.109696: step 1517, loss 0.579761.
Train: 2018-08-05T23:25:08.281500: step 1518, loss 0.645732.
Train: 2018-08-05T23:25:08.437713: step 1519, loss 0.537903.
Train: 2018-08-05T23:25:08.593926: step 1520, loss 0.604084.
Test: 2018-08-05T23:25:08.828277: step 1520, loss 0.549373.
Train: 2018-08-05T23:25:08.984490: step 1521, loss 0.595766.
Train: 2018-08-05T23:25:09.140706: step 1522, loss 0.538923.
Train: 2018-08-05T23:25:09.296917: step 1523, loss 0.514485.
Train: 2018-08-05T23:25:09.453129: step 1524, loss 0.514716.
Train: 2018-08-05T23:25:09.609337: step 1525, loss 0.514675.
Train: 2018-08-05T23:25:09.765556: step 1526, loss 0.564012.
Train: 2018-08-05T23:25:09.921770: step 1527, loss 0.612193.
Train: 2018-08-05T23:25:10.077983: step 1528, loss 0.521599.
Train: 2018-08-05T23:25:10.234167: step 1529, loss 0.578918.
Train: 2018-08-05T23:25:10.390378: step 1530, loss 0.529513.
Test: 2018-08-05T23:25:10.624699: step 1530, loss 0.549008.
Train: 2018-08-05T23:25:10.780913: step 1531, loss 0.570955.
Train: 2018-08-05T23:25:10.952746: step 1532, loss 0.58825.
Train: 2018-08-05T23:25:11.108990: step 1533, loss 0.61341.
Train: 2018-08-05T23:25:11.265202: step 1534, loss 0.537601.
Train: 2018-08-05T23:25:11.405798: step 1535, loss 0.596849.
Train: 2018-08-05T23:25:11.562008: step 1536, loss 0.537959.
Train: 2018-08-05T23:25:11.718217: step 1537, loss 0.587489.
Train: 2018-08-05T23:25:11.869840: step 1538, loss 0.588884.
Train: 2018-08-05T23:25:12.026022: step 1539, loss 0.545778.
Train: 2018-08-05T23:25:12.182236: step 1540, loss 0.605905.
Test: 2018-08-05T23:25:12.416587: step 1540, loss 0.548803.
Train: 2018-08-05T23:25:12.572799: step 1541, loss 0.571868.
Train: 2018-08-05T23:25:12.729014: step 1542, loss 0.537411.
Train: 2018-08-05T23:25:12.885226: step 1543, loss 0.545637.
Train: 2018-08-05T23:25:13.041439: step 1544, loss 0.529991.
Train: 2018-08-05T23:25:13.197623: step 1545, loss 0.554261.
Train: 2018-08-05T23:25:13.353859: step 1546, loss 0.587646.
Train: 2018-08-05T23:25:13.494458: step 1547, loss 0.562416.
Train: 2018-08-05T23:25:13.650641: step 1548, loss 0.571585.
Train: 2018-08-05T23:25:13.806884: step 1549, loss 0.630999.
Train: 2018-08-05T23:25:13.963068: step 1550, loss 0.504392.
Test: 2018-08-05T23:25:14.197387: step 1550, loss 0.548691.
Train: 2018-08-05T23:25:14.353600: step 1551, loss 0.554975.
Train: 2018-08-05T23:25:14.525435: step 1552, loss 0.529286.
Train: 2018-08-05T23:25:14.681649: step 1553, loss 0.503496.
Train: 2018-08-05T23:25:14.837861: step 1554, loss 0.520005.
Train: 2018-08-05T23:25:14.978454: step 1555, loss 0.562802.
Train: 2018-08-05T23:25:15.150318: step 1556, loss 0.49449.
Train: 2018-08-05T23:25:15.290910: step 1557, loss 0.554302.
Train: 2018-08-05T23:25:15.447093: step 1558, loss 0.536717.
Train: 2018-08-05T23:25:15.603337: step 1559, loss 0.580771.
Train: 2018-08-05T23:25:15.759550: step 1560, loss 0.597144.
Test: 2018-08-05T23:25:15.993840: step 1560, loss 0.548277.
Train: 2018-08-05T23:25:16.165675: step 1561, loss 0.580279.
Train: 2018-08-05T23:25:16.306296: step 1562, loss 0.580806.
Train: 2018-08-05T23:25:16.478132: step 1563, loss 0.536553.
Train: 2018-08-05T23:25:16.634343: step 1564, loss 0.537395.
Train: 2018-08-05T23:25:16.774935: step 1565, loss 0.579752.
Train: 2018-08-05T23:25:16.946741: step 1566, loss 0.606852.
Train: 2018-08-05T23:25:17.102953: step 1567, loss 0.590857.
Train: 2018-08-05T23:25:17.259166: step 1568, loss 0.493202.
Train: 2018-08-05T23:25:17.415380: step 1569, loss 0.517353.
Train: 2018-08-05T23:25:17.571593: step 1570, loss 0.519019.
Test: 2018-08-05T23:25:17.805915: step 1570, loss 0.54821.
Train: 2018-08-05T23:25:17.962156: step 1571, loss 0.545769.
Train: 2018-08-05T23:25:18.118370: step 1572, loss 0.625018.
Train: 2018-08-05T23:25:18.274583: step 1573, loss 0.510077.
Train: 2018-08-05T23:25:18.430796: step 1574, loss 0.634154.
Train: 2018-08-05T23:25:18.587003: step 1575, loss 0.563434.
Train: 2018-08-05T23:25:18.743192: step 1576, loss 0.580848.
Train: 2018-08-05T23:25:18.899430: step 1577, loss 0.607506.
Train: 2018-08-05T23:25:19.055650: step 1578, loss 0.457095.
Train: 2018-08-05T23:25:19.227484: step 1579, loss 0.580914.
Train: 2018-08-05T23:25:19.383666: step 1580, loss 0.571253.
Test: 2018-08-05T23:25:19.633639: step 1580, loss 0.548233.
Train: 2018-08-05T23:25:19.789853: step 1581, loss 0.546133.
Train: 2018-08-05T23:25:19.946034: step 1582, loss 0.51063.
Train: 2018-08-05T23:25:20.102248: step 1583, loss 0.572139.
Train: 2018-08-05T23:25:20.258460: step 1584, loss 0.561923.
Train: 2018-08-05T23:25:20.414674: step 1585, loss 0.562579.
Train: 2018-08-05T23:25:20.570887: step 1586, loss 0.642217.
Train: 2018-08-05T23:25:20.727132: step 1587, loss 0.545053.
Train: 2018-08-05T23:25:20.883313: step 1588, loss 0.55426.
Train: 2018-08-05T23:25:21.039527: step 1589, loss 0.544587.
Train: 2018-08-05T23:25:21.195740: step 1590, loss 0.528017.
Test: 2018-08-05T23:25:21.430062: step 1590, loss 0.548281.
Train: 2018-08-05T23:25:21.586275: step 1591, loss 0.588814.
Train: 2018-08-05T23:25:21.742517: step 1592, loss 0.588768.
Train: 2018-08-05T23:25:21.914321: step 1593, loss 0.519455.
Train: 2018-08-05T23:25:22.070564: step 1594, loss 0.545828.
Train: 2018-08-05T23:25:22.211127: step 1595, loss 0.554187.
Train: 2018-08-05T23:25:22.382962: step 1596, loss 0.642345.
Train: 2018-08-05T23:25:22.539204: step 1597, loss 0.562638.
Train: 2018-08-05T23:25:22.695388: step 1598, loss 0.624039.
Train: 2018-08-05T23:25:22.851637: step 1599, loss 0.64192.
Train: 2018-08-05T23:25:23.007839: step 1600, loss 0.485203.
Test: 2018-08-05T23:25:23.242166: step 1600, loss 0.548478.
Train: 2018-08-05T23:25:23.991982: step 1601, loss 0.536817.
Train: 2018-08-05T23:25:24.165738: step 1602, loss 0.502903.
Train: 2018-08-05T23:25:24.312957: step 1603, loss 0.639029.
Train: 2018-08-05T23:25:24.484762: step 1604, loss 0.537508.
Train: 2018-08-05T23:25:24.640975: step 1605, loss 0.580548.
Train: 2018-08-05T23:25:24.797188: step 1606, loss 0.53797.
Train: 2018-08-05T23:25:24.953402: step 1607, loss 0.528798.
Train: 2018-08-05T23:25:25.109615: step 1608, loss 0.469149.
Train: 2018-08-05T23:25:25.265829: step 1609, loss 0.486419.
Train: 2018-08-05T23:25:25.422042: step 1610, loss 0.562326.
Test: 2018-08-05T23:25:25.671992: step 1610, loss 0.548419.
Train: 2018-08-05T23:25:25.828197: step 1611, loss 0.544748.
Train: 2018-08-05T23:25:25.984410: step 1612, loss 0.606566.
Train: 2018-08-05T23:25:26.140622: step 1613, loss 0.519417.
Train: 2018-08-05T23:25:26.296867: step 1614, loss 0.571395.
Train: 2018-08-05T23:25:26.453050: step 1615, loss 0.579421.
Train: 2018-08-05T23:25:26.609293: step 1616, loss 0.493312.
Train: 2018-08-05T23:25:26.765506: step 1617, loss 0.536843.
Train: 2018-08-05T23:25:26.921719: step 1618, loss 0.616073.
Train: 2018-08-05T23:25:27.109180: step 1619, loss 0.606974.
Train: 2018-08-05T23:25:27.249736: step 1620, loss 0.528114.
Test: 2018-08-05T23:25:27.484088: step 1620, loss 0.548177.
Train: 2018-08-05T23:25:27.640294: step 1621, loss 0.500871.
Train: 2018-08-05T23:25:27.796514: step 1622, loss 0.571966.
Train: 2018-08-05T23:25:27.952696: step 1623, loss 0.606787.
Train: 2018-08-05T23:25:28.112410: step 1624, loss 0.520026.
Train: 2018-08-05T23:25:28.268624: step 1625, loss 0.599322.
Train: 2018-08-05T23:25:28.424807: step 1626, loss 0.465882.
Train: 2018-08-05T23:25:28.581019: step 1627, loss 0.527286.
Train: 2018-08-05T23:25:28.737232: step 1628, loss 0.598581.
Train: 2018-08-05T23:25:28.909067: step 1629, loss 0.545012.
Train: 2018-08-05T23:25:29.065280: step 1630, loss 0.518572.
Test: 2018-08-05T23:25:29.299602: step 1630, loss 0.548092.
Train: 2018-08-05T23:25:29.455814: step 1631, loss 0.606923.
Train: 2018-08-05T23:25:29.612027: step 1632, loss 0.527683.
Train: 2018-08-05T23:25:29.768240: step 1633, loss 0.572715.
Train: 2018-08-05T23:25:29.924453: step 1634, loss 0.552549.
Train: 2018-08-05T23:25:30.080666: step 1635, loss 0.572395.
Train: 2018-08-05T23:25:30.236881: step 1636, loss 0.536288.
Train: 2018-08-05T23:25:30.393123: step 1637, loss 0.626207.
Train: 2018-08-05T23:25:30.550270: step 1638, loss 0.526516.
Train: 2018-08-05T23:25:30.706513: step 1639, loss 0.644277.
Train: 2018-08-05T23:25:30.862696: step 1640, loss 0.572498.
Test: 2018-08-05T23:25:31.097017: step 1640, loss 0.548116.
Train: 2018-08-05T23:25:31.253230: step 1641, loss 0.588639.
Train: 2018-08-05T23:25:31.409473: step 1642, loss 0.528089.
Train: 2018-08-05T23:25:31.550065: step 1643, loss 0.598864.
Train: 2018-08-05T23:25:31.706278: step 1644, loss 0.545861.
Train: 2018-08-05T23:25:31.862492: step 1645, loss 0.562636.
Train: 2018-08-05T23:25:32.018704: step 1646, loss 0.59726.
Train: 2018-08-05T23:25:32.174918: step 1647, loss 0.589441.
Train: 2018-08-05T23:25:32.331101: step 1648, loss 0.562169.
Train: 2018-08-05T23:25:32.471722: step 1649, loss 0.632032.
Train: 2018-08-05T23:25:32.640491: step 1650, loss 0.581386.
Test: 2018-08-05T23:25:32.874843: step 1650, loss 0.548463.
Train: 2018-08-05T23:25:33.031050: step 1651, loss 0.614646.
Train: 2018-08-05T23:25:33.187268: step 1652, loss 0.555208.
Train: 2018-08-05T23:25:33.343481: step 1653, loss 0.613064.
Train: 2018-08-05T23:25:33.499665: step 1654, loss 0.545925.
Train: 2018-08-05T23:25:33.655907: step 1655, loss 0.504559.
Train: 2018-08-05T23:25:33.812124: step 1656, loss 0.55455.
Train: 2018-08-05T23:25:33.968333: step 1657, loss 0.596639.
Train: 2018-08-05T23:25:34.124548: step 1658, loss 0.570811.
Train: 2018-08-05T23:25:34.280760: step 1659, loss 0.637799.
Train: 2018-08-05T23:25:34.436975: step 1660, loss 0.62098.
Test: 2018-08-05T23:25:34.671265: step 1660, loss 0.549192.
Train: 2018-08-05T23:25:34.811885: step 1661, loss 0.580678.
Train: 2018-08-05T23:25:34.968069: step 1662, loss 0.661571.
Train: 2018-08-05T23:25:35.124313: step 1663, loss 0.603555.
Train: 2018-08-05T23:25:35.280525: step 1664, loss 0.530766.
Train: 2018-08-05T23:25:35.421118: step 1665, loss 0.530891.
Train: 2018-08-05T23:25:35.577301: step 1666, loss 0.619652.
Train: 2018-08-05T23:25:35.749135: step 1667, loss 0.475609.
Train: 2018-08-05T23:25:35.905378: step 1668, loss 0.587483.
Train: 2018-08-05T23:25:36.045971: step 1669, loss 0.547415.
Train: 2018-08-05T23:25:36.202183: step 1670, loss 0.579522.
Test: 2018-08-05T23:25:36.436474: step 1670, loss 0.55002.
Train: 2018-08-05T23:25:36.592710: step 1671, loss 0.587709.
Train: 2018-08-05T23:25:36.748931: step 1672, loss 0.611225.
Train: 2018-08-05T23:25:36.905143: step 1673, loss 0.547872.
Train: 2018-08-05T23:25:37.061356: step 1674, loss 0.555323.
Train: 2018-08-05T23:25:37.217570: step 1675, loss 0.468017.
Train: 2018-08-05T23:25:37.373789: step 1676, loss 0.571261.
Train: 2018-08-05T23:25:37.529996: step 1677, loss 0.595247.
Train: 2018-08-05T23:25:37.670588: step 1678, loss 0.579633.
Train: 2018-08-05T23:25:37.826772: step 1679, loss 0.555492.
Train: 2018-08-05T23:25:37.982986: step 1680, loss 0.563035.
Test: 2018-08-05T23:25:38.222213: step 1680, loss 0.549591.
Train: 2018-08-05T23:25:38.378451: step 1681, loss 0.514568.
Train: 2018-08-05T23:25:38.534670: step 1682, loss 0.571491.
Train: 2018-08-05T23:25:38.690882: step 1683, loss 0.588051.
Train: 2018-08-05T23:25:38.862686: step 1684, loss 0.538385.
Train: 2018-08-05T23:25:39.003278: step 1685, loss 0.530092.
Train: 2018-08-05T23:25:39.159492: step 1686, loss 0.571063.
Train: 2018-08-05T23:25:39.315735: step 1687, loss 0.645855.
Train: 2018-08-05T23:25:39.471949: step 1688, loss 0.554847.
Train: 2018-08-05T23:25:39.628132: step 1689, loss 0.546521.
Train: 2018-08-05T23:25:39.784345: step 1690, loss 0.588429.
Test: 2018-08-05T23:25:40.034318: step 1690, loss 0.548918.
Train: 2018-08-05T23:25:40.190524: step 1691, loss 0.546182.
Train: 2018-08-05T23:25:40.346737: step 1692, loss 0.504047.
Train: 2018-08-05T23:25:40.502944: step 1693, loss 0.479069.
Train: 2018-08-05T23:25:40.659140: step 1694, loss 0.512312.
Train: 2018-08-05T23:25:40.815353: step 1695, loss 0.639434.
Train: 2018-08-05T23:25:40.971566: step 1696, loss 0.579775.
Train: 2018-08-05T23:25:41.127810: step 1697, loss 0.519557.
Train: 2018-08-05T23:25:41.284026: step 1698, loss 0.416959.
Train: 2018-08-05T23:25:41.424614: step 1699, loss 0.580036.
Train: 2018-08-05T23:25:41.596443: step 1700, loss 0.649641.
Test: 2018-08-05T23:25:41.830772: step 1700, loss 0.548261.
Train: 2018-08-05T23:25:42.564941: step 1701, loss 0.510729.
Train: 2018-08-05T23:25:42.721154: step 1702, loss 0.545428.
Train: 2018-08-05T23:25:42.877397: step 1703, loss 0.545302.
Train: 2018-08-05T23:25:43.033612: step 1704, loss 0.589284.
Train: 2018-08-05T23:25:43.189824: step 1705, loss 0.563375.
Train: 2018-08-05T23:25:43.346040: step 1706, loss 0.597631.
Train: 2018-08-05T23:25:43.502252: step 1707, loss 0.598611.
Train: 2018-08-05T23:25:43.658464: step 1708, loss 0.615925.
Train: 2018-08-05T23:25:43.814671: step 1709, loss 0.571911.
Train: 2018-08-05T23:25:43.970861: step 1710, loss 0.537111.
Test: 2018-08-05T23:25:44.205181: step 1710, loss 0.548136.
Train: 2018-08-05T23:25:44.361423: step 1711, loss 0.572.
Train: 2018-08-05T23:25:44.517637: step 1712, loss 0.553439.
Train: 2018-08-05T23:25:44.673852: step 1713, loss 0.614978.
Train: 2018-08-05T23:25:44.830063: step 1714, loss 0.518964.
Train: 2018-08-05T23:25:44.986271: step 1715, loss 0.53635.
Train: 2018-08-05T23:25:45.126869: step 1716, loss 0.562414.
Train: 2018-08-05T23:25:45.283082: step 1717, loss 0.553801.
Train: 2018-08-05T23:25:45.439295: step 1718, loss 0.502032.
Train: 2018-08-05T23:25:45.611133: step 1719, loss 0.580495.
Train: 2018-08-05T23:25:45.767312: step 1720, loss 0.597946.
Test: 2018-08-05T23:25:46.001633: step 1720, loss 0.548192.
Train: 2018-08-05T23:25:46.157876: step 1721, loss 0.536222.
Train: 2018-08-05T23:25:46.314090: step 1722, loss 0.562788.
Train: 2018-08-05T23:25:46.470305: step 1723, loss 0.606859.
Train: 2018-08-05T23:25:46.626516: step 1724, loss 0.527551.
Train: 2018-08-05T23:25:46.767079: step 1725, loss 0.493491.
Train: 2018-08-05T23:25:46.923320: step 1726, loss 0.633051.
Train: 2018-08-05T23:25:47.079534: step 1727, loss 0.527741.
Train: 2018-08-05T23:25:47.235747: step 1728, loss 0.589157.
Train: 2018-08-05T23:25:47.391960: step 1729, loss 0.492934.
Train: 2018-08-05T23:25:47.548173: step 1730, loss 0.598041.
Test: 2018-08-05T23:25:47.782495: step 1730, loss 0.548195.
Train: 2018-08-05T23:25:47.938706: step 1731, loss 0.564117.
Train: 2018-08-05T23:25:48.110511: step 1732, loss 0.614713.
Train: 2018-08-05T23:25:48.266724: step 1733, loss 0.631812.
Train: 2018-08-05T23:25:48.422938: step 1734, loss 0.527793.
Train: 2018-08-05T23:25:48.579151: step 1735, loss 0.632306.
Train: 2018-08-05T23:25:48.735364: step 1736, loss 0.510927.
Train: 2018-08-05T23:25:48.891577: step 1737, loss 0.58847.
Train: 2018-08-05T23:25:49.047790: step 1738, loss 0.554412.
Train: 2018-08-05T23:25:49.188383: step 1739, loss 0.553712.
Train: 2018-08-05T23:25:49.360218: step 1740, loss 0.588347.
Test: 2018-08-05T23:25:49.594566: step 1740, loss 0.548437.
Train: 2018-08-05T23:25:49.750781: step 1741, loss 0.605775.
Train: 2018-08-05T23:25:49.906994: step 1742, loss 0.502866.
Train: 2018-08-05T23:25:50.078829: step 1743, loss 0.571402.
Train: 2018-08-05T23:25:50.219391: step 1744, loss 0.596795.
Train: 2018-08-05T23:25:50.375635: step 1745, loss 0.647571.
Train: 2018-08-05T23:25:50.531846: step 1746, loss 0.554206.
Train: 2018-08-05T23:25:50.688059: step 1747, loss 0.528744.
Train: 2018-08-05T23:25:50.844273: step 1748, loss 0.561978.
Train: 2018-08-05T23:25:51.000490: step 1749, loss 0.655481.
Train: 2018-08-05T23:25:51.156670: step 1750, loss 0.689167.
Test: 2018-08-05T23:25:51.390990: step 1750, loss 0.548865.
Train: 2018-08-05T23:25:51.547236: step 1751, loss 0.462837.
Train: 2018-08-05T23:25:51.703446: step 1752, loss 0.60435.
Train: 2018-08-05T23:25:51.859653: step 1753, loss 0.53002.
Train: 2018-08-05T23:25:52.015873: step 1754, loss 0.554354.
Train: 2018-08-05T23:25:52.172086: step 1755, loss 0.554415.
Train: 2018-08-05T23:25:52.328299: step 1756, loss 0.513314.
Train: 2018-08-05T23:25:52.468891: step 1757, loss 0.603629.
Train: 2018-08-05T23:25:52.625075: step 1758, loss 0.513041.
Train: 2018-08-05T23:25:52.781317: step 1759, loss 0.538066.
Train: 2018-08-05T23:25:52.937531: step 1760, loss 0.578869.
Test: 2018-08-05T23:25:53.171851: step 1760, loss 0.548966.
Train: 2018-08-05T23:25:53.328034: step 1761, loss 0.546481.
Train: 2018-08-05T23:25:53.484277: step 1762, loss 0.488227.
Train: 2018-08-05T23:25:53.640494: step 1763, loss 0.487945.
Train: 2018-08-05T23:25:53.812296: step 1764, loss 0.629712.
Train: 2018-08-05T23:25:53.968508: step 1765, loss 0.579516.
Train: 2018-08-05T23:25:54.124721: step 1766, loss 0.562531.
Train: 2018-08-05T23:25:54.280934: step 1767, loss 0.537337.
Train: 2018-08-05T23:25:54.438221: step 1768, loss 0.571216.
Train: 2018-08-05T23:25:54.578813: step 1769, loss 0.588083.
Train: 2018-08-05T23:25:54.750681: step 1770, loss 0.528085.
Test: 2018-08-05T23:25:54.984999: step 1770, loss 0.548426.
Train: 2018-08-05T23:25:55.141181: step 1771, loss 0.554435.
Train: 2018-08-05T23:25:55.297394: step 1772, loss 0.57102.
Train: 2018-08-05T23:25:55.465368: step 1773, loss 0.632496.
Train: 2018-08-05T23:25:55.627190: step 1774, loss 0.597347.
Train: 2018-08-05T23:25:55.767782: step 1775, loss 0.493623.
Train: 2018-08-05T23:25:55.930264: step 1776, loss 0.5111.
Train: 2018-08-05T23:25:56.086478: step 1777, loss 0.58862.
Train: 2018-08-05T23:25:56.242723: step 1778, loss 0.579004.
Train: 2018-08-05T23:25:56.398904: step 1779, loss 0.579838.
Train: 2018-08-05T23:25:56.570764: step 1780, loss 0.502674.
Test: 2018-08-05T23:25:56.805090: step 1780, loss 0.548271.
Train: 2018-08-05T23:25:56.961273: step 1781, loss 0.632237.
Train: 2018-08-05T23:25:57.117486: step 1782, loss 0.519929.
Train: 2018-08-05T23:25:57.273699: step 1783, loss 0.62281.
Train: 2018-08-05T23:25:57.429942: step 1784, loss 0.588837.
Train: 2018-08-05T23:25:57.570534: step 1785, loss 0.528444.
Train: 2018-08-05T23:25:57.726718: step 1786, loss 0.571476.
Train: 2018-08-05T23:25:57.882960: step 1787, loss 0.49456.
Train: 2018-08-05T23:25:58.039174: step 1788, loss 0.527461.
Train: 2018-08-05T23:25:58.195386: step 1789, loss 0.554489.
Train: 2018-08-05T23:25:58.351595: step 1790, loss 0.588917.
Test: 2018-08-05T23:25:58.585890: step 1790, loss 0.548248.
Train: 2018-08-05T23:25:58.742135: step 1791, loss 0.536768.
Train: 2018-08-05T23:25:58.898346: step 1792, loss 0.563571.
Train: 2018-08-05T23:25:59.054560: step 1793, loss 0.597728.
Train: 2018-08-05T23:25:59.210773: step 1794, loss 0.623519.
Train: 2018-08-05T23:25:59.366957: step 1795, loss 0.555161.
Train: 2018-08-05T23:25:59.523199: step 1796, loss 0.57115.
Train: 2018-08-05T23:25:59.679408: step 1797, loss 0.554285.
Train: 2018-08-05T23:25:59.835626: step 1798, loss 0.545885.
Train: 2018-08-05T23:25:59.993953: step 1799, loss 0.536894.
Train: 2018-08-05T23:26:00.143230: step 1800, loss 0.62264.
Test: 2018-08-05T23:26:00.377578: step 1800, loss 0.548338.
Train: 2018-08-05T23:26:01.064918: step 1801, loss 0.57201.
Train: 2018-08-05T23:26:01.236752: step 1802, loss 0.58825.
Train: 2018-08-05T23:26:01.392935: step 1803, loss 0.494122.
Train: 2018-08-05T23:26:01.549179: step 1804, loss 0.511374.
Train: 2018-08-05T23:26:01.705392: step 1805, loss 0.494523.
Train: 2018-08-05T23:26:01.865748: step 1806, loss 0.545744.
Train: 2018-08-05T23:26:02.021998: step 1807, loss 0.640891.
Train: 2018-08-05T23:26:02.178213: step 1808, loss 0.519286.
Train: 2018-08-05T23:26:02.334386: step 1809, loss 0.553658.
Train: 2018-08-05T23:26:02.490629: step 1810, loss 0.519909.
Test: 2018-08-05T23:26:02.724921: step 1810, loss 0.548211.
Train: 2018-08-05T23:26:02.881157: step 1811, loss 0.562956.
Train: 2018-08-05T23:26:03.021725: step 1812, loss 0.563086.
Train: 2018-08-05T23:26:03.177972: step 1813, loss 0.588938.
Train: 2018-08-05T23:26:03.334181: step 1814, loss 0.475828.
Train: 2018-08-05T23:26:03.490365: step 1815, loss 0.474514.
Train: 2018-08-05T23:26:03.646577: step 1816, loss 0.571546.
Train: 2018-08-05T23:26:03.802791: step 1817, loss 0.563535.
Train: 2018-08-05T23:26:03.946979: step 1818, loss 0.500837.
Train: 2018-08-05T23:26:04.103161: step 1819, loss 0.588957.
Train: 2018-08-05T23:26:04.268102: step 1820, loss 0.526318.
Test: 2018-08-05T23:26:04.502421: step 1820, loss 0.547956.
Train: 2018-08-05T23:26:04.658634: step 1821, loss 0.571236.
Train: 2018-08-05T23:26:04.814878: step 1822, loss 0.571735.
Train: 2018-08-05T23:26:04.971091: step 1823, loss 0.553564.
Train: 2018-08-05T23:26:05.127305: step 1824, loss 0.562951.
Train: 2018-08-05T23:26:05.283518: step 1825, loss 0.635946.
Train: 2018-08-05T23:26:05.439701: step 1826, loss 0.599022.
Train: 2018-08-05T23:26:05.595914: step 1827, loss 0.562457.
Train: 2018-08-05T23:26:05.752128: step 1828, loss 0.545202.
Train: 2018-08-05T23:26:05.923986: step 1829, loss 0.598715.
Train: 2018-08-05T23:26:06.080199: step 1830, loss 0.526807.
Test: 2018-08-05T23:26:06.314526: step 1830, loss 0.547964.
Train: 2018-08-05T23:26:06.470739: step 1831, loss 0.58137.
Train: 2018-08-05T23:26:06.626951: step 1832, loss 0.607086.
Train: 2018-08-05T23:26:06.767543: step 1833, loss 0.510126.
Train: 2018-08-05T23:26:06.923727: step 1834, loss 0.598611.
Train: 2018-08-05T23:26:07.095561: step 1835, loss 0.580112.
Train: 2018-08-05T23:26:07.251774: step 1836, loss 0.554246.
Train: 2018-08-05T23:26:07.407987: step 1837, loss 0.571382.
Train: 2018-08-05T23:26:07.564200: step 1838, loss 0.597692.
Train: 2018-08-05T23:26:07.720413: step 1839, loss 0.5633.
Train: 2018-08-05T23:26:07.876627: step 1840, loss 0.641989.
Test: 2018-08-05T23:26:08.110978: step 1840, loss 0.548241.
Train: 2018-08-05T23:26:08.267161: step 1841, loss 0.511053.
Train: 2018-08-05T23:26:08.438995: step 1842, loss 0.536603.
Train: 2018-08-05T23:26:08.595238: step 1843, loss 0.562468.
Train: 2018-08-05T23:26:08.751451: step 1844, loss 0.587871.
Train: 2018-08-05T23:26:08.923256: step 1845, loss 0.563077.
Train: 2018-08-05T23:26:09.079469: step 1846, loss 0.571002.
Train: 2018-08-05T23:26:09.235713: step 1847, loss 0.56242.
Train: 2018-08-05T23:26:09.391926: step 1848, loss 0.596657.
Train: 2018-08-05T23:26:09.552299: step 1849, loss 0.563559.
Train: 2018-08-05T23:26:09.708542: step 1850, loss 0.486813.
Test: 2018-08-05T23:26:09.942863: step 1850, loss 0.548585.
Train: 2018-08-05T23:26:10.114667: step 1851, loss 0.588214.
Train: 2018-08-05T23:26:10.270879: step 1852, loss 0.571318.
Train: 2018-08-05T23:26:10.411505: step 1853, loss 0.503922.
Train: 2018-08-05T23:26:10.583337: step 1854, loss 0.570676.
Train: 2018-08-05T23:26:10.739550: step 1855, loss 0.554315.
Train: 2018-08-05T23:26:10.895766: step 1856, loss 0.613584.
Train: 2018-08-05T23:26:11.051979: step 1857, loss 0.596051.
Train: 2018-08-05T23:26:11.208190: step 1858, loss 0.57122.
Train: 2018-08-05T23:26:11.364374: step 1859, loss 0.596407.
Train: 2018-08-05T23:26:11.520619: step 1860, loss 0.571428.
Test: 2018-08-05T23:26:11.770527: step 1860, loss 0.548668.
Train: 2018-08-05T23:26:11.989259: step 1861, loss 0.587981.
Train: 2018-08-05T23:26:12.161062: step 1862, loss 0.562332.
Train: 2018-08-05T23:26:12.317274: step 1863, loss 0.571069.
Train: 2018-08-05T23:26:12.489133: step 1864, loss 0.529525.
Train: 2018-08-05T23:26:12.645347: step 1865, loss 0.612793.
Train: 2018-08-05T23:26:12.817157: step 1866, loss 0.537984.
Train: 2018-08-05T23:26:12.973401: step 1867, loss 0.496097.
Train: 2018-08-05T23:26:13.145206: step 1868, loss 0.537044.
Train: 2018-08-05T23:26:13.317040: step 1869, loss 0.57083.
Train: 2018-08-05T23:26:13.520117: step 1870, loss 0.537672.
Test: 2018-08-05T23:26:13.754435: step 1870, loss 0.548687.
Train: 2018-08-05T23:26:13.910649: step 1871, loss 0.588353.
Train: 2018-08-05T23:26:14.066862: step 1872, loss 0.537485.
Train: 2018-08-05T23:26:14.238729: step 1873, loss 0.571551.
Train: 2018-08-05T23:26:14.394934: step 1874, loss 0.537618.
Train: 2018-08-05T23:26:14.551129: step 1875, loss 0.554374.
Train: 2018-08-05T23:26:14.707337: step 1876, loss 0.536872.
Train: 2018-08-05T23:26:14.863580: step 1877, loss 0.511507.
Train: 2018-08-05T23:26:15.051007: step 1878, loss 0.554615.
Train: 2018-08-05T23:26:15.207249: step 1879, loss 0.502394.
Train: 2018-08-05T23:26:15.394675: step 1880, loss 0.537164.
Test: 2018-08-05T23:26:15.660237: step 1880, loss 0.548199.
Train: 2018-08-05T23:26:15.816451: step 1881, loss 0.580626.
Train: 2018-08-05T23:26:16.019530: step 1882, loss 0.571156.
Train: 2018-08-05T23:26:16.238230: step 1883, loss 0.509992.
Train: 2018-08-05T23:26:16.456927: step 1884, loss 0.536455.
Train: 2018-08-05T23:26:16.613139: step 1885, loss 0.527764.
Train: 2018-08-05T23:26:16.816216: step 1886, loss 0.492398.
Train: 2018-08-05T23:26:16.988050: step 1887, loss 0.580917.
Train: 2018-08-05T23:26:17.144263: step 1888, loss 0.535854.
Train: 2018-08-05T23:26:17.300476: step 1889, loss 0.589829.
Train: 2018-08-05T23:26:17.456714: step 1890, loss 0.563244.
Test: 2018-08-05T23:26:17.691041: step 1890, loss 0.547886.
Train: 2018-08-05T23:26:17.847222: step 1891, loss 0.482658.
Train: 2018-08-05T23:26:18.019058: step 1892, loss 0.580962.
Train: 2018-08-05T23:26:18.159650: step 1893, loss 0.608444.
Train: 2018-08-05T23:26:18.331484: step 1894, loss 0.490007.
Train: 2018-08-05T23:26:18.487698: step 1895, loss 0.672493.
Train: 2018-08-05T23:26:18.643943: step 1896, loss 0.51803.
Train: 2018-08-05T23:26:18.800154: step 1897, loss 0.517753.
Train: 2018-08-05T23:26:18.956338: step 1898, loss 0.526241.
Train: 2018-08-05T23:26:19.112580: step 1899, loss 0.563411.
Train: 2018-08-05T23:26:19.268796: step 1900, loss 0.626383.
Test: 2018-08-05T23:26:19.503116: step 1900, loss 0.547863.
Train: 2018-08-05T23:26:20.284181: step 1901, loss 0.526767.
Train: 2018-08-05T23:26:20.440387: step 1902, loss 0.644891.
Train: 2018-08-05T23:26:20.596576: step 1903, loss 0.608621.
Train: 2018-08-05T23:26:20.833024: step 1904, loss 0.598786.
Train: 2018-08-05T23:26:20.987363: step 1905, loss 0.544913.
Train: 2018-08-05T23:26:21.164142: step 1906, loss 0.52676.
Train: 2018-08-05T23:26:21.350234: step 1907, loss 0.616186.
Train: 2018-08-05T23:26:21.515824: step 1908, loss 0.553647.
Train: 2018-08-05T23:26:21.678389: step 1909, loss 0.581004.
Train: 2018-08-05T23:26:21.855881: step 1910, loss 0.598623.
Test: 2018-08-05T23:26:22.102254: step 1910, loss 0.54802.
Train: 2018-08-05T23:26:22.274415: step 1911, loss 0.571724.
Train: 2018-08-05T23:26:22.466900: step 1912, loss 0.563136.
Train: 2018-08-05T23:26:22.633466: step 1913, loss 0.501482.
Train: 2018-08-05T23:26:22.790175: step 1914, loss 0.588677.
Train: 2018-08-05T23:26:22.946494: step 1915, loss 0.54551.
Train: 2018-08-05T23:26:23.113156: step 1916, loss 0.579906.
Train: 2018-08-05T23:26:23.288064: step 1917, loss 0.631585.
Train: 2018-08-05T23:26:23.454375: step 1918, loss 0.49385.
Train: 2018-08-05T23:26:23.626177: step 1919, loss 0.553911.
Train: 2018-08-05T23:26:23.783009: step 1920, loss 0.596576.
Test: 2018-08-05T23:26:24.023540: step 1920, loss 0.548337.
Train: 2018-08-05T23:26:24.202935: step 1921, loss 0.588364.
Train: 2018-08-05T23:26:24.353597: step 1922, loss 0.554184.
Train: 2018-08-05T23:26:24.507469: step 1923, loss 0.52004.
Train: 2018-08-05T23:26:24.663682: step 1924, loss 0.511774.
Train: 2018-08-05T23:26:24.824765: step 1925, loss 0.571169.
Train: 2018-08-05T23:26:25.004005: step 1926, loss 0.562796.
Train: 2018-08-05T23:26:25.222665: step 1927, loss 0.605209.
Train: 2018-08-05T23:26:25.381491: step 1928, loss 0.554376.
Train: 2018-08-05T23:26:25.532740: step 1929, loss 0.537121.
Train: 2018-08-05T23:26:25.696270: step 1930, loss 0.596668.
Test: 2018-08-05T23:26:25.939087: step 1930, loss 0.548416.
Train: 2018-08-05T23:26:26.132566: step 1931, loss 0.537424.
Train: 2018-08-05T23:26:26.320064: step 1932, loss 0.553803.
Train: 2018-08-05T23:26:26.462110: step 1933, loss 0.597011.
Train: 2018-08-05T23:26:26.635499: step 1934, loss 0.562887.
Train: 2018-08-05T23:26:26.794074: step 1935, loss 0.579661.
Train: 2018-08-05T23:26:26.963699: step 1936, loss 0.588351.
Train: 2018-08-05T23:26:27.142174: step 1937, loss 0.537439.
Train: 2018-08-05T23:26:27.312705: step 1938, loss 0.571445.
Train: 2018-08-05T23:26:27.530884: step 1939, loss 0.630286.
Train: 2018-08-05T23:26:27.710404: step 1940, loss 0.520612.
Test: 2018-08-05T23:26:27.945899: step 1940, loss 0.5485.
Train: 2018-08-05T23:26:28.115002: step 1941, loss 0.587969.
Train: 2018-08-05T23:26:28.370790: step 1942, loss 0.545988.
Train: 2018-08-05T23:26:28.618128: step 1943, loss 0.512012.
Train: 2018-08-05T23:26:28.880430: step 1944, loss 0.638877.
Train: 2018-08-05T23:26:29.105827: step 1945, loss 0.571529.
Train: 2018-08-05T23:26:29.277403: step 1946, loss 0.596781.
Train: 2018-08-05T23:26:29.481817: step 1947, loss 0.571204.
Train: 2018-08-05T23:26:29.647375: step 1948, loss 0.579698.
Train: 2018-08-05T23:26:29.824902: step 1949, loss 0.55444.
Train: 2018-08-05T23:26:29.986468: step 1950, loss 0.521104.
Test: 2018-08-05T23:26:30.225828: step 1950, loss 0.548673.
Train: 2018-08-05T23:26:30.425295: step 1951, loss 0.537486.
Train: 2018-08-05T23:26:30.597833: step 1952, loss 0.512344.
Train: 2018-08-05T23:26:30.788324: step 1953, loss 0.503947.
Train: 2018-08-05T23:26:30.999758: step 1954, loss 0.596352.
Train: 2018-08-05T23:26:31.242110: step 1955, loss 0.613611.
Train: 2018-08-05T23:26:31.550286: step 1956, loss 0.52878.
Train: 2018-08-05T23:26:31.797626: step 1957, loss 0.554261.
Train: 2018-08-05T23:26:32.040975: step 1958, loss 0.630706.
Train: 2018-08-05T23:26:32.266370: step 1959, loss 0.529001.
Train: 2018-08-05T23:26:32.506729: step 1960, loss 0.588287.
Test: 2018-08-05T23:26:32.742784: step 1960, loss 0.548437.
Train: 2018-08-05T23:26:32.912968: step 1961, loss 0.553807.
Train: 2018-08-05T23:26:33.071805: step 1962, loss 0.545634.
Train: 2018-08-05T23:26:33.244031: step 1963, loss 0.635661.
Train: 2018-08-05T23:26:33.418566: step 1964, loss 0.503327.
Train: 2018-08-05T23:26:33.586118: step 1965, loss 0.588227.
Train: 2018-08-05T23:26:33.738025: step 1966, loss 0.554514.
Train: 2018-08-05T23:26:33.906697: step 1967, loss 0.545717.
Train: 2018-08-05T23:26:34.062912: step 1968, loss 0.571359.
Train: 2018-08-05T23:26:34.233244: step 1969, loss 0.511613.
Train: 2018-08-05T23:26:34.409824: step 1970, loss 0.588328.
Test: 2018-08-05T23:26:34.640120: step 1970, loss 0.54837.
Train: 2018-08-05T23:26:34.808363: step 1971, loss 0.596526.
Train: 2018-08-05T23:26:34.979034: step 1972, loss 0.545717.
Train: 2018-08-05T23:26:35.184670: step 1973, loss 0.579837.
Train: 2018-08-05T23:26:35.354653: step 1974, loss 0.536996.
Train: 2018-08-05T23:26:35.514237: step 1975, loss 0.579643.
Train: 2018-08-05T23:26:35.685525: step 1976, loss 0.528434.
Train: 2018-08-05T23:26:35.848721: step 1977, loss 0.554142.
Train: 2018-08-05T23:26:36.008203: step 1978, loss 0.554185.
Train: 2018-08-05T23:26:36.195849: step 1979, loss 0.57116.
Train: 2018-08-05T23:26:36.352044: step 1980, loss 0.52816.
Test: 2018-08-05T23:26:36.601722: step 1980, loss 0.548266.
Train: 2018-08-05T23:26:36.760388: step 1981, loss 0.536971.
Train: 2018-08-05T23:26:36.935078: step 1982, loss 0.467833.
Train: 2018-08-05T23:26:37.085849: step 1983, loss 0.553877.
Train: 2018-08-05T23:26:37.237193: step 1984, loss 0.606322.
Train: 2018-08-05T23:26:37.415257: step 1985, loss 0.545409.
Train: 2018-08-05T23:26:37.579493: step 1986, loss 0.571294.
Train: 2018-08-05T23:26:37.737319: step 1987, loss 0.641368.
Train: 2018-08-05T23:26:37.883233: step 1988, loss 0.52782.
Train: 2018-08-05T23:26:38.061500: step 1989, loss 0.606353.
Train: 2018-08-05T23:26:38.223556: step 1990, loss 0.598002.
Test: 2018-08-05T23:26:38.446341: step 1990, loss 0.548079.
Train: 2018-08-05T23:26:38.622373: step 1991, loss 0.606642.
Train: 2018-08-05T23:26:38.774886: step 1992, loss 0.527393.
Train: 2018-08-05T23:26:38.942407: step 1993, loss 0.562596.
Train: 2018-08-05T23:26:39.105108: step 1994, loss 0.614996.
Train: 2018-08-05T23:26:39.251403: step 1995, loss 0.571015.
Train: 2018-08-05T23:26:39.422091: step 1996, loss 0.536876.
Train: 2018-08-05T23:26:39.591122: step 1997, loss 0.536865.
Train: 2018-08-05T23:26:39.757236: step 1998, loss 0.562637.
Train: 2018-08-05T23:26:39.915950: step 1999, loss 0.571414.
Train: 2018-08-05T23:26:40.077823: step 2000, loss 0.536816.
Test: 2018-08-05T23:26:40.313908: step 2000, loss 0.548264.
Train: 2018-08-05T23:26:41.054975: step 2001, loss 0.511341.
Train: 2018-08-05T23:26:41.223821: step 2002, loss 0.528159.
Train: 2018-08-05T23:26:41.368436: step 2003, loss 0.570985.
Train: 2018-08-05T23:26:41.536114: step 2004, loss 0.562488.
Train: 2018-08-05T23:26:41.711536: step 2005, loss 0.554413.
Train: 2018-08-05T23:26:41.885942: step 2006, loss 0.623362.
Train: 2018-08-05T23:26:42.049115: step 2007, loss 0.528285.
Train: 2018-08-05T23:26:42.215277: step 2008, loss 0.553893.
Train: 2018-08-05T23:26:42.367563: step 2009, loss 0.553737.
Train: 2018-08-05T23:26:42.554979: step 2010, loss 0.536336.
Test: 2018-08-05T23:26:42.794337: step 2010, loss 0.548155.
Train: 2018-08-05T23:26:42.949904: step 2011, loss 0.563068.
Train: 2018-08-05T23:26:43.110789: step 2012, loss 0.623159.
Train: 2018-08-05T23:26:43.267351: step 2013, loss 0.545564.
Train: 2018-08-05T23:26:43.421797: step 2014, loss 0.562778.
Train: 2018-08-05T23:26:43.587855: step 2015, loss 0.614635.
Train: 2018-08-05T23:26:43.811990: step 2016, loss 0.545018.
Train: 2018-08-05T23:26:44.043370: step 2017, loss 0.545202.
Train: 2018-08-05T23:26:44.199024: step 2018, loss 0.502242.
Train: 2018-08-05T23:26:44.383551: step 2019, loss 0.545081.
Train: 2018-08-05T23:26:44.558924: step 2020, loss 0.64023.
Test: 2018-08-05T23:26:44.783044: step 2020, loss 0.548166.
Train: 2018-08-05T23:26:44.963480: step 2021, loss 0.623113.
Train: 2018-08-05T23:26:45.161269: step 2022, loss 0.536841.
Train: 2018-08-05T23:26:45.343040: step 2023, loss 0.519563.
Train: 2018-08-05T23:26:45.530496: step 2024, loss 0.596876.
Train: 2018-08-05T23:26:45.702330: step 2025, loss 0.570921.
Train: 2018-08-05T23:26:45.894572: step 2026, loss 0.631037.
Train: 2018-08-05T23:26:46.101022: step 2027, loss 0.537147.
Train: 2018-08-05T23:26:46.359330: step 2028, loss 0.468426.
Train: 2018-08-05T23:26:46.572759: step 2029, loss 0.468082.
Train: 2018-08-05T23:26:46.826082: step 2030, loss 0.571204.
Test: 2018-08-05T23:26:47.086385: step 2030, loss 0.548208.
Train: 2018-08-05T23:26:47.329734: step 2031, loss 0.519564.
Train: 2018-08-05T23:26:47.556130: step 2032, loss 0.519497.
Train: 2018-08-05T23:26:47.795489: step 2033, loss 0.579967.
Train: 2018-08-05T23:26:48.042827: step 2034, loss 0.510517.
Train: 2018-08-05T23:26:48.251270: step 2035, loss 0.562607.
Train: 2018-08-05T23:26:48.438768: step 2036, loss 0.536569.
Train: 2018-08-05T23:26:48.625270: step 2037, loss 0.536769.
Train: 2018-08-05T23:26:48.806784: step 2038, loss 0.544385.
Train: 2018-08-05T23:26:48.995280: step 2039, loss 0.562503.
Train: 2018-08-05T23:26:49.199733: step 2040, loss 0.491998.
Test: 2018-08-05T23:26:49.439093: step 2040, loss 0.547878.
Train: 2018-08-05T23:26:49.639557: step 2041, loss 0.580383.
Train: 2018-08-05T23:26:49.837030: step 2042, loss 0.545425.
Train: 2018-08-05T23:26:50.030512: step 2043, loss 0.581042.
Train: 2018-08-05T23:26:50.218011: step 2044, loss 0.481718.
Train: 2018-08-05T23:26:50.415482: step 2045, loss 0.509293.
Train: 2018-08-05T23:26:50.614949: step 2046, loss 0.554066.
Train: 2018-08-05T23:26:50.821396: step 2047, loss 0.535839.
Train: 2018-08-05T23:26:51.009893: step 2048, loss 0.60852.
Train: 2018-08-05T23:26:51.206367: step 2049, loss 0.553679.
Train: 2018-08-05T23:26:51.428772: step 2050, loss 0.553265.
Test: 2018-08-05T23:26:51.669129: step 2050, loss 0.547796.
Train: 2018-08-05T23:26:51.889540: step 2051, loss 0.654488.
Train: 2018-08-05T23:26:52.131891: step 2052, loss 0.58162.
Train: 2018-08-05T23:26:52.358286: step 2053, loss 0.635569.
Train: 2018-08-05T23:26:52.569721: step 2054, loss 0.608593.
Train: 2018-08-05T23:26:52.778164: step 2055, loss 0.581003.
Train: 2018-08-05T23:26:52.974637: step 2056, loss 0.580999.
Train: 2018-08-05T23:26:53.163134: step 2057, loss 0.724475.
Train: 2018-08-05T23:26:53.348637: step 2058, loss 0.61576.
Train: 2018-08-05T23:26:53.535140: step 2059, loss 0.580395.
Train: 2018-08-05T23:26:53.731614: step 2060, loss 0.632155.
Test: 2018-08-05T23:26:53.968979: step 2060, loss 0.548127.
Train: 2018-08-05T23:26:54.149496: step 2061, loss 0.527894.
Train: 2018-08-05T23:26:54.336995: step 2062, loss 0.519601.
Train: 2018-08-05T23:26:54.533469: step 2063, loss 0.605264.
Train: 2018-08-05T23:26:54.714985: step 2064, loss 0.553922.
Train: 2018-08-05T23:26:54.922429: step 2065, loss 0.613454.
Train: 2018-08-05T23:26:55.130873: step 2066, loss 0.520709.
Train: 2018-08-05T23:26:55.331338: step 2067, loss 0.537737.
Train: 2018-08-05T23:26:55.532796: step 2068, loss 0.537775.
Train: 2018-08-05T23:26:55.737250: step 2069, loss 0.479534.
Train: 2018-08-05T23:26:55.931730: step 2070, loss 0.521022.
Test: 2018-08-05T23:26:56.170093: step 2070, loss 0.548742.
Train: 2018-08-05T23:26:56.363574: step 2071, loss 0.587764.
Train: 2018-08-05T23:26:56.569026: step 2072, loss 0.579252.
Train: 2018-08-05T23:26:56.785447: step 2073, loss 0.554263.
Train: 2018-08-05T23:26:56.998877: step 2074, loss 0.596325.
Train: 2018-08-05T23:26:57.212306: step 2075, loss 0.604709.
Train: 2018-08-05T23:26:57.394843: step 2076, loss 0.60466.
Train: 2018-08-05T23:26:57.561372: step 2077, loss 0.621103.
Train: 2018-08-05T23:26:57.729921: step 2078, loss 0.554357.
Train: 2018-08-05T23:26:57.899467: step 2079, loss 0.59587.
Train: 2018-08-05T23:26:58.059042: step 2080, loss 0.529645.
Test: 2018-08-05T23:26:58.296405: step 2080, loss 0.548917.
Train: 2018-08-05T23:26:58.479914: step 2081, loss 0.637158.
Train: 2018-08-05T23:26:58.647467: step 2082, loss 0.472139.
Train: 2018-08-05T23:26:58.804344: step 2083, loss 0.521784.
Train: 2018-08-05T23:26:58.974698: step 2084, loss 0.53791.
Train: 2018-08-05T23:26:59.162986: step 2085, loss 0.562653.
Train: 2018-08-05T23:26:59.333852: step 2086, loss 0.579331.
Train: 2018-08-05T23:26:59.492294: step 2087, loss 0.587868.
Train: 2018-08-05T23:26:59.648509: step 2088, loss 0.579227.
Train: 2018-08-05T23:26:59.825495: step 2089, loss 0.512755.
Train: 2018-08-05T23:26:59.984156: step 2090, loss 0.596049.
Test: 2018-08-05T23:27:00.225514: step 2090, loss 0.548753.
Train: 2018-08-05T23:27:00.396053: step 2091, loss 0.587721.
Train: 2018-08-05T23:27:00.567597: step 2092, loss 0.579308.
Train: 2018-08-05T23:27:00.743126: step 2093, loss 0.471099.
Train: 2018-08-05T23:27:00.895900: step 2094, loss 0.613134.
Train: 2018-08-05T23:27:01.072460: step 2095, loss 0.546016.
Train: 2018-08-05T23:27:01.253116: step 2096, loss 0.554171.
Train: 2018-08-05T23:27:01.450648: step 2097, loss 0.571058.
Train: 2018-08-05T23:27:01.636982: step 2098, loss 0.545835.
Train: 2018-08-05T23:27:01.804206: step 2099, loss 0.562599.
Train: 2018-08-05T23:27:01.958267: step 2100, loss 0.604637.
Test: 2018-08-05T23:27:02.208306: step 2100, loss 0.5485.
Train: 2018-08-05T23:27:02.943340: step 2101, loss 0.545894.
Train: 2018-08-05T23:27:03.099556: step 2102, loss 0.638783.
Train: 2018-08-05T23:27:03.265249: step 2103, loss 0.537253.
Train: 2018-08-05T23:27:03.410307: step 2104, loss 0.537463.
Train: 2018-08-05T23:27:03.577256: step 2105, loss 0.587901.
Train: 2018-08-05T23:27:03.751417: step 2106, loss 0.495136.
Train: 2018-08-05T23:27:03.904745: step 2107, loss 0.630192.
Train: 2018-08-05T23:27:04.057810: step 2108, loss 0.460963.
Train: 2018-08-05T23:27:04.223337: step 2109, loss 0.528913.
Train: 2018-08-05T23:27:04.379584: step 2110, loss 0.570992.
Test: 2018-08-05T23:27:04.621364: step 2110, loss 0.548316.
Train: 2018-08-05T23:27:04.780953: step 2111, loss 0.588139.
Train: 2018-08-05T23:27:04.942923: step 2112, loss 0.553961.
Train: 2018-08-05T23:27:05.099137: step 2113, loss 0.588309.
Train: 2018-08-05T23:27:05.248584: step 2114, loss 0.599329.
Train: 2018-08-05T23:27:05.412211: step 2115, loss 0.527942.
Train: 2018-08-05T23:27:05.583078: step 2116, loss 0.528275.
Train: 2018-08-05T23:27:05.738153: step 2117, loss 0.52828.
Train: 2018-08-05T23:27:05.880647: step 2118, loss 0.554032.
Train: 2018-08-05T23:27:06.036860: step 2119, loss 0.519523.
Train: 2018-08-05T23:27:06.193106: step 2120, loss 0.536536.
Test: 2018-08-05T23:27:06.435371: step 2120, loss 0.548074.
Train: 2018-08-05T23:27:06.591584: step 2121, loss 0.510511.
Train: 2018-08-05T23:27:06.742001: step 2122, loss 0.562601.
Train: 2018-08-05T23:27:06.903932: step 2123, loss 0.623438.
Train: 2018-08-05T23:27:07.060177: step 2124, loss 0.492655.
Train: 2018-08-05T23:27:07.224104: step 2125, loss 0.597649.
Train: 2018-08-05T23:27:07.380517: step 2126, loss 0.553929.
Train: 2018-08-05T23:27:07.546939: step 2127, loss 0.57155.
Train: 2018-08-05T23:27:07.715148: step 2128, loss 0.518444.
Train: 2018-08-05T23:27:07.871115: step 2129, loss 0.553883.
Train: 2018-08-05T23:27:08.027330: step 2130, loss 0.589211.
Test: 2018-08-05T23:27:08.264006: step 2130, loss 0.547876.
Train: 2018-08-05T23:27:08.423023: step 2131, loss 0.58947.
Train: 2018-08-05T23:27:08.583957: step 2132, loss 0.589118.
Train: 2018-08-05T23:27:08.748659: step 2133, loss 0.589381.
Train: 2018-08-05T23:27:08.917909: step 2134, loss 0.518606.
Train: 2018-08-05T23:27:09.074123: step 2135, loss 0.589009.
Train: 2018-08-05T23:27:09.224021: step 2136, loss 0.562274.
Train: 2018-08-05T23:27:09.401022: step 2137, loss 0.641872.
Train: 2018-08-05T23:27:09.587609: step 2138, loss 0.553925.
Train: 2018-08-05T23:27:09.758361: step 2139, loss 0.536059.
Train: 2018-08-05T23:27:09.921434: step 2140, loss 0.632656.
Test: 2018-08-05T23:27:10.165791: step 2140, loss 0.548011.
Train: 2018-08-05T23:27:10.334582: step 2141, loss 0.562399.
Train: 2018-08-05T23:27:10.501278: step 2142, loss 0.571427.
Train: 2018-08-05T23:27:10.656451: step 2143, loss 0.501932.
Train: 2018-08-05T23:27:10.832232: step 2144, loss 0.614536.
Train: 2018-08-05T23:27:10.993883: step 2145, loss 0.49342.
Train: 2018-08-05T23:27:11.156455: step 2146, loss 0.493794.
Train: 2018-08-05T23:27:11.315795: step 2147, loss 0.588512.
Train: 2018-08-05T23:27:11.469618: step 2148, loss 0.519504.
Train: 2018-08-05T23:27:11.650610: step 2149, loss 0.562854.
Train: 2018-08-05T23:27:11.828339: step 2150, loss 0.597483.
Test: 2018-08-05T23:27:12.048946: step 2150, loss 0.548095.
Train: 2018-08-05T23:27:12.234830: step 2151, loss 0.545304.
Train: 2018-08-05T23:27:12.398992: step 2152, loss 0.58844.
Train: 2018-08-05T23:27:12.567792: step 2153, loss 0.528185.
Train: 2018-08-05T23:27:12.738649: step 2154, loss 0.588411.
Train: 2018-08-05T23:27:12.904150: step 2155, loss 0.553941.
Train: 2018-08-05T23:27:13.070990: step 2156, loss 0.571066.
Train: 2018-08-05T23:27:13.226000: step 2157, loss 0.545412.
Train: 2018-08-05T23:27:13.396961: step 2158, loss 0.597386.
Train: 2018-08-05T23:27:13.569777: step 2159, loss 0.545543.
Train: 2018-08-05T23:27:13.722320: step 2160, loss 0.56223.
Test: 2018-08-05T23:27:13.957617: step 2160, loss 0.548117.
Train: 2018-08-05T23:27:14.114660: step 2161, loss 0.493708.
Train: 2018-08-05T23:27:14.286202: step 2162, loss 0.5105.
Train: 2018-08-05T23:27:14.451202: step 2163, loss 0.605731.
Train: 2018-08-05T23:27:14.614489: step 2164, loss 0.571112.
Train: 2018-08-05T23:27:14.786178: step 2165, loss 0.536915.
Train: 2018-08-05T23:27:14.934848: step 2166, loss 0.684125.
Train: 2018-08-05T23:27:15.086233: step 2167, loss 0.64051.
Train: 2018-08-05T23:27:15.258067: step 2168, loss 0.545359.
Train: 2018-08-05T23:27:15.403841: step 2169, loss 0.596745.
Train: 2018-08-05T23:27:15.555727: step 2170, loss 0.614184.
Test: 2018-08-05T23:27:15.808957: step 2170, loss 0.548242.
Train: 2018-08-05T23:27:15.974728: step 2171, loss 0.53744.
Train: 2018-08-05T23:27:16.115597: step 2172, loss 0.545579.
Train: 2018-08-05T23:27:16.265836: step 2173, loss 0.630556.
Train: 2018-08-05T23:27:16.434004: step 2174, loss 0.596187.
Train: 2018-08-05T23:27:16.595551: step 2175, loss 0.495245.
Train: 2018-08-05T23:27:16.756486: step 2176, loss 0.528962.
Train: 2018-08-05T23:27:16.918427: step 2177, loss 0.528978.
Train: 2018-08-05T23:27:17.074642: step 2178, loss 0.545938.
Train: 2018-08-05T23:27:17.230854: step 2179, loss 0.587941.
Train: 2018-08-05T23:27:17.402368: step 2180, loss 0.537276.
Test: 2018-08-05T23:27:17.636688: step 2180, loss 0.548483.
Train: 2018-08-05T23:27:17.788959: step 2181, loss 0.59614.
Train: 2018-08-05T23:27:17.957926: step 2182, loss 0.545886.
Train: 2018-08-05T23:27:18.114140: step 2183, loss 0.587912.
Train: 2018-08-05T23:27:18.265427: step 2184, loss 0.52028.
Train: 2018-08-05T23:27:18.418655: step 2185, loss 0.63885.
Train: 2018-08-05T23:27:18.574899: step 2186, loss 0.52069.
Train: 2018-08-05T23:27:18.731081: step 2187, loss 0.495325.
Train: 2018-08-05T23:27:18.894121: step 2188, loss 0.587999.
Train: 2018-08-05T23:27:19.050303: step 2189, loss 0.570963.
Train: 2018-08-05T23:27:19.190926: step 2190, loss 0.545601.
Test: 2018-08-05T23:27:19.438874: step 2190, loss 0.548381.
Train: 2018-08-05T23:27:19.595087: step 2191, loss 0.5542.
Train: 2018-08-05T23:27:19.763190: step 2192, loss 0.477734.
Train: 2018-08-05T23:27:19.909596: step 2193, loss 0.503153.
Train: 2018-08-05T23:27:20.065817: step 2194, loss 0.613741.
Train: 2018-08-05T23:27:20.222026: step 2195, loss 0.502274.
Train: 2018-08-05T23:27:20.378292: step 2196, loss 0.59718.
Train: 2018-08-05T23:27:20.531201: step 2197, loss 0.527989.
Train: 2018-08-05T23:27:20.687385: step 2198, loss 0.510652.
Train: 2018-08-05T23:27:20.839400: step 2199, loss 0.62336.
Train: 2018-08-05T23:27:20.995644: step 2200, loss 0.580136.
Test: 2018-08-05T23:27:21.229968: step 2200, loss 0.547991.
Train: 2018-08-05T23:27:22.011075: step 2201, loss 0.527747.
Train: 2018-08-05T23:27:22.167255: step 2202, loss 0.52769.
Train: 2018-08-05T23:27:22.332260: step 2203, loss 0.588564.
Train: 2018-08-05T23:27:22.488509: step 2204, loss 0.580261.
Train: 2018-08-05T23:27:22.636446: step 2205, loss 0.624195.
Train: 2018-08-05T23:27:22.788775: step 2206, loss 0.553964.
Train: 2018-08-05T23:27:22.957602: step 2207, loss 0.544998.
Train: 2018-08-05T23:27:23.113816: step 2208, loss 0.615208.
Train: 2018-08-05T23:27:23.279835: step 2209, loss 0.536279.
Train: 2018-08-05T23:27:23.425044: step 2210, loss 0.562285.
Test: 2018-08-05T23:27:23.674985: step 2210, loss 0.547967.
Train: 2018-08-05T23:27:23.837733: step 2211, loss 0.579789.
Train: 2018-08-05T23:27:23.978324: step 2212, loss 0.475292.
Train: 2018-08-05T23:27:24.134536: step 2213, loss 0.518836.
Train: 2018-08-05T23:27:24.288866: step 2214, loss 0.562835.
Train: 2018-08-05T23:27:24.459887: step 2215, loss 0.474793.
Train: 2018-08-05T23:27:24.617842: step 2216, loss 0.589045.
Train: 2018-08-05T23:27:24.764112: step 2217, loss 0.500976.
Train: 2018-08-05T23:27:24.924944: step 2218, loss 0.536044.
Train: 2018-08-05T23:27:25.081159: step 2219, loss 0.615884.
Train: 2018-08-05T23:27:25.221783: step 2220, loss 0.571686.
Test: 2018-08-05T23:27:25.470585: step 2220, loss 0.547834.
Train: 2018-08-05T23:27:25.629624: step 2221, loss 0.580431.
Train: 2018-08-05T23:27:25.771084: step 2222, loss 0.526881.
Train: 2018-08-05T23:27:25.932925: step 2223, loss 0.500599.
Train: 2018-08-05T23:27:26.089139: step 2224, loss 0.634153.
Train: 2018-08-05T23:27:26.251653: step 2225, loss 0.500464.
Train: 2018-08-05T23:27:26.392537: step 2226, loss 0.518373.
Train: 2018-08-05T23:27:26.548749: step 2227, loss 0.571281.
Train: 2018-08-05T23:27:26.704963: step 2228, loss 0.544908.
Train: 2018-08-05T23:27:26.861326: step 2229, loss 0.553922.
Train: 2018-08-05T23:27:27.017542: step 2230, loss 0.508675.
Test: 2018-08-05T23:27:27.255680: step 2230, loss 0.547771.
Train: 2018-08-05T23:27:27.414866: step 2231, loss 0.616856.
Train: 2018-08-05T23:27:27.571085: step 2232, loss 0.652528.
Train: 2018-08-05T23:27:27.727302: step 2233, loss 0.518182.
Train: 2018-08-05T23:27:27.884696: step 2234, loss 0.589646.
Train: 2018-08-05T23:27:28.046115: step 2235, loss 0.571442.
Train: 2018-08-05T23:27:28.189177: step 2236, loss 0.500466.
Train: 2018-08-05T23:27:28.353063: step 2237, loss 0.607485.
Train: 2018-08-05T23:27:28.505340: step 2238, loss 0.536005.
Train: 2018-08-05T23:27:28.666447: step 2239, loss 0.562685.
Train: 2018-08-05T23:27:28.841267: step 2240, loss 0.580143.
Test: 2018-08-05T23:27:29.066055: step 2240, loss 0.547831.
Train: 2018-08-05T23:27:29.238298: step 2241, loss 0.571545.
Train: 2018-08-05T23:27:29.396275: step 2242, loss 0.589299.
Train: 2018-08-05T23:27:29.552490: step 2243, loss 0.633632.
Train: 2018-08-05T23:27:29.717139: step 2244, loss 0.580023.
Train: 2018-08-05T23:27:29.876018: step 2245, loss 0.57116.
Train: 2018-08-05T23:27:30.016611: step 2246, loss 0.571446.
Train: 2018-08-05T23:27:30.172824: step 2247, loss 0.554222.
Train: 2018-08-05T23:27:30.330111: step 2248, loss 0.579849.
Train: 2018-08-05T23:27:30.495471: step 2249, loss 0.57112.
Train: 2018-08-05T23:27:30.667330: step 2250, loss 0.562927.
Test: 2018-08-05T23:27:30.909415: step 2250, loss 0.548161.
Train: 2018-08-05T23:27:31.066696: step 2251, loss 0.597108.
Train: 2018-08-05T23:27:31.222883: step 2252, loss 0.55385.
Train: 2018-08-05T23:27:31.383139: step 2253, loss 0.579731.
Train: 2018-08-05T23:27:31.531378: step 2254, loss 0.562399.
Train: 2018-08-05T23:27:31.687591: step 2255, loss 0.647304.
Train: 2018-08-05T23:27:31.853623: step 2256, loss 0.605109.
Train: 2018-08-05T23:27:32.004715: step 2257, loss 0.62172.
Train: 2018-08-05T23:27:32.160934: step 2258, loss 0.495528.
Train: 2018-08-05T23:27:32.322506: step 2259, loss 0.545957.
Train: 2018-08-05T23:27:32.479121: step 2260, loss 0.587748.
Test: 2018-08-05T23:27:32.713444: step 2260, loss 0.548784.
Train: 2018-08-05T23:27:32.876480: step 2261, loss 0.562687.
Train: 2018-08-05T23:27:33.032693: step 2262, loss 0.596085.
Train: 2018-08-05T23:27:33.188937: step 2263, loss 0.554358.
Train: 2018-08-05T23:27:33.345113: step 2264, loss 0.562602.
Train: 2018-08-05T23:27:33.485706: step 2265, loss 0.527969.
Train: 2018-08-05T23:27:33.641920: step 2266, loss 0.546524.
Train: 2018-08-05T23:27:33.794345: step 2267, loss 0.56274.
Train: 2018-08-05T23:27:33.954305: step 2268, loss 0.628511.
Train: 2018-08-05T23:27:34.110487: step 2269, loss 0.562861.
Train: 2018-08-05T23:27:34.262833: step 2270, loss 0.505411.
Test: 2018-08-05T23:27:34.501133: step 2270, loss 0.548981.
Train: 2018-08-05T23:27:34.657312: step 2271, loss 0.603818.
Train: 2018-08-05T23:27:34.829170: step 2272, loss 0.546569.
Train: 2018-08-05T23:27:34.985416: step 2273, loss 0.529829.
Train: 2018-08-05T23:27:35.141597: step 2274, loss 0.538003.
Train: 2018-08-05T23:27:35.293087: step 2275, loss 0.54603.
Train: 2018-08-05T23:27:35.454974: step 2276, loss 0.571066.
Train: 2018-08-05T23:27:35.611188: step 2277, loss 0.529524.
Train: 2018-08-05T23:27:35.778131: step 2278, loss 0.596013.
Train: 2018-08-05T23:27:35.962746: step 2279, loss 0.520836.
Train: 2018-08-05T23:27:36.129408: step 2280, loss 0.562766.
Test: 2018-08-05T23:27:36.352801: step 2280, loss 0.548546.
Train: 2018-08-05T23:27:36.509012: step 2281, loss 0.570938.
Train: 2018-08-05T23:27:36.665226: step 2282, loss 0.596452.
Train: 2018-08-05T23:27:36.828163: step 2283, loss 0.528982.
Train: 2018-08-05T23:27:36.990748: step 2284, loss 0.587928.
Train: 2018-08-05T23:27:37.146992: step 2285, loss 0.57935.
Train: 2018-08-05T23:27:37.293370: step 2286, loss 0.545865.
Train: 2018-08-05T23:27:37.454325: step 2287, loss 0.596497.
Train: 2018-08-05T23:27:37.610514: step 2288, loss 0.511604.
Train: 2018-08-05T23:27:37.770151: step 2289, loss 0.494537.
Train: 2018-08-05T23:27:37.924879: step 2290, loss 0.562659.
Test: 2018-08-05T23:27:38.143609: step 2290, loss 0.548236.
Train: 2018-08-05T23:27:38.293826: step 2291, loss 0.656418.
Train: 2018-08-05T23:27:38.453713: step 2292, loss 0.511265.
Train: 2018-08-05T23:27:38.625578: step 2293, loss 0.5624.
Train: 2018-08-05T23:27:38.785019: step 2294, loss 0.562787.
Train: 2018-08-05T23:27:38.934472: step 2295, loss 0.493948.
Train: 2018-08-05T23:27:39.090687: step 2296, loss 0.622996.
Train: 2018-08-05T23:27:39.255669: step 2297, loss 0.519323.
Train: 2018-08-05T23:27:39.399299: step 2298, loss 0.493564.
Train: 2018-08-05T23:27:39.555512: step 2299, loss 0.579999.
Train: 2018-08-05T23:27:39.725291: step 2300, loss 0.510425.
Test: 2018-08-05T23:27:39.962413: step 2300, loss 0.547987.
Train: 2018-08-05T23:27:40.664029: step 2301, loss 0.588695.
Train: 2018-08-05T23:27:40.833539: step 2302, loss 0.684894.
Train: 2018-08-05T23:27:40.977607: step 2303, loss 0.597548.
Train: 2018-08-05T23:27:41.133820: step 2304, loss 0.492917.
Train: 2018-08-05T23:27:41.301669: step 2305, loss 0.527903.
Train: 2018-08-05T23:27:41.454872: step 2306, loss 0.59755.
Train: 2018-08-05T23:27:41.595464: step 2307, loss 0.501502.
Train: 2018-08-05T23:27:41.752812: step 2308, loss 0.597274.
Train: 2018-08-05T23:27:41.914827: step 2309, loss 0.553826.
Train: 2018-08-05T23:27:42.071008: step 2310, loss 0.536614.
Test: 2018-08-05T23:27:42.301554: step 2310, loss 0.547956.
Train: 2018-08-05T23:27:42.454833: step 2311, loss 0.536579.
Train: 2018-08-05T23:27:42.611023: step 2312, loss 0.466501.
Train: 2018-08-05T23:27:42.777773: step 2313, loss 0.492558.
Train: 2018-08-05T23:27:42.921524: step 2314, loss 0.545014.
Train: 2018-08-05T23:27:43.077739: step 2315, loss 0.553848.
Train: 2018-08-05T23:27:43.244385: step 2316, loss 0.553845.
Train: 2018-08-05T23:27:43.392178: step 2317, loss 0.553873.
Train: 2018-08-05T23:27:43.548416: step 2318, loss 0.616373.
Train: 2018-08-05T23:27:43.704604: step 2319, loss 0.669876.
Train: 2018-08-05T23:27:43.859631: step 2320, loss 0.554044.
Test: 2018-08-05T23:27:44.093952: step 2320, loss 0.547792.
Train: 2018-08-05T23:27:44.260955: step 2321, loss 0.571592.
Train: 2018-08-05T23:27:44.407265: step 2322, loss 0.536048.
Train: 2018-08-05T23:27:44.579066: step 2323, loss 0.615951.
Train: 2018-08-05T23:27:44.737747: step 2324, loss 0.518343.
Train: 2018-08-05T23:27:44.876650: step 2325, loss 0.562693.
Train: 2018-08-05T23:27:45.032864: step 2326, loss 0.571587.
Train: 2018-08-05T23:27:45.189077: step 2327, loss 0.509752.
Train: 2018-08-05T23:27:45.344264: step 2328, loss 0.492103.
Train: 2018-08-05T23:27:45.500459: step 2329, loss 0.47431.
Train: 2018-08-05T23:27:45.658961: step 2330, loss 0.562677.
Test: 2018-08-05T23:27:45.882347: step 2330, loss 0.547809.
Train: 2018-08-05T23:27:46.047988: step 2331, loss 0.5627.
Train: 2018-08-05T23:27:46.204231: step 2332, loss 0.607222.
Train: 2018-08-05T23:27:46.351595: step 2333, loss 0.57154.
Train: 2018-08-05T23:27:46.507799: step 2334, loss 0.500508.
Train: 2018-08-05T23:27:46.663982: step 2335, loss 0.509371.
Train: 2018-08-05T23:27:46.833686: step 2336, loss 0.509227.
Train: 2018-08-05T23:27:46.989531: step 2337, loss 0.544721.
Train: 2018-08-05T23:27:47.145775: step 2338, loss 0.481925.
Train: 2018-08-05T23:27:47.307925: step 2339, loss 0.562849.
Train: 2018-08-05T23:27:47.453315: step 2340, loss 0.553967.
Test: 2018-08-05T23:27:47.703251: step 2340, loss 0.547717.
Train: 2018-08-05T23:27:47.851434: step 2341, loss 0.590098.
Train: 2018-08-05T23:27:48.007678: step 2342, loss 0.60823.
Train: 2018-08-05T23:27:48.179508: step 2343, loss 0.581001.
Train: 2018-08-05T23:27:48.335679: step 2344, loss 0.571894.
Train: 2018-08-05T23:27:48.491924: step 2345, loss 0.626315.
Train: 2018-08-05T23:27:48.648135: step 2346, loss 0.626207.
Train: 2018-08-05T23:27:48.801624: step 2347, loss 0.562832.
Train: 2018-08-05T23:27:48.959824: step 2348, loss 0.598712.
Train: 2018-08-05T23:27:49.119021: step 2349, loss 0.518103.
Train: 2018-08-05T23:27:49.269251: step 2350, loss 0.643055.
Test: 2018-08-05T23:27:49.516288: step 2350, loss 0.547802.
Train: 2018-08-05T23:27:49.672526: step 2351, loss 0.562725.
Train: 2018-08-05T23:27:49.834861: step 2352, loss 0.562604.
Train: 2018-08-05T23:27:49.991074: step 2353, loss 0.580488.
Train: 2018-08-05T23:27:50.147318: step 2354, loss 0.571253.
Train: 2018-08-05T23:27:50.300715: step 2355, loss 0.431768.
Train: 2018-08-05T23:27:50.459850: step 2356, loss 0.588712.
Train: 2018-08-05T23:27:50.616063: step 2357, loss 0.562365.
Train: 2018-08-05T23:27:50.769119: step 2358, loss 0.588725.
Train: 2018-08-05T23:27:50.926395: step 2359, loss 0.623108.
Train: 2018-08-05T23:27:51.082242: step 2360, loss 0.536692.
Test: 2018-08-05T23:27:51.324864: step 2360, loss 0.548091.
Train: 2018-08-05T23:27:51.468899: step 2361, loss 0.562534.
Train: 2018-08-05T23:27:51.625117: step 2362, loss 0.502431.
Train: 2018-08-05T23:27:51.776257: step 2363, loss 0.588313.
Train: 2018-08-05T23:27:51.937227: step 2364, loss 0.502493.
Train: 2018-08-05T23:27:52.090058: step 2365, loss 0.554152.
Train: 2018-08-05T23:27:52.251733: step 2366, loss 0.502401.
Train: 2018-08-05T23:27:52.397625: step 2367, loss 0.67428.
Train: 2018-08-05T23:27:52.553835: step 2368, loss 0.622669.
Train: 2018-08-05T23:27:52.710048: step 2369, loss 0.571068.
Train: 2018-08-05T23:27:52.867072: step 2370, loss 0.588167.
Test: 2018-08-05T23:27:53.101393: step 2370, loss 0.548243.
Train: 2018-08-05T23:27:53.268457: step 2371, loss 0.528267.
Train: 2018-08-05T23:27:53.421835: step 2372, loss 0.571164.
Train: 2018-08-05T23:27:53.578055: step 2373, loss 0.622532.
Train: 2018-08-05T23:27:53.745526: step 2374, loss 0.503205.
Train: 2018-08-05T23:27:53.897121: step 2375, loss 0.621545.
Train: 2018-08-05T23:27:54.053335: step 2376, loss 0.562948.
Train: 2018-08-05T23:27:54.207501: step 2377, loss 0.579776.
Train: 2018-08-05T23:27:54.381356: step 2378, loss 0.679721.
Train: 2018-08-05T23:27:54.537595: step 2379, loss 0.562729.
Train: 2018-08-05T23:27:54.678195: step 2380, loss 0.529674.
Test: 2018-08-05T23:27:54.928919: step 2380, loss 0.548783.
Train: 2018-08-05T23:27:55.085132: step 2381, loss 0.579419.
Train: 2018-08-05T23:27:55.247842: step 2382, loss 0.586724.
Train: 2018-08-05T23:27:55.389867: step 2383, loss 0.595995.
Train: 2018-08-05T23:27:55.546051: step 2384, loss 0.547344.
Train: 2018-08-05T23:27:55.702288: step 2385, loss 0.587957.
Train: 2018-08-05T23:27:55.874191: step 2386, loss 0.55483.
Train: 2018-08-05T23:27:56.030435: step 2387, loss 0.58691.
Train: 2018-08-05T23:27:56.186648: step 2388, loss 0.546023.
Train: 2018-08-05T23:27:56.343053: step 2389, loss 0.521891.
Train: 2018-08-05T23:27:56.499268: step 2390, loss 0.530207.
Test: 2018-08-05T23:27:56.739831: step 2390, loss 0.549104.
Train: 2018-08-05T23:27:56.896366: step 2391, loss 0.562393.
Train: 2018-08-05T23:27:57.052609: step 2392, loss 0.578908.
Train: 2018-08-05T23:27:57.208816: step 2393, loss 0.595703.
Train: 2018-08-05T23:27:57.373858: step 2394, loss 0.530107.
Train: 2018-08-05T23:27:57.514451: step 2395, loss 0.521532.
Train: 2018-08-05T23:27:57.686285: step 2396, loss 0.627906.
Train: 2018-08-05T23:27:57.843273: step 2397, loss 0.61253.
Train: 2018-08-05T23:27:57.999486: step 2398, loss 0.488566.
Train: 2018-08-05T23:27:58.158402: step 2399, loss 0.579176.
Train: 2018-08-05T23:27:58.305934: step 2400, loss 0.579528.
Test: 2018-08-05T23:27:58.552937: step 2400, loss 0.548758.
Train: 2018-08-05T23:27:59.278360: step 2401, loss 0.546589.
Train: 2018-08-05T23:27:59.444298: step 2402, loss 0.57923.
Train: 2018-08-05T23:27:59.600501: step 2403, loss 0.629575.
Train: 2018-08-05T23:27:59.758508: step 2404, loss 0.546033.
Train: 2018-08-05T23:27:59.904706: step 2405, loss 0.620906.
Train: 2018-08-05T23:28:00.060950: step 2406, loss 0.545886.
Train: 2018-08-05T23:28:00.217163: step 2407, loss 0.562463.
Train: 2018-08-05T23:28:00.366784: step 2408, loss 0.621172.
Train: 2018-08-05T23:28:00.523027: step 2409, loss 0.58703.
Train: 2018-08-05T23:28:00.679241: step 2410, loss 0.595813.
Test: 2018-08-05T23:28:00.904611: step 2410, loss 0.548811.
Train: 2018-08-05T23:28:01.076451: step 2411, loss 0.538025.
Train: 2018-08-05T23:28:01.224619: step 2412, loss 0.496295.
Train: 2018-08-05T23:28:01.389782: step 2413, loss 0.529773.
Train: 2018-08-05T23:28:01.546304: step 2414, loss 0.537916.
Train: 2018-08-05T23:28:01.702312: step 2415, loss 0.57946.
Train: 2018-08-05T23:28:01.842593: step 2416, loss 0.491254.
Train: 2018-08-05T23:28:01.998832: step 2417, loss 0.529219.
Train: 2018-08-05T23:28:02.155052: step 2418, loss 0.587343.
Train: 2018-08-05T23:28:02.312157: step 2419, loss 0.571037.
Train: 2018-08-05T23:28:02.465191: step 2420, loss 0.638509.
Test: 2018-08-05T23:28:02.699512: step 2420, loss 0.548397.
Train: 2018-08-05T23:28:02.849626: step 2421, loss 0.638336.
Train: 2018-08-05T23:28:03.021491: step 2422, loss 0.588231.
Train: 2018-08-05T23:28:03.175045: step 2423, loss 0.503518.
Train: 2018-08-05T23:28:03.334167: step 2424, loss 0.587789.
Train: 2018-08-05T23:28:03.490405: step 2425, loss 0.587535.
Train: 2018-08-05T23:28:03.646594: step 2426, loss 0.596695.
Train: 2018-08-05T23:28:03.798712: step 2427, loss 0.528884.
Train: 2018-08-05T23:28:03.966758: step 2428, loss 0.554328.
Train: 2018-08-05T23:28:04.122979: step 2429, loss 0.646694.
Train: 2018-08-05T23:28:04.274189: step 2430, loss 0.613023.
Test: 2018-08-05T23:28:04.515550: step 2430, loss 0.548583.
Train: 2018-08-05T23:28:04.671758: step 2431, loss 0.537637.
Train: 2018-08-05T23:28:04.837441: step 2432, loss 0.595973.
Train: 2018-08-05T23:28:04.975366: step 2433, loss 0.470893.
Train: 2018-08-05T23:28:05.147231: step 2434, loss 0.638066.
Train: 2018-08-05T23:28:05.298536: step 2435, loss 0.58761.
Train: 2018-08-05T23:28:05.450793: step 2436, loss 0.53771.
Train: 2018-08-05T23:28:05.622623: step 2437, loss 0.545987.
Train: 2018-08-05T23:28:05.777880: step 2438, loss 0.64573.
Train: 2018-08-05T23:28:05.942820: step 2439, loss 0.653787.
Train: 2018-08-05T23:28:06.099035: step 2440, loss 0.537726.
Test: 2018-08-05T23:28:06.341147: step 2440, loss 0.548906.
Train: 2018-08-05T23:28:06.490068: step 2441, loss 0.529907.
Train: 2018-08-05T23:28:06.661937: step 2442, loss 0.538212.
Train: 2018-08-05T23:28:06.825585: step 2443, loss 0.661383.
Train: 2018-08-05T23:28:06.974201: step 2444, loss 0.472165.
Train: 2018-08-05T23:28:07.130385: step 2445, loss 0.61202.
Train: 2018-08-05T23:28:07.281696: step 2446, loss 0.554758.
Train: 2018-08-05T23:28:07.442693: step 2447, loss 0.571013.
Train: 2018-08-05T23:28:07.598903: step 2448, loss 0.562864.
Train: 2018-08-05T23:28:07.766802: step 2449, loss 0.52225.
Train: 2018-08-05T23:28:07.919203: step 2450, loss 0.505439.
Test: 2018-08-05T23:28:08.168624: step 2450, loss 0.54891.
Train: 2018-08-05T23:28:08.333863: step 2451, loss 0.480264.
Train: 2018-08-05T23:28:08.482510: step 2452, loss 0.529448.
Train: 2018-08-05T23:28:08.642190: step 2453, loss 0.529553.
Train: 2018-08-05T23:28:08.795411: step 2454, loss 0.571047.
Train: 2018-08-05T23:28:09.004814: step 2455, loss 0.469826.
Train: 2018-08-05T23:28:09.161057: step 2456, loss 0.587986.
Train: 2018-08-05T23:28:09.329497: step 2457, loss 0.579521.
Train: 2018-08-05T23:28:09.482126: step 2458, loss 0.52831.
Train: 2018-08-05T23:28:09.622719: step 2459, loss 0.605529.
Train: 2018-08-05T23:28:09.795676: step 2460, loss 0.527981.
Test: 2018-08-05T23:28:10.035933: step 2460, loss 0.548002.
Train: 2018-08-05T23:28:10.192145: step 2461, loss 0.536387.
Train: 2018-08-05T23:28:10.357311: step 2462, loss 0.492856.
Train: 2018-08-05T23:28:10.513524: step 2463, loss 0.52757.
Train: 2018-08-05T23:28:10.667258: step 2464, loss 0.553766.
Train: 2018-08-05T23:28:10.836947: step 2465, loss 0.624444.
Train: 2018-08-05T23:28:10.997188: step 2466, loss 0.51837.
Train: 2018-08-05T23:28:11.153432: step 2467, loss 0.536043.
Train: 2018-08-05T23:28:11.305798: step 2468, loss 0.518148.
Train: 2018-08-05T23:28:11.459151: step 2469, loss 0.54485.
Train: 2018-08-05T23:28:11.615364: step 2470, loss 0.473042.
Test: 2018-08-05T23:28:11.848595: step 2470, loss 0.547705.
Train: 2018-08-05T23:28:12.036052: step 2471, loss 0.571874.
Train: 2018-08-05T23:28:12.192233: step 2472, loss 0.526694.
Train: 2018-08-05T23:28:12.348898: step 2473, loss 0.490318.
Train: 2018-08-05T23:28:12.516715: step 2474, loss 0.580957.
Train: 2018-08-05T23:28:12.676471: step 2475, loss 0.51734.
Train: 2018-08-05T23:28:12.832694: step 2476, loss 0.553843.
Train: 2018-08-05T23:28:12.988933: step 2477, loss 0.470939.
Train: 2018-08-05T23:28:13.145156: step 2478, loss 0.535457.
Train: 2018-08-05T23:28:13.315310: step 2479, loss 0.609499.
Train: 2018-08-05T23:28:13.469453: step 2480, loss 0.526113.
Test: 2018-08-05T23:28:13.709108: step 2480, loss 0.547732.
Train: 2018-08-05T23:28:13.862596: step 2481, loss 0.600605.
Train: 2018-08-05T23:28:14.034399: step 2482, loss 0.591442.
Train: 2018-08-05T23:28:14.190612: step 2483, loss 0.666004.
Train: 2018-08-05T23:28:14.340026: step 2484, loss 0.516715.
Train: 2018-08-05T23:28:14.505430: step 2485, loss 0.544654.
Train: 2018-08-05T23:28:14.654265: step 2486, loss 0.526144.
Train: 2018-08-05T23:28:14.821506: step 2487, loss 0.563435.
Train: 2018-08-05T23:28:14.973138: step 2488, loss 0.553874.
Train: 2018-08-05T23:28:15.129320: step 2489, loss 0.544799.
Train: 2018-08-05T23:28:15.281664: step 2490, loss 0.59095.
Test: 2018-08-05T23:28:15.528161: step 2490, loss 0.547691.
Train: 2018-08-05T23:28:15.672960: step 2491, loss 0.563146.
Train: 2018-08-05T23:28:15.846589: step 2492, loss 0.609103.
Train: 2018-08-05T23:28:16.002802: step 2493, loss 0.498865.
Train: 2018-08-05T23:28:16.159019: step 2494, loss 0.617969.
Train: 2018-08-05T23:28:16.327688: step 2495, loss 0.544802.
Train: 2018-08-05T23:28:16.472330: step 2496, loss 0.590283.
Train: 2018-08-05T23:28:16.626456: step 2497, loss 0.590017.
Train: 2018-08-05T23:28:16.802134: step 2498, loss 0.526636.
Train: 2018-08-05T23:28:16.957475: step 2499, loss 0.526778.
Train: 2018-08-05T23:28:17.113718: step 2500, loss 0.544787.
Test: 2018-08-05T23:28:17.346409: step 2500, loss 0.547707.
Train: 2018-08-05T23:28:18.096682: step 2501, loss 0.598769.
Train: 2018-08-05T23:28:18.258888: step 2502, loss 0.553568.
Train: 2018-08-05T23:28:18.410200: step 2503, loss 0.580679.
Train: 2018-08-05T23:28:18.566408: step 2504, loss 0.500302.
Train: 2018-08-05T23:28:18.704269: step 2505, loss 0.633966.
Train: 2018-08-05T23:28:18.871423: step 2506, loss 0.571525.
Train: 2018-08-05T23:28:19.027638: step 2507, loss 0.500564.
Train: 2018-08-05T23:28:19.183852: step 2508, loss 0.642293.
Train: 2018-08-05T23:28:19.347980: step 2509, loss 0.589072.
Train: 2018-08-05T23:28:19.504193: step 2510, loss 0.580105.
Test: 2018-08-05T23:28:19.744466: step 2510, loss 0.547887.
Train: 2018-08-05T23:28:19.913559: step 2511, loss 0.606369.
Train: 2018-08-05T23:28:20.069748: step 2512, loss 0.510261.
Train: 2018-08-05T23:28:20.232556: step 2513, loss 0.562519.
Train: 2018-08-05T23:28:20.397261: step 2514, loss 0.536503.
Train: 2018-08-05T23:28:20.560075: step 2515, loss 0.588427.
Train: 2018-08-05T23:28:20.707113: step 2516, loss 0.502072.
Train: 2018-08-05T23:28:20.870198: step 2517, loss 0.527956.
Train: 2018-08-05T23:28:21.045366: step 2518, loss 0.56252.
Train: 2018-08-05T23:28:21.203705: step 2519, loss 0.57974.
Train: 2018-08-05T23:28:21.354657: step 2520, loss 0.579738.
Test: 2018-08-05T23:28:21.603777: step 2520, loss 0.548069.
Train: 2018-08-05T23:28:21.784017: step 2521, loss 0.536639.
Train: 2018-08-05T23:28:21.935503: step 2522, loss 0.502187.
Train: 2018-08-05T23:28:22.082378: step 2523, loss 0.622937.
Train: 2018-08-05T23:28:22.241353: step 2524, loss 0.605645.
Train: 2018-08-05T23:28:22.408418: step 2525, loss 0.510791.
Train: 2018-08-05T23:28:22.550453: step 2526, loss 0.571163.
Train: 2018-08-05T23:28:22.710714: step 2527, loss 0.61415.
Train: 2018-08-05T23:28:22.904986: step 2528, loss 0.536688.
Train: 2018-08-05T23:28:23.055783: step 2529, loss 0.639778.
Train: 2018-08-05T23:28:23.218567: step 2530, loss 0.502505.
Test: 2018-08-05T23:28:23.452647: step 2530, loss 0.548147.
Train: 2018-08-05T23:28:23.608854: step 2531, loss 0.562511.
Train: 2018-08-05T23:28:23.783960: step 2532, loss 0.494012.
Train: 2018-08-05T23:28:23.944063: step 2533, loss 0.596742.
Train: 2018-08-05T23:28:24.103313: step 2534, loss 0.622524.
Train: 2018-08-05T23:28:24.252539: step 2535, loss 0.631031.
Train: 2018-08-05T23:28:24.408829: step 2536, loss 0.51984.
Train: 2018-08-05T23:28:24.570348: step 2537, loss 0.545434.
Train: 2018-08-05T23:28:24.726561: step 2538, loss 0.5539.
Train: 2018-08-05T23:28:24.885761: step 2539, loss 0.571073.
Train: 2018-08-05T23:28:25.039558: step 2540, loss 0.570946.
Test: 2018-08-05T23:28:25.273909: step 2540, loss 0.54826.
Train: 2018-08-05T23:28:25.432951: step 2541, loss 0.528566.
Train: 2018-08-05T23:28:25.589131: step 2542, loss 0.60507.
Train: 2018-08-05T23:28:25.745376: step 2543, loss 0.579539.
Train: 2018-08-05T23:28:25.908869: step 2544, loss 0.579492.
Train: 2018-08-05T23:28:26.065081: step 2545, loss 0.604868.
Train: 2018-08-05T23:28:26.221295: step 2546, loss 0.494782.
Train: 2018-08-05T23:28:26.376529: step 2547, loss 0.545525.
Train: 2018-08-05T23:28:26.532708: step 2548, loss 0.562476.
Train: 2018-08-05T23:28:26.704543: step 2549, loss 0.681045.
Train: 2018-08-05T23:28:26.862672: step 2550, loss 0.638666.
Test: 2018-08-05T23:28:27.096820: step 2550, loss 0.548439.
Train: 2018-08-05T23:28:27.268625: step 2551, loss 0.604589.
Train: 2018-08-05T23:28:27.423987: step 2552, loss 0.621224.
Train: 2018-08-05T23:28:27.580232: step 2553, loss 0.562578.
Train: 2018-08-05T23:28:27.736443: step 2554, loss 0.479568.
Train: 2018-08-05T23:28:27.905849: step 2555, loss 0.57097.
Train: 2018-08-05T23:28:28.060598: step 2556, loss 0.562678.
Train: 2018-08-05T23:28:28.222937: step 2557, loss 0.521278.
Train: 2018-08-05T23:28:28.385929: step 2558, loss 0.546123.
Train: 2018-08-05T23:28:28.561962: step 2559, loss 0.570953.
Train: 2018-08-05T23:28:28.714440: step 2560, loss 0.537869.
Test: 2018-08-05T23:28:28.943995: step 2560, loss 0.548766.
Train: 2018-08-05T23:28:29.107958: step 2561, loss 0.579191.
Train: 2018-08-05T23:28:29.299840: step 2562, loss 0.579198.
Train: 2018-08-05T23:28:29.462344: step 2563, loss 0.512867.
Train: 2018-08-05T23:28:29.609334: step 2564, loss 0.570936.
Train: 2018-08-05T23:28:29.773253: step 2565, loss 0.487723.
Train: 2018-08-05T23:28:29.930168: step 2566, loss 0.579246.
Train: 2018-08-05T23:28:30.101558: step 2567, loss 0.616191.
Train: 2018-08-05T23:28:30.267142: step 2568, loss 0.537413.
Train: 2018-08-05T23:28:30.431846: step 2569, loss 0.470219.
Train: 2018-08-05T23:28:30.588087: step 2570, loss 0.621525.
Test: 2018-08-05T23:28:30.829851: step 2570, loss 0.548379.
Train: 2018-08-05T23:28:30.987463: step 2571, loss 0.613102.
Train: 2018-08-05T23:28:31.143708: step 2572, loss 0.587857.
Train: 2018-08-05T23:28:31.297983: step 2573, loss 0.537194.
Train: 2018-08-05T23:28:31.447288: step 2574, loss 0.587902.
Train: 2018-08-05T23:28:31.603477: step 2575, loss 0.604781.
Train: 2018-08-05T23:28:31.766134: step 2576, loss 0.579407.
Train: 2018-08-05T23:28:31.916305: step 2577, loss 0.537123.
Train: 2018-08-05T23:28:32.072489: step 2578, loss 0.469479.
Train: 2018-08-05T23:28:32.234469: step 2579, loss 0.545544.
Train: 2018-08-05T23:28:32.385957: step 2580, loss 0.553992.
Test: 2018-08-05T23:28:32.635898: step 2580, loss 0.548241.
Train: 2018-08-05T23:28:32.789239: step 2581, loss 0.588059.
Train: 2018-08-05T23:28:32.939363: step 2582, loss 0.562439.
Train: 2018-08-05T23:28:33.095579: step 2583, loss 0.502726.
Train: 2018-08-05T23:28:33.253144: step 2584, loss 0.545341.
Train: 2018-08-05T23:28:33.415354: step 2585, loss 0.648277.
Train: 2018-08-05T23:28:33.563019: step 2586, loss 0.536686.
Train: 2018-08-05T23:28:33.726666: step 2587, loss 0.579646.
Train: 2018-08-05T23:28:33.877834: step 2588, loss 0.545252.
Train: 2018-08-05T23:28:34.034047: step 2589, loss 0.536533.
Train: 2018-08-05T23:28:34.194910: step 2590, loss 0.519388.
Test: 2018-08-05T23:28:34.442701: step 2590, loss 0.547997.
Train: 2018-08-05T23:28:34.598945: step 2591, loss 0.519065.
Train: 2018-08-05T23:28:34.757288: step 2592, loss 0.571041.
Train: 2018-08-05T23:28:34.924028: step 2593, loss 0.623551.
Train: 2018-08-05T23:28:35.080241: step 2594, loss 0.571219.
Train: 2018-08-05T23:28:35.242128: step 2595, loss 0.509828.
Train: 2018-08-05T23:28:35.408939: step 2596, loss 0.596531.
Train: 2018-08-05T23:28:35.570944: step 2597, loss 0.518947.
Train: 2018-08-05T23:28:35.738195: step 2598, loss 0.518655.
Train: 2018-08-05T23:28:35.886412: step 2599, loss 0.561972.
Train: 2018-08-05T23:28:36.042621: step 2600, loss 0.463826.
Test: 2018-08-05T23:28:36.293567: step 2600, loss 0.547659.
Train: 2018-08-05T23:28:37.002127: step 2601, loss 0.570827.
Train: 2018-08-05T23:28:37.170638: step 2602, loss 0.538573.
Train: 2018-08-05T23:28:37.326054: step 2603, loss 0.579741.
Train: 2018-08-05T23:28:37.477633: step 2604, loss 0.578979.
Train: 2018-08-05T23:28:37.633845: step 2605, loss 0.572063.
Train: 2018-08-05T23:28:37.796922: step 2606, loss 0.650465.
Train: 2018-08-05T23:28:37.951100: step 2607, loss 0.635245.
Train: 2018-08-05T23:28:38.114914: step 2608, loss 0.518845.
Train: 2018-08-05T23:28:38.264507: step 2609, loss 0.436866.
Train: 2018-08-05T23:28:38.414875: step 2610, loss 0.509748.
Test: 2018-08-05T23:28:38.669268: step 2610, loss 0.547713.
Train: 2018-08-05T23:28:38.825449: step 2611, loss 0.546806.
Train: 2018-08-05T23:28:38.967480: step 2612, loss 0.535773.
Train: 2018-08-05T23:28:39.123664: step 2613, loss 0.590038.
Train: 2018-08-05T23:28:39.273431: step 2614, loss 0.642666.
Train: 2018-08-05T23:28:39.430471: step 2615, loss 0.474177.
Train: 2018-08-05T23:28:39.586686: step 2616, loss 0.562514.
Train: 2018-08-05T23:28:39.740696: step 2617, loss 0.676694.
Train: 2018-08-05T23:28:39.891199: step 2618, loss 0.62384.
Train: 2018-08-05T23:28:40.071386: step 2619, loss 0.510354.
Train: 2018-08-05T23:28:40.236562: step 2620, loss 0.519359.
Test: 2018-08-05T23:28:40.468836: step 2620, loss 0.548022.
Train: 2018-08-05T23:28:40.625045: step 2621, loss 0.596988.
Train: 2018-08-05T23:28:40.777673: step 2622, loss 0.571143.
Train: 2018-08-05T23:28:40.938605: step 2623, loss 0.571119.
Train: 2018-08-05T23:28:41.094848: step 2624, loss 0.54529.
Train: 2018-08-05T23:28:41.241510: step 2625, loss 0.510955.
Train: 2018-08-05T23:28:41.398560: step 2626, loss 0.562492.
Train: 2018-08-05T23:28:41.570405: step 2627, loss 0.519491.
Train: 2018-08-05T23:28:41.718585: step 2628, loss 0.52811.
Train: 2018-08-05T23:28:41.883474: step 2629, loss 0.49341.
Train: 2018-08-05T23:28:42.039687: step 2630, loss 0.588495.
Test: 2018-08-05T23:28:42.278074: step 2630, loss 0.547985.
Train: 2018-08-05T23:28:42.430687: step 2631, loss 0.683789.
Train: 2018-08-05T23:28:42.586871: step 2632, loss 0.579768.
Train: 2018-08-05T23:28:42.747889: step 2633, loss 0.545258.
Train: 2018-08-05T23:28:42.913924: step 2634, loss 0.579793.
Train: 2018-08-05T23:28:43.054510: step 2635, loss 0.536631.
Train: 2018-08-05T23:28:43.208734: step 2636, loss 0.562455.
Train: 2018-08-05T23:28:43.376580: step 2637, loss 0.545257.
Train: 2018-08-05T23:28:43.532817: step 2638, loss 0.61433.
Train: 2018-08-05T23:28:43.688140: step 2639, loss 0.562371.
Train: 2018-08-05T23:28:43.851958: step 2640, loss 0.553793.
Test: 2018-08-05T23:28:44.082408: step 2640, loss 0.548076.
Train: 2018-08-05T23:28:44.249313: step 2641, loss 0.596937.
Train: 2018-08-05T23:28:44.398636: step 2642, loss 0.536481.
Train: 2018-08-05T23:28:44.554818: step 2643, loss 0.52817.
Train: 2018-08-05T23:28:44.702070: step 2644, loss 0.485197.
Train: 2018-08-05T23:28:44.867907: step 2645, loss 0.536731.
Train: 2018-08-05T23:28:45.008524: step 2646, loss 0.666048.
Train: 2018-08-05T23:28:45.175179: step 2647, loss 0.545265.
Train: 2018-08-05T23:28:45.329491: step 2648, loss 0.596825.
Train: 2018-08-05T23:28:45.477078: step 2649, loss 0.54522.
Train: 2018-08-05T23:28:45.617669: step 2650, loss 0.536436.
Test: 2018-08-05T23:28:45.867421: step 2650, loss 0.548064.
Train: 2018-08-05T23:28:46.021672: step 2651, loss 0.605641.
Train: 2018-08-05T23:28:46.185874: step 2652, loss 0.502168.
Train: 2018-08-05T23:28:46.336159: step 2653, loss 0.562504.
Train: 2018-08-05T23:28:46.492403: step 2654, loss 0.571244.
Train: 2018-08-05T23:28:46.648616: step 2655, loss 0.553782.
Train: 2018-08-05T23:28:46.795851: step 2656, loss 0.536636.
Train: 2018-08-05T23:28:46.955477: step 2657, loss 0.597143.
Train: 2018-08-05T23:28:47.111690: step 2658, loss 0.49329.
Train: 2018-08-05T23:28:47.270778: step 2659, loss 0.519296.
Train: 2018-08-05T23:28:47.430143: step 2660, loss 0.623063.
Test: 2018-08-05T23:28:47.680423: step 2660, loss 0.547973.
Train: 2018-08-05T23:28:47.835574: step 2661, loss 0.518965.
Train: 2018-08-05T23:28:48.002427: step 2662, loss 0.553681.
Train: 2018-08-05T23:28:48.150736: step 2663, loss 0.53675.
Train: 2018-08-05T23:28:48.304132: step 2664, loss 0.519155.
Train: 2018-08-05T23:28:48.460245: step 2665, loss 0.588644.
Train: 2018-08-05T23:28:48.628656: step 2666, loss 0.614921.
Train: 2018-08-05T23:28:48.779010: step 2667, loss 0.597251.
Train: 2018-08-05T23:28:48.943547: step 2668, loss 0.518939.
Train: 2018-08-05T23:28:49.115166: step 2669, loss 0.614941.
Train: 2018-08-05T23:28:49.266312: step 2670, loss 0.518824.
Test: 2018-08-05T23:28:49.512642: step 2670, loss 0.547885.
Train: 2018-08-05T23:28:49.692296: step 2671, loss 0.589018.
Train: 2018-08-05T23:28:49.851663: step 2672, loss 0.509912.
Train: 2018-08-05T23:28:50.008132: step 2673, loss 0.562544.
Train: 2018-08-05T23:28:50.160940: step 2674, loss 0.562565.
Train: 2018-08-05T23:28:50.334147: step 2675, loss 0.562724.
Train: 2018-08-05T23:28:50.498968: step 2676, loss 0.536512.
Train: 2018-08-05T23:28:50.647450: step 2677, loss 0.536366.
Train: 2018-08-05T23:28:50.803662: step 2678, loss 0.54509.
Train: 2018-08-05T23:28:50.977935: step 2679, loss 0.58908.
Train: 2018-08-05T23:28:51.134151: step 2680, loss 0.571116.
Test: 2018-08-05T23:28:51.375428: step 2680, loss 0.547857.
Train: 2018-08-05T23:28:51.534749: step 2681, loss 0.632443.
Train: 2018-08-05T23:28:51.690962: step 2682, loss 0.518656.
Train: 2018-08-05T23:28:51.850740: step 2683, loss 0.588679.
Train: 2018-08-05T23:28:52.005887: step 2684, loss 0.58003.
Train: 2018-08-05T23:28:52.155674: step 2685, loss 0.614883.
Train: 2018-08-05T23:28:52.327220: step 2686, loss 0.562624.
Train: 2018-08-05T23:28:52.465854: step 2687, loss 0.493232.
Train: 2018-08-05T23:28:52.637688: step 2688, loss 0.545094.
Train: 2018-08-05T23:28:52.778251: step 2689, loss 0.606045.
Train: 2018-08-05T23:28:52.958953: step 2690, loss 0.614289.
Test: 2018-08-05T23:28:53.196767: step 2690, loss 0.548024.
Train: 2018-08-05T23:28:53.343600: step 2691, loss 0.588499.
Train: 2018-08-05T23:28:53.512766: step 2692, loss 0.597024.
Train: 2018-08-05T23:28:53.668992: step 2693, loss 0.553888.
Train: 2018-08-05T23:28:53.826052: step 2694, loss 0.511034.
Train: 2018-08-05T23:28:53.978297: step 2695, loss 0.596592.
Train: 2018-08-05T23:28:54.134539: step 2696, loss 0.596434.
Train: 2018-08-05T23:28:54.290723: step 2697, loss 0.588031.
Train: 2018-08-05T23:28:54.449560: step 2698, loss 0.587856.
Train: 2018-08-05T23:28:54.605744: step 2699, loss 0.588087.
Train: 2018-08-05T23:28:54.761956: step 2700, loss 0.562534.
Test: 2018-08-05T23:28:55.011475: step 2700, loss 0.548431.
Train: 2018-08-05T23:28:55.714923: step 2701, loss 0.579276.
Train: 2018-08-05T23:28:55.898218: step 2702, loss 0.58762.
Train: 2018-08-05T23:28:56.052162: step 2703, loss 0.612725.
Train: 2018-08-05T23:28:56.225349: step 2704, loss 0.587684.
Train: 2018-08-05T23:28:56.373615: step 2705, loss 0.587504.
Train: 2018-08-05T23:28:56.527377: step 2706, loss 0.521169.
Train: 2018-08-05T23:28:56.683559: step 2707, loss 0.554401.
Train: 2018-08-05T23:28:56.847616: step 2708, loss 0.57917.
Train: 2018-08-05T23:28:56.996680: step 2709, loss 0.570802.
Train: 2018-08-05T23:28:57.148326: step 2710, loss 0.554472.
Test: 2018-08-05T23:28:57.401605: step 2710, loss 0.548954.
Train: 2018-08-05T23:28:57.550507: step 2711, loss 0.497156.
Train: 2018-08-05T23:28:57.706751: step 2712, loss 0.546219.
Train: 2018-08-05T23:28:57.865812: step 2713, loss 0.620229.
Train: 2018-08-05T23:28:58.017830: step 2714, loss 0.612105.
Train: 2018-08-05T23:28:58.176692: step 2715, loss 0.529841.
Train: 2018-08-05T23:28:58.334798: step 2716, loss 0.59565.
Train: 2018-08-05T23:28:58.488634: step 2717, loss 0.554413.
Train: 2018-08-05T23:28:58.644815: step 2718, loss 0.422291.
Train: 2018-08-05T23:28:58.807769: step 2719, loss 0.521494.
Train: 2018-08-05T23:28:58.952514: step 2720, loss 0.6618.
Test: 2018-08-05T23:28:59.196699: step 2720, loss 0.548701.
Train: 2018-08-05T23:28:59.358802: step 2721, loss 0.479591.
Train: 2018-08-05T23:28:59.518258: step 2722, loss 0.537441.
Train: 2018-08-05T23:28:59.674442: step 2723, loss 0.529069.
Train: 2018-08-05T23:28:59.833279: step 2724, loss 0.612853.
Train: 2018-08-05T23:28:59.979759: step 2725, loss 0.570942.
Train: 2018-08-05T23:29:00.151563: step 2726, loss 0.587689.
Train: 2018-08-05T23:29:00.292185: step 2727, loss 0.486313.
Train: 2018-08-05T23:29:00.457204: step 2728, loss 0.486093.
Train: 2018-08-05T23:29:00.613426: step 2729, loss 0.562357.
Train: 2018-08-05T23:29:00.769662: step 2730, loss 0.4511.
Test: 2018-08-05T23:29:01.013421: step 2730, loss 0.548034.
Train: 2018-08-05T23:29:01.200270: step 2731, loss 0.553832.
Train: 2018-08-05T23:29:01.369815: step 2732, loss 0.484338.
Train: 2018-08-05T23:29:01.548338: step 2733, loss 0.536355.
Train: 2018-08-05T23:29:01.722871: step 2734, loss 0.579868.
Train: 2018-08-05T23:29:01.895443: step 2735, loss 0.597935.
Train: 2018-08-05T23:29:02.078918: step 2736, loss 0.562378.
Train: 2018-08-05T23:29:02.246603: step 2737, loss 0.518165.
Train: 2018-08-05T23:29:02.408689: step 2738, loss 0.562831.
Train: 2018-08-05T23:29:02.564903: step 2739, loss 0.544845.
Train: 2018-08-05T23:29:02.721147: step 2740, loss 0.60773.
Test: 2018-08-05T23:29:02.955668: step 2740, loss 0.547667.
Train: 2018-08-05T23:29:03.125902: step 2741, loss 0.553695.
Train: 2018-08-05T23:29:03.284102: step 2742, loss 0.562704.
Train: 2018-08-05T23:29:03.432393: step 2743, loss 0.51782.
Train: 2018-08-05T23:29:03.588606: step 2744, loss 0.55351.
Train: 2018-08-05T23:29:03.744814: step 2745, loss 0.580725.
Train: 2018-08-05T23:29:03.917461: step 2746, loss 0.508524.
Train: 2018-08-05T23:29:04.074530: step 2747, loss 0.535761.
Train: 2018-08-05T23:29:04.263870: step 2748, loss 0.58105.
Train: 2018-08-05T23:29:04.431934: step 2749, loss 0.59009.
Train: 2018-08-05T23:29:04.588144: step 2750, loss 0.535673.
Test: 2018-08-05T23:29:04.832607: step 2750, loss 0.547644.
Train: 2018-08-05T23:29:04.994578: step 2751, loss 0.526641.
Train: 2018-08-05T23:29:05.149685: step 2752, loss 0.490071.
Train: 2018-08-05T23:29:05.301524: step 2753, loss 0.563071.
Train: 2018-08-05T23:29:05.456774: step 2754, loss 0.608372.
Train: 2018-08-05T23:29:05.612989: step 2755, loss 0.581196.
Train: 2018-08-05T23:29:05.769202: step 2756, loss 0.49004.
Train: 2018-08-05T23:29:05.936120: step 2757, loss 0.499193.
Train: 2018-08-05T23:29:06.092302: step 2758, loss 0.626631.
Train: 2018-08-05T23:29:06.232894: step 2759, loss 0.508007.
Train: 2018-08-05T23:29:06.416598: step 2760, loss 0.489821.
Test: 2018-08-05T23:29:06.653605: step 2760, loss 0.547641.
Train: 2018-08-05T23:29:06.841635: step 2761, loss 0.645185.
Train: 2018-08-05T23:29:06.984051: step 2762, loss 0.62694.
Train: 2018-08-05T23:29:07.140264: step 2763, loss 0.599455.
Train: 2018-08-05T23:29:07.314807: step 2764, loss 0.508349.
Train: 2018-08-05T23:29:07.489578: step 2765, loss 0.49935.
Train: 2018-08-05T23:29:07.633519: step 2766, loss 0.553789.
Train: 2018-08-05T23:29:07.805381: step 2767, loss 0.517192.
Train: 2018-08-05T23:29:07.954202: step 2768, loss 0.535591.
Train: 2018-08-05T23:29:08.122173: step 2769, loss 0.590104.
Train: 2018-08-05T23:29:08.288641: step 2770, loss 0.617311.
Test: 2018-08-05T23:29:08.524681: step 2770, loss 0.547647.
Train: 2018-08-05T23:29:08.680918: step 2771, loss 0.571793.
Train: 2018-08-05T23:29:08.851314: step 2772, loss 0.50854.
Train: 2018-08-05T23:29:09.009925: step 2773, loss 0.617055.
Train: 2018-08-05T23:29:09.166107: step 2774, loss 0.599.
Train: 2018-08-05T23:29:09.333304: step 2775, loss 0.571777.
Train: 2018-08-05T23:29:09.478837: step 2776, loss 0.473099.
Train: 2018-08-05T23:29:09.644405: step 2777, loss 0.669745.
Train: 2018-08-05T23:29:09.800618: step 2778, loss 0.589289.
Train: 2018-08-05T23:29:09.964117: step 2779, loss 0.527052.
Train: 2018-08-05T23:29:10.116810: step 2780, loss 0.607036.
Test: 2018-08-05T23:29:10.363526: step 2780, loss 0.547788.
Train: 2018-08-05T23:29:10.530462: step 2781, loss 0.536189.
Train: 2018-08-05T23:29:10.675003: step 2782, loss 0.536302.
Train: 2018-08-05T23:29:10.847501: step 2783, loss 0.562427.
Train: 2018-08-05T23:29:11.007255: step 2784, loss 0.571181.
Train: 2018-08-05T23:29:11.163501: step 2785, loss 0.588858.
Train: 2018-08-05T23:29:11.330241: step 2786, loss 0.605831.
Train: 2018-08-05T23:29:11.492699: step 2787, loss 0.5363.
Train: 2018-08-05T23:29:11.656342: step 2788, loss 0.570857.
Train: 2018-08-05T23:29:11.823979: step 2789, loss 0.545144.
Train: 2018-08-05T23:29:11.980131: step 2790, loss 0.536616.
Test: 2018-08-05T23:29:12.213481: step 2790, loss 0.548055.
Train: 2018-08-05T23:29:12.374758: step 2791, loss 0.571117.
Train: 2018-08-05T23:29:12.533696: step 2792, loss 0.528078.
Train: 2018-08-05T23:29:12.692426: step 2793, loss 0.605413.
Train: 2018-08-05T23:29:12.864206: step 2794, loss 0.588345.
Train: 2018-08-05T23:29:13.020419: step 2795, loss 0.579483.
Train: 2018-08-05T23:29:13.176606: step 2796, loss 0.630907.
Train: 2018-08-05T23:29:13.349188: step 2797, loss 0.579776.
Train: 2018-08-05T23:29:13.515145: step 2798, loss 0.588043.
Train: 2018-08-05T23:29:13.681623: step 2799, loss 0.562561.
Train: 2018-08-05T23:29:13.843224: step 2800, loss 0.528583.
Test: 2018-08-05T23:29:14.081862: step 2800, loss 0.548403.
Train: 2018-08-05T23:29:14.849070: step 2801, loss 0.59628.
Train: 2018-08-05T23:29:15.003136: step 2802, loss 0.545739.
Train: 2018-08-05T23:29:15.159322: step 2803, loss 0.52911.
Train: 2018-08-05T23:29:15.334683: step 2804, loss 0.529059.
Train: 2018-08-05T23:29:15.489255: step 2805, loss 0.595889.
Train: 2018-08-05T23:29:15.645467: step 2806, loss 0.436777.
Train: 2018-08-05T23:29:15.801680: step 2807, loss 0.570677.
Train: 2018-08-05T23:29:15.958865: step 2808, loss 0.562686.
Train: 2018-08-05T23:29:16.115112: step 2809, loss 0.554074.
Train: 2018-08-05T23:29:16.271289: step 2810, loss 0.596415.
Test: 2018-08-05T23:29:16.505686: step 2810, loss 0.548282.
Train: 2018-08-05T23:29:16.677492: step 2811, loss 0.460838.
Train: 2018-08-05T23:29:16.894281: step 2812, loss 0.630571.
Train: 2018-08-05T23:29:17.079759: step 2813, loss 0.605026.
Train: 2018-08-05T23:29:17.265268: step 2814, loss 0.613732.
Train: 2018-08-05T23:29:17.448781: step 2815, loss 0.60488.
Train: 2018-08-05T23:29:17.628788: step 2816, loss 0.596367.
Train: 2018-08-05T23:29:17.786714: step 2817, loss 0.58763.
Train: 2018-08-05T23:29:17.971748: step 2818, loss 0.571093.
Train: 2018-08-05T23:29:18.176275: step 2819, loss 0.562291.
Train: 2018-08-05T23:29:18.339838: step 2820, loss 0.638297.
Test: 2018-08-05T23:29:18.578860: step 2820, loss 0.548412.
Train: 2018-08-05T23:29:18.747408: step 2821, loss 0.571053.
Train: 2018-08-05T23:29:18.897243: step 2822, loss 0.579276.
Train: 2018-08-05T23:29:19.056125: step 2823, loss 0.579363.
Train: 2018-08-05T23:29:19.206232: step 2824, loss 0.487509.
Train: 2018-08-05T23:29:19.362450: step 2825, loss 0.587697.
Train: 2018-08-05T23:29:19.518662: step 2826, loss 0.6041.
Train: 2018-08-05T23:29:19.681302: step 2827, loss 0.529389.
Train: 2018-08-05T23:29:19.843767: step 2828, loss 0.529366.
Train: 2018-08-05T23:29:20.010069: step 2829, loss 0.595858.
Train: 2018-08-05T23:29:20.163742: step 2830, loss 0.579067.
Test: 2018-08-05T23:29:20.402345: step 2830, loss 0.548701.
Train: 2018-08-05T23:29:20.573224: step 2831, loss 0.570813.
Train: 2018-08-05T23:29:20.740097: step 2832, loss 0.512728.
Train: 2018-08-05T23:29:20.899347: step 2833, loss 0.595846.
Train: 2018-08-05T23:29:21.052037: step 2834, loss 0.554174.
Train: 2018-08-05T23:29:21.205457: step 2835, loss 0.51289.
Train: 2018-08-05T23:29:21.366678: step 2836, loss 0.512893.
Train: 2018-08-05T23:29:21.531107: step 2837, loss 0.637543.
Train: 2018-08-05T23:29:21.679676: step 2838, loss 0.520727.
Train: 2018-08-05T23:29:21.837488: step 2839, loss 0.57089.
Train: 2018-08-05T23:29:21.989637: step 2840, loss 0.54576.
Test: 2018-08-05T23:29:22.225797: step 2840, loss 0.548455.
Train: 2018-08-05T23:29:22.381555: step 2841, loss 0.520595.
Train: 2018-08-05T23:29:22.550344: step 2842, loss 0.562558.
Train: 2018-08-05T23:29:22.717312: step 2843, loss 0.579349.
Train: 2018-08-05T23:29:22.863081: step 2844, loss 0.570769.
Train: 2018-08-05T23:29:23.018388: step 2845, loss 0.486127.
Train: 2018-08-05T23:29:23.172275: step 2846, loss 0.528558.
Train: 2018-08-05T23:29:23.328488: step 2847, loss 0.545426.
Train: 2018-08-05T23:29:23.498775: step 2848, loss 0.588034.
Train: 2018-08-05T23:29:23.654708: step 2849, loss 0.579562.
Train: 2018-08-05T23:29:23.815390: step 2850, loss 0.571087.
Test: 2018-08-05T23:29:24.057300: step 2850, loss 0.548016.
Train: 2018-08-05T23:29:24.208268: step 2851, loss 0.536499.
Train: 2018-08-05T23:29:24.372517: step 2852, loss 0.562674.
Train: 2018-08-05T23:29:24.522445: step 2853, loss 0.588564.
Train: 2018-08-05T23:29:24.689039: step 2854, loss 0.580096.
Train: 2018-08-05T23:29:24.845253: step 2855, loss 0.631519.
Train: 2018-08-05T23:29:25.001465: step 2856, loss 0.519146.
Train: 2018-08-05T23:29:25.161001: step 2857, loss 0.544904.
Train: 2018-08-05T23:29:25.317184: step 2858, loss 0.571103.
Train: 2018-08-05T23:29:25.473427: step 2859, loss 0.631906.
Train: 2018-08-05T23:29:25.642796: step 2860, loss 0.571073.
Test: 2018-08-05T23:29:25.874673: step 2860, loss 0.547994.
Train: 2018-08-05T23:29:26.045093: step 2861, loss 0.571103.
Train: 2018-08-05T23:29:26.203904: step 2862, loss 0.536564.
Train: 2018-08-05T23:29:26.350749: step 2863, loss 0.588419.
Train: 2018-08-05T23:29:26.504277: step 2864, loss 0.553808.
Train: 2018-08-05T23:29:26.678508: step 2865, loss 0.528039.
Train: 2018-08-05T23:29:26.833145: step 2866, loss 0.631134.
Train: 2018-08-05T23:29:27.005957: step 2867, loss 0.613988.
Train: 2018-08-05T23:29:27.164336: step 2868, loss 0.656561.
Train: 2018-08-05T23:29:27.317285: step 2869, loss 0.544107.
Train: 2018-08-05T23:29:27.473523: step 2870, loss 0.571163.
Test: 2018-08-05T23:29:27.721542: step 2870, loss 0.548319.
Train: 2018-08-05T23:29:27.890396: step 2871, loss 0.579462.
Train: 2018-08-05T23:29:28.048779: step 2872, loss 0.62184.
Train: 2018-08-05T23:29:28.198399: step 2873, loss 0.503838.
Train: 2018-08-05T23:29:28.376124: step 2874, loss 0.56255.
Train: 2018-08-05T23:29:28.565836: step 2875, loss 0.546084.
Train: 2018-08-05T23:29:28.735318: step 2876, loss 0.612533.
Train: 2018-08-05T23:29:28.916833: step 2877, loss 0.571117.
Train: 2018-08-05T23:29:29.105585: step 2878, loss 0.546191.
Train: 2018-08-05T23:29:29.266176: step 2879, loss 0.546012.
Train: 2018-08-05T23:29:29.423561: step 2880, loss 0.562429.
Test: 2018-08-05T23:29:29.668907: step 2880, loss 0.548697.
Train: 2018-08-05T23:29:29.836971: step 2881, loss 0.496258.
Train: 2018-08-05T23:29:30.017202: step 2882, loss 0.562741.
Train: 2018-08-05T23:29:30.186866: step 2883, loss 0.554345.
Train: 2018-08-05T23:29:30.340582: step 2884, loss 0.554235.
Train: 2018-08-05T23:29:30.503087: step 2885, loss 0.529313.
Train: 2018-08-05T23:29:30.673501: step 2886, loss 0.495537.
Train: 2018-08-05T23:29:30.822519: step 2887, loss 0.537141.
Train: 2018-08-05T23:29:31.005190: step 2888, loss 0.613036.
Train: 2018-08-05T23:29:31.183738: step 2889, loss 0.57114.
Train: 2018-08-05T23:29:31.373205: step 2890, loss 0.503122.
Test: 2018-08-05T23:29:31.612342: step 2890, loss 0.548232.
Train: 2018-08-05T23:29:31.760080: step 2891, loss 0.579351.
Train: 2018-08-05T23:29:31.923807: step 2892, loss 0.536899.
Train: 2018-08-05T23:29:32.108281: step 2893, loss 0.528476.
Train: 2018-08-05T23:29:32.275479: step 2894, loss 0.588058.
Train: 2018-08-05T23:29:32.425240: step 2895, loss 0.545459.
Train: 2018-08-05T23:29:32.589801: step 2896, loss 0.536377.
Train: 2018-08-05T23:29:32.734354: step 2897, loss 0.571344.
Train: 2018-08-05T23:29:32.897445: step 2898, loss 0.519101.
Train: 2018-08-05T23:29:33.047973: step 2899, loss 0.492812.
Train: 2018-08-05T23:29:33.196756: step 2900, loss 0.59728.
Test: 2018-08-05T23:29:33.444719: step 2900, loss 0.547843.
Train: 2018-08-05T23:29:34.235911: step 2901, loss 0.518854.
Train: 2018-08-05T23:29:34.401469: step 2902, loss 0.53633.
Train: 2018-08-05T23:29:34.562623: step 2903, loss 0.562587.
Train: 2018-08-05T23:29:34.708755: step 2904, loss 0.536333.
Train: 2018-08-05T23:29:34.884208: step 2905, loss 0.606988.
Train: 2018-08-05T23:29:35.049643: step 2906, loss 0.491498.
Train: 2018-08-05T23:29:35.206220: step 2907, loss 0.571503.
Train: 2018-08-05T23:29:35.367758: step 2908, loss 0.526993.
Train: 2018-08-05T23:29:35.526343: step 2909, loss 0.580403.
Train: 2018-08-05T23:29:35.698653: step 2910, loss 0.526795.
Test: 2018-08-05T23:29:35.936167: step 2910, loss 0.547662.
Train: 2018-08-05T23:29:36.096791: step 2911, loss 0.472909.
Train: 2018-08-05T23:29:36.297819: step 2912, loss 0.526802.
Train: 2018-08-05T23:29:36.507256: step 2913, loss 0.598857.
Train: 2018-08-05T23:29:36.690766: step 2914, loss 0.571646.
Train: 2018-08-05T23:29:36.862308: step 2915, loss 0.607953.
Train: 2018-08-05T23:29:37.023911: step 2916, loss 0.60822.
Train: 2018-08-05T23:29:37.191455: step 2917, loss 0.581063.
Train: 2018-08-05T23:29:37.359976: step 2918, loss 0.608254.
Train: 2018-08-05T23:29:37.527562: step 2919, loss 0.571845.
Train: 2018-08-05T23:29:37.715026: step 2920, loss 0.499878.
Test: 2018-08-05T23:29:37.958377: step 2920, loss 0.547654.
Train: 2018-08-05T23:29:38.161832: step 2921, loss 0.589791.
Train: 2018-08-05T23:29:38.341351: step 2922, loss 0.61644.
Train: 2018-08-05T23:29:38.508435: step 2923, loss 0.598335.
Train: 2018-08-05T23:29:38.674368: step 2924, loss 0.509314.
Train: 2018-08-05T23:29:38.842883: step 2925, loss 0.500504.
Train: 2018-08-05T23:29:39.000925: step 2926, loss 0.535952.
Train: 2018-08-05T23:29:39.212268: step 2927, loss 0.588938.
Train: 2018-08-05T23:29:39.400762: step 2928, loss 0.553693.
Train: 2018-08-05T23:29:39.567349: step 2929, loss 0.544894.
Train: 2018-08-05T23:29:39.718848: step 2930, loss 0.536025.
Test: 2018-08-05T23:29:39.948627: step 2930, loss 0.547747.
Train: 2018-08-05T23:29:40.146856: step 2931, loss 0.562472.
Train: 2018-08-05T23:29:40.327356: step 2932, loss 0.553725.
Train: 2018-08-05T23:29:40.486274: step 2933, loss 0.571576.
Train: 2018-08-05T23:29:40.647787: step 2934, loss 0.597886.
Train: 2018-08-05T23:29:40.807171: step 2935, loss 0.562577.
Train: 2018-08-05T23:29:40.970852: step 2936, loss 0.641696.
Train: 2018-08-05T23:29:41.134578: step 2937, loss 0.641286.
Train: 2018-08-05T23:29:41.298143: step 2938, loss 0.536172.
Train: 2018-08-05T23:29:41.449727: step 2939, loss 0.597318.
Train: 2018-08-05T23:29:41.605939: step 2940, loss 0.562422.
Test: 2018-08-05T23:29:41.843887: step 2940, loss 0.547982.
Train: 2018-08-05T23:29:42.000100: step 2941, loss 0.588265.
Train: 2018-08-05T23:29:42.165833: step 2942, loss 0.570923.
Train: 2018-08-05T23:29:42.332420: step 2943, loss 0.562514.
Train: 2018-08-05T23:29:42.491992: step 2944, loss 0.613605.
Train: 2018-08-05T23:29:42.640330: step 2945, loss 0.596676.
Train: 2018-08-05T23:29:42.808781: step 2946, loss 0.536918.
Train: 2018-08-05T23:29:42.971002: step 2947, loss 0.596243.
Train: 2018-08-05T23:29:43.120334: step 2948, loss 0.579237.
Train: 2018-08-05T23:29:43.297176: step 2949, loss 0.545844.
Train: 2018-08-05T23:29:43.456553: step 2950, loss 0.562462.
Test: 2018-08-05T23:29:43.688572: step 2950, loss 0.54856.
Train: 2018-08-05T23:29:43.870681: step 2951, loss 0.512625.
Train: 2018-08-05T23:29:44.031888: step 2952, loss 0.554116.
Train: 2018-08-05T23:29:44.186020: step 2953, loss 0.529231.
Train: 2018-08-05T23:29:44.353148: step 2954, loss 0.512518.
Train: 2018-08-05T23:29:44.508012: step 2955, loss 0.51245.
Train: 2018-08-05T23:29:44.676921: step 2956, loss 0.579287.
Train: 2018-08-05T23:29:44.838866: step 2957, loss 0.571072.
Train: 2018-08-05T23:29:45.007398: step 2958, loss 0.587805.
Train: 2018-08-05T23:29:45.177975: step 2959, loss 0.629833.
Train: 2018-08-05T23:29:45.344496: step 2960, loss 0.579254.
Test: 2018-08-05T23:29:45.583018: step 2960, loss 0.548401.
Train: 2018-08-05T23:29:45.753839: step 2961, loss 0.5288.
Train: 2018-08-05T23:29:45.901841: step 2962, loss 0.587772.
Train: 2018-08-05T23:29:46.059634: step 2963, loss 0.537341.
Train: 2018-08-05T23:29:46.219251: step 2964, loss 0.545577.
Train: 2018-08-05T23:29:46.392045: step 2965, loss 0.604741.
Train: 2018-08-05T23:29:46.560763: step 2966, loss 0.562494.
Train: 2018-08-05T23:29:46.705954: step 2967, loss 0.50353.
Train: 2018-08-05T23:29:46.882188: step 2968, loss 0.604623.
Train: 2018-08-05T23:29:47.035328: step 2969, loss 0.562423.
Train: 2018-08-05T23:29:47.192737: step 2970, loss 0.520257.
Test: 2018-08-05T23:29:47.427079: step 2970, loss 0.5483.
Train: 2018-08-05T23:29:47.583292: step 2971, loss 0.494667.
Train: 2018-08-05T23:29:47.755133: step 2972, loss 0.570976.
Train: 2018-08-05T23:29:47.913307: step 2973, loss 0.605153.
Train: 2018-08-05T23:29:48.093847: step 2974, loss 0.536846.
Train: 2018-08-05T23:29:48.241056: step 2975, loss 0.596573.
Train: 2018-08-05T23:29:48.424246: step 2976, loss 0.579583.
Train: 2018-08-05T23:29:48.577524: step 2977, loss 0.579482.
Train: 2018-08-05T23:29:48.733738: step 2978, loss 0.519761.
Train: 2018-08-05T23:29:48.889953: step 2979, loss 0.528154.
Train: 2018-08-05T23:29:49.057605: step 2980, loss 0.442611.
Test: 2018-08-05T23:29:49.295327: step 2980, loss 0.548044.
Train: 2018-08-05T23:29:49.453014: step 2981, loss 0.605748.
Train: 2018-08-05T23:29:49.608124: step 2982, loss 0.570978.
Train: 2018-08-05T23:29:49.779984: step 2983, loss 0.458696.
Train: 2018-08-05T23:29:49.945387: step 2984, loss 0.553767.
Train: 2018-08-05T23:29:50.109425: step 2985, loss 0.597252.
Train: 2018-08-05T23:29:50.263314: step 2986, loss 0.580005.
Train: 2018-08-05T23:29:50.433258: step 2987, loss 0.641089.
Train: 2018-08-05T23:29:50.592341: step 2988, loss 0.61487.
Train: 2018-08-05T23:29:50.764175: step 2989, loss 0.54512.
Train: 2018-08-05T23:29:50.917434: step 2990, loss 0.588764.
Test: 2018-08-05T23:29:51.148263: step 2990, loss 0.547869.
Train: 2018-08-05T23:29:51.311662: step 2991, loss 0.501373.
Train: 2018-08-05T23:29:51.463065: step 2992, loss 0.527723.
Train: 2018-08-05T23:29:51.623972: step 2993, loss 0.614963.
Train: 2018-08-05T23:29:51.795232: step 2994, loss 0.606057.
Train: 2018-08-05T23:29:51.947527: step 2995, loss 0.536371.
Train: 2018-08-05T23:29:52.108335: step 2996, loss 0.60589.
Train: 2018-08-05T23:29:52.264550: step 2997, loss 0.510225.
Train: 2018-08-05T23:29:52.416828: step 2998, loss 0.60587.
Train: 2018-08-05T23:29:52.577763: step 2999, loss 0.527665.
Train: 2018-08-05T23:29:52.734007: step 3000, loss 0.545249.
Test: 2018-08-05T23:29:52.961823: step 3000, loss 0.547951.
Train: 2018-08-05T23:29:53.679114: step 3001, loss 0.527785.
Train: 2018-08-05T23:29:53.835327: step 3002, loss 0.571008.
Train: 2018-08-05T23:29:53.991293: step 3003, loss 0.605725.
Train: 2018-08-05T23:29:54.155426: step 3004, loss 0.579726.
Train: 2018-08-05T23:29:54.311609: step 3005, loss 0.570945.
Train: 2018-08-05T23:29:54.461973: step 3006, loss 0.545166.
Train: 2018-08-05T23:29:54.623767: step 3007, loss 0.52799.
Train: 2018-08-05T23:29:54.779981: step 3008, loss 0.622985.
Train: 2018-08-05T23:29:54.945769: step 3009, loss 0.553675.
Train: 2018-08-05T23:29:55.092869: step 3010, loss 0.631424.
Test: 2018-08-05T23:29:55.327189: step 3010, loss 0.548057.
Train: 2018-08-05T23:29:55.494092: step 3011, loss 0.553841.
Train: 2018-08-05T23:29:55.638665: step 3012, loss 0.648241.
Train: 2018-08-05T23:29:55.794879: step 3013, loss 0.579458.
Train: 2018-08-05T23:29:55.949851: step 3014, loss 0.64762.
Train: 2018-08-05T23:29:56.116682: step 3015, loss 0.570943.
Train: 2018-08-05T23:29:56.272886: step 3016, loss 0.54549.
Train: 2018-08-05T23:29:56.423202: step 3017, loss 0.528944.
Train: 2018-08-05T23:29:56.582279: step 3018, loss 0.570763.
Train: 2018-08-05T23:29:56.725923: step 3019, loss 0.554251.
Train: 2018-08-05T23:29:56.876236: step 3020, loss 0.526964.
Test: 2018-08-05T23:29:57.123610: step 3020, loss 0.54858.
Train: 2018-08-05T23:29:57.279818: step 3021, loss 0.512542.
Train: 2018-08-05T23:29:57.432039: step 3022, loss 0.570709.
Train: 2018-08-05T23:29:57.584330: step 3023, loss 0.579161.
Train: 2018-08-05T23:29:57.740574: step 3024, loss 0.512555.
Train: 2018-08-05T23:29:57.907607: step 3025, loss 0.545854.
Train: 2018-08-05T23:29:58.048201: step 3026, loss 0.604324.
Train: 2018-08-05T23:29:58.209966: step 3027, loss 0.52058.
Train: 2018-08-05T23:29:58.376919: step 3028, loss 0.553979.
Train: 2018-08-05T23:29:58.533162: step 3029, loss 0.621394.
Train: 2018-08-05T23:29:58.694195: step 3030, loss 0.554286.
Test: 2018-08-05T23:29:58.922800: step 3030, loss 0.548378.
Train: 2018-08-05T23:29:59.084671: step 3031, loss 0.655173.
Train: 2018-08-05T23:29:59.245645: step 3032, loss 0.528916.
Train: 2018-08-05T23:29:59.410469: step 3033, loss 0.646445.
Train: 2018-08-05T23:29:59.570508: step 3034, loss 0.461893.
Train: 2018-08-05T23:29:59.717106: step 3035, loss 0.596083.
Train: 2018-08-05T23:29:59.883056: step 3036, loss 0.596203.
Train: 2018-08-05T23:30:00.025678: step 3037, loss 0.528974.
Train: 2018-08-05T23:30:00.193575: step 3038, loss 0.512117.
Train: 2018-08-05T23:30:00.352204: step 3039, loss 0.528956.
Train: 2018-08-05T23:30:00.501150: step 3040, loss 0.587665.
Test: 2018-08-05T23:30:00.739594: step 3040, loss 0.548371.
Train: 2018-08-05T23:30:00.897871: step 3041, loss 0.570994.
Train: 2018-08-05T23:30:01.060255: step 3042, loss 0.537093.
Train: 2018-08-05T23:30:01.216843: step 3043, loss 0.562463.
Train: 2018-08-05T23:30:01.373943: step 3044, loss 0.613184.
Train: 2018-08-05T23:30:01.515920: step 3045, loss 0.596417.
Train: 2018-08-05T23:30:01.677766: step 3046, loss 0.554034.
Train: 2018-08-05T23:30:01.834013: step 3047, loss 0.587844.
Train: 2018-08-05T23:30:01.992192: step 3048, loss 0.554078.
Train: 2018-08-05T23:30:02.154048: step 3049, loss 0.53713.
Train: 2018-08-05T23:30:02.310259: step 3050, loss 0.554041.
Test: 2018-08-05T23:30:02.556783: step 3050, loss 0.548292.
Train: 2018-08-05T23:30:02.709749: step 3051, loss 0.655561.
Train: 2018-08-05T23:30:02.875687: step 3052, loss 0.587863.
Train: 2018-08-05T23:30:03.016279: step 3053, loss 0.60477.
Train: 2018-08-05T23:30:03.178060: step 3054, loss 0.621402.
Train: 2018-08-05T23:30:03.334243: step 3055, loss 0.50382.
Train: 2018-08-05T23:30:03.501194: step 3056, loss 0.596071.
Train: 2018-08-05T23:30:03.662272: step 3057, loss 0.537501.
Train: 2018-08-05T23:30:03.818486: step 3058, loss 0.529199.
Train: 2018-08-05T23:30:03.979004: step 3059, loss 0.512606.
Train: 2018-08-05T23:30:04.146754: step 3060, loss 0.562577.
Test: 2018-08-05T23:30:04.382216: step 3060, loss 0.548519.
Train: 2018-08-05T23:30:04.538427: step 3061, loss 0.537423.
Train: 2018-08-05T23:30:04.700391: step 3062, loss 0.579168.
Train: 2018-08-05T23:30:04.860006: step 3063, loss 0.512183.
Train: 2018-08-05T23:30:05.009026: step 3064, loss 0.587832.
Train: 2018-08-05T23:30:05.161521: step 3065, loss 0.587807.
Train: 2018-08-05T23:30:05.317736: step 3066, loss 0.545637.
Train: 2018-08-05T23:30:05.469040: step 3067, loss 0.520312.
Train: 2018-08-05T23:30:05.630896: step 3068, loss 0.596134.
Train: 2018-08-05T23:30:05.787080: step 3069, loss 0.537052.
Train: 2018-08-05T23:30:05.959543: step 3070, loss 0.630369.
Test: 2018-08-05T23:30:06.205872: step 3070, loss 0.54825.
Train: 2018-08-05T23:30:06.389414: step 3071, loss 0.55405.
Train: 2018-08-05T23:30:06.571925: step 3072, loss 0.54552.
Train: 2018-08-05T23:30:06.755403: step 3073, loss 0.570934.
Train: 2018-08-05T23:30:06.944662: step 3074, loss 0.579443.
Train: 2018-08-05T23:30:07.104208: step 3075, loss 0.553883.
Train: 2018-08-05T23:30:07.258392: step 3076, loss 0.511524.
Train: 2018-08-05T23:30:07.424947: step 3077, loss 0.511348.
Train: 2018-08-05T23:30:07.598482: step 3078, loss 0.553819.
Train: 2018-08-05T23:30:07.778034: step 3079, loss 0.588186.
Train: 2018-08-05T23:30:07.957926: step 3080, loss 0.579505.
Test: 2018-08-05T23:30:08.205957: step 3080, loss 0.548067.
Train: 2018-08-05T23:30:08.381407: step 3081, loss 0.519518.
Train: 2018-08-05T23:30:08.543132: step 3082, loss 0.53657.
Train: 2018-08-05T23:30:08.699781: step 3083, loss 0.519402.
Train: 2018-08-05T23:30:08.875231: step 3084, loss 0.501989.
Train: 2018-08-05T23:30:09.106491: step 3085, loss 0.597122.
Train: 2018-08-05T23:30:09.280246: step 3086, loss 0.614672.
Train: 2018-08-05T23:30:09.436691: step 3087, loss 0.518913.
Train: 2018-08-05T23:30:09.602713: step 3088, loss 0.492663.
Train: 2018-08-05T23:30:09.770265: step 3089, loss 0.553746.
Train: 2018-08-05T23:30:09.938789: step 3090, loss 0.448338.
Test: 2018-08-05T23:30:10.179145: step 3090, loss 0.547753.
Train: 2018-08-05T23:30:10.348726: step 3091, loss 0.54483.
Train: 2018-08-05T23:30:10.514726: step 3092, loss 0.571419.
Train: 2018-08-05T23:30:10.680947: step 3093, loss 0.651487.
Train: 2018-08-05T23:30:10.834701: step 3094, loss 0.535894.
Train: 2018-08-05T23:30:10.990914: step 3095, loss 0.518129.
Train: 2018-08-05T23:30:11.159005: step 3096, loss 0.562772.
Train: 2018-08-05T23:30:11.299597: step 3097, loss 0.553713.
Train: 2018-08-05T23:30:11.471431: step 3098, loss 0.562466.
Train: 2018-08-05T23:30:11.621638: step 3099, loss 0.526655.
Train: 2018-08-05T23:30:11.787630: step 3100, loss 0.526784.
Test: 2018-08-05T23:30:12.024338: step 3100, loss 0.547631.
Train: 2018-08-05T23:30:12.762991: step 3101, loss 0.544755.
Train: 2018-08-05T23:30:12.919231: step 3102, loss 0.517554.
Train: 2018-08-05T23:30:13.086449: step 3103, loss 0.526561.
Train: 2018-08-05T23:30:13.243079: step 3104, loss 0.535668.
Train: 2018-08-05T23:30:13.403855: step 3105, loss 0.562897.
Train: 2018-08-05T23:30:13.565646: step 3106, loss 0.553924.
Train: 2018-08-05T23:30:13.716306: step 3107, loss 0.517199.
Train: 2018-08-05T23:30:13.872521: step 3108, loss 0.581013.
Train: 2018-08-05T23:30:14.029453: step 3109, loss 0.581305.
Train: 2018-08-05T23:30:14.185665: step 3110, loss 0.590434.
Test: 2018-08-05T23:30:14.419988: step 3110, loss 0.54761.
Train: 2018-08-05T23:30:14.572203: step 3111, loss 0.590359.
Train: 2018-08-05T23:30:14.728417: step 3112, loss 0.626995.
Train: 2018-08-05T23:30:14.884660: step 3113, loss 0.52636.
Train: 2018-08-05T23:30:15.041474: step 3114, loss 0.535584.
Train: 2018-08-05T23:30:15.191823: step 3115, loss 0.508266.
Train: 2018-08-05T23:30:15.348066: step 3116, loss 0.553908.
Train: 2018-08-05T23:30:15.515100: step 3117, loss 0.508323.
Train: 2018-08-05T23:30:15.678644: step 3118, loss 0.490226.
Train: 2018-08-05T23:30:15.849207: step 3119, loss 0.599268.
Train: 2018-08-05T23:30:16.023720: step 3120, loss 0.653692.
Test: 2018-08-05T23:30:16.265076: step 3120, loss 0.547612.
Train: 2018-08-05T23:30:16.439608: step 3121, loss 0.535536.
Train: 2018-08-05T23:30:16.610994: step 3122, loss 0.562767.
Train: 2018-08-05T23:30:16.777548: step 3123, loss 0.526601.
Train: 2018-08-05T23:30:16.947095: step 3124, loss 0.562758.
Train: 2018-08-05T23:30:17.129631: step 3125, loss 0.626072.
Train: 2018-08-05T23:30:17.316108: step 3126, loss 0.625832.
Train: 2018-08-05T23:30:17.488671: step 3127, loss 0.562565.
Train: 2018-08-05T23:30:17.652208: step 3128, loss 0.589477.
Train: 2018-08-05T23:30:17.813776: step 3129, loss 0.589403.
Train: 2018-08-05T23:30:17.974350: step 3130, loss 0.500435.
Test: 2018-08-05T23:30:18.219707: step 3130, loss 0.547723.
Train: 2018-08-05T23:30:18.403218: step 3131, loss 0.624543.
Train: 2018-08-05T23:30:18.571750: step 3132, loss 0.571299.
Train: 2018-08-05T23:30:18.753265: step 3133, loss 0.615246.
Train: 2018-08-05T23:30:18.924836: step 3134, loss 0.536269.
Train: 2018-08-05T23:30:19.097345: step 3135, loss 0.55371.
Train: 2018-08-05T23:30:19.292827: step 3136, loss 0.588562.
Train: 2018-08-05T23:30:19.467354: step 3137, loss 0.510551.
Train: 2018-08-05T23:30:19.648872: step 3138, loss 0.545177.
Train: 2018-08-05T23:30:19.829386: step 3139, loss 0.605505.
Train: 2018-08-05T23:30:19.998933: step 3140, loss 0.562479.
Test: 2018-08-05T23:30:20.237766: step 3140, loss 0.548062.
Train: 2018-08-05T23:30:20.397014: step 3141, loss 0.570959.
Train: 2018-08-05T23:30:20.597478: step 3142, loss 0.605267.
Train: 2018-08-05T23:30:20.768022: step 3143, loss 0.562373.
Train: 2018-08-05T23:30:20.919766: step 3144, loss 0.579465.
Train: 2018-08-05T23:30:21.076011: step 3145, loss 0.579439.
Train: 2018-08-05T23:30:21.235013: step 3146, loss 0.604777.
Train: 2018-08-05T23:30:21.394856: step 3147, loss 0.477998.
Train: 2018-08-05T23:30:21.550061: step 3148, loss 0.520297.
Train: 2018-08-05T23:30:21.712192: step 3149, loss 0.537141.
Train: 2018-08-05T23:30:21.870964: step 3150, loss 0.554021.
Test: 2018-08-05T23:30:22.109385: step 3150, loss 0.548306.
Train: 2018-08-05T23:30:22.302867: step 3151, loss 0.554017.
Train: 2018-08-05T23:30:22.457896: step 3152, loss 0.562464.
Train: 2018-08-05T23:30:22.614109: step 3153, loss 0.554045.
Train: 2018-08-05T23:30:22.770323: step 3154, loss 0.511587.
Train: 2018-08-05T23:30:22.926536: step 3155, loss 0.68142.
Train: 2018-08-05T23:30:23.082750: step 3156, loss 0.587919.
Train: 2018-08-05T23:30:23.254616: step 3157, loss 0.63029.
Train: 2018-08-05T23:30:23.410829: step 3158, loss 0.579398.
Train: 2018-08-05T23:30:23.551415: step 3159, loss 0.57091.
Train: 2018-08-05T23:30:23.707633: step 3160, loss 0.621463.
Test: 2018-08-05T23:30:23.941922: step 3160, loss 0.548419.
Train: 2018-08-05T23:30:24.113757: step 3161, loss 0.503689.
Train: 2018-08-05T23:30:24.269971: step 3162, loss 0.579277.
Train: 2018-08-05T23:30:24.422453: step 3163, loss 0.579307.
Train: 2018-08-05T23:30:24.578639: step 3164, loss 0.529033.
Train: 2018-08-05T23:30:24.743014: step 3165, loss 0.537408.
Train: 2018-08-05T23:30:24.886929: step 3166, loss 0.604246.
Train: 2018-08-05T23:30:25.046389: step 3167, loss 0.579133.
Train: 2018-08-05T23:30:25.202577: step 3168, loss 0.587552.
Train: 2018-08-05T23:30:25.358822: step 3169, loss 0.537511.
Train: 2018-08-05T23:30:25.515036: step 3170, loss 0.570864.
Test: 2018-08-05T23:30:25.749323: step 3170, loss 0.548601.
Train: 2018-08-05T23:30:25.905574: step 3171, loss 0.615708.
Train: 2018-08-05T23:30:26.061783: step 3172, loss 0.595701.
Train: 2018-08-05T23:30:26.217989: step 3173, loss 0.521139.
Train: 2018-08-05T23:30:26.374176: step 3174, loss 0.620572.
Train: 2018-08-05T23:30:26.514800: step 3175, loss 0.4964.
Train: 2018-08-05T23:30:26.686635: step 3176, loss 0.554298.
Train: 2018-08-05T23:30:26.842842: step 3177, loss 0.603984.
Train: 2018-08-05T23:30:27.003469: step 3178, loss 0.58735.
Train: 2018-08-05T23:30:27.159676: step 3179, loss 0.48817.
Train: 2018-08-05T23:30:27.315864: step 3180, loss 0.545981.
Test: 2018-08-05T23:30:27.550184: step 3180, loss 0.548658.
Train: 2018-08-05T23:30:27.706429: step 3181, loss 0.512816.
Train: 2018-08-05T23:30:27.862611: step 3182, loss 0.587431.
Train: 2018-08-05T23:30:28.017010: step 3183, loss 0.52916.
Train: 2018-08-05T23:30:28.173230: step 3184, loss 0.58758.
Train: 2018-08-05T23:30:28.329412: step 3185, loss 0.545716.
Train: 2018-08-05T23:30:28.485657: step 3186, loss 0.520482.
Train: 2018-08-05T23:30:28.657486: step 3187, loss 0.528771.
Train: 2018-08-05T23:30:28.813706: step 3188, loss 0.604676.
Train: 2018-08-05T23:30:28.989612: step 3189, loss 0.469099.
Train: 2018-08-05T23:30:29.145824: step 3190, loss 0.562482.
Test: 2018-08-05T23:30:29.380145: step 3190, loss 0.548109.
Train: 2018-08-05T23:30:29.536358: step 3191, loss 0.613677.
Train: 2018-08-05T23:30:29.692605: step 3192, loss 0.596597.
Train: 2018-08-05T23:30:29.848810: step 3193, loss 0.510964.
Train: 2018-08-05T23:30:30.005023: step 3194, loss 0.648446.
Train: 2018-08-05T23:30:30.176831: step 3195, loss 0.536682.
Train: 2018-08-05T23:30:30.333071: step 3196, loss 0.631354.
Train: 2018-08-05T23:30:30.489291: step 3197, loss 0.519412.
Train: 2018-08-05T23:30:30.645498: step 3198, loss 0.476292.
Train: 2018-08-05T23:30:30.801721: step 3199, loss 0.622696.
Train: 2018-08-05T23:30:30.957930: step 3200, loss 0.519285.
Test: 2018-08-05T23:30:31.192249: step 3200, loss 0.547955.
Train: 2018-08-05T23:30:31.910825: step 3201, loss 0.54528.
Train: 2018-08-05T23:30:32.067037: step 3202, loss 0.571087.
Train: 2018-08-05T23:30:32.223258: step 3203, loss 0.666553.
Train: 2018-08-05T23:30:32.379470: step 3204, loss 0.545115.
Train: 2018-08-05T23:30:32.535686: step 3205, loss 0.493125.
Train: 2018-08-05T23:30:32.691894: step 3206, loss 0.5971.
Train: 2018-08-05T23:30:32.848110: step 3207, loss 0.605859.
Train: 2018-08-05T23:30:33.004324: step 3208, loss 0.588357.
Train: 2018-08-05T23:30:33.144884: step 3209, loss 0.545131.
Train: 2018-08-05T23:30:33.316718: step 3210, loss 0.588318.
Test: 2018-08-05T23:30:33.551069: step 3210, loss 0.548007.
Train: 2018-08-05T23:30:33.707284: step 3211, loss 0.614118.
Train: 2018-08-05T23:30:33.879085: step 3212, loss 0.528126.
Train: 2018-08-05T23:30:34.019703: step 3213, loss 0.450963.
Train: 2018-08-05T23:30:34.175927: step 3214, loss 0.519531.
Train: 2018-08-05T23:30:34.332104: step 3215, loss 0.596781.
Train: 2018-08-05T23:30:34.488343: step 3216, loss 0.553984.
Train: 2018-08-05T23:30:34.644562: step 3217, loss 0.510883.
Train: 2018-08-05T23:30:34.816391: step 3218, loss 0.622633.
Train: 2018-08-05T23:30:34.968308: step 3219, loss 0.519244.
Train: 2018-08-05T23:30:35.124516: step 3220, loss 0.579807.
Test: 2018-08-05T23:30:35.374448: step 3220, loss 0.547944.
Train: 2018-08-05T23:30:35.530676: step 3221, loss 0.588173.
Train: 2018-08-05T23:30:35.686858: step 3222, loss 0.545228.
Train: 2018-08-05T23:30:35.858724: step 3223, loss 0.623105.
Train: 2018-08-05T23:30:36.014761: step 3224, loss 0.536387.
Train: 2018-08-05T23:30:36.171013: step 3225, loss 0.570938.
Train: 2018-08-05T23:30:36.327221: step 3226, loss 0.527913.
Train: 2018-08-05T23:30:36.483425: step 3227, loss 0.502006.
Train: 2018-08-05T23:30:36.639613: step 3228, loss 0.545321.
Train: 2018-08-05T23:30:36.795827: step 3229, loss 0.545052.
Train: 2018-08-05T23:30:36.952066: step 3230, loss 0.545177.
Test: 2018-08-05T23:30:37.186360: step 3230, loss 0.547883.
Train: 2018-08-05T23:30:37.342598: step 3231, loss 0.545207.
Train: 2018-08-05T23:30:37.506301: step 3232, loss 0.641031.
Train: 2018-08-05T23:30:37.652266: step 3233, loss 0.571281.
Train: 2018-08-05T23:30:37.824068: step 3234, loss 0.518898.
Train: 2018-08-05T23:30:37.978363: step 3235, loss 0.571119.
Train: 2018-08-05T23:30:38.134576: step 3236, loss 0.527587.
Train: 2018-08-05T23:30:38.290789: step 3237, loss 0.658499.
Train: 2018-08-05T23:30:38.447036: step 3238, loss 0.518765.
Train: 2018-08-05T23:30:38.603216: step 3239, loss 0.553729.
Train: 2018-08-05T23:30:38.759429: step 3240, loss 0.536433.
Test: 2018-08-05T23:30:38.998307: step 3240, loss 0.547869.
Train: 2018-08-05T23:30:39.154546: step 3241, loss 0.623439.
Train: 2018-08-05T23:30:39.310732: step 3242, loss 0.458053.
Train: 2018-08-05T23:30:39.466946: step 3243, loss 0.553679.
Train: 2018-08-05T23:30:39.607538: step 3244, loss 0.553735.
Train: 2018-08-05T23:30:39.779372: step 3245, loss 0.562431.
Train: 2018-08-05T23:30:39.919964: step 3246, loss 0.597332.
Train: 2018-08-05T23:30:40.076178: step 3247, loss 0.588669.
Train: 2018-08-05T23:30:40.232391: step 3248, loss 0.562406.
Train: 2018-08-05T23:30:40.388630: step 3249, loss 0.588594.
Train: 2018-08-05T23:30:40.544817: step 3250, loss 0.536193.
Test: 2018-08-05T23:30:40.779137: step 3250, loss 0.547863.
Train: 2018-08-05T23:30:40.935350: step 3251, loss 0.544911.
Train: 2018-08-05T23:30:41.091563: step 3252, loss 0.545017.
Train: 2018-08-05T23:30:41.247777: step 3253, loss 0.58867.
Train: 2018-08-05T23:30:41.388369: step 3254, loss 0.536279.
Train: 2018-08-05T23:30:41.560203: step 3255, loss 0.553549.
Train: 2018-08-05T23:30:41.716416: step 3256, loss 0.483961.
Train: 2018-08-05T23:30:41.872630: step 3257, loss 0.597324.
Train: 2018-08-05T23:30:42.044498: step 3258, loss 0.606215.
Train: 2018-08-05T23:30:42.200677: step 3259, loss 0.553526.
Train: 2018-08-05T23:30:42.356891: step 3260, loss 0.553683.
Test: 2018-08-05T23:30:42.591211: step 3260, loss 0.547838.
Train: 2018-08-05T23:30:42.763075: step 3261, loss 0.614938.
Train: 2018-08-05T23:30:42.903663: step 3262, loss 0.553757.
Train: 2018-08-05T23:30:43.075472: step 3263, loss 0.597629.
Train: 2018-08-05T23:30:43.231686: step 3264, loss 0.519011.
Train: 2018-08-05T23:30:43.387898: step 3265, loss 0.527726.
Train: 2018-08-05T23:30:43.544111: step 3266, loss 0.519049.
Train: 2018-08-05T23:30:43.700352: step 3267, loss 0.492775.
Train: 2018-08-05T23:30:43.872192: step 3268, loss 0.597458.
Train: 2018-08-05T23:30:44.043994: step 3269, loss 0.579921.
Train: 2018-08-05T23:30:44.200207: step 3270, loss 0.510071.
Test: 2018-08-05T23:30:44.434529: step 3270, loss 0.547823.
Train: 2018-08-05T23:30:44.590766: step 3271, loss 0.509965.
Train: 2018-08-05T23:30:44.746980: step 3272, loss 0.571222.
Train: 2018-08-05T23:30:44.918788: step 3273, loss 0.571512.
Train: 2018-08-05T23:30:45.075001: step 3274, loss 0.527411.
Train: 2018-08-05T23:30:45.231215: step 3275, loss 0.483366.
Train: 2018-08-05T23:30:45.387464: step 3276, loss 0.57156.
Train: 2018-08-05T23:30:45.543667: step 3277, loss 0.589082.
Train: 2018-08-05T23:30:45.699887: step 3278, loss 0.598123.
Train: 2018-08-05T23:30:45.856094: step 3279, loss 0.51801.
Train: 2018-08-05T23:30:46.012618: step 3280, loss 0.5626.
Test: 2018-08-05T23:30:46.246939: step 3280, loss 0.547688.
Train: 2018-08-05T23:30:46.418799: step 3281, loss 0.56244.
Train: 2018-08-05T23:30:46.559391: step 3282, loss 0.544844.
Train: 2018-08-05T23:30:46.715606: step 3283, loss 0.58029.
Train: 2018-08-05T23:30:46.871819: step 3284, loss 0.642577.
Train: 2018-08-05T23:30:47.032581: step 3285, loss 0.598031.
Train: 2018-08-05T23:30:47.188790: step 3286, loss 0.553743.
Train: 2018-08-05T23:30:47.344972: step 3287, loss 0.527065.
Train: 2018-08-05T23:30:47.501217: step 3288, loss 0.580353.
Train: 2018-08-05T23:30:47.657399: step 3289, loss 0.580172.
Train: 2018-08-05T23:30:47.813612: step 3290, loss 0.606632.
Test: 2018-08-05T23:30:48.059330: step 3290, loss 0.547792.
Train: 2018-08-05T23:30:48.215543: step 3291, loss 0.580073.
Train: 2018-08-05T23:30:48.371763: step 3292, loss 0.510034.
Train: 2018-08-05T23:30:48.527969: step 3293, loss 0.614738.
Train: 2018-08-05T23:30:48.684183: step 3294, loss 0.605952.
Train: 2018-08-05T23:30:48.824775: step 3295, loss 0.571104.
Train: 2018-08-05T23:30:48.994910: step 3296, loss 0.510554.
Train: 2018-08-05T23:30:49.158228: step 3297, loss 0.493362.
Train: 2018-08-05T23:30:49.329393: step 3298, loss 0.545196.
Train: 2018-08-05T23:30:49.511476: step 3299, loss 0.639966.
Train: 2018-08-05T23:30:49.696975: step 3300, loss 0.553877.
Test: 2018-08-05T23:30:49.919807: step 3300, loss 0.54803.
Train: 2018-08-05T23:30:50.671918: step 3301, loss 0.588237.
Train: 2018-08-05T23:30:50.836175: step 3302, loss 0.562216.
Train: 2018-08-05T23:30:50.985254: step 3303, loss 0.545248.
Train: 2018-08-05T23:30:51.156655: step 3304, loss 0.502508.
Train: 2018-08-05T23:30:51.318222: step 3305, loss 0.605316.
Train: 2018-08-05T23:30:51.480787: step 3306, loss 0.605137.
Train: 2018-08-05T23:30:51.624960: step 3307, loss 0.699209.
Train: 2018-08-05T23:30:51.812416: step 3308, loss 0.630544.
Train: 2018-08-05T23:30:51.968631: step 3309, loss 0.545616.
Train: 2018-08-05T23:30:52.138876: step 3310, loss 0.57095.
Test: 2018-08-05T23:30:52.388820: step 3310, loss 0.548421.
Train: 2018-08-05T23:30:52.560623: step 3311, loss 0.587627.
Train: 2018-08-05T23:30:52.748077: step 3312, loss 0.512366.
Train: 2018-08-05T23:30:52.982397: step 3313, loss 0.579239.
Train: 2018-08-05T23:30:53.247960: step 3314, loss 0.52929.
Train: 2018-08-05T23:30:53.419818: step 3315, loss 0.579189.
Train: 2018-08-05T23:30:53.560409: step 3316, loss 0.604029.
Train: 2018-08-05T23:30:53.716628: step 3317, loss 0.570878.
Train: 2018-08-05T23:30:53.904081: step 3318, loss 0.529528.
Train: 2018-08-05T23:30:54.060297: step 3319, loss 0.587291.
Train: 2018-08-05T23:30:54.216511: step 3320, loss 0.546023.
Test: 2018-08-05T23:30:54.453528: step 3320, loss 0.548788.
Train: 2018-08-05T23:30:54.618981: step 3321, loss 0.562564.
Train: 2018-08-05T23:30:54.797837: step 3322, loss 0.59776.
Train: 2018-08-05T23:30:54.983549: step 3323, loss 0.49665.
Train: 2018-08-05T23:30:55.134128: step 3324, loss 0.55442.
Train: 2018-08-05T23:30:55.299521: step 3325, loss 0.479963.
Train: 2018-08-05T23:30:55.458267: step 3326, loss 0.529336.
Train: 2018-08-05T23:30:55.614481: step 3327, loss 0.595833.
Train: 2018-08-05T23:30:55.770693: step 3328, loss 0.570877.
Train: 2018-08-05T23:30:55.937167: step 3329, loss 0.595976.
Train: 2018-08-05T23:30:56.101589: step 3330, loss 0.495517.
Test: 2018-08-05T23:30:56.334323: step 3330, loss 0.548404.
Train: 2018-08-05T23:30:56.490515: step 3331, loss 0.528902.
Train: 2018-08-05T23:30:56.662350: step 3332, loss 0.570809.
Train: 2018-08-05T23:30:56.825671: step 3333, loss 0.57937.
Train: 2018-08-05T23:30:56.994423: step 3334, loss 0.681081.
Train: 2018-08-05T23:30:57.151025: step 3335, loss 0.545462.
Train: 2018-08-05T23:30:57.310278: step 3336, loss 0.579365.
Train: 2018-08-05T23:30:57.466493: step 3337, loss 0.587819.
Train: 2018-08-05T23:30:57.622706: step 3338, loss 0.579346.
Train: 2018-08-05T23:30:57.800844: step 3339, loss 0.60476.
Train: 2018-08-05T23:30:57.971479: step 3340, loss 0.579293.
Test: 2018-08-05T23:30:58.209009: step 3340, loss 0.54833.
Train: 2018-08-05T23:30:58.372815: step 3341, loss 0.494993.
Train: 2018-08-05T23:30:58.529054: step 3342, loss 0.596148.
Train: 2018-08-05T23:30:58.692955: step 3343, loss 0.579344.
Train: 2018-08-05T23:30:58.856921: step 3344, loss 0.638272.
Train: 2018-08-05T23:30:59.037382: step 3345, loss 0.554063.
Train: 2018-08-05T23:30:59.197114: step 3346, loss 0.554047.
Train: 2018-08-05T23:30:59.363944: step 3347, loss 0.512323.
Train: 2018-08-05T23:30:59.509644: step 3348, loss 0.570854.
Train: 2018-08-05T23:30:59.674487: step 3349, loss 0.595889.
Train: 2018-08-05T23:30:59.833723: step 3350, loss 0.554131.
Test: 2018-08-05T23:31:00.075679: step 3350, loss 0.548489.
Train: 2018-08-05T23:31:00.231892: step 3351, loss 0.621055.
Train: 2018-08-05T23:31:00.396529: step 3352, loss 0.53742.
Train: 2018-08-05T23:31:00.552743: step 3353, loss 0.512442.
Train: 2018-08-05T23:31:00.708953: step 3354, loss 0.612635.
Train: 2018-08-05T23:31:00.864780: step 3355, loss 0.495777.
Train: 2018-08-05T23:31:01.036591: step 3356, loss 0.604268.
Train: 2018-08-05T23:31:01.192827: step 3357, loss 0.579258.
Train: 2018-08-05T23:31:01.357475: step 3358, loss 0.612669.
Train: 2018-08-05T23:31:01.498068: step 3359, loss 0.612611.
Train: 2018-08-05T23:31:01.685524: step 3360, loss 0.520869.
Test: 2018-08-05T23:31:01.911993: step 3360, loss 0.548556.
Train: 2018-08-05T23:31:02.083803: step 3361, loss 0.529258.
Train: 2018-08-05T23:31:02.240016: step 3362, loss 0.529161.
Train: 2018-08-05T23:31:02.404845: step 3363, loss 0.545793.
Train: 2018-08-05T23:31:02.561029: step 3364, loss 0.528952.
Train: 2018-08-05T23:31:02.717243: step 3365, loss 0.595956.
Train: 2018-08-05T23:31:02.904927: step 3366, loss 0.545695.
Train: 2018-08-05T23:31:03.076761: step 3367, loss 0.587762.
Train: 2018-08-05T23:31:03.232974: step 3368, loss 0.537217.
Train: 2018-08-05T23:31:03.389650: step 3369, loss 0.520403.
Train: 2018-08-05T23:31:03.545864: step 3370, loss 0.528643.
Test: 2018-08-05T23:31:03.780183: step 3370, loss 0.548241.
Train: 2018-08-05T23:31:03.952051: step 3371, loss 0.587938.
Train: 2018-08-05T23:31:04.108237: step 3372, loss 0.613427.
Train: 2018-08-05T23:31:04.264450: step 3373, loss 0.545408.
Train: 2018-08-05T23:31:04.420091: step 3374, loss 0.570961.
Train: 2018-08-05T23:31:04.576333: step 3375, loss 0.579477.
Train: 2018-08-05T23:31:04.732518: step 3376, loss 0.528285.
Train: 2018-08-05T23:31:04.888959: step 3377, loss 0.579567.
Train: 2018-08-05T23:31:05.045172: step 3378, loss 0.65645.
Train: 2018-08-05T23:31:05.201386: step 3379, loss 0.596582.
Train: 2018-08-05T23:31:05.364675: step 3380, loss 0.485779.
Test: 2018-08-05T23:31:05.599026: step 3380, loss 0.54816.
Train: 2018-08-05T23:31:05.755208: step 3381, loss 0.55392.
Train: 2018-08-05T23:31:05.924581: step 3382, loss 0.536831.
Train: 2018-08-05T23:31:06.080794: step 3383, loss 0.528322.
Train: 2018-08-05T23:31:06.237008: step 3384, loss 0.605134.
Train: 2018-08-05T23:31:06.388531: step 3385, loss 0.51115.
Train: 2018-08-05T23:31:06.546421: step 3386, loss 0.451209.
Train: 2018-08-05T23:31:06.687014: step 3387, loss 0.528077.
Train: 2018-08-05T23:31:06.848339: step 3388, loss 0.605568.
Train: 2018-08-05T23:31:07.005194: step 3389, loss 0.536511.
Train: 2018-08-05T23:31:07.161408: step 3390, loss 0.562371.
Test: 2018-08-05T23:31:07.388696: step 3390, loss 0.547881.
Train: 2018-08-05T23:31:07.555064: step 3391, loss 0.571125.
Train: 2018-08-05T23:31:07.699896: step 3392, loss 0.545119.
Train: 2018-08-05T23:31:07.856675: step 3393, loss 0.475151.
Train: 2018-08-05T23:31:08.020119: step 3394, loss 0.509923.
Train: 2018-08-05T23:31:08.160682: step 3395, loss 0.544927.
Train: 2018-08-05T23:31:08.332443: step 3396, loss 0.589089.
Train: 2018-08-05T23:31:08.473036: step 3397, loss 0.580316.
Train: 2018-08-05T23:31:08.644899: step 3398, loss 0.544761.
Train: 2018-08-05T23:31:08.801629: step 3399, loss 0.598166.
Train: 2018-08-05T23:31:08.946644: step 3400, loss 0.562626.
Test: 2018-08-05T23:31:09.196557: step 3400, loss 0.547664.
Train: 2018-08-05T23:31:09.949331: step 3401, loss 0.518035.
Train: 2018-08-05T23:31:10.105549: step 3402, loss 0.482249.
Train: 2018-08-05T23:31:10.277380: step 3403, loss 0.490951.
Train: 2018-08-05T23:31:10.434679: step 3404, loss 0.607574.
Train: 2018-08-05T23:31:10.588521: step 3405, loss 0.562724.
Train: 2018-08-05T23:31:10.744734: step 3406, loss 0.52665.
Train: 2018-08-05T23:31:10.921766: step 3407, loss 0.589836.
Train: 2018-08-05T23:31:11.078010: step 3408, loss 0.589928.
Train: 2018-08-05T23:31:11.234193: step 3409, loss 0.571736.
Train: 2018-08-05T23:31:11.388670: step 3410, loss 0.49951.
Test: 2018-08-05T23:31:11.623019: step 3410, loss 0.547604.
Train: 2018-08-05T23:31:11.779233: step 3411, loss 0.59899.
Train: 2018-08-05T23:31:11.941457: step 3412, loss 0.526628.
Train: 2018-08-05T23:31:12.097661: step 3413, loss 0.56277.
Train: 2018-08-05T23:31:12.253875: step 3414, loss 0.49941.
Train: 2018-08-05T23:31:12.419553: step 3415, loss 0.463194.
Train: 2018-08-05T23:31:12.575786: step 3416, loss 0.617323.
Train: 2018-08-05T23:31:12.732945: step 3417, loss 0.608219.
Train: 2018-08-05T23:31:12.887603: step 3418, loss 0.535615.
Train: 2018-08-05T23:31:13.059462: step 3419, loss 0.553694.
Train: 2018-08-05T23:31:13.202662: step 3420, loss 0.553714.
Test: 2018-08-05T23:31:13.450827: step 3420, loss 0.547597.
Train: 2018-08-05T23:31:13.607072: step 3421, loss 0.608137.
Train: 2018-08-05T23:31:13.763284: step 3422, loss 0.608107.
Train: 2018-08-05T23:31:13.919210: step 3423, loss 0.617073.
Train: 2018-08-05T23:31:14.075395: step 3424, loss 0.643858.
Train: 2018-08-05T23:31:14.231637: step 3425, loss 0.589589.
Train: 2018-08-05T23:31:14.388157: step 3426, loss 0.491158.
Train: 2018-08-05T23:31:14.544371: step 3427, loss 0.527018.
Train: 2018-08-05T23:31:14.700584: step 3428, loss 0.58028.
Train: 2018-08-05T23:31:14.871850: step 3429, loss 0.589047.
Train: 2018-08-05T23:31:15.028094: step 3430, loss 0.597822.
Test: 2018-08-05T23:31:15.262414: step 3430, loss 0.547769.
Train: 2018-08-05T23:31:15.419224: step 3431, loss 0.553716.
Train: 2018-08-05T23:31:15.575437: step 3432, loss 0.544966.
Train: 2018-08-05T23:31:15.731680: step 3433, loss 0.527492.
Train: 2018-08-05T23:31:15.887494: step 3434, loss 0.588633.
Train: 2018-08-05T23:31:16.043737: step 3435, loss 0.588497.
Train: 2018-08-05T23:31:16.199920: step 3436, loss 0.545092.
Train: 2018-08-05T23:31:16.356541: step 3437, loss 0.588387.
Train: 2018-08-05T23:31:16.512754: step 3438, loss 0.51916.
Train: 2018-08-05T23:31:16.668967: step 3439, loss 0.536526.
Train: 2018-08-05T23:31:16.839377: step 3440, loss 0.614157.
Test: 2018-08-05T23:31:17.073130: step 3440, loss 0.548009.
Train: 2018-08-05T23:31:17.229333: step 3441, loss 0.59685.
Train: 2018-08-05T23:31:17.387722: step 3442, loss 0.588153.
Train: 2018-08-05T23:31:17.543935: step 3443, loss 0.588074.
Train: 2018-08-05T23:31:17.715771: step 3444, loss 0.553894.
Train: 2018-08-05T23:31:17.871398: step 3445, loss 0.587989.
Train: 2018-08-05T23:31:18.031134: step 3446, loss 0.613404.
Train: 2018-08-05T23:31:18.202974: step 3447, loss 0.537037.
Train: 2018-08-05T23:31:18.355428: step 3448, loss 0.621557.
Train: 2018-08-05T23:31:18.511678: step 3449, loss 0.537213.
Train: 2018-08-05T23:31:18.667891: step 3450, loss 0.512122.
Test: 2018-08-05T23:31:18.918097: step 3450, loss 0.548443.
Train: 2018-08-05T23:31:19.074281: step 3451, loss 0.554049.
Train: 2018-08-05T23:31:19.230493: step 3452, loss 0.537353.
Train: 2018-08-05T23:31:19.393502: step 3453, loss 0.604396.
Train: 2018-08-05T23:31:19.549715: step 3454, loss 0.529003.
Train: 2018-08-05T23:31:19.705958: step 3455, loss 0.621107.
Train: 2018-08-05T23:31:19.862286: step 3456, loss 0.587592.
Train: 2018-08-05T23:31:20.014502: step 3457, loss 0.487332.
Train: 2018-08-05T23:31:20.170715: step 3458, loss 0.629299.
Train: 2018-08-05T23:31:20.330997: step 3459, loss 0.587527.
Train: 2018-08-05T23:31:20.487211: step 3460, loss 0.595839.
Test: 2018-08-05T23:31:20.721565: step 3460, loss 0.548567.
Train: 2018-08-05T23:31:20.893464: step 3461, loss 0.529163.
Train: 2018-08-05T23:31:21.047978: step 3462, loss 0.662395.
Train: 2018-08-05T23:31:21.188572: step 3463, loss 0.554201.
Train: 2018-08-05T23:31:21.362097: step 3464, loss 0.562595.
Train: 2018-08-05T23:31:21.518281: step 3465, loss 0.56257.
Train: 2018-08-05T23:31:21.674518: step 3466, loss 0.504721.
Train: 2018-08-05T23:31:21.839573: step 3467, loss 0.570817.
Train: 2018-08-05T23:31:21.986614: step 3468, loss 0.512962.
Train: 2018-08-05T23:31:22.142827: step 3469, loss 0.637102.
Train: 2018-08-05T23:31:22.317149: step 3470, loss 0.512944.
Test: 2018-08-05T23:31:22.543268: step 3470, loss 0.54867.
Train: 2018-08-05T23:31:22.699481: step 3471, loss 0.55429.
Train: 2018-08-05T23:31:22.877513: step 3472, loss 0.521038.
Train: 2018-08-05T23:31:23.018100: step 3473, loss 0.562484.
Train: 2018-08-05T23:31:23.189939: step 3474, loss 0.637582.
Train: 2018-08-05T23:31:23.348366: step 3475, loss 0.5792.
Train: 2018-08-05T23:31:23.504609: step 3476, loss 0.50408.
Train: 2018-08-05T23:31:23.660817: step 3477, loss 0.54575.
Train: 2018-08-05T23:31:23.835888: step 3478, loss 0.570895.
Train: 2018-08-05T23:31:23.980599: step 3479, loss 0.570879.
Train: 2018-08-05T23:31:24.136782: step 3480, loss 0.54559.
Test: 2018-08-05T23:31:24.386189: step 3480, loss 0.548361.
Train: 2018-08-05T23:31:24.531564: step 3481, loss 0.53727.
Train: 2018-08-05T23:31:24.703375: step 3482, loss 0.511857.
Train: 2018-08-05T23:31:24.863792: step 3483, loss 0.570884.
Train: 2018-08-05T23:31:25.020004: step 3484, loss 0.4606.
Train: 2018-08-05T23:31:25.176217: step 3485, loss 0.596533.
Train: 2018-08-05T23:31:25.345988: step 3486, loss 0.570976.
Train: 2018-08-05T23:31:25.502198: step 3487, loss 0.571009.
Train: 2018-08-05T23:31:25.658412: step 3488, loss 0.510777.
Train: 2018-08-05T23:31:25.824143: step 3489, loss 0.648692.
Train: 2018-08-05T23:31:25.979712: step 3490, loss 0.605565.
Test: 2018-08-05T23:31:26.214063: step 3490, loss 0.547946.
Train: 2018-08-05T23:31:26.370698: step 3491, loss 0.57112.
Train: 2018-08-05T23:31:26.526911: step 3492, loss 0.493278.
Train: 2018-08-05T23:31:26.683155: step 3493, loss 0.588366.
Train: 2018-08-05T23:31:26.854300: step 3494, loss 0.553731.
Train: 2018-08-05T23:31:27.015134: step 3495, loss 0.545026.
Train: 2018-08-05T23:31:27.171347: step 3496, loss 0.536457.
Train: 2018-08-05T23:31:27.332381: step 3497, loss 0.510383.
Train: 2018-08-05T23:31:27.479062: step 3498, loss 0.597367.
Train: 2018-08-05T23:31:27.635275: step 3499, loss 0.527627.
Train: 2018-08-05T23:31:27.814648: step 3500, loss 0.571224.
Test: 2018-08-05T23:31:28.050646: step 3500, loss 0.547821.
Train: 2018-08-05T23:31:28.776218: step 3501, loss 0.510045.
Train: 2018-08-05T23:31:28.932660: step 3502, loss 0.562386.
Train: 2018-08-05T23:31:29.104470: step 3503, loss 0.527399.
Train: 2018-08-05T23:31:29.260719: step 3504, loss 0.527308.
Train: 2018-08-05T23:31:29.423242: step 3505, loss 0.509638.
Train: 2018-08-05T23:31:29.579483: step 3506, loss 0.597939.
Train: 2018-08-05T23:31:29.735698: step 3507, loss 0.589145.
Train: 2018-08-05T23:31:29.893330: step 3508, loss 0.597996.
Train: 2018-08-05T23:31:30.049574: step 3509, loss 0.651287.
Train: 2018-08-05T23:31:30.205787: step 3510, loss 0.598.
Test: 2018-08-05T23:31:30.448013: step 3510, loss 0.547719.
Train: 2018-08-05T23:31:30.604208: step 3511, loss 0.571357.
Train: 2018-08-05T23:31:30.760428: step 3512, loss 0.518369.
Train: 2018-08-05T23:31:30.924589: step 3513, loss 0.518456.
Train: 2018-08-05T23:31:31.080804: step 3514, loss 0.571253.
Train: 2018-08-05T23:31:31.237046: step 3515, loss 0.65914.
Train: 2018-08-05T23:31:31.401345: step 3516, loss 0.483582.
Train: 2018-08-05T23:31:31.557559: step 3517, loss 0.457392.
Train: 2018-08-05T23:31:31.729393: step 3518, loss 0.553692.
Train: 2018-08-05T23:31:31.884861: step 3519, loss 0.571271.
Train: 2018-08-05T23:31:32.050847: step 3520, loss 0.536121.
Test: 2018-08-05T23:31:32.292380: step 3520, loss 0.547764.
Train: 2018-08-05T23:31:32.448037: step 3521, loss 0.606367.
Train: 2018-08-05T23:31:32.604251: step 3522, loss 0.518542.
Train: 2018-08-05T23:31:32.776115: step 3523, loss 0.59771.
Train: 2018-08-05T23:31:32.932575: step 3524, loss 0.588812.
Train: 2018-08-05T23:31:33.088757: step 3525, loss 0.518575.
Train: 2018-08-05T23:31:33.245004: step 3526, loss 0.562505.
Train: 2018-08-05T23:31:33.401151: step 3527, loss 0.562439.
Train: 2018-08-05T23:31:33.572953: step 3528, loss 0.624002.
Train: 2018-08-05T23:31:33.729166: step 3529, loss 0.492299.
Train: 2018-08-05T23:31:33.893804: step 3530, loss 0.527423.
Test: 2018-08-05T23:31:34.128095: step 3530, loss 0.547777.
Train: 2018-08-05T23:31:34.304107: step 3531, loss 0.579981.
Train: 2018-08-05T23:31:34.456274: step 3532, loss 0.536093.
Train: 2018-08-05T23:31:34.596865: step 3533, loss 0.580018.
Train: 2018-08-05T23:31:34.753078: step 3534, loss 0.553615.
Train: 2018-08-05T23:31:34.932022: step 3535, loss 0.597713.
Train: 2018-08-05T23:31:35.116538: step 3536, loss 0.562473.
Train: 2018-08-05T23:31:35.298043: step 3537, loss 0.641459.
Train: 2018-08-05T23:31:35.483573: step 3538, loss 0.60623.
Train: 2018-08-05T23:31:35.682017: step 3539, loss 0.45783.
Train: 2018-08-05T23:31:35.870832: step 3540, loss 0.562433.
Test: 2018-08-05T23:31:36.111186: step 3540, loss 0.547863.
Train: 2018-08-05T23:31:36.284730: step 3541, loss 0.597148.
Train: 2018-08-05T23:31:36.459288: step 3542, loss 0.536411.
Train: 2018-08-05T23:31:36.627117: step 3543, loss 0.484316.
Train: 2018-08-05T23:31:36.791703: step 3544, loss 0.545092.
Train: 2018-08-05T23:31:36.955684: step 3545, loss 0.571124.
Train: 2018-08-05T23:31:37.129187: step 3546, loss 0.545024.
Train: 2018-08-05T23:31:37.287348: step 3547, loss 0.510173.
Train: 2018-08-05T23:31:37.454924: step 3548, loss 0.544988.
Train: 2018-08-05T23:31:37.630456: step 3549, loss 0.579957.
Train: 2018-08-05T23:31:37.794049: step 3550, loss 0.483694.
Test: 2018-08-05T23:31:38.028842: step 3550, loss 0.547775.
Train: 2018-08-05T23:31:38.185082: step 3551, loss 0.492265.
Train: 2018-08-05T23:31:38.346048: step 3552, loss 0.641854.
Train: 2018-08-05T23:31:38.486666: step 3553, loss 0.580176.
Train: 2018-08-05T23:31:38.658561: step 3554, loss 0.580154.
Train: 2018-08-05T23:31:38.836411: step 3555, loss 0.650884.
Train: 2018-08-05T23:31:39.011942: step 3556, loss 0.553789.
Train: 2018-08-05T23:31:39.170846: step 3557, loss 0.606525.
Train: 2018-08-05T23:31:39.343003: step 3558, loss 0.606464.
Train: 2018-08-05T23:31:39.493857: step 3559, loss 0.536173.
Train: 2018-08-05T23:31:39.659885: step 3560, loss 0.614909.
Test: 2018-08-05T23:31:39.907707: step 3560, loss 0.547851.
Train: 2018-08-05T23:31:40.066041: step 3561, loss 0.623463.
Train: 2018-08-05T23:31:40.216287: step 3562, loss 0.527681.
Train: 2018-08-05T23:31:40.372473: step 3563, loss 0.536501.
Train: 2018-08-05T23:31:40.528717: step 3564, loss 0.571021.
Train: 2018-08-05T23:31:40.679493: step 3565, loss 0.493516.
Train: 2018-08-05T23:31:40.838267: step 3566, loss 0.553786.
Train: 2018-08-05T23:31:40.992572: step 3567, loss 0.648395.
Train: 2018-08-05T23:31:41.148753: step 3568, loss 0.588184.
Train: 2018-08-05T23:31:41.305575: step 3569, loss 0.570997.
Train: 2018-08-05T23:31:41.477442: step 3570, loss 0.605086.
Test: 2018-08-05T23:31:41.719994: step 3570, loss 0.548174.
Train: 2018-08-05T23:31:41.868880: step 3571, loss 0.570872.
Train: 2018-08-05T23:31:42.028070: step 3572, loss 0.596349.
Train: 2018-08-05T23:31:42.184258: step 3573, loss 0.503277.
Train: 2018-08-05T23:31:42.339330: step 3574, loss 0.562416.
Train: 2018-08-05T23:31:42.498643: step 3575, loss 0.663649.
Train: 2018-08-05T23:31:42.649170: step 3576, loss 0.604474.
Train: 2018-08-05T23:31:42.817969: step 3577, loss 0.554086.
Train: 2018-08-05T23:31:42.974183: step 3578, loss 0.60421.
Train: 2018-08-05T23:31:43.130395: step 3579, loss 0.537552.
Train: 2018-08-05T23:31:43.286602: step 3580, loss 0.488017.
Test: 2018-08-05T23:31:43.541970: step 3580, loss 0.548679.
Train: 2018-08-05T23:31:43.720491: step 3581, loss 0.554262.
Train: 2018-08-05T23:31:43.886408: step 3582, loss 0.603971.
Train: 2018-08-05T23:31:44.053481: step 3583, loss 0.554275.
Train: 2018-08-05T23:31:44.229181: step 3584, loss 0.455068.
Train: 2018-08-05T23:31:44.393412: step 3585, loss 0.562555.
Train: 2018-08-05T23:31:44.556960: step 3586, loss 0.554233.
Train: 2018-08-05T23:31:44.719535: step 3587, loss 0.537551.
Train: 2018-08-05T23:31:44.880091: step 3588, loss 0.562467.
Train: 2018-08-05T23:31:45.049654: step 3589, loss 0.595936.
Train: 2018-08-05T23:31:45.213208: step 3590, loss 0.503728.
Test: 2018-08-05T23:31:45.451544: step 3590, loss 0.548365.
Train: 2018-08-05T23:31:45.628104: step 3591, loss 0.537198.
Train: 2018-08-05T23:31:45.794627: step 3592, loss 0.579335.
Train: 2018-08-05T23:31:45.961233: step 3593, loss 0.553959.
Train: 2018-08-05T23:31:46.139730: step 3594, loss 0.61339.
Train: 2018-08-05T23:31:46.331191: step 3595, loss 0.63045.
Train: 2018-08-05T23:31:46.495752: step 3596, loss 0.579409.
Train: 2018-08-05T23:31:46.658317: step 3597, loss 0.562392.
Train: 2018-08-05T23:31:46.835842: step 3598, loss 0.562395.
Train: 2018-08-05T23:31:46.994267: step 3599, loss 0.604802.
Train: 2018-08-05T23:31:47.151045: step 3600, loss 0.494645.
Test: 2018-08-05T23:31:47.389377: step 3600, loss 0.548228.
Train: 2018-08-05T23:31:48.169294: step 3601, loss 0.579357.
Train: 2018-08-05T23:31:48.331642: step 3602, loss 0.460651.
Train: 2018-08-05T23:31:48.487852: step 3603, loss 0.502874.
Train: 2018-08-05T23:31:48.646780: step 3604, loss 0.588008.
Train: 2018-08-05T23:31:48.789864: step 3605, loss 0.493988.
Train: 2018-08-05T23:31:48.954257: step 3606, loss 0.639709.
Train: 2018-08-05T23:31:49.126091: step 3607, loss 0.588227.
Train: 2018-08-05T23:31:49.292038: step 3608, loss 0.553766.
Train: 2018-08-05T23:31:49.476378: step 3609, loss 0.6487.
Train: 2018-08-05T23:31:49.627436: step 3610, loss 0.53667.
Test: 2018-08-05T23:31:49.861756: step 3610, loss 0.548.
Train: 2018-08-05T23:31:50.013702: step 3611, loss 0.536592.
Train: 2018-08-05T23:31:50.185536: step 3612, loss 0.527963.
Train: 2018-08-05T23:31:50.326129: step 3613, loss 0.605484.
Train: 2018-08-05T23:31:50.482342: step 3614, loss 0.605461.
Train: 2018-08-05T23:31:50.638555: step 3615, loss 0.545201.
Train: 2018-08-05T23:31:50.794794: step 3616, loss 0.579649.
Train: 2018-08-05T23:31:50.949141: step 3617, loss 0.485068.
Train: 2018-08-05T23:31:51.120975: step 3618, loss 0.570986.
Train: 2018-08-05T23:31:51.277156: step 3619, loss 0.605533.
Train: 2018-08-05T23:31:51.433371: step 3620, loss 0.605393.
Test: 2018-08-05T23:31:51.667692: step 3620, loss 0.548019.
Train: 2018-08-05T23:31:51.826495: step 3621, loss 0.588221.
Train: 2018-08-05T23:31:51.985670: step 3622, loss 0.562409.
Train: 2018-08-05T23:31:52.141908: step 3623, loss 0.511016.
Train: 2018-08-05T23:31:52.298097: step 3624, loss 0.580602.
Train: 2018-08-05T23:31:52.454309: step 3625, loss 0.528177.
Train: 2018-08-05T23:31:52.610523: step 3626, loss 0.519604.
Train: 2018-08-05T23:31:52.766735: step 3627, loss 0.596703.
Train: 2018-08-05T23:31:52.922950: step 3628, loss 0.562394.
Train: 2018-08-05T23:31:53.079163: step 3629, loss 0.528075.
Train: 2018-08-05T23:31:53.235413: step 3630, loss 0.53668.
Test: 2018-08-05T23:31:53.469695: step 3630, loss 0.548023.
Train: 2018-08-05T23:31:53.632396: step 3631, loss 0.571086.
Train: 2018-08-05T23:31:53.788634: step 3632, loss 0.545234.
Train: 2018-08-05T23:31:53.944823: step 3633, loss 0.502102.
Train: 2018-08-05T23:31:54.101035: step 3634, loss 0.657314.
Train: 2018-08-05T23:31:54.257250: step 3635, loss 0.562399.
Train: 2018-08-05T23:31:54.432576: step 3636, loss 0.588362.
Train: 2018-08-05T23:31:54.590832: step 3637, loss 0.57111.
Train: 2018-08-05T23:31:54.734660: step 3638, loss 0.605638.
Train: 2018-08-05T23:31:54.890880: step 3639, loss 0.665603.
Train: 2018-08-05T23:31:55.047062: step 3640, loss 0.510865.
Test: 2018-08-05T23:31:55.297028: step 3640, loss 0.548076.
Train: 2018-08-05T23:31:55.453247: step 3641, loss 0.570998.
Train: 2018-08-05T23:31:55.609454: step 3642, loss 0.519664.
Train: 2018-08-05T23:31:55.765668: step 3643, loss 0.604986.
Train: 2018-08-05T23:31:55.937492: step 3644, loss 0.49421.
Train: 2018-08-05T23:31:56.093730: step 3645, loss 0.579458.
Train: 2018-08-05T23:31:56.249941: step 3646, loss 0.460188.
Train: 2018-08-05T23:31:56.406149: step 3647, loss 0.570964.
Train: 2018-08-05T23:31:56.562329: step 3648, loss 0.51113.
Train: 2018-08-05T23:31:56.718576: step 3649, loss 0.519616.
Train: 2018-08-05T23:31:56.874756: step 3650, loss 0.553806.
Test: 2018-08-05T23:31:57.124731: step 3650, loss 0.547983.
Train: 2018-08-05T23:31:57.292826: step 3651, loss 0.553754.
Train: 2018-08-05T23:31:57.453408: step 3652, loss 0.588383.
Train: 2018-08-05T23:31:57.614578: step 3653, loss 0.553785.
Train: 2018-08-05T23:31:57.767460: step 3654, loss 0.571102.
Train: 2018-08-05T23:31:57.952468: step 3655, loss 0.623288.
Train: 2018-08-05T23:31:58.124301: step 3656, loss 0.579817.
Train: 2018-08-05T23:31:58.280511: step 3657, loss 0.588429.
Train: 2018-08-05T23:31:58.467947: step 3658, loss 0.53637.
Train: 2018-08-05T23:31:58.639776: step 3659, loss 0.605694.
Train: 2018-08-05T23:31:58.889716: step 3660, loss 0.571006.
Test: 2018-08-05T23:31:59.126549: step 3660, loss 0.547937.
Train: 2018-08-05T23:31:59.298399: step 3661, loss 0.562409.
Train: 2018-08-05T23:31:59.454614: step 3662, loss 0.571041.
Train: 2018-08-05T23:31:59.610826: step 3663, loss 0.536554.
Train: 2018-08-05T23:31:59.814555: step 3664, loss 0.484823.
Train: 2018-08-05T23:31:59.970517: step 3665, loss 0.605557.
Train: 2018-08-05T23:32:00.126728: step 3666, loss 0.597005.
Train: 2018-08-05T23:32:00.288761: step 3667, loss 0.55376.
Train: 2018-08-05T23:32:00.444973: step 3668, loss 0.536574.
Train: 2018-08-05T23:32:00.601212: step 3669, loss 0.60546.
Train: 2018-08-05T23:32:00.764143: step 3670, loss 0.605493.
Test: 2018-08-05T23:32:00.996591: step 3670, loss 0.548023.
Train: 2018-08-05T23:32:01.168433: step 3671, loss 0.596783.
Train: 2018-08-05T23:32:01.328617: step 3672, loss 0.562339.
Train: 2018-08-05T23:32:01.469210: step 3673, loss 0.553864.
Train: 2018-08-05T23:32:01.641074: step 3674, loss 0.553889.
Train: 2018-08-05T23:32:01.795356: step 3675, loss 0.545363.
Train: 2018-08-05T23:32:01.953996: step 3676, loss 0.502746.
Train: 2018-08-05T23:32:02.110213: step 3677, loss 0.545417.
Train: 2018-08-05T23:32:02.273101: step 3678, loss 0.588109.
Train: 2018-08-05T23:32:02.413723: step 3679, loss 0.571143.
Train: 2018-08-05T23:32:02.569939: step 3680, loss 0.579502.
Test: 2018-08-05T23:32:02.811112: step 3680, loss 0.548108.
Train: 2018-08-05T23:32:02.967353: step 3681, loss 0.570915.
Train: 2018-08-05T23:32:03.123536: step 3682, loss 0.571009.
Train: 2018-08-05T23:32:03.280778: step 3683, loss 0.528351.
Train: 2018-08-05T23:32:03.436961: step 3684, loss 0.587973.
Train: 2018-08-05T23:32:03.593175: step 3685, loss 0.519703.
Train: 2018-08-05T23:32:03.749736: step 3686, loss 0.553896.
Train: 2018-08-05T23:32:03.905918: step 3687, loss 0.545315.
Train: 2018-08-05T23:32:04.062133: step 3688, loss 0.459832.
Train: 2018-08-05T23:32:04.226903: step 3689, loss 0.570964.
Train: 2018-08-05T23:32:04.374575: step 3690, loss 0.562424.
Test: 2018-08-05T23:32:04.624484: step 3690, loss 0.547965.
Train: 2018-08-05T23:32:04.787560: step 3691, loss 0.605578.
Train: 2018-08-05T23:32:04.943773: step 3692, loss 0.55386.
Train: 2018-08-05T23:32:05.099988: step 3693, loss 0.493122.
Train: 2018-08-05T23:32:05.257043: step 3694, loss 0.562495.
Train: 2018-08-05T23:32:05.413286: step 3695, loss 0.605936.
Train: 2018-08-05T23:32:05.569468: step 3696, loss 0.553675.
Train: 2018-08-05T23:32:05.725592: step 3697, loss 0.623353.
Train: 2018-08-05T23:32:05.881807: step 3698, loss 0.640727.
Train: 2018-08-05T23:32:06.038050: step 3699, loss 0.501649.
Train: 2018-08-05T23:32:06.200436: step 3700, loss 0.536371.
Test: 2018-08-05T23:32:06.435517: step 3700, loss 0.547889.
Train: 2018-08-05T23:32:07.171117: step 3701, loss 0.588418.
Train: 2018-08-05T23:32:07.327234: step 3702, loss 0.597047.
Train: 2018-08-05T23:32:07.483440: step 3703, loss 0.571047.
Train: 2018-08-05T23:32:07.639630: step 3704, loss 0.475823.
Train: 2018-08-05T23:32:07.792939: step 3705, loss 0.579658.
Train: 2018-08-05T23:32:07.963010: step 3706, loss 0.596943.
Train: 2018-08-05T23:32:08.119223: step 3707, loss 0.597113.
Train: 2018-08-05T23:32:08.296042: step 3708, loss 0.588223.
Train: 2018-08-05T23:32:08.452255: step 3709, loss 0.631301.
Train: 2018-08-05T23:32:08.608498: step 3710, loss 0.545277.
Test: 2018-08-05T23:32:08.842979: step 3710, loss 0.548046.
Train: 2018-08-05T23:32:09.026669: step 3711, loss 0.588196.
Train: 2018-08-05T23:32:09.233760: step 3712, loss 0.588074.
Train: 2018-08-05T23:32:09.389998: step 3713, loss 0.673306.
Train: 2018-08-05T23:32:09.546218: step 3714, loss 0.647275.
Train: 2018-08-05T23:32:09.712675: step 3715, loss 0.545538.
Train: 2018-08-05T23:32:09.865459: step 3716, loss 0.54563.
Train: 2018-08-05T23:32:10.021703: step 3717, loss 0.537553.
Train: 2018-08-05T23:32:10.177886: step 3718, loss 0.546007.
Train: 2018-08-05T23:32:10.333996: step 3719, loss 0.579149.
Train: 2018-08-05T23:32:10.490209: step 3720, loss 0.545974.
Test: 2018-08-05T23:32:10.727149: step 3720, loss 0.548731.
Train: 2018-08-05T23:32:10.873888: step 3721, loss 0.54603.
Train: 2018-08-05T23:32:11.028570: step 3722, loss 0.521288.
Train: 2018-08-05T23:32:11.184813: step 3723, loss 0.521257.
Train: 2018-08-05T23:32:11.341983: step 3724, loss 0.545995.
Train: 2018-08-05T23:32:11.498226: step 3725, loss 0.57923.
Train: 2018-08-05T23:32:11.654443: step 3726, loss 0.62062.
Train: 2018-08-05T23:32:11.817622: step 3727, loss 0.537597.
Train: 2018-08-05T23:32:11.969559: step 3728, loss 0.521021.
Train: 2018-08-05T23:32:12.125802: step 3729, loss 0.512609.
Train: 2018-08-05T23:32:12.295781: step 3730, loss 0.596036.
Test: 2018-08-05T23:32:12.530134: step 3730, loss 0.548476.
Train: 2018-08-05T23:32:12.733179: step 3731, loss 0.54573.
Train: 2018-08-05T23:32:12.889424: step 3732, loss 0.596041.
Train: 2018-08-05T23:32:13.061258: step 3733, loss 0.520418.
Train: 2018-08-05T23:32:13.232071: step 3734, loss 0.553971.
Train: 2018-08-05T23:32:13.373711: step 3735, loss 0.545453.
Train: 2018-08-05T23:32:13.529925: step 3736, loss 0.545471.
Train: 2018-08-05T23:32:13.686138: step 3737, loss 0.570949.
Train: 2018-08-05T23:32:13.842262: step 3738, loss 0.638892.
Train: 2018-08-05T23:32:13.998475: step 3739, loss 0.579502.
Train: 2018-08-05T23:32:14.154688: step 3740, loss 0.562289.
Test: 2018-08-05T23:32:14.389482: step 3740, loss 0.548161.
Train: 2018-08-05T23:32:14.561286: step 3741, loss 0.545261.
Train: 2018-08-05T23:32:14.720104: step 3742, loss 0.536885.
Train: 2018-08-05T23:32:14.873784: step 3743, loss 0.468624.
Train: 2018-08-05T23:32:15.030028: step 3744, loss 0.579659.
Train: 2018-08-05T23:32:15.186211: step 3745, loss 0.570839.
Train: 2018-08-05T23:32:15.342378: step 3746, loss 0.588182.
Train: 2018-08-05T23:32:15.498622: step 3747, loss 0.648551.
Train: 2018-08-05T23:32:15.654834: step 3748, loss 0.553766.
Train: 2018-08-05T23:32:15.810944: step 3749, loss 0.545074.
Train: 2018-08-05T23:32:15.967127: step 3750, loss 0.562425.
Test: 2018-08-05T23:32:16.212741: step 3750, loss 0.548014.
Train: 2018-08-05T23:32:16.357690: step 3751, loss 0.493594.
Train: 2018-08-05T23:32:16.513903: step 3752, loss 0.588057.
Train: 2018-08-05T23:32:16.670117: step 3753, loss 0.545058.
Train: 2018-08-05T23:32:16.826330: step 3754, loss 0.510615.
Train: 2018-08-05T23:32:16.982544: step 3755, loss 0.588195.
Train: 2018-08-05T23:32:17.138756: step 3756, loss 0.571375.
Train: 2018-08-05T23:32:17.310514: step 3757, loss 0.55434.
Train: 2018-08-05T23:32:17.466728: step 3758, loss 0.501658.
Train: 2018-08-05T23:32:17.607350: step 3759, loss 0.536338.
Train: 2018-08-05T23:32:17.772124: step 3760, loss 0.484252.
Test: 2018-08-05T23:32:18.012791: step 3760, loss 0.547807.
Train: 2018-08-05T23:32:18.169029: step 3761, loss 0.571466.
Train: 2018-08-05T23:32:18.326237: step 3762, loss 0.544932.
Train: 2018-08-05T23:32:18.466859: step 3763, loss 0.597681.
Train: 2018-08-05T23:32:18.623073: step 3764, loss 0.544593.
Train: 2018-08-05T23:32:18.779228: step 3765, loss 0.615692.
Train: 2018-08-05T23:32:18.949153: step 3766, loss 0.562589.
Train: 2018-08-05T23:32:19.105366: step 3767, loss 0.526766.
Train: 2018-08-05T23:32:19.262696: step 3768, loss 0.562362.
Train: 2018-08-05T23:32:19.418876: step 3769, loss 0.633015.
Train: 2018-08-05T23:32:19.575090: step 3770, loss 0.553754.
Test: 2018-08-05T23:32:19.810578: step 3770, loss 0.547721.
Train: 2018-08-05T23:32:19.969233: step 3771, loss 0.465376.
Train: 2018-08-05T23:32:20.141034: step 3772, loss 0.589405.
Train: 2018-08-05T23:32:20.294715: step 3773, loss 0.580249.
Train: 2018-08-05T23:32:20.450929: step 3774, loss 0.518357.
Train: 2018-08-05T23:32:20.607142: step 3775, loss 0.63791.
Train: 2018-08-05T23:32:20.763318: step 3776, loss 0.589185.
Train: 2018-08-05T23:32:20.926698: step 3777, loss 0.553533.
Train: 2018-08-05T23:32:21.091928: step 3778, loss 0.571455.
Train: 2018-08-05T23:32:21.262668: step 3779, loss 0.536092.
Train: 2018-08-05T23:32:21.418912: step 3780, loss 0.544968.
Test: 2018-08-05T23:32:21.653234: step 3780, loss 0.547791.
Train: 2018-08-05T23:32:21.825144: step 3781, loss 0.597214.
Train: 2018-08-05T23:32:21.971193: step 3782, loss 0.614532.
Train: 2018-08-05T23:32:22.143051: step 3783, loss 0.588689.
Train: 2018-08-05T23:32:22.310266: step 3784, loss 0.580002.
Train: 2018-08-05T23:32:22.466449: step 3785, loss 0.579609.
Train: 2018-08-05T23:32:22.622692: step 3786, loss 0.605883.
Train: 2018-08-05T23:32:22.777829: step 3787, loss 0.605095.
Train: 2018-08-05T23:32:22.934072: step 3788, loss 0.605281.
Train: 2018-08-05T23:32:23.090286: step 3789, loss 0.639175.
Train: 2018-08-05T23:32:23.263439: step 3790, loss 0.596348.
Test: 2018-08-05T23:32:23.497731: step 3790, loss 0.548367.
Train: 2018-08-05T23:32:23.669592: step 3791, loss 0.51195.
Train: 2018-08-05T23:32:23.824820: step 3792, loss 0.587555.
Train: 2018-08-05T23:32:23.981027: step 3793, loss 0.595916.
Train: 2018-08-05T23:32:24.137216: step 3794, loss 0.604035.
Train: 2018-08-05T23:32:24.309206: step 3795, loss 0.55425.
Train: 2018-08-05T23:32:24.465414: step 3796, loss 0.472315.
Train: 2018-08-05T23:32:24.621603: step 3797, loss 0.496972.
Train: 2018-08-05T23:32:24.785087: step 3798, loss 0.587181.
Train: 2018-08-05T23:32:24.967978: step 3799, loss 0.595419.
Train: 2018-08-05T23:32:25.148496: step 3800, loss 0.529776.
Test: 2018-08-05T23:32:25.392841: step 3800, loss 0.548854.
Train: 2018-08-05T23:32:26.167181: step 3801, loss 0.472274.
Train: 2018-08-05T23:32:26.354608: step 3802, loss 0.587321.
Train: 2018-08-05T23:32:26.542062: step 3803, loss 0.529538.
Train: 2018-08-05T23:32:26.760765: step 3804, loss 0.521097.
Train: 2018-08-05T23:32:27.038327: step 3805, loss 0.587343.
Train: 2018-08-05T23:32:27.225784: step 3806, loss 0.495573.
Train: 2018-08-05T23:32:27.397648: step 3807, loss 0.503854.
Train: 2018-08-05T23:32:27.553863: step 3808, loss 0.553904.
Train: 2018-08-05T23:32:27.710044: step 3809, loss 0.630307.
Train: 2018-08-05T23:32:27.866259: step 3810, loss 0.553928.
Test: 2018-08-05T23:32:28.116229: step 3810, loss 0.548152.
Train: 2018-08-05T23:32:28.272412: step 3811, loss 0.587934.
Train: 2018-08-05T23:32:28.428649: step 3812, loss 0.502811.
Train: 2018-08-05T23:32:28.584838: step 3813, loss 0.476802.
Train: 2018-08-05T23:32:28.741082: step 3814, loss 0.553765.
Train: 2018-08-05T23:32:28.897295: step 3815, loss 0.588071.
Train: 2018-08-05T23:32:29.061789: step 3816, loss 0.510425.
Train: 2018-08-05T23:32:29.233590: step 3817, loss 0.501218.
Train: 2018-08-05T23:32:29.389828: step 3818, loss 0.536208.
Train: 2018-08-05T23:32:29.561639: step 3819, loss 0.615451.
Train: 2018-08-05T23:32:29.717882: step 3820, loss 0.500504.
Test: 2018-08-05T23:32:29.950418: step 3820, loss 0.547681.
Train: 2018-08-05T23:32:30.106629: step 3821, loss 0.527051.
Train: 2018-08-05T23:32:30.262843: step 3822, loss 0.490979.
Train: 2018-08-05T23:32:30.419027: step 3823, loss 0.562703.
Train: 2018-08-05T23:32:30.590861: step 3824, loss 0.544584.
Train: 2018-08-05T23:32:30.731453: step 3825, loss 0.59906.
Train: 2018-08-05T23:32:30.903317: step 3826, loss 0.572338.
Train: 2018-08-05T23:32:31.061554: step 3827, loss 0.572582.
Train: 2018-08-05T23:32:31.203699: step 3828, loss 0.544485.
Train: 2018-08-05T23:32:31.359940: step 3829, loss 0.517655.
Train: 2018-08-05T23:32:31.516130: step 3830, loss 0.53591.
Test: 2018-08-05T23:32:31.750420: step 3830, loss 0.547587.
Train: 2018-08-05T23:32:31.913140: step 3831, loss 0.544026.
Train: 2018-08-05T23:32:32.069384: step 3832, loss 0.535776.
Train: 2018-08-05T23:32:32.225592: step 3833, loss 0.545132.
Train: 2018-08-05T23:32:32.381805: step 3834, loss 0.55343.
Train: 2018-08-05T23:32:32.538024: step 3835, loss 0.526161.
Train: 2018-08-05T23:32:32.694236: step 3836, loss 0.553388.
Train: 2018-08-05T23:32:32.850449: step 3837, loss 0.518061.
Train: 2018-08-05T23:32:33.006666: step 3838, loss 0.599651.
Train: 2018-08-05T23:32:33.162876: step 3839, loss 0.591231.
Train: 2018-08-05T23:32:33.319060: step 3840, loss 0.562541.
Test: 2018-08-05T23:32:33.553381: step 3840, loss 0.547594.
Train: 2018-08-05T23:32:33.709594: step 3841, loss 0.563026.
Train: 2018-08-05T23:32:33.865836: step 3842, loss 0.608876.
Train: 2018-08-05T23:32:34.022049: step 3843, loss 0.58125.
Train: 2018-08-05T23:32:34.178263: step 3844, loss 0.62703.
Train: 2018-08-05T23:32:34.334476: step 3845, loss 0.544382.
Train: 2018-08-05T23:32:34.506282: step 3846, loss 0.535499.
Train: 2018-08-05T23:32:34.662524: step 3847, loss 0.490525.
Train: 2018-08-05T23:32:34.818736: step 3848, loss 0.652958.
Train: 2018-08-05T23:32:34.974950: step 3849, loss 0.490625.
Train: 2018-08-05T23:32:35.131166: step 3850, loss 0.562529.
Test: 2018-08-05T23:32:35.365482: step 3850, loss 0.547624.
Train: 2018-08-05T23:32:35.521697: step 3851, loss 0.598635.
Train: 2018-08-05T23:32:35.693502: step 3852, loss 0.59847.
Train: 2018-08-05T23:32:35.834123: step 3853, loss 0.464713.
Train: 2018-08-05T23:32:36.005927: step 3854, loss 0.642435.
Train: 2018-08-05T23:32:36.162166: step 3855, loss 0.571272.
Train: 2018-08-05T23:32:36.334002: step 3856, loss 0.491666.
Train: 2018-08-05T23:32:36.474567: step 3857, loss 0.606817.
Train: 2018-08-05T23:32:36.630813: step 3858, loss 0.553529.
Train: 2018-08-05T23:32:36.797774: step 3859, loss 0.527423.
Train: 2018-08-05T23:32:36.954018: step 3860, loss 0.571357.
Test: 2018-08-05T23:32:37.188308: step 3860, loss 0.547768.
Train: 2018-08-05T23:32:37.344521: step 3861, loss 0.579972.
Train: 2018-08-05T23:32:37.500734: step 3862, loss 0.545008.
Train: 2018-08-05T23:32:37.656978: step 3863, loss 0.614806.
Train: 2018-08-05T23:32:37.813196: step 3864, loss 0.597335.
Train: 2018-08-05T23:32:37.979534: step 3865, loss 0.492526.
Train: 2018-08-05T23:32:38.135718: step 3866, loss 0.597154.
Train: 2018-08-05T23:32:38.291961: step 3867, loss 0.579782.
Train: 2018-08-05T23:32:38.448175: step 3868, loss 0.56237.
Train: 2018-08-05T23:32:38.604388: step 3869, loss 0.527813.
Train: 2018-08-05T23:32:38.760571: step 3870, loss 0.579907.
Test: 2018-08-05T23:32:38.994892: step 3870, loss 0.547967.
Train: 2018-08-05T23:32:39.166790: step 3871, loss 0.527872.
Train: 2018-08-05T23:32:39.323003: step 3872, loss 0.545144.
Train: 2018-08-05T23:32:39.463565: step 3873, loss 0.553616.
Train: 2018-08-05T23:32:39.635398: step 3874, loss 0.562254.
Train: 2018-08-05T23:32:39.807233: step 3875, loss 0.657116.
Train: 2018-08-05T23:32:39.968845: step 3876, loss 0.545357.
Train: 2018-08-05T23:32:40.125061: step 3877, loss 0.493741.
Train: 2018-08-05T23:32:40.281274: step 3878, loss 0.57109.
Train: 2018-08-05T23:32:40.453080: step 3879, loss 0.622495.
Train: 2018-08-05T23:32:40.627839: step 3880, loss 0.630993.
Test: 2018-08-05T23:32:40.876521: step 3880, loss 0.548074.
Train: 2018-08-05T23:32:41.034466: step 3881, loss 0.468272.
Train: 2018-08-05T23:32:41.200297: step 3882, loss 0.528316.
Train: 2018-08-05T23:32:41.372132: step 3883, loss 0.596647.
Train: 2018-08-05T23:32:41.528347: step 3884, loss 0.511141.
Train: 2018-08-05T23:32:41.697974: step 3885, loss 0.536551.
Train: 2018-08-05T23:32:41.854188: step 3886, loss 0.528078.
Train: 2018-08-05T23:32:42.038405: step 3887, loss 0.536614.
Train: 2018-08-05T23:32:42.190623: step 3888, loss 0.605235.
Train: 2018-08-05T23:32:42.362456: step 3889, loss 0.622597.
Train: 2018-08-05T23:32:42.518669: step 3890, loss 0.631589.
Test: 2018-08-05T23:32:42.761786: step 3890, loss 0.547996.
Train: 2018-08-05T23:32:42.939932: step 3891, loss 0.622423.
Train: 2018-08-05T23:32:43.086900: step 3892, loss 0.596892.
Train: 2018-08-05T23:32:43.238212: step 3893, loss 0.630435.
Train: 2018-08-05T23:32:43.394392: step 3894, loss 0.528046.
Train: 2018-08-05T23:32:43.550635: step 3895, loss 0.587846.
Train: 2018-08-05T23:32:43.706127: step 3896, loss 0.562504.
Train: 2018-08-05T23:32:43.862340: step 3897, loss 0.562522.
Train: 2018-08-05T23:32:44.018553: step 3898, loss 0.562206.
Train: 2018-08-05T23:32:44.183231: step 3899, loss 0.61265.
Train: 2018-08-05T23:32:44.339476: step 3900, loss 0.512383.
Test: 2018-08-05T23:32:44.573797: step 3900, loss 0.548494.
Train: 2018-08-05T23:32:45.308233: step 3901, loss 0.554279.
Train: 2018-08-05T23:32:45.464447: step 3902, loss 0.504308.
Train: 2018-08-05T23:32:45.619580: step 3903, loss 0.504062.
Train: 2018-08-05T23:32:45.775793: step 3904, loss 0.529502.
Train: 2018-08-05T23:32:45.932008: step 3905, loss 0.604639.
Train: 2018-08-05T23:32:46.088253: step 3906, loss 0.579367.
Train: 2018-08-05T23:32:46.237684: step 3907, loss 0.57083.
Train: 2018-08-05T23:32:46.393894: step 3908, loss 0.545624.
Train: 2018-08-05T23:32:46.550108: step 3909, loss 0.604672.
Train: 2018-08-05T23:32:46.699136: step 3910, loss 0.545361.
Test: 2018-08-05T23:32:46.933486: step 3910, loss 0.548326.
Train: 2018-08-05T23:32:47.111803: step 3911, loss 0.688964.
Train: 2018-08-05T23:32:47.269358: step 3912, loss 0.58796.
Train: 2018-08-05T23:32:47.425602: step 3913, loss 0.545645.
Train: 2018-08-05T23:32:47.581815: step 3914, loss 0.587495.
Train: 2018-08-05T23:32:47.745655: step 3915, loss 0.504004.
Train: 2018-08-05T23:32:47.901838: step 3916, loss 0.545669.
Train: 2018-08-05T23:32:48.058051: step 3917, loss 0.529012.
Train: 2018-08-05T23:32:48.206429: step 3918, loss 0.529124.
Train: 2018-08-05T23:32:48.362673: step 3919, loss 0.503796.
Train: 2018-08-05T23:32:48.534508: step 3920, loss 0.722155.
Test: 2018-08-05T23:32:48.777499: step 3920, loss 0.548391.
Train: 2018-08-05T23:32:48.933682: step 3921, loss 0.570889.
Train: 2018-08-05T23:32:49.089925: step 3922, loss 0.587763.
Train: 2018-08-05T23:32:49.236301: step 3923, loss 0.60448.
Train: 2018-08-05T23:32:49.392514: step 3924, loss 0.487236.
Train: 2018-08-05T23:32:49.548730: step 3925, loss 0.528964.
Train: 2018-08-05T23:32:49.714379: step 3926, loss 0.562245.
Train: 2018-08-05T23:32:49.854941: step 3927, loss 0.604424.
Train: 2018-08-05T23:32:50.026776: step 3928, loss 0.545671.
Train: 2018-08-05T23:32:50.175143: step 3929, loss 0.571047.
Train: 2018-08-05T23:32:50.331358: step 3930, loss 0.621352.
Test: 2018-08-05T23:32:50.565708: step 3930, loss 0.548418.
Train: 2018-08-05T23:32:50.713988: step 3931, loss 0.537372.
Train: 2018-08-05T23:32:50.885823: step 3932, loss 0.637801.
Train: 2018-08-05T23:32:51.040681: step 3933, loss 0.579203.
Train: 2018-08-05T23:32:51.190094: step 3934, loss 0.57083.
Train: 2018-08-05T23:32:51.357640: step 3935, loss 0.512499.
Train: 2018-08-05T23:32:51.513854: step 3936, loss 0.637562.
Train: 2018-08-05T23:32:51.667008: step 3937, loss 0.562506.
Train: 2018-08-05T23:32:51.823222: step 3938, loss 0.562564.
Train: 2018-08-05T23:32:51.983763: step 3939, loss 0.570898.
Train: 2018-08-05T23:32:52.136006: step 3940, loss 0.487774.
Test: 2018-08-05T23:32:52.370326: step 3940, loss 0.54862.
Train: 2018-08-05T23:32:52.526570: step 3941, loss 0.537624.
Train: 2018-08-05T23:32:52.674715: step 3942, loss 0.487641.
Train: 2018-08-05T23:32:52.830929: step 3943, loss 0.562608.
Train: 2018-08-05T23:32:52.985364: step 3944, loss 0.587636.
Train: 2018-08-05T23:32:53.151305: step 3945, loss 0.545756.
Train: 2018-08-05T23:32:53.307547: step 3946, loss 0.604327.
Train: 2018-08-05T23:32:53.463762: step 3947, loss 0.587682.
Train: 2018-08-05T23:32:53.627559: step 3948, loss 0.570753.
Train: 2018-08-05T23:32:53.783802: step 3949, loss 0.511818.
Train: 2018-08-05T23:32:53.942479: step 3950, loss 0.59615.
Test: 2018-08-05T23:32:54.174884: step 3950, loss 0.548267.
Train: 2018-08-05T23:32:54.331101: step 3951, loss 0.5539.
Train: 2018-08-05T23:32:54.487315: step 3952, loss 0.630394.
Train: 2018-08-05T23:32:54.650188: step 3953, loss 0.52024.
Train: 2018-08-05T23:32:54.806401: step 3954, loss 0.58786.
Train: 2018-08-05T23:32:54.972261: step 3955, loss 0.571078.
Train: 2018-08-05T23:32:55.127722: step 3956, loss 0.511737.
Train: 2018-08-05T23:32:55.276372: step 3957, loss 0.511607.
Train: 2018-08-05T23:32:55.432584: step 3958, loss 0.604909.
Train: 2018-08-05T23:32:55.588768: step 3959, loss 0.587981.
Train: 2018-08-05T23:32:55.737042: step 3960, loss 0.511404.
Test: 2018-08-05T23:32:55.987032: step 3960, loss 0.548152.
Train: 2018-08-05T23:32:56.150917: step 3961, loss 0.579366.
Train: 2018-08-05T23:32:56.307130: step 3962, loss 0.596506.
Train: 2018-08-05T23:32:56.463378: step 3963, loss 0.570815.
Train: 2018-08-05T23:32:56.624181: step 3964, loss 0.545317.
Train: 2018-08-05T23:32:56.780394: step 3965, loss 0.588049.
Train: 2018-08-05T23:32:56.936638: step 3966, loss 0.468572.
Train: 2018-08-05T23:32:57.092853: step 3967, loss 0.528172.
Train: 2018-08-05T23:32:57.244288: step 3968, loss 0.519535.
Train: 2018-08-05T23:32:57.400501: step 3969, loss 0.579575.
Train: 2018-08-05T23:32:57.556747: step 3970, loss 0.579583.
Test: 2018-08-05T23:32:57.797839: step 3970, loss 0.547952.
Train: 2018-08-05T23:32:57.954040: step 3971, loss 0.605588.
Train: 2018-08-05T23:32:58.123649: step 3972, loss 0.536457.
Train: 2018-08-05T23:32:58.278844: step 3973, loss 0.519268.
Train: 2018-08-05T23:32:58.446985: step 3974, loss 0.579755.
Train: 2018-08-05T23:32:58.611386: step 3975, loss 0.536462.
Train: 2018-08-05T23:32:58.794972: step 3976, loss 0.545081.
Train: 2018-08-05T23:32:58.977483: step 3977, loss 0.588558.
Train: 2018-08-05T23:32:59.159998: step 3978, loss 0.640832.
Train: 2018-08-05T23:32:59.357467: step 3979, loss 0.510209.
Train: 2018-08-05T23:32:59.553943: step 3980, loss 0.562435.
Test: 2018-08-05T23:32:59.797290: step 3980, loss 0.547845.
Train: 2018-08-05T23:32:59.966837: step 3981, loss 0.5276.
Train: 2018-08-05T23:33:00.137392: step 3982, loss 0.58862.
Train: 2018-08-05T23:33:00.309451: step 3983, loss 0.562464.
Train: 2018-08-05T23:33:00.466595: step 3984, loss 0.553658.
Train: 2018-08-05T23:33:00.651103: step 3985, loss 0.623526.
Train: 2018-08-05T23:33:00.823674: step 3986, loss 0.605937.
Train: 2018-08-05T23:33:00.995183: step 3987, loss 0.588472.
Train: 2018-08-05T23:33:01.175706: step 3988, loss 0.510486.
Train: 2018-08-05T23:33:01.349748: step 3989, loss 0.56245.
Train: 2018-08-05T23:33:01.521131: step 3990, loss 0.579726.
Test: 2018-08-05T23:33:01.759626: step 3990, loss 0.547949.
Train: 2018-08-05T23:33:01.926629: step 3991, loss 0.519166.
Train: 2018-08-05T23:33:02.091189: step 3992, loss 0.519355.
Train: 2018-08-05T23:33:02.238807: step 3993, loss 0.61424.
Train: 2018-08-05T23:33:02.409245: step 3994, loss 0.605552.
Train: 2018-08-05T23:33:02.574777: step 3995, loss 0.596949.
Train: 2018-08-05T23:33:02.739590: step 3996, loss 0.62262.
Train: 2018-08-05T23:33:02.906144: step 3997, loss 0.553761.
Train: 2018-08-05T23:33:03.067066: step 3998, loss 0.545345.
Train: 2018-08-05T23:33:03.213849: step 3999, loss 0.545434.
Train: 2018-08-05T23:33:03.384722: step 4000, loss 0.579333.
Test: 2018-08-05T23:33:03.612894: step 4000, loss 0.548205.
Train: 2018-08-05T23:33:04.359050: step 4001, loss 0.621937.
Train: 2018-08-05T23:33:04.515296: step 4002, loss 0.545493.
Train: 2018-08-05T23:33:04.691621: step 4003, loss 0.613232.
Train: 2018-08-05T23:33:04.847837: step 4004, loss 0.469964.
Train: 2018-08-05T23:33:05.000045: step 4005, loss 0.520546.
Train: 2018-08-05T23:33:05.171880: step 4006, loss 0.54564.
Train: 2018-08-05T23:33:05.336619: step 4007, loss 0.587775.
Train: 2018-08-05T23:33:05.485046: step 4008, loss 0.646936.
Train: 2018-08-05T23:33:05.655945: step 4009, loss 0.554021.
Train: 2018-08-05T23:33:05.824484: step 4010, loss 0.562457.
Test: 2018-08-05T23:33:06.061772: step 4010, loss 0.548349.
Train: 2018-08-05T23:33:06.204321: step 4011, loss 0.545623.
Train: 2018-08-05T23:33:06.374755: step 4012, loss 0.528819.
Train: 2018-08-05T23:33:06.529998: step 4013, loss 0.545691.
Train: 2018-08-05T23:33:06.692469: step 4014, loss 0.596208.
Train: 2018-08-05T23:33:06.859775: step 4015, loss 0.613017.
Train: 2018-08-05T23:33:07.020771: step 4016, loss 0.587679.
Train: 2018-08-05T23:33:07.187605: step 4017, loss 0.478382.
Train: 2018-08-05T23:33:07.349201: step 4018, loss 0.587724.
Train: 2018-08-05T23:33:07.506099: step 4019, loss 0.579382.
Train: 2018-08-05T23:33:07.662323: step 4020, loss 0.61288.
Test: 2018-08-05T23:33:07.899797: step 4020, loss 0.548377.
Train: 2018-08-05T23:33:08.064718: step 4021, loss 0.621284.
Train: 2018-08-05T23:33:08.213403: step 4022, loss 0.503725.
Train: 2018-08-05T23:33:08.369616: step 4023, loss 0.537208.
Train: 2018-08-05T23:33:08.539699: step 4024, loss 0.579244.
Train: 2018-08-05T23:33:08.705931: step 4025, loss 0.562547.
Train: 2018-08-05T23:33:08.880707: step 4026, loss 0.528962.
Train: 2018-08-05T23:33:09.042914: step 4027, loss 0.537266.
Train: 2018-08-05T23:33:09.199893: step 4028, loss 0.495231.
Train: 2018-08-05T23:33:09.347940: step 4029, loss 0.579265.
Train: 2018-08-05T23:33:09.520711: step 4030, loss 0.553932.
Test: 2018-08-05T23:33:09.761070: step 4030, loss 0.548221.
Train: 2018-08-05T23:33:09.922128: step 4031, loss 0.503075.
Train: 2018-08-05T23:33:10.103835: step 4032, loss 0.587928.
Train: 2018-08-05T23:33:10.278342: step 4033, loss 0.485585.
Train: 2018-08-05T23:33:10.449065: step 4034, loss 0.605235.
Train: 2018-08-05T23:33:10.615899: step 4035, loss 0.54515.
Train: 2018-08-05T23:33:10.769101: step 4036, loss 0.527887.
Train: 2018-08-05T23:33:10.920530: step 4037, loss 0.605537.
Train: 2018-08-05T23:33:11.082224: step 4038, loss 0.52769.
Train: 2018-08-05T23:33:11.239780: step 4039, loss 0.588588.
Train: 2018-08-05T23:33:11.396833: step 4040, loss 0.579752.
Test: 2018-08-05T23:33:11.635427: step 4040, loss 0.547837.
Train: 2018-08-05T23:33:11.808965: step 4041, loss 0.614606.
Train: 2018-08-05T23:33:11.974102: step 4042, loss 0.606003.
Train: 2018-08-05T23:33:12.139694: step 4043, loss 0.518962.
Train: 2018-08-05T23:33:12.298083: step 4044, loss 0.518979.
Train: 2018-08-05T23:33:12.454271: step 4045, loss 0.606075.
Train: 2018-08-05T23:33:12.631390: step 4046, loss 0.614506.
Train: 2018-08-05T23:33:12.803224: step 4047, loss 0.597169.
Train: 2018-08-05T23:33:12.987644: step 4048, loss 0.553849.
Train: 2018-08-05T23:33:13.159480: step 4049, loss 0.527925.
Train: 2018-08-05T23:33:13.315717: step 4050, loss 0.501809.
Test: 2018-08-05T23:33:13.581256: step 4050, loss 0.547924.
Train: 2018-08-05T23:33:13.815578: step 4051, loss 0.579609.
Train: 2018-08-05T23:33:13.985664: step 4052, loss 0.579647.
Train: 2018-08-05T23:33:14.141878: step 4053, loss 0.614324.
Train: 2018-08-05T23:33:14.298121: step 4054, loss 0.622794.
Train: 2018-08-05T23:33:14.454333: step 4055, loss 0.519204.
Train: 2018-08-05T23:33:14.610517: step 4056, loss 0.47649.
Train: 2018-08-05T23:33:14.782382: step 4057, loss 0.545061.
Train: 2018-08-05T23:33:14.944094: step 4058, loss 0.588511.
Train: 2018-08-05T23:33:15.138201: step 4059, loss 0.562647.
Train: 2018-08-05T23:33:15.285333: step 4060, loss 0.51069.
Test: 2018-08-05T23:33:15.535274: step 4060, loss 0.547962.
Train: 2018-08-05T23:33:15.707596: step 4061, loss 0.510782.
Train: 2018-08-05T23:33:15.879432: step 4062, loss 0.605405.
Train: 2018-08-05T23:33:16.020022: step 4063, loss 0.528087.
Train: 2018-08-05T23:33:16.198490: step 4064, loss 0.518983.
Train: 2018-08-05T23:33:16.339081: step 4065, loss 0.658088.
Train: 2018-08-05T23:33:16.510948: step 4066, loss 0.527729.
Train: 2018-08-05T23:33:16.675918: step 4067, loss 0.562377.
Train: 2018-08-05T23:33:16.847751: step 4068, loss 0.545127.
Train: 2018-08-05T23:33:17.003965: step 4069, loss 0.545122.
Train: 2018-08-05T23:33:17.161193: step 4070, loss 0.501348.
Test: 2018-08-05T23:33:17.395514: step 4070, loss 0.54783.
Train: 2018-08-05T23:33:17.567916: step 4071, loss 0.536426.
Train: 2018-08-05T23:33:17.723481: step 4072, loss 0.571173.
Train: 2018-08-05T23:33:17.879724: step 4073, loss 0.527591.
Train: 2018-08-05T23:33:18.035907: step 4074, loss 0.510041.
Train: 2018-08-05T23:33:18.191042: step 4075, loss 0.536357.
Train: 2018-08-05T23:33:18.347286: step 4076, loss 0.562538.
Train: 2018-08-05T23:33:18.503499: step 4077, loss 0.638317.
Train: 2018-08-05T23:33:18.660735: step 4078, loss 0.536042.
Train: 2018-08-05T23:33:18.832571: step 4079, loss 0.527119.
Train: 2018-08-05T23:33:19.004404: step 4080, loss 0.607044.
Test: 2018-08-05T23:33:19.238716: step 4080, loss 0.547682.
Train: 2018-08-05T23:33:19.410539: step 4081, loss 0.553682.
Train: 2018-08-05T23:33:19.573345: step 4082, loss 0.580354.
Train: 2018-08-05T23:33:19.722983: step 4083, loss 0.509374.
Train: 2018-08-05T23:33:19.894809: step 4084, loss 0.544753.
Train: 2018-08-05T23:33:20.063390: step 4085, loss 0.571372.
Train: 2018-08-05T23:33:20.222951: step 4086, loss 0.527036.
Train: 2018-08-05T23:33:20.379192: step 4087, loss 0.58919.
Train: 2018-08-05T23:33:20.551447: step 4088, loss 0.589169.
Train: 2018-08-05T23:33:20.697748: step 4089, loss 0.58027.
Train: 2018-08-05T23:33:20.869584: step 4090, loss 0.651186.
Test: 2018-08-05T23:33:21.113702: step 4090, loss 0.547705.
Train: 2018-08-05T23:33:21.278048: step 4091, loss 0.518335.
Train: 2018-08-05T23:33:21.435493: step 4092, loss 0.474249.
Train: 2018-08-05T23:33:21.591707: step 4093, loss 0.580173.
Train: 2018-08-05T23:33:21.774703: step 4094, loss 0.650693.
Train: 2018-08-05T23:33:21.927461: step 4095, loss 0.527319.
Train: 2018-08-05T23:33:22.097494: step 4096, loss 0.536123.
Train: 2018-08-05T23:33:22.247323: step 4097, loss 0.588766.
Train: 2018-08-05T23:33:22.403535: step 4098, loss 0.641284.
Train: 2018-08-05T23:33:22.559749: step 4099, loss 0.614827.
Train: 2018-08-05T23:33:22.706154: step 4100, loss 0.562417.
Test: 2018-08-05T23:33:22.954184: step 4100, loss 0.547897.
Train: 2018-08-05T23:33:23.726148: step 4101, loss 0.597109.
Train: 2018-08-05T23:33:23.885903: step 4102, loss 0.596969.
Train: 2018-08-05T23:33:24.041519: step 4103, loss 0.493612.
Train: 2018-08-05T23:33:24.213324: step 4104, loss 0.562439.
Train: 2018-08-05T23:33:24.369537: step 4105, loss 0.468274.
Train: 2018-08-05T23:33:24.541401: step 4106, loss 0.605184.
Train: 2018-08-05T23:33:24.713205: step 4107, loss 0.51965.
Train: 2018-08-05T23:33:24.869419: step 4108, loss 0.562376.
Train: 2018-08-05T23:33:25.030358: step 4109, loss 0.596632.
Train: 2018-08-05T23:33:25.199719: step 4110, loss 0.588043.
Test: 2018-08-05T23:33:25.449662: step 4110, loss 0.548105.
Train: 2018-08-05T23:33:25.605875: step 4111, loss 0.596588.
Train: 2018-08-05T23:33:25.762117: step 4112, loss 0.613562.
Train: 2018-08-05T23:33:25.933947: step 4113, loss 0.545421.
Train: 2018-08-05T23:33:26.090165: step 4114, loss 0.621812.
Train: 2018-08-05T23:33:26.246379: step 4115, loss 0.570901.
Train: 2018-08-05T23:33:26.402561: step 4116, loss 0.587743.
Train: 2018-08-05T23:33:26.558807: step 4117, loss 0.604486.
Train: 2018-08-05T23:33:26.746587: step 4118, loss 0.520563.
Train: 2018-08-05T23:33:26.897197: step 4119, loss 0.470497.
Train: 2018-08-05T23:33:27.053411: step 4120, loss 0.56247.
Test: 2018-08-05T23:33:27.294480: step 4120, loss 0.548457.
Train: 2018-08-05T23:33:27.450722: step 4121, loss 0.579218.
Train: 2018-08-05T23:33:27.645798: step 4122, loss 0.512266.
Train: 2018-08-05T23:33:27.803165: step 4123, loss 0.604381.
Train: 2018-08-05T23:33:27.959349: step 4124, loss 0.562465.
Train: 2018-08-05T23:33:28.142855: step 4125, loss 0.545677.
Train: 2018-08-05T23:33:28.302728: step 4126, loss 0.562468.
Train: 2018-08-05T23:33:28.458911: step 4127, loss 0.545658.
Train: 2018-08-05T23:33:28.615124: step 4128, loss 0.503554.
Train: 2018-08-05T23:33:28.763322: step 4129, loss 0.537142.
Train: 2018-08-05T23:33:28.925709: step 4130, loss 0.621645.
Test: 2018-08-05T23:33:29.174209: step 4130, loss 0.548239.
Train: 2018-08-05T23:33:29.333291: step 4131, loss 0.562432.
Train: 2018-08-05T23:33:29.505252: step 4132, loss 0.536961.
Train: 2018-08-05T23:33:29.658187: step 4133, loss 0.587889.
Train: 2018-08-05T23:33:29.818622: step 4134, loss 0.58791.
Train: 2018-08-05T23:33:29.974869: step 4135, loss 0.587953.
Train: 2018-08-05T23:33:30.131050: step 4136, loss 0.604963.
Train: 2018-08-05T23:33:30.294115: step 4137, loss 0.51144.
Train: 2018-08-05T23:33:30.450299: step 4138, loss 0.545399.
Train: 2018-08-05T23:33:30.621683: step 4139, loss 0.553922.
Train: 2018-08-05T23:33:30.786696: step 4140, loss 0.570907.
Test: 2018-08-05T23:33:31.020988: step 4140, loss 0.548139.
Train: 2018-08-05T23:33:31.177199: step 4141, loss 0.596507.
Train: 2018-08-05T23:33:31.341277: step 4142, loss 0.596466.
Train: 2018-08-05T23:33:31.497490: step 4143, loss 0.55388.
Train: 2018-08-05T23:33:31.653705: step 4144, loss 0.528361.
Train: 2018-08-05T23:33:31.802980: step 4145, loss 0.579441.
Train: 2018-08-05T23:33:31.959164: step 4146, loss 0.545374.
Train: 2018-08-05T23:33:32.115378: step 4147, loss 0.468777.
Train: 2018-08-05T23:33:32.278314: step 4148, loss 0.562428.
Train: 2018-08-05T23:33:32.450118: step 4149, loss 0.502563.
Train: 2018-08-05T23:33:32.606332: step 4150, loss 0.553848.
Test: 2018-08-05T23:33:32.833651: step 4150, loss 0.547991.
Train: 2018-08-05T23:33:32.988083: step 4151, loss 0.588167.
Train: 2018-08-05T23:33:33.144326: step 4152, loss 0.57101.
Train: 2018-08-05T23:33:33.308382: step 4153, loss 0.588296.
Train: 2018-08-05T23:33:33.464595: step 4154, loss 0.59704.
Train: 2018-08-05T23:33:33.620808: step 4155, loss 0.545033.
Train: 2018-08-05T23:33:33.770990: step 4156, loss 0.614385.
Train: 2018-08-05T23:33:33.936702: step 4157, loss 0.53653.
Train: 2018-08-05T23:33:34.092884: step 4158, loss 0.562476.
Train: 2018-08-05T23:33:34.251982: step 4159, loss 0.553803.
Train: 2018-08-05T23:33:34.395597: step 4160, loss 0.588289.
Test: 2018-08-05T23:33:34.645538: step 4160, loss 0.547938.
Train: 2018-08-05T23:33:34.793714: step 4161, loss 0.519151.
Train: 2018-08-05T23:33:34.947915: step 4162, loss 0.519214.
Train: 2018-08-05T23:33:35.104159: step 4163, loss 0.519137.
Train: 2018-08-05T23:33:35.271096: step 4164, loss 0.562466.
Train: 2018-08-05T23:33:35.427339: step 4165, loss 0.536392.
Train: 2018-08-05T23:33:35.583552: step 4166, loss 0.571144.
Train: 2018-08-05T23:33:35.744408: step 4167, loss 0.606002.
Train: 2018-08-05T23:33:35.902666: step 4168, loss 0.579938.
Train: 2018-08-05T23:33:36.046109: step 4169, loss 0.63212.
Train: 2018-08-05T23:33:36.217949: step 4170, loss 0.553667.
Test: 2018-08-05T23:33:36.457069: step 4170, loss 0.547866.
Train: 2018-08-05T23:33:36.613282: step 4171, loss 0.484185.
Train: 2018-08-05T23:33:36.770431: step 4172, loss 0.649456.
Train: 2018-08-05T23:33:36.925131: step 4173, loss 0.510357.
Train: 2018-08-05T23:33:37.092804: step 4174, loss 0.501695.
Train: 2018-08-05T23:33:37.254676: step 4175, loss 0.562418.
Train: 2018-08-05T23:33:37.395305: step 4176, loss 0.55376.
Train: 2018-08-05T23:33:37.551512: step 4177, loss 0.484166.
Train: 2018-08-05T23:33:37.707695: step 4178, loss 0.562449.
Train: 2018-08-05T23:33:37.871526: step 4179, loss 0.588676.
Train: 2018-08-05T23:33:38.027737: step 4180, loss 0.579924.
Test: 2018-08-05T23:33:38.269961: step 4180, loss 0.547797.
Train: 2018-08-05T23:33:38.426152: step 4181, loss 0.588684.
Train: 2018-08-05T23:33:38.582396: step 4182, loss 0.527471.
Train: 2018-08-05T23:33:38.746304: step 4183, loss 0.606198.
Train: 2018-08-05T23:33:38.886927: step 4184, loss 0.553688.
Train: 2018-08-05T23:33:39.043110: step 4185, loss 0.51873.
Train: 2018-08-05T23:33:39.199356: step 4186, loss 0.457458.
Train: 2018-08-05T23:33:39.364322: step 4187, loss 0.623854.
Train: 2018-08-05T23:33:39.520535: step 4188, loss 0.588769.
Train: 2018-08-05T23:33:39.676717: step 4189, loss 0.544904.
Train: 2018-08-05T23:33:39.839817: step 4190, loss 0.457138.
Test: 2018-08-05T23:33:40.089728: step 4190, loss 0.547741.
Train: 2018-08-05T23:33:40.250789: step 4191, loss 0.553706.
Train: 2018-08-05T23:33:40.410005: step 4192, loss 0.500749.
Train: 2018-08-05T23:33:40.566243: step 4193, loss 0.482889.
Train: 2018-08-05T23:33:40.722754: step 4194, loss 0.571396.
Train: 2018-08-05T23:33:40.869892: step 4195, loss 0.544736.
Train: 2018-08-05T23:33:41.026135: step 4196, loss 0.607334.
Train: 2018-08-05T23:33:41.182319: step 4197, loss 0.562613.
Train: 2018-08-05T23:33:41.348319: step 4198, loss 0.58952.
Train: 2018-08-05T23:33:41.504532: step 4199, loss 0.562687.
Train: 2018-08-05T23:33:41.660747: step 4200, loss 0.589597.
Test: 2018-08-05T23:33:41.901707: step 4200, loss 0.547612.
Train: 2018-08-05T23:33:42.666654: step 4201, loss 0.589585.
Train: 2018-08-05T23:33:42.831501: step 4202, loss 0.580525.
Train: 2018-08-05T23:33:42.989390: step 4203, loss 0.589447.
Train: 2018-08-05T23:33:43.145604: step 4204, loss 0.509035.
Train: 2018-08-05T23:33:43.308552: step 4205, loss 0.53581.
Train: 2018-08-05T23:33:43.464735: step 4206, loss 0.607132.
Train: 2018-08-05T23:33:43.620949: step 4207, loss 0.526976.
Train: 2018-08-05T23:33:43.785702: step 4208, loss 0.633713.
Train: 2018-08-05T23:33:43.930698: step 4209, loss 0.642373.
Train: 2018-08-05T23:33:44.086910: step 4210, loss 0.562515.
Test: 2018-08-05T23:33:44.332122: step 4210, loss 0.547737.
Train: 2018-08-05T23:33:44.488365: step 4211, loss 0.483242.
Train: 2018-08-05T23:33:44.644578: step 4212, loss 0.641561.
Train: 2018-08-05T23:33:44.799864: step 4213, loss 0.509874.
Train: 2018-08-05T23:33:44.950745: step 4214, loss 0.53623.
Train: 2018-08-05T23:33:45.122574: step 4215, loss 0.545001.
Train: 2018-08-05T23:33:45.284582: step 4216, loss 0.57989.
Train: 2018-08-05T23:33:45.440830: step 4217, loss 0.53633.
Train: 2018-08-05T23:33:45.597008: step 4218, loss 0.510199.
Train: 2018-08-05T23:33:45.754120: step 4219, loss 0.562442.
Train: 2018-08-05T23:33:45.919939: step 4220, loss 0.649481.
Test: 2018-08-05T23:33:46.142744: step 4220, loss 0.547868.
Train: 2018-08-05T23:33:46.315618: step 4221, loss 0.571126.
Train: 2018-08-05T23:33:46.471806: step 4222, loss 0.51901.
Train: 2018-08-05T23:33:46.628026: step 4223, loss 0.501724.
Train: 2018-08-05T23:33:46.800739: step 4224, loss 0.571104.
Train: 2018-08-05T23:33:46.943651: step 4225, loss 0.571105.
Train: 2018-08-05T23:33:47.115485: step 4226, loss 0.605776.
Train: 2018-08-05T23:33:47.268703: step 4227, loss 0.623096.
Train: 2018-08-05T23:33:47.424924: step 4228, loss 0.580848.
Train: 2018-08-05T23:33:47.565515: step 4229, loss 0.579645.
Train: 2018-08-05T23:33:47.742936: step 4230, loss 0.579633.
Test: 2018-08-05T23:33:47.966728: step 4230, loss 0.548028.
Train: 2018-08-05T23:33:48.138531: step 4231, loss 0.485157.
Train: 2018-08-05T23:33:48.299662: step 4232, loss 0.519527.
Train: 2018-08-05T23:33:48.455876: step 4233, loss 0.519477.
Train: 2018-08-05T23:33:48.612089: step 4234, loss 0.528038.
Train: 2018-08-05T23:33:48.768326: step 4235, loss 0.614023.
Train: 2018-08-05T23:33:48.924538: step 4236, loss 0.510777.
Train: 2018-08-05T23:33:49.080752: step 4237, loss 0.614101.
Train: 2018-08-05T23:33:49.243782: step 4238, loss 0.605499.
Train: 2018-08-05T23:33:49.399996: step 4239, loss 0.510671.
Train: 2018-08-05T23:33:49.556239: step 4240, loss 0.502123.
Test: 2018-08-05T23:33:49.800171: step 4240, loss 0.547951.
Train: 2018-08-05T23:33:49.956354: step 4241, loss 0.553795.
Train: 2018-08-05T23:33:50.112604: step 4242, loss 0.57104.
Train: 2018-08-05T23:33:50.267888: step 4243, loss 0.536458.
Train: 2018-08-05T23:33:50.424071: step 4244, loss 0.562412.
Train: 2018-08-05T23:33:50.595905: step 4245, loss 0.518984.
Train: 2018-08-05T23:33:50.751134: step 4246, loss 0.562428.
Train: 2018-08-05T23:33:50.907381: step 4247, loss 0.527545.
Train: 2018-08-05T23:33:51.063585: step 4248, loss 0.571208.
Train: 2018-08-05T23:33:51.237084: step 4249, loss 0.614971.
Train: 2018-08-05T23:33:51.396197: step 4250, loss 0.606242.
Test: 2018-08-05T23:33:51.623029: step 4250, loss 0.547792.
Train: 2018-08-05T23:33:51.791848: step 4251, loss 0.588672.
Train: 2018-08-05T23:33:51.948092: step 4252, loss 0.579909.
Train: 2018-08-05T23:33:52.104275: step 4253, loss 0.571148.
Train: 2018-08-05T23:33:52.252653: step 4254, loss 0.63215.
Train: 2018-08-05T23:33:52.408866: step 4255, loss 0.51031.
Train: 2018-08-05T23:33:52.565081: step 4256, loss 0.579755.
Train: 2018-08-05T23:33:52.733538: step 4257, loss 0.536443.
Train: 2018-08-05T23:33:52.894139: step 4258, loss 0.553735.
Train: 2018-08-05T23:33:53.039047: step 4259, loss 0.545126.
Train: 2018-08-05T23:33:53.195261: step 4260, loss 0.614206.
Test: 2018-08-05T23:33:53.424799: step 4260, loss 0.547968.
Train: 2018-08-05T23:33:53.596601: step 4261, loss 0.493449.
Train: 2018-08-05T23:33:53.753751: step 4262, loss 0.605475.
Train: 2018-08-05T23:33:53.894344: step 4263, loss 0.502114.
Train: 2018-08-05T23:33:54.064185: step 4264, loss 0.579639.
Train: 2018-08-05T23:33:54.235655: step 4265, loss 0.571011.
Train: 2018-08-05T23:33:54.393260: step 4266, loss 0.596859.
Train: 2018-08-05T23:33:54.549472: step 4267, loss 0.571011.
Train: 2018-08-05T23:33:54.705717: step 4268, loss 0.528047.
Train: 2018-08-05T23:33:54.864893: step 4269, loss 0.613966.
Train: 2018-08-05T23:33:55.019213: step 4270, loss 0.596703.
Test: 2018-08-05T23:33:55.259462: step 4270, loss 0.548052.
Train: 2018-08-05T23:33:55.415674: step 4271, loss 0.562463.
Train: 2018-08-05T23:33:55.572773: step 4272, loss 0.562409.
Train: 2018-08-05T23:33:55.713371: step 4273, loss 0.511148.
Train: 2018-08-05T23:33:55.881288: step 4274, loss 0.579479.
Train: 2018-08-05T23:33:56.035868: step 4275, loss 0.59655.
Train: 2018-08-05T23:33:56.192078: step 4276, loss 0.511232.
Train: 2018-08-05T23:33:56.346429: step 4277, loss 0.587943.
Train: 2018-08-05T23:33:56.502641: step 4278, loss 0.562428.
Train: 2018-08-05T23:33:56.658855: step 4279, loss 0.63908.
Train: 2018-08-05T23:33:56.819074: step 4280, loss 0.553902.
Test: 2018-08-05T23:33:57.055808: step 4280, loss 0.548204.
Train: 2018-08-05T23:33:57.212046: step 4281, loss 0.570937.
Train: 2018-08-05T23:33:57.376993: step 4282, loss 0.570888.
Train: 2018-08-05T23:33:57.533172: step 4283, loss 0.570858.
Train: 2018-08-05T23:33:57.689416: step 4284, loss 0.494855.
Train: 2018-08-05T23:33:57.845628: step 4285, loss 0.553987.
Train: 2018-08-05T23:33:58.001841: step 4286, loss 0.53708.
Train: 2018-08-05T23:33:58.158024: step 4287, loss 0.537028.
Train: 2018-08-05T23:33:58.314601: step 4288, loss 0.587867.
Train: 2018-08-05T23:33:58.470815: step 4289, loss 0.520056.
Train: 2018-08-05T23:33:58.627058: step 4290, loss 0.519935.
Test: 2018-08-05T23:33:58.860511: step 4290, loss 0.54814.
Train: 2018-08-05T23:33:59.016726: step 4291, loss 0.56239.
Train: 2018-08-05T23:33:59.188552: step 4292, loss 0.51972.
Train: 2018-08-05T23:33:59.346112: step 4293, loss 0.528166.
Train: 2018-08-05T23:33:59.502295: step 4294, loss 0.528006.
Train: 2018-08-05T23:33:59.658509: step 4295, loss 0.588258.
Train: 2018-08-05T23:33:59.814128: step 4296, loss 0.614279.
Train: 2018-08-05T23:33:59.970342: step 4297, loss 0.501742.
Train: 2018-08-05T23:34:00.142177: step 4298, loss 0.571147.
Train: 2018-08-05T23:34:00.282658: step 4299, loss 0.58857.
Train: 2018-08-05T23:34:00.438871: step 4300, loss 0.501441.
Test: 2018-08-05T23:34:00.688860: step 4300, loss 0.547813.
Train: 2018-08-05T23:34:01.389124: step 4301, loss 0.6235.
Train: 2018-08-05T23:34:01.545338: step 4302, loss 0.606132.
Train: 2018-08-05T23:34:01.701552: step 4303, loss 0.518757.
Train: 2018-08-05T23:34:01.868778: step 4304, loss 0.553648.
Train: 2018-08-05T23:34:02.024983: step 4305, loss 0.588702.
Train: 2018-08-05T23:34:02.181202: step 4306, loss 0.518673.
Train: 2018-08-05T23:34:02.341366: step 4307, loss 0.49242.
Train: 2018-08-05T23:34:02.513190: step 4308, loss 0.544917.
Train: 2018-08-05T23:34:02.669403: step 4309, loss 0.483372.
Train: 2018-08-05T23:34:02.822652: step 4310, loss 0.536062.
Test: 2018-08-05T23:34:03.072228: step 4310, loss 0.547699.
Train: 2018-08-05T23:34:03.230911: step 4311, loss 0.553694.
Train: 2018-08-05T23:34:03.374958: step 4312, loss 0.589043.
Train: 2018-08-05T23:34:03.546758: step 4313, loss 0.491435.
Train: 2018-08-05T23:34:03.703003: step 4314, loss 0.580476.
Train: 2018-08-05T23:34:03.868870: step 4315, loss 0.589499.
Train: 2018-08-05T23:34:04.024286: step 4316, loss 0.598436.
Train: 2018-08-05T23:34:04.180464: step 4317, loss 0.517882.
Train: 2018-08-05T23:34:04.328884: step 4318, loss 0.517852.
Train: 2018-08-05T23:34:04.485097: step 4319, loss 0.517773.
Train: 2018-08-05T23:34:04.641310: step 4320, loss 0.553629.
Test: 2018-08-05T23:34:04.875834: step 4320, loss 0.547598.
Train: 2018-08-05T23:34:05.043696: step 4321, loss 0.652849.
Train: 2018-08-05T23:34:05.199909: step 4322, loss 0.607658.
Train: 2018-08-05T23:34:05.369011: step 4323, loss 0.544655.
Train: 2018-08-05T23:34:05.525194: step 4324, loss 0.661388.
Train: 2018-08-05T23:34:05.681437: step 4325, loss 0.571544.
Train: 2018-08-05T23:34:05.843440: step 4326, loss 0.616114.
Train: 2018-08-05T23:34:06.007441: step 4327, loss 0.580253.
Train: 2018-08-05T23:34:06.163622: step 4328, loss 0.589041.
Train: 2018-08-05T23:34:06.317781: step 4329, loss 0.632904.
Train: 2018-08-05T23:34:06.489615: step 4330, loss 0.544922.
Test: 2018-08-05T23:34:06.729548: step 4330, loss 0.547838.
Train: 2018-08-05T23:34:06.875502: step 4331, loss 0.510094.
Train: 2018-08-05T23:34:07.045923: step 4332, loss 0.588459.
Train: 2018-08-05T23:34:07.202141: step 4333, loss 0.579682.
Train: 2018-08-05T23:34:07.356758: step 4334, loss 0.519265.
Train: 2018-08-05T23:34:07.512971: step 4335, loss 0.579577.
Train: 2018-08-05T23:34:07.669185: step 4336, loss 0.536683.
Train: 2018-08-05T23:34:07.837294: step 4337, loss 0.50247.
Train: 2018-08-05T23:34:07.999987: step 4338, loss 0.579574.
Train: 2018-08-05T23:34:08.156203: step 4339, loss 0.630782.
Train: 2018-08-05T23:34:08.305401: step 4340, loss 0.528244.
Test: 2018-08-05T23:34:08.539756: step 4340, loss 0.548124.
Train: 2018-08-05T23:34:08.695933: step 4341, loss 0.468601.
Train: 2018-08-05T23:34:08.859976: step 4342, loss 0.519773.
Train: 2018-08-05T23:34:09.031809: step 4343, loss 0.528242.
Train: 2018-08-05T23:34:09.241862: step 4344, loss 0.562441.
Train: 2018-08-05T23:34:09.398075: step 4345, loss 0.553689.
Train: 2018-08-05T23:34:09.554320: step 4346, loss 0.553788.
Train: 2018-08-05T23:34:09.710502: step 4347, loss 0.527839.
Train: 2018-08-05T23:34:09.859963: step 4348, loss 0.631579.
Train: 2018-08-05T23:34:10.031823: step 4349, loss 0.605715.
Train: 2018-08-05T23:34:10.188011: step 4350, loss 0.562426.
Test: 2018-08-05T23:34:10.421425: step 4350, loss 0.547915.
Train: 2018-08-05T23:34:10.577637: step 4351, loss 0.579799.
Train: 2018-08-05T23:34:10.744272: step 4352, loss 0.622891.
Train: 2018-08-05T23:34:10.883243: step 4353, loss 0.640095.
Train: 2018-08-05T23:34:11.039456: step 4354, loss 0.57095.
Train: 2018-08-05T23:34:11.195669: step 4355, loss 0.519444.
Train: 2018-08-05T23:34:11.359585: step 4356, loss 0.536673.
Train: 2018-08-05T23:34:11.515798: step 4357, loss 0.54538.
Train: 2018-08-05T23:34:11.672042: step 4358, loss 0.528212.
Train: 2018-08-05T23:34:11.820493: step 4359, loss 0.55391.
Train: 2018-08-05T23:34:11.976706: step 4360, loss 0.605174.
Test: 2018-08-05T23:34:12.211052: step 4360, loss 0.548096.
Train: 2018-08-05T23:34:12.375225: step 4361, loss 0.588006.
Train: 2018-08-05T23:34:12.531444: step 4362, loss 0.622116.
Train: 2018-08-05T23:34:12.687663: step 4363, loss 0.58799.
Train: 2018-08-05T23:34:12.835958: step 4364, loss 0.621865.
Train: 2018-08-05T23:34:12.992171: step 4365, loss 0.528531.
Train: 2018-08-05T23:34:13.164014: step 4366, loss 0.579237.
Train: 2018-08-05T23:34:13.320127: step 4367, loss 0.55409.
Train: 2018-08-05T23:34:13.476345: step 4368, loss 0.520312.
Train: 2018-08-05T23:34:13.632553: step 4369, loss 0.646515.
Train: 2018-08-05T23:34:13.803440: step 4370, loss 0.570801.
Test: 2018-08-05T23:34:14.026632: step 4370, loss 0.548471.
Train: 2018-08-05T23:34:14.258268: step 4371, loss 0.579245.
Train: 2018-08-05T23:34:14.414512: step 4372, loss 0.554061.
Train: 2018-08-05T23:34:14.570694: step 4373, loss 0.529218.
Train: 2018-08-05T23:34:14.737394: step 4374, loss 0.604071.
Train: 2018-08-05T23:34:14.889974: step 4375, loss 0.537573.
Train: 2018-08-05T23:34:15.044552: step 4376, loss 0.537597.
Train: 2018-08-05T23:34:15.216418: step 4377, loss 0.504315.
Train: 2018-08-05T23:34:15.367677: step 4378, loss 0.579103.
Train: 2018-08-05T23:34:15.523899: step 4379, loss 0.615885.
Train: 2018-08-05T23:34:15.680080: step 4380, loss 0.537507.
Test: 2018-08-05T23:34:15.923963: step 4380, loss 0.548512.
Train: 2018-08-05T23:34:16.080200: step 4381, loss 0.554218.
Train: 2018-08-05T23:34:16.244938: step 4382, loss 0.503996.
Train: 2018-08-05T23:34:16.398531: step 4383, loss 0.52896.
Train: 2018-08-05T23:34:16.554744: step 4384, loss 0.570917.
Train: 2018-08-05T23:34:16.710988: step 4385, loss 0.587743.
Train: 2018-08-05T23:34:16.859257: step 4386, loss 0.520245.
Train: 2018-08-05T23:34:17.016271: step 4387, loss 0.511703.
Train: 2018-08-05T23:34:17.188101: step 4388, loss 0.596415.
Train: 2018-08-05T23:34:17.342464: step 4389, loss 0.579368.
Train: 2018-08-05T23:34:17.498654: step 4390, loss 0.443008.
Test: 2018-08-05T23:34:17.741456: step 4390, loss 0.548058.
Train: 2018-08-05T23:34:17.889395: step 4391, loss 0.553823.
Train: 2018-08-05T23:34:18.048504: step 4392, loss 0.450506.
Train: 2018-08-05T23:34:18.204742: step 4393, loss 0.519223.
Train: 2018-08-05T23:34:18.367070: step 4394, loss 0.527513.
Train: 2018-08-05T23:34:18.523253: step 4395, loss 0.685173.
Train: 2018-08-05T23:34:18.679467: step 4396, loss 0.518432.
Train: 2018-08-05T23:34:18.842487: step 4397, loss 0.580156.
Train: 2018-08-05T23:34:18.998670: step 4398, loss 0.589053.
Train: 2018-08-05T23:34:19.154884: step 4399, loss 0.633508.
Train: 2018-08-05T23:34:19.326590: step 4400, loss 0.518341.
Test: 2018-08-05T23:34:19.560917: step 4400, loss 0.547683.
Train: 2018-08-05T23:34:20.280294: step 4401, loss 0.536003.
Train: 2018-08-05T23:34:20.436539: step 4402, loss 0.589258.
Train: 2018-08-05T23:34:20.592745: step 4403, loss 0.49144.
Train: 2018-08-05T23:34:20.757624: step 4404, loss 0.607012.
Train: 2018-08-05T23:34:20.913838: step 4405, loss 0.598034.
Train: 2018-08-05T23:34:21.070051: step 4406, loss 0.606846.
Train: 2018-08-05T23:34:21.241555: step 4407, loss 0.54476.
Train: 2018-08-05T23:34:21.389221: step 4408, loss 0.52705.
Train: 2018-08-05T23:34:21.544370: step 4409, loss 0.624335.
Train: 2018-08-05T23:34:21.700585: step 4410, loss 0.509527.
Test: 2018-08-05T23:34:21.944593: step 4410, loss 0.547717.
Train: 2018-08-05T23:34:22.100839: step 4411, loss 0.633113.
Train: 2018-08-05T23:34:22.264849: step 4412, loss 0.571362.
Train: 2018-08-05T23:34:22.421033: step 4413, loss 0.571261.
Train: 2018-08-05T23:34:22.577246: step 4414, loss 0.536165.
Train: 2018-08-05T23:34:22.748917: step 4415, loss 0.579845.
Train: 2018-08-05T23:34:22.897505: step 4416, loss 0.597252.
Train: 2018-08-05T23:34:23.053717: step 4417, loss 0.588629.
Train: 2018-08-05T23:34:23.228947: step 4418, loss 0.623215.
Train: 2018-08-05T23:34:23.389522: step 4419, loss 0.597064.
Train: 2018-08-05T23:34:23.545766: step 4420, loss 0.562395.
Test: 2018-08-05T23:34:23.772420: step 4420, loss 0.548052.
Train: 2018-08-05T23:34:23.928613: step 4421, loss 0.553867.
Train: 2018-08-05T23:34:24.100453: step 4422, loss 0.596545.
Train: 2018-08-05T23:34:24.248720: step 4423, loss 0.485811.
Train: 2018-08-05T23:34:24.420585: step 4424, loss 0.562392.
Train: 2018-08-05T23:34:24.576799: step 4425, loss 0.519986.
Train: 2018-08-05T23:34:24.746484: step 4426, loss 0.528434.
Train: 2018-08-05T23:34:24.896100: step 4427, loss 0.494507.
Train: 2018-08-05T23:34:25.050924: step 4428, loss 0.596274.
Train: 2018-08-05T23:34:25.207106: step 4429, loss 0.571036.
Train: 2018-08-05T23:34:25.381984: step 4430, loss 0.545395.
Test: 2018-08-05T23:34:25.616312: step 4430, loss 0.548101.
Train: 2018-08-05T23:34:25.787328: step 4431, loss 0.562111.
Train: 2018-08-05T23:34:25.946036: step 4432, loss 0.45958.
Train: 2018-08-05T23:34:26.131780: step 4433, loss 0.623008.
Train: 2018-08-05T23:34:26.310299: step 4434, loss 0.510595.
Train: 2018-08-05T23:34:26.486826: step 4435, loss 0.519289.
Train: 2018-08-05T23:34:26.695269: step 4436, loss 0.492737.
Train: 2018-08-05T23:34:26.860827: step 4437, loss 0.55342.
Train: 2018-08-05T23:34:27.030402: step 4438, loss 0.536946.
Train: 2018-08-05T23:34:27.183476: step 4439, loss 0.608034.
Train: 2018-08-05T23:34:27.351500: step 4440, loss 0.589227.
Test: 2018-08-05T23:34:27.590859: step 4440, loss 0.54766.
Train: 2018-08-05T23:34:27.767418: step 4441, loss 0.527576.
Train: 2018-08-05T23:34:27.943915: step 4442, loss 0.652204.
Train: 2018-08-05T23:34:28.120469: step 4443, loss 0.500503.
Train: 2018-08-05T23:34:28.296978: step 4444, loss 0.536413.
Train: 2018-08-05T23:34:28.467515: step 4445, loss 0.536113.
Train: 2018-08-05T23:34:28.634100: step 4446, loss 0.456586.
Train: 2018-08-05T23:34:28.802652: step 4447, loss 0.509575.
Train: 2018-08-05T23:34:28.946670: step 4448, loss 0.597988.
Train: 2018-08-05T23:34:29.118513: step 4449, loss 0.61591.
Train: 2018-08-05T23:34:29.274692: step 4450, loss 0.589124.
Test: 2018-08-05T23:34:29.513528: step 4450, loss 0.547659.
Train: 2018-08-05T23:34:29.677987: step 4451, loss 0.500355.
Train: 2018-08-05T23:34:29.867364: step 4452, loss 0.589147.
Train: 2018-08-05T23:34:30.071793: step 4453, loss 0.58033.
Train: 2018-08-05T23:34:30.258323: step 4454, loss 0.499958.
Train: 2018-08-05T23:34:30.443829: step 4455, loss 0.616117.
Train: 2018-08-05T23:34:30.634317: step 4456, loss 0.669677.
Train: 2018-08-05T23:34:30.806824: step 4457, loss 0.57144.
Train: 2018-08-05T23:34:30.969421: step 4458, loss 0.580201.
Train: 2018-08-05T23:34:31.158912: step 4459, loss 0.589157.
Train: 2018-08-05T23:34:31.335429: step 4460, loss 0.624309.
Test: 2018-08-05T23:34:31.578761: step 4460, loss 0.547762.
Train: 2018-08-05T23:34:31.749304: step 4461, loss 0.56249.
Train: 2018-08-05T23:34:31.917880: step 4462, loss 0.60611.
Train: 2018-08-05T23:34:32.089395: step 4463, loss 0.571208.
Train: 2018-08-05T23:34:32.248970: step 4464, loss 0.571132.
Train: 2018-08-05T23:34:32.408575: step 4465, loss 0.596975.
Train: 2018-08-05T23:34:32.583074: step 4466, loss 0.596656.
Train: 2018-08-05T23:34:32.744642: step 4467, loss 0.630571.
Train: 2018-08-05T23:34:32.918179: step 4468, loss 0.613432.
Train: 2018-08-05T23:34:33.077753: step 4469, loss 0.545474.
Train: 2018-08-05T23:34:33.239320: step 4470, loss 0.570812.
Test: 2018-08-05T23:34:33.475688: step 4470, loss 0.54849.
Train: 2018-08-05T23:34:33.639276: step 4471, loss 0.520702.
Train: 2018-08-05T23:34:33.800817: step 4472, loss 0.55418.
Train: 2018-08-05T23:34:33.957400: step 4473, loss 0.612347.
Train: 2018-08-05T23:34:34.117972: step 4474, loss 0.51302.
Train: 2018-08-05T23:34:34.276547: step 4475, loss 0.57088.
Train: 2018-08-05T23:34:34.436120: step 4476, loss 0.587318.
Train: 2018-08-05T23:34:34.600680: step 4477, loss 0.529716.
Train: 2018-08-05T23:34:34.759286: step 4478, loss 0.546112.
Train: 2018-08-05T23:34:34.920883: step 4479, loss 0.504962.
Train: 2018-08-05T23:34:35.081427: step 4480, loss 0.513123.
Test: 2018-08-05T23:34:35.319756: step 4480, loss 0.548714.
Train: 2018-08-05T23:34:35.485313: step 4481, loss 0.562599.
Train: 2018-08-05T23:34:35.651869: step 4482, loss 0.579063.
Train: 2018-08-05T23:34:35.809447: step 4483, loss 0.570844.
Train: 2018-08-05T23:34:35.979029: step 4484, loss 0.545881.
Train: 2018-08-05T23:34:36.140561: step 4485, loss 0.57084.
Train: 2018-08-05T23:34:36.300166: step 4486, loss 0.529109.
Train: 2018-08-05T23:34:36.470678: step 4487, loss 0.537304.
Train: 2018-08-05T23:34:36.635264: step 4488, loss 0.54566.
Train: 2018-08-05T23:34:36.797802: step 4489, loss 0.579338.
Train: 2018-08-05T23:34:36.957407: step 4490, loss 0.655495.
Test: 2018-08-05T23:34:37.195739: step 4490, loss 0.548267.
Train: 2018-08-05T23:34:37.372292: step 4491, loss 0.537084.
Train: 2018-08-05T23:34:37.538822: step 4492, loss 0.570876.
Train: 2018-08-05T23:34:37.700390: step 4493, loss 0.570878.
Train: 2018-08-05T23:34:37.859962: step 4494, loss 0.511596.
Train: 2018-08-05T23:34:38.016575: step 4495, loss 0.545471.
Train: 2018-08-05T23:34:38.177116: step 4496, loss 0.562442.
Train: 2018-08-05T23:34:38.338683: step 4497, loss 0.553844.
Train: 2018-08-05T23:34:38.498288: step 4498, loss 0.502676.
Train: 2018-08-05T23:34:38.665839: step 4499, loss 0.596632.
Train: 2018-08-05T23:34:38.823412: step 4500, loss 0.570897.
Test: 2018-08-05T23:34:39.061748: step 4500, loss 0.548019.
Train: 2018-08-05T23:34:39.807525: step 4501, loss 0.605379.
Train: 2018-08-05T23:34:39.972084: step 4502, loss 0.485161.
Train: 2018-08-05T23:34:40.133652: step 4503, loss 0.528006.
Train: 2018-08-05T23:34:40.294254: step 4504, loss 0.545073.
Train: 2018-08-05T23:34:40.458782: step 4505, loss 0.562274.
Train: 2018-08-05T23:34:40.617391: step 4506, loss 0.562257.
Train: 2018-08-05T23:34:40.778956: step 4507, loss 0.614538.
Train: 2018-08-05T23:34:40.947475: step 4508, loss 0.588713.
Train: 2018-08-05T23:34:41.107049: step 4509, loss 0.510253.
Train: 2018-08-05T23:34:41.267653: step 4510, loss 0.570956.
Test: 2018-08-05T23:34:41.505982: step 4510, loss 0.547798.
Train: 2018-08-05T23:34:41.672569: step 4511, loss 0.527443.
Train: 2018-08-05T23:34:41.840130: step 4512, loss 0.59759.
Train: 2018-08-05T23:34:41.996700: step 4513, loss 0.623865.
Train: 2018-08-05T23:34:42.159235: step 4514, loss 0.562027.
Train: 2018-08-05T23:34:42.316839: step 4515, loss 0.57106.
Train: 2018-08-05T23:34:42.477410: step 4516, loss 0.536139.
Train: 2018-08-05T23:34:42.639982: step 4517, loss 0.527623.
Train: 2018-08-05T23:34:42.801516: step 4518, loss 0.56247.
Train: 2018-08-05T23:34:42.960119: step 4519, loss 0.624172.
Train: 2018-08-05T23:34:43.118700: step 4520, loss 0.562793.
Test: 2018-08-05T23:34:43.355037: step 4520, loss 0.547832.
Train: 2018-08-05T23:34:43.516637: step 4521, loss 0.562524.
Train: 2018-08-05T23:34:43.678198: step 4522, loss 0.596926.
Train: 2018-08-05T23:34:43.841762: step 4523, loss 0.562525.
Train: 2018-08-05T23:34:43.999344: step 4524, loss 0.519286.
Train: 2018-08-05T23:34:44.162877: step 4525, loss 0.605533.
Train: 2018-08-05T23:34:44.327436: step 4526, loss 0.605508.
Train: 2018-08-05T23:34:44.489037: step 4527, loss 0.562418.
Train: 2018-08-05T23:34:44.646582: step 4528, loss 0.588047.
Train: 2018-08-05T23:34:44.812140: step 4529, loss 0.587958.
Train: 2018-08-05T23:34:44.967724: step 4530, loss 0.580582.
Test: 2018-08-05T23:34:45.206086: step 4530, loss 0.548222.
Train: 2018-08-05T23:34:45.373679: step 4531, loss 0.528481.
Train: 2018-08-05T23:34:45.533224: step 4532, loss 0.630014.
Train: 2018-08-05T23:34:45.698769: step 4533, loss 0.579246.
Train: 2018-08-05T23:34:45.865350: step 4534, loss 0.495204.
Train: 2018-08-05T23:34:46.031878: step 4535, loss 0.570848.
Train: 2018-08-05T23:34:46.189482: step 4536, loss 0.486932.
Train: 2018-08-05T23:34:46.360997: step 4537, loss 0.537225.
Train: 2018-08-05T23:34:46.521602: step 4538, loss 0.495123.
Train: 2018-08-05T23:34:46.682171: step 4539, loss 0.562359.
Train: 2018-08-05T23:34:46.844704: step 4540, loss 0.545551.
Test: 2018-08-05T23:34:47.084072: step 4540, loss 0.548213.
Train: 2018-08-05T23:34:47.244635: step 4541, loss 0.596347.
Train: 2018-08-05T23:34:47.403237: step 4542, loss 0.553942.
Train: 2018-08-05T23:34:47.556830: step 4543, loss 0.596504.
Train: 2018-08-05T23:34:47.719365: step 4544, loss 0.579461.
Train: 2018-08-05T23:34:47.875946: step 4545, loss 0.596543.
Train: 2018-08-05T23:34:48.037513: step 4546, loss 0.553786.
Train: 2018-08-05T23:34:48.194107: step 4547, loss 0.553864.
Train: 2018-08-05T23:34:48.352672: step 4548, loss 0.639144.
Train: 2018-08-05T23:34:48.505290: step 4549, loss 0.570956.
Train: 2018-08-05T23:34:48.664864: step 4550, loss 0.587915.
Test: 2018-08-05T23:34:48.905194: step 4550, loss 0.548196.
Train: 2018-08-05T23:34:49.064767: step 4551, loss 0.579393.
Train: 2018-08-05T23:34:49.224339: step 4552, loss 0.494612.
Train: 2018-08-05T23:34:49.387902: step 4553, loss 0.537014.
Train: 2018-08-05T23:34:49.547475: step 4554, loss 0.587868.
Train: 2018-08-05T23:34:49.705080: step 4555, loss 0.554009.
Train: 2018-08-05T23:34:49.862632: step 4556, loss 0.57935.
Train: 2018-08-05T23:34:50.026222: step 4557, loss 0.545492.
Train: 2018-08-05T23:34:50.186765: step 4558, loss 0.520024.
Train: 2018-08-05T23:34:50.348339: step 4559, loss 0.545435.
Train: 2018-08-05T23:34:50.514897: step 4560, loss 0.61341.
Test: 2018-08-05T23:34:50.752260: step 4560, loss 0.548168.
Train: 2018-08-05T23:34:50.913853: step 4561, loss 0.622018.
Train: 2018-08-05T23:34:51.074392: step 4562, loss 0.587789.
Train: 2018-08-05T23:34:51.235992: step 4563, loss 0.638804.
Train: 2018-08-05T23:34:51.395559: step 4564, loss 0.545434.
Train: 2018-08-05T23:34:51.551118: step 4565, loss 0.528582.
Train: 2018-08-05T23:34:51.709727: step 4566, loss 0.570938.
Train: 2018-08-05T23:34:51.875251: step 4567, loss 0.604573.
Train: 2018-08-05T23:34:52.037816: step 4568, loss 0.520458.
Train: 2018-08-05T23:34:52.193432: step 4569, loss 0.545637.
Train: 2018-08-05T23:34:52.353004: step 4570, loss 0.570768.
Test: 2018-08-05T23:34:52.591335: step 4570, loss 0.548381.
Train: 2018-08-05T23:34:52.755895: step 4571, loss 0.512041.
Train: 2018-08-05T23:34:52.916491: step 4572, loss 0.629717.
Train: 2018-08-05T23:34:53.079032: step 4573, loss 0.654982.
Train: 2018-08-05T23:34:53.245596: step 4574, loss 0.587552.
Train: 2018-08-05T23:34:53.411174: step 4575, loss 0.58746.
Train: 2018-08-05T23:34:53.571748: step 4576, loss 0.537486.
Train: 2018-08-05T23:34:53.742258: step 4577, loss 0.520877.
Train: 2018-08-05T23:34:53.903852: step 4578, loss 0.620772.
Train: 2018-08-05T23:34:54.064427: step 4579, loss 0.545869.
Train: 2018-08-05T23:34:54.220977: step 4580, loss 0.570784.
Test: 2018-08-05T23:34:54.458343: step 4580, loss 0.548631.
Train: 2018-08-05T23:34:54.618914: step 4581, loss 0.570908.
Train: 2018-08-05T23:34:54.779484: step 4582, loss 0.579134.
Train: 2018-08-05T23:34:54.943047: step 4583, loss 0.570825.
Train: 2018-08-05T23:34:55.107606: step 4584, loss 0.562616.
Train: 2018-08-05T23:34:55.265184: step 4585, loss 0.579082.
Train: 2018-08-05T23:34:55.436726: step 4586, loss 0.554316.
Train: 2018-08-05T23:34:55.596300: step 4587, loss 0.52127.
Train: 2018-08-05T23:34:55.759894: step 4588, loss 0.471674.
Train: 2018-08-05T23:34:55.928411: step 4589, loss 0.546002.
Train: 2018-08-05T23:34:56.088012: step 4590, loss 0.545825.
Test: 2018-08-05T23:34:56.325350: step 4590, loss 0.548503.
Train: 2018-08-05T23:34:56.486917: step 4591, loss 0.545862.
Train: 2018-08-05T23:34:56.651478: step 4592, loss 0.570873.
Train: 2018-08-05T23:34:56.809088: step 4593, loss 0.562408.
Train: 2018-08-05T23:34:56.974647: step 4594, loss 0.587762.
Train: 2018-08-05T23:34:57.134187: step 4595, loss 0.54549.
Train: 2018-08-05T23:34:57.290794: step 4596, loss 0.519986.
Train: 2018-08-05T23:34:57.450341: step 4597, loss 0.570853.
Train: 2018-08-05T23:34:57.608944: step 4598, loss 0.519762.
Train: 2018-08-05T23:34:57.764513: step 4599, loss 0.536724.
Train: 2018-08-05T23:34:57.923102: step 4600, loss 0.485205.
Test: 2018-08-05T23:34:58.161453: step 4600, loss 0.547958.
Train: 2018-08-05T23:34:58.894593: step 4601, loss 0.596831.
Train: 2018-08-05T23:34:59.053168: step 4602, loss 0.519115.
Train: 2018-08-05T23:34:59.217703: step 4603, loss 0.527615.
Train: 2018-08-05T23:34:59.377304: step 4604, loss 0.597103.
Train: 2018-08-05T23:34:59.536878: step 4605, loss 0.518797.
Train: 2018-08-05T23:34:59.699415: step 4606, loss 0.553513.
Train: 2018-08-05T23:34:59.861015: step 4607, loss 0.589236.
Train: 2018-08-05T23:35:00.017595: step 4608, loss 0.60641.
Train: 2018-08-05T23:35:00.173181: step 4609, loss 0.571485.
Train: 2018-08-05T23:35:00.343693: step 4610, loss 0.51826.
Test: 2018-08-05T23:35:00.583058: step 4610, loss 0.547666.
Train: 2018-08-05T23:35:00.755620: step 4611, loss 0.571703.
Train: 2018-08-05T23:35:00.916167: step 4612, loss 0.59829.
Train: 2018-08-05T23:35:01.075734: step 4613, loss 0.615809.
Train: 2018-08-05T23:35:01.240325: step 4614, loss 0.668847.
Train: 2018-08-05T23:35:01.400866: step 4615, loss 0.571404.
Train: 2018-08-05T23:35:01.572438: step 4616, loss 0.536247.
Train: 2018-08-05T23:35:01.735001: step 4617, loss 0.553772.
Train: 2018-08-05T23:35:01.894545: step 4618, loss 0.553709.
Train: 2018-08-05T23:35:02.054144: step 4619, loss 0.492744.
Train: 2018-08-05T23:35:02.215712: step 4620, loss 0.579747.
Test: 2018-08-05T23:35:02.453077: step 4620, loss 0.547843.
Train: 2018-08-05T23:35:02.615616: step 4621, loss 0.545032.
Train: 2018-08-05T23:35:02.776215: step 4622, loss 0.614588.
Train: 2018-08-05T23:35:02.935798: step 4623, loss 0.518981.
Train: 2018-08-05T23:35:03.096364: step 4624, loss 0.545027.
Train: 2018-08-05T23:35:03.257898: step 4625, loss 0.510345.
Train: 2018-08-05T23:35:03.416485: step 4626, loss 0.579857.
Train: 2018-08-05T23:35:03.576048: step 4627, loss 0.553692.
Train: 2018-08-05T23:35:03.734623: step 4628, loss 0.614503.
Train: 2018-08-05T23:35:03.889209: step 4629, loss 0.597135.
Train: 2018-08-05T23:35:04.047786: step 4630, loss 0.605689.
Test: 2018-08-05T23:35:04.286173: step 4630, loss 0.547927.
Train: 2018-08-05T23:35:04.442730: step 4631, loss 0.553682.
Train: 2018-08-05T23:35:04.599320: step 4632, loss 0.545193.
Train: 2018-08-05T23:35:04.758915: step 4633, loss 0.63134.
Train: 2018-08-05T23:35:04.920452: step 4634, loss 0.596747.
Train: 2018-08-05T23:35:05.087005: step 4635, loss 0.545275.
Train: 2018-08-05T23:35:05.257579: step 4636, loss 0.58801.
Train: 2018-08-05T23:35:05.416158: step 4637, loss 0.562364.
Train: 2018-08-05T23:35:05.583693: step 4638, loss 0.579417.
Train: 2018-08-05T23:35:05.743252: step 4639, loss 0.494633.
Train: 2018-08-05T23:35:05.913827: step 4640, loss 0.562422.
Test: 2018-08-05T23:35:06.150171: step 4640, loss 0.548249.
Train: 2018-08-05T23:35:06.314723: step 4641, loss 0.579364.
Train: 2018-08-05T23:35:06.473335: step 4642, loss 0.50322.
Train: 2018-08-05T23:35:06.631875: step 4643, loss 0.587777.
Train: 2018-08-05T23:35:06.791447: step 4644, loss 0.562432.
Train: 2018-08-05T23:35:06.945063: step 4645, loss 0.520136.
Train: 2018-08-05T23:35:07.107603: step 4646, loss 0.68101.
Train: 2018-08-05T23:35:07.263185: step 4647, loss 0.553947.
Train: 2018-08-05T23:35:07.423757: step 4648, loss 0.545546.
Train: 2018-08-05T23:35:07.582359: step 4649, loss 0.579325.
Train: 2018-08-05T23:35:07.742936: step 4650, loss 0.638375.
Test: 2018-08-05T23:35:07.982272: step 4650, loss 0.548351.
Train: 2018-08-05T23:35:08.149841: step 4651, loss 0.520366.
Train: 2018-08-05T23:35:08.305400: step 4652, loss 0.570841.
Train: 2018-08-05T23:35:08.463974: step 4653, loss 0.604473.
Train: 2018-08-05T23:35:08.624547: step 4654, loss 0.612769.
Train: 2018-08-05T23:35:08.782129: step 4655, loss 0.520691.
Train: 2018-08-05T23:35:08.938705: step 4656, loss 0.579183.
Train: 2018-08-05T23:35:09.105291: step 4657, loss 0.554148.
Train: 2018-08-05T23:35:09.264864: step 4658, loss 0.579129.
Train: 2018-08-05T23:35:09.434411: step 4659, loss 0.545878.
Train: 2018-08-05T23:35:09.599964: step 4660, loss 0.496012.
Test: 2018-08-05T23:35:09.838300: step 4660, loss 0.548558.
Train: 2018-08-05T23:35:09.999868: step 4661, loss 0.545832.
Train: 2018-08-05T23:35:10.158444: step 4662, loss 0.562482.
Train: 2018-08-05T23:35:10.314027: step 4663, loss 0.570805.
Train: 2018-08-05T23:35:10.477590: step 4664, loss 0.587565.
Train: 2018-08-05T23:35:10.631232: step 4665, loss 0.621044.
Train: 2018-08-05T23:35:10.792748: step 4666, loss 0.487166.
Train: 2018-08-05T23:35:10.948331: step 4667, loss 0.537318.
Train: 2018-08-05T23:35:11.103945: step 4668, loss 0.554067.
Train: 2018-08-05T23:35:11.263520: step 4669, loss 0.537196.
Train: 2018-08-05T23:35:11.423062: step 4670, loss 0.520312.
Test: 2018-08-05T23:35:11.663423: step 4670, loss 0.548251.
Train: 2018-08-05T23:35:11.826982: step 4671, loss 0.545497.
Train: 2018-08-05T23:35:12.032433: step 4672, loss 0.553894.
Train: 2018-08-05T23:35:12.177587: step 4673, loss 0.528328.
Train: 2018-08-05T23:35:12.336498: step 4674, loss 0.53677.
Train: 2018-08-05T23:35:12.503148: step 4675, loss 0.528096.
Train: 2018-08-05T23:35:12.656653: step 4676, loss 0.588275.
Train: 2018-08-05T23:35:12.812843: step 4677, loss 0.510567.
Train: 2018-08-05T23:35:12.969092: step 4678, loss 0.553732.
Train: 2018-08-05T23:35:13.127570: step 4679, loss 0.579901.
Train: 2018-08-05T23:35:13.299556: step 4680, loss 0.527434.
Test: 2018-08-05T23:35:13.536933: step 4680, loss 0.547775.
Train: 2018-08-05T23:35:13.700524: step 4681, loss 0.468987.
Train: 2018-08-05T23:35:13.856544: step 4682, loss 0.58007.
Train: 2018-08-05T23:35:14.021298: step 4683, loss 0.65094.
Train: 2018-08-05T23:35:14.197814: step 4684, loss 0.624426.
Train: 2018-08-05T23:35:14.372380: step 4685, loss 0.571374.
Train: 2018-08-05T23:35:14.523654: step 4686, loss 0.544843.
Train: 2018-08-05T23:35:14.755704: step 4687, loss 0.553675.
Train: 2018-08-05T23:35:14.934241: step 4688, loss 0.500619.
Train: 2018-08-05T23:35:15.109757: step 4689, loss 0.518303.
Train: 2018-08-05T23:35:15.276342: step 4690, loss 0.535854.
Test: 2018-08-05T23:35:15.517666: step 4690, loss 0.547666.
Train: 2018-08-05T23:35:15.691226: step 4691, loss 0.64245.
Train: 2018-08-05T23:35:15.868727: step 4692, loss 0.562538.
Train: 2018-08-05T23:35:16.034315: step 4693, loss 0.562535.
Train: 2018-08-05T23:35:16.208818: step 4694, loss 0.598065.
Train: 2018-08-05T23:35:16.371384: step 4695, loss 0.562529.
Train: 2018-08-05T23:35:16.538965: step 4696, loss 0.553618.
Train: 2018-08-05T23:35:16.699505: step 4697, loss 0.580209.
Train: 2018-08-05T23:35:16.865063: step 4698, loss 0.580141.
Train: 2018-08-05T23:35:17.024672: step 4699, loss 0.544876.
Train: 2018-08-05T23:35:17.186204: step 4700, loss 0.50963.
Test: 2018-08-05T23:35:17.426581: step 4700, loss 0.54774.
Train: 2018-08-05T23:35:18.216176: step 4701, loss 0.500894.
Train: 2018-08-05T23:35:18.378773: step 4702, loss 0.553695.
Train: 2018-08-05T23:35:18.544300: step 4703, loss 0.491945.
Train: 2018-08-05T23:35:18.716837: step 4704, loss 0.535985.
Train: 2018-08-05T23:35:18.883392: step 4705, loss 0.518312.
Train: 2018-08-05T23:35:19.049946: step 4706, loss 0.56256.
Train: 2018-08-05T23:35:19.216500: step 4707, loss 0.544723.
Train: 2018-08-05T23:35:19.390038: step 4708, loss 0.58044.
Train: 2018-08-05T23:35:19.560582: step 4709, loss 0.526735.
Train: 2018-08-05T23:35:19.733119: step 4710, loss 0.571641.
Test: 2018-08-05T23:35:19.972479: step 4710, loss 0.547617.
Train: 2018-08-05T23:35:20.152025: step 4711, loss 0.571652.
Train: 2018-08-05T23:35:20.316560: step 4712, loss 0.580518.
Train: 2018-08-05T23:35:20.482116: step 4713, loss 0.580485.
Train: 2018-08-05T23:35:20.646707: step 4714, loss 0.607461.
Train: 2018-08-05T23:35:20.809275: step 4715, loss 0.553582.
Train: 2018-08-05T23:35:20.974799: step 4716, loss 0.589459.
Train: 2018-08-05T23:35:21.144345: step 4717, loss 0.589321.
Train: 2018-08-05T23:35:21.309935: step 4718, loss 0.562465.
Train: 2018-08-05T23:35:21.472497: step 4719, loss 0.527058.
Train: 2018-08-05T23:35:21.637028: step 4720, loss 0.562593.
Test: 2018-08-05T23:35:21.882371: step 4720, loss 0.547693.
Train: 2018-08-05T23:35:22.051918: step 4721, loss 0.642172.
Train: 2018-08-05T23:35:22.224458: step 4722, loss 0.527196.
Train: 2018-08-05T23:35:22.400984: step 4723, loss 0.650398.
Train: 2018-08-05T23:35:22.569533: step 4724, loss 0.536252.
Train: 2018-08-05T23:35:22.738082: step 4725, loss 0.658605.
Train: 2018-08-05T23:35:22.907663: step 4726, loss 0.536362.
Train: 2018-08-05T23:35:23.103131: step 4727, loss 0.510487.
Train: 2018-08-05T23:35:23.295592: step 4728, loss 0.614163.
Train: 2018-08-05T23:35:23.479101: step 4729, loss 0.553782.
Train: 2018-08-05T23:35:23.659620: step 4730, loss 0.570962.
Test: 2018-08-05T23:35:23.907958: step 4730, loss 0.548079.
Train: 2018-08-05T23:35:24.096449: step 4731, loss 0.52821.
Train: 2018-08-05T23:35:24.288936: step 4732, loss 0.579487.
Train: 2018-08-05T23:35:24.478430: step 4733, loss 0.553898.
Train: 2018-08-05T23:35:24.637302: step 4734, loss 0.519903.
Train: 2018-08-05T23:35:24.850719: step 4735, loss 0.613444.
Train: 2018-08-05T23:35:25.014145: step 4736, loss 0.604856.
Train: 2018-08-05T23:35:25.204873: step 4737, loss 0.63018.
Train: 2018-08-05T23:35:25.384397: step 4738, loss 0.503334.
Train: 2018-08-05T23:35:25.531214: step 4739, loss 0.621429.
Train: 2018-08-05T23:35:25.716580: step 4740, loss 0.57085.
Test: 2018-08-05T23:35:25.955787: step 4740, loss 0.548421.
Train: 2018-08-05T23:35:26.127064: step 4741, loss 0.629517.
Train: 2018-08-05T23:35:26.318520: step 4742, loss 0.554124.
Train: 2018-08-05T23:35:26.472971: step 4743, loss 0.495891.
Train: 2018-08-05T23:35:26.634181: step 4744, loss 0.537557.
Train: 2018-08-05T23:35:26.786138: step 4745, loss 0.512623.
Train: 2018-08-05T23:35:26.951630: step 4746, loss 0.512558.
Train: 2018-08-05T23:35:27.097350: step 4747, loss 0.562482.
Train: 2018-08-05T23:35:27.275188: step 4748, loss 0.570828.
Train: 2018-08-05T23:35:27.426587: step 4749, loss 0.570839.
Train: 2018-08-05T23:35:27.601573: step 4750, loss 0.554069.
Test: 2018-08-05T23:35:27.849906: step 4750, loss 0.548372.
Train: 2018-08-05T23:35:28.002575: step 4751, loss 0.604467.
Train: 2018-08-05T23:35:28.166979: step 4752, loss 0.486755.
Train: 2018-08-05T23:35:28.340523: step 4753, loss 0.613016.
Train: 2018-08-05T23:35:28.534037: step 4754, loss 0.587748.
Train: 2018-08-05T23:35:28.684552: step 4755, loss 0.545537.
Train: 2018-08-05T23:35:28.854490: step 4756, loss 0.553975.
Train: 2018-08-05T23:35:29.015031: step 4757, loss 0.579335.
Train: 2018-08-05T23:35:29.208488: step 4758, loss 0.570887.
Train: 2018-08-05T23:35:29.388704: step 4759, loss 0.613247.
Train: 2018-08-05T23:35:29.560271: step 4760, loss 0.570885.
Test: 2018-08-05T23:35:29.785930: step 4760, loss 0.548249.
Train: 2018-08-05T23:35:29.963999: step 4761, loss 0.587805.
Train: 2018-08-05T23:35:30.121406: step 4762, loss 0.562424.
Train: 2018-08-05T23:35:30.276759: step 4763, loss 0.511762.
Train: 2018-08-05T23:35:30.449857: step 4764, loss 0.596221.
Train: 2018-08-05T23:35:30.610555: step 4765, loss 0.545541.
Train: 2018-08-05T23:35:30.761265: step 4766, loss 0.570869.
Train: 2018-08-05T23:35:30.917484: step 4767, loss 0.52019.
Train: 2018-08-05T23:35:31.073667: step 4768, loss 0.53705.
Train: 2018-08-05T23:35:31.229879: step 4769, loss 0.54548.
Train: 2018-08-05T23:35:31.401714: step 4770, loss 0.545448.
Test: 2018-08-05T23:35:31.638731: step 4770, loss 0.54817.
Train: 2018-08-05T23:35:31.805086: step 4771, loss 0.528403.
Train: 2018-08-05T23:35:32.022505: step 4772, loss 0.570928.
Train: 2018-08-05T23:35:32.193047: step 4773, loss 0.519686.
Train: 2018-08-05T23:35:32.373564: step 4774, loss 0.596668.
Train: 2018-08-05T23:35:32.538154: step 4775, loss 0.613901.
Train: 2018-08-05T23:35:32.706707: step 4776, loss 0.562395.
Train: 2018-08-05T23:35:32.885195: step 4777, loss 0.562401.
Train: 2018-08-05T23:35:33.067708: step 4778, loss 0.467823.
Train: 2018-08-05T23:35:33.261190: step 4779, loss 0.510684.
Train: 2018-08-05T23:35:33.464646: step 4780, loss 0.527819.
Test: 2018-08-05T23:35:33.706004: step 4780, loss 0.54788.
Train: 2018-08-05T23:35:33.877548: step 4781, loss 0.597126.
Train: 2018-08-05T23:35:34.052662: step 4782, loss 0.562431.
Train: 2018-08-05T23:35:34.241160: step 4783, loss 0.606016.
Train: 2018-08-05T23:35:34.412565: step 4784, loss 0.510074.
Train: 2018-08-05T23:35:34.579758: step 4785, loss 0.606155.
Train: 2018-08-05T23:35:34.741038: step 4786, loss 0.536194.
Train: 2018-08-05T23:35:34.892761: step 4787, loss 0.614996.
Train: 2018-08-05T23:35:35.063322: step 4788, loss 0.466137.
Train: 2018-08-05T23:35:35.226302: step 4789, loss 0.518611.
Train: 2018-08-05T23:35:35.404866: step 4790, loss 0.580048.
Test: 2018-08-05T23:35:35.651167: step 4790, loss 0.547732.
Train: 2018-08-05T23:35:35.826698: step 4791, loss 0.588883.
Train: 2018-08-05T23:35:35.998237: step 4792, loss 0.580097.
Train: 2018-08-05T23:35:36.173769: step 4793, loss 0.509611.
Train: 2018-08-05T23:35:36.350327: step 4794, loss 0.633077.
Train: 2018-08-05T23:35:36.526823: step 4795, loss 0.536036.
Train: 2018-08-05T23:35:36.695374: step 4796, loss 0.562482.
Train: 2018-08-05T23:35:36.853840: step 4797, loss 0.518417.
Train: 2018-08-05T23:35:37.016643: step 4798, loss 0.562489.
Train: 2018-08-05T23:35:37.172856: step 4799, loss 0.597781.
Train: 2018-08-05T23:35:37.348119: step 4800, loss 0.527224.
Test: 2018-08-05T23:35:37.592501: step 4800, loss 0.547717.
Train: 2018-08-05T23:35:38.415927: step 4801, loss 0.518397.
Train: 2018-08-05T23:35:38.604454: step 4802, loss 0.527178.
Train: 2018-08-05T23:35:38.800895: step 4803, loss 0.544817.
Train: 2018-08-05T23:35:38.954297: step 4804, loss 0.615627.
Train: 2018-08-05T23:35:39.104121: step 4805, loss 0.509397.
Train: 2018-08-05T23:35:39.291335: step 4806, loss 0.571384.
Train: 2018-08-05T23:35:39.490778: step 4807, loss 0.571391.
Train: 2018-08-05T23:35:39.653975: step 4808, loss 0.589128.
Train: 2018-08-05T23:35:39.804010: step 4809, loss 0.562522.
Train: 2018-08-05T23:35:39.962073: step 4810, loss 0.535946.
Test: 2018-08-05T23:35:40.192753: step 4810, loss 0.547684.
Train: 2018-08-05T23:35:40.372457: step 4811, loss 0.535947.
Train: 2018-08-05T23:35:40.556966: step 4812, loss 0.589094.
Train: 2018-08-05T23:35:40.710028: step 4813, loss 0.61564.
Train: 2018-08-05T23:35:40.879864: step 4814, loss 0.544829.
Train: 2018-08-05T23:35:41.059681: step 4815, loss 0.544838.
Train: 2018-08-05T23:35:41.227929: step 4816, loss 0.518386.
Train: 2018-08-05T23:35:41.403706: step 4817, loss 0.571306.
Train: 2018-08-05T23:35:41.579462: step 4818, loss 0.624192.
Train: 2018-08-05T23:35:41.730922: step 4819, loss 0.544873.
Train: 2018-08-05T23:35:41.887135: step 4820, loss 0.492161.
Test: 2018-08-05T23:35:42.135867: step 4820, loss 0.547746.
Train: 2018-08-05T23:35:42.310426: step 4821, loss 0.562464.
Train: 2018-08-05T23:35:42.457788: step 4822, loss 0.580047.
Train: 2018-08-05T23:35:42.614002: step 4823, loss 0.544891.
Train: 2018-08-05T23:35:42.791263: step 4824, loss 0.544896.
Train: 2018-08-05T23:35:42.958848: step 4825, loss 0.553678.
Train: 2018-08-05T23:35:43.135343: step 4826, loss 0.536106.
Train: 2018-08-05T23:35:43.308879: step 4827, loss 0.606419.
Train: 2018-08-05T23:35:43.485440: step 4828, loss 0.562464.
Train: 2018-08-05T23:35:43.665924: step 4829, loss 0.650276.
Train: 2018-08-05T23:35:43.844479: step 4830, loss 0.50989.
Test: 2018-08-05T23:35:44.084804: step 4830, loss 0.54779.
Train: 2018-08-05T23:35:44.267350: step 4831, loss 0.54494.
Train: 2018-08-05T23:35:44.420210: step 4832, loss 0.674341.
Train: 2018-08-05T23:35:44.598825: step 4833, loss 0.536284.
Train: 2018-08-05T23:35:44.759706: step 4834, loss 0.50156.
Train: 2018-08-05T23:35:44.901540: step 4835, loss 0.571104.
Train: 2018-08-05T23:35:45.073343: step 4836, loss 0.466972.
Train: 2018-08-05T23:35:45.246740: step 4837, loss 0.657946.
Train: 2018-08-05T23:35:45.428253: step 4838, loss 0.510363.
Train: 2018-08-05T23:35:45.595807: step 4839, loss 0.57109.
Train: 2018-08-05T23:35:45.760854: step 4840, loss 0.501704.
Test: 2018-08-05T23:35:46.002207: step 4840, loss 0.547876.
Train: 2018-08-05T23:35:46.184746: step 4841, loss 0.605818.
Train: 2018-08-05T23:35:46.379235: step 4842, loss 0.519017.
Train: 2018-08-05T23:35:46.562725: step 4843, loss 0.562416.
Train: 2018-08-05T23:35:46.711103: step 4844, loss 0.527649.
Train: 2018-08-05T23:35:46.890104: step 4845, loss 0.571123.
Train: 2018-08-05T23:35:47.070847: step 4846, loss 0.588552.
Train: 2018-08-05T23:35:47.242716: step 4847, loss 0.562427.
Train: 2018-08-05T23:35:47.398896: step 4848, loss 0.562426.
Train: 2018-08-05T23:35:47.567385: step 4849, loss 0.597275.
Train: 2018-08-05T23:35:47.753029: step 4850, loss 0.545017.
Test: 2018-08-05T23:35:47.993563: step 4850, loss 0.547848.
Train: 2018-08-05T23:35:48.160781: step 4851, loss 0.640737.
Train: 2018-08-05T23:35:48.359250: step 4852, loss 0.52768.
Train: 2018-08-05T23:35:48.513463: step 4853, loss 0.605777.
Train: 2018-08-05T23:35:48.669645: step 4854, loss 0.527787.
Train: 2018-08-05T23:35:48.825857: step 4855, loss 0.545114.
Train: 2018-08-05T23:35:48.990103: step 4856, loss 0.562405.
Train: 2018-08-05T23:35:49.152699: step 4857, loss 0.510605.
Train: 2018-08-05T23:35:49.311625: step 4858, loss 0.62286.
Train: 2018-08-05T23:35:49.473192: step 4859, loss 0.579661.
Train: 2018-08-05T23:35:49.615656: step 4860, loss 0.536542.
Test: 2018-08-05T23:35:49.856335: step 4860, loss 0.547975.
Train: 2018-08-05T23:35:50.040513: step 4861, loss 0.63131.
Train: 2018-08-05T23:35:50.205040: step 4862, loss 0.570996.
Train: 2018-08-05T23:35:50.369482: step 4863, loss 0.519504.
Train: 2018-08-05T23:35:50.520447: step 4864, loss 0.528111.
Train: 2018-08-05T23:35:50.676661: step 4865, loss 0.570969.
Train: 2018-08-05T23:35:50.838657: step 4866, loss 0.50241.
Train: 2018-08-05T23:35:50.999243: step 4867, loss 0.510927.
Train: 2018-08-05T23:35:51.154679: step 4868, loss 0.58819.
Train: 2018-08-05T23:35:51.304800: step 4869, loss 0.571008.
Train: 2018-08-05T23:35:51.460979: step 4870, loss 0.571016.
Test: 2018-08-05T23:35:51.710920: step 4870, loss 0.547962.
Train: 2018-08-05T23:35:51.867134: step 4871, loss 0.579646.
Train: 2018-08-05T23:35:52.023347: step 4872, loss 0.571026.
Train: 2018-08-05T23:35:52.179560: step 4873, loss 0.519284.
Train: 2018-08-05T23:35:52.335773: step 4874, loss 0.545143.
Train: 2018-08-05T23:35:52.491987: step 4875, loss 0.596968.
Train: 2018-08-05T23:35:52.648232: step 4876, loss 0.57969.
Train: 2018-08-05T23:35:52.804414: step 4877, loss 0.657458.
Train: 2018-08-05T23:35:52.976274: step 4878, loss 0.571024.
Train: 2018-08-05T23:35:53.132487: step 4879, loss 0.553795.
Train: 2018-08-05T23:35:53.295164: step 4880, loss 0.519459.
Test: 2018-08-05T23:35:53.529485: step 4880, loss 0.548026.
Train: 2018-08-05T23:35:53.685723: step 4881, loss 0.528073.
Train: 2018-08-05T23:35:53.873155: step 4882, loss 0.553817.
Train: 2018-08-05T23:35:54.029392: step 4883, loss 0.510893.
Train: 2018-08-05T23:35:54.185580: step 4884, loss 0.562398.
Train: 2018-08-05T23:35:54.341793: step 4885, loss 0.510764.
Train: 2018-08-05T23:35:54.498006: step 4886, loss 0.502029.
Train: 2018-08-05T23:35:54.654219: step 4887, loss 0.562408.
Train: 2018-08-05T23:35:54.810433: step 4888, loss 0.579766.
Train: 2018-08-05T23:35:54.966646: step 4889, loss 0.536334.
Train: 2018-08-05T23:35:55.138480: step 4890, loss 0.571144.
Test: 2018-08-05T23:35:55.378843: step 4890, loss 0.547811.
Train: 2018-08-05T23:35:55.542406: step 4891, loss 0.579896.
Train: 2018-08-05T23:35:55.706985: step 4892, loss 0.632377.
Train: 2018-08-05T23:35:55.868164: step 4893, loss 0.676038.
Train: 2018-08-05T23:35:56.043463: step 4894, loss 0.483995.
Train: 2018-08-05T23:35:56.202658: step 4895, loss 0.527601.
Train: 2018-08-05T23:35:56.351135: step 4896, loss 0.57983.
Train: 2018-08-05T23:35:56.507348: step 4897, loss 0.553723.
Train: 2018-08-05T23:35:56.663587: step 4898, loss 0.62328.
Train: 2018-08-05T23:35:56.840922: step 4899, loss 0.571095.
Train: 2018-08-05T23:35:56.994503: step 4900, loss 0.588397.
Test: 2018-08-05T23:35:57.228829: step 4900, loss 0.547929.
Train: 2018-08-05T23:35:57.950514: step 4901, loss 0.553762.
Train: 2018-08-05T23:35:58.122349: step 4902, loss 0.510638.
Train: 2018-08-05T23:35:58.274466: step 4903, loss 0.596888.
Train: 2018-08-05T23:35:58.434971: step 4904, loss 0.605456.
Train: 2018-08-05T23:35:58.609656: step 4905, loss 0.5624.
Train: 2018-08-05T23:35:58.774208: step 4906, loss 0.562396.
Train: 2018-08-05T23:35:58.934326: step 4907, loss 0.605209.
Train: 2018-08-05T23:35:59.119830: step 4908, loss 0.596561.
Train: 2018-08-05T23:35:59.298374: step 4909, loss 0.579432.
Train: 2018-08-05T23:35:59.479867: step 4910, loss 0.545425.
Test: 2018-08-05T23:35:59.714240: step 4910, loss 0.548229.
Train: 2018-08-05T23:35:59.893327: step 4911, loss 0.553944.
Train: 2018-08-05T23:36:00.073865: step 4912, loss 0.579337.
Train: 2018-08-05T23:36:00.249395: step 4913, loss 0.545552.
Train: 2018-08-05T23:36:00.414617: step 4914, loss 0.553998.
Train: 2018-08-05T23:36:00.560230: step 4915, loss 0.461388.
Train: 2018-08-05T23:36:00.732589: step 4916, loss 0.511823.
Train: 2018-08-05T23:36:00.894832: step 4917, loss 0.528607.
Train: 2018-08-05T23:36:01.075465: step 4918, loss 0.570901.
Train: 2018-08-05T23:36:01.256978: step 4919, loss 0.587924.
Train: 2018-08-05T23:36:01.430876: step 4920, loss 0.47719.
Test: 2018-08-05T23:36:01.673087: step 4920, loss 0.548077.
Train: 2018-08-05T23:36:01.857593: step 4921, loss 0.562395.
Train: 2018-08-05T23:36:02.013428: step 4922, loss 0.562398.
Train: 2018-08-05T23:36:02.177614: step 4923, loss 0.536584.
Train: 2018-08-05T23:36:02.333828: step 4924, loss 0.614195.
Train: 2018-08-05T23:36:02.511437: step 4925, loss 0.553757.
Train: 2018-08-05T23:36:02.674943: step 4926, loss 0.57107.
Train: 2018-08-05T23:36:02.841101: step 4927, loss 0.588428.
Train: 2018-08-05T23:36:03.010405: step 4928, loss 0.536386.
Train: 2018-08-05T23:36:03.170373: step 4929, loss 0.571098.
Train: 2018-08-05T23:36:03.340352: step 4930, loss 0.545036.
Test: 2018-08-05T23:36:03.578582: step 4930, loss 0.547854.
Train: 2018-08-05T23:36:03.732056: step 4931, loss 0.579815.
Train: 2018-08-05T23:36:03.901564: step 4932, loss 0.597221.
Train: 2018-08-05T23:36:04.057809: step 4933, loss 0.562419.
Train: 2018-08-05T23:36:04.234166: step 4934, loss 0.562419.
Train: 2018-08-05T23:36:04.396753: step 4935, loss 0.536354.
Train: 2018-08-05T23:36:04.545984: step 4936, loss 0.527663.
Train: 2018-08-05T23:36:04.702198: step 4937, loss 0.536336.
Train: 2018-08-05T23:36:04.876898: step 4938, loss 0.545013.
Train: 2018-08-05T23:36:05.050476: step 4939, loss 0.527571.
Train: 2018-08-05T23:36:05.197235: step 4940, loss 0.606083.
Test: 2018-08-05T23:36:05.447187: step 4940, loss 0.547805.
Train: 2018-08-05T23:36:05.600999: step 4941, loss 0.606121.
Train: 2018-08-05T23:36:05.770822: step 4942, loss 0.553703.
Train: 2018-08-05T23:36:05.934368: step 4943, loss 0.518789.
Train: 2018-08-05T23:36:06.105941: step 4944, loss 0.562434.
Train: 2018-08-05T23:36:06.283434: step 4945, loss 0.562436.
Train: 2018-08-05T23:36:06.462980: step 4946, loss 0.553696.
Train: 2018-08-05T23:36:06.609025: step 4947, loss 0.649875.
Train: 2018-08-05T23:36:06.771879: step 4948, loss 0.588618.
Train: 2018-08-05T23:36:06.945055: step 4949, loss 0.597235.
Train: 2018-08-05T23:36:07.115622: step 4950, loss 0.553707.
Test: 2018-08-05T23:36:07.355732: step 4950, loss 0.547912.
Train: 2018-08-05T23:36:07.531252: step 4951, loss 0.553715.
Train: 2018-08-05T23:36:07.690065: step 4952, loss 0.545083.
Train: 2018-08-05T23:36:07.858905: step 4953, loss 0.484985.
Train: 2018-08-05T23:36:08.044440: step 4954, loss 0.545239.
Train: 2018-08-05T23:36:08.204020: step 4955, loss 0.571085.
Train: 2018-08-05T23:36:08.372694: step 4956, loss 0.484603.
Train: 2018-08-05T23:36:08.531798: step 4957, loss 0.54505.
Train: 2018-08-05T23:36:08.687979: step 4958, loss 0.605856.
Train: 2018-08-05T23:36:08.862207: step 4959, loss 0.55372.
Train: 2018-08-05T23:36:09.019521: step 4960, loss 0.58854.
Test: 2018-08-05T23:36:09.265343: step 4960, loss 0.547841.
Train: 2018-08-05T23:36:09.437185: step 4961, loss 0.597249.
Train: 2018-08-05T23:36:09.655876: step 4962, loss 0.605925.
Train: 2018-08-05T23:36:09.843333: step 4963, loss 0.588478.
Train: 2018-08-05T23:36:10.063691: step 4964, loss 0.588426.
Train: 2018-08-05T23:36:10.266768: step 4965, loss 0.596998.
Train: 2018-08-05T23:36:10.485466: step 4966, loss 0.605494.
Train: 2018-08-05T23:36:10.657299: step 4967, loss 0.57101.
Train: 2018-08-05T23:36:10.813544: step 4968, loss 0.596615.
Train: 2018-08-05T23:36:10.988951: step 4969, loss 0.53685.
Train: 2018-08-05T23:36:11.145194: step 4970, loss 0.638972.
Test: 2018-08-05T23:36:11.379515: step 4970, loss 0.548245.
Train: 2018-08-05T23:36:11.566979: step 4971, loss 0.503143.
Train: 2018-08-05T23:36:11.723153: step 4972, loss 0.579386.
Train: 2018-08-05T23:36:11.879394: step 4973, loss 0.528755.
Train: 2018-08-05T23:36:12.035604: step 4974, loss 0.570854.
Train: 2018-08-05T23:36:12.191831: step 4975, loss 0.495207.
Train: 2018-08-05T23:36:12.348036: step 4976, loss 0.545589.
Train: 2018-08-05T23:36:12.504220: step 4977, loss 0.570903.
Train: 2018-08-05T23:36:12.660462: step 4978, loss 0.520287.
Train: 2018-08-05T23:36:12.816679: step 4979, loss 0.587651.
Train: 2018-08-05T23:36:12.974325: step 4980, loss 0.596136.
Test: 2018-08-05T23:36:13.224293: step 4980, loss 0.548301.
Train: 2018-08-05T23:36:13.380479: step 4981, loss 0.55396.
Train: 2018-08-05T23:36:13.536692: step 4982, loss 0.579324.
Train: 2018-08-05T23:36:13.692936: step 4983, loss 0.580457.
Train: 2018-08-05T23:36:13.849149: step 4984, loss 0.587731.
Train: 2018-08-05T23:36:14.005363: step 4985, loss 0.587854.
Train: 2018-08-05T23:36:14.161546: step 4986, loss 0.537252.
Train: 2018-08-05T23:36:14.333380: step 4987, loss 0.503515.
Train: 2018-08-05T23:36:14.489594: step 4988, loss 0.520209.
Train: 2018-08-05T23:36:14.630186: step 4989, loss 0.570866.
Train: 2018-08-05T23:36:14.786429: step 4990, loss 0.553925.
Test: 2018-08-05T23:36:15.036372: step 4990, loss 0.548215.
Train: 2018-08-05T23:36:15.223826: step 4991, loss 0.579427.
Train: 2018-08-05T23:36:15.380009: step 4992, loss 0.681193.
Train: 2018-08-05T23:36:15.536223: step 4993, loss 0.486146.
Train: 2018-08-05T23:36:15.692436: step 4994, loss 0.6219.
Train: 2018-08-05T23:36:15.864295: step 4995, loss 0.545507.
Train: 2018-08-05T23:36:16.020484: step 4996, loss 0.604862.
Train: 2018-08-05T23:36:16.176727: step 4997, loss 0.621653.
Train: 2018-08-05T23:36:16.332911: step 4998, loss 0.545496.
Train: 2018-08-05T23:36:16.489148: step 4999, loss 0.570911.
Train: 2018-08-05T23:36:16.645362: step 5000, loss 0.545576.
Test: 2018-08-05T23:36:16.895278: step 5000, loss 0.548353.
Train: 2018-08-05T23:36:17.629480: step 5001, loss 0.604509.
Train: 2018-08-05T23:36:17.801315: step 5002, loss 0.579318.
Train: 2018-08-05T23:36:17.957559: step 5003, loss 0.528977.
Train: 2018-08-05T23:36:18.113772: step 5004, loss 0.562421.
Train: 2018-08-05T23:36:18.269988: step 5005, loss 0.562473.
Train: 2018-08-05T23:36:18.426167: step 5006, loss 0.562405.
Train: 2018-08-05T23:36:18.582381: step 5007, loss 0.587599.
Train: 2018-08-05T23:36:18.738594: step 5008, loss 0.495547.
Train: 2018-08-05T23:36:18.894807: step 5009, loss 0.503836.
Train: 2018-08-05T23:36:19.055446: step 5010, loss 0.512086.
Test: 2018-08-05T23:36:19.289768: step 5010, loss 0.548325.
Train: 2018-08-05T23:36:19.461601: step 5011, loss 0.570852.
Train: 2018-08-05T23:36:19.617844: step 5012, loss 0.579327.
Train: 2018-08-05T23:36:19.774028: step 5013, loss 0.520076.
Train: 2018-08-05T23:36:19.928494: step 5014, loss 0.528424.
Train: 2018-08-05T23:36:20.084709: step 5015, loss 0.562401.
Train: 2018-08-05T23:36:20.240921: step 5016, loss 0.493947.
Train: 2018-08-05T23:36:20.412756: step 5017, loss 0.656938.
Train: 2018-08-05T23:36:20.569001: step 5018, loss 0.571008.
Train: 2018-08-05T23:36:20.725183: step 5019, loss 0.571025.
Train: 2018-08-05T23:36:20.881396: step 5020, loss 0.484711.
Test: 2018-08-05T23:36:21.120232: step 5020, loss 0.547911.
Train: 2018-08-05T23:36:21.276445: step 5021, loss 0.54509.
Train: 2018-08-05T23:36:21.432691: step 5022, loss 0.640524.
Train: 2018-08-05T23:36:21.588871: step 5023, loss 0.527673.
Train: 2018-08-05T23:36:21.745120: step 5024, loss 0.518936.
Train: 2018-08-05T23:36:21.901327: step 5025, loss 0.571139.
Train: 2018-08-05T23:36:22.057535: step 5026, loss 0.606077.
Train: 2018-08-05T23:36:22.213754: step 5027, loss 0.536239.
Train: 2018-08-05T23:36:22.369968: step 5028, loss 0.527483.
Train: 2018-08-05T23:36:22.526182: step 5029, loss 0.448675.
Train: 2018-08-05T23:36:22.682397: step 5030, loss 0.553679.
Test: 2018-08-05T23:36:22.932331: step 5030, loss 0.547725.
Train: 2018-08-05T23:36:23.088548: step 5031, loss 0.650589.
Train: 2018-08-05T23:36:23.244735: step 5032, loss 0.518401.
Train: 2018-08-05T23:36:23.400978: step 5033, loss 0.571333.
Train: 2018-08-05T23:36:23.557188: step 5034, loss 0.562507.
Train: 2018-08-05T23:36:23.713402: step 5035, loss 0.491709.
Train: 2018-08-05T23:36:23.869615: step 5036, loss 0.544786.
Train: 2018-08-05T23:36:24.025828: step 5037, loss 0.571426.
Train: 2018-08-05T23:36:24.182044: step 5038, loss 0.553661.
Train: 2018-08-05T23:36:24.338255: step 5039, loss 0.518008.
Train: 2018-08-05T23:36:24.494468: step 5040, loss 0.562587.
Test: 2018-08-05T23:36:24.728790: step 5040, loss 0.547623.
Train: 2018-08-05T23:36:24.900592: step 5041, loss 0.571548.
Train: 2018-08-05T23:36:25.056806: step 5042, loss 0.59842.
Train: 2018-08-05T23:36:25.213049: step 5043, loss 0.661082.
Train: 2018-08-05T23:36:25.384883: step 5044, loss 0.535794.
Train: 2018-08-05T23:36:25.541066: step 5045, loss 0.607167.
Train: 2018-08-05T23:36:25.712902: step 5046, loss 0.624825.
Train: 2018-08-05T23:36:25.869114: step 5047, loss 0.535929.
Train: 2018-08-05T23:36:26.025327: step 5048, loss 0.465287.
Train: 2018-08-05T23:36:26.181540: step 5049, loss 0.580155.
Train: 2018-08-05T23:36:26.337754: step 5050, loss 0.536029.
Test: 2018-08-05T23:36:26.572075: step 5050, loss 0.547722.
Train: 2018-08-05T23:36:26.728318: step 5051, loss 0.571294.
Train: 2018-08-05T23:36:26.900146: step 5052, loss 0.659327.
Train: 2018-08-05T23:36:27.040745: step 5053, loss 0.553681.
Train: 2018-08-05T23:36:27.209332: step 5054, loss 0.536178.
Train: 2018-08-05T23:36:27.365512: step 5055, loss 0.544959.
Train: 2018-08-05T23:36:27.521726: step 5056, loss 0.544978.
Train: 2018-08-05T23:36:27.677969: step 5057, loss 0.544991.
Train: 2018-08-05T23:36:27.834185: step 5058, loss 0.544998.
Train: 2018-08-05T23:36:28.006016: step 5059, loss 0.571134.
Train: 2018-08-05T23:36:28.162199: step 5060, loss 0.588542.
Test: 2018-08-05T23:36:28.396551: step 5060, loss 0.547855.
Train: 2018-08-05T23:36:28.568354: step 5061, loss 0.51024.
Train: 2018-08-05T23:36:28.724567: step 5062, loss 0.640691.
Train: 2018-08-05T23:36:28.880811: step 5063, loss 0.640556.
Train: 2018-08-05T23:36:29.052645: step 5064, loss 0.536445.
Train: 2018-08-05T23:36:29.208828: step 5065, loss 0.493326.
Train: 2018-08-05T23:36:29.380664: step 5066, loss 0.562402.
Train: 2018-08-05T23:36:29.536877: step 5067, loss 0.484795.
Train: 2018-08-05T23:36:29.693123: step 5068, loss 0.571034.
Train: 2018-08-05T23:36:29.864954: step 5069, loss 0.60559.
Train: 2018-08-05T23:36:30.011112: step 5070, loss 0.562401.
Test: 2018-08-05T23:36:30.261060: step 5070, loss 0.547945.
Train: 2018-08-05T23:36:30.417242: step 5071, loss 0.657357.
Train: 2018-08-05T23:36:30.573455: step 5072, loss 0.579624.
Train: 2018-08-05T23:36:30.745290: step 5073, loss 0.562397.
Train: 2018-08-05T23:36:30.901502: step 5074, loss 0.536679.
Train: 2018-08-05T23:36:31.056523: step 5075, loss 0.519601.
Train: 2018-08-05T23:36:31.212736: step 5076, loss 0.528177.
Train: 2018-08-05T23:36:31.368949: step 5077, loss 0.613746.
Train: 2018-08-05T23:36:31.525163: step 5078, loss 0.545287.
Train: 2018-08-05T23:36:31.681377: step 5079, loss 0.605149.
Train: 2018-08-05T23:36:31.853210: step 5080, loss 0.536776.
Test: 2018-08-05T23:36:32.090280: step 5080, loss 0.548105.
Train: 2018-08-05T23:36:32.262113: step 5081, loss 0.622161.
Train: 2018-08-05T23:36:32.418296: step 5082, loss 0.639091.
Train: 2018-08-05T23:36:32.574539: step 5083, loss 0.562414.
Train: 2018-08-05T23:36:32.730747: step 5084, loss 0.553952.
Train: 2018-08-05T23:36:32.886935: step 5085, loss 0.6047.
Train: 2018-08-05T23:36:33.043245: step 5086, loss 0.545606.
Train: 2018-08-05T23:36:33.215111: step 5087, loss 0.57084.
Train: 2018-08-05T23:36:33.371323: step 5088, loss 0.579226.
Train: 2018-08-05T23:36:33.527536: step 5089, loss 0.637817.
Train: 2018-08-05T23:36:33.699372: step 5090, loss 0.537458.
Test: 2018-08-05T23:36:33.933661: step 5090, loss 0.548559.
Train: 2018-08-05T23:36:34.089874: step 5091, loss 0.620768.
Train: 2018-08-05T23:36:34.261709: step 5092, loss 0.562513.
Train: 2018-08-05T23:36:34.413916: step 5093, loss 0.60388.
Train: 2018-08-05T23:36:34.585751: step 5094, loss 0.554312.
Train: 2018-08-05T23:36:34.741995: step 5095, loss 0.628369.
Train: 2018-08-05T23:36:34.898177: step 5096, loss 0.521695.
Train: 2018-08-05T23:36:35.054392: step 5097, loss 0.595346.
Train: 2018-08-05T23:36:35.210634: step 5098, loss 0.538233.
Train: 2018-08-05T23:36:35.366849: step 5099, loss 0.611528.
Train: 2018-08-05T23:36:35.538682: step 5100, loss 0.538372.
Test: 2018-08-05T23:36:35.772972: step 5100, loss 0.549153.
Train: 2018-08-05T23:36:36.522825: step 5101, loss 0.522131.
Train: 2018-08-05T23:36:36.679008: step 5102, loss 0.505929.
Train: 2018-08-05T23:36:36.835221: step 5103, loss 0.587097.
Train: 2018-08-05T23:36:36.991466: step 5104, loss 0.587164.
Train: 2018-08-05T23:36:37.163301: step 5105, loss 0.554505.
Train: 2018-08-05T23:36:37.319482: step 5106, loss 0.513717.
Train: 2018-08-05T23:36:37.475696: step 5107, loss 0.570817.
Train: 2018-08-05T23:36:37.631942: step 5108, loss 0.513422.
Train: 2018-08-05T23:36:37.803751: step 5109, loss 0.529641.
Train: 2018-08-05T23:36:37.959988: step 5110, loss 0.537708.
Test: 2018-08-05T23:36:38.194308: step 5110, loss 0.548613.
Train: 2018-08-05T23:36:38.350491: step 5111, loss 0.512728.
Train: 2018-08-05T23:36:38.506735: step 5112, loss 0.495716.
Train: 2018-08-05T23:36:38.662918: step 5113, loss 0.629662.
Train: 2018-08-05T23:36:38.834751: step 5114, loss 0.520243.
Train: 2018-08-05T23:36:38.990964: step 5115, loss 0.613283.
Train: 2018-08-05T23:36:39.147177: step 5116, loss 0.519877.
Train: 2018-08-05T23:36:39.303421: step 5117, loss 0.570941.
Train: 2018-08-05T23:36:39.459605: step 5118, loss 0.528123.
Train: 2018-08-05T23:36:39.615818: step 5119, loss 0.5796.
Train: 2018-08-05T23:36:39.772030: step 5120, loss 0.467508.
Test: 2018-08-05T23:36:40.006351: step 5120, loss 0.547894.
Train: 2018-08-05T23:36:40.162565: step 5121, loss 0.562411.
Train: 2018-08-05T23:36:40.334429: step 5122, loss 0.571123.
Train: 2018-08-05T23:36:40.490611: step 5123, loss 0.623559.
Train: 2018-08-05T23:36:40.662477: step 5124, loss 0.553694.
Train: 2018-08-05T23:36:40.818690: step 5125, loss 0.536168.
Train: 2018-08-05T23:36:40.981386: step 5126, loss 0.571244.
Train: 2018-08-05T23:36:41.137634: step 5127, loss 0.571262.
Train: 2018-08-05T23:36:41.293847: step 5128, loss 0.571271.
Train: 2018-08-05T23:36:41.450055: step 5129, loss 0.544868.
Train: 2018-08-05T23:36:41.621889: step 5130, loss 0.553664.
Test: 2018-08-05T23:36:41.856210: step 5130, loss 0.547718.
Train: 2018-08-05T23:36:42.011970: step 5131, loss 0.518396.
Train: 2018-08-05T23:36:42.168153: step 5132, loss 0.597813.
Train: 2018-08-05T23:36:42.339987: step 5133, loss 0.58897.
Train: 2018-08-05T23:36:42.480580: step 5134, loss 0.505956.
Train: 2018-08-05T23:36:42.652444: step 5135, loss 0.482931.
Train: 2018-08-05T23:36:42.808627: step 5136, loss 0.51839.
Train: 2018-08-05T23:36:42.968315: step 5137, loss 0.589194.
Train: 2018-08-05T23:36:43.124529: step 5138, loss 0.535938.
Train: 2018-08-05T23:36:43.280743: step 5139, loss 0.526918.
Train: 2018-08-05T23:36:43.436955: step 5140, loss 0.678629.
Test: 2018-08-05T23:36:43.671278: step 5140, loss 0.547636.
Train: 2018-08-05T23:36:43.843141: step 5141, loss 0.553665.
Train: 2018-08-05T23:36:43.999354: step 5142, loss 0.508974.
Train: 2018-08-05T23:36:44.155572: step 5143, loss 0.518051.
Train: 2018-08-05T23:36:44.311750: step 5144, loss 0.580446.
Train: 2018-08-05T23:36:44.467963: step 5145, loss 0.607255.
Train: 2018-08-05T23:36:44.624201: step 5146, loss 0.598292.
Train: 2018-08-05T23:36:44.780420: step 5147, loss 0.607149.
Train: 2018-08-05T23:36:44.936603: step 5148, loss 0.518147.
Train: 2018-08-05T23:36:45.092846: step 5149, loss 0.588932.
Train: 2018-08-05T23:36:45.249054: step 5150, loss 0.518548.
Test: 2018-08-05T23:36:45.489934: step 5150, loss 0.547725.
Train: 2018-08-05T23:36:45.646177: step 5151, loss 0.571112.
Train: 2018-08-05T23:36:45.818012: step 5152, loss 0.57993.
Train: 2018-08-05T23:36:45.974196: step 5153, loss 0.571264.
Train: 2018-08-05T23:36:46.130437: step 5154, loss 0.61515.
Train: 2018-08-05T23:36:46.286620: step 5155, loss 0.562376.
Train: 2018-08-05T23:36:46.442864: step 5156, loss 0.536012.
Train: 2018-08-05T23:36:46.614699: step 5157, loss 0.604688.
Train: 2018-08-05T23:36:46.770881: step 5158, loss 0.570737.
Train: 2018-08-05T23:36:46.927094: step 5159, loss 0.535656.
Train: 2018-08-05T23:36:47.098929: step 5160, loss 0.627342.
Test: 2018-08-05T23:36:47.333281: step 5160, loss 0.549169.
Train: 2018-08-05T23:36:47.489493: step 5161, loss 0.519465.
Train: 2018-08-05T23:36:47.661328: step 5162, loss 0.582959.
Train: 2018-08-05T23:36:47.817535: step 5163, loss 0.542569.
Train: 2018-08-05T23:36:47.973754: step 5164, loss 0.598469.
Train: 2018-08-05T23:36:48.129937: step 5165, loss 0.529469.
Train: 2018-08-05T23:36:48.286153: step 5166, loss 0.587983.
Train: 2018-08-05T23:36:48.442394: step 5167, loss 0.589933.
Train: 2018-08-05T23:36:48.614228: step 5168, loss 0.522683.
Train: 2018-08-05T23:36:48.770442: step 5169, loss 0.622.
Train: 2018-08-05T23:36:48.926655: step 5170, loss 0.570157.
Test: 2018-08-05T23:36:49.160948: step 5170, loss 0.547996.
Train: 2018-08-05T23:36:49.332780: step 5171, loss 0.54517.
Train: 2018-08-05T23:36:49.489028: step 5172, loss 0.553824.
Train: 2018-08-05T23:36:49.645205: step 5173, loss 0.622821.
Train: 2018-08-05T23:36:49.817071: step 5174, loss 0.588246.
Train: 2018-08-05T23:36:49.973253: step 5175, loss 0.501975.
Train: 2018-08-05T23:36:50.129467: step 5176, loss 0.588311.
Train: 2018-08-05T23:36:50.285681: step 5177, loss 0.536513.
Train: 2018-08-05T23:36:50.441924: step 5178, loss 0.484747.
Train: 2018-08-05T23:36:50.598107: step 5179, loss 0.527787.
Train: 2018-08-05T23:36:50.769971: step 5180, loss 0.553779.
Test: 2018-08-05T23:36:51.004261: step 5180, loss 0.547878.
Train: 2018-08-05T23:36:51.160474: step 5181, loss 0.527665.
Train: 2018-08-05T23:36:51.332340: step 5182, loss 0.545077.
Train: 2018-08-05T23:36:51.488523: step 5183, loss 0.501384.
Train: 2018-08-05T23:36:51.644768: step 5184, loss 0.571184.
Train: 2018-08-05T23:36:51.800948: step 5185, loss 0.623863.
Train: 2018-08-05T23:36:51.967537: step 5186, loss 0.57999.
Train: 2018-08-05T23:36:52.123749: step 5187, loss 0.483398.
Train: 2018-08-05T23:36:52.279996: step 5188, loss 0.500845.
Train: 2018-08-05T23:36:52.436176: step 5189, loss 0.544802.
Train: 2018-08-05T23:36:52.592419: step 5190, loss 0.606708.
Test: 2018-08-05T23:36:52.842333: step 5190, loss 0.547669.
Train: 2018-08-05T23:36:52.996741: step 5191, loss 0.518194.
Train: 2018-08-05T23:36:53.168575: step 5192, loss 0.58914.
Train: 2018-08-05T23:36:53.309167: step 5193, loss 0.5893.
Train: 2018-08-05T23:36:53.465349: step 5194, loss 0.553697.
Train: 2018-08-05T23:36:53.637184: step 5195, loss 0.491361.
Train: 2018-08-05T23:36:53.793397: step 5196, loss 0.580361.
Train: 2018-08-05T23:36:53.938420: step 5197, loss 0.562324.
Train: 2018-08-05T23:36:54.110255: step 5198, loss 0.481901.
Train: 2018-08-05T23:36:54.266498: step 5199, loss 0.571697.
Train: 2018-08-05T23:36:54.422720: step 5200, loss 0.553847.
Test: 2018-08-05T23:36:54.657001: step 5200, loss 0.547589.
Train: 2018-08-05T23:36:55.359960: step 5201, loss 0.49111.
Train: 2018-08-05T23:36:55.516173: step 5202, loss 0.499878.
Train: 2018-08-05T23:36:55.672387: step 5203, loss 0.508375.
Train: 2018-08-05T23:36:55.844221: step 5204, loss 0.563004.
Train: 2018-08-05T23:36:56.000435: step 5205, loss 0.690468.
Train: 2018-08-05T23:36:56.156648: step 5206, loss 0.544425.
Train: 2018-08-05T23:36:56.328482: step 5207, loss 0.508145.
Train: 2018-08-05T23:36:56.484695: step 5208, loss 0.526553.
Train: 2018-08-05T23:36:56.640934: step 5209, loss 0.580962.
Train: 2018-08-05T23:36:56.797156: step 5210, loss 0.580341.
Test: 2018-08-05T23:36:57.031478: step 5210, loss 0.547569.
Train: 2018-08-05T23:36:57.187691: step 5211, loss 0.644864.
Train: 2018-08-05T23:36:57.343870: step 5212, loss 0.663297.
Train: 2018-08-05T23:36:57.500082: step 5213, loss 0.553868.
Train: 2018-08-05T23:36:57.670386: step 5214, loss 0.589861.
Train: 2018-08-05T23:36:57.826624: step 5215, loss 0.607577.
Train: 2018-08-05T23:36:57.982813: step 5216, loss 0.580352.
Train: 2018-08-05T23:36:58.139051: step 5217, loss 0.562507.
Train: 2018-08-05T23:36:58.295239: step 5218, loss 0.500865.
Train: 2018-08-05T23:36:58.451484: step 5219, loss 0.553698.
Train: 2018-08-05T23:36:58.623288: step 5220, loss 0.50993.
Test: 2018-08-05T23:36:58.857609: step 5220, loss 0.547807.
Train: 2018-08-05T23:36:59.013820: step 5221, loss 0.623598.
Train: 2018-08-05T23:36:59.170034: step 5222, loss 0.510114.
Train: 2018-08-05T23:36:59.326247: step 5223, loss 0.553713.
Train: 2018-08-05T23:36:59.498112: step 5224, loss 0.623245.
Train: 2018-08-05T23:36:59.654328: step 5225, loss 0.588461.
Train: 2018-08-05T23:36:59.810539: step 5226, loss 0.562418.
Train: 2018-08-05T23:36:59.966752: step 5227, loss 0.536546.
Train: 2018-08-05T23:37:00.122934: step 5228, loss 0.536555.
Train: 2018-08-05T23:37:00.279181: step 5229, loss 0.631248.
Train: 2018-08-05T23:37:00.450983: step 5230, loss 0.562408.
Test: 2018-08-05T23:37:00.685303: step 5230, loss 0.548059.
Train: 2018-08-05T23:37:00.841546: step 5231, loss 0.570962.
Train: 2018-08-05T23:37:01.013384: step 5232, loss 0.545312.
Train: 2018-08-05T23:37:01.169588: step 5233, loss 0.545337.
Train: 2018-08-05T23:37:01.325801: step 5234, loss 0.536827.
Train: 2018-08-05T23:37:01.497611: step 5235, loss 0.570933.
Train: 2018-08-05T23:37:01.638203: step 5236, loss 0.579449.
Train: 2018-08-05T23:37:01.810068: step 5237, loss 0.502808.
Train: 2018-08-05T23:37:01.966251: step 5238, loss 0.545369.
Train: 2018-08-05T23:37:02.122112: step 5239, loss 0.605057.
Train: 2018-08-05T23:37:02.293978: step 5240, loss 0.656241.
Test: 2018-08-05T23:37:02.528300: step 5240, loss 0.548153.
Train: 2018-08-05T23:37:02.684505: step 5241, loss 0.553897.
Train: 2018-08-05T23:37:02.840724: step 5242, loss 0.468925.
Train: 2018-08-05T23:37:02.997510: step 5243, loss 0.511387.
Train: 2018-08-05T23:37:03.169526: step 5244, loss 0.519807.
Train: 2018-08-05T23:37:03.325739: step 5245, loss 0.588038.
Train: 2018-08-05T23:37:03.481953: step 5246, loss 0.66508.
Train: 2018-08-05T23:37:03.638198: step 5247, loss 0.528203.
Train: 2018-08-05T23:37:03.794379: step 5248, loss 0.553854.
Train: 2018-08-05T23:37:03.954181: step 5249, loss 0.536738.
Train: 2018-08-05T23:37:04.110427: step 5250, loss 0.52815.
Test: 2018-08-05T23:37:04.360364: step 5250, loss 0.548039.
Train: 2018-08-05T23:37:04.516578: step 5251, loss 0.579553.
Train: 2018-08-05T23:37:04.688416: step 5252, loss 0.605324.
Train: 2018-08-05T23:37:04.844596: step 5253, loss 0.545234.
Train: 2018-08-05T23:37:05.000810: step 5254, loss 0.476529.
Train: 2018-08-05T23:37:05.157023: step 5255, loss 0.553798.
Train: 2018-08-05T23:37:05.313271: step 5256, loss 0.596902.
Train: 2018-08-05T23:37:05.469490: step 5257, loss 0.519239.
Train: 2018-08-05T23:37:05.641314: step 5258, loss 0.614304.
Train: 2018-08-05T23:37:05.797497: step 5259, loss 0.562403.
Train: 2018-08-05T23:37:05.953735: step 5260, loss 0.501785.
Test: 2018-08-05T23:37:06.188062: step 5260, loss 0.547887.
Train: 2018-08-05T23:37:06.344243: step 5261, loss 0.605758.
Train: 2018-08-05T23:37:06.500456: step 5262, loss 0.588461.
Train: 2018-08-05T23:37:06.672292: step 5263, loss 0.597121.
Train: 2018-08-05T23:37:06.828505: step 5264, loss 0.501681.
Train: 2018-08-05T23:37:06.984718: step 5265, loss 0.588472.
Train: 2018-08-05T23:37:07.140930: step 5266, loss 0.553745.
Train: 2018-08-05T23:37:07.297175: step 5267, loss 0.562412.
Train: 2018-08-05T23:37:07.453387: step 5268, loss 0.5364.
Train: 2018-08-05T23:37:07.609570: step 5269, loss 0.562374.
Train: 2018-08-05T23:37:07.765808: step 5270, loss 0.536385.
Test: 2018-08-05T23:37:08.015756: step 5270, loss 0.547869.
Train: 2018-08-05T23:37:08.171963: step 5271, loss 0.553752.
Train: 2018-08-05T23:37:08.328176: step 5272, loss 0.562413.
Train: 2018-08-05T23:37:08.484396: step 5273, loss 0.63207.
Train: 2018-08-05T23:37:08.640604: step 5274, loss 0.597211.
Train: 2018-08-05T23:37:08.812440: step 5275, loss 0.579744.
Train: 2018-08-05T23:37:08.953005: step 5276, loss 0.527772.
Train: 2018-08-05T23:37:09.109250: step 5277, loss 0.597013.
Train: 2018-08-05T23:37:09.265465: step 5278, loss 0.614261.
Train: 2018-08-05T23:37:09.421675: step 5279, loss 0.614055.
Train: 2018-08-05T23:37:09.577887: step 5280, loss 0.553819.
Test: 2018-08-05T23:37:09.827800: step 5280, loss 0.548068.
Train: 2018-08-05T23:37:09.984012: step 5281, loss 0.536753.
Train: 2018-08-05T23:37:10.140226: step 5282, loss 0.57948.
Train: 2018-08-05T23:37:10.296438: step 5283, loss 0.622022.
Train: 2018-08-05T23:37:10.452682: step 5284, loss 0.519995.
Train: 2018-08-05T23:37:10.608895: step 5285, loss 0.544339.
Train: 2018-08-05T23:37:10.780725: step 5286, loss 0.494736.
Train: 2018-08-05T23:37:10.936937: step 5287, loss 0.604764.
Train: 2018-08-05T23:37:11.093126: step 5288, loss 0.528583.
Train: 2018-08-05T23:37:11.249372: step 5289, loss 0.443976.
Train: 2018-08-05T23:37:11.405583: step 5290, loss 0.52848.
Test: 2018-08-05T23:37:11.639903: step 5290, loss 0.548142.
Train: 2018-08-05T23:37:11.811707: step 5291, loss 0.664636.
Train: 2018-08-05T23:37:11.967920: step 5292, loss 0.630565.
Train: 2018-08-05T23:37:12.124133: step 5293, loss 0.536853.
Train: 2018-08-05T23:37:12.295992: step 5294, loss 0.536847.
Train: 2018-08-05T23:37:12.436584: step 5295, loss 0.553878.
Train: 2018-08-05T23:37:12.608427: step 5296, loss 0.570912.
Train: 2018-08-05T23:37:12.764608: step 5297, loss 0.605045.
Train: 2018-08-05T23:37:12.927396: step 5298, loss 0.562384.
Train: 2018-08-05T23:37:13.078277: step 5299, loss 0.613589.
Train: 2018-08-05T23:37:13.234464: step 5300, loss 0.536876.
Test: 2018-08-05T23:37:13.468784: step 5300, loss 0.548142.
Train: 2018-08-05T23:37:14.207627: step 5301, loss 0.57949.
Train: 2018-08-05T23:37:14.379491: step 5302, loss 0.596461.
Train: 2018-08-05T23:37:14.535705: step 5303, loss 0.54541.
Train: 2018-08-05T23:37:14.707510: step 5304, loss 0.520012.
Train: 2018-08-05T23:37:14.863722: step 5305, loss 0.545476.
Train: 2018-08-05T23:37:15.019936: step 5306, loss 0.553929.
Train: 2018-08-05T23:37:15.191769: step 5307, loss 0.562425.
Train: 2018-08-05T23:37:15.332395: step 5308, loss 0.621877.
Train: 2018-08-05T23:37:15.504221: step 5309, loss 0.613342.
Train: 2018-08-05T23:37:15.660434: step 5310, loss 0.579369.
Test: 2018-08-05T23:37:15.894760: step 5310, loss 0.548255.
Train: 2018-08-05T23:37:16.066594: step 5311, loss 0.545493.
Train: 2018-08-05T23:37:16.222777: step 5312, loss 0.621561.
Train: 2018-08-05T23:37:16.378990: step 5313, loss 0.579313.
Train: 2018-08-05T23:37:16.550855: step 5314, loss 0.579264.
Train: 2018-08-05T23:37:16.707071: step 5315, loss 0.596005.
Train: 2018-08-05T23:37:16.863252: step 5316, loss 0.520611.
Train: 2018-08-05T23:37:17.019495: step 5317, loss 0.587518.
Train: 2018-08-05T23:37:17.191330: step 5318, loss 0.520794.
Train: 2018-08-05T23:37:17.347512: step 5319, loss 0.562457.
Train: 2018-08-05T23:37:17.503751: step 5320, loss 0.562466.
Test: 2018-08-05T23:37:17.738048: step 5320, loss 0.548538.
Train: 2018-08-05T23:37:17.909881: step 5321, loss 0.479187.
Train: 2018-08-05T23:37:18.064666: step 5322, loss 0.55412.
Train: 2018-08-05T23:37:18.220882: step 5323, loss 0.579194.
Train: 2018-08-05T23:37:18.377093: step 5324, loss 0.612749.
Train: 2018-08-05T23:37:18.533275: step 5325, loss 0.562443.
Train: 2018-08-05T23:37:18.705109: step 5326, loss 0.537238.
Train: 2018-08-05T23:37:18.861323: step 5327, loss 0.545624.
Train: 2018-08-05T23:37:19.017536: step 5328, loss 0.570845.
Train: 2018-08-05T23:37:19.173774: step 5329, loss 0.545753.
Train: 2018-08-05T23:37:19.345584: step 5330, loss 0.503439.
Test: 2018-08-05T23:37:19.579941: step 5330, loss 0.548232.
Train: 2018-08-05T23:37:19.736118: step 5331, loss 0.55387.
Train: 2018-08-05T23:37:19.892332: step 5332, loss 0.502945.
Train: 2018-08-05T23:37:20.048544: step 5333, loss 0.51122.
Train: 2018-08-05T23:37:20.204758: step 5334, loss 0.622498.
Train: 2018-08-05T23:37:20.360970: step 5335, loss 0.54513.
Train: 2018-08-05T23:37:20.517214: step 5336, loss 0.579827.
Train: 2018-08-05T23:37:20.673428: step 5337, loss 0.605734.
Train: 2018-08-05T23:37:20.845266: step 5338, loss 0.631696.
Train: 2018-08-05T23:37:21.001445: step 5339, loss 0.536441.
Train: 2018-08-05T23:37:21.157657: step 5340, loss 0.544974.
Test: 2018-08-05T23:37:21.392011: step 5340, loss 0.547928.
Train: 2018-08-05T23:37:21.548192: step 5341, loss 0.536505.
Train: 2018-08-05T23:37:21.720026: step 5342, loss 0.683572.
Train: 2018-08-05T23:37:21.876264: step 5343, loss 0.527881.
Train: 2018-08-05T23:37:22.032453: step 5344, loss 0.640157.
Train: 2018-08-05T23:37:22.188696: step 5345, loss 0.527958.
Train: 2018-08-05T23:37:22.344912: step 5346, loss 0.536666.
Train: 2018-08-05T23:37:22.501122: step 5347, loss 0.553838.
Train: 2018-08-05T23:37:22.672957: step 5348, loss 0.57954.
Train: 2018-08-05T23:37:22.829171: step 5349, loss 0.553804.
Train: 2018-08-05T23:37:22.987041: step 5350, loss 0.459746.
Test: 2018-08-05T23:37:23.221366: step 5350, loss 0.548054.
Train: 2018-08-05T23:37:23.377576: step 5351, loss 0.596694.
Train: 2018-08-05T23:37:23.533763: step 5352, loss 0.536687.
Train: 2018-08-05T23:37:23.689977: step 5353, loss 0.5195.
Train: 2018-08-05T23:37:23.861845: step 5354, loss 0.588206.
Train: 2018-08-05T23:37:24.016965: step 5355, loss 0.545138.
Train: 2018-08-05T23:37:24.173179: step 5356, loss 0.519274.
Train: 2018-08-05T23:37:24.329392: step 5357, loss 0.57964.
Train: 2018-08-05T23:37:24.485576: step 5358, loss 0.57978.
Train: 2018-08-05T23:37:24.641818: step 5359, loss 0.553761.
Train: 2018-08-05T23:37:24.798031: step 5360, loss 0.536345.
Test: 2018-08-05T23:37:25.036264: step 5360, loss 0.547866.
Train: 2018-08-05T23:37:25.208067: step 5361, loss 0.58848.
Train: 2018-08-05T23:37:25.364281: step 5362, loss 0.658115.
Train: 2018-08-05T23:37:25.520494: step 5363, loss 0.553734.
Train: 2018-08-05T23:37:25.676707: step 5364, loss 0.545123.
Train: 2018-08-05T23:37:25.832951: step 5365, loss 0.571096.
Train: 2018-08-05T23:37:26.004786: step 5366, loss 0.553725.
Train: 2018-08-05T23:37:26.160998: step 5367, loss 0.501943.
Train: 2018-08-05T23:37:26.317212: step 5368, loss 0.545109.
Train: 2018-08-05T23:37:26.473428: step 5369, loss 0.553728.
Train: 2018-08-05T23:37:26.629638: step 5370, loss 0.571083.
Test: 2018-08-05T23:37:26.879550: step 5370, loss 0.547892.
Train: 2018-08-05T23:37:27.035789: step 5371, loss 0.571063.
Train: 2018-08-05T23:37:27.192006: step 5372, loss 0.597049.
Train: 2018-08-05T23:37:27.348189: step 5373, loss 0.614365.
Train: 2018-08-05T23:37:27.504403: step 5374, loss 0.527778.
Train: 2018-08-05T23:37:27.660646: step 5375, loss 0.527808.
Train: 2018-08-05T23:37:27.816859: step 5376, loss 0.588368.
Train: 2018-08-05T23:37:27.988664: step 5377, loss 0.545103.
Train: 2018-08-05T23:37:28.129008: step 5378, loss 0.519235.
Train: 2018-08-05T23:37:28.300813: step 5379, loss 0.605677.
Train: 2018-08-05T23:37:28.457057: step 5380, loss 0.545082.
Test: 2018-08-05T23:37:28.691349: step 5380, loss 0.547925.
Train: 2018-08-05T23:37:28.847559: step 5381, loss 0.562353.
Train: 2018-08-05T23:37:29.003803: step 5382, loss 0.510584.
Train: 2018-08-05T23:37:29.160018: step 5383, loss 0.467199.
Train: 2018-08-05T23:37:29.316229: step 5384, loss 0.58842.
Train: 2018-08-05T23:37:29.472412: step 5385, loss 0.623286.
Train: 2018-08-05T23:37:29.628650: step 5386, loss 0.527536.
Train: 2018-08-05T23:37:29.784840: step 5387, loss 0.55367.
Train: 2018-08-05T23:37:29.972296: step 5388, loss 0.466345.
Train: 2018-08-05T23:37:30.128508: step 5389, loss 0.562344.
Train: 2018-08-05T23:37:30.284722: step 5390, loss 0.50094.
Test: 2018-08-05T23:37:30.519043: step 5390, loss 0.547717.
Train: 2018-08-05T23:37:30.690910: step 5391, loss 0.580147.
Train: 2018-08-05T23:37:30.847089: step 5392, loss 0.588911.
Train: 2018-08-05T23:37:31.003332: step 5393, loss 0.535913.
Train: 2018-08-05T23:37:31.175137: step 5394, loss 0.615465.
Train: 2018-08-05T23:37:31.331350: step 5395, loss 0.527485.
Train: 2018-08-05T23:37:31.487594: step 5396, loss 0.562785.
Train: 2018-08-05T23:37:31.643809: step 5397, loss 0.544951.
Train: 2018-08-05T23:37:31.800020: step 5398, loss 0.624949.
Train: 2018-08-05T23:37:31.956233: step 5399, loss 0.535589.
Train: 2018-08-05T23:37:32.112447: step 5400, loss 0.517934.
Test: 2018-08-05T23:37:32.364857: step 5400, loss 0.547648.
Train: 2018-08-05T23:37:33.105266: step 5401, loss 0.642794.
Train: 2018-08-05T23:37:33.277100: step 5402, loss 0.589509.
Train: 2018-08-05T23:37:33.431322: step 5403, loss 0.571505.
Train: 2018-08-05T23:37:33.587566: step 5404, loss 0.580144.
Train: 2018-08-05T23:37:33.743781: step 5405, loss 0.553523.
Train: 2018-08-05T23:37:33.917349: step 5406, loss 0.527112.
Train: 2018-08-05T23:37:34.069999: step 5407, loss 0.580107.
Train: 2018-08-05T23:37:34.226242: step 5408, loss 0.536368.
Train: 2018-08-05T23:37:34.382450: step 5409, loss 0.527129.
Train: 2018-08-05T23:37:34.538669: step 5410, loss 0.606131.
Test: 2018-08-05T23:37:34.772990: step 5410, loss 0.54779.
Train: 2018-08-05T23:37:34.934641: step 5411, loss 0.553739.
Train: 2018-08-05T23:37:35.090883: step 5412, loss 0.579964.
Train: 2018-08-05T23:37:35.247066: step 5413, loss 0.518885.
Train: 2018-08-05T23:37:35.403279: step 5414, loss 0.614659.
Train: 2018-08-05T23:37:35.559523: step 5415, loss 0.588609.
Train: 2018-08-05T23:37:35.715706: step 5416, loss 0.536323.
Train: 2018-08-05T23:37:35.887540: step 5417, loss 0.605903.
Train: 2018-08-05T23:37:36.043753: step 5418, loss 0.588509.
Train: 2018-08-05T23:37:36.199998: step 5419, loss 0.605637.
Train: 2018-08-05T23:37:36.356212: step 5420, loss 0.570962.
Test: 2018-08-05T23:37:36.590502: step 5420, loss 0.548052.
Train: 2018-08-05T23:37:36.746745: step 5421, loss 0.553834.
Train: 2018-08-05T23:37:36.918579: step 5422, loss 0.570912.
Train: 2018-08-05T23:37:37.074791: step 5423, loss 0.596454.
Train: 2018-08-05T23:37:37.231006: step 5424, loss 0.486062.
Train: 2018-08-05T23:37:37.387187: step 5425, loss 0.570882.
Train: 2018-08-05T23:37:37.543401: step 5426, loss 0.520037.
Train: 2018-08-05T23:37:37.699614: step 5427, loss 0.520003.
Train: 2018-08-05T23:37:37.855827: step 5428, loss 0.587879.
Train: 2018-08-05T23:37:38.012071: step 5429, loss 0.596372.
Train: 2018-08-05T23:37:38.168254: step 5430, loss 0.562459.
Test: 2018-08-05T23:37:38.402574: step 5430, loss 0.548202.
Train: 2018-08-05T23:37:38.558787: step 5431, loss 0.587868.
Train: 2018-08-05T23:37:38.730651: step 5432, loss 0.587787.
Train: 2018-08-05T23:37:38.886835: step 5433, loss 0.621695.
Train: 2018-08-05T23:37:39.043073: step 5434, loss 0.511748.
Train: 2018-08-05T23:37:39.199292: step 5435, loss 0.511788.
Train: 2018-08-05T23:37:39.355507: step 5436, loss 0.526319.
Train: 2018-08-05T23:37:39.511719: step 5437, loss 0.587763.
Train: 2018-08-05T23:37:39.683553: step 5438, loss 0.562245.
Train: 2018-08-05T23:37:39.839766: step 5439, loss 0.545564.
Train: 2018-08-05T23:37:39.995949: step 5440, loss 0.5111.
Test: 2018-08-05T23:37:40.230269: step 5440, loss 0.548059.
Train: 2018-08-05T23:37:40.402103: step 5441, loss 0.554325.
Train: 2018-08-05T23:37:40.558347: step 5442, loss 0.553792.
Train: 2018-08-05T23:37:40.714530: step 5443, loss 0.562725.
Train: 2018-08-05T23:37:40.870774: step 5444, loss 0.545574.
Train: 2018-08-05T23:37:41.026956: step 5445, loss 0.648797.
Train: 2018-08-05T23:37:41.183200: step 5446, loss 0.596965.
Train: 2018-08-05T23:37:41.339414: step 5447, loss 0.492702.
Train: 2018-08-05T23:37:41.511248: step 5448, loss 0.537425.
Train: 2018-08-05T23:37:41.667461: step 5449, loss 0.510373.
Train: 2018-08-05T23:37:41.823674: step 5450, loss 0.587456.
Test: 2018-08-05T23:37:42.057996: step 5450, loss 0.54784.
Train: 2018-08-05T23:37:42.214207: step 5451, loss 0.518315.
Train: 2018-08-05T23:37:42.386060: step 5452, loss 0.536766.
Train: 2018-08-05T23:37:42.542255: step 5453, loss 0.615908.
Train: 2018-08-05T23:37:42.698468: step 5454, loss 0.545096.
Train: 2018-08-05T23:37:42.854652: step 5455, loss 0.561027.
Train: 2018-08-05T23:37:43.014498: step 5456, loss 0.563376.
Train: 2018-08-05T23:37:43.170713: step 5457, loss 0.5997.
Train: 2018-08-05T23:37:43.326927: step 5458, loss 0.492692.
Train: 2018-08-05T23:37:43.483107: step 5459, loss 0.536678.
Train: 2018-08-05T23:37:43.639321: step 5460, loss 0.590336.
Test: 2018-08-05T23:37:43.889296: step 5460, loss 0.54778.
Train: 2018-08-05T23:37:44.044209: step 5461, loss 0.605865.
Train: 2018-08-05T23:37:44.200422: step 5462, loss 0.519242.
Train: 2018-08-05T23:37:44.356634: step 5463, loss 0.562079.
Train: 2018-08-05T23:37:44.512848: step 5464, loss 0.588481.
Train: 2018-08-05T23:37:44.684713: step 5465, loss 0.640415.
Train: 2018-08-05T23:37:44.840926: step 5466, loss 0.622892.
Train: 2018-08-05T23:37:45.001698: step 5467, loss 0.665653.
Train: 2018-08-05T23:37:45.157881: step 5468, loss 0.613718.
Train: 2018-08-05T23:37:45.314095: step 5469, loss 0.621914.
Train: 2018-08-05T23:37:45.470309: step 5470, loss 0.587716.
Test: 2018-08-05T23:37:45.704628: step 5470, loss 0.548398.
Train: 2018-08-05T23:37:45.876466: step 5471, loss 0.520501.
Train: 2018-08-05T23:37:46.048321: step 5472, loss 0.629329.
Train: 2018-08-05T23:37:46.204510: step 5473, loss 0.512724.
Train: 2018-08-05T23:37:46.360723: step 5474, loss 0.5211.
Train: 2018-08-05T23:37:46.532587: step 5475, loss 0.546067.
Train: 2018-08-05T23:37:46.688771: step 5476, loss 0.603788.
Train: 2018-08-05T23:37:46.860635: step 5477, loss 0.57905.
Train: 2018-08-05T23:37:47.016849: step 5478, loss 0.529733.
Train: 2018-08-05T23:37:47.173031: step 5479, loss 0.47216.
Train: 2018-08-05T23:37:47.329244: step 5480, loss 0.546129.
Test: 2018-08-05T23:37:47.563566: step 5480, loss 0.548705.
Train: 2018-08-05T23:37:47.735399: step 5481, loss 0.496365.
Train: 2018-08-05T23:37:47.891643: step 5482, loss 0.570882.
Train: 2018-08-05T23:37:48.047859: step 5483, loss 0.603992.
Train: 2018-08-05T23:37:48.204039: step 5484, loss 0.579352.
Train: 2018-08-05T23:37:48.360283: step 5485, loss 0.587525.
Train: 2018-08-05T23:37:48.516495: step 5486, loss 0.613784.
Train: 2018-08-05T23:37:48.672712: step 5487, loss 0.52813.
Train: 2018-08-05T23:37:48.828927: step 5488, loss 0.621674.
Train: 2018-08-05T23:37:48.985135: step 5489, loss 0.571235.
Train: 2018-08-05T23:37:49.141318: step 5490, loss 0.604506.
Test: 2018-08-05T23:37:49.375671: step 5490, loss 0.548428.
Train: 2018-08-05T23:37:49.531882: step 5491, loss 0.486905.
Train: 2018-08-05T23:37:49.703687: step 5492, loss 0.579002.
Train: 2018-08-05T23:37:49.859899: step 5493, loss 0.562356.
Train: 2018-08-05T23:37:50.016143: step 5494, loss 0.562648.
Train: 2018-08-05T23:37:50.172359: step 5495, loss 0.520558.
Train: 2018-08-05T23:37:50.344190: step 5496, loss 0.587162.
Train: 2018-08-05T23:37:50.500374: step 5497, loss 0.528742.
Train: 2018-08-05T23:37:50.651501: step 5498, loss 0.562365.
Train: 2018-08-05T23:37:50.807716: step 5499, loss 0.536573.
Train: 2018-08-05T23:37:50.963928: step 5500, loss 0.561565.
Test: 2018-08-05T23:37:51.213900: step 5500, loss 0.547959.
Train: 2018-08-05T23:37:51.916858: step 5501, loss 0.537283.
Train: 2018-08-05T23:37:52.073072: step 5502, loss 0.518162.
Train: 2018-08-05T23:37:52.244877: step 5503, loss 0.589202.
Train: 2018-08-05T23:37:52.401089: step 5504, loss 0.631971.
Train: 2018-08-05T23:37:52.557303: step 5505, loss 0.569865.
Train: 2018-08-05T23:37:52.729137: step 5506, loss 0.529272.
Train: 2018-08-05T23:37:52.885351: step 5507, loss 0.511887.
Train: 2018-08-05T23:37:53.041564: step 5508, loss 0.510748.
Train: 2018-08-05T23:37:53.197803: step 5509, loss 0.57805.
Train: 2018-08-05T23:37:53.369611: step 5510, loss 0.526472.
Test: 2018-08-05T23:37:53.603933: step 5510, loss 0.547624.
Train: 2018-08-05T23:37:53.760146: step 5511, loss 0.542704.
Train: 2018-08-05T23:37:53.922829: step 5512, loss 0.491328.
Train: 2018-08-05T23:37:54.079043: step 5513, loss 0.520926.
Train: 2018-08-05T23:37:54.235290: step 5514, loss 0.533675.
Train: 2018-08-05T23:37:54.391499: step 5515, loss 0.56924.
Train: 2018-08-05T23:37:54.547683: step 5516, loss 0.600472.
Train: 2018-08-05T23:37:54.703895: step 5517, loss 0.638324.
Train: 2018-08-05T23:37:54.860138: step 5518, loss 0.594732.
Train: 2018-08-05T23:37:55.032046: step 5519, loss 0.541121.
Train: 2018-08-05T23:37:55.188260: step 5520, loss 0.551038.
Test: 2018-08-05T23:37:55.422608: step 5520, loss 0.547601.
Train: 2018-08-05T23:37:55.578823: step 5521, loss 0.546417.
Train: 2018-08-05T23:37:55.735007: step 5522, loss 0.666458.
Train: 2018-08-05T23:37:55.908237: step 5523, loss 0.481905.
Train: 2018-08-05T23:37:56.067495: step 5524, loss 0.579718.
Train: 2018-08-05T23:37:56.223737: step 5525, loss 0.599628.
Train: 2018-08-05T23:37:56.379945: step 5526, loss 0.588124.
Train: 2018-08-05T23:37:56.536135: step 5527, loss 0.563343.
Train: 2018-08-05T23:37:56.707999: step 5528, loss 0.622244.
Train: 2018-08-05T23:37:56.864181: step 5529, loss 0.494574.
Train: 2018-08-05T23:37:57.020394: step 5530, loss 0.596289.
Test: 2018-08-05T23:37:57.270368: step 5530, loss 0.548387.
Train: 2018-08-05T23:37:57.426579: step 5531, loss 0.571216.
Train: 2018-08-05T23:37:57.582762: step 5532, loss 0.545475.
Train: 2018-08-05T23:37:57.738975: step 5533, loss 0.546126.
Train: 2018-08-05T23:37:57.895219: step 5534, loss 0.562466.
Train: 2018-08-05T23:37:58.067023: step 5535, loss 0.537548.
Train: 2018-08-05T23:37:58.223268: step 5536, loss 0.521071.
Train: 2018-08-05T23:37:58.379480: step 5537, loss 0.504312.
Train: 2018-08-05T23:37:58.540324: step 5538, loss 0.537612.
Train: 2018-08-05T23:37:58.696537: step 5539, loss 0.537514.
Train: 2018-08-05T23:37:58.868375: step 5540, loss 0.562424.
Test: 2018-08-05T23:37:59.102664: step 5540, loss 0.548341.
Train: 2018-08-05T23:37:59.258905: step 5541, loss 0.663501.
Train: 2018-08-05T23:37:59.415119: step 5542, loss 0.596165.
Train: 2018-08-05T23:37:59.571338: step 5543, loss 0.604715.
Train: 2018-08-05T23:37:59.743137: step 5544, loss 0.604746.
Train: 2018-08-05T23:37:59.899350: step 5545, loss 0.537147.
Train: 2018-08-05T23:38:00.055562: step 5546, loss 0.537254.
Train: 2018-08-05T23:38:00.211775: step 5547, loss 0.512101.
Train: 2018-08-05T23:38:00.367989: step 5548, loss 0.570862.
Train: 2018-08-05T23:38:00.524203: step 5549, loss 0.528759.
Train: 2018-08-05T23:38:00.680416: step 5550, loss 0.579194.
Test: 2018-08-05T23:38:00.930359: step 5550, loss 0.548308.
Train: 2018-08-05T23:38:01.086600: step 5551, loss 0.520288.
Train: 2018-08-05T23:38:01.258429: step 5552, loss 0.570865.
Train: 2018-08-05T23:38:01.414649: step 5553, loss 0.520018.
Train: 2018-08-05T23:38:01.570831: step 5554, loss 0.520068.
Train: 2018-08-05T23:38:01.742665: step 5555, loss 0.528468.
Train: 2018-08-05T23:38:01.914528: step 5556, loss 0.536667.
Train: 2018-08-05T23:38:02.070743: step 5557, loss 0.510736.
Train: 2018-08-05T23:38:02.226927: step 5558, loss 0.493285.
Train: 2018-08-05T23:38:02.389871: step 5559, loss 0.510352.
Train: 2018-08-05T23:38:02.546055: step 5560, loss 0.52756.
Test: 2018-08-05T23:38:02.775750: step 5560, loss 0.547743.
Train: 2018-08-05T23:38:02.931963: step 5561, loss 0.623961.
Train: 2018-08-05T23:38:03.103798: step 5562, loss 0.597714.
Train: 2018-08-05T23:38:03.260012: step 5563, loss 0.509331.
Train: 2018-08-05T23:38:03.416249: step 5564, loss 0.615973.
Train: 2018-08-05T23:38:03.572439: step 5565, loss 0.580295.
Train: 2018-08-05T23:38:03.742460: step 5566, loss 0.616108.
Train: 2018-08-05T23:38:03.898674: step 5567, loss 0.536171.
Train: 2018-08-05T23:38:04.054887: step 5568, loss 0.580738.
Train: 2018-08-05T23:38:04.226751: step 5569, loss 0.571441.
Train: 2018-08-05T23:38:04.382935: step 5570, loss 0.544949.
Test: 2018-08-05T23:38:04.617289: step 5570, loss 0.547657.
Train: 2018-08-05T23:38:04.773468: step 5571, loss 0.642499.
Train: 2018-08-05T23:38:04.943495: step 5572, loss 0.509171.
Train: 2018-08-05T23:38:05.099708: step 5573, loss 0.589307.
Train: 2018-08-05T23:38:05.271543: step 5574, loss 0.615592.
Train: 2018-08-05T23:38:05.459001: step 5575, loss 0.544619.
Train: 2018-08-05T23:38:05.615242: step 5576, loss 0.606539.
Train: 2018-08-05T23:38:05.771426: step 5577, loss 0.527402.
Train: 2018-08-05T23:38:05.932094: step 5578, loss 0.56233.
Train: 2018-08-05T23:38:06.088308: step 5579, loss 0.579802.
Train: 2018-08-05T23:38:06.260173: step 5580, loss 0.579863.
Test: 2018-08-05T23:38:06.494494: step 5580, loss 0.547867.
Train: 2018-08-05T23:38:06.650675: step 5581, loss 0.518961.
Train: 2018-08-05T23:38:06.822542: step 5582, loss 0.553738.
Train: 2018-08-05T23:38:06.978757: step 5583, loss 0.571145.
Train: 2018-08-05T23:38:07.134967: step 5584, loss 0.588395.
Train: 2018-08-05T23:38:07.306771: step 5585, loss 0.571012.
Train: 2018-08-05T23:38:07.447396: step 5586, loss 0.527774.
Train: 2018-08-05T23:38:07.603607: step 5587, loss 0.63587.
Train: 2018-08-05T23:38:07.775411: step 5588, loss 0.553825.
Train: 2018-08-05T23:38:07.931655: step 5589, loss 0.570973.
Train: 2018-08-05T23:38:08.087870: step 5590, loss 0.485268.
Test: 2018-08-05T23:38:08.322191: step 5590, loss 0.54806.
Train: 2018-08-05T23:38:08.478402: step 5591, loss 0.596668.
Train: 2018-08-05T23:38:08.650235: step 5592, loss 0.579508.
Train: 2018-08-05T23:38:08.806448: step 5593, loss 0.545306.
Train: 2018-08-05T23:38:08.962663: step 5594, loss 0.519662.
Train: 2018-08-05T23:38:09.118845: step 5595, loss 0.485529.
Train: 2018-08-05T23:38:09.275058: step 5596, loss 0.588002.
Train: 2018-08-05T23:38:09.446925: step 5597, loss 0.630976.
Train: 2018-08-05T23:38:09.665621: step 5598, loss 0.545255.
Train: 2018-08-05T23:38:09.821834: step 5599, loss 0.571068.
Train: 2018-08-05T23:38:09.962398: step 5600, loss 0.562479.
Test: 2018-08-05T23:38:10.212370: step 5600, loss 0.548039.
Train: 2018-08-05T23:38:10.962161: step 5601, loss 0.553825.
Train: 2018-08-05T23:38:11.133996: step 5602, loss 0.519706.
Train: 2018-08-05T23:38:11.290245: step 5603, loss 0.62251.
Train: 2018-08-05T23:38:11.446454: step 5604, loss 0.519552.
Train: 2018-08-05T23:38:11.618288: step 5605, loss 0.596888.
Train: 2018-08-05T23:38:11.774471: step 5606, loss 0.570962.
Train: 2018-08-05T23:38:11.930684: step 5607, loss 0.588003.
Train: 2018-08-05T23:38:12.086896: step 5608, loss 0.528199.
Train: 2018-08-05T23:38:12.243141: step 5609, loss 0.536693.
Train: 2018-08-05T23:38:12.414946: step 5610, loss 0.57095.
Test: 2018-08-05T23:38:12.649267: step 5610, loss 0.548039.
Train: 2018-08-05T23:38:12.805478: step 5611, loss 0.528095.
Train: 2018-08-05T23:38:12.961723: step 5612, loss 0.579538.
Train: 2018-08-05T23:38:13.123551: step 5613, loss 0.502277.
Train: 2018-08-05T23:38:13.295360: step 5614, loss 0.562363.
Train: 2018-08-05T23:38:13.451600: step 5615, loss 0.502075.
Train: 2018-08-05T23:38:13.607824: step 5616, loss 0.510496.
Train: 2018-08-05T23:38:13.779621: step 5617, loss 0.458366.
Train: 2018-08-05T23:38:13.935835: step 5618, loss 0.518672.
Train: 2018-08-05T23:38:14.092047: step 5619, loss 0.597393.
Train: 2018-08-05T23:38:14.248260: step 5620, loss 0.606571.
Test: 2018-08-05T23:38:14.498205: step 5620, loss 0.547684.
Train: 2018-08-05T23:38:14.654415: step 5621, loss 0.598287.
Train: 2018-08-05T23:38:14.810628: step 5622, loss 0.562451.
Train: 2018-08-05T23:38:14.964986: step 5623, loss 0.544593.
Train: 2018-08-05T23:38:15.121169: step 5624, loss 0.55368.
Train: 2018-08-05T23:38:15.277412: step 5625, loss 0.536037.
Train: 2018-08-05T23:38:15.449246: step 5626, loss 0.553635.
Train: 2018-08-05T23:38:15.605460: step 5627, loss 0.54474.
Train: 2018-08-05T23:38:15.761642: step 5628, loss 0.535697.
Train: 2018-08-05T23:38:15.923401: step 5629, loss 0.526935.
Train: 2018-08-05T23:38:16.079616: step 5630, loss 0.616568.
Test: 2018-08-05T23:38:16.329556: step 5630, loss 0.547616.
Train: 2018-08-05T23:38:16.517020: step 5631, loss 0.634025.
Train: 2018-08-05T23:38:16.688876: step 5632, loss 0.491089.
Train: 2018-08-05T23:38:16.845060: step 5633, loss 0.562691.
Train: 2018-08-05T23:38:17.016923: step 5634, loss 0.508978.
Train: 2018-08-05T23:38:17.173107: step 5635, loss 0.544617.
Train: 2018-08-05T23:38:17.344967: step 5636, loss 0.526892.
Train: 2018-08-05T23:38:17.501185: step 5637, loss 0.472621.
Train: 2018-08-05T23:38:17.657369: step 5638, loss 0.697735.
Train: 2018-08-05T23:38:17.813612: step 5639, loss 0.580222.
Train: 2018-08-05T23:38:17.969826: step 5640, loss 0.643364.
Test: 2018-08-05T23:38:18.219737: step 5640, loss 0.547615.
Train: 2018-08-05T23:38:18.375950: step 5641, loss 0.553622.
Train: 2018-08-05T23:38:18.532162: step 5642, loss 0.571576.
Train: 2018-08-05T23:38:18.688406: step 5643, loss 0.526904.
Train: 2018-08-05T23:38:18.860210: step 5644, loss 0.509148.
Train: 2018-08-05T23:38:19.016423: step 5645, loss 0.571663.
Train: 2018-08-05T23:38:19.172636: step 5646, loss 0.624415.
Train: 2018-08-05T23:38:19.328850: step 5647, loss 0.580454.
Train: 2018-08-05T23:38:19.485063: step 5648, loss 0.588857.
Train: 2018-08-05T23:38:19.641276: step 5649, loss 0.562556.
Train: 2018-08-05T23:38:19.797490: step 5650, loss 0.535985.
Test: 2018-08-05T23:38:20.031811: step 5650, loss 0.547769.
Train: 2018-08-05T23:38:20.188023: step 5651, loss 0.571438.
Train: 2018-08-05T23:38:20.359888: step 5652, loss 0.562609.
Train: 2018-08-05T23:38:20.516071: step 5653, loss 0.527473.
Train: 2018-08-05T23:38:20.672314: step 5654, loss 0.510013.
Train: 2018-08-05T23:38:20.828528: step 5655, loss 0.623271.
Train: 2018-08-05T23:38:21.000362: step 5656, loss 0.597083.
Train: 2018-08-05T23:38:21.156576: step 5657, loss 0.545257.
Train: 2018-08-05T23:38:21.312759: step 5658, loss 0.545079.
Train: 2018-08-05T23:38:21.469003: step 5659, loss 0.545069.
Train: 2018-08-05T23:38:21.625223: step 5660, loss 0.562474.
Test: 2018-08-05T23:38:21.859536: step 5660, loss 0.547919.
Train: 2018-08-05T23:38:22.031371: step 5661, loss 0.571088.
Train: 2018-08-05T23:38:22.187582: step 5662, loss 0.579863.
Train: 2018-08-05T23:38:22.343798: step 5663, loss 0.510652.
Train: 2018-08-05T23:38:22.500018: step 5664, loss 0.484624.
Train: 2018-08-05T23:38:22.656193: step 5665, loss 0.52771.
Train: 2018-08-05T23:38:22.812429: step 5666, loss 0.553612.
Train: 2018-08-05T23:38:22.968619: step 5667, loss 0.57111.
Train: 2018-08-05T23:38:23.124862: step 5668, loss 0.509999.
Train: 2018-08-05T23:38:23.281629: step 5669, loss 0.544845.
Train: 2018-08-05T23:38:23.453464: step 5670, loss 0.492422.
Test: 2018-08-05T23:38:23.687784: step 5670, loss 0.547753.
Train: 2018-08-05T23:38:23.844028: step 5671, loss 0.57979.
Train: 2018-08-05T23:38:24.015831: step 5672, loss 0.553684.
Train: 2018-08-05T23:38:24.172044: step 5673, loss 0.561953.
Train: 2018-08-05T23:38:24.328258: step 5674, loss 0.606856.
Train: 2018-08-05T23:38:24.484506: step 5675, loss 0.55391.
Train: 2018-08-05T23:38:24.656350: step 5676, loss 0.544513.
Train: 2018-08-05T23:38:24.812553: step 5677, loss 0.615972.
Train: 2018-08-05T23:38:24.968317: step 5678, loss 0.518014.
Train: 2018-08-05T23:38:25.124530: step 5679, loss 0.713829.
Train: 2018-08-05T23:38:25.280776: step 5680, loss 0.562149.
Test: 2018-08-05T23:38:25.530711: step 5680, loss 0.547701.
Train: 2018-08-05T23:38:25.686897: step 5681, loss 0.650645.
Train: 2018-08-05T23:38:25.858763: step 5682, loss 0.536221.
Train: 2018-08-05T23:38:26.014469: step 5683, loss 0.563056.
Train: 2018-08-05T23:38:26.170644: step 5684, loss 0.518896.
Train: 2018-08-05T23:38:26.326858: step 5685, loss 0.640609.
Train: 2018-08-05T23:38:26.498721: step 5686, loss 0.545107.
Train: 2018-08-05T23:38:26.654937: step 5687, loss 0.606003.
Train: 2018-08-05T23:38:26.811119: step 5688, loss 0.562484.
Train: 2018-08-05T23:38:26.965226: step 5689, loss 0.562234.
Train: 2018-08-05T23:38:27.137091: step 5690, loss 0.57096.
Test: 2018-08-05T23:38:27.371380: step 5690, loss 0.548115.
Train: 2018-08-05T23:38:27.543214: step 5691, loss 0.579621.
Train: 2018-08-05T23:38:27.699453: step 5692, loss 0.545383.
Train: 2018-08-05T23:38:27.855672: step 5693, loss 0.520086.
Train: 2018-08-05T23:38:28.011884: step 5694, loss 0.587722.
Train: 2018-08-05T23:38:28.168069: step 5695, loss 0.613106.
Train: 2018-08-05T23:38:28.324312: step 5696, loss 0.520191.
Train: 2018-08-05T23:38:28.480525: step 5697, loss 0.562371.
Train: 2018-08-05T23:38:28.636738: step 5698, loss 0.545454.
Train: 2018-08-05T23:38:28.792951: step 5699, loss 0.520502.
Train: 2018-08-05T23:38:28.964788: step 5700, loss 0.612844.
Test: 2018-08-05T23:38:29.200727: step 5700, loss 0.548334.
Train: 2018-08-05T23:38:29.950550: step 5701, loss 0.528846.
Train: 2018-08-05T23:38:30.106764: step 5702, loss 0.528829.
Train: 2018-08-05T23:38:30.263009: step 5703, loss 0.621355.
Train: 2018-08-05T23:38:30.419215: step 5704, loss 0.604716.
Train: 2018-08-05T23:38:30.591055: step 5705, loss 0.528723.
Train: 2018-08-05T23:38:30.747263: step 5706, loss 0.571057.
Train: 2018-08-05T23:38:30.919073: step 5707, loss 0.553815.
Train: 2018-08-05T23:38:31.075285: step 5708, loss 0.604521.
Train: 2018-08-05T23:38:31.231524: step 5709, loss 0.587657.
Train: 2018-08-05T23:38:31.387742: step 5710, loss 0.53698.
Test: 2018-08-05T23:38:31.637653: step 5710, loss 0.548341.
Train: 2018-08-05T23:38:31.793897: step 5711, loss 0.57942.
Train: 2018-08-05T23:38:31.965727: step 5712, loss 0.562459.
Train: 2018-08-05T23:38:32.121952: step 5713, loss 0.604491.
Train: 2018-08-05T23:38:32.278159: step 5714, loss 0.680018.
Train: 2018-08-05T23:38:32.435891: step 5715, loss 0.57088.
Train: 2018-08-05T23:38:32.607724: step 5716, loss 0.495973.
Train: 2018-08-05T23:38:32.763938: step 5717, loss 0.629031.
Train: 2018-08-05T23:38:32.920151: step 5718, loss 0.520969.
Train: 2018-08-05T23:38:33.069712: step 5719, loss 0.479733.
Train: 2018-08-05T23:38:33.225925: step 5720, loss 0.686978.
Test: 2018-08-05T23:38:33.460246: step 5720, loss 0.548672.
Train: 2018-08-05T23:38:33.632082: step 5721, loss 0.554098.
Train: 2018-08-05T23:38:33.788265: step 5722, loss 0.562699.
Train: 2018-08-05T23:38:33.944506: step 5723, loss 0.612006.
Train: 2018-08-05T23:38:34.091558: step 5724, loss 0.562731.
Train: 2018-08-05T23:38:34.247796: step 5725, loss 0.546141.
Train: 2018-08-05T23:38:34.403985: step 5726, loss 0.546064.
Train: 2018-08-05T23:38:34.560228: step 5727, loss 0.56253.
Train: 2018-08-05T23:38:34.716410: step 5728, loss 0.611821.
Train: 2018-08-05T23:38:34.872654: step 5729, loss 0.587305.
Train: 2018-08-05T23:38:35.044458: step 5730, loss 0.570838.
Test: 2018-08-05T23:38:35.278779: step 5730, loss 0.548857.
Train: 2018-08-05T23:38:35.434992: step 5731, loss 0.570558.
Train: 2018-08-05T23:38:35.591236: step 5732, loss 0.513223.
Train: 2018-08-05T23:38:35.747418: step 5733, loss 0.554428.
Train: 2018-08-05T23:38:35.903633: step 5734, loss 0.54612.
Train: 2018-08-05T23:38:36.074186: step 5735, loss 0.570678.
Train: 2018-08-05T23:38:36.246044: step 5736, loss 0.521376.
Train: 2018-08-05T23:38:36.402258: step 5737, loss 0.587569.
Train: 2018-08-05T23:38:36.558476: step 5738, loss 0.509021.
Train: 2018-08-05T23:38:36.714690: step 5739, loss 0.528973.
Train: 2018-08-05T23:38:36.870898: step 5740, loss 0.537524.
Test: 2018-08-05T23:38:37.108910: step 5740, loss 0.5483.
Train: 2018-08-05T23:38:37.280742: step 5741, loss 0.571088.
Train: 2018-08-05T23:38:37.436957: step 5742, loss 0.571446.
Train: 2018-08-05T23:38:37.593168: step 5743, loss 0.536698.
Train: 2018-08-05T23:38:37.749353: step 5744, loss 0.545091.
Train: 2018-08-05T23:38:37.921210: step 5745, loss 0.596168.
Train: 2018-08-05T23:38:38.077398: step 5746, loss 0.577956.
Train: 2018-08-05T23:38:38.233612: step 5747, loss 0.511406.
Train: 2018-08-05T23:38:38.389825: step 5748, loss 0.605681.
Train: 2018-08-05T23:38:38.561660: step 5749, loss 0.544262.
Train: 2018-08-05T23:38:38.717873: step 5750, loss 0.588034.
Test: 2018-08-05T23:38:38.952193: step 5750, loss 0.547787.
Train: 2018-08-05T23:38:39.108431: step 5751, loss 0.475785.
Train: 2018-08-05T23:38:39.264650: step 5752, loss 0.535941.
Train: 2018-08-05T23:38:39.436454: step 5753, loss 0.534818.
Train: 2018-08-05T23:38:39.592667: step 5754, loss 0.511944.
Train: 2018-08-05T23:38:39.748911: step 5755, loss 0.565162.
Train: 2018-08-05T23:38:39.905124: step 5756, loss 0.553609.
Train: 2018-08-05T23:38:40.061307: step 5757, loss 0.587991.
Train: 2018-08-05T23:38:40.217553: step 5758, loss 0.60749.
Train: 2018-08-05T23:38:40.373764: step 5759, loss 0.571337.
Train: 2018-08-05T23:38:40.529978: step 5760, loss 0.623355.
Test: 2018-08-05T23:38:40.764268: step 5760, loss 0.547626.
Train: 2018-08-05T23:38:40.936131: step 5761, loss 0.508301.
Train: 2018-08-05T23:38:41.092316: step 5762, loss 0.614828.
Train: 2018-08-05T23:38:41.248528: step 5763, loss 0.554225.
Train: 2018-08-05T23:38:41.420362: step 5764, loss 0.59632.
Train: 2018-08-05T23:38:41.576601: step 5765, loss 0.653836.
Train: 2018-08-05T23:38:41.732788: step 5766, loss 0.553056.
Train: 2018-08-05T23:38:41.889033: step 5767, loss 0.466108.
Train: 2018-08-05T23:38:42.045216: step 5768, loss 0.543857.
Train: 2018-08-05T23:38:42.201429: step 5769, loss 0.546958.
Train: 2018-08-05T23:38:42.357666: step 5770, loss 0.544276.
Test: 2018-08-05T23:38:42.591990: step 5770, loss 0.547851.
Train: 2018-08-05T23:38:42.748205: step 5771, loss 0.56261.
Train: 2018-08-05T23:38:42.920040: step 5772, loss 0.638673.
Train: 2018-08-05T23:38:43.076222: step 5773, loss 0.536414.
Train: 2018-08-05T23:38:43.232454: step 5774, loss 0.572701.
Train: 2018-08-05T23:38:43.388667: step 5775, loss 0.546016.
Train: 2018-08-05T23:38:43.544880: step 5776, loss 0.586999.
Train: 2018-08-05T23:38:43.716716: step 5777, loss 0.502278.
Train: 2018-08-05T23:38:43.872928: step 5778, loss 0.596212.
Train: 2018-08-05T23:38:44.044764: step 5779, loss 0.571035.
Train: 2018-08-05T23:38:44.201007: step 5780, loss 0.587988.
Test: 2018-08-05T23:38:44.435329: step 5780, loss 0.54804.
Train: 2018-08-05T23:38:44.607135: step 5781, loss 0.562576.
Train: 2018-08-05T23:38:44.763375: step 5782, loss 0.622969.
Train: 2018-08-05T23:38:44.919584: step 5783, loss 0.597479.
Train: 2018-08-05T23:38:45.091428: step 5784, loss 0.520081.
Train: 2018-08-05T23:38:45.247630: step 5785, loss 0.520013.
Train: 2018-08-05T23:38:45.419440: step 5786, loss 0.562957.
Train: 2018-08-05T23:38:45.575652: step 5787, loss 0.588576.
Train: 2018-08-05T23:38:45.731896: step 5788, loss 0.528572.
Train: 2018-08-05T23:38:45.888109: step 5789, loss 0.570895.
Train: 2018-08-05T23:38:46.045059: step 5790, loss 0.587558.
Test: 2018-08-05T23:38:46.279380: step 5790, loss 0.548369.
Train: 2018-08-05T23:38:46.451182: step 5791, loss 0.570641.
Train: 2018-08-05T23:38:46.607425: step 5792, loss 0.562643.
Train: 2018-08-05T23:38:46.763639: step 5793, loss 0.545945.
Train: 2018-08-05T23:38:46.924372: step 5794, loss 0.537627.
Train: 2018-08-05T23:38:47.080585: step 5795, loss 0.587172.
Train: 2018-08-05T23:38:47.252420: step 5796, loss 0.520191.
Train: 2018-08-05T23:38:47.408633: step 5797, loss 0.503878.
Train: 2018-08-05T23:38:47.564846: step 5798, loss 0.519991.
Train: 2018-08-05T23:38:47.721085: step 5799, loss 0.587537.
Train: 2018-08-05T23:38:47.892895: step 5800, loss 0.486175.
Test: 2018-08-05T23:38:48.127246: step 5800, loss 0.548131.
Train: 2018-08-05T23:38:48.845825: step 5801, loss 0.52004.
Train: 2018-08-05T23:38:49.017629: step 5802, loss 0.51951.
Train: 2018-08-05T23:38:49.173873: step 5803, loss 0.605447.
Train: 2018-08-05T23:38:49.330057: step 5804, loss 0.579874.
Train: 2018-08-05T23:38:49.501922: step 5805, loss 0.553797.
Train: 2018-08-05T23:38:49.658104: step 5806, loss 0.519377.
Train: 2018-08-05T23:38:49.814342: step 5807, loss 0.61455.
Train: 2018-08-05T23:38:49.970530: step 5808, loss 0.501305.
Train: 2018-08-05T23:38:50.126744: step 5809, loss 0.615079.
Train: 2018-08-05T23:38:50.282987: step 5810, loss 0.527203.
Test: 2018-08-05T23:38:50.517309: step 5810, loss 0.547731.
Train: 2018-08-05T23:38:50.689111: step 5811, loss 0.553694.
Train: 2018-08-05T23:38:50.845325: step 5812, loss 0.570391.
Train: 2018-08-05T23:38:51.001539: step 5813, loss 0.526931.
Train: 2018-08-05T23:38:51.157782: step 5814, loss 0.482931.
Train: 2018-08-05T23:38:51.313995: step 5815, loss 0.562374.
Train: 2018-08-05T23:38:51.485800: step 5816, loss 0.589415.
Train: 2018-08-05T23:38:51.642012: step 5817, loss 0.553019.
Train: 2018-08-05T23:38:51.798226: step 5818, loss 0.500081.
Train: 2018-08-05T23:38:51.954440: step 5819, loss 0.589114.
Train: 2018-08-05T23:38:52.110652: step 5820, loss 0.600301.
Test: 2018-08-05T23:38:52.360625: step 5820, loss 0.547601.
Train: 2018-08-05T23:38:52.516808: step 5821, loss 0.580932.
Train: 2018-08-05T23:38:52.688666: step 5822, loss 0.552401.
Train: 2018-08-05T23:38:52.844854: step 5823, loss 0.581439.
Train: 2018-08-05T23:38:53.001067: step 5824, loss 0.572679.
Train: 2018-08-05T23:38:53.157281: step 5825, loss 0.562838.
Train: 2018-08-05T23:38:53.329146: step 5826, loss 0.536074.
Train: 2018-08-05T23:38:53.485359: step 5827, loss 0.553526.
Train: 2018-08-05T23:38:53.641543: step 5828, loss 0.571683.
Train: 2018-08-05T23:38:53.797788: step 5829, loss 0.518032.
Train: 2018-08-05T23:38:53.953998: step 5830, loss 0.625062.
Test: 2018-08-05T23:38:54.203910: step 5830, loss 0.547685.
Train: 2018-08-05T23:38:54.360124: step 5831, loss 0.535746.
Train: 2018-08-05T23:38:54.531958: step 5832, loss 0.615856.
Train: 2018-08-05T23:38:54.688202: step 5833, loss 0.518067.
Train: 2018-08-05T23:38:54.844385: step 5834, loss 0.509436.
Train: 2018-08-05T23:38:55.000621: step 5835, loss 0.615572.
Train: 2018-08-05T23:38:55.156842: step 5836, loss 0.588216.
Train: 2018-08-05T23:38:55.313054: step 5837, loss 0.685306.
Train: 2018-08-05T23:38:55.469267: step 5838, loss 0.466396.
Train: 2018-08-05T23:38:55.625476: step 5839, loss 0.536757.
Train: 2018-08-05T23:38:55.781663: step 5840, loss 0.56271.
Test: 2018-08-05T23:38:56.017738: step 5840, loss 0.547896.
Train: 2018-08-05T23:38:56.173950: step 5841, loss 0.527864.
Train: 2018-08-05T23:38:56.330193: step 5842, loss 0.54518.
Train: 2018-08-05T23:38:56.486401: step 5843, loss 0.519284.
Train: 2018-08-05T23:38:56.642620: step 5844, loss 0.553434.
Train: 2018-08-05T23:38:56.798835: step 5845, loss 0.536326.
Train: 2018-08-05T23:38:56.969002: step 5846, loss 0.536285.
Train: 2018-08-05T23:38:57.125246: step 5847, loss 0.623737.
Train: 2018-08-05T23:38:57.281430: step 5848, loss 0.623508.
Train: 2018-08-05T23:38:57.437674: step 5849, loss 0.501316.
Train: 2018-08-05T23:38:57.609508: step 5850, loss 0.554086.
Test: 2018-08-05T23:38:57.843801: step 5850, loss 0.547873.
Train: 2018-08-05T23:38:58.003639: step 5851, loss 0.545276.
Train: 2018-08-05T23:38:58.159855: step 5852, loss 0.527446.
Train: 2018-08-05T23:38:58.316066: step 5853, loss 0.588514.
Train: 2018-08-05T23:38:58.472249: step 5854, loss 0.606367.
Train: 2018-08-05T23:38:58.628496: step 5855, loss 0.58868.
Train: 2018-08-05T23:38:58.784676: step 5856, loss 0.61425.
Train: 2018-08-05T23:38:58.940890: step 5857, loss 0.579925.
Train: 2018-08-05T23:38:59.097133: step 5858, loss 0.562682.
Train: 2018-08-05T23:38:59.253316: step 5859, loss 0.562512.
Train: 2018-08-05T23:38:59.427381: step 5860, loss 0.571323.
Test: 2018-08-05T23:38:59.656808: step 5860, loss 0.54805.
Train: 2018-08-05T23:38:59.813025: step 5861, loss 0.510866.
Train: 2018-08-05T23:38:59.984831: step 5862, loss 0.622061.
Train: 2018-08-05T23:39:00.141040: step 5863, loss 0.639349.
Train: 2018-08-05T23:39:00.312874: step 5864, loss 0.562266.
Train: 2018-08-05T23:39:00.469087: step 5865, loss 0.587876.
Train: 2018-08-05T23:39:00.625330: step 5866, loss 0.579425.
Train: 2018-08-05T23:39:00.797135: step 5867, loss 0.62135.
Train: 2018-08-05T23:39:00.953349: step 5868, loss 0.562402.
Train: 2018-08-05T23:39:01.109561: step 5869, loss 0.529095.
Train: 2018-08-05T23:39:01.265775: step 5870, loss 0.545711.
Test: 2018-08-05T23:39:01.515718: step 5870, loss 0.548567.
Train: 2018-08-05T23:39:01.671930: step 5871, loss 0.520817.
Train: 2018-08-05T23:39:01.828142: step 5872, loss 0.579151.
Train: 2018-08-05T23:39:01.984356: step 5873, loss 0.579193.
Train: 2018-08-05T23:39:02.140569: step 5874, loss 0.554222.
Train: 2018-08-05T23:39:02.296782: step 5875, loss 0.562584.
Train: 2018-08-05T23:39:02.452995: step 5876, loss 0.545849.
Train: 2018-08-05T23:39:02.607208: step 5877, loss 0.587304.
Train: 2018-08-05T23:39:02.779073: step 5878, loss 0.595649.
Train: 2018-08-05T23:39:02.935257: step 5879, loss 0.620613.
Train: 2018-08-05T23:39:03.091469: step 5880, loss 0.620698.
Test: 2018-08-05T23:39:03.331650: step 5880, loss 0.54871.
Train: 2018-08-05T23:39:03.487863: step 5881, loss 0.570711.
Train: 2018-08-05T23:39:03.644113: step 5882, loss 0.513038.
Train: 2018-08-05T23:39:03.815912: step 5883, loss 0.595553.
Train: 2018-08-05T23:39:03.972124: step 5884, loss 0.505132.
Train: 2018-08-05T23:39:04.128338: step 5885, loss 0.546181.
Train: 2018-08-05T23:39:04.284551: step 5886, loss 0.562789.
Train: 2018-08-05T23:39:04.454540: step 5887, loss 0.521388.
Train: 2018-08-05T23:39:04.610754: step 5888, loss 0.579018.
Train: 2018-08-05T23:39:04.766968: step 5889, loss 0.580322.
Train: 2018-08-05T23:39:04.923180: step 5890, loss 0.504353.
Test: 2018-08-05T23:39:05.173153: step 5890, loss 0.548596.
Train: 2018-08-05T23:39:05.329365: step 5891, loss 0.57899.
Train: 2018-08-05T23:39:05.501195: step 5892, loss 0.512615.
Train: 2018-08-05T23:39:05.657416: step 5893, loss 0.503922.
Train: 2018-08-05T23:39:05.813626: step 5894, loss 0.621237.
Train: 2018-08-05T23:39:05.969845: step 5895, loss 0.621346.
Train: 2018-08-05T23:39:06.126022: step 5896, loss 0.612942.
Train: 2018-08-05T23:39:06.282269: step 5897, loss 0.579177.
Train: 2018-08-05T23:39:06.454101: step 5898, loss 0.562356.
Train: 2018-08-05T23:39:06.610314: step 5899, loss 0.58784.
Train: 2018-08-05T23:39:06.782118: step 5900, loss 0.570811.
Test: 2018-08-05T23:39:07.020071: step 5900, loss 0.548351.
Train: 2018-08-05T23:39:07.769923: step 5901, loss 0.596208.
Train: 2018-08-05T23:39:07.928948: step 5902, loss 0.528723.
Train: 2018-08-05T23:39:08.100752: step 5903, loss 0.596149.
Train: 2018-08-05T23:39:08.256966: step 5904, loss 0.461874.
Train: 2018-08-05T23:39:08.413212: step 5905, loss 0.545504.
Train: 2018-08-05T23:39:08.585051: step 5906, loss 0.612902.
Train: 2018-08-05T23:39:08.741258: step 5907, loss 0.579275.
Train: 2018-08-05T23:39:08.913061: step 5908, loss 0.537241.
Train: 2018-08-05T23:39:09.069274: step 5909, loss 0.5286.
Train: 2018-08-05T23:39:09.225488: step 5910, loss 0.604739.
Test: 2018-08-05T23:39:09.459808: step 5910, loss 0.548255.
Train: 2018-08-05T23:39:09.631643: step 5911, loss 0.520118.
Train: 2018-08-05T23:39:09.787889: step 5912, loss 0.562395.
Train: 2018-08-05T23:39:09.944069: step 5913, loss 0.528439.
Train: 2018-08-05T23:39:10.115903: step 5914, loss 0.562241.
Train: 2018-08-05T23:39:10.272117: step 5915, loss 0.536916.
Train: 2018-08-05T23:39:10.428330: step 5916, loss 0.519797.
Train: 2018-08-05T23:39:10.584543: step 5917, loss 0.562283.
Train: 2018-08-05T23:39:10.740756: step 5918, loss 0.545015.
Train: 2018-08-05T23:39:10.896969: step 5919, loss 0.527893.
Train: 2018-08-05T23:39:11.068804: step 5920, loss 0.579637.
Test: 2018-08-05T23:39:11.303125: step 5920, loss 0.547866.
Train: 2018-08-05T23:39:11.459368: step 5921, loss 0.562344.
Train: 2018-08-05T23:39:11.615551: step 5922, loss 0.51004.
Train: 2018-08-05T23:39:11.771765: step 5923, loss 0.553577.
Train: 2018-08-05T23:39:11.927977: step 5924, loss 0.571805.
Train: 2018-08-05T23:39:12.084224: step 5925, loss 0.54481.
Train: 2018-08-05T23:39:12.240403: step 5926, loss 0.544284.
Train: 2018-08-05T23:39:12.396647: step 5927, loss 0.562503.
Train: 2018-08-05T23:39:12.552830: step 5928, loss 0.536266.
Train: 2018-08-05T23:39:12.724664: step 5929, loss 0.562413.
Train: 2018-08-05T23:39:12.880877: step 5930, loss 0.517749.
Test: 2018-08-05T23:39:13.115232: step 5930, loss 0.547621.
Train: 2018-08-05T23:39:13.271411: step 5931, loss 0.554028.
Train: 2018-08-05T23:39:13.435559: step 5932, loss 0.56208.
Train: 2018-08-05T23:39:13.607362: step 5933, loss 0.57146.
Train: 2018-08-05T23:39:13.763609: step 5934, loss 0.571959.
Train: 2018-08-05T23:39:13.919819: step 5935, loss 0.581002.
Train: 2018-08-05T23:39:14.076001: step 5936, loss 0.527607.
Train: 2018-08-05T23:39:14.232215: step 5937, loss 0.617032.
Train: 2018-08-05T23:39:14.388429: step 5938, loss 0.527301.
Train: 2018-08-05T23:39:14.544672: step 5939, loss 0.661076.
Train: 2018-08-05T23:39:14.716476: step 5940, loss 0.562654.
Test: 2018-08-05T23:39:14.950797: step 5940, loss 0.547689.
Train: 2018-08-05T23:39:15.107040: step 5941, loss 0.52726.
Train: 2018-08-05T23:39:15.263222: step 5942, loss 0.527172.
Train: 2018-08-05T23:39:15.435060: step 5943, loss 0.580206.
Train: 2018-08-05T23:39:15.591271: step 5944, loss 0.536085.
Train: 2018-08-05T23:39:15.747514: step 5945, loss 0.518397.
Train: 2018-08-05T23:39:15.903697: step 5946, loss 0.5097.
Train: 2018-08-05T23:39:16.059911: step 5947, loss 0.580328.
Train: 2018-08-05T23:39:16.216154: step 5948, loss 0.562845.
Train: 2018-08-05T23:39:16.372370: step 5949, loss 0.615595.
Train: 2018-08-05T23:39:16.528580: step 5950, loss 0.580203.
Test: 2018-08-05T23:39:16.778491: step 5950, loss 0.5477.
Train: 2018-08-05T23:39:16.933171: step 5951, loss 0.5095.
Train: 2018-08-05T23:39:17.089384: step 5952, loss 0.535991.
Train: 2018-08-05T23:39:17.245598: step 5953, loss 0.509119.
Train: 2018-08-05T23:39:17.401810: step 5954, loss 0.554179.
Train: 2018-08-05T23:39:17.558024: step 5955, loss 0.562565.
Train: 2018-08-05T23:39:17.729858: step 5956, loss 0.624678.
Train: 2018-08-05T23:39:17.886072: step 5957, loss 0.598262.
Train: 2018-08-05T23:39:18.046277: step 5958, loss 0.598743.
Train: 2018-08-05T23:39:18.202490: step 5959, loss 0.500234.
Train: 2018-08-05T23:39:18.389978: step 5960, loss 0.615214.
Test: 2018-08-05T23:39:18.624270: step 5960, loss 0.547741.
Train: 2018-08-05T23:39:18.796101: step 5961, loss 0.598277.
Train: 2018-08-05T23:39:18.952346: step 5962, loss 0.606144.
Train: 2018-08-05T23:39:19.108558: step 5963, loss 0.527568.
Train: 2018-08-05T23:39:19.264740: step 5964, loss 0.545292.
Train: 2018-08-05T23:39:19.420955: step 5965, loss 0.475594.
Train: 2018-08-05T23:39:19.577167: step 5966, loss 0.614294.
Train: 2018-08-05T23:39:19.749002: step 5967, loss 0.527731.
Train: 2018-08-05T23:39:19.905244: step 5968, loss 0.631546.
Train: 2018-08-05T23:39:20.077079: step 5969, loss 0.553698.
Train: 2018-08-05T23:39:20.233262: step 5970, loss 0.502064.
Test: 2018-08-05T23:39:20.467583: step 5970, loss 0.547955.
Train: 2018-08-05T23:39:20.639417: step 5971, loss 0.562359.
Train: 2018-08-05T23:39:20.795660: step 5972, loss 0.527833.
Train: 2018-08-05T23:39:20.951873: step 5973, loss 0.553778.
Train: 2018-08-05T23:39:21.123678: step 5974, loss 0.536572.
Train: 2018-08-05T23:39:21.279921: step 5975, loss 0.579806.
Train: 2018-08-05T23:39:21.436104: step 5976, loss 0.614372.
Train: 2018-08-05T23:39:21.592317: step 5977, loss 0.510384.
Train: 2018-08-05T23:39:21.748531: step 5978, loss 0.614552.
Train: 2018-08-05T23:39:21.920365: step 5979, loss 0.597116.
Train: 2018-08-05T23:39:22.076609: step 5980, loss 0.519237.
Test: 2018-08-05T23:39:22.310933: step 5980, loss 0.547956.
Train: 2018-08-05T23:39:22.467112: step 5981, loss 0.571043.
Train: 2018-08-05T23:39:22.638946: step 5982, loss 0.622666.
Train: 2018-08-05T23:39:22.795190: step 5983, loss 0.545157.
Train: 2018-08-05T23:39:22.951374: step 5984, loss 0.605358.
Train: 2018-08-05T23:39:23.123232: step 5985, loss 0.562346.
Train: 2018-08-05T23:39:23.279451: step 5986, loss 0.570862.
Train: 2018-08-05T23:39:23.435633: step 5987, loss 0.588134.
Train: 2018-08-05T23:39:23.591848: step 5988, loss 0.630495.
Train: 2018-08-05T23:39:23.748061: step 5989, loss 0.579371.
Train: 2018-08-05T23:39:23.904304: step 5990, loss 0.621634.
Test: 2018-08-05T23:39:24.138625: step 5990, loss 0.548334.
Train: 2018-08-05T23:39:24.310429: step 5991, loss 0.562494.
Train: 2018-08-05T23:39:24.466641: step 5992, loss 0.562438.
Train: 2018-08-05T23:39:24.622855: step 5993, loss 0.545814.
Train: 2018-08-05T23:39:24.779068: step 5994, loss 0.579135.
Train: 2018-08-05T23:39:24.935281: step 5995, loss 0.529282.
Train: 2018-08-05T23:39:25.091494: step 5996, loss 0.529255.
Train: 2018-08-05T23:39:25.247707: step 5997, loss 0.595686.
Train: 2018-08-05T23:39:25.403921: step 5998, loss 0.595634.
Train: 2018-08-05T23:39:25.560134: step 5999, loss 0.579036.
Train: 2018-08-05T23:39:25.731993: step 6000, loss 0.587361.
Test: 2018-08-05T23:39:25.966292: step 6000, loss 0.548732.
Train: 2018-08-05T23:39:26.684869: step 6001, loss 0.636796.
Train: 2018-08-05T23:39:26.856734: step 6002, loss 0.562575.
Train: 2018-08-05T23:39:27.012296: step 6003, loss 0.587294.
Train: 2018-08-05T23:39:27.184131: step 6004, loss 0.448061.
Train: 2018-08-05T23:39:27.340343: step 6005, loss 0.546162.
Train: 2018-08-05T23:39:27.496556: step 6006, loss 0.603648.
Train: 2018-08-05T23:39:27.652770: step 6007, loss 0.455962.
Train: 2018-08-05T23:39:27.824604: step 6008, loss 0.562645.
Train: 2018-08-05T23:39:27.970673: step 6009, loss 0.529474.
Train: 2018-08-05T23:39:28.142508: step 6010, loss 0.504377.
Test: 2018-08-05T23:39:28.376858: step 6010, loss 0.548536.
Train: 2018-08-05T23:39:28.548687: step 6011, loss 0.554121.
Train: 2018-08-05T23:39:28.704444: step 6012, loss 0.571018.
Train: 2018-08-05T23:39:28.860623: step 6013, loss 0.62991.
Train: 2018-08-05T23:39:29.016867: step 6014, loss 0.520304.
Train: 2018-08-05T23:39:29.188671: step 6015, loss 0.570865.
Train: 2018-08-05T23:39:29.344922: step 6016, loss 0.579451.
Train: 2018-08-05T23:39:29.501133: step 6017, loss 0.57949.
Train: 2018-08-05T23:39:29.657310: step 6018, loss 0.570786.
Train: 2018-08-05T23:39:29.829170: step 6019, loss 0.536808.
Train: 2018-08-05T23:39:29.985357: step 6020, loss 0.614034.
Test: 2018-08-05T23:39:30.220637: step 6020, loss 0.548068.
Train: 2018-08-05T23:39:30.376848: step 6021, loss 0.536674.
Train: 2018-08-05T23:39:30.533094: step 6022, loss 0.553686.
Train: 2018-08-05T23:39:30.689304: step 6023, loss 0.510898.
Train: 2018-08-05T23:39:30.845488: step 6024, loss 0.571003.
Train: 2018-08-05T23:39:31.017353: step 6025, loss 0.631671.
Train: 2018-08-05T23:39:31.173565: step 6026, loss 0.588431.
Train: 2018-08-05T23:39:31.329748: step 6027, loss 0.519511.
Train: 2018-08-05T23:39:31.485987: step 6028, loss 0.596573.
Train: 2018-08-05T23:39:31.642175: step 6029, loss 0.562235.
Train: 2018-08-05T23:39:31.798419: step 6030, loss 0.562377.
Test: 2018-08-05T23:39:32.048330: step 6030, loss 0.548067.
Train: 2018-08-05T23:39:32.204573: step 6031, loss 0.605248.
Train: 2018-08-05T23:39:32.360787: step 6032, loss 0.528124.
Train: 2018-08-05T23:39:32.517002: step 6033, loss 0.579567.
Train: 2018-08-05T23:39:32.673213: step 6034, loss 0.545254.
Train: 2018-08-05T23:39:32.825408: step 6035, loss 0.553814.
Train: 2018-08-05T23:39:32.981620: step 6036, loss 0.553747.
Train: 2018-08-05T23:39:33.137833: step 6037, loss 0.553887.
Train: 2018-08-05T23:39:33.294047: step 6038, loss 0.562464.
Train: 2018-08-05T23:39:33.450260: step 6039, loss 0.545009.
Train: 2018-08-05T23:39:33.606444: step 6040, loss 0.54384.
Test: 2018-08-05T23:39:33.841642: step 6040, loss 0.548026.
Train: 2018-08-05T23:39:34.013446: step 6041, loss 0.605309.
Train: 2018-08-05T23:39:34.169689: step 6042, loss 0.528122.
Train: 2018-08-05T23:39:34.341523: step 6043, loss 0.502337.
Train: 2018-08-05T23:39:34.497736: step 6044, loss 0.554044.
Train: 2018-08-05T23:39:34.653919: step 6045, loss 0.55398.
Train: 2018-08-05T23:39:34.809029: step 6046, loss 0.527875.
Train: 2018-08-05T23:39:34.965273: step 6047, loss 0.579349.
Train: 2018-08-05T23:39:35.121492: step 6048, loss 0.527681.
Train: 2018-08-05T23:39:35.277702: step 6049, loss 0.571029.
Train: 2018-08-05T23:39:35.433913: step 6050, loss 0.509987.
Test: 2018-08-05T23:39:35.683858: step 6050, loss 0.547767.
Train: 2018-08-05T23:39:35.840038: step 6051, loss 0.562565.
Train: 2018-08-05T23:39:35.996281: step 6052, loss 0.597951.
Train: 2018-08-05T23:39:36.152497: step 6053, loss 0.597269.
Train: 2018-08-05T23:39:36.324323: step 6054, loss 0.50982.
Train: 2018-08-05T23:39:36.480542: step 6055, loss 0.589235.
Train: 2018-08-05T23:39:36.636755: step 6056, loss 0.580185.
Train: 2018-08-05T23:39:36.792968: step 6057, loss 0.483039.
Train: 2018-08-05T23:39:36.950888: step 6058, loss 0.526713.
Train: 2018-08-05T23:39:37.107108: step 6059, loss 0.526411.
Train: 2018-08-05T23:39:37.263323: step 6060, loss 0.54403.
Test: 2018-08-05T23:39:37.513231: step 6060, loss 0.547647.
Train: 2018-08-05T23:39:37.669469: step 6061, loss 0.545093.
Train: 2018-08-05T23:39:37.825658: step 6062, loss 0.589799.
Train: 2018-08-05T23:39:37.985767: step 6063, loss 0.562682.
Train: 2018-08-05T23:39:38.142012: step 6064, loss 0.54472.
Train: 2018-08-05T23:39:38.298223: step 6065, loss 0.598188.
Train: 2018-08-05T23:39:38.454406: step 6066, loss 0.615292.
Train: 2018-08-05T23:39:38.610620: step 6067, loss 0.545109.
Train: 2018-08-05T23:39:38.766864: step 6068, loss 0.553113.
Train: 2018-08-05T23:39:38.923076: step 6069, loss 0.572149.
Train: 2018-08-05T23:39:39.079290: step 6070, loss 0.571169.
Test: 2018-08-05T23:39:39.329232: step 6070, loss 0.547635.
Train: 2018-08-05T23:39:39.485447: step 6071, loss 0.616704.
Train: 2018-08-05T23:39:39.641658: step 6072, loss 0.597875.
Train: 2018-08-05T23:39:39.797842: step 6073, loss 0.606837.
Train: 2018-08-05T23:39:39.954087: step 6074, loss 0.579622.
Train: 2018-08-05T23:39:40.110292: step 6075, loss 0.588818.
Train: 2018-08-05T23:39:40.282101: step 6076, loss 0.527821.
Train: 2018-08-05T23:39:40.438345: step 6077, loss 0.554162.
Train: 2018-08-05T23:39:40.594559: step 6078, loss 0.510783.
Train: 2018-08-05T23:39:40.750773: step 6079, loss 0.588198.
Train: 2018-08-05T23:39:40.922576: step 6080, loss 0.519235.
Test: 2018-08-05T23:39:41.156898: step 6080, loss 0.548.
Train: 2018-08-05T23:39:41.313134: step 6081, loss 0.527475.
Train: 2018-08-05T23:39:41.469323: step 6082, loss 0.527692.
Train: 2018-08-05T23:39:41.625567: step 6083, loss 0.579982.
Train: 2018-08-05T23:39:41.781779: step 6084, loss 0.519493.
Train: 2018-08-05T23:39:41.953611: step 6085, loss 0.588605.
Train: 2018-08-05T23:39:42.109796: step 6086, loss 0.510977.
Train: 2018-08-05T23:39:42.266009: step 6087, loss 0.545559.
Train: 2018-08-05T23:39:42.422223: step 6088, loss 0.527851.
Train: 2018-08-05T23:39:42.578467: step 6089, loss 0.562595.
Train: 2018-08-05T23:39:42.734650: step 6090, loss 0.597312.
Test: 2018-08-05T23:39:42.968972: step 6090, loss 0.547856.
Train: 2018-08-05T23:39:43.125210: step 6091, loss 0.570935.
Train: 2018-08-05T23:39:43.297018: step 6092, loss 0.588398.
Train: 2018-08-05T23:39:43.453230: step 6093, loss 0.535713.
Train: 2018-08-05T23:39:43.607876: step 6094, loss 0.597977.
Train: 2018-08-05T23:39:43.764090: step 6095, loss 0.588083.
Train: 2018-08-05T23:39:43.920303: step 6096, loss 0.570421.
Train: 2018-08-05T23:39:44.076516: step 6097, loss 0.536014.
Train: 2018-08-05T23:39:44.232760: step 6098, loss 0.553747.
Train: 2018-08-05T23:39:44.388973: step 6099, loss 0.518756.
Train: 2018-08-05T23:39:44.560778: step 6100, loss 0.475409.
Test: 2018-08-05T23:39:44.795101: step 6100, loss 0.547825.
Train: 2018-08-05T23:39:45.498058: step 6101, loss 0.588487.
Train: 2018-08-05T23:39:45.654301: step 6102, loss 0.605809.
Train: 2018-08-05T23:39:45.810483: step 6103, loss 0.623953.
Train: 2018-08-05T23:39:45.966697: step 6104, loss 0.493168.
Train: 2018-08-05T23:39:46.122940: step 6105, loss 0.625293.
Train: 2018-08-05T23:39:46.294774: step 6106, loss 0.581036.
Train: 2018-08-05T23:39:46.450990: step 6107, loss 0.605236.
Train: 2018-08-05T23:39:46.607171: step 6108, loss 0.519394.
Train: 2018-08-05T23:39:46.763384: step 6109, loss 0.484237.
Train: 2018-08-05T23:39:46.937877: step 6110, loss 0.622887.
Test: 2018-08-05T23:39:47.172236: step 6110, loss 0.547931.
Train: 2018-08-05T23:39:47.344032: step 6111, loss 0.519263.
Train: 2018-08-05T23:39:47.500276: step 6112, loss 0.510859.
Train: 2018-08-05T23:39:47.656492: step 6113, loss 0.570861.
Train: 2018-08-05T23:39:47.812672: step 6114, loss 0.640554.
Train: 2018-08-05T23:39:47.967659: step 6115, loss 0.579073.
Train: 2018-08-05T23:39:48.123873: step 6116, loss 0.571114.
Train: 2018-08-05T23:39:48.295737: step 6117, loss 0.5537.
Train: 2018-08-05T23:39:48.451921: step 6118, loss 0.51091.
Train: 2018-08-05T23:39:48.592542: step 6119, loss 0.640508.
Train: 2018-08-05T23:39:48.779969: step 6120, loss 0.553613.
Test: 2018-08-05T23:39:49.027884: step 6120, loss 0.548048.
Train: 2018-08-05T23:39:49.184098: step 6121, loss 0.55378.
Train: 2018-08-05T23:39:49.340341: step 6122, loss 0.60529.
Train: 2018-08-05T23:39:49.496554: step 6123, loss 0.545184.
Train: 2018-08-05T23:39:49.652767: step 6124, loss 0.587778.
Train: 2018-08-05T23:39:49.824572: step 6125, loss 0.579322.
Train: 2018-08-05T23:39:49.985313: step 6126, loss 0.553811.
Train: 2018-08-05T23:39:50.141522: step 6127, loss 0.545639.
Train: 2018-08-05T23:39:50.297710: step 6128, loss 0.579149.
Train: 2018-08-05T23:39:50.453952: step 6129, loss 0.604945.
Train: 2018-08-05T23:39:50.610166: step 6130, loss 0.596503.
Test: 2018-08-05T23:39:50.860078: step 6130, loss 0.548322.
Train: 2018-08-05T23:39:51.016314: step 6131, loss 0.596199.
Train: 2018-08-05T23:39:51.172503: step 6132, loss 0.579344.
Train: 2018-08-05T23:39:51.328747: step 6133, loss 0.554125.
Train: 2018-08-05T23:39:51.500586: step 6134, loss 0.545707.
Train: 2018-08-05T23:39:51.656794: step 6135, loss 0.529107.
Train: 2018-08-05T23:39:51.828623: step 6136, loss 0.56243.
Train: 2018-08-05T23:39:51.984843: step 6137, loss 0.587602.
Train: 2018-08-05T23:39:52.141060: step 6138, loss 0.579004.
Train: 2018-08-05T23:39:52.297272: step 6139, loss 0.645895.
Train: 2018-08-05T23:39:52.453452: step 6140, loss 0.595647.
Test: 2018-08-05T23:39:52.703395: step 6140, loss 0.548694.
Train: 2018-08-05T23:39:52.859606: step 6141, loss 0.554242.
Train: 2018-08-05T23:39:53.015855: step 6142, loss 0.562494.
Train: 2018-08-05T23:39:53.172063: step 6143, loss 0.562565.
Train: 2018-08-05T23:39:53.328247: step 6144, loss 0.587292.
Train: 2018-08-05T23:39:53.484459: step 6145, loss 0.570838.
Train: 2018-08-05T23:39:53.640673: step 6146, loss 0.554324.
Train: 2018-08-05T23:39:53.796916: step 6147, loss 0.529724.
Train: 2018-08-05T23:39:53.953131: step 6148, loss 0.538051.
Train: 2018-08-05T23:39:54.124933: step 6149, loss 0.570883.
Train: 2018-08-05T23:39:54.281178: step 6150, loss 0.628271.
Test: 2018-08-05T23:39:54.515467: step 6150, loss 0.548871.
Train: 2018-08-05T23:39:54.671710: step 6151, loss 0.496897.
Train: 2018-08-05T23:39:54.827924: step 6152, loss 0.513229.
Train: 2018-08-05T23:39:54.999760: step 6153, loss 0.587368.
Train: 2018-08-05T23:39:55.155941: step 6154, loss 0.529416.
Train: 2018-08-05T23:39:55.312154: step 6155, loss 0.562536.
Train: 2018-08-05T23:39:55.484013: step 6156, loss 0.578985.
Train: 2018-08-05T23:39:55.640232: step 6157, loss 0.529303.
Train: 2018-08-05T23:39:55.812037: step 6158, loss 0.578838.
Train: 2018-08-05T23:39:55.968250: step 6159, loss 0.587584.
Train: 2018-08-05T23:39:56.140084: step 6160, loss 0.562475.
Test: 2018-08-05T23:39:56.374434: step 6160, loss 0.548366.
Train: 2018-08-05T23:39:56.530649: step 6161, loss 0.537367.
Train: 2018-08-05T23:39:56.686831: step 6162, loss 0.57093.
Train: 2018-08-05T23:39:56.843074: step 6163, loss 0.511836.
Train: 2018-08-05T23:39:57.016663: step 6164, loss 0.579401.
Train: 2018-08-05T23:39:57.172844: step 6165, loss 0.545614.
Train: 2018-08-05T23:39:57.329090: step 6166, loss 0.43489.
Train: 2018-08-05T23:39:57.485272: step 6167, loss 0.588284.
Train: 2018-08-05T23:39:57.641516: step 6168, loss 0.528056.
Train: 2018-08-05T23:39:57.813319: step 6169, loss 0.484846.
Train: 2018-08-05T23:39:57.960320: step 6170, loss 0.510226.
Test: 2018-08-05T23:39:58.210290: step 6170, loss 0.547804.
Train: 2018-08-05T23:39:58.366513: step 6171, loss 0.544833.
Train: 2018-08-05T23:39:58.522687: step 6172, loss 0.536368.
Train: 2018-08-05T23:39:58.678931: step 6173, loss 0.597534.
Train: 2018-08-05T23:39:58.850767: step 6174, loss 0.624663.
Train: 2018-08-05T23:39:59.016201: step 6175, loss 0.580918.
Train: 2018-08-05T23:39:59.172419: step 6176, loss 0.553469.
Train: 2018-08-05T23:39:59.328627: step 6177, loss 0.562919.
Train: 2018-08-05T23:39:59.500432: step 6178, loss 0.642796.
Train: 2018-08-05T23:39:59.656674: step 6179, loss 0.572.
Train: 2018-08-05T23:39:59.812893: step 6180, loss 0.544518.
Test: 2018-08-05T23:40:00.061861: step 6180, loss 0.547661.
Train: 2018-08-05T23:40:00.218098: step 6181, loss 0.553709.
Train: 2018-08-05T23:40:00.374288: step 6182, loss 0.527487.
Train: 2018-08-05T23:40:00.530531: step 6183, loss 0.500374.
Train: 2018-08-05T23:40:00.686713: step 6184, loss 0.46505.
Train: 2018-08-05T23:40:00.845485: step 6185, loss 0.562594.
Train: 2018-08-05T23:40:01.017290: step 6186, loss 0.562385.
Train: 2018-08-05T23:40:01.173503: step 6187, loss 0.616224.
Train: 2018-08-05T23:40:01.341258: step 6188, loss 0.571631.
Train: 2018-08-05T23:40:01.497502: step 6189, loss 0.607423.
Train: 2018-08-05T23:40:01.669336: step 6190, loss 0.669758.
Test: 2018-08-05T23:40:01.903659: step 6190, loss 0.547661.
Train: 2018-08-05T23:40:02.059870: step 6191, loss 0.638754.
Train: 2018-08-05T23:40:02.216084: step 6192, loss 0.59813.
Train: 2018-08-05T23:40:02.372267: step 6193, loss 0.535964.
Train: 2018-08-05T23:40:02.544131: step 6194, loss 0.562343.
Train: 2018-08-05T23:40:02.715965: step 6195, loss 0.623384.
Train: 2018-08-05T23:40:02.872149: step 6196, loss 0.571082.
Train: 2018-08-05T23:40:03.027524: step 6197, loss 0.571011.
Train: 2018-08-05T23:40:03.199359: step 6198, loss 0.57103.
Train: 2018-08-05T23:40:03.355573: step 6199, loss 0.536792.
Train: 2018-08-05T23:40:03.511785: step 6200, loss 0.53697.
Test: 2018-08-05T23:40:03.761759: step 6200, loss 0.548221.
Train: 2018-08-05T23:40:04.538048: step 6201, loss 0.5879.
Train: 2018-08-05T23:40:04.694290: step 6202, loss 0.545432.
Train: 2018-08-05T23:40:04.850474: step 6203, loss 0.655154.
Train: 2018-08-05T23:40:05.022307: step 6204, loss 0.495148.
Train: 2018-08-05T23:40:05.175891: step 6205, loss 0.512162.
Train: 2018-08-05T23:40:05.347721: step 6206, loss 0.646127.
Train: 2018-08-05T23:40:05.503934: step 6207, loss 0.59568.
Train: 2018-08-05T23:40:05.660117: step 6208, loss 0.554659.
Train: 2018-08-05T23:40:05.831951: step 6209, loss 0.595143.
Train: 2018-08-05T23:40:05.988195: step 6210, loss 0.620783.
Test: 2018-08-05T23:40:06.222518: step 6210, loss 0.548795.
Train: 2018-08-05T23:40:06.378723: step 6211, loss 0.579279.
Train: 2018-08-05T23:40:06.550533: step 6212, loss 0.611783.
Train: 2018-08-05T23:40:06.706776: step 6213, loss 0.612204.
Train: 2018-08-05T23:40:06.862960: step 6214, loss 0.563139.
Train: 2018-08-05T23:40:07.034584: step 6215, loss 0.514208.
Train: 2018-08-05T23:40:07.190798: step 6216, loss 0.554352.
Train: 2018-08-05T23:40:07.347019: step 6217, loss 0.562604.
Train: 2018-08-05T23:40:07.503228: step 6218, loss 0.530195.
Train: 2018-08-05T23:40:07.675028: step 6219, loss 0.457686.
Train: 2018-08-05T23:40:07.831273: step 6220, loss 0.603779.
Test: 2018-08-05T23:40:08.079604: step 6220, loss 0.54907.
Train: 2018-08-05T23:40:08.235784: step 6221, loss 0.603615.
Train: 2018-08-05T23:40:08.392027: step 6222, loss 0.55438.
Train: 2018-08-05T23:40:08.548241: step 6223, loss 0.669071.
Train: 2018-08-05T23:40:08.704423: step 6224, loss 0.529829.
Train: 2018-08-05T23:40:08.876288: step 6225, loss 0.546511.
Train: 2018-08-05T23:40:09.036531: step 6226, loss 0.612179.
Train: 2018-08-05T23:40:09.192744: step 6227, loss 0.521395.
Train: 2018-08-05T23:40:09.348957: step 6228, loss 0.538077.
Train: 2018-08-05T23:40:09.505201: step 6229, loss 0.587126.
Train: 2018-08-05T23:40:09.723869: step 6230, loss 0.612086.
Test: 2018-08-05T23:40:09.957581: step 6230, loss 0.548742.
Train: 2018-08-05T23:40:10.113761: step 6231, loss 0.513129.
Train: 2018-08-05T23:40:10.285596: step 6232, loss 0.479856.
Train: 2018-08-05T23:40:10.441839: step 6233, loss 0.595742.
Train: 2018-08-05T23:40:10.598052: step 6234, loss 0.579145.
Train: 2018-08-05T23:40:10.769857: step 6235, loss 0.570853.
Train: 2018-08-05T23:40:10.929512: step 6236, loss 0.554078.
Train: 2018-08-05T23:40:11.085726: step 6237, loss 0.528949.
Train: 2018-08-05T23:40:11.241939: step 6238, loss 0.579228.
Train: 2018-08-05T23:40:11.398152: step 6239, loss 0.545611.
Train: 2018-08-05T23:40:11.554366: step 6240, loss 0.545534.
Test: 2018-08-05T23:40:11.788686: step 6240, loss 0.54824.
Train: 2018-08-05T23:40:11.944898: step 6241, loss 0.503187.
Train: 2018-08-05T23:40:12.132355: step 6242, loss 0.562431.
Train: 2018-08-05T23:40:12.304214: step 6243, loss 0.519713.
Train: 2018-08-05T23:40:12.452825: step 6244, loss 0.528109.
Train: 2018-08-05T23:40:12.609003: step 6245, loss 0.562477.
Train: 2018-08-05T23:40:12.780875: step 6246, loss 0.519198.
Train: 2018-08-05T23:40:12.937076: step 6247, loss 0.518976.
Train: 2018-08-05T23:40:13.108917: step 6248, loss 0.56246.
Train: 2018-08-05T23:40:13.265098: step 6249, loss 0.571442.
Train: 2018-08-05T23:40:13.436934: step 6250, loss 0.632714.
Test: 2018-08-05T23:40:13.671253: step 6250, loss 0.547721.
Train: 2018-08-05T23:40:13.825595: step 6251, loss 0.536222.
Train: 2018-08-05T23:40:13.981838: step 6252, loss 0.553961.
Train: 2018-08-05T23:40:14.138054: step 6253, loss 0.562414.
Train: 2018-08-05T23:40:14.309856: step 6254, loss 0.509471.
Train: 2018-08-05T23:40:14.466099: step 6255, loss 0.527182.
Train: 2018-08-05T23:40:14.606694: step 6256, loss 0.562694.
Train: 2018-08-05T23:40:14.778495: step 6257, loss 0.473668.
Train: 2018-08-05T23:40:14.934739: step 6258, loss 0.518364.
Train: 2018-08-05T23:40:15.090922: step 6259, loss 0.607098.
Train: 2018-08-05T23:40:15.247165: step 6260, loss 0.562176.
Test: 2018-08-05T23:40:15.481485: step 6260, loss 0.547612.
Train: 2018-08-05T23:40:15.653320: step 6261, loss 0.607626.
Train: 2018-08-05T23:40:15.809527: step 6262, loss 0.616594.
Train: 2018-08-05T23:40:15.965717: step 6263, loss 0.624798.
Train: 2018-08-05T23:40:16.121959: step 6264, loss 0.588204.
Train: 2018-08-05T23:40:16.278143: step 6265, loss 0.544383.
Train: 2018-08-05T23:40:16.434355: step 6266, loss 0.492895.
Train: 2018-08-05T23:40:16.590602: step 6267, loss 0.594058.
Train: 2018-08-05T23:40:16.762404: step 6268, loss 0.568367.
Train: 2018-08-05T23:40:16.918647: step 6269, loss 0.496908.
Train: 2018-08-05T23:40:17.074860: step 6270, loss 0.546841.
Test: 2018-08-05T23:40:17.324802: step 6270, loss 0.54832.
Train: 2018-08-05T23:40:17.527848: step 6271, loss 0.620909.
Train: 2018-08-05T23:40:17.684092: step 6272, loss 0.549658.
Train: 2018-08-05T23:40:17.855897: step 6273, loss 0.621068.
Train: 2018-08-05T23:40:18.011255: step 6274, loss 0.546999.
Train: 2018-08-05T23:40:18.183084: step 6275, loss 0.463468.
Train: 2018-08-05T23:40:18.339298: step 6276, loss 0.533425.
Train: 2018-08-05T23:40:18.495515: step 6277, loss 0.553918.
Train: 2018-08-05T23:40:18.667315: step 6278, loss 0.581827.
Train: 2018-08-05T23:40:18.823529: step 6279, loss 0.534765.
Train: 2018-08-05T23:40:18.984166: step 6280, loss 0.500385.
Test: 2018-08-05T23:40:19.218517: step 6280, loss 0.547613.
Train: 2018-08-05T23:40:19.374730: step 6281, loss 0.535842.
Train: 2018-08-05T23:40:19.546534: step 6282, loss 0.436383.
Train: 2018-08-05T23:40:19.702747: step 6283, loss 0.617334.
Train: 2018-08-05T23:40:19.858961: step 6284, loss 0.581054.
Train: 2018-08-05T23:40:20.021661: step 6285, loss 0.617676.
Train: 2018-08-05T23:40:20.177874: step 6286, loss 0.48977.
Train: 2018-08-05T23:40:20.334117: step 6287, loss 0.572052.
Train: 2018-08-05T23:40:20.490331: step 6288, loss 0.52632.
Train: 2018-08-05T23:40:20.662135: step 6289, loss 0.608715.
Train: 2018-08-05T23:40:20.818348: step 6290, loss 0.544631.
Test: 2018-08-05T23:40:21.036127: step 6290, loss 0.54757.
Train: 2018-08-05T23:40:21.207992: step 6291, loss 0.58123.
Train: 2018-08-05T23:40:21.364176: step 6292, loss 0.572152.
Train: 2018-08-05T23:40:21.520389: step 6293, loss 0.526359.
Train: 2018-08-05T23:40:21.676602: step 6294, loss 0.553806.
Train: 2018-08-05T23:40:21.832815: step 6295, loss 0.608583.
Train: 2018-08-05T23:40:21.992572: step 6296, loss 0.562825.
Train: 2018-08-05T23:40:22.148815: step 6297, loss 0.517496.
Train: 2018-08-05T23:40:22.320656: step 6298, loss 0.608186.
Train: 2018-08-05T23:40:22.476866: step 6299, loss 0.508234.
Train: 2018-08-05T23:40:22.633046: step 6300, loss 0.571797.
Test: 2018-08-05T23:40:22.867402: step 6300, loss 0.547587.
Train: 2018-08-05T23:40:23.601568: step 6301, loss 0.472545.
Train: 2018-08-05T23:40:23.757812: step 6302, loss 0.616814.
Train: 2018-08-05T23:40:23.914026: step 6303, loss 0.481587.
Train: 2018-08-05T23:40:24.085831: step 6304, loss 0.616929.
Train: 2018-08-05T23:40:24.257664: step 6305, loss 0.517479.
Train: 2018-08-05T23:40:24.413877: step 6306, loss 0.571512.
Train: 2018-08-05T23:40:24.570090: step 6307, loss 0.55391.
Train: 2018-08-05T23:40:24.741924: step 6308, loss 0.562529.
Train: 2018-08-05T23:40:24.898138: step 6309, loss 0.652351.
Train: 2018-08-05T23:40:25.054351: step 6310, loss 0.616373.
Test: 2018-08-05T23:40:25.288672: step 6310, loss 0.547632.
Train: 2018-08-05T23:40:25.444884: step 6311, loss 0.535763.
Train: 2018-08-05T23:40:25.616719: step 6312, loss 0.571652.
Train: 2018-08-05T23:40:25.772932: step 6313, loss 0.544847.
Train: 2018-08-05T23:40:25.929146: step 6314, loss 0.536007.
Train: 2018-08-05T23:40:26.100982: step 6315, loss 0.739192.
Train: 2018-08-05T23:40:26.257224: step 6316, loss 0.536076.
Train: 2018-08-05T23:40:26.413406: step 6317, loss 0.53622.
Train: 2018-08-05T23:40:26.569621: step 6318, loss 0.606065.
Train: 2018-08-05T23:40:26.725866: step 6319, loss 0.545313.
Train: 2018-08-05T23:40:26.882046: step 6320, loss 0.545091.
Test: 2018-08-05T23:40:27.116376: step 6320, loss 0.548019.
Train: 2018-08-05T23:40:27.272609: step 6321, loss 0.631127.
Train: 2018-08-05T23:40:27.444414: step 6322, loss 0.553828.
Train: 2018-08-05T23:40:27.585006: step 6323, loss 0.5795.
Train: 2018-08-05T23:40:27.756841: step 6324, loss 0.588217.
Train: 2018-08-05T23:40:27.897432: step 6325, loss 0.646458.
Train: 2018-08-05T23:40:28.061031: step 6326, loss 0.546061.
Train: 2018-08-05T23:40:28.217239: step 6327, loss 0.604688.
Train: 2018-08-05T23:40:28.373428: step 6328, loss 0.579331.
Train: 2018-08-05T23:40:28.529641: step 6329, loss 0.538864.
Train: 2018-08-05T23:40:28.685855: step 6330, loss 0.498462.
Test: 2018-08-05T23:40:28.933970: step 6330, loss 0.548946.
Train: 2018-08-05T23:40:29.090183: step 6331, loss 0.530045.
Train: 2018-08-05T23:40:29.246396: step 6332, loss 0.530078.
Train: 2018-08-05T23:40:29.418262: step 6333, loss 0.604193.
Train: 2018-08-05T23:40:29.574468: step 6334, loss 0.587407.
Train: 2018-08-05T23:40:29.730682: step 6335, loss 0.587091.
Train: 2018-08-05T23:40:29.902492: step 6336, loss 0.637295.
Train: 2018-08-05T23:40:30.062793: step 6337, loss 0.521121.
Train: 2018-08-05T23:40:30.234628: step 6338, loss 0.512654.
Train: 2018-08-05T23:40:30.375219: step 6339, loss 0.496251.
Train: 2018-08-05T23:40:30.547084: step 6340, loss 0.529301.
Test: 2018-08-05T23:40:30.781410: step 6340, loss 0.548526.
Train: 2018-08-05T23:40:30.944889: step 6341, loss 0.596442.
Train: 2018-08-05T23:40:31.101072: step 6342, loss 0.544869.
Train: 2018-08-05T23:40:31.272907: step 6343, loss 0.54601.
Train: 2018-08-05T23:40:31.453337: step 6344, loss 0.570593.
Train: 2018-08-05T23:40:31.609518: step 6345, loss 0.537294.
Train: 2018-08-05T23:40:31.765763: step 6346, loss 0.579466.
Train: 2018-08-05T23:40:31.920095: step 6347, loss 0.579394.
Train: 2018-08-05T23:40:32.076309: step 6348, loss 0.613443.
Train: 2018-08-05T23:40:32.248144: step 6349, loss 0.494965.
Train: 2018-08-05T23:40:32.404356: step 6350, loss 0.511368.
Test: 2018-08-05T23:40:32.638678: step 6350, loss 0.548143.
Train: 2018-08-05T23:40:32.794889: step 6351, loss 0.570999.
Train: 2018-08-05T23:40:32.953611: step 6352, loss 0.570897.
Train: 2018-08-05T23:40:33.109827: step 6353, loss 0.476842.
Train: 2018-08-05T23:40:33.277563: step 6354, loss 0.622868.
Train: 2018-08-05T23:40:33.433775: step 6355, loss 0.605664.
Train: 2018-08-05T23:40:33.589989: step 6356, loss 0.501902.
Train: 2018-08-05T23:40:33.746202: step 6357, loss 0.545139.
Train: 2018-08-05T23:40:33.918037: step 6358, loss 0.519143.
Train: 2018-08-05T23:40:34.077751: step 6359, loss 0.588582.
Train: 2018-08-05T23:40:34.224426: step 6360, loss 0.475405.
Test: 2018-08-05T23:40:34.477543: step 6360, loss 0.547795.
Train: 2018-08-05T23:40:34.623677: step 6361, loss 0.553746.
Train: 2018-08-05T23:40:34.779889: step 6362, loss 0.553504.
Train: 2018-08-05T23:40:34.951717: step 6363, loss 0.615268.
Train: 2018-08-05T23:40:35.107936: step 6364, loss 0.562532.
Train: 2018-08-05T23:40:35.264119: step 6365, loss 0.483076.
Train: 2018-08-05T23:40:35.420363: step 6366, loss 0.677837.
Train: 2018-08-05T23:40:35.577580: step 6367, loss 0.589198.
Train: 2018-08-05T23:40:35.733790: step 6368, loss 0.536005.
Train: 2018-08-05T23:40:35.889999: step 6369, loss 0.580212.
Train: 2018-08-05T23:40:36.061842: step 6370, loss 0.615459.
Test: 2018-08-05T23:40:36.296142: step 6370, loss 0.547724.
Train: 2018-08-05T23:40:36.452342: step 6371, loss 0.579986.
Train: 2018-08-05T23:40:36.608555: step 6372, loss 0.597642.
Train: 2018-08-05T23:40:36.780389: step 6373, loss 0.597462.
Train: 2018-08-05T23:40:36.936635: step 6374, loss 0.48384.
Train: 2018-08-05T23:40:37.092816: step 6375, loss 0.579862.
Train: 2018-08-05T23:40:37.249059: step 6376, loss 0.536495.
Train: 2018-08-05T23:40:37.405272: step 6377, loss 0.562481.
Train: 2018-08-05T23:40:37.592728: step 6378, loss 0.501713.
Train: 2018-08-05T23:40:37.764532: step 6379, loss 0.588427.
Train: 2018-08-05T23:40:37.905155: step 6380, loss 0.58852.
Test: 2018-08-05T23:40:38.155066: step 6380, loss 0.547905.
Train: 2018-08-05T23:40:38.311279: step 6381, loss 0.527824.
Train: 2018-08-05T23:40:38.467492: step 6382, loss 0.545156.
Train: 2018-08-05T23:40:38.623705: step 6383, loss 0.518856.
Train: 2018-08-05T23:40:38.779918: step 6384, loss 0.484243.
Train: 2018-08-05T23:40:38.936763: step 6385, loss 0.684195.
Train: 2018-08-05T23:40:39.093008: step 6386, loss 0.53632.
Train: 2018-08-05T23:40:39.264811: step 6387, loss 0.562323.
Train: 2018-08-05T23:40:39.421026: step 6388, loss 0.596966.
Train: 2018-08-05T23:40:39.577268: step 6389, loss 0.57996.
Train: 2018-08-05T23:40:39.733482: step 6390, loss 0.553761.
Test: 2018-08-05T23:40:39.971303: step 6390, loss 0.547898.
Train: 2018-08-05T23:40:40.127514: step 6391, loss 0.597031.
Train: 2018-08-05T23:40:40.299317: step 6392, loss 0.562489.
Train: 2018-08-05T23:40:40.455563: step 6393, loss 0.588429.
Train: 2018-08-05T23:40:40.611744: step 6394, loss 0.579751.
Train: 2018-08-05T23:40:40.767957: step 6395, loss 0.502051.
Train: 2018-08-05T23:40:40.939817: step 6396, loss 0.5538.
Train: 2018-08-05T23:40:41.096006: step 6397, loss 0.562315.
Train: 2018-08-05T23:40:41.252249: step 6398, loss 0.55379.
Train: 2018-08-05T23:40:41.408432: step 6399, loss 0.519752.
Train: 2018-08-05T23:40:41.580297: step 6400, loss 0.54539.
Test: 2018-08-05T23:40:41.814618: step 6400, loss 0.547985.
Train: 2018-08-05T23:40:42.595462: step 6401, loss 0.536604.
Train: 2018-08-05T23:40:42.751674: step 6402, loss 0.519443.
Train: 2018-08-05T23:40:42.925964: step 6403, loss 0.596891.
Train: 2018-08-05T23:40:43.097800: step 6404, loss 0.545152.
Train: 2018-08-05T23:40:43.254012: step 6405, loss 0.544968.
Train: 2018-08-05T23:40:43.410225: step 6406, loss 0.52776.
Train: 2018-08-05T23:40:43.566469: step 6407, loss 0.588637.
Train: 2018-08-05T23:40:43.722677: step 6408, loss 0.553616.
Train: 2018-08-05T23:40:43.878895: step 6409, loss 0.56243.
Train: 2018-08-05T23:40:44.042761: step 6410, loss 0.571264.
Test: 2018-08-05T23:40:44.277051: step 6410, loss 0.54778.
Train: 2018-08-05T23:40:44.433263: step 6411, loss 0.606105.
Train: 2018-08-05T23:40:44.605098: step 6412, loss 0.562569.
Train: 2018-08-05T23:40:44.761342: step 6413, loss 0.623636.
Train: 2018-08-05T23:40:44.917525: step 6414, loss 0.562353.
Train: 2018-08-05T23:40:45.089360: step 6415, loss 0.536181.
Train: 2018-08-05T23:40:45.245573: step 6416, loss 0.544972.
Train: 2018-08-05T23:40:45.401824: step 6417, loss 0.571293.
Train: 2018-08-05T23:40:45.558029: step 6418, loss 0.5277.
Train: 2018-08-05T23:40:45.714243: step 6419, loss 0.527628.
Train: 2018-08-05T23:40:45.870462: step 6420, loss 0.58828.
Test: 2018-08-05T23:40:46.104746: step 6420, loss 0.547874.
Train: 2018-08-05T23:40:46.276580: step 6421, loss 0.588588.
Train: 2018-08-05T23:40:46.432793: step 6422, loss 0.605721.
Train: 2018-08-05T23:40:46.589007: step 6423, loss 0.596777.
Train: 2018-08-05T23:40:46.745221: step 6424, loss 0.537132.
Train: 2018-08-05T23:40:46.901434: step 6425, loss 0.494749.
Train: 2018-08-05T23:40:47.073305: step 6426, loss 0.536503.
Train: 2018-08-05T23:40:47.229481: step 6427, loss 0.570562.
Train: 2018-08-05T23:40:47.385694: step 6428, loss 0.622555.
Train: 2018-08-05T23:40:47.541937: step 6429, loss 0.588185.
Train: 2018-08-05T23:40:47.698151: step 6430, loss 0.562582.
Test: 2018-08-05T23:40:47.932471: step 6430, loss 0.548051.
Train: 2018-08-05T23:40:48.104305: step 6431, loss 0.519808.
Train: 2018-08-05T23:40:48.260489: step 6432, loss 0.545339.
Train: 2018-08-05T23:40:48.416702: step 6433, loss 0.536588.
Train: 2018-08-05T23:40:48.572915: step 6434, loss 0.614115.
Train: 2018-08-05T23:40:48.744749: step 6435, loss 0.510679.
Train: 2018-08-05T23:40:48.900993: step 6436, loss 0.596913.
Train: 2018-08-05T23:40:49.048958: step 6437, loss 0.55384.
Train: 2018-08-05T23:40:49.220816: step 6438, loss 0.605587.
Train: 2018-08-05T23:40:49.377029: step 6439, loss 0.60563.
Train: 2018-08-05T23:40:49.533217: step 6440, loss 0.579683.
Test: 2018-08-05T23:40:49.767540: step 6440, loss 0.548011.
Train: 2018-08-05T23:40:49.937380: step 6441, loss 0.562547.
Train: 2018-08-05T23:40:50.093596: step 6442, loss 0.553834.
Train: 2018-08-05T23:40:50.249773: step 6443, loss 0.553877.
Train: 2018-08-05T23:40:50.405986: step 6444, loss 0.536802.
Train: 2018-08-05T23:40:50.562225: step 6445, loss 0.545251.
Train: 2018-08-05T23:40:50.718413: step 6446, loss 0.605177.
Train: 2018-08-05T23:40:50.874626: step 6447, loss 0.571026.
Train: 2018-08-05T23:40:51.035314: step 6448, loss 0.570979.
Train: 2018-08-05T23:40:51.191527: step 6449, loss 0.61357.
Train: 2018-08-05T23:40:51.347741: step 6450, loss 0.477232.
Test: 2018-08-05T23:40:51.582095: step 6450, loss 0.548143.
Train: 2018-08-05T23:40:51.753895: step 6451, loss 0.51971.
Train: 2018-08-05T23:40:51.921687: step 6452, loss 0.579325.
Train: 2018-08-05T23:40:52.080001: step 6453, loss 0.562498.
Train: 2018-08-05T23:40:52.236206: step 6454, loss 0.536818.
Train: 2018-08-05T23:40:52.392419: step 6455, loss 0.485553.
Train: 2018-08-05T23:40:52.564255: step 6456, loss 0.485274.
Train: 2018-08-05T23:40:52.720504: step 6457, loss 0.614044.
Train: 2018-08-05T23:40:52.892302: step 6458, loss 0.65729.
Train: 2018-08-05T23:40:53.037652: step 6459, loss 0.605418.
Train: 2018-08-05T23:40:53.193869: step 6460, loss 0.536529.
Test: 2018-08-05T23:40:53.443792: step 6460, loss 0.547962.
Train: 2018-08-05T23:40:53.615644: step 6461, loss 0.665657.
Train: 2018-08-05T23:40:53.854489: step 6462, loss 0.510814.
Train: 2018-08-05T23:40:54.040043: step 6463, loss 0.605362.
Train: 2018-08-05T23:40:54.206577: step 6464, loss 0.476639.
Train: 2018-08-05T23:40:54.358498: step 6465, loss 0.536698.
Train: 2018-08-05T23:40:54.514743: step 6466, loss 0.553782.
Train: 2018-08-05T23:40:54.670926: step 6467, loss 0.553626.
Train: 2018-08-05T23:40:54.842760: step 6468, loss 0.579542.
Train: 2018-08-05T23:40:55.014594: step 6469, loss 0.553869.
Train: 2018-08-05T23:40:55.186460: step 6470, loss 0.571085.
Test: 2018-08-05T23:40:55.420779: step 6470, loss 0.547988.
Train: 2018-08-05T23:40:55.592607: step 6471, loss 0.596814.
Train: 2018-08-05T23:40:55.764417: step 6472, loss 0.57108.
Train: 2018-08-05T23:40:55.951903: step 6473, loss 0.631175.
Train: 2018-08-05T23:40:56.108116: step 6474, loss 0.55388.
Train: 2018-08-05T23:40:56.264332: step 6475, loss 0.536828.
Train: 2018-08-05T23:40:56.436134: step 6476, loss 0.622153.
Train: 2018-08-05T23:40:56.592378: step 6477, loss 0.588123.
Train: 2018-08-05T23:40:56.748598: step 6478, loss 0.519707.
Train: 2018-08-05T23:40:56.920396: step 6479, loss 0.604878.
Train: 2018-08-05T23:40:57.076608: step 6480, loss 0.511563.
Test: 2018-08-05T23:40:57.310929: step 6480, loss 0.54821.
Train: 2018-08-05T23:40:57.467172: step 6481, loss 0.587887.
Train: 2018-08-05T23:40:57.639008: step 6482, loss 0.511639.
Train: 2018-08-05T23:40:57.795221: step 6483, loss 0.519971.
Train: 2018-08-05T23:40:57.951404: step 6484, loss 0.553805.
Train: 2018-08-05T23:40:58.107646: step 6485, loss 0.604807.
Train: 2018-08-05T23:40:58.279482: step 6486, loss 0.536953.
Train: 2018-08-05T23:40:58.435698: step 6487, loss 0.553978.
Train: 2018-08-05T23:40:58.591877: step 6488, loss 0.536725.
Train: 2018-08-05T23:40:58.748123: step 6489, loss 0.5626.
Train: 2018-08-05T23:40:58.904304: step 6490, loss 0.613568.
Test: 2018-08-05T23:40:59.154244: step 6490, loss 0.548095.
Train: 2018-08-05T23:40:59.310488: step 6491, loss 0.571022.
Train: 2018-08-05T23:40:59.482292: step 6492, loss 0.519808.
Train: 2018-08-05T23:40:59.638538: step 6493, loss 0.580637.
Train: 2018-08-05T23:40:59.794718: step 6494, loss 0.579504.
Train: 2018-08-05T23:40:59.950058: step 6495, loss 0.528107.
Train: 2018-08-05T23:41:00.121887: step 6496, loss 0.562387.
Train: 2018-08-05T23:41:00.278106: step 6497, loss 0.630892.
Train: 2018-08-05T23:41:00.434322: step 6498, loss 0.519725.
Train: 2018-08-05T23:41:00.590532: step 6499, loss 0.528037.
Train: 2018-08-05T23:41:00.746715: step 6500, loss 0.502233.
Test: 2018-08-05T23:41:01.000171: step 6500, loss 0.548009.
Train: 2018-08-05T23:41:01.734401: step 6501, loss 0.528136.
Train: 2018-08-05T23:41:01.907446: step 6502, loss 0.54529.
Train: 2018-08-05T23:41:02.052124: step 6503, loss 0.51899.
Train: 2018-08-05T23:41:02.216665: step 6504, loss 0.527594.
Train: 2018-08-05T23:41:02.372883: step 6505, loss 0.545107.
Train: 2018-08-05T23:41:02.529060: step 6506, loss 0.509972.
Train: 2018-08-05T23:41:02.685299: step 6507, loss 0.536193.
Train: 2018-08-05T23:41:02.841523: step 6508, loss 0.562424.
Train: 2018-08-05T23:41:02.995751: step 6509, loss 0.553747.
Train: 2018-08-05T23:41:03.164627: step 6510, loss 0.54488.
Test: 2018-08-05T23:41:03.414538: step 6510, loss 0.547647.
Train: 2018-08-05T23:41:03.572455: step 6511, loss 0.642826.
Train: 2018-08-05T23:41:03.744289: step 6512, loss 0.571369.
Train: 2018-08-05T23:41:03.900503: step 6513, loss 0.553693.
Train: 2018-08-05T23:41:04.060883: step 6514, loss 0.589382.
Train: 2018-08-05T23:41:04.217071: step 6515, loss 0.580341.
Train: 2018-08-05T23:41:04.390408: step 6516, loss 0.491273.
Train: 2018-08-05T23:41:04.558931: step 6517, loss 0.526979.
Train: 2018-08-05T23:41:04.714207: step 6518, loss 0.571468.
Train: 2018-08-05T23:41:04.901637: step 6519, loss 0.6073.
Train: 2018-08-05T23:41:05.053478: step 6520, loss 0.562527.
Test: 2018-08-05T23:41:05.303389: step 6520, loss 0.547643.
Train: 2018-08-05T23:41:05.459633: step 6521, loss 0.625109.
Train: 2018-08-05T23:41:05.615845: step 6522, loss 0.544798.
Train: 2018-08-05T23:41:05.772060: step 6523, loss 0.56254.
Train: 2018-08-05T23:41:05.928272: step 6524, loss 0.597906.
Train: 2018-08-05T23:41:06.092897: step 6525, loss 0.588805.
Train: 2018-08-05T23:41:06.249111: step 6526, loss 0.509824.
Train: 2018-08-05T23:41:06.420974: step 6527, loss 0.544823.
Train: 2018-08-05T23:41:06.592779: step 6528, loss 0.59748.
Train: 2018-08-05T23:41:06.749019: step 6529, loss 0.562907.
Train: 2018-08-05T23:41:06.932352: step 6530, loss 0.562772.
Test: 2018-08-05T23:41:07.166642: step 6530, loss 0.547839.
Train: 2018-08-05T23:41:07.338505: step 6531, loss 0.640899.
Train: 2018-08-05T23:41:07.494751: step 6532, loss 0.492897.
Train: 2018-08-05T23:41:07.650900: step 6533, loss 0.640382.
Train: 2018-08-05T23:41:07.822735: step 6534, loss 0.544919.
Train: 2018-08-05T23:41:07.994600: step 6535, loss 0.562211.
Train: 2018-08-05T23:41:08.150813: step 6536, loss 0.588041.
Train: 2018-08-05T23:41:08.322650: step 6537, loss 0.621761.
Train: 2018-08-05T23:41:08.478831: step 6538, loss 0.553424.
Train: 2018-08-05T23:41:08.650666: step 6539, loss 0.52133.
Train: 2018-08-05T23:41:08.838152: step 6540, loss 0.545174.
Test: 2018-08-05T23:41:09.085965: step 6540, loss 0.548603.
Train: 2018-08-05T23:41:09.258504: step 6541, loss 0.563719.
Train: 2018-08-05T23:41:09.408110: step 6542, loss 0.562069.
Train: 2018-08-05T23:41:09.564356: step 6543, loss 0.528036.
Train: 2018-08-05T23:41:09.720566: step 6544, loss 0.578599.
Train: 2018-08-05T23:41:09.892396: step 6545, loss 0.59621.
Train: 2018-08-05T23:41:10.045627: step 6546, loss 0.572188.
Train: 2018-08-05T23:41:10.217501: step 6547, loss 0.588045.
Train: 2018-08-05T23:41:10.373705: step 6548, loss 0.580422.
Train: 2018-08-05T23:41:10.529919: step 6549, loss 0.586342.
Train: 2018-08-05T23:41:10.686100: step 6550, loss 0.504054.
Test: 2018-08-05T23:41:10.934002: step 6550, loss 0.54852.
Train: 2018-08-05T23:41:11.105836: step 6551, loss 0.545406.
Train: 2018-08-05T23:41:11.262050: step 6552, loss 0.520433.
Train: 2018-08-05T23:41:11.418263: step 6553, loss 0.528895.
Train: 2018-08-05T23:41:11.590128: step 6554, loss 0.562774.
Train: 2018-08-05T23:41:11.746343: step 6555, loss 0.622218.
Train: 2018-08-05T23:41:11.902555: step 6556, loss 0.519575.
Train: 2018-08-05T23:41:12.063290: step 6557, loss 0.528066.
Train: 2018-08-05T23:41:12.235125: step 6558, loss 0.519406.
Train: 2018-08-05T23:41:12.406930: step 6559, loss 0.588368.
Train: 2018-08-05T23:41:12.563174: step 6560, loss 0.562328.
Test: 2018-08-05T23:41:12.797488: step 6560, loss 0.547883.
Train: 2018-08-05T23:41:12.967994: step 6561, loss 0.588592.
Train: 2018-08-05T23:41:13.124217: step 6562, loss 0.605973.
Train: 2018-08-05T23:41:13.296047: step 6563, loss 0.536342.
Train: 2018-08-05T23:41:13.467883: step 6564, loss 0.588568.
Train: 2018-08-05T23:41:13.639717: step 6565, loss 0.571156.
Train: 2018-08-05T23:41:13.795901: step 6566, loss 0.623183.
Train: 2018-08-05T23:41:13.965750: step 6567, loss 0.562343.
Train: 2018-08-05T23:41:14.137585: step 6568, loss 0.562431.
Train: 2018-08-05T23:41:14.306538: step 6569, loss 0.527885.
Train: 2018-08-05T23:41:14.462721: step 6570, loss 0.597058.
Test: 2018-08-05T23:41:14.697072: step 6570, loss 0.547999.
Train: 2018-08-05T23:41:14.884499: step 6571, loss 0.62262.
Train: 2018-08-05T23:41:15.047465: step 6572, loss 0.588035.
Train: 2018-08-05T23:41:15.234919: step 6573, loss 0.579588.
Train: 2018-08-05T23:41:15.391162: step 6574, loss 0.553867.
Train: 2018-08-05T23:41:15.547376: step 6575, loss 0.596527.
Train: 2018-08-05T23:41:15.719212: step 6576, loss 0.53692.
Train: 2018-08-05T23:41:15.891044: step 6577, loss 0.579345.
Train: 2018-08-05T23:41:16.047227: step 6578, loss 0.570648.
Train: 2018-08-05T23:41:16.203441: step 6579, loss 0.553969.
Train: 2018-08-05T23:41:16.375275: step 6580, loss 0.428424.
Test: 2018-08-05T23:41:16.609627: step 6580, loss 0.548365.
Train: 2018-08-05T23:41:16.781454: step 6581, loss 0.520257.
Train: 2018-08-05T23:41:16.937644: step 6582, loss 0.587569.
Train: 2018-08-05T23:41:17.093889: step 6583, loss 0.520347.
Train: 2018-08-05T23:41:17.265691: step 6584, loss 0.553977.
Train: 2018-08-05T23:41:17.421935: step 6585, loss 0.528465.
Train: 2018-08-05T23:41:17.578151: step 6586, loss 0.621875.
Train: 2018-08-05T23:41:17.749987: step 6587, loss 0.562318.
Train: 2018-08-05T23:41:17.921788: step 6588, loss 0.485122.
Train: 2018-08-05T23:41:18.078000: step 6589, loss 0.562381.
Train: 2018-08-05T23:41:18.249136: step 6590, loss 0.554001.
Test: 2018-08-05T23:41:18.483457: step 6590, loss 0.547942.
Train: 2018-08-05T23:41:18.639668: step 6591, loss 0.58841.
Train: 2018-08-05T23:41:18.795881: step 6592, loss 0.545261.
Train: 2018-08-05T23:41:18.967717: step 6593, loss 0.588592.
Train: 2018-08-05T23:41:19.123923: step 6594, loss 0.579758.
Train: 2018-08-05T23:41:19.280142: step 6595, loss 0.56234.
Train: 2018-08-05T23:41:19.451946: step 6596, loss 0.588429.
Train: 2018-08-05T23:41:19.623781: step 6597, loss 0.51884.
Train: 2018-08-05T23:41:19.780025: step 6598, loss 0.466807.
Train: 2018-08-05T23:41:19.967455: step 6599, loss 0.597166.
Train: 2018-08-05T23:41:20.123694: step 6600, loss 0.544897.
Test: 2018-08-05T23:41:20.357991: step 6600, loss 0.547794.
Train: 2018-08-05T23:41:21.137748: step 6601, loss 0.57113.
Train: 2018-08-05T23:41:21.317248: step 6602, loss 0.536241.
Train: 2018-08-05T23:41:21.479488: step 6603, loss 0.535903.
Train: 2018-08-05T23:41:21.651317: step 6604, loss 0.483454.
Train: 2018-08-05T23:41:21.823127: step 6605, loss 0.606325.
Train: 2018-08-05T23:41:21.986754: step 6606, loss 0.518342.
Train: 2018-08-05T23:41:22.158589: step 6607, loss 0.500509.
Train: 2018-08-05T23:41:22.314802: step 6608, loss 0.571381.
Train: 2018-08-05T23:41:22.486637: step 6609, loss 0.509066.
Train: 2018-08-05T23:41:22.642850: step 6610, loss 0.62548.
Test: 2018-08-05T23:41:22.892817: step 6610, loss 0.547621.
Train: 2018-08-05T23:41:23.049035: step 6611, loss 0.499589.
Train: 2018-08-05T23:41:23.236461: step 6612, loss 0.499934.
Train: 2018-08-05T23:41:23.392703: step 6613, loss 0.589409.
Train: 2018-08-05T23:41:23.564509: step 6614, loss 0.589863.
Train: 2018-08-05T23:41:23.720753: step 6615, loss 0.617057.
Train: 2018-08-05T23:41:23.892581: step 6616, loss 0.572167.
Train: 2018-08-05T23:41:24.077937: step 6617, loss 0.589624.
Train: 2018-08-05T23:41:24.249771: step 6618, loss 0.553549.
Train: 2018-08-05T23:41:24.431840: step 6619, loss 0.634674.
Train: 2018-08-05T23:41:24.597403: step 6620, loss 0.652282.
Test: 2018-08-05T23:41:24.835156: step 6620, loss 0.547641.
Train: 2018-08-05T23:41:24.989345: step 6621, loss 0.517889.
Train: 2018-08-05T23:41:25.161213: step 6622, loss 0.598055.
Train: 2018-08-05T23:41:25.333043: step 6623, loss 0.615635.
Train: 2018-08-05T23:41:25.504876: step 6624, loss 0.571329.
Train: 2018-08-05T23:41:25.661087: step 6625, loss 0.62369.
Train: 2018-08-05T23:41:25.832896: step 6626, loss 0.553707.
Train: 2018-08-05T23:41:25.993819: step 6627, loss 0.571034.
Train: 2018-08-05T23:41:26.165660: step 6628, loss 0.613837.
Train: 2018-08-05T23:41:26.321896: step 6629, loss 0.545278.
Train: 2018-08-05T23:41:26.478108: step 6630, loss 0.502708.
Test: 2018-08-05T23:41:26.728049: step 6630, loss 0.548194.
Train: 2018-08-05T23:41:26.884234: step 6631, loss 0.562032.
Train: 2018-08-05T23:41:27.056100: step 6632, loss 0.570921.
Train: 2018-08-05T23:41:27.227903: step 6633, loss 0.587848.
Train: 2018-08-05T23:41:27.399737: step 6634, loss 0.503537.
Train: 2018-08-05T23:41:27.555951: step 6635, loss 0.570956.
Train: 2018-08-05T23:41:27.712188: step 6636, loss 0.503959.
Train: 2018-08-05T23:41:27.868377: step 6637, loss 0.503308.
Train: 2018-08-05T23:41:28.024590: step 6638, loss 0.570573.
Train: 2018-08-05T23:41:28.196427: step 6639, loss 0.545428.
Train: 2018-08-05T23:41:28.352639: step 6640, loss 0.571299.
Test: 2018-08-05T23:41:28.602581: step 6640, loss 0.548205.
Train: 2018-08-05T23:41:28.774442: step 6641, loss 0.562435.
Train: 2018-08-05T23:41:28.930658: step 6642, loss 0.647293.
Train: 2018-08-05T23:41:29.102461: step 6643, loss 0.596601.
Train: 2018-08-05T23:41:29.243087: step 6644, loss 0.616911.
Train: 2018-08-05T23:41:29.414919: step 6645, loss 0.553956.
Train: 2018-08-05T23:41:29.571103: step 6646, loss 0.545636.
Train: 2018-08-05T23:41:29.742936: step 6647, loss 0.51158.
Train: 2018-08-05T23:41:29.930392: step 6648, loss 0.503527.
Train: 2018-08-05T23:41:30.070984: step 6649, loss 0.596126.
Train: 2018-08-05T23:41:30.242819: step 6650, loss 0.630432.
Test: 2018-08-05T23:41:30.477173: step 6650, loss 0.548234.
Train: 2018-08-05T23:41:30.649004: step 6651, loss 0.52841.
Train: 2018-08-05T23:41:30.805185: step 6652, loss 0.596335.
Train: 2018-08-05T23:41:30.985362: step 6653, loss 0.570925.
Train: 2018-08-05T23:41:31.157199: step 6654, loss 0.604684.
Train: 2018-08-05T23:41:31.313380: step 6655, loss 0.545964.
Train: 2018-08-05T23:41:31.485243: step 6656, loss 0.596341.
Train: 2018-08-05T23:41:31.657081: step 6657, loss 0.57078.
Train: 2018-08-05T23:41:31.813292: step 6658, loss 0.646313.
Train: 2018-08-05T23:41:31.983465: step 6659, loss 0.562485.
Train: 2018-08-05T23:41:32.139648: step 6660, loss 0.562471.
Test: 2018-08-05T23:41:32.373997: step 6660, loss 0.548523.
Train: 2018-08-05T23:41:32.545834: step 6661, loss 0.545798.
Train: 2018-08-05T23:41:32.717667: step 6662, loss 0.571044.
Train: 2018-08-05T23:41:32.869158: step 6663, loss 0.6039.
Train: 2018-08-05T23:41:33.047695: step 6664, loss 0.629116.
Train: 2018-08-05T23:41:33.203910: step 6665, loss 0.52931.
Train: 2018-08-05T23:41:33.360120: step 6666, loss 0.579192.
Train: 2018-08-05T23:41:33.531935: step 6667, loss 0.60405.
Train: 2018-08-05T23:41:33.703659: step 6668, loss 0.537982.
Train: 2018-08-05T23:41:33.856918: step 6669, loss 0.546248.
Train: 2018-08-05T23:41:34.013133: step 6670, loss 0.554668.
Test: 2018-08-05T23:41:34.247484: step 6670, loss 0.548937.
Train: 2018-08-05T23:41:34.415257: step 6671, loss 0.529948.
Train: 2018-08-05T23:41:34.571470: step 6672, loss 0.538345.
Train: 2018-08-05T23:41:34.743304: step 6673, loss 0.570861.
Train: 2018-08-05T23:41:34.899517: step 6674, loss 0.546365.
Train: 2018-08-05T23:41:35.054139: step 6675, loss 0.479913.
Train: 2018-08-05T23:41:35.210384: step 6676, loss 0.553951.
Train: 2018-08-05T23:41:35.382194: step 6677, loss 0.587557.
Train: 2018-08-05T23:41:35.542369: step 6678, loss 0.503913.
Train: 2018-08-05T23:41:35.698558: step 6679, loss 0.553925.
Train: 2018-08-05T23:41:35.886013: step 6680, loss 0.553987.
Test: 2018-08-05T23:41:36.149512: step 6680, loss 0.548234.
Train: 2018-08-05T23:41:36.399454: step 6681, loss 0.579204.
Train: 2018-08-05T23:41:36.573486: step 6682, loss 0.545351.
Train: 2018-08-05T23:41:36.729700: step 6683, loss 0.6136.
Train: 2018-08-05T23:41:36.885914: step 6684, loss 0.579453.
Train: 2018-08-05T23:41:37.042126: step 6685, loss 0.596534.
Train: 2018-08-05T23:41:37.198340: step 6686, loss 0.553795.
Train: 2018-08-05T23:41:37.370204: step 6687, loss 0.536707.
Train: 2018-08-05T23:41:37.526386: step 6688, loss 0.587945.
Train: 2018-08-05T23:41:37.698221: step 6689, loss 0.571504.
Train: 2018-08-05T23:41:37.854465: step 6690, loss 0.61513.
Test: 2018-08-05T23:41:38.088755: step 6690, loss 0.547997.
Train: 2018-08-05T23:41:38.244968: step 6691, loss 0.605375.
Train: 2018-08-05T23:41:38.401212: step 6692, loss 0.511241.
Train: 2018-08-05T23:41:38.573046: step 6693, loss 0.570743.
Train: 2018-08-05T23:41:38.729230: step 6694, loss 0.503067.
Train: 2018-08-05T23:41:38.885478: step 6695, loss 0.579265.
Train: 2018-08-05T23:41:39.057277: step 6696, loss 0.613289.
Train: 2018-08-05T23:41:39.213515: step 6697, loss 0.545435.
Train: 2018-08-05T23:41:39.385325: step 6698, loss 0.502753.
Train: 2018-08-05T23:41:39.557161: step 6699, loss 0.605206.
Train: 2018-08-05T23:41:39.714891: step 6700, loss 0.511223.
Test: 2018-08-05T23:41:39.949242: step 6700, loss 0.548078.
Train: 2018-08-05T23:41:40.636580: step 6701, loss 0.596457.
Train: 2018-08-05T23:41:40.808383: step 6702, loss 0.5537.
Train: 2018-08-05T23:41:40.964598: step 6703, loss 0.562503.
Train: 2018-08-05T23:41:41.120841: step 6704, loss 0.579326.
Train: 2018-08-05T23:41:41.292670: step 6705, loss 0.605341.
Train: 2018-08-05T23:41:41.448900: step 6706, loss 0.596522.
Train: 2018-08-05T23:41:41.605095: step 6707, loss 0.587959.
Train: 2018-08-05T23:41:41.761285: step 6708, loss 0.588328.
Train: 2018-08-05T23:41:41.928001: step 6709, loss 0.639056.
Train: 2018-08-05T23:41:42.087284: step 6710, loss 0.545543.
Test: 2018-08-05T23:41:42.321611: step 6710, loss 0.548282.
Train: 2018-08-05T23:41:42.477792: step 6711, loss 0.604157.
Train: 2018-08-05T23:41:42.649651: step 6712, loss 0.554212.
Train: 2018-08-05T23:41:42.805874: step 6713, loss 0.554205.
Train: 2018-08-05T23:41:42.966455: step 6714, loss 0.579187.
Train: 2018-08-05T23:41:43.122638: step 6715, loss 0.520647.
Train: 2018-08-05T23:41:43.278851: step 6716, loss 0.512657.
Train: 2018-08-05T23:41:43.435100: step 6717, loss 0.521361.
Train: 2018-08-05T23:41:43.606930: step 6718, loss 0.529462.
Train: 2018-08-05T23:41:43.763112: step 6719, loss 0.520606.
Train: 2018-08-05T23:41:43.919356: step 6720, loss 0.537472.
Test: 2018-08-05T23:41:44.153674: step 6720, loss 0.548328.
Train: 2018-08-05T23:41:44.309889: step 6721, loss 0.553863.
Train: 2018-08-05T23:41:44.481724: step 6722, loss 0.579221.
Train: 2018-08-05T23:41:44.647500: step 6723, loss 0.545365.
Train: 2018-08-05T23:41:44.803713: step 6724, loss 0.579533.
Train: 2018-08-05T23:41:44.961597: step 6725, loss 0.647626.
Train: 2018-08-05T23:41:45.133433: step 6726, loss 0.588858.
Train: 2018-08-05T23:41:45.289676: step 6727, loss 0.553469.
Train: 2018-08-05T23:41:45.445859: step 6728, loss 0.545332.
Train: 2018-08-05T23:41:45.617724: step 6729, loss 0.588058.
Train: 2018-08-05T23:41:45.773937: step 6730, loss 0.597053.
Test: 2018-08-05T23:41:46.011815: step 6730, loss 0.548144.
Train: 2018-08-05T23:41:46.183645: step 6731, loss 0.66493.
Train: 2018-08-05T23:41:46.339858: step 6732, loss 0.545502.
Train: 2018-08-05T23:41:46.496072: step 6733, loss 0.605473.
Train: 2018-08-05T23:41:46.652315: step 6734, loss 0.511728.
Train: 2018-08-05T23:41:46.824119: step 6735, loss 0.570853.
Train: 2018-08-05T23:41:46.980333: step 6736, loss 0.587994.
Train: 2018-08-05T23:41:47.136577: step 6737, loss 0.612544.
Train: 2018-08-05T23:41:47.292759: step 6738, loss 0.59624.
Train: 2018-08-05T23:41:47.448972: step 6739, loss 0.512397.
Train: 2018-08-05T23:41:47.620832: step 6740, loss 0.512594.
Test: 2018-08-05T23:41:47.855158: step 6740, loss 0.548564.
Train: 2018-08-05T23:41:48.011340: step 6741, loss 0.537634.
Train: 2018-08-05T23:41:48.167553: step 6742, loss 0.520954.
Train: 2018-08-05T23:41:48.339387: step 6743, loss 0.562633.
Train: 2018-08-05T23:41:48.495631: step 6744, loss 0.537449.
Train: 2018-08-05T23:41:48.651844: step 6745, loss 0.554336.
Train: 2018-08-05T23:41:48.808028: step 6746, loss 0.528996.
Train: 2018-08-05T23:41:48.964241: step 6747, loss 0.503497.
Train: 2018-08-05T23:41:49.120454: step 6748, loss 0.604651.
Train: 2018-08-05T23:41:49.276667: step 6749, loss 0.562408.
Train: 2018-08-05T23:41:49.448532: step 6750, loss 0.596481.
Test: 2018-08-05T23:41:49.682854: step 6750, loss 0.548089.
Train: 2018-08-05T23:41:49.854687: step 6751, loss 0.485543.
Train: 2018-08-05T23:41:50.010900: step 6752, loss 0.656849.
Train: 2018-08-05T23:41:50.167083: step 6753, loss 0.562442.
Train: 2018-08-05T23:41:50.338918: step 6754, loss 0.528039.
Train: 2018-08-05T23:41:50.495131: step 6755, loss 0.527848.
Train: 2018-08-05T23:41:50.651344: step 6756, loss 0.553852.
Train: 2018-08-05T23:41:50.807557: step 6757, loss 0.579684.
Train: 2018-08-05T23:41:50.948179: step 6758, loss 0.61414.
Train: 2018-08-05T23:41:51.120014: step 6759, loss 0.527873.
Train: 2018-08-05T23:41:51.276228: step 6760, loss 0.605739.
Test: 2018-08-05T23:41:51.510553: step 6760, loss 0.547921.
Train: 2018-08-05T23:41:51.682381: step 6761, loss 0.545171.
Train: 2018-08-05T23:41:51.838594: step 6762, loss 0.510536.
Train: 2018-08-05T23:41:51.996507: step 6763, loss 0.553858.
Train: 2018-08-05T23:41:52.152691: step 6764, loss 0.571184.
Train: 2018-08-05T23:41:52.308934: step 6765, loss 0.562463.
Train: 2018-08-05T23:41:52.465118: step 6766, loss 0.579736.
Train: 2018-08-05T23:41:52.621331: step 6767, loss 0.553735.
Train: 2018-08-05T23:41:52.777543: step 6768, loss 0.527721.
Train: 2018-08-05T23:41:52.921729: step 6769, loss 0.553731.
Train: 2018-08-05T23:41:53.093565: step 6770, loss 0.588459.
Test: 2018-08-05T23:41:53.327889: step 6770, loss 0.547839.
Train: 2018-08-05T23:41:53.484129: step 6771, loss 0.544941.
Train: 2018-08-05T23:41:53.655963: step 6772, loss 0.536296.
Train: 2018-08-05T23:41:53.812145: step 6773, loss 0.562494.
Train: 2018-08-05T23:41:53.968360: step 6774, loss 0.518778.
Train: 2018-08-05T23:41:54.124572: step 6775, loss 0.49239.
Train: 2018-08-05T23:41:54.296440: step 6776, loss 0.527348.
Train: 2018-08-05T23:41:54.452621: step 6777, loss 0.597664.
Train: 2018-08-05T23:41:54.608833: step 6778, loss 0.544809.
Train: 2018-08-05T23:41:54.765079: step 6779, loss 0.553715.
Train: 2018-08-05T23:41:54.933504: step 6780, loss 0.588998.
Test: 2018-08-05T23:41:55.167800: step 6780, loss 0.547675.
Train: 2018-08-05T23:41:55.339670: step 6781, loss 0.500493.
Train: 2018-08-05T23:41:55.495878: step 6782, loss 0.55386.
Train: 2018-08-05T23:41:55.652061: step 6783, loss 0.553682.
Train: 2018-08-05T23:41:55.823926: step 6784, loss 0.589435.
Train: 2018-08-05T23:41:55.978048: step 6785, loss 0.607216.
Train: 2018-08-05T23:41:56.134230: step 6786, loss 0.580473.
Train: 2018-08-05T23:41:56.290442: step 6787, loss 0.535822.
Train: 2018-08-05T23:41:56.446655: step 6788, loss 0.518057.
Train: 2018-08-05T23:41:56.602869: step 6789, loss 0.544804.
Train: 2018-08-05T23:41:56.759082: step 6790, loss 0.500277.
Test: 2018-08-05T23:41:56.997974: step 6790, loss 0.547643.
Train: 2018-08-05T23:41:57.169778: step 6791, loss 0.553744.
Train: 2018-08-05T23:41:57.325992: step 6792, loss 0.553639.
Train: 2018-08-05T23:41:57.482205: step 6793, loss 0.553645.
Train: 2018-08-05T23:41:57.654074: step 6794, loss 0.544702.
Train: 2018-08-05T23:41:57.794661: step 6795, loss 0.562673.
Train: 2018-08-05T23:41:57.966490: step 6796, loss 0.544762.
Train: 2018-08-05T23:41:58.122678: step 6797, loss 0.553591.
Train: 2018-08-05T23:41:58.278893: step 6798, loss 0.616551.
Train: 2018-08-05T23:41:58.435130: step 6799, loss 0.607669.
Train: 2018-08-05T23:41:58.606970: step 6800, loss 0.634387.
Test: 2018-08-05T23:41:58.841261: step 6800, loss 0.547633.
Train: 2018-08-05T23:41:59.575462: step 6801, loss 0.589392.
Train: 2018-08-05T23:41:59.747296: step 6802, loss 0.535948.
Train: 2018-08-05T23:41:59.903542: step 6803, loss 0.553631.
Train: 2018-08-05T23:42:00.059724: step 6804, loss 0.430016.
Train: 2018-08-05T23:42:00.231558: step 6805, loss 0.518347.
Train: 2018-08-05T23:42:00.387771: step 6806, loss 0.571316.
Train: 2018-08-05T23:42:00.543984: step 6807, loss 0.589097.
Train: 2018-08-05T23:42:00.700199: step 6808, loss 0.58907.
Train: 2018-08-05T23:42:00.856411: step 6809, loss 0.562535.
Train: 2018-08-05T23:42:01.028246: step 6810, loss 0.615496.
Test: 2018-08-05T23:42:01.262596: step 6810, loss 0.547722.
Train: 2018-08-05T23:42:01.418779: step 6811, loss 0.562499.
Train: 2018-08-05T23:42:01.590637: step 6812, loss 0.606452.
Train: 2018-08-05T23:42:01.746827: step 6813, loss 0.579994.
Train: 2018-08-05T23:42:01.922865: step 6814, loss 0.501259.
Train: 2018-08-05T23:42:02.075497: step 6815, loss 0.518803.
Train: 2018-08-05T23:42:02.231680: step 6816, loss 0.56243.
Train: 2018-08-05T23:42:02.403545: step 6817, loss 0.562463.
Train: 2018-08-05T23:42:02.559767: step 6818, loss 0.571155.
Train: 2018-08-05T23:42:02.715971: step 6819, loss 0.614655.
Train: 2018-08-05T23:42:02.872153: step 6820, loss 0.597163.
Test: 2018-08-05T23:42:03.110107: step 6820, loss 0.547898.
Train: 2018-08-05T23:42:03.281954: step 6821, loss 0.605685.
Train: 2018-08-05T23:42:03.438153: step 6822, loss 0.501965.
Train: 2018-08-05T23:42:03.598943: step 6823, loss 0.605539.
Train: 2018-08-05T23:42:03.755160: step 6824, loss 0.493765.
Train: 2018-08-05T23:42:03.911368: step 6825, loss 0.536625.
Train: 2018-08-05T23:42:04.067607: step 6826, loss 0.545327.
Train: 2018-08-05T23:42:04.237411: step 6827, loss 0.553802.
Train: 2018-08-05T23:42:04.393625: step 6828, loss 0.61403.
Train: 2018-08-05T23:42:04.549838: step 6829, loss 0.519362.
Train: 2018-08-05T23:42:04.706051: step 6830, loss 0.57961.
Test: 2018-08-05T23:42:04.940403: step 6830, loss 0.547993.
Train: 2018-08-05T23:42:05.112237: step 6831, loss 0.562367.
Train: 2018-08-05T23:42:05.268420: step 6832, loss 0.639798.
Train: 2018-08-05T23:42:05.424633: step 6833, loss 0.502317.
Train: 2018-08-05T23:42:05.580876: step 6834, loss 0.579548.
Train: 2018-08-05T23:42:05.737060: step 6835, loss 0.528082.
Train: 2018-08-05T23:42:05.918192: step 6836, loss 0.553785.
Train: 2018-08-05T23:42:06.063583: step 6837, loss 0.545211.
Train: 2018-08-05T23:42:06.235451: step 6838, loss 0.519511.
Train: 2018-08-05T23:42:06.391661: step 6839, loss 0.510828.
Train: 2018-08-05T23:42:06.547869: step 6840, loss 0.467669.
Test: 2018-08-05T23:42:06.797786: step 6840, loss 0.547915.
Train: 2018-08-05T23:42:06.951890: step 6841, loss 0.622991.
Train: 2018-08-05T23:42:07.101993: step 6842, loss 0.562476.
Train: 2018-08-05T23:42:07.258205: step 6843, loss 0.545046.
Train: 2018-08-05T23:42:07.430074: step 6844, loss 0.579907.
Train: 2018-08-05T23:42:07.586283: step 6845, loss 0.588615.
Train: 2018-08-05T23:42:07.742497: step 6846, loss 0.544903.
Train: 2018-08-05T23:42:07.898681: step 6847, loss 0.641151.
Train: 2018-08-05T23:42:08.062449: step 6848, loss 0.510071.
Train: 2018-08-05T23:42:08.218693: step 6849, loss 0.597338.
Train: 2018-08-05T23:42:08.390496: step 6850, loss 0.54489.
Test: 2018-08-05T23:42:08.624849: step 6850, loss 0.547817.
Train: 2018-08-05T23:42:08.781030: step 6851, loss 0.483863.
Train: 2018-08-05T23:42:08.937274: step 6852, loss 0.527531.
Train: 2018-08-05T23:42:09.109078: step 6853, loss 0.562534.
Train: 2018-08-05T23:42:09.265292: step 6854, loss 0.580032.
Train: 2018-08-05T23:42:09.421505: step 6855, loss 0.553694.
Train: 2018-08-05T23:42:09.593338: step 6856, loss 0.509741.
Train: 2018-08-05T23:42:09.765174: step 6857, loss 0.615291.
Train: 2018-08-05T23:42:09.921387: step 6858, loss 0.659385.
Train: 2018-08-05T23:42:10.077600: step 6859, loss 0.544895.
Train: 2018-08-05T23:42:10.249436: step 6860, loss 0.606361.
Test: 2018-08-05T23:42:10.483758: step 6860, loss 0.547782.
Train: 2018-08-05T23:42:10.655619: step 6861, loss 0.518691.
Train: 2018-08-05T23:42:10.811833: step 6862, loss 0.588604.
Train: 2018-08-05T23:42:10.968015: step 6863, loss 0.606159.
Train: 2018-08-05T23:42:11.124229: step 6864, loss 0.553675.
Train: 2018-08-05T23:42:11.280442: step 6865, loss 0.588645.
Train: 2018-08-05T23:42:11.452277: step 6866, loss 0.501658.
Train: 2018-08-05T23:42:11.608490: step 6867, loss 0.493224.
Train: 2018-08-05T23:42:11.764704: step 6868, loss 0.562282.
Train: 2018-08-05T23:42:11.936163: step 6869, loss 0.57981.
Train: 2018-08-05T23:42:12.092376: step 6870, loss 0.571131.
Test: 2018-08-05T23:42:12.326729: step 6870, loss 0.547909.
Train: 2018-08-05T23:42:12.494546: step 6871, loss 0.588423.
Train: 2018-08-05T23:42:12.666380: step 6872, loss 0.545138.
Train: 2018-08-05T23:42:12.822593: step 6873, loss 0.571083.
Train: 2018-08-05T23:42:12.986356: step 6874, loss 0.579691.
Train: 2018-08-05T23:42:13.142539: step 6875, loss 0.46743.
Train: 2018-08-05T23:42:13.298752: step 6876, loss 0.501962.
Train: 2018-08-05T23:42:13.470594: step 6877, loss 0.579711.
Train: 2018-08-05T23:42:13.626801: step 6878, loss 0.588461.
Train: 2018-08-05T23:42:13.783044: step 6879, loss 0.536356.
Train: 2018-08-05T23:42:13.939257: step 6880, loss 0.571129.
Test: 2018-08-05T23:42:14.189168: step 6880, loss 0.547838.
Train: 2018-08-05T23:42:14.345381: step 6881, loss 0.501494.
Train: 2018-08-05T23:42:14.501596: step 6882, loss 0.431518.
Train: 2018-08-05T23:42:14.673428: step 6883, loss 0.579963.
Train: 2018-08-05T23:42:14.829642: step 6884, loss 0.527264.
Train: 2018-08-05T23:42:14.986694: step 6885, loss 0.491839.
Train: 2018-08-05T23:42:15.142907: step 6886, loss 0.65129.
Train: 2018-08-05T23:42:15.299121: step 6887, loss 0.526942.
Train: 2018-08-05T23:42:15.470924: step 6888, loss 0.607169.
Train: 2018-08-05T23:42:15.627137: step 6889, loss 0.553665.
Train: 2018-08-05T23:42:15.783352: step 6890, loss 0.553653.
Test: 2018-08-05T23:42:16.017710: step 6890, loss 0.547619.
Train: 2018-08-05T23:42:16.189536: step 6891, loss 0.562594.
Train: 2018-08-05T23:42:16.345749: step 6892, loss 0.607399.
Train: 2018-08-05T23:42:16.501931: step 6893, loss 0.562611.
Train: 2018-08-05T23:42:16.673793: step 6894, loss 0.553656.
Train: 2018-08-05T23:42:16.830004: step 6895, loss 0.571532.
Train: 2018-08-05T23:42:16.984190: step 6896, loss 0.589371.
Train: 2018-08-05T23:42:17.140435: step 6897, loss 0.526909.
Train: 2018-08-05T23:42:17.296650: step 6898, loss 0.500221.
Train: 2018-08-05T23:42:17.468451: step 6899, loss 0.61602.
Train: 2018-08-05T23:42:17.624665: step 6900, loss 0.491329.
Test: 2018-08-05T23:42:17.843395: step 6900, loss 0.547644.
Train: 2018-08-05T23:42:18.629097: step 6901, loss 0.53584.
Train: 2018-08-05T23:42:18.785311: step 6902, loss 0.687405.
Train: 2018-08-05T23:42:18.941523: step 6903, loss 0.580377.
Train: 2018-08-05T23:42:19.097737: step 6904, loss 0.64235.
Train: 2018-08-05T23:42:19.253950: step 6905, loss 0.474196.
Train: 2018-08-05T23:42:19.410198: step 6906, loss 0.509607.
Train: 2018-08-05T23:42:19.566410: step 6907, loss 0.588896.
Train: 2018-08-05T23:42:19.738212: step 6908, loss 0.55368.
Train: 2018-08-05T23:42:19.894449: step 6909, loss 0.571258.
Train: 2018-08-05T23:42:20.050637: step 6910, loss 0.571239.
Test: 2018-08-05T23:42:20.284989: step 6910, loss 0.547774.
Train: 2018-08-05T23:42:20.456793: step 6911, loss 0.544925.
Train: 2018-08-05T23:42:20.613038: step 6912, loss 0.64996.
Train: 2018-08-05T23:42:20.769218: step 6913, loss 0.58861.
Train: 2018-08-05T23:42:20.925431: step 6914, loss 0.562417.
Train: 2018-08-05T23:42:21.081645: step 6915, loss 0.588447.
Train: 2018-08-05T23:42:21.237859: step 6916, loss 0.475974.
Train: 2018-08-05T23:42:21.409693: step 6917, loss 0.527852.
Train: 2018-08-05T23:42:21.565938: step 6918, loss 0.536501.
Train: 2018-08-05T23:42:21.722120: step 6919, loss 0.571029.
Train: 2018-08-05T23:42:21.878333: step 6920, loss 0.622826.
Test: 2018-08-05T23:42:22.126038: step 6920, loss 0.547959.
Train: 2018-08-05T23:42:22.282245: step 6921, loss 0.536539.
Train: 2018-08-05T23:42:22.438456: step 6922, loss 0.553789.
Train: 2018-08-05T23:42:22.594700: step 6923, loss 0.588254.
Train: 2018-08-05T23:42:22.766504: step 6924, loss 0.510802.
Train: 2018-08-05T23:42:22.920732: step 6925, loss 0.56245.
Train: 2018-08-05T23:42:23.076976: step 6926, loss 0.484918.
Train: 2018-08-05T23:42:23.233159: step 6927, loss 0.545143.
Train: 2018-08-05T23:42:23.405025: step 6928, loss 0.562418.
Train: 2018-08-05T23:42:23.576859: step 6929, loss 0.501788.
Train: 2018-08-05T23:42:23.733072: step 6930, loss 0.588445.
Test: 2018-08-05T23:42:23.969799: step 6930, loss 0.547849.
Train: 2018-08-05T23:42:24.126029: step 6931, loss 0.597197.
Train: 2018-08-05T23:42:24.282241: step 6932, loss 0.588588.
Train: 2018-08-05T23:42:24.438424: step 6933, loss 0.649519.
Train: 2018-08-05T23:42:24.610259: step 6934, loss 0.536349.
Train: 2018-08-05T23:42:24.766472: step 6935, loss 0.605802.
Train: 2018-08-05T23:42:24.922716: step 6936, loss 0.579736.
Train: 2018-08-05T23:42:25.078929: step 6937, loss 0.571051.
Train: 2018-08-05T23:42:25.250764: step 6938, loss 0.571068.
Train: 2018-08-05T23:42:25.406982: step 6939, loss 0.605407.
Train: 2018-08-05T23:42:25.578815: step 6940, loss 0.639678.
Test: 2018-08-05T23:42:25.813104: step 6940, loss 0.548096.
Train: 2018-08-05T23:42:25.969314: step 6941, loss 0.536822.
Train: 2018-08-05T23:42:26.125559: step 6942, loss 0.630501.
Train: 2018-08-05T23:42:26.281771: step 6943, loss 0.570888.
Train: 2018-08-05T23:42:26.453576: step 6944, loss 0.570857.
Train: 2018-08-05T23:42:26.609790: step 6945, loss 0.53725.
Train: 2018-08-05T23:42:26.766033: step 6946, loss 0.50878.
Train: 2018-08-05T23:42:26.922843: step 6947, loss 0.587619.
Train: 2018-08-05T23:42:27.079088: step 6948, loss 0.59588.
Train: 2018-08-05T23:42:27.235306: step 6949, loss 0.562471.
Train: 2018-08-05T23:42:27.407105: step 6950, loss 0.504206.
Test: 2018-08-05T23:42:27.641456: step 6950, loss 0.548555.
Train: 2018-08-05T23:42:27.797638: step 6951, loss 0.587473.
Train: 2018-08-05T23:42:27.957268: step 6952, loss 0.520881.
Train: 2018-08-05T23:42:28.113450: step 6953, loss 0.537487.
Train: 2018-08-05T23:42:28.285287: step 6954, loss 0.604163.
Train: 2018-08-05T23:42:28.441523: step 6955, loss 0.587545.
Train: 2018-08-05T23:42:28.613363: step 6956, loss 0.554172.
Train: 2018-08-05T23:42:28.769546: step 6957, loss 0.579166.
Train: 2018-08-05T23:42:28.925760: step 6958, loss 0.654268.
Train: 2018-08-05T23:42:29.081973: step 6959, loss 0.504256.
Train: 2018-08-05T23:42:29.238217: step 6960, loss 0.53751.
Test: 2018-08-05T23:42:29.472509: step 6960, loss 0.548534.
Train: 2018-08-05T23:42:29.628719: step 6961, loss 0.579153.
Train: 2018-08-05T23:42:29.800584: step 6962, loss 0.57081.
Train: 2018-08-05T23:42:29.956767: step 6963, loss 0.570818.
Train: 2018-08-05T23:42:30.112980: step 6964, loss 0.612531.
Train: 2018-08-05T23:42:30.284815: step 6965, loss 0.587469.
Train: 2018-08-05T23:42:30.456649: step 6966, loss 0.512559.
Train: 2018-08-05T23:42:30.612863: step 6967, loss 0.629.
Train: 2018-08-05T23:42:30.769077: step 6968, loss 0.504366.
Train: 2018-08-05T23:42:30.925320: step 6969, loss 0.562446.
Train: 2018-08-05T23:42:31.081502: step 6970, loss 0.529265.
Test: 2018-08-05T23:42:31.315854: step 6970, loss 0.548561.
Train: 2018-08-05T23:42:31.472036: step 6971, loss 0.537512.
Train: 2018-08-05T23:42:31.643871: step 6972, loss 0.529117.
Train: 2018-08-05T23:42:31.800114: step 6973, loss 0.579282.
Train: 2018-08-05T23:42:31.956297: step 6974, loss 0.562477.
Train: 2018-08-05T23:42:32.112509: step 6975, loss 0.545607.
Train: 2018-08-05T23:42:32.268723: step 6976, loss 0.579308.
Train: 2018-08-05T23:42:32.424967: step 6977, loss 0.554046.
Train: 2018-08-05T23:42:32.581185: step 6978, loss 0.613145.
Train: 2018-08-05T23:42:32.737393: step 6979, loss 0.51167.
Train: 2018-08-05T23:42:32.913922: step 6980, loss 0.604765.
Test: 2018-08-05T23:42:33.141695: step 6980, loss 0.54821.
Train: 2018-08-05T23:42:33.313498: step 6981, loss 0.562398.
Train: 2018-08-05T23:42:33.469740: step 6982, loss 0.562422.
Train: 2018-08-05T23:42:33.625923: step 6983, loss 0.536889.
Train: 2018-08-05T23:42:33.797758: step 6984, loss 0.604912.
Train: 2018-08-05T23:42:33.959596: step 6985, loss 0.545423.
Train: 2018-08-05T23:42:34.115801: step 6986, loss 0.536832.
Train: 2018-08-05T23:42:34.280514: step 6987, loss 0.57946.
Train: 2018-08-05T23:42:34.436758: step 6988, loss 0.613523.
Train: 2018-08-05T23:42:34.591050: step 6989, loss 0.562345.
Train: 2018-08-05T23:42:34.747258: step 6990, loss 0.622052.
Test: 2018-08-05T23:42:34.981554: step 6990, loss 0.548167.
Train: 2018-08-05T23:42:35.153387: step 6991, loss 0.553857.
Train: 2018-08-05T23:42:35.309600: step 6992, loss 0.536977.
Train: 2018-08-05T23:42:35.465844: step 6993, loss 0.596317.
Train: 2018-08-05T23:42:35.622028: step 6994, loss 0.587757.
Train: 2018-08-05T23:42:35.793861: step 6995, loss 0.562441.
Train: 2018-08-05T23:42:35.950121: step 6996, loss 0.613159.
Train: 2018-08-05T23:42:36.106320: step 6997, loss 0.595959.
Train: 2018-08-05T23:42:36.262532: step 6998, loss 0.612815.
Train: 2018-08-05T23:42:36.418739: step 6999, loss 0.545843.
Train: 2018-08-05T23:42:36.581603: step 7000, loss 0.562413.
Test: 2018-08-05T23:42:36.815924: step 7000, loss 0.548607.
Train: 2018-08-05T23:42:37.564418: step 7001, loss 0.487932.
Train: 2018-08-05T23:42:37.736252: step 7002, loss 0.554116.
Train: 2018-08-05T23:42:37.892465: step 7003, loss 0.604252.
Train: 2018-08-05T23:42:38.052531: step 7004, loss 0.578875.
Train: 2018-08-05T23:42:38.208774: step 7005, loss 0.562738.
Train: 2018-08-05T23:42:38.380603: step 7006, loss 0.487883.
Train: 2018-08-05T23:42:38.536792: step 7007, loss 0.52111.
Train: 2018-08-05T23:42:38.693005: step 7008, loss 0.562475.
Train: 2018-08-05T23:42:38.849218: step 7009, loss 0.571069.
Train: 2018-08-05T23:42:39.021053: step 7010, loss 0.53737.
Test: 2018-08-05T23:42:39.255374: step 7010, loss 0.548357.
Train: 2018-08-05T23:42:39.411586: step 7011, loss 0.495124.
Train: 2018-08-05T23:42:39.583421: step 7012, loss 0.562406.
Train: 2018-08-05T23:42:39.739635: step 7013, loss 0.536918.
Train: 2018-08-05T23:42:39.895877: step 7014, loss 0.570918.
Train: 2018-08-05T23:42:40.067707: step 7015, loss 0.536746.
Train: 2018-08-05T23:42:40.223930: step 7016, loss 0.613853.
Train: 2018-08-05T23:42:40.380133: step 7017, loss 0.579579.
Train: 2018-08-05T23:42:40.536354: step 7018, loss 0.536579.
Train: 2018-08-05T23:42:40.692558: step 7019, loss 0.596893.
Train: 2018-08-05T23:42:40.848777: step 7020, loss 0.527878.
Test: 2018-08-05T23:42:41.083098: step 7020, loss 0.547926.
Train: 2018-08-05T23:42:41.239307: step 7021, loss 0.484605.
Train: 2018-08-05T23:42:41.411116: step 7022, loss 0.51039.
Train: 2018-08-05T23:42:41.567364: step 7023, loss 0.588518.
Train: 2018-08-05T23:42:41.723572: step 7024, loss 0.553711.
Train: 2018-08-05T23:42:41.879755: step 7025, loss 0.667375.
Train: 2018-08-05T23:42:42.035968: step 7026, loss 0.536211.
Train: 2018-08-05T23:42:42.192182: step 7027, loss 0.632433.
Train: 2018-08-05T23:42:42.348394: step 7028, loss 0.55369.
Train: 2018-08-05T23:42:42.504609: step 7029, loss 0.56221.
Train: 2018-08-05T23:42:42.660821: step 7030, loss 0.571228.
Test: 2018-08-05T23:42:42.895175: step 7030, loss 0.547888.
Train: 2018-08-05T23:42:43.049812: step 7031, loss 0.59719.
Train: 2018-08-05T23:42:43.221617: step 7032, loss 0.527957.
Train: 2018-08-05T23:42:43.377861: step 7033, loss 0.605265.
Train: 2018-08-05T23:42:43.534076: step 7034, loss 0.510674.
Train: 2018-08-05T23:42:43.690258: step 7035, loss 0.477407.
Train: 2018-08-05T23:42:43.862092: step 7036, loss 0.57125.
Train: 2018-08-05T23:42:44.022193: step 7037, loss 0.553738.
Train: 2018-08-05T23:42:44.178406: step 7038, loss 0.571023.
Train: 2018-08-05T23:42:44.334650: step 7039, loss 0.658226.
Train: 2018-08-05T23:42:44.490862: step 7040, loss 0.571095.
Test: 2018-08-05T23:42:44.725183: step 7040, loss 0.547878.
Train: 2018-08-05T23:42:44.897019: step 7041, loss 0.519052.
Train: 2018-08-05T23:42:45.053230: step 7042, loss 0.519035.
Train: 2018-08-05T23:42:45.209443: step 7043, loss 0.571096.
Train: 2018-08-05T23:42:45.379898: step 7044, loss 0.562424.
Train: 2018-08-05T23:42:45.536111: step 7045, loss 0.562447.
Train: 2018-08-05T23:42:45.692326: step 7046, loss 0.631931.
Train: 2018-08-05T23:42:45.848538: step 7047, loss 0.536388.
Train: 2018-08-05T23:42:46.004756: step 7048, loss 0.5364.
Train: 2018-08-05T23:42:46.160965: step 7049, loss 0.519093.
Train: 2018-08-05T23:42:46.317178: step 7050, loss 0.588403.
Test: 2018-08-05T23:42:46.567120: step 7050, loss 0.547894.
Train: 2018-08-05T23:42:46.723327: step 7051, loss 0.553715.
Train: 2018-08-05T23:42:46.879545: step 7052, loss 0.527738.
Train: 2018-08-05T23:42:47.049639: step 7053, loss 0.579763.
Train: 2018-08-05T23:42:47.205851: step 7054, loss 0.553731.
Train: 2018-08-05T23:42:47.362036: step 7055, loss 0.545051.
Train: 2018-08-05T23:42:47.518248: step 7056, loss 0.536367.
Train: 2018-08-05T23:42:47.674492: step 7057, loss 0.631852.
Train: 2018-08-05T23:42:47.830705: step 7058, loss 0.588552.
Train: 2018-08-05T23:42:47.991336: step 7059, loss 0.545174.
Train: 2018-08-05T23:42:48.147549: step 7060, loss 0.631662.
Test: 2018-08-05T23:42:48.381870: step 7060, loss 0.547926.
Train: 2018-08-05T23:42:48.553698: step 7061, loss 0.571202.
Train: 2018-08-05T23:42:48.709886: step 7062, loss 0.536489.
Train: 2018-08-05T23:42:48.866130: step 7063, loss 0.502049.
Train: 2018-08-05T23:42:49.037964: step 7064, loss 0.562399.
Train: 2018-08-05T23:42:49.209769: step 7065, loss 0.622632.
Train: 2018-08-05T23:42:49.365982: step 7066, loss 0.579585.
Train: 2018-08-05T23:42:49.522228: step 7067, loss 0.536523.
Train: 2018-08-05T23:42:49.678439: step 7068, loss 0.553834.
Train: 2018-08-05T23:42:49.834654: step 7069, loss 0.605132.
Train: 2018-08-05T23:42:49.990867: step 7070, loss 0.553929.
Test: 2018-08-05T23:42:50.240777: step 7070, loss 0.548089.
Train: 2018-08-05T23:42:50.397020: step 7071, loss 0.570931.
Train: 2018-08-05T23:42:50.553202: step 7072, loss 0.58811.
Train: 2018-08-05T23:42:50.725037: step 7073, loss 0.58798.
Train: 2018-08-05T23:42:50.881251: step 7074, loss 0.579442.
Train: 2018-08-05T23:42:51.037464: step 7075, loss 0.570827.
Train: 2018-08-05T23:42:51.193707: step 7076, loss 0.545464.
Train: 2018-08-05T23:42:51.365512: step 7077, loss 0.528744.
Train: 2018-08-05T23:42:51.521725: step 7078, loss 0.571034.
Train: 2018-08-05T23:42:51.677968: step 7079, loss 0.570985.
Train: 2018-08-05T23:42:51.834151: step 7080, loss 0.495009.
Test: 2018-08-05T23:42:52.068503: step 7080, loss 0.548268.
Train: 2018-08-05T23:42:52.224684: step 7081, loss 0.503243.
Train: 2018-08-05T23:42:52.380898: step 7082, loss 0.503151.
Train: 2018-08-05T23:42:52.552733: step 7083, loss 0.57951.
Train: 2018-08-05T23:42:52.708946: step 7084, loss 0.562541.
Train: 2018-08-05T23:42:52.865189: step 7085, loss 0.51959.
Train: 2018-08-05T23:42:53.020620: step 7086, loss 0.553857.
Train: 2018-08-05T23:42:53.176802: step 7087, loss 0.536571.
Train: 2018-08-05T23:42:53.333044: step 7088, loss 0.449936.
Train: 2018-08-05T23:42:53.489257: step 7089, loss 0.614599.
Train: 2018-08-05T23:42:53.645473: step 7090, loss 0.562445.
Test: 2018-08-05T23:42:53.895393: step 7090, loss 0.547776.
Train: 2018-08-05T23:42:54.055273: step 7091, loss 0.54491.
Train: 2018-08-05T23:42:54.211516: step 7092, loss 0.571245.
Train: 2018-08-05T23:42:54.367724: step 7093, loss 0.606544.
Train: 2018-08-05T23:42:54.523943: step 7094, loss 0.562494.
Train: 2018-08-05T23:42:54.680127: step 7095, loss 0.527181.
Train: 2018-08-05T23:42:54.851991: step 7096, loss 0.509453.
Train: 2018-08-05T23:42:54.992554: step 7097, loss 0.60034.
Train: 2018-08-05T23:42:55.164417: step 7098, loss 0.571356.
Train: 2018-08-05T23:42:55.320634: step 7099, loss 0.527019.
Train: 2018-08-05T23:42:55.476846: step 7100, loss 0.553539.
Test: 2018-08-05T23:42:55.711172: step 7100, loss 0.547641.
Train: 2018-08-05T23:42:56.429746: step 7101, loss 0.642786.
Train: 2018-08-05T23:42:56.601582: step 7102, loss 0.607095.
Train: 2018-08-05T23:42:56.757787: step 7103, loss 0.562498.
Train: 2018-08-05T23:42:56.927607: step 7104, loss 0.589008.
Train: 2018-08-05T23:42:57.083845: step 7105, loss 0.623752.
Train: 2018-08-05T23:42:57.255673: step 7106, loss 0.562768.
Train: 2018-08-05T23:42:57.411869: step 7107, loss 0.544728.
Train: 2018-08-05T23:42:57.568081: step 7108, loss 0.545414.
Train: 2018-08-05T23:42:57.724294: step 7109, loss 0.563147.
Train: 2018-08-05T23:42:57.880508: step 7110, loss 0.571996.
Test: 2018-08-05T23:42:58.120241: step 7110, loss 0.547995.
Train: 2018-08-05T23:42:58.276451: step 7111, loss 0.545549.
Train: 2018-08-05T23:42:58.448286: step 7112, loss 0.544851.
Train: 2018-08-05T23:42:58.604499: step 7113, loss 0.554201.
Train: 2018-08-05T23:42:58.760706: step 7114, loss 0.527732.
Train: 2018-08-05T23:42:58.916895: step 7115, loss 0.536512.
Train: 2018-08-05T23:42:59.073139: step 7116, loss 0.493059.
Train: 2018-08-05T23:42:59.244967: step 7117, loss 0.597215.
Train: 2018-08-05T23:42:59.401186: step 7118, loss 0.64081.
Train: 2018-08-05T23:42:59.557401: step 7119, loss 0.623412.
Train: 2018-08-05T23:42:59.713613: step 7120, loss 0.571085.
Test: 2018-08-05T23:42:59.963555: step 7120, loss 0.547898.
Train: 2018-08-05T23:43:00.119762: step 7121, loss 0.553695.
Train: 2018-08-05T23:43:00.275950: step 7122, loss 0.536464.
Train: 2018-08-05T23:43:00.432195: step 7123, loss 0.579667.
Train: 2018-08-05T23:43:00.604023: step 7124, loss 0.553719.
Train: 2018-08-05T23:43:00.760242: step 7125, loss 0.614077.
Train: 2018-08-05T23:43:00.916425: step 7126, loss 0.502204.
Train: 2018-08-05T23:43:01.072669: step 7127, loss 0.596818.
Train: 2018-08-05T23:43:01.228851: step 7128, loss 0.605376.
Train: 2018-08-05T23:43:01.400685: step 7129, loss 0.605253.
Train: 2018-08-05T23:43:01.556898: step 7130, loss 0.588062.
Test: 2018-08-05T23:43:01.791219: step 7130, loss 0.548151.
Train: 2018-08-05T23:43:01.947456: step 7131, loss 0.5709.
Train: 2018-08-05T23:43:02.119297: step 7132, loss 0.486038.
Train: 2018-08-05T23:43:02.275504: step 7133, loss 0.621763.
Train: 2018-08-05T23:43:02.431730: step 7134, loss 0.62163.
Train: 2018-08-05T23:43:02.587942: step 7135, loss 0.53714.
Train: 2018-08-05T23:43:02.744156: step 7136, loss 0.554065.
Train: 2018-08-05T23:43:02.916610: step 7137, loss 0.638033.
Train: 2018-08-05T23:43:03.070305: step 7138, loss 0.595961.
Train: 2018-08-05T23:43:03.226548: step 7139, loss 0.52079.
Train: 2018-08-05T23:43:03.382761: step 7140, loss 0.537543.
Test: 2018-08-05T23:43:03.617085: step 7140, loss 0.548594.
Train: 2018-08-05T23:43:03.773288: step 7141, loss 0.537515.
Train: 2018-08-05T23:43:03.935136: step 7142, loss 0.570841.
Train: 2018-08-05T23:43:04.091350: step 7143, loss 0.54595.
Train: 2018-08-05T23:43:04.247533: step 7144, loss 0.5459.
Train: 2018-08-05T23:43:04.403745: step 7145, loss 0.595735.
Train: 2018-08-05T23:43:04.559958: step 7146, loss 0.604086.
Train: 2018-08-05T23:43:04.731819: step 7147, loss 0.587456.
Train: 2018-08-05T23:43:04.887471: step 7148, loss 0.562504.
Train: 2018-08-05T23:43:05.037132: step 7149, loss 0.604005.
Train: 2018-08-05T23:43:05.195439: step 7150, loss 0.529387.
Test: 2018-08-05T23:43:05.429759: step 7150, loss 0.548675.
Train: 2018-08-05T23:43:05.601617: step 7151, loss 0.512875.
Train: 2018-08-05T23:43:05.773428: step 7152, loss 0.579142.
Train: 2018-08-05T23:43:05.929641: step 7153, loss 0.537653.
Train: 2018-08-05T23:43:06.101475: step 7154, loss 0.496068.
Train: 2018-08-05T23:43:06.257688: step 7155, loss 0.595838.
Train: 2018-08-05T23:43:06.413902: step 7156, loss 0.537462.
Train: 2018-08-05T23:43:06.585766: step 7157, loss 0.595965.
Train: 2018-08-05T23:43:06.741975: step 7158, loss 0.587608.
Train: 2018-08-05T23:43:06.898164: step 7159, loss 0.554047.
Train: 2018-08-05T23:43:07.071097: step 7160, loss 0.520436.
Test: 2018-08-05T23:43:07.302336: step 7160, loss 0.548339.
Train: 2018-08-05T23:43:07.458545: step 7161, loss 0.554019.
Train: 2018-08-05T23:43:07.614763: step 7162, loss 0.545552.
Train: 2018-08-05T23:43:07.786588: step 7163, loss 0.545473.
Train: 2018-08-05T23:43:07.948272: step 7164, loss 0.553938.
Train: 2018-08-05T23:43:08.110630: step 7165, loss 0.562383.
Train: 2018-08-05T23:43:08.269677: step 7166, loss 0.562362.
Train: 2018-08-05T23:43:08.425890: step 7167, loss 0.54528.
Train: 2018-08-05T23:43:08.582104: step 7168, loss 0.605281.
Train: 2018-08-05T23:43:08.738316: step 7169, loss 0.493777.
Train: 2018-08-05T23:43:08.894529: step 7170, loss 0.596779.
Test: 2018-08-05T23:43:09.144471: step 7170, loss 0.547974.
Train: 2018-08-05T23:43:09.300717: step 7171, loss 0.622688.
Train: 2018-08-05T23:43:09.456898: step 7172, loss 0.639963.
Train: 2018-08-05T23:43:09.613111: step 7173, loss 0.553826.
Train: 2018-08-05T23:43:09.769324: step 7174, loss 0.545241.
Train: 2018-08-05T23:43:09.925571: step 7175, loss 0.562368.
Train: 2018-08-05T23:43:10.097371: step 7176, loss 0.57101.
Train: 2018-08-05T23:43:10.253585: step 7177, loss 0.54524.
Train: 2018-08-05T23:43:10.409798: step 7178, loss 0.622371.
Train: 2018-08-05T23:43:10.566011: step 7179, loss 0.587957.
Train: 2018-08-05T23:43:10.737846: step 7180, loss 0.502736.
Test: 2018-08-05T23:43:10.972166: step 7180, loss 0.548127.
Train: 2018-08-05T23:43:11.128409: step 7181, loss 0.519749.
Train: 2018-08-05T23:43:11.284593: step 7182, loss 0.553892.
Train: 2018-08-05T23:43:11.440836: step 7183, loss 0.528214.
Train: 2018-08-05T23:43:11.597049: step 7184, loss 0.528193.
Train: 2018-08-05T23:43:11.753257: step 7185, loss 0.648103.
Train: 2018-08-05T23:43:11.909445: step 7186, loss 0.605146.
Train: 2018-08-05T23:43:12.081311: step 7187, loss 0.545258.
Train: 2018-08-05T23:43:12.237494: step 7188, loss 0.519657.
Train: 2018-08-05T23:43:12.393707: step 7189, loss 0.545331.
Train: 2018-08-05T23:43:12.549953: step 7190, loss 0.57093.
Test: 2018-08-05T23:43:12.784247: step 7190, loss 0.548042.
Train: 2018-08-05T23:43:12.949978: step 7191, loss 0.588116.
Train: 2018-08-05T23:43:13.106221: step 7192, loss 0.545262.
Train: 2018-08-05T23:43:13.262404: step 7193, loss 0.493798.
Train: 2018-08-05T23:43:13.418617: step 7194, loss 0.536664.
Train: 2018-08-05T23:43:13.574866: step 7195, loss 0.60556.
Train: 2018-08-05T23:43:13.746689: step 7196, loss 0.48487.
Train: 2018-08-05T23:43:13.902878: step 7197, loss 0.493146.
Train: 2018-08-05T23:43:14.063826: step 7198, loss 0.588578.
Train: 2018-08-05T23:43:14.220040: step 7199, loss 0.545099.
Train: 2018-08-05T23:43:14.376253: step 7200, loss 0.562445.
Test: 2018-08-05T23:43:14.610604: step 7200, loss 0.54779.
Train: 2018-08-05T23:43:15.344776: step 7201, loss 0.544998.
Train: 2018-08-05T23:43:15.516610: step 7202, loss 0.492305.
Train: 2018-08-05T23:43:15.677410: step 7203, loss 0.641664.
Train: 2018-08-05T23:43:15.836763: step 7204, loss 0.597761.
Train: 2018-08-05T23:43:15.992975: step 7205, loss 0.500835.
Train: 2018-08-05T23:43:16.149159: step 7206, loss 0.50075.
Train: 2018-08-05T23:43:16.305372: step 7207, loss 0.571361.
Train: 2018-08-05T23:43:16.461586: step 7208, loss 0.553681.
Train: 2018-08-05T23:43:16.617798: step 7209, loss 0.571389.
Train: 2018-08-05T23:43:16.774012: step 7210, loss 0.553662.
Test: 2018-08-05T23:43:17.009126: step 7210, loss 0.547652.
Train: 2018-08-05T23:43:17.180991: step 7211, loss 0.544775.
Train: 2018-08-05T23:43:17.337174: step 7212, loss 0.607105.
Train: 2018-08-05T23:43:17.493418: step 7213, loss 0.580362.
Train: 2018-08-05T23:43:17.649631: step 7214, loss 0.518063.
Train: 2018-08-05T23:43:17.821435: step 7215, loss 0.491338.
Train: 2018-08-05T23:43:17.981436: step 7216, loss 0.56258.
Train: 2018-08-05T23:43:18.137650: step 7217, loss 0.544722.
Train: 2018-08-05T23:43:18.293401: step 7218, loss 0.544706.
Train: 2018-08-05T23:43:18.449645: step 7219, loss 0.562608.
Train: 2018-08-05T23:43:18.621479: step 7220, loss 0.634319.
Test: 2018-08-05T23:43:18.855801: step 7220, loss 0.547616.
Train: 2018-08-05T23:43:19.011983: step 7221, loss 0.508896.
Train: 2018-08-05T23:43:19.168195: step 7222, loss 0.544729.
Train: 2018-08-05T23:43:19.340030: step 7223, loss 0.526774.
Train: 2018-08-05T23:43:19.496243: step 7224, loss 0.508865.
Train: 2018-08-05T23:43:19.652456: step 7225, loss 0.571587.
Train: 2018-08-05T23:43:19.808669: step 7226, loss 0.481719.
Train: 2018-08-05T23:43:19.964914: step 7227, loss 0.580638.
Train: 2018-08-05T23:43:20.136717: step 7228, loss 0.589963.
Train: 2018-08-05T23:43:20.292968: step 7229, loss 0.562871.
Train: 2018-08-05T23:43:20.449145: step 7230, loss 0.517315.
Test: 2018-08-05T23:43:20.683496: step 7230, loss 0.547575.
Train: 2018-08-05T23:43:20.855299: step 7231, loss 0.589822.
Train: 2018-08-05T23:43:21.011512: step 7232, loss 0.580802.
Train: 2018-08-05T23:43:21.167726: step 7233, loss 0.608115.
Train: 2018-08-05T23:43:21.323968: step 7234, loss 0.580742.
Train: 2018-08-05T23:43:21.480182: step 7235, loss 0.625912.
Train: 2018-08-05T23:43:21.652016: step 7236, loss 0.526795.
Train: 2018-08-05T23:43:21.808231: step 7237, loss 0.59831.
Train: 2018-08-05T23:43:21.964443: step 7238, loss 0.535845.
Train: 2018-08-05T23:43:22.120627: step 7239, loss 0.527014.
Train: 2018-08-05T23:43:22.276876: step 7240, loss 0.597985.
Test: 2018-08-05T23:43:22.511191: step 7240, loss 0.547694.
Train: 2018-08-05T23:43:22.682994: step 7241, loss 0.500603.
Train: 2018-08-05T23:43:22.839237: step 7242, loss 0.589012.
Train: 2018-08-05T23:43:22.999670: step 7243, loss 0.553709.
Train: 2018-08-05T23:43:23.171480: step 7244, loss 0.580056.
Train: 2018-08-05T23:43:23.312102: step 7245, loss 0.483288.
Train: 2018-08-05T23:43:23.483936: step 7246, loss 0.544922.
Train: 2018-08-05T23:43:23.640120: step 7247, loss 0.562479.
Train: 2018-08-05T23:43:23.780736: step 7248, loss 0.71275.
Train: 2018-08-05T23:43:23.950523: step 7249, loss 0.518601.
Train: 2018-08-05T23:43:24.106726: step 7250, loss 0.501264.
Test: 2018-08-05T23:43:24.341053: step 7250, loss 0.547797.
Train: 2018-08-05T23:43:24.497234: step 7251, loss 0.588597.
Train: 2018-08-05T23:43:24.669101: step 7252, loss 0.61482.
Train: 2018-08-05T23:43:24.825312: step 7253, loss 0.588533.
Train: 2018-08-05T23:43:24.983854: step 7254, loss 0.518976.
Train: 2018-08-05T23:43:25.140038: step 7255, loss 0.545032.
Train: 2018-08-05T23:43:25.296275: step 7256, loss 0.501807.
Train: 2018-08-05T23:43:25.452463: step 7257, loss 0.467145.
Train: 2018-08-05T23:43:25.608676: step 7258, loss 0.562357.
Train: 2018-08-05T23:43:25.764922: step 7259, loss 0.527671.
Train: 2018-08-05T23:43:25.936754: step 7260, loss 0.53624.
Test: 2018-08-05T23:43:26.171077: step 7260, loss 0.547788.
Train: 2018-08-05T23:43:26.342903: step 7261, loss 0.623678.
Train: 2018-08-05T23:43:26.499125: step 7262, loss 0.562562.
Train: 2018-08-05T23:43:26.655305: step 7263, loss 0.597372.
Train: 2018-08-05T23:43:26.811519: step 7264, loss 0.474808.
Train: 2018-08-05T23:43:26.965710: step 7265, loss 0.509768.
Train: 2018-08-05T23:43:27.137521: step 7266, loss 0.589113.
Train: 2018-08-05T23:43:27.293733: step 7267, loss 0.615408.
Train: 2018-08-05T23:43:27.465568: step 7268, loss 0.518418.
Train: 2018-08-05T23:43:27.621781: step 7269, loss 0.500888.
Train: 2018-08-05T23:43:27.793640: step 7270, loss 0.589146.
Test: 2018-08-05T23:43:28.033420: step 7270, loss 0.547686.
Train: 2018-08-05T23:43:28.189631: step 7271, loss 0.59784.
Train: 2018-08-05T23:43:28.345845: step 7272, loss 0.56248.
Train: 2018-08-05T23:43:28.502088: step 7273, loss 0.580126.
Train: 2018-08-05T23:43:28.673894: step 7274, loss 0.580037.
Train: 2018-08-05T23:43:28.830136: step 7275, loss 0.518294.
Train: 2018-08-05T23:43:28.986349: step 7276, loss 0.686198.
Train: 2018-08-05T23:43:29.142533: step 7277, loss 0.580487.
Train: 2018-08-05T23:43:29.298775: step 7278, loss 0.518512.
Train: 2018-08-05T23:43:29.454989: step 7279, loss 0.553664.
Train: 2018-08-05T23:43:29.611202: step 7280, loss 0.55366.
Test: 2018-08-05T23:43:29.845492: step 7280, loss 0.547801.
Train: 2018-08-05T23:43:30.017327: step 7281, loss 0.56219.
Train: 2018-08-05T23:43:30.189161: step 7282, loss 0.536324.
Train: 2018-08-05T23:43:30.345373: step 7283, loss 0.571253.
Train: 2018-08-05T23:43:30.517208: step 7284, loss 0.5536.
Train: 2018-08-05T23:43:30.673421: step 7285, loss 0.579824.
Train: 2018-08-05T23:43:30.829660: step 7286, loss 0.53642.
Train: 2018-08-05T23:43:30.985878: step 7287, loss 0.527359.
Train: 2018-08-05T23:43:31.157713: step 7288, loss 0.606604.
Train: 2018-08-05T23:43:31.313927: step 7289, loss 0.562115.
Train: 2018-08-05T23:43:31.470140: step 7290, loss 0.57989.
Test: 2018-08-05T23:43:31.704461: step 7290, loss 0.547871.
Train: 2018-08-05T23:43:31.876294: step 7291, loss 0.579752.
Train: 2018-08-05T23:43:32.032476: step 7292, loss 0.605935.
Train: 2018-08-05T23:43:32.188690: step 7293, loss 0.536525.
Train: 2018-08-05T23:43:32.344904: step 7294, loss 0.510934.
Train: 2018-08-05T23:43:32.501147: step 7295, loss 0.562435.
Train: 2018-08-05T23:43:32.657360: step 7296, loss 0.493688.
Train: 2018-08-05T23:43:32.829189: step 7297, loss 0.622606.
Train: 2018-08-05T23:43:32.985377: step 7298, loss 0.528032.
Train: 2018-08-05T23:43:33.141621: step 7299, loss 0.545223.
Train: 2018-08-05T23:43:33.297805: step 7300, loss 0.571021.
Test: 2018-08-05T23:43:33.547746: step 7300, loss 0.547979.
Train: 2018-08-05T23:43:34.265861: step 7301, loss 0.570999.
Train: 2018-08-05T23:43:34.422046: step 7302, loss 0.553781.
Train: 2018-08-05T23:43:34.578260: step 7303, loss 0.536558.
Train: 2018-08-05T23:43:34.750092: step 7304, loss 0.571043.
Train: 2018-08-05T23:43:34.909717: step 7305, loss 0.562382.
Train: 2018-08-05T23:43:35.065931: step 7306, loss 0.571056.
Train: 2018-08-05T23:43:35.222144: step 7307, loss 0.519233.
Train: 2018-08-05T23:43:35.388974: step 7308, loss 0.579718.
Train: 2018-08-05T23:43:35.529571: step 7309, loss 0.536447.
Train: 2018-08-05T23:43:35.701405: step 7310, loss 0.536397.
Test: 2018-08-05T23:43:35.938272: step 7310, loss 0.547879.
Train: 2018-08-05T23:43:36.110129: step 7311, loss 0.614451.
Train: 2018-08-05T23:43:36.281939: step 7312, loss 0.562425.
Train: 2018-08-05T23:43:36.438183: step 7313, loss 0.510342.
Train: 2018-08-05T23:43:36.594366: step 7314, loss 0.536332.
Train: 2018-08-05T23:43:36.750610: step 7315, loss 0.54502.
Train: 2018-08-05T23:43:36.922024: step 7316, loss 0.606074.
Train: 2018-08-05T23:43:37.078267: step 7317, loss 0.553662.
Train: 2018-08-05T23:43:37.234484: step 7318, loss 0.571204.
Train: 2018-08-05T23:43:37.390695: step 7319, loss 0.562452.
Train: 2018-08-05T23:43:37.546907: step 7320, loss 0.544992.
Test: 2018-08-05T23:43:37.786880: step 7320, loss 0.547807.
Train: 2018-08-05T23:43:37.941251: step 7321, loss 0.562454.
Train: 2018-08-05T23:43:38.113080: step 7322, loss 0.518757.
Train: 2018-08-05T23:43:38.269299: step 7323, loss 0.510022.
Train: 2018-08-05T23:43:38.425495: step 7324, loss 0.571286.
Train: 2018-08-05T23:43:38.597341: step 7325, loss 0.571226.
Train: 2018-08-05T23:43:38.754355: step 7326, loss 0.553739.
Train: 2018-08-05T23:43:38.913931: step 7327, loss 0.544908.
Train: 2018-08-05T23:43:39.071160: step 7328, loss 0.562463.
Train: 2018-08-05T23:43:39.227405: step 7329, loss 0.544846.
Train: 2018-08-05T23:43:39.383587: step 7330, loss 0.571316.
Test: 2018-08-05T23:43:39.617938: step 7330, loss 0.547709.
Train: 2018-08-05T23:43:39.789741: step 7331, loss 0.571324.
Train: 2018-08-05T23:43:39.945955: step 7332, loss 0.500672.
Train: 2018-08-05T23:43:40.117819: step 7333, loss 0.606733.
Train: 2018-08-05T23:43:40.274033: step 7334, loss 0.615539.
Train: 2018-08-05T23:43:40.430246: step 7335, loss 0.589004.
Train: 2018-08-05T23:43:40.586460: step 7336, loss 0.536041.
Train: 2018-08-05T23:43:40.758271: step 7337, loss 0.55367.
Train: 2018-08-05T23:43:40.914477: step 7338, loss 0.571274.
Train: 2018-08-05T23:43:41.070690: step 7339, loss 0.606396.
Train: 2018-08-05T23:43:41.226903: step 7340, loss 0.597518.
Test: 2018-08-05T23:43:41.461256: step 7340, loss 0.547794.
Train: 2018-08-05T23:43:41.633082: step 7341, loss 0.562437.
Train: 2018-08-05T23:43:41.789271: step 7342, loss 0.597307.
Train: 2018-08-05T23:43:41.945517: step 7343, loss 0.571108.
Train: 2018-08-05T23:43:42.101728: step 7344, loss 0.588403.
Train: 2018-08-05T23:43:42.273557: step 7345, loss 0.545133.
Train: 2018-08-05T23:43:42.429776: step 7346, loss 0.527945.
Train: 2018-08-05T23:43:42.585959: step 7347, loss 0.519396.
Train: 2018-08-05T23:43:42.742173: step 7348, loss 0.536624.
Train: 2018-08-05T23:43:42.898385: step 7349, loss 0.467883.
Train: 2018-08-05T23:43:43.054598: step 7350, loss 0.57961.
Test: 2018-08-05T23:43:43.288951: step 7350, loss 0.54796.
Train: 2018-08-05T23:43:43.460784: step 7351, loss 0.605493.
Train: 2018-08-05T23:43:43.616997: step 7352, loss 0.605534.
Train: 2018-08-05T23:43:43.773180: step 7353, loss 0.527928.
Train: 2018-08-05T23:43:43.928166: step 7354, loss 0.571025.
Train: 2018-08-05T23:43:44.100002: step 7355, loss 0.562404.
Train: 2018-08-05T23:43:44.256245: step 7356, loss 0.579637.
Train: 2018-08-05T23:43:44.412459: step 7357, loss 0.605488.
Train: 2018-08-05T23:43:44.568671: step 7358, loss 0.52794.
Train: 2018-08-05T23:43:44.724884: step 7359, loss 0.562346.
Train: 2018-08-05T23:43:44.896689: step 7360, loss 0.56242.
Test: 2018-08-05T23:43:45.135439: step 7360, loss 0.54802.
Train: 2018-08-05T23:43:45.291681: step 7361, loss 0.596738.
Train: 2018-08-05T23:43:45.447864: step 7362, loss 0.553793.
Train: 2018-08-05T23:43:45.604107: step 7363, loss 0.476711.
Train: 2018-08-05T23:43:45.775936: step 7364, loss 0.596689.
Train: 2018-08-05T23:43:45.932150: step 7365, loss 0.553812.
Train: 2018-08-05T23:43:46.095513: step 7366, loss 0.639631.
Train: 2018-08-05T23:43:46.250337: step 7367, loss 0.545324.
Train: 2018-08-05T23:43:46.406580: step 7368, loss 0.562233.
Train: 2018-08-05T23:43:46.562796: step 7369, loss 0.579449.
Train: 2018-08-05T23:43:46.719007: step 7370, loss 0.604992.
Test: 2018-08-05T23:43:46.953304: step 7370, loss 0.548154.
Train: 2018-08-05T23:43:47.125131: step 7371, loss 0.621928.
Train: 2018-08-05T23:43:47.281375: step 7372, loss 0.579565.
Train: 2018-08-05T23:43:47.437588: step 7373, loss 0.554106.
Train: 2018-08-05T23:43:47.609392: step 7374, loss 0.529037.
Train: 2018-08-05T23:43:47.765637: step 7375, loss 0.570711.
Train: 2018-08-05T23:43:47.928228: step 7376, loss 0.537702.
Train: 2018-08-05T23:43:48.084441: step 7377, loss 0.520287.
Train: 2018-08-05T23:43:48.240653: step 7378, loss 0.570649.
Train: 2018-08-05T23:43:48.381251: step 7379, loss 0.553812.
Train: 2018-08-05T23:43:48.553086: step 7380, loss 0.545371.
Test: 2018-08-05T23:43:48.787379: step 7380, loss 0.54832.
Train: 2018-08-05T23:43:48.957196: step 7381, loss 0.621311.
Train: 2018-08-05T23:43:49.113411: step 7382, loss 0.528839.
Train: 2018-08-05T23:43:49.269658: step 7383, loss 0.520057.
Train: 2018-08-05T23:43:49.425837: step 7384, loss 0.596166.
Train: 2018-08-05T23:43:49.582083: step 7385, loss 0.537028.
Train: 2018-08-05T23:43:49.738294: step 7386, loss 0.680852.
Train: 2018-08-05T23:43:49.894507: step 7387, loss 0.621609.
Train: 2018-08-05T23:43:50.087067: step 7388, loss 0.663502.
Train: 2018-08-05T23:43:50.243280: step 7389, loss 0.579229.
Train: 2018-08-05T23:43:50.415085: step 7390, loss 0.612379.
Test: 2018-08-05T23:43:50.649405: step 7390, loss 0.548805.
Train: 2018-08-05T23:43:50.805618: step 7391, loss 0.464669.
Train: 2018-08-05T23:43:50.977483: step 7392, loss 0.5797.
Train: 2018-08-05T23:43:51.141281: step 7393, loss 0.554118.
Train: 2018-08-05T23:43:51.297494: step 7394, loss 0.529554.
Train: 2018-08-05T23:43:51.453708: step 7395, loss 0.579434.
Train: 2018-08-05T23:43:51.609890: step 7396, loss 0.530316.
Train: 2018-08-05T23:43:51.781724: step 7397, loss 0.570963.
Train: 2018-08-05T23:43:51.937938: step 7398, loss 0.579546.
Train: 2018-08-05T23:43:52.094151: step 7399, loss 0.562662.
Train: 2018-08-05T23:43:52.250364: step 7400, loss 0.57066.
Test: 2018-08-05T23:43:52.484685: step 7400, loss 0.548603.
Train: 2018-08-05T23:43:53.250160: step 7401, loss 0.537608.
Train: 2018-08-05T23:43:53.406375: step 7402, loss 0.495856.
Train: 2018-08-05T23:43:53.578214: step 7403, loss 0.520752.
Train: 2018-08-05T23:43:53.734421: step 7404, loss 0.604543.
Train: 2018-08-05T23:43:53.906444: step 7405, loss 0.520353.
Train: 2018-08-05T23:43:54.056123: step 7406, loss 0.579334.
Train: 2018-08-05T23:43:54.227953: step 7407, loss 0.503106.
Train: 2018-08-05T23:43:54.399758: step 7408, loss 0.545396.
Train: 2018-08-05T23:43:54.556002: step 7409, loss 0.579473.
Train: 2018-08-05T23:43:54.727804: step 7410, loss 0.51958.
Test: 2018-08-05T23:43:54.960173: step 7410, loss 0.548.
Train: 2018-08-05T23:43:55.132031: step 7411, loss 0.579569.
Train: 2018-08-05T23:43:55.288220: step 7412, loss 0.545156.
Train: 2018-08-05T23:43:55.460084: step 7413, loss 0.467335.
Train: 2018-08-05T23:43:55.616298: step 7414, loss 0.54507.
Train: 2018-08-05T23:43:55.772480: step 7415, loss 0.597385.
Train: 2018-08-05T23:43:55.948837: step 7416, loss 0.57112.
Train: 2018-08-05T23:43:56.105050: step 7417, loss 0.518629.
Train: 2018-08-05T23:43:56.276899: step 7418, loss 0.500845.
Train: 2018-08-05T23:43:56.433127: step 7419, loss 0.518265.
Train: 2018-08-05T23:43:56.604944: step 7420, loss 0.58034.
Test: 2018-08-05T23:43:56.839252: step 7420, loss 0.547634.
Train: 2018-08-05T23:43:57.011117: step 7421, loss 0.544704.
Train: 2018-08-05T23:43:57.167300: step 7422, loss 0.571657.
Train: 2018-08-05T23:43:57.323544: step 7423, loss 0.571634.
Train: 2018-08-05T23:43:57.495349: step 7424, loss 0.553695.
Train: 2018-08-05T23:43:57.651561: step 7425, loss 0.544752.
Train: 2018-08-05T23:43:57.807774: step 7426, loss 0.508626.
Train: 2018-08-05T23:43:57.963987: step 7427, loss 0.508452.
Train: 2018-08-05T23:43:58.135822: step 7428, loss 0.571868.
Train: 2018-08-05T23:43:58.292065: step 7429, loss 0.508057.
Train: 2018-08-05T23:43:58.463900: step 7430, loss 0.599808.
Test: 2018-08-05T23:43:58.698190: step 7430, loss 0.547569.
Train: 2018-08-05T23:43:58.854435: step 7431, loss 0.526714.
Train: 2018-08-05T23:43:59.025181: step 7432, loss 0.535325.
Train: 2018-08-05T23:43:59.181395: step 7433, loss 0.507903.
Train: 2018-08-05T23:43:59.337640: step 7434, loss 0.682293.
Train: 2018-08-05T23:43:59.493821: step 7435, loss 0.581343.
Train: 2018-08-05T23:43:59.665680: step 7436, loss 0.608622.
Train: 2018-08-05T23:43:59.821897: step 7437, loss 0.544604.
Train: 2018-08-05T23:43:59.998139: step 7438, loss 0.535528.
Train: 2018-08-05T23:44:00.154365: step 7439, loss 0.590175.
Train: 2018-08-05T23:44:00.310576: step 7440, loss 0.589988.
Test: 2018-08-05T23:44:00.560512: step 7440, loss 0.547579.
Train: 2018-08-05T23:44:00.716730: step 7441, loss 0.481342.
Train: 2018-08-05T23:44:00.902169: step 7442, loss 0.598825.
Train: 2018-08-05T23:44:01.079077: step 7443, loss 0.553665.
Train: 2018-08-05T23:44:01.235319: step 7444, loss 0.589652.
Train: 2018-08-05T23:44:01.391502: step 7445, loss 0.580639.
Train: 2018-08-05T23:44:01.545607: step 7446, loss 0.598525.
Train: 2018-08-05T23:44:01.733072: step 7447, loss 0.56265.
Train: 2018-08-05T23:44:01.904895: step 7448, loss 0.535895.
Train: 2018-08-05T23:44:02.107647: step 7449, loss 0.52706.
Train: 2018-08-05T23:44:02.310714: step 7450, loss 0.518248.
Test: 2018-08-05T23:44:02.545035: step 7450, loss 0.547693.
Train: 2018-08-05T23:44:02.826218: step 7451, loss 0.615583.
Train: 2018-08-05T23:44:03.013675: step 7452, loss 0.571345.
Train: 2018-08-05T23:44:03.189300: step 7453, loss 0.544851.
Train: 2018-08-05T23:44:03.345512: step 7454, loss 0.615116.
Train: 2018-08-05T23:44:03.519717: step 7455, loss 0.597377.
Train: 2018-08-05T23:44:03.675931: step 7456, loss 0.588661.
Train: 2018-08-05T23:44:03.879009: step 7457, loss 0.536385.
Train: 2018-08-05T23:44:04.035222: step 7458, loss 0.56248.
Train: 2018-08-05T23:44:04.191465: step 7459, loss 0.579553.
Train: 2018-08-05T23:44:04.363301: step 7460, loss 0.5281.
Test: 2018-08-05T23:44:04.587522: step 7460, loss 0.548011.
Train: 2018-08-05T23:44:04.759357: step 7461, loss 0.553553.
Train: 2018-08-05T23:44:04.926752: step 7462, loss 0.596811.
Train: 2018-08-05T23:44:05.072387: step 7463, loss 0.647837.
Train: 2018-08-05T23:44:05.244222: step 7464, loss 0.56263.
Train: 2018-08-05T23:44:05.400466: step 7465, loss 0.511157.
Train: 2018-08-05T23:44:05.572303: step 7466, loss 0.545681.
Train: 2018-08-05T23:44:05.728513: step 7467, loss 0.604485.
Train: 2018-08-05T23:44:05.879930: step 7468, loss 0.554276.
Train: 2018-08-05T23:44:06.044821: step 7469, loss 0.538074.
Train: 2018-08-05T23:44:06.201064: step 7470, loss 0.537499.
Test: 2018-08-05T23:44:06.450974: step 7470, loss 0.54835.
Train: 2018-08-05T23:44:06.607220: step 7471, loss 0.52874.
Train: 2018-08-05T23:44:06.771963: step 7472, loss 0.545545.
Train: 2018-08-05T23:44:06.943767: step 7473, loss 0.519978.
Train: 2018-08-05T23:44:07.115603: step 7474, loss 0.545499.
Train: 2018-08-05T23:44:07.271816: step 7475, loss 0.587943.
Train: 2018-08-05T23:44:07.443680: step 7476, loss 0.528195.
Train: 2018-08-05T23:44:07.615515: step 7477, loss 0.58806.
Train: 2018-08-05T23:44:07.771728: step 7478, loss 0.562368.
Train: 2018-08-05T23:44:07.927911: step 7479, loss 0.502192.
Train: 2018-08-05T23:44:08.099745: step 7480, loss 0.5624.
Test: 2018-08-05T23:44:08.347537: step 7480, loss 0.547937.
Train: 2018-08-05T23:44:08.520579: step 7481, loss 0.605554.
Train: 2018-08-05T23:44:08.801754: step 7482, loss 0.519148.
Train: 2018-08-05T23:44:08.996601: step 7483, loss 0.605659.
Train: 2018-08-05T23:44:09.168435: step 7484, loss 0.536422.
Train: 2018-08-05T23:44:09.340269: step 7485, loss 0.579776.
Train: 2018-08-05T23:44:09.503733: step 7486, loss 0.562494.
Train: 2018-08-05T23:44:09.659981: step 7487, loss 0.510311.
Train: 2018-08-05T23:44:09.878646: step 7488, loss 0.536307.
Train: 2018-08-05T23:44:10.049143: step 7489, loss 0.588436.
Train: 2018-08-05T23:44:10.205326: step 7490, loss 0.606058.
Test: 2018-08-05T23:44:10.439648: step 7490, loss 0.547816.
Train: 2018-08-05T23:44:10.627101: step 7491, loss 0.5975.
Train: 2018-08-05T23:44:10.798967: step 7492, loss 0.597601.
Train: 2018-08-05T23:44:10.959682: step 7493, loss 0.501529.
Train: 2018-08-05T23:44:11.131554: step 7494, loss 0.536518.
Train: 2018-08-05T23:44:11.287731: step 7495, loss 0.579829.
Train: 2018-08-05T23:44:11.459565: step 7496, loss 0.518939.
Train: 2018-08-05T23:44:11.615777: step 7497, loss 0.640546.
Train: 2018-08-05T23:44:11.787643: step 7498, loss 0.518937.
Train: 2018-08-05T23:44:11.943856: step 7499, loss 0.527796.
Train: 2018-08-05T23:44:12.115690: step 7500, loss 0.562492.
Test: 2018-08-05T23:44:12.350019: step 7500, loss 0.547863.
Train: 2018-08-05T23:44:13.068591: step 7501, loss 0.553648.
Train: 2018-08-05T23:44:13.224774: step 7502, loss 0.536196.
Train: 2018-08-05T23:44:13.396617: step 7503, loss 0.649518.
Train: 2018-08-05T23:44:13.552821: step 7504, loss 0.623231.
Train: 2018-08-05T23:44:13.709035: step 7505, loss 0.484333.
Train: 2018-08-05T23:44:13.865248: step 7506, loss 0.605659.
Train: 2018-08-05T23:44:14.021493: step 7507, loss 0.553811.
Train: 2018-08-05T23:44:14.193327: step 7508, loss 0.571023.
Train: 2018-08-05T23:44:14.365165: step 7509, loss 0.570931.
Train: 2018-08-05T23:44:14.521376: step 7510, loss 0.562451.
Test: 2018-08-05T23:44:14.771285: step 7510, loss 0.548007.
Train: 2018-08-05T23:44:14.919265: step 7511, loss 0.613966.
Train: 2018-08-05T23:44:15.091130: step 7512, loss 0.622492.
Train: 2018-08-05T23:44:15.247343: step 7513, loss 0.570853.
Train: 2018-08-05T23:44:15.403557: step 7514, loss 0.502669.
Train: 2018-08-05T23:44:15.575361: step 7515, loss 0.587887.
Train: 2018-08-05T23:44:15.731605: step 7516, loss 0.638633.
Train: 2018-08-05T23:44:15.887817: step 7517, loss 0.570936.
Train: 2018-08-05T23:44:16.042340: step 7518, loss 0.562347.
Train: 2018-08-05T23:44:16.198553: step 7519, loss 0.503745.
Train: 2018-08-05T23:44:16.386009: step 7520, loss 0.56228.
Test: 2018-08-05T23:44:16.616408: step 7520, loss 0.548454.
Train: 2018-08-05T23:44:16.788242: step 7521, loss 0.663083.
Train: 2018-08-05T23:44:16.937281: step 7522, loss 0.603926.
Train: 2018-08-05T23:44:17.109090: step 7523, loss 0.52892.
Train: 2018-08-05T23:44:17.265299: step 7524, loss 0.538012.
Train: 2018-08-05T23:44:17.437135: step 7525, loss 0.537878.
Train: 2018-08-05T23:44:17.593377: step 7526, loss 0.562581.
Train: 2018-08-05T23:44:17.765213: step 7527, loss 0.604394.
Train: 2018-08-05T23:44:17.937018: step 7528, loss 0.554477.
Train: 2018-08-05T23:44:18.108851: step 7529, loss 0.587293.
Train: 2018-08-05T23:44:18.265064: step 7530, loss 0.636822.
Test: 2018-08-05T23:44:18.515036: step 7530, loss 0.548791.
Train: 2018-08-05T23:44:18.718116: step 7531, loss 0.603873.
Train: 2018-08-05T23:44:18.889943: step 7532, loss 0.554166.
Train: 2018-08-05T23:44:19.046132: step 7533, loss 0.52939.
Train: 2018-08-05T23:44:19.233587: step 7534, loss 0.652792.
Train: 2018-08-05T23:44:19.405423: step 7535, loss 0.586994.
Train: 2018-08-05T23:44:19.577280: step 7536, loss 0.562591.
Train: 2018-08-05T23:44:19.764711: step 7537, loss 0.611183.
Train: 2018-08-05T23:44:19.933623: step 7538, loss 0.602845.
Train: 2018-08-05T23:44:20.106526: step 7539, loss 0.538425.
Train: 2018-08-05T23:44:20.262739: step 7540, loss 0.554424.
Test: 2018-08-05T23:44:20.512681: step 7540, loss 0.549532.
Train: 2018-08-05T23:44:20.668894: step 7541, loss 0.507336.
Train: 2018-08-05T23:44:20.840728: step 7542, loss 0.554991.
Train: 2018-08-05T23:44:21.001550: step 7543, loss 0.618486.
Train: 2018-08-05T23:44:21.173415: step 7544, loss 0.570954.
Train: 2018-08-05T23:44:21.345219: step 7545, loss 0.514517.
Train: 2018-08-05T23:44:21.501463: step 7546, loss 0.587016.
Train: 2018-08-05T23:44:21.673267: step 7547, loss 0.522887.
Train: 2018-08-05T23:44:21.829480: step 7548, loss 0.5467.
Train: 2018-08-05T23:44:21.985724: step 7549, loss 0.530527.
Train: 2018-08-05T23:44:22.141931: step 7550, loss 0.51111.
Test: 2018-08-05T23:44:22.376227: step 7550, loss 0.548968.
Train: 2018-08-05T23:44:22.548060: step 7551, loss 0.529923.
Train: 2018-08-05T23:44:22.704275: step 7552, loss 0.546132.
Train: 2018-08-05T23:44:22.860516: step 7553, loss 0.521376.
Train: 2018-08-05T23:44:23.032323: step 7554, loss 0.512248.
Train: 2018-08-05T23:44:23.188537: step 7555, loss 0.605583.
Train: 2018-08-05T23:44:23.360403: step 7556, loss 0.562316.
Train: 2018-08-05T23:44:23.532204: step 7557, loss 0.528332.
Train: 2018-08-05T23:44:23.704039: step 7558, loss 0.545329.
Train: 2018-08-05T23:44:23.875874: step 7559, loss 0.570882.
Train: 2018-08-05T23:44:24.063819: step 7560, loss 0.580456.
Test: 2018-08-05T23:44:24.298140: step 7560, loss 0.547802.
Train: 2018-08-05T23:44:24.501217: step 7561, loss 0.544935.
Train: 2018-08-05T23:44:24.688673: step 7562, loss 0.527237.
Train: 2018-08-05T23:44:24.860508: step 7563, loss 0.589069.
Train: 2018-08-05T23:44:25.032341: step 7564, loss 0.563203.
Train: 2018-08-05T23:44:25.219797: step 7565, loss 0.588979.
Train: 2018-08-05T23:44:25.407253: step 7566, loss 0.527067.
Train: 2018-08-05T23:44:25.594709: step 7567, loss 0.51852.
Train: 2018-08-05T23:44:25.750922: step 7568, loss 0.562503.
Train: 2018-08-05T23:44:25.920430: step 7569, loss 0.571399.
Train: 2018-08-05T23:44:26.092265: step 7570, loss 0.562588.
Test: 2018-08-05T23:44:26.326585: step 7570, loss 0.547682.
Train: 2018-08-05T23:44:26.498419: step 7571, loss 0.580355.
Train: 2018-08-05T23:44:26.670285: step 7572, loss 0.606844.
Train: 2018-08-05T23:44:26.842089: step 7573, loss 0.597951.
Train: 2018-08-05T23:44:26.996319: step 7574, loss 0.580213.
Train: 2018-08-05T23:44:27.168129: step 7575, loss 0.580143.
Train: 2018-08-05T23:44:27.339962: step 7576, loss 0.562335.
Train: 2018-08-05T23:44:27.511798: step 7577, loss 0.579762.
Train: 2018-08-05T23:44:27.683633: step 7578, loss 0.571242.
Train: 2018-08-05T23:44:27.871087: step 7579, loss 0.53627.
Train: 2018-08-05T23:44:28.047662: step 7580, loss 0.475876.
Test: 2018-08-05T23:44:28.297601: step 7580, loss 0.547923.
Train: 2018-08-05T23:44:28.453815: step 7581, loss 0.536636.
Train: 2018-08-05T23:44:28.625674: step 7582, loss 0.545406.
Train: 2018-08-05T23:44:28.813105: step 7583, loss 0.640421.
Train: 2018-08-05T23:44:28.984940: step 7584, loss 0.527617.
Train: 2018-08-05T23:44:29.156800: step 7585, loss 0.580156.
Train: 2018-08-05T23:44:29.328609: step 7586, loss 0.623095.
Train: 2018-08-05T23:44:29.500444: step 7587, loss 0.5188.
Train: 2018-08-05T23:44:29.672278: step 7588, loss 0.545095.
Train: 2018-08-05T23:44:29.844113: step 7589, loss 0.691955.
Train: 2018-08-05T23:44:30.000039: step 7590, loss 0.605811.
Test: 2018-08-05T23:44:30.249981: step 7590, loss 0.548112.
Train: 2018-08-05T23:44:30.515544: step 7591, loss 0.571281.
Train: 2018-08-05T23:44:30.703001: step 7592, loss 0.51931.
Train: 2018-08-05T23:44:30.874869: step 7593, loss 0.638779.
Train: 2018-08-05T23:44:31.045703: step 7594, loss 0.50308.
Train: 2018-08-05T23:44:31.201916: step 7595, loss 0.579195.
Train: 2018-08-05T23:44:31.389342: step 7596, loss 0.512902.
Train: 2018-08-05T23:44:31.545554: step 7597, loss 0.571733.
Train: 2018-08-05T23:44:31.717390: step 7598, loss 0.563779.
Train: 2018-08-05T23:44:31.873635: step 7599, loss 0.520375.
Train: 2018-08-05T23:44:32.032297: step 7600, loss 0.628704.
Test: 2018-08-05T23:44:32.282210: step 7600, loss 0.548406.
Train: 2018-08-05T23:44:33.094518: step 7601, loss 0.46145.
Train: 2018-08-05T23:44:33.250761: step 7602, loss 0.529152.
Train: 2018-08-05T23:44:33.422565: step 7603, loss 0.562551.
Train: 2018-08-05T23:44:33.578778: step 7604, loss 0.605575.
Train: 2018-08-05T23:44:33.734991: step 7605, loss 0.536663.
Train: 2018-08-05T23:44:33.906850: step 7606, loss 0.596444.
Train: 2018-08-05T23:44:34.063039: step 7607, loss 0.528083.
Train: 2018-08-05T23:44:34.219252: step 7608, loss 0.588159.
Train: 2018-08-05T23:44:34.375466: step 7609, loss 0.519289.
Train: 2018-08-05T23:44:34.531679: step 7610, loss 0.570971.
Test: 2018-08-05T23:44:34.781621: step 7610, loss 0.547924.
Train: 2018-08-05T23:44:34.937833: step 7611, loss 0.562406.
Train: 2018-08-05T23:44:35.094072: step 7612, loss 0.579759.
Train: 2018-08-05T23:44:35.265906: step 7613, loss 0.5104.
Train: 2018-08-05T23:44:35.422095: step 7614, loss 0.579687.
Train: 2018-08-05T23:44:35.578308: step 7615, loss 0.571205.
Train: 2018-08-05T23:44:35.750142: step 7616, loss 0.5015.
Train: 2018-08-05T23:44:35.906355: step 7617, loss 0.614612.
Train: 2018-08-05T23:44:36.062569: step 7618, loss 0.571231.
Train: 2018-08-05T23:44:36.218782: step 7619, loss 0.623506.
Train: 2018-08-05T23:44:36.384689: step 7620, loss 0.52761.
Test: 2018-08-05T23:44:36.619011: step 7620, loss 0.547856.
Train: 2018-08-05T23:44:36.790846: step 7621, loss 0.52746.
Train: 2018-08-05T23:44:36.952366: step 7622, loss 0.597183.
Train: 2018-08-05T23:44:37.108549: step 7623, loss 0.579905.
Train: 2018-08-05T23:44:37.280382: step 7624, loss 0.553761.
Train: 2018-08-05T23:44:37.436597: step 7625, loss 0.588438.
Train: 2018-08-05T23:44:37.610957: step 7626, loss 0.571123.
Train: 2018-08-05T23:44:37.760637: step 7627, loss 0.545045.
Train: 2018-08-05T23:44:37.929433: step 7628, loss 0.53638.
Train: 2018-08-05T23:44:38.086781: step 7629, loss 0.527727.
Train: 2018-08-05T23:44:38.258616: step 7630, loss 0.614153.
Test: 2018-08-05T23:44:38.492954: step 7630, loss 0.547951.
Train: 2018-08-05T23:44:38.649120: step 7631, loss 0.605578.
Train: 2018-08-05T23:44:38.820954: step 7632, loss 0.605412.
Train: 2018-08-05T23:44:38.999454: step 7633, loss 0.613959.
Train: 2018-08-05T23:44:39.174803: step 7634, loss 0.519859.
Train: 2018-08-05T23:44:39.346637: step 7635, loss 0.604964.
Train: 2018-08-05T23:44:39.502880: step 7636, loss 0.588026.
Train: 2018-08-05T23:44:39.659095: step 7637, loss 0.528807.
Train: 2018-08-05T23:44:39.830904: step 7638, loss 0.520237.
Train: 2018-08-05T23:44:40.002763: step 7639, loss 0.545477.
Train: 2018-08-05T23:44:40.158097: step 7640, loss 0.545515.
Test: 2018-08-05T23:44:40.408036: step 7640, loss 0.548252.
Train: 2018-08-05T23:44:40.579871: step 7641, loss 0.536994.
Train: 2018-08-05T23:44:40.751737: step 7642, loss 0.562449.
Train: 2018-08-05T23:44:40.920504: step 7643, loss 0.579561.
Train: 2018-08-05T23:44:41.077813: step 7644, loss 0.545539.
Train: 2018-08-05T23:44:41.249647: step 7645, loss 0.528427.
Train: 2018-08-05T23:44:41.421450: step 7646, loss 0.536785.
Train: 2018-08-05T23:44:41.577695: step 7647, loss 0.621999.
Train: 2018-08-05T23:44:41.749500: step 7648, loss 0.494081.
Train: 2018-08-05T23:44:41.910176: step 7649, loss 0.579466.
Train: 2018-08-05T23:44:42.082042: step 7650, loss 0.545365.
Test: 2018-08-05T23:44:42.316363: step 7650, loss 0.548013.
Train: 2018-08-05T23:44:42.472578: step 7651, loss 0.562507.
Train: 2018-08-05T23:44:42.644410: step 7652, loss 0.510882.
Train: 2018-08-05T23:44:42.800593: step 7653, loss 0.571053.
Train: 2018-08-05T23:44:42.972458: step 7654, loss 0.62294.
Train: 2018-08-05T23:44:43.128677: step 7655, loss 0.579744.
Train: 2018-08-05T23:44:43.284889: step 7656, loss 0.501868.
Train: 2018-08-05T23:44:43.441098: step 7657, loss 0.57112.
Train: 2018-08-05T23:44:43.612903: step 7658, loss 0.623126.
Train: 2018-08-05T23:44:43.800357: step 7659, loss 0.553787.
Train: 2018-08-05T23:44:44.003435: step 7660, loss 0.527719.
Test: 2018-08-05T23:44:44.253376: step 7660, loss 0.547888.
Train: 2018-08-05T23:44:44.503317: step 7661, loss 0.63173.
Train: 2018-08-05T23:44:44.815743: step 7662, loss 0.545079.
Train: 2018-08-05T23:44:45.003200: step 7663, loss 0.545052.
Train: 2018-08-05T23:44:45.206277: step 7664, loss 0.527788.
Train: 2018-08-05T23:44:45.440598: step 7665, loss 0.571054.
Train: 2018-08-05T23:44:45.690538: step 7666, loss 0.536398.
Train: 2018-08-05T23:44:45.893615: step 7667, loss 0.553705.
Train: 2018-08-05T23:44:46.081075: step 7668, loss 0.588384.
Train: 2018-08-05T23:44:46.268559: step 7669, loss 0.536467.
Train: 2018-08-05T23:44:46.424770: step 7670, loss 0.571076.
Test: 2018-08-05T23:44:46.674691: step 7670, loss 0.547894.
Train: 2018-08-05T23:44:46.830937: step 7671, loss 0.536448.
Train: 2018-08-05T23:44:47.002728: step 7672, loss 0.597104.
Train: 2018-08-05T23:44:47.153028: step 7673, loss 0.510341.
Train: 2018-08-05T23:44:47.324900: step 7674, loss 0.597228.
Train: 2018-08-05T23:44:47.496728: step 7675, loss 0.545094.
Train: 2018-08-05T23:44:47.652912: step 7676, loss 0.605911.
Train: 2018-08-05T23:44:47.809159: step 7677, loss 0.510327.
Train: 2018-08-05T23:44:47.979044: step 7678, loss 0.527645.
Train: 2018-08-05T23:44:48.135252: step 7679, loss 0.614637.
Train: 2018-08-05T23:44:48.291441: step 7680, loss 0.579814.
Test: 2018-08-05T23:44:48.525763: step 7680, loss 0.547859.
Train: 2018-08-05T23:44:48.713243: step 7681, loss 0.553729.
Train: 2018-08-05T23:44:48.869431: step 7682, loss 0.58849.
Train: 2018-08-05T23:44:49.045367: step 7683, loss 0.631849.
Train: 2018-08-05T23:44:49.201610: step 7684, loss 0.623026.
Train: 2018-08-05T23:44:49.357792: step 7685, loss 0.57104.
Train: 2018-08-05T23:44:49.514007: step 7686, loss 0.613929.
Train: 2018-08-05T23:44:49.685871: step 7687, loss 0.528133.
Train: 2018-08-05T23:44:49.842054: step 7688, loss 0.553864.
Train: 2018-08-05T23:44:49.998298: step 7689, loss 0.596389.
Train: 2018-08-05T23:44:50.154515: step 7690, loss 0.553945.
Test: 2018-08-05T23:44:50.404422: step 7690, loss 0.548252.
Train: 2018-08-05T23:44:50.560635: step 7691, loss 0.553983.
Train: 2018-08-05T23:44:50.732472: step 7692, loss 0.553927.
Train: 2018-08-05T23:44:50.919926: step 7693, loss 0.553973.
Train: 2018-08-05T23:44:51.074683: step 7694, loss 0.511926.
Train: 2018-08-05T23:44:51.230928: step 7695, loss 0.511963.
Train: 2018-08-05T23:44:51.402730: step 7696, loss 0.511799.
Train: 2018-08-05T23:44:51.558974: step 7697, loss 0.520093.
Train: 2018-08-05T23:44:51.730779: step 7698, loss 0.503066.
Train: 2018-08-05T23:44:51.886991: step 7699, loss 0.553865.
Train: 2018-08-05T23:44:52.047173: step 7700, loss 0.58803.
Test: 2018-08-05T23:44:52.297085: step 7700, loss 0.547995.
Train: 2018-08-05T23:44:53.015695: step 7701, loss 0.672442.
Train: 2018-08-05T23:44:53.171878: step 7702, loss 0.528061.
Train: 2018-08-05T23:44:53.343738: step 7703, loss 0.562363.
Train: 2018-08-05T23:44:53.499951: step 7704, loss 0.493484.
Train: 2018-08-05T23:44:53.656138: step 7705, loss 0.571015.
Train: 2018-08-05T23:44:53.812382: step 7706, loss 0.588314.
Train: 2018-08-05T23:44:53.968595: step 7707, loss 0.579787.
Train: 2018-08-05T23:44:54.140401: step 7708, loss 0.5884.
Train: 2018-08-05T23:44:54.296613: step 7709, loss 0.649041.
Train: 2018-08-05T23:44:54.452826: step 7710, loss 0.588352.
Test: 2018-08-05T23:44:54.702800: step 7710, loss 0.547964.
Train: 2018-08-05T23:44:54.859010: step 7711, loss 0.51067.
Train: 2018-08-05T23:44:55.030840: step 7712, loss 0.536626.
Train: 2018-08-05T23:44:55.187059: step 7713, loss 0.605383.
Train: 2018-08-05T23:44:55.343244: step 7714, loss 0.571035.
Train: 2018-08-05T23:44:55.499479: step 7715, loss 0.57094.
Train: 2018-08-05T23:44:55.655698: step 7716, loss 0.579553.
Train: 2018-08-05T23:44:55.827504: step 7717, loss 0.494007.
Train: 2018-08-05T23:44:55.999339: step 7718, loss 0.596601.
Train: 2018-08-05T23:44:56.171196: step 7719, loss 0.630811.
Train: 2018-08-05T23:44:56.327416: step 7720, loss 0.579356.
Test: 2018-08-05T23:44:56.561705: step 7720, loss 0.548163.
Train: 2018-08-05T23:44:56.717948: step 7721, loss 0.579343.
Train: 2018-08-05T23:44:56.874163: step 7722, loss 0.562497.
Train: 2018-08-05T23:44:57.045516: step 7723, loss 0.537088.
Train: 2018-08-05T23:44:57.217326: step 7724, loss 0.511728.
Train: 2018-08-05T23:44:57.404785: step 7725, loss 0.596271.
Train: 2018-08-05T23:44:57.576621: step 7726, loss 0.520273.
Train: 2018-08-05T23:44:57.748489: step 7727, loss 0.604725.
Train: 2018-08-05T23:44:57.996699: step 7728, loss 0.520156.
Train: 2018-08-05T23:44:58.199772: step 7729, loss 0.537075.
Train: 2018-08-05T23:44:58.434094: step 7730, loss 0.469044.
Test: 2018-08-05T23:44:58.684043: step 7730, loss 0.548152.
Train: 2018-08-05T23:44:58.840246: step 7731, loss 0.621978.
Train: 2018-08-05T23:44:59.017542: step 7732, loss 0.562405.
Train: 2018-08-05T23:44:59.204996: step 7733, loss 0.528247.
Train: 2018-08-05T23:44:59.376831: step 7734, loss 0.50259.
Train: 2018-08-05T23:44:59.548666: step 7735, loss 0.485184.
Train: 2018-08-05T23:44:59.720537: step 7736, loss 0.605467.
Train: 2018-08-05T23:44:59.876712: step 7737, loss 0.597138.
Train: 2018-08-05T23:45:00.048577: step 7738, loss 0.536311.
Train: 2018-08-05T23:45:00.204790: step 7739, loss 0.588685.
Train: 2018-08-05T23:45:00.376594: step 7740, loss 0.501451.
Test: 2018-08-05T23:45:00.610914: step 7740, loss 0.547805.
Train: 2018-08-05T23:45:00.782748: step 7741, loss 0.580035.
Train: 2018-08-05T23:45:00.936828: step 7742, loss 0.579919.
Train: 2018-08-05T23:45:01.108660: step 7743, loss 0.545006.
Train: 2018-08-05T23:45:01.280496: step 7744, loss 0.597539.
Train: 2018-08-05T23:45:01.436708: step 7745, loss 0.606177.
Train: 2018-08-05T23:45:01.608512: step 7746, loss 0.492431.
Train: 2018-08-05T23:45:01.764725: step 7747, loss 0.536148.
Train: 2018-08-05T23:45:01.934676: step 7748, loss 0.544912.
Train: 2018-08-05T23:45:02.090919: step 7749, loss 0.615119.
Train: 2018-08-05T23:45:02.262726: step 7750, loss 0.536122.
Test: 2018-08-05T23:45:02.497046: step 7750, loss 0.547751.
Train: 2018-08-05T23:45:02.653273: step 7751, loss 0.509775.
Train: 2018-08-05T23:45:02.825093: step 7752, loss 0.562388.
Train: 2018-08-05T23:45:02.985704: step 7753, loss 0.597698.
Train: 2018-08-05T23:45:03.141917: step 7754, loss 0.544892.
Train: 2018-08-05T23:45:03.298130: step 7755, loss 0.580043.
Train: 2018-08-05T23:45:03.454343: step 7756, loss 0.562423.
Train: 2018-08-05T23:45:03.626178: step 7757, loss 0.544871.
Train: 2018-08-05T23:45:03.798013: step 7758, loss 0.624023.
Train: 2018-08-05T23:45:03.954226: step 7759, loss 0.571295.
Train: 2018-08-05T23:45:04.110440: step 7760, loss 0.571162.
Test: 2018-08-05T23:45:04.360381: step 7760, loss 0.547787.
Train: 2018-08-05T23:45:04.516625: step 7761, loss 0.518696.
Train: 2018-08-05T23:45:04.672837: step 7762, loss 0.58887.
Train: 2018-08-05T23:45:04.829021: step 7763, loss 0.527572.
Train: 2018-08-05T23:45:04.985263: step 7764, loss 0.518812.
Train: 2018-08-05T23:45:05.141446: step 7765, loss 0.544953.
Train: 2018-08-05T23:45:05.313311: step 7766, loss 0.518746.
Train: 2018-08-05T23:45:05.485133: step 7767, loss 0.579942.
Train: 2018-08-05T23:45:05.641360: step 7768, loss 0.562412.
Train: 2018-08-05T23:45:05.797544: step 7769, loss 0.597486.
Train: 2018-08-05T23:45:05.969407: step 7770, loss 0.501112.
Test: 2018-08-05T23:45:06.219334: step 7770, loss 0.547765.
Train: 2018-08-05T23:45:06.375562: step 7771, loss 0.571248.
Train: 2018-08-05T23:45:06.531775: step 7772, loss 0.553658.
Train: 2018-08-05T23:45:06.703579: step 7773, loss 0.588826.
Train: 2018-08-05T23:45:06.872817: step 7774, loss 0.562434.
Train: 2018-08-05T23:45:07.029001: step 7775, loss 0.527338.
Train: 2018-08-05T23:45:07.216459: step 7776, loss 0.518605.
Train: 2018-08-05T23:45:07.388316: step 7777, loss 0.69422.
Train: 2018-08-05T23:45:07.560127: step 7778, loss 0.536154.
Train: 2018-08-05T23:45:07.716369: step 7779, loss 0.606272.
Train: 2018-08-05T23:45:07.888207: step 7780, loss 0.492522.
Test: 2018-08-05T23:45:08.108572: step 7780, loss 0.547807.
Train: 2018-08-05T23:45:08.280438: step 7781, loss 0.545016.
Train: 2018-08-05T23:45:08.436621: step 7782, loss 0.510041.
Train: 2018-08-05T23:45:08.591838: step 7783, loss 0.518773.
Train: 2018-08-05T23:45:08.763703: step 7784, loss 0.474821.
Train: 2018-08-05T23:45:08.924358: step 7785, loss 0.597671.
Train: 2018-08-05T23:45:09.080602: step 7786, loss 0.597626.
Train: 2018-08-05T23:45:09.236785: step 7787, loss 0.588918.
Train: 2018-08-05T23:45:09.408619: step 7788, loss 0.544895.
Train: 2018-08-05T23:45:09.564851: step 7789, loss 0.59782.
Train: 2018-08-05T23:45:09.733741: step 7790, loss 0.580067.
Test: 2018-08-05T23:45:09.968036: step 7790, loss 0.54773.
Train: 2018-08-05T23:45:10.139861: step 7791, loss 0.57128.
Train: 2018-08-05T23:45:10.296108: step 7792, loss 0.536088.
Train: 2018-08-05T23:45:10.452316: step 7793, loss 0.641509.
Train: 2018-08-05T23:45:10.608530: step 7794, loss 0.553655.
Train: 2018-08-05T23:45:10.779375: step 7795, loss 0.562491.
Train: 2018-08-05T23:45:10.935563: step 7796, loss 0.571137.
Train: 2018-08-05T23:45:11.107397: step 7797, loss 0.605955.
Train: 2018-08-05T23:45:11.263611: step 7798, loss 0.579836.
Train: 2018-08-05T23:45:11.419824: step 7799, loss 0.56249.
Train: 2018-08-05T23:45:11.591682: step 7800, loss 0.631539.
Test: 2018-08-05T23:45:11.825994: step 7800, loss 0.547998.
Train: 2018-08-05T23:45:12.577054: step 7801, loss 0.545155.
Train: 2018-08-05T23:45:12.733237: step 7802, loss 0.570903.
Train: 2018-08-05T23:45:12.889450: step 7803, loss 0.579527.
Train: 2018-08-05T23:45:13.048982: step 7804, loss 0.553855.
Train: 2018-08-05T23:45:13.220816: step 7805, loss 0.553965.
Train: 2018-08-05T23:45:13.377030: step 7806, loss 0.553912.
Train: 2018-08-05T23:45:13.548864: step 7807, loss 0.646993.
Train: 2018-08-05T23:45:13.705077: step 7808, loss 0.528759.
Train: 2018-08-05T23:45:13.861290: step 7809, loss 0.570864.
Train: 2018-08-05T23:45:14.033156: step 7810, loss 0.621316.
Test: 2018-08-05T23:45:14.267479: step 7810, loss 0.548487.
Train: 2018-08-05T23:45:14.423689: step 7811, loss 0.554089.
Train: 2018-08-05T23:45:14.595523: step 7812, loss 0.554398.
Train: 2018-08-05T23:45:14.767326: step 7813, loss 0.562519.
Train: 2018-08-05T23:45:14.923540: step 7814, loss 0.521134.
Train: 2018-08-05T23:45:15.079784: step 7815, loss 0.57091.
Train: 2018-08-05T23:45:15.251590: step 7816, loss 0.587362.
Train: 2018-08-05T23:45:15.407802: step 7817, loss 0.546025.
Train: 2018-08-05T23:45:15.579636: step 7818, loss 0.579076.
Train: 2018-08-05T23:45:15.735874: step 7819, loss 0.637159.
Train: 2018-08-05T23:45:15.892094: step 7820, loss 0.612185.
Test: 2018-08-05T23:45:16.126415: step 7820, loss 0.548714.
Train: 2018-08-05T23:45:16.298217: step 7821, loss 0.471697.
Train: 2018-08-05T23:45:16.454430: step 7822, loss 0.562577.
Train: 2018-08-05T23:45:16.626265: step 7823, loss 0.529435.
Train: 2018-08-05T23:45:16.798100: step 7824, loss 0.587324.
Train: 2018-08-05T23:45:16.954343: step 7825, loss 0.496227.
Train: 2018-08-05T23:45:17.110550: step 7826, loss 0.604057.
Train: 2018-08-05T23:45:17.282385: step 7827, loss 0.562713.
Train: 2018-08-05T23:45:17.438605: step 7828, loss 0.579209.
Train: 2018-08-05T23:45:17.616468: step 7829, loss 0.562571.
Train: 2018-08-05T23:45:17.788305: step 7830, loss 0.537455.
Test: 2018-08-05T23:45:18.033009: step 7830, loss 0.548454.
Train: 2018-08-05T23:45:18.204812: step 7831, loss 0.495541.
Train: 2018-08-05T23:45:18.376672: step 7832, loss 0.562476.
Train: 2018-08-05T23:45:18.532885: step 7833, loss 0.545594.
Train: 2018-08-05T23:45:18.704695: step 7834, loss 0.511723.
Train: 2018-08-05T23:45:18.876565: step 7835, loss 0.536981.
Train: 2018-08-05T23:45:19.046948: step 7836, loss 0.605006.
Train: 2018-08-05T23:45:19.203137: step 7837, loss 0.579501.
Train: 2018-08-05T23:45:19.359350: step 7838, loss 0.630891.
Train: 2018-08-05T23:45:19.531186: step 7839, loss 0.536631.
Train: 2018-08-05T23:45:19.687428: step 7840, loss 0.579627.
Test: 2018-08-05T23:45:19.927317: step 7840, loss 0.548068.
Train: 2018-08-05T23:45:20.083531: step 7841, loss 0.527965.
Train: 2018-08-05T23:45:20.255399: step 7842, loss 0.49401.
Train: 2018-08-05T23:45:20.427199: step 7843, loss 0.536714.
Train: 2018-08-05T23:45:20.599065: step 7844, loss 0.56252.
Train: 2018-08-05T23:45:20.755249: step 7845, loss 0.562579.
Train: 2018-08-05T23:45:20.911460: step 7846, loss 0.545075.
Train: 2018-08-05T23:45:21.067674: step 7847, loss 0.588533.
Train: 2018-08-05T23:45:21.223917: step 7848, loss 0.614725.
Train: 2018-08-05T23:45:21.395746: step 7849, loss 0.518846.
Train: 2018-08-05T23:45:21.551967: step 7850, loss 0.588589.
Test: 2018-08-05T23:45:21.786286: step 7850, loss 0.547822.
Train: 2018-08-05T23:45:21.940287: step 7851, loss 0.571203.
Train: 2018-08-05T23:45:22.096507: step 7852, loss 0.562493.
Train: 2018-08-05T23:45:22.268311: step 7853, loss 0.553673.
Train: 2018-08-05T23:45:22.424557: step 7854, loss 0.545013.
Train: 2018-08-05T23:45:22.580737: step 7855, loss 0.492606.
Train: 2018-08-05T23:45:22.752571: step 7856, loss 0.55362.
Train: 2018-08-05T23:45:22.922483: step 7857, loss 0.509986.
Train: 2018-08-05T23:45:23.078697: step 7858, loss 0.615023.
Train: 2018-08-05T23:45:23.234909: step 7859, loss 0.54475.
Train: 2018-08-05T23:45:23.406774: step 7860, loss 0.58096.
Test: 2018-08-05T23:45:23.641109: step 7860, loss 0.547699.
Train: 2018-08-05T23:45:23.797302: step 7861, loss 0.509925.
Train: 2018-08-05T23:45:23.973635: step 7862, loss 0.54457.
Train: 2018-08-05T23:45:24.129853: step 7863, loss 0.535603.
Train: 2018-08-05T23:45:24.301696: step 7864, loss 0.526913.
Train: 2018-08-05T23:45:24.473523: step 7865, loss 0.544504.
Train: 2018-08-05T23:45:24.629736: step 7866, loss 0.581686.
Train: 2018-08-05T23:45:24.801540: step 7867, loss 0.554461.
Train: 2018-08-05T23:45:24.957790: step 7868, loss 0.525963.
Train: 2018-08-05T23:45:25.113998: step 7869, loss 0.670687.
Train: 2018-08-05T23:45:25.285802: step 7870, loss 0.571564.
Test: 2018-08-05T23:45:25.520148: step 7870, loss 0.547619.
Train: 2018-08-05T23:45:25.691987: step 7871, loss 0.634341.
Train: 2018-08-05T23:45:25.863821: step 7872, loss 0.589232.
Train: 2018-08-05T23:45:26.020034: step 7873, loss 0.571334.
Train: 2018-08-05T23:45:26.176216: step 7874, loss 0.536061.
Train: 2018-08-05T23:45:26.348052: step 7875, loss 0.518568.
Train: 2018-08-05T23:45:26.519920: step 7876, loss 0.606112.
Train: 2018-08-05T23:45:26.676099: step 7877, loss 0.501403.
Train: 2018-08-05T23:45:26.847965: step 7878, loss 0.571073.
Train: 2018-08-05T23:45:27.004148: step 7879, loss 0.544938.
Train: 2018-08-05T23:45:27.160384: step 7880, loss 0.562469.
Test: 2018-08-05T23:45:27.410302: step 7880, loss 0.547844.
Train: 2018-08-05T23:45:27.566539: step 7881, loss 0.545093.
Train: 2018-08-05T23:45:27.738386: step 7882, loss 0.649523.
Train: 2018-08-05T23:45:27.910184: step 7883, loss 0.510164.
Train: 2018-08-05T23:45:28.082020: step 7884, loss 0.605648.
Train: 2018-08-05T23:45:28.238232: step 7885, loss 0.536517.
Train: 2018-08-05T23:45:28.410067: step 7886, loss 0.536416.
Train: 2018-08-05T23:45:28.597526: step 7887, loss 0.657514.
Train: 2018-08-05T23:45:28.753737: step 7888, loss 0.614082.
Train: 2018-08-05T23:45:28.923658: step 7889, loss 0.631159.
Train: 2018-08-05T23:45:29.095496: step 7890, loss 0.536898.
Test: 2018-08-05T23:45:29.345466: step 7890, loss 0.54816.
Train: 2018-08-05T23:45:29.517304: step 7891, loss 0.613547.
Train: 2018-08-05T23:45:29.673483: step 7892, loss 0.520131.
Train: 2018-08-05T23:45:29.829726: step 7893, loss 0.562298.
Train: 2018-08-05T23:45:30.014251: step 7894, loss 0.520508.
Train: 2018-08-05T23:45:30.165040: step 7895, loss 0.604657.
Train: 2018-08-05T23:45:30.336875: step 7896, loss 0.62101.
Train: 2018-08-05T23:45:30.493087: step 7897, loss 0.529175.
Train: 2018-08-05T23:45:30.664923: step 7898, loss 0.512238.
Train: 2018-08-05T23:45:30.836782: step 7899, loss 0.587637.
Train: 2018-08-05T23:45:31.008621: step 7900, loss 0.570829.
Test: 2018-08-05T23:45:31.242911: step 7900, loss 0.548534.
Train: 2018-08-05T23:45:31.961491: step 7901, loss 0.587352.
Train: 2018-08-05T23:45:32.133327: step 7902, loss 0.67911.
Train: 2018-08-05T23:45:32.289541: step 7903, loss 0.562201.
Train: 2018-08-05T23:45:32.445753: step 7904, loss 0.496329.
Train: 2018-08-05T23:45:32.617587: step 7905, loss 0.529688.
Train: 2018-08-05T23:45:32.789421: step 7906, loss 0.562441.
Train: 2018-08-05T23:45:32.945392: step 7907, loss 0.578686.
Train: 2018-08-05T23:45:33.117195: step 7908, loss 0.571199.
Train: 2018-08-05T23:45:33.289030: step 7909, loss 0.587023.
Train: 2018-08-05T23:45:33.445244: step 7910, loss 0.563146.
Test: 2018-08-05T23:45:33.679596: step 7910, loss 0.548783.
Train: 2018-08-05T23:45:33.851398: step 7911, loss 0.496954.
Train: 2018-08-05T23:45:34.026915: step 7912, loss 0.587438.
Train: 2018-08-05T23:45:34.183127: step 7913, loss 0.529441.
Train: 2018-08-05T23:45:34.339311: step 7914, loss 0.546371.
Train: 2018-08-05T23:45:34.511177: step 7915, loss 0.562615.
Train: 2018-08-05T23:45:34.667383: step 7916, loss 0.587393.
Train: 2018-08-05T23:45:34.839193: step 7917, loss 0.470196.
Train: 2018-08-05T23:45:34.995430: step 7918, loss 0.604523.
Train: 2018-08-05T23:45:35.151619: step 7919, loss 0.537127.
Train: 2018-08-05T23:45:35.323453: step 7920, loss 0.536948.
Test: 2018-08-05T23:45:35.557804: step 7920, loss 0.548198.
Train: 2018-08-05T23:45:35.714011: step 7921, loss 0.596221.
Train: 2018-08-05T23:45:35.870233: step 7922, loss 0.545441.
Train: 2018-08-05T23:45:36.042059: step 7923, loss 0.545336.
Train: 2018-08-05T23:45:36.198279: step 7924, loss 0.622312.
Train: 2018-08-05T23:45:36.370083: step 7925, loss 0.510954.
Train: 2018-08-05T23:45:36.526295: step 7926, loss 0.562442.
Train: 2018-08-05T23:45:36.682508: step 7927, loss 0.57102.
Train: 2018-08-05T23:45:36.838752: step 7928, loss 0.588208.
Train: 2018-08-05T23:45:37.010589: step 7929, loss 0.501987.
Train: 2018-08-05T23:45:37.166769: step 7930, loss 0.605736.
Test: 2018-08-05T23:45:37.406772: step 7930, loss 0.547932.
Train: 2018-08-05T23:45:37.562955: step 7931, loss 0.527928.
Train: 2018-08-05T23:45:37.734819: step 7932, loss 0.553824.
Train: 2018-08-05T23:45:37.891004: step 7933, loss 0.571122.
Train: 2018-08-05T23:45:38.062867: step 7934, loss 0.579894.
Train: 2018-08-05T23:45:38.219081: step 7935, loss 0.65774.
Train: 2018-08-05T23:45:38.375295: step 7936, loss 0.5971.
Train: 2018-08-05T23:45:38.547128: step 7937, loss 0.614278.
Train: 2018-08-05T23:45:38.750205: step 7938, loss 0.519364.
Train: 2018-08-05T23:45:38.911799: step 7939, loss 0.58808.
Train: 2018-08-05T23:45:39.068014: step 7940, loss 0.545223.
Test: 2018-08-05T23:45:39.317954: step 7940, loss 0.548076.
Train: 2018-08-05T23:45:39.469067: step 7941, loss 0.553973.
Train: 2018-08-05T23:45:39.640903: step 7942, loss 0.570935.
Train: 2018-08-05T23:45:39.797147: step 7943, loss 0.528345.
Train: 2018-08-05T23:45:39.951492: step 7944, loss 0.562387.
Train: 2018-08-05T23:45:40.123322: step 7945, loss 0.545404.
Train: 2018-08-05T23:45:40.279511: step 7946, loss 0.579456.
Train: 2018-08-05T23:45:40.447348: step 7947, loss 0.562401.
Train: 2018-08-05T23:45:40.603562: step 7948, loss 0.494238.
Train: 2018-08-05T23:45:40.759807: step 7949, loss 0.553895.
Train: 2018-08-05T23:45:40.922528: step 7950, loss 0.511171.
Test: 2018-08-05T23:45:41.156881: step 7950, loss 0.548057.
Train: 2018-08-05T23:45:41.328683: step 7951, loss 0.630897.
Train: 2018-08-05T23:45:41.493513: step 7952, loss 0.545286.
Train: 2018-08-05T23:45:41.665348: step 7953, loss 0.596733.
Train: 2018-08-05T23:45:41.821562: step 7954, loss 0.528071.
Train: 2018-08-05T23:45:41.977811: step 7955, loss 0.562363.
Train: 2018-08-05T23:45:42.149609: step 7956, loss 0.639741.
Train: 2018-08-05T23:45:42.305824: step 7957, loss 0.485158.
Train: 2018-08-05T23:45:42.462036: step 7958, loss 0.648307.
Train: 2018-08-05T23:45:42.633874: step 7959, loss 0.613887.
Train: 2018-08-05T23:45:42.790113: step 7960, loss 0.588082.
Test: 2018-08-05T23:45:43.039801: step 7960, loss 0.548101.
Train: 2018-08-05T23:45:43.196015: step 7961, loss 0.545314.
Train: 2018-08-05T23:45:43.352194: step 7962, loss 0.54536.
Train: 2018-08-05T23:45:43.524055: step 7963, loss 0.570901.
Train: 2018-08-05T23:45:43.680273: step 7964, loss 0.528397.
Train: 2018-08-05T23:45:43.836455: step 7965, loss 0.536899.
Train: 2018-08-05T23:45:43.996509: step 7966, loss 0.621891.
Train: 2018-08-05T23:45:44.168345: step 7967, loss 0.536933.
Train: 2018-08-05T23:45:44.324528: step 7968, loss 0.486041.
Train: 2018-08-05T23:45:44.480740: step 7969, loss 0.511455.
Train: 2018-08-05T23:45:44.652575: step 7970, loss 0.596449.
Test: 2018-08-05T23:45:44.886894: step 7970, loss 0.548111.
Train: 2018-08-05T23:45:45.058760: step 7971, loss 0.536792.
Train: 2018-08-05T23:45:45.214972: step 7972, loss 0.476877.
Train: 2018-08-05T23:45:45.371187: step 7973, loss 0.562404.
Train: 2018-08-05T23:45:45.527398: step 7974, loss 0.510724.
Train: 2018-08-05T23:45:45.699234: step 7975, loss 0.536465.
Train: 2018-08-05T23:45:45.855443: step 7976, loss 0.571069.
Train: 2018-08-05T23:45:46.011630: step 7977, loss 0.518777.
Train: 2018-08-05T23:45:46.183464: step 7978, loss 0.571224.
Train: 2018-08-05T23:45:46.339679: step 7979, loss 0.580024.
Train: 2018-08-05T23:45:46.495891: step 7980, loss 0.535852.
Test: 2018-08-05T23:45:46.745832: step 7980, loss 0.547659.
Train: 2018-08-05T23:45:46.917676: step 7981, loss 0.571573.
Train: 2018-08-05T23:45:47.073879: step 7982, loss 0.670163.
Train: 2018-08-05T23:45:47.230094: step 7983, loss 0.544478.
Train: 2018-08-05T23:45:47.401953: step 7984, loss 0.55355.
Train: 2018-08-05T23:45:47.558141: step 7985, loss 0.562354.
Train: 2018-08-05T23:45:47.714384: step 7986, loss 0.571379.
Train: 2018-08-05T23:45:47.886219: step 7987, loss 0.562431.
Train: 2018-08-05T23:45:48.042426: step 7988, loss 0.553587.
Train: 2018-08-05T23:45:48.197640: step 7989, loss 0.58025.
Train: 2018-08-05T23:45:48.353853: step 7990, loss 0.518409.
Test: 2018-08-05T23:45:48.588175: step 7990, loss 0.547731.
Train: 2018-08-05T23:45:48.760039: step 7991, loss 0.562523.
Train: 2018-08-05T23:45:48.916220: step 7992, loss 0.562466.
Train: 2018-08-05T23:45:49.072434: step 7993, loss 0.536047.
Train: 2018-08-05T23:45:49.228677: step 7994, loss 0.509664.
Train: 2018-08-05T23:45:49.400514: step 7995, loss 0.632877.
Train: 2018-08-05T23:45:49.572318: step 7996, loss 0.562444.
Train: 2018-08-05T23:45:49.728529: step 7997, loss 0.50095.
Train: 2018-08-05T23:45:49.900412: step 7998, loss 0.580057.
Train: 2018-08-05T23:45:50.056845: step 7999, loss 0.544776.
Train: 2018-08-05T23:45:50.213089: step 8000, loss 0.624215.
Test: 2018-08-05T23:45:50.463033: step 8000, loss 0.547745.
Train: 2018-08-05T23:45:51.186136: step 8001, loss 0.562436.
Train: 2018-08-05T23:45:51.357920: step 8002, loss 0.536197.
Train: 2018-08-05T23:45:51.514134: step 8003, loss 0.543717.
Train: 2018-08-05T23:45:51.670378: step 8004, loss 0.536172.
Train: 2018-08-05T23:45:51.842206: step 8005, loss 0.465951.
Train: 2018-08-05T23:45:52.014016: step 8006, loss 0.518531.
Train: 2018-08-05T23:45:52.170266: step 8007, loss 0.50959.
Train: 2018-08-05T23:45:52.373307: step 8008, loss 0.57126.
Train: 2018-08-05T23:45:52.529551: step 8009, loss 0.562604.
Train: 2018-08-05T23:45:52.685764: step 8010, loss 0.589274.
Test: 2018-08-05T23:45:52.931050: step 8010, loss 0.547635.
Train: 2018-08-05T23:45:53.090365: step 8011, loss 0.606987.
Train: 2018-08-05T23:45:53.262200: step 8012, loss 0.518275.
Train: 2018-08-05T23:45:53.418413: step 8013, loss 0.446488.
Train: 2018-08-05T23:45:53.590278: step 8014, loss 0.508615.
Train: 2018-08-05T23:45:53.762082: step 8015, loss 0.562566.
Train: 2018-08-05T23:45:53.927285: step 8016, loss 0.544947.
Train: 2018-08-05T23:45:54.083528: step 8017, loss 0.507964.
Train: 2018-08-05T23:45:54.255331: step 8018, loss 0.580683.
Train: 2018-08-05T23:45:54.406527: step 8019, loss 0.58185.
Train: 2018-08-05T23:45:54.578361: step 8020, loss 0.544538.
Test: 2018-08-05T23:45:54.812721: step 8020, loss 0.547582.
Train: 2018-08-05T23:45:54.984517: step 8021, loss 0.563417.
Train: 2018-08-05T23:45:55.188670: step 8022, loss 0.554361.
Train: 2018-08-05T23:45:55.347039: step 8023, loss 0.627587.
Train: 2018-08-05T23:45:55.515306: step 8024, loss 0.534858.
Train: 2018-08-05T23:45:55.683854: step 8025, loss 0.553673.
Train: 2018-08-05T23:45:55.857399: step 8026, loss 0.663254.
Train: 2018-08-05T23:45:56.041928: step 8027, loss 0.56281.
Train: 2018-08-05T23:45:56.191288: step 8028, loss 0.571832.
Train: 2018-08-05T23:45:56.347508: step 8029, loss 0.580611.
Train: 2018-08-05T23:45:56.513713: step 8030, loss 0.535807.
Test: 2018-08-05T23:45:56.763135: step 8030, loss 0.547637.
Train: 2018-08-05T23:45:56.927534: step 8031, loss 0.544746.
Train: 2018-08-05T23:45:57.103835: step 8032, loss 0.58924.
Train: 2018-08-05T23:45:57.250848: step 8033, loss 0.562451.
Train: 2018-08-05T23:45:57.442208: step 8034, loss 0.624347.
Train: 2018-08-05T23:45:57.618706: step 8035, loss 0.606425.
Train: 2018-08-05T23:45:57.792285: step 8036, loss 0.702249.
Train: 2018-08-05T23:45:57.961026: step 8037, loss 0.545652.
Train: 2018-08-05T23:45:58.139890: step 8038, loss 0.596296.
Train: 2018-08-05T23:45:58.287004: step 8039, loss 0.597294.
Train: 2018-08-05T23:45:58.456267: step 8040, loss 0.511885.
Test: 2018-08-05T23:45:58.706163: step 8040, loss 0.548631.
Train: 2018-08-05T23:45:58.852336: step 8041, loss 0.596951.
Train: 2018-08-05T23:45:59.030931: step 8042, loss 0.630666.
Train: 2018-08-05T23:45:59.194013: step 8043, loss 0.563595.
Train: 2018-08-05T23:45:59.364131: step 8044, loss 0.594737.
Train: 2018-08-05T23:45:59.532980: step 8045, loss 0.512901.
Train: 2018-08-05T23:45:59.695922: step 8046, loss 0.528924.
Train: 2018-08-05T23:45:59.846633: step 8047, loss 0.561979.
Train: 2018-08-05T23:46:00.020049: step 8048, loss 0.514054.
Train: 2018-08-05T23:46:00.178626: step 8049, loss 0.586504.
Train: 2018-08-05T23:46:00.349200: step 8050, loss 0.620322.
Test: 2018-08-05T23:46:00.582760: step 8050, loss 0.548964.
Train: 2018-08-05T23:46:00.740573: step 8051, loss 0.628222.
Train: 2018-08-05T23:46:00.912444: step 8052, loss 0.587582.
Train: 2018-08-05T23:46:01.076832: step 8053, loss 0.515672.
Train: 2018-08-05T23:46:01.234556: step 8054, loss 0.53935.
Train: 2018-08-05T23:46:01.386679: step 8055, loss 0.561992.
Train: 2018-08-05T23:46:01.551296: step 8056, loss 0.56272.
Train: 2018-08-05T23:46:01.711701: step 8057, loss 0.571185.
Train: 2018-08-05T23:46:01.867362: step 8058, loss 0.529663.
Train: 2018-08-05T23:46:02.021739: step 8059, loss 0.554389.
Train: 2018-08-05T23:46:02.193598: step 8060, loss 0.611998.
Test: 2018-08-05T23:46:02.419859: step 8060, loss 0.548689.
Train: 2018-08-05T23:46:02.583744: step 8061, loss 0.587393.
Train: 2018-08-05T23:46:02.754449: step 8062, loss 0.54604.
Train: 2018-08-05T23:46:02.925768: step 8063, loss 0.496281.
Train: 2018-08-05T23:46:03.087335: step 8064, loss 0.529243.
Train: 2018-08-05T23:46:03.245943: step 8065, loss 0.570885.
Train: 2018-08-05T23:46:03.413279: step 8066, loss 0.528941.
Train: 2018-08-05T23:46:03.574092: step 8067, loss 0.528905.
Train: 2018-08-05T23:46:03.721213: step 8068, loss 0.55397.
Train: 2018-08-05T23:46:03.898614: step 8069, loss 0.587804.
Train: 2018-08-05T23:46:04.056286: step 8070, loss 0.604985.
Test: 2018-08-05T23:46:04.301050: step 8070, loss 0.548132.
Train: 2018-08-05T23:46:04.468534: step 8071, loss 0.562338.
Train: 2018-08-05T23:46:04.627925: step 8072, loss 0.536692.
Train: 2018-08-05T23:46:04.798600: step 8073, loss 0.536782.
Train: 2018-08-05T23:46:04.965714: step 8074, loss 0.553901.
Train: 2018-08-05T23:46:05.164183: step 8075, loss 0.588017.
Train: 2018-08-05T23:46:05.341913: step 8076, loss 0.596978.
Train: 2018-08-05T23:46:05.508489: step 8077, loss 0.553805.
Train: 2018-08-05T23:46:05.672167: step 8078, loss 0.501539.
Train: 2018-08-05T23:46:05.826335: step 8079, loss 0.57939.
Train: 2018-08-05T23:46:05.989250: step 8080, loss 0.527482.
Test: 2018-08-05T23:46:06.221266: step 8080, loss 0.547815.
Train: 2018-08-05T23:46:06.394061: step 8081, loss 0.527847.
Train: 2018-08-05T23:46:06.541223: step 8082, loss 0.535558.
Train: 2018-08-05T23:46:06.704753: step 8083, loss 0.606757.
Train: 2018-08-05T23:46:06.885043: step 8084, loss 0.589791.
Train: 2018-08-05T23:46:07.031781: step 8085, loss 0.553392.
Train: 2018-08-05T23:46:07.209997: step 8086, loss 0.510083.
Train: 2018-08-05T23:46:07.358867: step 8087, loss 0.598146.
Train: 2018-08-05T23:46:07.537507: step 8088, loss 0.641921.
Train: 2018-08-05T23:46:07.686354: step 8089, loss 0.518387.
Train: 2018-08-05T23:46:07.855692: step 8090, loss 0.623511.
Test: 2018-08-05T23:46:08.082520: step 8090, loss 0.547786.
Train: 2018-08-05T23:46:08.261885: step 8091, loss 0.510168.
Train: 2018-08-05T23:46:08.440875: step 8092, loss 0.519115.
Train: 2018-08-05T23:46:08.594845: step 8093, loss 0.527204.
Train: 2018-08-05T23:46:08.765110: step 8094, loss 0.510399.
Train: 2018-08-05T23:46:08.939761: step 8095, loss 0.517962.
Train: 2018-08-05T23:46:09.113554: step 8096, loss 0.563019.
Train: 2018-08-05T23:46:09.283750: step 8097, loss 0.607551.
Train: 2018-08-05T23:46:09.466551: step 8098, loss 0.616827.
Train: 2018-08-05T23:46:09.619534: step 8099, loss 0.563074.
Train: 2018-08-05T23:46:09.784593: step 8100, loss 0.483267.
Test: 2018-08-05T23:46:10.084931: step 8100, loss 0.547758.
Train: 2018-08-05T23:46:10.820962: step 8101, loss 0.553927.
Train: 2018-08-05T23:46:10.974818: step 8102, loss 0.606085.
Train: 2018-08-05T23:46:11.142954: step 8103, loss 0.58881.
Train: 2018-08-05T23:46:11.297899: step 8104, loss 0.544911.
Train: 2018-08-05T23:46:11.454113: step 8105, loss 0.527254.
Train: 2018-08-05T23:46:11.602011: step 8106, loss 0.580128.
Train: 2018-08-05T23:46:11.771139: step 8107, loss 0.606142.
Train: 2018-08-05T23:46:11.927387: step 8108, loss 0.61498.
Train: 2018-08-05T23:46:12.091091: step 8109, loss 0.457924.
Train: 2018-08-05T23:46:12.261990: step 8110, loss 0.518977.
Test: 2018-08-05T23:46:12.501869: step 8110, loss 0.547852.
Train: 2018-08-05T23:46:12.690250: step 8111, loss 0.588717.
Train: 2018-08-05T23:46:12.842319: step 8112, loss 0.527573.
Train: 2018-08-05T23:46:13.013946: step 8113, loss 0.60608.
Train: 2018-08-05T23:46:13.176505: step 8114, loss 0.553995.
Train: 2018-08-05T23:46:13.338047: step 8115, loss 0.536495.
Train: 2018-08-05T23:46:13.508278: step 8116, loss 0.571235.
Train: 2018-08-05T23:46:13.652291: step 8117, loss 0.54534.
Train: 2018-08-05T23:46:13.816947: step 8118, loss 0.570933.
Train: 2018-08-05T23:46:13.973161: step 8119, loss 0.553752.
Train: 2018-08-05T23:46:14.136810: step 8120, loss 0.518963.
Test: 2018-08-05T23:46:14.377853: step 8120, loss 0.547814.
Train: 2018-08-05T23:46:14.548149: step 8121, loss 0.509901.
Train: 2018-08-05T23:46:14.711004: step 8122, loss 0.588756.
Train: 2018-08-05T23:46:14.858931: step 8123, loss 0.545167.
Train: 2018-08-05T23:46:15.031244: step 8124, loss 0.518601.
Train: 2018-08-05T23:46:15.179618: step 8125, loss 0.65967.
Train: 2018-08-05T23:46:15.338477: step 8126, loss 0.641366.
Train: 2018-08-05T23:46:15.493663: step 8127, loss 0.527604.
Train: 2018-08-05T23:46:15.664454: step 8128, loss 0.562562.
Train: 2018-08-05T23:46:15.823831: step 8129, loss 0.579641.
Train: 2018-08-05T23:46:15.988585: step 8130, loss 0.640913.
Test: 2018-08-05T23:46:16.211604: step 8130, loss 0.54787.
Train: 2018-08-05T23:46:16.392514: step 8131, loss 0.571078.
Train: 2018-08-05T23:46:16.564055: step 8132, loss 0.597219.
Train: 2018-08-05T23:46:16.729613: step 8133, loss 0.553845.
Train: 2018-08-05T23:46:16.898071: step 8134, loss 0.51092.
Train: 2018-08-05T23:46:17.061666: step 8135, loss 0.588161.
Train: 2018-08-05T23:46:17.227192: step 8136, loss 0.536737.
Train: 2018-08-05T23:46:17.397735: step 8137, loss 0.570793.
Train: 2018-08-05T23:46:17.563255: step 8138, loss 0.52835.
Train: 2018-08-05T23:46:17.726288: step 8139, loss 0.502557.
Train: 2018-08-05T23:46:17.891503: step 8140, loss 0.562177.
Test: 2018-08-05T23:46:18.121822: step 8140, loss 0.548046.
Train: 2018-08-05T23:46:18.297849: step 8141, loss 0.528067.
Train: 2018-08-05T23:46:18.476389: step 8142, loss 0.571217.
Train: 2018-08-05T23:46:18.645109: step 8143, loss 0.553396.
Train: 2018-08-05T23:46:18.806270: step 8144, loss 0.527741.
Train: 2018-08-05T23:46:18.977069: step 8145, loss 0.589757.
Train: 2018-08-05T23:46:19.152624: step 8146, loss 0.650428.
Train: 2018-08-05T23:46:19.313316: step 8147, loss 0.605533.
Train: 2018-08-05T23:46:19.489144: step 8148, loss 0.554075.
Train: 2018-08-05T23:46:19.645739: step 8149, loss 0.588225.
Train: 2018-08-05T23:46:19.817467: step 8150, loss 0.56255.
Test: 2018-08-05T23:46:20.058831: step 8150, loss 0.548129.
Train: 2018-08-05T23:46:20.275251: step 8151, loss 0.55374.
Train: 2018-08-05T23:46:20.487682: step 8152, loss 0.511477.
Train: 2018-08-05T23:46:20.651276: step 8153, loss 0.579353.
Train: 2018-08-05T23:46:20.833756: step 8154, loss 0.63475.
Train: 2018-08-05T23:46:21.028237: step 8155, loss 0.604747.
Train: 2018-08-05T23:46:21.227703: step 8156, loss 0.545653.
Train: 2018-08-05T23:46:21.421185: step 8157, loss 0.536913.
Train: 2018-08-05T23:46:21.582828: step 8158, loss 0.578988.
Train: 2018-08-05T23:46:21.768192: step 8159, loss 0.545261.
Train: 2018-08-05T23:46:21.925471: step 8160, loss 0.48638.
Test: 2018-08-05T23:46:22.176889: step 8160, loss 0.548249.
Train: 2018-08-05T23:46:22.344917: step 8161, loss 0.494744.
Train: 2018-08-05T23:46:22.533379: step 8162, loss 0.562466.
Train: 2018-08-05T23:46:22.694752: step 8163, loss 0.554053.
Train: 2018-08-05T23:46:22.857353: step 8164, loss 0.571268.
Train: 2018-08-05T23:46:23.041859: step 8165, loss 0.475045.
Train: 2018-08-05T23:46:23.204469: step 8166, loss 0.561959.
Train: 2018-08-05T23:46:23.371565: step 8167, loss 0.587928.
Train: 2018-08-05T23:46:23.570041: step 8168, loss 0.46481.
Train: 2018-08-05T23:46:23.744671: step 8169, loss 0.572661.
Train: 2018-08-05T23:46:23.898890: step 8170, loss 0.535691.
Test: 2018-08-05T23:46:24.146345: step 8170, loss 0.547626.
Train: 2018-08-05T23:46:24.343819: step 8171, loss 0.571306.
Train: 2018-08-05T23:46:24.508195: step 8172, loss 0.510773.
Train: 2018-08-05T23:46:24.664413: step 8173, loss 0.562994.
Train: 2018-08-05T23:46:24.869995: step 8174, loss 0.591302.
Train: 2018-08-05T23:46:25.029310: step 8175, loss 0.471436.
Train: 2018-08-05T23:46:25.198045: step 8176, loss 0.53866.
Train: 2018-08-05T23:46:25.354284: step 8177, loss 0.542284.
Train: 2018-08-05T23:46:25.526119: step 8178, loss 0.625154.
Train: 2018-08-05T23:46:25.682333: step 8179, loss 0.626956.
Train: 2018-08-05T23:46:25.838521: step 8180, loss 0.580338.
Test: 2018-08-05T23:46:26.072875: step 8180, loss 0.547621.
Train: 2018-08-05T23:46:26.244676: step 8181, loss 0.582967.
Train: 2018-08-05T23:46:26.408374: step 8182, loss 0.517653.
Train: 2018-08-05T23:46:26.556048: step 8183, loss 0.535912.
Train: 2018-08-05T23:46:26.712261: step 8184, loss 0.645663.
Train: 2018-08-05T23:46:26.868506: step 8185, loss 0.48954.
Train: 2018-08-05T23:46:27.024688: step 8186, loss 0.599666.
Train: 2018-08-05T23:46:27.196523: step 8187, loss 0.526595.
Train: 2018-08-05T23:46:27.352736: step 8188, loss 0.535381.
Train: 2018-08-05T23:46:27.508949: step 8189, loss 0.536848.
Train: 2018-08-05T23:46:27.665197: step 8190, loss 0.571987.
Test: 2018-08-05T23:46:27.899518: step 8190, loss 0.547712.
Train: 2018-08-05T23:46:28.071342: step 8191, loss 0.561565.
Train: 2018-08-05T23:46:28.227530: step 8192, loss 0.552457.
Train: 2018-08-05T23:46:28.383744: step 8193, loss 0.554342.
Train: 2018-08-05T23:46:28.539991: step 8194, loss 0.526658.
Train: 2018-08-05T23:46:28.707848: step 8195, loss 0.545352.
Train: 2018-08-05T23:46:28.884343: step 8196, loss 0.615672.
Train: 2018-08-05T23:46:29.063864: step 8197, loss 0.508544.
Train: 2018-08-05T23:46:29.245379: step 8198, loss 0.668574.
Train: 2018-08-05T23:46:29.422762: step 8199, loss 0.554901.
Train: 2018-08-05T23:46:29.603481: step 8200, loss 0.537092.
Test: 2018-08-05T23:46:29.846831: step 8200, loss 0.547836.
Train: 2018-08-05T23:46:30.561174: step 8201, loss 0.649753.
Train: 2018-08-05T23:46:30.713527: step 8202, loss 0.485126.
Train: 2018-08-05T23:46:30.890585: step 8203, loss 0.631695.
Train: 2018-08-05T23:46:31.064544: step 8204, loss 0.501797.
Train: 2018-08-05T23:46:31.223527: step 8205, loss 0.57083.
Train: 2018-08-05T23:46:31.397274: step 8206, loss 0.59678.
Train: 2018-08-05T23:46:31.564447: step 8207, loss 0.604853.
Train: 2018-08-05T23:46:31.716054: step 8208, loss 0.588185.
Train: 2018-08-05T23:46:31.872906: step 8209, loss 0.587831.
Train: 2018-08-05T23:46:32.036860: step 8210, loss 0.503335.
Test: 2018-08-05T23:46:32.286292: step 8210, loss 0.548373.
Train: 2018-08-05T23:46:32.449386: step 8211, loss 0.52104.
Train: 2018-08-05T23:46:32.611916: step 8212, loss 0.579858.
Train: 2018-08-05T23:46:32.780972: step 8213, loss 0.696545.
Train: 2018-08-05T23:46:32.956576: step 8214, loss 0.553998.
Train: 2018-08-05T23:46:33.114587: step 8215, loss 0.545975.
Train: 2018-08-05T23:46:33.285599: step 8216, loss 0.621029.
Train: 2018-08-05T23:46:33.473121: step 8217, loss 0.570976.
Train: 2018-08-05T23:46:33.641228: step 8218, loss 0.545852.
Train: 2018-08-05T23:46:33.796002: step 8219, loss 0.562897.
Train: 2018-08-05T23:46:33.965917: step 8220, loss 0.570908.
Test: 2018-08-05T23:46:34.203283: step 8220, loss 0.549184.
Train: 2018-08-05T23:46:34.377817: step 8221, loss 0.538772.
Train: 2018-08-05T23:46:34.545985: step 8222, loss 0.579566.
Train: 2018-08-05T23:46:34.698104: step 8223, loss 0.563155.
Train: 2018-08-05T23:46:34.876391: step 8224, loss 0.529409.
Train: 2018-08-05T23:46:35.038861: step 8225, loss 0.53852.
Train: 2018-08-05T23:46:35.210246: step 8226, loss 0.529818.
Train: 2018-08-05T23:46:35.364586: step 8227, loss 0.570531.
Train: 2018-08-05T23:46:35.536450: step 8228, loss 0.521466.
Train: 2018-08-05T23:46:35.704364: step 8229, loss 0.504727.
Train: 2018-08-05T23:46:35.876479: step 8230, loss 0.504267.
Test: 2018-08-05T23:46:36.112357: step 8230, loss 0.548427.
Train: 2018-08-05T23:46:36.281370: step 8231, loss 0.512144.
Train: 2018-08-05T23:46:36.443782: step 8232, loss 0.554356.
Train: 2018-08-05T23:46:36.606375: step 8233, loss 0.579247.
Train: 2018-08-05T23:46:36.785898: step 8234, loss 0.597475.
Train: 2018-08-05T23:46:36.955637: step 8235, loss 0.553112.
Train: 2018-08-05T23:46:37.130214: step 8236, loss 0.554129.
Train: 2018-08-05T23:46:37.277550: step 8237, loss 0.649931.
Train: 2018-08-05T23:46:37.441054: step 8238, loss 0.570845.
Train: 2018-08-05T23:46:37.615148: step 8239, loss 0.553901.
Train: 2018-08-05T23:46:37.763153: step 8240, loss 0.561892.
Test: 2018-08-05T23:46:38.015744: step 8240, loss 0.547818.
Train: 2018-08-05T23:46:38.185279: step 8241, loss 0.641419.
Train: 2018-08-05T23:46:38.359217: step 8242, loss 0.491855.
Train: 2018-08-05T23:46:38.518078: step 8243, loss 0.581008.
Train: 2018-08-05T23:46:38.672734: step 8244, loss 0.555249.
Train: 2018-08-05T23:46:38.845017: step 8245, loss 0.504454.
Train: 2018-08-05T23:46:39.007595: step 8246, loss 0.571194.
Train: 2018-08-05T23:46:39.173153: step 8247, loss 0.526248.
Train: 2018-08-05T23:46:39.334106: step 8248, loss 0.491094.
Train: 2018-08-05T23:46:39.510492: step 8249, loss 0.653283.
Train: 2018-08-05T23:46:39.677329: step 8250, loss 0.526749.
Test: 2018-08-05T23:46:39.911910: step 8250, loss 0.547618.
Train: 2018-08-05T23:46:40.083553: step 8251, loss 0.525298.
Train: 2018-08-05T23:46:40.275077: step 8252, loss 0.544611.
Train: 2018-08-05T23:46:40.460576: step 8253, loss 0.56118.
Train: 2018-08-05T23:46:40.626133: step 8254, loss 0.551861.
Train: 2018-08-05T23:46:40.791670: step 8255, loss 0.549724.
Train: 2018-08-05T23:46:40.954668: step 8256, loss 0.626324.
Train: 2018-08-05T23:46:41.107792: step 8257, loss 0.518359.
Train: 2018-08-05T23:46:41.264740: step 8258, loss 0.610836.
Train: 2018-08-05T23:46:41.444973: step 8259, loss 0.568113.
Train: 2018-08-05T23:46:41.603916: step 8260, loss 0.552162.
Test: 2018-08-05T23:46:41.850014: step 8260, loss 0.547667.
Train: 2018-08-05T23:46:41.997153: step 8261, loss 0.609797.
Train: 2018-08-05T23:46:42.169012: step 8262, loss 0.561527.
Train: 2018-08-05T23:46:42.335914: step 8263, loss 0.604713.
Train: 2018-08-05T23:46:42.529371: step 8264, loss 0.56324.
Train: 2018-08-05T23:46:42.679908: step 8265, loss 0.579746.
Train: 2018-08-05T23:46:42.864067: step 8266, loss 0.562121.
Train: 2018-08-05T23:46:43.029420: step 8267, loss 0.596343.
Train: 2018-08-05T23:46:43.190478: step 8268, loss 0.594064.
Train: 2018-08-05T23:46:43.357742: step 8269, loss 0.593981.
Train: 2018-08-05T23:46:43.509828: step 8270, loss 0.549178.
Test: 2018-08-05T23:46:43.754752: step 8270, loss 0.551125.
Train: 2018-08-05T23:46:43.926425: step 8271, loss 0.571596.
Train: 2018-08-05T23:46:44.092977: step 8272, loss 0.618478.
Train: 2018-08-05T23:46:44.258541: step 8273, loss 0.593627.
Train: 2018-08-05T23:46:44.475928: step 8274, loss 0.524628.
Train: 2018-08-05T23:46:44.670407: step 8275, loss 0.61646.
Train: 2018-08-05T23:46:44.853917: step 8276, loss 0.600174.
Train: 2018-08-05T23:46:45.033436: step 8277, loss 0.560203.
Train: 2018-08-05T23:46:45.206972: step 8278, loss 0.540514.
Train: 2018-08-05T23:46:45.401451: step 8279, loss 0.578641.
Train: 2018-08-05T23:46:45.587953: step 8280, loss 0.548463.
Test: 2018-08-05T23:46:45.837286: step 8280, loss 0.549976.
Train: 2018-08-05T23:46:46.029773: step 8281, loss 0.491349.
Train: 2018-08-05T23:46:46.222294: step 8282, loss 0.580067.
Train: 2018-08-05T23:46:46.415765: step 8283, loss 0.52097.
Train: 2018-08-05T23:46:46.592269: step 8284, loss 0.588136.
Train: 2018-08-05T23:46:46.769093: step 8285, loss 0.528964.
Train: 2018-08-05T23:46:46.977507: step 8286, loss 0.613363.
Train: 2018-08-05T23:46:47.164009: step 8287, loss 0.596645.
Train: 2018-08-05T23:46:47.366470: step 8288, loss 0.579548.
Train: 2018-08-05T23:46:47.558952: step 8289, loss 0.605372.
Train: 2018-08-05T23:46:47.814273: step 8290, loss 0.61341.
Test: 2018-08-05T23:46:48.053630: step 8290, loss 0.548551.
Train: 2018-08-05T23:46:48.243123: step 8291, loss 0.486896.
Train: 2018-08-05T23:46:48.435608: step 8292, loss 0.579759.
Train: 2018-08-05T23:46:48.613133: step 8293, loss 0.571905.
Train: 2018-08-05T23:46:48.783677: step 8294, loss 0.503939.
Train: 2018-08-05T23:46:48.961323: step 8295, loss 0.554613.
Train: 2018-08-05T23:46:49.144832: step 8296, loss 0.60564.
Train: 2018-08-05T23:46:49.320363: step 8297, loss 0.535239.
Train: 2018-08-05T23:46:49.509856: step 8298, loss 0.632838.
Train: 2018-08-05T23:46:49.674417: step 8299, loss 0.56365.
Train: 2018-08-05T23:46:49.839369: step 8300, loss 0.605034.
Test: 2018-08-05T23:46:50.078545: step 8300, loss 0.548785.
Train: 2018-08-05T23:46:50.900221: step 8301, loss 0.569661.
Train: 2018-08-05T23:46:51.086694: step 8302, loss 0.533579.
Train: 2018-08-05T23:46:51.259232: step 8303, loss 0.591418.
Train: 2018-08-05T23:46:51.436759: step 8304, loss 0.479945.
Train: 2018-08-05T23:46:51.606305: step 8305, loss 0.544444.
Train: 2018-08-05T23:46:51.815745: step 8306, loss 0.611095.
Train: 2018-08-05T23:46:52.049121: step 8307, loss 0.508032.
Train: 2018-08-05T23:46:52.245596: step 8308, loss 0.566654.
Train: 2018-08-05T23:46:52.423156: step 8309, loss 0.527596.
Train: 2018-08-05T23:46:52.594661: step 8310, loss 0.571003.
Test: 2018-08-05T23:46:52.843995: step 8310, loss 0.547935.
Train: 2018-08-05T23:46:53.030495: step 8311, loss 0.569735.
Train: 2018-08-05T23:46:53.202059: step 8312, loss 0.5288.
Train: 2018-08-05T23:46:53.398512: step 8313, loss 0.647985.
Train: 2018-08-05T23:46:53.583042: step 8314, loss 0.581907.
Train: 2018-08-05T23:46:53.778505: step 8315, loss 0.539585.
Train: 2018-08-05T23:46:53.977961: step 8316, loss 0.546454.
Train: 2018-08-05T23:46:54.146511: step 8317, loss 0.512169.
Train: 2018-08-05T23:46:54.316057: step 8318, loss 0.58971.
Train: 2018-08-05T23:46:54.494608: step 8319, loss 0.592538.
Train: 2018-08-05T23:46:54.671141: step 8320, loss 0.514466.
Test: 2018-08-05T23:46:54.914458: step 8320, loss 0.549583.
Train: 2018-08-05T23:46:55.099961: step 8321, loss 0.557332.
Train: 2018-08-05T23:46:55.270536: step 8322, loss 0.57454.
Train: 2018-08-05T23:46:55.464986: step 8323, loss 0.57108.
Train: 2018-08-05T23:46:55.671432: step 8324, loss 0.564228.
Train: 2018-08-05T23:46:55.834995: step 8325, loss 0.556167.
Train: 2018-08-05T23:46:56.032470: step 8326, loss 0.56506.
Train: 2018-08-05T23:46:56.214980: step 8327, loss 0.521567.
Train: 2018-08-05T23:46:56.405470: step 8328, loss 0.518968.
Train: 2018-08-05T23:46:56.588981: step 8329, loss 0.484455.
Train: 2018-08-05T23:46:56.755535: step 8330, loss 0.571715.
Test: 2018-08-05T23:46:56.997889: step 8330, loss 0.548405.
Train: 2018-08-05T23:46:57.161474: step 8331, loss 0.498992.
Train: 2018-08-05T23:46:57.320023: step 8332, loss 0.622526.
Train: 2018-08-05T23:46:57.508520: step 8333, loss 0.517438.
Train: 2018-08-05T23:46:57.677069: step 8334, loss 0.518122.
Train: 2018-08-05T23:46:57.852599: step 8335, loss 0.564084.
Train: 2018-08-05T23:46:58.029129: step 8336, loss 0.583822.
Train: 2018-08-05T23:46:58.238573: step 8337, loss 0.556986.
Train: 2018-08-05T23:46:58.411106: step 8338, loss 0.599856.
Train: 2018-08-05T23:46:58.575692: step 8339, loss 0.616366.
Train: 2018-08-05T23:46:58.731276: step 8340, loss 0.550986.
Test: 2018-08-05T23:46:58.978588: step 8340, loss 0.554634.
Train: 2018-08-05T23:46:59.175090: step 8341, loss 0.538361.
Train: 2018-08-05T23:46:59.344609: step 8342, loss 0.586516.
Train: 2018-08-05T23:46:59.510188: step 8343, loss 0.608929.
Train: 2018-08-05T23:46:59.686695: step 8344, loss 0.645466.
Train: 2018-08-05T23:46:59.880179: step 8345, loss 0.538171.
Train: 2018-08-05T23:47:00.051719: step 8346, loss 0.582677.
Train: 2018-08-05T23:47:00.219296: step 8347, loss 0.479778.
Train: 2018-08-05T23:47:00.384828: step 8348, loss 0.592593.
Train: 2018-08-05T23:47:00.568337: step 8349, loss 0.531524.
Train: 2018-08-05T23:47:00.735888: step 8350, loss 0.488552.
Test: 2018-08-05T23:47:00.977244: step 8350, loss 0.551359.
Train: 2018-08-05T23:47:01.154800: step 8351, loss 0.475934.
Train: 2018-08-05T23:47:01.333291: step 8352, loss 0.697839.
Train: 2018-08-05T23:47:01.503835: step 8353, loss 0.539168.
Train: 2018-08-05T23:47:01.714272: step 8354, loss 0.60177.
Train: 2018-08-05T23:47:01.904764: step 8355, loss 0.602638.
Train: 2018-08-05T23:47:02.082316: step 8356, loss 0.626612.
Train: 2018-08-05T23:47:02.258817: step 8357, loss 0.67234.
Train: 2018-08-05T23:47:02.433349: step 8358, loss 0.595195.
Train: 2018-08-05T23:47:02.617857: step 8359, loss 0.662369.
Train: 2018-08-05T23:47:02.793386: step 8360, loss 0.658137.
Test: 2018-08-05T23:47:03.041754: step 8360, loss 0.605273.
Train: 2018-08-05T23:47:03.212267: step 8361, loss 0.68634.
Train: 2018-08-05T23:47:03.387797: step 8362, loss 0.662759.
Train: 2018-08-05T23:47:03.560336: step 8363, loss 0.758264.
Train: 2018-08-05T23:47:03.739855: step 8364, loss 0.714584.
Train: 2018-08-05T23:47:03.916384: step 8365, loss 0.662269.
Train: 2018-08-05T23:47:04.090948: step 8366, loss 0.668587.
Train: 2018-08-05T23:47:04.266468: step 8367, loss 0.596721.
Train: 2018-08-05T23:47:04.445983: step 8368, loss 0.736187.
Train: 2018-08-05T23:47:04.627510: step 8369, loss 0.711955.
Train: 2018-08-05T23:47:04.805032: step 8370, loss 0.680434.
Test: 2018-08-05T23:47:05.044367: step 8370, loss 0.661691.
Train: 2018-08-05T23:47:05.238846: step 8371, loss 0.691618.
Train: 2018-08-05T23:47:05.417398: step 8372, loss 0.772276.
Train: 2018-08-05T23:47:05.596890: step 8373, loss 0.721683.
Train: 2018-08-05T23:47:05.791401: step 8374, loss 0.65102.
Train: 2018-08-05T23:47:05.972917: step 8375, loss 0.709676.
Train: 2018-08-05T23:47:06.153401: step 8376, loss 0.675289.
Train: 2018-08-05T23:47:06.334937: step 8377, loss 0.831978.
Train: 2018-08-05T23:47:06.518426: step 8378, loss 0.7617.
Train: 2018-08-05T23:47:06.696948: step 8379, loss 0.73871.
Train: 2018-08-05T23:47:06.877493: step 8380, loss 0.750611.
Test: 2018-08-05T23:47:07.115853: step 8380, loss 0.707479.
Train: 2018-08-05T23:47:07.302330: step 8381, loss 0.715588.
Train: 2018-08-05T23:47:07.491850: step 8382, loss 0.834944.
Train: 2018-08-05T23:47:07.670358: step 8383, loss 0.782127.
Train: 2018-08-05T23:47:07.850887: step 8384, loss 0.797113.
Train: 2018-08-05T23:47:08.036364: step 8385, loss 0.738253.
Train: 2018-08-05T23:47:08.229847: step 8386, loss 0.692127.
Train: 2018-08-05T23:47:08.409368: step 8387, loss 0.650359.
Train: 2018-08-05T23:47:08.588888: step 8388, loss 0.775065.
Train: 2018-08-05T23:47:08.773393: step 8389, loss 0.734984.
Train: 2018-08-05T23:47:08.963919: step 8390, loss 0.74261.
Test: 2018-08-05T23:47:09.203245: step 8390, loss 0.739791.
Train: 2018-08-05T23:47:09.394734: step 8391, loss 0.782112.
Train: 2018-08-05T23:47:09.574278: step 8392, loss 0.748037.
Train: 2018-08-05T23:47:09.754803: step 8393, loss 0.764333.
Train: 2018-08-05T23:47:09.939277: step 8394, loss 0.757143.
Train: 2018-08-05T23:47:10.129801: step 8395, loss 0.699301.
Train: 2018-08-05T23:47:10.311282: step 8396, loss 0.769241.
Train: 2018-08-05T23:47:10.505760: step 8397, loss 0.715954.
Train: 2018-08-05T23:47:10.690294: step 8398, loss 0.73508.
Train: 2018-08-05T23:47:10.872811: step 8399, loss 0.75322.
Train: 2018-08-05T23:47:11.053332: step 8400, loss 0.67167.
Test: 2018-08-05T23:47:11.295650: step 8400, loss 0.697166.
Train: 2018-08-05T23:47:12.109532: step 8401, loss 0.706778.
Train: 2018-08-05T23:47:12.291014: step 8402, loss 0.767088.
Train: 2018-08-05T23:47:12.473525: step 8403, loss 0.748618.
Train: 2018-08-05T23:47:12.655068: step 8404, loss 0.691673.
Train: 2018-08-05T23:47:12.840575: step 8405, loss 0.747804.
Train: 2018-08-05T23:47:13.018099: step 8406, loss 0.661738.
Train: 2018-08-05T23:47:13.199615: step 8407, loss 0.747852.
Train: 2018-08-05T23:47:13.368160: step 8408, loss 0.831521.
Train: 2018-08-05T23:47:13.558743: step 8409, loss 0.753916.
Train: 2018-08-05T23:47:13.724913: step 8410, loss 0.741041.
Test: 2018-08-05T23:47:13.977249: step 8410, loss 0.700226.
Train: 2018-08-05T23:47:14.163182: step 8411, loss 0.637457.
Train: 2018-08-05T23:47:14.333117: step 8412, loss 0.740928.
Train: 2018-08-05T23:47:14.500852: step 8413, loss 0.856066.
Train: 2018-08-05T23:47:14.688306: step 8414, loss 0.677693.
Train: 2018-08-05T23:47:14.862698: step 8415, loss 0.792888.
Train: 2018-08-05T23:47:15.049290: step 8416, loss 0.693576.
Train: 2018-08-05T23:47:15.221123: step 8417, loss 0.674814.
Train: 2018-08-05T23:47:15.408581: step 8418, loss 0.740114.
Train: 2018-08-05T23:47:15.592981: step 8419, loss 0.883061.
Train: 2018-08-05T23:47:15.783494: step 8420, loss 0.720652.
Test: 2018-08-05T23:47:16.017298: step 8420, loss 0.685879.
Train: 2018-08-05T23:47:16.204780: step 8421, loss 0.716716.
Train: 2018-08-05T23:47:16.376589: step 8422, loss 0.668474.
Train: 2018-08-05T23:47:16.567858: step 8423, loss 0.643851.
Train: 2018-08-05T23:47:16.758578: step 8424, loss 0.74287.
Train: 2018-08-05T23:47:16.954269: step 8425, loss 0.677948.
Train: 2018-08-05T23:47:17.150982: step 8426, loss 0.725862.
Train: 2018-08-05T23:47:17.331525: step 8427, loss 0.71423.
Train: 2018-08-05T23:47:17.527114: step 8428, loss 0.758498.
Train: 2018-08-05T23:47:17.717791: step 8429, loss 0.745317.
Train: 2018-08-05T23:47:17.904355: step 8430, loss 0.664043.
Test: 2018-08-05T23:47:18.140617: step 8430, loss 0.673241.
Train: 2018-08-05T23:47:18.312447: step 8431, loss 0.744319.
Train: 2018-08-05T23:47:18.518215: step 8432, loss 0.708562.
Train: 2018-08-05T23:47:18.712680: step 8433, loss 0.657558.
Train: 2018-08-05T23:47:18.884348: step 8434, loss 0.671522.
Train: 2018-08-05T23:47:19.091198: step 8435, loss 0.719392.
Train: 2018-08-05T23:47:19.263461: step 8436, loss 0.771252.
Train: 2018-08-05T23:47:19.461107: step 8437, loss 0.691621.
Train: 2018-08-05T23:47:19.644730: step 8438, loss 0.666169.
Train: 2018-08-05T23:47:19.831078: step 8439, loss 0.705077.
Train: 2018-08-05T23:47:20.015706: step 8440, loss 0.707474.
Test: 2018-08-05T23:47:20.255611: step 8440, loss 0.652552.
Train: 2018-08-05T23:47:20.424042: step 8441, loss 0.569324.
Train: 2018-08-05T23:47:20.609667: step 8442, loss 0.741006.
Train: 2018-08-05T23:47:20.797123: step 8443, loss 0.610347.
Train: 2018-08-05T23:47:20.982611: step 8444, loss 0.654672.
Train: 2018-08-05T23:47:21.174099: step 8445, loss 0.571385.
Train: 2018-08-05T23:47:21.365068: step 8446, loss 0.713433.
Train: 2018-08-05T23:47:21.551317: step 8447, loss 0.666575.
Train: 2018-08-05T23:47:21.734430: step 8448, loss 0.687491.
Train: 2018-08-05T23:47:21.934216: step 8449, loss 0.674019.
Train: 2018-08-05T23:47:22.122178: step 8450, loss 0.69026.
Test: 2018-08-05T23:47:22.361982: step 8450, loss 0.647431.
Train: 2018-08-05T23:47:22.559200: step 8451, loss 0.632278.
Train: 2018-08-05T23:47:22.742718: step 8452, loss 0.541092.
Train: 2018-08-05T23:47:22.931068: step 8453, loss 0.939328.
Train: 2018-08-05T23:47:23.115651: step 8454, loss 0.689729.
Train: 2018-08-05T23:47:23.317960: step 8455, loss 0.675696.
Train: 2018-08-05T23:47:23.512440: step 8456, loss 0.736407.
Train: 2018-08-05T23:47:23.710909: step 8457, loss 0.601497.
Train: 2018-08-05T23:47:23.902424: step 8458, loss 0.667967.
Train: 2018-08-05T23:47:24.088898: step 8459, loss 0.744728.
Train: 2018-08-05T23:47:24.285399: step 8460, loss 0.608636.
Test: 2018-08-05T23:47:24.527725: step 8460, loss 0.682927.
Train: 2018-08-05T23:47:24.699166: step 8461, loss 0.711411.
Train: 2018-08-05T23:47:24.905077: step 8462, loss 0.631526.
Train: 2018-08-05T23:47:25.106376: step 8463, loss 0.751684.
Train: 2018-08-05T23:47:25.282970: step 8464, loss 0.660484.
Train: 2018-08-05T23:47:25.465939: step 8465, loss 0.650476.
Train: 2018-08-05T23:47:25.652077: step 8466, loss 0.722524.
Train: 2018-08-05T23:47:25.855213: step 8467, loss 0.699782.
Train: 2018-08-05T23:47:26.041755: step 8468, loss 0.741248.
Train: 2018-08-05T23:47:26.223040: step 8469, loss 0.72809.
Train: 2018-08-05T23:47:26.429325: step 8470, loss 0.66279.
Test: 2018-08-05T23:47:26.656562: step 8470, loss 0.678051.
Train: 2018-08-05T23:47:26.851340: step 8471, loss 0.690376.
Train: 2018-08-05T23:47:27.054068: step 8472, loss 0.685436.
Train: 2018-08-05T23:47:27.250574: step 8473, loss 0.716147.
Train: 2018-08-05T23:47:27.429945: step 8474, loss 0.739468.
Train: 2018-08-05T23:47:27.607517: step 8475, loss 0.771107.
Train: 2018-08-05T23:47:27.790343: step 8476, loss 0.655686.
Train: 2018-08-05T23:47:27.982237: step 8477, loss 0.67586.
Train: 2018-08-05T23:47:28.146463: step 8478, loss 0.636795.
Train: 2018-08-05T23:47:28.347495: step 8479, loss 0.719105.
Train: 2018-08-05T23:47:28.530760: step 8480, loss 0.735708.
Test: 2018-08-05T23:47:28.765106: step 8480, loss 0.662219.
Train: 2018-08-05T23:47:28.950193: step 8481, loss 0.631598.
Train: 2018-08-05T23:47:29.129203: step 8482, loss 0.667978.
Train: 2018-08-05T23:47:29.325467: step 8483, loss 0.782335.
Train: 2018-08-05T23:47:29.522540: step 8484, loss 0.670271.
Train: 2018-08-05T23:47:29.694375: step 8485, loss 0.741275.
Train: 2018-08-05T23:47:29.886586: step 8486, loss 0.675616.
Train: 2018-08-05T23:47:30.066925: step 8487, loss 0.633308.
Train: 2018-08-05T23:47:30.238760: step 8488, loss 0.5995.
Train: 2018-08-05T23:47:30.429952: step 8489, loss 0.738284.
Train: 2018-08-05T23:47:30.629402: step 8490, loss 0.676912.
Test: 2018-08-05T23:47:30.860598: step 8490, loss 0.649038.
Train: 2018-08-05T23:47:31.044801: step 8491, loss 0.656013.
Train: 2018-08-05T23:47:31.232265: step 8492, loss 0.724336.
Train: 2018-08-05T23:47:31.404096: step 8493, loss 0.579309.
Train: 2018-08-05T23:47:31.581701: step 8494, loss 0.682854.
Train: 2018-08-05T23:47:31.769187: step 8495, loss 0.739632.
Train: 2018-08-05T23:47:31.961284: step 8496, loss 0.696407.
Train: 2018-08-05T23:47:32.152072: step 8497, loss 0.694953.
Train: 2018-08-05T23:47:32.343526: step 8498, loss 0.646433.
Train: 2018-08-05T23:47:32.533393: step 8499, loss 0.618575.
Train: 2018-08-05T23:47:32.721376: step 8500, loss 0.633507.
Test: 2018-08-05T23:47:32.962530: step 8500, loss 0.64162.
Train: 2018-08-05T23:47:33.711598: step 8501, loss 0.594498.
Train: 2018-08-05T23:47:33.890674: step 8502, loss 0.651997.
Train: 2018-08-05T23:47:34.075774: step 8503, loss 0.62781.
Train: 2018-08-05T23:47:34.263235: step 8504, loss 0.605827.
Train: 2018-08-05T23:47:34.435070: step 8505, loss 0.652339.
Train: 2018-08-05T23:47:34.622496: step 8506, loss 0.641151.
Train: 2018-08-05T23:47:34.794331: step 8507, loss 0.575051.
Train: 2018-08-05T23:47:34.986748: step 8508, loss 0.624839.
Train: 2018-08-05T23:47:35.177607: step 8509, loss 0.69563.
Train: 2018-08-05T23:47:35.343923: step 8510, loss 0.713274.
Test: 2018-08-05T23:47:35.590807: step 8510, loss 0.63128.
Train: 2018-08-05T23:47:35.778263: step 8511, loss 0.749652.
Train: 2018-08-05T23:47:35.948198: step 8512, loss 0.610498.
Train: 2018-08-05T23:47:36.135628: step 8513, loss 0.592497.
Train: 2018-08-05T23:47:36.323109: step 8514, loss 0.69042.
Train: 2018-08-05T23:47:36.506681: step 8515, loss 0.658089.
Train: 2018-08-05T23:47:36.689671: step 8516, loss 0.605571.
Train: 2018-08-05T23:47:36.884259: step 8517, loss 0.657212.
Train: 2018-08-05T23:47:37.069418: step 8518, loss 0.677761.
Train: 2018-08-05T23:47:37.257247: step 8519, loss 0.575369.
Train: 2018-08-05T23:47:37.446771: step 8520, loss 0.551256.
Test: 2018-08-05T23:47:37.690078: step 8520, loss 0.622809.
Train: 2018-08-05T23:47:37.883562: step 8521, loss 0.676778.
Train: 2018-08-05T23:47:38.059518: step 8522, loss 0.691478.
Train: 2018-08-05T23:47:38.228409: step 8523, loss 0.614606.
Train: 2018-08-05T23:47:38.412574: step 8524, loss 0.610643.
Train: 2018-08-05T23:47:38.584256: step 8525, loss 0.639822.
Train: 2018-08-05T23:47:38.788336: step 8526, loss 0.665211.
Train: 2018-08-05T23:47:38.961483: step 8527, loss 0.648592.
Train: 2018-08-05T23:47:39.148939: step 8528, loss 0.663838.
Train: 2018-08-05T23:47:39.320804: step 8529, loss 0.636524.
Train: 2018-08-05T23:47:39.519798: step 8530, loss 0.617067.
Test: 2018-08-05T23:47:39.759977: step 8530, loss 0.622672.
Train: 2018-08-05T23:47:39.945539: step 8531, loss 0.689129.
Train: 2018-08-05T23:47:40.129084: step 8532, loss 0.578909.
Train: 2018-08-05T23:47:40.316574: step 8533, loss 0.700271.
Train: 2018-08-05T23:47:40.488374: step 8534, loss 0.602117.
Train: 2018-08-05T23:47:40.675860: step 8535, loss 0.622632.
Train: 2018-08-05T23:47:40.863316: step 8536, loss 0.676848.
Train: 2018-08-05T23:47:41.042996: step 8537, loss 0.56755.
Train: 2018-08-05T23:47:41.227315: step 8538, loss 0.593885.
Train: 2018-08-05T23:47:41.411322: step 8539, loss 0.646789.
Train: 2018-08-05T23:47:41.617090: step 8540, loss 0.632745.
Test: 2018-08-05T23:47:41.860409: step 8540, loss 0.610576.
Train: 2018-08-05T23:47:42.062868: step 8541, loss 0.578003.
Train: 2018-08-05T23:47:42.260340: step 8542, loss 0.565754.
Train: 2018-08-05T23:47:42.447505: step 8543, loss 0.580229.
Train: 2018-08-05T23:47:42.626759: step 8544, loss 0.665701.
Train: 2018-08-05T23:47:42.811554: step 8545, loss 0.668954.
Train: 2018-08-05T23:47:43.035980: step 8546, loss 0.578231.
Train: 2018-08-05T23:47:43.240407: step 8547, loss 0.647741.
Train: 2018-08-05T23:47:43.447854: step 8548, loss 0.56739.
Train: 2018-08-05T23:47:43.625369: step 8549, loss 0.620541.
Train: 2018-08-05T23:47:43.813390: step 8550, loss 0.615878.
Test: 2018-08-05T23:47:44.059369: step 8550, loss 0.604002.
Train: 2018-08-05T23:47:44.260489: step 8551, loss 0.726198.
Train: 2018-08-05T23:47:44.458959: step 8552, loss 0.607824.
Train: 2018-08-05T23:47:44.647169: step 8553, loss 0.719256.
Train: 2018-08-05T23:47:44.823293: step 8554, loss 0.689242.
Train: 2018-08-05T23:47:45.006241: step 8555, loss 0.639294.
Train: 2018-08-05T23:47:45.195384: step 8556, loss 0.623229.
Train: 2018-08-05T23:47:45.386579: step 8557, loss 0.545858.
Train: 2018-08-05T23:47:45.563246: step 8558, loss 0.567318.
Train: 2018-08-05T23:47:45.728518: step 8559, loss 0.52847.
Train: 2018-08-05T23:47:45.911544: step 8560, loss 0.624302.
Test: 2018-08-05T23:47:46.164568: step 8560, loss 0.599292.
Train: 2018-08-05T23:47:46.344149: step 8561, loss 0.55286.
Train: 2018-08-05T23:47:46.528411: step 8562, loss 0.662586.
Train: 2018-08-05T23:47:46.707716: step 8563, loss 0.640002.
Train: 2018-08-05T23:47:46.899477: step 8564, loss 0.623753.
Train: 2018-08-05T23:47:47.063074: step 8565, loss 0.711585.
Train: 2018-08-05T23:47:47.243293: step 8566, loss 0.697688.
Train: 2018-08-05T23:47:47.451483: step 8567, loss 0.632943.
Train: 2018-08-05T23:47:47.631222: step 8568, loss 0.652683.
Train: 2018-08-05T23:47:47.826284: step 8569, loss 0.594998.
Train: 2018-08-05T23:47:47.994788: step 8570, loss 0.687341.
Test: 2018-08-05T23:47:48.238310: step 8570, loss 0.592715.
Train: 2018-08-05T23:47:48.424896: step 8571, loss 0.591036.
Train: 2018-08-05T23:47:48.605050: step 8572, loss 0.580417.
Train: 2018-08-05T23:47:48.776885: step 8573, loss 0.687739.
Train: 2018-08-05T23:47:48.945945: step 8574, loss 0.612582.
Train: 2018-08-05T23:47:49.136051: step 8575, loss 0.581273.
Train: 2018-08-05T23:47:49.302503: step 8576, loss 0.667854.
Train: 2018-08-05T23:47:49.493821: step 8577, loss 0.637523.
Train: 2018-08-05T23:47:49.662528: step 8578, loss 0.534335.
Train: 2018-08-05T23:47:49.845482: step 8579, loss 0.595958.
Train: 2018-08-05T23:47:50.037591: step 8580, loss 0.538686.
Test: 2018-08-05T23:47:50.277316: step 8580, loss 0.588433.
Train: 2018-08-05T23:47:50.460458: step 8581, loss 0.617598.
Train: 2018-08-05T23:47:50.632917: step 8582, loss 0.651609.
Train: 2018-08-05T23:47:50.811774: step 8583, loss 0.617947.
Train: 2018-08-05T23:47:50.990035: step 8584, loss 0.593116.
Train: 2018-08-05T23:47:51.168359: step 8585, loss 0.574533.
Train: 2018-08-05T23:47:51.331863: step 8586, loss 0.617736.
Train: 2018-08-05T23:47:51.519543: step 8587, loss 0.565523.
Train: 2018-08-05T23:47:51.698705: step 8588, loss 0.571074.
Train: 2018-08-05T23:47:51.888971: step 8589, loss 0.570515.
Train: 2018-08-05T23:47:52.067081: step 8590, loss 0.621038.
Test: 2018-08-05T23:47:52.309778: step 8590, loss 0.582433.
Train: 2018-08-05T23:47:52.496509: step 8591, loss 0.632513.
Train: 2018-08-05T23:47:52.676028: step 8592, loss 0.535833.
Train: 2018-08-05T23:47:52.865490: step 8593, loss 0.605223.
Train: 2018-08-05T23:47:53.058973: step 8594, loss 0.585023.
Train: 2018-08-05T23:47:53.248466: step 8595, loss 0.675666.
Train: 2018-08-05T23:47:53.418082: step 8596, loss 0.588924.
Train: 2018-08-05T23:47:53.596973: step 8597, loss 0.605858.
Train: 2018-08-05T23:47:53.786078: step 8598, loss 0.635992.
Train: 2018-08-05T23:47:53.993144: step 8599, loss 0.567834.
Train: 2018-08-05T23:47:54.171633: step 8600, loss 0.576321.
Test: 2018-08-05T23:47:54.412631: step 8600, loss 0.578416.
Train: 2018-08-05T23:47:55.157746: step 8601, loss 0.574918.
Train: 2018-08-05T23:47:55.329612: step 8602, loss 0.585893.
Train: 2018-08-05T23:47:55.509812: step 8603, loss 0.574381.
Train: 2018-08-05T23:47:55.681676: step 8604, loss 0.594427.
Train: 2018-08-05T23:47:55.851543: step 8605, loss 0.459748.
Train: 2018-08-05T23:47:56.038127: step 8606, loss 0.575845.
Train: 2018-08-05T23:47:56.204712: step 8607, loss 0.678423.
Train: 2018-08-05T23:47:56.375695: step 8608, loss 0.620312.
Train: 2018-08-05T23:47:56.547500: step 8609, loss 0.572721.
Train: 2018-08-05T23:47:56.719366: step 8610, loss 0.612907.
Test: 2018-08-05T23:47:56.968367: step 8610, loss 0.582595.
Train: 2018-08-05T23:47:57.142067: step 8611, loss 0.554333.
Train: 2018-08-05T23:47:57.312407: step 8612, loss 0.641851.
Train: 2018-08-05T23:47:57.497083: step 8613, loss 0.607219.
Train: 2018-08-05T23:47:57.655177: step 8614, loss 0.668774.
Train: 2018-08-05T23:47:57.834056: step 8615, loss 0.68084.
Train: 2018-08-05T23:47:58.014476: step 8616, loss 0.575408.
Train: 2018-08-05T23:47:58.172904: step 8617, loss 0.576462.
Train: 2018-08-05T23:47:58.351752: step 8618, loss 0.587163.
Train: 2018-08-05T23:47:58.530568: step 8619, loss 0.639579.
Train: 2018-08-05T23:47:58.700528: step 8620, loss 0.618013.
Test: 2018-08-05T23:47:58.927630: step 8620, loss 0.587088.
Train: 2018-08-05T23:47:59.106464: step 8621, loss 0.627903.
Train: 2018-08-05T23:47:59.281477: step 8622, loss 0.612093.
Train: 2018-08-05T23:47:59.436211: step 8623, loss 0.635225.
Train: 2018-08-05T23:47:59.618198: step 8624, loss 0.617776.
Train: 2018-08-05T23:47:59.778239: step 8625, loss 0.572491.
Train: 2018-08-05T23:47:59.947171: step 8626, loss 0.585763.
Train: 2018-08-05T23:48:00.122682: step 8627, loss 0.586642.
Train: 2018-08-05T23:48:00.287130: step 8628, loss 0.584285.
Train: 2018-08-05T23:48:00.463877: step 8629, loss 0.612693.
Train: 2018-08-05T23:48:00.637867: step 8630, loss 0.574552.
Test: 2018-08-05T23:48:00.863953: step 8630, loss 0.583564.
Train: 2018-08-05T23:48:01.043338: step 8631, loss 0.580841.
Train: 2018-08-05T23:48:01.207198: step 8632, loss 0.616256.
Train: 2018-08-05T23:48:01.370356: step 8633, loss 0.58234.
Train: 2018-08-05T23:48:01.548860: step 8634, loss 0.632838.
Train: 2018-08-05T23:48:01.717980: step 8635, loss 0.564922.
Train: 2018-08-05T23:48:01.891331: step 8636, loss 0.587193.
Train: 2018-08-05T23:48:02.053454: step 8637, loss 0.598582.
Train: 2018-08-05T23:48:02.232275: step 8638, loss 0.613064.
Train: 2018-08-05T23:48:02.409762: step 8639, loss 0.557205.
Train: 2018-08-05T23:48:02.565970: step 8640, loss 0.599912.
Test: 2018-08-05T23:48:02.813731: step 8640, loss 0.577748.
Train: 2018-08-05T23:48:02.985792: step 8641, loss 0.558616.
Train: 2018-08-05T23:48:03.162316: step 8642, loss 0.571301.
Train: 2018-08-05T23:48:03.321048: step 8643, loss 0.61568.
Train: 2018-08-05T23:48:03.497120: step 8644, loss 0.54032.
Train: 2018-08-05T23:48:03.648205: step 8645, loss 0.635902.
Train: 2018-08-05T23:48:03.820038: step 8646, loss 0.573208.
Train: 2018-08-05T23:48:03.983527: step 8647, loss 0.626354.
Train: 2018-08-05T23:48:04.166864: step 8648, loss 0.571509.
Train: 2018-08-05T23:48:04.320530: step 8649, loss 0.627041.
Train: 2018-08-05T23:48:04.490936: step 8650, loss 0.64086.
Test: 2018-08-05T23:48:04.735167: step 8650, loss 0.574232.
Train: 2018-08-05T23:48:04.911530: step 8651, loss 0.562828.
Train: 2018-08-05T23:48:05.075677: step 8652, loss 0.596229.
Train: 2018-08-05T23:48:05.241493: step 8653, loss 0.598103.
Train: 2018-08-05T23:48:05.404347: step 8654, loss 0.571615.
Train: 2018-08-05T23:48:05.576182: step 8655, loss 0.521194.
Train: 2018-08-05T23:48:05.731955: step 8656, loss 0.595617.
Train: 2018-08-05T23:48:05.894867: step 8657, loss 0.534896.
Train: 2018-08-05T23:48:06.077184: step 8658, loss 0.593964.
Train: 2018-08-05T23:48:06.245765: step 8659, loss 0.46816.
Train: 2018-08-05T23:48:06.394388: step 8660, loss 0.60375.
Test: 2018-08-05T23:48:06.638936: step 8660, loss 0.571012.
Train: 2018-08-05T23:48:06.820612: step 8661, loss 0.535341.
Train: 2018-08-05T23:48:06.983412: step 8662, loss 0.663892.
Train: 2018-08-05T23:48:07.144267: step 8663, loss 0.568415.
Train: 2018-08-05T23:48:07.307476: step 8664, loss 0.586458.
Train: 2018-08-05T23:48:07.463659: step 8665, loss 0.609651.
Train: 2018-08-05T23:48:07.619872: step 8666, loss 0.619464.
Train: 2018-08-05T23:48:07.792684: step 8667, loss 0.532895.
Train: 2018-08-05T23:48:07.961845: step 8668, loss 0.505506.
Train: 2018-08-05T23:48:08.116815: step 8669, loss 0.610632.
Train: 2018-08-05T23:48:08.291229: step 8670, loss 0.599918.
Test: 2018-08-05T23:48:08.525582: step 8670, loss 0.568425.
Train: 2018-08-05T23:48:08.704713: step 8671, loss 0.609592.
Train: 2018-08-05T23:48:08.859654: step 8672, loss 0.558478.
Train: 2018-08-05T23:48:09.031490: step 8673, loss 0.572299.
Train: 2018-08-05T23:48:09.195279: step 8674, loss 0.551188.
Train: 2018-08-05T23:48:09.358809: step 8675, loss 0.581954.
Train: 2018-08-05T23:48:09.537955: step 8676, loss 0.4588.
Train: 2018-08-05T23:48:09.701484: step 8677, loss 0.602684.
Train: 2018-08-05T23:48:09.865023: step 8678, loss 0.660019.
Train: 2018-08-05T23:48:10.090164: step 8679, loss 0.529445.
Train: 2018-08-05T23:48:10.253716: step 8680, loss 0.638938.
Test: 2018-08-05T23:48:10.495368: step 8680, loss 0.570982.
Train: 2018-08-05T23:48:10.643337: step 8681, loss 0.655177.
Train: 2018-08-05T23:48:10.822444: step 8682, loss 0.557675.
Train: 2018-08-05T23:48:10.993272: step 8683, loss 0.627517.
Train: 2018-08-05T23:48:11.158445: step 8684, loss 0.641903.
Train: 2018-08-05T23:48:11.330282: step 8685, loss 0.540434.
Train: 2018-08-05T23:48:11.493303: step 8686, loss 0.534699.
Train: 2018-08-05T23:48:11.675161: step 8687, loss 0.684609.
Train: 2018-08-05T23:48:11.822417: step 8688, loss 0.592164.
Train: 2018-08-05T23:48:12.000874: step 8689, loss 0.671868.
Train: 2018-08-05T23:48:12.157118: step 8690, loss 0.532874.
Test: 2018-08-05T23:48:12.412987: step 8690, loss 0.569896.
Train: 2018-08-05T23:48:12.577361: step 8691, loss 0.60948.
Train: 2018-08-05T23:48:12.740569: step 8692, loss 0.574957.
Train: 2018-08-05T23:48:12.899426: step 8693, loss 0.577731.
Train: 2018-08-05T23:48:13.069531: step 8694, loss 0.584425.
Train: 2018-08-05T23:48:13.242040: step 8695, loss 0.610193.
Train: 2018-08-05T23:48:13.400048: step 8696, loss 0.614565.
Train: 2018-08-05T23:48:13.556262: step 8697, loss 0.534531.
Train: 2018-08-05T23:48:13.728097: step 8698, loss 0.515667.
Train: 2018-08-05T23:48:13.892595: step 8699, loss 0.573717.
Train: 2018-08-05T23:48:14.067081: step 8700, loss 0.556452.
Test: 2018-08-05T23:48:14.305620: step 8700, loss 0.567205.
Train: 2018-08-05T23:48:15.021936: step 8701, loss 0.548272.
Train: 2018-08-05T23:48:15.191918: step 8702, loss 0.57365.
Train: 2018-08-05T23:48:15.370729: step 8703, loss 0.615378.
Train: 2018-08-05T23:48:15.533886: step 8704, loss 0.553519.
Train: 2018-08-05T23:48:15.699292: step 8705, loss 0.563124.
Train: 2018-08-05T23:48:15.855536: step 8706, loss 0.588942.
Train: 2018-08-05T23:48:16.030921: step 8707, loss 0.631728.
Train: 2018-08-05T23:48:16.197476: step 8708, loss 0.581907.
Train: 2018-08-05T23:48:16.363798: step 8709, loss 0.577321.
Train: 2018-08-05T23:48:16.519980: step 8710, loss 0.602849.
Test: 2018-08-05T23:48:16.767332: step 8710, loss 0.564897.
Train: 2018-08-05T23:48:16.939820: step 8711, loss 0.595619.
Train: 2018-08-05T23:48:17.105882: step 8712, loss 0.543696.
Train: 2018-08-05T23:48:17.265451: step 8713, loss 0.596827.
Train: 2018-08-05T23:48:17.422705: step 8714, loss 0.641805.
Train: 2018-08-05T23:48:17.585493: step 8715, loss 0.572069.
Train: 2018-08-05T23:48:17.763921: step 8716, loss 0.517196.
Train: 2018-08-05T23:48:17.930157: step 8717, loss 0.561421.
Train: 2018-08-05T23:48:18.113083: step 8718, loss 0.569562.
Train: 2018-08-05T23:48:18.265200: step 8719, loss 0.492801.
Train: 2018-08-05T23:48:18.443668: step 8720, loss 0.642504.
Test: 2018-08-05T23:48:18.669087: step 8720, loss 0.564046.
Train: 2018-08-05T23:48:18.847652: step 8721, loss 0.558291.
Train: 2018-08-05T23:48:19.010938: step 8722, loss 0.582389.
Train: 2018-08-05T23:48:19.185976: step 8723, loss 0.565101.
Train: 2018-08-05T23:48:19.342677: step 8724, loss 0.578647.
Train: 2018-08-05T23:48:19.498860: step 8725, loss 0.576028.
Train: 2018-08-05T23:48:19.668742: step 8726, loss 0.552462.
Train: 2018-08-05T23:48:19.822001: step 8727, loss 0.530179.
Train: 2018-08-05T23:48:19.997520: step 8728, loss 0.599981.
Train: 2018-08-05T23:48:20.161013: step 8729, loss 0.549895.
Train: 2018-08-05T23:48:20.317227: step 8730, loss 0.55956.
Test: 2018-08-05T23:48:20.558842: step 8730, loss 0.563579.
Train: 2018-08-05T23:48:20.768718: step 8731, loss 0.543462.
Train: 2018-08-05T23:48:20.932402: step 8732, loss 0.55147.
Train: 2018-08-05T23:48:21.096615: step 8733, loss 0.596371.
Train: 2018-08-05T23:48:21.262782: step 8734, loss 0.6282.
Train: 2018-08-05T23:48:21.427312: step 8735, loss 0.530292.
Train: 2018-08-05T23:48:21.590136: step 8736, loss 0.555047.
Train: 2018-08-05T23:48:21.756382: step 8737, loss 0.568535.
Train: 2018-08-05T23:48:21.910834: step 8738, loss 0.610417.
Train: 2018-08-05T23:48:22.074650: step 8739, loss 0.646854.
Train: 2018-08-05T23:48:22.258687: step 8740, loss 0.626508.
Test: 2018-08-05T23:48:22.496890: step 8740, loss 0.564594.
Train: 2018-08-05T23:48:22.653102: step 8741, loss 0.59545.
Train: 2018-08-05T23:48:22.832570: step 8742, loss 0.529163.
Train: 2018-08-05T23:48:23.001755: step 8743, loss 0.563526.
Train: 2018-08-05T23:48:23.170327: step 8744, loss 0.562047.
Train: 2018-08-05T23:48:23.332988: step 8745, loss 0.592008.
Train: 2018-08-05T23:48:23.497440: step 8746, loss 0.535044.
Train: 2018-08-05T23:48:23.669305: step 8747, loss 0.622819.
Train: 2018-08-05T23:48:23.840161: step 8748, loss 0.552373.
Train: 2018-08-05T23:48:24.002726: step 8749, loss 0.565211.
Train: 2018-08-05T23:48:24.166204: step 8750, loss 0.577685.
Test: 2018-08-05T23:48:24.408893: step 8750, loss 0.563152.
Train: 2018-08-05T23:48:24.573048: step 8751, loss 0.596978.
Train: 2018-08-05T23:48:24.750068: step 8752, loss 0.60202.
Train: 2018-08-05T23:48:24.900388: step 8753, loss 0.542241.
Train: 2018-08-05T23:48:25.082304: step 8754, loss 0.627759.
Train: 2018-08-05T23:48:25.241739: step 8755, loss 0.627402.
Train: 2018-08-05T23:48:25.404885: step 8756, loss 0.570916.
Train: 2018-08-05T23:48:25.574444: step 8757, loss 0.55976.
Train: 2018-08-05T23:48:25.728206: step 8758, loss 0.557153.
Train: 2018-08-05T23:48:25.891322: step 8759, loss 0.575145.
Train: 2018-08-05T23:48:26.074113: step 8760, loss 0.557351.
Test: 2018-08-05T23:48:26.312460: step 8760, loss 0.561474.
Train: 2018-08-05T23:48:26.471177: step 8761, loss 0.583842.
Train: 2018-08-05T23:48:26.642738: step 8762, loss 0.561527.
Train: 2018-08-05T23:48:26.796992: step 8763, loss 0.556993.
Train: 2018-08-05T23:48:26.971078: step 8764, loss 0.609496.
Train: 2018-08-05T23:48:27.122841: step 8765, loss 0.576015.
Train: 2018-08-05T23:48:27.294681: step 8766, loss 0.565945.
Train: 2018-08-05T23:48:27.459178: step 8767, loss 0.6198.
Train: 2018-08-05T23:48:27.629807: step 8768, loss 0.566072.
Train: 2018-08-05T23:48:27.779683: step 8769, loss 0.574867.
Train: 2018-08-05T23:48:27.950000: step 8770, loss 0.616529.
Test: 2018-08-05T23:48:28.194326: step 8770, loss 0.560406.
Train: 2018-08-05T23:48:28.354890: step 8771, loss 0.599386.
Train: 2018-08-05T23:48:28.518341: step 8772, loss 0.6491.
Train: 2018-08-05T23:48:28.684159: step 8773, loss 0.67646.
Train: 2018-08-05T23:48:28.858443: step 8774, loss 0.626322.
Train: 2018-08-05T23:48:29.018890: step 8775, loss 0.549422.
Train: 2018-08-05T23:48:29.188146: step 8776, loss 0.525215.
Train: 2018-08-05T23:48:29.351241: step 8777, loss 0.508147.
Train: 2018-08-05T23:48:29.514784: step 8778, loss 0.551354.
Train: 2018-08-05T23:48:29.683444: step 8779, loss 0.541485.
Train: 2018-08-05T23:48:29.834184: step 8780, loss 0.638896.
Test: 2018-08-05T23:48:30.070358: step 8780, loss 0.560472.
Train: 2018-08-05T23:48:30.256361: step 8781, loss 0.622398.
Train: 2018-08-05T23:48:30.412343: step 8782, loss 0.551339.
Train: 2018-08-05T23:48:30.586158: step 8783, loss 0.54897.
Train: 2018-08-05T23:48:30.741355: step 8784, loss 0.654039.
Train: 2018-08-05T23:48:30.913160: step 8785, loss 0.556017.
Train: 2018-08-05T23:48:31.093285: step 8786, loss 0.59629.
Train: 2018-08-05T23:48:31.249499: step 8787, loss 0.573277.
Train: 2018-08-05T23:48:31.428665: step 8788, loss 0.63535.
Train: 2018-08-05T23:48:31.576547: step 8789, loss 0.589538.
Train: 2018-08-05T23:48:31.740239: step 8790, loss 0.597208.
Test: 2018-08-05T23:48:31.981944: step 8790, loss 0.560696.
Train: 2018-08-05T23:48:32.161094: step 8791, loss 0.572363.
Train: 2018-08-05T23:48:32.323658: step 8792, loss 0.55185.
Train: 2018-08-05T23:48:32.487229: step 8793, loss 0.589406.
Train: 2018-08-05T23:48:32.682275: step 8794, loss 0.527272.
Train: 2018-08-05T23:48:32.830155: step 8795, loss 0.612706.
Train: 2018-08-05T23:48:33.011222: step 8796, loss 0.5981.
Train: 2018-08-05T23:48:33.180770: step 8797, loss 0.517037.
Train: 2018-08-05T23:48:33.357291: step 8798, loss 0.556787.
Train: 2018-08-05T23:48:33.511968: step 8799, loss 0.564577.
Train: 2018-08-05T23:48:33.688696: step 8800, loss 0.530597.
Test: 2018-08-05T23:48:33.930103: step 8800, loss 0.558571.
Train: 2018-08-05T23:48:34.628838: step 8801, loss 0.514799.
Train: 2018-08-05T23:48:34.788714: step 8802, loss 0.523498.
Train: 2018-08-05T23:48:34.960549: step 8803, loss 0.621768.
Train: 2018-08-05T23:48:35.132383: step 8804, loss 0.52025.
Train: 2018-08-05T23:48:35.280287: step 8805, loss 0.605855.
Train: 2018-08-05T23:48:35.451080: step 8806, loss 0.537633.
Train: 2018-08-05T23:48:35.630209: step 8807, loss 0.519316.
Train: 2018-08-05T23:48:35.786453: step 8808, loss 0.552516.
Train: 2018-08-05T23:48:35.965213: step 8809, loss 0.571325.
Train: 2018-08-05T23:48:36.140774: step 8810, loss 0.517588.
Test: 2018-08-05T23:48:36.368922: step 8810, loss 0.556646.
Train: 2018-08-05T23:48:36.549506: step 8811, loss 0.496368.
Train: 2018-08-05T23:48:36.695248: step 8812, loss 0.619211.
Train: 2018-08-05T23:48:36.858151: step 8813, loss 0.561639.
Train: 2018-08-05T23:48:37.030015: step 8814, loss 0.584243.
Train: 2018-08-05T23:48:37.195430: step 8815, loss 0.553795.
Train: 2018-08-05T23:48:37.359091: step 8816, loss 0.590116.
Train: 2018-08-05T23:48:37.521877: step 8817, loss 0.554773.
Train: 2018-08-05T23:48:37.688295: step 8818, loss 0.582194.
Train: 2018-08-05T23:48:37.841041: step 8819, loss 0.537721.
Train: 2018-08-05T23:48:38.019521: step 8820, loss 0.525909.
Test: 2018-08-05T23:48:38.245333: step 8820, loss 0.556012.
Train: 2018-08-05T23:48:38.420817: step 8821, loss 0.62779.
Train: 2018-08-05T23:48:38.592622: step 8822, loss 0.553582.
Train: 2018-08-05T23:48:38.746857: step 8823, loss 0.61766.
Train: 2018-08-05T23:48:38.915638: step 8824, loss 0.516308.
Train: 2018-08-05T23:48:39.087503: step 8825, loss 0.698661.
Train: 2018-08-05T23:48:39.247506: step 8826, loss 0.588122.
Train: 2018-08-05T23:48:39.410970: step 8827, loss 0.445365.
Train: 2018-08-05T23:48:39.574494: step 8828, loss 0.614366.
Train: 2018-08-05T23:48:39.737983: step 8829, loss 0.473649.
Train: 2018-08-05T23:48:39.894227: step 8830, loss 0.589118.
Test: 2018-08-05T23:48:40.136818: step 8830, loss 0.555516.
Train: 2018-08-05T23:48:40.316094: step 8831, loss 0.552374.
Train: 2018-08-05T23:48:40.480015: step 8832, loss 0.605142.
Train: 2018-08-05T23:48:40.644474: step 8833, loss 0.544231.
Train: 2018-08-05T23:48:40.814398: step 8834, loss 0.543572.
Train: 2018-08-05T23:48:40.970610: step 8835, loss 0.55268.
Train: 2018-08-05T23:48:41.134307: step 8836, loss 0.587981.
Train: 2018-08-05T23:48:41.313401: step 8837, loss 0.508789.
Train: 2018-08-05T23:48:41.476842: step 8838, loss 0.657703.
Train: 2018-08-05T23:48:41.640357: step 8839, loss 0.526563.
Train: 2018-08-05T23:48:41.819596: step 8840, loss 0.60916.
Test: 2018-08-05T23:48:42.051880: step 8840, loss 0.560505.
Train: 2018-08-05T23:48:42.223714: step 8841, loss 0.55376.
Train: 2018-08-05T23:48:42.387316: step 8842, loss 0.651404.
Train: 2018-08-05T23:48:42.551047: step 8843, loss 0.563039.
Train: 2018-08-05T23:48:42.731140: step 8844, loss 0.584894.
Train: 2018-08-05T23:48:42.886663: step 8845, loss 0.586991.
Train: 2018-08-05T23:48:43.068494: step 8846, loss 0.623985.
Train: 2018-08-05T23:48:43.237041: step 8847, loss 0.504427.
Train: 2018-08-05T23:48:43.391549: step 8848, loss 0.548886.
Train: 2018-08-05T23:48:43.556018: step 8849, loss 0.633066.
Train: 2018-08-05T23:48:43.736188: step 8850, loss 0.531737.
Test: 2018-08-05T23:48:43.968721: step 8850, loss 0.569123.
Train: 2018-08-05T23:48:44.133260: step 8851, loss 0.600846.
Train: 2018-08-05T23:48:44.312416: step 8852, loss 0.557441.
Train: 2018-08-05T23:48:44.468630: step 8853, loss 0.522513.
Train: 2018-08-05T23:48:44.632551: step 8854, loss 0.590394.
Train: 2018-08-05T23:48:44.796067: step 8855, loss 0.565141.
Train: 2018-08-05T23:48:44.966160: step 8856, loss 0.608901.
Train: 2018-08-05T23:48:45.122403: step 8857, loss 0.591431.
Train: 2018-08-05T23:48:45.301532: step 8858, loss 0.535431.
Train: 2018-08-05T23:48:45.457171: step 8859, loss 0.563902.
Train: 2018-08-05T23:48:45.636229: step 8860, loss 0.651465.
Test: 2018-08-05T23:48:45.868697: step 8860, loss 0.564487.
Train: 2018-08-05T23:48:46.046070: step 8861, loss 0.581622.
Train: 2018-08-05T23:48:46.213626: step 8862, loss 0.691982.
Train: 2018-08-05T23:48:46.372354: step 8863, loss 0.622382.
Train: 2018-08-05T23:48:46.528568: step 8864, loss 0.524251.
Train: 2018-08-05T23:48:46.707819: step 8865, loss 0.543621.
Train: 2018-08-05T23:48:46.876920: step 8866, loss 0.604673.
Train: 2018-08-05T23:48:47.024650: step 8867, loss 0.5253.
Train: 2018-08-05T23:48:47.203868: step 8868, loss 0.525981.
Train: 2018-08-05T23:48:47.378929: step 8869, loss 0.584822.
Train: 2018-08-05T23:48:47.539077: step 8870, loss 0.499651.
Test: 2018-08-05T23:48:47.773401: step 8870, loss 0.560261.
Train: 2018-08-05T23:48:47.936383: step 8871, loss 0.524175.
Train: 2018-08-05T23:48:48.108217: step 8872, loss 0.6013.
Train: 2018-08-05T23:48:48.287520: step 8873, loss 0.556488.
Train: 2018-08-05T23:48:48.441683: step 8874, loss 0.617912.
Train: 2018-08-05T23:48:48.611141: step 8875, loss 0.599686.
Train: 2018-08-05T23:48:48.783006: step 8876, loss 0.548182.
Train: 2018-08-05T23:48:48.939214: step 8877, loss 0.599861.
Train: 2018-08-05T23:48:49.119412: step 8878, loss 0.572477.
Train: 2018-08-05T23:48:49.282869: step 8879, loss 0.616081.
Train: 2018-08-05T23:48:49.461994: step 8880, loss 0.536689.
Test: 2018-08-05T23:48:49.703698: step 8880, loss 0.557687.
Train: 2018-08-05T23:48:49.868196: step 8881, loss 0.589082.
Train: 2018-08-05T23:48:50.031795: step 8882, loss 0.554749.
Train: 2018-08-05T23:48:50.210058: step 8883, loss 0.561976.
Train: 2018-08-05T23:48:50.373966: step 8884, loss 0.51176.
Train: 2018-08-05T23:48:50.553064: step 8885, loss 0.536986.
Train: 2018-08-05T23:48:50.716563: step 8886, loss 0.614275.
Train: 2018-08-05T23:48:50.888397: step 8887, loss 0.589799.
Train: 2018-08-05T23:48:51.057675: step 8888, loss 0.536747.
Train: 2018-08-05T23:48:51.221172: step 8889, loss 0.597588.
Train: 2018-08-05T23:48:51.400320: step 8890, loss 0.518555.
Test: 2018-08-05T23:48:51.627338: step 8890, loss 0.556228.
Train: 2018-08-05T23:48:51.807253: step 8891, loss 0.49309.
Train: 2018-08-05T23:48:51.986127: step 8892, loss 0.596651.
Train: 2018-08-05T23:48:52.133479: step 8893, loss 0.553073.
Train: 2018-08-05T23:48:52.315344: step 8894, loss 0.623353.
Train: 2018-08-05T23:48:52.475937: step 8895, loss 0.59762.
Train: 2018-08-05T23:48:52.639147: step 8896, loss 0.658303.
Train: 2018-08-05T23:48:52.804987: step 8897, loss 0.552527.
Train: 2018-08-05T23:48:52.987439: step 8898, loss 0.56123.
Train: 2018-08-05T23:48:53.156984: step 8899, loss 0.588282.
Train: 2018-08-05T23:48:53.322543: step 8900, loss 0.552562.
Test: 2018-08-05T23:48:53.559978: step 8900, loss 0.555283.
Train: 2018-08-05T23:48:54.293840: step 8901, loss 0.645686.
Train: 2018-08-05T23:48:54.465670: step 8902, loss 0.577417.
Train: 2018-08-05T23:48:54.628669: step 8903, loss 0.543653.
Train: 2018-08-05T23:48:54.810081: step 8904, loss 0.562226.
Train: 2018-08-05T23:48:54.966295: step 8905, loss 0.677775.
Train: 2018-08-05T23:48:55.129964: step 8906, loss 0.537982.
Train: 2018-08-05T23:48:55.309149: step 8907, loss 0.555232.
Train: 2018-08-05T23:48:55.472587: step 8908, loss 0.54488.
Train: 2018-08-05T23:48:55.637165: step 8909, loss 0.532325.
Train: 2018-08-05T23:48:55.824646: step 8910, loss 0.635556.
Test: 2018-08-05T23:48:56.066655: step 8910, loss 0.564776.
Train: 2018-08-05T23:48:56.242191: step 8911, loss 0.587573.
Train: 2018-08-05T23:48:56.407097: step 8912, loss 0.624721.
Train: 2018-08-05T23:48:56.585057: step 8913, loss 0.585954.
Train: 2018-08-05T23:48:56.749274: step 8914, loss 0.55599.
Train: 2018-08-05T23:48:56.912529: step 8915, loss 0.625268.
Train: 2018-08-05T23:48:57.089944: step 8916, loss 0.577087.
Train: 2018-08-05T23:48:57.250267: step 8917, loss 0.560675.
Train: 2018-08-05T23:48:57.413196: step 8918, loss 0.629528.
Train: 2018-08-05T23:48:57.576069: step 8919, loss 0.604937.
Train: 2018-08-05T23:48:57.745344: step 8920, loss 0.664733.
Test: 2018-08-05T23:48:57.986981: step 8920, loss 0.58173.
Train: 2018-08-05T23:48:58.166845: step 8921, loss 0.586843.
Train: 2018-08-05T23:48:58.338679: step 8922, loss 0.620053.
Train: 2018-08-05T23:48:58.495880: step 8923, loss 0.643583.
Train: 2018-08-05T23:48:58.667683: step 8924, loss 0.609993.
Train: 2018-08-05T23:48:58.853315: step 8925, loss 0.615495.
Train: 2018-08-05T23:48:59.006779: step 8926, loss 0.540052.
Train: 2018-08-05T23:48:59.182301: step 8927, loss 0.605048.
Train: 2018-08-05T23:48:59.345722: step 8928, loss 0.62907.
Train: 2018-08-05T23:48:59.517557: step 8929, loss 0.612045.
Train: 2018-08-05T23:48:59.681117: step 8930, loss 0.536786.
Test: 2018-08-05T23:48:59.923784: step 8930, loss 0.570618.
Train: 2018-08-05T23:49:00.087470: step 8931, loss 0.650377.
Train: 2018-08-05T23:49:00.266610: step 8932, loss 0.574999.
Train: 2018-08-05T23:49:00.430163: step 8933, loss 0.532581.
Train: 2018-08-05T23:49:00.595689: step 8934, loss 0.573201.
Train: 2018-08-05T23:49:00.760202: step 8935, loss 0.597158.
Train: 2018-08-05T23:49:00.939408: step 8936, loss 0.573058.
Train: 2018-08-05T23:49:01.103067: step 8937, loss 0.612102.
Train: 2018-08-05T23:49:01.282241: step 8938, loss 0.553432.
Train: 2018-08-05T23:49:01.458918: step 8939, loss 0.551305.
Train: 2018-08-05T23:49:01.619268: step 8940, loss 0.642027.
Test: 2018-08-05T23:49:01.858473: step 8940, loss 0.562747.
Train: 2018-08-05T23:49:02.033122: step 8941, loss 0.541831.
Train: 2018-08-05T23:49:02.193747: step 8942, loss 0.600623.
Train: 2018-08-05T23:49:02.357423: step 8943, loss 0.59235.
Train: 2018-08-05T23:49:02.538777: step 8944, loss 0.548733.
Train: 2018-08-05T23:49:02.702619: step 8945, loss 0.608979.
Train: 2018-08-05T23:49:02.858862: step 8946, loss 0.622263.
Train: 2018-08-05T23:49:03.040754: step 8947, loss 0.633614.
Train: 2018-08-05T23:49:03.208338: step 8948, loss 0.593059.
Train: 2018-08-05T23:49:03.383868: step 8949, loss 0.572075.
Train: 2018-08-05T23:49:03.548335: step 8950, loss 0.590267.
Test: 2018-08-05T23:49:03.784051: step 8950, loss 0.559274.
Train: 2018-08-05T23:49:03.962492: step 8951, loss 0.630467.
Train: 2018-08-05T23:49:04.132246: step 8952, loss 0.589344.
Train: 2018-08-05T23:49:04.291116: step 8953, loss 0.59004.
Train: 2018-08-05T23:49:04.467649: step 8954, loss 0.597129.
Train: 2018-08-05T23:49:04.632230: step 8955, loss 0.621349.
Train: 2018-08-05T23:49:04.804066: step 8956, loss 0.539175.
Train: 2018-08-05T23:49:04.960277: step 8957, loss 0.571035.
Train: 2018-08-05T23:49:05.129378: step 8958, loss 0.53016.
Train: 2018-08-05T23:49:05.305145: step 8959, loss 0.587382.
Train: 2018-08-05T23:49:05.470438: step 8960, loss 0.506879.
Test: 2018-08-05T23:49:05.709381: step 8960, loss 0.5577.
Train: 2018-08-05T23:49:05.881215: step 8961, loss 0.507485.
Train: 2018-08-05T23:49:06.055523: step 8962, loss 0.578301.
Train: 2018-08-05T23:49:06.220068: step 8963, loss 0.55331.
Train: 2018-08-05T23:49:06.376718: step 8964, loss 0.586282.
Train: 2018-08-05T23:49:06.540280: step 8965, loss 0.60258.
Train: 2018-08-05T23:49:06.712149: step 8966, loss 0.527043.
Train: 2018-08-05T23:49:06.875767: step 8967, loss 0.537941.
Train: 2018-08-05T23:49:07.058177: step 8968, loss 0.553388.
Train: 2018-08-05T23:49:07.210055: step 8969, loss 0.595545.
Train: 2018-08-05T23:49:07.390215: step 8970, loss 0.611391.
Test: 2018-08-05T23:49:07.617256: step 8970, loss 0.555263.
Train: 2018-08-05T23:49:07.800277: step 8971, loss 0.629121.
Train: 2018-08-05T23:49:07.953324: step 8972, loss 0.560445.
Train: 2018-08-05T23:49:08.139365: step 8973, loss 0.553531.
Train: 2018-08-05T23:49:08.301946: step 8974, loss 0.603058.
Train: 2018-08-05T23:49:08.465607: step 8975, loss 0.619722.
Train: 2018-08-05T23:49:08.644434: step 8976, loss 0.558888.
Train: 2018-08-05T23:49:08.800676: step 8977, loss 0.56013.
Train: 2018-08-05T23:49:08.982203: step 8978, loss 0.484636.
Train: 2018-08-05T23:49:09.145176: step 8979, loss 0.552651.
Train: 2018-08-05T23:49:09.309014: step 8980, loss 0.603444.
Test: 2018-08-05T23:49:09.557313: step 8980, loss 0.554326.
Train: 2018-08-05T23:49:09.736609: step 8981, loss 0.568497.
Train: 2018-08-05T23:49:09.899831: step 8982, loss 0.559917.
Train: 2018-08-05T23:49:10.071666: step 8983, loss 0.552545.
Train: 2018-08-05T23:49:10.230595: step 8984, loss 0.585772.
Train: 2018-08-05T23:49:10.402406: step 8985, loss 0.54255.
Train: 2018-08-05T23:49:10.572555: step 8986, loss 0.568077.
Train: 2018-08-05T23:49:10.741449: step 8987, loss 0.525374.
Train: 2018-08-05T23:49:10.917081: step 8988, loss 0.585462.
Train: 2018-08-05T23:49:11.081599: step 8989, loss 0.559825.
Train: 2018-08-05T23:49:11.253459: step 8990, loss 0.645294.
Test: 2018-08-05T23:49:11.479582: step 8990, loss 0.553448.
Train: 2018-08-05T23:49:11.659110: step 8991, loss 0.516498.
Train: 2018-08-05T23:49:11.822263: step 8992, loss 0.517062.
Train: 2018-08-05T23:49:11.985923: step 8993, loss 0.550004.
Train: 2018-08-05T23:49:12.149387: step 8994, loss 0.594793.
Train: 2018-08-05T23:49:12.321253: step 8995, loss 0.550437.
Train: 2018-08-05T23:49:12.492066: step 8996, loss 0.655745.
Train: 2018-08-05T23:49:12.655525: step 8997, loss 0.62952.
Train: 2018-08-05T23:49:12.818995: step 8998, loss 0.558709.
Train: 2018-08-05T23:49:12.993732: step 8999, loss 0.58585.
Train: 2018-08-05T23:49:13.160287: step 9000, loss 0.549973.
Test: 2018-08-05T23:49:13.387488: step 9000, loss 0.552984.
Train: 2018-08-05T23:49:14.072600: step 9001, loss 0.569036.
Train: 2018-08-05T23:49:14.236109: step 9002, loss 0.506025.
Train: 2018-08-05T23:49:14.392323: step 9003, loss 0.584848.
Train: 2018-08-05T23:49:14.563417: step 9004, loss 0.593149.
Train: 2018-08-05T23:49:14.726942: step 9005, loss 0.610142.
Train: 2018-08-05T23:49:14.906096: step 9006, loss 0.541142.
Train: 2018-08-05T23:49:15.070644: step 9007, loss 0.575602.
Train: 2018-08-05T23:49:15.226857: step 9008, loss 0.583786.
Train: 2018-08-05T23:49:15.406723: step 9009, loss 0.508144.
Train: 2018-08-05T23:49:15.561412: step 9010, loss 0.610133.
Test: 2018-08-05T23:49:15.805932: step 9010, loss 0.552733.
Train: 2018-08-05T23:49:15.989884: step 9011, loss 0.566921.
Train: 2018-08-05T23:49:16.160458: step 9012, loss 0.600549.
Train: 2018-08-05T23:49:16.329002: step 9013, loss 0.549355.
Train: 2018-08-05T23:49:16.478383: step 9014, loss 0.575656.
Train: 2018-08-05T23:49:16.657018: step 9015, loss 0.592793.
Train: 2018-08-05T23:49:16.823441: step 9016, loss 0.558523.
Train: 2018-08-05T23:49:16.980381: step 9017, loss 0.549593.
Train: 2018-08-05T23:49:17.143867: step 9018, loss 0.592309.
Train: 2018-08-05T23:49:17.323042: step 9019, loss 0.583601.
Train: 2018-08-05T23:49:17.479289: step 9020, loss 0.541281.
Test: 2018-08-05T23:49:17.720882: step 9020, loss 0.552578.
Train: 2018-08-05T23:49:17.901132: step 9021, loss 0.626495.
Train: 2018-08-05T23:49:18.069680: step 9022, loss 0.592083.
Train: 2018-08-05T23:49:18.248891: step 9023, loss 0.524538.
Train: 2018-08-05T23:49:18.412402: step 9024, loss 0.524914.
Train: 2018-08-05T23:49:18.585792: step 9025, loss 0.507847.
Train: 2018-08-05T23:49:18.749575: step 9026, loss 0.524439.
Train: 2018-08-05T23:49:18.904441: step 9027, loss 0.508449.
Train: 2018-08-05T23:49:19.067375: step 9028, loss 0.549311.
Train: 2018-08-05T23:49:19.246424: step 9029, loss 0.549409.
Train: 2018-08-05T23:49:19.409600: step 9030, loss 0.472223.
Test: 2018-08-05T23:49:19.639118: step 9030, loss 0.55191.
Train: 2018-08-05T23:49:19.819154: step 9031, loss 0.566076.
Train: 2018-08-05T23:49:19.973929: step 9032, loss 0.478359.
Train: 2018-08-05T23:49:20.144236: step 9033, loss 0.55834.
Train: 2018-08-05T23:49:20.322674: step 9034, loss 0.539795.
Train: 2018-08-05T23:49:20.486271: step 9035, loss 0.548925.
Train: 2018-08-05T23:49:20.657964: step 9036, loss 0.539793.
Train: 2018-08-05T23:49:20.805794: step 9037, loss 0.568087.
Train: 2018-08-05T23:49:20.979373: step 9038, loss 0.566323.
Train: 2018-08-05T23:49:21.151162: step 9039, loss 0.48361.
Train: 2018-08-05T23:49:21.305413: step 9040, loss 0.508507.
Test: 2018-08-05T23:49:21.553277: step 9040, loss 0.552004.
Train: 2018-08-05T23:49:21.725140: step 9041, loss 0.518826.
Train: 2018-08-05T23:49:21.894241: step 9042, loss 0.576051.
Train: 2018-08-05T23:49:22.058367: step 9043, loss 0.474999.
Train: 2018-08-05T23:49:22.230226: step 9044, loss 0.742161.
Train: 2018-08-05T23:49:22.393075: step 9045, loss 0.595056.
Train: 2018-08-05T23:49:22.556588: step 9046, loss 0.551724.
Train: 2018-08-05T23:49:22.719543: step 9047, loss 0.627473.
Train: 2018-08-05T23:49:22.898668: step 9048, loss 0.627178.
Train: 2018-08-05T23:49:23.065232: step 9049, loss 0.564045.
Train: 2018-08-05T23:49:23.233750: step 9050, loss 0.548668.
Test: 2018-08-05T23:49:23.465971: step 9050, loss 0.56152.
Train: 2018-08-05T23:49:23.628620: step 9051, loss 0.566738.
Train: 2018-08-05T23:49:23.807770: step 9052, loss 0.568649.
Train: 2018-08-05T23:49:23.971287: step 9053, loss 0.540656.
Train: 2018-08-05T23:49:24.142336: step 9054, loss 0.619963.
Train: 2018-08-05T23:49:24.298549: step 9055, loss 0.569833.
Train: 2018-08-05T23:49:24.477846: step 9056, loss 0.500406.
Train: 2018-08-05T23:49:24.641422: step 9057, loss 0.561401.
Train: 2018-08-05T23:49:24.804888: step 9058, loss 0.560556.
Train: 2018-08-05T23:49:24.968524: step 9059, loss 0.525161.
Train: 2018-08-05T23:49:25.132039: step 9060, loss 0.520621.
Test: 2018-08-05T23:49:25.380076: step 9060, loss 0.564529.
Train: 2018-08-05T23:49:25.537556: step 9061, loss 0.552004.
Train: 2018-08-05T23:49:25.716820: step 9062, loss 0.596963.
Train: 2018-08-05T23:49:25.880301: step 9063, loss 0.552991.
Train: 2018-08-05T23:49:26.057290: step 9064, loss 0.541249.
Train: 2018-08-05T23:49:26.224811: step 9065, loss 0.562361.
Train: 2018-08-05T23:49:26.387407: step 9066, loss 0.559078.
Train: 2018-08-05T23:49:26.540566: step 9067, loss 0.530425.
Train: 2018-08-05T23:49:26.703598: step 9068, loss 0.500677.
Train: 2018-08-05T23:49:26.869775: step 9069, loss 0.536616.
Train: 2018-08-05T23:49:27.050089: step 9070, loss 0.621558.
Test: 2018-08-05T23:49:27.291061: step 9070, loss 0.560161.
Train: 2018-08-05T23:49:27.460494: step 9071, loss 0.566844.
Train: 2018-08-05T23:49:27.628011: step 9072, loss 0.602352.
Train: 2018-08-05T23:49:27.774098: step 9073, loss 0.58232.
Train: 2018-08-05T23:49:27.952980: step 9074, loss 0.620511.
Train: 2018-08-05T23:49:28.126588: step 9075, loss 0.516657.
Train: 2018-08-05T23:49:28.282833: step 9076, loss 0.531622.
Train: 2018-08-05T23:49:28.463215: step 9077, loss 0.522276.
Train: 2018-08-05T23:49:28.624603: step 9078, loss 0.61905.
Train: 2018-08-05T23:49:28.793422: step 9079, loss 0.575345.
Train: 2018-08-05T23:49:28.953328: step 9080, loss 0.529542.
Test: 2018-08-05T23:49:29.195041: step 9080, loss 0.55983.
Train: 2018-08-05T23:49:29.358622: step 9081, loss 0.565361.
Train: 2018-08-05T23:49:29.523212: step 9082, loss 0.539324.
Train: 2018-08-05T23:49:29.695017: step 9083, loss 0.638007.
Train: 2018-08-05T23:49:29.858023: step 9084, loss 0.662438.
Train: 2018-08-05T23:49:30.036431: step 9085, loss 0.657615.
Train: 2018-08-05T23:49:30.199947: step 9086, loss 0.571247.
Train: 2018-08-05T23:49:30.379165: step 9087, loss 0.526388.
Train: 2018-08-05T23:49:30.546816: step 9088, loss 0.540237.
Train: 2018-08-05T23:49:30.723858: step 9089, loss 0.56533.
Train: 2018-08-05T23:49:30.899425: step 9090, loss 0.574998.
Test: 2018-08-05T23:49:31.124994: step 9090, loss 0.559673.
Train: 2018-08-05T23:49:31.308406: step 9091, loss 0.627951.
Train: 2018-08-05T23:49:31.471306: step 9092, loss 0.628559.
Train: 2018-08-05T23:49:31.634120: step 9093, loss 0.574734.
Train: 2018-08-05T23:49:31.812695: step 9094, loss 0.547556.
Train: 2018-08-05T23:49:31.993102: step 9095, loss 0.619475.
Train: 2018-08-05T23:49:32.148167: step 9096, loss 0.555756.
Train: 2018-08-05T23:49:32.311335: step 9097, loss 0.60161.
Train: 2018-08-05T23:49:32.485716: step 9098, loss 0.573495.
Train: 2018-08-05T23:49:32.657551: step 9099, loss 0.556121.
Train: 2018-08-05T23:49:32.827450: step 9100, loss 0.537847.
Test: 2018-08-05T23:49:33.067290: step 9100, loss 0.557896.
Train: 2018-08-05T23:49:33.820259: step 9101, loss 0.616406.
Train: 2018-08-05T23:49:33.992064: step 9102, loss 0.571491.
Train: 2018-08-05T23:49:34.155742: step 9103, loss 0.589567.
Train: 2018-08-05T23:49:34.334852: step 9104, loss 0.650005.
Train: 2018-08-05T23:49:34.482650: step 9105, loss 0.579818.
Train: 2018-08-05T23:49:34.653421: step 9106, loss 0.570662.
Train: 2018-08-05T23:49:34.809659: step 9107, loss 0.562093.
Train: 2018-08-05T23:49:34.988975: step 9108, loss 0.605395.
Train: 2018-08-05T23:49:35.152499: step 9109, loss 0.561635.
Train: 2018-08-05T23:49:35.315940: step 9110, loss 0.552981.
Test: 2018-08-05T23:49:35.556942: step 9110, loss 0.555717.
Train: 2018-08-05T23:49:35.742532: step 9111, loss 0.552714.
Train: 2018-08-05T23:49:35.906078: step 9112, loss 0.611583.
Train: 2018-08-05T23:49:36.074500: step 9113, loss 0.577344.
Train: 2018-08-05T23:49:36.245039: step 9114, loss 0.584468.
Train: 2018-08-05T23:49:36.411829: step 9115, loss 0.535384.
Train: 2018-08-05T23:49:36.575360: step 9116, loss 0.601726.
Train: 2018-08-05T23:49:36.730595: step 9117, loss 0.593332.
Train: 2018-08-05T23:49:36.886779: step 9118, loss 0.568463.
Train: 2018-08-05T23:49:37.066153: step 9119, loss 0.57584.
Train: 2018-08-05T23:49:37.230638: step 9120, loss 0.604783.
Test: 2018-08-05T23:49:37.472980: step 9120, loss 0.562208.
Train: 2018-08-05T23:49:37.642740: step 9121, loss 0.531396.
Train: 2018-08-05T23:49:37.805992: step 9122, loss 0.528129.
Train: 2018-08-05T23:49:37.969121: step 9123, loss 0.544457.
Train: 2018-08-05T23:49:38.144171: step 9124, loss 0.586477.
Train: 2018-08-05T23:49:38.304191: step 9125, loss 0.58784.
Train: 2018-08-05T23:49:38.469674: step 9126, loss 0.57922.
Train: 2018-08-05T23:49:38.633241: step 9127, loss 0.54635.
Train: 2018-08-05T23:49:38.813378: step 9128, loss 0.547212.
Train: 2018-08-05T23:49:38.976903: step 9129, loss 0.538793.
Train: 2018-08-05T23:49:39.139723: step 9130, loss 0.574378.
Test: 2018-08-05T23:49:39.382310: step 9130, loss 0.558586.
Train: 2018-08-05T23:49:39.546098: step 9131, loss 0.547318.
Train: 2018-08-05T23:49:39.709626: step 9132, loss 0.624694.
Train: 2018-08-05T23:49:39.888737: step 9133, loss 0.571296.
Train: 2018-08-05T23:49:40.058557: step 9134, loss 0.562894.
Train: 2018-08-05T23:49:40.214770: step 9135, loss 0.607927.
Train: 2018-08-05T23:49:40.393926: step 9136, loss 0.527586.
Train: 2018-08-05T23:49:40.573029: step 9137, loss 0.529404.
Train: 2018-08-05T23:49:40.752140: step 9138, loss 0.481931.
Train: 2018-08-05T23:49:40.915755: step 9139, loss 0.586428.
Train: 2018-08-05T23:49:41.079306: step 9140, loss 0.567608.
Test: 2018-08-05T23:49:41.312682: step 9140, loss 0.558186.
Train: 2018-08-05T23:49:41.484541: step 9141, loss 0.53419.
Train: 2018-08-05T23:49:41.647487: step 9142, loss 0.675728.
Train: 2018-08-05T23:49:41.826649: step 9143, loss 0.587433.
Train: 2018-08-05T23:49:41.990173: step 9144, loss 0.535054.
Train: 2018-08-05T23:49:42.159561: step 9145, loss 0.563735.
Train: 2018-08-05T23:49:42.315773: step 9146, loss 0.58084.
Train: 2018-08-05T23:49:42.479528: step 9147, loss 0.573303.
Train: 2018-08-05T23:49:42.658753: step 9148, loss 0.546077.
Train: 2018-08-05T23:49:42.822295: step 9149, loss 0.546288.
Train: 2018-08-05T23:49:42.996663: step 9150, loss 0.546043.
Test: 2018-08-05T23:49:43.235025: step 9150, loss 0.557727.
Train: 2018-08-05T23:49:43.402691: step 9151, loss 0.563409.
Train: 2018-08-05T23:49:43.558938: step 9152, loss 0.536593.
Train: 2018-08-05T23:49:43.737573: step 9153, loss 0.554561.
Train: 2018-08-05T23:49:43.904183: step 9154, loss 0.588884.
Train: 2018-08-05T23:49:44.067961: step 9155, loss 0.607259.
Train: 2018-08-05T23:49:44.231684: step 9156, loss 0.623457.
Train: 2018-08-05T23:49:44.403549: step 9157, loss 0.482837.
Train: 2018-08-05T23:49:44.566559: step 9158, loss 0.587693.
Train: 2018-08-05T23:49:44.729394: step 9159, loss 0.604591.
Train: 2018-08-05T23:49:44.892565: step 9160, loss 0.533615.
Test: 2018-08-05T23:49:45.129696: step 9160, loss 0.554491.
Train: 2018-08-05T23:49:45.301557: step 9161, loss 0.622539.
Train: 2018-08-05T23:49:45.471449: step 9162, loss 0.664578.
Train: 2018-08-05T23:49:45.642189: step 9163, loss 0.577267.
Train: 2018-08-05T23:49:45.814024: step 9164, loss 0.560466.
Train: 2018-08-05T23:49:45.990360: step 9165, loss 0.551079.
Train: 2018-08-05T23:49:46.153952: step 9166, loss 0.607074.
Train: 2018-08-05T23:49:46.325491: step 9167, loss 0.525365.
Train: 2018-08-05T23:49:46.483427: step 9168, loss 0.602042.
Train: 2018-08-05T23:49:46.658106: step 9169, loss 0.542321.
Train: 2018-08-05T23:49:46.818458: step 9170, loss 0.593945.
Test: 2018-08-05T23:49:47.063070: step 9170, loss 0.55389.
Train: 2018-08-05T23:49:47.226790: step 9171, loss 0.500009.
Train: 2018-08-05T23:49:47.397419: step 9172, loss 0.567929.
Train: 2018-08-05T23:49:47.560943: step 9173, loss 0.560155.
Train: 2018-08-05T23:49:47.740260: step 9174, loss 0.525512.
Train: 2018-08-05T23:49:47.910770: step 9175, loss 0.671716.
Train: 2018-08-05T23:49:48.065860: step 9176, loss 0.55038.
Train: 2018-08-05T23:49:48.240735: step 9177, loss 0.611282.
Train: 2018-08-05T23:49:48.403572: step 9178, loss 0.602622.
Train: 2018-08-05T23:49:48.566533: step 9179, loss 0.482198.
Train: 2018-08-05T23:49:48.751564: step 9180, loss 0.583664.
Test: 2018-08-05T23:49:48.987796: step 9180, loss 0.552986.
Train: 2018-08-05T23:49:49.153135: step 9181, loss 0.550679.
Train: 2018-08-05T23:49:49.307255: step 9182, loss 0.54272.
Train: 2018-08-05T23:49:49.481742: step 9183, loss 0.652371.
Train: 2018-08-05T23:49:49.637954: step 9184, loss 0.515695.
Train: 2018-08-05T23:49:49.807835: step 9185, loss 0.584103.
Train: 2018-08-05T23:49:49.976667: step 9186, loss 0.574373.
Train: 2018-08-05T23:49:50.132909: step 9187, loss 0.515313.
Train: 2018-08-05T23:49:50.308545: step 9188, loss 0.609031.
Train: 2018-08-05T23:49:50.472030: step 9189, loss 0.515015.
Train: 2018-08-05T23:49:50.635533: step 9190, loss 0.583872.
Test: 2018-08-05T23:49:50.861701: step 9190, loss 0.5519.
Train: 2018-08-05T23:49:51.040841: step 9191, loss 0.599791.
Train: 2018-08-05T23:49:51.197055: step 9192, loss 0.540208.
Train: 2018-08-05T23:49:51.375755: step 9193, loss 0.566617.
Train: 2018-08-05T23:49:51.539691: step 9194, loss 0.472229.
Train: 2018-08-05T23:49:51.703196: step 9195, loss 0.643305.
Train: 2018-08-05T23:49:51.873350: step 9196, loss 0.514241.
Train: 2018-08-05T23:49:52.029565: step 9197, loss 0.548074.
Train: 2018-08-05T23:49:52.206603: step 9198, loss 0.540354.
Train: 2018-08-05T23:49:52.365208: step 9199, loss 0.574042.
Train: 2018-08-05T23:49:52.528700: step 9200, loss 0.504584.
Test: 2018-08-05T23:49:52.777833: step 9200, loss 0.551111.
Train: 2018-08-05T23:49:53.512111: step 9201, loss 0.600956.
Train: 2018-08-05T23:49:53.678620: step 9202, loss 0.566611.
Train: 2018-08-05T23:49:53.848959: step 9203, loss 0.565238.
Train: 2018-08-05T23:49:54.016207: step 9204, loss 0.522551.
Train: 2018-08-05T23:49:54.175209: step 9205, loss 0.609445.
Train: 2018-08-05T23:49:54.354041: step 9206, loss 0.574428.
Train: 2018-08-05T23:49:54.524401: step 9207, loss 0.556422.
Train: 2018-08-05T23:49:54.691758: step 9208, loss 0.618008.
Train: 2018-08-05T23:49:54.854764: step 9209, loss 0.512121.
Train: 2018-08-05T23:49:55.019121: step 9210, loss 0.582622.
Test: 2018-08-05T23:49:55.266457: step 9210, loss 0.550688.
Train: 2018-08-05T23:49:55.430044: step 9211, loss 0.639408.
Train: 2018-08-05T23:49:55.593336: step 9212, loss 0.470121.
Train: 2018-08-05T23:49:55.765166: step 9213, loss 0.521774.
Train: 2018-08-05T23:49:55.923127: step 9214, loss 0.600264.
Train: 2018-08-05T23:49:56.105525: step 9215, loss 0.530342.
Train: 2018-08-05T23:49:56.274080: step 9216, loss 0.635132.
Train: 2018-08-05T23:49:56.429381: step 9217, loss 0.582417.
Train: 2018-08-05T23:49:56.601247: step 9218, loss 0.591656.
Train: 2018-08-05T23:49:56.771076: step 9219, loss 0.599684.
Train: 2018-08-05T23:49:56.936702: step 9220, loss 0.538809.
Test: 2018-08-05T23:49:57.178487: step 9220, loss 0.550564.
Train: 2018-08-05T23:49:57.350353: step 9221, loss 0.521509.
Train: 2018-08-05T23:49:57.514000: step 9222, loss 0.54815.
Train: 2018-08-05T23:49:57.677570: step 9223, loss 0.539285.
Train: 2018-08-05T23:49:57.841110: step 9224, loss 0.522161.
Train: 2018-08-05T23:49:58.017166: step 9225, loss 0.55704.
Train: 2018-08-05T23:49:58.182826: step 9226, loss 0.556473.
Train: 2018-08-05T23:49:58.345547: step 9227, loss 0.495072.
Train: 2018-08-05T23:49:58.509042: step 9228, loss 0.634303.
Train: 2018-08-05T23:49:58.672533: step 9229, loss 0.600059.
Train: 2018-08-05T23:49:58.828723: step 9230, loss 0.485809.
Test: 2018-08-05T23:49:59.077712: step 9230, loss 0.550229.
Train: 2018-08-05T23:49:59.240326: step 9231, loss 0.617601.
Train: 2018-08-05T23:49:59.403132: step 9232, loss 0.538256.
Train: 2018-08-05T23:49:59.566635: step 9233, loss 0.556284.
Train: 2018-08-05T23:49:59.730197: step 9234, loss 0.564851.
Train: 2018-08-05T23:49:59.897613: step 9235, loss 0.556549.
Train: 2018-08-05T23:50:00.051805: step 9236, loss 0.696348.
Train: 2018-08-05T23:50:00.222214: step 9237, loss 0.520612.
Train: 2018-08-05T23:50:00.385748: step 9238, loss 0.555386.
Train: 2018-08-05T23:50:00.548938: step 9239, loss 0.581537.
Train: 2018-08-05T23:50:00.721584: step 9240, loss 0.56514.
Test: 2018-08-05T23:50:00.957393: step 9240, loss 0.550144.
Train: 2018-08-05T23:50:01.127956: step 9241, loss 0.565715.
Train: 2018-08-05T23:50:01.296277: step 9242, loss 0.546972.
Train: 2018-08-05T23:50:01.459175: step 9243, loss 0.55649.
Train: 2018-08-05T23:50:01.637725: step 9244, loss 0.599474.
Train: 2018-08-05T23:50:01.801295: step 9245, loss 0.51295.
Train: 2018-08-05T23:50:01.976090: step 9246, loss 0.513293.
Train: 2018-08-05T23:50:02.136462: step 9247, loss 0.572862.
Train: 2018-08-05T23:50:02.294560: step 9248, loss 0.607545.
Train: 2018-08-05T23:50:02.466394: step 9249, loss 0.529537.
Train: 2018-08-05T23:50:02.635484: step 9250, loss 0.546983.
Test: 2018-08-05T23:50:02.866820: step 9250, loss 0.550016.
Train: 2018-08-05T23:50:03.052592: step 9251, loss 0.521779.
Train: 2018-08-05T23:50:03.226161: step 9252, loss 0.608474.
Train: 2018-08-05T23:50:03.388694: step 9253, loss 0.563727.
Train: 2018-08-05T23:50:03.543115: step 9254, loss 0.563231.
Train: 2018-08-05T23:50:03.714918: step 9255, loss 0.616404.
Train: 2018-08-05T23:50:03.894152: step 9256, loss 0.503692.
Train: 2018-08-05T23:50:04.057754: step 9257, loss 0.512854.
Train: 2018-08-05T23:50:04.235490: step 9258, loss 0.538782.
Train: 2018-08-05T23:50:04.379206: step 9259, loss 0.529458.
Train: 2018-08-05T23:50:04.549537: step 9260, loss 0.572858.
Test: 2018-08-05T23:50:04.793929: step 9260, loss 0.549754.
Train: 2018-08-05T23:50:04.953572: step 9261, loss 0.477232.
Train: 2018-08-05T23:50:05.116973: step 9262, loss 0.608426.
Train: 2018-08-05T23:50:05.282802: step 9263, loss 0.538011.
Train: 2018-08-05T23:50:05.461259: step 9264, loss 0.565043.
Train: 2018-08-05T23:50:05.617503: step 9265, loss 0.582017.
Train: 2018-08-05T23:50:05.803519: step 9266, loss 0.510921.
Train: 2018-08-05T23:50:05.966447: step 9267, loss 0.511542.
Train: 2018-08-05T23:50:06.132227: step 9268, loss 0.582045.
Train: 2018-08-05T23:50:06.302771: step 9269, loss 0.564404.
Train: 2018-08-05T23:50:06.467685: step 9270, loss 0.609234.
Test: 2018-08-05T23:50:06.696470: step 9270, loss 0.549459.
Train: 2018-08-05T23:50:06.877281: step 9271, loss 0.519306.
Train: 2018-08-05T23:50:07.040862: step 9272, loss 0.672892.
Train: 2018-08-05T23:50:07.201793: step 9273, loss 0.537779.
Train: 2018-08-05T23:50:07.365100: step 9274, loss 0.644635.
Train: 2018-08-05T23:50:07.536934: step 9275, loss 0.528865.
Train: 2018-08-05T23:50:07.693150: step 9276, loss 0.653523.
Train: 2018-08-05T23:50:07.870420: step 9277, loss 0.546542.
Train: 2018-08-05T23:50:08.018735: step 9278, loss 0.625858.
Train: 2018-08-05T23:50:08.187870: step 9279, loss 0.581666.
Train: 2018-08-05T23:50:08.363323: step 9280, loss 0.512411.
Test: 2018-08-05T23:50:08.597645: step 9280, loss 0.549621.
Train: 2018-08-05T23:50:08.761151: step 9281, loss 0.625174.
Train: 2018-08-05T23:50:08.931361: step 9282, loss 0.58966.
Train: 2018-08-05T23:50:09.087576: step 9283, loss 0.555259.
Train: 2018-08-05T23:50:09.266744: step 9284, loss 0.572294.
Train: 2018-08-05T23:50:09.430305: step 9285, loss 0.479139.
Train: 2018-08-05T23:50:09.593785: step 9286, loss 0.504848.
Train: 2018-08-05T23:50:09.758375: step 9287, loss 0.539066.
Train: 2018-08-05T23:50:09.936656: step 9288, loss 0.580726.
Train: 2018-08-05T23:50:10.169431: step 9289, loss 0.598841.
Train: 2018-08-05T23:50:10.325675: step 9290, loss 0.573118.
Test: 2018-08-05T23:50:10.568342: step 9290, loss 0.549774.
Train: 2018-08-05T23:50:10.732713: step 9291, loss 0.51339.
Train: 2018-08-05T23:50:10.903002: step 9292, loss 0.530276.
Train: 2018-08-05T23:50:11.067511: step 9293, loss 0.529549.
Train: 2018-08-05T23:50:11.246545: step 9294, loss 0.606546.
Train: 2018-08-05T23:50:11.413288: step 9295, loss 0.598783.
Train: 2018-08-05T23:50:11.566239: step 9296, loss 0.615945.
Train: 2018-08-05T23:50:11.746871: step 9297, loss 0.658201.
Train: 2018-08-05T23:50:11.911612: step 9298, loss 0.555331.
Train: 2018-08-05T23:50:12.075456: step 9299, loss 0.538851.
Train: 2018-08-05T23:50:12.246088: step 9300, loss 0.496153.
Test: 2018-08-05T23:50:12.471955: step 9300, loss 0.549708.
Train: 2018-08-05T23:50:13.228343: step 9301, loss 0.50446.
Train: 2018-08-05T23:50:13.397915: step 9302, loss 0.538553.
Train: 2018-08-05T23:50:13.551807: step 9303, loss 0.606693.
Train: 2018-08-05T23:50:13.720950: step 9304, loss 0.546338.
Train: 2018-08-05T23:50:13.891780: step 9305, loss 0.547146.
Train: 2018-08-05T23:50:14.063198: step 9306, loss 0.511883.
Train: 2018-08-05T23:50:14.221208: step 9307, loss 0.53824.
Train: 2018-08-05T23:50:14.377421: step 9308, loss 0.589613.
Train: 2018-08-05T23:50:14.541125: step 9309, loss 0.520824.
Train: 2018-08-05T23:50:14.720559: step 9310, loss 0.61636.
Test: 2018-08-05T23:50:14.948433: step 9310, loss 0.549349.
Train: 2018-08-05T23:50:15.126879: step 9311, loss 0.529198.
Train: 2018-08-05T23:50:15.283118: step 9312, loss 0.641978.
Train: 2018-08-05T23:50:15.446362: step 9313, loss 0.607147.
Train: 2018-08-05T23:50:15.609194: step 9314, loss 0.589697.
Train: 2018-08-05T23:50:15.772778: step 9315, loss 0.624595.
Train: 2018-08-05T23:50:15.958237: step 9316, loss 0.623936.
Train: 2018-08-05T23:50:16.127162: step 9317, loss 0.486331.
Train: 2018-08-05T23:50:16.293685: step 9318, loss 0.538431.
Train: 2018-08-05T23:50:16.455254: step 9319, loss 0.538091.
Train: 2018-08-05T23:50:16.612959: step 9320, loss 0.623897.
Test: 2018-08-05T23:50:16.860218: step 9320, loss 0.549475.
Train: 2018-08-05T23:50:17.018037: step 9321, loss 0.563723.
Train: 2018-08-05T23:50:17.184515: step 9322, loss 0.623727.
Train: 2018-08-05T23:50:17.353366: step 9323, loss 0.598163.
Train: 2018-08-05T23:50:17.513257: step 9324, loss 0.57274.
Train: 2018-08-05T23:50:17.688727: step 9325, loss 0.57166.
Train: 2018-08-05T23:50:17.851886: step 9326, loss 0.580381.
Train: 2018-08-05T23:50:18.017756: step 9327, loss 0.521717.
Train: 2018-08-05T23:50:18.197176: step 9328, loss 0.56355.
Train: 2018-08-05T23:50:18.367095: step 9329, loss 0.589096.
Train: 2018-08-05T23:50:18.530593: step 9330, loss 0.521915.
Test: 2018-08-05T23:50:18.778021: step 9330, loss 0.549809.
Train: 2018-08-05T23:50:18.948530: step 9331, loss 0.630518.
Train: 2018-08-05T23:50:19.117466: step 9332, loss 0.5723.
Train: 2018-08-05T23:50:19.275484: step 9333, loss 0.580152.
Train: 2018-08-05T23:50:19.447320: step 9334, loss 0.505496.
Train: 2018-08-05T23:50:19.625748: step 9335, loss 0.530724.
Train: 2018-08-05T23:50:19.804186: step 9336, loss 0.580119.
Train: 2018-08-05T23:50:19.960430: step 9337, loss 0.555794.
Train: 2018-08-05T23:50:20.142270: step 9338, loss 0.538586.
Train: 2018-08-05T23:50:20.305352: step 9339, loss 0.530392.
Train: 2018-08-05T23:50:20.468207: step 9340, loss 0.521588.
Test: 2018-08-05T23:50:20.709151: step 9340, loss 0.549672.
Train: 2018-08-05T23:50:20.935231: step 9341, loss 0.614535.
Train: 2018-08-05T23:50:21.098387: step 9342, loss 0.580596.
Train: 2018-08-05T23:50:21.277446: step 9343, loss 0.563741.
Train: 2018-08-05T23:50:21.429152: step 9344, loss 0.563509.
Train: 2018-08-05T23:50:21.611125: step 9345, loss 0.571591.
Train: 2018-08-05T23:50:21.770801: step 9346, loss 0.504477.
Train: 2018-08-05T23:50:21.939651: step 9347, loss 0.563664.
Train: 2018-08-05T23:50:22.115200: step 9348, loss 0.657952.
Train: 2018-08-05T23:50:22.278695: step 9349, loss 0.597961.
Train: 2018-08-05T23:50:22.449677: step 9350, loss 0.581305.
Test: 2018-08-05T23:50:22.678907: step 9350, loss 0.549427.
Train: 2018-08-05T23:50:22.860569: step 9351, loss 0.555656.
Train: 2018-08-05T23:50:23.028166: step 9352, loss 0.580639.
Train: 2018-08-05T23:50:23.198679: step 9353, loss 0.529782.
Train: 2018-08-05T23:50:23.370220: step 9354, loss 0.545947.
Train: 2018-08-05T23:50:23.521316: step 9355, loss 0.529081.
Train: 2018-08-05T23:50:23.685803: step 9356, loss 0.547213.
Train: 2018-08-05T23:50:23.854994: step 9357, loss 0.572544.
Train: 2018-08-05T23:50:24.011192: step 9358, loss 0.580175.
Train: 2018-08-05T23:50:24.180682: step 9359, loss 0.572027.
Train: 2018-08-05T23:50:24.349155: step 9360, loss 0.520751.
Test: 2018-08-05T23:50:24.591031: step 9360, loss 0.549292.
Train: 2018-08-05T23:50:24.747072: step 9361, loss 0.537392.
Train: 2018-08-05T23:50:24.909970: step 9362, loss 0.653741.
Train: 2018-08-05T23:50:25.088536: step 9363, loss 0.521003.
Train: 2018-08-05T23:50:25.262953: step 9364, loss 0.563053.
Train: 2018-08-05T23:50:25.410277: step 9365, loss 0.606303.
Train: 2018-08-05T23:50:25.588798: step 9366, loss 0.580358.
Train: 2018-08-05T23:50:25.751681: step 9367, loss 0.606847.
Train: 2018-08-05T23:50:25.930870: step 9368, loss 0.63116.
Train: 2018-08-05T23:50:26.103175: step 9369, loss 0.614803.
Train: 2018-08-05T23:50:26.270761: step 9370, loss 0.546602.
Test: 2018-08-05T23:50:26.496938: step 9370, loss 0.549444.
Train: 2018-08-05T23:50:26.667118: step 9371, loss 0.530088.
Train: 2018-08-05T23:50:26.861369: step 9372, loss 0.63932.
Train: 2018-08-05T23:50:27.025892: step 9373, loss 0.521166.
Train: 2018-08-05T23:50:27.189088: step 9374, loss 0.580363.
Train: 2018-08-05T23:50:27.360923: step 9375, loss 0.546811.
Train: 2018-08-05T23:50:27.520860: step 9376, loss 0.555435.
Train: 2018-08-05T23:50:27.692694: step 9377, loss 0.563033.
Train: 2018-08-05T23:50:27.847061: step 9378, loss 0.538745.
Train: 2018-08-05T23:50:28.016155: step 9379, loss 0.588214.
Train: 2018-08-05T23:50:28.176439: step 9380, loss 0.563505.
Test: 2018-08-05T23:50:28.422303: step 9380, loss 0.549647.
Train: 2018-08-05T23:50:28.585639: step 9381, loss 0.497224.
Train: 2018-08-05T23:50:28.750564: step 9382, loss 0.571768.
Train: 2018-08-05T23:50:28.922774: step 9383, loss 0.547292.
Train: 2018-08-05T23:50:29.085917: step 9384, loss 0.571555.
Train: 2018-08-05T23:50:29.250941: step 9385, loss 0.546863.
Train: 2018-08-05T23:50:29.413737: step 9386, loss 0.520983.
Train: 2018-08-05T23:50:29.592370: step 9387, loss 0.57197.
Train: 2018-08-05T23:50:29.754898: step 9388, loss 0.605707.
Train: 2018-08-05T23:50:29.917837: step 9389, loss 0.579789.
Train: 2018-08-05T23:50:30.097965: step 9390, loss 0.538363.
Test: 2018-08-05T23:50:30.323714: step 9390, loss 0.549222.
Train: 2018-08-05T23:50:30.497191: step 9391, loss 0.538154.
Train: 2018-08-05T23:50:30.653404: step 9392, loss 0.631728.
Train: 2018-08-05T23:50:30.838828: step 9393, loss 0.512727.
Train: 2018-08-05T23:50:31.007745: step 9394, loss 0.563635.
Train: 2018-08-05T23:50:31.186747: step 9395, loss 0.61432.
Train: 2018-08-05T23:50:31.339907: step 9396, loss 0.580193.
Train: 2018-08-05T23:50:31.511741: step 9397, loss 0.580922.
Train: 2018-08-05T23:50:31.675282: step 9398, loss 0.580534.
Train: 2018-08-05T23:50:31.838762: step 9399, loss 0.537678.
Train: 2018-08-05T23:50:32.010597: step 9400, loss 0.605.
Test: 2018-08-05T23:50:32.243927: step 9400, loss 0.549112.
Train: 2018-08-05T23:50:32.960905: step 9401, loss 0.555456.
Train: 2018-08-05T23:50:33.136474: step 9402, loss 0.537894.
Train: 2018-08-05T23:50:33.312010: step 9403, loss 0.529619.
Train: 2018-08-05T23:50:33.481802: step 9404, loss 0.520891.
Train: 2018-08-05T23:50:33.637472: step 9405, loss 0.605959.
Train: 2018-08-05T23:50:33.816750: step 9406, loss 0.521275.
Train: 2018-08-05T23:50:33.980348: step 9407, loss 0.512509.
Train: 2018-08-05T23:50:34.143830: step 9408, loss 0.571924.
Train: 2018-08-05T23:50:34.321838: step 9409, loss 0.554813.
Train: 2018-08-05T23:50:34.480200: step 9410, loss 0.572059.
Test: 2018-08-05T23:50:34.721878: step 9410, loss 0.548844.
Train: 2018-08-05T23:50:34.901242: step 9411, loss 0.554452.
Train: 2018-08-05T23:50:35.071389: step 9412, loss 0.624405.
Train: 2018-08-05T23:50:35.234890: step 9413, loss 0.502879.
Train: 2018-08-05T23:50:35.398333: step 9414, loss 0.545784.
Train: 2018-08-05T23:50:35.570168: step 9415, loss 0.55448.
Train: 2018-08-05T23:50:35.733872: step 9416, loss 0.632774.
Train: 2018-08-05T23:50:35.897533: step 9417, loss 0.537197.
Train: 2018-08-05T23:50:36.077220: step 9418, loss 0.528933.
Train: 2018-08-05T23:50:36.245761: step 9419, loss 0.53684.
Train: 2018-08-05T23:50:36.420269: step 9420, loss 0.546162.
Test: 2018-08-05T23:50:36.652825: step 9420, loss 0.548655.
Train: 2018-08-05T23:50:36.816426: step 9421, loss 0.572417.
Train: 2018-08-05T23:50:36.988261: step 9422, loss 0.475632.
Train: 2018-08-05T23:50:37.159046: step 9423, loss 0.580642.
Train: 2018-08-05T23:50:37.322545: step 9424, loss 0.519285.
Train: 2018-08-05T23:50:37.486038: step 9425, loss 0.571797.
Train: 2018-08-05T23:50:37.650599: step 9426, loss 0.492663.
Train: 2018-08-05T23:50:37.822459: step 9427, loss 0.45725.
Train: 2018-08-05T23:50:37.987033: step 9428, loss 0.653194.
Train: 2018-08-05T23:50:38.163722: step 9429, loss 0.544941.
Train: 2018-08-05T23:50:38.320664: step 9430, loss 0.518657.
Test: 2018-08-05T23:50:38.562260: step 9430, loss 0.548426.
Train: 2018-08-05T23:50:38.725558: step 9431, loss 0.518863.
Train: 2018-08-05T23:50:38.907147: step 9432, loss 0.599765.
Train: 2018-08-05T23:50:39.063361: step 9433, loss 0.591569.
Train: 2018-08-05T23:50:39.226381: step 9434, loss 0.618364.
Train: 2018-08-05T23:50:39.389220: step 9435, loss 0.563542.
Train: 2018-08-05T23:50:39.558704: step 9436, loss 0.54577.
Train: 2018-08-05T23:50:39.737849: step 9437, loss 0.46372.
Train: 2018-08-05T23:50:39.894092: step 9438, loss 0.563932.
Train: 2018-08-05T23:50:40.073122: step 9439, loss 0.518555.
Train: 2018-08-05T23:50:40.231967: step 9440, loss 0.52711.
Test: 2018-08-05T23:50:40.481935: step 9440, loss 0.548371.
Train: 2018-08-05T23:50:40.651890: step 9441, loss 0.572965.
Train: 2018-08-05T23:50:40.822564: step 9442, loss 0.554382.
Train: 2018-08-05T23:50:40.996428: step 9443, loss 0.544592.
Train: 2018-08-05T23:50:41.168228: step 9444, loss 0.544995.
Train: 2018-08-05T23:50:41.324473: step 9445, loss 0.527094.
Train: 2018-08-05T23:50:41.503634: step 9446, loss 0.545031.
Train: 2018-08-05T23:50:41.667267: step 9447, loss 0.581938.
Train: 2018-08-05T23:50:41.830792: step 9448, loss 0.627759.
Train: 2018-08-05T23:50:41.995542: step 9449, loss 0.572871.
Train: 2018-08-05T23:50:42.167376: step 9450, loss 0.472408.
Test: 2018-08-05T23:50:42.401467: step 9450, loss 0.548335.
Train: 2018-08-05T23:50:42.579936: step 9451, loss 0.545469.
Train: 2018-08-05T23:50:42.746170: step 9452, loss 0.481845.
Train: 2018-08-05T23:50:42.902157: step 9453, loss 0.665028.
Train: 2018-08-05T23:50:43.090809: step 9454, loss 0.617818.
Train: 2018-08-05T23:50:43.261348: step 9455, loss 0.599738.
Train: 2018-08-05T23:50:43.422076: step 9456, loss 0.509334.
Train: 2018-08-05T23:50:43.584878: step 9457, loss 0.545214.
Train: 2018-08-05T23:50:43.767009: step 9458, loss 0.581926.
Train: 2018-08-05T23:50:43.927055: step 9459, loss 0.554695.
Train: 2018-08-05T23:50:44.090271: step 9460, loss 0.545597.
Test: 2018-08-05T23:50:44.335296: step 9460, loss 0.548336.
Train: 2018-08-05T23:50:44.498162: step 9461, loss 0.635167.
Train: 2018-08-05T23:50:44.676703: step 9462, loss 0.536377.
Train: 2018-08-05T23:50:44.840160: step 9463, loss 0.626368.
Train: 2018-08-05T23:50:45.003305: step 9464, loss 0.527734.
Train: 2018-08-05T23:50:45.167980: step 9465, loss 0.607349.
Train: 2018-08-05T23:50:45.331149: step 9466, loss 0.519249.
Train: 2018-08-05T23:50:45.487363: step 9467, loss 0.572219.
Train: 2018-08-05T23:50:45.661904: step 9468, loss 0.528108.
Train: 2018-08-05T23:50:45.833769: step 9469, loss 0.589219.
Train: 2018-08-05T23:50:45.988183: step 9470, loss 0.641621.
Test: 2018-08-05T23:50:46.240021: step 9470, loss 0.548573.
Train: 2018-08-05T23:50:46.418544: step 9471, loss 0.528666.
Train: 2018-08-05T23:50:46.576969: step 9472, loss 0.563027.
Train: 2018-08-05T23:50:46.743418: step 9473, loss 0.537443.
Train: 2018-08-05T23:50:46.919096: step 9474, loss 0.54589.
Train: 2018-08-05T23:50:47.075340: step 9475, loss 0.537165.
Train: 2018-08-05T23:50:47.238907: step 9476, loss 0.63207.
Train: 2018-08-05T23:50:47.418325: step 9477, loss 0.580072.
Train: 2018-08-05T23:50:47.581804: step 9478, loss 0.631488.
Train: 2018-08-05T23:50:47.761063: step 9479, loss 0.520536.
Train: 2018-08-05T23:50:47.926767: step 9480, loss 0.580333.
Test: 2018-08-05T23:50:48.157987: step 9480, loss 0.548864.
Train: 2018-08-05T23:50:48.327775: step 9481, loss 0.47834.
Train: 2018-08-05T23:50:48.477800: step 9482, loss 0.596387.
Train: 2018-08-05T23:50:48.656701: step 9483, loss 0.648108.
Train: 2018-08-05T23:50:48.820205: step 9484, loss 0.503683.
Train: 2018-08-05T23:50:48.990783: step 9485, loss 0.563158.
Train: 2018-08-05T23:50:49.148744: step 9486, loss 0.588445.
Train: 2018-08-05T23:50:49.310287: step 9487, loss 0.571753.
Train: 2018-08-05T23:50:49.474553: step 9488, loss 0.5208.
Train: 2018-08-05T23:50:49.639302: step 9489, loss 0.630394.
Train: 2018-08-05T23:50:49.817711: step 9490, loss 0.597208.
Test: 2018-08-05T23:50:50.058810: step 9490, loss 0.549043.
Train: 2018-08-05T23:50:50.228026: step 9491, loss 0.571524.
Train: 2018-08-05T23:50:50.393406: step 9492, loss 0.604971.
Train: 2018-08-05T23:50:50.546132: step 9493, loss 0.630049.
Train: 2018-08-05T23:50:50.709201: step 9494, loss 0.546472.
Train: 2018-08-05T23:50:50.885817: step 9495, loss 0.579789.
Train: 2018-08-05T23:50:51.057653: step 9496, loss 0.571042.
Train: 2018-08-05T23:50:51.220436: step 9497, loss 0.57131.
Train: 2018-08-05T23:50:51.383912: step 9498, loss 0.620675.
Train: 2018-08-05T23:50:51.537112: step 9499, loss 0.563216.
Train: 2018-08-05T23:50:51.712646: step 9500, loss 0.505717.
Test: 2018-08-05T23:50:51.954249: step 9500, loss 0.54961.
Train: 2018-08-05T23:50:52.682742: step 9501, loss 0.579551.
Train: 2018-08-05T23:50:52.870229: step 9502, loss 0.538989.
Train: 2018-08-05T23:50:53.045355: step 9503, loss 0.546958.
Train: 2018-08-05T23:50:53.213873: step 9504, loss 0.612055.
Train: 2018-08-05T23:50:53.383452: step 9505, loss 0.563585.
Train: 2018-08-05T23:50:53.538772: step 9506, loss 0.53891.
Train: 2018-08-05T23:50:53.717583: step 9507, loss 0.587491.
Train: 2018-08-05T23:50:53.889420: step 9508, loss 0.652829.
Train: 2018-08-05T23:50:54.055319: step 9509, loss 0.457188.
Train: 2018-08-05T23:50:54.218447: step 9510, loss 0.595889.
Test: 2018-08-05T23:50:54.450297: step 9510, loss 0.549547.
Train: 2018-08-05T23:50:54.628857: step 9511, loss 0.588137.
Train: 2018-08-05T23:50:54.792426: step 9512, loss 0.457089.
Train: 2018-08-05T23:50:54.962155: step 9513, loss 0.527445.
Train: 2018-08-05T23:50:55.127582: step 9514, loss 0.505406.
Train: 2018-08-05T23:50:55.301113: step 9515, loss 0.580019.
Train: 2018-08-05T23:50:55.472949: step 9516, loss 0.613185.
Train: 2018-08-05T23:50:55.627099: step 9517, loss 0.563438.
Train: 2018-08-05T23:50:55.795972: step 9518, loss 0.580151.
Train: 2018-08-05T23:50:55.971557: step 9519, loss 0.4872.
Train: 2018-08-05T23:50:56.146016: step 9520, loss 0.511891.
Test: 2018-08-05T23:50:56.383408: step 9520, loss 0.548795.
Train: 2018-08-05T23:50:56.550341: step 9521, loss 0.596901.
Train: 2018-08-05T23:50:56.706555: step 9522, loss 0.546375.
Train: 2018-08-05T23:50:56.870215: step 9523, loss 0.511726.
Train: 2018-08-05T23:50:57.033713: step 9524, loss 0.571656.
Train: 2018-08-05T23:50:57.213041: step 9525, loss 0.580259.
Train: 2018-08-05T23:50:57.370722: step 9526, loss 0.571586.
Train: 2018-08-05T23:50:57.540893: step 9527, loss 0.554732.
Train: 2018-08-05T23:50:57.704433: step 9528, loss 0.484394.
Train: 2018-08-05T23:50:57.867819: step 9529, loss 0.527815.
Train: 2018-08-05T23:50:58.046656: step 9530, loss 0.580361.
Test: 2018-08-05T23:50:58.290655: step 9530, loss 0.548305.
Train: 2018-08-05T23:50:58.453574: step 9531, loss 0.57181.
Train: 2018-08-05T23:50:58.625439: step 9532, loss 0.668886.
Train: 2018-08-05T23:50:58.778983: step 9533, loss 0.519298.
Train: 2018-08-05T23:50:58.941821: step 9534, loss 0.554637.
Train: 2018-08-05T23:50:59.120999: step 9535, loss 0.528018.
Train: 2018-08-05T23:50:59.285063: step 9536, loss 0.589699.
Train: 2018-08-05T23:50:59.441307: step 9537, loss 0.589856.
Train: 2018-08-05T23:50:59.614871: step 9538, loss 0.553603.
Train: 2018-08-05T23:50:59.771050: step 9539, loss 0.545343.
Train: 2018-08-05T23:50:59.941025: step 9540, loss 0.60717.
Test: 2018-08-05T23:51:00.187944: step 9540, loss 0.548251.
Train: 2018-08-05T23:51:00.347862: step 9541, loss 0.580553.
Train: 2018-08-05T23:51:00.527929: step 9542, loss 0.60738.
Train: 2018-08-05T23:51:00.692458: step 9543, loss 0.571592.
Train: 2018-08-05T23:51:00.864323: step 9544, loss 0.580804.
Train: 2018-08-05T23:51:01.043555: step 9545, loss 0.493057.
Train: 2018-08-05T23:51:01.207181: step 9546, loss 0.553989.
Train: 2018-08-05T23:51:01.371713: step 9547, loss 0.562564.
Train: 2018-08-05T23:51:01.534686: step 9548, loss 0.52801.
Train: 2018-08-05T23:51:01.706522: step 9549, loss 0.580417.
Train: 2018-08-05T23:51:01.877593: step 9550, loss 0.589155.
Test: 2018-08-05T23:51:02.118339: step 9550, loss 0.548377.
Train: 2018-08-05T23:51:02.281885: step 9551, loss 0.510694.
Train: 2018-08-05T23:51:02.445530: step 9552, loss 0.571235.
Train: 2018-08-05T23:51:02.624802: step 9553, loss 0.571662.
Train: 2018-08-05T23:51:02.789349: step 9554, loss 0.511193.
Train: 2018-08-05T23:51:02.952836: step 9555, loss 0.519543.
Train: 2018-08-05T23:51:03.132930: step 9556, loss 0.579955.
Train: 2018-08-05T23:51:03.310455: step 9557, loss 0.623995.
Train: 2018-08-05T23:51:03.475056: step 9558, loss 0.493338.
Train: 2018-08-05T23:51:03.666426: step 9559, loss 0.615177.
Train: 2018-08-05T23:51:03.829925: step 9560, loss 0.519945.
Test: 2018-08-05T23:51:04.080710: step 9560, loss 0.548324.
Train: 2018-08-05T23:51:04.244088: step 9561, loss 0.641916.
Train: 2018-08-05T23:51:04.418386: step 9562, loss 0.528705.
Train: 2018-08-05T23:51:04.579038: step 9563, loss 0.606247.
Train: 2018-08-05T23:51:04.745111: step 9564, loss 0.502718.
Train: 2018-08-05T23:51:04.923496: step 9565, loss 0.597179.
Train: 2018-08-05T23:51:05.086392: step 9566, loss 0.53709.
Train: 2018-08-05T23:51:05.259062: step 9567, loss 0.606889.
Train: 2018-08-05T23:51:05.411756: step 9568, loss 0.589007.
Train: 2018-08-05T23:51:05.575063: step 9569, loss 0.519823.
Train: 2018-08-05T23:51:05.738239: step 9570, loss 0.485003.
Test: 2018-08-05T23:51:05.991009: step 9570, loss 0.54838.
Train: 2018-08-05T23:51:06.160371: step 9571, loss 0.545643.
Train: 2018-08-05T23:51:06.331944: step 9572, loss 0.502428.
Train: 2018-08-05T23:51:06.495912: step 9573, loss 0.528327.
Train: 2018-08-05T23:51:06.673633: step 9574, loss 0.605976.
Train: 2018-08-05T23:51:06.838974: step 9575, loss 0.633324.
Train: 2018-08-05T23:51:07.002241: step 9576, loss 0.510686.
Train: 2018-08-05T23:51:07.181335: step 9577, loss 0.545756.
Train: 2018-08-05T23:51:07.338311: step 9578, loss 0.501523.
Train: 2018-08-05T23:51:07.501419: step 9579, loss 0.63272.
Train: 2018-08-05T23:51:07.674389: step 9580, loss 0.483554.
Test: 2018-08-05T23:51:07.909115: step 9580, loss 0.54819.
Train: 2018-08-05T23:51:08.080981: step 9581, loss 0.563053.
Train: 2018-08-05T23:51:08.251089: step 9582, loss 0.580969.
Train: 2018-08-05T23:51:08.413988: step 9583, loss 0.606748.
Train: 2018-08-05T23:51:08.577430: step 9584, loss 0.553848.
Train: 2018-08-05T23:51:08.740570: step 9585, loss 0.598267.
Train: 2018-08-05T23:51:08.912438: step 9586, loss 0.589687.
Train: 2018-08-05T23:51:09.090936: step 9587, loss 0.606979.
Train: 2018-08-05T23:51:09.262743: step 9588, loss 0.580821.
Train: 2018-08-05T23:51:09.434608: step 9589, loss 0.57152.
Train: 2018-08-05T23:51:09.598291: step 9590, loss 0.632956.
Test: 2018-08-05T23:51:09.847300: step 9590, loss 0.5483.
Train: 2018-08-05T23:51:10.001225: step 9591, loss 0.667105.
Train: 2018-08-05T23:51:10.173029: step 9592, loss 0.589048.
Train: 2018-08-05T23:51:10.353166: step 9593, loss 0.519923.
Train: 2018-08-05T23:51:10.509410: step 9594, loss 0.554573.
Train: 2018-08-05T23:51:10.673174: step 9595, loss 0.597106.
Train: 2018-08-05T23:51:10.852306: step 9596, loss 0.546043.
Train: 2018-08-05T23:51:11.015953: step 9597, loss 0.520531.
Train: 2018-08-05T23:51:11.172132: step 9598, loss 0.59647.
Train: 2018-08-05T23:51:11.342914: step 9599, loss 0.48753.
Train: 2018-08-05T23:51:11.505587: step 9600, loss 0.545952.
Test: 2018-08-05T23:51:11.748121: step 9600, loss 0.548784.
Train: 2018-08-05T23:51:12.488382: step 9601, loss 0.528967.
Train: 2018-08-05T23:51:12.653267: step 9602, loss 0.57107.
Train: 2018-08-05T23:51:12.830749: step 9603, loss 0.546025.
Train: 2018-08-05T23:51:12.987972: step 9604, loss 0.571269.
Train: 2018-08-05T23:51:13.165841: step 9605, loss 0.621669.
Train: 2018-08-05T23:51:13.333368: step 9606, loss 0.562856.
Train: 2018-08-05T23:51:13.492718: step 9607, loss 0.579681.
Train: 2018-08-05T23:51:13.668761: step 9608, loss 0.588145.
Train: 2018-08-05T23:51:13.827445: step 9609, loss 0.562717.
Train: 2018-08-05T23:51:13.991442: step 9610, loss 0.604947.
Test: 2018-08-05T23:51:14.232740: step 9610, loss 0.548754.
Train: 2018-08-05T23:51:14.407161: step 9611, loss 0.596422.
Train: 2018-08-05T23:51:14.578995: step 9612, loss 0.621383.
Train: 2018-08-05T23:51:14.748865: step 9613, loss 0.521108.
Train: 2018-08-05T23:51:14.919794: step 9614, loss 0.529307.
Train: 2018-08-05T23:51:15.088223: step 9615, loss 0.529206.
Train: 2018-08-05T23:51:15.249926: step 9616, loss 0.613077.
Train: 2018-08-05T23:51:15.421760: step 9617, loss 0.604642.
Train: 2018-08-05T23:51:15.585237: step 9618, loss 0.554313.
Train: 2018-08-05T23:51:15.748841: step 9619, loss 0.57124.
Train: 2018-08-05T23:51:15.929067: step 9620, loss 0.546092.
Test: 2018-08-05T23:51:16.170263: step 9620, loss 0.548972.
Train: 2018-08-05T23:51:16.343800: step 9621, loss 0.554474.
Train: 2018-08-05T23:51:16.516473: step 9622, loss 0.629581.
Train: 2018-08-05T23:51:16.676693: step 9623, loss 0.512895.
Train: 2018-08-05T23:51:16.839623: step 9624, loss 0.521208.
Train: 2018-08-05T23:51:17.003134: step 9625, loss 0.629512.
Train: 2018-08-05T23:51:17.167640: step 9626, loss 0.554474.
Train: 2018-08-05T23:51:17.339474: step 9627, loss 0.546337.
Train: 2018-08-05T23:51:17.503728: step 9628, loss 0.562943.
Train: 2018-08-05T23:51:17.677389: step 9629, loss 0.596406.
Train: 2018-08-05T23:51:17.837338: step 9630, loss 0.596041.
Test: 2018-08-05T23:51:18.079856: step 9630, loss 0.548934.
Train: 2018-08-05T23:51:18.244031: step 9631, loss 0.513004.
Train: 2018-08-05T23:51:18.424496: step 9632, loss 0.462871.
Train: 2018-08-05T23:51:18.592189: step 9633, loss 0.596101.
Train: 2018-08-05T23:51:18.755443: step 9634, loss 0.579794.
Train: 2018-08-05T23:51:18.934253: step 9635, loss 0.579974.
Train: 2018-08-05T23:51:19.087680: step 9636, loss 0.47879.
Train: 2018-08-05T23:51:19.251257: step 9637, loss 0.546015.
Train: 2018-08-05T23:51:19.435276: step 9638, loss 0.596459.
Train: 2018-08-05T23:51:19.586344: step 9639, loss 0.630298.
Train: 2018-08-05T23:51:19.760833: step 9640, loss 0.545868.
Test: 2018-08-05T23:51:20.016008: step 9640, loss 0.548528.
Train: 2018-08-05T23:51:20.180969: step 9641, loss 0.554465.
Train: 2018-08-05T23:51:20.349879: step 9642, loss 0.562973.
Train: 2018-08-05T23:51:20.510093: step 9643, loss 0.571087.
Train: 2018-08-05T23:51:20.681922: step 9644, loss 0.605275.
Train: 2018-08-05T23:51:20.845538: step 9645, loss 0.503141.
Train: 2018-08-05T23:51:21.017347: step 9646, loss 0.562758.
Train: 2018-08-05T23:51:21.180826: step 9647, loss 0.545777.
Train: 2018-08-05T23:51:21.344283: step 9648, loss 0.562745.
Train: 2018-08-05T23:51:21.507774: step 9649, loss 0.58832.
Train: 2018-08-05T23:51:21.679639: step 9650, loss 0.51978.
Test: 2018-08-05T23:51:21.917124: step 9650, loss 0.548348.
Train: 2018-08-05T23:51:22.080613: step 9651, loss 0.588764.
Train: 2018-08-05T23:51:22.259949: step 9652, loss 0.502708.
Train: 2018-08-05T23:51:22.423424: step 9653, loss 0.597352.
Train: 2018-08-05T23:51:22.586868: step 9654, loss 0.597237.
Train: 2018-08-05T23:51:22.750418: step 9655, loss 0.493402.
Train: 2018-08-05T23:51:22.940074: step 9656, loss 0.571503.
Train: 2018-08-05T23:51:23.136499: step 9657, loss 0.579588.
Train: 2018-08-05T23:51:23.341006: step 9658, loss 0.519446.
Train: 2018-08-05T23:51:23.496019: step 9659, loss 0.597301.
Train: 2018-08-05T23:51:23.674638: step 9660, loss 0.562802.
Test: 2018-08-05T23:51:23.911339: step 9660, loss 0.548205.
Train: 2018-08-05T23:51:24.096906: step 9661, loss 0.510314.
Train: 2018-08-05T23:51:24.275475: step 9662, loss 0.571569.
Train: 2018-08-05T23:51:24.439435: step 9663, loss 0.510582.
Train: 2018-08-05T23:51:24.602808: step 9664, loss 0.506975.
Train: 2018-08-05T23:51:24.774643: step 9665, loss 0.615238.
Train: 2018-08-05T23:51:24.955743: step 9666, loss 0.527862.
Train: 2018-08-05T23:51:25.122301: step 9667, loss 0.571986.
Train: 2018-08-05T23:51:25.297598: step 9668, loss 0.536274.
Train: 2018-08-05T23:51:25.469411: step 9669, loss 0.580268.
Train: 2018-08-05T23:51:25.622951: step 9670, loss 0.553814.
Test: 2018-08-05T23:51:25.864311: step 9670, loss 0.548028.
Train: 2018-08-05T23:51:26.038892: step 9671, loss 0.500729.
Train: 2018-08-05T23:51:26.219793: step 9672, loss 0.571902.
Train: 2018-08-05T23:51:26.388312: step 9673, loss 0.553775.
Train: 2018-08-05T23:51:26.541699: step 9674, loss 0.589706.
Train: 2018-08-05T23:51:26.704618: step 9675, loss 0.536106.
Train: 2018-08-05T23:51:26.876482: step 9676, loss 0.580748.
Train: 2018-08-05T23:51:27.039975: step 9677, loss 0.545217.
Train: 2018-08-05T23:51:27.208979: step 9678, loss 0.562849.
Train: 2018-08-05T23:51:27.365224: step 9679, loss 0.589366.
Train: 2018-08-05T23:51:27.528856: step 9680, loss 0.607152.
Test: 2018-08-05T23:51:27.770665: step 9680, loss 0.547979.
Train: 2018-08-05T23:51:27.934184: step 9681, loss 0.491725.
Train: 2018-08-05T23:51:28.113256: step 9682, loss 0.473712.
Train: 2018-08-05T23:51:28.267564: step 9683, loss 0.562507.
Train: 2018-08-05T23:51:28.439431: step 9684, loss 0.625211.
Train: 2018-08-05T23:51:28.603045: step 9685, loss 0.517983.
Train: 2018-08-05T23:51:28.781829: step 9686, loss 0.580518.
Train: 2018-08-05T23:51:28.946412: step 9687, loss 0.500712.
Train: 2018-08-05T23:51:29.110974: step 9688, loss 0.527293.
Train: 2018-08-05T23:51:29.275425: step 9689, loss 0.518481.
Train: 2018-08-05T23:51:29.440058: step 9690, loss 0.500339.
Test: 2018-08-05T23:51:29.681661: step 9690, loss 0.54791.
Train: 2018-08-05T23:51:29.860258: step 9691, loss 0.571427.
Train: 2018-08-05T23:51:30.026485: step 9692, loss 0.51783.
Train: 2018-08-05T23:51:30.189355: step 9693, loss 0.517739.
Train: 2018-08-05T23:51:30.352277: step 9694, loss 0.54518.
Train: 2018-08-05T23:51:30.524143: step 9695, loss 0.53596.
Train: 2018-08-05T23:51:30.687014: step 9696, loss 0.481012.
Train: 2018-08-05T23:51:30.851698: step 9697, loss 0.600326.
Train: 2018-08-05T23:51:31.047407: step 9698, loss 0.563524.
Train: 2018-08-05T23:51:31.214780: step 9699, loss 0.517502.
Train: 2018-08-05T23:51:31.375215: step 9700, loss 0.581498.
Test: 2018-08-05T23:51:31.623224: step 9700, loss 0.547871.
Train: 2018-08-05T23:51:32.378653: step 9701, loss 0.470863.
Train: 2018-08-05T23:51:32.547465: step 9702, loss 0.516949.
Train: 2018-08-05T23:51:32.710222: step 9703, loss 0.508066.
Train: 2018-08-05T23:51:32.889372: step 9704, loss 0.57258.
Train: 2018-08-05T23:51:33.061772: step 9705, loss 0.582111.
Train: 2018-08-05T23:51:33.231317: step 9706, loss 0.573145.
Train: 2018-08-05T23:51:33.413829: step 9707, loss 0.59133.
Train: 2018-08-05T23:51:33.573182: step 9708, loss 0.572941.
Train: 2018-08-05T23:51:33.751362: step 9709, loss 0.55398.
Train: 2018-08-05T23:51:33.914213: step 9710, loss 0.553895.
Test: 2018-08-05T23:51:34.155915: step 9710, loss 0.547898.
Train: 2018-08-05T23:51:34.319534: step 9711, loss 0.553841.
Train: 2018-08-05T23:51:34.498985: step 9712, loss 0.573038.
Train: 2018-08-05T23:51:34.684527: step 9713, loss 0.479285.
Train: 2018-08-05T23:51:34.843181: step 9714, loss 0.665979.
Train: 2018-08-05T23:51:35.019375: step 9715, loss 0.461502.
Train: 2018-08-05T23:51:35.182761: step 9716, loss 0.600726.
Train: 2018-08-05T23:51:35.347945: step 9717, loss 0.554219.
Train: 2018-08-05T23:51:35.510843: step 9718, loss 0.535936.
Train: 2018-08-05T23:51:35.667058: step 9719, loss 0.544498.
Train: 2018-08-05T23:51:35.829982: step 9720, loss 0.608845.
Test: 2018-08-05T23:51:36.083241: step 9720, loss 0.547844.
Train: 2018-08-05T23:51:36.260765: step 9721, loss 0.508194.
Train: 2018-08-05T23:51:36.427348: step 9722, loss 0.590138.
Train: 2018-08-05T23:51:36.582084: step 9723, loss 0.526515.
Train: 2018-08-05T23:51:36.753892: step 9724, loss 0.499445.
Train: 2018-08-05T23:51:36.910138: step 9725, loss 0.544557.
Train: 2018-08-05T23:51:37.072911: step 9726, loss 0.508094.
Train: 2018-08-05T23:51:37.244776: step 9727, loss 0.590828.
Train: 2018-08-05T23:51:37.398216: step 9728, loss 0.481267.
Train: 2018-08-05T23:51:37.567686: step 9729, loss 0.527123.
Train: 2018-08-05T23:51:37.723885: step 9730, loss 0.63606.
Test: 2018-08-05T23:51:37.965597: step 9730, loss 0.547834.
Train: 2018-08-05T23:51:38.137551: step 9731, loss 0.581171.
Train: 2018-08-05T23:51:38.301085: step 9732, loss 0.627059.
Train: 2018-08-05T23:51:38.464635: step 9733, loss 0.590184.
Train: 2018-08-05T23:51:38.628306: step 9734, loss 0.626219.
Train: 2018-08-05T23:51:38.792691: step 9735, loss 0.599232.
Train: 2018-08-05T23:51:38.977159: step 9736, loss 0.616094.
Train: 2018-08-05T23:51:39.135490: step 9737, loss 0.518037.
Train: 2018-08-05T23:51:39.299264: step 9738, loss 0.553791.
Train: 2018-08-05T23:51:39.478408: step 9739, loss 0.571984.
Train: 2018-08-05T23:51:39.641845: step 9740, loss 0.562906.
Test: 2018-08-05T23:51:39.884537: step 9740, loss 0.548025.
Train: 2018-08-05T23:51:40.039808: step 9741, loss 0.537148.
Train: 2018-08-05T23:51:40.211642: step 9742, loss 0.579656.
Train: 2018-08-05T23:51:40.390876: step 9743, loss 0.519412.
Train: 2018-08-05T23:51:40.553887: step 9744, loss 0.536656.
Train: 2018-08-05T23:51:40.717393: step 9745, loss 0.579898.
Train: 2018-08-05T23:51:40.896494: step 9746, loss 0.536535.
Train: 2018-08-05T23:51:41.065617: step 9747, loss 0.562558.
Train: 2018-08-05T23:51:41.221831: step 9748, loss 0.484713.
Train: 2018-08-05T23:51:41.385412: step 9749, loss 0.571091.
Train: 2018-08-05T23:51:41.548947: step 9750, loss 0.658106.
Test: 2018-08-05T23:51:41.790638: step 9750, loss 0.54817.
Train: 2018-08-05T23:51:41.969909: step 9751, loss 0.657783.
Train: 2018-08-05T23:51:42.124997: step 9752, loss 0.57062.
Train: 2018-08-05T23:51:42.296836: step 9753, loss 0.665628.
Train: 2018-08-05T23:51:42.460365: step 9754, loss 0.502647.
Train: 2018-08-05T23:51:42.624047: step 9755, loss 0.571194.
Train: 2018-08-05T23:51:42.787552: step 9756, loss 0.605042.
Train: 2018-08-05T23:51:42.942037: step 9757, loss 0.621561.
Train: 2018-08-05T23:51:43.124105: step 9758, loss 0.537596.
Train: 2018-08-05T23:51:43.290704: step 9759, loss 0.571176.
Train: 2018-08-05T23:51:43.461054: step 9760, loss 0.571489.
Test: 2018-08-05T23:51:43.687155: step 9760, loss 0.548826.
Train: 2018-08-05T23:51:43.850858: step 9761, loss 0.587578.
Train: 2018-08-05T23:51:44.014446: step 9762, loss 0.554484.
Train: 2018-08-05T23:51:44.185751: step 9763, loss 0.587658.
Train: 2018-08-05T23:51:44.342982: step 9764, loss 0.537957.
Train: 2018-08-05T23:51:44.520797: step 9765, loss 0.604219.
Train: 2018-08-05T23:51:44.684062: step 9766, loss 0.505183.
Train: 2018-08-05T23:51:44.844185: step 9767, loss 0.554731.
Train: 2018-08-05T23:51:45.008048: step 9768, loss 0.579746.
Train: 2018-08-05T23:51:45.179916: step 9769, loss 0.603831.
Train: 2018-08-05T23:51:45.342786: step 9770, loss 0.603813.
Test: 2018-08-05T23:51:45.586360: step 9770, loss 0.549155.
Train: 2018-08-05T23:51:45.763635: step 9771, loss 0.579144.
Train: 2018-08-05T23:51:45.926910: step 9772, loss 0.579001.
Train: 2018-08-05T23:51:46.096942: step 9773, loss 0.63607.
Train: 2018-08-05T23:51:46.264518: step 9774, loss 0.571178.
Train: 2018-08-05T23:51:46.433043: step 9775, loss 0.611579.
Train: 2018-08-05T23:51:46.587170: step 9776, loss 0.555009.
Train: 2018-08-05T23:51:46.743413: step 9777, loss 0.538342.
Train: 2018-08-05T23:51:46.915437: step 9778, loss 0.547249.
Train: 2018-08-05T23:51:47.081924: step 9779, loss 0.523048.
Train: 2018-08-05T23:51:47.250885: step 9780, loss 0.611112.
Test: 2018-08-05T23:51:47.491991: step 9780, loss 0.549513.
Train: 2018-08-05T23:51:47.656502: step 9781, loss 0.538978.
Train: 2018-08-05T23:51:47.812715: step 9782, loss 0.571033.
Train: 2018-08-05T23:51:47.975676: step 9783, loss 0.55462.
Train: 2018-08-05T23:51:48.154945: step 9784, loss 0.514159.
Train: 2018-08-05T23:51:48.318407: step 9785, loss 0.595353.
Train: 2018-08-05T23:51:48.488517: step 9786, loss 0.603469.
Train: 2018-08-05T23:51:48.644730: step 9787, loss 0.603357.
Train: 2018-08-05T23:51:48.809693: step 9788, loss 0.595018.
Train: 2018-08-05T23:51:48.972601: step 9789, loss 0.571102.
Train: 2018-08-05T23:51:49.145848: step 9790, loss 0.579239.
Test: 2018-08-05T23:51:49.387028: step 9790, loss 0.549271.
Train: 2018-08-05T23:51:49.556737: step 9791, loss 0.546677.
Train: 2018-08-05T23:51:49.736093: step 9792, loss 0.562655.
Train: 2018-08-05T23:51:49.892307: step 9793, loss 0.611696.
Train: 2018-08-05T23:51:50.070683: step 9794, loss 0.611783.
Train: 2018-08-05T23:51:50.226898: step 9795, loss 0.538786.
Train: 2018-08-05T23:51:50.405196: step 9796, loss 0.538855.
Train: 2018-08-05T23:51:50.569681: step 9797, loss 0.522172.
Train: 2018-08-05T23:51:50.738871: step 9798, loss 0.522425.
Train: 2018-08-05T23:51:50.895115: step 9799, loss 0.513854.
Train: 2018-08-05T23:51:51.070552: step 9800, loss 0.579287.
Test: 2018-08-05T23:51:51.312453: step 9800, loss 0.548979.
Train: 2018-08-05T23:51:52.022798: step 9801, loss 0.562789.
Train: 2018-08-05T23:51:52.200157: step 9802, loss 0.579605.
Train: 2018-08-05T23:51:52.358825: step 9803, loss 0.562275.
Train: 2018-08-05T23:51:52.528979: step 9804, loss 0.570628.
Train: 2018-08-05T23:51:52.691738: step 9805, loss 0.562656.
Train: 2018-08-05T23:51:52.855359: step 9806, loss 0.604742.
Train: 2018-08-05T23:51:53.037989: step 9807, loss 0.629736.
Train: 2018-08-05T23:51:53.219487: step 9808, loss 0.470169.
Train: 2018-08-05T23:51:53.390001: step 9809, loss 0.638547.
Train: 2018-08-05T23:51:53.548404: step 9810, loss 0.554529.
Test: 2018-08-05T23:51:53.773776: step 9810, loss 0.548607.
Train: 2018-08-05T23:51:53.952463: step 9811, loss 0.579726.
Train: 2018-08-05T23:51:54.132929: step 9812, loss 0.537207.
Train: 2018-08-05T23:51:54.327401: step 9813, loss 0.579291.
Train: 2018-08-05T23:51:54.515713: step 9814, loss 0.537366.
Train: 2018-08-05T23:51:54.687548: step 9815, loss 0.527551.
Train: 2018-08-05T23:51:54.857413: step 9816, loss 0.57083.
Train: 2018-08-05T23:51:55.030823: step 9817, loss 0.537222.
Train: 2018-08-05T23:51:55.198229: step 9818, loss 0.545708.
Train: 2018-08-05T23:51:55.358279: step 9819, loss 0.536417.
Train: 2018-08-05T23:51:55.537351: step 9820, loss 0.588025.
Test: 2018-08-05T23:51:55.778445: step 9820, loss 0.548333.
Train: 2018-08-05T23:51:55.972742: step 9821, loss 0.613202.
Train: 2018-08-05T23:51:56.155782: step 9822, loss 0.571041.
Train: 2018-08-05T23:51:56.325354: step 9823, loss 0.57151.
Train: 2018-08-05T23:51:56.500892: step 9824, loss 0.571036.
Train: 2018-08-05T23:51:56.652828: step 9825, loss 0.622744.
Train: 2018-08-05T23:51:56.831360: step 9826, loss 0.519989.
Train: 2018-08-05T23:51:57.003195: step 9827, loss 0.554092.
Train: 2018-08-05T23:51:57.169232: step 9828, loss 0.604683.
Train: 2018-08-05T23:51:57.341068: step 9829, loss 0.486235.
Train: 2018-08-05T23:51:57.510986: step 9830, loss 0.485589.
Test: 2018-08-05T23:51:57.742920: step 9830, loss 0.548276.
Train: 2018-08-05T23:51:57.949693: step 9831, loss 0.656872.
Train: 2018-08-05T23:51:58.144441: step 9832, loss 0.544971.
Train: 2018-08-05T23:51:58.339225: step 9833, loss 0.528603.
Train: 2018-08-05T23:51:58.543491: step 9834, loss 0.58064.
Train: 2018-08-05T23:51:58.705888: step 9835, loss 0.528634.
Train: 2018-08-05T23:51:58.860342: step 9836, loss 0.579964.
Train: 2018-08-05T23:51:59.023881: step 9837, loss 0.588154.
Train: 2018-08-05T23:51:59.187466: step 9838, loss 0.588849.
Train: 2018-08-05T23:51:59.359296: step 9839, loss 0.502379.
Train: 2018-08-05T23:51:59.522186: step 9840, loss 0.571573.
Test: 2018-08-05T23:51:59.763813: step 9840, loss 0.54817.
Train: 2018-08-05T23:51:59.935765: step 9841, loss 0.596746.
Train: 2018-08-05T23:52:00.099316: step 9842, loss 0.537004.
Train: 2018-08-05T23:52:00.262919: step 9843, loss 0.54525.
Train: 2018-08-05T23:52:00.425839: step 9844, loss 0.580165.
Train: 2018-08-05T23:52:00.597707: step 9845, loss 0.597174.
Train: 2018-08-05T23:52:00.764870: step 9846, loss 0.571375.
Train: 2018-08-05T23:52:00.927795: step 9847, loss 0.502428.
Train: 2018-08-05T23:52:01.090658: step 9848, loss 0.580055.
Train: 2018-08-05T23:52:01.263015: step 9849, loss 0.597041.
Train: 2018-08-05T23:52:01.432523: step 9850, loss 0.545495.
Test: 2018-08-05T23:52:01.670567: step 9850, loss 0.548166.
Train: 2018-08-05T23:52:01.830571: step 9851, loss 0.57972.
Train: 2018-08-05T23:52:02.004098: step 9852, loss 0.5886.
Train: 2018-08-05T23:52:02.172106: step 9853, loss 0.545716.
Train: 2018-08-05T23:52:02.341971: step 9854, loss 0.553636.
Train: 2018-08-05T23:52:02.511554: step 9855, loss 0.596595.
Train: 2018-08-05T23:52:02.667798: step 9856, loss 0.554072.
Train: 2018-08-05T23:52:02.839634: step 9857, loss 0.57091.
Train: 2018-08-05T23:52:03.009596: step 9858, loss 0.520111.
Train: 2018-08-05T23:52:03.175343: step 9859, loss 0.587867.
Train: 2018-08-05T23:52:03.343892: step 9860, loss 0.570878.
Test: 2018-08-05T23:52:03.575466: step 9860, loss 0.548245.
Train: 2018-08-05T23:52:03.754258: step 9861, loss 0.571217.
Train: 2018-08-05T23:52:03.918857: step 9862, loss 0.605478.
Train: 2018-08-05T23:52:04.081653: step 9863, loss 0.51147.
Train: 2018-08-05T23:52:04.247179: step 9864, loss 0.579523.
Train: 2018-08-05T23:52:04.407694: step 9865, loss 0.588676.
Train: 2018-08-05T23:52:04.586577: step 9866, loss 0.588139.
Train: 2018-08-05T23:52:04.752474: step 9867, loss 0.503138.
Train: 2018-08-05T23:52:04.927551: step 9868, loss 0.545472.
Train: 2018-08-05T23:52:05.087270: step 9869, loss 0.579527.
Train: 2018-08-05T23:52:05.255474: step 9870, loss 0.579785.
Test: 2018-08-05T23:52:05.483223: step 9870, loss 0.548319.
Train: 2018-08-05T23:52:05.662451: step 9871, loss 0.656117.
Train: 2018-08-05T23:52:05.825666: step 9872, loss 0.60446.
Train: 2018-08-05T23:52:05.981849: step 9873, loss 0.605232.
Train: 2018-08-05T23:52:06.169372: step 9874, loss 0.563311.
Train: 2018-08-05T23:52:06.338918: step 9875, loss 0.503824.
Train: 2018-08-05T23:52:06.509462: step 9876, loss 0.545831.
Train: 2018-08-05T23:52:06.672802: step 9877, loss 0.579375.
Train: 2018-08-05T23:52:06.829014: step 9878, loss 0.537473.
Train: 2018-08-05T23:52:06.998902: step 9879, loss 0.57114.
Train: 2018-08-05T23:52:07.168363: step 9880, loss 0.579454.
Test: 2018-08-05T23:52:07.406380: step 9880, loss 0.548618.
Train: 2018-08-05T23:52:07.579449: step 9881, loss 0.571285.
Train: 2018-08-05T23:52:07.742066: step 9882, loss 0.62928.
Train: 2018-08-05T23:52:07.905826: step 9883, loss 0.528924.
Train: 2018-08-05T23:52:08.085022: step 9884, loss 0.595784.
Train: 2018-08-05T23:52:08.249535: step 9885, loss 0.537497.
Train: 2018-08-05T23:52:08.415292: step 9886, loss 0.49612.
Train: 2018-08-05T23:52:08.575699: step 9887, loss 0.546229.
Train: 2018-08-05T23:52:08.739524: step 9888, loss 0.512661.
Train: 2018-08-05T23:52:08.903090: step 9889, loss 0.570754.
Train: 2018-08-05T23:52:09.082192: step 9890, loss 0.503579.
Test: 2018-08-05T23:52:09.309364: step 9890, loss 0.548456.
Train: 2018-08-05T23:52:09.479299: step 9891, loss 0.503498.
Train: 2018-08-05T23:52:09.651108: step 9892, loss 0.562273.
Train: 2018-08-05T23:52:09.814824: step 9893, loss 0.605173.
Train: 2018-08-05T23:52:09.978577: step 9894, loss 0.528664.
Train: 2018-08-05T23:52:10.189341: step 9895, loss 0.459321.
Train: 2018-08-05T23:52:10.352823: step 9896, loss 0.536862.
Train: 2018-08-05T23:52:10.523772: step 9897, loss 0.519128.
Train: 2018-08-05T23:52:10.695605: step 9898, loss 0.588631.
Train: 2018-08-05T23:52:10.859143: step 9899, loss 0.518692.
Train: 2018-08-05T23:52:11.022661: step 9900, loss 0.492208.
Test: 2018-08-05T23:52:11.265351: step 9900, loss 0.547821.
Train: 2018-08-05T23:52:11.990626: step 9901, loss 0.46542.
Train: 2018-08-05T23:52:12.162431: step 9902, loss 0.66127.
Train: 2018-08-05T23:52:12.326713: step 9903, loss 0.589894.
Train: 2018-08-05T23:52:12.497706: step 9904, loss 0.509149.
Train: 2018-08-05T23:52:12.661163: step 9905, loss 0.581403.
Train: 2018-08-05T23:52:12.835170: step 9906, loss 0.535659.
Train: 2018-08-05T23:52:13.006596: step 9907, loss 0.56321.
Train: 2018-08-05T23:52:13.176117: step 9908, loss 0.635614.
Train: 2018-08-05T23:52:13.347659: step 9909, loss 0.517694.
Train: 2018-08-05T23:52:13.515211: step 9910, loss 0.672047.
Test: 2018-08-05T23:52:13.743201: step 9910, loss 0.547716.
Train: 2018-08-05T23:52:13.923388: step 9911, loss 0.553566.
Train: 2018-08-05T23:52:14.086725: step 9912, loss 0.427185.
Train: 2018-08-05T23:52:14.254596: step 9913, loss 0.590329.
Train: 2018-08-05T23:52:14.426434: step 9914, loss 0.625683.
Train: 2018-08-05T23:52:14.589562: step 9915, loss 0.562302.
Train: 2018-08-05T23:52:14.752591: step 9916, loss 0.535374.
Train: 2018-08-05T23:52:14.922711: step 9917, loss 0.581098.
Train: 2018-08-05T23:52:15.086217: step 9918, loss 0.472522.
Train: 2018-08-05T23:52:15.264221: step 9919, loss 0.590007.
Train: 2018-08-05T23:52:15.422413: step 9920, loss 0.571478.
Test: 2018-08-05T23:52:15.658457: step 9920, loss 0.547737.
Train: 2018-08-05T23:52:15.830290: step 9921, loss 0.59002.
Train: 2018-08-05T23:52:15.984202: step 9922, loss 0.535741.
Train: 2018-08-05T23:52:16.164577: step 9923, loss 0.535609.
Train: 2018-08-05T23:52:16.330133: step 9924, loss 0.545172.
Train: 2018-08-05T23:52:16.495834: step 9925, loss 0.580816.
Train: 2018-08-05T23:52:16.674841: step 9926, loss 0.509216.
Train: 2018-08-05T23:52:16.835229: step 9927, loss 0.571882.
Train: 2018-08-05T23:52:16.991443: step 9928, loss 0.553811.
Train: 2018-08-05T23:52:17.154406: step 9929, loss 0.616154.
Train: 2018-08-05T23:52:17.335911: step 9930, loss 0.634212.
Test: 2018-08-05T23:52:17.562040: step 9930, loss 0.54779.
Train: 2018-08-05T23:52:17.742289: step 9931, loss 0.56231.
Train: 2018-08-05T23:52:17.914074: step 9932, loss 0.536147.
Train: 2018-08-05T23:52:18.077777: step 9933, loss 0.535523.
Train: 2018-08-05T23:52:18.241317: step 9934, loss 0.580382.
Train: 2018-08-05T23:52:18.405866: step 9935, loss 0.571392.
Train: 2018-08-05T23:52:18.575212: step 9936, loss 0.641589.
Train: 2018-08-05T23:52:18.726222: step 9937, loss 0.518742.
Train: 2018-08-05T23:52:18.896836: step 9938, loss 0.483954.
Train: 2018-08-05T23:52:19.059793: step 9939, loss 0.571142.
Train: 2018-08-05T23:52:19.223162: step 9940, loss 0.510105.
Test: 2018-08-05T23:52:19.464564: step 9940, loss 0.547946.
Train: 2018-08-05T23:52:19.645995: step 9941, loss 0.545301.
Train: 2018-08-05T23:52:19.810364: step 9942, loss 0.5537.
Train: 2018-08-05T23:52:19.980686: step 9943, loss 0.657792.
Train: 2018-08-05T23:52:20.149920: step 9944, loss 0.545387.
Train: 2018-08-05T23:52:20.312852: step 9945, loss 0.640896.
Train: 2018-08-05T23:52:20.491672: step 9946, loss 0.570667.
Train: 2018-08-05T23:52:20.654822: step 9947, loss 0.553884.
Train: 2018-08-05T23:52:20.826688: step 9948, loss 0.553961.
Train: 2018-08-05T23:52:21.003282: step 9949, loss 0.588827.
Train: 2018-08-05T23:52:21.159526: step 9950, loss 0.588586.
Test: 2018-08-05T23:52:21.411920: step 9950, loss 0.548144.
Train: 2018-08-05T23:52:21.638401: step 9951, loss 0.528465.
Train: 2018-08-05T23:52:21.799150: step 9952, loss 0.52796.
Train: 2018-08-05T23:52:21.970985: step 9953, loss 0.613758.
Train: 2018-08-05T23:52:22.134466: step 9954, loss 0.511189.
Train: 2018-08-05T23:52:22.297984: step 9955, loss 0.588409.
Train: 2018-08-05T23:52:22.477125: step 9956, loss 0.580008.
Train: 2018-08-05T23:52:22.633339: step 9957, loss 0.53711.
Train: 2018-08-05T23:52:22.796881: step 9958, loss 0.529103.
Train: 2018-08-05T23:52:22.977000: step 9959, loss 0.545622.
Train: 2018-08-05T23:52:23.145465: step 9960, loss 0.528581.
Test: 2018-08-05T23:52:23.386819: step 9960, loss 0.548226.
Train: 2018-08-05T23:52:23.543823: step 9961, loss 0.59632.
Train: 2018-08-05T23:52:23.718202: step 9962, loss 0.588274.
Train: 2018-08-05T23:52:23.878842: step 9963, loss 0.562407.
Train: 2018-08-05T23:52:24.044282: step 9964, loss 0.562565.
Train: 2018-08-05T23:52:24.207084: step 9965, loss 0.605255.
Train: 2018-08-05T23:52:24.369896: step 9966, loss 0.635517.
Train: 2018-08-05T23:52:24.540869: step 9967, loss 0.571475.
Train: 2018-08-05T23:52:24.701651: step 9968, loss 0.562224.
Train: 2018-08-05T23:52:24.866208: step 9969, loss 0.621487.
Train: 2018-08-05T23:52:25.030334: step 9970, loss 0.512198.
Test: 2018-08-05T23:52:25.267326: step 9970, loss 0.548442.
Train: 2018-08-05T23:52:25.439154: step 9971, loss 0.603985.
Train: 2018-08-05T23:52:25.593405: step 9972, loss 0.621569.
Train: 2018-08-05T23:52:25.762220: step 9973, loss 0.521003.
Train: 2018-08-05T23:52:25.918434: step 9974, loss 0.571327.
Train: 2018-08-05T23:52:26.094928: step 9975, loss 0.562674.
Train: 2018-08-05T23:52:26.265513: step 9976, loss 0.554299.
Train: 2018-08-05T23:52:26.429034: step 9977, loss 0.595904.
Train: 2018-08-05T23:52:26.585051: step 9978, loss 0.521366.
Train: 2018-08-05T23:52:26.756903: step 9979, loss 0.529497.
Train: 2018-08-05T23:52:26.951645: step 9980, loss 0.620672.
Test: 2018-08-05T23:52:27.193546: step 9980, loss 0.548721.
Train: 2018-08-05T23:52:27.357104: step 9981, loss 0.612283.
Train: 2018-08-05T23:52:27.527041: step 9982, loss 0.529487.
Train: 2018-08-05T23:52:27.690535: step 9983, loss 0.537518.
Train: 2018-08-05T23:52:27.855121: step 9984, loss 0.529806.
Train: 2018-08-05T23:52:28.026953: step 9985, loss 0.529298.
Train: 2018-08-05T23:52:28.190597: step 9986, loss 0.620897.
Train: 2018-08-05T23:52:28.353535: step 9987, loss 0.537672.
Train: 2018-08-05T23:52:28.516991: step 9988, loss 0.562881.
Train: 2018-08-05T23:52:28.685941: step 9989, loss 0.537587.
Train: 2018-08-05T23:52:28.842164: step 9990, loss 0.587804.
Test: 2018-08-05T23:52:29.091273: step 9990, loss 0.548542.
Train: 2018-08-05T23:52:29.263138: step 9991, loss 0.554253.
Train: 2018-08-05T23:52:29.426720: step 9992, loss 0.520405.
Train: 2018-08-05T23:52:29.590256: step 9993, loss 0.512388.
Train: 2018-08-05T23:52:29.760075: step 9994, loss 0.545759.
Train: 2018-08-05T23:52:29.923636: step 9995, loss 0.630093.
Train: 2018-08-05T23:52:30.095504: step 9996, loss 0.519971.
Train: 2018-08-05T23:52:30.259050: step 9997, loss 0.486363.
Train: 2018-08-05T23:52:30.424177: step 9998, loss 0.545504.
Train: 2018-08-05T23:52:30.587686: step 9999, loss 0.570653.
Train: 2018-08-05T23:52:30.756893: step 10000, loss 0.485008.
Test: 2018-08-05T23:52:30.999378: step 10000, loss 0.548028.
Train: 2018-08-05T23:52:31.725609: step 10001, loss 0.639963.
Train: 2018-08-05T23:52:31.888671: step 10002, loss 0.545051.
Train: 2018-08-05T23:52:32.073722: step 10003, loss 0.562591.
Train: 2018-08-05T23:52:32.252913: step 10004, loss 0.632202.
Train: 2018-08-05T23:52:32.423318: step 10005, loss 0.632259.
Train: 2018-08-05T23:52:32.587733: step 10006, loss 0.501658.
Train: 2018-08-05T23:52:32.746670: step 10007, loss 0.554106.
Train: 2018-08-05T23:52:32.918503: step 10008, loss 0.571064.
Train: 2018-08-05T23:52:33.097699: step 10009, loss 0.571313.
Train: 2018-08-05T23:52:33.271236: step 10010, loss 0.562774.
Test: 2018-08-05T23:52:33.500498: step 10010, loss 0.547946.
Train: 2018-08-05T23:52:33.659402: step 10011, loss 0.544972.
Train: 2018-08-05T23:52:33.829718: step 10012, loss 0.527903.
Train: 2018-08-05T23:52:34.004216: step 10013, loss 0.605846.
Train: 2018-08-05T23:52:34.160460: step 10014, loss 0.518868.
Train: 2018-08-05T23:52:34.324133: step 10015, loss 0.518812.
Train: 2018-08-05T23:52:34.487706: step 10016, loss 0.510413.
Train: 2018-08-05T23:52:34.664048: step 10017, loss 0.562631.
Train: 2018-08-05T23:52:34.824773: step 10018, loss 0.597712.
Train: 2018-08-05T23:52:34.979667: step 10019, loss 0.580033.
Train: 2018-08-05T23:52:35.158429: step 10020, loss 0.518174.
Test: 2018-08-05T23:52:35.384314: step 10020, loss 0.547852.
Train: 2018-08-05T23:52:35.563203: step 10021, loss 0.588665.
Train: 2018-08-05T23:52:35.729069: step 10022, loss 0.527399.
Train: 2018-08-05T23:52:35.906446: step 10023, loss 0.52741.
Train: 2018-08-05T23:52:36.063837: step 10024, loss 0.518676.
Train: 2018-08-05T23:52:36.231759: step 10025, loss 0.580031.
Train: 2018-08-05T23:52:36.402304: step 10026, loss 0.571338.
Train: 2018-08-05T23:52:36.567991: step 10027, loss 0.642055.
Train: 2018-08-05T23:52:36.730961: step 10028, loss 0.527326.
Train: 2018-08-05T23:52:36.887175: step 10029, loss 0.518369.
Train: 2018-08-05T23:52:37.053922: step 10030, loss 0.518312.
Test: 2018-08-05T23:52:37.294929: step 10030, loss 0.547782.
Train: 2018-08-05T23:52:37.473393: step 10031, loss 0.544847.
Train: 2018-08-05T23:52:37.636315: step 10032, loss 0.571414.
Train: 2018-08-05T23:52:37.815573: step 10033, loss 0.562916.
Train: 2018-08-05T23:52:37.983310: step 10034, loss 0.500408.
Train: 2018-08-05T23:52:38.135107: step 10035, loss 0.615626.
Train: 2018-08-05T23:52:38.308743: step 10036, loss 0.642136.
Train: 2018-08-05T23:52:38.464951: step 10037, loss 0.615308.
Train: 2018-08-05T23:52:38.634758: step 10038, loss 0.47388.
Train: 2018-08-05T23:52:38.789935: step 10039, loss 0.641853.
Train: 2018-08-05T23:52:38.969057: step 10040, loss 0.589106.
Test: 2018-08-05T23:52:39.198233: step 10040, loss 0.547825.
Train: 2018-08-05T23:52:39.370065: step 10041, loss 0.580288.
Train: 2018-08-05T23:52:39.533802: step 10042, loss 0.553945.
Train: 2018-08-05T23:52:39.697567: step 10043, loss 0.545039.
Train: 2018-08-05T23:52:39.876707: step 10044, loss 0.544901.
Train: 2018-08-05T23:52:40.041334: step 10045, loss 0.501682.
Train: 2018-08-05T23:52:40.213141: step 10046, loss 0.61483.
Train: 2018-08-05T23:52:40.377391: step 10047, loss 0.562614.
Train: 2018-08-05T23:52:40.547720: step 10048, loss 0.623664.
Train: 2018-08-05T23:52:40.711197: step 10049, loss 0.580016.
Train: 2018-08-05T23:52:40.874708: step 10050, loss 0.631465.
Test: 2018-08-05T23:52:41.116457: step 10050, loss 0.548088.
Train: 2018-08-05T23:52:41.283306: step 10051, loss 0.442235.
Train: 2018-08-05T23:52:41.455140: step 10052, loss 0.596664.
Train: 2018-08-05T23:52:41.610349: step 10053, loss 0.622884.
Train: 2018-08-05T23:52:41.788902: step 10054, loss 0.477127.
Train: 2018-08-05T23:52:41.951876: step 10055, loss 0.656629.
Train: 2018-08-05T23:52:42.115499: step 10056, loss 0.596494.
Train: 2018-08-05T23:52:42.271743: step 10057, loss 0.528288.
Train: 2018-08-05T23:52:42.434966: step 10058, loss 0.570994.
Train: 2018-08-05T23:52:42.609463: step 10059, loss 0.486594.
Train: 2018-08-05T23:52:42.765677: step 10060, loss 0.570837.
Test: 2018-08-05T23:52:43.008398: step 10060, loss 0.548348.
Train: 2018-08-05T23:52:43.184505: step 10061, loss 0.647339.
Train: 2018-08-05T23:52:43.355019: step 10062, loss 0.571089.
Train: 2018-08-05T23:52:43.513054: step 10063, loss 0.528814.
Train: 2018-08-05T23:52:43.676436: step 10064, loss 0.503687.
Train: 2018-08-05T23:52:43.842281: step 10065, loss 0.613179.
Train: 2018-08-05T23:52:44.014117: step 10066, loss 0.553833.
Train: 2018-08-05T23:52:44.168428: step 10067, loss 0.646557.
Train: 2018-08-05T23:52:44.352439: step 10068, loss 0.54569.
Train: 2018-08-05T23:52:44.509703: step 10069, loss 0.529156.
Train: 2018-08-05T23:52:44.681537: step 10070, loss 0.612445.
Test: 2018-08-05T23:52:44.929657: step 10070, loss 0.548501.
Train: 2018-08-05T23:52:45.093344: step 10071, loss 0.503756.
Train: 2018-08-05T23:52:45.256971: step 10072, loss 0.495359.
Train: 2018-08-05T23:52:45.420518: step 10073, loss 0.562998.
Train: 2018-08-05T23:52:45.599500: step 10074, loss 0.613007.
Train: 2018-08-05T23:52:45.757679: step 10075, loss 0.520406.
Train: 2018-08-05T23:52:45.927199: step 10076, loss 0.613257.
Train: 2018-08-05T23:52:46.106366: step 10077, loss 0.613348.
Train: 2018-08-05T23:52:46.279397: step 10078, loss 0.613114.
Train: 2018-08-05T23:52:46.445960: step 10079, loss 0.579954.
Train: 2018-08-05T23:52:46.610774: step 10080, loss 0.511919.
Test: 2018-08-05T23:52:46.835722: step 10080, loss 0.548426.
Train: 2018-08-05T23:52:47.020549: step 10081, loss 0.596213.
Train: 2018-08-05T23:52:47.180567: step 10082, loss 0.528919.
Train: 2018-08-05T23:52:47.341236: step 10083, loss 0.562747.
Train: 2018-08-05T23:52:47.504720: step 10084, loss 0.554097.
Train: 2018-08-05T23:52:47.670557: step 10085, loss 0.520294.
Train: 2018-08-05T23:52:47.846586: step 10086, loss 0.520113.
Train: 2018-08-05T23:52:48.005349: step 10087, loss 0.520111.
Train: 2018-08-05T23:52:48.172849: step 10088, loss 0.579872.
Train: 2018-08-05T23:52:48.337874: step 10089, loss 0.451882.
Train: 2018-08-05T23:52:48.501461: step 10090, loss 0.56242.
Test: 2018-08-05T23:52:48.742877: step 10090, loss 0.548095.
Train: 2018-08-05T23:52:48.918936: step 10091, loss 0.570783.
Train: 2018-08-05T23:52:49.073937: step 10092, loss 0.588379.
Train: 2018-08-05T23:52:49.251954: step 10093, loss 0.675721.
Train: 2018-08-05T23:52:49.415606: step 10094, loss 0.519534.
Train: 2018-08-05T23:52:49.568854: step 10095, loss 0.519166.
Train: 2018-08-05T23:52:49.744503: step 10096, loss 0.527959.
Train: 2018-08-05T23:52:49.908024: step 10097, loss 0.518745.
Train: 2018-08-05T23:52:50.079888: step 10098, loss 0.571436.
Train: 2018-08-05T23:52:50.243724: step 10099, loss 0.60661.
Train: 2018-08-05T23:52:50.407287: step 10100, loss 0.553746.
Test: 2018-08-05T23:52:50.648954: step 10100, loss 0.547898.
Train: 2018-08-05T23:52:51.356832: step 10101, loss 0.518784.
Train: 2018-08-05T23:52:51.528696: step 10102, loss 0.51877.
Train: 2018-08-05T23:52:51.692288: step 10103, loss 0.55369.
Train: 2018-08-05T23:52:51.871452: step 10104, loss 0.527397.
Train: 2018-08-05T23:52:52.034981: step 10105, loss 0.633166.
Train: 2018-08-05T23:52:52.198505: step 10106, loss 0.527292.
Train: 2018-08-05T23:52:52.361998: step 10107, loss 0.562498.
Train: 2018-08-05T23:52:52.541297: step 10108, loss 0.571347.
Train: 2018-08-05T23:52:52.704818: step 10109, loss 0.598008.
Train: 2018-08-05T23:52:52.868325: step 10110, loss 0.562999.
Test: 2018-08-05T23:52:53.124119: step 10110, loss 0.547784.
Train: 2018-08-05T23:52:53.291692: step 10111, loss 0.544995.
Train: 2018-08-05T23:52:53.456231: step 10112, loss 0.580672.
Train: 2018-08-05T23:52:53.621851: step 10113, loss 0.545153.
Train: 2018-08-05T23:52:53.783411: step 10114, loss 0.606512.
Train: 2018-08-05T23:52:53.946769: step 10115, loss 0.623869.
Train: 2018-08-05T23:52:54.110263: step 10116, loss 0.527573.
Train: 2018-08-05T23:52:54.274758: step 10117, loss 0.674007.
Train: 2018-08-05T23:52:54.438199: step 10118, loss 0.649415.
Train: 2018-08-05T23:52:54.600845: step 10119, loss 0.571221.
Train: 2018-08-05T23:52:54.764380: step 10120, loss 0.631324.
Test: 2018-08-05T23:52:55.006358: step 10120, loss 0.548171.
Train: 2018-08-05T23:52:55.169886: step 10121, loss 0.528389.
Train: 2018-08-05T23:52:55.348954: step 10122, loss 0.477445.
Train: 2018-08-05T23:52:55.496807: step 10123, loss 0.57934.
Train: 2018-08-05T23:52:55.660323: step 10124, loss 0.537412.
Train: 2018-08-05T23:52:55.835474: step 10125, loss 0.562583.
Train: 2018-08-05T23:52:55.999125: step 10126, loss 0.579316.
Train: 2018-08-05T23:52:56.173941: step 10127, loss 0.612734.
Train: 2018-08-05T23:52:56.343488: step 10128, loss 0.537224.
Train: 2018-08-05T23:52:56.510066: step 10129, loss 0.562395.
Train: 2018-08-05T23:52:56.677693: step 10130, loss 0.604778.
Test: 2018-08-05T23:52:56.901263: step 10130, loss 0.548592.
Train: 2018-08-05T23:52:57.080272: step 10131, loss 0.537691.
Train: 2018-08-05T23:52:57.246012: step 10132, loss 0.55436.
Train: 2018-08-05T23:52:57.409819: step 10133, loss 0.570631.
Train: 2018-08-05T23:52:57.566063: step 10134, loss 0.529201.
Train: 2018-08-05T23:52:57.736288: step 10135, loss 0.595904.
Train: 2018-08-05T23:52:57.899307: step 10136, loss 0.537671.
Train: 2018-08-05T23:52:58.062984: step 10137, loss 0.529138.
Train: 2018-08-05T23:52:58.226095: step 10138, loss 0.579307.
Train: 2018-08-05T23:52:58.397931: step 10139, loss 0.554224.
Train: 2018-08-05T23:52:58.572788: step 10140, loss 0.528946.
Test: 2018-08-05T23:52:58.805149: step 10140, loss 0.548492.
Train: 2018-08-05T23:52:58.981131: step 10141, loss 0.554045.
Train: 2018-08-05T23:52:59.147789: step 10142, loss 0.512223.
Train: 2018-08-05T23:52:59.307197: step 10143, loss 0.587848.
Train: 2018-08-05T23:52:59.479034: step 10144, loss 0.646852.
Train: 2018-08-05T23:52:59.643634: step 10145, loss 0.503212.
Train: 2018-08-05T23:52:59.815439: step 10146, loss 0.587978.
Train: 2018-08-05T23:52:59.978962: step 10147, loss 0.579485.
Train: 2018-08-05T23:53:00.142563: step 10148, loss 0.537113.
Train: 2018-08-05T23:53:00.321672: step 10149, loss 0.553913.
Train: 2018-08-05T23:53:00.488442: step 10150, loss 0.588023.
Test: 2018-08-05T23:53:00.712968: step 10150, loss 0.548214.
Train: 2018-08-05T23:53:00.883397: step 10151, loss 0.596246.
Train: 2018-08-05T23:53:01.057929: step 10152, loss 0.690231.
Train: 2018-08-05T23:53:01.218517: step 10153, loss 0.545296.
Train: 2018-08-05T23:53:01.381707: step 10154, loss 0.596417.
Train: 2018-08-05T23:53:01.547481: step 10155, loss 0.571091.
Train: 2018-08-05T23:53:01.725952: step 10156, loss 0.562544.
Train: 2018-08-05T23:53:01.897788: step 10157, loss 0.595983.
Train: 2018-08-05T23:53:02.068251: step 10158, loss 0.554381.
Train: 2018-08-05T23:53:02.231215: step 10159, loss 0.612756.
Train: 2018-08-05T23:53:02.394951: step 10160, loss 0.504261.
Test: 2018-08-05T23:53:02.636179: step 10160, loss 0.548637.
Train: 2018-08-05T23:53:02.810740: step 10161, loss 0.570615.
Train: 2018-08-05T23:53:02.982580: step 10162, loss 0.570819.
Train: 2018-08-05T23:53:03.154386: step 10163, loss 0.587469.
Train: 2018-08-05T23:53:03.321944: step 10164, loss 0.562249.
Train: 2018-08-05T23:53:03.490493: step 10165, loss 0.546103.
Train: 2018-08-05T23:53:03.640325: step 10166, loss 0.554028.
Train: 2018-08-05T23:53:03.820094: step 10167, loss 0.587259.
Train: 2018-08-05T23:53:03.986691: step 10168, loss 0.488205.
Train: 2018-08-05T23:53:04.146783: step 10169, loss 0.595437.
Train: 2018-08-05T23:53:04.318617: step 10170, loss 0.512931.
Test: 2018-08-05T23:53:04.567477: step 10170, loss 0.54866.
Train: 2018-08-05T23:53:04.724916: step 10171, loss 0.537528.
Train: 2018-08-05T23:53:04.888581: step 10172, loss 0.620755.
Train: 2018-08-05T23:53:05.067662: step 10173, loss 0.587784.
Train: 2018-08-05T23:53:05.236869: step 10174, loss 0.587684.
Train: 2018-08-05T23:53:05.400306: step 10175, loss 0.479035.
Train: 2018-08-05T23:53:05.572175: step 10176, loss 0.613113.
Train: 2018-08-05T23:53:05.734903: step 10177, loss 0.520769.
Train: 2018-08-05T23:53:05.898485: step 10178, loss 0.571093.
Train: 2018-08-05T23:53:06.063018: step 10179, loss 0.553966.
Train: 2018-08-05T23:53:06.238646: step 10180, loss 0.528495.
Test: 2018-08-05T23:53:06.480968: step 10180, loss 0.54835.
Train: 2018-08-05T23:53:06.637993: step 10181, loss 0.486631.
Train: 2018-08-05T23:53:06.809859: step 10182, loss 0.596107.
Train: 2018-08-05T23:53:06.973394: step 10183, loss 0.502924.
Train: 2018-08-05T23:53:07.136982: step 10184, loss 0.48545.
Train: 2018-08-05T23:53:07.307803: step 10185, loss 0.588148.
Train: 2018-08-05T23:53:07.471220: step 10186, loss 0.588347.
Train: 2018-08-05T23:53:07.643055: step 10187, loss 0.545236.
Train: 2018-08-05T23:53:07.805873: step 10188, loss 0.632284.
Train: 2018-08-05T23:53:07.969418: step 10189, loss 0.605794.
Train: 2018-08-05T23:53:08.132933: step 10190, loss 0.658051.
Test: 2018-08-05T23:53:08.380451: step 10190, loss 0.547954.
Train: 2018-08-05T23:53:08.544019: step 10191, loss 0.571165.
Train: 2018-08-05T23:53:08.722107: step 10192, loss 0.571486.
Train: 2018-08-05T23:53:08.880589: step 10193, loss 0.596898.
Train: 2018-08-05T23:53:09.044924: step 10194, loss 0.545179.
Train: 2018-08-05T23:53:09.208361: step 10195, loss 0.622953.
Train: 2018-08-05T23:53:09.378619: step 10196, loss 0.631242.
Train: 2018-08-05T23:53:09.537440: step 10197, loss 0.520035.
Train: 2018-08-05T23:53:09.697932: step 10198, loss 0.570807.
Train: 2018-08-05T23:53:09.861158: step 10199, loss 0.520016.
Train: 2018-08-05T23:53:10.027016: step 10200, loss 0.494898.
Test: 2018-08-05T23:53:10.268161: step 10200, loss 0.548291.
Train: 2018-08-05T23:53:11.037938: step 10201, loss 0.596347.
Train: 2018-08-05T23:53:11.194152: step 10202, loss 0.5882.
Train: 2018-08-05T23:53:11.369736: step 10203, loss 0.562476.
Train: 2018-08-05T23:53:11.525950: step 10204, loss 0.570683.
Train: 2018-08-05T23:53:11.695823: step 10205, loss 0.604966.
Train: 2018-08-05T23:53:11.851007: step 10206, loss 0.562694.
Train: 2018-08-05T23:53:12.024789: step 10207, loss 0.503423.
Train: 2018-08-05T23:53:12.196624: step 10208, loss 0.545486.
Train: 2018-08-05T23:53:12.352837: step 10209, loss 0.495117.
Train: 2018-08-05T23:53:12.532291: step 10210, loss 0.528467.
Test: 2018-08-05T23:53:12.758438: step 10210, loss 0.548283.
Train: 2018-08-05T23:53:12.938629: step 10211, loss 0.528414.
Train: 2018-08-05T23:53:13.107706: step 10212, loss 0.502996.
Train: 2018-08-05T23:53:13.273268: step 10213, loss 0.571022.
Train: 2018-08-05T23:53:13.445801: step 10214, loss 0.536629.
Train: 2018-08-05T23:53:13.607506: step 10215, loss 0.484927.
Train: 2018-08-05T23:53:13.780443: step 10216, loss 0.510422.
Train: 2018-08-05T23:53:13.934922: step 10217, loss 0.631954.
Train: 2018-08-05T23:53:14.115373: step 10218, loss 0.641144.
Train: 2018-08-05T23:53:14.282755: step 10219, loss 0.527827.
Train: 2018-08-05T23:53:14.435851: step 10220, loss 0.527557.
Test: 2018-08-05T23:53:14.676849: step 10220, loss 0.5478.
Train: 2018-08-05T23:53:14.855577: step 10221, loss 0.632654.
Train: 2018-08-05T23:53:15.019078: step 10222, loss 0.553619.
Train: 2018-08-05T23:53:15.181967: step 10223, loss 0.527593.
Train: 2018-08-05T23:53:15.345143: step 10224, loss 0.527323.
Train: 2018-08-05T23:53:15.517008: step 10225, loss 0.562616.
Train: 2018-08-05T23:53:15.676952: step 10226, loss 0.55373.
Train: 2018-08-05T23:53:15.848786: step 10227, loss 0.571406.
Train: 2018-08-05T23:53:16.004205: step 10228, loss 0.63328.
Train: 2018-08-05T23:53:16.181902: step 10229, loss 0.597612.
Train: 2018-08-05T23:53:16.349454: step 10230, loss 0.483031.
Test: 2018-08-05T23:53:16.574033: step 10230, loss 0.547757.
Train: 2018-08-05T23:53:16.742935: step 10231, loss 0.536237.
Train: 2018-08-05T23:53:16.914770: step 10232, loss 0.55387.
Train: 2018-08-05T23:53:17.070983: step 10233, loss 0.518452.
Train: 2018-08-05T23:53:17.234241: step 10234, loss 0.606649.
Train: 2018-08-05T23:53:17.397680: step 10235, loss 0.553482.
Train: 2018-08-05T23:53:17.561197: step 10236, loss 0.562905.
Train: 2018-08-05T23:53:17.724634: step 10237, loss 0.553951.
Train: 2018-08-05T23:53:17.887638: step 10238, loss 0.553601.
Train: 2018-08-05T23:53:18.051099: step 10239, loss 0.491612.
Train: 2018-08-05T23:53:18.215559: step 10240, loss 0.518246.
Test: 2018-08-05T23:53:18.457968: step 10240, loss 0.547721.
Train: 2018-08-05T23:53:18.621590: step 10241, loss 0.668871.
Train: 2018-08-05T23:53:18.802697: step 10242, loss 0.51834.
Train: 2018-08-05T23:53:18.968010: step 10243, loss 0.482877.
Train: 2018-08-05T23:53:19.127741: step 10244, loss 0.571407.
Train: 2018-08-05T23:53:19.290941: step 10245, loss 0.535877.
Train: 2018-08-05T23:53:19.457673: step 10246, loss 0.589558.
Train: 2018-08-05T23:53:19.617610: step 10247, loss 0.509359.
Train: 2018-08-05T23:53:19.786643: step 10248, loss 0.562632.
Train: 2018-08-05T23:53:19.958476: step 10249, loss 0.562738.
Train: 2018-08-05T23:53:20.129136: step 10250, loss 0.544764.
Test: 2018-08-05T23:53:20.361190: step 10250, loss 0.547665.
Train: 2018-08-05T23:53:20.532994: step 10251, loss 0.580539.
Train: 2018-08-05T23:53:20.712158: step 10252, loss 0.526773.
Train: 2018-08-05T23:53:20.865481: step 10253, loss 0.554145.
Train: 2018-08-05T23:53:21.045565: step 10254, loss 0.517727.
Train: 2018-08-05T23:53:21.217426: step 10255, loss 0.54483.
Train: 2018-08-05T23:53:21.380296: step 10256, loss 0.625526.
Train: 2018-08-05T23:53:21.543943: step 10257, loss 0.607962.
Train: 2018-08-05T23:53:21.707362: step 10258, loss 0.580713.
Train: 2018-08-05T23:53:21.877601: step 10259, loss 0.562763.
Train: 2018-08-05T23:53:22.033814: step 10260, loss 0.509276.
Test: 2018-08-05T23:53:22.276571: step 10260, loss 0.547678.
Train: 2018-08-05T23:53:22.456341: step 10261, loss 0.562843.
Train: 2018-08-05T23:53:22.651453: step 10262, loss 0.598108.
Train: 2018-08-05T23:53:22.816173: step 10263, loss 0.571471.
Train: 2018-08-05T23:53:22.970254: step 10264, loss 0.597924.
Train: 2018-08-05T23:53:23.147635: step 10265, loss 0.571657.
Train: 2018-08-05T23:53:23.318148: step 10266, loss 0.527318.
Train: 2018-08-05T23:53:23.482709: step 10267, loss 0.624298.
Train: 2018-08-05T23:53:23.630489: step 10268, loss 0.48753.
Train: 2018-08-05T23:53:23.793273: step 10269, loss 0.562653.
Train: 2018-08-05T23:53:23.963293: step 10270, loss 0.509815.
Test: 2018-08-05T23:53:24.206003: step 10270, loss 0.54783.
Train: 2018-08-05T23:53:24.368904: step 10271, loss 0.579687.
Train: 2018-08-05T23:53:24.525148: step 10272, loss 0.536213.
Train: 2018-08-05T23:53:24.683633: step 10273, loss 0.59738.
Train: 2018-08-05T23:53:24.868261: step 10274, loss 0.536325.
Train: 2018-08-05T23:53:25.027359: step 10275, loss 0.588694.
Train: 2018-08-05T23:53:25.190833: step 10276, loss 0.579993.
Train: 2018-08-05T23:53:25.362668: step 10277, loss 0.579841.
Train: 2018-08-05T23:53:25.525872: step 10278, loss 0.536595.
Train: 2018-08-05T23:53:25.693131: step 10279, loss 0.536629.
Train: 2018-08-05T23:53:25.857956: step 10280, loss 0.597098.
Test: 2018-08-05T23:53:26.098851: step 10280, loss 0.547928.
Train: 2018-08-05T23:53:26.283254: step 10281, loss 0.527859.
Train: 2018-08-05T23:53:26.447812: step 10282, loss 0.649063.
Train: 2018-08-05T23:53:26.602160: step 10283, loss 0.640391.
Train: 2018-08-05T23:53:26.767057: step 10284, loss 0.536726.
Train: 2018-08-05T23:53:26.929924: step 10285, loss 0.493784.
Train: 2018-08-05T23:53:27.092850: step 10286, loss 0.528125.
Train: 2018-08-05T23:53:27.262633: step 10287, loss 0.536706.
Train: 2018-08-05T23:53:27.430462: step 10288, loss 0.485337.
Train: 2018-08-05T23:53:27.597546: step 10289, loss 0.545217.
Train: 2018-08-05T23:53:27.760692: step 10290, loss 0.519326.
Test: 2018-08-05T23:53:28.005058: step 10290, loss 0.547967.
Train: 2018-08-05T23:53:28.183540: step 10291, loss 0.579884.
Train: 2018-08-05T23:53:28.347470: step 10292, loss 0.579693.
Train: 2018-08-05T23:53:28.514884: step 10293, loss 0.579655.
Train: 2018-08-05T23:53:28.685512: step 10294, loss 0.562452.
Train: 2018-08-05T23:53:28.846861: step 10295, loss 0.536493.
Train: 2018-08-05T23:53:29.010189: step 10296, loss 0.579891.
Train: 2018-08-05T23:53:29.183771: step 10297, loss 0.606084.
Train: 2018-08-05T23:53:29.355607: step 10298, loss 0.562463.
Train: 2018-08-05T23:53:29.525423: step 10299, loss 0.562482.
Train: 2018-08-05T23:53:29.681637: step 10300, loss 0.510285.
Test: 2018-08-05T23:53:29.936901: step 10300, loss 0.54789.
Train: 2018-08-05T23:53:30.659618: step 10301, loss 0.56252.
Train: 2018-08-05T23:53:30.838737: step 10302, loss 0.579925.
Train: 2018-08-05T23:53:31.002167: step 10303, loss 0.605953.
Train: 2018-08-05T23:53:31.174003: step 10304, loss 0.527601.
Train: 2018-08-05T23:53:31.336772: step 10305, loss 0.52765.
Train: 2018-08-05T23:53:31.522805: step 10306, loss 0.588578.
Train: 2018-08-05T23:53:31.686302: step 10307, loss 0.536247.
Train: 2018-08-05T23:53:31.849769: step 10308, loss 0.605996.
Train: 2018-08-05T23:53:32.024833: step 10309, loss 0.527641.
Train: 2018-08-05T23:53:32.185336: step 10310, loss 0.484169.
Test: 2018-08-05T23:53:32.426928: step 10310, loss 0.54788.
Train: 2018-08-05T23:53:32.598053: step 10311, loss 0.519077.
Train: 2018-08-05T23:53:32.760711: step 10312, loss 0.56245.
Train: 2018-08-05T23:53:32.940750: step 10313, loss 0.527556.
Train: 2018-08-05T23:53:33.112892: step 10314, loss 0.544949.
Train: 2018-08-05T23:53:33.279478: step 10315, loss 0.588847.
Train: 2018-08-05T23:53:33.443035: step 10316, loss 0.536207.
Train: 2018-08-05T23:53:33.609449: step 10317, loss 0.53592.
Train: 2018-08-05T23:53:33.772388: step 10318, loss 0.588928.
Train: 2018-08-05T23:53:33.936044: step 10319, loss 0.50948.
Train: 2018-08-05T23:53:34.115490: step 10320, loss 0.562576.
Test: 2018-08-05T23:53:34.358916: step 10320, loss 0.547705.
Train: 2018-08-05T23:53:34.521716: step 10321, loss 0.517984.
Train: 2018-08-05T23:53:34.693551: step 10322, loss 0.527031.
Train: 2018-08-05T23:53:34.856599: step 10323, loss 0.544807.
Train: 2018-08-05T23:53:35.019693: step 10324, loss 0.464315.
Train: 2018-08-05T23:53:35.183186: step 10325, loss 0.616488.
Train: 2018-08-05T23:53:35.361965: step 10326, loss 0.517779.
Train: 2018-08-05T23:53:35.518208: step 10327, loss 0.742937.
Train: 2018-08-05T23:53:35.678148: step 10328, loss 0.490633.
Train: 2018-08-05T23:53:35.850012: step 10329, loss 0.436512.
Train: 2018-08-05T23:53:36.019531: step 10330, loss 0.598786.
Test: 2018-08-05T23:53:36.262090: step 10330, loss 0.547619.
Train: 2018-08-05T23:53:36.428670: step 10331, loss 0.544712.
Train: 2018-08-05T23:53:36.589342: step 10332, loss 0.517521.
Train: 2018-08-05T23:53:36.759367: step 10333, loss 0.499379.
Train: 2018-08-05T23:53:36.931173: step 10334, loss 0.562864.
Train: 2018-08-05T23:53:37.087418: step 10335, loss 0.535571.
Train: 2018-08-05T23:53:37.251131: step 10336, loss 0.635778.
Train: 2018-08-05T23:53:37.416390: step 10337, loss 0.58122.
Train: 2018-08-05T23:53:37.596512: step 10338, loss 0.653995.
Train: 2018-08-05T23:53:37.759289: step 10339, loss 0.535644.
Train: 2018-08-05T23:53:37.925677: step 10340, loss 0.562875.
Test: 2018-08-05T23:53:38.176468: step 10340, loss 0.547614.
Train: 2018-08-05T23:53:38.326527: step 10341, loss 0.589964.
Train: 2018-08-05T23:53:38.509010: step 10342, loss 0.553727.
Train: 2018-08-05T23:53:38.671818: step 10343, loss 0.553537.
Train: 2018-08-05T23:53:38.834642: step 10344, loss 0.544802.
Train: 2018-08-05T23:53:39.015249: step 10345, loss 0.580621.
Train: 2018-08-05T23:53:39.175985: step 10346, loss 0.4554.
Train: 2018-08-05T23:53:39.340439: step 10347, loss 0.660931.
Train: 2018-08-05T23:53:39.503702: step 10348, loss 0.509045.
Train: 2018-08-05T23:53:39.659919: step 10349, loss 0.607106.
Train: 2018-08-05T23:53:39.833609: step 10350, loss 0.571444.
Test: 2018-08-05T23:53:40.066053: step 10350, loss 0.547704.
Train: 2018-08-05T23:53:40.236765: step 10351, loss 0.606904.
Train: 2018-08-05T23:53:40.408601: step 10352, loss 0.500483.
Train: 2018-08-05T23:53:40.566842: step 10353, loss 0.571358.
Train: 2018-08-05T23:53:40.723056: step 10354, loss 0.597726.
Train: 2018-08-05T23:53:40.903230: step 10355, loss 0.553586.
Train: 2018-08-05T23:53:41.066802: step 10356, loss 0.553574.
Train: 2018-08-05T23:53:41.223046: step 10357, loss 0.606287.
Train: 2018-08-05T23:53:41.402304: step 10358, loss 0.710839.
Train: 2018-08-05T23:53:41.565859: step 10359, loss 0.51911.
Train: 2018-08-05T23:53:41.729378: step 10360, loss 0.579724.
Test: 2018-08-05T23:53:41.955432: step 10360, loss 0.548012.
Train: 2018-08-05T23:53:42.125512: step 10361, loss 0.579732.
Train: 2018-08-05T23:53:42.297346: step 10362, loss 0.622469.
Train: 2018-08-05T23:53:42.460850: step 10363, loss 0.639156.
Train: 2018-08-05T23:53:42.624609: step 10364, loss 0.553956.
Train: 2018-08-05T23:53:42.789900: step 10365, loss 0.545574.
Train: 2018-08-05T23:53:42.960233: step 10366, loss 0.629639.
Train: 2018-08-05T23:53:43.141583: step 10367, loss 0.579304.
Train: 2018-08-05T23:53:43.311131: step 10368, loss 0.487851.
Train: 2018-08-05T23:53:43.476656: step 10369, loss 0.579059.
Train: 2018-08-05T23:53:43.629706: step 10370, loss 0.603845.
Test: 2018-08-05T23:53:43.869263: step 10370, loss 0.548828.
Train: 2018-08-05T23:53:44.056125: step 10371, loss 0.504996.
Train: 2018-08-05T23:53:44.217559: step 10372, loss 0.505279.
Train: 2018-08-05T23:53:44.381214: step 10373, loss 0.612109.
Train: 2018-08-05T23:53:44.558278: step 10374, loss 0.612041.
Train: 2018-08-05T23:53:44.716337: step 10375, loss 0.537916.
Train: 2018-08-05T23:53:44.888150: step 10376, loss 0.570769.
Train: 2018-08-05T23:53:45.050869: step 10377, loss 0.620098.
Train: 2018-08-05T23:53:45.215290: step 10378, loss 0.562774.
Train: 2018-08-05T23:53:45.384337: step 10379, loss 0.579162.
Train: 2018-08-05T23:53:45.540574: step 10380, loss 0.497312.
Test: 2018-08-05T23:53:45.786869: step 10380, loss 0.548995.
Train: 2018-08-05T23:53:45.943083: step 10381, loss 0.546422.
Train: 2018-08-05T23:53:46.124209: step 10382, loss 0.570886.
Train: 2018-08-05T23:53:46.292757: step 10383, loss 0.546354.
Train: 2018-08-05T23:53:46.457321: step 10384, loss 0.619954.
Train: 2018-08-05T23:53:46.618684: step 10385, loss 0.57909.
Train: 2018-08-05T23:53:46.774866: step 10386, loss 0.562724.
Train: 2018-08-05T23:53:46.938410: step 10387, loss 0.53814.
Train: 2018-08-05T23:53:47.121785: step 10388, loss 0.570743.
Train: 2018-08-05T23:53:47.304137: step 10389, loss 0.570886.
Train: 2018-08-05T23:53:47.468064: step 10390, loss 0.579149.
Test: 2018-08-05T23:53:47.709179: step 10390, loss 0.548759.
Train: 2018-08-05T23:53:47.875368: step 10391, loss 0.546015.
Train: 2018-08-05T23:53:48.058356: step 10392, loss 0.661657.
Train: 2018-08-05T23:53:48.210218: step 10393, loss 0.595518.
Train: 2018-08-05T23:53:48.395074: step 10394, loss 0.562534.
Train: 2018-08-05T23:53:48.551294: step 10395, loss 0.603874.
Train: 2018-08-05T23:53:48.720488: step 10396, loss 0.587303.
Train: 2018-08-05T23:53:48.891293: step 10397, loss 0.57098.
Train: 2018-08-05T23:53:49.060849: step 10398, loss 0.472604.
Train: 2018-08-05T23:53:49.235934: step 10399, loss 0.579061.
Train: 2018-08-05T23:53:49.403691: step 10400, loss 0.513635.
Test: 2018-08-05T23:53:49.633961: step 10400, loss 0.548889.
Train: 2018-08-05T23:53:50.416740: step 10401, loss 0.496917.
Train: 2018-08-05T23:53:50.580436: step 10402, loss 0.603764.
Train: 2018-08-05T23:53:50.750322: step 10403, loss 0.587376.
Train: 2018-08-05T23:53:50.919980: step 10404, loss 0.529582.
Train: 2018-08-05T23:53:51.091814: step 10405, loss 0.579202.
Train: 2018-08-05T23:53:51.247998: step 10406, loss 0.545852.
Train: 2018-08-05T23:53:51.411667: step 10407, loss 0.50416.
Train: 2018-08-05T23:53:51.590800: step 10408, loss 0.545621.
Train: 2018-08-05T23:53:51.755359: step 10409, loss 0.512173.
Train: 2018-08-05T23:53:51.911542: step 10410, loss 0.646828.
Test: 2018-08-05T23:53:52.146769: step 10410, loss 0.548301.
Train: 2018-08-05T23:53:52.325233: step 10411, loss 0.613073.
Train: 2018-08-05T23:53:52.507236: step 10412, loss 0.53711.
Train: 2018-08-05T23:53:52.666883: step 10413, loss 0.511677.
Train: 2018-08-05T23:53:52.845812: step 10414, loss 0.638852.
Train: 2018-08-05T23:53:53.011663: step 10415, loss 0.587973.
Train: 2018-08-05T23:53:53.201673: step 10416, loss 0.579314.
Train: 2018-08-05T23:53:53.388175: step 10417, loss 0.502955.
Train: 2018-08-05T23:53:53.604699: step 10418, loss 0.56254.
Train: 2018-08-05T23:53:53.805221: step 10419, loss 0.417119.
Train: 2018-08-05T23:53:53.983442: step 10420, loss 0.562492.
Test: 2018-08-05T23:53:54.221504: step 10420, loss 0.548051.
Train: 2018-08-05T23:53:54.391165: step 10421, loss 0.519521.
Train: 2018-08-05T23:53:54.562969: step 10422, loss 0.502031.
Train: 2018-08-05T23:53:54.734835: step 10423, loss 0.588371.
Train: 2018-08-05T23:53:54.913690: step 10424, loss 0.666814.
Train: 2018-08-05T23:53:55.079522: step 10425, loss 0.475354.
Train: 2018-08-05T23:53:55.242909: step 10426, loss 0.571207.
Train: 2018-08-05T23:53:55.407445: step 10427, loss 0.623789.
Train: 2018-08-05T23:53:55.579313: step 10428, loss 0.615115.
Train: 2018-08-05T23:53:55.742050: step 10429, loss 0.553654.
Train: 2018-08-05T23:53:55.906206: step 10430, loss 0.6149.
Test: 2018-08-05T23:53:56.156869: step 10430, loss 0.547827.
Train: 2018-08-05T23:53:56.327443: step 10431, loss 0.588704.
Train: 2018-08-05T23:53:56.495962: step 10432, loss 0.527658.
Train: 2018-08-05T23:53:56.646157: step 10433, loss 0.579801.
Train: 2018-08-05T23:53:56.814044: step 10434, loss 0.579778.
Train: 2018-08-05T23:53:56.997339: step 10435, loss 0.518979.
Train: 2018-08-05T23:53:57.176292: step 10436, loss 0.571124.
Train: 2018-08-05T23:53:57.357102: step 10437, loss 0.562545.
Train: 2018-08-05T23:53:57.520874: step 10438, loss 0.675114.
Train: 2018-08-05T23:53:57.695260: step 10439, loss 0.588388.
Train: 2018-08-05T23:53:57.884382: step 10440, loss 0.545304.
Test: 2018-08-05T23:53:58.119237: step 10440, loss 0.548053.
Train: 2018-08-05T23:53:58.308994: step 10441, loss 0.545307.
Train: 2018-08-05T23:53:58.498857: step 10442, loss 0.528206.
Train: 2018-08-05T23:53:58.686541: step 10443, loss 0.528363.
Train: 2018-08-05T23:53:58.857108: step 10444, loss 0.562467.
Train: 2018-08-05T23:53:59.030788: step 10445, loss 0.579573.
Train: 2018-08-05T23:53:59.218287: step 10446, loss 0.630749.
Train: 2018-08-05T23:53:59.381962: step 10447, loss 0.570954.
Train: 2018-08-05T23:53:59.559693: step 10448, loss 0.587917.
Train: 2018-08-05T23:53:59.733503: step 10449, loss 0.613463.
Train: 2018-08-05T23:53:59.900056: step 10450, loss 0.680956.
Test: 2018-08-05T23:54:00.136819: step 10450, loss 0.548363.
Train: 2018-08-05T23:54:00.312249: step 10451, loss 0.537209.
Train: 2018-08-05T23:54:00.485307: step 10452, loss 0.478613.
Train: 2018-08-05T23:54:00.647387: step 10453, loss 0.503879.
Train: 2018-08-05T23:54:00.818957: step 10454, loss 0.562419.
Train: 2018-08-05T23:54:00.966827: step 10455, loss 0.554148.
Train: 2018-08-05T23:54:01.145638: step 10456, loss 0.579231.
Train: 2018-08-05T23:54:01.323990: step 10457, loss 0.56248.
Train: 2018-08-05T23:54:01.487782: step 10458, loss 0.554161.
Train: 2018-08-05T23:54:01.650572: step 10459, loss 0.520579.
Train: 2018-08-05T23:54:01.813603: step 10460, loss 0.62955.
Test: 2018-08-05T23:54:02.054721: step 10460, loss 0.548435.
Train: 2018-08-05T23:54:02.233931: step 10461, loss 0.520461.
Train: 2018-08-05T23:54:02.399053: step 10462, loss 0.621322.
Train: 2018-08-05T23:54:02.555299: step 10463, loss 0.562504.
Train: 2018-08-05T23:54:02.729946: step 10464, loss 0.596044.
Train: 2018-08-05T23:54:02.901746: step 10465, loss 0.512092.
Train: 2018-08-05T23:54:03.071732: step 10466, loss 0.545713.
Train: 2018-08-05T23:54:03.240217: step 10467, loss 0.487004.
Train: 2018-08-05T23:54:03.408767: step 10468, loss 0.545692.
Train: 2018-08-05T23:54:03.582281: step 10469, loss 0.621574.
Train: 2018-08-05T23:54:03.743844: step 10470, loss 0.570847.
Test: 2018-08-05T23:54:03.967663: step 10470, loss 0.548271.
Train: 2018-08-05T23:54:04.139492: step 10471, loss 0.554.
Train: 2018-08-05T23:54:04.302440: step 10472, loss 0.579425.
Train: 2018-08-05T23:54:04.482841: step 10473, loss 0.579375.
Train: 2018-08-05T23:54:04.646283: step 10474, loss 0.604867.
Train: 2018-08-05T23:54:04.816283: step 10475, loss 0.477593.
Train: 2018-08-05T23:54:04.993308: step 10476, loss 0.60482.
Train: 2018-08-05T23:54:05.151629: step 10477, loss 0.545376.
Train: 2018-08-05T23:54:05.315291: step 10478, loss 0.562459.
Train: 2018-08-05T23:54:05.478750: step 10479, loss 0.536879.
Train: 2018-08-05T23:54:05.642210: step 10480, loss 0.588002.
Test: 2018-08-05T23:54:05.891156: step 10480, loss 0.54815.
Train: 2018-08-05T23:54:06.040344: step 10481, loss 0.528318.
Train: 2018-08-05T23:54:06.230368: step 10482, loss 0.519776.
Train: 2018-08-05T23:54:06.398885: step 10483, loss 0.545308.
Train: 2018-08-05T23:54:06.575875: step 10484, loss 0.562459.
Train: 2018-08-05T23:54:06.745184: step 10485, loss 0.562427.
Train: 2018-08-05T23:54:06.901563: step 10486, loss 0.528011.
Train: 2018-08-05T23:54:07.068360: step 10487, loss 0.536508.
Train: 2018-08-05T23:54:07.240219: step 10488, loss 0.553781.
Train: 2018-08-05T23:54:07.403842: step 10489, loss 0.571111.
Train: 2018-08-05T23:54:07.567405: step 10490, loss 0.484314.
Test: 2018-08-05T23:54:07.824961: step 10490, loss 0.547849.
Train: 2018-08-05T23:54:07.988549: step 10491, loss 0.484059.
Train: 2018-08-05T23:54:08.157498: step 10492, loss 0.536241.
Train: 2018-08-05T23:54:08.322163: step 10493, loss 0.580103.
Train: 2018-08-05T23:54:08.494030: step 10494, loss 0.606625.
Train: 2018-08-05T23:54:08.657602: step 10495, loss 0.624604.
Train: 2018-08-05T23:54:08.822858: step 10496, loss 0.562532.
Train: 2018-08-05T23:54:08.986418: step 10497, loss 0.589082.
Train: 2018-08-05T23:54:09.155686: step 10498, loss 0.615739.
Train: 2018-08-05T23:54:09.315522: step 10499, loss 0.544835.
Train: 2018-08-05T23:54:09.491015: step 10500, loss 0.518375.
Test: 2018-08-05T23:54:09.732286: step 10500, loss 0.54773.
Train: 2018-08-05T23:54:10.459817: step 10501, loss 0.58899.
Train: 2018-08-05T23:54:10.685818: step 10502, loss 0.500765.
Train: 2018-08-05T23:54:10.849039: step 10503, loss 0.624228.
Train: 2018-08-05T23:54:11.020842: step 10504, loss 0.492035.
Train: 2018-08-05T23:54:11.193216: step 10505, loss 0.588921.
Train: 2018-08-05T23:54:11.375443: step 10506, loss 0.580119.
Train: 2018-08-05T23:54:11.533194: step 10507, loss 0.56251.
Train: 2018-08-05T23:54:11.704060: step 10508, loss 0.624085.
Train: 2018-08-05T23:54:11.873400: step 10509, loss 0.606343.
Train: 2018-08-05T23:54:12.043942: step 10510, loss 0.474918.
Test: 2018-08-05T23:54:12.285321: step 10510, loss 0.547814.
Train: 2018-08-05T23:54:12.465366: step 10511, loss 0.562451.
Train: 2018-08-05T23:54:12.621579: step 10512, loss 0.588635.
Train: 2018-08-05T23:54:12.805343: step 10513, loss 0.59736.
Train: 2018-08-05T23:54:12.971899: step 10514, loss 0.571094.
Train: 2018-08-05T23:54:13.135537: step 10515, loss 0.501638.
Train: 2018-08-05T23:54:13.312040: step 10516, loss 0.553706.
Train: 2018-08-05T23:54:13.480588: step 10517, loss 0.605825.
Train: 2018-08-05T23:54:13.647256: step 10518, loss 0.545092.
Train: 2018-08-05T23:54:13.819123: step 10519, loss 0.588323.
Train: 2018-08-05T23:54:13.982098: step 10520, loss 0.545082.
Test: 2018-08-05T23:54:14.226382: step 10520, loss 0.547947.
Train: 2018-08-05T23:54:14.389852: step 10521, loss 0.545215.
Train: 2018-08-05T23:54:14.585607: step 10522, loss 0.519368.
Train: 2018-08-05T23:54:14.764698: step 10523, loss 0.476009.
Train: 2018-08-05T23:54:14.928160: step 10524, loss 0.614367.
Train: 2018-08-05T23:54:15.084411: step 10525, loss 0.597019.
Train: 2018-08-05T23:54:15.263239: step 10526, loss 0.57111.
Train: 2018-08-05T23:54:15.443384: step 10527, loss 0.605812.
Train: 2018-08-05T23:54:15.592343: step 10528, loss 0.484506.
Train: 2018-08-05T23:54:15.761728: step 10529, loss 0.553981.
Train: 2018-08-05T23:54:15.933564: step 10530, loss 0.553678.
Test: 2018-08-05T23:54:16.183551: step 10530, loss 0.547908.
Train: 2018-08-05T23:54:16.355091: step 10531, loss 0.579824.
Train: 2018-08-05T23:54:16.520648: step 10532, loss 0.57981.
Train: 2018-08-05T23:54:16.687472: step 10533, loss 0.475677.
Train: 2018-08-05T23:54:16.850928: step 10534, loss 0.518941.
Train: 2018-08-05T23:54:17.015475: step 10535, loss 0.527477.
Train: 2018-08-05T23:54:17.171688: step 10536, loss 0.562545.
Train: 2018-08-05T23:54:17.335230: step 10537, loss 0.544838.
Train: 2018-08-05T23:54:17.514849: step 10538, loss 0.588872.
Train: 2018-08-05T23:54:17.678337: step 10539, loss 0.527047.
Train: 2018-08-05T23:54:17.832760: step 10540, loss 0.598148.
Test: 2018-08-05T23:54:18.074418: step 10540, loss 0.547736.
Train: 2018-08-05T23:54:18.238966: step 10541, loss 0.544767.
Train: 2018-08-05T23:54:18.410826: step 10542, loss 0.606849.
Train: 2018-08-05T23:54:18.576363: step 10543, loss 0.527296.
Train: 2018-08-05T23:54:18.753161: step 10544, loss 0.606488.
Train: 2018-08-05T23:54:18.910192: step 10545, loss 0.597454.
Train: 2018-08-05T23:54:19.074775: step 10546, loss 0.562565.
Train: 2018-08-05T23:54:19.259834: step 10547, loss 0.527228.
Train: 2018-08-05T23:54:19.409900: step 10548, loss 0.544819.
Train: 2018-08-05T23:54:19.576021: step 10549, loss 0.492054.
Train: 2018-08-05T23:54:19.754541: step 10550, loss 0.535781.
Test: 2018-08-05T23:54:19.980040: step 10550, loss 0.547732.
Train: 2018-08-05T23:54:20.149547: step 10551, loss 0.553663.
Train: 2018-08-05T23:54:20.328747: step 10552, loss 0.57133.
Train: 2018-08-05T23:54:20.505753: step 10553, loss 0.474395.
Train: 2018-08-05T23:54:20.663790: step 10554, loss 0.536126.
Train: 2018-08-05T23:54:20.828246: step 10555, loss 0.589533.
Train: 2018-08-05T23:54:21.000109: step 10556, loss 0.5532.
Train: 2018-08-05T23:54:21.156292: step 10557, loss 0.562433.
Train: 2018-08-05T23:54:21.335392: step 10558, loss 0.536062.
Train: 2018-08-05T23:54:21.491610: step 10559, loss 0.544711.
Train: 2018-08-05T23:54:21.678011: step 10560, loss 0.625578.
Test: 2018-08-05T23:54:21.916128: step 10560, loss 0.547631.
Train: 2018-08-05T23:54:22.157684: step 10561, loss 0.625993.
Train: 2018-08-05T23:54:22.325460: step 10562, loss 0.562685.
Train: 2018-08-05T23:54:22.497325: step 10563, loss 0.598289.
Train: 2018-08-05T23:54:22.660785: step 10564, loss 0.633622.
Train: 2018-08-05T23:54:22.827264: step 10565, loss 0.597842.
Train: 2018-08-05T23:54:23.005248: step 10566, loss 0.597798.
Train: 2018-08-05T23:54:23.170830: step 10567, loss 0.509988.
Train: 2018-08-05T23:54:23.347362: step 10568, loss 0.553825.
Train: 2018-08-05T23:54:23.513887: step 10569, loss 0.501544.
Train: 2018-08-05T23:54:23.667869: step 10570, loss 0.580816.
Test: 2018-08-05T23:54:23.909058: step 10570, loss 0.547918.
Train: 2018-08-05T23:54:24.075161: step 10571, loss 0.579855.
Train: 2018-08-05T23:54:24.247502: step 10572, loss 0.622865.
Train: 2018-08-05T23:54:24.394260: step 10573, loss 0.579619.
Train: 2018-08-05T23:54:24.573188: step 10574, loss 0.493745.
Train: 2018-08-05T23:54:24.737937: step 10575, loss 0.553843.
Train: 2018-08-05T23:54:24.898764: step 10576, loss 0.485395.
Train: 2018-08-05T23:54:25.067094: step 10577, loss 0.562365.
Train: 2018-08-05T23:54:25.218556: step 10578, loss 0.579598.
Train: 2018-08-05T23:54:25.381068: step 10579, loss 0.468045.
Train: 2018-08-05T23:54:25.559530: step 10580, loss 0.536609.
Test: 2018-08-05T23:54:25.791565: step 10580, loss 0.547981.
Train: 2018-08-05T23:54:25.962146: step 10581, loss 0.553767.
Train: 2018-08-05T23:54:26.135525: step 10582, loss 0.510512.
Train: 2018-08-05T23:54:26.300086: step 10583, loss 0.49301.
Train: 2018-08-05T23:54:26.467636: step 10584, loss 0.527595.
Train: 2018-08-05T23:54:26.623350: step 10585, loss 0.545021.
Train: 2018-08-05T23:54:26.795184: step 10586, loss 0.588829.
Train: 2018-08-05T23:54:26.958694: step 10587, loss 0.571407.
Train: 2018-08-05T23:54:27.122187: step 10588, loss 0.571406.
Train: 2018-08-05T23:54:27.294027: step 10589, loss 0.580268.
Train: 2018-08-05T23:54:27.468027: step 10590, loss 0.553722.
Test: 2018-08-05T23:54:27.705604: step 10590, loss 0.547687.
Train: 2018-08-05T23:54:27.868439: step 10591, loss 0.509301.
Train: 2018-08-05T23:54:28.031280: step 10592, loss 0.518134.
Train: 2018-08-05T23:54:28.211452: step 10593, loss 0.58929.
Train: 2018-08-05T23:54:28.374933: step 10594, loss 0.553682.
Train: 2018-08-05T23:54:28.538475: step 10595, loss 0.634046.
Train: 2018-08-05T23:54:28.702027: step 10596, loss 0.526942.
Train: 2018-08-05T23:54:28.865732: step 10597, loss 0.526767.
Train: 2018-08-05T23:54:29.029169: step 10598, loss 0.642991.
Train: 2018-08-05T23:54:29.208275: step 10599, loss 0.669814.
Train: 2018-08-05T23:54:29.364489: step 10600, loss 0.553648.
Test: 2018-08-05T23:54:29.605440: step 10600, loss 0.547685.
Train: 2018-08-05T23:54:30.300654: step 10601, loss 0.500361.
Train: 2018-08-05T23:54:30.479203: step 10602, loss 0.509377.
Train: 2018-08-05T23:54:30.650555: step 10603, loss 0.580162.
Train: 2018-08-05T23:54:30.814205: step 10604, loss 0.633445.
Train: 2018-08-05T23:54:30.977316: step 10605, loss 0.483094.
Train: 2018-08-05T23:54:31.140895: step 10606, loss 0.580096.
Train: 2018-08-05T23:54:31.335284: step 10607, loss 0.606388.
Train: 2018-08-05T23:54:31.491465: step 10608, loss 0.571256.
Train: 2018-08-05T23:54:31.665458: step 10609, loss 0.553773.
Train: 2018-08-05T23:54:31.840441: step 10610, loss 0.55371.
Test: 2018-08-05T23:54:32.066690: step 10610, loss 0.547817.
Train: 2018-08-05T23:54:32.222905: step 10611, loss 0.518755.
Train: 2018-08-05T23:54:32.399239: step 10612, loss 0.518721.
Train: 2018-08-05T23:54:32.547082: step 10613, loss 0.623495.
Train: 2018-08-05T23:54:32.726271: step 10614, loss 0.614854.
Train: 2018-08-05T23:54:32.882453: step 10615, loss 0.562417.
Train: 2018-08-05T23:54:33.045260: step 10616, loss 0.562423.
Train: 2018-08-05T23:54:33.223687: step 10617, loss 0.614536.
Train: 2018-08-05T23:54:33.392236: step 10618, loss 0.553829.
Train: 2018-08-05T23:54:33.556796: step 10619, loss 0.614179.
Train: 2018-08-05T23:54:33.725446: step 10620, loss 0.648488.
Test: 2018-08-05T23:54:33.960000: step 10620, loss 0.548079.
Train: 2018-08-05T23:54:34.130061: step 10621, loss 0.545254.
Train: 2018-08-05T23:54:34.288424: step 10622, loss 0.588053.
Train: 2018-08-05T23:54:34.456063: step 10623, loss 0.579439.
Train: 2018-08-05T23:54:34.622814: step 10624, loss 0.511738.
Train: 2018-08-05T23:54:34.787422: step 10625, loss 0.604492.
Train: 2018-08-05T23:54:34.964911: step 10626, loss 0.503606.
Train: 2018-08-05T23:54:35.131500: step 10627, loss 0.54564.
Train: 2018-08-05T23:54:35.283167: step 10628, loss 0.604403.
Train: 2018-08-05T23:54:35.461740: step 10629, loss 0.520583.
Train: 2018-08-05T23:54:35.627972: step 10630, loss 0.554169.
Test: 2018-08-05T23:54:35.868992: step 10630, loss 0.548449.
Train: 2018-08-05T23:54:36.064011: step 10631, loss 0.545723.
Train: 2018-08-05T23:54:36.240018: step 10632, loss 0.654622.
Train: 2018-08-05T23:54:36.414548: step 10633, loss 0.604332.
Train: 2018-08-05T23:54:36.585115: step 10634, loss 0.58754.
Train: 2018-08-05T23:54:36.751693: step 10635, loss 0.479186.
Train: 2018-08-05T23:54:36.940265: step 10636, loss 0.612385.
Train: 2018-08-05T23:54:37.109212: step 10637, loss 0.620661.
Train: 2018-08-05T23:54:37.279781: step 10638, loss 0.587428.
Train: 2018-08-05T23:54:37.439599: step 10639, loss 0.554205.
Train: 2018-08-05T23:54:37.611400: step 10640, loss 0.546073.
Test: 2018-08-05T23:54:37.853140: step 10640, loss 0.54873.
Train: 2018-08-05T23:54:38.020027: step 10641, loss 0.554275.
Train: 2018-08-05T23:54:38.179331: step 10642, loss 0.546104.
Train: 2018-08-05T23:54:38.346998: step 10643, loss 0.60384.
Train: 2018-08-05T23:54:38.510293: step 10644, loss 0.570816.
Train: 2018-08-05T23:54:38.681838: step 10645, loss 0.521366.
Train: 2018-08-05T23:54:38.848791: step 10646, loss 0.554267.
Train: 2018-08-05T23:54:39.026348: step 10647, loss 0.603692.
Train: 2018-08-05T23:54:39.183538: step 10648, loss 0.537666.
Train: 2018-08-05T23:54:39.339783: step 10649, loss 0.488026.
Train: 2018-08-05T23:54:39.511618: step 10650, loss 0.529401.
Test: 2018-08-05T23:54:39.754291: step 10650, loss 0.548584.
Train: 2018-08-05T23:54:39.913079: step 10651, loss 0.53762.
Train: 2018-08-05T23:54:40.088792: step 10652, loss 0.503962.
Train: 2018-08-05T23:54:40.253350: step 10653, loss 0.545655.
Train: 2018-08-05T23:54:40.425184: step 10654, loss 0.528783.
Train: 2018-08-05T23:54:40.588057: step 10655, loss 0.553954.
Train: 2018-08-05T23:54:40.767578: step 10656, loss 0.604949.
Train: 2018-08-05T23:54:40.931071: step 10657, loss 0.545271.
Train: 2018-08-05T23:54:41.101239: step 10658, loss 0.605314.
Train: 2018-08-05T23:54:41.257455: step 10659, loss 0.510957.
Train: 2018-08-05T23:54:41.436781: step 10660, loss 0.631305.
Test: 2018-08-05T23:54:41.664106: step 10660, loss 0.547967.
Train: 2018-08-05T23:54:41.829563: step 10661, loss 0.648563.
Train: 2018-08-05T23:54:42.010858: step 10662, loss 0.596884.
Train: 2018-08-05T23:54:42.164253: step 10663, loss 0.536524.
Train: 2018-08-05T23:54:42.343033: step 10664, loss 0.562516.
Train: 2018-08-05T23:54:42.499271: step 10665, loss 0.6141.
Train: 2018-08-05T23:54:42.662593: step 10666, loss 0.56251.
Train: 2018-08-05T23:54:42.845452: step 10667, loss 0.519465.
Train: 2018-08-05T23:54:42.995246: step 10668, loss 0.562726.
Train: 2018-08-05T23:54:43.170342: step 10669, loss 0.571113.
Train: 2018-08-05T23:54:43.341857: step 10670, loss 0.528132.
Test: 2018-08-05T23:54:43.576765: step 10670, loss 0.548082.
Train: 2018-08-05T23:54:43.754782: step 10671, loss 0.588004.
Train: 2018-08-05T23:54:43.913285: step 10672, loss 0.605236.
Train: 2018-08-05T23:54:44.076757: step 10673, loss 0.528341.
Train: 2018-08-05T23:54:44.248618: step 10674, loss 0.493952.
Train: 2018-08-05T23:54:44.418439: step 10675, loss 0.570941.
Train: 2018-08-05T23:54:44.571736: step 10676, loss 0.519557.
Train: 2018-08-05T23:54:44.743601: step 10677, loss 0.536807.
Train: 2018-08-05T23:54:44.919622: step 10678, loss 0.5968.
Train: 2018-08-05T23:54:45.067501: step 10679, loss 0.56245.
Train: 2018-08-05T23:54:45.246645: step 10680, loss 0.605697.
Test: 2018-08-05T23:54:45.488340: step 10680, loss 0.547994.
Train: 2018-08-05T23:54:45.652817: step 10681, loss 0.588133.
Train: 2018-08-05T23:54:45.832934: step 10682, loss 0.562435.
Train: 2018-08-05T23:54:45.989174: step 10683, loss 0.614028.
Train: 2018-08-05T23:54:46.166046: step 10684, loss 0.570951.
Train: 2018-08-05T23:54:46.332600: step 10685, loss 0.588124.
Train: 2018-08-05T23:54:46.502171: step 10686, loss 0.468292.
Train: 2018-08-05T23:54:46.656807: step 10687, loss 0.528029.
Train: 2018-08-05T23:54:46.834768: step 10688, loss 0.528074.
Train: 2018-08-05T23:54:46.991849: step 10689, loss 0.519483.
Train: 2018-08-05T23:54:47.157034: step 10690, loss 0.536647.
Test: 2018-08-05T23:54:47.398079: step 10690, loss 0.547973.
Train: 2018-08-05T23:54:47.560847: step 10691, loss 0.579642.
Train: 2018-08-05T23:54:47.746009: step 10692, loss 0.588341.
Train: 2018-08-05T23:54:47.909612: step 10693, loss 0.527848.
Train: 2018-08-05T23:54:48.088650: step 10694, loss 0.553734.
Train: 2018-08-05T23:54:48.260336: step 10695, loss 0.597046.
Train: 2018-08-05T23:54:48.419171: step 10696, loss 0.614607.
Train: 2018-08-05T23:54:48.591038: step 10697, loss 0.562374.
Train: 2018-08-05T23:54:48.745215: step 10698, loss 0.571092.
Train: 2018-08-05T23:54:48.916062: step 10699, loss 0.553555.
Train: 2018-08-05T23:54:49.089183: step 10700, loss 0.536646.
Test: 2018-08-05T23:54:49.324348: step 10700, loss 0.547901.
Train: 2018-08-05T23:54:50.050304: step 10701, loss 0.492925.
Train: 2018-08-05T23:54:50.214451: step 10702, loss 0.57975.
Train: 2018-08-05T23:54:50.384879: step 10703, loss 0.527685.
Train: 2018-08-05T23:54:50.546266: step 10704, loss 0.484081.
Train: 2018-08-05T23:54:50.711299: step 10705, loss 0.579746.
Train: 2018-08-05T23:54:50.892282: step 10706, loss 0.536108.
Train: 2018-08-05T23:54:51.047977: step 10707, loss 0.474793.
Train: 2018-08-05T23:54:51.226463: step 10708, loss 0.544964.
Train: 2018-08-05T23:54:51.411441: step 10709, loss 0.58019.
Train: 2018-08-05T23:54:51.588815: step 10710, loss 0.571254.
Test: 2018-08-05T23:54:51.824712: step 10710, loss 0.547675.
Train: 2018-08-05T23:54:51.987217: step 10711, loss 0.642455.
Train: 2018-08-05T23:54:52.159820: step 10712, loss 0.660425.
Train: 2018-08-05T23:54:52.329442: step 10713, loss 0.553649.
Train: 2018-08-05T23:54:52.527680: step 10714, loss 0.544758.
Train: 2018-08-05T23:54:52.692331: step 10715, loss 0.589217.
Train: 2018-08-05T23:54:52.864189: step 10716, loss 0.562461.
Train: 2018-08-05T23:54:53.020378: step 10717, loss 0.571575.
Train: 2018-08-05T23:54:53.196716: step 10718, loss 0.588417.
Train: 2018-08-05T23:54:53.378230: step 10719, loss 0.544883.
Train: 2018-08-05T23:54:53.550800: step 10720, loss 0.562365.
Test: 2018-08-05T23:54:53.789370: step 10720, loss 0.54782.
Train: 2018-08-05T23:54:53.999676: step 10721, loss 0.599684.
Train: 2018-08-05T23:54:54.187118: step 10722, loss 0.536141.
Train: 2018-08-05T23:54:54.366461: step 10723, loss 0.554036.
Train: 2018-08-05T23:54:54.529250: step 10724, loss 0.597151.
Train: 2018-08-05T23:54:54.695534: step 10725, loss 0.631801.
Train: 2018-08-05T23:54:54.870590: step 10726, loss 0.649012.
Train: 2018-08-05T23:54:55.030458: step 10727, loss 0.588233.
Train: 2018-08-05T23:54:55.203330: step 10728, loss 0.562358.
Train: 2018-08-05T23:54:55.364332: step 10729, loss 0.485829.
Train: 2018-08-05T23:54:55.528830: step 10730, loss 0.528548.
Test: 2018-08-05T23:54:55.770688: step 10730, loss 0.548227.
Train: 2018-08-05T23:54:55.940131: step 10731, loss 0.52847.
Train: 2018-08-05T23:54:56.107780: step 10732, loss 0.469275.
Train: 2018-08-05T23:54:56.275887: step 10733, loss 0.596367.
Train: 2018-08-05T23:54:56.445430: step 10734, loss 0.596404.
Train: 2018-08-05T23:54:56.602368: step 10735, loss 0.485891.
Train: 2018-08-05T23:54:56.781263: step 10736, loss 0.545326.
Train: 2018-08-05T23:54:56.937477: step 10737, loss 0.528421.
Train: 2018-08-05T23:54:57.103372: step 10738, loss 0.502653.
Train: 2018-08-05T23:54:57.266400: step 10739, loss 0.545222.
Train: 2018-08-05T23:54:57.445942: step 10740, loss 0.571006.
Test: 2018-08-05T23:54:57.678738: step 10740, loss 0.547953.
Train: 2018-08-05T23:54:57.857952: step 10741, loss 0.562396.
Train: 2018-08-05T23:54:58.021092: step 10742, loss 0.562451.
Train: 2018-08-05T23:54:58.192959: step 10743, loss 0.588602.
Train: 2018-08-05T23:54:58.368117: step 10744, loss 0.588497.
Train: 2018-08-05T23:54:58.539949: step 10745, loss 0.554022.
Train: 2018-08-05T23:54:58.707499: step 10746, loss 0.57108.
Train: 2018-08-05T23:54:58.879378: step 10747, loss 0.536528.
Train: 2018-08-05T23:54:59.046268: step 10748, loss 0.536206.
Train: 2018-08-05T23:54:59.218105: step 10749, loss 0.484018.
Train: 2018-08-05T23:54:59.374287: step 10750, loss 0.562498.
Test: 2018-08-05T23:54:59.623290: step 10750, loss 0.54778.
Train: 2018-08-05T23:54:59.780759: step 10751, loss 0.588682.
Train: 2018-08-05T23:54:59.959942: step 10752, loss 0.623918.
Train: 2018-08-05T23:55:00.123416: step 10753, loss 0.62391.
Train: 2018-08-05T23:55:00.286900: step 10754, loss 0.553668.
Train: 2018-08-05T23:55:00.458962: step 10755, loss 0.536206.
Train: 2018-08-05T23:55:00.630143: step 10756, loss 0.544868.
Train: 2018-08-05T23:55:00.777070: step 10757, loss 0.562614.
Train: 2018-08-05T23:55:00.956350: step 10758, loss 0.492544.
Train: 2018-08-05T23:55:01.119912: step 10759, loss 0.536203.
Train: 2018-08-05T23:55:01.298984: step 10760, loss 0.61495.
Test: 2018-08-05T23:55:01.524476: step 10760, loss 0.547785.
Train: 2018-08-05T23:55:01.702810: step 10761, loss 0.510012.
Train: 2018-08-05T23:55:01.867170: step 10762, loss 0.509748.
Train: 2018-08-05T23:55:02.045852: step 10763, loss 0.580133.
Train: 2018-08-05T23:55:02.209334: step 10764, loss 0.553693.
Train: 2018-08-05T23:55:02.372814: step 10765, loss 0.597702.
Train: 2018-08-05T23:55:02.551938: step 10766, loss 0.545075.
Train: 2018-08-05T23:55:02.714530: step 10767, loss 0.562297.
Train: 2018-08-05T23:55:02.877635: step 10768, loss 0.571332.
Train: 2018-08-05T23:55:03.056781: step 10769, loss 0.509411.
Train: 2018-08-05T23:55:03.234856: step 10770, loss 0.518305.
Test: 2018-08-05T23:55:03.475213: step 10770, loss 0.547713.
Train: 2018-08-05T23:55:03.641617: step 10771, loss 0.527185.
Train: 2018-08-05T23:55:03.811810: step 10772, loss 0.580163.
Train: 2018-08-05T23:55:03.986790: step 10773, loss 0.571357.
Train: 2018-08-05T23:55:04.147174: step 10774, loss 0.615748.
Train: 2018-08-05T23:55:04.310398: step 10775, loss 0.615691.
Train: 2018-08-05T23:55:04.484888: step 10776, loss 0.535937.
Train: 2018-08-05T23:55:04.659492: step 10777, loss 0.527151.
Train: 2018-08-05T23:55:04.815735: step 10778, loss 0.527181.
Train: 2018-08-05T23:55:04.986614: step 10779, loss 0.571303.
Train: 2018-08-05T23:55:05.158449: step 10780, loss 0.527233.
Test: 2018-08-05T23:55:05.394909: step 10780, loss 0.547706.
Train: 2018-08-05T23:55:05.566745: step 10781, loss 0.536069.
Train: 2018-08-05T23:55:05.729672: step 10782, loss 0.518105.
Train: 2018-08-05T23:55:05.908924: step 10783, loss 0.722063.
Train: 2018-08-05T23:55:06.072464: step 10784, loss 0.606849.
Train: 2018-08-05T23:55:06.249860: step 10785, loss 0.606431.
Train: 2018-08-05T23:55:06.423427: step 10786, loss 0.527353.
Train: 2018-08-05T23:55:06.587695: step 10787, loss 0.562333.
Train: 2018-08-05T23:55:06.742893: step 10788, loss 0.606075.
Train: 2018-08-05T23:55:06.921970: step 10789, loss 0.536369.
Train: 2018-08-05T23:55:07.086012: step 10790, loss 0.588448.
Test: 2018-08-05T23:55:07.327173: step 10790, loss 0.547908.
Train: 2018-08-05T23:55:07.493360: step 10791, loss 0.484493.
Train: 2018-08-05T23:55:07.669388: step 10792, loss 0.510438.
Train: 2018-08-05T23:55:07.828146: step 10793, loss 0.570997.
Train: 2018-08-05T23:55:07.991365: step 10794, loss 0.475823.
Train: 2018-08-05T23:55:08.154301: step 10795, loss 0.553585.
Train: 2018-08-05T23:55:08.333556: step 10796, loss 0.545005.
Train: 2018-08-05T23:55:08.502952: step 10797, loss 0.545144.
Train: 2018-08-05T23:55:08.668670: step 10798, loss 0.579821.
Train: 2018-08-05T23:55:08.827539: step 10799, loss 0.605976.
Train: 2018-08-05T23:55:08.999373: step 10800, loss 0.544973.
Test: 2018-08-05T23:55:09.231907: step 10800, loss 0.547814.
Train: 2018-08-05T23:55:09.935102: step 10801, loss 0.579948.
Train: 2018-08-05T23:55:10.106962: step 10802, loss 0.588669.
Train: 2018-08-05T23:55:10.270503: step 10803, loss 0.571022.
Train: 2018-08-05T23:55:10.434003: step 10804, loss 0.553895.
Train: 2018-08-05T23:55:10.613189: step 10805, loss 0.518888.
Train: 2018-08-05T23:55:10.776675: step 10806, loss 0.492544.
Train: 2018-08-05T23:55:10.932889: step 10807, loss 0.658588.
Train: 2018-08-05T23:55:11.102120: step 10808, loss 0.536573.
Train: 2018-08-05T23:55:11.280341: step 10809, loss 0.571367.
Train: 2018-08-05T23:55:11.443840: step 10810, loss 0.580061.
Test: 2018-08-05T23:55:11.685490: step 10810, loss 0.547843.
Train: 2018-08-05T23:55:11.849020: step 10811, loss 0.536324.
Train: 2018-08-05T23:55:12.029238: step 10812, loss 0.605973.
Train: 2018-08-05T23:55:12.192790: step 10813, loss 0.55379.
Train: 2018-08-05T23:55:12.355469: step 10814, loss 0.640674.
Train: 2018-08-05T23:55:12.518914: step 10815, loss 0.56255.
Train: 2018-08-05T23:55:12.698103: step 10816, loss 0.53644.
Train: 2018-08-05T23:55:12.870602: step 10817, loss 0.519385.
Train: 2018-08-05T23:55:13.035447: step 10818, loss 0.614019.
Train: 2018-08-05T23:55:13.206865: step 10819, loss 0.519282.
Train: 2018-08-05T23:55:13.375415: step 10820, loss 0.553872.
Test: 2018-08-05T23:55:13.610236: step 10820, loss 0.547999.
Train: 2018-08-05T23:55:13.773153: step 10821, loss 0.605427.
Train: 2018-08-05T23:55:13.939072: step 10822, loss 0.571021.
Train: 2018-08-05T23:55:14.102524: step 10823, loss 0.545213.
Train: 2018-08-05T23:55:14.274359: step 10824, loss 0.596756.
Train: 2018-08-05T23:55:14.453573: step 10825, loss 0.502467.
Train: 2018-08-05T23:55:14.617261: step 10826, loss 0.528078.
Train: 2018-08-05T23:55:14.784983: step 10827, loss 0.502475.
Train: 2018-08-05T23:55:14.964174: step 10828, loss 0.622626.
Train: 2018-08-05T23:55:15.120417: step 10829, loss 0.570954.
Train: 2018-08-05T23:55:15.299769: step 10830, loss 0.536612.
Test: 2018-08-05T23:55:15.532361: step 10830, loss 0.548013.
Train: 2018-08-05T23:55:15.710892: step 10831, loss 0.52794.
Train: 2018-08-05T23:55:15.874387: step 10832, loss 0.502176.
Train: 2018-08-05T23:55:16.037893: step 10833, loss 0.545175.
Train: 2018-08-05T23:55:16.214330: step 10834, loss 0.614246.
Train: 2018-08-05T23:55:16.385840: step 10835, loss 0.527854.
Train: 2018-08-05T23:55:16.556415: step 10836, loss 0.605863.
Train: 2018-08-05T23:55:16.723985: step 10837, loss 0.570979.
Train: 2018-08-05T23:55:16.877388: step 10838, loss 0.640639.
Train: 2018-08-05T23:55:17.056210: step 10839, loss 0.553703.
Train: 2018-08-05T23:55:17.228577: step 10840, loss 0.588344.
Test: 2018-08-05T23:55:17.463216: step 10840, loss 0.547929.
Train: 2018-08-05T23:55:17.639251: step 10841, loss 0.501896.
Train: 2018-08-05T23:55:17.804407: step 10842, loss 0.501927.
Train: 2018-08-05T23:55:17.967370: step 10843, loss 0.605739.
Train: 2018-08-05T23:55:18.147505: step 10844, loss 0.579802.
Train: 2018-08-05T23:55:18.310793: step 10845, loss 0.597128.
Train: 2018-08-05T23:55:18.467006: step 10846, loss 0.674644.
Train: 2018-08-05T23:55:18.641519: step 10847, loss 0.596839.
Train: 2018-08-05T23:55:18.813354: step 10848, loss 0.528162.
Train: 2018-08-05T23:55:18.983279: step 10849, loss 0.545486.
Train: 2018-08-05T23:55:19.152142: step 10850, loss 0.553901.
Test: 2018-08-05T23:55:19.390310: step 10850, loss 0.548136.
Train: 2018-08-05T23:55:19.553808: step 10851, loss 0.562324.
Train: 2018-08-05T23:55:19.717324: step 10852, loss 0.596456.
Train: 2018-08-05T23:55:19.896572: step 10853, loss 0.528468.
Train: 2018-08-05T23:55:20.052785: step 10854, loss 0.562387.
Train: 2018-08-05T23:55:20.232214: step 10855, loss 0.536993.
Train: 2018-08-05T23:55:20.395736: step 10856, loss 0.587799.
Train: 2018-08-05T23:55:20.559207: step 10857, loss 0.587866.
Train: 2018-08-05T23:55:20.739415: step 10858, loss 0.596278.
Train: 2018-08-05T23:55:20.888274: step 10859, loss 0.537024.
Train: 2018-08-05T23:55:21.067084: step 10860, loss 0.52866.
Test: 2018-08-05T23:55:21.292621: step 10860, loss 0.548303.
Train: 2018-08-05T23:55:21.471177: step 10861, loss 0.570743.
Train: 2018-08-05T23:55:21.634653: step 10862, loss 0.61308.
Train: 2018-08-05T23:55:21.797422: step 10863, loss 0.520265.
Train: 2018-08-05T23:55:21.981705: step 10864, loss 0.545598.
Train: 2018-08-05T23:55:22.139069: step 10865, loss 0.59619.
Train: 2018-08-05T23:55:22.302674: step 10866, loss 0.596224.
Train: 2018-08-05T23:55:22.481901: step 10867, loss 0.638255.
Train: 2018-08-05T23:55:22.645479: step 10868, loss 0.50356.
Train: 2018-08-05T23:55:22.822520: step 10869, loss 0.59599.
Train: 2018-08-05T23:55:22.990128: step 10870, loss 0.595971.
Test: 2018-08-05T23:55:23.231106: step 10870, loss 0.548449.
Train: 2018-08-05T23:55:23.401634: step 10871, loss 0.537364.
Train: 2018-08-05T23:55:23.565228: step 10872, loss 0.616072.
Train: 2018-08-05T23:55:23.717855: step 10873, loss 0.54574.
Train: 2018-08-05T23:55:23.881435: step 10874, loss 0.595955.
Train: 2018-08-05T23:55:24.052196: step 10875, loss 0.562487.
Train: 2018-08-05T23:55:24.225702: step 10876, loss 0.629017.
Train: 2018-08-05T23:55:24.387286: step 10877, loss 0.554291.
Train: 2018-08-05T23:55:24.566116: step 10878, loss 0.58737.
Train: 2018-08-05T23:55:24.731925: step 10879, loss 0.587348.
Train: 2018-08-05T23:55:24.894863: step 10880, loss 0.620373.
Test: 2018-08-05T23:55:25.135971: step 10880, loss 0.548854.
Train: 2018-08-05T23:55:25.305252: step 10881, loss 0.603657.
Train: 2018-08-05T23:55:25.472613: step 10882, loss 0.562666.
Train: 2018-08-05T23:55:25.620997: step 10883, loss 0.513818.
Train: 2018-08-05T23:55:25.799968: step 10884, loss 0.554505.
Train: 2018-08-05T23:55:25.962802: step 10885, loss 0.538262.
Train: 2018-08-05T23:55:26.134667: step 10886, loss 0.562699.
Train: 2018-08-05T23:55:26.318026: step 10887, loss 0.554395.
Train: 2018-08-05T23:55:26.489568: step 10888, loss 0.546458.
Train: 2018-08-05T23:55:26.642886: step 10889, loss 0.562741.
Train: 2018-08-05T23:55:26.809989: step 10890, loss 0.595397.
Test: 2018-08-05T23:55:27.051796: step 10890, loss 0.548945.
Train: 2018-08-05T23:55:27.216630: step 10891, loss 0.521777.
Train: 2018-08-05T23:55:27.388495: step 10892, loss 0.529862.
Train: 2018-08-05T23:55:27.558307: step 10893, loss 0.603692.
Train: 2018-08-05T23:55:27.734404: step 10894, loss 0.504933.
Train: 2018-08-05T23:55:27.884068: step 10895, loss 0.562427.
Train: 2018-08-05T23:55:28.055902: step 10896, loss 0.587383.
Train: 2018-08-05T23:55:28.219349: step 10897, loss 0.587447.
Train: 2018-08-05T23:55:28.389204: step 10898, loss 0.546017.
Train: 2018-08-05T23:55:28.562217: step 10899, loss 0.529251.
Train: 2018-08-05T23:55:28.729147: step 10900, loss 0.63755.
Test: 2018-08-05T23:55:28.966721: step 10900, loss 0.548485.
Train: 2018-08-05T23:55:29.739201: step 10901, loss 0.545733.
Train: 2018-08-05T23:55:29.918436: step 10902, loss 0.587573.
Train: 2018-08-05T23:55:30.085255: step 10903, loss 0.637798.
Train: 2018-08-05T23:55:30.248710: step 10904, loss 0.545741.
Train: 2018-08-05T23:55:30.411658: step 10905, loss 0.604204.
Train: 2018-08-05T23:55:30.577992: step 10906, loss 0.629306.
Train: 2018-08-05T23:55:30.738619: step 10907, loss 0.620802.
Train: 2018-08-05T23:55:30.917678: step 10908, loss 0.587455.
Train: 2018-08-05T23:55:31.084588: step 10909, loss 0.479721.
Train: 2018-08-05T23:55:31.256424: step 10910, loss 0.570835.
Test: 2018-08-05T23:55:31.504083: step 10910, loss 0.548748.
Train: 2018-08-05T23:55:31.691412: step 10911, loss 0.537858.
Train: 2018-08-05T23:55:31.846124: step 10912, loss 0.50461.
Train: 2018-08-05T23:55:32.026367: step 10913, loss 0.52955.
Train: 2018-08-05T23:55:32.189778: step 10914, loss 0.529341.
Train: 2018-08-05T23:55:32.363171: step 10915, loss 0.604016.
Train: 2018-08-05T23:55:32.519385: step 10916, loss 0.554201.
Train: 2018-08-05T23:55:32.697724: step 10917, loss 0.562488.
Train: 2018-08-05T23:55:32.861025: step 10918, loss 0.512369.
Train: 2018-08-05T23:55:33.021449: step 10919, loss 0.520482.
Train: 2018-08-05T23:55:33.198920: step 10920, loss 0.461465.
Test: 2018-08-05T23:55:33.438304: step 10920, loss 0.548251.
Train: 2018-08-05T23:55:33.604726: step 10921, loss 0.537053.
Train: 2018-08-05T23:55:33.771265: step 10922, loss 0.57081.
Train: 2018-08-05T23:55:33.941746: step 10923, loss 0.51963.
Train: 2018-08-05T23:55:34.097929: step 10924, loss 0.484814.
Train: 2018-08-05T23:55:34.269793: step 10925, loss 0.519178.
Train: 2018-08-05T23:55:34.440629: step 10926, loss 0.501219.
Train: 2018-08-05T23:55:34.604083: step 10927, loss 0.518546.
Train: 2018-08-05T23:55:34.783211: step 10928, loss 0.633192.
Train: 2018-08-05T23:55:34.946802: step 10929, loss 0.571423.
Train: 2018-08-05T23:55:35.116634: step 10930, loss 0.544761.
Test: 2018-08-05T23:55:35.358684: step 10930, loss 0.547622.
Train: 2018-08-05T23:55:35.521258: step 10931, loss 0.589438.
Train: 2018-08-05T23:55:35.699432: step 10932, loss 0.580579.
Train: 2018-08-05T23:55:35.862668: step 10933, loss 0.526772.
Train: 2018-08-05T23:55:36.026195: step 10934, loss 0.553617.
Train: 2018-08-05T23:55:36.206169: step 10935, loss 0.644056.
Train: 2018-08-05T23:55:36.385689: step 10936, loss 0.517654.
Train: 2018-08-05T23:55:36.577177: step 10937, loss 0.571981.
Train: 2018-08-05T23:55:36.738773: step 10938, loss 0.553817.
Train: 2018-08-05T23:55:36.896395: step 10939, loss 0.535769.
Train: 2018-08-05T23:55:37.075284: step 10940, loss 0.526597.
Test: 2018-08-05T23:55:37.303717: step 10940, loss 0.547589.
Train: 2018-08-05T23:55:37.482210: step 10941, loss 0.544692.
Train: 2018-08-05T23:55:37.645034: step 10942, loss 0.544635.
Train: 2018-08-05T23:55:37.808530: step 10943, loss 0.571773.
Train: 2018-08-05T23:55:37.971408: step 10944, loss 0.526442.
Train: 2018-08-05T23:55:38.134989: step 10945, loss 0.47214.
Train: 2018-08-05T23:55:38.298181: step 10946, loss 0.571888.
Train: 2018-08-05T23:55:38.470017: step 10947, loss 0.599214.
Train: 2018-08-05T23:55:38.630059: step 10948, loss 0.590087.
Train: 2018-08-05T23:55:38.801893: step 10949, loss 0.68083.
Train: 2018-08-05T23:55:38.972292: step 10950, loss 0.617232.
Test: 2018-08-05T23:55:39.203684: step 10950, loss 0.547594.
Train: 2018-08-05T23:55:39.379327: step 10951, loss 0.553709.
Train: 2018-08-05T23:55:39.543886: step 10952, loss 0.562713.
Train: 2018-08-05T23:55:39.708343: step 10953, loss 0.517751.
Train: 2018-08-05T23:55:39.888526: step 10954, loss 0.509071.
Train: 2018-08-05T23:55:40.060391: step 10955, loss 0.616128.
Train: 2018-08-05T23:55:40.223859: step 10956, loss 0.589501.
Train: 2018-08-05T23:55:40.388376: step 10957, loss 0.58924.
Train: 2018-08-05T23:55:40.568249: step 10958, loss 0.6598.
Train: 2018-08-05T23:55:40.732674: step 10959, loss 0.606435.
Train: 2018-08-05T23:55:40.911174: step 10960, loss 0.606212.
Test: 2018-08-05T23:55:41.160461: step 10960, loss 0.547867.
Train: 2018-08-05T23:55:41.332774: step 10961, loss 0.492859.
Train: 2018-08-05T23:55:41.495466: step 10962, loss 0.484398.
Train: 2018-08-05T23:55:41.660552: step 10963, loss 0.562403.
Train: 2018-08-05T23:55:41.823395: step 10964, loss 0.553875.
Train: 2018-08-05T23:55:41.987226: step 10965, loss 0.553857.
Train: 2018-08-05T23:55:42.175471: step 10966, loss 0.61394.
Train: 2018-08-05T23:55:42.345029: step 10967, loss 0.493797.
Train: 2018-08-05T23:55:42.495852: step 10968, loss 0.528111.
Train: 2018-08-05T23:55:42.659023: step 10969, loss 0.622477.
Train: 2018-08-05T23:55:42.838065: step 10970, loss 0.605203.
Test: 2018-08-05T23:55:43.073974: step 10970, loss 0.548086.
Train: 2018-08-05T23:55:43.254057: step 10971, loss 0.596606.
Train: 2018-08-05T23:55:43.419644: step 10972, loss 0.570951.
Train: 2018-08-05T23:55:43.591181: step 10973, loss 0.528403.
Train: 2018-08-05T23:55:43.762459: step 10974, loss 0.536907.
Train: 2018-08-05T23:55:43.926321: step 10975, loss 0.468958.
Train: 2018-08-05T23:55:44.105075: step 10976, loss 0.494364.
Train: 2018-08-05T23:55:44.267574: step 10977, loss 0.54538.
Train: 2018-08-05T23:55:44.432075: step 10978, loss 0.61377.
Train: 2018-08-05T23:55:44.603884: step 10979, loss 0.54525.
Train: 2018-08-05T23:55:44.767472: step 10980, loss 0.631062.
Test: 2018-08-05T23:55:44.993669: step 10980, loss 0.548022.
Train: 2018-08-05T23:55:45.163600: step 10981, loss 0.613881.
Train: 2018-08-05T23:55:45.342753: step 10982, loss 0.605209.
Train: 2018-08-05T23:55:45.491611: step 10983, loss 0.571025.
Train: 2018-08-05T23:55:45.670794: step 10984, loss 0.579584.
Train: 2018-08-05T23:55:45.827009: step 10985, loss 0.545361.
Train: 2018-08-05T23:55:46.006379: step 10986, loss 0.536834.
Train: 2018-08-05T23:55:46.181550: step 10987, loss 0.605028.
Train: 2018-08-05T23:55:46.355086: step 10988, loss 0.562454.
Train: 2018-08-05T23:55:46.523604: step 10989, loss 0.451989.
Train: 2018-08-05T23:55:46.695432: step 10990, loss 0.613451.
Test: 2018-08-05T23:55:46.935252: step 10990, loss 0.548168.
Train: 2018-08-05T23:55:47.096634: step 10991, loss 0.545439.
Train: 2018-08-05T23:55:47.267198: step 10992, loss 0.553875.
Train: 2018-08-05T23:55:47.438296: step 10993, loss 0.528305.
Train: 2018-08-05T23:55:47.593309: step 10994, loss 0.605012.
Train: 2018-08-05T23:55:47.772174: step 10995, loss 0.596535.
Train: 2018-08-05T23:55:47.945720: step 10996, loss 0.553894.
Train: 2018-08-05T23:55:48.111123: step 10997, loss 0.69028.
Train: 2018-08-05T23:55:48.274323: step 10998, loss 0.528411.
Train: 2018-08-05T23:55:48.437156: step 10999, loss 0.604797.
Train: 2018-08-05T23:55:48.606514: step 11000, loss 0.570882.
Test: 2018-08-05T23:55:48.848206: step 11000, loss 0.548288.
Train: 2018-08-05T23:55:49.576025: step 11001, loss 0.520203.
Train: 2018-08-05T23:55:49.751008: step 11002, loss 0.562486.
Train: 2018-08-05T23:55:49.901098: step 11003, loss 0.596141.
Train: 2018-08-05T23:55:50.076752: step 11004, loss 0.56241.
Train: 2018-08-05T23:55:50.240172: step 11005, loss 0.512007.
Train: 2018-08-05T23:55:50.404672: step 11006, loss 0.596086.
Train: 2018-08-05T23:55:50.560886: step 11007, loss 0.579252.
Train: 2018-08-05T23:55:50.740149: step 11008, loss 0.621265.
Train: 2018-08-05T23:55:50.904597: step 11009, loss 0.570882.
Train: 2018-08-05T23:55:51.068095: step 11010, loss 0.554081.
Test: 2018-08-05T23:55:51.311385: step 11010, loss 0.548493.
Train: 2018-08-05T23:55:51.475890: step 11011, loss 0.57084.
Train: 2018-08-05T23:55:51.638408: step 11012, loss 0.462375.
Train: 2018-08-05T23:55:51.814444: step 11013, loss 0.587562.
Train: 2018-08-05T23:55:51.980647: step 11014, loss 0.545749.
Train: 2018-08-05T23:55:52.143892: step 11015, loss 0.604309.
Train: 2018-08-05T23:55:52.309694: step 11016, loss 0.562453.
Train: 2018-08-05T23:55:52.481677: step 11017, loss 0.461988.
Train: 2018-08-05T23:55:52.644418: step 11018, loss 0.646429.
Train: 2018-08-05T23:55:52.809552: step 11019, loss 0.537273.
Train: 2018-08-05T23:55:52.977593: step 11020, loss 0.478359.
Test: 2018-08-05T23:55:53.222171: step 11020, loss 0.548308.
Train: 2018-08-05T23:55:53.392716: step 11021, loss 0.511828.
Train: 2018-08-05T23:55:53.568279: step 11022, loss 0.528539.
Train: 2018-08-05T23:55:53.730656: step 11023, loss 0.580514.
Train: 2018-08-05T23:55:53.907832: step 11024, loss 0.56239.
Train: 2018-08-05T23:55:54.077193: step 11025, loss 0.562378.
Train: 2018-08-05T23:55:54.246884: step 11026, loss 0.674002.
Train: 2018-08-05T23:55:54.420321: step 11027, loss 0.553823.
Train: 2018-08-05T23:55:54.585188: step 11028, loss 0.467974.
Train: 2018-08-05T23:55:54.754710: step 11029, loss 0.502151.
Train: 2018-08-05T23:55:54.926545: step 11030, loss 0.588344.
Test: 2018-08-05T23:55:55.168234: step 11030, loss 0.547913.
Train: 2018-08-05T23:55:55.362992: step 11031, loss 0.545074.
Train: 2018-08-05T23:55:55.538947: step 11032, loss 0.58852.
Train: 2018-08-05T23:55:55.705668: step 11033, loss 0.518967.
Train: 2018-08-05T23:55:55.888952: step 11034, loss 0.597215.
Train: 2018-08-05T23:55:56.063729: step 11035, loss 0.597321.
Train: 2018-08-05T23:55:56.236757: step 11036, loss 0.579861.
Train: 2018-08-05T23:55:56.417312: step 11037, loss 0.440267.
Train: 2018-08-05T23:55:56.587906: step 11038, loss 0.544984.
Train: 2018-08-05T23:55:56.765600: step 11039, loss 0.641383.
Train: 2018-08-05T23:55:56.932756: step 11040, loss 0.650137.
Test: 2018-08-05T23:55:57.172231: step 11040, loss 0.547787.
Train: 2018-08-05T23:55:57.331308: step 11041, loss 0.544899.
Train: 2018-08-05T23:55:57.511778: step 11042, loss 0.649876.
Train: 2018-08-05T23:55:57.674545: step 11043, loss 0.545011.
Train: 2018-08-05T23:55:57.854905: step 11044, loss 0.52765.
Train: 2018-08-05T23:55:58.022947: step 11045, loss 0.527633.
Train: 2018-08-05T23:55:58.194395: step 11046, loss 0.536354.
Train: 2018-08-05T23:55:58.359534: step 11047, loss 0.518978.
Train: 2018-08-05T23:55:58.522880: step 11048, loss 0.440779.
Train: 2018-08-05T23:55:58.697468: step 11049, loss 0.562424.
Train: 2018-08-05T23:55:58.853711: step 11050, loss 0.606143.
Test: 2018-08-05T23:55:59.101772: step 11050, loss 0.547791.
Train: 2018-08-05T23:55:59.254943: step 11051, loss 0.59738.
Train: 2018-08-05T23:55:59.441012: step 11052, loss 0.571162.
Train: 2018-08-05T23:55:59.602381: step 11053, loss 0.579994.
Train: 2018-08-05T23:55:59.781686: step 11054, loss 0.544938.
Train: 2018-08-05T23:55:59.945337: step 11055, loss 0.518702.
Train: 2018-08-05T23:56:00.125035: step 11056, loss 0.562425.
Train: 2018-08-05T23:56:00.289557: step 11057, loss 0.632618.
Train: 2018-08-05T23:56:00.461359: step 11058, loss 0.571217.
Train: 2018-08-05T23:56:00.609317: step 11059, loss 0.623734.
Train: 2018-08-05T23:56:00.788491: step 11060, loss 0.510072.
Test: 2018-08-05T23:56:01.021974: step 11060, loss 0.547829.
Train: 2018-08-05T23:56:01.201020: step 11061, loss 0.588611.
Train: 2018-08-05T23:56:01.376840: step 11062, loss 0.614623.
Train: 2018-08-05T23:56:01.556127: step 11063, loss 0.510318.
Train: 2018-08-05T23:56:01.728086: step 11064, loss 0.467039.
Train: 2018-08-05T23:56:01.891485: step 11065, loss 0.631828.
Train: 2018-08-05T23:56:02.060994: step 11066, loss 0.631767.
Train: 2018-08-05T23:56:02.238674: step 11067, loss 0.579707.
Train: 2018-08-05T23:56:02.403196: step 11068, loss 0.562424.
Train: 2018-08-05T23:56:02.567676: step 11069, loss 0.545176.
Train: 2018-08-05T23:56:02.739541: step 11070, loss 0.493625.
Test: 2018-08-05T23:56:02.966332: step 11070, loss 0.548005.
Train: 2018-08-05T23:56:03.152515: step 11071, loss 0.588209.
Train: 2018-08-05T23:56:03.342378: step 11072, loss 0.510845.
Train: 2018-08-05T23:56:03.518883: step 11073, loss 0.536545.
Train: 2018-08-05T23:56:03.672952: step 11074, loss 0.579585.
Train: 2018-08-05T23:56:03.850499: step 11075, loss 0.519316.
Train: 2018-08-05T23:56:04.027708: step 11076, loss 0.562411.
Train: 2018-08-05T23:56:04.183897: step 11077, loss 0.562422.
Train: 2018-08-05T23:56:04.363167: step 11078, loss 0.527824.
Train: 2018-08-05T23:56:04.539689: step 11079, loss 0.579687.
Train: 2018-08-05T23:56:04.699163: step 11080, loss 0.510426.
Test: 2018-08-05T23:56:04.956494: step 11080, loss 0.547879.
Train: 2018-08-05T23:56:05.120007: step 11081, loss 0.6318.
Train: 2018-08-05T23:56:05.299142: step 11082, loss 0.50163.
Train: 2018-08-05T23:56:05.470096: step 11083, loss 0.545098.
Train: 2018-08-05T23:56:05.650644: step 11084, loss 0.51021.
Train: 2018-08-05T23:56:05.821120: step 11085, loss 0.544995.
Train: 2018-08-05T23:56:05.999774: step 11086, loss 0.5537.
Train: 2018-08-05T23:56:06.167889: step 11087, loss 0.606329.
Train: 2018-08-05T23:56:06.337403: step 11088, loss 0.536135.
Train: 2018-08-05T23:56:06.551830: step 11089, loss 0.597529.
Train: 2018-08-05T23:56:06.720173: step 11090, loss 0.562474.
Test: 2018-08-05T23:56:06.961569: step 11090, loss 0.547738.
Train: 2018-08-05T23:56:07.149019: step 11091, loss 0.562446.
Train: 2018-08-05T23:56:07.336457: step 11092, loss 0.544808.
Train: 2018-08-05T23:56:07.505859: step 11093, loss 0.536026.
Train: 2018-08-05T23:56:07.689536: step 11094, loss 0.553636.
Train: 2018-08-05T23:56:07.862549: step 11095, loss 0.606567.
Train: 2018-08-05T23:56:08.068230: step 11096, loss 0.509626.
Train: 2018-08-05T23:56:08.248022: step 11097, loss 0.571322.
Train: 2018-08-05T23:56:08.418949: step 11098, loss 0.633007.
Train: 2018-08-05T23:56:08.599102: step 11099, loss 0.624131.
Train: 2018-08-05T23:56:08.796361: step 11100, loss 0.571202.
Test: 2018-08-05T23:56:09.037683: step 11100, loss 0.547776.
Train: 2018-08-05T23:56:09.815209: step 11101, loss 0.509922.
Train: 2018-08-05T23:56:09.975005: step 11102, loss 0.553678.
Train: 2018-08-05T23:56:10.157943: step 11103, loss 0.53623.
Train: 2018-08-05T23:56:10.326768: step 11104, loss 0.614833.
Train: 2018-08-05T23:56:10.505427: step 11105, loss 0.57112.
Train: 2018-08-05T23:56:10.699994: step 11106, loss 0.562372.
Train: 2018-08-05T23:56:10.878519: step 11107, loss 0.571023.
Train: 2018-08-05T23:56:11.057767: step 11108, loss 0.588498.
Train: 2018-08-05T23:56:11.236640: step 11109, loss 0.545153.
Train: 2018-08-05T23:56:11.424086: step 11110, loss 0.579694.
Test: 2018-08-05T23:56:11.661204: step 11110, loss 0.547959.
Train: 2018-08-05T23:56:11.846660: step 11111, loss 0.588251.
Train: 2018-08-05T23:56:12.031152: step 11112, loss 0.553842.
Train: 2018-08-05T23:56:12.210239: step 11113, loss 0.613991.
Train: 2018-08-05T23:56:12.378524: step 11114, loss 0.493866.
Train: 2018-08-05T23:56:12.542286: step 11115, loss 0.622278.
Train: 2018-08-05T23:56:12.705830: step 11116, loss 0.528237.
Train: 2018-08-05T23:56:12.871758: step 11117, loss 0.613572.
Train: 2018-08-05T23:56:13.051886: step 11118, loss 0.528383.
Train: 2018-08-05T23:56:13.231305: step 11119, loss 0.545489.
Train: 2018-08-05T23:56:13.412888: step 11120, loss 0.553968.
Test: 2018-08-05T23:56:13.653308: step 11120, loss 0.548196.
Train: 2018-08-05T23:56:13.814353: step 11121, loss 0.536929.
Train: 2018-08-05T23:56:13.992806: step 11122, loss 0.553872.
Train: 2018-08-05T23:56:14.156322: step 11123, loss 0.562503.
Train: 2018-08-05T23:56:14.319787: step 11124, loss 0.579318.
Train: 2018-08-05T23:56:14.496870: step 11125, loss 0.48597.
Train: 2018-08-05T23:56:14.656984: step 11126, loss 0.553852.
Train: 2018-08-05T23:56:14.826810: step 11127, loss 0.545331.
Train: 2018-08-05T23:56:14.990259: step 11128, loss 0.502582.
Train: 2018-08-05T23:56:15.169348: step 11129, loss 0.596706.
Train: 2018-08-05T23:56:15.332550: step 11130, loss 0.588189.
Test: 2018-08-05T23:56:15.576523: step 11130, loss 0.547996.
Train: 2018-08-05T23:56:15.746068: step 11131, loss 0.545195.
Train: 2018-08-05T23:56:15.911445: step 11132, loss 0.519353.
Train: 2018-08-05T23:56:16.080888: step 11133, loss 0.562382.
Train: 2018-08-05T23:56:16.250279: step 11134, loss 0.545116.
Train: 2018-08-05T23:56:16.419805: step 11135, loss 0.597118.
Train: 2018-08-05T23:56:16.593348: step 11136, loss 0.54505.
Train: 2018-08-05T23:56:16.743050: step 11137, loss 0.588532.
Train: 2018-08-05T23:56:16.924140: step 11138, loss 0.623378.
Train: 2018-08-05T23:56:17.090618: step 11139, loss 0.49284.
Train: 2018-08-05T23:56:17.257516: step 11140, loss 0.588506.
Test: 2018-08-05T23:56:17.499244: step 11140, loss 0.547859.
Train: 2018-08-05T23:56:17.662449: step 11141, loss 0.545036.
Train: 2018-08-05T23:56:17.834286: step 11142, loss 0.562393.
Train: 2018-08-05T23:56:18.006090: step 11143, loss 0.536402.
Train: 2018-08-05T23:56:18.186760: step 11144, loss 0.571177.
Train: 2018-08-05T23:56:18.356493: step 11145, loss 0.571114.
Train: 2018-08-05T23:56:18.532295: step 11146, loss 0.553671.
Train: 2018-08-05T23:56:18.706892: step 11147, loss 0.545.
Train: 2018-08-05T23:56:18.863110: step 11148, loss 0.579863.
Train: 2018-08-05T23:56:19.043238: step 11149, loss 0.527612.
Train: 2018-08-05T23:56:19.245501: step 11150, loss 0.544972.
Test: 2018-08-05T23:56:19.471497: step 11150, loss 0.547819.
Train: 2018-08-05T23:56:19.649868: step 11151, loss 0.51882.
Train: 2018-08-05T23:56:19.806052: step 11152, loss 0.562471.
Train: 2018-08-05T23:56:19.985628: step 11153, loss 0.614946.
Train: 2018-08-05T23:56:20.156546: step 11154, loss 0.614955.
Train: 2018-08-05T23:56:20.320001: step 11155, loss 0.492538.
Train: 2018-08-05T23:56:20.483465: step 11156, loss 0.562399.
Train: 2018-08-05T23:56:20.639704: step 11157, loss 0.562448.
Train: 2018-08-05T23:56:20.818971: step 11158, loss 0.649987.
Train: 2018-08-05T23:56:20.997564: step 11159, loss 0.614801.
Train: 2018-08-05T23:56:21.161028: step 11160, loss 0.536325.
Test: 2018-08-05T23:56:21.393436: step 11160, loss 0.547868.
Train: 2018-08-05T23:56:21.572566: step 11161, loss 0.562436.
Train: 2018-08-05T23:56:21.736209: step 11162, loss 0.510372.
Train: 2018-08-05T23:56:21.908042: step 11163, loss 0.63172.
Train: 2018-08-05T23:56:22.073566: step 11164, loss 0.545102.
Train: 2018-08-05T23:56:22.243830: step 11165, loss 0.640067.
Train: 2018-08-05T23:56:22.407371: step 11166, loss 0.60544.
Train: 2018-08-05T23:56:22.571874: step 11167, loss 0.52808.
Train: 2018-08-05T23:56:22.743708: step 11168, loss 0.630859.
Train: 2018-08-05T23:56:22.922160: step 11169, loss 0.443075.
Train: 2018-08-05T23:56:23.085227: step 11170, loss 0.605009.
Test: 2018-08-05T23:56:23.334338: step 11170, loss 0.548173.
Train: 2018-08-05T23:56:23.554626: step 11171, loss 0.502869.
Train: 2018-08-05T23:56:23.713661: step 11172, loss 0.613409.
Train: 2018-08-05T23:56:23.876685: step 11173, loss 0.545431.
Train: 2018-08-05T23:56:24.027205: step 11174, loss 0.562507.
Train: 2018-08-05T23:56:24.214232: step 11175, loss 0.587908.
Train: 2018-08-05T23:56:24.377616: step 11176, loss 0.579318.
Train: 2018-08-05T23:56:24.541209: step 11177, loss 0.562439.
Train: 2018-08-05T23:56:24.703977: step 11178, loss 0.537025.
Train: 2018-08-05T23:56:24.867651: step 11179, loss 0.54552.
Train: 2018-08-05T23:56:25.040027: step 11180, loss 0.680694.
Test: 2018-08-05T23:56:25.280912: step 11180, loss 0.548315.
Train: 2018-08-05T23:56:25.438937: step 11181, loss 0.520336.
Train: 2018-08-05T23:56:25.618936: step 11182, loss 0.570856.
Train: 2018-08-05T23:56:25.796244: step 11183, loss 0.562471.
Train: 2018-08-05T23:56:25.965353: step 11184, loss 0.604445.
Train: 2018-08-05T23:56:26.141250: step 11185, loss 0.570886.
Train: 2018-08-05T23:56:26.316110: step 11186, loss 0.637826.
Train: 2018-08-05T23:56:26.483693: step 11187, loss 0.529109.
Train: 2018-08-05T23:56:26.638620: step 11188, loss 0.570816.
Train: 2018-08-05T23:56:26.810454: step 11189, loss 0.512593.
Train: 2018-08-05T23:56:26.989468: step 11190, loss 0.54592.
Test: 2018-08-05T23:56:27.217525: step 11190, loss 0.548562.
Train: 2018-08-05T23:56:27.396852: step 11191, loss 0.662327.
Train: 2018-08-05T23:56:27.575947: step 11192, loss 0.545832.
Train: 2018-08-05T23:56:27.729795: step 11193, loss 0.554242.
Train: 2018-08-05T23:56:27.909947: step 11194, loss 0.612301.
Train: 2018-08-05T23:56:28.081813: step 11195, loss 0.55422.
Train: 2018-08-05T23:56:28.245490: step 11196, loss 0.579157.
Train: 2018-08-05T23:56:28.424590: step 11197, loss 0.529503.
Train: 2018-08-05T23:56:28.588093: step 11198, loss 0.587322.
Train: 2018-08-05T23:56:28.758109: step 11199, loss 0.587331.
Train: 2018-08-05T23:56:28.929944: step 11200, loss 0.579093.
Test: 2018-08-05T23:56:29.163414: step 11200, loss 0.548769.
Train: 2018-08-05T23:56:29.865801: step 11201, loss 0.513165.
Train: 2018-08-05T23:56:30.045206: step 11202, loss 0.513141.
Train: 2018-08-05T23:56:30.225768: step 11203, loss 0.5625.
Train: 2018-08-05T23:56:30.381982: step 11204, loss 0.570895.
Train: 2018-08-05T23:56:30.547281: step 11205, loss 0.537662.
Train: 2018-08-05T23:56:30.725790: step 11206, loss 0.54582.
Train: 2018-08-05T23:56:30.888605: step 11207, loss 0.545783.
Train: 2018-08-05T23:56:31.052442: step 11208, loss 0.504029.
Train: 2018-08-05T23:56:31.230028: step 11209, loss 0.562476.
Train: 2018-08-05T23:56:31.417485: step 11210, loss 0.511931.
Test: 2018-08-05T23:56:31.643544: step 11210, loss 0.548269.
Train: 2018-08-05T23:56:31.841234: step 11211, loss 0.545533.
Train: 2018-08-05T23:56:32.000604: step 11212, loss 0.545406.
Train: 2018-08-05T23:56:32.170305: step 11213, loss 0.596563.
Train: 2018-08-05T23:56:32.324316: step 11214, loss 0.588035.
Train: 2018-08-05T23:56:32.499200: step 11215, loss 0.605273.
Train: 2018-08-05T23:56:32.662655: step 11216, loss 0.459401.
Train: 2018-08-05T23:56:32.825887: step 11217, loss 0.605518.
Train: 2018-08-05T23:56:33.002210: step 11218, loss 0.527917.
Train: 2018-08-05T23:56:33.167735: step 11219, loss 0.536488.
Train: 2018-08-05T23:56:33.347177: step 11220, loss 0.562405.
Test: 2018-08-05T23:56:33.585512: step 11220, loss 0.547857.
Train: 2018-08-05T23:56:33.750721: step 11221, loss 0.492872.
Train: 2018-08-05T23:56:33.915352: step 11222, loss 0.544936.
Train: 2018-08-05T23:56:34.087179: step 11223, loss 0.553687.
Train: 2018-08-05T23:56:34.250805: step 11224, loss 0.544917.
Train: 2018-08-05T23:56:34.430000: step 11225, loss 0.641845.
Train: 2018-08-05T23:56:34.593307: step 11226, loss 0.509587.
Train: 2018-08-05T23:56:34.770187: step 11227, loss 0.49177.
Train: 2018-08-05T23:56:34.926401: step 11228, loss 0.509362.
Train: 2018-08-05T23:56:35.097004: step 11229, loss 0.562584.
Train: 2018-08-05T23:56:35.278245: step 11230, loss 0.589323.
Test: 2018-08-05T23:56:35.516980: step 11230, loss 0.547627.
Train: 2018-08-05T23:56:35.680328: step 11231, loss 0.598365.
Train: 2018-08-05T23:56:35.857094: step 11232, loss 0.553681.
Train: 2018-08-05T23:56:36.020975: step 11233, loss 0.553712.
Train: 2018-08-05T23:56:36.196444: step 11234, loss 0.517779.
Train: 2018-08-05T23:56:36.374965: step 11235, loss 0.553614.
Train: 2018-08-05T23:56:36.544538: step 11236, loss 0.643554.
Train: 2018-08-05T23:56:36.704845: step 11237, loss 0.58058.
Train: 2018-08-05T23:56:36.867702: step 11238, loss 0.580557.
Train: 2018-08-05T23:56:37.038849: step 11239, loss 0.52681.
Train: 2018-08-05T23:56:37.212183: step 11240, loss 0.607257.
Test: 2018-08-05T23:56:37.449480: step 11240, loss 0.547639.
Train: 2018-08-05T23:56:37.618692: step 11241, loss 0.580424.
Train: 2018-08-05T23:56:37.785047: step 11242, loss 0.535851.
Train: 2018-08-05T23:56:37.948947: step 11243, loss 0.615865.
Train: 2018-08-05T23:56:38.120758: step 11244, loss 0.562489.
Train: 2018-08-05T23:56:38.294111: step 11245, loss 0.624401.
Train: 2018-08-05T23:56:38.461637: step 11246, loss 0.483209.
Train: 2018-08-05T23:56:38.631061: step 11247, loss 0.641614.
Train: 2018-08-05T23:56:38.812097: step 11248, loss 0.597521.
Train: 2018-08-05T23:56:38.978361: step 11249, loss 0.518838.
Train: 2018-08-05T23:56:39.134572: step 11250, loss 0.553704.
Test: 2018-08-05T23:56:39.383769: step 11250, loss 0.547879.
Train: 2018-08-05T23:56:39.547413: step 11251, loss 0.562361.
Train: 2018-08-05T23:56:39.711881: step 11252, loss 0.553777.
Train: 2018-08-05T23:56:39.876451: step 11253, loss 0.527798.
Train: 2018-08-05T23:56:40.048286: step 11254, loss 0.553769.
Train: 2018-08-05T23:56:40.212658: step 11255, loss 0.588311.
Train: 2018-08-05T23:56:40.382253: step 11256, loss 0.571017.
Train: 2018-08-05T23:56:40.561399: step 11257, loss 0.665718.
Train: 2018-08-05T23:56:40.726434: step 11258, loss 0.613858.
Train: 2018-08-05T23:56:40.896191: step 11259, loss 0.622195.
Train: 2018-08-05T23:56:41.061503: step 11260, loss 0.570928.
Test: 2018-08-05T23:56:41.306743: step 11260, loss 0.548244.
Train: 2018-08-05T23:56:41.468812: step 11261, loss 0.570906.
Train: 2018-08-05T23:56:41.639522: step 11262, loss 0.562425.
Train: 2018-08-05T23:56:41.818020: step 11263, loss 0.545658.
Train: 2018-08-05T23:56:41.981619: step 11264, loss 0.562483.
Train: 2018-08-05T23:56:42.160589: step 11265, loss 0.579174.
Train: 2018-08-05T23:56:42.316833: step 11266, loss 0.562502.
Train: 2018-08-05T23:56:42.492371: step 11267, loss 0.595758.
Train: 2018-08-05T23:56:42.664236: step 11268, loss 0.570818.
Train: 2018-08-05T23:56:42.818433: step 11269, loss 0.587342.
Train: 2018-08-05T23:56:42.987355: step 11270, loss 0.513038.
Test: 2018-08-05T23:56:43.235640: step 11270, loss 0.548765.
Train: 2018-08-05T23:56:43.408208: step 11271, loss 0.537845.
Train: 2018-08-05T23:56:43.582743: step 11272, loss 0.595543.
Train: 2018-08-05T23:56:43.742298: step 11273, loss 0.562567.
Train: 2018-08-05T23:56:43.905937: step 11274, loss 0.579023.
Train: 2018-08-05T23:56:44.085709: step 11275, loss 0.562606.
Train: 2018-08-05T23:56:44.249173: step 11276, loss 0.611964.
Train: 2018-08-05T23:56:44.412663: step 11277, loss 0.595507.
Train: 2018-08-05T23:56:44.584532: step 11278, loss 0.480488.
Train: 2018-08-05T23:56:44.740797: step 11279, loss 0.587257.
Train: 2018-08-05T23:56:44.925853: step 11280, loss 0.57903.
Test: 2018-08-05T23:56:45.162516: step 11280, loss 0.548839.
Train: 2018-08-05T23:56:45.325366: step 11281, loss 0.529729.
Train: 2018-08-05T23:56:45.491559: step 11282, loss 0.537933.
Train: 2018-08-05T23:56:45.669991: step 11283, loss 0.603782.
Train: 2018-08-05T23:56:45.826238: step 11284, loss 0.595571.
Train: 2018-08-05T23:56:45.989179: step 11285, loss 0.554324.
Train: 2018-08-05T23:56:46.152038: step 11286, loss 0.471805.
Train: 2018-08-05T23:56:46.334051: step 11287, loss 0.545966.
Train: 2018-08-05T23:56:46.532532: step 11288, loss 0.496143.
Train: 2018-08-05T23:56:46.685489: step 11289, loss 0.462426.
Train: 2018-08-05T23:56:46.866026: step 11290, loss 0.570856.
Test: 2018-08-05T23:56:47.100345: step 11290, loss 0.548309.
Train: 2018-08-05T23:56:47.270161: step 11291, loss 0.579279.
Train: 2018-08-05T23:56:47.441997: step 11292, loss 0.664102.
Train: 2018-08-05T23:56:47.616882: step 11293, loss 0.52848.
Train: 2018-08-05T23:56:47.779655: step 11294, loss 0.58791.
Train: 2018-08-05T23:56:47.951490: step 11295, loss 0.468624.
Train: 2018-08-05T23:56:48.117563: step 11296, loss 0.57098.
Train: 2018-08-05T23:56:48.305237: step 11297, loss 0.553846.
Train: 2018-08-05T23:56:48.468537: step 11298, loss 0.493479.
Train: 2018-08-05T23:56:48.632083: step 11299, loss 0.562417.
Train: 2018-08-05T23:56:48.795590: step 11300, loss 0.466985.
Test: 2018-08-05T23:56:49.044756: step 11300, loss 0.547826.
Train: 2018-08-05T23:56:49.774917: step 11301, loss 0.579882.
Train: 2018-08-05T23:56:49.947858: step 11302, loss 0.518652.
Train: 2018-08-05T23:56:50.119639: step 11303, loss 0.588847.
Train: 2018-08-05T23:56:50.282471: step 11304, loss 0.535988.
Train: 2018-08-05T23:56:50.445933: step 11305, loss 0.544862.
Train: 2018-08-05T23:56:50.625049: step 11306, loss 0.526978.
Train: 2018-08-05T23:56:50.796885: step 11307, loss 0.526924.
Train: 2018-08-05T23:56:50.960439: step 11308, loss 0.589489.
Train: 2018-08-05T23:56:51.131498: step 11309, loss 0.580528.
Train: 2018-08-05T23:56:51.294002: step 11310, loss 0.535711.
Test: 2018-08-05T23:56:51.535754: step 11310, loss 0.547597.
Train: 2018-08-05T23:56:51.714876: step 11311, loss 0.508635.
Train: 2018-08-05T23:56:51.894569: step 11312, loss 0.598794.
Train: 2018-08-05T23:56:52.052007: step 11313, loss 0.517558.
Train: 2018-08-05T23:56:52.222155: step 11314, loss 0.517492.
Train: 2018-08-05T23:56:52.400819: step 11315, loss 0.617303.
Train: 2018-08-05T23:56:52.561475: step 11316, loss 0.635463.
Train: 2018-08-05T23:56:52.724658: step 11317, loss 0.626263.
Train: 2018-08-05T23:56:52.905925: step 11318, loss 0.55369.
Train: 2018-08-05T23:56:53.062169: step 11319, loss 0.571711.
Train: 2018-08-05T23:56:53.247454: step 11320, loss 0.544625.
Test: 2018-08-05T23:56:53.485817: step 11320, loss 0.547602.
Train: 2018-08-05T23:56:53.651888: step 11321, loss 0.625703.
Train: 2018-08-05T23:56:53.814464: step 11322, loss 0.526823.
Train: 2018-08-05T23:56:53.993588: step 11323, loss 0.499984.
Train: 2018-08-05T23:56:54.157114: step 11324, loss 0.553675.
Train: 2018-08-05T23:56:54.328948: step 11325, loss 0.695702.
Train: 2018-08-05T23:56:54.500024: step 11326, loss 0.562559.
Train: 2018-08-05T23:56:54.662919: step 11327, loss 0.482837.
Train: 2018-08-05T23:56:54.826431: step 11328, loss 0.562485.
Train: 2018-08-05T23:56:55.005619: step 11329, loss 0.491894.
Train: 2018-08-05T23:56:55.181668: step 11330, loss 0.571305.
Test: 2018-08-05T23:56:55.412562: step 11330, loss 0.547721.
Train: 2018-08-05T23:56:55.594300: step 11331, loss 0.588894.
Train: 2018-08-05T23:56:55.757323: step 11332, loss 0.615269.
Train: 2018-08-05T23:56:55.918039: step 11333, loss 0.492222.
Train: 2018-08-05T23:56:56.096904: step 11334, loss 0.536127.
Train: 2018-08-05T23:56:56.267535: step 11335, loss 0.553657.
Train: 2018-08-05T23:56:56.437088: step 11336, loss 0.588756.
Train: 2018-08-05T23:56:56.613624: step 11337, loss 0.553669.
Train: 2018-08-05T23:56:56.773484: step 11338, loss 0.448445.
Train: 2018-08-05T23:56:56.952290: step 11339, loss 0.562459.
Train: 2018-08-05T23:56:57.117496: step 11340, loss 0.624003.
Test: 2018-08-05T23:56:57.358908: step 11340, loss 0.547746.
Train: 2018-08-05T23:56:57.532389: step 11341, loss 0.492125.
Train: 2018-08-05T23:56:57.704226: step 11342, loss 0.562517.
Train: 2018-08-05T23:56:57.874003: step 11343, loss 0.597713.
Train: 2018-08-05T23:56:58.042869: step 11344, loss 0.553734.
Train: 2018-08-05T23:56:58.214739: step 11345, loss 0.562483.
Train: 2018-08-05T23:56:58.390637: step 11346, loss 0.668224.
Train: 2018-08-05T23:56:58.538518: step 11347, loss 0.597647.
Train: 2018-08-05T23:56:58.717636: step 11348, loss 0.55369.
Train: 2018-08-05T23:56:58.881235: step 11349, loss 0.614883.
Train: 2018-08-05T23:56:59.053071: step 11350, loss 0.527611.
Test: 2018-08-05T23:56:59.304095: step 11350, loss 0.547862.
Train: 2018-08-05T23:56:59.475929: step 11351, loss 0.579791.
Train: 2018-08-05T23:56:59.640205: step 11352, loss 0.536423.
Train: 2018-08-05T23:56:59.819902: step 11353, loss 0.536438.
Train: 2018-08-05T23:56:59.988649: step 11354, loss 0.519172.
Train: 2018-08-05T23:57:00.167442: step 11355, loss 0.596992.
Train: 2018-08-05T23:57:00.338176: step 11356, loss 0.545172.
Train: 2018-08-05T23:57:00.502580: step 11357, loss 0.579675.
Train: 2018-08-05T23:57:00.669871: step 11358, loss 0.5365.
Train: 2018-08-05T23:57:00.832674: step 11359, loss 0.631375.
Train: 2018-08-05T23:57:00.996528: step 11360, loss 0.51074.
Test: 2018-08-05T23:57:01.243975: step 11360, loss 0.547998.
Train: 2018-08-05T23:57:01.407677: step 11361, loss 0.553759.
Train: 2018-08-05T23:57:01.590197: step 11362, loss 0.519459.
Train: 2018-08-05T23:57:01.758486: step 11363, loss 0.596792.
Train: 2018-08-05T23:57:01.917478: step 11364, loss 0.545172.
Train: 2018-08-05T23:57:02.089312: step 11365, loss 0.527979.
Train: 2018-08-05T23:57:02.259243: step 11366, loss 0.579594.
Train: 2018-08-05T23:57:02.414392: step 11367, loss 0.648573.
Train: 2018-08-05T23:57:02.598053: step 11368, loss 0.562428.
Train: 2018-08-05T23:57:02.761430: step 11369, loss 0.562435.
Train: 2018-08-05T23:57:02.933230: step 11370, loss 0.52811.
Test: 2018-08-05T23:57:03.159376: step 11370, loss 0.548032.
Train: 2018-08-05T23:57:03.353100: step 11371, loss 0.545183.
Train: 2018-08-05T23:57:03.527660: step 11372, loss 0.656758.
Train: 2018-08-05T23:57:03.679961: step 11373, loss 0.57956.
Train: 2018-08-05T23:57:03.859468: step 11374, loss 0.579483.
Train: 2018-08-05T23:57:04.035727: step 11375, loss 0.545371.
Train: 2018-08-05T23:57:04.187741: step 11376, loss 0.460268.
Train: 2018-08-05T23:57:04.350664: step 11377, loss 0.553901.
Train: 2018-08-05T23:57:04.514207: step 11378, loss 0.536847.
Train: 2018-08-05T23:57:04.694082: step 11379, loss 0.511127.
Train: 2018-08-05T23:57:04.850265: step 11380, loss 0.545297.
Test: 2018-08-05T23:57:05.103110: step 11380, loss 0.548032.
Train: 2018-08-05T23:57:05.257456: step 11381, loss 0.562425.
Train: 2018-08-05T23:57:05.441808: step 11382, loss 0.545212.
Train: 2018-08-05T23:57:05.598047: step 11383, loss 0.536527.
Train: 2018-08-05T23:57:05.774012: step 11384, loss 0.501908.
Train: 2018-08-05T23:57:05.937487: step 11385, loss 0.527772.
Train: 2018-08-05T23:57:06.116662: step 11386, loss 0.536287.
Train: 2018-08-05T23:57:06.289048: step 11387, loss 0.606082.
Train: 2018-08-05T23:57:06.471591: step 11388, loss 0.518624.
Train: 2018-08-05T23:57:06.639846: step 11389, loss 0.509748.
Train: 2018-08-05T23:57:06.809283: step 11390, loss 0.535994.
Test: 2018-08-05T23:57:07.040319: step 11390, loss 0.54769.
Train: 2018-08-05T23:57:07.219677: step 11391, loss 0.509397.
Train: 2018-08-05T23:57:07.383154: step 11392, loss 0.544871.
Train: 2018-08-05T23:57:07.546635: step 11393, loss 0.544792.
Train: 2018-08-05T23:57:07.725670: step 11394, loss 0.643231.
Train: 2018-08-05T23:57:07.897504: step 11395, loss 0.553634.
Train: 2018-08-05T23:57:08.072830: step 11396, loss 0.526783.
Train: 2018-08-05T23:57:08.242613: step 11397, loss 0.553719.
Train: 2018-08-05T23:57:08.405541: step 11398, loss 0.526684.
Train: 2018-08-05T23:57:08.584855: step 11399, loss 0.499482.
Train: 2018-08-05T23:57:08.758629: step 11400, loss 0.535562.
Test: 2018-08-05T23:57:08.982575: step 11400, loss 0.547589.
Train: 2018-08-05T23:57:09.728482: step 11401, loss 0.562788.
Train: 2018-08-05T23:57:09.900317: step 11402, loss 0.617387.
Train: 2018-08-05T23:57:10.063951: step 11403, loss 0.644675.
Train: 2018-08-05T23:57:10.243077: step 11404, loss 0.59913.
Train: 2018-08-05T23:57:10.414877: step 11405, loss 0.571781.
Train: 2018-08-05T23:57:10.578539: step 11406, loss 0.580907.
Train: 2018-08-05T23:57:10.734782: step 11407, loss 0.508623.
Train: 2018-08-05T23:57:10.913295: step 11408, loss 0.643836.
Train: 2018-08-05T23:57:11.080737: step 11409, loss 0.508723.
Train: 2018-08-05T23:57:11.245463: step 11410, loss 0.562626.
Test: 2018-08-05T23:57:11.486639: step 11410, loss 0.547636.
Train: 2018-08-05T23:57:11.665122: step 11411, loss 0.616217.
Train: 2018-08-05T23:57:11.836127: step 11412, loss 0.571513.
Train: 2018-08-05T23:57:12.000755: step 11413, loss 0.598035.
Train: 2018-08-05T23:57:12.164504: step 11414, loss 0.57139.
Train: 2018-08-05T23:57:12.338009: step 11415, loss 0.509553.
Train: 2018-08-05T23:57:12.509838: step 11416, loss 0.580119.
Train: 2018-08-05T23:57:12.679637: step 11417, loss 0.53615.
Train: 2018-08-05T23:57:12.852673: step 11418, loss 0.509913.
Train: 2018-08-05T23:57:13.023420: step 11419, loss 0.614928.
Train: 2018-08-05T23:57:13.196533: step 11420, loss 0.684768.
Test: 2018-08-05T23:57:13.436892: step 11420, loss 0.547863.
Train: 2018-08-05T23:57:13.608432: step 11421, loss 0.614655.
Train: 2018-08-05T23:57:13.764454: step 11422, loss 0.588378.
Train: 2018-08-05T23:57:13.944245: step 11423, loss 0.571029.
Train: 2018-08-05T23:57:14.108776: step 11424, loss 0.57954.
Train: 2018-08-05T23:57:14.288930: step 11425, loss 0.570941.
Train: 2018-08-05T23:57:14.460734: step 11426, loss 0.587891.
Train: 2018-08-05T23:57:14.632587: step 11427, loss 0.596251.
Train: 2018-08-05T23:57:14.801690: step 11428, loss 0.554048.
Train: 2018-08-05T23:57:14.974733: step 11429, loss 0.621158.
Train: 2018-08-05T23:57:15.137568: step 11430, loss 0.570823.
Test: 2018-08-05T23:57:15.381933: step 11430, loss 0.548619.
Train: 2018-08-05T23:57:15.544840: step 11431, loss 0.595766.
Train: 2018-08-05T23:57:15.728851: step 11432, loss 0.52123.
Train: 2018-08-05T23:57:15.895184: step 11433, loss 0.562667.
Train: 2018-08-05T23:57:16.058151: step 11434, loss 0.570875.
Train: 2018-08-05T23:57:16.246527: step 11435, loss 0.579021.
Train: 2018-08-05T23:57:16.417071: step 11436, loss 0.587209.
Train: 2018-08-05T23:57:16.587645: step 11437, loss 0.538139.
Train: 2018-08-05T23:57:16.757791: step 11438, loss 0.627922.
Train: 2018-08-05T23:57:16.927433: step 11439, loss 0.595227.
Train: 2018-08-05T23:57:17.099111: step 11440, loss 0.530267.
Test: 2018-08-05T23:57:17.331654: step 11440, loss 0.549185.
Train: 2018-08-05T23:57:17.510361: step 11441, loss 0.514171.
Train: 2018-08-05T23:57:17.676287: step 11442, loss 0.61949.
Train: 2018-08-05T23:57:17.855731: step 11443, loss 0.538443.
Train: 2018-08-05T23:57:18.018610: step 11444, loss 0.611332.
Train: 2018-08-05T23:57:18.190445: step 11445, loss 0.570848.
Train: 2018-08-05T23:57:18.369176: step 11446, loss 0.546605.
Train: 2018-08-05T23:57:18.531911: step 11447, loss 0.514218.
Train: 2018-08-05T23:57:18.695061: step 11448, loss 0.603317.
Train: 2018-08-05T23:57:18.878014: step 11449, loss 0.554669.
Train: 2018-08-05T23:57:19.043395: step 11450, loss 0.538468.
Test: 2018-08-05T23:57:19.275738: step 11450, loss 0.549097.
Train: 2018-08-05T23:57:19.460282: step 11451, loss 0.595227.
Train: 2018-08-05T23:57:19.616463: step 11452, loss 0.587041.
Train: 2018-08-05T23:57:19.792054: step 11453, loss 0.505676.
Train: 2018-08-05T23:57:19.955785: step 11454, loss 0.521828.
Train: 2018-08-05T23:57:20.120292: step 11455, loss 0.562621.
Train: 2018-08-05T23:57:20.299463: step 11456, loss 0.58725.
Train: 2018-08-05T23:57:20.494203: step 11457, loss 0.521396.
Train: 2018-08-05T23:57:20.666038: step 11458, loss 0.579105.
Train: 2018-08-05T23:57:20.829613: step 11459, loss 0.52105.
Train: 2018-08-05T23:57:21.009094: step 11460, loss 0.470986.
Test: 2018-08-05T23:57:21.242425: step 11460, loss 0.548459.
Train: 2018-08-05T23:57:21.421588: step 11461, loss 0.537383.
Train: 2018-08-05T23:57:21.600859: step 11462, loss 0.554026.
Train: 2018-08-05T23:57:21.764378: step 11463, loss 0.604736.
Train: 2018-08-05T23:57:21.942501: step 11464, loss 0.579394.
Train: 2018-08-05T23:57:22.099829: step 11465, loss 0.48575.
Train: 2018-08-05T23:57:22.269888: step 11466, loss 0.562427.
Train: 2018-08-05T23:57:22.432449: step 11467, loss 0.596777.
Train: 2018-08-05T23:57:22.612658: step 11468, loss 0.579648.
Train: 2018-08-05T23:57:22.776158: step 11469, loss 0.579649.
Train: 2018-08-05T23:57:22.948023: step 11470, loss 0.536454.
Test: 2018-08-05T23:57:23.197610: step 11470, loss 0.547906.
Train: 2018-08-05T23:57:23.372142: step 11471, loss 0.579751.
Train: 2018-08-05T23:57:23.542687: step 11472, loss 0.605775.
Train: 2018-08-05T23:57:23.707282: step 11473, loss 0.519025.
Train: 2018-08-05T23:57:23.870067: step 11474, loss 0.588467.
Train: 2018-08-05T23:57:24.036217: step 11475, loss 0.588476.
Train: 2018-08-05T23:57:24.209316: step 11476, loss 0.580997.
Train: 2018-08-05T23:57:24.370970: step 11477, loss 0.518979.
Train: 2018-08-05T23:57:24.549552: step 11478, loss 0.571101.
Train: 2018-08-05T23:57:24.712433: step 11479, loss 0.605851.
Train: 2018-08-05T23:57:24.875981: step 11480, loss 0.545068.
Test: 2018-08-05T23:57:25.117297: step 11480, loss 0.547897.
Train: 2018-08-05T23:57:25.297306: step 11481, loss 0.527746.
Train: 2018-08-05T23:57:25.463668: step 11482, loss 0.553737.
Train: 2018-08-05T23:57:25.645030: step 11483, loss 0.553752.
Train: 2018-08-05T23:57:25.805349: step 11484, loss 0.588392.
Train: 2018-08-05T23:57:25.990011: step 11485, loss 0.623134.
Train: 2018-08-05T23:57:26.165629: step 11486, loss 0.562443.
Train: 2018-08-05T23:57:26.344737: step 11487, loss 0.519183.
Train: 2018-08-05T23:57:26.519258: step 11488, loss 0.493255.
Train: 2018-08-05T23:57:26.683582: step 11489, loss 0.579702.
Train: 2018-08-05T23:57:26.860301: step 11490, loss 0.597054.
Test: 2018-08-05T23:57:27.089453: step 11490, loss 0.547918.
Train: 2018-08-05T23:57:27.261312: step 11491, loss 0.562409.
Train: 2018-08-05T23:57:27.435631: step 11492, loss 0.562424.
Train: 2018-08-05T23:57:27.605334: step 11493, loss 0.579724.
Train: 2018-08-05T23:57:27.768515: step 11494, loss 0.49327.
Train: 2018-08-05T23:57:27.947827: step 11495, loss 0.571051.
Train: 2018-08-05T23:57:28.124518: step 11496, loss 0.536452.
Train: 2018-08-05T23:57:28.282831: step 11497, loss 0.579734.
Train: 2018-08-05T23:57:28.454666: step 11498, loss 0.597096.
Train: 2018-08-05T23:57:28.627473: step 11499, loss 0.536422.
Train: 2018-08-05T23:57:28.796384: step 11500, loss 0.467117.
Test: 2018-08-05T23:57:29.043811: step 11500, loss 0.54788.
Train: 2018-08-05T23:57:29.770032: step 11501, loss 0.545047.
Train: 2018-08-05T23:57:29.950183: step 11502, loss 0.510244.
Train: 2018-08-05T23:57:30.121987: step 11503, loss 0.510102.
Train: 2018-08-05T23:57:30.301158: step 11504, loss 0.544938.
Train: 2018-08-05T23:57:30.463979: step 11505, loss 0.527294.
Train: 2018-08-05T23:57:30.643164: step 11506, loss 0.544844.
Train: 2018-08-05T23:57:30.813909: step 11507, loss 0.606791.
Train: 2018-08-05T23:57:30.994103: step 11508, loss 0.606867.
Train: 2018-08-05T23:57:31.165908: step 11509, loss 0.527027.
Train: 2018-08-05T23:57:31.329432: step 11510, loss 0.598075.
Test: 2018-08-05T23:57:31.571184: step 11510, loss 0.54766.
Train: 2018-08-05T23:57:31.741406: step 11511, loss 0.562573.
Train: 2018-08-05T23:57:31.930133: step 11512, loss 0.56255.
Train: 2018-08-05T23:57:32.088653: step 11513, loss 0.526989.
Train: 2018-08-05T23:57:32.267192: step 11514, loss 0.58036.
Train: 2018-08-05T23:57:32.437587: step 11515, loss 0.553673.
Train: 2018-08-05T23:57:32.605293: step 11516, loss 0.580366.
Train: 2018-08-05T23:57:32.768252: step 11517, loss 0.642617.
Train: 2018-08-05T23:57:32.946742: step 11518, loss 0.651285.
Train: 2018-08-05T23:57:33.109577: step 11519, loss 0.509447.
Train: 2018-08-05T23:57:33.300545: step 11520, loss 0.588918.
Test: 2018-08-05T23:57:33.537910: step 11520, loss 0.547751.
Train: 2018-08-05T23:57:33.710577: step 11521, loss 0.518508.
Train: 2018-08-05T23:57:33.882381: step 11522, loss 0.562485.
Train: 2018-08-05T23:57:34.046912: step 11523, loss 0.7288.
Train: 2018-08-05T23:57:34.218746: step 11524, loss 0.588573.
Train: 2018-08-05T23:57:34.387991: step 11525, loss 0.614421.
Train: 2018-08-05T23:57:34.555468: step 11526, loss 0.553799.
Train: 2018-08-05T23:57:34.727962: step 11527, loss 0.502412.
Train: 2018-08-05T23:57:34.897107: step 11528, loss 0.579489.
Train: 2018-08-05T23:57:35.072527: step 11529, loss 0.519842.
Train: 2018-08-05T23:57:35.232467: step 11530, loss 0.562416.
Test: 2018-08-05T23:57:35.473766: step 11530, loss 0.548217.
Train: 2018-08-05T23:57:35.639708: step 11531, loss 0.579376.
Train: 2018-08-05T23:57:35.818183: step 11532, loss 0.596282.
Train: 2018-08-05T23:57:35.997633: step 11533, loss 0.613088.
Train: 2018-08-05T23:57:36.167116: step 11534, loss 0.621349.
Train: 2018-08-05T23:57:36.346191: step 11535, loss 0.512191.
Train: 2018-08-05T23:57:36.518730: step 11536, loss 0.570844.
Train: 2018-08-05T23:57:36.688475: step 11537, loss 0.545802.
Train: 2018-08-05T23:57:36.851967: step 11538, loss 0.587487.
Train: 2018-08-05T23:57:37.014932: step 11539, loss 0.595751.
Train: 2018-08-05T23:57:37.184266: step 11540, loss 0.554241.
Test: 2018-08-05T23:57:37.433588: step 11540, loss 0.548674.
Train: 2018-08-05T23:57:37.596434: step 11541, loss 0.554263.
Train: 2018-08-05T23:57:37.774932: step 11542, loss 0.537739.
Train: 2018-08-05T23:57:37.946796: step 11543, loss 0.504679.
Train: 2018-08-05T23:57:38.117117: step 11544, loss 0.545985.
Train: 2018-08-05T23:57:38.295606: step 11545, loss 0.554269.
Train: 2018-08-05T23:57:38.459125: step 11546, loss 0.479472.
Train: 2018-08-05T23:57:38.622456: step 11547, loss 0.562478.
Train: 2018-08-05T23:57:38.809918: step 11548, loss 0.512279.
Train: 2018-08-05T23:57:38.983538: step 11549, loss 0.596043.
Train: 2018-08-05T23:57:39.160561: step 11550, loss 0.528722.
Test: 2018-08-05T23:57:39.386748: step 11550, loss 0.54826.
Train: 2018-08-05T23:57:39.574409: step 11551, loss 0.587795.
Train: 2018-08-05T23:57:39.737112: step 11552, loss 0.570917.
Train: 2018-08-05T23:57:39.898922: step 11553, loss 0.519906.
Train: 2018-08-05T23:57:40.078023: step 11554, loss 0.553875.
Train: 2018-08-05T23:57:40.241554: step 11555, loss 0.511079.
Train: 2018-08-05T23:57:40.421716: step 11556, loss 0.605326.
Train: 2018-08-05T23:57:40.593581: step 11557, loss 0.61404.
Train: 2018-08-05T23:57:40.757406: step 11558, loss 0.510687.
Train: 2018-08-05T23:57:40.936556: step 11559, loss 0.588331.
Train: 2018-08-05T23:57:41.101004: step 11560, loss 0.571065.
Test: 2018-08-05T23:57:41.333423: step 11560, loss 0.547916.
Train: 2018-08-05T23:57:41.512485: step 11561, loss 0.553746.
Train: 2018-08-05T23:57:41.676993: step 11562, loss 0.527746.
Train: 2018-08-05T23:57:41.848858: step 11563, loss 0.571094.
Train: 2018-08-05T23:57:42.028693: step 11564, loss 0.571092.
Train: 2018-08-05T23:57:42.198889: step 11565, loss 0.553727.
Train: 2018-08-05T23:57:42.362363: step 11566, loss 0.579832.
Train: 2018-08-05T23:57:42.542255: step 11567, loss 0.562436.
Train: 2018-08-05T23:57:42.716013: step 11568, loss 0.649514.
Train: 2018-08-05T23:57:42.888737: step 11569, loss 0.614603.
Train: 2018-08-05T23:57:43.065420: step 11570, loss 0.510367.
Test: 2018-08-05T23:57:43.309980: step 11570, loss 0.54791.
Train: 2018-08-05T23:57:43.484538: step 11571, loss 0.553743.
Train: 2018-08-05T23:57:43.639197: step 11572, loss 0.545131.
Train: 2018-08-05T23:57:43.802577: step 11573, loss 0.493248.
Train: 2018-08-05T23:57:43.985074: step 11574, loss 0.571049.
Train: 2018-08-05T23:57:44.159437: step 11575, loss 0.562403.
Train: 2018-08-05T23:57:44.319802: step 11576, loss 0.571052.
Train: 2018-08-05T23:57:44.505875: step 11577, loss 0.51044.
Train: 2018-08-05T23:57:44.668903: step 11578, loss 0.571067.
Train: 2018-08-05T23:57:44.848110: step 11579, loss 0.501669.
Train: 2018-08-05T23:57:45.026940: step 11580, loss 0.6059.
Test: 2018-08-05T23:57:45.263027: step 11580, loss 0.547851.
Train: 2018-08-05T23:57:45.434825: step 11581, loss 0.562433.
Train: 2018-08-05T23:57:45.604641: step 11582, loss 0.579868.
Train: 2018-08-05T23:57:45.773449: step 11583, loss 0.553711.
Train: 2018-08-05T23:57:45.945315: step 11584, loss 0.562435.
Train: 2018-08-05T23:57:46.105294: step 11585, loss 0.597264.
Train: 2018-08-05T23:57:46.293080: step 11586, loss 0.64073.
Train: 2018-08-05T23:57:46.461628: step 11587, loss 0.536325.
Train: 2018-08-05T23:57:46.632206: step 11588, loss 0.623122.
Train: 2018-08-05T23:57:46.804123: step 11589, loss 0.536431.
Train: 2018-08-05T23:57:46.961441: step 11590, loss 0.640096.
Test: 2018-08-05T23:57:47.203442: step 11590, loss 0.547991.
Train: 2018-08-05T23:57:47.381929: step 11591, loss 0.588205.
Train: 2018-08-05T23:57:47.552326: step 11592, loss 0.570989.
Train: 2018-08-05T23:57:47.715735: step 11593, loss 0.519655.
Train: 2018-08-05T23:57:47.879248: step 11594, loss 0.51124.
Train: 2018-08-05T23:57:48.059405: step 11595, loss 0.477174.
Train: 2018-08-05T23:57:48.231240: step 11596, loss 0.587998.
Train: 2018-08-05T23:57:48.395452: step 11597, loss 0.570975.
Train: 2018-08-05T23:57:48.569187: step 11598, loss 0.553866.
Train: 2018-08-05T23:57:48.729357: step 11599, loss 0.605049.
Train: 2018-08-05T23:57:48.909506: step 11600, loss 0.579497.
Test: 2018-08-05T23:57:49.151057: step 11600, loss 0.548123.
Train: 2018-08-05T23:57:49.900647: step 11601, loss 0.536846.
Train: 2018-08-05T23:57:50.063864: step 11602, loss 0.596531.
Train: 2018-08-05T23:57:50.244091: step 11603, loss 0.579441.
Train: 2018-08-05T23:57:50.407278: step 11604, loss 0.613536.
Train: 2018-08-05T23:57:50.587320: step 11605, loss 0.579422.
Train: 2018-08-05T23:57:50.754630: step 11606, loss 0.503116.
Train: 2018-08-05T23:57:50.927119: step 11607, loss 0.570844.
Train: 2018-08-05T23:57:51.097557: step 11608, loss 0.579345.
Train: 2018-08-05T23:57:51.266635: step 11609, loss 0.528631.
Train: 2018-08-05T23:57:51.442224: step 11610, loss 0.537094.
Test: 2018-08-05T23:57:51.668391: step 11610, loss 0.54825.
Train: 2018-08-05T23:57:51.847467: step 11611, loss 0.57939.
Train: 2018-08-05T23:57:52.010965: step 11612, loss 0.57089.
Train: 2018-08-05T23:57:52.190080: step 11613, loss 0.545511.
Train: 2018-08-05T23:57:52.361916: step 11614, loss 0.570902.
Train: 2018-08-05T23:57:52.525569: step 11615, loss 0.528552.
Train: 2018-08-05T23:57:52.689077: step 11616, loss 0.536998.
Train: 2018-08-05T23:57:52.868181: step 11617, loss 0.562423.
Train: 2018-08-05T23:57:53.022826: step 11618, loss 0.562394.
Train: 2018-08-05T23:57:53.209115: step 11619, loss 0.528323.
Train: 2018-08-05T23:57:53.374657: step 11620, loss 0.536779.
Test: 2018-08-05T23:57:53.613021: step 11620, loss 0.548074.
Train: 2018-08-05T23:57:53.784346: step 11621, loss 0.588082.
Train: 2018-08-05T23:57:53.954794: step 11622, loss 0.622413.
Train: 2018-08-05T23:57:54.117644: step 11623, loss 0.570965.
Train: 2018-08-05T23:57:54.281214: step 11624, loss 0.536682.
Train: 2018-08-05T23:57:54.466535: step 11625, loss 0.562389.
Train: 2018-08-05T23:57:54.631840: step 11626, loss 0.493777.
Train: 2018-08-05T23:57:54.790651: step 11627, loss 0.56243.
Train: 2018-08-05T23:57:54.962455: step 11628, loss 0.553777.
Train: 2018-08-05T23:57:55.132262: step 11629, loss 0.519283.
Train: 2018-08-05T23:57:55.287501: step 11630, loss 0.545113.
Test: 2018-08-05T23:57:55.539553: step 11630, loss 0.547901.
Train: 2018-08-05T23:57:55.711356: step 11631, loss 0.510438.
Train: 2018-08-05T23:57:55.874990: step 11632, loss 0.571127.
Train: 2018-08-05T23:57:56.031236: step 11633, loss 0.536273.
Train: 2018-08-05T23:57:56.210611: step 11634, loss 0.544958.
Train: 2018-08-05T23:57:56.389159: step 11635, loss 0.509871.
Train: 2018-08-05T23:57:56.555714: step 11636, loss 0.588858.
Train: 2018-08-05T23:57:56.715165: step 11637, loss 0.59776.
Train: 2018-08-05T23:57:56.894295: step 11638, loss 0.650827.
Train: 2018-08-05T23:57:57.056414: step 11639, loss 0.509522.
Train: 2018-08-05T23:57:57.219455: step 11640, loss 0.553649.
Test: 2018-08-05T23:57:57.461241: step 11640, loss 0.547709.
Train: 2018-08-05T23:57:57.624598: step 11641, loss 0.527158.
Train: 2018-08-05T23:57:57.799125: step 11642, loss 0.633236.
Train: 2018-08-05T23:57:57.970960: step 11643, loss 0.518344.
Train: 2018-08-05T23:57:58.140837: step 11644, loss 0.527173.
Train: 2018-08-05T23:57:58.309663: step 11645, loss 0.536003.
Train: 2018-08-05T23:57:58.481528: step 11646, loss 0.544802.
Train: 2018-08-05T23:57:58.657147: step 11647, loss 0.527094.
Train: 2018-08-05T23:57:58.821710: step 11648, loss 0.491552.
Train: 2018-08-05T23:57:58.985275: step 11649, loss 0.580325.
Train: 2018-08-05T23:57:59.164382: step 11650, loss 0.58932.
Test: 2018-08-05T23:57:59.390444: step 11650, loss 0.547642.
Train: 2018-08-05T23:57:59.569597: step 11651, loss 0.553636.
Train: 2018-08-05T23:57:59.741432: step 11652, loss 0.642947.
Train: 2018-08-05T23:57:59.904941: step 11653, loss 0.544738.
Train: 2018-08-05T23:58:00.093383: step 11654, loss 0.55363.
Train: 2018-08-05T23:58:00.256775: step 11655, loss 0.544818.
Train: 2018-08-05T23:58:00.435961: step 11656, loss 0.553638.
Train: 2018-08-05T23:58:00.611322: step 11657, loss 0.580317.
Train: 2018-08-05T23:58:00.770679: step 11658, loss 0.589174.
Train: 2018-08-05T23:58:00.952728: step 11659, loss 0.58027.
Train: 2018-08-05T23:58:01.115747: step 11660, loss 0.51822.
Test: 2018-08-05T23:58:01.356691: step 11660, loss 0.547691.
Train: 2018-08-05T23:58:01.535669: step 11661, loss 0.624492.
Train: 2018-08-05T23:58:01.714888: step 11662, loss 0.57132.
Train: 2018-08-05T23:58:01.894697: step 11663, loss 0.60658.
Train: 2018-08-05T23:58:02.050907: step 11664, loss 0.597598.
Train: 2018-08-05T23:58:02.223552: step 11665, loss 0.544961.
Train: 2018-08-05T23:58:02.395361: step 11666, loss 0.54497.
Train: 2018-08-05T23:58:02.596602: step 11667, loss 0.571152.
Train: 2018-08-05T23:58:02.765904: step 11668, loss 0.56239.
Train: 2018-08-05T23:58:02.926238: step 11669, loss 0.588401.
Train: 2018-08-05T23:58:03.098043: step 11670, loss 0.596954.
Test: 2018-08-05T23:58:03.347964: step 11670, loss 0.547978.
Train: 2018-08-05T23:58:03.524518: step 11671, loss 0.571025.
Train: 2018-08-05T23:58:03.696127: step 11672, loss 0.588139.
Train: 2018-08-05T23:58:03.852341: step 11673, loss 0.519603.
Train: 2018-08-05T23:58:04.015957: step 11674, loss 0.596571.
Train: 2018-08-05T23:58:04.179484: step 11675, loss 0.570898.
Train: 2018-08-05T23:58:04.358664: step 11676, loss 0.536943.
Train: 2018-08-05T23:58:04.529577: step 11677, loss 0.562384.
Train: 2018-08-05T23:58:04.701414: step 11678, loss 0.562438.
Train: 2018-08-05T23:58:04.865037: step 11679, loss 0.537075.
Train: 2018-08-05T23:58:05.044601: step 11680, loss 0.553999.
Test: 2018-08-05T23:58:05.272440: step 11680, loss 0.548262.
Train: 2018-08-05T23:58:05.459140: step 11681, loss 0.570892.
Train: 2018-08-05T23:58:05.638228: step 11682, loss 0.587785.
Train: 2018-08-05T23:58:05.818354: step 11683, loss 0.570882.
Train: 2018-08-05T23:58:05.997089: step 11684, loss 0.528681.
Train: 2018-08-05T23:58:06.178050: step 11685, loss 0.570825.
Train: 2018-08-05T23:58:06.352882: step 11686, loss 0.545549.
Train: 2018-08-05T23:58:06.525422: step 11687, loss 0.5202.
Train: 2018-08-05T23:58:06.697068: step 11688, loss 0.579358.
Train: 2018-08-05T23:58:06.875585: step 11689, loss 0.520096.
Train: 2018-08-05T23:58:07.056046: step 11690, loss 0.536934.
Test: 2018-08-05T23:58:07.281811: step 11690, loss 0.548164.
Train: 2018-08-05T23:58:07.464081: step 11691, loss 0.485843.
Train: 2018-08-05T23:58:07.635915: step 11692, loss 0.553857.
Train: 2018-08-05T23:58:07.807750: step 11693, loss 0.562379.
Train: 2018-08-05T23:58:07.972322: step 11694, loss 0.588203.
Train: 2018-08-05T23:58:08.149929: step 11695, loss 0.571017.
Train: 2018-08-05T23:58:08.329542: step 11696, loss 0.605614.
Train: 2018-08-05T23:58:08.499007: step 11697, loss 0.622874.
Train: 2018-08-05T23:58:08.670872: step 11698, loss 0.553789.
Train: 2018-08-05T23:58:08.835349: step 11699, loss 0.571062.
Train: 2018-08-05T23:58:09.007215: step 11700, loss 0.571025.
Test: 2018-08-05T23:58:09.249107: step 11700, loss 0.547972.
Train: 2018-08-05T23:58:09.959583: step 11701, loss 0.588269.
Train: 2018-08-05T23:58:10.122841: step 11702, loss 0.58823.
Train: 2018-08-05T23:58:10.301675: step 11703, loss 0.57957.
Train: 2018-08-05T23:58:10.473510: step 11704, loss 0.519545.
Train: 2018-08-05T23:58:10.639432: step 11705, loss 0.545263.
Train: 2018-08-05T23:58:10.818001: step 11706, loss 0.656583.
Train: 2018-08-05T23:58:10.979424: step 11707, loss 0.51112.
Train: 2018-08-05T23:58:11.150853: step 11708, loss 0.511158.
Train: 2018-08-05T23:58:11.330928: step 11709, loss 0.579494.
Train: 2018-08-05T23:58:11.501238: step 11710, loss 0.545303.
Test: 2018-08-05T23:58:11.726731: step 11710, loss 0.548099.
Train: 2018-08-05T23:58:11.909827: step 11711, loss 0.56239.
Train: 2018-08-05T23:58:12.072762: step 11712, loss 0.56239.
Train: 2018-08-05T23:58:12.252206: step 11713, loss 0.528194.
Train: 2018-08-05T23:58:12.415975: step 11714, loss 0.502528.
Train: 2018-08-05T23:58:12.593495: step 11715, loss 0.519537.
Train: 2018-08-05T23:58:12.790157: step 11716, loss 0.545238.
Train: 2018-08-05T23:58:12.945494: step 11717, loss 0.5969.
Train: 2018-08-05T23:58:13.120762: step 11718, loss 0.519195.
Train: 2018-08-05T23:58:13.298633: step 11719, loss 0.545097.
Train: 2018-08-05T23:58:13.470141: step 11720, loss 0.614511.
Test: 2018-08-05T23:58:13.697981: step 11720, loss 0.547858.
Train: 2018-08-05T23:58:13.873506: step 11721, loss 0.605914.
Train: 2018-08-05T23:58:14.052652: step 11722, loss 0.553769.
Train: 2018-08-05T23:58:14.217125: step 11723, loss 0.527683.
Train: 2018-08-05T23:58:14.388991: step 11724, loss 0.588519.
Train: 2018-08-05T23:58:14.552703: step 11725, loss 0.597259.
Train: 2018-08-05T23:58:14.731855: step 11726, loss 0.59723.
Train: 2018-08-05T23:58:14.895374: step 11727, loss 0.597183.
Train: 2018-08-05T23:58:15.066700: step 11728, loss 0.553743.
Train: 2018-08-05T23:58:15.225850: step 11729, loss 0.614317.
Train: 2018-08-05T23:58:15.402382: step 11730, loss 0.579701.
Test: 2018-08-05T23:58:15.631110: step 11730, loss 0.547983.
Train: 2018-08-05T23:58:15.806224: step 11731, loss 0.562402.
Train: 2018-08-05T23:58:15.985331: step 11732, loss 0.536666.
Train: 2018-08-05T23:58:16.152157: step 11733, loss 0.579534.
Train: 2018-08-05T23:58:16.331869: step 11734, loss 0.519575.
Train: 2018-08-05T23:58:16.506377: step 11735, loss 0.511115.
Train: 2018-08-05T23:58:16.677919: step 11736, loss 0.596637.
Train: 2018-08-05T23:58:16.852470: step 11737, loss 0.570969.
Train: 2018-08-05T23:58:17.023368: step 11738, loss 0.545365.
Train: 2018-08-05T23:58:17.186914: step 11739, loss 0.5709.
Train: 2018-08-05T23:58:17.364638: step 11740, loss 0.570935.
Test: 2018-08-05T23:58:17.592852: step 11740, loss 0.548103.
Train: 2018-08-05T23:58:17.764686: step 11741, loss 0.588021.
Train: 2018-08-05T23:58:17.928202: step 11742, loss 0.485611.
Train: 2018-08-05T23:58:18.107337: step 11743, loss 0.459938.
Train: 2018-08-05T23:58:18.279196: step 11744, loss 0.536704.
Train: 2018-08-05T23:58:18.450131: step 11745, loss 0.588196.
Train: 2018-08-05T23:58:18.614560: step 11746, loss 0.588222.
Train: 2018-08-05T23:58:18.809367: step 11747, loss 0.536535.
Train: 2018-08-05T23:58:18.972297: step 11748, loss 0.614318.
Train: 2018-08-05T23:58:19.150620: step 11749, loss 0.519179.
Train: 2018-08-05T23:58:19.310343: step 11750, loss 0.458458.
Test: 2018-08-05T23:58:19.557531: step 11750, loss 0.547863.
Train: 2018-08-05T23:58:19.714284: step 11751, loss 0.562473.
Train: 2018-08-05T23:58:19.893201: step 11752, loss 0.597304.
Train: 2018-08-05T23:58:20.057853: step 11753, loss 0.562409.
Train: 2018-08-05T23:58:20.221022: step 11754, loss 0.623638.
Train: 2018-08-05T23:58:20.392887: step 11755, loss 0.544941.
Train: 2018-08-05T23:58:20.567572: step 11756, loss 0.571176.
Train: 2018-08-05T23:58:20.723785: step 11757, loss 0.536173.
Train: 2018-08-05T23:58:20.893892: step 11758, loss 0.536182.
Train: 2018-08-05T23:58:21.062858: step 11759, loss 0.562458.
Train: 2018-08-05T23:58:21.238414: step 11760, loss 0.492283.
Test: 2018-08-05T23:58:21.448893: step 11760, loss 0.54775.
Train: 2018-08-05T23:58:21.628147: step 11761, loss 0.597597.
Train: 2018-08-05T23:58:21.799981: step 11762, loss 0.553633.
Train: 2018-08-05T23:58:21.979055: step 11763, loss 0.544887.
Train: 2018-08-05T23:58:22.142675: step 11764, loss 0.624147.
Train: 2018-08-05T23:58:22.306196: step 11765, loss 0.580097.
Train: 2018-08-05T23:58:22.486999: step 11766, loss 0.518508.
Train: 2018-08-05T23:58:22.643243: step 11767, loss 0.562517.
Train: 2018-08-05T23:58:22.814591: step 11768, loss 0.544862.
Train: 2018-08-05T23:58:22.995898: step 11769, loss 0.571287.
Train: 2018-08-05T23:58:23.156365: step 11770, loss 0.54486.
Test: 2018-08-05T23:58:23.403239: step 11770, loss 0.547738.
Train: 2018-08-05T23:58:23.619713: step 11771, loss 0.641648.
Train: 2018-08-05T23:58:23.785320: step 11772, loss 0.641545.
Train: 2018-08-05T23:58:23.949855: step 11773, loss 0.632545.
Train: 2018-08-05T23:58:24.113330: step 11774, loss 0.466556.
Train: 2018-08-05T23:58:24.284220: step 11775, loss 0.545005.
Train: 2018-08-05T23:58:24.448859: step 11776, loss 0.605828.
Train: 2018-08-05T23:58:24.628090: step 11777, loss 0.571107.
Train: 2018-08-05T23:58:24.791586: step 11778, loss 0.672994.
Train: 2018-08-05T23:58:24.955084: step 11779, loss 0.62262.
Train: 2018-08-05T23:58:25.118555: step 11780, loss 0.562405.
Test: 2018-08-05T23:58:25.367509: step 11780, loss 0.548149.
Train: 2018-08-05T23:58:25.523722: step 11781, loss 0.570898.
Train: 2018-08-05T23:58:25.703062: step 11782, loss 0.579406.
Train: 2018-08-05T23:58:25.866551: step 11783, loss 0.537126.
Train: 2018-08-05T23:58:26.045687: step 11784, loss 0.587679.
Train: 2018-08-05T23:58:26.200115: step 11785, loss 0.537323.
Train: 2018-08-05T23:58:26.393057: step 11786, loss 0.51233.
Train: 2018-08-05T23:58:26.569615: step 11787, loss 0.504015.
Train: 2018-08-05T23:58:26.734675: step 11788, loss 0.512327.
Train: 2018-08-05T23:58:26.929442: step 11789, loss 0.512252.
Train: 2018-08-05T23:58:27.110800: step 11790, loss 0.612812.
Test: 2018-08-05T23:58:27.341943: step 11790, loss 0.548368.
Train: 2018-08-05T23:58:27.505465: step 11791, loss 0.562469.
Train: 2018-08-05T23:58:27.685553: step 11792, loss 0.587693.
Train: 2018-08-05T23:58:27.854982: step 11793, loss 0.596122.
Train: 2018-08-05T23:58:28.021915: step 11794, loss 0.655174.
Train: 2018-08-05T23:58:28.192260: step 11795, loss 0.528795.
Train: 2018-08-05T23:58:28.355786: step 11796, loss 0.570841.
Train: 2018-08-05T23:58:28.520156: step 11797, loss 0.646405.
Train: 2018-08-05T23:58:28.699043: step 11798, loss 0.579211.
Train: 2018-08-05T23:58:28.855256: step 11799, loss 0.554124.
Train: 2018-08-05T23:58:29.039705: step 11800, loss 0.529137.
Test: 2018-08-05T23:58:29.273056: step 11800, loss 0.548542.
Train: 2018-08-05T23:58:30.004010: step 11801, loss 0.529176.
Train: 2018-08-05T23:58:30.187508: step 11802, loss 0.59584.
Train: 2018-08-05T23:58:30.354141: step 11803, loss 0.52087.
Train: 2018-08-05T23:58:30.518062: step 11804, loss 0.654148.
Train: 2018-08-05T23:58:30.696111: step 11805, loss 0.612425.
Train: 2018-08-05T23:58:30.868612: step 11806, loss 0.504395.
Train: 2018-08-05T23:58:31.034025: step 11807, loss 0.529331.
Train: 2018-08-05T23:58:31.196844: step 11808, loss 0.579114.
Train: 2018-08-05T23:58:31.375460: step 11809, loss 0.587423.
Train: 2018-08-05T23:58:31.553917: step 11810, loss 0.637237.
Test: 2018-08-05T23:58:31.795760: step 11810, loss 0.54865.
Train: 2018-08-05T23:58:31.974597: step 11811, loss 0.603967.
Train: 2018-08-05T23:58:32.130843: step 11812, loss 0.579085.
Train: 2018-08-05T23:58:32.305258: step 11813, loss 0.504822.
Train: 2018-08-05T23:58:32.477125: step 11814, loss 0.562566.
Train: 2018-08-05T23:58:32.661984: step 11815, loss 0.554338.
Train: 2018-08-05T23:58:32.831286: step 11816, loss 0.595551.
Train: 2018-08-05T23:58:33.007190: step 11817, loss 0.595532.
Train: 2018-08-05T23:58:33.179036: step 11818, loss 0.554331.
Train: 2018-08-05T23:58:33.354906: step 11819, loss 0.611933.
Train: 2018-08-05T23:58:33.523425: step 11820, loss 0.529731.
Test: 2018-08-05T23:58:33.760697: step 11820, loss 0.548858.
Train: 2018-08-05T23:58:33.917407: step 11821, loss 0.496923.
Train: 2018-08-05T23:58:34.083180: step 11822, loss 0.496767.
Train: 2018-08-05T23:58:34.262303: step 11823, loss 0.546086.
Train: 2018-08-05T23:58:34.425808: step 11824, loss 0.570844.
Train: 2018-08-05T23:58:34.611323: step 11825, loss 0.545887.
Train: 2018-08-05T23:58:34.783159: step 11826, loss 0.51251.
Train: 2018-08-05T23:58:34.946865: step 11827, loss 0.562472.
Train: 2018-08-05T23:58:35.110457: step 11828, loss 0.470097.
Train: 2018-08-05T23:58:35.291265: step 11829, loss 0.537083.
Train: 2018-08-05T23:58:35.455714: step 11830, loss 0.570873.
Test: 2018-08-05T23:58:35.688415: step 11830, loss 0.548107.
Train: 2018-08-05T23:58:35.868205: step 11831, loss 0.622149.
Train: 2018-08-05T23:58:36.050212: step 11832, loss 0.596642.
Train: 2018-08-05T23:58:36.218839: step 11833, loss 0.536653.
Train: 2018-08-05T23:58:36.387987: step 11834, loss 0.571004.
Train: 2018-08-05T23:58:36.556564: step 11835, loss 0.588277.
Train: 2018-08-05T23:58:36.722632: step 11836, loss 0.527869.
Train: 2018-08-05T23:58:36.890139: step 11837, loss 0.571031.
Train: 2018-08-05T23:58:37.058747: step 11838, loss 0.44987.
Train: 2018-08-05T23:58:37.221129: step 11839, loss 0.518965.
Train: 2018-08-05T23:58:37.384375: step 11840, loss 0.571144.
Test: 2018-08-05T23:58:37.621444: step 11840, loss 0.547788.
Train: 2018-08-05T23:58:37.793275: step 11841, loss 0.518685.
Train: 2018-08-05T23:58:37.963125: step 11842, loss 0.588812.
Train: 2018-08-05T23:58:38.131947: step 11843, loss 0.597703.
Train: 2018-08-05T23:58:38.305996: step 11844, loss 0.527205.
Train: 2018-08-05T23:58:38.465502: step 11845, loss 0.527144.
Train: 2018-08-05T23:58:38.627297: step 11846, loss 0.509303.
Train: 2018-08-05T23:58:38.802332: step 11847, loss 0.544749.
Train: 2018-08-05T23:58:38.965860: step 11848, loss 0.544747.
Train: 2018-08-05T23:58:39.145972: step 11849, loss 0.571535.
Train: 2018-08-05T23:58:39.302215: step 11850, loss 0.589513.
Test: 2018-08-05T23:58:39.543910: step 11850, loss 0.547609.
Train: 2018-08-05T23:58:39.707437: step 11851, loss 0.481893.
Train: 2018-08-05T23:58:39.893960: step 11852, loss 0.598647.
Train: 2018-08-05T23:58:40.057470: step 11853, loss 0.562688.
Train: 2018-08-05T23:58:40.236564: step 11854, loss 0.580706.
Train: 2018-08-05T23:58:40.401023: step 11855, loss 0.607786.
Train: 2018-08-05T23:58:40.572883: step 11856, loss 0.499584.
Train: 2018-08-05T23:58:40.737069: step 11857, loss 0.571693.
Train: 2018-08-05T23:58:40.906854: step 11858, loss 0.571704.
Train: 2018-08-05T23:58:41.055324: step 11859, loss 0.517647.
Train: 2018-08-05T23:58:41.234250: step 11860, loss 0.517643.
Test: 2018-08-05T23:58:41.475696: step 11860, loss 0.547593.
Train: 2018-08-05T23:58:41.651183: step 11861, loss 0.463586.
Train: 2018-08-05T23:58:41.813460: step 11862, loss 0.553681.
Train: 2018-08-05T23:58:41.976358: step 11863, loss 0.553676.
Train: 2018-08-05T23:58:42.154774: step 11864, loss 0.626206.
Train: 2018-08-05T23:58:42.308613: step 11865, loss 0.562754.
Train: 2018-08-05T23:58:42.487751: step 11866, loss 0.508315.
Train: 2018-08-05T23:58:42.664674: step 11867, loss 0.51741.
Train: 2018-08-05T23:58:42.822657: step 11868, loss 0.581003.
Train: 2018-08-05T23:58:42.999173: step 11869, loss 0.580946.
Train: 2018-08-05T23:58:43.155417: step 11870, loss 0.562788.
Test: 2018-08-05T23:58:43.408368: step 11870, loss 0.547575.
Train: 2018-08-05T23:58:43.579890: step 11871, loss 0.635473.
Train: 2018-08-05T23:58:43.740405: step 11872, loss 0.508338.
Train: 2018-08-05T23:58:43.896618: step 11873, loss 0.589909.
Train: 2018-08-05T23:58:44.080619: step 11874, loss 0.544584.
Train: 2018-08-05T23:58:44.238383: step 11875, loss 0.526631.
Train: 2018-08-05T23:58:44.408307: step 11876, loss 0.508573.
Train: 2018-08-05T23:58:44.577669: step 11877, loss 0.607807.
Train: 2018-08-05T23:58:44.752351: step 11878, loss 0.670818.
Train: 2018-08-05T23:58:44.916882: step 11879, loss 0.580583.
Train: 2018-08-05T23:58:45.079742: step 11880, loss 0.571567.
Test: 2018-08-05T23:58:45.324508: step 11880, loss 0.547641.
Train: 2018-08-05T23:58:45.484869: step 11881, loss 0.607158.
Train: 2018-08-05T23:58:45.648366: step 11882, loss 0.544798.
Train: 2018-08-05T23:58:45.814185: step 11883, loss 0.571373.
Train: 2018-08-05T23:58:45.992750: step 11884, loss 0.588899.
Train: 2018-08-05T23:58:46.168126: step 11885, loss 0.615138.
Train: 2018-08-05T23:58:46.337155: step 11886, loss 0.518812.
Train: 2018-08-05T23:58:46.500749: step 11887, loss 0.588523.
Train: 2018-08-05T23:58:46.672289: step 11888, loss 0.58839.
Train: 2018-08-05T23:58:46.822587: step 11889, loss 0.545116.
Train: 2018-08-05T23:58:47.007229: step 11890, loss 0.545144.
Test: 2018-08-05T23:58:47.243677: step 11890, loss 0.548017.
Train: 2018-08-05T23:58:47.413202: step 11891, loss 0.596739.
Train: 2018-08-05T23:58:47.587915: step 11892, loss 0.485333.
Train: 2018-08-05T23:58:47.748805: step 11893, loss 0.519628.
Train: 2018-08-05T23:58:47.912076: step 11894, loss 0.64795.
Train: 2018-08-05T23:58:48.083881: step 11895, loss 0.562383.
Train: 2018-08-05T23:58:48.240124: step 11896, loss 0.562409.
Train: 2018-08-05T23:58:48.409265: step 11897, loss 0.553885.
Train: 2018-08-05T23:58:48.589936: step 11898, loss 0.50293.
Train: 2018-08-05T23:58:48.750348: step 11899, loss 0.638935.
Train: 2018-08-05T23:58:48.910339: step 11900, loss 0.511466.
Test: 2018-08-05T23:58:49.160251: step 11900, loss 0.548192.
Train: 2018-08-05T23:58:49.869287: step 11901, loss 0.570921.
Train: 2018-08-05T23:58:50.048388: step 11902, loss 0.519962.
Train: 2018-08-05T23:58:50.204602: step 11903, loss 0.596442.
Train: 2018-08-05T23:58:50.375148: step 11904, loss 0.536918.
Train: 2018-08-05T23:58:50.538550: step 11905, loss 0.579413.
Train: 2018-08-05T23:58:50.717656: step 11906, loss 0.579415.
Train: 2018-08-05T23:58:50.881195: step 11907, loss 0.528403.
Train: 2018-08-05T23:58:51.053061: step 11908, loss 0.630487.
Train: 2018-08-05T23:58:51.216680: step 11909, loss 0.528409.
Train: 2018-08-05T23:58:51.380133: step 11910, loss 0.621936.
Test: 2018-08-05T23:58:51.627560: step 11910, loss 0.548188.
Train: 2018-08-05T23:58:51.791025: step 11911, loss 0.502964.
Train: 2018-08-05T23:58:51.970222: step 11912, loss 0.528442.
Train: 2018-08-05T23:58:52.133733: step 11913, loss 0.587935.
Train: 2018-08-05T23:58:52.305554: step 11914, loss 0.579411.
Train: 2018-08-05T23:58:52.460899: step 11915, loss 0.613418.
Train: 2018-08-05T23:58:52.641058: step 11916, loss 0.519921.
Train: 2018-08-05T23:58:52.803996: step 11917, loss 0.596359.
Train: 2018-08-05T23:58:52.967873: step 11918, loss 0.528424.
Train: 2018-08-05T23:58:53.139738: step 11919, loss 0.536922.
Train: 2018-08-05T23:58:53.320137: step 11920, loss 0.570893.
Test: 2018-08-05T23:58:53.557471: step 11920, loss 0.548163.
Train: 2018-08-05T23:58:53.712818: step 11921, loss 0.579442.
Train: 2018-08-05T23:58:53.884676: step 11922, loss 0.596445.
Train: 2018-08-05T23:58:54.054067: step 11923, loss 0.545414.
Train: 2018-08-05T23:58:54.225398: step 11924, loss 0.621913.
Train: 2018-08-05T23:58:54.376864: step 11925, loss 0.562421.
Train: 2018-08-05T23:58:54.552010: step 11926, loss 0.553923.
Train: 2018-08-05T23:58:54.709197: step 11927, loss 0.638671.
Train: 2018-08-05T23:58:54.871906: step 11928, loss 0.494773.
Train: 2018-08-05T23:58:55.035416: step 11929, loss 0.598509.
Train: 2018-08-05T23:58:55.214022: step 11930, loss 0.604623.
Test: 2018-08-05T23:58:55.446641: step 11930, loss 0.548328.
Train: 2018-08-05T23:58:55.616221: step 11931, loss 0.562416.
Train: 2018-08-05T23:58:55.775151: step 11932, loss 0.511962.
Train: 2018-08-05T23:58:55.950966: step 11933, loss 0.596076.
Train: 2018-08-05T23:58:56.114384: step 11934, loss 0.596038.
Train: 2018-08-05T23:58:56.300385: step 11935, loss 0.604429.
Train: 2018-08-05T23:58:56.468968: step 11936, loss 0.554121.
Train: 2018-08-05T23:58:56.637516: step 11937, loss 0.54573.
Train: 2018-08-05T23:58:56.794786: step 11938, loss 0.637638.
Train: 2018-08-05T23:58:56.974908: step 11939, loss 0.579176.
Train: 2018-08-05T23:58:57.139573: step 11940, loss 0.587393.
Test: 2018-08-05T23:58:57.365549: step 11940, loss 0.548651.
Train: 2018-08-05T23:58:57.541947: step 11941, loss 0.562522.
Train: 2018-08-05T23:58:57.702582: step 11942, loss 0.529455.
Train: 2018-08-05T23:58:57.881665: step 11943, loss 0.479944.
Train: 2018-08-05T23:58:58.045202: step 11944, loss 0.587344.
Train: 2018-08-05T23:58:58.217066: step 11945, loss 0.570808.
Train: 2018-08-05T23:58:58.380719: step 11946, loss 0.545955.
Train: 2018-08-05T23:58:58.551986: step 11947, loss 0.537651.
Train: 2018-08-05T23:58:58.715505: step 11948, loss 0.545887.
Train: 2018-08-05T23:58:58.880062: step 11949, loss 0.562481.
Train: 2018-08-05T23:58:59.051897: step 11950, loss 0.52082.
Test: 2018-08-05T23:58:59.285323: step 11950, loss 0.548461.
Train: 2018-08-05T23:58:59.457188: step 11951, loss 0.545731.
Train: 2018-08-05T23:58:59.631343: step 11952, loss 0.486872.
Train: 2018-08-05T23:58:59.806461: step 11953, loss 0.596219.
Train: 2018-08-05T23:58:59.970037: step 11954, loss 0.621701.
Train: 2018-08-05T23:59:00.134520: step 11955, loss 0.596362.
Train: 2018-08-05T23:59:00.306325: step 11956, loss 0.536932.
Train: 2018-08-05T23:59:00.470841: step 11957, loss 0.536952.
Train: 2018-08-05T23:59:00.641432: step 11958, loss 0.502848.
Train: 2018-08-05T23:59:00.804915: step 11959, loss 0.502671.
Train: 2018-08-05T23:59:00.984361: step 11960, loss 0.536688.
Test: 2018-08-05T23:59:01.210164: step 11960, loss 0.547983.
Train: 2018-08-05T23:59:01.392457: step 11961, loss 0.605504.
Train: 2018-08-05T23:59:01.548670: step 11962, loss 0.510619.
Train: 2018-08-05T23:59:01.727148: step 11963, loss 0.55373.
Train: 2018-08-05T23:59:01.898209: step 11964, loss 0.562429.
Train: 2018-08-05T23:59:02.060752: step 11965, loss 0.605947.
Train: 2018-08-05T23:59:02.225333: step 11966, loss 0.475104.
Train: 2018-08-05T23:59:02.397169: step 11967, loss 0.606296.
Train: 2018-08-05T23:59:02.560959: step 11968, loss 0.544917.
Train: 2018-08-05T23:59:02.724586: step 11969, loss 0.536078.
Train: 2018-08-05T23:59:02.903739: step 11970, loss 0.641871.
Test: 2018-08-05T23:59:03.136115: step 11970, loss 0.54772.
Train: 2018-08-05T23:59:03.322742: step 11971, loss 0.544791.
Train: 2018-08-05T23:59:03.493286: step 11972, loss 0.588962.
Train: 2018-08-05T23:59:03.655970: step 11973, loss 0.553544.
Train: 2018-08-05T23:59:03.812208: step 11974, loss 0.597769.
Train: 2018-08-05T23:59:03.982734: step 11975, loss 0.544902.
Train: 2018-08-05T23:59:04.145567: step 11976, loss 0.580058.
Train: 2018-08-05T23:59:04.324738: step 11977, loss 0.615225.
Train: 2018-08-05T23:59:04.487842: step 11978, loss 0.553749.
Train: 2018-08-05T23:59:04.644055: step 11979, loss 0.571175.
Train: 2018-08-05T23:59:04.818220: step 11980, loss 0.553674.
Test: 2018-08-05T23:59:05.051095: step 11980, loss 0.547824.
Train: 2018-08-05T23:59:05.206835: step 11981, loss 0.545043.
Train: 2018-08-05T23:59:05.391460: step 11982, loss 0.527609.
Train: 2018-08-05T23:59:05.551333: step 11983, loss 0.553754.
Train: 2018-08-05T23:59:05.730486: step 11984, loss 0.545022.
Train: 2018-08-05T23:59:05.894089: step 11985, loss 0.553671.
Train: 2018-08-05T23:59:06.065954: step 11986, loss 0.614667.
Train: 2018-08-05T23:59:06.228767: step 11987, loss 0.527653.
Train: 2018-08-05T23:59:06.405908: step 11988, loss 0.588475.
Train: 2018-08-05T23:59:06.574427: step 11989, loss 0.562439.
Train: 2018-08-05T23:59:06.733860: step 11990, loss 0.51903.
Test: 2018-08-05T23:59:06.974947: step 11990, loss 0.547881.
Train: 2018-08-05T23:59:07.144504: step 11991, loss 0.597155.
Train: 2018-08-05T23:59:07.324160: step 11992, loss 0.510373.
Train: 2018-08-05T23:59:07.480056: step 11993, loss 0.59709.
Train: 2018-08-05T23:59:07.643286: step 11994, loss 0.536373.
Train: 2018-08-05T23:59:07.817719: step 11995, loss 0.623096.
Train: 2018-08-05T23:59:07.989554: step 11996, loss 0.579704.
Train: 2018-08-05T23:59:08.159402: step 11997, loss 0.597023.
Train: 2018-08-05T23:59:08.327110: step 11998, loss 0.510624.
Train: 2018-08-05T23:59:08.484512: step 11999, loss 0.596927.
Train: 2018-08-05T23:59:08.646176: step 12000, loss 0.596842.
Test: 2018-08-05T23:59:08.888812: step 12000, loss 0.54801.
Train: 2018-08-05T23:59:09.638539: step 12001, loss 0.562403.
Train: 2018-08-05T23:59:09.825896: step 12002, loss 0.519467.
Train: 2018-08-05T23:59:09.985835: step 12003, loss 0.545238.
Train: 2018-08-05T23:59:10.156299: step 12004, loss 0.553852.
Train: 2018-08-05T23:59:10.332179: step 12005, loss 0.485311.
Train: 2018-08-05T23:59:10.498256: step 12006, loss 0.613885.
Train: 2018-08-05T23:59:10.661441: step 12007, loss 0.553837.
Train: 2018-08-05T23:59:10.835742: step 12008, loss 0.519434.
Train: 2018-08-05T23:59:10.999179: step 12009, loss 0.571005.
Train: 2018-08-05T23:59:11.162349: step 12010, loss 0.648414.
Test: 2018-08-05T23:59:11.409764: step 12010, loss 0.548004.
Train: 2018-08-05T23:59:11.572693: step 12011, loss 0.571002.
Train: 2018-08-05T23:59:11.736325: step 12012, loss 0.536639.
Train: 2018-08-05T23:59:11.900586: step 12013, loss 0.553826.
Train: 2018-08-05T23:59:12.072422: step 12014, loss 0.553784.
Train: 2018-08-05T23:59:12.231476: step 12015, loss 0.502321.
Train: 2018-08-05T23:59:12.403307: step 12016, loss 0.622564.
Train: 2018-08-05T23:59:12.558199: step 12017, loss 0.596771.
Train: 2018-08-05T23:59:12.727133: step 12018, loss 0.485123.
Train: 2018-08-05T23:59:12.887049: step 12019, loss 0.553803.
Train: 2018-08-05T23:59:13.058908: step 12020, loss 0.545186.
Test: 2018-08-05T23:59:13.302112: step 12020, loss 0.547982.
Train: 2018-08-05T23:59:13.469664: step 12021, loss 0.596835.
Train: 2018-08-05T23:59:13.636218: step 12022, loss 0.527972.
Train: 2018-08-05T23:59:13.791188: step 12023, loss 0.536567.
Train: 2018-08-05T23:59:13.954121: step 12024, loss 0.562389.
Train: 2018-08-05T23:59:14.127774: step 12025, loss 0.588322.
Train: 2018-08-05T23:59:14.287534: step 12026, loss 0.5192.
Train: 2018-08-05T23:59:14.457518: step 12027, loss 0.562379.
Train: 2018-08-05T23:59:14.620584: step 12028, loss 0.631759.
Train: 2018-08-05T23:59:14.783838: step 12029, loss 0.527787.
Train: 2018-08-05T23:59:14.948773: step 12030, loss 0.484404.
Test: 2018-08-05T23:59:15.192572: step 12030, loss 0.547879.
Train: 2018-08-05T23:59:15.365870: step 12031, loss 0.562408.
Train: 2018-08-05T23:59:15.527261: step 12032, loss 0.536333.
Train: 2018-08-05T23:59:15.690345: step 12033, loss 0.579802.
Train: 2018-08-05T23:59:15.853229: step 12034, loss 0.562426.
Train: 2018-08-05T23:59:16.016728: step 12035, loss 0.658457.
Train: 2018-08-05T23:59:16.195094: step 12036, loss 0.536246.
Train: 2018-08-05T23:59:16.367159: step 12037, loss 0.536295.
Train: 2018-08-05T23:59:16.536674: step 12038, loss 0.588581.
Train: 2018-08-05T23:59:16.701233: step 12039, loss 0.56242.
Train: 2018-08-05T23:59:16.856937: step 12040, loss 0.605908.
Test: 2018-08-05T23:59:17.088723: step 12040, loss 0.547862.
Train: 2018-08-05T23:59:17.262087: step 12041, loss 0.640644.
Train: 2018-08-05T23:59:17.426627: step 12042, loss 0.536425.
Train: 2018-08-05T23:59:17.581746: step 12043, loss 0.519173.
Train: 2018-08-05T23:59:17.739596: step 12044, loss 0.536509.
Train: 2018-08-05T23:59:17.901143: step 12045, loss 0.605567.
Train: 2018-08-05T23:59:18.065812: step 12046, loss 0.501992.
Train: 2018-08-05T23:59:18.230569: step 12047, loss 0.536552.
Train: 2018-08-05T23:59:18.386782: step 12048, loss 0.579637.
Train: 2018-08-05T23:59:18.565243: step 12049, loss 0.605507.
Train: 2018-08-05T23:59:18.731336: step 12050, loss 0.545161.
Test: 2018-08-05T23:59:18.960756: step 12050, loss 0.547969.
Train: 2018-08-05T23:59:19.139916: step 12051, loss 0.519309.
Train: 2018-08-05T23:59:19.303174: step 12052, loss 0.657243.
Train: 2018-08-05T23:59:19.477279: step 12053, loss 0.553767.
Train: 2018-08-05T23:59:19.633955: step 12054, loss 0.579586.
Train: 2018-08-05T23:59:19.790139: step 12055, loss 0.545244.
Train: 2018-08-05T23:59:19.960084: step 12056, loss 0.562421.
Train: 2018-08-05T23:59:20.113779: step 12057, loss 0.510984.
Train: 2018-08-05T23:59:20.288611: step 12058, loss 0.502404.
Train: 2018-08-05T23:59:20.445975: step 12059, loss 0.510919.
Train: 2018-08-05T23:59:20.602185: step 12060, loss 0.553787.
Test: 2018-08-05T23:59:20.843846: step 12060, loss 0.54796.
Train: 2018-08-05T23:59:21.007450: step 12061, loss 0.571024.
Train: 2018-08-05T23:59:21.186773: step 12062, loss 0.622877.
Train: 2018-08-05T23:59:21.334690: step 12063, loss 0.605631.
Train: 2018-08-05T23:59:21.511116: step 12064, loss 0.501938.
Train: 2018-08-05T23:59:21.671816: step 12065, loss 0.536474.
Train: 2018-08-05T23:59:21.826480: step 12066, loss 0.55377.
Train: 2018-08-05T23:59:21.990985: step 12067, loss 0.579736.
Train: 2018-08-05T23:59:22.154451: step 12068, loss 0.61441.
Train: 2018-08-05T23:59:22.324165: step 12069, loss 0.467124.
Train: 2018-08-05T23:59:22.489539: step 12070, loss 0.597087.
Test: 2018-08-05T23:59:22.717806: step 12070, loss 0.54788.
Train: 2018-08-05T23:59:22.880965: step 12071, loss 0.562427.
Train: 2018-08-05T23:59:23.044185: step 12072, loss 0.605805.
Train: 2018-08-05T23:59:23.213440: step 12073, loss 0.553739.
Train: 2018-08-05T23:59:23.386271: step 12074, loss 0.579735.
Train: 2018-08-05T23:59:23.548867: step 12075, loss 0.6144.
Train: 2018-08-05T23:59:23.701903: step 12076, loss 0.553784.
Train: 2018-08-05T23:59:23.880577: step 12077, loss 0.545126.
Train: 2018-08-05T23:59:24.033775: step 12078, loss 0.545116.
Train: 2018-08-05T23:59:24.198978: step 12079, loss 0.588265.
Train: 2018-08-05T23:59:24.363759: step 12080, loss 0.635938.
Test: 2018-08-05T23:59:24.602725: step 12080, loss 0.547996.
Train: 2018-08-05T23:59:24.760195: step 12081, loss 0.562426.
Train: 2018-08-05T23:59:24.918352: step 12082, loss 0.631054.
Train: 2018-08-05T23:59:25.074593: step 12083, loss 0.553868.
Train: 2018-08-05T23:59:25.260044: step 12084, loss 0.50267.
Train: 2018-08-05T23:59:25.413243: step 12085, loss 0.57945.
Train: 2018-08-05T23:59:25.589631: step 12086, loss 0.528385.
Train: 2018-08-05T23:59:25.776644: step 12087, loss 0.553909.
Train: 2018-08-05T23:59:25.932857: step 12088, loss 0.5539.
Train: 2018-08-05T23:59:26.095664: step 12089, loss 0.604898.
Train: 2018-08-05T23:59:26.259122: step 12090, loss 0.536942.
Test: 2018-08-05T23:59:26.499204: step 12090, loss 0.548195.
Train: 2018-08-05T23:59:26.661799: step 12091, loss 0.545474.
Train: 2018-08-05T23:59:26.816962: step 12092, loss 0.570875.
Train: 2018-08-05T23:59:26.973206: step 12093, loss 0.519965.
Train: 2018-08-05T23:59:27.127437: step 12094, loss 0.5539.
Train: 2018-08-05T23:59:27.290962: step 12095, loss 0.477365.
Train: 2018-08-05T23:59:27.454447: step 12096, loss 0.545327.
Train: 2018-08-05T23:59:27.610661: step 12097, loss 0.519653.
Train: 2018-08-05T23:59:27.774676: step 12098, loss 0.631044.
Train: 2018-08-05T23:59:27.938121: step 12099, loss 0.519465.
Train: 2018-08-05T23:59:28.101519: step 12100, loss 0.605477.
Test: 2018-08-05T23:59:28.333875: step 12100, loss 0.547959.
Train: 2018-08-05T23:59:29.013517: step 12101, loss 0.605528.
Train: 2018-08-05T23:59:29.178105: step 12102, loss 0.55378.
Train: 2018-08-05T23:59:29.347276: step 12103, loss 0.605562.
Train: 2018-08-05T23:59:29.507116: step 12104, loss 0.484791.
Train: 2018-08-05T23:59:29.666859: step 12105, loss 0.510621.
Train: 2018-08-05T23:59:29.830100: step 12106, loss 0.631589.
Train: 2018-08-05T23:59:29.980376: step 12107, loss 0.674834.
Train: 2018-08-05T23:59:30.154806: step 12108, loss 0.553788.
Train: 2018-08-05T23:59:30.311620: step 12109, loss 0.527943.
Train: 2018-08-05T23:59:30.470477: step 12110, loss 0.605434.
Test: 2018-08-05T23:59:30.711470: step 12110, loss 0.547997.
Train: 2018-08-05T23:59:30.874335: step 12111, loss 0.682754.
Train: 2018-08-05T23:59:31.037530: step 12112, loss 0.605252.
Train: 2018-08-05T23:59:31.193718: step 12113, loss 0.502689.
Train: 2018-08-05T23:59:31.353667: step 12114, loss 0.596429.
Train: 2018-08-05T23:59:31.509857: step 12115, loss 0.604825.
Train: 2018-08-05T23:59:31.665400: step 12116, loss 0.562417.
Train: 2018-08-05T23:59:31.834193: step 12117, loss 0.554.
Train: 2018-08-05T23:59:31.994204: step 12118, loss 0.604483.
Train: 2018-08-05T23:59:32.150448: step 12119, loss 0.562458.
Train: 2018-08-05T23:59:32.314131: step 12120, loss 0.554107.
Test: 2018-08-05T23:59:32.555757: step 12120, loss 0.548509.
Train: 2018-08-05T23:59:32.719439: step 12121, loss 0.554135.
Train: 2018-08-05T23:59:32.882922: step 12122, loss 0.595817.
Train: 2018-08-05T23:59:33.031926: step 12123, loss 0.53757.
Train: 2018-08-05T23:59:33.188172: step 12124, loss 0.570814.
Train: 2018-08-05T23:59:33.365574: step 12125, loss 0.512761.
Train: 2018-08-05T23:59:33.532159: step 12126, loss 0.570816.
Train: 2018-08-05T23:59:33.691801: step 12127, loss 0.545921.
Train: 2018-08-05T23:59:33.854637: step 12128, loss 0.504384.
Train: 2018-08-05T23:59:34.010851: step 12129, loss 0.50426.
Train: 2018-08-05T23:59:34.158865: step 12130, loss 0.562484.
Test: 2018-08-05T23:59:34.403029: step 12130, loss 0.548448.
Train: 2018-08-05T23:59:34.566927: step 12131, loss 0.554089.
Train: 2018-08-05T23:59:34.729840: step 12132, loss 0.562457.
Train: 2018-08-05T23:59:34.893133: step 12133, loss 0.570847.
Train: 2018-08-05T23:59:35.039869: step 12134, loss 0.587734.
Train: 2018-08-05T23:59:35.187806: step 12135, loss 0.562426.
Train: 2018-08-05T23:59:35.351048: step 12136, loss 0.520111.
Train: 2018-08-05T23:59:35.507290: step 12137, loss 0.545457.
Train: 2018-08-05T23:59:35.680924: step 12138, loss 0.562406.
Train: 2018-08-05T23:59:35.837129: step 12139, loss 0.587966.
Train: 2018-08-05T23:59:35.991628: step 12140, loss 0.502666.
Test: 2018-08-05T23:59:36.238583: step 12140, loss 0.54807.
Train: 2018-08-05T23:59:36.401163: step 12141, loss 0.579507.
Train: 2018-08-05T23:59:36.560766: step 12142, loss 0.553835.
Train: 2018-08-05T23:59:36.715055: step 12143, loss 0.562403.
Train: 2018-08-05T23:59:36.867964: step 12144, loss 0.51938.
Train: 2018-08-05T23:59:37.024175: step 12145, loss 0.640005.
Train: 2018-08-05T23:59:37.187634: step 12146, loss 0.510637.
Train: 2018-08-05T23:59:37.367782: step 12147, loss 0.57969.
Train: 2018-08-05T23:59:37.523996: step 12148, loss 0.536448.
Train: 2018-08-05T23:59:37.688320: step 12149, loss 0.579763.
Train: 2018-08-05T23:59:37.848420: step 12150, loss 0.519052.
Test: 2018-08-05T23:59:38.079418: step 12150, loss 0.547871.
Train: 2018-08-05T23:59:38.232481: step 12151, loss 0.492953.
Train: 2018-08-05T23:59:38.395706: step 12152, loss 0.55372.
Train: 2018-08-05T23:59:38.551889: step 12153, loss 0.536261.
Train: 2018-08-05T23:59:38.703130: step 12154, loss 0.527438.
Train: 2018-08-05T23:59:38.865933: step 12155, loss 0.553685.
Train: 2018-08-05T23:59:39.014210: step 12156, loss 0.500869.
Train: 2018-08-05T23:59:39.178658: step 12157, loss 0.553668.
Train: 2018-08-05T23:59:39.342126: step 12158, loss 0.562522.
Train: 2018-08-05T23:59:39.488894: step 12159, loss 0.54478.
Train: 2018-08-05T23:59:39.652798: step 12160, loss 0.464616.
Test: 2018-08-05T23:59:39.896141: step 12160, loss 0.547625.
Train: 2018-08-05T23:59:40.056937: step 12161, loss 0.571538.
Train: 2018-08-05T23:59:40.200722: step 12162, loss 0.508813.
Train: 2018-08-05T23:59:40.373192: step 12163, loss 0.598668.
Train: 2018-08-05T23:59:40.533126: step 12164, loss 0.544663.
Train: 2018-08-05T23:59:40.681006: step 12165, loss 0.571756.
Train: 2018-08-05T23:59:40.837251: step 12166, loss 0.517466.
Train: 2018-08-05T23:59:40.985386: step 12167, loss 0.608137.
Train: 2018-08-05T23:59:41.143138: step 12168, loss 0.571869.
Train: 2018-08-05T23:59:41.299351: step 12169, loss 0.562795.
Train: 2018-08-05T23:59:41.470992: step 12170, loss 0.608204.
Test: 2018-08-05T23:59:41.700852: step 12170, loss 0.547573.
Train: 2018-08-05T23:59:41.857064: step 12171, loss 0.544621.
Train: 2018-08-05T23:59:42.019870: step 12172, loss 0.526483.
Train: 2018-08-05T23:59:42.176076: step 12173, loss 0.517434.
Train: 2018-08-05T23:59:42.324963: step 12174, loss 0.481149.
Train: 2018-08-05T23:59:42.491561: step 12175, loss 0.544622.
Train: 2018-08-05T23:59:42.651877: step 12176, loss 0.517333.
Train: 2018-08-05T23:59:42.799864: step 12177, loss 0.571935.
Train: 2018-08-05T23:59:42.962932: step 12178, loss 0.535484.
Train: 2018-08-05T23:59:43.123622: step 12179, loss 0.590247.
Train: 2018-08-05T23:59:43.281062: step 12180, loss 0.48979.
Test: 2018-08-05T23:59:43.519426: step 12180, loss 0.547569.
Train: 2018-08-05T23:59:43.672799: step 12181, loss 0.544596.
Train: 2018-08-05T23:59:43.833138: step 12182, loss 0.599542.
Train: 2018-08-05T23:59:43.996419: step 12183, loss 0.517126.
Train: 2018-08-05T23:59:44.152633: step 12184, loss 0.590437.
Train: 2018-08-05T23:59:44.308849: step 12185, loss 0.617908.
Train: 2018-08-05T23:59:44.466094: step 12186, loss 0.544604.
Train: 2018-08-05T23:59:44.634943: step 12187, loss 0.553747.
Train: 2018-08-05T23:59:44.788228: step 12188, loss 0.581137.
Train: 2018-08-05T23:59:44.949782: step 12189, loss 0.59931.
Train: 2018-08-05T23:59:45.106006: step 12190, loss 0.608308.
Test: 2018-08-05T23:59:45.348153: step 12190, loss 0.547574.
Train: 2018-08-05T23:59:45.502504: step 12191, loss 0.544628.
Train: 2018-08-05T23:59:45.650778: step 12192, loss 0.562725.
Train: 2018-08-05T23:59:45.815936: step 12193, loss 0.57171.
Train: 2018-08-05T23:59:45.979812: step 12194, loss 0.571653.
Train: 2018-08-05T23:59:46.136051: step 12195, loss 0.580563.
Train: 2018-08-05T23:59:46.285555: step 12196, loss 0.607297.
Train: 2018-08-05T23:59:46.444531: step 12197, loss 0.616019.
Train: 2018-08-05T23:59:46.602109: step 12198, loss 0.63346.
Train: 2018-08-05T23:59:46.754789: step 12199, loss 0.606577.
Train: 2018-08-05T23:59:46.911002: step 12200, loss 0.606282.
Test: 2018-08-05T23:59:47.157983: step 12200, loss 0.547836.
Train: 2018-08-05T23:59:47.867237: step 12201, loss 0.614674.
Train: 2018-08-05T23:59:48.032085: step 12202, loss 0.553752.
Train: 2018-08-05T23:59:48.192613: step 12203, loss 0.553821.
Train: 2018-08-05T23:59:48.353708: step 12204, loss 0.545297.
Train: 2018-08-05T23:59:48.500843: step 12205, loss 0.604963.
Train: 2018-08-05T23:59:48.665675: step 12206, loss 0.53699.
Train: 2018-08-05T23:59:48.822049: step 12207, loss 0.511751.
Train: 2018-08-05T23:59:48.969170: step 12208, loss 0.528709.
Train: 2018-08-05T23:59:49.132929: step 12209, loss 0.604512.
Train: 2018-08-05T23:59:49.296114: step 12210, loss 0.512088.
Test: 2018-08-05T23:59:49.533100: step 12210, loss 0.548403.
Train: 2018-08-05T23:59:49.689315: step 12211, loss 0.579272.
Train: 2018-08-05T23:59:49.843514: step 12212, loss 0.587579.
Train: 2018-08-05T23:59:49.999758: step 12213, loss 0.554096.
Train: 2018-08-05T23:59:50.153200: step 12214, loss 0.60432.
Train: 2018-08-05T23:59:50.297474: step 12215, loss 0.554121.
Train: 2018-08-05T23:59:50.462042: step 12216, loss 0.504032.
Train: 2018-08-05T23:59:50.618285: step 12217, loss 0.545775.
Train: 2018-08-05T23:59:50.766329: step 12218, loss 0.60427.
Train: 2018-08-05T23:59:50.929800: step 12219, loss 0.604277.
Train: 2018-08-05T23:59:51.093288: step 12220, loss 0.587525.
Test: 2018-08-05T23:59:51.320386: step 12220, loss 0.548508.
Train: 2018-08-05T23:59:51.492212: step 12221, loss 0.670948.
Train: 2018-08-05T23:59:51.641385: step 12222, loss 0.562487.
Train: 2018-08-05T23:59:51.789276: step 12223, loss 0.612325.
Train: 2018-08-05T23:59:51.942950: step 12224, loss 0.545998.
Train: 2018-08-05T23:59:52.102760: step 12225, loss 0.636831.
Train: 2018-08-05T23:59:52.262509: step 12226, loss 0.480406.
Train: 2018-08-05T23:59:52.410122: step 12227, loss 0.579032.
Train: 2018-08-05T23:59:52.575874: step 12228, loss 0.521614.
Train: 2018-08-05T23:59:52.716500: step 12229, loss 0.55442.
Train: 2018-08-05T23:59:52.879425: step 12230, loss 0.570854.
Test: 2018-08-05T23:59:53.113060: step 12230, loss 0.548874.
Train: 2018-08-05T23:59:53.277411: step 12231, loss 0.527572.
Train: 2018-08-05T23:59:53.433998: step 12232, loss 0.562589.
Train: 2018-08-05T23:59:53.586592: step 12233, loss 0.587268.
Train: 2018-08-05T23:59:53.733343: step 12234, loss 0.513211.
Train: 2018-08-05T23:59:53.896193: step 12235, loss 0.570804.
Train: 2018-08-05T23:59:54.043532: step 12236, loss 0.562549.
Train: 2018-08-05T23:59:54.214137: step 12237, loss 0.579099.
Train: 2018-08-05T23:59:54.398935: step 12238, loss 0.512783.
Train: 2018-08-05T23:59:54.550657: step 12239, loss 0.537585.
Train: 2018-08-05T23:59:54.698340: step 12240, loss 0.512489.
Test: 2018-08-05T23:59:54.949938: step 12240, loss 0.548457.
Train: 2018-08-05T23:59:55.106173: step 12241, loss 0.545733.
Train: 2018-08-05T23:59:55.260597: step 12242, loss 0.495268.
Train: 2018-08-05T23:59:55.415764: step 12243, loss 0.55399.
Train: 2018-08-05T23:59:55.572007: step 12244, loss 0.570882.
Train: 2018-08-05T23:59:55.714775: step 12245, loss 0.553893.
Train: 2018-08-05T23:59:55.870958: step 12246, loss 0.570941.
Train: 2018-08-05T23:59:56.018783: step 12247, loss 0.630956.
Train: 2018-08-05T23:59:56.190648: step 12248, loss 0.562397.
Train: 2018-08-05T23:59:56.349596: step 12249, loss 0.59678.
Train: 2018-08-05T23:59:56.509193: step 12250, loss 0.545199.
Test: 2018-08-05T23:59:56.734709: step 12250, loss 0.547981.
Train: 2018-08-05T23:59:56.897300: step 12251, loss 0.510742.
Train: 2018-08-05T23:59:57.046247: step 12252, loss 0.614142.
Train: 2018-08-05T23:59:57.209748: step 12253, loss 0.527893.
Train: 2018-08-05T23:59:57.372403: step 12254, loss 0.553765.
Train: 2018-08-05T23:59:57.520311: step 12255, loss 0.579706.
Train: 2018-08-05T23:59:57.687901: step 12256, loss 0.536445.
Train: 2018-08-05T23:59:57.833901: step 12257, loss 0.588395.
Train: 2018-08-05T23:59:57.997494: step 12258, loss 0.61441.
Train: 2018-08-05T23:59:58.146293: step 12259, loss 0.605712.
Train: 2018-08-05T23:59:58.300912: step 12260, loss 0.527812.
Test: 2018-08-05T23:59:58.543528: step 12260, loss 0.547928.
Train: 2018-08-05T23:59:58.706995: step 12261, loss 0.527828.
Train: 2018-08-05T23:59:58.864255: step 12262, loss 0.588327.
Train: 2018-08-05T23:59:59.013047: step 12263, loss 0.588306.
Train: 2018-08-05T23:59:59.177523: step 12264, loss 0.622814.
Train: 2018-08-05T23:59:59.332790: step 12265, loss 0.648516.
Train: 2018-08-05T23:59:59.506471: step 12266, loss 0.519494.
Train: 2018-08-05T23:59:59.652315: step 12267, loss 0.536705.
Train: 2018-08-05T23:59:59.815751: step 12268, loss 0.562395.
Train: 2018-08-05T23:59:59.981598: step 12269, loss 0.511182.
Train: 2018-08-06T00:00:00.137817: step 12270, loss 0.528266.
Test: 2018-08-06T00:00:00.385496: step 12270, loss 0.548104.
Train: 2018-08-06T00:00:00.549941: step 12271, loss 0.519724.
Train: 2018-08-06T00:00:00.717306: step 12272, loss 0.536761.
Train: 2018-08-06T00:00:00.860020: step 12273, loss 0.562395.
Train: 2018-08-06T00:00:01.027029: step 12274, loss 0.579542.
Train: 2018-08-06T00:00:01.170169: step 12275, loss 0.562388.
Train: 2018-08-06T00:00:01.345340: step 12276, loss 0.562388.
Train: 2018-08-06T00:00:01.486909: step 12277, loss 0.588179.
Train: 2018-08-06T00:00:01.640139: step 12278, loss 0.485001.
Train: 2018-08-06T00:00:01.793583: step 12279, loss 0.56241.
Train: 2018-08-06T00:00:01.965418: step 12280, loss 0.614158.
Test: 2018-08-06T00:00:02.203609: step 12280, loss 0.547949.
Train: 2018-08-06T00:00:02.359851: step 12281, loss 0.579657.
Train: 2018-08-06T00:00:02.529689: step 12282, loss 0.553769.
Train: 2018-08-06T00:00:02.677559: step 12283, loss 0.605536.
Train: 2018-08-06T00:00:02.842123: step 12284, loss 0.57964.
Train: 2018-08-06T00:00:02.998308: step 12285, loss 0.571002.
Train: 2018-08-06T00:00:03.146357: step 12286, loss 0.605425.
Train: 2018-08-06T00:00:03.338122: step 12287, loss 0.570986.
Train: 2018-08-06T00:00:03.503650: step 12288, loss 0.58812.
Train: 2018-08-06T00:00:03.658235: step 12289, loss 0.588067.
Train: 2018-08-06T00:00:03.812475: step 12290, loss 0.536809.
Test: 2018-08-06T00:00:04.052708: step 12290, loss 0.548137.
Train: 2018-08-06T00:00:04.196098: step 12291, loss 0.579433.
Train: 2018-08-06T00:00:04.359301: step 12292, loss 0.596407.
Train: 2018-08-06T00:00:04.523161: step 12293, loss 0.630275.
Train: 2018-08-06T00:00:04.685514: step 12294, loss 0.511691.
Train: 2018-08-06T00:00:04.842227: step 12295, loss 0.520244.
Train: 2018-08-06T00:00:04.989918: step 12296, loss 0.528704.
Train: 2018-08-06T00:00:05.138866: step 12297, loss 0.50342.
Train: 2018-08-06T00:00:05.315267: step 12298, loss 0.545551.
Train: 2018-08-06T00:00:05.458396: step 12299, loss 0.520139.
Train: 2018-08-06T00:00:05.616291: step 12300, loss 0.536989.
Test: 2018-08-06T00:00:05.866204: step 12300, loss 0.548178.
Train: 2018-08-06T00:00:06.584773: step 12301, loss 0.562414.
Train: 2018-08-06T00:00:06.730201: step 12302, loss 0.596466.
Train: 2018-08-06T00:00:06.883490: step 12303, loss 0.596525.
Train: 2018-08-06T00:00:07.039704: step 12304, loss 0.536794.
Train: 2018-08-06T00:00:07.202888: step 12305, loss 0.622214.
Train: 2018-08-06T00:00:07.352792: step 12306, loss 0.545303.
Train: 2018-08-06T00:00:07.509008: step 12307, loss 0.553851.
Train: 2018-08-06T00:00:07.672572: step 12308, loss 0.562395.
Train: 2018-08-06T00:00:07.820446: step 12309, loss 0.639386.
Train: 2018-08-06T00:00:08.007898: step 12310, loss 0.519672.
Test: 2018-08-06T00:00:08.234256: step 12310, loss 0.548091.
Train: 2018-08-06T00:00:08.398162: step 12311, loss 0.605106.
Train: 2018-08-06T00:00:08.554350: step 12312, loss 0.59653.
Train: 2018-08-06T00:00:08.719472: step 12313, loss 0.630562.
Train: 2018-08-06T00:00:08.866723: step 12314, loss 0.553913.
Train: 2018-08-06T00:00:09.029679: step 12315, loss 0.570902.
Train: 2018-08-06T00:00:09.185898: step 12316, loss 0.520113.
Train: 2018-08-06T00:00:09.333541: step 12317, loss 0.604692.
Train: 2018-08-06T00:00:09.498180: step 12318, loss 0.537097.
Train: 2018-08-06T00:00:09.661376: step 12319, loss 0.537137.
Train: 2018-08-06T00:00:09.817595: step 12320, loss 0.638288.
Test: 2018-08-06T00:00:10.052776: step 12320, loss 0.548345.
Train: 2018-08-06T00:00:10.222642: step 12321, loss 0.570863.
Train: 2018-08-06T00:00:10.363265: step 12322, loss 0.554042.
Train: 2018-08-06T00:00:10.532122: step 12323, loss 0.570837.
Train: 2018-08-06T00:00:10.691993: step 12324, loss 0.612753.
Train: 2018-08-06T00:00:10.903343: step 12325, loss 0.51227.
Train: 2018-08-06T00:00:11.061363: step 12326, loss 0.503961.
Train: 2018-08-06T00:00:11.206480: step 12327, loss 0.56246.
Train: 2018-08-06T00:00:11.370159: step 12328, loss 0.57084.
Train: 2018-08-06T00:00:11.533725: step 12329, loss 0.537323.
Train: 2018-08-06T00:00:11.682661: step 12330, loss 0.537288.
Test: 2018-08-06T00:00:11.924962: step 12330, loss 0.548371.
Train: 2018-08-06T00:00:12.065758: step 12331, loss 0.537241.
Train: 2018-08-06T00:00:12.235994: step 12332, loss 0.596118.
Train: 2018-08-06T00:00:12.399413: step 12333, loss 0.587713.
Train: 2018-08-06T00:00:12.562813: step 12334, loss 0.596176.
Train: 2018-08-06T00:00:12.721548: step 12335, loss 0.613059.
Train: 2018-08-06T00:00:12.877795: step 12336, loss 0.511836.
Train: 2018-08-06T00:00:13.035183: step 12337, loss 0.570875.
Train: 2018-08-06T00:00:13.180006: step 12338, loss 0.511817.
Train: 2018-08-06T00:00:13.360287: step 12339, loss 0.638436.
Train: 2018-08-06T00:00:13.515870: step 12340, loss 0.6722.
Test: 2018-08-06T00:00:13.753481: step 12340, loss 0.548328.
Train: 2018-08-06T00:00:13.909663: step 12341, loss 0.537169.
Train: 2018-08-06T00:00:14.073599: step 12342, loss 0.520395.
Train: 2018-08-06T00:00:14.237085: step 12343, loss 0.579257.
Train: 2018-08-06T00:00:14.400561: step 12344, loss 0.646434.
Train: 2018-08-06T00:00:14.556806: step 12345, loss 0.595975.
Train: 2018-08-06T00:00:14.720294: step 12346, loss 0.61262.
Train: 2018-08-06T00:00:14.883893: step 12347, loss 0.562483.
Train: 2018-08-06T00:00:15.031723: step 12348, loss 0.562508.
Train: 2018-08-06T00:00:15.187937: step 12349, loss 0.57081.
Train: 2018-08-06T00:00:15.351713: step 12350, loss 0.504657.
Test: 2018-08-06T00:00:15.585089: step 12350, loss 0.548707.
Train: 2018-08-06T00:00:15.747904: step 12351, loss 0.488163.
Train: 2018-08-06T00:00:15.895773: step 12352, loss 0.554265.
Train: 2018-08-06T00:00:16.059396: step 12353, loss 0.61224.
Train: 2018-08-06T00:00:16.215612: step 12354, loss 0.587396.
Train: 2018-08-06T00:00:16.383983: step 12355, loss 0.504501.
Train: 2018-08-06T00:00:16.540539: step 12356, loss 0.587422.
Train: 2018-08-06T00:00:16.695126: step 12357, loss 0.595734.
Train: 2018-08-06T00:00:16.858120: step 12358, loss 0.520962.
Train: 2018-08-06T00:00:17.014335: step 12359, loss 0.512584.
Train: 2018-08-06T00:00:17.162390: step 12360, loss 0.612514.
Test: 2018-08-06T00:00:17.405791: step 12360, loss 0.548498.
Train: 2018-08-06T00:00:17.572705: step 12361, loss 0.604221.
Train: 2018-08-06T00:00:17.725086: step 12362, loss 0.520723.
Train: 2018-08-06T00:00:17.883981: step 12363, loss 0.50395.
Train: 2018-08-06T00:00:18.044840: step 12364, loss 0.587593.
Train: 2018-08-06T00:00:18.208297: step 12365, loss 0.545657.
Train: 2018-08-06T00:00:18.358453: step 12366, loss 0.495147.
Train: 2018-08-06T00:00:18.524836: step 12367, loss 0.511791.
Train: 2018-08-06T00:00:18.677727: step 12368, loss 0.570889.
Train: 2018-08-06T00:00:18.824995: step 12369, loss 0.468875.
Train: 2018-08-06T00:00:18.995162: step 12370, loss 0.622224.
Test: 2018-08-06T00:00:19.222165: step 12370, loss 0.548038.
Train: 2018-08-06T00:00:19.385409: step 12371, loss 0.553821.
Train: 2018-08-06T00:00:19.557212: step 12372, loss 0.579598.
Train: 2018-08-06T00:00:19.698692: step 12373, loss 0.527912.
Train: 2018-08-06T00:00:19.870560: step 12374, loss 0.571046.
Train: 2018-08-06T00:00:20.024165: step 12375, loss 0.536422.
Train: 2018-08-06T00:00:20.193024: step 12376, loss 0.562421.
Train: 2018-08-06T00:00:20.333616: step 12377, loss 0.545008.
Train: 2018-08-06T00:00:20.493597: step 12378, loss 0.518793.
Train: 2018-08-06T00:00:20.657221: step 12379, loss 0.501163.
Train: 2018-08-06T00:00:20.806071: step 12380, loss 0.536096.
Test: 2018-08-06T00:00:21.047728: step 12380, loss 0.547718.
Train: 2018-08-06T00:00:21.212127: step 12381, loss 0.51841.
Train: 2018-08-06T00:00:21.368364: step 12382, loss 0.524771.
Train: 2018-08-06T00:00:21.516316: step 12383, loss 0.544758.
Train: 2018-08-06T00:00:21.679839: step 12384, loss 0.535817.
Train: 2018-08-06T00:00:21.843330: step 12385, loss 0.517852.
Train: 2018-08-06T00:00:21.991241: step 12386, loss 0.544669.
Train: 2018-08-06T00:00:22.154012: step 12387, loss 0.526594.
Train: 2018-08-06T00:00:22.317461: step 12388, loss 0.526524.
Train: 2018-08-06T00:00:22.465401: step 12389, loss 0.562805.
Train: 2018-08-06T00:00:22.637258: step 12390, loss 0.617539.
Test: 2018-08-06T00:00:22.872402: step 12390, loss 0.547569.
Train: 2018-08-06T00:00:23.020908: step 12391, loss 0.508084.
Train: 2018-08-06T00:00:23.192042: step 12392, loss 0.61781.
Train: 2018-08-06T00:00:23.349550: step 12393, loss 0.572062.
Train: 2018-08-06T00:00:23.510152: step 12394, loss 0.590368.
Train: 2018-08-06T00:00:23.666726: step 12395, loss 0.590342.
Train: 2018-08-06T00:00:23.812094: step 12396, loss 0.63603.
Train: 2018-08-06T00:00:23.975072: step 12397, loss 0.544587.
Train: 2018-08-06T00:00:24.153483: step 12398, loss 0.526407.
Train: 2018-08-06T00:00:24.316288: step 12399, loss 0.60819.
Train: 2018-08-06T00:00:24.484829: step 12400, loss 0.562751.
Test: 2018-08-06T00:00:24.721559: step 12400, loss 0.547583.
Train: 2018-08-06T00:00:25.448395: step 12401, loss 0.508515.
Train: 2018-08-06T00:00:25.612003: step 12402, loss 0.589764.
Train: 2018-08-06T00:00:25.775185: step 12403, loss 0.53567.
Train: 2018-08-06T00:00:25.931430: step 12404, loss 0.517703.
Train: 2018-08-06T00:00:26.089471: step 12405, loss 0.535689.
Train: 2018-08-06T00:00:26.245654: step 12406, loss 0.562641.
Train: 2018-08-06T00:00:26.420091: step 12407, loss 0.526753.
Train: 2018-08-06T00:00:26.583622: step 12408, loss 0.616415.
Train: 2018-08-06T00:00:26.738209: step 12409, loss 0.508887.
Train: 2018-08-06T00:00:26.902859: step 12410, loss 0.526786.
Test: 2018-08-06T00:00:27.130948: step 12410, loss 0.547616.
Train: 2018-08-06T00:00:27.302776: step 12411, loss 0.571581.
Train: 2018-08-06T00:00:27.450752: step 12412, loss 0.517865.
Train: 2018-08-06T00:00:27.614390: step 12413, loss 0.589473.
Train: 2018-08-06T00:00:27.770570: step 12414, loss 0.553663.
Train: 2018-08-06T00:00:27.934329: step 12415, loss 0.598436.
Train: 2018-08-06T00:00:28.097354: step 12416, loss 0.652034.
Train: 2018-08-06T00:00:28.260850: step 12417, loss 0.535817.
Train: 2018-08-06T00:00:28.414439: step 12418, loss 0.62484.
Train: 2018-08-06T00:00:28.586274: step 12419, loss 0.553659.
Train: 2018-08-06T00:00:28.734278: step 12420, loss 0.535975.
Test: 2018-08-06T00:00:28.976061: step 12420, loss 0.547712.
Train: 2018-08-06T00:00:29.139540: step 12421, loss 0.536021.
Train: 2018-08-06T00:00:29.303081: step 12422, loss 0.580081.
Train: 2018-08-06T00:00:29.456946: step 12423, loss 0.527295.
Train: 2018-08-06T00:00:29.628781: step 12424, loss 0.553681.
Train: 2018-08-06T00:00:29.792331: step 12425, loss 0.536143.
Train: 2018-08-06T00:00:29.955921: step 12426, loss 0.53615.
Train: 2018-08-06T00:00:30.118039: step 12427, loss 0.544922.
Train: 2018-08-06T00:00:30.277466: step 12428, loss 0.685172.
Train: 2018-08-06T00:00:30.431961: step 12429, loss 0.571187.
Train: 2018-08-06T00:00:30.594959: step 12430, loss 0.63225.
Test: 2018-08-06T00:00:30.836376: step 12430, loss 0.547853.
Train: 2018-08-06T00:00:30.999758: step 12431, loss 0.614605.
Train: 2018-08-06T00:00:31.165513: step 12432, loss 0.553748.
Train: 2018-08-06T00:00:31.333953: step 12433, loss 0.553775.
Train: 2018-08-06T00:00:31.489332: step 12434, loss 0.579597.
Train: 2018-08-06T00:00:31.669830: step 12435, loss 0.596696.
Train: 2018-08-06T00:00:31.820185: step 12436, loss 0.570952.
Train: 2018-08-06T00:00:31.999339: step 12437, loss 0.52834.
Train: 2018-08-06T00:00:32.172390: step 12438, loss 0.630382.
Train: 2018-08-06T00:00:32.319158: step 12439, loss 0.604755.
Train: 2018-08-06T00:00:32.482868: step 12440, loss 0.528687.
Test: 2018-08-06T00:00:32.716394: step 12440, loss 0.548351.
Train: 2018-08-06T00:00:32.878922: step 12441, loss 0.570851.
Train: 2018-08-06T00:00:33.073621: step 12442, loss 0.570843.
Train: 2018-08-06T00:00:33.222592: step 12443, loss 0.51224.
Train: 2018-08-06T00:00:33.396024: step 12444, loss 0.579194.
Train: 2018-08-06T00:00:33.557591: step 12445, loss 0.620948.
Train: 2018-08-06T00:00:33.727305: step 12446, loss 0.562488.
Train: 2018-08-06T00:00:33.875147: step 12447, loss 0.49593.
Train: 2018-08-06T00:00:34.038984: step 12448, loss 0.579143.
Train: 2018-08-06T00:00:34.195228: step 12449, loss 0.454369.
Train: 2018-08-06T00:00:34.358231: step 12450, loss 0.595829.
Test: 2018-08-06T00:00:34.602465: step 12450, loss 0.5485.
Train: 2018-08-06T00:00:34.749706: step 12451, loss 0.579175.
Train: 2018-08-06T00:00:34.912562: step 12452, loss 0.612607.
Train: 2018-08-06T00:00:35.081108: step 12453, loss 0.637668.
Train: 2018-08-06T00:00:35.232096: step 12454, loss 0.495737.
Train: 2018-08-06T00:00:35.394899: step 12455, loss 0.595861.
Train: 2018-08-06T00:00:35.558157: step 12456, loss 0.570823.
Train: 2018-08-06T00:00:35.714346: step 12457, loss 0.504118.
Train: 2018-08-06T00:00:35.874252: step 12458, loss 0.512401.
Train: 2018-08-06T00:00:36.046087: step 12459, loss 0.621012.
Train: 2018-08-06T00:00:36.200519: step 12460, loss 0.562466.
Test: 2018-08-06T00:00:36.446412: step 12460, loss 0.548432.
Train: 2018-08-06T00:00:36.622970: step 12461, loss 0.57921.
Train: 2018-08-06T00:00:36.774651: step 12462, loss 0.545696.
Train: 2018-08-06T00:00:36.941251: step 12463, loss 0.604387.
Train: 2018-08-06T00:00:37.094924: step 12464, loss 0.579226.
Train: 2018-08-06T00:00:37.251169: step 12465, loss 0.587609.
Train: 2018-08-06T00:00:37.414405: step 12466, loss 0.545695.
Train: 2018-08-06T00:00:37.565236: step 12467, loss 0.554077.
Train: 2018-08-06T00:00:37.744290: step 12468, loss 0.604363.
Train: 2018-08-06T00:00:37.900503: step 12469, loss 0.570836.
Train: 2018-08-06T00:00:38.055732: step 12470, loss 0.587581.
Test: 2018-08-06T00:00:38.297400: step 12470, loss 0.548463.
Train: 2018-08-06T00:00:38.460876: step 12471, loss 0.562477.
Train: 2018-08-06T00:00:38.608853: step 12472, loss 0.470548.
Train: 2018-08-06T00:00:38.790175: step 12473, loss 0.58757.
Train: 2018-08-06T00:00:38.945978: step 12474, loss 0.604338.
Train: 2018-08-06T00:00:39.100634: step 12475, loss 0.503834.
Train: 2018-08-06T00:00:39.279786: step 12476, loss 0.52053.
Train: 2018-08-06T00:00:39.427751: step 12477, loss 0.579252.
Train: 2018-08-06T00:00:39.595453: step 12478, loss 0.579273.
Train: 2018-08-06T00:00:39.747158: step 12479, loss 0.503438.
Train: 2018-08-06T00:00:39.897357: step 12480, loss 0.587767.
Test: 2018-08-06T00:00:40.138388: step 12480, loss 0.548246.
Train: 2018-08-06T00:00:40.303146: step 12481, loss 0.570881.
Train: 2018-08-06T00:00:40.472498: step 12482, loss 0.596306.
Train: 2018-08-06T00:00:40.628523: step 12483, loss 0.553933.
Train: 2018-08-06T00:00:40.792288: step 12484, loss 0.511494.
Train: 2018-08-06T00:00:40.939908: step 12485, loss 0.562407.
Train: 2018-08-06T00:00:41.114393: step 12486, loss 0.545375.
Train: 2018-08-06T00:00:41.270575: step 12487, loss 0.553868.
Train: 2018-08-06T00:00:41.440410: step 12488, loss 0.519667.
Train: 2018-08-06T00:00:41.634379: step 12489, loss 0.579535.
Train: 2018-08-06T00:00:41.796712: step 12490, loss 0.570981.
Test: 2018-08-06T00:00:42.025895: step 12490, loss 0.547998.
Train: 2018-08-06T00:00:42.197729: step 12491, loss 0.553798.
Train: 2018-08-06T00:00:42.361276: step 12492, loss 0.57962.
Train: 2018-08-06T00:00:42.524694: step 12493, loss 0.55378.
Train: 2018-08-06T00:00:42.686042: step 12494, loss 0.519254.
Train: 2018-08-06T00:00:42.844404: step 12495, loss 0.571048.
Train: 2018-08-06T00:00:43.003559: step 12496, loss 0.605691.
Train: 2018-08-06T00:00:43.166999: step 12497, loss 0.579727.
Train: 2018-08-06T00:00:43.344744: step 12498, loss 0.545086.
Train: 2018-08-06T00:00:43.503326: step 12499, loss 0.510432.
Train: 2018-08-06T00:00:43.667855: step 12500, loss 0.553737.
Test: 2018-08-06T00:00:43.906587: step 12500, loss 0.547871.
Train: 2018-08-06T00:00:44.626888: step 12501, loss 0.571095.
Train: 2018-08-06T00:00:44.789876: step 12502, loss 0.536347.
Train: 2018-08-06T00:00:44.966563: step 12503, loss 0.518911.
Train: 2018-08-06T00:00:45.125259: step 12504, loss 0.632166.
Train: 2018-08-06T00:00:45.288596: step 12505, loss 0.649627.
Train: 2018-08-06T00:00:45.463126: step 12506, loss 0.510181.
Train: 2018-08-06T00:00:45.619334: step 12507, loss 0.484087.
Train: 2018-08-06T00:00:45.789167: step 12508, loss 0.640838.
Train: 2018-08-06T00:00:45.958747: step 12509, loss 0.553717.
Train: 2018-08-06T00:00:46.114416: step 12510, loss 0.518901.
Test: 2018-08-06T00:00:46.354267: step 12510, loss 0.54784.
Train: 2018-08-06T00:00:46.515835: step 12511, loss 0.579835.
Train: 2018-08-06T00:00:46.681420: step 12512, loss 0.562422.
Train: 2018-08-06T00:00:46.841737: step 12513, loss 0.536309.
Train: 2018-08-06T00:00:47.005854: step 12514, loss 0.553717.
Train: 2018-08-06T00:00:47.162067: step 12515, loss 0.597265.
Train: 2018-08-06T00:00:47.327641: step 12516, loss 0.614671.
Train: 2018-08-06T00:00:47.491123: step 12517, loss 0.527628.
Train: 2018-08-06T00:00:47.654614: step 12518, loss 0.518966.
Train: 2018-08-06T00:00:47.824728: step 12519, loss 0.501575.
Train: 2018-08-06T00:00:47.980940: step 12520, loss 0.562421.
Test: 2018-08-06T00:00:48.222687: step 12520, loss 0.547834.
Train: 2018-08-06T00:00:48.386306: step 12521, loss 0.553713.
Train: 2018-08-06T00:00:48.550345: step 12522, loss 0.562427.
Train: 2018-08-06T00:00:48.713755: step 12523, loss 0.562435.
Train: 2018-08-06T00:00:48.869312: step 12524, loss 0.536231.
Train: 2018-08-06T00:00:49.042327: step 12525, loss 0.597418.
Train: 2018-08-06T00:00:49.204821: step 12526, loss 0.614918.
Train: 2018-08-06T00:00:49.367865: step 12527, loss 0.527481.
Train: 2018-08-06T00:00:49.549638: step 12528, loss 0.52749.
Train: 2018-08-06T00:00:49.714361: step 12529, loss 0.509999.
Train: 2018-08-06T00:00:49.870575: step 12530, loss 0.588691.
Test: 2018-08-06T00:00:50.124777: step 12530, loss 0.547782.
Train: 2018-08-06T00:00:50.274548: step 12531, loss 0.553689.
Train: 2018-08-06T00:00:50.437719: step 12532, loss 0.553687.
Train: 2018-08-06T00:00:50.600858: step 12533, loss 0.487646.
Train: 2018-08-06T00:00:50.773942: step 12534, loss 0.632719.
Train: 2018-08-06T00:00:50.932660: step 12535, loss 0.553674.
Train: 2018-08-06T00:00:51.088844: step 12536, loss 0.483388.
Train: 2018-08-06T00:00:51.243161: step 12537, loss 0.518477.
Train: 2018-08-06T00:00:51.412101: step 12538, loss 0.527215.
Train: 2018-08-06T00:00:51.571934: step 12539, loss 0.562505.
Train: 2018-08-06T00:00:51.746007: step 12540, loss 0.695414.
Test: 2018-08-06T00:00:51.977385: step 12540, loss 0.547686.
Train: 2018-08-06T00:00:52.133603: step 12541, loss 0.606775.
Train: 2018-08-06T00:00:52.297048: step 12542, loss 0.500631.
Train: 2018-08-06T00:00:52.460547: step 12543, loss 0.500654.
Train: 2018-08-06T00:00:52.624103: step 12544, loss 0.55366.
Train: 2018-08-06T00:00:52.780283: step 12545, loss 0.482886.
Train: 2018-08-06T00:00:52.959678: step 12546, loss 0.562518.
Train: 2018-08-06T00:00:53.115138: step 12547, loss 0.54478.
Train: 2018-08-06T00:00:53.293779: step 12548, loss 0.580328.
Train: 2018-08-06T00:00:53.462299: step 12549, loss 0.562553.
Train: 2018-08-06T00:00:53.626888: step 12550, loss 0.518039.
Test: 2018-08-06T00:00:53.851231: step 12550, loss 0.547639.
Train: 2018-08-06T00:00:54.025263: step 12551, loss 0.598235.
Train: 2018-08-06T00:00:54.178293: step 12552, loss 0.669593.
Train: 2018-08-06T00:00:54.341862: step 12553, loss 0.54475.
Train: 2018-08-06T00:00:54.505412: step 12554, loss 0.58921.
Train: 2018-08-06T00:00:54.668938: step 12555, loss 0.580273.
Train: 2018-08-06T00:00:54.840802: step 12556, loss 0.580217.
Train: 2018-08-06T00:00:55.004730: step 12557, loss 0.553663.
Train: 2018-08-06T00:00:55.172545: step 12558, loss 0.580097.
Train: 2018-08-06T00:00:55.322577: step 12559, loss 0.588834.
Train: 2018-08-06T00:00:55.486061: step 12560, loss 0.474792.
Test: 2018-08-06T00:00:55.727688: step 12560, loss 0.54778.
Train: 2018-08-06T00:00:55.902841: step 12561, loss 0.527416.
Train: 2018-08-06T00:00:56.063185: step 12562, loss 0.588707.
Train: 2018-08-06T00:00:56.218722: step 12563, loss 0.588682.
Train: 2018-08-06T00:00:56.391950: step 12564, loss 0.562433.
Train: 2018-08-06T00:00:56.564489: step 12565, loss 0.527536.
Train: 2018-08-06T00:00:56.729018: step 12566, loss 0.510113.
Train: 2018-08-06T00:00:56.892509: step 12567, loss 0.588595.
Train: 2018-08-06T00:00:57.057973: step 12568, loss 0.536269.
Train: 2018-08-06T00:00:57.226734: step 12569, loss 0.544982.
Train: 2018-08-06T00:00:57.391719: step 12570, loss 0.553704.
Test: 2018-08-06T00:00:57.617441: step 12570, loss 0.54781.
Train: 2018-08-06T00:00:57.796708: step 12571, loss 0.518777.
Train: 2018-08-06T00:00:57.965173: step 12572, loss 0.614888.
Train: 2018-08-06T00:00:58.117835: step 12573, loss 0.492499.
Train: 2018-08-06T00:00:58.281576: step 12574, loss 0.571198.
Train: 2018-08-06T00:00:58.446349: step 12575, loss 0.509888.
Train: 2018-08-06T00:00:58.601183: step 12576, loss 0.580006.
Train: 2018-08-06T00:00:58.778608: step 12577, loss 0.51854.
Train: 2018-08-06T00:00:58.936811: step 12578, loss 0.615267.
Train: 2018-08-06T00:00:59.100029: step 12579, loss 0.536068.
Train: 2018-08-06T00:00:59.258919: step 12580, loss 0.624146.
Test: 2018-08-06T00:00:59.508860: step 12580, loss 0.54773.
Train: 2018-08-06T00:00:59.678785: step 12581, loss 0.588891.
Train: 2018-08-06T00:00:59.833897: step 12582, loss 0.544878.
Train: 2018-08-06T00:01:00.009277: step 12583, loss 0.632761.
Train: 2018-08-06T00:01:00.164039: step 12584, loss 0.650145.
Train: 2018-08-06T00:01:00.320252: step 12585, loss 0.562436.
Train: 2018-08-06T00:01:00.483805: step 12586, loss 0.658211.
Train: 2018-08-06T00:01:00.648383: step 12587, loss 0.527746.
Train: 2018-08-06T00:01:00.820217: step 12588, loss 0.571032.
Train: 2018-08-06T00:01:00.983306: step 12589, loss 0.527991.
Train: 2018-08-06T00:01:01.146937: step 12590, loss 0.53666.
Test: 2018-08-06T00:01:01.372869: step 12590, loss 0.548058.
Train: 2018-08-06T00:01:01.536362: step 12591, loss 0.562397.
Train: 2018-08-06T00:01:01.706400: step 12592, loss 0.528208.
Train: 2018-08-06T00:01:01.878384: step 12593, loss 0.579482.
Train: 2018-08-06T00:01:02.041798: step 12594, loss 0.605057.
Train: 2018-08-06T00:01:02.205436: step 12595, loss 0.477238.
Train: 2018-08-06T00:01:02.368940: step 12596, loss 0.545368.
Train: 2018-08-06T00:01:02.533489: step 12597, loss 0.545356.
Train: 2018-08-06T00:01:02.696090: step 12598, loss 0.613571.
Train: 2018-08-06T00:01:02.860236: step 12599, loss 0.570926.
Train: 2018-08-06T00:01:03.022835: step 12600, loss 0.58797.
Test: 2018-08-06T00:01:03.263453: step 12600, loss 0.548142.
Train: 2018-08-06T00:01:04.032327: step 12601, loss 0.511316.
Train: 2018-08-06T00:01:04.192799: step 12602, loss 0.587956.
Train: 2018-08-06T00:01:04.361617: step 12603, loss 0.553887.
Train: 2018-08-06T00:01:04.531184: step 12604, loss 0.57092.
Train: 2018-08-06T00:01:04.690654: step 12605, loss 0.622009.
Train: 2018-08-06T00:01:04.860071: step 12606, loss 0.58792.
Train: 2018-08-06T00:01:05.009898: step 12607, loss 0.511469.
Train: 2018-08-06T00:01:05.174665: step 12608, loss 0.52846.
Train: 2018-08-06T00:01:05.345433: step 12609, loss 0.630337.
Train: 2018-08-06T00:01:05.510482: step 12610, loss 0.553929.
Test: 2018-08-06T00:01:05.735850: step 12610, loss 0.548216.
Train: 2018-08-06T00:01:05.914456: step 12611, loss 0.511551.
Train: 2018-08-06T00:01:06.063630: step 12612, loss 0.553928.
Train: 2018-08-06T00:01:06.240028: step 12613, loss 0.52846.
Train: 2018-08-06T00:01:06.408197: step 12614, loss 0.528421.
Train: 2018-08-06T00:01:06.570732: step 12615, loss 0.59646.
Train: 2018-08-06T00:01:06.744292: step 12616, loss 0.536832.
Train: 2018-08-06T00:01:06.893110: step 12617, loss 0.528262.
Train: 2018-08-06T00:01:07.056641: step 12618, loss 0.545295.
Train: 2018-08-06T00:01:07.227403: step 12619, loss 0.528112.
Train: 2018-08-06T00:01:07.405742: step 12620, loss 0.570991.
Test: 2018-08-06T00:01:07.644571: step 12620, loss 0.547976.
Train: 2018-08-06T00:01:07.807630: step 12621, loss 0.605458.
Train: 2018-08-06T00:01:07.973118: step 12622, loss 0.571023.
Train: 2018-08-06T00:01:08.129331: step 12623, loss 0.493369.
Train: 2018-08-06T00:01:08.308475: step 12624, loss 0.510526.
Train: 2018-08-06T00:01:08.472981: step 12625, loss 0.536404.
Train: 2018-08-06T00:01:08.645968: step 12626, loss 0.623292.
Train: 2018-08-06T00:01:08.808978: step 12627, loss 0.588546.
Train: 2018-08-06T00:01:08.963755: step 12628, loss 0.605997.
Train: 2018-08-06T00:01:09.127242: step 12629, loss 0.527577.
Train: 2018-08-06T00:01:09.321728: step 12630, loss 0.588574.
Test: 2018-08-06T00:01:09.547394: step 12630, loss 0.547832.
Train: 2018-08-06T00:01:09.728837: step 12631, loss 0.588561.
Train: 2018-08-06T00:01:09.908011: step 12632, loss 0.605954.
Train: 2018-08-06T00:01:10.073250: step 12633, loss 0.510254.
Train: 2018-08-06T00:01:10.253336: step 12634, loss 0.623241.
Train: 2018-08-06T00:01:10.429052: step 12635, loss 0.510366.
Train: 2018-08-06T00:01:10.592368: step 12636, loss 0.562409.
Train: 2018-08-06T00:01:10.757247: step 12637, loss 0.579743.
Train: 2018-08-06T00:01:10.931338: step 12638, loss 0.579727.
Train: 2018-08-06T00:01:11.092150: step 12639, loss 0.493218.
Train: 2018-08-06T00:01:11.261591: step 12640, loss 0.553752.
Test: 2018-08-06T00:01:11.502630: step 12640, loss 0.547914.
Train: 2018-08-06T00:01:11.667360: step 12641, loss 0.519136.
Train: 2018-08-06T00:01:11.830698: step 12642, loss 0.579736.
Train: 2018-08-06T00:01:12.008702: step 12643, loss 0.631758.
Train: 2018-08-06T00:01:12.176171: step 12644, loss 0.484455.
Train: 2018-08-06T00:01:12.352548: step 12645, loss 0.597087.
Train: 2018-08-06T00:01:12.517902: step 12646, loss 0.588411.
Train: 2018-08-06T00:01:12.686834: step 12647, loss 0.519096.
Train: 2018-08-06T00:01:12.846497: step 12648, loss 0.588401.
Train: 2018-08-06T00:01:13.009538: step 12649, loss 0.58839.
Train: 2018-08-06T00:01:13.188657: step 12650, loss 0.562403.
Test: 2018-08-06T00:01:13.426707: step 12650, loss 0.547922.
Train: 2018-08-06T00:01:13.601240: step 12651, loss 0.588342.
Train: 2018-08-06T00:01:13.772268: step 12652, loss 0.527858.
Train: 2018-08-06T00:01:13.934777: step 12653, loss 0.588297.
Train: 2018-08-06T00:01:14.098363: step 12654, loss 0.588276.
Train: 2018-08-06T00:01:14.293035: step 12655, loss 0.622687.
Train: 2018-08-06T00:01:14.463174: step 12656, loss 0.502255.
Train: 2018-08-06T00:01:14.626694: step 12657, loss 0.562396.
Train: 2018-08-06T00:01:14.798530: step 12658, loss 0.58812.
Train: 2018-08-06T00:01:14.962119: step 12659, loss 0.579528.
Train: 2018-08-06T00:01:15.141246: step 12660, loss 0.579497.
Test: 2018-08-06T00:01:15.389384: step 12660, loss 0.548105.
Train: 2018-08-06T00:01:15.552923: step 12661, loss 0.519722.
Train: 2018-08-06T00:01:15.716458: step 12662, loss 0.494163.
Train: 2018-08-06T00:01:15.880088: step 12663, loss 0.588013.
Train: 2018-08-06T00:01:16.051954: step 12664, loss 0.553871.
Train: 2018-08-06T00:01:16.215640: step 12665, loss 0.570942.
Train: 2018-08-06T00:01:16.379123: step 12666, loss 0.570947.
Train: 2018-08-06T00:01:16.555160: step 12667, loss 0.570941.
Train: 2018-08-06T00:01:16.723709: step 12668, loss 0.528233.
Train: 2018-08-06T00:01:16.882729: step 12669, loss 0.570947.
Train: 2018-08-06T00:01:17.077098: step 12670, loss 0.51965.
Test: 2018-08-06T00:01:17.316138: step 12670, loss 0.548061.
Train: 2018-08-06T00:01:17.482799: step 12671, loss 0.570959.
Train: 2018-08-06T00:01:17.657621: step 12672, loss 0.553824.
Train: 2018-08-06T00:01:17.815414: step 12673, loss 0.553822.
Train: 2018-08-06T00:01:17.994344: step 12674, loss 0.605326.
Train: 2018-08-06T00:01:18.162700: step 12675, loss 0.493699.
Train: 2018-08-06T00:01:18.317628: step 12676, loss 0.596795.
Train: 2018-08-06T00:01:18.496277: step 12677, loss 0.614025.
Train: 2018-08-06T00:01:18.659112: step 12678, loss 0.570992.
Train: 2018-08-06T00:01:18.828683: step 12679, loss 0.536617.
Train: 2018-08-06T00:01:18.997700: step 12680, loss 0.545208.
Test: 2018-08-06T00:01:19.233808: step 12680, loss 0.548.
Train: 2018-08-06T00:01:19.390011: step 12681, loss 0.562398.
Train: 2018-08-06T00:01:19.564035: step 12682, loss 0.527999.
Train: 2018-08-06T00:01:19.735841: step 12683, loss 0.605435.
Train: 2018-08-06T00:01:19.889386: step 12684, loss 0.525678.
Train: 2018-08-06T00:01:20.060153: step 12685, loss 0.605459.
Train: 2018-08-06T00:01:20.234994: step 12686, loss 0.510743.
Train: 2018-08-06T00:01:20.391207: step 12687, loss 0.527939.
Train: 2018-08-06T00:01:20.547420: step 12688, loss 0.579658.
Train: 2018-08-06T00:01:20.711901: step 12689, loss 0.579672.
Train: 2018-08-06T00:01:20.891109: step 12690, loss 0.588316.
Test: 2018-08-06T00:01:21.132821: step 12690, loss 0.547939.
Train: 2018-08-06T00:01:21.296279: step 12691, loss 0.55377.
Train: 2018-08-06T00:01:21.475425: step 12692, loss 0.579676.
Train: 2018-08-06T00:01:21.647289: step 12693, loss 0.553771.
Train: 2018-08-06T00:01:21.818040: step 12694, loss 0.536514.
Train: 2018-08-06T00:01:21.980808: step 12695, loss 0.596937.
Train: 2018-08-06T00:01:22.159955: step 12696, loss 0.665951.
Train: 2018-08-06T00:01:22.323473: step 12697, loss 0.519356.
Train: 2018-08-06T00:01:22.479719: step 12698, loss 0.64836.
Train: 2018-08-06T00:01:22.643393: step 12699, loss 0.630973.
Train: 2018-08-06T00:01:22.812817: step 12700, loss 0.528249.
Test: 2018-08-06T00:01:23.053593: step 12700, loss 0.548148.
Train: 2018-08-06T00:01:23.800575: step 12701, loss 0.570915.
Train: 2018-08-06T00:01:23.954432: step 12702, loss 0.638811.
Train: 2018-08-06T00:01:24.126297: step 12703, loss 0.570876.
Train: 2018-08-06T00:01:24.289089: step 12704, loss 0.478188.
Train: 2018-08-06T00:01:24.452718: step 12705, loss 0.528793.
Train: 2018-08-06T00:01:24.615764: step 12706, loss 0.55404.
Train: 2018-08-06T00:01:24.779269: step 12707, loss 0.579254.
Train: 2018-08-06T00:01:24.958466: step 12708, loss 0.570846.
Train: 2018-08-06T00:01:25.122112: step 12709, loss 0.562449.
Train: 2018-08-06T00:01:25.285070: step 12710, loss 0.554058.
Test: 2018-08-06T00:01:25.526788: step 12710, loss 0.548398.
Train: 2018-08-06T00:01:25.705911: step 12711, loss 0.587629.
Train: 2018-08-06T00:01:25.871195: step 12712, loss 0.554067.
Train: 2018-08-06T00:01:26.040223: step 12713, loss 0.545679.
Train: 2018-08-06T00:01:26.212062: step 12714, loss 0.57923.
Train: 2018-08-06T00:01:26.375724: step 12715, loss 0.503731.
Train: 2018-08-06T00:01:26.552842: step 12716, loss 0.562446.
Train: 2018-08-06T00:01:26.734322: step 12717, loss 0.621316.
Train: 2018-08-06T00:01:26.896001: step 12718, loss 0.554029.
Train: 2018-08-06T00:01:27.066138: step 12719, loss 0.604504.
Train: 2018-08-06T00:01:27.237943: step 12720, loss 0.528807.
Test: 2018-08-06T00:01:27.472273: step 12720, loss 0.548352.
Train: 2018-08-06T00:01:27.644138: step 12721, loss 0.56244.
Train: 2018-08-06T00:01:27.807780: step 12722, loss 0.528778.
Train: 2018-08-06T00:01:27.971292: step 12723, loss 0.596134.
Train: 2018-08-06T00:01:28.142044: step 12724, loss 0.621426.
Train: 2018-08-06T00:01:28.306577: step 12725, loss 0.621386.
Train: 2018-08-06T00:01:28.478412: step 12726, loss 0.537221.
Train: 2018-08-06T00:01:28.649509: step 12727, loss 0.604437.
Train: 2018-08-06T00:01:28.813126: step 12728, loss 0.612761.
Train: 2018-08-06T00:01:28.992272: step 12729, loss 0.52065.
Train: 2018-08-06T00:01:29.161218: step 12730, loss 0.537412.
Test: 2018-08-06T00:01:29.387285: step 12730, loss 0.548495.
Train: 2018-08-06T00:01:29.566409: step 12731, loss 0.595878.
Train: 2018-08-06T00:01:29.738274: step 12732, loss 0.529121.
Train: 2018-08-06T00:01:29.901988: step 12733, loss 0.537461.
Train: 2018-08-06T00:01:30.086819: step 12734, loss 0.520752.
Train: 2018-08-06T00:01:30.250254: step 12735, loss 0.58755.
Train: 2018-08-06T00:01:30.413802: step 12736, loss 0.59593.
Train: 2018-08-06T00:01:30.590809: step 12737, loss 0.528993.
Train: 2018-08-06T00:01:30.749230: step 12738, loss 0.554081.
Train: 2018-08-06T00:01:30.928517: step 12739, loss 0.596005.
Train: 2018-08-06T00:01:31.091984: step 12740, loss 0.503709.
Test: 2018-08-06T00:01:31.324385: step 12740, loss 0.548364.
Train: 2018-08-06T00:01:31.503542: step 12741, loss 0.587659.
Train: 2018-08-06T00:01:31.682721: step 12742, loss 0.537188.
Train: 2018-08-06T00:01:31.861849: step 12743, loss 0.61301.
Train: 2018-08-06T00:01:32.033687: step 12744, loss 0.52869.
Train: 2018-08-06T00:01:32.203808: step 12745, loss 0.579317.
Train: 2018-08-06T00:01:32.367562: step 12746, loss 0.528612.
Train: 2018-08-06T00:01:32.546729: step 12747, loss 0.553955.
Train: 2018-08-06T00:01:32.711325: step 12748, loss 0.503063.
Train: 2018-08-06T00:01:32.867546: step 12749, loss 0.5029.
Train: 2018-08-06T00:01:33.047341: step 12750, loss 0.596528.
Test: 2018-08-06T00:01:33.280083: step 12750, loss 0.548073.
Train: 2018-08-06T00:01:33.473421: step 12751, loss 0.519632.
Train: 2018-08-06T00:01:33.650946: step 12752, loss 0.570976.
Train: 2018-08-06T00:01:33.832050: step 12753, loss 0.502168.
Train: 2018-08-06T00:01:34.010574: step 12754, loss 0.493329.
Train: 2018-08-06T00:01:34.174105: step 12755, loss 0.597104.
Train: 2018-08-06T00:01:34.354302: step 12756, loss 0.52761.
Train: 2018-08-06T00:01:34.526140: step 12757, loss 0.544964.
Train: 2018-08-06T00:01:34.689928: step 12758, loss 0.597505.
Train: 2018-08-06T00:01:34.853612: step 12759, loss 0.483402.
Train: 2018-08-06T00:01:35.017182: step 12760, loss 0.571303.
Test: 2018-08-06T00:01:35.258854: step 12760, loss 0.547697.
Train: 2018-08-06T00:01:35.438001: step 12761, loss 0.642062.
Train: 2018-08-06T00:01:35.594187: step 12762, loss 0.535964.
Train: 2018-08-06T00:01:35.767099: step 12763, loss 0.606815.
Train: 2018-08-06T00:01:35.945282: step 12764, loss 0.53594.
Train: 2018-08-06T00:01:36.108160: step 12765, loss 0.580253.
Train: 2018-08-06T00:01:36.274341: step 12766, loss 0.580252.
Train: 2018-08-06T00:01:36.455304: step 12767, loss 0.562519.
Train: 2018-08-06T00:01:36.626876: step 12768, loss 0.535949.
Train: 2018-08-06T00:01:36.801412: step 12769, loss 0.50054.
Train: 2018-08-06T00:01:36.965646: step 12770, loss 0.527072.
Test: 2018-08-06T00:01:37.204656: step 12770, loss 0.547671.
Train: 2018-08-06T00:01:37.369198: step 12771, loss 0.544783.
Train: 2018-08-06T00:01:37.529641: step 12772, loss 0.562542.
Train: 2018-08-06T00:01:37.695477: step 12773, loss 0.482511.
Train: 2018-08-06T00:01:37.880589: step 12774, loss 0.535831.
Train: 2018-08-06T00:01:38.050069: step 12775, loss 0.562583.
Train: 2018-08-06T00:01:38.206311: step 12776, loss 0.517855.
Train: 2018-08-06T00:01:38.381131: step 12777, loss 0.481892.
Train: 2018-08-06T00:01:38.551406: step 12778, loss 0.517658.
Train: 2018-08-06T00:01:38.729542: step 12779, loss 0.607881.
Train: 2018-08-06T00:01:38.892899: step 12780, loss 0.535589.
Test: 2018-08-06T00:01:39.134524: step 12780, loss 0.547575.
Train: 2018-08-06T00:01:39.297428: step 12781, loss 0.544621.
Train: 2018-08-06T00:01:39.473357: step 12782, loss 0.580987.
Train: 2018-08-06T00:01:39.632046: step 12783, loss 0.581033.
Train: 2018-08-06T00:01:39.802323: step 12784, loss 0.526386.
Train: 2018-08-06T00:01:39.965223: step 12785, loss 0.608437.
Train: 2018-08-06T00:01:40.144423: step 12786, loss 0.544607.
Train: 2018-08-06T00:01:40.314145: step 12787, loss 0.553717.
Train: 2018-08-06T00:01:40.479449: step 12788, loss 0.553727.
Train: 2018-08-06T00:01:40.654993: step 12789, loss 0.626618.
Train: 2018-08-06T00:01:40.811236: step 12790, loss 0.644683.
Test: 2018-08-06T00:01:41.059265: step 12790, loss 0.547576.
Train: 2018-08-06T00:01:41.228264: step 12791, loss 0.481149.
Train: 2018-08-06T00:01:41.388232: step 12792, loss 0.553688.
Train: 2018-08-06T00:01:41.567292: step 12793, loss 0.553667.
Train: 2018-08-06T00:01:41.731795: step 12794, loss 0.553673.
Train: 2018-08-06T00:01:41.903660: step 12795, loss 0.589729.
Train: 2018-08-06T00:01:42.066467: step 12796, loss 0.553663.
Train: 2018-08-06T00:01:42.245602: step 12797, loss 0.571625.
Train: 2018-08-06T00:01:42.409162: step 12798, loss 0.580544.
Train: 2018-08-06T00:01:42.588312: step 12799, loss 0.517892.
Train: 2018-08-06T00:01:42.760176: step 12800, loss 0.553657.
Test: 2018-08-06T00:01:42.989398: step 12800, loss 0.547638.
Train: 2018-08-06T00:01:43.759951: step 12801, loss 0.544738.
Train: 2018-08-06T00:01:43.932609: step 12802, loss 0.544747.
Train: 2018-08-06T00:01:44.096073: step 12803, loss 0.562557.
Train: 2018-08-06T00:01:44.275048: step 12804, loss 0.509186.
Train: 2018-08-06T00:01:44.453873: step 12805, loss 0.607024.
Train: 2018-08-06T00:01:44.627889: step 12806, loss 0.535882.
Train: 2018-08-06T00:01:44.791558: step 12807, loss 0.615838.
Train: 2018-08-06T00:01:44.970182: step 12808, loss 0.598001.
Train: 2018-08-06T00:01:45.148642: step 12809, loss 0.527121.
Train: 2018-08-06T00:01:45.318800: step 12810, loss 0.571338.
Test: 2018-08-06T00:01:45.560482: step 12810, loss 0.547714.
Train: 2018-08-06T00:01:45.739211: step 12811, loss 0.580122.
Train: 2018-08-06T00:01:45.916322: step 12812, loss 0.57129.
Train: 2018-08-06T00:01:46.085872: step 12813, loss 0.544885.
Train: 2018-08-06T00:01:46.264201: step 12814, loss 0.615094.
Train: 2018-08-06T00:01:46.435230: step 12815, loss 0.544943.
Train: 2018-08-06T00:01:46.606782: step 12816, loss 0.571173.
Train: 2018-08-06T00:01:46.776342: step 12817, loss 0.588577.
Train: 2018-08-06T00:01:46.954017: step 12818, loss 0.605887.
Train: 2018-08-06T00:01:47.121618: step 12819, loss 0.519077.
Train: 2018-08-06T00:01:47.280929: step 12820, loss 0.536456.
Test: 2018-08-06T00:01:47.523566: step 12820, loss 0.547937.
Train: 2018-08-06T00:01:47.696713: step 12821, loss 0.553767.
Train: 2018-08-06T00:01:47.878859: step 12822, loss 0.562403.
Train: 2018-08-06T00:01:48.037919: step 12823, loss 0.588255.
Train: 2018-08-06T00:01:48.217718: step 12824, loss 0.571003.
Train: 2018-08-06T00:01:48.385672: step 12825, loss 0.570994.
Train: 2018-08-06T00:01:48.549185: step 12826, loss 0.562394.
Train: 2018-08-06T00:01:48.726523: step 12827, loss 0.545253.
Train: 2018-08-06T00:01:48.899965: step 12828, loss 0.596649.
Train: 2018-08-06T00:01:49.059752: step 12829, loss 0.570945.
Train: 2018-08-06T00:01:49.231619: step 12830, loss 0.639238.
Test: 2018-08-06T00:01:49.479555: step 12830, loss 0.548146.
Train: 2018-08-06T00:01:49.634733: step 12831, loss 0.596451.
Train: 2018-08-06T00:01:49.808643: step 12832, loss 0.53695.
Train: 2018-08-06T00:01:49.980479: step 12833, loss 0.545481.
Train: 2018-08-06T00:01:50.145006: step 12834, loss 0.562424.
Train: 2018-08-06T00:01:50.301250: step 12835, loss 0.544415.
Train: 2018-08-06T00:01:50.480504: step 12836, loss 0.528702.
Train: 2018-08-06T00:01:50.645071: step 12837, loss 0.528714.
Train: 2018-08-06T00:01:50.824274: step 12838, loss 0.55399.
Train: 2018-08-06T00:01:50.987754: step 12839, loss 0.537087.
Train: 2018-08-06T00:01:51.159620: step 12840, loss 0.528603.
Test: 2018-08-06T00:01:51.400447: step 12840, loss 0.548222.
Train: 2018-08-06T00:01:51.572283: step 12841, loss 0.596303.
Train: 2018-08-06T00:01:51.735961: step 12842, loss 0.596348.
Train: 2018-08-06T00:01:51.916019: step 12843, loss 0.486017.
Train: 2018-08-06T00:01:52.095989: step 12844, loss 0.664477.
Train: 2018-08-06T00:01:52.260540: step 12845, loss 0.553902.
Train: 2018-08-06T00:01:52.446550: step 12846, loss 0.511392.
Train: 2018-08-06T00:01:52.621037: step 12847, loss 0.519847.
Train: 2018-08-06T00:01:52.781672: step 12848, loss 0.570943.
Train: 2018-08-06T00:01:52.960612: step 12849, loss 0.553863.
Train: 2018-08-06T00:01:53.141912: step 12850, loss 0.579509.
Test: 2018-08-06T00:01:53.377760: step 12850, loss 0.548057.
Train: 2018-08-06T00:01:53.554287: step 12851, loss 0.52815.
Train: 2018-08-06T00:01:53.727891: step 12852, loss 0.562393.
Train: 2018-08-06T00:01:53.896301: step 12853, loss 0.536629.
Train: 2018-08-06T00:01:54.062889: step 12854, loss 0.579621.
Train: 2018-08-06T00:01:54.238108: step 12855, loss 0.553779.
Train: 2018-08-06T00:01:54.401630: step 12856, loss 0.640059.
Train: 2018-08-06T00:01:54.577056: step 12857, loss 0.588277.
Train: 2018-08-06T00:01:54.743340: step 12858, loss 0.657198.
Train: 2018-08-06T00:01:54.912951: step 12859, loss 0.579581.
Train: 2018-08-06T00:01:55.084761: step 12860, loss 0.545255.
Test: 2018-08-06T00:01:55.326649: step 12860, loss 0.548074.
Train: 2018-08-06T00:01:55.498481: step 12861, loss 0.528188.
Train: 2018-08-06T00:01:55.668111: step 12862, loss 0.588027.
Train: 2018-08-06T00:01:55.830733: step 12863, loss 0.5624.
Train: 2018-08-06T00:01:56.010939: step 12864, loss 0.536854.
Train: 2018-08-06T00:01:56.191006: step 12865, loss 0.460278.
Train: 2018-08-06T00:01:56.368365: step 12866, loss 0.605021.
Train: 2018-08-06T00:01:56.540508: step 12867, loss 0.511244.
Train: 2018-08-06T00:01:56.711020: step 12868, loss 0.570937.
Train: 2018-08-06T00:01:56.881563: step 12869, loss 0.588045.
Train: 2018-08-06T00:01:57.053317: step 12870, loss 0.57951.
Test: 2018-08-06T00:01:57.280642: step 12870, loss 0.54807.
Train: 2018-08-06T00:01:57.460476: step 12871, loss 0.511065.
Train: 2018-08-06T00:01:57.623589: step 12872, loss 0.588092.
Train: 2018-08-06T00:01:57.802056: step 12873, loss 0.579539.
Train: 2018-08-06T00:01:57.958299: step 12874, loss 0.562393.
Train: 2018-08-06T00:01:58.136902: step 12875, loss 0.57954.
Train: 2018-08-06T00:01:58.301452: step 12876, loss 0.665256.
Train: 2018-08-06T00:01:58.480289: step 12877, loss 0.52819.
Train: 2018-08-06T00:01:58.655248: step 12878, loss 0.553854.
Train: 2018-08-06T00:01:58.827696: step 12879, loss 0.570931.
Train: 2018-08-06T00:01:58.999043: step 12880, loss 0.587973.
Test: 2018-08-06T00:01:59.229086: step 12880, loss 0.548149.
Train: 2018-08-06T00:01:59.406565: step 12881, loss 0.647528.
Train: 2018-08-06T00:01:59.576107: step 12882, loss 0.536947.
Train: 2018-08-06T00:01:59.755319: step 12883, loss 0.579357.
Train: 2018-08-06T00:01:59.918807: step 12884, loss 0.528628.
Train: 2018-08-06T00:02:00.097951: step 12885, loss 0.537108.
Train: 2018-08-06T00:02:00.261463: step 12886, loss 0.503377.
Train: 2018-08-06T00:02:00.433298: step 12887, loss 0.55398.
Train: 2018-08-06T00:02:00.597113: step 12888, loss 0.570874.
Train: 2018-08-06T00:02:00.776191: step 12889, loss 0.61317.
Train: 2018-08-06T00:02:00.939686: step 12890, loss 0.630069.
Test: 2018-08-06T00:02:01.173228: step 12890, loss 0.548286.
Train: 2018-08-06T00:02:01.353423: step 12891, loss 0.537093.
Train: 2018-08-06T00:02:01.516882: step 12892, loss 0.461196.
Train: 2018-08-06T00:02:01.688749: step 12893, loss 0.494838.
Train: 2018-08-06T00:02:01.867950: step 12894, loss 0.621723.
Train: 2018-08-06T00:02:02.036963: step 12895, loss 0.519996.
Train: 2018-08-06T00:02:02.208828: step 12896, loss 0.553912.
Train: 2018-08-06T00:02:02.375606: step 12897, loss 0.536855.
Train: 2018-08-06T00:02:02.553949: step 12898, loss 0.545321.
Train: 2018-08-06T00:02:02.726185: step 12899, loss 0.639432.
Train: 2018-08-06T00:02:02.891727: step 12900, loss 0.5624.
Test: 2018-08-06T00:02:03.133554: step 12900, loss 0.548042.
Train: 2018-08-06T00:02:03.856993: step 12901, loss 0.502394.
Train: 2018-08-06T00:02:04.036220: step 12902, loss 0.502284.
Train: 2018-08-06T00:02:04.208054: step 12903, loss 0.596844.
Train: 2018-08-06T00:02:04.378559: step 12904, loss 0.519271.
Train: 2018-08-06T00:02:04.542024: step 12905, loss 0.614292.
Train: 2018-08-06T00:02:04.705529: step 12906, loss 0.527779.
Train: 2018-08-06T00:02:04.884742: step 12907, loss 0.527718.
Train: 2018-08-06T00:02:05.063799: step 12908, loss 0.588489.
Train: 2018-08-06T00:02:05.237552: step 12909, loss 0.605934.
Train: 2018-08-06T00:02:05.408066: step 12910, loss 0.50149.
Test: 2018-08-06T00:02:05.636850: step 12910, loss 0.547828.
Train: 2018-08-06T00:02:05.812996: step 12911, loss 0.606011.
Train: 2018-08-06T00:02:05.977550: step 12912, loss 0.536272.
Train: 2018-08-06T00:02:06.142367: step 12913, loss 0.597339.
Train: 2018-08-06T00:02:06.320817: step 12914, loss 0.562434.
Train: 2018-08-06T00:02:06.511147: step 12915, loss 0.623501.
Train: 2018-08-06T00:02:06.683655: step 12916, loss 0.527574.
Train: 2018-08-06T00:02:06.856194: step 12917, loss 0.579838.
Train: 2018-08-06T00:02:07.028873: step 12918, loss 0.501532.
Train: 2018-08-06T00:02:07.192168: step 12919, loss 0.510219.
Train: 2018-08-06T00:02:07.372330: step 12920, loss 0.545006.
Test: 2018-08-06T00:02:07.606682: step 12920, loss 0.547823.
Train: 2018-08-06T00:02:07.778516: step 12921, loss 0.51882.
Train: 2018-08-06T00:02:07.957735: step 12922, loss 0.606131.
Train: 2018-08-06T00:02:08.131170: step 12923, loss 0.579932.
Train: 2018-08-06T00:02:08.299489: step 12924, loss 0.544946.
Train: 2018-08-06T00:02:08.469179: step 12925, loss 0.571199.
Train: 2018-08-06T00:02:08.656640: step 12926, loss 0.588706.
Train: 2018-08-06T00:02:08.820189: step 12927, loss 0.588697.
Train: 2018-08-06T00:02:08.999286: step 12928, loss 0.544954.
Train: 2018-08-06T00:02:09.179443: step 12929, loss 0.527484.
Train: 2018-08-06T00:02:09.351279: step 12930, loss 0.501269.
Test: 2018-08-06T00:02:09.593600: step 12930, loss 0.54779.
Train: 2018-08-06T00:02:09.764884: step 12931, loss 0.527449.
Train: 2018-08-06T00:02:09.930413: step 12932, loss 0.536162.
Train: 2018-08-06T00:02:10.091907: step 12933, loss 0.632683.
Train: 2018-08-06T00:02:10.270685: step 12934, loss 0.457097.
Train: 2018-08-06T00:02:10.436438: step 12935, loss 0.659255.
Train: 2018-08-06T00:02:10.608273: step 12936, loss 0.553673.
Train: 2018-08-06T00:02:10.771231: step 12937, loss 0.562473.
Train: 2018-08-06T00:02:11.003143: step 12938, loss 0.571266.
Train: 2018-08-06T00:02:11.166356: step 12939, loss 0.597643.
Train: 2018-08-06T00:02:11.345612: step 12940, loss 0.509758.
Test: 2018-08-06T00:02:11.571346: step 12940, loss 0.547752.
Train: 2018-08-06T00:02:11.744866: step 12941, loss 0.544895.
Train: 2018-08-06T00:02:11.916701: step 12942, loss 0.53611.
Train: 2018-08-06T00:02:12.086508: step 12943, loss 0.588831.
Train: 2018-08-06T00:02:12.272985: step 12944, loss 0.571253.
Train: 2018-08-06T00:02:12.451063: step 12945, loss 0.527327.
Train: 2018-08-06T00:02:12.618756: step 12946, loss 0.527319.
Train: 2018-08-06T00:02:12.790621: step 12947, loss 0.57126.
Train: 2018-08-06T00:02:12.953475: step 12948, loss 0.632834.
Train: 2018-08-06T00:02:13.133666: step 12949, loss 0.615181.
Train: 2018-08-06T00:02:13.313015: step 12950, loss 0.553684.
Test: 2018-08-06T00:02:13.555337: step 12950, loss 0.547785.
Train: 2018-08-06T00:02:13.727905: step 12951, loss 0.553692.
Train: 2018-08-06T00:02:13.884786: step 12952, loss 0.553697.
Train: 2018-08-06T00:02:14.070441: step 12953, loss 0.632252.
Train: 2018-08-06T00:02:14.249620: step 12954, loss 0.640766.
Train: 2018-08-06T00:02:14.413272: step 12955, loss 0.519054.
Train: 2018-08-06T00:02:14.588672: step 12956, loss 0.50187.
Train: 2018-08-06T00:02:14.783510: step 12957, loss 0.562402.
Train: 2018-08-06T00:02:14.963772: step 12958, loss 0.493394.
Train: 2018-08-06T00:02:15.127215: step 12959, loss 0.614165.
Train: 2018-08-06T00:02:15.306337: step 12960, loss 0.665844.
Test: 2018-08-06T00:02:15.548128: step 12960, loss 0.548.
Train: 2018-08-06T00:02:15.743019: step 12961, loss 0.63117.
Train: 2018-08-06T00:02:15.924854: step 12962, loss 0.536708.
Train: 2018-08-06T00:02:16.095202: step 12963, loss 0.536785.
Train: 2018-08-06T00:02:16.257816: step 12964, loss 0.613519.
Train: 2018-08-06T00:02:16.438763: step 12965, loss 0.519936.
Train: 2018-08-06T00:02:16.615262: step 12966, loss 0.562412.
Train: 2018-08-06T00:02:16.787800: step 12967, loss 0.570883.
Train: 2018-08-06T00:02:16.945413: step 12968, loss 0.520149.
Train: 2018-08-06T00:02:17.125634: step 12969, loss 0.52862.
Train: 2018-08-06T00:02:17.290163: step 12970, loss 0.537059.
Test: 2018-08-06T00:02:17.532734: step 12970, loss 0.548247.
Train: 2018-08-06T00:02:17.711848: step 12971, loss 0.562419.
Train: 2018-08-06T00:02:17.906736: step 12972, loss 0.604756.
Train: 2018-08-06T00:02:18.101128: step 12973, loss 0.596291.
Train: 2018-08-06T00:02:18.282427: step 12974, loss 0.511637.
Train: 2018-08-06T00:02:18.480751: step 12975, loss 0.528544.
Train: 2018-08-06T00:02:18.642699: step 12976, loss 0.621759.
Train: 2018-08-06T00:02:18.803721: step 12977, loss 0.536981.
Train: 2018-08-06T00:02:18.982655: step 12978, loss 0.630266.
Train: 2018-08-06T00:02:19.164069: step 12979, loss 0.545466.
Train: 2018-08-06T00:02:19.343596: step 12980, loss 0.528526.
Test: 2018-08-06T00:02:19.575741: step 12980, loss 0.548219.
Train: 2018-08-06T00:02:19.755258: step 12981, loss 0.613265.
Train: 2018-08-06T00:02:19.928039: step 12982, loss 0.570884.
Train: 2018-08-06T00:02:20.091746: step 12983, loss 0.528559.
Train: 2018-08-06T00:02:20.255013: step 12984, loss 0.553949.
Train: 2018-08-06T00:02:20.428494: step 12985, loss 0.528539.
Train: 2018-08-06T00:02:20.600321: step 12986, loss 0.598583.
Train: 2018-08-06T00:02:20.770247: step 12987, loss 0.596329.
Train: 2018-08-06T00:02:20.939162: step 12988, loss 0.57089.
Train: 2018-08-06T00:02:21.110996: step 12989, loss 0.570886.
Train: 2018-08-06T00:02:21.270972: step 12990, loss 0.579358.
Test: 2018-08-06T00:02:21.513663: step 12990, loss 0.548245.
Train: 2018-08-06T00:02:21.693023: step 12991, loss 0.562419.
Train: 2018-08-06T00:02:21.856578: step 12992, loss 0.562418.
Train: 2018-08-06T00:02:22.020067: step 12993, loss 0.596234.
Train: 2018-08-06T00:02:22.198079: step 12994, loss 0.613083.
Train: 2018-08-06T00:02:22.355435: step 12995, loss 0.621424.
Train: 2018-08-06T00:02:22.518320: step 12996, loss 0.528826.
Train: 2018-08-06T00:02:22.697421: step 12997, loss 0.671509.
Train: 2018-08-06T00:02:22.867606: step 12998, loss 0.428754.
Train: 2018-08-06T00:02:23.032116: step 12999, loss 0.520706.
Train: 2018-08-06T00:02:23.203975: step 13000, loss 0.57083.
Test: 2018-08-06T00:02:23.456184: step 13000, loss 0.548467.
Train: 2018-08-06T00:02:24.169017: step 13001, loss 0.478859.
Train: 2018-08-06T00:02:24.331973: step 13002, loss 0.562454.
Train: 2018-08-06T00:02:24.513733: step 13003, loss 0.554055.
Train: 2018-08-06T00:02:24.691084: step 13004, loss 0.612929.
Train: 2018-08-06T00:02:24.854120: step 13005, loss 0.469776.
Train: 2018-08-06T00:02:25.032962: step 13006, loss 0.663789.
Train: 2018-08-06T00:02:25.195894: step 13007, loss 0.621592.
Train: 2018-08-06T00:02:25.375136: step 13008, loss 0.630004.
Train: 2018-08-06T00:02:25.538268: step 13009, loss 0.537135.
Train: 2018-08-06T00:02:25.710103: step 13010, loss 0.495052.
Test: 2018-08-06T00:02:25.947156: step 13010, loss 0.548319.
Train: 2018-08-06T00:02:26.163835: step 13011, loss 0.612997.
Train: 2018-08-06T00:02:26.334609: step 13012, loss 0.629817.
Train: 2018-08-06T00:02:26.515305: step 13013, loss 0.604483.
Train: 2018-08-06T00:02:26.688840: step 13014, loss 0.57923.
Train: 2018-08-06T00:02:26.857388: step 13015, loss 0.528983.
Train: 2018-08-06T00:02:27.028092: step 13016, loss 0.487247.
Train: 2018-08-06T00:02:27.198884: step 13017, loss 0.520655.
Train: 2018-08-06T00:02:27.362109: step 13018, loss 0.562461.
Train: 2018-08-06T00:02:27.537146: step 13019, loss 0.57923.
Train: 2018-08-06T00:02:27.699657: step 13020, loss 0.596024.
Test: 2018-08-06T00:02:27.941288: step 13020, loss 0.548385.
Train: 2018-08-06T00:02:28.096614: step 13021, loss 0.587638.
Train: 2018-08-06T00:02:28.275464: step 13022, loss 0.562448.
Train: 2018-08-06T00:02:28.439908: step 13023, loss 0.545662.
Train: 2018-08-06T00:02:28.603429: step 13024, loss 0.478461.
Train: 2018-08-06T00:02:28.775264: step 13025, loss 0.520355.
Train: 2018-08-06T00:02:28.954521: step 13026, loss 0.579307.
Train: 2018-08-06T00:02:29.108817: step 13027, loss 0.579339.
Train: 2018-08-06T00:02:29.287920: step 13028, loss 0.562412.
Train: 2018-08-06T00:02:29.451432: step 13029, loss 0.486002.
Train: 2018-08-06T00:02:29.628475: step 13030, loss 0.613494.
Test: 2018-08-06T00:02:29.856713: step 13030, loss 0.548113.
Train: 2018-08-06T00:02:30.028514: step 13031, loss 0.460034.
Train: 2018-08-06T00:02:30.199245: step 13032, loss 0.562396.
Train: 2018-08-06T00:02:30.371315: step 13033, loss 0.562397.
Train: 2018-08-06T00:02:30.528598: step 13034, loss 0.571012.
Train: 2018-08-06T00:02:30.692399: step 13035, loss 0.553767.
Train: 2018-08-06T00:02:30.848643: step 13036, loss 0.640297.
Train: 2018-08-06T00:02:31.029545: step 13037, loss 0.536427.
Train: 2018-08-06T00:02:31.192412: step 13038, loss 0.527737.
Train: 2018-08-06T00:02:31.355199: step 13039, loss 0.579774.
Train: 2018-08-06T00:02:31.519027: step 13040, loss 0.640614.
Test: 2018-08-06T00:02:31.759029: step 13040, loss 0.547871.
Train: 2018-08-06T00:02:31.942704: step 13041, loss 0.527682.
Train: 2018-08-06T00:02:32.111090: step 13042, loss 0.536365.
Train: 2018-08-06T00:02:32.270099: step 13043, loss 0.571101.
Train: 2018-08-06T00:02:32.437001: step 13044, loss 0.510287.
Train: 2018-08-06T00:02:32.606575: step 13045, loss 0.52763.
Train: 2018-08-06T00:02:32.776033: step 13046, loss 0.597268.
Train: 2018-08-06T00:02:32.950934: step 13047, loss 0.562426.
Train: 2018-08-06T00:02:33.126995: step 13048, loss 0.632201.
Train: 2018-08-06T00:02:33.293080: step 13049, loss 0.588567.
Train: 2018-08-06T00:02:33.464722: step 13050, loss 0.545016.
Test: 2018-08-06T00:02:33.702061: step 13050, loss 0.547857.
Train: 2018-08-06T00:02:33.859227: step 13051, loss 0.579805.
Train: 2018-08-06T00:02:34.037855: step 13052, loss 0.605829.
Train: 2018-08-06T00:02:34.202438: step 13053, loss 0.623068.
Train: 2018-08-06T00:02:34.365866: step 13054, loss 0.553763.
Train: 2018-08-06T00:02:34.522110: step 13055, loss 0.527928.
Train: 2018-08-06T00:02:34.685262: step 13056, loss 0.605414.
Train: 2018-08-06T00:02:34.864491: step 13057, loss 0.59673.
Train: 2018-08-06T00:02:35.028070: step 13058, loss 0.596634.
Train: 2018-08-06T00:02:35.207169: step 13059, loss 0.511207.
Train: 2018-08-06T00:02:35.363412: step 13060, loss 0.502787.
Test: 2018-08-06T00:02:35.610456: step 13060, loss 0.548146.
Train: 2018-08-06T00:02:35.777112: step 13061, loss 0.494302.
Train: 2018-08-06T00:02:35.939509: step 13062, loss 0.596485.
Train: 2018-08-06T00:02:36.104234: step 13063, loss 0.579447.
Train: 2018-08-06T00:02:36.276744: step 13064, loss 0.545357.
Train: 2018-08-06T00:02:36.442556: step 13065, loss 0.528304.
Train: 2018-08-06T00:02:36.625097: step 13066, loss 0.545335.
Train: 2018-08-06T00:02:36.797630: step 13067, loss 0.451346.
Train: 2018-08-06T00:02:36.969335: step 13068, loss 0.528115.
Train: 2018-08-06T00:02:37.125517: step 13069, loss 0.579599.
Train: 2018-08-06T00:02:37.299584: step 13070, loss 0.588277.
Test: 2018-08-06T00:02:37.538520: step 13070, loss 0.547927.
Train: 2018-08-06T00:02:37.701924: step 13071, loss 0.588333.
Train: 2018-08-06T00:02:37.866424: step 13072, loss 0.579715.
Train: 2018-08-06T00:02:38.038253: step 13073, loss 0.501773.
Train: 2018-08-06T00:02:38.201891: step 13074, loss 0.449597.
Train: 2018-08-06T00:02:38.365561: step 13075, loss 0.553713.
Train: 2018-08-06T00:02:38.529042: step 13076, loss 0.65858.
Train: 2018-08-06T00:02:38.698327: step 13077, loss 0.606197.
Train: 2018-08-06T00:02:38.854510: step 13078, loss 0.501173.
Train: 2018-08-06T00:02:39.017474: step 13079, loss 0.536159.
Train: 2018-08-06T00:02:39.181031: step 13080, loss 0.518573.
Test: 2018-08-06T00:02:39.424389: step 13080, loss 0.547739.
Train: 2018-08-06T00:02:39.588927: step 13081, loss 0.518495.
Train: 2018-08-06T00:02:39.759304: step 13082, loss 0.553667.
Train: 2018-08-06T00:02:39.918147: step 13083, loss 0.571336.
Train: 2018-08-06T00:02:40.079966: step 13084, loss 0.633339.
Train: 2018-08-06T00:02:40.243219: step 13085, loss 0.580224.
Train: 2018-08-06T00:02:40.408988: step 13086, loss 0.606776.
Train: 2018-08-06T00:02:40.572840: step 13087, loss 0.509448.
Train: 2018-08-06T00:02:40.744707: step 13088, loss 0.562501.
Train: 2018-08-06T00:02:40.915296: step 13089, loss 0.544824.
Train: 2018-08-06T00:02:41.078220: step 13090, loss 0.589008.
Test: 2018-08-06T00:02:41.319883: step 13090, loss 0.547704.
Train: 2018-08-06T00:02:41.483098: step 13091, loss 0.544832.
Train: 2018-08-06T00:02:41.657618: step 13092, loss 0.571317.
Train: 2018-08-06T00:02:41.813826: step 13093, loss 0.544845.
Train: 2018-08-06T00:02:41.999426: step 13094, loss 0.633019.
Train: 2018-08-06T00:02:42.172879: step 13095, loss 0.544869.
Train: 2018-08-06T00:02:42.324577: step 13096, loss 0.544885.
Train: 2018-08-06T00:02:42.500247: step 13097, loss 0.500996.
Train: 2018-08-06T00:02:42.663697: step 13098, loss 0.536116.
Train: 2018-08-06T00:02:42.827207: step 13099, loss 0.54489.
Train: 2018-08-06T00:02:43.002570: step 13100, loss 0.562465.
Test: 2018-08-06T00:02:43.233853: step 13100, loss 0.547738.
Train: 2018-08-06T00:02:43.955322: step 13101, loss 0.544878.
Train: 2018-08-06T00:02:44.118766: step 13102, loss 0.659285.
Train: 2018-08-06T00:02:44.282225: step 13103, loss 0.553675.
Train: 2018-08-06T00:02:44.455353: step 13104, loss 0.580018.
Train: 2018-08-06T00:02:44.602125: step 13105, loss 0.544916.
Train: 2018-08-06T00:02:44.781424: step 13106, loss 0.54493.
Train: 2018-08-06T00:02:44.944973: step 13107, loss 0.588699.
Train: 2018-08-06T00:02:45.100262: step 13108, loss 0.562437.
Train: 2018-08-06T00:02:45.272095: step 13109, loss 0.579894.
Train: 2018-08-06T00:02:45.452665: step 13110, loss 0.536271.
Test: 2018-08-06T00:02:45.678576: step 13110, loss 0.547835.
Train: 2018-08-06T00:02:45.842063: step 13111, loss 0.588554.
Train: 2018-08-06T00:02:46.023823: step 13112, loss 0.484131.
Train: 2018-08-06T00:02:46.176960: step 13113, loss 0.536318.
Train: 2018-08-06T00:02:46.371425: step 13114, loss 0.597242.
Train: 2018-08-06T00:02:46.543929: step 13115, loss 0.484088.
Train: 2018-08-06T00:02:46.719461: step 13116, loss 0.571139.
Train: 2018-08-06T00:02:46.887005: step 13117, loss 0.501373.
Train: 2018-08-06T00:02:47.044538: step 13118, loss 0.527482.
Train: 2018-08-06T00:02:47.208516: step 13119, loss 0.518656.
Train: 2018-08-06T00:02:47.372993: step 13120, loss 0.544896.
Test: 2018-08-06T00:02:47.614757: step 13120, loss 0.547728.
Train: 2018-08-06T00:02:47.790919: step 13121, loss 0.518448.
Train: 2018-08-06T00:02:47.950263: step 13122, loss 0.677318.
Train: 2018-08-06T00:02:48.113849: step 13123, loss 0.544824.
Train: 2018-08-06T00:02:48.278356: step 13124, loss 0.562504.
Train: 2018-08-06T00:02:48.448274: step 13125, loss 0.606748.
Train: 2018-08-06T00:02:48.604462: step 13126, loss 0.580192.
Train: 2018-08-06T00:02:48.768047: step 13127, loss 0.527154.
Train: 2018-08-06T00:02:48.932790: step 13128, loss 0.615497.
Train: 2018-08-06T00:02:49.097067: step 13129, loss 0.624239.
Train: 2018-08-06T00:02:49.261549: step 13130, loss 0.553671.
Test: 2018-08-06T00:02:49.509872: step 13130, loss 0.547753.
Train: 2018-08-06T00:02:49.673307: step 13131, loss 0.553678.
Train: 2018-08-06T00:02:49.855350: step 13132, loss 0.54492.
Train: 2018-08-06T00:02:50.008765: step 13133, loss 0.614953.
Train: 2018-08-06T00:02:50.173748: step 13134, loss 0.562432.
Train: 2018-08-06T00:02:50.337535: step 13135, loss 0.614692.
Train: 2018-08-06T00:02:50.516036: step 13136, loss 0.571098.
Train: 2018-08-06T00:02:50.663843: step 13137, loss 0.543933.
Train: 2018-08-06T00:02:50.826804: step 13138, loss 0.51057.
Train: 2018-08-06T00:02:50.990504: step 13139, loss 0.579657.
Train: 2018-08-06T00:02:51.169330: step 13140, loss 0.49347.
Test: 2018-08-06T00:02:51.406064: step 13140, loss 0.547968.
Train: 2018-08-06T00:02:51.577922: step 13141, loss 0.579631.
Train: 2018-08-06T00:02:51.747717: step 13142, loss 0.571011.
Train: 2018-08-06T00:02:51.916613: step 13143, loss 0.614052.
Train: 2018-08-06T00:02:52.088460: step 13144, loss 0.545203.
Train: 2018-08-06T00:02:52.248781: step 13145, loss 0.613926.
Train: 2018-08-06T00:02:52.443527: step 13146, loss 0.61383.
Train: 2018-08-06T00:02:52.622633: step 13147, loss 0.588045.
Train: 2018-08-06T00:02:52.786135: step 13148, loss 0.630596.
Train: 2018-08-06T00:02:52.960187: step 13149, loss 0.587881.
Train: 2018-08-06T00:02:53.121653: step 13150, loss 0.494762.
Test: 2018-08-06T00:02:53.365525: step 13150, loss 0.548294.
Train: 2018-08-06T00:02:53.534074: step 13151, loss 0.469604.
Train: 2018-08-06T00:02:53.696652: step 13152, loss 0.621491.
Train: 2018-08-06T00:02:53.852000: step 13153, loss 0.495007.
Train: 2018-08-06T00:02:54.016219: step 13154, loss 0.537137.
Train: 2018-08-06T00:02:54.189571: step 13155, loss 0.60462.
Train: 2018-08-06T00:02:54.351414: step 13156, loss 0.62994.
Train: 2018-08-06T00:02:54.517258: step 13157, loss 0.579289.
Train: 2018-08-06T00:02:54.695675: step 13158, loss 0.562436.
Train: 2018-08-06T00:02:54.859621: step 13159, loss 0.579257.
Train: 2018-08-06T00:02:55.037237: step 13160, loss 0.562446.
Test: 2018-08-06T00:02:55.270650: step 13160, loss 0.548403.
Train: 2018-08-06T00:02:55.452087: step 13161, loss 0.545675.
Train: 2018-08-06T00:02:55.605931: step 13162, loss 0.604375.
Train: 2018-08-06T00:02:55.763823: step 13163, loss 0.621079.
Train: 2018-08-06T00:02:55.935653: step 13164, loss 0.579184.
Train: 2018-08-06T00:02:56.105456: step 13165, loss 0.579161.
Train: 2018-08-06T00:02:56.258839: step 13166, loss 0.504262.
Train: 2018-08-06T00:02:56.430675: step 13167, loss 0.579132.
Train: 2018-08-06T00:02:56.603171: step 13168, loss 0.595741.
Train: 2018-08-06T00:02:56.764739: step 13169, loss 0.579114.
Train: 2018-08-06T00:02:56.931053: step 13170, loss 0.587388.
Test: 2018-08-06T00:02:57.157528: step 13170, loss 0.548682.
Train: 2018-08-06T00:02:57.337691: step 13171, loss 0.512894.
Train: 2018-08-06T00:02:57.501241: step 13172, loss 0.545995.
Train: 2018-08-06T00:02:57.673076: step 13173, loss 0.570813.
Train: 2018-08-06T00:02:57.820961: step 13174, loss 0.537703.
Train: 2018-08-06T00:02:58.018136: step 13175, loss 0.496245.
Train: 2018-08-06T00:02:58.196662: step 13176, loss 0.554207.
Train: 2018-08-06T00:02:58.360222: step 13177, loss 0.545845.
Train: 2018-08-06T00:02:58.528365: step 13178, loss 0.570826.
Train: 2018-08-06T00:02:58.696945: step 13179, loss 0.537364.
Train: 2018-08-06T00:02:58.869484: step 13180, loss 0.545671.
Test: 2018-08-06T00:02:59.110807: step 13180, loss 0.548346.
Train: 2018-08-06T00:02:59.281352: step 13181, loss 0.520368.
Train: 2018-08-06T00:02:59.458279: step 13182, loss 0.537094.
Train: 2018-08-06T00:02:59.624865: step 13183, loss 0.596313.
Train: 2018-08-06T00:02:59.798911: step 13184, loss 0.562406.
Train: 2018-08-06T00:02:59.968978: step 13185, loss 0.494247.
Train: 2018-08-06T00:03:00.150494: step 13186, loss 0.613697.
Train: 2018-08-06T00:03:00.322064: step 13187, loss 0.588103.
Train: 2018-08-06T00:03:00.487593: step 13188, loss 0.545232.
Train: 2018-08-06T00:03:00.653179: step 13189, loss 0.596783.
Train: 2018-08-06T00:03:00.823404: step 13190, loss 0.562396.
Test: 2018-08-06T00:03:01.064759: step 13190, loss 0.547977.
Train: 2018-08-06T00:03:01.235853: step 13191, loss 0.553787.
Train: 2018-08-06T00:03:01.409378: step 13192, loss 0.502074.
Train: 2018-08-06T00:03:01.577934: step 13193, loss 0.527865.
Train: 2018-08-06T00:03:01.748447: step 13194, loss 0.501827.
Train: 2018-08-06T00:03:01.914039: step 13195, loss 0.510321.
Train: 2018-08-06T00:03:02.079561: step 13196, loss 0.623423.
Train: 2018-08-06T00:03:02.258086: step 13197, loss 0.571168.
Train: 2018-08-06T00:03:02.428638: step 13198, loss 0.54494.
Train: 2018-08-06T00:03:02.597869: step 13199, loss 0.562447.
Train: 2018-08-06T00:03:02.764774: step 13200, loss 0.571227.
Test: 2018-08-06T00:03:03.001142: step 13200, loss 0.547745.
Train: 2018-08-06T00:03:03.759884: step 13201, loss 0.580044.
Train: 2018-08-06T00:03:03.932395: step 13202, loss 0.544872.
Train: 2018-08-06T00:03:04.093988: step 13203, loss 0.55367.
Train: 2018-08-06T00:03:04.261765: step 13204, loss 0.632954.
Train: 2018-08-06T00:03:04.424395: step 13205, loss 0.518479.
Train: 2018-08-06T00:03:04.591942: step 13206, loss 0.606478.
Train: 2018-08-06T00:03:04.760466: step 13207, loss 0.553674.
Train: 2018-08-06T00:03:04.930012: step 13208, loss 0.553679.
Train: 2018-08-06T00:03:05.097564: step 13209, loss 0.501002.
Train: 2018-08-06T00:03:05.276115: step 13210, loss 0.580029.
Test: 2018-08-06T00:03:05.514501: step 13210, loss 0.547751.
Train: 2018-08-06T00:03:05.683191: step 13211, loss 0.509755.
Train: 2018-08-06T00:03:05.854984: step 13212, loss 0.597628.
Train: 2018-08-06T00:03:06.027524: step 13213, loss 0.588836.
Train: 2018-08-06T00:03:06.198091: step 13214, loss 0.544887.
Train: 2018-08-06T00:03:06.358638: step 13215, loss 0.588805.
Train: 2018-08-06T00:03:06.531209: step 13216, loss 0.580003.
Train: 2018-08-06T00:03:06.696764: step 13217, loss 0.588742.
Train: 2018-08-06T00:03:06.864369: step 13218, loss 0.597447.
Train: 2018-08-06T00:03:07.030907: step 13219, loss 0.527506.
Train: 2018-08-06T00:03:07.194003: step 13220, loss 0.527551.
Test: 2018-08-06T00:03:07.430705: step 13220, loss 0.547832.
Train: 2018-08-06T00:03:07.601273: step 13221, loss 0.571138.
Train: 2018-08-06T00:03:07.765833: step 13222, loss 0.588531.
Train: 2018-08-06T00:03:07.940341: step 13223, loss 0.449408.
Train: 2018-08-06T00:03:08.110886: step 13224, loss 0.545019.
Train: 2018-08-06T00:03:08.280457: step 13225, loss 0.545005.
Train: 2018-08-06T00:03:08.439045: step 13226, loss 0.536263.
Train: 2018-08-06T00:03:08.595589: step 13227, loss 0.518757.
Train: 2018-08-06T00:03:08.761147: step 13228, loss 0.64121.
Train: 2018-08-06T00:03:08.931721: step 13229, loss 0.544937.
Train: 2018-08-06T00:03:09.097248: step 13230, loss 0.615012.
Test: 2018-08-06T00:03:09.334612: step 13230, loss 0.547781.
Train: 2018-08-06T00:03:09.504159: step 13231, loss 0.518678.
Train: 2018-08-06T00:03:09.673706: step 13232, loss 0.544939.
Train: 2018-08-06T00:03:09.837299: step 13233, loss 0.562445.
Train: 2018-08-06T00:03:10.007812: step 13234, loss 0.544925.
Train: 2018-08-06T00:03:10.170407: step 13235, loss 0.553671.
Train: 2018-08-06T00:03:10.337930: step 13236, loss 0.52737.
Train: 2018-08-06T00:03:10.511464: step 13237, loss 0.597567.
Train: 2018-08-06T00:03:10.676025: step 13238, loss 0.544895.
Train: 2018-08-06T00:03:10.847566: step 13239, loss 0.544906.
Train: 2018-08-06T00:03:11.021102: step 13240, loss 0.615198.
Test: 2018-08-06T00:03:11.261461: step 13240, loss 0.547749.
Train: 2018-08-06T00:03:11.426018: step 13241, loss 0.580042.
Train: 2018-08-06T00:03:11.596589: step 13242, loss 0.544901.
Train: 2018-08-06T00:03:11.763143: step 13243, loss 0.579985.
Train: 2018-08-06T00:03:11.927708: step 13244, loss 0.571211.
Train: 2018-08-06T00:03:12.100247: step 13245, loss 0.553682.
Train: 2018-08-06T00:03:12.268798: step 13246, loss 0.588667.
Train: 2018-08-06T00:03:12.428338: step 13247, loss 0.518786.
Train: 2018-08-06T00:03:12.596913: step 13248, loss 0.606059.
Train: 2018-08-06T00:03:12.761479: step 13249, loss 0.562427.
Train: 2018-08-06T00:03:12.926007: step 13250, loss 0.562424.
Test: 2018-08-06T00:03:13.165368: step 13250, loss 0.547861.
Train: 2018-08-06T00:03:13.329952: step 13251, loss 0.631941.
Train: 2018-08-06T00:03:13.495486: step 13252, loss 0.597081.
Train: 2018-08-06T00:03:13.656057: step 13253, loss 0.579687.
Train: 2018-08-06T00:03:13.822611: step 13254, loss 0.510699.
Train: 2018-08-06T00:03:13.986197: step 13255, loss 0.519391.
Train: 2018-08-06T00:03:14.163698: step 13256, loss 0.570991.
Train: 2018-08-06T00:03:14.328260: step 13257, loss 0.639679.
Train: 2018-08-06T00:03:14.495834: step 13258, loss 0.502424.
Train: 2018-08-06T00:03:14.653410: step 13259, loss 0.588076.
Train: 2018-08-06T00:03:14.818976: step 13260, loss 0.553849.
Test: 2018-08-06T00:03:15.057320: step 13260, loss 0.548095.
Train: 2018-08-06T00:03:15.223895: step 13261, loss 0.613643.
Train: 2018-08-06T00:03:15.384433: step 13262, loss 0.570926.
Train: 2018-08-06T00:03:15.545042: step 13263, loss 0.638987.
Train: 2018-08-06T00:03:15.706571: step 13264, loss 0.486067.
Train: 2018-08-06T00:03:15.875121: step 13265, loss 0.570886.
Train: 2018-08-06T00:03:16.040679: step 13266, loss 0.553953.
Train: 2018-08-06T00:03:16.205263: step 13267, loss 0.52015.
Train: 2018-08-06T00:03:16.371793: step 13268, loss 0.587785.
Train: 2018-08-06T00:03:16.536377: step 13269, loss 0.56242.
Train: 2018-08-06T00:03:16.692964: step 13270, loss 0.52017.
Test: 2018-08-06T00:03:16.931297: step 13270, loss 0.548258.
Train: 2018-08-06T00:03:17.100871: step 13271, loss 0.62161.
Train: 2018-08-06T00:03:17.265433: step 13272, loss 0.528614.
Train: 2018-08-06T00:03:17.428966: step 13273, loss 0.511694.
Train: 2018-08-06T00:03:17.594554: step 13274, loss 0.621669.
Train: 2018-08-06T00:03:17.757118: step 13275, loss 0.52009.
Train: 2018-08-06T00:03:17.921648: step 13276, loss 0.613264.
Train: 2018-08-06T00:03:18.085211: step 13277, loss 0.587836.
Train: 2018-08-06T00:03:18.257749: step 13278, loss 0.545473.
Train: 2018-08-06T00:03:18.429321: step 13279, loss 0.562411.
Train: 2018-08-06T00:03:18.592854: step 13280, loss 0.570888.
Test: 2018-08-06T00:03:18.830218: step 13280, loss 0.548233.
Train: 2018-08-06T00:03:19.029684: step 13281, loss 0.570884.
Train: 2018-08-06T00:03:19.190257: step 13282, loss 0.579345.
Train: 2018-08-06T00:03:19.350850: step 13283, loss 0.503187.
Train: 2018-08-06T00:03:19.510428: step 13284, loss 0.528538.
Train: 2018-08-06T00:03:19.670970: step 13285, loss 0.570896.
Train: 2018-08-06T00:03:19.829570: step 13286, loss 0.536933.
Train: 2018-08-06T00:03:20.013085: step 13287, loss 0.49436.
Train: 2018-08-06T00:03:20.163680: step 13288, loss 0.616988.
Train: 2018-08-06T00:03:20.326217: step 13289, loss 0.570939.
Train: 2018-08-06T00:03:20.487815: step 13290, loss 0.570948.
Test: 2018-08-06T00:03:20.724153: step 13290, loss 0.54806.
Train: 2018-08-06T00:03:20.899683: step 13291, loss 0.673678.
Train: 2018-08-06T00:03:21.060296: step 13292, loss 0.562396.
Train: 2018-08-06T00:03:21.224845: step 13293, loss 0.53678.
Train: 2018-08-06T00:03:21.384388: step 13294, loss 0.579471.
Train: 2018-08-06T00:03:21.543960: step 13295, loss 0.502705.
Train: 2018-08-06T00:03:21.711543: step 13296, loss 0.587998.
Train: 2018-08-06T00:03:21.872113: step 13297, loss 0.55387.
Train: 2018-08-06T00:03:22.033681: step 13298, loss 0.630657.
Train: 2018-08-06T00:03:22.200205: step 13299, loss 0.587964.
Train: 2018-08-06T00:03:22.362796: step 13300, loss 0.494328.
Test: 2018-08-06T00:03:22.599138: step 13300, loss 0.548152.
Train: 2018-08-06T00:03:23.332443: step 13301, loss 0.511345.
Train: 2018-08-06T00:03:23.491992: step 13302, loss 0.664614.
Train: 2018-08-06T00:03:23.654556: step 13303, loss 0.604945.
Train: 2018-08-06T00:03:23.818144: step 13304, loss 0.545422.
Train: 2018-08-06T00:03:23.985672: step 13305, loss 0.477563.
Train: 2018-08-06T00:03:24.145245: step 13306, loss 0.46902.
Train: 2018-08-06T00:03:24.308807: step 13307, loss 0.596449.
Train: 2018-08-06T00:03:24.473391: step 13308, loss 0.605022.
Train: 2018-08-06T00:03:24.636957: step 13309, loss 0.494158.
Train: 2018-08-06T00:03:24.799495: step 13310, loss 0.579486.
Test: 2018-08-06T00:03:25.036860: step 13310, loss 0.548064.
Train: 2018-08-06T00:03:25.202448: step 13311, loss 0.570947.
Train: 2018-08-06T00:03:25.370966: step 13312, loss 0.493849.
Train: 2018-08-06T00:03:25.529573: step 13313, loss 0.588156.
Train: 2018-08-06T00:03:25.688149: step 13314, loss 0.493581.
Train: 2018-08-06T00:03:25.851680: step 13315, loss 0.536527.
Train: 2018-08-06T00:03:26.027211: step 13316, loss 0.545088.
Train: 2018-08-06T00:03:26.188809: step 13317, loss 0.545063.
Train: 2018-08-06T00:03:26.350372: step 13318, loss 0.588527.
Train: 2018-08-06T00:03:26.508922: step 13319, loss 0.536261.
Train: 2018-08-06T00:03:26.682490: step 13320, loss 0.571188.
Test: 2018-08-06T00:03:26.920822: step 13320, loss 0.547782.
Train: 2018-08-06T00:03:27.093359: step 13321, loss 0.536163.
Train: 2018-08-06T00:03:27.253931: step 13322, loss 0.597548.
Train: 2018-08-06T00:03:27.421516: step 13323, loss 0.571268.
Train: 2018-08-06T00:03:27.585046: step 13324, loss 0.58007.
Train: 2018-08-06T00:03:27.749618: step 13325, loss 0.571244.
Train: 2018-08-06T00:03:27.914189: step 13326, loss 0.536109.
Train: 2018-08-06T00:03:28.077758: step 13327, loss 0.527302.
Train: 2018-08-06T00:03:28.234309: step 13328, loss 0.597659.
Train: 2018-08-06T00:03:28.398869: step 13329, loss 0.562464.
Train: 2018-08-06T00:03:28.558473: step 13330, loss 0.580065.
Test: 2018-08-06T00:03:28.794810: step 13330, loss 0.547742.
Train: 2018-08-06T00:03:28.971337: step 13331, loss 0.536087.
Train: 2018-08-06T00:03:29.136919: step 13332, loss 0.597637.
Train: 2018-08-06T00:03:29.306441: step 13333, loss 0.509773.
Train: 2018-08-06T00:03:29.466015: step 13334, loss 0.492208.
Train: 2018-08-06T00:03:29.626616: step 13335, loss 0.562476.
Train: 2018-08-06T00:03:29.791173: step 13336, loss 0.553672.
Train: 2018-08-06T00:03:29.954740: step 13337, loss 0.544858.
Train: 2018-08-06T00:03:30.117297: step 13338, loss 0.544846.
Train: 2018-08-06T00:03:30.282841: step 13339, loss 0.509504.
Train: 2018-08-06T00:03:30.443400: step 13340, loss 0.597914.
Test: 2018-08-06T00:03:30.679783: step 13340, loss 0.547681.
Train: 2018-08-06T00:03:30.843332: step 13341, loss 0.606819.
Train: 2018-08-06T00:03:31.007917: step 13342, loss 0.518223.
Train: 2018-08-06T00:03:31.169459: step 13343, loss 0.553659.
Train: 2018-08-06T00:03:31.343992: step 13344, loss 0.562521.
Train: 2018-08-06T00:03:31.507554: step 13345, loss 0.527042.
Train: 2018-08-06T00:03:31.668156: step 13346, loss 0.54477.
Train: 2018-08-06T00:03:31.840663: step 13347, loss 0.562537.
Train: 2018-08-06T00:03:32.002231: step 13348, loss 0.52698.
Train: 2018-08-06T00:03:32.176765: step 13349, loss 0.544752.
Train: 2018-08-06T00:03:32.337361: step 13350, loss 0.624947.
Test: 2018-08-06T00:03:32.575698: step 13350, loss 0.547643.
Train: 2018-08-06T00:03:32.739262: step 13351, loss 0.589283.
Train: 2018-08-06T00:03:32.902824: step 13352, loss 0.607066.
Train: 2018-08-06T00:03:33.065388: step 13353, loss 0.62475.
Train: 2018-08-06T00:03:33.231974: step 13354, loss 0.61567.
Train: 2018-08-06T00:03:33.394508: step 13355, loss 0.562497.
Train: 2018-08-06T00:03:33.585029: step 13356, loss 0.536083.
Train: 2018-08-06T00:03:33.744603: step 13357, loss 0.580005.
Train: 2018-08-06T00:03:33.905173: step 13358, loss 0.571189.
Train: 2018-08-06T00:03:34.069728: step 13359, loss 0.501375.
Train: 2018-08-06T00:03:34.244236: step 13360, loss 0.614674.
Test: 2018-08-06T00:03:34.481602: step 13360, loss 0.547865.
Train: 2018-08-06T00:03:34.647189: step 13361, loss 0.631916.
Train: 2018-08-06T00:03:34.806731: step 13362, loss 0.52778.
Train: 2018-08-06T00:03:34.969330: step 13363, loss 0.579669.
Train: 2018-08-06T00:03:35.153824: step 13364, loss 0.596843.
Train: 2018-08-06T00:03:35.315402: step 13365, loss 0.596734.
Train: 2018-08-06T00:03:35.483946: step 13366, loss 0.58806.
Train: 2018-08-06T00:03:35.647484: step 13367, loss 0.562401.
Train: 2018-08-06T00:03:35.810048: step 13368, loss 0.570902.
Train: 2018-08-06T00:03:35.970649: step 13369, loss 0.553945.
Train: 2018-08-06T00:03:36.136202: step 13370, loss 0.613111.
Test: 2018-08-06T00:03:36.372569: step 13370, loss 0.548332.
Train: 2018-08-06T00:03:36.537103: step 13371, loss 0.545593.
Train: 2018-08-06T00:03:36.704687: step 13372, loss 0.562447.
Train: 2018-08-06T00:03:36.868219: step 13373, loss 0.612744.
Train: 2018-08-06T00:03:37.031781: step 13374, loss 0.579187.
Train: 2018-08-06T00:03:37.190388: step 13375, loss 0.520819.
Train: 2018-08-06T00:03:37.354925: step 13376, loss 0.529215.
Train: 2018-08-06T00:03:37.519508: step 13377, loss 0.495974.
Train: 2018-08-06T00:03:37.687029: step 13378, loss 0.612442.
Train: 2018-08-06T00:03:37.846632: step 13379, loss 0.612443.
Train: 2018-08-06T00:03:38.017146: step 13380, loss 0.545868.
Test: 2018-08-06T00:03:38.252528: step 13380, loss 0.54858.
Train: 2018-08-06T00:03:38.431039: step 13381, loss 0.55419.
Train: 2018-08-06T00:03:38.617566: step 13382, loss 0.570818.
Train: 2018-08-06T00:03:38.780132: step 13383, loss 0.579133.
Train: 2018-08-06T00:03:38.950678: step 13384, loss 0.612382.
Train: 2018-08-06T00:03:39.113245: step 13385, loss 0.628945.
Train: 2018-08-06T00:03:39.278772: step 13386, loss 0.487962.
Train: 2018-08-06T00:03:39.448343: step 13387, loss 0.521116.
Train: 2018-08-06T00:03:39.613876: step 13388, loss 0.595685.
Train: 2018-08-06T00:03:39.774478: step 13389, loss 0.496199.
Train: 2018-08-06T00:03:39.947981: step 13390, loss 0.537598.
Test: 2018-08-06T00:03:40.185348: step 13390, loss 0.548559.
Train: 2018-08-06T00:03:40.352900: step 13391, loss 0.595788.
Train: 2018-08-06T00:03:40.526435: step 13392, loss 0.620828.
Train: 2018-08-06T00:03:40.693020: step 13393, loss 0.645844.
Train: 2018-08-06T00:03:40.860568: step 13394, loss 0.545849.
Train: 2018-08-06T00:03:41.024105: step 13395, loss 0.53755.
Train: 2018-08-06T00:03:41.193651: step 13396, loss 0.495967.
Train: 2018-08-06T00:03:41.355244: step 13397, loss 0.562492.
Train: 2018-08-06T00:03:41.522806: step 13398, loss 0.537461.
Train: 2018-08-06T00:03:41.686365: step 13399, loss 0.554114.
Train: 2018-08-06T00:03:41.862892: step 13400, loss 0.612706.
Test: 2018-08-06T00:03:42.096237: step 13400, loss 0.548418.
Train: 2018-08-06T00:03:42.868756: step 13401, loss 0.587605.
Train: 2018-08-06T00:03:43.035332: step 13402, loss 0.579222.
Train: 2018-08-06T00:03:43.200865: step 13403, loss 0.545677.
Train: 2018-08-06T00:03:43.366424: step 13404, loss 0.646357.
Train: 2018-08-06T00:03:43.533975: step 13405, loss 0.612746.
Train: 2018-08-06T00:03:43.698559: step 13406, loss 0.503907.
Train: 2018-08-06T00:03:43.865089: step 13407, loss 0.520657.
Train: 2018-08-06T00:03:44.057574: step 13408, loss 0.562464.
Train: 2018-08-06T00:03:44.224128: step 13409, loss 0.570839.
Train: 2018-08-06T00:03:44.398662: step 13410, loss 0.52895.
Test: 2018-08-06T00:03:44.636039: step 13410, loss 0.548404.
Train: 2018-08-06T00:03:44.806602: step 13411, loss 0.520523.
Train: 2018-08-06T00:03:44.971132: step 13412, loss 0.58765.
Train: 2018-08-06T00:03:45.132706: step 13413, loss 0.520357.
Train: 2018-08-06T00:03:45.298256: step 13414, loss 0.604615.
Train: 2018-08-06T00:03:45.471803: step 13415, loss 0.477949.
Train: 2018-08-06T00:03:45.635353: step 13416, loss 0.570899.
Train: 2018-08-06T00:03:45.795926: step 13417, loss 0.502958.
Train: 2018-08-06T00:03:45.974447: step 13418, loss 0.562418.
Train: 2018-08-06T00:03:46.139008: step 13419, loss 0.493998.
Train: 2018-08-06T00:03:46.309565: step 13420, loss 0.570993.
Test: 2018-08-06T00:03:46.545951: step 13420, loss 0.547969.
Train: 2018-08-06T00:03:46.720480: step 13421, loss 0.545158.
Train: 2018-08-06T00:03:46.892022: step 13422, loss 0.545111.
Train: 2018-08-06T00:03:47.059578: step 13423, loss 0.631848.
Train: 2018-08-06T00:03:47.241090: step 13424, loss 0.571137.
Train: 2018-08-06T00:03:47.406677: step 13425, loss 0.605947.
Train: 2018-08-06T00:03:47.577189: step 13426, loss 0.545021.
Train: 2018-08-06T00:03:47.739755: step 13427, loss 0.579841.
Train: 2018-08-06T00:03:47.910299: step 13428, loss 0.553717.
Train: 2018-08-06T00:03:48.069872: step 13429, loss 0.571129.
Train: 2018-08-06T00:03:48.236427: step 13430, loss 0.649487.
Test: 2018-08-06T00:03:48.473793: step 13430, loss 0.547861.
Train: 2018-08-06T00:03:48.643374: step 13431, loss 0.545034.
Train: 2018-08-06T00:03:48.809892: step 13432, loss 0.571092.
Train: 2018-08-06T00:03:48.976449: step 13433, loss 0.597078.
Train: 2018-08-06T00:03:49.140011: step 13434, loss 0.44995.
Train: 2018-08-06T00:03:49.304605: step 13435, loss 0.484526.
Train: 2018-08-06T00:03:49.470128: step 13436, loss 0.536403.
Train: 2018-08-06T00:03:49.632723: step 13437, loss 0.571097.
Train: 2018-08-06T00:03:49.794261: step 13438, loss 0.501522.
Train: 2018-08-06T00:03:49.958845: step 13439, loss 0.562425.
Train: 2018-08-06T00:03:50.126402: step 13440, loss 0.56244.
Test: 2018-08-06T00:03:50.363737: step 13440, loss 0.547778.
Train: 2018-08-06T00:03:50.533314: step 13441, loss 0.597475.
Train: 2018-08-06T00:03:50.709811: step 13442, loss 0.606282.
Train: 2018-08-06T00:03:50.874373: step 13443, loss 0.650104.
Train: 2018-08-06T00:03:51.040957: step 13444, loss 0.579954.
Train: 2018-08-06T00:03:51.217453: step 13445, loss 0.606099.
Train: 2018-08-06T00:03:51.397005: step 13446, loss 0.614683.
Train: 2018-08-06T00:03:51.561565: step 13447, loss 0.571096.
Train: 2018-08-06T00:03:51.731082: step 13448, loss 0.536462.
Train: 2018-08-06T00:03:51.897636: step 13449, loss 0.553764.
Train: 2018-08-06T00:03:52.064220: step 13450, loss 0.57101.
Test: 2018-08-06T00:03:52.300557: step 13450, loss 0.548011.
Train: 2018-08-06T00:03:52.464121: step 13451, loss 0.596759.
Train: 2018-08-06T00:03:52.636658: step 13452, loss 0.562399.
Train: 2018-08-06T00:03:52.806206: step 13453, loss 0.528224.
Train: 2018-08-06T00:03:52.971762: step 13454, loss 0.59654.
Train: 2018-08-06T00:03:53.140342: step 13455, loss 0.528333.
Train: 2018-08-06T00:03:53.320828: step 13456, loss 0.596447.
Train: 2018-08-06T00:03:53.491374: step 13457, loss 0.536918.
Train: 2018-08-06T00:03:53.662938: step 13458, loss 0.536944.
Train: 2018-08-06T00:03:53.821520: step 13459, loss 0.60484.
Train: 2018-08-06T00:03:53.990040: step 13460, loss 0.536979.
Test: 2018-08-06T00:03:54.227406: step 13460, loss 0.548221.
Train: 2018-08-06T00:03:54.395978: step 13461, loss 0.579365.
Train: 2018-08-06T00:03:54.560514: step 13462, loss 0.562417.
Train: 2018-08-06T00:03:54.730089: step 13463, loss 0.570882.
Train: 2018-08-06T00:03:54.898640: step 13464, loss 0.621639.
Train: 2018-08-06T00:03:55.074139: step 13465, loss 0.537083.
Train: 2018-08-06T00:03:55.239698: step 13466, loss 0.587745.
Train: 2018-08-06T00:03:55.409828: step 13467, loss 0.486573.
Train: 2018-08-06T00:03:55.577828: step 13468, loss 0.537132.
Train: 2018-08-06T00:03:55.746375: step 13469, loss 0.604633.
Train: 2018-08-06T00:03:55.912929: step 13470, loss 0.562425.
Test: 2018-08-06T00:03:56.151261: step 13470, loss 0.548281.
Train: 2018-08-06T00:03:56.318849: step 13471, loss 0.562425.
Train: 2018-08-06T00:03:56.483373: step 13472, loss 0.503296.
Train: 2018-08-06T00:03:56.646935: step 13473, loss 0.545499.
Train: 2018-08-06T00:03:56.808503: step 13474, loss 0.55394.
Train: 2018-08-06T00:03:56.974086: step 13475, loss 0.587876.
Train: 2018-08-06T00:03:57.143632: step 13476, loss 0.562406.
Train: 2018-08-06T00:03:57.305176: step 13477, loss 0.553896.
Train: 2018-08-06T00:03:57.473749: step 13478, loss 0.579438.
Train: 2018-08-06T00:03:57.640278: step 13479, loss 0.562401.
Train: 2018-08-06T00:03:57.804869: step 13480, loss 0.605051.
Test: 2018-08-06T00:03:58.037223: step 13480, loss 0.548117.
Train: 2018-08-06T00:03:58.212748: step 13481, loss 0.570929.
Train: 2018-08-06T00:03:58.392296: step 13482, loss 0.562401.
Train: 2018-08-06T00:03:58.564839: step 13483, loss 0.502735.
Train: 2018-08-06T00:03:58.732392: step 13484, loss 0.545336.
Train: 2018-08-06T00:03:58.909884: step 13485, loss 0.511143.
Train: 2018-08-06T00:03:59.072483: step 13486, loss 0.562397.
Train: 2018-08-06T00:03:59.244988: step 13487, loss 0.65675.
Train: 2018-08-06T00:03:59.414533: step 13488, loss 0.562397.
Train: 2018-08-06T00:03:59.584111: step 13489, loss 0.459486.
Train: 2018-08-06T00:03:59.753628: step 13490, loss 0.588174.
Test: 2018-08-06T00:03:59.992986: step 13490, loss 0.54799.
Train: 2018-08-06T00:04:00.163530: step 13491, loss 0.536589.
Train: 2018-08-06T00:04:00.332080: step 13492, loss 0.5107.
Train: 2018-08-06T00:04:00.495656: step 13493, loss 0.536491.
Train: 2018-08-06T00:04:00.669203: step 13494, loss 0.597045.
Train: 2018-08-06T00:04:00.834766: step 13495, loss 0.536389.
Train: 2018-08-06T00:04:01.022235: step 13496, loss 0.571107.
Train: 2018-08-06T00:04:01.184800: step 13497, loss 0.527605.
Train: 2018-08-06T00:04:01.353373: step 13498, loss 0.518823.
Train: 2018-08-06T00:04:01.521899: step 13499, loss 0.606152.
Train: 2018-08-06T00:04:01.689450: step 13500, loss 0.597462.
Test: 2018-08-06T00:04:01.925818: step 13500, loss 0.547778.
Train: 2018-08-06T00:04:02.791900: step 13501, loss 0.571205.
Train: 2018-08-06T00:04:02.975435: step 13502, loss 0.536167.
Train: 2018-08-06T00:04:03.142987: step 13503, loss 0.56245.
Train: 2018-08-06T00:04:03.314503: step 13504, loss 0.536147.
Train: 2018-08-06T00:04:03.482054: step 13505, loss 0.51858.
Train: 2018-08-06T00:04:03.660576: step 13506, loss 0.544889.
Train: 2018-08-06T00:04:03.830131: step 13507, loss 0.580073.
Train: 2018-08-06T00:04:03.998704: step 13508, loss 0.536052.
Train: 2018-08-06T00:04:04.167224: step 13509, loss 0.633035.
Train: 2018-08-06T00:04:04.343759: step 13510, loss 0.500774.
Test: 2018-08-06T00:04:04.579122: step 13510, loss 0.547713.
Train: 2018-08-06T00:04:04.751659: step 13511, loss 0.615417.
Train: 2018-08-06T00:04:04.922203: step 13512, loss 0.536031.
Train: 2018-08-06T00:04:05.086763: step 13513, loss 0.500764.
Train: 2018-08-06T00:04:05.269306: step 13514, loss 0.553664.
Train: 2018-08-06T00:04:05.439850: step 13515, loss 0.544829.
Train: 2018-08-06T00:04:05.604410: step 13516, loss 0.51829.
Train: 2018-08-06T00:04:05.770935: step 13517, loss 0.553658.
Train: 2018-08-06T00:04:05.934527: step 13518, loss 0.544787.
Train: 2018-08-06T00:04:06.103077: step 13519, loss 0.642486.
Train: 2018-08-06T00:04:06.279606: step 13520, loss 0.509252.
Test: 2018-08-06T00:04:06.517938: step 13520, loss 0.547659.
Train: 2018-08-06T00:04:06.684492: step 13521, loss 0.553655.
Train: 2018-08-06T00:04:06.852042: step 13522, loss 0.482529.
Train: 2018-08-06T00:04:07.022619: step 13523, loss 0.518028.
Train: 2018-08-06T00:04:07.189169: step 13524, loss 0.500096.
Train: 2018-08-06T00:04:07.359713: step 13525, loss 0.562611.
Train: 2018-08-06T00:04:07.525272: step 13526, loss 0.49981.
Train: 2018-08-06T00:04:07.692825: step 13527, loss 0.562672.
Train: 2018-08-06T00:04:07.862340: step 13528, loss 0.535622.
Train: 2018-08-06T00:04:08.036898: step 13529, loss 0.617042.
Train: 2018-08-06T00:04:08.205448: step 13530, loss 0.680552.
Test: 2018-08-06T00:04:08.440795: step 13530, loss 0.547578.
Train: 2018-08-06T00:04:08.609352: step 13531, loss 0.562736.
Train: 2018-08-06T00:04:08.778914: step 13532, loss 0.562718.
Train: 2018-08-06T00:04:08.944475: step 13533, loss 0.526599.
Train: 2018-08-06T00:04:09.111032: step 13534, loss 0.499567.
Train: 2018-08-06T00:04:09.280572: step 13535, loss 0.571711.
Train: 2018-08-06T00:04:09.453087: step 13536, loss 0.508591.
Train: 2018-08-06T00:04:09.621636: step 13537, loss 0.571716.
Train: 2018-08-06T00:04:09.790217: step 13538, loss 0.598782.
Train: 2018-08-06T00:04:09.970701: step 13539, loss 0.535643.
Train: 2018-08-06T00:04:10.141245: step 13540, loss 0.55367.
Test: 2018-08-06T00:04:10.378611: step 13540, loss 0.547592.
Train: 2018-08-06T00:04:10.549154: step 13541, loss 0.643737.
Train: 2018-08-06T00:04:10.715709: step 13542, loss 0.571641.
Train: 2018-08-06T00:04:10.883287: step 13543, loss 0.661286.
Train: 2018-08-06T00:04:11.102170: step 13544, loss 0.553656.
Train: 2018-08-06T00:04:11.266761: step 13545, loss 0.589263.
Train: 2018-08-06T00:04:11.448275: step 13546, loss 0.589128.
Train: 2018-08-06T00:04:11.616827: step 13547, loss 0.527169.
Train: 2018-08-06T00:04:11.785343: step 13548, loss 0.509654.
Train: 2018-08-06T00:04:11.948933: step 13549, loss 0.58882.
Train: 2018-08-06T00:04:12.117479: step 13550, loss 0.536156.
Test: 2018-08-06T00:04:12.356814: step 13550, loss 0.547789.
Train: 2018-08-06T00:04:12.529379: step 13551, loss 0.614935.
Train: 2018-08-06T00:04:12.697933: step 13552, loss 0.56243.
Train: 2018-08-06T00:04:12.863460: step 13553, loss 0.527605.
Train: 2018-08-06T00:04:13.035000: step 13554, loss 0.571105.
Train: 2018-08-06T00:04:13.201580: step 13555, loss 0.527712.
Train: 2018-08-06T00:04:13.373097: step 13556, loss 0.571075.
Train: 2018-08-06T00:04:13.543641: step 13557, loss 0.493144.
Train: 2018-08-06T00:04:13.737147: step 13558, loss 0.536425.
Train: 2018-08-06T00:04:13.916645: step 13559, loss 0.527739.
Train: 2018-08-06T00:04:14.082199: step 13560, loss 0.553733.
Test: 2018-08-06T00:04:14.321594: step 13560, loss 0.547862.
Train: 2018-08-06T00:04:14.489146: step 13561, loss 0.597175.
Train: 2018-08-06T00:04:14.668645: step 13562, loss 0.527642.
Train: 2018-08-06T00:04:14.848164: step 13563, loss 0.59723.
Train: 2018-08-06T00:04:15.015737: step 13564, loss 0.510197.
Train: 2018-08-06T00:04:15.192231: step 13565, loss 0.605991.
Train: 2018-08-06T00:04:15.361809: step 13566, loss 0.579853.
Train: 2018-08-06T00:04:15.533350: step 13567, loss 0.53629.
Train: 2018-08-06T00:04:15.703888: step 13568, loss 0.544999.
Train: 2018-08-06T00:04:15.872413: step 13569, loss 0.632152.
Train: 2018-08-06T00:04:16.043987: step 13570, loss 0.536302.
Test: 2018-08-06T00:04:16.284311: step 13570, loss 0.547846.
Train: 2018-08-06T00:04:16.450890: step 13571, loss 0.588527.
Train: 2018-08-06T00:04:16.622408: step 13572, loss 0.54503.
Train: 2018-08-06T00:04:16.789989: step 13573, loss 0.553727.
Train: 2018-08-06T00:04:16.961501: step 13574, loss 0.510311.
Train: 2018-08-06T00:04:17.132074: step 13575, loss 0.53635.
Train: 2018-08-06T00:04:17.300617: step 13576, loss 0.545026.
Train: 2018-08-06T00:04:17.469143: step 13577, loss 0.632066.
Train: 2018-08-06T00:04:17.641706: step 13578, loss 0.571122.
Train: 2018-08-06T00:04:17.806265: step 13579, loss 0.579812.
Train: 2018-08-06T00:04:17.974846: step 13580, loss 0.588482.
Test: 2018-08-06T00:04:18.214150: step 13580, loss 0.54788.
Train: 2018-08-06T00:04:18.389706: step 13581, loss 0.631825.
Train: 2018-08-06T00:04:18.556236: step 13582, loss 0.510482.
Train: 2018-08-06T00:04:18.725815: step 13583, loss 0.553761.
Train: 2018-08-06T00:04:18.892366: step 13584, loss 0.588297.
Train: 2018-08-06T00:04:19.056896: step 13585, loss 0.571018.
Train: 2018-08-06T00:04:19.227440: step 13586, loss 0.536583.
Train: 2018-08-06T00:04:19.392998: step 13587, loss 0.613977.
Train: 2018-08-06T00:04:19.561546: step 13588, loss 0.59672.
Train: 2018-08-06T00:04:19.737077: step 13589, loss 0.588079.
Train: 2018-08-06T00:04:19.894686: step 13590, loss 0.562399.
Test: 2018-08-06T00:04:20.132021: step 13590, loss 0.548136.
Train: 2018-08-06T00:04:20.303564: step 13591, loss 0.596475.
Train: 2018-08-06T00:04:20.476125: step 13592, loss 0.562408.
Train: 2018-08-06T00:04:20.643653: step 13593, loss 0.562414.
Train: 2018-08-06T00:04:20.815194: step 13594, loss 0.596243.
Train: 2018-08-06T00:04:20.980781: step 13595, loss 0.570863.
Train: 2018-08-06T00:04:21.159274: step 13596, loss 0.671817.
Train: 2018-08-06T00:04:21.327822: step 13597, loss 0.512194.
Train: 2018-08-06T00:04:21.497395: step 13598, loss 0.579183.
Train: 2018-08-06T00:04:21.667944: step 13599, loss 0.637478.
Train: 2018-08-06T00:04:21.837489: step 13600, loss 0.628915.
Test: 2018-08-06T00:04:22.061859: step 13600, loss 0.548718.
Train: 2018-08-06T00:04:22.796343: step 13601, loss 0.546032.
Train: 2018-08-06T00:04:22.968851: step 13602, loss 0.570815.
Train: 2018-08-06T00:04:23.148370: step 13603, loss 0.505203.
Train: 2018-08-06T00:04:23.322904: step 13604, loss 0.497123.
Train: 2018-08-06T00:04:23.501426: step 13605, loss 0.570821.
Train: 2018-08-06T00:04:23.668978: step 13606, loss 0.51348.
Train: 2018-08-06T00:04:23.835562: step 13607, loss 0.595427.
Train: 2018-08-06T00:04:24.005105: step 13608, loss 0.570817.
Train: 2018-08-06T00:04:24.178643: step 13609, loss 0.611894.
Train: 2018-08-06T00:04:24.349184: step 13610, loss 0.6201.
Test: 2018-08-06T00:04:24.585527: step 13610, loss 0.548874.
Train: 2018-08-06T00:04:24.756072: step 13611, loss 0.513389.
Train: 2018-08-06T00:04:24.927613: step 13612, loss 0.480557.
Train: 2018-08-06T00:04:25.093170: step 13613, loss 0.595484.
Train: 2018-08-06T00:04:25.259724: step 13614, loss 0.546112.
Train: 2018-08-06T00:04:25.427301: step 13615, loss 0.620306.
Train: 2018-08-06T00:04:25.600843: step 13616, loss 0.603827.
Train: 2018-08-06T00:04:25.771356: step 13617, loss 0.529555.
Train: 2018-08-06T00:04:25.942897: step 13618, loss 0.521268.
Train: 2018-08-06T00:04:26.114438: step 13619, loss 0.546.
Train: 2018-08-06T00:04:26.286011: step 13620, loss 0.587391.
Test: 2018-08-06T00:04:26.522378: step 13620, loss 0.548616.
Train: 2018-08-06T00:04:26.752850: step 13621, loss 0.496117.
Train: 2018-08-06T00:04:26.930401: step 13622, loss 0.604114.
Train: 2018-08-06T00:04:27.103943: step 13623, loss 0.579163.
Train: 2018-08-06T00:04:27.274456: step 13624, loss 0.562477.
Train: 2018-08-06T00:04:27.454973: step 13625, loss 0.621002.
Train: 2018-08-06T00:04:27.624550: step 13626, loss 0.587558.
Train: 2018-08-06T00:04:27.797058: step 13627, loss 0.595914.
Train: 2018-08-06T00:04:27.968631: step 13628, loss 0.604247.
Train: 2018-08-06T00:04:28.140169: step 13629, loss 0.570825.
Train: 2018-08-06T00:04:28.310684: step 13630, loss 0.537498.
Test: 2018-08-06T00:04:28.548049: step 13630, loss 0.548551.
Train: 2018-08-06T00:04:28.716599: step 13631, loss 0.504212.
Train: 2018-08-06T00:04:28.892130: step 13632, loss 0.570823.
Train: 2018-08-06T00:04:29.062674: step 13633, loss 0.570825.
Train: 2018-08-06T00:04:29.236240: step 13634, loss 0.51242.
Train: 2018-08-06T00:04:29.402802: step 13635, loss 0.595903.
Train: 2018-08-06T00:04:29.576299: step 13636, loss 0.529004.
Train: 2018-08-06T00:04:29.744850: step 13637, loss 0.512182.
Train: 2018-08-06T00:04:29.920381: step 13638, loss 0.570846.
Train: 2018-08-06T00:04:30.088928: step 13639, loss 0.570856.
Train: 2018-08-06T00:04:30.267452: step 13640, loss 0.520245.
Test: 2018-08-06T00:04:30.504815: step 13640, loss 0.548252.
Train: 2018-08-06T00:04:30.678354: step 13641, loss 0.587797.
Train: 2018-08-06T00:04:30.847942: step 13642, loss 0.46918.
Train: 2018-08-06T00:04:31.022433: step 13643, loss 0.536885.
Train: 2018-08-06T00:04:31.193974: step 13644, loss 0.596557.
Train: 2018-08-06T00:04:31.371498: step 13645, loss 0.57096.
Train: 2018-08-06T00:04:31.546057: step 13646, loss 0.605314.
Train: 2018-08-06T00:04:31.715579: step 13647, loss 0.570992.
Train: 2018-08-06T00:04:31.894101: step 13648, loss 0.553795.
Train: 2018-08-06T00:04:32.063648: step 13649, loss 0.614065.
Train: 2018-08-06T00:04:32.240176: step 13650, loss 0.596839.
Test: 2018-08-06T00:04:32.475546: step 13650, loss 0.547989.
Train: 2018-08-06T00:04:32.648084: step 13651, loss 0.657036.
Train: 2018-08-06T00:04:32.847550: step 13652, loss 0.596723.
Train: 2018-08-06T00:04:33.025094: step 13653, loss 0.493948.
Train: 2018-08-06T00:04:33.197614: step 13654, loss 0.519674.
Train: 2018-08-06T00:04:33.363172: step 13655, loss 0.588025.
Train: 2018-08-06T00:04:33.549704: step 13656, loss 0.605077.
Train: 2018-08-06T00:04:33.719248: step 13657, loss 0.460134.
Train: 2018-08-06T00:04:33.894797: step 13658, loss 0.545345.
Train: 2018-08-06T00:04:34.066292: step 13659, loss 0.588009.
Train: 2018-08-06T00:04:34.233874: step 13660, loss 0.55386.
Test: 2018-08-06T00:04:34.471211: step 13660, loss 0.548089.
Train: 2018-08-06T00:04:34.640756: step 13661, loss 0.570943.
Train: 2018-08-06T00:04:34.812297: step 13662, loss 0.536758.
Train: 2018-08-06T00:04:34.982841: step 13663, loss 0.605167.
Train: 2018-08-06T00:04:35.155378: step 13664, loss 0.48542.
Train: 2018-08-06T00:04:35.332904: step 13665, loss 0.545266.
Train: 2018-08-06T00:04:35.503452: step 13666, loss 0.605294.
Train: 2018-08-06T00:04:35.678012: step 13667, loss 0.553812.
Train: 2018-08-06T00:04:35.845534: step 13668, loss 0.648308.
Train: 2018-08-06T00:04:36.021095: step 13669, loss 0.631056.
Train: 2018-08-06T00:04:36.190611: step 13670, loss 0.59665.
Test: 2018-08-06T00:04:36.427976: step 13670, loss 0.548096.
Train: 2018-08-06T00:04:36.610489: step 13671, loss 0.494077.
Train: 2018-08-06T00:04:36.778040: step 13672, loss 0.55387.
Train: 2018-08-06T00:04:36.947611: step 13673, loss 0.485679.
Train: 2018-08-06T00:04:37.127131: step 13674, loss 0.545336.
Train: 2018-08-06T00:04:37.300643: step 13675, loss 0.536775.
Train: 2018-08-06T00:04:37.473180: step 13676, loss 0.60517.
Train: 2018-08-06T00:04:37.645719: step 13677, loss 0.55384.
Train: 2018-08-06T00:04:37.816264: step 13678, loss 0.553833.
Train: 2018-08-06T00:04:37.985809: step 13679, loss 0.596682.
Train: 2018-08-06T00:04:38.186274: step 13680, loss 0.545253.
Test: 2018-08-06T00:04:38.425634: step 13680, loss 0.548036.
Train: 2018-08-06T00:04:38.597207: step 13681, loss 0.493798.
Train: 2018-08-06T00:04:38.768715: step 13682, loss 0.613932.
Train: 2018-08-06T00:04:38.945243: step 13683, loss 0.49365.
Train: 2018-08-06T00:04:39.117782: step 13684, loss 0.588225.
Train: 2018-08-06T00:04:39.286332: step 13685, loss 0.622724.
Train: 2018-08-06T00:04:39.453914: step 13686, loss 0.614095.
Train: 2018-08-06T00:04:39.628417: step 13687, loss 0.502159.
Train: 2018-08-06T00:04:39.799966: step 13688, loss 0.57961.
Train: 2018-08-06T00:04:39.975489: step 13689, loss 0.61402.
Train: 2018-08-06T00:04:40.147041: step 13690, loss 0.545212.
Test: 2018-08-06T00:04:40.386390: step 13690, loss 0.548018.
Train: 2018-08-06T00:04:40.562948: step 13691, loss 0.536641.
Train: 2018-08-06T00:04:40.733461: step 13692, loss 0.528064.
Train: 2018-08-06T00:04:40.904005: step 13693, loss 0.55381.
Train: 2018-08-06T00:04:41.078539: step 13694, loss 0.579582.
Train: 2018-08-06T00:04:41.251105: step 13695, loss 0.553804.
Train: 2018-08-06T00:04:41.420655: step 13696, loss 0.536609.
Train: 2018-08-06T00:04:41.592165: step 13697, loss 0.53659.
Train: 2018-08-06T00:04:41.765733: step 13698, loss 0.545174.
Train: 2018-08-06T00:04:41.940235: step 13699, loss 0.571025.
Train: 2018-08-06T00:04:42.116762: step 13700, loss 0.545135.
Test: 2018-08-06T00:04:42.354127: step 13700, loss 0.547926.
Train: 2018-08-06T00:04:43.099451: step 13701, loss 0.536471.
Train: 2018-08-06T00:04:43.270997: step 13702, loss 0.519117.
Train: 2018-08-06T00:04:43.447491: step 13703, loss 0.571089.
Train: 2018-08-06T00:04:43.618033: step 13704, loss 0.562417.
Train: 2018-08-06T00:04:43.790574: step 13705, loss 0.649477.
Train: 2018-08-06T00:04:43.961115: step 13706, loss 0.518907.
Train: 2018-08-06T00:04:44.135651: step 13707, loss 0.536302.
Train: 2018-08-06T00:04:44.311181: step 13708, loss 0.475286.
Train: 2018-08-06T00:04:44.480727: step 13709, loss 0.588637.
Train: 2018-08-06T00:04:44.649275: step 13710, loss 0.536198.
Test: 2018-08-06T00:04:44.884646: step 13710, loss 0.547773.
Train: 2018-08-06T00:04:45.057211: step 13711, loss 0.588738.
Train: 2018-08-06T00:04:45.238700: step 13712, loss 0.58877.
Train: 2018-08-06T00:04:45.411238: step 13713, loss 0.562456.
Train: 2018-08-06T00:04:45.577793: step 13714, loss 0.492246.
Train: 2018-08-06T00:04:45.747340: step 13715, loss 0.544888.
Train: 2018-08-06T00:04:45.916886: step 13716, loss 0.562473.
Train: 2018-08-06T00:04:46.082444: step 13717, loss 0.544857.
Train: 2018-08-06T00:04:46.251990: step 13718, loss 0.553665.
Train: 2018-08-06T00:04:46.424530: step 13719, loss 0.668503.
Train: 2018-08-06T00:04:46.593077: step 13720, loss 0.54484.
Test: 2018-08-06T00:04:46.829445: step 13720, loss 0.547716.
Train: 2018-08-06T00:04:47.004977: step 13721, loss 0.544847.
Train: 2018-08-06T00:04:47.176517: step 13722, loss 0.518406.
Train: 2018-08-06T00:04:47.348059: step 13723, loss 0.544848.
Train: 2018-08-06T00:04:47.515612: step 13724, loss 0.588956.
Train: 2018-08-06T00:04:47.687153: step 13725, loss 0.483097.
Train: 2018-08-06T00:04:47.870661: step 13726, loss 0.606653.
Train: 2018-08-06T00:04:48.040207: step 13727, loss 0.641991.
Train: 2018-08-06T00:04:48.208757: step 13728, loss 0.562486.
Train: 2018-08-06T00:04:48.380300: step 13729, loss 0.51844.
Train: 2018-08-06T00:04:48.548878: step 13730, loss 0.544869.
Test: 2018-08-06T00:04:48.790233: step 13730, loss 0.547735.
Train: 2018-08-06T00:04:48.975706: step 13731, loss 0.536074.
Train: 2018-08-06T00:04:49.145252: step 13732, loss 0.553672.
Train: 2018-08-06T00:04:49.317791: step 13733, loss 0.562473.
Train: 2018-08-06T00:04:49.493353: step 13734, loss 0.553672.
Train: 2018-08-06T00:04:49.691791: step 13735, loss 0.536068.
Train: 2018-08-06T00:04:49.875350: step 13736, loss 0.571281.
Train: 2018-08-06T00:04:50.057812: step 13737, loss 0.615315.
Train: 2018-08-06T00:04:50.230350: step 13738, loss 0.553673.
Train: 2018-08-06T00:04:50.409870: step 13739, loss 0.553675.
Train: 2018-08-06T00:04:50.584404: step 13740, loss 0.509755.
Test: 2018-08-06T00:04:50.823763: step 13740, loss 0.547748.
Train: 2018-08-06T00:04:50.992343: step 13741, loss 0.487485.
Train: 2018-08-06T00:04:51.167842: step 13742, loss 0.580071.
Train: 2018-08-06T00:04:51.380274: step 13743, loss 0.615325.
Train: 2018-08-06T00:04:51.553812: step 13744, loss 0.509642.
Train: 2018-08-06T00:04:51.733360: step 13745, loss 0.632969.
Train: 2018-08-06T00:04:51.907889: step 13746, loss 0.597689.
Train: 2018-08-06T00:04:52.085391: step 13747, loss 0.562465.
Train: 2018-08-06T00:04:52.269906: step 13748, loss 0.571234.
Train: 2018-08-06T00:04:52.476343: step 13749, loss 0.50987.
Train: 2018-08-06T00:04:52.646889: step 13750, loss 0.571203.
Test: 2018-08-06T00:04:52.886247: step 13750, loss 0.547786.
Train: 2018-08-06T00:04:53.064801: step 13751, loss 0.588697.
Train: 2018-08-06T00:04:53.236311: step 13752, loss 0.64984.
Train: 2018-08-06T00:04:53.418832: step 13753, loss 0.536277.
Train: 2018-08-06T00:04:53.594386: step 13754, loss 0.536325.
Train: 2018-08-06T00:04:53.770882: step 13755, loss 0.545052.
Train: 2018-08-06T00:04:53.967356: step 13756, loss 0.501706.
Train: 2018-08-06T00:04:54.152860: step 13757, loss 0.562412.
Train: 2018-08-06T00:04:54.360305: step 13758, loss 0.545059.
Train: 2018-08-06T00:04:54.523869: step 13759, loss 0.562409.
Train: 2018-08-06T00:04:54.696408: step 13760, loss 0.579778.
Test: 2018-08-06T00:04:54.931778: step 13760, loss 0.547875.
Train: 2018-08-06T00:04:55.103319: step 13761, loss 0.53637.
Train: 2018-08-06T00:04:55.273886: step 13762, loss 0.536361.
Train: 2018-08-06T00:04:55.442437: step 13763, loss 0.614546.
Train: 2018-08-06T00:04:55.617943: step 13764, loss 0.518993.
Train: 2018-08-06T00:04:55.786491: step 13765, loss 0.55373.
Train: 2018-08-06T00:04:55.963018: step 13766, loss 0.518952.
Train: 2018-08-06T00:04:56.134585: step 13767, loss 0.518908.
Train: 2018-08-06T00:04:56.303109: step 13768, loss 0.623464.
Train: 2018-08-06T00:04:56.476647: step 13769, loss 0.606038.
Train: 2018-08-06T00:04:56.654204: step 13770, loss 0.518821.
Test: 2018-08-06T00:04:56.891592: step 13770, loss 0.547822.
Train: 2018-08-06T00:04:57.070084: step 13771, loss 0.562427.
Train: 2018-08-06T00:04:57.253598: step 13772, loss 0.544975.
Train: 2018-08-06T00:04:57.430095: step 13773, loss 0.553712.
Train: 2018-08-06T00:04:57.602667: step 13774, loss 0.614806.
Train: 2018-08-06T00:04:57.769225: step 13775, loss 0.544995.
Train: 2018-08-06T00:04:57.937737: step 13776, loss 0.527555.
Train: 2018-08-06T00:04:58.109310: step 13777, loss 0.457717.
Train: 2018-08-06T00:04:58.290794: step 13778, loss 0.474978.
Train: 2018-08-06T00:04:58.495247: step 13779, loss 0.579993.
Train: 2018-08-06T00:04:58.664793: step 13780, loss 0.536012.
Test: 2018-08-06T00:04:58.904163: step 13780, loss 0.547698.
Train: 2018-08-06T00:04:59.088685: step 13781, loss 0.509426.
Train: 2018-08-06T00:04:59.255246: step 13782, loss 0.527103.
Train: 2018-08-06T00:04:59.426757: step 13783, loss 0.535975.
Train: 2018-08-06T00:04:59.593311: step 13784, loss 0.517555.
Train: 2018-08-06T00:04:59.761860: step 13785, loss 0.51764.
Train: 2018-08-06T00:04:59.936393: step 13786, loss 0.535763.
Train: 2018-08-06T00:05:00.106937: step 13787, loss 0.516915.
Train: 2018-08-06T00:05:00.274488: step 13788, loss 0.561906.
Train: 2018-08-06T00:05:00.445032: step 13789, loss 0.581525.
Train: 2018-08-06T00:05:00.613583: step 13790, loss 0.572769.
Test: 2018-08-06T00:05:00.850948: step 13790, loss 0.54767.
Train: 2018-08-06T00:05:01.031466: step 13791, loss 0.542217.
Train: 2018-08-06T00:05:01.215972: step 13792, loss 0.5442.
Train: 2018-08-06T00:05:01.390539: step 13793, loss 0.5272.
Train: 2018-08-06T00:05:01.567032: step 13794, loss 0.603267.
Train: 2018-08-06T00:05:01.743593: step 13795, loss 0.602462.
Train: 2018-08-06T00:05:01.920118: step 13796, loss 0.537371.
Train: 2018-08-06T00:05:02.095619: step 13797, loss 0.555041.
Train: 2018-08-06T00:05:02.275139: step 13798, loss 0.52435.
Train: 2018-08-06T00:05:02.445717: step 13799, loss 0.591487.
Train: 2018-08-06T00:05:02.620247: step 13800, loss 0.581404.
Test: 2018-08-06T00:05:02.857605: step 13800, loss 0.54757.
Train: 2018-08-06T00:05:03.614224: step 13801, loss 0.535438.
Train: 2018-08-06T00:05:03.793769: step 13802, loss 0.517409.
Train: 2018-08-06T00:05:03.973264: step 13803, loss 0.535564.
Train: 2018-08-06T00:05:04.136858: step 13804, loss 0.598925.
Train: 2018-08-06T00:05:04.307395: step 13805, loss 0.580711.
Train: 2018-08-06T00:05:04.477945: step 13806, loss 0.54483.
Train: 2018-08-06T00:05:04.644470: step 13807, loss 0.499856.
Train: 2018-08-06T00:05:04.827005: step 13808, loss 0.526569.
Train: 2018-08-06T00:05:04.998553: step 13809, loss 0.589361.
Train: 2018-08-06T00:05:05.170062: step 13810, loss 0.56243.
Test: 2018-08-06T00:05:05.407437: step 13810, loss 0.547618.
Train: 2018-08-06T00:05:05.577998: step 13811, loss 0.56251.
Train: 2018-08-06T00:05:05.746552: step 13812, loss 0.598763.
Train: 2018-08-06T00:05:05.922084: step 13813, loss 0.580731.
Train: 2018-08-06T00:05:06.095587: step 13814, loss 0.509104.
Train: 2018-08-06T00:05:06.262167: step 13815, loss 0.517775.
Train: 2018-08-06T00:05:06.428727: step 13816, loss 0.61614.
Train: 2018-08-06T00:05:06.593257: step 13817, loss 0.607491.
Train: 2018-08-06T00:05:06.763801: step 13818, loss 0.642201.
Train: 2018-08-06T00:05:06.929358: step 13819, loss 0.562494.
Train: 2018-08-06T00:05:07.097938: step 13820, loss 0.614982.
Test: 2018-08-06T00:05:07.335273: step 13820, loss 0.547814.
Train: 2018-08-06T00:05:07.504846: step 13821, loss 0.519234.
Train: 2018-08-06T00:05:07.677388: step 13822, loss 0.562266.
Train: 2018-08-06T00:05:07.846931: step 13823, loss 0.597285.
Train: 2018-08-06T00:05:08.022459: step 13824, loss 0.544503.
Train: 2018-08-06T00:05:08.192979: step 13825, loss 0.648535.
Train: 2018-08-06T00:05:08.361542: step 13826, loss 0.570349.
Train: 2018-08-06T00:05:08.536061: step 13827, loss 0.528229.
Train: 2018-08-06T00:05:08.706617: step 13828, loss 0.51859.
Train: 2018-08-06T00:05:08.882135: step 13829, loss 0.545713.
Train: 2018-08-06T00:05:09.051708: step 13830, loss 0.569811.
Test: 2018-08-06T00:05:09.289058: step 13830, loss 0.548577.
Train: 2018-08-06T00:05:09.460619: step 13831, loss 0.620585.
Train: 2018-08-06T00:05:09.655068: step 13832, loss 0.604389.
Train: 2018-08-06T00:05:09.821653: step 13833, loss 0.490773.
Train: 2018-08-06T00:05:10.005163: step 13834, loss 0.562492.
Train: 2018-08-06T00:05:10.176705: step 13835, loss 0.512822.
Train: 2018-08-06T00:05:10.350240: step 13836, loss 0.56068.
Train: 2018-08-06T00:05:10.519756: step 13837, loss 0.557161.
Train: 2018-08-06T00:05:10.688306: step 13838, loss 0.578861.
Train: 2018-08-06T00:05:10.853893: step 13839, loss 0.552633.
Train: 2018-08-06T00:05:11.022413: step 13840, loss 0.537119.
Test: 2018-08-06T00:05:11.260774: step 13840, loss 0.54833.
Train: 2018-08-06T00:05:11.426332: step 13841, loss 0.622385.
Train: 2018-08-06T00:05:11.597873: step 13842, loss 0.553426.
Train: 2018-08-06T00:05:11.771440: step 13843, loss 0.537406.
Train: 2018-08-06T00:05:11.951952: step 13844, loss 0.579986.
Train: 2018-08-06T00:05:12.117513: step 13845, loss 0.580541.
Train: 2018-08-06T00:05:12.286063: step 13846, loss 0.546021.
Train: 2018-08-06T00:05:12.453615: step 13847, loss 0.603215.
Train: 2018-08-06T00:05:12.620173: step 13848, loss 0.605483.
Train: 2018-08-06T00:05:12.789716: step 13849, loss 0.546197.
Train: 2018-08-06T00:05:12.961245: step 13850, loss 0.51941.
Test: 2018-08-06T00:05:13.200587: step 13850, loss 0.548052.
Train: 2018-08-06T00:05:13.372159: step 13851, loss 0.571238.
Train: 2018-08-06T00:05:13.540677: step 13852, loss 0.570568.
Train: 2018-08-06T00:05:13.710224: step 13853, loss 0.571355.
Train: 2018-08-06T00:05:13.881791: step 13854, loss 0.562657.
Train: 2018-08-06T00:05:14.054329: step 13855, loss 0.56203.
Train: 2018-08-06T00:05:14.225875: step 13856, loss 0.571042.
Train: 2018-08-06T00:05:14.395392: step 13857, loss 0.596287.
Train: 2018-08-06T00:05:14.560979: step 13858, loss 0.528404.
Train: 2018-08-06T00:05:14.727534: step 13859, loss 0.545594.
Train: 2018-08-06T00:05:14.897079: step 13860, loss 0.579491.
Test: 2018-08-06T00:05:15.133418: step 13860, loss 0.548014.
Train: 2018-08-06T00:05:15.301966: step 13861, loss 0.596912.
Train: 2018-08-06T00:05:15.474505: step 13862, loss 0.613548.
Train: 2018-08-06T00:05:15.647045: step 13863, loss 0.536519.
Train: 2018-08-06T00:05:15.815619: step 13864, loss 0.562395.
Train: 2018-08-06T00:05:15.982154: step 13865, loss 0.571224.
Train: 2018-08-06T00:05:16.150722: step 13866, loss 0.55357.
Train: 2018-08-06T00:05:16.318280: step 13867, loss 0.476674.
Train: 2018-08-06T00:05:16.485801: step 13868, loss 0.562563.
Train: 2018-08-06T00:05:16.655379: step 13869, loss 0.536372.
Train: 2018-08-06T00:05:16.825922: step 13870, loss 0.49419.
Test: 2018-08-06T00:05:17.063256: step 13870, loss 0.548023.
Train: 2018-08-06T00:05:17.233834: step 13871, loss 0.588215.
Train: 2018-08-06T00:05:17.403348: step 13872, loss 0.631879.
Train: 2018-08-06T00:05:17.575916: step 13873, loss 0.613666.
Train: 2018-08-06T00:05:17.741443: step 13874, loss 0.528445.
Train: 2018-08-06T00:05:17.913010: step 13875, loss 0.691851.
Train: 2018-08-06T00:05:18.083559: step 13876, loss 0.553762.
Train: 2018-08-06T00:05:18.250082: step 13877, loss 0.536392.
Train: 2018-08-06T00:05:18.423662: step 13878, loss 0.502215.
Train: 2018-08-06T00:05:18.589207: step 13879, loss 0.639504.
Train: 2018-08-06T00:05:18.758755: step 13880, loss 0.579143.
Test: 2018-08-06T00:05:18.999079: step 13880, loss 0.548139.
Train: 2018-08-06T00:05:19.171643: step 13881, loss 0.545134.
Train: 2018-08-06T00:05:19.341164: step 13882, loss 0.604922.
Train: 2018-08-06T00:05:19.509745: step 13883, loss 0.56274.
Train: 2018-08-06T00:05:19.673307: step 13884, loss 0.629978.
Train: 2018-08-06T00:05:19.842854: step 13885, loss 0.604679.
Train: 2018-08-06T00:05:20.011423: step 13886, loss 0.537585.
Train: 2018-08-06T00:05:20.177957: step 13887, loss 0.562377.
Train: 2018-08-06T00:05:20.342486: step 13888, loss 0.545837.
Train: 2018-08-06T00:05:20.512060: step 13889, loss 0.595395.
Train: 2018-08-06T00:05:20.680582: step 13890, loss 0.537827.
Test: 2018-08-06T00:05:20.922943: step 13890, loss 0.548602.
Train: 2018-08-06T00:05:21.095473: step 13891, loss 0.570567.
Train: 2018-08-06T00:05:21.261055: step 13892, loss 0.597816.
Train: 2018-08-06T00:05:21.428593: step 13893, loss 0.570191.
Train: 2018-08-06T00:05:21.599127: step 13894, loss 0.519926.
Train: 2018-08-06T00:05:21.767708: step 13895, loss 0.569793.
Train: 2018-08-06T00:05:21.935227: step 13896, loss 0.570461.
Train: 2018-08-06T00:05:22.105771: step 13897, loss 0.511945.
Train: 2018-08-06T00:05:22.275349: step 13898, loss 0.613442.
Train: 2018-08-06T00:05:22.444895: step 13899, loss 0.537943.
Train: 2018-08-06T00:05:22.613448: step 13900, loss 0.610884.
Test: 2018-08-06T00:05:22.852773: step 13900, loss 0.548944.
Train: 2018-08-06T00:05:23.602996: step 13901, loss 0.563301.
Train: 2018-08-06T00:05:23.773535: step 13902, loss 0.514163.
Train: 2018-08-06T00:05:23.942056: step 13903, loss 0.589358.
Train: 2018-08-06T00:05:24.108610: step 13904, loss 0.54601.
Train: 2018-08-06T00:05:24.276163: step 13905, loss 0.562853.
Train: 2018-08-06T00:05:24.446740: step 13906, loss 0.579873.
Train: 2018-08-06T00:05:24.616253: step 13907, loss 0.579163.
Train: 2018-08-06T00:05:24.781809: step 13908, loss 0.57073.
Train: 2018-08-06T00:05:24.950384: step 13909, loss 0.546518.
Train: 2018-08-06T00:05:25.115947: step 13910, loss 0.495677.
Test: 2018-08-06T00:05:25.353282: step 13910, loss 0.548378.
Train: 2018-08-06T00:05:25.522829: step 13911, loss 0.638268.
Train: 2018-08-06T00:05:25.690380: step 13912, loss 0.52809.
Train: 2018-08-06T00:05:25.854940: step 13913, loss 0.56289.
Train: 2018-08-06T00:05:26.022493: step 13914, loss 0.597027.
Train: 2018-08-06T00:05:26.195031: step 13915, loss 0.536862.
Train: 2018-08-06T00:05:26.366573: step 13916, loss 0.503089.
Train: 2018-08-06T00:05:26.531165: step 13917, loss 0.562386.
Train: 2018-08-06T00:05:26.705665: step 13918, loss 0.605235.
Train: 2018-08-06T00:05:26.871222: step 13919, loss 0.528155.
Train: 2018-08-06T00:05:27.044758: step 13920, loss 0.570943.
Test: 2018-08-06T00:05:27.280128: step 13920, loss 0.547985.
Train: 2018-08-06T00:05:27.450673: step 13921, loss 0.579653.
Train: 2018-08-06T00:05:27.620244: step 13922, loss 0.596871.
Train: 2018-08-06T00:05:27.786802: step 13923, loss 0.579663.
Train: 2018-08-06T00:05:27.966293: step 13924, loss 0.588227.
Train: 2018-08-06T00:05:28.133877: step 13925, loss 0.596844.
Train: 2018-08-06T00:05:28.308379: step 13926, loss 0.656982.
Train: 2018-08-06T00:05:28.473936: step 13927, loss 0.528107.
Train: 2018-08-06T00:05:28.639493: step 13928, loss 0.605154.
Train: 2018-08-06T00:05:28.808071: step 13929, loss 0.613582.
Train: 2018-08-06T00:05:28.981579: step 13930, loss 0.494407.
Test: 2018-08-06T00:05:29.219958: step 13930, loss 0.548198.
Train: 2018-08-06T00:05:29.390513: step 13931, loss 0.51997.
Train: 2018-08-06T00:05:29.559059: step 13932, loss 0.528495.
Train: 2018-08-06T00:05:29.723616: step 13933, loss 0.528492.
Train: 2018-08-06T00:05:29.889151: step 13934, loss 0.528456.
Train: 2018-08-06T00:05:30.057701: step 13935, loss 0.545409.
Train: 2018-08-06T00:05:30.222291: step 13936, loss 0.553888.
Train: 2018-08-06T00:05:30.390854: step 13937, loss 0.570926.
Train: 2018-08-06T00:05:30.559359: step 13938, loss 0.519688.
Train: 2018-08-06T00:05:30.738879: step 13939, loss 0.570963.
Train: 2018-08-06T00:05:30.902442: step 13940, loss 0.510941.
Test: 2018-08-06T00:05:31.140804: step 13940, loss 0.547996.
Train: 2018-08-06T00:05:31.310381: step 13941, loss 0.622596.
Train: 2018-08-06T00:05:31.483912: step 13942, loss 0.596834.
Train: 2018-08-06T00:05:31.645454: step 13943, loss 0.52795.
Train: 2018-08-06T00:05:31.813031: step 13944, loss 0.596874.
Train: 2018-08-06T00:05:31.986542: step 13945, loss 0.545155.
Train: 2018-08-06T00:05:32.150135: step 13946, loss 0.5279.
Train: 2018-08-06T00:05:32.326632: step 13947, loss 0.562402.
Train: 2018-08-06T00:05:32.493188: step 13948, loss 0.571045.
Train: 2018-08-06T00:05:32.666724: step 13949, loss 0.571053.
Train: 2018-08-06T00:05:32.830310: step 13950, loss 0.640263.
Test: 2018-08-06T00:05:33.067651: step 13950, loss 0.547931.
Train: 2018-08-06T00:05:33.237199: step 13951, loss 0.53648.
Train: 2018-08-06T00:05:33.405777: step 13952, loss 0.579671.
Train: 2018-08-06T00:05:33.570333: step 13953, loss 0.467505.
Train: 2018-08-06T00:05:33.737889: step 13954, loss 0.562401.
Train: 2018-08-06T00:05:33.900454: step 13955, loss 0.536473.
Train: 2018-08-06T00:05:34.068006: step 13956, loss 0.545099.
Train: 2018-08-06T00:05:34.231539: step 13957, loss 0.640411.
Train: 2018-08-06T00:05:34.401115: step 13958, loss 0.623053.
Train: 2018-08-06T00:05:34.579607: step 13959, loss 0.605661.
Train: 2018-08-06T00:05:34.745190: step 13960, loss 0.588296.
Test: 2018-08-06T00:05:34.982546: step 13960, loss 0.547978.
Train: 2018-08-06T00:05:35.152102: step 13961, loss 0.510733.
Train: 2018-08-06T00:05:35.317635: step 13962, loss 0.545198.
Train: 2018-08-06T00:05:35.494188: step 13963, loss 0.665503.
Train: 2018-08-06T00:05:35.660742: step 13964, loss 0.528123.
Train: 2018-08-06T00:05:35.827271: step 13965, loss 0.682136.
Train: 2018-08-06T00:05:35.990834: step 13966, loss 0.536854.
Train: 2018-08-06T00:05:36.157420: step 13967, loss 0.553921.
Train: 2018-08-06T00:05:36.324965: step 13968, loss 0.596291.
Train: 2018-08-06T00:05:36.491522: step 13969, loss 0.545533.
Train: 2018-08-06T00:05:36.655057: step 13970, loss 0.52872.
Test: 2018-08-06T00:05:36.893420: step 13970, loss 0.54834.
Train: 2018-08-06T00:05:37.063997: step 13971, loss 0.545606.
Train: 2018-08-06T00:05:37.231515: step 13972, loss 0.587671.
Train: 2018-08-06T00:05:37.398070: step 13973, loss 0.646458.
Train: 2018-08-06T00:05:37.570609: step 13974, loss 0.587595.
Train: 2018-08-06T00:05:37.736166: step 13975, loss 0.520681.
Train: 2018-08-06T00:05:37.906710: step 13976, loss 0.65428.
Train: 2018-08-06T00:05:38.077254: step 13977, loss 0.570822.
Train: 2018-08-06T00:05:38.248795: step 13978, loss 0.612309.
Train: 2018-08-06T00:05:38.417344: step 13979, loss 0.587337.
Train: 2018-08-06T00:05:38.583930: step 13980, loss 0.5461.
Test: 2018-08-06T00:05:38.821264: step 13980, loss 0.54884.
Train: 2018-08-06T00:05:38.990842: step 13981, loss 0.570828.
Train: 2018-08-06T00:05:39.157396: step 13982, loss 0.628196.
Train: 2018-08-06T00:05:39.320960: step 13983, loss 0.578976.
Train: 2018-08-06T00:05:39.485518: step 13984, loss 0.546379.
Train: 2018-08-06T00:05:39.655065: step 13985, loss 0.603327.
Train: 2018-08-06T00:05:39.816637: step 13986, loss 0.538435.
Train: 2018-08-06T00:05:39.979167: step 13987, loss 0.522302.
Train: 2018-08-06T00:05:40.155696: step 13988, loss 0.554676.
Train: 2018-08-06T00:05:40.326272: step 13989, loss 0.578937.
Train: 2018-08-06T00:05:40.494788: step 13990, loss 0.570841.
Test: 2018-08-06T00:05:40.733160: step 13990, loss 0.549213.
Train: 2018-08-06T00:05:40.915672: step 13991, loss 0.449469.
Train: 2018-08-06T00:05:41.082245: step 13992, loss 0.554615.
Train: 2018-08-06T00:05:41.256775: step 13993, loss 0.570841.
Train: 2018-08-06T00:05:41.424327: step 13994, loss 0.578968.
Train: 2018-08-06T00:05:41.592852: step 13995, loss 0.52173.
Train: 2018-08-06T00:05:41.758408: step 13996, loss 0.611888.
Train: 2018-08-06T00:05:41.921971: step 13997, loss 0.546109.
Train: 2018-08-06T00:05:42.089526: step 13998, loss 0.488347.
Train: 2018-08-06T00:05:42.256108: step 13999, loss 0.562524.
Train: 2018-08-06T00:05:42.426653: step 14000, loss 0.562511.
Test: 2018-08-06T00:05:42.663986: step 14000, loss 0.548519.
Train: 2018-08-06T00:05:43.451744: step 14001, loss 0.579152.
Train: 2018-08-06T00:05:43.627274: step 14002, loss 0.487205.
Train: 2018-08-06T00:05:43.795815: step 14003, loss 0.503657.
Train: 2018-08-06T00:05:43.961378: step 14004, loss 0.528661.
Train: 2018-08-06T00:05:44.136878: step 14005, loss 0.553925.
Train: 2018-08-06T00:05:44.303458: step 14006, loss 0.587977.
Train: 2018-08-06T00:05:44.470013: step 14007, loss 0.553843.
Train: 2018-08-06T00:05:44.636542: step 14008, loss 0.579595.
Train: 2018-08-06T00:05:44.809107: step 14009, loss 0.579639.
Train: 2018-08-06T00:05:44.976633: step 14010, loss 0.571045.
Test: 2018-08-06T00:05:45.215993: step 14010, loss 0.547906.
Train: 2018-08-06T00:05:45.386561: step 14011, loss 0.545089.
Train: 2018-08-06T00:05:45.555086: step 14012, loss 0.48433.
Train: 2018-08-06T00:05:45.721671: step 14013, loss 0.553715.
Train: 2018-08-06T00:05:45.888219: step 14014, loss 0.510052.
Train: 2018-08-06T00:05:46.052761: step 14015, loss 0.562454.
Train: 2018-08-06T00:05:46.232274: step 14016, loss 0.571258.
Train: 2018-08-06T00:05:46.397832: step 14017, loss 0.500793.
Train: 2018-08-06T00:05:46.565417: step 14018, loss 0.53598.
Train: 2018-08-06T00:05:46.728945: step 14019, loss 0.589131.
Train: 2018-08-06T00:05:46.898499: step 14020, loss 0.535862.
Test: 2018-08-06T00:05:47.124887: step 14020, loss 0.547638.
Train: 2018-08-06T00:05:47.291466: step 14021, loss 0.544745.
Train: 2018-08-06T00:05:47.461019: step 14022, loss 0.553645.
Train: 2018-08-06T00:05:47.629595: step 14023, loss 0.598431.
Train: 2018-08-06T00:05:47.798094: step 14024, loss 0.535747.
Train: 2018-08-06T00:05:47.964642: step 14025, loss 0.634479.
Train: 2018-08-06T00:05:48.153138: step 14026, loss 0.58961.
Train: 2018-08-06T00:05:48.319717: step 14027, loss 0.598477.
Train: 2018-08-06T00:05:48.488271: step 14028, loss 0.580494.
Train: 2018-08-06T00:05:48.655793: step 14029, loss 0.589364.
Train: 2018-08-06T00:05:48.825343: step 14030, loss 0.500226.
Test: 2018-08-06T00:05:49.061707: step 14030, loss 0.547654.
Train: 2018-08-06T00:05:49.229259: step 14031, loss 0.509186.
Train: 2018-08-06T00:05:49.399829: step 14032, loss 0.526988.
Train: 2018-08-06T00:05:49.568380: step 14033, loss 0.553655.
Train: 2018-08-06T00:05:49.742917: step 14034, loss 0.607016.
Train: 2018-08-06T00:05:49.911467: step 14035, loss 0.62475.
Train: 2018-08-06T00:05:50.076028: step 14036, loss 0.509328.
Train: 2018-08-06T00:05:50.244544: step 14037, loss 0.562519.
Train: 2018-08-06T00:05:50.410126: step 14038, loss 0.535962.
Train: 2018-08-06T00:05:50.584666: step 14039, loss 0.553659.
Train: 2018-08-06T00:05:50.750205: step 14040, loss 0.491743.
Test: 2018-08-06T00:05:50.986560: step 14040, loss 0.547687.
Train: 2018-08-06T00:05:51.160127: step 14041, loss 0.509399.
Train: 2018-08-06T00:05:51.354600: step 14042, loss 0.535935.
Train: 2018-08-06T00:05:51.510191: step 14043, loss 0.600439.
Train: 2018-08-06T00:05:51.683731: step 14044, loss 0.553637.
Train: 2018-08-06T00:05:51.851247: step 14045, loss 0.562565.
Train: 2018-08-06T00:05:52.019828: step 14046, loss 0.589288.
Train: 2018-08-06T00:05:52.188370: step 14047, loss 0.482579.
Train: 2018-08-06T00:05:52.354908: step 14048, loss 0.526875.
Train: 2018-08-06T00:05:52.520484: step 14049, loss 0.553704.
Train: 2018-08-06T00:05:52.686014: step 14050, loss 0.553679.
Test: 2018-08-06T00:05:52.923381: step 14050, loss 0.547626.
Train: 2018-08-06T00:05:53.092927: step 14051, loss 0.553669.
Train: 2018-08-06T00:05:53.259480: step 14052, loss 0.562644.
Train: 2018-08-06T00:05:53.429028: step 14053, loss 0.535733.
Train: 2018-08-06T00:05:53.598600: step 14054, loss 0.580561.
Train: 2018-08-06T00:05:53.778094: step 14055, loss 0.535724.
Train: 2018-08-06T00:05:53.956618: step 14056, loss 0.517783.
Train: 2018-08-06T00:05:54.118184: step 14057, loss 0.625495.
Train: 2018-08-06T00:05:54.284740: step 14058, loss 0.607527.
Train: 2018-08-06T00:05:54.457278: step 14059, loss 0.535718.
Train: 2018-08-06T00:05:54.620845: step 14060, loss 0.499936.
Test: 2018-08-06T00:05:54.856220: step 14060, loss 0.547613.
Train: 2018-08-06T00:05:55.040749: step 14061, loss 0.598472.
Train: 2018-08-06T00:05:55.209269: step 14062, loss 0.634219.
Train: 2018-08-06T00:05:55.371864: step 14063, loss 0.553654.
Train: 2018-08-06T00:05:55.540412: step 14064, loss 0.50016.
Train: 2018-08-06T00:05:55.708930: step 14065, loss 0.553666.
Train: 2018-08-06T00:05:55.878507: step 14066, loss 0.571469.
Train: 2018-08-06T00:05:56.050050: step 14067, loss 0.598129.
Train: 2018-08-06T00:05:56.216603: step 14068, loss 0.464849.
Train: 2018-08-06T00:05:56.381132: step 14069, loss 0.633599.
Train: 2018-08-06T00:05:56.551676: step 14070, loss 0.633483.
Test: 2018-08-06T00:05:56.790040: step 14070, loss 0.547691.
Train: 2018-08-06T00:05:56.962578: step 14071, loss 0.650963.
Train: 2018-08-06T00:05:57.129139: step 14072, loss 0.51843.
Train: 2018-08-06T00:05:57.294689: step 14073, loss 0.553674.
Train: 2018-08-06T00:05:57.459250: step 14074, loss 0.579942.
Train: 2018-08-06T00:05:57.626802: step 14075, loss 0.562416.
Train: 2018-08-06T00:05:57.789398: step 14076, loss 0.579855.
Train: 2018-08-06T00:05:57.958914: step 14077, loss 0.579737.
Train: 2018-08-06T00:05:58.132450: step 14078, loss 0.484533.
Train: 2018-08-06T00:05:58.294050: step 14079, loss 0.605645.
Train: 2018-08-06T00:05:58.467585: step 14080, loss 0.50203.
Test: 2018-08-06T00:05:58.704920: step 14080, loss 0.547972.
Train: 2018-08-06T00:05:58.874497: step 14081, loss 0.562371.
Train: 2018-08-06T00:05:59.043045: step 14082, loss 0.579607.
Train: 2018-08-06T00:05:59.211594: step 14083, loss 0.605233.
Train: 2018-08-06T00:05:59.381140: step 14084, loss 0.502193.
Train: 2018-08-06T00:05:59.546691: step 14085, loss 0.57096.
Train: 2018-08-06T00:05:59.716214: step 14086, loss 0.579998.
Train: 2018-08-06T00:05:59.880799: step 14087, loss 0.519946.
Train: 2018-08-06T00:06:00.049347: step 14088, loss 0.553823.
Train: 2018-08-06T00:06:00.234826: step 14089, loss 0.511025.
Train: 2018-08-06T00:06:00.409359: step 14090, loss 0.545437.
Test: 2018-08-06T00:06:00.647722: step 14090, loss 0.547965.
Train: 2018-08-06T00:06:00.810291: step 14091, loss 0.579614.
Train: 2018-08-06T00:06:00.991806: step 14092, loss 0.519221.
Train: 2018-08-06T00:06:01.156390: step 14093, loss 0.631679.
Train: 2018-08-06T00:06:01.325557: step 14094, loss 0.657691.
Train: 2018-08-06T00:06:01.507070: step 14095, loss 0.553752.
Train: 2018-08-06T00:06:01.675620: step 14096, loss 0.545121.
Train: 2018-08-06T00:06:01.842803: step 14097, loss 0.502059.
Train: 2018-08-06T00:06:02.013820: step 14098, loss 0.553775.
Train: 2018-08-06T00:06:02.195359: step 14099, loss 0.588273.
Train: 2018-08-06T00:06:02.367906: step 14100, loss 0.545146.
Test: 2018-08-06T00:06:02.605238: step 14100, loss 0.547952.
Train: 2018-08-06T00:06:03.326038: step 14101, loss 0.553788.
Train: 2018-08-06T00:06:03.497580: step 14102, loss 0.571042.
Train: 2018-08-06T00:06:03.693027: step 14103, loss 0.605561.
Train: 2018-08-06T00:06:03.867608: step 14104, loss 0.54517.
Train: 2018-08-06T00:06:04.040128: step 14105, loss 0.562404.
Train: 2018-08-06T00:06:04.210667: step 14106, loss 0.562402.
Train: 2018-08-06T00:06:04.396158: step 14107, loss 0.562418.
Train: 2018-08-06T00:06:04.593617: step 14108, loss 0.510734.
Train: 2018-08-06T00:06:04.771143: step 14109, loss 0.510708.
Train: 2018-08-06T00:06:04.938694: step 14110, loss 0.588291.
Test: 2018-08-06T00:06:05.180050: step 14110, loss 0.547937.
Train: 2018-08-06T00:06:05.385501: step 14111, loss 0.596953.
Train: 2018-08-06T00:06:05.554072: step 14112, loss 0.622867.
Train: 2018-08-06T00:06:05.721629: step 14113, loss 0.527892.
Train: 2018-08-06T00:06:05.894140: step 14114, loss 0.579649.
Train: 2018-08-06T00:06:06.064683: step 14115, loss 0.502064.
Train: 2018-08-06T00:06:06.226252: step 14116, loss 0.596913.
Train: 2018-08-06T00:06:06.410758: step 14117, loss 0.519291.
Train: 2018-08-06T00:06:06.585312: step 14118, loss 0.631403.
Train: 2018-08-06T00:06:06.764811: step 14119, loss 0.614114.
Train: 2018-08-06T00:06:06.938348: step 14120, loss 0.562429.
Test: 2018-08-06T00:06:07.177708: step 14120, loss 0.548015.
Train: 2018-08-06T00:06:07.374181: step 14121, loss 0.562414.
Train: 2018-08-06T00:06:07.537777: step 14122, loss 0.579553.
Train: 2018-08-06T00:06:07.717265: step 14123, loss 0.605233.
Train: 2018-08-06T00:06:07.899775: step 14124, loss 0.579435.
Train: 2018-08-06T00:06:08.070330: step 14125, loss 0.545345.
Train: 2018-08-06T00:06:08.241861: step 14126, loss 0.468832.
Train: 2018-08-06T00:06:08.410439: step 14127, loss 0.528438.
Train: 2018-08-06T00:06:08.577963: step 14128, loss 0.53684.
Train: 2018-08-06T00:06:08.751498: step 14129, loss 0.596495.
Train: 2018-08-06T00:06:08.926031: step 14130, loss 0.570969.
Test: 2018-08-06T00:06:09.162399: step 14130, loss 0.54811.
Train: 2018-08-06T00:06:09.333098: step 14131, loss 0.553809.
Train: 2018-08-06T00:06:09.502656: step 14132, loss 0.639265.
Train: 2018-08-06T00:06:09.675164: step 14133, loss 0.545246.
Train: 2018-08-06T00:06:09.845734: step 14134, loss 0.536762.
Train: 2018-08-06T00:06:10.024264: step 14135, loss 0.553927.
Train: 2018-08-06T00:06:10.190786: step 14136, loss 0.545285.
Train: 2018-08-06T00:06:10.360356: step 14137, loss 0.53692.
Train: 2018-08-06T00:06:10.535862: step 14138, loss 0.528324.
Train: 2018-08-06T00:06:10.702424: step 14139, loss 0.570886.
Train: 2018-08-06T00:06:10.870992: step 14140, loss 0.553776.
Test: 2018-08-06T00:06:11.110326: step 14140, loss 0.548034.
Train: 2018-08-06T00:06:11.282890: step 14141, loss 0.588142.
Train: 2018-08-06T00:06:11.506672: step 14142, loss 0.579553.
Train: 2018-08-06T00:06:11.675251: step 14143, loss 0.528061.
Train: 2018-08-06T00:06:11.842748: step 14144, loss 0.570971.
Train: 2018-08-06T00:06:12.012337: step 14145, loss 0.484985.
Train: 2018-08-06T00:06:12.189820: step 14146, loss 0.510696.
Train: 2018-08-06T00:06:12.360398: step 14147, loss 0.527912.
Train: 2018-08-06T00:06:12.541905: step 14148, loss 0.597065.
Train: 2018-08-06T00:06:12.712423: step 14149, loss 0.562427.
Train: 2018-08-06T00:06:12.883997: step 14150, loss 0.527513.
Test: 2018-08-06T00:06:13.114348: step 14150, loss 0.547812.
Train: 2018-08-06T00:06:13.286887: step 14151, loss 0.536244.
Train: 2018-08-06T00:06:13.461420: step 14152, loss 0.501257.
Train: 2018-08-06T00:06:13.635953: step 14153, loss 0.50981.
Train: 2018-08-06T00:06:13.807494: step 14154, loss 0.553647.
Train: 2018-08-06T00:06:13.978039: step 14155, loss 0.597943.
Train: 2018-08-06T00:06:14.147619: step 14156, loss 0.544812.
Train: 2018-08-06T00:06:14.319151: step 14157, loss 0.544709.
Train: 2018-08-06T00:06:14.492695: step 14158, loss 0.526915.
Train: 2018-08-06T00:06:14.667196: step 14159, loss 0.598351.
Train: 2018-08-06T00:06:14.842726: step 14160, loss 0.553653.
Test: 2018-08-06T00:06:15.083116: step 14160, loss 0.547614.
Train: 2018-08-06T00:06:15.253687: step 14161, loss 0.625331.
Train: 2018-08-06T00:06:15.425227: step 14162, loss 0.607374.
Train: 2018-08-06T00:06:15.596773: step 14163, loss 0.652081.
Train: 2018-08-06T00:06:15.768283: step 14164, loss 0.562564.
Train: 2018-08-06T00:06:15.937830: step 14165, loss 0.660407.
Train: 2018-08-06T00:06:16.105411: step 14166, loss 0.509417.
Train: 2018-08-06T00:06:16.267980: step 14167, loss 0.571291.
Train: 2018-08-06T00:06:16.437518: step 14168, loss 0.544857.
Train: 2018-08-06T00:06:16.612052: step 14169, loss 0.56244.
Train: 2018-08-06T00:06:16.781604: step 14170, loss 0.55371.
Test: 2018-08-06T00:06:17.018938: step 14170, loss 0.547819.
Train: 2018-08-06T00:06:17.194500: step 14171, loss 0.457729.
Train: 2018-08-06T00:06:17.361023: step 14172, loss 0.518814.
Train: 2018-08-06T00:06:17.530571: step 14173, loss 0.606112.
Train: 2018-08-06T00:06:17.706101: step 14174, loss 0.562436.
Train: 2018-08-06T00:06:17.872682: step 14175, loss 0.562409.
Train: 2018-08-06T00:06:18.043199: step 14176, loss 0.641019.
Train: 2018-08-06T00:06:18.208756: step 14177, loss 0.5537.
Train: 2018-08-06T00:06:18.377306: step 14178, loss 0.54504.
Train: 2018-08-06T00:06:18.547876: step 14179, loss 0.588572.
Train: 2018-08-06T00:06:18.717429: step 14180, loss 0.571045.
Test: 2018-08-06T00:06:18.952767: step 14180, loss 0.547905.
Train: 2018-08-06T00:06:19.137273: step 14181, loss 0.640343.
Train: 2018-08-06T00:06:19.308846: step 14182, loss 0.510656.
Train: 2018-08-06T00:06:19.502328: step 14183, loss 0.605493.
Train: 2018-08-06T00:06:19.684810: step 14184, loss 0.527959.
Train: 2018-08-06T00:06:19.857374: step 14185, loss 0.545296.
Train: 2018-08-06T00:06:20.035871: step 14186, loss 0.571035.
Train: 2018-08-06T00:06:20.197236: step 14187, loss 0.536641.
Train: 2018-08-06T00:06:20.369796: step 14188, loss 0.553907.
Train: 2018-08-06T00:06:20.539352: step 14189, loss 0.570994.
Train: 2018-08-06T00:06:20.714853: step 14190, loss 0.690747.
Test: 2018-08-06T00:06:20.953216: step 14190, loss 0.548126.
Train: 2018-08-06T00:06:21.124756: step 14191, loss 0.519807.
Train: 2018-08-06T00:06:21.297300: step 14192, loss 0.579463.
Train: 2018-08-06T00:06:21.471856: step 14193, loss 0.553978.
Train: 2018-08-06T00:06:21.633397: step 14194, loss 0.562383.
Train: 2018-08-06T00:06:21.804938: step 14195, loss 0.570869.
Train: 2018-08-06T00:06:21.973511: step 14196, loss 0.621649.
Train: 2018-08-06T00:06:22.141040: step 14197, loss 0.638496.
Train: 2018-08-06T00:06:22.310585: step 14198, loss 0.528751.
Train: 2018-08-06T00:06:22.481128: step 14199, loss 0.562434.
Train: 2018-08-06T00:06:22.651699: step 14200, loss 0.621047.
Test: 2018-08-06T00:06:22.888040: step 14200, loss 0.548512.
Train: 2018-08-06T00:06:23.636717: step 14201, loss 0.512428.
Train: 2018-08-06T00:06:23.817199: step 14202, loss 0.529277.
Train: 2018-08-06T00:06:23.987745: step 14203, loss 0.512562.
Train: 2018-08-06T00:06:24.175242: step 14204, loss 0.654167.
Train: 2018-08-06T00:06:24.357754: step 14205, loss 0.612581.
Train: 2018-08-06T00:06:24.530293: step 14206, loss 0.562549.
Train: 2018-08-06T00:06:24.699870: step 14207, loss 0.471149.
Train: 2018-08-06T00:06:24.871380: step 14208, loss 0.59571.
Train: 2018-08-06T00:06:25.048905: step 14209, loss 0.612351.
Train: 2018-08-06T00:06:25.222468: step 14210, loss 0.56248.
Test: 2018-08-06T00:06:25.461802: step 14210, loss 0.548611.
Train: 2018-08-06T00:06:25.636335: step 14211, loss 0.570766.
Train: 2018-08-06T00:06:25.804910: step 14212, loss 0.570857.
Train: 2018-08-06T00:06:25.972460: step 14213, loss 0.5791.
Train: 2018-08-06T00:06:26.142007: step 14214, loss 0.587434.
Train: 2018-08-06T00:06:26.308536: step 14215, loss 0.504622.
Train: 2018-08-06T00:06:26.483100: step 14216, loss 0.546022.
Train: 2018-08-06T00:06:26.656607: step 14217, loss 0.529424.
Train: 2018-08-06T00:06:26.825154: step 14218, loss 0.504346.
Train: 2018-08-06T00:06:26.987721: step 14219, loss 0.52919.
Train: 2018-08-06T00:06:27.153310: step 14220, loss 0.604297.
Test: 2018-08-06T00:06:27.388666: step 14220, loss 0.548431.
Train: 2018-08-06T00:06:27.596306: step 14221, loss 0.528958.
Train: 2018-08-06T00:06:27.763887: step 14222, loss 0.545638.
Train: 2018-08-06T00:06:27.936422: step 14223, loss 0.604587.
Train: 2018-08-06T00:06:28.114918: step 14224, loss 0.63841.
Train: 2018-08-06T00:06:28.284464: step 14225, loss 0.62156.
Train: 2018-08-06T00:06:28.453044: step 14226, loss 0.528713.
Train: 2018-08-06T00:06:28.628543: step 14227, loss 0.52862.
Train: 2018-08-06T00:06:28.797126: step 14228, loss 0.562384.
Train: 2018-08-06T00:06:28.966639: step 14229, loss 0.647003.
Train: 2018-08-06T00:06:29.134224: step 14230, loss 0.579363.
Test: 2018-08-06T00:06:29.374549: step 14230, loss 0.548302.
Train: 2018-08-06T00:06:29.544128: step 14231, loss 0.55395.
Train: 2018-08-06T00:06:29.715636: step 14232, loss 0.554001.
Train: 2018-08-06T00:06:29.884185: step 14233, loss 0.537209.
Train: 2018-08-06T00:06:30.049775: step 14234, loss 0.579315.
Train: 2018-08-06T00:06:30.219290: step 14235, loss 0.629812.
Train: 2018-08-06T00:06:30.384847: step 14236, loss 0.537191.
Train: 2018-08-06T00:06:30.560410: step 14237, loss 0.604454.
Train: 2018-08-06T00:06:30.723939: step 14238, loss 0.63799.
Train: 2018-08-06T00:06:30.899471: step 14239, loss 0.579188.
Train: 2018-08-06T00:06:31.066024: step 14240, loss 0.545766.
Test: 2018-08-06T00:06:31.291425: step 14240, loss 0.548544.
Train: 2018-08-06T00:06:31.462968: step 14241, loss 0.637405.
Train: 2018-08-06T00:06:31.635502: step 14242, loss 0.604055.
Train: 2018-08-06T00:06:31.800062: step 14243, loss 0.579122.
Train: 2018-08-06T00:06:31.985594: step 14244, loss 0.554288.
Train: 2018-08-06T00:06:32.150160: step 14245, loss 0.587245.
Train: 2018-08-06T00:06:32.330643: step 14246, loss 0.546251.
Train: 2018-08-06T00:06:32.500221: step 14247, loss 0.521784.
Train: 2018-08-06T00:06:32.669767: step 14248, loss 0.562753.
Train: 2018-08-06T00:06:32.839283: step 14249, loss 0.554523.
Train: 2018-08-06T00:06:33.010855: step 14250, loss 0.5627.
Test: 2018-08-06T00:06:33.248189: step 14250, loss 0.548958.
Train: 2018-08-06T00:06:33.417736: step 14251, loss 0.668945.
Train: 2018-08-06T00:06:33.588310: step 14252, loss 0.595352.
Train: 2018-08-06T00:06:33.758849: step 14253, loss 0.578939.
Train: 2018-08-06T00:06:33.924380: step 14254, loss 0.587129.
Train: 2018-08-06T00:06:34.087943: step 14255, loss 0.611498.
Train: 2018-08-06T00:06:34.257490: step 14256, loss 0.546606.
Train: 2018-08-06T00:06:34.421052: step 14257, loss 0.611139.
Train: 2018-08-06T00:06:34.597612: step 14258, loss 0.58702.
Train: 2018-08-06T00:06:34.763168: step 14259, loss 0.530747.
Train: 2018-08-06T00:06:34.938669: step 14260, loss 0.490638.
Test: 2018-08-06T00:06:35.176033: step 14260, loss 0.549409.
Train: 2018-08-06T00:06:35.357548: step 14261, loss 0.570921.
Train: 2018-08-06T00:06:35.526128: step 14262, loss 0.578963.
Train: 2018-08-06T00:06:35.695644: step 14263, loss 0.466241.
Train: 2018-08-06T00:06:35.861200: step 14264, loss 0.530473.
Train: 2018-08-06T00:06:36.042740: step 14265, loss 0.530319.
Train: 2018-08-06T00:06:36.210267: step 14266, loss 0.513861.
Train: 2018-08-06T00:06:36.375824: step 14267, loss 0.587183.
Train: 2018-08-06T00:06:36.546368: step 14268, loss 0.488603.
Train: 2018-08-06T00:06:36.715915: step 14269, loss 0.488113.
Train: 2018-08-06T00:06:36.882479: step 14270, loss 0.537518.
Test: 2018-08-06T00:06:37.118858: step 14270, loss 0.548419.
Train: 2018-08-06T00:06:37.288392: step 14271, loss 0.512168.
Train: 2018-08-06T00:06:37.456971: step 14272, loss 0.553981.
Train: 2018-08-06T00:06:37.638485: step 14273, loss 0.545419.
Train: 2018-08-06T00:06:37.814983: step 14274, loss 0.562403.
Train: 2018-08-06T00:06:37.982535: step 14275, loss 0.614005.
Train: 2018-08-06T00:06:38.155104: step 14276, loss 0.553771.
Train: 2018-08-06T00:06:38.321658: step 14277, loss 0.623086.
Train: 2018-08-06T00:06:38.491188: step 14278, loss 0.54504.
Train: 2018-08-06T00:06:38.656733: step 14279, loss 0.51889.
Train: 2018-08-06T00:06:38.824314: step 14280, loss 0.448941.
Test: 2018-08-06T00:06:39.062646: step 14280, loss 0.547764.
Train: 2018-08-06T00:06:39.232222: step 14281, loss 0.562441.
Train: 2018-08-06T00:06:39.401765: step 14282, loss 0.562451.
Train: 2018-08-06T00:06:39.569291: step 14283, loss 0.518266.
Train: 2018-08-06T00:06:39.738838: step 14284, loss 0.59812.
Train: 2018-08-06T00:06:39.906391: step 14285, loss 0.571506.
Train: 2018-08-06T00:06:40.071948: step 14286, loss 0.571534.
Train: 2018-08-06T00:06:40.240528: step 14287, loss 0.526837.
Train: 2018-08-06T00:06:40.411065: step 14288, loss 0.571471.
Train: 2018-08-06T00:06:40.581584: step 14289, loss 0.535752.
Train: 2018-08-06T00:06:40.752128: step 14290, loss 0.589547.
Test: 2018-08-06T00:06:40.988506: step 14290, loss 0.547603.
Train: 2018-08-06T00:06:41.164054: step 14291, loss 0.580501.
Train: 2018-08-06T00:06:41.331609: step 14292, loss 0.526748.
Train: 2018-08-06T00:06:41.500129: step 14293, loss 0.643438.
Train: 2018-08-06T00:06:41.668708: step 14294, loss 0.589534.
Train: 2018-08-06T00:06:41.835232: step 14295, loss 0.58054.
Train: 2018-08-06T00:06:42.005806: step 14296, loss 0.571484.
Train: 2018-08-06T00:06:42.176343: step 14297, loss 0.544745.
Train: 2018-08-06T00:06:42.346862: step 14298, loss 0.58909.
Train: 2018-08-06T00:06:42.517407: step 14299, loss 0.544881.
Train: 2018-08-06T00:06:42.684986: step 14300, loss 0.553569.
Test: 2018-08-06T00:06:42.920329: step 14300, loss 0.547708.
Train: 2018-08-06T00:06:43.676983: step 14301, loss 0.447755.
Train: 2018-08-06T00:06:43.847527: step 14302, loss 0.536061.
Train: 2018-08-06T00:06:44.020091: step 14303, loss 0.589018.
Train: 2018-08-06T00:06:44.201580: step 14304, loss 0.553731.
Train: 2018-08-06T00:06:44.375117: step 14305, loss 0.535711.
Train: 2018-08-06T00:06:44.548676: step 14306, loss 0.518162.
Train: 2018-08-06T00:06:44.721190: step 14307, loss 0.472707.
Train: 2018-08-06T00:06:44.890737: step 14308, loss 0.544642.
Train: 2018-08-06T00:06:45.060283: step 14309, loss 0.590446.
Train: 2018-08-06T00:06:45.233819: step 14310, loss 0.544424.
Test: 2018-08-06T00:06:45.474189: step 14310, loss 0.547573.
Train: 2018-08-06T00:06:45.644720: step 14311, loss 0.590624.
Train: 2018-08-06T00:06:45.858176: step 14312, loss 0.545376.
Train: 2018-08-06T00:06:46.027697: step 14313, loss 0.526527.
Train: 2018-08-06T00:06:46.205222: step 14314, loss 0.500112.
Train: 2018-08-06T00:06:46.382772: step 14315, loss 0.516609.
Train: 2018-08-06T00:06:46.557311: step 14316, loss 0.554476.
Train: 2018-08-06T00:06:46.727824: step 14317, loss 0.563862.
Train: 2018-08-06T00:06:46.895376: step 14318, loss 0.618001.
Train: 2018-08-06T00:06:47.061963: step 14319, loss 0.619471.
Train: 2018-08-06T00:06:47.233471: step 14320, loss 0.472025.
Test: 2018-08-06T00:06:47.471834: step 14320, loss 0.547583.
Train: 2018-08-06T00:06:47.644373: step 14321, loss 0.562352.
Train: 2018-08-06T00:06:47.812922: step 14322, loss 0.625739.
Train: 2018-08-06T00:06:47.979476: step 14323, loss 0.535702.
Train: 2018-08-06T00:06:48.146031: step 14324, loss 0.535702.
Train: 2018-08-06T00:06:48.318570: step 14325, loss 0.571577.
Train: 2018-08-06T00:06:48.489114: step 14326, loss 0.660765.
Train: 2018-08-06T00:06:48.662674: step 14327, loss 0.669332.
Train: 2018-08-06T00:06:48.831223: step 14328, loss 0.535972.
Train: 2018-08-06T00:06:49.000744: step 14329, loss 0.500776.
Train: 2018-08-06T00:06:49.172286: step 14330, loss 0.536125.
Test: 2018-08-06T00:06:49.412644: step 14330, loss 0.54776.
Train: 2018-08-06T00:06:49.591166: step 14331, loss 0.562511.
Train: 2018-08-06T00:06:49.764733: step 14332, loss 0.544991.
Train: 2018-08-06T00:06:49.938271: step 14333, loss 0.527396.
Train: 2018-08-06T00:06:50.111774: step 14334, loss 0.553708.
Train: 2018-08-06T00:06:50.275387: step 14335, loss 0.571219.
Train: 2018-08-06T00:06:50.462834: step 14336, loss 0.606144.
Train: 2018-08-06T00:06:50.634375: step 14337, loss 0.509959.
Train: 2018-08-06T00:06:50.805918: step 14338, loss 0.588707.
Train: 2018-08-06T00:06:50.978490: step 14339, loss 0.518779.
Train: 2018-08-06T00:06:51.149998: step 14340, loss 0.562503.
Test: 2018-08-06T00:06:51.390383: step 14340, loss 0.547816.
Train: 2018-08-06T00:06:51.564888: step 14341, loss 0.536241.
Train: 2018-08-06T00:06:51.734434: step 14342, loss 0.562451.
Train: 2018-08-06T00:06:51.903980: step 14343, loss 0.536334.
Train: 2018-08-06T00:06:52.072530: step 14344, loss 0.597359.
Train: 2018-08-06T00:06:52.233101: step 14345, loss 0.58079.
Train: 2018-08-06T00:06:52.405640: step 14346, loss 0.518757.
Train: 2018-08-06T00:06:52.573194: step 14347, loss 0.570992.
Train: 2018-08-06T00:06:52.740742: step 14348, loss 0.509835.
Train: 2018-08-06T00:06:52.913326: step 14349, loss 0.571173.
Train: 2018-08-06T00:06:53.091804: step 14350, loss 0.544898.
Test: 2018-08-06T00:06:53.331164: step 14350, loss 0.547767.
Train: 2018-08-06T00:06:53.504700: step 14351, loss 0.623972.
Train: 2018-08-06T00:06:53.678236: step 14352, loss 0.518961.
Train: 2018-08-06T00:06:53.847821: step 14353, loss 0.606442.
Train: 2018-08-06T00:06:54.022316: step 14354, loss 0.518514.
Train: 2018-08-06T00:06:54.195890: step 14355, loss 0.606039.
Train: 2018-08-06T00:06:54.380389: step 14356, loss 0.466535.
Train: 2018-08-06T00:06:54.550902: step 14357, loss 0.545038.
Train: 2018-08-06T00:06:54.734412: step 14358, loss 0.571387.
Train: 2018-08-06T00:06:54.901965: step 14359, loss 0.509115.
Train: 2018-08-06T00:06:55.082480: step 14360, loss 0.57151.
Test: 2018-08-06T00:06:55.319870: step 14360, loss 0.547715.
Train: 2018-08-06T00:06:55.494380: step 14361, loss 0.589094.
Train: 2018-08-06T00:06:55.685867: step 14362, loss 0.500654.
Train: 2018-08-06T00:06:55.853418: step 14363, loss 0.52777.
Train: 2018-08-06T00:06:56.027959: step 14364, loss 0.597904.
Train: 2018-08-06T00:06:56.205478: step 14365, loss 0.518345.
Train: 2018-08-06T00:06:56.383002: step 14366, loss 0.615491.
Train: 2018-08-06T00:06:56.560528: step 14367, loss 0.606979.
Train: 2018-08-06T00:06:56.735095: step 14368, loss 0.527257.
Train: 2018-08-06T00:06:56.909625: step 14369, loss 0.562628.
Train: 2018-08-06T00:06:57.089141: step 14370, loss 0.607303.
Test: 2018-08-06T00:06:57.326505: step 14370, loss 0.547698.
Train: 2018-08-06T00:06:57.502009: step 14371, loss 0.580039.
Train: 2018-08-06T00:06:57.675576: step 14372, loss 0.526918.
Train: 2018-08-06T00:06:57.864060: step 14373, loss 0.52681.
Train: 2018-08-06T00:06:58.044584: step 14374, loss 0.527504.
Train: 2018-08-06T00:06:58.216124: step 14375, loss 0.509225.
Train: 2018-08-06T00:06:58.386645: step 14376, loss 0.615895.
Train: 2018-08-06T00:06:58.558186: step 14377, loss 0.562481.
Train: 2018-08-06T00:06:58.745718: step 14378, loss 0.589106.
Train: 2018-08-06T00:06:58.918222: step 14379, loss 0.554122.
Train: 2018-08-06T00:06:59.088793: step 14380, loss 0.588866.
Test: 2018-08-06T00:06:59.326133: step 14380, loss 0.547752.
Train: 2018-08-06T00:06:59.498696: step 14381, loss 0.579694.
Train: 2018-08-06T00:06:59.676238: step 14382, loss 0.510204.
Train: 2018-08-06T00:06:59.849746: step 14383, loss 0.615302.
Train: 2018-08-06T00:07:00.033240: step 14384, loss 0.570918.
Train: 2018-08-06T00:07:00.207475: step 14385, loss 0.553851.
Train: 2018-08-06T00:07:00.380554: step 14386, loss 0.492854.
Train: 2018-08-06T00:07:00.554056: step 14387, loss 0.614796.
Train: 2018-08-06T00:07:00.734600: step 14388, loss 0.518821.
Train: 2018-08-06T00:07:00.912135: step 14389, loss 0.492882.
Train: 2018-08-06T00:07:01.081644: step 14390, loss 0.597127.
Test: 2018-08-06T00:07:01.320007: step 14390, loss 0.547847.
Train: 2018-08-06T00:07:01.492546: step 14391, loss 0.519114.
Train: 2018-08-06T00:07:01.664117: step 14392, loss 0.562415.
Train: 2018-08-06T00:07:01.834631: step 14393, loss 0.518575.
Train: 2018-08-06T00:07:02.015157: step 14394, loss 0.527373.
Train: 2018-08-06T00:07:02.188712: step 14395, loss 0.562288.
Train: 2018-08-06T00:07:02.361255: step 14396, loss 0.483594.
Train: 2018-08-06T00:07:02.530768: step 14397, loss 0.633101.
Train: 2018-08-06T00:07:02.702340: step 14398, loss 0.580187.
Train: 2018-08-06T00:07:02.872855: step 14399, loss 0.536323.
Train: 2018-08-06T00:07:03.046391: step 14400, loss 0.535721.
Test: 2018-08-06T00:07:03.284754: step 14400, loss 0.547701.
Train: 2018-08-06T00:07:04.057631: step 14401, loss 0.624221.
Train: 2018-08-06T00:07:04.231165: step 14402, loss 0.553875.
Train: 2018-08-06T00:07:04.413708: step 14403, loss 0.553258.
Train: 2018-08-06T00:07:04.583224: step 14404, loss 0.571086.
Train: 2018-08-06T00:07:04.787677: step 14405, loss 0.571355.
Train: 2018-08-06T00:07:05.016067: step 14406, loss 0.491675.
Train: 2018-08-06T00:07:05.186635: step 14407, loss 0.571279.
Train: 2018-08-06T00:07:05.357156: step 14408, loss 0.527271.
Train: 2018-08-06T00:07:05.531718: step 14409, loss 0.536228.
Train: 2018-08-06T00:07:05.704257: step 14410, loss 0.55359.
Test: 2018-08-06T00:07:05.942588: step 14410, loss 0.547675.
Train: 2018-08-06T00:07:06.117157: step 14411, loss 0.535592.
Train: 2018-08-06T00:07:06.289661: step 14412, loss 0.535776.
Train: 2018-08-06T00:07:06.463197: step 14413, loss 0.589766.
Train: 2018-08-06T00:07:06.640723: step 14414, loss 0.500041.
Train: 2018-08-06T00:07:06.816277: step 14415, loss 0.544956.
Train: 2018-08-06T00:07:06.989789: step 14416, loss 0.598534.
Train: 2018-08-06T00:07:07.164352: step 14417, loss 0.508928.
Train: 2018-08-06T00:07:07.337858: step 14418, loss 0.508808.
Train: 2018-08-06T00:07:07.509397: step 14419, loss 0.535923.
Train: 2018-08-06T00:07:07.681964: step 14420, loss 0.6705.
Test: 2018-08-06T00:07:07.921297: step 14420, loss 0.547603.
Train: 2018-08-06T00:07:08.103810: step 14421, loss 0.518029.
Train: 2018-08-06T00:07:08.275381: step 14422, loss 0.607107.
Train: 2018-08-06T00:07:08.452875: step 14423, loss 0.589323.
Train: 2018-08-06T00:07:08.619430: step 14424, loss 0.598276.
Train: 2018-08-06T00:07:08.801967: step 14425, loss 0.57159.
Train: 2018-08-06T00:07:08.977497: step 14426, loss 0.625231.
Train: 2018-08-06T00:07:09.148047: step 14427, loss 0.598255.
Train: 2018-08-06T00:07:09.319558: step 14428, loss 0.51835.
Train: 2018-08-06T00:07:09.487109: step 14429, loss 0.61544.
Train: 2018-08-06T00:07:09.675605: step 14430, loss 0.527673.
Test: 2018-08-06T00:07:09.913969: step 14430, loss 0.547771.
Train: 2018-08-06T00:07:10.087504: step 14431, loss 0.571085.
Train: 2018-08-06T00:07:10.262037: step 14432, loss 0.60623.
Train: 2018-08-06T00:07:10.444549: step 14433, loss 0.579799.
Train: 2018-08-06T00:07:10.618111: step 14434, loss 0.536458.
Train: 2018-08-06T00:07:10.792619: step 14435, loss 0.571121.
Train: 2018-08-06T00:07:10.966154: step 14436, loss 0.544966.
Train: 2018-08-06T00:07:11.145704: step 14437, loss 0.57131.
Train: 2018-08-06T00:07:11.333173: step 14438, loss 0.52805.
Train: 2018-08-06T00:07:11.507706: step 14439, loss 0.545337.
Train: 2018-08-06T00:07:11.695205: step 14440, loss 0.553836.
Test: 2018-08-06T00:07:11.933605: step 14440, loss 0.548065.
Train: 2018-08-06T00:07:12.108125: step 14441, loss 0.562441.
Train: 2018-08-06T00:07:12.287651: step 14442, loss 0.562456.
Train: 2018-08-06T00:07:12.457166: step 14443, loss 0.604976.
Train: 2018-08-06T00:07:12.629706: step 14444, loss 0.511235.
Train: 2018-08-06T00:07:12.800280: step 14445, loss 0.485558.
Train: 2018-08-06T00:07:12.973785: step 14446, loss 0.536678.
Train: 2018-08-06T00:07:13.148319: step 14447, loss 0.613779.
Train: 2018-08-06T00:07:13.328837: step 14448, loss 0.61387.
Train: 2018-08-06T00:07:13.508355: step 14449, loss 0.459586.
Train: 2018-08-06T00:07:13.689870: step 14450, loss 0.639818.
Test: 2018-08-06T00:07:13.930227: step 14450, loss 0.548021.
Train: 2018-08-06T00:07:14.102796: step 14451, loss 0.571029.
Train: 2018-08-06T00:07:14.276303: step 14452, loss 0.588242.
Train: 2018-08-06T00:07:14.446846: step 14453, loss 0.571034.
Train: 2018-08-06T00:07:14.624372: step 14454, loss 0.613815.
Train: 2018-08-06T00:07:14.794914: step 14455, loss 0.511096.
Train: 2018-08-06T00:07:14.968489: step 14456, loss 0.494065.
Train: 2018-08-06T00:07:15.140022: step 14457, loss 0.528261.
Train: 2018-08-06T00:07:15.313528: step 14458, loss 0.60517.
Train: 2018-08-06T00:07:15.484071: step 14459, loss 0.545285.
Train: 2018-08-06T00:07:15.655639: step 14460, loss 0.570951.
Test: 2018-08-06T00:07:15.891989: step 14460, loss 0.548027.
Train: 2018-08-06T00:07:16.070503: step 14461, loss 0.579359.
Train: 2018-08-06T00:07:16.247033: step 14462, loss 0.519394.
Train: 2018-08-06T00:07:16.419601: step 14463, loss 0.579516.
Train: 2018-08-06T00:07:16.587121: step 14464, loss 0.631242.
Train: 2018-08-06T00:07:16.757667: step 14465, loss 0.588133.
Train: 2018-08-06T00:07:16.932224: step 14466, loss 0.55376.
Train: 2018-08-06T00:07:17.105765: step 14467, loss 0.656659.
Train: 2018-08-06T00:07:17.281265: step 14468, loss 0.52826.
Train: 2018-08-06T00:07:17.463808: step 14469, loss 0.52828.
Train: 2018-08-06T00:07:17.636347: step 14470, loss 0.605013.
Test: 2018-08-06T00:07:17.873682: step 14470, loss 0.548155.
Train: 2018-08-06T00:07:18.047218: step 14471, loss 0.562334.
Train: 2018-08-06T00:07:18.219756: step 14472, loss 0.570918.
Train: 2018-08-06T00:07:18.391321: step 14473, loss 0.562382.
Train: 2018-08-06T00:07:18.561867: step 14474, loss 0.553977.
Train: 2018-08-06T00:07:18.731417: step 14475, loss 0.587735.
Train: 2018-08-06T00:07:18.905920: step 14476, loss 0.528577.
Train: 2018-08-06T00:07:19.077463: step 14477, loss 0.621672.
Train: 2018-08-06T00:07:19.247007: step 14478, loss 0.511985.
Train: 2018-08-06T00:07:19.415559: step 14479, loss 0.520472.
Train: 2018-08-06T00:07:19.588096: step 14480, loss 0.621683.
Test: 2018-08-06T00:07:19.825462: step 14480, loss 0.5483.
Train: 2018-08-06T00:07:20.005978: step 14481, loss 0.495004.
Train: 2018-08-06T00:07:20.178516: step 14482, loss 0.570918.
Train: 2018-08-06T00:07:20.348090: step 14483, loss 0.579366.
Train: 2018-08-06T00:07:20.517635: step 14484, loss 0.630189.
Train: 2018-08-06T00:07:20.688155: step 14485, loss 0.579299.
Train: 2018-08-06T00:07:20.871662: step 14486, loss 0.579318.
Train: 2018-08-06T00:07:21.051184: step 14487, loss 0.52031.
Train: 2018-08-06T00:07:21.220729: step 14488, loss 0.579312.
Train: 2018-08-06T00:07:21.397258: step 14489, loss 0.579294.
Train: 2018-08-06T00:07:21.570817: step 14490, loss 0.570804.
Test: 2018-08-06T00:07:21.809157: step 14490, loss 0.548367.
Train: 2018-08-06T00:07:21.983690: step 14491, loss 0.621328.
Train: 2018-08-06T00:07:22.154265: step 14492, loss 0.579276.
Train: 2018-08-06T00:07:22.323811: step 14493, loss 0.53732.
Train: 2018-08-06T00:07:22.494324: step 14494, loss 0.679567.
Train: 2018-08-06T00:07:22.663870: step 14495, loss 0.537448.
Train: 2018-08-06T00:07:22.829427: step 14496, loss 0.580304.
Train: 2018-08-06T00:07:23.004984: step 14497, loss 0.521024.
Train: 2018-08-06T00:07:23.179522: step 14498, loss 0.587428.
Train: 2018-08-06T00:07:23.352034: step 14499, loss 0.579074.
Train: 2018-08-06T00:07:23.521577: step 14500, loss 0.587286.
Test: 2018-08-06T00:07:23.758986: step 14500, loss 0.548739.
Train: 2018-08-06T00:07:24.503064: step 14501, loss 0.529548.
Train: 2018-08-06T00:07:24.677597: step 14502, loss 0.513129.
Train: 2018-08-06T00:07:24.845180: step 14503, loss 0.57084.
Train: 2018-08-06T00:07:25.020696: step 14504, loss 0.545944.
Train: 2018-08-06T00:07:25.201195: step 14505, loss 0.578979.
Train: 2018-08-06T00:07:25.372742: step 14506, loss 0.612269.
Train: 2018-08-06T00:07:25.542284: step 14507, loss 0.570808.
Train: 2018-08-06T00:07:25.710864: step 14508, loss 0.603986.
Train: 2018-08-06T00:07:25.892365: step 14509, loss 0.55434.
Train: 2018-08-06T00:07:26.072896: step 14510, loss 0.479873.
Test: 2018-08-06T00:07:26.312224: step 14510, loss 0.548669.
Train: 2018-08-06T00:07:26.483766: step 14511, loss 0.587308.
Train: 2018-08-06T00:07:26.662319: step 14512, loss 0.579079.
Train: 2018-08-06T00:07:26.840837: step 14513, loss 0.545941.
Train: 2018-08-06T00:07:27.015376: step 14514, loss 0.520949.
Train: 2018-08-06T00:07:27.188911: step 14515, loss 0.570859.
Train: 2018-08-06T00:07:27.367423: step 14516, loss 0.637531.
Train: 2018-08-06T00:07:27.539967: step 14517, loss 0.529051.
Train: 2018-08-06T00:07:27.712479: step 14518, loss 0.612737.
Train: 2018-08-06T00:07:27.888011: step 14519, loss 0.537413.
Train: 2018-08-06T00:07:28.057557: step 14520, loss 0.512429.
Test: 2018-08-06T00:07:28.282954: step 14520, loss 0.548441.
Train: 2018-08-06T00:07:28.464470: step 14521, loss 0.604366.
Train: 2018-08-06T00:07:28.664963: step 14522, loss 0.579217.
Train: 2018-08-06T00:07:28.836498: step 14523, loss 0.595984.
Train: 2018-08-06T00:07:29.014996: step 14524, loss 0.512113.
Train: 2018-08-06T00:07:29.190526: step 14525, loss 0.579252.
Train: 2018-08-06T00:07:29.361100: step 14526, loss 0.545653.
Train: 2018-08-06T00:07:29.529646: step 14527, loss 0.570831.
Train: 2018-08-06T00:07:29.702190: step 14528, loss 0.579214.
Train: 2018-08-06T00:07:29.871704: step 14529, loss 0.495027.
Train: 2018-08-06T00:07:30.043246: step 14530, loss 0.604673.
Test: 2018-08-06T00:07:30.280620: step 14530, loss 0.548278.
Train: 2018-08-06T00:07:30.465143: step 14531, loss 0.596165.
Train: 2018-08-06T00:07:30.640679: step 14532, loss 0.528648.
Train: 2018-08-06T00:07:30.815182: step 14533, loss 0.5709.
Train: 2018-08-06T00:07:30.983758: step 14534, loss 0.503178.
Train: 2018-08-06T00:07:31.153277: step 14535, loss 0.536959.
Train: 2018-08-06T00:07:31.330834: step 14536, loss 0.536871.
Train: 2018-08-06T00:07:31.506335: step 14537, loss 0.536791.
Train: 2018-08-06T00:07:31.692835: step 14538, loss 0.528168.
Train: 2018-08-06T00:07:31.862407: step 14539, loss 0.553869.
Train: 2018-08-06T00:07:32.048908: step 14540, loss 0.510562.
Test: 2018-08-06T00:07:32.289239: step 14540, loss 0.547914.
Train: 2018-08-06T00:07:32.474743: step 14541, loss 0.536503.
Train: 2018-08-06T00:07:32.651272: step 14542, loss 0.5624.
Train: 2018-08-06T00:07:32.820846: step 14543, loss 0.510215.
Train: 2018-08-06T00:07:32.993381: step 14544, loss 0.553538.
Train: 2018-08-06T00:07:33.188834: step 14545, loss 0.606821.
Train: 2018-08-06T00:07:33.371354: step 14546, loss 0.562426.
Train: 2018-08-06T00:07:33.576796: step 14547, loss 0.545065.
Train: 2018-08-06T00:07:33.781250: step 14548, loss 0.554048.
Train: 2018-08-06T00:07:33.975729: step 14549, loss 0.633843.
Train: 2018-08-06T00:07:34.150264: step 14550, loss 0.589314.
Test: 2018-08-06T00:07:34.387629: step 14550, loss 0.547696.
Train: 2018-08-06T00:07:34.564156: step 14551, loss 0.544893.
Train: 2018-08-06T00:07:34.734699: step 14552, loss 0.606647.
Train: 2018-08-06T00:07:34.913223: step 14553, loss 0.606522.
Train: 2018-08-06T00:07:35.102716: step 14554, loss 0.5096.
Train: 2018-08-06T00:07:35.278247: step 14555, loss 0.535785.
Train: 2018-08-06T00:07:35.455144: step 14556, loss 0.5538.
Train: 2018-08-06T00:07:35.628262: step 14557, loss 0.604823.
Train: 2018-08-06T00:07:35.803182: step 14558, loss 0.561319.
Train: 2018-08-06T00:07:35.977708: step 14559, loss 0.512478.
Train: 2018-08-06T00:07:36.156208: step 14560, loss 0.544902.
Test: 2018-08-06T00:07:36.397561: step 14560, loss 0.548316.
Train: 2018-08-06T00:07:36.568105: step 14561, loss 0.472271.
Train: 2018-08-06T00:07:36.737651: step 14562, loss 0.59679.
Train: 2018-08-06T00:07:36.907199: step 14563, loss 0.632628.
Train: 2018-08-06T00:07:37.083762: step 14564, loss 0.598228.
Train: 2018-08-06T00:07:37.267235: step 14565, loss 0.545419.
Train: 2018-08-06T00:07:37.451741: step 14566, loss 0.573345.
Train: 2018-08-06T00:07:37.624305: step 14567, loss 0.535661.
Train: 2018-08-06T00:07:37.805795: step 14568, loss 0.561625.
Train: 2018-08-06T00:07:37.981357: step 14569, loss 0.580297.
Train: 2018-08-06T00:07:38.153890: step 14570, loss 0.553292.
Test: 2018-08-06T00:07:38.392240: step 14570, loss 0.547676.
Train: 2018-08-06T00:07:38.569752: step 14571, loss 0.607268.
Train: 2018-08-06T00:07:38.738302: step 14572, loss 0.553077.
Train: 2018-08-06T00:07:38.907848: step 14573, loss 0.599089.
Train: 2018-08-06T00:07:39.089397: step 14574, loss 0.589058.
Train: 2018-08-06T00:07:39.273868: step 14575, loss 0.536488.
Train: 2018-08-06T00:07:39.446413: step 14576, loss 0.562262.
Train: 2018-08-06T00:07:39.625927: step 14577, loss 0.536011.
Train: 2018-08-06T00:07:39.796471: step 14578, loss 0.588464.
Train: 2018-08-06T00:07:39.959036: step 14579, loss 0.649133.
Train: 2018-08-06T00:07:40.134572: step 14580, loss 0.605642.
Test: 2018-08-06T00:07:40.371932: step 14580, loss 0.547973.
Train: 2018-08-06T00:07:40.546494: step 14581, loss 0.502075.
Train: 2018-08-06T00:07:40.718006: step 14582, loss 0.467814.
Train: 2018-08-06T00:07:40.891567: step 14583, loss 0.545292.
Train: 2018-08-06T00:07:41.065079: step 14584, loss 0.562387.
Train: 2018-08-06T00:07:41.248616: step 14585, loss 0.639943.
Train: 2018-08-06T00:07:41.417163: step 14586, loss 0.502242.
Train: 2018-08-06T00:07:41.588678: step 14587, loss 0.545401.
Train: 2018-08-06T00:07:41.765207: step 14588, loss 0.519569.
Train: 2018-08-06T00:07:41.936748: step 14589, loss 0.553784.
Train: 2018-08-06T00:07:42.103301: step 14590, loss 0.51086.
Test: 2018-08-06T00:07:42.346650: step 14590, loss 0.547969.
Train: 2018-08-06T00:07:42.520188: step 14591, loss 0.545186.
Train: 2018-08-06T00:07:42.694746: step 14592, loss 0.527881.
Train: 2018-08-06T00:07:42.863271: step 14593, loss 0.53644.
Train: 2018-08-06T00:07:43.037803: step 14594, loss 0.562396.
Train: 2018-08-06T00:07:43.204358: step 14595, loss 0.614679.
Train: 2018-08-06T00:07:43.376896: step 14596, loss 0.588593.
Train: 2018-08-06T00:07:43.548468: step 14597, loss 0.571136.
Train: 2018-08-06T00:07:43.717012: step 14598, loss 0.510122.
Train: 2018-08-06T00:07:43.901518: step 14599, loss 0.562415.
Train: 2018-08-06T00:07:44.074057: step 14600, loss 0.562427.
Test: 2018-08-06T00:07:44.314388: step 14600, loss 0.547797.
Train: 2018-08-06T00:07:45.162459: step 14601, loss 0.606138.
Train: 2018-08-06T00:07:45.334971: step 14602, loss 0.544998.
Train: 2018-08-06T00:07:45.506514: step 14603, loss 0.562462.
Train: 2018-08-06T00:07:45.679052: step 14604, loss 0.553701.
Train: 2018-08-06T00:07:45.852618: step 14605, loss 0.510002.
Train: 2018-08-06T00:07:46.029115: step 14606, loss 0.623687.
Train: 2018-08-06T00:07:46.205643: step 14607, loss 0.606165.
Train: 2018-08-06T00:07:46.373228: step 14608, loss 0.597365.
Train: 2018-08-06T00:07:46.545767: step 14609, loss 0.54499.
Train: 2018-08-06T00:07:46.715311: step 14610, loss 0.692968.
Test: 2018-08-06T00:07:46.954640: step 14610, loss 0.54789.
Train: 2018-08-06T00:07:47.126196: step 14611, loss 0.55379.
Train: 2018-08-06T00:07:47.294761: step 14612, loss 0.545091.
Train: 2018-08-06T00:07:47.459321: step 14613, loss 0.639932.
Train: 2018-08-06T00:07:47.634821: step 14614, loss 0.570968.
Train: 2018-08-06T00:07:47.809382: step 14615, loss 0.562428.
Train: 2018-08-06T00:07:47.981923: step 14616, loss 0.604949.
Train: 2018-08-06T00:07:48.150442: step 14617, loss 0.630257.
Train: 2018-08-06T00:07:48.319022: step 14618, loss 0.5878.
Train: 2018-08-06T00:07:48.484566: step 14619, loss 0.537304.
Train: 2018-08-06T00:07:48.657112: step 14620, loss 0.487191.
Test: 2018-08-06T00:07:48.895450: step 14620, loss 0.548486.
Train: 2018-08-06T00:07:49.067026: step 14621, loss 0.587536.
Train: 2018-08-06T00:07:49.240526: step 14622, loss 0.529129.
Train: 2018-08-06T00:07:49.410106: step 14623, loss 0.570809.
Train: 2018-08-06T00:07:49.579676: step 14624, loss 0.537492.
Train: 2018-08-06T00:07:49.750189: step 14625, loss 0.545835.
Train: 2018-08-06T00:07:49.924697: step 14626, loss 0.595824.
Train: 2018-08-06T00:07:50.104250: step 14627, loss 0.570784.
Train: 2018-08-06T00:07:50.276787: step 14628, loss 0.545874.
Train: 2018-08-06T00:07:50.449325: step 14629, loss 0.520889.
Train: 2018-08-06T00:07:50.629812: step 14630, loss 0.504065.
Test: 2018-08-06T00:07:50.869171: step 14630, loss 0.548481.
Train: 2018-08-06T00:07:51.039746: step 14631, loss 0.587514.
Train: 2018-08-06T00:07:51.209293: step 14632, loss 0.520587.
Train: 2018-08-06T00:07:51.377810: step 14633, loss 0.579271.
Train: 2018-08-06T00:07:51.545394: step 14634, loss 0.520402.
Train: 2018-08-06T00:07:51.712953: step 14635, loss 0.520217.
Train: 2018-08-06T00:07:51.882461: step 14636, loss 0.511593.
Train: 2018-08-06T00:07:52.049017: step 14637, loss 0.553889.
Train: 2018-08-06T00:07:52.217565: step 14638, loss 0.553894.
Train: 2018-08-06T00:07:52.403069: step 14639, loss 0.605224.
Train: 2018-08-06T00:07:52.579597: step 14640, loss 0.562457.
Test: 2018-08-06T00:07:52.816067: step 14640, loss 0.547976.
Train: 2018-08-06T00:07:52.991583: step 14641, loss 0.536543.
Train: 2018-08-06T00:07:53.162128: step 14642, loss 0.519188.
Train: 2018-08-06T00:07:53.337658: step 14643, loss 0.519112.
Train: 2018-08-06T00:07:53.505228: step 14644, loss 0.562391.
Train: 2018-08-06T00:07:53.679768: step 14645, loss 0.667137.
Train: 2018-08-06T00:07:53.847321: step 14646, loss 0.623543.
Train: 2018-08-06T00:07:54.012853: step 14647, loss 0.655553.
Train: 2018-08-06T00:07:54.180435: step 14648, loss 0.518868.
Train: 2018-08-06T00:07:54.353973: step 14649, loss 0.562409.
Train: 2018-08-06T00:07:54.527476: step 14650, loss 0.492887.
Test: 2018-08-06T00:07:54.765839: step 14650, loss 0.547875.
Train: 2018-08-06T00:07:54.933421: step 14651, loss 0.553743.
Train: 2018-08-06T00:07:55.103934: step 14652, loss 0.623193.
Train: 2018-08-06T00:07:55.267497: step 14653, loss 0.501622.
Train: 2018-08-06T00:07:55.435078: step 14654, loss 0.536479.
Train: 2018-08-06T00:07:55.605618: step 14655, loss 0.510261.
Train: 2018-08-06T00:07:55.775139: step 14656, loss 0.597183.
Train: 2018-08-06T00:07:55.946680: step 14657, loss 0.562451.
Train: 2018-08-06T00:07:56.115255: step 14658, loss 0.518832.
Train: 2018-08-06T00:07:56.295746: step 14659, loss 0.562486.
Train: 2018-08-06T00:07:56.467314: step 14660, loss 0.562545.
Test: 2018-08-06T00:07:56.704654: step 14660, loss 0.547803.
Train: 2018-08-06T00:07:56.880185: step 14661, loss 0.614819.
Train: 2018-08-06T00:07:57.058740: step 14662, loss 0.55378.
Train: 2018-08-06T00:07:57.229251: step 14663, loss 0.597283.
Train: 2018-08-06T00:07:57.396835: step 14664, loss 0.623539.
Train: 2018-08-06T00:07:57.579315: step 14665, loss 0.501442.
Train: 2018-08-06T00:07:57.749889: step 14666, loss 0.562454.
Train: 2018-08-06T00:07:57.924391: step 14667, loss 0.562325.
Train: 2018-08-06T00:07:58.087980: step 14668, loss 0.579702.
Train: 2018-08-06T00:07:58.252545: step 14669, loss 0.623162.
Train: 2018-08-06T00:07:58.418099: step 14670, loss 0.55366.
Test: 2018-08-06T00:07:58.655437: step 14670, loss 0.547932.
Train: 2018-08-06T00:07:58.822022: step 14671, loss 0.545119.
Train: 2018-08-06T00:07:58.986560: step 14672, loss 0.596875.
Train: 2018-08-06T00:07:59.154120: step 14673, loss 0.484938.
Train: 2018-08-06T00:07:59.322677: step 14674, loss 0.519425.
Train: 2018-08-06T00:07:59.491201: step 14675, loss 0.588138.
Train: 2018-08-06T00:07:59.661776: step 14676, loss 0.622718.
Train: 2018-08-06T00:07:59.827303: step 14677, loss 0.579607.
Train: 2018-08-06T00:07:59.997846: step 14678, loss 0.562412.
Train: 2018-08-06T00:08:00.167420: step 14679, loss 0.476648.
Train: 2018-08-06T00:08:00.335943: step 14680, loss 0.528043.
Test: 2018-08-06T00:08:00.573307: step 14680, loss 0.547997.
Train: 2018-08-06T00:08:00.744879: step 14681, loss 0.493566.
Train: 2018-08-06T00:08:00.912401: step 14682, loss 0.553664.
Train: 2018-08-06T00:08:01.076991: step 14683, loss 0.467291.
Train: 2018-08-06T00:08:01.254487: step 14684, loss 0.597268.
Train: 2018-08-06T00:08:01.419077: step 14685, loss 0.527517.
Train: 2018-08-06T00:08:01.597593: step 14686, loss 0.623527.
Train: 2018-08-06T00:08:01.772101: step 14687, loss 0.632449.
Train: 2018-08-06T00:08:01.950282: step 14688, loss 0.544986.
Train: 2018-08-06T00:08:02.119857: step 14689, loss 0.536061.
Train: 2018-08-06T00:08:02.286889: step 14690, loss 0.614867.
Test: 2018-08-06T00:08:02.525252: step 14690, loss 0.547791.
Train: 2018-08-06T00:08:02.694799: step 14691, loss 0.544914.
Train: 2018-08-06T00:08:02.866366: step 14692, loss 0.579887.
Train: 2018-08-06T00:08:03.036885: step 14693, loss 0.579786.
Train: 2018-08-06T00:08:03.213412: step 14694, loss 0.509997.
Train: 2018-08-06T00:08:03.391934: step 14695, loss 0.492561.
Train: 2018-08-06T00:08:03.564473: step 14696, loss 0.623739.
Train: 2018-08-06T00:08:03.742995: step 14697, loss 0.606003.
Train: 2018-08-06T00:08:03.912542: step 14698, loss 0.632343.
Train: 2018-08-06T00:08:04.113007: step 14699, loss 0.588689.
Train: 2018-08-06T00:08:04.276593: step 14700, loss 0.510487.
Test: 2018-08-06T00:08:04.515928: step 14700, loss 0.547874.
Train: 2018-08-06T00:08:05.334871: step 14701, loss 0.562271.
Train: 2018-08-06T00:08:05.552290: step 14702, loss 0.527918.
Train: 2018-08-06T00:08:05.722859: step 14703, loss 0.614229.
Train: 2018-08-06T00:08:05.890410: step 14704, loss 0.501787.
Train: 2018-08-06T00:08:06.053973: step 14705, loss 0.597032.
Train: 2018-08-06T00:08:06.226505: step 14706, loss 0.571031.
Train: 2018-08-06T00:08:06.400022: step 14707, loss 0.553599.
Train: 2018-08-06T00:08:06.571595: step 14708, loss 0.571029.
Train: 2018-08-06T00:08:06.735153: step 14709, loss 0.562493.
Train: 2018-08-06T00:08:06.901682: step 14710, loss 0.656962.
Test: 2018-08-06T00:08:07.139047: step 14710, loss 0.548021.
Train: 2018-08-06T00:08:07.311610: step 14711, loss 0.596788.
Train: 2018-08-06T00:08:07.480135: step 14712, loss 0.494042.
Train: 2018-08-06T00:08:07.649714: step 14713, loss 0.596705.
Train: 2018-08-06T00:08:07.819248: step 14714, loss 0.536688.
Train: 2018-08-06T00:08:07.984785: step 14715, loss 0.605035.
Train: 2018-08-06T00:08:08.155329: step 14716, loss 0.562299.
Train: 2018-08-06T00:08:08.327870: step 14717, loss 0.528571.
Train: 2018-08-06T00:08:08.503399: step 14718, loss 0.55389.
Train: 2018-08-06T00:08:08.669952: step 14719, loss 0.562218.
Train: 2018-08-06T00:08:08.839499: step 14720, loss 0.553876.
Test: 2018-08-06T00:08:09.075868: step 14720, loss 0.548227.
Train: 2018-08-06T00:08:09.240458: step 14721, loss 0.579252.
Train: 2018-08-06T00:08:09.412990: step 14722, loss 0.61324.
Train: 2018-08-06T00:08:09.574564: step 14723, loss 0.587793.
Train: 2018-08-06T00:08:09.743082: step 14724, loss 0.545589.
Train: 2018-08-06T00:08:09.912629: step 14725, loss 0.54539.
Train: 2018-08-06T00:08:10.081203: step 14726, loss 0.486368.
Train: 2018-08-06T00:08:10.249753: step 14727, loss 0.53716.
Train: 2018-08-06T00:08:10.417280: step 14728, loss 0.587737.
Train: 2018-08-06T00:08:10.592841: step 14729, loss 0.545404.
Train: 2018-08-06T00:08:10.761388: step 14730, loss 0.604954.
Test: 2018-08-06T00:08:10.997727: step 14730, loss 0.548195.
Train: 2018-08-06T00:08:11.165310: step 14731, loss 0.562241.
Train: 2018-08-06T00:08:11.334857: step 14732, loss 0.485825.
Train: 2018-08-06T00:08:11.555236: step 14733, loss 0.596739.
Train: 2018-08-06T00:08:11.723816: step 14734, loss 0.477552.
Train: 2018-08-06T00:08:11.906339: step 14735, loss 0.528409.
Train: 2018-08-06T00:08:12.083853: step 14736, loss 0.562333.
Train: 2018-08-06T00:08:12.251411: step 14737, loss 0.570749.
Train: 2018-08-06T00:08:12.417929: step 14738, loss 0.57998.
Train: 2018-08-06T00:08:12.588498: step 14739, loss 0.596588.
Train: 2018-08-06T00:08:12.769993: step 14740, loss 0.54494.
Test: 2018-08-06T00:08:13.005359: step 14740, loss 0.547914.
Train: 2018-08-06T00:08:13.178920: step 14741, loss 0.527633.
Train: 2018-08-06T00:08:13.351432: step 14742, loss 0.562604.
Train: 2018-08-06T00:08:13.521004: step 14743, loss 0.545387.
Train: 2018-08-06T00:08:13.689530: step 14744, loss 0.49279.
Train: 2018-08-06T00:08:13.858116: step 14745, loss 0.519012.
Train: 2018-08-06T00:08:14.029645: step 14746, loss 0.518728.
Train: 2018-08-06T00:08:14.206172: step 14747, loss 0.580392.
Train: 2018-08-06T00:08:14.371703: step 14748, loss 0.527203.
Train: 2018-08-06T00:08:14.543245: step 14749, loss 0.561929.
Train: 2018-08-06T00:08:14.711825: step 14750, loss 0.580592.
Test: 2018-08-06T00:08:14.950182: step 14750, loss 0.547661.
Train: 2018-08-06T00:08:15.129677: step 14751, loss 0.571133.
Train: 2018-08-06T00:08:15.302216: step 14752, loss 0.59786.
Train: 2018-08-06T00:08:15.478743: step 14753, loss 0.554147.
Train: 2018-08-06T00:08:15.658294: step 14754, loss 0.598216.
Train: 2018-08-06T00:08:15.831830: step 14755, loss 0.508695.
Train: 2018-08-06T00:08:16.001376: step 14756, loss 0.696916.
Train: 2018-08-06T00:08:16.170892: step 14757, loss 0.589442.
Train: 2018-08-06T00:08:16.340439: step 14758, loss 0.527133.
Train: 2018-08-06T00:08:16.510992: step 14759, loss 0.606942.
Train: 2018-08-06T00:08:16.692528: step 14760, loss 0.536032.
Test: 2018-08-06T00:08:16.931857: step 14760, loss 0.547763.
Train: 2018-08-06T00:08:17.105405: step 14761, loss 0.518608.
Train: 2018-08-06T00:08:17.273943: step 14762, loss 0.562108.
Train: 2018-08-06T00:08:17.439500: step 14763, loss 0.641341.
Train: 2018-08-06T00:08:17.616053: step 14764, loss 0.501327.
Train: 2018-08-06T00:08:17.784577: step 14765, loss 0.562175.
Train: 2018-08-06T00:08:17.955121: step 14766, loss 0.553673.
Train: 2018-08-06T00:08:18.127660: step 14767, loss 0.536375.
Train: 2018-08-06T00:08:18.299201: step 14768, loss 0.562472.
Train: 2018-08-06T00:08:18.464789: step 14769, loss 0.536517.
Train: 2018-08-06T00:08:18.637297: step 14770, loss 0.536423.
Test: 2018-08-06T00:08:18.873664: step 14770, loss 0.547854.
Train: 2018-08-06T00:08:19.052217: step 14771, loss 0.562433.
Train: 2018-08-06T00:08:19.235722: step 14772, loss 0.53623.
Train: 2018-08-06T00:08:19.410262: step 14773, loss 0.580028.
Train: 2018-08-06T00:08:19.584763: step 14774, loss 0.544904.
Train: 2018-08-06T00:08:19.752341: step 14775, loss 0.510234.
Train: 2018-08-06T00:08:19.926881: step 14776, loss 0.536134.
Train: 2018-08-06T00:08:20.095397: step 14777, loss 0.614935.
Train: 2018-08-06T00:08:20.265966: step 14778, loss 0.614814.
Train: 2018-08-06T00:08:20.439477: step 14779, loss 0.562522.
Train: 2018-08-06T00:08:20.610051: step 14780, loss 0.649381.
Test: 2018-08-06T00:08:20.845392: step 14780, loss 0.547891.
Train: 2018-08-06T00:08:21.021942: step 14781, loss 0.527368.
Train: 2018-08-06T00:08:21.195461: step 14782, loss 0.484548.
Train: 2018-08-06T00:08:21.366030: step 14783, loss 0.597291.
Train: 2018-08-06T00:08:21.537571: step 14784, loss 0.553862.
Train: 2018-08-06T00:08:21.712098: step 14785, loss 0.545725.
Train: 2018-08-06T00:08:21.890598: step 14786, loss 0.562336.
Train: 2018-08-06T00:08:22.061171: step 14787, loss 0.622628.
Train: 2018-08-06T00:08:22.242687: step 14788, loss 0.587719.
Train: 2018-08-06T00:08:22.412201: step 14789, loss 0.578891.
Train: 2018-08-06T00:08:22.579752: step 14790, loss 0.595807.
Test: 2018-08-06T00:08:22.817118: step 14790, loss 0.54838.
Train: 2018-08-06T00:08:22.990679: step 14791, loss 0.588292.
Train: 2018-08-06T00:08:23.158232: step 14792, loss 0.648194.
Train: 2018-08-06T00:08:23.320802: step 14793, loss 0.571.
Train: 2018-08-06T00:08:23.493340: step 14794, loss 0.555523.
Train: 2018-08-06T00:08:23.664890: step 14795, loss 0.601701.
Train: 2018-08-06T00:08:23.833400: step 14796, loss 0.513394.
Train: 2018-08-06T00:08:24.003975: step 14797, loss 0.52932.
Train: 2018-08-06T00:08:24.189447: step 14798, loss 0.632898.
Train: 2018-08-06T00:08:24.370993: step 14799, loss 0.546228.
Train: 2018-08-06T00:08:24.542528: step 14800, loss 0.578966.
Test: 2018-08-06T00:08:24.779869: step 14800, loss 0.549241.
Train: 2018-08-06T00:08:25.540069: step 14801, loss 0.596516.
Train: 2018-08-06T00:08:25.720619: step 14802, loss 0.545488.
Train: 2018-08-06T00:08:25.895119: step 14803, loss 0.536745.
Train: 2018-08-06T00:08:26.095583: step 14804, loss 0.498517.
Train: 2018-08-06T00:08:26.269119: step 14805, loss 0.537522.
Train: 2018-08-06T00:08:26.443655: step 14806, loss 0.603176.
Train: 2018-08-06T00:08:26.633145: step 14807, loss 0.505219.
Train: 2018-08-06T00:08:26.808677: step 14808, loss 0.545184.
Train: 2018-08-06T00:08:26.982211: step 14809, loss 0.596427.
Train: 2018-08-06T00:08:27.159766: step 14810, loss 0.54554.
Test: 2018-08-06T00:08:27.399100: step 14810, loss 0.548159.
Train: 2018-08-06T00:08:27.628517: step 14811, loss 0.562229.
Train: 2018-08-06T00:08:27.800058: step 14812, loss 0.493974.
Train: 2018-08-06T00:08:27.978549: step 14813, loss 0.544721.
Train: 2018-08-06T00:08:28.156103: step 14814, loss 0.510641.
Train: 2018-08-06T00:08:28.331604: step 14815, loss 0.545067.
Train: 2018-08-06T00:08:28.521097: step 14816, loss 0.588289.
Train: 2018-08-06T00:08:28.694632: step 14817, loss 0.571551.
Train: 2018-08-06T00:08:28.874660: step 14818, loss 0.544773.
Train: 2018-08-06T00:08:29.049193: step 14819, loss 0.48304.
Train: 2018-08-06T00:08:29.227715: step 14820, loss 0.606654.
Test: 2018-08-06T00:08:29.471065: step 14820, loss 0.547676.
Train: 2018-08-06T00:08:29.641633: step 14821, loss 0.518082.
Train: 2018-08-06T00:08:29.812157: step 14822, loss 0.553815.
Train: 2018-08-06T00:08:29.981732: step 14823, loss 0.553594.
Train: 2018-08-06T00:08:30.154238: step 14824, loss 0.445594.
Train: 2018-08-06T00:08:30.324780: step 14825, loss 0.625624.
Train: 2018-08-06T00:08:30.497351: step 14826, loss 0.54539.
Train: 2018-08-06T00:08:30.667864: step 14827, loss 0.518547.
Train: 2018-08-06T00:08:30.840436: step 14828, loss 0.554374.
Train: 2018-08-06T00:08:31.018951: step 14829, loss 0.571831.
Train: 2018-08-06T00:08:31.214429: step 14830, loss 0.53502.
Test: 2018-08-06T00:08:31.452767: step 14830, loss 0.547575.
Train: 2018-08-06T00:08:31.635277: step 14831, loss 0.563122.
Train: 2018-08-06T00:08:31.823798: step 14832, loss 0.52683.
Train: 2018-08-06T00:08:32.003293: step 14833, loss 0.535341.
Train: 2018-08-06T00:08:32.178848: step 14834, loss 0.580782.
Train: 2018-08-06T00:08:32.352384: step 14835, loss 0.489597.
Train: 2018-08-06T00:08:32.531878: step 14836, loss 0.48902.
Train: 2018-08-06T00:08:32.708407: step 14837, loss 0.488893.
Train: 2018-08-06T00:08:32.882953: step 14838, loss 0.609445.
Train: 2018-08-06T00:08:33.060464: step 14839, loss 0.563596.
Train: 2018-08-06T00:08:33.233054: step 14840, loss 0.649002.
Test: 2018-08-06T00:08:33.471368: step 14840, loss 0.547666.
Train: 2018-08-06T00:08:33.657867: step 14841, loss 0.646424.
Train: 2018-08-06T00:08:33.833398: step 14842, loss 0.526636.
Train: 2018-08-06T00:08:34.013916: step 14843, loss 0.516632.
Train: 2018-08-06T00:08:34.186454: step 14844, loss 0.544267.
Train: 2018-08-06T00:08:34.368966: step 14845, loss 0.55362.
Train: 2018-08-06T00:08:34.551477: step 14846, loss 0.618445.
Train: 2018-08-06T00:08:34.728037: step 14847, loss 0.545009.
Train: 2018-08-06T00:08:34.903537: step 14848, loss 0.581423.
Train: 2018-08-06T00:08:35.077097: step 14849, loss 0.616911.
Train: 2018-08-06T00:08:35.251639: step 14850, loss 0.598599.
Test: 2018-08-06T00:08:35.489968: step 14850, loss 0.547617.
Train: 2018-08-06T00:08:35.671507: step 14851, loss 0.634722.
Train: 2018-08-06T00:08:35.849040: step 14852, loss 0.473744.
Train: 2018-08-06T00:08:36.024538: step 14853, loss 0.580041.
Train: 2018-08-06T00:08:36.199070: step 14854, loss 0.588903.
Train: 2018-08-06T00:08:36.375599: step 14855, loss 0.650381.
Train: 2018-08-06T00:08:36.547168: step 14856, loss 0.606222.
Train: 2018-08-06T00:08:36.734672: step 14857, loss 0.587904.
Train: 2018-08-06T00:08:36.905208: step 14858, loss 0.527864.
Train: 2018-08-06T00:08:37.076755: step 14859, loss 0.587646.
Train: 2018-08-06T00:08:37.249310: step 14860, loss 0.595232.
Test: 2018-08-06T00:08:37.488623: step 14860, loss 0.548929.
Train: 2018-08-06T00:08:37.661172: step 14861, loss 0.659785.
Train: 2018-08-06T00:08:37.835719: step 14862, loss 0.56241.
Train: 2018-08-06T00:08:38.013220: step 14863, loss 0.582695.
Train: 2018-08-06T00:08:38.180798: step 14864, loss 0.589362.
Train: 2018-08-06T00:08:38.354308: step 14865, loss 0.623812.
Train: 2018-08-06T00:08:38.528840: step 14866, loss 0.544495.
Train: 2018-08-06T00:08:38.703374: step 14867, loss 0.566398.
Train: 2018-08-06T00:08:38.880928: step 14868, loss 0.486795.
Train: 2018-08-06T00:08:39.061448: step 14869, loss 0.556339.
Train: 2018-08-06T00:08:39.236947: step 14870, loss 0.589266.
Test: 2018-08-06T00:08:39.477305: step 14870, loss 0.548968.
Train: 2018-08-06T00:08:39.663804: step 14871, loss 0.571048.
Train: 2018-08-06T00:08:39.836344: step 14872, loss 0.530071.
Train: 2018-08-06T00:08:40.010896: step 14873, loss 0.637433.
Train: 2018-08-06T00:08:40.184414: step 14874, loss 0.629106.
Train: 2018-08-06T00:08:40.355985: step 14875, loss 0.604212.
Train: 2018-08-06T00:08:40.533511: step 14876, loss 0.669999.
Train: 2018-08-06T00:08:40.707015: step 14877, loss 0.480631.
Train: 2018-08-06T00:08:40.887563: step 14878, loss 0.570856.
Train: 2018-08-06T00:08:41.063063: step 14879, loss 0.635881.
Train: 2018-08-06T00:08:41.234605: step 14880, loss 0.513527.
Test: 2018-08-06T00:08:41.471969: step 14880, loss 0.549121.
Train: 2018-08-06T00:08:41.659468: step 14881, loss 0.644179.
Train: 2018-08-06T00:08:41.832007: step 14882, loss 0.643876.
Train: 2018-08-06T00:08:42.004579: step 14883, loss 0.611261.
Train: 2018-08-06T00:08:42.189052: step 14884, loss 0.579261.
Train: 2018-08-06T00:08:42.374557: step 14885, loss 0.642271.
Train: 2018-08-06T00:08:42.553078: step 14886, loss 0.561851.
Train: 2018-08-06T00:08:42.726614: step 14887, loss 0.586775.
Train: 2018-08-06T00:08:42.898186: step 14888, loss 0.579223.
Train: 2018-08-06T00:08:43.071702: step 14889, loss 0.53301.
Train: 2018-08-06T00:08:43.245228: step 14890, loss 0.534695.
Test: 2018-08-06T00:08:43.483590: step 14890, loss 0.551583.
Train: 2018-08-06T00:08:43.667124: step 14891, loss 0.583726.
Train: 2018-08-06T00:08:43.852634: step 14892, loss 0.590991.
Train: 2018-08-06T00:08:44.055068: step 14893, loss 0.553494.
Train: 2018-08-06T00:08:44.234581: step 14894, loss 0.533357.
Train: 2018-08-06T00:08:44.406153: step 14895, loss 0.486918.
Train: 2018-08-06T00:08:44.582685: step 14896, loss 0.626045.
Train: 2018-08-06T00:08:44.758214: step 14897, loss 0.556338.
Train: 2018-08-06T00:08:44.936732: step 14898, loss 0.571316.
Train: 2018-08-06T00:08:45.112237: step 14899, loss 0.563103.
Train: 2018-08-06T00:08:45.291779: step 14900, loss 0.572577.
Test: 2018-08-06T00:08:45.528122: step 14900, loss 0.54939.
Train: 2018-08-06T00:08:46.306651: step 14901, loss 0.538454.
Train: 2018-08-06T00:08:46.478192: step 14902, loss 0.602568.
Train: 2018-08-06T00:08:46.665730: step 14903, loss 0.505576.
Train: 2018-08-06T00:08:46.846232: step 14904, loss 0.530365.
Train: 2018-08-06T00:08:47.028719: step 14905, loss 0.587413.
Train: 2018-08-06T00:08:47.206244: step 14906, loss 0.562325.
Train: 2018-08-06T00:08:47.379286: step 14907, loss 0.512792.
Train: 2018-08-06T00:08:47.556812: step 14908, loss 0.496027.
Train: 2018-08-06T00:08:47.731371: step 14909, loss 0.595929.
Train: 2018-08-06T00:08:47.914853: step 14910, loss 0.579297.
Test: 2018-08-06T00:08:48.153218: step 14910, loss 0.548284.
Train: 2018-08-06T00:08:48.330743: step 14911, loss 0.604711.
Train: 2018-08-06T00:08:48.516277: step 14912, loss 0.638169.
Train: 2018-08-06T00:08:48.689787: step 14913, loss 0.503292.
Train: 2018-08-06T00:08:48.859354: step 14914, loss 0.604631.
Train: 2018-08-06T00:08:49.029873: step 14915, loss 0.60525.
Train: 2018-08-06T00:08:49.202442: step 14916, loss 0.570944.
Train: 2018-08-06T00:08:49.380950: step 14917, loss 0.613594.
Train: 2018-08-06T00:08:49.553474: step 14918, loss 0.54612.
Train: 2018-08-06T00:08:49.725044: step 14919, loss 0.495166.
Train: 2018-08-06T00:08:49.899548: step 14920, loss 0.545297.
Test: 2018-08-06T00:08:50.135914: step 14920, loss 0.548239.
Train: 2018-08-06T00:08:50.326441: step 14921, loss 0.528791.
Train: 2018-08-06T00:08:50.509655: step 14922, loss 0.54592.
Train: 2018-08-06T00:08:50.679801: step 14923, loss 0.570729.
Train: 2018-08-06T00:08:50.854303: step 14924, loss 0.510773.
Train: 2018-08-06T00:08:51.024848: step 14925, loss 0.562098.
Train: 2018-08-06T00:08:51.199382: step 14926, loss 0.614192.
Train: 2018-08-06T00:08:51.371950: step 14927, loss 0.570881.
Train: 2018-08-06T00:08:51.554431: step 14928, loss 0.536429.
Train: 2018-08-06T00:08:51.734949: step 14929, loss 0.49402.
Train: 2018-08-06T00:08:51.906489: step 14930, loss 0.579779.
Test: 2018-08-06T00:08:52.143878: step 14930, loss 0.547968.
Train: 2018-08-06T00:08:52.333347: step 14931, loss 0.519335.
Train: 2018-08-06T00:08:52.507911: step 14932, loss 0.536604.
Train: 2018-08-06T00:08:52.687401: step 14933, loss 0.527719.
Train: 2018-08-06T00:08:52.865925: step 14934, loss 0.536582.
Train: 2018-08-06T00:08:53.040487: step 14935, loss 0.519414.
Train: 2018-08-06T00:08:53.223991: step 14936, loss 0.579761.
Train: 2018-08-06T00:08:53.402489: step 14937, loss 0.509371.
Train: 2018-08-06T00:08:53.580023: step 14938, loss 0.544982.
Train: 2018-08-06T00:08:53.752584: step 14939, loss 0.588969.
Train: 2018-08-06T00:08:53.937059: step 14940, loss 0.615783.
Test: 2018-08-06T00:08:54.175423: step 14940, loss 0.547654.
Train: 2018-08-06T00:08:54.354967: step 14941, loss 0.535469.
Train: 2018-08-06T00:08:54.537454: step 14942, loss 0.526929.
Train: 2018-08-06T00:08:54.709993: step 14943, loss 0.455323.
Train: 2018-08-06T00:08:54.892530: step 14944, loss 0.499967.
Train: 2018-08-06T00:08:55.071058: step 14945, loss 0.589539.
Train: 2018-08-06T00:08:55.244573: step 14946, loss 0.590089.
Train: 2018-08-06T00:08:55.420101: step 14947, loss 0.480944.
Train: 2018-08-06T00:08:55.594678: step 14948, loss 0.617425.
Train: 2018-08-06T00:08:55.762179: step 14949, loss 0.503567.
Train: 2018-08-06T00:08:55.937716: step 14950, loss 0.626695.
Test: 2018-08-06T00:08:56.182059: step 14950, loss 0.547579.
Train: 2018-08-06T00:08:56.357589: step 14951, loss 0.535496.
Train: 2018-08-06T00:08:56.536107: step 14952, loss 0.516956.
Train: 2018-08-06T00:08:56.714630: step 14953, loss 0.590051.
Train: 2018-08-06T00:08:56.888166: step 14954, loss 0.535543.
Train: 2018-08-06T00:08:57.061702: step 14955, loss 0.636598.
Train: 2018-08-06T00:08:57.234267: step 14956, loss 0.544877.
Train: 2018-08-06T00:08:57.411791: step 14957, loss 0.607646.
Train: 2018-08-06T00:08:57.590289: step 14958, loss 0.554169.
Train: 2018-08-06T00:08:57.761856: step 14959, loss 0.572005.
Train: 2018-08-06T00:08:57.939386: step 14960, loss 0.56212.
Test: 2018-08-06T00:08:58.177732: step 14960, loss 0.547594.
Train: 2018-08-06T00:08:58.355270: step 14961, loss 0.553709.
Train: 2018-08-06T00:08:58.526785: step 14962, loss 0.61671.
Train: 2018-08-06T00:08:58.698357: step 14963, loss 0.464078.
Train: 2018-08-06T00:08:58.875884: step 14964, loss 0.588981.
Train: 2018-08-06T00:08:59.051421: step 14965, loss 0.553662.
Train: 2018-08-06T00:08:59.222923: step 14966, loss 0.572546.
Train: 2018-08-06T00:08:59.394464: step 14967, loss 0.580383.
Train: 2018-08-06T00:08:59.572987: step 14968, loss 0.572108.
Train: 2018-08-06T00:08:59.754502: step 14969, loss 0.633828.
Train: 2018-08-06T00:08:59.927040: step 14970, loss 0.659312.
Test: 2018-08-06T00:09:00.165403: step 14970, loss 0.54778.
Train: 2018-08-06T00:09:00.355892: step 14971, loss 0.597538.
Train: 2018-08-06T00:09:00.533419: step 14972, loss 0.501772.
Train: 2018-08-06T00:09:00.707952: step 14973, loss 0.501444.
Train: 2018-08-06T00:09:00.879492: step 14974, loss 0.562076.
Train: 2018-08-06T00:09:01.060041: step 14975, loss 0.571268.
Train: 2018-08-06T00:09:01.236568: step 14976, loss 0.553975.
Train: 2018-08-06T00:09:01.409107: step 14977, loss 0.528549.
Train: 2018-08-06T00:09:01.585637: step 14978, loss 0.519721.
Train: 2018-08-06T00:09:01.766121: step 14979, loss 0.54551.
Train: 2018-08-06T00:09:01.957640: step 14980, loss 0.527792.
Test: 2018-08-06T00:09:02.193977: step 14980, loss 0.548025.
Train: 2018-08-06T00:09:02.371503: step 14981, loss 0.493772.
Train: 2018-08-06T00:09:02.547059: step 14982, loss 0.49357.
Train: 2018-08-06T00:09:02.715582: step 14983, loss 0.614527.
Train: 2018-08-06T00:09:02.891112: step 14984, loss 0.622651.
Train: 2018-08-06T00:09:03.071632: step 14985, loss 0.562488.
Train: 2018-08-06T00:09:03.248159: step 14986, loss 0.570807.
Train: 2018-08-06T00:09:03.424711: step 14987, loss 0.492795.
Train: 2018-08-06T00:09:03.599218: step 14988, loss 0.57157.
Train: 2018-08-06T00:09:03.769764: step 14989, loss 0.545083.
Train: 2018-08-06T00:09:03.941335: step 14990, loss 0.684602.
Test: 2018-08-06T00:09:04.178675: step 14990, loss 0.547866.
Train: 2018-08-06T00:09:04.355228: step 14991, loss 0.536454.
Train: 2018-08-06T00:09:04.531749: step 14992, loss 0.579636.
Train: 2018-08-06T00:09:04.710279: step 14993, loss 0.536626.
Train: 2018-08-06T00:09:04.878797: step 14994, loss 0.5624.
Train: 2018-08-06T00:09:05.067293: step 14995, loss 0.553756.
Train: 2018-08-06T00:09:05.235842: step 14996, loss 0.545195.
Train: 2018-08-06T00:09:05.425366: step 14997, loss 0.61385.
Train: 2018-08-06T00:09:05.601864: step 14998, loss 0.519404.
Train: 2018-08-06T00:09:05.785401: step 14999, loss 0.475881.
Train: 2018-08-06T00:09:05.957911: step 15000, loss 0.536703.
Test: 2018-08-06T00:09:06.195277: step 15000, loss 0.547886.
Train: 2018-08-06T00:09:06.922079: step 15001, loss 0.579602.
Train: 2018-08-06T00:09:07.099635: step 15002, loss 0.570804.
Train: 2018-08-06T00:09:07.282142: step 15003, loss 0.545229.
Train: 2018-08-06T00:09:07.456659: step 15004, loss 0.544708.
Train: 2018-08-06T00:09:07.635174: step 15005, loss 0.580041.
Train: 2018-08-06T00:09:07.832644: step 15006, loss 0.623855.
Train: 2018-08-06T00:09:08.009684: step 15007, loss 0.55378.
Train: 2018-08-06T00:09:08.179588: step 15008, loss 0.56235.
Train: 2018-08-06T00:09:08.356112: step 15009, loss 0.562319.
Train: 2018-08-06T00:09:08.536602: step 15010, loss 0.510213.
Test: 2018-08-06T00:09:08.774963: step 15010, loss 0.547823.
Train: 2018-08-06T00:09:08.950521: step 15011, loss 0.553667.
Train: 2018-08-06T00:09:09.131044: step 15012, loss 0.588541.
Train: 2018-08-06T00:09:09.328510: step 15013, loss 0.553595.
Train: 2018-08-06T00:09:09.504040: step 15014, loss 0.588667.
Train: 2018-08-06T00:09:09.676553: step 15015, loss 0.509685.
Train: 2018-08-06T00:09:09.854087: step 15016, loss 0.50129.
Train: 2018-08-06T00:09:10.035623: step 15017, loss 0.527693.
Train: 2018-08-06T00:09:10.218136: step 15018, loss 0.536039.
Train: 2018-08-06T00:09:10.393667: step 15019, loss 0.536109.
Train: 2018-08-06T00:09:10.564180: step 15020, loss 0.456937.
Test: 2018-08-06T00:09:10.801545: step 15020, loss 0.547703.
Train: 2018-08-06T00:09:10.978072: step 15021, loss 0.624274.
Train: 2018-08-06T00:09:11.161607: step 15022, loss 0.580253.
Train: 2018-08-06T00:09:11.332125: step 15023, loss 0.571144.
Train: 2018-08-06T00:09:11.510680: step 15024, loss 0.571651.
Train: 2018-08-06T00:09:11.684184: step 15025, loss 0.580512.
Train: 2018-08-06T00:09:11.864707: step 15026, loss 0.562402.
Train: 2018-08-06T00:09:12.042227: step 15027, loss 0.491439.
Train: 2018-08-06T00:09:12.218755: step 15028, loss 0.615676.
Train: 2018-08-06T00:09:12.404258: step 15029, loss 0.580427.
Train: 2018-08-06T00:09:12.574833: step 15030, loss 0.606928.
Test: 2018-08-06T00:09:12.811175: step 15030, loss 0.547667.
Train: 2018-08-06T00:09:12.991687: step 15031, loss 0.58877.
Train: 2018-08-06T00:09:13.162232: step 15032, loss 0.580069.
Train: 2018-08-06T00:09:13.331811: step 15033, loss 0.579993.
Train: 2018-08-06T00:09:13.505314: step 15034, loss 0.535997.
Train: 2018-08-06T00:09:13.679877: step 15035, loss 0.632324.
Train: 2018-08-06T00:09:13.858380: step 15036, loss 0.615194.
Train: 2018-08-06T00:09:14.036919: step 15037, loss 0.545342.
Train: 2018-08-06T00:09:14.211451: step 15038, loss 0.570958.
Train: 2018-08-06T00:09:14.383019: step 15039, loss 0.63985.
Train: 2018-08-06T00:09:14.556502: step 15040, loss 0.59727.
Test: 2018-08-06T00:09:14.794865: step 15040, loss 0.548062.
Train: 2018-08-06T00:09:14.981395: step 15041, loss 0.527707.
Train: 2018-08-06T00:09:15.167897: step 15042, loss 0.562433.
Train: 2018-08-06T00:09:15.341404: step 15043, loss 0.554101.
Train: 2018-08-06T00:09:15.521920: step 15044, loss 0.588088.
Train: 2018-08-06T00:09:15.691498: step 15045, loss 0.520608.
Train: 2018-08-06T00:09:15.867995: step 15046, loss 0.68933.
Train: 2018-08-06T00:09:16.047561: step 15047, loss 0.495219.
Train: 2018-08-06T00:09:16.232053: step 15048, loss 0.613259.
Train: 2018-08-06T00:09:16.412538: step 15049, loss 0.529095.
Train: 2018-08-06T00:09:16.594054: step 15050, loss 0.530169.
Test: 2018-08-06T00:09:16.834410: step 15050, loss 0.548573.
Train: 2018-08-06T00:09:17.018918: step 15051, loss 0.546545.
Train: 2018-08-06T00:09:17.191481: step 15052, loss 0.596123.
Train: 2018-08-06T00:09:17.368030: step 15053, loss 0.496328.
Train: 2018-08-06T00:09:17.537529: step 15054, loss 0.571392.
Train: 2018-08-06T00:09:17.718074: step 15055, loss 0.596404.
Train: 2018-08-06T00:09:17.890613: step 15056, loss 0.570132.
Train: 2018-08-06T00:09:18.069119: step 15057, loss 0.545569.
Train: 2018-08-06T00:09:18.237686: step 15058, loss 0.546035.
Train: 2018-08-06T00:09:18.415183: step 15059, loss 0.495075.
Train: 2018-08-06T00:09:18.590714: step 15060, loss 0.562749.
Test: 2018-08-06T00:09:18.828080: step 15060, loss 0.548276.
Train: 2018-08-06T00:09:19.034528: step 15061, loss 0.503233.
Train: 2018-08-06T00:09:19.221028: step 15062, loss 0.494218.
Train: 2018-08-06T00:09:19.426481: step 15063, loss 0.562739.
Train: 2018-08-06T00:09:19.600015: step 15064, loss 0.545095.
Train: 2018-08-06T00:09:19.772553: step 15065, loss 0.484367.
Train: 2018-08-06T00:09:19.942100: step 15066, loss 0.631712.
Train: 2018-08-06T00:09:20.112643: step 15067, loss 0.465964.
Train: 2018-08-06T00:09:20.277203: step 15068, loss 0.606279.
Train: 2018-08-06T00:09:20.450739: step 15069, loss 0.535737.
Train: 2018-08-06T00:09:20.630259: step 15070, loss 0.615513.
Test: 2018-08-06T00:09:20.869624: step 15070, loss 0.547595.
Train: 2018-08-06T00:09:21.043155: step 15071, loss 0.643818.
Train: 2018-08-06T00:09:21.211735: step 15072, loss 0.598059.
Train: 2018-08-06T00:09:21.382278: step 15073, loss 0.518495.
Train: 2018-08-06T00:09:21.551826: step 15074, loss 0.535656.
Train: 2018-08-06T00:09:21.722364: step 15075, loss 0.509026.
Train: 2018-08-06T00:09:21.890912: step 15076, loss 0.571832.
Train: 2018-08-06T00:09:22.064454: step 15077, loss 0.614972.
Train: 2018-08-06T00:09:22.233995: step 15078, loss 0.545523.
Train: 2018-08-06T00:09:22.404545: step 15079, loss 0.589843.
Train: 2018-08-06T00:09:22.581042: step 15080, loss 0.492338.
Test: 2018-08-06T00:09:22.817411: step 15080, loss 0.547801.
Train: 2018-08-06T00:09:22.997953: step 15081, loss 0.588798.
Train: 2018-08-06T00:09:23.169493: step 15082, loss 0.553091.
Train: 2018-08-06T00:09:23.342037: step 15083, loss 0.501776.
Train: 2018-08-06T00:09:23.520529: step 15084, loss 0.51006.
Train: 2018-08-06T00:09:23.692102: step 15085, loss 0.579842.
Train: 2018-08-06T00:09:23.866631: step 15086, loss 0.483567.
Train: 2018-08-06T00:09:24.038170: step 15087, loss 0.623901.
Train: 2018-08-06T00:09:24.213702: step 15088, loss 0.606054.
Train: 2018-08-06T00:09:24.393196: step 15089, loss 0.580118.
Train: 2018-08-06T00:09:24.564737: step 15090, loss 0.536524.
Test: 2018-08-06T00:09:24.794140: step 15090, loss 0.547776.
Train: 2018-08-06T00:09:24.968656: step 15091, loss 0.570907.
Train: 2018-08-06T00:09:25.157153: step 15092, loss 0.53642.
Train: 2018-08-06T00:09:25.324704: step 15093, loss 0.597451.
Train: 2018-08-06T00:09:25.497267: step 15094, loss 0.448945.
Train: 2018-08-06T00:09:25.687733: step 15095, loss 0.606327.
Train: 2018-08-06T00:09:25.855285: step 15096, loss 0.579964.
Train: 2018-08-06T00:09:26.031813: step 15097, loss 0.615427.
Train: 2018-08-06T00:09:26.203374: step 15098, loss 0.606731.
Train: 2018-08-06T00:09:26.372900: step 15099, loss 0.623809.
Train: 2018-08-06T00:09:26.538489: step 15100, loss 0.636917.
Test: 2018-08-06T00:09:26.775824: step 15100, loss 0.547948.
