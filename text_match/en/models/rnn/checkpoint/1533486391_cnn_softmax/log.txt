Train: 2018-08-06T00:26:39.285626: step 1, loss 0.782454.
Train: 2018-08-06T00:26:39.601811: step 2, loss 0.769005.
Train: 2018-08-06T00:26:39.879039: step 3, loss 0.665194.
Train: 2018-08-06T00:26:40.153338: step 4, loss 0.701473.
Train: 2018-08-06T00:26:40.424579: step 5, loss 0.707267.
Train: 2018-08-06T00:26:40.703833: step 6, loss 0.634669.
Train: 2018-08-06T00:26:40.980094: step 7, loss 0.862323.
Train: 2018-08-06T00:26:41.261372: step 8, loss 0.756595.
Train: 2018-08-06T00:26:41.534642: step 9, loss 0.723227.
Train: 2018-08-06T00:26:41.805915: step 10, loss 0.701253.
Test: 2018-08-06T00:26:43.423559: step 10, loss 0.693147.
Train: 2018-08-06T00:26:43.698856: step 11, loss 0.694338.
Train: 2018-08-06T00:26:43.979104: step 12, loss 0.696272.
Train: 2018-08-06T00:26:44.247386: step 13, loss 0.693746.
Train: 2018-08-06T00:26:44.514640: step 14, loss 0.693147.
Train: 2018-08-06T00:26:44.782923: step 15, loss 0.693147.
Train: 2018-08-06T00:26:45.057220: step 16, loss 0.692073.
Train: 2018-08-06T00:26:45.329502: step 17, loss 0.693147.
Train: 2018-08-06T00:26:45.617691: step 18, loss 0.693147.
Train: 2018-08-06T00:26:45.887968: step 19, loss 0.693147.
Train: 2018-08-06T00:26:46.160241: step 20, loss 0.69503.
Test: 2018-08-06T00:26:47.430842: step 20, loss 0.693147.
Train: 2018-08-06T00:26:47.694164: step 21, loss 0.693147.
Train: 2018-08-06T00:26:47.971397: step 22, loss 0.693147.
Train: 2018-08-06T00:26:48.240707: step 23, loss 0.693147.
Train: 2018-08-06T00:26:48.523954: step 24, loss 0.692822.
Train: 2018-08-06T00:26:48.802209: step 25, loss 0.693147.
Train: 2018-08-06T00:26:49.075490: step 26, loss 0.693147.
Train: 2018-08-06T00:26:49.344723: step 27, loss 0.693147.
Train: 2018-08-06T00:26:49.619018: step 28, loss 0.693147.
Train: 2018-08-06T00:26:49.891263: step 29, loss 0.693147.
Train: 2018-08-06T00:26:50.160568: step 30, loss 0.693147.
Test: 2018-08-06T00:26:51.420174: step 30, loss 0.693147.
Train: 2018-08-06T00:26:51.698465: step 31, loss 0.693147.
Train: 2018-08-06T00:26:51.978711: step 32, loss 0.693147.
Train: 2018-08-06T00:26:52.251949: step 33, loss 0.693147.
Train: 2018-08-06T00:26:52.521260: step 34, loss 0.693147.
Train: 2018-08-06T00:26:52.793531: step 35, loss 0.693147.
Train: 2018-08-06T00:26:53.058816: step 36, loss 0.693207.
Train: 2018-08-06T00:26:53.328071: step 37, loss 0.693147.
Train: 2018-08-06T00:26:53.606357: step 38, loss 0.693005.
Train: 2018-08-06T00:26:53.879641: step 39, loss 0.693147.
Train: 2018-08-06T00:26:54.153863: step 40, loss 0.693147.
Test: 2018-08-06T00:26:55.422470: step 40, loss 0.693147.
Train: 2018-08-06T00:26:55.695764: step 41, loss 0.693147.
Train: 2018-08-06T00:26:55.966046: step 42, loss 0.693147.
Train: 2018-08-06T00:26:56.242278: step 43, loss 0.693122.
Train: 2018-08-06T00:26:56.518539: step 44, loss 0.692612.
Train: 2018-08-06T00:26:56.792839: step 45, loss 0.692166.
Train: 2018-08-06T00:26:57.071077: step 46, loss 0.691773.
Train: 2018-08-06T00:26:57.351342: step 47, loss 0.687959.
Train: 2018-08-06T00:26:57.626600: step 48, loss 0.652977.
Train: 2018-08-06T00:26:57.906826: step 49, loss 0.64137.
Train: 2018-08-06T00:26:58.188099: step 50, loss 0.593728.
Test: 2018-08-06T00:26:59.446708: step 50, loss 0.657075.
Train: 2018-08-06T00:26:59.711001: step 51, loss 0.521836.
Train: 2018-08-06T00:26:59.987286: step 52, loss 0.585669.
Train: 2018-08-06T00:27:00.256542: step 53, loss 0.58396.
Train: 2018-08-06T00:27:00.532804: step 54, loss 0.579073.
Train: 2018-08-06T00:27:00.818039: step 55, loss 0.627954.
Train: 2018-08-06T00:27:01.090326: step 56, loss 0.517259.
Train: 2018-08-06T00:27:01.365607: step 57, loss 0.586334.
Train: 2018-08-06T00:27:01.638847: step 58, loss 0.530407.
Train: 2018-08-06T00:27:01.907158: step 59, loss 0.584044.
Train: 2018-08-06T00:27:02.186407: step 60, loss 0.538798.
Test: 2018-08-06T00:27:03.453990: step 60, loss 0.598975.
Train: 2018-08-06T00:27:03.720280: step 61, loss 0.660659.
Train: 2018-08-06T00:27:03.997537: step 62, loss 0.57737.
Train: 2018-08-06T00:27:04.267842: step 63, loss 0.560877.
Train: 2018-08-06T00:27:04.539100: step 64, loss 0.581262.
Train: 2018-08-06T00:27:04.812357: step 65, loss 0.573726.
Train: 2018-08-06T00:27:05.090644: step 66, loss 0.597804.
Train: 2018-08-06T00:27:05.368871: step 67, loss 0.566628.
Train: 2018-08-06T00:27:05.643169: step 68, loss 0.598123.
Train: 2018-08-06T00:27:05.914444: step 69, loss 0.616997.
Train: 2018-08-06T00:27:06.206661: step 70, loss 0.526419.
Test: 2018-08-06T00:27:07.471248: step 70, loss 0.586846.
Train: 2018-08-06T00:27:07.736543: step 71, loss 0.615195.
Train: 2018-08-06T00:27:08.015797: step 72, loss 0.561007.
Train: 2018-08-06T00:27:08.289098: step 73, loss 0.535561.
Train: 2018-08-06T00:27:08.561369: step 74, loss 0.595984.
Train: 2018-08-06T00:27:08.840591: step 75, loss 0.644495.
Train: 2018-08-06T00:27:09.107877: step 76, loss 0.611748.
Train: 2018-08-06T00:27:09.381146: step 77, loss 0.604624.
Train: 2018-08-06T00:27:09.650426: step 78, loss 0.521854.
Train: 2018-08-06T00:27:09.922722: step 79, loss 0.558208.
Train: 2018-08-06T00:27:10.196990: step 80, loss 0.549795.
Test: 2018-08-06T00:27:11.462578: step 80, loss 0.574531.
Train: 2018-08-06T00:27:11.729864: step 81, loss 0.583609.
Train: 2018-08-06T00:27:12.003133: step 82, loss 0.587315.
Train: 2018-08-06T00:27:12.276403: step 83, loss 0.583052.
Train: 2018-08-06T00:27:12.557683: step 84, loss 0.546782.
Train: 2018-08-06T00:27:12.832926: step 85, loss 0.556101.
Train: 2018-08-06T00:27:13.106209: step 86, loss 0.518145.
Train: 2018-08-06T00:27:13.382471: step 87, loss 0.623225.
Train: 2018-08-06T00:27:13.655742: step 88, loss 0.61122.
Train: 2018-08-06T00:27:13.929982: step 89, loss 0.561524.
Train: 2018-08-06T00:27:14.200283: step 90, loss 0.567507.
Test: 2018-08-06T00:27:15.486816: step 90, loss 0.5668.
Train: 2018-08-06T00:27:15.750139: step 91, loss 0.610887.
Train: 2018-08-06T00:27:16.023381: step 92, loss 0.556073.
Train: 2018-08-06T00:27:16.304657: step 93, loss 0.611557.
Train: 2018-08-06T00:27:16.577929: step 94, loss 0.652261.
Train: 2018-08-06T00:27:16.851169: step 95, loss 0.529372.
Train: 2018-08-06T00:27:17.126431: step 96, loss 0.566022.
Train: 2018-08-06T00:27:17.398705: step 97, loss 0.574695.
Train: 2018-08-06T00:27:17.672994: step 98, loss 0.55088.
Train: 2018-08-06T00:27:17.950228: step 99, loss 0.58207.
Train: 2018-08-06T00:27:18.221530: step 100, loss 0.553362.
Test: 2018-08-06T00:27:19.479140: step 100, loss 0.556447.
Train: 2018-08-06T00:27:20.497791: step 101, loss 0.553174.
Train: 2018-08-06T00:27:20.775018: step 102, loss 0.474254.
Train: 2018-08-06T00:27:21.052308: step 103, loss 0.544855.
Train: 2018-08-06T00:27:21.324580: step 104, loss 0.580721.
Train: 2018-08-06T00:27:21.598817: step 105, loss 0.537504.
Train: 2018-08-06T00:27:21.867099: step 106, loss 0.547422.
Train: 2018-08-06T00:27:22.141398: step 107, loss 0.549173.
Train: 2018-08-06T00:27:22.420648: step 108, loss 0.546984.
Train: 2018-08-06T00:27:22.694884: step 109, loss 0.635312.
Train: 2018-08-06T00:27:22.963198: step 110, loss 0.559684.
Test: 2018-08-06T00:27:24.238755: step 110, loss 0.55657.
Train: 2018-08-06T00:27:24.498088: step 111, loss 0.580334.
Train: 2018-08-06T00:27:24.774368: step 112, loss 0.602517.
Train: 2018-08-06T00:27:25.062581: step 113, loss 0.544457.
Train: 2018-08-06T00:27:25.336837: step 114, loss 0.653232.
Train: 2018-08-06T00:27:25.621059: step 115, loss 0.583814.
Train: 2018-08-06T00:27:25.891367: step 116, loss 0.561865.
Train: 2018-08-06T00:27:26.169623: step 117, loss 0.599861.
Train: 2018-08-06T00:27:26.447848: step 118, loss 0.580357.
Train: 2018-08-06T00:27:26.722115: step 119, loss 0.628701.
Train: 2018-08-06T00:27:27.007382: step 120, loss 0.537199.
Test: 2018-08-06T00:27:28.303884: step 120, loss 0.566595.
Train: 2018-08-06T00:27:28.575159: step 121, loss 0.563199.
Train: 2018-08-06T00:27:28.858402: step 122, loss 0.603467.
Train: 2018-08-06T00:27:29.141643: step 123, loss 0.500504.
Train: 2018-08-06T00:27:29.417931: step 124, loss 0.58288.
Train: 2018-08-06T00:27:29.693169: step 125, loss 0.542141.
Train: 2018-08-06T00:27:29.969458: step 126, loss 0.507377.
Train: 2018-08-06T00:27:30.244694: step 127, loss 0.622684.
Train: 2018-08-06T00:27:30.517989: step 128, loss 0.515714.
Train: 2018-08-06T00:27:30.796218: step 129, loss 0.494136.
Train: 2018-08-06T00:27:31.073507: step 130, loss 0.547123.
Test: 2018-08-06T00:27:32.334105: step 130, loss 0.553793.
Train: 2018-08-06T00:27:32.600419: step 131, loss 0.519887.
Train: 2018-08-06T00:27:32.885656: step 132, loss 0.614429.
Train: 2018-08-06T00:27:33.170878: step 133, loss 0.586562.
Train: 2018-08-06T00:27:33.453140: step 134, loss 0.55151.
Train: 2018-08-06T00:27:33.733394: step 135, loss 0.599931.
Train: 2018-08-06T00:27:34.010624: step 136, loss 0.559535.
Train: 2018-08-06T00:27:34.282927: step 137, loss 0.560155.
Train: 2018-08-06T00:27:34.556194: step 138, loss 0.601056.
Train: 2018-08-06T00:27:34.840434: step 139, loss 0.55378.
Train: 2018-08-06T00:27:35.126638: step 140, loss 0.591473.
Test: 2018-08-06T00:27:36.392253: step 140, loss 0.555603.
Train: 2018-08-06T00:27:36.659538: step 141, loss 0.603547.
Train: 2018-08-06T00:27:36.935825: step 142, loss 0.560817.
Train: 2018-08-06T00:27:37.218044: step 143, loss 0.527984.
Train: 2018-08-06T00:27:37.489345: step 144, loss 0.525361.
Train: 2018-08-06T00:27:37.760594: step 145, loss 0.51143.
Train: 2018-08-06T00:27:38.031899: step 146, loss 0.584008.
Train: 2018-08-06T00:27:38.311120: step 147, loss 0.538488.
Train: 2018-08-06T00:27:38.581399: step 148, loss 0.503272.
Train: 2018-08-06T00:27:38.859679: step 149, loss 0.58275.
Train: 2018-08-06T00:27:39.134949: step 150, loss 0.578419.
Test: 2018-08-06T00:27:40.392554: step 150, loss 0.547579.
Train: 2018-08-06T00:27:40.824400: step 151, loss 0.45884.
Train: 2018-08-06T00:27:41.107671: step 152, loss 0.556991.
Train: 2018-08-06T00:27:41.388931: step 153, loss 0.634002.
Train: 2018-08-06T00:27:41.665183: step 154, loss 0.5774.
Train: 2018-08-06T00:27:41.939444: step 155, loss 0.559828.
Train: 2018-08-06T00:27:42.217674: step 156, loss 0.584253.
Train: 2018-08-06T00:27:42.501913: step 157, loss 0.565804.
Train: 2018-08-06T00:27:42.779172: step 158, loss 0.548114.
Train: 2018-08-06T00:27:43.060451: step 159, loss 0.530092.
Train: 2018-08-06T00:27:43.338709: step 160, loss 0.573948.
Test: 2018-08-06T00:27:44.610275: step 160, loss 0.551142.
Train: 2018-08-06T00:27:44.879586: step 161, loss 0.598541.
Train: 2018-08-06T00:27:45.153847: step 162, loss 0.547766.
Train: 2018-08-06T00:27:45.428123: step 163, loss 0.5732.
Train: 2018-08-06T00:27:45.719310: step 164, loss 0.56827.
Train: 2018-08-06T00:27:46.005569: step 165, loss 0.593956.
Train: 2018-08-06T00:27:46.281831: step 166, loss 0.632969.
Train: 2018-08-06T00:27:46.561058: step 167, loss 0.597104.
Train: 2018-08-06T00:27:46.851283: step 168, loss 0.608879.
Train: 2018-08-06T00:27:47.124551: step 169, loss 0.608584.
Train: 2018-08-06T00:27:47.409788: step 170, loss 0.626466.
Test: 2018-08-06T00:27:48.676401: step 170, loss 0.55481.
Train: 2018-08-06T00:27:48.945706: step 171, loss 0.583231.
Train: 2018-08-06T00:27:49.224964: step 172, loss 0.591653.
Train: 2018-08-06T00:27:49.504188: step 173, loss 0.587596.
Train: 2018-08-06T00:27:49.780450: step 174, loss 0.592216.
Train: 2018-08-06T00:27:50.055712: step 175, loss 0.555417.
Train: 2018-08-06T00:27:50.329012: step 176, loss 0.583274.
Train: 2018-08-06T00:27:50.617237: step 177, loss 0.550653.
Train: 2018-08-06T00:27:50.894502: step 178, loss 0.546762.
Train: 2018-08-06T00:27:51.168764: step 179, loss 0.562843.
Train: 2018-08-06T00:27:51.443037: step 180, loss 0.507521.
Test: 2018-08-06T00:27:52.711609: step 180, loss 0.553657.
Train: 2018-08-06T00:27:52.980889: step 181, loss 0.572402.
Train: 2018-08-06T00:27:53.256153: step 182, loss 0.550397.
Train: 2018-08-06T00:27:53.543418: step 183, loss 0.549698.
Train: 2018-08-06T00:27:53.832612: step 184, loss 0.612533.
Train: 2018-08-06T00:27:54.112891: step 185, loss 0.478277.
Train: 2018-08-06T00:27:54.394110: step 186, loss 0.524246.
Train: 2018-08-06T00:27:54.669398: step 187, loss 0.523692.
Train: 2018-08-06T00:27:54.954611: step 188, loss 0.477486.
Train: 2018-08-06T00:27:55.232868: step 189, loss 0.622072.
Train: 2018-08-06T00:27:55.505169: step 190, loss 0.551476.
Test: 2018-08-06T00:27:56.777735: step 190, loss 0.547756.
Train: 2018-08-06T00:27:57.044022: step 191, loss 0.598945.
Train: 2018-08-06T00:27:57.318324: step 192, loss 0.524025.
Train: 2018-08-06T00:27:57.591609: step 193, loss 0.594387.
Train: 2018-08-06T00:27:57.869845: step 194, loss 0.50781.
Train: 2018-08-06T00:27:58.159042: step 195, loss 0.529491.
Train: 2018-08-06T00:27:58.432310: step 196, loss 0.554796.
Train: 2018-08-06T00:27:58.709572: step 197, loss 0.636219.
Train: 2018-08-06T00:27:58.980854: step 198, loss 0.647791.
Train: 2018-08-06T00:27:59.251149: step 199, loss 0.583965.
Train: 2018-08-06T00:27:59.524416: step 200, loss 0.54236.
Test: 2018-08-06T00:28:00.789008: step 200, loss 0.547423.
Train: 2018-08-06T00:28:01.697755: step 201, loss 0.565125.
Train: 2018-08-06T00:28:01.978973: step 202, loss 0.5678.
Train: 2018-08-06T00:28:02.256238: step 203, loss 0.630674.
Train: 2018-08-06T00:28:02.531495: step 204, loss 0.621932.
Train: 2018-08-06T00:28:02.809783: step 205, loss 0.599621.
Train: 2018-08-06T00:28:03.084048: step 206, loss 0.54677.
Train: 2018-08-06T00:28:03.358314: step 207, loss 0.595177.
Train: 2018-08-06T00:28:03.648539: step 208, loss 0.52814.
Train: 2018-08-06T00:28:03.931749: step 209, loss 0.574127.
Train: 2018-08-06T00:28:04.210011: step 210, loss 0.561178.
Test: 2018-08-06T00:28:05.466644: step 210, loss 0.553998.
Train: 2018-08-06T00:28:05.739932: step 211, loss 0.569409.
Train: 2018-08-06T00:28:06.021161: step 212, loss 0.546827.
Train: 2018-08-06T00:28:06.296427: step 213, loss 0.555938.
Train: 2018-08-06T00:28:06.581689: step 214, loss 0.520317.
Train: 2018-08-06T00:28:06.858923: step 215, loss 0.549711.
Train: 2018-08-06T00:28:07.137178: step 216, loss 0.552172.
Train: 2018-08-06T00:28:07.410447: step 217, loss 0.597437.
Train: 2018-08-06T00:28:07.679726: step 218, loss 0.573945.
Train: 2018-08-06T00:28:07.955997: step 219, loss 0.540167.
Train: 2018-08-06T00:28:08.233271: step 220, loss 0.500994.
Test: 2018-08-06T00:28:09.481907: step 220, loss 0.549624.
Train: 2018-08-06T00:28:09.745203: step 221, loss 0.556626.
Train: 2018-08-06T00:28:10.014483: step 222, loss 0.545098.
Train: 2018-08-06T00:28:10.287784: step 223, loss 0.495618.
Train: 2018-08-06T00:28:10.555038: step 224, loss 0.559996.
Train: 2018-08-06T00:28:10.832296: step 225, loss 0.638415.
Train: 2018-08-06T00:28:11.106590: step 226, loss 0.53858.
Train: 2018-08-06T00:28:11.394791: step 227, loss 0.56844.
Train: 2018-08-06T00:28:11.669089: step 228, loss 0.539735.
Train: 2018-08-06T00:28:11.938338: step 229, loss 0.610476.
Train: 2018-08-06T00:28:12.214625: step 230, loss 0.597083.
Test: 2018-08-06T00:28:13.483206: step 230, loss 0.547856.
Train: 2018-08-06T00:28:13.746533: step 231, loss 0.601204.
Train: 2018-08-06T00:28:14.031739: step 232, loss 0.60659.
Train: 2018-08-06T00:28:14.313984: step 233, loss 0.549962.
Train: 2018-08-06T00:28:14.583264: step 234, loss 0.649948.
Train: 2018-08-06T00:28:14.858528: step 235, loss 0.558142.
Train: 2018-08-06T00:28:15.130818: step 236, loss 0.568803.
Train: 2018-08-06T00:28:15.407089: step 237, loss 0.60587.
Train: 2018-08-06T00:28:15.687312: step 238, loss 0.505142.
Train: 2018-08-06T00:28:15.987540: step 239, loss 0.570096.
Train: 2018-08-06T00:28:16.270783: step 240, loss 0.548012.
Test: 2018-08-06T00:28:17.520440: step 240, loss 0.549957.
Train: 2018-08-06T00:28:17.795705: step 241, loss 0.561402.
Train: 2018-08-06T00:28:18.070967: step 242, loss 0.631593.
Train: 2018-08-06T00:28:18.357203: step 243, loss 0.627876.
Train: 2018-08-06T00:28:18.638451: step 244, loss 0.539946.
Train: 2018-08-06T00:28:18.908727: step 245, loss 0.544806.
Train: 2018-08-06T00:28:19.185987: step 246, loss 0.503303.
Train: 2018-08-06T00:28:19.474216: step 247, loss 0.528589.
Train: 2018-08-06T00:28:19.750478: step 248, loss 0.568782.
Train: 2018-08-06T00:28:20.026738: step 249, loss 0.579535.
Train: 2018-08-06T00:28:20.299038: step 250, loss 0.59557.
Test: 2018-08-06T00:28:21.580582: step 250, loss 0.55.
Train: 2018-08-06T00:28:21.844901: step 251, loss 0.600627.
Train: 2018-08-06T00:28:22.117178: step 252, loss 0.515142.
Train: 2018-08-06T00:28:22.406375: step 253, loss 0.52957.
Train: 2018-08-06T00:28:22.694604: step 254, loss 0.532876.
Train: 2018-08-06T00:28:22.969868: step 255, loss 0.55268.
Train: 2018-08-06T00:28:23.248123: step 256, loss 0.590661.
Train: 2018-08-06T00:28:23.522390: step 257, loss 0.524908.
Train: 2018-08-06T00:28:23.796681: step 258, loss 0.540428.
Train: 2018-08-06T00:28:24.072917: step 259, loss 0.591283.
Train: 2018-08-06T00:28:24.344192: step 260, loss 0.523328.
Test: 2018-08-06T00:28:25.614793: step 260, loss 0.547771.
Train: 2018-08-06T00:28:25.882104: step 261, loss 0.567683.
Train: 2018-08-06T00:28:26.152364: step 262, loss 0.621052.
Train: 2018-08-06T00:28:26.442580: step 263, loss 0.537925.
Train: 2018-08-06T00:28:26.719869: step 264, loss 0.586426.
Train: 2018-08-06T00:28:26.990116: step 265, loss 0.575767.
Train: 2018-08-06T00:28:27.274389: step 266, loss 0.463549.
Train: 2018-08-06T00:28:27.550647: step 267, loss 0.592133.
Train: 2018-08-06T00:28:27.838879: step 268, loss 0.573739.
Train: 2018-08-06T00:28:28.118100: step 269, loss 0.534898.
Train: 2018-08-06T00:28:28.409346: step 270, loss 0.504952.
Test: 2018-08-06T00:28:29.678924: step 270, loss 0.549249.
Train: 2018-08-06T00:28:29.944246: step 271, loss 0.568414.
Train: 2018-08-06T00:28:30.220476: step 272, loss 0.62479.
Train: 2018-08-06T00:28:30.494773: step 273, loss 0.607181.
Train: 2018-08-06T00:28:30.777014: step 274, loss 0.608246.
Train: 2018-08-06T00:28:31.053250: step 275, loss 0.498793.
Train: 2018-08-06T00:28:31.341511: step 276, loss 0.543672.
Train: 2018-08-06T00:28:31.617767: step 277, loss 0.56732.
Train: 2018-08-06T00:28:31.897991: step 278, loss 0.547435.
Train: 2018-08-06T00:28:32.237140: step 279, loss 0.539132.
Train: 2018-08-06T00:28:32.507389: step 280, loss 0.562979.
Test: 2018-08-06T00:28:33.779982: step 280, loss 0.548221.
Train: 2018-08-06T00:28:34.111122: step 281, loss 0.551295.
Train: 2018-08-06T00:28:34.387386: step 282, loss 0.569736.
Train: 2018-08-06T00:28:34.657644: step 283, loss 0.539504.
Train: 2018-08-06T00:28:34.930906: step 284, loss 0.582851.
Train: 2018-08-06T00:28:35.208164: step 285, loss 0.514797.
Train: 2018-08-06T00:28:35.481463: step 286, loss 0.582462.
Train: 2018-08-06T00:28:35.771661: step 287, loss 0.518881.
Train: 2018-08-06T00:28:36.054899: step 288, loss 0.506535.
Train: 2018-08-06T00:28:36.329165: step 289, loss 0.587176.
Train: 2018-08-06T00:28:36.608419: step 290, loss 0.591795.
Test: 2018-08-06T00:28:37.900961: step 290, loss 0.548845.
Train: 2018-08-06T00:28:38.175229: step 291, loss 0.563004.
Train: 2018-08-06T00:28:38.456505: step 292, loss 0.56803.
Train: 2018-08-06T00:28:38.743736: step 293, loss 0.565783.
Train: 2018-08-06T00:28:39.022962: step 294, loss 0.529313.
Train: 2018-08-06T00:28:39.296261: step 295, loss 0.542387.
Train: 2018-08-06T00:28:39.572522: step 296, loss 0.51886.
Train: 2018-08-06T00:28:39.851770: step 297, loss 0.592574.
Train: 2018-08-06T00:28:40.128006: step 298, loss 0.44131.
Train: 2018-08-06T00:28:40.401306: step 299, loss 0.561377.
Train: 2018-08-06T00:28:40.672550: step 300, loss 0.565145.
Test: 2018-08-06T00:28:41.940159: step 300, loss 0.549477.
Train: 2018-08-06T00:28:42.925382: step 301, loss 0.477782.
Train: 2018-08-06T00:28:43.150790: step 302, loss 0.587059.
Train: 2018-08-06T00:28:43.424073: step 303, loss 0.496404.
Train: 2018-08-06T00:28:43.695352: step 304, loss 0.58556.
Train: 2018-08-06T00:28:43.976601: step 305, loss 0.568079.
Train: 2018-08-06T00:28:44.256854: step 306, loss 0.544766.
Train: 2018-08-06T00:28:44.542058: step 307, loss 0.65053.
Train: 2018-08-06T00:28:44.817321: step 308, loss 0.570357.
Train: 2018-08-06T00:28:45.092618: step 309, loss 0.539619.
Train: 2018-08-06T00:28:45.364883: step 310, loss 0.645005.
Test: 2018-08-06T00:28:46.623491: step 310, loss 0.547567.
Train: 2018-08-06T00:28:46.885823: step 311, loss 0.520512.
Train: 2018-08-06T00:28:47.170058: step 312, loss 0.607848.
Train: 2018-08-06T00:28:47.443298: step 313, loss 0.578918.
Train: 2018-08-06T00:28:47.718561: step 314, loss 0.573716.
Train: 2018-08-06T00:28:47.996818: step 315, loss 0.667028.
Train: 2018-08-06T00:28:48.280088: step 316, loss 0.492055.
Train: 2018-08-06T00:28:48.554359: step 317, loss 0.522645.
Train: 2018-08-06T00:28:48.827597: step 318, loss 0.568463.
Train: 2018-08-06T00:28:49.114859: step 319, loss 0.551973.
Train: 2018-08-06T00:28:49.393110: step 320, loss 0.524649.
Test: 2018-08-06T00:28:50.683632: step 320, loss 0.549589.
Train: 2018-08-06T00:28:50.945931: step 321, loss 0.548623.
Train: 2018-08-06T00:28:51.219237: step 322, loss 0.554585.
Train: 2018-08-06T00:28:51.494493: step 323, loss 0.531819.
Train: 2018-08-06T00:28:51.766736: step 324, loss 0.623078.
Train: 2018-08-06T00:28:52.043994: step 325, loss 0.599311.
Train: 2018-08-06T00:28:52.330229: step 326, loss 0.507227.
Train: 2018-08-06T00:28:52.602500: step 327, loss 0.530872.
Train: 2018-08-06T00:28:52.895743: step 328, loss 0.602148.
Train: 2018-08-06T00:28:53.168014: step 329, loss 0.570042.
Train: 2018-08-06T00:28:53.442258: step 330, loss 0.578787.
Test: 2018-08-06T00:28:54.714851: step 330, loss 0.549698.
Train: 2018-08-06T00:28:54.984165: step 331, loss 0.614661.
Train: 2018-08-06T00:28:55.259421: step 332, loss 0.545542.
Train: 2018-08-06T00:28:55.536689: step 333, loss 0.547936.
Train: 2018-08-06T00:28:55.809954: step 334, loss 0.541359.
Train: 2018-08-06T00:28:56.085187: step 335, loss 0.522577.
Train: 2018-08-06T00:28:56.378433: step 336, loss 0.527351.
Train: 2018-08-06T00:28:56.651735: step 337, loss 0.608201.
Train: 2018-08-06T00:28:56.929961: step 338, loss 0.624275.
Train: 2018-08-06T00:28:57.205218: step 339, loss 0.583439.
Train: 2018-08-06T00:28:57.483474: step 340, loss 0.58164.
Test: 2018-08-06T00:28:58.745073: step 340, loss 0.549237.
Train: 2018-08-06T00:28:59.011403: step 341, loss 0.622073.
Train: 2018-08-06T00:28:59.283664: step 342, loss 0.605681.
Train: 2018-08-06T00:28:59.567879: step 343, loss 0.54642.
Train: 2018-08-06T00:28:59.847156: step 344, loss 0.56472.
Train: 2018-08-06T00:29:00.122389: step 345, loss 0.560466.
Train: 2018-08-06T00:29:00.401642: step 346, loss 0.484846.
Train: 2018-08-06T00:29:00.674948: step 347, loss 0.54027.
Train: 2018-08-06T00:29:00.949207: step 348, loss 0.532085.
Train: 2018-08-06T00:29:01.220454: step 349, loss 0.526017.
Train: 2018-08-06T00:29:01.490757: step 350, loss 0.630291.
Test: 2018-08-06T00:29:02.738394: step 350, loss 0.549844.
Train: 2018-08-06T00:29:03.000718: step 351, loss 0.51068.
Train: 2018-08-06T00:29:03.285955: step 352, loss 0.563773.
Train: 2018-08-06T00:29:03.557205: step 353, loss 0.492392.
Train: 2018-08-06T00:29:03.831502: step 354, loss 0.582176.
Train: 2018-08-06T00:29:04.119701: step 355, loss 0.563875.
Train: 2018-08-06T00:29:04.393000: step 356, loss 0.501531.
Train: 2018-08-06T00:29:04.669256: step 357, loss 0.548867.
Train: 2018-08-06T00:29:04.941531: step 358, loss 0.523931.
Train: 2018-08-06T00:29:05.213799: step 359, loss 0.48993.
Train: 2018-08-06T00:29:05.484080: step 360, loss 0.620032.
Test: 2018-08-06T00:29:06.747672: step 360, loss 0.548678.
Train: 2018-08-06T00:29:07.010968: step 361, loss 0.567783.
Train: 2018-08-06T00:29:07.293243: step 362, loss 0.54741.
Train: 2018-08-06T00:29:07.582453: step 363, loss 0.497466.
Train: 2018-08-06T00:29:07.861727: step 364, loss 0.565451.
Train: 2018-08-06T00:29:08.137982: step 365, loss 0.505748.
Train: 2018-08-06T00:29:08.416236: step 366, loss 0.523428.
Train: 2018-08-06T00:29:08.691474: step 367, loss 0.464041.
Train: 2018-08-06T00:29:08.969729: step 368, loss 0.472911.
Train: 2018-08-06T00:29:09.238038: step 369, loss 0.527431.
Train: 2018-08-06T00:29:09.511282: step 370, loss 0.553293.
Test: 2018-08-06T00:29:10.802827: step 370, loss 0.548251.
Train: 2018-08-06T00:29:11.067121: step 371, loss 0.631142.
Train: 2018-08-06T00:29:11.337427: step 372, loss 0.534409.
Train: 2018-08-06T00:29:11.606703: step 373, loss 0.515366.
Train: 2018-08-06T00:29:11.878980: step 374, loss 0.598313.
Train: 2018-08-06T00:29:12.157236: step 375, loss 0.619201.
Train: 2018-08-06T00:29:12.428480: step 376, loss 0.50221.
Train: 2018-08-06T00:29:12.697758: step 377, loss 0.498758.
Train: 2018-08-06T00:29:12.972026: step 378, loss 0.582376.
Train: 2018-08-06T00:29:13.246323: step 379, loss 0.524919.
Train: 2018-08-06T00:29:13.519562: step 380, loss 0.636199.
Test: 2018-08-06T00:29:14.776200: step 380, loss 0.547542.
Train: 2018-08-06T00:29:15.033513: step 381, loss 0.528986.
Train: 2018-08-06T00:29:15.308777: step 382, loss 0.556004.
Train: 2018-08-06T00:29:15.578088: step 383, loss 0.537687.
Train: 2018-08-06T00:29:15.847337: step 384, loss 0.580016.
Train: 2018-08-06T00:29:16.115619: step 385, loss 0.555699.
Train: 2018-08-06T00:29:16.390923: step 386, loss 0.503501.
Train: 2018-08-06T00:29:16.653232: step 387, loss 0.569959.
Train: 2018-08-06T00:29:16.920466: step 388, loss 0.472546.
Train: 2018-08-06T00:29:17.201740: step 389, loss 0.577652.
Train: 2018-08-06T00:29:17.480978: step 390, loss 0.614049.
Test: 2018-08-06T00:29:18.744588: step 390, loss 0.54733.
Train: 2018-08-06T00:29:19.005890: step 391, loss 0.57666.
Train: 2018-08-06T00:29:19.272224: step 392, loss 0.53771.
Train: 2018-08-06T00:29:19.532481: step 393, loss 0.701078.
Train: 2018-08-06T00:29:19.793782: step 394, loss 0.512534.
Train: 2018-08-06T00:29:20.065083: step 395, loss 0.52202.
Train: 2018-08-06T00:29:20.327355: step 396, loss 0.531939.
Train: 2018-08-06T00:29:20.590676: step 397, loss 0.563438.
Train: 2018-08-06T00:29:20.859930: step 398, loss 0.546432.
Train: 2018-08-06T00:29:21.124224: step 399, loss 0.623907.
Train: 2018-08-06T00:29:21.387521: step 400, loss 0.566394.
Test: 2018-08-06T00:29:22.645155: step 400, loss 0.549876.
Train: 2018-08-06T00:29:23.550691: step 401, loss 0.580663.
Train: 2018-08-06T00:29:23.817009: step 402, loss 0.539893.
Train: 2018-08-06T00:29:24.075318: step 403, loss 0.57975.
Train: 2018-08-06T00:29:24.336617: step 404, loss 0.60909.
Train: 2018-08-06T00:29:24.593931: step 405, loss 0.53251.
Train: 2018-08-06T00:29:24.877143: step 406, loss 0.592307.
Train: 2018-08-06T00:29:25.150436: step 407, loss 0.550246.
Train: 2018-08-06T00:29:25.412741: step 408, loss 0.610762.
Train: 2018-08-06T00:29:25.674012: step 409, loss 0.507122.
Train: 2018-08-06T00:29:25.936330: step 410, loss 0.549006.
Test: 2018-08-06T00:29:27.198933: step 410, loss 0.551184.
Train: 2018-08-06T00:29:27.450261: step 411, loss 0.571916.
Train: 2018-08-06T00:29:27.709568: step 412, loss 0.555013.
Train: 2018-08-06T00:29:27.966879: step 413, loss 0.627991.
Train: 2018-08-06T00:29:28.226187: step 414, loss 0.582434.
Train: 2018-08-06T00:29:28.481526: step 415, loss 0.560672.
Train: 2018-08-06T00:29:28.749834: step 416, loss 0.595887.
Train: 2018-08-06T00:29:29.016074: step 417, loss 0.642321.
Train: 2018-08-06T00:29:29.287354: step 418, loss 0.541172.
Train: 2018-08-06T00:29:29.543688: step 419, loss 0.562579.
Train: 2018-08-06T00:29:29.802976: step 420, loss 0.540654.
Test: 2018-08-06T00:29:31.079554: step 420, loss 0.550431.
Train: 2018-08-06T00:29:31.325927: step 421, loss 0.587228.
Train: 2018-08-06T00:29:31.587231: step 422, loss 0.528.
Train: 2018-08-06T00:29:31.854482: step 423, loss 0.550348.
Train: 2018-08-06T00:29:32.113789: step 424, loss 0.579223.
Train: 2018-08-06T00:29:32.368140: step 425, loss 0.554159.
Train: 2018-08-06T00:29:32.619469: step 426, loss 0.589149.
Train: 2018-08-06T00:29:32.869792: step 427, loss 0.555902.
Train: 2018-08-06T00:29:33.130072: step 428, loss 0.525985.
Train: 2018-08-06T00:29:33.389378: step 429, loss 0.556928.
Train: 2018-08-06T00:29:33.643698: step 430, loss 0.554089.
Test: 2018-08-06T00:29:34.902332: step 430, loss 0.548966.
Train: 2018-08-06T00:29:35.150693: step 431, loss 0.530845.
Train: 2018-08-06T00:29:35.400998: step 432, loss 0.562246.
Train: 2018-08-06T00:29:35.647370: step 433, loss 0.594276.
Train: 2018-08-06T00:29:35.893680: step 434, loss 0.603791.
Train: 2018-08-06T00:29:36.145039: step 435, loss 0.556194.
Train: 2018-08-06T00:29:36.401323: step 436, loss 0.579075.
Train: 2018-08-06T00:29:36.647664: step 437, loss 0.584782.
Train: 2018-08-06T00:29:36.899990: step 438, loss 0.57089.
Train: 2018-08-06T00:29:37.147327: step 439, loss 0.56319.
Train: 2018-08-06T00:29:37.397659: step 440, loss 0.685645.
Test: 2018-08-06T00:29:38.663273: step 440, loss 0.550752.
Train: 2018-08-06T00:29:38.902634: step 441, loss 0.531648.
Train: 2018-08-06T00:29:39.155981: step 442, loss 0.594365.
Train: 2018-08-06T00:29:39.407309: step 443, loss 0.563134.
Train: 2018-08-06T00:29:39.659625: step 444, loss 0.540597.
Train: 2018-08-06T00:29:39.921908: step 445, loss 0.595198.
Train: 2018-08-06T00:29:40.174263: step 446, loss 0.580529.
Train: 2018-08-06T00:29:40.427588: step 447, loss 0.531503.
Train: 2018-08-06T00:29:40.670905: step 448, loss 0.643273.
Train: 2018-08-06T00:29:40.916273: step 449, loss 0.529981.
Train: 2018-08-06T00:29:41.160595: step 450, loss 0.623175.
Test: 2018-08-06T00:29:42.409256: step 450, loss 0.549064.
Train: 2018-08-06T00:29:42.645649: step 451, loss 0.562391.
Train: 2018-08-06T00:29:42.904930: step 452, loss 0.632757.
Train: 2018-08-06T00:29:43.099410: step 453, loss 0.499217.
Train: 2018-08-06T00:29:43.342790: step 454, loss 0.548618.
Train: 2018-08-06T00:29:43.605057: step 455, loss 0.568353.
Train: 2018-08-06T00:29:43.843446: step 456, loss 0.601012.
Train: 2018-08-06T00:29:44.091782: step 457, loss 0.541525.
Train: 2018-08-06T00:29:44.343083: step 458, loss 0.580502.
Train: 2018-08-06T00:29:44.593415: step 459, loss 0.531723.
Train: 2018-08-06T00:29:44.850726: step 460, loss 0.510602.
Test: 2018-08-06T00:29:46.121328: step 460, loss 0.549333.
Train: 2018-08-06T00:29:46.356729: step 461, loss 0.531719.
Train: 2018-08-06T00:29:46.617033: step 462, loss 0.517563.
Train: 2018-08-06T00:29:46.872351: step 463, loss 0.611061.
Train: 2018-08-06T00:29:47.127636: step 464, loss 0.473195.
Train: 2018-08-06T00:29:47.375006: step 465, loss 0.595998.
Train: 2018-08-06T00:29:47.625306: step 466, loss 0.603021.
Train: 2018-08-06T00:29:47.872645: step 467, loss 0.561084.
Train: 2018-08-06T00:29:48.117989: step 468, loss 0.549339.
Train: 2018-08-06T00:29:48.359368: step 469, loss 0.58851.
Train: 2018-08-06T00:29:48.605713: step 470, loss 0.594959.
Test: 2018-08-06T00:29:49.867310: step 470, loss 0.548037.
Train: 2018-08-06T00:29:50.100717: step 471, loss 0.588506.
Train: 2018-08-06T00:29:50.346060: step 472, loss 0.622321.
Train: 2018-08-06T00:29:50.592404: step 473, loss 0.598333.
Train: 2018-08-06T00:29:50.834747: step 474, loss 0.479929.
Train: 2018-08-06T00:29:51.078072: step 475, loss 0.532697.
Train: 2018-08-06T00:29:51.332392: step 476, loss 0.586506.
Train: 2018-08-06T00:29:51.585715: step 477, loss 0.487926.
Train: 2018-08-06T00:29:51.829093: step 478, loss 0.487724.
Train: 2018-08-06T00:29:52.073441: step 479, loss 0.561278.
Train: 2018-08-06T00:29:52.325736: step 480, loss 0.537275.
Test: 2018-08-06T00:29:53.610300: step 480, loss 0.547849.
Train: 2018-08-06T00:29:53.847665: step 481, loss 0.538261.
Train: 2018-08-06T00:29:54.103979: step 482, loss 0.563167.
Train: 2018-08-06T00:29:54.352346: step 483, loss 0.598608.
Train: 2018-08-06T00:29:54.602680: step 484, loss 0.579861.
Train: 2018-08-06T00:29:54.848015: step 485, loss 0.554216.
Train: 2018-08-06T00:29:55.096326: step 486, loss 0.563247.
Train: 2018-08-06T00:29:55.340673: step 487, loss 0.560655.
Train: 2018-08-06T00:29:55.596987: step 488, loss 0.555153.
Train: 2018-08-06T00:29:55.853301: step 489, loss 0.659376.
Train: 2018-08-06T00:29:56.100671: step 490, loss 0.613795.
Test: 2018-08-06T00:29:57.361269: step 490, loss 0.548086.
Train: 2018-08-06T00:29:57.594645: step 491, loss 0.558076.
Train: 2018-08-06T00:29:57.839989: step 492, loss 0.535899.
Train: 2018-08-06T00:29:58.085333: step 493, loss 0.563285.
Train: 2018-08-06T00:29:58.329679: step 494, loss 0.555583.
Train: 2018-08-06T00:29:58.575053: step 495, loss 0.548862.
Train: 2018-08-06T00:29:58.824357: step 496, loss 0.574043.
Train: 2018-08-06T00:29:59.083663: step 497, loss 0.532121.
Train: 2018-08-06T00:29:59.334023: step 498, loss 0.521429.
Train: 2018-08-06T00:29:59.584349: step 499, loss 0.556151.
Train: 2018-08-06T00:29:59.832696: step 500, loss 0.514225.
Test: 2018-08-06T00:30:01.084311: step 500, loss 0.548362.
Train: 2018-08-06T00:30:02.047454: step 501, loss 0.531701.
Train: 2018-08-06T00:30:02.294774: step 502, loss 0.52186.
Train: 2018-08-06T00:30:02.537140: step 503, loss 0.554861.
Train: 2018-08-06T00:30:02.780486: step 504, loss 0.545432.
Train: 2018-08-06T00:30:03.025825: step 505, loss 0.595964.
Train: 2018-08-06T00:30:03.271176: step 506, loss 0.636684.
Train: 2018-08-06T00:30:03.531486: step 507, loss 0.563442.
Train: 2018-08-06T00:30:03.777794: step 508, loss 0.569411.
Train: 2018-08-06T00:30:04.024135: step 509, loss 0.571383.
Train: 2018-08-06T00:30:04.274466: step 510, loss 0.537541.
Test: 2018-08-06T00:30:05.533099: step 510, loss 0.549726.
Train: 2018-08-06T00:30:05.767503: step 511, loss 0.562121.
Train: 2018-08-06T00:30:06.011819: step 512, loss 0.53149.
Train: 2018-08-06T00:30:06.257163: step 513, loss 0.571465.
Train: 2018-08-06T00:30:06.502535: step 514, loss 0.530693.
Train: 2018-08-06T00:30:06.748863: step 515, loss 0.604518.
Train: 2018-08-06T00:30:06.994212: step 516, loss 0.637901.
Train: 2018-08-06T00:30:07.251504: step 517, loss 0.605014.
Train: 2018-08-06T00:30:07.497877: step 518, loss 0.545554.
Train: 2018-08-06T00:30:07.738203: step 519, loss 0.539004.
Train: 2018-08-06T00:30:07.986539: step 520, loss 0.570907.
Test: 2018-08-06T00:30:09.244175: step 520, loss 0.548687.
Train: 2018-08-06T00:30:09.485560: step 521, loss 0.564614.
Train: 2018-08-06T00:30:09.735860: step 522, loss 0.554976.
Train: 2018-08-06T00:30:09.986191: step 523, loss 0.548734.
Train: 2018-08-06T00:30:10.234557: step 524, loss 0.58693.
Train: 2018-08-06T00:30:10.492867: step 525, loss 0.562176.
Train: 2018-08-06T00:30:10.734222: step 526, loss 0.620066.
Train: 2018-08-06T00:30:10.975575: step 527, loss 0.516238.
Train: 2018-08-06T00:30:11.221911: step 528, loss 0.548089.
Train: 2018-08-06T00:30:11.471219: step 529, loss 0.658248.
Train: 2018-08-06T00:30:11.715573: step 530, loss 0.585923.
Test: 2018-08-06T00:30:12.977191: step 530, loss 0.550287.
Train: 2018-08-06T00:30:13.211565: step 531, loss 0.601792.
Train: 2018-08-06T00:30:13.452919: step 532, loss 0.50754.
Train: 2018-08-06T00:30:13.696269: step 533, loss 0.562771.
Train: 2018-08-06T00:30:13.943607: step 534, loss 0.524225.
Train: 2018-08-06T00:30:14.189973: step 535, loss 0.594949.
Train: 2018-08-06T00:30:14.437311: step 536, loss 0.500816.
Train: 2018-08-06T00:30:14.679648: step 537, loss 0.571145.
Train: 2018-08-06T00:30:14.918997: step 538, loss 0.58682.
Train: 2018-08-06T00:30:15.175312: step 539, loss 0.586784.
Train: 2018-08-06T00:30:15.418693: step 540, loss 0.563223.
Test: 2018-08-06T00:30:16.679291: step 540, loss 0.550584.
Train: 2018-08-06T00:30:16.914662: step 541, loss 0.562695.
Train: 2018-08-06T00:30:17.162025: step 542, loss 0.552725.
Train: 2018-08-06T00:30:17.409339: step 543, loss 0.579816.
Train: 2018-08-06T00:30:17.658702: step 544, loss 0.626053.
Train: 2018-08-06T00:30:17.909032: step 545, loss 0.507077.
Train: 2018-08-06T00:30:18.160356: step 546, loss 0.635708.
Train: 2018-08-06T00:30:18.416644: step 547, loss 0.594565.
Train: 2018-08-06T00:30:18.656004: step 548, loss 0.562084.
Train: 2018-08-06T00:30:18.898356: step 549, loss 0.492537.
Train: 2018-08-06T00:30:19.138738: step 550, loss 0.562664.
Test: 2018-08-06T00:30:20.395352: step 550, loss 0.550093.
Train: 2018-08-06T00:30:20.628741: step 551, loss 0.516973.
Train: 2018-08-06T00:30:20.879090: step 552, loss 0.562902.
Train: 2018-08-06T00:30:21.125425: step 553, loss 0.563996.
Train: 2018-08-06T00:30:21.383710: step 554, loss 0.498712.
Train: 2018-08-06T00:30:21.628086: step 555, loss 0.554666.
Train: 2018-08-06T00:30:21.886365: step 556, loss 0.562377.
Train: 2018-08-06T00:30:22.129748: step 557, loss 0.513721.
Train: 2018-08-06T00:30:22.376088: step 558, loss 0.554778.
Train: 2018-08-06T00:30:22.622427: step 559, loss 0.571359.
Train: 2018-08-06T00:30:22.864775: step 560, loss 0.513056.
Test: 2018-08-06T00:30:24.129383: step 560, loss 0.54795.
Train: 2018-08-06T00:30:24.382718: step 561, loss 0.560242.
Train: 2018-08-06T00:30:24.637017: step 562, loss 0.521368.
Train: 2018-08-06T00:30:24.885345: step 563, loss 0.63073.
Train: 2018-08-06T00:30:25.127697: step 564, loss 0.56945.
Train: 2018-08-06T00:30:25.369052: step 565, loss 0.562762.
Train: 2018-08-06T00:30:25.622374: step 566, loss 0.613531.
Train: 2018-08-06T00:30:25.867717: step 567, loss 0.587431.
Train: 2018-08-06T00:30:26.119045: step 568, loss 0.613105.
Train: 2018-08-06T00:30:26.373365: step 569, loss 0.55404.
Train: 2018-08-06T00:30:26.617743: step 570, loss 0.611255.
Test: 2018-08-06T00:30:27.878340: step 570, loss 0.54982.
Train: 2018-08-06T00:30:28.113735: step 571, loss 0.537366.
Train: 2018-08-06T00:30:28.366066: step 572, loss 0.620587.
Train: 2018-08-06T00:30:28.625369: step 573, loss 0.547018.
Train: 2018-08-06T00:30:28.867726: step 574, loss 0.554594.
Train: 2018-08-06T00:30:29.112040: step 575, loss 0.529883.
Train: 2018-08-06T00:30:29.354393: step 576, loss 0.570063.
Train: 2018-08-06T00:30:29.603756: step 577, loss 0.628141.
Train: 2018-08-06T00:30:29.849101: step 578, loss 0.513857.
Train: 2018-08-06T00:30:30.094414: step 579, loss 0.539396.
Train: 2018-08-06T00:30:30.337793: step 580, loss 0.529361.
Test: 2018-08-06T00:30:31.596397: step 580, loss 0.549343.
Train: 2018-08-06T00:30:31.841740: step 581, loss 0.563005.
Train: 2018-08-06T00:30:32.080134: step 582, loss 0.545963.
Train: 2018-08-06T00:30:32.344551: step 583, loss 0.530468.
Train: 2018-08-06T00:30:32.598898: step 584, loss 0.578968.
Train: 2018-08-06T00:30:32.850226: step 585, loss 0.5394.
Train: 2018-08-06T00:30:33.101530: step 586, loss 0.612397.
Train: 2018-08-06T00:30:33.344877: step 587, loss 0.522354.
Train: 2018-08-06T00:30:33.587229: step 588, loss 0.547927.
Train: 2018-08-06T00:30:33.832572: step 589, loss 0.511556.
Train: 2018-08-06T00:30:34.085923: step 590, loss 0.487987.
Test: 2018-08-06T00:30:35.348518: step 590, loss 0.54888.
Train: 2018-08-06T00:30:35.644727: step 591, loss 0.560919.
Train: 2018-08-06T00:30:35.896056: step 592, loss 0.535417.
Train: 2018-08-06T00:30:36.153367: step 593, loss 0.521029.
Train: 2018-08-06T00:30:36.398735: step 594, loss 0.61951.
Train: 2018-08-06T00:30:36.641075: step 595, loss 0.53593.
Train: 2018-08-06T00:30:36.881420: step 596, loss 0.511785.
Train: 2018-08-06T00:30:37.121807: step 597, loss 0.604533.
Train: 2018-08-06T00:30:37.364129: step 598, loss 0.569375.
Train: 2018-08-06T00:30:37.622445: step 599, loss 0.640822.
Train: 2018-08-06T00:30:37.865811: step 600, loss 0.571447.
Test: 2018-08-06T00:30:39.131401: step 600, loss 0.547857.
Train: 2018-08-06T00:30:40.061230: step 601, loss 0.561996.
Train: 2018-08-06T00:30:40.305549: step 602, loss 0.698338.
Train: 2018-08-06T00:30:40.551912: step 603, loss 0.55162.
Train: 2018-08-06T00:30:40.739411: step 604, loss 0.564911.
Train: 2018-08-06T00:30:40.986749: step 605, loss 0.5548.
Train: 2018-08-06T00:30:41.243069: step 606, loss 0.552576.
Train: 2018-08-06T00:30:41.490409: step 607, loss 0.562878.
Train: 2018-08-06T00:30:41.736718: step 608, loss 0.531576.
Train: 2018-08-06T00:30:41.977106: step 609, loss 0.586548.
Train: 2018-08-06T00:30:42.219454: step 610, loss 0.644524.
Test: 2018-08-06T00:30:43.486040: step 610, loss 0.548691.
Train: 2018-08-06T00:30:43.723431: step 611, loss 0.587431.
Train: 2018-08-06T00:30:43.964788: step 612, loss 0.580217.
Train: 2018-08-06T00:30:44.219111: step 613, loss 0.564581.
Train: 2018-08-06T00:30:44.464423: step 614, loss 0.546856.
Train: 2018-08-06T00:30:44.707773: step 615, loss 0.564249.
Train: 2018-08-06T00:30:44.956139: step 616, loss 0.587798.
Train: 2018-08-06T00:30:45.199457: step 617, loss 0.563257.
Train: 2018-08-06T00:30:45.448792: step 618, loss 0.555212.
Train: 2018-08-06T00:30:45.694134: step 619, loss 0.60395.
Train: 2018-08-06T00:30:45.942495: step 620, loss 0.595519.
Test: 2018-08-06T00:30:47.205093: step 620, loss 0.550478.
Train: 2018-08-06T00:30:47.442459: step 621, loss 0.571954.
Train: 2018-08-06T00:30:47.690820: step 622, loss 0.595228.
Train: 2018-08-06T00:30:47.938172: step 623, loss 0.609468.
Train: 2018-08-06T00:30:48.183508: step 624, loss 0.548865.
Train: 2018-08-06T00:30:48.437831: step 625, loss 0.563579.
Train: 2018-08-06T00:30:48.681172: step 626, loss 0.525405.
Train: 2018-08-06T00:30:48.946464: step 627, loss 0.556718.
Train: 2018-08-06T00:30:49.210755: step 628, loss 0.602326.
Train: 2018-08-06T00:30:49.462059: step 629, loss 0.541113.
Train: 2018-08-06T00:30:49.716398: step 630, loss 0.526206.
Test: 2018-08-06T00:30:50.989973: step 630, loss 0.550916.
Train: 2018-08-06T00:30:51.240303: step 631, loss 0.610157.
Train: 2018-08-06T00:30:51.495645: step 632, loss 0.609601.
Train: 2018-08-06T00:30:51.736973: step 633, loss 0.556084.
Train: 2018-08-06T00:30:51.984338: step 634, loss 0.578636.
Train: 2018-08-06T00:30:52.230683: step 635, loss 0.533523.
Train: 2018-08-06T00:30:52.481983: step 636, loss 0.540582.
Train: 2018-08-06T00:30:52.731346: step 637, loss 0.547877.
Train: 2018-08-06T00:30:52.988655: step 638, loss 0.57859.
Train: 2018-08-06T00:30:53.241950: step 639, loss 0.632922.
Train: 2018-08-06T00:30:53.491283: step 640, loss 0.587121.
Test: 2018-08-06T00:30:54.753905: step 640, loss 0.551239.
Train: 2018-08-06T00:30:54.989302: step 641, loss 0.609736.
Train: 2018-08-06T00:30:55.245622: step 642, loss 0.540806.
Train: 2018-08-06T00:30:55.487942: step 643, loss 0.594736.
Train: 2018-08-06T00:30:55.732319: step 644, loss 0.540301.
Train: 2018-08-06T00:30:55.982619: step 645, loss 0.587503.
Train: 2018-08-06T00:30:56.232950: step 646, loss 0.478326.
Train: 2018-08-06T00:30:56.480316: step 647, loss 0.547678.
Train: 2018-08-06T00:30:56.724636: step 648, loss 0.531029.
Train: 2018-08-06T00:30:56.981948: step 649, loss 0.493375.
Train: 2018-08-06T00:30:57.240256: step 650, loss 0.508058.
Test: 2018-08-06T00:30:58.498890: step 650, loss 0.549213.
Train: 2018-08-06T00:30:58.736255: step 651, loss 0.49949.
Train: 2018-08-06T00:30:58.985589: step 652, loss 0.54628.
Train: 2018-08-06T00:30:59.235930: step 653, loss 0.57834.
Train: 2018-08-06T00:30:59.484256: step 654, loss 0.613895.
Train: 2018-08-06T00:30:59.726638: step 655, loss 0.585668.
Train: 2018-08-06T00:30:59.970955: step 656, loss 0.603642.
Train: 2018-08-06T00:31:00.221311: step 657, loss 0.589374.
Train: 2018-08-06T00:31:00.467659: step 658, loss 0.603351.
Train: 2018-08-06T00:31:00.710988: step 659, loss 0.536906.
Train: 2018-08-06T00:31:00.955352: step 660, loss 0.528744.
Test: 2018-08-06T00:31:02.230909: step 660, loss 0.548123.
Train: 2018-08-06T00:31:02.475281: step 661, loss 0.56971.
Train: 2018-08-06T00:31:02.733599: step 662, loss 0.621728.
Train: 2018-08-06T00:31:02.980938: step 663, loss 0.570765.
Train: 2018-08-06T00:31:03.222285: step 664, loss 0.529264.
Train: 2018-08-06T00:31:03.483560: step 665, loss 0.529526.
Train: 2018-08-06T00:31:03.730923: step 666, loss 0.539758.
Train: 2018-08-06T00:31:03.975245: step 667, loss 0.52938.
Train: 2018-08-06T00:31:04.233583: step 668, loss 0.587461.
Train: 2018-08-06T00:31:04.477926: step 669, loss 0.529562.
Train: 2018-08-06T00:31:04.729229: step 670, loss 0.519919.
Test: 2018-08-06T00:31:05.999830: step 670, loss 0.547661.
Train: 2018-08-06T00:31:06.233235: step 671, loss 0.496724.
Train: 2018-08-06T00:31:06.477584: step 672, loss 0.606916.
Train: 2018-08-06T00:31:06.722930: step 673, loss 0.560938.
Train: 2018-08-06T00:31:06.969263: step 674, loss 0.50279.
Train: 2018-08-06T00:31:07.211612: step 675, loss 0.563225.
Train: 2018-08-06T00:31:07.467936: step 676, loss 0.545101.
Train: 2018-08-06T00:31:07.711261: step 677, loss 0.604815.
Train: 2018-08-06T00:31:07.951617: step 678, loss 0.605142.
Train: 2018-08-06T00:31:08.195977: step 679, loss 0.486967.
Train: 2018-08-06T00:31:08.446295: step 680, loss 0.537072.
Test: 2018-08-06T00:31:09.718891: step 680, loss 0.549507.
Train: 2018-08-06T00:31:09.954260: step 681, loss 0.580319.
Train: 2018-08-06T00:31:10.198608: step 682, loss 0.545074.
Train: 2018-08-06T00:31:10.445947: step 683, loss 0.590358.
Train: 2018-08-06T00:31:10.688341: step 684, loss 0.553414.
Train: 2018-08-06T00:31:10.930651: step 685, loss 0.511903.
Train: 2018-08-06T00:31:11.176025: step 686, loss 0.588888.
Train: 2018-08-06T00:31:11.420366: step 687, loss 0.598155.
Train: 2018-08-06T00:31:11.664711: step 688, loss 0.544197.
Train: 2018-08-06T00:31:11.907049: step 689, loss 0.571042.
Train: 2018-08-06T00:31:12.148420: step 690, loss 0.461359.
Test: 2018-08-06T00:31:13.406030: step 690, loss 0.547917.
Train: 2018-08-06T00:31:13.652372: step 691, loss 0.503388.
Train: 2018-08-06T00:31:13.895751: step 692, loss 0.595974.
Train: 2018-08-06T00:31:14.138098: step 693, loss 0.629024.
Train: 2018-08-06T00:31:14.382419: step 694, loss 0.596371.
Train: 2018-08-06T00:31:14.632779: step 695, loss 0.571035.
Train: 2018-08-06T00:31:14.879092: step 696, loss 0.629358.
Train: 2018-08-06T00:31:15.122487: step 697, loss 0.579638.
Train: 2018-08-06T00:31:15.369804: step 698, loss 0.537517.
Train: 2018-08-06T00:31:15.614125: step 699, loss 0.594712.
Train: 2018-08-06T00:31:15.856503: step 700, loss 0.586626.
Test: 2018-08-06T00:31:17.114114: step 700, loss 0.547721.
Train: 2018-08-06T00:31:18.030928: step 701, loss 0.529371.
Train: 2018-08-06T00:31:18.282266: step 702, loss 0.571043.
Train: 2018-08-06T00:31:18.535555: step 703, loss 0.545894.
Train: 2018-08-06T00:31:18.792860: step 704, loss 0.521932.
Train: 2018-08-06T00:31:19.039199: step 705, loss 0.554757.
Train: 2018-08-06T00:31:19.286539: step 706, loss 0.652417.
Train: 2018-08-06T00:31:19.529890: step 707, loss 0.546097.
Train: 2018-08-06T00:31:19.773238: step 708, loss 0.539272.
Train: 2018-08-06T00:31:20.016587: step 709, loss 0.587641.
Train: 2018-08-06T00:31:20.262953: step 710, loss 0.611476.
Test: 2018-08-06T00:31:21.517572: step 710, loss 0.549598.
Train: 2018-08-06T00:31:21.753971: step 711, loss 0.570134.
Train: 2018-08-06T00:31:22.003306: step 712, loss 0.634736.
Train: 2018-08-06T00:31:22.259587: step 713, loss 0.540185.
Train: 2018-08-06T00:31:22.498948: step 714, loss 0.579662.
Train: 2018-08-06T00:31:22.751273: step 715, loss 0.500214.
Train: 2018-08-06T00:31:22.995621: step 716, loss 0.610446.
Train: 2018-08-06T00:31:23.249972: step 717, loss 0.562755.
Train: 2018-08-06T00:31:23.492290: step 718, loss 0.507974.
Train: 2018-08-06T00:31:23.737635: step 719, loss 0.572387.
Train: 2018-08-06T00:31:23.978990: step 720, loss 0.531823.
Test: 2018-08-06T00:31:25.254578: step 720, loss 0.549964.
Train: 2018-08-06T00:31:25.488953: step 721, loss 0.570803.
Train: 2018-08-06T00:31:25.732305: step 722, loss 0.57814.
Train: 2018-08-06T00:31:25.982631: step 723, loss 0.578532.
Train: 2018-08-06T00:31:26.222989: step 724, loss 0.491762.
Train: 2018-08-06T00:31:26.467336: step 725, loss 0.508163.
Train: 2018-08-06T00:31:26.723650: step 726, loss 0.618744.
Train: 2018-08-06T00:31:26.972016: step 727, loss 0.587206.
Train: 2018-08-06T00:31:27.212367: step 728, loss 0.491229.
Train: 2018-08-06T00:31:27.457711: step 729, loss 0.562739.
Train: 2018-08-06T00:31:27.703030: step 730, loss 0.530924.
Test: 2018-08-06T00:31:28.955679: step 730, loss 0.548841.
Train: 2018-08-06T00:31:29.190053: step 731, loss 0.513644.
Train: 2018-08-06T00:31:29.430443: step 732, loss 0.489151.
Train: 2018-08-06T00:31:29.675783: step 733, loss 0.505265.
Train: 2018-08-06T00:31:29.919104: step 734, loss 0.554635.
Train: 2018-08-06T00:31:30.176417: step 735, loss 0.638761.
Train: 2018-08-06T00:31:30.417770: step 736, loss 0.620462.
Train: 2018-08-06T00:31:30.660123: step 737, loss 0.512504.
Train: 2018-08-06T00:31:30.910454: step 738, loss 0.528944.
Train: 2018-08-06T00:31:31.156794: step 739, loss 0.569665.
Train: 2018-08-06T00:31:31.400144: step 740, loss 0.536384.
Test: 2018-08-06T00:31:32.662767: step 740, loss 0.549758.
Train: 2018-08-06T00:31:32.897164: step 741, loss 0.579309.
Train: 2018-08-06T00:31:33.151461: step 742, loss 0.562031.
Train: 2018-08-06T00:31:33.400823: step 743, loss 0.561986.
Train: 2018-08-06T00:31:33.641150: step 744, loss 0.529604.
Train: 2018-08-06T00:31:33.883534: step 745, loss 0.554008.
Train: 2018-08-06T00:31:34.129843: step 746, loss 0.597436.
Train: 2018-08-06T00:31:34.375215: step 747, loss 0.528142.
Train: 2018-08-06T00:31:34.627545: step 748, loss 0.570847.
Train: 2018-08-06T00:31:34.876845: step 749, loss 0.570318.
Train: 2018-08-06T00:31:35.129196: step 750, loss 0.537843.
Test: 2018-08-06T00:31:36.379825: step 750, loss 0.547878.
Train: 2018-08-06T00:31:36.613226: step 751, loss 0.57959.
Train: 2018-08-06T00:31:36.854557: step 752, loss 0.492085.
Train: 2018-08-06T00:31:37.111892: step 753, loss 0.640478.
Train: 2018-08-06T00:31:37.354252: step 754, loss 0.597471.
Train: 2018-08-06T00:31:37.538757: step 755, loss 0.598567.
Train: 2018-08-06T00:31:37.785100: step 756, loss 0.612146.
Train: 2018-08-06T00:31:38.033403: step 757, loss 0.503417.
Train: 2018-08-06T00:31:38.280743: step 758, loss 0.563454.
Train: 2018-08-06T00:31:38.536090: step 759, loss 0.562227.
Train: 2018-08-06T00:31:38.776416: step 760, loss 0.562331.
Test: 2018-08-06T00:31:40.027071: step 760, loss 0.547793.
Train: 2018-08-06T00:31:40.260447: step 761, loss 0.594661.
Train: 2018-08-06T00:31:40.511806: step 762, loss 0.537712.
Train: 2018-08-06T00:31:40.754127: step 763, loss 0.595683.
Train: 2018-08-06T00:31:40.998475: step 764, loss 0.604398.
Train: 2018-08-06T00:31:41.241824: step 765, loss 0.57861.
Train: 2018-08-06T00:31:41.489187: step 766, loss 0.537884.
Train: 2018-08-06T00:31:41.732541: step 767, loss 0.546582.
Train: 2018-08-06T00:31:41.980848: step 768, loss 0.545833.
Train: 2018-08-06T00:31:42.220237: step 769, loss 0.562028.
Train: 2018-08-06T00:31:42.469564: step 770, loss 0.603141.
Test: 2018-08-06T00:31:43.741138: step 770, loss 0.549045.
Train: 2018-08-06T00:31:43.977538: step 771, loss 0.561915.
Train: 2018-08-06T00:31:44.222876: step 772, loss 0.490378.
Train: 2018-08-06T00:31:44.467228: step 773, loss 0.522087.
Train: 2018-08-06T00:31:44.714535: step 774, loss 0.537745.
Train: 2018-08-06T00:31:44.960913: step 775, loss 0.578621.
Train: 2018-08-06T00:31:45.209243: step 776, loss 0.56186.
Train: 2018-08-06T00:31:45.461537: step 777, loss 0.561792.
Train: 2018-08-06T00:31:45.707880: step 778, loss 0.603152.
Train: 2018-08-06T00:31:45.948274: step 779, loss 0.570204.
Train: 2018-08-06T00:31:46.194611: step 780, loss 0.579463.
Test: 2018-08-06T00:31:47.454208: step 780, loss 0.548941.
Train: 2018-08-06T00:31:47.686588: step 781, loss 0.562478.
Train: 2018-08-06T00:31:47.927975: step 782, loss 0.611466.
Train: 2018-08-06T00:31:48.174282: step 783, loss 0.554821.
Train: 2018-08-06T00:31:48.419628: step 784, loss 0.563348.
Train: 2018-08-06T00:31:48.663011: step 785, loss 0.572153.
Train: 2018-08-06T00:31:48.912310: step 786, loss 0.578552.
Train: 2018-08-06T00:31:49.156681: step 787, loss 0.577992.
Train: 2018-08-06T00:31:49.412004: step 788, loss 0.586745.
Train: 2018-08-06T00:31:49.658315: step 789, loss 0.611163.
Train: 2018-08-06T00:31:49.904656: step 790, loss 0.578571.
Test: 2018-08-06T00:31:51.178249: step 790, loss 0.548527.
Train: 2018-08-06T00:31:51.415640: step 791, loss 0.531142.
Train: 2018-08-06T00:31:51.656970: step 792, loss 0.553835.
Train: 2018-08-06T00:31:51.902338: step 793, loss 0.514875.
Train: 2018-08-06T00:31:52.156633: step 794, loss 0.547188.
Train: 2018-08-06T00:31:52.405000: step 795, loss 0.58767.
Train: 2018-08-06T00:31:52.663303: step 796, loss 0.546504.
Train: 2018-08-06T00:31:52.909620: step 797, loss 0.531589.
Train: 2018-08-06T00:31:53.150974: step 798, loss 0.546478.
Train: 2018-08-06T00:31:53.398326: step 799, loss 0.530691.
Train: 2018-08-06T00:31:53.641692: step 800, loss 0.570294.
Test: 2018-08-06T00:31:54.918247: step 800, loss 0.550161.
Train: 2018-08-06T00:31:55.879883: step 801, loss 0.569936.
Train: 2018-08-06T00:31:56.126227: step 802, loss 0.506565.
Train: 2018-08-06T00:31:56.387531: step 803, loss 0.63459.
Train: 2018-08-06T00:31:56.635864: step 804, loss 0.545738.
Train: 2018-08-06T00:31:56.886180: step 805, loss 0.595963.
Train: 2018-08-06T00:31:57.130542: step 806, loss 0.570481.
Train: 2018-08-06T00:31:57.372862: step 807, loss 0.5702.
Train: 2018-08-06T00:31:57.616211: step 808, loss 0.611423.
Train: 2018-08-06T00:31:57.857596: step 809, loss 0.472856.
Train: 2018-08-06T00:31:58.111886: step 810, loss 0.462581.
Test: 2018-08-06T00:31:59.366530: step 810, loss 0.549012.
Train: 2018-08-06T00:31:59.600929: step 811, loss 0.593551.
Train: 2018-08-06T00:31:59.856222: step 812, loss 0.553886.
Train: 2018-08-06T00:32:00.099569: step 813, loss 0.595344.
Train: 2018-08-06T00:32:00.351895: step 814, loss 0.61201.
Train: 2018-08-06T00:32:00.599264: step 815, loss 0.570372.
Train: 2018-08-06T00:32:00.843579: step 816, loss 0.586157.
Train: 2018-08-06T00:32:01.086960: step 817, loss 0.496066.
Train: 2018-08-06T00:32:01.333270: step 818, loss 0.57098.
Train: 2018-08-06T00:32:01.579642: step 819, loss 0.586391.
Train: 2018-08-06T00:32:01.830970: step 820, loss 0.579271.
Test: 2018-08-06T00:32:03.088575: step 820, loss 0.549889.
Train: 2018-08-06T00:32:03.326964: step 821, loss 0.627466.
Train: 2018-08-06T00:32:03.568323: step 822, loss 0.494223.
Train: 2018-08-06T00:32:03.814665: step 823, loss 0.587591.
Train: 2018-08-06T00:32:04.059011: step 824, loss 0.569875.
Train: 2018-08-06T00:32:04.302360: step 825, loss 0.500934.
Train: 2018-08-06T00:32:04.543715: step 826, loss 0.544702.
Train: 2018-08-06T00:32:04.799998: step 827, loss 0.505136.
Train: 2018-08-06T00:32:05.042386: step 828, loss 0.510286.
Train: 2018-08-06T00:32:05.298665: step 829, loss 0.561301.
Train: 2018-08-06T00:32:05.540020: step 830, loss 0.58817.
Test: 2018-08-06T00:32:06.802643: step 830, loss 0.547221.
Train: 2018-08-06T00:32:07.036049: step 831, loss 0.468295.
Train: 2018-08-06T00:32:07.285378: step 832, loss 0.500107.
Train: 2018-08-06T00:32:07.529733: step 833, loss 0.579899.
Train: 2018-08-06T00:32:07.773048: step 834, loss 0.617696.
Train: 2018-08-06T00:32:08.014402: step 835, loss 0.537811.
Train: 2018-08-06T00:32:08.262739: step 836, loss 0.510832.
Train: 2018-08-06T00:32:08.509110: step 837, loss 0.579963.
Train: 2018-08-06T00:32:08.754423: step 838, loss 0.60301.
Train: 2018-08-06T00:32:08.993814: step 839, loss 0.522546.
Train: 2018-08-06T00:32:09.232176: step 840, loss 0.537994.
Test: 2018-08-06T00:32:10.491777: step 840, loss 0.546996.
Train: 2018-08-06T00:32:10.727178: step 841, loss 0.642764.
Train: 2018-08-06T00:32:10.976512: step 842, loss 0.528367.
Train: 2018-08-06T00:32:11.223851: step 843, loss 0.648847.
Train: 2018-08-06T00:32:11.479137: step 844, loss 0.523108.
Train: 2018-08-06T00:32:11.721488: step 845, loss 0.562828.
Train: 2018-08-06T00:32:11.967860: step 846, loss 0.504506.
Train: 2018-08-06T00:32:12.214172: step 847, loss 0.611805.
Train: 2018-08-06T00:32:12.458517: step 848, loss 0.564099.
Train: 2018-08-06T00:32:12.712867: step 849, loss 0.554765.
Train: 2018-08-06T00:32:12.957184: step 850, loss 0.560477.
Test: 2018-08-06T00:32:14.219807: step 850, loss 0.548753.
Train: 2018-08-06T00:32:14.462186: step 851, loss 0.63635.
Train: 2018-08-06T00:32:14.702542: step 852, loss 0.48851.
Train: 2018-08-06T00:32:14.948857: step 853, loss 0.57232.
Train: 2018-08-06T00:32:15.196195: step 854, loss 0.553622.
Train: 2018-08-06T00:32:15.439545: step 855, loss 0.597316.
Train: 2018-08-06T00:32:15.685917: step 856, loss 0.553647.
Train: 2018-08-06T00:32:15.929235: step 857, loss 0.562953.
Train: 2018-08-06T00:32:16.179592: step 858, loss 0.498784.
Train: 2018-08-06T00:32:16.426949: step 859, loss 0.570916.
Train: 2018-08-06T00:32:16.667293: step 860, loss 0.5967.
Test: 2018-08-06T00:32:17.932877: step 860, loss 0.549399.
Train: 2018-08-06T00:32:18.172265: step 861, loss 0.529598.
Train: 2018-08-06T00:32:18.428582: step 862, loss 0.547588.
Train: 2018-08-06T00:32:18.673926: step 863, loss 0.666774.
Train: 2018-08-06T00:32:18.925223: step 864, loss 0.57975.
Train: 2018-08-06T00:32:19.167575: step 865, loss 0.572157.
Train: 2018-08-06T00:32:19.409926: step 866, loss 0.690608.
Train: 2018-08-06T00:32:19.658263: step 867, loss 0.50643.
Train: 2018-08-06T00:32:19.902609: step 868, loss 0.538449.
Train: 2018-08-06T00:32:20.146987: step 869, loss 0.625093.
Train: 2018-08-06T00:32:20.392299: step 870, loss 0.657531.
Test: 2018-08-06T00:32:21.668885: step 870, loss 0.54997.
Train: 2018-08-06T00:32:21.903284: step 871, loss 0.539966.
Train: 2018-08-06T00:32:22.149631: step 872, loss 0.485381.
Train: 2018-08-06T00:32:22.395941: step 873, loss 0.609551.
Train: 2018-08-06T00:32:22.642283: step 874, loss 0.516608.
Train: 2018-08-06T00:32:22.896633: step 875, loss 0.525406.
Train: 2018-08-06T00:32:23.139983: step 876, loss 0.586821.
Train: 2018-08-06T00:32:23.396266: step 877, loss 0.523581.
Train: 2018-08-06T00:32:23.639615: step 878, loss 0.532978.
Train: 2018-08-06T00:32:23.884988: step 879, loss 0.516522.
Train: 2018-08-06T00:32:24.137315: step 880, loss 0.516604.
Test: 2018-08-06T00:32:25.426835: step 880, loss 0.55096.
Train: 2018-08-06T00:32:25.662206: step 881, loss 0.547145.
Train: 2018-08-06T00:32:25.904587: step 882, loss 0.515606.
Train: 2018-08-06T00:32:26.156890: step 883, loss 0.578675.
Train: 2018-08-06T00:32:26.407214: step 884, loss 0.53071.
Train: 2018-08-06T00:32:26.651561: step 885, loss 0.59509.
Train: 2018-08-06T00:32:26.902905: step 886, loss 0.669943.
Train: 2018-08-06T00:32:27.146268: step 887, loss 0.538323.
Train: 2018-08-06T00:32:27.394573: step 888, loss 0.571319.
Train: 2018-08-06T00:32:27.638951: step 889, loss 0.611582.
Train: 2018-08-06T00:32:27.886288: step 890, loss 0.578696.
Test: 2018-08-06T00:32:29.150876: step 890, loss 0.54923.
Train: 2018-08-06T00:32:29.386247: step 891, loss 0.522068.
Train: 2018-08-06T00:32:29.630618: step 892, loss 0.578897.
Train: 2018-08-06T00:32:29.870950: step 893, loss 0.562395.
Train: 2018-08-06T00:32:30.116324: step 894, loss 0.586577.
Train: 2018-08-06T00:32:30.362636: step 895, loss 0.56203.
Train: 2018-08-06T00:32:30.606010: step 896, loss 0.480485.
Train: 2018-08-06T00:32:30.861302: step 897, loss 0.529744.
Train: 2018-08-06T00:32:31.109638: step 898, loss 0.587639.
Train: 2018-08-06T00:32:31.353998: step 899, loss 0.522064.
Train: 2018-08-06T00:32:31.596367: step 900, loss 0.570131.
Test: 2018-08-06T00:32:32.869930: step 900, loss 0.549181.
Train: 2018-08-06T00:32:33.827314: step 901, loss 0.595537.
Train: 2018-08-06T00:32:34.136972: step 902, loss 0.528294.
Train: 2018-08-06T00:32:34.381284: step 903, loss 0.579561.
Train: 2018-08-06T00:32:34.627661: step 904, loss 0.612474.
Train: 2018-08-06T00:32:34.867983: step 905, loss 0.530396.
Train: 2018-08-06T00:32:35.055481: step 906, loss 0.543537.
Train: 2018-08-06T00:32:35.309805: step 907, loss 0.644549.
Train: 2018-08-06T00:32:35.550188: step 908, loss 0.56234.
Train: 2018-08-06T00:32:35.792541: step 909, loss 0.603152.
Train: 2018-08-06T00:32:36.040846: step 910, loss 0.521773.
Test: 2018-08-06T00:32:37.299479: step 910, loss 0.549394.
Train: 2018-08-06T00:32:37.590701: step 911, loss 0.57963.
Train: 2018-08-06T00:32:37.841043: step 912, loss 0.579391.
Train: 2018-08-06T00:32:38.083383: step 913, loss 0.59545.
Train: 2018-08-06T00:32:38.324738: step 914, loss 0.506165.
Train: 2018-08-06T00:32:38.566093: step 915, loss 0.610459.
Train: 2018-08-06T00:32:38.807480: step 916, loss 0.554659.
Train: 2018-08-06T00:32:39.056820: step 917, loss 0.595152.
Train: 2018-08-06T00:32:39.302150: step 918, loss 0.538796.
Train: 2018-08-06T00:32:39.546501: step 919, loss 0.481703.
Train: 2018-08-06T00:32:39.797830: step 920, loss 0.522155.
Test: 2018-08-06T00:32:41.057429: step 920, loss 0.549119.
Train: 2018-08-06T00:32:41.288844: step 921, loss 0.481465.
Train: 2018-08-06T00:32:41.537148: step 922, loss 0.619688.
Train: 2018-08-06T00:32:41.790470: step 923, loss 0.570514.
Train: 2018-08-06T00:32:42.039831: step 924, loss 0.652959.
Train: 2018-08-06T00:32:42.283182: step 925, loss 0.529601.
Train: 2018-08-06T00:32:42.527503: step 926, loss 0.578659.
Train: 2018-08-06T00:32:42.768877: step 927, loss 0.578939.
Train: 2018-08-06T00:32:43.014227: step 928, loss 0.620627.
Train: 2018-08-06T00:32:43.259573: step 929, loss 0.637659.
Train: 2018-08-06T00:32:43.509901: step 930, loss 0.585658.
Test: 2018-08-06T00:32:44.787454: step 930, loss 0.548527.
Train: 2018-08-06T00:32:45.021828: step 931, loss 0.570057.
Train: 2018-08-06T00:32:45.271161: step 932, loss 0.585941.
Train: 2018-08-06T00:32:45.517558: step 933, loss 0.507068.
Train: 2018-08-06T00:32:45.759878: step 934, loss 0.531619.
Train: 2018-08-06T00:32:46.002230: step 935, loss 0.579299.
Train: 2018-08-06T00:32:46.247607: step 936, loss 0.482638.
Train: 2018-08-06T00:32:46.506911: step 937, loss 0.577975.
Train: 2018-08-06T00:32:46.760203: step 938, loss 0.586208.
Train: 2018-08-06T00:32:47.000593: step 939, loss 0.514304.
Train: 2018-08-06T00:32:47.247927: step 940, loss 0.579341.
Test: 2018-08-06T00:32:48.496559: step 940, loss 0.548875.
Train: 2018-08-06T00:32:48.733925: step 941, loss 0.595402.
Train: 2018-08-06T00:32:48.976324: step 942, loss 0.610888.
Train: 2018-08-06T00:32:49.222617: step 943, loss 0.562009.
Train: 2018-08-06T00:32:49.467961: step 944, loss 0.458737.
Train: 2018-08-06T00:32:49.717295: step 945, loss 0.554299.
Train: 2018-08-06T00:32:49.960644: step 946, loss 0.571055.
Train: 2018-08-06T00:32:50.210974: step 947, loss 0.539394.
Train: 2018-08-06T00:32:50.455354: step 948, loss 0.562446.
Train: 2018-08-06T00:32:50.703688: step 949, loss 0.604243.
Train: 2018-08-06T00:32:50.954013: step 950, loss 0.498012.
Test: 2018-08-06T00:32:52.223592: step 950, loss 0.549031.
Train: 2018-08-06T00:32:52.469958: step 951, loss 0.61903.
Train: 2018-08-06T00:32:52.717272: step 952, loss 0.521356.
Train: 2018-08-06T00:32:52.961618: step 953, loss 0.523435.
Train: 2018-08-06T00:32:53.202001: step 954, loss 0.620702.
Train: 2018-08-06T00:32:53.455330: step 955, loss 0.560993.
Train: 2018-08-06T00:32:53.697681: step 956, loss 0.529499.
Train: 2018-08-06T00:32:53.936047: step 957, loss 0.611665.
Train: 2018-08-06T00:32:54.194322: step 958, loss 0.59643.
Train: 2018-08-06T00:32:54.440675: step 959, loss 0.636568.
Train: 2018-08-06T00:32:54.686019: step 960, loss 0.497169.
Test: 2018-08-06T00:32:55.942647: step 960, loss 0.549495.
Train: 2018-08-06T00:32:56.177020: step 961, loss 0.587013.
Train: 2018-08-06T00:32:56.419396: step 962, loss 0.636584.
Train: 2018-08-06T00:32:56.674688: step 963, loss 0.554797.
Train: 2018-08-06T00:32:56.920057: step 964, loss 0.529962.
Train: 2018-08-06T00:32:57.167382: step 965, loss 0.514156.
Train: 2018-08-06T00:32:57.408756: step 966, loss 0.555301.
Train: 2018-08-06T00:32:57.649092: step 967, loss 0.611216.
Train: 2018-08-06T00:32:57.892432: step 968, loss 0.554783.
Train: 2018-08-06T00:32:58.144792: step 969, loss 0.546906.
Train: 2018-08-06T00:32:58.391098: step 970, loss 0.474266.
Test: 2018-08-06T00:32:59.660703: step 970, loss 0.548627.
Train: 2018-08-06T00:32:59.898099: step 971, loss 0.610311.
Train: 2018-08-06T00:33:00.149398: step 972, loss 0.52136.
Train: 2018-08-06T00:33:00.403717: step 973, loss 0.563033.
Train: 2018-08-06T00:33:00.644098: step 974, loss 0.628219.
Train: 2018-08-06T00:33:00.887453: step 975, loss 0.537354.
Train: 2018-08-06T00:33:01.135758: step 976, loss 0.572996.
Train: 2018-08-06T00:33:01.380130: step 977, loss 0.536826.
Train: 2018-08-06T00:33:01.626445: step 978, loss 0.634397.
Train: 2018-08-06T00:33:01.870792: step 979, loss 0.57002.
Train: 2018-08-06T00:33:02.121154: step 980, loss 0.587134.
Test: 2018-08-06T00:33:03.384743: step 980, loss 0.548944.
Train: 2018-08-06T00:33:03.619116: step 981, loss 0.547018.
Train: 2018-08-06T00:33:03.864485: step 982, loss 0.578789.
Train: 2018-08-06T00:33:04.112827: step 983, loss 0.537913.
Train: 2018-08-06T00:33:04.358164: step 984, loss 0.555109.
Train: 2018-08-06T00:33:04.601522: step 985, loss 0.570675.
Train: 2018-08-06T00:33:04.852848: step 986, loss 0.521751.
Train: 2018-08-06T00:33:05.098166: step 987, loss 0.57974.
Train: 2018-08-06T00:33:05.351522: step 988, loss 0.522358.
Train: 2018-08-06T00:33:05.594837: step 989, loss 0.579392.
Train: 2018-08-06T00:33:05.837189: step 990, loss 0.555071.
Test: 2018-08-06T00:33:07.095824: step 990, loss 0.549833.
Train: 2018-08-06T00:33:07.327205: step 991, loss 0.529886.
Train: 2018-08-06T00:33:07.572549: step 992, loss 0.587594.
Train: 2018-08-06T00:33:07.820885: step 993, loss 0.562669.
Train: 2018-08-06T00:33:08.073220: step 994, loss 0.481163.
Train: 2018-08-06T00:33:08.319552: step 995, loss 0.464901.
Train: 2018-08-06T00:33:08.556916: step 996, loss 0.596632.
Train: 2018-08-06T00:33:08.800265: step 997, loss 0.512881.
Train: 2018-08-06T00:33:09.048600: step 998, loss 0.537852.
Train: 2018-08-06T00:33:09.303919: step 999, loss 0.512584.
Train: 2018-08-06T00:33:09.551266: step 1000, loss 0.593777.
Test: 2018-08-06T00:33:10.806898: step 1000, loss 0.548345.
Train: 2018-08-06T00:33:11.711790: step 1001, loss 0.546114.
Train: 2018-08-06T00:33:11.953143: step 1002, loss 0.603764.
Train: 2018-08-06T00:33:12.198454: step 1003, loss 0.478051.
Train: 2018-08-06T00:33:12.439809: step 1004, loss 0.604425.
Train: 2018-08-06T00:33:12.686180: step 1005, loss 0.553453.
Train: 2018-08-06T00:33:12.943493: step 1006, loss 0.528747.
Train: 2018-08-06T00:33:13.188824: step 1007, loss 0.588025.
Train: 2018-08-06T00:33:13.437140: step 1008, loss 0.483959.
Train: 2018-08-06T00:33:13.682484: step 1009, loss 0.554319.
Train: 2018-08-06T00:33:13.936805: step 1010, loss 0.63087.
Test: 2018-08-06T00:33:15.195438: step 1010, loss 0.547763.
Train: 2018-08-06T00:33:15.428861: step 1011, loss 0.553548.
Train: 2018-08-06T00:33:15.686157: step 1012, loss 0.564274.
Train: 2018-08-06T00:33:15.930472: step 1013, loss 0.553413.
Train: 2018-08-06T00:33:16.181802: step 1014, loss 0.562593.
Train: 2018-08-06T00:33:16.430168: step 1015, loss 0.520043.
Train: 2018-08-06T00:33:16.669497: step 1016, loss 0.553303.
Train: 2018-08-06T00:33:16.909903: step 1017, loss 0.54596.
Train: 2018-08-06T00:33:17.169161: step 1018, loss 0.563717.
Train: 2018-08-06T00:33:17.410548: step 1019, loss 0.527984.
Train: 2018-08-06T00:33:17.660874: step 1020, loss 0.63108.
Test: 2018-08-06T00:33:18.908508: step 1020, loss 0.547444.
Train: 2018-08-06T00:33:19.141911: step 1021, loss 0.50276.
Train: 2018-08-06T00:33:19.387253: step 1022, loss 0.6484.
Train: 2018-08-06T00:33:19.642545: step 1023, loss 0.56223.
Train: 2018-08-06T00:33:19.901851: step 1024, loss 0.529449.
Train: 2018-08-06T00:33:20.141243: step 1025, loss 0.580259.
Train: 2018-08-06T00:33:20.392575: step 1026, loss 0.596349.
Train: 2018-08-06T00:33:20.647857: step 1027, loss 0.587405.
Train: 2018-08-06T00:33:20.891238: step 1028, loss 0.596512.
Train: 2018-08-06T00:33:21.135583: step 1029, loss 0.547105.
Train: 2018-08-06T00:33:21.382922: step 1030, loss 0.529.
Test: 2018-08-06T00:33:22.644517: step 1030, loss 0.549181.
Train: 2018-08-06T00:33:22.878890: step 1031, loss 0.537592.
Train: 2018-08-06T00:33:23.122270: step 1032, loss 0.56172.
Train: 2018-08-06T00:33:23.376594: step 1033, loss 0.529818.
Train: 2018-08-06T00:33:23.618911: step 1034, loss 0.529221.
Train: 2018-08-06T00:33:23.869242: step 1035, loss 0.5793.
Train: 2018-08-06T00:33:24.111624: step 1036, loss 0.578867.
Train: 2018-08-06T00:33:24.352973: step 1037, loss 0.554477.
Train: 2018-08-06T00:33:24.598318: step 1038, loss 0.603687.
Train: 2018-08-06T00:33:24.840644: step 1039, loss 0.578566.
Train: 2018-08-06T00:33:25.087016: step 1040, loss 0.595771.
Test: 2018-08-06T00:33:26.344621: step 1040, loss 0.548912.
Train: 2018-08-06T00:33:26.593988: step 1041, loss 0.60366.
Train: 2018-08-06T00:33:26.837335: step 1042, loss 0.603418.
Train: 2018-08-06T00:33:27.093649: step 1043, loss 0.546719.
Train: 2018-08-06T00:33:27.338991: step 1044, loss 0.521768.
Train: 2018-08-06T00:33:27.581314: step 1045, loss 0.562521.
Train: 2018-08-06T00:33:27.824703: step 1046, loss 0.578876.
Train: 2018-08-06T00:33:28.067016: step 1047, loss 0.505792.
Train: 2018-08-06T00:33:28.325356: step 1048, loss 0.498209.
Train: 2018-08-06T00:33:28.576653: step 1049, loss 0.579365.
Train: 2018-08-06T00:33:28.823992: step 1050, loss 0.570772.
Test: 2018-08-06T00:33:30.079632: step 1050, loss 0.549788.
Train: 2018-08-06T00:33:30.315005: step 1051, loss 0.570617.
Train: 2018-08-06T00:33:30.562370: step 1052, loss 0.603207.
Train: 2018-08-06T00:33:30.804724: step 1053, loss 0.618638.
Train: 2018-08-06T00:33:31.052033: step 1054, loss 0.498356.
Train: 2018-08-06T00:33:31.294410: step 1055, loss 0.595152.
Train: 2018-08-06T00:33:31.537734: step 1056, loss 0.546907.
Train: 2018-08-06T00:33:31.723238: step 1057, loss 0.511059.
Train: 2018-08-06T00:33:31.968612: step 1058, loss 0.570169.
Train: 2018-08-06T00:33:32.219936: step 1059, loss 0.570573.
Train: 2018-08-06T00:33:32.462288: step 1060, loss 0.546755.
Test: 2018-08-06T00:33:33.735855: step 1060, loss 0.547461.
Train: 2018-08-06T00:33:33.968264: step 1061, loss 0.595228.
Train: 2018-08-06T00:33:34.210585: step 1062, loss 0.538222.
Train: 2018-08-06T00:33:34.453934: step 1063, loss 0.579201.
Train: 2018-08-06T00:33:34.708255: step 1064, loss 0.530168.
Train: 2018-08-06T00:33:34.952632: step 1065, loss 0.570347.
Train: 2018-08-06T00:33:35.197974: step 1066, loss 0.579057.
Train: 2018-08-06T00:33:35.442325: step 1067, loss 0.570603.
Train: 2018-08-06T00:33:35.689657: step 1068, loss 0.538079.
Train: 2018-08-06T00:33:35.930985: step 1069, loss 0.497069.
Train: 2018-08-06T00:33:36.182345: step 1070, loss 0.595524.
Test: 2018-08-06T00:33:37.440946: step 1070, loss 0.549644.
Train: 2018-08-06T00:33:37.677315: step 1071, loss 0.578906.
Train: 2018-08-06T00:33:37.921696: step 1072, loss 0.595266.
Train: 2018-08-06T00:33:38.170994: step 1073, loss 0.579089.
Train: 2018-08-06T00:33:38.423320: step 1074, loss 0.473031.
Train: 2018-08-06T00:33:38.670657: step 1075, loss 0.57107.
Train: 2018-08-06T00:33:38.928971: step 1076, loss 0.571365.
Train: 2018-08-06T00:33:39.171325: step 1077, loss 0.504743.
Train: 2018-08-06T00:33:39.415667: step 1078, loss 0.571217.
Train: 2018-08-06T00:33:39.661010: step 1079, loss 0.627738.
Train: 2018-08-06T00:33:39.904389: step 1080, loss 0.545408.
Test: 2018-08-06T00:33:41.149029: step 1080, loss 0.548139.
Train: 2018-08-06T00:33:41.392404: step 1081, loss 0.538733.
Train: 2018-08-06T00:33:41.642736: step 1082, loss 0.56248.
Train: 2018-08-06T00:33:41.896063: step 1083, loss 0.587053.
Train: 2018-08-06T00:33:42.153375: step 1084, loss 0.612084.
Train: 2018-08-06T00:33:42.396693: step 1085, loss 0.512263.
Train: 2018-08-06T00:33:42.637051: step 1086, loss 0.554217.
Train: 2018-08-06T00:33:42.891396: step 1087, loss 0.55517.
Train: 2018-08-06T00:33:43.150678: step 1088, loss 0.512779.
Train: 2018-08-06T00:33:43.402005: step 1089, loss 0.537191.
Train: 2018-08-06T00:33:43.642395: step 1090, loss 0.554472.
Test: 2018-08-06T00:33:44.892019: step 1090, loss 0.547446.
Train: 2018-08-06T00:33:45.126394: step 1091, loss 0.586513.
Train: 2018-08-06T00:33:45.370764: step 1092, loss 0.503937.
Train: 2018-08-06T00:33:45.622068: step 1093, loss 0.520611.
Train: 2018-08-06T00:33:45.869406: step 1094, loss 0.587082.
Train: 2018-08-06T00:33:46.124754: step 1095, loss 0.54541.
Train: 2018-08-06T00:33:46.379068: step 1096, loss 0.604458.
Train: 2018-08-06T00:33:46.626382: step 1097, loss 0.527554.
Train: 2018-08-06T00:33:46.870729: step 1098, loss 0.571581.
Train: 2018-08-06T00:33:47.117101: step 1099, loss 0.563147.
Train: 2018-08-06T00:33:47.374382: step 1100, loss 0.680874.
Test: 2018-08-06T00:33:48.649969: step 1100, loss 0.548545.
Train: 2018-08-06T00:33:49.590980: step 1101, loss 0.520564.
Train: 2018-08-06T00:33:49.833332: step 1102, loss 0.545748.
Train: 2018-08-06T00:33:50.078650: step 1103, loss 0.554002.
Train: 2018-08-06T00:33:50.322054: step 1104, loss 0.630674.
Train: 2018-08-06T00:33:50.568371: step 1105, loss 0.495713.
Train: 2018-08-06T00:33:50.812712: step 1106, loss 0.571029.
Train: 2018-08-06T00:33:51.058047: step 1107, loss 0.579314.
Train: 2018-08-06T00:33:51.300408: step 1108, loss 0.612724.
Train: 2018-08-06T00:33:51.548748: step 1109, loss 0.587579.
Train: 2018-08-06T00:33:51.795060: step 1110, loss 0.604758.
Test: 2018-08-06T00:33:53.053693: step 1110, loss 0.548863.
Train: 2018-08-06T00:33:53.296046: step 1111, loss 0.586779.
Train: 2018-08-06T00:33:53.539420: step 1112, loss 0.54595.
Train: 2018-08-06T00:33:53.784763: step 1113, loss 0.570239.
Train: 2018-08-06T00:33:54.027140: step 1114, loss 0.587338.
Train: 2018-08-06T00:33:54.272435: step 1115, loss 0.571038.
Train: 2018-08-06T00:33:54.522790: step 1116, loss 0.563063.
Train: 2018-08-06T00:33:54.776101: step 1117, loss 0.643398.
Train: 2018-08-06T00:33:55.025451: step 1118, loss 0.586758.
Train: 2018-08-06T00:33:55.266806: step 1119, loss 0.562487.
Train: 2018-08-06T00:33:55.518136: step 1120, loss 0.594836.
Test: 2018-08-06T00:33:56.775739: step 1120, loss 0.55074.
Train: 2018-08-06T00:33:57.011135: step 1121, loss 0.500197.
Train: 2018-08-06T00:33:57.253492: step 1122, loss 0.571139.
Train: 2018-08-06T00:33:57.496812: step 1123, loss 0.539671.
Train: 2018-08-06T00:33:57.740164: step 1124, loss 0.531802.
Train: 2018-08-06T00:33:57.980548: step 1125, loss 0.601774.
Train: 2018-08-06T00:33:58.223867: step 1126, loss 0.547281.
Train: 2018-08-06T00:33:58.467216: step 1127, loss 0.602446.
Train: 2018-08-06T00:33:58.709569: step 1128, loss 0.508639.
Train: 2018-08-06T00:33:58.954943: step 1129, loss 0.547608.
Train: 2018-08-06T00:33:59.212222: step 1130, loss 0.508132.
Test: 2018-08-06T00:34:00.462878: step 1130, loss 0.549248.
Train: 2018-08-06T00:34:00.702238: step 1131, loss 0.492327.
Train: 2018-08-06T00:34:00.944622: step 1132, loss 0.500157.
Train: 2018-08-06T00:34:01.198936: step 1133, loss 0.514442.
Train: 2018-08-06T00:34:01.452264: step 1134, loss 0.515271.
Train: 2018-08-06T00:34:01.692620: step 1135, loss 0.562147.
Train: 2018-08-06T00:34:01.938931: step 1136, loss 0.53014.
Train: 2018-08-06T00:34:02.181284: step 1137, loss 0.60421.
Train: 2018-08-06T00:34:02.441633: step 1138, loss 0.579536.
Train: 2018-08-06T00:34:02.684937: step 1139, loss 0.529278.
Train: 2018-08-06T00:34:02.925293: step 1140, loss 0.537403.
Test: 2018-08-06T00:34:04.183927: step 1140, loss 0.549232.
Train: 2018-08-06T00:34:04.418327: step 1141, loss 0.470501.
Train: 2018-08-06T00:34:04.663675: step 1142, loss 0.595901.
Train: 2018-08-06T00:34:04.910016: step 1143, loss 0.54615.
Train: 2018-08-06T00:34:05.154359: step 1144, loss 0.562499.
Train: 2018-08-06T00:34:05.397683: step 1145, loss 0.49405.
Train: 2018-08-06T00:34:05.644049: step 1146, loss 0.60444.
Train: 2018-08-06T00:34:05.885408: step 1147, loss 0.552818.
Train: 2018-08-06T00:34:06.127760: step 1148, loss 0.571259.
Train: 2018-08-06T00:34:06.379056: step 1149, loss 0.596501.
Train: 2018-08-06T00:34:06.637398: step 1150, loss 0.578988.
Test: 2018-08-06T00:34:07.891013: step 1150, loss 0.550307.
Train: 2018-08-06T00:34:08.124389: step 1151, loss 0.597193.
Train: 2018-08-06T00:34:08.367738: step 1152, loss 0.535973.
Train: 2018-08-06T00:34:08.624052: step 1153, loss 0.682385.
Train: 2018-08-06T00:34:08.868400: step 1154, loss 0.622784.
Train: 2018-08-06T00:34:09.112767: step 1155, loss 0.536815.
Train: 2018-08-06T00:34:09.358115: step 1156, loss 0.628908.
Train: 2018-08-06T00:34:09.600443: step 1157, loss 0.562663.
Train: 2018-08-06T00:34:09.842793: step 1158, loss 0.503147.
Train: 2018-08-06T00:34:10.097114: step 1159, loss 0.512388.
Train: 2018-08-06T00:34:10.344478: step 1160, loss 0.60489.
Test: 2018-08-06T00:34:11.592116: step 1160, loss 0.548332.
Train: 2018-08-06T00:34:11.826488: step 1161, loss 0.587015.
Train: 2018-08-06T00:34:12.071834: step 1162, loss 0.553905.
Train: 2018-08-06T00:34:12.315210: step 1163, loss 0.578185.
Train: 2018-08-06T00:34:12.554541: step 1164, loss 0.587306.
Train: 2018-08-06T00:34:12.797922: step 1165, loss 0.529676.
Train: 2018-08-06T00:34:13.039255: step 1166, loss 0.497379.
Train: 2018-08-06T00:34:13.292568: step 1167, loss 0.521466.
Train: 2018-08-06T00:34:13.536946: step 1168, loss 0.570618.
Train: 2018-08-06T00:34:13.776306: step 1169, loss 0.505615.
Train: 2018-08-06T00:34:14.019654: step 1170, loss 0.603701.
Test: 2018-08-06T00:34:15.276263: step 1170, loss 0.548807.
Train: 2018-08-06T00:34:15.518614: step 1171, loss 0.612465.
Train: 2018-08-06T00:34:15.759998: step 1172, loss 0.562194.
Train: 2018-08-06T00:34:16.003344: step 1173, loss 0.643445.
Train: 2018-08-06T00:34:16.245695: step 1174, loss 0.57061.
Train: 2018-08-06T00:34:16.490016: step 1175, loss 0.529989.
Train: 2018-08-06T00:34:16.733396: step 1176, loss 0.627777.
Train: 2018-08-06T00:34:16.980735: step 1177, loss 0.610884.
Train: 2018-08-06T00:34:17.233055: step 1178, loss 0.514141.
Train: 2018-08-06T00:34:17.480394: step 1179, loss 0.578404.
Train: 2018-08-06T00:34:17.723742: step 1180, loss 0.522478.
Test: 2018-08-06T00:34:18.982351: step 1180, loss 0.549721.
Train: 2018-08-06T00:34:19.214730: step 1181, loss 0.547013.
Train: 2018-08-06T00:34:19.472067: step 1182, loss 0.5541.
Train: 2018-08-06T00:34:19.714419: step 1183, loss 0.586718.
Train: 2018-08-06T00:34:19.963758: step 1184, loss 0.538278.
Train: 2018-08-06T00:34:20.207076: step 1185, loss 0.53922.
Train: 2018-08-06T00:34:20.454415: step 1186, loss 0.603134.
Train: 2018-08-06T00:34:20.700755: step 1187, loss 0.537754.
Train: 2018-08-06T00:34:20.945102: step 1188, loss 0.490325.
Train: 2018-08-06T00:34:21.188452: step 1189, loss 0.546212.
Train: 2018-08-06T00:34:21.434792: step 1190, loss 0.497502.
Test: 2018-08-06T00:34:22.694423: step 1190, loss 0.548311.
Train: 2018-08-06T00:34:22.928798: step 1191, loss 0.514414.
Train: 2018-08-06T00:34:23.177157: step 1192, loss 0.628654.
Train: 2018-08-06T00:34:23.428460: step 1193, loss 0.547024.
Train: 2018-08-06T00:34:23.676822: step 1194, loss 0.553917.
Train: 2018-08-06T00:34:23.917153: step 1195, loss 0.59561.
Train: 2018-08-06T00:34:24.162499: step 1196, loss 0.563308.
Train: 2018-08-06T00:34:24.418843: step 1197, loss 0.572001.
Train: 2018-08-06T00:34:24.675127: step 1198, loss 0.637092.
Train: 2018-08-06T00:34:24.916507: step 1199, loss 0.570402.
Train: 2018-08-06T00:34:25.169804: step 1200, loss 0.561331.
Test: 2018-08-06T00:34:26.428437: step 1200, loss 0.548766.
Train: 2018-08-06T00:34:27.388101: step 1201, loss 0.5951.
Train: 2018-08-06T00:34:27.635445: step 1202, loss 0.562258.
Train: 2018-08-06T00:34:27.887763: step 1203, loss 0.6032.
Train: 2018-08-06T00:34:28.143055: step 1204, loss 0.472847.
Train: 2018-08-06T00:34:28.387402: step 1205, loss 0.505305.
Train: 2018-08-06T00:34:28.629785: step 1206, loss 0.627392.
Train: 2018-08-06T00:34:28.879118: step 1207, loss 0.59502.
Train: 2018-08-06T00:34:29.072570: step 1208, loss 0.546363.
Train: 2018-08-06T00:34:29.321936: step 1209, loss 0.57895.
Train: 2018-08-06T00:34:29.568244: step 1210, loss 0.554664.
Test: 2018-08-06T00:34:30.838847: step 1210, loss 0.54806.
Train: 2018-08-06T00:34:31.072222: step 1211, loss 0.60286.
Train: 2018-08-06T00:34:31.314575: step 1212, loss 0.595483.
Train: 2018-08-06T00:34:31.569922: step 1213, loss 0.53815.
Train: 2018-08-06T00:34:31.815266: step 1214, loss 0.595814.
Train: 2018-08-06T00:34:32.060609: step 1215, loss 0.521362.
Train: 2018-08-06T00:34:32.316894: step 1216, loss 0.553807.
Train: 2018-08-06T00:34:32.562269: step 1217, loss 0.490429.
Train: 2018-08-06T00:34:32.810573: step 1218, loss 0.506575.
Train: 2018-08-06T00:34:33.053922: step 1219, loss 0.513597.
Train: 2018-08-06T00:34:33.311235: step 1220, loss 0.562934.
Test: 2018-08-06T00:34:34.566876: step 1220, loss 0.548178.
Train: 2018-08-06T00:34:34.801275: step 1221, loss 0.603702.
Train: 2018-08-06T00:34:35.068732: step 1222, loss 0.578974.
Train: 2018-08-06T00:34:35.314135: step 1223, loss 0.537626.
Train: 2018-08-06T00:34:35.566459: step 1224, loss 0.587938.
Train: 2018-08-06T00:34:35.810805: step 1225, loss 0.595437.
Train: 2018-08-06T00:34:36.052161: step 1226, loss 0.537458.
Train: 2018-08-06T00:34:36.295497: step 1227, loss 0.596488.
Train: 2018-08-06T00:34:36.535862: step 1228, loss 0.555023.
Train: 2018-08-06T00:34:36.780183: step 1229, loss 0.530608.
Train: 2018-08-06T00:34:37.026559: step 1230, loss 0.546857.
Test: 2018-08-06T00:34:38.289147: step 1230, loss 0.548407.
Train: 2018-08-06T00:34:38.568410: step 1231, loss 0.563168.
Train: 2018-08-06T00:34:38.813744: step 1232, loss 0.562876.
Train: 2018-08-06T00:34:39.059119: step 1233, loss 0.53767.
Train: 2018-08-06T00:34:39.305457: step 1234, loss 0.553726.
Train: 2018-08-06T00:34:39.547795: step 1235, loss 0.603491.
Train: 2018-08-06T00:34:39.803129: step 1236, loss 0.48936.
Train: 2018-08-06T00:34:40.044478: step 1237, loss 0.571652.
Train: 2018-08-06T00:34:40.286807: step 1238, loss 0.504096.
Train: 2018-08-06T00:34:40.543150: step 1239, loss 0.521016.
Train: 2018-08-06T00:34:40.792453: step 1240, loss 0.520973.
Test: 2018-08-06T00:34:42.039118: step 1240, loss 0.550244.
Train: 2018-08-06T00:34:42.274490: step 1241, loss 0.546061.
Train: 2018-08-06T00:34:42.516842: step 1242, loss 0.537854.
Train: 2018-08-06T00:34:42.760190: step 1243, loss 0.57926.
Train: 2018-08-06T00:34:43.004570: step 1244, loss 0.519101.
Train: 2018-08-06T00:34:43.246889: step 1245, loss 0.57885.
Train: 2018-08-06T00:34:43.493229: step 1246, loss 0.572096.
Train: 2018-08-06T00:34:43.752536: step 1247, loss 0.570185.
Train: 2018-08-06T00:34:43.996919: step 1248, loss 0.605239.
Train: 2018-08-06T00:34:44.242253: step 1249, loss 0.528566.
Train: 2018-08-06T00:34:44.488569: step 1250, loss 0.55439.
Test: 2018-08-06T00:34:45.737228: step 1250, loss 0.548181.
Train: 2018-08-06T00:34:45.971628: step 1251, loss 0.570904.
Train: 2018-08-06T00:34:46.209965: step 1252, loss 0.494304.
Train: 2018-08-06T00:34:46.452347: step 1253, loss 0.519368.
Train: 2018-08-06T00:34:46.695690: step 1254, loss 0.537632.
Train: 2018-08-06T00:34:46.938017: step 1255, loss 0.6221.
Train: 2018-08-06T00:34:47.191340: step 1256, loss 0.56266.
Train: 2018-08-06T00:34:47.438704: step 1257, loss 0.578969.
Train: 2018-08-06T00:34:47.692001: step 1258, loss 0.503245.
Train: 2018-08-06T00:34:47.938370: step 1259, loss 0.545457.
Train: 2018-08-06T00:34:48.182715: step 1260, loss 0.588413.
Test: 2018-08-06T00:34:49.438330: step 1260, loss 0.548498.
Train: 2018-08-06T00:34:49.673732: step 1261, loss 0.587738.
Train: 2018-08-06T00:34:49.919046: step 1262, loss 0.537277.
Train: 2018-08-06T00:34:50.163391: step 1263, loss 0.528291.
Train: 2018-08-06T00:34:50.414744: step 1264, loss 0.5794.
Train: 2018-08-06T00:34:50.663086: step 1265, loss 0.520361.
Train: 2018-08-06T00:34:50.905438: step 1266, loss 0.570147.
Train: 2018-08-06T00:34:51.162720: step 1267, loss 0.545506.
Train: 2018-08-06T00:34:51.409060: step 1268, loss 0.580088.
Train: 2018-08-06T00:34:51.654405: step 1269, loss 0.545221.
Train: 2018-08-06T00:34:51.896756: step 1270, loss 0.59656.
Test: 2018-08-06T00:34:53.144420: step 1270, loss 0.548211.
Train: 2018-08-06T00:34:53.380787: step 1271, loss 0.54511.
Train: 2018-08-06T00:34:53.620172: step 1272, loss 0.604956.
Train: 2018-08-06T00:34:53.861503: step 1273, loss 0.587681.
Train: 2018-08-06T00:34:54.101859: step 1274, loss 0.587608.
Train: 2018-08-06T00:34:54.354215: step 1275, loss 0.621455.
Train: 2018-08-06T00:34:54.611521: step 1276, loss 0.595791.
Train: 2018-08-06T00:34:54.854875: step 1277, loss 0.562488.
Train: 2018-08-06T00:34:55.100215: step 1278, loss 0.529194.
Train: 2018-08-06T00:34:55.348526: step 1279, loss 0.587592.
Train: 2018-08-06T00:34:55.594901: step 1280, loss 0.586902.
Test: 2018-08-06T00:34:56.842529: step 1280, loss 0.549693.
Train: 2018-08-06T00:34:57.075905: step 1281, loss 0.586817.
Train: 2018-08-06T00:34:57.315290: step 1282, loss 0.611407.
Train: 2018-08-06T00:34:57.564599: step 1283, loss 0.514249.
Train: 2018-08-06T00:34:57.807976: step 1284, loss 0.538592.
Train: 2018-08-06T00:34:58.051327: step 1285, loss 0.506639.
Train: 2018-08-06T00:34:58.304622: step 1286, loss 0.522426.
Train: 2018-08-06T00:34:58.547994: step 1287, loss 0.570969.
Train: 2018-08-06T00:34:58.790346: step 1288, loss 0.586844.
Train: 2018-08-06T00:34:59.035665: step 1289, loss 0.522461.
Train: 2018-08-06T00:34:59.278017: step 1290, loss 0.570915.
Test: 2018-08-06T00:35:00.532661: step 1290, loss 0.549646.
Train: 2018-08-06T00:35:00.771028: step 1291, loss 0.562825.
Train: 2018-08-06T00:35:01.028373: step 1292, loss 0.522539.
Train: 2018-08-06T00:35:01.271720: step 1293, loss 0.546387.
Train: 2018-08-06T00:35:01.529026: step 1294, loss 0.522162.
Train: 2018-08-06T00:35:01.770355: step 1295, loss 0.522158.
Train: 2018-08-06T00:35:02.025673: step 1296, loss 0.546339.
Train: 2018-08-06T00:35:02.269052: step 1297, loss 0.537827.
Train: 2018-08-06T00:35:02.512395: step 1298, loss 0.587081.
Train: 2018-08-06T00:35:02.752754: step 1299, loss 0.562725.
Train: 2018-08-06T00:35:03.004056: step 1300, loss 0.554204.
Test: 2018-08-06T00:35:04.266679: step 1300, loss 0.550376.
Train: 2018-08-06T00:35:05.182138: step 1301, loss 0.56218.
Train: 2018-08-06T00:35:05.426485: step 1302, loss 0.587481.
Train: 2018-08-06T00:35:05.681800: step 1303, loss 0.521031.
Train: 2018-08-06T00:35:05.940110: step 1304, loss 0.628847.
Train: 2018-08-06T00:35:06.184456: step 1305, loss 0.512672.
Train: 2018-08-06T00:35:06.434813: step 1306, loss 0.595959.
Train: 2018-08-06T00:35:06.683124: step 1307, loss 0.587408.
Train: 2018-08-06T00:35:06.927470: step 1308, loss 0.562476.
Train: 2018-08-06T00:35:07.171816: step 1309, loss 0.57909.
Train: 2018-08-06T00:35:07.416207: step 1310, loss 0.50455.
Test: 2018-08-06T00:35:08.674797: step 1310, loss 0.548188.
Train: 2018-08-06T00:35:08.914156: step 1311, loss 0.58727.
Train: 2018-08-06T00:35:09.156508: step 1312, loss 0.570722.
Train: 2018-08-06T00:35:09.412823: step 1313, loss 0.587251.
Train: 2018-08-06T00:35:09.658197: step 1314, loss 0.603885.
Train: 2018-08-06T00:35:09.913516: step 1315, loss 0.628648.
Train: 2018-08-06T00:35:10.162818: step 1316, loss 0.554204.
Train: 2018-08-06T00:35:10.414145: step 1317, loss 0.570663.
Train: 2018-08-06T00:35:10.659490: step 1318, loss 0.521636.
Train: 2018-08-06T00:35:10.903837: step 1319, loss 0.529719.
Train: 2018-08-06T00:35:11.148215: step 1320, loss 0.578956.
Test: 2018-08-06T00:35:12.412800: step 1320, loss 0.548803.
Train: 2018-08-06T00:35:12.650166: step 1321, loss 0.554186.
Train: 2018-08-06T00:35:12.904515: step 1322, loss 0.595178.
Train: 2018-08-06T00:35:13.154846: step 1323, loss 0.522043.
Train: 2018-08-06T00:35:13.400190: step 1324, loss 0.570643.
Train: 2018-08-06T00:35:13.654480: step 1325, loss 0.586923.
Train: 2018-08-06T00:35:13.899824: step 1326, loss 0.497398.
Train: 2018-08-06T00:35:14.147175: step 1327, loss 0.587049.
Train: 2018-08-06T00:35:14.392536: step 1328, loss 0.55462.
Train: 2018-08-06T00:35:14.643864: step 1329, loss 0.52213.
Train: 2018-08-06T00:35:14.903166: step 1330, loss 0.635945.
Test: 2018-08-06T00:35:16.184712: step 1330, loss 0.549032.
Train: 2018-08-06T00:35:16.424074: step 1331, loss 0.611514.
Train: 2018-08-06T00:35:16.665457: step 1332, loss 0.603216.
Train: 2018-08-06T00:35:16.918780: step 1333, loss 0.562692.
Train: 2018-08-06T00:35:17.176062: step 1334, loss 0.611354.
Train: 2018-08-06T00:35:17.421406: step 1335, loss 0.619238.
Train: 2018-08-06T00:35:17.660791: step 1336, loss 0.611037.
Train: 2018-08-06T00:35:17.912092: step 1337, loss 0.571109.
Train: 2018-08-06T00:35:18.157438: step 1338, loss 0.563187.
Train: 2018-08-06T00:35:18.409762: step 1339, loss 0.555023.
Train: 2018-08-06T00:35:18.655114: step 1340, loss 0.49218.
Test: 2018-08-06T00:35:19.917729: step 1340, loss 0.549038.
Train: 2018-08-06T00:35:20.151107: step 1341, loss 0.594668.
Train: 2018-08-06T00:35:20.389499: step 1342, loss 0.524011.
Train: 2018-08-06T00:35:20.631819: step 1343, loss 0.563069.
Train: 2018-08-06T00:35:20.880184: step 1344, loss 0.60268.
Train: 2018-08-06T00:35:21.122508: step 1345, loss 0.571066.
Train: 2018-08-06T00:35:21.365857: step 1346, loss 0.570837.
Train: 2018-08-06T00:35:21.611234: step 1347, loss 0.586892.
Train: 2018-08-06T00:35:21.857543: step 1348, loss 0.547616.
Train: 2018-08-06T00:35:22.099925: step 1349, loss 0.54756.
Train: 2018-08-06T00:35:22.344240: step 1350, loss 0.61817.
Test: 2018-08-06T00:35:24.167364: step 1350, loss 0.550317.
Train: 2018-08-06T00:35:24.407752: step 1351, loss 0.563293.
Train: 2018-08-06T00:35:24.663069: step 1352, loss 0.516184.
Train: 2018-08-06T00:35:24.920351: step 1353, loss 0.602527.
Train: 2018-08-06T00:35:25.166693: step 1354, loss 0.563206.
Train: 2018-08-06T00:35:25.415028: step 1355, loss 0.53184.
Train: 2018-08-06T00:35:25.658376: step 1356, loss 0.555311.
Train: 2018-08-06T00:35:25.902724: step 1357, loss 0.555336.
Train: 2018-08-06T00:35:26.151060: step 1358, loss 0.578855.
Train: 2018-08-06T00:35:26.341556: step 1359, loss 0.630178.
Train: 2018-08-06T00:35:26.585896: step 1360, loss 0.531682.
Test: 2018-08-06T00:35:27.840542: step 1360, loss 0.54992.
Train: 2018-08-06T00:35:28.075942: step 1361, loss 0.594754.
Train: 2018-08-06T00:35:28.324248: step 1362, loss 0.555217.
Train: 2018-08-06T00:35:28.568622: step 1363, loss 0.555238.
Train: 2018-08-06T00:35:28.809979: step 1364, loss 0.523677.
Train: 2018-08-06T00:35:29.056299: step 1365, loss 0.570831.
Train: 2018-08-06T00:35:29.298672: step 1366, loss 0.562961.
Train: 2018-08-06T00:35:29.550966: step 1367, loss 0.586804.
Train: 2018-08-06T00:35:29.796312: step 1368, loss 0.618577.
Train: 2018-08-06T00:35:30.041670: step 1369, loss 0.626501.
Train: 2018-08-06T00:35:30.292983: step 1370, loss 0.618615.
Test: 2018-08-06T00:35:31.581537: step 1370, loss 0.550214.
Train: 2018-08-06T00:35:31.814911: step 1371, loss 0.602585.
Train: 2018-08-06T00:35:32.059284: step 1372, loss 0.555233.
Train: 2018-08-06T00:35:32.312581: step 1373, loss 0.539577.
Train: 2018-08-06T00:35:32.563942: step 1374, loss 0.500536.
Train: 2018-08-06T00:35:32.810284: step 1375, loss 0.4848.
Train: 2018-08-06T00:35:33.067563: step 1376, loss 0.547481.
Train: 2018-08-06T00:35:33.312906: step 1377, loss 0.618281.
Train: 2018-08-06T00:35:33.559248: step 1378, loss 0.626141.
Train: 2018-08-06T00:35:33.824562: step 1379, loss 0.547308.
Train: 2018-08-06T00:35:34.080852: step 1380, loss 0.54729.
Test: 2018-08-06T00:35:35.342479: step 1380, loss 0.550571.
Train: 2018-08-06T00:35:35.576876: step 1381, loss 0.547291.
Train: 2018-08-06T00:35:35.819204: step 1382, loss 0.602617.
Train: 2018-08-06T00:35:36.064578: step 1383, loss 0.578903.
Train: 2018-08-06T00:35:36.307895: step 1384, loss 0.634356.
Train: 2018-08-06T00:35:36.558226: step 1385, loss 0.507681.
Train: 2018-08-06T00:35:36.813544: step 1386, loss 0.578865.
Train: 2018-08-06T00:35:37.068861: step 1387, loss 0.539323.
Train: 2018-08-06T00:35:37.309248: step 1388, loss 0.563073.
Train: 2018-08-06T00:35:37.551571: step 1389, loss 0.578848.
Train: 2018-08-06T00:35:37.795917: step 1390, loss 0.594765.
Test: 2018-08-06T00:35:39.046573: step 1390, loss 0.551062.
Train: 2018-08-06T00:35:39.282941: step 1391, loss 0.563022.
Train: 2018-08-06T00:35:39.525315: step 1392, loss 0.626426.
Train: 2018-08-06T00:35:39.768641: step 1393, loss 0.539262.
Train: 2018-08-06T00:35:40.019001: step 1394, loss 0.539393.
Train: 2018-08-06T00:35:40.280273: step 1395, loss 0.555151.
Train: 2018-08-06T00:35:40.535590: step 1396, loss 0.547294.
Train: 2018-08-06T00:35:40.778939: step 1397, loss 0.515484.
Train: 2018-08-06T00:35:41.020304: step 1398, loss 0.594785.
Train: 2018-08-06T00:35:41.264641: step 1399, loss 0.547121.
Train: 2018-08-06T00:35:41.511013: step 1400, loss 0.555016.
Test: 2018-08-06T00:35:42.805525: step 1400, loss 0.548846.
Train: 2018-08-06T00:35:43.683923: step 1401, loss 0.531083.
Train: 2018-08-06T00:35:43.927246: step 1402, loss 0.514962.
Train: 2018-08-06T00:35:44.171620: step 1403, loss 0.546832.
Train: 2018-08-06T00:35:44.429928: step 1404, loss 0.55468.
Train: 2018-08-06T00:35:44.673251: step 1405, loss 0.538369.
Train: 2018-08-06T00:35:44.917599: step 1406, loss 0.546428.
Train: 2018-08-06T00:35:45.163970: step 1407, loss 0.489256.
Train: 2018-08-06T00:35:45.405325: step 1408, loss 0.538022.
Train: 2018-08-06T00:35:45.648676: step 1409, loss 0.570834.
Train: 2018-08-06T00:35:45.904989: step 1410, loss 0.579143.
Test: 2018-08-06T00:35:47.163592: step 1410, loss 0.548783.
Train: 2018-08-06T00:35:47.397966: step 1411, loss 0.520904.
Train: 2018-08-06T00:35:47.643308: step 1412, loss 0.479124.
Train: 2018-08-06T00:35:47.890673: step 1413, loss 0.537254.
Train: 2018-08-06T00:35:48.133998: step 1414, loss 0.579094.
Train: 2018-08-06T00:35:48.383331: step 1415, loss 0.519932.
Train: 2018-08-06T00:35:48.629670: step 1416, loss 0.519836.
Train: 2018-08-06T00:35:48.880999: step 1417, loss 0.579404.
Train: 2018-08-06T00:35:49.128338: step 1418, loss 0.5366.
Train: 2018-08-06T00:35:49.372684: step 1419, loss 0.622579.
Train: 2018-08-06T00:35:49.619024: step 1420, loss 0.5709.
Test: 2018-08-06T00:35:50.883643: step 1420, loss 0.549321.
Train: 2018-08-06T00:35:51.124025: step 1421, loss 0.519109.
Train: 2018-08-06T00:35:51.366383: step 1422, loss 0.605337.
Train: 2018-08-06T00:35:51.607738: step 1423, loss 0.562244.
Train: 2018-08-06T00:35:51.858068: step 1424, loss 0.570883.
Train: 2018-08-06T00:35:52.106373: step 1425, loss 0.570842.
Train: 2018-08-06T00:35:52.351722: step 1426, loss 0.536496.
Train: 2018-08-06T00:35:52.597092: step 1427, loss 0.545114.
Train: 2018-08-06T00:35:52.846395: step 1428, loss 0.570942.
Train: 2018-08-06T00:35:53.097722: step 1429, loss 0.570999.
Train: 2018-08-06T00:35:53.341096: step 1430, loss 0.527906.
Test: 2018-08-06T00:35:54.611672: step 1430, loss 0.548371.
Train: 2018-08-06T00:35:54.857016: step 1431, loss 0.45915.
Train: 2018-08-06T00:35:55.099395: step 1432, loss 0.545199.
Train: 2018-08-06T00:35:55.357679: step 1433, loss 0.501772.
Train: 2018-08-06T00:35:55.610004: step 1434, loss 0.545095.
Train: 2018-08-06T00:35:55.857373: step 1435, loss 0.553607.
Train: 2018-08-06T00:35:56.098725: step 1436, loss 0.501628.
Train: 2018-08-06T00:35:56.357014: step 1437, loss 0.605898.
Train: 2018-08-06T00:35:56.603348: step 1438, loss 0.501476.
Train: 2018-08-06T00:35:56.850685: step 1439, loss 0.640749.
Train: 2018-08-06T00:35:57.093038: step 1440, loss 0.475103.
Test: 2018-08-06T00:35:58.359649: step 1440, loss 0.548988.
Train: 2018-08-06T00:35:58.594047: step 1441, loss 0.571205.
Train: 2018-08-06T00:35:58.836376: step 1442, loss 0.614571.
Train: 2018-08-06T00:35:59.083715: step 1443, loss 0.597235.
Train: 2018-08-06T00:35:59.329058: step 1444, loss 0.579616.
Train: 2018-08-06T00:35:59.572445: step 1445, loss 0.571096.
Train: 2018-08-06T00:35:59.816752: step 1446, loss 0.518945.
Train: 2018-08-06T00:36:00.063120: step 1447, loss 0.536249.
Train: 2018-08-06T00:36:00.307470: step 1448, loss 0.588191.
Train: 2018-08-06T00:36:00.547797: step 1449, loss 0.562279.
Train: 2018-08-06T00:36:00.788180: step 1450, loss 0.622699.
Test: 2018-08-06T00:36:02.054767: step 1450, loss 0.546826.
Train: 2018-08-06T00:36:02.288174: step 1451, loss 0.587955.
Train: 2018-08-06T00:36:02.533518: step 1452, loss 0.493839.
Train: 2018-08-06T00:36:02.778856: step 1453, loss 0.613431.
Train: 2018-08-06T00:36:03.027200: step 1454, loss 0.562417.
Train: 2018-08-06T00:36:03.270547: step 1455, loss 0.630145.
Train: 2018-08-06T00:36:03.515888: step 1456, loss 0.553982.
Train: 2018-08-06T00:36:03.764196: step 1457, loss 0.461805.
Train: 2018-08-06T00:36:04.020511: step 1458, loss 0.56224.
Train: 2018-08-06T00:36:04.261866: step 1459, loss 0.612514.
Train: 2018-08-06T00:36:04.515218: step 1460, loss 0.554016.
Test: 2018-08-06T00:36:05.780802: step 1460, loss 0.548747.
Train: 2018-08-06T00:36:06.029140: step 1461, loss 0.562391.
Train: 2018-08-06T00:36:06.273485: step 1462, loss 0.512631.
Train: 2018-08-06T00:36:06.529801: step 1463, loss 0.570816.
Train: 2018-08-06T00:36:06.782126: step 1464, loss 0.545815.
Train: 2018-08-06T00:36:07.025475: step 1465, loss 0.595558.
Train: 2018-08-06T00:36:07.271816: step 1466, loss 0.562447.
Train: 2018-08-06T00:36:07.515191: step 1467, loss 0.545924.
Train: 2018-08-06T00:36:07.762548: step 1468, loss 0.587291.
Train: 2018-08-06T00:36:08.005853: step 1469, loss 0.603421.
Train: 2018-08-06T00:36:08.248205: step 1470, loss 0.537952.
Test: 2018-08-06T00:36:09.501851: step 1470, loss 0.549436.
Train: 2018-08-06T00:36:09.736224: step 1471, loss 0.554512.
Train: 2018-08-06T00:36:09.981602: step 1472, loss 0.546185.
Train: 2018-08-06T00:36:10.229935: step 1473, loss 0.59508.
Train: 2018-08-06T00:36:10.483253: step 1474, loss 0.579016.
Train: 2018-08-06T00:36:10.726575: step 1475, loss 0.6523.
Train: 2018-08-06T00:36:10.969927: step 1476, loss 0.611361.
Train: 2018-08-06T00:36:11.222276: step 1477, loss 0.586953.
Train: 2018-08-06T00:36:11.467595: step 1478, loss 0.562735.
Train: 2018-08-06T00:36:11.713962: step 1479, loss 0.522682.
Train: 2018-08-06T00:36:11.960281: step 1480, loss 0.482698.
Test: 2018-08-06T00:36:13.223898: step 1480, loss 0.548776.
Train: 2018-08-06T00:36:13.459269: step 1481, loss 0.563082.
Train: 2018-08-06T00:36:13.703645: step 1482, loss 0.578753.
Train: 2018-08-06T00:36:13.947960: step 1483, loss 0.650662.
Train: 2018-08-06T00:36:14.189346: step 1484, loss 0.539019.
Train: 2018-08-06T00:36:14.432664: step 1485, loss 0.634529.
Train: 2018-08-06T00:36:14.674020: step 1486, loss 0.523165.
Train: 2018-08-06T00:36:14.922389: step 1487, loss 0.5711.
Train: 2018-08-06T00:36:15.164735: step 1488, loss 0.547438.
Train: 2018-08-06T00:36:15.408057: step 1489, loss 0.657921.
Train: 2018-08-06T00:36:15.648414: step 1490, loss 0.555276.
Test: 2018-08-06T00:36:16.946956: step 1490, loss 0.548726.
Train: 2018-08-06T00:36:17.185305: step 1491, loss 0.563068.
Train: 2018-08-06T00:36:17.427654: step 1492, loss 0.571221.
Train: 2018-08-06T00:36:17.673997: step 1493, loss 0.570838.
Train: 2018-08-06T00:36:17.922366: step 1494, loss 0.63366.
Train: 2018-08-06T00:36:18.164712: step 1495, loss 0.516562.
Train: 2018-08-06T00:36:18.414048: step 1496, loss 0.563365.
Train: 2018-08-06T00:36:18.660385: step 1497, loss 0.508687.
Train: 2018-08-06T00:36:18.900715: step 1498, loss 0.516504.
Train: 2018-08-06T00:36:19.158028: step 1499, loss 0.555385.
Train: 2018-08-06T00:36:19.416337: step 1500, loss 0.578822.
Test: 2018-08-06T00:36:20.679957: step 1500, loss 0.549521.
Train: 2018-08-06T00:36:21.619008: step 1501, loss 0.610063.
Train: 2018-08-06T00:36:21.873326: step 1502, loss 0.53949.
Train: 2018-08-06T00:36:22.115680: step 1503, loss 0.563431.
Train: 2018-08-06T00:36:22.360053: step 1504, loss 0.626247.
Train: 2018-08-06T00:36:22.608362: step 1505, loss 0.492001.
Train: 2018-08-06T00:36:22.864700: step 1506, loss 0.539581.
Train: 2018-08-06T00:36:23.108055: step 1507, loss 0.594736.
Train: 2018-08-06T00:36:23.355389: step 1508, loss 0.539037.
Train: 2018-08-06T00:36:23.602741: step 1509, loss 0.579028.
Train: 2018-08-06T00:36:23.796184: step 1510, loss 0.647559.
Test: 2018-08-06T00:36:25.066785: step 1510, loss 0.549443.
Train: 2018-08-06T00:36:25.310137: step 1511, loss 0.531022.
Train: 2018-08-06T00:36:25.562486: step 1512, loss 0.626901.
Train: 2018-08-06T00:36:25.822790: step 1513, loss 0.483404.
Train: 2018-08-06T00:36:26.067145: step 1514, loss 0.586782.
Train: 2018-08-06T00:36:26.310471: step 1515, loss 0.562815.
Train: 2018-08-06T00:36:26.555832: step 1516, loss 0.595082.
Train: 2018-08-06T00:36:26.801183: step 1517, loss 0.490968.
Train: 2018-08-06T00:36:27.056465: step 1518, loss 0.587223.
Train: 2018-08-06T00:36:27.303834: step 1519, loss 0.54665.
Train: 2018-08-06T00:36:27.565135: step 1520, loss 0.603029.
Test: 2018-08-06T00:36:28.825733: step 1520, loss 0.548137.
Train: 2018-08-06T00:36:29.065124: step 1521, loss 0.59494.
Train: 2018-08-06T00:36:29.309466: step 1522, loss 0.506567.
Train: 2018-08-06T00:36:29.556779: step 1523, loss 0.562758.
Train: 2018-08-06T00:36:29.798167: step 1524, loss 0.691215.
Train: 2018-08-06T00:36:30.042507: step 1525, loss 0.570598.
Train: 2018-08-06T00:36:30.299821: step 1526, loss 0.562835.
Train: 2018-08-06T00:36:30.542144: step 1527, loss 0.578784.
Train: 2018-08-06T00:36:30.783499: step 1528, loss 0.570714.
Train: 2018-08-06T00:36:31.031834: step 1529, loss 0.538938.
Train: 2018-08-06T00:36:31.278200: step 1530, loss 0.57107.
Test: 2018-08-06T00:36:32.551768: step 1530, loss 0.550024.
Train: 2018-08-06T00:36:32.795146: step 1531, loss 0.59469.
Train: 2018-08-06T00:36:33.042489: step 1532, loss 0.53923.
Train: 2018-08-06T00:36:33.298796: step 1533, loss 0.53905.
Train: 2018-08-06T00:36:33.549128: step 1534, loss 0.586563.
Train: 2018-08-06T00:36:33.799431: step 1535, loss 0.618708.
Train: 2018-08-06T00:36:34.041783: step 1536, loss 0.562871.
Train: 2018-08-06T00:36:34.286156: step 1537, loss 0.562791.
Train: 2018-08-06T00:36:34.537457: step 1538, loss 0.49183.
Train: 2018-08-06T00:36:34.786793: step 1539, loss 0.586953.
Train: 2018-08-06T00:36:35.033158: step 1540, loss 0.507662.
Test: 2018-08-06T00:36:36.368621: step 1540, loss 0.549856.
Train: 2018-08-06T00:36:36.610002: step 1541, loss 0.539131.
Train: 2018-08-06T00:36:36.857345: step 1542, loss 0.467167.
Train: 2018-08-06T00:36:37.108667: step 1543, loss 0.554953.
Train: 2018-08-06T00:36:37.360967: step 1544, loss 0.578842.
Train: 2018-08-06T00:36:37.604317: step 1545, loss 0.61953.
Train: 2018-08-06T00:36:37.850684: step 1546, loss 0.58706.
Train: 2018-08-06T00:36:38.096998: step 1547, loss 0.570562.
Train: 2018-08-06T00:36:38.339379: step 1548, loss 0.587212.
Train: 2018-08-06T00:36:38.595697: step 1549, loss 0.530055.
Train: 2018-08-06T00:36:38.839014: step 1550, loss 0.619236.
Test: 2018-08-06T00:36:40.104630: step 1550, loss 0.548699.
Train: 2018-08-06T00:36:40.362939: step 1551, loss 0.579153.
Train: 2018-08-06T00:36:40.607311: step 1552, loss 0.554683.
Train: 2018-08-06T00:36:40.853657: step 1553, loss 0.546201.
Train: 2018-08-06T00:36:41.104988: step 1554, loss 0.619203.
Train: 2018-08-06T00:36:41.352318: step 1555, loss 0.554606.
Train: 2018-08-06T00:36:41.600654: step 1556, loss 0.603025.
Train: 2018-08-06T00:36:41.855956: step 1557, loss 0.546679.
Train: 2018-08-06T00:36:42.102314: step 1558, loss 0.579139.
Train: 2018-08-06T00:36:42.348656: step 1559, loss 0.63506.
Train: 2018-08-06T00:36:42.593002: step 1560, loss 0.554966.
Test: 2018-08-06T00:36:43.859588: step 1560, loss 0.548862.
Train: 2018-08-06T00:36:44.094983: step 1561, loss 0.490675.
Train: 2018-08-06T00:36:44.335346: step 1562, loss 0.546664.
Train: 2018-08-06T00:36:44.578693: step 1563, loss 0.506796.
Train: 2018-08-06T00:36:44.834996: step 1564, loss 0.498404.
Train: 2018-08-06T00:36:45.080324: step 1565, loss 0.51423.
Train: 2018-08-06T00:36:45.323705: step 1566, loss 0.554405.
Train: 2018-08-06T00:36:45.579023: step 1567, loss 0.546155.
Train: 2018-08-06T00:36:45.834307: step 1568, loss 0.554622.
Train: 2018-08-06T00:36:46.078654: step 1569, loss 0.546263.
Train: 2018-08-06T00:36:46.325999: step 1570, loss 0.496611.
Test: 2018-08-06T00:36:47.597592: step 1570, loss 0.547126.
Train: 2018-08-06T00:36:47.832961: step 1571, loss 0.521177.
Train: 2018-08-06T00:36:48.080301: step 1572, loss 0.554029.
Train: 2018-08-06T00:36:48.331627: step 1573, loss 0.521241.
Train: 2018-08-06T00:36:48.577970: step 1574, loss 0.536573.
Train: 2018-08-06T00:36:48.819359: step 1575, loss 0.553646.
Train: 2018-08-06T00:36:49.062673: step 1576, loss 0.587698.
Train: 2018-08-06T00:36:49.309015: step 1577, loss 0.55534.
Train: 2018-08-06T00:36:49.553392: step 1578, loss 0.596532.
Train: 2018-08-06T00:36:49.802693: step 1579, loss 0.587984.
Train: 2018-08-06T00:36:50.048063: step 1580, loss 0.571406.
Test: 2018-08-06T00:36:51.302682: step 1580, loss 0.548415.
Train: 2018-08-06T00:36:51.537087: step 1581, loss 0.536959.
Train: 2018-08-06T00:36:51.781402: step 1582, loss 0.639639.
Train: 2018-08-06T00:36:52.033728: step 1583, loss 0.528152.
Train: 2018-08-06T00:36:52.277076: step 1584, loss 0.54544.
Train: 2018-08-06T00:36:52.519428: step 1585, loss 0.571211.
Train: 2018-08-06T00:36:52.759785: step 1586, loss 0.528055.
Train: 2018-08-06T00:36:53.005155: step 1587, loss 0.605323.
Train: 2018-08-06T00:36:53.250473: step 1588, loss 0.511617.
Train: 2018-08-06T00:36:53.493823: step 1589, loss 0.51163.
Train: 2018-08-06T00:36:53.740163: step 1590, loss 0.553984.
Test: 2018-08-06T00:36:54.986830: step 1590, loss 0.549156.
Train: 2018-08-06T00:36:55.227212: step 1591, loss 0.554331.
Train: 2018-08-06T00:36:55.475556: step 1592, loss 0.562352.
Train: 2018-08-06T00:36:55.717906: step 1593, loss 0.604895.
Train: 2018-08-06T00:36:55.960253: step 1594, loss 0.477616.
Train: 2018-08-06T00:36:56.210588: step 1595, loss 0.613594.
Train: 2018-08-06T00:36:56.452940: step 1596, loss 0.48621.
Train: 2018-08-06T00:36:56.702267: step 1597, loss 0.511688.
Train: 2018-08-06T00:36:56.954598: step 1598, loss 0.570685.
Train: 2018-08-06T00:36:57.214871: step 1599, loss 0.596026.
Train: 2018-08-06T00:36:57.471211: step 1600, loss 0.596327.
Test: 2018-08-06T00:36:58.726827: step 1600, loss 0.548575.
Train: 2018-08-06T00:36:59.645085: step 1601, loss 0.579486.
Train: 2018-08-06T00:36:59.891451: step 1602, loss 0.613037.
Train: 2018-08-06T00:37:00.143785: step 1603, loss 0.528654.
Train: 2018-08-06T00:37:00.390094: step 1604, loss 0.579071.
Train: 2018-08-06T00:37:00.650414: step 1605, loss 0.528601.
Train: 2018-08-06T00:37:00.893793: step 1606, loss 0.511866.
Train: 2018-08-06T00:37:01.131137: step 1607, loss 0.512049.
Train: 2018-08-06T00:37:01.391415: step 1608, loss 0.620935.
Train: 2018-08-06T00:37:01.633766: step 1609, loss 0.612914.
Train: 2018-08-06T00:37:01.887090: step 1610, loss 0.545638.
Test: 2018-08-06T00:37:03.154699: step 1610, loss 0.548038.
Train: 2018-08-06T00:37:03.389103: step 1611, loss 0.56224.
Train: 2018-08-06T00:37:03.630451: step 1612, loss 0.570732.
Train: 2018-08-06T00:37:03.870810: step 1613, loss 0.537466.
Train: 2018-08-06T00:37:04.116154: step 1614, loss 0.570913.
Train: 2018-08-06T00:37:04.364463: step 1615, loss 0.520689.
Train: 2018-08-06T00:37:04.608811: step 1616, loss 0.662211.
Train: 2018-08-06T00:37:04.851193: step 1617, loss 0.595626.
Train: 2018-08-06T00:37:05.101493: step 1618, loss 0.603809.
Train: 2018-08-06T00:37:05.345839: step 1619, loss 0.554388.
Train: 2018-08-06T00:37:05.588190: step 1620, loss 0.644683.
Test: 2018-08-06T00:37:06.846825: step 1620, loss 0.548754.
Train: 2018-08-06T00:37:07.079203: step 1621, loss 0.562676.
Train: 2018-08-06T00:37:07.330563: step 1622, loss 0.554544.
Train: 2018-08-06T00:37:07.576898: step 1623, loss 0.603187.
Train: 2018-08-06T00:37:07.817230: step 1624, loss 0.643315.
Train: 2018-08-06T00:37:08.059581: step 1625, loss 0.498801.
Train: 2018-08-06T00:37:08.303954: step 1626, loss 0.562851.
Train: 2018-08-06T00:37:08.552264: step 1627, loss 0.570858.
Train: 2018-08-06T00:37:08.792620: step 1628, loss 0.563037.
Train: 2018-08-06T00:37:09.037966: step 1629, loss 0.562986.
Train: 2018-08-06T00:37:09.280343: step 1630, loss 0.499829.
Test: 2018-08-06T00:37:10.547927: step 1630, loss 0.550056.
Train: 2018-08-06T00:37:10.792273: step 1631, loss 0.578819.
Train: 2018-08-06T00:37:11.044630: step 1632, loss 0.634179.
Train: 2018-08-06T00:37:11.289976: step 1633, loss 0.531513.
Train: 2018-08-06T00:37:11.542269: step 1634, loss 0.57888.
Train: 2018-08-06T00:37:11.792599: step 1635, loss 0.602414.
Train: 2018-08-06T00:37:12.033978: step 1636, loss 0.602482.
Train: 2018-08-06T00:37:12.286278: step 1637, loss 0.602504.
Train: 2018-08-06T00:37:12.530656: step 1638, loss 0.555468.
Train: 2018-08-06T00:37:12.787963: step 1639, loss 0.547684.
Train: 2018-08-06T00:37:13.033306: step 1640, loss 0.547738.
Test: 2018-08-06T00:37:14.336793: step 1640, loss 0.54891.
Train: 2018-08-06T00:37:14.575188: step 1641, loss 0.547859.
Train: 2018-08-06T00:37:14.815539: step 1642, loss 0.555533.
Train: 2018-08-06T00:37:15.072851: step 1643, loss 0.602336.
Train: 2018-08-06T00:37:15.313182: step 1644, loss 0.477712.
Train: 2018-08-06T00:37:15.556532: step 1645, loss 0.579037.
Train: 2018-08-06T00:37:15.798883: step 1646, loss 0.539769.
Train: 2018-08-06T00:37:16.047250: step 1647, loss 0.594572.
Train: 2018-08-06T00:37:16.295587: step 1648, loss 0.602481.
Train: 2018-08-06T00:37:16.546915: step 1649, loss 0.610434.
Train: 2018-08-06T00:37:16.803197: step 1650, loss 0.500362.
Test: 2018-08-06T00:37:18.057843: step 1650, loss 0.551095.
Train: 2018-08-06T00:37:18.293238: step 1651, loss 0.547425.
Train: 2018-08-06T00:37:18.545545: step 1652, loss 0.539505.
Train: 2018-08-06T00:37:18.793875: step 1653, loss 0.562938.
Train: 2018-08-06T00:37:19.040242: step 1654, loss 0.570887.
Train: 2018-08-06T00:37:19.285585: step 1655, loss 0.539225.
Train: 2018-08-06T00:37:19.528940: step 1656, loss 0.539143.
Train: 2018-08-06T00:37:19.779259: step 1657, loss 0.531034.
Train: 2018-08-06T00:37:20.030593: step 1658, loss 0.562883.
Train: 2018-08-06T00:37:20.279947: step 1659, loss 0.506594.
Train: 2018-08-06T00:37:20.524247: step 1660, loss 0.578821.
Test: 2018-08-06T00:37:21.779889: step 1660, loss 0.549532.
Train: 2018-08-06T00:37:21.957439: step 1661, loss 0.545455.
Train: 2018-08-06T00:37:22.201760: step 1662, loss 0.521984.
Train: 2018-08-06T00:37:22.446106: step 1663, loss 0.554394.
Train: 2018-08-06T00:37:22.692473: step 1664, loss 0.521543.
Train: 2018-08-06T00:37:22.949787: step 1665, loss 0.587183.
Train: 2018-08-06T00:37:23.193109: step 1666, loss 0.587358.
Train: 2018-08-06T00:37:23.439450: step 1667, loss 0.529215.
Train: 2018-08-06T00:37:23.682799: step 1668, loss 0.529284.
Train: 2018-08-06T00:37:23.932164: step 1669, loss 0.537434.
Train: 2018-08-06T00:37:24.174485: step 1670, loss 0.587495.
Test: 2018-08-06T00:37:25.459049: step 1670, loss 0.548569.
Train: 2018-08-06T00:37:25.707386: step 1671, loss 0.528819.
Train: 2018-08-06T00:37:25.958739: step 1672, loss 0.587734.
Train: 2018-08-06T00:37:26.203060: step 1673, loss 0.520226.
Train: 2018-08-06T00:37:26.447433: step 1674, loss 0.56259.
Train: 2018-08-06T00:37:26.692750: step 1675, loss 0.528681.
Train: 2018-08-06T00:37:26.939091: step 1676, loss 0.613201.
Train: 2018-08-06T00:37:27.180446: step 1677, loss 0.570936.
Train: 2018-08-06T00:37:27.438789: step 1678, loss 0.579261.
Train: 2018-08-06T00:37:27.685097: step 1679, loss 0.61335.
Train: 2018-08-06T00:37:27.935427: step 1680, loss 0.587755.
Test: 2018-08-06T00:37:29.194060: step 1680, loss 0.548444.
Train: 2018-08-06T00:37:29.430443: step 1681, loss 0.511617.
Train: 2018-08-06T00:37:29.676770: step 1682, loss 0.553831.
Train: 2018-08-06T00:37:29.920145: step 1683, loss 0.638272.
Train: 2018-08-06T00:37:30.170450: step 1684, loss 0.612827.
Train: 2018-08-06T00:37:30.423806: step 1685, loss 0.537256.
Train: 2018-08-06T00:37:30.675125: step 1686, loss 0.595898.
Train: 2018-08-06T00:37:30.927426: step 1687, loss 0.545875.
Train: 2018-08-06T00:37:31.180800: step 1688, loss 0.595446.
Train: 2018-08-06T00:37:31.424125: step 1689, loss 0.620407.
Train: 2018-08-06T00:37:31.665452: step 1690, loss 0.51326.
Test: 2018-08-06T00:37:32.960986: step 1690, loss 0.549238.
Train: 2018-08-06T00:37:33.198377: step 1691, loss 0.505208.
Train: 2018-08-06T00:37:33.442700: step 1692, loss 0.587177.
Train: 2018-08-06T00:37:33.694026: step 1693, loss 0.521776.
Train: 2018-08-06T00:37:33.953332: step 1694, loss 0.579046.
Train: 2018-08-06T00:37:34.196716: step 1695, loss 0.538134.
Train: 2018-08-06T00:37:34.439034: step 1696, loss 0.546248.
Train: 2018-08-06T00:37:34.683380: step 1697, loss 0.497368.
Train: 2018-08-06T00:37:34.931743: step 1698, loss 0.529893.
Train: 2018-08-06T00:37:35.183045: step 1699, loss 0.554345.
Train: 2018-08-06T00:37:35.438362: step 1700, loss 0.578954.
Test: 2018-08-06T00:37:36.698989: step 1700, loss 0.548066.
Train: 2018-08-06T00:37:37.634754: step 1701, loss 0.537889.
Train: 2018-08-06T00:37:37.881126: step 1702, loss 0.58731.
Train: 2018-08-06T00:37:38.122477: step 1703, loss 0.55435.
Train: 2018-08-06T00:37:38.368790: step 1704, loss 0.554248.
Train: 2018-08-06T00:37:38.615162: step 1705, loss 0.463453.
Train: 2018-08-06T00:37:38.858481: step 1706, loss 0.603934.
Train: 2018-08-06T00:37:39.111831: step 1707, loss 0.52097.
Train: 2018-08-06T00:37:39.359167: step 1708, loss 0.537453.
Train: 2018-08-06T00:37:39.600497: step 1709, loss 0.595795.
Train: 2018-08-06T00:37:39.855847: step 1710, loss 0.595904.
Test: 2018-08-06T00:37:41.120431: step 1710, loss 0.548449.
Train: 2018-08-06T00:37:41.366801: step 1711, loss 0.595874.
Train: 2018-08-06T00:37:41.611152: step 1712, loss 0.570763.
Train: 2018-08-06T00:37:41.853498: step 1713, loss 0.587432.
Train: 2018-08-06T00:37:42.093859: step 1714, loss 0.604112.
Train: 2018-08-06T00:37:42.338188: step 1715, loss 0.579063.
Train: 2018-08-06T00:37:42.584541: step 1716, loss 0.562448.
Train: 2018-08-06T00:37:42.826892: step 1717, loss 0.59561.
Train: 2018-08-06T00:37:43.075235: step 1718, loss 0.488164.
Train: 2018-08-06T00:37:43.323540: step 1719, loss 0.529506.
Train: 2018-08-06T00:37:43.566920: step 1720, loss 0.570687.
Test: 2018-08-06T00:37:44.837490: step 1720, loss 0.547988.
Train: 2018-08-06T00:37:45.084862: step 1721, loss 0.504822.
Train: 2018-08-06T00:37:45.338182: step 1722, loss 0.52126.
Train: 2018-08-06T00:37:45.583527: step 1723, loss 0.521152.
Train: 2018-08-06T00:37:45.839835: step 1724, loss 0.570731.
Train: 2018-08-06T00:37:46.084187: step 1725, loss 0.562496.
Train: 2018-08-06T00:37:46.330497: step 1726, loss 0.570736.
Train: 2018-08-06T00:37:46.574875: step 1727, loss 0.637363.
Train: 2018-08-06T00:37:46.820219: step 1728, loss 0.512529.
Train: 2018-08-06T00:37:47.064536: step 1729, loss 0.628947.
Train: 2018-08-06T00:37:47.311905: step 1730, loss 0.612393.
Test: 2018-08-06T00:37:48.590453: step 1730, loss 0.548622.
Train: 2018-08-06T00:37:48.825862: step 1731, loss 0.603859.
Train: 2018-08-06T00:37:49.080144: step 1732, loss 0.595535.
Train: 2018-08-06T00:37:49.322496: step 1733, loss 0.587178.
Train: 2018-08-06T00:37:49.579836: step 1734, loss 0.587224.
Train: 2018-08-06T00:37:49.819196: step 1735, loss 0.546294.
Train: 2018-08-06T00:37:50.058560: step 1736, loss 0.522067.
Train: 2018-08-06T00:37:50.304869: step 1737, loss 0.546435.
Train: 2018-08-06T00:37:50.558192: step 1738, loss 0.619443.
Train: 2018-08-06T00:37:50.800543: step 1739, loss 0.538426.
Train: 2018-08-06T00:37:51.047883: step 1740, loss 0.594982.
Test: 2018-08-06T00:37:52.302526: step 1740, loss 0.549327.
Train: 2018-08-06T00:37:52.541886: step 1741, loss 0.55474.
Train: 2018-08-06T00:37:52.787263: step 1742, loss 0.538714.
Train: 2018-08-06T00:37:53.030580: step 1743, loss 0.530694.
Train: 2018-08-06T00:37:53.274956: step 1744, loss 0.554823.
Train: 2018-08-06T00:37:53.520271: step 1745, loss 0.578856.
Train: 2018-08-06T00:37:53.763649: step 1746, loss 0.522681.
Train: 2018-08-06T00:37:54.009988: step 1747, loss 0.562804.
Train: 2018-08-06T00:37:54.255303: step 1748, loss 0.450148.
Train: 2018-08-06T00:37:54.514641: step 1749, loss 0.603115.
Train: 2018-08-06T00:37:54.757977: step 1750, loss 0.570789.
Test: 2018-08-06T00:37:56.018588: step 1750, loss 0.548022.
Train: 2018-08-06T00:37:56.254985: step 1751, loss 0.546414.
Train: 2018-08-06T00:37:56.498305: step 1752, loss 0.578945.
Train: 2018-08-06T00:37:56.741655: step 1753, loss 0.562638.
Train: 2018-08-06T00:37:56.986029: step 1754, loss 0.521858.
Train: 2018-08-06T00:37:57.233373: step 1755, loss 0.578971.
Train: 2018-08-06T00:37:57.478712: step 1756, loss 0.562583.
Train: 2018-08-06T00:37:57.735024: step 1757, loss 0.554338.
Train: 2018-08-06T00:37:57.990316: step 1758, loss 0.595378.
Train: 2018-08-06T00:37:58.233664: step 1759, loss 0.50501.
Train: 2018-08-06T00:37:58.481003: step 1760, loss 0.504913.
Test: 2018-08-06T00:37:59.740634: step 1760, loss 0.548728.
Train: 2018-08-06T00:37:59.976004: step 1761, loss 0.562435.
Train: 2018-08-06T00:38:00.226364: step 1762, loss 0.545934.
Train: 2018-08-06T00:38:00.485642: step 1763, loss 0.587308.
Train: 2018-08-06T00:38:00.733990: step 1764, loss 0.620581.
Train: 2018-08-06T00:38:00.980319: step 1765, loss 0.545838.
Train: 2018-08-06T00:38:01.234664: step 1766, loss 0.520872.
Train: 2018-08-06T00:38:01.476026: step 1767, loss 0.620699.
Train: 2018-08-06T00:38:01.723360: step 1768, loss 0.562405.
Train: 2018-08-06T00:38:01.977651: step 1769, loss 0.545766.
Train: 2018-08-06T00:38:02.222995: step 1770, loss 0.579129.
Test: 2018-08-06T00:38:03.475645: step 1770, loss 0.547233.
Train: 2018-08-06T00:38:03.709052: step 1771, loss 0.587503.
Train: 2018-08-06T00:38:03.951399: step 1772, loss 0.553938.
Train: 2018-08-06T00:38:04.196748: step 1773, loss 0.53758.
Train: 2018-08-06T00:38:04.439069: step 1774, loss 0.587428.
Train: 2018-08-06T00:38:04.691427: step 1775, loss 0.603866.
Train: 2018-08-06T00:38:04.939761: step 1776, loss 0.562486.
Train: 2018-08-06T00:38:05.197067: step 1777, loss 0.587326.
Train: 2018-08-06T00:38:05.441413: step 1778, loss 0.554369.
Train: 2018-08-06T00:38:05.686732: step 1779, loss 0.521376.
Train: 2018-08-06T00:38:05.927090: step 1780, loss 0.554244.
Test: 2018-08-06T00:38:07.194699: step 1780, loss 0.548857.
Train: 2018-08-06T00:38:07.435082: step 1781, loss 0.537861.
Train: 2018-08-06T00:38:07.676442: step 1782, loss 0.628192.
Train: 2018-08-06T00:38:07.932726: step 1783, loss 0.636385.
Train: 2018-08-06T00:38:08.173114: step 1784, loss 0.562619.
Train: 2018-08-06T00:38:08.428401: step 1785, loss 0.611595.
Train: 2018-08-06T00:38:08.671750: step 1786, loss 0.546402.
Train: 2018-08-06T00:38:08.921113: step 1787, loss 0.554516.
Train: 2018-08-06T00:38:09.169449: step 1788, loss 0.554695.
Train: 2018-08-06T00:38:09.413796: step 1789, loss 0.562809.
Train: 2018-08-06T00:38:09.670105: step 1790, loss 0.506531.
Test: 2018-08-06T00:38:10.929710: step 1790, loss 0.54937.
Train: 2018-08-06T00:38:11.167109: step 1791, loss 0.635199.
Train: 2018-08-06T00:38:11.410456: step 1792, loss 0.546656.
Train: 2018-08-06T00:38:11.651811: step 1793, loss 0.594868.
Train: 2018-08-06T00:38:11.897154: step 1794, loss 0.59487.
Train: 2018-08-06T00:38:12.138477: step 1795, loss 0.522974.
Train: 2018-08-06T00:38:12.388810: step 1796, loss 0.578921.
Train: 2018-08-06T00:38:12.632158: step 1797, loss 0.467356.
Train: 2018-08-06T00:38:12.884513: step 1798, loss 0.522985.
Train: 2018-08-06T00:38:13.126835: step 1799, loss 0.618912.
Train: 2018-08-06T00:38:13.370215: step 1800, loss 0.514908.
Test: 2018-08-06T00:38:14.637794: step 1800, loss 0.548677.
Train: 2018-08-06T00:38:15.573257: step 1801, loss 0.578911.
Train: 2018-08-06T00:38:15.831537: step 1802, loss 0.546737.
Train: 2018-08-06T00:38:16.091870: step 1803, loss 0.610987.
Train: 2018-08-06T00:38:16.334190: step 1804, loss 0.603109.
Train: 2018-08-06T00:38:16.579535: step 1805, loss 0.546714.
Train: 2018-08-06T00:38:16.819891: step 1806, loss 0.57081.
Train: 2018-08-06T00:38:17.067230: step 1807, loss 0.53056.
Train: 2018-08-06T00:38:17.307614: step 1808, loss 0.594943.
Train: 2018-08-06T00:38:17.550970: step 1809, loss 0.554699.
Train: 2018-08-06T00:38:17.795314: step 1810, loss 0.498221.
Test: 2018-08-06T00:38:19.061896: step 1810, loss 0.549053.
Train: 2018-08-06T00:38:19.298263: step 1811, loss 0.578901.
Train: 2018-08-06T00:38:19.483769: step 1812, loss 0.528222.
Train: 2018-08-06T00:38:19.732103: step 1813, loss 0.595049.
Train: 2018-08-06T00:38:19.990414: step 1814, loss 0.586945.
Train: 2018-08-06T00:38:20.232796: step 1815, loss 0.546494.
Train: 2018-08-06T00:38:20.477142: step 1816, loss 0.554633.
Train: 2018-08-06T00:38:20.733427: step 1817, loss 0.546358.
Train: 2018-08-06T00:38:20.977773: step 1818, loss 0.562718.
Train: 2018-08-06T00:38:21.237110: step 1819, loss 0.587165.
Train: 2018-08-06T00:38:21.493393: step 1820, loss 0.489261.
Test: 2018-08-06T00:38:22.751030: step 1820, loss 0.548756.
Train: 2018-08-06T00:38:22.987405: step 1821, loss 0.652443.
Train: 2018-08-06T00:38:23.239761: step 1822, loss 0.587144.
Train: 2018-08-06T00:38:23.485092: step 1823, loss 0.546286.
Train: 2018-08-06T00:38:23.740414: step 1824, loss 0.562634.
Train: 2018-08-06T00:38:23.992734: step 1825, loss 0.521847.
Train: 2018-08-06T00:38:24.239051: step 1826, loss 0.603422.
Train: 2018-08-06T00:38:24.497391: step 1827, loss 0.481003.
Train: 2018-08-06T00:38:24.742705: step 1828, loss 0.603456.
Train: 2018-08-06T00:38:24.985086: step 1829, loss 0.611672.
Train: 2018-08-06T00:38:25.241371: step 1830, loss 0.489049.
Test: 2018-08-06T00:38:26.501001: step 1830, loss 0.548736.
Train: 2018-08-06T00:38:26.734378: step 1831, loss 0.595382.
Train: 2018-08-06T00:38:26.986745: step 1832, loss 0.578972.
Train: 2018-08-06T00:38:27.231051: step 1833, loss 0.546234.
Train: 2018-08-06T00:38:27.474423: step 1834, loss 0.570733.
Train: 2018-08-06T00:38:27.714755: step 1835, loss 0.595272.
Train: 2018-08-06T00:38:27.961127: step 1836, loss 0.587145.
Train: 2018-08-06T00:38:28.204447: step 1837, loss 0.546265.
Train: 2018-08-06T00:38:28.447795: step 1838, loss 0.587057.
Train: 2018-08-06T00:38:28.695140: step 1839, loss 0.546319.
Train: 2018-08-06T00:38:28.953468: step 1840, loss 0.554502.
Test: 2018-08-06T00:38:30.214072: step 1840, loss 0.54926.
Train: 2018-08-06T00:38:30.450441: step 1841, loss 0.521963.
Train: 2018-08-06T00:38:30.699773: step 1842, loss 0.546327.
Train: 2018-08-06T00:38:30.947112: step 1843, loss 0.61958.
Train: 2018-08-06T00:38:31.197443: step 1844, loss 0.538241.
Train: 2018-08-06T00:38:31.442786: step 1845, loss 0.562589.
Train: 2018-08-06T00:38:31.684168: step 1846, loss 0.538199.
Train: 2018-08-06T00:38:31.932478: step 1847, loss 0.570839.
Train: 2018-08-06T00:38:32.178846: step 1848, loss 0.587084.
Train: 2018-08-06T00:38:32.438148: step 1849, loss 0.595224.
Train: 2018-08-06T00:38:32.696463: step 1850, loss 0.521906.
Test: 2018-08-06T00:38:33.953072: step 1850, loss 0.548667.
Train: 2018-08-06T00:38:34.184453: step 1851, loss 0.546364.
Train: 2018-08-06T00:38:34.426805: step 1852, loss 0.611562.
Train: 2018-08-06T00:38:34.675172: step 1853, loss 0.554506.
Train: 2018-08-06T00:38:34.926494: step 1854, loss 0.554582.
Train: 2018-08-06T00:38:35.171839: step 1855, loss 0.587016.
Train: 2018-08-06T00:38:35.473226: step 1856, loss 0.562661.
Train: 2018-08-06T00:38:35.718547: step 1857, loss 0.611424.
Train: 2018-08-06T00:38:35.964879: step 1858, loss 0.497864.
Train: 2018-08-06T00:38:36.220222: step 1859, loss 0.603206.
Train: 2018-08-06T00:38:36.471523: step 1860, loss 0.546483.
Test: 2018-08-06T00:38:37.741128: step 1860, loss 0.549364.
Train: 2018-08-06T00:38:37.975532: step 1861, loss 0.538354.
Train: 2018-08-06T00:38:38.225858: step 1862, loss 0.570849.
Train: 2018-08-06T00:38:38.470204: step 1863, loss 0.595053.
Train: 2018-08-06T00:38:38.715548: step 1864, loss 0.497955.
Train: 2018-08-06T00:38:38.962892: step 1865, loss 0.587039.
Train: 2018-08-06T00:38:39.217181: step 1866, loss 0.522209.
Train: 2018-08-06T00:38:39.460561: step 1867, loss 0.546427.
Train: 2018-08-06T00:38:39.702882: step 1868, loss 0.49755.
Train: 2018-08-06T00:38:39.946257: step 1869, loss 0.611469.
Train: 2018-08-06T00:38:40.189607: step 1870, loss 0.538112.
Test: 2018-08-06T00:38:41.455195: step 1870, loss 0.548918.
Train: 2018-08-06T00:38:41.720486: step 1871, loss 0.562599.
Train: 2018-08-06T00:38:41.963866: step 1872, loss 0.546174.
Train: 2018-08-06T00:38:42.210207: step 1873, loss 0.570823.
Train: 2018-08-06T00:38:42.451561: step 1874, loss 0.546068.
Train: 2018-08-06T00:38:42.695877: step 1875, loss 0.562532.
Train: 2018-08-06T00:38:42.947207: step 1876, loss 0.587261.
Train: 2018-08-06T00:38:43.194575: step 1877, loss 0.488131.
Train: 2018-08-06T00:38:43.448864: step 1878, loss 0.578997.
Train: 2018-08-06T00:38:43.695205: step 1879, loss 0.487849.
Train: 2018-08-06T00:38:43.940581: step 1880, loss 0.604036.
Test: 2018-08-06T00:38:45.202175: step 1880, loss 0.548523.
Train: 2018-08-06T00:38:45.439566: step 1881, loss 0.604039.
Train: 2018-08-06T00:38:45.686878: step 1882, loss 0.562453.
Train: 2018-08-06T00:38:45.935245: step 1883, loss 0.554036.
Train: 2018-08-06T00:38:46.178564: step 1884, loss 0.612473.
Train: 2018-08-06T00:38:46.423907: step 1885, loss 0.604116.
Train: 2018-08-06T00:38:46.668254: step 1886, loss 0.595722.
Train: 2018-08-06T00:38:46.918616: step 1887, loss 0.58727.
Train: 2018-08-06T00:38:47.161935: step 1888, loss 0.545991.
Train: 2018-08-06T00:38:47.410295: step 1889, loss 0.620337.
Train: 2018-08-06T00:38:47.656635: step 1890, loss 0.521453.
Test: 2018-08-06T00:38:48.941175: step 1890, loss 0.549057.
Train: 2018-08-06T00:38:49.174552: step 1891, loss 0.652796.
Train: 2018-08-06T00:38:49.416908: step 1892, loss 0.521734.
Train: 2018-08-06T00:38:49.659256: step 1893, loss 0.587148.
Train: 2018-08-06T00:38:49.905628: step 1894, loss 0.522069.
Train: 2018-08-06T00:38:50.155952: step 1895, loss 0.546422.
Train: 2018-08-06T00:38:50.399277: step 1896, loss 0.554653.
Train: 2018-08-06T00:38:50.641629: step 1897, loss 0.562764.
Train: 2018-08-06T00:38:50.884985: step 1898, loss 0.546566.
Train: 2018-08-06T00:38:51.135325: step 1899, loss 0.546594.
Train: 2018-08-06T00:38:51.390625: step 1900, loss 0.546587.
Test: 2018-08-06T00:38:52.658234: step 1900, loss 0.547555.
Train: 2018-08-06T00:38:53.596045: step 1901, loss 0.530451.
Train: 2018-08-06T00:38:53.839394: step 1902, loss 0.522238.
Train: 2018-08-06T00:38:54.086732: step 1903, loss 0.562597.
Train: 2018-08-06T00:38:54.331104: step 1904, loss 0.570772.
Train: 2018-08-06T00:38:54.574459: step 1905, loss 0.52191.
Train: 2018-08-06T00:38:54.818805: step 1906, loss 0.578942.
Train: 2018-08-06T00:38:55.062150: step 1907, loss 0.529808.
Train: 2018-08-06T00:38:55.305504: step 1908, loss 0.554347.
Train: 2018-08-06T00:38:55.550818: step 1909, loss 0.579019.
Train: 2018-08-06T00:38:55.795163: step 1910, loss 0.546161.
Test: 2018-08-06T00:38:57.057787: step 1910, loss 0.548544.
Train: 2018-08-06T00:38:57.305158: step 1911, loss 0.562601.
Train: 2018-08-06T00:38:57.551468: step 1912, loss 0.595612.
Train: 2018-08-06T00:38:57.792823: step 1913, loss 0.562352.
Train: 2018-08-06T00:38:58.034175: step 1914, loss 0.554255.
Train: 2018-08-06T00:38:58.291487: step 1915, loss 0.570686.
Train: 2018-08-06T00:38:58.537828: step 1916, loss 0.537709.
Train: 2018-08-06T00:38:58.793147: step 1917, loss 0.587343.
Train: 2018-08-06T00:38:59.048464: step 1918, loss 0.512806.
Train: 2018-08-06T00:38:59.288820: step 1919, loss 0.620483.
Train: 2018-08-06T00:38:59.531174: step 1920, loss 0.645367.
Test: 2018-08-06T00:39:00.791801: step 1920, loss 0.550591.
Train: 2018-08-06T00:39:01.028169: step 1921, loss 0.521176.
Train: 2018-08-06T00:39:01.273512: step 1922, loss 0.578992.
Train: 2018-08-06T00:39:01.527866: step 1923, loss 0.62023.
Train: 2018-08-06T00:39:01.772205: step 1924, loss 0.505055.
Train: 2018-08-06T00:39:02.019517: step 1925, loss 0.537968.
Train: 2018-08-06T00:39:02.263896: step 1926, loss 0.57072.
Train: 2018-08-06T00:39:02.512205: step 1927, loss 0.587175.
Train: 2018-08-06T00:39:02.770515: step 1928, loss 0.529836.
Train: 2018-08-06T00:39:03.015884: step 1929, loss 0.595322.
Train: 2018-08-06T00:39:03.260231: step 1930, loss 0.55451.
Test: 2018-08-06T00:39:04.517841: step 1930, loss 0.548979.
Train: 2018-08-06T00:39:04.753213: step 1931, loss 0.538096.
Train: 2018-08-06T00:39:05.000581: step 1932, loss 0.611579.
Train: 2018-08-06T00:39:05.243900: step 1933, loss 0.587126.
Train: 2018-08-06T00:39:05.486278: step 1934, loss 0.489437.
Train: 2018-08-06T00:39:05.733616: step 1935, loss 0.554491.
Train: 2018-08-06T00:39:05.979932: step 1936, loss 0.51386.
Train: 2018-08-06T00:39:06.223281: step 1937, loss 0.52189.
Train: 2018-08-06T00:39:06.467628: step 1938, loss 0.56258.
Train: 2018-08-06T00:39:06.711974: step 1939, loss 0.636192.
Train: 2018-08-06T00:39:06.958315: step 1940, loss 0.570773.
Test: 2018-08-06T00:39:08.212960: step 1940, loss 0.549289.
Train: 2018-08-06T00:39:08.454314: step 1941, loss 0.4807.
Train: 2018-08-06T00:39:08.700684: step 1942, loss 0.554367.
Train: 2018-08-06T00:39:08.950020: step 1943, loss 0.554317.
Train: 2018-08-06T00:39:09.192373: step 1944, loss 0.546072.
Train: 2018-08-06T00:39:09.437685: step 1945, loss 0.578948.
Train: 2018-08-06T00:39:09.693002: step 1946, loss 0.620263.
Train: 2018-08-06T00:39:09.935366: step 1947, loss 0.529476.
Train: 2018-08-06T00:39:10.178702: step 1948, loss 0.521238.
Train: 2018-08-06T00:39:10.419093: step 1949, loss 0.587325.
Train: 2018-08-06T00:39:10.664428: step 1950, loss 0.587348.
Test: 2018-08-06T00:39:11.922040: step 1950, loss 0.54867.
Train: 2018-08-06T00:39:12.156413: step 1951, loss 0.579058.
Train: 2018-08-06T00:39:12.414723: step 1952, loss 0.579063.
Train: 2018-08-06T00:39:12.659095: step 1953, loss 0.554254.
Train: 2018-08-06T00:39:12.903416: step 1954, loss 0.644986.
Train: 2018-08-06T00:39:13.146792: step 1955, loss 0.562559.
Train: 2018-08-06T00:39:13.390145: step 1956, loss 0.505103.
Train: 2018-08-06T00:39:13.635488: step 1957, loss 0.529725.
Train: 2018-08-06T00:39:13.881799: step 1958, loss 0.570776.
Train: 2018-08-06T00:39:14.122156: step 1959, loss 0.505176.
Train: 2018-08-06T00:39:14.374482: step 1960, loss 0.513335.
Test: 2018-08-06T00:39:15.642092: step 1960, loss 0.548801.
Train: 2018-08-06T00:39:15.876489: step 1961, loss 0.595454.
Train: 2018-08-06T00:39:16.118818: step 1962, loss 0.537851.
Train: 2018-08-06T00:39:16.307337: step 1963, loss 0.61532.
Train: 2018-08-06T00:39:16.552657: step 1964, loss 0.578972.
Train: 2018-08-06T00:39:16.798000: step 1965, loss 0.562481.
Train: 2018-08-06T00:39:17.040352: step 1966, loss 0.52128.
Train: 2018-08-06T00:39:17.283750: step 1967, loss 0.554242.
Train: 2018-08-06T00:39:17.528048: step 1968, loss 0.562489.
Train: 2018-08-06T00:39:17.770426: step 1969, loss 0.488208.
Train: 2018-08-06T00:39:18.016767: step 1970, loss 0.529428.
Test: 2018-08-06T00:39:19.301306: step 1970, loss 0.547667.
Train: 2018-08-06T00:39:19.538704: step 1971, loss 0.587289.
Train: 2018-08-06T00:39:19.784045: step 1972, loss 0.545821.
Train: 2018-08-06T00:39:20.028361: step 1973, loss 0.579063.
Train: 2018-08-06T00:39:20.279715: step 1974, loss 0.529112.
Train: 2018-08-06T00:39:20.526041: step 1975, loss 0.50404.
Train: 2018-08-06T00:39:20.780350: step 1976, loss 0.503857.
Train: 2018-08-06T00:39:21.023742: step 1977, loss 0.60443.
Train: 2018-08-06T00:39:21.266082: step 1978, loss 0.486651.
Train: 2018-08-06T00:39:21.511401: step 1979, loss 0.57081.
Train: 2018-08-06T00:39:21.753756: step 1980, loss 0.587704.
Test: 2018-08-06T00:39:23.017368: step 1980, loss 0.548377.
Train: 2018-08-06T00:39:23.248749: step 1981, loss 0.536938.
Train: 2018-08-06T00:39:23.492128: step 1982, loss 0.545258.
Train: 2018-08-06T00:39:23.736445: step 1983, loss 0.545229.
Train: 2018-08-06T00:39:23.989767: step 1984, loss 0.622071.
Train: 2018-08-06T00:39:24.234145: step 1985, loss 0.485495.
Train: 2018-08-06T00:39:24.478491: step 1986, loss 0.562326.
Train: 2018-08-06T00:39:24.723803: step 1987, loss 0.562315.
Train: 2018-08-06T00:39:24.977155: step 1988, loss 0.622239.
Train: 2018-08-06T00:39:25.223468: step 1989, loss 0.570804.
Train: 2018-08-06T00:39:25.471816: step 1990, loss 0.664819.
Test: 2018-08-06T00:39:26.726460: step 1990, loss 0.5489.
Train: 2018-08-06T00:39:26.958871: step 1991, loss 0.579387.
Train: 2018-08-06T00:39:27.201190: step 1992, loss 0.562363.
Train: 2018-08-06T00:39:27.448554: step 1993, loss 0.621539.
Train: 2018-08-06T00:39:27.694902: step 1994, loss 0.562322.
Train: 2018-08-06T00:39:27.941242: step 1995, loss 0.545677.
Train: 2018-08-06T00:39:28.185558: step 1996, loss 0.545762.
Train: 2018-08-06T00:39:28.429937: step 1997, loss 0.562407.
Train: 2018-08-06T00:39:28.675274: step 1998, loss 0.554187.
Train: 2018-08-06T00:39:28.918597: step 1999, loss 0.595594.
Train: 2018-08-06T00:39:29.165936: step 2000, loss 0.620281.
Test: 2018-08-06T00:39:30.430554: step 2000, loss 0.549029.
Train: 2018-08-06T00:39:31.358652: step 2001, loss 0.496916.
Train: 2018-08-06T00:39:31.604973: step 2002, loss 0.578931.
Train: 2018-08-06T00:39:31.847313: step 2003, loss 0.513645.
Train: 2018-08-06T00:39:32.088695: step 2004, loss 0.595203.
Train: 2018-08-06T00:39:32.347005: step 2005, loss 0.587084.
Train: 2018-08-06T00:39:32.592352: step 2006, loss 0.554494.
Train: 2018-08-06T00:39:32.839685: step 2007, loss 0.554564.
Train: 2018-08-06T00:39:33.082042: step 2008, loss 0.497891.
Train: 2018-08-06T00:39:33.324395: step 2009, loss 0.554654.
Train: 2018-08-06T00:39:33.568710: step 2010, loss 0.578968.
Test: 2018-08-06T00:39:34.823354: step 2010, loss 0.548767.
Train: 2018-08-06T00:39:35.059751: step 2011, loss 0.554491.
Train: 2018-08-06T00:39:35.304099: step 2012, loss 0.497835.
Train: 2018-08-06T00:39:35.560414: step 2013, loss 0.595228.
Train: 2018-08-06T00:39:35.819689: step 2014, loss 0.530148.
Train: 2018-08-06T00:39:36.062056: step 2015, loss 0.578861.
Train: 2018-08-06T00:39:36.314366: step 2016, loss 0.6115.
Train: 2018-08-06T00:39:36.556720: step 2017, loss 0.570816.
Train: 2018-08-06T00:39:36.805082: step 2018, loss 0.505552.
Train: 2018-08-06T00:39:37.048442: step 2019, loss 0.579016.
Train: 2018-08-06T00:39:37.293748: step 2020, loss 0.480976.
Test: 2018-08-06T00:39:38.561358: step 2020, loss 0.549475.
Train: 2018-08-06T00:39:38.809718: step 2021, loss 0.538044.
Train: 2018-08-06T00:39:39.052045: step 2022, loss 0.529675.
Train: 2018-08-06T00:39:39.312378: step 2023, loss 0.56258.
Train: 2018-08-06T00:39:39.554701: step 2024, loss 0.479914.
Train: 2018-08-06T00:39:39.805062: step 2025, loss 0.512746.
Train: 2018-08-06T00:39:40.050406: step 2026, loss 0.529116.
Train: 2018-08-06T00:39:40.297752: step 2027, loss 0.579073.
Train: 2018-08-06T00:39:40.548076: step 2028, loss 0.587609.
Train: 2018-08-06T00:39:40.792392: step 2029, loss 0.545421.
Train: 2018-08-06T00:39:41.038757: step 2030, loss 0.621552.
Test: 2018-08-06T00:39:42.325292: step 2030, loss 0.546649.
Train: 2018-08-06T00:39:42.560687: step 2031, loss 0.537002.
Train: 2018-08-06T00:39:42.805040: step 2032, loss 0.519934.
Train: 2018-08-06T00:39:43.049389: step 2033, loss 0.553902.
Train: 2018-08-06T00:39:43.289713: step 2034, loss 0.485733.
Train: 2018-08-06T00:39:43.533090: step 2035, loss 0.528272.
Train: 2018-08-06T00:39:43.776451: step 2036, loss 0.579486.
Train: 2018-08-06T00:39:44.029733: step 2037, loss 0.596693.
Train: 2018-08-06T00:39:44.273083: step 2038, loss 0.510683.
Train: 2018-08-06T00:39:44.513471: step 2039, loss 0.588081.
Train: 2018-08-06T00:39:44.756818: step 2040, loss 0.61402.
Test: 2018-08-06T00:39:46.009439: step 2040, loss 0.54796.
Train: 2018-08-06T00:39:46.254783: step 2041, loss 0.570973.
Train: 2018-08-06T00:39:46.500127: step 2042, loss 0.570876.
Train: 2018-08-06T00:39:46.743477: step 2043, loss 0.562286.
Train: 2018-08-06T00:39:46.988851: step 2044, loss 0.528031.
Train: 2018-08-06T00:39:47.229178: step 2045, loss 0.605043.
Train: 2018-08-06T00:39:47.468561: step 2046, loss 0.588097.
Train: 2018-08-06T00:39:47.710890: step 2047, loss 0.579349.
Train: 2018-08-06T00:39:47.959277: step 2048, loss 0.579394.
Train: 2018-08-06T00:39:48.205599: step 2049, loss 0.536928.
Train: 2018-08-06T00:39:48.449913: step 2050, loss 0.477769.
Test: 2018-08-06T00:39:49.722510: step 2050, loss 0.546708.
Train: 2018-08-06T00:39:49.969848: step 2051, loss 0.545503.
Train: 2018-08-06T00:39:50.217186: step 2052, loss 0.705915.
Train: 2018-08-06T00:39:50.456546: step 2053, loss 0.65489.
Train: 2018-08-06T00:39:50.703910: step 2054, loss 0.620884.
Train: 2018-08-06T00:39:50.947264: step 2055, loss 0.554211.
Train: 2018-08-06T00:39:51.197590: step 2056, loss 0.562504.
Train: 2018-08-06T00:39:51.441942: step 2057, loss 0.636522.
Train: 2018-08-06T00:39:51.684293: step 2058, loss 0.587042.
Train: 2018-08-06T00:39:51.924651: step 2059, loss 0.554585.
Train: 2018-08-06T00:39:52.177967: step 2060, loss 0.546617.
Test: 2018-08-06T00:39:53.442559: step 2060, loss 0.55109.
Train: 2018-08-06T00:39:53.692890: step 2061, loss 0.578835.
Train: 2018-08-06T00:39:53.942256: step 2062, loss 0.54695.
Train: 2018-08-06T00:39:54.188596: step 2063, loss 0.578855.
Train: 2018-08-06T00:39:54.428922: step 2064, loss 0.563036.
Train: 2018-08-06T00:39:54.671305: step 2065, loss 0.594646.
Train: 2018-08-06T00:39:54.914653: step 2066, loss 0.531642.
Train: 2018-08-06T00:39:55.156004: step 2067, loss 0.610332.
Train: 2018-08-06T00:39:55.399327: step 2068, loss 0.571057.
Train: 2018-08-06T00:39:55.655642: step 2069, loss 0.563245.
Train: 2018-08-06T00:39:55.899989: step 2070, loss 0.586723.
Test: 2018-08-06T00:39:57.165603: step 2070, loss 0.549265.
Train: 2018-08-06T00:39:57.401972: step 2071, loss 0.524448.
Train: 2018-08-06T00:39:57.653300: step 2072, loss 0.571151.
Train: 2018-08-06T00:39:57.897678: step 2073, loss 0.547808.
Train: 2018-08-06T00:39:58.142017: step 2074, loss 0.586687.
Train: 2018-08-06T00:39:58.383347: step 2075, loss 0.563363.
Train: 2018-08-06T00:39:58.635673: step 2076, loss 0.586677.
Train: 2018-08-06T00:39:58.880019: step 2077, loss 0.594426.
Train: 2018-08-06T00:39:59.131346: step 2078, loss 0.594431.
Train: 2018-08-06T00:39:59.382675: step 2079, loss 0.547923.
Train: 2018-08-06T00:39:59.636024: step 2080, loss 0.478252.
Test: 2018-08-06T00:40:00.901612: step 2080, loss 0.550961.
Train: 2018-08-06T00:40:01.136982: step 2081, loss 0.609974.
Train: 2018-08-06T00:40:01.377340: step 2082, loss 0.57112.
Train: 2018-08-06T00:40:01.623715: step 2083, loss 0.586689.
Train: 2018-08-06T00:40:01.867061: step 2084, loss 0.547807.
Train: 2018-08-06T00:40:02.112407: step 2085, loss 0.555532.
Train: 2018-08-06T00:40:02.358716: step 2086, loss 0.500897.
Train: 2018-08-06T00:40:02.605102: step 2087, loss 0.657115.
Train: 2018-08-06T00:40:02.849431: step 2088, loss 0.516277.
Train: 2018-08-06T00:40:03.091755: step 2089, loss 0.508291.
Train: 2018-08-06T00:40:03.334110: step 2090, loss 0.50805.
Test: 2018-08-06T00:40:04.591743: step 2090, loss 0.550392.
Train: 2018-08-06T00:40:04.834127: step 2091, loss 0.507708.
Train: 2018-08-06T00:40:05.077469: step 2092, loss 0.594762.
Train: 2018-08-06T00:40:05.329802: step 2093, loss 0.586869.
Train: 2018-08-06T00:40:05.576136: step 2094, loss 0.578864.
Train: 2018-08-06T00:40:05.828465: step 2095, loss 0.506541.
Train: 2018-08-06T00:40:06.071816: step 2096, loss 0.635373.
Train: 2018-08-06T00:40:06.325134: step 2097, loss 0.570793.
Train: 2018-08-06T00:40:06.569480: step 2098, loss 0.570812.
Train: 2018-08-06T00:40:06.809846: step 2099, loss 0.530274.
Train: 2018-08-06T00:40:07.060172: step 2100, loss 0.595121.
Test: 2018-08-06T00:40:08.319773: step 2100, loss 0.550012.
Train: 2018-08-06T00:40:09.262651: step 2101, loss 0.611425.
Train: 2018-08-06T00:40:09.506032: step 2102, loss 0.619542.
Train: 2018-08-06T00:40:09.748383: step 2103, loss 0.587011.
Train: 2018-08-06T00:40:09.992699: step 2104, loss 0.570792.
Train: 2018-08-06T00:40:10.242031: step 2105, loss 0.481898.
Train: 2018-08-06T00:40:10.483411: step 2106, loss 0.562708.
Train: 2018-08-06T00:40:10.729726: step 2107, loss 0.586979.
Train: 2018-08-06T00:40:10.973107: step 2108, loss 0.48998.
Train: 2018-08-06T00:40:11.229418: step 2109, loss 0.578912.
Train: 2018-08-06T00:40:11.483717: step 2110, loss 0.554578.
Test: 2018-08-06T00:40:12.763288: step 2110, loss 0.548939.
Train: 2018-08-06T00:40:13.010627: step 2111, loss 0.514015.
Train: 2018-08-06T00:40:13.257001: step 2112, loss 0.546395.
Train: 2018-08-06T00:40:13.499376: step 2113, loss 0.635966.
Train: 2018-08-06T00:40:13.699785: step 2114, loss 0.580029.
Train: 2018-08-06T00:40:13.959121: step 2115, loss 0.562641.
Train: 2018-08-06T00:40:14.205463: step 2116, loss 0.554474.
Train: 2018-08-06T00:40:14.446816: step 2117, loss 0.603388.
Train: 2018-08-06T00:40:14.692131: step 2118, loss 0.538179.
Train: 2018-08-06T00:40:14.940497: step 2119, loss 0.530043.
Train: 2018-08-06T00:40:15.185809: step 2120, loss 0.562633.
Test: 2018-08-06T00:40:16.440454: step 2120, loss 0.548432.
Train: 2018-08-06T00:40:16.675824: step 2121, loss 0.53814.
Train: 2018-08-06T00:40:16.920204: step 2122, loss 0.513608.
Train: 2018-08-06T00:40:17.173519: step 2123, loss 0.619869.
Train: 2018-08-06T00:40:17.413883: step 2124, loss 0.562577.
Train: 2018-08-06T00:40:17.657200: step 2125, loss 0.513436.
Train: 2018-08-06T00:40:17.902576: step 2126, loss 0.578973.
Train: 2018-08-06T00:40:18.144927: step 2127, loss 0.570754.
Train: 2018-08-06T00:40:18.404227: step 2128, loss 0.562546.
Train: 2018-08-06T00:40:18.648575: step 2129, loss 0.595425.
Train: 2018-08-06T00:40:18.901871: step 2130, loss 0.570766.
Test: 2018-08-06T00:40:20.170478: step 2130, loss 0.54807.
Train: 2018-08-06T00:40:20.417848: step 2131, loss 0.521457.
Train: 2018-08-06T00:40:20.659204: step 2132, loss 0.578965.
Train: 2018-08-06T00:40:20.904516: step 2133, loss 0.521448.
Train: 2018-08-06T00:40:21.145871: step 2134, loss 0.554321.
Train: 2018-08-06T00:40:21.387224: step 2135, loss 0.537828.
Train: 2018-08-06T00:40:21.635591: step 2136, loss 0.521286.
Train: 2018-08-06T00:40:21.877912: step 2137, loss 0.66163.
Train: 2018-08-06T00:40:22.123256: step 2138, loss 0.54599.
Train: 2018-08-06T00:40:22.369598: step 2139, loss 0.603786.
Train: 2018-08-06T00:40:22.612972: step 2140, loss 0.546009.
Test: 2018-08-06T00:40:23.876567: step 2140, loss 0.548744.
Train: 2018-08-06T00:40:24.111971: step 2141, loss 0.570735.
Train: 2018-08-06T00:40:24.352295: step 2142, loss 0.529569.
Train: 2018-08-06T00:40:24.602656: step 2143, loss 0.513094.
Train: 2018-08-06T00:40:24.846973: step 2144, loss 0.644975.
Train: 2018-08-06T00:40:25.090352: step 2145, loss 0.661359.
Train: 2018-08-06T00:40:25.334668: step 2146, loss 0.578954.
Train: 2018-08-06T00:40:25.581035: step 2147, loss 0.587134.
Train: 2018-08-06T00:40:25.833335: step 2148, loss 0.595251.
Train: 2018-08-06T00:40:26.079676: step 2149, loss 0.635751.
Train: 2018-08-06T00:40:26.324048: step 2150, loss 0.546567.
Test: 2018-08-06T00:40:27.589637: step 2150, loss 0.548623.
Train: 2018-08-06T00:40:27.826005: step 2151, loss 0.506494.
Train: 2018-08-06T00:40:28.074341: step 2152, loss 0.651027.
Train: 2018-08-06T00:40:28.318695: step 2153, loss 0.586844.
Train: 2018-08-06T00:40:28.562069: step 2154, loss 0.539104.
Train: 2018-08-06T00:40:28.812420: step 2155, loss 0.650172.
Train: 2018-08-06T00:40:29.070695: step 2156, loss 0.618283.
Train: 2018-08-06T00:40:29.310037: step 2157, loss 0.571035.
Train: 2018-08-06T00:40:29.567380: step 2158, loss 0.602296.
Train: 2018-08-06T00:40:29.809731: step 2159, loss 0.586667.
Train: 2018-08-06T00:40:30.052078: step 2160, loss 0.509505.
Test: 2018-08-06T00:40:31.328638: step 2160, loss 0.551437.
Train: 2018-08-06T00:40:31.566034: step 2161, loss 0.555871.
Train: 2018-08-06T00:40:31.810380: step 2162, loss 0.602008.
Train: 2018-08-06T00:40:32.058685: step 2163, loss 0.525405.
Train: 2018-08-06T00:40:32.307054: step 2164, loss 0.563705.
Train: 2018-08-06T00:40:32.564333: step 2165, loss 0.632497.
Train: 2018-08-06T00:40:32.805713: step 2166, loss 0.563756.
Train: 2018-08-06T00:40:33.058014: step 2167, loss 0.510493.
Train: 2018-08-06T00:40:33.298395: step 2168, loss 0.540952.
Train: 2018-08-06T00:40:33.537739: step 2169, loss 0.662949.
Train: 2018-08-06T00:40:33.784071: step 2170, loss 0.609515.
Test: 2018-08-06T00:40:35.037718: step 2170, loss 0.55024.
Train: 2018-08-06T00:40:35.277078: step 2171, loss 0.548594.
Train: 2018-08-06T00:40:35.583971: step 2172, loss 0.563855.
Train: 2018-08-06T00:40:35.826299: step 2173, loss 0.525859.
Train: 2018-08-06T00:40:36.069681: step 2174, loss 0.563813.
Train: 2018-08-06T00:40:36.311033: step 2175, loss 0.609529.
Train: 2018-08-06T00:40:36.555350: step 2176, loss 0.579008.
Train: 2018-08-06T00:40:36.808672: step 2177, loss 0.518034.
Train: 2018-08-06T00:40:37.053019: step 2178, loss 0.525548.
Train: 2018-08-06T00:40:37.300383: step 2179, loss 0.571326.
Train: 2018-08-06T00:40:37.544737: step 2180, loss 0.532862.
Test: 2018-08-06T00:40:38.814307: step 2180, loss 0.549402.
Train: 2018-08-06T00:40:39.049678: step 2181, loss 0.57125.
Train: 2018-08-06T00:40:39.295023: step 2182, loss 0.5093.
Train: 2018-08-06T00:40:39.538385: step 2183, loss 0.540044.
Train: 2018-08-06T00:40:39.780724: step 2184, loss 0.508558.
Train: 2018-08-06T00:40:40.031084: step 2185, loss 0.571037.
Train: 2018-08-06T00:40:40.279414: step 2186, loss 0.594678.
Train: 2018-08-06T00:40:40.522740: step 2187, loss 0.539127.
Train: 2018-08-06T00:40:40.764094: step 2188, loss 0.586851.
Train: 2018-08-06T00:40:41.016417: step 2189, loss 0.642926.
Train: 2018-08-06T00:40:41.259767: step 2190, loss 0.562796.
Test: 2018-08-06T00:40:42.518401: step 2190, loss 0.548661.
Train: 2018-08-06T00:40:42.810646: step 2191, loss 0.506612.
Train: 2018-08-06T00:40:43.069935: step 2192, loss 0.562777.
Train: 2018-08-06T00:40:43.324248: step 2193, loss 0.562743.
Train: 2018-08-06T00:40:43.565600: step 2194, loss 0.57081.
Train: 2018-08-06T00:40:43.820918: step 2195, loss 0.554613.
Train: 2018-08-06T00:40:44.067259: step 2196, loss 0.546412.
Train: 2018-08-06T00:40:44.312603: step 2197, loss 0.570781.
Train: 2018-08-06T00:40:44.553957: step 2198, loss 0.62787.
Train: 2018-08-06T00:40:44.818252: step 2199, loss 0.570745.
Train: 2018-08-06T00:40:45.059633: step 2200, loss 0.554496.
Test: 2018-08-06T00:40:46.313253: step 2200, loss 0.549387.
Train: 2018-08-06T00:40:47.200151: step 2201, loss 0.529997.
Train: 2018-08-06T00:40:47.439512: step 2202, loss 0.538152.
Train: 2018-08-06T00:40:47.682846: step 2203, loss 0.497237.
Train: 2018-08-06T00:40:47.927200: step 2204, loss 0.57075.
Train: 2018-08-06T00:40:48.173517: step 2205, loss 0.546153.
Train: 2018-08-06T00:40:48.414904: step 2206, loss 0.603683.
Train: 2018-08-06T00:40:48.662209: step 2207, loss 0.529572.
Train: 2018-08-06T00:40:48.905591: step 2208, loss 0.603736.
Train: 2018-08-06T00:40:49.149906: step 2209, loss 0.554226.
Train: 2018-08-06T00:40:49.396246: step 2210, loss 0.570735.
Test: 2018-08-06T00:40:50.662859: step 2210, loss 0.549076.
Train: 2018-08-06T00:40:50.906237: step 2211, loss 0.537704.
Train: 2018-08-06T00:40:51.147594: step 2212, loss 0.587271.
Train: 2018-08-06T00:40:51.390913: step 2213, loss 0.603851.
Train: 2018-08-06T00:40:51.633290: step 2214, loss 0.612025.
Train: 2018-08-06T00:40:51.880603: step 2215, loss 0.595473.
Train: 2018-08-06T00:40:52.127941: step 2216, loss 0.578967.
Train: 2018-08-06T00:40:52.384256: step 2217, loss 0.603528.
Train: 2018-08-06T00:40:52.628640: step 2218, loss 0.587128.
Train: 2018-08-06T00:40:52.872974: step 2219, loss 0.570785.
Train: 2018-08-06T00:40:53.115300: step 2220, loss 0.562664.
Test: 2018-08-06T00:40:54.376926: step 2220, loss 0.549789.
Train: 2018-08-06T00:40:54.619303: step 2221, loss 0.506165.
Train: 2018-08-06T00:40:54.862652: step 2222, loss 0.59497.
Train: 2018-08-06T00:40:55.107972: step 2223, loss 0.562788.
Train: 2018-08-06T00:40:55.361293: step 2224, loss 0.538696.
Train: 2018-08-06T00:40:55.608657: step 2225, loss 0.578882.
Train: 2018-08-06T00:40:55.853977: step 2226, loss 0.570868.
Train: 2018-08-06T00:40:56.115308: step 2227, loss 0.530817.
Train: 2018-08-06T00:40:56.372625: step 2228, loss 0.530831.
Train: 2018-08-06T00:40:56.617964: step 2229, loss 0.514748.
Train: 2018-08-06T00:40:56.862283: step 2230, loss 0.603049.
Test: 2018-08-06T00:40:58.125900: step 2230, loss 0.549194.
Train: 2018-08-06T00:40:58.369251: step 2231, loss 0.570833.
Train: 2018-08-06T00:40:58.612600: step 2232, loss 0.594947.
Train: 2018-08-06T00:40:58.866945: step 2233, loss 0.530649.
Train: 2018-08-06T00:40:59.111290: step 2234, loss 0.570849.
Train: 2018-08-06T00:40:59.357637: step 2235, loss 0.635232.
Train: 2018-08-06T00:40:59.600980: step 2236, loss 0.578885.
Train: 2018-08-06T00:40:59.850317: step 2237, loss 0.538726.
Train: 2018-08-06T00:41:00.092675: step 2238, loss 0.466538.
Train: 2018-08-06T00:41:00.343994: step 2239, loss 0.530654.
Train: 2018-08-06T00:41:00.587343: step 2240, loss 0.546649.
Test: 2018-08-06T00:41:01.853930: step 2240, loss 0.549792.
Train: 2018-08-06T00:41:02.096313: step 2241, loss 0.481917.
Train: 2018-08-06T00:41:02.339632: step 2242, loss 0.546427.
Train: 2018-08-06T00:41:02.589996: step 2243, loss 0.54633.
Train: 2018-08-06T00:41:02.843285: step 2244, loss 0.603488.
Train: 2018-08-06T00:41:03.087662: step 2245, loss 0.587169.
Train: 2018-08-06T00:41:03.333006: step 2246, loss 0.472012.
Train: 2018-08-06T00:41:03.576349: step 2247, loss 0.504689.
Train: 2018-08-06T00:41:03.822712: step 2248, loss 0.479426.
Train: 2018-08-06T00:41:04.070004: step 2249, loss 0.562398.
Train: 2018-08-06T00:41:04.310386: step 2250, loss 0.612764.
Test: 2018-08-06T00:41:05.577971: step 2250, loss 0.548502.
Train: 2018-08-06T00:41:05.821345: step 2251, loss 0.579176.
Train: 2018-08-06T00:41:06.066664: step 2252, loss 0.477895.
Train: 2018-08-06T00:41:06.307046: step 2253, loss 0.460585.
Train: 2018-08-06T00:41:06.551394: step 2254, loss 0.53674.
Train: 2018-08-06T00:41:06.804723: step 2255, loss 0.596651.
Train: 2018-08-06T00:41:07.046076: step 2256, loss 0.579552.
Train: 2018-08-06T00:41:07.289429: step 2257, loss 0.562369.
Train: 2018-08-06T00:41:07.543739: step 2258, loss 0.527767.
Train: 2018-08-06T00:41:07.795068: step 2259, loss 0.562396.
Train: 2018-08-06T00:41:08.043408: step 2260, loss 0.562451.
Test: 2018-08-06T00:41:09.291041: step 2260, loss 0.54763.
Train: 2018-08-06T00:41:09.525443: step 2261, loss 0.553652.
Train: 2018-08-06T00:41:09.773765: step 2262, loss 0.536188.
Train: 2018-08-06T00:41:10.017100: step 2263, loss 0.483901.
Train: 2018-08-06T00:41:10.264463: step 2264, loss 0.553613.
Train: 2018-08-06T00:41:10.452965: step 2265, loss 0.543583.
Train: 2018-08-06T00:41:10.703290: step 2266, loss 0.553582.
Train: 2018-08-06T00:41:10.960607: step 2267, loss 0.518415.
Train: 2018-08-06T00:41:11.210937: step 2268, loss 0.597686.
Train: 2018-08-06T00:41:11.454285: step 2269, loss 0.527058.
Train: 2018-08-06T00:41:11.708607: step 2270, loss 0.615413.
Test: 2018-08-06T00:41:12.987156: step 2270, loss 0.547298.
Train: 2018-08-06T00:41:13.223550: step 2271, loss 0.52717.
Train: 2018-08-06T00:41:13.470863: step 2272, loss 0.509599.
Train: 2018-08-06T00:41:13.716241: step 2273, loss 0.509575.
Train: 2018-08-06T00:41:13.962579: step 2274, loss 0.606471.
Train: 2018-08-06T00:41:14.219888: step 2275, loss 0.579946.
Train: 2018-08-06T00:41:14.464233: step 2276, loss 0.606363.
Train: 2018-08-06T00:41:14.721543: step 2277, loss 0.597538.
Train: 2018-08-06T00:41:14.976836: step 2278, loss 0.571071.
Train: 2018-08-06T00:41:15.219213: step 2279, loss 0.518796.
Train: 2018-08-06T00:41:15.464560: step 2280, loss 0.54501.
Test: 2018-08-06T00:41:16.723165: step 2280, loss 0.549438.
Train: 2018-08-06T00:41:16.963522: step 2281, loss 0.492815.
Train: 2018-08-06T00:41:17.207899: step 2282, loss 0.466745.
Train: 2018-08-06T00:41:17.466204: step 2283, loss 0.518827.
Train: 2018-08-06T00:41:17.708561: step 2284, loss 0.501464.
Train: 2018-08-06T00:41:17.954870: step 2285, loss 0.562323.
Train: 2018-08-06T00:41:18.200214: step 2286, loss 0.536286.
Train: 2018-08-06T00:41:18.449578: step 2287, loss 0.544832.
Train: 2018-08-06T00:41:18.691930: step 2288, loss 0.518661.
Train: 2018-08-06T00:41:18.937270: step 2289, loss 0.588684.
Train: 2018-08-06T00:41:19.179628: step 2290, loss 0.544691.
Test: 2018-08-06T00:41:20.436235: step 2290, loss 0.547911.
Train: 2018-08-06T00:41:20.676592: step 2291, loss 0.606399.
Train: 2018-08-06T00:41:20.918977: step 2292, loss 0.55367.
Train: 2018-08-06T00:41:21.169303: step 2293, loss 0.553721.
Train: 2018-08-06T00:41:21.422628: step 2294, loss 0.527171.
Train: 2018-08-06T00:41:21.667942: step 2295, loss 0.536031.
Train: 2018-08-06T00:41:21.923284: step 2296, loss 0.562234.
Train: 2018-08-06T00:41:22.172623: step 2297, loss 0.615058.
Train: 2018-08-06T00:41:22.414943: step 2298, loss 0.579799.
Train: 2018-08-06T00:41:22.659289: step 2299, loss 0.536132.
Train: 2018-08-06T00:41:22.901641: step 2300, loss 0.562525.
Test: 2018-08-06T00:41:24.179225: step 2300, loss 0.547031.
Train: 2018-08-06T00:41:25.094677: step 2301, loss 0.579775.
Train: 2018-08-06T00:41:25.336032: step 2302, loss 0.605834.
Train: 2018-08-06T00:41:25.573395: step 2303, loss 0.484543.
Train: 2018-08-06T00:41:25.816773: step 2304, loss 0.562451.
Train: 2018-08-06T00:41:26.059127: step 2305, loss 0.570975.
Train: 2018-08-06T00:41:26.303468: step 2306, loss 0.545118.
Train: 2018-08-06T00:41:26.543825: step 2307, loss 0.605102.
Train: 2018-08-06T00:41:26.783160: step 2308, loss 0.630862.
Train: 2018-08-06T00:41:27.033517: step 2309, loss 0.5113.
Train: 2018-08-06T00:41:27.290803: step 2310, loss 0.638646.
Test: 2018-08-06T00:41:28.557415: step 2310, loss 0.548469.
Train: 2018-08-06T00:41:28.791813: step 2311, loss 0.638251.
Train: 2018-08-06T00:41:29.037161: step 2312, loss 0.612776.
Train: 2018-08-06T00:41:29.285499: step 2313, loss 0.562459.
Train: 2018-08-06T00:41:29.537819: step 2314, loss 0.554167.
Train: 2018-08-06T00:41:29.794111: step 2315, loss 0.562474.
Train: 2018-08-06T00:41:30.042445: step 2316, loss 0.513389.
Train: 2018-08-06T00:41:30.285792: step 2317, loss 0.546262.
Train: 2018-08-06T00:41:30.529144: step 2318, loss 0.489366.
Train: 2018-08-06T00:41:30.773489: step 2319, loss 0.55451.
Train: 2018-08-06T00:41:31.022822: step 2320, loss 0.497609.
Test: 2018-08-06T00:41:32.282453: step 2320, loss 0.548293.
Train: 2018-08-06T00:41:32.515858: step 2321, loss 0.56265.
Train: 2018-08-06T00:41:32.761205: step 2322, loss 0.513739.
Train: 2018-08-06T00:41:33.003524: step 2323, loss 0.562615.
Train: 2018-08-06T00:41:33.252858: step 2324, loss 0.505265.
Train: 2018-08-06T00:41:33.500196: step 2325, loss 0.6036.
Train: 2018-08-06T00:41:33.760502: step 2326, loss 0.513224.
Train: 2018-08-06T00:41:34.008851: step 2327, loss 0.513068.
Train: 2018-08-06T00:41:34.270169: step 2328, loss 0.612091.
Train: 2018-08-06T00:41:34.511494: step 2329, loss 0.587294.
Train: 2018-08-06T00:41:34.754840: step 2330, loss 0.554112.
Test: 2018-08-06T00:41:36.020457: step 2330, loss 0.549534.
Train: 2018-08-06T00:41:36.268826: step 2331, loss 0.587337.
Train: 2018-08-06T00:41:36.513140: step 2332, loss 0.595658.
Train: 2018-08-06T00:41:36.760478: step 2333, loss 0.579079.
Train: 2018-08-06T00:41:37.004831: step 2334, loss 0.60392.
Train: 2018-08-06T00:41:37.247176: step 2335, loss 0.521007.
Train: 2018-08-06T00:41:37.500529: step 2336, loss 0.661715.
Train: 2018-08-06T00:41:37.747836: step 2337, loss 0.529473.
Train: 2018-08-06T00:41:37.989220: step 2338, loss 0.603643.
Train: 2018-08-06T00:41:38.235559: step 2339, loss 0.562489.
Train: 2018-08-06T00:41:38.489871: step 2340, loss 0.53801.
Test: 2018-08-06T00:41:39.766438: step 2340, loss 0.548762.
Train: 2018-08-06T00:41:40.006823: step 2341, loss 0.562485.
Train: 2018-08-06T00:41:40.251143: step 2342, loss 0.546216.
Train: 2018-08-06T00:41:40.494491: step 2343, loss 0.578986.
Train: 2018-08-06T00:41:40.738839: step 2344, loss 0.46497.
Train: 2018-08-06T00:41:40.980192: step 2345, loss 0.489357.
Train: 2018-08-06T00:41:41.220578: step 2346, loss 0.54617.
Train: 2018-08-06T00:41:41.465896: step 2347, loss 0.562478.
Train: 2018-08-06T00:41:41.723234: step 2348, loss 0.562407.
Train: 2018-08-06T00:41:41.964561: step 2349, loss 0.587033.
Train: 2018-08-06T00:41:42.213893: step 2350, loss 0.620404.
Test: 2018-08-06T00:41:43.471530: step 2350, loss 0.54892.
Train: 2018-08-06T00:41:43.708920: step 2351, loss 0.463395.
Train: 2018-08-06T00:41:43.960247: step 2352, loss 0.595565.
Train: 2018-08-06T00:41:44.210554: step 2353, loss 0.578746.
Train: 2018-08-06T00:41:44.467891: step 2354, loss 0.570669.
Train: 2018-08-06T00:41:44.709221: step 2355, loss 0.5211.
Train: 2018-08-06T00:41:44.955563: step 2356, loss 0.553995.
Train: 2018-08-06T00:41:45.196949: step 2357, loss 0.57078.
Train: 2018-08-06T00:41:45.441261: step 2358, loss 0.562851.
Train: 2018-08-06T00:41:45.683614: step 2359, loss 0.520789.
Train: 2018-08-06T00:41:45.931981: step 2360, loss 0.595309.
Test: 2018-08-06T00:41:47.218509: step 2360, loss 0.549469.
Train: 2018-08-06T00:41:47.459864: step 2361, loss 0.562285.
Train: 2018-08-06T00:41:47.709198: step 2362, loss 0.528879.
Train: 2018-08-06T00:41:47.970526: step 2363, loss 0.53746.
Train: 2018-08-06T00:41:48.219832: step 2364, loss 0.546001.
Train: 2018-08-06T00:41:48.463215: step 2365, loss 0.579305.
Train: 2018-08-06T00:41:48.710544: step 2366, loss 0.603977.
Train: 2018-08-06T00:41:48.955863: step 2367, loss 0.579257.
Train: 2018-08-06T00:41:49.211181: step 2368, loss 0.645559.
Train: 2018-08-06T00:41:49.455527: step 2369, loss 0.687071.
Train: 2018-08-06T00:41:49.697880: step 2370, loss 0.587245.
Test: 2018-08-06T00:41:50.952523: step 2370, loss 0.548927.
Train: 2018-08-06T00:41:51.186897: step 2371, loss 0.562818.
Train: 2018-08-06T00:41:51.427280: step 2372, loss 0.529574.
Train: 2018-08-06T00:41:51.672598: step 2373, loss 0.529998.
Train: 2018-08-06T00:41:51.917942: step 2374, loss 0.619874.
Train: 2018-08-06T00:41:52.160295: step 2375, loss 0.611222.
Train: 2018-08-06T00:41:52.400681: step 2376, loss 0.594627.
Train: 2018-08-06T00:41:52.647022: step 2377, loss 0.554561.
Train: 2018-08-06T00:41:52.893366: step 2378, loss 0.555002.
Train: 2018-08-06T00:41:53.137712: step 2379, loss 0.499211.
Train: 2018-08-06T00:41:53.382051: step 2380, loss 0.515046.
Test: 2018-08-06T00:41:54.645646: step 2380, loss 0.549088.
Train: 2018-08-06T00:41:54.880019: step 2381, loss 0.555342.
Train: 2018-08-06T00:41:55.129385: step 2382, loss 0.578977.
Train: 2018-08-06T00:41:55.373739: step 2383, loss 0.539286.
Train: 2018-08-06T00:41:55.618077: step 2384, loss 0.531204.
Train: 2018-08-06T00:41:55.865385: step 2385, loss 0.642599.
Train: 2018-08-06T00:41:56.111727: step 2386, loss 0.586775.
Train: 2018-08-06T00:41:56.359065: step 2387, loss 0.626501.
Train: 2018-08-06T00:41:56.600443: step 2388, loss 0.546923.
Train: 2018-08-06T00:41:56.843768: step 2389, loss 0.602619.
Train: 2018-08-06T00:41:57.084124: step 2390, loss 0.578975.
Test: 2018-08-06T00:41:58.344754: step 2390, loss 0.549306.
Train: 2018-08-06T00:41:58.578129: step 2391, loss 0.594508.
Train: 2018-08-06T00:41:58.819515: step 2392, loss 0.523721.
Train: 2018-08-06T00:41:59.060866: step 2393, loss 0.492072.
Train: 2018-08-06T00:41:59.319182: step 2394, loss 0.554918.
Train: 2018-08-06T00:41:59.564491: step 2395, loss 0.62627.
Train: 2018-08-06T00:41:59.817815: step 2396, loss 0.507814.
Train: 2018-08-06T00:42:00.071162: step 2397, loss 0.531491.
Train: 2018-08-06T00:42:00.314485: step 2398, loss 0.515246.
Train: 2018-08-06T00:42:00.562847: step 2399, loss 0.586574.
Train: 2018-08-06T00:42:00.803213: step 2400, loss 0.506863.
Test: 2018-08-06T00:42:02.068793: step 2400, loss 0.549106.
Train: 2018-08-06T00:42:02.978824: step 2401, loss 0.595275.
Train: 2018-08-06T00:42:03.232120: step 2402, loss 0.571277.
Train: 2018-08-06T00:42:03.493453: step 2403, loss 0.618162.
Train: 2018-08-06T00:42:03.738767: step 2404, loss 0.57107.
Train: 2018-08-06T00:42:03.982116: step 2405, loss 0.619722.
Train: 2018-08-06T00:42:04.226461: step 2406, loss 0.554889.
Train: 2018-08-06T00:42:04.468844: step 2407, loss 0.594975.
Train: 2018-08-06T00:42:04.715189: step 2408, loss 0.562935.
Train: 2018-08-06T00:42:04.961496: step 2409, loss 0.587026.
Train: 2018-08-06T00:42:05.205842: step 2410, loss 0.554828.
Test: 2018-08-06T00:42:06.461484: step 2410, loss 0.549841.
Train: 2018-08-06T00:42:06.704833: step 2411, loss 0.586945.
Train: 2018-08-06T00:42:06.958187: step 2412, loss 0.546903.
Train: 2018-08-06T00:42:07.202503: step 2413, loss 0.578592.
Train: 2018-08-06T00:42:07.455826: step 2414, loss 0.586573.
Train: 2018-08-06T00:42:07.701203: step 2415, loss 0.554877.
Train: 2018-08-06T00:42:07.889665: step 2416, loss 0.478418.
Train: 2018-08-06T00:42:08.133046: step 2417, loss 0.595266.
Train: 2018-08-06T00:42:08.389328: step 2418, loss 0.642722.
Train: 2018-08-06T00:42:08.635670: step 2419, loss 0.563077.
Train: 2018-08-06T00:42:08.883010: step 2420, loss 0.523084.
Test: 2018-08-06T00:42:10.172560: step 2420, loss 0.548937.
Train: 2018-08-06T00:42:10.416906: step 2421, loss 0.642241.
Train: 2018-08-06T00:42:10.662276: step 2422, loss 0.570795.
Train: 2018-08-06T00:42:10.908592: step 2423, loss 0.539291.
Train: 2018-08-06T00:42:11.152938: step 2424, loss 0.523231.
Train: 2018-08-06T00:42:11.396287: step 2425, loss 0.618342.
Train: 2018-08-06T00:42:11.637667: step 2426, loss 0.571056.
Train: 2018-08-06T00:42:11.879994: step 2427, loss 0.523781.
Train: 2018-08-06T00:42:12.139301: step 2428, loss 0.642563.
Train: 2018-08-06T00:42:12.383687: step 2429, loss 0.594462.
Train: 2018-08-06T00:42:12.630022: step 2430, loss 0.547191.
Test: 2018-08-06T00:42:13.891614: step 2430, loss 0.548505.
Train: 2018-08-06T00:42:14.132005: step 2431, loss 0.586946.
Train: 2018-08-06T00:42:14.374351: step 2432, loss 0.633931.
Train: 2018-08-06T00:42:14.627646: step 2433, loss 0.524025.
Train: 2018-08-06T00:42:14.879004: step 2434, loss 0.539663.
Train: 2018-08-06T00:42:15.122353: step 2435, loss 0.555718.
Train: 2018-08-06T00:42:15.366704: step 2436, loss 0.555465.
Train: 2018-08-06T00:42:15.613010: step 2437, loss 0.602428.
Train: 2018-08-06T00:42:15.856386: step 2438, loss 0.579055.
Train: 2018-08-06T00:42:16.098711: step 2439, loss 0.555479.
Train: 2018-08-06T00:42:16.345053: step 2440, loss 0.571122.
Test: 2018-08-06T00:42:17.602689: step 2440, loss 0.550037.
Train: 2018-08-06T00:42:17.846065: step 2441, loss 0.461693.
Train: 2018-08-06T00:42:18.091425: step 2442, loss 0.61827.
Train: 2018-08-06T00:42:18.335760: step 2443, loss 0.547556.
Train: 2018-08-06T00:42:18.588079: step 2444, loss 0.468815.
Train: 2018-08-06T00:42:18.837394: step 2445, loss 0.563.
Train: 2018-08-06T00:42:19.079771: step 2446, loss 0.586796.
Train: 2018-08-06T00:42:19.333062: step 2447, loss 0.523248.
Train: 2018-08-06T00:42:19.576417: step 2448, loss 0.5469.
Train: 2018-08-06T00:42:19.821792: step 2449, loss 0.63502.
Train: 2018-08-06T00:42:20.064137: step 2450, loss 0.594935.
Test: 2018-08-06T00:42:21.339701: step 2450, loss 0.549024.
Train: 2018-08-06T00:42:21.575102: step 2451, loss 0.490429.
Train: 2018-08-06T00:42:21.829422: step 2452, loss 0.570804.
Train: 2018-08-06T00:42:22.078752: step 2453, loss 0.52242.
Train: 2018-08-06T00:42:22.325097: step 2454, loss 0.603227.
Train: 2018-08-06T00:42:22.566421: step 2455, loss 0.554553.
Train: 2018-08-06T00:42:22.815792: step 2456, loss 0.52194.
Train: 2018-08-06T00:42:23.059128: step 2457, loss 0.513559.
Train: 2018-08-06T00:42:23.305468: step 2458, loss 0.562709.
Train: 2018-08-06T00:42:23.551811: step 2459, loss 0.538101.
Train: 2018-08-06T00:42:23.803144: step 2460, loss 0.521167.
Test: 2018-08-06T00:42:25.065737: step 2460, loss 0.549432.
Train: 2018-08-06T00:42:25.302105: step 2461, loss 0.463055.
Train: 2018-08-06T00:42:25.544485: step 2462, loss 0.587532.
Train: 2018-08-06T00:42:25.791834: step 2463, loss 0.587276.
Train: 2018-08-06T00:42:26.038135: step 2464, loss 0.545779.
Train: 2018-08-06T00:42:26.285505: step 2465, loss 0.570594.
Train: 2018-08-06T00:42:26.531845: step 2466, loss 0.629671.
Train: 2018-08-06T00:42:26.778157: step 2467, loss 0.579073.
Train: 2018-08-06T00:42:27.020537: step 2468, loss 0.562441.
Train: 2018-08-06T00:42:27.267876: step 2469, loss 0.604712.
Train: 2018-08-06T00:42:27.510225: step 2470, loss 0.5881.
Test: 2018-08-06T00:42:28.783792: step 2470, loss 0.548706.
Train: 2018-08-06T00:42:29.030177: step 2471, loss 0.554145.
Train: 2018-08-06T00:42:29.286483: step 2472, loss 0.554179.
Train: 2018-08-06T00:42:29.532814: step 2473, loss 0.553925.
Train: 2018-08-06T00:42:29.772149: step 2474, loss 0.587561.
Train: 2018-08-06T00:42:30.014502: step 2475, loss 0.570831.
Train: 2018-08-06T00:42:30.270847: step 2476, loss 0.520501.
Train: 2018-08-06T00:42:30.517158: step 2477, loss 0.587387.
Train: 2018-08-06T00:42:30.765524: step 2478, loss 0.595448.
Train: 2018-08-06T00:42:31.010862: step 2479, loss 0.57076.
Train: 2018-08-06T00:42:31.255183: step 2480, loss 0.587621.
Test: 2018-08-06T00:42:32.520799: step 2480, loss 0.549282.
Train: 2018-08-06T00:42:32.762154: step 2481, loss 0.603794.
Train: 2018-08-06T00:42:33.007498: step 2482, loss 0.587446.
Train: 2018-08-06T00:42:33.249848: step 2483, loss 0.546025.
Train: 2018-08-06T00:42:33.498216: step 2484, loss 0.562436.
Train: 2018-08-06T00:42:33.756528: step 2485, loss 0.513802.
Train: 2018-08-06T00:42:33.998870: step 2486, loss 0.611501.
Train: 2018-08-06T00:42:34.245186: step 2487, loss 0.562619.
Train: 2018-08-06T00:42:34.496549: step 2488, loss 0.619315.
Train: 2018-08-06T00:42:34.744881: step 2489, loss 0.514383.
Train: 2018-08-06T00:42:34.989197: step 2490, loss 0.546758.
Test: 2018-08-06T00:42:36.251821: step 2490, loss 0.548784.
Train: 2018-08-06T00:42:36.488189: step 2491, loss 0.530553.
Train: 2018-08-06T00:42:36.779770: step 2492, loss 0.538663.
Train: 2018-08-06T00:42:37.022092: step 2493, loss 0.578856.
Train: 2018-08-06T00:42:37.269465: step 2494, loss 0.554657.
Train: 2018-08-06T00:42:37.521786: step 2495, loss 0.554786.
Train: 2018-08-06T00:42:37.763138: step 2496, loss 0.562721.
Train: 2018-08-06T00:42:38.015468: step 2497, loss 0.603102.
Train: 2018-08-06T00:42:38.269755: step 2498, loss 0.530524.
Train: 2018-08-06T00:42:38.511109: step 2499, loss 0.522431.
Train: 2018-08-06T00:42:38.766452: step 2500, loss 0.578965.
Test: 2018-08-06T00:42:40.054980: step 2500, loss 0.547532.
Train: 2018-08-06T00:42:41.005488: step 2501, loss 0.497991.
Train: 2018-08-06T00:42:41.262827: step 2502, loss 0.603122.
Train: 2018-08-06T00:42:41.506179: step 2503, loss 0.627756.
Train: 2018-08-06T00:42:41.765454: step 2504, loss 0.546476.
Train: 2018-08-06T00:42:42.005836: step 2505, loss 0.554491.
Train: 2018-08-06T00:42:42.261160: step 2506, loss 0.530217.
Train: 2018-08-06T00:42:42.504511: step 2507, loss 0.586999.
Train: 2018-08-06T00:42:42.749823: step 2508, loss 0.587071.
Train: 2018-08-06T00:42:43.004141: step 2509, loss 0.570778.
Train: 2018-08-06T00:42:43.255471: step 2510, loss 0.465161.
Test: 2018-08-06T00:42:44.503133: step 2510, loss 0.550162.
Train: 2018-08-06T00:42:44.793357: step 2511, loss 0.570817.
Train: 2018-08-06T00:42:45.047677: step 2512, loss 0.587018.
Train: 2018-08-06T00:42:45.305986: step 2513, loss 0.619833.
Train: 2018-08-06T00:42:45.548338: step 2514, loss 0.578873.
Train: 2018-08-06T00:42:45.790691: step 2515, loss 0.603433.
Train: 2018-08-06T00:42:46.046032: step 2516, loss 0.554527.
Train: 2018-08-06T00:42:46.291364: step 2517, loss 0.505806.
Train: 2018-08-06T00:42:46.532705: step 2518, loss 0.497661.
Train: 2018-08-06T00:42:46.789020: step 2519, loss 0.562663.
Train: 2018-08-06T00:42:47.044370: step 2520, loss 0.521919.
Test: 2018-08-06T00:42:48.306960: step 2520, loss 0.548768.
Train: 2018-08-06T00:42:48.554298: step 2521, loss 0.587044.
Train: 2018-08-06T00:42:48.797649: step 2522, loss 0.636124.
Train: 2018-08-06T00:42:49.041029: step 2523, loss 0.546307.
Train: 2018-08-06T00:42:49.289362: step 2524, loss 0.578933.
Train: 2018-08-06T00:42:49.535705: step 2525, loss 0.538021.
Train: 2018-08-06T00:42:49.779048: step 2526, loss 0.595344.
Train: 2018-08-06T00:42:50.023370: step 2527, loss 0.587159.
Train: 2018-08-06T00:42:50.276693: step 2528, loss 0.570792.
Train: 2018-08-06T00:42:50.521073: step 2529, loss 0.570801.
Train: 2018-08-06T00:42:50.776390: step 2530, loss 0.627808.
Test: 2018-08-06T00:42:52.040974: step 2530, loss 0.547612.
Train: 2018-08-06T00:42:52.291305: step 2531, loss 0.522011.
Train: 2018-08-06T00:42:52.536674: step 2532, loss 0.554515.
Train: 2018-08-06T00:42:52.778004: step 2533, loss 0.635485.
Train: 2018-08-06T00:42:53.022381: step 2534, loss 0.546609.
Train: 2018-08-06T00:42:53.269719: step 2535, loss 0.538574.
Train: 2018-08-06T00:42:53.514066: step 2536, loss 0.49032.
Train: 2018-08-06T00:42:53.764376: step 2537, loss 0.578913.
Train: 2018-08-06T00:42:54.006717: step 2538, loss 0.643422.
Train: 2018-08-06T00:42:54.252090: step 2539, loss 0.603028.
Train: 2018-08-06T00:42:54.496435: step 2540, loss 0.562804.
Test: 2018-08-06T00:42:55.756039: step 2540, loss 0.547975.
Train: 2018-08-06T00:42:55.995399: step 2541, loss 0.594855.
Train: 2018-08-06T00:42:56.241765: step 2542, loss 0.522883.
Train: 2018-08-06T00:42:56.481100: step 2543, loss 0.514882.
Train: 2018-08-06T00:42:56.732459: step 2544, loss 0.530909.
Train: 2018-08-06T00:42:56.974780: step 2545, loss 0.530904.
Train: 2018-08-06T00:42:57.234086: step 2546, loss 0.53881.
Train: 2018-08-06T00:42:57.489404: step 2547, loss 0.594958.
Train: 2018-08-06T00:42:57.738738: step 2548, loss 0.538695.
Train: 2018-08-06T00:42:57.985077: step 2549, loss 0.570755.
Train: 2018-08-06T00:42:58.241392: step 2550, loss 0.587093.
Test: 2018-08-06T00:42:59.499029: step 2550, loss 0.549417.
Train: 2018-08-06T00:42:59.735422: step 2551, loss 0.562718.
Train: 2018-08-06T00:42:59.979768: step 2552, loss 0.578859.
Train: 2018-08-06T00:43:00.221122: step 2553, loss 0.554569.
Train: 2018-08-06T00:43:00.465476: step 2554, loss 0.498022.
Train: 2018-08-06T00:43:00.712814: step 2555, loss 0.513949.
Train: 2018-08-06T00:43:00.959125: step 2556, loss 0.562597.
Train: 2018-08-06T00:43:01.207460: step 2557, loss 0.570873.
Train: 2018-08-06T00:43:01.463774: step 2558, loss 0.497062.
Train: 2018-08-06T00:43:01.708127: step 2559, loss 0.57073.
Train: 2018-08-06T00:43:01.949476: step 2560, loss 0.52141.
Test: 2018-08-06T00:43:03.221075: step 2560, loss 0.548492.
Train: 2018-08-06T00:43:03.455449: step 2561, loss 0.537719.
Train: 2018-08-06T00:43:03.706810: step 2562, loss 0.50441.
Train: 2018-08-06T00:43:03.950156: step 2563, loss 0.537339.
Train: 2018-08-06T00:43:04.196468: step 2564, loss 0.537285.
Train: 2018-08-06T00:43:04.438819: step 2565, loss 0.562302.
Train: 2018-08-06T00:43:04.692172: step 2566, loss 0.562186.
Train: 2018-08-06T00:43:04.894599: step 2567, loss 0.580457.
Train: 2018-08-06T00:43:05.143963: step 2568, loss 0.630308.
Train: 2018-08-06T00:43:05.387312: step 2569, loss 0.622021.
Train: 2018-08-06T00:43:05.633654: step 2570, loss 0.6133.
Test: 2018-08-06T00:43:06.891260: step 2570, loss 0.548011.
Train: 2018-08-06T00:43:07.125633: step 2571, loss 0.528479.
Train: 2018-08-06T00:43:07.368016: step 2572, loss 0.570778.
Train: 2018-08-06T00:43:07.613354: step 2573, loss 0.596021.
Train: 2018-08-06T00:43:07.857676: step 2574, loss 0.570976.
Train: 2018-08-06T00:43:08.100052: step 2575, loss 0.528803.
Train: 2018-08-06T00:43:08.346368: step 2576, loss 0.587555.
Train: 2018-08-06T00:43:08.591738: step 2577, loss 0.646212.
Train: 2018-08-06T00:43:08.839051: step 2578, loss 0.504299.
Train: 2018-08-06T00:43:09.082431: step 2579, loss 0.520846.
Train: 2018-08-06T00:43:09.338715: step 2580, loss 0.529256.
Test: 2018-08-06T00:43:10.614303: step 2580, loss 0.549391.
Train: 2018-08-06T00:43:10.849698: step 2581, loss 0.554204.
Train: 2018-08-06T00:43:11.091059: step 2582, loss 0.52938.
Train: 2018-08-06T00:43:11.334377: step 2583, loss 0.645305.
Train: 2018-08-06T00:43:11.579746: step 2584, loss 0.587238.
Train: 2018-08-06T00:43:11.825091: step 2585, loss 0.529417.
Train: 2018-08-06T00:43:12.069442: step 2586, loss 0.529527.
Train: 2018-08-06T00:43:12.328751: step 2587, loss 0.595541.
Train: 2018-08-06T00:43:12.575090: step 2588, loss 0.529633.
Train: 2018-08-06T00:43:12.827385: step 2589, loss 0.463853.
Train: 2018-08-06T00:43:13.072728: step 2590, loss 0.496618.
Test: 2018-08-06T00:43:14.326375: step 2590, loss 0.548302.
Train: 2018-08-06T00:43:14.573714: step 2591, loss 0.537688.
Train: 2018-08-06T00:43:14.819116: step 2592, loss 0.554218.
Train: 2018-08-06T00:43:15.064402: step 2593, loss 0.587417.
Train: 2018-08-06T00:43:15.306754: step 2594, loss 0.58738.
Train: 2018-08-06T00:43:15.551125: step 2595, loss 0.54572.
Train: 2018-08-06T00:43:15.803467: step 2596, loss 0.59582.
Train: 2018-08-06T00:43:16.048770: step 2597, loss 0.545645.
Train: 2018-08-06T00:43:16.306112: step 2598, loss 0.595912.
Train: 2018-08-06T00:43:16.562395: step 2599, loss 0.545672.
Train: 2018-08-06T00:43:16.807765: step 2600, loss 0.537315.
Test: 2018-08-06T00:43:18.060389: step 2600, loss 0.549013.
Train: 2018-08-06T00:43:18.969046: step 2601, loss 0.545717.
Train: 2018-08-06T00:43:19.212428: step 2602, loss 0.545748.
Train: 2018-08-06T00:43:19.458763: step 2603, loss 0.604311.
Train: 2018-08-06T00:43:19.703084: step 2604, loss 0.612577.
Train: 2018-08-06T00:43:19.951447: step 2605, loss 0.646015.
Train: 2018-08-06T00:43:20.211724: step 2606, loss 0.504135.
Train: 2018-08-06T00:43:20.464074: step 2607, loss 0.612281.
Train: 2018-08-06T00:43:20.714379: step 2608, loss 0.570734.
Train: 2018-08-06T00:43:20.957777: step 2609, loss 0.521188.
Train: 2018-08-06T00:43:21.198116: step 2610, loss 0.521272.
Test: 2018-08-06T00:43:22.459710: step 2610, loss 0.549885.
Train: 2018-08-06T00:43:22.693112: step 2611, loss 0.578979.
Train: 2018-08-06T00:43:22.935456: step 2612, loss 0.545989.
Train: 2018-08-06T00:43:23.177843: step 2613, loss 0.488455.
Train: 2018-08-06T00:43:23.423161: step 2614, loss 0.57077.
Train: 2018-08-06T00:43:23.668480: step 2615, loss 0.529547.
Train: 2018-08-06T00:43:23.908836: step 2616, loss 0.512911.
Train: 2018-08-06T00:43:24.154206: step 2617, loss 0.529398.
Train: 2018-08-06T00:43:24.396562: step 2618, loss 0.570807.
Train: 2018-08-06T00:43:24.637914: step 2619, loss 0.645594.
Train: 2018-08-06T00:43:24.895197: step 2620, loss 0.562405.
Test: 2018-08-06T00:43:26.174775: step 2620, loss 0.549705.
Train: 2018-08-06T00:43:26.413163: step 2621, loss 0.495894.
Train: 2018-08-06T00:43:26.665488: step 2622, loss 0.545767.
Train: 2018-08-06T00:43:26.916791: step 2623, loss 0.512447.
Train: 2018-08-06T00:43:27.163158: step 2624, loss 0.58742.
Train: 2018-08-06T00:43:27.406515: step 2625, loss 0.554001.
Train: 2018-08-06T00:43:27.651851: step 2626, loss 0.595904.
Train: 2018-08-06T00:43:27.896203: step 2627, loss 0.604383.
Train: 2018-08-06T00:43:28.144534: step 2628, loss 0.621161.
Train: 2018-08-06T00:43:28.387882: step 2629, loss 0.595817.
Train: 2018-08-06T00:43:28.632217: step 2630, loss 0.537487.
Test: 2018-08-06T00:43:29.923749: step 2630, loss 0.548363.
Train: 2018-08-06T00:43:30.169138: step 2631, loss 0.570765.
Train: 2018-08-06T00:43:30.425408: step 2632, loss 0.487841.
Train: 2018-08-06T00:43:30.671750: step 2633, loss 0.554136.
Train: 2018-08-06T00:43:30.917093: step 2634, loss 0.537573.
Train: 2018-08-06T00:43:31.160442: step 2635, loss 0.595629.
Train: 2018-08-06T00:43:31.408778: step 2636, loss 0.603883.
Train: 2018-08-06T00:43:31.652152: step 2637, loss 0.587277.
Train: 2018-08-06T00:43:31.891487: step 2638, loss 0.587281.
Train: 2018-08-06T00:43:32.136831: step 2639, loss 0.562497.
Train: 2018-08-06T00:43:32.379183: step 2640, loss 0.521448.
Test: 2018-08-06T00:43:33.643801: step 2640, loss 0.548653.
Train: 2018-08-06T00:43:33.877177: step 2641, loss 0.603646.
Train: 2018-08-06T00:43:34.129503: step 2642, loss 0.50526.
Train: 2018-08-06T00:43:34.370856: step 2643, loss 0.529913.
Train: 2018-08-06T00:43:34.623207: step 2644, loss 0.619901.
Train: 2018-08-06T00:43:34.867528: step 2645, loss 0.603539.
Train: 2018-08-06T00:43:35.115895: step 2646, loss 0.619734.
Train: 2018-08-06T00:43:35.360228: step 2647, loss 0.513838.
Train: 2018-08-06T00:43:35.606578: step 2648, loss 0.554498.
Train: 2018-08-06T00:43:35.866856: step 2649, loss 0.611392.
Train: 2018-08-06T00:43:36.114225: step 2650, loss 0.554619.
Test: 2018-08-06T00:43:37.387788: step 2650, loss 0.548681.
Train: 2018-08-06T00:43:37.621196: step 2651, loss 0.562734.
Train: 2018-08-06T00:43:37.868528: step 2652, loss 0.611165.
Train: 2018-08-06T00:43:38.111883: step 2653, loss 0.530608.
Train: 2018-08-06T00:43:38.357227: step 2654, loss 0.602964.
Train: 2018-08-06T00:43:38.602575: step 2655, loss 0.562835.
Train: 2018-08-06T00:43:38.846917: step 2656, loss 0.530862.
Train: 2018-08-06T00:43:39.092258: step 2657, loss 0.498945.
Train: 2018-08-06T00:43:39.339569: step 2658, loss 0.562862.
Train: 2018-08-06T00:43:39.598876: step 2659, loss 0.546838.
Train: 2018-08-06T00:43:39.858210: step 2660, loss 0.514723.
Test: 2018-08-06T00:43:41.127786: step 2660, loss 0.548636.
Train: 2018-08-06T00:43:41.363191: step 2661, loss 0.611015.
Train: 2018-08-06T00:43:41.608519: step 2662, loss 0.619105.
Train: 2018-08-06T00:43:41.851849: step 2663, loss 0.611047.
Train: 2018-08-06T00:43:42.102181: step 2664, loss 0.586894.
Train: 2018-08-06T00:43:42.344574: step 2665, loss 0.554814.
Train: 2018-08-06T00:43:42.587915: step 2666, loss 0.634915.
Train: 2018-08-06T00:43:42.831237: step 2667, loss 0.530961.
Train: 2018-08-06T00:43:43.077603: step 2668, loss 0.578873.
Train: 2018-08-06T00:43:43.321951: step 2669, loss 0.586814.
Train: 2018-08-06T00:43:43.564298: step 2670, loss 0.547091.
Test: 2018-08-06T00:43:44.830882: step 2670, loss 0.5492.
Train: 2018-08-06T00:43:45.067251: step 2671, loss 0.562996.
Train: 2018-08-06T00:43:45.315587: step 2672, loss 0.594702.
Train: 2018-08-06T00:43:45.570904: step 2673, loss 0.523481.
Train: 2018-08-06T00:43:45.814279: step 2674, loss 0.539313.
Train: 2018-08-06T00:43:46.071565: step 2675, loss 0.515552.
Train: 2018-08-06T00:43:46.316908: step 2676, loss 0.594709.
Train: 2018-08-06T00:43:46.571230: step 2677, loss 0.602664.
Train: 2018-08-06T00:43:46.814604: step 2678, loss 0.555058.
Train: 2018-08-06T00:43:47.062946: step 2679, loss 0.578853.
Train: 2018-08-06T00:43:47.321250: step 2680, loss 0.5868.
Test: 2018-08-06T00:43:48.577862: step 2680, loss 0.549731.
Train: 2018-08-06T00:43:48.818248: step 2681, loss 0.602668.
Train: 2018-08-06T00:43:49.061569: step 2682, loss 0.523365.
Train: 2018-08-06T00:43:49.304927: step 2683, loss 0.51544.
Train: 2018-08-06T00:43:49.549269: step 2684, loss 0.555049.
Train: 2018-08-06T00:43:49.798597: step 2685, loss 0.491393.
Train: 2018-08-06T00:43:50.056932: step 2686, loss 0.610761.
Train: 2018-08-06T00:43:50.301253: step 2687, loss 0.626823.
Train: 2018-08-06T00:43:50.543636: step 2688, loss 0.570859.
Train: 2018-08-06T00:43:50.785957: step 2689, loss 0.546869.
Train: 2018-08-06T00:43:51.026314: step 2690, loss 0.450816.
Test: 2018-08-06T00:43:52.310878: step 2690, loss 0.548107.
Train: 2018-08-06T00:43:52.544279: step 2691, loss 0.602968.
Train: 2018-08-06T00:43:52.784637: step 2692, loss 0.570831.
Train: 2018-08-06T00:43:53.025966: step 2693, loss 0.603061.
Train: 2018-08-06T00:43:53.271310: step 2694, loss 0.570806.
Train: 2018-08-06T00:43:53.515688: step 2695, loss 0.586963.
Train: 2018-08-06T00:43:53.762032: step 2696, loss 0.522382.
Train: 2018-08-06T00:43:54.009369: step 2697, loss 0.506181.
Train: 2018-08-06T00:43:54.255717: step 2698, loss 0.514114.
Train: 2018-08-06T00:43:54.498061: step 2699, loss 0.530169.
Train: 2018-08-06T00:43:54.740381: step 2700, loss 0.562648.
Test: 2018-08-06T00:43:56.004001: step 2700, loss 0.548547.
Train: 2018-08-06T00:43:56.948345: step 2701, loss 0.628023.
Train: 2018-08-06T00:43:57.188739: step 2702, loss 0.51343.
Train: 2018-08-06T00:43:57.440064: step 2703, loss 0.587188.
Train: 2018-08-06T00:43:57.684377: step 2704, loss 0.521414.
Train: 2018-08-06T00:43:57.944712: step 2705, loss 0.587243.
Train: 2018-08-06T00:43:58.190050: step 2706, loss 0.61202.
Train: 2018-08-06T00:43:58.434371: step 2707, loss 0.529479.
Train: 2018-08-06T00:43:58.682708: step 2708, loss 0.496401.
Train: 2018-08-06T00:43:58.939047: step 2709, loss 0.537621.
Train: 2018-08-06T00:43:59.196365: step 2710, loss 0.471177.
Test: 2018-08-06T00:44:00.454968: step 2710, loss 0.548704.
Train: 2018-08-06T00:44:00.696353: step 2711, loss 0.504112.
Train: 2018-08-06T00:44:00.938674: step 2712, loss 0.604272.
Train: 2018-08-06T00:44:01.189005: step 2713, loss 0.604377.
Train: 2018-08-06T00:44:01.429396: step 2714, loss 0.537109.
Train: 2018-08-06T00:44:01.673735: step 2715, loss 0.511742.
Train: 2018-08-06T00:44:01.928059: step 2716, loss 0.604663.
Train: 2018-08-06T00:44:02.172375: step 2717, loss 0.604735.
Train: 2018-08-06T00:44:02.357904: step 2718, loss 0.45392.
Train: 2018-08-06T00:44:02.615216: step 2719, loss 0.553853.
Train: 2018-08-06T00:44:02.858571: step 2720, loss 0.57936.
Test: 2018-08-06T00:44:04.110192: step 2720, loss 0.548108.
Train: 2018-08-06T00:44:04.361521: step 2721, loss 0.57939.
Train: 2018-08-06T00:44:04.605899: step 2722, loss 0.51968.
Train: 2018-08-06T00:44:04.850244: step 2723, loss 0.55377.
Train: 2018-08-06T00:44:05.095588: step 2724, loss 0.56233.
Train: 2018-08-06T00:44:05.344891: step 2725, loss 0.62218.
Train: 2018-08-06T00:44:05.600207: step 2726, loss 0.596563.
Train: 2018-08-06T00:44:05.846548: step 2727, loss 0.579401.
Train: 2018-08-06T00:44:06.094886: step 2728, loss 0.570866.
Train: 2018-08-06T00:44:06.347238: step 2729, loss 0.562352.
Train: 2018-08-06T00:44:06.589597: step 2730, loss 0.579287.
Test: 2018-08-06T00:44:07.843209: step 2730, loss 0.548645.
Train: 2018-08-06T00:44:08.077583: step 2731, loss 0.596178.
Train: 2018-08-06T00:44:08.322926: step 2732, loss 0.528719.
Train: 2018-08-06T00:44:08.565306: step 2733, loss 0.612792.
Train: 2018-08-06T00:44:08.812617: step 2734, loss 0.620975.
Train: 2018-08-06T00:44:09.065970: step 2735, loss 0.579081.
Train: 2018-08-06T00:44:09.316305: step 2736, loss 0.670269.
Train: 2018-08-06T00:44:09.557692: step 2737, loss 0.529567.
Train: 2018-08-06T00:44:09.803036: step 2738, loss 0.611738.
Train: 2018-08-06T00:44:10.053352: step 2739, loss 0.554491.
Train: 2018-08-06T00:44:10.301695: step 2740, loss 0.546486.
Test: 2018-08-06T00:44:11.574265: step 2740, loss 0.549094.
Train: 2018-08-06T00:44:11.808670: step 2741, loss 0.522424.
Train: 2018-08-06T00:44:12.067976: step 2742, loss 0.466278.
Train: 2018-08-06T00:44:12.320296: step 2743, loss 0.55475.
Train: 2018-08-06T00:44:12.563653: step 2744, loss 0.522579.
Train: 2018-08-06T00:44:12.805971: step 2745, loss 0.546678.
Train: 2018-08-06T00:44:13.066309: step 2746, loss 0.611133.
Train: 2018-08-06T00:44:13.337574: step 2747, loss 0.570811.
Train: 2018-08-06T00:44:13.580930: step 2748, loss 0.570821.
Train: 2018-08-06T00:44:13.824248: step 2749, loss 0.595007.
Train: 2018-08-06T00:44:14.065603: step 2750, loss 0.603047.
Test: 2018-08-06T00:44:15.333213: step 2750, loss 0.548427.
Train: 2018-08-06T00:44:15.569581: step 2751, loss 0.546696.
Train: 2018-08-06T00:44:15.822902: step 2752, loss 0.651193.
Train: 2018-08-06T00:44:16.070272: step 2753, loss 0.562841.
Train: 2018-08-06T00:44:16.314618: step 2754, loss 0.554887.
Train: 2018-08-06T00:44:16.556965: step 2755, loss 0.554933.
Train: 2018-08-06T00:44:16.801317: step 2756, loss 0.515154.
Train: 2018-08-06T00:44:17.044664: step 2757, loss 0.570895.
Train: 2018-08-06T00:44:17.295995: step 2758, loss 0.586823.
Train: 2018-08-06T00:44:17.546294: step 2759, loss 0.507244.
Train: 2018-08-06T00:44:17.800645: step 2760, loss 0.483279.
Test: 2018-08-06T00:44:19.065232: step 2760, loss 0.549544.
Train: 2018-08-06T00:44:19.309578: step 2761, loss 0.578864.
Train: 2018-08-06T00:44:19.558913: step 2762, loss 0.514808.
Train: 2018-08-06T00:44:19.804256: step 2763, loss 0.643146.
Train: 2018-08-06T00:44:20.046608: step 2764, loss 0.522565.
Train: 2018-08-06T00:44:20.298965: step 2765, loss 0.562761.
Train: 2018-08-06T00:44:20.542315: step 2766, loss 0.514275.
Train: 2018-08-06T00:44:20.800617: step 2767, loss 0.554599.
Train: 2018-08-06T00:44:21.043940: step 2768, loss 0.6114.
Train: 2018-08-06T00:44:21.288287: step 2769, loss 0.530109.
Train: 2018-08-06T00:44:21.532632: step 2770, loss 0.562623.
Test: 2018-08-06T00:44:22.833155: step 2770, loss 0.548589.
Train: 2018-08-06T00:44:23.068527: step 2771, loss 0.513607.
Train: 2018-08-06T00:44:23.311900: step 2772, loss 0.611709.
Train: 2018-08-06T00:44:23.564200: step 2773, loss 0.521554.
Train: 2018-08-06T00:44:23.807549: step 2774, loss 0.578975.
Train: 2018-08-06T00:44:24.053891: step 2775, loss 0.587223.
Train: 2018-08-06T00:44:24.311232: step 2776, loss 0.521342.
Train: 2018-08-06T00:44:24.564531: step 2777, loss 0.52126.
Train: 2018-08-06T00:44:24.809870: step 2778, loss 0.587295.
Train: 2018-08-06T00:44:25.054214: step 2779, loss 0.504521.
Train: 2018-08-06T00:44:25.297565: step 2780, loss 0.487758.
Test: 2018-08-06T00:44:26.559191: step 2780, loss 0.548133.
Train: 2018-08-06T00:44:26.793563: step 2781, loss 0.554102.
Train: 2018-08-06T00:44:27.045919: step 2782, loss 0.570771.
Train: 2018-08-06T00:44:27.308188: step 2783, loss 0.554019.
Train: 2018-08-06T00:44:27.552535: step 2784, loss 0.570787.
Train: 2018-08-06T00:44:27.800900: step 2785, loss 0.545523.
Train: 2018-08-06T00:44:28.042224: step 2786, loss 0.570808.
Train: 2018-08-06T00:44:28.301561: step 2787, loss 0.562362.
Train: 2018-08-06T00:44:28.543914: step 2788, loss 0.613127.
Train: 2018-08-06T00:44:28.794212: step 2789, loss 0.494686.
Train: 2018-08-06T00:44:29.043572: step 2790, loss 0.520018.
Test: 2018-08-06T00:44:30.324122: step 2790, loss 0.54799.
Train: 2018-08-06T00:44:30.560534: step 2791, loss 0.545386.
Train: 2018-08-06T00:44:30.819796: step 2792, loss 0.545354.
Train: 2018-08-06T00:44:31.066165: step 2793, loss 0.5283.
Train: 2018-08-06T00:44:31.311513: step 2794, loss 0.536763.
Train: 2018-08-06T00:44:31.552836: step 2795, loss 0.476891.
Train: 2018-08-06T00:44:31.798204: step 2796, loss 0.49374.
Train: 2018-08-06T00:44:32.043523: step 2797, loss 0.545107.
Train: 2018-08-06T00:44:32.294889: step 2798, loss 0.579631.
Train: 2018-08-06T00:44:32.551199: step 2799, loss 0.562346.
Train: 2018-08-06T00:44:32.799500: step 2800, loss 0.544982.
Test: 2018-08-06T00:44:34.069106: step 2800, loss 0.54581.
Train: 2018-08-06T00:44:35.050377: step 2801, loss 0.605903.
Train: 2018-08-06T00:44:35.299738: step 2802, loss 0.501349.
Train: 2018-08-06T00:44:35.545055: step 2803, loss 0.606009.
Train: 2018-08-06T00:44:35.802398: step 2804, loss 0.58856.
Train: 2018-08-06T00:44:36.048709: step 2805, loss 0.536197.
Train: 2018-08-06T00:44:36.299039: step 2806, loss 0.640849.
Train: 2018-08-06T00:44:36.543386: step 2807, loss 0.597152.
Train: 2018-08-06T00:44:36.842827: step 2808, loss 0.597037.
Train: 2018-08-06T00:44:37.091175: step 2809, loss 0.553705.
Train: 2018-08-06T00:44:37.334529: step 2810, loss 0.502083.
Test: 2018-08-06T00:44:38.626044: step 2810, loss 0.547992.
Train: 2018-08-06T00:44:38.870426: step 2811, loss 0.536565.
Train: 2018-08-06T00:44:39.116733: step 2812, loss 0.613772.
Train: 2018-08-06T00:44:39.361134: step 2813, loss 0.545243.
Train: 2018-08-06T00:44:39.603462: step 2814, loss 0.545288.
Train: 2018-08-06T00:44:39.848806: step 2815, loss 0.613394.
Train: 2018-08-06T00:44:40.103128: step 2816, loss 0.553871.
Train: 2018-08-06T00:44:40.347441: step 2817, loss 0.604641.
Train: 2018-08-06T00:44:40.603755: step 2818, loss 0.469698.
Train: 2018-08-06T00:44:40.863074: step 2819, loss 0.59603.
Train: 2018-08-06T00:44:41.114416: step 2820, loss 0.579177.
Test: 2018-08-06T00:44:42.381002: step 2820, loss 0.548806.
Train: 2018-08-06T00:44:42.620363: step 2821, loss 0.554033.
Train: 2018-08-06T00:44:42.866704: step 2822, loss 0.537356.
Train: 2018-08-06T00:44:43.110052: step 2823, loss 0.604124.
Train: 2018-08-06T00:44:43.357392: step 2824, loss 0.579082.
Train: 2018-08-06T00:44:43.606756: step 2825, loss 0.487789.
Train: 2018-08-06T00:44:43.852093: step 2826, loss 0.612207.
Train: 2018-08-06T00:44:44.094451: step 2827, loss 0.545937.
Train: 2018-08-06T00:44:44.340812: step 2828, loss 0.52945.
Train: 2018-08-06T00:44:44.582117: step 2829, loss 0.628547.
Train: 2018-08-06T00:44:44.825496: step 2830, loss 0.529566.
Test: 2018-08-06T00:44:46.098061: step 2830, loss 0.548031.
Train: 2018-08-06T00:44:46.381330: step 2831, loss 0.570758.
Train: 2018-08-06T00:44:46.625650: step 2832, loss 0.521459.
Train: 2018-08-06T00:44:46.869031: step 2833, loss 0.521473.
Train: 2018-08-06T00:44:47.117362: step 2834, loss 0.57898.
Train: 2018-08-06T00:44:47.361707: step 2835, loss 0.562538.
Train: 2018-08-06T00:44:47.615013: step 2836, loss 0.5872.
Train: 2018-08-06T00:44:47.863365: step 2837, loss 0.595412.
Train: 2018-08-06T00:44:48.106714: step 2838, loss 0.529729.
Train: 2018-08-06T00:44:48.359046: step 2839, loss 0.578964.
Train: 2018-08-06T00:44:48.603388: step 2840, loss 0.587155.
Test: 2018-08-06T00:44:49.865985: step 2840, loss 0.548343.
Train: 2018-08-06T00:44:50.101356: step 2841, loss 0.587136.
Train: 2018-08-06T00:44:50.349717: step 2842, loss 0.546254.
Train: 2018-08-06T00:44:50.593040: step 2843, loss 0.513644.
Train: 2018-08-06T00:44:50.846377: step 2844, loss 0.546287.
Train: 2018-08-06T00:44:51.090709: step 2845, loss 0.619759.
Train: 2018-08-06T00:44:51.348053: step 2846, loss 0.570773.
Train: 2018-08-06T00:44:51.592402: step 2847, loss 0.521887.
Train: 2018-08-06T00:44:51.838742: step 2848, loss 0.546332.
Train: 2018-08-06T00:44:52.092032: step 2849, loss 0.562624.
Train: 2018-08-06T00:44:52.342396: step 2850, loss 0.603391.
Test: 2018-08-06T00:44:53.604985: step 2850, loss 0.54808.
Train: 2018-08-06T00:44:53.851327: step 2851, loss 0.595223.
Train: 2018-08-06T00:44:54.097669: step 2852, loss 0.538229.
Train: 2018-08-06T00:44:54.343045: step 2853, loss 0.530119.
Train: 2018-08-06T00:44:54.589384: step 2854, loss 0.619588.
Train: 2018-08-06T00:44:54.832728: step 2855, loss 0.578911.
Train: 2018-08-06T00:44:55.087023: step 2856, loss 0.554561.
Train: 2018-08-06T00:44:55.330402: step 2857, loss 0.562689.
Train: 2018-08-06T00:44:55.575715: step 2858, loss 0.506012.
Train: 2018-08-06T00:44:55.817070: step 2859, loss 0.595105.
Train: 2018-08-06T00:44:56.061417: step 2860, loss 0.481684.
Test: 2018-08-06T00:44:57.331020: step 2860, loss 0.549494.
Train: 2018-08-06T00:44:57.564422: step 2861, loss 0.611366.
Train: 2018-08-06T00:44:57.806748: step 2862, loss 0.627621.
Train: 2018-08-06T00:44:58.051126: step 2863, loss 0.546462.
Train: 2018-08-06T00:44:58.295441: step 2864, loss 0.570794.
Train: 2018-08-06T00:44:58.542781: step 2865, loss 0.562697.
Train: 2018-08-06T00:44:58.800124: step 2866, loss 0.562705.
Train: 2018-08-06T00:44:59.046459: step 2867, loss 0.611257.
Train: 2018-08-06T00:44:59.301751: step 2868, loss 0.546573.
Train: 2018-08-06T00:44:59.492271: step 2869, loss 0.631612.
Train: 2018-08-06T00:44:59.741606: step 2870, loss 0.514471.
Test: 2018-08-06T00:45:01.016165: step 2870, loss 0.550652.
Train: 2018-08-06T00:45:01.252533: step 2871, loss 0.562788.
Train: 2018-08-06T00:45:01.496881: step 2872, loss 0.474404.
Train: 2018-08-06T00:45:01.744250: step 2873, loss 0.56278.
Train: 2018-08-06T00:45:01.990585: step 2874, loss 0.562762.
Train: 2018-08-06T00:45:02.230918: step 2875, loss 0.530467.
Train: 2018-08-06T00:45:02.473269: step 2876, loss 0.603146.
Train: 2018-08-06T00:45:02.713626: step 2877, loss 0.546523.
Train: 2018-08-06T00:45:02.963988: step 2878, loss 0.522181.
Train: 2018-08-06T00:45:03.209331: step 2879, loss 0.595145.
Train: 2018-08-06T00:45:03.453678: step 2880, loss 0.595169.
Test: 2018-08-06T00:45:04.726244: step 2880, loss 0.548887.
Train: 2018-08-06T00:45:04.959619: step 2881, loss 0.570783.
Train: 2018-08-06T00:45:05.200007: step 2882, loss 0.513866.
Train: 2018-08-06T00:45:05.443352: step 2883, loss 0.57892.
Train: 2018-08-06T00:45:05.695682: step 2884, loss 0.578923.
Train: 2018-08-06T00:45:05.950994: step 2885, loss 0.530031.
Train: 2018-08-06T00:45:06.195326: step 2886, loss 0.529987.
Train: 2018-08-06T00:45:06.441682: step 2887, loss 0.68516.
Train: 2018-08-06T00:45:06.690023: step 2888, loss 0.595251.
Train: 2018-08-06T00:45:06.939351: step 2889, loss 0.603351.
Train: 2018-08-06T00:45:07.182675: step 2890, loss 0.587029.
Test: 2018-08-06T00:45:08.458262: step 2890, loss 0.549367.
Train: 2018-08-06T00:45:08.699618: step 2891, loss 0.578895.
Train: 2018-08-06T00:45:08.953963: step 2892, loss 0.619241.
Train: 2018-08-06T00:45:09.208288: step 2893, loss 0.594949.
Train: 2018-08-06T00:45:09.453626: step 2894, loss 0.530845.
Train: 2018-08-06T00:45:09.699943: step 2895, loss 0.554925.
Train: 2018-08-06T00:45:09.945285: step 2896, loss 0.491314.
Train: 2018-08-06T00:45:10.188635: step 2897, loss 0.547035.
Train: 2018-08-06T00:45:10.442987: step 2898, loss 0.570901.
Train: 2018-08-06T00:45:10.688299: step 2899, loss 0.475401.
Train: 2018-08-06T00:45:10.933676: step 2900, loss 0.554928.
Test: 2018-08-06T00:45:12.188305: step 2900, loss 0.549514.
Train: 2018-08-06T00:45:13.107275: step 2901, loss 0.602853.
Train: 2018-08-06T00:45:13.351648: step 2902, loss 0.530821.
Train: 2018-08-06T00:45:13.596966: step 2903, loss 0.530726.
Train: 2018-08-06T00:45:13.850289: step 2904, loss 0.578875.
Train: 2018-08-06T00:45:14.093672: step 2905, loss 0.562756.
Train: 2018-08-06T00:45:14.340015: step 2906, loss 0.554652.
Train: 2018-08-06T00:45:14.585354: step 2907, loss 0.554611.
Train: 2018-08-06T00:45:14.831663: step 2908, loss 0.530237.
Train: 2018-08-06T00:45:15.076045: step 2909, loss 0.56265.
Train: 2018-08-06T00:45:15.322353: step 2910, loss 0.53817.
Test: 2018-08-06T00:45:16.581983: step 2910, loss 0.549133.
Train: 2018-08-06T00:45:16.816381: step 2911, loss 0.570768.
Train: 2018-08-06T00:45:17.062732: step 2912, loss 0.562572.
Train: 2018-08-06T00:45:17.308042: step 2913, loss 0.570761.
Train: 2018-08-06T00:45:17.555405: step 2914, loss 0.546098.
Train: 2018-08-06T00:45:17.813715: step 2915, loss 0.628399.
Train: 2018-08-06T00:45:18.060031: step 2916, loss 0.611929.
Train: 2018-08-06T00:45:18.305375: step 2917, loss 0.480288.
Train: 2018-08-06T00:45:18.560690: step 2918, loss 0.554296.
Train: 2018-08-06T00:45:18.807060: step 2919, loss 0.529568.
Train: 2018-08-06T00:45:19.056366: step 2920, loss 0.537758.
Test: 2018-08-06T00:45:20.316994: step 2920, loss 0.548309.
Train: 2018-08-06T00:45:20.554388: step 2921, loss 0.537698.
Train: 2018-08-06T00:45:20.798706: step 2922, loss 0.54591.
Train: 2018-08-06T00:45:21.047043: step 2923, loss 0.537557.
Train: 2018-08-06T00:45:21.293408: step 2924, loss 0.570762.
Train: 2018-08-06T00:45:21.542718: step 2925, loss 0.545755.
Train: 2018-08-06T00:45:21.783073: step 2926, loss 0.57077.
Train: 2018-08-06T00:45:22.024459: step 2927, loss 0.554042.
Train: 2018-08-06T00:45:22.269773: step 2928, loss 0.512137.
Train: 2018-08-06T00:45:22.513152: step 2929, loss 0.553993.
Train: 2018-08-06T00:45:22.759489: step 2930, loss 0.579205.
Test: 2018-08-06T00:45:24.022085: step 2930, loss 0.548112.
Train: 2018-08-06T00:45:24.269425: step 2931, loss 0.570798.
Train: 2018-08-06T00:45:24.511776: step 2932, loss 0.621385.
Train: 2018-08-06T00:45:24.752134: step 2933, loss 0.545525.
Train: 2018-08-06T00:45:24.997477: step 2934, loss 0.562376.
Train: 2018-08-06T00:45:25.241849: step 2935, loss 0.663367.
Train: 2018-08-06T00:45:25.485197: step 2936, loss 0.528826.
Train: 2018-08-06T00:45:25.734507: step 2937, loss 0.612648.
Train: 2018-08-06T00:45:26.015085: step 2938, loss 0.537374.
Train: 2018-08-06T00:45:26.260403: step 2939, loss 0.562435.
Train: 2018-08-06T00:45:26.501789: step 2940, loss 0.529207.
Test: 2018-08-06T00:45:27.801282: step 2940, loss 0.547643.
Train: 2018-08-06T00:45:28.041639: step 2941, loss 0.504363.
Train: 2018-08-06T00:45:28.288008: step 2942, loss 0.637156.
Train: 2018-08-06T00:45:28.530364: step 2943, loss 0.512762.
Train: 2018-08-06T00:45:28.778668: step 2944, loss 0.587318.
Train: 2018-08-06T00:45:29.030995: step 2945, loss 0.496317.
Train: 2018-08-06T00:45:29.279331: step 2946, loss 0.603854.
Train: 2018-08-06T00:45:29.525671: step 2947, loss 0.53768.
Train: 2018-08-06T00:45:29.769050: step 2948, loss 0.537683.
Train: 2018-08-06T00:45:30.018384: step 2949, loss 0.562485.
Train: 2018-08-06T00:45:30.273672: step 2950, loss 0.661773.
Test: 2018-08-06T00:45:31.560230: step 2950, loss 0.55042.
Train: 2018-08-06T00:45:31.802612: step 2951, loss 0.537728.
Train: 2018-08-06T00:45:32.041942: step 2952, loss 0.570756.
Train: 2018-08-06T00:45:32.290278: step 2953, loss 0.562523.
Train: 2018-08-06T00:45:32.536620: step 2954, loss 0.587203.
Train: 2018-08-06T00:45:32.778983: step 2955, loss 0.644625.
Train: 2018-08-06T00:45:33.022345: step 2956, loss 0.578945.
Train: 2018-08-06T00:45:33.269658: step 2957, loss 0.55448.
Train: 2018-08-06T00:45:33.515012: step 2958, loss 0.587031.
Train: 2018-08-06T00:45:33.759349: step 2959, loss 0.578894.
Train: 2018-08-06T00:45:34.008682: step 2960, loss 0.490135.
Test: 2018-08-06T00:45:35.265321: step 2960, loss 0.54875.
Train: 2018-08-06T00:45:35.509694: step 2961, loss 0.627226.
Train: 2018-08-06T00:45:35.757014: step 2962, loss 0.522615.
Train: 2018-08-06T00:45:36.011326: step 2963, loss 0.602948.
Train: 2018-08-06T00:45:36.258690: step 2964, loss 0.530808.
Train: 2018-08-06T00:45:36.505005: step 2965, loss 0.554859.
Train: 2018-08-06T00:45:36.759357: step 2966, loss 0.562869.
Train: 2018-08-06T00:45:37.007692: step 2967, loss 0.562876.
Train: 2018-08-06T00:45:37.250044: step 2968, loss 0.570871.
Train: 2018-08-06T00:45:37.496385: step 2969, loss 0.602825.
Train: 2018-08-06T00:45:37.744690: step 2970, loss 0.483119.
Test: 2018-08-06T00:45:39.007313: step 2970, loss 0.548999.
Train: 2018-08-06T00:45:39.241687: step 2971, loss 0.570875.
Train: 2018-08-06T00:45:39.488059: step 2972, loss 0.530905.
Train: 2018-08-06T00:45:39.733403: step 2973, loss 0.538836.
Train: 2018-08-06T00:45:39.975725: step 2974, loss 0.562823.
Train: 2018-08-06T00:45:40.216115: step 2975, loss 0.522601.
Train: 2018-08-06T00:45:40.462448: step 2976, loss 0.522449.
Train: 2018-08-06T00:45:40.703776: step 2977, loss 0.578892.
Train: 2018-08-06T00:45:40.951146: step 2978, loss 0.497768.
Train: 2018-08-06T00:45:41.198485: step 2979, loss 0.595218.
Train: 2018-08-06T00:45:41.444797: step 2980, loss 0.57894.
Test: 2018-08-06T00:45:42.727366: step 2980, loss 0.550595.
Train: 2018-08-06T00:45:42.960777: step 2981, loss 0.562575.
Train: 2018-08-06T00:45:43.213092: step 2982, loss 0.595378.
Train: 2018-08-06T00:45:43.457443: step 2983, loss 0.52969.
Train: 2018-08-06T00:45:43.703753: step 2984, loss 0.521395.
Train: 2018-08-06T00:45:43.962064: step 2985, loss 0.58725.
Train: 2018-08-06T00:45:44.203417: step 2986, loss 0.496426.
Train: 2018-08-06T00:45:44.447789: step 2987, loss 0.479655.
Train: 2018-08-06T00:45:44.690117: step 2988, loss 0.620667.
Train: 2018-08-06T00:45:44.933490: step 2989, loss 0.604117.
Train: 2018-08-06T00:45:45.193800: step 2990, loss 0.620852.
Test: 2018-08-06T00:45:46.461379: step 2990, loss 0.54964.
Train: 2018-08-06T00:45:46.696781: step 2991, loss 0.562423.
Train: 2018-08-06T00:45:46.945086: step 2992, loss 0.512386.
Train: 2018-08-06T00:45:47.187436: step 2993, loss 0.654216.
Train: 2018-08-06T00:45:47.446744: step 2994, loss 0.604088.
Train: 2018-08-06T00:45:47.694113: step 2995, loss 0.512589.
Train: 2018-08-06T00:45:47.939428: step 2996, loss 0.52926.
Train: 2018-08-06T00:45:48.184771: step 2997, loss 0.562462.
Train: 2018-08-06T00:45:48.430114: step 2998, loss 0.545883.
Train: 2018-08-06T00:45:48.680444: step 2999, loss 0.537597.
Train: 2018-08-06T00:45:48.922798: step 3000, loss 0.587343.
Test: 2018-08-06T00:45:50.192400: step 3000, loss 0.549005.
Train: 2018-08-06T00:45:51.119565: step 3001, loss 0.595627.
Train: 2018-08-06T00:45:51.365906: step 3002, loss 0.587317.
Train: 2018-08-06T00:45:51.610251: step 3003, loss 0.512892.
Train: 2018-08-06T00:45:51.850640: step 3004, loss 0.554231.
Train: 2018-08-06T00:45:52.094957: step 3005, loss 0.612056.
Train: 2018-08-06T00:45:52.356258: step 3006, loss 0.529517.
Train: 2018-08-06T00:45:52.602597: step 3007, loss 0.603728.
Train: 2018-08-06T00:45:52.845977: step 3008, loss 0.546068.
Train: 2018-08-06T00:45:53.093285: step 3009, loss 0.587201.
Train: 2018-08-06T00:45:53.344644: step 3010, loss 0.595385.
Test: 2018-08-06T00:45:54.623194: step 3010, loss 0.549085.
Train: 2018-08-06T00:45:54.860587: step 3011, loss 0.513434.
Train: 2018-08-06T00:45:55.107898: step 3012, loss 0.578949.
Train: 2018-08-06T00:45:55.365210: step 3013, loss 0.587114.
Train: 2018-08-06T00:45:55.609557: step 3014, loss 0.619729.
Train: 2018-08-06T00:45:55.857893: step 3015, loss 0.538235.
Train: 2018-08-06T00:45:56.111239: step 3016, loss 0.611388.
Train: 2018-08-06T00:45:56.352587: step 3017, loss 0.538412.
Train: 2018-08-06T00:45:56.610877: step 3018, loss 0.562727.
Train: 2018-08-06T00:45:56.856222: step 3019, loss 0.554684.
Train: 2018-08-06T00:45:57.043721: step 3020, loss 0.562767.
Test: 2018-08-06T00:45:58.312328: step 3020, loss 0.550642.
Train: 2018-08-06T00:45:58.559691: step 3021, loss 0.514507.
Train: 2018-08-06T00:45:58.804028: step 3022, loss 0.506447.
Train: 2018-08-06T00:45:59.051353: step 3023, loss 0.498275.
Train: 2018-08-06T00:45:59.297693: step 3024, loss 0.538465.
Train: 2018-08-06T00:45:59.545073: step 3025, loss 0.562679.
Train: 2018-08-06T00:45:59.803341: step 3026, loss 0.513823.
Train: 2018-08-06T00:46:00.049717: step 3027, loss 0.497254.
Train: 2018-08-06T00:46:00.309002: step 3028, loss 0.562551.
Train: 2018-08-06T00:46:00.554365: step 3029, loss 0.513037.
Train: 2018-08-06T00:46:00.797707: step 3030, loss 0.56247.
Test: 2018-08-06T00:46:02.049334: step 3030, loss 0.547004.
Train: 2018-08-06T00:46:02.286699: step 3031, loss 0.595735.
Train: 2018-08-06T00:46:02.535036: step 3032, loss 0.60417.
Train: 2018-08-06T00:46:02.782375: step 3033, loss 0.629327.
Train: 2018-08-06T00:46:03.030710: step 3034, loss 0.54568.
Train: 2018-08-06T00:46:03.275056: step 3035, loss 0.554041.
Train: 2018-08-06T00:46:03.534389: step 3036, loss 0.579142.
Train: 2018-08-06T00:46:03.780733: step 3037, loss 0.562408.
Train: 2018-08-06T00:46:04.038041: step 3038, loss 0.59586.
Train: 2018-08-06T00:46:04.283360: step 3039, loss 0.53736.
Train: 2018-08-06T00:46:04.527707: step 3040, loss 0.529035.
Test: 2018-08-06T00:46:05.786340: step 3040, loss 0.548481.
Train: 2018-08-06T00:46:06.025700: step 3041, loss 0.612502.
Train: 2018-08-06T00:46:06.270046: step 3042, loss 0.587439.
Train: 2018-08-06T00:46:06.511407: step 3043, loss 0.662304.
Train: 2018-08-06T00:46:06.753752: step 3044, loss 0.54589.
Train: 2018-08-06T00:46:06.997132: step 3045, loss 0.554232.
Train: 2018-08-06T00:46:07.243476: step 3046, loss 0.603711.
Train: 2018-08-06T00:46:07.484829: step 3047, loss 0.488667.
Train: 2018-08-06T00:46:07.730142: step 3048, loss 0.546172.
Train: 2018-08-06T00:46:07.974487: step 3049, loss 0.603518.
Train: 2018-08-06T00:46:08.220860: step 3050, loss 0.562596.
Test: 2018-08-06T00:46:09.490434: step 3050, loss 0.549176.
Train: 2018-08-06T00:46:09.728797: step 3051, loss 0.513663.
Train: 2018-08-06T00:46:09.974153: step 3052, loss 0.562619.
Train: 2018-08-06T00:46:10.233448: step 3053, loss 0.497406.
Train: 2018-08-06T00:46:10.480787: step 3054, loss 0.546286.
Train: 2018-08-06T00:46:10.727127: step 3055, loss 0.554421.
Train: 2018-08-06T00:46:10.978485: step 3056, loss 0.603508.
Train: 2018-08-06T00:46:11.228785: step 3057, loss 0.497066.
Train: 2018-08-06T00:46:11.475154: step 3058, loss 0.562558.
Train: 2018-08-06T00:46:11.734433: step 3059, loss 0.636493.
Train: 2018-08-06T00:46:11.980805: step 3060, loss 0.562545.
Test: 2018-08-06T00:46:13.245391: step 3060, loss 0.549401.
Train: 2018-08-06T00:46:13.495723: step 3061, loss 0.587183.
Train: 2018-08-06T00:46:13.741066: step 3062, loss 0.496921.
Train: 2018-08-06T00:46:13.986411: step 3063, loss 0.55434.
Train: 2018-08-06T00:46:14.246714: step 3064, loss 0.620058.
Train: 2018-08-06T00:46:14.494054: step 3065, loss 0.546127.
Train: 2018-08-06T00:46:14.738429: step 3066, loss 0.595387.
Train: 2018-08-06T00:46:14.983773: step 3067, loss 0.611764.
Train: 2018-08-06T00:46:15.232104: step 3068, loss 0.529849.
Train: 2018-08-06T00:46:15.480414: step 3069, loss 0.497203.
Train: 2018-08-06T00:46:15.737753: step 3070, loss 0.513523.
Test: 2018-08-06T00:46:16.999378: step 3070, loss 0.548329.
Train: 2018-08-06T00:46:17.234725: step 3071, loss 0.578954.
Train: 2018-08-06T00:46:17.480068: step 3072, loss 0.546166.
Train: 2018-08-06T00:46:17.722444: step 3073, loss 0.628223.
Train: 2018-08-06T00:46:17.968762: step 3074, loss 0.562556.
Train: 2018-08-06T00:46:18.213108: step 3075, loss 0.677387.
Train: 2018-08-06T00:46:18.456456: step 3076, loss 0.587121.
Train: 2018-08-06T00:46:18.700830: step 3077, loss 0.538177.
Train: 2018-08-06T00:46:18.959141: step 3078, loss 0.538265.
Train: 2018-08-06T00:46:19.204480: step 3079, loss 0.570789.
Train: 2018-08-06T00:46:19.461802: step 3080, loss 0.587002.
Test: 2018-08-06T00:46:20.728379: step 3080, loss 0.549955.
Train: 2018-08-06T00:46:20.972768: step 3081, loss 0.55463.
Train: 2018-08-06T00:46:21.217108: step 3082, loss 0.554662.
Train: 2018-08-06T00:46:21.465434: step 3083, loss 0.514364.
Train: 2018-08-06T00:46:21.705767: step 3084, loss 0.538552.
Train: 2018-08-06T00:46:21.952108: step 3085, loss 0.522379.
Train: 2018-08-06T00:46:22.209443: step 3086, loss 0.514202.
Train: 2018-08-06T00:46:22.457755: step 3087, loss 0.570793.
Train: 2018-08-06T00:46:22.716075: step 3088, loss 0.530151.
Train: 2018-08-06T00:46:22.957418: step 3089, loss 0.546326.
Train: 2018-08-06T00:46:23.200803: step 3090, loss 0.513553.
Test: 2018-08-06T00:46:24.462393: step 3090, loss 0.547722.
Train: 2018-08-06T00:46:24.697790: step 3091, loss 0.480513.
Train: 2018-08-06T00:46:24.941115: step 3092, loss 0.587251.
Train: 2018-08-06T00:46:25.195435: step 3093, loss 0.554195.
Train: 2018-08-06T00:46:25.438783: step 3094, loss 0.562449.
Train: 2018-08-06T00:46:25.680163: step 3095, loss 0.637478.
Train: 2018-08-06T00:46:25.921501: step 3096, loss 0.520677.
Train: 2018-08-06T00:46:26.180847: step 3097, loss 0.612592.
Train: 2018-08-06T00:46:26.425146: step 3098, loss 0.604243.
Train: 2018-08-06T00:46:26.674478: step 3099, loss 0.512243.
Train: 2018-08-06T00:46:26.918849: step 3100, loss 0.512223.
Test: 2018-08-06T00:46:28.187431: step 3100, loss 0.548414.
Train: 2018-08-06T00:46:29.156659: step 3101, loss 0.51215.
Train: 2018-08-06T00:46:29.402004: step 3102, loss 0.545604.
Train: 2018-08-06T00:46:29.661343: step 3103, loss 0.579203.
Train: 2018-08-06T00:46:29.914663: step 3104, loss 0.528681.
Train: 2018-08-06T00:46:30.159976: step 3105, loss 0.596122.
Train: 2018-08-06T00:46:30.403354: step 3106, loss 0.646816.
Train: 2018-08-06T00:46:30.654654: step 3107, loss 0.579237.
Train: 2018-08-06T00:46:30.911966: step 3108, loss 0.570795.
Train: 2018-08-06T00:46:31.168313: step 3109, loss 0.511967.
Train: 2018-08-06T00:46:31.412657: step 3110, loss 0.520406.
Test: 2018-08-06T00:46:32.677244: step 3110, loss 0.547011.
Train: 2018-08-06T00:46:32.911650: step 3111, loss 0.587579.
Train: 2018-08-06T00:46:33.169927: step 3112, loss 0.587566.
Train: 2018-08-06T00:46:33.415303: step 3113, loss 0.512111.
Train: 2018-08-06T00:46:33.674577: step 3114, loss 0.621056.
Train: 2018-08-06T00:46:33.934881: step 3115, loss 0.453645.
Train: 2018-08-06T00:46:34.175263: step 3116, loss 0.570776.
Train: 2018-08-06T00:46:34.416594: step 3117, loss 0.50376.
Train: 2018-08-06T00:46:34.664959: step 3118, loss 0.595953.
Train: 2018-08-06T00:46:34.914294: step 3119, loss 0.553995.
Train: 2018-08-06T00:46:35.156638: step 3120, loss 0.579185.
Test: 2018-08-06T00:46:36.425221: step 3120, loss 0.547975.
Train: 2018-08-06T00:46:36.661614: step 3121, loss 0.688361.
Train: 2018-08-06T00:46:36.959237: step 3122, loss 0.637763.
Train: 2018-08-06T00:46:37.206605: step 3123, loss 0.587437.
Train: 2018-08-06T00:46:37.451944: step 3124, loss 0.595649.
Train: 2018-08-06T00:46:37.703260: step 3125, loss 0.554247.
Train: 2018-08-06T00:46:37.945631: step 3126, loss 0.52967.
Train: 2018-08-06T00:46:38.192937: step 3127, loss 0.554383.
Train: 2018-08-06T00:46:38.441298: step 3128, loss 0.578937.
Train: 2018-08-06T00:46:38.684622: step 3129, loss 0.521921.
Train: 2018-08-06T00:46:38.926006: step 3130, loss 0.619557.
Test: 2018-08-06T00:46:40.193585: step 3130, loss 0.548405.
Train: 2018-08-06T00:46:40.442919: step 3131, loss 0.587005.
Train: 2018-08-06T00:46:40.690287: step 3132, loss 0.570807.
Train: 2018-08-06T00:46:40.938593: step 3133, loss 0.586935.
Train: 2018-08-06T00:46:41.182939: step 3134, loss 0.530686.
Train: 2018-08-06T00:46:41.426322: step 3135, loss 0.562837.
Train: 2018-08-06T00:46:41.670666: step 3136, loss 0.546861.
Train: 2018-08-06T00:46:41.918971: step 3137, loss 0.554887.
Train: 2018-08-06T00:46:42.167333: step 3138, loss 0.538929.
Train: 2018-08-06T00:46:42.411654: step 3139, loss 0.610809.
Train: 2018-08-06T00:46:42.655033: step 3140, loss 0.538965.
Test: 2018-08-06T00:46:43.907653: step 3140, loss 0.550133.
Train: 2018-08-06T00:46:44.150006: step 3141, loss 0.562905.
Train: 2018-08-06T00:46:44.393353: step 3142, loss 0.554929.
Train: 2018-08-06T00:46:44.644710: step 3143, loss 0.594818.
Train: 2018-08-06T00:46:44.888058: step 3144, loss 0.57886.
Train: 2018-08-06T00:46:45.133375: step 3145, loss 0.523067.
Train: 2018-08-06T00:46:45.380739: step 3146, loss 0.52304.
Train: 2018-08-06T00:46:45.626058: step 3147, loss 0.514969.
Train: 2018-08-06T00:46:45.871425: step 3148, loss 0.562849.
Train: 2018-08-06T00:46:46.116751: step 3149, loss 0.48254.
Train: 2018-08-06T00:46:46.362088: step 3150, loss 0.554696.
Test: 2018-08-06T00:46:47.632690: step 3150, loss 0.549375.
Train: 2018-08-06T00:46:47.913970: step 3151, loss 0.595082.
Train: 2018-08-06T00:46:48.159308: step 3152, loss 0.578906.
Train: 2018-08-06T00:46:48.404627: step 3153, loss 0.49756.
Train: 2018-08-06T00:46:48.649972: step 3154, loss 0.619758.
Train: 2018-08-06T00:46:48.894350: step 3155, loss 0.611664.
Train: 2018-08-06T00:46:49.151630: step 3156, loss 0.546215.
Train: 2018-08-06T00:46:49.398967: step 3157, loss 0.554383.
Train: 2018-08-06T00:46:49.652291: step 3158, loss 0.570762.
Train: 2018-08-06T00:46:49.907637: step 3159, loss 0.578964.
Train: 2018-08-06T00:46:50.156939: step 3160, loss 0.603576.
Test: 2018-08-06T00:46:51.406597: step 3160, loss 0.548689.
Train: 2018-08-06T00:46:51.639975: step 3161, loss 0.595352.
Train: 2018-08-06T00:46:51.892298: step 3162, loss 0.480745.
Train: 2018-08-06T00:46:52.142630: step 3163, loss 0.58714.
Train: 2018-08-06T00:46:52.387975: step 3164, loss 0.587137.
Train: 2018-08-06T00:46:52.632320: step 3165, loss 0.587127.
Train: 2018-08-06T00:46:52.886640: step 3166, loss 0.595279.
Train: 2018-08-06T00:46:53.131984: step 3167, loss 0.587083.
Train: 2018-08-06T00:46:53.375333: step 3168, loss 0.578916.
Train: 2018-08-06T00:46:53.623703: step 3169, loss 0.570789.
Train: 2018-08-06T00:46:53.870010: step 3170, loss 0.578895.
Test: 2018-08-06T00:46:55.125652: step 3170, loss 0.549058.
Train: 2018-08-06T00:46:55.305171: step 3171, loss 0.614424.
Train: 2018-08-06T00:46:55.550547: step 3172, loss 0.619122.
Train: 2018-08-06T00:46:55.792899: step 3173, loss 0.54681.
Train: 2018-08-06T00:46:56.038212: step 3174, loss 0.546916.
Train: 2018-08-06T00:46:56.288541: step 3175, loss 0.491242.
Train: 2018-08-06T00:46:56.530920: step 3176, loss 0.531092.
Train: 2018-08-06T00:46:56.775240: step 3177, loss 0.570894.
Train: 2018-08-06T00:46:57.021582: step 3178, loss 0.658534.
Train: 2018-08-06T00:46:57.267957: step 3179, loss 0.562954.
Train: 2018-08-06T00:46:57.517256: step 3180, loss 0.594738.
Test: 2018-08-06T00:46:58.788855: step 3180, loss 0.549051.
Train: 2018-08-06T00:46:59.023229: step 3181, loss 0.515474.
Train: 2018-08-06T00:46:59.269571: step 3182, loss 0.539266.
Train: 2018-08-06T00:46:59.513941: step 3183, loss 0.570939.
Train: 2018-08-06T00:46:59.758262: step 3184, loss 0.523404.
Train: 2018-08-06T00:47:00.005601: step 3185, loss 0.547129.
Train: 2018-08-06T00:47:00.253937: step 3186, loss 0.4994.
Train: 2018-08-06T00:47:00.502304: step 3187, loss 0.539002.
Train: 2018-08-06T00:47:00.746619: step 3188, loss 0.594863.
Train: 2018-08-06T00:47:00.990001: step 3189, loss 0.586888.
Train: 2018-08-06T00:47:01.237322: step 3190, loss 0.514593.
Test: 2018-08-06T00:47:02.506912: step 3190, loss 0.55042.
Train: 2018-08-06T00:47:02.743314: step 3191, loss 0.546648.
Train: 2018-08-06T00:47:02.985631: step 3192, loss 0.627379.
Train: 2018-08-06T00:47:03.233968: step 3193, loss 0.570801.
Train: 2018-08-06T00:47:03.479336: step 3194, loss 0.538401.
Train: 2018-08-06T00:47:03.721664: step 3195, loss 0.619456.
Train: 2018-08-06T00:47:03.967007: step 3196, loss 0.603234.
Train: 2018-08-06T00:47:04.212376: step 3197, loss 0.578898.
Train: 2018-08-06T00:47:04.456724: step 3198, loss 0.5708.
Train: 2018-08-06T00:47:04.700047: step 3199, loss 0.578889.
Train: 2018-08-06T00:47:04.951399: step 3200, loss 0.603099.
Test: 2018-08-06T00:47:06.249903: step 3200, loss 0.549134.
Train: 2018-08-06T00:47:07.161829: step 3201, loss 0.578877.
Train: 2018-08-06T00:47:07.406169: step 3202, loss 0.570836.
Train: 2018-08-06T00:47:07.649519: step 3203, loss 0.562832.
Train: 2018-08-06T00:47:07.900821: step 3204, loss 0.570862.
Train: 2018-08-06T00:47:08.153172: step 3205, loss 0.554899.
Train: 2018-08-06T00:47:08.407497: step 3206, loss 0.55493.
Train: 2018-08-06T00:47:08.650852: step 3207, loss 0.554951.
Train: 2018-08-06T00:47:08.912148: step 3208, loss 0.539031.
Train: 2018-08-06T00:47:09.157490: step 3209, loss 0.570891.
Train: 2018-08-06T00:47:09.409791: step 3210, loss 0.562921.
Test: 2018-08-06T00:47:10.692360: step 3210, loss 0.549971.
Train: 2018-08-06T00:47:10.940696: step 3211, loss 0.515089.
Train: 2018-08-06T00:47:11.192024: step 3212, loss 0.562894.
Train: 2018-08-06T00:47:11.433379: step 3213, loss 0.594854.
Train: 2018-08-06T00:47:11.677752: step 3214, loss 0.522858.
Train: 2018-08-06T00:47:11.923070: step 3215, loss 0.530781.
Train: 2018-08-06T00:47:12.165447: step 3216, loss 0.570838.
Train: 2018-08-06T00:47:12.413783: step 3217, loss 0.562777.
Train: 2018-08-06T00:47:12.662102: step 3218, loss 0.59501.
Train: 2018-08-06T00:47:12.905475: step 3219, loss 0.554667.
Train: 2018-08-06T00:47:13.151815: step 3220, loss 0.546563.
Test: 2018-08-06T00:47:14.410417: step 3220, loss 0.548446.
Train: 2018-08-06T00:47:14.646786: step 3221, loss 0.578893.
Train: 2018-08-06T00:47:14.902104: step 3222, loss 0.522193.
Train: 2018-08-06T00:47:15.146480: step 3223, loss 0.627599.
Train: 2018-08-06T00:47:15.391824: step 3224, loss 0.578906.
Train: 2018-08-06T00:47:15.637161: step 3225, loss 0.546445.
Train: 2018-08-06T00:47:15.879521: step 3226, loss 0.489623.
Train: 2018-08-06T00:47:16.120870: step 3227, loss 0.521988.
Train: 2018-08-06T00:47:16.365189: step 3228, loss 0.554465.
Train: 2018-08-06T00:47:16.609562: step 3229, loss 0.570768.
Train: 2018-08-06T00:47:16.868881: step 3230, loss 0.537989.
Test: 2018-08-06T00:47:18.138448: step 3230, loss 0.548639.
Train: 2018-08-06T00:47:18.374816: step 3231, loss 0.496835.
Train: 2018-08-06T00:47:18.623152: step 3232, loss 0.513046.
Train: 2018-08-06T00:47:18.866500: step 3233, loss 0.579038.
Train: 2018-08-06T00:47:19.113839: step 3234, loss 0.56245.
Train: 2018-08-06T00:47:19.362206: step 3235, loss 0.56243.
Train: 2018-08-06T00:47:19.608547: step 3236, loss 0.671053.
Train: 2018-08-06T00:47:19.853859: step 3237, loss 0.57077.
Train: 2018-08-06T00:47:20.100233: step 3238, loss 0.562418.
Train: 2018-08-06T00:47:20.344548: step 3239, loss 0.637537.
Train: 2018-08-06T00:47:20.587931: step 3240, loss 0.570763.
Test: 2018-08-06T00:47:21.858499: step 3240, loss 0.549727.
Train: 2018-08-06T00:47:22.094899: step 3241, loss 0.529225.
Train: 2018-08-06T00:47:22.340244: step 3242, loss 0.587347.
Train: 2018-08-06T00:47:22.585554: step 3243, loss 0.545923.
Train: 2018-08-06T00:47:22.833890: step 3244, loss 0.554225.
Train: 2018-08-06T00:47:23.076242: step 3245, loss 0.579011.
Train: 2018-08-06T00:47:23.323625: step 3246, loss 0.669666.
Train: 2018-08-06T00:47:23.564935: step 3247, loss 0.57897.
Train: 2018-08-06T00:47:23.809306: step 3248, loss 0.529878.
Train: 2018-08-06T00:47:24.054626: step 3249, loss 0.54631.
Train: 2018-08-06T00:47:24.310967: step 3250, loss 0.603328.
Test: 2018-08-06T00:47:25.579547: step 3250, loss 0.549501.
Train: 2018-08-06T00:47:25.813920: step 3251, loss 0.660037.
Train: 2018-08-06T00:47:26.067269: step 3252, loss 0.530443.
Train: 2018-08-06T00:47:26.309595: step 3253, loss 0.578874.
Train: 2018-08-06T00:47:26.556935: step 3254, loss 0.546803.
Train: 2018-08-06T00:47:26.801306: step 3255, loss 0.538893.
Train: 2018-08-06T00:47:27.054603: step 3256, loss 0.610778.
Train: 2018-08-06T00:47:27.296954: step 3257, loss 0.547026.
Train: 2018-08-06T00:47:27.543296: step 3258, loss 0.578858.
Train: 2018-08-06T00:47:27.795652: step 3259, loss 0.563004.
Train: 2018-08-06T00:47:28.051936: step 3260, loss 0.555116.
Test: 2018-08-06T00:47:29.311566: step 3260, loss 0.55021.
Train: 2018-08-06T00:47:29.544946: step 3261, loss 0.539331.
Train: 2018-08-06T00:47:29.785300: step 3262, loss 0.531437.
Train: 2018-08-06T00:47:30.031666: step 3263, loss 0.58677.
Train: 2018-08-06T00:47:30.278014: step 3264, loss 0.586772.
Train: 2018-08-06T00:47:30.523325: step 3265, loss 0.57886.
Train: 2018-08-06T00:47:30.774653: step 3266, loss 0.563048.
Train: 2018-08-06T00:47:31.025981: step 3267, loss 0.499815.
Train: 2018-08-06T00:47:31.276312: step 3268, loss 0.586776.
Train: 2018-08-06T00:47:31.521682: step 3269, loss 0.547162.
Train: 2018-08-06T00:47:31.777004: step 3270, loss 0.451893.
Test: 2018-08-06T00:47:33.068519: step 3270, loss 0.549615.
Train: 2018-08-06T00:47:33.313864: step 3271, loss 0.634637.
Train: 2018-08-06T00:47:33.560204: step 3272, loss 0.530952.
Train: 2018-08-06T00:47:33.803553: step 3273, loss 0.51481.
Train: 2018-08-06T00:47:34.049897: step 3274, loss 0.578872.
Train: 2018-08-06T00:47:34.302220: step 3275, loss 0.57888.
Train: 2018-08-06T00:47:34.543574: step 3276, loss 0.570807.
Train: 2018-08-06T00:47:34.785926: step 3277, loss 0.562702.
Train: 2018-08-06T00:47:35.045264: step 3278, loss 0.522122.
Train: 2018-08-06T00:47:35.288608: step 3279, loss 0.505718.
Train: 2018-08-06T00:47:35.544898: step 3280, loss 0.538118.
Test: 2018-08-06T00:47:36.849407: step 3280, loss 0.548127.
Train: 2018-08-06T00:47:37.084779: step 3281, loss 0.521592.
Train: 2018-08-06T00:47:37.342121: step 3282, loss 0.5296.
Train: 2018-08-06T00:47:37.603392: step 3283, loss 0.595566.
Train: 2018-08-06T00:47:37.846771: step 3284, loss 0.587354.
Train: 2018-08-06T00:47:38.091118: step 3285, loss 0.570761.
Train: 2018-08-06T00:47:38.337428: step 3286, loss 0.545766.
Train: 2018-08-06T00:47:38.588782: step 3287, loss 0.512328.
Train: 2018-08-06T00:47:38.845102: step 3288, loss 0.562404.
Train: 2018-08-06T00:47:39.103381: step 3289, loss 0.570783.
Train: 2018-08-06T00:47:39.350747: step 3290, loss 0.587599.
Test: 2018-08-06T00:47:40.625309: step 3290, loss 0.549492.
Train: 2018-08-06T00:47:40.865699: step 3291, loss 0.604442.
Train: 2018-08-06T00:47:41.109047: step 3292, loss 0.61284.
Train: 2018-08-06T00:47:41.356354: step 3293, loss 0.528802.
Train: 2018-08-06T00:47:41.614665: step 3294, loss 0.528833.
Train: 2018-08-06T00:47:41.864996: step 3295, loss 0.562393.
Train: 2018-08-06T00:47:42.119327: step 3296, loss 0.646261.
Train: 2018-08-06T00:47:42.369646: step 3297, loss 0.537303.
Train: 2018-08-06T00:47:42.611030: step 3298, loss 0.562415.
Train: 2018-08-06T00:47:42.853382: step 3299, loss 0.545742.
Train: 2018-08-06T00:47:43.105702: step 3300, loss 0.529104.
Test: 2018-08-06T00:47:44.392236: step 3300, loss 0.548525.
Train: 2018-08-06T00:47:45.297573: step 3301, loss 0.562435.
Train: 2018-08-06T00:47:45.538903: step 3302, loss 0.495839.
Train: 2018-08-06T00:47:45.784247: step 3303, loss 0.604096.
Train: 2018-08-06T00:47:46.034577: step 3304, loss 0.595759.
Train: 2018-08-06T00:47:46.276928: step 3305, loss 0.604055.
Train: 2018-08-06T00:47:46.522297: step 3306, loss 0.529225.
Train: 2018-08-06T00:47:46.767648: step 3307, loss 0.579056.
Train: 2018-08-06T00:47:47.014987: step 3308, loss 0.612185.
Train: 2018-08-06T00:47:47.271294: step 3309, loss 0.579021.
Train: 2018-08-06T00:47:47.515615: step 3310, loss 0.562515.
Test: 2018-08-06T00:47:48.798185: step 3310, loss 0.54824.
Train: 2018-08-06T00:47:49.035550: step 3311, loss 0.595422.
Train: 2018-08-06T00:47:49.275909: step 3312, loss 0.537979.
Train: 2018-08-06T00:47:49.521283: step 3313, loss 0.587122.
Train: 2018-08-06T00:47:49.771607: step 3314, loss 0.529992.
Train: 2018-08-06T00:47:50.017924: step 3315, loss 0.570778.
Train: 2018-08-06T00:47:50.260306: step 3316, loss 0.522002.
Train: 2018-08-06T00:47:50.507615: step 3317, loss 0.53828.
Train: 2018-08-06T00:47:50.752989: step 3318, loss 0.465126.
Train: 2018-08-06T00:47:50.997335: step 3319, loss 0.562629.
Train: 2018-08-06T00:47:51.244644: step 3320, loss 0.538107.
Test: 2018-08-06T00:47:52.495297: step 3320, loss 0.549848.
Train: 2018-08-06T00:47:52.746627: step 3321, loss 0.570765.
Train: 2018-08-06T00:47:52.936119: step 3322, loss 0.597561.
Train: 2018-08-06T00:47:53.184487: step 3323, loss 0.562547.
Train: 2018-08-06T00:47:53.436780: step 3324, loss 0.570759.
Train: 2018-08-06T00:47:53.697115: step 3325, loss 0.496738.
Train: 2018-08-06T00:47:53.943425: step 3326, loss 0.537794.
Train: 2018-08-06T00:47:54.185803: step 3327, loss 0.537717.
Train: 2018-08-06T00:47:54.431122: step 3328, loss 0.645287.
Train: 2018-08-06T00:47:54.677489: step 3329, loss 0.537622.
Train: 2018-08-06T00:47:54.924832: step 3330, loss 0.6205.
Test: 2018-08-06T00:47:56.189419: step 3330, loss 0.548827.
Train: 2018-08-06T00:47:56.425818: step 3331, loss 0.537618.
Train: 2018-08-06T00:47:56.671130: step 3332, loss 0.570757.
Train: 2018-08-06T00:47:56.912516: step 3333, loss 0.504515.
Train: 2018-08-06T00:47:57.159855: step 3334, loss 0.637057.
Train: 2018-08-06T00:47:57.400180: step 3335, loss 0.628712.
Train: 2018-08-06T00:47:57.646522: step 3336, loss 0.554238.
Train: 2018-08-06T00:47:57.888899: step 3337, loss 0.521308.
Train: 2018-08-06T00:47:58.134244: step 3338, loss 0.620156.
Train: 2018-08-06T00:47:58.377567: step 3339, loss 0.620045.
Train: 2018-08-06T00:47:58.623940: step 3340, loss 0.628068.
Test: 2018-08-06T00:47:59.887529: step 3340, loss 0.54827.
Train: 2018-08-06T00:48:00.136887: step 3341, loss 0.55448.
Train: 2018-08-06T00:48:00.381239: step 3342, loss 0.497749.
Train: 2018-08-06T00:48:00.626553: step 3343, loss 0.538402.
Train: 2018-08-06T00:48:00.868904: step 3344, loss 0.58698.
Train: 2018-08-06T00:48:01.111257: step 3345, loss 0.586961.
Train: 2018-08-06T00:48:01.357597: step 3346, loss 0.506346.
Train: 2018-08-06T00:48:01.606962: step 3347, loss 0.538598.
Train: 2018-08-06T00:48:01.855298: step 3348, loss 0.562763.
Train: 2018-08-06T00:48:02.111582: step 3349, loss 0.595001.
Train: 2018-08-06T00:48:02.357923: step 3350, loss 0.586936.
Test: 2018-08-06T00:48:03.621542: step 3350, loss 0.549518.
Train: 2018-08-06T00:48:03.859906: step 3351, loss 0.546676.
Train: 2018-08-06T00:48:04.105276: step 3352, loss 0.490358.
Train: 2018-08-06T00:48:04.360566: step 3353, loss 0.554702.
Train: 2018-08-06T00:48:04.603916: step 3354, loss 0.546597.
Train: 2018-08-06T00:48:04.855245: step 3355, loss 0.546543.
Train: 2018-08-06T00:48:05.097595: step 3356, loss 0.538379.
Train: 2018-08-06T00:48:05.340945: step 3357, loss 0.538288.
Train: 2018-08-06T00:48:05.588282: step 3358, loss 0.538187.
Train: 2018-08-06T00:48:05.832629: step 3359, loss 0.554422.
Train: 2018-08-06T00:48:06.074981: step 3360, loss 0.57896.
Test: 2018-08-06T00:48:07.342591: step 3360, loss 0.548254.
Train: 2018-08-06T00:48:07.575968: step 3361, loss 0.554327.
Train: 2018-08-06T00:48:07.818351: step 3362, loss 0.603694.
Train: 2018-08-06T00:48:08.065706: step 3363, loss 0.570757.
Train: 2018-08-06T00:48:08.314992: step 3364, loss 0.554262.
Train: 2018-08-06T00:48:08.559363: step 3365, loss 0.562503.
Train: 2018-08-06T00:48:08.811694: step 3366, loss 0.579014.
Train: 2018-08-06T00:48:09.061022: step 3367, loss 0.562498.
Train: 2018-08-06T00:48:09.305342: step 3368, loss 0.587273.
Train: 2018-08-06T00:48:09.549689: step 3369, loss 0.669791.
Train: 2018-08-06T00:48:09.794067: step 3370, loss 0.587212.
Test: 2018-08-06T00:48:11.061645: step 3370, loss 0.548681.
Train: 2018-08-06T00:48:11.300009: step 3371, loss 0.570762.
Train: 2018-08-06T00:48:11.542359: step 3372, loss 0.546253.
Train: 2018-08-06T00:48:11.787736: step 3373, loss 0.554474.
Train: 2018-08-06T00:48:12.036040: step 3374, loss 0.587048.
Train: 2018-08-06T00:48:12.280419: step 3375, loss 0.587015.
Train: 2018-08-06T00:48:12.524764: step 3376, loss 0.586981.
Train: 2018-08-06T00:48:12.767110: step 3377, loss 0.578881.
Train: 2018-08-06T00:48:13.016417: step 3378, loss 0.578873.
Train: 2018-08-06T00:48:13.262760: step 3379, loss 0.626949.
Train: 2018-08-06T00:48:13.508129: step 3380, loss 0.507046.
Test: 2018-08-06T00:48:14.795659: step 3380, loss 0.549095.
Train: 2018-08-06T00:48:15.033049: step 3381, loss 0.62662.
Train: 2018-08-06T00:48:15.279391: step 3382, loss 0.555065.
Train: 2018-08-06T00:48:15.521749: step 3383, loss 0.507686.
Train: 2018-08-06T00:48:15.764069: step 3384, loss 0.507765.
Train: 2018-08-06T00:48:16.022379: step 3385, loss 0.531436.
Train: 2018-08-06T00:48:16.272741: step 3386, loss 0.491785.
Train: 2018-08-06T00:48:16.516090: step 3387, loss 0.547091.
Train: 2018-08-06T00:48:16.762431: step 3388, loss 0.57089.
Train: 2018-08-06T00:48:17.008741: step 3389, loss 0.586856.
Train: 2018-08-06T00:48:17.255113: step 3390, loss 0.522787.
Test: 2018-08-06T00:48:18.528676: step 3390, loss 0.549564.
Train: 2018-08-06T00:48:18.763077: step 3391, loss 0.554764.
Train: 2018-08-06T00:48:19.008424: step 3392, loss 0.554699.
Train: 2018-08-06T00:48:19.265705: step 3393, loss 0.627396.
Train: 2018-08-06T00:48:19.514072: step 3394, loss 0.489862.
Train: 2018-08-06T00:48:19.765368: step 3395, loss 0.595139.
Train: 2018-08-06T00:48:20.011739: step 3396, loss 0.578914.
Train: 2018-08-06T00:48:20.259082: step 3397, loss 0.595202.
Train: 2018-08-06T00:48:20.505421: step 3398, loss 0.578922.
Train: 2018-08-06T00:48:20.752754: step 3399, loss 0.562634.
Train: 2018-08-06T00:48:20.996078: step 3400, loss 0.521918.
Test: 2018-08-06T00:48:22.274658: step 3400, loss 0.548637.
Train: 2018-08-06T00:48:23.185298: step 3401, loss 0.587077.
Train: 2018-08-06T00:48:23.424658: step 3402, loss 0.603386.
Train: 2018-08-06T00:48:23.668002: step 3403, loss 0.546336.
Train: 2018-08-06T00:48:23.914348: step 3404, loss 0.603356.
Train: 2018-08-06T00:48:24.173672: step 3405, loss 0.538241.
Train: 2018-08-06T00:48:24.418998: step 3406, loss 0.595176.
Train: 2018-08-06T00:48:24.666306: step 3407, loss 0.53018.
Train: 2018-08-06T00:48:24.911650: step 3408, loss 0.522074.
Train: 2018-08-06T00:48:25.157019: step 3409, loss 0.57891.
Train: 2018-08-06T00:48:25.402347: step 3410, loss 0.546403.
Test: 2018-08-06T00:48:26.669947: step 3410, loss 0.549629.
Train: 2018-08-06T00:48:26.906317: step 3411, loss 0.538251.
Train: 2018-08-06T00:48:27.153670: step 3412, loss 0.587063.
Train: 2018-08-06T00:48:27.413959: step 3413, loss 0.603362.
Train: 2018-08-06T00:48:27.664295: step 3414, loss 0.57892.
Train: 2018-08-06T00:48:27.909632: step 3415, loss 0.619593.
Train: 2018-08-06T00:48:28.166970: step 3416, loss 0.587025.
Train: 2018-08-06T00:48:28.418306: step 3417, loss 0.603192.
Train: 2018-08-06T00:48:28.662619: step 3418, loss 0.595031.
Train: 2018-08-06T00:48:28.906997: step 3419, loss 0.594963.
Train: 2018-08-06T00:48:29.154335: step 3420, loss 0.586878.
Test: 2018-08-06T00:48:30.417925: step 3420, loss 0.548838.
Train: 2018-08-06T00:48:30.657315: step 3421, loss 0.54694.
Train: 2018-08-06T00:48:30.903656: step 3422, loss 0.547038.
Train: 2018-08-06T00:48:31.149968: step 3423, loss 0.547112.
Train: 2018-08-06T00:48:31.398306: step 3424, loss 0.594708.
Train: 2018-08-06T00:48:31.645642: step 3425, loss 0.555137.
Train: 2018-08-06T00:48:31.888990: step 3426, loss 0.570966.
Train: 2018-08-06T00:48:32.133368: step 3427, loss 0.539436.
Train: 2018-08-06T00:48:32.379679: step 3428, loss 0.634039.
Train: 2018-08-06T00:48:32.624048: step 3429, loss 0.547404.
Train: 2018-08-06T00:48:32.867400: step 3430, loss 0.523876.
Test: 2018-08-06T00:48:34.134984: step 3430, loss 0.549492.
Train: 2018-08-06T00:48:34.380328: step 3431, loss 0.555299.
Train: 2018-08-06T00:48:34.622679: step 3432, loss 0.602453.
Train: 2018-08-06T00:48:34.869022: step 3433, loss 0.586729.
Train: 2018-08-06T00:48:35.116359: step 3434, loss 0.578872.
Train: 2018-08-06T00:48:35.369713: step 3435, loss 0.531801.
Train: 2018-08-06T00:48:35.615059: step 3436, loss 0.594568.
Train: 2018-08-06T00:48:35.859372: step 3437, loss 0.57103.
Train: 2018-08-06T00:48:36.103750: step 3438, loss 0.571034.
Train: 2018-08-06T00:48:36.351083: step 3439, loss 0.524001.
Train: 2018-08-06T00:48:36.600390: step 3440, loss 0.547486.
Test: 2018-08-06T00:48:37.887947: step 3440, loss 0.550934.
Train: 2018-08-06T00:48:38.135310: step 3441, loss 0.547437.
Train: 2018-08-06T00:48:38.434035: step 3442, loss 0.523753.
Train: 2018-08-06T00:48:38.696303: step 3443, loss 0.610448.
Train: 2018-08-06T00:48:38.940680: step 3444, loss 0.515589.
Train: 2018-08-06T00:48:39.184023: step 3445, loss 0.531273.
Train: 2018-08-06T00:48:39.428345: step 3446, loss 0.578859.
Train: 2018-08-06T00:48:39.671695: step 3447, loss 0.554918.
Train: 2018-08-06T00:48:39.913073: step 3448, loss 0.514835.
Train: 2018-08-06T00:48:40.159421: step 3449, loss 0.530665.
Train: 2018-08-06T00:48:40.420692: step 3450, loss 0.546607.
Test: 2018-08-06T00:48:41.693287: step 3450, loss 0.54953.
Train: 2018-08-06T00:48:41.928688: step 3451, loss 0.546484.
Train: 2018-08-06T00:48:42.174028: step 3452, loss 0.546364.
Train: 2018-08-06T00:48:42.415451: step 3453, loss 0.529903.
Train: 2018-08-06T00:48:42.658807: step 3454, loss 0.587181.
Train: 2018-08-06T00:48:42.902124: step 3455, loss 0.504845.
Train: 2018-08-06T00:48:43.150460: step 3456, loss 0.612133.
Train: 2018-08-06T00:48:43.397824: step 3457, loss 0.554163.
Train: 2018-08-06T00:48:43.644164: step 3458, loss 0.554124.
Train: 2018-08-06T00:48:43.902449: step 3459, loss 0.537411.
Train: 2018-08-06T00:48:44.148822: step 3460, loss 0.570772.
Test: 2018-08-06T00:48:45.419392: step 3460, loss 0.547253.
Train: 2018-08-06T00:48:45.653776: step 3461, loss 0.537269.
Train: 2018-08-06T00:48:45.910104: step 3462, loss 0.503619.
Train: 2018-08-06T00:48:46.155469: step 3463, loss 0.587642.
Train: 2018-08-06T00:48:46.393787: step 3464, loss 0.486405.
Train: 2018-08-06T00:48:46.647107: step 3465, loss 0.562355.
Train: 2018-08-06T00:48:46.895443: step 3466, loss 0.536867.
Train: 2018-08-06T00:48:47.139790: step 3467, loss 0.519751.
Train: 2018-08-06T00:48:47.384162: step 3468, loss 0.587977.
Train: 2018-08-06T00:48:47.628484: step 3469, loss 0.519509.
Train: 2018-08-06T00:48:47.874825: step 3470, loss 0.519395.
Test: 2018-08-06T00:48:49.134456: step 3470, loss 0.546562.
Train: 2018-08-06T00:48:49.432691: step 3471, loss 0.553722.
Train: 2018-08-06T00:48:49.681027: step 3472, loss 0.519155.
Train: 2018-08-06T00:48:49.869552: step 3473, loss 0.617786.
Train: 2018-08-06T00:48:50.119852: step 3474, loss 0.605713.
Train: 2018-08-06T00:48:50.366193: step 3475, loss 0.579691.
Train: 2018-08-06T00:48:50.609544: step 3476, loss 0.57101.
Train: 2018-08-06T00:48:50.859874: step 3477, loss 0.493107.
Train: 2018-08-06T00:48:51.104220: step 3478, loss 0.579653.
Train: 2018-08-06T00:48:51.348600: step 3479, loss 0.475843.
Train: 2018-08-06T00:48:51.608901: step 3480, loss 0.562344.
Test: 2018-08-06T00:48:52.868502: step 3480, loss 0.549272.
Train: 2018-08-06T00:48:53.103873: step 3481, loss 0.519027.
Train: 2018-08-06T00:48:53.352234: step 3482, loss 0.501632.
Train: 2018-08-06T00:48:53.605562: step 3483, loss 0.53628.
Train: 2018-08-06T00:48:53.860879: step 3484, loss 0.492691.
Train: 2018-08-06T00:48:54.106192: step 3485, loss 0.501231.
Train: 2018-08-06T00:48:54.352559: step 3486, loss 0.562386.
Train: 2018-08-06T00:48:54.597908: step 3487, loss 0.53603.
Train: 2018-08-06T00:48:54.842249: step 3488, loss 0.518346.
Train: 2018-08-06T00:48:55.084606: step 3489, loss 0.641998.
Train: 2018-08-06T00:48:55.327924: step 3490, loss 0.588978.
Test: 2018-08-06T00:48:56.588553: step 3490, loss 0.548484.
Train: 2018-08-06T00:48:56.821928: step 3491, loss 0.53591.
Train: 2018-08-06T00:48:57.071293: step 3492, loss 0.580118.
Train: 2018-08-06T00:48:57.315609: step 3493, loss 0.544765.
Train: 2018-08-06T00:48:57.559955: step 3494, loss 0.562424.
Train: 2018-08-06T00:48:57.805325: step 3495, loss 0.580045.
Train: 2018-08-06T00:48:58.054633: step 3496, loss 0.553608.
Train: 2018-08-06T00:48:58.301997: step 3497, loss 0.579959.
Train: 2018-08-06T00:48:58.546318: step 3498, loss 0.509818.
Train: 2018-08-06T00:48:58.798642: step 3499, loss 0.527383.
Train: 2018-08-06T00:48:59.043986: step 3500, loss 0.483709.
Test: 2018-08-06T00:49:00.301624: step 3500, loss 0.547368.
Train: 2018-08-06T00:49:01.197111: step 3501, loss 0.623586.
Train: 2018-08-06T00:49:01.439468: step 3502, loss 0.579838.
Train: 2018-08-06T00:49:01.688771: step 3503, loss 0.544926.
Train: 2018-08-06T00:49:01.934114: step 3504, loss 0.614585.
Train: 2018-08-06T00:49:02.191427: step 3505, loss 0.510271.
Train: 2018-08-06T00:49:02.433816: step 3506, loss 0.61433.
Train: 2018-08-06T00:49:02.680158: step 3507, loss 0.56234.
Train: 2018-08-06T00:49:02.940461: step 3508, loss 0.493441.
Train: 2018-08-06T00:49:03.189795: step 3509, loss 0.553737.
Train: 2018-08-06T00:49:03.444125: step 3510, loss 0.502231.
Test: 2018-08-06T00:49:04.711725: step 3510, loss 0.547802.
Train: 2018-08-06T00:49:04.948125: step 3511, loss 0.536583.
Train: 2018-08-06T00:49:05.202412: step 3512, loss 0.622424.
Train: 2018-08-06T00:49:05.460735: step 3513, loss 0.588049.
Train: 2018-08-06T00:49:05.705069: step 3514, loss 0.664966.
Train: 2018-08-06T00:49:05.951440: step 3515, loss 0.553827.
Train: 2018-08-06T00:49:06.198748: step 3516, loss 0.579312.
Train: 2018-08-06T00:49:06.440103: step 3517, loss 0.545475.
Train: 2018-08-06T00:49:06.683452: step 3518, loss 0.553965.
Train: 2018-08-06T00:49:06.926814: step 3519, loss 0.612715.
Train: 2018-08-06T00:49:07.171147: step 3520, loss 0.554066.
Test: 2018-08-06T00:49:08.425792: step 3520, loss 0.550078.
Train: 2018-08-06T00:49:08.662185: step 3521, loss 0.570762.
Train: 2018-08-06T00:49:08.915508: step 3522, loss 0.579047.
Train: 2018-08-06T00:49:09.159829: step 3523, loss 0.529464.
Train: 2018-08-06T00:49:09.407193: step 3524, loss 0.554285.
Train: 2018-08-06T00:49:09.660490: step 3525, loss 0.505024.
Train: 2018-08-06T00:49:09.903863: step 3526, loss 0.513287.
Train: 2018-08-06T00:49:10.158159: step 3527, loss 0.513262.
Train: 2018-08-06T00:49:10.411531: step 3528, loss 0.504948.
Train: 2018-08-06T00:49:10.663857: step 3529, loss 0.546012.
Train: 2018-08-06T00:49:10.907232: step 3530, loss 0.554216.
Test: 2018-08-06T00:49:12.186792: step 3530, loss 0.547092.
Train: 2018-08-06T00:49:12.429136: step 3531, loss 0.537592.
Train: 2018-08-06T00:49:12.685481: step 3532, loss 0.504245.
Train: 2018-08-06T00:49:12.932821: step 3533, loss 0.579113.
Train: 2018-08-06T00:49:13.179130: step 3534, loss 0.579144.
Train: 2018-08-06T00:49:13.422510: step 3535, loss 0.570781.
Train: 2018-08-06T00:49:13.665853: step 3536, loss 0.60438.
Train: 2018-08-06T00:49:13.924171: step 3537, loss 0.595989.
Train: 2018-08-06T00:49:14.175466: step 3538, loss 0.5456.
Train: 2018-08-06T00:49:14.423802: step 3539, loss 0.570783.
Train: 2018-08-06T00:49:14.678121: step 3540, loss 0.537237.
Test: 2018-08-06T00:49:15.943737: step 3540, loss 0.548008.
Train: 2018-08-06T00:49:16.182099: step 3541, loss 0.595932.
Train: 2018-08-06T00:49:16.427444: step 3542, loss 0.545651.
Train: 2018-08-06T00:49:16.672817: step 3543, loss 0.545668.
Train: 2018-08-06T00:49:16.920159: step 3544, loss 0.470388.
Train: 2018-08-06T00:49:17.166486: step 3545, loss 0.62105.
Train: 2018-08-06T00:49:17.418791: step 3546, loss 0.595913.
Train: 2018-08-06T00:49:17.663139: step 3547, loss 0.570775.
Train: 2018-08-06T00:49:17.916491: step 3548, loss 0.587493.
Train: 2018-08-06T00:49:18.159841: step 3549, loss 0.570768.
Train: 2018-08-06T00:49:18.399169: step 3550, loss 0.504123.
Test: 2018-08-06T00:49:19.657804: step 3550, loss 0.547382.
Train: 2018-08-06T00:49:19.894172: step 3551, loss 0.545785.
Train: 2018-08-06T00:49:20.151514: step 3552, loss 0.604061.
Train: 2018-08-06T00:49:20.399820: step 3553, loss 0.579075.
Train: 2018-08-06T00:49:20.642171: step 3554, loss 0.537553.
Train: 2018-08-06T00:49:20.886546: step 3555, loss 0.545877.
Train: 2018-08-06T00:49:21.130864: step 3556, loss 0.496155.
Train: 2018-08-06T00:49:21.390170: step 3557, loss 0.603947.
Train: 2018-08-06T00:49:21.646485: step 3558, loss 0.554167.
Train: 2018-08-06T00:49:21.893855: step 3559, loss 0.537578.
Train: 2018-08-06T00:49:22.137204: step 3560, loss 0.620549.
Test: 2018-08-06T00:49:23.398799: step 3560, loss 0.546715.
Train: 2018-08-06T00:49:23.638184: step 3561, loss 0.554178.
Train: 2018-08-06T00:49:23.880542: step 3562, loss 0.545909.
Train: 2018-08-06T00:49:24.129844: step 3563, loss 0.595594.
Train: 2018-08-06T00:49:24.376213: step 3564, loss 0.504608.
Train: 2018-08-06T00:49:24.625519: step 3565, loss 0.603835.
Train: 2018-08-06T00:49:24.876877: step 3566, loss 0.58728.
Train: 2018-08-06T00:49:25.120229: step 3567, loss 0.653256.
Train: 2018-08-06T00:49:25.377544: step 3568, loss 0.595422.
Train: 2018-08-06T00:49:25.624852: step 3569, loss 0.554388.
Train: 2018-08-06T00:49:25.870221: step 3570, loss 0.587093.
Test: 2018-08-06T00:49:27.120850: step 3570, loss 0.547951.
Train: 2018-08-06T00:49:27.358216: step 3571, loss 0.611435.
Train: 2018-08-06T00:49:27.609544: step 3572, loss 0.497955.
Train: 2018-08-06T00:49:27.856911: step 3573, loss 0.514292.
Train: 2018-08-06T00:49:28.101228: step 3574, loss 0.514351.
Train: 2018-08-06T00:49:28.341618: step 3575, loss 0.61116.
Train: 2018-08-06T00:49:28.584935: step 3576, loss 0.586944.
Train: 2018-08-06T00:49:28.832299: step 3577, loss 0.578877.
Train: 2018-08-06T00:49:29.080637: step 3578, loss 0.586914.
Train: 2018-08-06T00:49:29.343906: step 3579, loss 0.578869.
Train: 2018-08-06T00:49:29.589275: step 3580, loss 0.554833.
Test: 2018-08-06T00:49:30.849878: step 3580, loss 0.549139.
Train: 2018-08-06T00:49:31.089264: step 3581, loss 0.522871.
Train: 2018-08-06T00:49:31.340590: step 3582, loss 0.538878.
Train: 2018-08-06T00:49:31.586938: step 3583, loss 0.54686.
Train: 2018-08-06T00:49:31.839240: step 3584, loss 0.642932.
Train: 2018-08-06T00:49:32.082582: step 3585, loss 0.586863.
Train: 2018-08-06T00:49:32.326959: step 3586, loss 0.58685.
Train: 2018-08-06T00:49:32.572297: step 3587, loss 0.554937.
Train: 2018-08-06T00:49:32.817615: step 3588, loss 0.531075.
Train: 2018-08-06T00:49:33.071982: step 3589, loss 0.570897.
Train: 2018-08-06T00:49:33.319300: step 3590, loss 0.586819.
Test: 2018-08-06T00:49:34.595859: step 3590, loss 0.549117.
Train: 2018-08-06T00:49:34.830261: step 3591, loss 0.547042.
Train: 2018-08-06T00:49:35.074580: step 3592, loss 0.586812.
Train: 2018-08-06T00:49:35.321945: step 3593, loss 0.563004.
Train: 2018-08-06T00:49:35.564303: step 3594, loss 0.65037.
Train: 2018-08-06T00:49:35.808616: step 3595, loss 0.570932.
Train: 2018-08-06T00:49:36.053961: step 3596, loss 0.594679.
Train: 2018-08-06T00:49:36.303293: step 3597, loss 0.578864.
Train: 2018-08-06T00:49:36.554653: step 3598, loss 0.602471.
Train: 2018-08-06T00:49:36.797997: step 3599, loss 0.500445.
Train: 2018-08-06T00:49:37.047329: step 3600, loss 0.578878.
Test: 2018-08-06T00:49:38.318903: step 3600, loss 0.550698.
Train: 2018-08-06T00:49:39.259255: step 3601, loss 0.563228.
Train: 2018-08-06T00:49:39.501638: step 3602, loss 0.563243.
Train: 2018-08-06T00:49:39.756925: step 3603, loss 0.586701.
Train: 2018-08-06T00:49:40.019222: step 3604, loss 0.555457.
Train: 2018-08-06T00:49:40.265565: step 3605, loss 0.547657.
Train: 2018-08-06T00:49:40.525899: step 3606, loss 0.563265.
Train: 2018-08-06T00:49:40.767224: step 3607, loss 0.563254.
Train: 2018-08-06T00:49:41.021542: step 3608, loss 0.547597.
Train: 2018-08-06T00:49:41.267884: step 3609, loss 0.563215.
Train: 2018-08-06T00:49:41.515223: step 3610, loss 0.50829.
Test: 2018-08-06T00:49:42.802779: step 3610, loss 0.549821.
Train: 2018-08-06T00:49:43.056101: step 3611, loss 0.555272.
Train: 2018-08-06T00:49:43.300448: step 3612, loss 0.578863.
Train: 2018-08-06T00:49:43.552773: step 3613, loss 0.578861.
Train: 2018-08-06T00:49:43.799145: step 3614, loss 0.578859.
Train: 2018-08-06T00:49:44.048480: step 3615, loss 0.491559.
Train: 2018-08-06T00:49:44.290802: step 3616, loss 0.539045.
Train: 2018-08-06T00:49:44.534148: step 3617, loss 0.634809.
Train: 2018-08-06T00:49:44.778495: step 3618, loss 0.58687.
Train: 2018-08-06T00:49:45.023839: step 3619, loss 0.554826.
Train: 2018-08-06T00:49:45.272174: step 3620, loss 0.570845.
Test: 2018-08-06T00:49:46.540782: step 3620, loss 0.549398.
Train: 2018-08-06T00:49:46.779145: step 3621, loss 0.635082.
Train: 2018-08-06T00:49:47.033465: step 3622, loss 0.514682.
Train: 2018-08-06T00:49:47.287785: step 3623, loss 0.651115.
Train: 2018-08-06T00:49:47.477278: step 3624, loss 0.477354.
Train: 2018-08-06T00:49:47.724641: step 3625, loss 0.506688.
Train: 2018-08-06T00:49:47.984921: step 3626, loss 0.578872.
Train: 2018-08-06T00:49:48.237277: step 3627, loss 0.603025.
Train: 2018-08-06T00:49:48.488573: step 3628, loss 0.578877.
Train: 2018-08-06T00:49:48.733954: step 3629, loss 0.578878.
Train: 2018-08-06T00:49:48.980258: step 3630, loss 0.570824.
Test: 2018-08-06T00:49:50.253852: step 3630, loss 0.550071.
Train: 2018-08-06T00:49:50.493243: step 3631, loss 0.578876.
Train: 2018-08-06T00:49:50.740550: step 3632, loss 0.554737.
Train: 2018-08-06T00:49:50.988918: step 3633, loss 0.603004.
Train: 2018-08-06T00:49:51.234232: step 3634, loss 0.562803.
Train: 2018-08-06T00:49:51.477579: step 3635, loss 0.562817.
Train: 2018-08-06T00:49:51.723920: step 3636, loss 0.498671.
Train: 2018-08-06T00:49:51.983275: step 3637, loss 0.570842.
Train: 2018-08-06T00:49:52.226577: step 3638, loss 0.546737.
Train: 2018-08-06T00:49:52.479899: step 3639, loss 0.546703.
Train: 2018-08-06T00:49:52.720282: step 3640, loss 0.603042.
Test: 2018-08-06T00:49:54.005818: step 3640, loss 0.549305.
Train: 2018-08-06T00:49:54.242187: step 3641, loss 0.578879.
Train: 2018-08-06T00:49:54.489555: step 3642, loss 0.538586.
Train: 2018-08-06T00:49:54.750826: step 3643, loss 0.522432.
Train: 2018-08-06T00:49:54.996170: step 3644, loss 0.514264.
Train: 2018-08-06T00:49:55.246499: step 3645, loss 0.562697.
Train: 2018-08-06T00:49:55.504809: step 3646, loss 0.587028.
Train: 2018-08-06T00:49:55.758132: step 3647, loss 0.570782.
Train: 2018-08-06T00:49:56.004503: step 3648, loss 0.52191.
Train: 2018-08-06T00:49:56.253806: step 3649, loss 0.570771.
Train: 2018-08-06T00:49:56.499151: step 3650, loss 0.505354.
Test: 2018-08-06T00:49:57.762770: step 3650, loss 0.548299.
Train: 2018-08-06T00:49:58.010134: step 3651, loss 0.578963.
Train: 2018-08-06T00:49:58.255486: step 3652, loss 0.480352.
Train: 2018-08-06T00:49:58.501794: step 3653, loss 0.504751.
Train: 2018-08-06T00:49:58.747139: step 3654, loss 0.612208.
Train: 2018-08-06T00:49:58.991539: step 3655, loss 0.612337.
Train: 2018-08-06T00:49:59.236862: step 3656, loss 0.587418.
Train: 2018-08-06T00:49:59.485165: step 3657, loss 0.537434.
Train: 2018-08-06T00:49:59.727540: step 3658, loss 0.59579.
Train: 2018-08-06T00:49:59.976850: step 3659, loss 0.570766.
Train: 2018-08-06T00:50:00.226216: step 3660, loss 0.579105.
Test: 2018-08-06T00:50:01.505760: step 3660, loss 0.549281.
Train: 2018-08-06T00:50:01.743150: step 3661, loss 0.579098.
Train: 2018-08-06T00:50:02.001465: step 3662, loss 0.579086.
Train: 2018-08-06T00:50:02.249795: step 3663, loss 0.429481.
Train: 2018-08-06T00:50:02.501115: step 3664, loss 0.604057.
Train: 2018-08-06T00:50:02.742485: step 3665, loss 0.512477.
Train: 2018-08-06T00:50:02.987811: step 3666, loss 0.55409.
Train: 2018-08-06T00:50:03.243170: step 3667, loss 0.529025.
Train: 2018-08-06T00:50:03.487493: step 3668, loss 0.579137.
Train: 2018-08-06T00:50:03.728846: step 3669, loss 0.55403.
Train: 2018-08-06T00:50:03.975189: step 3670, loss 0.62107.
Test: 2018-08-06T00:50:05.262713: step 3670, loss 0.548795.
Train: 2018-08-06T00:50:05.511049: step 3671, loss 0.621042.
Train: 2018-08-06T00:50:05.754414: step 3672, loss 0.612578.
Train: 2018-08-06T00:50:06.008719: step 3673, loss 0.570765.
Train: 2018-08-06T00:50:06.251069: step 3674, loss 0.57076.
Train: 2018-08-06T00:50:06.496413: step 3675, loss 0.603899.
Train: 2018-08-06T00:50:06.746745: step 3676, loss 0.570756.
Train: 2018-08-06T00:50:06.995080: step 3677, loss 0.537864.
Train: 2018-08-06T00:50:07.240425: step 3678, loss 0.513356.
Train: 2018-08-06T00:50:07.488760: step 3679, loss 0.64447.
Train: 2018-08-06T00:50:07.736099: step 3680, loss 0.570771.
Test: 2018-08-06T00:50:09.012702: step 3680, loss 0.548489.
Train: 2018-08-06T00:50:09.258028: step 3681, loss 0.513814.
Train: 2018-08-06T00:50:09.500380: step 3682, loss 0.59516.
Train: 2018-08-06T00:50:09.750743: step 3683, loss 0.587007.
Train: 2018-08-06T00:50:09.996055: step 3684, loss 0.627407.
Train: 2018-08-06T00:50:10.250401: step 3685, loss 0.530547.
Train: 2018-08-06T00:50:10.497735: step 3686, loss 0.546735.
Train: 2018-08-06T00:50:10.746062: step 3687, loss 0.602924.
Train: 2018-08-06T00:50:10.995409: step 3688, loss 0.506879.
Train: 2018-08-06T00:50:11.241725: step 3689, loss 0.570871.
Train: 2018-08-06T00:50:11.502026: step 3690, loss 0.570876.
Test: 2018-08-06T00:50:12.761658: step 3690, loss 0.549214.
Train: 2018-08-06T00:50:12.999024: step 3691, loss 0.554927.
Train: 2018-08-06T00:50:13.250352: step 3692, loss 0.554938.
Train: 2018-08-06T00:50:13.501696: step 3693, loss 0.5071.
Train: 2018-08-06T00:50:13.746026: step 3694, loss 0.602815.
Train: 2018-08-06T00:50:13.992404: step 3695, loss 0.554898.
Train: 2018-08-06T00:50:14.247717: step 3696, loss 0.514915.
Train: 2018-08-06T00:50:14.492031: step 3697, loss 0.602891.
Train: 2018-08-06T00:50:14.736408: step 3698, loss 0.4907.
Train: 2018-08-06T00:50:14.976734: step 3699, loss 0.546727.
Train: 2018-08-06T00:50:15.235043: step 3700, loss 0.562762.
Test: 2018-08-06T00:50:16.525592: step 3700, loss 0.548305.
Train: 2018-08-06T00:50:17.430527: step 3701, loss 0.570808.
Train: 2018-08-06T00:50:17.676868: step 3702, loss 0.570798.
Train: 2018-08-06T00:50:17.918221: step 3703, loss 0.619461.
Train: 2018-08-06T00:50:18.171548: step 3704, loss 0.546448.
Train: 2018-08-06T00:50:18.416919: step 3705, loss 0.54643.
Train: 2018-08-06T00:50:18.659240: step 3706, loss 0.570784.
Train: 2018-08-06T00:50:18.916578: step 3707, loss 0.587048.
Train: 2018-08-06T00:50:19.164887: step 3708, loss 0.578915.
Train: 2018-08-06T00:50:19.420230: step 3709, loss 0.562651.
Train: 2018-08-06T00:50:19.660594: step 3710, loss 0.627689.
Test: 2018-08-06T00:50:20.919196: step 3710, loss 0.55043.
Train: 2018-08-06T00:50:21.155589: step 3711, loss 0.55456.
Train: 2018-08-06T00:50:21.400908: step 3712, loss 0.554588.
Train: 2018-08-06T00:50:21.645261: step 3713, loss 0.554609.
Train: 2018-08-06T00:50:21.891620: step 3714, loss 0.619341.
Train: 2018-08-06T00:50:22.139932: step 3715, loss 0.595033.
Train: 2018-08-06T00:50:22.389264: step 3716, loss 0.49834.
Train: 2018-08-06T00:50:22.635630: step 3717, loss 0.627169.
Train: 2018-08-06T00:50:22.879986: step 3718, loss 0.602967.
Train: 2018-08-06T00:50:23.126293: step 3719, loss 0.450706.
Train: 2018-08-06T00:50:23.373632: step 3720, loss 0.64297.
Test: 2018-08-06T00:50:24.630271: step 3720, loss 0.548948.
Train: 2018-08-06T00:50:24.875640: step 3721, loss 0.554862.
Train: 2018-08-06T00:50:25.133925: step 3722, loss 0.522919.
Train: 2018-08-06T00:50:25.376276: step 3723, loss 0.562876.
Train: 2018-08-06T00:50:25.624638: step 3724, loss 0.562873.
Train: 2018-08-06T00:50:25.866989: step 3725, loss 0.522885.
Train: 2018-08-06T00:50:26.118316: step 3726, loss 0.514803.
Train: 2018-08-06T00:50:26.363636: step 3727, loss 0.578869.
Train: 2018-08-06T00:50:26.614964: step 3728, loss 0.603004.
Train: 2018-08-06T00:50:26.862333: step 3729, loss 0.498376.
Train: 2018-08-06T00:50:27.114654: step 3730, loss 0.603091.
Test: 2018-08-06T00:50:28.386226: step 3730, loss 0.54905.
Train: 2018-08-06T00:50:28.624615: step 3731, loss 0.55465.
Train: 2018-08-06T00:50:28.880935: step 3732, loss 0.554622.
Train: 2018-08-06T00:50:29.128243: step 3733, loss 0.457374.
Train: 2018-08-06T00:50:29.375582: step 3734, loss 0.562649.
Train: 2018-08-06T00:50:29.622945: step 3735, loss 0.595255.
Train: 2018-08-06T00:50:29.870257: step 3736, loss 0.521684.
Train: 2018-08-06T00:50:30.119591: step 3737, loss 0.603582.
Train: 2018-08-06T00:50:30.363938: step 3738, loss 0.603637.
Train: 2018-08-06T00:50:30.609281: step 3739, loss 0.620104.
Train: 2018-08-06T00:50:30.862604: step 3740, loss 0.496807.
Test: 2018-08-06T00:50:32.130214: step 3740, loss 0.548425.
Train: 2018-08-06T00:50:32.379573: step 3741, loss 0.578981.
Train: 2018-08-06T00:50:32.624917: step 3742, loss 0.537861.
Train: 2018-08-06T00:50:32.875248: step 3743, loss 0.529607.
Train: 2018-08-06T00:50:33.118572: step 3744, loss 0.570757.
Train: 2018-08-06T00:50:33.374886: step 3745, loss 0.529509.
Train: 2018-08-06T00:50:33.625217: step 3746, loss 0.562494.
Train: 2018-08-06T00:50:33.872585: step 3747, loss 0.628673.
Train: 2018-08-06T00:50:34.129896: step 3748, loss 0.537674.
Train: 2018-08-06T00:50:34.376214: step 3749, loss 0.521126.
Train: 2018-08-06T00:50:34.621551: step 3750, loss 0.612158.
Test: 2018-08-06T00:50:35.890158: step 3750, loss 0.548845.
Train: 2018-08-06T00:50:36.140489: step 3751, loss 0.570757.
Train: 2018-08-06T00:50:36.382868: step 3752, loss 0.570756.
Train: 2018-08-06T00:50:36.632200: step 3753, loss 0.56249.
Train: 2018-08-06T00:50:36.876552: step 3754, loss 0.603798.
Train: 2018-08-06T00:50:37.122892: step 3755, loss 0.554264.
Train: 2018-08-06T00:50:37.378205: step 3756, loss 0.513114.
Train: 2018-08-06T00:50:37.621559: step 3757, loss 0.562524.
Train: 2018-08-06T00:50:37.868868: step 3758, loss 0.529599.
Train: 2018-08-06T00:50:38.116205: step 3759, loss 0.603702.
Train: 2018-08-06T00:50:38.360551: step 3760, loss 0.554294.
Test: 2018-08-06T00:50:39.676900: step 3760, loss 0.547839.
Train: 2018-08-06T00:50:39.912305: step 3761, loss 0.513155.
Train: 2018-08-06T00:50:40.153658: step 3762, loss 0.562521.
Train: 2018-08-06T00:50:40.407975: step 3763, loss 0.496585.
Train: 2018-08-06T00:50:40.653288: step 3764, loss 0.562498.
Train: 2018-08-06T00:50:40.904623: step 3765, loss 0.570756.
Train: 2018-08-06T00:50:41.149998: step 3766, loss 0.661889.
Train: 2018-08-06T00:50:41.398295: step 3767, loss 0.554206.
Train: 2018-08-06T00:50:41.656605: step 3768, loss 0.537686.
Train: 2018-08-06T00:50:41.900951: step 3769, loss 0.488109.
Train: 2018-08-06T00:50:42.143332: step 3770, loss 0.537653.
Test: 2018-08-06T00:50:43.411911: step 3770, loss 0.547859.
Train: 2018-08-06T00:50:43.646284: step 3771, loss 0.562467.
Train: 2018-08-06T00:50:43.890659: step 3772, loss 0.562457.
Train: 2018-08-06T00:50:44.151932: step 3773, loss 0.562448.
Train: 2018-08-06T00:50:44.400292: step 3774, loss 0.554121.
Train: 2018-08-06T00:50:44.602726: step 3775, loss 0.633506.
Train: 2018-08-06T00:50:44.849067: step 3776, loss 0.637334.
Train: 2018-08-06T00:50:45.095409: step 3777, loss 0.562459.
Train: 2018-08-06T00:50:45.338789: step 3778, loss 0.545917.
Train: 2018-08-06T00:50:45.585134: step 3779, loss 0.562492.
Train: 2018-08-06T00:50:45.832438: step 3780, loss 0.529508.
Test: 2018-08-06T00:50:47.108026: step 3780, loss 0.54856.
Train: 2018-08-06T00:50:47.344425: step 3781, loss 0.562514.
Train: 2018-08-06T00:50:47.594765: step 3782, loss 0.595463.
Train: 2018-08-06T00:50:47.839070: step 3783, loss 0.562537.
Train: 2018-08-06T00:50:48.088430: step 3784, loss 0.505082.
Train: 2018-08-06T00:50:48.333748: step 3785, loss 0.611808.
Train: 2018-08-06T00:50:48.578095: step 3786, loss 0.505165.
Train: 2018-08-06T00:50:48.821443: step 3787, loss 0.570762.
Train: 2018-08-06T00:50:49.070808: step 3788, loss 0.529755.
Train: 2018-08-06T00:50:49.318146: step 3789, loss 0.521516.
Train: 2018-08-06T00:50:49.563461: step 3790, loss 0.529656.
Test: 2018-08-06T00:50:50.844035: step 3790, loss 0.548191.
Train: 2018-08-06T00:50:51.112317: step 3791, loss 0.562519.
Train: 2018-08-06T00:50:51.356663: step 3792, loss 0.545995.
Train: 2018-08-06T00:50:51.602032: step 3793, loss 0.554216.
Train: 2018-08-06T00:50:51.859319: step 3794, loss 0.562471.
Train: 2018-08-06T00:50:52.104663: step 3795, loss 0.562459.
Train: 2018-08-06T00:50:52.353034: step 3796, loss 0.545826.
Train: 2018-08-06T00:50:52.596373: step 3797, loss 0.595735.
Train: 2018-08-06T00:50:52.839698: step 3798, loss 0.487478.
Train: 2018-08-06T00:50:53.084069: step 3799, loss 0.595805.
Train: 2018-08-06T00:50:53.332379: step 3800, loss 0.595831.
Test: 2018-08-06T00:50:54.609963: step 3800, loss 0.549618.
Train: 2018-08-06T00:50:55.537358: step 3801, loss 0.545709.
Train: 2018-08-06T00:50:55.780731: step 3802, loss 0.595837.
Train: 2018-08-06T00:50:56.024091: step 3803, loss 0.545718.
Train: 2018-08-06T00:50:56.269400: step 3804, loss 0.595809.
Train: 2018-08-06T00:50:56.514781: step 3805, loss 0.6458.
Train: 2018-08-06T00:50:56.764103: step 3806, loss 0.57076.
Train: 2018-08-06T00:50:57.009471: step 3807, loss 0.579043.
Train: 2018-08-06T00:50:57.253767: step 3808, loss 0.603791.
Train: 2018-08-06T00:50:57.498113: step 3809, loss 0.463822.
Train: 2018-08-06T00:50:57.758418: step 3810, loss 0.63648.
Test: 2018-08-06T00:50:59.018048: step 3810, loss 0.548705.
Train: 2018-08-06T00:50:59.263425: step 3811, loss 0.554382.
Train: 2018-08-06T00:50:59.510758: step 3812, loss 0.546258.
Train: 2018-08-06T00:50:59.751114: step 3813, loss 0.489225.
Train: 2018-08-06T00:50:59.998458: step 3814, loss 0.538152.
Train: 2018-08-06T00:51:00.242774: step 3815, loss 0.627896.
Train: 2018-08-06T00:51:00.494129: step 3816, loss 0.456639.
Train: 2018-08-06T00:51:00.742436: step 3817, loss 0.562604.
Train: 2018-08-06T00:51:00.984788: step 3818, loss 0.562589.
Train: 2018-08-06T00:51:01.227141: step 3819, loss 0.497069.
Train: 2018-08-06T00:51:01.485451: step 3820, loss 0.529708.
Test: 2018-08-06T00:51:02.745081: step 3820, loss 0.549335.
Train: 2018-08-06T00:51:02.982446: step 3821, loss 0.677827.
Train: 2018-08-06T00:51:03.228788: step 3822, loss 0.554283.
Train: 2018-08-06T00:51:03.482110: step 3823, loss 0.562518.
Train: 2018-08-06T00:51:03.738424: step 3824, loss 0.537798.
Train: 2018-08-06T00:51:03.984792: step 3825, loss 0.570757.
Train: 2018-08-06T00:51:04.229139: step 3826, loss 0.52127.
Train: 2018-08-06T00:51:04.480441: step 3827, loss 0.52121.
Train: 2018-08-06T00:51:04.727812: step 3828, loss 0.570756.
Train: 2018-08-06T00:51:04.970163: step 3829, loss 0.496181.
Train: 2018-08-06T00:51:05.215476: step 3830, loss 0.504285.
Test: 2018-08-06T00:51:06.475106: step 3830, loss 0.548497.
Train: 2018-08-06T00:51:06.710477: step 3831, loss 0.587447.
Train: 2018-08-06T00:51:06.957815: step 3832, loss 0.52896.
Train: 2018-08-06T00:51:07.206182: step 3833, loss 0.545618.
Train: 2018-08-06T00:51:07.451496: step 3834, loss 0.63809.
Train: 2018-08-06T00:51:07.695874: step 3835, loss 0.562376.
Train: 2018-08-06T00:51:07.944178: step 3836, loss 0.579221.
Train: 2018-08-06T00:51:08.189521: step 3837, loss 0.60449.
Train: 2018-08-06T00:51:08.432870: step 3838, loss 0.545552.
Train: 2018-08-06T00:51:08.678252: step 3839, loss 0.629635.
Train: 2018-08-06T00:51:08.930572: step 3840, loss 0.629485.
Test: 2018-08-06T00:51:10.191167: step 3840, loss 0.548655.
Train: 2018-08-06T00:51:10.437540: step 3841, loss 0.57077.
Train: 2018-08-06T00:51:10.683876: step 3842, loss 0.595732.
Train: 2018-08-06T00:51:10.929195: step 3843, loss 0.521031.
Train: 2018-08-06T00:51:11.172569: step 3844, loss 0.537704.
Train: 2018-08-06T00:51:11.418885: step 3845, loss 0.570757.
Train: 2018-08-06T00:51:11.664229: step 3846, loss 0.554304.
Train: 2018-08-06T00:51:11.906607: step 3847, loss 0.488645.
Train: 2018-08-06T00:51:12.152952: step 3848, loss 0.644665.
Train: 2018-08-06T00:51:12.398306: step 3849, loss 0.505197.
Train: 2018-08-06T00:51:12.642645: step 3850, loss 0.578956.
Test: 2018-08-06T00:51:13.897257: step 3850, loss 0.548343.
Train: 2018-08-06T00:51:14.150605: step 3851, loss 0.554394.
Train: 2018-08-06T00:51:14.390982: step 3852, loss 0.578947.
Train: 2018-08-06T00:51:14.635283: step 3853, loss 0.627986.
Train: 2018-08-06T00:51:14.885645: step 3854, loss 0.603393.
Train: 2018-08-06T00:51:15.133980: step 3855, loss 0.61956.
Train: 2018-08-06T00:51:15.379294: step 3856, loss 0.578895.
Train: 2018-08-06T00:51:15.633638: step 3857, loss 0.546629.
Train: 2018-08-06T00:51:15.879979: step 3858, loss 0.586909.
Train: 2018-08-06T00:51:16.120336: step 3859, loss 0.594883.
Train: 2018-08-06T00:51:16.366680: step 3860, loss 0.586839.
Test: 2018-08-06T00:51:17.647228: step 3860, loss 0.550971.
Train: 2018-08-06T00:51:17.881600: step 3861, loss 0.53912.
Train: 2018-08-06T00:51:18.127942: step 3862, loss 0.459979.
Train: 2018-08-06T00:51:18.378299: step 3863, loss 0.634348.
Train: 2018-08-06T00:51:18.627632: step 3864, loss 0.499704.
Train: 2018-08-06T00:51:18.874944: step 3865, loss 0.586779.
Train: 2018-08-06T00:51:19.123312: step 3866, loss 0.547185.
Train: 2018-08-06T00:51:19.377631: step 3867, loss 0.547168.
Train: 2018-08-06T00:51:19.631954: step 3868, loss 0.610582.
Train: 2018-08-06T00:51:19.879258: step 3869, loss 0.570929.
Train: 2018-08-06T00:51:20.137596: step 3870, loss 0.507503.
Test: 2018-08-06T00:51:21.390218: step 3870, loss 0.550269.
Train: 2018-08-06T00:51:21.624618: step 3871, loss 0.523283.
Train: 2018-08-06T00:51:21.874948: step 3872, loss 0.515193.
Train: 2018-08-06T00:51:22.119295: step 3873, loss 0.602818.
Train: 2018-08-06T00:51:22.365609: step 3874, loss 0.514838.
Train: 2018-08-06T00:51:22.610953: step 3875, loss 0.57084.
Train: 2018-08-06T00:51:22.867268: step 3876, loss 0.627188.
Train: 2018-08-06T00:51:23.111645: step 3877, loss 0.570819.
Train: 2018-08-06T00:51:23.373946: step 3878, loss 0.595015.
Train: 2018-08-06T00:51:23.623271: step 3879, loss 0.619215.
Train: 2018-08-06T00:51:23.864632: step 3880, loss 0.562766.
Test: 2018-08-06T00:51:25.156146: step 3880, loss 0.548972.
Train: 2018-08-06T00:51:25.393513: step 3881, loss 0.586922.
Train: 2018-08-06T00:51:25.645837: step 3882, loss 0.578871.
Train: 2018-08-06T00:51:25.890211: step 3883, loss 0.538762.
Train: 2018-08-06T00:51:26.142508: step 3884, loss 0.554823.
Train: 2018-08-06T00:51:26.396855: step 3885, loss 0.562844.
Train: 2018-08-06T00:51:26.643170: step 3886, loss 0.570857.
Train: 2018-08-06T00:51:26.895523: step 3887, loss 0.458794.
Train: 2018-08-06T00:51:27.148849: step 3888, loss 0.554799.
Train: 2018-08-06T00:51:27.395158: step 3889, loss 0.570832.
Train: 2018-08-06T00:51:27.656492: step 3890, loss 0.578879.
Test: 2018-08-06T00:51:28.919083: step 3890, loss 0.549083.
Train: 2018-08-06T00:51:29.171439: step 3891, loss 0.586952.
Train: 2018-08-06T00:51:29.413761: step 3892, loss 0.55466.
Train: 2018-08-06T00:51:29.657110: step 3893, loss 0.546557.
Train: 2018-08-06T00:51:29.902480: step 3894, loss 0.546521.
Train: 2018-08-06T00:51:30.160789: step 3895, loss 0.514052.
Train: 2018-08-06T00:51:30.408100: step 3896, loss 0.643928.
Train: 2018-08-06T00:51:30.652473: step 3897, loss 0.595175.
Train: 2018-08-06T00:51:30.896793: step 3898, loss 0.546401.
Train: 2018-08-06T00:51:31.140143: step 3899, loss 0.465119.
Train: 2018-08-06T00:51:31.384489: step 3900, loss 0.570776.
Test: 2018-08-06T00:51:32.635145: step 3900, loss 0.548974.
Train: 2018-08-06T00:51:33.586503: step 3901, loss 0.570771.
Train: 2018-08-06T00:51:33.830851: step 3902, loss 0.513534.
Train: 2018-08-06T00:51:34.079187: step 3903, loss 0.60355.
Train: 2018-08-06T00:51:34.335500: step 3904, loss 0.578968.
Train: 2018-08-06T00:51:34.593810: step 3905, loss 0.578973.
Train: 2018-08-06T00:51:34.838187: step 3906, loss 0.578975.
Train: 2018-08-06T00:51:35.084524: step 3907, loss 0.603616.
Train: 2018-08-06T00:51:35.333857: step 3908, loss 0.546147.
Train: 2018-08-06T00:51:35.578205: step 3909, loss 0.611756.
Train: 2018-08-06T00:51:35.820529: step 3910, loss 0.578949.
Test: 2018-08-06T00:51:37.101104: step 3910, loss 0.549338.
Train: 2018-08-06T00:51:37.335480: step 3911, loss 0.521765.
Train: 2018-08-06T00:51:37.580857: step 3912, loss 0.521811.
Train: 2018-08-06T00:51:37.825167: step 3913, loss 0.546287.
Train: 2018-08-06T00:51:38.081521: step 3914, loss 0.521775.
Train: 2018-08-06T00:51:38.328822: step 3915, loss 0.488992.
Train: 2018-08-06T00:51:38.575163: step 3916, loss 0.570762.
Train: 2018-08-06T00:51:38.818538: step 3917, loss 0.628314.
Train: 2018-08-06T00:51:39.073855: step 3918, loss 0.529621.
Train: 2018-08-06T00:51:39.318176: step 3919, loss 0.60371.
Train: 2018-08-06T00:51:39.560564: step 3920, loss 0.644912.
Test: 2018-08-06T00:51:40.828137: step 3920, loss 0.549366.
Train: 2018-08-06T00:51:41.063541: step 3921, loss 0.620105.
Train: 2018-08-06T00:51:41.321842: step 3922, loss 0.554362.
Train: 2018-08-06T00:51:41.564200: step 3923, loss 0.497157.
Train: 2018-08-06T00:51:41.812505: step 3924, loss 0.50539.
Train: 2018-08-06T00:51:42.059892: step 3925, loss 0.472638.
Train: 2018-08-06T00:51:42.255345: step 3926, loss 0.615037.
Train: 2018-08-06T00:51:42.508669: step 3927, loss 0.505095.
Train: 2018-08-06T00:51:42.762964: step 3928, loss 0.546077.
Train: 2018-08-06T00:51:43.022295: step 3929, loss 0.504782.
Train: 2018-08-06T00:51:43.269634: step 3930, loss 0.595582.
Test: 2018-08-06T00:51:44.519265: step 3930, loss 0.548423.
Train: 2018-08-06T00:51:44.755635: step 3931, loss 0.595639.
Train: 2018-08-06T00:51:45.002001: step 3932, loss 0.562456.
Train: 2018-08-06T00:51:45.259287: step 3933, loss 0.562449.
Train: 2018-08-06T00:51:45.502664: step 3934, loss 0.587397.
Train: 2018-08-06T00:51:45.746016: step 3935, loss 0.653939.
Train: 2018-08-06T00:51:45.996346: step 3936, loss 0.520958.
Train: 2018-08-06T00:51:46.246666: step 3937, loss 0.562467.
Train: 2018-08-06T00:51:46.489029: step 3938, loss 0.562475.
Train: 2018-08-06T00:51:46.731350: step 3939, loss 0.670041.
Train: 2018-08-06T00:51:46.977692: step 3940, loss 0.661453.
Test: 2018-08-06T00:51:48.258266: step 3940, loss 0.548111.
Train: 2018-08-06T00:51:48.493637: step 3941, loss 0.53796.
Train: 2018-08-06T00:51:48.738981: step 3942, loss 0.603429.
Train: 2018-08-06T00:51:48.991337: step 3943, loss 0.546411.
Train: 2018-08-06T00:51:49.241665: step 3944, loss 0.61126.
Train: 2018-08-06T00:51:49.487977: step 3945, loss 0.586931.
Train: 2018-08-06T00:51:49.743323: step 3946, loss 0.554823.
Train: 2018-08-06T00:51:49.989667: step 3947, loss 0.538951.
Train: 2018-08-06T00:51:50.241989: step 3948, loss 0.594775.
Train: 2018-08-06T00:51:50.483350: step 3949, loss 0.562996.
Train: 2018-08-06T00:51:50.724701: step 3950, loss 0.539318.
Test: 2018-08-06T00:51:52.011229: step 3950, loss 0.551164.
Train: 2018-08-06T00:51:52.247598: step 3951, loss 0.570969.
Train: 2018-08-06T00:51:52.494936: step 3952, loss 0.578865.
Train: 2018-08-06T00:51:52.739282: step 3953, loss 0.594602.
Train: 2018-08-06T00:51:52.999594: step 3954, loss 0.492528.
Train: 2018-08-06T00:51:53.254904: step 3955, loss 0.555323.
Train: 2018-08-06T00:51:53.496258: step 3956, loss 0.618137.
Train: 2018-08-06T00:51:53.741631: step 3957, loss 0.618108.
Train: 2018-08-06T00:51:53.994924: step 3958, loss 0.594543.
Train: 2018-08-06T00:51:54.241294: step 3959, loss 0.492934.
Train: 2018-08-06T00:51:54.487638: step 3960, loss 0.633579.
Test: 2018-08-06T00:51:55.753222: step 3960, loss 0.55134.
Train: 2018-08-06T00:51:55.991585: step 3961, loss 0.539888.
Train: 2018-08-06T00:51:56.235956: step 3962, loss 0.571098.
Train: 2018-08-06T00:51:56.480309: step 3963, loss 0.672398.
Train: 2018-08-06T00:51:56.727642: step 3964, loss 0.57114.
Train: 2018-08-06T00:51:56.985951: step 3965, loss 0.509191.
Train: 2018-08-06T00:51:57.228306: step 3966, loss 0.462793.
Train: 2018-08-06T00:51:57.472657: step 3967, loss 0.594432.
Train: 2018-08-06T00:51:57.724950: step 3968, loss 0.547825.
Train: 2018-08-06T00:51:57.969296: step 3969, loss 0.501041.
Train: 2018-08-06T00:51:58.213643: step 3970, loss 0.500748.
Test: 2018-08-06T00:51:59.489230: step 3970, loss 0.549328.
Train: 2018-08-06T00:51:59.726595: step 3971, loss 0.571019.
Train: 2018-08-06T00:51:59.985903: step 3972, loss 0.555199.
Train: 2018-08-06T00:52:00.233266: step 3973, loss 0.539244.
Train: 2018-08-06T00:52:00.476622: step 3974, loss 0.578859.
Train: 2018-08-06T00:52:00.723929: step 3975, loss 0.546897.
Train: 2018-08-06T00:52:00.977278: step 3976, loss 0.586892.
Train: 2018-08-06T00:52:01.220600: step 3977, loss 0.514489.
Train: 2018-08-06T00:52:01.462984: step 3978, loss 0.538485.
Train: 2018-08-06T00:52:01.712323: step 3979, loss 0.538332.
Train: 2018-08-06T00:52:01.955634: step 3980, loss 0.521875.
Test: 2018-08-06T00:52:03.234216: step 3980, loss 0.547012.
Train: 2018-08-06T00:52:03.470617: step 3981, loss 0.513437.
Train: 2018-08-06T00:52:03.718951: step 3982, loss 0.587226.
Train: 2018-08-06T00:52:03.965260: step 3983, loss 0.529408.
Train: 2018-08-06T00:52:04.209607: step 3984, loss 0.587373.
Train: 2018-08-06T00:52:04.466950: step 3985, loss 0.545759.
Train: 2018-08-06T00:52:04.713287: step 3986, loss 0.528959.
Train: 2018-08-06T00:52:04.957632: step 3987, loss 0.553998.
Train: 2018-08-06T00:52:05.199960: step 3988, loss 0.528696.
Train: 2018-08-06T00:52:05.459296: step 3989, loss 0.570812.
Train: 2018-08-06T00:52:05.710594: step 3990, loss 0.579299.
Test: 2018-08-06T00:52:06.988176: step 3990, loss 0.546217.
Train: 2018-08-06T00:52:07.223572: step 3991, loss 0.596305.
Train: 2018-08-06T00:52:07.480859: step 3992, loss 0.553851.
Train: 2018-08-06T00:52:07.727200: step 3993, loss 0.53684.
Train: 2018-08-06T00:52:07.971546: step 3994, loss 0.536813.
Train: 2018-08-06T00:52:08.216921: step 3995, loss 0.570861.
Train: 2018-08-06T00:52:08.472208: step 3996, loss 0.613496.
Train: 2018-08-06T00:52:08.722539: step 3997, loss 0.613461.
Train: 2018-08-06T00:52:08.976858: step 3998, loss 0.494316.
Train: 2018-08-06T00:52:09.219247: step 3999, loss 0.570845.
Train: 2018-08-06T00:52:09.466575: step 4000, loss 0.562348.
Test: 2018-08-06T00:52:10.740142: step 4000, loss 0.547986.
Train: 2018-08-06T00:52:11.685338: step 4001, loss 0.511446.
Train: 2018-08-06T00:52:11.929690: step 4002, loss 0.485991.
Train: 2018-08-06T00:52:12.176026: step 4003, loss 0.468882.
Train: 2018-08-06T00:52:12.422341: step 4004, loss 0.579387.
Train: 2018-08-06T00:52:12.670677: step 4005, loss 0.476916.
Train: 2018-08-06T00:52:12.915024: step 4006, loss 0.510899.
Train: 2018-08-06T00:52:13.168348: step 4007, loss 0.57955.
Train: 2018-08-06T00:52:13.410730: step 4008, loss 0.553706.
Train: 2018-08-06T00:52:13.664049: step 4009, loss 0.648896.
Train: 2018-08-06T00:52:13.910387: step 4010, loss 0.588316.
Test: 2018-08-06T00:52:15.167001: step 4010, loss 0.548291.
Train: 2018-08-06T00:52:15.404368: step 4011, loss 0.536386.
Train: 2018-08-06T00:52:15.652729: step 4012, loss 0.527744.
Train: 2018-08-06T00:52:15.907053: step 4013, loss 0.527739.
Train: 2018-08-06T00:52:16.152397: step 4014, loss 0.579654.
Train: 2018-08-06T00:52:16.397743: step 4015, loss 0.579651.
Train: 2018-08-06T00:52:16.646047: step 4016, loss 0.536399.
Train: 2018-08-06T00:52:16.894383: step 4017, loss 0.53641.
Train: 2018-08-06T00:52:17.141722: step 4018, loss 0.570982.
Train: 2018-08-06T00:52:17.387066: step 4019, loss 0.519156.
Train: 2018-08-06T00:52:17.633431: step 4020, loss 0.570977.
Test: 2018-08-06T00:52:18.916973: step 4020, loss 0.54792.
Train: 2018-08-06T00:52:19.166306: step 4021, loss 0.579606.
Train: 2018-08-06T00:52:19.418655: step 4022, loss 0.588213.
Train: 2018-08-06T00:52:19.663975: step 4023, loss 0.519285.
Train: 2018-08-06T00:52:19.917329: step 4024, loss 0.588142.
Train: 2018-08-06T00:52:20.171643: step 4025, loss 0.527983.
Train: 2018-08-06T00:52:20.412972: step 4026, loss 0.613812.
Train: 2018-08-06T00:52:20.667324: step 4027, loss 0.536655.
Train: 2018-08-06T00:52:20.926629: step 4028, loss 0.536701.
Train: 2018-08-06T00:52:21.173937: step 4029, loss 0.639151.
Train: 2018-08-06T00:52:21.416321: step 4030, loss 0.587871.
Test: 2018-08-06T00:52:22.689884: step 4030, loss 0.54858.
Train: 2018-08-06T00:52:22.928246: step 4031, loss 0.562351.
Train: 2018-08-06T00:52:23.175584: step 4032, loss 0.528554.
Train: 2018-08-06T00:52:23.418934: step 4033, loss 0.511783.
Train: 2018-08-06T00:52:23.662309: step 4034, loss 0.562376.
Train: 2018-08-06T00:52:23.911651: step 4035, loss 0.553972.
Train: 2018-08-06T00:52:24.159952: step 4036, loss 0.570786.
Train: 2018-08-06T00:52:24.406318: step 4037, loss 0.528839.
Train: 2018-08-06T00:52:24.649641: step 4038, loss 0.57078.
Train: 2018-08-06T00:52:24.895984: step 4039, loss 0.612655.
Train: 2018-08-06T00:52:25.142324: step 4040, loss 0.570771.
Test: 2018-08-06T00:52:26.411929: step 4040, loss 0.547921.
Train: 2018-08-06T00:52:26.649295: step 4041, loss 0.554086.
Train: 2018-08-06T00:52:26.894639: step 4042, loss 0.554113.
Train: 2018-08-06T00:52:27.138012: step 4043, loss 0.612316.
Train: 2018-08-06T00:52:27.384328: step 4044, loss 0.521024.
Train: 2018-08-06T00:52:27.626680: step 4045, loss 0.537653.
Train: 2018-08-06T00:52:27.885988: step 4046, loss 0.620366.
Train: 2018-08-06T00:52:28.130365: step 4047, loss 0.570756.
Train: 2018-08-06T00:52:28.375676: step 4048, loss 0.578989.
Train: 2018-08-06T00:52:28.622043: step 4049, loss 0.529704.
Train: 2018-08-06T00:52:28.875367: step 4050, loss 0.636353.
Test: 2018-08-06T00:52:30.124001: step 4050, loss 0.548944.
Train: 2018-08-06T00:52:30.359393: step 4051, loss 0.521726.
Train: 2018-08-06T00:52:30.604715: step 4052, loss 0.513662.
Train: 2018-08-06T00:52:30.853051: step 4053, loss 0.554465.
Train: 2018-08-06T00:52:31.096425: step 4054, loss 0.627839.
Train: 2018-08-06T00:52:31.348737: step 4055, loss 0.554505.
Train: 2018-08-06T00:52:31.594070: step 4056, loss 0.570785.
Train: 2018-08-06T00:52:31.850386: step 4057, loss 0.554561.
Train: 2018-08-06T00:52:32.094760: step 4058, loss 0.530265.
Train: 2018-08-06T00:52:32.341103: step 4059, loss 0.651839.
Train: 2018-08-06T00:52:32.591429: step 4060, loss 0.530372.
Test: 2018-08-06T00:52:33.856020: step 4060, loss 0.548501.
Train: 2018-08-06T00:52:34.095382: step 4061, loss 0.554656.
Train: 2018-08-06T00:52:34.357680: step 4062, loss 0.643442.
Train: 2018-08-06T00:52:34.614991: step 4063, loss 0.603021.
Train: 2018-08-06T00:52:34.861333: step 4064, loss 0.546779.
Train: 2018-08-06T00:52:35.107698: step 4065, loss 0.570861.
Train: 2018-08-06T00:52:35.352051: step 4066, loss 0.594828.
Train: 2018-08-06T00:52:35.600357: step 4067, loss 0.547013.
Train: 2018-08-06T00:52:35.845726: step 4068, loss 0.65037.
Train: 2018-08-06T00:52:36.092041: step 4069, loss 0.57886.
Train: 2018-08-06T00:52:36.343400: step 4070, loss 0.61041.
Test: 2018-08-06T00:52:37.616967: step 4070, loss 0.549692.
Train: 2018-08-06T00:52:37.856324: step 4071, loss 0.578872.
Train: 2018-08-06T00:52:38.113665: step 4072, loss 0.461583.
Train: 2018-08-06T00:52:38.361983: step 4073, loss 0.532008.
Train: 2018-08-06T00:52:38.643460: step 4074, loss 0.563256.
Train: 2018-08-06T00:52:38.888793: step 4075, loss 0.52416.
Train: 2018-08-06T00:52:39.130133: step 4076, loss 0.477087.
Train: 2018-08-06T00:52:39.329625: step 4077, loss 0.663758.
Train: 2018-08-06T00:52:39.588932: step 4078, loss 0.563131.
Train: 2018-08-06T00:52:39.834282: step 4079, loss 0.578866.
Train: 2018-08-06T00:52:40.082617: step 4080, loss 0.555212.
Test: 2018-08-06T00:52:41.375129: step 4080, loss 0.551348.
Train: 2018-08-06T00:52:41.612526: step 4081, loss 0.555183.
Train: 2018-08-06T00:52:41.857840: step 4082, loss 0.586765.
Train: 2018-08-06T00:52:42.109198: step 4083, loss 0.539308.
Train: 2018-08-06T00:52:42.351545: step 4084, loss 0.555093.
Train: 2018-08-06T00:52:42.597891: step 4085, loss 0.618536.
Train: 2018-08-06T00:52:42.844202: step 4086, loss 0.562983.
Train: 2018-08-06T00:52:43.093560: step 4087, loss 0.523273.
Train: 2018-08-06T00:52:43.338903: step 4088, loss 0.539096.
Train: 2018-08-06T00:52:43.581255: step 4089, loss 0.539016.
Train: 2018-08-06T00:52:43.840538: step 4090, loss 0.498973.
Test: 2018-08-06T00:52:45.102162: step 4090, loss 0.548141.
Train: 2018-08-06T00:52:45.350498: step 4091, loss 0.490645.
Train: 2018-08-06T00:52:45.605817: step 4092, loss 0.546629.
Train: 2018-08-06T00:52:45.849166: step 4093, loss 0.603214.
Train: 2018-08-06T00:52:46.094534: step 4094, loss 0.55451.
Train: 2018-08-06T00:52:46.341873: step 4095, loss 0.554442.
Train: 2018-08-06T00:52:46.582235: step 4096, loss 0.611724.
Train: 2018-08-06T00:52:46.826582: step 4097, loss 0.554346.
Train: 2018-08-06T00:52:47.067912: step 4098, loss 0.652983.
Train: 2018-08-06T00:52:47.315245: step 4099, loss 0.578978.
Train: 2018-08-06T00:52:47.564578: step 4100, loss 0.620031.
Test: 2018-08-06T00:52:48.836177: step 4100, loss 0.548505.
Train: 2018-08-06T00:52:49.734875: step 4101, loss 0.488816.
Train: 2018-08-06T00:52:49.978220: step 4102, loss 0.554378.
Train: 2018-08-06T00:52:50.221542: step 4103, loss 0.603534.
Train: 2018-08-06T00:52:50.482844: step 4104, loss 0.570765.
Train: 2018-08-06T00:52:50.725197: step 4105, loss 0.538066.
Train: 2018-08-06T00:52:50.971537: step 4106, loss 0.546254.
Train: 2018-08-06T00:52:51.231872: step 4107, loss 0.529912.
Train: 2018-08-06T00:52:51.492144: step 4108, loss 0.529881.
Train: 2018-08-06T00:52:51.737515: step 4109, loss 0.570764.
Train: 2018-08-06T00:52:51.983874: step 4110, loss 0.546171.
Test: 2018-08-06T00:52:53.247450: step 4110, loss 0.548657.
Train: 2018-08-06T00:52:53.535705: step 4111, loss 0.529723.
Train: 2018-08-06T00:52:53.780027: step 4112, loss 0.562535.
Train: 2018-08-06T00:52:54.037372: step 4113, loss 0.488393.
Train: 2018-08-06T00:52:54.294650: step 4114, loss 0.612066.
Train: 2018-08-06T00:52:54.545977: step 4115, loss 0.562481.
Train: 2018-08-06T00:52:54.805284: step 4116, loss 0.570757.
Train: 2018-08-06T00:52:55.046639: step 4117, loss 0.570758.
Train: 2018-08-06T00:52:55.293977: step 4118, loss 0.562459.
Train: 2018-08-06T00:52:55.538355: step 4119, loss 0.612274.
Train: 2018-08-06T00:52:55.779680: step 4120, loss 0.512685.
Test: 2018-08-06T00:52:57.058260: step 4120, loss 0.5486.
Train: 2018-08-06T00:52:57.295655: step 4121, loss 0.587356.
Train: 2018-08-06T00:52:57.543961: step 4122, loss 0.51269.
Train: 2018-08-06T00:52:57.798311: step 4123, loss 0.587361.
Train: 2018-08-06T00:52:58.046616: step 4124, loss 0.545856.
Train: 2018-08-06T00:52:58.301964: step 4125, loss 0.529244.
Train: 2018-08-06T00:52:58.563235: step 4126, loss 0.604002.
Train: 2018-08-06T00:52:58.813566: step 4127, loss 0.57076.
Train: 2018-08-06T00:52:59.058910: step 4128, loss 0.579064.
Train: 2018-08-06T00:52:59.306303: step 4129, loss 0.529274.
Train: 2018-08-06T00:52:59.565584: step 4130, loss 0.595644.
Test: 2018-08-06T00:53:00.827180: step 4130, loss 0.548439.
Train: 2018-08-06T00:53:01.062576: step 4131, loss 0.587332.
Train: 2018-08-06T00:53:01.306943: step 4132, loss 0.537658.
Train: 2018-08-06T00:53:01.566237: step 4133, loss 0.504618.
Train: 2018-08-06T00:53:01.807584: step 4134, loss 0.504588.
Train: 2018-08-06T00:53:02.054927: step 4135, loss 0.70331.
Train: 2018-08-06T00:53:02.299244: step 4136, loss 0.521138.
Train: 2018-08-06T00:53:02.553562: step 4137, loss 0.521175.
Train: 2018-08-06T00:53:02.813866: step 4138, loss 0.488104.
Train: 2018-08-06T00:53:03.073173: step 4139, loss 0.620437.
Train: 2018-08-06T00:53:03.318543: step 4140, loss 0.504503.
Test: 2018-08-06T00:53:04.588135: step 4140, loss 0.548232.
Train: 2018-08-06T00:53:04.836505: step 4141, loss 0.562464.
Train: 2018-08-06T00:53:05.082813: step 4142, loss 0.537545.
Train: 2018-08-06T00:53:05.340156: step 4143, loss 0.587394.
Train: 2018-08-06T00:53:05.587497: step 4144, loss 0.570762.
Train: 2018-08-06T00:53:05.846795: step 4145, loss 0.604064.
Train: 2018-08-06T00:53:06.095137: step 4146, loss 0.587401.
Train: 2018-08-06T00:53:06.340477: step 4147, loss 0.512599.
Train: 2018-08-06T00:53:06.586791: step 4148, loss 0.562452.
Train: 2018-08-06T00:53:06.829142: step 4149, loss 0.545842.
Train: 2018-08-06T00:53:07.077478: step 4150, loss 0.512615.
Test: 2018-08-06T00:53:08.347084: step 4150, loss 0.546835.
Train: 2018-08-06T00:53:08.582454: step 4151, loss 0.545815.
Train: 2018-08-06T00:53:08.825829: step 4152, loss 0.579089.
Train: 2018-08-06T00:53:09.071178: step 4153, loss 0.579095.
Train: 2018-08-06T00:53:09.321508: step 4154, loss 0.612425.
Train: 2018-08-06T00:53:09.575797: step 4155, loss 0.545794.
Train: 2018-08-06T00:53:09.822169: step 4156, loss 0.537495.
Train: 2018-08-06T00:53:10.071472: step 4157, loss 0.537502.
Train: 2018-08-06T00:53:10.328808: step 4158, loss 0.604029.
Train: 2018-08-06T00:53:10.581108: step 4159, loss 0.57907.
Train: 2018-08-06T00:53:10.831439: step 4160, loss 0.545857.
Test: 2018-08-06T00:53:12.086083: step 4160, loss 0.54823.
Train: 2018-08-06T00:53:12.322452: step 4161, loss 0.645404.
Train: 2018-08-06T00:53:12.581785: step 4162, loss 0.562484.
Train: 2018-08-06T00:53:12.831124: step 4163, loss 0.554253.
Train: 2018-08-06T00:53:13.080451: step 4164, loss 0.554289.
Train: 2018-08-06T00:53:13.333772: step 4165, loss 0.652956.
Train: 2018-08-06T00:53:13.578094: step 4166, loss 0.521628.
Train: 2018-08-06T00:53:13.832414: step 4167, loss 0.5626.
Train: 2018-08-06T00:53:14.076786: step 4168, loss 0.513713.
Train: 2018-08-06T00:53:14.333074: step 4169, loss 0.587067.
Train: 2018-08-06T00:53:14.580413: step 4170, loss 0.635855.
Test: 2018-08-06T00:53:15.836055: step 4170, loss 0.549882.
Train: 2018-08-06T00:53:16.076438: step 4171, loss 0.635678.
Train: 2018-08-06T00:53:16.319761: step 4172, loss 0.578886.
Train: 2018-08-06T00:53:16.566103: step 4173, loss 0.570831.
Train: 2018-08-06T00:53:16.811447: step 4174, loss 0.586876.
Train: 2018-08-06T00:53:17.066763: step 4175, loss 0.538969.
Train: 2018-08-06T00:53:17.312138: step 4176, loss 0.539085.
Train: 2018-08-06T00:53:17.555487: step 4177, loss 0.56298.
Train: 2018-08-06T00:53:17.802826: step 4178, loss 0.610564.
Train: 2018-08-06T00:53:18.050165: step 4179, loss 0.570954.
Train: 2018-08-06T00:53:18.308443: step 4180, loss 0.515754.
Test: 2018-08-06T00:53:19.572065: step 4180, loss 0.550296.
Train: 2018-08-06T00:53:19.809429: step 4181, loss 0.594631.
Train: 2018-08-06T00:53:20.067764: step 4182, loss 0.602487.
Train: 2018-08-06T00:53:20.328042: step 4183, loss 0.571012.
Train: 2018-08-06T00:53:20.573411: step 4184, loss 0.523962.
Train: 2018-08-06T00:53:20.820724: step 4185, loss 0.649452.
Train: 2018-08-06T00:53:21.072051: step 4186, loss 0.59453.
Train: 2018-08-06T00:53:21.321388: step 4187, loss 0.578891.
Train: 2018-08-06T00:53:21.579726: step 4188, loss 0.501067.
Train: 2018-08-06T00:53:21.841021: step 4189, loss 0.555563.
Train: 2018-08-06T00:53:22.091331: step 4190, loss 0.578902.
Test: 2018-08-06T00:53:23.380878: step 4190, loss 0.550174.
Train: 2018-08-06T00:53:23.617245: step 4191, loss 0.547786.
Train: 2018-08-06T00:53:23.868573: step 4192, loss 0.5789.
Train: 2018-08-06T00:53:24.121920: step 4193, loss 0.516612.
Train: 2018-08-06T00:53:24.365245: step 4194, loss 0.508698.
Train: 2018-08-06T00:53:24.609622: step 4195, loss 0.618003.
Train: 2018-08-06T00:53:24.859922: step 4196, loss 0.555365.
Train: 2018-08-06T00:53:25.107292: step 4197, loss 0.555314.
Train: 2018-08-06T00:53:25.366566: step 4198, loss 0.563129.
Train: 2018-08-06T00:53:25.607949: step 4199, loss 0.578864.
Train: 2018-08-06T00:53:25.853265: step 4200, loss 0.626255.
Test: 2018-08-06T00:53:27.132844: step 4200, loss 0.549323.
Train: 2018-08-06T00:53:28.067630: step 4201, loss 0.523564.
Train: 2018-08-06T00:53:28.311977: step 4202, loss 0.626317.
Train: 2018-08-06T00:53:28.564336: step 4203, loss 0.523511.
Train: 2018-08-06T00:53:28.826633: step 4204, loss 0.570946.
Train: 2018-08-06T00:53:29.074967: step 4205, loss 0.5551.
Train: 2018-08-06T00:53:29.328259: step 4206, loss 0.586787.
Train: 2018-08-06T00:53:29.576595: step 4207, loss 0.562995.
Train: 2018-08-06T00:53:29.820941: step 4208, loss 0.53124.
Train: 2018-08-06T00:53:30.077289: step 4209, loss 0.523223.
Train: 2018-08-06T00:53:30.323598: step 4210, loss 0.594793.
Test: 2018-08-06T00:53:31.587218: step 4210, loss 0.550128.
Train: 2018-08-06T00:53:31.824616: step 4211, loss 0.546945.
Train: 2018-08-06T00:53:32.080928: step 4212, loss 0.5309.
Train: 2018-08-06T00:53:32.326272: step 4213, loss 0.578866.
Train: 2018-08-06T00:53:32.579564: step 4214, loss 0.54675.
Train: 2018-08-06T00:53:32.841861: step 4215, loss 0.546683.
Train: 2018-08-06T00:53:33.095209: step 4216, loss 0.578883.
Train: 2018-08-06T00:53:33.351498: step 4217, loss 0.489974.
Train: 2018-08-06T00:53:33.599866: step 4218, loss 0.570791.
Train: 2018-08-06T00:53:33.845212: step 4219, loss 0.578916.
Train: 2018-08-06T00:53:34.092518: step 4220, loss 0.595235.
Test: 2018-08-06T00:53:35.350154: step 4220, loss 0.549723.
Train: 2018-08-06T00:53:35.586553: step 4221, loss 0.538114.
Train: 2018-08-06T00:53:35.826880: step 4222, loss 0.497163.
Train: 2018-08-06T00:53:36.082225: step 4223, loss 0.554355.
Train: 2018-08-06T00:53:36.331530: step 4224, loss 0.53785.
Train: 2018-08-06T00:53:36.578892: step 4225, loss 0.479978.
Train: 2018-08-06T00:53:36.822242: step 4226, loss 0.479561.
Train: 2018-08-06T00:53:37.072579: step 4227, loss 0.637483.
Train: 2018-08-06T00:53:37.266057: step 4228, loss 0.651672.
Train: 2018-08-06T00:53:37.511405: step 4229, loss 0.570778.
Train: 2018-08-06T00:53:37.768720: step 4230, loss 0.562397.
Test: 2018-08-06T00:53:39.024328: step 4230, loss 0.549548.
Train: 2018-08-06T00:53:39.273686: step 4231, loss 0.562395.
Train: 2018-08-06T00:53:39.523992: step 4232, loss 0.61272.
Train: 2018-08-06T00:53:39.769362: step 4233, loss 0.570778.
Train: 2018-08-06T00:53:40.012718: step 4234, loss 0.570774.
Train: 2018-08-06T00:53:40.255037: step 4235, loss 0.50392.
Train: 2018-08-06T00:53:40.505401: step 4236, loss 0.545705.
Train: 2018-08-06T00:53:40.748748: step 4237, loss 0.537346.
Train: 2018-08-06T00:53:40.994094: step 4238, loss 0.629297.
Train: 2018-08-06T00:53:41.237434: step 4239, loss 0.562418.
Train: 2018-08-06T00:53:41.480758: step 4240, loss 0.512373.
Test: 2018-08-06T00:53:42.733408: step 4240, loss 0.550221.
Train: 2018-08-06T00:53:42.969776: step 4241, loss 0.537399.
Train: 2018-08-06T00:53:43.227089: step 4242, loss 0.595803.
Train: 2018-08-06T00:53:43.474427: step 4243, loss 0.554084.
Train: 2018-08-06T00:53:43.719770: step 4244, loss 0.554089.
Train: 2018-08-06T00:53:43.965114: step 4245, loss 0.579101.
Train: 2018-08-06T00:53:44.224452: step 4246, loss 0.587425.
Train: 2018-08-06T00:53:44.469765: step 4247, loss 0.537484.
Train: 2018-08-06T00:53:44.726080: step 4248, loss 0.628955.
Train: 2018-08-06T00:53:44.970455: step 4249, loss 0.520989.
Train: 2018-08-06T00:53:45.213801: step 4250, loss 0.587327.
Test: 2018-08-06T00:53:46.476398: step 4250, loss 0.549243.
Train: 2018-08-06T00:53:46.711769: step 4251, loss 0.587299.
Train: 2018-08-06T00:53:46.971111: step 4252, loss 0.612022.
Train: 2018-08-06T00:53:47.222434: step 4253, loss 0.50494.
Train: 2018-08-06T00:53:47.476755: step 4254, loss 0.529688.
Train: 2018-08-06T00:53:47.737060: step 4255, loss 0.521508.
Train: 2018-08-06T00:53:47.982397: step 4256, loss 0.620027.
Train: 2018-08-06T00:53:48.235725: step 4257, loss 0.546155.
Train: 2018-08-06T00:53:48.480066: step 4258, loss 0.521582.
Train: 2018-08-06T00:53:48.725417: step 4259, loss 0.505166.
Train: 2018-08-06T00:53:48.972747: step 4260, loss 0.537909.
Test: 2018-08-06T00:53:50.237340: step 4260, loss 0.549544.
Train: 2018-08-06T00:53:50.476732: step 4261, loss 0.595444.
Train: 2018-08-06T00:53:50.722045: step 4262, loss 0.554284.
Train: 2018-08-06T00:53:50.965427: step 4263, loss 0.570757.
Train: 2018-08-06T00:53:51.209768: step 4264, loss 0.521253.
Train: 2018-08-06T00:53:51.460100: step 4265, loss 0.570756.
Train: 2018-08-06T00:53:51.710425: step 4266, loss 0.570756.
Train: 2018-08-06T00:53:51.959768: step 4267, loss 0.537648.
Train: 2018-08-06T00:53:52.205103: step 4268, loss 0.570757.
Train: 2018-08-06T00:53:52.450422: step 4269, loss 0.595636.
Train: 2018-08-06T00:53:52.702756: step 4270, loss 0.579049.
Test: 2018-08-06T00:53:53.959386: step 4270, loss 0.54787.
Train: 2018-08-06T00:53:54.199773: step 4271, loss 0.579043.
Train: 2018-08-06T00:53:54.445088: step 4272, loss 0.512813.
Train: 2018-08-06T00:53:54.688466: step 4273, loss 0.54592.
Train: 2018-08-06T00:53:54.933780: step 4274, loss 0.554192.
Train: 2018-08-06T00:53:55.179124: step 4275, loss 0.479608.
Train: 2018-08-06T00:53:55.425492: step 4276, loss 0.579064.
Train: 2018-08-06T00:53:55.667849: step 4277, loss 0.520856.
Train: 2018-08-06T00:53:55.914158: step 4278, loss 0.554093.
Train: 2018-08-06T00:53:56.162525: step 4279, loss 0.57077.
Train: 2018-08-06T00:53:56.402851: step 4280, loss 0.587506.
Test: 2018-08-06T00:53:57.678441: step 4280, loss 0.547458.
Train: 2018-08-06T00:53:57.916803: step 4281, loss 0.562404.
Train: 2018-08-06T00:53:58.162180: step 4282, loss 0.579153.
Train: 2018-08-06T00:53:58.413475: step 4283, loss 0.520523.
Train: 2018-08-06T00:53:58.656823: step 4284, loss 0.562397.
Train: 2018-08-06T00:53:58.904196: step 4285, loss 0.545622.
Train: 2018-08-06T00:53:59.151534: step 4286, loss 0.579175.
Train: 2018-08-06T00:53:59.398873: step 4287, loss 0.58757.
Train: 2018-08-06T00:53:59.646203: step 4288, loss 0.595946.
Train: 2018-08-06T00:53:59.892520: step 4289, loss 0.503769.
Train: 2018-08-06T00:54:00.141854: step 4290, loss 0.495401.
Test: 2018-08-06T00:54:01.404501: step 4290, loss 0.548777.
Train: 2018-08-06T00:54:01.646829: step 4291, loss 0.595937.
Train: 2018-08-06T00:54:01.897158: step 4292, loss 0.520457.
Train: 2018-08-06T00:54:02.140538: step 4293, loss 0.478431.
Train: 2018-08-06T00:54:02.400845: step 4294, loss 0.553959.
Train: 2018-08-06T00:54:02.646154: step 4295, loss 0.629867.
Train: 2018-08-06T00:54:02.890503: step 4296, loss 0.537041.
Train: 2018-08-06T00:54:03.136868: step 4297, loss 0.570811.
Train: 2018-08-06T00:54:03.382187: step 4298, loss 0.494752.
Train: 2018-08-06T00:54:03.628528: step 4299, loss 0.553891.
Train: 2018-08-06T00:54:03.880884: step 4300, loss 0.503012.
Test: 2018-08-06T00:54:05.140484: step 4300, loss 0.547177.
Train: 2018-08-06T00:54:06.057881: step 4301, loss 0.579341.
Train: 2018-08-06T00:54:06.301257: step 4302, loss 0.5283.
Train: 2018-08-06T00:54:06.547595: step 4303, loss 0.56234.
Train: 2018-08-06T00:54:06.792940: step 4304, loss 0.587958.
Train: 2018-08-06T00:54:07.042248: step 4305, loss 0.587973.
Train: 2018-08-06T00:54:07.289624: step 4306, loss 0.605052.
Train: 2018-08-06T00:54:07.538949: step 4307, loss 0.519683.
Train: 2018-08-06T00:54:07.785261: step 4308, loss 0.570866.
Train: 2018-08-06T00:54:08.031601: step 4309, loss 0.468636.
Train: 2018-08-06T00:54:08.287944: step 4310, loss 0.587921.
Test: 2018-08-06T00:54:09.560512: step 4310, loss 0.54849.
Train: 2018-08-06T00:54:09.797878: step 4311, loss 0.545284.
Train: 2018-08-06T00:54:10.044219: step 4312, loss 0.622049.
Train: 2018-08-06T00:54:10.289594: step 4313, loss 0.587898.
Train: 2018-08-06T00:54:10.535935: step 4314, loss 0.57935.
Train: 2018-08-06T00:54:10.780250: step 4315, loss 0.596281.
Train: 2018-08-06T00:54:11.024625: step 4316, loss 0.621559.
Train: 2018-08-06T00:54:11.268944: step 4317, loss 0.520267.
Train: 2018-08-06T00:54:11.530245: step 4318, loss 0.553993.
Train: 2018-08-06T00:54:11.782599: step 4319, loss 0.604269.
Train: 2018-08-06T00:54:12.026943: step 4320, loss 0.504016.
Test: 2018-08-06T00:54:13.291534: step 4320, loss 0.548337.
Train: 2018-08-06T00:54:13.528926: step 4321, loss 0.562436.
Train: 2018-08-06T00:54:13.774277: step 4322, loss 0.545825.
Train: 2018-08-06T00:54:14.020611: step 4323, loss 0.49606.
Train: 2018-08-06T00:54:14.268956: step 4324, loss 0.562458.
Train: 2018-08-06T00:54:14.515292: step 4325, loss 0.587361.
Train: 2018-08-06T00:54:14.766590: step 4326, loss 0.637124.
Train: 2018-08-06T00:54:15.013962: step 4327, loss 0.554204.
Train: 2018-08-06T00:54:15.261268: step 4328, loss 0.529458.
Train: 2018-08-06T00:54:15.508605: step 4329, loss 0.554256.
Train: 2018-08-06T00:54:15.755945: step 4330, loss 0.521298.
Test: 2018-08-06T00:54:17.027542: step 4330, loss 0.546468.
Train: 2018-08-06T00:54:17.267901: step 4331, loss 0.579.
Train: 2018-08-06T00:54:17.524241: step 4332, loss 0.554275.
Train: 2018-08-06T00:54:17.781552: step 4333, loss 0.578996.
Train: 2018-08-06T00:54:18.031856: step 4334, loss 0.603696.
Train: 2018-08-06T00:54:18.279222: step 4335, loss 0.62009.
Train: 2018-08-06T00:54:18.523542: step 4336, loss 0.578961.
Train: 2018-08-06T00:54:18.768886: step 4337, loss 0.52989.
Train: 2018-08-06T00:54:19.013264: step 4338, loss 0.554451.
Train: 2018-08-06T00:54:19.260571: step 4339, loss 0.562628.
Train: 2018-08-06T00:54:19.509905: step 4340, loss 0.57078.
Test: 2018-08-06T00:54:20.793472: step 4340, loss 0.550584.
Train: 2018-08-06T00:54:21.030837: step 4341, loss 0.538282.
Train: 2018-08-06T00:54:21.275184: step 4342, loss 0.562667.
Train: 2018-08-06T00:54:21.516566: step 4343, loss 0.587021.
Train: 2018-08-06T00:54:21.763876: step 4344, loss 0.587008.
Train: 2018-08-06T00:54:22.011243: step 4345, loss 0.562705.
Train: 2018-08-06T00:54:22.258554: step 4346, loss 0.562721.
Train: 2018-08-06T00:54:22.505923: step 4347, loss 0.546586.
Train: 2018-08-06T00:54:22.759241: step 4348, loss 0.619233.
Train: 2018-08-06T00:54:23.004593: step 4349, loss 0.482218.
Train: 2018-08-06T00:54:23.249908: step 4350, loss 0.554705.
Test: 2018-08-06T00:54:24.503549: step 4350, loss 0.548363.
Train: 2018-08-06T00:54:24.741938: step 4351, loss 0.562756.
Train: 2018-08-06T00:54:24.986289: step 4352, loss 0.546614.
Train: 2018-08-06T00:54:25.244594: step 4353, loss 0.603109.
Train: 2018-08-06T00:54:25.496924: step 4354, loss 0.562737.
Train: 2018-08-06T00:54:25.743265: step 4355, loss 0.635405.
Train: 2018-08-06T00:54:26.000547: step 4356, loss 0.546638.
Train: 2018-08-06T00:54:26.246888: step 4357, loss 0.538617.
Train: 2018-08-06T00:54:26.506224: step 4358, loss 0.562777.
Train: 2018-08-06T00:54:26.752537: step 4359, loss 0.522543.
Train: 2018-08-06T00:54:27.005858: step 4360, loss 0.635257.
Test: 2018-08-06T00:54:28.273467: step 4360, loss 0.549528.
Train: 2018-08-06T00:54:28.511832: step 4361, loss 0.546688.
Train: 2018-08-06T00:54:28.755206: step 4362, loss 0.562787.
Train: 2018-08-06T00:54:29.003550: step 4363, loss 0.594956.
Train: 2018-08-06T00:54:29.248859: step 4364, loss 0.530668.
Train: 2018-08-06T00:54:29.492223: step 4365, loss 0.611007.
Train: 2018-08-06T00:54:29.733562: step 4366, loss 0.554793.
Train: 2018-08-06T00:54:29.980901: step 4367, loss 0.594906.
Train: 2018-08-06T00:54:30.240238: step 4368, loss 0.642936.
Train: 2018-08-06T00:54:30.487547: step 4369, loss 0.530955.
Train: 2018-08-06T00:54:30.739900: step 4370, loss 0.523073.
Test: 2018-08-06T00:54:32.020446: step 4370, loss 0.550177.
Train: 2018-08-06T00:54:32.257812: step 4371, loss 0.570894.
Train: 2018-08-06T00:54:32.500163: step 4372, loss 0.62662.
Train: 2018-08-06T00:54:32.744511: step 4373, loss 0.578858.
Train: 2018-08-06T00:54:32.986862: step 4374, loss 0.515433.
Train: 2018-08-06T00:54:33.232231: step 4375, loss 0.52339.
Train: 2018-08-06T00:54:33.479545: step 4376, loss 0.666084.
Train: 2018-08-06T00:54:33.740872: step 4377, loss 0.547199.
Train: 2018-08-06T00:54:33.998177: step 4378, loss 0.499793.
Train: 2018-08-06T00:54:34.193661: step 4379, loss 0.630558.
Train: 2018-08-06T00:54:34.452977: step 4380, loss 0.539329.
Test: 2018-08-06T00:54:35.706588: step 4380, loss 0.549661.
Train: 2018-08-06T00:54:35.945974: step 4381, loss 0.570954.
Train: 2018-08-06T00:54:36.191294: step 4382, loss 0.507704.
Train: 2018-08-06T00:54:36.445639: step 4383, loss 0.570941.
Train: 2018-08-06T00:54:36.699933: step 4384, loss 0.539214.
Train: 2018-08-06T00:54:36.944278: step 4385, loss 0.570914.
Train: 2018-08-06T00:54:37.192647: step 4386, loss 0.523156.
Train: 2018-08-06T00:54:37.442946: step 4387, loss 0.586839.
Train: 2018-08-06T00:54:37.687291: step 4388, loss 0.538895.
Train: 2018-08-06T00:54:37.933659: step 4389, loss 0.62694.
Train: 2018-08-06T00:54:38.193963: step 4390, loss 0.554814.
Test: 2018-08-06T00:54:39.475509: step 4390, loss 0.549047.
Train: 2018-08-06T00:54:39.726837: step 4391, loss 0.586894.
Train: 2018-08-06T00:54:39.991183: step 4392, loss 0.586896.
Train: 2018-08-06T00:54:40.244536: step 4393, loss 0.522691.
Train: 2018-08-06T00:54:40.505806: step 4394, loss 0.554774.
Train: 2018-08-06T00:54:40.755139: step 4395, loss 0.619075.
Train: 2018-08-06T00:54:41.002512: step 4396, loss 0.570835.
Train: 2018-08-06T00:54:41.249850: step 4397, loss 0.619041.
Train: 2018-08-06T00:54:41.496188: step 4398, loss 0.522725.
Train: 2018-08-06T00:54:41.744527: step 4399, loss 0.562833.
Train: 2018-08-06T00:54:41.987874: step 4400, loss 0.49071.
Test: 2018-08-06T00:54:43.261436: step 4400, loss 0.549411.
Train: 2018-08-06T00:54:44.191949: step 4401, loss 0.522683.
Train: 2018-08-06T00:54:44.436295: step 4402, loss 0.570829.
Train: 2018-08-06T00:54:44.680673: step 4403, loss 0.578881.
Train: 2018-08-06T00:54:44.935984: step 4404, loss 0.562735.
Train: 2018-08-06T00:54:45.184293: step 4405, loss 0.603152.
Train: 2018-08-06T00:54:45.431631: step 4406, loss 0.554622.
Train: 2018-08-06T00:54:45.687946: step 4407, loss 0.578894.
Train: 2018-08-06T00:54:45.930329: step 4408, loss 0.522224.
Train: 2018-08-06T00:54:46.180628: step 4409, loss 0.554584.
Train: 2018-08-06T00:54:46.425006: step 4410, loss 0.595135.
Test: 2018-08-06T00:54:47.683608: step 4410, loss 0.548737.
Train: 2018-08-06T00:54:47.922997: step 4411, loss 0.635736.
Train: 2018-08-06T00:54:48.169310: step 4412, loss 0.61133.
Train: 2018-08-06T00:54:48.413656: step 4413, loss 0.506098.
Train: 2018-08-06T00:54:48.659000: step 4414, loss 0.570806.
Train: 2018-08-06T00:54:48.905368: step 4415, loss 0.554659.
Train: 2018-08-06T00:54:49.151715: step 4416, loss 0.530454.
Train: 2018-08-06T00:54:49.410020: step 4417, loss 0.60311.
Train: 2018-08-06T00:54:49.662356: step 4418, loss 0.619235.
Train: 2018-08-06T00:54:49.921624: step 4419, loss 0.603045.
Train: 2018-08-06T00:54:50.182927: step 4420, loss 0.562801.
Test: 2018-08-06T00:54:51.437569: step 4420, loss 0.548151.
Train: 2018-08-06T00:54:51.677952: step 4421, loss 0.530763.
Train: 2018-08-06T00:54:51.928284: step 4422, loss 0.562848.
Train: 2018-08-06T00:54:52.173601: step 4423, loss 0.530858.
Train: 2018-08-06T00:54:52.429941: step 4424, loss 0.514853.
Train: 2018-08-06T00:54:52.673266: step 4425, loss 0.570854.
Train: 2018-08-06T00:54:52.918648: step 4426, loss 0.578868.
Train: 2018-08-06T00:54:53.164951: step 4427, loss 0.635051.
Train: 2018-08-06T00:54:53.409296: step 4428, loss 0.546797.
Train: 2018-08-06T00:54:53.656635: step 4429, loss 0.659001.
Train: 2018-08-06T00:54:53.900982: step 4430, loss 0.498943.
Test: 2018-08-06T00:54:55.190533: step 4430, loss 0.549185.
Train: 2018-08-06T00:54:55.494783: step 4431, loss 0.634762.
Train: 2018-08-06T00:54:55.738162: step 4432, loss 0.610728.
Train: 2018-08-06T00:54:55.986498: step 4433, loss 0.539148.
Train: 2018-08-06T00:54:56.233832: step 4434, loss 0.610561.
Train: 2018-08-06T00:54:56.476157: step 4435, loss 0.586763.
Train: 2018-08-06T00:54:56.727510: step 4436, loss 0.56311.
Train: 2018-08-06T00:54:56.987789: step 4437, loss 0.571014.
Train: 2018-08-06T00:54:57.241137: step 4438, loss 0.563199.
Train: 2018-08-06T00:54:57.487492: step 4439, loss 0.563234.
Train: 2018-08-06T00:54:57.731831: step 4440, loss 0.586699.
Test: 2018-08-06T00:54:59.000407: step 4440, loss 0.549913.
Train: 2018-08-06T00:54:59.242790: step 4441, loss 0.563296.
Train: 2018-08-06T00:54:59.498101: step 4442, loss 0.610051.
Train: 2018-08-06T00:54:59.743420: step 4443, loss 0.532278.
Train: 2018-08-06T00:54:59.988764: step 4444, loss 0.602205.
Train: 2018-08-06T00:55:00.237131: step 4445, loss 0.555659.
Train: 2018-08-06T00:55:00.483467: step 4446, loss 0.547939.
Train: 2018-08-06T00:55:00.725794: step 4447, loss 0.633136.
Train: 2018-08-06T00:55:00.970140: step 4448, loss 0.509344.
Train: 2018-08-06T00:55:01.215484: step 4449, loss 0.532529.
Train: 2018-08-06T00:55:01.473807: step 4450, loss 0.59441.
Test: 2018-08-06T00:55:02.734436: step 4450, loss 0.551732.
Train: 2018-08-06T00:55:02.968810: step 4451, loss 0.555676.
Train: 2018-08-06T00:55:03.211194: step 4452, loss 0.563403.
Train: 2018-08-06T00:55:03.455508: step 4453, loss 0.555613.
Train: 2018-08-06T00:55:03.699892: step 4454, loss 0.547793.
Train: 2018-08-06T00:55:03.945199: step 4455, loss 0.555515.
Train: 2018-08-06T00:55:04.201543: step 4456, loss 0.500774.
Train: 2018-08-06T00:55:04.461848: step 4457, loss 0.578875.
Train: 2018-08-06T00:55:04.720127: step 4458, loss 0.618206.
Train: 2018-08-06T00:55:04.981428: step 4459, loss 0.523696.
Train: 2018-08-06T00:55:05.226800: step 4460, loss 0.578861.
Test: 2018-08-06T00:55:06.477427: step 4460, loss 0.549425.
Train: 2018-08-06T00:55:06.726760: step 4461, loss 0.594701.
Train: 2018-08-06T00:55:06.984104: step 4462, loss 0.507475.
Train: 2018-08-06T00:55:07.236397: step 4463, loss 0.539091.
Train: 2018-08-06T00:55:07.479776: step 4464, loss 0.570881.
Train: 2018-08-06T00:55:07.730107: step 4465, loss 0.602867.
Train: 2018-08-06T00:55:07.976446: step 4466, loss 0.602909.
Train: 2018-08-06T00:55:08.223784: step 4467, loss 0.506698.
Train: 2018-08-06T00:55:08.470173: step 4468, loss 0.554767.
Train: 2018-08-06T00:55:08.716515: step 4469, loss 0.482264.
Train: 2018-08-06T00:55:08.963858: step 4470, loss 0.46574.
Test: 2018-08-06T00:55:10.238419: step 4470, loss 0.548705.
Train: 2018-08-06T00:55:10.479774: step 4471, loss 0.562655.
Train: 2018-08-06T00:55:10.727143: step 4472, loss 0.546255.
Train: 2018-08-06T00:55:10.973453: step 4473, loss 0.521485.
Train: 2018-08-06T00:55:11.218796: step 4474, loss 0.562499.
Train: 2018-08-06T00:55:11.477137: step 4475, loss 0.487791.
Train: 2018-08-06T00:55:11.734418: step 4476, loss 0.562422.
Train: 2018-08-06T00:55:11.986744: step 4477, loss 0.545616.
Train: 2018-08-06T00:55:12.234082: step 4478, loss 0.58766.
Train: 2018-08-06T00:55:12.479425: step 4479, loss 0.587738.
Train: 2018-08-06T00:55:12.737761: step 4480, loss 0.587794.
Test: 2018-08-06T00:55:14.005345: step 4480, loss 0.548944.
Train: 2018-08-06T00:55:14.242735: step 4481, loss 0.638792.
Train: 2018-08-06T00:55:14.486059: step 4482, loss 0.502922.
Train: 2018-08-06T00:55:14.734426: step 4483, loss 0.587831.
Train: 2018-08-06T00:55:14.978742: step 4484, loss 0.502896.
Train: 2018-08-06T00:55:15.237051: step 4485, loss 0.579347.
Train: 2018-08-06T00:55:15.486417: step 4486, loss 0.647372.
Train: 2018-08-06T00:55:15.731727: step 4487, loss 0.553862.
Train: 2018-08-06T00:55:15.976074: step 4488, loss 0.528462.
Train: 2018-08-06T00:55:16.221418: step 4489, loss 0.503103.
Train: 2018-08-06T00:55:16.466787: step 4490, loss 0.477688.
Test: 2018-08-06T00:55:17.758308: step 4490, loss 0.548965.
Train: 2018-08-06T00:55:17.992712: step 4491, loss 0.570833.
Train: 2018-08-06T00:55:18.237034: step 4492, loss 0.638789.
Train: 2018-08-06T00:55:18.491348: step 4493, loss 0.570837.
Train: 2018-08-06T00:55:18.750655: step 4494, loss 0.570831.
Train: 2018-08-06T00:55:19.000019: step 4495, loss 0.553887.
Train: 2018-08-06T00:55:19.245370: step 4496, loss 0.579275.
Train: 2018-08-06T00:55:19.492670: step 4497, loss 0.646809.
Train: 2018-08-06T00:55:19.736050: step 4498, loss 0.61287.
Train: 2018-08-06T00:55:19.990343: step 4499, loss 0.5624.
Train: 2018-08-06T00:55:20.235682: step 4500, loss 0.545736.
Test: 2018-08-06T00:55:21.514263: step 4500, loss 0.549325.
Train: 2018-08-06T00:55:22.435334: step 4501, loss 0.512557.
Train: 2018-08-06T00:55:22.679711: step 4502, loss 0.496083.
Train: 2018-08-06T00:55:22.933003: step 4503, loss 0.545878.
Train: 2018-08-06T00:55:23.179319: step 4504, loss 0.554174.
Train: 2018-08-06T00:55:23.424695: step 4505, loss 0.554174.
Train: 2018-08-06T00:55:23.671006: step 4506, loss 0.595635.
Train: 2018-08-06T00:55:23.926322: step 4507, loss 0.579043.
Train: 2018-08-06T00:55:24.173662: step 4508, loss 0.537651.
Train: 2018-08-06T00:55:24.422994: step 4509, loss 0.570756.
Train: 2018-08-06T00:55:24.675350: step 4510, loss 0.587287.
Test: 2018-08-06T00:55:25.934950: step 4510, loss 0.549476.
Train: 2018-08-06T00:55:26.175333: step 4511, loss 0.479955.
Train: 2018-08-06T00:55:26.431623: step 4512, loss 0.603796.
Train: 2018-08-06T00:55:26.676965: step 4513, loss 0.579013.
Train: 2018-08-06T00:55:26.922335: step 4514, loss 0.52126.
Train: 2018-08-06T00:55:27.166656: step 4515, loss 0.554255.
Train: 2018-08-06T00:55:27.421973: step 4516, loss 0.545999.
Train: 2018-08-06T00:55:27.669312: step 4517, loss 0.554242.
Train: 2018-08-06T00:55:27.912662: step 4518, loss 0.570756.
Train: 2018-08-06T00:55:28.158035: step 4519, loss 0.59555.
Train: 2018-08-06T00:55:28.404372: step 4520, loss 0.545977.
Test: 2018-08-06T00:55:29.663977: step 4520, loss 0.548706.
Train: 2018-08-06T00:55:29.918323: step 4521, loss 0.603789.
Train: 2018-08-06T00:55:30.163666: step 4522, loss 0.521266.
Train: 2018-08-06T00:55:30.413005: step 4523, loss 0.554263.
Train: 2018-08-06T00:55:30.657321: step 4524, loss 0.504783.
Train: 2018-08-06T00:55:30.912667: step 4525, loss 0.579013.
Train: 2018-08-06T00:55:31.155987: step 4526, loss 0.488131.
Train: 2018-08-06T00:55:31.414327: step 4527, loss 0.562476.
Train: 2018-08-06T00:55:31.663661: step 4528, loss 0.520976.
Train: 2018-08-06T00:55:31.925953: step 4529, loss 0.520851.
Train: 2018-08-06T00:55:32.116450: step 4530, loss 0.651433.
Test: 2018-08-06T00:55:33.384029: step 4530, loss 0.548856.
Train: 2018-08-06T00:55:33.624386: step 4531, loss 0.537366.
Train: 2018-08-06T00:55:33.871724: step 4532, loss 0.63765.
Train: 2018-08-06T00:55:34.115099: step 4533, loss 0.529676.
Train: 2018-08-06T00:55:34.373384: step 4534, loss 0.554406.
Train: 2018-08-06T00:55:34.619757: step 4535, loss 0.50529.
Train: 2018-08-06T00:55:34.868061: step 4536, loss 0.537515.
Train: 2018-08-06T00:55:35.114426: step 4537, loss 0.562399.
Train: 2018-08-06T00:55:35.361739: step 4538, loss 0.579174.
Train: 2018-08-06T00:55:35.609078: step 4539, loss 0.545593.
Train: 2018-08-06T00:55:35.863398: step 4540, loss 0.503549.
Test: 2018-08-06T00:55:37.121034: step 4540, loss 0.547923.
Train: 2018-08-06T00:55:37.360395: step 4541, loss 0.553954.
Train: 2018-08-06T00:55:37.607764: step 4542, loss 0.553932.
Train: 2018-08-06T00:55:37.855096: step 4543, loss 0.528563.
Train: 2018-08-06T00:55:38.118399: step 4544, loss 0.596223.
Train: 2018-08-06T00:55:38.368702: step 4545, loss 0.528456.
Train: 2018-08-06T00:55:38.614073: step 4546, loss 0.528408.
Train: 2018-08-06T00:55:38.872384: step 4547, loss 0.545345.
Train: 2018-08-06T00:55:39.119700: step 4548, loss 0.562342.
Train: 2018-08-06T00:55:39.369033: step 4549, loss 0.60497.
Train: 2018-08-06T00:55:39.617399: step 4550, loss 0.528234.
Test: 2018-08-06T00:55:40.901933: step 4550, loss 0.548288.
Train: 2018-08-06T00:55:41.141318: step 4551, loss 0.502622.
Train: 2018-08-06T00:55:41.386637: step 4552, loss 0.562337.
Train: 2018-08-06T00:55:41.629014: step 4553, loss 0.536673.
Train: 2018-08-06T00:55:41.873335: step 4554, loss 0.545204.
Train: 2018-08-06T00:55:42.121672: step 4555, loss 0.605221.
Train: 2018-08-06T00:55:42.370040: step 4556, loss 0.622378.
Train: 2018-08-06T00:55:42.616379: step 4557, loss 0.5709.
Train: 2018-08-06T00:55:42.862720: step 4558, loss 0.553787.
Train: 2018-08-06T00:55:43.110059: step 4559, loss 0.587944.
Train: 2018-08-06T00:55:43.354374: step 4560, loss 0.587889.
Test: 2018-08-06T00:55:44.631958: step 4560, loss 0.549339.
Train: 2018-08-06T00:55:44.874309: step 4561, loss 0.570839.
Train: 2018-08-06T00:55:45.125662: step 4562, loss 0.621622.
Train: 2018-08-06T00:55:45.376998: step 4563, loss 0.587665.
Train: 2018-08-06T00:55:45.626330: step 4564, loss 0.553996.
Train: 2018-08-06T00:55:45.873642: step 4565, loss 0.587492.
Train: 2018-08-06T00:55:46.125018: step 4566, loss 0.512493.
Train: 2018-08-06T00:55:46.368324: step 4567, loss 0.562458.
Train: 2018-08-06T00:55:46.611693: step 4568, loss 0.529367.
Train: 2018-08-06T00:55:46.869006: step 4569, loss 0.479857.
Train: 2018-08-06T00:55:47.118308: step 4570, loss 0.512897.
Test: 2018-08-06T00:55:48.393898: step 4570, loss 0.548848.
Train: 2018-08-06T00:55:48.628303: step 4571, loss 0.487992.
Train: 2018-08-06T00:55:48.873645: step 4572, loss 0.570759.
Train: 2018-08-06T00:55:49.123945: step 4573, loss 0.628988.
Train: 2018-08-06T00:55:49.372280: step 4574, loss 0.562441.
Train: 2018-08-06T00:55:49.614632: step 4575, loss 0.570762.
Train: 2018-08-06T00:55:49.872941: step 4576, loss 0.653991.
Train: 2018-08-06T00:55:50.119283: step 4577, loss 0.570759.
Train: 2018-08-06T00:55:50.375628: step 4578, loss 0.562472.
Train: 2018-08-06T00:55:50.636929: step 4579, loss 0.545954.
Train: 2018-08-06T00:55:50.889255: step 4580, loss 0.570756.
Test: 2018-08-06T00:55:52.162818: step 4580, loss 0.548946.
Train: 2018-08-06T00:55:52.409159: step 4581, loss 0.529559.
Train: 2018-08-06T00:55:52.657525: step 4582, loss 0.562525.
Train: 2018-08-06T00:55:52.916827: step 4583, loss 0.562532.
Train: 2018-08-06T00:55:53.176121: step 4584, loss 0.636513.
Train: 2018-08-06T00:55:53.431456: step 4585, loss 0.488771.
Train: 2018-08-06T00:55:53.679787: step 4586, loss 0.578958.
Train: 2018-08-06T00:55:53.926102: step 4587, loss 0.578952.
Train: 2018-08-06T00:55:54.171446: step 4588, loss 0.578945.
Train: 2018-08-06T00:55:54.425766: step 4589, loss 0.546274.
Train: 2018-08-06T00:55:54.681114: step 4590, loss 0.513669.
Test: 2018-08-06T00:55:55.983600: step 4590, loss 0.548233.
Train: 2018-08-06T00:55:56.219968: step 4591, loss 0.538132.
Train: 2018-08-06T00:55:56.469332: step 4592, loss 0.546269.
Train: 2018-08-06T00:55:56.722624: step 4593, loss 0.595296.
Train: 2018-08-06T00:55:56.979935: step 4594, loss 0.546233.
Train: 2018-08-06T00:55:57.240239: step 4595, loss 0.546218.
Train: 2018-08-06T00:55:57.487577: step 4596, loss 0.546195.
Train: 2018-08-06T00:55:57.739903: step 4597, loss 0.562563.
Train: 2018-08-06T00:55:57.984250: step 4598, loss 0.595383.
Train: 2018-08-06T00:55:58.227624: step 4599, loss 0.628219.
Train: 2018-08-06T00:55:58.475979: step 4600, loss 0.578959.
Test: 2018-08-06T00:55:59.746536: step 4600, loss 0.549296.
Train: 2018-08-06T00:56:00.659074: step 4601, loss 0.497127.
Train: 2018-08-06T00:56:00.903419: step 4602, loss 0.58713.
Train: 2018-08-06T00:56:01.162702: step 4603, loss 0.578944.
Train: 2018-08-06T00:56:01.423031: step 4604, loss 0.570769.
Train: 2018-08-06T00:56:01.668374: step 4605, loss 0.619731.
Train: 2018-08-06T00:56:01.916686: step 4606, loss 0.587061.
Train: 2018-08-06T00:56:02.166019: step 4607, loss 0.619503.
Train: 2018-08-06T00:56:02.410364: step 4608, loss 0.570803.
Train: 2018-08-06T00:56:02.657702: step 4609, loss 0.603061.
Train: 2018-08-06T00:56:02.903078: step 4610, loss 0.619006.
Test: 2018-08-06T00:56:04.179632: step 4610, loss 0.549548.
Train: 2018-08-06T00:56:04.416032: step 4611, loss 0.498989.
Train: 2018-08-06T00:56:04.660347: step 4612, loss 0.523107.
Train: 2018-08-06T00:56:04.904723: step 4613, loss 0.610671.
Train: 2018-08-06T00:56:05.148048: step 4614, loss 0.610596.
Train: 2018-08-06T00:56:05.402368: step 4615, loss 0.594679.
Train: 2018-08-06T00:56:05.644745: step 4616, loss 0.468521.
Train: 2018-08-06T00:56:05.892088: step 4617, loss 0.515848.
Train: 2018-08-06T00:56:06.138399: step 4618, loss 0.555213.
Train: 2018-08-06T00:56:06.384741: step 4619, loss 0.531505.
Train: 2018-08-06T00:56:06.641055: step 4620, loss 0.539317.
Test: 2018-08-06T00:56:07.907667: step 4620, loss 0.551219.
Train: 2018-08-06T00:56:08.148025: step 4621, loss 0.57093.
Train: 2018-08-06T00:56:08.388413: step 4622, loss 0.539125.
Train: 2018-08-06T00:56:08.632729: step 4623, loss 0.47527.
Train: 2018-08-06T00:56:08.877076: step 4624, loss 0.634907.
Train: 2018-08-06T00:56:09.122450: step 4625, loss 0.62703.
Train: 2018-08-06T00:56:09.377775: step 4626, loss 0.53067.
Train: 2018-08-06T00:56:09.624078: step 4627, loss 0.546689.
Train: 2018-08-06T00:56:09.868424: step 4628, loss 0.538572.
Train: 2018-08-06T00:56:10.119752: step 4629, loss 0.546565.
Train: 2018-08-06T00:56:10.380055: step 4630, loss 0.603201.
Test: 2018-08-06T00:56:11.653649: step 4630, loss 0.548944.
Train: 2018-08-06T00:56:11.894007: step 4631, loss 0.611352.
Train: 2018-08-06T00:56:12.137356: step 4632, loss 0.611355.
Train: 2018-08-06T00:56:12.383734: step 4633, loss 0.554586.
Train: 2018-08-06T00:56:12.630038: step 4634, loss 0.570797.
Train: 2018-08-06T00:56:12.883360: step 4635, loss 0.595077.
Train: 2018-08-06T00:56:13.127745: step 4636, loss 0.562727.
Train: 2018-08-06T00:56:13.376074: step 4637, loss 0.506253.
Train: 2018-08-06T00:56:13.631385: step 4638, loss 0.635389.
Train: 2018-08-06T00:56:13.879729: step 4639, loss 0.595001.
Train: 2018-08-06T00:56:14.130038: step 4640, loss 0.554742.
Test: 2018-08-06T00:56:15.389657: step 4640, loss 0.548839.
Train: 2018-08-06T00:56:15.628021: step 4641, loss 0.522649.
Train: 2018-08-06T00:56:15.872412: step 4642, loss 0.619014.
Train: 2018-08-06T00:56:16.115716: step 4643, loss 0.490691.
Train: 2018-08-06T00:56:16.362083: step 4644, loss 0.55481.
Train: 2018-08-06T00:56:16.613386: step 4645, loss 0.4906.
Train: 2018-08-06T00:56:16.864713: step 4646, loss 0.578874.
Train: 2018-08-06T00:56:17.116072: step 4647, loss 0.611113.
Train: 2018-08-06T00:56:17.376345: step 4648, loss 0.530502.
Train: 2018-08-06T00:56:17.621690: step 4649, loss 0.538516.
Train: 2018-08-06T00:56:17.867049: step 4650, loss 0.562714.
Test: 2018-08-06T00:56:19.139629: step 4650, loss 0.548229.
Train: 2018-08-06T00:56:19.375998: step 4651, loss 0.578898.
Train: 2018-08-06T00:56:19.621341: step 4652, loss 0.61135.
Train: 2018-08-06T00:56:19.866716: step 4653, loss 0.522124.
Train: 2018-08-06T00:56:20.118044: step 4654, loss 0.530195.
Train: 2018-08-06T00:56:20.361362: step 4655, loss 0.635836.
Train: 2018-08-06T00:56:20.611719: step 4656, loss 0.595172.
Train: 2018-08-06T00:56:20.858034: step 4657, loss 0.5383.
Train: 2018-08-06T00:56:21.101383: step 4658, loss 0.54643.
Train: 2018-08-06T00:56:21.347754: step 4659, loss 0.651991.
Train: 2018-08-06T00:56:21.595088: step 4660, loss 0.643735.
Test: 2018-08-06T00:56:22.853697: step 4660, loss 0.548507.
Train: 2018-08-06T00:56:23.094053: step 4661, loss 0.514286.
Train: 2018-08-06T00:56:23.341393: step 4662, loss 0.594995.
Train: 2018-08-06T00:56:23.594745: step 4663, loss 0.490465.
Train: 2018-08-06T00:56:23.843082: step 4664, loss 0.586904.
Train: 2018-08-06T00:56:24.105350: step 4665, loss 0.562819.
Train: 2018-08-06T00:56:24.349696: step 4666, loss 0.578867.
Train: 2018-08-06T00:56:24.595072: step 4667, loss 0.538813.
Train: 2018-08-06T00:56:24.839387: step 4668, loss 0.586873.
Train: 2018-08-06T00:56:25.087723: step 4669, loss 0.594869.
Train: 2018-08-06T00:56:25.334064: step 4670, loss 0.546893.
Test: 2018-08-06T00:56:26.595689: step 4670, loss 0.550836.
Train: 2018-08-06T00:56:26.831059: step 4671, loss 0.578861.
Train: 2018-08-06T00:56:27.086377: step 4672, loss 0.515025.
Train: 2018-08-06T00:56:27.339730: step 4673, loss 0.491045.
Train: 2018-08-06T00:56:27.599036: step 4674, loss 0.538856.
Train: 2018-08-06T00:56:27.845346: step 4675, loss 0.522705.
Train: 2018-08-06T00:56:28.094684: step 4676, loss 0.49031.
Train: 2018-08-06T00:56:28.340044: step 4677, loss 0.603167.
Train: 2018-08-06T00:56:28.584397: step 4678, loss 0.587027.
Train: 2018-08-06T00:56:28.828742: step 4679, loss 0.505652.
Train: 2018-08-06T00:56:29.073090: step 4680, loss 0.58711.
Test: 2018-08-06T00:56:30.334690: step 4680, loss 0.547945.
Train: 2018-08-06T00:56:30.521191: step 4681, loss 0.492657.
Train: 2018-08-06T00:56:30.782492: step 4682, loss 0.537854.
Train: 2018-08-06T00:56:31.032822: step 4683, loss 0.570756.
Train: 2018-08-06T00:56:31.280161: step 4684, loss 0.554181.
Train: 2018-08-06T00:56:31.526503: step 4685, loss 0.529186.
Train: 2018-08-06T00:56:31.773872: step 4686, loss 0.47898.
Train: 2018-08-06T00:56:32.018188: step 4687, loss 0.53724.
Train: 2018-08-06T00:56:32.280512: step 4688, loss 0.553948.
Train: 2018-08-06T00:56:32.522869: step 4689, loss 0.469277.
Train: 2018-08-06T00:56:32.782144: step 4690, loss 0.562343.
Test: 2018-08-06T00:56:34.064714: step 4690, loss 0.548054.
Train: 2018-08-06T00:56:34.318037: step 4691, loss 0.57944.
Train: 2018-08-06T00:56:34.576379: step 4692, loss 0.527999.
Train: 2018-08-06T00:56:34.835653: step 4693, loss 0.570954.
Train: 2018-08-06T00:56:35.075043: step 4694, loss 0.545054.
Train: 2018-08-06T00:56:35.318395: step 4695, loss 0.510343.
Train: 2018-08-06T00:56:35.578665: step 4696, loss 0.597136.
Train: 2018-08-06T00:56:35.832020: step 4697, loss 0.536225.
Train: 2018-08-06T00:56:36.091325: step 4698, loss 0.597282.
Train: 2018-08-06T00:56:36.346611: step 4699, loss 0.544901.
Train: 2018-08-06T00:56:36.590959: step 4700, loss 0.536153.
Test: 2018-08-06T00:56:37.853582: step 4700, loss 0.548574.
Train: 2018-08-06T00:56:38.765431: step 4701, loss 0.544883.
Train: 2018-08-06T00:56:39.020743: step 4702, loss 0.606142.
Train: 2018-08-06T00:56:39.265089: step 4703, loss 0.553629.
Train: 2018-08-06T00:56:39.512428: step 4704, loss 0.606077.
Train: 2018-08-06T00:56:39.759749: step 4705, loss 0.553641.
Train: 2018-08-06T00:56:40.031041: step 4706, loss 0.571067.
Train: 2018-08-06T00:56:40.279377: step 4707, loss 0.518906.
Train: 2018-08-06T00:56:40.538677: step 4708, loss 0.640445.
Train: 2018-08-06T00:56:40.787986: step 4709, loss 0.562342.
Train: 2018-08-06T00:56:41.043329: step 4710, loss 0.57096.
Test: 2018-08-06T00:56:42.316896: step 4710, loss 0.549367.
Train: 2018-08-06T00:56:42.558278: step 4711, loss 0.588118.
Train: 2018-08-06T00:56:42.804592: step 4712, loss 0.647953.
Train: 2018-08-06T00:56:43.047966: step 4713, loss 0.536798.
Train: 2018-08-06T00:56:43.294283: step 4714, loss 0.562353.
Train: 2018-08-06T00:56:43.542619: step 4715, loss 0.511737.
Train: 2018-08-06T00:56:43.786973: step 4716, loss 0.495076.
Train: 2018-08-06T00:56:44.027353: step 4717, loss 0.545583.
Train: 2018-08-06T00:56:44.272666: step 4718, loss 0.579176.
Train: 2018-08-06T00:56:44.517044: step 4719, loss 0.662967.
Train: 2018-08-06T00:56:44.764377: step 4720, loss 0.512324.
Test: 2018-08-06T00:56:46.064873: step 4720, loss 0.548714.
Train: 2018-08-06T00:56:46.302272: step 4721, loss 0.495795.
Train: 2018-08-06T00:56:46.546591: step 4722, loss 0.54579.
Train: 2018-08-06T00:56:46.790963: step 4723, loss 0.604053.
Train: 2018-08-06T00:56:47.046285: step 4724, loss 0.587384.
Train: 2018-08-06T00:56:47.293623: step 4725, loss 0.55417.
Train: 2018-08-06T00:56:47.544920: step 4726, loss 0.579038.
Train: 2018-08-06T00:56:47.789267: step 4727, loss 0.570756.
Train: 2018-08-06T00:56:48.045607: step 4728, loss 0.579004.
Train: 2018-08-06T00:56:48.295938: step 4729, loss 0.578987.
Train: 2018-08-06T00:56:48.540299: step 4730, loss 0.537916.
Test: 2018-08-06T00:56:49.811859: step 4730, loss 0.54906.
Train: 2018-08-06T00:56:50.047254: step 4731, loss 0.496981.
Train: 2018-08-06T00:56:50.291576: step 4732, loss 0.505148.
Train: 2018-08-06T00:56:50.546893: step 4733, loss 0.505078.
Train: 2018-08-06T00:56:50.805203: step 4734, loss 0.587365.
Train: 2018-08-06T00:56:51.061547: step 4735, loss 0.570646.
Train: 2018-08-06T00:56:51.300876: step 4736, loss 0.587665.
Train: 2018-08-06T00:56:51.548215: step 4737, loss 0.587198.
Train: 2018-08-06T00:56:51.795584: step 4738, loss 0.587285.
Train: 2018-08-06T00:56:52.043920: step 4739, loss 0.529545.
Train: 2018-08-06T00:56:52.292225: step 4740, loss 0.562515.
Test: 2018-08-06T00:56:53.580779: step 4740, loss 0.548371.
Train: 2018-08-06T00:56:53.823166: step 4741, loss 0.587242.
Train: 2018-08-06T00:56:54.069503: step 4742, loss 0.554299.
Train: 2018-08-06T00:56:54.319828: step 4743, loss 0.578984.
Train: 2018-08-06T00:56:54.578139: step 4744, loss 0.587189.
Train: 2018-08-06T00:56:54.826447: step 4745, loss 0.546102.
Train: 2018-08-06T00:56:55.070819: step 4746, loss 0.595398.
Train: 2018-08-06T00:56:55.315141: step 4747, loss 0.546168.
Train: 2018-08-06T00:56:55.560485: step 4748, loss 0.546218.
Train: 2018-08-06T00:56:55.808839: step 4749, loss 0.488922.
Train: 2018-08-06T00:56:56.067156: step 4750, loss 0.578994.
Test: 2018-08-06T00:56:57.338729: step 4750, loss 0.547754.
Train: 2018-08-06T00:56:57.647061: step 4751, loss 0.669138.
Train: 2018-08-06T00:56:57.893371: step 4752, loss 0.595262.
Train: 2018-08-06T00:56:58.140738: step 4753, loss 0.595227.
Train: 2018-08-06T00:56:58.387050: step 4754, loss 0.570786.
Train: 2018-08-06T00:56:58.635387: step 4755, loss 0.473558.
Train: 2018-08-06T00:56:58.882726: step 4756, loss 0.595092.
Train: 2018-08-06T00:56:59.126075: step 4757, loss 0.578891.
Train: 2018-08-06T00:56:59.371441: step 4758, loss 0.538502.
Train: 2018-08-06T00:56:59.624764: step 4759, loss 0.522377.
Train: 2018-08-06T00:56:59.869110: step 4760, loss 0.595042.
Test: 2018-08-06T00:57:01.149685: step 4760, loss 0.549058.
Train: 2018-08-06T00:57:01.391040: step 4761, loss 0.546589.
Train: 2018-08-06T00:57:01.633392: step 4762, loss 0.498089.
Train: 2018-08-06T00:57:01.878736: step 4763, loss 0.595094.
Train: 2018-08-06T00:57:02.124104: step 4764, loss 0.530319.
Train: 2018-08-06T00:57:02.366431: step 4765, loss 0.570761.
Train: 2018-08-06T00:57:02.615765: step 4766, loss 0.530007.
Train: 2018-08-06T00:57:02.865098: step 4767, loss 0.521862.
Train: 2018-08-06T00:57:03.112437: step 4768, loss 0.562824.
Train: 2018-08-06T00:57:03.369779: step 4769, loss 0.603085.
Train: 2018-08-06T00:57:03.614125: step 4770, loss 0.529482.
Test: 2018-08-06T00:57:04.876736: step 4770, loss 0.548786.
Train: 2018-08-06T00:57:05.115093: step 4771, loss 0.513889.
Train: 2018-08-06T00:57:05.371395: step 4772, loss 0.578621.
Train: 2018-08-06T00:57:05.616738: step 4773, loss 0.570494.
Train: 2018-08-06T00:57:05.860118: step 4774, loss 0.546538.
Train: 2018-08-06T00:57:06.107459: step 4775, loss 0.486592.
Train: 2018-08-06T00:57:06.349806: step 4776, loss 0.528436.
Train: 2018-08-06T00:57:06.609085: step 4777, loss 0.544345.
Train: 2018-08-06T00:57:06.865400: step 4778, loss 0.605033.
Train: 2018-08-06T00:57:07.122711: step 4779, loss 0.606048.
Train: 2018-08-06T00:57:07.369079: step 4780, loss 0.632257.
Test: 2018-08-06T00:57:08.671570: step 4780, loss 0.548054.
Train: 2018-08-06T00:57:08.919905: step 4781, loss 0.538221.
Train: 2018-08-06T00:57:09.167299: step 4782, loss 0.476285.
Train: 2018-08-06T00:57:09.412618: step 4783, loss 0.656125.
Train: 2018-08-06T00:57:09.659927: step 4784, loss 0.545783.
Train: 2018-08-06T00:57:09.904298: step 4785, loss 0.612808.
Train: 2018-08-06T00:57:10.150614: step 4786, loss 0.503888.
Train: 2018-08-06T00:57:10.409920: step 4787, loss 0.571146.
Train: 2018-08-06T00:57:10.668263: step 4788, loss 0.562625.
Train: 2018-08-06T00:57:10.915569: step 4789, loss 0.59556.
Train: 2018-08-06T00:57:11.164901: step 4790, loss 0.579006.
Test: 2018-08-06T00:57:12.437498: step 4790, loss 0.548818.
Train: 2018-08-06T00:57:12.685834: step 4791, loss 0.562542.
Train: 2018-08-06T00:57:12.930180: step 4792, loss 0.595366.
Train: 2018-08-06T00:57:13.186526: step 4793, loss 0.529865.
Train: 2018-08-06T00:57:13.434832: step 4794, loss 0.521757.
Train: 2018-08-06T00:57:13.682195: step 4795, loss 0.652444.
Train: 2018-08-06T00:57:13.923557: step 4796, loss 0.505612.
Train: 2018-08-06T00:57:14.172858: step 4797, loss 0.619617.
Train: 2018-08-06T00:57:14.420220: step 4798, loss 0.554531.
Train: 2018-08-06T00:57:14.669529: step 4799, loss 0.619427.
Train: 2018-08-06T00:57:14.910883: step 4800, loss 0.562687.
Test: 2018-08-06T00:57:16.179491: step 4800, loss 0.549084.
Train: 2018-08-06T00:57:17.157805: step 4801, loss 0.562733.
Train: 2018-08-06T00:57:17.409125: step 4802, loss 0.594934.
Train: 2018-08-06T00:57:17.655457: step 4803, loss 0.554703.
Train: 2018-08-06T00:57:17.901774: step 4804, loss 0.578881.
Train: 2018-08-06T00:57:18.150140: step 4805, loss 0.53887.
Train: 2018-08-06T00:57:18.393464: step 4806, loss 0.546843.
Train: 2018-08-06T00:57:18.638803: step 4807, loss 0.578869.
Train: 2018-08-06T00:57:18.887169: step 4808, loss 0.59482.
Train: 2018-08-06T00:57:19.136471: step 4809, loss 0.602702.
Train: 2018-08-06T00:57:19.383836: step 4810, loss 0.483556.
Test: 2018-08-06T00:57:20.656406: step 4810, loss 0.549525.
Train: 2018-08-06T00:57:20.901778: step 4811, loss 0.642367.
Train: 2018-08-06T00:57:21.147094: step 4812, loss 0.594776.
Train: 2018-08-06T00:57:21.406405: step 4813, loss 0.570949.
Train: 2018-08-06T00:57:21.656756: step 4814, loss 0.499955.
Train: 2018-08-06T00:57:21.901079: step 4815, loss 0.563106.
Train: 2018-08-06T00:57:22.146422: step 4816, loss 0.602479.
Train: 2018-08-06T00:57:22.400741: step 4817, loss 0.515746.
Train: 2018-08-06T00:57:22.653098: step 4818, loss 0.563083.
Train: 2018-08-06T00:57:22.900431: step 4819, loss 0.586751.
Train: 2018-08-06T00:57:23.146773: step 4820, loss 0.499853.
Test: 2018-08-06T00:57:24.401391: step 4820, loss 0.550717.
Train: 2018-08-06T00:57:24.650755: step 4821, loss 0.570933.
Train: 2018-08-06T00:57:24.899087: step 4822, loss 0.547153.
Train: 2018-08-06T00:57:25.147421: step 4823, loss 0.578855.
Train: 2018-08-06T00:57:25.394768: step 4824, loss 0.555004.
Train: 2018-08-06T00:57:25.641101: step 4825, loss 0.530998.
Train: 2018-08-06T00:57:25.887417: step 4826, loss 0.578906.
Train: 2018-08-06T00:57:26.134787: step 4827, loss 0.610888.
Train: 2018-08-06T00:57:26.380099: step 4828, loss 0.554813.
Train: 2018-08-06T00:57:26.628449: step 4829, loss 0.55481.
Train: 2018-08-06T00:57:26.884749: step 4830, loss 0.619023.
Test: 2018-08-06T00:57:28.147373: step 4830, loss 0.54867.
Train: 2018-08-06T00:57:28.394736: step 4831, loss 0.546756.
Train: 2018-08-06T00:57:28.590214: step 4832, loss 0.648465.
Train: 2018-08-06T00:57:28.837552: step 4833, loss 0.506743.
Train: 2018-08-06T00:57:29.082871: step 4834, loss 0.514763.
Train: 2018-08-06T00:57:29.327219: step 4835, loss 0.562836.
Train: 2018-08-06T00:57:29.574583: step 4836, loss 0.554783.
Train: 2018-08-06T00:57:29.827905: step 4837, loss 0.562794.
Train: 2018-08-06T00:57:30.073247: step 4838, loss 0.546681.
Train: 2018-08-06T00:57:30.316616: step 4839, loss 0.482146.
Train: 2018-08-06T00:57:30.564914: step 4840, loss 0.619334.
Test: 2018-08-06T00:57:31.834518: step 4840, loss 0.54879.
Train: 2018-08-06T00:57:32.078876: step 4841, loss 0.595101.
Train: 2018-08-06T00:57:32.325230: step 4842, loss 0.603226.
Train: 2018-08-06T00:57:32.584512: step 4843, loss 0.522153.
Train: 2018-08-06T00:57:32.830853: step 4844, loss 0.64381.
Train: 2018-08-06T00:57:33.090160: step 4845, loss 0.619417.
Train: 2018-08-06T00:57:33.337525: step 4846, loss 0.586974.
Train: 2018-08-06T00:57:33.596805: step 4847, loss 0.538565.
Train: 2018-08-06T00:57:33.849161: step 4848, loss 0.570827.
Train: 2018-08-06T00:57:34.095497: step 4849, loss 0.562801.
Train: 2018-08-06T00:57:34.339849: step 4850, loss 0.578869.
Test: 2018-08-06T00:57:35.598451: step 4850, loss 0.548729.
Train: 2018-08-06T00:57:35.849780: step 4851, loss 0.506774.
Train: 2018-08-06T00:57:36.095123: step 4852, loss 0.458712.
Train: 2018-08-06T00:57:36.340493: step 4853, loss 0.506591.
Train: 2018-08-06T00:57:36.582844: step 4854, loss 0.50633.
Train: 2018-08-06T00:57:36.842126: step 4855, loss 0.5303.
Train: 2018-08-06T00:57:37.103427: step 4856, loss 0.489373.
Train: 2018-08-06T00:57:37.350766: step 4857, loss 0.6363.
Train: 2018-08-06T00:57:37.594141: step 4858, loss 0.59543.
Train: 2018-08-06T00:57:37.839458: step 4859, loss 0.554262.
Train: 2018-08-06T00:57:38.087825: step 4860, loss 0.562486.
Test: 2018-08-06T00:57:39.357399: step 4860, loss 0.549591.
Train: 2018-08-06T00:57:39.595762: step 4861, loss 0.620459.
Train: 2018-08-06T00:57:39.845138: step 4862, loss 0.529316.
Train: 2018-08-06T00:57:40.099416: step 4863, loss 0.645457.
Train: 2018-08-06T00:57:40.346754: step 4864, loss 0.6288.
Train: 2018-08-06T00:57:40.599105: step 4865, loss 0.587298.
Train: 2018-08-06T00:57:40.844422: step 4866, loss 0.570757.
Train: 2018-08-06T00:57:41.090764: step 4867, loss 0.587212.
Train: 2018-08-06T00:57:41.351067: step 4868, loss 0.611768.
Train: 2018-08-06T00:57:41.608379: step 4869, loss 0.529927.
Train: 2018-08-06T00:57:41.855718: step 4870, loss 0.54634.
Test: 2018-08-06T00:57:43.122330: step 4870, loss 0.54983.
Train: 2018-08-06T00:57:43.360733: step 4871, loss 0.538272.
Train: 2018-08-06T00:57:43.614046: step 4872, loss 0.627607.
Train: 2018-08-06T00:57:43.876313: step 4873, loss 0.578894.
Train: 2018-08-06T00:57:44.120692: step 4874, loss 0.522379.
Train: 2018-08-06T00:57:44.373984: step 4875, loss 0.5547.
Train: 2018-08-06T00:57:44.632292: step 4876, loss 0.514469.
Train: 2018-08-06T00:57:44.883651: step 4877, loss 0.58693.
Train: 2018-08-06T00:57:45.129962: step 4878, loss 0.538626.
Train: 2018-08-06T00:57:45.371317: step 4879, loss 0.522507.
Train: 2018-08-06T00:57:45.614666: step 4880, loss 0.546626.
Test: 2018-08-06T00:57:46.883272: step 4880, loss 0.5485.
Train: 2018-08-06T00:57:47.122633: step 4881, loss 0.538502.
Train: 2018-08-06T00:57:47.369971: step 4882, loss 0.514143.
Train: 2018-08-06T00:57:47.614342: step 4883, loss 0.635737.
Train: 2018-08-06T00:57:47.857667: step 4884, loss 0.578912.
Train: 2018-08-06T00:57:48.105034: step 4885, loss 0.538254.
Train: 2018-08-06T00:57:48.350380: step 4886, loss 0.530072.
Train: 2018-08-06T00:57:48.604669: step 4887, loss 0.562618.
Train: 2018-08-06T00:57:48.850041: step 4888, loss 0.619781.
Train: 2018-08-06T00:57:49.104359: step 4889, loss 0.546261.
Train: 2018-08-06T00:57:49.353696: step 4890, loss 0.60346.
Test: 2018-08-06T00:57:50.619281: step 4890, loss 0.549711.
Train: 2018-08-06T00:57:50.866619: step 4891, loss 0.497252.
Train: 2018-08-06T00:57:51.110973: step 4892, loss 0.53806.
Train: 2018-08-06T00:57:51.359302: step 4893, loss 0.538009.
Train: 2018-08-06T00:57:51.604677: step 4894, loss 0.628189.
Train: 2018-08-06T00:57:51.864981: step 4895, loss 0.529736.
Train: 2018-08-06T00:57:52.116304: step 4896, loss 0.521488.
Train: 2018-08-06T00:57:52.362619: step 4897, loss 0.578984.
Train: 2018-08-06T00:57:52.617014: step 4898, loss 0.562523.
Train: 2018-08-06T00:57:52.864356: step 4899, loss 0.546031.
Train: 2018-08-06T00:57:53.107675: step 4900, loss 0.587259.
Test: 2018-08-06T00:57:54.397225: step 4900, loss 0.548906.
Train: 2018-08-06T00:57:55.394563: step 4901, loss 0.587266.
Train: 2018-08-06T00:57:55.638904: step 4902, loss 0.529494.
Train: 2018-08-06T00:57:55.898213: step 4903, loss 0.570756.
Train: 2018-08-06T00:57:56.153531: step 4904, loss 0.545984.
Train: 2018-08-06T00:57:56.406847: step 4905, loss 0.59554.
Train: 2018-08-06T00:57:56.651194: step 4906, loss 0.620303.
Train: 2018-08-06T00:57:56.898538: step 4907, loss 0.570757.
Train: 2018-08-06T00:57:57.146843: step 4908, loss 0.537844.
Train: 2018-08-06T00:57:57.408176: step 4909, loss 0.56254.
Train: 2018-08-06T00:57:57.660498: step 4910, loss 0.56255.
Test: 2018-08-06T00:57:58.929076: step 4910, loss 0.547916.
Train: 2018-08-06T00:57:59.163449: step 4911, loss 0.554357.
Train: 2018-08-06T00:57:59.409791: step 4912, loss 0.488798.
Train: 2018-08-06T00:57:59.653141: step 4913, loss 0.546147.
Train: 2018-08-06T00:57:59.898484: step 4914, loss 0.570759.
Train: 2018-08-06T00:58:00.146820: step 4915, loss 0.578981.
Train: 2018-08-06T00:58:00.399170: step 4916, loss 0.603662.
Train: 2018-08-06T00:58:00.658483: step 4917, loss 0.620083.
Train: 2018-08-06T00:58:00.916761: step 4918, loss 0.529737.
Train: 2018-08-06T00:58:01.162138: step 4919, loss 0.537976.
Train: 2018-08-06T00:58:01.407477: step 4920, loss 0.603537.
Test: 2018-08-06T00:58:02.685031: step 4920, loss 0.549105.
Train: 2018-08-06T00:58:02.921400: step 4921, loss 0.521669.
Train: 2018-08-06T00:58:03.170733: step 4922, loss 0.619851.
Train: 2018-08-06T00:58:03.418072: step 4923, loss 0.644281.
Train: 2018-08-06T00:58:03.661421: step 4924, loss 0.538216.
Train: 2018-08-06T00:58:03.921725: step 4925, loss 0.513939.
Train: 2018-08-06T00:58:04.164107: step 4926, loss 0.505888.
Train: 2018-08-06T00:58:04.410451: step 4927, loss 0.627604.
Train: 2018-08-06T00:58:04.655761: step 4928, loss 0.595116.
Train: 2018-08-06T00:58:04.905095: step 4929, loss 0.546519.
Train: 2018-08-06T00:58:05.156422: step 4930, loss 0.570805.
Test: 2018-08-06T00:58:06.434006: step 4930, loss 0.548506.
Train: 2018-08-06T00:58:06.674364: step 4931, loss 0.498136.
Train: 2018-08-06T00:58:06.921732: step 4932, loss 0.562729.
Train: 2018-08-06T00:58:07.170069: step 4933, loss 0.546555.
Train: 2018-08-06T00:58:07.424392: step 4934, loss 0.530348.
Train: 2018-08-06T00:58:07.668735: step 4935, loss 0.522172.
Train: 2018-08-06T00:58:07.928012: step 4936, loss 0.546415.
Train: 2018-08-06T00:58:08.175375: step 4937, loss 0.538199.
Train: 2018-08-06T00:58:08.431665: step 4938, loss 0.652453.
Train: 2018-08-06T00:58:08.679031: step 4939, loss 0.554423.
Train: 2018-08-06T00:58:08.938333: step 4940, loss 0.497168.
Test: 2018-08-06T00:58:10.213896: step 4940, loss 0.549826.
Train: 2018-08-06T00:58:10.452259: step 4941, loss 0.578957.
Train: 2018-08-06T00:58:10.698601: step 4942, loss 0.603587.
Train: 2018-08-06T00:58:10.947935: step 4943, loss 0.505081.
Train: 2018-08-06T00:58:11.203308: step 4944, loss 0.55429.
Train: 2018-08-06T00:58:11.450621: step 4945, loss 0.611977.
Train: 2018-08-06T00:58:11.696956: step 4946, loss 0.65323.
Train: 2018-08-06T00:58:11.944303: step 4947, loss 0.455657.
Train: 2018-08-06T00:58:12.190611: step 4948, loss 0.587211.
Train: 2018-08-06T00:58:12.442935: step 4949, loss 0.587214.
Train: 2018-08-06T00:58:12.692294: step 4950, loss 0.611877.
Test: 2018-08-06T00:58:13.948908: step 4950, loss 0.549219.
Train: 2018-08-06T00:58:14.192283: step 4951, loss 0.652846.
Train: 2018-08-06T00:58:14.449599: step 4952, loss 0.513507.
Train: 2018-08-06T00:58:14.699928: step 4953, loss 0.52998.
Train: 2018-08-06T00:58:14.954221: step 4954, loss 0.521863.
Train: 2018-08-06T00:58:15.202555: step 4955, loss 0.595222.
Train: 2018-08-06T00:58:15.447900: step 4956, loss 0.481203.
Train: 2018-08-06T00:58:15.695264: step 4957, loss 0.56262.
Train: 2018-08-06T00:58:15.949558: step 4958, loss 0.587086.
Train: 2018-08-06T00:58:16.195924: step 4959, loss 0.505475.
Train: 2018-08-06T00:58:16.443242: step 4960, loss 0.538072.
Test: 2018-08-06T00:58:17.712842: step 4960, loss 0.548705.
Train: 2018-08-06T00:58:17.952237: step 4961, loss 0.628094.
Train: 2018-08-06T00:58:18.204552: step 4962, loss 0.53799.
Train: 2018-08-06T00:58:18.461840: step 4963, loss 0.562563.
Train: 2018-08-06T00:58:18.716193: step 4964, loss 0.595374.
Train: 2018-08-06T00:58:18.970480: step 4965, loss 0.570761.
Train: 2018-08-06T00:58:19.222830: step 4966, loss 0.546162.
Train: 2018-08-06T00:58:19.469146: step 4967, loss 0.619962.
Train: 2018-08-06T00:58:19.716484: step 4968, loss 0.636275.
Train: 2018-08-06T00:58:19.962860: step 4969, loss 0.52178.
Train: 2018-08-06T00:58:20.209166: step 4970, loss 0.546322.
Test: 2018-08-06T00:58:21.487747: step 4970, loss 0.549226.
Train: 2018-08-06T00:58:21.725113: step 4971, loss 0.546353.
Train: 2018-08-06T00:58:21.970456: step 4972, loss 0.52196.
Train: 2018-08-06T00:58:22.219828: step 4973, loss 0.635899.
Train: 2018-08-06T00:58:22.471117: step 4974, loss 0.546395.
Train: 2018-08-06T00:58:22.716461: step 4975, loss 0.619527.
Train: 2018-08-06T00:58:22.963826: step 4976, loss 0.546477.
Train: 2018-08-06T00:58:23.216158: step 4977, loss 0.522237.
Train: 2018-08-06T00:58:23.458508: step 4978, loss 0.586985.
Train: 2018-08-06T00:58:23.710830: step 4979, loss 0.570805.
Train: 2018-08-06T00:58:23.970139: step 4980, loss 0.530419.
Test: 2018-08-06T00:58:25.247691: step 4980, loss 0.548867.
Train: 2018-08-06T00:58:25.487077: step 4981, loss 0.554647.
Train: 2018-08-06T00:58:25.732421: step 4982, loss 0.56274.
Train: 2018-08-06T00:58:25.920925: step 4983, loss 0.476495.
Train: 2018-08-06T00:58:26.169228: step 4984, loss 0.514061.
Train: 2018-08-06T00:58:26.413574: step 4985, loss 0.546334.
Train: 2018-08-06T00:58:26.659940: step 4986, loss 0.603437.
Train: 2018-08-06T00:58:26.908261: step 4987, loss 0.579159.
Train: 2018-08-06T00:58:27.156587: step 4988, loss 0.587293.
Train: 2018-08-06T00:58:27.413924: step 4989, loss 0.587154.
Train: 2018-08-06T00:58:27.663276: step 4990, loss 0.587137.
Test: 2018-08-06T00:58:28.923860: step 4990, loss 0.549296.
Train: 2018-08-06T00:58:29.161226: step 4991, loss 0.603495.
Train: 2018-08-06T00:58:29.406569: step 4992, loss 0.562598.
Train: 2018-08-06T00:58:29.649919: step 4993, loss 0.521814.
Train: 2018-08-06T00:58:29.908227: step 4994, loss 0.554454.
Train: 2018-08-06T00:58:30.161556: step 4995, loss 0.497359.
Train: 2018-08-06T00:58:30.406928: step 4996, loss 0.644304.
Train: 2018-08-06T00:58:30.654261: step 4997, loss 0.570738.
Train: 2018-08-06T00:58:30.898580: step 4998, loss 0.652465.
Train: 2018-08-06T00:58:31.141955: step 4999, loss 0.587036.
Train: 2018-08-06T00:58:31.390264: step 5000, loss 0.55454.
Test: 2018-08-06T00:58:32.661863: step 5000, loss 0.548445.
Train: 2018-08-06T00:58:33.571153: step 5001, loss 0.595082.
Train: 2018-08-06T00:58:33.825456: step 5002, loss 0.554663.
Train: 2018-08-06T00:58:34.076770: step 5003, loss 0.586918.
Train: 2018-08-06T00:58:34.329126: step 5004, loss 0.506637.
Train: 2018-08-06T00:58:34.590423: step 5005, loss 0.56283.
Train: 2018-08-06T00:58:34.848705: step 5006, loss 0.554835.
Train: 2018-08-06T00:58:35.095048: step 5007, loss 0.610888.
Train: 2018-08-06T00:58:35.341387: step 5008, loss 0.578862.
Train: 2018-08-06T00:58:35.586732: step 5009, loss 0.522998.
Train: 2018-08-06T00:58:35.835069: step 5010, loss 0.515042.
Test: 2018-08-06T00:58:37.106667: step 5010, loss 0.549556.
Train: 2018-08-06T00:58:37.349043: step 5011, loss 0.514983.
Train: 2018-08-06T00:58:37.594389: step 5012, loss 0.522853.
Train: 2018-08-06T00:58:37.842700: step 5013, loss 0.538744.
Train: 2018-08-06T00:58:38.101012: step 5014, loss 0.586928.
Train: 2018-08-06T00:58:38.357323: step 5015, loss 0.522386.
Train: 2018-08-06T00:58:38.606655: step 5016, loss 0.554605.
Train: 2018-08-06T00:58:38.852001: step 5017, loss 0.497689.
Train: 2018-08-06T00:58:39.099337: step 5018, loss 0.603401.
Train: 2018-08-06T00:58:39.345679: step 5019, loss 0.595309.
Train: 2018-08-06T00:58:39.601022: step 5020, loss 0.570763.
Test: 2018-08-06T00:58:40.867609: step 5020, loss 0.549033.
Train: 2018-08-06T00:58:41.104974: step 5021, loss 0.587177.
Train: 2018-08-06T00:58:41.379364: step 5022, loss 0.537903.
Train: 2018-08-06T00:58:41.639694: step 5023, loss 0.562532.
Train: 2018-08-06T00:58:41.880027: step 5024, loss 0.570767.
Train: 2018-08-06T00:58:42.129360: step 5025, loss 0.504856.
Train: 2018-08-06T00:58:42.374734: step 5026, loss 0.537738.
Train: 2018-08-06T00:58:42.624062: step 5027, loss 0.636934.
Train: 2018-08-06T00:58:42.869381: step 5028, loss 0.562485.
Train: 2018-08-06T00:58:43.116719: step 5029, loss 0.504572.
Train: 2018-08-06T00:58:43.365056: step 5030, loss 0.595611.
Test: 2018-08-06T00:58:44.629673: step 5030, loss 0.547486.
Train: 2018-08-06T00:58:44.868066: step 5031, loss 0.554184.
Train: 2018-08-06T00:58:45.116402: step 5032, loss 0.529306.
Train: 2018-08-06T00:58:45.359720: step 5033, loss 0.587357.
Train: 2018-08-06T00:58:45.608056: step 5034, loss 0.545855.
Train: 2018-08-06T00:58:45.854399: step 5035, loss 0.587372.
Train: 2018-08-06T00:58:46.097747: step 5036, loss 0.653807.
Train: 2018-08-06T00:58:46.350098: step 5037, loss 0.529328.
Train: 2018-08-06T00:58:46.597441: step 5038, loss 0.636952.
Train: 2018-08-06T00:58:46.842755: step 5039, loss 0.595505.
Train: 2018-08-06T00:58:47.098072: step 5040, loss 0.578979.
Test: 2018-08-06T00:58:48.377650: step 5040, loss 0.548327.
Train: 2018-08-06T00:58:48.628005: step 5041, loss 0.546191.
Train: 2018-08-06T00:58:48.872326: step 5042, loss 0.546268.
Train: 2018-08-06T00:58:49.124652: step 5043, loss 0.554478.
Train: 2018-08-06T00:58:49.385984: step 5044, loss 0.595184.
Train: 2018-08-06T00:58:49.629302: step 5045, loss 0.55456.
Train: 2018-08-06T00:58:49.876670: step 5046, loss 0.570797.
Train: 2018-08-06T00:58:50.125007: step 5047, loss 0.538472.
Train: 2018-08-06T00:58:50.371351: step 5048, loss 0.530438.
Train: 2018-08-06T00:58:50.634613: step 5049, loss 0.570812.
Train: 2018-08-06T00:58:50.877963: step 5050, loss 0.506248.
Test: 2018-08-06T00:58:52.132607: step 5050, loss 0.549978.
Train: 2018-08-06T00:58:52.370003: step 5051, loss 0.562728.
Train: 2018-08-06T00:58:52.614349: step 5052, loss 0.562715.
Train: 2018-08-06T00:58:52.861688: step 5053, loss 0.538415.
Train: 2018-08-06T00:58:53.112014: step 5054, loss 0.595117.
Train: 2018-08-06T00:58:53.356337: step 5055, loss 0.473432.
Train: 2018-08-06T00:58:53.600709: step 5056, loss 0.562646.
Train: 2018-08-06T00:58:53.848044: step 5057, loss 0.54631.
Train: 2018-08-06T00:58:54.103336: step 5058, loss 0.505366.
Train: 2018-08-06T00:58:54.350675: step 5059, loss 0.546147.
Train: 2018-08-06T00:58:54.597016: step 5060, loss 0.55429.
Test: 2018-08-06T00:58:55.888562: step 5060, loss 0.549459.
Train: 2018-08-06T00:58:56.126956: step 5061, loss 0.537711.
Train: 2018-08-06T00:58:56.371272: step 5062, loss 0.570757.
Train: 2018-08-06T00:58:56.615619: step 5063, loss 0.545819.
Train: 2018-08-06T00:58:56.858967: step 5064, loss 0.545753.
Train: 2018-08-06T00:58:57.109298: step 5065, loss 0.595854.
Train: 2018-08-06T00:58:57.352648: step 5066, loss 0.528902.
Train: 2018-08-06T00:58:57.602011: step 5067, loss 0.528823.
Train: 2018-08-06T00:58:57.848352: step 5068, loss 0.469836.
Train: 2018-08-06T00:58:58.103669: step 5069, loss 0.537021.
Train: 2018-08-06T00:58:58.352001: step 5070, loss 0.579314.
Test: 2018-08-06T00:58:59.612603: step 5070, loss 0.54794.
Train: 2018-08-06T00:58:59.905838: step 5071, loss 0.528314.
Train: 2018-08-06T00:59:00.153208: step 5072, loss 0.579408.
Train: 2018-08-06T00:59:00.415475: step 5073, loss 0.579446.
Train: 2018-08-06T00:59:00.663811: step 5074, loss 0.596606.
Train: 2018-08-06T00:59:00.918137: step 5075, loss 0.519481.
Train: 2018-08-06T00:59:01.170456: step 5076, loss 0.579494.
Train: 2018-08-06T00:59:01.415805: step 5077, loss 0.53659.
Train: 2018-08-06T00:59:01.661170: step 5078, loss 0.467884.
Train: 2018-08-06T00:59:01.905521: step 5079, loss 0.53652.
Train: 2018-08-06T00:59:02.152863: step 5080, loss 0.579588.
Test: 2018-08-06T00:59:03.413458: step 5080, loss 0.547915.
Train: 2018-08-06T00:59:03.648828: step 5081, loss 0.640072.
Train: 2018-08-06T00:59:03.896200: step 5082, loss 0.527813.
Train: 2018-08-06T00:59:04.145501: step 5083, loss 0.605489.
Train: 2018-08-06T00:59:04.391841: step 5084, loss 0.570956.
Train: 2018-08-06T00:59:04.639179: step 5085, loss 0.570941.
Train: 2018-08-06T00:59:04.886543: step 5086, loss 0.613866.
Train: 2018-08-06T00:59:05.128901: step 5087, loss 0.596584.
Train: 2018-08-06T00:59:05.376209: step 5088, loss 0.536748.
Train: 2018-08-06T00:59:05.622581: step 5089, loss 0.562344.
Train: 2018-08-06T00:59:05.875898: step 5090, loss 0.647142.
Test: 2018-08-06T00:59:07.147472: step 5090, loss 0.54769.
Train: 2018-08-06T00:59:07.384862: step 5091, loss 0.52861.
Train: 2018-08-06T00:59:07.627188: step 5092, loss 0.562382.
Train: 2018-08-06T00:59:07.875524: step 5093, loss 0.629431.
Train: 2018-08-06T00:59:08.127881: step 5094, loss 0.620801.
Train: 2018-08-06T00:59:08.372197: step 5095, loss 0.579049.
Train: 2018-08-06T00:59:08.617571: step 5096, loss 0.611982.
Train: 2018-08-06T00:59:08.867902: step 5097, loss 0.603537.
Train: 2018-08-06T00:59:09.114240: step 5098, loss 0.586959.
Train: 2018-08-06T00:59:09.360552: step 5099, loss 0.523283.
Train: 2018-08-06T00:59:09.602905: step 5100, loss 0.53915.
Test: 2018-08-06T00:59:10.850568: step 5100, loss 0.549997.
Train: 2018-08-06T00:59:11.761719: step 5101, loss 0.546798.
Train: 2018-08-06T00:59:12.008049: step 5102, loss 0.538891.
Train: 2018-08-06T00:59:12.256410: step 5103, loss 0.562902.
Train: 2018-08-06T00:59:12.505748: step 5104, loss 0.554959.
Train: 2018-08-06T00:59:12.760038: step 5105, loss 0.547028.
Train: 2018-08-06T00:59:13.005382: step 5106, loss 0.539089.
Train: 2018-08-06T00:59:13.250756: step 5107, loss 0.61864.
Train: 2018-08-06T00:59:13.503050: step 5108, loss 0.586807.
Train: 2018-08-06T00:59:13.763385: step 5109, loss 0.562984.
Train: 2018-08-06T00:59:14.009725: step 5110, loss 0.539213.
Test: 2018-08-06T00:59:15.284286: step 5110, loss 0.5494.
Train: 2018-08-06T00:59:15.519658: step 5111, loss 0.531295.
Train: 2018-08-06T00:59:15.768990: step 5112, loss 0.499529.
Train: 2018-08-06T00:59:16.016329: step 5113, loss 0.578858.
Train: 2018-08-06T00:59:16.259678: step 5114, loss 0.55496.
Train: 2018-08-06T00:59:16.516033: step 5115, loss 0.562896.
Train: 2018-08-06T00:59:16.762349: step 5116, loss 0.594858.
Train: 2018-08-06T00:59:17.009718: step 5117, loss 0.594876.
Train: 2018-08-06T00:59:17.254065: step 5118, loss 0.554842.
Train: 2018-08-06T00:59:17.510362: step 5119, loss 0.594889.
Train: 2018-08-06T00:59:17.754728: step 5120, loss 0.530805.
Test: 2018-08-06T00:59:19.019313: step 5120, loss 0.548343.
Train: 2018-08-06T00:59:19.261706: step 5121, loss 0.586882.
Train: 2018-08-06T00:59:19.518977: step 5122, loss 0.562834.
Train: 2018-08-06T00:59:19.768310: step 5123, loss 0.482655.
Train: 2018-08-06T00:59:20.010663: step 5124, loss 0.611012.
Train: 2018-08-06T00:59:20.269999: step 5125, loss 0.554747.
Train: 2018-08-06T00:59:20.519337: step 5126, loss 0.52252.
Train: 2018-08-06T00:59:20.770630: step 5127, loss 0.554682.
Train: 2018-08-06T00:59:21.024980: step 5128, loss 0.586972.
Train: 2018-08-06T00:59:21.268324: step 5129, loss 0.538426.
Train: 2018-08-06T00:59:21.529600: step 5130, loss 0.595117.
Test: 2018-08-06T00:59:22.785241: step 5130, loss 0.548188.
Train: 2018-08-06T00:59:23.022608: step 5131, loss 0.578905.
Train: 2018-08-06T00:59:23.273969: step 5132, loss 0.570788.
Train: 2018-08-06T00:59:23.521300: step 5133, loss 0.554547.
Train: 2018-08-06T00:59:23.718770: step 5134, loss 0.458686.
Train: 2018-08-06T00:59:23.980077: step 5135, loss 0.513754.
Train: 2018-08-06T00:59:24.229415: step 5136, loss 0.521711.
Train: 2018-08-06T00:59:24.476719: step 5137, loss 0.562549.
Train: 2018-08-06T00:59:24.724092: step 5138, loss 0.595482.
Train: 2018-08-06T00:59:24.969431: step 5139, loss 0.512917.
Train: 2018-08-06T00:59:25.219732: step 5140, loss 0.521015.
Test: 2018-08-06T00:59:26.512290: step 5140, loss 0.548157.
Train: 2018-08-06T00:59:26.751665: step 5141, loss 0.587407.
Train: 2018-08-06T00:59:26.997976: step 5142, loss 0.562422.
Train: 2018-08-06T00:59:27.247308: step 5143, loss 0.545676.
Train: 2018-08-06T00:59:27.492678: step 5144, loss 0.545622.
Train: 2018-08-06T00:59:27.738027: step 5145, loss 0.663255.
Train: 2018-08-06T00:59:27.979376: step 5146, loss 0.553979.
Train: 2018-08-06T00:59:28.226690: step 5147, loss 0.537177.
Train: 2018-08-06T00:59:28.473056: step 5148, loss 0.587599.
Train: 2018-08-06T00:59:28.721366: step 5149, loss 0.62119.
Train: 2018-08-06T00:59:28.967734: step 5150, loss 0.612705.
Test: 2018-08-06T00:59:30.240304: step 5150, loss 0.54845.
Train: 2018-08-06T00:59:30.478668: step 5151, loss 0.595851.
Train: 2018-08-06T00:59:30.732986: step 5152, loss 0.545773.
Train: 2018-08-06T00:59:30.979349: step 5153, loss 0.554147.
Train: 2018-08-06T00:59:31.223703: step 5154, loss 0.537617.
Train: 2018-08-06T00:59:31.468045: step 5155, loss 0.570756.
Train: 2018-08-06T00:59:31.727358: step 5156, loss 0.628535.
Train: 2018-08-06T00:59:31.969710: step 5157, loss 0.587212.
Train: 2018-08-06T00:59:32.225027: step 5158, loss 0.562564.
Train: 2018-08-06T00:59:32.469374: step 5159, loss 0.505399.
Train: 2018-08-06T00:59:32.718707: step 5160, loss 0.513668.
Test: 2018-08-06T00:59:33.995262: step 5160, loss 0.548998.
Train: 2018-08-06T00:59:34.231658: step 5161, loss 0.570775.
Train: 2018-08-06T00:59:34.485951: step 5162, loss 0.530017.
Train: 2018-08-06T00:59:34.733315: step 5163, loss 0.652314.
Train: 2018-08-06T00:59:34.977635: step 5164, loss 0.619616.
Train: 2018-08-06T00:59:35.221013: step 5165, loss 0.505866.
Train: 2018-08-06T00:59:35.471346: step 5166, loss 0.538374.
Train: 2018-08-06T00:59:35.717657: step 5167, loss 0.554595.
Train: 2018-08-06T00:59:35.965025: step 5168, loss 0.514112.
Train: 2018-08-06T00:59:36.217332: step 5169, loss 0.530266.
Train: 2018-08-06T00:59:36.467666: step 5170, loss 0.546432.
Test: 2018-08-06T00:59:37.724289: step 5170, loss 0.548501.
Train: 2018-08-06T00:59:37.966642: step 5171, loss 0.473173.
Train: 2018-08-06T00:59:38.225947: step 5172, loss 0.619759.
Train: 2018-08-06T00:59:38.472290: step 5173, loss 0.570766.
Train: 2018-08-06T00:59:38.720651: step 5174, loss 0.529788.
Train: 2018-08-06T00:59:38.969984: step 5175, loss 0.578973.
Train: 2018-08-06T00:59:39.218323: step 5176, loss 0.620114.
Train: 2018-08-06T00:59:39.465634: step 5177, loss 0.537853.
Train: 2018-08-06T00:59:39.713995: step 5178, loss 0.603681.
Train: 2018-08-06T00:59:39.957344: step 5179, loss 0.570758.
Train: 2018-08-06T00:59:40.216624: step 5180, loss 0.488544.
Test: 2018-08-06T00:59:41.479247: step 5180, loss 0.548404.
Train: 2018-08-06T00:59:41.717611: step 5181, loss 0.636599.
Train: 2018-08-06T00:59:41.964975: step 5182, loss 0.554311.
Train: 2018-08-06T00:59:42.225283: step 5183, loss 0.570759.
Train: 2018-08-06T00:59:42.468632: step 5184, loss 0.595398.
Train: 2018-08-06T00:59:42.712948: step 5185, loss 0.603565.
Train: 2018-08-06T00:59:42.959290: step 5186, loss 0.554402.
Train: 2018-08-06T00:59:43.210643: step 5187, loss 0.587103.
Train: 2018-08-06T00:59:43.464958: step 5188, loss 0.570776.
Train: 2018-08-06T00:59:43.712306: step 5189, loss 0.587042.
Train: 2018-08-06T00:59:43.970585: step 5190, loss 0.538359.
Test: 2018-08-06T00:59:45.241187: step 5190, loss 0.54956.
Train: 2018-08-06T00:59:45.480547: step 5191, loss 0.538422.
Train: 2018-08-06T00:59:45.726921: step 5192, loss 0.53037.
Train: 2018-08-06T00:59:45.986220: step 5193, loss 0.546545.
Train: 2018-08-06T00:59:46.234530: step 5194, loss 0.562713.
Train: 2018-08-06T00:59:46.482897: step 5195, loss 0.473689.
Train: 2018-08-06T00:59:46.741206: step 5196, loss 0.603242.
Train: 2018-08-06T00:59:46.992530: step 5197, loss 0.570786.
Train: 2018-08-06T00:59:47.238870: step 5198, loss 0.554519.
Train: 2018-08-06T00:59:47.500174: step 5199, loss 0.562638.
Train: 2018-08-06T00:59:47.745490: step 5200, loss 0.570776.
Test: 2018-08-06T00:59:49.042022: step 5200, loss 0.549.
Train: 2018-08-06T00:59:49.960754: step 5201, loss 0.570774.
Train: 2018-08-06T00:59:50.209089: step 5202, loss 0.570773.
Train: 2018-08-06T00:59:50.465429: step 5203, loss 0.595253.
Train: 2018-08-06T00:59:50.715763: step 5204, loss 0.529996.
Train: 2018-08-06T00:59:50.960107: step 5205, loss 0.505512.
Train: 2018-08-06T00:59:51.217422: step 5206, loss 0.562599.
Train: 2018-08-06T00:59:51.462737: step 5207, loss 0.554403.
Train: 2018-08-06T00:59:51.709103: step 5208, loss 0.595343.
Train: 2018-08-06T00:59:51.956416: step 5209, loss 0.505185.
Train: 2018-08-06T00:59:52.200763: step 5210, loss 0.546126.
Test: 2018-08-06T00:59:53.471364: step 5210, loss 0.54917.
Train: 2018-08-06T00:59:53.721726: step 5211, loss 0.546077.
Train: 2018-08-06T00:59:53.964046: step 5212, loss 0.546025.
Train: 2018-08-06T00:59:54.213412: step 5213, loss 0.529447.
Train: 2018-08-06T00:59:54.469725: step 5214, loss 0.579041.
Train: 2018-08-06T00:59:54.722051: step 5215, loss 0.520963.
Train: 2018-08-06T00:59:54.976339: step 5216, loss 0.587403.
Train: 2018-08-06T00:59:55.218691: step 5217, loss 0.595767.
Train: 2018-08-06T00:59:55.465075: step 5218, loss 0.587444.
Train: 2018-08-06T00:59:55.708413: step 5219, loss 0.520737.
Train: 2018-08-06T00:59:55.954749: step 5220, loss 0.537391.
Test: 2018-08-06T00:59:57.253250: step 5220, loss 0.548274.
Train: 2018-08-06T00:59:57.508594: step 5221, loss 0.554064.
Train: 2018-08-06T00:59:57.751916: step 5222, loss 0.595857.
Train: 2018-08-06T00:59:58.000252: step 5223, loss 0.554048.
Train: 2018-08-06T00:59:58.260588: step 5224, loss 0.579136.
Train: 2018-08-06T00:59:58.506897: step 5225, loss 0.554052.
Train: 2018-08-06T00:59:58.755234: step 5226, loss 0.620913.
Train: 2018-08-06T00:59:59.016566: step 5227, loss 0.554081.
Train: 2018-08-06T00:59:59.272850: step 5228, loss 0.537442.
Train: 2018-08-06T00:59:59.519190: step 5229, loss 0.520823.
Train: 2018-08-06T00:59:59.779495: step 5230, loss 0.579086.
Test: 2018-08-06T01:00:01.059072: step 5230, loss 0.549696.
Train: 2018-08-06T01:00:01.296437: step 5231, loss 0.579082.
Train: 2018-08-06T01:00:01.549789: step 5232, loss 0.545822.
Train: 2018-08-06T01:00:01.799124: step 5233, loss 0.562451.
Train: 2018-08-06T01:00:02.047464: step 5234, loss 0.52924.
Train: 2018-08-06T01:00:02.307761: step 5235, loss 0.570759.
Train: 2018-08-06T01:00:02.551082: step 5236, loss 0.520935.
Train: 2018-08-06T01:00:02.795429: step 5237, loss 0.57907.
Train: 2018-08-06T01:00:03.048760: step 5238, loss 0.545825.
Train: 2018-08-06T01:00:03.295112: step 5239, loss 0.545815.
Train: 2018-08-06T01:00:03.538468: step 5240, loss 0.545799.
Test: 2018-08-06T01:00:04.801064: step 5240, loss 0.548527.
Train: 2018-08-06T01:00:05.041422: step 5241, loss 0.579092.
Train: 2018-08-06T01:00:05.301756: step 5242, loss 0.479126.
Train: 2018-08-06T01:00:05.563060: step 5243, loss 0.604159.
Train: 2018-08-06T01:00:05.807398: step 5244, loss 0.595833.
Train: 2018-08-06T01:00:06.053714: step 5245, loss 0.637592.
Train: 2018-08-06T01:00:06.303074: step 5246, loss 0.570765.
Train: 2018-08-06T01:00:06.551410: step 5247, loss 0.562442.
Train: 2018-08-06T01:00:06.795756: step 5248, loss 0.587363.
Train: 2018-08-06T01:00:07.043094: step 5249, loss 0.554195.
Train: 2018-08-06T01:00:07.288444: step 5250, loss 0.562491.
Test: 2018-08-06T01:00:08.574973: step 5250, loss 0.548922.
Train: 2018-08-06T01:00:08.812369: step 5251, loss 0.48827.
Train: 2018-08-06T01:00:09.062720: step 5252, loss 0.480029.
Train: 2018-08-06T01:00:09.318011: step 5253, loss 0.521182.
Train: 2018-08-06T01:00:09.571343: step 5254, loss 0.595607.
Train: 2018-08-06T01:00:09.816683: step 5255, loss 0.537583.
Train: 2018-08-06T01:00:10.062022: step 5256, loss 0.529221.
Train: 2018-08-06T01:00:10.308337: step 5257, loss 0.579088.
Train: 2018-08-06T01:00:10.555675: step 5258, loss 0.554089.
Train: 2018-08-06T01:00:10.799026: step 5259, loss 0.537369.
Train: 2018-08-06T01:00:11.048357: step 5260, loss 0.620956.
Test: 2018-08-06T01:00:12.319956: step 5260, loss 0.54921.
Train: 2018-08-06T01:00:12.565302: step 5261, loss 0.637689.
Train: 2018-08-06T01:00:12.811643: step 5262, loss 0.629201.
Train: 2018-08-06T01:00:13.066959: step 5263, loss 0.587391.
Train: 2018-08-06T01:00:13.317290: step 5264, loss 0.636991.
Train: 2018-08-06T01:00:13.563632: step 5265, loss 0.562487.
Train: 2018-08-06T01:00:13.810970: step 5266, loss 0.603542.
Train: 2018-08-06T01:00:14.061304: step 5267, loss 0.538591.
Train: 2018-08-06T01:00:14.303678: step 5268, loss 0.595501.
Train: 2018-08-06T01:00:14.560989: step 5269, loss 0.587046.
Train: 2018-08-06T01:00:14.816312: step 5270, loss 0.530509.
Test: 2018-08-06T01:00:16.097853: step 5270, loss 0.548975.
Train: 2018-08-06T01:00:16.336216: step 5271, loss 0.546691.
Train: 2018-08-06T01:00:16.593527: step 5272, loss 0.514659.
Train: 2018-08-06T01:00:16.846851: step 5273, loss 0.594914.
Train: 2018-08-06T01:00:17.097182: step 5274, loss 0.578833.
Train: 2018-08-06T01:00:17.357486: step 5275, loss 0.507045.
Train: 2018-08-06T01:00:17.612828: step 5276, loss 0.570922.
Train: 2018-08-06T01:00:17.868120: step 5277, loss 0.562933.
Train: 2018-08-06T01:00:18.125457: step 5278, loss 0.610855.
Train: 2018-08-06T01:00:18.382743: step 5279, loss 0.554987.
Train: 2018-08-06T01:00:18.629129: step 5280, loss 0.57088.
Test: 2018-08-06T01:00:19.907672: step 5280, loss 0.54978.
Train: 2018-08-06T01:00:20.147050: step 5281, loss 0.546968.
Train: 2018-08-06T01:00:20.395389: step 5282, loss 0.554937.
Train: 2018-08-06T01:00:20.639751: step 5283, loss 0.570878.
Train: 2018-08-06T01:00:20.894052: step 5284, loss 0.55492.
Train: 2018-08-06T01:00:21.083520: step 5285, loss 0.443696.
Train: 2018-08-06T01:00:21.330858: step 5286, loss 0.578866.
Train: 2018-08-06T01:00:21.577201: step 5287, loss 0.578847.
Train: 2018-08-06T01:00:21.820552: step 5288, loss 0.506494.
Train: 2018-08-06T01:00:22.062929: step 5289, loss 0.578952.
Train: 2018-08-06T01:00:22.313301: step 5290, loss 0.611291.
Test: 2018-08-06T01:00:23.585897: step 5290, loss 0.548043.
Train: 2018-08-06T01:00:23.835230: step 5291, loss 0.546508.
Train: 2018-08-06T01:00:24.078604: step 5292, loss 0.546419.
Train: 2018-08-06T01:00:24.323923: step 5293, loss 0.619606.
Train: 2018-08-06T01:00:24.566301: step 5294, loss 0.546354.
Train: 2018-08-06T01:00:24.822590: step 5295, loss 0.546339.
Train: 2018-08-06T01:00:25.080929: step 5296, loss 0.603395.
Train: 2018-08-06T01:00:25.330256: step 5297, loss 0.53.
Train: 2018-08-06T01:00:25.576604: step 5298, loss 0.529972.
Train: 2018-08-06T01:00:25.822946: step 5299, loss 0.562598.
Train: 2018-08-06T01:00:26.068283: step 5300, loss 0.578949.
Test: 2018-08-06T01:00:27.341851: step 5300, loss 0.547962.
Train: 2018-08-06T01:00:28.238434: step 5301, loss 0.595326.
Train: 2018-08-06T01:00:28.488801: step 5302, loss 0.538019.
Train: 2018-08-06T01:00:28.741119: step 5303, loss 0.636283.
Train: 2018-08-06T01:00:28.989450: step 5304, loss 0.55441.
Train: 2018-08-06T01:00:29.245740: step 5305, loss 0.578938.
Train: 2018-08-06T01:00:29.496117: step 5306, loss 0.61973.
Train: 2018-08-06T01:00:29.744431: step 5307, loss 0.546365.
Train: 2018-08-06T01:00:29.991745: step 5308, loss 0.619524.
Train: 2018-08-06T01:00:30.236115: step 5309, loss 0.522208.
Train: 2018-08-06T01:00:30.487449: step 5310, loss 0.554637.
Test: 2018-08-06T01:00:31.769989: step 5310, loss 0.548326.
Train: 2018-08-06T01:00:32.010371: step 5311, loss 0.570811.
Train: 2018-08-06T01:00:32.254698: step 5312, loss 0.55469.
Train: 2018-08-06T01:00:32.514030: step 5313, loss 0.586934.
Train: 2018-08-06T01:00:32.759343: step 5314, loss 0.562778.
Train: 2018-08-06T01:00:33.007678: step 5315, loss 0.619048.
Train: 2018-08-06T01:00:33.256040: step 5316, loss 0.602922.
Train: 2018-08-06T01:00:33.503379: step 5317, loss 0.53892.
Train: 2018-08-06T01:00:33.746701: step 5318, loss 0.578856.
Train: 2018-08-06T01:00:33.995072: step 5319, loss 0.52315.
Train: 2018-08-06T01:00:34.243402: step 5320, loss 0.547049.
Test: 2018-08-06T01:00:35.508989: step 5320, loss 0.550043.
Train: 2018-08-06T01:00:35.748349: step 5321, loss 0.562951.
Train: 2018-08-06T01:00:36.001678: step 5322, loss 0.555006.
Train: 2018-08-06T01:00:36.249045: step 5323, loss 0.618638.
Train: 2018-08-06T01:00:36.495351: step 5324, loss 0.555025.
Train: 2018-08-06T01:00:36.741693: step 5325, loss 0.578864.
Train: 2018-08-06T01:00:36.985042: step 5326, loss 0.634401.
Train: 2018-08-06T01:00:37.238365: step 5327, loss 0.523447.
Train: 2018-08-06T01:00:37.483739: step 5328, loss 0.523499.
Train: 2018-08-06T01:00:37.728056: step 5329, loss 0.547215.
Train: 2018-08-06T01:00:37.973400: step 5330, loss 0.563022.
Test: 2018-08-06T01:00:39.248987: step 5330, loss 0.549408.
Train: 2018-08-06T01:00:39.499318: step 5331, loss 0.64226.
Train: 2018-08-06T01:00:39.752667: step 5332, loss 0.610529.
Train: 2018-08-06T01:00:39.997011: step 5333, loss 0.515648.
Train: 2018-08-06T01:00:40.242332: step 5334, loss 0.507773.
Train: 2018-08-06T01:00:40.492692: step 5335, loss 0.618403.
Train: 2018-08-06T01:00:40.742020: step 5336, loss 0.570955.
Train: 2018-08-06T01:00:40.983356: step 5337, loss 0.507701.
Train: 2018-08-06T01:00:41.232711: step 5338, loss 0.491763.
Train: 2018-08-06T01:00:41.522907: step 5339, loss 0.602686.
Train: 2018-08-06T01:00:41.773237: step 5340, loss 0.523146.
Test: 2018-08-06T01:00:43.045833: step 5340, loss 0.548829.
Train: 2018-08-06T01:00:43.283199: step 5341, loss 0.570879.
Train: 2018-08-06T01:00:43.527573: step 5342, loss 0.546851.
Train: 2018-08-06T01:00:43.775912: step 5343, loss 0.530718.
Train: 2018-08-06T01:00:44.022222: step 5344, loss 0.55472.
Train: 2018-08-06T01:00:44.274547: step 5345, loss 0.554651.
Train: 2018-08-06T01:00:44.520913: step 5346, loss 0.530274.
Train: 2018-08-06T01:00:44.772241: step 5347, loss 0.587048.
Train: 2018-08-06T01:00:45.021561: step 5348, loss 0.636019.
Train: 2018-08-06T01:00:45.269914: step 5349, loss 0.570771.
Train: 2018-08-06T01:00:45.515254: step 5350, loss 0.554441.
Test: 2018-08-06T01:00:46.798796: step 5350, loss 0.549143.
Train: 2018-08-06T01:00:47.038158: step 5351, loss 0.587108.
Train: 2018-08-06T01:00:47.286493: step 5352, loss 0.578939.
Train: 2018-08-06T01:00:47.532859: step 5353, loss 0.529943.
Train: 2018-08-06T01:00:47.777211: step 5354, loss 0.57077.
Train: 2018-08-06T01:00:48.022552: step 5355, loss 0.529922.
Train: 2018-08-06T01:00:48.266901: step 5356, loss 0.562591.
Train: 2018-08-06T01:00:48.518224: step 5357, loss 0.48894.
Train: 2018-08-06T01:00:48.767542: step 5358, loss 0.570762.
Train: 2018-08-06T01:00:49.012901: step 5359, loss 0.505015.
Train: 2018-08-06T01:00:49.260240: step 5360, loss 0.611971.
Test: 2018-08-06T01:00:50.550762: step 5360, loss 0.550044.
Train: 2018-08-06T01:00:50.789125: step 5361, loss 0.54599.
Train: 2018-08-06T01:00:51.040463: step 5362, loss 0.562487.
Train: 2018-08-06T01:00:51.298761: step 5363, loss 0.562476.
Train: 2018-08-06T01:00:51.543112: step 5364, loss 0.529301.
Train: 2018-08-06T01:00:51.802440: step 5365, loss 0.612289.
Train: 2018-08-06T01:00:52.045790: step 5366, loss 0.554142.
Train: 2018-08-06T01:00:52.292105: step 5367, loss 0.595697.
Train: 2018-08-06T01:00:52.535480: step 5368, loss 0.529218.
Train: 2018-08-06T01:00:52.783792: step 5369, loss 0.628931.
Train: 2018-08-06T01:00:53.037113: step 5370, loss 0.587355.
Test: 2018-08-06T01:00:54.306717: step 5370, loss 0.549596.
Train: 2018-08-06T01:00:54.546108: step 5371, loss 0.587322.
Train: 2018-08-06T01:00:54.793442: step 5372, loss 0.545969.
Train: 2018-08-06T01:00:55.038760: step 5373, loss 0.587251.
Train: 2018-08-06T01:00:55.282114: step 5374, loss 0.56253.
Train: 2018-08-06T01:00:55.530445: step 5375, loss 0.455808.
Train: 2018-08-06T01:00:55.773795: step 5376, loss 0.603621.
Train: 2018-08-06T01:00:56.026144: step 5377, loss 0.521493.
Train: 2018-08-06T01:00:56.273458: step 5378, loss 0.55433.
Train: 2018-08-06T01:00:56.518833: step 5379, loss 0.636516.
Train: 2018-08-06T01:00:56.773121: step 5380, loss 0.546132.
Test: 2018-08-06T01:00:58.035744: step 5380, loss 0.547537.
Train: 2018-08-06T01:00:58.273110: step 5381, loss 0.611776.
Train: 2018-08-06T01:00:58.518465: step 5382, loss 0.570765.
Train: 2018-08-06T01:00:58.759808: step 5383, loss 0.546253.
Train: 2018-08-06T01:00:59.020142: step 5384, loss 0.546289.
Train: 2018-08-06T01:00:59.268451: step 5385, loss 0.578928.
Train: 2018-08-06T01:00:59.514789: step 5386, loss 0.587067.
Train: 2018-08-06T01:00:59.770108: step 5387, loss 0.505728.
Train: 2018-08-06T01:01:00.016447: step 5388, loss 0.546389.
Train: 2018-08-06T01:01:00.266779: step 5389, loss 0.619585.
Train: 2018-08-06T01:01:00.508163: step 5390, loss 0.505782.
Test: 2018-08-06T01:01:01.774746: step 5390, loss 0.548704.
Train: 2018-08-06T01:01:02.066965: step 5391, loss 0.570783.
Train: 2018-08-06T01:01:02.316298: step 5392, loss 0.530129.
Train: 2018-08-06T01:01:02.559675: step 5393, loss 0.554502.
Train: 2018-08-06T01:01:02.805016: step 5394, loss 0.603367.
Train: 2018-08-06T01:01:03.068287: step 5395, loss 0.52189.
Train: 2018-08-06T01:01:03.316647: step 5396, loss 0.538151.
Train: 2018-08-06T01:01:03.570968: step 5397, loss 0.578937.
Train: 2018-08-06T01:01:03.824296: step 5398, loss 0.529892.
Train: 2018-08-06T01:01:04.069608: step 5399, loss 0.619893.
Train: 2018-08-06T01:01:04.313954: step 5400, loss 0.554387.
Test: 2018-08-06T01:01:05.578572: step 5400, loss 0.548518.
Train: 2018-08-06T01:01:06.447576: step 5401, loss 0.587145.
Train: 2018-08-06T01:01:06.695909: step 5402, loss 0.603513.
Train: 2018-08-06T01:01:06.942221: step 5403, loss 0.578943.
Train: 2018-08-06T01:01:07.188563: step 5404, loss 0.546282.
Train: 2018-08-06T01:01:07.439917: step 5405, loss 0.46477.
Train: 2018-08-06T01:01:07.689224: step 5406, loss 0.578936.
Train: 2018-08-06T01:01:07.935598: step 5407, loss 0.538081.
Train: 2018-08-06T01:01:08.181908: step 5408, loss 0.538036.
Train: 2018-08-06T01:01:08.426254: step 5409, loss 0.480599.
Train: 2018-08-06T01:01:08.678609: step 5410, loss 0.529633.
Test: 2018-08-06T01:01:09.967132: step 5410, loss 0.549091.
Train: 2018-08-06T01:01:10.202528: step 5411, loss 0.529474.
Train: 2018-08-06T01:01:10.449842: step 5412, loss 0.554176.
Train: 2018-08-06T01:01:10.701171: step 5413, loss 0.579085.
Train: 2018-08-06T01:01:10.945516: step 5414, loss 0.529036.
Train: 2018-08-06T01:01:11.193879: step 5415, loss 0.545653.
Train: 2018-08-06T01:01:11.436204: step 5416, loss 0.570786.
Train: 2018-08-06T01:01:11.683543: step 5417, loss 0.537111.
Train: 2018-08-06T01:01:11.928886: step 5418, loss 0.553923.
Train: 2018-08-06T01:01:12.179243: step 5419, loss 0.545426.
Train: 2018-08-06T01:01:12.422566: step 5420, loss 0.562351.
Test: 2018-08-06T01:01:13.696159: step 5420, loss 0.548344.
Train: 2018-08-06T01:01:13.932527: step 5421, loss 0.502833.
Train: 2018-08-06T01:01:14.187869: step 5422, loss 0.570873.
Train: 2018-08-06T01:01:14.443163: step 5423, loss 0.562347.
Train: 2018-08-06T01:01:14.700475: step 5424, loss 0.562349.
Train: 2018-08-06T01:01:14.956790: step 5425, loss 0.528049.
Train: 2018-08-06T01:01:15.201136: step 5426, loss 0.53657.
Train: 2018-08-06T01:01:15.449471: step 5427, loss 0.527969.
Train: 2018-08-06T01:01:15.696811: step 5428, loss 0.536496.
Train: 2018-08-06T01:01:15.943152: step 5429, loss 0.631455.
Train: 2018-08-06T01:01:16.194509: step 5430, loss 0.570983.
Test: 2018-08-06T01:01:17.479042: step 5430, loss 0.547332.
Train: 2018-08-06T01:01:17.722426: step 5431, loss 0.570951.
Train: 2018-08-06T01:01:17.968733: step 5432, loss 0.596818.
Train: 2018-08-06T01:01:18.219095: step 5433, loss 0.57094.
Train: 2018-08-06T01:01:18.473415: step 5434, loss 0.553748.
Train: 2018-08-06T01:01:18.723715: step 5435, loss 0.562335.
Train: 2018-08-06T01:01:18.912241: step 5436, loss 0.635329.
Train: 2018-08-06T01:01:19.160577: step 5437, loss 0.485632.
Train: 2018-08-06T01:01:19.407911: step 5438, loss 0.596384.
Train: 2018-08-06T01:01:19.654225: step 5439, loss 0.51992.
Train: 2018-08-06T01:01:19.901590: step 5440, loss 0.503047.
Test: 2018-08-06T01:01:21.175158: step 5440, loss 0.548209.
Train: 2018-08-06T01:01:21.409562: step 5441, loss 0.553887.
Train: 2018-08-06T01:01:21.656895: step 5442, loss 0.553893.
Train: 2018-08-06T01:01:21.905206: step 5443, loss 0.579287.
Train: 2018-08-06T01:01:22.156535: step 5444, loss 0.621565.
Train: 2018-08-06T01:01:22.402876: step 5445, loss 0.520172.
Train: 2018-08-06T01:01:22.649247: step 5446, loss 0.545515.
Train: 2018-08-06T01:01:22.897552: step 5447, loss 0.553956.
Train: 2018-08-06T01:01:23.156889: step 5448, loss 0.562379.
Train: 2018-08-06T01:01:23.405194: step 5449, loss 0.511948.
Train: 2018-08-06T01:01:23.647546: step 5450, loss 0.596012.
Test: 2018-08-06T01:01:24.916153: step 5450, loss 0.548936.
Train: 2018-08-06T01:01:25.153518: step 5451, loss 0.59599.
Train: 2018-08-06T01:01:25.398893: step 5452, loss 0.621107.
Train: 2018-08-06T01:01:25.645205: step 5453, loss 0.545682.
Train: 2018-08-06T01:01:25.904512: step 5454, loss 0.595799.
Train: 2018-08-06T01:01:26.156837: step 5455, loss 0.587399.
Train: 2018-08-06T01:01:26.401213: step 5456, loss 0.579046.
Train: 2018-08-06T01:01:26.653540: step 5457, loss 0.570757.
Train: 2018-08-06T01:01:26.900846: step 5458, loss 0.554293.
Train: 2018-08-06T01:01:27.146204: step 5459, loss 0.537929.
Train: 2018-08-06T01:01:27.391534: step 5460, loss 0.505243.
Test: 2018-08-06T01:01:28.662135: step 5460, loss 0.548344.
Train: 2018-08-06T01:01:28.902494: step 5461, loss 0.578952.
Train: 2018-08-06T01:01:29.151856: step 5462, loss 0.570769.
Train: 2018-08-06T01:01:29.396204: step 5463, loss 0.562601.
Train: 2018-08-06T01:01:29.645505: step 5464, loss 0.611577.
Train: 2018-08-06T01:01:29.891847: step 5465, loss 0.513769.
Train: 2018-08-06T01:01:30.137219: step 5466, loss 0.513809.
Train: 2018-08-06T01:01:30.382534: step 5467, loss 0.635925.
Train: 2018-08-06T01:01:30.632891: step 5468, loss 0.570781.
Train: 2018-08-06T01:01:30.884227: step 5469, loss 0.538296.
Train: 2018-08-06T01:01:31.128540: step 5470, loss 0.538315.
Test: 2018-08-06T01:01:32.396149: step 5470, loss 0.548922.
Train: 2018-08-06T01:01:32.637537: step 5471, loss 0.538311.
Train: 2018-08-06T01:01:32.881875: step 5472, loss 0.62766.
Train: 2018-08-06T01:01:33.127196: step 5473, loss 0.578906.
Train: 2018-08-06T01:01:33.381515: step 5474, loss 0.578901.
Train: 2018-08-06T01:01:33.633870: step 5475, loss 0.457452.
Train: 2018-08-06T01:01:33.881211: step 5476, loss 0.55458.
Train: 2018-08-06T01:01:34.129528: step 5477, loss 0.52208.
Train: 2018-08-06T01:01:34.373862: step 5478, loss 0.570781.
Train: 2018-08-06T01:01:34.639183: step 5479, loss 0.595229.
Train: 2018-08-06T01:01:34.884496: step 5480, loss 0.587089.
Test: 2018-08-06T01:01:36.139139: step 5480, loss 0.549736.
Train: 2018-08-06T01:01:36.376529: step 5481, loss 0.578932.
Train: 2018-08-06T01:01:36.620852: step 5482, loss 0.546298.
Train: 2018-08-06T01:01:36.866228: step 5483, loss 0.570772.
Train: 2018-08-06T01:01:37.126527: step 5484, loss 0.587092.
Train: 2018-08-06T01:01:37.371843: step 5485, loss 0.595238.
Train: 2018-08-06T01:01:37.629156: step 5486, loss 0.481195.
Train: 2018-08-06T01:01:37.873527: step 5487, loss 0.562625.
Train: 2018-08-06T01:01:38.122860: step 5488, loss 0.59524.
Train: 2018-08-06T01:01:38.376157: step 5489, loss 0.56262.
Train: 2018-08-06T01:01:38.619505: step 5490, loss 0.578928.
Test: 2018-08-06T01:01:39.882129: step 5490, loss 0.548643.
Train: 2018-08-06T01:01:40.120523: step 5491, loss 0.53818.
Train: 2018-08-06T01:01:40.375835: step 5492, loss 0.538176.
Train: 2018-08-06T01:01:40.620155: step 5493, loss 0.619707.
Train: 2018-08-06T01:01:40.868492: step 5494, loss 0.521879.
Train: 2018-08-06T01:01:41.115831: step 5495, loss 0.505559.
Train: 2018-08-06T01:01:41.378129: step 5496, loss 0.55444.
Train: 2018-08-06T01:01:41.622474: step 5497, loss 0.636199.
Train: 2018-08-06T01:01:41.881807: step 5498, loss 0.660712.
Train: 2018-08-06T01:01:42.141089: step 5499, loss 0.529995.
Train: 2018-08-06T01:01:42.386431: step 5500, loss 0.546349.
Test: 2018-08-06T01:01:43.670996: step 5500, loss 0.548311.
Train: 2018-08-06T01:01:44.549083: step 5501, loss 0.63586.
Train: 2018-08-06T01:01:44.797389: step 5502, loss 0.578904.
Train: 2018-08-06T01:01:45.044727: step 5503, loss 0.603166.
Train: 2018-08-06T01:01:45.289074: step 5504, loss 0.55469.
Train: 2018-08-06T01:01:45.548408: step 5505, loss 0.530629.
Train: 2018-08-06T01:01:45.798711: step 5506, loss 0.586896.
Train: 2018-08-06T01:01:46.045083: step 5507, loss 0.546823.
Train: 2018-08-06T01:01:46.294384: step 5508, loss 0.562863.
Train: 2018-08-06T01:01:46.540756: step 5509, loss 0.602835.
Train: 2018-08-06T01:01:46.788095: step 5510, loss 0.586836.
Test: 2018-08-06T01:01:48.060661: step 5510, loss 0.550749.
Train: 2018-08-06T01:01:48.301019: step 5511, loss 0.523149.
Train: 2018-08-06T01:01:48.560325: step 5512, loss 0.531147.
Train: 2018-08-06T01:01:48.804703: step 5513, loss 0.507279.
Train: 2018-08-06T01:01:49.052041: step 5514, loss 0.475293.
Train: 2018-08-06T01:01:49.297384: step 5515, loss 0.546875.
Train: 2018-08-06T01:01:49.545689: step 5516, loss 0.530699.
Train: 2018-08-06T01:01:49.793028: step 5517, loss 0.55469.
Train: 2018-08-06T01:01:50.040367: step 5518, loss 0.522213.
Train: 2018-08-06T01:01:50.287736: step 5519, loss 0.587053.
Train: 2018-08-06T01:01:50.537070: step 5520, loss 0.505441.
Test: 2018-08-06T01:01:51.808638: step 5520, loss 0.549988.
Train: 2018-08-06T01:01:52.046033: step 5521, loss 0.578966.
Train: 2018-08-06T01:01:52.298359: step 5522, loss 0.496641.
Train: 2018-08-06T01:01:52.552649: step 5523, loss 0.570757.
Train: 2018-08-06T01:01:52.801009: step 5524, loss 0.587378.
Train: 2018-08-06T01:01:53.061288: step 5525, loss 0.570765.
Train: 2018-08-06T01:01:53.307629: step 5526, loss 0.520648.
Train: 2018-08-06T01:01:53.551014: step 5527, loss 0.61267.
Train: 2018-08-06T01:01:53.796358: step 5528, loss 0.562392.
Train: 2018-08-06T01:01:54.044663: step 5529, loss 0.595984.
Train: 2018-08-06T01:01:54.292027: step 5530, loss 0.553987.
Test: 2018-08-06T01:01:55.558614: step 5530, loss 0.54855.
Train: 2018-08-06T01:01:55.795007: step 5531, loss 0.553985.
Train: 2018-08-06T01:01:56.039329: step 5532, loss 0.520376.
Train: 2018-08-06T01:01:56.297638: step 5533, loss 0.58761.
Train: 2018-08-06T01:01:56.546000: step 5534, loss 0.604432.
Train: 2018-08-06T01:01:56.794325: step 5535, loss 0.545582.
Train: 2018-08-06T01:01:57.036662: step 5536, loss 0.637952.
Train: 2018-08-06T01:01:57.283003: step 5537, loss 0.595902.
Train: 2018-08-06T01:01:57.542310: step 5538, loss 0.529024.
Train: 2018-08-06T01:01:57.786682: step 5539, loss 0.562433.
Train: 2018-08-06T01:01:58.034020: step 5540, loss 0.504254.
Test: 2018-08-06T01:01:59.293625: step 5540, loss 0.548003.
Train: 2018-08-06T01:01:59.528995: step 5541, loss 0.57076.
Train: 2018-08-06T01:01:59.777332: step 5542, loss 0.496052.
Train: 2018-08-06T01:02:00.019683: step 5543, loss 0.512616.
Train: 2018-08-06T01:02:00.271043: step 5544, loss 0.595721.
Train: 2018-08-06T01:02:00.516356: step 5545, loss 0.604062.
Train: 2018-08-06T01:02:00.761725: step 5546, loss 0.629005.
Train: 2018-08-06T01:02:01.004076: step 5547, loss 0.54585.
Train: 2018-08-06T01:02:01.255430: step 5548, loss 0.587337.
Train: 2018-08-06T01:02:01.497761: step 5549, loss 0.628663.
Train: 2018-08-06T01:02:01.743075: step 5550, loss 0.546026.
Test: 2018-08-06T01:02:03.028637: step 5550, loss 0.549946.
Train: 2018-08-06T01:02:03.264008: step 5551, loss 0.587199.
Train: 2018-08-06T01:02:03.508379: step 5552, loss 0.578957.
Train: 2018-08-06T01:02:03.758685: step 5553, loss 0.529935.
Train: 2018-08-06T01:02:04.019019: step 5554, loss 0.595223.
Train: 2018-08-06T01:02:04.273339: step 5555, loss 0.546405.
Train: 2018-08-06T01:02:04.518653: step 5556, loss 0.522134.
Train: 2018-08-06T01:02:04.769013: step 5557, loss 0.465461.
Train: 2018-08-06T01:02:05.016322: step 5558, loss 0.603251.
Train: 2018-08-06T01:02:05.264658: step 5559, loss 0.651982.
Train: 2018-08-06T01:02:05.512020: step 5560, loss 0.570794.
Test: 2018-08-06T01:02:06.767637: step 5560, loss 0.549194.
Train: 2018-08-06T01:02:07.014004: step 5561, loss 0.570801.
Train: 2018-08-06T01:02:07.268299: step 5562, loss 0.465787.
Train: 2018-08-06T01:02:07.511648: step 5563, loss 0.554633.
Train: 2018-08-06T01:02:07.760982: step 5564, loss 0.546517.
Train: 2018-08-06T01:02:08.007353: step 5565, loss 0.578899.
Train: 2018-08-06T01:02:08.263662: step 5566, loss 0.554567.
Train: 2018-08-06T01:02:08.509007: step 5567, loss 0.554547.
Train: 2018-08-06T01:02:08.753358: step 5568, loss 0.497619.
Train: 2018-08-06T01:02:08.998671: step 5569, loss 0.587076.
Train: 2018-08-06T01:02:09.244041: step 5570, loss 0.529949.
Test: 2018-08-06T01:02:10.518606: step 5570, loss 0.549104.
Train: 2018-08-06T01:02:10.769934: step 5571, loss 0.595315.
Train: 2018-08-06T01:02:11.013283: step 5572, loss 0.562569.
Train: 2018-08-06T01:02:11.257630: step 5573, loss 0.636387.
Train: 2018-08-06T01:02:11.503998: step 5574, loss 0.611748.
Train: 2018-08-06T01:02:11.747321: step 5575, loss 0.644403.
Train: 2018-08-06T01:02:12.001670: step 5576, loss 0.603384.
Train: 2018-08-06T01:02:12.253996: step 5577, loss 0.473372.
Train: 2018-08-06T01:02:12.501329: step 5578, loss 0.538376.
Train: 2018-08-06T01:02:12.746674: step 5579, loss 0.595091.
Train: 2018-08-06T01:02:12.995982: step 5580, loss 0.570805.
Test: 2018-08-06T01:02:14.274561: step 5580, loss 0.549629.
Train: 2018-08-06T01:02:14.509931: step 5581, loss 0.538521.
Train: 2018-08-06T01:02:14.751287: step 5582, loss 0.554682.
Train: 2018-08-06T01:02:14.997627: step 5583, loss 0.611134.
Train: 2018-08-06T01:02:15.256960: step 5584, loss 0.514467.
Train: 2018-08-06T01:02:15.509260: step 5585, loss 0.562775.
Train: 2018-08-06T01:02:15.755631: step 5586, loss 0.514475.
Train: 2018-08-06T01:02:15.948086: step 5587, loss 0.545565.
Train: 2018-08-06T01:02:16.197419: step 5588, loss 0.603103.
Train: 2018-08-06T01:02:16.441765: step 5589, loss 0.586963.
Train: 2018-08-06T01:02:16.684143: step 5590, loss 0.578886.
Test: 2018-08-06T01:02:17.966687: step 5590, loss 0.549629.
Train: 2018-08-06T01:02:18.206072: step 5591, loss 0.546595.
Train: 2018-08-06T01:02:18.465379: step 5592, loss 0.586958.
Train: 2018-08-06T01:02:18.714687: step 5593, loss 0.51433.
Train: 2018-08-06T01:02:18.960056: step 5594, loss 0.530431.
Train: 2018-08-06T01:02:19.204378: step 5595, loss 0.514184.
Train: 2018-08-06T01:02:19.458697: step 5596, loss 0.505904.
Train: 2018-08-06T01:02:19.705064: step 5597, loss 0.513744.
Train: 2018-08-06T01:02:19.949386: step 5598, loss 0.546131.
Train: 2018-08-06T01:02:20.199715: step 5599, loss 0.595415.
Train: 2018-08-06T01:02:20.442084: step 5600, loss 0.587091.
Test: 2018-08-06T01:02:21.720648: step 5600, loss 0.548635.
Train: 2018-08-06T01:02:22.669260: step 5601, loss 0.521043.
Train: 2018-08-06T01:02:22.915601: step 5602, loss 0.55303.
Train: 2018-08-06T01:02:23.164962: step 5603, loss 0.606681.
Train: 2018-08-06T01:02:23.413271: step 5604, loss 0.603496.
Train: 2018-08-06T01:02:23.660633: step 5605, loss 0.537699.
Train: 2018-08-06T01:02:23.906949: step 5606, loss 0.520863.
Train: 2018-08-06T01:02:24.157306: step 5607, loss 0.512198.
Train: 2018-08-06T01:02:24.412628: step 5608, loss 0.629137.
Train: 2018-08-06T01:02:24.670932: step 5609, loss 0.545361.
Train: 2018-08-06T01:02:24.927221: step 5610, loss 0.570975.
Test: 2018-08-06T01:02:26.178873: step 5610, loss 0.549011.
Train: 2018-08-06T01:02:26.418234: step 5611, loss 0.587655.
Train: 2018-08-06T01:02:26.662580: step 5612, loss 0.579126.
Train: 2018-08-06T01:02:26.921885: step 5613, loss 0.537513.
Train: 2018-08-06T01:02:27.174244: step 5614, loss 0.520902.
Train: 2018-08-06T01:02:27.419582: step 5615, loss 0.562448.
Train: 2018-08-06T01:02:27.678891: step 5616, loss 0.512571.
Train: 2018-08-06T01:02:27.926225: step 5617, loss 0.579096.
Train: 2018-08-06T01:02:28.170548: step 5618, loss 0.645706.
Train: 2018-08-06T01:02:28.415890: step 5619, loss 0.595704.
Train: 2018-08-06T01:02:28.667245: step 5620, loss 0.537572.
Test: 2018-08-06T01:02:29.930839: step 5620, loss 0.54845.
Train: 2018-08-06T01:02:30.169202: step 5621, loss 0.603881.
Train: 2018-08-06T01:02:30.416575: step 5622, loss 0.578999.
Train: 2018-08-06T01:02:30.660920: step 5623, loss 0.587248.
Train: 2018-08-06T01:02:30.909223: step 5624, loss 0.521507.
Train: 2018-08-06T01:02:31.152572: step 5625, loss 0.562528.
Train: 2018-08-06T01:02:31.399910: step 5626, loss 0.54622.
Train: 2018-08-06T01:02:31.642264: step 5627, loss 0.521684.
Train: 2018-08-06T01:02:31.889627: step 5628, loss 0.56257.
Train: 2018-08-06T01:02:32.134977: step 5629, loss 0.48892.
Train: 2018-08-06T01:02:32.388294: step 5630, loss 0.61174.
Test: 2018-08-06T01:02:33.658869: step 5630, loss 0.548679.
Train: 2018-08-06T01:02:33.898230: step 5631, loss 0.521569.
Train: 2018-08-06T01:02:34.144597: step 5632, loss 0.603604.
Train: 2018-08-06T01:02:34.399913: step 5633, loss 0.579038.
Train: 2018-08-06T01:02:34.659207: step 5634, loss 0.537948.
Train: 2018-08-06T01:02:34.909556: step 5635, loss 0.579003.
Train: 2018-08-06T01:02:35.160886: step 5636, loss 0.603582.
Train: 2018-08-06T01:02:35.407225: step 5637, loss 0.546393.
Train: 2018-08-06T01:02:35.653535: step 5638, loss 0.603274.
Train: 2018-08-06T01:02:35.897909: step 5639, loss 0.538639.
Train: 2018-08-06T01:02:36.143227: step 5640, loss 0.554689.
Test: 2018-08-06T01:02:37.410835: step 5640, loss 0.54778.
Train: 2018-08-06T01:02:37.650195: step 5641, loss 0.522037.
Train: 2018-08-06T01:02:37.896567: step 5642, loss 0.546241.
Train: 2018-08-06T01:02:38.148886: step 5643, loss 0.529861.
Train: 2018-08-06T01:02:38.392244: step 5644, loss 0.488822.
Train: 2018-08-06T01:02:38.638552: step 5645, loss 0.488576.
Train: 2018-08-06T01:02:38.888907: step 5646, loss 0.587271.
Train: 2018-08-06T01:02:39.138248: step 5647, loss 0.521053.
Train: 2018-08-06T01:02:39.395559: step 5648, loss 0.60403.
Train: 2018-08-06T01:02:39.652841: step 5649, loss 0.612453.
Train: 2018-08-06T01:02:39.899211: step 5650, loss 0.620839.
Test: 2018-08-06T01:02:41.173771: step 5650, loss 0.548304.
Train: 2018-08-06T01:02:41.412135: step 5651, loss 0.512383.
Train: 2018-08-06T01:02:41.711849: step 5652, loss 0.545733.
Train: 2018-08-06T01:02:41.965170: step 5653, loss 0.520666.
Train: 2018-08-06T01:02:42.209517: step 5654, loss 0.587501.
Train: 2018-08-06T01:02:42.453864: step 5655, loss 0.579143.
Train: 2018-08-06T01:02:42.699207: step 5656, loss 0.579144.
Train: 2018-08-06T01:02:42.959511: step 5657, loss 0.604219.
Train: 2018-08-06T01:02:43.203857: step 5658, loss 0.529046.
Train: 2018-08-06T01:02:43.451197: step 5659, loss 0.595793.
Train: 2018-08-06T01:02:43.701526: step 5660, loss 0.579098.
Test: 2018-08-06T01:02:44.992074: step 5660, loss 0.547791.
Train: 2018-08-06T01:02:45.230436: step 5661, loss 0.62065.
Train: 2018-08-06T01:02:45.479805: step 5662, loss 0.52931.
Train: 2018-08-06T01:02:45.726142: step 5663, loss 0.512851.
Train: 2018-08-06T01:02:45.970459: step 5664, loss 0.504626.
Train: 2018-08-06T01:02:46.214806: step 5665, loss 0.579026.
Train: 2018-08-06T01:02:46.461164: step 5666, loss 0.570756.
Train: 2018-08-06T01:02:46.715497: step 5667, loss 0.496339.
Train: 2018-08-06T01:02:46.960835: step 5668, loss 0.603871.
Train: 2018-08-06T01:02:47.207182: step 5669, loss 0.595593.
Train: 2018-08-06T01:02:47.465460: step 5670, loss 0.537668.
Test: 2018-08-06T01:02:48.730077: step 5670, loss 0.548674.
Train: 2018-08-06T01:02:48.967444: step 5671, loss 0.537674.
Train: 2018-08-06T01:02:49.216807: step 5672, loss 0.562484.
Train: 2018-08-06T01:02:49.463117: step 5673, loss 0.537658.
Train: 2018-08-06T01:02:49.724420: step 5674, loss 0.587319.
Train: 2018-08-06T01:02:49.977740: step 5675, loss 0.521076.
Train: 2018-08-06T01:02:50.229070: step 5676, loss 0.537608.
Train: 2018-08-06T01:02:50.482393: step 5677, loss 0.56246.
Train: 2018-08-06T01:02:50.727735: step 5678, loss 0.603981.
Train: 2018-08-06T01:02:50.980092: step 5679, loss 0.570761.
Train: 2018-08-06T01:02:51.235380: step 5680, loss 0.562455.
Test: 2018-08-06T01:02:52.494012: step 5680, loss 0.549175.
Train: 2018-08-06T01:02:52.733377: step 5681, loss 0.487776.
Train: 2018-08-06T01:02:52.978734: step 5682, loss 0.53753.
Train: 2018-08-06T01:02:53.225062: step 5683, loss 0.579071.
Train: 2018-08-06T01:02:53.471434: step 5684, loss 0.554102.
Train: 2018-08-06T01:02:53.730736: step 5685, loss 0.570738.
Train: 2018-08-06T01:02:53.980043: step 5686, loss 0.579117.
Train: 2018-08-06T01:02:54.230405: step 5687, loss 0.562433.
Train: 2018-08-06T01:02:54.478710: step 5688, loss 0.570773.
Train: 2018-08-06T01:02:54.723086: step 5689, loss 0.570771.
Train: 2018-08-06T01:02:54.970395: step 5690, loss 0.58742.
Test: 2018-08-06T01:02:56.240000: step 5690, loss 0.548347.
Train: 2018-08-06T01:02:56.479359: step 5691, loss 0.562422.
Train: 2018-08-06T01:02:56.727695: step 5692, loss 0.537518.
Train: 2018-08-06T01:02:56.973041: step 5693, loss 0.520882.
Train: 2018-08-06T01:02:57.221405: step 5694, loss 0.58741.
Train: 2018-08-06T01:02:57.466752: step 5695, loss 0.520923.
Train: 2018-08-06T01:02:57.722036: step 5696, loss 0.545871.
Train: 2018-08-06T01:02:57.970371: step 5697, loss 0.604005.
Train: 2018-08-06T01:02:58.218732: step 5698, loss 0.51255.
Train: 2018-08-06T01:02:58.461086: step 5699, loss 0.554163.
Train: 2018-08-06T01:02:58.708398: step 5700, loss 0.54582.
Test: 2018-08-06T01:02:59.975010: step 5700, loss 0.548128.
Train: 2018-08-06T01:03:00.918327: step 5701, loss 0.545718.
Train: 2018-08-06T01:03:01.168684: step 5702, loss 0.570728.
Train: 2018-08-06T01:03:01.414003: step 5703, loss 0.562409.
Train: 2018-08-06T01:03:01.660343: step 5704, loss 0.545752.
Train: 2018-08-06T01:03:01.903692: step 5705, loss 0.587508.
Train: 2018-08-06T01:03:02.150042: step 5706, loss 0.570684.
Train: 2018-08-06T01:03:02.397373: step 5707, loss 0.570805.
Train: 2018-08-06T01:03:02.644736: step 5708, loss 0.587457.
Train: 2018-08-06T01:03:02.897050: step 5709, loss 0.529175.
Train: 2018-08-06T01:03:03.150389: step 5710, loss 0.595684.
Test: 2018-08-06T01:03:04.428938: step 5710, loss 0.550268.
Train: 2018-08-06T01:03:04.728383: step 5711, loss 0.52086.
Train: 2018-08-06T01:03:04.972731: step 5712, loss 0.604079.
Train: 2018-08-06T01:03:05.218075: step 5713, loss 0.545808.
Train: 2018-08-06T01:03:05.465428: step 5714, loss 0.579.
Train: 2018-08-06T01:03:05.707810: step 5715, loss 0.562511.
Train: 2018-08-06T01:03:05.955118: step 5716, loss 0.51287.
Train: 2018-08-06T01:03:06.214451: step 5717, loss 0.587312.
Train: 2018-08-06T01:03:06.471764: step 5718, loss 0.554271.
Train: 2018-08-06T01:03:06.727055: step 5719, loss 0.529487.
Train: 2018-08-06T01:03:06.974424: step 5720, loss 0.562515.
Test: 2018-08-06T01:03:08.237016: step 5720, loss 0.549468.
Train: 2018-08-06T01:03:08.474413: step 5721, loss 0.636806.
Train: 2018-08-06T01:03:08.732690: step 5722, loss 0.603724.
Train: 2018-08-06T01:03:08.978065: step 5723, loss 0.620095.
Train: 2018-08-06T01:03:09.224376: step 5724, loss 0.562574.
Train: 2018-08-06T01:03:09.470748: step 5725, loss 0.578933.
Train: 2018-08-06T01:03:09.716060: step 5726, loss 0.627714.
Train: 2018-08-06T01:03:09.967414: step 5727, loss 0.570801.
Train: 2018-08-06T01:03:10.216753: step 5728, loss 0.570821.
Train: 2018-08-06T01:03:10.476059: step 5729, loss 0.554786.
Train: 2018-08-06T01:03:10.720375: step 5730, loss 0.506866.
Test: 2018-08-06T01:03:11.992971: step 5730, loss 0.551019.
Train: 2018-08-06T01:03:12.233331: step 5731, loss 0.578865.
Train: 2018-08-06T01:03:12.480667: step 5732, loss 0.594814.
Train: 2018-08-06T01:03:12.733018: step 5733, loss 0.578858.
Train: 2018-08-06T01:03:12.980362: step 5734, loss 0.634424.
Train: 2018-08-06T01:03:13.225701: step 5735, loss 0.563049.
Train: 2018-08-06T01:03:13.471043: step 5736, loss 0.547334.
Train: 2018-08-06T01:03:13.717385: step 5737, loss 0.61033.
Train: 2018-08-06T01:03:13.920816: step 5738, loss 0.479565.
Train: 2018-08-06T01:03:14.172172: step 5739, loss 0.500512.
Train: 2018-08-06T01:03:14.430485: step 5740, loss 0.571024.
Test: 2018-08-06T01:03:15.697065: step 5740, loss 0.550752.
Train: 2018-08-06T01:03:15.943407: step 5741, loss 0.539579.
Train: 2018-08-06T01:03:16.190771: step 5742, loss 0.539495.
Train: 2018-08-06T01:03:16.440110: step 5743, loss 0.55518.
Train: 2018-08-06T01:03:16.692435: step 5744, loss 0.547193.
Train: 2018-08-06T01:03:16.936808: step 5745, loss 0.515339.
Train: 2018-08-06T01:03:17.187112: step 5746, loss 0.515075.
Train: 2018-08-06T01:03:17.445421: step 5747, loss 0.546804.
Train: 2018-08-06T01:03:17.703755: step 5748, loss 0.603041.
Train: 2018-08-06T01:03:17.951100: step 5749, loss 0.49804.
Train: 2018-08-06T01:03:18.199434: step 5750, loss 0.530113.
Test: 2018-08-06T01:03:19.466018: step 5750, loss 0.548353.
Train: 2018-08-06T01:03:19.716347: step 5751, loss 0.578842.
Train: 2018-08-06T01:03:19.960694: step 5752, loss 0.562311.
Train: 2018-08-06T01:03:20.208065: step 5753, loss 0.61193.
Train: 2018-08-06T01:03:20.464379: step 5754, loss 0.587743.
Train: 2018-08-06T01:03:20.708695: step 5755, loss 0.628664.
Train: 2018-08-06T01:03:20.958053: step 5756, loss 0.562373.
Train: 2018-08-06T01:03:21.209355: step 5757, loss 0.60345.
Train: 2018-08-06T01:03:21.458689: step 5758, loss 0.579394.
Train: 2018-08-06T01:03:21.710017: step 5759, loss 0.653136.
Train: 2018-08-06T01:03:21.954363: step 5760, loss 0.562649.
Test: 2018-08-06T01:03:23.225987: step 5760, loss 0.548655.
Train: 2018-08-06T01:03:23.469312: step 5761, loss 0.562631.
Train: 2018-08-06T01:03:23.717666: step 5762, loss 0.59514.
Train: 2018-08-06T01:03:23.963990: step 5763, loss 0.570816.
Train: 2018-08-06T01:03:24.209333: step 5764, loss 0.554679.
Train: 2018-08-06T01:03:24.455674: step 5765, loss 0.570839.
Train: 2018-08-06T01:03:24.702040: step 5766, loss 0.594977.
Train: 2018-08-06T01:03:24.950384: step 5767, loss 0.56292.
Train: 2018-08-06T01:03:25.204695: step 5768, loss 0.602765.
Train: 2018-08-06T01:03:25.451039: step 5769, loss 0.578888.
Train: 2018-08-06T01:03:25.695358: step 5770, loss 0.570881.
Test: 2018-08-06T01:03:26.968952: step 5770, loss 0.549505.
Train: 2018-08-06T01:03:27.206349: step 5771, loss 0.562963.
Train: 2018-08-06T01:03:27.451692: step 5772, loss 0.500074.
Train: 2018-08-06T01:03:27.703032: step 5773, loss 0.476434.
Train: 2018-08-06T01:03:27.950353: step 5774, loss 0.555081.
Train: 2018-08-06T01:03:28.203676: step 5775, loss 0.586779.
Train: 2018-08-06T01:03:28.447000: step 5776, loss 0.626457.
Train: 2018-08-06T01:03:28.699325: step 5777, loss 0.491773.
Train: 2018-08-06T01:03:28.955638: step 5778, loss 0.539143.
Train: 2018-08-06T01:03:29.199992: step 5779, loss 0.539098.
Train: 2018-08-06T01:03:29.443335: step 5780, loss 0.554781.
Test: 2018-08-06T01:03:30.707953: step 5780, loss 0.548767.
Train: 2018-08-06T01:03:30.942325: step 5781, loss 0.490886.
Train: 2018-08-06T01:03:31.191659: step 5782, loss 0.579021.
Train: 2018-08-06T01:03:31.438998: step 5783, loss 0.562665.
Train: 2018-08-06T01:03:31.692355: step 5784, loss 0.635581.
Train: 2018-08-06T01:03:31.934704: step 5785, loss 0.570761.
Train: 2018-08-06T01:03:32.178021: step 5786, loss 0.497817.
Train: 2018-08-06T01:03:32.424362: step 5787, loss 0.587037.
Train: 2018-08-06T01:03:32.670735: step 5788, loss 0.570808.
Train: 2018-08-06T01:03:32.925023: step 5789, loss 0.513749.
Train: 2018-08-06T01:03:33.169376: step 5790, loss 0.554393.
Test: 2018-08-06T01:03:34.429999: step 5790, loss 0.548535.
Train: 2018-08-06T01:03:34.677336: step 5791, loss 0.570778.
Train: 2018-08-06T01:03:34.936669: step 5792, loss 0.521573.
Train: 2018-08-06T01:03:35.179997: step 5793, loss 0.51322.
Train: 2018-08-06T01:03:35.421348: step 5794, loss 0.636732.
Train: 2018-08-06T01:03:35.666693: step 5795, loss 0.587285.
Train: 2018-08-06T01:03:35.921011: step 5796, loss 0.570753.
Train: 2018-08-06T01:03:36.174366: step 5797, loss 0.6534.
Train: 2018-08-06T01:03:36.432643: step 5798, loss 0.537768.
Train: 2018-08-06T01:03:36.678021: step 5799, loss 0.529572.
Train: 2018-08-06T01:03:36.927321: step 5800, loss 0.471958.
Test: 2018-08-06T01:03:38.176978: step 5800, loss 0.547595.
Train: 2018-08-06T01:03:39.073804: step 5801, loss 0.521265.
Train: 2018-08-06T01:03:39.323124: step 5802, loss 0.537678.
Train: 2018-08-06T01:03:39.583441: step 5803, loss 0.628796.
Train: 2018-08-06T01:03:39.834768: step 5804, loss 0.50433.
Train: 2018-08-06T01:03:40.080117: step 5805, loss 0.570826.
Train: 2018-08-06T01:03:40.327426: step 5806, loss 0.512464.
Train: 2018-08-06T01:03:40.586743: step 5807, loss 0.554093.
Train: 2018-08-06T01:03:40.832076: step 5808, loss 0.503778.
Train: 2018-08-06T01:03:41.079443: step 5809, loss 0.503513.
Train: 2018-08-06T01:03:41.334732: step 5810, loss 0.562455.
Test: 2018-08-06T01:03:42.614310: step 5810, loss 0.547992.
Train: 2018-08-06T01:03:42.852698: step 5811, loss 0.511548.
Train: 2018-08-06T01:03:43.099046: step 5812, loss 0.571137.
Train: 2018-08-06T01:03:43.343417: step 5813, loss 0.570815.
Train: 2018-08-06T01:03:43.588730: step 5814, loss 0.562404.
Train: 2018-08-06T01:03:43.844045: step 5815, loss 0.579483.
Train: 2018-08-06T01:03:44.086373: step 5816, loss 0.570992.
Train: 2018-08-06T01:03:44.334709: step 5817, loss 0.528079.
Train: 2018-08-06T01:03:44.579089: step 5818, loss 0.519489.
Train: 2018-08-06T01:03:44.827422: step 5819, loss 0.536568.
Train: 2018-08-06T01:03:45.077721: step 5820, loss 0.527948.
Test: 2018-08-06T01:03:46.360291: step 5820, loss 0.54874.
Train: 2018-08-06T01:03:46.598654: step 5821, loss 0.665735.
Train: 2018-08-06T01:03:46.847022: step 5822, loss 0.527908.
Train: 2018-08-06T01:03:47.102338: step 5823, loss 0.579546.
Train: 2018-08-06T01:03:47.348649: step 5824, loss 0.61393.
Train: 2018-08-06T01:03:47.597007: step 5825, loss 0.579503.
Train: 2018-08-06T01:03:47.845345: step 5826, loss 0.545222.
Train: 2018-08-06T01:03:48.098668: step 5827, loss 0.519624.
Train: 2018-08-06T01:03:48.346978: step 5828, loss 0.553809.
Train: 2018-08-06T01:03:48.599330: step 5829, loss 0.621993.
Train: 2018-08-06T01:03:48.848668: step 5830, loss 0.528355.
Test: 2018-08-06T01:03:50.109265: step 5830, loss 0.547401.
Train: 2018-08-06T01:03:50.352614: step 5831, loss 0.604783.
Train: 2018-08-06T01:03:50.591974: step 5832, loss 0.56235.
Train: 2018-08-06T01:03:50.850322: step 5833, loss 0.604549.
Train: 2018-08-06T01:03:51.110587: step 5834, loss 0.596013.
Train: 2018-08-06T01:03:51.351942: step 5835, loss 0.503794.
Train: 2018-08-06T01:03:51.605264: step 5836, loss 0.537356.
Train: 2018-08-06T01:03:51.849611: step 5837, loss 0.645863.
Train: 2018-08-06T01:03:52.095983: step 5838, loss 0.570759.
Train: 2018-08-06T01:03:52.355258: step 5839, loss 0.562482.
Train: 2018-08-06T01:03:52.598633: step 5840, loss 0.504776.
Test: 2018-08-06T01:03:53.858240: step 5840, loss 0.549151.
Train: 2018-08-06T01:03:54.097630: step 5841, loss 0.562519.
Train: 2018-08-06T01:03:54.343941: step 5842, loss 0.529656.
Train: 2018-08-06T01:03:54.593311: step 5843, loss 0.53787.
Train: 2018-08-06T01:03:54.845624: step 5844, loss 0.620078.
Train: 2018-08-06T01:03:55.092963: step 5845, loss 0.578949.
Train: 2018-08-06T01:03:55.346261: step 5846, loss 0.521628.
Train: 2018-08-06T01:03:55.592627: step 5847, loss 0.51347.
Train: 2018-08-06T01:03:55.840937: step 5848, loss 0.52165.
Train: 2018-08-06T01:03:56.095269: step 5849, loss 0.595372.
Train: 2018-08-06T01:03:56.341623: step 5850, loss 0.603581.
Test: 2018-08-06T01:03:57.616189: step 5850, loss 0.548097.
Train: 2018-08-06T01:03:57.854583: step 5851, loss 0.570799.
Train: 2018-08-06T01:03:58.102921: step 5852, loss 0.562546.
Train: 2018-08-06T01:03:58.351254: step 5853, loss 0.587148.
Train: 2018-08-06T01:03:58.598587: step 5854, loss 0.538054.
Train: 2018-08-06T01:03:58.842934: step 5855, loss 0.611676.
Train: 2018-08-06T01:03:59.091269: step 5856, loss 0.570775.
Train: 2018-08-06T01:03:59.340578: step 5857, loss 0.546342.
Train: 2018-08-06T01:03:59.591928: step 5858, loss 0.562642.
Train: 2018-08-06T01:03:59.839244: step 5859, loss 0.660236.
Train: 2018-08-06T01:04:00.085616: step 5860, loss 0.627601.
Test: 2018-08-06T01:04:01.341227: step 5860, loss 0.549088.
Train: 2018-08-06T01:04:01.586596: step 5861, loss 0.619236.
Train: 2018-08-06T01:04:01.833935: step 5862, loss 0.578875.
Train: 2018-08-06T01:04:02.076261: step 5863, loss 0.538962.
Train: 2018-08-06T01:04:02.323648: step 5864, loss 0.570905.
Train: 2018-08-06T01:04:02.581952: step 5865, loss 0.531311.
Train: 2018-08-06T01:04:02.827296: step 5866, loss 0.673792.
Train: 2018-08-06T01:04:03.073647: step 5867, loss 0.52374.
Train: 2018-08-06T01:04:03.331953: step 5868, loss 0.547514.
Train: 2018-08-06T01:04:03.584257: step 5869, loss 0.524031.
Train: 2018-08-06T01:04:03.832614: step 5870, loss 0.51608.
Test: 2018-08-06T01:04:05.100193: step 5870, loss 0.549211.
Train: 2018-08-06T01:04:05.337583: step 5871, loss 0.57113.
Train: 2018-08-06T01:04:05.583899: step 5872, loss 0.55499.
Train: 2018-08-06T01:04:05.832235: step 5873, loss 0.619106.
Train: 2018-08-06T01:04:06.076582: step 5874, loss 0.594626.
Train: 2018-08-06T01:04:06.328938: step 5875, loss 0.555554.
Train: 2018-08-06T01:04:06.576245: step 5876, loss 0.586458.
Train: 2018-08-06T01:04:06.830594: step 5877, loss 0.515962.
Train: 2018-08-06T01:04:07.079898: step 5878, loss 0.539259.
Train: 2018-08-06T01:04:07.324276: step 5879, loss 0.626704.
Train: 2018-08-06T01:04:07.570617: step 5880, loss 0.610385.
Test: 2018-08-06T01:04:08.847171: step 5880, loss 0.549329.
Train: 2018-08-06T01:04:09.087535: step 5881, loss 0.508195.
Train: 2018-08-06T01:04:09.340886: step 5882, loss 0.586639.
Train: 2018-08-06T01:04:09.596169: step 5883, loss 0.531892.
Train: 2018-08-06T01:04:09.852515: step 5884, loss 0.516057.
Train: 2018-08-06T01:04:10.100846: step 5885, loss 0.602799.
Train: 2018-08-06T01:04:10.349186: step 5886, loss 0.53967.
Train: 2018-08-06T01:04:10.607494: step 5887, loss 0.578804.
Train: 2018-08-06T01:04:10.852840: step 5888, loss 0.531567.
Train: 2018-08-06T01:04:11.041329: step 5889, loss 0.563061.
Train: 2018-08-06T01:04:11.287678: step 5890, loss 0.578846.
Test: 2018-08-06T01:04:12.569218: step 5890, loss 0.549551.
Train: 2018-08-06T01:04:12.807580: step 5891, loss 0.642345.
Train: 2018-08-06T01:04:13.052926: step 5892, loss 0.531266.
Train: 2018-08-06T01:04:13.309240: step 5893, loss 0.499476.
Train: 2018-08-06T01:04:13.552589: step 5894, loss 0.499284.
Train: 2018-08-06T01:04:13.808928: step 5895, loss 0.650761.
Train: 2018-08-06T01:04:14.054297: step 5896, loss 0.55486.
Train: 2018-08-06T01:04:14.302609: step 5897, loss 0.594883.
Train: 2018-08-06T01:04:14.561890: step 5898, loss 0.578878.
Train: 2018-08-06T01:04:14.812249: step 5899, loss 0.562851.
Train: 2018-08-06T01:04:15.057595: step 5900, loss 0.562828.
Test: 2018-08-06T01:04:16.336145: step 5900, loss 0.548297.
Train: 2018-08-06T01:04:17.229471: step 5901, loss 0.651118.
Train: 2018-08-06T01:04:17.481853: step 5902, loss 0.578862.
Train: 2018-08-06T01:04:17.733150: step 5903, loss 0.514862.
Train: 2018-08-06T01:04:17.981460: step 5904, loss 0.626852.
Train: 2018-08-06T01:04:18.225836: step 5905, loss 0.578868.
Train: 2018-08-06T01:04:18.473171: step 5906, loss 0.539014.
Train: 2018-08-06T01:04:18.720483: step 5907, loss 0.586817.
Train: 2018-08-06T01:04:18.968820: step 5908, loss 0.491429.
Train: 2018-08-06T01:04:19.213166: step 5909, loss 0.554998.
Train: 2018-08-06T01:04:19.459536: step 5910, loss 0.539048.
Test: 2018-08-06T01:04:20.755043: step 5910, loss 0.549041.
Train: 2018-08-06T01:04:21.007369: step 5911, loss 0.538981.
Train: 2018-08-06T01:04:21.254707: step 5912, loss 0.522927.
Train: 2018-08-06T01:04:21.503043: step 5913, loss 0.586876.
Train: 2018-08-06T01:04:21.749404: step 5914, loss 0.530665.
Train: 2018-08-06T01:04:22.008702: step 5915, loss 0.562763.
Train: 2018-08-06T01:04:22.258054: step 5916, loss 0.586974.
Train: 2018-08-06T01:04:22.513341: step 5917, loss 0.667939.
Train: 2018-08-06T01:04:22.757691: step 5918, loss 0.578867.
Train: 2018-08-06T01:04:23.009024: step 5919, loss 0.578892.
Train: 2018-08-06T01:04:23.258349: step 5920, loss 0.586918.
Test: 2018-08-06T01:04:24.514987: step 5920, loss 0.549883.
Train: 2018-08-06T01:04:24.755344: step 5921, loss 0.530554.
Train: 2018-08-06T01:04:25.000699: step 5922, loss 0.594983.
Train: 2018-08-06T01:04:25.247059: step 5923, loss 0.514601.
Train: 2018-08-06T01:04:25.501383: step 5924, loss 0.562802.
Train: 2018-08-06T01:04:25.746718: step 5925, loss 0.562821.
Train: 2018-08-06T01:04:25.996027: step 5926, loss 0.603001.
Train: 2018-08-06T01:04:26.243396: step 5927, loss 0.602959.
Train: 2018-08-06T01:04:26.490735: step 5928, loss 0.546778.
Train: 2018-08-06T01:04:26.739074: step 5929, loss 0.538817.
Train: 2018-08-06T01:04:26.997375: step 5930, loss 0.546825.
Test: 2018-08-06T01:04:28.270942: step 5930, loss 0.549646.
Train: 2018-08-06T01:04:28.505316: step 5931, loss 0.530803.
Train: 2018-08-06T01:04:28.753678: step 5932, loss 0.570857.
Train: 2018-08-06T01:04:29.008970: step 5933, loss 0.498556.
Train: 2018-08-06T01:04:29.258302: step 5934, loss 0.554688.
Train: 2018-08-06T01:04:29.504643: step 5935, loss 0.554601.
Train: 2018-08-06T01:04:29.750984: step 5936, loss 0.457378.
Train: 2018-08-06T01:04:30.010320: step 5937, loss 0.570524.
Train: 2018-08-06T01:04:30.260622: step 5938, loss 0.55399.
Train: 2018-08-06T01:04:30.505966: step 5939, loss 0.64596.
Train: 2018-08-06T01:04:30.754301: step 5940, loss 0.529233.
Test: 2018-08-06T01:04:32.014930: step 5940, loss 0.548727.
Train: 2018-08-06T01:04:32.255313: step 5941, loss 0.587281.
Train: 2018-08-06T01:04:32.498667: step 5942, loss 0.571386.
Train: 2018-08-06T01:04:32.752988: step 5943, loss 0.51239.
Train: 2018-08-06T01:04:32.996333: step 5944, loss 0.587658.
Train: 2018-08-06T01:04:33.244642: step 5945, loss 0.571041.
Train: 2018-08-06T01:04:33.491979: step 5946, loss 0.554102.
Train: 2018-08-06T01:04:33.739344: step 5947, loss 0.603713.
Train: 2018-08-06T01:04:33.992669: step 5948, loss 0.570931.
Train: 2018-08-06T01:04:34.240977: step 5949, loss 0.545974.
Train: 2018-08-06T01:04:34.483328: step 5950, loss 0.587286.
Test: 2018-08-06T01:04:35.738970: step 5950, loss 0.550641.
Train: 2018-08-06T01:04:35.977364: step 5951, loss 0.56251.
Train: 2018-08-06T01:04:36.226692: step 5952, loss 0.488426.
Train: 2018-08-06T01:04:36.471014: step 5953, loss 0.620183.
Train: 2018-08-06T01:04:36.717383: step 5954, loss 0.562522.
Train: 2018-08-06T01:04:36.964724: step 5955, loss 0.562548.
Train: 2018-08-06T01:04:37.216052: step 5956, loss 0.562535.
Train: 2018-08-06T01:04:37.466382: step 5957, loss 0.562549.
Train: 2018-08-06T01:04:37.712724: step 5958, loss 0.505106.
Train: 2018-08-06T01:04:37.968010: step 5959, loss 0.652898.
Train: 2018-08-06T01:04:38.230308: step 5960, loss 0.521552.
Test: 2018-08-06T01:04:39.494925: step 5960, loss 0.54925.
Train: 2018-08-06T01:04:39.734286: step 5961, loss 0.587153.
Train: 2018-08-06T01:04:39.980627: step 5962, loss 0.619901.
Train: 2018-08-06T01:04:40.231955: step 5963, loss 0.497226.
Train: 2018-08-06T01:04:40.475358: step 5964, loss 0.570774.
Train: 2018-08-06T01:04:40.722671: step 5965, loss 0.48915.
Train: 2018-08-06T01:04:40.968018: step 5966, loss 0.619797.
Train: 2018-08-06T01:04:41.214328: step 5967, loss 0.546264.
Train: 2018-08-06T01:04:41.460669: step 5968, loss 0.636133.
Train: 2018-08-06T01:04:41.758373: step 5969, loss 0.56262.
Train: 2018-08-06T01:04:42.013702: step 5970, loss 0.578921.
Test: 2018-08-06T01:04:43.292252: step 5970, loss 0.54758.
Train: 2018-08-06T01:04:43.540587: step 5971, loss 0.554524.
Train: 2018-08-06T01:04:43.786953: step 5972, loss 0.587023.
Train: 2018-08-06T01:04:44.046259: step 5973, loss 0.554587.
Train: 2018-08-06T01:04:44.290582: step 5974, loss 0.554616.
Train: 2018-08-06T01:04:44.538946: step 5975, loss 0.562721.
Train: 2018-08-06T01:04:44.786256: step 5976, loss 0.554653.
Train: 2018-08-06T01:04:45.035590: step 5977, loss 0.554665.
Train: 2018-08-06T01:04:45.290906: step 5978, loss 0.619241.
Train: 2018-08-06T01:04:45.535253: step 5979, loss 0.578879.
Train: 2018-08-06T01:04:45.780597: step 5980, loss 0.546694.
Test: 2018-08-06T01:04:47.053194: step 5980, loss 0.549377.
Train: 2018-08-06T01:04:47.301529: step 5981, loss 0.602982.
Train: 2018-08-06T01:04:47.549866: step 5982, loss 0.474589.
Train: 2018-08-06T01:04:47.809173: step 5983, loss 0.522687.
Train: 2018-08-06T01:04:48.055544: step 5984, loss 0.611027.
Train: 2018-08-06T01:04:48.302853: step 5985, loss 0.586914.
Train: 2018-08-06T01:04:48.565183: step 5986, loss 0.570834.
Train: 2018-08-06T01:04:48.811492: step 5987, loss 0.554762.
Train: 2018-08-06T01:04:49.059853: step 5988, loss 0.538689.
Train: 2018-08-06T01:04:49.316141: step 5989, loss 0.56279.
Train: 2018-08-06T01:04:49.562508: step 5990, loss 0.627157.
Test: 2018-08-06T01:04:50.822114: step 5990, loss 0.550107.
Train: 2018-08-06T01:04:51.057509: step 5991, loss 0.554753.
Train: 2018-08-06T01:04:51.304851: step 5992, loss 0.554767.
Train: 2018-08-06T01:04:51.552163: step 5993, loss 0.611003.
Train: 2018-08-06T01:04:51.811467: step 5994, loss 0.578866.
Train: 2018-08-06T01:04:52.055845: step 5995, loss 0.522787.
Train: 2018-08-06T01:04:52.307143: step 5996, loss 0.546827.
Train: 2018-08-06T01:04:52.558470: step 5997, loss 0.578867.
Train: 2018-08-06T01:04:52.810795: step 5998, loss 0.610912.
Train: 2018-08-06T01:04:53.062124: step 5999, loss 0.594869.
Train: 2018-08-06T01:04:53.318438: step 6000, loss 0.570873.
Test: 2018-08-06T01:04:54.595024: step 6000, loss 0.549776.
Train: 2018-08-06T01:04:55.478887: step 6001, loss 0.610759.
Train: 2018-08-06T01:04:55.726214: step 6002, loss 0.578857.
Train: 2018-08-06T01:04:55.976544: step 6003, loss 0.547126.
Train: 2018-08-06T01:04:56.222892: step 6004, loss 0.555099.
Train: 2018-08-06T01:04:56.467251: step 6005, loss 0.499766.
Train: 2018-08-06T01:04:56.715574: step 6006, loss 0.56303.
Train: 2018-08-06T01:04:56.962936: step 6007, loss 0.618462.
Train: 2018-08-06T01:04:57.209254: step 6008, loss 0.539277.
Train: 2018-08-06T01:04:57.462571: step 6009, loss 0.523435.
Train: 2018-08-06T01:04:57.708886: step 6010, loss 0.523357.
Test: 2018-08-06T01:04:58.983478: step 6010, loss 0.549871.
Train: 2018-08-06T01:04:59.220843: step 6011, loss 0.562962.
Train: 2018-08-06T01:04:59.473195: step 6012, loss 0.523097.
Train: 2018-08-06T01:04:59.734471: step 6013, loss 0.634805.
Train: 2018-08-06T01:04:59.980842: step 6014, loss 0.618866.
Train: 2018-08-06T01:05:00.227179: step 6015, loss 0.514845.
Train: 2018-08-06T01:05:00.472496: step 6016, loss 0.586852.
Train: 2018-08-06T01:05:00.728836: step 6017, loss 0.538754.
Train: 2018-08-06T01:05:00.975178: step 6018, loss 0.514548.
Train: 2018-08-06T01:05:01.223519: step 6019, loss 0.554921.
Train: 2018-08-06T01:05:01.470857: step 6020, loss 0.530106.
Test: 2018-08-06T01:05:02.752399: step 6020, loss 0.548571.
Train: 2018-08-06T01:05:02.989775: step 6021, loss 0.546521.
Train: 2018-08-06T01:05:03.237138: step 6022, loss 0.595289.
Train: 2018-08-06T01:05:03.484441: step 6023, loss 0.512803.
Train: 2018-08-06T01:05:03.729785: step 6024, loss 0.520768.
Train: 2018-08-06T01:05:03.970141: step 6025, loss 0.560916.
Train: 2018-08-06T01:05:04.219476: step 6026, loss 0.572533.
Train: 2018-08-06T01:05:04.463821: step 6027, loss 0.516944.
Train: 2018-08-06T01:05:04.713154: step 6028, loss 0.543911.
Train: 2018-08-06T01:05:04.973458: step 6029, loss 0.482323.
Train: 2018-08-06T01:05:05.232793: step 6030, loss 0.485501.
Test: 2018-08-06T01:05:06.522316: step 6030, loss 0.548108.
Train: 2018-08-06T01:05:06.819550: step 6031, loss 0.593259.
Train: 2018-08-06T01:05:07.078892: step 6032, loss 0.543939.
Train: 2018-08-06T01:05:07.328214: step 6033, loss 0.570375.
Train: 2018-08-06T01:05:07.571564: step 6034, loss 0.615963.
Train: 2018-08-06T01:05:07.820898: step 6035, loss 0.45283.
Train: 2018-08-06T01:05:08.069207: step 6036, loss 0.581886.
Train: 2018-08-06T01:05:08.314582: step 6037, loss 0.579417.
Train: 2018-08-06T01:05:08.559936: step 6038, loss 0.557196.
Train: 2018-08-06T01:05:08.816235: step 6039, loss 0.582596.
Train: 2018-08-06T01:05:09.010720: step 6040, loss 0.396942.
Test: 2018-08-06T01:05:10.269323: step 6040, loss 0.547199.
Train: 2018-08-06T01:05:10.509681: step 6041, loss 0.614333.
Train: 2018-08-06T01:05:10.758017: step 6042, loss 0.570427.
Train: 2018-08-06T01:05:11.005380: step 6043, loss 0.589162.
Train: 2018-08-06T01:05:11.253718: step 6044, loss 0.570335.
Train: 2018-08-06T01:05:11.497071: step 6045, loss 0.621022.
Train: 2018-08-06T01:05:11.743382: step 6046, loss 0.63763.
Train: 2018-08-06T01:05:11.989722: step 6047, loss 0.644989.
Train: 2018-08-06T01:05:12.245040: step 6048, loss 0.53796.
Train: 2018-08-06T01:05:12.497364: step 6049, loss 0.530034.
Train: 2018-08-06T01:05:12.742708: step 6050, loss 0.578908.
Test: 2018-08-06T01:05:14.012313: step 6050, loss 0.549387.
Train: 2018-08-06T01:05:14.251673: step 6051, loss 0.595071.
Train: 2018-08-06T01:05:14.499013: step 6052, loss 0.514407.
Train: 2018-08-06T01:05:14.746376: step 6053, loss 0.530632.
Train: 2018-08-06T01:05:14.995683: step 6054, loss 0.514597.
Train: 2018-08-06T01:05:15.247043: step 6055, loss 0.530632.
Train: 2018-08-06T01:05:15.491383: step 6056, loss 0.514495.
Train: 2018-08-06T01:05:15.739718: step 6057, loss 0.570753.
Train: 2018-08-06T01:05:15.999000: step 6058, loss 0.570793.
Train: 2018-08-06T01:05:16.244372: step 6059, loss 0.570843.
Train: 2018-08-06T01:05:16.485704: step 6060, loss 0.497904.
Test: 2018-08-06T01:05:17.750316: step 6060, loss 0.548703.
Train: 2018-08-06T01:05:17.986715: step 6061, loss 0.578851.
Train: 2018-08-06T01:05:18.240032: step 6062, loss 0.546278.
Train: 2018-08-06T01:05:18.486348: step 6063, loss 0.489138.
Train: 2018-08-06T01:05:18.729734: step 6064, loss 0.554327.
Train: 2018-08-06T01:05:18.990014: step 6065, loss 0.595539.
Train: 2018-08-06T01:05:19.245331: step 6066, loss 0.554354.
Train: 2018-08-06T01:05:19.492669: step 6067, loss 0.554346.
Train: 2018-08-06T01:05:19.746989: step 6068, loss 0.620526.
Train: 2018-08-06T01:05:19.991360: step 6069, loss 0.579036.
Train: 2018-08-06T01:05:20.241665: step 6070, loss 0.570835.
Test: 2018-08-06T01:05:21.508278: step 6070, loss 0.547511.
Train: 2018-08-06T01:05:21.746640: step 6071, loss 0.521152.
Train: 2018-08-06T01:05:21.992015: step 6072, loss 0.587478.
Train: 2018-08-06T01:05:22.240321: step 6073, loss 0.545886.
Train: 2018-08-06T01:05:22.488656: step 6074, loss 0.612127.
Train: 2018-08-06T01:05:22.733035: step 6075, loss 0.579043.
Train: 2018-08-06T01:05:22.981365: step 6076, loss 0.636874.
Train: 2018-08-06T01:05:23.227679: step 6077, loss 0.545989.
Train: 2018-08-06T01:05:23.473032: step 6078, loss 0.60361.
Train: 2018-08-06T01:05:23.724377: step 6079, loss 0.497124.
Train: 2018-08-06T01:05:23.979669: step 6080, loss 0.538055.
Test: 2018-08-06T01:05:25.237305: step 6080, loss 0.549718.
Train: 2018-08-06T01:05:25.482675: step 6081, loss 0.587122.
Train: 2018-08-06T01:05:25.728991: step 6082, loss 0.530025.
Train: 2018-08-06T01:05:25.974334: step 6083, loss 0.611508.
Train: 2018-08-06T01:05:26.221698: step 6084, loss 0.587057.
Train: 2018-08-06T01:05:26.477987: step 6085, loss 0.554536.
Train: 2018-08-06T01:05:26.738292: step 6086, loss 0.578896.
Train: 2018-08-06T01:05:26.987649: step 6087, loss 0.578898.
Train: 2018-08-06T01:05:27.232999: step 6088, loss 0.522301.
Train: 2018-08-06T01:05:27.489282: step 6089, loss 0.490021.
Train: 2018-08-06T01:05:27.738641: step 6090, loss 0.611211.
Test: 2018-08-06T01:05:29.015202: step 6090, loss 0.548465.
Train: 2018-08-06T01:05:29.253566: step 6091, loss 0.498004.
Train: 2018-08-06T01:05:29.497936: step 6092, loss 0.595068.
Train: 2018-08-06T01:05:29.747276: step 6093, loss 0.562687.
Train: 2018-08-06T01:05:29.995615: step 6094, loss 0.635684.
Train: 2018-08-06T01:05:30.242949: step 6095, loss 0.562691.
Train: 2018-08-06T01:05:30.505218: step 6096, loss 0.506055.
Train: 2018-08-06T01:05:30.755582: step 6097, loss 0.611281.
Train: 2018-08-06T01:05:31.011893: step 6098, loss 0.627429.
Train: 2018-08-06T01:05:31.264243: step 6099, loss 0.562747.
Train: 2018-08-06T01:05:31.508559: step 6100, loss 0.546635.
Test: 2018-08-06T01:05:32.779136: step 6100, loss 0.54971.
Train: 2018-08-06T01:05:33.668259: step 6101, loss 0.570825.
Train: 2018-08-06T01:05:33.913628: step 6102, loss 0.594958.
Train: 2018-08-06T01:05:34.156978: step 6103, loss 0.578858.
Train: 2018-08-06T01:05:34.409277: step 6104, loss 0.570863.
Train: 2018-08-06T01:05:34.660604: step 6105, loss 0.570863.
Train: 2018-08-06T01:05:34.914956: step 6106, loss 0.570886.
Train: 2018-08-06T01:05:35.172237: step 6107, loss 0.554971.
Train: 2018-08-06T01:05:35.419576: step 6108, loss 0.586819.
Train: 2018-08-06T01:05:35.665916: step 6109, loss 0.555029.
Train: 2018-08-06T01:05:35.923229: step 6110, loss 0.586795.
Test: 2018-08-06T01:05:37.193830: step 6110, loss 0.549418.
Train: 2018-08-06T01:05:37.434213: step 6111, loss 0.578858.
Train: 2018-08-06T01:05:37.686520: step 6112, loss 0.523493.
Train: 2018-08-06T01:05:37.934879: step 6113, loss 0.49186.
Train: 2018-08-06T01:05:38.188171: step 6114, loss 0.523394.
Train: 2018-08-06T01:05:38.432517: step 6115, loss 0.555019.
Train: 2018-08-06T01:05:38.681881: step 6116, loss 0.546989.
Train: 2018-08-06T01:05:38.933179: step 6117, loss 0.554891.
Train: 2018-08-06T01:05:39.185504: step 6118, loss 0.538787.
Train: 2018-08-06T01:05:39.432842: step 6119, loss 0.611048.
Train: 2018-08-06T01:05:39.689182: step 6120, loss 0.595004.
Test: 2018-08-06T01:05:40.959758: step 6120, loss 0.548532.
Train: 2018-08-06T01:05:41.200141: step 6121, loss 0.554682.
Train: 2018-08-06T01:05:41.444493: step 6122, loss 0.538501.
Train: 2018-08-06T01:05:41.690828: step 6123, loss 0.61125.
Train: 2018-08-06T01:05:41.936178: step 6124, loss 0.554608.
Train: 2018-08-06T01:05:42.183512: step 6125, loss 0.506011.
Train: 2018-08-06T01:05:42.427858: step 6126, loss 0.5789.
Train: 2018-08-06T01:05:42.676195: step 6127, loss 0.570784.
Train: 2018-08-06T01:05:42.924546: step 6128, loss 0.562653.
Train: 2018-08-06T01:05:43.171875: step 6129, loss 0.473071.
Train: 2018-08-06T01:05:43.418185: step 6130, loss 0.5871.
Test: 2018-08-06T01:05:44.681804: step 6130, loss 0.54797.
Train: 2018-08-06T01:05:44.926153: step 6131, loss 0.505288.
Train: 2018-08-06T01:05:45.172524: step 6132, loss 0.578965.
Train: 2018-08-06T01:05:45.421826: step 6133, loss 0.554266.
Train: 2018-08-06T01:05:45.667211: step 6134, loss 0.587271.
Train: 2018-08-06T01:05:45.926507: step 6135, loss 0.595549.
Train: 2018-08-06T01:05:46.172818: step 6136, loss 0.496355.
Train: 2018-08-06T01:05:46.419189: step 6137, loss 0.628776.
Train: 2018-08-06T01:05:46.668492: step 6138, loss 0.59556.
Train: 2018-08-06T01:05:46.916860: step 6139, loss 0.587337.
Train: 2018-08-06T01:05:47.164166: step 6140, loss 0.570749.
Test: 2018-08-06T01:05:48.443744: step 6140, loss 0.548707.
Train: 2018-08-06T01:05:48.681140: step 6141, loss 0.570756.
Train: 2018-08-06T01:05:48.929446: step 6142, loss 0.513018.
Train: 2018-08-06T01:05:49.185791: step 6143, loss 0.513041.
Train: 2018-08-06T01:05:49.437113: step 6144, loss 0.554239.
Train: 2018-08-06T01:05:49.684426: step 6145, loss 0.653371.
Train: 2018-08-06T01:05:49.934782: step 6146, loss 0.496473.
Train: 2018-08-06T01:05:50.182095: step 6147, loss 0.488258.
Train: 2018-08-06T01:05:50.431459: step 6148, loss 0.587179.
Train: 2018-08-06T01:05:50.683754: step 6149, loss 0.471453.
Train: 2018-08-06T01:05:50.935107: step 6150, loss 0.570737.
Test: 2018-08-06T01:05:52.194712: step 6150, loss 0.548123.
Train: 2018-08-06T01:05:52.432109: step 6151, loss 0.562286.
Train: 2018-08-06T01:05:52.682422: step 6152, loss 0.570776.
Train: 2018-08-06T01:05:52.936758: step 6153, loss 0.528656.
Train: 2018-08-06T01:05:53.183070: step 6154, loss 0.562553.
Train: 2018-08-06T01:05:53.430439: step 6155, loss 0.554508.
Train: 2018-08-06T01:05:53.685741: step 6156, loss 0.578815.
Train: 2018-08-06T01:05:53.956002: step 6157, loss 0.579177.
Train: 2018-08-06T01:05:54.204338: step 6158, loss 0.54473.
Train: 2018-08-06T01:05:54.449706: step 6159, loss 0.554022.
Train: 2018-08-06T01:05:54.705030: step 6160, loss 0.689181.
Test: 2018-08-06T01:05:55.972609: step 6160, loss 0.549214.
Train: 2018-08-06T01:05:56.209975: step 6161, loss 0.519788.
Train: 2018-08-06T01:05:56.470279: step 6162, loss 0.553981.
Train: 2018-08-06T01:05:56.732608: step 6163, loss 0.553708.
Train: 2018-08-06T01:05:56.982940: step 6164, loss 0.596348.
Train: 2018-08-06T01:05:57.245237: step 6165, loss 0.511631.
Train: 2018-08-06T01:05:57.503539: step 6166, loss 0.561983.
Train: 2018-08-06T01:05:57.748859: step 6167, loss 0.53753.
Train: 2018-08-06T01:05:58.014197: step 6168, loss 0.49468.
Train: 2018-08-06T01:05:58.261514: step 6169, loss 0.528446.
Train: 2018-08-06T01:05:58.507829: step 6170, loss 0.53567.
Test: 2018-08-06T01:05:59.787408: step 6170, loss 0.549257.
Train: 2018-08-06T01:06:00.028787: step 6171, loss 0.52137.
Train: 2018-08-06T01:06:00.274106: step 6172, loss 0.510623.
Train: 2018-08-06T01:06:00.522445: step 6173, loss 0.575686.
Train: 2018-08-06T01:06:00.769779: step 6174, loss 0.570957.
Train: 2018-08-06T01:06:01.014152: step 6175, loss 0.530077.
Train: 2018-08-06T01:06:01.259503: step 6176, loss 0.608908.
Train: 2018-08-06T01:06:01.508803: step 6177, loss 0.481663.
Train: 2018-08-06T01:06:01.758137: step 6178, loss 0.563306.
Train: 2018-08-06T01:06:02.019438: step 6179, loss 0.602276.
Train: 2018-08-06T01:06:02.275784: step 6180, loss 0.639538.
Test: 2018-08-06T01:06:03.552338: step 6180, loss 0.546987.
Train: 2018-08-06T01:06:03.794722: step 6181, loss 0.536686.
Train: 2018-08-06T01:06:04.042029: step 6182, loss 0.494383.
Train: 2018-08-06T01:06:04.285378: step 6183, loss 0.544734.
Train: 2018-08-06T01:06:04.540706: step 6184, loss 0.553417.
Train: 2018-08-06T01:06:04.800027: step 6185, loss 0.552665.
Train: 2018-08-06T01:06:05.048343: step 6186, loss 0.646815.
Train: 2018-08-06T01:06:05.309676: step 6187, loss 0.502089.
Train: 2018-08-06T01:06:05.560007: step 6188, loss 0.579393.
Train: 2018-08-06T01:06:05.815292: step 6189, loss 0.603991.
Train: 2018-08-06T01:06:06.064627: step 6190, loss 0.605397.
Test: 2018-08-06T01:06:07.333234: step 6190, loss 0.54748.
Train: 2018-08-06T01:06:07.527759: step 6191, loss 0.580142.
Train: 2018-08-06T01:06:07.785056: step 6192, loss 0.545827.
Train: 2018-08-06T01:06:08.032394: step 6193, loss 0.5047.
Train: 2018-08-06T01:06:08.283690: step 6194, loss 0.53763.
Train: 2018-08-06T01:06:08.530031: step 6195, loss 0.594845.
Train: 2018-08-06T01:06:08.785374: step 6196, loss 0.595619.
Train: 2018-08-06T01:06:09.030720: step 6197, loss 0.570746.
Train: 2018-08-06T01:06:09.278033: step 6198, loss 0.603169.
Train: 2018-08-06T01:06:09.540356: step 6199, loss 0.448538.
Train: 2018-08-06T01:06:09.791691: step 6200, loss 0.57911.
Test: 2018-08-06T01:06:11.077220: step 6200, loss 0.54882.
Train: 2018-08-06T01:06:11.996668: step 6201, loss 0.562438.
Train: 2018-08-06T01:06:12.244007: step 6202, loss 0.579038.
Train: 2018-08-06T01:06:12.501346: step 6203, loss 0.530244.
Train: 2018-08-06T01:06:12.750653: step 6204, loss 0.546367.
Train: 2018-08-06T01:06:12.999987: step 6205, loss 0.587311.
Train: 2018-08-06T01:06:13.247324: step 6206, loss 0.603823.
Train: 2018-08-06T01:06:13.502670: step 6207, loss 0.579101.
Train: 2018-08-06T01:06:13.757959: step 6208, loss 0.530388.
Train: 2018-08-06T01:06:14.005328: step 6209, loss 0.546417.
Train: 2018-08-06T01:06:14.253664: step 6210, loss 0.5302.
Test: 2018-08-06T01:06:15.507280: step 6210, loss 0.547482.
Train: 2018-08-06T01:06:15.745675: step 6211, loss 0.514107.
Train: 2018-08-06T01:06:15.997970: step 6212, loss 0.652089.
Train: 2018-08-06T01:06:16.246337: step 6213, loss 0.586936.
Train: 2018-08-06T01:06:16.491679: step 6214, loss 0.587002.
Train: 2018-08-06T01:06:16.738988: step 6215, loss 0.554621.
Train: 2018-08-06T01:06:16.995301: step 6216, loss 0.651611.
Train: 2018-08-06T01:06:17.256603: step 6217, loss 0.570805.
Train: 2018-08-06T01:06:17.508928: step 6218, loss 0.530677.
Train: 2018-08-06T01:06:17.756291: step 6219, loss 0.602916.
Train: 2018-08-06T01:06:18.004656: step 6220, loss 0.554867.
Test: 2018-08-06T01:06:19.266227: step 6220, loss 0.549185.
Train: 2018-08-06T01:06:19.505588: step 6221, loss 0.570873.
Train: 2018-08-06T01:06:19.751928: step 6222, loss 0.570884.
Train: 2018-08-06T01:06:19.998269: step 6223, loss 0.554966.
Train: 2018-08-06T01:06:20.250600: step 6224, loss 0.554968.
Train: 2018-08-06T01:06:20.501925: step 6225, loss 0.594634.
Train: 2018-08-06T01:06:20.759263: step 6226, loss 0.538952.
Train: 2018-08-06T01:06:21.009566: step 6227, loss 0.578641.
Train: 2018-08-06T01:06:21.258925: step 6228, loss 0.595005.
Train: 2018-08-06T01:06:21.509231: step 6229, loss 0.595183.
Train: 2018-08-06T01:06:21.754606: step 6230, loss 0.57838.
Test: 2018-08-06T01:06:23.041132: step 6230, loss 0.54893.
Train: 2018-08-06T01:06:23.283508: step 6231, loss 0.57925.
Train: 2018-08-06T01:06:23.529857: step 6232, loss 0.523574.
Train: 2018-08-06T01:06:23.777197: step 6233, loss 0.578701.
Train: 2018-08-06T01:06:24.025499: step 6234, loss 0.570587.
Train: 2018-08-06T01:06:24.274866: step 6235, loss 0.602376.
Train: 2018-08-06T01:06:24.533143: step 6236, loss 0.562409.
Train: 2018-08-06T01:06:24.780506: step 6237, loss 0.51554.
Train: 2018-08-06T01:06:25.021836: step 6238, loss 0.625558.
Train: 2018-08-06T01:06:25.269204: step 6239, loss 0.579141.
Train: 2018-08-06T01:06:25.515514: step 6240, loss 0.594005.
Test: 2018-08-06T01:06:26.773151: step 6240, loss 0.548982.
Train: 2018-08-06T01:06:27.013509: step 6241, loss 0.608139.
Train: 2018-08-06T01:06:27.265834: step 6242, loss 0.579706.
Train: 2018-08-06T01:06:27.513197: step 6243, loss 0.524111.
Train: 2018-08-06T01:06:27.763534: step 6244, loss 0.537884.
Train: 2018-08-06T01:06:28.011840: step 6245, loss 0.6156.
Train: 2018-08-06T01:06:28.260176: step 6246, loss 0.575213.
Train: 2018-08-06T01:06:28.509507: step 6247, loss 0.520773.
Train: 2018-08-06T01:06:28.762830: step 6248, loss 0.481979.
Train: 2018-08-06T01:06:29.012191: step 6249, loss 0.566184.
Train: 2018-08-06T01:06:29.261527: step 6250, loss 0.599332.
Test: 2018-08-06T01:06:30.525117: step 6250, loss 0.54772.
Train: 2018-08-06T01:06:30.765476: step 6251, loss 0.529213.
Train: 2018-08-06T01:06:31.009854: step 6252, loss 0.553206.
Train: 2018-08-06T01:06:31.261180: step 6253, loss 0.659419.
Train: 2018-08-06T01:06:31.509513: step 6254, loss 0.583532.
Train: 2018-08-06T01:06:31.754854: step 6255, loss 0.536558.
Train: 2018-08-06T01:06:32.003166: step 6256, loss 0.579352.
Train: 2018-08-06T01:06:32.251502: step 6257, loss 0.596622.
Train: 2018-08-06T01:06:32.497873: step 6258, loss 0.531339.
Train: 2018-08-06T01:06:32.740193: step 6259, loss 0.627275.
Train: 2018-08-06T01:06:32.994515: step 6260, loss 0.510165.
Test: 2018-08-06T01:06:34.252150: step 6260, loss 0.551569.
Train: 2018-08-06T01:06:34.486524: step 6261, loss 0.548881.
Train: 2018-08-06T01:06:34.732896: step 6262, loss 0.601978.
Train: 2018-08-06T01:06:34.986187: step 6263, loss 0.54867.
Train: 2018-08-06T01:06:35.243500: step 6264, loss 0.556322.
Train: 2018-08-06T01:06:35.493830: step 6265, loss 0.541212.
Train: 2018-08-06T01:06:35.739204: step 6266, loss 0.541118.
Train: 2018-08-06T01:06:35.989534: step 6267, loss 0.556206.
Train: 2018-08-06T01:06:36.237873: step 6268, loss 0.525565.
Train: 2018-08-06T01:06:36.486182: step 6269, loss 0.594322.
Train: 2018-08-06T01:06:36.742526: step 6270, loss 0.632837.
Test: 2018-08-06T01:06:38.002122: step 6270, loss 0.551728.
Train: 2018-08-06T01:06:38.239512: step 6271, loss 0.594357.
Train: 2018-08-06T01:06:38.487822: step 6272, loss 0.548112.
Train: 2018-08-06T01:06:38.735162: step 6273, loss 0.55575.
Train: 2018-08-06T01:06:38.986490: step 6274, loss 0.648604.
Train: 2018-08-06T01:06:39.244798: step 6275, loss 0.586684.
Train: 2018-08-06T01:06:39.493165: step 6276, loss 0.540254.
Train: 2018-08-06T01:06:39.740472: step 6277, loss 0.532519.
Train: 2018-08-06T01:06:39.988808: step 6278, loss 0.594423.
Train: 2018-08-06T01:06:40.237144: step 6279, loss 0.547919.
Train: 2018-08-06T01:06:40.485481: step 6280, loss 0.470238.
Test: 2018-08-06T01:06:41.754087: step 6280, loss 0.550106.
Train: 2018-08-06T01:06:42.002451: step 6281, loss 0.609988.
Train: 2018-08-06T01:06:42.302764: step 6282, loss 0.555467.
Train: 2018-08-06T01:06:42.556088: step 6283, loss 0.492693.
Train: 2018-08-06T01:06:42.815360: step 6284, loss 0.555145.
Train: 2018-08-06T01:06:43.066721: step 6285, loss 0.594665.
Train: 2018-08-06T01:06:43.314060: step 6286, loss 0.475766.
Train: 2018-08-06T01:06:43.561365: step 6287, loss 0.515005.
Train: 2018-08-06T01:06:43.816716: step 6288, loss 0.530728.
Train: 2018-08-06T01:06:44.064020: step 6289, loss 0.627312.
Train: 2018-08-06T01:06:44.318371: step 6290, loss 0.530244.
Test: 2018-08-06T01:06:45.589940: step 6290, loss 0.548667.
Train: 2018-08-06T01:06:45.829333: step 6291, loss 0.530008.
Train: 2018-08-06T01:06:46.087637: step 6292, loss 0.570801.
Train: 2018-08-06T01:06:46.333981: step 6293, loss 0.603678.
Train: 2018-08-06T01:06:46.582316: step 6294, loss 0.513683.
Train: 2018-08-06T01:06:46.827630: step 6295, loss 0.587212.
Train: 2018-08-06T01:06:47.082947: step 6296, loss 0.521041.
Train: 2018-08-06T01:06:47.327323: step 6297, loss 0.471055.
Train: 2018-08-06T01:06:47.572669: step 6298, loss 0.60412.
Train: 2018-08-06T01:06:47.827985: step 6299, loss 0.587562.
Train: 2018-08-06T01:06:48.088284: step 6300, loss 0.537215.
Test: 2018-08-06T01:06:49.349885: step 6300, loss 0.548521.
Train: 2018-08-06T01:06:50.313407: step 6301, loss 0.503501.
Train: 2018-08-06T01:06:50.556757: step 6302, loss 0.596187.
Train: 2018-08-06T01:06:50.816062: step 6303, loss 0.570809.
Train: 2018-08-06T01:06:51.073375: step 6304, loss 0.511563.
Train: 2018-08-06T01:06:51.324735: step 6305, loss 0.596295.
Train: 2018-08-06T01:06:51.574036: step 6306, loss 0.528365.
Train: 2018-08-06T01:06:51.832376: step 6307, loss 0.511309.
Train: 2018-08-06T01:06:52.087688: step 6308, loss 0.51118.
Train: 2018-08-06T01:06:52.333031: step 6309, loss 0.647919.
Train: 2018-08-06T01:06:52.585344: step 6310, loss 0.570974.
Test: 2018-08-06T01:06:53.861917: step 6310, loss 0.548635.
Train: 2018-08-06T01:06:54.105267: step 6311, loss 0.519552.
Train: 2018-08-06T01:06:54.353633: step 6312, loss 0.596588.
Train: 2018-08-06T01:06:54.604940: step 6313, loss 0.596555.
Train: 2018-08-06T01:06:54.851272: step 6314, loss 0.545227.
Train: 2018-08-06T01:06:55.105622: step 6315, loss 0.528194.
Train: 2018-08-06T01:06:55.362904: step 6316, loss 0.511152.
Train: 2018-08-06T01:06:55.614257: step 6317, loss 0.553806.
Train: 2018-08-06T01:06:55.867583: step 6318, loss 0.639176.
Train: 2018-08-06T01:06:56.120876: step 6319, loss 0.502664.
Train: 2018-08-06T01:06:56.367217: step 6320, loss 0.57938.
Test: 2018-08-06T01:06:57.618870: step 6320, loss 0.547533.
Train: 2018-08-06T01:06:57.858230: step 6321, loss 0.553829.
Train: 2018-08-06T01:06:58.114575: step 6322, loss 0.587871.
Train: 2018-08-06T01:06:58.361935: step 6323, loss 0.56234.
Train: 2018-08-06T01:06:58.607257: step 6324, loss 0.536906.
Train: 2018-08-06T01:06:58.861546: step 6325, loss 0.519991.
Train: 2018-08-06T01:06:59.104896: step 6326, loss 0.562354.
Train: 2018-08-06T01:06:59.367231: step 6327, loss 0.604688.
Train: 2018-08-06T01:06:59.613536: step 6328, loss 0.503177.
Train: 2018-08-06T01:06:59.862869: step 6329, loss 0.570813.
Train: 2018-08-06T01:07:00.111205: step 6330, loss 0.621502.
Test: 2018-08-06T01:07:01.366846: step 6330, loss 0.548289.
Train: 2018-08-06T01:07:01.609198: step 6331, loss 0.528645.
Train: 2018-08-06T01:07:01.856562: step 6332, loss 0.553953.
Train: 2018-08-06T01:07:02.110856: step 6333, loss 0.545556.
Train: 2018-08-06T01:07:02.367171: step 6334, loss 0.562383.
Train: 2018-08-06T01:07:02.621492: step 6335, loss 0.528793.
Train: 2018-08-06T01:07:02.867858: step 6336, loss 0.553991.
Train: 2018-08-06T01:07:03.113177: step 6337, loss 0.604372.
Train: 2018-08-06T01:07:03.362540: step 6338, loss 0.595941.
Train: 2018-08-06T01:07:03.609848: step 6339, loss 0.545664.
Train: 2018-08-06T01:07:03.866163: step 6340, loss 0.50391.
Test: 2018-08-06T01:07:05.125793: step 6340, loss 0.547305.
Train: 2018-08-06T01:07:05.374129: step 6341, loss 0.545702.
Train: 2018-08-06T01:07:05.572599: step 6342, loss 0.580241.
Train: 2018-08-06T01:07:05.835925: step 6343, loss 0.495592.
Train: 2018-08-06T01:07:06.094204: step 6344, loss 0.55405.
Train: 2018-08-06T01:07:06.336556: step 6345, loss 0.562405.
Train: 2018-08-06T01:07:06.584892: step 6346, loss 0.570779.
Train: 2018-08-06T01:07:06.847198: step 6347, loss 0.57078.
Train: 2018-08-06T01:07:07.102540: step 6348, loss 0.554022.
Train: 2018-08-06T01:07:07.352869: step 6349, loss 0.528891.
Train: 2018-08-06T01:07:07.600176: step 6350, loss 0.537249.
Test: 2018-08-06T01:07:08.880751: step 6350, loss 0.546833.
Train: 2018-08-06T01:07:09.185999: step 6351, loss 0.503656.
Train: 2018-08-06T01:07:09.434333: step 6352, loss 0.629647.
Train: 2018-08-06T01:07:09.685664: step 6353, loss 0.570791.
Train: 2018-08-06T01:07:09.935993: step 6354, loss 0.638055.
Train: 2018-08-06T01:07:10.182333: step 6355, loss 0.570783.
Train: 2018-08-06T01:07:10.426683: step 6356, loss 0.579149.
Train: 2018-08-06T01:07:10.676979: step 6357, loss 0.537358.
Train: 2018-08-06T01:07:10.925342: step 6358, loss 0.64581.
Train: 2018-08-06T01:07:11.172654: step 6359, loss 0.579068.
Train: 2018-08-06T01:07:11.418023: step 6360, loss 0.645254.
Test: 2018-08-06T01:07:12.697575: step 6360, loss 0.548394.
Train: 2018-08-06T01:07:12.934966: step 6361, loss 0.562524.
Train: 2018-08-06T01:07:13.182280: step 6362, loss 0.628117.
Train: 2018-08-06T01:07:13.427654: step 6363, loss 0.652224.
Train: 2018-08-06T01:07:13.676956: step 6364, loss 0.546555.
Train: 2018-08-06T01:07:13.923329: step 6365, loss 0.506581.
Train: 2018-08-06T01:07:14.172662: step 6366, loss 0.546873.
Train: 2018-08-06T01:07:14.426982: step 6367, loss 0.562919.
Train: 2018-08-06T01:07:14.676285: step 6368, loss 0.555018.
Train: 2018-08-06T01:07:14.926646: step 6369, loss 0.547146.
Train: 2018-08-06T01:07:15.181962: step 6370, loss 0.563028.
Test: 2018-08-06T01:07:16.457520: step 6370, loss 0.550392.
Train: 2018-08-06T01:07:16.704859: step 6371, loss 0.578861.
Train: 2018-08-06T01:07:16.958213: step 6372, loss 0.539387.
Train: 2018-08-06T01:07:17.205548: step 6373, loss 0.531515.
Train: 2018-08-06T01:07:17.451860: step 6374, loss 0.578862.
Train: 2018-08-06T01:07:17.698228: step 6375, loss 0.539372.
Train: 2018-08-06T01:07:17.947535: step 6376, loss 0.61839.
Train: 2018-08-06T01:07:18.192910: step 6377, loss 0.570958.
Train: 2018-08-06T01:07:18.439221: step 6378, loss 0.515654.
Train: 2018-08-06T01:07:18.686590: step 6379, loss 0.507675.
Train: 2018-08-06T01:07:18.946894: step 6380, loss 0.578859.
Test: 2018-08-06T01:07:20.206494: step 6380, loss 0.549881.
Train: 2018-08-06T01:07:20.444899: step 6381, loss 0.515296.
Train: 2018-08-06T01:07:20.705191: step 6382, loss 0.499157.
Train: 2018-08-06T01:07:20.951533: step 6383, loss 0.570858.
Train: 2018-08-06T01:07:21.195849: step 6384, loss 0.602985.
Train: 2018-08-06T01:07:21.457149: step 6385, loss 0.562762.
Train: 2018-08-06T01:07:21.704515: step 6386, loss 0.562731.
Train: 2018-08-06T01:07:21.963819: step 6387, loss 0.586991.
Train: 2018-08-06T01:07:22.214149: step 6388, loss 0.546469.
Train: 2018-08-06T01:07:22.462462: step 6389, loss 0.522055.
Train: 2018-08-06T01:07:22.707805: step 6390, loss 0.603348.
Test: 2018-08-06T01:07:23.983393: step 6390, loss 0.547691.
Train: 2018-08-06T01:07:24.225754: step 6391, loss 0.603388.
Train: 2018-08-06T01:07:24.470091: step 6392, loss 0.578929.
Train: 2018-08-06T01:07:24.720436: step 6393, loss 0.513699.
Train: 2018-08-06T01:07:24.966764: step 6394, loss 0.554449.
Train: 2018-08-06T01:07:25.213131: step 6395, loss 0.489065.
Train: 2018-08-06T01:07:25.462444: step 6396, loss 0.570763.
Train: 2018-08-06T01:07:25.709782: step 6397, loss 0.620023.
Train: 2018-08-06T01:07:25.962139: step 6398, loss 0.58719.
Train: 2018-08-06T01:07:26.215429: step 6399, loss 0.578974.
Train: 2018-08-06T01:07:26.475770: step 6400, loss 0.562549.
Test: 2018-08-06T01:07:27.747332: step 6400, loss 0.548281.
Train: 2018-08-06T01:07:28.663146: step 6401, loss 0.52152.
Train: 2018-08-06T01:07:28.919436: step 6402, loss 0.513285.
Train: 2018-08-06T01:07:29.166798: step 6403, loss 0.6201.
Train: 2018-08-06T01:07:29.414139: step 6404, loss 0.537864.
Train: 2018-08-06T01:07:29.659489: step 6405, loss 0.521389.
Train: 2018-08-06T01:07:29.905796: step 6406, loss 0.537798.
Train: 2018-08-06T01:07:30.157158: step 6407, loss 0.529484.
Train: 2018-08-06T01:07:30.409481: step 6408, loss 0.57903.
Train: 2018-08-06T01:07:30.667759: step 6409, loss 0.562471.
Train: 2018-08-06T01:07:30.915100: step 6410, loss 0.595652.
Test: 2018-08-06T01:07:32.170739: step 6410, loss 0.548977.
Train: 2018-08-06T01:07:32.407140: step 6411, loss 0.545856.
Train: 2018-08-06T01:07:32.654445: step 6412, loss 0.537535.
Train: 2018-08-06T01:07:32.908797: step 6413, loss 0.579076.
Train: 2018-08-06T01:07:33.159123: step 6414, loss 0.562443.
Train: 2018-08-06T01:07:33.407432: step 6415, loss 0.579083.
Train: 2018-08-06T01:07:33.666770: step 6416, loss 0.579081.
Train: 2018-08-06T01:07:33.915100: step 6417, loss 0.554133.
Train: 2018-08-06T01:07:34.161416: step 6418, loss 0.562451.
Train: 2018-08-06T01:07:34.412744: step 6419, loss 0.562455.
Train: 2018-08-06T01:07:34.660083: step 6420, loss 0.487763.
Test: 2018-08-06T01:07:35.925697: step 6420, loss 0.547428.
Train: 2018-08-06T01:07:36.165057: step 6421, loss 0.554143.
Train: 2018-08-06T01:07:36.413393: step 6422, loss 0.554127.
Train: 2018-08-06T01:07:36.663725: step 6423, loss 0.620719.
Train: 2018-08-06T01:07:36.905078: step 6424, loss 0.570762.
Train: 2018-08-06T01:07:37.152452: step 6425, loss 0.537498.
Train: 2018-08-06T01:07:37.399791: step 6426, loss 0.637273.
Train: 2018-08-06T01:07:37.649088: step 6427, loss 0.520973.
Train: 2018-08-06T01:07:37.899448: step 6428, loss 0.579047.
Train: 2018-08-06T01:07:38.151771: step 6429, loss 0.562478.
Train: 2018-08-06T01:07:38.398116: step 6430, loss 0.570756.
Test: 2018-08-06T01:07:39.684644: step 6430, loss 0.54833.
Train: 2018-08-06T01:07:39.922009: step 6431, loss 0.58727.
Train: 2018-08-06T01:07:40.169348: step 6432, loss 0.529551.
Train: 2018-08-06T01:07:40.410703: step 6433, loss 0.57899.
Train: 2018-08-06T01:07:40.661034: step 6434, loss 0.496761.
Train: 2018-08-06T01:07:40.919344: step 6435, loss 0.562534.
Train: 2018-08-06T01:07:41.166682: step 6436, loss 0.58721.
Train: 2018-08-06T01:07:41.424024: step 6437, loss 0.562536.
Train: 2018-08-06T01:07:41.673354: step 6438, loss 0.578978.
Train: 2018-08-06T01:07:41.932657: step 6439, loss 0.595397.
Train: 2018-08-06T01:07:42.182989: step 6440, loss 0.570762.
Test: 2018-08-06T01:07:43.443592: step 6440, loss 0.547776.
Train: 2018-08-06T01:07:43.685969: step 6441, loss 0.603511.
Train: 2018-08-06T01:07:43.941292: step 6442, loss 0.652435.
Train: 2018-08-06T01:07:44.187633: step 6443, loss 0.587045.
Train: 2018-08-06T01:07:44.438961: step 6444, loss 0.586989.
Train: 2018-08-06T01:07:44.687266: step 6445, loss 0.522478.
Train: 2018-08-06T01:07:44.947569: step 6446, loss 0.602967.
Train: 2018-08-06T01:07:45.194933: step 6447, loss 0.626877.
Train: 2018-08-06T01:07:45.450255: step 6448, loss 0.594786.
Train: 2018-08-06T01:07:45.708535: step 6449, loss 0.53132.
Train: 2018-08-06T01:07:45.954877: step 6450, loss 0.484128.
Test: 2018-08-06T01:07:47.228470: step 6450, loss 0.54974.
Train: 2018-08-06T01:07:47.470841: step 6451, loss 0.578864.
Train: 2018-08-06T01:07:47.716166: step 6452, loss 0.563113.
Train: 2018-08-06T01:07:47.963535: step 6453, loss 0.570998.
Train: 2018-08-06T01:07:48.213863: step 6454, loss 0.547417.
Train: 2018-08-06T01:07:48.461174: step 6455, loss 0.571007.
Train: 2018-08-06T01:07:48.716492: step 6456, loss 0.602453.
Train: 2018-08-06T01:07:48.962832: step 6457, loss 0.563164.
Train: 2018-08-06T01:07:49.207179: step 6458, loss 0.53963.
Train: 2018-08-06T01:07:49.454517: step 6459, loss 0.516067.
Train: 2018-08-06T01:07:49.702854: step 6460, loss 0.508098.
Test: 2018-08-06T01:07:50.961486: step 6460, loss 0.549915.
Train: 2018-08-06T01:07:51.199850: step 6461, loss 0.610414.
Train: 2018-08-06T01:07:51.447188: step 6462, loss 0.531454.
Train: 2018-08-06T01:07:51.694526: step 6463, loss 0.539252.
Train: 2018-08-06T01:07:51.944856: step 6464, loss 0.49146.
Train: 2018-08-06T01:07:52.191229: step 6465, loss 0.538951.
Train: 2018-08-06T01:07:52.439564: step 6466, loss 0.594913.
Train: 2018-08-06T01:07:52.694853: step 6467, loss 0.52252.
Train: 2018-08-06T01:07:52.939227: step 6468, loss 0.53037.
Train: 2018-08-06T01:07:53.189530: step 6469, loss 0.53017.
Train: 2018-08-06T01:07:53.443849: step 6470, loss 0.55444.
Test: 2018-08-06T01:07:54.690514: step 6470, loss 0.549228.
Train: 2018-08-06T01:07:54.927881: step 6471, loss 0.603608.
Train: 2018-08-06T01:07:55.171253: step 6472, loss 0.587187.
Train: 2018-08-06T01:07:55.418570: step 6473, loss 0.504746.
Train: 2018-08-06T01:07:55.667902: step 6474, loss 0.620431.
Train: 2018-08-06T01:07:55.911248: step 6475, loss 0.545916.
Train: 2018-08-06T01:07:56.162602: step 6476, loss 0.504367.
Train: 2018-08-06T01:07:56.408918: step 6477, loss 0.570779.
Train: 2018-08-06T01:07:56.660247: step 6478, loss 0.554084.
Train: 2018-08-06T01:07:56.905590: step 6479, loss 0.495549.
Train: 2018-08-06T01:07:57.152960: step 6480, loss 0.537243.
Test: 2018-08-06T01:07:58.431509: step 6480, loss 0.548525.
Train: 2018-08-06T01:07:58.668874: step 6481, loss 0.60444.
Train: 2018-08-06T01:07:58.913222: step 6482, loss 0.545518.
Train: 2018-08-06T01:07:59.159563: step 6483, loss 0.579252.
Train: 2018-08-06T01:07:59.405934: step 6484, loss 0.587719.
Train: 2018-08-06T01:07:59.663241: step 6485, loss 0.520086.
Train: 2018-08-06T01:07:59.909587: step 6486, loss 0.613138.
Train: 2018-08-06T01:08:00.159888: step 6487, loss 0.604659.
Train: 2018-08-06T01:08:00.405258: step 6488, loss 0.562363.
Train: 2018-08-06T01:08:00.652594: step 6489, loss 0.579237.
Train: 2018-08-06T01:08:00.910909: step 6490, loss 0.495035.
Test: 2018-08-06T01:08:02.203421: step 6490, loss 0.548132.
Train: 2018-08-06T01:08:02.449764: step 6491, loss 0.587621.
Train: 2018-08-06T01:08:02.695137: step 6492, loss 0.537168.
Train: 2018-08-06T01:08:02.891606: step 6493, loss 0.472782.
Train: 2018-08-06T01:08:03.146898: step 6494, loss 0.579204.
Train: 2018-08-06T01:08:03.396258: step 6495, loss 0.55396.
Train: 2018-08-06T01:08:03.641576: step 6496, loss 0.57922.
Train: 2018-08-06T01:08:03.888950: step 6497, loss 0.57922.
Train: 2018-08-06T01:08:04.137254: step 6498, loss 0.587631.
Train: 2018-08-06T01:08:04.385631: step 6499, loss 0.4783.
Train: 2018-08-06T01:08:04.630956: step 6500, loss 0.49507.
Test: 2018-08-06T01:08:05.928460: step 6500, loss 0.548874.
Train: 2018-08-06T01:08:06.825153: step 6501, loss 0.570801.
Train: 2018-08-06T01:08:07.071520: step 6502, loss 0.562366.
Train: 2018-08-06T01:08:07.317836: step 6503, loss 0.52856.
Train: 2018-08-06T01:08:07.571158: step 6504, loss 0.630063.
Train: 2018-08-06T01:08:07.820495: step 6505, loss 0.630042.
Train: 2018-08-06T01:08:08.063869: step 6506, loss 0.553921.
Train: 2018-08-06T01:08:08.314206: step 6507, loss 0.638229.
Train: 2018-08-06T01:08:08.558543: step 6508, loss 0.529414.
Train: 2018-08-06T01:08:08.802901: step 6509, loss 0.562552.
Train: 2018-08-06T01:08:09.058208: step 6510, loss 0.529908.
Test: 2018-08-06T01:08:10.342746: step 6510, loss 0.549566.
Train: 2018-08-06T01:08:10.580136: step 6511, loss 0.44745.
Train: 2018-08-06T01:08:10.841413: step 6512, loss 0.537327.
Train: 2018-08-06T01:08:11.088782: step 6513, loss 0.520521.
Train: 2018-08-06T01:08:11.349056: step 6514, loss 0.57918.
Train: 2018-08-06T01:08:11.596394: step 6515, loss 0.486698.
Train: 2018-08-06T01:08:11.847722: step 6516, loss 0.672013.
Train: 2018-08-06T01:08:12.096087: step 6517, loss 0.494893.
Train: 2018-08-06T01:08:12.348383: step 6518, loss 0.562364.
Train: 2018-08-06T01:08:12.595750: step 6519, loss 0.587725.
Train: 2018-08-06T01:08:12.856026: step 6520, loss 0.56236.
Test: 2018-08-06T01:08:14.115656: step 6520, loss 0.547264.
Train: 2018-08-06T01:08:14.360034: step 6521, loss 0.613096.
Train: 2018-08-06T01:08:14.609361: step 6522, loss 0.570809.
Train: 2018-08-06T01:08:14.854681: step 6523, loss 0.638252.
Train: 2018-08-06T01:08:15.103042: step 6524, loss 0.587576.
Train: 2018-08-06T01:08:15.357335: step 6525, loss 0.487102.
Train: 2018-08-06T01:08:15.603677: step 6526, loss 0.595837.
Train: 2018-08-06T01:08:15.857000: step 6527, loss 0.570765.
Train: 2018-08-06T01:08:16.106333: step 6528, loss 0.5874.
Train: 2018-08-06T01:08:16.351703: step 6529, loss 0.521002.
Train: 2018-08-06T01:08:16.600012: step 6530, loss 0.554198.
Test: 2018-08-06T01:08:17.878592: step 6530, loss 0.548297.
Train: 2018-08-06T01:08:18.125956: step 6531, loss 0.496343.
Train: 2018-08-06T01:08:18.374301: step 6532, loss 0.570756.
Train: 2018-08-06T01:08:18.619612: step 6533, loss 0.595562.
Train: 2018-08-06T01:08:18.873957: step 6534, loss 0.537719.
Train: 2018-08-06T01:08:19.120272: step 6535, loss 0.512964.
Train: 2018-08-06T01:08:19.370602: step 6536, loss 0.570755.
Train: 2018-08-06T01:08:19.616972: step 6537, loss 0.570758.
Train: 2018-08-06T01:08:19.862287: step 6538, loss 0.545959.
Train: 2018-08-06T01:08:20.112649: step 6539, loss 0.545947.
Train: 2018-08-06T01:08:20.356965: step 6540, loss 0.620406.
Test: 2018-08-06T01:08:21.654494: step 6540, loss 0.549061.
Train: 2018-08-06T01:08:21.890862: step 6541, loss 0.579023.
Train: 2018-08-06T01:08:22.139229: step 6542, loss 0.570756.
Train: 2018-08-06T01:08:22.384569: step 6543, loss 0.611995.
Train: 2018-08-06T01:08:22.645844: step 6544, loss 0.554306.
Train: 2018-08-06T01:08:22.895183: step 6545, loss 0.56255.
Train: 2018-08-06T01:08:23.141549: step 6546, loss 0.644529.
Train: 2018-08-06T01:08:23.389885: step 6547, loss 0.529936.
Train: 2018-08-06T01:08:23.652184: step 6548, loss 0.513751.
Train: 2018-08-06T01:08:23.896500: step 6549, loss 0.481267.
Train: 2018-08-06T01:08:24.138850: step 6550, loss 0.554486.
Test: 2018-08-06T01:08:25.414439: step 6550, loss 0.548252.
Train: 2018-08-06T01:08:25.650807: step 6551, loss 0.619697.
Train: 2018-08-06T01:08:25.906131: step 6552, loss 0.562624.
Train: 2018-08-06T01:08:26.154490: step 6553, loss 0.595224.
Train: 2018-08-06T01:08:26.407809: step 6554, loss 0.473121.
Train: 2018-08-06T01:08:26.655140: step 6555, loss 0.554478.
Train: 2018-08-06T01:08:26.902460: step 6556, loss 0.546299.
Train: 2018-08-06T01:08:27.150796: step 6557, loss 0.627962.
Train: 2018-08-06T01:08:27.401127: step 6558, loss 0.521757.
Train: 2018-08-06T01:08:27.646469: step 6559, loss 0.521718.
Train: 2018-08-06T01:08:27.898824: step 6560, loss 0.578953.
Test: 2018-08-06T01:08:29.189344: step 6560, loss 0.550008.
Train: 2018-08-06T01:08:29.428704: step 6561, loss 0.570763.
Train: 2018-08-06T01:08:29.683054: step 6562, loss 0.570761.
Train: 2018-08-06T01:08:29.932356: step 6563, loss 0.513315.
Train: 2018-08-06T01:08:30.178729: step 6564, loss 0.587197.
Train: 2018-08-06T01:08:30.432021: step 6565, loss 0.554308.
Train: 2018-08-06T01:08:30.681410: step 6566, loss 0.653075.
Train: 2018-08-06T01:08:30.928718: step 6567, loss 0.595419.
Train: 2018-08-06T01:08:31.181019: step 6568, loss 0.628183.
Train: 2018-08-06T01:08:31.427359: step 6569, loss 0.570768.
Train: 2018-08-06T01:08:31.680691: step 6570, loss 0.538184.
Test: 2018-08-06T01:08:32.951283: step 6570, loss 0.549454.
Train: 2018-08-06T01:08:33.187652: step 6571, loss 0.676453.
Train: 2018-08-06T01:08:33.434017: step 6572, loss 0.562713.
Train: 2018-08-06T01:08:33.679337: step 6573, loss 0.554715.
Train: 2018-08-06T01:08:33.924711: step 6574, loss 0.594918.
Train: 2018-08-06T01:08:34.185018: step 6575, loss 0.586855.
Train: 2018-08-06T01:08:34.432322: step 6576, loss 0.55498.
Train: 2018-08-06T01:08:34.680686: step 6577, loss 0.570926.
Train: 2018-08-06T01:08:34.928996: step 6578, loss 0.563044.
Train: 2018-08-06T01:08:35.182317: step 6579, loss 0.626188.
Train: 2018-08-06T01:08:35.428658: step 6580, loss 0.594585.
Test: 2018-08-06T01:08:36.703249: step 6580, loss 0.551241.
Train: 2018-08-06T01:08:36.939618: step 6581, loss 0.5241.
Train: 2018-08-06T01:08:37.184960: step 6582, loss 0.571082.
Train: 2018-08-06T01:08:37.433325: step 6583, loss 0.524366.
Train: 2018-08-06T01:08:37.683657: step 6584, loss 0.571115.
Train: 2018-08-06T01:08:37.931964: step 6585, loss 0.54.
Train: 2018-08-06T01:08:38.178339: step 6586, loss 0.532203.
Train: 2018-08-06T01:08:38.428635: step 6587, loss 0.571102.
Train: 2018-08-06T01:08:38.674976: step 6588, loss 0.57889.
Train: 2018-08-06T01:08:38.929321: step 6589, loss 0.586698.
Train: 2018-08-06T01:08:39.176659: step 6590, loss 0.524181.
Test: 2018-08-06T01:08:40.447236: step 6590, loss 0.54979.
Train: 2018-08-06T01:08:40.693578: step 6591, loss 0.602365.
Train: 2018-08-06T01:08:40.937925: step 6592, loss 0.594546.
Train: 2018-08-06T01:08:41.180277: step 6593, loss 0.555375.
Train: 2018-08-06T01:08:41.424623: step 6594, loss 0.571038.
Train: 2018-08-06T01:08:41.672993: step 6595, loss 0.594559.
Train: 2018-08-06T01:08:41.917333: step 6596, loss 0.586716.
Train: 2018-08-06T01:08:42.163677: step 6597, loss 0.586713.
Train: 2018-08-06T01:08:42.429935: step 6598, loss 0.547566.
Train: 2018-08-06T01:08:42.675306: step 6599, loss 0.594535.
Train: 2018-08-06T01:08:42.933622: step 6600, loss 0.524134.
Test: 2018-08-06T01:08:44.185239: step 6600, loss 0.54926.
Train: 2018-08-06T01:08:45.162144: step 6601, loss 0.56323.
Train: 2018-08-06T01:08:45.407495: step 6602, loss 0.563217.
Train: 2018-08-06T01:08:45.655830: step 6603, loss 0.571039.
Train: 2018-08-06T01:08:45.906164: step 6604, loss 0.523966.
Train: 2018-08-06T01:08:46.153498: step 6605, loss 0.492419.
Train: 2018-08-06T01:08:46.399841: step 6606, loss 0.59464.
Train: 2018-08-06T01:08:46.649169: step 6607, loss 0.63423.
Train: 2018-08-06T01:08:46.903492: step 6608, loss 0.570944.
Train: 2018-08-06T01:08:47.162794: step 6609, loss 0.563017.
Train: 2018-08-06T01:08:47.415095: step 6610, loss 0.507509.
Test: 2018-08-06T01:08:48.674725: step 6610, loss 0.550064.
Train: 2018-08-06T01:08:48.915083: step 6611, loss 0.555022.
Train: 2018-08-06T01:08:49.159477: step 6612, loss 0.547003.
Train: 2018-08-06T01:08:49.405770: step 6613, loss 0.538936.
Train: 2018-08-06T01:08:49.650150: step 6614, loss 0.498772.
Train: 2018-08-06T01:08:49.897487: step 6615, loss 0.578874.
Train: 2018-08-06T01:08:50.143797: step 6616, loss 0.538516.
Train: 2018-08-06T01:08:50.391171: step 6617, loss 0.514058.
Train: 2018-08-06T01:08:50.640500: step 6618, loss 0.587064.
Train: 2018-08-06T01:08:50.888805: step 6619, loss 0.521732.
Train: 2018-08-06T01:08:51.133151: step 6620, loss 0.595382.
Test: 2018-08-06T01:08:52.406745: step 6620, loss 0.547832.
Train: 2018-08-06T01:08:52.643114: step 6621, loss 0.570758.
Train: 2018-08-06T01:08:52.898461: step 6622, loss 0.620264.
Train: 2018-08-06T01:08:53.142822: step 6623, loss 0.529468.
Train: 2018-08-06T01:08:53.397121: step 6624, loss 0.595564.
Train: 2018-08-06T01:08:53.642469: step 6625, loss 0.595574.
Train: 2018-08-06T01:08:53.885789: step 6626, loss 0.504606.
Train: 2018-08-06T01:08:54.128141: step 6627, loss 0.529377.
Train: 2018-08-06T01:08:54.376477: step 6628, loss 0.512743.
Train: 2018-08-06T01:08:54.618829: step 6629, loss 0.554145.
Train: 2018-08-06T01:08:54.866201: step 6630, loss 0.487506.
Test: 2018-08-06T01:08:56.134775: step 6630, loss 0.548268.
Train: 2018-08-06T01:08:56.371143: step 6631, loss 0.545704.
Train: 2018-08-06T01:08:56.619479: step 6632, loss 0.503706.
Train: 2018-08-06T01:08:56.867814: step 6633, loss 0.537115.
Train: 2018-08-06T01:08:57.117179: step 6634, loss 0.536993.
Train: 2018-08-06T01:08:57.364487: step 6635, loss 0.511403.
Train: 2018-08-06T01:08:57.612847: step 6636, loss 0.570869.
Train: 2018-08-06T01:08:57.858194: step 6637, loss 0.536649.
Train: 2018-08-06T01:08:58.105504: step 6638, loss 0.596709.
Train: 2018-08-06T01:08:58.350849: step 6639, loss 0.622629.
Train: 2018-08-06T01:08:58.602210: step 6640, loss 0.545102.
Test: 2018-08-06T01:08:59.872778: step 6640, loss 0.546945.
Train: 2018-08-06T01:09:00.122112: step 6641, loss 0.519227.
Train: 2018-08-06T01:09:00.382447: step 6642, loss 0.605497.
Train: 2018-08-06T01:09:00.638755: step 6643, loss 0.570968.
Train: 2018-08-06T01:09:00.838222: step 6644, loss 0.562338.
Train: 2018-08-06T01:09:01.098525: step 6645, loss 0.570954.
Train: 2018-08-06T01:09:01.346870: step 6646, loss 0.613975.
Train: 2018-08-06T01:09:01.596194: step 6647, loss 0.60526.
Train: 2018-08-06T01:09:01.842536: step 6648, loss 0.459666.
Train: 2018-08-06T01:09:02.089880: step 6649, loss 0.553791.
Train: 2018-08-06T01:09:02.351175: step 6650, loss 0.562338.
Test: 2018-08-06T01:09:03.619759: step 6650, loss 0.547314.
Train: 2018-08-06T01:09:03.870114: step 6651, loss 0.553813.
Train: 2018-08-06T01:09:04.120419: step 6652, loss 0.536789.
Train: 2018-08-06T01:09:04.364766: step 6653, loss 0.613414.
Train: 2018-08-06T01:09:04.613101: step 6654, loss 0.638807.
Train: 2018-08-06T01:09:04.857448: step 6655, loss 0.494637.
Train: 2018-08-06T01:09:05.104787: step 6656, loss 0.596156.
Train: 2018-08-06T01:09:05.349134: step 6657, loss 0.537097.
Train: 2018-08-06T01:09:05.602456: step 6658, loss 0.52034.
Train: 2018-08-06T01:09:05.851789: step 6659, loss 0.553987.
Train: 2018-08-06T01:09:06.099134: step 6660, loss 0.579175.
Test: 2018-08-06T01:09:07.359756: step 6660, loss 0.547824.
Train: 2018-08-06T01:09:07.602141: step 6661, loss 0.629441.
Train: 2018-08-06T01:09:07.857450: step 6662, loss 0.620909.
Train: 2018-08-06T01:09:08.103797: step 6663, loss 0.495857.
Train: 2018-08-06T01:09:08.354123: step 6664, loss 0.512629.
Train: 2018-08-06T01:09:08.608425: step 6665, loss 0.587351.
Train: 2018-08-06T01:09:08.855754: step 6666, loss 0.570757.
Train: 2018-08-06T01:09:09.115063: step 6667, loss 0.595566.
Train: 2018-08-06T01:09:09.366414: step 6668, loss 0.570756.
Train: 2018-08-06T01:09:09.611759: step 6669, loss 0.546063.
Train: 2018-08-06T01:09:09.858103: step 6670, loss 0.587193.
Test: 2018-08-06T01:09:11.145631: step 6670, loss 0.547551.
Train: 2018-08-06T01:09:11.439844: step 6671, loss 0.570762.
Train: 2018-08-06T01:09:11.685188: step 6672, loss 0.538045.
Train: 2018-08-06T01:09:11.937546: step 6673, loss 0.652454.
Train: 2018-08-06T01:09:12.182882: step 6674, loss 0.562638.
Train: 2018-08-06T01:09:12.432216: step 6675, loss 0.57079.
Train: 2018-08-06T01:09:12.682521: step 6676, loss 0.570801.
Train: 2018-08-06T01:09:12.929893: step 6677, loss 0.570814.
Train: 2018-08-06T01:09:13.177227: step 6678, loss 0.619109.
Train: 2018-08-06T01:09:13.422560: step 6679, loss 0.586884.
Train: 2018-08-06T01:09:13.668911: step 6680, loss 0.594833.
Test: 2018-08-06T01:09:14.926520: step 6680, loss 0.549855.
Train: 2018-08-06T01:09:15.172885: step 6681, loss 0.555001.
Train: 2018-08-06T01:09:15.417234: step 6682, loss 0.547158.
Train: 2018-08-06T01:09:15.668535: step 6683, loss 0.618386.
Train: 2018-08-06T01:09:15.916902: step 6684, loss 0.51585.
Train: 2018-08-06T01:09:16.171190: step 6685, loss 0.563144.
Train: 2018-08-06T01:09:16.418555: step 6686, loss 0.633832.
Train: 2018-08-06T01:09:16.660910: step 6687, loss 0.594537.
Train: 2018-08-06T01:09:16.906226: step 6688, loss 0.532068.
Train: 2018-08-06T01:09:17.147579: step 6689, loss 0.610054.
Train: 2018-08-06T01:09:17.396913: step 6690, loss 0.571139.
Test: 2018-08-06T01:09:18.690453: step 6690, loss 0.549759.
Train: 2018-08-06T01:09:18.939788: step 6691, loss 0.555666.
Train: 2018-08-06T01:09:19.186153: step 6692, loss 0.517019.
Train: 2018-08-06T01:09:19.445463: step 6693, loss 0.486058.
Train: 2018-08-06T01:09:19.704768: step 6694, loss 0.547885.
Train: 2018-08-06T01:09:19.951111: step 6695, loss 0.563339.
Train: 2018-08-06T01:09:20.200417: step 6696, loss 0.610097.
Train: 2018-08-06T01:09:20.447785: step 6697, loss 0.539814.
Train: 2018-08-06T01:09:20.698109: step 6698, loss 0.563193.
Train: 2018-08-06T01:09:20.944427: step 6699, loss 0.523916.
Train: 2018-08-06T01:09:21.192790: step 6700, loss 0.610516.
Test: 2018-08-06T01:09:22.463363: step 6700, loss 0.54988.
Train: 2018-08-06T01:09:23.388056: step 6701, loss 0.539394.
Train: 2018-08-06T01:09:23.635405: step 6702, loss 0.539276.
Train: 2018-08-06T01:09:23.881768: step 6703, loss 0.563019.
Train: 2018-08-06T01:09:24.129120: step 6704, loss 0.491332.
Train: 2018-08-06T01:09:24.401372: step 6705, loss 0.522898.
Train: 2018-08-06T01:09:24.646693: step 6706, loss 0.554866.
Train: 2018-08-06T01:09:24.909016: step 6707, loss 0.481838.
Train: 2018-08-06T01:09:25.155362: step 6708, loss 0.562521.
Train: 2018-08-06T01:09:25.400703: step 6709, loss 0.571167.
Train: 2018-08-06T01:09:25.659004: step 6710, loss 0.537477.
Test: 2018-08-06T01:09:26.935570: step 6710, loss 0.549313.
Train: 2018-08-06T01:09:27.174931: step 6711, loss 0.545598.
Train: 2018-08-06T01:09:27.420304: step 6712, loss 0.537249.
Train: 2018-08-06T01:09:27.668611: step 6713, loss 0.537155.
Train: 2018-08-06T01:09:27.914950: step 6714, loss 0.563552.
Train: 2018-08-06T01:09:28.157303: step 6715, loss 0.519949.
Train: 2018-08-06T01:09:28.405672: step 6716, loss 0.597286.
Train: 2018-08-06T01:09:28.649987: step 6717, loss 0.621552.
Train: 2018-08-06T01:09:28.895330: step 6718, loss 0.562653.
Train: 2018-08-06T01:09:29.144693: step 6719, loss 0.485439.
Train: 2018-08-06T01:09:29.393011: step 6720, loss 0.562587.
Test: 2018-08-06T01:09:30.648640: step 6720, loss 0.548514.
Train: 2018-08-06T01:09:30.887999: step 6721, loss 0.562222.
Train: 2018-08-06T01:09:31.132346: step 6722, loss 0.545058.
Train: 2018-08-06T01:09:31.380684: step 6723, loss 0.519827.
Train: 2018-08-06T01:09:31.628021: step 6724, loss 0.545136.
Train: 2018-08-06T01:09:31.878376: step 6725, loss 0.545499.
Train: 2018-08-06T01:09:32.129707: step 6726, loss 0.647914.
Train: 2018-08-06T01:09:32.380035: step 6727, loss 0.545401.
Train: 2018-08-06T01:09:32.623360: step 6728, loss 0.570934.
Train: 2018-08-06T01:09:32.872693: step 6729, loss 0.587927.
Train: 2018-08-06T01:09:33.120031: step 6730, loss 0.502728.
Test: 2018-08-06T01:09:34.376670: step 6730, loss 0.548527.
Train: 2018-08-06T01:09:34.614052: step 6731, loss 0.511293.
Train: 2018-08-06T01:09:34.859380: step 6732, loss 0.570855.
Train: 2018-08-06T01:09:35.107748: step 6733, loss 0.468702.
Train: 2018-08-06T01:09:35.363033: step 6734, loss 0.562339.
Train: 2018-08-06T01:09:35.611393: step 6735, loss 0.57088.
Train: 2018-08-06T01:09:35.855715: step 6736, loss 0.50248.
Train: 2018-08-06T01:09:36.106071: step 6737, loss 0.639442.
Train: 2018-08-06T01:09:36.355380: step 6738, loss 0.588035.
Train: 2018-08-06T01:09:36.601745: step 6739, loss 0.562337.
Train: 2018-08-06T01:09:36.846066: step 6740, loss 0.511047.
Test: 2018-08-06T01:09:38.103702: step 6740, loss 0.547275.
Train: 2018-08-06T01:09:38.343064: step 6741, loss 0.622171.
Train: 2018-08-06T01:09:38.591398: step 6742, loss 0.502608.
Train: 2018-08-06T01:09:38.838737: step 6743, loss 0.587927.
Train: 2018-08-06T01:09:39.087099: step 6744, loss 0.511222.
Train: 2018-08-06T01:09:39.345385: step 6745, loss 0.613451.
Train: 2018-08-06T01:09:39.591723: step 6746, loss 0.502806.
Train: 2018-08-06T01:09:39.836096: step 6747, loss 0.570847.
Train: 2018-08-06T01:09:40.085404: step 6748, loss 0.53685.
Train: 2018-08-06T01:09:40.334736: step 6749, loss 0.528361.
Train: 2018-08-06T01:09:40.584095: step 6750, loss 0.570846.
Test: 2018-08-06T01:09:41.849685: step 6750, loss 0.548739.
Train: 2018-08-06T01:09:42.088067: step 6751, loss 0.630335.
Train: 2018-08-06T01:09:42.333392: step 6752, loss 0.5878.
Train: 2018-08-06T01:09:42.593695: step 6753, loss 0.579284.
Train: 2018-08-06T01:09:42.840061: step 6754, loss 0.553925.
Train: 2018-08-06T01:09:43.087376: step 6755, loss 0.545534.
Train: 2018-08-06T01:09:43.333749: step 6756, loss 0.486749.
Train: 2018-08-06T01:09:43.582077: step 6757, loss 0.629601.
Train: 2018-08-06T01:09:43.840362: step 6758, loss 0.621098.
Train: 2018-08-06T01:09:44.084708: step 6759, loss 0.562412.
Train: 2018-08-06T01:09:44.331049: step 6760, loss 0.529089.
Test: 2018-08-06T01:09:45.614616: step 6760, loss 0.54836.
Train: 2018-08-06T01:09:45.857965: step 6761, loss 0.595716.
Train: 2018-08-06T01:09:46.115278: step 6762, loss 0.504391.
Train: 2018-08-06T01:09:46.366630: step 6763, loss 0.603901.
Train: 2018-08-06T01:09:46.611949: step 6764, loss 0.521145.
Train: 2018-08-06T01:09:46.859287: step 6765, loss 0.562496.
Train: 2018-08-06T01:09:47.103658: step 6766, loss 0.546.
Train: 2018-08-06T01:09:47.354988: step 6767, loss 0.504777.
Train: 2018-08-06T01:09:47.599310: step 6768, loss 0.545995.
Train: 2018-08-06T01:09:47.846647: step 6769, loss 0.579018.
Train: 2018-08-06T01:09:48.094983: step 6770, loss 0.537694.
Test: 2018-08-06T01:09:49.360597: step 6770, loss 0.549618.
Train: 2018-08-06T01:09:49.605942: step 6771, loss 0.521117.
Train: 2018-08-06T01:09:49.853280: step 6772, loss 0.579044.
Train: 2018-08-06T01:09:50.099622: step 6773, loss 0.570758.
Train: 2018-08-06T01:09:50.350980: step 6774, loss 0.653774.
Train: 2018-08-06T01:09:50.593301: step 6775, loss 0.587334.
Train: 2018-08-06T01:09:50.855628: step 6776, loss 0.645199.
Train: 2018-08-06T01:09:51.103937: step 6777, loss 0.562517.
Train: 2018-08-06T01:09:51.356260: step 6778, loss 0.546128.
Train: 2018-08-06T01:09:51.606592: step 6779, loss 0.578952.
Train: 2018-08-06T01:09:51.867893: step 6780, loss 0.578934.
Test: 2018-08-06T01:09:53.124531: step 6780, loss 0.549612.
Train: 2018-08-06T01:09:53.363922: step 6781, loss 0.546365.
Train: 2018-08-06T01:09:53.608263: step 6782, loss 0.55455.
Train: 2018-08-06T01:09:53.859592: step 6783, loss 0.538382.
Train: 2018-08-06T01:09:54.105938: step 6784, loss 0.546517.
Train: 2018-08-06T01:09:54.371226: step 6785, loss 0.635518.
Train: 2018-08-06T01:09:54.618536: step 6786, loss 0.627312.
Train: 2018-08-06T01:09:54.871883: step 6787, loss 0.611046.
Train: 2018-08-06T01:09:55.117204: step 6788, loss 0.554839.
Train: 2018-08-06T01:09:55.365538: step 6789, loss 0.54694.
Train: 2018-08-06T01:09:55.613875: step 6790, loss 0.539067.
Test: 2018-08-06T01:09:56.877495: step 6790, loss 0.549701.
Train: 2018-08-06T01:09:57.114860: step 6791, loss 0.562969.
Train: 2018-08-06T01:09:57.362199: step 6792, loss 0.531259.
Train: 2018-08-06T01:09:57.624528: step 6793, loss 0.531276.
Train: 2018-08-06T01:09:57.867872: step 6794, loss 0.555052.
Train: 2018-08-06T01:09:58.063325: step 6795, loss 0.562973.
Train: 2018-08-06T01:09:58.320668: step 6796, loss 0.61066.
Train: 2018-08-06T01:09:58.567974: step 6797, loss 0.499371.
Train: 2018-08-06T01:09:58.822323: step 6798, loss 0.578859.
Train: 2018-08-06T01:09:59.067637: step 6799, loss 0.634649.
Train: 2018-08-06T01:09:59.328967: step 6800, loss 0.51514.
Test: 2018-08-06T01:10:00.597546: step 6800, loss 0.548139.
Train: 2018-08-06T01:10:01.522166: step 6801, loss 0.682477.
Train: 2018-08-06T01:10:01.768482: step 6802, loss 0.547045.
Train: 2018-08-06T01:10:02.014822: step 6803, loss 0.555033.
Train: 2018-08-06T01:10:02.259194: step 6804, loss 0.570924.
Train: 2018-08-06T01:10:02.521467: step 6805, loss 0.547151.
Train: 2018-08-06T01:10:02.765847: step 6806, loss 0.586784.
Train: 2018-08-06T01:10:03.024149: step 6807, loss 0.578859.
Train: 2018-08-06T01:10:03.279471: step 6808, loss 0.507649.
Train: 2018-08-06T01:10:03.523813: step 6809, loss 0.547186.
Train: 2018-08-06T01:10:03.772154: step 6810, loss 0.602644.
Test: 2018-08-06T01:10:05.064666: step 6810, loss 0.549206.
Train: 2018-08-06T01:10:05.316010: step 6811, loss 0.586789.
Train: 2018-08-06T01:10:05.563348: step 6812, loss 0.467854.
Train: 2018-08-06T01:10:05.825661: step 6813, loss 0.578858.
Train: 2018-08-06T01:10:06.071006: step 6814, loss 0.562935.
Train: 2018-08-06T01:10:06.320308: step 6815, loss 0.570884.
Train: 2018-08-06T01:10:06.567648: step 6816, loss 0.594838.
Train: 2018-08-06T01:10:06.815982: step 6817, loss 0.610838.
Train: 2018-08-06T01:10:07.062354: step 6818, loss 0.578862.
Train: 2018-08-06T01:10:07.314680: step 6819, loss 0.562891.
Train: 2018-08-06T01:10:07.562012: step 6820, loss 0.57886.
Test: 2018-08-06T01:10:08.847549: step 6820, loss 0.550876.
Train: 2018-08-06T01:10:09.095885: step 6821, loss 0.523036.
Train: 2018-08-06T01:10:09.346234: step 6822, loss 0.554925.
Train: 2018-08-06T01:10:09.611506: step 6823, loss 0.491037.
Train: 2018-08-06T01:10:09.862835: step 6824, loss 0.578864.
Train: 2018-08-06T01:10:10.109175: step 6825, loss 0.490651.
Train: 2018-08-06T01:10:10.358545: step 6826, loss 0.474243.
Train: 2018-08-06T01:10:10.605846: step 6827, loss 0.595078.
Train: 2018-08-06T01:10:10.864186: step 6828, loss 0.489527.
Train: 2018-08-06T01:10:11.110498: step 6829, loss 0.521749.
Train: 2018-08-06T01:10:11.357836: step 6830, loss 0.562542.
Test: 2018-08-06T01:10:12.633424: step 6830, loss 0.550222.
Train: 2018-08-06T01:10:12.872785: step 6831, loss 0.570756.
Train: 2018-08-06T01:10:13.122117: step 6832, loss 0.579054.
Train: 2018-08-06T01:10:13.367476: step 6833, loss 0.545788.
Train: 2018-08-06T01:10:13.616820: step 6834, loss 0.545711.
Train: 2018-08-06T01:10:13.877129: step 6835, loss 0.59592.
Train: 2018-08-06T01:10:14.125435: step 6836, loss 0.528798.
Train: 2018-08-06T01:10:14.374792: step 6837, loss 0.528705.
Train: 2018-08-06T01:10:14.620111: step 6838, loss 0.579249.
Train: 2018-08-06T01:10:14.866452: step 6839, loss 0.52007.
Train: 2018-08-06T01:10:15.115816: step 6840, loss 0.519957.
Test: 2018-08-06T01:10:16.374420: step 6840, loss 0.549708.
Train: 2018-08-06T01:10:16.610788: step 6841, loss 0.494313.
Train: 2018-08-06T01:10:16.869123: step 6842, loss 0.553801.
Train: 2018-08-06T01:10:17.118430: step 6843, loss 0.570902.
Train: 2018-08-06T01:10:17.363805: step 6844, loss 0.536565.
Train: 2018-08-06T01:10:17.607156: step 6845, loss 0.63124.
Train: 2018-08-06T01:10:17.865458: step 6846, loss 0.553718.
Train: 2018-08-06T01:10:18.113768: step 6847, loss 0.536467.
Train: 2018-08-06T01:10:18.362104: step 6848, loss 0.588227.
Train: 2018-08-06T01:10:18.610471: step 6849, loss 0.579595.
Train: 2018-08-06T01:10:18.854812: step 6850, loss 0.614066.
Test: 2018-08-06T01:10:20.100454: step 6850, loss 0.547571.
Train: 2018-08-06T01:10:20.339815: step 6851, loss 0.596749.
Train: 2018-08-06T01:10:20.582167: step 6852, loss 0.59665.
Train: 2018-08-06T01:10:20.830503: step 6853, loss 0.562337.
Train: 2018-08-06T01:10:21.075877: step 6854, loss 0.545305.
Train: 2018-08-06T01:10:21.337179: step 6855, loss 0.596318.
Train: 2018-08-06T01:10:21.583489: step 6856, loss 0.511588.
Train: 2018-08-06T01:10:21.842795: step 6857, loss 0.621457.
Train: 2018-08-06T01:10:22.085148: step 6858, loss 0.528736.
Train: 2018-08-06T01:10:22.348474: step 6859, loss 0.562393.
Train: 2018-08-06T01:10:22.594815: step 6860, loss 0.595875.
Test: 2018-08-06T01:10:23.862394: step 6860, loss 0.548496.
Train: 2018-08-06T01:10:24.099790: step 6861, loss 0.612468.
Train: 2018-08-06T01:10:24.351087: step 6862, loss 0.562453.
Train: 2018-08-06T01:10:24.599454: step 6863, loss 0.579031.
Train: 2018-08-06T01:10:24.851779: step 6864, loss 0.554273.
Train: 2018-08-06T01:10:25.101082: step 6865, loss 0.521477.
Train: 2018-08-06T01:10:25.346456: step 6866, loss 0.636332.
Train: 2018-08-06T01:10:25.596786: step 6867, loss 0.603434.
Train: 2018-08-06T01:10:25.846115: step 6868, loss 0.64396.
Train: 2018-08-06T01:10:26.105421: step 6869, loss 0.530387.
Train: 2018-08-06T01:10:26.349742: step 6870, loss 0.554732.
Test: 2018-08-06T01:10:27.643282: step 6870, loss 0.548518.
Train: 2018-08-06T01:10:27.882643: step 6871, loss 0.643009.
Train: 2018-08-06T01:10:28.141982: step 6872, loss 0.586836.
Train: 2018-08-06T01:10:28.387294: step 6873, loss 0.570924.
Train: 2018-08-06T01:10:28.633659: step 6874, loss 0.578862.
Train: 2018-08-06T01:10:28.880974: step 6875, loss 0.547428.
Train: 2018-08-06T01:10:29.130307: step 6876, loss 0.649374.
Train: 2018-08-06T01:10:29.389644: step 6877, loss 0.578896.
Train: 2018-08-06T01:10:29.637982: step 6878, loss 0.578916.
Train: 2018-08-06T01:10:29.882320: step 6879, loss 0.58666.
Train: 2018-08-06T01:10:30.130662: step 6880, loss 0.525162.
Test: 2018-08-06T01:10:31.405227: step 6880, loss 0.551201.
Train: 2018-08-06T01:10:31.646583: step 6881, loss 0.48699.
Train: 2018-08-06T01:10:31.904929: step 6882, loss 0.586649.
Train: 2018-08-06T01:10:32.148273: step 6883, loss 0.548328.
Train: 2018-08-06T01:10:32.397573: step 6884, loss 0.58665.
Train: 2018-08-06T01:10:32.640949: step 6885, loss 0.532963.
Train: 2018-08-06T01:10:32.902224: step 6886, loss 0.540573.
Train: 2018-08-06T01:10:33.147569: step 6887, loss 0.55587.
Train: 2018-08-06T01:10:33.405908: step 6888, loss 0.540371.
Train: 2018-08-06T01:10:33.653216: step 6889, loss 0.56345.
Train: 2018-08-06T01:10:33.904574: step 6890, loss 0.540103.
Test: 2018-08-06T01:10:35.197087: step 6890, loss 0.54977.
Train: 2018-08-06T01:10:35.449437: step 6891, loss 0.532158.
Train: 2018-08-06T01:10:35.695753: step 6892, loss 0.578882.
Train: 2018-08-06T01:10:35.943123: step 6893, loss 0.508205.
Train: 2018-08-06T01:10:36.189457: step 6894, loss 0.539412.
Train: 2018-08-06T01:10:36.446770: step 6895, loss 0.570928.
Train: 2018-08-06T01:10:36.695106: step 6896, loss 0.570892.
Train: 2018-08-06T01:10:36.953397: step 6897, loss 0.506873.
Train: 2018-08-06T01:10:37.206737: step 6898, loss 0.530636.
Train: 2018-08-06T01:10:37.453079: step 6899, loss 0.603137.
Train: 2018-08-06T01:10:37.703410: step 6900, loss 0.57079.
Test: 2018-08-06T01:10:38.966008: step 6900, loss 0.547916.
Train: 2018-08-06T01:10:39.903792: step 6901, loss 0.530071.
Train: 2018-08-06T01:10:40.154150: step 6902, loss 0.554424.
Train: 2018-08-06T01:10:40.403451: step 6903, loss 0.562561.
Train: 2018-08-06T01:10:40.646832: step 6904, loss 0.488486.
Train: 2018-08-06T01:10:40.893173: step 6905, loss 0.63687.
Train: 2018-08-06T01:10:41.139483: step 6906, loss 0.554191.
Train: 2018-08-06T01:10:41.384858: step 6907, loss 0.595661.
Train: 2018-08-06T01:10:41.642148: step 6908, loss 0.520903.
Train: 2018-08-06T01:10:41.890474: step 6909, loss 0.612385.
Train: 2018-08-06T01:10:42.139808: step 6910, loss 0.587419.
Test: 2018-08-06T01:10:43.418388: step 6910, loss 0.548342.
Train: 2018-08-06T01:10:43.658745: step 6911, loss 0.579088.
Train: 2018-08-06T01:10:43.925059: step 6912, loss 0.57908.
Train: 2018-08-06T01:10:44.173394: step 6913, loss 0.57076.
Train: 2018-08-06T01:10:44.415746: step 6914, loss 0.52927.
Train: 2018-08-06T01:10:44.667074: step 6915, loss 0.60393.
Train: 2018-08-06T01:10:44.923389: step 6916, loss 0.62872.
Train: 2018-08-06T01:10:45.169761: step 6917, loss 0.504709.
Train: 2018-08-06T01:10:45.416071: step 6918, loss 0.455337.
Train: 2018-08-06T01:10:45.665405: step 6919, loss 0.48822.
Train: 2018-08-06T01:10:45.921720: step 6920, loss 0.504552.
Test: 2018-08-06T01:10:47.194316: step 6920, loss 0.548009.
Train: 2018-08-06T01:10:47.429720: step 6921, loss 0.612288.
Train: 2018-08-06T01:10:47.678022: step 6922, loss 0.545794.
Train: 2018-08-06T01:10:47.931344: step 6923, loss 0.520726.
Train: 2018-08-06T01:10:48.188683: step 6924, loss 0.512234.
Train: 2018-08-06T01:10:48.434031: step 6925, loss 0.587565.
Train: 2018-08-06T01:10:48.683358: step 6926, loss 0.537147.
Train: 2018-08-06T01:10:48.929674: step 6927, loss 0.511776.
Train: 2018-08-06T01:10:49.173024: step 6928, loss 0.562358.
Train: 2018-08-06T01:10:49.426347: step 6929, loss 0.519939.
Train: 2018-08-06T01:10:49.674710: step 6930, loss 0.630414.
Test: 2018-08-06T01:10:50.947279: step 6930, loss 0.548311.
Train: 2018-08-06T01:10:51.183657: step 6931, loss 0.443082.
Train: 2018-08-06T01:10:51.438989: step 6932, loss 0.61362.
Train: 2018-08-06T01:10:51.693291: step 6933, loss 0.553774.
Train: 2018-08-06T01:10:51.938628: step 6934, loss 0.553761.
Train: 2018-08-06T01:10:52.184975: step 6935, loss 0.605257.
Train: 2018-08-06T01:10:52.428350: step 6936, loss 0.519415.
Train: 2018-08-06T01:10:52.682666: step 6937, loss 0.553746.
Train: 2018-08-06T01:10:52.925016: step 6938, loss 0.613894.
Train: 2018-08-06T01:10:53.171362: step 6939, loss 0.588091.
Train: 2018-08-06T01:10:53.426659: step 6940, loss 0.579477.
Test: 2018-08-06T01:10:54.701239: step 6940, loss 0.547461.
Train: 2018-08-06T01:10:54.939604: step 6941, loss 0.536677.
Train: 2018-08-06T01:10:55.182953: step 6942, loss 0.596497.
Train: 2018-08-06T01:10:55.428295: step 6943, loss 0.647535.
Train: 2018-08-06T01:10:55.690595: step 6944, loss 0.536899.
Train: 2018-08-06T01:10:55.947907: step 6945, loss 0.621542.
Train: 2018-08-06T01:10:56.139395: step 6946, loss 0.580332.
Train: 2018-08-06T01:10:56.386732: step 6947, loss 0.604282.
Train: 2018-08-06T01:10:56.631109: step 6948, loss 0.587428.
Train: 2018-08-06T01:10:56.878444: step 6949, loss 0.58733.
Train: 2018-08-06T01:10:57.124759: step 6950, loss 0.595476.
Test: 2018-08-06T01:10:58.397356: step 6950, loss 0.548516.
Train: 2018-08-06T01:10:58.632751: step 6951, loss 0.603528.
Train: 2018-08-06T01:10:58.887073: step 6952, loss 0.521939.
Train: 2018-08-06T01:10:59.130420: step 6953, loss 0.619402.
Train: 2018-08-06T01:10:59.375739: step 6954, loss 0.49833.
Train: 2018-08-06T01:10:59.631056: step 6955, loss 0.546764.
Train: 2018-08-06T01:10:59.888398: step 6956, loss 0.554844.
Train: 2018-08-06T01:11:00.136730: step 6957, loss 0.538911.
Train: 2018-08-06T01:11:00.385040: step 6958, loss 0.586841.
Train: 2018-08-06T01:11:00.629386: step 6959, loss 0.610731.
Train: 2018-08-06T01:11:00.878753: step 6960, loss 0.53912.
Test: 2018-08-06T01:11:02.172260: step 6960, loss 0.55028.
Train: 2018-08-06T01:11:02.411620: step 6961, loss 0.523308.
Train: 2018-08-06T01:11:02.660981: step 6962, loss 0.523314.
Train: 2018-08-06T01:11:02.920290: step 6963, loss 0.562979.
Train: 2018-08-06T01:11:03.168628: step 6964, loss 0.586809.
Train: 2018-08-06T01:11:03.423912: step 6965, loss 0.562952.
Train: 2018-08-06T01:11:03.669289: step 6966, loss 0.594774.
Train: 2018-08-06T01:11:03.920615: step 6967, loss 0.53908.
Train: 2018-08-06T01:11:04.183881: step 6968, loss 0.594779.
Train: 2018-08-06T01:11:04.430222: step 6969, loss 0.5709.
Train: 2018-08-06T01:11:04.691524: step 6970, loss 0.602729.
Test: 2018-08-06T01:11:05.949159: step 6970, loss 0.549688.
Train: 2018-08-06T01:11:06.184530: step 6971, loss 0.547064.
Train: 2018-08-06T01:11:06.431869: step 6972, loss 0.578858.
Train: 2018-08-06T01:11:06.687185: step 6973, loss 0.602679.
Train: 2018-08-06T01:11:06.935523: step 6974, loss 0.531294.
Train: 2018-08-06T01:11:07.195850: step 6975, loss 0.586785.
Train: 2018-08-06T01:11:07.444161: step 6976, loss 0.56302.
Train: 2018-08-06T01:11:07.700476: step 6977, loss 0.563027.
Train: 2018-08-06T01:11:07.961809: step 6978, loss 0.610515.
Train: 2018-08-06T01:11:08.220087: step 6979, loss 0.586765.
Train: 2018-08-06T01:11:08.464465: step 6980, loss 0.507847.
Test: 2018-08-06T01:11:09.727055: step 6980, loss 0.550445.
Train: 2018-08-06T01:11:09.967413: step 6981, loss 0.523622.
Train: 2018-08-06T01:11:10.214753: step 6982, loss 0.547256.
Train: 2018-08-06T01:11:10.471097: step 6983, loss 0.555115.
Train: 2018-08-06T01:11:10.719401: step 6984, loss 0.53921.
Train: 2018-08-06T01:11:10.970761: step 6985, loss 0.515265.
Train: 2018-08-06T01:11:11.217096: step 6986, loss 0.530998.
Train: 2018-08-06T01:11:11.462415: step 6987, loss 0.570856.
Train: 2018-08-06T01:11:11.710750: step 6988, loss 0.594948.
Train: 2018-08-06T01:11:11.955128: step 6989, loss 0.530534.
Train: 2018-08-06T01:11:12.202457: step 6990, loss 0.578888.
Test: 2018-08-06T01:11:13.456083: step 6990, loss 0.549725.
Train: 2018-08-06T01:11:13.745335: step 6991, loss 0.554594.
Train: 2018-08-06T01:11:13.992649: step 6992, loss 0.538304.
Train: 2018-08-06T01:11:14.239986: step 6993, loss 0.51378.
Train: 2018-08-06T01:11:14.484332: step 6994, loss 0.57894.
Train: 2018-08-06T01:11:14.745661: step 6995, loss 0.660912.
Train: 2018-08-06T01:11:14.993969: step 6996, loss 0.628141.
Train: 2018-08-06T01:11:15.244331: step 6997, loss 0.538021.
Train: 2018-08-06T01:11:15.499649: step 6998, loss 0.636213.
Train: 2018-08-06T01:11:15.760950: step 6999, loss 0.521807.
Train: 2018-08-06T01:11:16.015240: step 7000, loss 0.611536.
Test: 2018-08-06T01:11:17.286838: step 7000, loss 0.548875.
Train: 2018-08-06T01:11:18.197615: step 7001, loss 0.489436.
Train: 2018-08-06T01:11:18.441962: step 7002, loss 0.570782.
Train: 2018-08-06T01:11:18.684314: step 7003, loss 0.611433.
Train: 2018-08-06T01:11:18.939662: step 7004, loss 0.554552.
Train: 2018-08-06T01:11:19.185000: step 7005, loss 0.578901.
Train: 2018-08-06T01:11:19.431320: step 7006, loss 0.554602.
Train: 2018-08-06T01:11:19.679653: step 7007, loss 0.586981.
Train: 2018-08-06T01:11:19.927989: step 7008, loss 0.586965.
Train: 2018-08-06T01:11:20.186322: step 7009, loss 0.522439.
Train: 2018-08-06T01:11:20.446601: step 7010, loss 0.546647.
Test: 2018-08-06T01:11:21.735155: step 7010, loss 0.550421.
Train: 2018-08-06T01:11:21.978530: step 7011, loss 0.578879.
Train: 2018-08-06T01:11:22.236844: step 7012, loss 0.586931.
Train: 2018-08-06T01:11:22.481191: step 7013, loss 0.570828.
Train: 2018-08-06T01:11:22.725508: step 7014, loss 0.578872.
Train: 2018-08-06T01:11:22.972876: step 7015, loss 0.546752.
Train: 2018-08-06T01:11:23.218216: step 7016, loss 0.570844.
Train: 2018-08-06T01:11:23.468544: step 7017, loss 0.514708.
Train: 2018-08-06T01:11:23.717861: step 7018, loss 0.554792.
Train: 2018-08-06T01:11:23.967186: step 7019, loss 0.530674.
Train: 2018-08-06T01:11:24.225495: step 7020, loss 0.594967.
Test: 2018-08-06T01:11:25.480140: step 7020, loss 0.55025.
Train: 2018-08-06T01:11:25.723488: step 7021, loss 0.562771.
Train: 2018-08-06T01:11:25.970858: step 7022, loss 0.554699.
Train: 2018-08-06T01:11:26.216202: step 7023, loss 0.554683.
Train: 2018-08-06T01:11:26.473514: step 7024, loss 0.62735.
Train: 2018-08-06T01:11:26.722842: step 7025, loss 0.570808.
Train: 2018-08-06T01:11:26.984118: step 7026, loss 0.538543.
Train: 2018-08-06T01:11:27.237484: step 7027, loss 0.55468.
Train: 2018-08-06T01:11:27.496760: step 7028, loss 0.538523.
Train: 2018-08-06T01:11:27.746123: step 7029, loss 0.562726.
Train: 2018-08-06T01:11:28.006395: step 7030, loss 0.498014.
Test: 2018-08-06T01:11:29.274005: step 7030, loss 0.549512.
Train: 2018-08-06T01:11:29.514362: step 7031, loss 0.554557.
Train: 2018-08-06T01:11:29.763698: step 7032, loss 0.530139.
Train: 2018-08-06T01:11:30.018016: step 7033, loss 0.587077.
Train: 2018-08-06T01:11:30.265385: step 7034, loss 0.538224.
Train: 2018-08-06T01:11:30.512729: step 7035, loss 0.562662.
Train: 2018-08-06T01:11:30.767038: step 7036, loss 0.570766.
Train: 2018-08-06T01:11:31.021363: step 7037, loss 0.603641.
Train: 2018-08-06T01:11:31.266704: step 7038, loss 0.546095.
Train: 2018-08-06T01:11:31.516010: step 7039, loss 0.570758.
Train: 2018-08-06T01:11:31.774353: step 7040, loss 0.570758.
Test: 2018-08-06T01:11:33.028964: step 7040, loss 0.547832.
Train: 2018-08-06T01:11:33.265333: step 7041, loss 0.554296.
Train: 2018-08-06T01:11:33.516660: step 7042, loss 0.570758.
Train: 2018-08-06T01:11:33.765993: step 7043, loss 0.587226.
Train: 2018-08-06T01:11:34.012364: step 7044, loss 0.628367.
Train: 2018-08-06T01:11:34.274632: step 7045, loss 0.529689.
Train: 2018-08-06T01:11:34.523979: step 7046, loss 0.472305.
Train: 2018-08-06T01:11:34.765319: step 7047, loss 0.570766.
Train: 2018-08-06T01:11:35.015650: step 7048, loss 0.57899.
Train: 2018-08-06T01:11:35.268998: step 7049, loss 0.488559.
Train: 2018-08-06T01:11:35.529276: step 7050, loss 0.570757.
Test: 2018-08-06T01:11:36.794893: step 7050, loss 0.548736.
Train: 2018-08-06T01:11:37.032282: step 7051, loss 0.595492.
Train: 2018-08-06T01:11:37.283601: step 7052, loss 0.554278.
Train: 2018-08-06T01:11:37.530954: step 7053, loss 0.579004.
Train: 2018-08-06T01:11:37.787269: step 7054, loss 0.512973.
Train: 2018-08-06T01:11:38.032583: step 7055, loss 0.521174.
Train: 2018-08-06T01:11:38.278924: step 7056, loss 0.628709.
Train: 2018-08-06T01:11:38.538262: step 7057, loss 0.52108.
Train: 2018-08-06T01:11:38.786591: step 7058, loss 0.537609.
Train: 2018-08-06T01:11:39.046879: step 7059, loss 0.628846.
Train: 2018-08-06T01:11:39.295207: step 7060, loss 0.537576.
Test: 2018-08-06T01:11:40.553840: step 7060, loss 0.548798.
Train: 2018-08-06T01:11:40.795218: step 7061, loss 0.56246.
Train: 2018-08-06T01:11:41.045525: step 7062, loss 0.554162.
Train: 2018-08-06T01:11:41.294858: step 7063, loss 0.653728.
Train: 2018-08-06T01:11:41.544192: step 7064, loss 0.537648.
Train: 2018-08-06T01:11:41.792559: step 7065, loss 0.512881.
Train: 2018-08-06T01:11:42.041860: step 7066, loss 0.562479.
Train: 2018-08-06T01:11:42.287203: step 7067, loss 0.587285.
Train: 2018-08-06T01:11:42.536562: step 7068, loss 0.512951.
Train: 2018-08-06T01:11:42.787867: step 7069, loss 0.562494.
Train: 2018-08-06T01:11:43.035236: step 7070, loss 0.537713.
Test: 2018-08-06T01:11:44.301816: step 7070, loss 0.547537.
Train: 2018-08-06T01:11:44.547160: step 7071, loss 0.570759.
Train: 2018-08-06T01:11:44.794499: step 7072, loss 0.463222.
Train: 2018-08-06T01:11:45.042835: step 7073, loss 0.504412.
Train: 2018-08-06T01:11:45.289206: step 7074, loss 0.595729.
Train: 2018-08-06T01:11:45.548514: step 7075, loss 0.5374.
Train: 2018-08-06T01:11:45.799842: step 7076, loss 0.503874.
Train: 2018-08-06T01:11:46.046183: step 7077, loss 0.587566.
Train: 2018-08-06T01:11:46.308451: step 7078, loss 0.579205.
Train: 2018-08-06T01:11:46.550835: step 7079, loss 0.612918.
Train: 2018-08-06T01:11:46.794206: step 7080, loss 0.604494.
Test: 2018-08-06T01:11:48.095671: step 7080, loss 0.54832.
Train: 2018-08-06T01:11:48.338022: step 7081, loss 0.629709.
Train: 2018-08-06T01:11:48.598357: step 7082, loss 0.545606.
Train: 2018-08-06T01:11:48.844698: step 7083, loss 0.604299.
Train: 2018-08-06T01:11:49.105002: step 7084, loss 0.462171.
Train: 2018-08-06T01:11:49.357296: step 7085, loss 0.545716.
Train: 2018-08-06T01:11:49.602639: step 7086, loss 0.537373.
Train: 2018-08-06T01:11:49.851974: step 7087, loss 0.612524.
Train: 2018-08-06T01:11:50.099339: step 7088, loss 0.570767.
Train: 2018-08-06T01:11:50.362608: step 7089, loss 0.554098.
Train: 2018-08-06T01:11:50.613961: step 7090, loss 0.562439.
Test: 2018-08-06T01:11:51.882543: step 7090, loss 0.547792.
Train: 2018-08-06T01:11:52.124895: step 7091, loss 0.604024.
Train: 2018-08-06T01:11:52.369241: step 7092, loss 0.520965.
Train: 2018-08-06T01:11:52.617577: step 7093, loss 0.504428.
Train: 2018-08-06T01:11:52.864917: step 7094, loss 0.529283.
Train: 2018-08-06T01:11:53.126217: step 7095, loss 0.595676.
Train: 2018-08-06T01:11:53.385528: step 7096, loss 0.562452.
Train: 2018-08-06T01:11:53.583994: step 7097, loss 0.491577.
Train: 2018-08-06T01:11:53.832360: step 7098, loss 0.512525.
Train: 2018-08-06T01:11:54.080664: step 7099, loss 0.587477.
Train: 2018-08-06T01:11:54.331021: step 7100, loss 0.570728.
Test: 2018-08-06T01:11:55.615559: step 7100, loss 0.549019.
Train: 2018-08-06T01:11:56.590285: step 7101, loss 0.662752.
Train: 2018-08-06T01:11:56.837593: step 7102, loss 0.554057.
Train: 2018-08-06T01:11:57.100891: step 7103, loss 0.495668.
Train: 2018-08-06T01:11:57.356237: step 7104, loss 0.595836.
Train: 2018-08-06T01:11:57.604542: step 7105, loss 0.537416.
Train: 2018-08-06T01:11:57.849912: step 7106, loss 0.545719.
Train: 2018-08-06T01:11:58.097237: step 7107, loss 0.470643.
Train: 2018-08-06T01:11:58.345591: step 7108, loss 0.604343.
Train: 2018-08-06T01:11:58.591932: step 7109, loss 0.537423.
Train: 2018-08-06T01:11:58.839240: step 7110, loss 0.621241.
Test: 2018-08-06T01:12:00.095879: step 7110, loss 0.547457.
Train: 2018-08-06T01:12:00.335239: step 7111, loss 0.570694.
Train: 2018-08-06T01:12:00.581612: step 7112, loss 0.529007.
Train: 2018-08-06T01:12:00.828953: step 7113, loss 0.562412.
Train: 2018-08-06T01:12:01.086257: step 7114, loss 0.512303.
Train: 2018-08-06T01:12:01.332572: step 7115, loss 0.545693.
Train: 2018-08-06T01:12:01.575952: step 7116, loss 0.58751.
Train: 2018-08-06T01:12:01.824257: step 7117, loss 0.537295.
Train: 2018-08-06T01:12:02.072594: step 7118, loss 0.629406.
Train: 2018-08-06T01:12:02.320960: step 7119, loss 0.545676.
Train: 2018-08-06T01:12:02.568294: step 7120, loss 0.512251.
Test: 2018-08-06T01:12:03.833882: step 7120, loss 0.548631.
Train: 2018-08-06T01:12:04.074240: step 7121, loss 0.562419.
Train: 2018-08-06T01:12:04.321603: step 7122, loss 0.570786.
Train: 2018-08-06T01:12:04.581914: step 7123, loss 0.570789.
Train: 2018-08-06T01:12:04.831246: step 7124, loss 0.554057.
Train: 2018-08-06T01:12:05.087561: step 7125, loss 0.587482.
Train: 2018-08-06T01:12:05.337862: step 7126, loss 0.58747.
Train: 2018-08-06T01:12:05.597193: step 7127, loss 0.512415.
Train: 2018-08-06T01:12:05.845504: step 7128, loss 0.562431.
Train: 2018-08-06T01:12:06.099853: step 7129, loss 0.579092.
Train: 2018-08-06T01:12:06.349181: step 7130, loss 0.595722.
Test: 2018-08-06T01:12:07.619758: step 7130, loss 0.548582.
Train: 2018-08-06T01:12:07.858120: step 7131, loss 0.520923.
Train: 2018-08-06T01:12:08.108477: step 7132, loss 0.545857.
Train: 2018-08-06T01:12:08.361773: step 7133, loss 0.56246.
Train: 2018-08-06T01:12:08.615121: step 7134, loss 0.58735.
Train: 2018-08-06T01:12:08.868450: step 7135, loss 0.570758.
Train: 2018-08-06T01:12:09.115757: step 7136, loss 0.603862.
Train: 2018-08-06T01:12:09.379079: step 7137, loss 0.587274.
Train: 2018-08-06T01:12:09.625394: step 7138, loss 0.496606.
Train: 2018-08-06T01:12:09.870769: step 7139, loss 0.496659.
Train: 2018-08-06T01:12:10.120076: step 7140, loss 0.521314.
Test: 2018-08-06T01:12:11.376710: step 7140, loss 0.548717.
Train: 2018-08-06T01:12:11.625047: step 7141, loss 0.545978.
Train: 2018-08-06T01:12:11.871418: step 7142, loss 0.521196.
Train: 2018-08-06T01:12:12.115759: step 7143, loss 0.645391.
Train: 2018-08-06T01:12:12.360082: step 7144, loss 0.545869.
Train: 2018-08-06T01:12:12.618421: step 7145, loss 0.603996.
Train: 2018-08-06T01:12:12.865754: step 7146, loss 0.545898.
Train: 2018-08-06T01:12:13.117056: step 7147, loss 0.587335.
Train: 2018-08-06T01:12:13.365423: step 7148, loss 0.554202.
Train: 2018-08-06T01:12:13.620734: step 7149, loss 0.529399.
Train: 2018-08-06T01:12:13.868079: step 7150, loss 0.521124.
Test: 2018-08-06T01:12:15.141642: step 7150, loss 0.54922.
Train: 2018-08-06T01:12:15.379007: step 7151, loss 0.562476.
Train: 2018-08-06T01:12:15.626346: step 7152, loss 0.56247.
Train: 2018-08-06T01:12:15.874681: step 7153, loss 0.570753.
Train: 2018-08-06T01:12:16.118062: step 7154, loss 0.537572.
Train: 2018-08-06T01:12:16.373348: step 7155, loss 0.537544.
Train: 2018-08-06T01:12:16.619689: step 7156, loss 0.645561.
Train: 2018-08-06T01:12:16.870020: step 7157, loss 0.512642.
Train: 2018-08-06T01:12:17.131346: step 7158, loss 0.56246.
Train: 2018-08-06T01:12:17.386647: step 7159, loss 0.537548.
Train: 2018-08-06T01:12:17.631014: step 7160, loss 0.653843.
Test: 2018-08-06T01:12:18.924525: step 7160, loss 0.548038.
Train: 2018-08-06T01:12:19.161922: step 7161, loss 0.545871.
Train: 2018-08-06T01:12:19.409263: step 7162, loss 0.504483.
Train: 2018-08-06T01:12:19.656598: step 7163, loss 0.562477.
Train: 2018-08-06T01:12:19.904903: step 7164, loss 0.579043.
Train: 2018-08-06T01:12:20.155234: step 7165, loss 0.512796.
Train: 2018-08-06T01:12:20.418555: step 7166, loss 0.57075.
Train: 2018-08-06T01:12:20.665869: step 7167, loss 0.521027.
Train: 2018-08-06T01:12:20.910224: step 7168, loss 0.529266.
Train: 2018-08-06T01:12:21.154591: step 7169, loss 0.537512.
Train: 2018-08-06T01:12:21.403894: step 7170, loss 0.479144.
Test: 2018-08-06T01:12:22.656544: step 7170, loss 0.547683.
Train: 2018-08-06T01:12:22.894907: step 7171, loss 0.604206.
Train: 2018-08-06T01:12:23.141248: step 7172, loss 0.604283.
Train: 2018-08-06T01:12:23.392607: step 7173, loss 0.54563.
Train: 2018-08-06T01:12:23.635954: step 7174, loss 0.64631.
Train: 2018-08-06T01:12:23.883279: step 7175, loss 0.646221.
Train: 2018-08-06T01:12:24.130631: step 7176, loss 0.587485.
Train: 2018-08-06T01:12:24.379960: step 7177, loss 0.570763.
Train: 2018-08-06T01:12:24.626308: step 7178, loss 0.603959.
Train: 2018-08-06T01:12:24.871620: step 7179, loss 0.570756.
Train: 2018-08-06T01:12:25.121951: step 7180, loss 0.537822.
Test: 2018-08-06T01:12:26.374600: step 7180, loss 0.548276.
Train: 2018-08-06T01:12:26.615956: step 7181, loss 0.587178.
Train: 2018-08-06T01:12:26.863319: step 7182, loss 0.619851.
Train: 2018-08-06T01:12:27.112627: step 7183, loss 0.497472.
Train: 2018-08-06T01:12:27.356008: step 7184, loss 0.635789.
Train: 2018-08-06T01:12:27.611295: step 7185, loss 0.546514.
Train: 2018-08-06T01:12:27.865614: step 7186, loss 0.562749.
Train: 2018-08-06T01:12:28.108987: step 7187, loss 0.603019.
Train: 2018-08-06T01:12:28.356333: step 7188, loss 0.522729.
Train: 2018-08-06T01:12:28.602673: step 7189, loss 0.498823.
Train: 2018-08-06T01:12:28.860983: step 7190, loss 0.530856.
Test: 2018-08-06T01:12:30.143521: step 7190, loss 0.549284.
Train: 2018-08-06T01:12:30.393877: step 7191, loss 0.578861.
Train: 2018-08-06T01:12:30.640226: step 7192, loss 0.538788.
Train: 2018-08-06T01:12:30.890555: step 7193, loss 0.562847.
Train: 2018-08-06T01:12:31.152823: step 7194, loss 0.506527.
Train: 2018-08-06T01:12:31.412129: step 7195, loss 0.611192.
Train: 2018-08-06T01:12:31.662492: step 7196, loss 0.578814.
Train: 2018-08-06T01:12:31.909828: step 7197, loss 0.482026.
Train: 2018-08-06T01:12:32.166144: step 7198, loss 0.64368.
Train: 2018-08-06T01:12:32.425445: step 7199, loss 0.562736.
Train: 2018-08-06T01:12:32.673755: step 7200, loss 0.578837.
Test: 2018-08-06T01:12:33.931391: step 7200, loss 0.549538.
Train: 2018-08-06T01:12:34.896541: step 7201, loss 0.514205.
Train: 2018-08-06T01:12:35.148832: step 7202, loss 0.546496.
Train: 2018-08-06T01:12:35.393179: step 7203, loss 0.570647.
Train: 2018-08-06T01:12:35.643535: step 7204, loss 0.522009.
Train: 2018-08-06T01:12:35.901819: step 7205, loss 0.570577.
Train: 2018-08-06T01:12:36.148161: step 7206, loss 0.554297.
Train: 2018-08-06T01:12:36.396524: step 7207, loss 0.54603.
Train: 2018-08-06T01:12:36.643862: step 7208, loss 0.554233.
Train: 2018-08-06T01:12:36.893167: step 7209, loss 0.652853.
Train: 2018-08-06T01:12:37.140506: step 7210, loss 0.611651.
Test: 2018-08-06T01:12:38.447012: step 7210, loss 0.549175.
Train: 2018-08-06T01:12:38.695375: step 7211, loss 0.471535.
Train: 2018-08-06T01:12:38.944709: step 7212, loss 0.570152.
Train: 2018-08-06T01:12:39.194039: step 7213, loss 0.554487.
Train: 2018-08-06T01:12:39.439383: step 7214, loss 0.552941.
Train: 2018-08-06T01:12:39.689714: step 7215, loss 0.615258.
Train: 2018-08-06T01:12:39.941042: step 7216, loss 0.585512.
Train: 2018-08-06T01:12:40.190351: step 7217, loss 0.579147.
Train: 2018-08-06T01:12:40.437713: step 7218, loss 0.57992.
Train: 2018-08-06T01:12:40.693031: step 7219, loss 0.545553.
Train: 2018-08-06T01:12:40.939375: step 7220, loss 0.595658.
Test: 2018-08-06T01:12:42.212940: step 7220, loss 0.548664.
Train: 2018-08-06T01:12:42.449308: step 7221, loss 0.545196.
Train: 2018-08-06T01:12:42.698642: step 7222, loss 0.620067.
Train: 2018-08-06T01:12:42.943986: step 7223, loss 0.55375.
Train: 2018-08-06T01:12:43.193319: step 7224, loss 0.531345.
Train: 2018-08-06T01:12:43.441683: step 7225, loss 0.602961.
Train: 2018-08-06T01:12:43.697995: step 7226, loss 0.579058.
Train: 2018-08-06T01:12:43.971518: step 7227, loss 0.571047.
Train: 2018-08-06T01:12:44.231820: step 7228, loss 0.530793.
Train: 2018-08-06T01:12:44.479128: step 7229, loss 0.523319.
Train: 2018-08-06T01:12:44.738437: step 7230, loss 0.578724.
Test: 2018-08-06T01:12:46.005046: step 7230, loss 0.548945.
Train: 2018-08-06T01:12:46.247429: step 7231, loss 0.571007.
Train: 2018-08-06T01:12:46.494762: step 7232, loss 0.555026.
Train: 2018-08-06T01:12:46.741085: step 7233, loss 0.602618.
Train: 2018-08-06T01:12:46.990443: step 7234, loss 0.555032.
Train: 2018-08-06T01:12:47.242762: step 7235, loss 0.563056.
Train: 2018-08-06T01:12:47.493067: step 7236, loss 0.626445.
Train: 2018-08-06T01:12:47.737444: step 7237, loss 0.56303.
Train: 2018-08-06T01:12:47.993753: step 7238, loss 0.65792.
Train: 2018-08-06T01:12:48.241092: step 7239, loss 0.578857.
Train: 2018-08-06T01:12:48.489403: step 7240, loss 0.571027.
Test: 2018-08-06T01:12:49.756015: step 7240, loss 0.550347.
Train: 2018-08-06T01:12:50.007389: step 7241, loss 0.641474.
Train: 2018-08-06T01:12:50.261663: step 7242, loss 0.578897.
Train: 2018-08-06T01:12:50.509001: step 7243, loss 0.485832.
Train: 2018-08-06T01:12:50.759336: step 7244, loss 0.439493.
Train: 2018-08-06T01:12:51.009662: step 7245, loss 0.516796.
Train: 2018-08-06T01:12:51.259993: step 7246, loss 0.508773.
Train: 2018-08-06T01:12:51.510323: step 7247, loss 0.50841.
Train: 2018-08-06T01:12:51.701836: step 7248, loss 0.563096.
Train: 2018-08-06T01:12:51.963143: step 7249, loss 0.610594.
Train: 2018-08-06T01:12:52.208457: step 7250, loss 0.62652.
Test: 2018-08-06T01:12:53.469086: step 7250, loss 0.549441.
Train: 2018-08-06T01:12:53.707449: step 7251, loss 0.539051.
Train: 2018-08-06T01:12:53.955823: step 7252, loss 0.546916.
Train: 2018-08-06T01:12:54.204119: step 7253, loss 0.562855.
Train: 2018-08-06T01:12:54.453479: step 7254, loss 0.538749.
Train: 2018-08-06T01:12:54.699824: step 7255, loss 0.562775.
Train: 2018-08-06T01:12:54.947133: step 7256, loss 0.522398.
Train: 2018-08-06T01:12:55.193498: step 7257, loss 0.603183.
Train: 2018-08-06T01:12:55.449819: step 7258, loss 0.562677.
Train: 2018-08-06T01:12:55.695132: step 7259, loss 0.587036.
Train: 2018-08-06T01:12:55.945496: step 7260, loss 0.611476.
Test: 2018-08-06T01:12:57.211077: step 7260, loss 0.548107.
Train: 2018-08-06T01:12:57.456421: step 7261, loss 0.530073.
Train: 2018-08-06T01:12:57.702784: step 7262, loss 0.619665.
Train: 2018-08-06T01:12:57.946143: step 7263, loss 0.578926.
Train: 2018-08-06T01:12:58.197473: step 7264, loss 0.554514.
Train: 2018-08-06T01:12:58.444779: step 7265, loss 0.538271.
Train: 2018-08-06T01:12:58.692147: step 7266, loss 0.505776.
Train: 2018-08-06T01:12:58.938471: step 7267, loss 0.603333.
Train: 2018-08-06T01:12:59.190784: step 7268, loss 0.611479.
Train: 2018-08-06T01:12:59.451118: step 7269, loss 0.554518.
Train: 2018-08-06T01:12:59.700453: step 7270, loss 0.562658.
Test: 2018-08-06T01:13:00.974014: step 7270, loss 0.549098.
Train: 2018-08-06T01:13:01.212416: step 7271, loss 0.570779.
Train: 2018-08-06T01:13:01.469688: step 7272, loss 0.595146.
Train: 2018-08-06T01:13:01.716030: step 7273, loss 0.546472.
Train: 2018-08-06T01:13:01.962401: step 7274, loss 0.546495.
Train: 2018-08-06T01:13:02.210766: step 7275, loss 0.578897.
Train: 2018-08-06T01:13:02.457074: step 7276, loss 0.546523.
Train: 2018-08-06T01:13:02.705428: step 7277, loss 0.611256.
Train: 2018-08-06T01:13:02.949757: step 7278, loss 0.635457.
Train: 2018-08-06T01:13:03.195074: step 7279, loss 0.562763.
Train: 2018-08-06T01:13:03.444432: step 7280, loss 0.570834.
Test: 2018-08-06T01:13:04.704039: step 7280, loss 0.549249.
Train: 2018-08-06T01:13:04.951409: step 7281, loss 0.522728.
Train: 2018-08-06T01:13:05.196722: step 7282, loss 0.578866.
Train: 2018-08-06T01:13:05.440070: step 7283, loss 0.554859.
Train: 2018-08-06T01:13:05.691398: step 7284, loss 0.498915.
Train: 2018-08-06T01:13:05.946716: step 7285, loss 0.570861.
Train: 2018-08-06T01:13:06.191087: step 7286, loss 0.578864.
Train: 2018-08-06T01:13:06.438403: step 7287, loss 0.586876.
Train: 2018-08-06T01:13:06.694747: step 7288, loss 0.506779.
Train: 2018-08-06T01:13:06.944064: step 7289, loss 0.514707.
Train: 2018-08-06T01:13:07.204383: step 7290, loss 0.570833.
Test: 2018-08-06T01:13:08.491908: step 7290, loss 0.550053.
Train: 2018-08-06T01:13:08.735258: step 7291, loss 0.570822.
Train: 2018-08-06T01:13:08.982597: step 7292, loss 0.522393.
Train: 2018-08-06T01:13:09.229935: step 7293, loss 0.481802.
Train: 2018-08-06T01:13:09.477305: step 7294, loss 0.603287.
Train: 2018-08-06T01:13:09.725640: step 7295, loss 0.611521.
Train: 2018-08-06T01:13:09.973977: step 7296, loss 0.513646.
Train: 2018-08-06T01:13:10.216324: step 7297, loss 0.554405.
Train: 2018-08-06T01:13:10.462669: step 7298, loss 0.529761.
Train: 2018-08-06T01:13:10.706019: step 7299, loss 0.521417.
Train: 2018-08-06T01:13:10.957342: step 7300, loss 0.512991.
Test: 2018-08-06T01:13:12.235896: step 7300, loss 0.549394.
Train: 2018-08-06T01:13:13.263729: step 7301, loss 0.554184.
Train: 2018-08-06T01:13:13.523042: step 7302, loss 0.529168.
Train: 2018-08-06T01:13:13.771347: step 7303, loss 0.595829.
Train: 2018-08-06T01:13:14.017687: step 7304, loss 0.562401.
Train: 2018-08-06T01:13:14.264030: step 7305, loss 0.545595.
Train: 2018-08-06T01:13:14.512364: step 7306, loss 0.553961.
Train: 2018-08-06T01:13:14.758707: step 7307, loss 0.545499.
Train: 2018-08-06T01:13:15.010065: step 7308, loss 0.537003.
Train: 2018-08-06T01:13:15.256376: step 7309, loss 0.587771.
Train: 2018-08-06T01:13:15.499726: step 7310, loss 0.56235.
Test: 2018-08-06T01:13:16.774316: step 7310, loss 0.54758.
Train: 2018-08-06T01:13:17.084516: step 7311, loss 0.655755.
Train: 2018-08-06T01:13:17.337809: step 7312, loss 0.562351.
Train: 2018-08-06T01:13:17.598113: step 7313, loss 0.579291.
Train: 2018-08-06T01:13:17.845459: step 7314, loss 0.604623.
Train: 2018-08-06T01:13:18.097777: step 7315, loss 0.621371.
Train: 2018-08-06T01:13:18.349130: step 7316, loss 0.52041.
Train: 2018-08-06T01:13:18.596474: step 7317, loss 0.621012.
Train: 2018-08-06T01:13:18.842784: step 7318, loss 0.579079.
Train: 2018-08-06T01:13:19.089127: step 7319, loss 0.496052.
Train: 2018-08-06T01:13:19.339487: step 7320, loss 0.570771.
Test: 2018-08-06T01:13:20.597092: step 7320, loss 0.547169.
Train: 2018-08-06T01:13:20.835485: step 7321, loss 0.63686.
Train: 2018-08-06T01:13:21.081827: step 7322, loss 0.504885.
Train: 2018-08-06T01:13:21.330133: step 7323, loss 0.570738.
Train: 2018-08-06T01:13:21.578467: step 7324, loss 0.546186.
Train: 2018-08-06T01:13:21.830826: step 7325, loss 0.603479.
Train: 2018-08-06T01:13:22.081123: step 7326, loss 0.513629.
Train: 2018-08-06T01:13:22.329461: step 7327, loss 0.521847.
Train: 2018-08-06T01:13:22.580788: step 7328, loss 0.57076.
Train: 2018-08-06T01:13:22.839098: step 7329, loss 0.635986.
Train: 2018-08-06T01:13:23.088433: step 7330, loss 0.684706.
Test: 2018-08-06T01:13:24.357036: step 7330, loss 0.548441.
Train: 2018-08-06T01:13:24.595399: step 7331, loss 0.570755.
Train: 2018-08-06T01:13:24.847755: step 7332, loss 0.482269.
Train: 2018-08-06T01:13:25.097089: step 7333, loss 0.619003.
Train: 2018-08-06T01:13:25.345394: step 7334, loss 0.58699.
Train: 2018-08-06T01:13:25.604701: step 7335, loss 0.578832.
Train: 2018-08-06T01:13:25.850048: step 7336, loss 0.515152.
Train: 2018-08-06T01:13:26.100375: step 7337, loss 0.515152.
Train: 2018-08-06T01:13:26.353697: step 7338, loss 0.515138.
Train: 2018-08-06T01:13:26.600039: step 7339, loss 0.554933.
Train: 2018-08-06T01:13:26.848399: step 7340, loss 0.522967.
Test: 2018-08-06T01:13:28.141914: step 7340, loss 0.547833.
Train: 2018-08-06T01:13:28.385289: step 7341, loss 0.506831.
Train: 2018-08-06T01:13:28.631636: step 7342, loss 0.586895.
Train: 2018-08-06T01:13:28.892908: step 7343, loss 0.627195.
Train: 2018-08-06T01:13:29.147226: step 7344, loss 0.578883.
Train: 2018-08-06T01:13:29.392601: step 7345, loss 0.578881.
Train: 2018-08-06T01:13:29.640933: step 7346, loss 0.562771.
Train: 2018-08-06T01:13:29.885278: step 7347, loss 0.514354.
Train: 2018-08-06T01:13:30.137579: step 7348, loss 0.538497.
Train: 2018-08-06T01:13:30.387934: step 7349, loss 0.497963.
Train: 2018-08-06T01:13:30.651206: step 7350, loss 0.627628.
Test: 2018-08-06T01:13:31.911833: step 7350, loss 0.549441.
Train: 2018-08-06T01:13:32.164189: step 7351, loss 0.521986.
Train: 2018-08-06T01:13:32.413516: step 7352, loss 0.521845.
Train: 2018-08-06T01:13:32.657838: step 7353, loss 0.562567.
Train: 2018-08-06T01:13:32.907172: step 7354, loss 0.603544.
Train: 2018-08-06T01:13:33.154522: step 7355, loss 0.529694.
Train: 2018-08-06T01:13:33.400850: step 7356, loss 0.513121.
Train: 2018-08-06T01:13:33.648188: step 7357, loss 0.529484.
Train: 2018-08-06T01:13:33.910514: step 7358, loss 0.579034.
Train: 2018-08-06T01:13:34.170792: step 7359, loss 0.586699.
Train: 2018-08-06T01:13:34.415169: step 7360, loss 0.562235.
Test: 2018-08-06T01:13:35.702695: step 7360, loss 0.548303.
Train: 2018-08-06T01:13:35.939064: step 7361, loss 0.545434.
Train: 2018-08-06T01:13:36.185405: step 7362, loss 0.553425.
Train: 2018-08-06T01:13:36.434737: step 7363, loss 0.579088.
Train: 2018-08-06T01:13:36.687062: step 7364, loss 0.520476.
Train: 2018-08-06T01:13:36.931409: step 7365, loss 0.613722.
Train: 2018-08-06T01:13:37.181740: step 7366, loss 0.562336.
Train: 2018-08-06T01:13:37.441047: step 7367, loss 0.588791.
Train: 2018-08-06T01:13:37.688422: step 7368, loss 0.546242.
Train: 2018-08-06T01:13:37.934725: step 7369, loss 0.528663.
Train: 2018-08-06T01:13:38.181068: step 7370, loss 0.562359.
Test: 2018-08-06T01:13:39.440698: step 7370, loss 0.548837.
Train: 2018-08-06T01:13:39.683050: step 7371, loss 0.579393.
Train: 2018-08-06T01:13:39.944352: step 7372, loss 0.553947.
Train: 2018-08-06T01:13:40.191690: step 7373, loss 0.645419.
Train: 2018-08-06T01:13:40.442039: step 7374, loss 0.62036.
Train: 2018-08-06T01:13:40.687364: step 7375, loss 0.463727.
Train: 2018-08-06T01:13:40.933736: step 7376, loss 0.529632.
Train: 2018-08-06T01:13:41.185033: step 7377, loss 0.578991.
Train: 2018-08-06T01:13:41.434391: step 7378, loss 0.529684.
Train: 2018-08-06T01:13:41.683724: step 7379, loss 0.60362.
Train: 2018-08-06T01:13:41.931069: step 7380, loss 0.537922.
Test: 2018-08-06T01:13:43.193661: step 7380, loss 0.548847.
Train: 2018-08-06T01:13:43.443026: step 7381, loss 0.529727.
Train: 2018-08-06T01:13:43.690350: step 7382, loss 0.554336.
Train: 2018-08-06T01:13:43.937703: step 7383, loss 0.587247.
Train: 2018-08-06T01:13:44.185042: step 7384, loss 0.521422.
Train: 2018-08-06T01:13:44.447339: step 7385, loss 0.578962.
Train: 2018-08-06T01:13:44.696643: step 7386, loss 0.595492.
Train: 2018-08-06T01:13:44.942984: step 7387, loss 0.54608.
Train: 2018-08-06T01:13:45.189325: step 7388, loss 0.570736.
Train: 2018-08-06T01:13:45.433670: step 7389, loss 0.562546.
Train: 2018-08-06T01:13:45.681040: step 7390, loss 0.595433.
Test: 2018-08-06T01:13:46.934656: step 7390, loss 0.548479.
Train: 2018-08-06T01:13:47.174018: step 7391, loss 0.505168.
Train: 2018-08-06T01:13:47.433354: step 7392, loss 0.554399.
Train: 2018-08-06T01:13:47.682657: step 7393, loss 0.578971.
Train: 2018-08-06T01:13:47.932990: step 7394, loss 0.562516.
Train: 2018-08-06T01:13:48.180326: step 7395, loss 0.636409.
Train: 2018-08-06T01:13:48.431653: step 7396, loss 0.554368.
Train: 2018-08-06T01:13:48.684977: step 7397, loss 0.513541.
Train: 2018-08-06T01:13:48.934309: step 7398, loss 0.521721.
Train: 2018-08-06T01:13:49.123802: step 7399, loss 0.492778.
Train: 2018-08-06T01:13:49.371142: step 7400, loss 0.636362.
Test: 2018-08-06T01:13:50.622793: step 7400, loss 0.549051.
Train: 2018-08-06T01:13:51.570339: step 7401, loss 0.652781.
Train: 2018-08-06T01:13:51.813689: step 7402, loss 0.521647.
Train: 2018-08-06T01:13:52.065050: step 7403, loss 0.611668.
Train: 2018-08-06T01:13:52.314355: step 7404, loss 0.578938.
Train: 2018-08-06T01:13:52.560690: step 7405, loss 0.497439.
Train: 2018-08-06T01:13:52.813072: step 7406, loss 0.603366.
Train: 2018-08-06T01:13:53.060355: step 7407, loss 0.456864.
Train: 2018-08-06T01:13:53.311713: step 7408, loss 0.603371.
Train: 2018-08-06T01:13:53.560049: step 7409, loss 0.587077.
Train: 2018-08-06T01:13:53.813340: step 7410, loss 0.619672.
Test: 2018-08-06T01:13:55.076961: step 7410, loss 0.549431.
Train: 2018-08-06T01:13:55.314357: step 7411, loss 0.570781.
Train: 2018-08-06T01:13:55.561665: step 7412, loss 0.562664.
Train: 2018-08-06T01:13:55.811028: step 7413, loss 0.514014.
Train: 2018-08-06T01:13:56.071301: step 7414, loss 0.489691.
Train: 2018-08-06T01:13:56.315679: step 7415, loss 0.587032.
Train: 2018-08-06T01:13:56.571963: step 7416, loss 0.55452.
Train: 2018-08-06T01:13:56.827285: step 7417, loss 0.546361.
Train: 2018-08-06T01:13:57.083600: step 7418, loss 0.530027.
Train: 2018-08-06T01:13:57.330970: step 7419, loss 0.652428.
Train: 2018-08-06T01:13:57.582267: step 7420, loss 0.562607.
Test: 2018-08-06T01:13:58.852869: step 7420, loss 0.548232.
Train: 2018-08-06T01:13:59.093251: step 7421, loss 0.570771.
Train: 2018-08-06T01:13:59.354534: step 7422, loss 0.538148.
Train: 2018-08-06T01:13:59.602901: step 7423, loss 0.529983.
Train: 2018-08-06T01:13:59.849211: step 7424, loss 0.480959.
Train: 2018-08-06T01:14:00.097548: step 7425, loss 0.587139.
Train: 2018-08-06T01:14:00.347878: step 7426, loss 0.570762.
Train: 2018-08-06T01:14:00.593247: step 7427, loss 0.611827.
Train: 2018-08-06T01:14:00.842556: step 7428, loss 0.587188.
Train: 2018-08-06T01:14:01.092917: step 7429, loss 0.57076.
Train: 2018-08-06T01:14:01.344214: step 7430, loss 0.595375.
Test: 2018-08-06T01:14:02.623792: step 7430, loss 0.548699.
Train: 2018-08-06T01:14:02.860190: step 7431, loss 0.529799.
Train: 2018-08-06T01:14:03.111512: step 7432, loss 0.53801.
Train: 2018-08-06T01:14:03.358851: step 7433, loss 0.578953.
Train: 2018-08-06T01:14:03.607195: step 7434, loss 0.587138.
Train: 2018-08-06T01:14:03.856495: step 7435, loss 0.472618.
Train: 2018-08-06T01:14:04.103858: step 7436, loss 0.497059.
Train: 2018-08-06T01:14:04.365134: step 7437, loss 0.636448.
Train: 2018-08-06T01:14:04.611476: step 7438, loss 0.562543.
Train: 2018-08-06T01:14:04.868818: step 7439, loss 0.578979.
Train: 2018-08-06T01:14:05.120156: step 7440, loss 0.636525.
Test: 2018-08-06T01:14:06.391714: step 7440, loss 0.549226.
Train: 2018-08-06T01:14:06.630077: step 7441, loss 0.529727.
Train: 2018-08-06T01:14:06.876418: step 7442, loss 0.546161.
Train: 2018-08-06T01:14:07.126750: step 7443, loss 0.595354.
Train: 2018-08-06T01:14:07.375110: step 7444, loss 0.521638.
Train: 2018-08-06T01:14:07.638380: step 7445, loss 0.529829.
Train: 2018-08-06T01:14:07.892701: step 7446, loss 0.554378.
Train: 2018-08-06T01:14:08.144029: step 7447, loss 0.619955.
Train: 2018-08-06T01:14:08.391395: step 7448, loss 0.619924.
Train: 2018-08-06T01:14:08.640701: step 7449, loss 0.546235.
Train: 2018-08-06T01:14:08.888039: step 7450, loss 0.57077.
Test: 2018-08-06T01:14:10.148667: step 7450, loss 0.548626.
Train: 2018-08-06T01:14:10.386054: step 7451, loss 0.595237.
Train: 2018-08-06T01:14:10.637361: step 7452, loss 0.538231.
Train: 2018-08-06T01:14:10.884699: step 7453, loss 0.595165.
Train: 2018-08-06T01:14:11.143022: step 7454, loss 0.514012.
Train: 2018-08-06T01:14:11.391375: step 7455, loss 0.522151.
Train: 2018-08-06T01:14:11.646690: step 7456, loss 0.56268.
Train: 2018-08-06T01:14:11.907000: step 7457, loss 0.530216.
Train: 2018-08-06T01:14:12.155302: step 7458, loss 0.611409.
Train: 2018-08-06T01:14:12.412613: step 7459, loss 0.652029.
Train: 2018-08-06T01:14:12.672918: step 7460, loss 0.52215.
Test: 2018-08-06T01:14:13.940526: step 7460, loss 0.548424.
Train: 2018-08-06T01:14:14.178890: step 7461, loss 0.603196.
Train: 2018-08-06T01:14:14.432242: step 7462, loss 0.570804.
Train: 2018-08-06T01:14:14.680574: step 7463, loss 0.619243.
Train: 2018-08-06T01:14:14.928884: step 7464, loss 0.594973.
Train: 2018-08-06T01:14:15.177219: step 7465, loss 0.602936.
Train: 2018-08-06T01:14:15.422589: step 7466, loss 0.578862.
Train: 2018-08-06T01:14:15.672894: step 7467, loss 0.610707.
Train: 2018-08-06T01:14:15.922228: step 7468, loss 0.531299.
Train: 2018-08-06T01:14:16.180549: step 7469, loss 0.523545.
Train: 2018-08-06T01:14:16.429901: step 7470, loss 0.563084.
Test: 2018-08-06T01:14:17.699473: step 7470, loss 0.548678.
Train: 2018-08-06T01:14:17.935866: step 7471, loss 0.673417.
Train: 2018-08-06T01:14:18.199169: step 7472, loss 0.555323.
Train: 2018-08-06T01:14:18.464463: step 7473, loss 0.531921.
Train: 2018-08-06T01:14:18.714759: step 7474, loss 0.594513.
Train: 2018-08-06T01:14:18.962123: step 7475, loss 0.563299.
Train: 2018-08-06T01:14:19.205446: step 7476, loss 0.532192.
Train: 2018-08-06T01:14:19.465750: step 7477, loss 0.57112.
Train: 2018-08-06T01:14:19.718100: step 7478, loss 0.610017.
Train: 2018-08-06T01:14:19.981371: step 7479, loss 0.540064.
Train: 2018-08-06T01:14:20.228736: step 7480, loss 0.555609.
Test: 2018-08-06T01:14:21.497317: step 7480, loss 0.550756.
Train: 2018-08-06T01:14:21.736677: step 7481, loss 0.586677.
Train: 2018-08-06T01:14:21.982046: step 7482, loss 0.509004.
Train: 2018-08-06T01:14:22.228362: step 7483, loss 0.633356.
Train: 2018-08-06T01:14:22.481709: step 7484, loss 0.547793.
Train: 2018-08-06T01:14:22.735044: step 7485, loss 0.594464.
Train: 2018-08-06T01:14:22.981382: step 7486, loss 0.493315.
Train: 2018-08-06T01:14:23.229721: step 7487, loss 0.563302.
Train: 2018-08-06T01:14:23.478052: step 7488, loss 0.563264.
Train: 2018-08-06T01:14:23.740331: step 7489, loss 0.52409.
Train: 2018-08-06T01:14:23.984697: step 7490, loss 0.547469.
Test: 2018-08-06T01:14:25.258265: step 7490, loss 0.549413.
Train: 2018-08-06T01:14:25.496628: step 7491, loss 0.594617.
Train: 2018-08-06T01:14:25.742970: step 7492, loss 0.507794.
Train: 2018-08-06T01:14:25.987316: step 7493, loss 0.586772.
Train: 2018-08-06T01:14:26.237645: step 7494, loss 0.515267.
Train: 2018-08-06T01:14:26.488975: step 7495, loss 0.53099.
Train: 2018-08-06T01:14:26.735357: step 7496, loss 0.610944.
Train: 2018-08-06T01:14:26.985670: step 7497, loss 0.562747.
Train: 2018-08-06T01:14:27.233008: step 7498, loss 0.506178.
Train: 2018-08-06T01:14:27.480348: step 7499, loss 0.53039.
Train: 2018-08-06T01:14:27.728686: step 7500, loss 0.546399.
Test: 2018-08-06T01:14:28.986294: step 7500, loss 0.548669.
Train: 2018-08-06T01:14:29.895450: step 7501, loss 0.546295.
Train: 2018-08-06T01:14:30.138799: step 7502, loss 0.505036.
Train: 2018-08-06T01:14:30.388158: step 7503, loss 0.570775.
Train: 2018-08-06T01:14:30.641480: step 7504, loss 0.595556.
Train: 2018-08-06T01:14:30.887800: step 7505, loss 0.570513.
Train: 2018-08-06T01:14:31.136170: step 7506, loss 0.536076.
Train: 2018-08-06T01:14:31.386463: step 7507, loss 0.535967.
Train: 2018-08-06T01:14:31.635797: step 7508, loss 0.521182.
Train: 2018-08-06T01:14:31.885129: step 7509, loss 0.501599.
Train: 2018-08-06T01:14:32.140478: step 7510, loss 0.514046.
Test: 2018-08-06T01:14:33.410051: step 7510, loss 0.547374.
Train: 2018-08-06T01:14:33.650413: step 7511, loss 0.572582.
Train: 2018-08-06T01:14:33.897747: step 7512, loss 0.577477.
Train: 2018-08-06T01:14:34.145129: step 7513, loss 0.626724.
Train: 2018-08-06T01:14:34.394452: step 7514, loss 0.597626.
Train: 2018-08-06T01:14:34.645773: step 7515, loss 0.577944.
Train: 2018-08-06T01:14:34.894114: step 7516, loss 0.574596.
Train: 2018-08-06T01:14:35.155384: step 7517, loss 0.61315.
Train: 2018-08-06T01:14:35.418704: step 7518, loss 0.573817.
Train: 2018-08-06T01:14:35.678018: step 7519, loss 0.564278.
Train: 2018-08-06T01:14:35.922333: step 7520, loss 0.571526.
Test: 2018-08-06T01:14:37.172988: step 7520, loss 0.549393.
Train: 2018-08-06T01:14:37.412348: step 7521, loss 0.52881.
Train: 2018-08-06T01:14:37.664699: step 7522, loss 0.603917.
Train: 2018-08-06T01:14:37.913035: step 7523, loss 0.562489.
Train: 2018-08-06T01:14:38.173344: step 7524, loss 0.496896.
Train: 2018-08-06T01:14:38.419684: step 7525, loss 0.595334.
Train: 2018-08-06T01:14:38.667018: step 7526, loss 0.578941.
Train: 2018-08-06T01:14:38.915379: step 7527, loss 0.578914.
Train: 2018-08-06T01:14:39.163678: step 7528, loss 0.578907.
Train: 2018-08-06T01:14:39.410033: step 7529, loss 0.562707.
Train: 2018-08-06T01:14:39.671340: step 7530, loss 0.546517.
Test: 2018-08-06T01:14:40.930938: step 7530, loss 0.550884.
Train: 2018-08-06T01:14:41.170329: step 7531, loss 0.554609.
Train: 2018-08-06T01:14:41.416652: step 7532, loss 0.546619.
Train: 2018-08-06T01:14:41.667999: step 7533, loss 0.554636.
Train: 2018-08-06T01:14:41.917300: step 7534, loss 0.538435.
Train: 2018-08-06T01:14:42.167661: step 7535, loss 0.54651.
Train: 2018-08-06T01:14:42.415995: step 7536, loss 0.497894.
Train: 2018-08-06T01:14:42.663305: step 7537, loss 0.514136.
Train: 2018-08-06T01:14:42.908650: step 7538, loss 0.513872.
Train: 2018-08-06T01:14:43.167959: step 7539, loss 0.578992.
Train: 2018-08-06T01:14:43.414297: step 7540, loss 0.56286.
Test: 2018-08-06T01:14:44.671933: step 7540, loss 0.548624.
Train: 2018-08-06T01:14:44.912321: step 7541, loss 0.496803.
Train: 2018-08-06T01:14:45.198718: step 7542, loss 0.479999.
Train: 2018-08-06T01:14:45.444095: step 7543, loss 0.512551.
Train: 2018-08-06T01:14:45.699386: step 7544, loss 0.570785.
Train: 2018-08-06T01:14:45.947716: step 7545, loss 0.554034.
Train: 2018-08-06T01:14:46.196071: step 7546, loss 0.520206.
Train: 2018-08-06T01:14:46.455385: step 7547, loss 0.664157.
Train: 2018-08-06T01:14:46.702727: step 7548, loss 0.545417.
Train: 2018-08-06T01:14:46.954055: step 7549, loss 0.61311.
Train: 2018-08-06T01:14:47.144515: step 7550, loss 0.489943.
Test: 2018-08-06T01:14:48.405143: step 7550, loss 0.547146.
Train: 2018-08-06T01:14:48.644503: step 7551, loss 0.656159.
Train: 2018-08-06T01:14:48.896844: step 7552, loss 0.570785.
Train: 2018-08-06T01:14:49.144192: step 7553, loss 0.55389.
Train: 2018-08-06T01:14:49.392504: step 7554, loss 0.545359.
Train: 2018-08-06T01:14:49.642859: step 7555, loss 0.562363.
Train: 2018-08-06T01:14:49.904163: step 7556, loss 0.503133.
Train: 2018-08-06T01:14:50.155494: step 7557, loss 0.57926.
Train: 2018-08-06T01:14:50.404796: step 7558, loss 0.621494.
Train: 2018-08-06T01:14:50.658147: step 7559, loss 0.486299.
Train: 2018-08-06T01:14:50.905457: step 7560, loss 0.520141.
Test: 2018-08-06T01:14:52.170100: step 7560, loss 0.547072.
Train: 2018-08-06T01:14:52.420438: step 7561, loss 0.486324.
Train: 2018-08-06T01:14:52.668762: step 7562, loss 0.494731.
Train: 2018-08-06T01:14:52.929092: step 7563, loss 0.545418.
Train: 2018-08-06T01:14:53.177381: step 7564, loss 0.587977.
Train: 2018-08-06T01:14:53.420731: step 7565, loss 0.596523.
Train: 2018-08-06T01:14:53.670109: step 7566, loss 0.579395.
Train: 2018-08-06T01:14:53.919428: step 7567, loss 0.579373.
Train: 2018-08-06T01:14:54.168761: step 7568, loss 0.596497.
Train: 2018-08-06T01:14:54.415103: step 7569, loss 0.519676.
Train: 2018-08-06T01:14:54.664437: step 7570, loss 0.55381.
Test: 2018-08-06T01:14:55.922040: step 7570, loss 0.547732.
Train: 2018-08-06T01:14:56.174367: step 7571, loss 0.562336.
Train: 2018-08-06T01:14:56.425726: step 7572, loss 0.596378.
Train: 2018-08-06T01:14:56.674056: step 7573, loss 0.570811.
Train: 2018-08-06T01:14:56.935330: step 7574, loss 0.562358.
Train: 2018-08-06T01:14:57.181695: step 7575, loss 0.494667.
Train: 2018-08-06T01:14:57.427016: step 7576, loss 0.553897.
Train: 2018-08-06T01:14:57.670365: step 7577, loss 0.613078.
Train: 2018-08-06T01:14:57.914740: step 7578, loss 0.579256.
Train: 2018-08-06T01:14:58.163073: step 7579, loss 0.570803.
Train: 2018-08-06T01:14:58.414406: step 7580, loss 0.537163.
Test: 2018-08-06T01:14:59.693953: step 7580, loss 0.547793.
Train: 2018-08-06T01:14:59.933315: step 7581, loss 0.545609.
Train: 2018-08-06T01:15:00.183645: step 7582, loss 0.587546.
Train: 2018-08-06T01:15:00.429012: step 7583, loss 0.570773.
Train: 2018-08-06T01:15:00.675363: step 7584, loss 0.620892.
Train: 2018-08-06T01:15:00.921695: step 7585, loss 0.579095.
Train: 2018-08-06T01:15:01.176988: step 7586, loss 0.537561.
Train: 2018-08-06T01:15:01.422331: step 7587, loss 0.504527.
Train: 2018-08-06T01:15:01.669703: step 7588, loss 0.579029.
Train: 2018-08-06T01:15:01.916037: step 7589, loss 0.595534.
Train: 2018-08-06T01:15:02.160358: step 7590, loss 0.496578.
Test: 2018-08-06T01:15:03.432955: step 7590, loss 0.549328.
Train: 2018-08-06T01:15:03.671317: step 7591, loss 0.570758.
Train: 2018-08-06T01:15:03.919652: step 7592, loss 0.546056.
Train: 2018-08-06T01:15:04.173008: step 7593, loss 0.603686.
Train: 2018-08-06T01:15:04.433279: step 7594, loss 0.480313.
Train: 2018-08-06T01:15:04.677626: step 7595, loss 0.653045.
Train: 2018-08-06T01:15:04.935963: step 7596, loss 0.570759.
Train: 2018-08-06T01:15:05.182300: step 7597, loss 0.554352.
Train: 2018-08-06T01:15:05.426654: step 7598, loss 0.578958.
Train: 2018-08-06T01:15:05.682964: step 7599, loss 0.52167.
Train: 2018-08-06T01:15:05.931298: step 7600, loss 0.611663.
Test: 2018-08-06T01:15:07.192899: step 7600, loss 0.549716.
Train: 2018-08-06T01:15:08.131796: step 7601, loss 0.554436.
Train: 2018-08-06T01:15:08.384121: step 7602, loss 0.497359.
Train: 2018-08-06T01:15:08.630436: step 7603, loss 0.570772.
Train: 2018-08-06T01:15:08.879772: step 7604, loss 0.497309.
Train: 2018-08-06T01:15:09.140100: step 7605, loss 0.570767.
Train: 2018-08-06T01:15:09.387414: step 7606, loss 0.587139.
Train: 2018-08-06T01:15:09.634781: step 7607, loss 0.587147.
Train: 2018-08-06T01:15:09.882089: step 7608, loss 0.619915.
Train: 2018-08-06T01:15:10.141396: step 7609, loss 0.546225.
Train: 2018-08-06T01:15:10.390755: step 7610, loss 0.497211.
Test: 2018-08-06T01:15:11.661331: step 7610, loss 0.548741.
Train: 2018-08-06T01:15:11.910665: step 7611, loss 0.644376.
Train: 2018-08-06T01:15:12.160033: step 7612, loss 0.578937.
Train: 2018-08-06T01:15:12.410353: step 7613, loss 0.636017.
Train: 2018-08-06T01:15:12.664649: step 7614, loss 0.578912.
Train: 2018-08-06T01:15:12.917971: step 7615, loss 0.514059.
Train: 2018-08-06T01:15:13.167337: step 7616, loss 0.554619.
Train: 2018-08-06T01:15:13.425614: step 7617, loss 0.554646.
Train: 2018-08-06T01:15:13.672951: step 7618, loss 0.498152.
Train: 2018-08-06T01:15:13.932283: step 7619, loss 0.538488.
Train: 2018-08-06T01:15:14.179596: step 7620, loss 0.546533.
Test: 2018-08-06T01:15:15.432246: step 7620, loss 0.548973.
Train: 2018-08-06T01:15:15.671606: step 7621, loss 0.587.
Train: 2018-08-06T01:15:15.923931: step 7622, loss 0.546461.
Train: 2018-08-06T01:15:16.170301: step 7623, loss 0.562667.
Train: 2018-08-06T01:15:16.419632: step 7624, loss 0.611434.
Train: 2018-08-06T01:15:16.669962: step 7625, loss 0.456986.
Train: 2018-08-06T01:15:16.918298: step 7626, loss 0.481152.
Train: 2018-08-06T01:15:17.164619: step 7627, loss 0.603491.
Train: 2018-08-06T01:15:17.412985: step 7628, loss 0.628175.
Train: 2018-08-06T01:15:17.665278: step 7629, loss 0.562555.
Train: 2018-08-06T01:15:17.915605: step 7630, loss 0.628262.
Test: 2018-08-06T01:15:19.182218: step 7630, loss 0.546957.
Train: 2018-08-06T01:15:19.498501: step 7631, loss 0.505096.
Train: 2018-08-06T01:15:19.743845: step 7632, loss 0.521492.
Train: 2018-08-06T01:15:20.007140: step 7633, loss 0.620089.
Train: 2018-08-06T01:15:20.253482: step 7634, loss 0.603639.
Train: 2018-08-06T01:15:20.505807: step 7635, loss 0.578971.
Train: 2018-08-06T01:15:20.751175: step 7636, loss 0.529756.
Train: 2018-08-06T01:15:20.997520: step 7637, loss 0.53798.
Train: 2018-08-06T01:15:21.257825: step 7638, loss 0.587155.
Train: 2018-08-06T01:15:21.515138: step 7639, loss 0.578957.
Train: 2018-08-06T01:15:21.774449: step 7640, loss 0.521657.
Test: 2018-08-06T01:15:23.043021: step 7640, loss 0.549098.
Train: 2018-08-06T01:15:23.290368: step 7641, loss 0.554391.
Train: 2018-08-06T01:15:23.535734: step 7642, loss 0.538013.
Train: 2018-08-06T01:15:23.781049: step 7643, loss 0.505208.
Train: 2018-08-06T01:15:24.030382: step 7644, loss 0.562554.
Train: 2018-08-06T01:15:24.277720: step 7645, loss 0.587204.
Train: 2018-08-06T01:15:24.526080: step 7646, loss 0.570761.
Train: 2018-08-06T01:15:24.776385: step 7647, loss 0.57076.
Train: 2018-08-06T01:15:25.029709: step 7648, loss 0.521326.
Train: 2018-08-06T01:15:25.286023: step 7649, loss 0.579002.
Train: 2018-08-06T01:15:25.534391: step 7650, loss 0.570771.
Test: 2018-08-06T01:15:26.790000: step 7650, loss 0.547943.
Train: 2018-08-06T01:15:27.027367: step 7651, loss 0.587279.
Train: 2018-08-06T01:15:27.275702: step 7652, loss 0.587271.
Train: 2018-08-06T01:15:27.538997: step 7653, loss 0.554257.
Train: 2018-08-06T01:15:27.781375: step 7654, loss 0.653175.
Train: 2018-08-06T01:15:28.024698: step 7655, loss 0.496785.
Train: 2018-08-06T01:15:28.276026: step 7656, loss 0.554331.
Train: 2018-08-06T01:15:28.522398: step 7657, loss 0.59538.
Train: 2018-08-06T01:15:28.770703: step 7658, loss 0.578957.
Train: 2018-08-06T01:15:29.034000: step 7659, loss 0.619842.
Train: 2018-08-06T01:15:29.289349: step 7660, loss 0.546311.
Test: 2018-08-06T01:15:30.559918: step 7660, loss 0.548683.
Train: 2018-08-06T01:15:30.801273: step 7661, loss 0.603324.
Train: 2018-08-06T01:15:31.051603: step 7662, loss 0.643794.
Train: 2018-08-06T01:15:31.295950: step 7663, loss 0.530437.
Train: 2018-08-06T01:15:31.559270: step 7664, loss 0.490349.
Train: 2018-08-06T01:15:31.815576: step 7665, loss 0.522604.
Train: 2018-08-06T01:15:32.063896: step 7666, loss 0.54672.
Train: 2018-08-06T01:15:32.315224: step 7667, loss 0.602998.
Train: 2018-08-06T01:15:32.565555: step 7668, loss 0.578872.
Train: 2018-08-06T01:15:32.812926: step 7669, loss 0.52265.
Train: 2018-08-06T01:15:33.061257: step 7670, loss 0.530666.
Test: 2018-08-06T01:15:34.331831: step 7670, loss 0.549538.
Train: 2018-08-06T01:15:34.572189: step 7671, loss 0.603005.
Train: 2018-08-06T01:15:34.816535: step 7672, loss 0.530605.
Train: 2018-08-06T01:15:35.066896: step 7673, loss 0.594983.
Train: 2018-08-06T01:15:35.332155: step 7674, loss 0.530552.
Train: 2018-08-06T01:15:35.589469: step 7675, loss 0.635316.
Train: 2018-08-06T01:15:35.844816: step 7676, loss 0.578878.
Train: 2018-08-06T01:15:36.092149: step 7677, loss 0.522541.
Train: 2018-08-06T01:15:36.344499: step 7678, loss 0.562778.
Train: 2018-08-06T01:15:36.605750: step 7679, loss 0.554726.
Train: 2018-08-06T01:15:36.853120: step 7680, loss 0.490291.
Test: 2018-08-06T01:15:38.114714: step 7680, loss 0.547964.
Train: 2018-08-06T01:15:38.355092: step 7681, loss 0.611166.
Train: 2018-08-06T01:15:38.604404: step 7682, loss 0.546577.
Train: 2018-08-06T01:15:38.852741: step 7683, loss 0.586978.
Train: 2018-08-06T01:15:39.103072: step 7684, loss 0.595075.
Train: 2018-08-06T01:15:39.352435: step 7685, loss 0.546534.
Train: 2018-08-06T01:15:39.601754: step 7686, loss 0.627439.
Train: 2018-08-06T01:15:39.857054: step 7687, loss 0.530405.
Train: 2018-08-06T01:15:40.108417: step 7688, loss 0.611197.
Train: 2018-08-06T01:15:40.356719: step 7689, loss 0.514353.
Train: 2018-08-06T01:15:40.604058: step 7690, loss 0.595013.
Test: 2018-08-06T01:15:41.872663: step 7690, loss 0.549672.
Train: 2018-08-06T01:15:42.113021: step 7691, loss 0.514406.
Train: 2018-08-06T01:15:42.356372: step 7692, loss 0.506311.
Train: 2018-08-06T01:15:42.615677: step 7693, loss 0.595043.
Train: 2018-08-06T01:15:42.859042: step 7694, loss 0.538461.
Train: 2018-08-06T01:15:43.107368: step 7695, loss 0.5627.
Train: 2018-08-06T01:15:43.355698: step 7696, loss 0.570792.
Train: 2018-08-06T01:15:43.600045: step 7697, loss 0.59514.
Train: 2018-08-06T01:15:43.850405: step 7698, loss 0.46524.
Train: 2018-08-06T01:15:44.099709: step 7699, loss 0.587058.
Train: 2018-08-06T01:15:44.348075: step 7700, loss 0.521857.
Test: 2018-08-06T01:15:45.624630: step 7700, loss 0.547817.
Train: 2018-08-06T01:15:46.467432: step 7701, loss 0.54516.
Train: 2018-08-06T01:15:46.719763: step 7702, loss 0.578957.
Train: 2018-08-06T01:15:46.978094: step 7703, loss 0.58718.
Train: 2018-08-06T01:15:47.223410: step 7704, loss 0.56254.
Train: 2018-08-06T01:15:47.472760: step 7705, loss 0.554303.
Train: 2018-08-06T01:15:47.721103: step 7706, loss 0.537815.
Train: 2018-08-06T01:15:47.968448: step 7707, loss 0.562509.
Train: 2018-08-06T01:15:48.229722: step 7708, loss 0.612044.
Train: 2018-08-06T01:15:48.479082: step 7709, loss 0.521215.
Train: 2018-08-06T01:15:48.726420: step 7710, loss 0.545966.
Test: 2018-08-06T01:15:49.987018: step 7710, loss 0.549051.
Train: 2018-08-06T01:15:50.227401: step 7711, loss 0.537669.
Train: 2018-08-06T01:15:50.471753: step 7712, loss 0.562473.
Train: 2018-08-06T01:15:50.718064: step 7713, loss 0.520997.
Train: 2018-08-06T01:15:50.966431: step 7714, loss 0.595688.
Train: 2018-08-06T01:15:51.215763: step 7715, loss 0.595711.
Train: 2018-08-06T01:15:51.461107: step 7716, loss 0.620658.
Train: 2018-08-06T01:15:51.706451: step 7717, loss 0.496022.
Train: 2018-08-06T01:15:51.954789: step 7718, loss 0.545845.
Train: 2018-08-06T01:15:52.201122: step 7719, loss 0.52922.
Train: 2018-08-06T01:15:52.457438: step 7720, loss 0.612341.
Test: 2018-08-06T01:15:53.735992: step 7720, loss 0.548947.
Train: 2018-08-06T01:15:53.981336: step 7721, loss 0.554134.
Train: 2018-08-06T01:15:54.227696: step 7722, loss 0.520892.
Train: 2018-08-06T01:15:54.478033: step 7723, loss 0.587394.
Train: 2018-08-06T01:15:54.737345: step 7724, loss 0.545813.
Train: 2018-08-06T01:15:54.993654: step 7725, loss 0.562444.
Train: 2018-08-06T01:15:55.245985: step 7726, loss 0.545807.
Train: 2018-08-06T01:15:55.495287: step 7727, loss 0.570762.
Train: 2018-08-06T01:15:55.746644: step 7728, loss 0.579083.
Train: 2018-08-06T01:15:56.002931: step 7729, loss 0.562444.
Train: 2018-08-06T01:15:56.261238: step 7730, loss 0.579074.
Test: 2018-08-06T01:15:57.534833: step 7730, loss 0.548392.
Train: 2018-08-06T01:15:57.772198: step 7731, loss 0.628896.
Train: 2018-08-06T01:15:58.020534: step 7732, loss 0.612183.
Train: 2018-08-06T01:15:58.268870: step 7733, loss 0.612041.
Train: 2018-08-06T01:15:58.522192: step 7734, loss 0.496762.
Train: 2018-08-06T01:15:58.767567: step 7735, loss 0.587166.
Train: 2018-08-06T01:15:59.012905: step 7736, loss 0.562587.
Train: 2018-08-06T01:15:59.255263: step 7737, loss 0.611569.
Train: 2018-08-06T01:15:59.507557: step 7738, loss 0.538256.
Train: 2018-08-06T01:15:59.754929: step 7739, loss 0.554568.
Train: 2018-08-06T01:16:00.001267: step 7740, loss 0.538419.
Test: 2018-08-06T01:16:01.256878: step 7740, loss 0.549961.
Train: 2018-08-06T01:16:01.494245: step 7741, loss 0.586975.
Train: 2018-08-06T01:16:01.745573: step 7742, loss 0.570812.
Train: 2018-08-06T01:16:01.995934: step 7743, loss 0.546642.
Train: 2018-08-06T01:16:02.242291: step 7744, loss 0.594979.
Train: 2018-08-06T01:16:02.490610: step 7745, loss 0.49849.
Train: 2018-08-06T01:16:02.738916: step 7746, loss 0.570833.
Train: 2018-08-06T01:16:02.988274: step 7747, loss 0.635152.
Train: 2018-08-06T01:16:03.247585: step 7748, loss 0.442413.
Train: 2018-08-06T01:16:03.496921: step 7749, loss 0.602989.
Train: 2018-08-06T01:16:03.758189: step 7750, loss 0.594959.
Test: 2018-08-06T01:16:05.007847: step 7750, loss 0.548809.
Train: 2018-08-06T01:16:05.260173: step 7751, loss 0.611035.
Train: 2018-08-06T01:16:05.507537: step 7752, loss 0.57887.
Train: 2018-08-06T01:16:05.753878: step 7753, loss 0.546797.
Train: 2018-08-06T01:16:06.002222: step 7754, loss 0.546824.
Train: 2018-08-06T01:16:06.250556: step 7755, loss 0.586873.
Train: 2018-08-06T01:16:06.498885: step 7756, loss 0.56286.
Train: 2018-08-06T01:16:06.757171: step 7757, loss 0.578863.
Train: 2018-08-06T01:16:07.000545: step 7758, loss 0.5469.
Train: 2018-08-06T01:16:07.249852: step 7759, loss 0.522943.
Train: 2018-08-06T01:16:07.496226: step 7760, loss 0.554877.
Test: 2018-08-06T01:16:08.757819: step 7760, loss 0.548937.
Train: 2018-08-06T01:16:09.009177: step 7761, loss 0.538846.
Train: 2018-08-06T01:16:09.260499: step 7762, loss 0.506716.
Train: 2018-08-06T01:16:09.512801: step 7763, loss 0.586913.
Train: 2018-08-06T01:16:09.776096: step 7764, loss 0.570821.
Train: 2018-08-06T01:16:10.037398: step 7765, loss 0.562742.
Train: 2018-08-06T01:16:10.284736: step 7766, loss 0.562721.
Train: 2018-08-06T01:16:10.532105: step 7767, loss 0.586992.
Train: 2018-08-06T01:16:10.782405: step 7768, loss 0.578898.
Train: 2018-08-06T01:16:11.029774: step 7769, loss 0.505941.
Train: 2018-08-06T01:16:11.283096: step 7770, loss 0.513944.
Test: 2018-08-06T01:16:12.550675: step 7770, loss 0.548475.
Train: 2018-08-06T01:16:12.788042: step 7771, loss 0.521923.
Train: 2018-08-06T01:16:13.033385: step 7772, loss 0.58711.
Train: 2018-08-06T01:16:13.284743: step 7773, loss 0.628097.
Train: 2018-08-06T01:16:13.531083: step 7774, loss 0.546175.
Train: 2018-08-06T01:16:13.780413: step 7775, loss 0.521538.
Train: 2018-08-06T01:16:14.028722: step 7776, loss 0.529667.
Train: 2018-08-06T01:16:14.290051: step 7777, loss 0.546045.
Train: 2018-08-06T01:16:14.532383: step 7778, loss 0.6781.
Train: 2018-08-06T01:16:14.786701: step 7779, loss 0.562504.
Train: 2018-08-06T01:16:15.031049: step 7780, loss 0.603749.
Test: 2018-08-06T01:16:16.315612: step 7780, loss 0.548577.
Train: 2018-08-06T01:16:16.553989: step 7781, loss 0.554285.
Train: 2018-08-06T01:16:16.799347: step 7782, loss 0.554306.
Train: 2018-08-06T01:16:17.045671: step 7783, loss 0.513231.
Train: 2018-08-06T01:16:17.294994: step 7784, loss 0.521438.
Train: 2018-08-06T01:16:17.543331: step 7785, loss 0.53784.
Train: 2018-08-06T01:16:17.792662: step 7786, loss 0.578999.
Train: 2018-08-06T01:16:18.037008: step 7787, loss 0.529508.
Train: 2018-08-06T01:16:18.288370: step 7788, loss 0.562494.
Train: 2018-08-06T01:16:18.541688: step 7789, loss 0.537664.
Train: 2018-08-06T01:16:18.801963: step 7790, loss 0.579044.
Test: 2018-08-06T01:16:20.063589: step 7790, loss 0.550326.
Train: 2018-08-06T01:16:20.304945: step 7791, loss 0.545871.
Train: 2018-08-06T01:16:20.551315: step 7792, loss 0.620595.
Train: 2018-08-06T01:16:20.799621: step 7793, loss 0.545849.
Train: 2018-08-06T01:16:21.053966: step 7794, loss 0.603973.
Train: 2018-08-06T01:16:21.297291: step 7795, loss 0.587348.
Train: 2018-08-06T01:16:21.551611: step 7796, loss 0.521065.
Train: 2018-08-06T01:16:21.801942: step 7797, loss 0.562479.
Train: 2018-08-06T01:16:22.053270: step 7798, loss 0.620392.
Train: 2018-08-06T01:16:22.308586: step 7799, loss 0.562499.
Train: 2018-08-06T01:16:22.554951: step 7800, loss 0.570757.
Test: 2018-08-06T01:16:23.819544: step 7800, loss 0.54936.
Train: 2018-08-06T01:16:24.822189: step 7801, loss 0.51317.
Train: 2018-08-06T01:16:25.066509: step 7802, loss 0.644757.
Train: 2018-08-06T01:16:25.311852: step 7803, loss 0.529753.
Train: 2018-08-06T01:16:25.560219: step 7804, loss 0.587143.
Train: 2018-08-06T01:16:25.807527: step 7805, loss 0.497204.
Train: 2018-08-06T01:16:26.053868: step 7806, loss 0.595284.
Train: 2018-08-06T01:16:26.304230: step 7807, loss 0.521788.
Train: 2018-08-06T01:16:26.551563: step 7808, loss 0.562607.
Train: 2018-08-06T01:16:26.799907: step 7809, loss 0.554441.
Train: 2018-08-06T01:16:27.049207: step 7810, loss 0.554437.
Test: 2018-08-06T01:16:28.318811: step 7810, loss 0.549332.
Train: 2018-08-06T01:16:28.568170: step 7811, loss 0.513582.
Train: 2018-08-06T01:16:28.826453: step 7812, loss 0.546221.
Train: 2018-08-06T01:16:29.073792: step 7813, loss 0.53798.
Train: 2018-08-06T01:16:29.319146: step 7814, loss 0.546122.
Train: 2018-08-06T01:16:29.568502: step 7815, loss 0.546066.
Train: 2018-08-06T01:16:29.819797: step 7816, loss 0.554257.
Train: 2018-08-06T01:16:30.065141: step 7817, loss 0.512882.
Train: 2018-08-06T01:16:30.309514: step 7818, loss 0.587343.
Train: 2018-08-06T01:16:30.556826: step 7819, loss 0.56245.
Train: 2018-08-06T01:16:30.810177: step 7820, loss 0.604061.
Test: 2018-08-06T01:16:32.066788: step 7820, loss 0.547182.
Train: 2018-08-06T01:16:32.319138: step 7821, loss 0.554105.
Train: 2018-08-06T01:16:32.573434: step 7822, loss 0.595766.
Train: 2018-08-06T01:16:32.818810: step 7823, loss 0.512447.
Train: 2018-08-06T01:16:33.065118: step 7824, loss 0.629124.
Train: 2018-08-06T01:16:33.325421: step 7825, loss 0.620736.
Train: 2018-08-06T01:16:33.569799: step 7826, loss 0.537522.
Train: 2018-08-06T01:16:33.821125: step 7827, loss 0.554166.
Train: 2018-08-06T01:16:34.066471: step 7828, loss 0.545902.
Train: 2018-08-06T01:16:34.318791: step 7829, loss 0.587311.
Train: 2018-08-06T01:16:34.567102: step 7830, loss 0.554226.
Test: 2018-08-06T01:16:35.841692: step 7830, loss 0.547194.
Train: 2018-08-06T01:16:36.093021: step 7831, loss 0.603777.
Train: 2018-08-06T01:16:36.338365: step 7832, loss 0.562519.
Train: 2018-08-06T01:16:36.589692: step 7833, loss 0.480319.
Train: 2018-08-06T01:16:36.851025: step 7834, loss 0.611871.
Train: 2018-08-06T01:16:37.110334: step 7835, loss 0.554332.
Train: 2018-08-06T01:16:37.358662: step 7836, loss 0.611795.
Train: 2018-08-06T01:16:37.616946: step 7837, loss 0.570764.
Train: 2018-08-06T01:16:37.866331: step 7838, loss 0.472667.
Train: 2018-08-06T01:16:38.116657: step 7839, loss 0.5953.
Train: 2018-08-06T01:16:38.364992: step 7840, loss 0.570768.
Test: 2018-08-06T01:16:39.629585: step 7840, loss 0.549899.
Train: 2018-08-06T01:16:39.865954: step 7841, loss 0.464577.
Train: 2018-08-06T01:16:40.118287: step 7842, loss 0.562583.
Train: 2018-08-06T01:16:40.369636: step 7843, loss 0.628128.
Train: 2018-08-06T01:16:40.630934: step 7844, loss 0.505216.
Train: 2018-08-06T01:16:40.884230: step 7845, loss 0.53795.
Train: 2018-08-06T01:16:41.146556: step 7846, loss 0.529682.
Train: 2018-08-06T01:16:41.391903: step 7847, loss 0.537826.
Train: 2018-08-06T01:16:41.638243: step 7848, loss 0.537744.
Train: 2018-08-06T01:16:41.891563: step 7849, loss 0.537656.
Train: 2018-08-06T01:16:42.150873: step 7850, loss 0.570759.
Test: 2018-08-06T01:16:43.431417: step 7850, loss 0.547593.
Train: 2018-08-06T01:16:43.679784: step 7851, loss 0.612353.
Train: 2018-08-06T01:16:43.873236: step 7852, loss 0.686765.
Train: 2018-08-06T01:16:44.126589: step 7853, loss 0.512605.
Train: 2018-08-06T01:16:44.370938: step 7854, loss 0.537557.
Train: 2018-08-06T01:16:44.622281: step 7855, loss 0.620545.
Train: 2018-08-06T01:16:44.867608: step 7856, loss 0.612176.
Train: 2018-08-06T01:16:45.115946: step 7857, loss 0.480894.
Train: 2018-08-06T01:16:45.425287: step 7858, loss 0.570757.
Train: 2018-08-06T01:16:45.679607: step 7859, loss 0.537758.
Train: 2018-08-06T01:16:45.925921: step 7860, loss 0.562508.
Test: 2018-08-06T01:16:47.186550: step 7860, loss 0.548549.
Train: 2018-08-06T01:16:47.438901: step 7861, loss 0.60374.
Train: 2018-08-06T01:16:47.688239: step 7862, loss 0.58723.
Train: 2018-08-06T01:16:47.949539: step 7863, loss 0.537871.
Train: 2018-08-06T01:16:48.199874: step 7864, loss 0.562546.
Train: 2018-08-06T01:16:48.449199: step 7865, loss 0.546144.
Train: 2018-08-06T01:16:48.697538: step 7866, loss 0.578963.
Train: 2018-08-06T01:16:48.950843: step 7867, loss 0.578957.
Train: 2018-08-06T01:16:49.200200: step 7868, loss 0.538028.
Train: 2018-08-06T01:16:49.454486: step 7869, loss 0.562586.
Train: 2018-08-06T01:16:49.704846: step 7870, loss 0.562591.
Test: 2018-08-06T01:16:50.991374: step 7870, loss 0.550076.
Train: 2018-08-06T01:16:51.232756: step 7871, loss 0.61163.
Train: 2018-08-06T01:16:51.481090: step 7872, loss 0.562613.
Train: 2018-08-06T01:16:51.729432: step 7873, loss 0.570776.
Train: 2018-08-06T01:16:51.977762: step 7874, loss 0.595189.
Train: 2018-08-06T01:16:52.223115: step 7875, loss 0.522071.
Train: 2018-08-06T01:16:52.483411: step 7876, loss 0.530229.
Train: 2018-08-06T01:16:52.740696: step 7877, loss 0.578905.
Train: 2018-08-06T01:16:52.989063: step 7878, loss 0.603233.
Train: 2018-08-06T01:16:53.237369: step 7879, loss 0.562697.
Train: 2018-08-06T01:16:53.484732: step 7880, loss 0.570802.
Test: 2018-08-06T01:16:54.754312: step 7880, loss 0.549789.
Train: 2018-08-06T01:16:54.993671: step 7881, loss 0.562726.
Train: 2018-08-06T01:16:55.239016: step 7882, loss 0.546593.
Train: 2018-08-06T01:16:55.485357: step 7883, loss 0.530468.
Train: 2018-08-06T01:16:55.734690: step 7884, loss 0.538521.
Train: 2018-08-06T01:16:55.981031: step 7885, loss 0.59505.
Train: 2018-08-06T01:16:56.227372: step 7886, loss 0.570806.
Train: 2018-08-06T01:16:56.476730: step 7887, loss 0.570806.
Train: 2018-08-06T01:16:56.736011: step 7888, loss 0.570807.
Train: 2018-08-06T01:16:56.981384: step 7889, loss 0.586967.
Train: 2018-08-06T01:16:57.228719: step 7890, loss 0.538519.
Test: 2018-08-06T01:16:58.494308: step 7890, loss 0.548886.
Train: 2018-08-06T01:16:58.738655: step 7891, loss 0.530448.
Train: 2018-08-06T01:16:58.999983: step 7892, loss 0.667755.
Train: 2018-08-06T01:16:59.245300: step 7893, loss 0.586946.
Train: 2018-08-06T01:16:59.499647: step 7894, loss 0.514495.
Train: 2018-08-06T01:16:59.748979: step 7895, loss 0.562789.
Train: 2018-08-06T01:17:00.001280: step 7896, loss 0.530646.
Train: 2018-08-06T01:17:00.247654: step 7897, loss 0.570832.
Train: 2018-08-06T01:17:00.494983: step 7898, loss 0.586916.
Train: 2018-08-06T01:17:00.753305: step 7899, loss 0.562793.
Train: 2018-08-06T01:17:01.003629: step 7900, loss 0.54672.
Test: 2018-08-06T01:17:02.278190: step 7900, loss 0.54918.
Train: 2018-08-06T01:17:03.202147: step 7901, loss 0.586913.
Train: 2018-08-06T01:17:03.463448: step 7902, loss 0.627101.
Train: 2018-08-06T01:17:03.710819: step 7903, loss 0.56282.
Train: 2018-08-06T01:17:03.958158: step 7904, loss 0.650978.
Train: 2018-08-06T01:17:04.207459: step 7905, loss 0.522963.
Train: 2018-08-06T01:17:04.453800: step 7906, loss 0.618708.
Train: 2018-08-06T01:17:04.715100: step 7907, loss 0.602696.
Train: 2018-08-06T01:17:04.961473: step 7908, loss 0.51552.
Train: 2018-08-06T01:17:05.207814: step 7909, loss 0.563056.
Train: 2018-08-06T01:17:05.457117: step 7910, loss 0.586754.
Test: 2018-08-06T01:17:06.741681: step 7910, loss 0.549779.
Train: 2018-08-06T01:17:06.983065: step 7911, loss 0.539485.
Train: 2018-08-06T01:17:07.233366: step 7912, loss 0.531651.
Train: 2018-08-06T01:17:07.481702: step 7913, loss 0.563125.
Train: 2018-08-06T01:17:07.727077: step 7914, loss 0.555244.
Train: 2018-08-06T01:17:07.973433: step 7915, loss 0.547345.
Train: 2018-08-06T01:17:08.222747: step 7916, loss 0.618312.
Train: 2018-08-06T01:17:08.469062: step 7917, loss 0.555196.
Train: 2018-08-06T01:17:08.734393: step 7918, loss 0.626211.
Train: 2018-08-06T01:17:08.978698: step 7919, loss 0.618275.
Train: 2018-08-06T01:17:09.226070: step 7920, loss 0.539547.
Test: 2018-08-06T01:17:10.502623: step 7920, loss 0.550225.
Train: 2018-08-06T01:17:10.753982: step 7921, loss 0.523889.
Train: 2018-08-06T01:17:11.016249: step 7922, loss 0.476753.
Train: 2018-08-06T01:17:11.261596: step 7923, loss 0.626113.
Train: 2018-08-06T01:17:11.507934: step 7924, loss 0.500057.
Train: 2018-08-06T01:17:11.761257: step 7925, loss 0.563061.
Train: 2018-08-06T01:17:12.017571: step 7926, loss 0.594698.
Train: 2018-08-06T01:17:12.262916: step 7927, loss 0.626447.
Train: 2018-08-06T01:17:12.511251: step 7928, loss 0.547135.
Train: 2018-08-06T01:17:12.756626: step 7929, loss 0.570924.
Train: 2018-08-06T01:17:13.003933: step 7930, loss 0.618547.
Test: 2018-08-06T01:17:14.268552: step 7930, loss 0.549387.
Train: 2018-08-06T01:17:14.504936: step 7931, loss 0.491621.
Train: 2018-08-06T01:17:14.750291: step 7932, loss 0.55504.
Train: 2018-08-06T01:17:14.997602: step 7933, loss 0.547058.
Train: 2018-08-06T01:17:15.259900: step 7934, loss 0.523112.
Train: 2018-08-06T01:17:15.503250: step 7935, loss 0.546921.
Train: 2018-08-06T01:17:15.751586: step 7936, loss 0.578865.
Train: 2018-08-06T01:17:16.012886: step 7937, loss 0.594922.
Train: 2018-08-06T01:17:16.269230: step 7938, loss 0.578872.
Train: 2018-08-06T01:17:16.528507: step 7939, loss 0.55474.
Train: 2018-08-06T01:17:16.789808: step 7940, loss 0.554717.
Test: 2018-08-06T01:17:18.065397: step 7940, loss 0.549473.
Train: 2018-08-06T01:17:18.305780: step 7941, loss 0.55469.
Train: 2018-08-06T01:17:18.553093: step 7942, loss 0.522364.
Train: 2018-08-06T01:17:18.800463: step 7943, loss 0.530338.
Train: 2018-08-06T01:17:19.050790: step 7944, loss 0.57079.
Train: 2018-08-06T01:17:19.300129: step 7945, loss 0.554512.
Train: 2018-08-06T01:17:19.546468: step 7946, loss 0.587082.
Train: 2018-08-06T01:17:19.794798: step 7947, loss 0.546272.
Train: 2018-08-06T01:17:20.056124: step 7948, loss 0.562586.
Train: 2018-08-06T01:17:20.306404: step 7949, loss 0.521606.
Train: 2018-08-06T01:17:20.553776: step 7950, loss 0.578972.
Test: 2018-08-06T01:17:21.810382: step 7950, loss 0.548605.
Train: 2018-08-06T01:17:22.121549: step 7951, loss 0.64479.
Train: 2018-08-06T01:17:22.371911: step 7952, loss 0.513204.
Train: 2018-08-06T01:17:22.631217: step 7953, loss 0.587213.
Train: 2018-08-06T01:17:22.878556: step 7954, loss 0.56253.
Train: 2018-08-06T01:17:23.123894: step 7955, loss 0.513166.
Train: 2018-08-06T01:17:23.379187: step 7956, loss 0.513104.
Train: 2018-08-06T01:17:23.626525: step 7957, loss 0.562503.
Train: 2018-08-06T01:17:23.874861: step 7958, loss 0.587291.
Train: 2018-08-06T01:17:24.133198: step 7959, loss 0.562481.
Train: 2018-08-06T01:17:24.393473: step 7960, loss 0.545911.
Test: 2018-08-06T01:17:25.644129: step 7960, loss 0.547859.
Train: 2018-08-06T01:17:25.894459: step 7961, loss 0.496146.
Train: 2018-08-06T01:17:26.147813: step 7962, loss 0.520903.
Train: 2018-08-06T01:17:26.398137: step 7963, loss 0.554097.
Train: 2018-08-06T01:17:26.656423: step 7964, loss 0.520632.
Train: 2018-08-06T01:17:26.901797: step 7965, loss 0.528862.
Train: 2018-08-06T01:17:27.163092: step 7966, loss 0.587618.
Train: 2018-08-06T01:17:27.410436: step 7967, loss 0.545502.
Train: 2018-08-06T01:17:27.671706: step 7968, loss 0.553907.
Train: 2018-08-06T01:17:27.919100: step 7969, loss 0.519995.
Train: 2018-08-06T01:17:28.179375: step 7970, loss 0.553853.
Test: 2018-08-06T01:17:29.439977: step 7970, loss 0.548318.
Train: 2018-08-06T01:17:29.676346: step 7971, loss 0.553828.
Train: 2018-08-06T01:17:29.922686: step 7972, loss 0.545275.
Train: 2018-08-06T01:17:30.172051: step 7973, loss 0.536688.
Train: 2018-08-06T01:17:30.416367: step 7974, loss 0.579471.
Train: 2018-08-06T01:17:30.666722: step 7975, loss 0.639545.
Train: 2018-08-06T01:17:30.916031: step 7976, loss 0.61377.
Train: 2018-08-06T01:17:31.165364: step 7977, loss 0.579447.
Train: 2018-08-06T01:17:31.410708: step 7978, loss 0.570873.
Train: 2018-08-06T01:17:31.663063: step 7979, loss 0.596392.
Train: 2018-08-06T01:17:31.910402: step 7980, loss 0.485992.
Test: 2018-08-06T01:17:33.188952: step 7980, loss 0.547426.
Train: 2018-08-06T01:17:33.427345: step 7981, loss 0.562355.
Train: 2018-08-06T01:17:33.683629: step 7982, loss 0.452404.
Train: 2018-08-06T01:17:33.944930: step 7983, loss 0.647006.
Train: 2018-08-06T01:17:34.193296: step 7984, loss 0.553907.
Train: 2018-08-06T01:17:34.437613: step 7985, loss 0.604585.
Train: 2018-08-06T01:17:34.687977: step 7986, loss 0.553947.
Train: 2018-08-06T01:17:34.935313: step 7987, loss 0.520333.
Train: 2018-08-06T01:17:35.189602: step 7988, loss 0.52878.
Train: 2018-08-06T01:17:35.450929: step 7989, loss 0.537189.
Train: 2018-08-06T01:17:35.719185: step 7990, loss 0.553986.
Test: 2018-08-06T01:17:36.967845: step 7990, loss 0.54758.
Train: 2018-08-06T01:17:37.204213: step 7991, loss 0.528778.
Train: 2018-08-06T01:17:37.450585: step 7992, loss 0.596015.
Train: 2018-08-06T01:17:37.701909: step 7993, loss 0.486725.
Train: 2018-08-06T01:17:37.955238: step 7994, loss 0.612882.
Train: 2018-08-06T01:17:38.206534: step 7995, loss 0.562378.
Train: 2018-08-06T01:17:38.450904: step 7996, loss 0.570793.
Train: 2018-08-06T01:17:38.699241: step 7997, loss 0.545561.
Train: 2018-08-06T01:17:38.944559: step 7998, loss 0.57079.
Train: 2018-08-06T01:17:39.190902: step 7999, loss 0.638007.
Train: 2018-08-06T01:17:39.437252: step 8000, loss 0.579162.
Test: 2018-08-06T01:17:40.716819: step 8000, loss 0.548834.
Train: 2018-08-06T01:17:41.623187: step 8001, loss 0.52061.
Train: 2018-08-06T01:17:41.886481: step 8002, loss 0.562421.
Train: 2018-08-06T01:17:42.076947: step 8003, loss 0.491314.
Train: 2018-08-06T01:17:42.324310: step 8004, loss 0.537426.
Train: 2018-08-06T01:17:42.570626: step 8005, loss 0.570766.
Train: 2018-08-06T01:17:42.816001: step 8006, loss 0.61247.
Train: 2018-08-06T01:17:43.069325: step 8007, loss 0.587427.
Train: 2018-08-06T01:17:43.332620: step 8008, loss 0.562444.
Train: 2018-08-06T01:17:43.577966: step 8009, loss 0.562454.
Train: 2018-08-06T01:17:43.836267: step 8010, loss 0.603926.
Test: 2018-08-06T01:17:45.097868: step 8010, loss 0.547908.
Train: 2018-08-06T01:17:45.337242: step 8011, loss 0.54594.
Train: 2018-08-06T01:17:45.585594: step 8012, loss 0.51296.
Train: 2018-08-06T01:17:45.834922: step 8013, loss 0.570756.
Train: 2018-08-06T01:17:46.082235: step 8014, loss 0.562512.
Train: 2018-08-06T01:17:46.330571: step 8015, loss 0.587234.
Train: 2018-08-06T01:17:46.578933: step 8016, loss 0.636578.
Train: 2018-08-06T01:17:46.839244: step 8017, loss 0.554355.
Train: 2018-08-06T01:17:47.088559: step 8018, loss 0.488938.
Train: 2018-08-06T01:17:47.340870: step 8019, loss 0.538042.
Train: 2018-08-06T01:17:47.589238: step 8020, loss 0.55437.
Test: 2018-08-06T01:17:48.862800: step 8020, loss 0.549227.
Train: 2018-08-06T01:17:49.108144: step 8021, loss 0.603337.
Train: 2018-08-06T01:17:49.354511: step 8022, loss 0.512743.
Train: 2018-08-06T01:17:49.597864: step 8023, loss 0.554372.
Train: 2018-08-06T01:17:49.848195: step 8024, loss 0.528823.
Train: 2018-08-06T01:17:50.110496: step 8025, loss 0.528213.
Train: 2018-08-06T01:17:50.360825: step 8026, loss 0.604012.
Train: 2018-08-06T01:17:50.609159: step 8027, loss 0.591697.
Train: 2018-08-06T01:17:50.860490: step 8028, loss 0.528029.
Train: 2018-08-06T01:17:51.109820: step 8029, loss 0.561372.
Train: 2018-08-06T01:17:51.355162: step 8030, loss 0.6127.
Test: 2018-08-06T01:17:52.634711: step 8030, loss 0.547278.
Train: 2018-08-06T01:17:52.875100: step 8031, loss 0.55347.
Train: 2018-08-06T01:17:53.127394: step 8032, loss 0.556495.
Train: 2018-08-06T01:17:53.380748: step 8033, loss 0.55351.
Train: 2018-08-06T01:17:53.623102: step 8034, loss 0.529489.
Train: 2018-08-06T01:17:53.868452: step 8035, loss 0.570912.
Train: 2018-08-06T01:17:54.115752: step 8036, loss 0.521246.
Train: 2018-08-06T01:17:54.367109: step 8037, loss 0.554459.
Train: 2018-08-06T01:17:54.611456: step 8038, loss 0.619735.
Train: 2018-08-06T01:17:54.859761: step 8039, loss 0.570954.
Train: 2018-08-06T01:17:55.121091: step 8040, loss 0.546437.
Test: 2018-08-06T01:17:56.396651: step 8040, loss 0.549442.
Train: 2018-08-06T01:17:56.648010: step 8041, loss 0.570774.
Train: 2018-08-06T01:17:56.895349: step 8042, loss 0.546398.
Train: 2018-08-06T01:17:57.155622: step 8043, loss 0.56264.
Train: 2018-08-06T01:17:57.401994: step 8044, loss 0.513991.
Train: 2018-08-06T01:17:57.649301: step 8045, loss 0.587008.
Train: 2018-08-06T01:17:57.896639: step 8046, loss 0.562662.
Train: 2018-08-06T01:17:58.142012: step 8047, loss 0.611396.
Train: 2018-08-06T01:17:58.389322: step 8048, loss 0.570785.
Train: 2018-08-06T01:17:58.645662: step 8049, loss 0.578916.
Train: 2018-08-06T01:17:58.892975: step 8050, loss 0.562684.
Test: 2018-08-06T01:18:00.160585: step 8050, loss 0.55012.
Train: 2018-08-06T01:18:00.399972: step 8051, loss 0.595079.
Train: 2018-08-06T01:18:00.645314: step 8052, loss 0.562728.
Train: 2018-08-06T01:18:00.891656: step 8053, loss 0.530473.
Train: 2018-08-06T01:18:01.147970: step 8054, loss 0.554688.
Train: 2018-08-06T01:18:01.397306: step 8055, loss 0.611134.
Train: 2018-08-06T01:18:01.647641: step 8056, loss 0.570824.
Train: 2018-08-06T01:18:01.896967: step 8057, loss 0.53866.
Train: 2018-08-06T01:18:02.145304: step 8058, loss 0.602989.
Train: 2018-08-06T01:18:02.403617: step 8059, loss 0.61901.
Train: 2018-08-06T01:18:02.652920: step 8060, loss 0.586873.
Test: 2018-08-06T01:18:03.913548: step 8060, loss 0.54992.
Train: 2018-08-06T01:18:04.154936: step 8061, loss 0.506989.
Train: 2018-08-06T01:18:04.418232: step 8062, loss 0.546948.
Train: 2018-08-06T01:18:04.667558: step 8063, loss 0.57886.
Train: 2018-08-06T01:18:04.929830: step 8064, loss 0.491189.
Train: 2018-08-06T01:18:05.186175: step 8065, loss 0.594822.
Train: 2018-08-06T01:18:05.440496: step 8066, loss 0.594832.
Train: 2018-08-06T01:18:05.699772: step 8067, loss 0.522976.
Train: 2018-08-06T01:18:05.953094: step 8068, loss 0.546898.
Train: 2018-08-06T01:18:06.199466: step 8069, loss 0.570862.
Train: 2018-08-06T01:18:06.450791: step 8070, loss 0.538813.
Test: 2018-08-06T01:18:07.703413: step 8070, loss 0.549235.
Train: 2018-08-06T01:18:07.940778: step 8071, loss 0.538749.
Train: 2018-08-06T01:18:08.192106: step 8072, loss 0.586915.
Train: 2018-08-06T01:18:08.440467: step 8073, loss 0.538612.
Train: 2018-08-06T01:18:08.688777: step 8074, loss 0.562746.
Train: 2018-08-06T01:18:08.938112: step 8075, loss 0.62739.
Train: 2018-08-06T01:18:09.191434: step 8076, loss 0.562721.
Train: 2018-08-06T01:18:09.452743: step 8077, loss 0.514206.
Train: 2018-08-06T01:18:09.701071: step 8078, loss 0.570799.
Train: 2018-08-06T01:18:09.948435: step 8079, loss 0.587004.
Train: 2018-08-06T01:18:10.196764: step 8080, loss 0.489711.
Test: 2018-08-06T01:18:11.464355: step 8080, loss 0.548715.
Train: 2018-08-06T01:18:11.707754: step 8081, loss 0.595162.
Train: 2018-08-06T01:18:11.955079: step 8082, loss 0.627725.
Train: 2018-08-06T01:18:12.217366: step 8083, loss 0.54639.
Train: 2018-08-06T01:18:12.463682: step 8084, loss 0.546391.
Train: 2018-08-06T01:18:12.714014: step 8085, loss 0.489448.
Train: 2018-08-06T01:18:12.968364: step 8086, loss 0.587076.
Train: 2018-08-06T01:18:13.214729: step 8087, loss 0.55445.
Train: 2018-08-06T01:18:13.475002: step 8088, loss 0.529908.
Train: 2018-08-06T01:18:13.724311: step 8089, loss 0.5462.
Train: 2018-08-06T01:18:13.969686: step 8090, loss 0.578967.
Test: 2018-08-06T01:18:15.231280: step 8090, loss 0.548059.
Train: 2018-08-06T01:18:15.475658: step 8091, loss 0.570759.
Train: 2018-08-06T01:18:15.736960: step 8092, loss 0.504939.
Train: 2018-08-06T01:18:15.986288: step 8093, loss 0.513031.
Train: 2018-08-06T01:18:16.237618: step 8094, loss 0.512848.
Train: 2018-08-06T01:18:16.483956: step 8095, loss 0.545845.
Train: 2018-08-06T01:18:16.730302: step 8096, loss 0.545755.
Train: 2018-08-06T01:18:16.976646: step 8097, loss 0.595879.
Train: 2018-08-06T01:18:17.226972: step 8098, loss 0.60433.
Train: 2018-08-06T01:18:17.474310: step 8099, loss 0.562389.
Train: 2018-08-06T01:18:17.727636: step 8100, loss 0.553982.
Test: 2018-08-06T01:18:18.988233: step 8100, loss 0.548337.
Train: 2018-08-06T01:18:19.979150: step 8101, loss 0.486697.
Train: 2018-08-06T01:18:20.237459: step 8102, loss 0.537088.
Train: 2018-08-06T01:18:20.481807: step 8103, loss 0.494776.
Train: 2018-08-06T01:18:20.732137: step 8104, loss 0.604739.
Train: 2018-08-06T01:18:20.979475: step 8105, loss 0.596326.
Train: 2018-08-06T01:18:21.231800: step 8106, loss 0.528341.
Train: 2018-08-06T01:18:21.480167: step 8107, loss 0.553831.
Train: 2018-08-06T01:18:21.729475: step 8108, loss 0.604949.
Train: 2018-08-06T01:18:21.983815: step 8109, loss 0.528259.
Train: 2018-08-06T01:18:22.235149: step 8110, loss 0.587911.
Test: 2018-08-06T01:18:23.489761: step 8110, loss 0.547132.
Train: 2018-08-06T01:18:23.728123: step 8111, loss 0.57938.
Train: 2018-08-06T01:18:23.974490: step 8112, loss 0.613408.
Train: 2018-08-06T01:18:24.233773: step 8113, loss 0.52838.
Train: 2018-08-06T01:18:24.478120: step 8114, loss 0.494517.
Train: 2018-08-06T01:18:24.730469: step 8115, loss 0.596268.
Train: 2018-08-06T01:18:24.985792: step 8116, loss 0.587767.
Train: 2018-08-06T01:18:25.239108: step 8117, loss 0.511618.
Train: 2018-08-06T01:18:25.487448: step 8118, loss 0.570813.
Train: 2018-08-06T01:18:25.740767: step 8119, loss 0.537029.
Train: 2018-08-06T01:18:25.986085: step 8120, loss 0.655231.
Test: 2018-08-06T01:18:27.261673: step 8120, loss 0.549089.
Train: 2018-08-06T01:18:27.502062: step 8121, loss 0.545536.
Train: 2018-08-06T01:18:27.761338: step 8122, loss 0.604395.
Train: 2018-08-06T01:18:28.004717: step 8123, loss 0.537271.
Train: 2018-08-06T01:18:28.256016: step 8124, loss 0.595843.
Train: 2018-08-06T01:18:28.503353: step 8125, loss 0.579097.
Train: 2018-08-06T01:18:28.750693: step 8126, loss 0.579066.
Train: 2018-08-06T01:18:28.997033: step 8127, loss 0.570757.
Train: 2018-08-06T01:18:29.246367: step 8128, loss 0.521247.
Train: 2018-08-06T01:18:29.495699: step 8129, loss 0.537819.
Train: 2018-08-06T01:18:29.742067: step 8130, loss 0.50497.
Test: 2018-08-06T01:18:31.025607: step 8130, loss 0.548988.
Train: 2018-08-06T01:18:31.264993: step 8131, loss 0.587207.
Train: 2018-08-06T01:18:31.524306: step 8132, loss 0.496777.
Train: 2018-08-06T01:18:31.770615: step 8133, loss 0.562528.
Train: 2018-08-06T01:18:32.028956: step 8134, loss 0.546048.
Train: 2018-08-06T01:18:32.275266: step 8135, loss 0.56251.
Train: 2018-08-06T01:18:32.524601: step 8136, loss 0.579011.
Train: 2018-08-06T01:18:32.771937: step 8137, loss 0.562497.
Train: 2018-08-06T01:18:33.031273: step 8138, loss 0.545981.
Train: 2018-08-06T01:18:33.277586: step 8139, loss 0.636864.
Train: 2018-08-06T01:18:33.523927: step 8140, loss 0.545997.
Test: 2018-08-06T01:18:34.817467: step 8140, loss 0.549309.
Train: 2018-08-06T01:18:35.055830: step 8141, loss 0.620231.
Train: 2018-08-06T01:18:35.300177: step 8142, loss 0.620128.
Train: 2018-08-06T01:18:35.556492: step 8143, loss 0.587163.
Train: 2018-08-06T01:18:35.827799: step 8144, loss 0.603452.
Train: 2018-08-06T01:18:36.084106: step 8145, loss 0.562644.
Train: 2018-08-06T01:18:36.330421: step 8146, loss 0.51406.
Train: 2018-08-06T01:18:36.579786: step 8147, loss 0.578889.
Train: 2018-08-06T01:18:36.841084: step 8148, loss 0.546607.
Train: 2018-08-06T01:18:37.104351: step 8149, loss 0.522484.
Train: 2018-08-06T01:18:37.354707: step 8150, loss 0.570829.
Test: 2018-08-06T01:18:38.620297: step 8150, loss 0.549136.
Train: 2018-08-06T01:18:38.867664: step 8151, loss 0.594973.
Train: 2018-08-06T01:18:39.116003: step 8152, loss 0.554687.
Train: 2018-08-06T01:18:39.368328: step 8153, loss 0.538656.
Train: 2018-08-06T01:18:39.557790: step 8154, loss 0.579789.
Train: 2018-08-06T01:18:39.806127: step 8155, loss 0.514311.
Train: 2018-08-06T01:18:40.054493: step 8156, loss 0.587629.
Train: 2018-08-06T01:18:40.298808: step 8157, loss 0.60294.
Train: 2018-08-06T01:18:40.545181: step 8158, loss 0.603553.
Train: 2018-08-06T01:18:40.796505: step 8159, loss 0.642742.
Train: 2018-08-06T01:18:41.041822: step 8160, loss 0.594884.
Test: 2018-08-06T01:18:42.318407: step 8160, loss 0.549768.
Train: 2018-08-06T01:18:42.559787: step 8161, loss 0.531125.
Train: 2018-08-06T01:18:42.808098: step 8162, loss 0.578795.
Train: 2018-08-06T01:18:43.062418: step 8163, loss 0.563003.
Train: 2018-08-06T01:18:43.324717: step 8164, loss 0.547256.
Train: 2018-08-06T01:18:43.571088: step 8165, loss 0.578871.
Train: 2018-08-06T01:18:43.820422: step 8166, loss 0.484358.
Train: 2018-08-06T01:18:44.073744: step 8167, loss 0.570982.
Train: 2018-08-06T01:18:44.331070: step 8168, loss 0.563086.
Train: 2018-08-06T01:18:44.587365: step 8169, loss 0.578862.
Train: 2018-08-06T01:18:44.846646: step 8170, loss 0.602563.
Test: 2018-08-06T01:18:46.130213: step 8170, loss 0.549512.
Train: 2018-08-06T01:18:46.376585: step 8171, loss 0.563066.
Train: 2018-08-06T01:18:46.683723: step 8172, loss 0.55517.
Train: 2018-08-06T01:18:46.944026: step 8173, loss 0.531465.
Train: 2018-08-06T01:18:47.190400: step 8174, loss 0.618407.
Train: 2018-08-06T01:18:47.431774: step 8175, loss 0.53932.
Train: 2018-08-06T01:18:47.679094: step 8176, loss 0.563038.
Train: 2018-08-06T01:18:47.929392: step 8177, loss 0.547183.
Train: 2018-08-06T01:18:48.174760: step 8178, loss 0.555071.
Train: 2018-08-06T01:18:48.424097: step 8179, loss 0.570916.
Train: 2018-08-06T01:18:48.671406: step 8180, loss 0.539106.
Test: 2018-08-06T01:18:49.928046: step 8180, loss 0.54926.
Train: 2018-08-06T01:18:50.183365: step 8181, loss 0.610718.
Train: 2018-08-06T01:18:50.442671: step 8182, loss 0.570889.
Train: 2018-08-06T01:18:50.690008: step 8183, loss 0.53102.
Train: 2018-08-06T01:18:50.940339: step 8184, loss 0.570878.
Train: 2018-08-06T01:18:51.187677: step 8185, loss 0.570867.
Train: 2018-08-06T01:18:51.442995: step 8186, loss 0.554861.
Train: 2018-08-06T01:18:51.690361: step 8187, loss 0.538819.
Train: 2018-08-06T01:18:51.936674: step 8188, loss 0.594914.
Train: 2018-08-06T01:18:52.182047: step 8189, loss 0.586898.
Train: 2018-08-06T01:18:52.434369: step 8190, loss 0.498561.
Test: 2018-08-06T01:18:53.710929: step 8190, loss 0.548234.
Train: 2018-08-06T01:18:53.950290: step 8191, loss 0.586921.
Train: 2018-08-06T01:18:54.199623: step 8192, loss 0.554712.
Train: 2018-08-06T01:18:54.467930: step 8193, loss 0.56275.
Train: 2018-08-06T01:18:54.729231: step 8194, loss 0.554659.
Train: 2018-08-06T01:18:54.978540: step 8195, loss 0.595064.
Train: 2018-08-06T01:18:55.226876: step 8196, loss 0.53844.
Train: 2018-08-06T01:18:55.480199: step 8197, loss 0.538401.
Train: 2018-08-06T01:18:55.725572: step 8198, loss 0.578903.
Train: 2018-08-06T01:18:55.978865: step 8199, loss 0.595149.
Train: 2018-08-06T01:18:56.222246: step 8200, loss 0.587031.
Test: 2018-08-06T01:18:57.517748: step 8200, loss 0.549484.
Train: 2018-08-06T01:18:58.476875: step 8201, loss 0.513957.
Train: 2018-08-06T01:18:58.727210: step 8202, loss 0.570785.
Train: 2018-08-06T01:18:58.974576: step 8203, loss 0.546392.
Train: 2018-08-06T01:18:59.223879: step 8204, loss 0.497539.
Train: 2018-08-06T01:18:59.470220: step 8205, loss 0.554458.
Train: 2018-08-06T01:18:59.715594: step 8206, loss 0.570767.
Train: 2018-08-06T01:18:59.966922: step 8207, loss 0.529801.
Train: 2018-08-06T01:19:00.217222: step 8208, loss 0.537908.
Train: 2018-08-06T01:19:00.466585: step 8209, loss 0.488398.
Train: 2018-08-06T01:19:00.721873: step 8210, loss 0.570757.
Test: 2018-08-06T01:19:02.003444: step 8210, loss 0.548599.
Train: 2018-08-06T01:19:02.245822: step 8211, loss 0.562459.
Train: 2018-08-06T01:19:02.496157: step 8212, loss 0.504166.
Train: 2018-08-06T01:19:02.746457: step 8213, loss 0.57077.
Train: 2018-08-06T01:19:02.995790: step 8214, loss 0.55401.
Train: 2018-08-06T01:19:03.241165: step 8215, loss 0.562381.
Train: 2018-08-06T01:19:03.488504: step 8216, loss 0.604525.
Train: 2018-08-06T01:19:03.733847: step 8217, loss 0.545488.
Train: 2018-08-06T01:19:03.994152: step 8218, loss 0.553913.
Train: 2018-08-06T01:19:04.241495: step 8219, loss 0.570817.
Train: 2018-08-06T01:19:04.488799: step 8220, loss 0.50311.
Test: 2018-08-06T01:19:05.749427: step 8220, loss 0.548779.
Train: 2018-08-06T01:19:05.989785: step 8221, loss 0.562352.
Train: 2018-08-06T01:19:06.238150: step 8222, loss 0.545371.
Train: 2018-08-06T01:19:06.486457: step 8223, loss 0.630348.
Train: 2018-08-06T01:19:06.734792: step 8224, loss 0.545355.
Train: 2018-08-06T01:19:06.980135: step 8225, loss 0.545361.
Train: 2018-08-06T01:19:07.229468: step 8226, loss 0.57084.
Train: 2018-08-06T01:19:07.476833: step 8227, loss 0.562349.
Train: 2018-08-06T01:19:07.729132: step 8228, loss 0.579315.
Train: 2018-08-06T01:19:07.991457: step 8229, loss 0.596243.
Train: 2018-08-06T01:19:08.246749: step 8230, loss 0.587726.
Test: 2018-08-06T01:19:09.504385: step 8230, loss 0.548091.
Train: 2018-08-06T01:19:09.742748: step 8231, loss 0.494903.
Train: 2018-08-06T01:19:09.991082: step 8232, loss 0.587648.
Train: 2018-08-06T01:19:10.246437: step 8233, loss 0.478268.
Train: 2018-08-06T01:19:10.493739: step 8234, loss 0.579206.
Train: 2018-08-06T01:19:10.742105: step 8235, loss 0.621259.
Train: 2018-08-06T01:19:10.989413: step 8236, loss 0.612767.
Train: 2018-08-06T01:19:11.235754: step 8237, loss 0.537287.
Train: 2018-08-06T01:19:11.491103: step 8238, loss 0.562416.
Train: 2018-08-06T01:19:11.738442: step 8239, loss 0.529083.
Train: 2018-08-06T01:19:11.993753: step 8240, loss 0.562437.
Test: 2018-08-06T01:19:13.265326: step 8240, loss 0.548365.
Train: 2018-08-06T01:19:13.504686: step 8241, loss 0.604026.
Train: 2018-08-06T01:19:13.763993: step 8242, loss 0.479479.
Train: 2018-08-06T01:19:14.014334: step 8243, loss 0.562461.
Train: 2018-08-06T01:19:14.261661: step 8244, loss 0.545871.
Train: 2018-08-06T01:19:14.510995: step 8245, loss 0.570758.
Train: 2018-08-06T01:19:14.759378: step 8246, loss 0.504395.
Train: 2018-08-06T01:19:15.004701: step 8247, loss 0.645501.
Train: 2018-08-06T01:19:15.253036: step 8248, loss 0.537574.
Train: 2018-08-06T01:19:15.516306: step 8249, loss 0.554173.
Train: 2018-08-06T01:19:15.760684: step 8250, loss 0.529308.
Test: 2018-08-06T01:19:17.031256: step 8250, loss 0.548995.
Train: 2018-08-06T01:19:17.268651: step 8251, loss 0.52929.
Train: 2018-08-06T01:19:17.514962: step 8252, loss 0.529247.
Train: 2018-08-06T01:19:17.764308: step 8253, loss 0.554129.
Train: 2018-08-06T01:19:18.016646: step 8254, loss 0.545776.
Train: 2018-08-06T01:19:18.261994: step 8255, loss 0.545737.
Train: 2018-08-06T01:19:18.509333: step 8256, loss 0.562413.
Train: 2018-08-06T01:19:18.756640: step 8257, loss 0.595886.
Train: 2018-08-06T01:19:19.014951: step 8258, loss 0.570776.
Train: 2018-08-06T01:19:19.268273: step 8259, loss 0.49541.
Train: 2018-08-06T01:19:19.516609: step 8260, loss 0.52885.
Test: 2018-08-06T01:19:20.785217: step 8260, loss 0.54932.
Train: 2018-08-06T01:19:21.025576: step 8261, loss 0.604396.
Train: 2018-08-06T01:19:21.270916: step 8262, loss 0.553975.
Train: 2018-08-06T01:19:21.517257: step 8263, loss 0.511908.
Train: 2018-08-06T01:19:21.764598: step 8264, loss 0.638195.
Train: 2018-08-06T01:19:22.009965: step 8265, loss 0.654997.
Train: 2018-08-06T01:19:22.256310: step 8266, loss 0.61277.
Train: 2018-08-06T01:19:22.498664: step 8267, loss 0.545678.
Train: 2018-08-06T01:19:22.747997: step 8268, loss 0.570766.
Train: 2018-08-06T01:19:22.995330: step 8269, loss 0.612321.
Train: 2018-08-06T01:19:23.236685: step 8270, loss 0.545923.
Test: 2018-08-06T01:19:24.496292: step 8270, loss 0.54835.
Train: 2018-08-06T01:19:24.808481: step 8271, loss 0.537759.
Train: 2018-08-06T01:19:25.057815: step 8272, loss 0.537845.
Train: 2018-08-06T01:19:25.304164: step 8273, loss 0.57076.
Train: 2018-08-06T01:19:25.553463: step 8274, loss 0.570762.
Train: 2018-08-06T01:19:25.798807: step 8275, loss 0.505302.
Train: 2018-08-06T01:19:26.046146: step 8276, loss 0.562586.
Train: 2018-08-06T01:19:26.309469: step 8277, loss 0.578945.
Train: 2018-08-06T01:19:26.558775: step 8278, loss 0.554425.
Train: 2018-08-06T01:19:26.802162: step 8279, loss 0.546265.
Train: 2018-08-06T01:19:27.049465: step 8280, loss 0.497258.
Test: 2018-08-06T01:19:28.306102: step 8280, loss 0.549111.
Train: 2018-08-06T01:19:28.543468: step 8281, loss 0.529863.
Train: 2018-08-06T01:19:28.788812: step 8282, loss 0.537969.
Train: 2018-08-06T01:19:29.048150: step 8283, loss 0.628292.
Train: 2018-08-06T01:19:29.296485: step 8284, loss 0.562534.
Train: 2018-08-06T01:19:29.546815: step 8285, loss 0.620129.
Train: 2018-08-06T01:19:29.798150: step 8286, loss 0.554317.
Train: 2018-08-06T01:19:30.045481: step 8287, loss 0.60362.
Train: 2018-08-06T01:19:30.295816: step 8288, loss 0.570762.
Train: 2018-08-06T01:19:30.544133: step 8289, loss 0.603515.
Train: 2018-08-06T01:19:30.792484: step 8290, loss 0.578937.
Test: 2018-08-06T01:19:32.054079: step 8290, loss 0.548469.
Train: 2018-08-06T01:19:32.291469: step 8291, loss 0.6115.
Train: 2018-08-06T01:19:32.537819: step 8292, loss 0.611365.
Train: 2018-08-06T01:19:32.797112: step 8293, loss 0.562729.
Train: 2018-08-06T01:19:33.044466: step 8294, loss 0.562781.
Train: 2018-08-06T01:19:33.291775: step 8295, loss 0.538774.
Train: 2018-08-06T01:19:33.547121: step 8296, loss 0.546868.
Train: 2018-08-06T01:19:33.799444: step 8297, loss 0.610801.
Train: 2018-08-06T01:19:34.047752: step 8298, loss 0.547004.
Train: 2018-08-06T01:19:34.296101: step 8299, loss 0.586807.
Train: 2018-08-06T01:19:34.547422: step 8300, loss 0.547132.
Test: 2018-08-06T01:19:35.823010: step 8300, loss 0.550701.
Train: 2018-08-06T01:19:36.771790: step 8301, loss 0.547176.
Train: 2018-08-06T01:19:37.019178: step 8302, loss 0.555114.
Train: 2018-08-06T01:19:37.266493: step 8303, loss 0.547206.
Train: 2018-08-06T01:19:37.510813: step 8304, loss 0.539279.
Train: 2018-08-06T01:19:37.702302: step 8305, loss 0.529196.
Train: 2018-08-06T01:19:37.952632: step 8306, loss 0.49945.
Train: 2018-08-06T01:19:38.209944: step 8307, loss 0.546983.
Train: 2018-08-06T01:19:38.459313: step 8308, loss 0.546867.
Train: 2018-08-06T01:19:38.722598: step 8309, loss 0.57084.
Train: 2018-08-06T01:19:38.974924: step 8310, loss 0.546651.
Test: 2018-08-06T01:19:40.234529: step 8310, loss 0.549032.
Train: 2018-08-06T01:19:40.472918: step 8311, loss 0.595059.
Train: 2018-08-06T01:19:40.728244: step 8312, loss 0.619418.
Train: 2018-08-06T01:19:40.977542: step 8313, loss 0.595122.
Train: 2018-08-06T01:19:41.237871: step 8314, loss 0.603229.
Train: 2018-08-06T01:19:41.483223: step 8315, loss 0.554595.
Train: 2018-08-06T01:19:41.732523: step 8316, loss 0.595084.
Train: 2018-08-06T01:19:41.979862: step 8317, loss 0.546555.
Train: 2018-08-06T01:19:42.239168: step 8318, loss 0.562733.
Train: 2018-08-06T01:19:42.493488: step 8319, loss 0.635383.
Train: 2018-08-06T01:19:42.753818: step 8320, loss 0.55472.
Test: 2018-08-06T01:19:44.015470: step 8320, loss 0.549745.
Train: 2018-08-06T01:19:44.256825: step 8321, loss 0.570835.
Train: 2018-08-06T01:19:44.519123: step 8322, loss 0.546778.
Train: 2018-08-06T01:19:44.768456: step 8323, loss 0.458664.
Train: 2018-08-06T01:19:45.014797: step 8324, loss 0.490572.
Train: 2018-08-06T01:19:45.259143: step 8325, loss 0.578878.
Train: 2018-08-06T01:19:45.516455: step 8326, loss 0.538504.
Train: 2018-08-06T01:19:45.761801: step 8327, loss 0.48168.
Train: 2018-08-06T01:19:46.010161: step 8328, loss 0.562639.
Train: 2018-08-06T01:19:46.255480: step 8329, loss 0.627996.
Train: 2018-08-06T01:19:46.507836: step 8330, loss 0.570763.
Test: 2018-08-06T01:19:47.767436: step 8330, loss 0.547519.
Train: 2018-08-06T01:19:48.009789: step 8331, loss 0.554342.
Train: 2018-08-06T01:19:48.256129: step 8332, loss 0.554311.
Train: 2018-08-06T01:19:48.503468: step 8333, loss 0.546042.
Train: 2018-08-06T01:19:48.753830: step 8334, loss 0.57901.
Train: 2018-08-06T01:19:49.001137: step 8335, loss 0.545963.
Train: 2018-08-06T01:19:49.248479: step 8336, loss 0.504544.
Train: 2018-08-06T01:19:49.505818: step 8337, loss 0.454585.
Train: 2018-08-06T01:19:49.762103: step 8338, loss 0.570765.
Train: 2018-08-06T01:19:50.008477: step 8339, loss 0.537293.
Train: 2018-08-06T01:19:50.252814: step 8340, loss 0.55398.
Test: 2018-08-06T01:19:51.522394: step 8340, loss 0.549448.
Train: 2018-08-06T01:19:51.762769: step 8341, loss 0.553935.
Train: 2018-08-06T01:19:52.008096: step 8342, loss 0.579281.
Train: 2018-08-06T01:19:52.272413: step 8343, loss 0.562351.
Train: 2018-08-06T01:19:52.534687: step 8344, loss 0.468872.
Train: 2018-08-06T01:19:52.782026: step 8345, loss 0.511171.
Train: 2018-08-06T01:19:53.029363: step 8346, loss 0.562336.
Train: 2018-08-06T01:19:53.280725: step 8347, loss 0.622482.
Train: 2018-08-06T01:19:53.531031: step 8348, loss 0.596757.
Train: 2018-08-06T01:19:53.793346: step 8349, loss 0.511104.
Train: 2018-08-06T01:19:54.052627: step 8350, loss 0.562335.
Test: 2018-08-06T01:19:55.320237: step 8350, loss 0.546742.
Train: 2018-08-06T01:19:55.559597: step 8351, loss 0.614087.
Train: 2018-08-06T01:19:55.805939: step 8352, loss 0.579574.
Train: 2018-08-06T01:19:56.070241: step 8353, loss 0.527905.
Train: 2018-08-06T01:19:56.315610: step 8354, loss 0.562335.
Train: 2018-08-06T01:19:56.559922: step 8355, loss 0.527959.
Train: 2018-08-06T01:19:56.812277: step 8356, loss 0.60529.
Train: 2018-08-06T01:19:57.058594: step 8357, loss 0.596648.
Train: 2018-08-06T01:19:57.309922: step 8358, loss 0.673587.
Train: 2018-08-06T01:19:57.559281: step 8359, loss 0.545309.
Train: 2018-08-06T01:19:57.820555: step 8360, loss 0.502986.
Test: 2018-08-06T01:19:59.092155: step 8360, loss 0.54726.
Train: 2018-08-06T01:19:59.328554: step 8361, loss 0.570817.
Train: 2018-08-06T01:19:59.575887: step 8362, loss 0.469591.
Train: 2018-08-06T01:19:59.823235: step 8363, loss 0.562371.
Train: 2018-08-06T01:20:00.071567: step 8364, loss 0.60449.
Train: 2018-08-06T01:20:00.315914: step 8365, loss 0.553974.
Train: 2018-08-06T01:20:00.563221: step 8366, loss 0.570785.
Train: 2018-08-06T01:20:00.808595: step 8367, loss 0.520496.
Train: 2018-08-06T01:20:01.069900: step 8368, loss 0.62102.
Train: 2018-08-06T01:20:01.319224: step 8369, loss 0.512279.
Train: 2018-08-06T01:20:01.567535: step 8370, loss 0.587464.
Test: 2018-08-06T01:20:02.850104: step 8370, loss 0.54966.
Train: 2018-08-06T01:20:03.096446: step 8371, loss 0.537424.
Train: 2018-08-06T01:20:03.356775: step 8372, loss 0.570763.
Train: 2018-08-06T01:20:03.605112: step 8373, loss 0.504218.
Train: 2018-08-06T01:20:03.853448: step 8374, loss 0.562442.
Train: 2018-08-06T01:20:04.105770: step 8375, loss 0.562441.
Train: 2018-08-06T01:20:04.359070: step 8376, loss 0.587403.
Train: 2018-08-06T01:20:04.619373: step 8377, loss 0.595704.
Train: 2018-08-06T01:20:04.866737: step 8378, loss 0.562457.
Train: 2018-08-06T01:20:05.116070: step 8379, loss 0.545888.
Train: 2018-08-06T01:20:05.364406: step 8380, loss 0.603883.
Test: 2018-08-06T01:20:06.637974: step 8380, loss 0.548117.
Train: 2018-08-06T01:20:06.886342: step 8381, loss 0.529431.
Train: 2018-08-06T01:20:07.136693: step 8382, loss 0.570756.
Train: 2018-08-06T01:20:07.384010: step 8383, loss 0.579003.
Train: 2018-08-06T01:20:07.641323: step 8384, loss 0.554291.
Train: 2018-08-06T01:20:07.899627: step 8385, loss 0.595428.
Train: 2018-08-06T01:20:08.159935: step 8386, loss 0.554347.
Train: 2018-08-06T01:20:08.408241: step 8387, loss 0.480632.
Train: 2018-08-06T01:20:08.656604: step 8388, loss 0.611751.
Train: 2018-08-06T01:20:08.909902: step 8389, loss 0.587147.
Train: 2018-08-06T01:20:09.156240: step 8390, loss 0.61167.
Test: 2018-08-06T01:20:10.438810: step 8390, loss 0.549356.
Train: 2018-08-06T01:20:10.693155: step 8391, loss 0.578933.
Train: 2018-08-06T01:20:10.936480: step 8392, loss 0.530075.
Train: 2018-08-06T01:20:11.186821: step 8393, loss 0.538269.
Train: 2018-08-06T01:20:11.433150: step 8394, loss 0.497683.
Train: 2018-08-06T01:20:11.677522: step 8395, loss 0.570783.
Train: 2018-08-06T01:20:11.924867: step 8396, loss 0.562645.
Train: 2018-08-06T01:20:12.177195: step 8397, loss 0.611482.
Train: 2018-08-06T01:20:12.437466: step 8398, loss 0.562645.
Train: 2018-08-06T01:20:12.683845: step 8399, loss 0.587045.
Train: 2018-08-06T01:20:12.939151: step 8400, loss 0.554542.
Test: 2018-08-06T01:20:14.241647: step 8400, loss 0.548558.
Train: 2018-08-06T01:20:15.204962: step 8401, loss 0.505859.
Train: 2018-08-06T01:20:15.450307: step 8402, loss 0.595153.
Train: 2018-08-06T01:20:15.709643: step 8403, loss 0.570787.
Train: 2018-08-06T01:20:15.966923: step 8404, loss 0.513954.
Train: 2018-08-06T01:20:16.215266: step 8405, loss 0.538277.
Train: 2018-08-06T01:20:16.465590: step 8406, loss 0.546363.
Train: 2018-08-06T01:20:16.708939: step 8407, loss 0.546315.
Train: 2018-08-06T01:20:16.954282: step 8408, loss 0.603446.
Train: 2018-08-06T01:20:17.211596: step 8409, loss 0.480839.
Train: 2018-08-06T01:20:17.459931: step 8410, loss 0.587157.
Test: 2018-08-06T01:20:18.718564: step 8410, loss 0.549402.
Train: 2018-08-06T01:20:18.955930: step 8411, loss 0.603605.
Train: 2018-08-06T01:20:19.205263: step 8412, loss 0.472169.
Train: 2018-08-06T01:20:19.451604: step 8413, loss 0.56252.
Train: 2018-08-06T01:20:19.703930: step 8414, loss 0.579013.
Train: 2018-08-06T01:20:19.950272: step 8415, loss 0.554217.
Train: 2018-08-06T01:20:20.197636: step 8416, loss 0.521062.
Train: 2018-08-06T01:20:20.452951: step 8417, loss 0.570759.
Train: 2018-08-06T01:20:20.700290: step 8418, loss 0.554129.
Train: 2018-08-06T01:20:20.956595: step 8419, loss 0.545773.
Train: 2018-08-06T01:20:21.215916: step 8420, loss 0.562422.
Test: 2018-08-06T01:20:22.478509: step 8420, loss 0.548648.
Train: 2018-08-06T01:20:22.716872: step 8421, loss 0.512269.
Train: 2018-08-06T01:20:22.972189: step 8422, loss 0.654549.
Train: 2018-08-06T01:20:23.223517: step 8423, loss 0.621033.
Train: 2018-08-06T01:20:23.472852: step 8424, loss 0.52896.
Train: 2018-08-06T01:20:23.722185: step 8425, loss 0.57077.
Train: 2018-08-06T01:20:23.975537: step 8426, loss 0.587462.
Train: 2018-08-06T01:20:24.225855: step 8427, loss 0.595764.
Train: 2018-08-06T01:20:24.476197: step 8428, loss 0.512568.
Train: 2018-08-06T01:20:24.721512: step 8429, loss 0.662106.
Train: 2018-08-06T01:20:24.968850: step 8430, loss 0.653517.
Test: 2018-08-06T01:20:26.248428: step 8430, loss 0.548208.
Train: 2018-08-06T01:20:26.493772: step 8431, loss 0.562525.
Train: 2018-08-06T01:20:26.743136: step 8432, loss 0.587149.
Train: 2018-08-06T01:20:26.988472: step 8433, loss 0.587079.
Train: 2018-08-06T01:20:27.243766: step 8434, loss 0.587013.
Train: 2018-08-06T01:20:27.502075: step 8435, loss 0.562745.
Train: 2018-08-06T01:20:27.749413: step 8436, loss 0.57887.
Train: 2018-08-06T01:20:27.997750: step 8437, loss 0.546881.
Train: 2018-08-06T01:20:28.244092: step 8438, loss 0.515124.
Train: 2018-08-06T01:20:28.489435: step 8439, loss 0.58681.
Train: 2018-08-06T01:20:28.748772: step 8440, loss 0.555054.
Test: 2018-08-06T01:20:30.025326: step 8440, loss 0.550332.
Train: 2018-08-06T01:20:30.269674: step 8441, loss 0.555093.
Train: 2018-08-06T01:20:30.523992: step 8442, loss 0.563033.
Train: 2018-08-06T01:20:30.772329: step 8443, loss 0.602582.
Train: 2018-08-06T01:20:31.015678: step 8444, loss 0.531496.
Train: 2018-08-06T01:20:31.258030: step 8445, loss 0.539408.
Train: 2018-08-06T01:20:31.505369: step 8446, loss 0.547286.
Train: 2018-08-06T01:20:31.752707: step 8447, loss 0.515648.
Train: 2018-08-06T01:20:32.000045: step 8448, loss 0.515504.
Train: 2018-08-06T01:20:32.246386: step 8449, loss 0.666267.
Train: 2018-08-06T01:20:32.493727: step 8450, loss 0.547056.
Test: 2018-08-06T01:20:33.757345: step 8450, loss 0.550382.
Train: 2018-08-06T01:20:34.009684: step 8451, loss 0.578859.
Train: 2018-08-06T01:20:34.270997: step 8452, loss 0.618679.
Train: 2018-08-06T01:20:34.527288: step 8453, loss 0.594776.
Train: 2018-08-06T01:20:34.785620: step 8454, loss 0.578858.
Train: 2018-08-06T01:20:35.034960: step 8455, loss 0.555048.
Train: 2018-08-06T01:20:35.225420: step 8456, loss 0.579916.
Train: 2018-08-06T01:20:35.478743: step 8457, loss 0.523417.
Train: 2018-08-06T01:20:35.728112: step 8458, loss 0.578859.
Train: 2018-08-06T01:20:35.985388: step 8459, loss 0.547177.
Train: 2018-08-06T01:20:36.232726: step 8460, loss 0.570935.
Test: 2018-08-06T01:20:37.511308: step 8460, loss 0.549765.
Train: 2018-08-06T01:20:37.755653: step 8461, loss 0.539222.
Train: 2018-08-06T01:20:38.002991: step 8462, loss 0.531242.
Train: 2018-08-06T01:20:38.265317: step 8463, loss 0.570907.
Train: 2018-08-06T01:20:38.508640: step 8464, loss 0.554965.
Train: 2018-08-06T01:20:38.756975: step 8465, loss 0.634717.
Train: 2018-08-06T01:20:39.002319: step 8466, loss 0.650673.
Train: 2018-08-06T01:20:39.246671: step 8467, loss 0.547011.
Train: 2018-08-06T01:20:39.494010: step 8468, loss 0.578858.
Train: 2018-08-06T01:20:39.742344: step 8469, loss 0.555042.
Train: 2018-08-06T01:20:39.996697: step 8470, loss 0.618512.
Test: 2018-08-06T01:20:41.260285: step 8470, loss 0.549635.
Train: 2018-08-06T01:20:41.500673: step 8471, loss 0.563033.
Train: 2018-08-06T01:20:41.749979: step 8472, loss 0.626255.
Train: 2018-08-06T01:20:42.000331: step 8473, loss 0.594615.
Train: 2018-08-06T01:20:42.246679: step 8474, loss 0.539636.
Train: 2018-08-06T01:20:42.492989: step 8475, loss 0.531901.
Train: 2018-08-06T01:20:42.742322: step 8476, loss 0.547594.
Train: 2018-08-06T01:20:43.003649: step 8477, loss 0.571063.
Train: 2018-08-06T01:20:43.247970: step 8478, loss 0.531974.
Train: 2018-08-06T01:20:43.492316: step 8479, loss 0.492804.
Train: 2018-08-06T01:20:43.748656: step 8480, loss 0.523941.
Test: 2018-08-06T01:20:45.008261: step 8480, loss 0.550499.
Train: 2018-08-06T01:20:45.246649: step 8481, loss 0.586743.
Train: 2018-08-06T01:20:45.494960: step 8482, loss 0.594663.
Train: 2018-08-06T01:20:45.743320: step 8483, loss 0.610527.
Train: 2018-08-06T01:20:45.993627: step 8484, loss 0.570936.
Train: 2018-08-06T01:20:46.243982: step 8485, loss 0.570931.
Train: 2018-08-06T01:20:46.506281: step 8486, loss 0.578859.
Train: 2018-08-06T01:20:46.808897: step 8487, loss 0.531254.
Train: 2018-08-06T01:20:47.056211: step 8488, loss 0.626518.
Train: 2018-08-06T01:20:47.304571: step 8489, loss 0.555039.
Train: 2018-08-06T01:20:47.550918: step 8490, loss 0.531222.
Test: 2018-08-06T01:20:48.815505: step 8490, loss 0.549327.
Train: 2018-08-06T01:20:49.058855: step 8491, loss 0.674219.
Train: 2018-08-06T01:20:49.307191: step 8492, loss 0.555063.
Train: 2018-08-06T01:20:49.559541: step 8493, loss 0.594702.
Train: 2018-08-06T01:20:49.806879: step 8494, loss 0.594674.
Train: 2018-08-06T01:20:50.052225: step 8495, loss 0.563087.
Train: 2018-08-06T01:20:50.301562: step 8496, loss 0.56312.
Train: 2018-08-06T01:20:50.552884: step 8497, loss 0.571008.
Train: 2018-08-06T01:20:50.809204: step 8498, loss 0.594573.
Train: 2018-08-06T01:20:51.056543: step 8499, loss 0.633728.
Train: 2018-08-06T01:20:51.302865: step 8500, loss 0.571078.
Test: 2018-08-06T01:20:52.569466: step 8500, loss 0.549782.
Train: 2018-08-06T01:20:53.471188: step 8501, loss 0.547751.
Train: 2018-08-06T01:20:53.714537: step 8502, loss 0.578907.
Train: 2018-08-06T01:20:53.968883: step 8503, loss 0.555647.
Train: 2018-08-06T01:20:54.220219: step 8504, loss 0.60216.
Train: 2018-08-06T01:20:54.473539: step 8505, loss 0.62532.
Train: 2018-08-06T01:20:54.720877: step 8506, loss 0.532703.
Train: 2018-08-06T01:20:54.967217: step 8507, loss 0.494314.
Train: 2018-08-06T01:20:55.216521: step 8508, loss 0.578954.
Train: 2018-08-06T01:20:55.475828: step 8509, loss 0.540427.
Train: 2018-08-06T01:20:55.727187: step 8510, loss 0.486351.
Test: 2018-08-06T01:20:56.989778: step 8510, loss 0.550144.
Train: 2018-08-06T01:20:57.241131: step 8511, loss 0.55569.
Train: 2018-08-06T01:20:57.501436: step 8512, loss 0.516716.
Train: 2018-08-06T01:20:57.747751: step 8513, loss 0.54764.
Train: 2018-08-06T01:20:57.999079: step 8514, loss 0.625979.
Train: 2018-08-06T01:20:58.244437: step 8515, loss 0.547365.
Train: 2018-08-06T01:20:58.505772: step 8516, loss 0.602567.
Train: 2018-08-06T01:20:58.753094: step 8517, loss 0.602616.
Train: 2018-08-06T01:20:58.999438: step 8518, loss 0.547149.
Train: 2018-08-06T01:20:59.243751: step 8519, loss 0.539159.
Train: 2018-08-06T01:20:59.494081: step 8520, loss 0.539075.
Test: 2018-08-06T01:21:00.752715: step 8520, loss 0.548846.
Train: 2018-08-06T01:21:00.992101: step 8521, loss 0.562904.
Train: 2018-08-06T01:21:01.240443: step 8522, loss 0.546871.
Train: 2018-08-06T01:21:01.487750: step 8523, loss 0.490648.
Train: 2018-08-06T01:21:01.737108: step 8524, loss 0.546658.
Train: 2018-08-06T01:21:01.983457: step 8525, loss 0.554622.
Train: 2018-08-06T01:21:02.235749: step 8526, loss 0.570786.
Train: 2018-08-06T01:21:02.483087: step 8527, loss 0.554469.
Train: 2018-08-06T01:21:02.739403: step 8528, loss 0.546225.
Train: 2018-08-06T01:21:02.985744: step 8529, loss 0.554344.
Train: 2018-08-06T01:21:03.233110: step 8530, loss 0.480175.
Test: 2018-08-06T01:21:04.485731: step 8530, loss 0.549426.
Train: 2018-08-06T01:21:04.724119: step 8531, loss 0.587305.
Train: 2018-08-06T01:21:04.972430: step 8532, loss 0.570759.
Train: 2018-08-06T01:21:05.219797: step 8533, loss 0.545777.
Train: 2018-08-06T01:21:05.471121: step 8534, loss 0.59583.
Train: 2018-08-06T01:21:05.715443: step 8535, loss 0.579142.
Train: 2018-08-06T01:21:05.969763: step 8536, loss 0.554024.
Train: 2018-08-06T01:21:06.217101: step 8537, loss 0.562395.
Train: 2018-08-06T01:21:06.466435: step 8538, loss 0.595956.
Train: 2018-08-06T01:21:06.722749: step 8539, loss 0.520448.
Train: 2018-08-06T01:21:06.970121: step 8540, loss 0.537208.
Test: 2018-08-06T01:21:08.248668: step 8540, loss 0.54874.
Train: 2018-08-06T01:21:08.492018: step 8541, loss 0.520376.
Train: 2018-08-06T01:21:08.740379: step 8542, loss 0.596041.
Train: 2018-08-06T01:21:08.988723: step 8543, loss 0.562376.
Train: 2018-08-06T01:21:09.237033: step 8544, loss 0.587644.
Train: 2018-08-06T01:21:09.484364: step 8545, loss 0.503439.
Train: 2018-08-06T01:21:09.732730: step 8546, loss 0.553947.
Train: 2018-08-06T01:21:09.981066: step 8547, loss 0.587667.
Train: 2018-08-06T01:21:10.231366: step 8548, loss 0.553939.
Train: 2018-08-06T01:21:10.485715: step 8549, loss 0.596093.
Train: 2018-08-06T01:21:10.741031: step 8550, loss 0.54553.
Test: 2018-08-06T01:21:12.026565: step 8550, loss 0.548903.
Train: 2018-08-06T01:21:12.264927: step 8551, loss 0.621292.
Train: 2018-08-06T01:21:12.511299: step 8552, loss 0.562387.
Train: 2018-08-06T01:21:12.758609: step 8553, loss 0.47859.
Train: 2018-08-06T01:21:13.012958: step 8554, loss 0.545638.
Train: 2018-08-06T01:21:13.265286: step 8555, loss 0.562398.
Train: 2018-08-06T01:21:13.514618: step 8556, loss 0.57916.
Train: 2018-08-06T01:21:13.763918: step 8557, loss 0.5624.
Train: 2018-08-06T01:21:14.015246: step 8558, loss 0.587519.
Train: 2018-08-06T01:21:14.264580: step 8559, loss 0.562411.
Train: 2018-08-06T01:21:14.512915: step 8560, loss 0.579119.
Test: 2018-08-06T01:21:15.767560: step 8560, loss 0.548315.
Train: 2018-08-06T01:21:16.010911: step 8561, loss 0.529084.
Train: 2018-08-06T01:21:16.259245: step 8562, loss 0.512459.
Train: 2018-08-06T01:21:16.518553: step 8563, loss 0.579096.
Train: 2018-08-06T01:21:16.769916: step 8564, loss 0.520786.
Train: 2018-08-06T01:21:17.020241: step 8565, loss 0.52909.
Train: 2018-08-06T01:21:17.276525: step 8566, loss 0.604148.
Train: 2018-08-06T01:21:17.521917: step 8567, loss 0.595804.
Train: 2018-08-06T01:21:17.772209: step 8568, loss 0.562428.
Train: 2018-08-06T01:21:18.019537: step 8569, loss 0.49579.
Train: 2018-08-06T01:21:18.266877: step 8570, loss 0.529087.
Test: 2018-08-06T01:21:19.539473: step 8570, loss 0.547523.
Train: 2018-08-06T01:21:19.793824: step 8571, loss 0.595805.
Train: 2018-08-06T01:21:20.042164: step 8572, loss 0.529031.
Train: 2018-08-06T01:21:20.295480: step 8573, loss 0.520644.
Train: 2018-08-06T01:21:20.558779: step 8574, loss 0.587509.
Train: 2018-08-06T01:21:20.810104: step 8575, loss 0.629391.
Train: 2018-08-06T01:21:21.056447: step 8576, loss 0.537314.
Train: 2018-08-06T01:21:21.314725: step 8577, loss 0.570772.
Train: 2018-08-06T01:21:21.561098: step 8578, loss 0.554062.
Train: 2018-08-06T01:21:21.814390: step 8579, loss 0.495629.
Train: 2018-08-06T01:21:22.075690: step 8580, loss 0.579126.
Test: 2018-08-06T01:21:23.332329: step 8580, loss 0.548647.
Train: 2018-08-06T01:21:23.573685: step 8581, loss 0.520623.
Train: 2018-08-06T01:21:23.836007: step 8582, loss 0.595875.
Train: 2018-08-06T01:21:24.086347: step 8583, loss 0.595876.
Train: 2018-08-06T01:21:24.328666: step 8584, loss 0.604212.
Train: 2018-08-06T01:21:24.587007: step 8585, loss 0.587456.
Train: 2018-08-06T01:21:24.838303: step 8586, loss 0.554114.
Train: 2018-08-06T01:21:25.099603: step 8587, loss 0.57076.
Train: 2018-08-06T01:21:25.343950: step 8588, loss 0.512734.
Train: 2018-08-06T01:21:25.597301: step 8589, loss 0.603884.
Train: 2018-08-06T01:21:25.853587: step 8590, loss 0.53769.
Test: 2018-08-06T01:21:27.119202: step 8590, loss 0.54985.
Train: 2018-08-06T01:21:27.388482: step 8591, loss 0.512958.
Train: 2018-08-06T01:21:27.633856: step 8592, loss 0.587271.
Train: 2018-08-06T01:21:27.883190: step 8593, loss 0.562504.
Train: 2018-08-06T01:21:28.136506: step 8594, loss 0.529518.
Train: 2018-08-06T01:21:28.382854: step 8595, loss 0.52951.
Train: 2018-08-06T01:21:28.637176: step 8596, loss 0.545986.
Train: 2018-08-06T01:21:28.883514: step 8597, loss 0.579022.
Train: 2018-08-06T01:21:29.139827: step 8598, loss 0.554215.
Train: 2018-08-06T01:21:29.386164: step 8599, loss 0.628687.
Train: 2018-08-06T01:21:29.640485: step 8600, loss 0.529423.
Test: 2018-08-06T01:21:30.908069: step 8600, loss 0.547358.
Train: 2018-08-06T01:21:31.813224: step 8601, loss 0.595549.
Train: 2018-08-06T01:21:32.064523: step 8602, loss 0.521224.
Train: 2018-08-06T01:21:32.324827: step 8603, loss 0.504719.
Train: 2018-08-06T01:21:32.574158: step 8604, loss 0.64514.
Train: 2018-08-06T01:21:32.835494: step 8605, loss 0.521212.
Train: 2018-08-06T01:21:33.085862: step 8606, loss 0.537724.
Train: 2018-08-06T01:21:33.278322: step 8607, loss 0.474355.
Train: 2018-08-06T01:21:33.527674: step 8608, loss 0.545905.
Train: 2018-08-06T01:21:33.774023: step 8609, loss 0.579065.
Train: 2018-08-06T01:21:34.020344: step 8610, loss 0.570762.
Test: 2018-08-06T01:21:35.279962: step 8610, loss 0.548325.
Train: 2018-08-06T01:21:35.520321: step 8611, loss 0.570764.
Train: 2018-08-06T01:21:35.778659: step 8612, loss 0.504078.
Train: 2018-08-06T01:21:36.040953: step 8613, loss 0.612556.
Train: 2018-08-06T01:21:36.290302: step 8614, loss 0.51224.
Train: 2018-08-06T01:21:36.534640: step 8615, loss 0.545653.
Train: 2018-08-06T01:21:36.780948: step 8616, loss 0.570782.
Train: 2018-08-06T01:21:37.042250: step 8617, loss 0.579183.
Train: 2018-08-06T01:21:37.304578: step 8618, loss 0.58759.
Train: 2018-08-06T01:21:37.554879: step 8619, loss 0.604381.
Train: 2018-08-06T01:21:37.800253: step 8620, loss 0.604329.
Test: 2018-08-06T01:21:39.062845: step 8620, loss 0.547085.
Train: 2018-08-06T01:21:39.306231: step 8621, loss 0.562407.
Train: 2018-08-06T01:21:39.550566: step 8622, loss 0.595812.
Train: 2018-08-06T01:21:39.795885: step 8623, loss 0.53747.
Train: 2018-08-06T01:21:40.054220: step 8624, loss 0.562454.
Train: 2018-08-06T01:21:40.299569: step 8625, loss 0.570757.
Train: 2018-08-06T01:21:40.551893: step 8626, loss 0.595565.
Train: 2018-08-06T01:21:40.799202: step 8627, loss 0.554264.
Train: 2018-08-06T01:21:41.048566: step 8628, loss 0.570758.
Train: 2018-08-06T01:21:41.295905: step 8629, loss 0.488694.
Train: 2018-08-06T01:21:41.542246: step 8630, loss 0.554355.
Test: 2018-08-06T01:21:42.808827: step 8630, loss 0.54792.
Train: 2018-08-06T01:21:43.045221: step 8631, loss 0.546158.
Train: 2018-08-06T01:21:43.295553: step 8632, loss 0.496937.
Train: 2018-08-06T01:21:43.543891: step 8633, loss 0.603626.
Train: 2018-08-06T01:21:43.794217: step 8634, loss 0.578979.
Train: 2018-08-06T01:21:44.040560: step 8635, loss 0.554317.
Train: 2018-08-06T01:21:44.287873: step 8636, loss 0.611869.
Train: 2018-08-06T01:21:44.550196: step 8637, loss 0.611826.
Train: 2018-08-06T01:21:44.798514: step 8638, loss 0.628129.
Train: 2018-08-06T01:21:45.054851: step 8639, loss 0.538107.
Train: 2018-08-06T01:21:45.304155: step 8640, loss 0.530057.
Test: 2018-08-06T01:21:46.579742: step 8640, loss 0.549259.
Train: 2018-08-06T01:21:46.820126: step 8641, loss 0.505734.
Train: 2018-08-06T01:21:47.068460: step 8642, loss 0.554521.
Train: 2018-08-06T01:21:47.314808: step 8643, loss 0.554517.
Train: 2018-08-06T01:21:47.569129: step 8644, loss 0.530103.
Train: 2018-08-06T01:21:47.816469: step 8645, loss 0.578922.
Train: 2018-08-06T01:21:48.062817: step 8646, loss 0.570775.
Train: 2018-08-06T01:21:48.311154: step 8647, loss 0.578928.
Train: 2018-08-06T01:21:48.559483: step 8648, loss 0.554469.
Train: 2018-08-06T01:21:48.816770: step 8649, loss 0.497386.
Train: 2018-08-06T01:21:49.075135: step 8650, loss 0.61978.
Test: 2018-08-06T01:21:50.340694: step 8650, loss 0.548391.
Train: 2018-08-06T01:21:50.586069: step 8651, loss 0.578939.
Train: 2018-08-06T01:21:50.833402: step 8652, loss 0.595272.
Train: 2018-08-06T01:21:51.082743: step 8653, loss 0.521819.
Train: 2018-08-06T01:21:51.332076: step 8654, loss 0.538135.
Train: 2018-08-06T01:21:51.580404: step 8655, loss 0.619758.
Train: 2018-08-06T01:21:51.825772: step 8656, loss 0.538142.
Train: 2018-08-06T01:21:52.085030: step 8657, loss 0.570773.
Train: 2018-08-06T01:21:52.330373: step 8658, loss 0.513698.
Train: 2018-08-06T01:21:52.581732: step 8659, loss 0.587094.
Train: 2018-08-06T01:21:52.833059: step 8660, loss 0.587097.
Test: 2018-08-06T01:21:54.094654: step 8660, loss 0.548611.
Train: 2018-08-06T01:21:54.332048: step 8661, loss 0.513657.
Train: 2018-08-06T01:21:54.587382: step 8662, loss 0.472779.
Train: 2018-08-06T01:21:54.835704: step 8663, loss 0.505247.
Train: 2018-08-06T01:21:55.094980: step 8664, loss 0.620092.
Train: 2018-08-06T01:21:55.345310: step 8665, loss 0.570757.
Train: 2018-08-06T01:21:55.592673: step 8666, loss 0.529489.
Train: 2018-08-06T01:21:55.843004: step 8667, loss 0.529398.
Train: 2018-08-06T01:21:56.104312: step 8668, loss 0.504406.
Train: 2018-08-06T01:21:56.354611: step 8669, loss 0.545789.
Train: 2018-08-06T01:21:56.620923: step 8670, loss 0.520642.
Test: 2018-08-06T01:21:57.898482: step 8670, loss 0.54819.
Train: 2018-08-06T01:21:58.137880: step 8671, loss 0.629502.
Train: 2018-08-06T01:21:58.394156: step 8672, loss 0.629623.
Train: 2018-08-06T01:21:58.644488: step 8673, loss 0.654842.
Train: 2018-08-06T01:21:58.901799: step 8674, loss 0.570781.
Train: 2018-08-06T01:21:59.151133: step 8675, loss 0.537306.
Train: 2018-08-06T01:21:59.399499: step 8676, loss 0.554064.
Train: 2018-08-06T01:21:59.659771: step 8677, loss 0.554085.
Train: 2018-08-06T01:21:59.910102: step 8678, loss 0.554103.
Train: 2018-08-06T01:22:00.158464: step 8679, loss 0.520831.
Train: 2018-08-06T01:22:00.421762: step 8680, loss 0.570762.
Test: 2018-08-06T01:22:01.696325: step 8680, loss 0.549125.
Train: 2018-08-06T01:22:01.932725: step 8681, loss 0.545806.
Train: 2018-08-06T01:22:02.182027: step 8682, loss 0.579079.
Train: 2018-08-06T01:22:02.435380: step 8683, loss 0.579074.
Train: 2018-08-06T01:22:02.694655: step 8684, loss 0.520926.
Train: 2018-08-06T01:22:02.939001: step 8685, loss 0.612288.
Train: 2018-08-06T01:22:03.185343: step 8686, loss 0.512692.
Train: 2018-08-06T01:22:03.433705: step 8687, loss 0.603937.
Train: 2018-08-06T01:22:03.683011: step 8688, loss 0.52933.
Train: 2018-08-06T01:22:03.937363: step 8689, loss 0.504495.
Train: 2018-08-06T01:22:04.198639: step 8690, loss 0.579049.
Test: 2018-08-06T01:22:05.456275: step 8690, loss 0.548038.
Train: 2018-08-06T01:22:05.693641: step 8691, loss 0.579053.
Train: 2018-08-06T01:22:05.942975: step 8692, loss 0.512702.
Train: 2018-08-06T01:22:06.192338: step 8693, loss 0.587363.
Train: 2018-08-06T01:22:06.452643: step 8694, loss 0.603974.
Train: 2018-08-06T01:22:06.698953: step 8695, loss 0.545869.
Train: 2018-08-06T01:22:06.947289: step 8696, loss 0.587342.
Train: 2018-08-06T01:22:07.192668: step 8697, loss 0.521059.
Train: 2018-08-06T01:22:07.437999: step 8698, loss 0.612167.
Train: 2018-08-06T01:22:07.690334: step 8699, loss 0.62038.
Train: 2018-08-06T01:22:07.935645: step 8700, loss 0.521265.
Test: 2018-08-06T01:22:09.208241: step 8700, loss 0.549145.
Train: 2018-08-06T01:22:10.082850: step 8701, loss 0.570757.
Train: 2018-08-06T01:22:10.342182: step 8702, loss 0.554312.
Train: 2018-08-06T01:22:10.603472: step 8703, loss 0.513272.
Train: 2018-08-06T01:22:10.846838: step 8704, loss 0.521486.
Train: 2018-08-06T01:22:11.097170: step 8705, loss 0.611859.
Train: 2018-08-06T01:22:11.346505: step 8706, loss 0.636494.
Train: 2018-08-06T01:22:11.593841: step 8707, loss 0.554364.
Train: 2018-08-06T01:22:11.846159: step 8708, loss 0.546212.
Train: 2018-08-06T01:22:12.096466: step 8709, loss 0.619815.
Train: 2018-08-06T01:22:12.343804: step 8710, loss 0.52185.
Test: 2018-08-06T01:22:13.614406: step 8710, loss 0.549221.
Train: 2018-08-06T01:22:13.864750: step 8711, loss 0.546347.
Train: 2018-08-06T01:22:14.113073: step 8712, loss 0.57078.
Train: 2018-08-06T01:22:14.364400: step 8713, loss 0.643958.
Train: 2018-08-06T01:22:14.607749: step 8714, loss 0.546468.
Train: 2018-08-06T01:22:14.855088: step 8715, loss 0.61126.
Train: 2018-08-06T01:22:15.100432: step 8716, loss 0.570815.
Train: 2018-08-06T01:22:15.344804: step 8717, loss 0.611052.
Train: 2018-08-06T01:22:15.606105: step 8718, loss 0.51475.
Train: 2018-08-06T01:22:15.850426: step 8719, loss 0.530875.
Train: 2018-08-06T01:22:16.098763: step 8720, loss 0.570871.
Test: 2018-08-06T01:22:17.371358: step 8720, loss 0.549746.
Train: 2018-08-06T01:22:17.609746: step 8721, loss 0.610793.
Train: 2018-08-06T01:22:17.863068: step 8722, loss 0.562924.
Train: 2018-08-06T01:22:18.122381: step 8723, loss 0.523175.
Train: 2018-08-06T01:22:18.368718: step 8724, loss 0.578858.
Train: 2018-08-06T01:22:18.619055: step 8725, loss 0.555015.
Train: 2018-08-06T01:22:18.866362: step 8726, loss 0.586805.
Train: 2018-08-06T01:22:19.118718: step 8727, loss 0.483556.
Train: 2018-08-06T01:22:19.364030: step 8728, loss 0.554994.
Train: 2018-08-06T01:22:19.620345: step 8729, loss 0.586828.
Train: 2018-08-06T01:22:19.877657: step 8730, loss 0.53099.
Test: 2018-08-06T01:22:21.158231: step 8730, loss 0.55081.
Train: 2018-08-06T01:22:21.395597: step 8731, loss 0.58686.
Train: 2018-08-06T01:22:21.642936: step 8732, loss 0.506825.
Train: 2018-08-06T01:22:21.891297: step 8733, loss 0.594918.
Train: 2018-08-06T01:22:22.139606: step 8734, loss 0.602989.
Train: 2018-08-06T01:22:22.384978: step 8735, loss 0.530612.
Train: 2018-08-06T01:22:22.630295: step 8736, loss 0.554711.
Train: 2018-08-06T01:22:22.890605: step 8737, loss 0.5708.
Train: 2018-08-06T01:22:23.139962: step 8738, loss 0.562723.
Train: 2018-08-06T01:22:23.398265: step 8739, loss 0.570782.
Train: 2018-08-06T01:22:23.646602: step 8740, loss 0.595165.
Test: 2018-08-06T01:22:24.921167: step 8740, loss 0.55009.
Train: 2018-08-06T01:22:25.160559: step 8741, loss 0.522073.
Train: 2018-08-06T01:22:25.407867: step 8742, loss 0.53014.
Train: 2018-08-06T01:22:25.655205: step 8743, loss 0.58698.
Train: 2018-08-06T01:22:25.901571: step 8744, loss 0.546116.
Train: 2018-08-06T01:22:26.146922: step 8745, loss 0.595621.
Train: 2018-08-06T01:22:26.390239: step 8746, loss 0.538456.
Train: 2018-08-06T01:22:26.639603: step 8747, loss 0.555231.
Train: 2018-08-06T01:22:26.885913: step 8748, loss 0.521113.
Train: 2018-08-06T01:22:27.134276: step 8749, loss 0.546266.
Train: 2018-08-06T01:22:27.378595: step 8750, loss 0.579235.
Test: 2018-08-06T01:22:28.658174: step 8750, loss 0.549378.
Train: 2018-08-06T01:22:28.897559: step 8751, loss 0.537958.
Train: 2018-08-06T01:22:29.144885: step 8752, loss 0.546216.
Train: 2018-08-06T01:22:29.401218: step 8753, loss 0.529638.
Train: 2018-08-06T01:22:29.656530: step 8754, loss 0.587222.
Train: 2018-08-06T01:22:29.901878: step 8755, loss 0.52949.
Train: 2018-08-06T01:22:30.150214: step 8756, loss 0.603831.
Train: 2018-08-06T01:22:30.400545: step 8757, loss 0.488007.
Train: 2018-08-06T01:22:30.611949: step 8758, loss 0.686337.
Train: 2018-08-06T01:22:30.870258: step 8759, loss 0.545892.
Train: 2018-08-06T01:22:31.124578: step 8760, loss 0.554189.
Test: 2018-08-06T01:22:32.403158: step 8760, loss 0.548832.
Train: 2018-08-06T01:22:32.660497: step 8761, loss 0.587322.
Train: 2018-08-06T01:22:32.907834: step 8762, loss 0.545932.
Train: 2018-08-06T01:22:33.157143: step 8763, loss 0.545943.
Train: 2018-08-06T01:22:33.415452: step 8764, loss 0.52114.
Train: 2018-08-06T01:22:33.665812: step 8765, loss 0.471439.
Train: 2018-08-06T01:22:33.910159: step 8766, loss 0.620552.
Train: 2018-08-06T01:22:34.158464: step 8767, loss 0.579073.
Train: 2018-08-06T01:22:34.412783: step 8768, loss 0.562454.
Train: 2018-08-06T01:22:34.660123: step 8769, loss 0.520878.
Train: 2018-08-06T01:22:34.906495: step 8770, loss 0.579093.
Test: 2018-08-06T01:22:36.179117: step 8770, loss 0.548721.
Train: 2018-08-06T01:22:36.417505: step 8771, loss 0.637376.
Train: 2018-08-06T01:22:36.664844: step 8772, loss 0.545815.
Train: 2018-08-06T01:22:36.916171: step 8773, loss 0.537529.
Train: 2018-08-06T01:22:37.162512: step 8774, loss 0.504332.
Train: 2018-08-06T01:22:37.409857: step 8775, loss 0.620621.
Train: 2018-08-06T01:22:37.656166: step 8776, loss 0.562454.
Train: 2018-08-06T01:22:37.902539: step 8777, loss 0.570757.
Train: 2018-08-06T01:22:38.151866: step 8778, loss 0.537589.
Train: 2018-08-06T01:22:38.404198: step 8779, loss 0.545889.
Train: 2018-08-06T01:22:38.651530: step 8780, loss 0.587337.
Test: 2018-08-06T01:22:39.916123: step 8780, loss 0.547494.
Train: 2018-08-06T01:22:40.168448: step 8781, loss 0.562473.
Train: 2018-08-06T01:22:40.422768: step 8782, loss 0.595592.
Train: 2018-08-06T01:22:40.671104: step 8783, loss 0.620354.
Train: 2018-08-06T01:22:40.916476: step 8784, loss 0.546026.
Train: 2018-08-06T01:22:41.164809: step 8785, loss 0.52963.
Train: 2018-08-06T01:22:41.417110: step 8786, loss 0.587191.
Train: 2018-08-06T01:22:41.678435: step 8787, loss 0.50515.
Train: 2018-08-06T01:22:41.926746: step 8788, loss 0.578961.
Train: 2018-08-06T01:22:42.174124: step 8789, loss 0.546177.
Train: 2018-08-06T01:22:42.421423: step 8790, loss 0.505214.
Test: 2018-08-06T01:22:43.695017: step 8790, loss 0.549801.
Train: 2018-08-06T01:22:43.932407: step 8791, loss 0.578966.
Train: 2018-08-06T01:22:44.178724: step 8792, loss 0.578969.
Train: 2018-08-06T01:22:44.440026: step 8793, loss 0.554346.
Train: 2018-08-06T01:22:44.696363: step 8794, loss 0.521482.
Train: 2018-08-06T01:22:44.944700: step 8795, loss 0.587195.
Train: 2018-08-06T01:22:45.193035: step 8796, loss 0.587211.
Train: 2018-08-06T01:22:45.437385: step 8797, loss 0.578982.
Train: 2018-08-06T01:22:45.686691: step 8798, loss 0.644745.
Train: 2018-08-06T01:22:45.934033: step 8799, loss 0.546171.
Train: 2018-08-06T01:22:46.181394: step 8800, loss 0.619855.
Test: 2018-08-06T01:22:47.446982: step 8800, loss 0.549937.
Train: 2018-08-06T01:22:48.343526: step 8801, loss 0.562619.
Train: 2018-08-06T01:22:48.666352: step 8802, loss 0.578914.
Train: 2018-08-06T01:22:48.911695: step 8803, loss 0.546472.
Train: 2018-08-06T01:22:49.157039: step 8804, loss 0.489916.
Train: 2018-08-06T01:22:49.404377: step 8805, loss 0.627414.
Train: 2018-08-06T01:22:49.651718: step 8806, loss 0.570811.
Train: 2018-08-06T01:22:49.897062: step 8807, loss 0.490226.
Train: 2018-08-06T01:22:50.143411: step 8808, loss 0.546632.
Train: 2018-08-06T01:22:50.393772: step 8809, loss 0.554678.
Train: 2018-08-06T01:22:50.652053: step 8810, loss 0.627342.
Test: 2018-08-06T01:22:51.921655: step 8810, loss 0.548336.
Train: 2018-08-06T01:22:52.160019: step 8811, loss 0.578884.
Train: 2018-08-06T01:22:52.404366: step 8812, loss 0.586943.
Train: 2018-08-06T01:22:52.660680: step 8813, loss 0.506418.
Train: 2018-08-06T01:22:52.911036: step 8814, loss 0.603034.
Train: 2018-08-06T01:22:53.158349: step 8815, loss 0.667385.
Train: 2018-08-06T01:22:53.406709: step 8816, loss 0.570845.
Train: 2018-08-06T01:22:53.664024: step 8817, loss 0.530886.
Train: 2018-08-06T01:22:53.914327: step 8818, loss 0.594823.
Train: 2018-08-06T01:22:54.165656: step 8819, loss 0.531082.
Train: 2018-08-06T01:22:54.414987: step 8820, loss 0.578858.
Test: 2018-08-06T01:22:55.676613: step 8820, loss 0.548975.
Train: 2018-08-06T01:22:55.913978: step 8821, loss 0.602688.
Train: 2018-08-06T01:22:56.165333: step 8822, loss 0.570932.
Train: 2018-08-06T01:22:56.420625: step 8823, loss 0.570949.
Train: 2018-08-06T01:22:56.666965: step 8824, loss 0.531477.
Train: 2018-08-06T01:22:56.919312: step 8825, loss 0.51572.
Train: 2018-08-06T01:22:57.163663: step 8826, loss 0.594661.
Train: 2018-08-06T01:22:57.415962: step 8827, loss 0.586762.
Train: 2018-08-06T01:22:57.664298: step 8828, loss 0.578862.
Train: 2018-08-06T01:22:57.913633: step 8829, loss 0.48413.
Train: 2018-08-06T01:22:58.161991: step 8830, loss 0.618402.
Test: 2018-08-06T01:22:59.445536: step 8830, loss 0.550009.
Train: 2018-08-06T01:22:59.684893: step 8831, loss 0.563039.
Train: 2018-08-06T01:22:59.942207: step 8832, loss 0.618432.
Train: 2018-08-06T01:23:00.187575: step 8833, loss 0.48397.
Train: 2018-08-06T01:23:00.435919: step 8834, loss 0.531346.
Train: 2018-08-06T01:23:00.683236: step 8835, loss 0.578859.
Train: 2018-08-06T01:23:00.930564: step 8836, loss 0.59476.
Train: 2018-08-06T01:23:01.179896: step 8837, loss 0.626608.
Train: 2018-08-06T01:23:01.428232: step 8838, loss 0.562953.
Train: 2018-08-06T01:23:01.688536: step 8839, loss 0.539112.
Train: 2018-08-06T01:23:01.946875: step 8840, loss 0.586811.
Test: 2018-08-06T01:23:03.197502: step 8840, loss 0.550594.
Train: 2018-08-06T01:23:03.441847: step 8841, loss 0.483451.
Train: 2018-08-06T01:23:03.688193: step 8842, loss 0.531059.
Train: 2018-08-06T01:23:03.933544: step 8843, loss 0.562884.
Train: 2018-08-06T01:23:04.183864: step 8844, loss 0.562847.
Train: 2018-08-06T01:23:04.441176: step 8845, loss 0.554784.
Train: 2018-08-06T01:23:04.689510: step 8846, loss 0.586923.
Train: 2018-08-06T01:23:04.940863: step 8847, loss 0.595004.
Train: 2018-08-06T01:23:05.185213: step 8848, loss 0.562747.
Train: 2018-08-06T01:23:05.434519: step 8849, loss 0.570813.
Train: 2018-08-06T01:23:05.679862: step 8850, loss 0.619275.
Test: 2018-08-06T01:23:06.947471: step 8850, loss 0.550007.
Train: 2018-08-06T01:23:07.185865: step 8851, loss 0.635378.
Train: 2018-08-06T01:23:07.433172: step 8852, loss 0.554726.
Train: 2018-08-06T01:23:07.686494: step 8853, loss 0.578871.
Train: 2018-08-06T01:23:07.936851: step 8854, loss 0.57085.
Train: 2018-08-06T01:23:08.184190: step 8855, loss 0.506862.
Train: 2018-08-06T01:23:08.432528: step 8856, loss 0.610853.
Train: 2018-08-06T01:23:08.681858: step 8857, loss 0.522957.
Train: 2018-08-06T01:23:08.933163: step 8858, loss 0.506994.
Train: 2018-08-06T01:23:09.194463: step 8859, loss 0.594856.
Train: 2018-08-06T01:23:09.442824: step 8860, loss 0.498854.
Test: 2018-08-06T01:23:10.701432: step 8860, loss 0.548515.
Train: 2018-08-06T01:23:10.940817: step 8861, loss 0.570848.
Train: 2018-08-06T01:23:11.187164: step 8862, loss 0.546736.
Train: 2018-08-06T01:23:11.438485: step 8863, loss 0.506415.
Train: 2018-08-06T01:23:11.688818: step 8864, loss 0.538493.
Train: 2018-08-06T01:23:11.940121: step 8865, loss 0.570793.
Train: 2018-08-06T01:23:12.187457: step 8866, loss 0.595185.
Train: 2018-08-06T01:23:12.434797: step 8867, loss 0.513715.
Train: 2018-08-06T01:23:12.684130: step 8868, loss 0.587119.
Train: 2018-08-06T01:23:12.930495: step 8869, loss 0.554376.
Train: 2018-08-06T01:23:13.178837: step 8870, loss 0.537918.
Test: 2018-08-06T01:23:14.459382: step 8870, loss 0.548024.
Train: 2018-08-06T01:23:14.705725: step 8871, loss 0.620141.
Train: 2018-08-06T01:23:14.954059: step 8872, loss 0.68607.
Train: 2018-08-06T01:23:15.210374: step 8873, loss 0.529672.
Train: 2018-08-06T01:23:15.459707: step 8874, loss 0.537935.
Train: 2018-08-06T01:23:15.709051: step 8875, loss 0.55436.
Train: 2018-08-06T01:23:15.954424: step 8876, loss 0.464194.
Train: 2018-08-06T01:23:16.204755: step 8877, loss 0.603613.
Train: 2018-08-06T01:23:16.464061: step 8878, loss 0.628295.
Train: 2018-08-06T01:23:16.726360: step 8879, loss 0.611818.
Train: 2018-08-06T01:23:16.970707: step 8880, loss 0.497011.
Test: 2018-08-06T01:23:18.251250: step 8880, loss 0.546628.
Train: 2018-08-06T01:23:18.487652: step 8881, loss 0.537996.
Train: 2018-08-06T01:23:18.746926: step 8882, loss 0.54618.
Train: 2018-08-06T01:23:18.992300: step 8883, loss 0.59536.
Train: 2018-08-06T01:23:19.239609: step 8884, loss 0.611748.
Train: 2018-08-06T01:23:19.490967: step 8885, loss 0.56269.
Train: 2018-08-06T01:23:19.741291: step 8886, loss 0.514499.
Train: 2018-08-06T01:23:19.985612: step 8887, loss 0.481416.
Train: 2018-08-06T01:23:20.231955: step 8888, loss 0.529801.
Train: 2018-08-06T01:23:20.482313: step 8889, loss 0.554332.
Train: 2018-08-06T01:23:20.731618: step 8890, loss 0.603696.
Test: 2018-08-06T01:23:22.012193: step 8890, loss 0.548553.
Train: 2018-08-06T01:23:22.259563: step 8891, loss 0.661447.
Train: 2018-08-06T01:23:22.511857: step 8892, loss 0.488417.
Train: 2018-08-06T01:23:22.768202: step 8893, loss 0.578995.
Train: 2018-08-06T01:23:23.013516: step 8894, loss 0.529564.
Train: 2018-08-06T01:23:23.261851: step 8895, loss 0.537776.
Train: 2018-08-06T01:23:23.518166: step 8896, loss 0.537736.
Train: 2018-08-06T01:23:23.774480: step 8897, loss 0.554221.
Train: 2018-08-06T01:23:24.026831: step 8898, loss 0.545913.
Train: 2018-08-06T01:23:24.276172: step 8899, loss 0.603939.
Train: 2018-08-06T01:23:24.529474: step 8900, loss 0.603951.
Test: 2018-08-06T01:23:25.786100: step 8900, loss 0.549184.
Train: 2018-08-06T01:23:26.701447: step 8901, loss 0.496105.
Train: 2018-08-06T01:23:26.954748: step 8902, loss 0.587366.
Train: 2018-08-06T01:23:27.201082: step 8903, loss 0.570752.
Train: 2018-08-06T01:23:27.454405: step 8904, loss 0.570762.
Train: 2018-08-06T01:23:27.700771: step 8905, loss 0.545857.
Train: 2018-08-06T01:23:27.948114: step 8906, loss 0.554157.
Train: 2018-08-06T01:23:28.208440: step 8907, loss 0.487769.
Train: 2018-08-06T01:23:28.457753: step 8908, loss 0.512523.
Train: 2018-08-06T01:23:28.658224: step 8909, loss 0.420159.
Train: 2018-08-06T01:23:28.907551: step 8910, loss 0.596009.
Test: 2018-08-06T01:23:30.160176: step 8910, loss 0.547514.
Train: 2018-08-06T01:23:30.515764: step 8911, loss 0.571002.
Train: 2018-08-06T01:23:30.766094: step 8912, loss 0.646851.
Train: 2018-08-06T01:23:31.014432: step 8913, loss 0.54538.
Train: 2018-08-06T01:23:31.260746: step 8914, loss 0.613085.
Train: 2018-08-06T01:23:31.509112: step 8915, loss 0.638467.
Train: 2018-08-06T01:23:31.757457: step 8916, loss 0.520219.
Train: 2018-08-06T01:23:32.005779: step 8917, loss 0.545555.
Train: 2018-08-06T01:23:32.253092: step 8918, loss 0.495142.
Train: 2018-08-06T01:23:32.503423: step 8919, loss 0.553973.
Train: 2018-08-06T01:23:32.755778: step 8920, loss 0.570792.
Test: 2018-08-06T01:23:34.015378: step 8920, loss 0.549103.
Train: 2018-08-06T01:23:34.254754: step 8921, loss 0.570794.
Train: 2018-08-06T01:23:34.500113: step 8922, loss 0.596021.
Train: 2018-08-06T01:23:34.750447: step 8923, loss 0.537172.
Train: 2018-08-06T01:23:34.999779: step 8924, loss 0.629554.
Train: 2018-08-06T01:23:35.248082: step 8925, loss 0.604275.
Train: 2018-08-06T01:23:35.496443: step 8926, loss 0.579129.
Train: 2018-08-06T01:23:35.751736: step 8927, loss 0.57076.
Train: 2018-08-06T01:23:36.013036: step 8928, loss 0.570744.
Train: 2018-08-06T01:23:36.273339: step 8929, loss 0.512881.
Train: 2018-08-06T01:23:36.521677: step 8930, loss 0.504735.
Test: 2018-08-06T01:23:37.787291: step 8930, loss 0.548918.
Train: 2018-08-06T01:23:38.025696: step 8931, loss 0.628496.
Train: 2018-08-06T01:23:38.270997: step 8932, loss 0.562526.
Train: 2018-08-06T01:23:38.518362: step 8933, loss 0.562547.
Train: 2018-08-06T01:23:38.766702: step 8934, loss 0.554344.
Train: 2018-08-06T01:23:39.013014: step 8935, loss 0.50522.
Train: 2018-08-06T01:23:39.257359: step 8936, loss 0.521593.
Train: 2018-08-06T01:23:39.506694: step 8937, loss 0.644579.
Train: 2018-08-06T01:23:39.759018: step 8938, loss 0.480629.
Train: 2018-08-06T01:23:40.007355: step 8939, loss 0.554356.
Train: 2018-08-06T01:23:40.254693: step 8940, loss 0.513282.
Test: 2018-08-06T01:23:41.521305: step 8940, loss 0.54822.
Train: 2018-08-06T01:23:41.772634: step 8941, loss 0.48025.
Train: 2018-08-06T01:23:42.021002: step 8942, loss 0.52947.
Train: 2018-08-06T01:23:42.270303: step 8943, loss 0.537588.
Train: 2018-08-06T01:23:42.520666: step 8944, loss 0.645759.
Train: 2018-08-06T01:23:42.768999: step 8945, loss 0.612475.
Train: 2018-08-06T01:23:43.024286: step 8946, loss 0.645836.
Train: 2018-08-06T01:23:43.266645: step 8947, loss 0.545782.
Train: 2018-08-06T01:23:43.517967: step 8948, loss 0.504235.
Train: 2018-08-06T01:23:43.762344: step 8949, loss 0.53749.
Train: 2018-08-06T01:23:44.011646: step 8950, loss 0.545774.
Test: 2018-08-06T01:23:45.284242: step 8950, loss 0.548335.
Train: 2018-08-06T01:23:45.522630: step 8951, loss 0.512476.
Train: 2018-08-06T01:23:45.781937: step 8952, loss 0.58746.
Train: 2018-08-06T01:23:46.032292: step 8953, loss 0.579088.
Train: 2018-08-06T01:23:46.285590: step 8954, loss 0.595891.
Train: 2018-08-06T01:23:46.548860: step 8955, loss 0.554034.
Train: 2018-08-06T01:23:46.796224: step 8956, loss 0.504034.
Train: 2018-08-06T01:23:47.048524: step 8957, loss 0.487281.
Train: 2018-08-06T01:23:47.301864: step 8958, loss 0.545667.
Train: 2018-08-06T01:23:47.554172: step 8959, loss 0.512062.
Train: 2018-08-06T01:23:47.801535: step 8960, loss 0.537133.
Test: 2018-08-06T01:23:49.057152: step 8960, loss 0.54789.
Train: 2018-08-06T01:23:49.296511: step 8961, loss 0.570803.
Train: 2018-08-06T01:23:49.542853: step 8962, loss 0.520055.
Train: 2018-08-06T01:23:49.790192: step 8963, loss 0.503001.
Train: 2018-08-06T01:23:50.036553: step 8964, loss 0.545302.
Train: 2018-08-06T01:23:50.282895: step 8965, loss 0.519607.
Train: 2018-08-06T01:23:50.530212: step 8966, loss 0.536614.
Train: 2018-08-06T01:23:50.777550: step 8967, loss 0.613975.
Train: 2018-08-06T01:23:51.033866: step 8968, loss 0.54509.
Train: 2018-08-06T01:23:51.281233: step 8969, loss 0.57098.
Train: 2018-08-06T01:23:51.527546: step 8970, loss 0.605576.
Test: 2018-08-06T01:23:52.809118: step 8970, loss 0.546508.
Train: 2018-08-06T01:23:53.049476: step 8971, loss 0.562343.
Train: 2018-08-06T01:23:53.297811: step 8972, loss 0.536412.
Train: 2018-08-06T01:23:53.547173: step 8973, loss 0.614183.
Train: 2018-08-06T01:23:53.796504: step 8974, loss 0.562338.
Train: 2018-08-06T01:23:54.045842: step 8975, loss 0.562336.
Train: 2018-08-06T01:23:54.292209: step 8976, loss 0.484949.
Train: 2018-08-06T01:23:54.539516: step 8977, loss 0.545131.
Train: 2018-08-06T01:23:54.786855: step 8978, loss 0.545153.
Train: 2018-08-06T01:23:55.048157: step 8979, loss 0.519347.
Train: 2018-08-06T01:23:55.307488: step 8980, loss 0.545165.
Test: 2018-08-06T01:23:56.566097: step 8980, loss 0.548147.
Train: 2018-08-06T01:23:56.818423: step 8981, loss 0.648468.
Train: 2018-08-06T01:23:57.076732: step 8982, loss 0.605282.
Train: 2018-08-06T01:23:57.321109: step 8983, loss 0.639495.
Train: 2018-08-06T01:23:57.567419: step 8984, loss 0.570894.
Train: 2018-08-06T01:23:57.814789: step 8985, loss 0.519828.
Train: 2018-08-06T01:23:58.067083: step 8986, loss 0.570829.
Train: 2018-08-06T01:23:58.319436: step 8987, loss 0.562362.
Train: 2018-08-06T01:23:58.567744: step 8988, loss 0.579221.
Train: 2018-08-06T01:23:58.828078: step 8989, loss 0.47843.
Train: 2018-08-06T01:23:59.078379: step 8990, loss 0.554009.
Test: 2018-08-06T01:24:00.351972: step 8990, loss 0.547833.
Train: 2018-08-06T01:24:00.603301: step 8991, loss 0.604284.
Train: 2018-08-06T01:24:00.849673: step 8992, loss 0.545693.
Train: 2018-08-06T01:24:01.099981: step 8993, loss 0.629192.
Train: 2018-08-06T01:24:01.347341: step 8994, loss 0.579082.
Train: 2018-08-06T01:24:01.598639: step 8995, loss 0.562465.
Train: 2018-08-06T01:24:01.844013: step 8996, loss 0.579025.
Train: 2018-08-06T01:24:02.091320: step 8997, loss 0.537789.
Train: 2018-08-06T01:24:02.340655: step 8998, loss 0.529646.
Train: 2018-08-06T01:24:02.591017: step 8999, loss 0.636448.
Train: 2018-08-06T01:24:02.838351: step 9000, loss 0.505277.
Test: 2018-08-06T01:24:04.107927: step 9000, loss 0.549882.
Train: 2018-08-06T01:24:05.065884: step 9001, loss 0.505372.
Train: 2018-08-06T01:24:05.313253: step 9002, loss 0.521717.
Train: 2018-08-06T01:24:05.559590: step 9003, loss 0.529849.
Train: 2018-08-06T01:24:05.819868: step 9004, loss 0.529778.
Train: 2018-08-06T01:24:06.077213: step 9005, loss 0.578975.
Train: 2018-08-06T01:24:06.327512: step 9006, loss 0.578986.
Train: 2018-08-06T01:24:06.575877: step 9007, loss 0.537811.
Train: 2018-08-06T01:24:06.834156: step 9008, loss 0.579005.
Train: 2018-08-06T01:24:07.084486: step 9009, loss 0.587267.
Train: 2018-08-06T01:24:07.333850: step 9010, loss 0.579012.
Test: 2018-08-06T01:24:08.600432: step 9010, loss 0.548909.
Train: 2018-08-06T01:24:08.839793: step 9011, loss 0.512983.
Train: 2018-08-06T01:24:09.088152: step 9012, loss 0.496418.
Train: 2018-08-06T01:24:09.341476: step 9013, loss 0.645256.
Train: 2018-08-06T01:24:09.588813: step 9014, loss 0.487981.
Train: 2018-08-06T01:24:09.846101: step 9015, loss 0.645376.
Train: 2018-08-06T01:24:10.099424: step 9016, loss 0.562468.
Train: 2018-08-06T01:24:10.347759: step 9017, loss 0.570752.
Train: 2018-08-06T01:24:10.597092: step 9018, loss 0.570761.
Train: 2018-08-06T01:24:10.846426: step 9019, loss 0.537691.
Train: 2018-08-06T01:24:11.096787: step 9020, loss 0.58727.
Test: 2018-08-06T01:24:12.371347: step 9020, loss 0.549471.
Train: 2018-08-06T01:24:12.612701: step 9021, loss 0.496455.
Train: 2018-08-06T01:24:12.872009: step 9022, loss 0.595514.
Train: 2018-08-06T01:24:13.122372: step 9023, loss 0.636835.
Train: 2018-08-06T01:24:13.370674: step 9024, loss 0.51305.
Train: 2018-08-06T01:24:13.622003: step 9025, loss 0.578984.
Train: 2018-08-06T01:24:13.869341: step 9026, loss 0.570815.
Train: 2018-08-06T01:24:14.122664: step 9027, loss 0.579015.
Train: 2018-08-06T01:24:14.369004: step 9028, loss 0.595381.
Train: 2018-08-06T01:24:14.616369: step 9029, loss 0.6035.
Train: 2018-08-06T01:24:14.863693: step 9030, loss 0.529983.
Test: 2018-08-06T01:24:16.122316: step 9030, loss 0.5481.
Train: 2018-08-06T01:24:16.374666: step 9031, loss 0.578921.
Train: 2018-08-06T01:24:16.619985: step 9032, loss 0.570785.
Train: 2018-08-06T01:24:16.867352: step 9033, loss 0.587017.
Train: 2018-08-06T01:24:17.111669: step 9034, loss 0.546543.
Train: 2018-08-06T01:24:17.365998: step 9035, loss 0.578894.
Train: 2018-08-06T01:24:17.615355: step 9036, loss 0.506339.
Train: 2018-08-06T01:24:17.867648: step 9037, loss 0.52248.
Train: 2018-08-06T01:24:18.113991: step 9038, loss 0.514369.
Train: 2018-08-06T01:24:18.371302: step 9039, loss 0.506178.
Train: 2018-08-06T01:24:18.621663: step 9040, loss 0.522173.
Test: 2018-08-06T01:24:19.877273: step 9040, loss 0.549244.
Train: 2018-08-06T01:24:20.116633: step 9041, loss 0.570788.
Train: 2018-08-06T01:24:20.376963: step 9042, loss 0.538126.
Train: 2018-08-06T01:24:20.622297: step 9043, loss 0.570759.
Train: 2018-08-06T01:24:20.873625: step 9044, loss 0.595409.
Train: 2018-08-06T01:24:21.119952: step 9045, loss 0.595448.
Train: 2018-08-06T01:24:21.367290: step 9046, loss 0.546053.
Train: 2018-08-06T01:24:21.614627: step 9047, loss 0.611971.
Train: 2018-08-06T01:24:21.868976: step 9048, loss 0.562516.
Train: 2018-08-06T01:24:22.114291: step 9049, loss 0.603711.
Train: 2018-08-06T01:24:22.359660: step 9050, loss 0.562531.
Test: 2018-08-06T01:24:23.619266: step 9050, loss 0.548814.
Train: 2018-08-06T01:24:23.867604: step 9051, loss 0.669382.
Train: 2018-08-06T01:24:24.113975: step 9052, loss 0.578946.
Train: 2018-08-06T01:24:24.362280: step 9053, loss 0.57077.
Train: 2018-08-06T01:24:24.620619: step 9054, loss 0.505723.
Train: 2018-08-06T01:24:24.871917: step 9055, loss 0.587005.
Train: 2018-08-06T01:24:25.132251: step 9056, loss 0.570797.
Train: 2018-08-06T01:24:25.381553: step 9057, loss 0.55466.
Train: 2018-08-06T01:24:25.633906: step 9058, loss 0.643614.
Train: 2018-08-06T01:24:25.882241: step 9059, loss 0.562804.
Train: 2018-08-06T01:24:26.076695: step 9060, loss 0.562838.
Test: 2018-08-06T01:24:27.353280: step 9060, loss 0.550076.
Train: 2018-08-06T01:24:27.594666: step 9061, loss 0.602843.
Train: 2018-08-06T01:24:27.852974: step 9062, loss 0.523084.
Train: 2018-08-06T01:24:28.110256: step 9063, loss 0.539087.
Train: 2018-08-06T01:24:28.356598: step 9064, loss 0.610657.
Train: 2018-08-06T01:24:28.614934: step 9065, loss 0.53919.
Train: 2018-08-06T01:24:28.862246: step 9066, loss 0.594704.
Train: 2018-08-06T01:24:29.124545: step 9067, loss 0.594654.
Train: 2018-08-06T01:24:29.371901: step 9068, loss 0.507754.
Train: 2018-08-06T01:24:29.620218: step 9069, loss 0.56306.
Train: 2018-08-06T01:24:29.867557: step 9070, loss 0.547286.
Test: 2018-08-06T01:24:31.129182: step 9070, loss 0.549127.
Train: 2018-08-06T01:24:31.367573: step 9071, loss 0.657847.
Train: 2018-08-06T01:24:31.611920: step 9072, loss 0.531567.
Train: 2018-08-06T01:24:31.861251: step 9073, loss 0.476394.
Train: 2018-08-06T01:24:32.113549: step 9074, loss 0.586744.
Train: 2018-08-06T01:24:32.360916: step 9075, loss 0.555137.
Train: 2018-08-06T01:24:32.610248: step 9076, loss 0.539265.
Train: 2018-08-06T01:24:32.858558: step 9077, loss 0.658261.
Train: 2018-08-06T01:24:33.108925: step 9078, loss 0.475766.
Train: 2018-08-06T01:24:33.358247: step 9079, loss 0.547087.
Train: 2018-08-06T01:24:33.619548: step 9080, loss 0.562933.
Test: 2018-08-06T01:24:34.897105: step 9080, loss 0.551409.
Train: 2018-08-06T01:24:35.138460: step 9081, loss 0.594819.
Train: 2018-08-06T01:24:35.386795: step 9082, loss 0.562881.
Train: 2018-08-06T01:24:35.650092: step 9083, loss 0.490858.
Train: 2018-08-06T01:24:35.896434: step 9084, loss 0.594918.
Train: 2018-08-06T01:24:36.154776: step 9085, loss 0.538668.
Train: 2018-08-06T01:24:36.398092: step 9086, loss 0.530512.
Train: 2018-08-06T01:24:36.646440: step 9087, loss 0.5142.
Train: 2018-08-06T01:24:36.895785: step 9088, loss 0.546411.
Train: 2018-08-06T01:24:37.143126: step 9089, loss 0.587061.
Train: 2018-08-06T01:24:37.396449: step 9090, loss 0.505281.
Test: 2018-08-06T01:24:38.677994: step 9090, loss 0.549558.
Train: 2018-08-06T01:24:38.915361: step 9091, loss 0.595445.
Train: 2018-08-06T01:24:39.167684: step 9092, loss 0.644826.
Train: 2018-08-06T01:24:39.412057: step 9093, loss 0.595427.
Train: 2018-08-06T01:24:39.661365: step 9094, loss 0.496621.
Train: 2018-08-06T01:24:39.910697: step 9095, loss 0.645159.
Train: 2018-08-06T01:24:40.156041: step 9096, loss 0.628675.
Train: 2018-08-06T01:24:40.402382: step 9097, loss 0.587182.
Train: 2018-08-06T01:24:40.650731: step 9098, loss 0.554397.
Train: 2018-08-06T01:24:40.897060: step 9099, loss 0.554434.
Train: 2018-08-06T01:24:41.142434: step 9100, loss 0.570775.
Test: 2018-08-06T01:24:42.421981: step 9100, loss 0.549241.
Train: 2018-08-06T01:24:43.367666: step 9101, loss 0.521959.
Train: 2018-08-06T01:24:43.615032: step 9102, loss 0.530116.
Train: 2018-08-06T01:24:43.858353: step 9103, loss 0.587052.
Train: 2018-08-06T01:24:44.115688: step 9104, loss 0.546393.
Train: 2018-08-06T01:24:44.364001: step 9105, loss 0.627664.
Train: 2018-08-06T01:24:44.612338: step 9106, loss 0.522035.
Train: 2018-08-06T01:24:44.859700: step 9107, loss 0.627594.
Train: 2018-08-06T01:24:45.104050: step 9108, loss 0.6274.
Train: 2018-08-06T01:24:45.351390: step 9109, loss 0.514228.
Train: 2018-08-06T01:24:45.597732: step 9110, loss 0.522399.
Test: 2018-08-06T01:24:46.870297: step 9110, loss 0.550036.
Train: 2018-08-06T01:24:47.106666: step 9111, loss 0.619081.
Train: 2018-08-06T01:24:47.357030: step 9112, loss 0.562769.
Train: 2018-08-06T01:24:47.613311: step 9113, loss 0.538678.
Train: 2018-08-06T01:24:47.861680: step 9114, loss 0.522524.
Train: 2018-08-06T01:24:48.109983: step 9115, loss 0.562828.
Train: 2018-08-06T01:24:48.376270: step 9116, loss 0.4744.
Train: 2018-08-06T01:24:48.680873: step 9117, loss 0.586891.
Train: 2018-08-06T01:24:48.942142: step 9118, loss 0.5547.
Train: 2018-08-06T01:24:49.188484: step 9119, loss 0.52231.
Train: 2018-08-06T01:24:49.446792: step 9120, loss 0.546377.
Test: 2018-08-06T01:24:50.741329: step 9120, loss 0.551121.
Train: 2018-08-06T01:24:50.991685: step 9121, loss 0.611345.
Train: 2018-08-06T01:24:51.239996: step 9122, loss 0.603414.
Train: 2018-08-06T01:24:51.492321: step 9123, loss 0.54639.
Train: 2018-08-06T01:24:51.742651: step 9124, loss 0.63593.
Train: 2018-08-06T01:24:52.004975: step 9125, loss 0.53009.
Train: 2018-08-06T01:24:52.255282: step 9126, loss 0.562649.
Train: 2018-08-06T01:24:52.503617: step 9127, loss 0.627695.
Train: 2018-08-06T01:24:52.764917: step 9128, loss 0.538334.
Train: 2018-08-06T01:24:53.009290: step 9129, loss 0.562674.
Train: 2018-08-06T01:24:53.259595: step 9130, loss 0.570788.
Test: 2018-08-06T01:24:54.536180: step 9130, loss 0.550123.
Train: 2018-08-06T01:24:54.774572: step 9131, loss 0.497964.
Train: 2018-08-06T01:24:55.025872: step 9132, loss 0.603192.
Train: 2018-08-06T01:24:55.274208: step 9133, loss 0.586992.
Train: 2018-08-06T01:24:55.525534: step 9134, loss 0.48989.
Train: 2018-08-06T01:24:55.774877: step 9135, loss 0.505994.
Train: 2018-08-06T01:24:56.018217: step 9136, loss 0.465224.
Train: 2018-08-06T01:24:56.266555: step 9137, loss 0.505526.
Train: 2018-08-06T01:24:56.520874: step 9138, loss 0.546144.
Train: 2018-08-06T01:24:56.770207: step 9139, loss 0.570786.
Train: 2018-08-06T01:24:57.022533: step 9140, loss 0.578997.
Test: 2018-08-06T01:24:58.287149: step 9140, loss 0.548938.
Train: 2018-08-06T01:24:58.528503: step 9141, loss 0.529209.
Train: 2018-08-06T01:24:58.777837: step 9142, loss 0.637566.
Train: 2018-08-06T01:24:59.026172: step 9143, loss 0.495569.
Train: 2018-08-06T01:24:59.274542: step 9144, loss 0.503496.
Train: 2018-08-06T01:24:59.523877: step 9145, loss 0.604502.
Train: 2018-08-06T01:24:59.776168: step 9146, loss 0.46943.
Train: 2018-08-06T01:25:00.024504: step 9147, loss 0.578952.
Train: 2018-08-06T01:25:00.268850: step 9148, loss 0.596917.
Train: 2018-08-06T01:25:00.515222: step 9149, loss 0.613937.
Train: 2018-08-06T01:25:00.763551: step 9150, loss 0.510758.
Test: 2018-08-06T01:25:02.038118: step 9150, loss 0.54903.
Train: 2018-08-06T01:25:02.276480: step 9151, loss 0.571594.
Train: 2018-08-06T01:25:02.518833: step 9152, loss 0.553749.
Train: 2018-08-06T01:25:02.770191: step 9153, loss 0.528073.
Train: 2018-08-06T01:25:03.012512: step 9154, loss 0.545131.
Train: 2018-08-06T01:25:03.259881: step 9155, loss 0.58799.
Train: 2018-08-06T01:25:03.509183: step 9156, loss 0.587958.
Train: 2018-08-06T01:25:03.758518: step 9157, loss 0.562485.
Train: 2018-08-06T01:25:04.007881: step 9158, loss 0.545314.
Train: 2018-08-06T01:25:04.256186: step 9159, loss 0.604902.
Train: 2018-08-06T01:25:04.503526: step 9160, loss 0.536897.
Test: 2018-08-06T01:25:05.786094: step 9160, loss 0.548791.
Train: 2018-08-06T01:25:06.024458: step 9161, loss 0.494574.
Train: 2018-08-06T01:25:06.287754: step 9162, loss 0.587769.
Train: 2018-08-06T01:25:06.534094: step 9163, loss 0.536956.
Train: 2018-08-06T01:25:06.792402: step 9164, loss 0.511565.
Train: 2018-08-06T01:25:07.036774: step 9165, loss 0.604712.
Train: 2018-08-06T01:25:07.287111: step 9166, loss 0.604695.
Train: 2018-08-06T01:25:07.533452: step 9167, loss 0.545447.
Train: 2018-08-06T01:25:07.785772: step 9168, loss 0.570809.
Train: 2018-08-06T01:25:08.033111: step 9169, loss 0.545503.
Train: 2018-08-06T01:25:08.289400: step 9170, loss 0.604496.
Test: 2018-08-06T01:25:09.550027: step 9170, loss 0.549113.
Train: 2018-08-06T01:25:09.789389: step 9171, loss 0.587608.
Train: 2018-08-06T01:25:10.039744: step 9172, loss 0.520456.
Train: 2018-08-06T01:25:10.289078: step 9173, loss 0.554022.
Train: 2018-08-06T01:25:10.537421: step 9174, loss 0.562419.
Train: 2018-08-06T01:25:10.786757: step 9175, loss 0.537361.
Train: 2018-08-06T01:25:11.033075: step 9176, loss 0.537388.
Train: 2018-08-06T01:25:11.282397: step 9177, loss 0.57078.
Train: 2018-08-06T01:25:11.529764: step 9178, loss 0.579103.
Train: 2018-08-06T01:25:11.779089: step 9179, loss 0.537444.
Train: 2018-08-06T01:25:12.035392: step 9180, loss 0.629063.
Test: 2018-08-06T01:25:13.299999: step 9180, loss 0.547613.
Train: 2018-08-06T01:25:13.536399: step 9181, loss 0.587382.
Train: 2018-08-06T01:25:13.783706: step 9182, loss 0.529302.
Train: 2018-08-06T01:25:14.032043: step 9183, loss 0.603867.
Train: 2018-08-06T01:25:14.277385: step 9184, loss 0.570757.
Train: 2018-08-06T01:25:14.527717: step 9185, loss 0.529569.
Train: 2018-08-06T01:25:14.776052: step 9186, loss 0.554307.
Train: 2018-08-06T01:25:15.032368: step 9187, loss 0.587191.
Train: 2018-08-06T01:25:15.290677: step 9188, loss 0.546156.
Train: 2018-08-06T01:25:15.540042: step 9189, loss 0.59534.
Train: 2018-08-06T01:25:15.788344: step 9190, loss 0.56259.
Test: 2018-08-06T01:25:17.058947: step 9190, loss 0.548975.
Train: 2018-08-06T01:25:17.296322: step 9191, loss 0.538119.
Train: 2018-08-06T01:25:17.560637: step 9192, loss 0.611551.
Train: 2018-08-06T01:25:17.811935: step 9193, loss 0.521944.
Train: 2018-08-06T01:25:18.061267: step 9194, loss 0.587047.
Train: 2018-08-06T01:25:18.309627: step 9195, loss 0.538299.
Train: 2018-08-06T01:25:18.556941: step 9196, loss 0.643842.
Train: 2018-08-06T01:25:18.807272: step 9197, loss 0.514115.
Train: 2018-08-06T01:25:19.058600: step 9198, loss 0.570801.
Train: 2018-08-06T01:25:19.307931: step 9199, loss 0.619295.
Train: 2018-08-06T01:25:19.554290: step 9200, loss 0.530504.
Test: 2018-08-06T01:25:20.798944: step 9200, loss 0.548579.
Train: 2018-08-06T01:25:21.708627: step 9201, loss 0.586932.
Train: 2018-08-06T01:25:21.955941: step 9202, loss 0.538667.
Train: 2018-08-06T01:25:22.202282: step 9203, loss 0.538697.
Train: 2018-08-06T01:25:22.449621: step 9204, loss 0.594944.
Train: 2018-08-06T01:25:22.695962: step 9205, loss 0.514648.
Train: 2018-08-06T01:25:22.939342: step 9206, loss 0.57887.
Train: 2018-08-06T01:25:23.201609: step 9207, loss 0.602974.
Train: 2018-08-06T01:25:23.448948: step 9208, loss 0.578868.
Train: 2018-08-06T01:25:23.692328: step 9209, loss 0.602925.
Train: 2018-08-06T01:25:23.948611: step 9210, loss 0.586871.
Test: 2018-08-06T01:25:25.220211: step 9210, loss 0.54935.
Train: 2018-08-06T01:25:25.404717: step 9211, loss 0.528782.
Train: 2018-08-06T01:25:25.651064: step 9212, loss 0.562893.
Train: 2018-08-06T01:25:25.897399: step 9213, loss 0.538967.
Train: 2018-08-06T01:25:26.145766: step 9214, loss 0.538973.
Train: 2018-08-06T01:25:26.390112: step 9215, loss 0.546905.
Train: 2018-08-06T01:25:26.643404: step 9216, loss 0.658819.
Train: 2018-08-06T01:25:26.890774: step 9217, loss 0.538915.
Train: 2018-08-06T01:25:27.148073: step 9218, loss 0.570884.
Train: 2018-08-06T01:25:27.397419: step 9219, loss 0.578833.
Train: 2018-08-06T01:25:27.656694: step 9220, loss 0.554919.
Test: 2018-08-06T01:25:28.913334: step 9220, loss 0.549405.
Train: 2018-08-06T01:25:29.149702: step 9221, loss 0.562894.
Train: 2018-08-06T01:25:29.396077: step 9222, loss 0.530962.
Train: 2018-08-06T01:25:29.647371: step 9223, loss 0.618754.
Train: 2018-08-06T01:25:29.892739: step 9224, loss 0.554719.
Train: 2018-08-06T01:25:30.141051: step 9225, loss 0.602871.
Train: 2018-08-06T01:25:30.388388: step 9226, loss 0.506872.
Train: 2018-08-06T01:25:30.635762: step 9227, loss 0.578984.
Train: 2018-08-06T01:25:30.884094: step 9228, loss 0.514651.
Train: 2018-08-06T01:25:31.130405: step 9229, loss 0.611137.
Train: 2018-08-06T01:25:31.381732: step 9230, loss 0.627044.
Test: 2018-08-06T01:25:32.643358: step 9230, loss 0.549177.
Train: 2018-08-06T01:25:32.973633: step 9231, loss 0.562867.
Train: 2018-08-06T01:25:33.222934: step 9232, loss 0.547252.
Train: 2018-08-06T01:25:33.471279: step 9233, loss 0.514523.
Train: 2018-08-06T01:25:33.727609: step 9234, loss 0.634951.
Train: 2018-08-06T01:25:33.980907: step 9235, loss 0.619276.
Train: 2018-08-06T01:25:34.235226: step 9236, loss 0.531135.
Train: 2018-08-06T01:25:34.483561: step 9237, loss 0.594874.
Train: 2018-08-06T01:25:34.739878: step 9238, loss 0.538928.
Train: 2018-08-06T01:25:34.986248: step 9239, loss 0.5471.
Train: 2018-08-06T01:25:35.248547: step 9240, loss 0.515313.
Test: 2018-08-06T01:25:36.511140: step 9240, loss 0.549893.
Train: 2018-08-06T01:25:36.764474: step 9241, loss 0.54727.
Train: 2018-08-06T01:25:37.008834: step 9242, loss 0.602658.
Train: 2018-08-06T01:25:37.259183: step 9243, loss 0.56294.
Train: 2018-08-06T01:25:37.506478: step 9244, loss 0.586861.
Train: 2018-08-06T01:25:37.755823: step 9245, loss 0.658398.
Train: 2018-08-06T01:25:38.002182: step 9246, loss 0.555064.
Train: 2018-08-06T01:25:38.249521: step 9247, loss 0.586794.
Train: 2018-08-06T01:25:38.504808: step 9248, loss 0.61845.
Train: 2018-08-06T01:25:38.753144: step 9249, loss 0.46055.
Train: 2018-08-06T01:25:39.011499: step 9250, loss 0.570994.
Test: 2018-08-06T01:25:40.287042: step 9250, loss 0.550102.
Train: 2018-08-06T01:25:40.533409: step 9251, loss 0.586742.
Train: 2018-08-06T01:25:40.784743: step 9252, loss 0.563076.
Train: 2018-08-06T01:25:41.027061: step 9253, loss 0.555214.
Train: 2018-08-06T01:25:41.282409: step 9254, loss 0.515811.
Train: 2018-08-06T01:25:41.539692: step 9255, loss 0.570959.
Train: 2018-08-06T01:25:41.790022: step 9256, loss 0.570938.
Train: 2018-08-06T01:25:42.036394: step 9257, loss 0.578851.
Train: 2018-08-06T01:25:42.281732: step 9258, loss 0.578902.
Train: 2018-08-06T01:25:42.528079: step 9259, loss 0.491654.
Train: 2018-08-06T01:25:42.775386: step 9260, loss 0.539096.
Test: 2018-08-06T01:25:44.035018: step 9260, loss 0.548679.
Train: 2018-08-06T01:25:44.283379: step 9261, loss 0.578882.
Train: 2018-08-06T01:25:44.527701: step 9262, loss 0.538825.
Train: 2018-08-06T01:25:44.778061: step 9263, loss 0.570859.
Train: 2018-08-06T01:25:45.023404: step 9264, loss 0.554733.
Train: 2018-08-06T01:25:45.270749: step 9265, loss 0.554558.
Train: 2018-08-06T01:25:45.518052: step 9266, loss 0.611225.
Train: 2018-08-06T01:25:45.766389: step 9267, loss 0.562712.
Train: 2018-08-06T01:25:46.010759: step 9268, loss 0.562721.
Train: 2018-08-06T01:25:46.255111: step 9269, loss 0.58702.
Train: 2018-08-06T01:25:46.503451: step 9270, loss 0.514023.
Test: 2018-08-06T01:25:47.782994: step 9270, loss 0.547375.
Train: 2018-08-06T01:25:48.021383: step 9271, loss 0.570738.
Train: 2018-08-06T01:25:48.268696: step 9272, loss 0.513878.
Train: 2018-08-06T01:25:48.515037: step 9273, loss 0.587218.
Train: 2018-08-06T01:25:48.762402: step 9274, loss 0.464494.
Train: 2018-08-06T01:25:49.009756: step 9275, loss 0.595598.
Train: 2018-08-06T01:25:49.258050: step 9276, loss 0.570759.
Train: 2018-08-06T01:25:49.513400: step 9277, loss 0.504625.
Train: 2018-08-06T01:25:49.772700: step 9278, loss 0.520778.
Train: 2018-08-06T01:25:50.019014: step 9279, loss 0.554467.
Train: 2018-08-06T01:25:50.267382: step 9280, loss 0.562533.
Test: 2018-08-06T01:25:51.540945: step 9280, loss 0.548195.
Train: 2018-08-06T01:25:51.789307: step 9281, loss 0.553423.
Train: 2018-08-06T01:25:52.034624: step 9282, loss 0.578437.
Train: 2018-08-06T01:25:52.281987: step 9283, loss 0.579298.
Train: 2018-08-06T01:25:52.529302: step 9284, loss 0.570135.
Train: 2018-08-06T01:25:52.776639: step 9285, loss 0.501653.
Train: 2018-08-06T01:25:53.023978: step 9286, loss 0.546066.
Train: 2018-08-06T01:25:53.272314: step 9287, loss 0.563621.
Train: 2018-08-06T01:25:53.523673: step 9288, loss 0.535756.
Train: 2018-08-06T01:25:53.772976: step 9289, loss 0.519887.
Train: 2018-08-06T01:25:54.031309: step 9290, loss 0.573983.
Test: 2018-08-06T01:25:55.285929: step 9290, loss 0.548643.
Train: 2018-08-06T01:25:55.522322: step 9291, loss 0.622871.
Train: 2018-08-06T01:25:55.773672: step 9292, loss 0.574478.
Train: 2018-08-06T01:25:56.019992: step 9293, loss 0.587998.
Train: 2018-08-06T01:25:56.268302: step 9294, loss 0.477522.
Train: 2018-08-06T01:25:56.519631: step 9295, loss 0.587775.
Train: 2018-08-06T01:25:56.766999: step 9296, loss 0.452742.
Train: 2018-08-06T01:25:57.014338: step 9297, loss 0.545653.
Train: 2018-08-06T01:25:57.261676: step 9298, loss 0.587887.
Train: 2018-08-06T01:25:57.513972: step 9299, loss 0.537042.
Train: 2018-08-06T01:25:57.765301: step 9300, loss 0.503262.
Test: 2018-08-06T01:25:59.048866: step 9300, loss 0.549406.
Train: 2018-08-06T01:25:59.995985: step 9301, loss 0.536984.
Train: 2018-08-06T01:26:00.242328: step 9302, loss 0.528462.
Train: 2018-08-06T01:26:00.494678: step 9303, loss 0.6303.
Train: 2018-08-06T01:26:00.745981: step 9304, loss 0.587844.
Train: 2018-08-06T01:26:00.995315: step 9305, loss 0.570845.
Train: 2018-08-06T01:26:01.239659: step 9306, loss 0.528372.
Train: 2018-08-06T01:26:01.493979: step 9307, loss 0.570844.
Train: 2018-08-06T01:26:01.739324: step 9308, loss 0.579338.
Train: 2018-08-06T01:26:01.987659: step 9309, loss 0.536898.
Train: 2018-08-06T01:26:02.249959: step 9310, loss 0.596273.
Test: 2018-08-06T01:26:03.512581: step 9310, loss 0.547431.
Train: 2018-08-06T01:26:03.752967: step 9311, loss 0.579306.
Train: 2018-08-06T01:26:03.997318: step 9312, loss 0.528543.
Train: 2018-08-06T01:26:04.257599: step 9313, loss 0.553914.
Train: 2018-08-06T01:26:04.505959: step 9314, loss 0.469559.
Train: 2018-08-06T01:26:04.762240: step 9315, loss 0.596151.
Train: 2018-08-06T01:26:05.007583: step 9316, loss 0.613031.
Train: 2018-08-06T01:26:05.256941: step 9317, loss 0.587659.
Train: 2018-08-06T01:26:05.503259: step 9318, loss 0.604484.
Train: 2018-08-06T01:26:05.752591: step 9319, loss 0.528813.
Train: 2018-08-06T01:26:06.002921: step 9320, loss 0.478612.
Test: 2018-08-06T01:26:07.264547: step 9320, loss 0.548983.
Train: 2018-08-06T01:26:07.516903: step 9321, loss 0.579167.
Train: 2018-08-06T01:26:07.766209: step 9322, loss 0.554028.
Train: 2018-08-06T01:26:08.014541: step 9323, loss 0.562391.
Train: 2018-08-06T01:26:08.266891: step 9324, loss 0.537304.
Train: 2018-08-06T01:26:08.516229: step 9325, loss 0.554018.
Train: 2018-08-06T01:26:08.768526: step 9326, loss 0.554041.
Train: 2018-08-06T01:26:09.013894: step 9327, loss 0.554046.
Train: 2018-08-06T01:26:09.263202: step 9328, loss 0.470347.
Train: 2018-08-06T01:26:09.522509: step 9329, loss 0.637861.
Train: 2018-08-06T01:26:09.772870: step 9330, loss 0.646237.
Test: 2018-08-06T01:26:11.046432: step 9330, loss 0.548237.
Train: 2018-08-06T01:26:11.286790: step 9331, loss 0.570776.
Train: 2018-08-06T01:26:11.532134: step 9332, loss 0.520658.
Train: 2018-08-06T01:26:11.797427: step 9333, loss 0.520724.
Train: 2018-08-06T01:26:12.043796: step 9334, loss 0.629159.
Train: 2018-08-06T01:26:12.291109: step 9335, loss 0.587415.
Train: 2018-08-06T01:26:12.537471: step 9336, loss 0.612283.
Train: 2018-08-06T01:26:12.784813: step 9337, loss 0.603872.
Train: 2018-08-06T01:26:13.041099: step 9338, loss 0.595492.
Train: 2018-08-06T01:26:13.294476: step 9339, loss 0.521503.
Train: 2018-08-06T01:26:13.552813: step 9340, loss 0.578954.
Test: 2018-08-06T01:26:14.828373: step 9340, loss 0.548614.
Train: 2018-08-06T01:26:15.068730: step 9341, loss 0.489192.
Train: 2018-08-06T01:26:15.318082: step 9342, loss 0.644116.
Train: 2018-08-06T01:26:15.567424: step 9343, loss 0.570785.
Train: 2018-08-06T01:26:15.815759: step 9344, loss 0.489778.
Train: 2018-08-06T01:26:16.065091: step 9345, loss 0.55461.
Train: 2018-08-06T01:26:16.313430: step 9346, loss 0.619352.
Train: 2018-08-06T01:26:16.561768: step 9347, loss 0.5385.
Train: 2018-08-06T01:26:16.812094: step 9348, loss 0.586953.
Train: 2018-08-06T01:26:17.057413: step 9349, loss 0.522467.
Train: 2018-08-06T01:26:17.305773: step 9350, loss 0.586936.
Test: 2018-08-06T01:26:18.585326: step 9350, loss 0.549698.
Train: 2018-08-06T01:26:18.823721: step 9351, loss 0.514465.
Train: 2018-08-06T01:26:19.071058: step 9352, loss 0.635271.
Train: 2018-08-06T01:26:19.318365: step 9353, loss 0.578875.
Train: 2018-08-06T01:26:19.578700: step 9354, loss 0.490472.
Train: 2018-08-06T01:26:19.826009: step 9355, loss 0.55475.
Train: 2018-08-06T01:26:20.074343: step 9356, loss 0.49839.
Train: 2018-08-06T01:26:20.320701: step 9357, loss 0.554671.
Train: 2018-08-06T01:26:20.573040: step 9358, loss 0.651681.
Train: 2018-08-06T01:26:20.819352: step 9359, loss 0.619373.
Train: 2018-08-06T01:26:21.063699: step 9360, loss 0.562748.
Test: 2018-08-06T01:26:22.338290: step 9360, loss 0.548348.
Train: 2018-08-06T01:26:22.591613: step 9361, loss 0.554681.
Train: 2018-08-06T01:26:22.791103: step 9362, loss 0.648738.
Train: 2018-08-06T01:26:23.038418: step 9363, loss 0.635147.
Train: 2018-08-06T01:26:23.286781: step 9364, loss 0.530825.
Train: 2018-08-06T01:26:23.538082: step 9365, loss 0.594825.
Train: 2018-08-06T01:26:23.785445: step 9366, loss 0.483342.
Train: 2018-08-06T01:26:24.036748: step 9367, loss 0.586811.
Train: 2018-08-06T01:26:24.284116: step 9368, loss 0.618577.
Train: 2018-08-06T01:26:24.538405: step 9369, loss 0.555088.
Train: 2018-08-06T01:26:24.786771: step 9370, loss 0.499735.
Test: 2018-08-06T01:26:26.050361: step 9370, loss 0.550726.
Train: 2018-08-06T01:26:26.288750: step 9371, loss 0.53137.
Train: 2018-08-06T01:26:26.543069: step 9372, loss 0.531325.
Train: 2018-08-06T01:26:26.800356: step 9373, loss 0.610594.
Train: 2018-08-06T01:26:27.062656: step 9374, loss 0.555034.
Train: 2018-08-06T01:26:27.311988: step 9375, loss 0.578848.
Train: 2018-08-06T01:26:27.560351: step 9376, loss 0.547024.
Train: 2018-08-06T01:26:27.808659: step 9377, loss 0.578841.
Train: 2018-08-06T01:26:28.055000: step 9378, loss 0.594797.
Train: 2018-08-06T01:26:28.301341: step 9379, loss 0.554951.
Train: 2018-08-06T01:26:28.548706: step 9380, loss 0.562906.
Test: 2018-08-06T01:26:29.814295: step 9380, loss 0.549963.
Train: 2018-08-06T01:26:30.052692: step 9381, loss 0.570887.
Train: 2018-08-06T01:26:30.299031: step 9382, loss 0.578861.
Train: 2018-08-06T01:26:30.549362: step 9383, loss 0.515061.
Train: 2018-08-06T01:26:30.798690: step 9384, loss 0.554913.
Train: 2018-08-06T01:26:31.055009: step 9385, loss 0.522888.
Train: 2018-08-06T01:26:31.302342: step 9386, loss 0.602911.
Train: 2018-08-06T01:26:31.564616: step 9387, loss 0.54677.
Train: 2018-08-06T01:26:31.813949: step 9388, loss 0.562799.
Train: 2018-08-06T01:26:32.074252: step 9389, loss 0.498387.
Train: 2018-08-06T01:26:32.322589: step 9390, loss 0.562739.
Test: 2018-08-06T01:26:33.591195: step 9390, loss 0.549559.
Train: 2018-08-06T01:26:33.834545: step 9391, loss 0.554611.
Train: 2018-08-06T01:26:34.087891: step 9392, loss 0.643834.
Train: 2018-08-06T01:26:34.336233: step 9393, loss 0.554547.
Train: 2018-08-06T01:26:34.594512: step 9394, loss 0.578911.
Train: 2018-08-06T01:26:34.840853: step 9395, loss 0.603295.
Train: 2018-08-06T01:26:35.086197: step 9396, loss 0.58703.
Train: 2018-08-06T01:26:35.333569: step 9397, loss 0.465341.
Train: 2018-08-06T01:26:35.581872: step 9398, loss 0.554542.
Train: 2018-08-06T01:26:35.852173: step 9399, loss 0.538256.
Train: 2018-08-06T01:26:36.106486: step 9400, loss 0.505601.
Test: 2018-08-06T01:26:37.372083: step 9400, loss 0.549703.
Train: 2018-08-06T01:26:38.370717: step 9401, loss 0.611621.
Train: 2018-08-06T01:26:38.621001: step 9402, loss 0.578956.
Train: 2018-08-06T01:26:38.880308: step 9403, loss 0.521619.
Train: 2018-08-06T01:26:39.125653: step 9404, loss 0.562566.
Train: 2018-08-06T01:26:39.373989: step 9405, loss 0.537889.
Train: 2018-08-06T01:26:39.619333: step 9406, loss 0.554283.
Train: 2018-08-06T01:26:39.869713: step 9407, loss 0.554247.
Train: 2018-08-06T01:26:40.123982: step 9408, loss 0.545908.
Train: 2018-08-06T01:26:40.369358: step 9409, loss 0.628733.
Train: 2018-08-06T01:26:40.622667: step 9410, loss 0.512671.
Test: 2018-08-06T01:26:41.887267: step 9410, loss 0.548357.
Train: 2018-08-06T01:26:42.127655: step 9411, loss 0.596018.
Train: 2018-08-06T01:26:42.389922: step 9412, loss 0.587564.
Train: 2018-08-06T01:26:42.638290: step 9413, loss 0.537595.
Train: 2018-08-06T01:26:42.888589: step 9414, loss 0.537612.
Train: 2018-08-06T01:26:43.151885: step 9415, loss 0.620518.
Train: 2018-08-06T01:26:43.400251: step 9416, loss 0.579042.
Train: 2018-08-06T01:26:43.648557: step 9417, loss 0.554211.
Train: 2018-08-06T01:26:43.900907: step 9418, loss 0.512884.
Train: 2018-08-06T01:26:44.164195: step 9419, loss 0.57906.
Train: 2018-08-06T01:26:44.409553: step 9420, loss 0.579032.
Test: 2018-08-06T01:26:45.684112: step 9420, loss 0.549265.
Train: 2018-08-06T01:26:45.936464: step 9421, loss 0.545969.
Train: 2018-08-06T01:26:46.183801: step 9422, loss 0.595531.
Train: 2018-08-06T01:26:46.433110: step 9423, loss 0.521257.
Train: 2018-08-06T01:26:46.682473: step 9424, loss 0.570729.
Train: 2018-08-06T01:26:46.945738: step 9425, loss 0.554287.
Train: 2018-08-06T01:26:47.196095: step 9426, loss 0.636699.
Train: 2018-08-06T01:26:47.453434: step 9427, loss 0.521418.
Train: 2018-08-06T01:26:47.705732: step 9428, loss 0.505048.
Train: 2018-08-06T01:26:47.951051: step 9429, loss 0.480319.
Train: 2018-08-06T01:26:48.198419: step 9430, loss 0.521305.
Test: 2018-08-06T01:26:49.489935: step 9430, loss 0.549631.
Train: 2018-08-06T01:26:49.734307: step 9431, loss 0.60387.
Train: 2018-08-06T01:26:50.035138: step 9432, loss 0.554169.
Train: 2018-08-06T01:26:50.283498: step 9433, loss 0.529276.
Train: 2018-08-06T01:26:50.529840: step 9434, loss 0.612314.
Train: 2018-08-06T01:26:50.777155: step 9435, loss 0.5292.
Train: 2018-08-06T01:26:51.027515: step 9436, loss 0.554121.
Train: 2018-08-06T01:26:51.274822: step 9437, loss 0.57076.
Train: 2018-08-06T01:26:51.525186: step 9438, loss 0.554089.
Train: 2018-08-06T01:26:51.787483: step 9439, loss 0.595802.
Train: 2018-08-06T01:26:52.037807: step 9440, loss 0.512366.
Test: 2018-08-06T01:26:53.303397: step 9440, loss 0.548857.
Train: 2018-08-06T01:26:53.542762: step 9441, loss 0.554069.
Train: 2018-08-06T01:26:53.790121: step 9442, loss 0.570778.
Train: 2018-08-06T01:26:54.038474: step 9443, loss 0.537332.
Train: 2018-08-06T01:26:54.288815: step 9444, loss 0.570774.
Train: 2018-08-06T01:26:54.539107: step 9445, loss 0.503801.
Train: 2018-08-06T01:26:54.787459: step 9446, loss 0.646245.
Train: 2018-08-06T01:26:55.032774: step 9447, loss 0.537296.
Train: 2018-08-06T01:26:55.289088: step 9448, loss 0.554017.
Train: 2018-08-06T01:26:55.537454: step 9449, loss 0.537253.
Train: 2018-08-06T01:26:55.791744: step 9450, loss 0.562403.
Test: 2018-08-06T01:26:57.070323: step 9450, loss 0.548781.
Train: 2018-08-06T01:26:57.311703: step 9451, loss 0.604323.
Train: 2018-08-06T01:26:57.572013: step 9452, loss 0.554021.
Train: 2018-08-06T01:26:57.819321: step 9453, loss 0.587512.
Train: 2018-08-06T01:26:58.066689: step 9454, loss 0.595843.
Train: 2018-08-06T01:26:58.322975: step 9455, loss 0.620799.
Train: 2018-08-06T01:26:58.569345: step 9456, loss 0.628937.
Train: 2018-08-06T01:26:58.815689: step 9457, loss 0.51285.
Train: 2018-08-06T01:26:59.062027: step 9458, loss 0.570756.
Train: 2018-08-06T01:26:59.313350: step 9459, loss 0.496764.
Train: 2018-08-06T01:26:59.560709: step 9460, loss 0.537914.
Test: 2018-08-06T01:27:00.836251: step 9460, loss 0.548659.
Train: 2018-08-06T01:27:01.076640: step 9461, loss 0.562551.
Train: 2018-08-06T01:27:01.323949: step 9462, loss 0.578964.
Train: 2018-08-06T01:27:01.572315: step 9463, loss 0.570763.
Train: 2018-08-06T01:27:01.823644: step 9464, loss 0.64447.
Train: 2018-08-06T01:27:02.076935: step 9465, loss 0.538132.
Train: 2018-08-06T01:27:02.332282: step 9466, loss 0.530054.
Train: 2018-08-06T01:27:02.593577: step 9467, loss 0.61146.
Train: 2018-08-06T01:27:02.840922: step 9468, loss 0.578915.
Train: 2018-08-06T01:27:03.085238: step 9469, loss 0.562702.
Train: 2018-08-06T01:27:03.341551: step 9470, loss 0.53037.
Test: 2018-08-06T01:27:04.601184: step 9470, loss 0.549057.
Train: 2018-08-06T01:27:04.835587: step 9471, loss 0.595036.
Train: 2018-08-06T01:27:05.081929: step 9472, loss 0.554696.
Train: 2018-08-06T01:27:05.335251: step 9473, loss 0.594974.
Train: 2018-08-06T01:27:05.584585: step 9474, loss 0.546706.
Train: 2018-08-06T01:27:05.831893: step 9475, loss 0.538714.
Train: 2018-08-06T01:27:06.081250: step 9476, loss 0.490564.
Train: 2018-08-06T01:27:06.326569: step 9477, loss 0.570819.
Train: 2018-08-06T01:27:06.580922: step 9478, loss 0.562784.
Train: 2018-08-06T01:27:06.827230: step 9479, loss 0.546638.
Train: 2018-08-06T01:27:07.085540: step 9480, loss 0.57889.
Test: 2018-08-06T01:27:08.344173: step 9480, loss 0.548493.
Train: 2018-08-06T01:27:08.585559: step 9481, loss 0.554653.
Train: 2018-08-06T01:27:08.833864: step 9482, loss 0.562719.
Train: 2018-08-06T01:27:09.080230: step 9483, loss 0.603186.
Train: 2018-08-06T01:27:09.327574: step 9484, loss 0.562704.
Train: 2018-08-06T01:27:09.582891: step 9485, loss 0.538412.
Train: 2018-08-06T01:27:09.828237: step 9486, loss 0.586998.
Train: 2018-08-06T01:27:10.090503: step 9487, loss 0.578899.
Train: 2018-08-06T01:27:10.338859: step 9488, loss 0.595104.
Train: 2018-08-06T01:27:10.603163: step 9489, loss 0.530337.
Train: 2018-08-06T01:27:10.848477: step 9490, loss 0.619376.
Test: 2018-08-06T01:27:12.131045: step 9490, loss 0.548858.
Train: 2018-08-06T01:27:12.368410: step 9491, loss 0.603127.
Train: 2018-08-06T01:27:12.617772: step 9492, loss 0.546629.
Train: 2018-08-06T01:27:12.862121: step 9493, loss 0.586926.
Train: 2018-08-06T01:27:13.110457: step 9494, loss 0.474412.
Train: 2018-08-06T01:27:13.370738: step 9495, loss 0.53062.
Train: 2018-08-06T01:27:13.622058: step 9496, loss 0.635258.
Train: 2018-08-06T01:27:13.870425: step 9497, loss 0.554708.
Train: 2018-08-06T01:27:14.115739: step 9498, loss 0.546697.
Train: 2018-08-06T01:27:14.363076: step 9499, loss 0.522542.
Train: 2018-08-06T01:27:14.612441: step 9500, loss 0.586954.
Test: 2018-08-06T01:27:15.886004: step 9500, loss 0.549446.
Train: 2018-08-06T01:27:16.785717: step 9501, loss 0.619208.
Train: 2018-08-06T01:27:17.031034: step 9502, loss 0.538593.
Train: 2018-08-06T01:27:17.293366: step 9503, loss 0.578875.
Train: 2018-08-06T01:27:17.540671: step 9504, loss 0.651378.
Train: 2018-08-06T01:27:17.793022: step 9505, loss 0.602972.
Train: 2018-08-06T01:27:18.054328: step 9506, loss 0.586873.
Train: 2018-08-06T01:27:18.299672: step 9507, loss 0.578859.
Train: 2018-08-06T01:27:18.550004: step 9508, loss 0.491294.
Train: 2018-08-06T01:27:18.801326: step 9509, loss 0.539109.
Train: 2018-08-06T01:27:19.047642: step 9510, loss 0.515208.
Test: 2018-08-06T01:27:20.319241: step 9510, loss 0.549069.
Train: 2018-08-06T01:27:20.557603: step 9511, loss 0.570918.
Train: 2018-08-06T01:27:20.809954: step 9512, loss 0.538932.
Train: 2018-08-06T01:27:21.001416: step 9513, loss 0.426102.
Train: 2018-08-06T01:27:21.250775: step 9514, loss 0.562522.
Train: 2018-08-06T01:27:21.506100: step 9515, loss 0.587585.
Train: 2018-08-06T01:27:21.754402: step 9516, loss 0.553917.
Train: 2018-08-06T01:27:21.999777: step 9517, loss 0.595134.
Train: 2018-08-06T01:27:22.247086: step 9518, loss 0.553864.
Train: 2018-08-06T01:27:22.492429: step 9519, loss 0.545142.
Train: 2018-08-06T01:27:22.741763: step 9520, loss 0.612543.
Test: 2018-08-06T01:27:23.998401: step 9520, loss 0.548187.
Train: 2018-08-06T01:27:24.238790: step 9521, loss 0.522207.
Train: 2018-08-06T01:27:24.499094: step 9522, loss 0.589354.
Train: 2018-08-06T01:27:24.757398: step 9523, loss 0.578251.
Train: 2018-08-06T01:27:25.005708: step 9524, loss 0.538258.
Train: 2018-08-06T01:27:25.256068: step 9525, loss 0.487901.
Train: 2018-08-06T01:27:25.505402: step 9526, loss 0.496663.
Train: 2018-08-06T01:27:25.753747: step 9527, loss 0.570826.
Train: 2018-08-06T01:27:26.008058: step 9528, loss 0.619955.
Train: 2018-08-06T01:27:26.256389: step 9529, loss 0.496308.
Train: 2018-08-06T01:27:26.504698: step 9530, loss 0.511889.
Test: 2018-08-06T01:27:27.772309: step 9530, loss 0.548632.
Train: 2018-08-06T01:27:28.015682: step 9531, loss 0.613.
Train: 2018-08-06T01:27:28.276990: step 9532, loss 0.578448.
Train: 2018-08-06T01:27:28.524297: step 9533, loss 0.553921.
Train: 2018-08-06T01:27:28.785600: step 9534, loss 0.554286.
Train: 2018-08-06T01:27:29.036958: step 9535, loss 0.503887.
Train: 2018-08-06T01:27:29.283299: step 9536, loss 0.587265.
Train: 2018-08-06T01:27:29.538610: step 9537, loss 0.545687.
Train: 2018-08-06T01:27:29.795943: step 9538, loss 0.561997.
Train: 2018-08-06T01:27:30.044233: step 9539, loss 0.528511.
Train: 2018-08-06T01:27:30.297586: step 9540, loss 0.571776.
Test: 2018-08-06T01:27:31.573144: step 9540, loss 0.548367.
Train: 2018-08-06T01:27:31.826492: step 9541, loss 0.527027.
Train: 2018-08-06T01:27:32.074803: step 9542, loss 0.699409.
Train: 2018-08-06T01:27:32.323166: step 9543, loss 0.57913.
Train: 2018-08-06T01:27:32.572496: step 9544, loss 0.595945.
Train: 2018-08-06T01:27:32.822838: step 9545, loss 0.596612.
Train: 2018-08-06T01:27:33.070172: step 9546, loss 0.487747.
Train: 2018-08-06T01:27:33.317510: step 9547, loss 0.520981.
Train: 2018-08-06T01:27:33.567840: step 9548, loss 0.620341.
Train: 2018-08-06T01:27:33.819139: step 9549, loss 0.521331.
Train: 2018-08-06T01:27:34.083430: step 9550, loss 0.636553.
Test: 2018-08-06T01:27:35.369990: step 9550, loss 0.549061.
Train: 2018-08-06T01:27:35.673332: step 9551, loss 0.55439.
Train: 2018-08-06T01:27:35.926624: step 9552, loss 0.513529.
Train: 2018-08-06T01:27:36.190917: step 9553, loss 0.578962.
Train: 2018-08-06T01:27:36.438287: step 9554, loss 0.562606.
Train: 2018-08-06T01:27:36.686592: step 9555, loss 0.513689.
Train: 2018-08-06T01:27:36.934927: step 9556, loss 0.562619.
Train: 2018-08-06T01:27:37.185282: step 9557, loss 0.53808.
Train: 2018-08-06T01:27:37.435589: step 9558, loss 0.546203.
Train: 2018-08-06T01:27:37.684953: step 9559, loss 0.620336.
Train: 2018-08-06T01:27:37.944261: step 9560, loss 0.562486.
Test: 2018-08-06T01:27:39.208846: step 9560, loss 0.548913.
Train: 2018-08-06T01:27:39.445213: step 9561, loss 0.521666.
Train: 2018-08-06T01:27:39.691579: step 9562, loss 0.505175.
Train: 2018-08-06T01:27:39.951894: step 9563, loss 0.595645.
Train: 2018-08-06T01:27:40.213159: step 9564, loss 0.546331.
Train: 2018-08-06T01:27:40.460499: step 9565, loss 0.620083.
Train: 2018-08-06T01:27:40.707868: step 9566, loss 0.538029.
Train: 2018-08-06T01:27:40.958198: step 9567, loss 0.537951.
Train: 2018-08-06T01:27:41.206504: step 9568, loss 0.521524.
Train: 2018-08-06T01:27:41.454870: step 9569, loss 0.636483.
Train: 2018-08-06T01:27:41.700234: step 9570, loss 0.578955.
Test: 2018-08-06T01:27:42.993724: step 9570, loss 0.548091.
Train: 2018-08-06T01:27:43.237072: step 9571, loss 0.595346.
Train: 2018-08-06T01:27:43.478458: step 9572, loss 0.603518.
Train: 2018-08-06T01:27:43.727760: step 9573, loss 0.529898.
Train: 2018-08-06T01:27:43.974102: step 9574, loss 0.538102.
Train: 2018-08-06T01:27:44.218448: step 9575, loss 0.464675.
Train: 2018-08-06T01:27:44.464790: step 9576, loss 0.480809.
Train: 2018-08-06T01:27:44.710164: step 9577, loss 0.56252.
Train: 2018-08-06T01:27:44.957503: step 9578, loss 0.570767.
Train: 2018-08-06T01:27:45.205808: step 9579, loss 0.57903.
Train: 2018-08-06T01:27:45.451152: step 9580, loss 0.587284.
Test: 2018-08-06T01:27:46.721754: step 9580, loss 0.548095.
Train: 2018-08-06T01:27:46.962137: step 9581, loss 0.62867.
Train: 2018-08-06T01:27:47.220420: step 9582, loss 0.562492.
Train: 2018-08-06T01:27:47.467759: step 9583, loss 0.603824.
Train: 2018-08-06T01:27:47.714100: step 9584, loss 0.546021.
Train: 2018-08-06T01:27:47.962466: step 9585, loss 0.488395.
Train: 2018-08-06T01:27:48.211795: step 9586, loss 0.471876.
Train: 2018-08-06T01:27:48.459139: step 9587, loss 0.686407.
Train: 2018-08-06T01:27:48.721437: step 9588, loss 0.554252.
Train: 2018-08-06T01:27:48.972735: step 9589, loss 0.496491.
Train: 2018-08-06T01:27:49.218104: step 9590, loss 0.545975.
Test: 2018-08-06T01:27:50.491671: step 9590, loss 0.547911.
Train: 2018-08-06T01:27:50.730066: step 9591, loss 0.636924.
Train: 2018-08-06T01:27:50.980389: step 9592, loss 0.529426.
Train: 2018-08-06T01:27:51.229731: step 9593, loss 0.562491.
Train: 2018-08-06T01:27:51.491996: step 9594, loss 0.60383.
Train: 2018-08-06T01:27:51.749308: step 9595, loss 0.587278.
Train: 2018-08-06T01:27:51.993680: step 9596, loss 0.546017.
Train: 2018-08-06T01:27:52.241028: step 9597, loss 0.529562.
Train: 2018-08-06T01:27:52.488358: step 9598, loss 0.595466.
Train: 2018-08-06T01:27:52.745676: step 9599, loss 0.56253.
Train: 2018-08-06T01:27:52.988993: step 9600, loss 0.496776.
Test: 2018-08-06T01:27:54.263584: step 9600, loss 0.548795.
Train: 2018-08-06T01:27:55.187919: step 9601, loss 0.595439.
Train: 2018-08-06T01:27:55.437251: step 9602, loss 0.554304.
Train: 2018-08-06T01:27:55.683625: step 9603, loss 0.554314.
Train: 2018-08-06T01:27:55.929964: step 9604, loss 0.587202.
Train: 2018-08-06T01:27:56.179297: step 9605, loss 0.603632.
Train: 2018-08-06T01:27:56.441598: step 9606, loss 0.52973.
Train: 2018-08-06T01:27:56.690897: step 9607, loss 0.537957.
Train: 2018-08-06T01:27:56.938237: step 9608, loss 0.529747.
Train: 2018-08-06T01:27:57.185614: step 9609, loss 0.611801.
Train: 2018-08-06T01:27:57.434927: step 9610, loss 0.529737.
Test: 2018-08-06T01:27:58.684586: step 9610, loss 0.548847.
Train: 2018-08-06T01:27:58.920984: step 9611, loss 0.587185.
Train: 2018-08-06T01:27:59.167325: step 9612, loss 0.496917.
Train: 2018-08-06T01:27:59.416656: step 9613, loss 0.578964.
Train: 2018-08-06T01:27:59.665960: step 9614, loss 0.554315.
Train: 2018-08-06T01:27:59.921278: step 9615, loss 0.521347.
Train: 2018-08-06T01:28:00.169639: step 9616, loss 0.480154.
Train: 2018-08-06T01:28:00.415988: step 9617, loss 0.686662.
Train: 2018-08-06T01:28:00.664322: step 9618, loss 0.529448.
Train: 2018-08-06T01:28:00.908639: step 9619, loss 0.537668.
Train: 2018-08-06T01:28:01.163954: step 9620, loss 0.579062.
Test: 2018-08-06T01:28:02.440541: step 9620, loss 0.548638.
Train: 2018-08-06T01:28:02.677931: step 9621, loss 0.612187.
Train: 2018-08-06T01:28:02.921288: step 9622, loss 0.620401.
Train: 2018-08-06T01:28:03.166600: step 9623, loss 0.554244.
Train: 2018-08-06T01:28:03.410976: step 9624, loss 0.53779.
Train: 2018-08-06T01:28:03.658285: step 9625, loss 0.55429.
Train: 2018-08-06T01:28:03.907618: step 9626, loss 0.578989.
Train: 2018-08-06T01:28:04.154987: step 9627, loss 0.521453.
Train: 2018-08-06T01:28:04.412267: step 9628, loss 0.595423.
Train: 2018-08-06T01:28:04.663597: step 9629, loss 0.554329.
Train: 2018-08-06T01:28:04.921925: step 9630, loss 0.603559.
Test: 2018-08-06T01:28:06.193505: step 9630, loss 0.550208.
Train: 2018-08-06T01:28:06.433887: step 9631, loss 0.554384.
Train: 2018-08-06T01:28:06.682197: step 9632, loss 0.587155.
Train: 2018-08-06T01:28:06.938513: step 9633, loss 0.538119.
Train: 2018-08-06T01:28:07.197819: step 9634, loss 0.603409.
Train: 2018-08-06T01:28:07.448150: step 9635, loss 0.603339.
Train: 2018-08-06T01:28:07.698511: step 9636, loss 0.505806.
Train: 2018-08-06T01:28:07.949808: step 9637, loss 0.611369.
Train: 2018-08-06T01:28:08.199167: step 9638, loss 0.5546.
Train: 2018-08-06T01:28:08.460472: step 9639, loss 0.578892.
Train: 2018-08-06T01:28:08.714792: step 9640, loss 0.530461.
Test: 2018-08-06T01:28:09.978383: step 9640, loss 0.549657.
Train: 2018-08-06T01:28:10.215749: step 9641, loss 0.578881.
Train: 2018-08-06T01:28:10.466077: step 9642, loss 0.586933.
Train: 2018-08-06T01:28:10.714445: step 9643, loss 0.530611.
Train: 2018-08-06T01:28:10.965767: step 9644, loss 0.562793.
Train: 2018-08-06T01:28:11.223053: step 9645, loss 0.554756.
Train: 2018-08-06T01:28:11.480366: step 9646, loss 0.586908.
Train: 2018-08-06T01:28:11.731695: step 9647, loss 0.522624.
Train: 2018-08-06T01:28:11.980030: step 9648, loss 0.570835.
Train: 2018-08-06T01:28:12.228411: step 9649, loss 0.578841.
Train: 2018-08-06T01:28:12.474707: step 9650, loss 0.619007.
Test: 2018-08-06T01:28:13.749298: step 9650, loss 0.548983.
Train: 2018-08-06T01:28:13.991675: step 9651, loss 0.578823.
Train: 2018-08-06T01:28:14.240992: step 9652, loss 0.522593.
Train: 2018-08-06T01:28:14.489351: step 9653, loss 0.586986.
Train: 2018-08-06T01:28:14.734664: step 9654, loss 0.611123.
Train: 2018-08-06T01:28:14.984020: step 9655, loss 0.514526.
Train: 2018-08-06T01:28:15.231334: step 9656, loss 0.530755.
Train: 2018-08-06T01:28:15.490642: step 9657, loss 0.490211.
Train: 2018-08-06T01:28:15.740996: step 9658, loss 0.594959.
Train: 2018-08-06T01:28:15.997287: step 9659, loss 0.537171.
Train: 2018-08-06T01:28:16.258588: step 9660, loss 0.578587.
Test: 2018-08-06T01:28:17.549136: step 9660, loss 0.548605.
Train: 2018-08-06T01:28:17.804453: step 9661, loss 0.512775.
Train: 2018-08-06T01:28:18.052788: step 9662, loss 0.580414.
Train: 2018-08-06T01:28:18.299155: step 9663, loss 0.588871.
Train: 2018-08-06T01:28:18.490634: step 9664, loss 0.615771.
Train: 2018-08-06T01:28:18.742976: step 9665, loss 0.552836.
Train: 2018-08-06T01:28:18.997264: step 9666, loss 0.537939.
Train: 2018-08-06T01:28:19.244601: step 9667, loss 0.570826.
Train: 2018-08-06T01:28:19.490973: step 9668, loss 0.556008.
Train: 2018-08-06T01:28:19.737315: step 9669, loss 0.488515.
Train: 2018-08-06T01:28:19.996621: step 9670, loss 0.545975.
Test: 2018-08-06T01:28:21.263204: step 9670, loss 0.549548.
Train: 2018-08-06T01:28:21.506552: step 9671, loss 0.488586.
Train: 2018-08-06T01:28:21.752924: step 9672, loss 0.569313.
Train: 2018-08-06T01:28:21.999261: step 9673, loss 0.570967.
Train: 2018-08-06T01:28:22.249565: step 9674, loss 0.554213.
Train: 2018-08-06T01:28:22.494939: step 9675, loss 0.544673.
Train: 2018-08-06T01:28:22.752220: step 9676, loss 0.579431.
Train: 2018-08-06T01:28:23.007537: step 9677, loss 0.547198.
Train: 2018-08-06T01:28:23.255873: step 9678, loss 0.562016.
Train: 2018-08-06T01:28:23.506234: step 9679, loss 0.49577.
Train: 2018-08-06T01:28:23.756534: step 9680, loss 0.589517.
Test: 2018-08-06T01:28:25.041100: step 9680, loss 0.549139.
Train: 2018-08-06T01:28:25.280458: step 9681, loss 0.545654.
Train: 2018-08-06T01:28:25.534810: step 9682, loss 0.519747.
Train: 2018-08-06T01:28:25.785110: step 9683, loss 0.534598.
Train: 2018-08-06T01:28:26.032479: step 9684, loss 0.509022.
Train: 2018-08-06T01:28:26.297738: step 9685, loss 0.555114.
Train: 2018-08-06T01:28:26.549068: step 9686, loss 0.58035.
Train: 2018-08-06T01:28:26.801420: step 9687, loss 0.616125.
Train: 2018-08-06T01:28:27.050759: step 9688, loss 0.562434.
Train: 2018-08-06T01:28:27.310062: step 9689, loss 0.579714.
Train: 2018-08-06T01:28:27.561385: step 9690, loss 0.564295.
Test: 2018-08-06T01:28:28.858889: step 9690, loss 0.547734.
Train: 2018-08-06T01:28:29.106227: step 9691, loss 0.546039.
Train: 2018-08-06T01:28:29.352594: step 9692, loss 0.562335.
Train: 2018-08-06T01:28:29.597938: step 9693, loss 0.537423.
Train: 2018-08-06T01:28:29.843281: step 9694, loss 0.537992.
Train: 2018-08-06T01:28:30.103561: step 9695, loss 0.554065.
Train: 2018-08-06T01:28:30.368864: step 9696, loss 0.612031.
Train: 2018-08-06T01:28:30.617213: step 9697, loss 0.579301.
Train: 2018-08-06T01:28:30.878517: step 9698, loss 0.570905.
Train: 2018-08-06T01:28:31.127829: step 9699, loss 0.570657.
Train: 2018-08-06T01:28:31.374194: step 9700, loss 0.529826.
Test: 2018-08-06T01:28:32.663719: step 9700, loss 0.5487.
Train: 2018-08-06T01:28:33.566994: step 9701, loss 0.562575.
Train: 2018-08-06T01:28:33.812366: step 9702, loss 0.611641.
Train: 2018-08-06T01:28:34.059701: step 9703, loss 0.529901.
Train: 2018-08-06T01:28:34.307045: step 9704, loss 0.538138.
Train: 2018-08-06T01:28:34.554353: step 9705, loss 0.57891.
Train: 2018-08-06T01:28:34.802713: step 9706, loss 0.578967.
Train: 2018-08-06T01:28:35.061029: step 9707, loss 0.57066.
Train: 2018-08-06T01:28:35.310362: step 9708, loss 0.554319.
Train: 2018-08-06T01:28:35.564679: step 9709, loss 0.546205.
Train: 2018-08-06T01:28:35.812989: step 9710, loss 0.554503.
Test: 2018-08-06T01:28:37.084586: step 9710, loss 0.549234.
Train: 2018-08-06T01:28:37.321951: step 9711, loss 0.513886.
Train: 2018-08-06T01:28:37.566329: step 9712, loss 0.578989.
Train: 2018-08-06T01:28:37.811642: step 9713, loss 0.53821.
Train: 2018-08-06T01:28:38.064990: step 9714, loss 0.546216.
Train: 2018-08-06T01:28:38.312305: step 9715, loss 0.521752.
Train: 2018-08-06T01:28:38.562634: step 9716, loss 0.587132.
Train: 2018-08-06T01:28:38.810004: step 9717, loss 0.570724.
Train: 2018-08-06T01:28:39.055345: step 9718, loss 0.52959.
Train: 2018-08-06T01:28:39.315633: step 9719, loss 0.529604.
Train: 2018-08-06T01:28:39.567976: step 9720, loss 0.545966.
Test: 2018-08-06T01:28:40.828573: step 9720, loss 0.548322.
Train: 2018-08-06T01:28:41.071948: step 9721, loss 0.554297.
Train: 2018-08-06T01:28:41.326268: step 9722, loss 0.512854.
Train: 2018-08-06T01:28:41.574610: step 9723, loss 0.537502.
Train: 2018-08-06T01:28:41.823946: step 9724, loss 0.612468.
Train: 2018-08-06T01:28:42.073293: step 9725, loss 0.6125.
Train: 2018-08-06T01:28:42.322607: step 9726, loss 0.579059.
Train: 2018-08-06T01:28:42.583879: step 9727, loss 0.537439.
Train: 2018-08-06T01:28:42.842221: step 9728, loss 0.595861.
Train: 2018-08-06T01:28:43.089553: step 9729, loss 0.554077.
Train: 2018-08-06T01:28:43.334871: step 9730, loss 0.520849.
Test: 2018-08-06T01:28:44.593504: step 9730, loss 0.548924.
Train: 2018-08-06T01:28:44.830869: step 9731, loss 0.495881.
Train: 2018-08-06T01:28:45.077211: step 9732, loss 0.55413.
Train: 2018-08-06T01:28:45.326544: step 9733, loss 0.579118.
Train: 2018-08-06T01:28:45.573908: step 9734, loss 0.671037.
Train: 2018-08-06T01:28:45.823241: step 9735, loss 0.595794.
Train: 2018-08-06T01:28:46.082553: step 9736, loss 0.579062.
Train: 2018-08-06T01:28:46.330883: step 9737, loss 0.520975.
Train: 2018-08-06T01:28:46.581189: step 9738, loss 0.58732.
Train: 2018-08-06T01:28:46.828554: step 9739, loss 0.562487.
Train: 2018-08-06T01:28:47.075896: step 9740, loss 0.579011.
Test: 2018-08-06T01:28:48.358436: step 9740, loss 0.548773.
Train: 2018-08-06T01:28:48.597795: step 9741, loss 0.578998.
Train: 2018-08-06T01:28:48.847129: step 9742, loss 0.51326.
Train: 2018-08-06T01:28:49.092510: step 9743, loss 0.505102.
Train: 2018-08-06T01:28:49.335822: step 9744, loss 0.54614.
Train: 2018-08-06T01:28:49.594158: step 9745, loss 0.554334.
Train: 2018-08-06T01:28:49.845460: step 9746, loss 0.513182.
Train: 2018-08-06T01:28:50.118919: step 9747, loss 0.55425.
Train: 2018-08-06T01:28:50.368250: step 9748, loss 0.587301.
Train: 2018-08-06T01:28:50.615590: step 9749, loss 0.537645.
Train: 2018-08-06T01:28:50.862929: step 9750, loss 0.554262.
Test: 2018-08-06T01:28:52.122529: step 9750, loss 0.548041.
Train: 2018-08-06T01:28:52.358922: step 9751, loss 0.595713.
Train: 2018-08-06T01:28:52.619201: step 9752, loss 0.579161.
Train: 2018-08-06T01:28:52.866564: step 9753, loss 0.496158.
Train: 2018-08-06T01:28:53.119893: step 9754, loss 0.579059.
Train: 2018-08-06T01:28:53.365232: step 9755, loss 0.512562.
Train: 2018-08-06T01:28:53.624538: step 9756, loss 0.587498.
Train: 2018-08-06T01:28:53.870853: step 9757, loss 0.545757.
Train: 2018-08-06T01:28:54.117226: step 9758, loss 0.56242.
Train: 2018-08-06T01:28:54.366553: step 9759, loss 0.562356.
Train: 2018-08-06T01:28:54.612875: step 9760, loss 0.437312.
Test: 2018-08-06T01:28:55.894447: step 9760, loss 0.548996.
Train: 2018-08-06T01:28:56.132844: step 9761, loss 0.486984.
Train: 2018-08-06T01:28:56.377157: step 9762, loss 0.68048.
Train: 2018-08-06T01:28:56.627487: step 9763, loss 0.494947.
Train: 2018-08-06T01:28:56.872832: step 9764, loss 0.579301.
Train: 2018-08-06T01:28:57.123195: step 9765, loss 0.646936.
Train: 2018-08-06T01:28:57.372520: step 9766, loss 0.520143.
Train: 2018-08-06T01:28:57.622869: step 9767, loss 0.579259.
Train: 2018-08-06T01:28:57.873168: step 9768, loss 0.579261.
Train: 2018-08-06T01:28:58.118500: step 9769, loss 0.562371.
Train: 2018-08-06T01:28:58.378804: step 9770, loss 0.56237.
Test: 2018-08-06T01:28:59.637437: step 9770, loss 0.548519.
Train: 2018-08-06T01:28:59.878822: step 9771, loss 0.520306.
Train: 2018-08-06T01:29:00.123163: step 9772, loss 0.62127.
Train: 2018-08-06T01:29:00.378455: step 9773, loss 0.511988.
Train: 2018-08-06T01:29:00.621805: step 9774, loss 0.528804.
Train: 2018-08-06T01:29:00.866153: step 9775, loss 0.570785.
Train: 2018-08-06T01:29:01.117506: step 9776, loss 0.595972.
Train: 2018-08-06T01:29:01.370826: step 9777, loss 0.579164.
Train: 2018-08-06T01:29:01.617143: step 9778, loss 0.554027.
Train: 2018-08-06T01:29:01.865480: step 9779, loss 0.545678.
Train: 2018-08-06T01:29:02.111820: step 9780, loss 0.579133.
Test: 2018-08-06T01:29:03.376438: step 9780, loss 0.548482.
Train: 2018-08-06T01:29:03.613831: step 9781, loss 0.503992.
Train: 2018-08-06T01:29:03.862193: step 9782, loss 0.620855.
Train: 2018-08-06T01:29:04.125462: step 9783, loss 0.587434.
Train: 2018-08-06T01:29:04.369813: step 9784, loss 0.512533.
Train: 2018-08-06T01:29:04.619115: step 9785, loss 0.60401.
Train: 2018-08-06T01:29:04.866484: step 9786, loss 0.520966.
Train: 2018-08-06T01:29:05.115834: step 9787, loss 0.61222.
Train: 2018-08-06T01:29:05.361131: step 9788, loss 0.487989.
Train: 2018-08-06T01:29:05.611461: step 9789, loss 0.628694.
Train: 2018-08-06T01:29:05.874788: step 9790, loss 0.570755.
Test: 2018-08-06T01:29:07.145358: step 9790, loss 0.547782.
Train: 2018-08-06T01:29:07.395690: step 9791, loss 0.56251.
Train: 2018-08-06T01:29:07.648014: step 9792, loss 0.595464.
Train: 2018-08-06T01:29:07.897347: step 9793, loss 0.562541.
Train: 2018-08-06T01:29:08.161667: step 9794, loss 0.652763.
Train: 2018-08-06T01:29:08.418984: step 9795, loss 0.57077.
Train: 2018-08-06T01:29:08.669314: step 9796, loss 0.619596.
Train: 2018-08-06T01:29:08.915689: step 9797, loss 0.59509.
Train: 2018-08-06T01:29:09.164020: step 9798, loss 0.554709.
Train: 2018-08-06T01:29:09.411357: step 9799, loss 0.626996.
Train: 2018-08-06T01:29:09.661664: step 9800, loss 0.642686.
Test: 2018-08-06T01:29:10.927280: step 9800, loss 0.548862.
Train: 2018-08-06T01:29:11.830627: step 9801, loss 0.547159.
Train: 2018-08-06T01:29:12.076969: step 9802, loss 0.570984.
Train: 2018-08-06T01:29:12.328326: step 9803, loss 0.547509.
Train: 2018-08-06T01:29:12.579632: step 9804, loss 0.586698.
Train: 2018-08-06T01:29:12.827990: step 9805, loss 0.532221.
Train: 2018-08-06T01:29:13.075329: step 9806, loss 0.586673.
Train: 2018-08-06T01:29:13.330645: step 9807, loss 0.594407.
Train: 2018-08-06T01:29:13.577955: step 9808, loss 0.594377.
Train: 2018-08-06T01:29:13.836272: step 9809, loss 0.663599.
Train: 2018-08-06T01:29:14.081607: step 9810, loss 0.556028.
Test: 2018-08-06T01:29:15.351212: step 9810, loss 0.552798.
Train: 2018-08-06T01:29:15.588578: step 9811, loss 0.579024.
Train: 2018-08-06T01:29:15.841899: step 9812, loss 0.541076.
Train: 2018-08-06T01:29:16.089262: step 9813, loss 0.548763.
Train: 2018-08-06T01:29:16.352555: step 9814, loss 0.594223.
Train: 2018-08-06T01:29:16.553005: step 9815, loss 0.547877.
Train: 2018-08-06T01:29:16.803328: step 9816, loss 0.609312.
Train: 2018-08-06T01:29:17.064660: step 9817, loss 0.556498.
Train: 2018-08-06T01:29:17.312966: step 9818, loss 0.548981.
Train: 2018-08-06T01:29:17.575295: step 9819, loss 0.639438.
Train: 2018-08-06T01:29:17.821606: step 9820, loss 0.579137.
Test: 2018-08-06T01:29:19.106169: step 9820, loss 0.551378.
Train: 2018-08-06T01:29:19.349519: step 9821, loss 0.57915.
Train: 2018-08-06T01:29:19.597855: step 9822, loss 0.601696.
Train: 2018-08-06T01:29:19.849213: step 9823, loss 0.541676.
Train: 2018-08-06T01:29:20.102506: step 9824, loss 0.556687.
Train: 2018-08-06T01:29:20.353833: step 9825, loss 0.54917.
Train: 2018-08-06T01:29:20.599202: step 9826, loss 0.556632.
Train: 2018-08-06T01:29:20.852499: step 9827, loss 0.533997.
Train: 2018-08-06T01:29:21.100834: step 9828, loss 0.541388.
Train: 2018-08-06T01:29:21.348174: step 9829, loss 0.62452.
Train: 2018-08-06T01:29:21.609509: step 9830, loss 0.579068.
Test: 2018-08-06T01:29:22.871100: step 9830, loss 0.550797.
Train: 2018-08-06T01:29:23.110461: step 9831, loss 0.579046.
Train: 2018-08-06T01:29:23.356808: step 9832, loss 0.57142.
Train: 2018-08-06T01:29:23.609167: step 9833, loss 0.58665.
Train: 2018-08-06T01:29:23.856506: step 9834, loss 0.586647.
Train: 2018-08-06T01:29:24.105803: step 9835, loss 0.563726.
Train: 2018-08-06T01:29:24.357132: step 9836, loss 0.525447.
Train: 2018-08-06T01:29:24.604470: step 9837, loss 0.632644.
Train: 2018-08-06T01:29:24.851822: step 9838, loss 0.540615.
Train: 2018-08-06T01:29:25.101142: step 9839, loss 0.563587.
Train: 2018-08-06T01:29:25.358455: step 9840, loss 0.571266.
Test: 2018-08-06T01:29:26.639029: step 9840, loss 0.549763.
Train: 2018-08-06T01:29:26.889361: step 9841, loss 0.524963.
Train: 2018-08-06T01:29:27.134703: step 9842, loss 0.540264.
Train: 2018-08-06T01:29:27.383040: step 9843, loss 0.641015.
Train: 2018-08-06T01:29:27.633371: step 9844, loss 0.571133.
Train: 2018-08-06T01:29:27.879711: step 9845, loss 0.571116.
Train: 2018-08-06T01:29:28.128047: step 9846, loss 0.5166.
Train: 2018-08-06T01:29:28.376384: step 9847, loss 0.602307.
Train: 2018-08-06T01:29:28.624720: step 9848, loss 0.586697.
Train: 2018-08-06T01:29:28.870064: step 9849, loss 0.586683.
Train: 2018-08-06T01:29:29.120395: step 9850, loss 0.486974.
Test: 2018-08-06T01:29:30.373043: step 9850, loss 0.549742.
Train: 2018-08-06T01:29:30.617389: step 9851, loss 0.578876.
Train: 2018-08-06T01:29:30.866751: step 9852, loss 0.508021.
Train: 2018-08-06T01:29:31.115059: step 9853, loss 0.555201.
Train: 2018-08-06T01:29:31.363396: step 9854, loss 0.539178.
Train: 2018-08-06T01:29:31.610780: step 9855, loss 0.562981.
Train: 2018-08-06T01:29:31.860098: step 9856, loss 0.594856.
Train: 2018-08-06T01:29:32.110433: step 9857, loss 0.578843.
Train: 2018-08-06T01:29:32.364747: step 9858, loss 0.562833.
Train: 2018-08-06T01:29:32.621056: step 9859, loss 0.603037.
Train: 2018-08-06T01:29:32.883355: step 9860, loss 0.627145.
Test: 2018-08-06T01:29:34.138971: step 9860, loss 0.548072.
Train: 2018-08-06T01:29:34.390330: step 9861, loss 0.514558.
Train: 2018-08-06T01:29:34.640658: step 9862, loss 0.578874.
Train: 2018-08-06T01:29:34.886005: step 9863, loss 0.594967.
Train: 2018-08-06T01:29:35.142290: step 9864, loss 0.562793.
Train: 2018-08-06T01:29:35.389628: step 9865, loss 0.522602.
Train: 2018-08-06T01:29:35.639988: step 9866, loss 0.578876.
Train: 2018-08-06T01:29:35.889290: step 9867, loss 0.506455.
Train: 2018-08-06T01:29:36.135631: step 9868, loss 0.546637.
Train: 2018-08-06T01:29:36.385962: step 9869, loss 0.562732.
Train: 2018-08-06T01:29:36.646267: step 9870, loss 0.562709.
Test: 2018-08-06T01:29:37.909887: step 9870, loss 0.548592.
Train: 2018-08-06T01:29:38.220090: step 9871, loss 0.562688.
Train: 2018-08-06T01:29:38.470416: step 9872, loss 0.554551.
Train: 2018-08-06T01:29:38.717727: step 9873, loss 0.481338.
Train: 2018-08-06T01:29:38.965067: step 9874, loss 0.489183.
Train: 2018-08-06T01:29:39.213400: step 9875, loss 0.669138.
Train: 2018-08-06T01:29:39.464730: step 9876, loss 0.496849.
Train: 2018-08-06T01:29:39.711071: step 9877, loss 0.56252.
Train: 2018-08-06T01:29:39.959406: step 9878, loss 0.512939.
Train: 2018-08-06T01:29:40.210760: step 9879, loss 0.504449.
Train: 2018-08-06T01:29:40.471058: step 9880, loss 0.562438.
Test: 2018-08-06T01:29:41.743634: step 9880, loss 0.549035.
Train: 2018-08-06T01:29:41.985006: step 9881, loss 0.50392.
Train: 2018-08-06T01:29:42.232328: step 9882, loss 0.495231.
Train: 2018-08-06T01:29:42.479691: step 9883, loss 0.621456.
Train: 2018-08-06T01:29:42.728036: step 9884, loss 0.460723.
Train: 2018-08-06T01:29:42.976338: step 9885, loss 0.587879.
Train: 2018-08-06T01:29:43.235675: step 9886, loss 0.553792.
Train: 2018-08-06T01:29:43.483982: step 9887, loss 0.545189.
Train: 2018-08-06T01:29:43.730321: step 9888, loss 0.44194.
Train: 2018-08-06T01:29:43.988632: step 9889, loss 0.622843.
Train: 2018-08-06T01:29:44.237964: step 9890, loss 0.545006.
Test: 2018-08-06T01:29:45.500586: step 9890, loss 0.547236.
Train: 2018-08-06T01:29:45.740975: step 9891, loss 0.562352.
Train: 2018-08-06T01:29:45.987285: step 9892, loss 0.605916.
Train: 2018-08-06T01:29:46.233626: step 9893, loss 0.527488.
Train: 2018-08-06T01:29:46.481987: step 9894, loss 0.571095.
Train: 2018-08-06T01:29:46.744289: step 9895, loss 0.536166.
Train: 2018-08-06T01:29:46.992623: step 9896, loss 0.553629.
Train: 2018-08-06T01:29:47.240934: step 9897, loss 0.553633.
Train: 2018-08-06T01:29:47.503231: step 9898, loss 0.536175.
Train: 2018-08-06T01:29:47.750595: step 9899, loss 0.536146.
Train: 2018-08-06T01:29:48.003904: step 9900, loss 0.56239.
Test: 2018-08-06T01:29:49.257550: step 9900, loss 0.546557.
Train: 2018-08-06T01:29:50.161798: step 9901, loss 0.606165.
Train: 2018-08-06T01:29:50.423099: step 9902, loss 0.571112.
Train: 2018-08-06T01:29:50.682398: step 9903, loss 0.527476.
Train: 2018-08-06T01:29:50.929713: step 9904, loss 0.562362.
Train: 2018-08-06T01:29:51.178074: step 9905, loss 0.571061.
Train: 2018-08-06T01:29:51.440348: step 9906, loss 0.527596.
Train: 2018-08-06T01:29:51.686720: step 9907, loss 0.562339.
Train: 2018-08-06T01:29:51.933054: step 9908, loss 0.527677.
Train: 2018-08-06T01:29:52.189377: step 9909, loss 0.519038.
Train: 2018-08-06T01:29:52.437689: step 9910, loss 0.501714.
Test: 2018-08-06T01:29:53.749172: step 9910, loss 0.546867.
Train: 2018-08-06T01:29:53.990552: step 9911, loss 0.562355.
Train: 2018-08-06T01:29:54.236895: step 9912, loss 0.623072.
Train: 2018-08-06T01:29:54.484206: step 9913, loss 0.562347.
Train: 2018-08-06T01:29:54.733566: step 9914, loss 0.536371.
Train: 2018-08-06T01:29:54.988897: step 9915, loss 0.545043.
Train: 2018-08-06T01:29:55.245205: step 9916, loss 0.54506.
Train: 2018-08-06T01:29:55.504509: step 9917, loss 0.596868.
Train: 2018-08-06T01:29:55.755831: step 9918, loss 0.60544.
Train: 2018-08-06T01:29:56.015138: step 9919, loss 0.596725.
Train: 2018-08-06T01:29:56.280429: step 9920, loss 0.502353.
Test: 2018-08-06T01:29:57.563971: step 9920, loss 0.548842.
Train: 2018-08-06T01:29:57.801335: step 9921, loss 0.57944.
Train: 2018-08-06T01:29:58.049675: step 9922, loss 0.511152.
Train: 2018-08-06T01:29:58.299035: step 9923, loss 0.545301.
Train: 2018-08-06T01:29:58.553357: step 9924, loss 0.6049.
Train: 2018-08-06T01:29:58.800663: step 9925, loss 0.587829.
Train: 2018-08-06T01:29:59.046031: step 9926, loss 0.58777.
Train: 2018-08-06T01:29:59.297334: step 9927, loss 0.596149.
Train: 2018-08-06T01:29:59.548693: step 9928, loss 0.596037.
Train: 2018-08-06T01:29:59.804011: step 9929, loss 0.520508.
Train: 2018-08-06T01:30:00.050320: step 9930, loss 0.537358.
Test: 2018-08-06T01:30:01.324912: step 9930, loss 0.55024.
Train: 2018-08-06T01:30:01.566297: step 9931, loss 0.520764.
Train: 2018-08-06T01:30:01.826570: step 9932, loss 0.529147.
Train: 2018-08-06T01:30:02.088874: step 9933, loss 0.620676.
Train: 2018-08-06T01:30:02.337205: step 9934, loss 0.587366.
Train: 2018-08-06T01:30:02.584568: step 9935, loss 0.554189.
Train: 2018-08-06T01:30:02.831890: step 9936, loss 0.529421.
Train: 2018-08-06T01:30:03.079220: step 9937, loss 0.727655.
Train: 2018-08-06T01:30:03.326590: step 9938, loss 0.554323.
Train: 2018-08-06T01:30:03.574896: step 9939, loss 0.578949.
Train: 2018-08-06T01:30:03.837193: step 9940, loss 0.570776.
Test: 2018-08-06T01:30:05.110787: step 9940, loss 0.548932.
Train: 2018-08-06T01:30:05.352173: step 9941, loss 0.538325.
Train: 2018-08-06T01:30:05.601500: step 9942, loss 0.570801.
Train: 2018-08-06T01:30:05.854797: step 9943, loss 0.6757.
Train: 2018-08-06T01:30:06.106127: step 9944, loss 0.570843.
Train: 2018-08-06T01:30:06.356457: step 9945, loss 0.546913.
Train: 2018-08-06T01:30:06.606787: step 9946, loss 0.594771.
Train: 2018-08-06T01:30:06.862130: step 9947, loss 0.563014.
Train: 2018-08-06T01:30:07.111438: step 9948, loss 0.570968.
Train: 2018-08-06T01:30:07.358806: step 9949, loss 0.539528.
Train: 2018-08-06T01:30:07.606114: step 9950, loss 0.55532.
Test: 2018-08-06T01:30:08.872726: step 9950, loss 0.550109.
Train: 2018-08-06T01:30:09.121062: step 9951, loss 0.547523.
Train: 2018-08-06T01:30:09.380393: step 9952, loss 0.563215.
Train: 2018-08-06T01:30:09.640704: step 9953, loss 0.547569.
Train: 2018-08-06T01:30:09.889042: step 9954, loss 0.563222.
Train: 2018-08-06T01:30:10.151333: step 9955, loss 0.618038.
Train: 2018-08-06T01:30:10.395653: step 9956, loss 0.508456.
Train: 2018-08-06T01:30:10.651981: step 9957, loss 0.586711.
Train: 2018-08-06T01:30:10.905292: step 9958, loss 0.625885.
Train: 2018-08-06T01:30:11.165625: step 9959, loss 0.555402.
Train: 2018-08-06T01:30:11.429913: step 9960, loss 0.563238.
Test: 2018-08-06T01:30:12.699492: step 9960, loss 0.550542.
Train: 2018-08-06T01:30:12.937901: step 9961, loss 0.602344.
Train: 2018-08-06T01:30:13.182202: step 9962, loss 0.539824.
Train: 2018-08-06T01:30:13.431535: step 9963, loss 0.492956.
Train: 2018-08-06T01:30:13.678904: step 9964, loss 0.586708.
Train: 2018-08-06T01:30:13.926225: step 9965, loss 0.524011.
Train: 2018-08-06T01:30:14.119727: step 9966, loss 0.563156.
Train: 2018-08-06T01:30:14.378003: step 9967, loss 0.578866.
Train: 2018-08-06T01:30:14.625374: step 9968, loss 0.657776.
Train: 2018-08-06T01:30:14.875672: step 9969, loss 0.570976.
Train: 2018-08-06T01:30:15.125033: step 9970, loss 0.57098.
Test: 2018-08-06T01:30:16.390621: step 9970, loss 0.549941.
Train: 2018-08-06T01:30:16.643971: step 9971, loss 0.649794.
Train: 2018-08-06T01:30:16.889318: step 9972, loss 0.571007.
Train: 2018-08-06T01:30:17.138652: step 9973, loss 0.523955.
Train: 2018-08-06T01:30:17.385959: step 9974, loss 0.539677.
Train: 2018-08-06T01:30:17.635294: step 9975, loss 0.586717.
Train: 2018-08-06T01:30:17.880643: step 9976, loss 0.586715.
Train: 2018-08-06T01:30:18.139970: step 9977, loss 0.500536.
Train: 2018-08-06T01:30:18.391302: step 9978, loss 0.555343.
Train: 2018-08-06T01:30:18.639607: step 9979, loss 0.508167.
Train: 2018-08-06T01:30:18.885974: step 9980, loss 0.53159.
Test: 2018-08-06T01:30:20.157547: step 9980, loss 0.550748.
Train: 2018-08-06T01:30:20.397935: step 9981, loss 0.523506.
Train: 2018-08-06T01:30:20.643254: step 9982, loss 0.507375.
Train: 2018-08-06T01:30:20.891621: step 9983, loss 0.538931.
Train: 2018-08-06T01:30:21.139925: step 9984, loss 0.619022.
Train: 2018-08-06T01:30:21.385313: step 9985, loss 0.603061.
Train: 2018-08-06T01:30:21.641608: step 9986, loss 0.586967.
Train: 2018-08-06T01:30:21.891946: step 9987, loss 0.562708.
Train: 2018-08-06T01:30:22.152218: step 9988, loss 0.603216.
Train: 2018-08-06T01:30:22.399583: step 9989, loss 0.627554.
Train: 2018-08-06T01:30:22.649918: step 9990, loss 0.514099.
Test: 2018-08-06T01:30:23.920489: step 9990, loss 0.548792.
Train: 2018-08-06T01:30:24.156903: step 9991, loss 0.554594.
Train: 2018-08-06T01:30:24.418159: step 9992, loss 0.595108.
Train: 2018-08-06T01:30:24.664526: step 9993, loss 0.514088.
Train: 2018-08-06T01:30:24.911838: step 9994, loss 0.546469.
Train: 2018-08-06T01:30:25.159177: step 9995, loss 0.570788.
Train: 2018-08-06T01:30:25.410509: step 9996, loss 0.513913.
Train: 2018-08-06T01:30:25.658841: step 9997, loss 0.635903.
Train: 2018-08-06T01:30:25.906180: step 9998, loss 0.570779.
Train: 2018-08-06T01:30:26.154539: step 9999, loss 0.562639.
Train: 2018-08-06T01:30:26.398862: step 10000, loss 0.513808.
Test: 2018-08-06T01:30:27.667469: step 10000, loss 0.548272.
Train: 2018-08-06T01:30:28.579673: step 10001, loss 0.538187.
Train: 2018-08-06T01:30:28.826020: step 10002, loss 0.627891.
Train: 2018-08-06T01:30:29.076314: step 10003, loss 0.546297.
Train: 2018-08-06T01:30:29.322655: step 10004, loss 0.578932.
Train: 2018-08-06T01:30:29.574012: step 10005, loss 0.595249.
Train: 2018-08-06T01:30:29.824319: step 10006, loss 0.546321.
Train: 2018-08-06T01:30:30.083646: step 10007, loss 0.546333.
Train: 2018-08-06T01:30:30.334948: step 10008, loss 0.538186.
Train: 2018-08-06T01:30:30.583284: step 10009, loss 0.530014.
Train: 2018-08-06T01:30:30.828652: step 10010, loss 0.570771.
Test: 2018-08-06T01:30:32.079282: step 10010, loss 0.54933.
Train: 2018-08-06T01:30:32.318642: step 10011, loss 0.562599.
Train: 2018-08-06T01:30:32.571996: step 10012, loss 0.554413.
Train: 2018-08-06T01:30:32.820333: step 10013, loss 0.59532.
Train: 2018-08-06T01:30:33.071628: step 10014, loss 0.529839.
Train: 2018-08-06T01:30:33.318969: step 10015, loss 0.480654.
Train: 2018-08-06T01:30:33.567319: step 10016, loss 0.595399.
Train: 2018-08-06T01:30:33.814641: step 10017, loss 0.611886.
Train: 2018-08-06T01:30:34.080931: step 10018, loss 0.59544.
Train: 2018-08-06T01:30:34.330288: step 10019, loss 0.562536.
Train: 2018-08-06T01:30:34.575608: step 10020, loss 0.554325.
Test: 2018-08-06T01:30:35.870144: step 10020, loss 0.548639.
Train: 2018-08-06T01:30:36.131471: step 10021, loss 0.496837.
Train: 2018-08-06T01:30:36.389780: step 10022, loss 0.537867.
Train: 2018-08-06T01:30:36.635099: step 10023, loss 0.636642.
Train: 2018-08-06T01:30:36.883440: step 10024, loss 0.587222.
Train: 2018-08-06T01:30:37.134763: step 10025, loss 0.595432.
Train: 2018-08-06T01:30:37.384127: step 10026, loss 0.546128.
Train: 2018-08-06T01:30:37.646394: step 10027, loss 0.595366.
Train: 2018-08-06T01:30:37.890740: step 10028, loss 0.480716.
Train: 2018-08-06T01:30:38.140099: step 10029, loss 0.628082.
Train: 2018-08-06T01:30:38.386417: step 10030, loss 0.578944.
Test: 2018-08-06T01:30:39.653029: step 10030, loss 0.547657.
Train: 2018-08-06T01:30:39.892401: step 10031, loss 0.546278.
Train: 2018-08-06T01:30:40.140737: step 10032, loss 0.546307.
Train: 2018-08-06T01:30:40.396074: step 10033, loss 0.578926.
Train: 2018-08-06T01:30:40.645387: step 10034, loss 0.497488.
Train: 2018-08-06T01:30:40.894706: step 10035, loss 0.619671.
Train: 2018-08-06T01:30:41.145037: step 10036, loss 0.562634.
Train: 2018-08-06T01:30:41.391379: step 10037, loss 0.538225.
Train: 2018-08-06T01:30:41.638742: step 10038, loss 0.546363.
Train: 2018-08-06T01:30:41.887108: step 10039, loss 0.668487.
Train: 2018-08-06T01:30:42.136387: step 10040, loss 0.522033.
Test: 2018-08-06T01:30:43.392028: step 10040, loss 0.549113.
Train: 2018-08-06T01:30:43.629405: step 10041, loss 0.513963.
Train: 2018-08-06T01:30:43.875735: step 10042, loss 0.554545.
Train: 2018-08-06T01:30:44.125067: step 10043, loss 0.635785.
Train: 2018-08-06T01:30:44.372431: step 10044, loss 0.562675.
Train: 2018-08-06T01:30:44.623759: step 10045, loss 0.554581.
Train: 2018-08-06T01:30:44.875093: step 10046, loss 0.522196.
Train: 2018-08-06T01:30:45.122426: step 10047, loss 0.505974.
Train: 2018-08-06T01:30:45.383727: step 10048, loss 0.619486.
Train: 2018-08-06T01:30:45.646999: step 10049, loss 0.603258.
Train: 2018-08-06T01:30:45.895362: step 10050, loss 0.587014.
Test: 2018-08-06T01:30:47.167930: step 10050, loss 0.547674.
Train: 2018-08-06T01:30:47.411313: step 10051, loss 0.6032.
Train: 2018-08-06T01:30:47.671584: step 10052, loss 0.530383.
Train: 2018-08-06T01:30:47.919920: step 10053, loss 0.546582.
Train: 2018-08-06T01:30:48.168281: step 10054, loss 0.52238.
Train: 2018-08-06T01:30:48.418585: step 10055, loss 0.578886.
Train: 2018-08-06T01:30:48.667919: step 10056, loss 0.554653.
Train: 2018-08-06T01:30:48.920245: step 10057, loss 0.52232.
Train: 2018-08-06T01:30:49.168587: step 10058, loss 0.595078.
Train: 2018-08-06T01:30:49.417912: step 10059, loss 0.530317.
Train: 2018-08-06T01:30:49.675243: step 10060, loss 0.595113.
Test: 2018-08-06T01:30:50.933859: step 10060, loss 0.550259.
Train: 2018-08-06T01:30:51.174216: step 10061, loss 0.570792.
Train: 2018-08-06T01:30:51.475535: step 10062, loss 0.570791.
Train: 2018-08-06T01:30:51.723866: step 10063, loss 0.56268.
Train: 2018-08-06T01:30:51.983178: step 10064, loss 0.570791.
Train: 2018-08-06T01:30:52.233509: step 10065, loss 0.554571.
Train: 2018-08-06T01:30:52.483846: step 10066, loss 0.481569.
Train: 2018-08-06T01:30:52.738153: step 10067, loss 0.522018.
Train: 2018-08-06T01:30:52.987462: step 10068, loss 0.554474.
Train: 2018-08-06T01:30:53.234801: step 10069, loss 0.529905.
Train: 2018-08-06T01:30:53.486154: step 10070, loss 0.554365.
Test: 2018-08-06T01:30:54.753738: step 10070, loss 0.54899.
Train: 2018-08-06T01:30:54.995092: step 10071, loss 0.620099.
Train: 2018-08-06T01:30:55.241465: step 10072, loss 0.587226.
Train: 2018-08-06T01:30:55.489770: step 10073, loss 0.529561.
Train: 2018-08-06T01:30:55.737109: step 10074, loss 0.579006.
Train: 2018-08-06T01:30:55.996437: step 10075, loss 0.562501.
Train: 2018-08-06T01:30:56.243753: step 10076, loss 0.603794.
Train: 2018-08-06T01:30:56.505054: step 10077, loss 0.554246.
Train: 2018-08-06T01:30:56.754389: step 10078, loss 0.546.
Train: 2018-08-06T01:30:57.000759: step 10079, loss 0.628521.
Train: 2018-08-06T01:30:57.264060: step 10080, loss 0.570757.
Test: 2018-08-06T01:30:58.534627: step 10080, loss 0.548796.
Train: 2018-08-06T01:30:58.782963: step 10081, loss 0.570758.
Train: 2018-08-06T01:30:59.037283: step 10082, loss 0.562549.
Train: 2018-08-06T01:30:59.292600: step 10083, loss 0.529772.
Train: 2018-08-06T01:30:59.554929: step 10084, loss 0.546187.
Train: 2018-08-06T01:30:59.815237: step 10085, loss 0.554384.
Train: 2018-08-06T01:31:00.061574: step 10086, loss 0.61171.
Train: 2018-08-06T01:31:00.304918: step 10087, loss 0.562588.
Train: 2018-08-06T01:31:00.565214: step 10088, loss 0.627957.
Train: 2018-08-06T01:31:00.822534: step 10089, loss 0.554478.
Train: 2018-08-06T01:31:01.072839: step 10090, loss 0.570783.
Test: 2018-08-06T01:31:02.354411: step 10090, loss 0.548377.
Train: 2018-08-06T01:31:02.595792: step 10091, loss 0.538332.
Train: 2018-08-06T01:31:02.848122: step 10092, loss 0.562691.
Train: 2018-08-06T01:31:03.097455: step 10093, loss 0.619368.
Train: 2018-08-06T01:31:03.346758: step 10094, loss 0.522354.
Train: 2018-08-06T01:31:03.596090: step 10095, loss 0.58695.
Train: 2018-08-06T01:31:03.852406: step 10096, loss 0.538602.
Train: 2018-08-06T01:31:04.098772: step 10097, loss 0.498382.
Train: 2018-08-06T01:31:04.345118: step 10098, loss 0.619165.
Train: 2018-08-06T01:31:04.594421: step 10099, loss 0.522499.
Train: 2018-08-06T01:31:04.847744: step 10100, loss 0.546642.
Test: 2018-08-06T01:31:06.141283: step 10100, loss 0.550944.
Train: 2018-08-06T01:31:07.142026: step 10101, loss 0.570815.
Train: 2018-08-06T01:31:07.390364: step 10102, loss 0.530439.
Train: 2018-08-06T01:31:07.638703: step 10103, loss 0.546543.
Train: 2018-08-06T01:31:07.885036: step 10104, loss 0.505982.
Train: 2018-08-06T01:31:08.131352: step 10105, loss 0.546406.
Train: 2018-08-06T01:31:08.388664: step 10106, loss 0.578927.
Train: 2018-08-06T01:31:08.636017: step 10107, loss 0.603455.
Train: 2018-08-06T01:31:08.894337: step 10108, loss 0.521678.
Train: 2018-08-06T01:31:09.146664: step 10109, loss 0.562565.
Train: 2018-08-06T01:31:09.399959: step 10110, loss 0.554335.
Test: 2018-08-06T01:31:10.677542: step 10110, loss 0.549738.
Train: 2018-08-06T01:31:10.917926: step 10111, loss 0.496715.
Train: 2018-08-06T01:31:11.163271: step 10112, loss 0.512991.
Train: 2018-08-06T01:31:11.422550: step 10113, loss 0.521055.
Train: 2018-08-06T01:31:11.667894: step 10114, loss 0.570762.
Train: 2018-08-06T01:31:11.918257: step 10115, loss 0.503977.
Train: 2018-08-06T01:31:12.169553: step 10116, loss 0.637865.
Train: 2018-08-06T01:31:12.358079: step 10117, loss 0.526531.
Train: 2018-08-06T01:31:12.606383: step 10118, loss 0.503402.
Train: 2018-08-06T01:31:12.866688: step 10119, loss 0.621533.
Train: 2018-08-06T01:31:13.119012: step 10120, loss 0.587755.
Test: 2018-08-06T01:31:14.371663: step 10120, loss 0.54918.
Train: 2018-08-06T01:31:14.617006: step 10121, loss 0.503049.
Train: 2018-08-06T01:31:14.861354: step 10122, loss 0.579322.
Train: 2018-08-06T01:31:15.118695: step 10123, loss 0.604821.
Train: 2018-08-06T01:31:15.371987: step 10124, loss 0.621795.
Train: 2018-08-06T01:31:15.629299: step 10125, loss 0.545397.
Train: 2018-08-06T01:31:15.876668: step 10126, loss 0.528492.
Train: 2018-08-06T01:31:16.127991: step 10127, loss 0.536978.
Train: 2018-08-06T01:31:16.387272: step 10128, loss 0.536986.
Train: 2018-08-06T01:31:16.647607: step 10129, loss 0.520065.
Train: 2018-08-06T01:31:16.904888: step 10130, loss 0.621619.
Test: 2018-08-06T01:31:18.171501: step 10130, loss 0.548619.
Train: 2018-08-06T01:31:18.408892: step 10131, loss 0.503138.
Train: 2018-08-06T01:31:18.658201: step 10132, loss 0.503109.
Train: 2018-08-06T01:31:18.917506: step 10133, loss 0.59626.
Train: 2018-08-06T01:31:19.181799: step 10134, loss 0.596273.
Train: 2018-08-06T01:31:19.425179: step 10135, loss 0.579305.
Train: 2018-08-06T01:31:19.688444: step 10136, loss 0.536954.
Train: 2018-08-06T01:31:19.934786: step 10137, loss 0.621592.
Train: 2018-08-06T01:31:20.191101: step 10138, loss 0.587699.
Train: 2018-08-06T01:31:20.438472: step 10139, loss 0.562374.
Train: 2018-08-06T01:31:20.684780: step 10140, loss 0.570788.
Test: 2018-08-06T01:31:21.950395: step 10140, loss 0.5484.
Train: 2018-08-06T01:31:22.187792: step 10141, loss 0.570779.
Train: 2018-08-06T01:31:22.435129: step 10142, loss 0.528976.
Train: 2018-08-06T01:31:22.695402: step 10143, loss 0.570767.
Train: 2018-08-06T01:31:22.941774: step 10144, loss 0.570763.
Train: 2018-08-06T01:31:23.192074: step 10145, loss 0.612319.
Train: 2018-08-06T01:31:23.444398: step 10146, loss 0.58733.
Train: 2018-08-06T01:31:23.703705: step 10147, loss 0.537723.
Train: 2018-08-06T01:31:23.956030: step 10148, loss 0.496623.
Train: 2018-08-06T01:31:24.209384: step 10149, loss 0.620139.
Train: 2018-08-06T01:31:24.457720: step 10150, loss 0.669311.
Test: 2018-08-06T01:31:25.722307: step 10150, loss 0.549125.
Train: 2018-08-06T01:31:25.974632: step 10151, loss 0.660706.
Train: 2018-08-06T01:31:26.223965: step 10152, loss 0.603284.
Train: 2018-08-06T01:31:26.478285: step 10153, loss 0.506248.
Train: 2018-08-06T01:31:26.727650: step 10154, loss 0.522638.
Train: 2018-08-06T01:31:26.975985: step 10155, loss 0.546832.
Train: 2018-08-06T01:31:27.226286: step 10156, loss 0.578862.
Train: 2018-08-06T01:31:27.473648: step 10157, loss 0.570889.
Train: 2018-08-06T01:31:27.717970: step 10158, loss 0.570906.
Train: 2018-08-06T01:31:27.962347: step 10159, loss 0.499499.
Train: 2018-08-06T01:31:28.214642: step 10160, loss 0.555055.
Test: 2018-08-06T01:31:29.492225: step 10160, loss 0.549001.
Train: 2018-08-06T01:31:29.729620: step 10161, loss 0.539178.
Train: 2018-08-06T01:31:29.974935: step 10162, loss 0.555028.
Train: 2018-08-06T01:31:30.224268: step 10163, loss 0.531143.
Train: 2018-08-06T01:31:30.470609: step 10164, loss 0.531051.
Train: 2018-08-06T01:31:30.718973: step 10165, loss 0.546904.
Train: 2018-08-06T01:31:30.967310: step 10166, loss 0.546816.
Train: 2018-08-06T01:31:31.223625: step 10167, loss 0.562798.
Train: 2018-08-06T01:31:31.468940: step 10168, loss 0.52246.
Train: 2018-08-06T01:31:31.721294: step 10169, loss 0.603158.
Train: 2018-08-06T01:31:31.973588: step 10170, loss 0.5789.
Test: 2018-08-06T01:31:33.246186: step 10170, loss 0.54929.
Train: 2018-08-06T01:31:33.497513: step 10171, loss 0.497699.
Train: 2018-08-06T01:31:33.742857: step 10172, loss 0.627799.
Train: 2018-08-06T01:31:34.000170: step 10173, loss 0.554461.
Train: 2018-08-06T01:31:34.248536: step 10174, loss 0.554437.
Train: 2018-08-06T01:31:34.496872: step 10175, loss 0.603478.
Train: 2018-08-06T01:31:34.751192: step 10176, loss 0.562587.
Train: 2018-08-06T01:31:34.998502: step 10177, loss 0.628035.
Train: 2018-08-06T01:31:35.246871: step 10178, loss 0.538088.
Train: 2018-08-06T01:31:35.493177: step 10179, loss 0.521782.
Train: 2018-08-06T01:31:35.753482: step 10180, loss 0.554435.
Test: 2018-08-06T01:31:37.012114: step 10180, loss 0.548009.
Train: 2018-08-06T01:31:37.266436: step 10181, loss 0.554425.
Train: 2018-08-06T01:31:37.521753: step 10182, loss 0.505352.
Train: 2018-08-06T01:31:37.767121: step 10183, loss 0.546187.
Train: 2018-08-06T01:31:38.014464: step 10184, loss 0.529714.
Train: 2018-08-06T01:31:38.259808: step 10185, loss 0.521376.
Train: 2018-08-06T01:31:38.523104: step 10186, loss 0.603783.
Train: 2018-08-06T01:31:38.772407: step 10187, loss 0.587301.
Train: 2018-08-06T01:31:39.031713: step 10188, loss 0.496229.
Train: 2018-08-06T01:31:39.279083: step 10189, loss 0.603961.
Train: 2018-08-06T01:31:39.538389: step 10190, loss 0.554141.
Test: 2018-08-06T01:31:40.822923: step 10190, loss 0.549125.
Train: 2018-08-06T01:31:41.130133: step 10191, loss 0.587398.
Train: 2018-08-06T01:31:41.376443: step 10192, loss 0.562441.
Train: 2018-08-06T01:31:41.624779: step 10193, loss 0.645663.
Train: 2018-08-06T01:31:41.869149: step 10194, loss 0.579067.
Train: 2018-08-06T01:31:42.120482: step 10195, loss 0.554179.
Train: 2018-08-06T01:31:42.368788: step 10196, loss 0.512837.
Train: 2018-08-06T01:31:42.621114: step 10197, loss 0.554217.
Train: 2018-08-06T01:31:42.868483: step 10198, loss 0.612087.
Train: 2018-08-06T01:31:43.117814: step 10199, loss 0.55425.
Train: 2018-08-06T01:31:43.367149: step 10200, loss 0.562514.
Test: 2018-08-06T01:31:44.633732: step 10200, loss 0.549345.
Train: 2018-08-06T01:31:45.543834: step 10201, loss 0.496665.
Train: 2018-08-06T01:31:45.791198: step 10202, loss 0.463694.
Train: 2018-08-06T01:31:46.038542: step 10203, loss 0.5625.
Train: 2018-08-06T01:31:46.288867: step 10204, loss 0.521105.
Train: 2018-08-06T01:31:46.544168: step 10205, loss 0.620552.
Train: 2018-08-06T01:31:46.790501: step 10206, loss 0.579068.
Train: 2018-08-06T01:31:47.049807: step 10207, loss 0.537506.
Train: 2018-08-06T01:31:47.312136: step 10208, loss 0.612371.
Train: 2018-08-06T01:31:47.564467: step 10209, loss 0.470934.
Train: 2018-08-06T01:31:47.817785: step 10210, loss 0.529101.
Test: 2018-08-06T01:31:49.091347: step 10210, loss 0.54828.
Train: 2018-08-06T01:31:49.329734: step 10211, loss 0.503965.
Train: 2018-08-06T01:31:49.582068: step 10212, loss 0.537272.
Train: 2018-08-06T01:31:49.826412: step 10213, loss 0.562385.
Train: 2018-08-06T01:31:50.087713: step 10214, loss 0.570798.
Train: 2018-08-06T01:31:50.332029: step 10215, loss 0.587696.
Train: 2018-08-06T01:31:50.582385: step 10216, loss 0.562361.
Train: 2018-08-06T01:31:50.843692: step 10217, loss 0.528515.
Train: 2018-08-06T01:31:51.101006: step 10218, loss 0.528468.
Train: 2018-08-06T01:31:51.349339: step 10219, loss 0.53689.
Train: 2018-08-06T01:31:51.605624: step 10220, loss 0.587853.
Test: 2018-08-06T01:31:52.867249: step 10220, loss 0.5495.
Train: 2018-08-06T01:31:53.104614: step 10221, loss 0.579368.
Train: 2018-08-06T01:31:53.346997: step 10222, loss 0.664496.
Train: 2018-08-06T01:31:53.597327: step 10223, loss 0.519876.
Train: 2018-08-06T01:31:53.844666: step 10224, loss 0.570834.
Train: 2018-08-06T01:31:54.092975: step 10225, loss 0.587767.
Train: 2018-08-06T01:31:54.341333: step 10226, loss 0.520096.
Train: 2018-08-06T01:31:54.590641: step 10227, loss 0.537035.
Train: 2018-08-06T01:31:54.841969: step 10228, loss 0.537055.
Train: 2018-08-06T01:31:55.090332: step 10229, loss 0.494882.
Train: 2018-08-06T01:31:55.339668: step 10230, loss 0.553919.
Test: 2018-08-06T01:31:56.615225: step 10230, loss 0.548631.
Train: 2018-08-06T01:31:56.856581: step 10231, loss 0.562362.
Train: 2018-08-06T01:31:57.108939: step 10232, loss 0.613118.
Train: 2018-08-06T01:31:57.354280: step 10233, loss 0.545452.
Train: 2018-08-06T01:31:57.598621: step 10234, loss 0.54546.
Train: 2018-08-06T01:31:57.847955: step 10235, loss 0.587711.
Train: 2018-08-06T01:31:58.108234: step 10236, loss 0.638346.
Train: 2018-08-06T01:31:58.366543: step 10237, loss 0.612896.
Train: 2018-08-06T01:31:58.614877: step 10238, loss 0.562393.
Train: 2018-08-06T01:31:58.871193: step 10239, loss 0.562411.
Train: 2018-08-06T01:31:59.117559: step 10240, loss 0.63743.
Test: 2018-08-06T01:32:00.389133: step 10240, loss 0.54804.
Train: 2018-08-06T01:32:00.628494: step 10241, loss 0.496112.
Train: 2018-08-06T01:32:00.875857: step 10242, loss 0.537674.
Train: 2018-08-06T01:32:01.123195: step 10243, loss 0.620277.
Train: 2018-08-06T01:32:01.369539: step 10244, loss 0.587212.
Train: 2018-08-06T01:32:01.615883: step 10245, loss 0.554366.
Train: 2018-08-06T01:32:01.865235: step 10246, loss 0.627977.
Train: 2018-08-06T01:32:02.115516: step 10247, loss 0.587053.
Train: 2018-08-06T01:32:02.366878: step 10248, loss 0.562695.
Train: 2018-08-06T01:32:02.630174: step 10249, loss 0.554678.
Train: 2018-08-06T01:32:02.890468: step 10250, loss 0.570832.
Test: 2018-08-06T01:32:04.163040: step 10250, loss 0.550001.
Train: 2018-08-06T01:32:04.401403: step 10251, loss 0.530772.
Train: 2018-08-06T01:32:04.648785: step 10252, loss 0.562863.
Train: 2018-08-06T01:32:04.895113: step 10253, loss 0.570875.
Train: 2018-08-06T01:32:05.144447: step 10254, loss 0.538992.
Train: 2018-08-06T01:32:05.391754: step 10255, loss 0.578859.
Train: 2018-08-06T01:32:05.640091: step 10256, loss 0.539058.
Train: 2018-08-06T01:32:05.890422: step 10257, loss 0.515185.
Train: 2018-08-06T01:32:06.144741: step 10258, loss 0.507141.
Train: 2018-08-06T01:32:06.391114: step 10259, loss 0.554894.
Train: 2018-08-06T01:32:06.638421: step 10260, loss 0.650956.
Test: 2018-08-06T01:32:07.907027: step 10260, loss 0.5478.
Train: 2018-08-06T01:32:08.145390: step 10261, loss 0.554829.
Train: 2018-08-06T01:32:08.391762: step 10262, loss 0.578867.
Train: 2018-08-06T01:32:08.643084: step 10263, loss 0.578867.
Train: 2018-08-06T01:32:08.892418: step 10264, loss 0.546798.
Train: 2018-08-06T01:32:09.153727: step 10265, loss 0.602928.
Train: 2018-08-06T01:32:09.406019: step 10266, loss 0.506727.
Train: 2018-08-06T01:32:09.652392: step 10267, loss 0.586891.
Train: 2018-08-06T01:32:09.845842: step 10268, loss 0.494329.
Train: 2018-08-06T01:32:10.097201: step 10269, loss 0.538661.
Train: 2018-08-06T01:32:10.345539: step 10270, loss 0.54663.
Test: 2018-08-06T01:32:11.614113: step 10270, loss 0.550149.
Train: 2018-08-06T01:32:11.850506: step 10271, loss 0.546552.
Train: 2018-08-06T01:32:12.106827: step 10272, loss 0.578901.
Train: 2018-08-06T01:32:12.353138: step 10273, loss 0.554534.
Train: 2018-08-06T01:32:12.601499: step 10274, loss 0.570778.
Train: 2018-08-06T01:32:12.854847: step 10275, loss 0.578931.
Train: 2018-08-06T01:32:13.101158: step 10276, loss 0.595273.
Train: 2018-08-06T01:32:13.361460: step 10277, loss 0.562598.
Train: 2018-08-06T01:32:13.617777: step 10278, loss 0.497214.
Train: 2018-08-06T01:32:13.863149: step 10279, loss 0.578952.
Train: 2018-08-06T01:32:14.111455: step 10280, loss 0.660929.
Test: 2018-08-06T01:32:15.383055: step 10280, loss 0.547775.
Train: 2018-08-06T01:32:15.626438: step 10281, loss 0.636258.
Train: 2018-08-06T01:32:15.874775: step 10282, loss 0.554445.
Train: 2018-08-06T01:32:16.137073: step 10283, loss 0.619639.
Train: 2018-08-06T01:32:16.385416: step 10284, loss 0.627594.
Train: 2018-08-06T01:32:16.636711: step 10285, loss 0.498116.
Train: 2018-08-06T01:32:16.883087: step 10286, loss 0.603045.
Train: 2018-08-06T01:32:17.135392: step 10287, loss 0.554782.
Train: 2018-08-06T01:32:17.384711: step 10288, loss 0.578865.
Train: 2018-08-06T01:32:17.634045: step 10289, loss 0.530945.
Train: 2018-08-06T01:32:17.879420: step 10290, loss 0.523046.
Test: 2018-08-06T01:32:19.146998: step 10290, loss 0.549789.
Train: 2018-08-06T01:32:19.386359: step 10291, loss 0.475237.
Train: 2018-08-06T01:32:19.629733: step 10292, loss 0.570874.
Train: 2018-08-06T01:32:19.876074: step 10293, loss 0.610865.
Train: 2018-08-06T01:32:20.126411: step 10294, loss 0.546848.
Train: 2018-08-06T01:32:20.370726: step 10295, loss 0.530798.
Train: 2018-08-06T01:32:20.621056: step 10296, loss 0.538745.
Train: 2018-08-06T01:32:20.870415: step 10297, loss 0.514535.
Train: 2018-08-06T01:32:21.119749: step 10298, loss 0.506271.
Train: 2018-08-06T01:32:21.370053: step 10299, loss 0.570796.
Train: 2018-08-06T01:32:21.635345: step 10300, loss 0.546386.
Test: 2018-08-06T01:32:22.899961: step 10300, loss 0.548226.
Train: 2018-08-06T01:32:23.916557: step 10301, loss 0.538123.
Train: 2018-08-06T01:32:24.171906: step 10302, loss 0.570763.
Train: 2018-08-06T01:32:24.416236: step 10303, loss 0.570759.
Train: 2018-08-06T01:32:24.664555: step 10304, loss 0.521301.
Train: 2018-08-06T01:32:24.909900: step 10305, loss 0.612106.
Train: 2018-08-06T01:32:25.159232: step 10306, loss 0.570757.
Train: 2018-08-06T01:32:25.408565: step 10307, loss 0.537576.
Train: 2018-08-06T01:32:25.656925: step 10308, loss 0.562451.
Train: 2018-08-06T01:32:25.903243: step 10309, loss 0.570762.
Train: 2018-08-06T01:32:26.149614: step 10310, loss 0.562434.
Test: 2018-08-06T01:32:27.429161: step 10310, loss 0.548893.
Train: 2018-08-06T01:32:27.669518: step 10311, loss 0.504084.
Train: 2018-08-06T01:32:27.922866: step 10312, loss 0.545716.
Train: 2018-08-06T01:32:28.169183: step 10313, loss 0.570775.
Train: 2018-08-06T01:32:28.432478: step 10314, loss 0.562399.
Train: 2018-08-06T01:32:28.681810: step 10315, loss 0.554005.
Train: 2018-08-06T01:32:28.931144: step 10316, loss 0.587581.
Train: 2018-08-06T01:32:29.181499: step 10317, loss 0.537187.
Train: 2018-08-06T01:32:29.432803: step 10318, loss 0.579194.
Train: 2018-08-06T01:32:29.681164: step 10319, loss 0.528761.
Train: 2018-08-06T01:32:29.927481: step 10320, loss 0.587613.
Test: 2018-08-06T01:32:31.240967: step 10320, loss 0.548724.
Train: 2018-08-06T01:32:31.480327: step 10321, loss 0.58761.
Train: 2018-08-06T01:32:31.729659: step 10322, loss 0.587592.
Train: 2018-08-06T01:32:31.993955: step 10323, loss 0.604342.
Train: 2018-08-06T01:32:32.249297: step 10324, loss 0.503817.
Train: 2018-08-06T01:32:32.497606: step 10325, loss 0.528965.
Train: 2018-08-06T01:32:32.740955: step 10326, loss 0.503899.
Train: 2018-08-06T01:32:33.000263: step 10327, loss 0.604241.
Train: 2018-08-06T01:32:33.246603: step 10328, loss 0.554043.
Train: 2018-08-06T01:32:33.496980: step 10329, loss 0.528954.
Train: 2018-08-06T01:32:33.751254: step 10330, loss 0.537302.
Test: 2018-08-06T01:32:35.013876: step 10330, loss 0.548222.
Train: 2018-08-06T01:32:35.254233: step 10331, loss 0.537276.
Train: 2018-08-06T01:32:35.504589: step 10332, loss 0.604322.
Train: 2018-08-06T01:32:35.768859: step 10333, loss 0.646248.
Train: 2018-08-06T01:32:36.020185: step 10334, loss 0.528932.
Train: 2018-08-06T01:32:36.271539: step 10335, loss 0.654356.
Train: 2018-08-06T01:32:36.520846: step 10336, loss 0.562433.
Train: 2018-08-06T01:32:36.772210: step 10337, loss 0.520925.
Train: 2018-08-06T01:32:37.027491: step 10338, loss 0.612207.
Train: 2018-08-06T01:32:37.279843: step 10339, loss 0.562491.
Train: 2018-08-06T01:32:37.530173: step 10340, loss 0.529542.
Test: 2018-08-06T01:32:38.780803: step 10340, loss 0.548787.
Train: 2018-08-06T01:32:39.035154: step 10341, loss 0.603672.
Train: 2018-08-06T01:32:39.288471: step 10342, loss 0.537932.
Train: 2018-08-06T01:32:39.549747: step 10343, loss 0.537994.
Train: 2018-08-06T01:32:39.802097: step 10344, loss 0.554398.
Train: 2018-08-06T01:32:40.060412: step 10345, loss 0.587122.
Train: 2018-08-06T01:32:40.303754: step 10346, loss 0.513603.
Train: 2018-08-06T01:32:40.560071: step 10347, loss 0.529938.
Train: 2018-08-06T01:32:40.814391: step 10348, loss 0.529906.
Train: 2018-08-06T01:32:41.064721: step 10349, loss 0.529845.
Train: 2018-08-06T01:32:41.309076: step 10350, loss 0.578962.
Test: 2018-08-06T01:32:42.604577: step 10350, loss 0.548832.
Train: 2018-08-06T01:32:42.849921: step 10351, loss 0.595399.
Train: 2018-08-06T01:32:43.095289: step 10352, loss 0.546111.
Train: 2018-08-06T01:32:43.342629: step 10353, loss 0.578981.
Train: 2018-08-06T01:32:43.602907: step 10354, loss 0.529636.
Train: 2018-08-06T01:32:43.855234: step 10355, loss 0.578987.
Train: 2018-08-06T01:32:44.100609: step 10356, loss 0.644885.
Train: 2018-08-06T01:32:44.348942: step 10357, loss 0.537866.
Train: 2018-08-06T01:32:44.607251: step 10358, loss 0.595407.
Train: 2018-08-06T01:32:44.856554: step 10359, loss 0.521549.
Train: 2018-08-06T01:32:45.101897: step 10360, loss 0.587157.
Test: 2018-08-06T01:32:46.395438: step 10360, loss 0.548711.
Train: 2018-08-06T01:32:46.637790: step 10361, loss 0.587143.
Train: 2018-08-06T01:32:46.885131: step 10362, loss 0.554415.
Train: 2018-08-06T01:32:47.130506: step 10363, loss 0.513609.
Train: 2018-08-06T01:32:47.373853: step 10364, loss 0.619768.
Train: 2018-08-06T01:32:47.629163: step 10365, loss 0.529996.
Train: 2018-08-06T01:32:47.875511: step 10366, loss 0.530014.
Train: 2018-08-06T01:32:48.135785: step 10367, loss 0.505532.
Train: 2018-08-06T01:32:48.393122: step 10368, loss 0.5626.
Train: 2018-08-06T01:32:48.639468: step 10369, loss 0.554402.
Train: 2018-08-06T01:32:48.887773: step 10370, loss 0.595345.
Test: 2018-08-06T01:32:50.145409: step 10370, loss 0.549059.
Train: 2018-08-06T01:32:50.380806: step 10371, loss 0.578961.
Train: 2018-08-06T01:32:50.626125: step 10372, loss 0.611762.
Train: 2018-08-06T01:32:50.873463: step 10373, loss 0.587147.
Train: 2018-08-06T01:32:51.125819: step 10374, loss 0.587123.
Train: 2018-08-06T01:32:51.384111: step 10375, loss 0.538125.
Train: 2018-08-06T01:32:51.660403: step 10376, loss 0.530016.
Train: 2018-08-06T01:32:51.914721: step 10377, loss 0.570776.
Train: 2018-08-06T01:32:52.172063: step 10378, loss 0.587067.
Train: 2018-08-06T01:32:52.419396: step 10379, loss 0.562644.
Train: 2018-08-06T01:32:52.668730: step 10380, loss 0.513879.
Test: 2018-08-06T01:32:53.945291: step 10380, loss 0.547946.
Train: 2018-08-06T01:32:54.182687: step 10381, loss 0.481328.
Train: 2018-08-06T01:32:54.429010: step 10382, loss 0.53002.
Train: 2018-08-06T01:32:54.676350: step 10383, loss 0.61164.
Train: 2018-08-06T01:32:54.927675: step 10384, loss 0.562576.
Train: 2018-08-06T01:32:55.176036: step 10385, loss 0.587149.
Train: 2018-08-06T01:32:55.423350: step 10386, loss 0.595374.
Train: 2018-08-06T01:32:55.677669: step 10387, loss 0.56256.
Train: 2018-08-06T01:32:55.929022: step 10388, loss 0.570752.
Train: 2018-08-06T01:32:56.178332: step 10389, loss 0.57078.
Train: 2018-08-06T01:32:56.422678: step 10390, loss 0.562575.
Test: 2018-08-06T01:32:57.690288: step 10390, loss 0.548742.
Train: 2018-08-06T01:32:57.932640: step 10391, loss 0.546228.
Train: 2018-08-06T01:32:58.183998: step 10392, loss 0.56259.
Train: 2018-08-06T01:32:58.429342: step 10393, loss 0.521732.
Train: 2018-08-06T01:32:58.677672: step 10394, loss 0.578947.
Train: 2018-08-06T01:32:58.925984: step 10395, loss 0.538054.
Train: 2018-08-06T01:32:59.177343: step 10396, loss 0.578942.
Train: 2018-08-06T01:32:59.439609: step 10397, loss 0.578953.
Train: 2018-08-06T01:32:59.687977: step 10398, loss 0.603529.
Train: 2018-08-06T01:32:59.937305: step 10399, loss 0.603493.
Train: 2018-08-06T01:33:00.185613: step 10400, loss 0.513609.
Test: 2018-08-06T01:33:01.446243: step 10400, loss 0.549358.
Train: 2018-08-06T01:33:02.340477: step 10401, loss 0.521806.
Train: 2018-08-06T01:33:02.589780: step 10402, loss 0.55444.
Train: 2018-08-06T01:33:02.844101: step 10403, loss 0.60344.
Train: 2018-08-06T01:33:03.088478: step 10404, loss 0.529931.
Train: 2018-08-06T01:33:03.341770: step 10405, loss 0.603489.
Train: 2018-08-06T01:33:03.586146: step 10406, loss 0.570807.
Train: 2018-08-06T01:33:03.839439: step 10407, loss 0.546289.
Train: 2018-08-06T01:33:04.088797: step 10408, loss 0.611537.
Train: 2018-08-06T01:33:04.335147: step 10409, loss 0.554497.
Train: 2018-08-06T01:33:04.580488: step 10410, loss 0.538262.
Test: 2018-08-06T01:33:05.840088: step 10410, loss 0.549647.
Train: 2018-08-06T01:33:06.076468: step 10411, loss 0.554531.
Train: 2018-08-06T01:33:06.322798: step 10412, loss 0.57891.
Train: 2018-08-06T01:33:06.573153: step 10413, loss 0.554545.
Train: 2018-08-06T01:33:06.821492: step 10414, loss 0.546431.
Train: 2018-08-06T01:33:07.082791: step 10415, loss 0.546426.
Train: 2018-08-06T01:33:07.333095: step 10416, loss 0.611411.
Train: 2018-08-06T01:33:07.581431: step 10417, loss 0.570787.
Train: 2018-08-06T01:33:07.825809: step 10418, loss 0.554563.
Train: 2018-08-06T01:33:08.016283: step 10419, loss 0.493473.
Train: 2018-08-06T01:33:08.267630: step 10420, loss 0.505819.
Test: 2018-08-06T01:33:09.532214: step 10420, loss 0.548289.
Train: 2018-08-06T01:33:09.778582: step 10421, loss 0.570778.
Train: 2018-08-06T01:33:10.026891: step 10422, loss 0.51366.
Train: 2018-08-06T01:33:10.280240: step 10423, loss 0.636237.
Train: 2018-08-06T01:33:10.542538: step 10424, loss 0.456064.
Train: 2018-08-06T01:33:10.791871: step 10425, loss 0.505.
Train: 2018-08-06T01:33:11.055175: step 10426, loss 0.579005.
Train: 2018-08-06T01:33:11.313479: step 10427, loss 0.579064.
Train: 2018-08-06T01:33:11.565809: step 10428, loss 0.562407.
Train: 2018-08-06T01:33:11.817130: step 10429, loss 0.595711.
Train: 2018-08-06T01:33:12.066475: step 10430, loss 0.604218.
Test: 2018-08-06T01:33:13.321082: step 10430, loss 0.547728.
Train: 2018-08-06T01:33:13.571413: step 10431, loss 0.579184.
Train: 2018-08-06T01:33:13.816787: step 10432, loss 0.604078.
Train: 2018-08-06T01:33:14.064119: step 10433, loss 0.512558.
Train: 2018-08-06T01:33:14.309464: step 10434, loss 0.495951.
Train: 2018-08-06T01:33:14.558803: step 10435, loss 0.570764.
Train: 2018-08-06T01:33:14.807138: step 10436, loss 0.562447.
Train: 2018-08-06T01:33:15.053476: step 10437, loss 0.479087.
Train: 2018-08-06T01:33:15.298823: step 10438, loss 0.55407.
Train: 2018-08-06T01:33:15.547159: step 10439, loss 0.554046.
Train: 2018-08-06T01:33:15.794500: step 10440, loss 0.461752.
Test: 2018-08-06T01:33:17.073048: step 10440, loss 0.547728.
Train: 2018-08-06T01:33:17.313406: step 10441, loss 0.537122.
Train: 2018-08-06T01:33:17.560743: step 10442, loss 0.58773.
Train: 2018-08-06T01:33:17.811074: step 10443, loss 0.528446.
Train: 2018-08-06T01:33:18.058446: step 10444, loss 0.570867.
Train: 2018-08-06T01:33:18.307779: step 10445, loss 0.494177.
Train: 2018-08-06T01:33:18.556113: step 10446, loss 0.511066.
Train: 2018-08-06T01:33:18.804418: step 10447, loss 0.562313.
Train: 2018-08-06T01:33:19.051786: step 10448, loss 0.545132.
Train: 2018-08-06T01:33:19.299121: step 10449, loss 0.570977.
Train: 2018-08-06T01:33:19.555437: step 10450, loss 0.579644.
Test: 2018-08-06T01:33:20.815040: step 10450, loss 0.549675.
Train: 2018-08-06T01:33:21.060417: step 10451, loss 0.493064.
Train: 2018-08-06T01:33:21.307747: step 10452, loss 0.614431.
Train: 2018-08-06T01:33:21.557082: step 10453, loss 0.544979.
Train: 2018-08-06T01:33:21.807418: step 10454, loss 0.54497.
Train: 2018-08-06T01:33:22.053752: step 10455, loss 0.553658.
Train: 2018-08-06T01:33:22.299071: step 10456, loss 0.510144.
Train: 2018-08-06T01:33:22.552427: step 10457, loss 0.562371.
Train: 2018-08-06T01:33:22.798774: step 10458, loss 0.579806.
Train: 2018-08-06T01:33:23.045076: step 10459, loss 0.5362.
Train: 2018-08-06T01:33:23.295432: step 10460, loss 0.518761.
Test: 2018-08-06T01:33:24.548056: step 10460, loss 0.547588.
Train: 2018-08-06T01:33:24.796419: step 10461, loss 0.597362.
Train: 2018-08-06T01:33:25.042766: step 10462, loss 0.52749.
Train: 2018-08-06T01:33:25.303063: step 10463, loss 0.536202.
Train: 2018-08-06T01:33:25.547385: step 10464, loss 0.632137.
Train: 2018-08-06T01:33:25.797716: step 10465, loss 0.47654.
Train: 2018-08-06T01:33:26.060014: step 10466, loss 0.562332.
Train: 2018-08-06T01:33:26.325329: step 10467, loss 0.527771.
Train: 2018-08-06T01:33:26.570678: step 10468, loss 0.492678.
Train: 2018-08-06T01:33:26.818012: step 10469, loss 0.501311.
Train: 2018-08-06T01:33:27.071337: step 10470, loss 0.641075.
Test: 2018-08-06T01:33:28.343905: step 10470, loss 0.54717.
Train: 2018-08-06T01:33:28.597229: step 10471, loss 0.579856.
Train: 2018-08-06T01:33:28.850584: step 10472, loss 0.59731.
Train: 2018-08-06T01:33:29.098917: step 10473, loss 0.544912.
Train: 2018-08-06T01:33:29.350248: step 10474, loss 0.553649.
Train: 2018-08-06T01:33:29.601542: step 10475, loss 0.605812.
Train: 2018-08-06T01:33:29.862874: step 10476, loss 0.545009.
Train: 2018-08-06T01:33:30.122175: step 10477, loss 0.545043.
Train: 2018-08-06T01:33:30.369523: step 10478, loss 0.536442.
Train: 2018-08-06T01:33:30.617870: step 10479, loss 0.510624.
Train: 2018-08-06T01:33:30.873172: step 10480, loss 0.614021.
Test: 2018-08-06T01:33:32.125791: step 10480, loss 0.546987.
Train: 2018-08-06T01:33:32.361162: step 10481, loss 0.536542.
Train: 2018-08-06T01:33:32.605509: step 10482, loss 0.510822.
Train: 2018-08-06T01:33:32.851880: step 10483, loss 0.613827.
Train: 2018-08-06T01:33:33.107198: step 10484, loss 0.588042.
Train: 2018-08-06T01:33:33.354530: step 10485, loss 0.545245.
Train: 2018-08-06T01:33:33.603869: step 10486, loss 0.664685.
Train: 2018-08-06T01:33:33.853200: step 10487, loss 0.596317.
Train: 2018-08-06T01:33:34.104530: step 10488, loss 0.553911.
Train: 2018-08-06T01:33:34.355858: step 10489, loss 0.553963.
Train: 2018-08-06T01:33:34.604163: step 10490, loss 0.562405.
Test: 2018-08-06T01:33:35.859805: step 10490, loss 0.548472.
Train: 2018-08-06T01:33:36.120109: step 10491, loss 0.587472.
Train: 2018-08-06T01:33:36.365453: step 10492, loss 0.595714.
Train: 2018-08-06T01:33:36.610797: step 10493, loss 0.545916.
Train: 2018-08-06T01:33:36.862150: step 10494, loss 0.554258.
Train: 2018-08-06T01:33:37.110486: step 10495, loss 0.521426.
Train: 2018-08-06T01:33:37.354837: step 10496, loss 0.603583.
Train: 2018-08-06T01:33:37.613118: step 10497, loss 0.546219.
Train: 2018-08-06T01:33:37.864445: step 10498, loss 0.513619.
Train: 2018-08-06T01:33:38.109789: step 10499, loss 0.603404.
Train: 2018-08-06T01:33:38.359121: step 10500, loss 0.473051.
Test: 2018-08-06T01:33:39.618752: step 10500, loss 0.548643.
Train: 2018-08-06T01:33:40.539507: step 10501, loss 0.546329.
Train: 2018-08-06T01:33:40.789836: step 10502, loss 0.587086.
Train: 2018-08-06T01:33:41.040192: step 10503, loss 0.595249.
Train: 2018-08-06T01:33:41.300472: step 10504, loss 0.611545.
Train: 2018-08-06T01:33:41.546813: step 10505, loss 0.562638.
Train: 2018-08-06T01:33:41.796146: step 10506, loss 0.595169.
Train: 2018-08-06T01:33:42.044482: step 10507, loss 0.554568.
Train: 2018-08-06T01:33:42.293845: step 10508, loss 0.611288.
Train: 2018-08-06T01:33:42.543149: step 10509, loss 0.619269.
Train: 2018-08-06T01:33:42.792482: step 10510, loss 0.570828.
Test: 2018-08-06T01:33:44.066075: step 10510, loss 0.550176.
Train: 2018-08-06T01:33:44.371259: step 10511, loss 0.522737.
Train: 2018-08-06T01:33:44.625604: step 10512, loss 0.626877.
Train: 2018-08-06T01:33:44.872917: step 10513, loss 0.538983.
Train: 2018-08-06T01:33:45.123252: step 10514, loss 0.586815.
Train: 2018-08-06T01:33:45.369616: step 10515, loss 0.610605.
Train: 2018-08-06T01:33:45.616951: step 10516, loss 0.586772.
Train: 2018-08-06T01:33:45.872245: step 10517, loss 0.578864.
Train: 2018-08-06T01:33:46.126565: step 10518, loss 0.610296.
Train: 2018-08-06T01:33:46.384905: step 10519, loss 0.547582.
Train: 2018-08-06T01:33:46.629251: step 10520, loss 0.532083.
Test: 2018-08-06T01:33:47.902813: step 10520, loss 0.550493.
Train: 2018-08-06T01:33:48.154142: step 10521, loss 0.594477.
Train: 2018-08-06T01:33:48.405469: step 10522, loss 0.586677.
Train: 2018-08-06T01:33:48.654802: step 10523, loss 0.540143.
Train: 2018-08-06T01:33:48.910119: step 10524, loss 0.555686.
Train: 2018-08-06T01:33:49.157460: step 10525, loss 0.516985.
Train: 2018-08-06T01:33:49.421777: step 10526, loss 0.563421.
Train: 2018-08-06T01:33:49.671099: step 10527, loss 0.524597.
Train: 2018-08-06T01:33:49.919453: step 10528, loss 0.571125.
Train: 2018-08-06T01:33:50.164764: step 10529, loss 0.58669.
Train: 2018-08-06T01:33:50.412103: step 10530, loss 0.563271.
Test: 2018-08-06T01:33:51.668742: step 10530, loss 0.549638.
Train: 2018-08-06T01:33:51.907136: step 10531, loss 0.539778.
Train: 2018-08-06T01:33:52.159458: step 10532, loss 0.618078.
Train: 2018-08-06T01:33:52.407766: step 10533, loss 0.571026.
Train: 2018-08-06T01:33:52.659095: step 10534, loss 0.500342.
Train: 2018-08-06T01:33:52.906434: step 10535, loss 0.523759.
Train: 2018-08-06T01:33:53.156791: step 10536, loss 0.515662.
Train: 2018-08-06T01:33:53.407124: step 10537, loss 0.499502.
Train: 2018-08-06T01:33:53.657454: step 10538, loss 0.57088.
Train: 2018-08-06T01:33:53.915733: step 10539, loss 0.538756.
Train: 2018-08-06T01:33:54.160079: step 10540, loss 0.55469.
Test: 2018-08-06T01:33:55.428686: step 10540, loss 0.549342.
Train: 2018-08-06T01:33:55.667049: step 10541, loss 0.595108.
Train: 2018-08-06T01:33:55.919400: step 10542, loss 0.530112.
Train: 2018-08-06T01:33:56.179680: step 10543, loss 0.57077.
Train: 2018-08-06T01:33:56.438985: step 10544, loss 0.562568.
Train: 2018-08-06T01:33:56.687346: step 10545, loss 0.570759.
Train: 2018-08-06T01:33:56.930670: step 10546, loss 0.620206.
Train: 2018-08-06T01:33:57.180003: step 10547, loss 0.579005.
Train: 2018-08-06T01:33:57.427373: step 10548, loss 0.628516.
Train: 2018-08-06T01:33:57.686673: step 10549, loss 0.578997.
Train: 2018-08-06T01:33:57.933988: step 10550, loss 0.570758.
Test: 2018-08-06T01:33:59.189630: step 10550, loss 0.549396.
Train: 2018-08-06T01:33:59.431981: step 10551, loss 0.652893.
Train: 2018-08-06T01:33:59.682342: step 10552, loss 0.619864.
Train: 2018-08-06T01:33:59.933638: step 10553, loss 0.530055.
Train: 2018-08-06T01:34:00.182974: step 10554, loss 0.595138.
Train: 2018-08-06T01:34:00.430311: step 10555, loss 0.554635.
Train: 2018-08-06T01:34:00.679675: step 10556, loss 0.554704.
Train: 2018-08-06T01:34:00.928977: step 10557, loss 0.482434.
Train: 2018-08-06T01:34:01.176317: step 10558, loss 0.619038.
Train: 2018-08-06T01:34:01.423685: step 10559, loss 0.626988.
Train: 2018-08-06T01:34:01.676011: step 10560, loss 0.538881.
Test: 2018-08-06T01:34:02.964534: step 10560, loss 0.549388.
Train: 2018-08-06T01:34:03.201898: step 10561, loss 0.57088.
Train: 2018-08-06T01:34:03.448241: step 10562, loss 0.682409.
Train: 2018-08-06T01:34:03.697604: step 10563, loss 0.547142.
Train: 2018-08-06T01:34:03.950895: step 10564, loss 0.539356.
Train: 2018-08-06T01:34:04.204247: step 10565, loss 0.531574.
Train: 2018-08-06T01:34:04.454581: step 10566, loss 0.570995.
Train: 2018-08-06T01:34:04.705891: step 10567, loss 0.571005.
Train: 2018-08-06T01:34:04.951253: step 10568, loss 0.602438.
Train: 2018-08-06T01:34:05.198567: step 10569, loss 0.52398.
Train: 2018-08-06T01:34:05.396030: step 10570, loss 0.663544.
Test: 2018-08-06T01:34:06.677603: step 10570, loss 0.549828.
Train: 2018-08-06T01:34:06.920954: step 10571, loss 0.563246.
Train: 2018-08-06T01:34:07.166325: step 10572, loss 0.586691.
Train: 2018-08-06T01:34:07.416628: step 10573, loss 0.586699.
Train: 2018-08-06T01:34:07.672942: step 10574, loss 0.532322.
Train: 2018-08-06T01:34:07.919315: step 10575, loss 0.609939.
Train: 2018-08-06T01:34:08.165624: step 10576, loss 0.586666.
Train: 2018-08-06T01:34:08.414958: step 10577, loss 0.602108.
Train: 2018-08-06T01:34:08.662297: step 10578, loss 0.548137.
Train: 2018-08-06T01:34:08.909660: step 10579, loss 0.64818.
Train: 2018-08-06T01:34:09.158968: step 10580, loss 0.563655.
Test: 2018-08-06T01:34:10.438545: step 10580, loss 0.551304.
Train: 2018-08-06T01:34:10.676933: step 10581, loss 0.548431.
Train: 2018-08-06T01:34:10.926242: step 10582, loss 0.517975.
Train: 2018-08-06T01:34:11.171610: step 10583, loss 0.60954.
Train: 2018-08-06T01:34:11.420931: step 10584, loss 0.594272.
Train: 2018-08-06T01:34:11.682243: step 10585, loss 0.556187.
Train: 2018-08-06T01:34:11.930580: step 10586, loss 0.540979.
Train: 2018-08-06T01:34:12.174927: step 10587, loss 0.594264.
Train: 2018-08-06T01:34:12.419278: step 10588, loss 0.54094.
Train: 2018-08-06T01:34:12.668612: step 10589, loss 0.533264.
Train: 2018-08-06T01:34:12.916917: step 10590, loss 0.571386.
Test: 2018-08-06T01:34:14.188517: step 10590, loss 0.551662.
Train: 2018-08-06T01:34:14.426905: step 10591, loss 0.555989.
Train: 2018-08-06T01:34:14.674244: step 10592, loss 0.632884.
Train: 2018-08-06T01:34:14.919590: step 10593, loss 0.532832.
Train: 2018-08-06T01:34:15.164931: step 10594, loss 0.563551.
Train: 2018-08-06T01:34:15.423216: step 10595, loss 0.501837.
Train: 2018-08-06T01:34:15.668584: step 10596, loss 0.555621.
Train: 2018-08-06T01:34:15.918889: step 10597, loss 0.469857.
Train: 2018-08-06T01:34:16.163266: step 10598, loss 0.563331.
Train: 2018-08-06T01:34:16.408579: step 10599, loss 0.538855.
Train: 2018-08-06T01:34:16.671876: step 10600, loss 0.579423.
Test: 2018-08-06T01:34:17.941480: step 10600, loss 0.548631.
Train: 2018-08-06T01:34:18.843937: step 10601, loss 0.561919.
Train: 2018-08-06T01:34:19.088285: step 10602, loss 0.488451.
Train: 2018-08-06T01:34:19.338587: step 10603, loss 0.568295.
Train: 2018-08-06T01:34:19.586946: step 10604, loss 0.505355.
Train: 2018-08-06T01:34:19.830301: step 10605, loss 0.588035.
Train: 2018-08-06T01:34:20.083595: step 10606, loss 0.562586.
Train: 2018-08-06T01:34:20.328968: step 10607, loss 0.538071.
Train: 2018-08-06T01:34:20.575307: step 10608, loss 0.530348.
Train: 2018-08-06T01:34:20.831592: step 10609, loss 0.533625.
Train: 2018-08-06T01:34:21.082921: step 10610, loss 0.572848.
Test: 2018-08-06T01:34:22.345544: step 10610, loss 0.547453.
Train: 2018-08-06T01:34:22.597894: step 10611, loss 0.518592.
Train: 2018-08-06T01:34:22.849226: step 10612, loss 0.575.
Train: 2018-08-06T01:34:23.100525: step 10613, loss 0.536236.
Train: 2018-08-06T01:34:23.360854: step 10614, loss 0.638178.
Train: 2018-08-06T01:34:23.606172: step 10615, loss 0.537089.
Train: 2018-08-06T01:34:23.853537: step 10616, loss 0.553456.
Train: 2018-08-06T01:34:24.102870: step 10617, loss 0.546444.
Train: 2018-08-06T01:34:24.354203: step 10618, loss 0.528908.
Train: 2018-08-06T01:34:24.601542: step 10619, loss 0.545248.
Train: 2018-08-06T01:34:24.849877: step 10620, loss 0.554049.
Test: 2018-08-06T01:34:26.134412: step 10620, loss 0.548852.
Train: 2018-08-06T01:34:26.372775: step 10621, loss 0.59584.
Train: 2018-08-06T01:34:26.621111: step 10622, loss 0.545909.
Train: 2018-08-06T01:34:26.872468: step 10623, loss 0.570947.
Train: 2018-08-06T01:34:27.117814: step 10624, loss 0.545876.
Train: 2018-08-06T01:34:27.368147: step 10625, loss 0.545781.
Train: 2018-08-06T01:34:27.616482: step 10626, loss 0.579077.
Train: 2018-08-06T01:34:27.861818: step 10627, loss 0.570766.
Train: 2018-08-06T01:34:28.110152: step 10628, loss 0.59572.
Train: 2018-08-06T01:34:28.361481: step 10629, loss 0.604.
Train: 2018-08-06T01:34:28.611787: step 10630, loss 0.620513.
Test: 2018-08-06T01:34:29.878398: step 10630, loss 0.549828.
Train: 2018-08-06T01:34:30.116762: step 10631, loss 0.537693.
Train: 2018-08-06T01:34:30.372079: step 10632, loss 0.546017.
Train: 2018-08-06T01:34:30.618419: step 10633, loss 0.51315.
Train: 2018-08-06T01:34:30.868780: step 10634, loss 0.529638.
Train: 2018-08-06T01:34:31.117112: step 10635, loss 0.554318.
Train: 2018-08-06T01:34:31.369412: step 10636, loss 0.472005.
Train: 2018-08-06T01:34:31.613784: step 10637, loss 0.669767.
Train: 2018-08-06T01:34:31.865084: step 10638, loss 0.5955.
Train: 2018-08-06T01:34:32.117436: step 10639, loss 0.554282.
Train: 2018-08-06T01:34:32.366745: step 10640, loss 0.611885.
Test: 2018-08-06T01:34:33.645325: step 10640, loss 0.548459.
Train: 2018-08-06T01:34:33.887703: step 10641, loss 0.49687.
Train: 2018-08-06T01:34:34.140002: step 10642, loss 0.570758.
Train: 2018-08-06T01:34:34.386343: step 10643, loss 0.595406.
Train: 2018-08-06T01:34:34.635711: step 10644, loss 0.529794.
Train: 2018-08-06T01:34:34.877057: step 10645, loss 0.546195.
Train: 2018-08-06T01:34:35.126394: step 10646, loss 0.628079.
Train: 2018-08-06T01:34:35.373733: step 10647, loss 0.611659.
Train: 2018-08-06T01:34:35.633009: step 10648, loss 0.505556.
Train: 2018-08-06T01:34:35.880347: step 10649, loss 0.554484.
Train: 2018-08-06T01:34:36.129681: step 10650, loss 0.578919.
Test: 2018-08-06T01:34:37.387318: step 10650, loss 0.548511.
Train: 2018-08-06T01:34:37.631664: step 10651, loss 0.603312.
Train: 2018-08-06T01:34:37.879033: step 10652, loss 0.530209.
Train: 2018-08-06T01:34:38.129357: step 10653, loss 0.5708.
Train: 2018-08-06T01:34:38.384680: step 10654, loss 0.562699.
Train: 2018-08-06T01:34:38.633012: step 10655, loss 0.554615.
Train: 2018-08-06T01:34:38.881362: step 10656, loss 0.506088.
Train: 2018-08-06T01:34:39.143645: step 10657, loss 0.506021.
Train: 2018-08-06T01:34:39.400933: step 10658, loss 0.595137.
Train: 2018-08-06T01:34:39.646276: step 10659, loss 0.570787.
Train: 2018-08-06T01:34:39.896632: step 10660, loss 0.538248.
Test: 2018-08-06T01:34:41.169203: step 10660, loss 0.549217.
Train: 2018-08-06T01:34:41.407566: step 10661, loss 0.619648.
Train: 2018-08-06T01:34:41.655936: step 10662, loss 0.652225.
Train: 2018-08-06T01:34:41.904263: step 10663, loss 0.505767.
Train: 2018-08-06T01:34:42.154593: step 10664, loss 0.578908.
Train: 2018-08-06T01:34:42.398944: step 10665, loss 0.562674.
Train: 2018-08-06T01:34:42.648279: step 10666, loss 0.546465.
Train: 2018-08-06T01:34:42.893626: step 10667, loss 0.514045.
Train: 2018-08-06T01:34:43.143947: step 10668, loss 0.627593.
Train: 2018-08-06T01:34:43.387305: step 10669, loss 0.505918.
Train: 2018-08-06T01:34:43.638630: step 10670, loss 0.538327.
Test: 2018-08-06T01:34:44.908203: step 10670, loss 0.547777.
Train: 2018-08-06T01:34:45.144596: step 10671, loss 0.432636.
Train: 2018-08-06T01:34:45.389916: step 10672, loss 0.578934.
Train: 2018-08-06T01:34:45.638278: step 10673, loss 0.529814.
Train: 2018-08-06T01:34:45.886588: step 10674, loss 0.595433.
Train: 2018-08-06T01:34:46.130959: step 10675, loss 0.52954.
Train: 2018-08-06T01:34:46.373285: step 10676, loss 0.512877.
Train: 2018-08-06T01:34:46.620623: step 10677, loss 0.520962.
Train: 2018-08-06T01:34:46.875966: step 10678, loss 0.604114.
Train: 2018-08-06T01:34:47.124308: step 10679, loss 0.554053.
Train: 2018-08-06T01:34:47.373636: step 10680, loss 0.604304.
Test: 2018-08-06T01:34:48.634239: step 10680, loss 0.547994.
Train: 2018-08-06T01:34:48.883574: step 10681, loss 0.554002.
Train: 2018-08-06T01:34:49.130910: step 10682, loss 0.612784.
Train: 2018-08-06T01:34:49.379277: step 10683, loss 0.545593.
Train: 2018-08-06T01:34:49.629578: step 10684, loss 0.54559.
Train: 2018-08-06T01:34:49.874951: step 10685, loss 0.595987.
Train: 2018-08-06T01:34:50.122266: step 10686, loss 0.562392.
Train: 2018-08-06T01:34:50.366645: step 10687, loss 0.621106.
Train: 2018-08-06T01:34:50.613972: step 10688, loss 0.554038.
Train: 2018-08-06T01:34:50.857293: step 10689, loss 0.595828.
Train: 2018-08-06T01:34:51.118625: step 10690, loss 0.54577.
Test: 2018-08-06T01:34:52.379224: step 10690, loss 0.548179.
Train: 2018-08-06T01:34:52.617611: step 10691, loss 0.529191.
Train: 2018-08-06T01:34:52.893038: step 10692, loss 0.653798.
Train: 2018-08-06T01:34:53.141374: step 10693, loss 0.645248.
Train: 2018-08-06T01:34:53.391705: step 10694, loss 0.587229.
Train: 2018-08-06T01:34:53.645027: step 10695, loss 0.529792.
Train: 2018-08-06T01:34:53.893364: step 10696, loss 0.587097.
Train: 2018-08-06T01:34:54.140714: step 10697, loss 0.562654.
Train: 2018-08-06T01:34:54.389037: step 10698, loss 0.514095.
Train: 2018-08-06T01:34:54.634381: step 10699, loss 0.562723.
Train: 2018-08-06T01:34:54.891724: step 10700, loss 0.578886.
Test: 2018-08-06T01:34:56.181244: step 10700, loss 0.549138.
Train: 2018-08-06T01:34:57.058476: step 10701, loss 0.594978.
Train: 2018-08-06T01:34:57.319779: step 10702, loss 0.586905.
Train: 2018-08-06T01:34:57.567090: step 10703, loss 0.666972.
Train: 2018-08-06T01:34:57.814459: step 10704, loss 0.554951.
Train: 2018-08-06T01:34:58.060770: step 10705, loss 0.523304.
Train: 2018-08-06T01:34:58.307136: step 10706, loss 0.563029.
Train: 2018-08-06T01:34:58.563425: step 10707, loss 0.642044.
Train: 2018-08-06T01:34:58.811760: step 10708, loss 0.531661.
Train: 2018-08-06T01:34:59.055110: step 10709, loss 0.594571.
Train: 2018-08-06T01:34:59.299456: step 10710, loss 0.508444.
Test: 2018-08-06T01:35:00.581029: step 10710, loss 0.549646.
Train: 2018-08-06T01:35:00.818395: step 10711, loss 0.539789.
Train: 2018-08-06T01:35:01.062772: step 10712, loss 0.524151.
Train: 2018-08-06T01:35:01.313071: step 10713, loss 0.531905.
Train: 2018-08-06T01:35:01.563404: step 10714, loss 0.578873.
Train: 2018-08-06T01:35:01.814759: step 10715, loss 0.571017.
Train: 2018-08-06T01:35:02.064062: step 10716, loss 0.610353.
Train: 2018-08-06T01:35:02.310405: step 10717, loss 0.523747.
Train: 2018-08-06T01:35:02.556779: step 10718, loss 0.48423.
Train: 2018-08-06T01:35:02.804085: step 10719, loss 0.523461.
Train: 2018-08-06T01:35:03.048456: step 10720, loss 0.578862.
Test: 2018-08-06T01:35:04.319032: step 10720, loss 0.548664.
Train: 2018-08-06T01:35:04.501575: step 10721, loss 0.596935.
Train: 2018-08-06T01:35:04.751876: step 10722, loss 0.522884.
Train: 2018-08-06T01:35:05.007223: step 10723, loss 0.546767.
Train: 2018-08-06T01:35:05.252537: step 10724, loss 0.603035.
Train: 2018-08-06T01:35:05.496883: step 10725, loss 0.578904.
Train: 2018-08-06T01:35:05.755192: step 10726, loss 0.586974.
Train: 2018-08-06T01:35:06.006521: step 10727, loss 0.554621.
Train: 2018-08-06T01:35:06.251894: step 10728, loss 0.562704.
Train: 2018-08-06T01:35:06.497208: step 10729, loss 0.449199.
Train: 2018-08-06T01:35:06.752526: step 10730, loss 0.49756.
Test: 2018-08-06T01:35:08.022129: step 10730, loss 0.549129.
Train: 2018-08-06T01:35:08.270466: step 10731, loss 0.64434.
Train: 2018-08-06T01:35:08.518827: step 10732, loss 0.595345.
Train: 2018-08-06T01:35:08.765172: step 10733, loss 0.652812.
Train: 2018-08-06T01:35:09.022453: step 10734, loss 0.537972.
Train: 2018-08-06T01:35:09.274804: step 10735, loss 0.587154.
Train: 2018-08-06T01:35:09.527129: step 10736, loss 0.59533.
Train: 2018-08-06T01:35:09.783449: step 10737, loss 0.562592.
Train: 2018-08-06T01:35:10.032752: step 10738, loss 0.595264.
Train: 2018-08-06T01:35:10.291088: step 10739, loss 0.521891.
Train: 2018-08-06T01:35:10.534441: step 10740, loss 0.5952.
Test: 2018-08-06T01:35:11.809001: step 10740, loss 0.548708.
Train: 2018-08-06T01:35:12.048362: step 10741, loss 0.522018.
Train: 2018-08-06T01:35:12.296728: step 10742, loss 0.505792.
Train: 2018-08-06T01:35:12.560019: step 10743, loss 0.56265.
Train: 2018-08-06T01:35:12.807362: step 10744, loss 0.56264.
Train: 2018-08-06T01:35:13.053701: step 10745, loss 0.554485.
Train: 2018-08-06T01:35:13.304003: step 10746, loss 0.636004.
Train: 2018-08-06T01:35:13.553370: step 10747, loss 0.603363.
Train: 2018-08-06T01:35:13.798709: step 10748, loss 0.570782.
Train: 2018-08-06T01:35:14.046018: step 10749, loss 0.538316.
Train: 2018-08-06T01:35:14.302364: step 10750, loss 0.578902.
Test: 2018-08-06T01:35:15.568946: step 10750, loss 0.549729.
Train: 2018-08-06T01:35:15.820305: step 10751, loss 0.554597.
Train: 2018-08-06T01:35:16.077586: step 10752, loss 0.595079.
Train: 2018-08-06T01:35:16.326949: step 10753, loss 0.603128.
Train: 2018-08-06T01:35:16.578249: step 10754, loss 0.506326.
Train: 2018-08-06T01:35:16.833565: step 10755, loss 0.55471.
Train: 2018-08-06T01:35:17.075916: step 10756, loss 0.554717.
Train: 2018-08-06T01:35:17.328243: step 10757, loss 0.58693.
Train: 2018-08-06T01:35:17.583558: step 10758, loss 0.603021.
Train: 2018-08-06T01:35:17.834894: step 10759, loss 0.611021.
Train: 2018-08-06T01:35:18.080263: step 10760, loss 0.578867.
Test: 2018-08-06T01:35:19.346843: step 10760, loss 0.549507.
Train: 2018-08-06T01:35:19.594213: step 10761, loss 0.530871.
Train: 2018-08-06T01:35:19.844543: step 10762, loss 0.530932.
Train: 2018-08-06T01:35:20.098854: step 10763, loss 0.586847.
Train: 2018-08-06T01:35:20.355146: step 10764, loss 0.59482.
Train: 2018-08-06T01:35:20.607486: step 10765, loss 0.515105.
Train: 2018-08-06T01:35:20.854855: step 10766, loss 0.554949.
Train: 2018-08-06T01:35:21.099170: step 10767, loss 0.57886.
Train: 2018-08-06T01:35:21.356513: step 10768, loss 0.626704.
Train: 2018-08-06T01:35:21.605814: step 10769, loss 0.539044.
Train: 2018-08-06T01:35:21.863157: step 10770, loss 0.531106.
Test: 2018-08-06T01:35:23.130736: step 10770, loss 0.55037.
Train: 2018-08-06T01:35:23.379098: step 10771, loss 0.562934.
Train: 2018-08-06T01:35:23.623450: step 10772, loss 0.602759.
Train: 2018-08-06T01:35:23.868788: step 10773, loss 0.586823.
Train: 2018-08-06T01:35:24.121119: step 10774, loss 0.547031.
Train: 2018-08-06T01:35:24.378431: step 10775, loss 0.554993.
Train: 2018-08-06T01:35:24.628762: step 10776, loss 0.547033.
Train: 2018-08-06T01:35:24.881056: step 10777, loss 0.602744.
Train: 2018-08-06T01:35:25.126400: step 10778, loss 0.56294.
Train: 2018-08-06T01:35:25.375764: step 10779, loss 0.547025.
Train: 2018-08-06T01:35:25.625067: step 10780, loss 0.53905.
Test: 2018-08-06T01:35:26.907636: step 10780, loss 0.54924.
Train: 2018-08-06T01:35:27.158964: step 10781, loss 0.570889.
Train: 2018-08-06T01:35:27.408297: step 10782, loss 0.594815.
Train: 2018-08-06T01:35:27.655636: step 10783, loss 0.586839.
Train: 2018-08-06T01:35:27.904003: step 10784, loss 0.570884.
Train: 2018-08-06T01:35:28.152333: step 10785, loss 0.531022.
Train: 2018-08-06T01:35:28.398674: step 10786, loss 0.57886.
Train: 2018-08-06T01:35:28.652968: step 10787, loss 0.57886.
Train: 2018-08-06T01:35:28.902334: step 10788, loss 0.515033.
Train: 2018-08-06T01:35:29.159644: step 10789, loss 0.53892.
Train: 2018-08-06T01:35:29.406983: step 10790, loss 0.546853.
Test: 2018-08-06T01:35:30.685532: step 10790, loss 0.548327.
Train: 2018-08-06T01:35:30.931874: step 10791, loss 0.554809.
Train: 2018-08-06T01:35:31.181238: step 10792, loss 0.586909.
Train: 2018-08-06T01:35:31.441510: step 10793, loss 0.522535.
Train: 2018-08-06T01:35:31.690845: step 10794, loss 0.578883.
Train: 2018-08-06T01:35:31.951158: step 10795, loss 0.514232.
Train: 2018-08-06T01:35:32.199514: step 10796, loss 0.530271.
Train: 2018-08-06T01:35:32.447820: step 10797, loss 0.538255.
Train: 2018-08-06T01:35:32.695159: step 10798, loss 0.546289.
Train: 2018-08-06T01:35:32.941530: step 10799, loss 0.554383.
Train: 2018-08-06T01:35:33.190857: step 10800, loss 0.546104.
Test: 2018-08-06T01:35:34.444480: step 10800, loss 0.549877.
Train: 2018-08-06T01:35:35.359945: step 10801, loss 0.529526.
Train: 2018-08-06T01:35:35.609277: step 10802, loss 0.579034.
Train: 2018-08-06T01:35:35.868585: step 10803, loss 0.554156.
Train: 2018-08-06T01:35:36.119888: step 10804, loss 0.545791.
Train: 2018-08-06T01:35:36.375235: step 10805, loss 0.554075.
Train: 2018-08-06T01:35:36.641499: step 10806, loss 0.579141.
Train: 2018-08-06T01:35:36.888844: step 10807, loss 0.570779.
Train: 2018-08-06T01:35:37.140158: step 10808, loss 0.537215.
Train: 2018-08-06T01:35:37.390488: step 10809, loss 0.587598.
Train: 2018-08-06T01:35:37.638824: step 10810, loss 0.579201.
Test: 2018-08-06T01:35:38.922392: step 10810, loss 0.548722.
Train: 2018-08-06T01:35:39.172724: step 10811, loss 0.54556.
Train: 2018-08-06T01:35:39.421083: step 10812, loss 0.579205.
Train: 2018-08-06T01:35:39.680397: step 10813, loss 0.50351.
Train: 2018-08-06T01:35:39.939673: step 10814, loss 0.646553.
Train: 2018-08-06T01:35:40.185016: step 10815, loss 0.503534.
Train: 2018-08-06T01:35:40.435372: step 10816, loss 0.57079.
Train: 2018-08-06T01:35:40.682716: step 10817, loss 0.612812.
Train: 2018-08-06T01:35:40.946011: step 10818, loss 0.579175.
Train: 2018-08-06T01:35:41.199328: step 10819, loss 0.51215.
Train: 2018-08-06T01:35:41.447669: step 10820, loss 0.595881.
Test: 2018-08-06T01:35:42.718240: step 10820, loss 0.548075.
Train: 2018-08-06T01:35:42.958625: step 10821, loss 0.503924.
Train: 2018-08-06T01:35:43.206960: step 10822, loss 0.595832.
Train: 2018-08-06T01:35:43.452277: step 10823, loss 0.579112.
Train: 2018-08-06T01:35:43.701636: step 10824, loss 0.520768.
Train: 2018-08-06T01:35:43.950981: step 10825, loss 0.587421.
Train: 2018-08-06T01:35:44.198308: step 10826, loss 0.579081.
Train: 2018-08-06T01:35:44.444648: step 10827, loss 0.562453.
Train: 2018-08-06T01:35:44.691994: step 10828, loss 0.496103.
Train: 2018-08-06T01:35:44.942294: step 10829, loss 0.603943.
Train: 2018-08-06T01:35:45.189662: step 10830, loss 0.463007.
Test: 2018-08-06T01:35:46.463225: step 10830, loss 0.549935.
Train: 2018-08-06T01:35:46.776418: step 10831, loss 0.587358.
Train: 2018-08-06T01:35:47.024730: step 10832, loss 0.612283.
Train: 2018-08-06T01:35:47.274093: step 10833, loss 0.595655.
Train: 2018-08-06T01:35:47.522392: step 10834, loss 0.52104.
Train: 2018-08-06T01:35:47.770733: step 10835, loss 0.628733.
Train: 2018-08-06T01:35:48.023080: step 10836, loss 0.570756.
Train: 2018-08-06T01:35:48.267431: step 10837, loss 0.579004.
Train: 2018-08-06T01:35:48.515735: step 10838, loss 0.570758.
Train: 2018-08-06T01:35:48.767089: step 10839, loss 0.505086.
Train: 2018-08-06T01:35:49.015432: step 10840, loss 0.578964.
Test: 2018-08-06T01:35:50.284007: step 10840, loss 0.549075.
Train: 2018-08-06T01:35:50.520375: step 10841, loss 0.578957.
Train: 2018-08-06T01:35:50.775719: step 10842, loss 0.562585.
Train: 2018-08-06T01:35:51.022062: step 10843, loss 0.57894.
Train: 2018-08-06T01:35:51.274359: step 10844, loss 0.521827.
Train: 2018-08-06T01:35:51.522695: step 10845, loss 0.56262.
Train: 2018-08-06T01:35:51.768063: step 10846, loss 0.505572.
Train: 2018-08-06T01:35:52.030362: step 10847, loss 0.578931.
Train: 2018-08-06T01:35:52.284687: step 10848, loss 0.521796.
Train: 2018-08-06T01:35:52.533024: step 10849, loss 0.59529.
Train: 2018-08-06T01:35:52.788335: step 10850, loss 0.562589.
Test: 2018-08-06T01:35:54.049935: step 10850, loss 0.548733.
Train: 2018-08-06T01:35:54.288330: step 10851, loss 0.505315.
Train: 2018-08-06T01:35:54.538674: step 10852, loss 0.562567.
Train: 2018-08-06T01:35:54.786990: step 10853, loss 0.521514.
Train: 2018-08-06T01:35:55.039289: step 10854, loss 0.595438.
Train: 2018-08-06T01:35:55.285631: step 10855, loss 0.628415.
Train: 2018-08-06T01:35:55.533967: step 10856, loss 0.562524.
Train: 2018-08-06T01:35:55.780333: step 10857, loss 0.570758.
Train: 2018-08-06T01:35:56.026675: step 10858, loss 0.529642.
Train: 2018-08-06T01:35:56.274986: step 10859, loss 0.554309.
Train: 2018-08-06T01:35:56.535290: step 10860, loss 0.61189.
Test: 2018-08-06T01:35:57.813869: step 10860, loss 0.548815.
Train: 2018-08-06T01:35:58.051235: step 10861, loss 0.496793.
Train: 2018-08-06T01:35:58.304583: step 10862, loss 0.636547.
Train: 2018-08-06T01:35:58.552894: step 10863, loss 0.546119.
Train: 2018-08-06T01:35:58.796244: step 10864, loss 0.480478.
Train: 2018-08-06T01:35:59.054577: step 10865, loss 0.513237.
Train: 2018-08-06T01:35:59.301915: step 10866, loss 0.521343.
Train: 2018-08-06T01:35:59.550252: step 10867, loss 0.545978.
Train: 2018-08-06T01:35:59.796568: step 10868, loss 0.529339.
Train: 2018-08-06T01:36:00.044904: step 10869, loss 0.620627.
Train: 2018-08-06T01:36:00.293268: step 10870, loss 0.570762.
Test: 2018-08-06T01:36:01.567830: step 10870, loss 0.550434.
Train: 2018-08-06T01:36:01.805196: step 10871, loss 0.595763.
Train: 2018-08-06T01:36:02.005707: step 10872, loss 0.49132.
Train: 2018-08-06T01:36:02.255023: step 10873, loss 0.587461.
Train: 2018-08-06T01:36:02.519298: step 10874, loss 0.554064.
Train: 2018-08-06T01:36:02.768649: step 10875, loss 0.537336.
Train: 2018-08-06T01:36:03.016986: step 10876, loss 0.629351.
Train: 2018-08-06T01:36:03.266288: step 10877, loss 0.56241.
Train: 2018-08-06T01:36:03.512628: step 10878, loss 0.604197.
Train: 2018-08-06T01:36:03.761962: step 10879, loss 0.620819.
Train: 2018-08-06T01:36:04.020297: step 10880, loss 0.579078.
Test: 2018-08-06T01:36:05.289876: step 10880, loss 0.549384.
Train: 2018-08-06T01:36:05.527276: step 10881, loss 0.620501.
Train: 2018-08-06T01:36:05.772585: step 10882, loss 0.595521.
Train: 2018-08-06T01:36:06.015934: step 10883, loss 0.578975.
Train: 2018-08-06T01:36:06.265267: step 10884, loss 0.546235.
Train: 2018-08-06T01:36:06.512633: step 10885, loss 0.521902.
Train: 2018-08-06T01:36:06.760943: step 10886, loss 0.554535.
Train: 2018-08-06T01:36:07.013268: step 10887, loss 0.497816.
Train: 2018-08-06T01:36:07.272575: step 10888, loss 0.570794.
Train: 2018-08-06T01:36:07.519913: step 10889, loss 0.505971.
Train: 2018-08-06T01:36:07.768248: step 10890, loss 0.603237.
Test: 2018-08-06T01:36:09.060791: step 10890, loss 0.54932.
Train: 2018-08-06T01:36:09.313118: step 10891, loss 0.619459.
Train: 2018-08-06T01:36:09.558489: step 10892, loss 0.497894.
Train: 2018-08-06T01:36:09.804801: step 10893, loss 0.514072.
Train: 2018-08-06T01:36:10.054172: step 10894, loss 0.546442.
Train: 2018-08-06T01:36:10.300507: step 10895, loss 0.635827.
Train: 2018-08-06T01:36:10.549835: step 10896, loss 0.505748.
Train: 2018-08-06T01:36:10.797149: step 10897, loss 0.5545.
Train: 2018-08-06T01:36:11.060470: step 10898, loss 0.652282.
Train: 2018-08-06T01:36:11.309808: step 10899, loss 0.644067.
Train: 2018-08-06T01:36:11.556118: step 10900, loss 0.513942.
Test: 2018-08-06T01:36:12.824725: step 10900, loss 0.54895.
Train: 2018-08-06T01:36:13.805731: step 10901, loss 0.570792.
Train: 2018-08-06T01:36:14.051105: step 10902, loss 0.586996.
Train: 2018-08-06T01:36:14.309414: step 10903, loss 0.627401.
Train: 2018-08-06T01:36:14.559715: step 10904, loss 0.570819.
Train: 2018-08-06T01:36:14.821049: step 10905, loss 0.570835.
Train: 2018-08-06T01:36:15.069382: step 10906, loss 0.554823.
Train: 2018-08-06T01:36:15.319713: step 10907, loss 0.570866.
Train: 2018-08-06T01:36:15.570043: step 10908, loss 0.522999.
Train: 2018-08-06T01:36:15.818348: step 10909, loss 0.594808.
Train: 2018-08-06T01:36:16.077677: step 10910, loss 0.570896.
Test: 2018-08-06T01:36:17.352246: step 10910, loss 0.549123.
Train: 2018-08-06T01:36:17.590609: step 10911, loss 0.499333.
Train: 2018-08-06T01:36:17.854903: step 10912, loss 0.586815.
Train: 2018-08-06T01:36:18.107228: step 10913, loss 0.547031.
Train: 2018-08-06T01:36:18.353570: step 10914, loss 0.602743.
Train: 2018-08-06T01:36:18.603899: step 10915, loss 0.499275.
Train: 2018-08-06T01:36:18.852235: step 10916, loss 0.539013.
Train: 2018-08-06T01:36:19.097613: step 10917, loss 0.538936.
Train: 2018-08-06T01:36:19.344916: step 10918, loss 0.562855.
Train: 2018-08-06T01:36:19.593298: step 10919, loss 0.602937.
Train: 2018-08-06T01:36:19.849598: step 10920, loss 0.538709.
Test: 2018-08-06T01:36:21.145102: step 10920, loss 0.548974.
Train: 2018-08-06T01:36:21.387455: step 10921, loss 0.538644.
Train: 2018-08-06T01:36:21.646792: step 10922, loss 0.562753.
Train: 2018-08-06T01:36:21.893102: step 10923, loss 0.522325.
Train: 2018-08-06T01:36:22.141438: step 10924, loss 0.538381.
Train: 2018-08-06T01:36:22.388802: step 10925, loss 0.546395.
Train: 2018-08-06T01:36:22.644119: step 10926, loss 0.554462.
Train: 2018-08-06T01:36:22.898447: step 10927, loss 0.488952.
Train: 2018-08-06T01:36:23.143758: step 10928, loss 0.562539.
Train: 2018-08-06T01:36:23.390098: step 10929, loss 0.562498.
Train: 2018-08-06T01:36:23.637437: step 10930, loss 0.587322.
Test: 2018-08-06T01:36:24.887095: step 10930, loss 0.549926.
Train: 2018-08-06T01:36:25.125458: step 10931, loss 0.562479.
Train: 2018-08-06T01:36:25.372797: step 10932, loss 0.53748.
Train: 2018-08-06T01:36:25.616180: step 10933, loss 0.512369.
Train: 2018-08-06T01:36:25.864512: step 10934, loss 0.52894.
Train: 2018-08-06T01:36:26.127808: step 10935, loss 0.537177.
Train: 2018-08-06T01:36:26.388082: step 10936, loss 0.562379.
Train: 2018-08-06T01:36:26.636418: step 10937, loss 0.520082.
Train: 2018-08-06T01:36:26.886749: step 10938, loss 0.485869.
Train: 2018-08-06T01:36:27.135111: step 10939, loss 0.587962.
Train: 2018-08-06T01:36:27.381456: step 10940, loss 0.570573.
Test: 2018-08-06T01:36:28.662997: step 10940, loss 0.548109.
Train: 2018-08-06T01:36:28.902357: step 10941, loss 0.622573.
Train: 2018-08-06T01:36:29.150726: step 10942, loss 0.562312.
Train: 2018-08-06T01:36:29.398031: step 10943, loss 0.536126.
Train: 2018-08-06T01:36:29.644403: step 10944, loss 0.57894.
Train: 2018-08-06T01:36:29.899721: step 10945, loss 0.607699.
Train: 2018-08-06T01:36:30.147053: step 10946, loss 0.552777.
Train: 2018-08-06T01:36:30.406336: step 10947, loss 0.520312.
Train: 2018-08-06T01:36:30.653674: step 10948, loss 0.519213.
Train: 2018-08-06T01:36:30.903057: step 10949, loss 0.622618.
Train: 2018-08-06T01:36:31.157352: step 10950, loss 0.485223.
Test: 2018-08-06T01:36:32.452861: step 10950, loss 0.548836.
Train: 2018-08-06T01:36:32.689229: step 10951, loss 0.562235.
Train: 2018-08-06T01:36:32.939561: step 10952, loss 0.528053.
Train: 2018-08-06T01:36:33.189891: step 10953, loss 0.605092.
Train: 2018-08-06T01:36:33.448231: step 10954, loss 0.545275.
Train: 2018-08-06T01:36:33.694567: step 10955, loss 0.485539.
Train: 2018-08-06T01:36:33.940916: step 10956, loss 0.545272.
Train: 2018-08-06T01:36:34.189250: step 10957, loss 0.570912.
Train: 2018-08-06T01:36:34.435587: step 10958, loss 0.562382.
Train: 2018-08-06T01:36:34.695863: step 10959, loss 0.570887.
Train: 2018-08-06T01:36:34.942230: step 10960, loss 0.630678.
Test: 2018-08-06T01:36:36.194855: step 10960, loss 0.547516.
Train: 2018-08-06T01:36:36.460179: step 10961, loss 0.536766.
Train: 2018-08-06T01:36:36.707483: step 10962, loss 0.579366.
Train: 2018-08-06T01:36:36.966790: step 10963, loss 0.604818.
Train: 2018-08-06T01:36:37.219159: step 10964, loss 0.655522.
Train: 2018-08-06T01:36:37.468448: step 10965, loss 0.528655.
Train: 2018-08-06T01:36:37.718812: step 10966, loss 0.595978.
Train: 2018-08-06T01:36:37.967115: step 10967, loss 0.570775.
Train: 2018-08-06T01:36:38.221436: step 10968, loss 0.545786.
Train: 2018-08-06T01:36:38.470768: step 10969, loss 0.554164.
Train: 2018-08-06T01:36:38.719128: step 10970, loss 0.512868.
Test: 2018-08-06T01:36:39.974745: step 10970, loss 0.547951.
Train: 2018-08-06T01:36:40.217098: step 10971, loss 0.537726.
Train: 2018-08-06T01:36:40.465459: step 10972, loss 0.636745.
Train: 2018-08-06T01:36:40.712772: step 10973, loss 0.554308.
Train: 2018-08-06T01:36:40.961139: step 10974, loss 0.529731.
Train: 2018-08-06T01:36:41.208472: step 10975, loss 0.61993.
Train: 2018-08-06T01:36:41.465784: step 10976, loss 0.538069.
Train: 2018-08-06T01:36:41.727091: step 10977, loss 0.529978.
Train: 2018-08-06T01:36:41.974429: step 10978, loss 0.513712.
Train: 2018-08-06T01:36:42.223732: step 10979, loss 0.587086.
Train: 2018-08-06T01:36:42.483070: step 10980, loss 0.587079.
Test: 2018-08-06T01:36:43.765607: step 10980, loss 0.549584.
Train: 2018-08-06T01:36:44.005996: step 10981, loss 0.521893.
Train: 2018-08-06T01:36:44.258290: step 10982, loss 0.538167.
Train: 2018-08-06T01:36:44.507624: step 10983, loss 0.627873.
Train: 2018-08-06T01:36:44.756957: step 10984, loss 0.570768.
Train: 2018-08-06T01:36:45.003297: step 10985, loss 0.521922.
Train: 2018-08-06T01:36:45.250662: step 10986, loss 0.611513.
Train: 2018-08-06T01:36:45.498005: step 10987, loss 0.554514.
Train: 2018-08-06T01:36:45.747309: step 10988, loss 0.513861.
Train: 2018-08-06T01:36:45.993674: step 10989, loss 0.578921.
Train: 2018-08-06T01:36:46.243980: step 10990, loss 0.538234.
Test: 2018-08-06T01:36:47.509596: step 10990, loss 0.550345.
Train: 2018-08-06T01:36:47.746960: step 10991, loss 0.5708.
Train: 2018-08-06T01:36:48.003275: step 10992, loss 0.530056.
Train: 2018-08-06T01:36:48.252608: step 10993, loss 0.521851.
Train: 2018-08-06T01:36:48.513939: step 10994, loss 0.603452.
Train: 2018-08-06T01:36:48.761282: step 10995, loss 0.521704.
Train: 2018-08-06T01:36:49.011609: step 10996, loss 0.60353.
Train: 2018-08-06T01:36:49.257950: step 10997, loss 0.595354.
Train: 2018-08-06T01:36:49.520252: step 10998, loss 0.578956.
Train: 2018-08-06T01:36:49.769576: step 10999, loss 0.578952.
Train: 2018-08-06T01:36:50.017911: step 11000, loss 0.546232.
Test: 2018-08-06T01:36:51.279513: step 11000, loss 0.548191.
Train: 2018-08-06T01:36:52.167716: step 11001, loss 0.59529.
Train: 2018-08-06T01:36:52.415052: step 11002, loss 0.595261.
Train: 2018-08-06T01:36:52.663388: step 11003, loss 0.6604.
Train: 2018-08-06T01:36:52.980919: step 11004, loss 0.546447.
Train: 2018-08-06T01:36:53.228287: step 11005, loss 0.55463.
Train: 2018-08-06T01:36:53.480582: step 11006, loss 0.530486.
Train: 2018-08-06T01:36:53.725950: step 11007, loss 0.6433.
Train: 2018-08-06T01:36:53.972291: step 11008, loss 0.55479.
Train: 2018-08-06T01:36:54.222597: step 11009, loss 0.554853.
Train: 2018-08-06T01:36:54.467972: step 11010, loss 0.498999.
Test: 2018-08-06T01:36:55.734554: step 11010, loss 0.549191.
Train: 2018-08-06T01:36:55.973914: step 11011, loss 0.562892.
Train: 2018-08-06T01:36:56.219283: step 11012, loss 0.578861.
Train: 2018-08-06T01:36:56.468591: step 11013, loss 0.554921.
Train: 2018-08-06T01:36:56.714957: step 11014, loss 0.546942.
Train: 2018-08-06T01:36:56.964265: step 11015, loss 0.538945.
Train: 2018-08-06T01:36:57.212632: step 11016, loss 0.554884.
Train: 2018-08-06T01:36:57.459970: step 11017, loss 0.594868.
Train: 2018-08-06T01:36:57.719277: step 11018, loss 0.546836.
Train: 2018-08-06T01:36:57.967583: step 11019, loss 0.546808.
Train: 2018-08-06T01:36:58.213923: step 11020, loss 0.594921.
Test: 2018-08-06T01:36:59.492503: step 11020, loss 0.549769.
Train: 2018-08-06T01:36:59.739841: step 11021, loss 0.506603.
Train: 2018-08-06T01:36:59.983192: step 11022, loss 0.562786.
Train: 2018-08-06T01:37:00.173682: step 11023, loss 0.51117.
Train: 2018-08-06T01:37:00.426032: step 11024, loss 0.586982.
Train: 2018-08-06T01:37:00.673370: step 11025, loss 0.611308.
Train: 2018-08-06T01:37:00.922680: step 11026, loss 0.538362.
Train: 2018-08-06T01:37:01.171015: step 11027, loss 0.538321.
Train: 2018-08-06T01:37:01.419379: step 11028, loss 0.59519.
Train: 2018-08-06T01:37:01.669681: step 11029, loss 0.627759.
Train: 2018-08-06T01:37:01.918043: step 11030, loss 0.570784.
Test: 2018-08-06T01:37:03.178645: step 11030, loss 0.548164.
Train: 2018-08-06T01:37:03.418006: step 11031, loss 0.578906.
Train: 2018-08-06T01:37:03.666368: step 11032, loss 0.554561.
Train: 2018-08-06T01:37:03.922655: step 11033, loss 0.562694.
Train: 2018-08-06T01:37:04.167011: step 11034, loss 0.603194.
Train: 2018-08-06T01:37:04.423343: step 11035, loss 0.554613.
Train: 2018-08-06T01:37:04.683646: step 11036, loss 0.603121.
Train: 2018-08-06T01:37:04.929986: step 11037, loss 0.562741.
Train: 2018-08-06T01:37:05.184308: step 11038, loss 0.635221.
Train: 2018-08-06T01:37:05.429650: step 11039, loss 0.522699.
Train: 2018-08-06T01:37:05.677962: step 11040, loss 0.554857.
Test: 2018-08-06T01:37:06.937594: step 11040, loss 0.549706.
Train: 2018-08-06T01:37:07.189919: step 11041, loss 0.546887.
Train: 2018-08-06T01:37:07.437288: step 11042, loss 0.530931.
Train: 2018-08-06T01:37:07.700560: step 11043, loss 0.594845.
Train: 2018-08-06T01:37:07.962882: step 11044, loss 0.483037.
Train: 2018-08-06T01:37:08.209220: step 11045, loss 0.586858.
Train: 2018-08-06T01:37:08.467537: step 11046, loss 0.530818.
Train: 2018-08-06T01:37:08.725811: step 11047, loss 0.530721.
Train: 2018-08-06T01:37:08.974147: step 11048, loss 0.514517.
Train: 2018-08-06T01:37:09.220487: step 11049, loss 0.603074.
Train: 2018-08-06T01:37:09.463868: step 11050, loss 0.546533.
Test: 2018-08-06T01:37:10.724465: step 11050, loss 0.549122.
Train: 2018-08-06T01:37:10.963825: step 11051, loss 0.562786.
Train: 2018-08-06T01:37:11.209169: step 11052, loss 0.4894.
Train: 2018-08-06T01:37:11.457506: step 11053, loss 0.570699.
Train: 2018-08-06T01:37:11.704844: step 11054, loss 0.56246.
Train: 2018-08-06T01:37:11.962156: step 11055, loss 0.595262.
Train: 2018-08-06T01:37:12.213484: step 11056, loss 0.554495.
Train: 2018-08-06T01:37:12.474784: step 11057, loss 0.487907.
Train: 2018-08-06T01:37:12.720159: step 11058, loss 0.562151.
Train: 2018-08-06T01:37:12.971455: step 11059, loss 0.562777.
Train: 2018-08-06T01:37:13.219821: step 11060, loss 0.545861.
Test: 2018-08-06T01:37:14.483414: step 11060, loss 0.547361.
Train: 2018-08-06T01:37:14.731773: step 11061, loss 0.554565.
Train: 2018-08-06T01:37:14.983076: step 11062, loss 0.595863.
Train: 2018-08-06T01:37:15.228468: step 11063, loss 0.537589.
Train: 2018-08-06T01:37:15.481774: step 11064, loss 0.612893.
Train: 2018-08-06T01:37:15.728115: step 11065, loss 0.562284.
Train: 2018-08-06T01:37:15.978424: step 11066, loss 0.596401.
Train: 2018-08-06T01:37:16.226751: step 11067, loss 0.62071.
Train: 2018-08-06T01:37:16.474089: step 11068, loss 0.529254.
Train: 2018-08-06T01:37:16.718467: step 11069, loss 0.537445.
Train: 2018-08-06T01:37:16.984722: step 11070, loss 0.537517.
Test: 2018-08-06T01:37:18.268292: step 11070, loss 0.548043.
Train: 2018-08-06T01:37:18.508651: step 11071, loss 0.595644.
Train: 2018-08-06T01:37:18.753992: step 11072, loss 0.545925.
Train: 2018-08-06T01:37:19.013310: step 11073, loss 0.628664.
Train: 2018-08-06T01:37:19.261664: step 11074, loss 0.537745.
Train: 2018-08-06T01:37:19.511005: step 11075, loss 0.504839.
Train: 2018-08-06T01:37:19.757308: step 11076, loss 0.529564.
Train: 2018-08-06T01:37:20.002652: step 11077, loss 0.53778.
Train: 2018-08-06T01:37:20.247997: step 11078, loss 0.529486.
Train: 2018-08-06T01:37:20.497330: step 11079, loss 0.645156.
Train: 2018-08-06T01:37:20.745696: step 11080, loss 0.595556.
Test: 2018-08-06T01:37:22.018262: step 11080, loss 0.548531.
Train: 2018-08-06T01:37:22.257622: step 11081, loss 0.570767.
Train: 2018-08-06T01:37:22.514962: step 11082, loss 0.496574.
Train: 2018-08-06T01:37:22.776260: step 11083, loss 0.52955.
Train: 2018-08-06T01:37:23.021628: step 11084, loss 0.587262.
Train: 2018-08-06T01:37:23.270920: step 11085, loss 0.521264.
Train: 2018-08-06T01:37:23.518250: step 11086, loss 0.537721.
Train: 2018-08-06T01:37:23.779551: step 11087, loss 0.562488.
Train: 2018-08-06T01:37:24.025924: step 11088, loss 0.554169.
Train: 2018-08-06T01:37:24.276223: step 11089, loss 0.545844.
Train: 2018-08-06T01:37:24.525585: step 11090, loss 0.562392.
Test: 2018-08-06T01:37:25.818100: step 11090, loss 0.547935.
Train: 2018-08-06T01:37:26.054468: step 11091, loss 0.612461.
Train: 2018-08-06T01:37:26.305827: step 11092, loss 0.529128.
Train: 2018-08-06T01:37:26.554132: step 11093, loss 0.579137.
Train: 2018-08-06T01:37:26.802498: step 11094, loss 0.612454.
Train: 2018-08-06T01:37:27.052828: step 11095, loss 0.537344.
Train: 2018-08-06T01:37:27.307145: step 11096, loss 0.529047.
Train: 2018-08-06T01:37:27.559443: step 11097, loss 0.629047.
Train: 2018-08-06T01:37:27.807778: step 11098, loss 0.64579.
Train: 2018-08-06T01:37:28.057116: step 11099, loss 0.554167.
Train: 2018-08-06T01:37:28.312429: step 11100, loss 0.546082.
Test: 2018-08-06T01:37:29.574055: step 11100, loss 0.54997.
Train: 2018-08-06T01:37:30.497657: step 11101, loss 0.562586.
Train: 2018-08-06T01:37:30.746011: step 11102, loss 0.570767.
Train: 2018-08-06T01:37:30.999290: step 11103, loss 0.578932.
Train: 2018-08-06T01:37:31.249620: step 11104, loss 0.530001.
Train: 2018-08-06T01:37:31.500979: step 11105, loss 0.538198.
Train: 2018-08-06T01:37:31.748286: step 11106, loss 0.578905.
Train: 2018-08-06T01:37:31.996648: step 11107, loss 0.570795.
Train: 2018-08-06T01:37:32.259950: step 11108, loss 0.570791.
Train: 2018-08-06T01:37:32.507257: step 11109, loss 0.530191.
Train: 2018-08-06T01:37:32.752636: step 11110, loss 0.59506.
Test: 2018-08-06T01:37:34.024200: step 11110, loss 0.550266.
Train: 2018-08-06T01:37:34.262590: step 11111, loss 0.530247.
Train: 2018-08-06T01:37:34.509901: step 11112, loss 0.522156.
Train: 2018-08-06T01:37:34.753281: step 11113, loss 0.57078.
Train: 2018-08-06T01:37:35.010588: step 11114, loss 0.603336.
Train: 2018-08-06T01:37:35.256929: step 11115, loss 0.578902.
Train: 2018-08-06T01:37:35.507261: step 11116, loss 0.538381.
Train: 2018-08-06T01:37:35.756600: step 11117, loss 0.546414.
Train: 2018-08-06T01:37:36.003932: step 11118, loss 0.497789.
Train: 2018-08-06T01:37:36.255234: step 11119, loss 0.481379.
Train: 2018-08-06T01:37:36.514565: step 11120, loss 0.570795.
Test: 2018-08-06T01:37:37.761206: step 11120, loss 0.548715.
Train: 2018-08-06T01:37:38.011537: step 11121, loss 0.603518.
Train: 2018-08-06T01:37:38.260871: step 11122, loss 0.529761.
Train: 2018-08-06T01:37:38.509207: step 11123, loss 0.537827.
Train: 2018-08-06T01:37:38.755578: step 11124, loss 0.579038.
Train: 2018-08-06T01:37:39.004906: step 11125, loss 0.562383.
Train: 2018-08-06T01:37:39.252245: step 11126, loss 0.570851.
Train: 2018-08-06T01:37:39.502549: step 11127, loss 0.512466.
Train: 2018-08-06T01:37:39.757897: step 11128, loss 0.54569.
Train: 2018-08-06T01:37:40.010191: step 11129, loss 0.561338.
Train: 2018-08-06T01:37:40.268505: step 11130, loss 0.545413.
Test: 2018-08-06T01:37:41.528131: step 11130, loss 0.546209.
Train: 2018-08-06T01:37:41.778496: step 11131, loss 0.572419.
Train: 2018-08-06T01:37:42.031816: step 11132, loss 0.553938.
Train: 2018-08-06T01:37:42.291125: step 11133, loss 0.563007.
Train: 2018-08-06T01:37:42.541422: step 11134, loss 0.563276.
Train: 2018-08-06T01:37:42.790756: step 11135, loss 0.562322.
Train: 2018-08-06T01:37:43.039123: step 11136, loss 0.494803.
Train: 2018-08-06T01:37:43.285432: step 11137, loss 0.57872.
Train: 2018-08-06T01:37:43.531775: step 11138, loss 0.6048.
Train: 2018-08-06T01:37:43.780141: step 11139, loss 0.596407.
Train: 2018-08-06T01:37:44.033432: step 11140, loss 0.570746.
Test: 2018-08-06T01:37:45.290071: step 11140, loss 0.54899.
Train: 2018-08-06T01:37:45.527436: step 11141, loss 0.520536.
Train: 2018-08-06T01:37:45.776771: step 11142, loss 0.537327.
Train: 2018-08-06T01:37:46.038071: step 11143, loss 0.595864.
Train: 2018-08-06T01:37:46.286407: step 11144, loss 0.570764.
Train: 2018-08-06T01:37:46.533745: step 11145, loss 0.654157.
Train: 2018-08-06T01:37:46.780117: step 11146, loss 0.512616.
Train: 2018-08-06T01:37:47.027453: step 11147, loss 0.57904.
Train: 2018-08-06T01:37:47.287728: step 11148, loss 0.529363.
Train: 2018-08-06T01:37:47.534070: step 11149, loss 0.587299.
Train: 2018-08-06T01:37:47.783403: step 11150, loss 0.521283.
Test: 2018-08-06T01:37:49.060986: step 11150, loss 0.550065.
Train: 2018-08-06T01:37:49.337248: step 11151, loss 0.612017.
Train: 2018-08-06T01:37:49.599546: step 11152, loss 0.570776.
Train: 2018-08-06T01:37:49.847881: step 11153, loss 0.611818.
Train: 2018-08-06T01:37:50.098243: step 11154, loss 0.529823.
Train: 2018-08-06T01:37:50.346579: step 11155, loss 0.578923.
Train: 2018-08-06T01:37:50.594938: step 11156, loss 0.587049.
Train: 2018-08-06T01:37:50.842250: step 11157, loss 0.60334.
Train: 2018-08-06T01:37:51.103525: step 11158, loss 0.554601.
Train: 2018-08-06T01:37:51.349898: step 11159, loss 0.546569.
Train: 2018-08-06T01:37:51.598232: step 11160, loss 0.595017.
Test: 2018-08-06T01:37:52.871794: step 11160, loss 0.549537.
Train: 2018-08-06T01:37:53.114147: step 11161, loss 0.578873.
Train: 2018-08-06T01:37:53.360516: step 11162, loss 0.546782.
Train: 2018-08-06T01:37:53.610850: step 11163, loss 0.570845.
Train: 2018-08-06T01:37:53.864173: step 11164, loss 0.602854.
Train: 2018-08-06T01:37:54.121490: step 11165, loss 0.570877.
Train: 2018-08-06T01:37:54.373784: step 11166, loss 0.515234.
Train: 2018-08-06T01:37:54.623118: step 11167, loss 0.562951.
Train: 2018-08-06T01:37:54.886413: step 11168, loss 0.531201.
Train: 2018-08-06T01:37:55.140733: step 11169, loss 0.539127.
Train: 2018-08-06T01:37:55.388098: step 11170, loss 0.602725.
Test: 2018-08-06T01:37:56.668646: step 11170, loss 0.550755.
Train: 2018-08-06T01:37:56.910999: step 11171, loss 0.562941.
Train: 2018-08-06T01:37:57.158338: step 11172, loss 0.531103.
Train: 2018-08-06T01:37:57.416677: step 11173, loss 0.562913.
Train: 2018-08-06T01:37:57.610130: step 11174, loss 0.460747.
Train: 2018-08-06T01:37:57.871456: step 11175, loss 0.578794.
Train: 2018-08-06T01:37:58.136753: step 11176, loss 0.514492.
Train: 2018-08-06T01:37:58.381068: step 11177, loss 0.54629.
Train: 2018-08-06T01:37:58.636418: step 11178, loss 0.644671.
Train: 2018-08-06T01:37:58.898682: step 11179, loss 0.603127.
Train: 2018-08-06T01:37:59.149015: step 11180, loss 0.537913.
Test: 2018-08-06T01:38:00.429589: step 11180, loss 0.548333.
Train: 2018-08-06T01:38:00.679919: step 11181, loss 0.57902.
Train: 2018-08-06T01:38:00.926285: step 11182, loss 0.521475.
Train: 2018-08-06T01:38:01.176607: step 11183, loss 0.628917.
Train: 2018-08-06T01:38:01.429914: step 11184, loss 0.628128.
Train: 2018-08-06T01:38:01.678252: step 11185, loss 0.53798.
Train: 2018-08-06T01:38:01.925594: step 11186, loss 0.570473.
Train: 2018-08-06T01:38:02.173956: step 11187, loss 0.538282.
Train: 2018-08-06T01:38:02.423287: step 11188, loss 0.554613.
Train: 2018-08-06T01:38:02.673587: step 11189, loss 0.554644.
Train: 2018-08-06T01:38:02.919929: step 11190, loss 0.546517.
Test: 2018-08-06T01:38:04.181554: step 11190, loss 0.549122.
Train: 2018-08-06T01:38:04.421926: step 11191, loss 0.505871.
Train: 2018-08-06T01:38:04.669250: step 11192, loss 0.54642.
Train: 2018-08-06T01:38:04.915592: step 11193, loss 0.60326.
Train: 2018-08-06T01:38:05.168941: step 11194, loss 0.529963.
Train: 2018-08-06T01:38:05.416253: step 11195, loss 0.49722.
Train: 2018-08-06T01:38:05.672598: step 11196, loss 0.661025.
Train: 2018-08-06T01:38:05.940849: step 11197, loss 0.60367.
Train: 2018-08-06T01:38:06.192176: step 11198, loss 0.562601.
Train: 2018-08-06T01:38:06.436550: step 11199, loss 0.603423.
Train: 2018-08-06T01:38:06.685888: step 11200, loss 0.521679.
Test: 2018-08-06T01:38:07.944490: step 11200, loss 0.549358.
Train: 2018-08-06T01:38:08.832152: step 11201, loss 0.538187.
Train: 2018-08-06T01:38:09.077522: step 11202, loss 0.521841.
Train: 2018-08-06T01:38:09.332821: step 11203, loss 0.554482.
Train: 2018-08-06T01:38:09.583142: step 11204, loss 0.61163.
Train: 2018-08-06T01:38:09.830456: step 11205, loss 0.570678.
Train: 2018-08-06T01:38:10.075801: step 11206, loss 0.546299.
Train: 2018-08-06T01:38:10.325133: step 11207, loss 0.538087.
Train: 2018-08-06T01:38:10.576486: step 11208, loss 0.497176.
Train: 2018-08-06T01:38:10.823825: step 11209, loss 0.546134.
Train: 2018-08-06T01:38:11.069170: step 11210, loss 0.603662.
Test: 2018-08-06T01:38:12.331766: step 11210, loss 0.549347.
Train: 2018-08-06T01:38:12.574151: step 11211, loss 0.55443.
Train: 2018-08-06T01:38:12.823483: step 11212, loss 0.537764.
Train: 2018-08-06T01:38:13.069818: step 11213, loss 0.554361.
Train: 2018-08-06T01:38:13.331126: step 11214, loss 0.521186.
Train: 2018-08-06T01:38:13.594390: step 11215, loss 0.570792.
Train: 2018-08-06T01:38:13.840757: step 11216, loss 0.545784.
Train: 2018-08-06T01:38:14.095051: step 11217, loss 0.537502.
Train: 2018-08-06T01:38:14.354391: step 11218, loss 0.604255.
Train: 2018-08-06T01:38:14.609676: step 11219, loss 0.637342.
Train: 2018-08-06T01:38:14.854050: step 11220, loss 0.55416.
Test: 2018-08-06T01:38:16.111658: step 11220, loss 0.548952.
Train: 2018-08-06T01:38:16.358025: step 11221, loss 0.603928.
Train: 2018-08-06T01:38:16.605349: step 11222, loss 0.562496.
Train: 2018-08-06T01:38:16.853700: step 11223, loss 0.554202.
Train: 2018-08-06T01:38:17.100041: step 11224, loss 0.554241.
Train: 2018-08-06T01:38:17.359352: step 11225, loss 0.513.
Train: 2018-08-06T01:38:17.608655: step 11226, loss 0.562508.
Train: 2018-08-06T01:38:17.857019: step 11227, loss 0.579024.
Train: 2018-08-06T01:38:18.119315: step 11228, loss 0.620217.
Train: 2018-08-06T01:38:18.367634: step 11229, loss 0.63659.
Train: 2018-08-06T01:38:18.615962: step 11230, loss 0.480581.
Test: 2018-08-06T01:38:19.898531: step 11230, loss 0.548149.
Train: 2018-08-06T01:38:20.147868: step 11231, loss 0.611703.
Train: 2018-08-06T01:38:20.395203: step 11232, loss 0.554433.
Train: 2018-08-06T01:38:20.641572: step 11233, loss 0.489245.
Train: 2018-08-06T01:38:20.899854: step 11234, loss 0.530004.
Train: 2018-08-06T01:38:21.159161: step 11235, loss 0.611579.
Train: 2018-08-06T01:38:21.408493: step 11236, loss 0.529985.
Train: 2018-08-06T01:38:21.664838: step 11237, loss 0.55445.
Train: 2018-08-06T01:38:21.922146: step 11238, loss 0.611599.
Train: 2018-08-06T01:38:22.167464: step 11239, loss 0.554451.
Train: 2018-08-06T01:38:22.415824: step 11240, loss 0.562614.
Test: 2018-08-06T01:38:23.686400: step 11240, loss 0.549004.
Train: 2018-08-06T01:38:23.922770: step 11241, loss 0.595236.
Train: 2018-08-06T01:38:24.172102: step 11242, loss 0.595209.
Train: 2018-08-06T01:38:24.429446: step 11243, loss 0.570783.
Train: 2018-08-06T01:38:24.692736: step 11244, loss 0.595134.
Train: 2018-08-06T01:38:24.939052: step 11245, loss 0.530325.
Train: 2018-08-06T01:38:25.186418: step 11246, loss 0.514216.
Train: 2018-08-06T01:38:25.433728: step 11247, loss 0.546555.
Train: 2018-08-06T01:38:25.680100: step 11248, loss 0.53845.
Train: 2018-08-06T01:38:25.927408: step 11249, loss 0.546509.
Train: 2018-08-06T01:38:26.176767: step 11250, loss 0.505926.
Test: 2018-08-06T01:38:27.438367: step 11250, loss 0.549261.
Train: 2018-08-06T01:38:27.676782: step 11251, loss 0.627693.
Train: 2018-08-06T01:38:27.926096: step 11252, loss 0.578923.
Train: 2018-08-06T01:38:28.176420: step 11253, loss 0.513792.
Train: 2018-08-06T01:38:28.421736: step 11254, loss 0.513686.
Train: 2018-08-06T01:38:28.664090: step 11255, loss 0.54627.
Train: 2018-08-06T01:38:28.911428: step 11256, loss 0.587108.
Train: 2018-08-06T01:38:29.158797: step 11257, loss 0.496767.
Train: 2018-08-06T01:38:29.406136: step 11258, loss 0.612093.
Train: 2018-08-06T01:38:29.662420: step 11259, loss 0.546078.
Train: 2018-08-06T01:38:29.905768: step 11260, loss 0.587295.
Test: 2018-08-06T01:38:31.185346: step 11260, loss 0.548821.
Train: 2018-08-06T01:38:31.423740: step 11261, loss 0.5955.
Train: 2018-08-06T01:38:31.682053: step 11262, loss 0.471516.
Train: 2018-08-06T01:38:31.929391: step 11263, loss 0.604075.
Train: 2018-08-06T01:38:32.176727: step 11264, loss 0.537579.
Train: 2018-08-06T01:38:32.424065: step 11265, loss 0.554156.
Train: 2018-08-06T01:38:32.674364: step 11266, loss 0.487661.
Train: 2018-08-06T01:38:32.918710: step 11267, loss 0.579096.
Train: 2018-08-06T01:38:33.168044: step 11268, loss 0.512337.
Train: 2018-08-06T01:38:33.414385: step 11269, loss 0.64611.
Train: 2018-08-06T01:38:33.675686: step 11270, loss 0.562405.
Test: 2018-08-06T01:38:34.948283: step 11270, loss 0.549185.
Train: 2018-08-06T01:38:35.184650: step 11271, loss 0.537282.
Train: 2018-08-06T01:38:35.444955: step 11272, loss 0.545635.
Train: 2018-08-06T01:38:35.691327: step 11273, loss 0.520449.
Train: 2018-08-06T01:38:35.936664: step 11274, loss 0.520387.
Train: 2018-08-06T01:38:36.188005: step 11275, loss 0.495024.
Train: 2018-08-06T01:38:36.437326: step 11276, loss 0.511678.
Train: 2018-08-06T01:38:36.688628: step 11277, loss 0.604753.
Train: 2018-08-06T01:38:36.931977: step 11278, loss 0.494335.
Train: 2018-08-06T01:38:37.181340: step 11279, loss 0.553804.
Train: 2018-08-06T01:38:37.429677: step 11280, loss 0.579449.
Test: 2018-08-06T01:38:38.690275: step 11280, loss 0.54703.
Train: 2018-08-06T01:38:38.926644: step 11281, loss 0.536619.
Train: 2018-08-06T01:38:39.175011: step 11282, loss 0.553741.
Train: 2018-08-06T01:38:39.423348: step 11283, loss 0.527905.
Train: 2018-08-06T01:38:39.672673: step 11284, loss 0.562332.
Train: 2018-08-06T01:38:39.924974: step 11285, loss 0.614204.
Train: 2018-08-06T01:38:40.176326: step 11286, loss 0.562335.
Train: 2018-08-06T01:38:40.422644: step 11287, loss 0.545062.
Train: 2018-08-06T01:38:40.667987: step 11288, loss 0.562338.
Train: 2018-08-06T01:38:40.919345: step 11289, loss 0.545072.
Train: 2018-08-06T01:38:41.168648: step 11290, loss 0.570969.
Test: 2018-08-06T01:38:42.429276: step 11290, loss 0.548728.
Train: 2018-08-06T01:38:42.669664: step 11291, loss 0.605459.
Train: 2018-08-06T01:38:42.917994: step 11292, loss 0.6391.
Train: 2018-08-06T01:38:43.179271: step 11293, loss 0.554149.
Train: 2018-08-06T01:38:43.427606: step 11294, loss 0.59518.
Train: 2018-08-06T01:38:43.686913: step 11295, loss 0.515365.
Train: 2018-08-06T01:38:43.936247: step 11296, loss 0.53939.
Train: 2018-08-06T01:38:44.184607: step 11297, loss 0.523328.
Train: 2018-08-06T01:38:44.430949: step 11298, loss 0.554738.
Train: 2018-08-06T01:38:44.681282: step 11299, loss 0.521354.
Train: 2018-08-06T01:38:44.928591: step 11300, loss 0.553904.
Test: 2018-08-06T01:38:46.190218: step 11300, loss 0.548084.
Train: 2018-08-06T01:38:47.086425: step 11301, loss 0.664836.
Train: 2018-08-06T01:38:47.335782: step 11302, loss 0.570762.
Train: 2018-08-06T01:38:47.597089: step 11303, loss 0.562316.
Train: 2018-08-06T01:38:47.844396: step 11304, loss 0.553858.
Train: 2018-08-06T01:38:48.094752: step 11305, loss 0.604569.
Train: 2018-08-06T01:38:48.340102: step 11306, loss 0.562341.
Train: 2018-08-06T01:38:48.588437: step 11307, loss 0.545369.
Train: 2018-08-06T01:38:48.832783: step 11308, loss 0.587514.
Train: 2018-08-06T01:38:49.079126: step 11309, loss 0.562008.
Train: 2018-08-06T01:38:49.327460: step 11310, loss 0.570619.
Test: 2018-08-06T01:38:50.597036: step 11310, loss 0.547142.
Train: 2018-08-06T01:38:50.837418: step 11311, loss 0.511697.
Train: 2018-08-06T01:38:51.090744: step 11312, loss 0.588321.
Train: 2018-08-06T01:38:51.339050: step 11313, loss 0.611886.
Train: 2018-08-06T01:38:51.587386: step 11314, loss 0.580051.
Train: 2018-08-06T01:38:51.834750: step 11315, loss 0.570134.
Train: 2018-08-06T01:38:52.084058: step 11316, loss 0.621575.
Train: 2018-08-06T01:38:52.332420: step 11317, loss 0.578813.
Train: 2018-08-06T01:38:52.586714: step 11318, loss 0.504985.
Train: 2018-08-06T01:38:52.834078: step 11319, loss 0.578864.
Train: 2018-08-06T01:38:53.173490: step 11320, loss 0.513696.
Test: 2018-08-06T01:38:54.460050: step 11320, loss 0.550412.
Train: 2018-08-06T01:38:54.702402: step 11321, loss 0.578947.
Train: 2018-08-06T01:38:54.949765: step 11322, loss 0.562598.
Train: 2018-08-06T01:38:55.210075: step 11323, loss 0.586824.
Train: 2018-08-06T01:38:55.458381: step 11324, loss 0.643621.
Train: 2018-08-06T01:38:55.659867: step 11325, loss 0.631125.
Train: 2018-08-06T01:38:55.907182: step 11326, loss 0.490871.
Train: 2018-08-06T01:38:56.158538: step 11327, loss 0.562965.
Train: 2018-08-06T01:38:56.403852: step 11328, loss 0.563049.
Train: 2018-08-06T01:38:56.655179: step 11329, loss 0.531155.
Train: 2018-08-06T01:38:56.909546: step 11330, loss 0.578817.
Test: 2018-08-06T01:38:58.178106: step 11330, loss 0.55029.
Train: 2018-08-06T01:38:58.418463: step 11331, loss 0.563189.
Train: 2018-08-06T01:38:58.665802: step 11332, loss 0.555016.
Train: 2018-08-06T01:38:58.920153: step 11333, loss 0.586971.
Train: 2018-08-06T01:38:59.168489: step 11334, loss 0.602901.
Train: 2018-08-06T01:38:59.415828: step 11335, loss 0.5708.
Train: 2018-08-06T01:38:59.676126: step 11336, loss 0.586777.
Train: 2018-08-06T01:38:59.922441: step 11337, loss 0.539596.
Train: 2018-08-06T01:39:00.172804: step 11338, loss 0.55531.
Train: 2018-08-06T01:39:00.422136: step 11339, loss 0.523924.
Train: 2018-08-06T01:39:00.679417: step 11340, loss 0.571013.
Test: 2018-08-06T01:39:01.948024: step 11340, loss 0.551088.
Train: 2018-08-06T01:39:02.187410: step 11341, loss 0.563136.
Train: 2018-08-06T01:39:02.435746: step 11342, loss 0.52375.
Train: 2018-08-06T01:39:02.683084: step 11343, loss 0.563079.
Train: 2018-08-06T01:39:02.931421: step 11344, loss 0.50769.
Train: 2018-08-06T01:39:03.180758: step 11345, loss 0.586793.
Train: 2018-08-06T01:39:03.443027: step 11346, loss 0.602731.
Train: 2018-08-06T01:39:03.692359: step 11347, loss 0.58683.
Train: 2018-08-06T01:39:03.940698: step 11348, loss 0.578869.
Train: 2018-08-06T01:39:04.186039: step 11349, loss 0.54694.
Train: 2018-08-06T01:39:04.437367: step 11350, loss 0.482995.
Test: 2018-08-06T01:39:05.695004: step 11350, loss 0.549083.
Train: 2018-08-06T01:39:05.933368: step 11351, loss 0.514752.
Train: 2018-08-06T01:39:06.190679: step 11352, loss 0.538663.
Train: 2018-08-06T01:39:06.443004: step 11353, loss 0.570809.
Train: 2018-08-06T01:39:06.691365: step 11354, loss 0.570792.
Train: 2018-08-06T01:39:06.953663: step 11355, loss 0.562642.
Train: 2018-08-06T01:39:07.203002: step 11356, loss 0.497292.
Train: 2018-08-06T01:39:07.454324: step 11357, loss 0.496875.
Train: 2018-08-06T01:39:07.698646: step 11358, loss 0.553989.
Train: 2018-08-06T01:39:07.947008: step 11359, loss 0.527877.
Train: 2018-08-06T01:39:08.206324: step 11360, loss 0.509205.
Test: 2018-08-06T01:39:09.471903: step 11360, loss 0.547621.
Train: 2018-08-06T01:39:09.709314: step 11361, loss 0.564328.
Train: 2018-08-06T01:39:09.955641: step 11362, loss 0.577682.
Train: 2018-08-06T01:39:10.204969: step 11363, loss 0.54173.
Train: 2018-08-06T01:39:10.458299: step 11364, loss 0.533024.
Train: 2018-08-06T01:39:10.706640: step 11365, loss 0.562842.
Train: 2018-08-06T01:39:10.956958: step 11366, loss 0.543008.
Train: 2018-08-06T01:39:11.204270: step 11367, loss 0.531013.
Train: 2018-08-06T01:39:11.466595: step 11368, loss 0.624541.
Train: 2018-08-06T01:39:11.713939: step 11369, loss 0.618745.
Train: 2018-08-06T01:39:11.963272: step 11370, loss 0.5196.
Test: 2018-08-06T01:39:13.227858: step 11370, loss 0.54824.
Train: 2018-08-06T01:39:13.478236: step 11371, loss 0.594309.
Train: 2018-08-06T01:39:13.730545: step 11372, loss 0.580699.
Train: 2018-08-06T01:39:13.984865: step 11373, loss 0.544671.
Train: 2018-08-06T01:39:14.232198: step 11374, loss 0.614796.
Train: 2018-08-06T01:39:14.492476: step 11375, loss 0.620964.
Train: 2018-08-06T01:39:14.753786: step 11376, loss 0.571079.
Train: 2018-08-06T01:39:14.999152: step 11377, loss 0.546099.
Train: 2018-08-06T01:39:15.262418: step 11378, loss 0.537969.
Train: 2018-08-06T01:39:15.508758: step 11379, loss 0.578884.
Train: 2018-08-06T01:39:15.757125: step 11380, loss 0.538251.
Test: 2018-08-06T01:39:17.022709: step 11380, loss 0.549879.
Train: 2018-08-06T01:39:17.261072: step 11381, loss 0.530212.
Train: 2018-08-06T01:39:17.504452: step 11382, loss 0.61132.
Train: 2018-08-06T01:39:17.752757: step 11383, loss 0.57884.
Train: 2018-08-06T01:39:18.011094: step 11384, loss 0.56275.
Train: 2018-08-06T01:39:18.260425: step 11385, loss 0.554801.
Train: 2018-08-06T01:39:18.519706: step 11386, loss 0.514545.
Train: 2018-08-06T01:39:18.767045: step 11387, loss 0.5627.
Train: 2018-08-06T01:39:19.016410: step 11388, loss 0.594958.
Train: 2018-08-06T01:39:19.262753: step 11389, loss 0.6032.
Train: 2018-08-06T01:39:19.513050: step 11390, loss 0.619045.
Test: 2018-08-06T01:39:20.797614: step 11390, loss 0.551363.
Train: 2018-08-06T01:39:21.037996: step 11391, loss 0.594799.
Train: 2018-08-06T01:39:21.285309: step 11392, loss 0.52319.
Train: 2018-08-06T01:39:21.532679: step 11393, loss 0.562825.
Train: 2018-08-06T01:39:21.784974: step 11394, loss 0.547026.
Train: 2018-08-06T01:39:22.042295: step 11395, loss 0.547187.
Train: 2018-08-06T01:39:22.287629: step 11396, loss 0.634637.
Train: 2018-08-06T01:39:22.535965: step 11397, loss 0.570936.
Train: 2018-08-06T01:39:22.786297: step 11398, loss 0.523598.
Train: 2018-08-06T01:39:23.035629: step 11399, loss 0.610286.
Train: 2018-08-06T01:39:23.282967: step 11400, loss 0.555138.
Test: 2018-08-06T01:39:24.553570: step 11400, loss 0.55029.
Train: 2018-08-06T01:39:25.494993: step 11401, loss 0.531498.
Train: 2018-08-06T01:39:25.742341: step 11402, loss 0.58679.
Train: 2018-08-06T01:39:25.996628: step 11403, loss 0.468367.
Train: 2018-08-06T01:39:26.246957: step 11404, loss 0.610532.
Train: 2018-08-06T01:39:26.495293: step 11405, loss 0.594647.
Train: 2018-08-06T01:39:26.742644: step 11406, loss 0.610608.
Train: 2018-08-06T01:39:26.988998: step 11407, loss 0.507666.
Train: 2018-08-06T01:39:27.235315: step 11408, loss 0.626428.
Train: 2018-08-06T01:39:27.485671: step 11409, loss 0.531477.
Train: 2018-08-06T01:39:27.736974: step 11410, loss 0.507621.
Test: 2018-08-06T01:39:29.015553: step 11410, loss 0.550688.
Train: 2018-08-06T01:39:29.250955: step 11411, loss 0.642285.
Train: 2018-08-06T01:39:29.497295: step 11412, loss 0.610551.
Train: 2018-08-06T01:39:29.744603: step 11413, loss 0.523594.
Train: 2018-08-06T01:39:29.993961: step 11414, loss 0.555175.
Train: 2018-08-06T01:39:30.255238: step 11415, loss 0.523485.
Train: 2018-08-06T01:39:30.506592: step 11416, loss 0.555047.
Train: 2018-08-06T01:39:30.753935: step 11417, loss 0.555005.
Train: 2018-08-06T01:39:31.010219: step 11418, loss 0.57086.
Train: 2018-08-06T01:39:31.258581: step 11419, loss 0.586803.
Train: 2018-08-06T01:39:31.507917: step 11420, loss 0.538939.
Test: 2018-08-06T01:39:32.781481: step 11420, loss 0.549139.
Train: 2018-08-06T01:39:33.032836: step 11421, loss 0.562924.
Train: 2018-08-06T01:39:33.282143: step 11422, loss 0.538775.
Train: 2018-08-06T01:39:33.528484: step 11423, loss 0.498602.
Train: 2018-08-06T01:39:33.783802: step 11424, loss 0.538413.
Train: 2018-08-06T01:39:34.045132: step 11425, loss 0.579222.
Train: 2018-08-06T01:39:34.294435: step 11426, loss 0.562614.
Train: 2018-08-06T01:39:34.541776: step 11427, loss 0.570703.
Train: 2018-08-06T01:39:34.803076: step 11428, loss 0.587386.
Train: 2018-08-06T01:39:35.052435: step 11429, loss 0.512986.
Train: 2018-08-06T01:39:35.302740: step 11430, loss 0.620167.
Test: 2018-08-06T01:39:36.577330: step 11430, loss 0.548895.
Train: 2018-08-06T01:39:36.822700: step 11431, loss 0.554728.
Train: 2018-08-06T01:39:37.076015: step 11432, loss 0.537384.
Train: 2018-08-06T01:39:37.329319: step 11433, loss 0.595519.
Train: 2018-08-06T01:39:37.577680: step 11434, loss 0.62041.
Train: 2018-08-06T01:39:37.823000: step 11435, loss 0.554272.
Train: 2018-08-06T01:39:38.069340: step 11436, loss 0.578986.
Train: 2018-08-06T01:39:38.325680: step 11437, loss 0.595467.
Train: 2018-08-06T01:39:38.573024: step 11438, loss 0.538017.
Train: 2018-08-06T01:39:38.819364: step 11439, loss 0.546266.
Train: 2018-08-06T01:39:39.067670: step 11440, loss 0.505409.
Test: 2018-08-06T01:39:40.361211: step 11440, loss 0.550259.
Train: 2018-08-06T01:39:40.601600: step 11441, loss 0.546174.
Train: 2018-08-06T01:39:40.851930: step 11442, loss 0.570779.
Train: 2018-08-06T01:39:41.098241: step 11443, loss 0.562657.
Train: 2018-08-06T01:39:41.348605: step 11444, loss 0.529611.
Train: 2018-08-06T01:39:41.591951: step 11445, loss 0.587192.
Train: 2018-08-06T01:39:41.854244: step 11446, loss 0.60362.
Train: 2018-08-06T01:39:42.100559: step 11447, loss 0.538035.
Train: 2018-08-06T01:39:42.360864: step 11448, loss 0.554533.
Train: 2018-08-06T01:39:42.607204: step 11449, loss 0.578698.
Train: 2018-08-06T01:39:42.857535: step 11450, loss 0.513301.
Test: 2018-08-06T01:39:44.130131: step 11450, loss 0.548825.
Train: 2018-08-06T01:39:44.371486: step 11451, loss 0.537866.
Train: 2018-08-06T01:39:44.615833: step 11452, loss 0.562705.
Train: 2018-08-06T01:39:44.868159: step 11453, loss 0.520986.
Train: 2018-08-06T01:39:45.115497: step 11454, loss 0.562516.
Train: 2018-08-06T01:39:45.376828: step 11455, loss 0.654343.
Train: 2018-08-06T01:39:45.628125: step 11456, loss 0.553973.
Train: 2018-08-06T01:39:45.877490: step 11457, loss 0.521117.
Train: 2018-08-06T01:39:46.124798: step 11458, loss 0.637109.
Train: 2018-08-06T01:39:46.373159: step 11459, loss 0.562771.
Train: 2018-08-06T01:39:46.616513: step 11460, loss 0.554094.
Test: 2018-08-06T01:39:47.877110: step 11460, loss 0.549199.
Train: 2018-08-06T01:39:48.118466: step 11461, loss 0.546023.
Train: 2018-08-06T01:39:48.367831: step 11462, loss 0.570769.
Train: 2018-08-06T01:39:48.616134: step 11463, loss 0.611894.
Train: 2018-08-06T01:39:48.860482: step 11464, loss 0.595279.
Train: 2018-08-06T01:39:49.106856: step 11465, loss 0.538165.
Train: 2018-08-06T01:39:49.355184: step 11466, loss 0.619592.
Train: 2018-08-06T01:39:49.606486: step 11467, loss 0.562691.
Train: 2018-08-06T01:39:49.851841: step 11468, loss 0.522284.
Train: 2018-08-06T01:39:50.100206: step 11469, loss 0.506202.
Train: 2018-08-06T01:39:50.350496: step 11470, loss 0.554651.
Test: 2018-08-06T01:39:51.625087: step 11470, loss 0.548666.
Train: 2018-08-06T01:39:51.944362: step 11471, loss 0.595058.
Train: 2018-08-06T01:39:52.191699: step 11472, loss 0.659701.
Train: 2018-08-06T01:39:52.441999: step 11473, loss 0.522453.
Train: 2018-08-06T01:39:52.700308: step 11474, loss 0.570831.
Train: 2018-08-06T01:39:52.956651: step 11475, loss 0.603001.
Train: 2018-08-06T01:39:53.149140: step 11476, loss 0.528576.
Train: 2018-08-06T01:39:53.400462: step 11477, loss 0.546798.
Train: 2018-08-06T01:39:53.651763: step 11478, loss 0.570853.
Train: 2018-08-06T01:39:53.902094: step 11479, loss 0.538808.
Train: 2018-08-06T01:39:54.153421: step 11480, loss 0.514764.
Test: 2018-08-06T01:39:55.439981: step 11480, loss 0.548298.
Train: 2018-08-06T01:39:55.694300: step 11481, loss 0.546723.
Train: 2018-08-06T01:39:55.939644: step 11482, loss 0.498351.
Train: 2018-08-06T01:39:56.188009: step 11483, loss 0.50617.
Train: 2018-08-06T01:39:56.443297: step 11484, loss 0.538106.
Train: 2018-08-06T01:39:56.689639: step 11485, loss 0.553564.
Train: 2018-08-06T01:39:56.949943: step 11486, loss 0.621797.
Train: 2018-08-06T01:39:57.195318: step 11487, loss 0.595236.
Train: 2018-08-06T01:39:57.445661: step 11488, loss 0.519514.
Train: 2018-08-06T01:39:57.705923: step 11489, loss 0.570975.
Train: 2018-08-06T01:39:57.960274: step 11490, loss 0.544275.
Test: 2018-08-06T01:39:59.230843: step 11490, loss 0.547953.
Train: 2018-08-06T01:39:59.470228: step 11491, loss 0.614233.
Train: 2018-08-06T01:39:59.732532: step 11492, loss 0.631685.
Train: 2018-08-06T01:39:59.992836: step 11493, loss 0.520183.
Train: 2018-08-06T01:40:00.236156: step 11494, loss 0.554134.
Train: 2018-08-06T01:40:00.483527: step 11495, loss 0.579727.
Train: 2018-08-06T01:40:00.737813: step 11496, loss 0.562335.
Train: 2018-08-06T01:40:00.982194: step 11497, loss 0.579062.
Train: 2018-08-06T01:40:01.237476: step 11498, loss 0.570649.
Train: 2018-08-06T01:40:01.488805: step 11499, loss 0.48881.
Train: 2018-08-06T01:40:01.752100: step 11500, loss 0.54632.
Test: 2018-08-06T01:40:03.028686: step 11500, loss 0.549277.
Train: 2018-08-06T01:40:03.948187: step 11501, loss 0.562492.
Train: 2018-08-06T01:40:04.198541: step 11502, loss 0.51343.
Train: 2018-08-06T01:40:04.450842: step 11503, loss 0.537955.
Train: 2018-08-06T01:40:04.699209: step 11504, loss 0.521439.
Train: 2018-08-06T01:40:04.950530: step 11505, loss 0.562529.
Train: 2018-08-06T01:40:05.196878: step 11506, loss 0.570764.
Train: 2018-08-06T01:40:05.444186: step 11507, loss 0.612142.
Train: 2018-08-06T01:40:05.695513: step 11508, loss 0.529363.
Train: 2018-08-06T01:40:05.942851: step 11509, loss 0.504456.
Train: 2018-08-06T01:40:06.202190: step 11510, loss 0.612328.
Test: 2018-08-06T01:40:07.478744: step 11510, loss 0.549902.
Train: 2018-08-06T01:40:07.718105: step 11511, loss 0.604007.
Train: 2018-08-06T01:40:07.968466: step 11512, loss 0.55414.
Train: 2018-08-06T01:40:08.224749: step 11513, loss 0.537535.
Train: 2018-08-06T01:40:08.486092: step 11514, loss 0.612304.
Train: 2018-08-06T01:40:08.735383: step 11515, loss 0.529255.
Train: 2018-08-06T01:40:08.994690: step 11516, loss 0.603954.
Train: 2018-08-06T01:40:09.242059: step 11517, loss 0.545891.
Train: 2018-08-06T01:40:09.501335: step 11518, loss 0.562476.
Train: 2018-08-06T01:40:09.751697: step 11519, loss 0.52111.
Train: 2018-08-06T01:40:10.012967: step 11520, loss 0.529375.
Test: 2018-08-06T01:40:11.300523: step 11520, loss 0.548259.
Train: 2018-08-06T01:40:11.544870: step 11521, loss 0.545907.
Train: 2018-08-06T01:40:11.801215: step 11522, loss 0.603926.
Train: 2018-08-06T01:40:12.051558: step 11523, loss 0.545887.
Train: 2018-08-06T01:40:12.299878: step 11524, loss 0.562463.
Train: 2018-08-06T01:40:12.545221: step 11525, loss 0.57905.
Train: 2018-08-06T01:40:12.789590: step 11526, loss 0.537581.
Train: 2018-08-06T01:40:13.043862: step 11527, loss 0.512698.
Train: 2018-08-06T01:40:13.288234: step 11528, loss 0.529146.
Train: 2018-08-06T01:40:13.540559: step 11529, loss 0.56229.
Train: 2018-08-06T01:40:13.785877: step 11530, loss 0.511881.
Test: 2018-08-06T01:40:15.042516: step 11530, loss 0.549148.
Train: 2018-08-06T01:40:15.283871: step 11531, loss 0.554888.
Train: 2018-08-06T01:40:15.531238: step 11532, loss 0.460958.
Train: 2018-08-06T01:40:15.786526: step 11533, loss 0.588624.
Train: 2018-08-06T01:40:16.036888: step 11534, loss 0.621695.
Train: 2018-08-06T01:40:16.282209: step 11535, loss 0.519139.
Train: 2018-08-06T01:40:16.535548: step 11536, loss 0.62196.
Train: 2018-08-06T01:40:16.782862: step 11537, loss 0.528295.
Train: 2018-08-06T01:40:17.034189: step 11538, loss 0.502376.
Train: 2018-08-06T01:40:17.279533: step 11539, loss 0.579259.
Train: 2018-08-06T01:40:17.534851: step 11540, loss 0.588338.
Test: 2018-08-06T01:40:18.798471: step 11540, loss 0.547454.
Train: 2018-08-06T01:40:19.038828: step 11541, loss 0.597374.
Train: 2018-08-06T01:40:19.283205: step 11542, loss 0.622238.
Train: 2018-08-06T01:40:19.532539: step 11543, loss 0.596093.
Train: 2018-08-06T01:40:19.781841: step 11544, loss 0.503607.
Train: 2018-08-06T01:40:20.029180: step 11545, loss 0.570777.
Train: 2018-08-06T01:40:20.289514: step 11546, loss 0.487227.
Train: 2018-08-06T01:40:20.537819: step 11547, loss 0.570767.
Train: 2018-08-06T01:40:20.786181: step 11548, loss 0.587473.
Train: 2018-08-06T01:40:21.034491: step 11549, loss 0.645852.
Train: 2018-08-06T01:40:21.283825: step 11550, loss 0.587474.
Test: 2018-08-06T01:40:22.549439: step 11550, loss 0.546703.
Train: 2018-08-06T01:40:22.787802: step 11551, loss 0.529165.
Train: 2018-08-06T01:40:23.036166: step 11552, loss 0.5956.
Train: 2018-08-06T01:40:23.286502: step 11553, loss 0.58715.
Train: 2018-08-06T01:40:23.534835: step 11554, loss 0.653046.
Train: 2018-08-06T01:40:23.784163: step 11555, loss 0.570906.
Train: 2018-08-06T01:40:24.030479: step 11556, loss 0.635883.
Train: 2018-08-06T01:40:24.277842: step 11557, loss 0.54654.
Train: 2018-08-06T01:40:24.526184: step 11558, loss 0.578831.
Train: 2018-08-06T01:40:24.773492: step 11559, loss 0.546888.
Train: 2018-08-06T01:40:25.022826: step 11560, loss 0.602799.
Test: 2018-08-06T01:40:26.281459: step 11560, loss 0.548851.
Train: 2018-08-06T01:40:26.522843: step 11561, loss 0.586786.
Train: 2018-08-06T01:40:26.771180: step 11562, loss 0.60253.
Train: 2018-08-06T01:40:27.019512: step 11563, loss 0.578885.
Train: 2018-08-06T01:40:27.268820: step 11564, loss 0.532053.
Train: 2018-08-06T01:40:27.530121: step 11565, loss 0.532216.
Train: 2018-08-06T01:40:27.782445: step 11566, loss 0.547831.
Train: 2018-08-06T01:40:28.030780: step 11567, loss 0.485796.
Train: 2018-08-06T01:40:28.279147: step 11568, loss 0.555595.
Train: 2018-08-06T01:40:28.537462: step 11569, loss 0.547759.
Train: 2018-08-06T01:40:28.784808: step 11570, loss 0.57894.
Test: 2018-08-06T01:40:30.046397: step 11570, loss 0.551092.
Train: 2018-08-06T01:40:30.290737: step 11571, loss 0.49289.
Train: 2018-08-06T01:40:30.539073: step 11572, loss 0.578873.
Train: 2018-08-06T01:40:30.800404: step 11573, loss 0.51589.
Train: 2018-08-06T01:40:31.045743: step 11574, loss 0.547237.
Train: 2018-08-06T01:40:31.295076: step 11575, loss 0.578862.
Train: 2018-08-06T01:40:31.542420: step 11576, loss 0.562912.
Train: 2018-08-06T01:40:31.791756: step 11577, loss 0.60288.
Train: 2018-08-06T01:40:32.038089: step 11578, loss 0.514712.
Train: 2018-08-06T01:40:32.283408: step 11579, loss 0.602991.
Train: 2018-08-06T01:40:32.533739: step 11580, loss 0.643423.
Test: 2018-08-06T01:40:33.797359: step 11580, loss 0.549857.
Train: 2018-08-06T01:40:34.041730: step 11581, loss 0.530512.
Train: 2018-08-06T01:40:34.289078: step 11582, loss 0.538549.
Train: 2018-08-06T01:40:34.535410: step 11583, loss 0.578858.
Train: 2018-08-06T01:40:34.793694: step 11584, loss 0.635453.
Train: 2018-08-06T01:40:35.039063: step 11585, loss 0.538475.
Train: 2018-08-06T01:40:35.287407: step 11586, loss 0.619191.
Train: 2018-08-06T01:40:35.533748: step 11587, loss 0.587037.
Train: 2018-08-06T01:40:35.791062: step 11588, loss 0.514376.
Train: 2018-08-06T01:40:36.037399: step 11589, loss 0.570832.
Train: 2018-08-06T01:40:36.285705: step 11590, loss 0.594978.
Test: 2018-08-06T01:40:37.566279: step 11590, loss 0.549337.
Train: 2018-08-06T01:40:37.807668: step 11591, loss 0.57884.
Train: 2018-08-06T01:40:38.052026: step 11592, loss 0.546776.
Train: 2018-08-06T01:40:38.306301: step 11593, loss 0.482411.
Train: 2018-08-06T01:40:38.551669: step 11594, loss 0.594765.
Train: 2018-08-06T01:40:38.800978: step 11595, loss 0.594962.
Train: 2018-08-06T01:40:39.049342: step 11596, loss 0.546641.
Train: 2018-08-06T01:40:39.296652: step 11597, loss 0.563367.
Train: 2018-08-06T01:40:39.549010: step 11598, loss 0.59508.
Train: 2018-08-06T01:40:39.795343: step 11599, loss 0.538657.
Train: 2018-08-06T01:40:40.047644: step 11600, loss 0.562458.
Test: 2018-08-06T01:40:41.314256: step 11600, loss 0.549233.
Train: 2018-08-06T01:40:42.230080: step 11601, loss 0.570762.
Train: 2018-08-06T01:40:42.478416: step 11602, loss 0.675326.
Train: 2018-08-06T01:40:42.731735: step 11603, loss 0.538436.
Train: 2018-08-06T01:40:42.987025: step 11604, loss 0.578771.
Train: 2018-08-06T01:40:43.233391: step 11605, loss 0.50635.
Train: 2018-08-06T01:40:43.481734: step 11606, loss 0.530138.
Train: 2018-08-06T01:40:43.738016: step 11607, loss 0.578762.
Train: 2018-08-06T01:40:43.991363: step 11608, loss 0.5807.
Train: 2018-08-06T01:40:44.239680: step 11609, loss 0.578723.
Train: 2018-08-06T01:40:44.494025: step 11610, loss 0.571208.
Test: 2018-08-06T01:40:45.767588: step 11610, loss 0.549817.
Train: 2018-08-06T01:40:46.015925: step 11611, loss 0.530857.
Train: 2018-08-06T01:40:46.262291: step 11612, loss 0.503411.
Train: 2018-08-06T01:40:46.519603: step 11613, loss 0.555225.
Train: 2018-08-06T01:40:46.767914: step 11614, loss 0.603784.
Train: 2018-08-06T01:40:47.028217: step 11615, loss 0.538704.
Train: 2018-08-06T01:40:47.274584: step 11616, loss 0.571622.
Train: 2018-08-06T01:40:47.524890: step 11617, loss 0.479621.
Train: 2018-08-06T01:40:47.772251: step 11618, loss 0.644986.
Train: 2018-08-06T01:40:48.018568: step 11619, loss 0.555509.
Train: 2018-08-06T01:40:48.267901: step 11620, loss 0.561664.
Test: 2018-08-06T01:40:49.532520: step 11620, loss 0.548921.
Train: 2018-08-06T01:40:49.770882: step 11621, loss 0.537899.
Train: 2018-08-06T01:40:50.026200: step 11622, loss 0.561701.
Train: 2018-08-06T01:40:50.272566: step 11623, loss 0.596563.
Train: 2018-08-06T01:40:50.525888: step 11624, loss 0.612755.
Train: 2018-08-06T01:40:50.780184: step 11625, loss 0.578752.
Train: 2018-08-06T01:40:51.034528: step 11626, loss 0.521681.
Train: 2018-08-06T01:40:51.237959: step 11627, loss 0.528692.
Train: 2018-08-06T01:40:51.486320: step 11628, loss 0.545673.
Train: 2018-08-06T01:40:51.736624: step 11629, loss 0.563947.
Train: 2018-08-06T01:40:51.983988: step 11630, loss 0.570727.
Test: 2018-08-06T01:40:53.255562: step 11630, loss 0.5485.
Train: 2018-08-06T01:40:53.494925: step 11631, loss 0.570405.
Train: 2018-08-06T01:40:53.803099: step 11632, loss 0.513703.
Train: 2018-08-06T01:40:54.057444: step 11633, loss 0.546371.
Train: 2018-08-06T01:40:54.304763: step 11634, loss 0.536919.
Train: 2018-08-06T01:40:54.558079: step 11635, loss 0.486205.
Train: 2018-08-06T01:40:54.819382: step 11636, loss 0.613704.
Train: 2018-08-06T01:40:55.076725: step 11637, loss 0.614715.
Train: 2018-08-06T01:40:55.339037: step 11638, loss 0.556115.
Train: 2018-08-06T01:40:55.594309: step 11639, loss 0.571018.
Train: 2018-08-06T01:40:55.843641: step 11640, loss 0.554268.
Test: 2018-08-06T01:40:57.112248: step 11640, loss 0.548092.
Train: 2018-08-06T01:40:57.350644: step 11641, loss 0.60413.
Train: 2018-08-06T01:40:57.594957: step 11642, loss 0.570727.
Train: 2018-08-06T01:40:57.853268: step 11643, loss 0.570538.
Train: 2018-08-06T01:40:58.102630: step 11644, loss 0.579418.
Train: 2018-08-06T01:40:58.362935: step 11645, loss 0.604172.
Train: 2018-08-06T01:40:58.612236: step 11646, loss 0.636176.
Train: 2018-08-06T01:40:58.859580: step 11647, loss 0.538942.
Train: 2018-08-06T01:40:59.106914: step 11648, loss 0.602627.
Train: 2018-08-06T01:40:59.353256: step 11649, loss 0.61089.
Train: 2018-08-06T01:40:59.599596: step 11650, loss 0.555053.
Test: 2018-08-06T01:41:00.874188: step 11650, loss 0.549737.
Train: 2018-08-06T01:41:01.112550: step 11651, loss 0.610337.
Train: 2018-08-06T01:41:01.356896: step 11652, loss 0.555338.
Train: 2018-08-06T01:41:01.604237: step 11653, loss 0.594515.
Train: 2018-08-06T01:41:01.858556: step 11654, loss 0.539984.
Train: 2018-08-06T01:41:02.106916: step 11655, loss 0.508982.
Train: 2018-08-06T01:41:02.357223: step 11656, loss 0.602219.
Train: 2018-08-06T01:41:02.603591: step 11657, loss 0.571138.
Train: 2018-08-06T01:41:02.850932: step 11658, loss 0.563423.
Train: 2018-08-06T01:41:03.096244: step 11659, loss 0.617736.
Train: 2018-08-06T01:41:03.343584: step 11660, loss 0.563431.
Test: 2018-08-06T01:41:04.605209: step 11660, loss 0.550237.
Train: 2018-08-06T01:41:04.849556: step 11661, loss 0.563451.
Train: 2018-08-06T01:41:05.097892: step 11662, loss 0.494098.
Train: 2018-08-06T01:41:05.343236: step 11663, loss 0.563433.
Train: 2018-08-06T01:41:05.597555: step 11664, loss 0.586807.
Train: 2018-08-06T01:41:05.851877: step 11665, loss 0.547966.
Train: 2018-08-06T01:41:06.102233: step 11666, loss 0.532389.
Train: 2018-08-06T01:41:06.345583: step 11667, loss 0.563386.
Train: 2018-08-06T01:41:06.592895: step 11668, loss 0.547731.
Train: 2018-08-06T01:41:06.844246: step 11669, loss 0.539746.
Train: 2018-08-06T01:41:07.089565: step 11670, loss 0.586764.
Test: 2018-08-06T01:41:08.383106: step 11670, loss 0.550141.
Train: 2018-08-06T01:41:08.623463: step 11671, loss 0.531549.
Train: 2018-08-06T01:41:08.870818: step 11672, loss 0.555197.
Train: 2018-08-06T01:41:09.117144: step 11673, loss 0.515414.
Train: 2018-08-06T01:41:09.363515: step 11674, loss 0.523096.
Train: 2018-08-06T01:41:09.608828: step 11675, loss 0.570794.
Train: 2018-08-06T01:41:09.864146: step 11676, loss 0.570718.
Train: 2018-08-06T01:41:10.114507: step 11677, loss 0.659619.
Train: 2018-08-06T01:41:10.367831: step 11678, loss 0.538066.
Train: 2018-08-06T01:41:10.614141: step 11679, loss 0.58718.
Train: 2018-08-06T01:41:10.867462: step 11680, loss 0.545731.
Test: 2018-08-06T01:41:12.134074: step 11680, loss 0.548445.
Train: 2018-08-06T01:41:12.374432: step 11681, loss 0.562489.
Train: 2018-08-06T01:41:12.624763: step 11682, loss 0.53755.
Train: 2018-08-06T01:41:12.886095: step 11683, loss 0.485507.
Train: 2018-08-06T01:41:13.138389: step 11684, loss 0.553984.
Train: 2018-08-06T01:41:13.383735: step 11685, loss 0.632649.
Train: 2018-08-06T01:41:13.629083: step 11686, loss 0.537675.
Train: 2018-08-06T01:41:13.886389: step 11687, loss 0.482375.
Train: 2018-08-06T01:41:14.134755: step 11688, loss 0.517806.
Train: 2018-08-06T01:41:14.391038: step 11689, loss 0.517798.
Train: 2018-08-06T01:41:14.656363: step 11690, loss 0.460253.
Test: 2018-08-06T01:41:15.919950: step 11690, loss 0.548176.
Train: 2018-08-06T01:41:16.162303: step 11691, loss 0.584067.
Train: 2018-08-06T01:41:16.413656: step 11692, loss 0.599885.
Train: 2018-08-06T01:41:16.663966: step 11693, loss 0.523952.
Train: 2018-08-06T01:41:16.915331: step 11694, loss 0.571064.
Train: 2018-08-06T01:41:17.163682: step 11695, loss 0.52592.
Train: 2018-08-06T01:41:17.426930: step 11696, loss 0.537865.
Train: 2018-08-06T01:41:17.674265: step 11697, loss 0.574247.
Train: 2018-08-06T01:41:17.919639: step 11698, loss 0.547978.
Train: 2018-08-06T01:41:18.172954: step 11699, loss 0.492657.
Train: 2018-08-06T01:41:18.428253: step 11700, loss 0.633597.
Test: 2018-08-06T01:41:19.689878: step 11700, loss 0.547344.
Train: 2018-08-06T01:41:20.586437: step 11701, loss 0.595356.
Train: 2018-08-06T01:41:20.845744: step 11702, loss 0.564936.
Train: 2018-08-06T01:41:21.092110: step 11703, loss 0.554206.
Train: 2018-08-06T01:41:21.341449: step 11704, loss 0.594972.
Train: 2018-08-06T01:41:21.599752: step 11705, loss 0.588613.
Train: 2018-08-06T01:41:21.849105: step 11706, loss 0.529658.
Train: 2018-08-06T01:41:22.097397: step 11707, loss 0.596101.
Train: 2018-08-06T01:41:22.347728: step 11708, loss 0.587084.
Train: 2018-08-06T01:41:22.593071: step 11709, loss 0.562606.
Train: 2018-08-06T01:41:22.856367: step 11710, loss 0.578879.
Test: 2018-08-06T01:41:24.126969: step 11710, loss 0.550156.
Train: 2018-08-06T01:41:24.368357: step 11711, loss 0.530387.
Train: 2018-08-06T01:41:24.616690: step 11712, loss 0.562722.
Train: 2018-08-06T01:41:24.867023: step 11713, loss 0.522521.
Train: 2018-08-06T01:41:25.125299: step 11714, loss 0.506489.
Train: 2018-08-06T01:41:25.376627: step 11715, loss 0.562797.
Train: 2018-08-06T01:41:25.634936: step 11716, loss 0.627219.
Train: 2018-08-06T01:41:25.887295: step 11717, loss 0.514438.
Train: 2018-08-06T01:41:26.145571: step 11718, loss 0.530469.
Train: 2018-08-06T01:41:26.394903: step 11719, loss 0.667686.
Train: 2018-08-06T01:41:26.652247: step 11720, loss 0.611089.
Test: 2018-08-06T01:41:27.914839: step 11720, loss 0.549329.
Train: 2018-08-06T01:41:28.166167: step 11721, loss 0.546774.
Train: 2018-08-06T01:41:28.412508: step 11722, loss 0.562838.
Train: 2018-08-06T01:41:28.668851: step 11723, loss 0.611081.
Train: 2018-08-06T01:41:28.917184: step 11724, loss 0.490756.
Train: 2018-08-06T01:41:29.159511: step 11725, loss 0.57079.
Train: 2018-08-06T01:41:29.408877: step 11726, loss 0.554865.
Train: 2018-08-06T01:41:29.657180: step 11727, loss 0.562882.
Train: 2018-08-06T01:41:29.903521: step 11728, loss 0.578983.
Train: 2018-08-06T01:41:30.150884: step 11729, loss 0.586843.
Train: 2018-08-06T01:41:30.405179: step 11730, loss 0.626922.
Test: 2018-08-06T01:41:31.675781: step 11730, loss 0.550294.
Train: 2018-08-06T01:41:31.915167: step 11731, loss 0.538982.
Train: 2018-08-06T01:41:32.160515: step 11732, loss 0.586853.
Train: 2018-08-06T01:41:32.412810: step 11733, loss 0.54703.
Train: 2018-08-06T01:41:32.658184: step 11734, loss 0.586788.
Train: 2018-08-06T01:41:32.902499: step 11735, loss 0.570901.
Train: 2018-08-06T01:41:33.148842: step 11736, loss 0.531256.
Train: 2018-08-06T01:41:33.397188: step 11737, loss 0.52321.
Train: 2018-08-06T01:41:33.645546: step 11738, loss 0.546916.
Train: 2018-08-06T01:41:33.899834: step 11739, loss 0.554994.
Train: 2018-08-06T01:41:34.150164: step 11740, loss 0.498742.
Test: 2018-08-06T01:41:35.420765: step 11740, loss 0.548861.
Train: 2018-08-06T01:41:35.658164: step 11741, loss 0.546425.
Train: 2018-08-06T01:41:35.907489: step 11742, loss 0.497194.
Train: 2018-08-06T01:41:36.155833: step 11743, loss 0.577725.
Train: 2018-08-06T01:41:36.402142: step 11744, loss 0.585514.
Train: 2018-08-06T01:41:36.669427: step 11745, loss 0.549379.
Train: 2018-08-06T01:41:36.928733: step 11746, loss 0.578229.
Train: 2018-08-06T01:41:37.175119: step 11747, loss 0.609995.
Train: 2018-08-06T01:41:37.422437: step 11748, loss 0.540214.
Train: 2018-08-06T01:41:37.674773: step 11749, loss 0.607295.
Train: 2018-08-06T01:41:37.928092: step 11750, loss 0.56433.
Test: 2018-08-06T01:41:39.214620: step 11750, loss 0.549045.
Train: 2018-08-06T01:41:39.454004: step 11751, loss 0.553407.
Train: 2018-08-06T01:41:39.701328: step 11752, loss 0.612672.
Train: 2018-08-06T01:41:39.951659: step 11753, loss 0.578675.
Train: 2018-08-06T01:41:40.214985: step 11754, loss 0.578636.
Train: 2018-08-06T01:41:40.464288: step 11755, loss 0.571005.
Train: 2018-08-06T01:41:40.710654: step 11756, loss 0.602887.
Train: 2018-08-06T01:41:40.958964: step 11757, loss 0.546906.
Train: 2018-08-06T01:41:41.207301: step 11758, loss 0.60278.
Train: 2018-08-06T01:41:41.470627: step 11759, loss 0.594759.
Train: 2018-08-06T01:41:41.733891: step 11760, loss 0.626396.
Test: 2018-08-06T01:41:43.008483: step 11760, loss 0.549733.
Train: 2018-08-06T01:41:43.255847: step 11761, loss 0.523619.
Train: 2018-08-06T01:41:43.506153: step 11762, loss 0.547399.
Train: 2018-08-06T01:41:43.754513: step 11763, loss 0.633947.
Train: 2018-08-06T01:41:44.004819: step 11764, loss 0.547495.
Train: 2018-08-06T01:41:44.247202: step 11765, loss 0.555471.
Train: 2018-08-06T01:41:44.492514: step 11766, loss 0.493071.
Train: 2018-08-06T01:41:44.751821: step 11767, loss 0.547734.
Train: 2018-08-06T01:41:45.001188: step 11768, loss 0.51632.
Train: 2018-08-06T01:41:45.258499: step 11769, loss 0.539768.
Train: 2018-08-06T01:41:45.505804: step 11770, loss 0.531675.
Test: 2018-08-06T01:41:46.771420: step 11770, loss 0.549707.
Train: 2018-08-06T01:41:47.011803: step 11771, loss 0.649998.
Train: 2018-08-06T01:41:47.258118: step 11772, loss 0.555344.
Train: 2018-08-06T01:41:47.513466: step 11773, loss 0.484011.
Train: 2018-08-06T01:41:47.768753: step 11774, loss 0.523082.
Train: 2018-08-06T01:41:48.029082: step 11775, loss 0.634838.
Train: 2018-08-06T01:41:48.285371: step 11776, loss 0.530903.
Train: 2018-08-06T01:41:48.540717: step 11777, loss 0.514876.
Train: 2018-08-06T01:41:48.731179: step 11778, loss 0.579678.
Train: 2018-08-06T01:41:48.982541: step 11779, loss 0.562723.
Train: 2018-08-06T01:41:49.230843: step 11780, loss 0.554748.
Test: 2018-08-06T01:41:50.501444: step 11780, loss 0.549488.
Train: 2018-08-06T01:41:50.737813: step 11781, loss 0.530138.
Train: 2018-08-06T01:41:50.986174: step 11782, loss 0.611289.
Train: 2018-08-06T01:41:51.233517: step 11783, loss 0.546288.
Train: 2018-08-06T01:41:51.483848: step 11784, loss 0.570808.
Train: 2018-08-06T01:41:51.734179: step 11785, loss 0.636173.
Train: 2018-08-06T01:41:51.994483: step 11786, loss 0.513599.
Train: 2018-08-06T01:41:52.256781: step 11787, loss 0.521673.
Train: 2018-08-06T01:41:52.519084: step 11788, loss 0.562664.
Train: 2018-08-06T01:41:52.769407: step 11789, loss 0.620023.
Train: 2018-08-06T01:41:53.020738: step 11790, loss 0.488698.
Test: 2018-08-06T01:41:54.289314: step 11790, loss 0.549569.
Train: 2018-08-06T01:41:54.580581: step 11791, loss 0.496775.
Train: 2018-08-06T01:41:54.830912: step 11792, loss 0.562478.
Train: 2018-08-06T01:41:55.078245: step 11793, loss 0.579015.
Train: 2018-08-06T01:41:55.335532: step 11794, loss 0.545892.
Train: 2018-08-06T01:41:55.588885: step 11795, loss 0.545871.
Train: 2018-08-06T01:41:55.836219: step 11796, loss 0.537509.
Train: 2018-08-06T01:41:56.095523: step 11797, loss 0.595765.
Train: 2018-08-06T01:41:56.342862: step 11798, loss 0.645889.
Train: 2018-08-06T01:41:56.594190: step 11799, loss 0.537412.
Train: 2018-08-06T01:41:56.841534: step 11800, loss 0.529095.
Test: 2018-08-06T01:41:58.105124: step 11800, loss 0.548318.
Train: 2018-08-06T01:41:58.993807: step 11801, loss 0.529095.
Train: 2018-08-06T01:41:59.242137: step 11802, loss 0.478992.
Train: 2018-08-06T01:41:59.497429: step 11803, loss 0.545677.
Train: 2018-08-06T01:41:59.742802: step 11804, loss 0.570787.
Train: 2018-08-06T01:41:59.990113: step 11805, loss 0.511965.
Train: 2018-08-06T01:42:00.240443: step 11806, loss 0.579227.
Train: 2018-08-06T01:42:00.501743: step 11807, loss 0.528594.
Train: 2018-08-06T01:42:00.764042: step 11808, loss 0.596206.
Train: 2018-08-06T01:42:01.010408: step 11809, loss 0.570825.
Train: 2018-08-06T01:42:01.256756: step 11810, loss 0.562351.
Test: 2018-08-06T01:42:02.511369: step 11810, loss 0.548966.
Train: 2018-08-06T01:42:02.750758: step 11811, loss 0.57083.
Train: 2018-08-06T01:42:02.997071: step 11812, loss 0.4945.
Train: 2018-08-06T01:42:03.243413: step 11813, loss 0.638776.
Train: 2018-08-06T01:42:03.492744: step 11814, loss 0.494465.
Train: 2018-08-06T01:42:03.754046: step 11815, loss 0.604802.
Train: 2018-08-06T01:42:04.001385: step 11816, loss 0.570836.
Train: 2018-08-06T01:42:04.247726: step 11817, loss 0.503008.
Train: 2018-08-06T01:42:04.510055: step 11818, loss 0.579312.
Train: 2018-08-06T01:42:04.757387: step 11819, loss 0.553874.
Train: 2018-08-06T01:42:05.003704: step 11820, loss 0.545402.
Test: 2018-08-06T01:42:06.272311: step 11820, loss 0.548591.
Train: 2018-08-06T01:42:06.518653: step 11821, loss 0.545405.
Train: 2018-08-06T01:42:06.778987: step 11822, loss 0.570826.
Train: 2018-08-06T01:42:07.029312: step 11823, loss 0.613187.
Train: 2018-08-06T01:42:07.279616: step 11824, loss 0.5539.
Train: 2018-08-06T01:42:07.527984: step 11825, loss 0.511684.
Train: 2018-08-06T01:42:07.775323: step 11826, loss 0.528591.
Train: 2018-08-06T01:42:08.037590: step 11827, loss 0.537026.
Train: 2018-08-06T01:42:08.282935: step 11828, loss 0.503211.
Train: 2018-08-06T01:42:08.531279: step 11829, loss 0.596218.
Train: 2018-08-06T01:42:08.777644: step 11830, loss 0.545419.
Test: 2018-08-06T01:42:10.038240: step 11830, loss 0.549171.
Train: 2018-08-06T01:42:10.283618: step 11831, loss 0.587776.
Train: 2018-08-06T01:42:10.542890: step 11832, loss 0.562356.
Train: 2018-08-06T01:42:10.801200: step 11833, loss 0.528466.
Train: 2018-08-06T01:42:11.045573: step 11834, loss 0.494551.
Train: 2018-08-06T01:42:11.295910: step 11835, loss 0.562357.
Train: 2018-08-06T01:42:11.546240: step 11836, loss 0.570844.
Train: 2018-08-06T01:42:11.804516: step 11837, loss 0.485813.
Train: 2018-08-06T01:42:12.065817: step 11838, loss 0.596431.
Train: 2018-08-06T01:42:12.316178: step 11839, loss 0.587925.
Train: 2018-08-06T01:42:12.567476: step 11840, loss 0.59646.
Test: 2018-08-06T01:42:13.845059: step 11840, loss 0.547915.
Train: 2018-08-06T01:42:14.091401: step 11841, loss 0.468615.
Train: 2018-08-06T01:42:14.343754: step 11842, loss 0.528221.
Train: 2018-08-06T01:42:14.593059: step 11843, loss 0.536717.
Train: 2018-08-06T01:42:14.845414: step 11844, loss 0.562346.
Train: 2018-08-06T01:42:15.093746: step 11845, loss 0.553764.
Train: 2018-08-06T01:42:15.354024: step 11846, loss 0.545189.
Train: 2018-08-06T01:42:15.601362: step 11847, loss 0.579506.
Train: 2018-08-06T01:42:15.862697: step 11848, loss 0.570918.
Train: 2018-08-06T01:42:16.124961: step 11849, loss 0.519431.
Train: 2018-08-06T01:42:16.371303: step 11850, loss 0.605261.
Test: 2018-08-06T01:42:17.634923: step 11850, loss 0.547811.
Train: 2018-08-06T01:42:17.888246: step 11851, loss 0.613807.
Train: 2018-08-06T01:42:18.146555: step 11852, loss 0.519532.
Train: 2018-08-06T01:42:18.405863: step 11853, loss 0.519585.
Train: 2018-08-06T01:42:18.654228: step 11854, loss 0.562337.
Train: 2018-08-06T01:42:18.900563: step 11855, loss 0.664843.
Train: 2018-08-06T01:42:19.148875: step 11856, loss 0.63899.
Train: 2018-08-06T01:42:19.398209: step 11857, loss 0.613214.
Train: 2018-08-06T01:42:19.645547: step 11858, loss 0.553941.
Train: 2018-08-06T01:42:19.892886: step 11859, loss 0.545617.
Train: 2018-08-06T01:42:20.141254: step 11860, loss 0.587474.
Test: 2018-08-06T01:42:21.412820: step 11860, loss 0.548944.
Train: 2018-08-06T01:42:21.667140: step 11861, loss 0.58739.
Train: 2018-08-06T01:42:21.915476: step 11862, loss 0.52111.
Train: 2018-08-06T01:42:22.163843: step 11863, loss 0.620232.
Train: 2018-08-06T01:42:22.418132: step 11864, loss 0.537927.
Train: 2018-08-06T01:42:22.666468: step 11865, loss 0.562587.
Train: 2018-08-06T01:42:22.912813: step 11866, loss 0.554469.
Train: 2018-08-06T01:42:23.167160: step 11867, loss 0.627697.
Train: 2018-08-06T01:42:23.417459: step 11868, loss 0.595092.
Train: 2018-08-06T01:42:23.677764: step 11869, loss 0.562756.
Train: 2018-08-06T01:42:23.927122: step 11870, loss 0.570839.
Test: 2018-08-06T01:42:25.189719: step 11870, loss 0.549313.
Train: 2018-08-06T01:42:25.426112: step 11871, loss 0.586865.
Train: 2018-08-06T01:42:25.673454: step 11872, loss 0.531032.
Train: 2018-08-06T01:42:25.922758: step 11873, loss 0.547053.
Train: 2018-08-06T01:42:26.172093: step 11874, loss 0.531232.
Train: 2018-08-06T01:42:26.425415: step 11875, loss 0.586792.
Train: 2018-08-06T01:42:26.673751: step 11876, loss 0.570934.
Train: 2018-08-06T01:42:26.923114: step 11877, loss 0.555106.
Train: 2018-08-06T01:42:27.171420: step 11878, loss 0.657999.
Train: 2018-08-06T01:42:27.426737: step 11879, loss 0.5394.
Train: 2018-08-06T01:42:27.689035: step 11880, loss 0.570986.
Test: 2018-08-06T01:42:28.964624: step 11880, loss 0.550355.
Train: 2018-08-06T01:42:29.205978: step 11881, loss 0.547398.
Train: 2018-08-06T01:42:29.462336: step 11882, loss 0.57887.
Train: 2018-08-06T01:42:29.712625: step 11883, loss 0.618144.
Train: 2018-08-06T01:42:29.967941: step 11884, loss 0.578877.
Train: 2018-08-06T01:42:30.219268: step 11885, loss 0.594528.
Train: 2018-08-06T01:42:30.463649: step 11886, loss 0.594499.
Train: 2018-08-06T01:42:30.712949: step 11887, loss 0.617812.
Train: 2018-08-06T01:42:30.962313: step 11888, loss 0.578917.
Train: 2018-08-06T01:42:31.212613: step 11889, loss 0.617568.
Train: 2018-08-06T01:42:31.476920: step 11890, loss 0.586653.
Test: 2018-08-06T01:42:32.740526: step 11890, loss 0.549988.
Train: 2018-08-06T01:42:32.978913: step 11891, loss 0.609633.
Train: 2018-08-06T01:42:33.224265: step 11892, loss 0.594273.
Train: 2018-08-06T01:42:33.468607: step 11893, loss 0.548704.
Train: 2018-08-06T01:42:33.717937: step 11894, loss 0.61691.
Train: 2018-08-06T01:42:33.966250: step 11895, loss 0.556537.
Train: 2018-08-06T01:42:34.228547: step 11896, loss 0.541623.
Train: 2018-08-06T01:42:34.480902: step 11897, loss 0.526713.
Train: 2018-08-06T01:42:34.738208: step 11898, loss 0.481734.
Train: 2018-08-06T01:42:34.982561: step 11899, loss 0.52653.
Train: 2018-08-06T01:42:35.227874: step 11900, loss 0.556467.
Test: 2018-08-06T01:42:36.487505: step 11900, loss 0.550552.
Train: 2018-08-06T01:42:37.451829: step 11901, loss 0.556329.
Train: 2018-08-06T01:42:37.699136: step 11902, loss 0.54096.
Train: 2018-08-06T01:42:37.948500: step 11903, loss 0.594301.
Train: 2018-08-06T01:42:38.201791: step 11904, loss 0.540548.
Train: 2018-08-06T01:42:38.450161: step 11905, loss 0.517188.
Train: 2018-08-06T01:42:38.701455: step 11906, loss 0.617724.
Train: 2018-08-06T01:42:38.958799: step 11907, loss 0.594481.
Train: 2018-08-06T01:42:39.211093: step 11908, loss 0.500734.
Train: 2018-08-06T01:42:39.458456: step 11909, loss 0.594573.
Train: 2018-08-06T01:42:39.705800: step 11910, loss 0.563112.
Test: 2018-08-06T01:42:40.974377: step 11910, loss 0.550223.
Train: 2018-08-06T01:42:41.222744: step 11911, loss 0.578861.
Train: 2018-08-06T01:42:41.471074: step 11912, loss 0.618476.
Train: 2018-08-06T01:42:41.722377: step 11913, loss 0.507468.
Train: 2018-08-06T01:42:41.972708: step 11914, loss 0.539094.
Train: 2018-08-06T01:42:42.220070: step 11915, loss 0.554929.
Train: 2018-08-06T01:42:42.467384: step 11916, loss 0.570862.
Train: 2018-08-06T01:42:42.718713: step 11917, loss 0.570845.
Train: 2018-08-06T01:42:42.961090: step 11918, loss 0.570832.
Train: 2018-08-06T01:42:43.208428: step 11919, loss 0.498314.
Train: 2018-08-06T01:42:43.465742: step 11920, loss 0.562722.
Test: 2018-08-06T01:42:44.751277: step 11920, loss 0.549515.
Train: 2018-08-06T01:42:44.991634: step 11921, loss 0.554576.
Train: 2018-08-06T01:42:45.237975: step 11922, loss 0.603313.
Train: 2018-08-06T01:42:45.484344: step 11923, loss 0.603363.
Train: 2018-08-06T01:42:45.741629: step 11924, loss 0.546321.
Train: 2018-08-06T01:42:45.989997: step 11925, loss 0.505505.
Train: 2018-08-06T01:42:46.248300: step 11926, loss 0.505357.
Train: 2018-08-06T01:42:46.496641: step 11927, loss 0.546151.
Train: 2018-08-06T01:42:46.745942: step 11928, loss 0.578988.
Train: 2018-08-06T01:42:46.936462: step 11929, loss 0.615316.
Train: 2018-08-06T01:42:47.186764: step 11930, loss 0.504677.
Test: 2018-08-06T01:42:48.440410: step 11930, loss 0.547892.
Train: 2018-08-06T01:42:48.677776: step 11931, loss 0.59559.
Train: 2018-08-06T01:42:48.925114: step 11932, loss 0.521036.
Train: 2018-08-06T01:42:49.171491: step 11933, loss 0.579061.
Train: 2018-08-06T01:42:49.419791: step 11934, loss 0.495949.
Train: 2018-08-06T01:42:49.669125: step 11935, loss 0.554098.
Train: 2018-08-06T01:42:49.916463: step 11936, loss 0.620887.
Train: 2018-08-06T01:42:50.174797: step 11937, loss 0.520622.
Train: 2018-08-06T01:42:50.432114: step 11938, loss 0.545666.
Train: 2018-08-06T01:42:50.690426: step 11939, loss 0.612692.
Train: 2018-08-06T01:42:50.937732: step 11940, loss 0.554014.
Test: 2018-08-06T01:42:52.207336: step 11940, loss 0.548202.
Train: 2018-08-06T01:42:52.449688: step 11941, loss 0.595931.
Train: 2018-08-06T01:42:52.701047: step 11942, loss 0.545647.
Train: 2018-08-06T01:42:52.949390: step 11943, loss 0.545658.
Train: 2018-08-06T01:42:53.200713: step 11944, loss 0.503807.
Train: 2018-08-06T01:42:53.450042: step 11945, loss 0.545639.
Train: 2018-08-06T01:42:53.706354: step 11946, loss 0.587562.
Train: 2018-08-06T01:42:53.977936: step 11947, loss 0.562391.
Train: 2018-08-06T01:42:54.225268: step 11948, loss 0.56239.
Train: 2018-08-06T01:42:54.487542: step 11949, loss 0.545605.
Train: 2018-08-06T01:42:54.741861: step 11950, loss 0.495233.
Test: 2018-08-06T01:42:56.006479: step 11950, loss 0.547953.
Train: 2018-08-06T01:42:56.244842: step 11951, loss 0.511936.
Train: 2018-08-06T01:42:56.497167: step 11952, loss 0.587656.
Train: 2018-08-06T01:42:56.753506: step 11953, loss 0.579246.
Train: 2018-08-06T01:42:57.002816: step 11954, loss 0.57081.
Train: 2018-08-06T01:42:57.257166: step 11955, loss 0.545467.
Train: 2018-08-06T01:42:57.506498: step 11956, loss 0.511652.
Train: 2018-08-06T01:42:57.761836: step 11957, loss 0.562357.
Train: 2018-08-06T01:42:58.009155: step 11958, loss 0.59624.
Train: 2018-08-06T01:42:58.272419: step 11959, loss 0.613181.
Train: 2018-08-06T01:42:58.521778: step 11960, loss 0.528521.
Test: 2018-08-06T01:42:59.787367: step 11960, loss 0.547853.
Train: 2018-08-06T01:43:00.037715: step 11961, loss 0.545453.
Train: 2018-08-06T01:43:00.281079: step 11962, loss 0.562362.
Train: 2018-08-06T01:43:00.531378: step 11963, loss 0.66371.
Train: 2018-08-06T01:43:00.780737: step 11964, loss 0.638159.
Train: 2018-08-06T01:43:01.032039: step 11965, loss 0.528869.
Train: 2018-08-06T01:43:01.279378: step 11966, loss 0.554064.
Train: 2018-08-06T01:43:01.541721: step 11967, loss 0.520802.
Train: 2018-08-06T01:43:01.791009: step 11968, loss 0.504275.
Train: 2018-08-06T01:43:02.045357: step 11969, loss 0.54584.
Train: 2018-08-06T01:43:02.289709: step 11970, loss 0.612285.
Test: 2018-08-06T01:43:03.552299: step 11970, loss 0.549568.
Train: 2018-08-06T01:43:03.792680: step 11971, loss 0.6454.
Train: 2018-08-06T01:43:04.041048: step 11972, loss 0.56249.
Train: 2018-08-06T01:43:04.295312: step 11973, loss 0.611963.
Train: 2018-08-06T01:43:04.546640: step 11974, loss 0.578969.
Train: 2018-08-06T01:43:04.794009: step 11975, loss 0.513538.
Train: 2018-08-06T01:43:05.039323: step 11976, loss 0.497374.
Train: 2018-08-06T01:43:05.286686: step 11977, loss 0.538174.
Train: 2018-08-06T01:43:05.532004: step 11978, loss 0.538174.
Train: 2018-08-06T01:43:05.793306: step 11979, loss 0.47291.
Train: 2018-08-06T01:43:06.051615: step 11980, loss 0.505347.
Test: 2018-08-06T01:43:07.318227: step 11980, loss 0.549408.
Train: 2018-08-06T01:43:07.557613: step 11981, loss 0.546146.
Train: 2018-08-06T01:43:07.818922: step 11982, loss 0.603718.
Train: 2018-08-06T01:43:08.074236: step 11983, loss 0.661601.
Train: 2018-08-06T01:43:08.323540: step 11984, loss 0.579012.
Train: 2018-08-06T01:43:08.578857: step 11985, loss 0.529508.
Train: 2018-08-06T01:43:08.830184: step 11986, loss 0.537754.
Train: 2018-08-06T01:43:09.075559: step 11987, loss 0.51297.
Train: 2018-08-06T01:43:09.338824: step 11988, loss 0.51287.
Train: 2018-08-06T01:43:09.587159: step 11989, loss 0.570767.
Train: 2018-08-06T01:43:09.848461: step 11990, loss 0.57907.
Test: 2018-08-06T01:43:11.121057: step 11990, loss 0.54836.
Train: 2018-08-06T01:43:11.360447: step 11991, loss 0.537482.
Train: 2018-08-06T01:43:11.620752: step 11992, loss 0.629088.
Train: 2018-08-06T01:43:11.885017: step 11993, loss 0.520788.
Train: 2018-08-06T01:43:12.136374: step 11994, loss 0.612457.
Train: 2018-08-06T01:43:12.398675: step 11995, loss 0.545776.
Train: 2018-08-06T01:43:12.645979: step 11996, loss 0.562436.
Train: 2018-08-06T01:43:12.908278: step 11997, loss 0.570763.
Train: 2018-08-06T01:43:13.162628: step 11998, loss 0.504208.
Train: 2018-08-06T01:43:13.410934: step 11999, loss 0.654011.
Train: 2018-08-06T01:43:13.671237: step 12000, loss 0.570782.
Test: 2018-08-06T01:43:14.944831: step 12000, loss 0.548216.
Train: 2018-08-06T01:43:15.834192: step 12001, loss 0.595663.
Train: 2018-08-06T01:43:16.080534: step 12002, loss 0.570758.
Train: 2018-08-06T01:43:16.327866: step 12003, loss 0.58729.
Train: 2018-08-06T01:43:16.584156: step 12004, loss 0.537807.
Train: 2018-08-06T01:43:16.844487: step 12005, loss 0.562537.
Train: 2018-08-06T01:43:17.098801: step 12006, loss 0.578976.
Train: 2018-08-06T01:43:17.346141: step 12007, loss 0.472452.
Train: 2018-08-06T01:43:17.596477: step 12008, loss 0.562583.
Train: 2018-08-06T01:43:17.843784: step 12009, loss 0.546159.
Train: 2018-08-06T01:43:18.101100: step 12010, loss 0.513373.
Test: 2018-08-06T01:43:19.378679: step 12010, loss 0.549024.
Train: 2018-08-06T01:43:19.619037: step 12011, loss 0.587185.
Train: 2018-08-06T01:43:19.867398: step 12012, loss 0.644705.
Train: 2018-08-06T01:43:20.116732: step 12013, loss 0.562551.
Train: 2018-08-06T01:43:20.375016: step 12014, loss 0.587156.
Train: 2018-08-06T01:43:20.622354: step 12015, loss 0.521653.
Train: 2018-08-06T01:43:20.866726: step 12016, loss 0.603495.
Train: 2018-08-06T01:43:21.114064: step 12017, loss 0.546259.
Train: 2018-08-06T01:43:21.360411: step 12018, loss 0.587098.
Train: 2018-08-06T01:43:21.606752: step 12019, loss 0.521863.
Train: 2018-08-06T01:43:21.856084: step 12020, loss 0.595224.
Test: 2018-08-06T01:43:23.133637: step 12020, loss 0.548477.
Train: 2018-08-06T01:43:23.384994: step 12021, loss 0.57892.
Train: 2018-08-06T01:43:23.633301: step 12022, loss 0.505725.
Train: 2018-08-06T01:43:23.889647: step 12023, loss 0.538237.
Train: 2018-08-06T01:43:24.141972: step 12024, loss 0.530083.
Train: 2018-08-06T01:43:24.402270: step 12025, loss 0.587079.
Train: 2018-08-06T01:43:24.664574: step 12026, loss 0.570786.
Train: 2018-08-06T01:43:24.907924: step 12027, loss 0.554459.
Train: 2018-08-06T01:43:25.163241: step 12028, loss 0.538059.
Train: 2018-08-06T01:43:25.412568: step 12029, loss 0.620004.
Train: 2018-08-06T01:43:25.674870: step 12030, loss 0.529943.
Test: 2018-08-06T01:43:26.940457: step 12030, loss 0.547822.
Train: 2018-08-06T01:43:27.190788: step 12031, loss 0.554433.
Train: 2018-08-06T01:43:27.441118: step 12032, loss 0.505372.
Train: 2018-08-06T01:43:27.690485: step 12033, loss 0.603515.
Train: 2018-08-06T01:43:27.935795: step 12034, loss 0.578954.
Train: 2018-08-06T01:43:28.182167: step 12035, loss 0.644532.
Train: 2018-08-06T01:43:28.440472: step 12036, loss 0.505286.
Train: 2018-08-06T01:43:28.687814: step 12037, loss 0.595331.
Train: 2018-08-06T01:43:28.938143: step 12038, loss 0.611665.
Train: 2018-08-06T01:43:29.189488: step 12039, loss 0.505488.
Train: 2018-08-06T01:43:29.434786: step 12040, loss 0.521845.
Test: 2018-08-06T01:43:30.700401: step 12040, loss 0.548799.
Train: 2018-08-06T01:43:30.950757: step 12041, loss 0.587124.
Train: 2018-08-06T01:43:31.198099: step 12042, loss 0.546301.
Train: 2018-08-06T01:43:31.445441: step 12043, loss 0.562635.
Train: 2018-08-06T01:43:31.700752: step 12044, loss 0.56261.
Train: 2018-08-06T01:43:31.951083: step 12045, loss 0.595254.
Train: 2018-08-06T01:43:32.195428: step 12046, loss 0.513689.
Train: 2018-08-06T01:43:32.448726: step 12047, loss 0.59525.
Train: 2018-08-06T01:43:32.697088: step 12048, loss 0.538144.
Train: 2018-08-06T01:43:32.951382: step 12049, loss 0.570772.
Train: 2018-08-06T01:43:33.202743: step 12050, loss 0.513645.
Test: 2018-08-06T01:43:34.467327: step 12050, loss 0.548199.
Train: 2018-08-06T01:43:34.705690: step 12051, loss 0.570768.
Train: 2018-08-06T01:43:34.952057: step 12052, loss 0.513523.
Train: 2018-08-06T01:43:35.204357: step 12053, loss 0.554393.
Train: 2018-08-06T01:43:35.453708: step 12054, loss 0.578983.
Train: 2018-08-06T01:43:35.714991: step 12055, loss 0.620068.
Train: 2018-08-06T01:43:35.962360: step 12056, loss 0.53789.
Train: 2018-08-06T01:43:36.210691: step 12057, loss 0.578976.
Train: 2018-08-06T01:43:36.455019: step 12058, loss 0.521448.
Train: 2018-08-06T01:43:36.711352: step 12059, loss 0.653004.
Train: 2018-08-06T01:43:36.974656: step 12060, loss 0.611828.
Test: 2018-08-06T01:43:38.229266: step 12060, loss 0.548503.
Train: 2018-08-06T01:43:38.468660: step 12061, loss 0.562567.
Train: 2018-08-06T01:43:38.716989: step 12062, loss 0.562589.
Train: 2018-08-06T01:43:38.967321: step 12063, loss 0.578935.
Train: 2018-08-06T01:43:39.214660: step 12064, loss 0.595217.
Train: 2018-08-06T01:43:39.461008: step 12065, loss 0.554532.
Train: 2018-08-06T01:43:39.717318: step 12066, loss 0.4897.
Train: 2018-08-06T01:43:39.962659: step 12067, loss 0.530258.
Train: 2018-08-06T01:43:40.208973: step 12068, loss 0.578903.
Train: 2018-08-06T01:43:40.459331: step 12069, loss 0.538342.
Train: 2018-08-06T01:43:40.704677: step 12070, loss 0.530201.
Test: 2018-08-06T01:43:41.981232: step 12070, loss 0.548893.
Train: 2018-08-06T01:43:42.217626: step 12071, loss 0.578912.
Train: 2018-08-06T01:43:42.462970: step 12072, loss 0.578917.
Train: 2018-08-06T01:43:42.709314: step 12073, loss 0.530086.
Train: 2018-08-06T01:43:42.955654: step 12074, loss 0.578923.
Train: 2018-08-06T01:43:43.204960: step 12075, loss 0.627844.
Train: 2018-08-06T01:43:43.453338: step 12076, loss 0.546339.
Train: 2018-08-06T01:43:43.709641: step 12077, loss 0.595206.
Train: 2018-08-06T01:43:43.959972: step 12078, loss 0.660257.
Train: 2018-08-06T01:43:44.203320: step 12079, loss 0.562683.
Train: 2018-08-06T01:43:44.394778: step 12080, loss 0.51096.
Test: 2018-08-06T01:43:45.685327: step 12080, loss 0.549059.
Train: 2018-08-06T01:43:45.927704: step 12081, loss 0.481967.
Train: 2018-08-06T01:43:46.174046: step 12082, loss 0.619298.
Train: 2018-08-06T01:43:46.424350: step 12083, loss 0.546578.
Train: 2018-08-06T01:43:46.669725: step 12084, loss 0.514279.
Train: 2018-08-06T01:43:46.918030: step 12085, loss 0.595057.
Train: 2018-08-06T01:43:47.165399: step 12086, loss 0.530377.
Train: 2018-08-06T01:43:47.409746: step 12087, loss 0.611266.
Train: 2018-08-06T01:43:47.655060: step 12088, loss 0.578893.
Train: 2018-08-06T01:43:47.917357: step 12089, loss 0.611242.
Train: 2018-08-06T01:43:48.160707: step 12090, loss 0.53043.
Test: 2018-08-06T01:43:49.439287: step 12090, loss 0.550004.
Train: 2018-08-06T01:43:49.678647: step 12091, loss 0.578884.
Train: 2018-08-06T01:43:49.925019: step 12092, loss 0.635337.
Train: 2018-08-06T01:43:50.172326: step 12093, loss 0.546688.
Train: 2018-08-06T01:43:50.419691: step 12094, loss 0.578871.
Train: 2018-08-06T01:43:50.666032: step 12095, loss 0.506694.
Train: 2018-08-06T01:43:50.922352: step 12096, loss 0.546797.
Train: 2018-08-06T01:43:51.167695: step 12097, loss 0.55481.
Train: 2018-08-06T01:43:51.409055: step 12098, loss 0.602939.
Train: 2018-08-06T01:43:51.656383: step 12099, loss 0.570849.
Train: 2018-08-06T01:43:51.911711: step 12100, loss 0.538789.
Test: 2018-08-06T01:43:53.178288: step 12100, loss 0.548152.
Train: 2018-08-06T01:43:54.129818: step 12101, loss 0.482662.
Train: 2018-08-06T01:43:54.388126: step 12102, loss 0.546737.
Train: 2018-08-06T01:43:54.634494: step 12103, loss 0.506411.
Train: 2018-08-06T01:43:54.876846: step 12104, loss 0.627359.
Train: 2018-08-06T01:43:55.134132: step 12105, loss 0.586985.
Train: 2018-08-06T01:43:55.394461: step 12106, loss 0.603195.
Train: 2018-08-06T01:43:55.647789: step 12107, loss 0.4898.
Train: 2018-08-06T01:43:55.894124: step 12108, loss 0.546452.
Train: 2018-08-06T01:43:56.145458: step 12109, loss 0.619551.
Train: 2018-08-06T01:43:56.390771: step 12110, loss 0.587045.
Test: 2018-08-06T01:43:57.664364: step 12110, loss 0.548887.
Train: 2018-08-06T01:43:57.937697: step 12111, loss 0.513869.
Train: 2018-08-06T01:43:58.186002: step 12112, loss 0.578918.
Train: 2018-08-06T01:43:58.431374: step 12113, loss 0.562635.
Train: 2018-08-06T01:43:58.673697: step 12114, loss 0.56263.
Train: 2018-08-06T01:43:58.920070: step 12115, loss 0.570775.
Train: 2018-08-06T01:43:59.178348: step 12116, loss 0.635989.
Train: 2018-08-06T01:43:59.424690: step 12117, loss 0.473093.
Train: 2018-08-06T01:43:59.676042: step 12118, loss 0.530046.
Train: 2018-08-06T01:43:59.928373: step 12119, loss 0.587087.
Train: 2018-08-06T01:44:00.173686: step 12120, loss 0.570771.
Test: 2018-08-06T01:44:01.453264: step 12120, loss 0.549155.
Train: 2018-08-06T01:44:01.705589: step 12121, loss 0.546273.
Train: 2018-08-06T01:44:01.961903: step 12122, loss 0.562597.
Train: 2018-08-06T01:44:02.209272: step 12123, loss 0.513528.
Train: 2018-08-06T01:44:02.454611: step 12124, loss 0.611715.
Train: 2018-08-06T01:44:02.706910: step 12125, loss 0.61173.
Train: 2018-08-06T01:44:02.954276: step 12126, loss 0.619889.
Train: 2018-08-06T01:44:03.200592: step 12127, loss 0.513566.
Train: 2018-08-06T01:44:03.455939: step 12128, loss 0.529939.
Train: 2018-08-06T01:44:03.701288: step 12129, loss 0.578937.
Train: 2018-08-06T01:44:03.944602: step 12130, loss 0.570771.
Test: 2018-08-06T01:44:05.222184: step 12130, loss 0.548417.
Train: 2018-08-06T01:44:05.464537: step 12131, loss 0.603416.
Train: 2018-08-06T01:44:05.713870: step 12132, loss 0.562624.
Train: 2018-08-06T01:44:05.970184: step 12133, loss 0.538213.
Train: 2018-08-06T01:44:06.217522: step 12134, loss 0.554505.
Train: 2018-08-06T01:44:06.460914: step 12135, loss 0.578916.
Train: 2018-08-06T01:44:06.706216: step 12136, loss 0.562652.
Train: 2018-08-06T01:44:06.963554: step 12137, loss 0.578911.
Train: 2018-08-06T01:44:07.209900: step 12138, loss 0.570787.
Train: 2018-08-06T01:44:07.452246: step 12139, loss 0.546452.
Train: 2018-08-06T01:44:07.695570: step 12140, loss 0.61134.
Test: 2018-08-06T01:44:08.956200: step 12140, loss 0.549175.
Train: 2018-08-06T01:44:09.189575: step 12141, loss 0.489815.
Train: 2018-08-06T01:44:09.432954: step 12142, loss 0.611302.
Train: 2018-08-06T01:44:09.681288: step 12143, loss 0.49794.
Train: 2018-08-06T01:44:09.925605: step 12144, loss 0.457366.
Train: 2018-08-06T01:44:10.172944: step 12145, loss 0.53015.
Train: 2018-08-06T01:44:10.422309: step 12146, loss 0.595239.
Train: 2018-08-06T01:44:10.675626: step 12147, loss 0.587117.
Train: 2018-08-06T01:44:10.920944: step 12148, loss 0.546201.
Train: 2018-08-06T01:44:11.171275: step 12149, loss 0.619977.
Train: 2018-08-06T01:44:11.425595: step 12150, loss 0.587172.
Test: 2018-08-06T01:44:12.694201: step 12150, loss 0.549424.
Train: 2018-08-06T01:44:12.934559: step 12151, loss 0.611779.
Train: 2018-08-06T01:44:13.178937: step 12152, loss 0.578957.
Train: 2018-08-06T01:44:13.423253: step 12153, loss 0.587127.
Train: 2018-08-06T01:44:13.670624: step 12154, loss 0.521782.
Train: 2018-08-06T01:44:13.915965: step 12155, loss 0.578931.
Train: 2018-08-06T01:44:14.171283: step 12156, loss 0.611519.
Train: 2018-08-06T01:44:14.419618: step 12157, loss 0.489465.
Train: 2018-08-06T01:44:14.665983: step 12158, loss 0.611431.
Train: 2018-08-06T01:44:14.912300: step 12159, loss 0.505835.
Train: 2018-08-06T01:44:15.165592: step 12160, loss 0.651987.
Test: 2018-08-06T01:44:16.435197: step 12160, loss 0.54878.
Train: 2018-08-06T01:44:16.678547: step 12161, loss 0.530269.
Train: 2018-08-06T01:44:16.921920: step 12162, loss 0.578896.
Train: 2018-08-06T01:44:17.168237: step 12163, loss 0.473741.
Train: 2018-08-06T01:44:17.411587: step 12164, loss 0.603183.
Train: 2018-08-06T01:44:17.654966: step 12165, loss 0.530318.
Train: 2018-08-06T01:44:17.904269: step 12166, loss 0.497875.
Train: 2018-08-06T01:44:18.154629: step 12167, loss 0.578907.
Train: 2018-08-06T01:44:18.411941: step 12168, loss 0.52199.
Train: 2018-08-06T01:44:18.654286: step 12169, loss 0.595227.
Train: 2018-08-06T01:44:18.901601: step 12170, loss 0.595256.
Test: 2018-08-06T01:44:20.165221: step 12170, loss 0.549718.
Train: 2018-08-06T01:44:20.402612: step 12171, loss 0.562605.
Train: 2018-08-06T01:44:20.643942: step 12172, loss 0.448224.
Train: 2018-08-06T01:44:20.905242: step 12173, loss 0.554377.
Train: 2018-08-06T01:44:21.149620: step 12174, loss 0.529685.
Train: 2018-08-06T01:44:21.407922: step 12175, loss 0.554275.
Train: 2018-08-06T01:44:21.658260: step 12176, loss 0.603812.
Train: 2018-08-06T01:44:21.902600: step 12177, loss 0.579035.
Train: 2018-08-06T01:44:22.148942: step 12178, loss 0.504456.
Train: 2018-08-06T01:44:22.394260: step 12179, loss 0.537546.
Train: 2018-08-06T01:44:22.644622: step 12180, loss 0.579098.
Test: 2018-08-06T01:44:23.906217: step 12180, loss 0.549462.
Train: 2018-08-06T01:44:24.143607: step 12181, loss 0.570764.
Train: 2018-08-06T01:44:24.394943: step 12182, loss 0.579119.
Train: 2018-08-06T01:44:24.641290: step 12183, loss 0.529005.
Train: 2018-08-06T01:44:24.885629: step 12184, loss 0.620943.
Train: 2018-08-06T01:44:25.131963: step 12185, loss 0.604209.
Train: 2018-08-06T01:44:25.384264: step 12186, loss 0.562419.
Train: 2018-08-06T01:44:25.645565: step 12187, loss 0.554088.
Train: 2018-08-06T01:44:25.889912: step 12188, loss 0.595755.
Train: 2018-08-06T01:44:26.131297: step 12189, loss 0.512548.
Train: 2018-08-06T01:44:26.374615: step 12190, loss 0.537516.
Test: 2018-08-06T01:44:27.633250: step 12190, loss 0.547997.
Train: 2018-08-06T01:44:27.866657: step 12191, loss 0.662172.
Train: 2018-08-06T01:44:28.122940: step 12192, loss 0.562466.
Train: 2018-08-06T01:44:28.380251: step 12193, loss 0.529386.
Train: 2018-08-06T01:44:28.623626: step 12194, loss 0.645127.
Train: 2018-08-06T01:44:28.873931: step 12195, loss 0.554278.
Train: 2018-08-06T01:44:29.128283: step 12196, loss 0.505011.
Train: 2018-08-06T01:44:29.373620: step 12197, loss 0.529713.
Train: 2018-08-06T01:44:29.627916: step 12198, loss 0.480488.
Train: 2018-08-06T01:44:29.874257: step 12199, loss 0.529669.
Train: 2018-08-06T01:44:30.120622: step 12200, loss 0.513121.
Test: 2018-08-06T01:44:31.398181: step 12200, loss 0.547952.
Train: 2018-08-06T01:44:32.311161: step 12201, loss 0.595524.
Train: 2018-08-06T01:44:32.567480: step 12202, loss 0.537677.
Train: 2018-08-06T01:44:32.813817: step 12203, loss 0.579043.
Train: 2018-08-06T01:44:33.058165: step 12204, loss 0.529273.
Train: 2018-08-06T01:44:33.302535: step 12205, loss 0.554136.
Train: 2018-08-06T01:44:33.546885: step 12206, loss 0.570763.
Train: 2018-08-06T01:44:33.789208: step 12207, loss 0.662471.
Train: 2018-08-06T01:44:34.033580: step 12208, loss 0.495807.
Train: 2018-08-06T01:44:34.277934: step 12209, loss 0.587427.
Train: 2018-08-06T01:44:34.525264: step 12210, loss 0.570763.
Test: 2018-08-06T01:44:35.797837: step 12210, loss 0.548918.
Train: 2018-08-06T01:44:36.036210: step 12211, loss 0.587412.
Train: 2018-08-06T01:44:36.280570: step 12212, loss 0.512552.
Train: 2018-08-06T01:44:36.525889: step 12213, loss 0.579076.
Train: 2018-08-06T01:44:36.775222: step 12214, loss 0.595694.
Train: 2018-08-06T01:44:37.033556: step 12215, loss 0.537553.
Train: 2018-08-06T01:44:37.281900: step 12216, loss 0.529278.
Train: 2018-08-06T01:44:37.527238: step 12217, loss 0.529277.
Train: 2018-08-06T01:44:37.782529: step 12218, loss 0.612266.
Train: 2018-08-06T01:44:38.029894: step 12219, loss 0.612242.
Train: 2018-08-06T01:44:38.275213: step 12220, loss 0.570757.
Test: 2018-08-06T01:44:39.538832: step 12220, loss 0.548105.
Train: 2018-08-06T01:44:39.786195: step 12221, loss 0.554217.
Train: 2018-08-06T01:44:40.028523: step 12222, loss 0.587272.
Train: 2018-08-06T01:44:40.273866: step 12223, loss 0.578999.
Train: 2018-08-06T01:44:40.520207: step 12224, loss 0.554307.
Train: 2018-08-06T01:44:40.763558: step 12225, loss 0.587183.
Train: 2018-08-06T01:44:41.023886: step 12226, loss 0.587151.
Train: 2018-08-06T01:44:41.272197: step 12227, loss 0.59529.
Train: 2018-08-06T01:44:41.515545: step 12228, loss 0.530025.
Train: 2018-08-06T01:44:41.758896: step 12229, loss 0.546379.
Train: 2018-08-06T01:44:42.012218: step 12230, loss 0.538294.
Test: 2018-08-06T01:44:43.285812: step 12230, loss 0.549302.
Train: 2018-08-06T01:44:43.479295: step 12231, loss 0.562671.
Train: 2018-08-06T01:44:43.723642: step 12232, loss 0.554566.
Train: 2018-08-06T01:44:43.964994: step 12233, loss 0.587011.
Train: 2018-08-06T01:44:44.213330: step 12234, loss 0.46546.
Train: 2018-08-06T01:44:44.459674: step 12235, loss 0.587018.
Train: 2018-08-06T01:44:44.704019: step 12236, loss 0.603268.
Train: 2018-08-06T01:44:44.951359: step 12237, loss 0.578907.
Train: 2018-08-06T01:44:45.194705: step 12238, loss 0.55456.
Train: 2018-08-06T01:44:45.436060: step 12239, loss 0.497775.
Train: 2018-08-06T01:44:45.681435: step 12240, loss 0.595154.
Test: 2018-08-06T01:44:46.950012: step 12240, loss 0.548901.
Train: 2018-08-06T01:44:47.184386: step 12241, loss 0.578911.
Train: 2018-08-06T01:44:47.429761: step 12242, loss 0.562658.
Train: 2018-08-06T01:44:47.689037: step 12243, loss 0.530152.
Train: 2018-08-06T01:44:47.933384: step 12244, loss 0.538251.
Train: 2018-08-06T01:44:48.181719: step 12245, loss 0.554493.
Train: 2018-08-06T01:44:48.425068: step 12246, loss 0.497398.
Train: 2018-08-06T01:44:48.684404: step 12247, loss 0.644331.
Train: 2018-08-06T01:44:48.929749: step 12248, loss 0.554411.
Train: 2018-08-06T01:44:49.186034: step 12249, loss 0.538033.
Train: 2018-08-06T01:44:49.431375: step 12250, loss 0.521612.
Test: 2018-08-06T01:44:50.701978: step 12250, loss 0.547903.
Train: 2018-08-06T01:44:50.951312: step 12251, loss 0.529726.
Train: 2018-08-06T01:44:51.196681: step 12252, loss 0.562532.
Train: 2018-08-06T01:44:51.440003: step 12253, loss 0.521303.
Train: 2018-08-06T01:44:51.680361: step 12254, loss 0.488117.
Train: 2018-08-06T01:44:51.923737: step 12255, loss 0.637124.
Train: 2018-08-06T01:44:52.167059: step 12256, loss 0.595692.
Train: 2018-08-06T01:44:52.411406: step 12257, loss 0.512536.
Train: 2018-08-06T01:44:52.660739: step 12258, loss 0.587428.
Train: 2018-08-06T01:44:52.905085: step 12259, loss 0.595785.
Train: 2018-08-06T01:44:53.145443: step 12260, loss 0.520725.
Test: 2018-08-06T01:44:54.402099: step 12260, loss 0.547137.
Train: 2018-08-06T01:44:54.639448: step 12261, loss 0.604155.
Train: 2018-08-06T01:44:54.942222: step 12262, loss 0.495666.
Train: 2018-08-06T01:44:55.187565: step 12263, loss 0.512297.
Train: 2018-08-06T01:44:55.432914: step 12264, loss 0.570774.
Train: 2018-08-06T01:44:55.682218: step 12265, loss 0.554019.
Train: 2018-08-06T01:44:55.928558: step 12266, loss 0.554002.
Train: 2018-08-06T01:44:56.170941: step 12267, loss 0.553986.
Train: 2018-08-06T01:44:56.417277: step 12268, loss 0.537153.
Train: 2018-08-06T01:44:56.661631: step 12269, loss 0.562376.
Train: 2018-08-06T01:44:56.907972: step 12270, loss 0.553942.
Test: 2018-08-06T01:44:58.186520: step 12270, loss 0.548858.
Train: 2018-08-06T01:44:58.432861: step 12271, loss 0.545492.
Train: 2018-08-06T01:44:58.675214: step 12272, loss 0.503238.
Train: 2018-08-06T01:44:58.929562: step 12273, loss 0.613136.
Train: 2018-08-06T01:44:59.172883: step 12274, loss 0.460739.
Train: 2018-08-06T01:44:59.421255: step 12275, loss 0.545373.
Train: 2018-08-06T01:44:59.667559: step 12276, loss 0.587866.
Train: 2018-08-06T01:44:59.913900: step 12277, loss 0.545304.
Train: 2018-08-06T01:45:00.156252: step 12278, loss 0.5794.
Train: 2018-08-06T01:45:00.399601: step 12279, loss 0.613554.
Train: 2018-08-06T01:45:00.644945: step 12280, loss 0.562339.
Test: 2018-08-06T01:45:01.901584: step 12280, loss 0.547123.
Train: 2018-08-06T01:45:02.140999: step 12281, loss 0.519719.
Train: 2018-08-06T01:45:02.387286: step 12282, loss 0.596436.
Train: 2018-08-06T01:45:02.639640: step 12283, loss 0.54531.
Train: 2018-08-06T01:45:02.894956: step 12284, loss 0.579364.
Train: 2018-08-06T01:45:03.139300: step 12285, loss 0.570846.
Train: 2018-08-06T01:45:03.385616: step 12286, loss 0.502928.
Train: 2018-08-06T01:45:03.632985: step 12287, loss 0.528404.
Train: 2018-08-06T01:45:03.878337: step 12288, loss 0.604791.
Train: 2018-08-06T01:45:04.122679: step 12289, loss 0.638686.
Train: 2018-08-06T01:45:04.369018: step 12290, loss 0.536974.
Test: 2018-08-06T01:45:05.625625: step 12290, loss 0.548842.
Train: 2018-08-06T01:45:05.868003: step 12291, loss 0.596147.
Train: 2018-08-06T01:45:06.130277: step 12292, loss 0.545524.
Train: 2018-08-06T01:45:06.386590: step 12293, loss 0.545568.
Train: 2018-08-06T01:45:06.627944: step 12294, loss 0.528818.
Train: 2018-08-06T01:45:06.872291: step 12295, loss 0.53724.
Train: 2018-08-06T01:45:07.124617: step 12296, loss 0.595922.
Train: 2018-08-06T01:45:07.373951: step 12297, loss 0.595887.
Train: 2018-08-06T01:45:07.634296: step 12298, loss 0.554061.
Train: 2018-08-06T01:45:07.881593: step 12299, loss 0.537404.
Train: 2018-08-06T01:45:08.125938: step 12300, loss 0.570764.
Test: 2018-08-06T01:45:09.373602: step 12300, loss 0.548928.
Train: 2018-08-06T01:45:10.341565: step 12301, loss 0.537479.
Train: 2018-08-06T01:45:10.596882: step 12302, loss 0.545816.
Train: 2018-08-06T01:45:10.856160: step 12303, loss 0.595695.
Train: 2018-08-06T01:45:11.102532: step 12304, loss 0.554155.
Train: 2018-08-06T01:45:11.349841: step 12305, loss 0.520992.
Train: 2018-08-06T01:45:11.596181: step 12306, loss 0.562464.
Train: 2018-08-06T01:45:11.852521: step 12307, loss 0.54588.
Train: 2018-08-06T01:45:12.095845: step 12308, loss 0.512701.
Train: 2018-08-06T01:45:12.344180: step 12309, loss 0.471125.
Train: 2018-08-06T01:45:12.588528: step 12310, loss 0.512481.
Test: 2018-08-06T01:45:13.861123: step 12310, loss 0.548463.
Train: 2018-08-06T01:45:14.096495: step 12311, loss 0.537352.
Train: 2018-08-06T01:45:14.343832: step 12312, loss 0.562396.
Train: 2018-08-06T01:45:14.587182: step 12313, loss 0.503522.
Train: 2018-08-06T01:45:14.843496: step 12314, loss 0.562366.
Train: 2018-08-06T01:45:15.087843: step 12315, loss 0.562356.
Train: 2018-08-06T01:45:15.342163: step 12316, loss 0.545369.
Train: 2018-08-06T01:45:15.588504: step 12317, loss 0.579365.
Train: 2018-08-06T01:45:15.836840: step 12318, loss 0.49413.
Train: 2018-08-06T01:45:16.082183: step 12319, loss 0.562337.
Train: 2018-08-06T01:45:16.327529: step 12320, loss 0.553767.
Test: 2018-08-06T01:45:17.609100: step 12320, loss 0.549776.
Train: 2018-08-06T01:45:17.848486: step 12321, loss 0.631016.
Train: 2018-08-06T01:45:18.102811: step 12322, loss 0.545162.
Train: 2018-08-06T01:45:18.349152: step 12323, loss 0.562335.
Train: 2018-08-06T01:45:18.598490: step 12324, loss 0.639634.
Train: 2018-08-06T01:45:18.842801: step 12325, loss 0.545187.
Train: 2018-08-06T01:45:19.090165: step 12326, loss 0.528091.
Train: 2018-08-06T01:45:19.337478: step 12327, loss 0.605105.
Train: 2018-08-06T01:45:19.585834: step 12328, loss 0.59649.
Train: 2018-08-06T01:45:19.844124: step 12329, loss 0.647504.
Train: 2018-08-06T01:45:20.104478: step 12330, loss 0.621716.
Test: 2018-08-06T01:45:21.383008: step 12330, loss 0.548471.
Train: 2018-08-06T01:45:21.632341: step 12331, loss 0.528618.
Train: 2018-08-06T01:45:21.890649: step 12332, loss 0.54558.
Train: 2018-08-06T01:45:22.133999: step 12333, loss 0.646133.
Train: 2018-08-06T01:45:22.380371: step 12334, loss 0.67907.
Train: 2018-08-06T01:45:22.627680: step 12335, loss 0.55421.
Train: 2018-08-06T01:45:22.873053: step 12336, loss 0.554316.
Train: 2018-08-06T01:45:23.118392: step 12337, loss 0.58712.
Train: 2018-08-06T01:45:23.362739: step 12338, loss 0.546389.
Train: 2018-08-06T01:45:23.607084: step 12339, loss 0.611267.
Train: 2018-08-06T01:45:23.851407: step 12340, loss 0.530569.
Test: 2018-08-06T01:45:25.106051: step 12340, loss 0.550543.
Train: 2018-08-06T01:45:25.348403: step 12341, loss 0.53877.
Train: 2018-08-06T01:45:25.596766: step 12342, loss 0.530888.
Train: 2018-08-06T01:45:25.843093: step 12343, loss 0.507033.
Train: 2018-08-06T01:45:26.086430: step 12344, loss 0.538969.
Train: 2018-08-06T01:45:26.331773: step 12345, loss 0.594822.
Train: 2018-08-06T01:45:26.588087: step 12346, loss 0.570882.
Train: 2018-08-06T01:45:26.832434: step 12347, loss 0.531006.
Train: 2018-08-06T01:45:27.081785: step 12348, loss 0.58684.
Train: 2018-08-06T01:45:27.333146: step 12349, loss 0.538961.
Train: 2018-08-06T01:45:27.584424: step 12350, loss 0.554905.
Test: 2018-08-06T01:45:28.863003: step 12350, loss 0.548794.
Train: 2018-08-06T01:45:29.100368: step 12351, loss 0.578862.
Train: 2018-08-06T01:45:29.342752: step 12352, loss 0.602852.
Train: 2018-08-06T01:45:29.595047: step 12353, loss 0.610836.
Train: 2018-08-06T01:45:29.841415: step 12354, loss 0.522972.
Train: 2018-08-06T01:45:30.089724: step 12355, loss 0.530963.
Train: 2018-08-06T01:45:30.335092: step 12356, loss 0.57886.
Train: 2018-08-06T01:45:30.579413: step 12357, loss 0.594841.
Train: 2018-08-06T01:45:30.825785: step 12358, loss 0.594847.
Train: 2018-08-06T01:45:31.073128: step 12359, loss 0.522975.
Train: 2018-08-06T01:45:31.316473: step 12360, loss 0.546922.
Test: 2018-08-06T01:45:32.578068: step 12360, loss 0.549164.
Train: 2018-08-06T01:45:32.814437: step 12361, loss 0.658789.
Train: 2018-08-06T01:45:33.059780: step 12362, loss 0.586825.
Train: 2018-08-06T01:45:33.305143: step 12363, loss 0.586824.
Train: 2018-08-06T01:45:33.548474: step 12364, loss 0.562965.
Train: 2018-08-06T01:45:33.806782: step 12365, loss 0.578861.
Train: 2018-08-06T01:45:34.060129: step 12366, loss 0.539264.
Train: 2018-08-06T01:45:34.306449: step 12367, loss 0.586772.
Train: 2018-08-06T01:45:34.551821: step 12368, loss 0.523545.
Train: 2018-08-06T01:45:34.794142: step 12369, loss 0.634176.
Train: 2018-08-06T01:45:35.045470: step 12370, loss 0.563083.
Test: 2018-08-06T01:45:36.313079: step 12370, loss 0.549397.
Train: 2018-08-06T01:45:36.563414: step 12371, loss 0.507935.
Train: 2018-08-06T01:45:36.808755: step 12372, loss 0.618284.
Train: 2018-08-06T01:45:37.077037: step 12373, loss 0.610376.
Train: 2018-08-06T01:45:37.323409: step 12374, loss 0.515948.
Train: 2018-08-06T01:45:37.580690: step 12375, loss 0.57887.
Train: 2018-08-06T01:45:37.829026: step 12376, loss 0.555288.
Train: 2018-08-06T01:45:38.078359: step 12377, loss 0.57887.
Train: 2018-08-06T01:45:38.327694: step 12378, loss 0.56315.
Train: 2018-08-06T01:45:38.572039: step 12379, loss 0.53957.
Train: 2018-08-06T01:45:38.847302: step 12380, loss 0.665392.
Test: 2018-08-06T01:45:40.138848: step 12380, loss 0.549505.
Train: 2018-08-06T01:45:40.384192: step 12381, loss 0.578871.
Train: 2018-08-06T01:45:40.570718: step 12382, loss 0.579921.
Train: 2018-08-06T01:45:40.817035: step 12383, loss 0.563221.
Train: 2018-08-06T01:45:41.061415: step 12384, loss 0.539784.
Train: 2018-08-06T01:45:41.306750: step 12385, loss 0.547616.
Train: 2018-08-06T01:45:41.550099: step 12386, loss 0.586703.
Train: 2018-08-06T01:45:41.810378: step 12387, loss 0.578884.
Train: 2018-08-06T01:45:42.055722: step 12388, loss 0.524172.
Train: 2018-08-06T01:45:42.300099: step 12389, loss 0.508475.
Train: 2018-08-06T01:45:42.546409: step 12390, loss 0.586717.
Test: 2018-08-06T01:45:43.810031: step 12390, loss 0.550228.
Train: 2018-08-06T01:45:44.045432: step 12391, loss 0.531748.
Train: 2018-08-06T01:45:44.297756: step 12392, loss 0.578867.
Train: 2018-08-06T01:45:44.543069: step 12393, loss 0.649861.
Train: 2018-08-06T01:45:44.794409: step 12394, loss 0.539423.
Train: 2018-08-06T01:45:45.040739: step 12395, loss 0.586756.
Train: 2018-08-06T01:45:45.294061: step 12396, loss 0.563072.
Train: 2018-08-06T01:45:45.541400: step 12397, loss 0.499885.
Train: 2018-08-06T01:45:45.789736: step 12398, loss 0.602598.
Train: 2018-08-06T01:45:46.041089: step 12399, loss 0.531337.
Train: 2018-08-06T01:45:46.287404: step 12400, loss 0.562991.
Test: 2018-08-06T01:45:47.556011: step 12400, loss 0.549874.
Train: 2018-08-06T01:45:48.492827: step 12401, loss 0.586806.
Train: 2018-08-06T01:45:48.736171: step 12402, loss 0.547034.
Train: 2018-08-06T01:45:48.984505: step 12403, loss 0.554956.
Train: 2018-08-06T01:45:49.232850: step 12404, loss 0.57886.
Train: 2018-08-06T01:45:49.480156: step 12405, loss 0.546902.
Train: 2018-08-06T01:45:49.722532: step 12406, loss 0.578864.
Train: 2018-08-06T01:45:49.970843: step 12407, loss 0.546822.
Train: 2018-08-06T01:45:50.216229: step 12408, loss 0.530734.
Train: 2018-08-06T01:45:50.467545: step 12409, loss 0.570834.
Train: 2018-08-06T01:45:50.718887: step 12410, loss 0.619143.
Test: 2018-08-06T01:45:51.994444: step 12410, loss 0.549311.
Train: 2018-08-06T01:45:52.232834: step 12411, loss 0.53054.
Train: 2018-08-06T01:45:52.479146: step 12412, loss 0.554686.
Train: 2018-08-06T01:45:52.726516: step 12413, loss 0.562735.
Train: 2018-08-06T01:45:52.972858: step 12414, loss 0.514212.
Train: 2018-08-06T01:45:53.217199: step 12415, loss 0.595102.
Train: 2018-08-06T01:45:53.460523: step 12416, loss 0.578903.
Train: 2018-08-06T01:45:53.709856: step 12417, loss 0.538313.
Train: 2018-08-06T01:45:53.974148: step 12418, loss 0.530139.
Train: 2018-08-06T01:45:54.219527: step 12419, loss 0.587066.
Train: 2018-08-06T01:45:54.463839: step 12420, loss 0.578928.
Test: 2018-08-06T01:45:55.726462: step 12420, loss 0.548987.
Train: 2018-08-06T01:45:55.964856: step 12421, loss 0.562613.
Train: 2018-08-06T01:45:56.209196: step 12422, loss 0.595263.
Train: 2018-08-06T01:45:56.455538: step 12423, loss 0.611585.
Train: 2018-08-06T01:45:56.704876: step 12424, loss 0.505551.
Train: 2018-08-06T01:45:56.949223: step 12425, loss 0.578928.
Train: 2018-08-06T01:45:57.194562: step 12426, loss 0.587081.
Train: 2018-08-06T01:45:57.442901: step 12427, loss 0.578922.
Train: 2018-08-06T01:45:57.687247: step 12428, loss 0.619609.
Train: 2018-08-06T01:45:57.930599: step 12429, loss 0.570787.
Train: 2018-08-06T01:45:58.190902: step 12430, loss 0.546482.
Test: 2018-08-06T01:45:59.452498: step 12430, loss 0.549194.
Train: 2018-08-06T01:45:59.738733: step 12431, loss 0.546525.
Train: 2018-08-06T01:45:59.984107: step 12432, loss 0.530381.
Train: 2018-08-06T01:46:00.231439: step 12433, loss 0.522301.
Train: 2018-08-06T01:46:00.477787: step 12434, loss 0.53035.
Train: 2018-08-06T01:46:00.723130: step 12435, loss 0.578898.
Train: 2018-08-06T01:46:00.971462: step 12436, loss 0.546455.
Train: 2018-08-06T01:46:01.229777: step 12437, loss 0.473298.
Train: 2018-08-06T01:46:01.488056: step 12438, loss 0.546304.
Train: 2018-08-06T01:46:01.736389: step 12439, loss 0.578925.
Train: 2018-08-06T01:46:01.985749: step 12440, loss 0.554395.
Test: 2018-08-06T01:46:03.267295: step 12440, loss 0.549532.
Train: 2018-08-06T01:46:03.504662: step 12441, loss 0.521409.
Train: 2018-08-06T01:46:03.753029: step 12442, loss 0.496339.
Train: 2018-08-06T01:46:04.000336: step 12443, loss 0.554353.
Train: 2018-08-06T01:46:04.257648: step 12444, loss 0.529055.
Train: 2018-08-06T01:46:04.511966: step 12445, loss 0.545808.
Train: 2018-08-06T01:46:04.762325: step 12446, loss 0.503579.
Train: 2018-08-06T01:46:05.021604: step 12447, loss 0.604686.
Train: 2018-08-06T01:46:05.266976: step 12448, loss 0.561662.
Train: 2018-08-06T01:46:05.526280: step 12449, loss 0.562024.
Train: 2018-08-06T01:46:05.777583: step 12450, loss 0.563181.
Test: 2018-08-06T01:46:07.074115: step 12450, loss 0.547562.
Train: 2018-08-06T01:46:07.319489: step 12451, loss 0.579132.
Train: 2018-08-06T01:46:07.563836: step 12452, loss 0.588347.
Train: 2018-08-06T01:46:07.811170: step 12453, loss 0.595488.
Train: 2018-08-06T01:46:08.059510: step 12454, loss 0.57053.
Train: 2018-08-06T01:46:08.307830: step 12455, loss 0.528058.
Train: 2018-08-06T01:46:08.552161: step 12456, loss 0.587609.
Train: 2018-08-06T01:46:08.796509: step 12457, loss 0.66505.
Train: 2018-08-06T01:46:09.047867: step 12458, loss 0.587906.
Train: 2018-08-06T01:46:09.309164: step 12459, loss 0.553866.
Train: 2018-08-06T01:46:09.556476: step 12460, loss 0.478742.
Test: 2018-08-06T01:46:10.818102: step 12460, loss 0.548289.
Train: 2018-08-06T01:46:11.056465: step 12461, loss 0.545789.
Train: 2018-08-06T01:46:11.308790: step 12462, loss 0.5624.
Train: 2018-08-06T01:46:11.557127: step 12463, loss 0.529137.
Train: 2018-08-06T01:46:11.804464: step 12464, loss 0.579081.
Train: 2018-08-06T01:46:12.060810: step 12465, loss 0.595681.
Train: 2018-08-06T01:46:12.309115: step 12466, loss 0.562464.
Train: 2018-08-06T01:46:12.551467: step 12467, loss 0.521057.
Train: 2018-08-06T01:46:12.800830: step 12468, loss 0.554217.
Train: 2018-08-06T01:46:13.052128: step 12469, loss 0.579028.
Train: 2018-08-06T01:46:13.296508: step 12470, loss 0.612097.
Test: 2018-08-06T01:46:14.556105: step 12470, loss 0.548722.
Train: 2018-08-06T01:46:14.791507: step 12471, loss 0.60376.
Train: 2018-08-06T01:46:15.048816: step 12472, loss 0.480212.
Train: 2018-08-06T01:46:15.293135: step 12473, loss 0.562534.
Train: 2018-08-06T01:46:15.537481: step 12474, loss 0.546093.
Train: 2018-08-06T01:46:15.791831: step 12475, loss 0.513233.
Train: 2018-08-06T01:46:16.039164: step 12476, loss 0.562528.
Train: 2018-08-06T01:46:16.288506: step 12477, loss 0.504906.
Train: 2018-08-06T01:46:16.537839: step 12478, loss 0.57076.
Train: 2018-08-06T01:46:16.788136: step 12479, loss 0.587271.
Train: 2018-08-06T01:46:17.049466: step 12480, loss 0.562497.
Test: 2018-08-06T01:46:18.318044: step 12480, loss 0.54887.
Train: 2018-08-06T01:46:18.557436: step 12481, loss 0.521149.
Train: 2018-08-06T01:46:18.803772: step 12482, loss 0.54592.
Train: 2018-08-06T01:46:19.052107: step 12483, loss 0.579048.
Train: 2018-08-06T01:46:19.303422: step 12484, loss 0.504379.
Train: 2018-08-06T01:46:19.549750: step 12485, loss 0.470999.
Train: 2018-08-06T01:46:19.795095: step 12486, loss 0.520711.
Train: 2018-08-06T01:46:20.042464: step 12487, loss 0.554026.
Train: 2018-08-06T01:46:20.291784: step 12488, loss 0.587599.
Train: 2018-08-06T01:46:20.547084: step 12489, loss 0.612931.
Train: 2018-08-06T01:46:20.794462: step 12490, loss 0.587675.
Test: 2018-08-06T01:46:22.065024: step 12490, loss 0.547497.
Train: 2018-08-06T01:46:22.308398: step 12491, loss 0.604559.
Train: 2018-08-06T01:46:22.556737: step 12492, loss 0.612964.
Train: 2018-08-06T01:46:22.805090: step 12493, loss 0.570794.
Train: 2018-08-06T01:46:23.058392: step 12494, loss 0.553984.
Train: 2018-08-06T01:46:23.308727: step 12495, loss 0.554008.
Train: 2018-08-06T01:46:23.557063: step 12496, loss 0.570777.
Train: 2018-08-06T01:46:23.804402: step 12497, loss 0.487183.
Train: 2018-08-06T01:46:24.051711: step 12498, loss 0.537336.
Train: 2018-08-06T01:46:24.298084: step 12499, loss 0.620942.
Train: 2018-08-06T01:46:24.545415: step 12500, loss 0.645956.
Test: 2018-08-06T01:46:25.837934: step 12500, loss 0.547564.
Train: 2018-08-06T01:46:26.798775: step 12501, loss 0.604083.
Train: 2018-08-06T01:46:27.047080: step 12502, loss 0.545857.
Train: 2018-08-06T01:46:27.293422: step 12503, loss 0.620419.
Train: 2018-08-06T01:46:27.555744: step 12504, loss 0.554266.
Train: 2018-08-06T01:46:27.802090: step 12505, loss 0.505019.
Train: 2018-08-06T01:46:28.051425: step 12506, loss 0.587166.
Train: 2018-08-06T01:46:28.297765: step 12507, loss 0.562582.
Train: 2018-08-06T01:46:28.546104: step 12508, loss 0.587104.
Train: 2018-08-06T01:46:28.794436: step 12509, loss 0.587072.
Train: 2018-08-06T01:46:29.042742: step 12510, loss 0.505768.
Test: 2018-08-06T01:46:30.320326: step 12510, loss 0.548366.
Train: 2018-08-06T01:46:30.560713: step 12511, loss 0.554554.
Train: 2018-08-06T01:46:30.821016: step 12512, loss 0.570787.
Train: 2018-08-06T01:46:31.068326: step 12513, loss 0.595101.
Train: 2018-08-06T01:46:31.317659: step 12514, loss 0.611262.
Train: 2018-08-06T01:46:31.563999: step 12515, loss 0.603103.
Train: 2018-08-06T01:46:31.807378: step 12516, loss 0.554731.
Train: 2018-08-06T01:46:32.070674: step 12517, loss 0.546755.
Train: 2018-08-06T01:46:32.321010: step 12518, loss 0.64298.
Train: 2018-08-06T01:46:32.582324: step 12519, loss 0.554898.
Train: 2018-08-06T01:46:32.832607: step 12520, loss 0.570894.
Test: 2018-08-06T01:46:34.089246: step 12520, loss 0.549148.
Train: 2018-08-06T01:46:34.328608: step 12521, loss 0.547074.
Train: 2018-08-06T01:46:34.583924: step 12522, loss 0.515404.
Train: 2018-08-06T01:46:34.831263: step 12523, loss 0.610573.
Train: 2018-08-06T01:46:35.079599: step 12524, loss 0.555105.
Train: 2018-08-06T01:46:35.335944: step 12525, loss 0.507656.
Train: 2018-08-06T01:46:35.583281: step 12526, loss 0.563027.
Train: 2018-08-06T01:46:35.842558: step 12527, loss 0.499634.
Train: 2018-08-06T01:46:36.100867: step 12528, loss 0.626501.
Train: 2018-08-06T01:46:36.348231: step 12529, loss 0.562967.
Train: 2018-08-06T01:46:36.594547: step 12530, loss 0.58681.
Test: 2018-08-06T01:46:37.859164: step 12530, loss 0.550033.
Train: 2018-08-06T01:46:38.102514: step 12531, loss 0.586813.
Train: 2018-08-06T01:46:38.347889: step 12532, loss 0.586811.
Train: 2018-08-06T01:46:38.538349: step 12533, loss 0.579918.
Train: 2018-08-06T01:46:38.800677: step 12534, loss 0.531209.
Train: 2018-08-06T01:46:39.050983: step 12535, loss 0.610629.
Train: 2018-08-06T01:46:39.301339: step 12536, loss 0.634411.
Train: 2018-08-06T01:46:39.553669: step 12537, loss 0.539268.
Train: 2018-08-06T01:46:39.812976: step 12538, loss 0.523507.
Train: 2018-08-06T01:46:40.069259: step 12539, loss 0.634207.
Train: 2018-08-06T01:46:40.330585: step 12540, loss 0.547288.
Test: 2018-08-06T01:46:41.605152: step 12540, loss 0.54992.
Train: 2018-08-06T01:46:41.845519: step 12541, loss 0.531545.
Train: 2018-08-06T01:46:42.108814: step 12542, loss 0.570977.
Train: 2018-08-06T01:46:42.356184: step 12543, loss 0.523655.
Train: 2018-08-06T01:46:42.604519: step 12544, loss 0.563071.
Train: 2018-08-06T01:46:42.856842: step 12545, loss 0.555146.
Train: 2018-08-06T01:46:43.102188: step 12546, loss 0.602606.
Train: 2018-08-06T01:46:43.353486: step 12547, loss 0.578862.
Train: 2018-08-06T01:46:43.600824: step 12548, loss 0.523417.
Train: 2018-08-06T01:46:43.855144: step 12549, loss 0.555068.
Train: 2018-08-06T01:46:44.113453: step 12550, loss 0.539151.
Test: 2018-08-06T01:46:45.384055: step 12550, loss 0.54929.
Train: 2018-08-06T01:46:45.636380: step 12551, loss 0.570901.
Train: 2018-08-06T01:46:45.894690: step 12552, loss 0.594798.
Train: 2018-08-06T01:46:46.141067: step 12553, loss 0.538975.
Train: 2018-08-06T01:46:46.386375: step 12554, loss 0.610819.
Train: 2018-08-06T01:46:46.634710: step 12555, loss 0.586858.
Train: 2018-08-06T01:46:46.884044: step 12556, loss 0.522927.
Train: 2018-08-06T01:46:47.138394: step 12557, loss 0.514883.
Train: 2018-08-06T01:46:47.386699: step 12558, loss 0.578866.
Train: 2018-08-06T01:46:47.631046: step 12559, loss 0.474544.
Train: 2018-08-06T01:46:47.879412: step 12560, loss 0.51446.
Test: 2018-08-06T01:46:49.144997: step 12560, loss 0.549029.
Train: 2018-08-06T01:46:49.387375: step 12561, loss 0.627403.
Train: 2018-08-06T01:46:49.631720: step 12562, loss 0.578901.
Train: 2018-08-06T01:46:49.891002: step 12563, loss 0.587025.
Train: 2018-08-06T01:46:50.141341: step 12564, loss 0.530145.
Train: 2018-08-06T01:46:50.399642: step 12565, loss 0.57078.
Train: 2018-08-06T01:46:50.649972: step 12566, loss 0.530013.
Train: 2018-08-06T01:46:50.896314: step 12567, loss 0.636115.
Train: 2018-08-06T01:46:51.142655: step 12568, loss 0.587107.
Train: 2018-08-06T01:46:51.386029: step 12569, loss 0.521775.
Train: 2018-08-06T01:46:51.634364: step 12570, loss 0.546259.
Test: 2018-08-06T01:46:52.904941: step 12570, loss 0.548747.
Train: 2018-08-06T01:46:53.142332: step 12571, loss 0.554414.
Train: 2018-08-06T01:46:53.402642: step 12572, loss 0.595316.
Train: 2018-08-06T01:46:53.662961: step 12573, loss 0.513476.
Train: 2018-08-06T01:46:53.921224: step 12574, loss 0.603536.
Train: 2018-08-06T01:46:54.171574: step 12575, loss 0.619927.
Train: 2018-08-06T01:46:54.430861: step 12576, loss 0.587134.
Train: 2018-08-06T01:46:54.682189: step 12577, loss 0.603454.
Train: 2018-08-06T01:46:54.982880: step 12578, loss 0.538166.
Train: 2018-08-06T01:46:55.232221: step 12579, loss 0.570779.
Train: 2018-08-06T01:46:55.479559: step 12580, loss 0.611417.
Test: 2018-08-06T01:46:56.741152: step 12580, loss 0.549522.
Train: 2018-08-06T01:46:56.978543: step 12581, loss 0.603219.
Train: 2018-08-06T01:46:57.226882: step 12582, loss 0.570807.
Train: 2018-08-06T01:46:57.475190: step 12583, loss 0.603051.
Train: 2018-08-06T01:46:57.732501: step 12584, loss 0.522663.
Train: 2018-08-06T01:46:57.979866: step 12585, loss 0.642963.
Train: 2018-08-06T01:46:58.229207: step 12586, loss 0.507016.
Train: 2018-08-06T01:46:58.484521: step 12587, loss 0.578859.
Train: 2018-08-06T01:46:58.734854: step 12588, loss 0.515248.
Train: 2018-08-06T01:46:58.990171: step 12589, loss 0.531179.
Train: 2018-08-06T01:46:59.241490: step 12590, loss 0.562961.
Test: 2018-08-06T01:47:00.514062: step 12590, loss 0.549674.
Train: 2018-08-06T01:47:00.752426: step 12591, loss 0.515242.
Train: 2018-08-06T01:47:01.000794: step 12592, loss 0.642575.
Train: 2018-08-06T01:47:01.251122: step 12593, loss 0.539048.
Train: 2018-08-06T01:47:01.511428: step 12594, loss 0.523101.
Train: 2018-08-06T01:47:01.763721: step 12595, loss 0.554931.
Train: 2018-08-06T01:47:02.014051: step 12596, loss 0.594839.
Train: 2018-08-06T01:47:02.260419: step 12597, loss 0.594851.
Train: 2018-08-06T01:47:02.511746: step 12598, loss 0.547039.
Train: 2018-08-06T01:47:02.759098: step 12599, loss 0.594852.
Train: 2018-08-06T01:47:03.005399: step 12600, loss 0.562886.
Test: 2018-08-06T01:47:04.272012: step 12600, loss 0.548959.
Train: 2018-08-06T01:47:05.208505: step 12601, loss 0.570865.
Train: 2018-08-06T01:47:05.459858: step 12602, loss 0.522887.
Train: 2018-08-06T01:47:05.708170: step 12603, loss 0.530842.
Train: 2018-08-06T01:47:05.957527: step 12604, loss 0.514735.
Train: 2018-08-06T01:47:06.206860: step 12605, loss 0.522611.
Train: 2018-08-06T01:47:06.456202: step 12606, loss 0.546626.
Train: 2018-08-06T01:47:06.704505: step 12607, loss 0.514165.
Train: 2018-08-06T01:47:06.953838: step 12608, loss 0.554537.
Train: 2018-08-06T01:47:07.209171: step 12609, loss 0.578929.
Train: 2018-08-06T01:47:07.457522: step 12610, loss 0.570766.
Test: 2018-08-06T01:47:08.726099: step 12610, loss 0.549992.
Train: 2018-08-06T01:47:08.967479: step 12611, loss 0.570761.
Train: 2018-08-06T01:47:09.222770: step 12612, loss 0.59542.
Train: 2018-08-06T01:47:09.473101: step 12613, loss 0.537836.
Train: 2018-08-06T01:47:09.734428: step 12614, loss 0.595486.
Train: 2018-08-06T01:47:09.988721: step 12615, loss 0.546011.
Train: 2018-08-06T01:47:10.240068: step 12616, loss 0.587268.
Train: 2018-08-06T01:47:10.489383: step 12617, loss 0.587271.
Train: 2018-08-06T01:47:10.744731: step 12618, loss 0.595518.
Train: 2018-08-06T01:47:10.996034: step 12619, loss 0.603736.
Train: 2018-08-06T01:47:11.244389: step 12620, loss 0.529613.
Test: 2018-08-06T01:47:12.536906: step 12620, loss 0.548622.
Train: 2018-08-06T01:47:12.782251: step 12621, loss 0.611859.
Train: 2018-08-06T01:47:13.031584: step 12622, loss 0.53795.
Train: 2018-08-06T01:47:13.282936: step 12623, loss 0.529805.
Train: 2018-08-06T01:47:13.537262: step 12624, loss 0.505267.
Train: 2018-08-06T01:47:13.787588: step 12625, loss 0.537991.
Train: 2018-08-06T01:47:14.044900: step 12626, loss 0.578964.
Train: 2018-08-06T01:47:14.294238: step 12627, loss 0.595384.
Train: 2018-08-06T01:47:14.547560: step 12628, loss 0.595381.
Train: 2018-08-06T01:47:14.801883: step 12629, loss 0.603561.
Train: 2018-08-06T01:47:15.052181: step 12630, loss 0.562579.
Test: 2018-08-06T01:47:16.331758: step 12630, loss 0.550074.
Train: 2018-08-06T01:47:16.572115: step 12631, loss 0.570768.
Train: 2018-08-06T01:47:16.833445: step 12632, loss 0.619734.
Train: 2018-08-06T01:47:17.082750: step 12633, loss 0.538225.
Train: 2018-08-06T01:47:17.330116: step 12634, loss 0.505798.
Train: 2018-08-06T01:47:17.580452: step 12635, loss 0.595148.
Train: 2018-08-06T01:47:17.828788: step 12636, loss 0.570791.
Train: 2018-08-06T01:47:18.082103: step 12637, loss 0.61941.
Train: 2018-08-06T01:47:18.331411: step 12638, loss 0.562721.
Train: 2018-08-06T01:47:18.584733: step 12639, loss 0.611157.
Train: 2018-08-06T01:47:18.829080: step 12640, loss 0.54669.
Test: 2018-08-06T01:47:20.091702: step 12640, loss 0.548478.
Train: 2018-08-06T01:47:20.344029: step 12641, loss 0.490548.
Train: 2018-08-06T01:47:20.592364: step 12642, loss 0.562814.
Train: 2018-08-06T01:47:20.844688: step 12643, loss 0.514655.
Train: 2018-08-06T01:47:21.092027: step 12644, loss 0.490483.
Train: 2018-08-06T01:47:21.338402: step 12645, loss 0.490256.
Train: 2018-08-06T01:47:21.597700: step 12646, loss 0.578892.
Train: 2018-08-06T01:47:21.850000: step 12647, loss 0.554558.
Train: 2018-08-06T01:47:22.101354: step 12648, loss 0.521931.
Train: 2018-08-06T01:47:22.359638: step 12649, loss 0.546254.
Train: 2018-08-06T01:47:22.607001: step 12650, loss 0.636375.
Test: 2018-08-06T01:47:23.864612: step 12650, loss 0.548067.
Train: 2018-08-06T01:47:24.102975: step 12651, loss 0.554328.
Train: 2018-08-06T01:47:24.351310: step 12652, loss 0.529612.
Train: 2018-08-06T01:47:24.596679: step 12653, loss 0.504781.
Train: 2018-08-06T01:47:24.843027: step 12654, loss 0.603846.
Train: 2018-08-06T01:47:25.092330: step 12655, loss 0.570757.
Train: 2018-08-06T01:47:25.340667: step 12656, loss 0.579058.
Train: 2018-08-06T01:47:25.589000: step 12657, loss 0.512616.
Train: 2018-08-06T01:47:25.835374: step 12658, loss 0.545801.
Train: 2018-08-06T01:47:26.084701: step 12659, loss 0.570765.
Train: 2018-08-06T01:47:26.332015: step 12660, loss 0.620844.
Test: 2018-08-06T01:47:27.600622: step 12660, loss 0.548677.
Train: 2018-08-06T01:47:27.849985: step 12661, loss 0.554077.
Train: 2018-08-06T01:47:28.102281: step 12662, loss 0.579112.
Train: 2018-08-06T01:47:28.350615: step 12663, loss 0.570766.
Train: 2018-08-06T01:47:28.597954: step 12664, loss 0.620771.
Train: 2018-08-06T01:47:28.853271: step 12665, loss 0.545808.
Train: 2018-08-06T01:47:29.103603: step 12666, loss 0.579065.
Train: 2018-08-06T01:47:29.357946: step 12667, loss 0.512726.
Train: 2018-08-06T01:47:29.613266: step 12668, loss 0.570757.
Train: 2018-08-06T01:47:29.861575: step 12669, loss 0.59559.
Train: 2018-08-06T01:47:30.109910: step 12670, loss 0.479842.
Test: 2018-08-06T01:47:31.388491: step 12670, loss 0.548111.
Train: 2018-08-06T01:47:31.628850: step 12671, loss 0.529419.
Train: 2018-08-06T01:47:31.881173: step 12672, loss 0.537657.
Train: 2018-08-06T01:47:32.135525: step 12673, loss 0.529331.
Train: 2018-08-06T01:47:32.383830: step 12674, loss 0.570759.
Train: 2018-08-06T01:47:32.631168: step 12675, loss 0.537517.
Train: 2018-08-06T01:47:32.879543: step 12676, loss 0.554114.
Train: 2018-08-06T01:47:33.136844: step 12677, loss 0.495733.
Train: 2018-08-06T01:47:33.382188: step 12678, loss 0.562412.
Train: 2018-08-06T01:47:33.629497: step 12679, loss 0.570778.
Train: 2018-08-06T01:47:33.886838: step 12680, loss 0.520428.
Test: 2018-08-06T01:47:35.153422: step 12680, loss 0.548332.
Train: 2018-08-06T01:47:35.398797: step 12681, loss 0.596027.
Train: 2018-08-06T01:47:35.650095: step 12682, loss 0.604485.
Train: 2018-08-06T01:47:35.903416: step 12683, loss 0.646602.
Train: 2018-08-06T01:47:36.097921: step 12684, loss 0.454763.
Train: 2018-08-06T01:47:36.349224: step 12685, loss 0.604437.
Train: 2018-08-06T01:47:36.597562: step 12686, loss 0.596008.
Train: 2018-08-06T01:47:36.865867: step 12687, loss 0.470046.
Train: 2018-08-06T01:47:37.120162: step 12688, loss 0.520391.
Train: 2018-08-06T01:47:37.374484: step 12689, loss 0.604427.
Train: 2018-08-06T01:47:37.622828: step 12690, loss 0.486695.
Test: 2018-08-06T01:47:38.887436: step 12690, loss 0.547341.
Train: 2018-08-06T01:47:39.138796: step 12691, loss 0.604484.
Train: 2018-08-06T01:47:39.388122: step 12692, loss 0.562374.
Train: 2018-08-06T01:47:39.638428: step 12693, loss 0.62135.
Train: 2018-08-06T01:47:39.882800: step 12694, loss 0.562379.
Train: 2018-08-06T01:47:40.137125: step 12695, loss 0.495151.
Train: 2018-08-06T01:47:40.399419: step 12696, loss 0.528764.
Train: 2018-08-06T01:47:40.643770: step 12697, loss 0.587611.
Train: 2018-08-06T01:47:40.891108: step 12698, loss 0.596019.
Train: 2018-08-06T01:47:41.141408: step 12699, loss 0.663205.
Train: 2018-08-06T01:47:41.401713: step 12700, loss 0.512142.
Test: 2018-08-06T01:47:42.662341: step 12700, loss 0.548447.
Train: 2018-08-06T01:47:43.558677: step 12701, loss 0.579133.
Train: 2018-08-06T01:47:43.810008: step 12702, loss 0.587454.
Train: 2018-08-06T01:47:44.057341: step 12703, loss 0.520825.
Train: 2018-08-06T01:47:44.306649: step 12704, loss 0.57907.
Train: 2018-08-06T01:47:44.554018: step 12705, loss 0.570758.
Train: 2018-08-06T01:47:44.804349: step 12706, loss 0.579037.
Train: 2018-08-06T01:47:45.050659: step 12707, loss 0.52118.
Train: 2018-08-06T01:47:45.300991: step 12708, loss 0.603771.
Train: 2018-08-06T01:47:45.561322: step 12709, loss 0.546043.
Train: 2018-08-06T01:47:45.813618: step 12710, loss 0.554306.
Test: 2018-08-06T01:47:47.071255: step 12710, loss 0.549387.
Train: 2018-08-06T01:47:47.316600: step 12711, loss 0.59541.
Train: 2018-08-06T01:47:47.562971: step 12712, loss 0.554357.
Train: 2018-08-06T01:47:47.814301: step 12713, loss 0.546193.
Train: 2018-08-06T01:47:48.062605: step 12714, loss 0.513489.
Train: 2018-08-06T01:47:48.326924: step 12715, loss 0.570766.
Train: 2018-08-06T01:47:48.588226: step 12716, loss 0.652598.
Train: 2018-08-06T01:47:48.837566: step 12717, loss 0.497267.
Train: 2018-08-06T01:47:49.097862: step 12718, loss 0.54628.
Train: 2018-08-06T01:47:49.352156: step 12719, loss 0.619753.
Train: 2018-08-06T01:47:49.602488: step 12720, loss 0.554467.
Test: 2018-08-06T01:47:50.871093: step 12720, loss 0.547902.
Train: 2018-08-06T01:47:51.109462: step 12721, loss 0.595214.
Train: 2018-08-06T01:47:51.362779: step 12722, loss 0.570782.
Train: 2018-08-06T01:47:51.623082: step 12723, loss 0.562667.
Train: 2018-08-06T01:47:51.874436: step 12724, loss 0.578901.
Train: 2018-08-06T01:47:52.120752: step 12725, loss 0.562703.
Train: 2018-08-06T01:47:52.370113: step 12726, loss 0.635485.
Train: 2018-08-06T01:47:52.619451: step 12727, loss 0.659504.
Train: 2018-08-06T01:47:52.867753: step 12728, loss 0.562818.
Train: 2018-08-06T01:47:53.113097: step 12729, loss 0.562877.
Train: 2018-08-06T01:47:53.371431: step 12730, loss 0.586823.
Test: 2018-08-06T01:47:54.643006: step 12730, loss 0.54846.
Train: 2018-08-06T01:47:54.891375: step 12731, loss 0.547121.
Train: 2018-08-06T01:47:55.137713: step 12732, loss 0.602596.
Train: 2018-08-06T01:47:55.385048: step 12733, loss 0.665608.
Train: 2018-08-06T01:47:55.632385: step 12734, loss 0.571029.
Train: 2018-08-06T01:47:55.889703: step 12735, loss 0.539847.
Train: 2018-08-06T01:47:56.140021: step 12736, loss 0.602245.
Train: 2018-08-06T01:47:56.388338: step 12737, loss 0.516906.
Train: 2018-08-06T01:47:56.637702: step 12738, loss 0.586664.
Train: 2018-08-06T01:47:56.887005: step 12739, loss 0.501757.
Train: 2018-08-06T01:47:57.135367: step 12740, loss 0.509492.
Test: 2018-08-06T01:47:58.411926: step 12740, loss 0.550219.
Train: 2018-08-06T01:47:58.661262: step 12741, loss 0.509389.
Train: 2018-08-06T01:47:58.909596: step 12742, loss 0.493684.
Train: 2018-08-06T01:47:59.170922: step 12743, loss 0.555552.
Train: 2018-08-06T01:47:59.419283: step 12744, loss 0.500718.
Train: 2018-08-06T01:47:59.678568: step 12745, loss 0.53171.
Train: 2018-08-06T01:47:59.923883: step 12746, loss 0.53933.
Train: 2018-08-06T01:48:00.179201: step 12747, loss 0.634531.
Train: 2018-08-06T01:48:00.427561: step 12748, loss 0.546927.
Train: 2018-08-06T01:48:00.671883: step 12749, loss 0.546808.
Train: 2018-08-06T01:48:00.919249: step 12750, loss 0.570829.
Test: 2018-08-06T01:48:02.189823: step 12750, loss 0.549255.
Train: 2018-08-06T01:48:02.479050: step 12751, loss 0.538518.
Train: 2018-08-06T01:48:02.728383: step 12752, loss 0.522179.
Train: 2018-08-06T01:48:02.978742: step 12753, loss 0.562644.
Train: 2018-08-06T01:48:03.226080: step 12754, loss 0.529935.
Train: 2018-08-06T01:48:03.476383: step 12755, loss 0.554362.
Train: 2018-08-06T01:48:03.724718: step 12756, loss 0.58722.
Train: 2018-08-06T01:48:03.973054: step 12757, loss 0.545994.
Train: 2018-08-06T01:48:04.218399: step 12758, loss 0.545924.
Train: 2018-08-06T01:48:04.466759: step 12759, loss 0.462848.
Train: 2018-08-06T01:48:04.715069: step 12760, loss 0.529074.
Test: 2018-08-06T01:48:05.982680: step 12760, loss 0.549375.
Train: 2018-08-06T01:48:06.220075: step 12761, loss 0.528894.
Train: 2018-08-06T01:48:06.467422: step 12762, loss 0.553961.
Train: 2018-08-06T01:48:06.717741: step 12763, loss 0.528554.
Train: 2018-08-06T01:48:06.966049: step 12764, loss 0.630248.
Train: 2018-08-06T01:48:07.216405: step 12765, loss 0.511283.
Train: 2018-08-06T01:48:07.462750: step 12766, loss 0.502579.
Train: 2018-08-06T01:48:07.713085: step 12767, loss 0.596642.
Train: 2018-08-06T01:48:07.962411: step 12768, loss 0.553694.
Train: 2018-08-06T01:48:08.209727: step 12769, loss 0.588114.
Train: 2018-08-06T01:48:08.468034: step 12770, loss 0.553747.
Test: 2018-08-06T01:48:09.728661: step 12770, loss 0.547532.
Train: 2018-08-06T01:48:09.969019: step 12771, loss 0.657396.
Train: 2018-08-06T01:48:10.214394: step 12772, loss 0.588178.
Train: 2018-08-06T01:48:10.461726: step 12773, loss 0.596693.
Train: 2018-08-06T01:48:10.708073: step 12774, loss 0.510946.
Train: 2018-08-06T01:48:10.960401: step 12775, loss 0.570888.
Train: 2018-08-06T01:48:11.207739: step 12776, loss 0.528198.
Train: 2018-08-06T01:48:11.456042: step 12777, loss 0.502656.
Train: 2018-08-06T01:48:11.702401: step 12778, loss 0.57938.
Train: 2018-08-06T01:48:11.950750: step 12779, loss 0.545293.
Train: 2018-08-06T01:48:12.198088: step 12780, loss 0.545298.
Test: 2018-08-06T01:48:13.468659: step 12780, loss 0.548119.
Train: 2018-08-06T01:48:13.707053: step 12781, loss 0.511245.
Train: 2018-08-06T01:48:13.952366: step 12782, loss 0.54533.
Train: 2018-08-06T01:48:14.199729: step 12783, loss 0.528223.
Train: 2018-08-06T01:48:14.457017: step 12784, loss 0.59644.
Train: 2018-08-06T01:48:14.702389: step 12785, loss 0.656276.
Train: 2018-08-06T01:48:14.949727: step 12786, loss 0.596402.
Train: 2018-08-06T01:48:15.194074: step 12787, loss 0.647331.
Train: 2018-08-06T01:48:15.449391: step 12788, loss 0.621619.
Train: 2018-08-06T01:48:15.696710: step 12789, loss 0.562373.
Train: 2018-08-06T01:48:15.948028: step 12790, loss 0.604277.
Test: 2018-08-06T01:48:17.206663: step 12790, loss 0.547374.
Train: 2018-08-06T01:48:17.444028: step 12791, loss 0.620742.
Train: 2018-08-06T01:48:17.693392: step 12792, loss 0.587316.
Train: 2018-08-06T01:48:17.939733: step 12793, loss 0.537852.
Train: 2018-08-06T01:48:18.186043: step 12794, loss 0.464365.
Train: 2018-08-06T01:48:18.429423: step 12795, loss 0.521786.
Train: 2018-08-06T01:48:18.676757: step 12796, loss 0.668611.
Train: 2018-08-06T01:48:18.925100: step 12797, loss 0.53016.
Train: 2018-08-06T01:48:19.174401: step 12798, loss 0.595121.
Train: 2018-08-06T01:48:19.421765: step 12799, loss 0.570815.
Train: 2018-08-06T01:48:19.669077: step 12800, loss 0.546643.
Test: 2018-08-06T01:48:20.932698: step 12800, loss 0.549534.
Train: 2018-08-06T01:48:21.806730: step 12801, loss 0.554745.
Train: 2018-08-06T01:48:22.054066: step 12802, loss 0.514596.
Train: 2018-08-06T01:48:22.311354: step 12803, loss 0.594932.
Train: 2018-08-06T01:48:22.554702: step 12804, loss 0.538783.
Train: 2018-08-06T01:48:22.802083: step 12805, loss 0.610963.
Train: 2018-08-06T01:48:23.051405: step 12806, loss 0.586918.
Train: 2018-08-06T01:48:23.301735: step 12807, loss 0.562849.
Train: 2018-08-06T01:48:23.549074: step 12808, loss 0.58683.
Train: 2018-08-06T01:48:23.801399: step 12809, loss 0.618716.
Train: 2018-08-06T01:48:24.058680: step 12810, loss 0.507287.
Test: 2018-08-06T01:48:25.318311: step 12810, loss 0.549516.
Train: 2018-08-06T01:48:25.569639: step 12811, loss 0.602681.
Train: 2018-08-06T01:48:25.817011: step 12812, loss 0.602645.
Train: 2018-08-06T01:48:26.064388: step 12813, loss 0.563038.
Train: 2018-08-06T01:48:26.315712: step 12814, loss 0.594664.
Train: 2018-08-06T01:48:26.563060: step 12815, loss 0.500047.
Train: 2018-08-06T01:48:26.812390: step 12816, loss 0.657654.
Train: 2018-08-06T01:48:27.061691: step 12817, loss 0.500284.
Train: 2018-08-06T01:48:27.309030: step 12818, loss 0.578858.
Train: 2018-08-06T01:48:27.555401: step 12819, loss 0.516074.
Train: 2018-08-06T01:48:27.817700: step 12820, loss 0.547449.
Test: 2018-08-06T01:48:29.084282: step 12820, loss 0.549641.
Train: 2018-08-06T01:48:29.318702: step 12821, loss 0.515943.
Train: 2018-08-06T01:48:29.566022: step 12822, loss 0.523671.
Train: 2018-08-06T01:48:29.813363: step 12823, loss 0.539271.
Train: 2018-08-06T01:48:30.070655: step 12824, loss 0.570969.
Train: 2018-08-06T01:48:30.323967: step 12825, loss 0.531058.
Train: 2018-08-06T01:48:30.569310: step 12826, loss 0.55485.
Train: 2018-08-06T01:48:30.813657: step 12827, loss 0.546663.
Train: 2018-08-06T01:48:31.058004: step 12828, loss 0.498209.
Train: 2018-08-06T01:48:31.304344: step 12829, loss 0.538455.
Train: 2018-08-06T01:48:31.558665: step 12830, loss 0.595219.
Test: 2018-08-06T01:48:32.831261: step 12830, loss 0.549574.
Train: 2018-08-06T01:48:33.070651: step 12831, loss 0.537567.
Train: 2018-08-06T01:48:33.320983: step 12832, loss 0.595142.
Train: 2018-08-06T01:48:33.567319: step 12833, loss 0.527949.
Train: 2018-08-06T01:48:33.816659: step 12834, loss 0.51032.
Train: 2018-08-06T01:48:34.007147: step 12835, loss 0.598535.
Train: 2018-08-06T01:48:34.256481: step 12836, loss 0.595754.
Train: 2018-08-06T01:48:34.504810: step 12837, loss 0.522071.
Train: 2018-08-06T01:48:34.753139: step 12838, loss 0.558091.
Train: 2018-08-06T01:48:35.003451: step 12839, loss 0.577203.
Train: 2018-08-06T01:48:35.247798: step 12840, loss 0.588177.
Test: 2018-08-06T01:48:36.538347: step 12840, loss 0.547731.
Train: 2018-08-06T01:48:36.779701: step 12841, loss 0.615765.
Train: 2018-08-06T01:48:37.041028: step 12842, loss 0.640376.
Train: 2018-08-06T01:48:37.298315: step 12843, loss 0.452136.
Train: 2018-08-06T01:48:37.540667: step 12844, loss 0.494489.
Train: 2018-08-06T01:48:37.787016: step 12845, loss 0.5454.
Train: 2018-08-06T01:48:38.034346: step 12846, loss 0.612357.
Train: 2018-08-06T01:48:38.281684: step 12847, loss 0.586986.
Train: 2018-08-06T01:48:38.530022: step 12848, loss 0.520053.
Train: 2018-08-06T01:48:38.775393: step 12849, loss 0.562775.
Train: 2018-08-06T01:48:39.018719: step 12850, loss 0.521228.
Test: 2018-08-06T01:48:40.289315: step 12850, loss 0.548698.
Train: 2018-08-06T01:48:40.535658: step 12851, loss 0.6208.
Train: 2018-08-06T01:48:40.783030: step 12852, loss 0.529091.
Train: 2018-08-06T01:48:41.039309: step 12853, loss 0.487599.
Train: 2018-08-06T01:48:41.286679: step 12854, loss 0.570532.
Train: 2018-08-06T01:48:41.531026: step 12855, loss 0.620705.
Train: 2018-08-06T01:48:41.777382: step 12856, loss 0.496172.
Train: 2018-08-06T01:48:42.019719: step 12857, loss 0.461978.
Train: 2018-08-06T01:48:42.269022: step 12858, loss 0.529567.
Train: 2018-08-06T01:48:42.517384: step 12859, loss 0.528709.
Train: 2018-08-06T01:48:42.764696: step 12860, loss 0.57098.
Test: 2018-08-06T01:48:44.022332: step 12860, loss 0.548226.
Train: 2018-08-06T01:48:44.267704: step 12861, loss 0.52013.
Train: 2018-08-06T01:48:44.517011: step 12862, loss 0.503255.
Train: 2018-08-06T01:48:44.764374: step 12863, loss 0.596632.
Train: 2018-08-06T01:48:45.012683: step 12864, loss 0.613968.
Train: 2018-08-06T01:48:45.271990: step 12865, loss 0.528432.
Train: 2018-08-06T01:48:45.520326: step 12866, loss 0.563065.
Train: 2018-08-06T01:48:45.766667: step 12867, loss 0.578895.
Train: 2018-08-06T01:48:46.014036: step 12868, loss 0.562061.
Train: 2018-08-06T01:48:46.260347: step 12869, loss 0.528251.
Train: 2018-08-06T01:48:46.505691: step 12870, loss 0.587559.
Test: 2018-08-06T01:48:47.783274: step 12870, loss 0.547288.
Train: 2018-08-06T01:48:48.030643: step 12871, loss 0.579471.
Train: 2018-08-06T01:48:48.279972: step 12872, loss 0.570877.
Train: 2018-08-06T01:48:48.527285: step 12873, loss 0.621876.
Train: 2018-08-06T01:48:48.770633: step 12874, loss 0.638964.
Train: 2018-08-06T01:48:49.022959: step 12875, loss 0.56249.
Train: 2018-08-06T01:48:49.271294: step 12876, loss 0.537189.
Train: 2018-08-06T01:48:49.520654: step 12877, loss 0.570606.
Train: 2018-08-06T01:48:49.769993: step 12878, loss 0.495794.
Train: 2018-08-06T01:48:50.015337: step 12879, loss 0.504281.
Train: 2018-08-06T01:48:50.263675: step 12880, loss 0.562446.
Test: 2018-08-06T01:48:51.531250: step 12880, loss 0.548373.
Train: 2018-08-06T01:48:51.774615: step 12881, loss 0.620583.
Train: 2018-08-06T01:48:52.017950: step 12882, loss 0.512679.
Train: 2018-08-06T01:48:52.268306: step 12883, loss 0.603901.
Train: 2018-08-06T01:48:52.518609: step 12884, loss 0.554152.
Train: 2018-08-06T01:48:52.764951: step 12885, loss 0.554231.
Train: 2018-08-06T01:48:53.007329: step 12886, loss 0.55426.
Train: 2018-08-06T01:48:53.258675: step 12887, loss 0.513021.
Train: 2018-08-06T01:48:53.507993: step 12888, loss 0.620311.
Train: 2018-08-06T01:48:53.776577: step 12889, loss 0.603769.
Train: 2018-08-06T01:48:54.022920: step 12890, loss 0.546051.
Test: 2018-08-06T01:48:55.321448: step 12890, loss 0.548413.
Train: 2018-08-06T01:48:55.559843: step 12891, loss 0.636576.
Train: 2018-08-06T01:48:55.864366: step 12892, loss 0.58717.
Train: 2018-08-06T01:48:56.109735: step 12893, loss 0.570767.
Train: 2018-08-06T01:48:56.359044: step 12894, loss 0.668642.
Train: 2018-08-06T01:48:56.604419: step 12895, loss 0.52209.
Train: 2018-08-06T01:48:56.867714: step 12896, loss 0.498009.
Train: 2018-08-06T01:48:57.113029: step 12897, loss 0.506216.
Train: 2018-08-06T01:48:57.358372: step 12898, loss 0.595029.
Train: 2018-08-06T01:48:57.604743: step 12899, loss 0.538558.
Train: 2018-08-06T01:48:57.855043: step 12900, loss 0.52244.
Test: 2018-08-06T01:48:59.128637: step 12900, loss 0.549271.
Train: 2018-08-06T01:49:00.049019: step 12901, loss 0.627293.
Train: 2018-08-06T01:49:00.294414: step 12902, loss 0.514377.
Train: 2018-08-06T01:49:00.541734: step 12903, loss 0.586945.
Train: 2018-08-06T01:49:00.798041: step 12904, loss 0.570816.
Train: 2018-08-06T01:49:01.046379: step 12905, loss 0.595008.
Train: 2018-08-06T01:49:01.292724: step 12906, loss 0.522475.
Train: 2018-08-06T01:49:01.539066: step 12907, loss 0.554701.
Train: 2018-08-06T01:49:01.783382: step 12908, loss 0.586943.
Train: 2018-08-06T01:49:02.027752: step 12909, loss 0.538567.
Train: 2018-08-06T01:49:02.274069: step 12910, loss 0.651481.
Test: 2018-08-06T01:49:03.546665: step 12910, loss 0.54876.
Train: 2018-08-06T01:49:03.791040: step 12911, loss 0.562771.
Train: 2018-08-06T01:49:04.047352: step 12912, loss 0.562786.
Train: 2018-08-06T01:49:04.291703: step 12913, loss 0.522613.
Train: 2018-08-06T01:49:04.537048: step 12914, loss 0.578869.
Train: 2018-08-06T01:49:04.789367: step 12915, loss 0.59494.
Train: 2018-08-06T01:49:05.040669: step 12916, loss 0.57084.
Train: 2018-08-06T01:49:05.284049: step 12917, loss 0.626992.
Train: 2018-08-06T01:49:05.532385: step 12918, loss 0.570861.
Train: 2018-08-06T01:49:05.777727: step 12919, loss 0.562888.
Train: 2018-08-06T01:49:06.035015: step 12920, loss 0.546968.
Test: 2018-08-06T01:49:07.306609: step 12920, loss 0.549811.
Train: 2018-08-06T01:49:07.554965: step 12921, loss 0.570897.
Train: 2018-08-06T01:49:07.804309: step 12922, loss 0.586816.
Train: 2018-08-06T01:49:08.059597: step 12923, loss 0.626532.
Train: 2018-08-06T01:49:08.304970: step 12924, loss 0.634341.
Train: 2018-08-06T01:49:08.550309: step 12925, loss 0.452519.
Train: 2018-08-06T01:49:08.795628: step 12926, loss 0.531517.
Train: 2018-08-06T01:49:09.045975: step 12927, loss 0.539402.
Train: 2018-08-06T01:49:09.289308: step 12928, loss 0.59466.
Train: 2018-08-06T01:49:09.544655: step 12929, loss 0.507752.
Train: 2018-08-06T01:49:09.805926: step 12930, loss 0.483895.
Test: 2018-08-06T01:49:11.062565: step 12930, loss 0.549535.
Train: 2018-08-06T01:49:11.301925: step 12931, loss 0.555039.
Train: 2018-08-06T01:49:11.545299: step 12932, loss 0.51513.
Train: 2018-08-06T01:49:11.791646: step 12933, loss 0.46688.
Train: 2018-08-06T01:49:12.035994: step 12934, loss 0.522556.
Train: 2018-08-06T01:49:12.291279: step 12935, loss 0.514135.
Train: 2018-08-06T01:49:12.539616: step 12936, loss 0.611516.
Train: 2018-08-06T01:49:12.787951: step 12937, loss 0.595327.
Train: 2018-08-06T01:49:13.045293: step 12938, loss 0.546107.
Train: 2018-08-06T01:49:13.289634: step 12939, loss 0.546016.
Train: 2018-08-06T01:49:13.535982: step 12940, loss 0.537655.
Test: 2018-08-06T01:49:14.791592: step 12940, loss 0.548584.
Train: 2018-08-06T01:49:15.028958: step 12941, loss 0.504319.
Train: 2018-08-06T01:49:15.274358: step 12942, loss 0.545744.
Train: 2018-08-06T01:49:15.519670: step 12943, loss 0.654521.
Train: 2018-08-06T01:49:15.768005: step 12944, loss 0.595948.
Train: 2018-08-06T01:49:16.013351: step 12945, loss 0.486834.
Train: 2018-08-06T01:49:16.267671: step 12946, loss 0.570792.
Train: 2018-08-06T01:49:16.519009: step 12947, loss 0.570799.
Train: 2018-08-06T01:49:16.780275: step 12948, loss 0.629836.
Train: 2018-08-06T01:49:17.027613: step 12949, loss 0.461223.
Train: 2018-08-06T01:49:17.272957: step 12950, loss 0.494843.
Test: 2018-08-06T01:49:18.534582: step 12950, loss 0.548813.
Train: 2018-08-06T01:49:18.770950: step 12951, loss 0.587739.
Train: 2018-08-06T01:49:19.017291: step 12952, loss 0.545408.
Train: 2018-08-06T01:49:19.275601: step 12953, loss 0.56235.
Train: 2018-08-06T01:49:19.522964: step 12954, loss 0.57084.
Train: 2018-08-06T01:49:19.769284: step 12955, loss 0.638855.
Train: 2018-08-06T01:49:20.017616: step 12956, loss 0.562346.
Train: 2018-08-06T01:49:20.262986: step 12957, loss 0.579316.
Train: 2018-08-06T01:49:20.513291: step 12958, loss 0.570823.
Train: 2018-08-06T01:49:20.758635: step 12959, loss 0.553903.
Train: 2018-08-06T01:49:21.001014: step 12960, loss 0.655237.
Test: 2018-08-06T01:49:22.264607: step 12960, loss 0.548906.
Train: 2018-08-06T01:49:22.501990: step 12961, loss 0.570796.
Train: 2018-08-06T01:49:22.743359: step 12962, loss 0.503688.
Train: 2018-08-06T01:49:22.990666: step 12963, loss 0.595886.
Train: 2018-08-06T01:49:23.244013: step 12964, loss 0.545721.
Train: 2018-08-06T01:49:23.490359: step 12965, loss 0.562432.
Train: 2018-08-06T01:49:23.746675: step 12966, loss 0.562446.
Train: 2018-08-06T01:49:23.992013: step 12967, loss 0.562459.
Train: 2018-08-06T01:49:24.238329: step 12968, loss 0.653612.
Train: 2018-08-06T01:49:24.485697: step 12969, loss 0.529475.
Train: 2018-08-06T01:49:24.733031: step 12970, loss 0.554286.
Test: 2018-08-06T01:49:26.006599: step 12970, loss 0.548817.
Train: 2018-08-06T01:49:26.245974: step 12971, loss 0.578977.
Train: 2018-08-06T01:49:26.499312: step 12972, loss 0.554365.
Train: 2018-08-06T01:49:26.750610: step 12973, loss 0.505306.
Train: 2018-08-06T01:49:27.000968: step 12974, loss 0.529879.
Train: 2018-08-06T01:49:27.262241: step 12975, loss 0.578945.
Train: 2018-08-06T01:49:27.505621: step 12976, loss 0.529885.
Train: 2018-08-06T01:49:27.748970: step 12977, loss 0.636208.
Train: 2018-08-06T01:49:27.990322: step 12978, loss 0.562598.
Train: 2018-08-06T01:49:28.250599: step 12979, loss 0.587096.
Train: 2018-08-06T01:49:28.509936: step 12980, loss 0.603378.
Test: 2018-08-06T01:49:29.783499: step 12980, loss 0.548691.
Train: 2018-08-06T01:49:30.016875: step 12981, loss 0.587048.
Train: 2018-08-06T01:49:30.260255: step 12982, loss 0.59513.
Train: 2018-08-06T01:49:30.511580: step 12983, loss 0.570802.
Train: 2018-08-06T01:49:30.756921: step 12984, loss 0.603088.
Train: 2018-08-06T01:49:31.004235: step 12985, loss 0.514538.
Train: 2018-08-06T01:49:31.192755: step 12986, loss 0.511441.
Train: 2018-08-06T01:49:31.451039: step 12987, loss 0.538752.
Train: 2018-08-06T01:49:31.708376: step 12988, loss 0.538746.
Train: 2018-08-06T01:49:31.950728: step 12989, loss 0.602961.
Train: 2018-08-06T01:49:32.193086: step 12990, loss 0.691279.
Test: 2018-08-06T01:49:33.454681: step 12990, loss 0.549848.
Train: 2018-08-06T01:49:33.696035: step 12991, loss 0.618898.
Train: 2018-08-06T01:49:33.940407: step 12992, loss 0.523017.
Train: 2018-08-06T01:49:34.197696: step 12993, loss 0.554982.
Train: 2018-08-06T01:49:34.444035: step 12994, loss 0.634471.
Train: 2018-08-06T01:49:34.694378: step 12995, loss 0.58678.
Train: 2018-08-06T01:49:34.940733: step 12996, loss 0.626234.
Train: 2018-08-06T01:49:35.201055: step 12997, loss 0.555281.
Train: 2018-08-06T01:49:35.458347: step 12998, loss 0.633732.
Train: 2018-08-06T01:49:35.705661: step 12999, loss 0.586693.
Train: 2018-08-06T01:49:35.951036: step 13000, loss 0.5323.
Test: 2018-08-06T01:49:37.227592: step 13000, loss 0.550141.
Train: 2018-08-06T01:49:38.110079: step 13001, loss 0.478237.
Train: 2018-08-06T01:49:38.356395: step 13002, loss 0.54796.
Train: 2018-08-06T01:49:38.604769: step 13003, loss 0.532468.
Train: 2018-08-06T01:49:38.853099: step 13004, loss 0.493655.
Train: 2018-08-06T01:49:39.097414: step 13005, loss 0.54004.
Train: 2018-08-06T01:49:39.346775: step 13006, loss 0.555496.
Train: 2018-08-06T01:49:39.591094: step 13007, loss 0.578882.
Train: 2018-08-06T01:49:39.838431: step 13008, loss 0.578874.
Train: 2018-08-06T01:49:40.084797: step 13009, loss 0.547406.
Train: 2018-08-06T01:49:40.329150: step 13010, loss 0.539431.
Test: 2018-08-06T01:49:41.617673: step 13010, loss 0.550556.
Train: 2018-08-06T01:49:41.856068: step 13011, loss 0.626325.
Train: 2018-08-06T01:49:42.102435: step 13012, loss 0.555091.
Train: 2018-08-06T01:49:42.348718: step 13013, loss 0.523308.
Train: 2018-08-06T01:49:42.595059: step 13014, loss 0.562948.
Train: 2018-08-06T01:49:42.844425: step 13015, loss 0.554938.
Train: 2018-08-06T01:49:43.087742: step 13016, loss 0.570869.
Train: 2018-08-06T01:49:43.337075: step 13017, loss 0.514789.
Train: 2018-08-06T01:49:43.583416: step 13018, loss 0.586904.
Train: 2018-08-06T01:49:43.835742: step 13019, loss 0.594978.
Train: 2018-08-06T01:49:44.083113: step 13020, loss 0.570818.
Test: 2018-08-06T01:49:45.371634: step 13020, loss 0.550564.
Train: 2018-08-06T01:49:45.617009: step 13021, loss 0.619238.
Train: 2018-08-06T01:49:45.863350: step 13022, loss 0.554674.
Train: 2018-08-06T01:49:46.118661: step 13023, loss 0.595024.
Train: 2018-08-06T01:49:46.364005: step 13024, loss 0.514356.
Train: 2018-08-06T01:49:46.608357: step 13025, loss 0.627305.
Train: 2018-08-06T01:49:46.852673: step 13026, loss 0.578881.
Train: 2018-08-06T01:49:47.110982: step 13027, loss 0.594989.
Train: 2018-08-06T01:49:47.360316: step 13028, loss 0.59496.
Train: 2018-08-06T01:49:47.609649: step 13029, loss 0.610977.
Train: 2018-08-06T01:49:47.868986: step 13030, loss 0.634897.
Test: 2018-08-06T01:49:49.127589: step 13030, loss 0.549415.
Train: 2018-08-06T01:49:49.372934: step 13031, loss 0.546968.
Train: 2018-08-06T01:49:49.619305: step 13032, loss 0.594754.
Train: 2018-08-06T01:49:49.879593: step 13033, loss 0.602622.
Train: 2018-08-06T01:49:50.131903: step 13034, loss 0.531519.
Train: 2018-08-06T01:49:50.380287: step 13035, loss 0.539516.
Train: 2018-08-06T01:49:50.627578: step 13036, loss 0.555299.
Train: 2018-08-06T01:49:50.888879: step 13037, loss 0.500389.
Train: 2018-08-06T01:49:51.138213: step 13038, loss 0.531755.
Train: 2018-08-06T01:49:51.383589: step 13039, loss 0.571005.
Train: 2018-08-06T01:49:51.629929: step 13040, loss 0.578867.
Test: 2018-08-06T01:49:52.901496: step 13040, loss 0.550486.
Train: 2018-08-06T01:49:53.138887: step 13041, loss 0.594627.
Train: 2018-08-06T01:49:53.383239: step 13042, loss 0.594631.
Train: 2018-08-06T01:49:53.631572: step 13043, loss 0.531579.
Train: 2018-08-06T01:49:53.876889: step 13044, loss 0.531549.
Train: 2018-08-06T01:49:54.128220: step 13045, loss 0.563068.
Train: 2018-08-06T01:49:54.376551: step 13046, loss 0.483964.
Train: 2018-08-06T01:49:54.622919: step 13047, loss 0.539194.
Train: 2018-08-06T01:49:54.881227: step 13048, loss 0.610701.
Train: 2018-08-06T01:49:55.127543: step 13049, loss 0.586838.
Train: 2018-08-06T01:49:55.375880: step 13050, loss 0.514937.
Test: 2018-08-06T01:49:56.659446: step 13050, loss 0.549097.
Train: 2018-08-06T01:49:56.897835: step 13051, loss 0.610908.
Train: 2018-08-06T01:49:57.145148: step 13052, loss 0.562825.
Train: 2018-08-06T01:49:57.405479: step 13053, loss 0.594933.
Train: 2018-08-06T01:49:57.653788: step 13054, loss 0.562801.
Train: 2018-08-06T01:49:57.897165: step 13055, loss 0.506514.
Train: 2018-08-06T01:49:58.153452: step 13056, loss 0.514447.
Train: 2018-08-06T01:49:58.412789: step 13057, loss 0.619261.
Train: 2018-08-06T01:49:58.656167: step 13058, loss 0.570805.
Train: 2018-08-06T01:49:58.917408: step 13059, loss 0.530335.
Train: 2018-08-06T01:49:59.161755: step 13060, loss 0.530264.
Test: 2018-08-06T01:50:00.434351: step 13060, loss 0.548909.
Train: 2018-08-06T01:50:00.670719: step 13061, loss 0.562663.
Train: 2018-08-06T01:50:00.916095: step 13062, loss 0.578918.
Train: 2018-08-06T01:50:01.163429: step 13063, loss 0.603377.
Train: 2018-08-06T01:50:01.420750: step 13064, loss 0.521851.
Train: 2018-08-06T01:50:01.664089: step 13065, loss 0.562607.
Train: 2018-08-06T01:50:01.920402: step 13066, loss 0.595287.
Train: 2018-08-06T01:50:02.165751: step 13067, loss 0.49719.
Train: 2018-08-06T01:50:02.410098: step 13068, loss 0.497068.
Train: 2018-08-06T01:50:02.657407: step 13069, loss 0.57076.
Train: 2018-08-06T01:50:02.901755: step 13070, loss 0.521378.
Test: 2018-08-06T01:50:04.154402: step 13070, loss 0.548339.
Train: 2018-08-06T01:50:04.455622: step 13071, loss 0.512982.
Train: 2018-08-06T01:50:04.700941: step 13072, loss 0.587322.
Train: 2018-08-06T01:50:04.948280: step 13073, loss 0.570759.
Train: 2018-08-06T01:50:05.194621: step 13074, loss 0.55412.
Train: 2018-08-06T01:50:05.443986: step 13075, loss 0.562428.
Train: 2018-08-06T01:50:05.700269: step 13076, loss 0.562418.
Train: 2018-08-06T01:50:05.956614: step 13077, loss 0.537324.
Train: 2018-08-06T01:50:06.214917: step 13078, loss 0.579152.
Train: 2018-08-06T01:50:06.460261: step 13079, loss 0.587547.
Train: 2018-08-06T01:50:06.704609: step 13080, loss 0.595939.
Test: 2018-08-06T01:50:07.962218: step 13080, loss 0.548207.
Train: 2018-08-06T01:50:08.212550: step 13081, loss 0.587542.
Train: 2018-08-06T01:50:08.462880: step 13082, loss 0.595892.
Train: 2018-08-06T01:50:08.708252: step 13083, loss 0.545699.
Train: 2018-08-06T01:50:08.955591: step 13084, loss 0.612496.
Train: 2018-08-06T01:50:09.197914: step 13085, loss 0.504154.
Train: 2018-08-06T01:50:09.443258: step 13086, loss 0.637301.
Train: 2018-08-06T01:50:09.687637: step 13087, loss 0.52098.
Train: 2018-08-06T01:50:09.944919: step 13088, loss 0.595608.
Train: 2018-08-06T01:50:10.192255: step 13089, loss 0.587289.
Train: 2018-08-06T01:50:10.439626: step 13090, loss 0.537774.
Test: 2018-08-06T01:50:11.705209: step 13090, loss 0.549727.
Train: 2018-08-06T01:50:11.941602: step 13091, loss 0.562526.
Train: 2018-08-06T01:50:12.185954: step 13092, loss 0.595412.
Train: 2018-08-06T01:50:12.435256: step 13093, loss 0.619959.
Train: 2018-08-06T01:50:12.677608: step 13094, loss 0.529902.
Train: 2018-08-06T01:50:12.921955: step 13095, loss 0.505532.
Train: 2018-08-06T01:50:13.168324: step 13096, loss 0.54633.
Train: 2018-08-06T01:50:13.412674: step 13097, loss 0.578923.
Train: 2018-08-06T01:50:13.672972: step 13098, loss 0.562639.
Train: 2018-08-06T01:50:13.919319: step 13099, loss 0.554511.
Train: 2018-08-06T01:50:14.166682: step 13100, loss 0.505726.
Test: 2018-08-06T01:50:15.431244: step 13100, loss 0.547548.
Train: 2018-08-06T01:50:16.394639: step 13101, loss 0.570779.
Train: 2018-08-06T01:50:16.642979: step 13102, loss 0.59521.
Train: 2018-08-06T01:50:16.899294: step 13103, loss 0.570778.
Train: 2018-08-06T01:50:17.143642: step 13104, loss 0.570779.
Train: 2018-08-06T01:50:17.394956: step 13105, loss 0.587056.
Train: 2018-08-06T01:50:17.641279: step 13106, loss 0.595175.
Train: 2018-08-06T01:50:17.903578: step 13107, loss 0.562669.
Train: 2018-08-06T01:50:18.158926: step 13108, loss 0.538358.
Train: 2018-08-06T01:50:18.419224: step 13109, loss 0.538381.
Train: 2018-08-06T01:50:18.667566: step 13110, loss 0.62752.
Test: 2018-08-06T01:50:19.934146: step 13110, loss 0.548634.
Train: 2018-08-06T01:50:20.171537: step 13111, loss 0.538431.
Train: 2018-08-06T01:50:20.430819: step 13112, loss 0.506107.
Train: 2018-08-06T01:50:20.680152: step 13113, loss 0.546524.
Train: 2018-08-06T01:50:20.926519: step 13114, loss 0.578897.
Train: 2018-08-06T01:50:21.171862: step 13115, loss 0.578899.
Train: 2018-08-06T01:50:21.414189: step 13116, loss 0.489737.
Train: 2018-08-06T01:50:21.658535: step 13117, loss 0.49771.
Train: 2018-08-06T01:50:21.909864: step 13118, loss 0.521917.
Train: 2018-08-06T01:50:22.155238: step 13119, loss 0.554424.
Train: 2018-08-06T01:50:22.412544: step 13120, loss 0.464175.
Test: 2018-08-06T01:50:23.687122: step 13120, loss 0.549514.
Train: 2018-08-06T01:50:23.926498: step 13121, loss 0.644917.
Train: 2018-08-06T01:50:24.172811: step 13122, loss 0.504657.
Train: 2018-08-06T01:50:24.426134: step 13123, loss 0.595631.
Train: 2018-08-06T01:50:24.669497: step 13124, loss 0.545825.
Train: 2018-08-06T01:50:24.915826: step 13125, loss 0.479115.
Train: 2018-08-06T01:50:25.163162: step 13126, loss 0.587507.
Train: 2018-08-06T01:50:25.413518: step 13127, loss 0.587557.
Train: 2018-08-06T01:50:25.666847: step 13128, loss 0.495181.
Train: 2018-08-06T01:50:25.916174: step 13129, loss 0.520246.
Train: 2018-08-06T01:50:26.163523: step 13130, loss 0.621526.
Test: 2018-08-06T01:50:27.442068: step 13130, loss 0.548218.
Train: 2018-08-06T01:50:27.682460: step 13131, loss 0.5031.
Train: 2018-08-06T01:50:27.930761: step 13132, loss 0.56235.
Train: 2018-08-06T01:50:28.174110: step 13133, loss 0.579349.
Train: 2018-08-06T01:50:28.427435: step 13134, loss 0.519779.
Train: 2018-08-06T01:50:28.671780: step 13135, loss 0.596451.
Train: 2018-08-06T01:50:28.922136: step 13136, loss 0.519669.
Train: 2018-08-06T01:50:29.119607: step 13137, loss 0.580566.
Train: 2018-08-06T01:50:29.365957: step 13138, loss 0.613635.
Train: 2018-08-06T01:50:29.612265: step 13139, loss 0.579426.
Train: 2018-08-06T01:50:29.865605: step 13140, loss 0.468459.
Test: 2018-08-06T01:50:31.135191: step 13140, loss 0.548076.
Train: 2018-08-06T01:50:31.373587: step 13141, loss 0.596495.
Train: 2018-08-06T01:50:31.619895: step 13142, loss 0.622093.
Train: 2018-08-06T01:50:31.863244: step 13143, loss 0.511206.
Train: 2018-08-06T01:50:32.112578: step 13144, loss 0.621957.
Train: 2018-08-06T01:50:32.370887: step 13145, loss 0.638843.
Train: 2018-08-06T01:50:32.617228: step 13146, loss 0.511529.
Train: 2018-08-06T01:50:32.865593: step 13147, loss 0.579265.
Train: 2018-08-06T01:50:33.124901: step 13148, loss 0.537081.
Train: 2018-08-06T01:50:33.371237: step 13149, loss 0.596034.
Train: 2018-08-06T01:50:33.619578: step 13150, loss 0.562391.
Test: 2018-08-06T01:50:34.927051: step 13150, loss 0.549.
Train: 2018-08-06T01:50:35.173401: step 13151, loss 0.54566.
Train: 2018-08-06T01:50:35.416767: step 13152, loss 0.537349.
Train: 2018-08-06T01:50:35.672083: step 13153, loss 0.562423.
Train: 2018-08-06T01:50:35.925382: step 13154, loss 0.520772.
Train: 2018-08-06T01:50:36.183690: step 13155, loss 0.520796.
Train: 2018-08-06T01:50:36.431054: step 13156, loss 0.545773.
Train: 2018-08-06T01:50:36.690335: step 13157, loss 0.50409.
Train: 2018-08-06T01:50:36.954660: step 13158, loss 0.470605.
Train: 2018-08-06T01:50:37.208982: step 13159, loss 0.55403.
Train: 2018-08-06T01:50:37.455314: step 13160, loss 0.604368.
Test: 2018-08-06T01:50:38.720904: step 13160, loss 0.546983.
Train: 2018-08-06T01:50:38.975250: step 13161, loss 0.663289.
Train: 2018-08-06T01:50:39.230567: step 13162, loss 0.553979.
Train: 2018-08-06T01:50:39.478909: step 13163, loss 0.562388.
Train: 2018-08-06T01:50:39.737186: step 13164, loss 0.537215.
Train: 2018-08-06T01:50:39.988514: step 13165, loss 0.537222.
Train: 2018-08-06T01:50:40.230897: step 13166, loss 0.537217.
Train: 2018-08-06T01:50:40.483223: step 13167, loss 0.545599.
Train: 2018-08-06T01:50:40.727572: step 13168, loss 0.570788.
Train: 2018-08-06T01:50:40.972883: step 13169, loss 0.612806.
Train: 2018-08-06T01:50:41.221243: step 13170, loss 0.537202.
Test: 2018-08-06T01:50:42.487831: step 13170, loss 0.547989.
Train: 2018-08-06T01:50:42.733174: step 13171, loss 0.537215.
Train: 2018-08-06T01:50:42.991518: step 13172, loss 0.528824.
Train: 2018-08-06T01:50:43.235871: step 13173, loss 0.604369.
Train: 2018-08-06T01:50:43.486172: step 13174, loss 0.570783.
Train: 2018-08-06T01:50:43.736501: step 13175, loss 0.612709.
Train: 2018-08-06T01:50:43.983865: step 13176, loss 0.604259.
Train: 2018-08-06T01:50:44.235167: step 13177, loss 0.512322.
Train: 2018-08-06T01:50:44.491483: step 13178, loss 0.637468.
Train: 2018-08-06T01:50:44.738851: step 13179, loss 0.562447.
Train: 2018-08-06T01:50:44.987157: step 13180, loss 0.570758.
Test: 2018-08-06T01:50:46.258755: step 13180, loss 0.547914.
Train: 2018-08-06T01:50:46.512109: step 13181, loss 0.488059.
Train: 2018-08-06T01:50:46.773411: step 13182, loss 0.612065.
Train: 2018-08-06T01:50:47.030718: step 13183, loss 0.471805.
Train: 2018-08-06T01:50:47.285040: step 13184, loss 0.554264.
Train: 2018-08-06T01:50:47.533372: step 13185, loss 0.603748.
Train: 2018-08-06T01:50:47.785672: step 13186, loss 0.554273.
Train: 2018-08-06T01:50:48.034042: step 13187, loss 0.56252.
Train: 2018-08-06T01:50:48.296355: step 13188, loss 0.554292.
Train: 2018-08-06T01:50:48.544642: step 13189, loss 0.570758.
Train: 2018-08-06T01:50:48.790983: step 13190, loss 0.521406.
Test: 2018-08-06T01:50:50.069564: step 13190, loss 0.548788.
Train: 2018-08-06T01:50:50.311916: step 13191, loss 0.578986.
Train: 2018-08-06T01:50:50.557291: step 13192, loss 0.513167.
Train: 2018-08-06T01:50:50.812578: step 13193, loss 0.570757.
Train: 2018-08-06T01:50:51.060913: step 13194, loss 0.587236.
Train: 2018-08-06T01:50:51.311245: step 13195, loss 0.620192.
Train: 2018-08-06T01:50:51.560603: step 13196, loss 0.504926.
Train: 2018-08-06T01:50:51.808941: step 13197, loss 0.603674.
Train: 2018-08-06T01:50:52.062236: step 13198, loss 0.644755.
Train: 2018-08-06T01:50:52.322574: step 13199, loss 0.603567.
Train: 2018-08-06T01:50:52.572869: step 13200, loss 0.587119.
Test: 2018-08-06T01:50:53.859429: step 13200, loss 0.549769.
Train: 2018-08-06T01:50:54.831918: step 13201, loss 0.587073.
Train: 2018-08-06T01:50:55.092221: step 13202, loss 0.570787.
Train: 2018-08-06T01:50:55.335602: step 13203, loss 0.554615.
Train: 2018-08-06T01:50:55.594908: step 13204, loss 0.595024.
Train: 2018-08-06T01:50:55.908735: step 13205, loss 0.57083.
Train: 2018-08-06T01:50:56.153082: step 13206, loss 0.554805.
Train: 2018-08-06T01:50:56.411361: step 13207, loss 0.61887.
Train: 2018-08-06T01:50:56.659697: step 13208, loss 0.538986.
Train: 2018-08-06T01:50:56.910026: step 13209, loss 0.586814.
Train: 2018-08-06T01:50:57.172358: step 13210, loss 0.531245.
Test: 2018-08-06T01:50:58.442927: step 13210, loss 0.549778.
Train: 2018-08-06T01:50:58.687300: step 13211, loss 0.713565.
Train: 2018-08-06T01:50:58.935609: step 13212, loss 0.523645.
Train: 2018-08-06T01:50:59.183972: step 13213, loss 0.47663.
Train: 2018-08-06T01:50:59.431315: step 13214, loss 0.547432.
Train: 2018-08-06T01:50:59.676653: step 13215, loss 0.57887.
Train: 2018-08-06T01:50:59.931951: step 13216, loss 0.508155.
Train: 2018-08-06T01:51:00.180282: step 13217, loss 0.578868.
Train: 2018-08-06T01:51:00.424627: step 13218, loss 0.523751.
Train: 2018-08-06T01:51:00.675986: step 13219, loss 0.507871.
Train: 2018-08-06T01:51:00.924291: step 13220, loss 0.594684.
Test: 2018-08-06T01:51:02.181927: step 13220, loss 0.549576.
Train: 2018-08-06T01:51:02.419327: step 13221, loss 0.586788.
Train: 2018-08-06T01:51:02.670645: step 13222, loss 0.531211.
Train: 2018-08-06T01:51:02.931953: step 13223, loss 0.531106.
Train: 2018-08-06T01:51:03.177299: step 13224, loss 0.538956.
Train: 2018-08-06T01:51:03.423607: step 13225, loss 0.546841.
Train: 2018-08-06T01:51:03.674963: step 13226, loss 0.546744.
Train: 2018-08-06T01:51:03.933277: step 13227, loss 0.546645.
Train: 2018-08-06T01:51:04.180615: step 13228, loss 0.546545.
Train: 2018-08-06T01:51:04.426958: step 13229, loss 0.538336.
Train: 2018-08-06T01:51:04.675259: step 13230, loss 0.595201.
Test: 2018-08-06T01:51:05.931899: step 13230, loss 0.548217.
Train: 2018-08-06T01:51:06.168297: step 13231, loss 0.521766.
Train: 2018-08-06T01:51:06.417625: step 13232, loss 0.562555.
Train: 2018-08-06T01:51:06.662945: step 13233, loss 0.537918.
Train: 2018-08-06T01:51:06.910307: step 13234, loss 0.546001.
Train: 2018-08-06T01:51:07.157622: step 13235, loss 0.537656.
Train: 2018-08-06T01:51:07.411974: step 13236, loss 0.604044.
Train: 2018-08-06T01:51:07.658308: step 13237, loss 0.570721.
Train: 2018-08-06T01:51:07.900664: step 13238, loss 0.545718.
Train: 2018-08-06T01:51:08.155976: step 13239, loss 0.562354.
Train: 2018-08-06T01:51:08.405285: step 13240, loss 0.595793.
Test: 2018-08-06T01:51:09.681870: step 13240, loss 0.548037.
Train: 2018-08-06T01:51:09.918270: step 13241, loss 0.537235.
Train: 2018-08-06T01:51:10.168569: step 13242, loss 0.571043.
Train: 2018-08-06T01:51:10.413913: step 13243, loss 0.579575.
Train: 2018-08-06T01:51:10.666264: step 13244, loss 0.620925.
Train: 2018-08-06T01:51:10.913602: step 13245, loss 0.537446.
Train: 2018-08-06T01:51:11.162935: step 13246, loss 0.5125.
Train: 2018-08-06T01:51:11.411276: step 13247, loss 0.570757.
Train: 2018-08-06T01:51:11.659582: step 13248, loss 0.587395.
Train: 2018-08-06T01:51:11.907918: step 13249, loss 0.562412.
Train: 2018-08-06T01:51:12.152294: step 13250, loss 0.570784.
Test: 2018-08-06T01:51:13.418876: step 13250, loss 0.547026.
Train: 2018-08-06T01:51:13.656258: step 13251, loss 0.570749.
Train: 2018-08-06T01:51:13.905574: step 13252, loss 0.595672.
Train: 2018-08-06T01:51:14.153911: step 13253, loss 0.537532.
Train: 2018-08-06T01:51:14.395307: step 13254, loss 0.562443.
Train: 2018-08-06T01:51:14.642613: step 13255, loss 0.52944.
Train: 2018-08-06T01:51:14.895926: step 13256, loss 0.670254.
Train: 2018-08-06T01:51:15.142292: step 13257, loss 0.554279.
Train: 2018-08-06T01:51:15.390636: step 13258, loss 0.57908.
Train: 2018-08-06T01:51:15.641931: step 13259, loss 0.60358.
Train: 2018-08-06T01:51:15.892288: step 13260, loss 0.529889.
Test: 2018-08-06T01:51:17.178821: step 13260, loss 0.549176.
Train: 2018-08-06T01:51:17.417184: step 13261, loss 0.570771.
Train: 2018-08-06T01:51:17.669537: step 13262, loss 0.554509.
Train: 2018-08-06T01:51:17.915851: step 13263, loss 0.530156.
Train: 2018-08-06T01:51:18.169223: step 13264, loss 0.538313.
Train: 2018-08-06T01:51:18.416544: step 13265, loss 0.546438.
Train: 2018-08-06T01:51:18.677852: step 13266, loss 0.603258.
Train: 2018-08-06T01:51:18.928184: step 13267, loss 0.530155.
Train: 2018-08-06T01:51:19.189478: step 13268, loss 0.595175.
Train: 2018-08-06T01:51:19.449759: step 13269, loss 0.530028.
Train: 2018-08-06T01:51:19.700119: step 13270, loss 0.554379.
Test: 2018-08-06T01:51:20.959719: step 13270, loss 0.547656.
Train: 2018-08-06T01:51:21.201108: step 13271, loss 0.652597.
Train: 2018-08-06T01:51:21.447448: step 13272, loss 0.554263.
Train: 2018-08-06T01:51:21.695784: step 13273, loss 0.587045.
Train: 2018-08-06T01:51:21.944118: step 13274, loss 0.562387.
Train: 2018-08-06T01:51:22.192456: step 13275, loss 0.570786.
Train: 2018-08-06T01:51:22.440783: step 13276, loss 0.489178.
Train: 2018-08-06T01:51:22.688097: step 13277, loss 0.521887.
Train: 2018-08-06T01:51:22.936433: step 13278, loss 0.514621.
Train: 2018-08-06T01:51:23.195773: step 13279, loss 0.529367.
Train: 2018-08-06T01:51:23.446121: step 13280, loss 0.538219.
Test: 2018-08-06T01:51:24.703706: step 13280, loss 0.547261.
Train: 2018-08-06T01:51:24.958051: step 13281, loss 0.562999.
Train: 2018-08-06T01:51:25.204394: step 13282, loss 0.590972.
Train: 2018-08-06T01:51:25.450741: step 13283, loss 0.596991.
Train: 2018-08-06T01:51:25.698047: step 13284, loss 0.527233.
Train: 2018-08-06T01:51:25.944420: step 13285, loss 0.571444.
Train: 2018-08-06T01:51:26.192769: step 13286, loss 0.562316.
Train: 2018-08-06T01:51:26.442090: step 13287, loss 0.513308.
Train: 2018-08-06T01:51:26.632549: step 13288, loss 0.563484.
Train: 2018-08-06T01:51:26.880915: step 13289, loss 0.595194.
Train: 2018-08-06T01:51:27.132213: step 13290, loss 0.620665.
Test: 2018-08-06T01:51:28.402814: step 13290, loss 0.549579.
Train: 2018-08-06T01:51:28.654167: step 13291, loss 0.578849.
Train: 2018-08-06T01:51:28.902512: step 13292, loss 0.587055.
Train: 2018-08-06T01:51:29.150845: step 13293, loss 0.497239.
Train: 2018-08-06T01:51:29.404141: step 13294, loss 0.529969.
Train: 2018-08-06T01:51:29.655464: step 13295, loss 0.562589.
Train: 2018-08-06T01:51:29.905828: step 13296, loss 0.603446.
Train: 2018-08-06T01:51:30.154130: step 13297, loss 0.595263.
Train: 2018-08-06T01:51:30.403488: step 13298, loss 0.603395.
Train: 2018-08-06T01:51:30.650837: step 13299, loss 0.619622.
Train: 2018-08-06T01:51:30.900164: step 13300, loss 0.619497.
Test: 2018-08-06T01:51:32.176721: step 13300, loss 0.549207.
Train: 2018-08-06T01:51:33.114425: step 13301, loss 0.538451.
Train: 2018-08-06T01:51:33.361733: step 13302, loss 0.595017.
Train: 2018-08-06T01:51:33.610068: step 13303, loss 0.570823.
Train: 2018-08-06T01:51:33.855413: step 13304, loss 0.554805.
Train: 2018-08-06T01:51:34.107768: step 13305, loss 0.650838.
Train: 2018-08-06T01:51:34.369037: step 13306, loss 0.554976.
Train: 2018-08-06T01:51:34.618397: step 13307, loss 0.562989.
Train: 2018-08-06T01:51:34.867717: step 13308, loss 0.531374.
Train: 2018-08-06T01:51:35.120104: step 13309, loss 0.57886.
Train: 2018-08-06T01:51:35.367443: step 13310, loss 0.602545.
Test: 2018-08-06T01:51:36.650012: step 13310, loss 0.549984.
Train: 2018-08-06T01:51:36.890371: step 13311, loss 0.578877.
Train: 2018-08-06T01:51:37.150673: step 13312, loss 0.555317.
Train: 2018-08-06T01:51:37.422946: step 13313, loss 0.508329.
Train: 2018-08-06T01:51:37.668315: step 13314, loss 0.524014.
Train: 2018-08-06T01:51:37.913679: step 13315, loss 0.516125.
Train: 2018-08-06T01:51:38.158978: step 13316, loss 0.523828.
Train: 2018-08-06T01:51:38.415292: step 13317, loss 0.563099.
Train: 2018-08-06T01:51:38.664656: step 13318, loss 0.531403.
Train: 2018-08-06T01:51:38.925926: step 13319, loss 0.562994.
Train: 2018-08-06T01:51:39.175284: step 13320, loss 0.53905.
Test: 2018-08-06T01:51:40.442868: step 13320, loss 0.549184.
Train: 2018-08-06T01:51:40.686244: step 13321, loss 0.602815.
Train: 2018-08-06T01:51:40.935550: step 13322, loss 0.642904.
Train: 2018-08-06T01:51:41.195886: step 13323, loss 0.594904.
Train: 2018-08-06T01:51:41.441200: step 13324, loss 0.530827.
Train: 2018-08-06T01:51:41.688568: step 13325, loss 0.586865.
Train: 2018-08-06T01:51:41.947844: step 13326, loss 0.594877.
Train: 2018-08-06T01:51:42.196180: step 13327, loss 0.522805.
Train: 2018-08-06T01:51:42.444516: step 13328, loss 0.530803.
Train: 2018-08-06T01:51:42.689860: step 13329, loss 0.546793.
Train: 2018-08-06T01:51:42.944180: step 13330, loss 0.610994.
Test: 2018-08-06T01:51:44.204808: step 13330, loss 0.54975.
Train: 2018-08-06T01:51:44.455138: step 13331, loss 0.619044.
Train: 2018-08-06T01:51:44.705502: step 13332, loss 0.53873.
Train: 2018-08-06T01:51:44.960822: step 13333, loss 0.538729.
Train: 2018-08-06T01:51:45.217101: step 13334, loss 0.586901.
Train: 2018-08-06T01:51:45.463473: step 13335, loss 0.530677.
Train: 2018-08-06T01:51:45.712777: step 13336, loss 0.611029.
Train: 2018-08-06T01:51:45.958145: step 13337, loss 0.586911.
Train: 2018-08-06T01:51:46.214459: step 13338, loss 0.530693.
Train: 2018-08-06T01:51:46.462769: step 13339, loss 0.522662.
Train: 2018-08-06T01:51:46.711137: step 13340, loss 0.514559.
Test: 2018-08-06T01:51:47.968742: step 13340, loss 0.549687.
Train: 2018-08-06T01:51:48.208102: step 13341, loss 0.546659.
Train: 2018-08-06T01:51:48.453446: step 13342, loss 0.54659.
Train: 2018-08-06T01:51:48.702779: step 13343, loss 0.595075.
Train: 2018-08-06T01:51:48.945162: step 13344, loss 0.530272.
Train: 2018-08-06T01:51:49.194495: step 13345, loss 0.611391.
Train: 2018-08-06T01:51:49.438811: step 13346, loss 0.546399.
Train: 2018-08-06T01:51:49.688144: step 13347, loss 0.554507.
Train: 2018-08-06T01:51:49.937477: step 13348, loss 0.570776.
Train: 2018-08-06T01:51:50.185838: step 13349, loss 0.636006.
Train: 2018-08-06T01:51:50.442129: step 13350, loss 0.587072.
Test: 2018-08-06T01:51:51.692782: step 13350, loss 0.549985.
Train: 2018-08-06T01:51:51.946136: step 13351, loss 0.63589.
Train: 2018-08-06T01:51:52.191450: step 13352, loss 0.611376.
Train: 2018-08-06T01:51:52.439786: step 13353, loss 0.578893.
Train: 2018-08-06T01:51:52.687123: step 13354, loss 0.578881.
Train: 2018-08-06T01:51:52.933496: step 13355, loss 0.554761.
Train: 2018-08-06T01:51:53.192772: step 13356, loss 0.554821.
Train: 2018-08-06T01:51:53.442129: step 13357, loss 0.554872.
Train: 2018-08-06T01:51:53.702408: step 13358, loss 0.570878.
Train: 2018-08-06T01:51:53.946785: step 13359, loss 0.443366.
Train: 2018-08-06T01:51:54.197085: step 13360, loss 0.5629.
Test: 2018-08-06T01:51:55.472674: step 13360, loss 0.549354.
Train: 2018-08-06T01:51:55.710039: step 13361, loss 0.554892.
Train: 2018-08-06T01:51:55.970343: step 13362, loss 0.578863.
Train: 2018-08-06T01:51:56.220699: step 13363, loss 0.530812.
Train: 2018-08-06T01:51:56.477019: step 13364, loss 0.554802.
Train: 2018-08-06T01:51:56.724327: step 13365, loss 0.651198.
Train: 2018-08-06T01:51:56.974666: step 13366, loss 0.546735.
Train: 2018-08-06T01:51:57.223024: step 13367, loss 0.482447.
Train: 2018-08-06T01:51:57.470363: step 13368, loss 0.538621.
Train: 2018-08-06T01:51:57.718668: step 13369, loss 0.619227.
Train: 2018-08-06T01:51:57.966006: step 13370, loss 0.5708.
Test: 2018-08-06T01:51:59.226634: step 13370, loss 0.549774.
Train: 2018-08-06T01:51:59.465023: step 13371, loss 0.627408.
Train: 2018-08-06T01:51:59.713333: step 13372, loss 0.595039.
Train: 2018-08-06T01:51:59.960702: step 13373, loss 0.570825.
Train: 2018-08-06T01:52:00.208009: step 13374, loss 0.611115.
Train: 2018-08-06T01:52:00.458341: step 13375, loss 0.562793.
Train: 2018-08-06T01:52:00.704712: step 13376, loss 0.586893.
Train: 2018-08-06T01:52:00.951024: step 13377, loss 0.514805.
Train: 2018-08-06T01:52:01.202382: step 13378, loss 0.506853.
Train: 2018-08-06T01:52:01.445713: step 13379, loss 0.530827.
Train: 2018-08-06T01:52:01.693040: step 13380, loss 0.538784.
Test: 2018-08-06T01:52:02.955661: step 13380, loss 0.549579.
Train: 2018-08-06T01:52:03.195047: step 13381, loss 0.56281.
Train: 2018-08-06T01:52:03.440397: step 13382, loss 0.611055.
Train: 2018-08-06T01:52:03.695683: step 13383, loss 0.627177.
Train: 2018-08-06T01:52:03.944019: step 13384, loss 0.594962.
Train: 2018-08-06T01:52:04.194375: step 13385, loss 0.594934.
Train: 2018-08-06T01:52:04.448738: step 13386, loss 0.506725.
Train: 2018-08-06T01:52:04.697042: step 13387, loss 0.498741.
Train: 2018-08-06T01:52:04.945379: step 13388, loss 0.562825.
Train: 2018-08-06T01:52:05.191720: step 13389, loss 0.54675.
Train: 2018-08-06T01:52:05.436092: step 13390, loss 0.49846.
Test: 2018-08-06T01:52:06.711669: step 13390, loss 0.548359.
Train: 2018-08-06T01:52:07.038221: step 13391, loss 0.490176.
Train: 2018-08-06T01:52:07.295526: step 13392, loss 0.514116.
Train: 2018-08-06T01:52:07.540900: step 13393, loss 0.586994.
Train: 2018-08-06T01:52:07.789233: step 13394, loss 0.448168.
Train: 2018-08-06T01:52:08.043525: step 13395, loss 0.546009.
Train: 2018-08-06T01:52:08.289866: step 13396, loss 0.57854.
Train: 2018-08-06T01:52:08.534237: step 13397, loss 0.528433.
Train: 2018-08-06T01:52:08.789530: step 13398, loss 0.461416.
Train: 2018-08-06T01:52:09.039859: step 13399, loss 0.555318.
Train: 2018-08-06T01:52:09.288222: step 13400, loss 0.577707.
Test: 2018-08-06T01:52:10.550818: step 13400, loss 0.547627.
Train: 2018-08-06T01:52:11.442479: step 13401, loss 0.56249.
Train: 2018-08-06T01:52:11.701773: step 13402, loss 0.573264.
Train: 2018-08-06T01:52:11.956097: step 13403, loss 0.525046.
Train: 2018-08-06T01:52:12.203436: step 13404, loss 0.625488.
Train: 2018-08-06T01:52:12.453768: step 13405, loss 0.631662.
Train: 2018-08-06T01:52:12.698083: step 13406, loss 0.63279.
Train: 2018-08-06T01:52:12.948445: step 13407, loss 0.545302.
Train: 2018-08-06T01:52:13.194754: step 13408, loss 0.5024.
Train: 2018-08-06T01:52:13.438104: step 13409, loss 0.596359.
Train: 2018-08-06T01:52:13.694419: step 13410, loss 0.579369.
Test: 2018-08-06T01:52:14.982971: step 13410, loss 0.546908.
Train: 2018-08-06T01:52:15.222332: step 13411, loss 0.545345.
Train: 2018-08-06T01:52:15.467675: step 13412, loss 0.503579.
Train: 2018-08-06T01:52:15.716012: step 13413, loss 0.570825.
Train: 2018-08-06T01:52:15.962378: step 13414, loss 0.612719.
Train: 2018-08-06T01:52:16.207701: step 13415, loss 0.537296.
Train: 2018-08-06T01:52:16.455060: step 13416, loss 0.537329.
Train: 2018-08-06T01:52:16.701408: step 13417, loss 0.554061.
Train: 2018-08-06T01:52:16.947744: step 13418, loss 0.57912.
Train: 2018-08-06T01:52:17.193087: step 13419, loss 0.545709.
Train: 2018-08-06T01:52:17.440432: step 13420, loss 0.562454.
Test: 2018-08-06T01:52:18.727956: step 13420, loss 0.549092.
Train: 2018-08-06T01:52:18.964334: step 13421, loss 0.562448.
Train: 2018-08-06T01:52:19.211663: step 13422, loss 0.612413.
Train: 2018-08-06T01:52:19.457033: step 13423, loss 0.504267.
Train: 2018-08-06T01:52:19.705355: step 13424, loss 0.520915.
Train: 2018-08-06T01:52:19.952705: step 13425, loss 0.58735.
Train: 2018-08-06T01:52:20.199033: step 13426, loss 0.52091.
Train: 2018-08-06T01:52:20.451348: step 13427, loss 0.554199.
Train: 2018-08-06T01:52:20.713671: step 13428, loss 0.479351.
Train: 2018-08-06T01:52:20.959017: step 13429, loss 0.562355.
Train: 2018-08-06T01:52:21.218296: step 13430, loss 0.520693.
Test: 2018-08-06T01:52:22.480919: step 13430, loss 0.547665.
Train: 2018-08-06T01:52:22.720279: step 13431, loss 0.562469.
Train: 2018-08-06T01:52:22.966621: step 13432, loss 0.512126.
Train: 2018-08-06T01:52:23.212993: step 13433, loss 0.545609.
Train: 2018-08-06T01:52:23.464322: step 13434, loss 0.528752.
Train: 2018-08-06T01:52:23.728620: step 13435, loss 0.553869.
Train: 2018-08-06T01:52:23.973927: step 13436, loss 0.647116.
Train: 2018-08-06T01:52:24.223287: step 13437, loss 0.494627.
Train: 2018-08-06T01:52:24.471597: step 13438, loss 0.630202.
Train: 2018-08-06T01:52:24.663109: step 13439, loss 0.56242.
Train: 2018-08-06T01:52:24.912455: step 13440, loss 0.519978.
Test: 2018-08-06T01:52:26.192992: step 13440, loss 0.548.
Train: 2018-08-06T01:52:26.431365: step 13441, loss 0.545449.
Train: 2018-08-06T01:52:26.676724: step 13442, loss 0.630221.
Train: 2018-08-06T01:52:26.924038: step 13443, loss 0.570832.
Train: 2018-08-06T01:52:27.172373: step 13444, loss 0.494668.
Train: 2018-08-06T01:52:27.426694: step 13445, loss 0.613127.
Train: 2018-08-06T01:52:27.677048: step 13446, loss 0.503221.
Train: 2018-08-06T01:52:27.923396: step 13447, loss 0.579268.
Train: 2018-08-06T01:52:28.167712: step 13448, loss 0.528576.
Train: 2018-08-06T01:52:28.423055: step 13449, loss 0.545476.
Train: 2018-08-06T01:52:28.668373: step 13450, loss 0.528575.
Test: 2018-08-06T01:52:29.944958: step 13450, loss 0.547077.
Train: 2018-08-06T01:52:30.195323: step 13451, loss 0.579266.
Train: 2018-08-06T01:52:30.443641: step 13452, loss 0.613068.
Train: 2018-08-06T01:52:30.701960: step 13453, loss 0.61304.
Train: 2018-08-06T01:52:30.955292: step 13454, loss 0.562373.
Train: 2018-08-06T01:52:31.216557: step 13455, loss 0.469873.
Train: 2018-08-06T01:52:31.474866: step 13456, loss 0.612835.
Train: 2018-08-06T01:52:31.722238: step 13457, loss 0.537194.
Train: 2018-08-06T01:52:31.976526: step 13458, loss 0.545612.
Train: 2018-08-06T01:52:32.225886: step 13459, loss 0.562401.
Train: 2018-08-06T01:52:32.473228: step 13460, loss 0.537248.
Test: 2018-08-06T01:52:33.723852: step 13460, loss 0.54917.
Train: 2018-08-06T01:52:33.964241: step 13461, loss 0.612697.
Train: 2018-08-06T01:52:34.208590: step 13462, loss 0.562409.
Train: 2018-08-06T01:52:34.455929: step 13463, loss 0.637655.
Train: 2018-08-06T01:52:34.712235: step 13464, loss 0.562422.
Train: 2018-08-06T01:52:34.956588: step 13465, loss 0.520868.
Train: 2018-08-06T01:52:35.215893: step 13466, loss 0.554153.
Train: 2018-08-06T01:52:35.464225: step 13467, loss 0.603924.
Train: 2018-08-06T01:52:35.709541: step 13468, loss 0.554209.
Train: 2018-08-06T01:52:35.957878: step 13469, loss 0.587281.
Train: 2018-08-06T01:52:36.204266: step 13470, loss 0.463614.
Test: 2018-08-06T01:52:37.488783: step 13470, loss 0.547993.
Train: 2018-08-06T01:52:37.743130: step 13471, loss 0.58724.
Train: 2018-08-06T01:52:37.991470: step 13472, loss 0.521334.
Train: 2018-08-06T01:52:38.234814: step 13473, loss 0.546041.
Train: 2018-08-06T01:52:38.484123: step 13474, loss 0.611976.
Train: 2018-08-06T01:52:38.731460: step 13475, loss 0.562515.
Train: 2018-08-06T01:52:38.979796: step 13476, loss 0.587225.
Train: 2018-08-06T01:52:39.224143: step 13477, loss 0.611894.
Train: 2018-08-06T01:52:39.470526: step 13478, loss 0.496874.
Train: 2018-08-06T01:52:39.717854: step 13479, loss 0.570763.
Train: 2018-08-06T01:52:39.962195: step 13480, loss 0.611762.
Test: 2018-08-06T01:52:41.243742: step 13480, loss 0.550411.
Train: 2018-08-06T01:52:41.488089: step 13481, loss 0.546201.
Train: 2018-08-06T01:52:41.739415: step 13482, loss 0.570768.
Train: 2018-08-06T01:52:41.984786: step 13483, loss 0.529926.
Train: 2018-08-06T01:52:42.232123: step 13484, loss 0.619763.
Train: 2018-08-06T01:52:42.478466: step 13485, loss 0.497404.
Train: 2018-08-06T01:52:42.729769: step 13486, loss 0.562623.
Train: 2018-08-06T01:52:42.987112: step 13487, loss 0.530018.
Train: 2018-08-06T01:52:43.232424: step 13488, loss 0.562616.
Train: 2018-08-06T01:52:43.477767: step 13489, loss 0.562611.
Train: 2018-08-06T01:52:43.725130: step 13490, loss 0.595264.
Test: 2018-08-06T01:52:44.979750: step 13490, loss 0.548597.
Train: 2018-08-06T01:52:45.216149: step 13491, loss 0.529953.
Train: 2018-08-06T01:52:45.461487: step 13492, loss 0.538099.
Train: 2018-08-06T01:52:45.703821: step 13493, loss 0.595294.
Train: 2018-08-06T01:52:45.951177: step 13494, loss 0.603473.
Train: 2018-08-06T01:52:46.194527: step 13495, loss 0.562598.
Train: 2018-08-06T01:52:46.440874: step 13496, loss 0.521779.
Train: 2018-08-06T01:52:46.696160: step 13497, loss 0.505433.
Train: 2018-08-06T01:52:46.955493: step 13498, loss 0.529873.
Train: 2018-08-06T01:52:47.207792: step 13499, loss 0.537986.
Train: 2018-08-06T01:52:47.453135: step 13500, loss 0.488638.
Test: 2018-08-06T01:52:48.730723: step 13500, loss 0.548941.
Train: 2018-08-06T01:52:49.650337: step 13501, loss 0.611959.
Train: 2018-08-06T01:52:49.892661: step 13502, loss 0.554249.
Train: 2018-08-06T01:52:50.141022: step 13503, loss 0.587307.
Train: 2018-08-06T01:52:50.387369: step 13504, loss 0.595617.
Train: 2018-08-06T01:52:50.629721: step 13505, loss 0.637054.
Train: 2018-08-06T01:52:50.878060: step 13506, loss 0.537652.
Train: 2018-08-06T01:52:51.125365: step 13507, loss 0.529404.
Train: 2018-08-06T01:52:51.372737: step 13508, loss 0.645186.
Train: 2018-08-06T01:52:51.620075: step 13509, loss 0.645049.
Train: 2018-08-06T01:52:51.864413: step 13510, loss 0.513167.
Test: 2018-08-06T01:52:53.132995: step 13510, loss 0.548272.
Train: 2018-08-06T01:52:53.368398: step 13511, loss 0.55434.
Train: 2018-08-06T01:52:53.625679: step 13512, loss 0.546175.
Train: 2018-08-06T01:52:53.884022: step 13513, loss 0.677183.
Train: 2018-08-06T01:52:54.141324: step 13514, loss 0.595246.
Train: 2018-08-06T01:52:54.388639: step 13515, loss 0.570785.
Train: 2018-08-06T01:52:54.649974: step 13516, loss 0.578895.
Train: 2018-08-06T01:52:54.902265: step 13517, loss 0.562746.
Train: 2018-08-06T01:52:55.149602: step 13518, loss 0.586916.
Train: 2018-08-06T01:52:55.392953: step 13519, loss 0.618947.
Train: 2018-08-06T01:52:55.636329: step 13520, loss 0.562895.
Test: 2018-08-06T01:52:56.969316: step 13520, loss 0.548934.
Train: 2018-08-06T01:52:57.207704: step 13521, loss 0.531133.
Train: 2018-08-06T01:52:57.454020: step 13522, loss 0.594728.
Train: 2018-08-06T01:52:57.702356: step 13523, loss 0.563035.
Train: 2018-08-06T01:52:57.955679: step 13524, loss 0.602544.
Train: 2018-08-06T01:52:58.202019: step 13525, loss 0.539512.
Train: 2018-08-06T01:52:58.445369: step 13526, loss 0.53959.
Train: 2018-08-06T01:52:58.704677: step 13527, loss 0.563177.
Train: 2018-08-06T01:52:58.949046: step 13528, loss 0.563189.
Train: 2018-08-06T01:52:59.207359: step 13529, loss 0.524003.
Train: 2018-08-06T01:52:59.451702: step 13530, loss 0.571032.
Test: 2018-08-06T01:53:00.711309: step 13530, loss 0.550609.
Train: 2018-08-06T01:53:00.960667: step 13531, loss 0.555329.
Train: 2018-08-06T01:53:01.203991: step 13532, loss 0.618158.
Train: 2018-08-06T01:53:01.449363: step 13533, loss 0.547457.
Train: 2018-08-06T01:53:01.698699: step 13534, loss 0.594584.
Train: 2018-08-06T01:53:01.944011: step 13535, loss 0.523897.
Train: 2018-08-06T01:53:02.187365: step 13536, loss 0.531711.
Train: 2018-08-06T01:53:02.452652: step 13537, loss 0.563122.
Train: 2018-08-06T01:53:02.705974: step 13538, loss 0.523667.
Train: 2018-08-06T01:53:02.950352: step 13539, loss 0.515622.
Train: 2018-08-06T01:53:03.198683: step 13540, loss 0.658179.
Test: 2018-08-06T01:53:04.461279: step 13540, loss 0.549532.
Train: 2018-08-06T01:53:04.702665: step 13541, loss 0.555036.
Train: 2018-08-06T01:53:04.960945: step 13542, loss 0.562956.
Train: 2018-08-06T01:53:05.205290: step 13543, loss 0.570897.
Train: 2018-08-06T01:53:05.454652: step 13544, loss 0.562919.
Train: 2018-08-06T01:53:05.701963: step 13545, loss 0.507053.
Train: 2018-08-06T01:53:05.968282: step 13546, loss 0.506891.
Train: 2018-08-06T01:53:06.214591: step 13547, loss 0.53875.
Train: 2018-08-06T01:53:06.461929: step 13548, loss 0.538614.
Train: 2018-08-06T01:53:06.706301: step 13549, loss 0.595056.
Train: 2018-08-06T01:53:06.953645: step 13550, loss 0.489737.
Test: 2018-08-06T01:53:08.204270: step 13550, loss 0.549799.
Train: 2018-08-06T01:53:08.443629: step 13551, loss 0.578919.
Train: 2018-08-06T01:53:08.686979: step 13552, loss 0.603431.
Train: 2018-08-06T01:53:08.942321: step 13553, loss 0.529853.
Train: 2018-08-06T01:53:09.194653: step 13554, loss 0.472321.
Train: 2018-08-06T01:53:09.444978: step 13555, loss 0.570758.
Train: 2018-08-06T01:53:09.693314: step 13556, loss 0.62035.
Train: 2018-08-06T01:53:09.936663: step 13557, loss 0.446541.
Train: 2018-08-06T01:53:10.181988: step 13558, loss 0.495937.
Train: 2018-08-06T01:53:10.429353: step 13559, loss 0.49559.
Train: 2018-08-06T01:53:10.687629: step 13560, loss 0.587584.
Test: 2018-08-06T01:53:11.964214: step 13560, loss 0.547505.
Train: 2018-08-06T01:53:12.202578: step 13561, loss 0.503324.
Train: 2018-08-06T01:53:12.448919: step 13562, loss 0.553878.
Train: 2018-08-06T01:53:12.695261: step 13563, loss 0.502758.
Train: 2018-08-06T01:53:12.941626: step 13564, loss 0.639317.
Train: 2018-08-06T01:53:13.192929: step 13565, loss 0.648095.
Train: 2018-08-06T01:53:13.438272: step 13566, loss 0.536593.
Train: 2018-08-06T01:53:13.684615: step 13567, loss 0.510814.
Train: 2018-08-06T01:53:13.929957: step 13568, loss 0.613928.
Train: 2018-08-06T01:53:14.184306: step 13569, loss 0.562335.
Train: 2018-08-06T01:53:14.431646: step 13570, loss 0.510748.
Test: 2018-08-06T01:53:15.687258: step 13570, loss 0.550344.
Train: 2018-08-06T01:53:15.924623: step 13571, loss 0.553733.
Train: 2018-08-06T01:53:16.170964: step 13572, loss 0.553729.
Train: 2018-08-06T01:53:16.416334: step 13573, loss 0.545117.
Train: 2018-08-06T01:53:16.666670: step 13574, loss 0.614012.
Train: 2018-08-06T01:53:16.910987: step 13575, loss 0.605362.
Train: 2018-08-06T01:53:17.158325: step 13576, loss 0.570925.
Train: 2018-08-06T01:53:17.405662: step 13577, loss 0.528046.
Train: 2018-08-06T01:53:17.666997: step 13578, loss 0.605137.
Train: 2018-08-06T01:53:17.914302: step 13579, loss 0.613582.
Train: 2018-08-06T01:53:18.164634: step 13580, loss 0.528289.
Test: 2018-08-06T01:53:19.435234: step 13580, loss 0.546992.
Train: 2018-08-06T01:53:19.674595: step 13581, loss 0.587825.
Train: 2018-08-06T01:53:19.917945: step 13582, loss 0.545418.
Train: 2018-08-06T01:53:20.167307: step 13583, loss 0.6046.
Train: 2018-08-06T01:53:20.414639: step 13584, loss 0.545535.
Train: 2018-08-06T01:53:20.664979: step 13585, loss 0.495203.
Train: 2018-08-06T01:53:20.923288: step 13586, loss 0.579167.
Train: 2018-08-06T01:53:21.170619: step 13587, loss 0.587524.
Train: 2018-08-06T01:53:21.417965: step 13588, loss 0.570773.
Train: 2018-08-06T01:53:21.667294: step 13589, loss 0.60413.
Train: 2018-08-06T01:53:21.856760: step 13590, loss 0.580184.
Test: 2018-08-06T01:53:23.127360: step 13590, loss 0.547283.
Train: 2018-08-06T01:53:23.365722: step 13591, loss 0.545881.
Train: 2018-08-06T01:53:23.615087: step 13592, loss 0.537672.
Train: 2018-08-06T01:53:23.867415: step 13593, loss 0.678093.
Train: 2018-08-06T01:53:24.113724: step 13594, loss 0.537861.
Train: 2018-08-06T01:53:24.359099: step 13595, loss 0.57896.
Train: 2018-08-06T01:53:24.600455: step 13596, loss 0.562594.
Train: 2018-08-06T01:53:24.850780: step 13597, loss 0.595206.
Train: 2018-08-06T01:53:25.098089: step 13598, loss 0.570788.
Train: 2018-08-06T01:53:25.345453: step 13599, loss 0.489802.
Train: 2018-08-06T01:53:25.603738: step 13600, loss 0.50606.
Test: 2018-08-06T01:53:26.867358: step 13600, loss 0.550092.
Train: 2018-08-06T01:53:27.784990: step 13601, loss 0.546588.
Train: 2018-08-06T01:53:28.032342: step 13602, loss 0.635924.
Train: 2018-08-06T01:53:28.279650: step 13603, loss 0.578979.
Train: 2018-08-06T01:53:28.526989: step 13604, loss 0.578892.
Train: 2018-08-06T01:53:28.769340: step 13605, loss 0.546642.
Train: 2018-08-06T01:53:29.027678: step 13606, loss 0.562772.
Train: 2018-08-06T01:53:29.287000: step 13607, loss 0.586925.
Train: 2018-08-06T01:53:29.536318: step 13608, loss 0.546716.
Train: 2018-08-06T01:53:29.780636: step 13609, loss 0.57886.
Train: 2018-08-06T01:53:30.030965: step 13610, loss 0.578857.
Test: 2018-08-06T01:53:31.303562: step 13610, loss 0.549785.
Train: 2018-08-06T01:53:31.541925: step 13611, loss 0.594925.
Train: 2018-08-06T01:53:31.785299: step 13612, loss 0.594837.
Train: 2018-08-06T01:53:32.036632: step 13613, loss 0.546835.
Train: 2018-08-06T01:53:32.293944: step 13614, loss 0.546962.
Train: 2018-08-06T01:53:32.555246: step 13615, loss 0.578847.
Train: 2018-08-06T01:53:32.817514: step 13616, loss 0.570871.
Train: 2018-08-06T01:53:33.063855: step 13617, loss 0.586885.
Train: 2018-08-06T01:53:33.310221: step 13618, loss 0.547071.
Train: 2018-08-06T01:53:33.562546: step 13619, loss 0.547072.
Train: 2018-08-06T01:53:33.818835: step 13620, loss 0.555017.
Test: 2018-08-06T01:53:35.105394: step 13620, loss 0.549336.
Train: 2018-08-06T01:53:35.342790: step 13621, loss 0.531171.
Train: 2018-08-06T01:53:35.587132: step 13622, loss 0.531148.
Train: 2018-08-06T01:53:35.838462: step 13623, loss 0.507191.
Train: 2018-08-06T01:53:36.100764: step 13624, loss 0.578859.
Train: 2018-08-06T01:53:36.343115: step 13625, loss 0.634907.
Train: 2018-08-06T01:53:36.590424: step 13626, loss 0.602889.
Train: 2018-08-06T01:53:36.836765: step 13627, loss 0.538833.
Train: 2018-08-06T01:53:37.088092: step 13628, loss 0.546824.
Train: 2018-08-06T01:53:37.333465: step 13629, loss 0.57888.
Train: 2018-08-06T01:53:37.582800: step 13630, loss 0.458551.
Test: 2018-08-06T01:53:38.847387: step 13630, loss 0.550086.
Train: 2018-08-06T01:53:39.087744: step 13631, loss 0.62715.
Train: 2018-08-06T01:53:39.333088: step 13632, loss 0.578863.
Train: 2018-08-06T01:53:39.578433: step 13633, loss 0.578885.
Train: 2018-08-06T01:53:39.832752: step 13634, loss 0.53046.
Train: 2018-08-06T01:53:40.076101: step 13635, loss 0.506188.
Train: 2018-08-06T01:53:40.323440: step 13636, loss 0.530313.
Train: 2018-08-06T01:53:40.572773: step 13637, loss 0.684706.
Train: 2018-08-06T01:53:40.829118: step 13638, loss 0.497512.
Train: 2018-08-06T01:53:41.079454: step 13639, loss 0.58716.
Train: 2018-08-06T01:53:41.327756: step 13640, loss 0.595145.
Test: 2018-08-06T01:53:42.592371: step 13640, loss 0.547716.
Train: 2018-08-06T01:53:42.835750: step 13641, loss 0.571051.
Train: 2018-08-06T01:53:43.086052: step 13642, loss 0.570685.
Train: 2018-08-06T01:53:43.343364: step 13643, loss 0.489504.
Train: 2018-08-06T01:53:43.589706: step 13644, loss 0.497499.
Train: 2018-08-06T01:53:43.838041: step 13645, loss 0.603381.
Train: 2018-08-06T01:53:44.097355: step 13646, loss 0.497033.
Train: 2018-08-06T01:53:44.343692: step 13647, loss 0.578846.
Train: 2018-08-06T01:53:44.588069: step 13648, loss 0.546162.
Train: 2018-08-06T01:53:44.838419: step 13649, loss 0.587312.
Train: 2018-08-06T01:53:45.086702: step 13650, loss 0.595879.
Test: 2018-08-06T01:53:46.345335: step 13650, loss 0.549765.
Train: 2018-08-06T01:53:46.581728: step 13651, loss 0.546612.
Train: 2018-08-06T01:53:46.825080: step 13652, loss 0.57077.
Train: 2018-08-06T01:53:47.072424: step 13653, loss 0.58699.
Train: 2018-08-06T01:53:47.322747: step 13654, loss 0.562521.
Train: 2018-08-06T01:53:47.582033: step 13655, loss 0.579004.
Train: 2018-08-06T01:53:47.827405: step 13656, loss 0.546149.
Train: 2018-08-06T01:53:48.074710: step 13657, loss 0.554379.
Train: 2018-08-06T01:53:48.322060: step 13658, loss 0.455719.
Train: 2018-08-06T01:53:48.579391: step 13659, loss 0.56254.
Train: 2018-08-06T01:53:48.824705: step 13660, loss 0.529539.
Test: 2018-08-06T01:53:50.103284: step 13660, loss 0.548698.
Train: 2018-08-06T01:53:50.344670: step 13661, loss 0.504677.
Train: 2018-08-06T01:53:50.598985: step 13662, loss 0.612185.
Train: 2018-08-06T01:53:50.860261: step 13663, loss 0.52926.
Train: 2018-08-06T01:53:51.121593: step 13664, loss 0.54581.
Train: 2018-08-06T01:53:51.366905: step 13665, loss 0.529091.
Train: 2018-08-06T01:53:51.624226: step 13666, loss 0.570772.
Train: 2018-08-06T01:53:51.867568: step 13667, loss 0.621005.
Train: 2018-08-06T01:53:52.115934: step 13668, loss 0.545651.
Train: 2018-08-06T01:53:52.363273: step 13669, loss 0.545635.
Train: 2018-08-06T01:53:52.609583: step 13670, loss 0.562395.
Test: 2018-08-06T01:53:53.892152: step 13670, loss 0.549533.
Train: 2018-08-06T01:53:54.129549: step 13671, loss 0.537212.
Train: 2018-08-06T01:53:54.385832: step 13672, loss 0.57079.
Train: 2018-08-06T01:53:54.631206: step 13673, loss 0.503555.
Train: 2018-08-06T01:53:54.881537: step 13674, loss 0.604462.
Train: 2018-08-06T01:53:55.127878: step 13675, loss 0.511855.
Train: 2018-08-06T01:53:55.373191: step 13676, loss 0.545511.
Train: 2018-08-06T01:53:55.623553: step 13677, loss 0.655201.
Train: 2018-08-06T01:53:55.871858: step 13678, loss 0.596095.
Train: 2018-08-06T01:53:56.118199: step 13679, loss 0.579214.
Train: 2018-08-06T01:53:56.364545: step 13680, loss 0.570788.
Test: 2018-08-06T01:53:57.639132: step 13680, loss 0.549354.
Train: 2018-08-06T01:53:57.874534: step 13681, loss 0.579167.
Train: 2018-08-06T01:53:58.121841: step 13682, loss 0.58751.
Train: 2018-08-06T01:53:58.372172: step 13683, loss 0.537386.
Train: 2018-08-06T01:53:58.619510: step 13684, loss 0.58742.
Train: 2018-08-06T01:53:58.867869: step 13685, loss 0.545833.
Train: 2018-08-06T01:53:59.122197: step 13686, loss 0.537587.
Train: 2018-08-06T01:53:59.369504: step 13687, loss 0.653575.
Train: 2018-08-06T01:53:59.617848: step 13688, loss 0.512961.
Train: 2018-08-06T01:53:59.865212: step 13689, loss 0.644928.
Train: 2018-08-06T01:54:00.115510: step 13690, loss 0.546119.
Test: 2018-08-06T01:54:01.382121: step 13690, loss 0.548516.
Train: 2018-08-06T01:54:01.619488: step 13691, loss 0.529808.
Train: 2018-08-06T01:54:01.864831: step 13692, loss 0.464475.
Train: 2018-08-06T01:54:02.110175: step 13693, loss 0.554408.
Train: 2018-08-06T01:54:02.364520: step 13694, loss 0.587132.
Train: 2018-08-06T01:54:02.611832: step 13695, loss 0.595313.
Train: 2018-08-06T01:54:02.858205: step 13696, loss 0.628001.
Train: 2018-08-06T01:54:03.104515: step 13697, loss 0.529974.
Train: 2018-08-06T01:54:03.347864: step 13698, loss 0.481125.
Train: 2018-08-06T01:54:03.594232: step 13699, loss 0.660481.
Train: 2018-08-06T01:54:03.854510: step 13700, loss 0.611495.
Test: 2018-08-06T01:54:05.130098: step 13700, loss 0.548157.
Train: 2018-08-06T01:54:06.042039: step 13701, loss 0.530162.
Train: 2018-08-06T01:54:06.290407: step 13702, loss 0.57079.
Train: 2018-08-06T01:54:06.534754: step 13703, loss 0.603204.
Train: 2018-08-06T01:54:06.790039: step 13704, loss 0.546549.
Train: 2018-08-06T01:54:07.034412: step 13705, loss 0.546593.
Train: 2018-08-06T01:54:07.286711: step 13706, loss 0.611143.
Train: 2018-08-06T01:54:07.534050: step 13707, loss 0.570826.
Train: 2018-08-06T01:54:07.779419: step 13708, loss 0.506546.
Train: 2018-08-06T01:54:08.025742: step 13709, loss 0.514603.
Train: 2018-08-06T01:54:08.271079: step 13710, loss 0.562793.
Test: 2018-08-06T01:54:09.542678: step 13710, loss 0.54879.
Train: 2018-08-06T01:54:09.832928: step 13711, loss 0.611059.
Train: 2018-08-06T01:54:10.081239: step 13712, loss 0.586918.
Train: 2018-08-06T01:54:10.328602: step 13713, loss 0.578872.
Train: 2018-08-06T01:54:10.571974: step 13714, loss 0.610964.
Train: 2018-08-06T01:54:10.818297: step 13715, loss 0.547195.
Train: 2018-08-06T01:54:11.078571: step 13716, loss 0.531412.
Train: 2018-08-06T01:54:11.320954: step 13717, loss 0.578864.
Train: 2018-08-06T01:54:11.577268: step 13718, loss 0.59487.
Train: 2018-08-06T01:54:11.822606: step 13719, loss 0.530887.
Train: 2018-08-06T01:54:12.066929: step 13720, loss 0.586861.
Test: 2018-08-06T01:54:13.330548: step 13720, loss 0.550269.
Train: 2018-08-06T01:54:13.581876: step 13721, loss 0.538906.
Train: 2018-08-06T01:54:13.832210: step 13722, loss 0.530907.
Train: 2018-08-06T01:54:14.074589: step 13723, loss 0.55486.
Train: 2018-08-06T01:54:14.326894: step 13724, loss 0.48276.
Train: 2018-08-06T01:54:14.586191: step 13725, loss 0.546734.
Train: 2018-08-06T01:54:14.844499: step 13726, loss 0.562763.
Train: 2018-08-06T01:54:15.094830: step 13727, loss 0.530381.
Train: 2018-08-06T01:54:15.351175: step 13728, loss 0.546499.
Train: 2018-08-06T01:54:15.599505: step 13729, loss 0.562576.
Train: 2018-08-06T01:54:15.849812: step 13730, loss 0.529833.
Test: 2018-08-06T01:54:17.136371: step 13730, loss 0.547284.
Train: 2018-08-06T01:54:17.386701: step 13731, loss 0.611958.
Train: 2018-08-06T01:54:17.634040: step 13732, loss 0.58744.
Train: 2018-08-06T01:54:17.883403: step 13733, loss 0.488314.
Train: 2018-08-06T01:54:18.128716: step 13734, loss 0.521374.
Train: 2018-08-06T01:54:18.377090: step 13735, loss 0.54584.
Train: 2018-08-06T01:54:18.636384: step 13736, loss 0.537031.
Train: 2018-08-06T01:54:18.882731: step 13737, loss 0.587728.
Train: 2018-08-06T01:54:19.130072: step 13738, loss 0.596584.
Train: 2018-08-06T01:54:19.374386: step 13739, loss 0.596004.
Train: 2018-08-06T01:54:19.617760: step 13740, loss 0.579166.
Test: 2018-08-06T01:54:20.892326: step 13740, loss 0.5498.
Train: 2018-08-06T01:54:21.088801: step 13741, loss 0.580292.
Train: 2018-08-06T01:54:21.334144: step 13742, loss 0.529054.
Train: 2018-08-06T01:54:21.576495: step 13743, loss 0.554177.
Train: 2018-08-06T01:54:21.823835: step 13744, loss 0.57078.
Train: 2018-08-06T01:54:22.067184: step 13745, loss 0.562448.
Train: 2018-08-06T01:54:22.316530: step 13746, loss 0.570759.
Train: 2018-08-06T01:54:22.563891: step 13747, loss 0.537552.
Train: 2018-08-06T01:54:22.818174: step 13748, loss 0.595658.
Train: 2018-08-06T01:54:23.068537: step 13749, loss 0.562468.
Train: 2018-08-06T01:54:23.324851: step 13750, loss 0.570748.
Test: 2018-08-06T01:54:24.589439: step 13750, loss 0.54788.
Train: 2018-08-06T01:54:24.824840: step 13751, loss 0.545901.
Train: 2018-08-06T01:54:25.073177: step 13752, loss 0.545923.
Train: 2018-08-06T01:54:25.319486: step 13753, loss 0.57075.
Train: 2018-08-06T01:54:25.568849: step 13754, loss 0.554232.
Train: 2018-08-06T01:54:25.816189: step 13755, loss 0.529398.
Train: 2018-08-06T01:54:26.071505: step 13756, loss 0.620341.
Train: 2018-08-06T01:54:26.320832: step 13757, loss 0.545956.
Train: 2018-08-06T01:54:26.580116: step 13758, loss 0.57902.
Train: 2018-08-06T01:54:26.839452: step 13759, loss 0.612011.
Train: 2018-08-06T01:54:27.086791: step 13760, loss 0.504852.
Test: 2018-08-06T01:54:28.366338: step 13760, loss 0.548208.
Train: 2018-08-06T01:54:28.612679: step 13761, loss 0.537804.
Train: 2018-08-06T01:54:28.858023: step 13762, loss 0.579013.
Train: 2018-08-06T01:54:29.104394: step 13763, loss 0.570777.
Train: 2018-08-06T01:54:29.360680: step 13764, loss 0.587228.
Train: 2018-08-06T01:54:29.620015: step 13765, loss 0.529701.
Train: 2018-08-06T01:54:29.873336: step 13766, loss 0.521506.
Train: 2018-08-06T01:54:30.141590: step 13767, loss 0.661113.
Train: 2018-08-06T01:54:30.398958: step 13768, loss 0.644558.
Train: 2018-08-06T01:54:30.655242: step 13769, loss 0.54625.
Train: 2018-08-06T01:54:30.926519: step 13770, loss 0.562623.
Test: 2018-08-06T01:54:32.204075: step 13770, loss 0.549254.
Train: 2018-08-06T01:54:32.458425: step 13771, loss 0.530119.
Train: 2018-08-06T01:54:32.728672: step 13772, loss 0.505814.
Train: 2018-08-06T01:54:32.995957: step 13773, loss 0.562665.
Train: 2018-08-06T01:54:33.256260: step 13774, loss 0.58703.
Train: 2018-08-06T01:54:33.518559: step 13775, loss 0.497727.
Train: 2018-08-06T01:54:33.773875: step 13776, loss 0.58704.
Train: 2018-08-06T01:54:34.034206: step 13777, loss 0.611432.
Train: 2018-08-06T01:54:34.292489: step 13778, loss 0.627665.
Train: 2018-08-06T01:54:34.553792: step 13779, loss 0.587008.
Train: 2018-08-06T01:54:34.819082: step 13780, loss 0.522271.
Test: 2018-08-06T01:54:36.117608: step 13780, loss 0.547559.
Train: 2018-08-06T01:54:36.369938: step 13781, loss 0.554639.
Train: 2018-08-06T01:54:36.634258: step 13782, loss 0.61924.
Train: 2018-08-06T01:54:36.896564: step 13783, loss 0.6111.
Train: 2018-08-06T01:54:37.159852: step 13784, loss 0.506544.
Train: 2018-08-06T01:54:37.423156: step 13785, loss 0.522667.
Train: 2018-08-06T01:54:37.690408: step 13786, loss 0.514649.
Train: 2018-08-06T01:54:37.953703: step 13787, loss 0.570858.
Train: 2018-08-06T01:54:38.218022: step 13788, loss 0.570856.
Train: 2018-08-06T01:54:38.482291: step 13789, loss 0.578882.
Train: 2018-08-06T01:54:38.754562: step 13790, loss 0.538626.
Test: 2018-08-06T01:54:40.037131: step 13790, loss 0.54838.
Train: 2018-08-06T01:54:40.300427: step 13791, loss 0.619158.
Train: 2018-08-06T01:54:40.562753: step 13792, loss 0.59499.
Train: 2018-08-06T01:54:40.827050: step 13793, loss 0.586905.
Train: 2018-08-06T01:54:41.091340: step 13794, loss 0.522643.
Train: 2018-08-06T01:54:41.352638: step 13795, loss 0.63509.
Train: 2018-08-06T01:54:41.617904: step 13796, loss 0.506718.
Train: 2018-08-06T01:54:41.884223: step 13797, loss 0.634967.
Train: 2018-08-06T01:54:42.149515: step 13798, loss 0.538864.
Train: 2018-08-06T01:54:42.427769: step 13799, loss 0.530909.
Train: 2018-08-06T01:54:42.695056: step 13800, loss 0.51493.
Test: 2018-08-06T01:54:43.966623: step 13800, loss 0.549318.
Train: 2018-08-06T01:54:44.881595: step 13801, loss 0.538863.
Train: 2018-08-06T01:54:45.156858: step 13802, loss 0.554828.
Train: 2018-08-06T01:54:45.415168: step 13803, loss 0.522687.
Train: 2018-08-06T01:54:45.679494: step 13804, loss 0.530602.
Train: 2018-08-06T01:54:45.944752: step 13805, loss 0.643434.
Train: 2018-08-06T01:54:46.215029: step 13806, loss 0.586964.
Train: 2018-08-06T01:54:46.484332: step 13807, loss 0.538488.
Train: 2018-08-06T01:54:46.752613: step 13808, loss 0.570803.
Train: 2018-08-06T01:54:47.012943: step 13809, loss 0.538429.
Train: 2018-08-06T01:54:47.288212: step 13810, loss 0.578897.
Test: 2018-08-06T01:54:48.565764: step 13810, loss 0.549143.
Train: 2018-08-06T01:54:48.819116: step 13811, loss 0.554577.
Train: 2018-08-06T01:54:49.086373: step 13812, loss 0.489638.
Train: 2018-08-06T01:54:49.353686: step 13813, loss 0.578917.
Train: 2018-08-06T01:54:49.615957: step 13814, loss 0.562626.
Train: 2018-08-06T01:54:49.881277: step 13815, loss 0.652384.
Train: 2018-08-06T01:54:50.158537: step 13816, loss 0.529986.
Train: 2018-08-06T01:54:50.435764: step 13817, loss 0.538133.
Train: 2018-08-06T01:54:50.701086: step 13818, loss 0.562605.
Train: 2018-08-06T01:54:50.967343: step 13819, loss 0.538084.
Train: 2018-08-06T01:54:51.234628: step 13820, loss 0.546225.
Test: 2018-08-06T01:54:52.508221: step 13820, loss 0.548139.
Train: 2018-08-06T01:54:52.776505: step 13821, loss 0.578955.
Train: 2018-08-06T01:54:53.054790: step 13822, loss 0.472391.
Train: 2018-08-06T01:54:53.320051: step 13823, loss 0.578978.
Train: 2018-08-06T01:54:53.589365: step 13824, loss 0.562523.
Train: 2018-08-06T01:54:53.854622: step 13825, loss 0.661489.
Train: 2018-08-06T01:54:54.121907: step 13826, loss 0.562513.
Train: 2018-08-06T01:54:54.387197: step 13827, loss 0.52132.
Train: 2018-08-06T01:54:54.652487: step 13828, loss 0.521304.
Train: 2018-08-06T01:54:54.931741: step 13829, loss 0.546004.
Train: 2018-08-06T01:54:55.216011: step 13830, loss 0.562495.
Test: 2018-08-06T01:54:56.497552: step 13830, loss 0.548105.
Train: 2018-08-06T01:54:56.753894: step 13831, loss 0.537678.
Train: 2018-08-06T01:54:57.085012: step 13832, loss 0.587319.
Train: 2018-08-06T01:54:57.365258: step 13833, loss 0.587331.
Train: 2018-08-06T01:54:57.630557: step 13834, loss 0.529323.
Train: 2018-08-06T01:54:57.909776: step 13835, loss 0.57905.
Train: 2018-08-06T01:54:58.176094: step 13836, loss 0.562465.
Train: 2018-08-06T01:54:58.450330: step 13837, loss 0.529289.
Train: 2018-08-06T01:54:58.719635: step 13838, loss 0.637152.
Train: 2018-08-06T01:54:58.985928: step 13839, loss 0.521012.
Train: 2018-08-06T01:54:59.264155: step 13840, loss 0.554178.
Test: 2018-08-06T01:55:00.553720: step 13840, loss 0.548815.
Train: 2018-08-06T01:55:00.813037: step 13841, loss 0.587336.
Train: 2018-08-06T01:55:01.093286: step 13842, loss 0.52105.
Train: 2018-08-06T01:55:01.360548: step 13843, loss 0.537613.
Train: 2018-08-06T01:55:01.627833: step 13844, loss 0.521011.
Train: 2018-08-06T01:55:01.901102: step 13845, loss 0.504341.
Train: 2018-08-06T01:55:02.172376: step 13846, loss 0.570761.
Train: 2018-08-06T01:55:02.438692: step 13847, loss 0.537419.
Train: 2018-08-06T01:55:02.705980: step 13848, loss 0.554063.
Train: 2018-08-06T01:55:02.982211: step 13849, loss 0.528934.
Train: 2018-08-06T01:55:03.258472: step 13850, loss 0.663038.
Test: 2018-08-06T01:55:04.552012: step 13850, loss 0.548583.
Train: 2018-08-06T01:55:04.820295: step 13851, loss 0.562393.
Train: 2018-08-06T01:55:05.088577: step 13852, loss 0.486947.
Train: 2018-08-06T01:55:05.352901: step 13853, loss 0.562386.
Train: 2018-08-06T01:55:05.630160: step 13854, loss 0.545584.
Train: 2018-08-06T01:55:05.904396: step 13855, loss 0.579198.
Train: 2018-08-06T01:55:06.171680: step 13856, loss 0.579209.
Train: 2018-08-06T01:55:06.454941: step 13857, loss 0.587611.
Train: 2018-08-06T01:55:06.722234: step 13858, loss 0.646453.
Train: 2018-08-06T01:55:06.992511: step 13859, loss 0.554011.
Train: 2018-08-06T01:55:07.260798: step 13860, loss 0.554037.
Test: 2018-08-06T01:55:08.547327: step 13860, loss 0.547507.
Train: 2018-08-06T01:55:08.808629: step 13861, loss 0.587474.
Train: 2018-08-06T01:55:09.092869: step 13862, loss 0.470771.
Train: 2018-08-06T01:55:09.359157: step 13863, loss 0.570764.
Train: 2018-08-06T01:55:09.627439: step 13864, loss 0.579088.
Train: 2018-08-06T01:55:09.898713: step 13865, loss 0.587395.
Train: 2018-08-06T01:55:10.166995: step 13866, loss 0.529234.
Train: 2018-08-06T01:55:10.437273: step 13867, loss 0.562459.
Train: 2018-08-06T01:55:10.702564: step 13868, loss 0.529292.
Train: 2018-08-06T01:55:10.967855: step 13869, loss 0.554173.
Train: 2018-08-06T01:55:11.243118: step 13870, loss 0.60393.
Test: 2018-08-06T01:55:12.536658: step 13870, loss 0.548062.
Train: 2018-08-06T01:55:12.802946: step 13871, loss 0.4879.
Train: 2018-08-06T01:55:13.072232: step 13872, loss 0.521012.
Train: 2018-08-06T01:55:13.340524: step 13873, loss 0.662081.
Train: 2018-08-06T01:55:13.608825: step 13874, loss 0.603937.
Train: 2018-08-06T01:55:13.886060: step 13875, loss 0.537635.
Train: 2018-08-06T01:55:14.151370: step 13876, loss 0.562485.
Train: 2018-08-06T01:55:14.415665: step 13877, loss 0.603808.
Train: 2018-08-06T01:55:14.687937: step 13878, loss 0.570756.
Train: 2018-08-06T01:55:14.953195: step 13879, loss 0.611917.
Train: 2018-08-06T01:55:15.237467: step 13880, loss 0.587179.
Test: 2018-08-06T01:55:16.513024: step 13880, loss 0.549477.
Train: 2018-08-06T01:55:16.775347: step 13881, loss 0.562581.
Train: 2018-08-06T01:55:17.057600: step 13882, loss 0.587095.
Train: 2018-08-06T01:55:17.327845: step 13883, loss 0.554505.
Train: 2018-08-06T01:55:17.601150: step 13884, loss 0.554554.
Train: 2018-08-06T01:55:17.869423: step 13885, loss 0.562697.
Train: 2018-08-06T01:55:18.135685: step 13886, loss 0.562719.
Train: 2018-08-06T01:55:18.405962: step 13887, loss 0.538524.
Train: 2018-08-06T01:55:18.674245: step 13888, loss 0.554688.
Train: 2018-08-06T01:55:18.943557: step 13889, loss 0.5547.
Train: 2018-08-06T01:55:19.212810: step 13890, loss 0.530534.
Test: 2018-08-06T01:55:20.483411: step 13890, loss 0.548924.
Train: 2018-08-06T01:55:20.755714: step 13891, loss 0.514391.
Train: 2018-08-06T01:55:20.979117: step 13892, loss 0.57996.
Train: 2018-08-06T01:55:21.249393: step 13893, loss 0.611215.
Train: 2018-08-06T01:55:21.519640: step 13894, loss 0.554852.
Train: 2018-08-06T01:55:21.787923: step 13895, loss 0.634761.
Train: 2018-08-06T01:55:22.053213: step 13896, loss 0.523899.
Train: 2018-08-06T01:55:22.330472: step 13897, loss 0.5082.
Train: 2018-08-06T01:55:22.600798: step 13898, loss 0.610764.
Train: 2018-08-06T01:55:22.868034: step 13899, loss 0.594917.
Train: 2018-08-06T01:55:23.142301: step 13900, loss 0.619056.
Test: 2018-08-06T01:55:24.405921: step 13900, loss 0.549379.
Train: 2018-08-06T01:55:25.374454: step 13901, loss 0.514998.
Train: 2018-08-06T01:55:25.643708: step 13902, loss 0.538732.
Train: 2018-08-06T01:55:25.922988: step 13903, loss 0.522404.
Train: 2018-08-06T01:55:26.195267: step 13904, loss 0.578886.
Train: 2018-08-06T01:55:26.462543: step 13905, loss 0.570807.
Train: 2018-08-06T01:55:26.729828: step 13906, loss 0.481814.
Train: 2018-08-06T01:55:26.996091: step 13907, loss 0.562679.
Train: 2018-08-06T01:55:27.262380: step 13908, loss 0.562647.
Train: 2018-08-06T01:55:27.529689: step 13909, loss 0.570789.
Train: 2018-08-06T01:55:27.795978: step 13910, loss 0.546315.
Test: 2018-08-06T01:55:29.075530: step 13910, loss 0.548395.
Train: 2018-08-06T01:55:29.341846: step 13911, loss 0.578943.
Train: 2018-08-06T01:55:29.608106: step 13912, loss 0.464445.
Train: 2018-08-06T01:55:29.877412: step 13913, loss 0.537942.
Train: 2018-08-06T01:55:30.145694: step 13914, loss 0.694194.
Train: 2018-08-06T01:55:30.413984: step 13915, loss 0.620152.
Train: 2018-08-06T01:55:30.685259: step 13916, loss 0.620089.
Train: 2018-08-06T01:55:30.957498: step 13917, loss 0.54615.
Train: 2018-08-06T01:55:31.237782: step 13918, loss 0.595334.
Train: 2018-08-06T01:55:31.505033: step 13919, loss 0.619798.
Train: 2018-08-06T01:55:31.774343: step 13920, loss 0.56263.
Test: 2018-08-06T01:55:33.038931: step 13920, loss 0.550596.
Train: 2018-08-06T01:55:33.299235: step 13921, loss 0.546428.
Train: 2018-08-06T01:55:33.571531: step 13922, loss 0.59511.
Train: 2018-08-06T01:55:33.840787: step 13923, loss 0.538468.
Train: 2018-08-06T01:55:34.105080: step 13924, loss 0.603101.
Train: 2018-08-06T01:55:34.376354: step 13925, loss 0.538601.
Train: 2018-08-06T01:55:34.651643: step 13926, loss 0.58694.
Train: 2018-08-06T01:55:34.917934: step 13927, loss 0.611029.
Train: 2018-08-06T01:55:35.188183: step 13928, loss 0.498841.
Train: 2018-08-06T01:55:35.453502: step 13929, loss 0.538893.
Train: 2018-08-06T01:55:35.720759: step 13930, loss 0.642778.
Test: 2018-08-06T01:55:36.981387: step 13930, loss 0.54922.
Train: 2018-08-06T01:55:37.261668: step 13931, loss 0.570893.
Train: 2018-08-06T01:55:37.540893: step 13932, loss 0.586825.
Train: 2018-08-06T01:55:37.806212: step 13933, loss 0.602692.
Train: 2018-08-06T01:55:38.087460: step 13934, loss 0.578858.
Train: 2018-08-06T01:55:38.350779: step 13935, loss 0.547245.
Train: 2018-08-06T01:55:38.618092: step 13936, loss 0.602534.
Train: 2018-08-06T01:55:38.900339: step 13937, loss 0.547379.
Train: 2018-08-06T01:55:39.168620: step 13938, loss 0.618168.
Train: 2018-08-06T01:55:39.436873: step 13939, loss 0.547514.
Train: 2018-08-06T01:55:39.706184: step 13940, loss 0.625848.
Test: 2018-08-06T01:55:40.973763: step 13940, loss 0.549343.
Train: 2018-08-06T01:55:41.233086: step 13941, loss 0.52425.
Train: 2018-08-06T01:55:41.501378: step 13942, loss 0.563302.
Train: 2018-08-06T01:55:41.766646: step 13943, loss 0.571106.
Train: 2018-08-06T01:55:42.035947: step 13944, loss 0.524438.
Train: 2018-08-06T01:55:42.305233: step 13945, loss 0.55555.
Train: 2018-08-06T01:55:42.577474: step 13946, loss 0.547747.
Train: 2018-08-06T01:55:42.843762: step 13947, loss 0.571096.
Train: 2018-08-06T01:55:43.124012: step 13948, loss 0.524247.
Train: 2018-08-06T01:55:43.393324: step 13949, loss 0.555417.
Train: 2018-08-06T01:55:43.659605: step 13950, loss 0.539673.
Test: 2018-08-06T01:55:44.929185: step 13950, loss 0.55092.
Train: 2018-08-06T01:55:45.192481: step 13951, loss 0.602451.
Train: 2018-08-06T01:55:45.475724: step 13952, loss 0.539485.
Train: 2018-08-06T01:55:45.744005: step 13953, loss 0.5394.
Train: 2018-08-06T01:55:46.016291: step 13954, loss 0.594703.
Train: 2018-08-06T01:55:46.279600: step 13955, loss 0.515396.
Train: 2018-08-06T01:55:46.560840: step 13956, loss 0.523155.
Train: 2018-08-06T01:55:46.832097: step 13957, loss 0.546878.
Train: 2018-08-06T01:55:47.104396: step 13958, loss 0.562953.
Train: 2018-08-06T01:55:47.377668: step 13959, loss 0.530623.
Train: 2018-08-06T01:55:47.650932: step 13960, loss 0.522101.
Test: 2018-08-06T01:55:48.938463: step 13960, loss 0.548086.
Train: 2018-08-06T01:55:49.211763: step 13961, loss 0.481291.
Train: 2018-08-06T01:55:49.487022: step 13962, loss 0.619639.
Train: 2018-08-06T01:55:49.756301: step 13963, loss 0.504482.
Train: 2018-08-06T01:55:50.027566: step 13964, loss 0.553747.
Train: 2018-08-06T01:55:50.299830: step 13965, loss 0.545654.
Train: 2018-08-06T01:55:50.569109: step 13966, loss 0.551752.
Train: 2018-08-06T01:55:50.848362: step 13967, loss 0.625438.
Train: 2018-08-06T01:55:51.117671: step 13968, loss 0.561255.
Train: 2018-08-06T01:55:51.385925: step 13969, loss 0.57828.
Train: 2018-08-06T01:55:51.660189: step 13970, loss 0.588293.
Test: 2018-08-06T01:55:52.956723: step 13970, loss 0.549906.
Train: 2018-08-06T01:55:53.218064: step 13971, loss 0.578824.
Train: 2018-08-06T01:55:53.488332: step 13972, loss 0.51152.
Train: 2018-08-06T01:55:53.762572: step 13973, loss 0.554966.
Train: 2018-08-06T01:55:54.035837: step 13974, loss 0.562612.
Train: 2018-08-06T01:55:54.304174: step 13975, loss 0.630369.
Train: 2018-08-06T01:55:54.570408: step 13976, loss 0.512853.
Train: 2018-08-06T01:55:54.839693: step 13977, loss 0.520159.
Train: 2018-08-06T01:55:55.106974: step 13978, loss 0.620435.
Train: 2018-08-06T01:55:55.374292: step 13979, loss 0.645918.
Train: 2018-08-06T01:55:55.646529: step 13980, loss 0.562591.
Test: 2018-08-06T01:55:56.909153: step 13980, loss 0.549355.
Train: 2018-08-06T01:55:57.163472: step 13981, loss 0.546046.
Train: 2018-08-06T01:55:57.440756: step 13982, loss 0.603599.
Train: 2018-08-06T01:55:57.706023: step 13983, loss 0.538048.
Train: 2018-08-06T01:55:57.976327: step 13984, loss 0.521763.
Train: 2018-08-06T01:55:58.243583: step 13985, loss 0.627902.
Train: 2018-08-06T01:55:58.511866: step 13986, loss 0.562635.
Train: 2018-08-06T01:55:58.781148: step 13987, loss 0.61954.
Train: 2018-08-06T01:55:59.058436: step 13988, loss 0.546461.
Train: 2018-08-06T01:55:59.330703: step 13989, loss 0.603103.
Train: 2018-08-06T01:55:59.600987: step 13990, loss 0.49826.
Test: 2018-08-06T01:56:00.898484: step 13990, loss 0.548934.
Train: 2018-08-06T01:56:01.160784: step 13991, loss 0.506395.
Train: 2018-08-06T01:56:01.431061: step 13992, loss 0.570725.
Train: 2018-08-06T01:56:01.702375: step 13993, loss 0.538615.
Train: 2018-08-06T01:56:01.976601: step 13994, loss 0.554588.
Train: 2018-08-06T01:56:02.241891: step 13995, loss 0.562829.
Train: 2018-08-06T01:56:02.510174: step 13996, loss 0.538348.
Train: 2018-08-06T01:56:02.775464: step 13997, loss 0.530303.
Train: 2018-08-06T01:56:03.037763: step 13998, loss 0.530204.
Train: 2018-08-06T01:56:03.305079: step 13999, loss 0.587136.
Train: 2018-08-06T01:56:03.575351: step 14000, loss 0.562695.
Test: 2018-08-06T01:56:04.842935: step 14000, loss 0.549139.
Train: 2018-08-06T01:56:05.759306: step 14001, loss 0.611438.
Train: 2018-08-06T01:56:06.027587: step 14002, loss 0.497395.
Train: 2018-08-06T01:56:06.295875: step 14003, loss 0.595432.
Train: 2018-08-06T01:56:06.574125: step 14004, loss 0.546182.
Train: 2018-08-06T01:56:06.853378: step 14005, loss 0.546208.
Train: 2018-08-06T01:56:07.133661: step 14006, loss 0.570741.
Train: 2018-08-06T01:56:07.401945: step 14007, loss 0.595401.
Train: 2018-08-06T01:56:07.669228: step 14008, loss 0.603371.
Train: 2018-08-06T01:56:07.935484: step 14009, loss 0.554396.
Train: 2018-08-06T01:56:08.206786: step 14010, loss 0.5708.
Test: 2018-08-06T01:56:09.474369: step 14010, loss 0.548509.
Train: 2018-08-06T01:56:09.732679: step 14011, loss 0.578853.
Train: 2018-08-06T01:56:10.000961: step 14012, loss 0.579003.
Train: 2018-08-06T01:56:10.273234: step 14013, loss 0.570601.
Train: 2018-08-06T01:56:10.544508: step 14014, loss 0.562623.
Train: 2018-08-06T01:56:10.807804: step 14015, loss 0.644241.
Train: 2018-08-06T01:56:11.076086: step 14016, loss 0.546575.
Train: 2018-08-06T01:56:11.343372: step 14017, loss 0.595122.
Train: 2018-08-06T01:56:11.620656: step 14018, loss 0.506269.
Train: 2018-08-06T01:56:11.890937: step 14019, loss 0.586923.
Train: 2018-08-06T01:56:12.161215: step 14020, loss 0.522594.
Test: 2018-08-06T01:56:13.425803: step 14020, loss 0.54882.
Train: 2018-08-06T01:56:13.751931: step 14021, loss 0.594947.
Train: 2018-08-06T01:56:14.021211: step 14022, loss 0.554781.
Train: 2018-08-06T01:56:14.296474: step 14023, loss 0.578869.
Train: 2018-08-06T01:56:14.564757: step 14024, loss 0.562831.
Train: 2018-08-06T01:56:14.834067: step 14025, loss 0.506764.
Train: 2018-08-06T01:56:15.101320: step 14026, loss 0.530767.
Train: 2018-08-06T01:56:15.376585: step 14027, loss 0.570842.
Train: 2018-08-06T01:56:15.644892: step 14028, loss 0.570835.
Train: 2018-08-06T01:56:15.917170: step 14029, loss 0.514512.
Train: 2018-08-06T01:56:16.199410: step 14030, loss 0.46602.
Test: 2018-08-06T01:56:17.452034: step 14030, loss 0.550682.
Train: 2018-08-06T01:56:17.715332: step 14031, loss 0.578897.
Train: 2018-08-06T01:56:17.984611: step 14032, loss 0.530195.
Train: 2018-08-06T01:56:18.260904: step 14033, loss 0.473012.
Train: 2018-08-06T01:56:18.527191: step 14034, loss 0.529823.
Train: 2018-08-06T01:56:18.795442: step 14035, loss 0.562521.
Train: 2018-08-06T01:56:19.063725: step 14036, loss 0.603827.
Train: 2018-08-06T01:56:19.332006: step 14037, loss 0.487802.
Train: 2018-08-06T01:56:19.607272: step 14038, loss 0.554122.
Train: 2018-08-06T01:56:19.874566: step 14039, loss 0.562418.
Train: 2018-08-06T01:56:20.144867: step 14040, loss 0.553997.
Test: 2018-08-06T01:56:21.429397: step 14040, loss 0.546959.
Train: 2018-08-06T01:56:21.687733: step 14041, loss 0.537108.
Train: 2018-08-06T01:56:21.955989: step 14042, loss 0.562361.
Train: 2018-08-06T01:56:22.171415: step 14043, loss 0.580381.
Train: 2018-08-06T01:56:22.444683: step 14044, loss 0.579363.
Train: 2018-08-06T01:56:22.710971: step 14045, loss 0.545329.
Train: 2018-08-06T01:56:22.980251: step 14046, loss 0.502827.
Train: 2018-08-06T01:56:23.257510: step 14047, loss 0.553869.
Train: 2018-08-06T01:56:23.525821: step 14048, loss 0.52823.
Train: 2018-08-06T01:56:23.793077: step 14049, loss 0.545256.
Train: 2018-08-06T01:56:24.059391: step 14050, loss 0.562323.
Test: 2018-08-06T01:56:25.349912: step 14050, loss 0.548394.
Train: 2018-08-06T01:56:25.607224: step 14051, loss 0.570902.
Train: 2018-08-06T01:56:25.876522: step 14052, loss 0.536556.
Train: 2018-08-06T01:56:26.156781: step 14053, loss 0.527911.
Train: 2018-08-06T01:56:26.427057: step 14054, loss 0.579597.
Train: 2018-08-06T01:56:26.691326: step 14055, loss 0.562331.
Train: 2018-08-06T01:56:26.964626: step 14056, loss 0.596828.
Train: 2018-08-06T01:56:27.230920: step 14057, loss 0.553713.
Train: 2018-08-06T01:56:27.500164: step 14058, loss 0.562335.
Train: 2018-08-06T01:56:27.770474: step 14059, loss 0.55372.
Train: 2018-08-06T01:56:28.041739: step 14060, loss 0.596748.
Test: 2018-08-06T01:56:29.313313: step 14060, loss 0.549379.
Train: 2018-08-06T01:56:29.584614: step 14061, loss 0.502227.
Train: 2018-08-06T01:56:29.851904: step 14062, loss 0.553749.
Train: 2018-08-06T01:56:30.125144: step 14063, loss 0.528031.
Train: 2018-08-06T01:56:30.393455: step 14064, loss 0.485127.
Train: 2018-08-06T01:56:30.660736: step 14065, loss 0.527988.
Train: 2018-08-06T01:56:30.929991: step 14066, loss 0.519324.
Train: 2018-08-06T01:56:31.200267: step 14067, loss 0.545093.
Train: 2018-08-06T01:56:31.472540: step 14068, loss 0.648681.
Train: 2018-08-06T01:56:31.740823: step 14069, loss 0.631402.
Train: 2018-08-06T01:56:32.004119: step 14070, loss 0.596853.
Test: 2018-08-06T01:56:33.267737: step 14070, loss 0.547975.
Train: 2018-08-06T01:56:33.539014: step 14071, loss 0.476338.
Train: 2018-08-06T01:56:33.808293: step 14072, loss 0.502195.
Train: 2018-08-06T01:56:34.079567: step 14073, loss 0.588163.
Train: 2018-08-06T01:56:34.351870: step 14074, loss 0.52804.
Train: 2018-08-06T01:56:34.623115: step 14075, loss 0.570979.
Train: 2018-08-06T01:56:34.891427: step 14076, loss 0.50229.
Train: 2018-08-06T01:56:35.163693: step 14077, loss 0.588081.
Train: 2018-08-06T01:56:35.433969: step 14078, loss 0.57949.
Train: 2018-08-06T01:56:35.705221: step 14079, loss 0.570903.
Train: 2018-08-06T01:56:35.973503: step 14080, loss 0.48531.
Test: 2018-08-06T01:56:37.240114: step 14080, loss 0.547448.
Train: 2018-08-06T01:56:37.505406: step 14081, loss 0.545214.
Train: 2018-08-06T01:56:37.781712: step 14082, loss 0.545209.
Train: 2018-08-06T01:56:38.050947: step 14083, loss 0.613735.
Train: 2018-08-06T01:56:38.316267: step 14084, loss 0.579453.
Train: 2018-08-06T01:56:38.591500: step 14085, loss 0.630733.
Train: 2018-08-06T01:56:38.860811: step 14086, loss 0.553817.
Train: 2018-08-06T01:56:39.131082: step 14087, loss 0.562341.
Train: 2018-08-06T01:56:39.393356: step 14088, loss 0.553865.
Train: 2018-08-06T01:56:39.672611: step 14089, loss 0.621633.
Train: 2018-08-06T01:56:39.935935: step 14090, loss 0.612999.
Test: 2018-08-06T01:56:41.198528: step 14090, loss 0.548156.
Train: 2018-08-06T01:56:41.456838: step 14091, loss 0.5792.
Train: 2018-08-06T01:56:41.723156: step 14092, loss 0.562406.
Train: 2018-08-06T01:56:41.994399: step 14093, loss 0.587438.
Train: 2018-08-06T01:56:42.269689: step 14094, loss 0.504352.
Train: 2018-08-06T01:56:42.534985: step 14095, loss 0.612152.
Train: 2018-08-06T01:56:42.801267: step 14096, loss 0.537756.
Train: 2018-08-06T01:56:43.069526: step 14097, loss 0.529619.
Train: 2018-08-06T01:56:43.337837: step 14098, loss 0.587186.
Train: 2018-08-06T01:56:43.618059: step 14099, loss 0.546176.
Train: 2018-08-06T01:56:43.882350: step 14100, loss 0.570766.
Test: 2018-08-06T01:56:45.154947: step 14100, loss 0.549145.
Train: 2018-08-06T01:56:46.075793: step 14101, loss 0.529924.
Train: 2018-08-06T01:56:46.339119: step 14102, loss 0.554443.
Train: 2018-08-06T01:56:46.609366: step 14103, loss 0.570778.
Train: 2018-08-06T01:56:46.874691: step 14104, loss 0.562624.
Train: 2018-08-06T01:56:47.141942: step 14105, loss 0.530019.
Train: 2018-08-06T01:56:47.409228: step 14106, loss 0.546313.
Train: 2018-08-06T01:56:47.672524: step 14107, loss 0.578929.
Train: 2018-08-06T01:56:47.939809: step 14108, loss 0.5708.
Train: 2018-08-06T01:56:48.203135: step 14109, loss 0.530004.
Train: 2018-08-06T01:56:48.481371: step 14110, loss 0.578948.
Test: 2018-08-06T01:56:49.774900: step 14110, loss 0.548031.
Train: 2018-08-06T01:56:50.031241: step 14111, loss 0.587117.
Train: 2018-08-06T01:56:50.293514: step 14112, loss 0.554402.
Train: 2018-08-06T01:56:50.563824: step 14113, loss 0.603412.
Train: 2018-08-06T01:56:50.830111: step 14114, loss 0.619713.
Train: 2018-08-06T01:56:51.097373: step 14115, loss 0.554553.
Train: 2018-08-06T01:56:51.359663: step 14116, loss 0.55456.
Train: 2018-08-06T01:56:51.640910: step 14117, loss 0.570796.
Train: 2018-08-06T01:56:51.911187: step 14118, loss 0.578893.
Train: 2018-08-06T01:56:52.171522: step 14119, loss 0.570807.
Train: 2018-08-06T01:56:52.432792: step 14120, loss 0.611155.
Test: 2018-08-06T01:56:53.702398: step 14120, loss 0.548779.
Train: 2018-08-06T01:56:53.960706: step 14121, loss 0.651317.
Train: 2018-08-06T01:56:54.227025: step 14122, loss 0.570851.
Train: 2018-08-06T01:56:54.484345: step 14123, loss 0.578858.
Train: 2018-08-06T01:56:54.750625: step 14124, loss 0.483346.
Train: 2018-08-06T01:56:55.013890: step 14125, loss 0.666294.
Train: 2018-08-06T01:56:55.286188: step 14126, loss 0.539249.
Train: 2018-08-06T01:56:55.555442: step 14127, loss 0.507732.
Train: 2018-08-06T01:56:55.813781: step 14128, loss 0.563079.
Train: 2018-08-06T01:56:56.071062: step 14129, loss 0.673579.
Train: 2018-08-06T01:56:56.328406: step 14130, loss 0.555261.
Test: 2018-08-06T01:56:57.590998: step 14130, loss 0.548424.
Train: 2018-08-06T01:56:57.840331: step 14131, loss 0.523899.
Train: 2018-08-06T01:56:58.159633: step 14132, loss 0.523952.
Train: 2018-08-06T01:56:58.418933: step 14133, loss 0.571025.
Train: 2018-08-06T01:56:58.682230: step 14134, loss 0.547476.
Train: 2018-08-06T01:56:58.939517: step 14135, loss 0.531743.
Train: 2018-08-06T01:56:59.195856: step 14136, loss 0.500195.
Train: 2018-08-06T01:56:59.452151: step 14137, loss 0.499956.
Train: 2018-08-06T01:56:59.709483: step 14138, loss 0.54716.
Train: 2018-08-06T01:56:59.968788: step 14139, loss 0.54703.
Train: 2018-08-06T01:57:00.226076: step 14140, loss 0.578854.
Test: 2018-08-06T01:57:01.488698: step 14140, loss 0.549795.
Train: 2018-08-06T01:57:01.738063: step 14141, loss 0.554806.
Train: 2018-08-06T01:57:01.990382: step 14142, loss 0.522527.
Train: 2018-08-06T01:57:02.243706: step 14143, loss 0.530385.
Train: 2018-08-06T01:57:02.500027: step 14144, loss 0.603271.
Train: 2018-08-06T01:57:02.755345: step 14145, loss 0.578941.
Train: 2018-08-06T01:57:03.012629: step 14146, loss 0.562617.
Train: 2018-08-06T01:57:03.263951: step 14147, loss 0.570783.
Train: 2018-08-06T01:57:03.511315: step 14148, loss 0.546176.
Train: 2018-08-06T01:57:03.771612: step 14149, loss 0.554344.
Train: 2018-08-06T01:57:04.031928: step 14150, loss 0.595429.
Test: 2018-08-06T01:57:05.304494: step 14150, loss 0.54859.
Train: 2018-08-06T01:57:05.561831: step 14151, loss 0.595452.
Train: 2018-08-06T01:57:05.818120: step 14152, loss 0.653082.
Train: 2018-08-06T01:57:06.067488: step 14153, loss 0.529671.
Train: 2018-08-06T01:57:06.320782: step 14154, loss 0.578969.
Train: 2018-08-06T01:57:06.577122: step 14155, loss 0.562562.
Train: 2018-08-06T01:57:06.830418: step 14156, loss 0.587144.
Train: 2018-08-06T01:57:07.081748: step 14157, loss 0.554414.
Train: 2018-08-06T01:57:07.332100: step 14158, loss 0.513601.
Train: 2018-08-06T01:57:07.595377: step 14159, loss 0.578938.
Train: 2018-08-06T01:57:07.855702: step 14160, loss 0.513638.
Test: 2018-08-06T01:57:09.117301: step 14160, loss 0.549151.
Train: 2018-08-06T01:57:09.363673: step 14161, loss 0.521765.
Train: 2018-08-06T01:57:09.614973: step 14162, loss 0.570767.
Train: 2018-08-06T01:57:09.861345: step 14163, loss 0.578953.
Train: 2018-08-06T01:57:10.118624: step 14164, loss 0.578956.
Train: 2018-08-06T01:57:10.379925: step 14165, loss 0.546182.
Train: 2018-08-06T01:57:10.629257: step 14166, loss 0.554367.
Train: 2018-08-06T01:57:10.883578: step 14167, loss 0.63638.
Train: 2018-08-06T01:57:11.130916: step 14168, loss 0.537988.
Train: 2018-08-06T01:57:11.376261: step 14169, loss 0.570764.
Train: 2018-08-06T01:57:11.622600: step 14170, loss 0.57895.
Test: 2018-08-06T01:57:12.884228: step 14170, loss 0.548555.
Train: 2018-08-06T01:57:13.132564: step 14171, loss 0.513522.
Train: 2018-08-06T01:57:13.392897: step 14172, loss 0.562586.
Train: 2018-08-06T01:57:13.643198: step 14173, loss 0.546224.
Train: 2018-08-06T01:57:13.890567: step 14174, loss 0.497098.
Train: 2018-08-06T01:57:14.136903: step 14175, loss 0.562561.
Train: 2018-08-06T01:57:14.384217: step 14176, loss 0.628256.
Train: 2018-08-06T01:57:14.638537: step 14177, loss 0.529691.
Train: 2018-08-06T01:57:14.897860: step 14178, loss 0.578973.
Train: 2018-08-06T01:57:15.150198: step 14179, loss 0.546115.
Train: 2018-08-06T01:57:15.403496: step 14180, loss 0.694141.
Test: 2018-08-06T01:57:16.652151: step 14180, loss 0.548475.
Train: 2018-08-06T01:57:16.891542: step 14181, loss 0.578966.
Train: 2018-08-06T01:57:17.141871: step 14182, loss 0.538025.
Train: 2018-08-06T01:57:17.392203: step 14183, loss 0.652482.
Train: 2018-08-06T01:57:17.639512: step 14184, loss 0.538205.
Train: 2018-08-06T01:57:17.886874: step 14185, loss 0.570789.
Train: 2018-08-06T01:57:18.148177: step 14186, loss 0.530279.
Train: 2018-08-06T01:57:18.393494: step 14187, loss 0.570802.
Train: 2018-08-06T01:57:18.641860: step 14188, loss 0.514242.
Train: 2018-08-06T01:57:18.890166: step 14189, loss 0.570818.
Train: 2018-08-06T01:57:19.150500: step 14190, loss 0.578904.
Test: 2018-08-06T01:57:20.432042: step 14190, loss 0.549828.
Train: 2018-08-06T01:57:20.683370: step 14191, loss 0.546613.
Train: 2018-08-06T01:57:20.929744: step 14192, loss 0.546618.
Train: 2018-08-06T01:57:21.181070: step 14193, loss 0.538532.
Train: 2018-08-06T01:57:21.370531: step 14194, loss 0.545506.
Train: 2018-08-06T01:57:21.629864: step 14195, loss 0.586969.
Train: 2018-08-06T01:57:21.883193: step 14196, loss 0.578891.
Train: 2018-08-06T01:57:22.131527: step 14197, loss 0.538449.
Train: 2018-08-06T01:57:22.381858: step 14198, loss 0.65174.
Train: 2018-08-06T01:57:22.628194: step 14199, loss 0.554638.
Train: 2018-08-06T01:57:22.877529: step 14200, loss 0.554657.
Test: 2018-08-06T01:57:24.155085: step 14200, loss 0.548889.
Train: 2018-08-06T01:57:25.065168: step 14201, loss 0.538526.
Train: 2018-08-06T01:57:25.316496: step 14202, loss 0.546597.
Train: 2018-08-06T01:57:25.563835: step 14203, loss 0.611187.
Train: 2018-08-06T01:57:25.811172: step 14204, loss 0.554672.
Train: 2018-08-06T01:57:26.056546: step 14205, loss 0.635359.
Train: 2018-08-06T01:57:26.303854: step 14206, loss 0.538611.
Train: 2018-08-06T01:57:26.551219: step 14207, loss 0.643232.
Train: 2018-08-06T01:57:26.812525: step 14208, loss 0.554797.
Train: 2018-08-06T01:57:27.060866: step 14209, loss 0.578864.
Train: 2018-08-06T01:57:27.327146: step 14210, loss 0.554893.
Test: 2018-08-06T01:57:28.600712: step 14210, loss 0.549404.
Train: 2018-08-06T01:57:28.852046: step 14211, loss 0.546956.
Train: 2018-08-06T01:57:29.099379: step 14212, loss 0.539022.
Train: 2018-08-06T01:57:29.349757: step 14213, loss 0.570895.
Train: 2018-08-06T01:57:29.597073: step 14214, loss 0.586821.
Train: 2018-08-06T01:57:29.850371: step 14215, loss 0.531122.
Train: 2018-08-06T01:57:30.097709: step 14216, loss 0.531115.
Train: 2018-08-06T01:57:30.348040: step 14217, loss 0.570894.
Train: 2018-08-06T01:57:30.594381: step 14218, loss 0.570888.
Train: 2018-08-06T01:57:30.842724: step 14219, loss 0.538978.
Train: 2018-08-06T01:57:31.098067: step 14220, loss 0.514972.
Test: 2018-08-06T01:57:32.356668: step 14220, loss 0.548567.
Train: 2018-08-06T01:57:32.599020: step 14221, loss 0.514831.
Train: 2018-08-06T01:57:32.854362: step 14222, loss 0.522665.
Train: 2018-08-06T01:57:33.107685: step 14223, loss 0.554701.
Train: 2018-08-06T01:57:33.353005: step 14224, loss 0.506092.
Train: 2018-08-06T01:57:33.601369: step 14225, loss 0.595162.
Train: 2018-08-06T01:57:33.849676: step 14226, loss 0.595235.
Train: 2018-08-06T01:57:34.097015: step 14227, loss 0.538083.
Train: 2018-08-06T01:57:34.345352: step 14228, loss 0.513418.
Train: 2018-08-06T01:57:34.605653: step 14229, loss 0.480345.
Train: 2018-08-06T01:57:34.853023: step 14230, loss 0.545983.
Test: 2018-08-06T01:57:36.131572: step 14230, loss 0.549375.
Train: 2018-08-06T01:57:36.376916: step 14231, loss 0.479521.
Train: 2018-08-06T01:57:36.626250: step 14232, loss 0.529063.
Train: 2018-08-06T01:57:36.875613: step 14233, loss 0.637878.
Train: 2018-08-06T01:57:37.136883: step 14234, loss 0.553963.
Train: 2018-08-06T01:57:37.388212: step 14235, loss 0.562366.
Train: 2018-08-06T01:57:37.639541: step 14236, loss 0.528508.
Train: 2018-08-06T01:57:37.899874: step 14237, loss 0.536892.
Train: 2018-08-06T01:57:38.161176: step 14238, loss 0.494264.
Train: 2018-08-06T01:57:38.422476: step 14239, loss 0.596501.
Train: 2018-08-06T01:57:38.674801: step 14240, loss 0.545213.
Test: 2018-08-06T01:57:39.955346: step 14240, loss 0.547806.
Train: 2018-08-06T01:57:40.191713: step 14241, loss 0.553754.
Train: 2018-08-06T01:57:40.440074: step 14242, loss 0.553737.
Train: 2018-08-06T01:57:40.687409: step 14243, loss 0.502047.
Train: 2018-08-06T01:57:40.937749: step 14244, loss 0.553705.
Train: 2018-08-06T01:57:41.186082: step 14245, loss 0.622904.
Train: 2018-08-06T01:57:41.433392: step 14246, loss 0.579655.
Train: 2018-08-06T01:57:41.682758: step 14247, loss 0.622926.
Train: 2018-08-06T01:57:41.937071: step 14248, loss 0.56234.
Train: 2018-08-06T01:57:42.197349: step 14249, loss 0.570964.
Train: 2018-08-06T01:57:42.447681: step 14250, loss 0.579557.
Test: 2018-08-06T01:57:43.704320: step 14250, loss 0.548581.
Train: 2018-08-06T01:57:43.940713: step 14251, loss 0.562335.
Train: 2018-08-06T01:57:44.190021: step 14252, loss 0.562335.
Train: 2018-08-06T01:57:44.436361: step 14253, loss 0.613645.
Train: 2018-08-06T01:57:44.695668: step 14254, loss 0.553817.
Train: 2018-08-06T01:57:44.955000: step 14255, loss 0.604837.
Train: 2018-08-06T01:57:45.206334: step 14256, loss 0.545422.
Train: 2018-08-06T01:57:45.454672: step 14257, loss 0.503289.
Train: 2018-08-06T01:57:45.711952: step 14258, loss 0.55395.
Train: 2018-08-06T01:57:45.964275: step 14259, loss 0.511922.
Train: 2018-08-06T01:57:46.218627: step 14260, loss 0.638024.
Test: 2018-08-06T01:57:47.487204: step 14260, loss 0.548583.
Train: 2018-08-06T01:57:47.730552: step 14261, loss 0.520465.
Train: 2018-08-06T01:57:47.976924: step 14262, loss 0.503777.
Train: 2018-08-06T01:57:48.225229: step 14263, loss 0.604272.
Train: 2018-08-06T01:57:48.471601: step 14264, loss 0.579138.
Train: 2018-08-06T01:57:48.720903: step 14265, loss 0.529003.
Train: 2018-08-06T01:57:48.966278: step 14266, loss 0.587463.
Train: 2018-08-06T01:57:49.218605: step 14267, loss 0.595776.
Train: 2018-08-06T01:57:49.474887: step 14268, loss 0.537477.
Train: 2018-08-06T01:57:49.723254: step 14269, loss 0.628932.
Train: 2018-08-06T01:57:49.986564: step 14270, loss 0.545892.
Test: 2018-08-06T01:57:51.273079: step 14270, loss 0.548291.
Train: 2018-08-06T01:57:51.520417: step 14271, loss 0.612113.
Train: 2018-08-06T01:57:51.768787: step 14272, loss 0.570756.
Train: 2018-08-06T01:57:52.031082: step 14273, loss 0.48853.
Train: 2018-08-06T01:57:52.291386: step 14274, loss 0.521478.
Train: 2018-08-06T01:57:52.540689: step 14275, loss 0.603608.
Train: 2018-08-06T01:57:52.788027: step 14276, loss 0.578965.
Train: 2018-08-06T01:57:53.047367: step 14277, loss 0.562571.
Train: 2018-08-06T01:57:53.297692: step 14278, loss 0.644408.
Train: 2018-08-06T01:57:53.547028: step 14279, loss 0.546294.
Train: 2018-08-06T01:57:53.795333: step 14280, loss 0.611486.
Test: 2018-08-06T01:57:55.059951: step 14280, loss 0.548182.
Train: 2018-08-06T01:57:55.298314: step 14281, loss 0.497736.
Train: 2018-08-06T01:57:55.545651: step 14282, loss 0.570793.
Train: 2018-08-06T01:57:55.806954: step 14283, loss 0.595086.
Train: 2018-08-06T01:57:56.058314: step 14284, loss 0.546566.
Train: 2018-08-06T01:57:56.307642: step 14285, loss 0.538533.
Train: 2018-08-06T01:57:56.555949: step 14286, loss 0.603078.
Train: 2018-08-06T01:57:56.816278: step 14287, loss 0.603041.
Train: 2018-08-06T01:57:57.068578: step 14288, loss 0.562797.
Train: 2018-08-06T01:57:57.315917: step 14289, loss 0.490607.
Train: 2018-08-06T01:57:57.568243: step 14290, loss 0.546772.
Test: 2018-08-06T01:57:58.831863: step 14290, loss 0.549776.
Train: 2018-08-06T01:57:59.084189: step 14291, loss 0.562814.
Train: 2018-08-06T01:57:59.330561: step 14292, loss 0.570839.
Train: 2018-08-06T01:57:59.593825: step 14293, loss 0.530663.
Train: 2018-08-06T01:57:59.857122: step 14294, loss 0.562787.
Train: 2018-08-06T01:58:00.106455: step 14295, loss 0.586928.
Train: 2018-08-06T01:58:00.359779: step 14296, loss 0.562767.
Train: 2018-08-06T01:58:00.602160: step 14297, loss 0.530523.
Train: 2018-08-06T01:58:00.850501: step 14298, loss 0.595021.
Train: 2018-08-06T01:58:01.113787: step 14299, loss 0.586957.
Train: 2018-08-06T01:58:01.374095: step 14300, loss 0.546598.
Test: 2018-08-06T01:58:02.640678: step 14300, loss 0.54851.
Train: 2018-08-06T01:58:03.566023: step 14301, loss 0.50622.
Train: 2018-08-06T01:58:03.817350: step 14302, loss 0.611235.
Train: 2018-08-06T01:58:04.077679: step 14303, loss 0.538445.
Train: 2018-08-06T01:58:04.328019: step 14304, loss 0.497929.
Train: 2018-08-06T01:58:04.577344: step 14305, loss 0.562675.
Train: 2018-08-06T01:58:04.828645: step 14306, loss 0.513865.
Train: 2018-08-06T01:58:05.078010: step 14307, loss 0.562619.
Train: 2018-08-06T01:58:05.330305: step 14308, loss 0.546242.
Train: 2018-08-06T01:58:05.578641: step 14309, loss 0.521581.
Train: 2018-08-06T01:58:05.839942: step 14310, loss 0.504973.
Test: 2018-08-06T01:58:07.087605: step 14310, loss 0.548901.
Train: 2018-08-06T01:58:07.337936: step 14311, loss 0.537731.
Train: 2018-08-06T01:58:07.588266: step 14312, loss 0.603917.
Train: 2018-08-06T01:58:07.837598: step 14313, loss 0.612323.
Train: 2018-08-06T01:58:08.083941: step 14314, loss 0.529144.
Train: 2018-08-06T01:58:08.343247: step 14315, loss 0.612458.
Train: 2018-08-06T01:58:08.594605: step 14316, loss 0.604137.
Train: 2018-08-06T01:58:08.845902: step 14317, loss 0.587442.
Train: 2018-08-06T01:58:09.095262: step 14318, loss 0.554105.
Train: 2018-08-06T01:58:09.356537: step 14319, loss 0.570762.
Train: 2018-08-06T01:58:09.605870: step 14320, loss 0.520881.
Test: 2018-08-06T01:58:10.874477: step 14320, loss 0.546846.
Train: 2018-08-06T01:58:11.114835: step 14321, loss 0.587383.
Train: 2018-08-06T01:58:11.371180: step 14322, loss 0.595675.
Train: 2018-08-06T01:58:11.634476: step 14323, loss 0.612223.
Train: 2018-08-06T01:58:11.880819: step 14324, loss 0.545938.
Train: 2018-08-06T01:58:12.134133: step 14325, loss 0.545985.
Train: 2018-08-06T01:58:12.396432: step 14326, loss 0.587247.
Train: 2018-08-06T01:58:12.645772: step 14327, loss 0.521377.
Train: 2018-08-06T01:58:12.897093: step 14328, loss 0.504975.
Train: 2018-08-06T01:58:13.142413: step 14329, loss 0.54608.
Train: 2018-08-06T01:58:13.394762: step 14330, loss 0.595452.
Test: 2018-08-06T01:58:14.665339: step 14330, loss 0.548593.
Train: 2018-08-06T01:58:14.968561: step 14331, loss 0.578988.
Train: 2018-08-06T01:58:15.221882: step 14332, loss 0.636565.
Train: 2018-08-06T01:58:15.478197: step 14333, loss 0.537924.
Train: 2018-08-06T01:58:15.740495: step 14334, loss 0.537971.
Train: 2018-08-06T01:58:15.987827: step 14335, loss 0.55438.
Train: 2018-08-06T01:58:16.245115: step 14336, loss 0.562577.
Train: 2018-08-06T01:58:16.492453: step 14337, loss 0.570766.
Train: 2018-08-06T01:58:16.740820: step 14338, loss 0.562588.
Train: 2018-08-06T01:58:16.990153: step 14339, loss 0.562594.
Train: 2018-08-06T01:58:17.247462: step 14340, loss 0.5626.
Test: 2018-08-06T01:58:18.514046: step 14340, loss 0.549908.
Train: 2018-08-06T01:58:18.767394: step 14341, loss 0.554439.
Train: 2018-08-06T01:58:19.022687: step 14342, loss 0.595261.
Train: 2018-08-06T01:58:19.275011: step 14343, loss 0.562618.
Train: 2018-08-06T01:58:19.539313: step 14344, loss 0.587073.
Train: 2018-08-06T01:58:19.728798: step 14345, loss 0.701533.
Train: 2018-08-06T01:58:19.984139: step 14346, loss 0.627529.
Train: 2018-08-06T01:58:20.235498: step 14347, loss 0.603073.
Train: 2018-08-06T01:58:20.485828: step 14348, loss 0.61897.
Train: 2018-08-06T01:58:20.735130: step 14349, loss 0.57886.
Train: 2018-08-06T01:58:20.982494: step 14350, loss 0.523372.
Test: 2018-08-06T01:58:22.234124: step 14350, loss 0.550436.
Train: 2018-08-06T01:58:22.482482: step 14351, loss 0.484136.
Train: 2018-08-06T01:58:22.728830: step 14352, loss 0.539467.
Train: 2018-08-06T01:58:22.988136: step 14353, loss 0.563122.
Train: 2018-08-06T01:58:23.241428: step 14354, loss 0.555267.
Train: 2018-08-06T01:58:23.489763: step 14355, loss 0.641789.
Train: 2018-08-06T01:58:23.737103: step 14356, loss 0.563171.
Train: 2018-08-06T01:58:23.984471: step 14357, loss 0.492647.
Train: 2018-08-06T01:58:24.230782: step 14358, loss 0.56319.
Train: 2018-08-06T01:58:24.493106: step 14359, loss 0.476857.
Train: 2018-08-06T01:58:24.739446: step 14360, loss 0.586736.
Test: 2018-08-06T01:58:26.020995: step 14360, loss 0.549745.
Train: 2018-08-06T01:58:26.267366: step 14361, loss 0.586749.
Train: 2018-08-06T01:58:26.513677: step 14362, loss 0.507796.
Train: 2018-08-06T01:58:26.764007: step 14363, loss 0.586777.
Train: 2018-08-06T01:58:27.009370: step 14364, loss 0.491586.
Train: 2018-08-06T01:58:27.263696: step 14365, loss 0.547011.
Train: 2018-08-06T01:58:27.515024: step 14366, loss 0.602835.
Train: 2018-08-06T01:58:27.767325: step 14367, loss 0.586876.
Train: 2018-08-06T01:58:28.015685: step 14368, loss 0.554793.
Train: 2018-08-06T01:58:28.264993: step 14369, loss 0.635155.
Train: 2018-08-06T01:58:28.515349: step 14370, loss 0.61104.
Test: 2018-08-06T01:58:29.799889: step 14370, loss 0.549752.
Train: 2018-08-06T01:58:30.040246: step 14371, loss 0.554766.
Train: 2018-08-06T01:58:30.299557: step 14372, loss 0.570839.
Train: 2018-08-06T01:58:30.548914: step 14373, loss 0.610976.
Train: 2018-08-06T01:58:30.799246: step 14374, loss 0.594896.
Train: 2018-08-06T01:58:31.049571: step 14375, loss 0.49087.
Train: 2018-08-06T01:58:31.298880: step 14376, loss 0.626855.
Train: 2018-08-06T01:58:31.558217: step 14377, loss 0.506973.
Train: 2018-08-06T01:58:31.810512: step 14378, loss 0.594838.
Train: 2018-08-06T01:58:32.071843: step 14379, loss 0.562893.
Train: 2018-08-06T01:58:32.317155: step 14380, loss 0.554918.
Test: 2018-08-06T01:58:33.581775: step 14380, loss 0.549572.
Train: 2018-08-06T01:58:33.823159: step 14381, loss 0.538959.
Train: 2018-08-06T01:58:34.075505: step 14382, loss 0.570876.
Train: 2018-08-06T01:58:34.340745: step 14383, loss 0.53892.
Train: 2018-08-06T01:58:34.592072: step 14384, loss 0.554874.
Train: 2018-08-06T01:58:34.841437: step 14385, loss 0.610887.
Train: 2018-08-06T01:58:35.092764: step 14386, loss 0.594877.
Train: 2018-08-06T01:58:35.343095: step 14387, loss 0.474834.
Train: 2018-08-06T01:58:35.597419: step 14388, loss 0.586881.
Train: 2018-08-06T01:58:35.842764: step 14389, loss 0.635022.
Train: 2018-08-06T01:58:36.096056: step 14390, loss 0.554816.
Test: 2018-08-06T01:58:37.377629: step 14390, loss 0.549822.
Train: 2018-08-06T01:58:37.618016: step 14391, loss 0.578866.
Train: 2018-08-06T01:58:37.866352: step 14392, loss 0.586875.
Train: 2018-08-06T01:58:38.116652: step 14393, loss 0.554858.
Train: 2018-08-06T01:58:38.366004: step 14394, loss 0.530879.
Train: 2018-08-06T01:58:38.618341: step 14395, loss 0.546866.
Train: 2018-08-06T01:58:38.883602: step 14396, loss 0.602878.
Train: 2018-08-06T01:58:39.131936: step 14397, loss 0.562857.
Train: 2018-08-06T01:58:39.381289: step 14398, loss 0.530848.
Train: 2018-08-06T01:58:39.632598: step 14399, loss 0.562848.
Train: 2018-08-06T01:58:39.884924: step 14400, loss 0.602909.
Test: 2018-08-06T01:58:41.173477: step 14400, loss 0.548719.
Train: 2018-08-06T01:58:42.093606: step 14401, loss 0.56284.
Train: 2018-08-06T01:58:42.343950: step 14402, loss 0.538801.
Train: 2018-08-06T01:58:42.591290: step 14403, loss 0.538779.
Train: 2018-08-06T01:58:42.839597: step 14404, loss 0.506631.
Train: 2018-08-06T01:58:43.100911: step 14405, loss 0.562785.
Train: 2018-08-06T01:58:43.363227: step 14406, loss 0.57888.
Train: 2018-08-06T01:58:43.610560: step 14407, loss 0.55466.
Train: 2018-08-06T01:58:43.860900: step 14408, loss 0.546537.
Train: 2018-08-06T01:58:44.111195: step 14409, loss 0.595107.
Train: 2018-08-06T01:58:44.363563: step 14410, loss 0.627577.
Test: 2018-08-06T01:58:45.630133: step 14410, loss 0.547835.
Train: 2018-08-06T01:58:45.872513: step 14411, loss 0.481596.
Train: 2018-08-06T01:58:46.122848: step 14412, loss 0.505828.
Train: 2018-08-06T01:58:46.369157: step 14413, loss 0.5545.
Train: 2018-08-06T01:58:46.620516: step 14414, loss 0.562613.
Train: 2018-08-06T01:58:46.870841: step 14415, loss 0.628004.
Train: 2018-08-06T01:58:47.126163: step 14416, loss 0.578947.
Train: 2018-08-06T01:58:47.374499: step 14417, loss 0.521674.
Train: 2018-08-06T01:58:47.638796: step 14418, loss 0.570764.
Train: 2018-08-06T01:58:47.886100: step 14419, loss 0.578958.
Train: 2018-08-06T01:58:48.136431: step 14420, loss 0.521579.
Test: 2018-08-06T01:58:49.408030: step 14420, loss 0.549984.
Train: 2018-08-06T01:58:49.653404: step 14421, loss 0.521526.
Train: 2018-08-06T01:58:49.901734: step 14422, loss 0.578979.
Train: 2018-08-06T01:58:50.163037: step 14423, loss 0.529607.
Train: 2018-08-06T01:58:50.425336: step 14424, loss 0.48007.
Train: 2018-08-06T01:58:50.675672: step 14425, loss 0.504592.
Train: 2018-08-06T01:58:50.924997: step 14426, loss 0.562455.
Train: 2018-08-06T01:58:51.177323: step 14427, loss 0.570764.
Train: 2018-08-06T01:58:51.425659: step 14428, loss 0.579127.
Train: 2018-08-06T01:58:51.675992: step 14429, loss 0.528906.
Train: 2018-08-06T01:58:51.926326: step 14430, loss 0.520416.
Test: 2018-08-06T01:58:53.196897: step 14430, loss 0.548703.
Train: 2018-08-06T01:58:53.443240: step 14431, loss 0.604471.
Train: 2018-08-06T01:58:53.694600: step 14432, loss 0.494908.
Train: 2018-08-06T01:58:53.942903: step 14433, loss 0.553905.
Train: 2018-08-06T01:58:54.194258: step 14434, loss 0.579303.
Train: 2018-08-06T01:58:54.445582: step 14435, loss 0.579326.
Train: 2018-08-06T01:58:54.695919: step 14436, loss 0.545353.
Train: 2018-08-06T01:58:54.944223: step 14437, loss 0.596366.
Train: 2018-08-06T01:58:55.193557: step 14438, loss 0.613379.
Train: 2018-08-06T01:58:55.439933: step 14439, loss 0.621819.
Train: 2018-08-06T01:58:55.686270: step 14440, loss 0.545401.
Test: 2018-08-06T01:58:56.954847: step 14440, loss 0.547452.
Train: 2018-08-06T01:58:57.195244: step 14441, loss 0.494685.
Train: 2018-08-06T01:58:57.445535: step 14442, loss 0.562361.
Train: 2018-08-06T01:58:57.691909: step 14443, loss 0.587706.
Train: 2018-08-06T01:58:57.949188: step 14444, loss 0.612988.
Train: 2018-08-06T01:58:58.263951: step 14445, loss 0.495042.
Train: 2018-08-06T01:58:58.509319: step 14446, loss 0.537154.
Train: 2018-08-06T01:58:58.759649: step 14447, loss 0.579194.
Train: 2018-08-06T01:58:59.007954: step 14448, loss 0.570786.
Train: 2018-08-06T01:58:59.268289: step 14449, loss 0.587559.
Train: 2018-08-06T01:58:59.520583: step 14450, loss 0.587526.
Test: 2018-08-06T01:59:00.791185: step 14450, loss 0.548649.
Train: 2018-08-06T01:59:01.038549: step 14451, loss 0.562414.
Train: 2018-08-06T01:59:01.285862: step 14452, loss 0.604128.
Train: 2018-08-06T01:59:01.537190: step 14453, loss 0.512538.
Train: 2018-08-06T01:59:01.786548: step 14454, loss 0.512627.
Train: 2018-08-06T01:59:02.037884: step 14455, loss 0.562458.
Train: 2018-08-06T01:59:02.287218: step 14456, loss 0.471201.
Train: 2018-08-06T01:59:02.546522: step 14457, loss 0.628911.
Train: 2018-08-06T01:59:02.797819: step 14458, loss 0.612286.
Train: 2018-08-06T01:59:03.060158: step 14459, loss 0.595641.
Train: 2018-08-06T01:59:03.309484: step 14460, loss 0.537644.
Test: 2018-08-06T01:59:04.561103: step 14460, loss 0.54773.
Train: 2018-08-06T01:59:04.801494: step 14461, loss 0.537687.
Train: 2018-08-06T01:59:05.051830: step 14462, loss 0.488144.
Train: 2018-08-06T01:59:05.307108: step 14463, loss 0.537685.
Train: 2018-08-06T01:59:05.559433: step 14464, loss 0.512815.
Train: 2018-08-06T01:59:05.816744: step 14465, loss 0.529288.
Train: 2018-08-06T01:59:06.073085: step 14466, loss 0.504251.
Train: 2018-08-06T01:59:06.320423: step 14467, loss 0.579107.
Train: 2018-08-06T01:59:06.568734: step 14468, loss 0.570772.
Train: 2018-08-06T01:59:06.818067: step 14469, loss 0.679682.
Train: 2018-08-06T01:59:07.068398: step 14470, loss 0.537291.
Test: 2018-08-06T01:59:08.341991: step 14470, loss 0.547659.
Train: 2018-08-06T01:59:08.583371: step 14471, loss 0.52893.
Train: 2018-08-06T01:59:08.827692: step 14472, loss 0.579147.
Train: 2018-08-06T01:59:09.078057: step 14473, loss 0.612624.
Train: 2018-08-06T01:59:09.340322: step 14474, loss 0.570771.
Train: 2018-08-06T01:59:09.591674: step 14475, loss 0.612503.
Train: 2018-08-06T01:59:09.842011: step 14476, loss 0.554109.
Train: 2018-08-06T01:59:10.090341: step 14477, loss 0.595688.
Train: 2018-08-06T01:59:10.348650: step 14478, loss 0.628768.
Train: 2018-08-06T01:59:10.602969: step 14479, loss 0.579012.
Train: 2018-08-06T01:59:10.850315: step 14480, loss 0.578982.
Test: 2018-08-06T01:59:12.102933: step 14480, loss 0.549644.
Train: 2018-08-06T01:59:12.353263: step 14481, loss 0.537995.
Train: 2018-08-06T01:59:12.600638: step 14482, loss 0.538099.
Train: 2018-08-06T01:59:12.852958: step 14483, loss 0.587075.
Train: 2018-08-06T01:59:13.100297: step 14484, loss 0.554525.
Train: 2018-08-06T01:59:13.348632: step 14485, loss 0.570791.
Train: 2018-08-06T01:59:13.606911: step 14486, loss 0.48174.
Train: 2018-08-06T01:59:13.854251: step 14487, loss 0.514128.
Train: 2018-08-06T01:59:14.100617: step 14488, loss 0.514066.
Train: 2018-08-06T01:59:14.349937: step 14489, loss 0.587028.
Train: 2018-08-06T01:59:14.600282: step 14490, loss 0.603305.
Test: 2018-08-06T01:59:15.883822: step 14490, loss 0.549627.
Train: 2018-08-06T01:59:16.127203: step 14491, loss 0.587048.
Train: 2018-08-06T01:59:16.375507: step 14492, loss 0.570783.
Train: 2018-08-06T01:59:16.639828: step 14493, loss 0.562655.
Train: 2018-08-06T01:59:16.894121: step 14494, loss 0.603288.
Train: 2018-08-06T01:59:17.154449: step 14495, loss 0.562672.
Train: 2018-08-06T01:59:17.346934: step 14496, loss 0.562684.
Train: 2018-08-06T01:59:17.595265: step 14497, loss 0.562695.
Train: 2018-08-06T01:59:17.854586: step 14498, loss 0.554609.
Train: 2018-08-06T01:59:18.109869: step 14499, loss 0.514161.
Train: 2018-08-06T01:59:18.359203: step 14500, loss 0.530315.
Test: 2018-08-06T01:59:19.628806: step 14500, loss 0.548026.
Train: 2018-08-06T01:59:20.550522: step 14501, loss 0.554578.
Train: 2018-08-06T01:59:20.794838: step 14502, loss 0.570788.
Train: 2018-08-06T01:59:21.051153: step 14503, loss 0.611421.
Train: 2018-08-06T01:59:21.296528: step 14504, loss 0.562657.
Train: 2018-08-06T01:59:21.544868: step 14505, loss 0.595165.
Train: 2018-08-06T01:59:21.794172: step 14506, loss 0.546423.
Train: 2018-08-06T01:59:22.049516: step 14507, loss 0.587026.
Train: 2018-08-06T01:59:22.294883: step 14508, loss 0.505886.
Train: 2018-08-06T01:59:22.543198: step 14509, loss 0.538318.
Train: 2018-08-06T01:59:22.790505: step 14510, loss 0.578911.
Test: 2018-08-06T01:59:24.065097: step 14510, loss 0.549633.
Train: 2018-08-06T01:59:24.312471: step 14511, loss 0.538257.
Train: 2018-08-06T01:59:24.560777: step 14512, loss 0.481237.
Train: 2018-08-06T01:59:24.813103: step 14513, loss 0.587096.
Train: 2018-08-06T01:59:25.057449: step 14514, loss 0.497166.
Train: 2018-08-06T01:59:25.306815: step 14515, loss 0.587168.
Train: 2018-08-06T01:59:25.555149: step 14516, loss 0.504989.
Train: 2018-08-06T01:59:25.813428: step 14517, loss 0.611992.
Train: 2018-08-06T01:59:26.059768: step 14518, loss 0.554233.
Train: 2018-08-06T01:59:26.308104: step 14519, loss 0.603859.
Train: 2018-08-06T01:59:26.558465: step 14520, loss 0.570757.
Test: 2018-08-06T01:59:27.839010: step 14520, loss 0.548832.
Train: 2018-08-06T01:59:28.076403: step 14521, loss 0.545908.
Train: 2018-08-06T01:59:28.325714: step 14522, loss 0.637056.
Train: 2018-08-06T01:59:28.579067: step 14523, loss 0.554201.
Train: 2018-08-06T01:59:28.826401: step 14524, loss 0.579027.
Train: 2018-08-06T01:59:29.074711: step 14525, loss 0.645099.
Train: 2018-08-06T01:59:29.318061: step 14526, loss 0.521337.
Train: 2018-08-06T01:59:29.565399: step 14527, loss 0.53787.
Train: 2018-08-06T01:59:29.826700: step 14528, loss 0.578973.
Train: 2018-08-06T01:59:30.072043: step 14529, loss 0.578963.
Train: 2018-08-06T01:59:30.322374: step 14530, loss 0.538012.
Test: 2018-08-06T01:59:31.595968: step 14530, loss 0.54949.
Train: 2018-08-06T01:59:31.834331: step 14531, loss 0.595307.
Train: 2018-08-06T01:59:32.086656: step 14532, loss 0.554436.
Train: 2018-08-06T01:59:32.335026: step 14533, loss 0.636029.
Train: 2018-08-06T01:59:32.588347: step 14534, loss 0.521971.
Train: 2018-08-06T01:59:32.846623: step 14535, loss 0.554541.
Train: 2018-08-06T01:59:33.096954: step 14536, loss 0.603241.
Train: 2018-08-06T01:59:33.355294: step 14537, loss 0.570798.
Train: 2018-08-06T01:59:33.609617: step 14538, loss 0.514234.
Train: 2018-08-06T01:59:33.852932: step 14539, loss 0.59504.
Train: 2018-08-06T01:59:34.102299: step 14540, loss 0.58695.
Test: 2018-08-06T01:59:35.356911: step 14540, loss 0.549131.
Train: 2018-08-06T01:59:35.596296: step 14541, loss 0.619151.
Train: 2018-08-06T01:59:35.841639: step 14542, loss 0.611007.
Train: 2018-08-06T01:59:36.096960: step 14543, loss 0.498789.
Train: 2018-08-06T01:59:36.344270: step 14544, loss 0.578863.
Train: 2018-08-06T01:59:36.599616: step 14545, loss 0.507014.
Train: 2018-08-06T01:59:36.842936: step 14546, loss 0.610789.
Train: 2018-08-06T01:59:37.095293: step 14547, loss 0.626702.
Train: 2018-08-06T01:59:37.358560: step 14548, loss 0.618638.
Train: 2018-08-06T01:59:37.605896: step 14549, loss 0.586789.
Train: 2018-08-06T01:59:37.852237: step 14550, loss 0.570956.
Test: 2018-08-06T01:59:39.118849: step 14550, loss 0.550301.
Train: 2018-08-06T01:59:39.367185: step 14551, loss 0.60251.
Train: 2018-08-06T01:59:39.612530: step 14552, loss 0.563161.
Train: 2018-08-06T01:59:39.857875: step 14553, loss 0.61804.
Train: 2018-08-06T01:59:40.105243: step 14554, loss 0.500855.
Train: 2018-08-06T01:59:40.352551: step 14555, loss 0.508781.
Train: 2018-08-06T01:59:40.600886: step 14556, loss 0.532154.
Train: 2018-08-06T01:59:40.848261: step 14557, loss 0.563298.
Train: 2018-08-06T01:59:41.098555: step 14558, loss 0.539861.
Train: 2018-08-06T01:59:41.351912: step 14559, loss 0.516332.
Train: 2018-08-06T01:59:41.599247: step 14560, loss 0.586717.
Test: 2018-08-06T01:59:42.889771: step 14560, loss 0.550032.
Train: 2018-08-06T01:59:43.125141: step 14561, loss 0.555297.
Train: 2018-08-06T01:59:43.371509: step 14562, loss 0.531612.
Train: 2018-08-06T01:59:43.622811: step 14563, loss 0.570963.
Train: 2018-08-06T01:59:43.877131: step 14564, loss 0.531343.
Train: 2018-08-06T01:59:44.124469: step 14565, loss 0.483521.
Train: 2018-08-06T01:59:44.372805: step 14566, loss 0.578861.
Train: 2018-08-06T01:59:44.626128: step 14567, loss 0.634967.
Train: 2018-08-06T01:59:44.874463: step 14568, loss 0.554777.
Train: 2018-08-06T01:59:45.121803: step 14569, loss 0.450103.
Train: 2018-08-06T01:59:45.382107: step 14570, loss 0.643558.
Test: 2018-08-06T01:59:46.650713: step 14570, loss 0.550096.
Train: 2018-08-06T01:59:46.889108: step 14571, loss 0.49788.
Train: 2018-08-06T01:59:47.146387: step 14572, loss 0.603301.
Train: 2018-08-06T01:59:47.393727: step 14573, loss 0.595219.
Train: 2018-08-06T01:59:47.646050: step 14574, loss 0.562614.
Train: 2018-08-06T01:59:47.899375: step 14575, loss 0.52993.
Train: 2018-08-06T01:59:48.158712: step 14576, loss 0.554401.
Train: 2018-08-06T01:59:48.403051: step 14577, loss 0.636333.
Train: 2018-08-06T01:59:48.654355: step 14578, loss 0.537978.
Train: 2018-08-06T01:59:48.909671: step 14579, loss 0.578962.
Train: 2018-08-06T01:59:49.155049: step 14580, loss 0.521559.
Test: 2018-08-06T01:59:50.416641: step 14580, loss 0.548279.
Train: 2018-08-06T01:59:50.654037: step 14581, loss 0.513306.
Train: 2018-08-06T01:59:50.905359: step 14582, loss 0.578981.
Train: 2018-08-06T01:59:51.153670: step 14583, loss 0.537828.
Train: 2018-08-06T01:59:51.396022: step 14584, loss 0.471812.
Train: 2018-08-06T01:59:51.639372: step 14585, loss 0.653474.
Train: 2018-08-06T01:59:51.885744: step 14586, loss 0.579036.
Train: 2018-08-06T01:59:52.132081: step 14587, loss 0.570755.
Train: 2018-08-06T01:59:52.377398: step 14588, loss 0.496189.
Train: 2018-08-06T01:59:52.625760: step 14589, loss 0.603947.
Train: 2018-08-06T01:59:52.874101: step 14590, loss 0.587366.
Test: 2018-08-06T01:59:54.148660: step 14590, loss 0.548219.
Train: 2018-08-06T01:59:54.385060: step 14591, loss 0.49608.
Train: 2018-08-06T01:59:54.637387: step 14592, loss 0.579066.
Train: 2018-08-06T01:59:54.884723: step 14593, loss 0.570764.
Train: 2018-08-06T01:59:55.128042: step 14594, loss 0.529198.
Train: 2018-08-06T01:59:55.375405: step 14595, loss 0.678901.
Train: 2018-08-06T01:59:55.627731: step 14596, loss 0.50433.
Train: 2018-08-06T01:59:55.889007: step 14597, loss 0.653756.
Train: 2018-08-06T01:59:56.134350: step 14598, loss 0.595597.
Train: 2018-08-06T01:59:56.384680: step 14599, loss 0.5625.
Train: 2018-08-06T01:59:56.637007: step 14600, loss 0.521348.
Test: 2018-08-06T01:59:57.910600: step 14600, loss 0.548806.
Train: 2018-08-06T01:59:58.841971: step 14601, loss 0.52965.
Train: 2018-08-06T01:59:59.090333: step 14602, loss 0.546114.
Train: 2018-08-06T01:59:59.337645: step 14603, loss 0.578971.
Train: 2018-08-06T01:59:59.586009: step 14604, loss 0.521525.
Train: 2018-08-06T01:59:59.829331: step 14605, loss 0.578967.
Train: 2018-08-06T02:00:00.073677: step 14606, loss 0.554352.
Train: 2018-08-06T02:00:00.333016: step 14607, loss 0.546146.
Train: 2018-08-06T02:00:00.591292: step 14608, loss 0.513313.
Train: 2018-08-06T02:00:00.838630: step 14609, loss 0.595409.
Train: 2018-08-06T02:00:01.083975: step 14610, loss 0.529663.
Test: 2018-08-06T02:00:02.366544: step 14610, loss 0.547653.
Train: 2018-08-06T02:00:02.615878: step 14611, loss 0.570759.
Train: 2018-08-06T02:00:02.864252: step 14612, loss 0.578991.
Train: 2018-08-06T02:00:03.112577: step 14613, loss 0.570757.
Train: 2018-08-06T02:00:03.359887: step 14614, loss 0.636635.
Train: 2018-08-06T02:00:03.615233: step 14615, loss 0.521428.
Train: 2018-08-06T02:00:03.864539: step 14616, loss 0.57076.
Train: 2018-08-06T02:00:04.111902: step 14617, loss 0.513287.
Train: 2018-08-06T02:00:04.359216: step 14618, loss 0.51327.
Train: 2018-08-06T02:00:04.604560: step 14619, loss 0.52142.
Train: 2018-08-06T02:00:04.858887: step 14620, loss 0.488368.
Test: 2018-08-06T02:00:06.161396: step 14620, loss 0.547736.
Train: 2018-08-06T02:00:06.403747: step 14621, loss 0.554226.
Train: 2018-08-06T02:00:06.660062: step 14622, loss 0.554178.
Train: 2018-08-06T02:00:06.906434: step 14623, loss 0.545822.
Train: 2018-08-06T02:00:07.151772: step 14624, loss 0.537423.
Train: 2018-08-06T02:00:07.400111: step 14625, loss 0.545694.
Train: 2018-08-06T02:00:07.650413: step 14626, loss 0.67137.
Train: 2018-08-06T02:00:07.894760: step 14627, loss 0.537243.
Train: 2018-08-06T02:00:08.147110: step 14628, loss 0.58756.
Train: 2018-08-06T02:00:08.402403: step 14629, loss 0.562394.
Train: 2018-08-06T02:00:08.661740: step 14630, loss 0.604325.
Test: 2018-08-06T02:00:09.938295: step 14630, loss 0.548026.
Train: 2018-08-06T02:00:10.176690: step 14631, loss 0.545648.
Train: 2018-08-06T02:00:10.424994: step 14632, loss 0.528929.
Train: 2018-08-06T02:00:10.674326: step 14633, loss 0.520569.
Train: 2018-08-06T02:00:10.938620: step 14634, loss 0.54566.
Train: 2018-08-06T02:00:11.196929: step 14635, loss 0.52889.
Train: 2018-08-06T02:00:11.455238: step 14636, loss 0.562394.
Train: 2018-08-06T02:00:11.707594: step 14637, loss 0.579179.
Train: 2018-08-06T02:00:11.968907: step 14638, loss 0.495204.
Train: 2018-08-06T02:00:12.218198: step 14639, loss 0.528738.
Train: 2018-08-06T02:00:12.475540: step 14640, loss 0.494961.
Test: 2018-08-06T02:00:13.749103: step 14640, loss 0.548638.
Train: 2018-08-06T02:00:13.996442: step 14641, loss 0.604615.
Train: 2018-08-06T02:00:14.244779: step 14642, loss 0.48618.
Train: 2018-08-06T02:00:14.491153: step 14643, loss 0.638733.
Train: 2018-08-06T02:00:14.741450: step 14644, loss 0.621808.
Train: 2018-08-06T02:00:15.002782: step 14645, loss 0.664215.
Train: 2018-08-06T02:00:15.264085: step 14646, loss 0.587754.
Train: 2018-08-06T02:00:15.452573: step 14647, loss 0.652392.
Train: 2018-08-06T02:00:15.698890: step 14648, loss 0.604389.
Train: 2018-08-06T02:00:15.947259: step 14649, loss 0.579128.
Train: 2018-08-06T02:00:16.207534: step 14650, loss 0.504244.
Test: 2018-08-06T02:00:17.481129: step 14650, loss 0.548063.
Train: 2018-08-06T02:00:17.823646: step 14651, loss 0.545902.
Train: 2018-08-06T02:00:18.070989: step 14652, loss 0.603797.
Train: 2018-08-06T02:00:18.325293: step 14653, loss 0.537833.
Train: 2018-08-06T02:00:18.583597: step 14654, loss 0.611808.
Train: 2018-08-06T02:00:18.829931: step 14655, loss 0.595305.
Train: 2018-08-06T02:00:19.076274: step 14656, loss 0.59522.
Train: 2018-08-06T02:00:19.320652: step 14657, loss 0.522107.
Train: 2018-08-06T02:00:19.563968: step 14658, loss 0.603162.
Train: 2018-08-06T02:00:19.812334: step 14659, loss 0.498257.
Train: 2018-08-06T02:00:20.058676: step 14660, loss 0.554729.
Test: 2018-08-06T02:00:21.323263: step 14660, loss 0.549001.
Train: 2018-08-06T02:00:21.562649: step 14661, loss 0.578873.
Train: 2018-08-06T02:00:21.812955: step 14662, loss 0.546761.
Train: 2018-08-06T02:00:22.073282: step 14663, loss 0.546788.
Train: 2018-08-06T02:00:22.334583: step 14664, loss 0.667053.
Train: 2018-08-06T02:00:22.578934: step 14665, loss 0.562868.
Train: 2018-08-06T02:00:22.831229: step 14666, loss 0.562901.
Train: 2018-08-06T02:00:23.078569: step 14667, loss 0.523103.
Train: 2018-08-06T02:00:23.324910: step 14668, loss 0.54702.
Train: 2018-08-06T02:00:23.569257: step 14669, loss 0.602735.
Train: 2018-08-06T02:00:23.814600: step 14670, loss 0.562955.
Test: 2018-08-06T02:00:25.064258: step 14670, loss 0.54805.
Train: 2018-08-06T02:00:25.303643: step 14671, loss 0.475559.
Train: 2018-08-06T02:00:25.548961: step 14672, loss 0.626603.
Train: 2018-08-06T02:00:25.800291: step 14673, loss 0.586812.
Train: 2018-08-06T02:00:26.046631: step 14674, loss 0.562941.
Train: 2018-08-06T02:00:26.298955: step 14675, loss 0.459524.
Train: 2018-08-06T02:00:26.543302: step 14676, loss 0.51511.
Train: 2018-08-06T02:00:26.798650: step 14677, loss 0.618974.
Train: 2018-08-06T02:00:27.044992: step 14678, loss 0.594836.
Train: 2018-08-06T02:00:27.293328: step 14679, loss 0.506653.
Train: 2018-08-06T02:00:27.554599: step 14680, loss 0.554744.
Test: 2018-08-06T02:00:28.838165: step 14680, loss 0.54927.
Train: 2018-08-06T02:00:29.075530: step 14681, loss 0.546666.
Train: 2018-08-06T02:00:29.322868: step 14682, loss 0.603439.
Train: 2018-08-06T02:00:29.572204: step 14683, loss 0.611265.
Train: 2018-08-06T02:00:29.821535: step 14684, loss 0.603109.
Train: 2018-08-06T02:00:30.065884: step 14685, loss 0.611158.
Train: 2018-08-06T02:00:30.314244: step 14686, loss 0.546661.
Train: 2018-08-06T02:00:30.559561: step 14687, loss 0.514504.
Train: 2018-08-06T02:00:30.807931: step 14688, loss 0.60302.
Train: 2018-08-06T02:00:31.057262: step 14689, loss 0.594952.
Train: 2018-08-06T02:00:31.305597: step 14690, loss 0.651228.
Test: 2018-08-06T02:00:32.594121: step 14690, loss 0.548905.
Train: 2018-08-06T02:00:32.837470: step 14691, loss 0.562859.
Train: 2018-08-06T02:00:33.085807: step 14692, loss 0.658809.
Train: 2018-08-06T02:00:33.330179: step 14693, loss 0.531129.
Train: 2018-08-06T02:00:33.578488: step 14694, loss 0.56302.
Train: 2018-08-06T02:00:33.825859: step 14695, loss 0.531385.
Train: 2018-08-06T02:00:34.073192: step 14696, loss 0.555177.
Train: 2018-08-06T02:00:34.317513: step 14697, loss 0.586746.
Train: 2018-08-06T02:00:34.565849: step 14698, loss 0.507963.
Train: 2018-08-06T02:00:34.812188: step 14699, loss 0.578861.
Train: 2018-08-06T02:00:35.074488: step 14700, loss 0.610379.
Test: 2018-08-06T02:00:36.331126: step 14700, loss 0.550524.
Train: 2018-08-06T02:00:37.262853: step 14701, loss 0.641832.
Train: 2018-08-06T02:00:37.515178: step 14702, loss 0.531762.
Train: 2018-08-06T02:00:37.767503: step 14703, loss 0.594559.
Train: 2018-08-06T02:00:38.025856: step 14704, loss 0.516253.
Train: 2018-08-06T02:00:38.273150: step 14705, loss 0.578881.
Train: 2018-08-06T02:00:38.521486: step 14706, loss 0.531943.
Train: 2018-08-06T02:00:38.770820: step 14707, loss 0.547569.
Train: 2018-08-06T02:00:39.024141: step 14708, loss 0.516189.
Train: 2018-08-06T02:00:39.271506: step 14709, loss 0.476761.
Train: 2018-08-06T02:00:39.527828: step 14710, loss 0.547346.
Test: 2018-08-06T02:00:40.804381: step 14710, loss 0.549416.
Train: 2018-08-06T02:00:41.054711: step 14711, loss 0.578852.
Train: 2018-08-06T02:00:41.300073: step 14712, loss 0.594812.
Train: 2018-08-06T02:00:41.550411: step 14713, loss 0.539058.
Train: 2018-08-06T02:00:41.803709: step 14714, loss 0.602872.
Train: 2018-08-06T02:00:42.050081: step 14715, loss 0.570848.
Train: 2018-08-06T02:00:42.296391: step 14716, loss 0.554804.
Train: 2018-08-06T02:00:42.541760: step 14717, loss 0.57084.
Train: 2018-08-06T02:00:42.787104: step 14718, loss 0.562777.
Train: 2018-08-06T02:00:43.032453: step 14719, loss 0.570829.
Train: 2018-08-06T02:00:43.291755: step 14720, loss 0.570824.
Test: 2018-08-06T02:00:44.573301: step 14720, loss 0.549626.
Train: 2018-08-06T02:00:44.814688: step 14721, loss 0.506225.
Train: 2018-08-06T02:00:45.063989: step 14722, loss 0.562723.
Train: 2018-08-06T02:00:45.318309: step 14723, loss 0.562679.
Train: 2018-08-06T02:00:45.567668: step 14724, loss 0.578951.
Train: 2018-08-06T02:00:45.815012: step 14725, loss 0.570963.
Train: 2018-08-06T02:00:46.074287: step 14726, loss 0.603239.
Train: 2018-08-06T02:00:46.320662: step 14727, loss 0.562557.
Train: 2018-08-06T02:00:46.568966: step 14728, loss 0.497442.
Train: 2018-08-06T02:00:46.819317: step 14729, loss 0.554393.
Train: 2018-08-06T02:00:47.079599: step 14730, loss 0.505481.
Test: 2018-08-06T02:00:48.336239: step 14730, loss 0.547803.
Train: 2018-08-06T02:00:48.576627: step 14731, loss 0.612129.
Train: 2018-08-06T02:00:48.826929: step 14732, loss 0.480135.
Train: 2018-08-06T02:00:49.071273: step 14733, loss 0.637983.
Train: 2018-08-06T02:00:49.331577: step 14734, loss 0.521518.
Train: 2018-08-06T02:00:49.577943: step 14735, loss 0.570831.
Train: 2018-08-06T02:00:49.823286: step 14736, loss 0.579033.
Train: 2018-08-06T02:00:50.080574: step 14737, loss 0.52941.
Train: 2018-08-06T02:00:50.325958: step 14738, loss 0.529579.
Train: 2018-08-06T02:00:50.573286: step 14739, loss 0.620303.
Train: 2018-08-06T02:00:50.818600: step 14740, loss 0.496445.
Test: 2018-08-06T02:00:52.086210: step 14740, loss 0.549443.
Train: 2018-08-06T02:00:52.338534: step 14741, loss 0.669955.
Train: 2018-08-06T02:00:52.587868: step 14742, loss 0.579016.
Train: 2018-08-06T02:00:52.843215: step 14743, loss 0.488261.
Train: 2018-08-06T02:00:53.089558: step 14744, loss 0.587258.
Train: 2018-08-06T02:00:53.336901: step 14745, loss 0.545994.
Train: 2018-08-06T02:00:53.598190: step 14746, loss 0.612024.
Train: 2018-08-06T02:00:53.860466: step 14747, loss 0.57075.
Train: 2018-08-06T02:00:54.119772: step 14748, loss 0.636644.
Train: 2018-08-06T02:00:54.366144: step 14749, loss 0.546115.
Train: 2018-08-06T02:00:54.616466: step 14750, loss 0.537913.
Test: 2018-08-06T02:00:55.911978: step 14750, loss 0.548695.
Train: 2018-08-06T02:00:56.152337: step 14751, loss 0.537984.
Train: 2018-08-06T02:00:56.399675: step 14752, loss 0.595367.
Train: 2018-08-06T02:00:56.650030: step 14753, loss 0.521617.
Train: 2018-08-06T02:00:56.901358: step 14754, loss 0.619918.
Train: 2018-08-06T02:00:57.158675: step 14755, loss 0.562584.
Train: 2018-08-06T02:00:57.420986: step 14756, loss 0.513607.
Train: 2018-08-06T02:00:57.669310: step 14757, loss 0.61972.
Train: 2018-08-06T02:00:57.914623: step 14758, loss 0.521839.
Train: 2018-08-06T02:00:58.162989: step 14759, loss 0.554434.
Train: 2018-08-06T02:00:58.457637: step 14760, loss 0.660531.
Test: 2018-08-06T02:00:59.728215: step 14760, loss 0.549242.
Train: 2018-08-06T02:00:59.968603: step 14761, loss 0.603528.
Train: 2018-08-06T02:01:00.215914: step 14762, loss 0.530297.
Train: 2018-08-06T02:01:00.464245: step 14763, loss 0.538509.
Train: 2018-08-06T02:01:00.711617: step 14764, loss 0.554694.
Train: 2018-08-06T02:01:00.959945: step 14765, loss 0.546662.
Train: 2018-08-06T02:01:01.202303: step 14766, loss 0.603039.
Train: 2018-08-06T02:01:01.463606: step 14767, loss 0.570833.
Train: 2018-08-06T02:01:01.710913: step 14768, loss 0.578874.
Train: 2018-08-06T02:01:01.956255: step 14769, loss 0.546772.
Train: 2018-08-06T02:01:02.205589: step 14770, loss 0.546788.
Test: 2018-08-06T02:01:03.472201: step 14770, loss 0.549441.
Train: 2018-08-06T02:01:03.711594: step 14771, loss 0.570846.
Train: 2018-08-06T02:01:03.956906: step 14772, loss 0.538793.
Train: 2018-08-06T02:01:04.211253: step 14773, loss 0.578876.
Train: 2018-08-06T02:01:04.457566: step 14774, loss 0.586874.
Train: 2018-08-06T02:01:04.704904: step 14775, loss 0.538792.
Train: 2018-08-06T02:01:04.951271: step 14776, loss 0.530756.
Train: 2018-08-06T02:01:05.202575: step 14777, loss 0.546763.
Train: 2018-08-06T02:01:05.449937: step 14778, loss 0.562788.
Train: 2018-08-06T02:01:05.707226: step 14779, loss 0.562799.
Train: 2018-08-06T02:01:05.950604: step 14780, loss 0.554695.
Test: 2018-08-06T02:01:07.208211: step 14780, loss 0.549645.
Train: 2018-08-06T02:01:07.461566: step 14781, loss 0.522413.
Train: 2018-08-06T02:01:07.721882: step 14782, loss 0.546574.
Train: 2018-08-06T02:01:07.972192: step 14783, loss 0.522191.
Train: 2018-08-06T02:01:08.222524: step 14784, loss 0.554563.
Train: 2018-08-06T02:01:08.472859: step 14785, loss 0.6278.
Train: 2018-08-06T02:01:08.719170: step 14786, loss 0.578936.
Train: 2018-08-06T02:01:08.966537: step 14787, loss 0.464654.
Train: 2018-08-06T02:01:09.213859: step 14788, loss 0.529862.
Train: 2018-08-06T02:01:09.459192: step 14789, loss 0.480432.
Train: 2018-08-06T02:01:09.710562: step 14790, loss 0.570772.
Test: 2018-08-06T02:01:10.982117: step 14790, loss 0.54808.
Train: 2018-08-06T02:01:11.223472: step 14791, loss 0.512864.
Train: 2018-08-06T02:01:11.475829: step 14792, loss 0.587433.
Train: 2018-08-06T02:01:11.732143: step 14793, loss 0.520678.
Train: 2018-08-06T02:01:11.978502: step 14794, loss 0.503781.
Train: 2018-08-06T02:01:12.224827: step 14795, loss 0.562346.
Train: 2018-08-06T02:01:12.478151: step 14796, loss 0.579264.
Train: 2018-08-06T02:01:12.728478: step 14797, loss 0.536706.
Train: 2018-08-06T02:01:12.915970: step 14798, loss 0.507456.
Train: 2018-08-06T02:01:13.162286: step 14799, loss 0.510909.
Train: 2018-08-06T02:01:13.411620: step 14800, loss 0.536157.
Test: 2018-08-06T02:01:14.687210: step 14800, loss 0.548833.
Train: 2018-08-06T02:01:15.592607: step 14801, loss 0.528045.
Train: 2018-08-06T02:01:15.842911: step 14802, loss 0.607655.
Train: 2018-08-06T02:01:16.090249: step 14803, loss 0.572419.
Train: 2018-08-06T02:01:16.345597: step 14804, loss 0.580542.
Train: 2018-08-06T02:01:16.592935: step 14805, loss 0.614696.
Train: 2018-08-06T02:01:16.836284: step 14806, loss 0.553621.
Train: 2018-08-06T02:01:17.085588: step 14807, loss 0.553748.
Train: 2018-08-06T02:01:17.333953: step 14808, loss 0.587958.
Train: 2018-08-06T02:01:17.582284: step 14809, loss 0.536406.
Train: 2018-08-06T02:01:17.843593: step 14810, loss 0.570921.
Test: 2018-08-06T02:01:19.126130: step 14810, loss 0.547988.
Train: 2018-08-06T02:01:19.364492: step 14811, loss 0.545176.
Train: 2018-08-06T02:01:19.613825: step 14812, loss 0.639534.
Train: 2018-08-06T02:01:19.859195: step 14813, loss 0.605097.
Train: 2018-08-06T02:01:20.108534: step 14814, loss 0.562341.
Train: 2018-08-06T02:01:20.353871: step 14815, loss 0.613313.
Train: 2018-08-06T02:01:20.608197: step 14816, loss 0.536977.
Train: 2018-08-06T02:01:20.857557: step 14817, loss 0.553942.
Train: 2018-08-06T02:01:21.104894: step 14818, loss 0.503537.
Train: 2018-08-06T02:01:21.353230: step 14819, loss 0.62115.
Train: 2018-08-06T02:01:21.606580: step 14820, loss 0.646093.
Test: 2018-08-06T02:01:22.891118: step 14820, loss 0.548901.
Train: 2018-08-06T02:01:23.133470: step 14821, loss 0.537437.
Train: 2018-08-06T02:01:23.379836: step 14822, loss 0.570758.
Train: 2018-08-06T02:01:23.630142: step 14823, loss 0.57077.
Train: 2018-08-06T02:01:23.877482: step 14824, loss 0.595513.
Train: 2018-08-06T02:01:24.127842: step 14825, loss 0.55435.
Train: 2018-08-06T02:01:24.385151: step 14826, loss 0.578919.
Train: 2018-08-06T02:01:24.636475: step 14827, loss 0.595258.
Train: 2018-08-06T02:01:24.887778: step 14828, loss 0.603313.
Train: 2018-08-06T02:01:25.138146: step 14829, loss 0.530291.
Train: 2018-08-06T02:01:25.398454: step 14830, loss 0.595032.
Test: 2018-08-06T02:01:26.683985: step 14830, loss 0.549709.
Train: 2018-08-06T02:01:26.928332: step 14831, loss 0.570838.
Train: 2018-08-06T02:01:27.176669: step 14832, loss 0.586882.
Train: 2018-08-06T02:01:27.422018: step 14833, loss 0.506882.
Train: 2018-08-06T02:01:27.676332: step 14834, loss 0.570887.
Train: 2018-08-06T02:01:27.924667: step 14835, loss 0.507139.
Train: 2018-08-06T02:01:28.170043: step 14836, loss 0.610752.
Train: 2018-08-06T02:01:28.417356: step 14837, loss 0.46741.
Train: 2018-08-06T02:01:28.662713: step 14838, loss 0.491185.
Train: 2018-08-06T02:01:28.917020: step 14839, loss 0.578871.
Train: 2018-08-06T02:01:29.166353: step 14840, loss 0.538812.
Test: 2018-08-06T02:01:30.433962: step 14840, loss 0.549209.
Train: 2018-08-06T02:01:30.672356: step 14841, loss 0.530681.
Train: 2018-08-06T02:01:30.919664: step 14842, loss 0.619164.
Train: 2018-08-06T02:01:31.168000: step 14843, loss 0.514328.
Train: 2018-08-06T02:01:31.414372: step 14844, loss 0.538443.
Train: 2018-08-06T02:01:31.662707: step 14845, loss 0.570791.
Train: 2018-08-06T02:01:31.919989: step 14846, loss 0.530121.
Train: 2018-08-06T02:01:32.178298: step 14847, loss 0.554465.
Train: 2018-08-06T02:01:32.437604: step 14848, loss 0.554412.
Train: 2018-08-06T02:01:32.691955: step 14849, loss 0.644548.
Train: 2018-08-06T02:01:32.941288: step 14850, loss 0.644593.
Test: 2018-08-06T02:01:34.234798: step 14850, loss 0.549639.
Train: 2018-08-06T02:01:34.473191: step 14851, loss 0.562569.
Train: 2018-08-06T02:01:34.724488: step 14852, loss 0.603507.
Train: 2018-08-06T02:01:34.973858: step 14853, loss 0.636136.
Train: 2018-08-06T02:01:35.228167: step 14854, loss 0.562632.
Train: 2018-08-06T02:01:35.475505: step 14855, loss 0.554542.
Train: 2018-08-06T02:01:35.736797: step 14856, loss 0.53028.
Train: 2018-08-06T02:01:35.981130: step 14857, loss 0.586985.
Train: 2018-08-06T02:01:36.236489: step 14858, loss 0.546577.
Train: 2018-08-06T02:01:36.480792: step 14859, loss 0.514342.
Train: 2018-08-06T02:01:36.733117: step 14860, loss 0.611153.
Test: 2018-08-06T02:01:38.014689: step 14860, loss 0.548929.
Train: 2018-08-06T02:01:38.256074: step 14861, loss 0.54664.
Train: 2018-08-06T02:01:38.500421: step 14862, loss 0.60305.
Train: 2018-08-06T02:01:38.748759: step 14863, loss 0.54669.
Train: 2018-08-06T02:01:38.998059: step 14864, loss 0.546713.
Train: 2018-08-06T02:01:39.242407: step 14865, loss 0.554759.
Train: 2018-08-06T02:01:39.491770: step 14866, loss 0.58691.
Train: 2018-08-06T02:01:39.739078: step 14867, loss 0.602973.
Train: 2018-08-06T02:01:40.002405: step 14868, loss 0.578869.
Train: 2018-08-06T02:01:40.263708: step 14869, loss 0.594894.
Train: 2018-08-06T02:01:40.512039: step 14870, loss 0.570862.
Test: 2018-08-06T02:01:41.773636: step 14870, loss 0.549548.
Train: 2018-08-06T02:01:42.024964: step 14871, loss 0.666717.
Train: 2018-08-06T02:01:42.277289: step 14872, loss 0.49929.
Train: 2018-08-06T02:01:42.522634: step 14873, loss 0.51532.
Train: 2018-08-06T02:01:42.774958: step 14874, loss 0.618552.
Train: 2018-08-06T02:01:43.037257: step 14875, loss 0.539228.
Train: 2018-08-06T02:01:43.289617: step 14876, loss 0.563019.
Train: 2018-08-06T02:01:43.538917: step 14877, loss 0.523445.
Train: 2018-08-06T02:01:43.802211: step 14878, loss 0.578859.
Train: 2018-08-06T02:01:44.056531: step 14879, loss 0.539245.
Train: 2018-08-06T02:01:44.302873: step 14880, loss 0.586789.
Test: 2018-08-06T02:01:45.558514: step 14880, loss 0.550839.
Train: 2018-08-06T02:01:45.795912: step 14881, loss 0.555059.
Train: 2018-08-06T02:01:46.044216: step 14882, loss 0.562981.
Train: 2018-08-06T02:01:46.299563: step 14883, loss 0.547081.
Train: 2018-08-06T02:01:46.556844: step 14884, loss 0.539092.
Train: 2018-08-06T02:01:46.814157: step 14885, loss 0.570892.
Train: 2018-08-06T02:01:47.065511: step 14886, loss 0.53897.
Train: 2018-08-06T02:01:47.310853: step 14887, loss 0.62682.
Train: 2018-08-06T02:01:47.570165: step 14888, loss 0.586858.
Train: 2018-08-06T02:01:47.817474: step 14889, loss 0.594854.
Train: 2018-08-06T02:01:48.061820: step 14890, loss 0.474983.
Test: 2018-08-06T02:01:49.336411: step 14890, loss 0.549312.
Train: 2018-08-06T02:01:49.577790: step 14891, loss 0.52285.
Train: 2018-08-06T02:01:49.833116: step 14892, loss 0.546789.
Train: 2018-08-06T02:01:50.094383: step 14893, loss 0.538678.
Train: 2018-08-06T02:01:50.340754: step 14894, loss 0.530514.
Train: 2018-08-06T02:01:50.587067: step 14895, loss 0.570804.
Train: 2018-08-06T02:01:50.831413: step 14896, loss 0.497814.
Train: 2018-08-06T02:01:51.076757: step 14897, loss 0.546359.
Train: 2018-08-06T02:01:51.327087: step 14898, loss 0.603452.
Train: 2018-08-06T02:01:51.585427: step 14899, loss 0.513425.
Train: 2018-08-06T02:01:51.836755: step 14900, loss 0.554323.
Test: 2018-08-06T02:01:53.102339: step 14900, loss 0.547608.
Train: 2018-08-06T02:01:54.030385: step 14901, loss 0.587243.
Train: 2018-08-06T02:01:54.292653: step 14902, loss 0.570756.
Train: 2018-08-06T02:01:54.539025: step 14903, loss 0.628687.
Train: 2018-08-06T02:01:54.789360: step 14904, loss 0.512816.
Train: 2018-08-06T02:01:55.036688: step 14905, loss 0.537613.
Train: 2018-08-06T02:01:55.281011: step 14906, loss 0.562461.
Train: 2018-08-06T02:01:55.543334: step 14907, loss 0.504299.
Train: 2018-08-06T02:01:55.802615: step 14908, loss 0.479184.
Train: 2018-08-06T02:01:56.050982: step 14909, loss 0.45381.
Train: 2018-08-06T02:01:56.297322: step 14910, loss 0.495211.
Test: 2018-08-06T02:01:57.567893: step 14910, loss 0.548453.
Train: 2018-08-06T02:01:57.813239: step 14911, loss 0.545473.
Train: 2018-08-06T02:01:58.061608: step 14912, loss 0.613293.
Train: 2018-08-06T02:01:58.310932: step 14913, loss 0.545298.
Train: 2018-08-06T02:01:58.562265: step 14914, loss 0.605088.
Train: 2018-08-06T02:01:58.807617: step 14915, loss 0.622307.
Train: 2018-08-06T02:01:59.052956: step 14916, loss 0.553764.
Train: 2018-08-06T02:01:59.307268: step 14917, loss 0.682382.
Train: 2018-08-06T02:01:59.553614: step 14918, loss 0.570893.
Train: 2018-08-06T02:01:59.804943: step 14919, loss 0.545264.
Train: 2018-08-06T02:02:00.054276: step 14920, loss 0.485656.
Test: 2018-08-06T02:02:01.320858: step 14920, loss 0.547335.
Train: 2018-08-06T02:02:01.574214: step 14921, loss 0.511244.
Train: 2018-08-06T02:02:01.833486: step 14922, loss 0.553822.
Train: 2018-08-06T02:02:02.079861: step 14923, loss 0.55382.
Train: 2018-08-06T02:02:02.327166: step 14924, loss 0.604955.
Train: 2018-08-06T02:02:02.578494: step 14925, loss 0.519761.
Train: 2018-08-06T02:02:02.833812: step 14926, loss 0.613432.
Train: 2018-08-06T02:02:03.094146: step 14927, loss 0.562345.
Train: 2018-08-06T02:02:03.344445: step 14928, loss 0.579333.
Train: 2018-08-06T02:02:03.592781: step 14929, loss 0.528437.
Train: 2018-08-06T02:02:03.840148: step 14930, loss 0.545415.
Test: 2018-08-06T02:02:05.094764: step 14930, loss 0.548613.
Train: 2018-08-06T02:02:05.335122: step 14931, loss 0.545431.
Train: 2018-08-06T02:02:05.584455: step 14932, loss 0.545443.
Train: 2018-08-06T02:02:05.830838: step 14933, loss 0.520084.
Train: 2018-08-06T02:02:06.077162: step 14934, loss 0.60465.
Train: 2018-08-06T02:02:06.324494: step 14935, loss 0.520098.
Train: 2018-08-06T02:02:06.572862: step 14936, loss 0.596173.
Train: 2018-08-06T02:02:06.820169: step 14937, loss 0.57081.
Train: 2018-08-06T02:02:07.064547: step 14938, loss 0.59612.
Train: 2018-08-06T02:02:07.310855: step 14939, loss 0.537103.
Train: 2018-08-06T02:02:07.558194: step 14940, loss 0.553965.
Test: 2018-08-06T02:02:08.822812: step 14940, loss 0.548541.
Train: 2018-08-06T02:02:09.068183: step 14941, loss 0.587598.
Train: 2018-08-06T02:02:09.314530: step 14942, loss 0.612741.
Train: 2018-08-06T02:02:09.576821: step 14943, loss 0.537294.
Train: 2018-08-06T02:02:09.832144: step 14944, loss 0.554061.
Train: 2018-08-06T02:02:10.080450: step 14945, loss 0.495702.
Train: 2018-08-06T02:02:10.329808: step 14946, loss 0.529075.
Train: 2018-08-06T02:02:10.577120: step 14947, loss 0.562426.
Train: 2018-08-06T02:02:10.824496: step 14948, loss 0.512372.
Train: 2018-08-06T02:02:11.013953: step 14949, loss 0.687122.
Train: 2018-08-06T02:02:11.260294: step 14950, loss 0.570765.
Test: 2018-08-06T02:02:12.532890: step 14950, loss 0.548157.
Train: 2018-08-06T02:02:12.773248: step 14951, loss 0.629022.
Train: 2018-08-06T02:02:13.019589: step 14952, loss 0.554162.
Train: 2018-08-06T02:02:13.268947: step 14953, loss 0.48799.
Train: 2018-08-06T02:02:13.516295: step 14954, loss 0.570756.
Train: 2018-08-06T02:02:13.775599: step 14955, loss 0.529454.
Train: 2018-08-06T02:02:14.029911: step 14956, loss 0.504695.
Train: 2018-08-06T02:02:14.285204: step 14957, loss 0.496379.
Train: 2018-08-06T02:02:14.531557: step 14958, loss 0.562476.
Train: 2018-08-06T02:02:14.778885: step 14959, loss 0.53758.
Train: 2018-08-06T02:02:15.027219: step 14960, loss 0.579071.
Test: 2018-08-06T02:02:16.288845: step 14960, loss 0.549308.
Train: 2018-08-06T02:02:16.527243: step 14961, loss 0.554118.
Train: 2018-08-06T02:02:16.774546: step 14962, loss 0.612427.
Train: 2018-08-06T02:02:17.020894: step 14963, loss 0.595763.
Train: 2018-08-06T02:02:17.265265: step 14964, loss 0.512473.
Train: 2018-08-06T02:02:17.518587: step 14965, loss 0.579093.
Train: 2018-08-06T02:02:17.766926: step 14966, loss 0.537452.
Train: 2018-08-06T02:02:18.016225: step 14967, loss 0.537444.
Train: 2018-08-06T02:02:18.270546: step 14968, loss 0.479081.
Train: 2018-08-06T02:02:18.520907: step 14969, loss 0.554065.
Train: 2018-08-06T02:02:18.769248: step 14970, loss 0.503827.
Test: 2018-08-06T02:02:20.042806: step 14970, loss 0.547797.
Train: 2018-08-06T02:02:20.319067: step 14971, loss 0.553999.
Train: 2018-08-06T02:02:20.565409: step 14972, loss 0.537141.
Train: 2018-08-06T02:02:20.808758: step 14973, loss 0.553933.
Train: 2018-08-06T02:02:21.058092: step 14974, loss 0.528542.
Train: 2018-08-06T02:02:21.304443: step 14975, loss 0.579308.
Train: 2018-08-06T02:02:21.549775: step 14976, loss 0.553848.
Train: 2018-08-06T02:02:21.794122: step 14977, loss 0.613385.
Train: 2018-08-06T02:02:22.052463: step 14978, loss 0.511509.
Train: 2018-08-06T02:02:22.312736: step 14979, loss 0.528265.
Train: 2018-08-06T02:02:22.559076: step 14980, loss 0.579404.
Test: 2018-08-06T02:02:23.846633: step 14980, loss 0.548081.
Train: 2018-08-06T02:02:24.096970: step 14981, loss 0.553802.
Train: 2018-08-06T02:02:24.347326: step 14982, loss 0.579421.
Train: 2018-08-06T02:02:24.609624: step 14983, loss 0.596506.
Train: 2018-08-06T02:02:24.858951: step 14984, loss 0.587944.
Train: 2018-08-06T02:02:25.106265: step 14985, loss 0.647575.
Train: 2018-08-06T02:02:25.353603: step 14986, loss 0.477375.
Train: 2018-08-06T02:02:25.598947: step 14987, loss 0.604778.
Train: 2018-08-06T02:02:25.842327: step 14988, loss 0.57082.
Train: 2018-08-06T02:02:26.091662: step 14989, loss 0.528573.
Train: 2018-08-06T02:02:26.344961: step 14990, loss 0.528631.
Test: 2018-08-06T02:02:27.609570: step 14990, loss 0.548881.
Train: 2018-08-06T02:02:27.844941: step 14991, loss 0.511812.
Train: 2018-08-06T02:02:28.096300: step 14992, loss 0.579226.
Train: 2018-08-06T02:02:28.346623: step 14993, loss 0.528684.
Train: 2018-08-06T02:02:28.594934: step 14994, loss 0.621339.
Train: 2018-08-06T02:02:28.849288: step 14995, loss 0.520314.
Train: 2018-08-06T02:02:29.094599: step 14996, loss 0.503516.
Train: 2018-08-06T02:02:29.341968: step 14997, loss 0.520304.
Train: 2018-08-06T02:02:29.589276: step 14998, loss 0.596074.
Train: 2018-08-06T02:02:29.833654: step 14999, loss 0.621368.
Train: 2018-08-06T02:02:30.079982: step 15000, loss 0.545537.
Test: 2018-08-06T02:02:31.339594: step 15000, loss 0.547747.
Train: 2018-08-06T02:02:32.271836: step 15001, loss 0.663341.
Train: 2018-08-06T02:02:32.520172: step 15002, loss 0.579173.
Train: 2018-08-06T02:02:32.772497: step 15003, loss 0.595871.
Train: 2018-08-06T02:02:33.019860: step 15004, loss 0.612452.
Train: 2018-08-06T02:02:33.280169: step 15005, loss 0.545851.
Train: 2018-08-06T02:02:33.540473: step 15006, loss 0.554209.
Train: 2018-08-06T02:02:33.790773: step 15007, loss 0.595501.
Train: 2018-08-06T02:02:34.041134: step 15008, loss 0.488565.
Train: 2018-08-06T02:02:34.288472: step 15009, loss 0.521528.
Train: 2018-08-06T02:02:34.535813: step 15010, loss 0.51336.
Test: 2018-08-06T02:02:35.803390: step 15010, loss 0.549046.
Train: 2018-08-06T02:02:36.041778: step 15011, loss 0.546155.
Train: 2018-08-06T02:02:36.295100: step 15012, loss 0.529707.
Train: 2018-08-06T02:02:36.544440: step 15013, loss 0.521444.
Train: 2018-08-06T02:02:36.801753: step 15014, loss 0.5378.
Train: 2018-08-06T02:02:37.044104: step 15015, loss 0.612111.
Train: 2018-08-06T02:02:37.299425: step 15016, loss 0.537636.
Train: 2018-08-06T02:02:37.573681: step 15017, loss 0.637039.
Train: 2018-08-06T02:02:37.816008: step 15018, loss 0.562459.
Train: 2018-08-06T02:02:38.075314: step 15019, loss 0.595568.
Train: 2018-08-06T02:02:38.324649: step 15020, loss 0.570758.
Test: 2018-08-06T02:02:39.581286: step 15020, loss 0.549328.
Train: 2018-08-06T02:02:39.822641: step 15021, loss 0.546039.
Train: 2018-08-06T02:02:40.066021: step 15022, loss 0.570757.
Train: 2018-08-06T02:02:40.328322: step 15023, loss 0.587205.
Train: 2018-08-06T02:02:40.574655: step 15024, loss 0.537913.
Train: 2018-08-06T02:02:40.818017: step 15025, loss 0.58717.
Train: 2018-08-06T02:02:41.064322: step 15026, loss 0.611737.
Train: 2018-08-06T02:02:41.324651: step 15027, loss 0.603479.
Train: 2018-08-06T02:02:41.579967: step 15028, loss 0.587083.
Train: 2018-08-06T02:02:41.825286: step 15029, loss 0.538269.
Train: 2018-08-06T02:02:42.070629: step 15030, loss 0.505904.
Test: 2018-08-06T02:02:43.336245: step 15030, loss 0.548781.
Train: 2018-08-06T02:02:43.577600: step 15031, loss 0.497853.
Train: 2018-08-06T02:02:43.824963: step 15032, loss 0.627568.
Train: 2018-08-06T02:02:44.080255: step 15033, loss 0.643732.
Train: 2018-08-06T02:02:44.340584: step 15034, loss 0.546551.
Train: 2018-08-06T02:02:44.586925: step 15035, loss 0.490102.
Train: 2018-08-06T02:02:44.832244: step 15036, loss 0.490101.
Train: 2018-08-06T02:02:45.078586: step 15037, loss 0.522301.
Train: 2018-08-06T02:02:45.327949: step 15038, loss 0.578898.
Train: 2018-08-06T02:02:45.571295: step 15039, loss 0.538319.
Train: 2018-08-06T02:02:45.813670: step 15040, loss 0.59519.
Test: 2018-08-06T02:02:47.089208: step 15040, loss 0.548655.
Train: 2018-08-06T02:02:47.327596: step 15041, loss 0.595212.
Train: 2018-08-06T02:02:47.574909: step 15042, loss 0.578925.
Train: 2018-08-06T02:02:47.830226: step 15043, loss 0.530034.
Train: 2018-08-06T02:02:48.078562: step 15044, loss 0.570776.
Train: 2018-08-06T02:02:48.323906: step 15045, loss 0.505504.
Train: 2018-08-06T02:02:48.574237: step 15046, loss 0.619799.
Train: 2018-08-06T02:02:48.824568: step 15047, loss 0.587116.
Train: 2018-08-06T02:02:49.070909: step 15048, loss 0.562597.
Train: 2018-08-06T02:02:49.321239: step 15049, loss 0.54626.
Train: 2018-08-06T02:02:49.581542: step 15050, loss 0.627963.
Test: 2018-08-06T02:02:50.852144: step 15050, loss 0.548983.
Train: 2018-08-06T02:02:51.088517: step 15051, loss 0.529971.
Train: 2018-08-06T02:02:51.335876: step 15052, loss 0.538147.
Train: 2018-08-06T02:02:51.596180: step 15053, loss 0.538145.
Train: 2018-08-06T02:02:51.843494: step 15054, loss 0.587096.
Train: 2018-08-06T02:02:52.097814: step 15055, loss 0.587095.
Train: 2018-08-06T02:02:52.345183: step 15056, loss 0.611562.
Train: 2018-08-06T02:02:52.598506: step 15057, loss 0.578922.
Train: 2018-08-06T02:02:52.846843: step 15058, loss 0.578915.
Train: 2018-08-06T02:02:53.095147: step 15059, loss 0.587023.
Train: 2018-08-06T02:02:53.342485: step 15060, loss 0.603199.
Test: 2018-08-06T02:02:54.612089: step 15060, loss 0.549238.
Train: 2018-08-06T02:02:54.861422: step 15061, loss 0.554652.
Train: 2018-08-06T02:02:55.106766: step 15062, loss 0.554699.
Train: 2018-08-06T02:02:55.351144: step 15063, loss 0.522555.
Train: 2018-08-06T02:02:55.604466: step 15064, loss 0.635152.
Train: 2018-08-06T02:02:55.857788: step 15065, loss 0.546777.
Train: 2018-08-06T02:02:56.109116: step 15066, loss 0.538811.
Train: 2018-08-06T02:02:56.355459: step 15067, loss 0.554849.
Train: 2018-08-06T02:02:56.602766: step 15068, loss 0.594867.
Train: 2018-08-06T02:02:56.849141: step 15069, loss 0.602844.
Train: 2018-08-06T02:02:57.096476: step 15070, loss 0.515015.
Test: 2018-08-06T02:02:58.382008: step 15070, loss 0.548295.
Train: 2018-08-06T02:02:58.626354: step 15071, loss 0.634706.
Train: 2018-08-06T02:02:58.934908: step 15072, loss 0.53904.
Train: 2018-08-06T02:02:59.180289: step 15073, loss 0.594771.
Train: 2018-08-06T02:02:59.426592: step 15074, loss 0.53119.
Train: 2018-08-06T02:02:59.669968: step 15075, loss 0.602682.
Train: 2018-08-06T02:02:59.923296: step 15076, loss 0.570927.
Train: 2018-08-06T02:03:00.169632: step 15077, loss 0.594704.
Train: 2018-08-06T02:03:00.425952: step 15078, loss 0.594681.
Train: 2018-08-06T02:03:00.671265: step 15079, loss 0.539389.
Train: 2018-08-06T02:03:00.920597: step 15080, loss 0.618294.
Test: 2018-08-06T02:03:02.194191: step 15080, loss 0.549984.
Train: 2018-08-06T02:03:02.432584: step 15081, loss 0.508039.
Train: 2018-08-06T02:03:02.689896: step 15082, loss 0.578868.
Train: 2018-08-06T02:03:02.944216: step 15083, loss 0.563146.
Train: 2018-08-06T02:03:03.188542: step 15084, loss 0.563153.
Train: 2018-08-06T02:03:03.436898: step 15085, loss 0.555298.
Train: 2018-08-06T02:03:03.686201: step 15086, loss 0.571011.
Train: 2018-08-06T02:03:03.939524: step 15087, loss 0.508125.
Train: 2018-08-06T02:03:04.186862: step 15088, loss 0.555248.
Train: 2018-08-06T02:03:04.435231: step 15089, loss 0.547318.
Train: 2018-08-06T02:03:04.679544: step 15090, loss 0.555153.
Test: 2018-08-06T02:03:05.940173: step 15090, loss 0.54888.
Train: 2018-08-06T02:03:06.176551: step 15091, loss 0.539259.
Train: 2018-08-06T02:03:06.421922: step 15092, loss 0.531213.
Train: 2018-08-06T02:03:06.673246: step 15093, loss 0.546995.
Train: 2018-08-06T02:03:06.934545: step 15094, loss 0.602838.
Train: 2018-08-06T02:03:07.193821: step 15095, loss 0.546826.
Train: 2018-08-06T02:03:07.454151: step 15096, loss 0.619016.
Train: 2018-08-06T02:03:07.698507: step 15097, loss 0.530648.
Train: 2018-08-06T02:03:07.949850: step 15098, loss 0.530572.
Train: 2018-08-06T02:03:08.199132: step 15099, loss 0.562748.
Train: 2018-08-06T02:03:08.392615: step 15100, loss 0.648967.
Test: 2018-08-06T02:03:09.673190: step 15100, loss 0.548656.
