Train: 2018-08-08T15:23:14.356185: step 1, loss 0.920763.
Train: 2018-08-08T15:23:17.489515: step 2, loss 3.02214.
Train: 2018-08-08T15:23:20.601790: step 3, loss 4.02952.
Train: 2018-08-08T15:23:23.759185: step 4, loss 5.0369.
Train: 2018-08-08T15:23:26.913571: step 5, loss 6.04429.
Train: 2018-08-08T15:23:30.108065: step 6, loss 3.52583.
Train: 2018-08-08T15:23:33.295539: step 7, loss 3.52583.
Train: 2018-08-08T15:23:36.432881: step 8, loss 5.0369.
Train: 2018-08-08T15:23:39.577241: step 9, loss 3.02214.
Train: 2018-08-08T15:23:42.732630: step 10, loss 4.28137.
Test: 2018-08-08T15:24:03.190020: step 10, loss 3.80308.
Train: 2018-08-08T15:24:06.359447: step 11, loss 4.53321.
Train: 2018-08-08T15:24:09.510826: step 12, loss 3.27399.
Train: 2018-08-08T15:24:12.661202: step 13, loss 3.52583.
Train: 2018-08-08T15:24:15.821604: step 14, loss 3.77768.
Train: 2018-08-08T15:24:18.984012: step 15, loss 4.53321.
Train: 2018-08-08T15:24:22.177503: step 16, loss 3.27399.
Train: 2018-08-08T15:24:25.466247: step 17, loss 3.52583.
Train: 2018-08-08T15:24:28.670767: step 18, loss 4.02952.
Train: 2018-08-08T15:24:31.844204: step 19, loss 1.51107.
Train: 2018-08-08T15:24:35.040703: step 20, loss 4.78506.
Test: 2018-08-08T15:24:55.715672: step 20, loss 3.78403.
Train: 2018-08-08T15:24:58.870058: step 21, loss 4.53321.
Train: 2018-08-08T15:25:02.046504: step 22, loss 5.0369.
Train: 2018-08-08T15:25:05.192869: step 23, loss 3.77768.
Train: 2018-08-08T15:25:08.337229: step 24, loss 3.52583.
Train: 2018-08-08T15:25:11.666079: step 25, loss 4.28137.
Train: 2018-08-08T15:25:14.910706: step 26, loss 3.27399.
Train: 2018-08-08T15:25:18.124250: step 27, loss 3.77768.
Train: 2018-08-08T15:25:21.308717: step 28, loss 3.52583.
Train: 2018-08-08T15:25:24.514239: step 29, loss 5.28875.
Train: 2018-08-08T15:25:27.771900: step 30, loss 2.51845.
Test: 2018-08-08T15:25:48.717590: step 30, loss 3.80308.
Train: 2018-08-08T15:25:52.000317: step 31, loss 5.28875.
Train: 2018-08-08T15:25:55.247951: step 32, loss 3.52583.
Train: 2018-08-08T15:25:58.447458: step 33, loss 4.53321.
Train: 2018-08-08T15:26:01.720159: step 34, loss 3.02214.
Train: 2018-08-08T15:26:04.976818: step 35, loss 3.77768.
Train: 2018-08-08T15:26:08.156271: step 36, loss 4.53321.
Train: 2018-08-08T15:26:11.327703: step 37, loss 3.02214.
Train: 2018-08-08T15:26:14.501140: step 38, loss 2.01476.
Train: 2018-08-08T15:26:17.672572: step 39, loss 3.52583.
Train: 2018-08-08T15:26:20.871076: step 40, loss 4.53321.
Test: 2018-08-08T15:26:41.628265: step 40, loss 2.09877.
Train: 2018-08-08T15:26:44.841808: step 41, loss 4.28137.
Train: 2018-08-08T15:26:48.138574: step 42, loss 4.02952.
Train: 2018-08-08T15:26:51.376181: step 43, loss 3.77768.
Train: 2018-08-08T15:26:54.672946: step 44, loss 4.02952.
Train: 2018-08-08T15:26:57.829338: step 45, loss 4.75284.
Train: 2018-08-08T15:27:00.989741: step 46, loss 3.32108.
Train: 2018-08-08T15:27:04.175210: step 47, loss 2.90179.
Train: 2018-08-08T15:27:07.372712: step 48, loss 3.99093.
Train: 2018-08-08T15:27:10.552165: step 49, loss 2.88074.
Train: 2018-08-08T15:27:13.808823: step 50, loss 1.29155.
Test: 2018-08-08T15:27:34.661265: step 50, loss 1.1733.
Train: 2018-08-08T15:27:37.834702: step 51, loss 0.782468.
Train: 2018-08-08T15:27:41.018165: step 52, loss 0.723769.
Train: 2018-08-08T15:27:44.191603: step 53, loss 0.741227.
Train: 2018-08-08T15:27:47.331952: step 54, loss 0.520622.
Train: 2018-08-08T15:27:50.523437: step 55, loss 0.581376.
Train: 2018-08-08T15:27:53.709909: step 56, loss 0.863604.
Train: 2018-08-08T15:27:56.887357: step 57, loss 0.654646.
Train: 2018-08-08T15:28:00.075834: step 58, loss 0.642451.
Train: 2018-08-08T15:28:03.273335: step 59, loss 0.6185.
Train: 2018-08-08T15:28:06.459807: step 60, loss 0.549372.
Test: 2018-08-08T15:28:27.324282: step 60, loss 1.0764.
Train: 2018-08-08T15:28:30.556875: step 61, loss 0.457539.
Train: 2018-08-08T15:28:33.717278: step 62, loss 0.934292.
Train: 2018-08-08T15:28:36.920795: step 63, loss 0.597163.
Train: 2018-08-08T15:28:40.107267: step 64, loss 0.642977.
Train: 2018-08-08T15:28:43.285717: step 65, loss 0.678607.
Train: 2018-08-08T15:28:46.431080: step 66, loss 0.55862.
Train: 2018-08-08T15:28:49.616549: step 67, loss 0.678088.
Train: 2018-08-08T15:28:52.811042: step 68, loss 0.715399.
Train: 2018-08-08T15:28:56.075722: step 69, loss 0.622763.
Train: 2018-08-08T15:28:59.229106: step 70, loss 0.586461.
Test: 2018-08-08T15:29:20.148727: step 70, loss 0.607595.
Train: 2018-08-08T15:29:23.335198: step 71, loss 0.591637.
Train: 2018-08-08T15:29:26.602886: step 72, loss 0.598542.
Train: 2018-08-08T15:29:29.791363: step 73, loss 0.645071.
Train: 2018-08-08T15:29:32.942742: step 74, loss 0.573065.
Train: 2018-08-08T15:29:36.113171: step 75, loss 0.509525.
Train: 2018-08-08T15:29:39.327718: step 76, loss 0.446859.
Train: 2018-08-08T15:29:42.489123: step 77, loss 0.428541.
Train: 2018-08-08T15:29:45.695648: step 78, loss 0.814887.
Train: 2018-08-08T15:29:48.869085: step 79, loss 0.516529.
Train: 2018-08-08T15:29:52.043525: step 80, loss 0.572561.
Test: 2018-08-08T15:30:13.077133: step 80, loss 0.648067.
Train: 2018-08-08T15:30:16.280650: step 81, loss 0.606822.
Train: 2018-08-08T15:30:19.492189: step 82, loss 0.486833.
Train: 2018-08-08T15:30:22.693700: step 83, loss 0.511756.
Train: 2018-08-08T15:30:25.911255: step 84, loss 0.578373.
Train: 2018-08-08T15:30:29.116778: step 85, loss 0.560995.
Train: 2018-08-08T15:30:32.307260: step 86, loss 0.554428.
Train: 2018-08-08T15:30:35.485711: step 87, loss 0.516058.
Train: 2018-08-08T15:30:38.699255: step 88, loss 0.56337.
Train: 2018-08-08T15:30:41.897759: step 89, loss 0.474381.
Train: 2018-08-08T15:30:45.067185: step 90, loss 0.443166.
Test: 2018-08-08T15:31:05.983797: step 90, loss 0.631971.
Train: 2018-08-08T15:31:09.234439: step 91, loss 0.596639.
Train: 2018-08-08T15:31:12.424922: step 92, loss 0.550404.
Train: 2018-08-08T15:31:15.620418: step 93, loss 0.489112.
Train: 2018-08-08T15:31:18.934229: step 94, loss 0.558562.
Train: 2018-08-08T15:31:22.117692: step 95, loss 0.51451.
Train: 2018-08-08T15:31:25.289125: step 96, loss 0.446327.
Train: 2018-08-08T15:31:28.545783: step 97, loss 0.565303.
Train: 2018-08-08T15:31:31.731252: step 98, loss 0.70258.
Train: 2018-08-08T15:31:34.896668: step 99, loss 0.58284.
Train: 2018-08-08T15:31:38.081135: step 100, loss 0.562147.
Test: 2018-08-08T15:31:59.031837: step 100, loss 0.627417.
Train: 2018-08-08T15:32:04.168494: step 101, loss 0.5071.
Train: 2018-08-08T15:32:07.363990: step 102, loss 0.512633.
Train: 2018-08-08T15:32:10.556478: step 103, loss 0.55435.
Train: 2018-08-08T15:32:13.741947: step 104, loss 0.579571.
Train: 2018-08-08T15:32:16.909368: step 105, loss 0.558642.
Train: 2018-08-08T15:32:20.107872: step 106, loss 0.565468.
Train: 2018-08-08T15:32:23.280307: step 107, loss 0.590351.
Train: 2018-08-08T15:32:26.444720: step 108, loss 0.620266.
Train: 2018-08-08T15:32:29.617155: step 109, loss 0.644426.
Train: 2018-08-08T15:32:32.803627: step 110, loss 0.581199.
Test: 2018-08-08T15:32:53.839556: step 110, loss 0.617858.
Train: 2018-08-08T15:32:57.088193: step 111, loss 0.495826.
Train: 2018-08-08T15:33:00.276670: step 112, loss 0.512192.
Train: 2018-08-08T15:33:03.644625: step 113, loss 0.699354.
Train: 2018-08-08T15:33:07.007566: step 114, loss 0.513935.
Train: 2018-08-08T15:33:10.364491: step 115, loss 0.492415.
Train: 2018-08-08T15:33:13.625160: step 116, loss 0.49454.
Train: 2018-08-08T15:33:16.853744: step 117, loss 0.603483.
Train: 2018-08-08T15:33:20.090349: step 118, loss 0.498437.
Train: 2018-08-08T15:33:23.415189: step 119, loss 0.559627.
Train: 2018-08-08T15:33:26.715965: step 120, loss 0.517353.
Test: 2018-08-08T15:33:47.989526: step 120, loss 0.607622.
Train: 2018-08-08T15:33:51.238163: step 121, loss 0.546117.
Train: 2018-08-08T15:33:54.482789: step 122, loss 0.583376.
Train: 2018-08-08T15:33:57.704354: step 123, loss 0.545068.
Train: 2018-08-08T15:34:00.951989: step 124, loss 0.55624.
Train: 2018-08-08T15:34:04.221682: step 125, loss 0.437213.
Train: 2018-08-08T15:34:07.534490: step 126, loss 0.652792.
Train: 2018-08-08T15:34:10.905453: step 127, loss 0.484622.
Train: 2018-08-08T15:34:14.175146: step 128, loss 0.554869.
Train: 2018-08-08T15:34:17.373650: step 129, loss 0.522214.
Train: 2018-08-08T15:34:20.555108: step 130, loss 0.546413.
Test: 2018-08-08T15:34:41.541907: step 130, loss 0.604215.
Train: 2018-08-08T15:34:44.766480: step 131, loss 0.558976.
Train: 2018-08-08T15:34:48.046199: step 132, loss 0.60675.
Train: 2018-08-08T15:34:51.361013: step 133, loss 0.524013.
Train: 2018-08-08T15:34:54.833244: step 134, loss 0.622679.
Train: 2018-08-08T15:34:58.185354: step 135, loss 0.587703.
Train: 2018-08-08T15:35:01.677762: step 136, loss 0.609214.
Train: 2018-08-08T15:35:04.958484: step 137, loss 0.484756.
Train: 2018-08-08T15:35:08.146962: step 138, loss 0.660192.
Train: 2018-08-08T15:35:11.352484: step 139, loss 0.495565.
Train: 2018-08-08T15:35:14.569036: step 140, loss 0.51232.
Test: 2018-08-08T15:35:36.541814: step 140, loss 0.603519.
Train: 2018-08-08T15:35:39.945491: step 141, loss 0.586142.
Train: 2018-08-08T15:35:43.194687: step 142, loss 0.620097.
Train: 2018-08-08T15:35:46.633067: step 143, loss 0.551328.
Train: 2018-08-08T15:35:49.991997: step 144, loss 0.560679.
Train: 2018-08-08T15:35:53.418106: step 145, loss 0.529459.
Train: 2018-08-08T15:35:56.906381: step 146, loss 0.457044.
Train: 2018-08-08T15:36:00.472863: step 147, loss 0.709371.
Train: 2018-08-08T15:36:03.877916: step 148, loss 0.722851.
Train: 2018-08-08T15:36:07.176686: step 149, loss 0.582312.
Train: 2018-08-08T15:36:10.555670: step 150, loss 0.695174.
Test: 2018-08-08T15:36:32.375355: step 150, loss 0.603073.
Train: 2018-08-08T15:36:35.827843: step 151, loss 0.585464.
Train: 2018-08-08T15:36:39.124608: step 152, loss 0.657734.
Train: 2018-08-08T15:36:42.398588: step 153, loss 0.592903.
Train: 2018-08-08T15:36:45.670287: step 154, loss 0.638899.
Train: 2018-08-08T15:36:49.254700: step 155, loss 0.549817.
Train: 2018-08-08T15:36:52.512361: step 156, loss 0.539141.
Train: 2018-08-08T15:36:55.856252: step 157, loss 0.509736.
Train: 2018-08-08T15:36:59.227214: step 158, loss 0.58441.
Train: 2018-08-08T15:37:02.580128: step 159, loss 0.498036.
Train: 2018-08-08T15:37:05.955102: step 160, loss 0.501798.
Test: 2018-08-08T15:37:27.890422: step 160, loss 0.583618.
Train: 2018-08-08T15:37:31.091934: step 161, loss 0.627128.
Train: 2018-08-08T15:37:34.285424: step 162, loss 0.536667.
Train: 2018-08-08T15:37:37.443821: step 163, loss 0.554369.
Train: 2018-08-08T15:37:40.623274: step 164, loss 0.556932.
Train: 2018-08-08T15:37:43.844840: step 165, loss 0.692091.
Train: 2018-08-08T15:37:47.017274: step 166, loss 0.482505.
Train: 2018-08-08T15:37:50.224803: step 167, loss 0.487812.
Train: 2018-08-08T15:37:53.398240: step 168, loss 0.617447.
Train: 2018-08-08T15:37:56.556637: step 169, loss 0.438427.
Train: 2018-08-08T15:37:59.749125: step 170, loss 0.745367.
Test: 2018-08-08T15:38:20.709855: step 170, loss 0.577917.
Train: 2018-08-08T15:38:23.896326: step 171, loss 0.558934.
Train: 2018-08-08T15:38:27.093827: step 172, loss 0.528493.
Train: 2018-08-08T15:38:30.290326: step 173, loss 0.667009.
Train: 2018-08-08T15:38:33.509885: step 174, loss 0.557388.
Train: 2018-08-08T15:38:36.674299: step 175, loss 0.63462.
Train: 2018-08-08T15:38:39.843725: step 176, loss 0.501587.
Train: 2018-08-08T15:38:43.016160: step 177, loss 0.537918.
Train: 2018-08-08T15:38:46.206643: step 178, loss 0.536561.
Train: 2018-08-08T15:38:49.371056: step 179, loss 0.62225.
Train: 2018-08-08T15:38:52.580589: step 180, loss 0.486302.
Test: 2018-08-08T15:39:13.538311: step 180, loss 0.575045.
Train: 2018-08-08T15:39:16.732803: step 181, loss 0.631968.
Train: 2018-08-08T15:39:19.914262: step 182, loss 0.583244.
Train: 2018-08-08T15:39:23.136830: step 183, loss 0.559202.
Train: 2018-08-08T15:39:26.335334: step 184, loss 0.496189.
Train: 2018-08-08T15:39:29.730120: step 185, loss 0.607955.
Train: 2018-08-08T15:39:33.040923: step 186, loss 0.550983.
Train: 2018-08-08T15:39:36.220376: step 187, loss 0.643793.
Train: 2018-08-08T15:39:39.411861: step 188, loss 0.598085.
Train: 2018-08-08T15:39:42.625405: step 189, loss 0.479692.
Train: 2018-08-08T15:39:45.855994: step 190, loss 0.531431.
Test: 2018-08-08T15:40:06.902675: step 190, loss 0.574789.
Train: 2018-08-08T15:40:10.102181: step 191, loss 0.506469.
Train: 2018-08-08T15:40:13.314722: step 192, loss 0.52976.
Train: 2018-08-08T15:40:16.511221: step 193, loss 0.523376.
Train: 2018-08-08T15:40:19.723762: step 194, loss 0.584808.
Train: 2018-08-08T15:40:22.964378: step 195, loss 0.472634.
Train: 2018-08-08T15:40:26.278188: step 196, loss 0.58659.
Train: 2018-08-08T15:40:29.470676: step 197, loss 0.514245.
Train: 2018-08-08T15:40:32.662162: step 198, loss 0.644638.
Train: 2018-08-08T15:40:35.858660: step 199, loss 0.608099.
Train: 2018-08-08T15:40:39.038114: step 200, loss 0.516516.
Test: 2018-08-08T15:40:59.947706: step 200, loss 0.576016.
Train: 2018-08-08T15:41:05.041248: step 201, loss 0.560014.
Train: 2018-08-08T15:41:08.231731: step 202, loss 0.538379.
Train: 2018-08-08T15:41:11.399152: step 203, loss 0.468137.
Train: 2018-08-08T15:41:14.573592: step 204, loss 0.40082.
Train: 2018-08-08T15:41:17.764075: step 205, loss 0.630612.
Train: 2018-08-08T15:41:20.954558: step 206, loss 0.520275.
Train: 2018-08-08T15:41:24.095910: step 207, loss 0.627743.
Train: 2018-08-08T15:41:27.279374: step 208, loss 0.5298.
Train: 2018-08-08T15:41:30.437771: step 209, loss 0.58288.
Train: 2018-08-08T15:41:33.616222: step 210, loss 0.532254.
Test: 2018-08-08T15:41:54.492727: step 210, loss 0.575553.
Train: 2018-08-08T15:41:57.699252: step 211, loss 0.527357.
Train: 2018-08-08T15:42:00.925830: step 212, loss 0.562041.
Train: 2018-08-08T15:42:04.132355: step 213, loss 0.513774.
Train: 2018-08-08T15:42:07.331862: step 214, loss 0.566066.
Train: 2018-08-08T15:42:10.522345: step 215, loss 0.565263.
Train: 2018-08-08T15:42:13.716838: step 216, loss 0.455604.
Train: 2018-08-08T15:42:16.931385: step 217, loss 0.702892.
Train: 2018-08-08T15:42:20.128886: step 218, loss 0.635665.
Train: 2018-08-08T15:42:23.320371: step 219, loss 0.570348.
Train: 2018-08-08T15:42:26.523889: step 220, loss 0.577961.
Test: 2018-08-08T15:42:47.376330: step 220, loss 0.571964.
Train: 2018-08-08T15:42:50.543750: step 221, loss 0.610961.
Train: 2018-08-08T15:42:53.763310: step 222, loss 0.566261.
Train: 2018-08-08T15:42:56.998913: step 223, loss 0.602066.
Train: 2018-08-08T15:43:00.184382: step 224, loss 0.584908.
Train: 2018-08-08T15:43:03.386897: step 225, loss 0.539007.
Train: 2018-08-08T15:43:06.559332: step 226, loss 0.529179.
Train: 2018-08-08T15:43:09.702689: step 227, loss 0.6741.
Train: 2018-08-08T15:43:12.898185: step 228, loss 0.605208.
Train: 2018-08-08T15:43:16.079643: step 229, loss 0.56907.
Train: 2018-08-08T15:43:19.230020: step 230, loss 0.532251.
Test: 2018-08-08T15:43:40.264946: step 230, loss 0.570403.
Train: 2018-08-08T15:43:43.421337: step 231, loss 0.554244.
Train: 2018-08-08T15:43:46.607809: step 232, loss 0.52969.
Train: 2018-08-08T15:43:49.795284: step 233, loss 0.568492.
Train: 2018-08-08T15:43:52.974737: step 234, loss 0.655088.
Train: 2018-08-08T15:43:56.168228: step 235, loss 0.535391.
Train: 2018-08-08T15:43:59.338657: step 236, loss 0.597212.
Train: 2018-08-08T15:44:02.651465: step 237, loss 0.665944.
Train: 2018-08-08T15:44:05.873030: step 238, loss 0.582086.
Train: 2018-08-08T15:44:09.052483: step 239, loss 0.533078.
Train: 2018-08-08T15:44:12.261014: step 240, loss 0.522186.
Test: 2018-08-08T15:44:33.225754: step 240, loss 0.572277.
Train: 2018-08-08T15:44:36.461356: step 241, loss 0.58046.
Train: 2018-08-08T15:44:39.652841: step 242, loss 0.568089.
Train: 2018-08-08T15:44:42.860370: step 243, loss 0.612619.
Train: 2018-08-08T15:44:46.049849: step 244, loss 0.605862.
Train: 2018-08-08T15:44:49.251361: step 245, loss 0.46398.
Train: 2018-08-08T15:44:52.434825: step 246, loss 0.560355.
Train: 2018-08-08T15:44:55.604252: step 247, loss 0.589925.
Train: 2018-08-08T15:44:58.807769: step 248, loss 0.531671.
Train: 2018-08-08T15:45:02.006273: step 249, loss 0.494714.
Train: 2018-08-08T15:45:05.207785: step 250, loss 0.585413.
Test: 2018-08-08T15:45:26.162498: step 250, loss 0.572107.
Train: 2018-08-08T15:45:29.333930: step 251, loss 0.427869.
Train: 2018-08-08T15:45:32.527420: step 252, loss 0.56994.
Train: 2018-08-08T15:45:35.724921: step 253, loss 0.548527.
Train: 2018-08-08T15:45:38.904375: step 254, loss 0.652377.
Train: 2018-08-08T15:45:42.103882: step 255, loss 0.497673.
Train: 2018-08-08T15:45:45.270300: step 256, loss 0.489103.
Train: 2018-08-08T15:45:48.433711: step 257, loss 0.521731.
Train: 2018-08-08T15:45:51.653271: step 258, loss 0.439602.
Train: 2018-08-08T15:45:54.847764: step 259, loss 0.536169.
Train: 2018-08-08T15:45:58.001148: step 260, loss 0.548132.
Test: 2018-08-08T15:46:18.938816: step 260, loss 0.569776.
Train: 2018-08-08T15:46:22.087186: step 261, loss 0.437308.
Train: 2018-08-08T15:46:25.300730: step 262, loss 0.499369.
Train: 2018-08-08T15:46:28.511266: step 263, loss 0.532148.
Train: 2018-08-08T15:46:31.688714: step 264, loss 0.632547.
Train: 2018-08-08T15:46:34.883207: step 265, loss 0.529553.
Train: 2018-08-08T15:46:38.068677: step 266, loss 0.604411.
Train: 2018-08-08T15:46:41.295255: step 267, loss 0.539636.
Train: 2018-08-08T15:46:44.493759: step 268, loss 0.553247.
Train: 2018-08-08T15:46:47.674215: step 269, loss 0.522232.
Train: 2018-08-08T15:46:50.849658: step 270, loss 0.655288.
Test: 2018-08-08T15:47:11.770281: step 270, loss 0.570007.
Train: 2018-08-08T15:47:15.120186: step 271, loss 0.571517.
Train: 2018-08-08T15:47:18.305655: step 272, loss 0.526719.
Train: 2018-08-08T15:47:21.522207: step 273, loss 0.534576.
Train: 2018-08-08T15:47:24.745778: step 274, loss 0.590023.
Train: 2018-08-08T15:47:27.927237: step 275, loss 0.539633.
Train: 2018-08-08T15:47:31.087639: step 276, loss 0.635503.
Train: 2018-08-08T15:47:34.283135: step 277, loss 0.52055.
Train: 2018-08-08T15:47:37.498684: step 278, loss 0.521846.
Train: 2018-08-08T15:47:40.722255: step 279, loss 0.603522.
Train: 2018-08-08T15:47:43.916748: step 280, loss 0.490052.
Test: 2018-08-08T15:48:04.890512: step 280, loss 0.569175.
Train: 2018-08-08T15:48:08.035874: step 281, loss 0.600883.
Train: 2018-08-08T15:48:11.219338: step 282, loss 0.551048.
Train: 2018-08-08T15:48:14.443912: step 283, loss 0.580139.
Train: 2018-08-08T15:48:17.631386: step 284, loss 0.535155.
Train: 2018-08-08T15:48:20.858967: step 285, loss 0.551783.
Train: 2018-08-08T15:48:24.072511: step 286, loss 0.529772.
Train: 2018-08-08T15:48:27.367271: step 287, loss 0.541857.
Train: 2018-08-08T15:48:30.686098: step 288, loss 0.504804.
Train: 2018-08-08T15:48:33.921252: step 289, loss 0.464642.
Train: 2018-08-08T15:48:37.131804: step 290, loss 0.529216.
Test: 2018-08-08T15:48:58.114591: step 290, loss 0.566122.
Train: 2018-08-08T15:49:01.292038: step 291, loss 0.661322.
Train: 2018-08-08T15:49:04.484526: step 292, loss 0.4874.
Train: 2018-08-08T15:49:07.697068: step 293, loss 0.552701.
Train: 2018-08-08T15:49:10.919635: step 294, loss 0.531514.
Train: 2018-08-08T15:49:14.115131: step 295, loss 0.49659.
Train: 2018-08-08T15:49:17.297593: step 296, loss 0.516419.
Train: 2018-08-08T15:49:20.519158: step 297, loss 0.567804.
Train: 2018-08-08T15:49:23.728691: step 298, loss 0.552107.
Train: 2018-08-08T15:49:26.929201: step 299, loss 0.522311.
Train: 2018-08-08T15:49:30.121688: step 300, loss 0.611161.
Test: 2018-08-08T15:49:51.076402: step 300, loss 0.564621.
Train: 2018-08-08T15:49:55.894210: step 301, loss 0.429115.
Train: 2018-08-08T15:49:59.116778: step 302, loss 0.616037.
Train: 2018-08-08T15:50:02.465565: step 303, loss 0.589158.
Train: 2018-08-08T15:50:05.680112: step 304, loss 0.622388.
Train: 2018-08-08T15:50:08.864578: step 305, loss 0.49458.
Train: 2018-08-08T15:50:12.091157: step 306, loss 0.481754.
Train: 2018-08-08T15:50:15.283645: step 307, loss 0.517869.
Train: 2018-08-08T15:50:18.469114: step 308, loss 0.557553.
Train: 2018-08-08T15:50:21.683661: step 309, loss 0.447732.
Train: 2018-08-08T15:50:24.885173: step 310, loss 0.662865.
Test: 2018-08-08T15:50:45.862947: step 310, loss 0.565362.
Train: 2018-08-08T15:50:49.028363: step 311, loss 0.519461.
Train: 2018-08-08T15:50:52.217842: step 312, loss 0.574059.
Train: 2018-08-08T15:50:55.405317: step 313, loss 0.552527.
Train: 2018-08-08T15:50:58.618861: step 314, loss 0.56239.
Train: 2018-08-08T15:51:01.818368: step 315, loss 0.56109.
Train: 2018-08-08T15:51:05.026898: step 316, loss 0.587.
Train: 2018-08-08T15:51:08.225402: step 317, loss 0.588413.
Train: 2018-08-08T15:51:11.407864: step 318, loss 0.60316.
Train: 2018-08-08T15:51:14.611381: step 319, loss 0.608254.
Train: 2018-08-08T15:51:17.827933: step 320, loss 0.517572.
Test: 2018-08-08T15:51:38.828769: step 320, loss 0.572519.
Train: 2018-08-08T15:51:42.023262: step 321, loss 0.611623.
Train: 2018-08-08T15:51:45.206726: step 322, loss 0.516927.
Train: 2018-08-08T15:51:48.383171: step 323, loss 0.563524.
Train: 2018-08-08T15:51:51.568640: step 324, loss 0.533329.
Train: 2018-08-08T15:51:54.792211: step 325, loss 0.555357.
Train: 2018-08-08T15:51:58.012773: step 326, loss 0.609139.
Train: 2018-08-08T15:52:01.189219: step 327, loss 0.523256.
Train: 2018-08-08T15:52:04.412789: step 328, loss 0.659637.
Train: 2018-08-08T15:52:07.610290: step 329, loss 0.519992.
Train: 2018-08-08T15:52:10.833861: step 330, loss 0.563619.
Test: 2018-08-08T15:52:31.807628: step 330, loss 0.575742.
Train: 2018-08-08T15:52:35.020166: step 331, loss 0.532594.
Train: 2018-08-08T15:52:38.239726: step 332, loss 0.530566.
Train: 2018-08-08T15:52:41.418176: step 333, loss 0.582602.
Train: 2018-08-08T15:52:44.633725: step 334, loss 0.492271.
Train: 2018-08-08T15:52:47.833232: step 335, loss 0.510785.
Train: 2018-08-08T15:52:51.033741: step 336, loss 0.575739.
Train: 2018-08-08T15:52:54.249290: step 337, loss 0.43709.
Train: 2018-08-08T15:52:57.474867: step 338, loss 0.483726.
Train: 2018-08-08T15:53:00.690416: step 339, loss 0.584271.
Train: 2018-08-08T15:53:03.896941: step 340, loss 0.520714.
Test: 2018-08-08T15:53:24.926854: step 340, loss 0.570781.
Train: 2018-08-08T15:53:28.142403: step 341, loss 0.527366.
Train: 2018-08-08T15:53:31.392043: step 342, loss 0.439946.
Train: 2018-08-08T15:53:34.602579: step 343, loss 0.541612.
Train: 2018-08-08T15:53:37.823141: step 344, loss 0.68145.
Train: 2018-08-08T15:53:41.035683: step 345, loss 0.630705.
Train: 2018-08-08T15:53:44.239200: step 346, loss 0.559271.
Train: 2018-08-08T15:53:47.423667: step 347, loss 0.528913.
Train: 2018-08-08T15:53:50.611141: step 348, loss 0.597484.
Train: 2018-08-08T15:53:53.805635: step 349, loss 0.499928.
Train: 2018-08-08T15:53:57.022186: step 350, loss 0.553626.
Test: 2018-08-08T15:54:18.084186: step 350, loss 0.569101.
Train: 2018-08-08T15:54:21.245589: step 351, loss 0.663348.
Train: 2018-08-08T15:54:24.459133: step 352, loss 0.446779.
Train: 2018-08-08T15:54:27.684710: step 353, loss 0.533522.
Train: 2018-08-08T15:54:30.928333: step 354, loss 0.617862.
Train: 2018-08-08T15:54:34.125835: step 355, loss 0.518067.
Train: 2018-08-08T15:54:37.327347: step 356, loss 0.567148.
Train: 2018-08-08T15:54:40.549914: step 357, loss 0.4571.
Train: 2018-08-08T15:54:43.748419: step 358, loss 0.414663.
Train: 2018-08-08T15:54:46.939904: step 359, loss 0.498827.
Train: 2018-08-08T15:54:50.118354: step 360, loss 0.501417.
Test: 2018-08-08T15:55:11.129217: step 360, loss 0.565323.
Train: 2018-08-08T15:55:14.351784: step 361, loss 0.620619.
Train: 2018-08-08T15:55:17.578363: step 362, loss 0.568024.
Train: 2018-08-08T15:55:20.801934: step 363, loss 0.411681.
Train: 2018-08-08T15:55:24.004448: step 364, loss 0.498629.
Train: 2018-08-08T15:55:27.269128: step 365, loss 0.420576.
Train: 2018-08-08T15:55:30.482672: step 366, loss 0.508687.
Train: 2018-08-08T15:55:33.712259: step 367, loss 0.504768.
Train: 2018-08-08T15:55:36.878677: step 368, loss 0.489302.
Train: 2018-08-08T15:55:40.098237: step 369, loss 0.602637.
Train: 2018-08-08T15:55:43.323813: step 370, loss 0.475444.
Test: 2018-08-08T15:56:04.314622: step 370, loss 0.564261.
Train: 2018-08-08T15:56:07.486054: step 371, loss 0.55979.
Train: 2018-08-08T15:56:10.693582: step 372, loss 0.535764.
Train: 2018-08-08T15:56:13.916149: step 373, loss 0.584245.
Train: 2018-08-08T15:56:17.135709: step 374, loss 0.540719.
Train: 2018-08-08T15:56:20.323184: step 375, loss 0.545081.
Train: 2018-08-08T15:56:23.505645: step 376, loss 0.549565.
Train: 2018-08-08T15:56:26.700139: step 377, loss 0.492603.
Train: 2018-08-08T15:56:29.915688: step 378, loss 0.628181.
Train: 2018-08-08T15:56:33.139258: step 379, loss 0.495013.
Train: 2018-08-08T15:56:36.344781: step 380, loss 0.570533.
Test: 2018-08-08T15:56:57.341606: step 380, loss 0.568819.
Train: 2018-08-08T15:57:00.546126: step 381, loss 0.479853.
Train: 2018-08-08T15:57:03.766688: step 382, loss 0.525251.
Train: 2018-08-08T15:57:06.977224: step 383, loss 0.47786.
Train: 2018-08-08T15:57:10.190768: step 384, loss 0.516837.
Train: 2018-08-08T15:57:13.416344: step 385, loss 0.47557.
Train: 2018-08-08T15:57:16.625877: step 386, loss 0.464472.
Train: 2018-08-08T15:57:19.851453: step 387, loss 0.458276.
Train: 2018-08-08T15:57:23.053968: step 388, loss 0.440136.
Train: 2018-08-08T15:57:26.262498: step 389, loss 0.457283.
Train: 2018-08-08T15:57:29.469024: step 390, loss 0.569715.
Test: 2018-08-08T15:57:50.601209: step 390, loss 0.56876.
Train: 2018-08-08T15:57:53.839819: step 391, loss 0.444935.
Train: 2018-08-08T15:57:57.050355: step 392, loss 0.612025.
Train: 2018-08-08T15:58:00.253872: step 393, loss 0.446122.
Train: 2018-08-08T15:58:03.468419: step 394, loss 0.565308.
Train: 2018-08-08T15:58:06.689984: step 395, loss 0.540917.
Train: 2018-08-08T15:58:09.889490: step 396, loss 0.709768.
Train: 2018-08-08T15:58:13.077968: step 397, loss 0.44324.
Train: 2018-08-08T15:58:16.394889: step 398, loss 0.465533.
Train: 2018-08-08T15:58:19.873935: step 399, loss 0.442509.
Train: 2018-08-08T15:58:23.203788: step 400, loss 0.680837.
Test: 2018-08-08T15:58:44.937654: step 400, loss 0.570046.
Train: 2018-08-08T15:58:50.016156: step 401, loss 0.463275.
Train: 2018-08-08T15:58:53.190596: step 402, loss 0.561954.
Train: 2018-08-08T15:58:56.380076: step 403, loss 0.590188.
Train: 2018-08-08T15:58:59.571561: step 404, loss 0.56305.
Train: 2018-08-08T15:59:02.775078: step 405, loss 0.552062.
Train: 2018-08-08T15:59:05.980601: step 406, loss 0.547956.
Train: 2018-08-08T15:59:09.180107: step 407, loss 0.502402.
Train: 2018-08-08T15:59:12.393651: step 408, loss 0.619776.
Train: 2018-08-08T15:59:15.573104: step 409, loss 0.521495.
Train: 2018-08-08T15:59:18.772611: step 410, loss 0.417065.
Test: 2018-08-08T15:59:39.769436: step 410, loss 0.570179.
Train: 2018-08-08T15:59:42.962926: step 411, loss 0.726783.
Train: 2018-08-08T15:59:46.166444: step 412, loss 0.560648.
Train: 2018-08-08T15:59:49.339881: step 413, loss 0.517586.
Train: 2018-08-08T15:59:52.499281: step 414, loss 0.652116.
Train: 2018-08-08T15:59:55.687758: step 415, loss 0.654173.
Train: 2018-08-08T15:59:58.910326: step 416, loss 0.518236.
Train: 2018-08-08T16:00:02.267252: step 417, loss 0.519452.
Train: 2018-08-08T16:00:05.461745: step 418, loss 0.499206.
Train: 2018-08-08T16:00:08.672281: step 419, loss 0.709866.
Train: 2018-08-08T16:00:11.874795: step 420, loss 0.517016.
Test: 2018-08-08T16:00:32.952836: step 420, loss 0.574818.
Train: 2018-08-08T16:00:36.192449: step 421, loss 0.505332.
Train: 2018-08-08T16:00:39.422035: step 422, loss 0.56683.
Train: 2018-08-08T16:00:42.638587: step 423, loss 0.550295.
Train: 2018-08-08T16:00:45.853134: step 424, loss 0.477849.
Train: 2018-08-08T16:00:49.035595: step 425, loss 0.494808.
Train: 2018-08-08T16:00:52.246131: step 426, loss 0.444233.
Train: 2018-08-08T16:00:55.453659: step 427, loss 0.598923.
Train: 2018-08-08T16:00:58.657176: step 428, loss 0.524059.
Train: 2018-08-08T16:01:01.868715: step 429, loss 0.481629.
Train: 2018-08-08T16:01:05.060200: step 430, loss 0.4859.
Test: 2018-08-08T16:01:26.084097: step 430, loss 0.573874.
Train: 2018-08-08T16:01:29.286612: step 431, loss 0.536025.
Train: 2018-08-08T16:01:32.484113: step 432, loss 0.539358.
Train: 2018-08-08T16:01:35.692644: step 433, loss 0.482378.
Train: 2018-08-08T16:01:38.866081: step 434, loss 0.619797.
Train: 2018-08-08T16:01:42.074612: step 435, loss 0.488771.
Train: 2018-08-08T16:01:45.250054: step 436, loss 0.462016.
Train: 2018-08-08T16:01:48.429507: step 437, loss 0.460557.
Train: 2018-08-08T16:01:51.632022: step 438, loss 0.582161.
Train: 2018-08-08T16:01:54.842558: step 439, loss 0.60212.
Train: 2018-08-08T16:01:58.024018: step 440, loss 0.503625.
Test: 2018-08-08T16:02:19.044057: step 440, loss 0.581253.
Train: 2018-08-08T16:02:22.475179: step 441, loss 0.628583.
Train: 2018-08-08T16:02:26.035097: step 442, loss 0.501513.
Train: 2018-08-08T16:02:29.448171: step 443, loss 0.473536.
Train: 2018-08-08T16:02:32.707838: step 444, loss 0.46676.
Train: 2018-08-08T16:02:35.975525: step 445, loss 0.49045.
Train: 2018-08-08T16:02:39.264270: step 446, loss 0.587842.
Train: 2018-08-08T16:02:42.531957: step 447, loss 0.606898.
Train: 2018-08-08T16:02:45.779592: step 448, loss 0.47733.
Train: 2018-08-08T16:02:48.985114: step 449, loss 0.477492.
Train: 2018-08-08T16:02:52.295917: step 450, loss 0.429476.
Test: 2018-08-08T16:03:13.558448: step 450, loss 0.566471.
Train: 2018-08-08T16:03:16.770989: step 451, loss 0.67374.
Train: 2018-08-08T16:03:20.061738: step 452, loss 0.449541.
Train: 2018-08-08T16:03:23.426049: step 453, loss 0.518614.
Train: 2018-08-08T16:03:26.666665: step 454, loss 0.527676.
Train: 2018-08-08T16:03:29.995516: step 455, loss 0.509245.
Train: 2018-08-08T16:03:33.323363: step 456, loss 0.584278.
Train: 2018-08-08T16:03:36.474742: step 457, loss 0.530435.
Train: 2018-08-08T16:03:39.736414: step 458, loss 0.507782.
Train: 2018-08-08T16:03:42.884785: step 459, loss 0.651223.
Train: 2018-08-08T16:03:46.091310: step 460, loss 0.455422.
Test: 2018-08-08T16:04:07.182386: step 460, loss 0.570725.
Train: 2018-08-08T16:04:10.369860: step 461, loss 0.48897.
Train: 2018-08-08T16:04:13.596439: step 462, loss 0.485673.
Train: 2018-08-08T16:04:16.757844: step 463, loss 0.522723.
Train: 2018-08-08T16:04:19.953340: step 464, loss 0.487822.
Train: 2018-08-08T16:04:23.136804: step 465, loss 0.533322.
Train: 2018-08-08T16:04:26.322273: step 466, loss 0.525083.
Train: 2018-08-08T16:04:29.498718: step 467, loss 0.69501.
Train: 2018-08-08T16:04:32.740337: step 468, loss 0.444849.
Train: 2018-08-08T16:04:35.931822: step 469, loss 0.58782.
Train: 2018-08-08T16:04:39.140353: step 470, loss 0.605213.
Test: 2018-08-08T16:05:00.216388: step 470, loss 0.572061.
Train: 2018-08-08T16:05:03.384812: step 471, loss 0.591182.
Train: 2018-08-08T16:05:06.583316: step 472, loss 0.55541.
Train: 2018-08-08T16:05:09.760764: step 473, loss 0.565415.
Train: 2018-08-08T16:05:12.916153: step 474, loss 0.586445.
Train: 2018-08-08T16:05:16.163788: step 475, loss 0.521852.
Train: 2018-08-08T16:05:19.353268: step 476, loss 0.539402.
Train: 2018-08-08T16:05:22.578844: step 477, loss 0.455883.
Train: 2018-08-08T16:05:25.742254: step 478, loss 0.532796.
Train: 2018-08-08T16:05:28.924715: step 479, loss 0.466555.
Train: 2018-08-08T16:05:32.180371: step 480, loss 0.642426.
Test: 2018-08-08T16:05:53.260418: step 480, loss 0.566207.
Train: 2018-08-08T16:05:56.474964: step 481, loss 0.559531.
Train: 2018-08-08T16:05:59.657425: step 482, loss 0.488868.
Train: 2018-08-08T16:06:02.842894: step 483, loss 0.666182.
Train: 2018-08-08T16:06:06.008311: step 484, loss 0.617068.
Train: 2018-08-08T16:06:09.197791: step 485, loss 0.584912.
Train: 2018-08-08T16:06:12.418353: step 486, loss 0.615353.
Train: 2018-08-08T16:06:15.594799: step 487, loss 0.53196.
Train: 2018-08-08T16:06:18.799318: step 488, loss 0.534553.
Train: 2018-08-08T16:06:21.955710: step 489, loss 0.632009.
Train: 2018-08-08T16:06:25.119121: step 490, loss 0.557637.
Test: 2018-08-08T16:06:46.118954: step 490, loss 0.567919.
Train: 2018-08-08T16:06:49.289383: step 491, loss 0.517125.
Train: 2018-08-08T16:06:52.483876: step 492, loss 0.55733.
Train: 2018-08-08T16:06:55.679372: step 493, loss 0.55714.
Train: 2018-08-08T16:06:58.876874: step 494, loss 0.575354.
Train: 2018-08-08T16:07:02.087409: step 495, loss 0.603702.
Train: 2018-08-08T16:07:05.256836: step 496, loss 0.575635.
Train: 2018-08-08T16:07:08.438295: step 497, loss 0.591546.
Train: 2018-08-08T16:07:11.634794: step 498, loss 0.647747.
Train: 2018-08-08T16:07:14.821265: step 499, loss 0.503526.
Train: 2018-08-08T16:07:17.980665: step 500, loss 0.564613.
Test: 2018-08-08T16:07:38.990526: step 500, loss 0.565883.
Train: 2018-08-08T16:07:43.988814: step 501, loss 0.596125.
Train: 2018-08-08T16:07:47.194336: step 502, loss 0.550632.
Train: 2018-08-08T16:07:50.358749: step 503, loss 0.581097.
Train: 2018-08-08T16:07:53.549232: step 504, loss 0.587448.
Train: 2018-08-08T16:07:56.720664: step 505, loss 0.528223.
Train: 2018-08-08T16:07:59.906133: step 506, loss 0.572874.
Train: 2018-08-08T16:08:03.097619: step 507, loss 0.584013.
Train: 2018-08-08T16:08:06.283088: step 508, loss 0.510018.
Train: 2018-08-08T16:08:09.467555: step 509, loss 0.545493.
Train: 2018-08-08T16:08:12.644000: step 510, loss 0.553832.
Test: 2018-08-08T16:08:33.583673: step 510, loss 0.564404.
Train: 2018-08-08T16:08:36.802230: step 511, loss 0.544776.
Train: 2018-08-08T16:08:40.017779: step 512, loss 0.545926.
Train: 2018-08-08T16:08:43.232326: step 513, loss 0.56455.
Train: 2018-08-08T16:08:46.447875: step 514, loss 0.514702.
Train: 2018-08-08T16:08:49.680470: step 515, loss 0.560756.
Train: 2018-08-08T16:08:52.905043: step 516, loss 0.462566.
Train: 2018-08-08T16:08:56.097531: step 517, loss 0.526517.
Train: 2018-08-08T16:08:59.289016: step 518, loss 0.625988.
Train: 2018-08-08T16:09:02.484512: step 519, loss 0.509032.
Train: 2018-08-08T16:09:05.648925: step 520, loss 0.492648.
Test: 2018-08-08T16:09:26.801163: step 520, loss 0.5649.
Train: 2018-08-08T16:09:29.968585: step 521, loss 0.439931.
Train: 2018-08-08T16:09:33.287408: step 522, loss 0.455689.
Train: 2018-08-08T16:09:36.526019: step 523, loss 0.69603.
Train: 2018-08-08T16:09:39.783680: step 524, loss 0.739406.
Train: 2018-08-08T16:09:42.989203: step 525, loss 0.536571.
Train: 2018-08-08T16:09:46.197733: step 526, loss 0.637871.
Train: 2018-08-08T16:09:49.438349: step 527, loss 0.559508.
Train: 2018-08-08T16:09:52.609781: step 528, loss 0.528897.
Train: 2018-08-08T16:09:55.778205: step 529, loss 0.526768.
Train: 2018-08-08T16:09:58.978715: step 530, loss 0.607279.
Test: 2018-08-08T16:10:20.113909: step 530, loss 0.570269.
Train: 2018-08-08T16:10:23.271302: step 531, loss 0.533.
Train: 2018-08-08T16:10:26.486851: step 532, loss 0.512524.
Train: 2018-08-08T16:10:29.684352: step 533, loss 0.462617.
Train: 2018-08-08T16:10:32.906920: step 534, loss 0.543945.
Train: 2018-08-08T16:10:36.127483: step 535, loss 0.551993.
Train: 2018-08-08T16:10:39.346040: step 536, loss 0.515654.
Train: 2018-08-08T16:10:42.515467: step 537, loss 0.539697.
Train: 2018-08-08T16:10:45.744051: step 538, loss 0.443354.
Train: 2018-08-08T16:10:48.930522: step 539, loss 0.589272.
Train: 2018-08-08T16:10:52.117997: step 540, loss 0.713091.
Test: 2018-08-08T16:11:13.150918: step 540, loss 0.56202.
Train: 2018-08-08T16:11:16.452696: step 541, loss 0.578416.
Train: 2018-08-08T16:11:19.657216: step 542, loss 0.554848.
Train: 2018-08-08T16:11:22.854718: step 543, loss 0.544028.
Train: 2018-08-08T16:11:26.014118: step 544, loss 0.510902.
Train: 2018-08-08T16:11:29.219640: step 545, loss 0.507006.
Train: 2018-08-08T16:11:32.431179: step 546, loss 0.520734.
Train: 2018-08-08T16:11:35.653747: step 547, loss 0.557706.
Train: 2018-08-08T16:11:38.866288: step 548, loss 0.661906.
Train: 2018-08-08T16:11:42.040728: step 549, loss 0.621354.
Train: 2018-08-08T16:11:45.218176: step 550, loss 0.574568.
Test: 2018-08-08T16:12:06.180910: step 550, loss 0.564949.
Train: 2018-08-08T16:12:09.364374: step 551, loss 0.570753.
Train: 2018-08-08T16:12:12.626046: step 552, loss 0.621006.
Train: 2018-08-08T16:12:15.829563: step 553, loss 0.514199.
Train: 2018-08-08T16:12:19.029069: step 554, loss 0.540433.
Train: 2018-08-08T16:12:22.220555: step 555, loss 0.649956.
Train: 2018-08-08T16:12:25.416051: step 556, loss 0.521556.
Train: 2018-08-08T16:12:28.636614: step 557, loss 0.580848.
Train: 2018-08-08T16:12:31.860184: step 558, loss 0.624768.
Train: 2018-08-08T16:12:35.066709: step 559, loss 0.616686.
Train: 2018-08-08T16:12:38.259197: step 560, loss 0.491616.
Test: 2018-08-08T16:12:59.253015: step 560, loss 0.569253.
Train: 2018-08-08T16:13:02.442494: step 561, loss 0.552467.
Train: 2018-08-08T16:13:05.642000: step 562, loss 0.552059.
Train: 2018-08-08T16:13:08.834488: step 563, loss 0.530868.
Train: 2018-08-08T16:13:12.003915: step 564, loss 0.661516.
Train: 2018-08-08T16:13:15.205427: step 565, loss 0.569124.
Train: 2018-08-08T16:13:18.412955: step 566, loss 0.522724.
Train: 2018-08-08T16:13:21.625496: step 567, loss 0.534416.
Train: 2018-08-08T16:13:24.832022: step 568, loss 0.510192.
Train: 2018-08-08T16:13:28.019496: step 569, loss 0.646203.
Train: 2018-08-08T16:13:31.228027: step 570, loss 0.571903.
Test: 2018-08-08T16:13:52.258943: step 570, loss 0.560146.
Train: 2018-08-08T16:13:55.461457: step 571, loss 0.64731.
Train: 2018-08-08T16:13:58.681017: step 572, loss 0.555574.
Train: 2018-08-08T16:14:01.901579: step 573, loss 0.608915.
Train: 2018-08-08T16:14:05.162248: step 574, loss 0.561206.
Train: 2018-08-08T16:14:08.329670: step 575, loss 0.598165.
Train: 2018-08-08T16:14:11.511128: step 576, loss 0.499405.
Train: 2018-08-08T16:14:14.732694: step 577, loss 0.506161.
Train: 2018-08-08T16:14:17.924179: step 578, loss 0.591325.
Train: 2018-08-08T16:14:21.141734: step 579, loss 0.630419.
Train: 2018-08-08T16:14:24.334222: step 580, loss 0.427311.
Test: 2018-08-08T16:14:45.375164: step 580, loss 0.563275.
Train: 2018-08-08T16:14:48.568654: step 581, loss 0.628447.
Train: 2018-08-08T16:14:51.757131: step 582, loss 0.526522.
Train: 2018-08-08T16:14:54.921544: step 583, loss 0.539856.
Train: 2018-08-08T16:14:58.131078: step 584, loss 0.534949.
Train: 2018-08-08T16:15:01.332590: step 585, loss 0.509324.
Train: 2018-08-08T16:15:04.521067: step 586, loss 0.663257.
Train: 2018-08-08T16:15:07.714558: step 587, loss 0.565362.
Train: 2018-08-08T16:15:10.868944: step 588, loss 0.472356.
Train: 2018-08-08T16:15:14.060429: step 589, loss 0.529605.
Train: 2018-08-08T16:15:17.252918: step 590, loss 0.468703.
Test: 2018-08-08T16:15:38.319929: step 590, loss 0.566519.
Train: 2018-08-08T16:15:41.721974: step 591, loss 0.499285.
Train: 2018-08-08T16:15:44.899422: step 592, loss 0.484403.
Train: 2018-08-08T16:15:48.087899: step 593, loss 0.617748.
Train: 2018-08-08T16:15:51.271363: step 594, loss 0.578862.
Train: 2018-08-08T16:15:54.439787: step 595, loss 0.533652.
Train: 2018-08-08T16:15:57.634280: step 596, loss 0.562779.
Train: 2018-08-08T16:16:00.815739: step 597, loss 0.497279.
Train: 2018-08-08T16:16:04.004217: step 598, loss 0.553464.
Train: 2018-08-08T16:16:07.199712: step 599, loss 0.516776.
Train: 2018-08-08T16:16:10.438323: step 600, loss 0.474289.
Test: 2018-08-08T16:16:31.453196: step 600, loss 0.571873.
Train: 2018-08-08T16:16:36.478557: step 601, loss 0.588057.
Train: 2018-08-08T16:16:39.521647: step 602, loss 0.504255.
Train: 2018-08-08T16:16:42.889602: step 603, loss 0.361924.
Train: 2018-08-08T16:16:46.093119: step 604, loss 0.432304.
Train: 2018-08-08T16:16:49.273575: step 605, loss 0.467041.
Train: 2018-08-08T16:16:52.465060: step 606, loss 0.491267.
Train: 2018-08-08T16:16:55.649527: step 607, loss 0.612261.
Train: 2018-08-08T16:16:58.859060: step 608, loss 0.524079.
Train: 2018-08-08T16:17:02.038513: step 609, loss 0.477832.
Train: 2018-08-08T16:17:05.224986: step 610, loss 0.479685.
Test: 2018-08-08T16:17:26.185715: step 610, loss 0.573071.
Train: 2018-08-08T16:17:29.362160: step 611, loss 0.445413.
Train: 2018-08-08T16:17:32.571693: step 612, loss 0.572332.
Train: 2018-08-08T16:17:35.736106: step 613, loss 0.528332.
Train: 2018-08-08T16:17:38.923580: step 614, loss 0.521429.
Train: 2018-08-08T16:17:42.075962: step 615, loss 0.597713.
Train: 2018-08-08T16:17:45.280482: step 616, loss 0.517532.
Train: 2018-08-08T16:17:48.483999: step 617, loss 0.559848.
Train: 2018-08-08T16:17:51.663452: step 618, loss 0.598608.
Train: 2018-08-08T16:17:54.860954: step 619, loss 0.537523.
Train: 2018-08-08T16:17:58.055447: step 620, loss 0.502289.
Test: 2018-08-08T16:18:19.130480: step 620, loss 0.572631.
Train: 2018-08-08T16:18:22.345026: step 621, loss 0.611614.
Train: 2018-08-08T16:18:25.543530: step 622, loss 0.602472.
Train: 2018-08-08T16:18:28.723986: step 623, loss 0.585477.
Train: 2018-08-08T16:18:32.026767: step 624, loss 0.626558.
Train: 2018-08-08T16:18:35.399735: step 625, loss 0.495185.
Train: 2018-08-08T16:18:38.662410: step 626, loss 0.569071.
Train: 2018-08-08T16:18:41.869938: step 627, loss 0.63704.
Train: 2018-08-08T16:18:45.061423: step 628, loss 0.494246.
Train: 2018-08-08T16:18:48.244887: step 629, loss 0.585618.
Train: 2018-08-08T16:18:51.422335: step 630, loss 0.447095.
Test: 2018-08-08T16:19:12.445229: step 630, loss 0.569927.
Train: 2018-08-08T16:19:15.715925: step 631, loss 0.487007.
Train: 2018-08-08T16:19:18.886354: step 632, loss 0.664271.
Train: 2018-08-08T16:19:22.106916: step 633, loss 0.53193.
Train: 2018-08-08T16:19:25.297400: step 634, loss 0.541309.
Train: 2018-08-08T16:19:28.500916: step 635, loss 0.456911.
Train: 2018-08-08T16:19:31.693405: step 636, loss 0.549282.
Train: 2018-08-08T16:19:34.892911: step 637, loss 0.593119.
Train: 2018-08-08T16:19:38.091415: step 638, loss 0.467826.
Train: 2018-08-08T16:19:41.310975: step 639, loss 0.500571.
Train: 2018-08-08T16:19:44.505468: step 640, loss 0.656994.
Test: 2018-08-08T16:20:05.626624: step 640, loss 0.56369.
Train: 2018-08-08T16:20:08.839165: step 641, loss 0.528882.
Train: 2018-08-08T16:20:12.057722: step 642, loss 0.643526.
Train: 2018-08-08T16:20:15.246199: step 643, loss 0.599034.
Train: 2018-08-08T16:20:18.422645: step 644, loss 0.544138.
Train: 2018-08-08T16:20:21.608114: step 645, loss 0.534841.
Train: 2018-08-08T16:20:24.816644: step 646, loss 0.561317.
Train: 2018-08-08T16:20:28.039212: step 647, loss 0.534434.
Train: 2018-08-08T16:20:31.295871: step 648, loss 0.614372.
Train: 2018-08-08T16:20:34.529468: step 649, loss 0.526698.
Train: 2018-08-08T16:20:37.741007: step 650, loss 0.552761.
Test: 2018-08-08T16:20:58.778942: step 650, loss 0.571254.
Train: 2018-08-08T16:21:01.966416: step 651, loss 0.461415.
Train: 2018-08-08T16:21:05.190989: step 652, loss 0.591911.
Train: 2018-08-08T16:21:08.407541: step 653, loss 0.525519.
Train: 2018-08-08T16:21:11.612061: step 654, loss 0.610556.
Train: 2018-08-08T16:21:14.811567: step 655, loss 0.506214.
Train: 2018-08-08T16:21:17.968962: step 656, loss 0.50889.
Train: 2018-08-08T16:21:21.175487: step 657, loss 0.628483.
Train: 2018-08-08T16:21:24.357948: step 658, loss 0.538866.
Train: 2018-08-08T16:21:27.549433: step 659, loss 0.546329.
Train: 2018-08-08T16:21:30.723873: step 660, loss 0.563285.
Test: 2018-08-08T16:21:51.730726: step 660, loss 0.568618.
Train: 2018-08-08T16:21:54.966328: step 661, loss 0.592219.
Train: 2018-08-08T16:21:58.145781: step 662, loss 0.542074.
Train: 2018-08-08T16:22:01.295154: step 663, loss 0.484145.
Train: 2018-08-08T16:22:04.456559: step 664, loss 0.628295.
Train: 2018-08-08T16:22:07.650050: step 665, loss 0.539476.
Train: 2018-08-08T16:22:10.867605: step 666, loss 0.519382.
Train: 2018-08-08T16:22:14.068114: step 667, loss 0.44727.
Train: 2018-08-08T16:22:17.283663: step 668, loss 0.471219.
Train: 2018-08-08T16:22:20.477154: step 669, loss 0.461948.
Train: 2018-08-08T16:22:23.691700: step 670, loss 0.583353.
Test: 2018-08-08T16:22:44.695544: step 670, loss 0.562291.
Train: 2018-08-08T16:22:47.902069: step 671, loss 0.526241.
Train: 2018-08-08T16:22:51.118621: step 672, loss 0.650519.
Train: 2018-08-08T16:22:54.297072: step 673, loss 0.535287.
Train: 2018-08-08T16:22:57.450455: step 674, loss 0.504724.
Train: 2018-08-08T16:23:00.639935: step 675, loss 0.521017.
Train: 2018-08-08T16:23:03.867517: step 676, loss 0.55828.
Train: 2018-08-08T16:23:07.080058: step 677, loss 0.569129.
Train: 2018-08-08T16:23:10.432973: step 678, loss 0.566448.
Train: 2018-08-08T16:23:13.617439: step 679, loss 0.47928.
Train: 2018-08-08T16:23:16.755783: step 680, loss 0.44302.
Test: 2018-08-08T16:23:37.884960: step 680, loss 0.567366.
Train: 2018-08-08T16:23:41.082461: step 681, loss 0.551123.
Train: 2018-08-08T16:23:44.285978: step 682, loss 0.598046.
Train: 2018-08-08T16:23:47.485485: step 683, loss 0.577614.
Train: 2018-08-08T16:23:50.693013: step 684, loss 0.598429.
Train: 2018-08-08T16:23:53.896530: step 685, loss 0.538433.
Train: 2018-08-08T16:23:57.061946: step 686, loss 0.557936.
Train: 2018-08-08T16:24:00.243405: step 687, loss 0.53225.
Train: 2018-08-08T16:24:03.518111: step 688, loss 0.497868.
Train: 2018-08-08T16:24:06.732658: step 689, loss 0.37824.
Train: 2018-08-08T16:24:09.913114: step 690, loss 0.647643.
Test: 2018-08-08T16:24:30.958067: step 690, loss 0.550279.
Train: 2018-08-08T16:24:34.191664: step 691, loss 0.521119.
Train: 2018-08-08T16:24:37.437293: step 692, loss 0.558311.
Train: 2018-08-08T16:24:40.661866: step 693, loss 0.512399.
Train: 2018-08-08T16:24:43.899474: step 694, loss 0.541218.
Train: 2018-08-08T16:24:47.095972: step 695, loss 0.520514.
Train: 2018-08-08T16:24:50.319543: step 696, loss 0.507771.
Train: 2018-08-08T16:24:53.507018: step 697, loss 0.567429.
Train: 2018-08-08T16:24:56.700508: step 698, loss 0.413437.
Train: 2018-08-08T16:24:59.898010: step 699, loss 0.583157.
Train: 2018-08-08T16:25:03.095511: step 700, loss 0.621347.
Test: 2018-08-08T16:25:24.081307: step 700, loss 0.571238.
Train: 2018-08-08T16:25:29.121708: step 701, loss 0.538345.
Train: 2018-08-08T16:25:32.309182: step 702, loss 0.427078.
Train: 2018-08-08T16:25:35.501670: step 703, loss 0.559799.
Train: 2018-08-08T16:25:38.677112: step 704, loss 0.61267.
Train: 2018-08-08T16:25:41.878624: step 705, loss 0.493633.
Train: 2018-08-08T16:25:45.050056: step 706, loss 0.606345.
Train: 2018-08-08T16:25:48.227504: step 707, loss 0.550729.
Train: 2018-08-08T16:25:51.430019: step 708, loss 0.504739.
Train: 2018-08-08T16:25:54.624512: step 709, loss 0.451801.
Train: 2018-08-08T16:25:57.824019: step 710, loss 0.477125.
Test: 2018-08-08T16:26:18.755670: step 710, loss 0.564561.
Train: 2018-08-08T16:26:21.938132: step 711, loss 0.529228.
Train: 2018-08-08T16:26:25.325138: step 712, loss 0.581902.
Train: 2018-08-08T16:26:28.632931: step 713, loss 0.526947.
Train: 2018-08-08T16:26:31.818401: step 714, loss 0.630031.
Train: 2018-08-08T16:26:35.036958: step 715, loss 0.510727.
Train: 2018-08-08T16:26:38.341744: step 716, loss 0.57951.
Train: 2018-08-08T16:26:41.559299: step 717, loss 0.594132.
Train: 2018-08-08T16:26:44.741760: step 718, loss 0.645519.
Train: 2018-08-08T16:26:47.927229: step 719, loss 0.473999.
Train: 2018-08-08T16:26:51.097659: step 720, loss 0.571166.
Test: 2018-08-08T16:27:12.083456: step 720, loss 0.575103.
Train: 2018-08-08T16:27:15.315046: step 721, loss 0.570694.
Train: 2018-08-08T16:27:18.480462: step 722, loss 0.677359.
Train: 2018-08-08T16:27:21.681974: step 723, loss 0.560542.
Train: 2018-08-08T16:27:24.886494: step 724, loss 0.53661.
Train: 2018-08-08T16:27:28.086000: step 725, loss 0.599897.
Train: 2018-08-08T16:27:31.270467: step 726, loss 0.501507.
Train: 2018-08-08T16:27:34.458945: step 727, loss 0.587955.
Train: 2018-08-08T16:27:37.653438: step 728, loss 0.531032.
Train: 2018-08-08T16:27:40.826875: step 729, loss 0.557485.
Train: 2018-08-08T16:27:44.023374: step 730, loss 0.488202.
Test: 2018-08-08T16:28:04.996135: step 730, loss 0.570044.
Train: 2018-08-08T16:28:08.147513: step 731, loss 0.479007.
Train: 2018-08-08T16:28:11.350028: step 732, loss 0.650195.
Train: 2018-08-08T16:28:14.547529: step 733, loss 0.433877.
Train: 2018-08-08T16:28:17.722971: step 734, loss 0.705918.
Train: 2018-08-08T16:28:20.926489: step 735, loss 0.55448.
Train: 2018-08-08T16:28:24.133014: step 736, loss 0.633926.
Train: 2018-08-08T16:28:27.341545: step 737, loss 0.602791.
Train: 2018-08-08T16:28:30.488913: step 738, loss 0.623189.
Train: 2018-08-08T16:28:33.687417: step 739, loss 0.577515.
Train: 2018-08-08T16:28:36.879904: step 740, loss 0.556232.
Test: 2018-08-08T16:28:57.854671: step 740, loss 0.58069.
Train: 2018-08-08T16:29:01.061196: step 741, loss 0.544014.
Train: 2018-08-08T16:29:04.230622: step 742, loss 0.580061.
Train: 2018-08-08T16:29:07.441158: step 743, loss 0.539881.
Train: 2018-08-08T16:29:10.657710: step 744, loss 0.503955.
Train: 2018-08-08T16:29:13.861227: step 745, loss 0.422099.
Train: 2018-08-08T16:29:17.066750: step 746, loss 0.645707.
Train: 2018-08-08T16:29:20.248209: step 747, loss 0.374848.
Train: 2018-08-08T16:29:23.463750: step 748, loss 0.509536.
Train: 2018-08-08T16:29:26.630169: step 749, loss 0.565494.
Train: 2018-08-08T16:29:29.807617: step 750, loss 0.699817.
Test: 2018-08-08T16:29:50.776367: step 750, loss 0.580886.
Train: 2018-08-08T16:29:53.989911: step 751, loss 0.506196.
Train: 2018-08-08T16:29:57.213481: step 752, loss 0.505364.
Train: 2018-08-08T16:30:00.463121: step 753, loss 0.541145.
Train: 2018-08-08T16:30:03.745849: step 754, loss 0.578379.
Train: 2018-08-08T16:30:06.923297: step 755, loss 0.487316.
Train: 2018-08-08T16:30:10.120798: step 756, loss 0.602077.
Train: 2018-08-08T16:30:13.316294: step 757, loss 0.547459.
Train: 2018-08-08T16:30:16.483716: step 758, loss 0.498962.
Train: 2018-08-08T16:30:19.652140: step 759, loss 0.49623.
Train: 2018-08-08T16:30:22.821566: step 760, loss 0.412449.
Test: 2018-08-08T16:30:43.922668: step 760, loss 0.57064.
Train: 2018-08-08T16:30:47.111145: step 761, loss 0.638031.
Train: 2018-08-08T16:30:50.349756: step 762, loss 0.618208.
Train: 2018-08-08T16:30:53.593380: step 763, loss 0.642732.
Train: 2018-08-08T16:30:56.798902: step 764, loss 0.511774.
Train: 2018-08-08T16:30:59.975348: step 765, loss 0.537741.
Train: 2018-08-08T16:31:03.163825: step 766, loss 0.566596.
Train: 2018-08-08T16:31:06.344281: step 767, loss 0.523809.
Train: 2018-08-08T16:31:09.515713: step 768, loss 0.570575.
Train: 2018-08-08T16:31:12.704190: step 769, loss 0.507106.
Train: 2018-08-08T16:31:15.901692: step 770, loss 0.497318.
Test: 2018-08-08T16:31:36.919573: step 770, loss 0.569212.
Train: 2018-08-08T16:31:40.120082: step 771, loss 0.419059.
Train: 2018-08-08T16:31:43.303546: step 772, loss 0.50444.
Train: 2018-08-08T16:31:46.469964: step 773, loss 0.530268.
Train: 2018-08-08T16:31:49.643401: step 774, loss 0.549403.
Train: 2018-08-08T16:31:52.835889: step 775, loss 0.609427.
Train: 2018-08-08T16:31:55.997295: step 776, loss 0.518638.
Train: 2018-08-08T16:31:59.230892: step 777, loss 0.488162.
Train: 2018-08-08T16:32:02.428393: step 778, loss 0.680337.
Train: 2018-08-08T16:32:05.621884: step 779, loss 0.481083.
Train: 2018-08-08T16:32:08.811364: step 780, loss 0.558195.
Test: 2018-08-08T16:32:29.848296: step 780, loss 0.564052.
Train: 2018-08-08T16:32:33.140047: step 781, loss 0.530012.
Train: 2018-08-08T16:32:36.470903: step 782, loss 0.573164.
Train: 2018-08-08T16:32:39.664394: step 783, loss 0.542654.
Train: 2018-08-08T16:32:42.855879: step 784, loss 0.585354.
Train: 2018-08-08T16:32:46.015279: step 785, loss 0.614105.
Train: 2018-08-08T16:32:49.220801: step 786, loss 0.539158.
Train: 2018-08-08T16:32:52.419305: step 787, loss 0.592512.
Train: 2018-08-08T16:32:55.583718: step 788, loss 0.579457.
Train: 2018-08-08T16:32:58.778212: step 789, loss 0.514443.
Train: 2018-08-08T16:33:01.930593: step 790, loss 0.451482.
Test: 2018-08-08T16:33:22.984571: step 790, loss 0.57055.
Train: 2018-08-08T16:33:26.197111: step 791, loss 0.515477.
Train: 2018-08-08T16:33:29.410655: step 792, loss 0.513953.
Train: 2018-08-08T16:33:32.634226: step 793, loss 0.508914.
Train: 2018-08-08T16:33:35.836740: step 794, loss 0.717641.
Train: 2018-08-08T16:33:39.076353: step 795, loss 0.561414.
Train: 2018-08-08T16:33:42.256809: step 796, loss 0.624319.
Train: 2018-08-08T16:33:45.556583: step 797, loss 0.568656.
Train: 2018-08-08T16:33:48.760100: step 798, loss 0.569854.
Train: 2018-08-08T16:33:51.940556: step 799, loss 0.577442.
Train: 2018-08-08T16:33:55.140062: step 800, loss 0.571243.
Test: 2018-08-08T16:34:16.220109: step 800, loss 0.574775.
Train: 2018-08-08T16:34:21.130163: step 801, loss 0.567674.
Train: 2018-08-08T16:34:24.357744: step 802, loss 0.503386.
Train: 2018-08-08T16:34:27.549229: step 803, loss 0.595054.
Train: 2018-08-08T16:34:30.724672: step 804, loss 0.589552.
Train: 2018-08-08T16:34:33.911144: step 805, loss 0.521697.
Train: 2018-08-08T16:34:37.108645: step 806, loss 0.627456.
Train: 2018-08-08T16:34:40.305747: step 807, loss 0.502665.
Train: 2018-08-08T16:34:43.487206: step 808, loss 0.603003.
Train: 2018-08-08T16:34:46.669667: step 809, loss 0.582372.
Train: 2018-08-08T16:34:49.846112: step 810, loss 0.491099.
Test: 2018-08-08T16:35:10.869007: step 810, loss 0.569818.
Train: 2018-08-08T16:35:14.120652: step 811, loss 0.479071.
Train: 2018-08-08T16:35:17.295092: step 812, loss 0.544689.
Train: 2018-08-08T16:35:20.491590: step 813, loss 0.638926.
Train: 2018-08-08T16:35:23.665028: step 814, loss 0.471821.
Train: 2018-08-08T16:35:26.889601: step 815, loss 0.432697.
Train: 2018-08-08T16:35:30.058025: step 816, loss 0.474314.
Train: 2018-08-08T16:35:33.238481: step 817, loss 0.690301.
Train: 2018-08-08T16:35:36.417934: step 818, loss 0.581461.
Train: 2018-08-08T16:35:39.587360: step 819, loss 0.670833.
Train: 2018-08-08T16:35:42.774835: step 820, loss 0.505996.
Test: 2018-08-08T16:36:03.817783: step 820, loss 0.565666.
Train: 2018-08-08T16:36:07.003252: step 821, loss 0.619517.
Train: 2018-08-08T16:36:10.200753: step 822, loss 0.563474.
Train: 2018-08-08T16:36:13.397252: step 823, loss 0.600224.
Train: 2018-08-08T16:36:16.578710: step 824, loss 0.575276.
Train: 2018-08-08T16:36:19.760169: step 825, loss 0.551643.
Train: 2018-08-08T16:36:22.917563: step 826, loss 0.568359.
Train: 2018-08-08T16:36:26.133113: step 827, loss 0.506683.
Train: 2018-08-08T16:36:29.325601: step 828, loss 0.552446.
Train: 2018-08-08T16:36:32.514078: step 829, loss 0.519517.
Train: 2018-08-08T16:36:35.712582: step 830, loss 0.530111.
Test: 2018-08-08T16:36:56.970104: step 830, loss 0.566096.
Train: 2018-08-08T16:37:00.384177: step 831, loss 0.532586.
Train: 2018-08-08T16:37:03.804270: step 832, loss 0.556936.
Train: 2018-08-08T16:37:07.206315: step 833, loss 0.551338.
Train: 2018-08-08T16:37:10.576275: step 834, loss 0.568153.
Train: 2018-08-08T16:37:14.055525: step 835, loss 0.68781.
Train: 2018-08-08T16:37:17.418466: step 836, loss 0.539949.
Train: 2018-08-08T16:37:20.845578: step 837, loss 0.60935.
Train: 2018-08-08T16:37:24.218546: step 838, loss 0.473392.
Train: 2018-08-08T16:37:27.485231: step 839, loss 0.579921.
Train: 2018-08-08T16:37:30.679725: step 840, loss 0.456136.
Test: 2018-08-08T16:37:51.755760: step 840, loss 0.566304.
Train: 2018-08-08T16:37:54.960280: step 841, loss 0.527836.
Train: 2018-08-08T16:37:58.145749: step 842, loss 0.588123.
Train: 2018-08-08T16:38:01.347262: step 843, loss 0.489244.
Train: 2018-08-08T16:38:04.539749: step 844, loss 0.554342.
Train: 2018-08-08T16:38:07.699149: step 845, loss 0.456473.
Train: 2018-08-08T16:38:10.889631: step 846, loss 0.577028.
Train: 2018-08-08T16:38:14.073096: step 847, loss 0.501603.
Train: 2018-08-08T16:38:17.254554: step 848, loss 0.489725.
Train: 2018-08-08T16:38:20.426988: step 849, loss 0.534723.
Train: 2018-08-08T16:38:23.625492: step 850, loss 0.486804.
Test: 2018-08-08T16:38:44.726598: step 850, loss 0.567645.
Train: 2018-08-08T16:38:47.903040: step 851, loss 0.445229.
Train: 2018-08-08T16:38:51.092520: step 852, loss 0.453102.
Train: 2018-08-08T16:38:54.284005: step 853, loss 0.503273.
Train: 2018-08-08T16:38:57.444408: step 854, loss 0.505998.
Train: 2018-08-08T16:39:00.621855: step 855, loss 0.436664.
Train: 2018-08-08T16:39:03.799304: step 856, loss 0.593021.
Train: 2018-08-08T16:39:06.991792: step 857, loss 0.483879.
Train: 2018-08-08T16:39:10.162221: step 858, loss 0.474411.
Train: 2018-08-08T16:39:13.347690: step 859, loss 0.481392.
Train: 2018-08-08T16:39:16.537170: step 860, loss 0.605394.
Test: 2018-08-08T16:39:37.556054: step 860, loss 0.568769.
Train: 2018-08-08T16:39:40.718461: step 861, loss 0.701641.
Train: 2018-08-08T16:39:43.877861: step 862, loss 0.453499.
Train: 2018-08-08T16:39:47.224760: step 863, loss 0.588927.
Train: 2018-08-08T16:39:50.485429: step 864, loss 0.607769.
Train: 2018-08-08T16:39:53.750109: step 865, loss 0.576383.
Train: 2018-08-08T16:39:57.087198: step 866, loss 0.528125.
Train: 2018-08-08T16:40:00.267656: step 867, loss 0.551813.
Train: 2018-08-08T16:40:03.541357: step 868, loss 0.49633.
Train: 2018-08-08T16:40:06.717803: step 869, loss 0.535079.
Train: 2018-08-08T16:40:09.893245: step 870, loss 0.549591.
Test: 2018-08-08T16:40:30.931180: step 870, loss 0.57243.
Train: 2018-08-08T16:40:34.150739: step 871, loss 0.540283.
Train: 2018-08-08T16:40:37.370299: step 872, loss 0.614838.
Train: 2018-08-08T16:40:40.556771: step 873, loss 0.544937.
Train: 2018-08-08T16:40:43.762294: step 874, loss 0.61024.
Train: 2018-08-08T16:40:46.972830: step 875, loss 0.49613.
Train: 2018-08-08T16:40:50.159301: step 876, loss 0.428525.
Train: 2018-08-08T16:40:53.336750: step 877, loss 0.537902.
Train: 2018-08-08T16:40:56.496150: step 878, loss 0.584993.
Train: 2018-08-08T16:40:59.652542: step 879, loss 0.609203.
Train: 2018-08-08T16:41:02.832998: step 880, loss 0.597208.
Test: 2018-08-08T16:41:23.811775: step 880, loss 0.578706.
Train: 2018-08-08T16:41:27.009276: step 881, loss 0.555049.
Train: 2018-08-08T16:41:30.159652: step 882, loss 0.562896.
Train: 2018-08-08T16:41:33.327074: step 883, loss 0.471666.
Train: 2018-08-08T16:41:36.505523: step 884, loss 0.492135.
Train: 2018-08-08T16:41:39.691995: step 885, loss 0.502681.
Train: 2018-08-08T16:41:42.886489: step 886, loss 0.605233.
Train: 2018-08-08T16:41:46.069953: step 887, loss 0.421367.
Train: 2018-08-08T16:41:49.225342: step 888, loss 0.503242.
Train: 2018-08-08T16:41:52.474982: step 889, loss 0.521964.
Train: 2018-08-08T16:41:55.679502: step 890, loss 0.606079.
Test: 2018-08-08T16:42:16.731473: step 890, loss 0.56683.
Train: 2018-08-08T16:42:19.910926: step 891, loss 0.438243.
Train: 2018-08-08T16:42:23.108428: step 892, loss 0.587088.
Train: 2018-08-08T16:42:26.290889: step 893, loss 0.49857.
Train: 2018-08-08T16:42:29.477361: step 894, loss 0.619521.
Train: 2018-08-08T16:42:32.676867: step 895, loss 0.518667.
Train: 2018-08-08T16:42:35.863339: step 896, loss 0.512572.
Train: 2018-08-08T16:42:39.060841: step 897, loss 0.633412.
Train: 2018-08-08T16:42:42.235281: step 898, loss 0.614364.
Train: 2018-08-08T16:42:45.442809: step 899, loss 0.682076.
Train: 2018-08-08T16:42:48.618251: step 900, loss 0.564823.
Test: 2018-08-08T16:43:09.611066: step 900, loss 0.574155.
Train: 2018-08-08T16:43:14.606347: step 901, loss 0.525999.
Train: 2018-08-08T16:43:17.788808: step 902, loss 0.564247.
Train: 2018-08-08T16:43:20.784773: step 903, loss 0.54613.
Train: 2018-08-08T16:43:24.029400: step 904, loss 0.534974.
Train: 2018-08-08T16:43:27.200832: step 905, loss 0.517402.
Train: 2018-08-08T16:43:30.410365: step 906, loss 0.551445.
Train: 2018-08-08T16:43:33.619898: step 907, loss 0.515302.
Train: 2018-08-08T16:43:36.838455: step 908, loss 0.488699.
Train: 2018-08-08T16:43:40.079071: step 909, loss 0.431726.
Train: 2018-08-08T16:43:43.312668: step 910, loss 0.481648.
Test: 2018-08-08T16:44:04.789169: step 910, loss 0.571142.
Train: 2018-08-08T16:44:07.980654: step 911, loss 0.499287.
Train: 2018-08-08T16:44:11.184171: step 912, loss 0.569547.
Train: 2018-08-08T16:44:14.393704: step 913, loss 0.497027.
Train: 2018-08-08T16:44:17.569147: step 914, loss 0.455287.
Train: 2018-08-08T16:44:20.755619: step 915, loss 0.540903.
Train: 2018-08-08T16:44:23.979190: step 916, loss 0.598544.
Train: 2018-08-08T16:44:27.180702: step 917, loss 0.543069.
Train: 2018-08-08T16:44:30.324059: step 918, loss 0.53628.
Train: 2018-08-08T16:44:33.489475: step 919, loss 0.633973.
Train: 2018-08-08T16:44:36.680960: step 920, loss 0.511841.
Test: 2018-08-08T16:44:57.852249: step 920, loss 0.572413.
Train: 2018-08-08T16:45:01.027692: step 921, loss 0.527249.
Train: 2018-08-08T16:45:04.246248: step 922, loss 0.538609.
Train: 2018-08-08T16:45:07.420689: step 923, loss 0.489479.
Train: 2018-08-08T16:45:10.605155: step 924, loss 0.643472.
Train: 2018-08-08T16:45:13.815691: step 925, loss 0.533914.
Train: 2018-08-08T16:45:17.013192: step 926, loss 0.524809.
Train: 2018-08-08T16:45:20.198662: step 927, loss 0.628708.
Train: 2018-08-08T16:45:23.335000: step 928, loss 0.527836.
Train: 2018-08-08T16:45:26.567595: step 929, loss 0.557588.
Train: 2018-08-08T16:45:29.766099: step 930, loss 0.545843.
Test: 2018-08-08T16:45:50.818071: step 930, loss 0.5599.
Train: 2018-08-08T16:45:54.004542: step 931, loss 0.473713.
Train: 2018-08-08T16:45:57.198033: step 932, loss 0.580919.
Train: 2018-08-08T16:46:00.376483: step 933, loss 0.661681.
Train: 2018-08-08T16:46:03.580000: step 934, loss 0.461604.
Train: 2018-08-08T16:46:06.749427: step 935, loss 0.562762.
Train: 2018-08-08T16:46:09.916848: step 936, loss 0.690872.
Train: 2018-08-08T16:46:13.111342: step 937, loss 0.620374.
Train: 2018-08-08T16:46:16.266731: step 938, loss 0.629132.
Train: 2018-08-08T16:46:19.430142: step 939, loss 0.562668.
Train: 2018-08-08T16:46:22.622630: step 940, loss 0.550967.
Test: 2018-08-08T16:46:43.692650: step 940, loss 0.560625.
Train: 2018-08-08T16:46:46.845030: step 941, loss 0.61572.
Train: 2018-08-08T16:46:50.042532: step 942, loss 0.60512.
Train: 2018-08-08T16:46:53.270113: step 943, loss 0.552435.
Train: 2018-08-08T16:46:56.419486: step 944, loss 0.577679.
Train: 2018-08-08T16:46:59.595931: step 945, loss 0.531704.
Train: 2018-08-08T16:47:02.794435: step 946, loss 0.504612.
Train: 2018-08-08T16:47:05.984918: step 947, loss 0.574031.
Train: 2018-08-08T16:47:09.178409: step 948, loss 0.489267.
Train: 2018-08-08T16:47:12.348838: step 949, loss 0.513294.
Train: 2018-08-08T16:47:15.497208: step 950, loss 0.459949.
Test: 2018-08-08T16:47:36.552188: step 950, loss 0.556741.
Train: 2018-08-08T16:47:39.726628: step 951, loss 0.732997.
Train: 2018-08-08T16:47:42.915105: step 952, loss 0.561029.
Train: 2018-08-08T16:47:46.115614: step 953, loss 0.53834.
Train: 2018-08-08T16:47:49.312113: step 954, loss 0.626216.
Train: 2018-08-08T16:47:52.467502: step 955, loss 0.49665.
Train: 2018-08-08T16:47:55.714134: step 956, loss 0.569584.
Train: 2018-08-08T16:47:58.971795: step 957, loss 0.515787.
Train: 2018-08-08T16:48:02.327718: step 958, loss 0.661849.
Train: 2018-08-08T16:48:05.516195: step 959, loss 0.551186.
Train: 2018-08-08T16:48:08.720715: step 960, loss 0.505823.
Test: 2018-08-08T16:48:29.785721: step 960, loss 0.555338.
Train: 2018-08-08T16:48:33.007286: step 961, loss 0.453792.
Train: 2018-08-08T16:48:36.204788: step 962, loss 0.551757.
Train: 2018-08-08T16:48:39.448411: step 963, loss 0.432925.
Train: 2018-08-08T16:48:42.607812: step 964, loss 0.644999.
Train: 2018-08-08T16:48:45.788268: step 965, loss 0.591915.
Train: 2018-08-08T16:48:48.997801: step 966, loss 0.529092.
Train: 2018-08-08T16:48:52.216358: step 967, loss 0.51315.
Train: 2018-08-08T16:48:55.387790: step 968, loss 0.432091.
Train: 2018-08-08T16:48:58.583286: step 969, loss 0.611906.
Train: 2018-08-08T16:49:01.770760: step 970, loss 0.521448.
Test: 2018-08-08T16:49:22.812706: step 970, loss 0.552922.
Train: 2018-08-08T16:49:25.988148: step 971, loss 0.632116.
Train: 2018-08-08T16:49:29.177628: step 972, loss 0.620076.
Train: 2018-08-08T16:49:32.369113: step 973, loss 0.550599.
Train: 2018-08-08T16:49:35.564609: step 974, loss 0.537824.
Train: 2018-08-08T16:49:38.746068: step 975, loss 0.624087.
Train: 2018-08-08T16:49:41.891430: step 976, loss 0.520153.
Train: 2018-08-08T16:49:45.079908: step 977, loss 0.571912.
Train: 2018-08-08T16:49:48.265377: step 978, loss 0.541675.
Train: 2018-08-08T16:49:51.415753: step 979, loss 0.545081.
Train: 2018-08-08T16:49:54.571142: step 980, loss 0.587857.
Test: 2018-08-08T16:50:15.664827: step 980, loss 0.561066.
Train: 2018-08-08T16:50:18.867341: step 981, loss 0.498257.
Train: 2018-08-08T16:50:22.042784: step 982, loss 0.586898.
Train: 2018-08-08T16:50:25.259336: step 983, loss 0.554258.
Train: 2018-08-08T16:50:28.460848: step 984, loss 0.475563.
Train: 2018-08-08T16:50:31.630274: step 985, loss 0.520837.
Train: 2018-08-08T16:50:34.884928: step 986, loss 0.752426.
Train: 2018-08-08T16:50:38.075410: step 987, loss 0.523834.
Train: 2018-08-08T16:50:41.220773: step 988, loss 0.537397.
Train: 2018-08-08T16:50:44.456376: step 989, loss 0.587674.
Train: 2018-08-08T16:50:47.788234: step 990, loss 0.624161.
Test: 2018-08-08T16:51:09.071824: step 990, loss 0.558289.
Train: 2018-08-08T16:51:12.313440: step 991, loss 0.591154.
Train: 2018-08-08T16:51:15.540018: step 992, loss 0.545658.
Train: 2018-08-08T16:51:18.723482: step 993, loss 0.585393.
Train: 2018-08-08T16:51:21.901933: step 994, loss 0.467094.
Train: 2018-08-08T16:51:25.118485: step 995, loss 0.490618.
Train: 2018-08-08T16:51:28.344061: step 996, loss 0.466847.
Train: 2018-08-08T16:51:31.550586: step 997, loss 0.693717.
Train: 2018-08-08T16:51:34.763127: step 998, loss 0.580029.
Train: 2018-08-08T16:51:37.933556: step 999, loss 0.545964.
Train: 2018-08-08T16:51:41.127047: step 1000, loss 0.481024.
Test: 2018-08-08T16:52:02.176011: step 1000, loss 0.562376.
Train: 2018-08-08T16:52:07.243483: step 1001, loss 0.514129.
Train: 2018-08-08T16:52:10.408900: step 1002, loss 0.573025.
Train: 2018-08-08T16:52:13.603393: step 1003, loss 0.615627.
Train: 2018-08-08T16:52:16.801897: step 1004, loss 0.535307.
Train: 2018-08-08T16:52:19.975334: step 1005, loss 0.589523.
Train: 2018-08-08T16:52:23.161806: step 1006, loss 0.600107.
Train: 2018-08-08T16:52:26.371339: step 1007, loss 0.595777.
Train: 2018-08-08T16:52:29.593907: step 1008, loss 0.558588.
Train: 2018-08-08T16:52:32.814469: step 1009, loss 0.521718.
Train: 2018-08-08T16:52:35.988910: step 1010, loss 0.549257.
Test: 2018-08-08T16:52:56.937607: step 1010, loss 0.569733.
Train: 2018-08-08T16:53:00.130094: step 1011, loss 0.502634.
Train: 2018-08-08T16:53:03.651457: step 1012, loss 0.419457.
Train: 2018-08-08T16:53:06.854974: step 1013, loss 0.5877.
Train: 2018-08-08T16:53:10.055483: step 1014, loss 0.673775.
Train: 2018-08-08T16:53:13.220899: step 1015, loss 0.452457.
Train: 2018-08-08T16:53:17.474207: step 1016, loss 0.568881.
Train: 2018-08-08T16:53:20.714823: step 1017, loss 0.439269.
Train: 2018-08-08T16:53:23.936389: step 1018, loss 0.620703.
Train: 2018-08-08T16:53:27.086764: step 1019, loss 0.615029.
Train: 2018-08-08T16:53:30.275242: step 1020, loss 0.609709.
Test: 2018-08-08T16:53:51.696644: step 1020, loss 0.571478.
Train: 2018-08-08T16:53:54.972353: step 1021, loss 0.563752.
Train: 2018-08-08T16:53:58.190910: step 1022, loss 0.538882.
Train: 2018-08-08T16:54:01.529788: step 1023, loss 0.512721.
Train: 2018-08-08T16:54:05.159146: step 1024, loss 0.541932.
Train: 2018-08-08T16:54:08.379709: step 1025, loss 0.522003.
Train: 2018-08-08T16:54:11.579215: step 1026, loss 0.507799.
Train: 2018-08-08T16:54:14.844898: step 1027, loss 0.549221.
Train: 2018-08-08T16:54:18.077493: step 1028, loss 0.597856.
Train: 2018-08-08T16:54:21.326130: step 1029, loss 0.425246.
Train: 2018-08-08T16:54:24.629913: step 1030, loss 0.57295.
Test: 2018-08-08T16:54:45.852338: step 1030, loss 0.562511.
Train: 2018-08-08T16:54:49.019759: step 1031, loss 0.497497.
Train: 2018-08-08T16:54:52.235308: step 1032, loss 0.442426.
Train: 2018-08-08T16:54:55.420778: step 1033, loss 0.575772.
Train: 2018-08-08T16:54:58.595218: step 1034, loss 0.594048.
Train: 2018-08-08T16:55:01.790714: step 1035, loss 0.547553.
Train: 2018-08-08T16:55:05.014284: step 1036, loss 0.551725.
Train: 2018-08-08T16:55:08.251893: step 1037, loss 0.476529.
Train: 2018-08-08T16:55:11.542735: step 1038, loss 0.519061.
Train: 2018-08-08T16:55:14.934754: step 1039, loss 0.553334.
Train: 2018-08-08T16:55:18.134261: step 1040, loss 0.516382.
Test: 2018-08-08T16:55:39.197262: step 1040, loss 0.564119.
Train: 2018-08-08T16:55:42.373706: step 1041, loss 0.502255.
Train: 2018-08-08T16:55:45.548146: step 1042, loss 0.654855.
Train: 2018-08-08T16:55:48.734618: step 1043, loss 0.50503.
Train: 2018-08-08T16:55:52.018349: step 1044, loss 0.59389.
Train: 2018-08-08T16:55:55.191786: step 1045, loss 0.626827.
Train: 2018-08-08T16:55:58.373245: step 1046, loss 0.486978.
Train: 2018-08-08T16:56:01.563727: step 1047, loss 0.566121.
Train: 2018-08-08T16:56:04.754210: step 1048, loss 0.573082.
Train: 2018-08-08T16:56:07.982794: step 1049, loss 0.581108.
Train: 2018-08-08T16:56:11.151218: step 1050, loss 0.522445.
Test: 2018-08-08T16:56:32.169101: step 1050, loss 0.565844.
Train: 2018-08-08T16:56:35.405704: step 1051, loss 0.538947.
Train: 2018-08-08T16:56:38.614234: step 1052, loss 0.441699.
Train: 2018-08-08T16:56:41.800707: step 1053, loss 0.754014.
Train: 2018-08-08T16:56:44.978154: step 1054, loss 0.496551.
Train: 2018-08-08T16:56:48.157608: step 1055, loss 0.445678.
Train: 2018-08-08T16:56:51.331045: step 1056, loss 0.493284.
Train: 2018-08-08T16:56:54.509496: step 1057, loss 0.523678.
Train: 2018-08-08T16:56:57.740085: step 1058, loss 0.566663.
Train: 2018-08-08T16:57:00.961650: step 1059, loss 0.457634.
Train: 2018-08-08T16:57:04.137093: step 1060, loss 0.600676.
Test: 2018-08-08T16:57:25.347486: step 1060, loss 0.566588.
Train: 2018-08-08T16:57:28.495856: step 1061, loss 0.532955.
Train: 2018-08-08T16:57:31.716418: step 1062, loss 0.58239.
Train: 2018-08-08T16:57:34.901888: step 1063, loss 0.571315.
Train: 2018-08-08T16:57:38.265831: step 1064, loss 0.441665.
Train: 2018-08-08T16:57:41.633786: step 1065, loss 0.677158.
Train: 2018-08-08T16:57:44.824269: step 1066, loss 0.59918.
Train: 2018-08-08T16:57:48.023775: step 1067, loss 0.446953.
Train: 2018-08-08T16:57:51.208242: step 1068, loss 0.466352.
Train: 2018-08-08T16:57:54.390703: step 1069, loss 0.486555.
Train: 2018-08-08T16:57:57.571159: step 1070, loss 0.545826.
Test: 2018-08-08T16:58:18.637168: step 1070, loss 0.570099.
Train: 2018-08-08T16:58:21.841688: step 1071, loss 0.529386.
Train: 2018-08-08T16:58:25.037184: step 1072, loss 0.505068.
Train: 2018-08-08T16:58:28.233682: step 1073, loss 0.480482.
Train: 2018-08-08T16:58:31.424165: step 1074, loss 0.575733.
Train: 2018-08-08T16:58:34.596600: step 1075, loss 0.618991.
Train: 2018-08-08T16:58:37.801120: step 1076, loss 0.524188.
Train: 2018-08-08T16:58:41.013661: step 1077, loss 0.545994.
Train: 2018-08-08T16:58:44.203141: step 1078, loss 0.662507.
Train: 2018-08-08T16:58:47.393623: step 1079, loss 0.452886.
Train: 2018-08-08T16:58:50.613183: step 1080, loss 0.54206.
Test: 2018-08-08T16:59:11.701251: step 1080, loss 0.571812.
Train: 2018-08-08T16:59:14.906773: step 1081, loss 0.477656.
Train: 2018-08-08T16:59:18.106279: step 1082, loss 0.490156.
Train: 2018-08-08T16:59:21.300773: step 1083, loss 0.504545.
Train: 2018-08-08T16:59:24.465186: step 1084, loss 0.453021.
Train: 2018-08-08T16:59:27.667701: step 1085, loss 0.439857.
Train: 2018-08-08T16:59:30.842141: step 1086, loss 0.651473.
Train: 2018-08-08T16:59:34.063706: step 1087, loss 0.682392.
Train: 2018-08-08T16:59:37.315351: step 1088, loss 0.476412.
Train: 2018-08-08T16:59:40.505834: step 1089, loss 0.537766.
Train: 2018-08-08T16:59:43.723388: step 1090, loss 0.527437.
Test: 2018-08-08T17:00:04.828210: step 1090, loss 0.564484.
Train: 2018-08-08T17:00:08.022703: step 1091, loss 0.500224.
Train: 2018-08-08T17:00:11.228225: step 1092, loss 0.596928.
Train: 2018-08-08T17:00:14.428735: step 1093, loss 0.671943.
Train: 2018-08-08T17:00:17.611196: step 1094, loss 0.58263.
Train: 2018-08-08T17:00:20.797668: step 1095, loss 0.650819.
Train: 2018-08-08T17:00:23.992161: step 1096, loss 0.571194.
Train: 2018-08-08T17:00:27.240798: step 1097, loss 0.568956.
Train: 2018-08-08T17:00:30.424262: step 1098, loss 0.565695.
Train: 2018-08-08T17:00:34.023833: step 1099, loss 0.518252.
Train: 2018-08-08T17:00:37.393792: step 1100, loss 0.555913.
Test: 2018-08-08T17:00:58.609199: step 1100, loss 0.570037.
Train: 2018-08-08T17:01:03.840106: step 1101, loss 0.598662.
Train: 2018-08-08T17:01:07.390546: step 1102, loss 0.604581.
Train: 2018-08-08T17:01:10.728420: step 1103, loss 0.550688.
Train: 2018-08-08T17:01:14.006135: step 1104, loss 0.525638.
Train: 2018-08-08T17:01:17.250761: step 1105, loss 0.634555.
Train: 2018-08-08T17:01:20.506417: step 1106, loss 0.493993.
Train: 2018-08-08T17:01:23.769091: step 1107, loss 0.544116.
Train: 2018-08-08T17:01:27.019734: step 1108, loss 0.74016.
Train: 2018-08-08T17:01:30.181139: step 1109, loss 0.535272.
Train: 2018-08-08T17:01:33.348561: step 1110, loss 0.482557.
Test: 2018-08-08T17:01:54.506816: step 1110, loss 0.579838.
Train: 2018-08-08T17:01:58.361691: step 1111, loss 0.459673.
Train: 2018-08-08T17:02:01.671497: step 1112, loss 0.509966.
Train: 2018-08-08T17:02:04.977286: step 1113, loss 0.639408.
Train: 2018-08-08T17:02:08.647043: step 1114, loss 0.470693.
Train: 2018-08-08T17:02:12.064880: step 1115, loss 0.488974.
Train: 2018-08-08T17:02:15.520066: step 1116, loss 0.616408.
Train: 2018-08-08T17:02:18.772713: step 1117, loss 0.577338.
Train: 2018-08-08T17:02:22.004306: step 1118, loss 0.466533.
Train: 2018-08-08T17:02:25.575802: step 1119, loss 0.502996.
Train: 2018-08-08T17:02:29.313046: step 1120, loss 0.523447.
Test: 2018-08-08T17:02:51.120024: step 1120, loss 0.576489.
Train: 2018-08-08T17:02:54.277418: step 1121, loss 0.519033.
Train: 2018-08-08T17:02:57.478930: step 1122, loss 0.559333.
Train: 2018-08-08T17:03:00.687460: step 1123, loss 0.450432.
Train: 2018-08-08T17:03:04.027340: step 1124, loss 0.568551.
Train: 2018-08-08T17:03:07.292020: step 1125, loss 0.551236.
Train: 2018-08-08T17:03:10.461447: step 1126, loss 0.586427.
Train: 2018-08-08T17:03:13.636890: step 1127, loss 0.527687.
Train: 2018-08-08T17:03:16.825367: step 1128, loss 0.62777.
Train: 2018-08-08T17:03:20.015849: step 1129, loss 0.576386.
Train: 2018-08-08T17:03:23.212348: step 1130, loss 0.584454.
Test: 2018-08-08T17:03:44.823321: step 1130, loss 0.582658.
Train: 2018-08-08T17:03:48.019819: step 1131, loss 0.536611.
Train: 2018-08-08T17:03:51.266451: step 1132, loss 0.462499.
Train: 2018-08-08T17:03:54.496038: step 1133, loss 0.503265.
Train: 2018-08-08T17:03:57.707576: step 1134, loss 0.56326.
Train: 2018-08-08T17:04:00.921120: step 1135, loss 0.513048.
Train: 2018-08-08T17:04:04.271027: step 1136, loss 0.475659.
Train: 2018-08-08T17:04:08.271530: step 1137, loss 0.609533.
Train: 2018-08-08T17:04:11.688615: step 1138, loss 0.469271.
Train: 2018-08-08T17:04:15.055568: step 1139, loss 0.689249.
Train: 2018-08-08T17:04:18.491703: step 1140, loss 0.562657.
Test: 2018-08-08T17:04:40.213956: step 1140, loss 0.573871.
Train: 2018-08-08T17:04:43.624524: step 1141, loss 0.474244.
Train: 2018-08-08T17:04:46.947860: step 1142, loss 0.554223.
Train: 2018-08-08T17:04:50.290748: step 1143, loss 0.533533.
Train: 2018-08-08T17:04:53.601550: step 1144, loss 0.580269.
Train: 2018-08-08T17:04:56.922881: step 1145, loss 0.502458.
Train: 2018-08-08T17:05:00.245715: step 1146, loss 0.55544.
Train: 2018-08-08T17:05:03.567045: step 1147, loss 0.540735.
Train: 2018-08-08T17:05:06.883363: step 1148, loss 0.582295.
Train: 2018-08-08T17:05:10.242293: step 1149, loss 0.503086.
Train: 2018-08-08T17:05:13.597213: step 1150, loss 0.437494.
Test: 2018-08-08T17:05:35.185611: step 1150, loss 0.562013.
Train: 2018-08-08T17:05:38.551560: step 1151, loss 0.535227.
Train: 2018-08-08T17:05:41.884421: step 1152, loss 0.447827.
Train: 2018-08-08T17:05:45.209261: step 1153, loss 0.582604.
Train: 2018-08-08T17:05:48.553151: step 1154, loss 0.572341.
Train: 2018-08-08T17:05:51.915591: step 1155, loss 0.481947.
Train: 2018-08-08T17:05:55.256474: step 1156, loss 0.579939.
Train: 2018-08-08T17:05:58.606380: step 1157, loss 0.502058.
Train: 2018-08-08T17:06:01.921694: step 1158, loss 0.530017.
Train: 2018-08-08T17:06:05.251046: step 1159, loss 0.56416.
Train: 2018-08-08T17:06:08.577390: step 1160, loss 0.598472.
Test: 2018-08-08T17:06:30.043964: step 1160, loss 0.565597.
Train: 2018-08-08T17:06:33.345241: step 1161, loss 0.554201.
Train: 2018-08-08T17:06:36.658050: step 1162, loss 0.526623.
Train: 2018-08-08T17:06:40.002942: step 1163, loss 0.464433.
Train: 2018-08-08T17:06:43.329286: step 1164, loss 0.59754.
Train: 2018-08-08T17:06:46.665155: step 1165, loss 0.609354.
Train: 2018-08-08T17:06:49.988992: step 1166, loss 0.670389.
Train: 2018-08-08T17:06:53.319848: step 1167, loss 0.593403.
Train: 2018-08-08T17:06:56.625136: step 1168, loss 0.588112.
Train: 2018-08-08T17:06:59.954488: step 1169, loss 0.519459.
Train: 2018-08-08T17:07:03.291861: step 1170, loss 0.565968.
Test: 2018-08-08T17:07:24.775480: step 1170, loss 0.573744.
Train: 2018-08-08T17:07:28.129397: step 1171, loss 0.522541.
Train: 2018-08-08T17:07:31.439698: step 1172, loss 0.510373.
Train: 2018-08-08T17:07:34.761530: step 1173, loss 0.584882.
Train: 2018-08-08T17:07:38.085868: step 1174, loss 0.564642.
Train: 2018-08-08T17:07:41.396671: step 1175, loss 0.564894.
Train: 2018-08-08T17:07:44.704465: step 1176, loss 0.366101.
Train: 2018-08-08T17:07:48.020783: step 1177, loss 0.556196.
Train: 2018-08-08T17:07:51.317548: step 1178, loss 0.464642.
Train: 2018-08-08T17:07:54.663945: step 1179, loss 0.688123.
Train: 2018-08-08T17:07:57.968731: step 1180, loss 0.445542.
Test: 2018-08-08T17:08:19.392190: step 1180, loss 0.574809.
Train: 2018-08-08T17:08:22.691469: step 1181, loss 0.600523.
Train: 2018-08-08T17:08:26.040874: step 1182, loss 0.526143.
Train: 2018-08-08T17:08:29.360199: step 1183, loss 0.396335.
Train: 2018-08-08T17:08:32.685039: step 1184, loss 0.544887.
Train: 2018-08-08T17:08:35.991831: step 1185, loss 0.596728.
Train: 2018-08-08T17:08:39.355775: step 1186, loss 0.533484.
Train: 2018-08-08T17:08:42.680113: step 1187, loss 0.577122.
Train: 2018-08-08T17:08:46.006457: step 1188, loss 0.631057.
Train: 2018-08-08T17:08:49.308737: step 1189, loss 0.545287.
Train: 2018-08-08T17:08:52.617534: step 1190, loss 0.477278.
Test: 2018-08-08T17:09:14.051521: step 1190, loss 0.574814.
Train: 2018-08-08T17:09:17.382878: step 1191, loss 0.544243.
Train: 2018-08-08T17:09:20.760359: step 1192, loss 0.498833.
Train: 2018-08-08T17:09:24.079683: step 1193, loss 0.547696.
Train: 2018-08-08T17:09:27.418059: step 1194, loss 0.512025.
Train: 2018-08-08T17:09:30.755934: step 1195, loss 0.571102.
Train: 2018-08-08T17:09:34.094811: step 1196, loss 0.468967.
Train: 2018-08-08T17:09:37.425667: step 1197, loss 0.545505.
Train: 2018-08-08T17:09:40.756522: step 1198, loss 0.636212.
Train: 2018-08-08T17:09:44.051282: step 1199, loss 0.470853.
Train: 2018-08-08T17:09:47.373114: step 1200, loss 0.522344.
Test: 2018-08-08T17:10:08.887816: step 1200, loss 0.572902.
Train: 2018-08-08T17:10:14.065081: step 1201, loss 0.522795.
Train: 2018-08-08T17:10:17.388918: step 1202, loss 0.534071.
Train: 2018-08-08T17:10:20.711752: step 1203, loss 0.589399.
Train: 2018-08-08T17:10:23.846587: step 1204, loss 0.554068.
Train: 2018-08-08T17:10:27.186968: step 1205, loss 0.588875.
Train: 2018-08-08T17:10:30.517322: step 1206, loss 0.554927.
Train: 2018-08-08T17:10:33.826621: step 1207, loss 0.535947.
Train: 2018-08-08T17:10:37.130907: step 1208, loss 0.514697.
Train: 2018-08-08T17:10:40.457752: step 1209, loss 0.637859.
Train: 2018-08-08T17:10:43.771562: step 1210, loss 0.574681.
Test: 2018-08-08T17:11:05.169454: step 1210, loss 0.577616.
Train: 2018-08-08T17:11:08.453685: step 1211, loss 0.457148.
Train: 2018-08-08T17:11:11.780029: step 1212, loss 0.491471.
Train: 2018-08-08T17:11:15.107375: step 1213, loss 0.54673.
Train: 2018-08-08T17:11:18.413164: step 1214, loss 0.542133.
Train: 2018-08-08T17:11:21.742516: step 1215, loss 0.639698.
Train: 2018-08-08T17:11:25.080892: step 1216, loss 0.599606.
Train: 2018-08-08T17:11:28.436814: step 1217, loss 0.42195.
Train: 2018-08-08T17:11:31.755638: step 1218, loss 0.633077.
Train: 2018-08-08T17:11:35.074462: step 1219, loss 0.511962.
Train: 2018-08-08T17:11:38.401809: step 1220, loss 0.504341.
Test: 2018-08-08T17:11:59.822761: step 1220, loss 0.573566.
Train: 2018-08-08T17:12:03.137579: step 1221, loss 0.629134.
Train: 2018-08-08T17:12:06.459911: step 1222, loss 0.600505.
Train: 2018-08-08T17:12:09.816837: step 1223, loss 0.525103.
Train: 2018-08-08T17:12:13.153709: step 1224, loss 0.561053.
Train: 2018-08-08T17:12:16.482559: step 1225, loss 0.648044.
Train: 2018-08-08T17:12:19.849009: step 1226, loss 0.481795.
Train: 2018-08-08T17:12:23.178361: step 1227, loss 0.53711.
Train: 2018-08-08T17:12:26.499191: step 1228, loss 0.530479.
Train: 2018-08-08T17:12:29.828041: step 1229, loss 0.567419.
Train: 2018-08-08T17:12:33.148380: step 1230, loss 0.519691.
Test: 2018-08-08T17:12:54.596410: step 1230, loss 0.570439.
Train: 2018-08-08T17:12:57.940802: step 1231, loss 0.516149.
Train: 2018-08-08T17:13:01.269151: step 1232, loss 0.625004.
Train: 2018-08-08T17:13:04.633596: step 1233, loss 0.507234.
Train: 2018-08-08T17:13:07.949914: step 1234, loss 0.507564.
Train: 2018-08-08T17:13:11.289794: step 1235, loss 0.465801.
Train: 2018-08-08T17:13:14.607615: step 1236, loss 0.575311.
Train: 2018-08-08T17:13:17.956017: step 1237, loss 0.552288.
Train: 2018-08-08T17:13:21.283364: step 1238, loss 0.477131.
Train: 2018-08-08T17:13:24.640790: step 1239, loss 0.468577.
Train: 2018-08-08T17:13:27.976157: step 1240, loss 0.593472.
Test: 2018-08-08T17:13:49.442230: step 1240, loss 0.571673.
Train: 2018-08-08T17:13:52.767572: step 1241, loss 0.572082.
Train: 2018-08-08T17:13:56.137030: step 1242, loss 0.557355.
Train: 2018-08-08T17:13:59.446830: step 1243, loss 0.618941.
Train: 2018-08-08T17:14:02.770667: step 1244, loss 0.557896.
Train: 2018-08-08T17:14:06.118067: step 1245, loss 0.523788.
Train: 2018-08-08T17:14:09.445413: step 1246, loss 0.483611.
Train: 2018-08-08T17:14:12.773261: step 1247, loss 0.532829.
Train: 2018-08-08T17:14:16.099108: step 1248, loss 0.506446.
Train: 2018-08-08T17:14:19.416427: step 1249, loss 0.53683.
Train: 2018-08-08T17:14:22.745779: step 1250, loss 0.588983.
Test: 2018-08-08T17:14:44.237419: step 1250, loss 0.55476.
Train: 2018-08-08T17:14:47.586323: step 1251, loss 0.57852.
Train: 2018-08-08T17:14:50.910160: step 1252, loss 0.487323.
Train: 2018-08-08T17:14:54.237507: step 1253, loss 0.589812.
Train: 2018-08-08T17:14:57.527254: step 1254, loss 0.542148.
Train: 2018-08-08T17:15:00.847080: step 1255, loss 0.48911.
Train: 2018-08-08T17:15:04.159387: step 1256, loss 0.628907.
Train: 2018-08-08T17:15:07.488739: step 1257, loss 0.519412.
Train: 2018-08-08T17:15:10.793026: step 1258, loss 0.507218.
Train: 2018-08-08T17:15:14.125383: step 1259, loss 0.608688.
Train: 2018-08-08T17:15:17.455236: step 1260, loss 0.583146.
Test: 2018-08-08T17:15:38.989490: step 1260, loss 0.563213.
Train: 2018-08-08T17:15:42.346917: step 1261, loss 0.499502.
Train: 2018-08-08T17:15:45.694316: step 1262, loss 0.655005.
Train: 2018-08-08T17:15:49.031690: step 1263, loss 0.555512.
Train: 2018-08-08T17:15:52.402652: step 1264, loss 0.623926.
Train: 2018-08-08T17:15:55.754063: step 1265, loss 0.593108.
Train: 2018-08-08T17:15:59.074892: step 1266, loss 0.627932.
Train: 2018-08-08T17:16:02.422793: step 1267, loss 0.499396.
Train: 2018-08-08T17:16:05.751645: step 1268, loss 0.527134.
Train: 2018-08-08T17:16:09.076483: step 1269, loss 0.51799.
Train: 2018-08-08T17:16:12.416864: step 1270, loss 0.485455.
Test: 2018-08-08T17:16:33.891459: step 1270, loss 0.568901.
Train: 2018-08-08T17:16:37.211787: step 1271, loss 0.479049.
Train: 2018-08-08T17:16:40.581747: step 1272, loss 0.49904.
Train: 2018-08-08T17:16:43.911600: step 1273, loss 0.467752.
Train: 2018-08-08T17:16:47.245965: step 1274, loss 0.469792.
Train: 2018-08-08T17:16:50.557269: step 1275, loss 0.626557.
Train: 2018-08-08T17:16:53.870077: step 1276, loss 0.448906.
Train: 2018-08-08T17:16:57.212463: step 1277, loss 0.455216.
Train: 2018-08-08T17:17:00.536802: step 1278, loss 0.552399.
Train: 2018-08-08T17:17:03.854122: step 1279, loss 0.559183.
Train: 2018-08-08T17:17:07.191996: step 1280, loss 0.747755.
Test: 2018-08-08T17:17:28.683135: step 1280, loss 0.568887.
Train: 2018-08-08T17:17:31.980402: step 1281, loss 0.602443.
Train: 2018-08-08T17:17:35.302736: step 1282, loss 0.544043.
Train: 2018-08-08T17:17:38.624065: step 1283, loss 0.59152.
Train: 2018-08-08T17:17:41.953417: step 1284, loss 0.503844.
Train: 2018-08-08T17:17:45.278257: step 1285, loss 0.519545.
Train: 2018-08-08T17:17:48.591065: step 1286, loss 0.530251.
Train: 2018-08-08T17:17:51.913900: step 1287, loss 0.561573.
Train: 2018-08-08T17:17:55.292883: step 1288, loss 0.523687.
Train: 2018-08-08T17:17:58.614214: step 1289, loss 0.504214.
Train: 2018-08-08T17:18:01.938051: step 1290, loss 0.431216.
Test: 2018-08-08T17:18:23.404123: step 1290, loss 0.54958.
Train: 2018-08-08T17:18:26.739491: step 1291, loss 0.507446.
Train: 2018-08-08T17:18:30.076864: step 1292, loss 0.527891.
Train: 2018-08-08T17:18:33.379144: step 1293, loss 0.45757.
Train: 2018-08-08T17:18:36.704485: step 1294, loss 0.491623.
Train: 2018-08-08T17:18:40.082968: step 1295, loss 0.52431.
Train: 2018-08-08T17:18:43.431872: step 1296, loss 0.592422.
Train: 2018-08-08T17:18:46.736658: step 1297, loss 0.4939.
Train: 2018-08-08T17:18:50.041444: step 1298, loss 0.608273.
Train: 2018-08-08T17:18:53.351746: step 1299, loss 0.573602.
Train: 2018-08-08T17:18:56.677087: step 1300, loss 0.607614.
Test: 2018-08-08T17:19:18.123106: step 1300, loss 0.571081.
Train: 2018-08-08T17:19:23.237203: step 1301, loss 0.559333.
Train: 2018-08-08T17:19:26.574075: step 1302, loss 0.538319.
Train: 2018-08-08T17:19:29.923480: step 1303, loss 0.491026.
Train: 2018-08-08T17:19:33.245312: step 1304, loss 0.454089.
Train: 2018-08-08T17:19:36.550600: step 1305, loss 0.542899.
Train: 2018-08-08T17:19:39.866415: step 1306, loss 0.516539.
Train: 2018-08-08T17:19:43.167191: step 1307, loss 0.578629.
Train: 2018-08-08T17:19:46.478996: step 1308, loss 0.603639.
Train: 2018-08-08T17:19:49.791804: step 1309, loss 0.578015.
Train: 2018-08-08T17:19:53.116644: step 1310, loss 0.503081.
Test: 2018-08-08T17:20:14.616807: step 1310, loss 0.569469.
Train: 2018-08-08T17:20:17.935129: step 1311, loss 0.595876.
Train: 2018-08-08T17:20:21.265985: step 1312, loss 0.589845.
Train: 2018-08-08T17:20:24.649982: step 1313, loss 0.469643.
Train: 2018-08-08T17:20:28.011921: step 1314, loss 0.530572.
Train: 2018-08-08T17:20:31.351299: step 1315, loss 0.590727.
Train: 2018-08-08T17:20:34.666113: step 1316, loss 0.487858.
Train: 2018-08-08T17:20:38.007998: step 1317, loss 0.553421.
Train: 2018-08-08T17:20:41.320305: step 1318, loss 0.465526.
Train: 2018-08-08T17:20:44.661187: step 1319, loss 0.49899.
Train: 2018-08-08T17:20:47.999566: step 1320, loss 0.576996.
Test: 2018-08-08T17:21:09.424028: step 1320, loss 0.570451.
Train: 2018-08-08T17:21:12.750373: step 1321, loss 0.575905.
Train: 2018-08-08T17:21:16.084736: step 1322, loss 0.500666.
Train: 2018-08-08T17:21:19.416595: step 1323, loss 0.484427.
Train: 2018-08-08T17:21:22.738427: step 1324, loss 0.486845.
Train: 2018-08-08T17:21:26.047224: step 1325, loss 0.57334.
Train: 2018-08-08T17:21:29.368053: step 1326, loss 0.557403.
Train: 2018-08-08T17:21:32.658301: step 1327, loss 0.456712.
Train: 2018-08-08T17:21:35.987152: step 1328, loss 0.456051.
Train: 2018-08-08T17:21:39.313495: step 1329, loss 0.548023.
Train: 2018-08-08T17:21:42.621290: step 1330, loss 0.689416.
Test: 2018-08-08T17:22:04.029208: step 1330, loss 0.571129.
Train: 2018-08-08T17:22:07.359061: step 1331, loss 0.537186.
Train: 2018-08-08T17:22:10.664349: step 1332, loss 0.624258.
Train: 2018-08-08T17:22:14.022276: step 1333, loss 0.555697.
Train: 2018-08-08T17:22:17.336589: step 1334, loss 0.575342.
Train: 2018-08-08T17:22:20.687498: step 1335, loss 0.655503.
Train: 2018-08-08T17:22:24.017351: step 1336, loss 0.550179.
Train: 2018-08-08T17:22:27.315118: step 1337, loss 0.533323.
Train: 2018-08-08T17:22:30.675553: step 1338, loss 0.605198.
Train: 2018-08-08T17:22:34.001897: step 1339, loss 0.557796.
Train: 2018-08-08T17:22:37.311195: step 1340, loss 0.617927.
Test: 2018-08-08T17:22:58.765737: step 1340, loss 0.573219.
Train: 2018-08-08T17:23:02.082054: step 1341, loss 0.59521.
Train: 2018-08-08T17:23:05.419929: step 1342, loss 0.476249.
Train: 2018-08-08T17:23:08.760310: step 1343, loss 0.516329.
Train: 2018-08-08T17:23:12.082643: step 1344, loss 0.49163.
Train: 2018-08-08T17:23:15.392443: step 1345, loss 0.417257.
Train: 2018-08-08T17:23:18.691213: step 1346, loss 0.570006.
Train: 2018-08-08T17:23:22.009536: step 1347, loss 0.534863.
Train: 2018-08-08T17:23:25.337384: step 1348, loss 0.441559.
Train: 2018-08-08T17:23:28.676762: step 1349, loss 0.555105.
Train: 2018-08-08T17:23:32.012130: step 1350, loss 0.451949.
Test: 2018-08-08T17:23:53.452133: step 1350, loss 0.574891.
Train: 2018-08-08T17:23:56.779981: step 1351, loss 0.546784.
Train: 2018-08-08T17:24:00.079754: step 1352, loss 0.516372.
Train: 2018-08-08T17:24:03.422642: step 1353, loss 0.630226.
Train: 2018-08-08T17:24:06.748484: step 1354, loss 0.599913.
Train: 2018-08-08T17:24:10.064801: step 1355, loss 0.505741.
Train: 2018-08-08T17:24:13.388137: step 1356, loss 0.644564.
Train: 2018-08-08T17:24:16.711473: step 1357, loss 0.618128.
Train: 2018-08-08T17:24:20.027791: step 1358, loss 0.531795.
Train: 2018-08-08T17:24:23.332577: step 1359, loss 0.512901.
Train: 2018-08-08T17:24:26.665438: step 1360, loss 0.526374.
Test: 2018-08-08T17:24:48.171617: step 1360, loss 0.577095.
Train: 2018-08-08T17:24:51.469385: step 1361, loss 0.516719.
Train: 2018-08-08T17:24:54.793723: step 1362, loss 0.606348.
Train: 2018-08-08T17:24:58.108035: step 1363, loss 0.489909.
Train: 2018-08-08T17:25:01.442400: step 1364, loss 0.42701.
Train: 2018-08-08T17:25:04.770750: step 1365, loss 0.579926.
Train: 2018-08-08T17:25:08.117648: step 1366, loss 0.457691.
Train: 2018-08-08T17:25:11.414915: step 1367, loss 0.459103.
Train: 2018-08-08T17:25:14.732235: step 1368, loss 0.572615.
Train: 2018-08-08T17:25:18.018472: step 1369, loss 0.407848.
Train: 2018-08-08T17:25:21.354842: step 1370, loss 0.779942.
Test: 2018-08-08T17:25:42.862531: step 1370, loss 0.566664.
Train: 2018-08-08T17:25:46.164310: step 1371, loss 0.521616.
Train: 2018-08-08T17:25:49.499176: step 1372, loss 0.598963.
Train: 2018-08-08T17:25:52.828027: step 1373, loss 0.578706.
Train: 2018-08-08T17:25:56.137827: step 1374, loss 0.562699.
Train: 2018-08-08T17:25:59.460160: step 1375, loss 0.564323.
Train: 2018-08-08T17:26:02.775475: step 1376, loss 0.600626.
Train: 2018-08-08T17:26:06.089786: step 1377, loss 0.481963.
Train: 2018-08-08T17:26:09.429666: step 1378, loss 0.576109.
Train: 2018-08-08T17:26:12.781077: step 1379, loss 0.518359.
Train: 2018-08-08T17:26:16.122967: step 1380, loss 0.487676.
Test: 2018-08-08T17:26:38.104910: step 1380, loss 0.561403.
Train: 2018-08-08T17:26:41.343021: step 1381, loss 0.561363.
Train: 2018-08-08T17:26:44.539593: step 1382, loss 0.521928.
Train: 2018-08-08T17:26:47.748199: step 1383, loss 0.509378.
Train: 2018-08-08T17:26:50.965795: step 1384, loss 0.573889.
Train: 2018-08-08T17:26:54.150266: step 1385, loss 0.470562.
Train: 2018-08-08T17:26:57.336738: step 1386, loss 0.571653.
Train: 2018-08-08T17:27:00.519199: step 1387, loss 0.49775.
Train: 2018-08-08T17:27:03.695644: step 1388, loss 0.546849.
Train: 2018-08-08T17:27:06.895151: step 1389, loss 0.588278.
Train: 2018-08-08T17:27:10.144791: step 1390, loss 0.537938.
Test: 2018-08-08T17:27:31.182725: step 1390, loss 0.568503.
Train: 2018-08-08T17:27:34.419330: step 1391, loss 0.495416.
Train: 2018-08-08T17:27:37.535616: step 1392, loss 0.497617.
Train: 2018-08-08T17:27:40.617810: step 1393, loss 0.524122.
Train: 2018-08-08T17:27:43.677946: step 1394, loss 0.570952.
Train: 2018-08-08T17:27:46.799245: step 1395, loss 0.437305.
Train: 2018-08-08T17:27:49.876426: step 1396, loss 0.636676.
Train: 2018-08-08T17:27:52.969650: step 1397, loss 0.561207.
Train: 2018-08-08T17:27:56.065882: step 1398, loss 0.533568.
Train: 2018-08-08T17:27:59.146072: step 1399, loss 0.565387.
Train: 2018-08-08T17:28:02.229269: step 1400, loss 0.543794.
Test: 2018-08-08T17:28:23.052634: step 1400, loss 0.57124.
Train: 2018-08-08T17:28:27.887487: step 1401, loss 0.484214.
Train: 2018-08-08T17:28:30.976701: step 1402, loss 0.484525.
Train: 2018-08-08T17:28:34.065914: step 1403, loss 0.536316.
Train: 2018-08-08T17:28:37.138082: step 1404, loss 0.56308.
Train: 2018-08-08T17:28:40.222282: step 1405, loss 0.460169.
Train: 2018-08-08T17:28:43.285426: step 1406, loss 0.480023.
Train: 2018-08-08T17:28:46.353584: step 1407, loss 0.697418.
Train: 2018-08-08T17:28:49.421741: step 1408, loss 0.531871.
Train: 2018-08-08T17:28:52.507946: step 1409, loss 0.580295.
Train: 2018-08-08T17:28:55.615208: step 1410, loss 0.586558.
Test: 2018-08-08T17:29:16.416513: step 1410, loss 0.570435.
Train: 2018-08-08T17:29:19.597971: step 1411, loss 0.587978.
Train: 2018-08-08T17:29:22.751355: step 1412, loss 0.554804.
Train: 2018-08-08T17:29:25.833550: step 1413, loss 0.541295.
Train: 2018-08-08T17:29:28.907723: step 1414, loss 0.551054.
Train: 2018-08-08T17:29:31.968862: step 1415, loss 0.524022.
Train: 2018-08-08T17:29:35.046043: step 1416, loss 0.586403.
Train: 2018-08-08T17:29:38.106179: step 1417, loss 0.475116.
Train: 2018-08-08T17:29:41.188374: step 1418, loss 0.513718.
Train: 2018-08-08T17:29:44.270569: step 1419, loss 0.722048.
Train: 2018-08-08T17:29:47.374822: step 1420, loss 0.541571.
Test: 2018-08-08T17:30:08.177130: step 1420, loss 0.574137.
Train: 2018-08-08T17:30:11.269351: step 1421, loss 0.514131.
Train: 2018-08-08T17:30:14.390650: step 1422, loss 0.530038.
Train: 2018-08-08T17:30:17.481868: step 1423, loss 0.508626.
Train: 2018-08-08T17:30:20.586122: step 1424, loss 0.57506.
Train: 2018-08-08T17:30:23.697394: step 1425, loss 0.632255.
Train: 2018-08-08T17:30:26.832730: step 1426, loss 0.603464.
Train: 2018-08-08T17:30:29.922946: step 1427, loss 0.597818.
Train: 2018-08-08T17:30:33.021183: step 1428, loss 0.62264.
Train: 2018-08-08T17:30:36.100370: step 1429, loss 0.576169.
Train: 2018-08-08T17:30:39.203621: step 1430, loss 0.517465.
Test: 2018-08-08T17:30:59.999912: step 1430, loss 0.574312.
Train: 2018-08-08T17:31:03.069072: step 1431, loss 0.583428.
Train: 2018-08-08T17:31:06.150264: step 1432, loss 0.601541.
Train: 2018-08-08T17:31:09.243488: step 1433, loss 0.500992.
Train: 2018-08-08T17:31:12.345736: step 1434, loss 0.509126.
Train: 2018-08-08T17:31:15.442971: step 1435, loss 0.519821.
Train: 2018-08-08T17:31:18.530179: step 1436, loss 0.596791.
Train: 2018-08-08T17:31:21.624405: step 1437, loss 0.500451.
Train: 2018-08-08T17:31:24.797843: step 1438, loss 0.475009.
Train: 2018-08-08T17:31:27.905104: step 1439, loss 0.758234.
Train: 2018-08-08T17:31:30.982286: step 1440, loss 0.482786.
Test: 2018-08-08T17:31:51.784594: step 1440, loss 0.568863.
Train: 2018-08-08T17:31:54.905892: step 1441, loss 0.519532.
Train: 2018-08-08T17:31:58.016161: step 1442, loss 0.517047.
Train: 2018-08-08T17:32:01.127433: step 1443, loss 0.534769.
Train: 2018-08-08T17:32:04.217649: step 1444, loss 0.569478.
Train: 2018-08-08T17:32:07.297839: step 1445, loss 0.496482.
Train: 2018-08-08T17:32:10.405100: step 1446, loss 0.560547.
Train: 2018-08-08T17:32:13.492308: step 1447, loss 0.58054.
Train: 2018-08-08T17:32:16.561468: step 1448, loss 0.550619.
Train: 2018-08-08T17:32:19.662714: step 1449, loss 0.646564.
Train: 2018-08-08T17:32:22.745911: step 1450, loss 0.624612.
Test: 2018-08-08T17:32:43.549222: step 1450, loss 0.570716.
Train: 2018-08-08T17:32:46.641442: step 1451, loss 0.627366.
Train: 2018-08-08T17:32:49.769760: step 1452, loss 0.523927.
Train: 2018-08-08T17:32:52.851955: step 1453, loss 0.625487.
Train: 2018-08-08T17:32:55.999323: step 1454, loss 0.564792.
Train: 2018-08-08T17:32:59.087533: step 1455, loss 0.510225.
Train: 2018-08-08T17:33:02.144661: step 1456, loss 0.501488.
Train: 2018-08-08T17:33:05.256936: step 1457, loss 0.527463.
Train: 2018-08-08T17:33:08.352166: step 1458, loss 0.608602.
Train: 2018-08-08T17:33:11.451405: step 1459, loss 0.635935.
Train: 2018-08-08T17:33:14.545632: step 1460, loss 0.547106.
Test: 2018-08-08T17:33:35.349946: step 1460, loss 0.575083.
Train: 2018-08-08T17:33:38.503329: step 1461, loss 0.591012.
Train: 2018-08-08T17:33:41.647689: step 1462, loss 0.518586.
Train: 2018-08-08T17:33:44.730886: step 1463, loss 0.618992.
Train: 2018-08-08T17:33:47.853188: step 1464, loss 0.619188.
Train: 2018-08-08T17:33:50.937388: step 1465, loss 0.515896.
Train: 2018-08-08T17:33:54.041641: step 1466, loss 0.555486.
Train: 2018-08-08T17:33:57.118823: step 1467, loss 0.542094.
Train: 2018-08-08T17:34:00.210041: step 1468, loss 0.587637.
Train: 2018-08-08T17:34:03.292236: step 1469, loss 0.419622.
Train: 2018-08-08T17:34:06.383454: step 1470, loss 0.458291.
Test: 2018-08-08T17:34:27.231885: step 1470, loss 0.566308.
Train: 2018-08-08T17:34:30.318090: step 1471, loss 0.53724.
Train: 2018-08-08T17:34:33.394269: step 1472, loss 0.720734.
Train: 2018-08-08T17:34:36.493509: step 1473, loss 0.551859.
Train: 2018-08-08T17:34:39.596759: step 1474, loss 0.573864.
Train: 2018-08-08T17:34:42.713045: step 1475, loss 0.575857.
Train: 2018-08-08T17:34:45.802258: step 1476, loss 0.604477.
Train: 2018-08-08T17:34:48.967674: step 1477, loss 0.539153.
Train: 2018-08-08T17:34:52.061901: step 1478, loss 0.489755.
Train: 2018-08-08T17:34:55.133066: step 1479, loss 0.527523.
Train: 2018-08-08T17:34:58.219272: step 1480, loss 0.584364.
Test: 2018-08-08T17:35:19.065697: step 1480, loss 0.555422.
Train: 2018-08-08T17:35:22.170952: step 1481, loss 0.539137.
Train: 2018-08-08T17:35:25.281222: step 1482, loss 0.650085.
Train: 2018-08-08T17:35:28.420568: step 1483, loss 0.651873.
Train: 2018-08-08T17:35:31.498753: step 1484, loss 0.569007.
Train: 2018-08-08T17:35:34.585961: step 1485, loss 0.546854.
Train: 2018-08-08T17:35:37.692219: step 1486, loss 0.517395.
Train: 2018-08-08T17:35:40.814520: step 1487, loss 0.497884.
Train: 2018-08-08T17:35:43.907745: step 1488, loss 0.536274.
Train: 2018-08-08T17:35:46.973897: step 1489, loss 0.679154.
Train: 2018-08-08T17:35:50.075142: step 1490, loss 0.493518.
Test: 2018-08-08T17:36:10.888479: step 1490, loss 0.568767.
Train: 2018-08-08T17:36:13.966663: step 1491, loss 0.413364.
Train: 2018-08-08T17:36:17.097988: step 1492, loss 0.475789.
Train: 2018-08-08T17:36:20.193218: step 1493, loss 0.558321.
Train: 2018-08-08T17:36:23.286442: step 1494, loss 0.446987.
Train: 2018-08-08T17:36:26.374652: step 1495, loss 0.487774.
Train: 2018-08-08T17:36:29.456847: step 1496, loss 0.591301.
Train: 2018-08-08T17:36:32.557090: step 1497, loss 0.559353.
Train: 2018-08-08T17:36:35.661343: step 1498, loss 0.532093.
Train: 2018-08-08T17:36:38.790663: step 1499, loss 0.653158.
Train: 2018-08-08T17:36:41.888901: step 1500, loss 0.533496.
Test: 2018-08-08T17:37:03.143411: step 1500, loss 0.568235.
Train: 2018-08-08T17:37:08.308142: step 1501, loss 0.484288.
Train: 2018-08-08T17:37:11.579840: step 1502, loss 0.48286.
Train: 2018-08-08T17:37:14.844520: step 1503, loss 0.499857.
Train: 2018-08-08T17:37:18.178384: step 1504, loss 0.531152.
Train: 2018-08-08T17:37:21.261582: step 1505, loss 0.48868.
Train: 2018-08-08T17:37:24.458080: step 1506, loss 0.47169.
Train: 2018-08-08T17:37:27.539272: step 1507, loss 0.446292.
Train: 2018-08-08T17:37:30.651547: step 1508, loss 0.442783.
Train: 2018-08-08T17:37:33.760814: step 1509, loss 0.700105.
Train: 2018-08-08T17:37:36.820950: step 1510, loss 0.491447.
Test: 2018-08-08T17:37:57.626266: step 1510, loss 0.557695.
Train: 2018-08-08T17:38:00.720492: step 1511, loss 0.617008.
Train: 2018-08-08T17:38:03.834772: step 1512, loss 0.553158.
Train: 2018-08-08T17:38:06.927996: step 1513, loss 0.625268.
Train: 2018-08-08T17:38:10.041273: step 1514, loss 0.52182.
Train: 2018-08-08T17:38:13.154550: step 1515, loss 0.604716.
Train: 2018-08-08T17:38:16.256799: step 1516, loss 0.58361.
Train: 2018-08-08T17:38:19.366065: step 1517, loss 0.577034.
Train: 2018-08-08T17:38:22.452271: step 1518, loss 0.608283.
Train: 2018-08-08T17:38:25.551511: step 1519, loss 0.561696.
Train: 2018-08-08T17:38:28.662783: step 1520, loss 0.438887.
Test: 2018-08-08T17:38:49.472110: step 1520, loss 0.565758.
Train: 2018-08-08T17:38:52.551296: step 1521, loss 0.500325.
Train: 2018-08-08T17:38:55.646525: step 1522, loss 0.44919.
Train: 2018-08-08T17:38:58.767824: step 1523, loss 0.499919.
Train: 2018-08-08T17:39:01.856035: step 1524, loss 0.495315.
Train: 2018-08-08T17:39:04.924192: step 1525, loss 0.478828.
Train: 2018-08-08T17:39:08.038472: step 1526, loss 0.629889.
Train: 2018-08-08T17:39:11.134704: step 1527, loss 0.642646.
Train: 2018-08-08T17:39:14.233944: step 1528, loss 0.470436.
Train: 2018-08-08T17:39:17.302101: step 1529, loss 0.5489.
Train: 2018-08-08T17:39:20.381288: step 1530, loss 0.625187.
Test: 2018-08-08T17:39:41.182593: step 1530, loss 0.555517.
Train: 2018-08-08T17:39:44.323945: step 1531, loss 0.472731.
Train: 2018-08-08T17:39:47.382076: step 1532, loss 0.574947.
Train: 2018-08-08T17:39:50.454243: step 1533, loss 0.608826.
Train: 2018-08-08T17:39:53.562508: step 1534, loss 0.55379.
Train: 2018-08-08T17:39:56.658740: step 1535, loss 0.55467.
Train: 2018-08-08T17:39:59.758982: step 1536, loss 0.543235.
Train: 2018-08-08T17:40:02.826137: step 1537, loss 0.473838.
Train: 2018-08-08T17:40:05.911340: step 1538, loss 0.471583.
Train: 2018-08-08T17:40:08.984510: step 1539, loss 0.464742.
Train: 2018-08-08T17:40:12.085756: step 1540, loss 0.602395.
Test: 2018-08-08T17:40:32.934186: step 1540, loss 0.562285.
Train: 2018-08-08T17:40:36.033426: step 1541, loss 0.544989.
Train: 2018-08-08T17:40:39.109605: step 1542, loss 0.639355.
Train: 2018-08-08T17:40:42.157709: step 1543, loss 0.551522.
Train: 2018-08-08T17:40:45.263968: step 1544, loss 0.479837.
Train: 2018-08-08T17:40:48.358194: step 1545, loss 0.481659.
Train: 2018-08-08T17:40:51.480495: step 1546, loss 0.566142.
Train: 2018-08-08T17:40:54.583746: step 1547, loss 0.468506.
Train: 2018-08-08T17:40:57.668949: step 1548, loss 0.454037.
Train: 2018-08-08T17:41:00.768189: step 1549, loss 0.672037.
Train: 2018-08-08T17:41:03.854394: step 1550, loss 0.565407.
Test: 2018-08-08T17:41:24.634644: step 1550, loss 0.566653.
Train: 2018-08-08T17:41:27.706811: step 1551, loss 0.499566.
Train: 2018-08-08T17:41:30.823097: step 1552, loss 0.540413.
Train: 2018-08-08T17:41:33.913312: step 1553, loss 0.587569.
Train: 2018-08-08T17:41:37.004531: step 1554, loss 0.591725.
Train: 2018-08-08T17:41:40.122822: step 1555, loss 0.464309.
Train: 2018-08-08T17:41:43.206019: step 1556, loss 0.491242.
Train: 2018-08-08T17:41:46.298241: step 1557, loss 0.58865.
Train: 2018-08-08T17:41:49.403497: step 1558, loss 0.610892.
Train: 2018-08-08T17:41:52.498726: step 1559, loss 0.535092.
Train: 2018-08-08T17:41:55.602980: step 1560, loss 0.606624.
Test: 2018-08-08T17:42:16.446397: step 1560, loss 0.570851.
Train: 2018-08-08T17:42:19.579727: step 1561, loss 0.56679.
Train: 2018-08-08T17:42:22.720076: step 1562, loss 0.496941.
Train: 2018-08-08T17:42:25.850399: step 1563, loss 0.541338.
Train: 2018-08-08T17:42:28.972700: step 1564, loss 0.556089.
Train: 2018-08-08T17:42:32.081967: step 1565, loss 0.558463.
Train: 2018-08-08T17:42:35.165164: step 1566, loss 0.623576.
Train: 2018-08-08T17:42:38.247359: step 1567, loss 0.587392.
Train: 2018-08-08T17:42:41.313511: step 1568, loss 0.537459.
Train: 2018-08-08T17:42:44.386682: step 1569, loss 0.516343.
Train: 2018-08-08T17:42:47.494946: step 1570, loss 0.439325.
Test: 2018-08-08T17:43:08.313296: step 1570, loss 0.568188.
Train: 2018-08-08T17:43:11.400504: step 1571, loss 0.656059.
Train: 2018-08-08T17:43:14.496736: step 1572, loss 0.576892.
Train: 2018-08-08T17:43:17.618035: step 1573, loss 0.575923.
Train: 2018-08-08T17:43:20.708251: step 1574, loss 0.519424.
Train: 2018-08-08T17:43:23.778413: step 1575, loss 0.483615.
Train: 2018-08-08T17:43:26.853590: step 1576, loss 0.54335.
Train: 2018-08-08T17:43:29.948819: step 1577, loss 0.534214.
Train: 2018-08-08T17:43:33.029008: step 1578, loss 0.525136.
Train: 2018-08-08T17:43:36.125240: step 1579, loss 0.520936.
Train: 2018-08-08T17:43:39.178358: step 1580, loss 0.611125.
Test: 2018-08-08T17:43:59.996708: step 1580, loss 0.564071.
Train: 2018-08-08T17:44:03.055841: step 1581, loss 0.549661.
Train: 2018-08-08T17:44:06.161097: step 1582, loss 0.449969.
Train: 2018-08-08T17:44:09.271367: step 1583, loss 0.532187.
Train: 2018-08-08T17:44:12.381636: step 1584, loss 0.383768.
Train: 2018-08-08T17:44:15.486892: step 1585, loss 0.521451.
Train: 2018-08-08T17:44:18.587135: step 1586, loss 0.654634.
Train: 2018-08-08T17:44:21.722471: step 1587, loss 0.640932.
Train: 2018-08-08T17:44:24.824719: step 1588, loss 0.576727.
Train: 2018-08-08T17:44:27.940001: step 1589, loss 0.579958.
Train: 2018-08-08T17:44:31.039241: step 1590, loss 0.570479.
Test: 2018-08-08T17:44:51.892685: step 1590, loss 0.559421.
Train: 2018-08-08T17:44:54.990923: step 1591, loss 0.601702.
Train: 2018-08-08T17:44:58.075122: step 1592, loss 0.577333.
Train: 2018-08-08T17:45:01.150299: step 1593, loss 0.597235.
Train: 2018-08-08T17:45:04.237506: step 1594, loss 0.509808.
Train: 2018-08-08T17:45:07.339754: step 1595, loss 0.512719.
Train: 2018-08-08T17:45:10.429970: step 1596, loss 0.506455.
Train: 2018-08-08T17:45:13.531216: step 1597, loss 0.532984.
Train: 2018-08-08T17:45:16.604387: step 1598, loss 0.615135.
Train: 2018-08-08T17:45:19.688587: step 1599, loss 0.624894.
Train: 2018-08-08T17:45:22.759752: step 1600, loss 0.515579.
Test: 2018-08-08T17:45:43.625228: step 1600, loss 0.570856.
Train: 2018-08-08T17:45:48.507207: step 1601, loss 0.56436.
Train: 2018-08-08T17:45:51.595418: step 1602, loss 0.533611.
Train: 2018-08-08T17:45:54.692653: step 1603, loss 0.500513.
Train: 2018-08-08T17:45:57.788885: step 1604, loss 0.617271.
Train: 2018-08-08T17:46:00.857042: step 1605, loss 0.580816.
Train: 2018-08-08T17:46:03.940239: step 1606, loss 0.579756.
Train: 2018-08-08T17:46:07.028450: step 1607, loss 0.610819.
Train: 2018-08-08T17:46:10.096608: step 1608, loss 0.522415.
Train: 2018-08-08T17:46:13.189832: step 1609, loss 0.569246.
Train: 2018-08-08T17:46:16.271024: step 1610, loss 0.667859.
Test: 2018-08-08T17:46:37.053279: step 1610, loss 0.574872.
Train: 2018-08-08T17:46:40.167558: step 1611, loss 0.54327.
Train: 2018-08-08T17:46:43.310915: step 1612, loss 0.714576.
Train: 2018-08-08T17:46:46.401131: step 1613, loss 0.468047.
Train: 2018-08-08T17:46:49.475305: step 1614, loss 0.517658.
Train: 2018-08-08T17:46:52.563515: step 1615, loss 0.49149.
Train: 2018-08-08T17:46:55.633678: step 1616, loss 0.523632.
Train: 2018-08-08T17:46:58.742945: step 1617, loss 0.512123.
Train: 2018-08-08T17:47:01.855220: step 1618, loss 0.558168.
Train: 2018-08-08T17:47:04.946438: step 1619, loss 0.412639.
Train: 2018-08-08T17:47:08.054702: step 1620, loss 0.621281.
Test: 2018-08-08T17:47:28.851997: step 1620, loss 0.562579.
Train: 2018-08-08T17:47:31.982319: step 1621, loss 0.622758.
Train: 2018-08-08T17:47:35.080556: step 1622, loss 0.464396.
Train: 2018-08-08T17:47:38.183807: step 1623, loss 0.499732.
Train: 2018-08-08T17:47:41.250962: step 1624, loss 0.602502.
Train: 2018-08-08T17:47:44.345188: step 1625, loss 0.544304.
Train: 2018-08-08T17:47:47.445431: step 1626, loss 0.423637.
Train: 2018-08-08T17:47:50.518602: step 1627, loss 0.541541.
Train: 2018-08-08T17:47:53.608818: step 1628, loss 0.545088.
Train: 2018-08-08T17:47:56.782255: step 1629, loss 0.460882.
Train: 2018-08-08T17:47:59.865453: step 1630, loss 0.474928.
Test: 2018-08-08T17:48:20.670769: step 1630, loss 0.562505.
Train: 2018-08-08T17:48:23.769005: step 1631, loss 0.559484.
Train: 2018-08-08T17:48:26.863232: step 1632, loss 0.518049.
Train: 2018-08-08T17:48:29.942419: step 1633, loss 0.485463.
Train: 2018-08-08T17:48:33.034640: step 1634, loss 0.60511.
Train: 2018-08-08T17:48:36.135886: step 1635, loss 0.500014.
Train: 2018-08-08T17:48:39.278240: step 1636, loss 0.595076.
Train: 2018-08-08T17:48:42.368456: step 1637, loss 0.438897.
Train: 2018-08-08T17:48:45.479728: step 1638, loss 0.493391.
Train: 2018-08-08T17:48:48.600024: step 1639, loss 0.406365.
Train: 2018-08-08T17:48:51.686230: step 1640, loss 0.458522.
Test: 2018-08-08T17:49:12.520623: step 1640, loss 0.545065.
Train: 2018-08-08T17:49:15.605825: step 1641, loss 0.553684.
Train: 2018-08-08T17:49:18.710079: step 1642, loss 0.626299.
Train: 2018-08-08T17:49:21.822353: step 1643, loss 0.562177.
Train: 2018-08-08T17:49:24.921593: step 1644, loss 0.528537.
Train: 2018-08-08T17:49:28.007799: step 1645, loss 0.449981.
Train: 2018-08-08T17:49:31.124084: step 1646, loss 0.711366.
Train: 2018-08-08T17:49:34.218311: step 1647, loss 0.554155.
Train: 2018-08-08T17:49:37.272431: step 1648, loss 0.576032.
Train: 2018-08-08T17:49:40.349612: step 1649, loss 0.542456.
Train: 2018-08-08T17:49:43.422783: step 1650, loss 0.498759.
Test: 2018-08-08T17:50:04.254168: step 1650, loss 0.566287.
Train: 2018-08-08T17:50:07.371456: step 1651, loss 0.554673.
Train: 2018-08-08T17:50:10.493757: step 1652, loss 0.503332.
Train: 2018-08-08T17:50:13.584976: step 1653, loss 0.527581.
Train: 2018-08-08T17:50:16.676194: step 1654, loss 0.588145.
Train: 2018-08-08T17:50:19.743349: step 1655, loss 0.55903.
Train: 2018-08-08T17:50:22.839581: step 1656, loss 0.48392.
Train: 2018-08-08T17:50:25.960880: step 1657, loss 0.51265.
Train: 2018-08-08T17:50:29.063128: step 1658, loss 0.50887.
Train: 2018-08-08T17:50:32.188437: step 1659, loss 0.508238.
Train: 2018-08-08T17:50:35.294696: step 1660, loss 0.733197.
Test: 2018-08-08T17:50:56.126081: step 1660, loss 0.567392.
Train: 2018-08-08T17:50:59.343641: step 1661, loss 0.584338.
Train: 2018-08-08T17:51:02.848955: step 1662, loss 0.610707.
Train: 2018-08-08T17:51:06.412766: step 1663, loss 0.489945.
Train: 2018-08-08T17:51:09.936133: step 1664, loss 0.576518.
Train: 2018-08-08T17:51:13.258466: step 1665, loss 0.585138.
Train: 2018-08-08T17:51:16.595840: step 1666, loss 0.596394.
Train: 2018-08-08T17:51:19.918674: step 1667, loss 0.549323.
Train: 2018-08-08T17:51:23.237499: step 1668, loss 0.611878.
Train: 2018-08-08T17:51:26.583394: step 1669, loss 0.510413.
Train: 2018-08-08T17:51:29.915754: step 1670, loss 0.477857.
Test: 2018-08-08T17:51:51.374306: step 1670, loss 0.565433.
Train: 2018-08-08T17:51:54.658037: step 1671, loss 0.427985.
Train: 2018-08-08T17:51:57.988398: step 1672, loss 0.635442.
Train: 2018-08-08T17:52:01.318250: step 1673, loss 0.625368.
Train: 2018-08-08T17:52:04.628552: step 1674, loss 0.598741.
Train: 2018-08-08T17:52:07.957903: step 1675, loss 0.616609.
Train: 2018-08-08T17:52:11.289261: step 1676, loss 0.671167.
Train: 2018-08-08T17:52:14.636159: step 1677, loss 0.552419.
Train: 2018-08-08T17:52:17.945457: step 1678, loss 0.560968.
Train: 2018-08-08T17:52:21.263780: step 1679, loss 0.585146.
Train: 2018-08-08T17:52:24.579095: step 1680, loss 0.564999.
Test: 2018-08-08T17:52:46.045669: step 1680, loss 0.563156.
Train: 2018-08-08T17:52:49.374018: step 1681, loss 0.556837.
Train: 2018-08-08T17:52:52.699860: step 1682, loss 0.597649.
Train: 2018-08-08T17:52:56.017681: step 1683, loss 0.619409.
Train: 2018-08-08T17:52:59.343523: step 1684, loss 0.595231.
Train: 2018-08-08T17:53:02.668364: step 1685, loss 0.634094.
Train: 2018-08-08T17:53:06.029800: step 1686, loss 0.57757.
Train: 2018-08-08T17:53:09.361157: step 1687, loss 0.564215.
Train: 2018-08-08T17:53:12.688504: step 1688, loss 0.531727.
Train: 2018-08-08T17:53:16.010336: step 1689, loss 0.58912.
Train: 2018-08-08T17:53:19.307602: step 1690, loss 0.570414.
Test: 2018-08-08T17:53:40.878453: step 1690, loss 0.553273.
Train: 2018-08-08T17:53:44.197277: step 1691, loss 0.522196.
Train: 2018-08-08T17:53:47.541168: step 1692, loss 0.5206.
Train: 2018-08-08T17:53:50.860493: step 1693, loss 0.660696.
Train: 2018-08-08T17:53:54.195359: step 1694, loss 0.58678.
Train: 2018-08-08T17:53:57.519698: step 1695, loss 0.635606.
Train: 2018-08-08T17:54:00.814959: step 1696, loss 0.593931.
Train: 2018-08-08T17:54:04.155846: step 1697, loss 0.66791.
Train: 2018-08-08T17:54:07.529315: step 1698, loss 0.508788.
Train: 2018-08-08T17:54:10.881227: step 1699, loss 0.662426.
Train: 2018-08-08T17:54:14.217096: step 1700, loss 0.519596.
Test: 2018-08-08T17:54:35.701216: step 1700, loss 0.559269.
Train: 2018-08-08T17:54:40.851409: step 1701, loss 0.572212.
Train: 2018-08-08T17:54:44.156697: step 1702, loss 0.541259.
Train: 2018-08-08T17:54:47.470007: step 1703, loss 0.554275.
Train: 2018-08-08T17:54:50.782313: step 1704, loss 0.482363.
Train: 2018-08-08T17:54:54.131718: step 1705, loss 0.491855.
Train: 2018-08-08T17:54:57.450542: step 1706, loss 0.573713.
Train: 2018-08-08T17:55:00.754827: step 1707, loss 0.669252.
Train: 2018-08-08T17:55:04.092200: step 1708, loss 0.528494.
Train: 2018-08-08T17:55:07.444613: step 1709, loss 0.550769.
Train: 2018-08-08T17:55:10.765442: step 1710, loss 0.456475.
Test: 2018-08-08T17:55:32.257588: step 1710, loss 0.551592.
Train: 2018-08-08T17:55:35.600977: step 1711, loss 0.455158.
Train: 2018-08-08T17:55:38.920803: step 1712, loss 0.461443.
Train: 2018-08-08T17:55:42.229099: step 1713, loss 0.65211.
Train: 2018-08-08T17:55:45.566974: step 1714, loss 0.75024.
Train: 2018-08-08T17:55:48.885296: step 1715, loss 0.477599.
Train: 2018-08-08T17:55:52.210638: step 1716, loss 0.511935.
Train: 2018-08-08T17:55:55.516426: step 1717, loss 0.530245.
Train: 2018-08-08T17:55:58.837757: step 1718, loss 0.566085.
Train: 2018-08-08T17:56:02.160592: step 1719, loss 0.542075.
Train: 2018-08-08T17:56:05.475405: step 1720, loss 0.508755.
Test: 2018-08-08T17:56:26.979578: step 1720, loss 0.554365.
Train: 2018-08-08T17:56:30.274840: step 1721, loss 0.552595.
Train: 2018-08-08T17:56:33.601183: step 1722, loss 0.60866.
Train: 2018-08-08T17:56:36.927527: step 1723, loss 0.490838.
Train: 2018-08-08T17:56:40.247855: step 1724, loss 0.519973.
Train: 2018-08-08T17:56:43.556652: step 1725, loss 0.527585.
Train: 2018-08-08T17:56:46.875978: step 1726, loss 0.548213.
Train: 2018-08-08T17:56:50.210343: step 1727, loss 0.59947.
Train: 2018-08-08T17:56:53.535182: step 1728, loss 0.628162.
Train: 2018-08-08T17:56:56.861526: step 1729, loss 0.560038.
Train: 2018-08-08T17:57:00.166313: step 1730, loss 0.620983.
Test: 2018-08-08T17:57:21.667478: step 1730, loss 0.55293.
Train: 2018-08-08T17:57:25.053481: step 1731, loss 0.619514.
Train: 2018-08-08T17:57:28.411910: step 1732, loss 0.652297.
Train: 2018-08-08T17:57:31.736750: step 1733, loss 0.552654.
Train: 2018-08-08T17:57:35.053568: step 1734, loss 0.59199.
Train: 2018-08-08T17:57:38.380915: step 1735, loss 0.62344.
Train: 2018-08-08T17:57:41.693221: step 1736, loss 0.526196.
Train: 2018-08-08T17:57:45.009538: step 1737, loss 0.489025.
Train: 2018-08-08T17:57:48.335381: step 1738, loss 0.50676.
Train: 2018-08-08T17:57:51.668744: step 1739, loss 0.70663.
Train: 2018-08-08T17:57:54.967514: step 1740, loss 0.502759.
Test: 2018-08-08T17:58:16.480214: step 1740, loss 0.554748.
Train: 2018-08-08T17:58:19.822609: step 1741, loss 0.547441.
Train: 2018-08-08T17:58:23.150958: step 1742, loss 0.592941.
Train: 2018-08-08T17:58:26.478305: step 1743, loss 0.592061.
Train: 2018-08-08T17:58:29.798632: step 1744, loss 0.444641.
Train: 2018-08-08T17:58:33.110939: step 1745, loss 0.485215.
Train: 2018-08-08T17:58:36.436781: step 1746, loss 0.558626.
Train: 2018-08-08T17:58:39.758112: step 1747, loss 0.556708.
Train: 2018-08-08T17:58:43.074930: step 1748, loss 0.536203.
Train: 2018-08-08T17:58:46.388240: step 1749, loss 0.576567.
Train: 2018-08-08T17:58:49.706562: step 1750, loss 0.613262.
Test: 2018-08-08T17:59:11.228282: step 1750, loss 0.551023.
Train: 2018-08-08T17:59:14.532066: step 1751, loss 0.534148.
Train: 2018-08-08T17:59:17.839359: step 1752, loss 0.583417.
Train: 2018-08-08T17:59:21.160690: step 1753, loss 0.573996.
Train: 2018-08-08T17:59:24.483023: step 1754, loss 0.501614.
Train: 2018-08-08T17:59:27.816887: step 1755, loss 0.564272.
Train: 2018-08-08T17:59:31.178825: step 1756, loss 0.612282.
Train: 2018-08-08T17:59:34.506172: step 1757, loss 0.602074.
Train: 2018-08-08T17:59:37.835023: step 1758, loss 0.554408.
Train: 2018-08-08T17:59:41.129281: step 1759, loss 0.573127.
Train: 2018-08-08T17:59:44.442089: step 1760, loss 0.481541.
Test: 2018-08-08T18:00:06.035499: step 1760, loss 0.551231.
Train: 2018-08-08T18:00:09.349811: step 1761, loss 0.444257.
Train: 2018-08-08T18:00:12.695206: step 1762, loss 0.476016.
Train: 2018-08-08T18:00:16.014531: step 1763, loss 0.56242.
Train: 2018-08-08T18:00:19.385493: step 1764, loss 0.585078.
Train: 2018-08-08T18:00:22.728381: step 1765, loss 0.541384.
Train: 2018-08-08T18:00:26.129424: step 1766, loss 0.514634.
Train: 2018-08-08T18:00:29.470306: step 1767, loss 0.560283.
Train: 2018-08-08T18:00:32.804171: step 1768, loss 0.587247.
Train: 2018-08-08T18:00:36.160093: step 1769, loss 0.535204.
Train: 2018-08-08T18:00:39.494458: step 1770, loss 0.601867.
Test: 2018-08-08T18:01:01.027210: step 1770, loss 0.553949.
Train: 2018-08-08T18:01:04.353555: step 1771, loss 0.5906.
Train: 2018-08-08T18:01:07.713989: step 1772, loss 0.595217.
Train: 2018-08-08T18:01:11.047853: step 1773, loss 0.624046.
Train: 2018-08-08T18:01:14.360661: step 1774, loss 0.552354.
Train: 2018-08-08T18:01:17.683996: step 1775, loss 0.581316.
Train: 2018-08-08T18:01:21.015855: step 1776, loss 0.506443.
Train: 2018-08-08T18:01:24.358242: step 1777, loss 0.578602.
Train: 2018-08-08T18:01:27.710154: step 1778, loss 0.711312.
Train: 2018-08-08T18:01:31.030481: step 1779, loss 0.605091.
Train: 2018-08-08T18:01:34.395929: step 1780, loss 0.561855.
Test: 2018-08-08T18:01:55.936700: step 1780, loss 0.55452.
Train: 2018-08-08T18:01:59.257529: step 1781, loss 0.578277.
Train: 2018-08-08T18:02:02.577857: step 1782, loss 0.58893.
Train: 2018-08-08T18:02:05.920243: step 1783, loss 0.445906.
Train: 2018-08-08T18:02:09.255110: step 1784, loss 0.561914.
Train: 2018-08-08T18:02:12.594488: step 1785, loss 0.578903.
Train: 2018-08-08T18:02:15.896768: step 1786, loss 0.588007.
Train: 2018-08-08T18:02:19.210578: step 1787, loss 0.694233.
Train: 2018-08-08T18:02:22.532912: step 1788, loss 0.556231.
Train: 2018-08-08T18:02:25.883320: step 1789, loss 0.620594.
Train: 2018-08-08T18:02:29.191615: step 1790, loss 0.589791.
Test: 2018-08-08T18:02:50.702813: step 1790, loss 0.554911.
Train: 2018-08-08T18:02:54.028656: step 1791, loss 0.552545.
Train: 2018-08-08T18:02:57.412653: step 1792, loss 0.548546.
Train: 2018-08-08T18:03:00.740501: step 1793, loss 0.56585.
Train: 2018-08-08T18:03:04.062332: step 1794, loss 0.580964.
Train: 2018-08-08T18:03:07.395695: step 1795, loss 0.626329.
Train: 2018-08-08T18:03:10.730561: step 1796, loss 0.567176.
Train: 2018-08-08T18:03:14.052895: step 1797, loss 0.563276.
Train: 2018-08-08T18:03:17.392774: step 1798, loss 0.556265.
Train: 2018-08-08T18:03:20.699065: step 1799, loss 0.576403.
Train: 2018-08-08T18:03:24.033430: step 1800, loss 0.572041.
Test: 2018-08-08T18:03:45.680985: step 1800, loss 0.55905.
Train: 2018-08-08T18:03:50.891338: step 1801, loss 0.490694.
Train: 2018-08-08T18:03:54.229713: step 1802, loss 0.533055.
Train: 2018-08-08T18:03:57.625743: step 1803, loss 0.507084.
Train: 2018-08-08T18:04:00.929527: step 1804, loss 0.525689.
Train: 2018-08-08T18:04:04.276425: step 1805, loss 0.57321.
Train: 2018-08-08T18:04:07.414769: step 1806, loss 0.459367.
Train: 2018-08-08T18:04:10.743620: step 1807, loss 0.576668.
Train: 2018-08-08T18:04:14.053920: step 1808, loss 0.556258.
Train: 2018-08-08T18:04:17.431910: step 1809, loss 0.662768.
Train: 2018-08-08T18:04:20.754243: step 1810, loss 0.626495.
Test: 2018-08-08T18:04:42.316070: step 1810, loss 0.556567.
Train: 2018-08-08T18:04:45.658456: step 1811, loss 0.619486.
Train: 2018-08-08T18:04:48.986304: step 1812, loss 0.528247.
Train: 2018-08-08T18:04:52.338717: step 1813, loss 0.593371.
Train: 2018-08-08T18:04:55.747281: step 1814, loss 0.540594.
Train: 2018-08-08T18:04:59.064098: step 1815, loss 0.549221.
Train: 2018-08-08T18:05:02.414506: step 1816, loss 0.519864.
Train: 2018-08-08T18:05:05.758898: step 1817, loss 0.471002.
Train: 2018-08-08T18:05:09.072708: step 1818, loss 0.581021.
Train: 2018-08-08T18:05:12.406572: step 1819, loss 0.548502.
Train: 2018-08-08T18:05:15.795081: step 1820, loss 0.512299.
Test: 2018-08-08T18:05:37.376460: step 1820, loss 0.554509.
Train: 2018-08-08T18:05:40.723359: step 1821, loss 0.565059.
Train: 2018-08-08T18:05:44.052209: step 1822, loss 0.519283.
Train: 2018-08-08T18:05:47.388078: step 1823, loss 0.607061.
Train: 2018-08-08T18:05:50.730465: step 1824, loss 0.638788.
Train: 2018-08-08T18:05:54.058814: step 1825, loss 0.570148.
Train: 2018-08-08T18:05:57.402704: step 1826, loss 0.593472.
Train: 2018-08-08T18:06:00.712504: step 1827, loss 0.71838.
Train: 2018-08-08T18:06:04.046368: step 1828, loss 0.559241.
Train: 2018-08-08T18:06:07.355667: step 1829, loss 0.560269.
Train: 2018-08-08T18:06:10.714597: step 1830, loss 0.506166.
Test: 2018-08-08T18:06:32.298984: step 1830, loss 0.556879.
Train: 2018-08-08T18:06:35.674458: step 1831, loss 0.487655.
Train: 2018-08-08T18:06:39.003810: step 1832, loss 0.522212.
Train: 2018-08-08T18:06:42.334666: step 1833, loss 0.510924.
Train: 2018-08-08T18:06:45.668530: step 1834, loss 0.551242.
Train: 2018-08-08T18:06:49.001391: step 1835, loss 0.516124.
Train: 2018-08-08T18:06:52.336257: step 1836, loss 0.551278.
Train: 2018-08-08T18:06:55.675134: step 1837, loss 0.545603.
Train: 2018-08-08T18:06:59.002481: step 1838, loss 0.598471.
Train: 2018-08-08T18:07:02.356900: step 1839, loss 0.508978.
Train: 2018-08-08T18:07:05.706806: step 1840, loss 0.717226.
Test: 2018-08-08T18:07:27.231033: step 1840, loss 0.556593.
Train: 2018-08-08T18:07:30.564897: step 1841, loss 0.454462.
Train: 2018-08-08T18:07:33.908787: step 1842, loss 0.630847.
Train: 2018-08-08T18:07:37.213574: step 1843, loss 0.515959.
Train: 2018-08-08T18:07:40.545934: step 1844, loss 0.542875.
Train: 2018-08-08T18:07:43.841697: step 1845, loss 0.64856.
Train: 2018-08-08T18:07:47.159016: step 1846, loss 0.566562.
Train: 2018-08-08T18:07:50.489371: step 1847, loss 0.497033.
Train: 2018-08-08T18:07:53.814712: step 1848, loss 0.54557.
Train: 2018-08-08T18:07:57.145068: step 1849, loss 0.504839.
Train: 2018-08-08T18:08:00.509010: step 1850, loss 0.566347.
Test: 2018-08-08T18:08:22.023210: step 1850, loss 0.557469.
Train: 2018-08-08T18:08:25.364093: step 1851, loss 0.601335.
Train: 2018-08-08T18:08:28.697956: step 1852, loss 0.456012.
Train: 2018-08-08T18:08:32.024300: step 1853, loss 0.593497.
Train: 2018-08-08T18:08:35.331594: step 1854, loss 0.471349.
Train: 2018-08-08T18:08:38.674481: step 1855, loss 0.567922.
Train: 2018-08-08T18:08:42.014863: step 1856, loss 0.662218.
Train: 2018-08-08T18:08:45.343212: step 1857, loss 0.586038.
Train: 2018-08-08T18:08:48.684094: step 1858, loss 0.54597.
Train: 2018-08-08T18:08:52.007432: step 1859, loss 0.601487.
Train: 2018-08-08T18:08:55.363854: step 1860, loss 0.552487.
Test: 2018-08-08T18:09:16.920667: step 1860, loss 0.560339.
Train: 2018-08-08T18:09:20.279097: step 1861, loss 0.619399.
Train: 2018-08-08T18:09:23.600427: step 1862, loss 0.557351.
Train: 2018-08-08T18:09:26.940307: step 1863, loss 0.487098.
Train: 2018-08-08T18:09:30.273168: step 1864, loss 0.601045.
Train: 2018-08-08T18:09:33.632600: step 1865, loss 0.553772.
Train: 2018-08-08T18:09:36.948416: step 1866, loss 0.665803.
Train: 2018-08-08T18:09:40.278269: step 1867, loss 0.46494.
Train: 2018-08-08T18:09:43.592079: step 1868, loss 0.490735.
Train: 2018-08-08T18:09:46.923437: step 1869, loss 0.541395.
Train: 2018-08-08T18:09:50.248276: step 1870, loss 0.725178.
Test: 2018-08-08T18:10:11.922402: step 1870, loss 0.561905.
Train: 2018-08-08T18:10:15.276820: step 1871, loss 0.65664.
Train: 2018-08-08T18:10:18.605671: step 1872, loss 0.685945.
Train: 2018-08-08T18:10:22.091440: step 1873, loss 0.560584.
Train: 2018-08-08T18:10:25.490475: step 1874, loss 0.566506.
Train: 2018-08-08T18:10:28.820830: step 1875, loss 0.520229.
Train: 2018-08-08T18:10:32.170236: step 1876, loss 0.677766.
Train: 2018-08-08T18:10:35.505102: step 1877, loss 0.587468.
Train: 2018-08-08T18:10:38.852000: step 1878, loss 0.570523.
Train: 2018-08-08T18:10:42.176840: step 1879, loss 0.484099.
Train: 2018-08-08T18:10:45.521232: step 1880, loss 0.57235.
Test: 2018-08-08T18:11:07.061501: step 1880, loss 0.561528.
Train: 2018-08-08T18:11:10.376816: step 1881, loss 0.567028.
Train: 2018-08-08T18:11:13.738754: step 1882, loss 0.600891.
Train: 2018-08-08T18:11:17.067103: step 1883, loss 0.585984.
Train: 2018-08-08T18:11:20.399463: step 1884, loss 0.515701.
Train: 2018-08-08T18:11:23.745359: step 1885, loss 0.571818.
Train: 2018-08-08T18:11:27.091255: step 1886, loss 0.567573.
Train: 2018-08-08T18:11:30.444671: step 1887, loss 0.540015.
Train: 2018-08-08T18:11:33.750460: step 1888, loss 0.60994.
Train: 2018-08-08T18:11:37.038703: step 1889, loss 0.430747.
Train: 2018-08-08T18:11:40.369558: step 1890, loss 0.565047.
Test: 2018-08-08T18:12:01.898297: step 1890, loss 0.555575.
Train: 2018-08-08T18:12:05.233665: step 1891, loss 0.593659.
Train: 2018-08-08T18:12:08.589588: step 1892, loss 0.529711.
Train: 2018-08-08T18:12:11.908411: step 1893, loss 0.507498.
Train: 2018-08-08T18:12:15.249294: step 1894, loss 0.520769.
Train: 2018-08-08T18:12:18.585665: step 1895, loss 0.592473.
Train: 2018-08-08T18:12:21.939080: step 1896, loss 0.56831.
Train: 2018-08-08T18:12:25.283973: step 1897, loss 0.500956.
Train: 2018-08-08T18:12:28.630371: step 1898, loss 0.577413.
Train: 2018-08-08T18:12:31.934656: step 1899, loss 0.613827.
Train: 2018-08-08T18:12:35.259997: step 1900, loss 0.619268.
Test: 2018-08-08T18:12:56.827339: step 1900, loss 0.560702.
Train: 2018-08-08T18:13:01.975025: step 1901, loss 0.568819.
Train: 2018-08-08T18:13:05.309390: step 1902, loss 0.566918.
Train: 2018-08-08T18:13:08.630721: step 1903, loss 0.722735.
Train: 2018-08-08T18:13:11.933000: step 1904, loss 0.602734.
Train: 2018-08-08T18:13:15.258843: step 1905, loss 0.57809.
Train: 2018-08-08T18:13:18.576664: step 1906, loss 0.501666.
Train: 2018-08-08T18:13:21.909525: step 1907, loss 0.544335.
Train: 2018-08-08T18:13:25.256424: step 1908, loss 0.49609.
Train: 2018-08-08T18:13:28.606330: step 1909, loss 0.576412.
Train: 2018-08-08T18:13:31.944205: step 1910, loss 0.541382.
Test: 2018-08-08T18:13:53.482970: step 1910, loss 0.55781.
Train: 2018-08-08T18:13:56.787757: step 1911, loss 0.702948.
Train: 2018-08-08T18:14:00.131146: step 1912, loss 0.493913.
Train: 2018-08-08T18:14:03.492583: step 1913, loss 0.66533.
Train: 2018-08-08T18:14:06.838980: step 1914, loss 0.529657.
Train: 2018-08-08T18:14:10.196908: step 1915, loss 0.587469.
Train: 2018-08-08T18:14:13.536286: step 1916, loss 0.512161.
Train: 2018-08-08T18:14:16.856614: step 1917, loss 0.546781.
Train: 2018-08-08T18:14:20.153379: step 1918, loss 0.558995.
Train: 2018-08-08T18:14:23.538379: step 1919, loss 0.639501.
Train: 2018-08-08T18:14:26.867732: step 1920, loss 0.543702.
Test: 2018-08-08T18:14:48.402486: step 1920, loss 0.560333.
Train: 2018-08-08T18:14:51.816563: step 1921, loss 0.66247.
Train: 2018-08-08T18:14:55.199558: step 1922, loss 0.455345.
Train: 2018-08-08T18:14:58.536429: step 1923, loss 0.497801.
Train: 2018-08-08T18:15:01.884331: step 1924, loss 0.570173.
Train: 2018-08-08T18:15:05.216690: step 1925, loss 0.424243.
Train: 2018-08-08T18:15:08.544037: step 1926, loss 0.528647.
Train: 2018-08-08T18:15:11.898459: step 1927, loss 0.554766.
Train: 2018-08-08T18:15:15.208760: step 1928, loss 0.534799.
Train: 2018-08-08T18:15:18.547136: step 1929, loss 0.685797.
Train: 2018-08-08T18:15:21.873981: step 1930, loss 0.576358.
Test: 2018-08-08T18:15:43.464885: step 1930, loss 0.557511.
Train: 2018-08-08T18:15:46.782706: step 1931, loss 0.551726.
Train: 2018-08-08T18:15:50.115066: step 1932, loss 0.472516.
Train: 2018-08-08T18:15:53.432386: step 1933, loss 0.516629.
Train: 2018-08-08T18:15:56.762740: step 1934, loss 0.573898.
Train: 2018-08-08T18:16:00.073543: step 1935, loss 0.539119.
Train: 2018-08-08T18:16:03.408911: step 1936, loss 0.505244.
Train: 2018-08-08T18:16:06.733249: step 1937, loss 0.471063.
Train: 2018-08-08T18:16:10.066110: step 1938, loss 0.522281.
Train: 2018-08-08T18:16:13.373404: step 1939, loss 0.531254.
Train: 2018-08-08T18:16:16.700750: step 1940, loss 0.662139.
Test: 2018-08-08T18:16:38.252553: step 1940, loss 0.554589.
Train: 2018-08-08T18:16:41.565863: step 1941, loss 0.499551.
Train: 2018-08-08T18:16:44.928804: step 1942, loss 0.503138.
Train: 2018-08-08T18:16:48.242624: step 1943, loss 0.589027.
Train: 2018-08-08T18:16:51.575986: step 1944, loss 0.532901.
Train: 2018-08-08T18:16:54.892303: step 1945, loss 0.410844.
Train: 2018-08-08T18:16:58.224161: step 1946, loss 0.588937.
Train: 2018-08-08T18:17:01.531956: step 1947, loss 0.612291.
Train: 2018-08-08T18:17:04.842758: step 1948, loss 0.617779.
Train: 2018-08-08T18:17:08.170606: step 1949, loss 0.558352.
Train: 2018-08-08T18:17:11.486422: step 1950, loss 0.651495.
Test: 2018-08-08T18:17:33.040228: step 1950, loss 0.558775.
Train: 2018-08-08T18:17:36.440268: step 1951, loss 0.509636.
Train: 2018-08-08T18:17:39.776638: step 1952, loss 0.554613.
Train: 2018-08-08T18:17:43.121030: step 1953, loss 0.60493.
Train: 2018-08-08T18:17:46.426819: step 1954, loss 0.572615.
Train: 2018-08-08T18:17:49.753664: step 1955, loss 0.618838.
Train: 2018-08-08T18:17:53.099560: step 1956, loss 0.575619.
Train: 2018-08-08T18:17:56.484058: step 1957, loss 0.504421.
Train: 2018-08-08T18:17:59.805390: step 1958, loss 0.559649.
Train: 2018-08-08T18:18:03.134239: step 1959, loss 0.43659.
Train: 2018-08-08T18:18:06.457575: step 1960, loss 0.552748.
Test: 2018-08-08T18:18:28.012885: step 1960, loss 0.559332.
Train: 2018-08-08T18:18:31.344744: step 1961, loss 0.597377.
Train: 2018-08-08T18:18:34.694149: step 1962, loss 0.508587.
Train: 2018-08-08T18:18:38.042050: step 1963, loss 0.49653.
Train: 2018-08-08T18:18:41.354858: step 1964, loss 0.546665.
Train: 2018-08-08T18:18:44.662151: step 1965, loss 0.543353.
Train: 2018-08-08T18:18:48.062190: step 1966, loss 0.539315.
Train: 2018-08-08T18:18:51.364971: step 1967, loss 0.618498.
Train: 2018-08-08T18:18:54.691316: step 1968, loss 0.489116.
Train: 2018-08-08T18:18:58.035707: step 1969, loss 0.564363.
Train: 2018-08-08T18:19:01.361550: step 1970, loss 0.592056.
Test: 2018-08-08T18:19:22.956465: step 1970, loss 0.560946.
Train: 2018-08-08T18:19:26.278798: step 1971, loss 0.556412.
Train: 2018-08-08T18:19:29.632213: step 1972, loss 0.549617.
Train: 2018-08-08T18:19:32.972595: step 1973, loss 0.525287.
Train: 2018-08-08T18:19:36.297936: step 1974, loss 0.581306.
Train: 2018-08-08T18:19:39.652856: step 1975, loss 0.487127.
Train: 2018-08-08T18:19:42.960650: step 1976, loss 0.515322.
Train: 2018-08-08T18:19:46.288498: step 1977, loss 0.568244.
Train: 2018-08-08T18:19:49.604314: step 1978, loss 0.457824.
Train: 2018-08-08T18:19:52.923639: step 1979, loss 0.560689.
Train: 2018-08-08T18:19:56.253492: step 1980, loss 0.500908.
Test: 2018-08-08T18:20:17.915089: step 1980, loss 0.561192.
Train: 2018-08-08T18:20:21.276025: step 1981, loss 0.451516.
Train: 2018-08-08T18:20:24.626934: step 1982, loss 0.490148.
Train: 2018-08-08T18:20:27.998398: step 1983, loss 0.416362.
Train: 2018-08-08T18:20:31.326245: step 1984, loss 0.589908.
Train: 2018-08-08T18:20:34.669635: step 1985, loss 0.588651.
Train: 2018-08-08T18:20:37.991467: step 1986, loss 0.418557.
Train: 2018-08-08T18:20:41.332851: step 1987, loss 0.532256.
Train: 2018-08-08T18:20:44.710832: step 1988, loss 0.444929.
Train: 2018-08-08T18:20:48.056727: step 1989, loss 0.595272.
Train: 2018-08-08T18:20:51.386079: step 1990, loss 0.578152.
Test: 2018-08-08T18:21:12.974477: step 1990, loss 0.560878.
Train: 2018-08-08T18:21:16.313354: step 1991, loss 0.638701.
Train: 2018-08-08T18:21:19.631183: step 1992, loss 0.425098.
Train: 2018-08-08T18:21:22.950007: step 1993, loss 0.442724.
Train: 2018-08-08T18:21:26.280361: step 1994, loss 0.698492.
Train: 2018-08-08T18:21:29.630268: step 1995, loss 0.558715.
Train: 2018-08-08T18:21:32.973158: step 1996, loss 0.591331.
Train: 2018-08-08T18:21:36.312537: step 1997, loss 0.522404.
Train: 2018-08-08T18:21:39.619830: step 1998, loss 0.48661.
Train: 2018-08-08T18:21:42.987283: step 1999, loss 0.642379.
Train: 2018-08-08T18:21:46.300091: step 2000, loss 0.48962.
Test: 2018-08-08T18:22:07.833843: step 2000, loss 0.556827.
Train: 2018-08-08T18:22:12.899812: step 2001, loss 0.568404.
Train: 2018-08-08T18:22:16.222145: step 2002, loss 0.521634.
Train: 2018-08-08T18:22:19.549996: step 2003, loss 0.558061.
Train: 2018-08-08T18:22:22.880852: step 2004, loss 0.549566.
Train: 2018-08-08T18:22:26.217222: step 2005, loss 0.553729.
Train: 2018-08-08T18:22:29.563619: step 2006, loss 0.5423.
Train: 2018-08-08T18:22:32.877931: step 2007, loss 0.475055.
Train: 2018-08-08T18:22:36.204776: step 2008, loss 0.493878.
Train: 2018-08-08T18:22:39.518587: step 2009, loss 0.59647.
Train: 2018-08-08T18:22:42.843928: step 2010, loss 0.561427.
Test: 2018-08-08T18:23:04.397236: step 2010, loss 0.556962.
Train: 2018-08-08T18:23:07.734108: step 2011, loss 0.440087.
Train: 2018-08-08T18:23:11.105071: step 2012, loss 0.62231.
Train: 2018-08-08T18:23:14.443447: step 2013, loss 0.601819.
Train: 2018-08-08T18:23:17.787337: step 2014, loss 0.64466.
Train: 2018-08-08T18:23:21.123707: step 2015, loss 0.545108.
Train: 2018-08-08T18:23:24.494169: step 2016, loss 0.514956.
Train: 2018-08-08T18:23:27.826027: step 2017, loss 0.648535.
Train: 2018-08-08T18:23:31.167411: step 2018, loss 0.589717.
Train: 2018-08-08T18:23:34.503280: step 2019, loss 0.530841.
Train: 2018-08-08T18:23:37.853187: step 2020, loss 0.531584.
Test: 2018-08-08T18:23:59.458128: step 2020, loss 0.561139.
Train: 2018-08-08T18:24:02.810542: step 2021, loss 0.569146.
Train: 2018-08-08T18:24:06.201557: step 2022, loss 0.518585.
Train: 2018-08-08T18:24:09.551965: step 2023, loss 0.562852.
Train: 2018-08-08T18:24:12.911898: step 2024, loss 0.626184.
Train: 2018-08-08T18:24:16.243756: step 2025, loss 0.466224.
Train: 2018-08-08T18:24:19.582634: step 2026, loss 0.553381.
Train: 2018-08-08T18:24:22.907473: step 2027, loss 0.562408.
Train: 2018-08-08T18:24:26.225295: step 2028, loss 0.577536.
Train: 2018-08-08T18:24:29.569687: step 2029, loss 0.506171.
Train: 2018-08-08T18:24:32.914580: step 2030, loss 0.471737.
Test: 2018-08-08T18:24:54.544086: step 2030, loss 0.554277.
Train: 2018-08-08T18:24:57.902014: step 2031, loss 0.459533.
Train: 2018-08-08T18:25:01.239387: step 2032, loss 0.390064.
Train: 2018-08-08T18:25:04.567235: step 2033, loss 0.75836.
Train: 2018-08-08T18:25:07.892576: step 2034, loss 0.46898.
Train: 2018-08-08T18:25:11.239976: step 2035, loss 0.445888.
Train: 2018-08-08T18:25:14.570331: step 2036, loss 0.673791.
Train: 2018-08-08T18:25:17.904194: step 2037, loss 0.51354.
Train: 2018-08-08T18:25:21.206474: step 2038, loss 0.472077.
Train: 2018-08-08T18:25:24.535826: step 2039, loss 0.483603.
Train: 2018-08-08T18:25:27.898767: step 2040, loss 0.5403.
Test: 2018-08-08T18:25:49.489185: step 2040, loss 0.554916.
Train: 2018-08-08T18:25:52.838088: step 2041, loss 0.608281.
Train: 2018-08-08T18:25:56.196016: step 2042, loss 0.471952.
Train: 2018-08-08T18:25:59.534392: step 2043, loss 0.452042.
Train: 2018-08-08T18:26:02.865749: step 2044, loss 0.551088.
Train: 2018-08-08T18:26:06.203624: step 2045, loss 0.582494.
Train: 2018-08-08T18:26:09.532976: step 2046, loss 0.51831.
Train: 2018-08-08T18:26:12.872363: step 2047, loss 0.557467.
Train: 2018-08-08T18:26:16.201714: step 2048, loss 0.532901.
Train: 2018-08-08T18:26:19.524549: step 2049, loss 0.516459.
Train: 2018-08-08T18:26:22.830840: step 2050, loss 0.550997.
Test: 2018-08-08T18:26:44.414224: step 2050, loss 0.553812.
Train: 2018-08-08T18:26:47.733549: step 2051, loss 0.558381.
Train: 2018-08-08T18:26:51.052373: step 2052, loss 0.533957.
Train: 2018-08-08T18:26:54.371197: step 2053, loss 0.651517.
Train: 2018-08-08T18:26:57.683503: step 2054, loss 0.615964.
Train: 2018-08-08T18:27:01.002327: step 2055, loss 0.503876.
Train: 2018-08-08T18:27:04.337193: step 2056, loss 0.472016.
Train: 2018-08-08T18:27:07.667047: step 2057, loss 0.55857.
Train: 2018-08-08T18:27:10.985870: step 2058, loss 0.613274.
Train: 2018-08-08T18:27:14.349814: step 2059, loss 0.503011.
Train: 2018-08-08T18:27:17.679667: step 2060, loss 0.458352.
Test: 2018-08-08T18:27:39.289622: step 2060, loss 0.552976.
Train: 2018-08-08T18:27:42.612457: step 2061, loss 0.522976.
Train: 2018-08-08T18:27:45.950331: step 2062, loss 0.535553.
Train: 2018-08-08T18:27:49.263641: step 2063, loss 0.383171.
Train: 2018-08-08T18:27:52.593995: step 2064, loss 0.586443.
Train: 2018-08-08T18:27:55.937886: step 2065, loss 0.612561.
Train: 2018-08-08T18:27:59.255707: step 2066, loss 0.478289.
Train: 2018-08-08T18:28:02.582552: step 2067, loss 0.589129.
Train: 2018-08-08T18:28:05.902378: step 2068, loss 0.458887.
Train: 2018-08-08T18:28:09.232733: step 2069, loss 0.650664.
Train: 2018-08-08T18:28:12.552559: step 2070, loss 0.52702.
Test: 2018-08-08T18:28:34.090824: step 2070, loss 0.556509.
Train: 2018-08-08T18:28:37.432709: step 2071, loss 0.458304.
Train: 2018-08-08T18:28:40.778103: step 2072, loss 0.549373.
Train: 2018-08-08T18:28:44.091915: step 2073, loss 0.54725.
Train: 2018-08-08T18:28:47.432295: step 2074, loss 0.545172.
Train: 2018-08-08T18:28:50.735578: step 2075, loss 0.518709.
Train: 2018-08-08T18:28:54.079468: step 2076, loss 0.476125.
Train: 2018-08-08T18:28:57.400798: step 2077, loss 0.550796.
Train: 2018-08-08T18:29:00.721126: step 2078, loss 0.530564.
Train: 2018-08-08T18:29:04.069529: step 2079, loss 0.548442.
Train: 2018-08-08T18:29:07.402390: step 2080, loss 0.590667.
Test: 2018-08-08T18:29:29.027389: step 2080, loss 0.553277.
Train: 2018-08-08T18:29:32.345210: step 2081, loss 0.512627.
Train: 2018-08-08T18:29:35.698626: step 2082, loss 0.60548.
Train: 2018-08-08T18:29:39.025471: step 2083, loss 0.546781.
Train: 2018-08-08T18:29:42.343794: step 2084, loss 0.535655.
Train: 2018-08-08T18:29:45.669135: step 2085, loss 0.607276.
Train: 2018-08-08T18:29:49.003500: step 2086, loss 0.536642.
Train: 2018-08-08T18:29:52.339369: step 2087, loss 0.544757.
Train: 2018-08-08T18:29:55.670226: step 2088, loss 0.645517.
Train: 2018-08-08T18:29:58.974009: step 2089, loss 0.571148.
Train: 2018-08-08T18:30:02.432204: step 2090, loss 0.476194.
Test: 2018-08-08T18:30:24.015087: step 2090, loss 0.557284.
Train: 2018-08-08T18:30:27.399090: step 2091, loss 0.570423.
Train: 2018-08-08T18:30:30.750500: step 2092, loss 0.45877.
Train: 2018-08-08T18:30:34.077346: step 2093, loss 0.457643.
Train: 2018-08-08T18:30:37.406196: step 2094, loss 0.578802.
Train: 2018-08-08T18:30:40.751591: step 2095, loss 0.560571.
Train: 2018-08-08T18:30:44.077433: step 2096, loss 0.5742.
Train: 2018-08-08T18:30:47.397762: step 2097, loss 0.562405.
Train: 2018-08-08T18:30:50.725613: step 2098, loss 0.475746.
Train: 2018-08-08T18:30:54.059979: step 2099, loss 0.454889.
Train: 2018-08-08T18:30:57.378802: step 2100, loss 0.542805.
Test: 2018-08-08T18:31:18.946145: step 2100, loss 0.557676.
Train: 2018-08-08T18:31:24.152989: step 2101, loss 0.632636.
Train: 2018-08-08T18:31:27.437725: step 2102, loss 0.546888.
Train: 2018-08-08T18:31:30.763066: step 2103, loss 0.544488.
Train: 2018-08-08T18:31:34.063842: step 2104, loss 0.723441.
Train: 2018-08-08T18:31:37.389183: step 2105, loss 0.524943.
Train: 2018-08-08T18:31:40.724049: step 2106, loss 0.618849.
Train: 2018-08-08T18:31:43.832313: step 2107, loss 0.43968.
Train: 2018-08-08T18:31:47.175201: step 2108, loss 0.469991.
Train: 2018-08-08T18:31:50.487507: step 2109, loss 0.546384.
Train: 2018-08-08T18:31:53.854459: step 2110, loss 0.656868.
Test: 2018-08-08T18:32:15.432332: step 2110, loss 0.562295.
Train: 2018-08-08T18:32:18.773713: step 2111, loss 0.408559.
Train: 2018-08-08T18:32:22.105571: step 2112, loss 0.550177.
Train: 2018-08-08T18:32:25.504107: step 2113, loss 0.654738.
Train: 2018-08-08T18:32:28.826941: step 2114, loss 0.540756.
Train: 2018-08-08T18:32:32.147771: step 2115, loss 0.643669.
Train: 2018-08-08T18:32:35.489154: step 2116, loss 0.599942.
Train: 2018-08-08T18:32:38.803968: step 2117, loss 0.510439.
Train: 2018-08-08T18:32:42.151869: step 2118, loss 0.544024.
Train: 2018-08-08T18:32:45.473199: step 2119, loss 0.5191.
Train: 2018-08-08T18:32:48.820098: step 2120, loss 0.541518.
Test: 2018-08-08T18:33:10.451610: step 2120, loss 0.55807.
Train: 2018-08-08T18:33:13.791991: step 2121, loss 0.550181.
Train: 2018-08-08T18:33:17.122848: step 2122, loss 0.538923.
Train: 2018-08-08T18:33:20.431143: step 2123, loss 0.646028.
Train: 2018-08-08T18:33:23.776537: step 2124, loss 0.580561.
Train: 2018-08-08T18:33:27.108396: step 2125, loss 0.553583.
Train: 2018-08-08T18:33:30.455294: step 2126, loss 0.580564.
Train: 2018-08-08T18:33:33.819238: step 2127, loss 0.533145.
Train: 2018-08-08T18:33:37.152099: step 2128, loss 0.610203.
Train: 2018-08-08T18:33:40.588235: step 2129, loss 0.518221.
Train: 2018-08-08T18:33:43.941149: step 2130, loss 0.503664.
Test: 2018-08-08T18:34:05.582187: step 2130, loss 0.551379.
Train: 2018-08-08T18:34:09.050909: step 2131, loss 0.564906.
Train: 2018-08-08T18:34:12.404325: step 2132, loss 0.613245.
Train: 2018-08-08T18:34:15.752226: step 2133, loss 0.467979.
Train: 2018-08-08T18:34:19.088095: step 2134, loss 0.569787.
Train: 2018-08-08T18:34:22.422460: step 2135, loss 0.531922.
Train: 2018-08-08T18:34:25.732261: step 2136, loss 0.490353.
Train: 2018-08-08T18:34:29.057601: step 2137, loss 0.437129.
Train: 2018-08-08T18:34:32.395977: step 2138, loss 0.496453.
Train: 2018-08-08T18:34:35.756412: step 2139, loss 0.589838.
Train: 2018-08-08T18:34:39.106823: step 2140, loss 0.616635.
Test: 2018-08-08T18:35:00.754378: step 2140, loss 0.550805.
Train: 2018-08-08T18:35:04.072199: step 2141, loss 0.621246.
Train: 2018-08-08T18:35:07.417594: step 2142, loss 0.560881.
Train: 2018-08-08T18:35:10.766998: step 2143, loss 0.576826.
Train: 2018-08-08T18:35:14.129438: step 2144, loss 0.555157.
Train: 2018-08-08T18:35:17.490875: step 2145, loss 0.577247.
Train: 2018-08-08T18:35:20.821230: step 2146, loss 0.473874.
Train: 2018-08-08T18:35:24.163616: step 2147, loss 0.651755.
Train: 2018-08-08T18:35:27.547613: step 2148, loss 0.488476.
Train: 2018-08-08T18:35:30.880976: step 2149, loss 0.603981.
Train: 2018-08-08T18:35:34.221357: step 2150, loss 0.616697.
Test: 2018-08-08T18:35:55.797723: step 2150, loss 0.558511.
Train: 2018-08-08T18:35:59.107522: step 2151, loss 0.631712.
Train: 2018-08-08T18:36:02.493024: step 2152, loss 0.556239.
Train: 2018-08-08T18:36:05.824882: step 2153, loss 0.487631.
Train: 2018-08-08T18:36:09.166266: step 2154, loss 0.604849.
Train: 2018-08-08T18:36:12.518679: step 2155, loss 0.544036.
Train: 2018-08-08T18:36:15.834996: step 2156, loss 0.591776.
Train: 2018-08-08T18:36:19.156327: step 2157, loss 0.647106.
Train: 2018-08-08T18:36:22.501220: step 2158, loss 0.542129.
Train: 2018-08-08T18:36:25.906273: step 2159, loss 0.625332.
Train: 2018-08-08T18:36:29.221587: step 2160, loss 0.578466.
Test: 2018-08-08T18:36:50.820513: step 2160, loss 0.554798.
Train: 2018-08-08T18:36:54.209523: step 2161, loss 0.484843.
Train: 2018-08-08T18:36:57.720859: step 2162, loss 0.435845.
Train: 2018-08-08T18:37:01.314915: step 2163, loss 0.528584.
Train: 2018-08-08T18:37:04.832275: step 2164, loss 0.574647.
Train: 2018-08-08T18:37:08.350128: step 2165, loss 0.505031.
Train: 2018-08-08T18:37:11.847928: step 2166, loss 0.50447.
Train: 2018-08-08T18:37:15.357760: step 2167, loss 0.417614.
Train: 2018-08-08T18:37:18.930258: step 2168, loss 0.465396.
Train: 2018-08-08T18:37:22.505764: step 2169, loss 0.711315.
Train: 2018-08-08T18:37:25.853164: step 2170, loss 0.531524.
Test: 2018-08-08T18:37:47.452091: step 2170, loss 0.557081.
Train: 2018-08-08T18:37:50.784951: step 2171, loss 0.440161.
Train: 2018-08-08T18:37:54.127337: step 2172, loss 0.57244.
Train: 2018-08-08T18:37:57.483761: step 2173, loss 0.476455.
Train: 2018-08-08T18:38:00.816622: step 2174, loss 0.585775.
Train: 2018-08-08T18:38:04.171542: step 2175, loss 0.600624.
Train: 2018-08-08T18:38:07.499891: step 2176, loss 0.542571.
Train: 2018-08-08T18:38:10.813200: step 2177, loss 0.600867.
Train: 2018-08-08T18:38:14.149571: step 2178, loss 0.605099.
Train: 2018-08-08T18:38:17.502986: step 2179, loss 0.522729.
Train: 2018-08-08T18:38:20.850386: step 2180, loss 0.5387.
Test: 2018-08-08T18:38:42.410709: step 2180, loss 0.548934.
Train: 2018-08-08T18:38:45.754098: step 2181, loss 0.630783.
Train: 2018-08-08T18:38:49.076933: step 2182, loss 0.594581.
Train: 2018-08-08T18:38:52.393250: step 2183, loss 0.599353.
Train: 2018-08-08T18:38:55.724106: step 2184, loss 0.56824.
Train: 2018-08-08T18:38:59.055965: step 2185, loss 0.542997.
Train: 2018-08-08T18:39:02.390832: step 2186, loss 0.55672.
Train: 2018-08-08T18:39:05.730209: step 2187, loss 0.475342.
Train: 2018-08-08T18:39:09.067583: step 2188, loss 0.58691.
Train: 2018-08-08T18:39:12.405457: step 2189, loss 0.61609.
Train: 2018-08-08T18:39:15.741326: step 2190, loss 0.55182.
Test: 2018-08-08T18:39:37.318695: step 2190, loss 0.555178.
Train: 2018-08-08T18:39:40.659076: step 2191, loss 0.584788.
Train: 2018-08-08T18:39:44.003969: step 2192, loss 0.500905.
Train: 2018-08-08T18:39:47.355881: step 2193, loss 0.633102.
Train: 2018-08-08T18:39:50.680219: step 2194, loss 0.519341.
Train: 2018-08-08T18:39:54.013582: step 2195, loss 0.657971.
Train: 2018-08-08T18:39:57.348949: step 2196, loss 0.569235.
Train: 2018-08-08T18:40:00.734953: step 2197, loss 0.527583.
Train: 2018-08-08T18:40:04.111429: step 2198, loss 0.594132.
Train: 2018-08-08T18:40:07.487405: step 2199, loss 0.546531.
Train: 2018-08-08T18:40:10.795204: step 2200, loss 0.583087.
Test: 2018-08-08T18:40:32.424210: step 2200, loss 0.55546.
Train: 2018-08-08T18:40:37.702744: step 2201, loss 0.59059.
Train: 2018-08-08T18:40:41.012545: step 2202, loss 0.637373.
Train: 2018-08-08T18:40:44.350418: step 2203, loss 0.593297.
Train: 2018-08-08T18:40:47.686789: step 2204, loss 0.593741.
Train: 2018-08-08T18:40:50.988066: step 2205, loss 0.562127.
Train: 2018-08-08T18:40:54.342485: step 2206, loss 0.539827.
Train: 2018-08-08T18:40:57.634237: step 2207, loss 0.534846.
Train: 2018-08-08T18:41:00.954564: step 2208, loss 0.572866.
Train: 2018-08-08T18:41:04.279404: step 2209, loss 0.566061.
Train: 2018-08-08T18:41:07.621791: step 2210, loss 0.582811.
Test: 2018-08-08T18:41:29.182615: step 2210, loss 0.559034.
Train: 2018-08-08T18:41:32.504948: step 2211, loss 0.554311.
Train: 2018-08-08T18:41:35.849340: step 2212, loss 0.555869.
Train: 2018-08-08T18:41:39.183204: step 2213, loss 0.610539.
Train: 2018-08-08T18:41:42.534614: step 2214, loss 0.544996.
Train: 2018-08-08T18:41:45.868979: step 2215, loss 0.617806.
Train: 2018-08-08T18:41:49.197328: step 2216, loss 0.497907.
Train: 2018-08-08T18:41:52.531693: step 2217, loss 0.583458.
Train: 2018-08-08T18:41:55.865056: step 2218, loss 0.494314.
Train: 2018-08-08T18:41:59.200424: step 2219, loss 0.543948.
Train: 2018-08-08T18:42:02.531781: step 2220, loss 0.503331.
Test: 2018-08-08T18:42:24.114664: step 2220, loss 0.555638.
Train: 2018-08-08T18:42:27.463568: step 2221, loss 0.552848.
Train: 2018-08-08T18:42:30.806462: step 2222, loss 0.548554.
Train: 2018-08-08T18:42:34.161883: step 2223, loss 0.547923.
Train: 2018-08-08T18:42:37.484718: step 2224, loss 0.630444.
Train: 2018-08-08T18:42:40.832117: step 2225, loss 0.585732.
Train: 2018-08-08T18:42:44.145427: step 2226, loss 0.543357.
Train: 2018-08-08T18:42:47.465253: step 2227, loss 0.4763.
Train: 2018-08-08T18:42:50.774051: step 2228, loss 0.606728.
Train: 2018-08-08T18:42:54.101898: step 2229, loss 0.563877.
Train: 2018-08-08T18:42:57.444786: step 2230, loss 0.501536.
Test: 2018-08-08T18:43:19.005610: step 2230, loss 0.556139.
Train: 2018-08-08T18:43:22.326439: step 2231, loss 0.515834.
Train: 2018-08-08T18:43:25.700918: step 2232, loss 0.585156.
Train: 2018-08-08T18:43:29.035283: step 2233, loss 0.637406.
Train: 2018-08-08T18:43:32.380677: step 2234, loss 0.467668.
Train: 2018-08-08T18:43:35.700002: step 2235, loss 0.542434.
Train: 2018-08-08T18:43:39.005290: step 2236, loss 0.566979.
Train: 2018-08-08T18:43:42.353693: step 2237, loss 0.669983.
Train: 2018-08-08T18:43:45.698586: step 2238, loss 0.567331.
Train: 2018-08-08T18:43:49.030946: step 2239, loss 0.480171.
Train: 2018-08-08T18:43:52.328212: step 2240, loss 0.674464.
Test: 2018-08-08T18:44:13.946188: step 2240, loss 0.560079.
Train: 2018-08-08T18:44:17.265012: step 2241, loss 0.617496.
Train: 2018-08-08T18:44:20.616924: step 2242, loss 0.506187.
Train: 2018-08-08T18:44:23.956303: step 2243, loss 0.476355.
Train: 2018-08-08T18:44:27.286156: step 2244, loss 0.535755.
Train: 2018-08-08T18:44:30.608489: step 2245, loss 0.50526.
Train: 2018-08-08T18:44:33.961905: step 2246, loss 0.732205.
Train: 2018-08-08T18:44:37.277721: step 2247, loss 0.501229.
Train: 2018-08-08T18:44:40.606571: step 2248, loss 0.633597.
Train: 2018-08-08T18:44:43.928403: step 2249, loss 0.571233.
Train: 2018-08-08T18:44:47.242213: step 2250, loss 0.454884.
Test: 2018-08-08T18:45:08.887261: step 2250, loss 0.562559.
Train: 2018-08-08T18:45:12.228650: step 2251, loss 0.50431.
Train: 2018-08-08T18:45:15.588082: step 2252, loss 0.537736.
Train: 2018-08-08T18:45:18.913925: step 2253, loss 0.626393.
Train: 2018-08-08T18:45:22.274860: step 2254, loss 0.574508.
Train: 2018-08-08T18:45:25.673903: step 2255, loss 0.572215.
Train: 2018-08-08T18:45:29.018796: step 2256, loss 0.476029.
Train: 2018-08-08T18:45:32.352659: step 2257, loss 0.568972.
Train: 2018-08-08T18:45:35.682012: step 2258, loss 0.517446.
Train: 2018-08-08T18:45:39.007854: step 2259, loss 0.678443.
Train: 2018-08-08T18:45:42.359264: step 2260, loss 0.5685.
Test: 2018-08-08T18:46:03.890009: step 2260, loss 0.562317.
Train: 2018-08-08T18:46:07.216352: step 2261, loss 0.51489.
Train: 2018-08-08T18:46:10.549213: step 2262, loss 0.613916.
Train: 2018-08-08T18:46:13.884581: step 2263, loss 0.562908.
Train: 2018-08-08T18:46:17.208418: step 2264, loss 0.598104.
Train: 2018-08-08T18:46:20.518218: step 2265, loss 0.657442.
Train: 2018-08-08T18:46:23.858599: step 2266, loss 0.536936.
Train: 2018-08-08T18:46:27.213018: step 2267, loss 0.549731.
Train: 2018-08-08T18:46:30.526327: step 2268, loss 0.53865.
Train: 2018-08-08T18:46:33.833620: step 2269, loss 0.485823.
Train: 2018-08-08T18:46:37.172999: step 2270, loss 0.564274.
Test: 2018-08-08T18:46:58.745855: step 2270, loss 0.561034.
Train: 2018-08-08T18:47:02.045628: step 2271, loss 0.585825.
Train: 2018-08-08T18:47:05.405060: step 2272, loss 0.656546.
Train: 2018-08-08T18:47:08.737420: step 2273, loss 0.456863.
Train: 2018-08-08T18:47:12.076798: step 2274, loss 0.494708.
Train: 2018-08-08T18:47:15.425201: step 2275, loss 0.506819.
Train: 2018-08-08T18:47:18.753550: step 2276, loss 0.527192.
Train: 2018-08-08T18:47:22.086411: step 2277, loss 0.618657.
Train: 2018-08-08T18:47:25.424792: step 2278, loss 0.655129.
Train: 2018-08-08T18:47:28.741610: step 2279, loss 0.581095.
Train: 2018-08-08T18:47:32.074471: step 2280, loss 0.515526.
Test: 2018-08-08T18:47:53.662371: step 2280, loss 0.564347.
Train: 2018-08-08T18:47:57.072939: step 2281, loss 0.526597.
Train: 2018-08-08T18:48:00.416329: step 2282, loss 0.464254.
Train: 2018-08-08T18:48:03.768741: step 2283, loss 0.454957.
Train: 2018-08-08T18:48:07.107618: step 2284, loss 0.633616.
Train: 2018-08-08T18:48:10.436970: step 2285, loss 0.573799.
Train: 2018-08-08T18:48:13.760306: step 2286, loss 0.539039.
Train: 2018-08-08T18:48:17.069103: step 2287, loss 0.566964.
Train: 2018-08-08T18:48:20.391436: step 2288, loss 0.461284.
Train: 2018-08-08T18:48:23.737332: step 2289, loss 0.533482.
Train: 2018-08-08T18:48:27.064177: step 2290, loss 0.549194.
Test: 2018-08-08T18:48:48.695188: step 2290, loss 0.567112.
Train: 2018-08-08T18:48:52.032561: step 2291, loss 0.578208.
Train: 2018-08-08T18:48:55.391993: step 2292, loss 0.534365.
Train: 2018-08-08T18:48:58.746913: step 2293, loss 0.525235.
Train: 2018-08-08T18:49:02.055710: step 2294, loss 0.542088.
Train: 2018-08-08T18:49:05.390075: step 2295, loss 0.555934.
Train: 2018-08-08T18:49:08.731459: step 2296, loss 0.553855.
Train: 2018-08-08T18:49:12.051286: step 2297, loss 0.570592.
Train: 2018-08-08T18:49:15.374120: step 2298, loss 0.474161.
Train: 2018-08-08T18:49:18.693947: step 2299, loss 0.610743.
Train: 2018-08-08T18:49:22.010264: step 2300, loss 0.459229.
Test: 2018-08-08T18:49:43.559557: step 2300, loss 0.564297.
Train: 2018-08-08T18:49:48.690700: step 2301, loss 0.463475.
Train: 2018-08-08T18:49:52.007518: step 2302, loss 0.644894.
Train: 2018-08-08T18:49:55.330854: step 2303, loss 0.600237.
Train: 2018-08-08T18:49:58.676750: step 2304, loss 0.52621.
Train: 2018-08-08T18:50:02.131937: step 2305, loss 0.465698.
Train: 2018-08-08T18:50:05.473320: step 2306, loss 0.500161.
Train: 2018-08-08T18:50:08.820723: step 2307, loss 0.468852.
Train: 2018-08-08T18:50:12.154086: step 2308, loss 0.626025.
Train: 2018-08-08T18:50:15.502990: step 2309, loss 0.622026.
Train: 2018-08-08T18:50:18.808278: step 2310, loss 0.519855.
Test: 2018-08-08T18:50:40.360579: step 2310, loss 0.563085.
Train: 2018-08-08T18:50:43.724523: step 2311, loss 0.588452.
Train: 2018-08-08T18:50:47.077438: step 2312, loss 0.437105.
Train: 2018-08-08T18:50:50.392251: step 2313, loss 0.508142.
Train: 2018-08-08T18:50:53.717592: step 2314, loss 0.538086.
Train: 2018-08-08T18:50:57.052458: step 2315, loss 0.892192.
Train: 2018-08-08T18:51:00.393842: step 2316, loss 0.640607.
Train: 2018-08-08T18:51:03.708655: step 2317, loss 0.569299.
Train: 2018-08-08T18:51:07.036001: step 2318, loss 0.563884.
Train: 2018-08-08T18:51:10.383903: step 2319, loss 0.598341.
Train: 2018-08-08T18:51:13.709244: step 2320, loss 0.59802.
Test: 2018-08-08T18:51:35.266561: step 2320, loss 0.564477.
Train: 2018-08-08T18:51:38.602430: step 2321, loss 0.504577.
Train: 2018-08-08T18:51:41.921759: step 2322, loss 0.608936.
Train: 2018-08-08T18:51:45.227549: step 2323, loss 0.551078.
Train: 2018-08-08T18:51:48.555396: step 2324, loss 0.574541.
Train: 2018-08-08T18:51:51.878231: step 2325, loss 0.598154.
Train: 2018-08-08T18:51:55.207582: step 2326, loss 0.632392.
Train: 2018-08-08T18:51:58.544956: step 2327, loss 0.561002.
Train: 2018-08-08T18:52:01.868793: step 2328, loss 0.560915.
Train: 2018-08-08T18:52:05.210177: step 2329, loss 0.571221.
Train: 2018-08-08T18:52:08.549555: step 2330, loss 0.570994.
Test: 2018-08-08T18:52:30.183083: step 2330, loss 0.560326.
Train: 2018-08-08T18:52:33.522462: step 2331, loss 0.495778.
Train: 2018-08-08T18:52:36.847803: step 2332, loss 0.530034.
Train: 2018-08-08T18:52:40.155096: step 2333, loss 0.566211.
Train: 2018-08-08T18:52:43.498987: step 2334, loss 0.564206.
Train: 2018-08-08T18:52:46.827837: step 2335, loss 0.554185.
Train: 2018-08-08T18:52:50.145658: step 2336, loss 0.547966.
Train: 2018-08-08T18:52:53.488045: step 2337, loss 0.569197.
Train: 2018-08-08T18:52:56.799850: step 2338, loss 0.578193.
Train: 2018-08-08T18:53:00.162290: step 2339, loss 0.546743.
Train: 2018-08-08T18:53:03.486628: step 2340, loss 0.607025.
Test: 2018-08-08T18:53:25.102097: step 2340, loss 0.552348.
Train: 2018-08-08T18:53:28.473060: step 2341, loss 0.525889.
Train: 2018-08-08T18:53:31.830487: step 2342, loss 0.572755.
Train: 2018-08-08T18:53:35.193428: step 2343, loss 0.544595.
Train: 2018-08-08T18:53:38.529297: step 2344, loss 0.533195.
Train: 2018-08-08T18:53:41.891737: step 2345, loss 0.517661.
Train: 2018-08-08T18:53:45.226102: step 2346, loss 0.498652.
Train: 2018-08-08T18:53:48.561470: step 2347, loss 0.641699.
Train: 2018-08-08T18:53:51.908368: step 2348, loss 0.58379.
Train: 2018-08-08T18:53:55.191597: step 2349, loss 0.570231.
Train: 2018-08-08T18:53:58.530475: step 2350, loss 0.622767.
Test: 2018-08-08T18:54:20.170008: step 2350, loss 0.553793.
Train: 2018-08-08T18:54:23.499360: step 2351, loss 0.620107.
Train: 2018-08-08T18:54:26.824200: step 2352, loss 0.532247.
Train: 2018-08-08T18:54:30.137008: step 2353, loss 0.585981.
Train: 2018-08-08T18:54:33.439789: step 2354, loss 0.54612.
Train: 2018-08-08T18:54:36.774154: step 2355, loss 0.551087.
Train: 2018-08-08T18:54:40.090480: step 2356, loss 0.551128.
Train: 2018-08-08T18:54:43.431363: step 2357, loss 0.541971.
Train: 2018-08-08T18:54:46.771243: step 2358, loss 0.437534.
Train: 2018-08-08T18:54:50.122152: step 2359, loss 0.571006.
Train: 2018-08-08T18:54:53.447994: step 2360, loss 0.59313.
Test: 2018-08-08T18:55:15.082515: step 2360, loss 0.554495.
Train: 2018-08-08T18:55:18.400837: step 2361, loss 0.631865.
Train: 2018-08-08T18:55:21.769794: step 2362, loss 0.603858.
Train: 2018-08-08T18:55:25.123711: step 2363, loss 0.498073.
Train: 2018-08-08T18:55:28.477127: step 2364, loss 0.48933.
Train: 2018-08-08T18:55:31.801466: step 2365, loss 0.540161.
Train: 2018-08-08T18:55:35.121292: step 2366, loss 0.471726.
Train: 2018-08-08T18:55:38.463679: step 2367, loss 0.522401.
Train: 2018-08-08T18:55:41.776486: step 2368, loss 0.588756.
Train: 2018-08-08T18:55:45.107844: step 2369, loss 0.554274.
Train: 2018-08-08T18:55:48.446721: step 2370, loss 0.434473.
Test: 2018-08-08T18:56:10.038131: step 2370, loss 0.550541.
Train: 2018-08-08T18:56:13.397563: step 2371, loss 0.633315.
Train: 2018-08-08T18:56:16.742456: step 2372, loss 0.540669.
Train: 2018-08-08T18:56:20.054762: step 2373, loss 0.481259.
Train: 2018-08-08T18:56:23.396146: step 2374, loss 0.524384.
Train: 2018-08-08T18:56:26.739034: step 2375, loss 0.474272.
Train: 2018-08-08T18:56:30.093954: step 2376, loss 0.664368.
Train: 2018-08-08T18:56:33.422804: step 2377, loss 0.522577.
Train: 2018-08-08T18:56:36.795271: step 2378, loss 0.552983.
Train: 2018-08-08T18:56:40.192804: step 2379, loss 0.500192.
Train: 2018-08-08T18:56:43.524662: step 2380, loss 0.45069.
Test: 2018-08-08T18:57:05.095513: step 2380, loss 0.551612.
Train: 2018-08-08T18:57:08.435894: step 2381, loss 0.698276.
Train: 2018-08-08T18:57:11.780286: step 2382, loss 0.588278.
Train: 2018-08-08T18:57:15.092092: step 2383, loss 0.570541.
Train: 2018-08-08T18:57:18.428963: step 2384, loss 0.555814.
Train: 2018-08-08T18:57:21.793911: step 2385, loss 0.530975.
Train: 2018-08-08T18:57:25.101704: step 2386, loss 0.574011.
Train: 2018-08-08T18:57:28.425040: step 2387, loss 0.54406.
Train: 2018-08-08T18:57:31.743864: step 2388, loss 0.558087.
Train: 2018-08-08T18:57:35.095776: step 2389, loss 0.508941.
Train: 2018-08-08T18:57:38.432146: step 2390, loss 0.459327.
Test: 2018-08-08T18:57:59.990471: step 2390, loss 0.553148.
Train: 2018-08-08T18:58:03.313807: step 2391, loss 0.544052.
Train: 2018-08-08T18:58:06.643660: step 2392, loss 0.495464.
Train: 2018-08-08T18:58:09.984041: step 2393, loss 0.652822.
Train: 2018-08-08T18:58:13.336454: step 2394, loss 0.506557.
Train: 2018-08-08T18:58:16.683854: step 2395, loss 0.603519.
Train: 2018-08-08T18:58:20.024235: step 2396, loss 0.480616.
Train: 2018-08-08T18:58:23.355592: step 2397, loss 0.579545.
Train: 2018-08-08T18:58:26.744101: step 2398, loss 0.626704.
Train: 2018-08-08T18:58:30.070445: step 2399, loss 0.526491.
Train: 2018-08-08T18:58:33.401802: step 2400, loss 0.557802.
Test: 2018-08-08T18:58:54.957112: step 2400, loss 0.553262.
Train: 2018-08-08T18:59:00.094771: step 2401, loss 0.592618.
Train: 2018-08-08T18:59:03.456214: step 2402, loss 0.547563.
Train: 2018-08-08T18:59:06.820152: step 2403, loss 0.474853.
Train: 2018-08-08T18:59:10.167051: step 2404, loss 0.494884.
Train: 2018-08-08T18:59:13.514952: step 2405, loss 0.490987.
Train: 2018-08-08T18:59:16.839792: step 2406, loss 0.460868.
Train: 2018-08-08T18:59:20.166136: step 2407, loss 0.622164.
Train: 2018-08-08T18:59:23.317013: step 2408, loss 0.455388.
Train: 2018-08-08T18:59:26.646365: step 2409, loss 0.500848.
Train: 2018-08-08T18:59:29.984742: step 2410, loss 0.566428.
Test: 2018-08-08T18:59:51.522002: step 2410, loss 0.55618.
Train: 2018-08-08T18:59:54.827791: step 2411, loss 0.579088.
Train: 2018-08-08T18:59:58.186221: step 2412, loss 0.549613.
Train: 2018-08-08T19:00:01.651433: step 2413, loss 0.581839.
Train: 2018-08-08T19:00:04.976274: step 2414, loss 0.528613.
Train: 2018-08-08T19:00:08.321668: step 2415, loss 0.488795.
Train: 2018-08-08T19:00:11.668065: step 2416, loss 0.592326.
Train: 2018-08-08T19:00:15.020478: step 2417, loss 0.558216.
Train: 2018-08-08T19:00:18.361862: step 2418, loss 0.549771.
Train: 2018-08-08T19:00:21.698232: step 2419, loss 0.539936.
Train: 2018-08-08T19:00:25.071702: step 2420, loss 0.524593.
Test: 2018-08-08T19:00:47.025594: step 2420, loss 0.554236.
Train: 2018-08-08T19:00:50.258159: step 2421, loss 0.593792.
Train: 2018-08-08T19:00:53.540887: step 2422, loss 0.600574.
Train: 2018-08-08T19:00:56.797546: step 2423, loss 0.482579.
Train: 2018-08-08T19:00:59.957950: step 2424, loss 0.606652.
Train: 2018-08-08T19:01:03.339882: step 2425, loss 0.54324.
Train: 2018-08-08T19:01:06.601554: step 2426, loss 0.509223.
Train: 2018-08-08T19:01:09.889295: step 2427, loss 0.591249.
Train: 2018-08-08T19:01:13.160994: step 2428, loss 0.612684.
Train: 2018-08-08T19:01:16.394591: step 2429, loss 0.498163.
Train: 2018-08-08T19:01:19.630194: step 2430, loss 0.53684.
Test: 2018-08-08T19:01:41.049088: step 2430, loss 0.55254.
Train: 2018-08-08T19:01:44.420051: step 2431, loss 0.514653.
Train: 2018-08-08T19:01:47.679717: step 2432, loss 0.549899.
Train: 2018-08-08T19:01:50.911309: step 2433, loss 0.515197.
Train: 2018-08-08T19:01:54.167967: step 2434, loss 0.676546.
Train: 2018-08-08T19:01:57.450695: step 2435, loss 0.53119.
Train: 2018-08-08T19:02:00.660228: step 2436, loss 0.565047.
Train: 2018-08-08T19:02:03.982060: step 2437, loss 0.465838.
Train: 2018-08-08T19:02:07.251753: step 2438, loss 0.471619.
Train: 2018-08-08T19:02:10.528465: step 2439, loss 0.597925.
Train: 2018-08-08T19:02:13.811193: step 2440, loss 0.502104.
Test: 2018-08-08T19:02:35.186023: step 2440, loss 0.550636.
Train: 2018-08-08T19:02:38.447695: step 2441, loss 0.477437.
Train: 2018-08-08T19:02:41.776545: step 2442, loss 0.586632.
Train: 2018-08-08T19:02:45.126452: step 2443, loss 0.519085.
Train: 2018-08-08T19:02:48.360435: step 2444, loss 0.545644.
Train: 2018-08-08T19:02:51.591505: step 2445, loss 0.462348.
Train: 2018-08-08T19:02:54.833124: step 2446, loss 0.630626.
Train: 2018-08-08T19:02:58.141921: step 2447, loss 0.573682.
Train: 2018-08-08T19:03:01.447710: step 2448, loss 0.63426.
Train: 2018-08-08T19:03:04.803633: step 2449, loss 0.452459.
Train: 2018-08-08T19:03:07.947993: step 2450, loss 0.527981.
Test: 2018-08-08T19:03:29.478236: step 2450, loss 0.546996.
Train: 2018-08-08T19:03:32.751939: step 2451, loss 0.533377.
Train: 2018-08-08T19:03:36.376322: step 2452, loss 0.661006.
Train: 2018-08-08T19:03:40.059333: step 2453, loss 0.613798.
Train: 2018-08-08T19:03:43.576685: step 2454, loss 0.509005.
Train: 2018-08-08T19:03:46.935321: step 2455, loss 0.558843.
Train: 2018-08-08T19:03:50.618327: step 2456, loss 0.55971.
Train: 2018-08-08T19:03:54.006636: step 2457, loss 0.454493.
Train: 2018-08-08T19:03:57.457840: step 2458, loss 0.603526.
Train: 2018-08-08T19:04:00.875562: step 2459, loss 0.48698.
Train: 2018-08-08T19:04:04.181358: step 2460, loss 0.526637.
Test: 2018-08-08T19:04:26.704763: step 2460, loss 0.54889.
Train: 2018-08-08T19:04:30.035618: step 2461, loss 0.582598.
Train: 2018-08-08T19:04:33.303306: step 2462, loss 0.503744.
Train: 2018-08-08T19:04:36.674268: step 2463, loss 0.724528.
Train: 2018-08-08T19:04:40.078319: step 2464, loss 0.469917.
Train: 2018-08-08T19:04:43.424184: step 2465, loss 0.566069.
Train: 2018-08-08T19:04:46.720949: step 2466, loss 0.511895.
Train: 2018-08-08T19:04:50.012701: step 2467, loss 0.638849.
Train: 2018-08-08T19:04:53.306458: step 2468, loss 0.576702.
Train: 2018-08-08T19:04:56.750615: step 2469, loss 0.549101.
Train: 2018-08-08T19:05:00.192766: step 2470, loss 0.530582.
Test: 2018-08-08T19:05:22.713643: step 2470, loss 0.549173.
Train: 2018-08-08T19:05:26.143763: step 2471, loss 0.526374.
Train: 2018-08-08T19:05:29.688187: step 2472, loss 0.591458.
Train: 2018-08-08T19:05:33.768048: step 2473, loss 0.513245.
Train: 2018-08-08T19:05:37.175107: step 2474, loss 0.509724.
Train: 2018-08-08T19:05:41.093005: step 2475, loss 0.604628.
Train: 2018-08-08T19:05:44.436696: step 2476, loss 0.605096.
Train: 2018-08-08T19:05:47.633195: step 2477, loss 0.588217.
Train: 2018-08-08T19:05:50.868798: step 2478, loss 0.566975.
Train: 2018-08-08T19:05:54.065296: step 2479, loss 0.561869.
Train: 2018-08-08T19:05:57.243747: step 2480, loss 0.58001.
Test: 2018-08-08T19:06:18.440103: step 2480, loss 0.551302.
Train: 2018-08-08T19:06:21.674702: step 2481, loss 0.559388.
Train: 2018-08-08T19:06:24.888246: step 2482, loss 0.704891.
Train: 2018-08-08T19:06:28.107806: step 2483, loss 0.523809.
Train: 2018-08-08T19:06:31.482779: step 2484, loss 0.552347.
Train: 2018-08-08T19:06:34.761496: step 2485, loss 0.621504.
Train: 2018-08-08T19:06:37.962005: step 2486, loss 0.467352.
Train: 2018-08-08T19:06:41.137448: step 2487, loss 0.545689.
Train: 2018-08-08T19:06:44.384080: step 2488, loss 0.602169.
Train: 2018-08-08T19:06:47.670818: step 2489, loss 0.602235.
Train: 2018-08-08T19:06:50.837237: step 2490, loss 0.536917.
Test: 2018-08-08T19:07:12.028579: step 2490, loss 0.553526.
Train: 2018-08-08T19:07:15.235104: step 2491, loss 0.611755.
Train: 2018-08-08T19:07:18.448648: step 2492, loss 0.531643.
Train: 2018-08-08T19:07:21.668208: step 2493, loss 0.481773.
Train: 2018-08-08T19:07:24.851672: step 2494, loss 0.698116.
Train: 2018-08-08T19:07:28.035136: step 2495, loss 0.546007.
Train: 2018-08-08T19:07:31.223613: step 2496, loss 0.542197.
Train: 2018-08-08T19:07:34.403067: step 2497, loss 0.605758.
Train: 2018-08-08T19:07:37.686797: step 2498, loss 0.596723.
Train: 2018-08-08T19:07:40.911371: step 2499, loss 0.63302.
Train: 2018-08-08T19:07:44.077789: step 2500, loss 0.436175.
Test: 2018-08-08T19:08:05.285250: step 2500, loss 0.556976.
Train: 2018-08-08T19:08:10.317629: step 2501, loss 0.492677.
Train: 2018-08-08T19:08:13.551227: step 2502, loss 0.649139.
Train: 2018-08-08T19:08:16.737699: step 2503, loss 0.552048.
Train: 2018-08-08T19:08:19.902113: step 2504, loss 0.534469.
Train: 2018-08-08T19:08:23.099613: step 2505, loss 0.516663.
Train: 2018-08-08T19:08:26.295109: step 2506, loss 0.54772.
Train: 2018-08-08T19:08:29.484589: step 2507, loss 0.537127.
Train: 2018-08-08T19:08:32.748670: step 2508, loss 0.501226.
Train: 2018-08-08T19:08:36.275614: step 2509, loss 0.719616.
Train: 2018-08-08T19:08:39.551324: step 2510, loss 0.599093.
Test: 2018-08-08T19:09:00.700553: step 2510, loss 0.555167.
Train: 2018-08-08T19:09:03.888028: step 2511, loss 0.60828.
Train: 2018-08-08T19:09:07.106585: step 2512, loss 0.462821.
Train: 2018-08-08T19:09:10.311105: step 2513, loss 0.576193.
Train: 2018-08-08T19:09:13.493566: step 2514, loss 0.627737.
Train: 2018-08-08T19:09:16.696081: step 2515, loss 0.54368.
Train: 2018-08-08T19:09:19.936697: step 2516, loss 0.522336.
Train: 2018-08-08T19:09:23.208395: step 2517, loss 0.609637.
Train: 2018-08-08T19:09:26.504284: step 2518, loss 0.538894.
Train: 2018-08-08T19:09:30.089272: step 2519, loss 0.56153.
Train: 2018-08-08T19:09:33.443761: step 2520, loss 0.558077.
Test: 2018-08-08T19:09:55.091219: step 2520, loss 0.555314.
Train: 2018-08-08T19:09:58.303760: step 2521, loss 0.587886.
Train: 2018-08-08T19:10:01.665805: step 2522, loss 0.529461.
Train: 2018-08-08T19:10:05.230394: step 2523, loss 0.484127.
Train: 2018-08-08T19:10:08.732800: step 2524, loss 0.602299.
Train: 2018-08-08T19:10:12.082079: step 2525, loss 0.468955.
Train: 2018-08-08T19:10:15.401607: step 2526, loss 0.434556.
Train: 2018-08-08T19:10:18.832827: step 2527, loss 0.697399.
Train: 2018-08-08T19:10:22.106643: step 2528, loss 0.613247.
Train: 2018-08-08T19:10:25.314170: step 2529, loss 0.487625.
Train: 2018-08-08T19:10:28.513677: step 2530, loss 0.563047.
Test: 2018-08-08T19:10:49.699004: step 2530, loss 0.557959.
Train: 2018-08-08T19:10:52.871438: step 2531, loss 0.605493.
Train: 2018-08-08T19:10:56.094005: step 2532, loss 0.572355.
Train: 2018-08-08T19:10:59.293512: step 2533, loss 0.57489.
Train: 2018-08-08T19:11:02.505051: step 2534, loss 0.524657.
Train: 2018-08-08T19:11:05.741644: step 2535, loss 0.591283.
Train: 2018-08-08T19:11:08.975241: step 2536, loss 0.621577.
Train: 2018-08-08T19:11:12.207836: step 2537, loss 0.688063.
Train: 2018-08-08T19:11:15.409348: step 2538, loss 0.561622.
Train: 2018-08-08T19:11:18.599830: step 2539, loss 0.53274.
Train: 2018-08-08T19:11:21.807359: step 2540, loss 0.65628.
Test: 2018-08-08T19:11:43.060356: step 2540, loss 0.557414.
Train: 2018-08-08T19:11:46.422452: step 2541, loss 0.55144.
Train: 2018-08-08T19:11:49.805948: step 2542, loss 0.544159.
Train: 2018-08-08T19:11:53.334329: step 2543, loss 0.554315.
Train: 2018-08-08T19:11:57.097752: step 2544, loss 0.596564.
Train: 2018-08-08T19:12:00.593903: step 2545, loss 0.607741.
Train: 2018-08-08T19:12:03.912801: step 2546, loss 0.639109.
Train: 2018-08-08T19:12:07.244461: step 2547, loss 0.566764.
Train: 2018-08-08T19:12:10.766835: step 2548, loss 0.530139.
Train: 2018-08-08T19:12:14.112730: step 2549, loss 0.578904.
Train: 2018-08-08T19:12:17.651124: step 2550, loss 0.582268.
Test: 2018-08-08T19:12:39.011825: step 2550, loss 0.555849.
Train: 2018-08-08T19:12:42.180248: step 2551, loss 0.478645.
Train: 2018-08-08T19:12:45.404822: step 2552, loss 0.582888.
Train: 2018-08-08T19:12:48.595304: step 2553, loss 0.496668.
Train: 2018-08-08T19:12:51.766736: step 2554, loss 0.542543.
Train: 2018-08-08T19:12:54.957219: step 2555, loss 0.507306.
Train: 2018-08-08T19:12:58.111605: step 2556, loss 0.685209.
Train: 2018-08-08T19:13:01.282035: step 2557, loss 0.553662.
Train: 2018-08-08T19:13:04.448453: step 2558, loss 0.61547.
Train: 2018-08-08T19:13:07.632920: step 2559, loss 0.473752.
Train: 2018-08-08T19:13:10.839445: step 2560, loss 0.626512.
Test: 2018-08-08T19:13:32.306523: step 2560, loss 0.554108.
Train: 2018-08-08T19:13:35.504022: step 2561, loss 0.495467.
Train: 2018-08-08T19:13:38.685480: step 2562, loss 0.670855.
Train: 2018-08-08T19:13:41.875963: step 2563, loss 0.603098.
Train: 2018-08-08T19:13:45.139640: step 2564, loss 0.558991.
Train: 2018-08-08T19:13:48.323104: step 2565, loss 0.471608.
Train: 2018-08-08T19:13:51.504563: step 2566, loss 0.556349.
Train: 2018-08-08T19:13:54.698053: step 2567, loss 0.497867.
Train: 2018-08-08T19:13:57.891544: step 2568, loss 0.59639.
Train: 2018-08-08T19:14:01.083029: step 2569, loss 0.550125.
Train: 2018-08-08T19:14:04.238419: step 2570, loss 0.615127.
Test: 2018-08-08T19:14:25.269334: step 2570, loss 0.554838.
Train: 2018-08-08T19:14:28.408681: step 2571, loss 0.510586.
Train: 2018-08-08T19:14:31.661328: step 2572, loss 0.61382.
Train: 2018-08-08T19:14:34.861838: step 2573, loss 0.463298.
Train: 2018-08-08T19:14:38.036277: step 2574, loss 0.468711.
Train: 2018-08-08T19:14:41.220744: step 2575, loss 0.654809.
Train: 2018-08-08T19:14:44.364102: step 2576, loss 0.527554.
Train: 2018-08-08T19:14:47.660867: step 2577, loss 0.595882.
Train: 2018-08-08T19:14:50.854357: step 2578, loss 0.552422.
Train: 2018-08-08T19:14:54.027794: step 2579, loss 0.580085.
Train: 2018-08-08T19:14:57.190202: step 2580, loss 0.517353.
Test: 2018-08-08T19:15:18.205076: step 2580, loss 0.555419.
Train: 2018-08-08T19:15:21.368486: step 2581, loss 0.635564.
Train: 2018-08-08T19:15:24.533902: step 2582, loss 0.488703.
Train: 2018-08-08T19:15:27.758475: step 2583, loss 0.598139.
Train: 2018-08-08T19:15:30.953971: step 2584, loss 0.621797.
Train: 2018-08-08T19:15:34.144454: step 2585, loss 0.507359.
Train: 2018-08-08T19:15:37.316888: step 2586, loss 0.582892.
Train: 2018-08-08T19:15:40.500352: step 2587, loss 0.570115.
Train: 2018-08-08T19:15:43.666771: step 2588, loss 0.558199.
Train: 2018-08-08T19:15:46.993616: step 2589, loss 0.614369.
Train: 2018-08-08T19:15:50.187106: step 2590, loss 0.538311.
Test: 2018-08-08T19:16:11.130791: step 2590, loss 0.552271.
Train: 2018-08-08T19:16:14.269134: step 2591, loss 0.564254.
Train: 2018-08-08T19:16:17.449590: step 2592, loss 0.633901.
Train: 2018-08-08T19:16:20.622024: step 2593, loss 0.627618.
Train: 2018-08-08T19:16:23.810502: step 2594, loss 0.532401.
Train: 2018-08-08T19:16:26.963886: step 2595, loss 0.534517.
Train: 2018-08-08T19:16:30.119275: step 2596, loss 0.556356.
Train: 2018-08-08T19:16:33.492243: step 2597, loss 0.480197.
Train: 2018-08-08T19:16:36.682726: step 2598, loss 0.599867.
Train: 2018-08-08T19:16:39.871203: step 2599, loss 0.524323.
Train: 2018-08-08T19:16:43.053664: step 2600, loss 0.441028.
Test: 2018-08-08T19:17:04.489475: step 2600, loss 0.546947.
Train: 2018-08-08T19:17:09.569982: step 2601, loss 0.429613.
Train: 2018-08-08T19:17:12.777510: step 2602, loss 0.634516.
Train: 2018-08-08T19:17:15.942926: step 2603, loss 0.615191.
Train: 2018-08-08T19:17:19.161483: step 2604, loss 0.591195.
Train: 2018-08-08T19:17:22.304840: step 2605, loss 0.588551.
Train: 2018-08-08T19:17:25.545456: step 2606, loss 0.587516.
Train: 2018-08-08T19:17:28.720899: step 2607, loss 0.637813.
Train: 2018-08-08T19:17:31.872278: step 2608, loss 0.609359.
Train: 2018-08-08T19:17:35.169043: step 2609, loss 0.531429.
Train: 2018-08-08T19:17:38.615573: step 2610, loss 0.582643.
Test: 2018-08-08T19:18:00.400911: step 2610, loss 0.555824.
Train: 2018-08-08T19:18:03.593399: step 2611, loss 0.504955.
Train: 2018-08-08T19:18:06.787893: step 2612, loss 0.521385.
Train: 2018-08-08T19:18:09.951303: step 2613, loss 0.47988.
Train: 2018-08-08T19:18:13.151812: step 2614, loss 0.482075.
Train: 2018-08-08T19:18:16.298178: step 2615, loss 0.616914.
Train: 2018-08-08T19:18:19.471615: step 2616, loss 0.575596.
Train: 2018-08-08T19:18:22.661095: step 2617, loss 0.530774.
Train: 2018-08-08T19:18:25.860602: step 2618, loss 0.55358.
Train: 2018-08-08T19:18:29.016994: step 2619, loss 0.492187.
Train: 2018-08-08T19:18:32.192436: step 2620, loss 0.575887.
Test: 2018-08-08T19:18:53.758939: step 2620, loss 0.552962.
Train: 2018-08-08T19:18:56.971480: step 2621, loss 0.530931.
Train: 2018-08-08T19:19:00.182016: step 2622, loss 0.550853.
Train: 2018-08-08T19:19:03.364477: step 2623, loss 0.470349.
Train: 2018-08-08T19:19:06.557967: step 2624, loss 0.621959.
Train: 2018-08-08T19:19:09.776525: step 2625, loss 0.542284.
Train: 2018-08-08T19:19:13.090963: step 2626, loss 0.546617.
Train: 2018-08-08T19:19:16.309520: step 2627, loss 0.582979.
Train: 2018-08-08T19:19:19.563171: step 2628, loss 0.607914.
Train: 2018-08-08T19:19:22.807797: step 2629, loss 0.500709.
Train: 2018-08-08T19:19:26.029362: step 2630, loss 0.548362.
Test: 2018-08-08T19:19:47.374112: step 2630, loss 0.551549.
Train: 2018-08-08T19:19:50.587656: step 2631, loss 0.475399.
Train: 2018-08-08T19:19:53.818245: step 2632, loss 0.539314.
Train: 2018-08-08T19:19:57.059864: step 2633, loss 0.593132.
Train: 2018-08-08T19:20:00.333568: step 2634, loss 0.473356.
Train: 2018-08-08T19:20:03.674565: step 2635, loss 0.599912.
Train: 2018-08-08T19:20:06.890114: step 2636, loss 0.553673.
Train: 2018-08-08T19:20:10.120703: step 2637, loss 0.607144.
Train: 2018-08-08T19:20:13.431506: step 2638, loss 0.640575.
Train: 2018-08-08T19:20:16.696186: step 2639, loss 0.525691.
Train: 2018-08-08T19:20:19.954879: step 2640, loss 0.614132.
Test: 2018-08-08T19:20:41.524278: step 2640, loss 0.555186.
Train: 2018-08-08T19:20:44.728798: step 2641, loss 0.597253.
Train: 2018-08-08T19:20:47.930309: step 2642, loss 0.566622.
Train: 2018-08-08T19:20:51.105752: step 2643, loss 0.536996.
Train: 2018-08-08T19:20:54.285205: step 2644, loss 0.501034.
Train: 2018-08-08T19:20:57.450621: step 2645, loss 0.423308.
Train: 2018-08-08T19:21:00.603003: step 2646, loss 0.462162.
Train: 2018-08-08T19:21:03.807523: step 2647, loss 0.427209.
Train: 2018-08-08T19:21:06.996000: step 2648, loss 0.406675.
Train: 2018-08-08T19:21:10.185480: step 2649, loss 0.601585.
Train: 2018-08-08T19:21:13.397018: step 2650, loss 0.568689.
Test: 2018-08-08T19:21:34.538230: step 2650, loss 0.5565.
Train: 2018-08-08T19:21:37.731718: step 2651, loss 0.737417.
Train: 2018-08-08T19:21:40.968400: step 2652, loss 0.539076.
Train: 2018-08-08T19:21:44.182947: step 2653, loss 0.587685.
Train: 2018-08-08T19:21:47.461664: step 2654, loss 0.510219.
Train: 2018-08-08T19:21:50.739379: step 2655, loss 0.670333.
Train: 2018-08-08T19:21:54.018096: step 2656, loss 0.593737.
Train: 2018-08-08T19:21:57.248078: step 2657, loss 0.543618.
Train: 2018-08-08T19:22:00.558881: step 2658, loss 0.646548.
Train: 2018-08-08T19:22:03.721289: step 2659, loss 0.537922.
Train: 2018-08-08T19:22:06.890715: step 2660, loss 0.6082.
Test: 2018-08-08T19:22:27.988810: step 2660, loss 0.558287.
Train: 2018-08-08T19:22:31.138184: step 2661, loss 0.502746.
Train: 2018-08-08T19:22:34.343705: step 2662, loss 0.555766.
Train: 2018-08-08T19:22:37.529175: step 2663, loss 0.636247.
Train: 2018-08-08T19:22:40.691583: step 2664, loss 0.512421.
Train: 2018-08-08T19:22:43.860006: step 2665, loss 0.525613.
Train: 2018-08-08T19:22:47.112654: step 2666, loss 0.552613.
Train: 2018-08-08T19:22:50.368310: step 2667, loss 0.577409.
Train: 2018-08-08T19:22:53.757321: step 2668, loss 0.604208.
Train: 2018-08-08T19:22:57.133296: step 2669, loss 0.549607.
Train: 2018-08-08T19:23:00.476184: step 2670, loss 0.50543.
Test: 2018-08-08T19:23:22.018460: step 2670, loss 0.552788.
Train: 2018-08-08T19:23:25.302190: step 2671, loss 0.594711.
Train: 2018-08-08T19:23:28.502699: step 2672, loss 0.609912.
Train: 2018-08-08T19:23:31.723262: step 2673, loss 0.58.
Train: 2018-08-08T19:23:34.918758: step 2674, loss 0.554255.
Train: 2018-08-08T19:23:38.098211: step 2675, loss 0.600506.
Train: 2018-08-08T19:23:41.252598: step 2676, loss 0.63787.
Train: 2018-08-08T19:23:44.458120: step 2677, loss 0.508368.
Train: 2018-08-08T19:23:47.681691: step 2678, loss 0.547281.
Train: 2018-08-08T19:23:50.886211: step 2679, loss 0.58619.
Train: 2018-08-08T19:23:54.105770: step 2680, loss 0.635373.
Test: 2018-08-08T19:24:15.294105: step 2680, loss 0.554806.
Train: 2018-08-08T19:24:18.492608: step 2681, loss 0.445813.
Train: 2018-08-08T19:24:21.694120: step 2682, loss 0.631597.
Train: 2018-08-08T19:24:24.975845: step 2683, loss 0.612305.
Train: 2018-08-08T19:24:28.508237: step 2684, loss 0.534009.
Train: 2018-08-08T19:24:31.771914: step 2685, loss 0.550015.
Train: 2018-08-08T19:24:35.074051: step 2686, loss 0.565423.
Train: 2018-08-08T19:24:38.263530: step 2687, loss 0.644041.
Train: 2018-08-08T19:24:41.459026: step 2688, loss 0.469545.
Train: 2018-08-08T19:24:44.655525: step 2689, loss 0.487355.
Train: 2018-08-08T19:24:47.846007: step 2690, loss 0.551936.
Test: 2018-08-08T19:25:08.955131: step 2690, loss 0.553486.
Train: 2018-08-08T19:25:12.141603: step 2691, loss 0.475268.
Train: 2018-08-08T19:25:15.333088: step 2692, loss 0.593631.
Train: 2018-08-08T19:25:18.554653: step 2693, loss 0.530587.
Train: 2018-08-08T19:25:21.713051: step 2694, loss 0.560784.
Train: 2018-08-08T19:25:24.897517: step 2695, loss 0.620062.
Train: 2018-08-08T19:25:28.095018: step 2696, loss 0.48682.
Train: 2018-08-08T19:25:31.322600: step 2697, loss 0.548905.
Train: 2018-08-08T19:25:34.504059: step 2698, loss 0.551226.
Train: 2018-08-08T19:25:37.763725: step 2699, loss 0.540777.
Train: 2018-08-08T19:25:40.940170: step 2700, loss 0.631343.
Test: 2018-08-08T19:26:01.986126: step 2700, loss 0.551587.
Train: 2018-08-08T19:26:07.039561: step 2701, loss 0.540449.
Train: 2018-08-08T19:26:10.286193: step 2702, loss 0.468836.
Train: 2018-08-08T19:26:13.468654: step 2703, loss 0.442495.
Train: 2018-08-08T19:26:16.855660: step 2704, loss 0.372179.
Train: 2018-08-08T19:26:20.208574: step 2705, loss 0.697948.
Train: 2018-08-08T19:26:23.561489: step 2706, loss 0.506651.
Train: 2018-08-08T19:26:26.925432: step 2707, loss 0.499823.
Train: 2018-08-08T19:26:30.312438: step 2708, loss 0.583438.
Train: 2018-08-08T19:26:33.395635: step 2709, loss 0.501495.
Train: 2018-08-08T19:26:36.717467: step 2710, loss 0.553607.
Test: 2018-08-08T19:26:57.857842: step 2710, loss 0.552175.
Train: 2018-08-08T19:27:01.323870: step 2711, loss 0.629604.
Train: 2018-08-08T19:27:04.560476: step 2712, loss 0.591759.
Train: 2018-08-08T19:27:07.763993: step 2713, loss 0.587918.
Train: 2018-08-08T19:27:10.951467: step 2714, loss 0.665445.
Train: 2018-08-08T19:27:14.070761: step 2715, loss 0.519041.
Train: 2018-08-08T19:27:17.286310: step 2716, loss 0.550404.
Train: 2018-08-08T19:27:20.479800: step 2717, loss 0.548759.
Train: 2018-08-08T19:27:23.682316: step 2718, loss 0.576798.
Train: 2018-08-08T19:27:26.882825: step 2719, loss 0.660701.
Train: 2018-08-08T19:27:30.158621: step 2720, loss 0.555984.
Test: 2018-08-08T19:27:52.147090: step 2720, loss 0.553185.
Train: 2018-08-08T19:27:55.350608: step 2721, loss 0.565354.
Train: 2018-08-08T19:27:58.545101: step 2722, loss 0.591192.
Train: 2018-08-08T19:28:01.734580: step 2723, loss 0.581863.
Train: 2018-08-08T19:28:04.988369: step 2724, loss 0.520366.
Train: 2018-08-08T19:28:08.202915: step 2725, loss 0.516024.
Train: 2018-08-08T19:28:11.426486: step 2726, loss 0.532333.
Train: 2018-08-08T19:28:14.631006: step 2727, loss 0.656483.
Train: 2018-08-08T19:28:17.831515: step 2728, loss 0.566404.
Train: 2018-08-08T19:28:21.026008: step 2729, loss 0.45166.
Train: 2018-08-08T19:28:24.208469: step 2730, loss 0.550672.
Test: 2018-08-08T19:28:45.387780: step 2730, loss 0.55738.
Train: 2018-08-08T19:28:48.827217: step 2731, loss 0.576855.
Train: 2018-08-08T19:28:52.017700: step 2732, loss 0.530587.
Train: 2018-08-08T19:28:55.190134: step 2733, loss 0.518297.
Train: 2018-08-08T19:28:58.426740: step 2734, loss 0.503001.
Train: 2018-08-08T19:29:01.577116: step 2735, loss 0.535821.
Train: 2018-08-08T19:29:04.786649: step 2736, loss 0.606323.
Train: 2018-08-08T19:29:07.991169: step 2737, loss 0.491995.
Train: 2018-08-08T19:29:11.193683: step 2738, loss 0.535296.
Train: 2018-08-08T19:29:14.402214: step 2739, loss 0.416929.
Train: 2018-08-08T19:29:17.610744: step 2740, loss 0.515042.
Test: 2018-08-08T19:29:39.064253: step 2740, loss 0.549373.
Train: 2018-08-08T19:29:42.426010: step 2741, loss 0.442582.
Train: 2018-08-08T19:29:45.757868: step 2742, loss 0.455511.
Train: 2018-08-08T19:29:48.974420: step 2743, loss 0.683341.
Train: 2018-08-08T19:29:52.169916: step 2744, loss 0.537463.
Train: 2018-08-08T19:29:55.375438: step 2745, loss 0.645348.
Train: 2018-08-08T19:29:58.568929: step 2746, loss 0.386298.
Train: 2018-08-08T19:30:01.927860: step 2747, loss 0.406635.
Train: 2018-08-08T19:30:05.139398: step 2748, loss 0.561649.
Train: 2018-08-08T19:30:08.367982: step 2749, loss 0.505145.
Train: 2018-08-08T19:30:11.607595: step 2750, loss 0.561956.
Test: 2018-08-08T19:30:32.848068: step 2750, loss 0.552487.
Train: 2018-08-08T19:30:36.151852: step 2751, loss 0.608538.
Train: 2018-08-08T19:30:39.619070: step 2752, loss 0.574598.
Train: 2018-08-08T19:30:43.013094: step 2753, loss 0.554299.
Train: 2018-08-08T19:30:46.382051: step 2754, loss 0.572806.
Train: 2018-08-08T19:30:49.742988: step 2755, loss 0.537924.
Train: 2018-08-08T19:30:53.070835: step 2756, loss 0.398606.
Train: 2018-08-08T19:30:56.370608: step 2757, loss 0.54242.
Train: 2018-08-08T19:30:59.621250: step 2758, loss 0.547809.
Train: 2018-08-08T19:31:02.863872: step 2759, loss 0.526749.
Train: 2018-08-08T19:31:06.138578: step 2760, loss 0.578374.
Test: 2018-08-08T19:31:27.271094: step 2760, loss 0.558449.
Train: 2018-08-08T19:31:30.616926: step 2761, loss 0.537429.
Train: 2018-08-08T19:31:33.904667: step 2762, loss 0.546925.
Train: 2018-08-08T19:31:37.254574: step 2763, loss 0.567645.
Train: 2018-08-08T19:31:40.446059: step 2764, loss 0.546746.
Train: 2018-08-08T19:31:43.684670: step 2765, loss 0.52334.
Train: 2018-08-08T19:31:46.880166: step 2766, loss 0.502188.
Train: 2018-08-08T19:31:50.068643: step 2767, loss 0.511494.
Train: 2018-08-08T19:31:53.255115: step 2768, loss 0.618439.
Train: 2018-08-08T19:31:56.466654: step 2769, loss 0.54353.
Train: 2018-08-08T19:31:59.687216: step 2770, loss 0.45552.
Test: 2018-08-08T19:32:20.747209: step 2770, loss 0.556433.
Train: 2018-08-08T19:32:23.939697: step 2771, loss 0.53278.
Train: 2018-08-08T19:32:27.116142: step 2772, loss 0.514812.
Train: 2018-08-08T19:32:30.323670: step 2773, loss 0.718614.
Train: 2018-08-08T19:32:33.503123: step 2774, loss 0.617789.
Train: 2018-08-08T19:32:36.766800: step 2775, loss 0.628002.
Train: 2018-08-08T19:32:39.946254: step 2776, loss 0.538211.
Train: 2018-08-08T19:32:43.158795: step 2777, loss 0.451003.
Train: 2018-08-08T19:32:46.338248: step 2778, loss 0.603402.
Train: 2018-08-08T19:32:49.542768: step 2779, loss 0.656064.
Train: 2018-08-08T19:32:52.735256: step 2780, loss 0.582595.
Test: 2018-08-08T19:33:13.907548: step 2780, loss 0.558809.
Train: 2018-08-08T19:33:17.128110: step 2781, loss 0.533069.
Train: 2018-08-08T19:33:20.432896: step 2782, loss 0.51629.
Train: 2018-08-08T19:33:23.710611: step 2783, loss 0.55056.
Train: 2018-08-08T19:33:26.922150: step 2784, loss 0.50091.
Train: 2018-08-08T19:33:30.117646: step 2785, loss 0.537981.
Train: 2018-08-08T19:33:33.354251: step 2786, loss 0.535253.
Train: 2018-08-08T19:33:36.632968: step 2787, loss 0.63466.
Train: 2018-08-08T19:33:39.863557: step 2788, loss 0.548543.
Train: 2018-08-08T19:33:43.108184: step 2789, loss 0.590778.
Train: 2018-08-08T19:33:46.285632: step 2790, loss 0.561037.
Test: 2018-08-08T19:34:07.571225: step 2790, loss 0.547935.
Train: 2018-08-08T19:34:10.860971: step 2791, loss 0.43267.
Train: 2018-08-08T19:34:14.105597: step 2792, loss 0.750656.
Train: 2018-08-08T19:34:17.392336: step 2793, loss 0.541081.
Train: 2018-08-08T19:34:20.648994: step 2794, loss 0.616578.
Train: 2018-08-08T19:34:23.875573: step 2795, loss 0.534861.
Train: 2018-08-08T19:34:27.127218: step 2796, loss 0.54332.
Train: 2018-08-08T19:34:30.357808: step 2797, loss 0.534841.
Train: 2018-08-08T19:34:33.639533: step 2798, loss 0.569875.
Train: 2018-08-08T19:34:36.883157: step 2799, loss 0.607766.
Train: 2018-08-08T19:34:40.161874: step 2800, loss 0.629331.
Test: 2018-08-08T19:35:01.272000: step 2800, loss 0.561956.
Train: 2018-08-08T19:35:06.262267: step 2801, loss 0.49945.
Train: 2018-08-08T19:35:09.435705: step 2802, loss 0.533708.
Train: 2018-08-08T19:35:12.683339: step 2803, loss 0.460884.
Train: 2018-08-08T19:35:15.916937: step 2804, loss 0.691867.
Train: 2018-08-08T19:35:19.089371: step 2805, loss 0.569936.
Train: 2018-08-08T19:35:22.309934: step 2806, loss 0.559759.
Train: 2018-08-08T19:35:25.537515: step 2807, loss 0.480168.
Train: 2018-08-08T19:35:28.719976: step 2808, loss 0.498049.
Train: 2018-08-08T19:35:31.884390: step 2809, loss 0.414759.
Train: 2018-08-08T19:35:35.106958: step 2810, loss 0.5783.
Test: 2018-08-08T19:35:56.136871: step 2810, loss 0.560512.
Train: 2018-08-08T19:35:59.394532: step 2811, loss 0.513724.
Train: 2018-08-08T19:36:02.598049: step 2812, loss 0.508953.
Train: 2018-08-08T19:36:05.807582: step 2813, loss 0.570963.
Train: 2018-08-08T19:36:09.001073: step 2814, loss 0.569424.
Train: 2018-08-08T19:36:12.203587: step 2815, loss 0.496044.
Train: 2018-08-08T19:36:15.380032: step 2816, loss 0.604263.
Train: 2018-08-08T19:36:18.574526: step 2817, loss 0.604364.
Train: 2018-08-08T19:36:21.739942: step 2818, loss 0.566225.
Train: 2018-08-08T19:36:24.935438: step 2819, loss 0.49554.
Train: 2018-08-08T19:36:28.257388: step 2820, loss 0.576852.
Test: 2018-08-08T19:36:49.415299: step 2820, loss 0.562359.
Train: 2018-08-08T19:36:52.680981: step 2821, loss 0.526278.
Train: 2018-08-08T19:36:55.942653: step 2822, loss 0.496546.
Train: 2018-08-08T19:36:59.509135: step 2823, loss 0.526026.
Train: 2018-08-08T19:37:02.966327: step 2824, loss 0.583215.
Train: 2018-08-08T19:37:06.428532: step 2825, loss 0.551131.
Train: 2018-08-08T19:37:10.068138: step 2826, loss 0.498033.
Train: 2018-08-08T19:37:13.732881: step 2827, loss 0.52915.
Train: 2018-08-08T19:37:17.330446: step 2828, loss 0.468767.
Train: 2018-08-08T19:37:20.966113: step 2829, loss 0.594023.
Train: 2018-08-08T19:37:24.534600: step 2830, loss 0.564837.
Test: 2018-08-08T19:37:46.173787: step 2830, loss 0.561226.
Train: 2018-08-08T19:37:49.347223: step 2831, loss 0.6223.
Train: 2018-08-08T19:37:52.527679: step 2832, loss 0.530308.
Train: 2018-08-08T19:37:55.736210: step 2833, loss 0.654232.
Train: 2018-08-08T19:37:58.921679: step 2834, loss 0.631262.
Train: 2018-08-08T19:38:02.079074: step 2835, loss 0.573721.
Train: 2018-08-08T19:38:05.287604: step 2836, loss 0.578282.
Train: 2018-08-08T19:38:08.547271: step 2837, loss 0.544832.
Train: 2018-08-08T19:38:11.735748: step 2838, loss 0.555559.
Train: 2018-08-08T19:38:14.915201: step 2839, loss 0.498247.
Train: 2018-08-08T19:38:18.125737: step 2840, loss 0.485152.
Test: 2018-08-08T19:38:39.557326: step 2840, loss 0.557546.
Train: 2018-08-08T19:38:42.788918: step 2841, loss 0.610795.
Train: 2018-08-08T19:38:46.102927: step 2842, loss 0.640485.
Train: 2018-08-08T19:38:49.394177: step 2843, loss 0.444133.
Train: 2018-08-08T19:38:52.618751: step 2844, loss 0.554009.
Train: 2018-08-08T19:38:55.832294: step 2845, loss 0.585641.
Train: 2018-08-08T19:38:59.016310: step 2846, loss 0.55987.
Train: 2018-08-08T19:39:02.211820: step 2847, loss 0.532023.
Train: 2018-08-08T19:39:05.428372: step 2848, loss 0.48275.
Train: 2018-08-08T19:39:08.696060: step 2849, loss 0.583482.
Train: 2018-08-08T19:39:11.872505: step 2850, loss 0.576619.
Test: 2018-08-08T19:39:32.904424: step 2850, loss 0.554896.
Train: 2018-08-08T19:39:36.159076: step 2851, loss 0.566968.
Train: 2018-08-08T19:39:39.362594: step 2852, loss 0.474051.
Train: 2018-08-08T19:39:42.542047: step 2853, loss 0.617323.
Train: 2018-08-08T19:39:45.744562: step 2854, loss 0.5681.
Train: 2018-08-08T19:39:48.929028: step 2855, loss 0.374114.
Train: 2018-08-08T19:39:52.079404: step 2856, loss 0.476955.
Train: 2018-08-08T19:39:55.201705: step 2857, loss 0.450975.
Train: 2018-08-08T19:39:58.387175: step 2858, loss 0.700872.
Train: 2018-08-08T19:40:01.721844: step 2859, loss 0.444007.
Train: 2018-08-08T19:40:04.913329: step 2860, loss 0.567842.
Test: 2018-08-08T19:40:26.026464: step 2860, loss 0.55334.
Train: 2018-08-08T19:40:29.209927: step 2861, loss 0.554188.
Train: 2018-08-08T19:40:32.399407: step 2862, loss 0.50307.
Train: 2018-08-08T19:40:35.593901: step 2863, loss 0.541049.
Train: 2018-08-08T19:40:38.773354: step 2864, loss 0.578878.
Train: 2018-08-08T19:40:41.966844: step 2865, loss 0.559759.
Train: 2018-08-08T19:40:45.161337: step 2866, loss 0.568269.
Train: 2018-08-08T19:40:48.339788: step 2867, loss 0.517743.
Train: 2018-08-08T19:40:51.522250: step 2868, loss 0.546459.
Train: 2018-08-08T19:40:54.706716: step 2869, loss 0.572731.
Train: 2018-08-08T19:40:57.855087: step 2870, loss 0.570852.
Test: 2018-08-08T19:41:18.966216: step 2870, loss 0.552502.
Train: 2018-08-08T19:41:22.123610: step 2871, loss 0.532701.
Train: 2018-08-08T19:41:25.336151: step 2872, loss 0.521877.
Train: 2018-08-08T19:41:28.560725: step 2873, loss 0.54.
Train: 2018-08-08T19:41:31.801340: step 2874, loss 0.485092.
Train: 2018-08-08T19:41:35.001850: step 2875, loss 0.472772.
Train: 2018-08-08T19:41:38.187319: step 2876, loss 0.502532.
Train: 2018-08-08T19:41:41.413898: step 2877, loss 0.632641.
Train: 2018-08-08T19:41:44.604380: step 2878, loss 0.598602.
Train: 2018-08-08T19:41:47.771801: step 2879, loss 0.507314.
Train: 2018-08-08T19:41:50.970305: step 2880, loss 0.596156.
Test: 2018-08-08T19:42:12.060378: step 2880, loss 0.55199.
Train: 2018-08-08T19:42:15.337090: step 2881, loss 0.621117.
Train: 2018-08-08T19:42:18.598762: step 2882, loss 0.495101.
Train: 2018-08-08T19:42:21.805287: step 2883, loss 0.52462.
Train: 2018-08-08T19:42:25.010810: step 2884, loss 0.462287.
Train: 2018-08-08T19:42:28.246412: step 2885, loss 0.501199.
Train: 2018-08-08T19:42:31.450932: step 2886, loss 0.425849.
Train: 2018-08-08T19:42:34.638407: step 2887, loss 0.625652.
Train: 2018-08-08T19:42:37.847940: step 2888, loss 0.514802.
Train: 2018-08-08T19:42:40.992300: step 2889, loss 0.501392.
Train: 2018-08-08T19:42:44.191807: step 2890, loss 0.59877.
Test: 2018-08-08T19:43:05.224728: step 2890, loss 0.552834.
Train: 2018-08-08T19:43:08.483391: step 2891, loss 0.624936.
Train: 2018-08-08T19:43:11.687912: step 2892, loss 0.655081.
Train: 2018-08-08T19:43:14.877391: step 2893, loss 0.561962.
Train: 2018-08-08T19:43:18.039799: step 2894, loss 0.524451.
Train: 2018-08-08T19:43:21.226271: step 2895, loss 0.522633.
Train: 2018-08-08T19:43:24.437810: step 2896, loss 0.614458.
Train: 2018-08-08T19:43:27.641327: step 2897, loss 0.519041.
Train: 2018-08-08T19:43:30.865900: step 2898, loss 0.613277.
Train: 2018-08-08T19:43:34.081449: step 2899, loss 0.527657.
Train: 2018-08-08T19:43:37.291986: step 2900, loss 0.585711.
Test: 2018-08-08T19:43:58.355990: step 2900, loss 0.556092.
Train: 2018-08-08T19:44:03.385360: step 2901, loss 0.531697.
Train: 2018-08-08T19:44:06.616952: step 2902, loss 0.578695.
Train: 2018-08-08T19:44:09.879627: step 2903, loss 0.52737.
Train: 2018-08-08T19:44:13.050056: step 2904, loss 0.485973.
Train: 2018-08-08T19:44:16.261595: step 2905, loss 0.742312.
Train: 2018-08-08T19:44:19.445059: step 2906, loss 0.57317.
Train: 2018-08-08T19:44:22.629525: step 2907, loss 0.590858.
Train: 2018-08-08T19:44:25.855101: step 2908, loss 0.592509.
Train: 2018-08-08T19:44:29.092709: step 2909, loss 0.513233.
Train: 2018-08-08T19:44:32.343352: step 2910, loss 0.619857.
Test: 2018-08-08T19:44:53.547729: step 2910, loss 0.556473.
Train: 2018-08-08T19:44:56.784334: step 2911, loss 0.577667.
Train: 2018-08-08T19:44:59.968800: step 2912, loss 0.618972.
Train: 2018-08-08T19:45:03.166301: step 2913, loss 0.5807.
Train: 2018-08-08T19:45:06.332720: step 2914, loss 0.548944.
Train: 2018-08-08T19:45:09.628483: step 2915, loss 0.617133.
Train: 2018-08-08T19:45:12.876117: step 2916, loss 0.699033.
Train: 2018-08-08T19:45:16.115730: step 2917, loss 0.535537.
Train: 2018-08-08T19:45:19.313232: step 2918, loss 0.518612.
Train: 2018-08-08T19:45:22.526775: step 2919, loss 0.55393.
Train: 2018-08-08T19:45:25.741322: step 2920, loss 0.586893.
Test: 2018-08-08T19:45:46.819363: step 2920, loss 0.557461.
Train: 2018-08-08T19:45:49.989792: step 2921, loss 0.495604.
Train: 2018-08-08T19:45:53.143176: step 2922, loss 0.545861.
Train: 2018-08-08T19:45:56.361733: step 2923, loss 0.66423.
Train: 2018-08-08T19:45:59.548205: step 2924, loss 0.557221.
Train: 2018-08-08T19:46:02.725653: step 2925, loss 0.524311.
Train: 2018-08-08T19:46:05.916136: step 2926, loss 0.711931.
Train: 2018-08-08T19:46:09.205882: step 2927, loss 0.5929.
Train: 2018-08-08T19:46:12.403384: step 2928, loss 0.529827.
Train: 2018-08-08T19:46:15.607903: step 2929, loss 0.60754.
Train: 2018-08-08T19:46:18.790365: step 2930, loss 0.495369.
Test: 2018-08-08T19:46:40.075958: step 2930, loss 0.552772.
Train: 2018-08-08T19:46:43.383530: step 2931, loss 0.606822.
Train: 2018-08-08T19:46:46.672775: step 2932, loss 0.499742.
Train: 2018-08-08T19:46:49.980068: step 2933, loss 0.60465.
Train: 2018-08-08T19:46:53.231215: step 2934, loss 0.592547.
Train: 2018-08-08T19:46:56.416275: step 2935, loss 0.46841.
Train: 2018-08-08T19:46:59.614795: step 2936, loss 0.526821.
Train: 2018-08-08T19:47:02.805278: step 2937, loss 0.476081.
Train: 2018-08-08T19:47:06.016816: step 2938, loss 0.600137.
Train: 2018-08-08T19:47:09.311576: step 2939, loss 0.540364.
Train: 2018-08-08T19:47:12.572246: step 2940, loss 0.530221.
Test: 2018-08-08T19:47:33.910980: step 2940, loss 0.556746.
Train: 2018-08-08T19:47:37.118507: step 2941, loss 0.557418.
Train: 2018-08-08T19:47:40.329043: step 2942, loss 0.5261.
Train: 2018-08-08T19:47:43.564646: step 2943, loss 0.459088.
Train: 2018-08-08T19:47:46.797240: step 2944, loss 0.612578.
Train: 2018-08-08T19:47:50.029835: step 2945, loss 0.61691.
Train: 2018-08-08T19:47:53.285491: step 2946, loss 0.53931.
Train: 2018-08-08T19:47:56.526106: step 2947, loss 0.573735.
Train: 2018-08-08T19:47:59.711576: step 2948, loss 0.59352.
Train: 2018-08-08T19:48:02.902058: step 2949, loss 0.603358.
Train: 2018-08-08T19:48:06.105576: step 2950, loss 0.625986.
Test: 2018-08-08T19:48:27.255808: step 2950, loss 0.557376.
Train: 2018-08-08T19:48:30.434259: step 2951, loss 0.564258.
Train: 2018-08-08T19:48:33.622736: step 2952, loss 0.538486.
Train: 2018-08-08T19:48:36.817229: step 2953, loss 0.610807.
Train: 2018-08-08T19:48:40.086923: step 2954, loss 0.564403.
Train: 2018-08-08T19:48:43.260360: step 2955, loss 0.518023.
Train: 2018-08-08T19:48:46.413744: step 2956, loss 0.54605.
Train: 2018-08-08T19:48:49.611245: step 2957, loss 0.44783.
Train: 2018-08-08T19:48:52.848853: step 2958, loss 0.497574.
Train: 2018-08-08T19:48:56.070418: step 2959, loss 0.432054.
Train: 2018-08-08T19:48:59.287973: step 2960, loss 0.461771.
Test: 2018-08-08T19:49:20.689850: step 2960, loss 0.550995.
Train: 2018-08-08T19:49:23.886348: step 2961, loss 0.482537.
Train: 2018-08-08T19:49:27.093876: step 2962, loss 0.730851.
Train: 2018-08-08T19:49:30.281351: step 2963, loss 0.599974.
Train: 2018-08-08T19:49:33.472836: step 2964, loss 0.583614.
Train: 2018-08-08T19:49:36.692396: step 2965, loss 0.589074.
Train: 2018-08-08T19:49:39.860822: step 2966, loss 0.589509.
Train: 2018-08-08T19:49:43.064337: step 2967, loss 0.52294.
Train: 2018-08-08T19:49:46.476409: step 2968, loss 0.597987.
Train: 2018-08-08T19:49:49.676918: step 2969, loss 0.62406.
Train: 2018-08-08T19:49:52.871412: step 2970, loss 0.579916.
Test: 2018-08-08T19:50:13.995575: step 2970, loss 0.557165.
Train: 2018-08-08T19:50:17.212127: step 2971, loss 0.55253.
Train: 2018-08-08T19:50:20.449735: step 2972, loss 0.593897.
Train: 2018-08-08T19:50:23.660271: step 2973, loss 0.634284.
Train: 2018-08-08T19:50:26.901889: step 2974, loss 0.566257.
Train: 2018-08-08T19:50:30.101396: step 2975, loss 0.592515.
Train: 2018-08-08T19:50:33.301905: step 2976, loss 0.481288.
Train: 2018-08-08T19:50:36.467321: step 2977, loss 0.529339.
Train: 2018-08-08T19:50:39.644769: step 2978, loss 0.552572.
Train: 2018-08-08T19:50:42.953566: step 2979, loss 0.595511.
Train: 2018-08-08T19:50:46.042780: step 2980, loss 0.621495.
Test: 2018-08-08T19:51:06.904245: step 2980, loss 0.555436.
Train: 2018-08-08T19:51:10.051612: step 2981, loss 0.550729.
Train: 2018-08-08T19:51:13.170906: step 2982, loss 0.577094.
Train: 2018-08-08T19:51:16.247084: step 2983, loss 0.473476.
Train: 2018-08-08T19:51:19.364373: step 2984, loss 0.55991.
Train: 2018-08-08T19:51:22.499708: step 2985, loss 0.609633.
Train: 2018-08-08T19:51:25.628026: step 2986, loss 0.512188.
Train: 2018-08-08T19:51:28.735287: step 2987, loss 0.494378.
Train: 2018-08-08T19:51:31.825503: step 2988, loss 0.486854.
Train: 2018-08-08T19:51:34.935773: step 2989, loss 0.517246.
Train: 2018-08-08T19:51:38.041029: step 2990, loss 0.60014.
Test: 2018-08-08T19:51:58.876425: step 2990, loss 0.549711.
Train: 2018-08-08T19:52:01.961627: step 2991, loss 0.59481.
Train: 2018-08-08T19:52:05.042819: step 2992, loss 0.528188.
Train: 2018-08-08T19:52:08.182166: step 2993, loss 0.57496.
Train: 2018-08-08T19:52:11.301459: step 2994, loss 0.530514.
Train: 2018-08-08T19:52:14.374630: step 2995, loss 0.58137.
Train: 2018-08-08T19:52:17.467854: step 2996, loss 0.553923.
Train: 2018-08-08T19:52:20.635275: step 2997, loss 0.54645.
Train: 2018-08-08T19:52:23.717470: step 2998, loss 0.540348.
Train: 2018-08-08T19:52:26.829744: step 2999, loss 0.553776.
Train: 2018-08-08T19:52:29.942019: step 3000, loss 0.637278.
Test: 2018-08-08T19:52:50.761373: step 3000, loss 0.551877.
Train: 2018-08-08T19:52:55.699501: step 3001, loss 0.624023.
Train: 2018-08-08T19:52:58.783701: step 3002, loss 0.595111.
Train: 2018-08-08T19:53:01.880936: step 3003, loss 0.551168.
Train: 2018-08-08T19:53:04.960122: step 3004, loss 0.753061.
Train: 2018-08-08T19:53:08.025272: step 3005, loss 0.50369.
Train: 2018-08-08T19:53:11.168629: step 3006, loss 0.692837.
Train: 2018-08-08T19:53:14.240797: step 3007, loss 0.520686.
Train: 2018-08-08T19:53:17.294917: step 3008, loss 0.620581.
Train: 2018-08-08T19:53:20.398168: step 3009, loss 0.593674.
Train: 2018-08-08T19:53:23.327957: step 3010, loss 0.535859.
Test: 2018-08-08T19:53:44.140292: step 3010, loss 0.55566.
Train: 2018-08-08T19:53:47.218476: step 3011, loss 0.633824.
Train: 2018-08-08T19:53:50.309694: step 3012, loss 0.567366.
Train: 2018-08-08T19:53:53.402918: step 3013, loss 0.553451.
Train: 2018-08-08T19:53:56.517199: step 3014, loss 0.593627.
Train: 2018-08-08T19:53:59.605409: step 3015, loss 0.524609.
Train: 2018-08-08T19:54:02.693620: step 3016, loss 0.620823.
Train: 2018-08-08T19:54:05.787846: step 3017, loss 0.509615.
Train: 2018-08-08T19:54:08.930201: step 3018, loss 0.496307.
Train: 2018-08-08T19:54:12.024428: step 3019, loss 0.579771.
Train: 2018-08-08T19:54:15.114644: step 3020, loss 0.537024.
Test: 2018-08-08T19:54:35.977112: step 3020, loss 0.55428.
Train: 2018-08-08T19:54:39.070335: step 3021, loss 0.493018.
Train: 2018-08-08T19:54:42.214695: step 3022, loss 0.624832.
Train: 2018-08-08T19:54:45.297893: step 3023, loss 0.521539.
Train: 2018-08-08T19:54:48.402146: step 3024, loss 0.563989.
Train: 2018-08-08T19:54:51.480330: step 3025, loss 0.574577.
Train: 2018-08-08T19:54:54.610653: step 3026, loss 0.518044.
Train: 2018-08-08T19:54:57.725935: step 3027, loss 0.606605.
Train: 2018-08-08T19:55:00.823170: step 3028, loss 0.560935.
Train: 2018-08-08T19:55:03.919402: step 3029, loss 0.576266.
Train: 2018-08-08T19:55:07.015634: step 3030, loss 0.460639.
Test: 2018-08-08T19:55:28.010454: step 3030, loss 0.551975.
Train: 2018-08-08T19:55:31.154814: step 3031, loss 0.579508.
Train: 2018-08-08T19:55:34.278118: step 3032, loss 0.591534.
Train: 2018-08-08T19:55:37.366328: step 3033, loss 0.511586.
Train: 2018-08-08T19:55:40.434486: step 3034, loss 0.51639.
Train: 2018-08-08T19:55:43.519688: step 3035, loss 0.490771.
Train: 2018-08-08T19:55:46.625947: step 3036, loss 0.554379.
Train: 2018-08-08T19:55:49.738222: step 3037, loss 0.548142.
Train: 2018-08-08T19:55:52.845483: step 3038, loss 0.558353.
Train: 2018-08-08T19:55:55.959763: step 3039, loss 0.544728.
Train: 2018-08-08T19:55:59.042960: step 3040, loss 0.620809.
Test: 2018-08-08T19:56:19.925482: step 3040, loss 0.548681.
Train: 2018-08-08T19:56:23.004668: step 3041, loss 0.557127.
Train: 2018-08-08T19:56:26.077839: step 3042, loss 0.529047.
Train: 2018-08-08T19:56:29.171063: step 3043, loss 0.518956.
Train: 2018-08-08T19:56:32.263284: step 3044, loss 0.49365.
Train: 2018-08-08T19:56:35.370546: step 3045, loss 0.489191.
Train: 2018-08-08T19:56:38.469786: step 3046, loss 0.526114.
Train: 2018-08-08T19:56:41.578050: step 3047, loss 0.671853.
Train: 2018-08-08T19:56:44.692330: step 3048, loss 0.559494.
Train: 2018-08-08T19:56:47.753468: step 3049, loss 0.471706.
Train: 2018-08-08T19:56:50.838671: step 3050, loss 0.621564.
Test: 2018-08-08T19:57:11.733225: step 3050, loss 0.55057.
Train: 2018-08-08T19:57:14.846501: step 3051, loss 0.67025.
Train: 2018-08-08T19:57:17.928696: step 3052, loss 0.451907.
Train: 2018-08-08T19:57:21.039968: step 3053, loss 0.557693.
Train: 2018-08-08T19:57:24.135197: step 3054, loss 0.581291.
Train: 2018-08-08T19:57:27.219397: step 3055, loss 0.510166.
Train: 2018-08-08T19:57:30.313624: step 3056, loss 0.546184.
Train: 2018-08-08T19:57:33.404843: step 3057, loss 0.634601.
Train: 2018-08-08T19:57:36.498067: step 3058, loss 0.567563.
Train: 2018-08-08T19:57:39.575248: step 3059, loss 0.641179.
Train: 2018-08-08T19:57:42.689528: step 3060, loss 0.594343.
Test: 2018-08-08T19:58:03.528935: step 3060, loss 0.551602.
Train: 2018-08-08T19:58:06.620153: step 3061, loss 0.579185.
Train: 2018-08-08T19:58:09.796598: step 3062, loss 0.669354.
Train: 2018-08-08T19:58:12.881801: step 3063, loss 0.580598.
Train: 2018-08-08T19:58:15.958982: step 3064, loss 0.608679.
Train: 2018-08-08T19:58:19.028142: step 3065, loss 0.53579.
Train: 2018-08-08T19:58:22.162476: step 3066, loss 0.696297.
Train: 2018-08-08T19:58:25.266729: step 3067, loss 0.651875.
Train: 2018-08-08T19:58:28.306812: step 3068, loss 0.536814.
Train: 2018-08-08T19:58:31.417081: step 3069, loss 0.536722.
Train: 2018-08-08T19:58:34.503287: step 3070, loss 0.651053.
Test: 2018-08-08T19:58:55.326650: step 3070, loss 0.552215.
Train: 2018-08-08T19:58:58.424888: step 3071, loss 0.479937.
Train: 2018-08-08T19:59:01.563232: step 3072, loss 0.47871.
Train: 2018-08-08T19:59:04.654450: step 3073, loss 0.578894.
Train: 2018-08-08T19:59:07.773744: step 3074, loss 0.563585.
Train: 2018-08-08T19:59:10.932141: step 3075, loss 0.549394.
Train: 2018-08-08T19:59:14.023359: step 3076, loss 0.519391.
Train: 2018-08-08T19:59:17.109565: step 3077, loss 0.455362.
Train: 2018-08-08T19:59:20.176720: step 3078, loss 0.478634.
Train: 2018-08-08T19:59:23.301026: step 3079, loss 0.55055.
Train: 2018-08-08T19:59:26.388234: step 3080, loss 0.535907.
Test: 2018-08-08T19:59:47.213603: step 3080, loss 0.550285.
Train: 2018-08-08T19:59:50.296801: step 3081, loss 0.489499.
Train: 2018-08-08T19:59:53.401054: step 3082, loss 0.498127.
Train: 2018-08-08T19:59:56.503302: step 3083, loss 0.510255.
Train: 2018-08-08T19:59:59.594521: step 3084, loss 0.580643.
Train: 2018-08-08T20:00:02.689750: step 3085, loss 0.622344.
Train: 2018-08-08T20:00:05.811049: step 3086, loss 0.5819.
Train: 2018-08-08T20:00:08.970449: step 3087, loss 0.59718.
Train: 2018-08-08T20:00:12.068686: step 3088, loss 0.563464.
Train: 2018-08-08T20:00:15.145867: step 3089, loss 0.579199.
Train: 2018-08-08T20:00:18.244105: step 3090, loss 0.530897.
Test: 2018-08-08T20:00:39.150690: step 3090, loss 0.548554.
Train: 2018-08-08T20:00:42.263967: step 3091, loss 0.484847.
Train: 2018-08-08T20:00:45.367217: step 3092, loss 0.496824.
Train: 2018-08-08T20:00:48.470468: step 3093, loss 0.492461.
Train: 2018-08-08T20:00:51.568706: step 3094, loss 0.489896.
Train: 2018-08-08T20:00:54.683988: step 3095, loss 0.423623.
Train: 2018-08-08T20:00:57.775207: step 3096, loss 0.597712.
Train: 2018-08-08T20:01:00.893498: step 3097, loss 0.577316.
Train: 2018-08-08T20:01:03.985719: step 3098, loss 0.547603.
Train: 2018-08-08T20:01:07.062900: step 3099, loss 0.530892.
Train: 2018-08-08T20:01:10.243356: step 3100, loss 0.594409.
Test: 2018-08-08T20:01:31.094795: step 3100, loss 0.548988.
Train: 2018-08-08T20:01:35.988806: step 3101, loss 0.515915.
Train: 2018-08-08T20:01:39.066990: step 3102, loss 0.467375.
Train: 2018-08-08T20:01:42.166230: step 3103, loss 0.464564.
Train: 2018-08-08T20:01:45.290537: step 3104, loss 0.447575.
Train: 2018-08-08T20:01:48.367718: step 3105, loss 0.529264.
Train: 2018-08-08T20:01:51.459940: step 3106, loss 0.682755.
Train: 2018-08-08T20:01:54.545142: step 3107, loss 0.56224.
Train: 2018-08-08T20:01:57.636361: step 3108, loss 0.513218.
Train: 2018-08-08T20:02:00.763676: step 3109, loss 0.51131.
Train: 2018-08-08T20:02:03.842863: step 3110, loss 0.546029.
Test: 2018-08-08T20:02:24.728392: step 3110, loss 0.547105.
Train: 2018-08-08T20:02:27.819610: step 3111, loss 0.511649.
Train: 2018-08-08T20:02:30.911832: step 3112, loss 0.679732.
Train: 2018-08-08T20:02:33.992021: step 3113, loss 0.679653.
Train: 2018-08-08T20:02:37.088253: step 3114, loss 0.580392.
Train: 2018-08-08T20:02:40.180474: step 3115, loss 0.646916.
Train: 2018-08-08T20:02:43.286733: step 3116, loss 0.548032.
Train: 2018-08-08T20:02:46.363914: step 3117, loss 0.546651.
Train: 2018-08-08T20:02:49.434077: step 3118, loss 0.58041.
Train: 2018-08-08T20:02:52.537328: step 3119, loss 0.512764.
Train: 2018-08-08T20:02:55.637570: step 3120, loss 0.511179.
Test: 2018-08-08T20:03:16.512071: step 3120, loss 0.548076.
Train: 2018-08-08T20:03:19.634371: step 3121, loss 0.629851.
Train: 2018-08-08T20:03:22.759681: step 3122, loss 0.529038.
Train: 2018-08-08T20:03:25.870953: step 3123, loss 0.678903.
Train: 2018-08-08T20:03:28.966182: step 3124, loss 0.628261.
Train: 2018-08-08T20:03:32.060409: step 3125, loss 0.676729.
Train: 2018-08-08T20:03:35.158646: step 3126, loss 0.593707.
Train: 2018-08-08T20:03:38.324062: step 3127, loss 0.611012.
Train: 2018-08-08T20:03:41.430321: step 3128, loss 0.594535.
Train: 2018-08-08T20:03:44.542595: step 3129, loss 0.546288.
Train: 2018-08-08T20:03:47.626795: step 3130, loss 0.548168.
Test: 2018-08-08T20:04:08.558448: step 3130, loss 0.551232.
Train: 2018-08-08T20:04:11.675735: step 3131, loss 0.593188.
Train: 2018-08-08T20:04:14.800042: step 3132, loss 0.531008.
Train: 2018-08-08T20:04:17.946407: step 3133, loss 0.578608.
Train: 2018-08-08T20:04:21.039631: step 3134, loss 0.470824.
Train: 2018-08-08T20:04:24.130850: step 3135, loss 0.593836.
Train: 2018-08-08T20:04:27.194996: step 3136, loss 0.580043.
Train: 2018-08-08T20:04:30.273180: step 3137, loss 0.47185.
Train: 2018-08-08T20:04:33.351364: step 3138, loss 0.440252.
Train: 2018-08-08T20:04:36.458626: step 3139, loss 0.609191.
Train: 2018-08-08T20:04:39.562879: step 3140, loss 0.517253.
Test: 2018-08-08T20:05:00.435374: step 3140, loss 0.550698.
Train: 2018-08-08T20:05:03.517568: step 3141, loss 0.564126.
Train: 2018-08-08T20:05:06.624829: step 3142, loss 0.483669.
Train: 2018-08-08T20:05:09.775205: step 3143, loss 0.53067.
Train: 2018-08-08T20:05:12.889485: step 3144, loss 0.611286.
Train: 2018-08-08T20:05:16.085984: step 3145, loss 0.676162.
Train: 2018-08-08T20:05:19.157149: step 3146, loss 0.59499.
Train: 2018-08-08T20:05:22.230320: step 3147, loss 0.514182.
Train: 2018-08-08T20:05:25.378691: step 3148, loss 0.594888.
Train: 2018-08-08T20:05:28.460886: step 3149, loss 0.627794.
Train: 2018-08-08T20:05:31.576168: step 3150, loss 0.529847.
Test: 2018-08-08T20:05:52.443649: step 3150, loss 0.549883.
Train: 2018-08-08T20:05:55.570964: step 3151, loss 0.530613.
Train: 2018-08-08T20:05:58.685243: step 3152, loss 0.595264.
Train: 2018-08-08T20:06:01.774457: step 3153, loss 0.529793.
Train: 2018-08-08T20:06:04.886732: step 3154, loss 0.562962.
Train: 2018-08-08T20:06:08.002014: step 3155, loss 0.578571.
Train: 2018-08-08T20:06:11.164422: step 3156, loss 0.611259.
Train: 2018-08-08T20:06:14.249625: step 3157, loss 0.595015.
Train: 2018-08-08T20:06:17.362902: step 3158, loss 0.660092.
Train: 2018-08-08T20:06:20.466153: step 3159, loss 0.497556.
Train: 2018-08-08T20:06:23.555366: step 3160, loss 0.595358.
Test: 2018-08-08T20:06:44.413824: step 3160, loss 0.548802.
Train: 2018-08-08T20:06:47.515068: step 3161, loss 0.579094.
Train: 2018-08-08T20:06:50.623333: step 3162, loss 0.675048.
Train: 2018-08-08T20:06:53.708535: step 3163, loss 0.498874.
Train: 2018-08-08T20:06:56.788724: step 3164, loss 0.578277.
Train: 2018-08-08T20:06:59.887965: step 3165, loss 0.594698.
Train: 2018-08-08T20:07:03.003247: step 3166, loss 0.594802.
Train: 2018-08-08T20:07:06.105495: step 3167, loss 0.547041.
Train: 2018-08-08T20:07:09.274922: step 3168, loss 0.531098.
Train: 2018-08-08T20:07:12.365138: step 3169, loss 0.562709.
Train: 2018-08-08T20:07:15.483428: step 3170, loss 0.642525.
Test: 2018-08-08T20:07:36.328851: step 3170, loss 0.551763.
Train: 2018-08-08T20:07:39.430096: step 3171, loss 0.562971.
Train: 2018-08-08T20:07:42.545379: step 3172, loss 0.595236.
Train: 2018-08-08T20:07:45.625568: step 3173, loss 0.563835.
Train: 2018-08-08T20:07:48.734835: step 3174, loss 0.53148.
Train: 2018-08-08T20:07:51.817029: step 3175, loss 0.468593.
Train: 2018-08-08T20:07:54.923288: step 3176, loss 0.578635.
Train: 2018-08-08T20:07:58.034561: step 3177, loss 0.594381.
Train: 2018-08-08T20:08:01.112744: step 3178, loss 0.531336.
Train: 2018-08-08T20:08:04.211984: step 3179, loss 0.594757.
Train: 2018-08-08T20:08:07.294179: step 3180, loss 0.531341.
Test: 2018-08-08T20:08:29.364171: step 3180, loss 0.551732.
Train: 2018-08-08T20:08:32.672968: step 3181, loss 0.531487.
Train: 2018-08-08T20:08:35.952688: step 3182, loss 0.578732.
Train: 2018-08-08T20:08:39.191298: step 3183, loss 0.546323.
Train: 2018-08-08T20:08:42.415872: step 3184, loss 0.515222.
Train: 2018-08-08T20:08:45.645458: step 3185, loss 0.659004.
Train: 2018-08-08T20:08:48.888080: step 3186, loss 0.515039.
Train: 2018-08-08T20:08:52.124685: step 3187, loss 0.64366.
Train: 2018-08-08T20:08:55.494645: step 3188, loss 0.594712.
Train: 2018-08-08T20:08:58.735260: step 3189, loss 0.579015.
Train: 2018-08-08T20:09:01.982895: step 3190, loss 0.515011.
Test: 2018-08-08T20:09:23.556846: step 3190, loss 0.550946.
Train: 2018-08-08T20:09:26.784427: step 3191, loss 0.595315.
Train: 2018-08-08T20:09:29.988947: step 3192, loss 0.675199.
Train: 2018-08-08T20:09:33.196475: step 3193, loss 0.499424.
Train: 2018-08-08T20:09:36.561421: step 3194, loss 0.563191.
Train: 2018-08-08T20:09:39.717813: step 3195, loss 0.626287.
Train: 2018-08-08T20:09:42.922333: step 3196, loss 0.531295.
Train: 2018-08-08T20:09:46.183003: step 3197, loss 0.578994.
Train: 2018-08-08T20:09:49.492802: step 3198, loss 0.452036.
Train: 2018-08-08T20:09:52.833685: step 3199, loss 0.562824.
Train: 2018-08-08T20:09:56.380190: step 3200, loss 0.562858.
Test: 2018-08-08T20:10:17.634701: step 3200, loss 0.549857.
Train: 2018-08-08T20:10:22.585864: step 3201, loss 0.547144.
Train: 2018-08-08T20:10:25.991920: step 3202, loss 0.499173.
Train: 2018-08-08T20:10:29.214488: step 3203, loss 0.610405.
Train: 2018-08-08T20:10:32.588459: step 3204, loss 0.578791.
Train: 2018-08-08T20:10:36.171986: step 3205, loss 0.562776.
Train: 2018-08-08T20:10:39.352442: step 3206, loss 0.675058.
Train: 2018-08-08T20:10:42.539917: step 3207, loss 0.690863.
Train: 2018-08-08T20:10:45.708340: step 3208, loss 0.626256.
Train: 2018-08-08T20:10:48.900828: step 3209, loss 0.530787.
Train: 2018-08-08T20:10:52.058223: step 3210, loss 0.626402.
Test: 2018-08-08T20:11:13.234526: step 3210, loss 0.549419.
Train: 2018-08-08T20:11:16.427013: step 3211, loss 0.515651.
Train: 2018-08-08T20:11:19.609474: step 3212, loss 0.531689.
Train: 2018-08-08T20:11:22.836053: step 3213, loss 0.499762.
Train: 2018-08-08T20:11:26.029543: step 3214, loss 0.484694.
Train: 2018-08-08T20:11:29.188943: step 3215, loss 0.516409.
Train: 2018-08-08T20:11:32.363383: step 3216, loss 0.562773.
Train: 2018-08-08T20:11:35.505738: step 3217, loss 0.546976.
Train: 2018-08-08T20:11:38.793479: step 3218, loss 0.594856.
Train: 2018-08-08T20:11:41.946863: step 3219, loss 0.578988.
Train: 2018-08-08T20:11:45.138348: step 3220, loss 0.546589.
Test: 2018-08-08T20:12:06.240454: step 3220, loss 0.550909.
Train: 2018-08-08T20:12:09.456002: step 3221, loss 0.514584.
Train: 2018-08-08T20:12:12.650495: step 3222, loss 0.674648.
Train: 2018-08-08T20:12:15.836967: step 3223, loss 0.643053.
Train: 2018-08-08T20:12:19.034469: step 3224, loss 0.498423.
Train: 2018-08-08T20:12:22.184845: step 3225, loss 0.562755.
Train: 2018-08-08T20:12:25.432479: step 3226, loss 0.530894.
Train: 2018-08-08T20:12:28.655047: step 3227, loss 0.691478.
Train: 2018-08-08T20:12:31.826479: step 3228, loss 0.466523.
Train: 2018-08-08T20:12:34.993900: step 3229, loss 0.546738.
Train: 2018-08-08T20:12:38.190399: step 3230, loss 0.594694.
Test: 2018-08-08T20:12:59.341636: step 3230, loss 0.550212.
Train: 2018-08-08T20:13:02.519082: step 3231, loss 0.578964.
Train: 2018-08-08T20:13:05.696530: step 3232, loss 0.562721.
Train: 2018-08-08T20:13:09.117626: step 3233, loss 0.53043.
Train: 2018-08-08T20:13:12.368269: step 3234, loss 0.594347.
Train: 2018-08-08T20:13:15.534687: step 3235, loss 0.627285.
Train: 2018-08-08T20:13:18.739207: step 3236, loss 0.562612.
Train: 2018-08-08T20:13:21.935706: step 3237, loss 0.531035.
Train: 2018-08-08T20:13:25.183340: step 3238, loss 0.467438.
Train: 2018-08-08T20:13:28.373823: step 3239, loss 0.59496.
Train: 2018-08-08T20:13:31.572327: step 3240, loss 0.594808.
Test: 2018-08-08T20:13:52.944149: step 3240, loss 0.55075.
Train: 2018-08-08T20:13:56.187773: step 3241, loss 0.611544.
Train: 2018-08-08T20:13:59.458468: step 3242, loss 0.465922.
Train: 2018-08-08T20:14:02.883723: step 3243, loss 0.546008.
Train: 2018-08-08T20:14:06.073204: step 3244, loss 0.578946.
Train: 2018-08-08T20:14:09.344902: step 3245, loss 0.481867.
Train: 2018-08-08T20:14:12.550424: step 3246, loss 0.59487.
Train: 2018-08-08T20:14:15.733889: step 3247, loss 0.563202.
Train: 2018-08-08T20:14:18.936403: step 3248, loss 0.612094.
Train: 2018-08-08T20:14:22.178022: step 3249, loss 0.612174.
Train: 2018-08-08T20:14:25.512888: step 3250, loss 0.691964.
Test: 2018-08-08T20:14:46.867665: step 3250, loss 0.5518.
Train: 2018-08-08T20:14:50.382008: step 3251, loss 0.562817.
Train: 2018-08-08T20:14:53.606582: step 3252, loss 0.546384.
Train: 2018-08-08T20:14:56.818120: step 3253, loss 0.611058.
Train: 2018-08-08T20:14:59.984539: step 3254, loss 0.595139.
Train: 2018-08-08T20:15:03.157976: step 3255, loss 0.579471.
Train: 2018-08-08T20:15:06.406613: step 3256, loss 0.530414.
Train: 2018-08-08T20:15:09.688338: step 3257, loss 0.626927.
Train: 2018-08-08T20:15:12.931962: step 3258, loss 0.611028.
Train: 2018-08-08T20:15:16.129464: step 3259, loss 0.53135.
Train: 2018-08-08T20:15:19.417206: step 3260, loss 0.562729.
Test: 2018-08-08T20:15:40.705806: step 3260, loss 0.549388.
Train: 2018-08-08T20:15:43.892277: step 3261, loss 0.626364.
Train: 2018-08-08T20:15:47.074739: step 3262, loss 0.578565.
Train: 2018-08-08T20:15:50.380528: step 3263, loss 0.547137.
Train: 2018-08-08T20:15:53.710381: step 3264, loss 0.53207.
Train: 2018-08-08T20:15:56.923925: step 3265, loss 0.610342.
Train: 2018-08-08T20:16:00.141479: step 3266, loss 0.562962.
Train: 2018-08-08T20:16:03.340986: step 3267, loss 0.594712.
Train: 2018-08-08T20:16:06.602658: step 3268, loss 0.547313.
Train: 2018-08-08T20:16:10.235782: step 3269, loss 0.609778.
Train: 2018-08-08T20:16:13.652867: step 3270, loss 0.563115.
Test: 2018-08-08T20:16:35.600219: step 3270, loss 0.549901.
Train: 2018-08-08T20:16:38.861891: step 3271, loss 0.453997.
Train: 2018-08-08T20:16:42.114539: step 3272, loss 0.672344.
Train: 2018-08-08T20:16:45.307026: step 3273, loss 0.547639.
Train: 2018-08-08T20:16:48.481467: step 3274, loss 0.516625.
Train: 2018-08-08T20:16:51.708045: step 3275, loss 0.563385.
Train: 2018-08-08T20:16:55.054944: step 3276, loss 0.626147.
Train: 2018-08-08T20:16:58.388807: step 3277, loss 0.484769.
Train: 2018-08-08T20:17:01.637445: step 3278, loss 0.610415.
Train: 2018-08-08T20:17:05.061549: step 3279, loss 0.484942.
Train: 2018-08-08T20:17:08.266068: step 3280, loss 0.484452.
Test: 2018-08-08T20:17:29.822381: step 3280, loss 0.550174.
Train: 2018-08-08T20:17:32.991807: step 3281, loss 0.59394.
Train: 2018-08-08T20:17:36.201340: step 3282, loss 0.609989.
Train: 2018-08-08T20:17:39.393828: step 3283, loss 0.547586.
Train: 2018-08-08T20:17:42.612386: step 3284, loss 0.689719.
Train: 2018-08-08T20:17:46.120713: step 3285, loss 0.531522.
Train: 2018-08-08T20:17:49.489670: step 3286, loss 0.57893.
Train: 2018-08-08T20:17:52.736302: step 3287, loss 0.579034.
Train: 2018-08-08T20:17:55.961878: step 3288, loss 0.499478.
Train: 2018-08-08T20:17:59.243604: step 3289, loss 0.515325.
Train: 2018-08-08T20:18:02.471185: step 3290, loss 0.659269.
Test: 2018-08-08T20:18:23.694613: step 3290, loss 0.55168.
Train: 2018-08-08T20:18:26.875068: step 3291, loss 0.594417.
Train: 2018-08-08T20:18:30.100644: step 3292, loss 0.547094.
Train: 2018-08-08T20:18:33.298145: step 3293, loss 0.451844.
Train: 2018-08-08T20:18:36.496649: step 3294, loss 0.530693.
Train: 2018-08-08T20:18:39.791409: step 3295, loss 0.627421.
Train: 2018-08-08T20:18:43.199470: step 3296, loss 0.499129.
Train: 2018-08-08T20:18:46.553388: step 3297, loss 0.498319.
Train: 2018-08-08T20:18:49.859177: step 3298, loss 0.54658.
Train: 2018-08-08T20:18:53.054672: step 3299, loss 0.579206.
Train: 2018-08-08T20:18:56.336398: step 3300, loss 0.562653.
Test: 2018-08-08T20:19:17.742310: step 3300, loss 0.548129.
Train: 2018-08-08T20:19:22.814796: step 3301, loss 0.52999.
Train: 2018-08-08T20:19:25.987231: step 3302, loss 0.611456.
Train: 2018-08-08T20:19:29.138609: step 3303, loss 0.611324.
Train: 2018-08-08T20:19:32.310041: step 3304, loss 0.497451.
Train: 2018-08-08T20:19:35.473452: step 3305, loss 0.579414.
Train: 2018-08-08T20:19:38.695017: step 3306, loss 0.432043.
Train: 2018-08-08T20:19:41.894524: step 3307, loss 0.497544.
Train: 2018-08-08T20:19:45.065956: step 3308, loss 0.480657.
Train: 2018-08-08T20:19:48.216332: step 3309, loss 0.612141.
Train: 2018-08-08T20:19:51.415838: step 3310, loss 0.645884.
Test: 2018-08-08T20:20:13.263939: step 3310, loss 0.547523.
Train: 2018-08-08T20:20:16.278955: step 3311, loss 0.544436.
Train: 2018-08-08T20:20:19.598781: step 3312, loss 0.562385.
Train: 2018-08-08T20:20:22.893541: step 3313, loss 0.629438.
Train: 2018-08-08T20:20:26.199330: step 3314, loss 0.561824.
Train: 2018-08-08T20:20:29.464010: step 3315, loss 0.579034.
Train: 2018-08-08T20:20:32.774813: step 3316, loss 0.679739.
Train: 2018-08-08T20:20:36.138756: step 3317, loss 0.545962.
Train: 2018-08-08T20:20:39.464599: step 3318, loss 0.662158.
Train: 2018-08-08T20:20:42.717247: step 3319, loss 0.678076.
Train: 2018-08-08T20:20:45.966887: step 3320, loss 0.447092.
Test: 2018-08-08T20:21:07.446997: step 3320, loss 0.549032.
Train: 2018-08-08T20:21:10.783868: step 3321, loss 0.496938.
Train: 2018-08-08T20:21:14.039524: step 3322, loss 0.595258.
Train: 2018-08-08T20:21:17.276129: step 3323, loss 0.529697.
Train: 2018-08-08T20:21:20.646089: step 3324, loss 0.513688.
Train: 2018-08-08T20:21:24.027078: step 3325, loss 0.529629.
Train: 2018-08-08T20:21:27.410073: step 3326, loss 0.644503.
Train: 2018-08-08T20:21:30.765995: step 3327, loss 0.628282.
Train: 2018-08-08T20:21:34.081811: step 3328, loss 0.513822.
Train: 2018-08-08T20:21:37.283323: step 3329, loss 0.529642.
Train: 2018-08-08T20:21:40.925463: step 3330, loss 0.595177.
Test: 2018-08-08T20:22:02.847793: step 3330, loss 0.550463.
Train: 2018-08-08T20:22:06.246835: step 3331, loss 0.643752.
Train: 2018-08-08T20:22:09.735130: step 3332, loss 0.49781.
Train: 2018-08-08T20:22:13.074007: step 3333, loss 0.545975.
Train: 2018-08-08T20:22:16.442965: step 3334, loss 0.497527.
Train: 2018-08-08T20:22:19.750759: step 3335, loss 0.579527.
Train: 2018-08-08T20:22:22.958287: step 3336, loss 0.513862.
Train: 2018-08-08T20:22:26.156791: step 3337, loss 0.595919.
Train: 2018-08-08T20:22:29.389385: step 3338, loss 0.514334.
Train: 2018-08-08T20:22:32.537756: step 3339, loss 0.545782.
Train: 2018-08-08T20:22:35.771353: step 3340, loss 0.643544.
Test: 2018-08-08T20:22:57.289448: step 3340, loss 0.549793.
Train: 2018-08-08T20:23:00.622308: step 3341, loss 0.660044.
Train: 2018-08-08T20:23:03.858914: step 3342, loss 0.530131.
Train: 2018-08-08T20:23:07.061428: step 3343, loss 0.51342.
Train: 2018-08-08T20:23:10.350172: step 3344, loss 0.481025.
Train: 2018-08-08T20:23:13.664979: step 3345, loss 0.497781.
Train: 2018-08-08T20:23:16.975781: step 3346, loss 0.530175.
Train: 2018-08-08T20:23:20.644536: step 3347, loss 0.529383.
Train: 2018-08-08T20:23:24.011487: step 3348, loss 0.49693.
Train: 2018-08-08T20:23:27.414535: step 3349, loss 0.612502.
Train: 2018-08-08T20:23:30.832623: step 3350, loss 0.512726.
Test: 2018-08-08T20:23:52.594482: step 3350, loss 0.551955.
Train: 2018-08-08T20:23:55.887240: step 3351, loss 0.578393.
Train: 2018-08-08T20:23:59.417622: step 3352, loss 0.562586.
Train: 2018-08-08T20:24:02.787582: step 3353, loss 0.562638.
Train: 2018-08-08T20:24:06.184614: step 3354, loss 0.496319.
Train: 2018-08-08T20:24:09.691939: step 3355, loss 0.679944.
Train: 2018-08-08T20:24:13.189431: step 3356, loss 0.646193.
Train: 2018-08-08T20:24:16.644618: step 3357, loss 0.529466.
Train: 2018-08-08T20:24:19.957426: step 3358, loss 0.579116.
Train: 2018-08-08T20:24:23.148911: step 3359, loss 0.495368.
Train: 2018-08-08T20:24:26.351426: step 3360, loss 0.462014.
Test: 2018-08-08T20:24:47.648048: step 3360, loss 0.548671.
Train: 2018-08-08T20:24:50.867607: step 3361, loss 0.563672.
Train: 2018-08-08T20:24:54.044053: step 3362, loss 0.512728.
Train: 2018-08-08T20:24:57.225511: step 3363, loss 0.645212.
Train: 2018-08-08T20:25:00.391930: step 3364, loss 0.461334.
Train: 2018-08-08T20:25:03.559351: step 3365, loss 0.596599.
Train: 2018-08-08T20:25:06.749834: step 3366, loss 0.579189.
Train: 2018-08-08T20:25:10.026546: step 3367, loss 0.528769.
Train: 2018-08-08T20:25:13.204996: step 3368, loss 0.478573.
Train: 2018-08-08T20:25:16.367404: step 3369, loss 0.613275.
Train: 2018-08-08T20:25:19.635093: step 3370, loss 0.630075.
Test: 2018-08-08T20:25:40.916675: step 3370, loss 0.549129.
Train: 2018-08-08T20:25:44.128213: step 3371, loss 0.529036.
Train: 2018-08-08T20:25:47.363815: step 3372, loss 0.646308.
Train: 2018-08-08T20:25:50.630500: step 3373, loss 0.512264.
Train: 2018-08-08T20:25:53.866103: step 3374, loss 0.595605.
Train: 2018-08-08T20:25:57.067615: step 3375, loss 0.528361.
Train: 2018-08-08T20:26:00.268124: step 3376, loss 0.494783.
Train: 2018-08-08T20:26:03.433540: step 3377, loss 0.579192.
Train: 2018-08-08T20:26:06.656108: step 3378, loss 0.579126.
Train: 2018-08-08T20:26:09.906751: step 3379, loss 0.562737.
Train: 2018-08-08T20:26:13.135334: step 3380, loss 0.613144.
Test: 2018-08-08T20:26:34.337706: step 3380, loss 0.54796.
Train: 2018-08-08T20:26:37.493095: step 3381, loss 0.545259.
Train: 2018-08-08T20:26:40.769807: step 3382, loss 0.545931.
Train: 2018-08-08T20:26:43.955276: step 3383, loss 0.562566.
Train: 2018-08-08T20:26:47.125705: step 3384, loss 0.461723.
Train: 2018-08-08T20:26:50.293127: step 3385, loss 0.596122.
Train: 2018-08-08T20:26:53.467567: step 3386, loss 0.495475.
Train: 2018-08-08T20:26:56.672087: step 3387, loss 0.545422.
Train: 2018-08-08T20:26:59.916716: step 3388, loss 0.595557.
Train: 2018-08-08T20:27:03.104188: step 3389, loss 0.73014.
Train: 2018-08-08T20:27:06.301689: step 3390, loss 0.579334.
Test: 2018-08-08T20:27:27.743698: step 3390, loss 0.548665.
Train: 2018-08-08T20:27:30.992335: step 3391, loss 0.596253.
Train: 2018-08-08T20:27:34.212898: step 3392, loss 0.496006.
Train: 2018-08-08T20:27:37.386334: step 3393, loss 0.628679.
Train: 2018-08-08T20:27:40.635975: step 3394, loss 0.612566.
Train: 2018-08-08T20:27:43.853529: step 3395, loss 0.612867.
Train: 2018-08-08T20:27:47.022956: step 3396, loss 0.513461.
Train: 2018-08-08T20:27:50.279614: step 3397, loss 0.529294.
Train: 2018-08-08T20:27:53.466086: step 3398, loss 0.546269.
Train: 2018-08-08T20:27:56.779897: step 3399, loss 0.611788.
Train: 2018-08-08T20:27:59.995446: step 3400, loss 0.562188.
Test: 2018-08-08T20:28:21.249956: step 3400, loss 0.547975.
Train: 2018-08-08T20:28:26.292362: step 3401, loss 0.497454.
Train: 2018-08-08T20:28:29.489864: step 3402, loss 0.562297.
Train: 2018-08-08T20:28:32.672325: step 3403, loss 0.529962.
Train: 2018-08-08T20:28:35.833730: step 3404, loss 0.644191.
Train: 2018-08-08T20:28:39.008170: step 3405, loss 0.579208.
Train: 2018-08-08T20:28:42.218706: step 3406, loss 0.594941.
Train: 2018-08-08T20:28:45.400165: step 3407, loss 0.611284.
Train: 2018-08-08T20:28:48.605687: step 3408, loss 0.643479.
Train: 2018-08-08T20:28:51.811210: step 3409, loss 0.595193.
Train: 2018-08-08T20:28:55.046812: step 3410, loss 0.54655.
Test: 2018-08-08T20:29:16.431669: step 3410, loss 0.548455.
Train: 2018-08-08T20:29:19.622151: step 3411, loss 0.51482.
Train: 2018-08-08T20:29:22.777541: step 3412, loss 0.547073.
Train: 2018-08-08T20:29:25.984066: step 3413, loss 0.578777.
Train: 2018-08-08T20:29:29.431804: step 3414, loss 0.578858.
Train: 2018-08-08T20:29:32.900062: step 3415, loss 0.53086.
Train: 2018-08-08T20:29:36.180131: step 3416, loss 0.563106.
Train: 2018-08-08T20:29:39.377632: step 3417, loss 0.54678.
Train: 2018-08-08T20:29:42.655668: step 3418, loss 0.499227.
Train: 2018-08-08T20:29:45.852347: step 3419, loss 0.515295.
Train: 2018-08-08T20:29:49.034808: step 3420, loss 0.562882.
Test: 2018-08-08T20:30:10.509104: step 3420, loss 0.547287.
Train: 2018-08-08T20:30:13.714626: step 3421, loss 0.498972.
Train: 2018-08-08T20:30:16.932181: step 3422, loss 0.610807.
Train: 2018-08-08T20:30:20.125672: step 3423, loss 0.578911.
Train: 2018-08-08T20:30:23.334202: step 3424, loss 0.546738.
Train: 2018-08-08T20:30:26.586850: step 3425, loss 0.49823.
Train: 2018-08-08T20:30:29.809418: step 3426, loss 0.530027.
Train: 2018-08-08T20:30:32.988871: step 3427, loss 0.514.
Train: 2018-08-08T20:30:36.162308: step 3428, loss 0.530104.
Train: 2018-08-08T20:30:39.355799: step 3429, loss 0.513749.
Train: 2018-08-08T20:30:42.566335: step 3430, loss 0.512992.
Test: 2018-08-08T20:31:03.813827: step 3430, loss 0.547735.
Train: 2018-08-08T20:31:07.016341: step 3431, loss 0.529142.
Train: 2018-08-08T20:31:10.242919: step 3432, loss 0.562349.
Train: 2018-08-08T20:31:13.453455: step 3433, loss 0.396683.
Train: 2018-08-08T20:31:16.664994: step 3434, loss 0.512255.
Train: 2018-08-08T20:31:19.852469: step 3435, loss 0.495402.
Train: 2018-08-08T20:31:23.031922: step 3436, loss 0.478653.
Train: 2018-08-08T20:31:26.229423: step 3437, loss 0.596546.
Train: 2018-08-08T20:31:29.498114: step 3438, loss 0.510763.
Train: 2018-08-08T20:31:32.729706: step 3439, loss 0.476807.
Train: 2018-08-08T20:31:35.873063: step 3440, loss 0.509221.
Test: 2018-08-08T20:31:57.200533: step 3440, loss 0.548668.
Train: 2018-08-08T20:32:00.410469: step 3441, loss 0.562674.
Train: 2018-08-08T20:32:03.633037: step 3442, loss 0.613585.
Train: 2018-08-08T20:32:06.866635: step 3443, loss 0.579664.
Train: 2018-08-08T20:32:10.163873: step 3444, loss 0.579286.
Train: 2018-08-08T20:32:13.411144: step 3445, loss 0.633937.
Train: 2018-08-08T20:32:16.601627: step 3446, loss 0.49163.
Train: 2018-08-08T20:32:19.812163: step 3447, loss 0.508797.
Train: 2018-08-08T20:32:23.026710: step 3448, loss 0.492198.
Train: 2018-08-08T20:32:26.236243: step 3449, loss 0.740291.
Train: 2018-08-08T20:32:29.426726: step 3450, loss 0.492052.
Test: 2018-08-08T20:32:50.705386: step 3450, loss 0.547848.
Train: 2018-08-08T20:32:53.954023: step 3451, loss 0.61648.
Train: 2018-08-08T20:32:57.211685: step 3452, loss 0.616583.
Train: 2018-08-08T20:33:00.493410: step 3453, loss 0.526358.
Train: 2018-08-08T20:33:03.803219: step 3454, loss 0.633118.
Train: 2018-08-08T20:33:06.988878: step 3455, loss 0.561903.
Train: 2018-08-08T20:33:10.278052: step 3456, loss 0.581018.
Train: 2018-08-08T20:33:13.459780: step 3457, loss 0.615021.
Train: 2018-08-08T20:33:16.641239: step 3458, loss 0.493333.
Train: 2018-08-08T20:33:19.845759: step 3459, loss 0.475343.
Train: 2018-08-08T20:33:23.059303: step 3460, loss 0.545211.
Test: 2018-08-08T20:33:44.439183: step 3460, loss 0.551351.
Train: 2018-08-08T20:33:47.726836: step 3461, loss 0.544894.
Train: 2018-08-08T20:33:50.933628: step 3462, loss 0.683069.
Train: 2018-08-08T20:33:54.223886: step 3463, loss 0.613753.
Train: 2018-08-08T20:33:57.438139: step 3464, loss 0.545086.
Train: 2018-08-08T20:34:00.704872: step 3465, loss 0.494054.
Train: 2018-08-08T20:34:03.938918: step 3466, loss 0.596826.
Train: 2018-08-08T20:34:07.178531: step 3467, loss 0.630962.
Train: 2018-08-08T20:34:10.517408: step 3468, loss 0.562793.
Train: 2018-08-08T20:34:13.752008: step 3469, loss 0.596109.
Train: 2018-08-08T20:34:17.034736: step 3470, loss 0.5618.
Test: 2018-08-08T20:34:38.236566: step 3470, loss 0.547248.
Train: 2018-08-08T20:34:41.411003: step 3471, loss 0.613068.
Train: 2018-08-08T20:34:44.577421: step 3472, loss 0.528718.
Train: 2018-08-08T20:34:47.790965: step 3473, loss 0.445299.
Train: 2018-08-08T20:34:50.998855: step 3474, loss 0.495662.
Train: 2018-08-08T20:34:54.281011: step 3475, loss 0.562315.
Train: 2018-08-08T20:34:57.541155: step 3476, loss 0.528574.
Train: 2018-08-08T20:35:00.771392: step 3477, loss 0.562639.
Train: 2018-08-08T20:35:03.967329: step 3478, loss 0.478835.
Train: 2018-08-08T20:35:07.176862: step 3479, loss 0.479196.
Train: 2018-08-08T20:35:10.435526: step 3480, loss 0.512054.
Test: 2018-08-08T20:35:31.863206: step 3480, loss 0.548533.
Train: 2018-08-08T20:35:35.141589: step 3481, loss 0.545486.
Train: 2018-08-08T20:35:38.346109: step 3482, loss 0.612758.
Train: 2018-08-08T20:35:41.572688: step 3483, loss 0.528423.
Train: 2018-08-08T20:35:44.740109: step 3484, loss 0.52869.
Train: 2018-08-08T20:35:47.903520: step 3485, loss 0.579442.
Train: 2018-08-08T20:35:51.093000: step 3486, loss 0.528389.
Train: 2018-08-08T20:35:54.306544: step 3487, loss 0.546099.
Train: 2018-08-08T20:35:57.514072: step 3488, loss 0.545489.
Train: 2018-08-08T20:36:00.725610: step 3489, loss 0.664381.
Train: 2018-08-08T20:36:03.942162: step 3490, loss 0.545333.
Test: 2018-08-08T20:36:25.247809: step 3490, loss 0.546368.
Train: 2018-08-08T20:36:28.385149: step 3491, loss 0.68125.
Train: 2018-08-08T20:36:31.610725: step 3492, loss 0.579855.
Train: 2018-08-08T20:36:34.809229: step 3493, loss 0.426578.
Train: 2018-08-08T20:36:38.050848: step 3494, loss 0.630497.
Train: 2018-08-08T20:36:41.264393: step 3495, loss 0.494582.
Train: 2018-08-08T20:36:44.492976: step 3496, loss 0.477522.
Train: 2018-08-08T20:36:47.733591: step 3497, loss 0.664284.
Train: 2018-08-08T20:36:50.972202: step 3498, loss 0.443628.
Train: 2018-08-08T20:36:54.200786: step 3499, loss 0.54562.
Train: 2018-08-08T20:36:57.572751: step 3500, loss 0.579564.
Test: 2018-08-08T20:37:20.232999: step 3500, loss 0.54645.
Train: 2018-08-08T20:37:25.413773: step 3501, loss 0.544904.
Train: 2018-08-08T20:37:28.671434: step 3502, loss 0.460314.
Train: 2018-08-08T20:37:31.934627: step 3503, loss 0.57876.
Train: 2018-08-08T20:37:35.185278: step 3504, loss 0.578758.
Train: 2018-08-08T20:37:38.458185: step 3505, loss 0.612721.
Train: 2018-08-08T20:37:41.621032: step 3506, loss 0.528452.
Train: 2018-08-08T20:37:44.901064: step 3507, loss 0.545331.
Train: 2018-08-08T20:37:48.066480: step 3508, loss 0.596242.
Train: 2018-08-08T20:37:51.271771: step 3509, loss 0.562434.
Train: 2018-08-08T20:37:54.609652: step 3510, loss 0.493865.
Test: 2018-08-08T20:38:16.081911: step 3510, loss 0.547646.
Train: 2018-08-08T20:38:19.408755: step 3511, loss 0.56205.
Train: 2018-08-08T20:38:22.642352: step 3512, loss 0.528775.
Train: 2018-08-08T20:38:25.898008: step 3513, loss 0.476703.
Train: 2018-08-08T20:38:29.084480: step 3514, loss 0.562341.
Train: 2018-08-08T20:38:32.210792: step 3515, loss 0.544652.
Train: 2018-08-08T20:38:35.343120: step 3516, loss 0.477279.
Train: 2018-08-08T20:38:38.497507: step 3517, loss 0.511309.
Train: 2018-08-08T20:38:41.657909: step 3518, loss 0.544557.
Train: 2018-08-08T20:38:44.888499: step 3519, loss 0.424573.
Train: 2018-08-08T20:38:48.070960: step 3520, loss 0.563008.
Test: 2018-08-08T20:39:09.441595: step 3520, loss 0.546051.
Train: 2018-08-08T20:39:12.664162: step 3521, loss 0.648778.
Train: 2018-08-08T20:39:15.889738: step 3522, loss 0.580467.
Train: 2018-08-08T20:39:19.079218: step 3523, loss 0.580752.
Train: 2018-08-08T20:39:22.268698: step 3524, loss 0.562694.
Train: 2018-08-08T20:39:25.512322: step 3525, loss 0.597369.
Train: 2018-08-08T20:39:28.726868: step 3526, loss 0.528006.
Train: 2018-08-08T20:39:31.878247: step 3527, loss 0.684201.
Train: 2018-08-08T20:39:35.112847: step 3528, loss 0.528401.
Train: 2018-08-08T20:39:38.259212: step 3529, loss 0.493282.
Train: 2018-08-08T20:39:41.399561: step 3530, loss 0.666854.
Test: 2018-08-08T20:40:02.465570: step 3530, loss 0.548121.
Train: 2018-08-08T20:40:05.626975: step 3531, loss 0.510013.
Train: 2018-08-08T20:40:08.871602: step 3532, loss 0.5794.
Train: 2018-08-08T20:40:12.005935: step 3533, loss 0.544786.
Train: 2018-08-08T20:40:15.158316: step 3534, loss 0.493529.
Train: 2018-08-08T20:40:18.293653: step 3535, loss 0.511143.
Train: 2018-08-08T20:40:21.441020: step 3536, loss 0.596491.
Train: 2018-08-08T20:40:24.560314: step 3537, loss 0.562935.
Train: 2018-08-08T20:40:27.770850: step 3538, loss 0.494294.
Train: 2018-08-08T20:40:30.948298: step 3539, loss 0.63081.
Train: 2018-08-08T20:40:34.092658: step 3540, loss 0.613689.
Test: 2018-08-08T20:40:55.067424: step 3540, loss 0.548288.
Train: 2018-08-08T20:40:58.216797: step 3541, loss 0.630754.
Train: 2018-08-08T20:41:01.374192: step 3542, loss 0.545384.
Train: 2018-08-08T20:41:04.541613: step 3543, loss 0.630075.
Train: 2018-08-08T20:41:07.677952: step 3544, loss 0.579309.
Train: 2018-08-08T20:41:10.913554: step 3545, loss 0.612803.
Train: 2018-08-08T20:41:14.051898: step 3546, loss 0.562411.
Train: 2018-08-08T20:41:17.271458: step 3547, loss 0.712765.
Train: 2018-08-08T20:41:20.453919: step 3548, loss 0.545842.
Train: 2018-08-08T20:41:23.756701: step 3549, loss 0.562311.
Train: 2018-08-08T20:41:26.934149: step 3550, loss 0.611883.
Test: 2018-08-08T20:41:48.131507: step 3550, loss 0.550939.
Train: 2018-08-08T20:41:51.285893: step 3551, loss 0.611618.
Train: 2018-08-08T20:41:54.462338: step 3552, loss 0.529979.
Train: 2018-08-08T20:41:57.660843: step 3553, loss 0.627645.
Train: 2018-08-08T20:42:00.857341: step 3554, loss 0.546596.
Train: 2018-08-08T20:42:04.048826: step 3555, loss 0.578911.
Train: 2018-08-08T20:42:07.242317: step 3556, loss 0.54688.
Train: 2018-08-08T20:42:10.519029: step 3557, loss 0.594812.
Train: 2018-08-08T20:42:13.824818: step 3558, loss 0.610734.
Train: 2018-08-08T20:42:17.061423: step 3559, loss 0.499497.
Train: 2018-08-08T20:42:20.267948: step 3560, loss 0.673876.
Test: 2018-08-08T20:42:41.479344: step 3560, loss 0.550704.
Train: 2018-08-08T20:42:44.763074: step 3561, loss 0.578843.
Train: 2018-08-08T20:42:48.021738: step 3562, loss 0.610367.
Train: 2018-08-08T20:42:51.299457: step 3563, loss 0.500638.
Train: 2018-08-08T20:42:54.481914: step 3564, loss 0.594506.
Train: 2018-08-08T20:42:57.664375: step 3565, loss 0.61005.
Train: 2018-08-08T20:43:00.947103: step 3566, loss 0.516776.
Train: 2018-08-08T20:43:04.119538: step 3567, loss 0.532389.
Train: 2018-08-08T20:43:07.281946: step 3568, loss 0.625416.
Train: 2018-08-08T20:43:10.492481: step 3569, loss 0.625354.
Train: 2018-08-08T20:43:13.670932: step 3570, loss 0.563498.
Test: 2018-08-08T20:43:35.524476: step 3570, loss 0.549677.
Train: 2018-08-08T20:43:38.838886: step 3571, loss 0.455642.
Train: 2018-08-08T20:43:42.084515: step 3572, loss 0.486424.
Train: 2018-08-08T20:43:45.329142: step 3573, loss 0.578936.
Train: 2018-08-08T20:43:48.560734: step 3574, loss 0.532498.
Train: 2018-08-08T20:43:51.756230: step 3575, loss 0.594427.
Train: 2018-08-08T20:43:54.935683: step 3576, loss 0.625518.
Train: 2018-08-08T20:43:58.098091: step 3577, loss 0.56336.
Train: 2018-08-08T20:44:01.301608: step 3578, loss 0.54779.
Train: 2018-08-08T20:44:04.566289: step 3579, loss 0.578899.
Train: 2018-08-08T20:44:07.985379: step 3580, loss 0.594481.
Test: 2018-08-08T20:44:29.318097: step 3580, loss 0.548692.
Train: 2018-08-08T20:44:32.483512: step 3581, loss 0.625671.
Train: 2018-08-08T20:44:35.704075: step 3582, loss 0.578896.
Train: 2018-08-08T20:44:38.877512: step 3583, loss 0.5166.
Train: 2018-08-08T20:44:42.059974: step 3584, loss 0.610059.
Train: 2018-08-08T20:44:45.336685: step 3585, loss 0.547743.
Train: 2018-08-08T20:44:48.526165: step 3586, loss 0.703539.
Train: 2018-08-08T20:44:51.714642: step 3587, loss 0.641098.
Train: 2018-08-08T20:44:54.850982: step 3588, loss 0.516913.
Train: 2018-08-08T20:44:58.040461: step 3589, loss 0.594402.
Train: 2018-08-08T20:45:01.328202: step 3590, loss 0.609827.
Test: 2018-08-08T20:45:22.777230: step 3590, loss 0.550271.
Train: 2018-08-08T20:45:26.029877: step 3591, loss 0.563543.
Train: 2018-08-08T20:45:29.212338: step 3592, loss 0.6251.
Train: 2018-08-08T20:45:32.421872: step 3593, loss 0.532962.
Train: 2018-08-08T20:45:35.599320: step 3594, loss 0.487099.
Train: 2018-08-08T20:45:38.787797: step 3595, loss 0.624944.
Train: 2018-08-08T20:45:42.147730: step 3596, loss 0.640224.
Train: 2018-08-08T20:45:45.450511: step 3597, loss 0.548447.
Train: 2018-08-08T20:45:48.667064: step 3598, loss 0.609542.
Train: 2018-08-08T20:45:52.026997: step 3599, loss 0.563787.
Train: 2018-08-08T20:45:55.429064: step 3600, loss 0.655147.
Test: 2018-08-08T20:46:17.263182: step 3600, loss 0.550219.
Train: 2018-08-08T20:46:22.322634: step 3601, loss 0.639796.
Train: 2018-08-08T20:46:25.530162: step 3602, loss 0.56395.
Train: 2018-08-08T20:46:28.678533: step 3603, loss 0.518739.
Train: 2018-08-08T20:46:31.880045: step 3604, loss 0.548978.
Train: 2018-08-08T20:46:35.053482: step 3605, loss 0.503812.
Train: 2018-08-08T20:46:38.280060: step 3606, loss 0.564049.
Train: 2018-08-08T20:46:41.422415: step 3607, loss 0.533838.
Train: 2018-08-08T20:46:44.643980: step 3608, loss 0.458154.
Train: 2018-08-08T20:46:47.946762: step 3609, loss 0.579057.
Train: 2018-08-08T20:46:51.229489: step 3610, loss 0.609491.
Test: 2018-08-08T20:47:12.835935: step 3610, loss 0.552335.
Train: 2018-08-08T20:47:16.024412: step 3611, loss 0.563749.
Train: 2018-08-08T20:47:19.044442: step 3612, loss 0.661627.
Train: 2018-08-08T20:47:22.362263: step 3613, loss 0.640245.
Train: 2018-08-08T20:47:25.542719: step 3614, loss 0.609612.
Train: 2018-08-08T20:47:28.723175: step 3615, loss 0.609581.
Train: 2018-08-08T20:47:31.928697: step 3616, loss 0.579011.
Train: 2018-08-08T20:47:35.092108: step 3617, loss 0.563784.
Train: 2018-08-08T20:47:38.275572: step 3618, loss 0.563808.
Train: 2018-08-08T20:47:41.443996: step 3619, loss 0.533388.
Train: 2018-08-08T20:47:44.571311: step 3620, loss 0.548604.
Test: 2018-08-08T20:48:06.183773: step 3620, loss 0.550684.
Train: 2018-08-08T20:48:09.555737: step 3621, loss 0.502881.
Train: 2018-08-08T20:48:12.777302: step 3622, loss 0.579015.
Train: 2018-08-08T20:48:15.905620: step 3623, loss 0.609579.
Train: 2018-08-08T20:48:19.202385: step 3624, loss 0.50249.
Train: 2018-08-08T20:48:22.509177: step 3625, loss 0.502305.
Train: 2018-08-08T20:48:25.790902: step 3626, loss 0.56357.
Train: 2018-08-08T20:48:29.019486: step 3627, loss 0.578944.
Train: 2018-08-08T20:48:32.201947: step 3628, loss 0.578933.
Train: 2018-08-08T20:48:35.422510: step 3629, loss 0.547884.
Train: 2018-08-08T20:48:38.649088: step 3630, loss 0.64112.
Test: 2018-08-08T20:49:00.068036: step 3630, loss 0.549913.
Train: 2018-08-08T20:49:03.314667: step 3631, loss 0.578894.
Train: 2018-08-08T20:49:06.479081: step 3632, loss 0.610076.
Train: 2018-08-08T20:49:09.700646: step 3633, loss 0.532116.
Train: 2018-08-08T20:49:12.850019: step 3634, loss 0.547682.
Train: 2018-08-08T20:49:16.005409: step 3635, loss 0.563265.
Train: 2018-08-08T20:49:19.146761: step 3636, loss 0.531954.
Train: 2018-08-08T20:49:22.290118: step 3637, loss 0.688575.
Train: 2018-08-08T20:49:25.461550: step 3638, loss 0.500553.
Train: 2018-08-08T20:49:28.651030: step 3639, loss 0.610237.
Train: 2018-08-08T20:49:31.836499: step 3640, loss 0.50047.
Test: 2018-08-08T20:49:53.038870: step 3640, loss 0.55024.
Train: 2018-08-08T20:49:56.276478: step 3641, loss 0.50036.
Train: 2018-08-08T20:49:59.569233: step 3642, loss 0.531652.
Train: 2018-08-08T20:50:02.922147: step 3643, loss 0.563084.
Train: 2018-08-08T20:50:06.150733: step 3644, loss 0.578863.
Train: 2018-08-08T20:50:09.547763: step 3645, loss 0.499568.
Train: 2018-08-08T20:50:13.035035: step 3646, loss 0.562947.
Train: 2018-08-08T20:50:16.468163: step 3647, loss 0.5629.
Train: 2018-08-08T20:50:19.815061: step 3648, loss 0.514869.
Train: 2018-08-08T20:50:23.171986: step 3649, loss 0.466512.
Train: 2018-08-08T20:50:26.361466: step 3650, loss 0.530508.
Test: 2018-08-08T20:50:47.583891: step 3650, loss 0.549358.
Train: 2018-08-08T20:50:50.765349: step 3651, loss 0.643693.
Train: 2018-08-08T20:50:54.076151: step 3652, loss 0.513918.
Train: 2018-08-08T20:50:57.229536: step 3653, loss 0.448534.
Train: 2018-08-08T20:51:00.389938: step 3654, loss 0.611703.
Train: 2018-08-08T20:51:03.550341: step 3655, loss 0.644714.
Train: 2018-08-08T20:51:06.710743: step 3656, loss 0.562528.
Train: 2018-08-08T20:51:09.948351: step 3657, loss 0.579004.
Train: 2018-08-08T20:51:13.150866: step 3658, loss 0.496447.
Train: 2018-08-08T20:51:16.345359: step 3659, loss 0.479742.
Train: 2018-08-08T20:51:19.645132: step 3660, loss 0.512662.
Test: 2018-08-08T20:51:40.701119: step 3660, loss 0.549349.
Train: 2018-08-08T20:51:43.833442: step 3661, loss 0.545782.
Train: 2018-08-08T20:51:47.039968: step 3662, loss 0.562424.
Train: 2018-08-08T20:51:50.471091: step 3663, loss 0.512128.
Train: 2018-08-08T20:51:53.629487: step 3664, loss 0.495142.
Train: 2018-08-08T20:51:56.902189: step 3665, loss 0.579212.
Train: 2018-08-08T20:52:00.169877: step 3666, loss 0.579257.
Train: 2018-08-08T20:52:03.349330: step 3667, loss 0.443623.
Train: 2018-08-08T20:52:06.530789: step 3668, loss 0.715592.
Train: 2018-08-08T20:52:09.744332: step 3669, loss 0.545307.
Train: 2018-08-08T20:52:12.870644: step 3670, loss 0.64764.
Test: 2018-08-08T20:52:33.937656: step 3670, loss 0.549587.
Train: 2018-08-08T20:52:37.070986: step 3671, loss 0.630515.
Train: 2018-08-08T20:52:40.178248: step 3672, loss 0.59641.
Train: 2018-08-08T20:52:43.348677: step 3673, loss 0.562329.
Train: 2018-08-08T20:52:46.486018: step 3674, loss 0.664068.
Train: 2018-08-08T20:52:49.658453: step 3675, loss 0.511684.
Train: 2018-08-08T20:52:52.794791: step 3676, loss 0.51182.
Train: 2018-08-08T20:52:55.945167: step 3677, loss 0.562369.
Train: 2018-08-08T20:52:59.086520: step 3678, loss 0.579206.
Train: 2018-08-08T20:53:02.247925: step 3679, loss 0.562403.
Train: 2018-08-08T20:53:05.385266: step 3680, loss 0.562391.
Test: 2018-08-08T20:53:27.244992: step 3680, loss 0.548647.
Train: 2018-08-08T20:53:30.492627: step 3681, loss 0.645996.
Train: 2018-08-08T20:53:33.689125: step 3682, loss 0.645768.
Train: 2018-08-08T20:53:36.861560: step 3683, loss 0.628874.
Train: 2018-08-08T20:53:40.030986: step 3684, loss 0.661672.
Train: 2018-08-08T20:53:43.192392: step 3685, loss 0.513231.
Train: 2018-08-08T20:53:46.333744: step 3686, loss 0.595311.
Train: 2018-08-08T20:53:49.491138: step 3687, loss 0.513761.
Train: 2018-08-08T20:53:52.660565: step 3688, loss 0.595143.
Train: 2018-08-08T20:53:55.881127: step 3689, loss 0.611254.
Train: 2018-08-08T20:53:59.119738: step 3690, loss 0.482171.
Test: 2018-08-08T20:54:20.338152: step 3690, loss 0.548937.
Train: 2018-08-08T20:54:23.502565: step 3691, loss 0.578874.
Train: 2018-08-08T20:54:26.695053: step 3692, loss 0.578869.
Train: 2018-08-08T20:54:29.917621: step 3693, loss 0.578865.
Train: 2018-08-08T20:54:33.134173: step 3694, loss 0.514943.
Train: 2018-08-08T20:54:36.364762: step 3695, loss 0.435197.
Train: 2018-08-08T20:54:39.636461: step 3696, loss 0.642789.
Train: 2018-08-08T20:54:42.777813: step 3697, loss 0.498965.
Train: 2018-08-08T20:54:45.881063: step 3698, loss 0.498889.
Train: 2018-08-08T20:54:48.995343: step 3699, loss 0.610919.
Train: 2018-08-08T20:54:52.152738: step 3700, loss 0.578868.
Test: 2018-08-08T20:55:13.268881: step 3700, loss 0.548984.
Train: 2018-08-08T20:55:18.174924: step 3701, loss 0.610981.
Train: 2018-08-08T20:55:21.351369: step 3702, loss 0.530704.
Train: 2018-08-08T20:55:24.477681: step 3703, loss 0.594934.
Train: 2018-08-08T20:55:27.673177: step 3704, loss 0.530679.
Train: 2018-08-08T20:55:30.842604: step 3705, loss 0.611019.
Train: 2018-08-08T20:55:33.989972: step 3706, loss 0.530661.
Train: 2018-08-08T20:55:37.122300: step 3707, loss 0.562796.
Train: 2018-08-08T20:55:40.308772: step 3708, loss 0.627121.
Train: 2018-08-08T20:55:43.456140: step 3709, loss 0.514581.
Train: 2018-08-08T20:55:46.660660: step 3710, loss 0.611026.
Test: 2018-08-08T20:56:08.480965: step 3710, loss 0.549569.
Train: 2018-08-08T20:56:11.952194: step 3711, loss 0.434252.
Train: 2018-08-08T20:56:15.364266: step 3712, loss 0.707643.
Train: 2018-08-08T20:56:18.629948: step 3713, loss 0.56279.
Train: 2018-08-08T20:56:21.891620: step 3714, loss 0.562799.
Train: 2018-08-08T20:56:25.174348: step 3715, loss 0.578871.
Train: 2018-08-08T20:56:28.595680: step 3716, loss 0.578869.
Train: 2018-08-08T20:56:32.076935: step 3717, loss 0.578867.
Train: 2018-08-08T20:56:35.350639: step 3718, loss 0.562841.
Train: 2018-08-08T20:56:38.561174: step 3719, loss 0.578865.
Train: 2018-08-08T20:56:41.770708: step 3720, loss 0.594861.
Test: 2018-08-08T20:57:02.957036: step 3720, loss 0.549122.
Train: 2018-08-08T20:57:06.253801: step 3721, loss 0.498974.
Train: 2018-08-08T20:57:09.454311: step 3722, loss 0.482999.
Train: 2018-08-08T20:57:12.643791: step 3723, loss 0.482871.
Train: 2018-08-08T20:57:15.934540: step 3724, loss 0.562827.
Train: 2018-08-08T20:57:19.160116: step 3725, loss 0.611029.
Train: 2018-08-08T20:57:22.301468: step 3726, loss 0.611078.
Train: 2018-08-08T20:57:25.485934: step 3727, loss 0.594987.
Train: 2018-08-08T20:57:28.733334: step 3728, loss 0.562768.
Train: 2018-08-08T20:57:31.970787: step 3729, loss 0.578878.
Train: 2018-08-08T20:57:35.124171: step 3730, loss 0.659415.
Test: 2018-08-08T20:57:56.250492: step 3730, loss 0.550158.
Train: 2018-08-08T20:57:59.387833: step 3731, loss 0.659276.
Train: 2018-08-08T20:58:02.567286: step 3732, loss 0.610931.
Train: 2018-08-08T20:58:05.731699: step 3733, loss 0.562884.
Train: 2018-08-08T20:58:08.891099: step 3734, loss 0.56293.
Train: 2018-08-08T20:58:12.021422: step 3735, loss 0.578858.
Train: 2018-08-08T20:58:15.161771: step 3736, loss 0.515476.
Train: 2018-08-08T20:58:18.308137: step 3737, loss 0.515571.
Train: 2018-08-08T20:58:21.456507: step 3738, loss 0.436508.
Train: 2018-08-08T20:58:24.581817: step 3739, loss 0.6264.
Train: 2018-08-08T20:58:27.738209: step 3740, loss 0.499569.
Test: 2018-08-08T20:58:48.720996: step 3740, loss 0.549286.
Train: 2018-08-08T20:58:51.868364: step 3741, loss 0.642415.
Train: 2018-08-08T20:58:55.026761: step 3742, loss 0.531178.
Train: 2018-08-08T20:58:58.186161: step 3743, loss 0.515224.
Train: 2018-08-08T20:59:01.372633: step 3744, loss 0.610731.
Train: 2018-08-08T20:59:04.530028: step 3745, loss 0.578854.
Train: 2018-08-08T20:59:07.658345: step 3746, loss 0.578857.
Train: 2018-08-08T20:59:10.802705: step 3747, loss 0.642732.
Train: 2018-08-08T20:59:13.943055: step 3748, loss 0.562913.
Train: 2018-08-08T20:59:17.106465: step 3749, loss 0.594805.
Train: 2018-08-08T20:59:20.277897: step 3750, loss 0.515158.
Test: 2018-08-08T20:59:41.332877: step 3750, loss 0.548624.
Train: 2018-08-08T20:59:44.493279: step 3751, loss 0.531102.
Train: 2018-08-08T20:59:47.639645: step 3752, loss 0.610716.
Train: 2018-08-08T20:59:50.787012: step 3753, loss 0.547011.
Train: 2018-08-08T20:59:53.947415: step 3754, loss 0.57887.
Train: 2018-08-08T20:59:57.072724: step 3755, loss 0.562936.
Train: 2018-08-08T21:00:00.216082: step 3756, loss 0.499229.
Train: 2018-08-08T21:00:03.481764: step 3757, loss 0.483205.
Train: 2018-08-08T21:00:06.634146: step 3758, loss 0.610831.
Train: 2018-08-08T21:00:09.777503: step 3759, loss 0.530854.
Train: 2018-08-08T21:00:12.915847: step 3760, loss 0.643007.
Test: 2018-08-08T21:00:34.011936: step 3760, loss 0.549007.
Train: 2018-08-08T21:00:37.164317: step 3761, loss 0.610944.
Train: 2018-08-08T21:00:40.293637: step 3762, loss 0.562831.
Train: 2018-08-08T21:00:43.449026: step 3763, loss 0.54681.
Train: 2018-08-08T21:00:46.595391: step 3764, loss 0.562823.
Train: 2018-08-08T21:00:49.714685: step 3765, loss 0.562828.
Train: 2018-08-08T21:00:52.872079: step 3766, loss 0.482618.
Train: 2018-08-08T21:00:55.991373: step 3767, loss 0.514603.
Train: 2018-08-08T21:00:59.138741: step 3768, loss 0.643283.
Train: 2018-08-08T21:01:02.286109: step 3769, loss 0.546645.
Train: 2018-08-08T21:01:05.447514: step 3770, loss 0.465971.
Test: 2018-08-08T21:01:26.463389: step 3770, loss 0.54879.
Train: 2018-08-08T21:01:29.601734: step 3771, loss 0.562714.
Train: 2018-08-08T21:01:32.736067: step 3772, loss 0.595107.
Train: 2018-08-08T21:01:35.870400: step 3773, loss 0.497742.
Train: 2018-08-08T21:01:39.015763: step 3774, loss 0.546379.
Train: 2018-08-08T21:01:42.165136: step 3775, loss 0.546315.
Train: 2018-08-08T21:01:45.326541: step 3776, loss 0.562594.
Train: 2018-08-08T21:01:48.463883: step 3777, loss 0.546183.
Train: 2018-08-08T21:01:51.593203: step 3778, loss 0.578973.
Train: 2018-08-08T21:01:54.718512: step 3779, loss 0.480277.
Train: 2018-08-08T21:01:57.911000: step 3780, loss 0.463519.
Test: 2018-08-08T21:02:19.671123: step 3780, loss 0.55134.
Train: 2018-08-08T21:02:22.985935: step 3781, loss 0.562475.
Train: 2018-08-08T21:02:26.321805: step 3782, loss 0.56245.
Train: 2018-08-08T21:02:29.551391: step 3783, loss 0.545764.
Train: 2018-08-08T21:02:32.873267: step 3784, loss 0.528954.
Train: 2018-08-08T21:02:36.190066: step 3785, loss 0.545625.
Train: 2018-08-08T21:02:39.454764: step 3786, loss 0.629681.
Train: 2018-08-08T21:02:42.773588: step 3787, loss 0.545516.
Train: 2018-08-08T21:02:46.108119: step 3788, loss 0.444253.
Train: 2018-08-08T21:02:49.464558: step 3789, loss 0.49467.
Train: 2018-08-08T21:02:52.948828: step 3790, loss 0.528389.
Test: 2018-08-08T21:03:14.817856: step 3790, loss 0.547016.
Train: 2018-08-08T21:03:18.079527: step 3791, loss 0.613481.
Train: 2018-08-08T21:03:21.268004: step 3792, loss 0.528184.
Train: 2018-08-08T21:03:24.496588: step 3793, loss 0.510987.
Train: 2018-08-08T21:03:27.666015: step 3794, loss 0.51082.
Train: 2018-08-08T21:03:30.877553: step 3795, loss 0.59678.
Train: 2018-08-08T21:03:34.097113: step 3796, loss 0.631346.
Train: 2018-08-08T21:03:37.221420: step 3797, loss 0.510541.
Train: 2018-08-08T21:03:40.477076: step 3798, loss 0.579626.
Train: 2018-08-08T21:03:43.639484: step 3799, loss 0.545033.
Train: 2018-08-08T21:03:46.826958: step 3800, loss 0.579649.
Test: 2018-08-08T21:04:08.032338: step 3800, loss 0.551363.
Train: 2018-08-08T21:04:13.001549: step 3801, loss 0.562331.
Train: 2018-08-08T21:04:16.166965: step 3802, loss 0.579644.
Train: 2018-08-08T21:04:19.376498: step 3803, loss 0.475871.
Train: 2018-08-08T21:04:22.520858: step 3804, loss 0.614235.
Train: 2018-08-08T21:04:25.674243: step 3805, loss 0.458587.
Train: 2018-08-08T21:04:28.866730: step 3806, loss 0.596966.
Train: 2018-08-08T21:04:32.049192: step 3807, loss 0.545029.
Train: 2018-08-08T21:04:35.241680: step 3808, loss 0.475779.
Train: 2018-08-08T21:04:38.386040: step 3809, loss 0.596998.
Train: 2018-08-08T21:04:41.578528: step 3810, loss 0.597012.
Test: 2018-08-08T21:05:02.863118: step 3810, loss 0.546763.
Train: 2018-08-08T21:05:06.096715: step 3811, loss 0.648977.
Train: 2018-08-08T21:05:09.292211: step 3812, loss 0.527727.
Train: 2018-08-08T21:05:12.515781: step 3813, loss 0.510542.
Train: 2018-08-08T21:05:15.723309: step 3814, loss 0.614132.
Train: 2018-08-08T21:05:18.898752: step 3815, loss 0.424431.
Train: 2018-08-08T21:05:22.071186: step 3816, loss 0.545062.
Train: 2018-08-08T21:05:25.203514: step 3817, loss 0.493309.
Train: 2018-08-08T21:05:28.416056: step 3818, loss 0.562347.
Train: 2018-08-08T21:05:31.626591: step 3819, loss 0.562341.
Train: 2018-08-08T21:05:34.945416: step 3820, loss 0.527707.
Test: 2018-08-08T21:05:56.474656: step 3820, loss 0.549394.
Train: 2018-08-08T21:05:59.702237: step 3821, loss 0.683563.
Train: 2018-08-08T21:06:03.035699: step 3822, loss 0.59693.
Train: 2018-08-08T21:06:06.380832: step 3823, loss 0.562361.
Train: 2018-08-08T21:06:09.697652: step 3824, loss 0.562315.
Train: 2018-08-08T21:06:13.069616: step 3825, loss 0.665545.
Train: 2018-08-08T21:06:16.657482: step 3826, loss 0.408013.
Train: 2018-08-08T21:06:20.012402: step 3827, loss 0.579493.
Train: 2018-08-08T21:06:23.406149: step 3828, loss 0.562328.
Train: 2018-08-08T21:06:26.656791: step 3829, loss 0.493985.
Train: 2018-08-08T21:06:30.011711: step 3830, loss 0.579417.
Test: 2018-08-08T21:06:51.842437: step 3830, loss 0.547635.
Train: 2018-08-08T21:06:55.027906: step 3831, loss 0.596455.
Train: 2018-08-08T21:06:58.206357: step 3832, loss 0.596443.
Train: 2018-08-08T21:07:01.449981: step 3833, loss 0.545297.
Train: 2018-08-08T21:07:04.689594: step 3834, loss 0.647332.
Train: 2018-08-08T21:07:07.895117: step 3835, loss 0.528453.
Train: 2018-08-08T21:07:11.111669: step 3836, loss 0.545448.
Train: 2018-08-08T21:07:14.331229: step 3837, loss 0.444204.
Train: 2018-08-08T21:07:17.556805: step 3838, loss 0.511717.
Train: 2018-08-08T21:07:20.777367: step 3839, loss 0.511691.
Train: 2018-08-08T21:07:23.942783: step 3840, loss 0.42704.
Test: 2018-08-08T21:07:45.191278: step 3840, loss 0.550946.
Train: 2018-08-08T21:07:48.523135: step 3841, loss 0.579268.
Train: 2018-08-08T21:07:51.738685: step 3842, loss 0.579349.
Train: 2018-08-08T21:07:54.888058: step 3843, loss 0.579364.
Train: 2018-08-08T21:07:58.114637: step 3844, loss 0.647598.
Train: 2018-08-08T21:08:01.331188: step 3845, loss 0.579373.
Train: 2018-08-08T21:08:04.519666: step 3846, loss 0.562384.
Train: 2018-08-08T21:08:07.713156: step 3847, loss 0.579358.
Train: 2018-08-08T21:08:10.876567: step 3848, loss 0.63037.
Train: 2018-08-08T21:08:14.097130: step 3849, loss 0.562359.
Train: 2018-08-08T21:08:17.275580: step 3850, loss 0.562373.
Test: 2018-08-08T21:08:38.632362: step 3850, loss 0.545898.
Train: 2018-08-08T21:08:42.001319: step 3851, loss 0.545486.
Train: 2018-08-08T21:08:45.196815: step 3852, loss 0.562389.
Train: 2018-08-08T21:08:48.425399: step 3853, loss 0.629731.
Train: 2018-08-08T21:08:51.684063: step 3854, loss 0.696718.
Train: 2018-08-08T21:08:54.883569: step 3855, loss 0.579125.
Train: 2018-08-08T21:08:58.020911: step 3856, loss 0.545801.
Train: 2018-08-08T21:09:01.176300: step 3857, loss 0.545895.
Train: 2018-08-08T21:09:04.315647: step 3858, loss 0.612062.
Train: 2018-08-08T21:09:07.482065: step 3859, loss 0.578988.
Train: 2018-08-08T21:09:10.656505: step 3860, loss 0.693771.
Test: 2018-08-08T21:09:31.655335: step 3860, loss 0.550416.
Train: 2018-08-08T21:09:34.811728: step 3861, loss 0.644183.
Train: 2018-08-08T21:09:37.952077: step 3862, loss 0.514047.
Train: 2018-08-08T21:09:41.099445: step 3863, loss 0.465931.
Train: 2018-08-08T21:09:44.263858: step 3864, loss 0.48232.
Train: 2018-08-08T21:09:47.391173: step 3865, loss 0.562797.
Train: 2018-08-08T21:09:50.509463: step 3866, loss 0.627052.
Train: 2018-08-08T21:09:53.651818: step 3867, loss 0.62696.
Train: 2018-08-08T21:09:56.769106: step 3868, loss 0.578862.
Train: 2018-08-08T21:09:59.904442: step 3869, loss 0.610756.
Train: 2018-08-08T21:10:03.332079: step 3870, loss 0.54706.
Test: 2018-08-08T21:10:25.006454: step 3870, loss 0.549335.
Train: 2018-08-08T21:10:28.316254: step 3871, loss 0.562997.
Train: 2018-08-08T21:10:31.636080: step 3872, loss 0.610518.
Train: 2018-08-08T21:10:34.840600: step 3873, loss 0.484131.
Train: 2018-08-08T21:10:38.060160: step 3874, loss 0.436886.
Train: 2018-08-08T21:10:41.393021: step 3875, loss 0.578862.
Train: 2018-08-08T21:10:44.590523: step 3876, loss 0.594678.
Train: 2018-08-08T21:10:47.798050: step 3877, loss 0.594688.
Train: 2018-08-08T21:10:50.979509: step 3878, loss 0.515538.
Train: 2018-08-08T21:10:54.125874: step 3879, loss 0.547165.
Train: 2018-08-08T21:10:57.307333: step 3880, loss 0.562991.
Test: 2018-08-08T21:11:18.992292: step 3880, loss 0.550487.
Train: 2018-08-08T21:11:22.660043: step 3881, loss 0.515305.
Train: 2018-08-08T21:11:25.953800: step 3882, loss 0.483335.
Train: 2018-08-08T21:11:29.302704: step 3883, loss 0.514972.
Train: 2018-08-08T21:11:32.536301: step 3884, loss 0.514736.
Train: 2018-08-08T21:11:35.967423: step 3885, loss 0.578876.
Train: 2018-08-08T21:11:39.213053: step 3886, loss 0.578887.
Train: 2018-08-08T21:11:42.450661: step 3887, loss 0.497865.
Train: 2018-08-08T21:11:45.693282: step 3888, loss 0.61145.
Train: 2018-08-08T21:11:48.914847: step 3889, loss 0.627857.
Train: 2018-08-08T21:11:52.125383: step 3890, loss 0.611597.
Test: 2018-08-08T21:12:13.513600: step 3890, loss 0.546059.
Train: 2018-08-08T21:12:16.716114: step 3891, loss 0.595274.
Train: 2018-08-08T21:12:19.887546: step 3892, loss 0.49727.
Train: 2018-08-08T21:12:23.060984: step 3893, loss 0.595289.
Train: 2018-08-08T21:12:26.239434: step 3894, loss 0.54624.
Train: 2018-08-08T21:12:29.399837: step 3895, loss 0.70982.
Train: 2018-08-08T21:12:32.553221: step 3896, loss 0.464639.
Train: 2018-08-08T21:12:35.711618: step 3897, loss 0.595259.
Train: 2018-08-08T21:12:38.883050: step 3898, loss 0.644186.
Train: 2018-08-08T21:12:42.068519: step 3899, loss 0.464939.
Train: 2018-08-08T21:12:45.229925: step 3900, loss 0.530078.
Test: 2018-08-08T21:13:06.327016: step 3900, loss 0.548606.
Train: 2018-08-08T21:13:11.326308: step 3901, loss 0.627781.
Train: 2018-08-08T21:13:14.443596: step 3902, loss 0.513816.
Train: 2018-08-08T21:13:17.566900: step 3903, loss 0.627753.
Train: 2018-08-08T21:13:20.682182: step 3904, loss 0.595179.
Train: 2018-08-08T21:13:23.825540: step 3905, loss 0.513938.
Train: 2018-08-08T21:13:26.962881: step 3906, loss 0.546432.
Train: 2018-08-08T21:13:30.116265: step 3907, loss 0.595142.
Train: 2018-08-08T21:13:33.243580: step 3908, loss 0.611356.
Train: 2018-08-08T21:13:36.376910: step 3909, loss 0.611309.
Train: 2018-08-08T21:13:39.507233: step 3910, loss 0.481842.
Test: 2018-08-08T21:14:00.820807: step 3910, loss 0.549405.
Train: 2018-08-08T21:14:04.160396: step 3911, loss 0.595055.
Train: 2018-08-08T21:14:07.607561: step 3912, loss 0.62735.
Train: 2018-08-08T21:14:10.585478: step 3913, loss 0.562752.
Train: 2018-08-08T21:14:13.777966: step 3914, loss 0.707688.
Train: 2018-08-08T21:14:16.965441: step 3915, loss 0.578864.
Train: 2018-08-08T21:14:20.097769: step 3916, loss 0.514922.
Train: 2018-08-08T21:14:23.340390: step 3917, loss 0.499125.
Train: 2018-08-08T21:14:26.596049: step 3918, loss 0.515133.
Train: 2018-08-08T21:14:29.840675: step 3919, loss 0.49919.
Train: 2018-08-08T21:14:33.001078: step 3920, loss 0.578856.
Test: 2018-08-08T21:14:54.703402: step 3920, loss 0.550957.
Train: 2018-08-08T21:14:57.847761: step 3921, loss 0.658675.
Train: 2018-08-08T21:15:01.151545: step 3922, loss 0.403388.
Train: 2018-08-08T21:15:04.434273: step 3923, loss 0.56287.
Train: 2018-08-08T21:15:07.751470: step 3924, loss 0.578864.
Train: 2018-08-08T21:15:11.338006: step 3925, loss 0.61095.
Train: 2018-08-08T21:15:14.534505: step 3926, loss 0.594921.
Train: 2018-08-08T21:15:17.695910: step 3927, loss 0.594937.
Train: 2018-08-08T21:15:20.827235: step 3928, loss 0.61096.
Train: 2018-08-08T21:15:24.053814: step 3929, loss 0.530771.
Train: 2018-08-08T21:15:27.239283: step 3930, loss 0.514793.
Test: 2018-08-08T21:15:48.392493: step 3930, loss 0.549029.
Train: 2018-08-08T21:15:51.576959: step 3931, loss 0.739188.
Train: 2018-08-08T21:15:54.705276: step 3932, loss 0.530883.
Train: 2018-08-08T21:15:57.857658: step 3933, loss 0.578853.
Train: 2018-08-08T21:16:01.113313: step 3934, loss 0.515102.
Train: 2018-08-08T21:16:04.278729: step 3935, loss 0.546992.
Train: 2018-08-08T21:16:07.406044: step 3936, loss 0.499208.
Train: 2018-08-08T21:16:10.570457: step 3937, loss 0.59481.
Train: 2018-08-08T21:16:13.712812: step 3938, loss 0.499101.
Train: 2018-08-08T21:16:16.865193: step 3939, loss 0.546894.
Train: 2018-08-08T21:16:19.989500: step 3940, loss 0.450824.
Test: 2018-08-08T21:16:41.035455: step 3940, loss 0.550191.
Train: 2018-08-08T21:16:44.202876: step 3941, loss 0.659159.
Train: 2018-08-08T21:16:47.326181: step 3942, loss 0.482361.
Train: 2018-08-08T21:16:50.468535: step 3943, loss 0.595013.
Train: 2018-08-08T21:16:53.601866: step 3944, loss 0.659712.
Train: 2018-08-08T21:16:56.723165: step 3945, loss 0.562739.
Train: 2018-08-08T21:16:59.897604: step 3946, loss 0.578899.
Train: 2018-08-08T21:17:03.089090: step 3947, loss 0.578896.
Train: 2018-08-08T21:17:06.210389: step 3948, loss 0.546552.
Train: 2018-08-08T21:17:09.359762: step 3949, loss 0.514216.
Train: 2018-08-08T21:17:12.518159: step 3950, loss 0.514156.
Test: 2018-08-08T21:17:33.473875: step 3950, loss 0.548115.
Train: 2018-08-08T21:17:36.727525: step 3951, loss 0.530269.
Train: 2018-08-08T21:17:39.873891: step 3952, loss 0.627627.
Train: 2018-08-08T21:17:43.057354: step 3953, loss 0.562655.
Train: 2018-08-08T21:17:46.262877: step 3954, loss 0.481325.
Train: 2018-08-08T21:17:49.369136: step 3955, loss 0.481146.
Train: 2018-08-08T21:17:52.660928: step 3956, loss 0.562595.
Train: 2018-08-08T21:17:55.919591: step 3957, loss 0.513385.
Train: 2018-08-08T21:17:59.254325: step 3958, loss 0.578987.
Train: 2018-08-08T21:18:02.387655: step 3959, loss 0.496569.
Train: 2018-08-08T21:18:05.551066: step 3960, loss 0.62865.
Test: 2018-08-08T21:18:26.539870: step 3960, loss 0.548824.
Train: 2018-08-08T21:18:29.698267: step 3961, loss 0.562467.
Train: 2018-08-08T21:18:32.830595: step 3962, loss 0.678641.
Train: 2018-08-08T21:18:35.953899: step 3963, loss 0.479498.
Train: 2018-08-08T21:18:39.194515: step 3964, loss 0.545853.
Train: 2018-08-08T21:18:42.357926: step 3965, loss 0.695422.
Train: 2018-08-08T21:18:45.510307: step 3966, loss 0.695294.
Train: 2018-08-08T21:18:48.639627: step 3967, loss 0.628686.
Train: 2018-08-08T21:18:51.784989: step 3968, loss 0.463604.
Train: 2018-08-08T21:18:54.925339: step 3969, loss 0.463814.
Train: 2018-08-08T21:18:58.070701: step 3970, loss 0.595436.
Test: 2018-08-08T21:19:19.067527: step 3970, loss 0.550254.
Train: 2018-08-08T21:19:22.192835: step 3971, loss 0.644705.
Train: 2018-08-08T21:19:25.340204: step 3972, loss 0.578963.
Train: 2018-08-08T21:19:28.485566: step 3973, loss 0.578947.
Train: 2018-08-08T21:19:31.627921: step 3974, loss 0.497297.
Train: 2018-08-08T21:19:34.790329: step 3975, loss 0.611552.
Train: 2018-08-08T21:19:37.924662: step 3976, loss 0.578918.
Train: 2018-08-08T21:19:41.083059: step 3977, loss 0.530146.
Train: 2018-08-08T21:19:44.182299: step 3978, loss 0.627616.
Train: 2018-08-08T21:19:47.331673: step 3979, loss 0.530273.
Train: 2018-08-08T21:19:50.470017: step 3980, loss 0.530324.
Test: 2018-08-08T21:20:11.552089: step 3980, loss 0.550001.
Train: 2018-08-08T21:20:14.677398: step 3981, loss 0.481813.
Train: 2018-08-08T21:20:17.827774: step 3982, loss 0.64368.
Train: 2018-08-08T21:20:20.973137: step 3983, loss 0.530341.
Train: 2018-08-08T21:20:24.208739: step 3984, loss 0.578897.
Train: 2018-08-08T21:20:27.472416: step 3985, loss 0.578902.
Train: 2018-08-08T21:20:30.624798: step 3986, loss 0.627427.
Train: 2018-08-08T21:20:33.782193: step 3987, loss 0.611193.
Train: 2018-08-08T21:20:36.923544: step 3988, loss 0.546635.
Train: 2018-08-08T21:20:40.043841: step 3989, loss 0.546681.
Train: 2018-08-08T21:20:43.186195: step 3990, loss 0.594951.
Test: 2018-08-08T21:21:04.158956: step 3990, loss 0.549585.
Train: 2018-08-08T21:21:07.349439: step 3991, loss 0.627046.
Train: 2018-08-08T21:21:10.503825: step 3992, loss 0.57887.
Train: 2018-08-08T21:21:13.680270: step 3993, loss 0.562876.
Train: 2018-08-08T21:21:16.858721: step 3994, loss 0.626727.
Train: 2018-08-08T21:21:20.009097: step 3995, loss 0.642506.
Train: 2018-08-08T21:21:23.137414: step 3996, loss 0.642273.
Train: 2018-08-08T21:21:26.291801: step 3997, loss 0.499959.
Train: 2018-08-08T21:21:29.453206: step 3998, loss 0.452994.
Train: 2018-08-08T21:21:32.610601: step 3999, loss 0.563146.
Train: 2018-08-08T21:21:35.764987: step 4000, loss 0.641737.
Test: 2018-08-08T21:21:56.748778: step 4000, loss 0.549668.
Train: 2018-08-08T21:22:01.710971: step 4001, loss 0.484727.
Train: 2018-08-08T21:22:04.835277: step 4002, loss 0.563183.
Train: 2018-08-08T21:22:07.971616: step 4003, loss 0.578874.
Train: 2018-08-08T21:22:11.116979: step 4004, loss 0.516098.
Train: 2018-08-08T21:22:14.274374: step 4005, loss 0.578872.
Train: 2018-08-08T21:22:17.402691: step 4006, loss 0.51599.
Train: 2018-08-08T21:22:20.556075: step 4007, loss 0.421418.
Train: 2018-08-08T21:22:23.760594: step 4008, loss 0.547248.
Train: 2018-08-08T21:22:26.978149: step 4009, loss 0.56299.
Train: 2018-08-08T21:22:30.353122: step 4010, loss 0.562933.
Test: 2018-08-08T21:22:51.449211: step 4010, loss 0.549724.
Train: 2018-08-08T21:22:54.596579: step 4011, loss 0.482991.
Train: 2018-08-08T21:22:57.762997: step 4012, loss 0.578868.
Train: 2018-08-08T21:23:00.919389: step 4013, loss 0.482234.
Train: 2018-08-08T21:23:04.048710: step 4014, loss 0.578893.
Train: 2018-08-08T21:23:07.224152: step 4015, loss 0.611401.
Train: 2018-08-08T21:23:10.349461: step 4016, loss 0.611505.
Train: 2018-08-08T21:23:13.486803: step 4017, loss 0.497335.
Train: 2018-08-08T21:23:16.645200: step 4018, loss 0.49714.
Train: 2018-08-08T21:23:19.799587: step 4019, loss 0.562553.
Train: 2018-08-08T21:23:22.947958: step 4020, loss 0.529597.
Test: 2018-08-08T21:23:44.019984: step 4020, loss 0.549521.
Train: 2018-08-08T21:23:47.203446: step 4021, loss 0.512951.
Train: 2018-08-08T21:23:50.401950: step 4022, loss 0.529322.
Train: 2018-08-08T21:23:53.555334: step 4023, loss 0.529175.
Train: 2018-08-08T21:23:56.686659: step 4024, loss 0.562421.
Train: 2018-08-08T21:23:59.847062: step 4025, loss 0.629399.
Train: 2018-08-08T21:24:02.982398: step 4026, loss 0.528834.
Train: 2018-08-08T21:24:06.135782: step 4027, loss 0.579196.
Train: 2018-08-08T21:24:09.357347: step 4028, loss 0.478187.
Train: 2018-08-08T21:24:12.512736: step 4029, loss 0.596125.
Train: 2018-08-08T21:24:15.650078: step 4030, loss 0.477831.
Test: 2018-08-08T21:24:37.028921: step 4030, loss 0.547754.
Train: 2018-08-08T21:24:40.279561: step 4031, loss 0.494557.
Train: 2018-08-08T21:24:43.475057: step 4032, loss 0.630355.
Train: 2018-08-08T21:24:46.598361: step 4033, loss 0.494217.
Train: 2018-08-08T21:24:49.729686: step 4034, loss 0.45991.
Train: 2018-08-08T21:24:52.904126: step 4035, loss 0.528075.
Train: 2018-08-08T21:24:56.061521: step 4036, loss 0.579524.
Train: 2018-08-08T21:24:59.222926: step 4037, loss 0.545103.
Train: 2018-08-08T21:25:02.355254: step 4038, loss 0.665996.
Train: 2018-08-08T21:25:05.520670: step 4039, loss 0.545055.
Train: 2018-08-08T21:25:08.684080: step 4040, loss 0.527753.
Test: 2018-08-08T21:25:29.743071: step 4040, loss 0.550707.
Train: 2018-08-08T21:25:32.942577: step 4041, loss 0.770031.
Train: 2018-08-08T21:25:36.081924: step 4042, loss 0.579604.
Train: 2018-08-08T21:25:39.236310: step 4043, loss 0.562336.
Train: 2018-08-08T21:25:42.417769: step 4044, loss 0.631041.
Train: 2018-08-08T21:25:45.579174: step 4045, loss 0.545218.
Train: 2018-08-08T21:25:48.725540: step 4046, loss 0.511142.
Train: 2018-08-08T21:25:51.919030: step 4047, loss 0.647487.
Train: 2018-08-08T21:25:55.043337: step 4048, loss 0.664184.
Train: 2018-08-08T21:25:58.216774: step 4049, loss 0.629941.
Train: 2018-08-08T21:26:01.356121: step 4050, loss 0.528774.
Test: 2018-08-08T21:26:22.414109: step 4050, loss 0.549894.
Train: 2018-08-08T21:26:25.638681: step 4051, loss 0.49548.
Train: 2018-08-08T21:26:28.812500: step 4052, loss 0.612468.
Train: 2018-08-08T21:26:31.990951: step 4053, loss 0.562449.
Train: 2018-08-08T21:26:35.163386: step 4054, loss 0.628739.
Train: 2018-08-08T21:26:38.349858: step 4055, loss 0.562507.
Train: 2018-08-08T21:26:41.510261: step 4056, loss 0.578978.
Train: 2018-08-08T21:26:44.662642: step 4057, loss 0.562575.
Train: 2018-08-08T21:26:47.814020: step 4058, loss 0.578934.
Train: 2018-08-08T21:26:50.924290: step 4059, loss 0.660279.
Train: 2018-08-08T21:26:54.071658: step 4060, loss 0.546497.
Test: 2018-08-08T21:27:15.176771: step 4060, loss 0.550063.
Train: 2018-08-08T21:27:18.372266: step 4061, loss 0.595023.
Train: 2018-08-08T21:27:21.599848: step 4062, loss 0.578872.
Train: 2018-08-08T21:27:24.812389: step 4063, loss 0.562848.
Train: 2018-08-08T21:27:28.078071: step 4064, loss 0.562896.
Train: 2018-08-08T21:27:31.292618: step 4065, loss 0.594777.
Train: 2018-08-08T21:27:34.508167: step 4066, loss 0.547119.
Train: 2018-08-08T21:27:37.745775: step 4067, loss 0.531365.
Train: 2018-08-08T21:27:40.913196: step 4068, loss 0.594669.
Train: 2018-08-08T21:27:44.078612: step 4069, loss 0.563084.
Train: 2018-08-08T21:27:47.214951: step 4070, loss 0.657642.
Test: 2018-08-08T21:28:08.256896: step 4070, loss 0.549628.
Train: 2018-08-08T21:28:11.410280: step 4071, loss 0.626003.
Train: 2018-08-08T21:28:14.606778: step 4072, loss 0.547569.
Train: 2018-08-08T21:28:17.788237: step 4073, loss 0.594502.
Train: 2018-08-08T21:28:20.937610: step 4074, loss 0.501059.
Train: 2018-08-08T21:28:24.086984: step 4075, loss 0.594453.
Train: 2018-08-08T21:28:27.257413: step 4076, loss 0.625482.
Train: 2018-08-08T21:28:30.399767: step 4077, loss 0.594411.
Train: 2018-08-08T21:28:33.554154: step 4078, loss 0.563487.
Train: 2018-08-08T21:28:36.734610: step 4079, loss 0.501858.
Train: 2018-08-08T21:28:39.913061: step 4080, loss 0.563541.
Test: 2018-08-08T21:29:00.929939: step 4080, loss 0.549694.
Train: 2018-08-08T21:29:04.100368: step 4081, loss 0.609764.
Train: 2018-08-08T21:29:07.268792: step 4082, loss 0.563565.
Train: 2018-08-08T21:29:10.436213: step 4083, loss 0.548192.
Train: 2018-08-08T21:29:13.591603: step 4084, loss 0.50204.
Train: 2018-08-08T21:29:16.729947: step 4085, loss 0.671379.
Train: 2018-08-08T21:29:19.930456: step 4086, loss 0.517372.
Train: 2018-08-08T21:29:23.065792: step 4087, loss 0.517335.
Train: 2018-08-08T21:29:26.226194: step 4088, loss 0.517232.
Train: 2018-08-08T21:29:29.369552: step 4089, loss 0.548002.
Train: 2018-08-08T21:29:32.541986: step 4090, loss 0.594424.
Test: 2018-08-08T21:29:53.541820: step 4090, loss 0.551166.
Train: 2018-08-08T21:29:56.708238: step 4091, loss 0.532301.
Train: 2018-08-08T21:29:59.863627: step 4092, loss 0.454301.
Train: 2018-08-08T21:30:03.167061: step 4093, loss 0.610166.
Train: 2018-08-08T21:30:06.387624: step 4094, loss 0.625951.
Train: 2018-08-08T21:30:09.606181: step 4095, loss 0.578868.
Train: 2018-08-08T21:30:12.820727: step 4096, loss 0.62611.
Train: 2018-08-08T21:30:16.052319: step 4097, loss 0.594624.
Train: 2018-08-08T21:30:19.179634: step 4098, loss 0.484333.
Train: 2018-08-08T21:30:22.310959: step 4099, loss 0.610421.
Train: 2018-08-08T21:30:25.517484: step 4100, loss 0.594652.
Test: 2018-08-08T21:30:46.516315: step 4100, loss 0.548273.
Train: 2018-08-08T21:30:51.414337: step 4101, loss 0.547279.
Train: 2018-08-08T21:30:54.559699: step 4102, loss 0.578862.
Train: 2018-08-08T21:30:57.708070: step 4103, loss 0.499833.
Train: 2018-08-08T21:31:00.849423: step 4104, loss 0.547203.
Train: 2018-08-08T21:31:03.989772: step 4105, loss 0.594714.
Train: 2018-08-08T21:31:07.149172: step 4106, loss 0.531239.
Train: 2018-08-08T21:31:10.296539: step 4107, loss 0.531162.
Train: 2018-08-08T21:31:13.448921: step 4108, loss 0.546995.
Train: 2018-08-08T21:31:16.607319: step 4109, loss 0.403222.
Train: 2018-08-08T21:31:19.775742: step 4110, loss 0.610952.
Test: 2018-08-08T21:31:40.721431: step 4110, loss 0.549521.
Train: 2018-08-08T21:31:43.932970: step 4111, loss 0.578876.
Train: 2018-08-08T21:31:47.075324: step 4112, loss 0.595029.
Train: 2018-08-08T21:31:50.240740: step 4113, loss 0.530355.
Train: 2018-08-08T21:31:53.375073: step 4114, loss 0.530246.
Train: 2018-08-08T21:31:56.497375: step 4115, loss 0.546388.
Train: 2018-08-08T21:31:59.626695: step 4116, loss 0.530005.
Train: 2018-08-08T21:32:02.757018: step 4117, loss 0.611658.
Train: 2018-08-08T21:32:05.919425: step 4118, loss 0.546183.
Train: 2018-08-08T21:32:09.070804: step 4119, loss 0.546133.
Train: 2018-08-08T21:32:12.220177: step 4120, loss 0.496728.
Test: 2018-08-08T21:32:33.210987: step 4120, loss 0.548299.
Train: 2018-08-08T21:32:36.348328: step 4121, loss 0.579005.
Train: 2018-08-08T21:32:39.529786: step 4122, loss 0.545957.
Train: 2018-08-08T21:32:42.683170: step 4123, loss 0.562473.
Train: 2018-08-08T21:32:45.834549: step 4124, loss 0.496058.
Train: 2018-08-08T21:32:48.980914: step 4125, loss 0.562439.
Train: 2018-08-08T21:32:52.094191: step 4126, loss 0.529059.
Train: 2018-08-08T21:32:55.232535: step 4127, loss 0.528959.
Train: 2018-08-08T21:32:58.412991: step 4128, loss 0.545626.
Train: 2018-08-08T21:33:01.552338: step 4129, loss 0.478318.
Train: 2018-08-08T21:33:04.733797: step 4130, loss 0.511755.
Test: 2018-08-08T21:33:25.709568: step 4130, loss 0.547129.
Train: 2018-08-08T21:33:28.842896: step 4131, loss 0.630093.
Train: 2018-08-08T21:33:32.028365: step 4132, loss 0.47749.
Train: 2018-08-08T21:33:35.208821: step 4133, loss 0.528291.
Train: 2018-08-08T21:33:38.397299: step 4134, loss 0.613574.
Train: 2018-08-08T21:33:41.561712: step 4135, loss 0.596559.
Train: 2018-08-08T21:33:44.727128: step 4136, loss 0.596594.
Train: 2018-08-08T21:33:47.880512: step 4137, loss 0.562336.
Train: 2018-08-08T21:33:51.030888: step 4138, loss 0.630879.
Train: 2018-08-08T21:33:54.189285: step 4139, loss 0.545218.
Train: 2018-08-08T21:33:57.340664: step 4140, loss 0.664949.
Test: 2018-08-08T21:34:18.500923: step 4140, loss 0.548935.
Train: 2018-08-08T21:34:21.694414: step 4141, loss 0.511158.
Train: 2018-08-08T21:34:24.842784: step 4142, loss 0.562343.
Train: 2018-08-08T21:34:27.987144: step 4143, loss 0.460313.
Train: 2018-08-08T21:34:31.127493: step 4144, loss 0.477326.
Train: 2018-08-08T21:34:34.259822: step 4145, loss 0.545321.
Train: 2018-08-08T21:34:37.375104: step 4146, loss 0.596418.
Train: 2018-08-08T21:34:40.565587: step 4147, loss 0.56234.
Train: 2018-08-08T21:34:43.700923: step 4148, loss 0.647565.
Train: 2018-08-08T21:34:46.857315: step 4149, loss 0.545319.
Train: 2018-08-08T21:34:50.000672: step 4150, loss 0.647366.
Test: 2018-08-08T21:35:10.934329: step 4150, loss 0.547097.
Train: 2018-08-08T21:35:14.102753: step 4151, loss 0.562351.
Train: 2018-08-08T21:35:17.271177: step 4152, loss 0.680834.
Train: 2018-08-08T21:35:20.435591: step 4153, loss 0.562371.
Train: 2018-08-08T21:35:23.607022: step 4154, loss 0.478401.
Train: 2018-08-08T21:35:26.786476: step 4155, loss 0.545636.
Train: 2018-08-08T21:35:29.945876: step 4156, loss 0.562408.
Train: 2018-08-08T21:35:33.133350: step 4157, loss 0.495605.
Train: 2018-08-08T21:35:36.289742: step 4158, loss 0.579114.
Train: 2018-08-08T21:35:39.405025: step 4159, loss 0.562428.
Train: 2018-08-08T21:35:42.543369: step 4160, loss 0.612426.
Test: 2018-08-08T21:36:03.503095: step 4160, loss 0.55.
Train: 2018-08-08T21:36:06.611359: step 4161, loss 0.695542.
Train: 2018-08-08T21:36:09.747697: step 4162, loss 0.512735.
Train: 2018-08-08T21:36:12.907097: step 4163, loss 0.562488.
Train: 2018-08-08T21:36:16.051457: step 4164, loss 0.579005.
Train: 2018-08-08T21:36:19.161727: step 4165, loss 0.595446.
Train: 2018-08-08T21:36:22.284028: step 4166, loss 0.546139.
Train: 2018-08-08T21:36:25.435407: step 4167, loss 0.562575.
Train: 2018-08-08T21:36:28.591799: step 4168, loss 0.562596.
Train: 2018-08-08T21:36:31.763231: step 4169, loss 0.464722.
Train: 2018-08-08T21:36:34.896561: step 4170, loss 0.497358.
Test: 2018-08-08T21:36:55.874336: step 4170, loss 0.549157.
Train: 2018-08-08T21:36:59.132999: step 4171, loss 0.529944.
Train: 2018-08-08T21:37:02.386650: step 4172, loss 0.628003.
Train: 2018-08-08T21:37:05.595180: step 4173, loss 0.464451.
Train: 2018-08-08T21:37:08.924031: step 4174, loss 0.546186.
Train: 2018-08-08T21:37:12.284967: step 4175, loss 0.562553.
Train: 2018-08-08T21:37:15.624847: step 4176, loss 0.59542.
Train: 2018-08-08T21:37:18.936652: step 4177, loss 0.578985.
Train: 2018-08-08T21:37:22.320649: step 4178, loss 0.595452.
Train: 2018-08-08T21:37:25.697627: step 4179, loss 0.677758.
Train: 2018-08-08T21:37:29.066585: step 4180, loss 0.578975.
Test: 2018-08-08T21:37:50.461468: step 4180, loss 0.547821.
Train: 2018-08-08T21:37:53.611843: step 4181, loss 0.447787.
Train: 2018-08-08T21:37:56.732139: step 4182, loss 0.513375.
Train: 2018-08-08T21:37:59.871486: step 4183, loss 0.562558.
Train: 2018-08-08T21:38:02.990779: step 4184, loss 0.595388.
Train: 2018-08-08T21:38:06.118094: step 4185, loss 0.57897.
Train: 2018-08-08T21:38:09.217334: step 4186, loss 0.595383.
Train: 2018-08-08T21:38:12.327603: step 4187, loss 0.660977.
Train: 2018-08-08T21:38:15.472966: step 4188, loss 0.529852.
Train: 2018-08-08T21:38:18.587246: step 4189, loss 0.61162.
Train: 2018-08-08T21:38:21.701526: step 4190, loss 0.513708.
Test: 2018-08-08T21:38:42.699354: step 4190, loss 0.54984.
Train: 2018-08-08T21:38:45.861761: step 4191, loss 0.562636.
Train: 2018-08-08T21:38:49.036201: step 4192, loss 0.643987.
Train: 2018-08-08T21:38:52.175548: step 4193, loss 0.497748.
Train: 2018-08-08T21:38:55.307876: step 4194, loss 0.562686.
Train: 2018-08-08T21:38:58.464268: step 4195, loss 0.627503.
Train: 2018-08-08T21:39:01.615647: step 4196, loss 0.546545.
Train: 2018-08-08T21:39:04.766023: step 4197, loss 0.481978.
Train: 2018-08-08T21:39:07.916399: step 4198, loss 0.562732.
Train: 2018-08-08T21:39:11.074796: step 4199, loss 0.627357.
Train: 2018-08-08T21:39:14.241215: step 4200, loss 0.514313.
Test: 2018-08-08T21:39:35.206775: step 4200, loss 0.548835.
Train: 2018-08-08T21:39:40.174984: step 4201, loss 0.578884.
Train: 2018-08-08T21:39:43.304304: step 4202, loss 0.449759.
Train: 2018-08-08T21:39:46.462701: step 4203, loss 0.530384.
Train: 2018-08-08T21:39:49.630123: step 4204, loss 0.546493.
Train: 2018-08-08T21:39:52.765458: step 4205, loss 0.513957.
Train: 2018-08-08T21:39:55.942907: step 4206, loss 0.432379.
Train: 2018-08-08T21:39:59.075235: step 4207, loss 0.611659.
Train: 2018-08-08T21:40:02.376096: step 4208, loss 0.529734.
Train: 2018-08-08T21:40:05.554547: step 4209, loss 0.562524.
Train: 2018-08-08T21:40:08.707931: step 4210, loss 0.529462.
Test: 2018-08-08T21:40:29.709769: step 4210, loss 0.548825.
Train: 2018-08-08T21:40:32.857137: step 4211, loss 0.529329.
Train: 2018-08-08T21:40:35.990468: step 4212, loss 0.628954.
Train: 2018-08-08T21:40:39.118785: step 4213, loss 0.61241.
Train: 2018-08-08T21:40:42.048574: step 4214, loss 0.562429.
Train: 2018-08-08T21:40:45.185916: step 4215, loss 0.495684.
Train: 2018-08-08T21:40:48.360356: step 4216, loss 0.62926.
Train: 2018-08-08T21:40:51.487670: step 4217, loss 0.662713.
Train: 2018-08-08T21:40:54.656094: step 4218, loss 0.579116.
Train: 2018-08-08T21:40:57.773382: step 4219, loss 0.495755.
Train: 2018-08-08T21:41:00.923758: step 4220, loss 0.629079.
Test: 2018-08-08T21:41:21.898525: step 4220, loss 0.551888.
Train: 2018-08-08T21:41:25.030852: step 4221, loss 0.6789.
Train: 2018-08-08T21:41:28.171202: step 4222, loss 0.579049.
Train: 2018-08-08T21:41:31.288490: step 4223, loss 0.645143.
Train: 2018-08-08T21:41:34.376700: step 4224, loss 0.578988.
Train: 2018-08-08T21:41:37.507023: step 4225, loss 0.562566.
Train: 2018-08-08T21:41:40.665420: step 4226, loss 0.448284.
Train: 2018-08-08T21:41:43.809780: step 4227, loss 0.595234.
Train: 2018-08-08T21:41:46.949127: step 4228, loss 0.578918.
Train: 2018-08-08T21:41:50.082458: step 4229, loss 0.481453.
Train: 2018-08-08T21:41:53.227820: step 4230, loss 0.578906.
Test: 2018-08-08T21:42:14.213616: step 4230, loss 0.551772.
Train: 2018-08-08T21:42:17.427160: step 4231, loss 0.497783.
Train: 2018-08-08T21:42:20.572522: step 4232, loss 0.562673.
Train: 2018-08-08T21:42:23.724904: step 4233, loss 0.562668.
Train: 2018-08-08T21:42:26.895333: step 4234, loss 0.578909.
Train: 2018-08-08T21:42:30.033677: step 4235, loss 0.530168.
Train: 2018-08-08T21:42:33.154976: step 4236, loss 0.643942.
Train: 2018-08-08T21:42:36.296329: step 4237, loss 0.6114.
Train: 2018-08-08T21:42:39.434672: step 4238, loss 0.432899.
Train: 2018-08-08T21:42:42.605101: step 4239, loss 0.530198.
Train: 2018-08-08T21:42:45.760490: step 4240, loss 0.465112.
Test: 2018-08-08T21:43:06.754308: step 4240, loss 0.547966.
Train: 2018-08-08T21:43:09.908694: step 4241, loss 0.497418.
Train: 2018-08-08T21:43:13.038014: step 4242, loss 0.595305.
Train: 2018-08-08T21:43:16.176358: step 4243, loss 0.693773.
Train: 2018-08-08T21:43:19.291640: step 4244, loss 0.693803.
Train: 2018-08-08T21:43:22.424971: step 4245, loss 0.644452.
Train: 2018-08-08T21:43:25.602419: step 4246, loss 0.562608.
Train: 2018-08-08T21:43:28.790896: step 4247, loss 0.546355.
Train: 2018-08-08T21:43:31.951299: step 4248, loss 0.530169.
Train: 2018-08-08T21:43:35.113707: step 4249, loss 0.627574.
Train: 2018-08-08T21:43:38.252051: step 4250, loss 0.46559.
Test: 2018-08-08T21:43:59.240854: step 4250, loss 0.548778.
Train: 2018-08-08T21:44:02.408275: step 4251, loss 0.449465.
Train: 2018-08-08T21:44:05.541606: step 4252, loss 0.676105.
Train: 2018-08-08T21:44:08.750137: step 4253, loss 0.643666.
Train: 2018-08-08T21:44:11.914550: step 4254, loss 0.54656.
Train: 2018-08-08T21:44:15.073950: step 4255, loss 0.627315.
Train: 2018-08-08T21:44:18.204273: step 4256, loss 0.578878.
Train: 2018-08-08T21:44:21.351641: step 4257, loss 0.466358.
Train: 2018-08-08T21:44:24.492993: step 4258, loss 0.466395.
Train: 2018-08-08T21:44:27.647379: step 4259, loss 0.482338.
Train: 2018-08-08T21:44:30.769681: step 4260, loss 0.562751.
Test: 2018-08-08T21:44:51.834687: step 4260, loss 0.549407.
Train: 2018-08-08T21:44:55.204646: step 4261, loss 0.627393.
Train: 2018-08-08T21:44:58.453284: step 4262, loss 0.562708.
Train: 2018-08-08T21:45:01.674849: step 4263, loss 0.643703.
Train: 2018-08-08T21:45:04.857310: step 4264, loss 0.53031.
Train: 2018-08-08T21:45:08.135690: step 4265, loss 0.611293.
Train: 2018-08-08T21:45:11.449764: step 4266, loss 0.433193.
Train: 2018-08-08T21:45:14.717451: step 4267, loss 0.562686.
Train: 2018-08-08T21:45:17.969097: step 4268, loss 0.54643.
Train: 2018-08-08T21:45:21.312988: step 4269, loss 0.69276.
Train: 2018-08-08T21:45:24.568643: step 4270, loss 0.513898.
Test: 2018-08-08T21:45:45.840199: step 4270, loss 0.550497.
Train: 2018-08-08T21:45:49.075584: step 4271, loss 0.611425.
Train: 2018-08-08T21:45:52.815115: step 4272, loss 0.595156.
Train: 2018-08-08T21:45:56.208136: step 4273, loss 0.546444.
Train: 2018-08-08T21:45:59.536987: step 4274, loss 0.530242.
Train: 2018-08-08T21:46:02.735491: step 4275, loss 0.578902.
Train: 2018-08-08T21:46:05.884864: step 4276, loss 0.514042.
Train: 2018-08-08T21:46:09.037245: step 4277, loss 0.562679.
Train: 2018-08-08T21:46:12.194640: step 4278, loss 0.578905.
Train: 2018-08-08T21:46:15.325965: step 4279, loss 0.578906.
Train: 2018-08-08T21:46:18.472331: step 4280, loss 0.611374.
Test: 2018-08-08T21:46:39.746899: step 4280, loss 0.551163.
Train: 2018-08-08T21:46:42.879222: step 4281, loss 0.497797.
Train: 2018-08-08T21:46:46.020574: step 4282, loss 0.578904.
Train: 2018-08-08T21:46:49.151899: step 4283, loss 0.562677.
Train: 2018-08-08T21:46:52.300270: step 4284, loss 0.562676.
Train: 2018-08-08T21:46:55.455659: step 4285, loss 0.530216.
Train: 2018-08-08T21:46:58.625086: step 4286, loss 0.530189.
Train: 2018-08-08T21:47:01.767440: step 4287, loss 0.562655.
Train: 2018-08-08T21:47:04.923832: step 4288, loss 0.546371.
Train: 2018-08-08T21:47:08.079222: step 4289, loss 0.513756.
Train: 2018-08-08T21:47:11.228595: step 4290, loss 0.513647.
Test: 2018-08-08T21:47:32.231436: step 4290, loss 0.55035.
Train: 2018-08-08T21:47:35.470046: step 4291, loss 0.709834.
Train: 2018-08-08T21:47:38.618417: step 4292, loss 0.529872.
Train: 2018-08-08T21:47:41.753753: step 4293, loss 0.464402.
Train: 2018-08-08T21:47:44.924182: step 4294, loss 0.578959.
Train: 2018-08-08T21:47:48.086590: step 4295, loss 0.546135.
Train: 2018-08-08T21:47:51.244987: step 4296, loss 0.513215.
Train: 2018-08-08T21:47:54.411406: step 4297, loss 0.62842.
Train: 2018-08-08T21:47:57.600886: step 4298, loss 0.579.
Train: 2018-08-08T21:48:00.737225: step 4299, loss 0.546014.
Train: 2018-08-08T21:48:03.874566: step 4300, loss 0.579009.
Test: 2018-08-08T21:48:25.261726: step 4300, loss 0.54953.
Train: 2018-08-08T21:48:30.277060: step 4301, loss 0.496465.
Train: 2018-08-08T21:48:33.469548: step 4302, loss 0.529433.
Train: 2018-08-08T21:48:36.629950: step 4303, loss 0.496261.
Train: 2018-08-08T21:48:39.862545: step 4304, loss 0.612242.
Train: 2018-08-08T21:48:43.011918: step 4305, loss 0.612298.
Train: 2018-08-08T21:48:46.158284: step 4306, loss 0.495967.
Train: 2018-08-08T21:48:49.327710: step 4307, loss 0.595725.
Train: 2018-08-08T21:48:52.485105: step 4308, loss 0.479185.
Train: 2018-08-08T21:48:55.669571: step 4309, loss 0.545747.
Train: 2018-08-08T21:48:58.804907: step 4310, loss 0.529001.
Test: 2018-08-08T21:49:19.957146: step 4310, loss 0.549252.
Train: 2018-08-08T21:49:23.097495: step 4311, loss 0.528926.
Train: 2018-08-08T21:49:26.276948: step 4312, loss 0.545618.
Train: 2018-08-08T21:49:29.471441: step 4313, loss 0.64644.
Train: 2018-08-08T21:49:32.618809: step 4314, loss 0.545561.
Train: 2018-08-08T21:49:35.829345: step 4315, loss 0.562379.
Train: 2018-08-08T21:49:39.032690: step 4316, loss 0.646565.
Train: 2018-08-08T21:49:42.235205: step 4317, loss 0.646488.
Train: 2018-08-08T21:49:45.431703: step 4318, loss 0.646311.
Train: 2018-08-08T21:49:48.590101: step 4319, loss 0.595864.
Train: 2018-08-08T21:49:51.709394: step 4320, loss 0.545766.
Test: 2018-08-08T21:50:12.898731: step 4320, loss 0.54877.
Train: 2018-08-08T21:50:16.095229: step 4321, loss 0.51261.
Train: 2018-08-08T21:50:19.246608: step 4322, loss 0.429837.
Train: 2018-08-08T21:50:22.408013: step 4323, loss 0.579048.
Train: 2018-08-08T21:50:25.588469: step 4324, loss 0.545893.
Train: 2018-08-08T21:50:28.758899: step 4325, loss 0.661925.
Train: 2018-08-08T21:50:31.926320: step 4326, loss 0.59558.
Train: 2018-08-08T21:50:35.109784: step 4327, loss 0.612047.
Train: 2018-08-08T21:50:38.275200: step 4328, loss 0.628412.
Train: 2018-08-08T21:50:41.603048: step 4329, loss 0.562552.
Train: 2018-08-08T21:50:44.962981: step 4330, loss 0.562583.
Test: 2018-08-08T21:51:06.068094: step 4330, loss 0.550409.
Train: 2018-08-08T21:51:09.228496: step 4331, loss 0.546295.
Train: 2018-08-08T21:51:12.376867: step 4332, loss 0.464954.
Train: 2018-08-08T21:51:15.534261: step 4333, loss 0.644007.
Train: 2018-08-08T21:51:18.707699: step 4334, loss 0.530177.
Train: 2018-08-08T21:51:21.866096: step 4335, loss 0.513996.
Train: 2018-08-08T21:51:25.016472: step 4336, loss 0.611353.
Train: 2018-08-08T21:51:28.172864: step 4337, loss 0.530269.
Train: 2018-08-08T21:51:31.323240: step 4338, loss 0.595104.
Train: 2018-08-08T21:51:34.454565: step 4339, loss 0.59509.
Train: 2018-08-08T21:51:37.607949: step 4340, loss 0.611242.
Test: 2018-08-08T21:51:58.757180: step 4340, loss 0.551273.
Train: 2018-08-08T21:52:01.960696: step 4341, loss 0.627323.
Train: 2018-08-08T21:52:05.201312: step 4342, loss 0.562773.
Train: 2018-08-08T21:52:08.358707: step 4343, loss 0.514608.
Train: 2018-08-08T21:52:11.521115: step 4344, loss 0.67514.
Train: 2018-08-08T21:52:14.638403: step 4345, loss 0.498872.
Train: 2018-08-08T21:52:17.772736: step 4346, loss 0.483018.
Train: 2018-08-08T21:52:20.927123: step 4347, loss 0.546912.
Train: 2018-08-08T21:52:24.090533: step 4348, loss 0.51494.
Train: 2018-08-08T21:52:27.237901: step 4349, loss 0.498865.
Train: 2018-08-08T21:52:30.363212: step 4350, loss 0.562832.
Test: 2018-08-08T21:52:51.366052: step 4350, loss 0.550786.
Train: 2018-08-08T21:52:54.555531: step 4351, loss 0.627074.
Train: 2018-08-08T21:52:57.696883: step 4352, loss 0.498467.
Train: 2018-08-08T21:53:00.830214: step 4353, loss 0.514434.
Train: 2018-08-08T21:53:03.964547: step 4354, loss 0.627339.
Train: 2018-08-08T21:53:07.060780: step 4355, loss 0.481864.
Train: 2018-08-08T21:53:10.177065: step 4356, loss 0.59511.
Train: 2018-08-08T21:53:13.323430: step 4357, loss 0.513955.
Train: 2018-08-08T21:53:16.548003: step 4358, loss 0.562642.
Train: 2018-08-08T21:53:19.852791: step 4359, loss 0.529999.
Train: 2018-08-08T21:53:23.059315: step 4360, loss 0.595291.
Test: 2018-08-08T21:53:44.301794: step 4360, loss 0.54971.
Train: 2018-08-08T21:53:47.463198: step 4361, loss 0.546201.
Train: 2018-08-08T21:53:50.623601: step 4362, loss 0.529753.
Train: 2018-08-08T21:53:53.752921: step 4363, loss 0.529667.
Train: 2018-08-08T21:53:56.914326: step 4364, loss 0.49662.
Train: 2018-08-08T21:54:00.068713: step 4365, loss 0.61207.
Train: 2018-08-08T21:54:03.263206: step 4366, loss 0.529362.
Train: 2018-08-08T21:54:06.468729: step 4367, loss 0.628838.
Train: 2018-08-08T21:54:09.686283: step 4368, loss 0.479409.
Train: 2018-08-08T21:54:12.867742: step 4369, loss 0.545798.
Train: 2018-08-08T21:54:15.996059: step 4370, loss 0.495724.
Test: 2018-08-08T21:54:37.098449: step 4370, loss 0.549905.
Train: 2018-08-08T21:54:40.366994: step 4371, loss 0.629298.
Train: 2018-08-08T21:54:43.642703: step 4372, loss 0.579148.
Train: 2018-08-08T21:54:46.891489: step 4373, loss 0.612675.
Train: 2018-08-08T21:54:50.122992: step 4374, loss 0.612672.
Train: 2018-08-08T21:54:53.345560: step 4375, loss 0.612631.
Train: 2018-08-08T21:54:56.521003: step 4376, loss 0.5457.
Train: 2018-08-08T21:54:59.789693: step 4377, loss 0.495656.
Train: 2018-08-08T21:55:03.171685: step 4378, loss 0.512363.
Train: 2018-08-08T21:55:06.506551: step 4379, loss 0.47895.
Train: 2018-08-08T21:55:09.836405: step 4380, loss 0.52897.
Test: 2018-08-08T21:55:31.387704: step 4380, loss 0.546072.
Train: 2018-08-08T21:55:34.652384: step 4381, loss 0.646159.
Train: 2018-08-08T21:55:37.913053: step 4382, loss 0.545643.
Train: 2018-08-08T21:55:41.061423: step 4383, loss 0.730035.
Train: 2018-08-08T21:55:44.248898: step 4384, loss 0.562409.
Train: 2018-08-08T21:55:47.411306: step 4385, loss 0.595804.
Train: 2018-08-08T21:55:50.596776: step 4386, loss 0.545789.
Train: 2018-08-08T21:55:53.753167: step 4387, loss 0.595682.
Train: 2018-08-08T21:55:56.911565: step 4388, loss 0.496174.
Train: 2018-08-08T21:56:00.055925: step 4389, loss 0.595587.
Train: 2018-08-08T21:56:03.200285: step 4390, loss 0.579019.
Test: 2018-08-08T21:56:24.247243: step 4390, loss 0.547677.
Train: 2018-08-08T21:56:27.428701: step 4391, loss 0.661482.
Train: 2018-08-08T21:56:30.641242: step 4392, loss 0.628307.
Train: 2018-08-08T21:56:33.792621: step 4393, loss 0.562576.
Train: 2018-08-08T21:56:36.995136: step 4394, loss 0.660521.
Train: 2018-08-08T21:56:40.213693: step 4395, loss 0.578907.
Train: 2018-08-08T21:56:43.484531: step 4396, loss 0.562724.
Train: 2018-08-08T21:56:46.768764: step 4397, loss 0.627168.
Train: 2018-08-08T21:56:50.195876: step 4398, loss 0.546824.
Train: 2018-08-08T21:56:53.461559: step 4399, loss 0.610775.
Train: 2018-08-08T21:56:56.842548: step 4400, loss 0.435862.
Test: 2018-08-08T21:57:17.845389: step 4400, loss 0.547534.
Train: 2018-08-08T21:57:22.796552: step 4401, loss 0.594721.
Train: 2018-08-08T21:57:25.937904: step 4402, loss 0.57886.
Train: 2018-08-08T21:57:29.085272: step 4403, loss 0.436648.
Train: 2018-08-08T21:57:32.185515: step 4404, loss 0.53143.
Train: 2018-08-08T21:57:35.324862: step 4405, loss 0.468062.
Train: 2018-08-08T21:57:38.467216: step 4406, loss 0.515372.
Train: 2018-08-08T21:57:41.590520: step 4407, loss 0.674409.
Train: 2018-08-08T21:57:44.735883: step 4408, loss 0.54697.
Train: 2018-08-08T21:57:47.875230: step 4409, loss 0.610797.
Train: 2018-08-08T21:57:51.014576: step 4410, loss 0.53093.
Test: 2018-08-08T21:58:11.971295: step 4410, loss 0.5503.
Train: 2018-08-08T21:58:15.023409: step 4411, loss 0.4509.
Train: 2018-08-08T21:58:18.102596: step 4412, loss 0.562825.
Train: 2018-08-08T21:58:21.147692: step 4413, loss 0.514531.
Train: 2018-08-08T21:58:24.198804: step 4414, loss 0.546607.
Train: 2018-08-08T21:58:27.256934: step 4415, loss 0.530324.
Train: 2018-08-08T21:58:30.294009: step 4416, loss 0.530171.
Train: 2018-08-08T21:58:33.390241: step 4417, loss 0.562622.
Train: 2018-08-08T21:58:36.441353: step 4418, loss 0.578946.
Train: 2018-08-08T21:58:39.479431: step 4419, loss 0.464154.
Train: 2018-08-08T21:58:42.531546: step 4420, loss 0.57899.
Test: 2018-08-08T21:59:03.260659: step 4420, loss 0.550768.
Train: 2018-08-08T21:59:06.321797: step 4421, loss 0.579015.
Train: 2018-08-08T21:59:09.415021: step 4422, loss 0.62872.
Train: 2018-08-08T21:59:12.476160: step 4423, loss 0.529304.
Train: 2018-08-08T21:59:15.555347: step 4424, loss 0.512633.
Train: 2018-08-08T21:59:18.602448: step 4425, loss 0.529155.
Train: 2018-08-08T21:59:21.660579: step 4426, loss 0.529062.
Train: 2018-08-08T21:59:24.725728: step 4427, loss 0.579134.
Train: 2018-08-08T21:59:27.762803: step 4428, loss 0.595912.
Train: 2018-08-08T21:59:30.810907: step 4429, loss 0.562394.
Train: 2018-08-08T21:59:33.856003: step 4430, loss 0.595971.
Test: 2018-08-08T21:59:54.599154: step 4430, loss 0.548558.
Train: 2018-08-08T21:59:57.650265: step 4431, loss 0.612771.
Train: 2018-08-08T22:00:00.684332: step 4432, loss 0.562391.
Train: 2018-08-08T22:00:03.715391: step 4433, loss 0.528852.
Train: 2018-08-08T22:00:06.761490: step 4434, loss 0.579164.
Train: 2018-08-08T22:00:09.821626: step 4435, loss 0.579158.
Train: 2018-08-08T22:00:12.879756: step 4436, loss 0.47868.
Train: 2018-08-08T22:00:15.930868: step 4437, loss 0.545649.
Train: 2018-08-08T22:00:18.966941: step 4438, loss 0.696485.
Train: 2018-08-08T22:00:22.034095: step 4439, loss 0.512207.
Train: 2018-08-08T22:00:25.087213: step 4440, loss 0.545692.
Test: 2018-08-08T22:00:45.832368: step 4440, loss 0.548021.
Train: 2018-08-08T22:00:48.914563: step 4441, loss 0.562415.
Train: 2018-08-08T22:00:51.984726: step 4442, loss 0.645923.
Train: 2018-08-08T22:00:55.054888: step 4443, loss 0.612438.
Train: 2018-08-08T22:00:58.091963: step 4444, loss 0.645582.
Train: 2018-08-08T22:01:01.127032: step 4445, loss 0.761271.
Train: 2018-08-08T22:01:04.161099: step 4446, loss 0.513146.
Train: 2018-08-08T22:01:07.200179: step 4447, loss 0.497067.
Train: 2018-08-08T22:01:10.270342: step 4448, loss 0.562613.
Train: 2018-08-08T22:01:13.313433: step 4449, loss 0.497582.
Train: 2018-08-08T22:01:16.360534: step 4450, loss 0.481487.
Test: 2018-08-08T22:01:37.107696: step 4450, loss 0.548694.
Train: 2018-08-08T22:01:40.163820: step 4451, loss 0.627597.
Train: 2018-08-08T22:01:43.209919: step 4452, loss 0.497853.
Train: 2018-08-08T22:01:46.261031: step 4453, loss 0.562692.
Train: 2018-08-08T22:01:49.332197: step 4454, loss 0.562693.
Train: 2018-08-08T22:01:52.359245: step 4455, loss 0.497878.
Train: 2018-08-08T22:01:55.417376: step 4456, loss 0.562682.
Train: 2018-08-08T22:01:58.488541: step 4457, loss 0.578906.
Train: 2018-08-08T22:02:01.529626: step 4458, loss 0.513929.
Train: 2018-08-08T22:02:04.577730: step 4459, loss 0.562649.
Train: 2018-08-08T22:02:07.636864: step 4460, loss 0.513778.
Test: 2018-08-08T22:02:28.362969: step 4460, loss 0.549178.
Train: 2018-08-08T22:02:31.413078: step 4461, loss 0.513666.
Train: 2018-08-08T22:02:34.433108: step 4462, loss 0.497164.
Train: 2018-08-08T22:02:37.515302: step 4463, loss 0.595375.
Train: 2018-08-08T22:02:40.584462: step 4464, loss 0.611878.
Train: 2018-08-08T22:02:43.654625: step 4465, loss 0.595462.
Train: 2018-08-08T22:02:46.719774: step 4466, loss 0.595477.
Train: 2018-08-08T22:02:49.794950: step 4467, loss 0.628438.
Train: 2018-08-08T22:02:52.861103: step 4468, loss 0.513138.
Train: 2018-08-08T22:02:55.951319: step 4469, loss 0.463772.
Train: 2018-08-08T22:02:58.969343: step 4470, loss 0.595479.
Test: 2018-08-08T22:03:19.743576: step 4470, loss 0.547682.
Train: 2018-08-08T22:03:22.848832: step 4471, loss 0.546019.
Train: 2018-08-08T22:03:25.911976: step 4472, loss 0.595514.
Train: 2018-08-08T22:03:28.978128: step 4473, loss 0.562502.
Train: 2018-08-08T22:03:32.018210: step 4474, loss 0.529483.
Train: 2018-08-08T22:03:35.079349: step 4475, loss 0.579016.
Train: 2018-08-08T22:03:38.165555: step 4476, loss 0.545973.
Train: 2018-08-08T22:03:41.198619: step 4477, loss 0.595549.
Train: 2018-08-08T22:03:44.247725: step 4478, loss 0.645119.
Train: 2018-08-08T22:03:47.269760: step 4479, loss 0.612006.
Train: 2018-08-08T22:03:50.332904: step 4480, loss 0.447279.
Test: 2018-08-08T22:04:11.126188: step 4480, loss 0.548972.
Train: 2018-08-08T22:04:14.200361: step 4481, loss 0.529606.
Train: 2018-08-08T22:04:17.253479: step 4482, loss 0.546059.
Train: 2018-08-08T22:04:20.395833: step 4483, loss 0.595468.
Train: 2018-08-08T22:04:23.438924: step 4484, loss 0.513104.
Train: 2018-08-08T22:04:26.457951: step 4485, loss 0.595481.
Train: 2018-08-08T22:04:29.498033: step 4486, loss 0.595482.
Train: 2018-08-08T22:04:32.548143: step 4487, loss 0.546045.
Train: 2018-08-08T22:04:35.591234: step 4488, loss 0.661346.
Train: 2018-08-08T22:04:38.652372: step 4489, loss 0.496775.
Train: 2018-08-08T22:04:41.705490: step 4490, loss 0.529683.
Test: 2018-08-08T22:05:02.439617: step 4490, loss 0.548394.
Train: 2018-08-08T22:05:05.499752: step 4491, loss 0.562545.
Train: 2018-08-08T22:05:08.555878: step 4492, loss 0.513262.
Train: 2018-08-08T22:05:11.608995: step 4493, loss 0.595417.
Train: 2018-08-08T22:05:14.658102: step 4494, loss 0.578979.
Train: 2018-08-08T22:05:17.720243: step 4495, loss 0.513234.
Train: 2018-08-08T22:05:20.774363: step 4496, loss 0.480313.
Train: 2018-08-08T22:05:23.805422: step 4497, loss 0.595467.
Train: 2018-08-08T22:05:26.850518: step 4498, loss 0.51304.
Train: 2018-08-08T22:05:29.876564: step 4499, loss 0.562497.
Train: 2018-08-08T22:05:32.945723: step 4500, loss 0.545944.
Test: 2018-08-08T22:05:53.737002: step 4500, loss 0.547578.
Train: 2018-08-08T22:05:58.469584: step 4501, loss 0.661873.
Train: 2018-08-08T22:06:01.524707: step 4502, loss 0.512789.
Train: 2018-08-08T22:06:04.550752: step 4503, loss 0.49619.
Train: 2018-08-08T22:06:07.560755: step 4504, loss 0.662026.
Train: 2018-08-08T22:06:10.614875: step 4505, loss 0.595641.
Train: 2018-08-08T22:06:13.672003: step 4506, loss 0.628767.
Train: 2018-08-08T22:06:16.734145: step 4507, loss 0.579028.
Train: 2018-08-08T22:06:19.794281: step 4508, loss 0.529481.
Train: 2018-08-08T22:06:22.863441: step 4509, loss 0.513048.
Train: 2018-08-08T22:06:25.919566: step 4510, loss 0.513075.
Test: 2018-08-08T22:06:46.660712: step 4510, loss 0.548937.
Train: 2018-08-08T22:06:49.701797: step 4511, loss 0.579.
Train: 2018-08-08T22:06:52.761932: step 4512, loss 0.628458.
Train: 2018-08-08T22:06:55.787978: step 4513, loss 0.463713.
Train: 2018-08-08T22:06:58.848114: step 4514, loss 0.529563.
Train: 2018-08-08T22:07:01.709722: step 4515, loss 0.56251.
Train: 2018-08-08T22:07:04.772866: step 4516, loss 0.496473.
Train: 2018-08-08T22:07:07.841023: step 4517, loss 0.595561.
Train: 2018-08-08T22:07:10.936253: step 4518, loss 0.496268.
Train: 2018-08-08T22:07:13.968314: step 4519, loss 0.645387.
Train: 2018-08-08T22:07:17.041485: step 4520, loss 0.612233.
Test: 2018-08-08T22:07:37.763580: step 4520, loss 0.547558.
Train: 2018-08-08T22:07:40.812687: step 4521, loss 0.59563.
Train: 2018-08-08T22:07:43.853772: step 4522, loss 0.479655.
Train: 2018-08-08T22:07:46.894857: step 4523, loss 0.479628.
Train: 2018-08-08T22:07:49.951986: step 4524, loss 0.51268.
Train: 2018-08-08T22:07:52.983044: step 4525, loss 0.562447.
Train: 2018-08-08T22:07:56.016108: step 4526, loss 0.512462.
Train: 2018-08-08T22:07:59.071231: step 4527, loss 0.495637.
Train: 2018-08-08T22:08:02.099281: step 4528, loss 0.495419.
Train: 2018-08-08T22:08:05.146383: step 4529, loss 0.562385.
Train: 2018-08-08T22:08:08.198497: step 4530, loss 0.562371.
Test: 2018-08-08T22:08:29.002810: step 4530, loss 0.544603.
Train: 2018-08-08T22:08:32.114082: step 4531, loss 0.562361.
Train: 2018-08-08T22:08:35.166198: step 4532, loss 0.494575.
Train: 2018-08-08T22:08:38.229342: step 4533, loss 0.562346.
Train: 2018-08-08T22:08:41.283461: step 4534, loss 0.579379.
Train: 2018-08-08T22:08:44.326552: step 4535, loss 0.528201.
Train: 2018-08-08T22:08:47.405739: step 4536, loss 0.493923.
Train: 2018-08-08T22:08:50.444819: step 4537, loss 0.528038.
Train: 2018-08-08T22:08:53.508965: step 4538, loss 0.665505.
Train: 2018-08-08T22:08:56.543032: step 4539, loss 0.648374.
Train: 2018-08-08T22:08:59.568075: step 4540, loss 0.648309.
Test: 2018-08-08T22:09:20.355343: step 4540, loss 0.549502.
Train: 2018-08-08T22:09:23.409463: step 4541, loss 0.562335.
Train: 2018-08-08T22:09:26.492660: step 4542, loss 0.596581.
Train: 2018-08-08T22:09:29.505671: step 4543, loss 0.494021.
Train: 2018-08-08T22:09:32.555780: step 4544, loss 0.579394.
Train: 2018-08-08T22:09:35.616919: step 4545, loss 0.545317.
Train: 2018-08-08T22:09:38.685076: step 4546, loss 0.579346.
Train: 2018-08-08T22:09:41.751228: step 4547, loss 0.562349.
Train: 2018-08-08T22:09:44.837434: step 4548, loss 0.579298.
Train: 2018-08-08T22:09:47.895564: step 4549, loss 0.56236.
Train: 2018-08-08T22:09:50.946677: step 4550, loss 0.528605.
Test: 2018-08-08T22:10:11.723917: step 4550, loss 0.545299.
Train: 2018-08-08T22:10:14.757984: step 4551, loss 0.629809.
Train: 2018-08-08T22:10:17.815112: step 4552, loss 0.596023.
Train: 2018-08-08T22:10:20.868230: step 4553, loss 0.528841.
Train: 2018-08-08T22:10:23.925358: step 4554, loss 0.612632.
Train: 2018-08-08T22:10:26.938369: step 4555, loss 0.529023.
Train: 2018-08-08T22:10:29.973438: step 4556, loss 0.595762.
Train: 2018-08-08T22:10:33.011515: step 4557, loss 0.562448.
Train: 2018-08-08T22:10:36.071651: step 4558, loss 0.562464.
Train: 2018-08-08T22:10:39.129782: step 4559, loss 0.512814.
Train: 2018-08-08T22:10:42.153822: step 4560, loss 0.479806.
Test: 2018-08-08T22:11:02.903992: step 4560, loss 0.548865.
Train: 2018-08-08T22:11:05.997215: step 4561, loss 0.645188.
Train: 2018-08-08T22:11:09.075399: step 4562, loss 0.562495.
Train: 2018-08-08T22:11:12.134533: step 4563, loss 0.62852.
Train: 2018-08-08T22:11:15.190658: step 4564, loss 0.677813.
Train: 2018-08-08T22:11:18.224724: step 4565, loss 0.546148.
Train: 2018-08-08T22:11:21.244754: step 4566, loss 0.611661.
Train: 2018-08-08T22:11:24.291855: step 4567, loss 0.562626.
Train: 2018-08-08T22:11:27.352994: step 4568, loss 0.562663.
Train: 2018-08-08T22:11:30.396085: step 4569, loss 0.578896.
Train: 2018-08-08T22:11:33.443186: step 4570, loss 0.675793.
Test: 2018-08-08T22:11:54.190348: step 4570, loss 0.548943.
Train: 2018-08-08T22:11:57.273545: step 4571, loss 0.530634.
Train: 2018-08-08T22:12:00.333681: step 4572, loss 0.482718.
Train: 2018-08-08T22:12:03.364740: step 4573, loss 0.578863.
Train: 2018-08-08T22:12:06.409836: step 4574, loss 0.562891.
Train: 2018-08-08T22:12:09.450921: step 4575, loss 0.546967.
Train: 2018-08-08T22:12:12.496017: step 4576, loss 0.674444.
Train: 2018-08-08T22:12:15.521060: step 4577, loss 0.626523.
Train: 2018-08-08T22:12:18.566156: step 4578, loss 0.626361.
Train: 2018-08-08T22:12:21.603230: step 4579, loss 0.594634.
Train: 2018-08-08T22:12:24.618247: step 4580, loss 0.578872.
Test: 2018-08-08T22:12:45.363403: step 4580, loss 0.550354.
Train: 2018-08-08T22:12:48.406493: step 4581, loss 0.578882.
Train: 2018-08-08T22:12:51.463621: step 4582, loss 0.594483.
Train: 2018-08-08T22:12:54.537794: step 4583, loss 0.57891.
Train: 2018-08-08T22:12:57.601941: step 4584, loss 0.547967.
Train: 2018-08-08T22:13:00.665085: step 4585, loss 0.501743.
Train: 2018-08-08T22:13:03.729232: step 4586, loss 0.656065.
Train: 2018-08-08T22:13:06.808419: step 4587, loss 0.563573.
Train: 2018-08-08T22:13:09.878581: step 4588, loss 0.532899.
Train: 2018-08-08T22:13:12.918664: step 4589, loss 0.517607.
Train: 2018-08-08T22:13:15.962757: step 4590, loss 0.4869.
Test: 2018-08-08T22:13:36.703903: step 4590, loss 0.552092.
Train: 2018-08-08T22:13:39.799132: step 4591, loss 0.594339.
Train: 2018-08-08T22:13:42.904388: step 4592, loss 0.609745.
Train: 2018-08-08T22:13:45.966529: step 4593, loss 0.609759.
Train: 2018-08-08T22:13:49.025663: step 4594, loss 0.517353.
Train: 2018-08-08T22:13:52.065745: step 4595, loss 0.455631.
Train: 2018-08-08T22:13:55.105828: step 4596, loss 0.501627.
Train: 2018-08-08T22:13:58.152929: step 4597, loss 0.67205.
Train: 2018-08-08T22:14:01.194015: step 4598, loss 0.547802.
Train: 2018-08-08T22:14:04.250140: step 4599, loss 0.469809.
Train: 2018-08-08T22:14:07.304260: step 4600, loss 0.5476.
Test: 2018-08-08T22:14:28.083507: step 4600, loss 0.552024.
Train: 2018-08-08T22:14:32.964484: step 4601, loss 0.610273.
Train: 2018-08-08T22:14:36.001559: step 4602, loss 0.594609.
Train: 2018-08-08T22:14:39.033620: step 4603, loss 0.547317.
Train: 2018-08-08T22:14:42.062673: step 4604, loss 0.547245.
Train: 2018-08-08T22:14:45.102756: step 4605, loss 0.499633.
Train: 2018-08-08T22:14:48.138828: step 4606, loss 0.562961.
Train: 2018-08-08T22:14:51.159860: step 4607, loss 0.626698.
Train: 2018-08-08T22:14:54.230023: step 4608, loss 0.594835.
Train: 2018-08-08T22:14:57.247044: step 4609, loss 0.690802.
Train: 2018-08-08T22:15:00.313197: step 4610, loss 0.467044.
Test: 2018-08-08T22:15:21.063366: step 4610, loss 0.549713.
Train: 2018-08-08T22:15:24.144558: step 4611, loss 0.466966.
Train: 2018-08-08T22:15:27.214720: step 4612, loss 0.578865.
Train: 2018-08-08T22:15:30.275859: step 4613, loss 0.562817.
Train: 2018-08-08T22:15:33.317952: step 4614, loss 0.627113.
Train: 2018-08-08T22:15:36.336974: step 4615, loss 0.466251.
Train: 2018-08-08T22:15:39.405131: step 4616, loss 0.482139.
Train: 2018-08-08T22:15:42.512393: step 4617, loss 0.562715.
Train: 2018-08-08T22:15:45.585563: step 4618, loss 0.497779.
Train: 2018-08-08T22:15:48.646702: step 4619, loss 0.595207.
Train: 2018-08-08T22:15:51.694806: step 4620, loss 0.595268.
Test: 2018-08-08T22:16:12.457007: step 4620, loss 0.547249.
Train: 2018-08-08T22:16:15.527170: step 4621, loss 0.546218.
Train: 2018-08-08T22:16:18.580287: step 4622, loss 0.628159.
Train: 2018-08-08T22:16:21.630396: step 4623, loss 0.628203.
Train: 2018-08-08T22:16:24.704570: step 4624, loss 0.628183.
Train: 2018-08-08T22:16:27.761698: step 4625, loss 0.513422.
Train: 2018-08-08T22:16:30.840885: step 4626, loss 0.546202.
Train: 2018-08-08T22:16:33.847879: step 4627, loss 0.578951.
Train: 2018-08-08T22:16:36.874927: step 4628, loss 0.611678.
Train: 2018-08-08T22:16:39.907992: step 4629, loss 0.578941.
Train: 2018-08-08T22:16:42.946069: step 4630, loss 0.529964.
Test: 2018-08-08T22:17:03.675182: step 4630, loss 0.547334.
Train: 2018-08-08T22:17:06.727296: step 4631, loss 0.595241.
Train: 2018-08-08T22:17:09.773395: step 4632, loss 0.692972.
Train: 2018-08-08T22:17:12.842555: step 4633, loss 0.513942.
Train: 2018-08-08T22:17:15.875620: step 4634, loss 0.514065.
Train: 2018-08-08T22:17:18.933750: step 4635, loss 0.514127.
Train: 2018-08-08T22:17:21.994889: step 4636, loss 0.676037.
Train: 2018-08-08T22:17:25.052017: step 4637, loss 0.595047.
Train: 2018-08-08T22:17:28.118169: step 4638, loss 0.595003.
Train: 2018-08-08T22:17:31.148225: step 4639, loss 0.56279.
Train: 2018-08-08T22:17:34.208361: step 4640, loss 0.594916.
Test: 2018-08-08T22:17:54.965550: step 4640, loss 0.549673.
Train: 2018-08-08T22:17:58.060778: step 4641, loss 0.562856.
Train: 2018-08-08T22:18:01.113896: step 4642, loss 0.514968.
Train: 2018-08-08T22:18:04.183056: step 4643, loss 0.515036.
Train: 2018-08-08T22:18:07.248205: step 4644, loss 0.530995.
Train: 2018-08-08T22:18:10.294304: step 4645, loss 0.530969.
Train: 2018-08-08T22:18:13.331378: step 4646, loss 0.610825.
Train: 2018-08-08T22:18:16.389509: step 4647, loss 0.530905.
Train: 2018-08-08T22:18:19.441624: step 4648, loss 0.498866.
Train: 2018-08-08T22:18:22.479701: step 4649, loss 0.546806.
Train: 2018-08-08T22:18:25.518781: step 4650, loss 0.46643.
Test: 2018-08-08T22:18:46.318081: step 4650, loss 0.550095.
Train: 2018-08-08T22:18:49.409300: step 4651, loss 0.498284.
Train: 2018-08-08T22:18:52.467430: step 4652, loss 0.611269.
Train: 2018-08-08T22:18:55.509518: step 4653, loss 0.56267.
Train: 2018-08-08T22:18:58.539574: step 4654, loss 0.497512.
Train: 2018-08-08T22:19:01.622772: step 4655, loss 0.546261.
Train: 2018-08-08T22:19:04.686919: step 4656, loss 0.595355.
Train: 2018-08-08T22:19:07.763097: step 4657, loss 0.546105.
Train: 2018-08-08T22:19:10.807191: step 4658, loss 0.513086.
Train: 2018-08-08T22:19:13.902420: step 4659, loss 0.595546.
Train: 2018-08-08T22:19:16.943505: step 4660, loss 0.529353.
Test: 2018-08-08T22:19:37.688662: step 4660, loss 0.548159.
Train: 2018-08-08T22:19:40.751805: step 4661, loss 0.612261.
Train: 2018-08-08T22:19:43.828986: step 4662, loss 0.529204.
Train: 2018-08-08T22:19:46.904163: step 4663, loss 0.579087.
Train: 2018-08-08T22:19:49.965301: step 4664, loss 0.562431.
Train: 2018-08-08T22:19:53.019421: step 4665, loss 0.495693.
Train: 2018-08-08T22:19:56.064518: step 4666, loss 0.579127.
Train: 2018-08-08T22:19:59.095576: step 4667, loss 0.629342.
Train: 2018-08-08T22:20:02.185792: step 4668, loss 0.562407.
Train: 2018-08-08T22:20:05.229885: step 4669, loss 0.528942.
Train: 2018-08-08T22:20:08.265958: step 4670, loss 0.595882.
Test: 2018-08-08T22:20:28.993066: step 4670, loss 0.549259.
Train: 2018-08-08T22:20:32.048188: step 4671, loss 0.52894.
Train: 2018-08-08T22:20:35.098297: step 4672, loss 0.646088.
Train: 2018-08-08T22:20:38.136375: step 4673, loss 0.478835.
Train: 2018-08-08T22:20:41.164426: step 4674, loss 0.495544.
Train: 2018-08-08T22:20:44.237596: step 4675, loss 0.512204.
Train: 2018-08-08T22:20:47.293722: step 4676, loss 0.579159.
Train: 2018-08-08T22:20:50.328791: step 4677, loss 0.545613.
Train: 2018-08-08T22:20:53.390932: step 4678, loss 0.646381.
Train: 2018-08-08T22:20:56.451069: step 4679, loss 0.595974.
Train: 2018-08-08T22:20:59.545295: step 4680, loss 0.545617.
Test: 2018-08-08T22:21:20.295464: step 4680, loss 0.550492.
Train: 2018-08-08T22:21:23.399718: step 4681, loss 0.478571.
Train: 2018-08-08T22:21:26.451832: step 4682, loss 0.579169.
Train: 2018-08-08T22:21:29.516982: step 4683, loss 0.562393.
Train: 2018-08-08T22:21:32.553054: step 4684, loss 0.528838.
Train: 2018-08-08T22:21:35.585115: step 4685, loss 0.579176.
Train: 2018-08-08T22:21:38.611161: step 4686, loss 0.595963.
Train: 2018-08-08T22:21:41.653249: step 4687, loss 0.528841.
Train: 2018-08-08T22:21:44.715390: step 4688, loss 0.51207.
Train: 2018-08-08T22:21:47.763494: step 4689, loss 0.579175.
Train: 2018-08-08T22:21:50.809593: step 4690, loss 0.629539.
Test: 2018-08-08T22:22:11.564776: step 4690, loss 0.549218.
Train: 2018-08-08T22:22:14.616890: step 4691, loss 0.595936.
Train: 2018-08-08T22:22:17.761250: step 4692, loss 0.545658.
Train: 2018-08-08T22:22:20.810356: step 4693, loss 0.579135.
Train: 2018-08-08T22:22:23.891549: step 4694, loss 0.57912.
Train: 2018-08-08T22:22:26.905562: step 4695, loss 0.629125.
Train: 2018-08-08T22:22:29.973719: step 4696, loss 0.612338.
Train: 2018-08-08T22:22:33.016810: step 4697, loss 0.562469.
Train: 2018-08-08T22:22:36.068925: step 4698, loss 0.512905.
Train: 2018-08-08T22:22:39.132069: step 4699, loss 0.628497.
Train: 2018-08-08T22:22:42.198221: step 4700, loss 0.546082.
Test: 2018-08-08T22:23:02.954407: step 4700, loss 0.545935.
Train: 2018-08-08T22:23:07.862455: step 4701, loss 0.513317.
Train: 2018-08-08T22:23:10.898527: step 4702, loss 0.529785.
Train: 2018-08-08T22:23:13.952647: step 4703, loss 0.497054.
Train: 2018-08-08T22:23:17.035845: step 4704, loss 0.595344.
Train: 2018-08-08T22:23:20.091970: step 4705, loss 0.726418.
Train: 2018-08-08T22:23:23.127039: step 4706, loss 0.513584.
Train: 2018-08-08T22:23:26.155090: step 4707, loss 0.546308.
Train: 2018-08-08T22:23:29.215226: step 4708, loss 0.644078.
Train: 2018-08-08T22:23:32.243277: step 4709, loss 0.530165.
Train: 2018-08-08T22:23:35.289376: step 4710, loss 0.465359.
Test: 2018-08-08T22:23:55.992420: step 4710, loss 0.548095.
Train: 2018-08-08T22:23:59.092662: step 4711, loss 0.578902.
Train: 2018-08-08T22:24:02.147785: step 4712, loss 0.595121.
Train: 2018-08-08T22:24:05.212934: step 4713, loss 0.416818.
Train: 2018-08-08T22:24:08.284100: step 4714, loss 0.578907.
Train: 2018-08-08T22:24:11.344235: step 4715, loss 0.595173.
Train: 2018-08-08T22:24:14.399358: step 4716, loss 0.497557.
Train: 2018-08-08T22:24:17.415377: step 4717, loss 0.513725.
Train: 2018-08-08T22:24:20.516623: step 4718, loss 0.5626.
Train: 2018-08-08T22:24:23.560716: step 4719, loss 0.578951.
Train: 2018-08-08T22:24:26.614836: step 4720, loss 0.578962.
Test: 2018-08-08T22:24:47.348963: step 4720, loss 0.547789.
Train: 2018-08-08T22:24:50.405088: step 4721, loss 0.546133.
Train: 2018-08-08T22:24:53.472242: step 4722, loss 0.447463.
Train: 2018-08-08T22:24:56.510320: step 4723, loss 0.611977.
Train: 2018-08-08T22:24:59.559426: step 4724, loss 0.562497.
Train: 2018-08-08T22:25:02.592490: step 4725, loss 0.595574.
Train: 2018-08-08T22:25:05.641597: step 4726, loss 0.545917.
Train: 2018-08-08T22:25:08.677669: step 4727, loss 0.579045.
Train: 2018-08-08T22:25:11.759864: step 4728, loss 0.595636.
Train: 2018-08-08T22:25:14.817995: step 4729, loss 0.545881.
Train: 2018-08-08T22:25:17.865096: step 4730, loss 0.579051.
Test: 2018-08-08T22:25:38.616269: step 4730, loss 0.548182.
Train: 2018-08-08T22:25:41.682422: step 4731, loss 0.645382.
Train: 2018-08-08T22:25:44.758598: step 4732, loss 0.661823.
Train: 2018-08-08T22:25:47.823748: step 4733, loss 0.479965.
Train: 2018-08-08T22:25:50.880876: step 4734, loss 0.644932.
Train: 2018-08-08T22:25:53.925972: step 4735, loss 0.463903.
Train: 2018-08-08T22:25:56.971068: step 4736, loss 0.611823.
Train: 2018-08-08T22:26:00.000121: step 4737, loss 0.398569.
Train: 2018-08-08T22:26:03.077303: step 4738, loss 0.693902.
Train: 2018-08-08T22:26:06.145460: step 4739, loss 0.611766.
Train: 2018-08-08T22:26:09.225650: step 4740, loss 0.611697.
Test: 2018-08-08T22:26:30.006902: step 4740, loss 0.547917.
Train: 2018-08-08T22:26:33.083080: step 4741, loss 0.578937.
Train: 2018-08-08T22:26:36.169285: step 4742, loss 0.497455.
Train: 2018-08-08T22:26:39.214381: step 4743, loss 0.595193.
Train: 2018-08-08T22:26:42.287552: step 4744, loss 0.611411.
Train: 2018-08-08T22:26:45.358717: step 4745, loss 0.514041.
Train: 2018-08-08T22:26:48.388774: step 4746, loss 0.481718.
Train: 2018-08-08T22:26:51.414819: step 4747, loss 0.51409.
Train: 2018-08-08T22:26:54.461920: step 4748, loss 0.643781.
Train: 2018-08-08T22:26:57.535091: step 4749, loss 0.562686.
Train: 2018-08-08T22:27:00.572166: step 4750, loss 0.546478.
Test: 2018-08-08T22:27:21.354421: step 4750, loss 0.548111.
Train: 2018-08-08T22:27:24.426588: step 4751, loss 0.432994.
Train: 2018-08-08T22:27:27.492740: step 4752, loss 0.51392.
Train: 2018-08-08T22:27:30.530818: step 4753, loss 0.595216.
Train: 2018-08-08T22:27:33.592959: step 4754, loss 0.595259.
Train: 2018-08-08T22:27:36.610983: step 4755, loss 0.562596.
Train: 2018-08-08T22:27:39.638031: step 4756, loss 0.611675.
Train: 2018-08-08T22:27:42.695159: step 4757, loss 0.562582.
Train: 2018-08-08T22:27:45.759306: step 4758, loss 0.562581.
Train: 2018-08-08T22:27:48.799389: step 4759, loss 0.660808.
Train: 2018-08-08T22:27:51.836464: step 4760, loss 0.644331.
Test: 2018-08-08T22:28:12.616713: step 4760, loss 0.54858.
Train: 2018-08-08T22:28:15.677851: step 4761, loss 0.676749.
Train: 2018-08-08T22:28:18.725955: step 4762, loss 0.578906.
Train: 2018-08-08T22:28:21.793110: step 4763, loss 0.514217.
Train: 2018-08-08T22:28:24.854248: step 4764, loss 0.562758.
Train: 2018-08-08T22:28:27.870267: step 4765, loss 0.562792.
Train: 2018-08-08T22:28:30.907342: step 4766, loss 0.482601.
Train: 2018-08-08T22:28:33.981515: step 4767, loss 0.48266.
Train: 2018-08-08T22:28:37.056692: step 4768, loss 0.418388.
Train: 2018-08-08T22:28:40.115825: step 4769, loss 0.611076.
Train: 2018-08-08T22:28:43.177966: step 4770, loss 0.498211.
Test: 2018-08-08T22:29:03.952200: step 4770, loss 0.550606.
Train: 2018-08-08T22:29:07.014340: step 4771, loss 0.497977.
Train: 2018-08-08T22:29:10.063447: step 4772, loss 0.530174.
Train: 2018-08-08T22:29:13.138623: step 4773, loss 0.530001.
Train: 2018-08-08T22:29:16.168679: step 4774, loss 0.611704.
Train: 2018-08-08T22:29:19.237839: step 4775, loss 0.529704.
Train: 2018-08-08T22:29:22.320034: step 4776, loss 0.611939.
Train: 2018-08-08T22:29:25.390197: step 4777, loss 0.529495.
Train: 2018-08-08T22:29:28.458355: step 4778, loss 0.628649.
Train: 2018-08-08T22:29:31.488410: step 4779, loss 0.545923.
Train: 2018-08-08T22:29:34.546541: step 4780, loss 0.595616.
Test: 2018-08-08T22:29:55.305735: step 4780, loss 0.548189.
Train: 2018-08-08T22:29:58.352836: step 4781, loss 0.579046.
Train: 2018-08-08T22:30:01.408961: step 4782, loss 0.579046.
Train: 2018-08-08T22:30:04.466089: step 4783, loss 0.545899.
Train: 2018-08-08T22:30:07.521212: step 4784, loss 0.579042.
Train: 2018-08-08T22:30:10.586361: step 4785, loss 0.579039.
Train: 2018-08-08T22:30:13.660534: step 4786, loss 0.56248.
Train: 2018-08-08T22:30:16.713652: step 4787, loss 0.612109.
Train: 2018-08-08T22:30:19.758748: step 4788, loss 0.645085.
Train: 2018-08-08T22:30:22.795822: step 4789, loss 0.595468.
Train: 2018-08-08T22:30:25.858967: step 4790, loss 0.529697.
Test: 2018-08-08T22:30:46.633200: step 4790, loss 0.549071.
Train: 2018-08-08T22:30:49.713389: step 4791, loss 0.546179.
Train: 2018-08-08T22:30:52.787562: step 4792, loss 0.595309.
Train: 2018-08-08T22:30:55.836669: step 4793, loss 0.578935.
Train: 2018-08-08T22:30:58.904826: step 4794, loss 0.497461.
Train: 2018-08-08T22:31:01.969976: step 4795, loss 0.595197.
Train: 2018-08-08T22:31:05.027104: step 4796, loss 0.497627.
Train: 2018-08-08T22:31:08.055155: step 4797, loss 0.546401.
Train: 2018-08-08T22:31:11.144368: step 4798, loss 0.611429.
Train: 2018-08-08T22:31:14.170413: step 4799, loss 0.562661.
Train: 2018-08-08T22:31:17.191446: step 4800, loss 0.611386.
Test: 2018-08-08T22:31:37.982724: step 4800, loss 0.54994.
Train: 2018-08-08T22:31:42.796522: step 4801, loss 0.578902.
Train: 2018-08-08T22:31:45.804520: step 4802, loss 0.578896.
Train: 2018-08-08T22:31:48.855632: step 4803, loss 0.627405.
Train: 2018-08-08T22:31:51.917773: step 4804, loss 0.578882.
Train: 2018-08-08T22:31:54.938805: step 4805, loss 0.530593.
Train: 2018-08-08T22:31:57.973875: step 4806, loss 0.498528.
Train: 2018-08-08T22:32:00.993904: step 4807, loss 0.482486.
Train: 2018-08-08T22:32:04.056046: step 4808, loss 0.49846.
Train: 2018-08-08T22:32:07.109163: step 4809, loss 0.530527.
Train: 2018-08-08T22:32:10.150248: step 4810, loss 0.46578.
Test: 2018-08-08T22:32:30.922477: step 4810, loss 0.546869.
Train: 2018-08-08T22:32:33.969577: step 4811, loss 0.530238.
Train: 2018-08-08T22:32:37.005649: step 4812, loss 0.627783.
Train: 2018-08-08T22:32:40.047738: step 4813, loss 0.546277.
Train: 2018-08-08T22:32:43.089826: step 4814, loss 0.497095.
Train: 2018-08-08T22:32:46.101834: step 4815, loss 0.529694.
Train: 2018-08-08T22:32:48.971463: step 4816, loss 0.562515.
Train: 2018-08-08T22:32:52.032602: step 4817, loss 0.678236.
Train: 2018-08-08T22:32:55.114797: step 4818, loss 0.612132.
Train: 2018-08-08T22:32:58.166911: step 4819, loss 0.595581.
Train: 2018-08-08T22:33:01.227047: step 4820, loss 0.545946.
Test: 2018-08-08T22:33:22.038380: step 4820, loss 0.550123.
Train: 2018-08-08T22:33:25.090494: step 4821, loss 0.562489.
Train: 2018-08-08T22:33:28.115536: step 4822, loss 0.562492.
Train: 2018-08-08T22:33:31.160632: step 4823, loss 0.595541.
Train: 2018-08-08T22:33:34.233803: step 4824, loss 0.545994.
Train: 2018-08-08T22:33:37.313993: step 4825, loss 0.579006.
Train: 2018-08-08T22:33:40.452337: step 4826, loss 0.595484.
Train: 2018-08-08T22:33:43.484398: step 4827, loss 0.546063.
Train: 2018-08-08T22:33:46.518465: step 4828, loss 0.562535.
Train: 2018-08-08T22:33:49.579604: step 4829, loss 0.661139.
Train: 2018-08-08T22:33:52.620689: step 4830, loss 0.562566.
Test: 2018-08-08T22:34:13.428010: step 4830, loss 0.550355.
Train: 2018-08-08T22:34:16.524242: step 4831, loss 0.611659.
Train: 2018-08-08T22:34:19.576357: step 4832, loss 0.546307.
Train: 2018-08-08T22:34:22.638498: step 4833, loss 0.595191.
Train: 2018-08-08T22:34:25.683594: step 4834, loss 0.61137.
Train: 2018-08-08T22:34:28.737714: step 4835, loss 0.595074.
Train: 2018-08-08T22:34:31.783813: step 4836, loss 0.595009.
Train: 2018-08-08T22:34:34.842946: step 4837, loss 0.530656.
Train: 2018-08-08T22:34:37.913109: step 4838, loss 0.546805.
Train: 2018-08-08T22:34:40.969235: step 4839, loss 0.594862.
Train: 2018-08-08T22:34:44.043408: step 4840, loss 0.546935.
Test: 2018-08-08T22:35:04.826665: step 4840, loss 0.551005.
Train: 2018-08-08T22:35:07.891814: step 4841, loss 0.562923.
Train: 2018-08-08T22:35:10.999075: step 4842, loss 0.515207.
Train: 2018-08-08T22:35:14.064225: step 4843, loss 0.578858.
Train: 2018-08-08T22:35:17.097289: step 4844, loss 0.547061.
Train: 2018-08-08T22:35:20.184497: step 4845, loss 0.547065.
Train: 2018-08-08T22:35:23.263683: step 4846, loss 0.59476.
Train: 2018-08-08T22:35:26.317804: step 4847, loss 0.515267.
Train: 2018-08-08T22:35:29.391977: step 4848, loss 0.49931.
Train: 2018-08-08T22:35:32.441084: step 4849, loss 0.594799.
Train: 2018-08-08T22:35:35.478158: step 4850, loss 0.57886.
Test: 2018-08-08T22:35:56.308541: step 4850, loss 0.547923.
Train: 2018-08-08T22:35:59.361658: step 4851, loss 0.546917.
Train: 2018-08-08T22:36:02.428813: step 4852, loss 0.674806.
Train: 2018-08-08T22:36:05.472906: step 4853, loss 0.578862.
Train: 2018-08-08T22:36:08.534045: step 4854, loss 0.499047.
Train: 2018-08-08T22:36:11.606213: step 4855, loss 0.530957.
Train: 2018-08-08T22:36:14.653314: step 4856, loss 0.51493.
Train: 2018-08-08T22:36:17.716458: step 4857, loss 0.562854.
Train: 2018-08-08T22:36:20.754536: step 4858, loss 0.514721.
Train: 2018-08-08T22:36:23.836730: step 4859, loss 0.61102.
Train: 2018-08-08T22:36:26.921933: step 4860, loss 0.578875.
Test: 2018-08-08T22:36:47.723238: step 4860, loss 0.550108.
Train: 2018-08-08T22:36:50.816462: step 4861, loss 0.498321.
Train: 2018-08-08T22:36:53.880609: step 4862, loss 0.659603.
Train: 2018-08-08T22:36:56.927710: step 4863, loss 0.595032.
Train: 2018-08-08T22:36:59.980828: step 4864, loss 0.659595.
Train: 2018-08-08T22:37:03.030937: step 4865, loss 0.498318.
Train: 2018-08-08T22:37:06.039937: step 4866, loss 0.546671.
Train: 2018-08-08T22:37:09.110100: step 4867, loss 0.530576.
Train: 2018-08-08T22:37:12.174246: step 4868, loss 0.56277.
Train: 2018-08-08T22:37:15.214329: step 4869, loss 0.514423.
Train: 2018-08-08T22:37:18.289505: step 4870, loss 0.61115.
Test: 2018-08-08T22:37:39.101840: step 4870, loss 0.551283.
Train: 2018-08-08T22:37:42.145933: step 4871, loss 0.562744.
Train: 2018-08-08T22:37:45.169973: step 4872, loss 0.514304.
Train: 2018-08-08T22:37:48.187997: step 4873, loss 0.659708.
Train: 2018-08-08T22:37:51.222064: step 4874, loss 0.546576.
Train: 2018-08-08T22:37:54.293229: step 4875, loss 0.643494.
Train: 2018-08-08T22:37:57.329301: step 4876, loss 0.514369.
Train: 2018-08-08T22:38:00.376403: step 4877, loss 0.562759.
Train: 2018-08-08T22:38:03.441552: step 4878, loss 0.530537.
Train: 2018-08-08T22:38:06.489656: step 4879, loss 0.546644.
Train: 2018-08-08T22:38:09.538763: step 4880, loss 0.530503.
Test: 2018-08-08T22:38:30.348090: step 4880, loss 0.548836.
Train: 2018-08-08T22:38:33.395191: step 4881, loss 0.562741.
Train: 2018-08-08T22:38:36.441290: step 4882, loss 0.54657.
Train: 2018-08-08T22:38:39.483377: step 4883, loss 0.59507.
Train: 2018-08-08T22:38:42.553540: step 4884, loss 0.530332.
Train: 2018-08-08T22:38:45.627713: step 4885, loss 0.546489.
Train: 2018-08-08T22:38:48.666794: step 4886, loss 0.578903.
Train: 2018-08-08T22:38:51.704871: step 4887, loss 0.595147.
Train: 2018-08-08T22:38:54.769018: step 4888, loss 0.49769.
Train: 2018-08-08T22:38:57.821133: step 4889, loss 0.465056.
Train: 2018-08-08T22:39:00.877258: step 4890, loss 0.546306.
Test: 2018-08-08T22:39:21.705635: step 4890, loss 0.549117.
Train: 2018-08-08T22:39:24.795851: step 4891, loss 0.497158.
Train: 2018-08-08T22:39:27.891080: step 4892, loss 0.496887.
Train: 2018-08-08T22:39:30.921136: step 4893, loss 0.611973.
Train: 2018-08-08T22:39:33.953198: step 4894, loss 0.529417.
Train: 2018-08-08T22:39:36.965206: step 4895, loss 0.628815.
Train: 2018-08-08T22:39:40.014312: step 4896, loss 0.529219.
Train: 2018-08-08T22:39:43.087483: step 4897, loss 0.562438.
Train: 2018-08-08T22:39:46.118542: step 4898, loss 0.612461.
Train: 2018-08-08T22:39:49.172662: step 4899, loss 0.495664.
Train: 2018-08-08T22:39:52.209737: step 4900, loss 0.495549.
Test: 2018-08-08T22:40:13.007031: step 4900, loss 0.548601.
Train: 2018-08-08T22:40:17.841885: step 4901, loss 0.646179.
Train: 2018-08-08T22:40:20.863920: step 4902, loss 0.6127.
Train: 2018-08-08T22:40:23.898990: step 4903, loss 0.562397.
Train: 2018-08-08T22:40:26.969152: step 4904, loss 0.512122.
Train: 2018-08-08T22:40:30.033299: step 4905, loss 0.629461.
Train: 2018-08-08T22:40:33.066363: step 4906, loss 0.629413.
Train: 2018-08-08T22:40:36.113465: step 4907, loss 0.495523.
Train: 2018-08-08T22:40:39.145526: step 4908, loss 0.512284.
Train: 2018-08-08T22:40:42.179593: step 4909, loss 0.562415.
Train: 2018-08-08T22:40:45.225691: step 4910, loss 0.562415.
Test: 2018-08-08T22:41:06.041034: step 4910, loss 0.548655.
Train: 2018-08-08T22:41:09.106183: step 4911, loss 0.528997.
Train: 2018-08-08T22:41:12.188378: step 4912, loss 0.645988.
Train: 2018-08-08T22:41:15.260546: step 4913, loss 0.545723.
Train: 2018-08-08T22:41:18.316671: step 4914, loss 0.512376.
Train: 2018-08-08T22:41:21.394855: step 4915, loss 0.579108.
Train: 2018-08-08T22:41:24.473039: step 4916, loss 0.59578.
Train: 2018-08-08T22:41:27.552226: step 4917, loss 0.529112.
Train: 2018-08-08T22:41:30.667509: step 4918, loss 0.545782.
Train: 2018-08-08T22:41:33.702578: step 4919, loss 0.512482.
Train: 2018-08-08T22:41:36.769733: step 4920, loss 0.629078.
Test: 2018-08-08T22:41:57.585076: step 4920, loss 0.549355.
Train: 2018-08-08T22:42:00.665265: step 4921, loss 0.462533.
Train: 2018-08-08T22:42:03.751470: step 4922, loss 0.579098.
Train: 2018-08-08T22:42:06.809601: step 4923, loss 0.595778.
Train: 2018-08-08T22:42:09.898814: step 4924, loss 0.445719.
Train: 2018-08-08T22:42:12.960955: step 4925, loss 0.529018.
Train: 2018-08-08T22:42:16.029113: step 4926, loss 0.495479.
Train: 2018-08-08T22:42:19.066187: step 4927, loss 0.595948.
Train: 2018-08-08T22:42:22.122313: step 4928, loss 0.461542.
Train: 2018-08-08T22:42:25.183451: step 4929, loss 0.545513.
Train: 2018-08-08T22:42:28.213508: step 4930, loss 0.646898.
Test: 2018-08-08T22:42:49.037875: step 4930, loss 0.545856.
Train: 2018-08-08T22:42:52.059909: step 4931, loss 0.579284.
Train: 2018-08-08T22:42:55.131074: step 4932, loss 0.579293.
Train: 2018-08-08T22:42:58.168149: step 4933, loss 0.596237.
Train: 2018-08-08T22:43:01.213245: step 4934, loss 0.494622.
Train: 2018-08-08T22:43:04.268367: step 4935, loss 0.579296.
Train: 2018-08-08T22:43:07.326498: step 4936, loss 0.562354.
Train: 2018-08-08T22:43:10.413706: step 4937, loss 0.528474.
Train: 2018-08-08T22:43:13.501917: step 4938, loss 0.494574.
Train: 2018-08-08T22:43:16.563056: step 4939, loss 0.562351.
Train: 2018-08-08T22:43:19.602136: step 4940, loss 0.579328.
Test: 2018-08-08T22:43:40.395420: step 4940, loss 0.550285.
Train: 2018-08-08T22:43:43.454553: step 4941, loss 0.54536.
Train: 2018-08-08T22:43:46.519702: step 4942, loss 0.579342.
Train: 2018-08-08T22:43:49.574825: step 4943, loss 0.579342.
Train: 2018-08-08T22:43:52.636966: step 4944, loss 0.4774.
Train: 2018-08-08T22:43:55.685070: step 4945, loss 0.511335.
Train: 2018-08-08T22:43:58.712118: step 4946, loss 0.545316.
Train: 2018-08-08T22:44:01.756212: step 4947, loss 0.511193.
Train: 2018-08-08T22:44:04.835398: step 4948, loss 0.596498.
Train: 2018-08-08T22:44:07.907566: step 4949, loss 0.528144.
Train: 2018-08-08T22:44:10.983745: step 4950, loss 0.665039.
Test: 2018-08-08T22:44:31.812122: step 4950, loss 0.546955.
Train: 2018-08-08T22:44:34.903341: step 4951, loss 0.647868.
Train: 2018-08-08T22:44:37.974506: step 4952, loss 0.545267.
Train: 2018-08-08T22:44:41.023613: step 4953, loss 0.647549.
Train: 2018-08-08T22:44:44.077733: step 4954, loss 0.613317.
Train: 2018-08-08T22:44:47.130850: step 4955, loss 0.748562.
Train: 2018-08-08T22:44:50.174944: step 4956, loss 0.511917.
Train: 2018-08-08T22:44:53.211016: step 4957, loss 0.478721.
Train: 2018-08-08T22:44:56.256112: step 4958, loss 0.645832.
Train: 2018-08-08T22:44:59.314243: step 4959, loss 0.529239.
Train: 2018-08-08T22:45:02.382400: step 4960, loss 0.562482.
Test: 2018-08-08T22:45:23.208772: step 4960, loss 0.551418.
Train: 2018-08-08T22:45:26.264897: step 4961, loss 0.546012.
Train: 2018-08-08T22:45:29.340073: step 4962, loss 0.54608.
Train: 2018-08-08T22:45:32.392188: step 4963, loss 0.611801.
Train: 2018-08-08T22:45:35.421241: step 4964, loss 0.56258.
Train: 2018-08-08T22:45:38.501431: step 4965, loss 0.595263.
Train: 2018-08-08T22:45:41.544521: step 4966, loss 0.595201.
Train: 2018-08-08T22:45:44.640753: step 4967, loss 0.660062.
Train: 2018-08-08T22:45:47.770073: step 4968, loss 0.611207.
Train: 2018-08-08T22:45:50.833217: step 4969, loss 0.514543.
Train: 2018-08-08T22:45:53.887338: step 4970, loss 0.61092.
Test: 2018-08-08T22:46:14.700675: step 4970, loss 0.551559.
Train: 2018-08-08T22:46:17.783872: step 4971, loss 0.546931.
Train: 2018-08-08T22:46:20.870077: step 4972, loss 0.5152.
Train: 2018-08-08T22:46:23.942245: step 4973, loss 0.578858.
Train: 2018-08-08T22:46:27.024440: step 4974, loss 0.531297.
Train: 2018-08-08T22:46:30.077557: step 4975, loss 0.57886.
Train: 2018-08-08T22:46:33.146717: step 4976, loss 0.57886.
Train: 2018-08-08T22:46:36.181787: step 4977, loss 0.594662.
Train: 2018-08-08T22:46:39.229891: step 4978, loss 0.531533.
Train: 2018-08-08T22:46:42.258944: step 4979, loss 0.484267.
Train: 2018-08-08T22:46:45.305043: step 4980, loss 0.626206.
Test: 2018-08-08T22:47:06.132417: step 4980, loss 0.551881.
Train: 2018-08-08T22:47:09.201577: step 4981, loss 0.657755.
Train: 2018-08-08T22:47:12.286780: step 4982, loss 0.641872.
Train: 2018-08-08T22:47:15.326862: step 4983, loss 0.500336.
Train: 2018-08-08T22:47:18.391009: step 4984, loss 0.594562.
Train: 2018-08-08T22:47:21.449140: step 4985, loss 0.578879.
Train: 2018-08-08T22:47:24.525319: step 4986, loss 0.594522.
Train: 2018-08-08T22:47:27.585455: step 4987, loss 0.563279.
Train: 2018-08-08T22:47:30.657623: step 4988, loss 0.625659.
Train: 2018-08-08T22:47:33.719764: step 4989, loss 0.516694.
Train: 2018-08-08T22:47:36.786919: step 4990, loss 0.470151.
Test: 2018-08-08T22:47:57.654400: step 4990, loss 0.549369.
Train: 2018-08-08T22:48:00.748627: step 4991, loss 0.5478.
Train: 2018-08-08T22:48:03.866917: step 4992, loss 0.578899.
Train: 2018-08-08T22:48:06.905997: step 4993, loss 0.578894.
Train: 2018-08-08T22:48:09.987189: step 4994, loss 0.688117.
Train: 2018-08-08T22:48:13.039304: step 4995, loss 0.54773.
Train: 2018-08-08T22:48:16.122501: step 4996, loss 0.532183.
Train: 2018-08-08T22:48:19.184643: step 4997, loss 0.563325.
Train: 2018-08-08T22:48:22.247787: step 4998, loss 0.547743.
Train: 2018-08-08T22:48:25.299902: step 4999, loss 0.625658.
Train: 2018-08-08T22:48:28.318928: step 5000, loss 0.578896.
Test: 2018-08-08T22:48:49.210474: step 5000, loss 0.548139.
Train: 2018-08-08T22:48:54.086437: step 5001, loss 0.53217.
Train: 2018-08-08T22:48:57.125517: step 5002, loss 0.532152.
Train: 2018-08-08T22:49:00.188661: step 5003, loss 0.578892.
Train: 2018-08-08T22:49:03.258824: step 5004, loss 0.610112.
Train: 2018-08-08T22:49:06.341018: step 5005, loss 0.563277.
Train: 2018-08-08T22:49:09.382104: step 5006, loss 0.578888.
Train: 2018-08-08T22:49:12.433216: step 5007, loss 0.532036.
Train: 2018-08-08T22:49:15.513405: step 5008, loss 0.578885.
Train: 2018-08-08T22:49:18.552485: step 5009, loss 0.516324.
Train: 2018-08-08T22:49:21.581539: step 5010, loss 0.610207.
Test: 2018-08-08T22:49:42.389863: step 5010, loss 0.5497.
Train: 2018-08-08T22:49:45.466041: step 5011, loss 0.610227.
Train: 2018-08-08T22:49:48.516150: step 5012, loss 0.610225.
Train: 2018-08-08T22:49:51.580297: step 5013, loss 0.453582.
Train: 2018-08-08T22:49:54.647452: step 5014, loss 0.672985.
Train: 2018-08-08T22:49:57.720623: step 5015, loss 0.531847.
Train: 2018-08-08T22:50:00.782764: step 5016, loss 0.594556.
Train: 2018-08-08T22:50:03.845908: step 5017, loss 0.437781.
Train: 2018-08-08T22:50:06.913063: step 5018, loss 0.547447.
Train: 2018-08-08T22:50:09.938106: step 5019, loss 0.547366.
Train: 2018-08-08T22:50:13.022305: step 5020, loss 0.499907.
Test: 2018-08-08T22:50:33.858704: step 5020, loss 0.55056.
Train: 2018-08-08T22:50:36.917838: step 5021, loss 0.531317.
Train: 2018-08-08T22:50:40.002037: step 5022, loss 0.56295.
Train: 2018-08-08T22:50:43.045128: step 5023, loss 0.578861.
Train: 2018-08-08T22:50:46.086213: step 5024, loss 0.546839.
Train: 2018-08-08T22:50:49.148355: step 5025, loss 0.562809.
Train: 2018-08-08T22:50:52.175403: step 5026, loss 0.611089.
Train: 2018-08-08T22:50:55.212477: step 5027, loss 0.498213.
Train: 2018-08-08T22:50:58.262587: step 5028, loss 0.514183.
Train: 2018-08-08T22:51:01.323726: step 5029, loss 0.530215.
Train: 2018-08-08T22:51:04.384865: step 5030, loss 0.546348.
Test: 2018-08-08T22:51:25.252345: step 5030, loss 0.548522.
Train: 2018-08-08T22:51:28.326519: step 5031, loss 0.480888.
Train: 2018-08-08T22:51:31.389662: step 5032, loss 0.513312.
Train: 2018-08-08T22:51:34.435761: step 5033, loss 0.447069.
Train: 2018-08-08T22:51:37.516953: step 5034, loss 0.612238.
Train: 2018-08-08T22:51:40.571074: step 5035, loss 0.49576.
Train: 2018-08-08T22:51:43.596117: step 5036, loss 0.76343.
Train: 2018-08-08T22:51:46.626173: step 5037, loss 0.663036.
Train: 2018-08-08T22:51:49.653221: step 5038, loss 0.562397.
Train: 2018-08-08T22:51:52.711351: step 5039, loss 0.612665.
Train: 2018-08-08T22:51:55.773492: step 5040, loss 0.59587.
Test: 2018-08-08T22:52:16.634958: step 5040, loss 0.549929.
Train: 2018-08-08T22:52:19.721163: step 5041, loss 0.49562.
Train: 2018-08-08T22:52:22.810376: step 5042, loss 0.529048.
Train: 2018-08-08T22:52:25.886555: step 5043, loss 0.495689.
Train: 2018-08-08T22:52:28.937667: step 5044, loss 0.595814.
Train: 2018-08-08T22:52:32.019862: step 5045, loss 0.478927.
Train: 2018-08-08T22:52:35.042899: step 5046, loss 0.528971.
Train: 2018-08-08T22:52:38.094011: step 5047, loss 0.44517.
Train: 2018-08-08T22:52:41.183225: step 5048, loss 0.528784.
Train: 2018-08-08T22:52:44.218294: step 5049, loss 0.629797.
Train: 2018-08-08T22:52:47.290462: step 5050, loss 0.545479.
Test: 2018-08-08T22:53:08.109815: step 5050, loss 0.547789.
Train: 2018-08-08T22:53:11.183988: step 5051, loss 0.443948.
Train: 2018-08-08T22:53:14.260167: step 5052, loss 0.528408.
Train: 2018-08-08T22:53:17.324314: step 5053, loss 0.579369.
Train: 2018-08-08T22:53:20.370412: step 5054, loss 0.494057.
Train: 2018-08-08T22:53:23.429546: step 5055, loss 0.545211.
Train: 2018-08-08T22:53:26.444562: step 5056, loss 0.579509.
Train: 2018-08-08T22:53:29.490660: step 5057, loss 0.562336.
Train: 2018-08-08T22:53:32.562829: step 5058, loss 0.424399.
Train: 2018-08-08T22:53:35.624970: step 5059, loss 0.596945.
Train: 2018-08-08T22:53:38.719197: step 5060, loss 0.631717.
Test: 2018-08-08T22:53:59.549579: step 5060, loss 0.550024.
Train: 2018-08-08T22:54:02.630771: step 5061, loss 0.562349.
Train: 2018-08-08T22:54:05.689904: step 5062, loss 0.544982.
Train: 2018-08-08T22:54:08.760067: step 5063, loss 0.527596.
Train: 2018-08-08T22:54:11.816192: step 5064, loss 0.544963.
Train: 2018-08-08T22:54:14.885352: step 5065, loss 0.597166.
Train: 2018-08-08T22:54:17.949499: step 5066, loss 0.47534.
Train: 2018-08-08T22:54:20.993593: step 5067, loss 0.527519.
Train: 2018-08-08T22:54:24.057739: step 5068, loss 0.492601.
Train: 2018-08-08T22:54:27.101833: step 5069, loss 0.562371.
Train: 2018-08-08T22:54:30.170993: step 5070, loss 0.579874.
Test: 2018-08-08T22:54:50.995360: step 5070, loss 0.54796.
Train: 2018-08-08T22:54:54.085575: step 5071, loss 0.49234.
Train: 2018-08-08T22:54:57.100591: step 5072, loss 0.579922.
Train: 2018-08-08T22:55:00.163735: step 5073, loss 0.562391.
Train: 2018-08-08T22:55:03.198805: step 5074, loss 0.597508.
Train: 2018-08-08T22:55:06.260946: step 5075, loss 0.597494.
Train: 2018-08-08T22:55:09.312058: step 5076, loss 0.527322.
Train: 2018-08-08T22:55:12.380216: step 5077, loss 0.544863.
Train: 2018-08-08T22:55:15.461408: step 5078, loss 0.632414.
Train: 2018-08-08T22:55:18.527560: step 5079, loss 0.579848.
Train: 2018-08-08T22:55:21.585690: step 5080, loss 0.579802.
Test: 2018-08-08T22:55:42.437129: step 5080, loss 0.548687.
Train: 2018-08-08T22:55:45.484230: step 5081, loss 0.614545.
Train: 2018-08-08T22:55:48.536344: step 5082, loss 0.631712.
Train: 2018-08-08T22:55:51.636587: step 5083, loss 0.493259.
Train: 2018-08-08T22:55:54.706750: step 5084, loss 0.493455.
Train: 2018-08-08T22:55:57.763878: step 5085, loss 0.493576.
Train: 2018-08-08T22:56:00.824014: step 5086, loss 0.545158.
Train: 2018-08-08T22:56:03.896182: step 5087, loss 0.528002.
Train: 2018-08-08T22:56:06.951305: step 5088, loss 0.562335.
Train: 2018-08-08T22:56:09.984369: step 5089, loss 0.562335.
Train: 2018-08-08T22:56:13.033476: step 5090, loss 0.630926.
Test: 2018-08-08T22:56:33.882909: step 5090, loss 0.54759.
Train: 2018-08-08T22:56:36.916975: step 5091, loss 0.510979.
Train: 2018-08-08T22:56:40.012205: step 5092, loss 0.613647.
Train: 2018-08-08T22:56:43.059306: step 5093, loss 0.579412.
Train: 2018-08-08T22:56:46.125458: step 5094, loss 0.57938.
Train: 2018-08-08T22:56:49.179578: step 5095, loss 0.545344.
Train: 2018-08-08T22:56:52.263778: step 5096, loss 0.56235.
Train: 2018-08-08T22:56:55.275786: step 5097, loss 0.596229.
Train: 2018-08-08T22:56:58.309853: step 5098, loss 0.596158.
Train: 2018-08-08T22:57:01.354949: step 5099, loss 0.596075.
Train: 2018-08-08T22:57:04.405059: step 5100, loss 0.545589.
Test: 2018-08-08T22:57:25.268529: step 5100, loss 0.54797.
Train: 2018-08-08T22:57:30.139479: step 5101, loss 0.545648.
Train: 2018-08-08T22:57:33.189589: step 5102, loss 0.528982.
Train: 2018-08-08T22:57:36.263762: step 5103, loss 0.579112.
Train: 2018-08-08T22:57:39.311866: step 5104, loss 0.529116.
Train: 2018-08-08T22:57:42.378018: step 5105, loss 0.495882.
Train: 2018-08-08T22:57:45.418101: step 5106, loss 0.579081.
Train: 2018-08-08T22:57:48.460189: step 5107, loss 0.562445.
Train: 2018-08-08T22:57:51.525338: step 5108, loss 0.695458.
Train: 2018-08-08T22:57:54.576450: step 5109, loss 0.579049.
Train: 2018-08-08T22:57:57.634581: step 5110, loss 0.545947.
Test: 2018-08-08T22:58:18.467972: step 5110, loss 0.549538.
Train: 2018-08-08T22:58:21.543147: step 5111, loss 0.579008.
Train: 2018-08-08T22:58:24.601278: step 5112, loss 0.562525.
Train: 2018-08-08T22:58:27.642364: step 5113, loss 0.611837.
Train: 2018-08-08T22:58:30.710521: step 5114, loss 0.56257.
Train: 2018-08-08T22:58:33.791713: step 5115, loss 0.54625.
Train: 2018-08-08T22:58:36.874910: step 5116, loss 0.57893.
Train: 2018-08-08T22:58:39.764593: step 5117, loss 0.510545.
Train: 2018-08-08T22:58:42.836761: step 5118, loss 0.595178.
Train: 2018-08-08T22:58:45.900908: step 5119, loss 0.627629.
Train: 2018-08-08T22:58:48.957033: step 5120, loss 0.627504.
Test: 2018-08-08T22:59:09.813485: step 5120, loss 0.546992.
Train: 2018-08-08T22:59:12.883648: step 5121, loss 0.578885.
Train: 2018-08-08T22:59:15.949799: step 5122, loss 0.627175.
Train: 2018-08-08T22:59:19.004922: step 5123, loss 0.610941.
Train: 2018-08-08T22:59:22.078093: step 5124, loss 0.658701.
Train: 2018-08-08T22:59:25.175328: step 5125, loss 0.531216.
Train: 2018-08-08T22:59:28.216413: step 5126, loss 0.499798.
Train: 2018-08-08T22:59:31.288581: step 5127, loss 0.50001.
Train: 2018-08-08T22:59:34.353731: step 5128, loss 0.641879.
Train: 2018-08-08T22:59:37.410859: step 5129, loss 0.437415.
Train: 2018-08-08T22:59:40.483027: step 5130, loss 0.531702.
Test: 2018-08-08T23:00:01.322433: step 5130, loss 0.547192.
Train: 2018-08-08T23:00:04.419668: step 5131, loss 0.484436.
Train: 2018-08-08T23:00:07.492838: step 5132, loss 0.578863.
Train: 2018-08-08T23:00:10.558990: step 5133, loss 0.62629.
Train: 2018-08-08T23:00:13.615116: step 5134, loss 0.610506.
Train: 2018-08-08T23:00:16.700318: step 5135, loss 0.62633.
Train: 2018-08-08T23:00:19.745415: step 5136, loss 0.578861.
Train: 2018-08-08T23:00:22.787503: step 5137, loss 0.610444.
Train: 2018-08-08T23:00:25.859671: step 5138, loss 0.500044.
Train: 2018-08-08T23:00:28.941865: step 5139, loss 0.563105.
Train: 2018-08-08T23:00:32.019047: step 5140, loss 0.484313.
Test: 2018-08-08T23:00:52.864470: step 5140, loss 0.551875.
Train: 2018-08-08T23:00:55.922600: step 5141, loss 0.594645.
Train: 2018-08-08T23:00:58.995770: step 5142, loss 0.547272.
Train: 2018-08-08T23:01:02.057912: step 5143, loss 0.515605.
Train: 2018-08-08T23:01:05.117045: step 5144, loss 0.515476.
Train: 2018-08-08T23:01:08.159133: step 5145, loss 0.547079.
Train: 2018-08-08T23:01:11.206235: step 5146, loss 0.6426.
Train: 2018-08-08T23:01:14.304472: step 5147, loss 0.515039.
Train: 2018-08-08T23:01:17.376640: step 5148, loss 0.51491.
Train: 2018-08-08T23:01:20.433768: step 5149, loss 0.562834.
Train: 2018-08-08T23:01:23.512955: step 5150, loss 0.594945.
Test: 2018-08-08T23:01:44.338324: step 5150, loss 0.54708.
Train: 2018-08-08T23:01:47.390439: step 5151, loss 0.514471.
Train: 2018-08-08T23:01:50.432526: step 5152, loss 0.53046.
Train: 2018-08-08T23:01:53.503692: step 5153, loss 0.49796.
Train: 2018-08-08T23:01:56.569844: step 5154, loss 0.562663.
Train: 2018-08-08T23:01:59.614940: step 5155, loss 0.513728.
Train: 2018-08-08T23:02:02.655023: step 5156, loss 0.497142.
Train: 2018-08-08T23:02:05.726188: step 5157, loss 0.562542.
Train: 2018-08-08T23:02:08.758249: step 5158, loss 0.546008.
Train: 2018-08-08T23:02:11.821394: step 5159, loss 0.645286.
Train: 2018-08-08T23:02:14.870500: step 5160, loss 0.512676.
Test: 2018-08-08T23:02:35.708904: step 5160, loss 0.549371.
Train: 2018-08-08T23:02:38.823184: step 5161, loss 0.529169.
Train: 2018-08-08T23:02:41.896355: step 5162, loss 0.52906.
Train: 2018-08-08T23:02:44.962507: step 5163, loss 0.495483.
Train: 2018-08-08T23:02:48.005597: step 5164, loss 0.512015.
Train: 2018-08-08T23:02:51.084784: step 5165, loss 0.612943.
Train: 2018-08-08T23:02:54.131886: step 5166, loss 0.596163.
Train: 2018-08-08T23:02:57.238144: step 5167, loss 0.545427.
Train: 2018-08-08T23:03:00.326355: step 5168, loss 0.511482.
Train: 2018-08-08T23:03:03.390502: step 5169, loss 0.528361.
Train: 2018-08-08T23:03:06.435598: step 5170, loss 0.528277.
Test: 2018-08-08T23:03:27.312103: step 5170, loss 0.546341.
Train: 2018-08-08T23:03:30.359204: step 5171, loss 0.613556.
Train: 2018-08-08T23:03:33.429367: step 5172, loss 0.596528.
Train: 2018-08-08T23:03:36.504543: step 5173, loss 0.579437.
Train: 2018-08-08T23:03:39.605788: step 5174, loss 0.51104.
Train: 2018-08-08T23:03:42.682969: step 5175, loss 0.562336.
Train: 2018-08-08T23:03:45.716033: step 5176, loss 0.596564.
Train: 2018-08-08T23:03:48.781183: step 5177, loss 0.59655.
Train: 2018-08-08T23:03:51.811239: step 5178, loss 0.613604.
Train: 2018-08-08T23:03:54.855332: step 5179, loss 0.511171.
Train: 2018-08-08T23:03:57.924492: step 5180, loss 0.630491.
Test: 2018-08-08T23:04:18.811024: step 5180, loss 0.548988.
Train: 2018-08-08T23:04:21.928312: step 5181, loss 0.613348.
Train: 2018-08-08T23:04:25.002485: step 5182, loss 0.647106.
Train: 2018-08-08T23:04:28.090696: step 5183, loss 0.545485.
Train: 2018-08-08T23:04:31.163867: step 5184, loss 0.612843.
Train: 2018-08-08T23:04:34.232024: step 5185, loss 0.562401.
Train: 2018-08-08T23:04:37.269099: step 5186, loss 0.545737.
Train: 2018-08-08T23:04:40.318206: step 5187, loss 0.595709.
Train: 2018-08-08T23:04:43.370320: step 5188, loss 0.529326.
Train: 2018-08-08T23:04:46.422435: step 5189, loss 0.46333.
Train: 2018-08-08T23:04:49.496608: step 5190, loss 0.612037.
Test: 2018-08-08T23:05:10.363087: step 5190, loss 0.547069.
Train: 2018-08-08T23:05:13.472353: step 5191, loss 0.595484.
Train: 2018-08-08T23:05:16.562569: step 5192, loss 0.513183.
Train: 2018-08-08T23:05:19.654790: step 5193, loss 0.546111.
Train: 2018-08-08T23:05:22.686852: step 5194, loss 0.496871.
Train: 2018-08-08T23:05:25.752001: step 5195, loss 0.578973.
Train: 2018-08-08T23:05:28.822164: step 5196, loss 0.644682.
Train: 2018-08-08T23:05:31.949479: step 5197, loss 0.611775.
Train: 2018-08-08T23:05:34.997583: step 5198, loss 0.595322.
Train: 2018-08-08T23:05:38.019617: step 5199, loss 0.562604.
Train: 2018-08-08T23:05:41.081759: step 5200, loss 0.59522.
Test: 2018-08-08T23:06:02.016419: step 5200, loss 0.549885.
Train: 2018-08-08T23:06:06.919454: step 5201, loss 0.530146.
Train: 2018-08-08T23:06:09.996636: step 5202, loss 0.432858.
Train: 2018-08-08T23:06:13.054766: step 5203, loss 0.562668.
Train: 2018-08-08T23:06:16.113900: step 5204, loss 0.627657.
Train: 2018-08-08T23:06:19.169023: step 5205, loss 0.530186.
Train: 2018-08-08T23:06:22.217127: step 5206, loss 0.546423.
Train: 2018-08-08T23:06:25.268239: step 5207, loss 0.497667.
Train: 2018-08-08T23:06:28.307319: step 5208, loss 0.578917.
Train: 2018-08-08T23:06:31.345396: step 5209, loss 0.513765.
Train: 2018-08-08T23:06:34.388487: step 5210, loss 0.513659.
Test: 2018-08-08T23:06:55.224886: step 5210, loss 0.546642.
Train: 2018-08-08T23:06:58.287027: step 5211, loss 0.644377.
Train: 2018-08-08T23:07:01.316080: step 5212, loss 0.595321.
Train: 2018-08-08T23:07:04.377219: step 5213, loss 0.51346.
Train: 2018-08-08T23:07:07.448384: step 5214, loss 0.562569.
Train: 2018-08-08T23:07:10.518547: step 5215, loss 0.529757.
Train: 2018-08-08T23:07:13.574672: step 5216, loss 0.480435.
Train: 2018-08-08T23:07:16.670904: step 5217, loss 0.546062.
Train: 2018-08-08T23:07:19.738059: step 5218, loss 0.479984.
Train: 2018-08-08T23:07:22.816243: step 5219, loss 0.562476.
Train: 2018-08-08T23:07:25.885403: step 5220, loss 0.51261.
Test: 2018-08-08T23:07:46.733834: step 5220, loss 0.548068.
Train: 2018-08-08T23:07:49.794972: step 5221, loss 0.579102.
Train: 2018-08-08T23:07:52.901230: step 5222, loss 0.612571.
Train: 2018-08-08T23:07:55.972396: step 5223, loss 0.57915.
Train: 2018-08-08T23:07:58.984404: step 5224, loss 0.579162.
Train: 2018-08-08T23:08:02.041532: step 5225, loss 0.595946.
Train: 2018-08-08T23:08:05.053540: step 5226, loss 0.528844.
Train: 2018-08-08T23:08:08.093623: step 5227, loss 0.595954.
Train: 2018-08-08T23:08:11.135711: step 5228, loss 0.545618.
Train: 2018-08-08T23:08:14.171783: step 5229, loss 0.528847.
Train: 2018-08-08T23:08:17.252975: step 5230, loss 0.629508.
Test: 2018-08-08T23:08:38.117448: step 5230, loss 0.549225.
Train: 2018-08-08T23:08:41.197637: step 5231, loss 0.595925.
Train: 2018-08-08T23:08:44.249752: step 5232, loss 0.495445.
Train: 2018-08-08T23:08:47.304875: step 5233, loss 0.595876.
Train: 2018-08-08T23:08:50.357992: step 5234, loss 0.562412.
Train: 2018-08-08T23:08:53.412112: step 5235, loss 0.595825.
Train: 2018-08-08T23:08:56.518371: step 5236, loss 0.495708.
Train: 2018-08-08T23:08:59.553440: step 5237, loss 0.495731.
Train: 2018-08-08T23:09:02.621598: step 5238, loss 0.495678.
Train: 2018-08-08T23:09:05.656667: step 5239, loss 0.562414.
Train: 2018-08-08T23:09:08.732846: step 5240, loss 0.545669.
Test: 2018-08-08T23:09:29.613362: step 5240, loss 0.548595.
Train: 2018-08-08T23:09:32.707588: step 5241, loss 0.562398.
Train: 2018-08-08T23:09:35.820866: step 5242, loss 0.528831.
Train: 2018-08-08T23:09:38.907071: step 5243, loss 0.427939.
Train: 2018-08-08T23:09:41.954172: step 5244, loss 0.511783.
Train: 2018-08-08T23:09:44.992250: step 5245, loss 0.596208.
Train: 2018-08-08T23:09:48.084471: step 5246, loss 0.59629.
Train: 2018-08-08T23:09:51.108511: step 5247, loss 0.494349.
Train: 2018-08-08T23:09:54.188700: step 5248, loss 0.5453.
Train: 2018-08-08T23:09:57.240815: step 5249, loss 0.494017.
Train: 2018-08-08T23:10:00.307970: step 5250, loss 0.562335.
Test: 2018-08-08T23:10:21.173446: step 5250, loss 0.545595.
Train: 2018-08-08T23:10:24.263661: step 5251, loss 0.562335.
Train: 2018-08-08T23:10:27.357888: step 5252, loss 0.613961.
Train: 2018-08-08T23:10:30.420029: step 5253, loss 0.441775.
Train: 2018-08-08T23:10:33.491195: step 5254, loss 0.510545.
Train: 2018-08-08T23:10:36.552334: step 5255, loss 0.63159.
Train: 2018-08-08T23:10:39.608459: step 5256, loss 0.510347.
Train: 2018-08-08T23:10:42.647539: step 5257, loss 0.510261.
Train: 2018-08-08T23:10:45.712688: step 5258, loss 0.527561.
Train: 2018-08-08T23:10:48.783854: step 5259, loss 0.510057.
Train: 2018-08-08T23:10:51.867051: step 5260, loss 0.509946.
Test: 2018-08-08T23:11:12.710468: step 5260, loss 0.548614.
Train: 2018-08-08T23:11:15.777623: step 5261, loss 0.544857.
Train: 2018-08-08T23:11:18.836756: step 5262, loss 0.544822.
Train: 2018-08-08T23:11:21.897895: step 5263, loss 0.544809.
Train: 2018-08-08T23:11:24.928954: step 5264, loss 0.685889.
Train: 2018-08-08T23:11:27.998114: step 5265, loss 0.491876.
Train: 2018-08-08T23:11:31.034186: step 5266, loss 0.580063.
Train: 2018-08-08T23:11:34.104348: step 5267, loss 0.615376.
Train: 2018-08-08T23:11:37.152452: step 5268, loss 0.597656.
Train: 2018-08-08T23:11:40.195543: step 5269, loss 0.650336.
Train: 2018-08-08T23:11:43.267711: step 5270, loss 0.492265.
Test: 2018-08-08T23:12:04.121156: step 5270, loss 0.54797.
Train: 2018-08-08T23:12:07.156224: step 5271, loss 0.597362.
Train: 2018-08-08T23:12:10.234409: step 5272, loss 0.527472.
Train: 2018-08-08T23:12:13.294544: step 5273, loss 0.475313.
Train: 2018-08-08T23:12:16.374734: step 5274, loss 0.440598.
Train: 2018-08-08T23:12:19.429856: step 5275, loss 0.544949.
Train: 2018-08-08T23:12:22.502024: step 5276, loss 0.579781.
Train: 2018-08-08T23:12:25.582214: step 5277, loss 0.562361.
Train: 2018-08-08T23:12:28.660398: step 5278, loss 0.597214.
Train: 2018-08-08T23:12:31.741590: step 5279, loss 0.510131.
Train: 2018-08-08T23:12:34.823784: step 5280, loss 0.649384.
Test: 2018-08-08T23:12:55.687255: step 5280, loss 0.545414.
Train: 2018-08-08T23:12:58.813567: step 5281, loss 0.631855.
Train: 2018-08-08T23:13:01.873703: step 5282, loss 0.596999.
Train: 2018-08-08T23:13:04.915791: step 5283, loss 0.61415.
Train: 2018-08-08T23:13:07.976930: step 5284, loss 0.59674.
Train: 2018-08-08T23:13:11.025034: step 5285, loss 0.596593.
Train: 2018-08-08T23:13:14.092188: step 5286, loss 0.630541.
Train: 2018-08-08T23:13:17.140293: step 5287, loss 0.731939.
Train: 2018-08-08T23:13:20.203436: step 5288, loss 0.562379.
Train: 2018-08-08T23:13:23.239509: step 5289, loss 0.612549.
Train: 2018-08-08T23:13:26.321703: step 5290, loss 0.628821.
Test: 2018-08-08T23:13:47.198208: step 5290, loss 0.548349.
Train: 2018-08-08T23:13:50.233278: step 5291, loss 0.496683.
Train: 2018-08-08T23:13:53.311462: step 5292, loss 0.529863.
Train: 2018-08-08T23:13:56.379619: step 5293, loss 0.644052.
Train: 2018-08-08T23:13:59.438752: step 5294, loss 0.46557.
Train: 2018-08-08T23:14:02.506910: step 5295, loss 0.498221.
Train: 2018-08-08T23:14:05.565040: step 5296, loss 0.594976.
Train: 2018-08-08T23:14:08.622169: step 5297, loss 0.594932.
Train: 2018-08-08T23:14:11.698347: step 5298, loss 0.578865.
Train: 2018-08-08T23:14:14.746451: step 5299, loss 0.642776.
Train: 2018-08-08T23:14:17.855718: step 5300, loss 0.562939.
Test: 2018-08-08T23:14:38.704149: step 5300, loss 0.549321.
Train: 2018-08-08T23:14:43.552037: step 5301, loss 0.420166.
Train: 2018-08-08T23:14:46.619192: step 5302, loss 0.61059.
Train: 2018-08-08T23:14:49.671307: step 5303, loss 0.531309.
Train: 2018-08-08T23:14:52.716403: step 5304, loss 0.578859.
Train: 2018-08-08T23:14:55.787568: step 5305, loss 0.65807.
Train: 2018-08-08T23:14:58.858734: step 5306, loss 0.626297.
Train: 2018-08-08T23:15:01.946944: step 5307, loss 0.641938.
Train: 2018-08-08T23:15:05.015102: step 5308, loss 0.594581.
Train: 2018-08-08T23:15:08.092283: step 5309, loss 0.578882.
Train: 2018-08-08T23:15:11.175481: step 5310, loss 0.563301.
Test: 2018-08-08T23:15:32.014887: step 5310, loss 0.551735.
Train: 2018-08-08T23:15:35.105103: step 5311, loss 0.547816.
Train: 2018-08-08T23:15:38.157218: step 5312, loss 0.532389.
Train: 2018-08-08T23:15:41.208330: step 5313, loss 0.563434.
Train: 2018-08-08T23:15:44.275484: step 5314, loss 0.594402.
Train: 2018-08-08T23:15:47.305540: step 5315, loss 0.548025.
Train: 2018-08-08T23:15:50.389740: step 5316, loss 0.517158.
Train: 2018-08-08T23:15:53.519061: step 5317, loss 0.548029.
Train: 2018-08-08T23:15:56.582204: step 5318, loss 0.563462.
Train: 2018-08-08T23:15:59.648357: step 5319, loss 0.516986.
Train: 2018-08-08T23:16:02.728546: step 5320, loss 0.578914.
Test: 2018-08-08T23:16:23.592016: step 5320, loss 0.550564.
Train: 2018-08-08T23:16:26.641123: step 5321, loss 0.516734.
Train: 2018-08-08T23:16:29.697248: step 5322, loss 0.532147.
Train: 2018-08-08T23:16:32.783454: step 5323, loss 0.68831.
Train: 2018-08-08T23:16:35.851611: step 5324, loss 0.547599.
Train: 2018-08-08T23:16:38.926787: step 5325, loss 0.610196.
Train: 2018-08-08T23:16:42.002966: step 5326, loss 0.610202.
Train: 2018-08-08T23:16:45.046056: step 5327, loss 0.657148.
Train: 2018-08-08T23:16:48.123238: step 5328, loss 0.594509.
Train: 2018-08-08T23:16:51.175352: step 5329, loss 0.594482.
Train: 2018-08-08T23:16:54.228470: step 5330, loss 0.594454.
Test: 2018-08-08T23:17:15.089935: step 5330, loss 0.550636.
Train: 2018-08-08T23:17:18.150071: step 5331, loss 0.547899.
Train: 2018-08-08T23:17:21.229257: step 5332, loss 0.578926.
Train: 2018-08-08T23:17:24.270343: step 5333, loss 0.60984.
Train: 2018-08-08T23:17:27.339503: step 5334, loss 0.609783.
Train: 2018-08-08T23:17:30.397634: step 5335, loss 0.594339.
Train: 2018-08-08T23:17:33.482836: step 5336, loss 0.502316.
Train: 2018-08-08T23:17:36.575058: step 5337, loss 0.487096.
Train: 2018-08-08T23:17:39.638202: step 5338, loss 0.502348.
Train: 2018-08-08T23:17:42.714380: step 5339, loss 0.532891.
Train: 2018-08-08T23:17:45.769503: step 5340, loss 0.686763.
Test: 2018-08-08T23:18:06.661049: step 5340, loss 0.550859.
Train: 2018-08-08T23:18:09.760288: step 5341, loss 0.563547.
Train: 2018-08-08T23:18:12.862536: step 5342, loss 0.548123.
Train: 2018-08-08T23:18:15.913648: step 5343, loss 0.486376.
Train: 2018-08-08T23:18:18.981806: step 5344, loss 0.470662.
Train: 2018-08-08T23:18:22.030912: step 5345, loss 0.516781.
Train: 2018-08-08T23:18:25.082024: step 5346, loss 0.625712.
Train: 2018-08-08T23:18:28.170235: step 5347, loss 0.610198.
Train: 2018-08-08T23:18:31.229369: step 5348, loss 0.578874.
Train: 2018-08-08T23:18:34.296523: step 5349, loss 0.57887.
Train: 2018-08-08T23:18:37.336606: step 5350, loss 0.594616.
Test: 2018-08-08T23:18:58.254221: step 5350, loss 0.548921.
Train: 2018-08-08T23:19:01.322377: step 5351, loss 0.578866.
Train: 2018-08-08T23:19:04.365468: step 5352, loss 0.547304.
Train: 2018-08-08T23:19:07.414575: step 5353, loss 0.610444.
Train: 2018-08-08T23:19:10.523842: step 5354, loss 0.610464.
Train: 2018-08-08T23:19:13.590996: step 5355, loss 0.56307.
Train: 2018-08-08T23:19:16.652135: step 5356, loss 0.563096.
Train: 2018-08-08T23:19:19.711268: step 5357, loss 0.641994.
Train: 2018-08-08T23:19:22.757367: step 5358, loss 0.515825.
Train: 2018-08-08T23:19:25.835551: step 5359, loss 0.704891.
Train: 2018-08-08T23:19:28.905714: step 5360, loss 0.578871.
Test: 2018-08-08T23:19:49.773195: step 5360, loss 0.549123.
Train: 2018-08-08T23:19:52.850376: step 5361, loss 0.500541.
Train: 2018-08-08T23:19:55.897478: step 5362, loss 0.54758.
Train: 2018-08-08T23:19:58.938563: step 5363, loss 0.56324.
Train: 2018-08-08T23:20:01.991680: step 5364, loss 0.578884.
Train: 2018-08-08T23:20:05.087912: step 5365, loss 0.563253.
Train: 2018-08-08T23:20:08.189158: step 5366, loss 0.563257.
Train: 2018-08-08T23:20:11.222222: step 5367, loss 0.578886.
Train: 2018-08-08T23:20:14.402678: step 5368, loss 0.51638.
Train: 2018-08-08T23:20:17.493897: step 5369, loss 0.51632.
Train: 2018-08-08T23:20:20.538993: step 5370, loss 0.610217.
Test: 2018-08-08T23:20:41.417503: step 5370, loss 0.550274.
Train: 2018-08-08T23:20:44.527772: step 5371, loss 0.59456.
Train: 2018-08-08T23:20:47.580889: step 5372, loss 0.563183.
Train: 2018-08-08T23:20:50.637015: step 5373, loss 0.610272.
Train: 2018-08-08T23:20:53.675092: step 5374, loss 0.578873.
Train: 2018-08-08T23:20:56.748263: step 5375, loss 0.531803.
Train: 2018-08-08T23:20:59.810404: step 5376, loss 0.54748.
Train: 2018-08-08T23:21:02.882572: step 5377, loss 0.657419.
Train: 2018-08-08T23:21:05.908618: step 5378, loss 0.484698.
Train: 2018-08-08T23:21:08.962738: step 5379, loss 0.468912.
Train: 2018-08-08T23:21:12.037914: step 5380, loss 0.626115.
Test: 2018-08-08T23:21:32.939486: step 5380, loss 0.548318.
Train: 2018-08-08T23:21:36.015664: step 5381, loss 0.563095.
Train: 2018-08-08T23:21:39.079811: step 5382, loss 0.563072.
Train: 2018-08-08T23:21:42.126912: step 5383, loss 0.610483.
Train: 2018-08-08T23:21:45.184041: step 5384, loss 0.563042.
Train: 2018-08-08T23:21:48.259216: step 5385, loss 0.642164.
Train: 2018-08-08T23:21:51.317347: step 5386, loss 0.626298.
Train: 2018-08-08T23:21:54.381494: step 5387, loss 0.563079.
Train: 2018-08-08T23:21:57.472712: step 5388, loss 0.594626.
Train: 2018-08-08T23:22:00.532849: step 5389, loss 0.563136.
Train: 2018-08-08T23:22:03.547865: step 5390, loss 0.547453.
Test: 2018-08-08T23:22:24.424369: step 5390, loss 0.552034.
Train: 2018-08-08T23:22:27.493529: step 5391, loss 0.578874.
Train: 2018-08-08T23:22:30.620844: step 5392, loss 0.437755.
Train: 2018-08-08T23:22:33.693012: step 5393, loss 0.468935.
Train: 2018-08-08T23:22:36.758161: step 5394, loss 0.547351.
Train: 2018-08-08T23:22:39.832335: step 5395, loss 0.578861.
Train: 2018-08-08T23:22:42.899490: step 5396, loss 0.499579.
Train: 2018-08-08T23:22:45.945588: step 5397, loss 0.626607.
Train: 2018-08-08T23:22:49.022770: step 5398, loss 0.626723.
Train: 2018-08-08T23:22:52.100954: step 5399, loss 0.578861.
Train: 2018-08-08T23:22:55.143042: step 5400, loss 0.642802.
Test: 2018-08-08T23:23:16.017542: step 5400, loss 0.54792.
Train: 2018-08-08T23:23:20.850391: step 5401, loss 0.514964.
Train: 2018-08-08T23:23:23.910527: step 5402, loss 0.451023.
Train: 2018-08-08T23:23:26.985703: step 5403, loss 0.450723.
Train: 2018-08-08T23:23:30.051855: step 5404, loss 0.707548.
Train: 2018-08-08T23:23:33.103969: step 5405, loss 0.578877.
Train: 2018-08-08T23:23:36.182153: step 5406, loss 0.546642.
Train: 2018-08-08T23:23:39.203186: step 5407, loss 0.48206.
Train: 2018-08-08T23:23:42.259311: step 5408, loss 0.498008.
Train: 2018-08-08T23:23:45.310423: step 5409, loss 0.562674.
Train: 2018-08-08T23:23:48.372565: step 5410, loss 0.464961.
Test: 2018-08-08T23:24:09.229017: step 5410, loss 0.547272.
Train: 2018-08-08T23:24:12.318230: step 5411, loss 0.578944.
Train: 2018-08-08T23:24:15.405438: step 5412, loss 0.529733.
Train: 2018-08-08T23:24:18.478608: step 5413, loss 0.52957.
Train: 2018-08-08T23:24:21.575843: step 5414, loss 0.579032.
Train: 2018-08-08T23:24:24.633974: step 5415, loss 0.579052.
Train: 2018-08-08T23:24:27.690099: step 5416, loss 0.628982.
Train: 2018-08-08T23:24:30.755248: step 5417, loss 0.429229.
Train: 2018-08-08T23:24:33.646936: step 5418, loss 0.544597.
Train: 2018-08-08T23:24:36.726123: step 5419, loss 0.595917.
Train: 2018-08-08T23:24:39.814334: step 5420, loss 0.528822.
Test: 2018-08-08T23:25:00.671788: step 5420, loss 0.547252.
Train: 2018-08-08T23:25:03.743956: step 5421, loss 0.562375.
Train: 2018-08-08T23:25:06.825148: step 5422, loss 0.461212.
Train: 2018-08-08T23:25:09.892303: step 5423, loss 0.596187.
Train: 2018-08-08T23:25:12.948428: step 5424, loss 0.596242.
Train: 2018-08-08T23:25:15.994527: step 5425, loss 0.477434.
Train: 2018-08-08T23:25:19.049650: step 5426, loss 0.562398.
Train: 2018-08-08T23:25:22.079706: step 5427, loss 0.562441.
Train: 2018-08-08T23:25:25.117783: step 5428, loss 0.476985.
Train: 2018-08-08T23:25:28.178922: step 5429, loss 0.562361.
Train: 2018-08-08T23:25:31.234045: step 5430, loss 0.579542.
Test: 2018-08-08T23:25:52.108545: step 5430, loss 0.547545.
Train: 2018-08-08T23:25:55.270952: step 5431, loss 0.579508.
Train: 2018-08-08T23:25:58.354150: step 5432, loss 0.562332.
Train: 2018-08-08T23:26:01.426317: step 5433, loss 0.596728.
Train: 2018-08-08T23:26:04.489462: step 5434, loss 0.562335.
Train: 2018-08-08T23:26:07.569651: step 5435, loss 0.596709.
Train: 2018-08-08T23:26:10.625776: step 5436, loss 0.579504.
Train: 2018-08-08T23:26:13.682904: step 5437, loss 0.699501.
Train: 2018-08-08T23:26:16.769110: step 5438, loss 0.613594.
Train: 2018-08-08T23:26:19.840275: step 5439, loss 0.511298.
Train: 2018-08-08T23:26:22.897403: step 5440, loss 0.562351.
Test: 2018-08-08T23:26:43.777919: step 5440, loss 0.54971.
Train: 2018-08-08T23:26:46.866129: step 5441, loss 0.613099.
Train: 2018-08-08T23:26:49.963364: step 5442, loss 0.562373.
Train: 2018-08-08T23:26:53.067618: step 5443, loss 0.612777.
Train: 2018-08-08T23:26:56.152820: step 5444, loss 0.612598.
Train: 2018-08-08T23:26:59.211954: step 5445, loss 0.479158.
Train: 2018-08-08T23:27:02.280111: step 5446, loss 0.462794.
Train: 2018-08-08T23:27:05.342253: step 5447, loss 0.479481.
Train: 2018-08-08T23:27:08.409407: step 5448, loss 0.612285.
Train: 2018-08-08T23:27:11.474556: step 5449, loss 0.49607.
Train: 2018-08-08T23:27:14.541711: step 5450, loss 0.562423.
Test: 2018-08-08T23:27:35.438270: step 5450, loss 0.548135.
Train: 2018-08-08T23:27:38.531493: step 5451, loss 0.495984.
Train: 2018-08-08T23:27:41.626723: step 5452, loss 0.662307.
Train: 2018-08-08T23:27:44.684854: step 5453, loss 0.54581.
Train: 2018-08-08T23:27:47.753011: step 5454, loss 0.512465.
Train: 2018-08-08T23:27:50.834203: step 5455, loss 0.51254.
Train: 2018-08-08T23:27:53.898350: step 5456, loss 0.579085.
Train: 2018-08-08T23:27:56.966507: step 5457, loss 0.595814.
Train: 2018-08-08T23:28:00.028648: step 5458, loss 0.579069.
Train: 2018-08-08T23:28:03.092795: step 5459, loss 0.562425.
Train: 2018-08-08T23:28:06.148921: step 5460, loss 0.562458.
Test: 2018-08-08T23:28:27.048487: step 5460, loss 0.548077.
Train: 2018-08-08T23:28:30.131684: step 5461, loss 0.529059.
Train: 2018-08-08T23:28:33.189815: step 5462, loss 0.562399.
Train: 2018-08-08T23:28:36.272009: step 5463, loss 0.61245.
Train: 2018-08-08T23:28:39.352199: step 5464, loss 0.529201.
Train: 2018-08-08T23:28:42.431386: step 5465, loss 0.645708.
Train: 2018-08-08T23:28:45.465452: step 5466, loss 0.512633.
Train: 2018-08-08T23:28:48.520575: step 5467, loss 0.612244.
Train: 2018-08-08T23:28:51.596754: step 5468, loss 0.579041.
Train: 2018-08-08T23:28:54.628815: step 5469, loss 0.463258.
Train: 2018-08-08T23:28:57.706999: step 5470, loss 0.545948.
Test: 2018-08-08T23:29:18.611579: step 5470, loss 0.547612.
Train: 2018-08-08T23:29:21.667704: step 5471, loss 0.529402.
Train: 2018-08-08T23:29:24.751904: step 5472, loss 0.529378.
Train: 2018-08-08T23:29:27.844125: step 5473, loss 0.645329.
Train: 2018-08-08T23:29:30.916294: step 5474, loss 0.496221.
Train: 2018-08-08T23:29:33.990467: step 5475, loss 0.628766.
Train: 2018-08-08T23:29:37.066646: step 5476, loss 0.512781.
Train: 2018-08-08T23:29:40.148840: step 5477, loss 0.628747.
Train: 2018-08-08T23:29:43.235046: step 5478, loss 0.512821.
Train: 2018-08-08T23:29:46.298190: step 5479, loss 0.562482.
Train: 2018-08-08T23:29:49.346294: step 5480, loss 0.579032.
Test: 2018-08-08T23:30:10.223802: step 5480, loss 0.550113.
Train: 2018-08-08T23:30:13.254860: step 5481, loss 0.595571.
Train: 2018-08-08T23:30:16.341065: step 5482, loss 0.612072.
Train: 2018-08-08T23:30:19.400199: step 5483, loss 0.562508.
Train: 2018-08-08T23:30:22.456324: step 5484, loss 0.578992.
Train: 2018-08-08T23:30:25.518466: step 5485, loss 0.578972.
Train: 2018-08-08T23:30:28.587626: step 5486, loss 0.628199.
Train: 2018-08-08T23:30:31.656786: step 5487, loss 0.611658.
Train: 2018-08-08T23:30:34.727951: step 5488, loss 0.578937.
Train: 2018-08-08T23:30:37.830199: step 5489, loss 0.627635.
Train: 2018-08-08T23:30:40.892340: step 5490, loss 0.562724.
Test: 2018-08-08T23:31:01.763832: step 5490, loss 0.547631.
Train: 2018-08-08T23:31:04.902176: step 5491, loss 0.659535.
Train: 2018-08-08T23:31:08.063581: step 5492, loss 0.49863.
Train: 2018-08-08T23:31:11.129733: step 5493, loss 0.610849.
Train: 2018-08-08T23:31:14.199896: step 5494, loss 0.56296.
Train: 2018-08-08T23:31:17.274069: step 5495, loss 0.467674.
Train: 2018-08-08T23:31:20.337213: step 5496, loss 0.563005.
Train: 2018-08-08T23:31:23.390331: step 5497, loss 0.515466.
Train: 2018-08-08T23:31:26.435427: step 5498, loss 0.515436.
Train: 2018-08-08T23:31:29.518624: step 5499, loss 0.547129.
Train: 2018-08-08T23:31:32.594803: step 5500, loss 0.642489.
Test: 2018-08-08T23:31:53.475319: step 5500, loss 0.551078.
Train: 2018-08-08T23:31:58.327219: step 5501, loss 0.610614.
Train: 2018-08-08T23:32:01.397381: step 5502, loss 0.467747.
Train: 2018-08-08T23:32:04.470552: step 5503, loss 0.515299.
Train: 2018-08-08T23:32:07.523669: step 5504, loss 0.578905.
Train: 2018-08-08T23:32:10.557736: step 5505, loss 0.626728.
Train: 2018-08-08T23:32:13.649957: step 5506, loss 0.531038.
Train: 2018-08-08T23:32:16.688035: step 5507, loss 0.515043.
Train: 2018-08-08T23:32:19.737142: step 5508, loss 0.562879.
Train: 2018-08-08T23:32:22.794269: step 5509, loss 0.594873.
Train: 2018-08-08T23:32:25.875462: step 5510, loss 0.546816.
Test: 2018-08-08T23:32:46.738932: step 5510, loss 0.549004.
Train: 2018-08-08T23:32:49.820124: step 5511, loss 0.51469.
Train: 2018-08-08T23:32:52.896302: step 5512, loss 0.562796.
Train: 2018-08-08T23:32:55.989527: step 5513, loss 0.530557.
Train: 2018-08-08T23:32:59.043647: step 5514, loss 0.595028.
Train: 2018-08-08T23:33:02.107793: step 5515, loss 0.546552.
Train: 2018-08-08T23:33:05.128826: step 5516, loss 0.5627.
Train: 2018-08-08T23:33:08.226060: step 5517, loss 0.530239.
Train: 2018-08-08T23:33:11.284191: step 5518, loss 0.513898.
Train: 2018-08-08T23:33:14.323271: step 5519, loss 0.562629.
Train: 2018-08-08T23:33:17.384410: step 5520, loss 0.49725.
Test: 2018-08-08T23:33:38.339123: step 5520, loss 0.548448.
Train: 2018-08-08T23:33:41.444378: step 5521, loss 0.660947.
Train: 2018-08-08T23:33:44.519554: step 5522, loss 0.595382.
Train: 2018-08-08T23:33:47.575680: step 5523, loss 0.529711.
Train: 2018-08-08T23:33:50.638824: step 5524, loss 0.628256.
Train: 2018-08-08T23:33:53.673893: step 5525, loss 0.578968.
Train: 2018-08-08T23:33:56.774136: step 5526, loss 0.562573.
Train: 2018-08-08T23:33:59.845301: step 5527, loss 0.562585.
Train: 2018-08-08T23:34:02.912456: step 5528, loss 0.546153.
Train: 2018-08-08T23:34:05.945520: step 5529, loss 0.595368.
Train: 2018-08-08T23:34:08.998638: step 5530, loss 0.546175.
Test: 2018-08-08T23:34:29.909234: step 5530, loss 0.549082.
Train: 2018-08-08T23:34:32.948313: step 5531, loss 0.611713.
Train: 2018-08-08T23:34:36.003436: step 5532, loss 0.595295.
Train: 2018-08-08T23:34:39.065577: step 5533, loss 0.562601.
Train: 2018-08-08T23:34:42.143761: step 5534, loss 0.562606.
Train: 2018-08-08T23:34:45.210916: step 5535, loss 0.513749.
Train: 2018-08-08T23:34:48.276065: step 5536, loss 0.627805.
Train: 2018-08-08T23:34:51.353247: step 5537, loss 0.400012.
Train: 2018-08-08T23:34:54.402353: step 5538, loss 0.546323.
Train: 2018-08-08T23:34:57.455471: step 5539, loss 0.595204.
Train: 2018-08-08T23:35:00.495553: step 5540, loss 0.52991.
Test: 2018-08-08T23:35:21.409158: step 5540, loss 0.550348.
Train: 2018-08-08T23:35:24.466285: step 5541, loss 0.513556.
Train: 2018-08-08T23:35:27.560512: step 5542, loss 0.529705.
Train: 2018-08-08T23:35:30.606611: step 5543, loss 0.562414.
Train: 2018-08-08T23:35:33.685797: step 5544, loss 0.562688.
Train: 2018-08-08T23:35:36.754957: step 5545, loss 0.446765.
Train: 2018-08-08T23:35:39.843168: step 5546, loss 0.495653.
Train: 2018-08-08T23:35:42.901299: step 5547, loss 0.59626.
Train: 2018-08-08T23:35:45.959430: step 5548, loss 0.630053.
Train: 2018-08-08T23:35:49.023576: step 5549, loss 0.646712.
Train: 2018-08-08T23:35:52.094741: step 5550, loss 0.595466.
Test: 2018-08-08T23:36:13.042436: step 5550, loss 0.549881.
Train: 2018-08-08T23:36:16.156716: step 5551, loss 0.528747.
Train: 2018-08-08T23:36:19.239913: step 5552, loss 0.595695.
Train: 2018-08-08T23:36:22.327121: step 5553, loss 0.579021.
Train: 2018-08-08T23:36:25.402297: step 5554, loss 0.562564.
Train: 2018-08-08T23:36:28.461431: step 5555, loss 0.545865.
Train: 2018-08-08T23:36:31.555657: step 5556, loss 0.545667.
Train: 2018-08-08T23:36:34.628828: step 5557, loss 0.529012.
Train: 2018-08-08T23:36:37.687961: step 5558, loss 0.545641.
Train: 2018-08-08T23:36:40.759127: step 5559, loss 0.629128.
Train: 2018-08-08T23:36:43.835305: step 5560, loss 0.579221.
Test: 2018-08-08T23:37:04.714819: step 5560, loss 0.548129.
Train: 2018-08-08T23:37:07.788992: step 5561, loss 0.495639.
Train: 2018-08-08T23:37:10.890237: step 5562, loss 0.678934.
Train: 2018-08-08T23:37:13.977445: step 5563, loss 0.595441.
Train: 2018-08-08T23:37:17.058637: step 5564, loss 0.579138.
Train: 2018-08-08T23:37:20.159882: step 5565, loss 0.496436.
Train: 2018-08-08T23:37:23.204978: step 5566, loss 0.480237.
Train: 2018-08-08T23:37:26.276144: step 5567, loss 0.496473.
Train: 2018-08-08T23:37:29.356333: step 5568, loss 0.678447.
Train: 2018-08-08T23:37:32.429504: step 5569, loss 0.479918.
Train: 2018-08-08T23:37:35.494653: step 5570, loss 0.579203.
Test: 2018-08-08T23:37:56.373164: step 5570, loss 0.548262.
Train: 2018-08-08T23:37:59.438313: step 5571, loss 0.529725.
Train: 2018-08-08T23:38:02.468369: step 5572, loss 0.595938.
Train: 2018-08-08T23:38:05.524494: step 5573, loss 0.628531.
Train: 2018-08-08T23:38:08.588641: step 5574, loss 0.595606.
Train: 2018-08-08T23:38:11.667828: step 5575, loss 0.694016.
Train: 2018-08-08T23:38:14.720945: step 5576, loss 0.546221.
Train: 2018-08-08T23:38:17.778073: step 5577, loss 0.562618.
Train: 2018-08-08T23:38:20.889345: step 5578, loss 0.513851.
Train: 2018-08-08T23:38:23.924415: step 5579, loss 0.465235.
Train: 2018-08-08T23:38:26.987559: step 5580, loss 0.611392.
Test: 2018-08-08T23:38:47.866069: step 5580, loss 0.549306.
Train: 2018-08-08T23:38:50.986365: step 5581, loss 0.53021.
Train: 2018-08-08T23:38:54.068560: step 5582, loss 0.578905.
Train: 2018-08-08T23:38:57.115661: step 5583, loss 0.530223.
Train: 2018-08-08T23:39:00.177802: step 5584, loss 0.432813.
Train: 2018-08-08T23:39:03.256989: step 5585, loss 0.546369.
Train: 2018-08-08T23:39:06.317125: step 5586, loss 0.660509.
Train: 2018-08-08T23:39:09.381272: step 5587, loss 0.611585.
Train: 2018-08-08T23:39:12.443413: step 5588, loss 0.611582.
Train: 2018-08-08T23:39:15.522600: step 5589, loss 0.54632.
Train: 2018-08-08T23:39:18.589755: step 5590, loss 0.595217.
Test: 2018-08-08T23:39:39.474281: step 5590, loss 0.547394.
Train: 2018-08-08T23:39:42.529403: step 5591, loss 0.676566.
Train: 2018-08-08T23:39:45.644686: step 5592, loss 0.611356.
Train: 2018-08-08T23:39:48.693793: step 5593, loss 0.481875.
Train: 2018-08-08T23:39:51.727860: step 5594, loss 0.530461.
Train: 2018-08-08T23:39:54.790001: step 5595, loss 0.627256.
Train: 2018-08-08T23:39:57.839108: step 5596, loss 0.627156.
Train: 2018-08-08T23:40:00.935340: step 5597, loss 0.56282.
Train: 2018-08-08T23:40:04.013524: step 5598, loss 0.674913.
Train: 2018-08-08T23:40:07.040572: step 5599, loss 0.531034.
Train: 2018-08-08T23:40:10.112740: step 5600, loss 0.531182.
Test: 2018-08-08T23:40:30.993256: step 5600, loss 0.55174.
Train: 2018-08-08T23:40:35.904313: step 5601, loss 0.483709.
Train: 2018-08-08T23:40:38.992523: step 5602, loss 0.610563.
Train: 2018-08-08T23:40:42.042633: step 5603, loss 0.594694.
Train: 2018-08-08T23:40:45.115804: step 5604, loss 0.65792.
Train: 2018-08-08T23:40:48.189977: step 5605, loss 0.563099.
Train: 2018-08-08T23:40:51.231062: step 5606, loss 0.531689.
Train: 2018-08-08T23:40:54.272148: step 5607, loss 0.468954.
Train: 2018-08-08T23:40:57.317244: step 5608, loss 0.673133.
Train: 2018-08-08T23:41:00.349305: step 5609, loss 0.500435.
Train: 2018-08-08T23:41:03.415457: step 5610, loss 0.531811.
Test: 2018-08-08T23:41:24.289957: step 5610, loss 0.549652.
Train: 2018-08-08T23:41:27.392205: step 5611, loss 0.484681.
Train: 2018-08-08T23:41:30.472394: step 5612, loss 0.641807.
Train: 2018-08-08T23:41:33.523506: step 5613, loss 0.468648.
Train: 2018-08-08T23:41:36.590661: step 5614, loss 0.48415.
Train: 2018-08-08T23:41:39.662829: step 5615, loss 0.563014.
Train: 2018-08-08T23:41:42.742016: step 5616, loss 0.658361.
Train: 2018-08-08T23:41:45.899410: step 5617, loss 0.562934.
Train: 2018-08-08T23:41:48.982608: step 5618, loss 0.642654.
Train: 2018-08-08T23:41:52.055779: step 5619, loss 0.451272.
Train: 2018-08-08T23:41:55.123936: step 5620, loss 0.514937.
Test: 2018-08-08T23:42:16.012473: step 5620, loss 0.549643.
Train: 2018-08-08T23:42:19.051553: step 5621, loss 0.498739.
Train: 2018-08-08T23:42:22.095646: step 5622, loss 0.691461.
Train: 2018-08-08T23:42:25.170822: step 5623, loss 0.594978.
Train: 2018-08-08T23:42:28.214916: step 5624, loss 0.48222.
Train: 2018-08-08T23:42:31.272044: step 5625, loss 0.675722.
Train: 2018-08-08T23:42:34.327166: step 5626, loss 0.643427.
Train: 2018-08-08T23:42:37.417382: step 5627, loss 0.530544.
Train: 2018-08-08T23:42:40.486543: step 5628, loss 0.53058.
Train: 2018-08-08T23:42:43.536652: step 5629, loss 0.530585.
Train: 2018-08-08T23:42:46.595785: step 5630, loss 0.578877.
Test: 2018-08-08T23:43:07.466275: step 5630, loss 0.548285.
Train: 2018-08-08T23:43:10.533429: step 5631, loss 0.530552.
Train: 2018-08-08T23:43:13.625650: step 5632, loss 0.595001.
Train: 2018-08-08T23:43:16.681776: step 5633, loss 0.546631.
Train: 2018-08-08T23:43:19.757954: step 5634, loss 0.578882.
Train: 2018-08-08T23:43:22.873237: step 5635, loss 0.514336.
Train: 2018-08-08T23:43:25.901288: step 5636, loss 0.51427.
Train: 2018-08-08T23:43:28.923323: step 5637, loss 0.595077.
Train: 2018-08-08T23:43:31.987469: step 5638, loss 0.611301.
Train: 2018-08-08T23:43:35.059637: step 5639, loss 0.530283.
Train: 2018-08-08T23:43:38.096712: step 5640, loss 0.546467.
Test: 2018-08-08T23:43:58.963190: step 5640, loss 0.54869.
Train: 2018-08-08T23:44:02.073459: step 5641, loss 0.513973.
Train: 2018-08-08T23:44:05.154652: step 5642, loss 0.67648.
Train: 2018-08-08T23:44:08.194734: step 5643, loss 0.530146.
Train: 2018-08-08T23:44:11.272918: step 5644, loss 0.676465.
Train: 2018-08-08T23:44:14.337065: step 5645, loss 0.562674.
Train: 2018-08-08T23:44:17.389180: step 5646, loss 0.562692.
Train: 2018-08-08T23:44:20.491428: step 5647, loss 0.562708.
Train: 2018-08-08T23:44:23.564598: step 5648, loss 0.546558.
Train: 2018-08-08T23:44:26.649801: step 5649, loss 0.627345.
Train: 2018-08-08T23:44:29.724977: step 5650, loss 0.562756.
Test: 2018-08-08T23:44:50.597472: step 5650, loss 0.5483.
Train: 2018-08-08T23:44:53.668637: step 5651, loss 0.530577.
Train: 2018-08-08T23:44:56.750831: step 5652, loss 0.530612.
Train: 2018-08-08T23:44:59.844055: step 5653, loss 0.546703.
Train: 2018-08-08T23:45:02.892160: step 5654, loss 0.514517.
Train: 2018-08-08T23:45:05.917202: step 5655, loss 0.594985.
Train: 2018-08-08T23:45:09.001402: step 5656, loss 0.546649.
Train: 2018-08-08T23:45:12.038477: step 5657, loss 0.611135.
Train: 2018-08-08T23:45:15.100618: step 5658, loss 0.611131.
Train: 2018-08-08T23:45:18.164765: step 5659, loss 0.498324.
Train: 2018-08-08T23:45:21.257989: step 5660, loss 0.562762.
Test: 2018-08-08T23:45:42.150537: step 5660, loss 0.54948.
Train: 2018-08-08T23:45:45.225713: step 5661, loss 0.57888.
Train: 2018-08-08T23:45:48.301892: step 5662, loss 0.562756.
Train: 2018-08-08T23:45:51.321921: step 5663, loss 0.578881.
Train: 2018-08-08T23:45:54.413140: step 5664, loss 0.595004.
Train: 2018-08-08T23:45:57.480294: step 5665, loss 0.594992.
Train: 2018-08-08T23:46:00.628665: step 5666, loss 0.546683.
Train: 2018-08-08T23:46:03.693814: step 5667, loss 0.611046.
Train: 2018-08-08T23:46:06.779017: step 5668, loss 0.530678.
Train: 2018-08-08T23:46:09.872241: step 5669, loss 0.530707.
Train: 2018-08-08T23:46:12.942404: step 5670, loss 0.466482.
Test: 2018-08-08T23:46:33.849992: step 5670, loss 0.548935.
Train: 2018-08-08T23:46:36.956250: step 5671, loss 0.530621.
Train: 2018-08-08T23:46:40.032428: step 5672, loss 0.62724.
Train: 2018-08-08T23:46:43.084543: step 5673, loss 0.611153.
Train: 2018-08-08T23:46:46.150695: step 5674, loss 0.546611.
Train: 2018-08-08T23:46:49.200805: step 5675, loss 0.578884.
Train: 2018-08-08T23:46:52.270968: step 5676, loss 0.498171.
Train: 2018-08-08T23:46:55.323082: step 5677, loss 0.627375.
Train: 2018-08-08T23:46:58.377202: step 5678, loss 0.643539.
Train: 2018-08-08T23:47:01.419290: step 5679, loss 0.449755.
Train: 2018-08-08T23:47:04.481432: step 5680, loss 0.465811.
Test: 2018-08-08T23:47:25.378993: step 5680, loss 0.551817.
Train: 2018-08-08T23:47:28.440131: step 5681, loss 0.54651.
Train: 2018-08-08T23:47:31.516310: step 5682, loss 0.481511.
Train: 2018-08-08T23:47:34.618558: step 5683, loss 0.54634.
Train: 2018-08-08T23:47:37.685713: step 5684, loss 0.546245.
Train: 2018-08-08T23:47:40.760889: step 5685, loss 0.546155.
Train: 2018-08-08T23:47:43.859126: step 5686, loss 0.480234.
Train: 2018-08-08T23:47:46.933300: step 5687, loss 0.545961.
Train: 2018-08-08T23:47:50.010481: step 5688, loss 0.512668.
Train: 2018-08-08T23:47:53.072622: step 5689, loss 0.529088.
Train: 2018-08-08T23:47:56.206956: step 5690, loss 0.528915.
Test: 2018-08-08T23:48:17.081456: step 5690, loss 0.548532.
Train: 2018-08-08T23:48:20.180695: step 5691, loss 0.579199.
Train: 2018-08-08T23:48:23.306004: step 5692, loss 0.477978.
Train: 2018-08-08T23:48:26.377170: step 5693, loss 0.579305.
Train: 2018-08-08T23:48:29.441317: step 5694, loss 0.52832.
Train: 2018-08-08T23:48:32.538551: step 5695, loss 0.511121.
Train: 2018-08-08T23:48:35.587658: step 5696, loss 0.613748.
Train: 2018-08-08T23:48:38.702941: step 5697, loss 0.442079.
Train: 2018-08-08T23:48:41.746031: step 5698, loss 0.665811.
Train: 2018-08-08T23:48:44.822210: step 5699, loss 0.631437.
Train: 2018-08-08T23:48:47.931477: step 5700, loss 0.648735.
Test: 2018-08-08T23:49:08.827034: step 5700, loss 0.546169.
Train: 2018-08-08T23:49:13.657876: step 5701, loss 0.614109.
Train: 2018-08-08T23:49:16.728039: step 5702, loss 0.510675.
Train: 2018-08-08T23:49:19.786170: step 5703, loss 0.493546.
Train: 2018-08-08T23:49:22.889420: step 5704, loss 0.613909.
Train: 2018-08-08T23:49:25.953567: step 5705, loss 0.545166.
Train: 2018-08-08T23:49:29.018717: step 5706, loss 0.579486.
Train: 2018-08-08T23:49:32.071834: step 5707, loss 0.630842.
Train: 2018-08-08T23:49:35.113922: step 5708, loss 0.596505.
Train: 2018-08-08T23:49:38.175061: step 5709, loss 0.528275.
Train: 2018-08-08T23:49:41.228178: step 5710, loss 0.494369.
Test: 2018-08-08T23:50:02.105686: step 5710, loss 0.549654.
Train: 2018-08-08T23:50:05.157800: step 5711, loss 0.494451.
Train: 2018-08-08T23:50:08.229968: step 5712, loss 0.477488.
Train: 2018-08-08T23:50:11.310158: step 5713, loss 0.579337.
Train: 2018-08-08T23:50:14.365281: step 5714, loss 0.596344.
Train: 2018-08-08T23:50:17.430430: step 5715, loss 0.579342.
Train: 2018-08-08T23:50:20.511622: step 5716, loss 0.494403.
Train: 2018-08-08T23:50:23.584793: step 5717, loss 0.562346.
Train: 2018-08-08T23:50:26.647937: step 5718, loss 0.54535.
Train: 2018-08-08T23:50:29.514558: step 5719, loss 0.634883.
Train: 2018-08-08T23:50:32.589734: step 5720, loss 0.613299.
Test: 2018-08-08T23:50:53.463231: step 5720, loss 0.549033.
Train: 2018-08-08T23:50:56.532391: step 5721, loss 0.528448.
Train: 2018-08-08T23:50:59.592527: step 5722, loss 0.494635.
Train: 2018-08-08T23:51:02.677730: step 5723, loss 0.494652.
Train: 2018-08-08T23:51:05.734858: step 5724, loss 0.647047.
Train: 2018-08-08T23:51:08.794994: step 5725, loss 0.579282.
Train: 2018-08-08T23:51:11.892229: step 5726, loss 0.613077.
Train: 2018-08-08T23:51:14.958381: step 5727, loss 0.562368.
Train: 2018-08-08T23:51:18.015509: step 5728, loss 0.562376.
Train: 2018-08-08T23:51:21.071634: step 5729, loss 0.511955.
Train: 2018-08-08T23:51:24.161850: step 5730, loss 0.545594.
Test: 2018-08-08T23:51:45.052393: step 5730, loss 0.547936.
Train: 2018-08-08T23:51:48.143611: step 5731, loss 0.679867.
Train: 2018-08-08T23:51:51.214776: step 5732, loss 0.562405.
Train: 2018-08-08T23:51:54.247840: step 5733, loss 0.612512.
Train: 2018-08-08T23:51:57.323016: step 5734, loss 0.595731.
Train: 2018-08-08T23:52:00.408219: step 5735, loss 0.545875.
Train: 2018-08-08T23:52:03.475374: step 5736, loss 0.612109.
Train: 2018-08-08T23:52:06.544534: step 5737, loss 0.546031.
Train: 2018-08-08T23:52:09.585619: step 5738, loss 0.496798.
Train: 2018-08-08T23:52:12.682854: step 5739, loss 0.628197.
Train: 2018-08-08T23:52:15.761039: step 5740, loss 0.57895.
Test: 2018-08-08T23:52:36.660605: step 5740, loss 0.551631.
Train: 2018-08-08T23:52:39.737786: step 5741, loss 0.513627.
Train: 2018-08-08T23:52:42.813965: step 5742, loss 0.66044.
Train: 2018-08-08T23:52:45.899167: step 5743, loss 0.53015.
Train: 2018-08-08T23:52:48.972338: step 5744, loss 0.562683.
Train: 2018-08-08T23:52:52.037487: step 5745, loss 0.69221.
Train: 2018-08-08T23:52:55.077570: step 5746, loss 0.514382.
Train: 2018-08-08T23:52:58.146730: step 5747, loss 0.530628.
Train: 2018-08-08T23:53:01.199847: step 5748, loss 0.53071.
Train: 2018-08-08T23:53:04.284047: step 5749, loss 0.578867.
Train: 2018-08-08T23:53:07.344183: step 5750, loss 0.498768.
Test: 2018-08-08T23:53:28.273830: step 5750, loss 0.550254.
Train: 2018-08-08T23:53:31.338979: step 5751, loss 0.578865.
Train: 2018-08-08T23:53:34.407136: step 5752, loss 0.594887.
Train: 2018-08-08T23:53:37.479304: step 5753, loss 0.530824.
Train: 2018-08-08T23:53:40.529414: step 5754, loss 0.610897.
Train: 2018-08-08T23:53:43.585539: step 5755, loss 0.578864.
Train: 2018-08-08T23:53:46.646678: step 5756, loss 0.546877.
Train: 2018-08-08T23:53:49.740905: step 5757, loss 0.450964.
Train: 2018-08-08T23:53:52.823099: step 5758, loss 0.514805.
Train: 2018-08-08T23:53:55.869198: step 5759, loss 0.57887.
Train: 2018-08-08T23:53:58.932342: step 5760, loss 0.546697.
Test: 2018-08-08T23:54:19.833914: step 5760, loss 0.548263.
Train: 2018-08-08T23:54:22.929143: step 5761, loss 0.562751.
Train: 2018-08-08T23:54:25.984266: step 5762, loss 0.546582.
Train: 2018-08-08T23:54:29.060445: step 5763, loss 0.530333.
Train: 2018-08-08T23:54:32.127599: step 5764, loss 0.530249.
Train: 2018-08-08T23:54:35.189741: step 5765, loss 0.595205.
Train: 2018-08-08T23:54:38.258900: step 5766, loss 0.49741.
Train: 2018-08-08T23:54:41.302994: step 5767, loss 0.578942.
Train: 2018-08-08T23:54:44.397221: step 5768, loss 0.513415.
Train: 2018-08-08T23:54:47.442317: step 5769, loss 0.693998.
Train: 2018-08-08T23:54:50.491424: step 5770, loss 0.480354.
Test: 2018-08-08T23:55:11.380963: step 5770, loss 0.548967.
Train: 2018-08-08T23:55:14.438091: step 5771, loss 0.496669.
Train: 2018-08-08T23:55:17.509257: step 5772, loss 0.529492.
Train: 2018-08-08T23:55:20.571398: step 5773, loss 0.529377.
Train: 2018-08-08T23:55:23.660611: step 5774, loss 0.462852.
Train: 2018-08-08T23:55:26.723755: step 5775, loss 0.512415.
Train: 2018-08-08T23:55:29.813971: step 5776, loss 0.545656.
Train: 2018-08-08T23:55:32.862075: step 5777, loss 0.495111.
Train: 2018-08-08T23:55:35.892132: step 5778, loss 0.494773.
Train: 2018-08-08T23:55:38.939233: step 5779, loss 0.664259.
Train: 2018-08-08T23:55:42.032457: step 5780, loss 0.579376.
Test: 2018-08-08T23:56:02.955085: step 5780, loss 0.546988.
Train: 2018-08-08T23:56:06.021236: step 5781, loss 0.562338.
Train: 2018-08-08T23:56:09.096413: step 5782, loss 0.579439.
Train: 2018-08-08T23:56:12.152538: step 5783, loss 0.528091.
Train: 2018-08-08T23:56:15.230722: step 5784, loss 0.545188.
Train: 2018-08-08T23:56:18.281834: step 5785, loss 0.545166.
Train: 2018-08-08T23:56:21.350994: step 5786, loss 0.562337.
Train: 2018-08-08T23:56:24.416143: step 5787, loss 0.682795.
Train: 2018-08-08T23:56:27.511373: step 5788, loss 0.562332.
Train: 2018-08-08T23:56:30.561482: step 5789, loss 0.493676.
Train: 2018-08-08T23:56:33.620615: step 5790, loss 0.528006.
Test: 2018-08-08T23:56:54.508150: step 5790, loss 0.548199.
Train: 2018-08-08T23:56:57.607390: step 5791, loss 0.54517.
Train: 2018-08-08T23:57:00.707632: step 5792, loss 0.476489.
Train: 2018-08-08T23:57:03.789827: step 5793, loss 0.545144.
Train: 2018-08-08T23:57:06.857985: step 5794, loss 0.596761.
Train: 2018-08-08T23:57:09.919123: step 5795, loss 0.579556.
Train: 2018-08-08T23:57:13.009339: step 5796, loss 0.562336.
Train: 2018-08-08T23:57:16.094542: step 5797, loss 0.682827.
Train: 2018-08-08T23:57:19.146657: step 5798, loss 0.424946.
Train: 2018-08-08T23:57:22.215817: step 5799, loss 0.613848.
Train: 2018-08-08T23:57:25.282972: step 5800, loss 0.648096.
Test: 2018-08-08T23:57:46.182538: step 5800, loss 0.545658.
Train: 2018-08-08T23:57:51.001349: step 5801, loss 0.545227.
Train: 2018-08-08T23:57:54.068504: step 5802, loss 0.528193.
Train: 2018-08-08T23:57:57.147691: step 5803, loss 0.630523.
Train: 2018-08-08T23:58:00.174739: step 5804, loss 0.647348.
Train: 2018-08-08T23:58:03.243899: step 5805, loss 0.57929.
Train: 2018-08-08T23:58:06.313059: step 5806, loss 0.629845.
Train: 2018-08-08T23:58:09.389238: step 5807, loss 0.595968.
Train: 2018-08-08T23:58:12.449374: step 5808, loss 0.562418.
Train: 2018-08-08T23:58:15.522544: step 5809, loss 0.595708.
Train: 2018-08-08T23:58:18.610755: step 5810, loss 0.545926.
Test: 2018-08-08T23:58:39.502300: step 5810, loss 0.549559.
Train: 2018-08-08T23:58:42.565444: step 5811, loss 0.562513.
Train: 2018-08-08T23:58:45.661676: step 5812, loss 0.611824.
Train: 2018-08-08T23:58:48.733844: step 5813, loss 0.595303.
Train: 2018-08-08T23:58:51.807015: step 5814, loss 0.57892.
Train: 2018-08-08T23:58:54.902244: step 5815, loss 0.578901.
Train: 2018-08-08T23:58:57.955362: step 5816, loss 0.659645.
Train: 2018-08-08T23:59:01.051594: step 5817, loss 0.562806.
Train: 2018-08-08T23:59:04.099698: step 5818, loss 0.594857.
Train: 2018-08-08T23:59:07.145797: step 5819, loss 0.562943.
Train: 2018-08-08T23:59:10.215959: step 5820, loss 0.547166.
Test: 2018-08-08T23:59:31.138588: step 5820, loss 0.550661.
Train: 2018-08-08T23:59:34.227800: step 5821, loss 0.610447.
Train: 2018-08-08T23:59:37.274901: step 5822, loss 0.547405.
Train: 2018-08-08T23:59:40.343059: step 5823, loss 0.500455.
Train: 2018-08-08T23:59:43.412219: step 5824, loss 0.563218.
Train: 2018-08-08T23:59:46.463331: step 5825, loss 0.61017.
Train: 2018-08-08T23:59:49.553547: step 5826, loss 0.610121.
Train: 2018-08-08T23:59:52.618696: step 5827, loss 0.547734.
Train: 2018-08-08T23:59:55.687857: step 5828, loss 0.563346.
Train: 2018-08-08T23:59:58.761027: step 5829, loss 0.563371.
Train: 2018-08-09T00:00:01.856256: step 5830, loss 0.532344.
Test: 2018-08-09T00:00:22.757828: step 5830, loss 0.550021.
Train: 2018-08-09T00:00:25.807937: step 5831, loss 0.516825.
Train: 2018-08-09T00:00:28.894143: step 5832, loss 0.485683.
Train: 2018-08-09T00:00:31.922193: step 5833, loss 0.501011.
Train: 2018-08-09T00:00:35.024441: step 5834, loss 0.563251.
Train: 2018-08-09T00:00:38.103628: step 5835, loss 0.594562.
Train: 2018-08-09T00:00:41.160756: step 5836, loss 0.547411.
Train: 2018-08-09T00:00:44.210866: step 5837, loss 0.547318.
Train: 2018-08-09T00:00:47.283034: step 5838, loss 0.531403.
Train: 2018-08-09T00:00:50.364226: step 5839, loss 0.515373.
Train: 2018-08-09T00:00:53.426367: step 5840, loss 0.594793.
Test: 2018-08-09T00:01:14.346989: step 5840, loss 0.547904.
Train: 2018-08-09T00:01:17.407125: step 5841, loss 0.514931.
Train: 2018-08-09T00:01:20.493331: step 5842, loss 0.482619.
Train: 2018-08-09T00:01:23.546448: step 5843, loss 0.49829.
Train: 2018-08-09T00:01:26.625635: step 5844, loss 0.546487.
Train: 2018-08-09T00:01:29.679755: step 5845, loss 0.497476.
Train: 2018-08-09T00:01:32.761950: step 5846, loss 0.595338.
Train: 2018-08-09T00:01:35.826096: step 5847, loss 0.513155.
Train: 2018-08-09T00:01:38.880216: step 5848, loss 0.595564.
Train: 2018-08-09T00:01:41.961408: step 5849, loss 0.562458.
Train: 2018-08-09T00:01:45.005502: step 5850, loss 0.595747.
Test: 2018-08-09T00:02:05.889026: step 5850, loss 0.549303.
Train: 2018-08-09T00:02:08.966207: step 5851, loss 0.512335.
Train: 2018-08-09T00:02:12.023335: step 5852, loss 0.528918.
Train: 2018-08-09T00:02:15.073444: step 5853, loss 0.512009.
Train: 2018-08-09T00:02:18.134583: step 5854, loss 0.545523.
Train: 2018-08-09T00:02:21.206751: step 5855, loss 0.613074.
Train: 2018-08-09T00:02:24.256860: step 5856, loss 0.494607.
Train: 2018-08-09T00:02:27.319002: step 5857, loss 0.443474.
Train: 2018-08-09T00:02:30.389164: step 5858, loss 0.613499.
Train: 2018-08-09T00:02:33.451306: step 5859, loss 0.545238.
Train: 2018-08-09T00:02:36.500413: step 5860, loss 0.562337.
Test: 2018-08-09T00:02:57.449110: step 5860, loss 0.548188.
Train: 2018-08-09T00:03:00.526291: step 5861, loss 0.510801.
Train: 2018-08-09T00:03:03.572390: step 5862, loss 0.613999.
Train: 2018-08-09T00:03:06.649571: step 5863, loss 0.614064.
Train: 2018-08-09T00:03:09.754827: step 5864, loss 0.510604.
Train: 2018-08-09T00:03:12.818974: step 5865, loss 0.562338.
Train: 2018-08-09T00:03:15.898160: step 5866, loss 0.61413.
Train: 2018-08-09T00:03:18.966318: step 5867, loss 0.527831.
Train: 2018-08-09T00:03:22.068566: step 5868, loss 0.545089.
Train: 2018-08-09T00:03:25.148755: step 5869, loss 0.527842.
Train: 2018-08-09T00:03:28.209894: step 5870, loss 0.59684.
Test: 2018-08-09T00:03:49.182655: step 5870, loss 0.54553.
Train: 2018-08-09T00:03:52.236775: step 5871, loss 0.596818.
Train: 2018-08-09T00:03:55.311951: step 5872, loss 0.476236.
Train: 2018-08-09T00:03:58.406177: step 5873, loss 0.493457.
Train: 2018-08-09T00:04:01.490377: step 5874, loss 0.47616.
Train: 2018-08-09T00:04:04.545500: step 5875, loss 0.562339.
Train: 2018-08-09T00:04:07.630703: step 5876, loss 0.545047.
Train: 2018-08-09T00:04:10.720919: step 5877, loss 0.562344.
Train: 2018-08-09T00:04:13.796095: step 5878, loss 0.562346.
Train: 2018-08-09T00:04:16.867260: step 5879, loss 0.701139.
Train: 2018-08-09T00:04:19.933412: step 5880, loss 0.545024.
Test: 2018-08-09T00:04:40.919208: step 5880, loss 0.549407.
Train: 2018-08-09T00:04:44.015440: step 5881, loss 0.614225.
Train: 2018-08-09T00:04:47.091618: step 5882, loss 0.648609.
Train: 2018-08-09T00:04:50.141728: step 5883, loss 0.562334.
Train: 2018-08-09T00:04:53.210888: step 5884, loss 0.596601.
Train: 2018-08-09T00:04:56.301104: step 5885, loss 0.630604.
Train: 2018-08-09T00:04:59.392323: step 5886, loss 0.528374.
Train: 2018-08-09T00:05:02.460480: step 5887, loss 0.613119.
Train: 2018-08-09T00:05:05.524627: step 5888, loss 0.629754.
Train: 2018-08-09T00:05:08.572731: step 5889, loss 0.562397.
Train: 2018-08-09T00:05:11.651917: step 5890, loss 0.729172.
Test: 2018-08-09T00:05:32.637713: step 5890, loss 0.5501.
Train: 2018-08-09T00:05:35.704868: step 5891, loss 0.661794.
Train: 2018-08-09T00:05:38.827169: step 5892, loss 0.546138.
Train: 2018-08-09T00:05:41.890313: step 5893, loss 0.611408.
Train: 2018-08-09T00:05:44.950449: step 5894, loss 0.562641.
Train: 2018-08-09T00:05:48.016601: step 5895, loss 0.61055.
Train: 2018-08-09T00:05:51.059692: step 5896, loss 0.626548.
Train: 2018-08-09T00:05:54.117822: step 5897, loss 0.532964.
Train: 2018-08-09T00:05:57.185980: step 5898, loss 0.641519.
Train: 2018-08-09T00:06:00.280206: step 5899, loss 0.594212.
Train: 2018-08-09T00:06:03.403511: step 5900, loss 0.564131.
Test: 2018-08-09T00:06:24.320123: step 5900, loss 0.550384.
Train: 2018-08-09T00:06:29.246219: step 5901, loss 0.563508.
Train: 2018-08-09T00:06:32.296329: step 5902, loss 0.548535.
Train: 2018-08-09T00:06:35.371505: step 5903, loss 0.624707.
Train: 2018-08-09T00:06:38.453699: step 5904, loss 0.533525.
Train: 2018-08-09T00:06:41.525867: step 5905, loss 0.53332.
Train: 2018-08-09T00:06:44.558931: step 5906, loss 0.487783.
Train: 2018-08-09T00:06:47.617062: step 5907, loss 0.533409.
Train: 2018-08-09T00:06:50.653134: step 5908, loss 0.502905.
Train: 2018-08-09T00:06:53.729313: step 5909, loss 0.563756.
Train: 2018-08-09T00:06:56.819529: step 5910, loss 0.502513.
Test: 2018-08-09T00:07:17.726114: step 5910, loss 0.549239.
Train: 2018-08-09T00:07:20.821343: step 5911, loss 0.486866.
Train: 2018-08-09T00:07:23.919581: step 5912, loss 0.563518.
Train: 2018-08-09T00:07:26.976708: step 5913, loss 0.501432.
Train: 2018-08-09T00:07:30.070935: step 5914, loss 0.485401.
Train: 2018-08-09T00:07:33.110016: step 5915, loss 0.625929.
Train: 2018-08-09T00:07:36.156114: step 5916, loss 0.515824.
Train: 2018-08-09T00:07:39.260367: step 5917, loss 0.578859.
Train: 2018-08-09T00:07:42.339554: step 5918, loss 0.562942.
Train: 2018-08-09T00:07:45.429770: step 5919, loss 0.530909.
Train: 2018-08-09T00:07:48.499933: step 5920, loss 0.562814.
Test: 2018-08-09T00:08:09.402508: step 5920, loss 0.550095.
Train: 2018-08-09T00:08:12.474675: step 5921, loss 0.659477.
Train: 2018-08-09T00:08:15.559878: step 5922, loss 0.433526.
Train: 2018-08-09T00:08:18.651096: step 5923, loss 0.595117.
Train: 2018-08-09T00:08:21.747329: step 5924, loss 0.51385.
Train: 2018-08-09T00:08:24.833534: step 5925, loss 0.546284.
Train: 2018-08-09T00:08:27.880635: step 5926, loss 0.595337.
Train: 2018-08-09T00:08:30.932750: step 5927, loss 0.513274.
Train: 2018-08-09T00:08:34.009931: step 5928, loss 0.677848.
Train: 2018-08-09T00:08:37.075081: step 5929, loss 0.529533.
Train: 2018-08-09T00:08:40.138225: step 5930, loss 0.612032.
Test: 2018-08-09T00:09:01.041802: step 5930, loss 0.548275.
Train: 2018-08-09T00:09:04.098930: step 5931, loss 0.612042.
Train: 2018-08-09T00:09:07.180122: step 5932, loss 0.529497.
Train: 2018-08-09T00:09:10.255298: step 5933, loss 0.612014.
Train: 2018-08-09T00:09:13.310421: step 5934, loss 0.57899.
Train: 2018-08-09T00:09:16.376573: step 5935, loss 0.628395.
Train: 2018-08-09T00:09:19.454757: step 5936, loss 0.578994.
Train: 2018-08-09T00:09:22.522914: step 5937, loss 0.562518.
Train: 2018-08-09T00:09:25.590069: step 5938, loss 0.546222.
Train: 2018-08-09T00:09:28.655218: step 5939, loss 0.546292.
Train: 2018-08-09T00:09:31.727386: step 5940, loss 0.546273.
Test: 2018-08-09T00:09:52.648009: step 5940, loss 0.547945.
Train: 2018-08-09T00:09:55.749254: step 5941, loss 0.578881.
Train: 2018-08-09T00:09:58.846488: step 5942, loss 0.578883.
Train: 2018-08-09T00:10:01.929686: step 5943, loss 0.61157.
Train: 2018-08-09T00:10:04.994835: step 5944, loss 0.497698.
Train: 2018-08-09T00:10:08.057979: step 5945, loss 0.595152.
Train: 2018-08-09T00:10:11.154211: step 5946, loss 0.643835.
Train: 2018-08-09T00:10:14.233398: step 5947, loss 0.595027.
Train: 2018-08-09T00:10:17.296542: step 5948, loss 0.611184.
Train: 2018-08-09T00:10:20.368710: step 5949, loss 0.418064.
Train: 2018-08-09T00:10:23.465945: step 5950, loss 0.594956.
Test: 2018-08-09T00:10:44.367517: step 5950, loss 0.550795.
Train: 2018-08-09T00:10:47.453721: step 5951, loss 0.594932.
Train: 2018-08-09T00:10:50.533911: step 5952, loss 0.546779.
Train: 2018-08-09T00:10:53.590036: step 5953, loss 0.546809.
Train: 2018-08-09T00:10:56.642151: step 5954, loss 0.578869.
Train: 2018-08-09T00:10:59.727354: step 5955, loss 0.530809.
Train: 2018-08-09T00:11:02.810551: step 5956, loss 0.5308.
Train: 2018-08-09T00:11:05.890740: step 5957, loss 0.562832.
Train: 2018-08-09T00:11:08.984967: step 5958, loss 0.578868.
Train: 2018-08-09T00:11:12.082202: step 5959, loss 0.514648.
Train: 2018-08-09T00:11:15.164397: step 5960, loss 0.675334.
Test: 2018-08-09T00:11:36.092038: step 5960, loss 0.550182.
Train: 2018-08-09T00:11:39.162200: step 5961, loss 0.594936.
Train: 2018-08-09T00:11:42.248405: step 5962, loss 0.530728.
Train: 2018-08-09T00:11:45.337619: step 5963, loss 0.626989.
Train: 2018-08-09T00:11:48.421819: step 5964, loss 0.578865.
Train: 2018-08-09T00:11:51.495992: step 5965, loss 0.594856.
Train: 2018-08-09T00:11:54.546101: step 5966, loss 0.53097.
Train: 2018-08-09T00:11:57.627294: step 5967, loss 0.499124.
Train: 2018-08-09T00:12:00.720518: step 5968, loss 0.690524.
Train: 2018-08-09T00:12:03.786670: step 5969, loss 0.610702.
Train: 2018-08-09T00:12:06.871873: step 5970, loss 0.547095.
Test: 2018-08-09T00:12:27.786479: step 5970, loss 0.551151.
Train: 2018-08-09T00:12:30.908780: step 5971, loss 0.547155.
Train: 2018-08-09T00:12:33.965908: step 5972, loss 0.483869.
Train: 2018-08-09T00:12:37.049105: step 5973, loss 0.57886.
Train: 2018-08-09T00:12:40.094201: step 5974, loss 0.467973.
Train: 2018-08-09T00:12:43.129271: step 5975, loss 0.610602.
Train: 2018-08-09T00:12:46.206452: step 5976, loss 0.483524.
Train: 2018-08-09T00:12:49.299676: step 5977, loss 0.451431.
Train: 2018-08-09T00:12:52.369839: step 5978, loss 0.466883.
Train: 2018-08-09T00:12:55.472087: step 5979, loss 0.57887.
Train: 2018-08-09T00:12:58.531220: step 5980, loss 0.498046.
Test: 2018-08-09T00:13:19.444824: step 5980, loss 0.5468.
Train: 2018-08-09T00:13:22.539051: step 5981, loss 0.562642.
Train: 2018-08-09T00:13:25.591165: step 5982, loss 0.595415.
Train: 2018-08-09T00:13:28.686395: step 5983, loss 0.529769.
Train: 2018-08-09T00:13:31.751544: step 5984, loss 0.513142.
Train: 2018-08-09T00:13:34.818699: step 5985, loss 0.595562.
Train: 2018-08-09T00:13:37.880840: step 5986, loss 0.562448.
Train: 2018-08-09T00:13:40.931952: step 5987, loss 0.628999.
Train: 2018-08-09T00:13:44.008131: step 5988, loss 0.529107.
Train: 2018-08-09T00:13:47.068267: step 5989, loss 0.462311.
Train: 2018-08-09T00:13:50.154472: step 5990, loss 0.512214.
Test: 2018-08-09T00:14:11.063063: step 5990, loss 0.547911.
Train: 2018-08-09T00:14:14.129215: step 5991, loss 0.629558.
Train: 2018-08-09T00:14:17.225446: step 5992, loss 0.495058.
Train: 2018-08-09T00:14:20.274553: step 5993, loss 0.663563.
Train: 2018-08-09T00:14:23.390839: step 5994, loss 0.511716.
Train: 2018-08-09T00:14:26.451977: step 5995, loss 0.562336.
Train: 2018-08-09T00:14:29.510108: step 5996, loss 0.528399.
Train: 2018-08-09T00:14:32.595311: step 5997, loss 0.545468.
Train: 2018-08-09T00:14:35.673495: step 5998, loss 0.511254.
Train: 2018-08-09T00:14:38.762708: step 5999, loss 0.613632.
Train: 2018-08-09T00:14:41.844903: step 6000, loss 0.511314.
Test: 2018-08-09T00:15:02.754496: step 6000, loss 0.54635.
Train: 2018-08-09T00:15:07.675579: step 6001, loss 0.579405.
Train: 2018-08-09T00:15:10.745742: step 6002, loss 0.61364.
Train: 2018-08-09T00:15:13.815905: step 6003, loss 0.511213.
Train: 2018-08-09T00:15:16.916147: step 6004, loss 0.681637.
Train: 2018-08-09T00:15:19.964252: step 6005, loss 0.528324.
Train: 2018-08-09T00:15:23.031406: step 6006, loss 0.49439.
Train: 2018-08-09T00:15:26.117612: step 6007, loss 0.596328.
Train: 2018-08-09T00:15:29.173737: step 6008, loss 0.460487.
Train: 2018-08-09T00:15:32.249916: step 6009, loss 0.647293.
Train: 2018-08-09T00:15:35.312057: step 6010, loss 0.528399.
Test: 2018-08-09T00:15:56.198589: step 6010, loss 0.54645.
Train: 2018-08-09T00:15:59.278778: step 6011, loss 0.664164.
Train: 2018-08-09T00:16:02.376013: step 6012, loss 0.528487.
Train: 2018-08-09T00:16:05.458208: step 6013, loss 0.697634.
Train: 2018-08-09T00:16:08.503304: step 6014, loss 0.562374.
Train: 2018-08-09T00:16:11.572464: step 6015, loss 0.528804.
Train: 2018-08-09T00:16:14.667693: step 6016, loss 0.629396.
Train: 2018-08-09T00:16:17.740864: step 6017, loss 0.562422.
Train: 2018-08-09T00:16:20.823058: step 6018, loss 0.612349.
Train: 2018-08-09T00:16:23.902245: step 6019, loss 0.562471.
Train: 2018-08-09T00:16:26.804963: step 6020, loss 0.474441.
Test: 2018-08-09T00:16:47.704529: step 6020, loss 0.548321.
Train: 2018-08-09T00:16:50.790734: step 6021, loss 0.513066.
Train: 2018-08-09T00:16:53.848865: step 6022, loss 0.562544.
Train: 2018-08-09T00:16:56.920030: step 6023, loss 0.628352.
Train: 2018-08-09T00:17:00.033308: step 6024, loss 0.562549.
Train: 2018-08-09T00:17:03.094446: step 6025, loss 0.529757.
Train: 2018-08-09T00:17:06.176641: step 6026, loss 0.529803.
Train: 2018-08-09T00:17:09.261844: step 6027, loss 0.529816.
Train: 2018-08-09T00:17:12.304934: step 6028, loss 0.447948.
Train: 2018-08-09T00:17:15.409188: step 6029, loss 0.546143.
Train: 2018-08-09T00:17:18.513441: step 6030, loss 0.529625.
Test: 2018-08-09T00:17:39.447098: step 6030, loss 0.549552.
Train: 2018-08-09T00:17:42.535309: step 6031, loss 0.546019.
Train: 2018-08-09T00:17:45.584415: step 6032, loss 0.628618.
Train: 2018-08-09T00:17:48.668615: step 6033, loss 0.512833.
Train: 2018-08-09T00:17:51.742789: step 6034, loss 0.529313.
Train: 2018-08-09T00:17:54.802925: step 6035, loss 0.479391.
Train: 2018-08-09T00:17:57.929237: step 6036, loss 0.52911.
Train: 2018-08-09T00:18:01.007421: step 6037, loss 0.512262.
Train: 2018-08-09T00:18:04.061541: step 6038, loss 0.528815.
Train: 2018-08-09T00:18:07.152760: step 6039, loss 0.511829.
Train: 2018-08-09T00:18:10.277066: step 6040, loss 0.613149.
Test: 2018-08-09T00:18:31.184654: step 6040, loss 0.549662.
Train: 2018-08-09T00:18:34.264843: step 6041, loss 0.562296.
Train: 2018-08-09T00:18:37.342025: step 6042, loss 0.613382.
Train: 2018-08-09T00:18:40.487387: step 6043, loss 0.664544.
Train: 2018-08-09T00:18:43.575598: step 6044, loss 0.647321.
Train: 2018-08-09T00:18:46.654785: step 6045, loss 0.56235.
Train: 2018-08-09T00:18:49.703891: step 6046, loss 0.494683.
Train: 2018-08-09T00:18:52.783078: step 6047, loss 0.511654.
Train: 2018-08-09T00:18:55.844217: step 6048, loss 0.511668.
Train: 2018-08-09T00:18:58.868257: step 6049, loss 0.579266.
Train: 2018-08-09T00:19:01.901321: step 6050, loss 0.56236.
Test: 2018-08-09T00:19:22.831970: step 6050, loss 0.550356.
Train: 2018-08-09T00:19:25.897119: step 6051, loss 0.444007.
Train: 2018-08-09T00:19:28.988338: step 6052, loss 0.562364.
Train: 2018-08-09T00:19:32.079557: step 6053, loss 0.528418.
Train: 2018-08-09T00:19:35.169773: step 6054, loss 0.528352.
Train: 2018-08-09T00:19:38.254975: step 6055, loss 0.715611.
Train: 2018-08-09T00:19:41.312104: step 6056, loss 0.630332.
Train: 2018-08-09T00:19:44.393295: step 6057, loss 0.630211.
Train: 2018-08-09T00:19:47.488525: step 6058, loss 0.579265.
Train: 2018-08-09T00:19:50.579743: step 6059, loss 0.494934.
Train: 2018-08-09T00:19:53.662941: step 6060, loss 0.646499.
Test: 2018-08-09T00:20:14.594593: step 6060, loss 0.547956.
Train: 2018-08-09T00:20:17.707869: step 6061, loss 0.763569.
Train: 2018-08-09T00:20:20.788059: step 6062, loss 0.479174.
Train: 2018-08-09T00:20:23.885294: step 6063, loss 0.529273.
Train: 2018-08-09T00:20:26.961472: step 6064, loss 0.512894.
Train: 2018-08-09T00:20:30.020606: step 6065, loss 0.529587.
Train: 2018-08-09T00:20:33.077734: step 6066, loss 0.529612.
Train: 2018-08-09T00:20:36.149902: step 6067, loss 0.562539.
Train: 2018-08-09T00:20:39.312310: step 6068, loss 0.611891.
Train: 2018-08-09T00:20:42.400521: step 6069, loss 0.513293.
Train: 2018-08-09T00:20:45.479707: step 6070, loss 0.513311.
Test: 2018-08-09T00:21:06.404341: step 6070, loss 0.551505.
Train: 2018-08-09T00:21:09.486535: step 6071, loss 0.480444.
Train: 2018-08-09T00:21:12.551684: step 6072, loss 0.611946.
Train: 2018-08-09T00:21:15.643906: step 6073, loss 0.529615.
Train: 2018-08-09T00:21:18.681983: step 6074, loss 0.546014.
Train: 2018-08-09T00:21:21.761170: step 6075, loss 0.579009.
Train: 2018-08-09T00:21:24.886479: step 6076, loss 0.546005.
Train: 2018-08-09T00:21:27.932578: step 6077, loss 0.430403.
Train: 2018-08-09T00:21:30.963636: step 6078, loss 0.496177.
Train: 2018-08-09T00:21:34.029788: step 6079, loss 0.495751.
Train: 2018-08-09T00:21:37.120004: step 6080, loss 0.495549.
Test: 2018-08-09T00:21:58.048648: step 6080, loss 0.549112.
Train: 2018-08-09T00:22:01.130843: step 6081, loss 0.444061.
Train: 2018-08-09T00:22:04.199000: step 6082, loss 0.681399.
Train: 2018-08-09T00:22:07.271168: step 6083, loss 0.595992.
Train: 2018-08-09T00:22:10.330301: step 6084, loss 0.545276.
Train: 2018-08-09T00:22:13.403472: step 6085, loss 0.665169.
Train: 2018-08-09T00:22:16.455587: step 6086, loss 0.631276.
Train: 2018-08-09T00:22:19.523744: step 6087, loss 0.545154.
Train: 2018-08-09T00:22:22.605939: step 6088, loss 0.613893.
Train: 2018-08-09T00:22:25.692144: step 6089, loss 0.528133.
Train: 2018-08-09T00:22:28.771331: step 6090, loss 0.528148.
Test: 2018-08-09T00:22:49.686941: step 6090, loss 0.550282.
Train: 2018-08-09T00:22:52.793199: step 6091, loss 0.612962.
Train: 2018-08-09T00:22:55.893441: step 6092, loss 0.562778.
Train: 2018-08-09T00:22:58.964607: step 6093, loss 0.562584.
Train: 2018-08-09T00:23:02.061841: step 6094, loss 0.511669.
Train: 2018-08-09T00:23:05.129999: step 6095, loss 0.494938.
Train: 2018-08-09T00:23:08.206177: step 6096, loss 0.562459.
Train: 2018-08-09T00:23:11.280351: step 6097, loss 0.478229.
Train: 2018-08-09T00:23:14.320434: step 6098, loss 0.461287.
Train: 2018-08-09T00:23:17.394607: step 6099, loss 0.596123.
Train: 2018-08-09T00:23:20.461762: step 6100, loss 0.596271.
Test: 2018-08-09T00:23:41.401435: step 6100, loss 0.549048.
Train: 2018-08-09T00:23:46.210220: step 6101, loss 0.663967.
Train: 2018-08-09T00:23:49.279380: step 6102, loss 0.61312.
Train: 2018-08-09T00:23:52.356561: step 6103, loss 0.596128.
Train: 2018-08-09T00:23:55.432740: step 6104, loss 0.680264.
Train: 2018-08-09T00:23:58.523958: step 6105, loss 0.61272.
Train: 2018-08-09T00:24:01.595124: step 6106, loss 0.612515.
Train: 2018-08-09T00:24:04.660273: step 6107, loss 0.612297.
Train: 2018-08-09T00:24:07.756505: step 6108, loss 0.562493.
Train: 2018-08-09T00:24:10.847724: step 6109, loss 0.480313.
Train: 2018-08-09T00:24:13.935935: step 6110, loss 0.513381.
Test: 2018-08-09T00:24:34.839512: step 6110, loss 0.547872.
Train: 2018-08-09T00:24:37.900650: step 6111, loss 0.562584.
Train: 2018-08-09T00:24:40.974823: step 6112, loss 0.660602.
Train: 2018-08-09T00:24:44.037968: step 6113, loss 0.627759.
Train: 2018-08-09T00:24:47.109133: step 6114, loss 0.497831.
Train: 2018-08-09T00:24:50.178293: step 6115, loss 0.675927.
Train: 2018-08-09T00:24:53.267506: step 6116, loss 0.546666.
Train: 2018-08-09T00:24:56.369754: step 6117, loss 0.498624.
Train: 2018-08-09T00:24:59.404824: step 6118, loss 0.530813.
Train: 2018-08-09T00:25:02.464960: step 6119, loss 0.434866.
Train: 2018-08-09T00:25:05.494013: step 6120, loss 0.546825.
Test: 2018-08-09T00:25:26.442711: step 6120, loss 0.548398.
Train: 2018-08-09T00:25:29.538942: step 6121, loss 0.659087.
Train: 2018-08-09T00:25:32.602086: step 6122, loss 0.514709.
Train: 2018-08-09T00:25:35.662222: step 6123, loss 0.546769.
Train: 2018-08-09T00:25:38.732385: step 6124, loss 0.530676.
Train: 2018-08-09T00:25:41.806558: step 6125, loss 0.562786.
Train: 2018-08-09T00:25:44.891761: step 6126, loss 0.578878.
Train: 2018-08-09T00:25:47.968942: step 6127, loss 0.643385.
Train: 2018-08-09T00:25:51.019052: step 6128, loss 0.466049.
Train: 2018-08-09T00:25:54.086206: step 6129, loss 0.595024.
Train: 2018-08-09T00:25:57.129297: step 6130, loss 0.578883.
Test: 2018-08-09T00:26:18.106069: step 6130, loss 0.548816.
Train: 2018-08-09T00:26:21.208317: step 6131, loss 0.562729.
Train: 2018-08-09T00:26:24.265445: step 6132, loss 0.465756.
Train: 2018-08-09T00:26:27.327586: step 6133, loss 0.611298.
Train: 2018-08-09T00:26:30.408778: step 6134, loss 0.57891.
Train: 2018-08-09T00:26:33.512029: step 6135, loss 0.530244.
Train: 2018-08-09T00:26:36.586202: step 6136, loss 0.578905.
Train: 2018-08-09T00:26:39.684440: step 6137, loss 0.57891.
Train: 2018-08-09T00:26:42.755605: step 6138, loss 0.611425.
Train: 2018-08-09T00:26:45.834792: step 6139, loss 0.595159.
Train: 2018-08-09T00:26:48.920997: step 6140, loss 0.530199.
Test: 2018-08-09T00:27:09.861674: step 6140, loss 0.548694.
Train: 2018-08-09T00:27:12.939857: step 6141, loss 0.48152.
Train: 2018-08-09T00:27:16.024057: step 6142, loss 0.497663.
Train: 2018-08-09T00:27:19.094219: step 6143, loss 0.530063.
Train: 2018-08-09T00:27:22.138313: step 6144, loss 0.644244.
Train: 2018-08-09T00:27:25.227526: step 6145, loss 0.578934.
Train: 2018-08-09T00:27:28.308718: step 6146, loss 0.627983.
Train: 2018-08-09T00:27:31.370860: step 6147, loss 0.546254.
Train: 2018-08-09T00:27:34.466089: step 6148, loss 0.529938.
Train: 2018-08-09T00:27:37.520209: step 6149, loss 0.546265.
Train: 2018-08-09T00:27:40.600398: step 6150, loss 0.513535.
Test: 2018-08-09T00:28:01.527037: step 6150, loss 0.549096.
Train: 2018-08-09T00:28:04.628282: step 6151, loss 0.546234.
Train: 2018-08-09T00:28:07.741559: step 6152, loss 0.546107.
Train: 2018-08-09T00:28:10.858847: step 6153, loss 0.578959.
Train: 2018-08-09T00:28:13.946055: step 6154, loss 0.546086.
Train: 2018-08-09T00:28:17.033263: step 6155, loss 0.578967.
Train: 2018-08-09T00:28:20.117464: step 6156, loss 0.595525.
Train: 2018-08-09T00:28:23.177599: step 6157, loss 0.480089.
Train: 2018-08-09T00:28:26.257789: step 6158, loss 0.595598.
Train: 2018-08-09T00:28:29.326949: step 6159, loss 0.529563.
Train: 2018-08-09T00:28:32.391096: step 6160, loss 0.595551.
Test: 2018-08-09T00:28:53.305702: step 6160, loss 0.547642.
Train: 2018-08-09T00:28:56.355811: step 6161, loss 0.595537.
Train: 2018-08-09T00:28:59.427979: step 6162, loss 0.512948.
Train: 2018-08-09T00:29:02.508168: step 6163, loss 0.562493.
Train: 2018-08-09T00:29:05.566299: step 6164, loss 0.545956.
Train: 2018-08-09T00:29:08.660526: step 6165, loss 0.545941.
Train: 2018-08-09T00:29:11.777814: step 6166, loss 0.54592.
Train: 2018-08-09T00:29:14.849982: step 6167, loss 0.47958.
Train: 2018-08-09T00:29:17.931174: step 6168, loss 0.678751.
Train: 2018-08-09T00:29:20.990307: step 6169, loss 0.545813.
Train: 2018-08-09T00:29:24.047436: step 6170, loss 0.579022.
Test: 2018-08-09T00:29:44.976079: step 6170, loss 0.548757.
Train: 2018-08-09T00:29:48.052257: step 6171, loss 0.645533.
Train: 2018-08-09T00:29:51.118410: step 6172, loss 0.711805.
Train: 2018-08-09T00:29:54.221660: step 6173, loss 0.54599.
Train: 2018-08-09T00:29:57.301850: step 6174, loss 0.513121.
Train: 2018-08-09T00:30:00.371010: step 6175, loss 0.546094.
Train: 2018-08-09T00:30:03.441172: step 6176, loss 0.611728.
Train: 2018-08-09T00:30:06.516348: step 6177, loss 0.595296.
Train: 2018-08-09T00:30:09.578490: step 6178, loss 0.513622.
Train: 2018-08-09T00:30:12.661687: step 6179, loss 0.513828.
Train: 2018-08-09T00:30:15.725834: step 6180, loss 0.52994.
Test: 2018-08-09T00:30:36.669518: step 6180, loss 0.549169.
Train: 2018-08-09T00:30:39.780789: step 6181, loss 0.529856.
Train: 2018-08-09T00:30:42.854963: step 6182, loss 0.594951.
Train: 2018-08-09T00:30:45.936155: step 6183, loss 0.59544.
Train: 2018-08-09T00:30:48.997294: step 6184, loss 0.644209.
Train: 2018-08-09T00:30:52.039382: step 6185, loss 0.546444.
Train: 2018-08-09T00:30:55.135614: step 6186, loss 0.611534.
Train: 2018-08-09T00:30:58.217808: step 6187, loss 0.51367.
Train: 2018-08-09T00:31:01.288974: step 6188, loss 0.595175.
Train: 2018-08-09T00:31:04.382198: step 6189, loss 0.643741.
Train: 2018-08-09T00:31:07.467401: step 6190, loss 0.482207.
Test: 2018-08-09T00:31:28.390028: step 6190, loss 0.548309.
Train: 2018-08-09T00:31:31.472223: step 6191, loss 0.562969.
Train: 2018-08-09T00:31:34.590513: step 6192, loss 0.57884.
Train: 2018-08-09T00:31:37.654660: step 6193, loss 0.594905.
Train: 2018-08-09T00:31:40.732844: step 6194, loss 0.610891.
Train: 2018-08-09T00:31:43.826068: step 6195, loss 0.498957.
Train: 2018-08-09T00:31:46.923303: step 6196, loss 0.451095.
Train: 2018-08-09T00:31:49.973412: step 6197, loss 0.482887.
Train: 2018-08-09T00:31:53.008482: step 6198, loss 0.643048.
Train: 2018-08-09T00:31:56.083658: step 6199, loss 0.482523.
Train: 2018-08-09T00:31:59.148807: step 6200, loss 0.466203.
Test: 2018-08-09T00:32:20.063414: step 6200, loss 0.550025.
Train: 2018-08-09T00:32:24.922332: step 6201, loss 0.627401.
Train: 2018-08-09T00:32:27.957401: step 6202, loss 0.465485.
Train: 2018-08-09T00:32:31.044609: step 6203, loss 0.546383.
Train: 2018-08-09T00:32:34.102740: step 6204, loss 0.644239.
Train: 2018-08-09T00:32:37.183932: step 6205, loss 0.578949.
Train: 2018-08-09T00:32:40.265124: step 6206, loss 0.529806.
Train: 2018-08-09T00:32:43.327265: step 6207, loss 0.661032.
Train: 2018-08-09T00:32:46.419487: step 6208, loss 0.611794.
Train: 2018-08-09T00:32:49.471601: step 6209, loss 0.496964.
Train: 2018-08-09T00:32:52.521711: step 6210, loss 0.546154.
Test: 2018-08-09T00:33:13.462387: step 6210, loss 0.550897.
Train: 2018-08-09T00:33:16.545584: step 6211, loss 0.496895.
Train: 2018-08-09T00:33:19.648835: step 6212, loss 0.595418.
Train: 2018-08-09T00:33:22.732032: step 6213, loss 0.463803.
Train: 2018-08-09T00:33:25.826258: step 6214, loss 0.628481.
Train: 2018-08-09T00:33:28.906448: step 6215, loss 0.496455.
Train: 2018-08-09T00:33:32.003683: step 6216, loss 0.61212.
Train: 2018-08-09T00:33:35.121973: step 6217, loss 0.545922.
Train: 2018-08-09T00:33:38.239261: step 6218, loss 0.512731.
Train: 2018-08-09T00:33:41.344517: step 6219, loss 0.562449.
Train: 2018-08-09T00:33:44.415683: step 6220, loss 0.479258.
Test: 2018-08-09T00:34:05.333297: step 6220, loss 0.549324.
Train: 2018-08-09T00:34:08.439555: step 6221, loss 0.529057.
Train: 2018-08-09T00:34:11.538796: step 6222, loss 0.696174.
Train: 2018-08-09T00:34:14.580883: step 6223, loss 0.579187.
Train: 2018-08-09T00:34:17.651046: step 6224, loss 0.579212.
Train: 2018-08-09T00:34:20.708174: step 6225, loss 0.545645.
Train: 2018-08-09T00:34:23.787361: step 6226, loss 0.545663.
Train: 2018-08-09T00:34:26.869556: step 6227, loss 0.629215.
Train: 2018-08-09T00:34:29.950748: step 6228, loss 0.545787.
Train: 2018-08-09T00:34:33.045977: step 6229, loss 0.712447.
Train: 2018-08-09T00:34:36.119147: step 6230, loss 0.545854.
Test: 2018-08-09T00:34:57.053808: step 6230, loss 0.546954.
Train: 2018-08-09T00:35:00.148034: step 6231, loss 0.496207.
Train: 2018-08-09T00:35:03.224212: step 6232, loss 0.496323.
Train: 2018-08-09T00:35:06.302397: step 6233, loss 0.545948.
Train: 2018-08-09T00:35:09.383589: step 6234, loss 0.512874.
Train: 2018-08-09T00:35:12.490850: step 6235, loss 0.645162.
Train: 2018-08-09T00:35:15.567029: step 6236, loss 0.628636.
Train: 2018-08-09T00:35:18.624157: step 6237, loss 0.546024.
Train: 2018-08-09T00:35:21.716379: step 6238, loss 0.562506.
Train: 2018-08-09T00:35:24.788546: step 6239, loss 0.595437.
Train: 2018-08-09T00:35:27.862720: step 6240, loss 0.578967.
Test: 2018-08-09T00:35:48.786350: step 6240, loss 0.54782.
Train: 2018-08-09T00:35:51.891606: step 6241, loss 0.529802.
Train: 2018-08-09T00:35:54.974803: step 6242, loss 0.562563.
Train: 2018-08-09T00:35:58.062011: step 6243, loss 0.529882.
Train: 2018-08-09T00:36:01.114126: step 6244, loss 0.480853.
Train: 2018-08-09T00:36:04.154209: step 6245, loss 0.562595.
Train: 2018-08-09T00:36:07.237407: step 6246, loss 0.480679.
Train: 2018-08-09T00:36:10.335643: step 6247, loss 0.52972.
Train: 2018-08-09T00:36:13.429870: step 6248, loss 0.54605.
Train: 2018-08-09T00:36:16.525099: step 6249, loss 0.579003.
Train: 2018-08-09T00:36:19.606291: step 6250, loss 0.595571.
Test: 2018-08-09T00:36:40.536941: step 6250, loss 0.545727.
Train: 2018-08-09T00:36:43.623146: step 6251, loss 0.562518.
Train: 2018-08-09T00:36:46.690300: step 6252, loss 0.496222.
Train: 2018-08-09T00:36:49.761466: step 6253, loss 0.529263.
Train: 2018-08-09T00:36:52.834636: step 6254, loss 0.579076.
Train: 2018-08-09T00:36:55.937887: step 6255, loss 0.579039.
Train: 2018-08-09T00:36:59.005042: step 6256, loss 0.579167.
Train: 2018-08-09T00:37:02.077210: step 6257, loss 0.679204.
Train: 2018-08-09T00:37:05.202519: step 6258, loss 0.529069.
Train: 2018-08-09T00:37:08.279701: step 6259, loss 0.645624.
Train: 2018-08-09T00:37:11.334823: step 6260, loss 0.529241.
Test: 2018-08-09T00:37:32.266476: step 6260, loss 0.54818.
Train: 2018-08-09T00:37:35.317587: step 6261, loss 0.479511.
Train: 2018-08-09T00:37:38.416827: step 6262, loss 0.612169.
Train: 2018-08-09T00:37:41.476963: step 6263, loss 0.562545.
Train: 2018-08-09T00:37:44.516043: step 6264, loss 0.479705.
Train: 2018-08-09T00:37:47.594227: step 6265, loss 0.562403.
Train: 2018-08-09T00:37:50.659376: step 6266, loss 0.579128.
Train: 2018-08-09T00:37:53.739566: step 6267, loss 0.562457.
Train: 2018-08-09T00:37:56.836801: step 6268, loss 0.612038.
Train: 2018-08-09T00:37:59.923006: step 6269, loss 0.545872.
Train: 2018-08-09T00:38:02.997179: step 6270, loss 0.545994.
Test: 2018-08-09T00:38:23.931840: step 6270, loss 0.550719.
Train: 2018-08-09T00:38:27.048124: step 6271, loss 0.628738.
Train: 2018-08-09T00:38:30.123300: step 6272, loss 0.545976.
Train: 2018-08-09T00:38:33.200481: step 6273, loss 0.529567.
Train: 2018-08-09T00:38:36.277663: step 6274, loss 0.628428.
Train: 2018-08-09T00:38:39.344817: step 6275, loss 0.578956.
Train: 2018-08-09T00:38:42.420996: step 6276, loss 0.562519.
Train: 2018-08-09T00:38:45.483137: step 6277, loss 0.513271.
Train: 2018-08-09T00:38:48.546282: step 6278, loss 0.57901.
Train: 2018-08-09T00:38:51.620455: step 6279, loss 0.726589.
Train: 2018-08-09T00:38:54.683599: step 6280, loss 0.562652.
Test: 2018-08-09T00:39:15.631294: step 6280, loss 0.548019.
Train: 2018-08-09T00:39:18.711482: step 6281, loss 0.432498.
Train: 2018-08-09T00:39:21.788664: step 6282, loss 0.562655.
Train: 2018-08-09T00:39:24.884896: step 6283, loss 0.627652.
Train: 2018-08-09T00:39:27.953053: step 6284, loss 0.530238.
Train: 2018-08-09T00:39:31.033243: step 6285, loss 0.481656.
Train: 2018-08-09T00:39:34.080344: step 6286, loss 0.4816.
Train: 2018-08-09T00:39:37.144491: step 6287, loss 0.530165.
Train: 2018-08-09T00:39:40.242728: step 6288, loss 0.611492.
Train: 2018-08-09T00:39:43.312891: step 6289, loss 0.644151.
Train: 2018-08-09T00:39:46.429176: step 6290, loss 0.627832.
Test: 2018-08-09T00:40:07.337767: step 6290, loss 0.549231.
Train: 2018-08-09T00:40:10.451044: step 6291, loss 0.546358.
Train: 2018-08-09T00:40:13.567329: step 6292, loss 0.578915.
Train: 2018-08-09T00:40:16.648521: step 6293, loss 0.611408.
Train: 2018-08-09T00:40:19.725702: step 6294, loss 0.64378.
Train: 2018-08-09T00:40:22.787844: step 6295, loss 0.514204.
Train: 2018-08-09T00:40:25.914156: step 6296, loss 0.530457.
Train: 2018-08-09T00:40:28.995348: step 6297, loss 0.611131.
Train: 2018-08-09T00:40:32.055484: step 6298, loss 0.594973.
Train: 2018-08-09T00:40:35.123641: step 6299, loss 0.594935.
Train: 2018-08-09T00:40:38.196812: step 6300, loss 0.56284.
Test: 2018-08-09T00:40:59.098384: step 6300, loss 0.547884.
Train: 2018-08-09T00:41:03.926219: step 6301, loss 0.51489.
Train: 2018-08-09T00:41:07.001395: step 6302, loss 0.578861.
Train: 2018-08-09T00:41:10.082588: step 6303, loss 0.546939.
Train: 2018-08-09T00:41:13.124676: step 6304, loss 0.610763.
Train: 2018-08-09T00:41:16.209878: step 6305, loss 0.626652.
Train: 2018-08-09T00:41:19.301097: step 6306, loss 0.578859.
Train: 2018-08-09T00:41:22.358225: step 6307, loss 0.562997.
Train: 2018-08-09T00:41:25.444430: step 6308, loss 0.547196.
Train: 2018-08-09T00:41:28.517601: step 6309, loss 0.642107.
Train: 2018-08-09T00:41:31.578740: step 6310, loss 0.468455.
Test: 2018-08-09T00:41:52.478307: step 6310, loss 0.548325.
Train: 2018-08-09T00:41:55.533429: step 6311, loss 0.594632.
Train: 2018-08-09T00:41:58.614621: step 6312, loss 0.594619.
Train: 2018-08-09T00:42:01.684783: step 6313, loss 0.531659.
Train: 2018-08-09T00:42:04.765976: step 6314, loss 0.673257.
Train: 2018-08-09T00:42:07.851178: step 6315, loss 0.610268.
Train: 2018-08-09T00:42:10.938386: step 6316, loss 0.578883.
Train: 2018-08-09T00:42:14.036624: step 6317, loss 0.563273.
Train: 2018-08-09T00:42:17.102776: step 6318, loss 0.532146.
Train: 2018-08-09T00:42:20.177952: step 6319, loss 0.5789.
Train: 2018-08-09T00:42:23.228061: step 6320, loss 0.532259.
Test: 2018-08-09T00:42:44.161719: step 6320, loss 0.554675.
Train: 2018-08-09T00:42:47.000265: step 6321, loss 0.546782.
Train: 2018-08-09T00:42:50.074438: step 6322, loss 0.610003.
Train: 2018-08-09T00:42:53.152622: step 6323, loss 0.563365.
Train: 2018-08-09T00:42:56.228801: step 6324, loss 0.62552.
Train: 2018-08-09T00:42:59.329044: step 6325, loss 0.563395.
Train: 2018-08-09T00:43:02.409233: step 6326, loss 0.547907.
Train: 2018-08-09T00:43:05.495438: step 6327, loss 0.609923.
Train: 2018-08-09T00:43:08.559585: step 6328, loss 0.516976.
Train: 2018-08-09T00:43:11.628745: step 6329, loss 0.547942.
Train: 2018-08-09T00:43:14.712945: step 6330, loss 0.501414.
Test: 2018-08-09T00:43:35.637579: step 6330, loss 0.551175.
Train: 2018-08-09T00:43:38.745842: step 6331, loss 0.563378.
Train: 2018-08-09T00:43:41.825029: step 6332, loss 0.610026.
Train: 2018-08-09T00:43:44.926274: step 6333, loss 0.563317.
Train: 2018-08-09T00:43:48.031530: step 6334, loss 0.656878.
Train: 2018-08-09T00:43:51.099687: step 6335, loss 0.563306.
Train: 2018-08-09T00:43:54.171855: step 6336, loss 0.703568.
Train: 2018-08-09T00:43:57.280119: step 6337, loss 0.439026.
Train: 2018-08-09T00:44:00.368330: step 6338, loss 0.609993.
Train: 2018-08-09T00:44:03.436488: step 6339, loss 0.62551.
Train: 2018-08-09T00:44:06.497626: step 6340, loss 0.51687.
Test: 2018-08-09T00:44:27.420254: step 6340, loss 0.551228.
Train: 2018-08-09T00:44:30.478385: step 6341, loss 0.625433.
Train: 2018-08-09T00:44:33.571609: step 6342, loss 0.485994.
Train: 2018-08-09T00:44:36.650795: step 6343, loss 0.62543.
Train: 2018-08-09T00:44:39.731987: step 6344, loss 0.563436.
Train: 2018-08-09T00:44:42.845265: step 6345, loss 0.640856.
Train: 2018-08-09T00:44:45.926457: step 6346, loss 0.609856.
Train: 2018-08-09T00:44:48.997622: step 6347, loss 0.578944.
Train: 2018-08-09T00:44:52.068787: step 6348, loss 0.563553.
Train: 2018-08-09T00:44:55.128924: step 6349, loss 0.60972.
Train: 2018-08-09T00:44:58.195076: step 6350, loss 0.548279.
Test: 2018-08-09T00:45:19.117704: step 6350, loss 0.549292.
Train: 2018-08-09T00:45:22.200900: step 6351, loss 0.502338.
Train: 2018-08-09T00:45:25.277079: step 6352, loss 0.563649.
Train: 2018-08-09T00:45:28.380330: step 6353, loss 0.532954.
Train: 2018-08-09T00:45:31.466535: step 6354, loss 0.578971.
Train: 2018-08-09T00:45:34.543717: step 6355, loss 0.640478.
Train: 2018-08-09T00:45:37.586807: step 6356, loss 0.686585.
Train: 2018-08-09T00:45:40.675018: step 6357, loss 0.486953.
Train: 2018-08-09T00:45:43.770247: step 6358, loss 0.563651.
Train: 2018-08-09T00:45:46.830383: step 6359, loss 0.532994.
Train: 2018-08-09T00:45:49.925613: step 6360, loss 0.563638.
Test: 2018-08-09T00:46:10.903388: step 6360, loss 0.552135.
Train: 2018-08-09T00:46:14.022680: step 6361, loss 0.517556.
Train: 2018-08-09T00:46:17.106880: step 6362, loss 0.563578.
Train: 2018-08-09T00:46:20.136936: step 6363, loss 0.594363.
Train: 2018-08-09T00:46:23.217126: step 6364, loss 0.548072.
Train: 2018-08-09T00:46:26.276259: step 6365, loss 0.486164.
Train: 2018-08-09T00:46:29.342411: step 6366, loss 0.563405.
Train: 2018-08-09T00:46:32.372468: step 6367, loss 0.62558.
Train: 2018-08-09T00:46:35.472710: step 6368, loss 0.625654.
Train: 2018-08-09T00:46:38.539865: step 6369, loss 0.532112.
Train: 2018-08-09T00:46:41.583958: step 6370, loss 0.516415.
Test: 2018-08-09T00:47:02.514607: step 6370, loss 0.550343.
Train: 2018-08-09T00:47:05.596801: step 6371, loss 0.547574.
Train: 2018-08-09T00:47:08.674986: step 6372, loss 0.594566.
Train: 2018-08-09T00:47:11.750162: step 6373, loss 0.59459.
Train: 2018-08-09T00:47:14.808293: step 6374, loss 0.57887.
Train: 2018-08-09T00:47:17.918562: step 6375, loss 0.531618.
Train: 2018-08-09T00:47:20.997748: step 6376, loss 0.515772.
Train: 2018-08-09T00:47:24.076935: step 6377, loss 0.531432.
Train: 2018-08-09T00:47:27.162138: step 6378, loss 0.563005.
Train: 2018-08-09T00:47:30.219266: step 6379, loss 0.531174.
Train: 2018-08-09T00:47:33.291434: step 6380, loss 0.515089.
Test: 2018-08-09T00:47:54.209049: step 6380, loss 0.551501.
Train: 2018-08-09T00:47:57.380480: step 6381, loss 0.498862.
Train: 2018-08-09T00:48:00.574973: step 6382, loss 0.562801.
Train: 2018-08-09T00:48:03.681232: step 6383, loss 0.595018.
Train: 2018-08-09T00:48:06.767437: step 6384, loss 0.562706.
Train: 2018-08-09T00:48:09.857653: step 6385, loss 0.578904.
Train: 2018-08-09T00:48:12.930824: step 6386, loss 0.578913.
Train: 2018-08-09T00:48:16.002992: step 6387, loss 0.497437.
Train: 2018-08-09T00:48:19.077165: step 6388, loss 0.56259.
Train: 2018-08-09T00:48:22.167382: step 6389, loss 0.464287.
Train: 2018-08-09T00:48:25.248574: step 6390, loss 0.644773.
Test: 2018-08-09T00:48:46.268460: step 6390, loss 0.547699.
Train: 2018-08-09T00:48:49.404798: step 6391, loss 0.414192.
Train: 2018-08-09T00:48:52.508049: step 6392, loss 0.430108.
Train: 2018-08-09T00:48:55.581220: step 6393, loss 0.545787.
Train: 2018-08-09T00:48:58.662412: step 6394, loss 0.51217.
Train: 2018-08-09T00:49:01.748617: step 6395, loss 0.528618.
Train: 2018-08-09T00:49:04.831815: step 6396, loss 0.562445.
Train: 2018-08-09T00:49:07.922031: step 6397, loss 0.613827.
Train: 2018-08-09T00:49:11.028289: step 6398, loss 0.545285.
Train: 2018-08-09T00:49:14.105471: step 6399, loss 0.596651.
Train: 2018-08-09T00:49:17.189671: step 6400, loss 0.579474.
Test: 2018-08-09T00:49:38.131349: step 6400, loss 0.547552.
Train: 2018-08-09T00:49:43.086523: step 6401, loss 0.579496.
Train: 2018-08-09T00:49:46.142648: step 6402, loss 0.510808.
Train: 2018-08-09T00:49:49.222838: step 6403, loss 0.613931.
Train: 2018-08-09T00:49:52.302025: step 6404, loss 0.613937.
Train: 2018-08-09T00:49:55.362161: step 6405, loss 0.510773.
Train: 2018-08-09T00:49:58.433326: step 6406, loss 0.648259.
Train: 2018-08-09T00:50:01.484438: step 6407, loss 0.562337.
Train: 2018-08-09T00:50:04.554601: step 6408, loss 0.562338.
Train: 2018-08-09T00:50:07.615740: step 6409, loss 0.528132.
Train: 2018-08-09T00:50:10.674873: step 6410, loss 0.511089.
Test: 2018-08-09T00:50:31.582461: step 6410, loss 0.548921.
Train: 2018-08-09T00:50:34.658639: step 6411, loss 0.528186.
Train: 2018-08-09T00:50:37.730807: step 6412, loss 0.528185.
Train: 2018-08-09T00:50:40.831050: step 6413, loss 0.613588.
Train: 2018-08-09T00:50:43.942322: step 6414, loss 0.613553.
Train: 2018-08-09T00:50:47.020506: step 6415, loss 0.596432.
Train: 2018-08-09T00:50:50.088664: step 6416, loss 0.528323.
Train: 2018-08-09T00:50:53.187903: step 6417, loss 0.460439.
Train: 2018-08-09T00:50:56.244029: step 6418, loss 0.528372.
Train: 2018-08-09T00:50:59.314191: step 6419, loss 0.511377.
Train: 2018-08-09T00:51:02.379341: step 6420, loss 0.664401.
Test: 2018-08-09T00:51:23.308988: step 6420, loss 0.550281.
Train: 2018-08-09T00:51:26.419256: step 6421, loss 0.613315.
Train: 2018-08-09T00:51:29.533536: step 6422, loss 0.579293.
Train: 2018-08-09T00:51:32.598686: step 6423, loss 0.494732.
Train: 2018-08-09T00:51:35.659824: step 6424, loss 0.511752.
Train: 2018-08-09T00:51:38.725976: step 6425, loss 0.494677.
Train: 2018-08-09T00:51:41.813184: step 6426, loss 0.613201.
Train: 2018-08-09T00:51:44.893374: step 6427, loss 0.49454.
Train: 2018-08-09T00:51:47.957520: step 6428, loss 0.52839.
Train: 2018-08-09T00:51:51.032697: step 6429, loss 0.562356.
Train: 2018-08-09T00:51:54.102859: step 6430, loss 0.579332.
Test: 2018-08-09T00:52:15.015461: step 6430, loss 0.547686.
Train: 2018-08-09T00:52:18.037495: step 6431, loss 0.42623.
Train: 2018-08-09T00:52:21.114676: step 6432, loss 0.59656.
Train: 2018-08-09T00:52:24.181831: step 6433, loss 0.562177.
Train: 2018-08-09T00:52:27.258010: step 6434, loss 0.665248.
Train: 2018-08-09T00:52:30.324162: step 6435, loss 0.493951.
Train: 2018-08-09T00:52:33.395327: step 6436, loss 0.545163.
Train: 2018-08-09T00:52:36.445437: step 6437, loss 0.579462.
Train: 2018-08-09T00:52:39.542671: step 6438, loss 0.579708.
Train: 2018-08-09T00:52:42.617847: step 6439, loss 0.596597.
Train: 2018-08-09T00:52:45.710069: step 6440, loss 0.562292.
Test: 2018-08-09T00:53:06.631694: step 6440, loss 0.547697.
Train: 2018-08-09T00:53:09.726923: step 6441, loss 0.630367.
Train: 2018-08-09T00:53:12.859251: step 6442, loss 0.545385.
Train: 2018-08-09T00:53:15.924400: step 6443, loss 0.511568.
Train: 2018-08-09T00:53:19.021635: step 6444, loss 0.54545.
Train: 2018-08-09T00:53:22.089792: step 6445, loss 0.646829.
Train: 2018-08-09T00:53:25.149928: step 6446, loss 0.629788.
Train: 2018-08-09T00:53:28.255184: step 6447, loss 0.59599.
Train: 2018-08-09T00:53:31.364451: step 6448, loss 0.545668.
Train: 2018-08-09T00:53:34.416566: step 6449, loss 0.545743.
Train: 2018-08-09T00:53:37.458654: step 6450, loss 0.612372.
Test: 2018-08-09T00:53:58.378273: step 6450, loss 0.550054.
Train: 2018-08-09T00:54:01.504585: step 6451, loss 0.496096.
Train: 2018-08-09T00:54:04.589788: step 6452, loss 0.612159.
Train: 2018-08-09T00:54:07.656943: step 6453, loss 0.479907.
Train: 2018-08-09T00:54:10.730114: step 6454, loss 0.529503.
Train: 2018-08-09T00:54:13.795263: step 6455, loss 0.562509.
Train: 2018-08-09T00:54:16.915559: step 6456, loss 0.611977.
Train: 2018-08-09T00:54:19.973689: step 6457, loss 0.546055.
Train: 2018-08-09T00:54:23.025804: step 6458, loss 0.677706.
Train: 2018-08-09T00:54:26.089951: step 6459, loss 0.578965.
Train: 2018-08-09T00:54:29.180167: step 6460, loss 0.513494.
Test: 2018-08-09T00:54:50.107809: step 6460, loss 0.551002.
Train: 2018-08-09T00:54:53.201032: step 6461, loss 0.448253.
Train: 2018-08-09T00:54:56.287237: step 6462, loss 0.64431.
Train: 2018-08-09T00:54:59.351384: step 6463, loss 0.627908.
Train: 2018-08-09T00:55:02.436587: step 6464, loss 0.497465.
Train: 2018-08-09T00:55:05.519784: step 6465, loss 0.56264.
Train: 2018-08-09T00:55:08.599974: step 6466, loss 0.546371.
Train: 2018-08-09T00:55:11.659107: step 6467, loss 0.481326.
Train: 2018-08-09T00:55:14.768373: step 6468, loss 0.578921.
Train: 2018-08-09T00:55:17.857587: step 6469, loss 0.530033.
Train: 2018-08-09T00:55:20.932763: step 6470, loss 0.52998.
Test: 2018-08-09T00:55:41.871433: step 6470, loss 0.550373.
Train: 2018-08-09T00:55:44.918534: step 6471, loss 0.497221.
Train: 2018-08-09T00:55:47.994713: step 6472, loss 0.628115.
Train: 2018-08-09T00:55:51.073900: step 6473, loss 0.496938.
Train: 2018-08-09T00:55:54.137044: step 6474, loss 0.661185.
Train: 2018-08-09T00:55:57.179132: step 6475, loss 0.529643.
Train: 2018-08-09T00:56:00.245284: step 6476, loss 0.578987.
Train: 2018-08-09T00:56:03.298402: step 6477, loss 0.546063.
Train: 2018-08-09T00:56:06.347508: step 6478, loss 0.5131.
Train: 2018-08-09T00:56:09.456775: step 6479, loss 0.62849.
Train: 2018-08-09T00:56:12.590105: step 6480, loss 0.546012.
Test: 2018-08-09T00:56:33.517746: step 6480, loss 0.550166.
Train: 2018-08-09T00:56:36.633029: step 6481, loss 0.57901.
Train: 2018-08-09T00:56:39.739287: step 6482, loss 0.628498.
Train: 2018-08-09T00:56:42.834517: step 6483, loss 0.579.
Train: 2018-08-09T00:56:45.923730: step 6484, loss 0.562532.
Train: 2018-08-09T00:56:48.995898: step 6485, loss 0.562542.
Train: 2018-08-09T00:56:52.066061: step 6486, loss 0.611794.
Train: 2018-08-09T00:56:55.129205: step 6487, loss 0.513428.
Train: 2018-08-09T00:56:58.209394: step 6488, loss 0.464385.
Train: 2018-08-09T00:57:01.300613: step 6489, loss 0.529816.
Train: 2018-08-09T00:57:04.385816: step 6490, loss 0.660962.
Test: 2018-08-09T00:57:25.316465: step 6490, loss 0.550928.
Train: 2018-08-09T00:57:28.369582: step 6491, loss 0.562568.
Train: 2018-08-09T00:57:31.467819: step 6492, loss 0.497041.
Train: 2018-08-09T00:57:34.534974: step 6493, loss 0.529781.
Train: 2018-08-09T00:57:37.579068: step 6494, loss 0.546144.
Train: 2018-08-09T00:57:40.689337: step 6495, loss 0.644701.
Train: 2018-08-09T00:57:43.777547: step 6496, loss 0.546118.
Train: 2018-08-09T00:57:46.852724: step 6497, loss 0.595399.
Train: 2018-08-09T00:57:49.920881: step 6498, loss 0.464051.
Train: 2018-08-09T00:57:52.993049: step 6499, loss 0.546106.
Train: 2018-08-09T00:57:56.071233: step 6500, loss 0.611904.
Test: 2018-08-09T00:58:16.999877: step 6500, loss 0.547106.
Train: 2018-08-09T00:58:21.871830: step 6501, loss 0.628362.
Train: 2018-08-09T00:58:24.959038: step 6502, loss 0.578982.
Train: 2018-08-09T00:58:28.039227: step 6503, loss 0.562547.
Train: 2018-08-09T00:58:31.121422: step 6504, loss 0.644598.
Train: 2018-08-09T00:58:34.200609: step 6505, loss 0.578951.
Train: 2018-08-09T00:58:37.281801: step 6506, loss 0.480951.
Train: 2018-08-09T00:58:40.382043: step 6507, loss 0.611568.
Train: 2018-08-09T00:58:43.460227: step 6508, loss 0.611509.
Train: 2018-08-09T00:58:46.529387: step 6509, loss 0.481371.
Train: 2018-08-09T00:58:49.595539: step 6510, loss 0.530168.
Test: 2018-08-09T00:59:10.539224: step 6510, loss 0.549896.
Train: 2018-08-09T00:59:13.731711: step 6511, loss 0.513919.
Train: 2018-08-09T00:59:16.802876: step 6512, loss 0.578914.
Train: 2018-08-09T00:59:19.870031: step 6513, loss 0.562646.
Train: 2018-08-09T00:59:22.939191: step 6514, loss 0.627752.
Train: 2018-08-09T00:59:26.025396: step 6515, loss 0.497579.
Train: 2018-08-09T00:59:29.103581: step 6516, loss 0.497539.
Train: 2018-08-09T00:59:32.228890: step 6517, loss 0.497418.
Train: 2018-08-09T00:59:35.299052: step 6518, loss 0.529913.
Train: 2018-08-09T00:59:38.378239: step 6519, loss 0.529794.
Train: 2018-08-09T00:59:41.449405: step 6520, loss 0.431023.
Test: 2018-08-09T01:00:02.344961: step 6520, loss 0.54764.
Train: 2018-08-09T01:00:05.412115: step 6521, loss 0.57902.
Train: 2018-08-09T01:00:08.508347: step 6522, loss 0.562461.
Train: 2018-08-09T01:00:11.583523: step 6523, loss 0.462535.
Train: 2018-08-09T01:00:14.675744: step 6524, loss 0.562414.
Train: 2018-08-09T01:00:17.754931: step 6525, loss 0.596003.
Train: 2018-08-09T01:00:20.827099: step 6526, loss 0.494945.
Train: 2018-08-09T01:00:23.912302: step 6527, loss 0.596188.
Train: 2018-08-09T01:00:26.975446: step 6528, loss 0.596229.
Train: 2018-08-09T01:00:30.056638: step 6529, loss 0.562202.
Train: 2018-08-09T01:00:33.108752: step 6530, loss 0.613208.
Test: 2018-08-09T01:00:54.034388: step 6530, loss 0.54957.
Train: 2018-08-09T01:00:57.080487: step 6531, loss 0.61363.
Train: 2018-08-09T01:01:00.165689: step 6532, loss 0.66482.
Train: 2018-08-09T01:01:03.269943: step 6533, loss 0.477221.
Train: 2018-08-09T01:01:06.349130: step 6534, loss 0.630489.
Train: 2018-08-09T01:01:09.446364: step 6535, loss 0.613327.
Train: 2018-08-09T01:01:12.545604: step 6536, loss 0.494793.
Train: 2018-08-09T01:01:15.618775: step 6537, loss 0.680243.
Train: 2018-08-09T01:01:18.714004: step 6538, loss 0.595961.
Train: 2018-08-09T01:01:21.784167: step 6539, loss 0.562415.
Train: 2018-08-09T01:01:24.853327: step 6540, loss 0.512468.
Test: 2018-08-09T01:01:45.782973: step 6540, loss 0.547512.
Train: 2018-08-09T01:01:48.894245: step 6541, loss 0.52922.
Train: 2018-08-09T01:01:52.025570: step 6542, loss 0.545877.
Train: 2018-08-09T01:01:55.092725: step 6543, loss 0.512776.
Train: 2018-08-09T01:01:58.170909: step 6544, loss 0.595595.
Train: 2018-08-09T01:02:01.267141: step 6545, loss 0.579028.
Train: 2018-08-09T01:02:04.335299: step 6546, loss 0.545973.
Train: 2018-08-09T01:02:07.398443: step 6547, loss 0.545995.
Train: 2018-08-09T01:02:10.462589: step 6548, loss 0.595493.
Train: 2018-08-09T01:02:13.521723: step 6549, loss 0.562525.
Train: 2018-08-09T01:02:16.612942: step 6550, loss 0.546024.
Test: 2018-08-09T01:02:37.545597: step 6550, loss 0.546524.
Train: 2018-08-09T01:02:40.643833: step 6551, loss 0.49679.
Train: 2018-08-09T01:02:43.747084: step 6552, loss 0.595316.
Train: 2018-08-09T01:02:46.819252: step 6553, loss 0.59536.
Train: 2018-08-09T01:02:49.920497: step 6554, loss 0.562451.
Train: 2018-08-09T01:02:52.992665: step 6555, loss 0.611792.
Train: 2018-08-09T01:02:56.093910: step 6556, loss 0.513534.
Train: 2018-08-09T01:02:59.129982: step 6557, loss 0.497382.
Train: 2018-08-09T01:03:02.227217: step 6558, loss 0.546428.
Train: 2018-08-09T01:03:05.316431: step 6559, loss 0.562633.
Train: 2018-08-09T01:03:08.389601: step 6560, loss 0.546133.
Test: 2018-08-09T01:03:29.377402: step 6560, loss 0.545875.
Train: 2018-08-09T01:03:32.425506: step 6561, loss 0.56252.
Train: 2018-08-09T01:03:35.515722: step 6562, loss 0.463779.
Train: 2018-08-09T01:03:38.627997: step 6563, loss 0.57903.
Train: 2018-08-09T01:03:41.705178: step 6564, loss 0.512893.
Train: 2018-08-09T01:03:44.798402: step 6565, loss 0.529322.
Train: 2018-08-09T01:03:47.903658: step 6566, loss 0.545834.
Train: 2018-08-09T01:03:50.995879: step 6567, loss 0.645735.
Train: 2018-08-09T01:03:54.094117: step 6568, loss 0.52908.
Train: 2018-08-09T01:03:57.188344: step 6569, loss 0.629199.
Train: 2018-08-09T01:04:00.217397: step 6570, loss 0.545728.
Test: 2018-08-09T01:04:21.173113: step 6570, loss 0.549934.
Train: 2018-08-09T01:04:24.340534: step 6571, loss 0.529029.
Train: 2018-08-09T01:04:27.394654: step 6572, loss 0.645941.
Train: 2018-08-09T01:04:30.512945: step 6573, loss 0.612489.
Train: 2018-08-09T01:04:33.602158: step 6574, loss 0.479134.
Train: 2018-08-09T01:04:36.686358: step 6575, loss 0.495815.
Train: 2018-08-09T01:04:39.747497: step 6576, loss 0.645771.
Train: 2018-08-09T01:04:42.818662: step 6577, loss 0.562436.
Train: 2018-08-09T01:04:45.887822: step 6578, loss 0.579083.
Train: 2018-08-09T01:04:48.969014: step 6579, loss 0.545824.
Train: 2018-08-09T01:04:52.065246: step 6580, loss 0.529229.
Test: 2018-08-09T01:05:13.000909: step 6580, loss 0.547504.
Train: 2018-08-09T01:05:16.058036: step 6581, loss 0.545804.
Train: 2018-08-09T01:05:19.153266: step 6582, loss 0.628943.
Train: 2018-08-09T01:05:22.234458: step 6583, loss 0.628788.
Train: 2018-08-09T01:05:25.306626: step 6584, loss 0.595913.
Train: 2018-08-09T01:05:28.386815: step 6585, loss 0.545933.
Train: 2018-08-09T01:05:31.506108: step 6586, loss 0.612002.
Train: 2018-08-09T01:05:34.588303: step 6587, loss 0.562517.
Train: 2018-08-09T01:05:37.682530: step 6588, loss 0.529795.
Train: 2018-08-09T01:05:40.758709: step 6589, loss 0.513497.
Train: 2018-08-09T01:05:43.801799: step 6590, loss 0.546238.
Test: 2018-08-09T01:06:04.740470: step 6590, loss 0.549755.
Train: 2018-08-09T01:06:07.819656: step 6591, loss 0.57894.
Train: 2018-08-09T01:06:10.929925: step 6592, loss 0.513598.
Train: 2018-08-09T01:06:14.069272: step 6593, loss 0.546276.
Train: 2018-08-09T01:06:17.159488: step 6594, loss 0.464497.
Train: 2018-08-09T01:06:20.229651: step 6595, loss 0.644532.
Train: 2018-08-09T01:06:23.300816: step 6596, loss 0.546215.
Train: 2018-08-09T01:06:26.387022: step 6597, loss 0.578969.
Train: 2018-08-09T01:06:29.459190: step 6598, loss 0.661015.
Train: 2018-08-09T01:06:32.540382: step 6599, loss 0.595334.
Train: 2018-08-09T01:06:35.615558: step 6600, loss 0.562599.
Test: 2018-08-09T01:06:56.560245: step 6600, loss 0.550406.
Train: 2018-08-09T01:07:01.579589: step 6601, loss 0.52997.
Train: 2018-08-09T01:07:04.673816: step 6602, loss 0.595231.
Train: 2018-08-09T01:07:07.750997: step 6603, loss 0.562638.
Train: 2018-08-09T01:07:10.819155: step 6604, loss 0.611437.
Train: 2018-08-09T01:07:13.878288: step 6605, loss 0.546442.
Train: 2018-08-09T01:07:16.944440: step 6606, loss 0.562697.
Train: 2018-08-09T01:07:20.027637: step 6607, loss 0.64363.
Train: 2018-08-09T01:07:23.093789: step 6608, loss 0.595032.
Train: 2018-08-09T01:07:26.127856: step 6609, loss 0.450098.
Train: 2018-08-09T01:07:29.198019: step 6610, loss 0.594964.
Test: 2018-08-09T01:07:50.100593: step 6610, loss 0.548953.
Train: 2018-08-09T01:07:53.174766: step 6611, loss 0.48243.
Train: 2018-08-09T01:07:56.279020: step 6612, loss 0.578874.
Train: 2018-08-09T01:07:59.379262: step 6613, loss 0.61105.
Train: 2018-08-09T01:08:02.463462: step 6614, loss 0.594953.
Train: 2018-08-09T01:08:05.511566: step 6615, loss 0.562807.
Train: 2018-08-09T01:08:08.592758: step 6616, loss 0.54677.
Train: 2018-08-09T01:08:11.666932: step 6617, loss 0.530738.
Train: 2018-08-09T01:08:14.744113: step 6618, loss 0.659104.
Train: 2018-08-09T01:08:17.841348: step 6619, loss 0.56284.
Train: 2018-08-09T01:08:20.923542: step 6620, loss 0.642885.
Test: 2018-08-09T01:08:41.867226: step 6620, loss 0.54914.
Train: 2018-08-09T01:08:44.937389: step 6621, loss 0.546922.
Train: 2018-08-09T01:08:47.849130: step 6622, loss 0.528907.
Train: 2018-08-09T01:08:50.928317: step 6623, loss 0.499238.
Train: 2018-08-09T01:08:53.988454: step 6624, loss 0.546888.
Train: 2018-08-09T01:08:57.042573: step 6625, loss 0.499012.
Train: 2018-08-09T01:09:00.095691: step 6626, loss 0.562982.
Train: 2018-08-09T01:09:03.139784: step 6627, loss 0.611477.
Train: 2018-08-09T01:09:06.202928: step 6628, loss 0.417851.
Train: 2018-08-09T01:09:09.282115: step 6629, loss 0.481209.
Train: 2018-08-09T01:09:12.362304: step 6630, loss 0.594825.
Test: 2018-08-09T01:09:33.295961: step 6630, loss 0.547656.
Train: 2018-08-09T01:09:36.421270: step 6631, loss 0.59693.
Train: 2018-08-09T01:09:39.498451: step 6632, loss 0.64556.
Train: 2018-08-09T01:09:42.559590: step 6633, loss 0.496972.
Train: 2018-08-09T01:09:45.642788: step 6634, loss 0.578146.
Train: 2018-08-09T01:09:48.691894: step 6635, loss 0.445604.
Train: 2018-08-09T01:09:51.773086: step 6636, loss 0.512175.
Train: 2018-08-09T01:09:54.838236: step 6637, loss 0.630408.
Train: 2018-08-09T01:09:57.916420: step 6638, loss 0.548212.
Train: 2018-08-09T01:10:01.011649: step 6639, loss 0.594769.
Train: 2018-08-09T01:10:04.106879: step 6640, loss 0.460032.
Test: 2018-08-09T01:10:25.028504: step 6640, loss 0.547635.
Train: 2018-08-09T01:10:28.116714: step 6641, loss 0.461049.
Train: 2018-08-09T01:10:31.228989: step 6642, loss 0.424891.
Train: 2018-08-09T01:10:34.308175: step 6643, loss 0.508768.
Train: 2018-08-09T01:10:37.381346: step 6644, loss 0.653558.
Train: 2018-08-09T01:10:40.447498: step 6645, loss 0.543487.
Train: 2018-08-09T01:10:43.518664: step 6646, loss 0.596005.
Train: 2018-08-09T01:10:46.597850: step 6647, loss 0.511704.
Train: 2018-08-09T01:10:49.704109: step 6648, loss 0.544516.
Train: 2018-08-09T01:10:52.812373: step 6649, loss 0.622271.
Train: 2018-08-09T01:10:55.909608: step 6650, loss 0.488981.
Test: 2018-08-09T01:11:16.826220: step 6650, loss 0.546393.
Train: 2018-08-09T01:11:19.918440: step 6651, loss 0.616859.
Train: 2018-08-09T01:11:22.997627: step 6652, loss 0.542948.
Train: 2018-08-09T01:11:26.085838: step 6653, loss 0.475147.
Train: 2018-08-09T01:11:29.167030: step 6654, loss 0.670593.
Train: 2018-08-09T01:11:32.240201: step 6655, loss 0.540807.
Train: 2018-08-09T01:11:35.334428: step 6656, loss 0.579736.
Train: 2018-08-09T01:11:38.427651: step 6657, loss 0.630853.
Train: 2018-08-09T01:11:41.517868: step 6658, loss 0.562158.
Train: 2018-08-09T01:11:44.575998: step 6659, loss 0.629361.
Train: 2018-08-09T01:11:47.647164: step 6660, loss 0.477563.
Test: 2018-08-09T01:12:08.562772: step 6660, loss 0.547097.
Train: 2018-08-09T01:12:11.654994: step 6661, loss 0.529756.
Train: 2018-08-09T01:12:14.767268: step 6662, loss 0.477605.
Train: 2018-08-09T01:12:17.875532: step 6663, loss 0.460616.
Train: 2018-08-09T01:12:20.970762: step 6664, loss 0.578927.
Train: 2018-08-09T01:12:24.014855: step 6665, loss 0.493863.
Train: 2018-08-09T01:12:27.086021: step 6666, loss 0.528317.
Train: 2018-08-09T01:12:30.167213: step 6667, loss 0.595731.
Train: 2018-08-09T01:12:33.237375: step 6668, loss 0.630798.
Train: 2018-08-09T01:12:36.300519: step 6669, loss 0.545273.
Train: 2018-08-09T01:12:39.361658: step 6670, loss 0.578546.
Test: 2018-08-09T01:13:00.297320: step 6670, loss 0.547439.
Train: 2018-08-09T01:13:03.353446: step 6671, loss 0.650079.
Train: 2018-08-09T01:13:06.421603: step 6672, loss 0.562795.
Train: 2018-08-09T01:13:09.501792: step 6673, loss 0.596949.
Train: 2018-08-09T01:13:12.579977: step 6674, loss 0.529029.
Train: 2018-08-09T01:13:15.661168: step 6675, loss 0.562472.
Train: 2018-08-09T01:13:18.750382: step 6676, loss 0.578271.
Train: 2018-08-09T01:13:21.811521: step 6677, loss 0.595423.
Train: 2018-08-09T01:13:24.896723: step 6678, loss 0.595924.
Train: 2018-08-09T01:13:27.978918: step 6679, loss 0.612513.
Train: 2018-08-09T01:13:31.037049: step 6680, loss 0.595334.
Test: 2018-08-09T01:13:51.965693: step 6680, loss 0.547383.
Train: 2018-08-09T01:13:55.033849: step 6681, loss 0.627883.
Train: 2018-08-09T01:13:58.151137: step 6682, loss 0.692124.
Train: 2018-08-09T01:14:01.223306: step 6683, loss 0.610971.
Train: 2018-08-09T01:14:04.294471: step 6684, loss 0.515146.
Train: 2018-08-09T01:14:07.423791: step 6685, loss 0.642236.
Train: 2018-08-09T01:14:10.510999: step 6686, loss 0.547372.
Train: 2018-08-09T01:14:13.599210: step 6687, loss 0.610251.
Train: 2018-08-09T01:14:16.665362: step 6688, loss 0.594509.
Train: 2018-08-09T01:14:19.744548: step 6689, loss 0.625485.
Train: 2018-08-09T01:14:22.844791: step 6690, loss 0.517185.
Test: 2018-08-09T01:14:43.800507: step 6690, loss 0.552071.
Train: 2018-08-09T01:14:46.913784: step 6691, loss 0.625113.
Train: 2018-08-09T01:14:50.021045: step 6692, loss 0.533024.
Train: 2018-08-09T01:14:53.123293: step 6693, loss 0.609571.
Train: 2018-08-09T01:14:56.224538: step 6694, loss 0.533375.
Train: 2018-08-09T01:14:59.324781: step 6695, loss 0.63979.
Train: 2018-08-09T01:15:02.429035: step 6696, loss 0.518551.
Train: 2018-08-09T01:15:05.514237: step 6697, loss 0.47335.
Train: 2018-08-09T01:15:08.587408: step 6698, loss 0.594211.
Train: 2018-08-09T01:15:11.683640: step 6699, loss 0.518616.
Train: 2018-08-09T01:15:14.773856: step 6700, loss 0.56396.
Test: 2018-08-09T01:15:35.735588: step 6700, loss 0.553137.
Train: 2018-08-09T01:15:40.620575: step 6701, loss 0.624586.
Train: 2018-08-09T01:15:43.683719: step 6702, loss 0.548749.
Train: 2018-08-09T01:15:46.743855: step 6703, loss 0.594244.
Train: 2018-08-09T01:15:49.821037: step 6704, loss 0.533496.
Train: 2018-08-09T01:15:52.911253: step 6705, loss 0.502977.
Train: 2018-08-09T01:15:55.939304: step 6706, loss 0.624779.
Train: 2018-08-09T01:15:58.985402: step 6707, loss 0.548449.
Train: 2018-08-09T01:16:02.065592: step 6708, loss 0.517764.
Train: 2018-08-09T01:16:05.154805: step 6709, loss 0.48685.
Train: 2018-08-09T01:16:08.248029: step 6710, loss 0.56351.
Test: 2018-08-09T01:16:29.225804: step 6710, loss 0.550676.
Train: 2018-08-09T01:16:32.334067: step 6711, loss 0.594403.
Train: 2018-08-09T01:16:35.425286: step 6712, loss 0.532362.
Train: 2018-08-09T01:16:38.522521: step 6713, loss 0.516516.
Train: 2018-08-09T01:16:41.566614: step 6714, loss 0.578777.
Train: 2018-08-09T01:16:44.631763: step 6715, loss 0.547562.
Train: 2018-08-09T01:16:47.715963: step 6716, loss 0.547236.
Train: 2018-08-09T01:16:50.810190: step 6717, loss 0.642253.
Train: 2018-08-09T01:16:53.900406: step 6718, loss 0.64266.
Train: 2018-08-09T01:16:56.989619: step 6719, loss 0.626635.
Train: 2018-08-09T01:17:00.062790: step 6720, loss 0.658203.
Test: 2018-08-09T01:17:20.997450: step 6720, loss 0.549394.
Train: 2018-08-09T01:17:24.084658: step 6721, loss 0.515542.
Train: 2018-08-09T01:17:27.174874: step 6722, loss 0.547232.
Train: 2018-08-09T01:17:30.265090: step 6723, loss 0.578861.
Train: 2018-08-09T01:17:33.358314: step 6724, loss 0.610459.
Train: 2018-08-09T01:17:36.475602: step 6725, loss 0.594643.
Train: 2018-08-09T01:17:39.563813: step 6726, loss 0.531601.
Train: 2018-08-09T01:17:42.639991: step 6727, loss 0.594612.
Train: 2018-08-09T01:17:45.712159: step 6728, loss 0.547415.
Train: 2018-08-09T01:17:48.810397: step 6729, loss 0.563153.
Train: 2018-08-09T01:17:51.914650: step 6730, loss 0.578869.
Test: 2018-08-09T01:18:12.892424: step 6730, loss 0.549044.
Train: 2018-08-09T01:18:15.978629: step 6731, loss 0.500347.
Train: 2018-08-09T01:18:19.084888: step 6732, loss 0.500269.
Train: 2018-08-09T01:18:22.191147: step 6733, loss 0.547356.
Train: 2018-08-09T01:18:25.267326: step 6734, loss 0.57888.
Train: 2018-08-09T01:18:28.334480: step 6735, loss 0.626331.
Train: 2018-08-09T01:18:31.426701: step 6736, loss 0.547195.
Train: 2018-08-09T01:18:34.445728: step 6737, loss 0.563008.
Train: 2018-08-09T01:18:37.534942: step 6738, loss 0.531288.
Train: 2018-08-09T01:18:40.674288: step 6739, loss 0.610619.
Train: 2018-08-09T01:18:43.737433: step 6740, loss 0.51526.
Test: 2018-08-09T01:19:04.682120: step 6740, loss 0.551032.
Train: 2018-08-09T01:19:07.782361: step 6741, loss 0.594791.
Train: 2018-08-09T01:19:10.897644: step 6742, loss 0.515108.
Train: 2018-08-09T01:19:13.993876: step 6743, loss 0.514999.
Train: 2018-08-09T01:19:17.065041: step 6744, loss 0.562855.
Train: 2018-08-09T01:19:20.137209: step 6745, loss 0.562826.
Train: 2018-08-09T01:19:23.225420: step 6746, loss 0.530643.
Train: 2018-08-09T01:19:26.272521: step 6747, loss 0.578887.
Train: 2018-08-09T01:19:29.365746: step 6748, loss 0.578886.
Train: 2018-08-09T01:19:32.428889: step 6749, loss 0.54653.
Train: 2018-08-09T01:19:35.511084: step 6750, loss 0.546464.
Test: 2018-08-09T01:19:56.458779: step 6750, loss 0.548672.
Train: 2018-08-09T01:19:59.572056: step 6751, loss 0.627609.
Train: 2018-08-09T01:20:02.659264: step 6752, loss 0.513879.
Train: 2018-08-09T01:20:05.757501: step 6753, loss 0.57892.
Train: 2018-08-09T01:20:08.832677: step 6754, loss 0.56268.
Train: 2018-08-09T01:20:11.932920: step 6755, loss 0.578875.
Train: 2018-08-09T01:20:15.019125: step 6756, loss 0.546327.
Train: 2018-08-09T01:20:18.105331: step 6757, loss 0.595232.
Train: 2018-08-09T01:20:21.181509: step 6758, loss 0.578924.
Train: 2018-08-09T01:20:24.258691: step 6759, loss 0.595265.
Train: 2018-08-09T01:20:27.346901: step 6760, loss 0.530023.
Test: 2018-08-09T01:20:48.344729: step 6760, loss 0.549192.
Train: 2018-08-09T01:20:51.410881: step 6761, loss 0.513705.
Train: 2018-08-09T01:20:54.532179: step 6762, loss 0.546286.
Train: 2018-08-09T01:20:57.586300: step 6763, loss 0.52993.
Train: 2018-08-09T01:21:00.626383: step 6764, loss 0.644419.
Train: 2018-08-09T01:21:03.706572: step 6765, loss 0.578939.
Train: 2018-08-09T01:21:06.774729: step 6766, loss 0.513552.
Train: 2018-08-09T01:21:09.858929: step 6767, loss 0.595312.
Train: 2018-08-09T01:21:12.972207: step 6768, loss 0.497184.
Train: 2018-08-09T01:21:16.037356: step 6769, loss 0.595317.
Train: 2018-08-09T01:21:19.104510: step 6770, loss 0.628071.
Test: 2018-08-09T01:21:40.085293: step 6770, loss 0.548499.
Train: 2018-08-09T01:21:43.149439: step 6771, loss 0.562586.
Train: 2018-08-09T01:21:46.254695: step 6772, loss 0.480883.
Train: 2018-08-09T01:21:49.345914: step 6773, loss 0.480815.
Train: 2018-08-09T01:21:52.405048: step 6774, loss 0.562565.
Train: 2018-08-09T01:21:55.498271: step 6775, loss 0.546143.
Train: 2018-08-09T01:21:58.565426: step 6776, loss 0.546083.
Train: 2018-08-09T01:22:01.670682: step 6777, loss 0.595481.
Train: 2018-08-09T01:22:04.739842: step 6778, loss 0.645002.
Train: 2018-08-09T01:22:07.844095: step 6779, loss 0.562511.
Train: 2018-08-09T01:22:10.940328: step 6780, loss 0.595472.
Test: 2018-08-09T01:22:31.909078: step 6780, loss 0.548341.
Train: 2018-08-09T01:22:35.011326: step 6781, loss 0.496662.
Train: 2018-08-09T01:22:38.075473: step 6782, loss 0.562503.
Train: 2018-08-09T01:22:41.164686: step 6783, loss 0.578989.
Train: 2018-08-09T01:22:44.221814: step 6784, loss 0.513066.
Train: 2018-08-09T01:22:47.302003: step 6785, loss 0.562482.
Train: 2018-08-09T01:22:50.410268: step 6786, loss 0.612021.
Train: 2018-08-09T01:22:53.493465: step 6787, loss 0.480097.
Train: 2018-08-09T01:22:56.583681: step 6788, loss 0.579085.
Train: 2018-08-09T01:22:59.674899: step 6789, loss 0.562522.
Train: 2018-08-09T01:23:02.741051: step 6790, loss 0.661623.
Test: 2018-08-09T01:23:23.690751: step 6790, loss 0.546398.
Train: 2018-08-09T01:23:26.780967: step 6791, loss 0.546025.
Train: 2018-08-09T01:23:29.907279: step 6792, loss 0.529512.
Train: 2018-08-09T01:23:32.986465: step 6793, loss 0.546009.
Train: 2018-08-09T01:23:36.084703: step 6794, loss 0.595438.
Train: 2018-08-09T01:23:39.157874: step 6795, loss 0.562485.
Train: 2018-08-09T01:23:42.205978: step 6796, loss 0.529599.
Train: 2018-08-09T01:23:45.280151: step 6797, loss 0.644865.
Train: 2018-08-09T01:23:48.386410: step 6798, loss 0.677745.
Train: 2018-08-09T01:23:51.469607: step 6799, loss 0.513356.
Train: 2018-08-09T01:23:54.572858: step 6800, loss 0.546278.
Test: 2018-08-09T01:24:15.510526: step 6800, loss 0.5467.
Train: 2018-08-09T01:24:20.330340: step 6801, loss 0.480988.
Train: 2018-08-09T01:24:23.439607: step 6802, loss 0.725867.
Train: 2018-08-09T01:24:26.519796: step 6803, loss 0.562657.
Train: 2018-08-09T01:24:29.606001: step 6804, loss 0.530207.
Train: 2018-08-09T01:24:32.689199: step 6805, loss 0.627512.
Train: 2018-08-09T01:24:35.741313: step 6806, loss 0.578878.
Train: 2018-08-09T01:24:38.827519: step 6807, loss 0.530488.
Train: 2018-08-09T01:24:41.911719: step 6808, loss 0.594769.
Train: 2018-08-09T01:24:44.997924: step 6809, loss 0.594808.
Train: 2018-08-09T01:24:48.056055: step 6810, loss 0.514503.
Test: 2018-08-09T01:25:08.974672: step 6810, loss 0.549549.
Train: 2018-08-09T01:25:12.092962: step 6811, loss 0.611211.
Train: 2018-08-09T01:25:15.176161: step 6812, loss 0.498264.
Train: 2018-08-09T01:25:18.248328: step 6813, loss 0.498417.
Train: 2018-08-09T01:25:21.328517: step 6814, loss 0.595625.
Train: 2018-08-09T01:25:24.408706: step 6815, loss 0.481196.
Train: 2018-08-09T01:25:27.478869: step 6816, loss 0.513782.
Train: 2018-08-09T01:25:30.605181: step 6817, loss 0.594172.
Train: 2018-08-09T01:25:33.685370: step 6818, loss 0.596609.
Train: 2018-08-09T01:25:36.781603: step 6819, loss 0.596041.
Train: 2018-08-09T01:25:39.833717: step 6820, loss 0.594675.
Test: 2018-08-09T01:26:00.771385: step 6820, loss 0.546859.
Train: 2018-08-09T01:26:03.837537: step 6821, loss 0.511091.
Train: 2018-08-09T01:26:06.909705: step 6822, loss 0.546194.
Train: 2018-08-09T01:26:09.999921: step 6823, loss 0.49612.
Train: 2018-08-09T01:26:13.087129: step 6824, loss 0.526171.
Train: 2018-08-09T01:26:16.187372: step 6825, loss 0.560759.
Train: 2018-08-09T01:26:19.320702: step 6826, loss 0.559524.
Train: 2018-08-09T01:26:22.408913: step 6827, loss 0.58439.
Train: 2018-08-09T01:26:25.497124: step 6828, loss 0.59855.
Train: 2018-08-09T01:26:28.581324: step 6829, loss 0.576588.
Train: 2018-08-09T01:26:31.624415: step 6830, loss 0.61574.
Test: 2018-08-09T01:26:52.559075: step 6830, loss 0.549583.
Train: 2018-08-09T01:26:55.611189: step 6831, loss 0.668544.
Train: 2018-08-09T01:26:58.694386: step 6832, loss 0.668089.
Train: 2018-08-09T01:27:01.782597: step 6833, loss 0.57833.
Train: 2018-08-09T01:27:04.869805: step 6834, loss 0.564089.
Train: 2018-08-09T01:27:07.940970: step 6835, loss 0.61002.
Train: 2018-08-09T01:27:11.033192: step 6836, loss 0.642277.
Train: 2018-08-09T01:27:14.106362: step 6837, loss 0.563184.
Train: 2018-08-09T01:27:17.184546: step 6838, loss 0.578929.
Train: 2018-08-09T01:27:20.266741: step 6839, loss 0.532556.
Train: 2018-08-09T01:27:23.347933: step 6840, loss 0.548136.
Test: 2018-08-09T01:27:44.295628: step 6840, loss 0.550402.
Train: 2018-08-09T01:27:47.422942: step 6841, loss 0.532984.
Train: 2018-08-09T01:27:50.547249: step 6842, loss 0.65564.
Train: 2018-08-09T01:27:53.632451: step 6843, loss 0.640151.
Train: 2018-08-09T01:27:56.695595: step 6844, loss 0.548593.
Train: 2018-08-09T01:27:59.802857: step 6845, loss 0.609429.
Train: 2018-08-09T01:28:02.903099: step 6846, loss 0.624546.
Train: 2018-08-09T01:28:05.972259: step 6847, loss 0.53386.
Train: 2018-08-09T01:28:09.025377: step 6848, loss 0.654439.
Train: 2018-08-09T01:28:12.110580: step 6849, loss 0.579193.
Train: 2018-08-09T01:28:15.173723: step 6850, loss 0.549313.
Test: 2018-08-09T01:28:36.123423: step 6850, loss 0.553174.
Train: 2018-08-09T01:28:39.204615: step 6851, loss 0.579257.
Train: 2018-08-09T01:28:42.296836: step 6852, loss 0.609032.
Train: 2018-08-09T01:28:45.393070: step 6853, loss 0.549617.
Train: 2018-08-09T01:28:48.484287: step 6854, loss 0.579319.
Train: 2018-08-09T01:28:51.571495: step 6855, loss 0.594139.
Train: 2018-08-09T01:28:54.628623: step 6856, loss 0.564571.
Train: 2018-08-09T01:28:57.708813: step 6857, loss 0.520286.
Train: 2018-08-09T01:29:00.797023: step 6858, loss 0.535021.
Train: 2018-08-09T01:29:03.871197: step 6859, loss 0.505317.
Train: 2018-08-09T01:29:06.947375: step 6860, loss 0.519873.
Test: 2018-08-09T01:29:27.902088: step 6860, loss 0.549754.
Train: 2018-08-09T01:29:30.989296: step 6861, loss 0.534318.
Train: 2018-08-09T01:29:34.073496: step 6862, loss 0.549259.
Train: 2018-08-09T01:29:37.145664: step 6863, loss 0.594479.
Train: 2018-08-09T01:29:40.236883: step 6864, loss 0.57895.
Train: 2018-08-09T01:29:43.314064: step 6865, loss 0.579215.
Train: 2018-08-09T01:29:46.417315: step 6866, loss 0.502883.
Train: 2018-08-09T01:29:49.502518: step 6867, loss 0.532795.
Train: 2018-08-09T01:29:52.608776: step 6868, loss 0.532571.
Train: 2018-08-09T01:29:55.674928: step 6869, loss 0.454292.
Train: 2018-08-09T01:29:58.767150: step 6870, loss 0.467047.
Test: 2018-08-09T01:30:19.702812: step 6870, loss 0.548406.
Train: 2018-08-09T01:30:22.824110: step 6871, loss 0.510121.
Train: 2018-08-09T01:30:25.897281: step 6872, loss 0.563956.
Train: 2018-08-09T01:30:28.954409: step 6873, loss 0.510038.
Train: 2018-08-09T01:30:32.048636: step 6874, loss 0.467306.
Train: 2018-08-09T01:30:35.124815: step 6875, loss 0.527819.
Train: 2018-08-09T01:30:38.188962: step 6876, loss 0.58515.
Train: 2018-08-09T01:30:41.283188: step 6877, loss 0.586273.
Train: 2018-08-09T01:30:44.333298: step 6878, loss 0.585634.
Train: 2018-08-09T01:30:47.444570: step 6879, loss 0.463701.
Train: 2018-08-09T01:30:50.534786: step 6880, loss 0.558627.
Test: 2018-08-09T01:31:11.495514: step 6880, loss 0.545677.
Train: 2018-08-09T01:31:14.577709: step 6881, loss 0.520038.
Train: 2018-08-09T01:31:17.665920: step 6882, loss 0.580136.
Train: 2018-08-09T01:31:20.741096: step 6883, loss 0.587198.
Train: 2018-08-09T01:31:23.832315: step 6884, loss 0.553258.
Train: 2018-08-09T01:31:26.891448: step 6885, loss 0.492767.
Train: 2018-08-09T01:31:29.966624: step 6886, loss 0.455391.
Train: 2018-08-09T01:31:33.046813: step 6887, loss 0.577727.
Train: 2018-08-09T01:31:36.114971: step 6888, loss 0.593976.
Train: 2018-08-09T01:31:39.206189: step 6889, loss 0.619368.
Train: 2018-08-09T01:31:42.295403: step 6890, loss 0.544467.
Test: 2018-08-09T01:32:03.263151: step 6890, loss 0.548728.
Train: 2018-08-09T01:32:06.341335: step 6891, loss 0.59455.
Train: 2018-08-09T01:32:09.494718: step 6892, loss 0.547223.
Train: 2018-08-09T01:32:12.595963: step 6893, loss 0.527604.
Train: 2018-08-09T01:32:15.664121: step 6894, loss 0.61255.
Train: 2018-08-09T01:32:18.770379: step 6895, loss 0.527761.
Train: 2018-08-09T01:32:21.848564: step 6896, loss 0.512734.
Train: 2018-08-09T01:32:24.923740: step 6897, loss 0.560543.
Train: 2018-08-09T01:32:28.002926: step 6898, loss 0.496116.
Train: 2018-08-09T01:32:31.074092: step 6899, loss 0.51285.
Train: 2018-08-09T01:32:34.176340: step 6900, loss 0.495145.
Test: 2018-08-09T01:32:55.143085: step 6900, loss 0.550961.
Train: 2018-08-09T01:33:00.033086: step 6901, loss 0.595879.
Train: 2018-08-09T01:33:03.117286: step 6902, loss 0.562251.
Train: 2018-08-09T01:33:06.176419: step 6903, loss 0.493732.
Train: 2018-08-09T01:33:09.254603: step 6904, loss 0.681515.
Train: 2018-08-09T01:33:12.382921: step 6905, loss 0.646122.
Train: 2018-08-09T01:33:15.500209: step 6906, loss 0.631789.
Train: 2018-08-09T01:33:18.570371: step 6907, loss 0.664365.
Train: 2018-08-09T01:33:21.656577: step 6908, loss 0.513124.
Train: 2018-08-09T01:33:24.729747: step 6909, loss 0.530153.
Train: 2018-08-09T01:33:27.818960: step 6910, loss 0.497327.
Test: 2018-08-09T01:33:48.848874: step 6910, loss 0.548574.
Train: 2018-08-09T01:33:51.898983: step 6911, loss 0.612348.
Train: 2018-08-09T01:33:54.978170: step 6912, loss 0.627627.
Train: 2018-08-09T01:33:58.079415: step 6913, loss 0.659802.
Train: 2018-08-09T01:34:01.161610: step 6914, loss 0.577754.
Train: 2018-08-09T01:34:04.228764: step 6915, loss 0.436477.
Train: 2018-08-09T01:34:07.316975: step 6916, loss 0.49957.
Train: 2018-08-09T01:34:10.423234: step 6917, loss 0.499628.
Train: 2018-08-09T01:34:13.530495: step 6918, loss 0.418266.
Train: 2018-08-09T01:34:16.653799: step 6919, loss 0.611216.
Train: 2018-08-09T01:34:19.771087: step 6920, loss 0.562756.
Test: 2018-08-09T01:34:40.732819: step 6920, loss 0.546097.
Train: 2018-08-09T01:34:43.808997: step 6921, loss 0.546266.
Train: 2018-08-09T01:34:46.898211: step 6922, loss 0.628359.
Train: 2018-08-09T01:34:49.807947: step 6923, loss 0.649721.
Train: 2018-08-09T01:34:52.942280: step 6924, loss 0.546186.
Train: 2018-08-09T01:34:56.039515: step 6925, loss 0.530235.
Train: 2018-08-09T01:34:59.104665: step 6926, loss 0.44866.
Train: 2018-08-09T01:35:02.186859: step 6927, loss 0.579027.
Train: 2018-08-09T01:35:05.285096: step 6928, loss 0.562547.
Train: 2018-08-09T01:35:08.386342: step 6929, loss 0.480575.
Train: 2018-08-09T01:35:11.465528: step 6930, loss 0.562482.
Test: 2018-08-09T01:35:32.427260: step 6930, loss 0.548288.
Train: 2018-08-09T01:35:35.532516: step 6931, loss 0.677999.
Train: 2018-08-09T01:35:38.625740: step 6932, loss 0.562425.
Train: 2018-08-09T01:35:41.716958: step 6933, loss 0.54582.
Train: 2018-08-09T01:35:44.838257: step 6934, loss 0.562385.
Train: 2018-08-09T01:35:47.931481: step 6935, loss 0.578565.
Train: 2018-08-09T01:35:51.025708: step 6936, loss 0.579534.
Train: 2018-08-09T01:35:54.136980: step 6937, loss 0.495562.
Train: 2018-08-09T01:35:57.214161: step 6938, loss 0.629819.
Train: 2018-08-09T01:36:00.279311: step 6939, loss 0.51229.
Train: 2018-08-09T01:36:03.387575: step 6940, loss 0.731325.
Test: 2018-08-09T01:36:24.403451: step 6940, loss 0.550045.
Train: 2018-08-09T01:36:27.475618: step 6941, loss 0.530291.
Train: 2018-08-09T01:36:30.707210: step 6942, loss 0.661845.
Train: 2018-08-09T01:36:33.834525: step 6943, loss 0.660958.
Train: 2018-08-09T01:36:36.973871: step 6944, loss 0.578986.
Train: 2018-08-09T01:36:40.073111: step 6945, loss 0.54653.
Train: 2018-08-09T01:36:43.165333: step 6946, loss 0.723923.
Train: 2018-08-09T01:36:46.267581: step 6947, loss 0.498809.
Train: 2018-08-09T01:36:49.319695: step 6948, loss 0.515099.
Train: 2018-08-09T01:36:52.374818: step 6949, loss 0.578835.
Train: 2018-08-09T01:36:55.504138: step 6950, loss 0.65807.
Test: 2018-08-09T01:37:16.479908: step 6950, loss 0.547727.
Train: 2018-08-09T01:37:19.560096: step 6951, loss 0.531561.
Train: 2018-08-09T01:37:22.645299: step 6952, loss 0.594558.
Train: 2018-08-09T01:37:25.773617: step 6953, loss 0.688693.
Train: 2018-08-09T01:37:28.878872: step 6954, loss 0.57892.
Train: 2018-08-09T01:37:31.980118: step 6955, loss 0.57893.
Train: 2018-08-09T01:37:35.091390: step 6956, loss 0.578916.
Train: 2018-08-09T01:37:38.197648: step 6957, loss 0.548153.
Train: 2018-08-09T01:37:41.310926: step 6958, loss 0.594263.
Train: 2018-08-09T01:37:44.390112: step 6959, loss 0.563925.
Train: 2018-08-09T01:37:47.466291: step 6960, loss 0.594411.
Test: 2018-08-09T01:38:08.411981: step 6960, loss 0.55305.
Train: 2018-08-09T01:38:11.931337: step 6961, loss 0.563915.
Train: 2018-08-09T01:38:15.031580: step 6962, loss 0.548667.
Train: 2018-08-09T01:38:18.094724: step 6963, loss 0.503231.
Train: 2018-08-09T01:38:21.171905: step 6964, loss 0.579038.
Train: 2018-08-09T01:38:24.264127: step 6965, loss 0.654964.
Train: 2018-08-09T01:38:27.359356: step 6966, loss 0.624564.
Train: 2018-08-09T01:38:30.428516: step 6967, loss 0.488371.
Train: 2018-08-09T01:38:33.540791: step 6968, loss 0.579091.
Train: 2018-08-09T01:38:36.645044: step 6969, loss 0.503482.
Train: 2018-08-09T01:38:39.719217: step 6970, loss 0.594229.
Test: 2018-08-09T01:39:00.661899: step 6970, loss 0.549709.
Train: 2018-08-09T01:39:03.751111: step 6971, loss 0.639731.
Train: 2018-08-09T01:39:06.891461: step 6972, loss 0.609387.
Train: 2018-08-09T01:39:09.990701: step 6973, loss 0.579089.
Train: 2018-08-09T01:39:13.089941: step 6974, loss 0.594227.
Train: 2018-08-09T01:39:16.186173: step 6975, loss 0.609322.
Train: 2018-08-09T01:39:19.248314: step 6976, loss 0.609264.
Train: 2018-08-09T01:39:22.317474: step 6977, loss 0.549018.
Train: 2018-08-09T01:39:25.396661: step 6978, loss 0.473877.
Train: 2018-08-09T01:39:28.482866: step 6979, loss 0.639352.
Train: 2018-08-09T01:39:31.606170: step 6980, loss 0.518618.
Test: 2018-08-09T01:39:52.547849: step 6980, loss 0.551492.
Train: 2018-08-09T01:39:55.658118: step 6981, loss 0.63936.
Train: 2018-08-09T01:39:58.737304: step 6982, loss 0.563734.
Train: 2018-08-09T01:40:01.831531: step 6983, loss 0.563104.
Train: 2018-08-09T01:40:04.899689: step 6984, loss 0.548276.
Train: 2018-08-09T01:40:07.989904: step 6985, loss 0.597516.
Train: 2018-08-09T01:40:11.063075: step 6986, loss 0.641929.
Train: 2018-08-09T01:40:14.126219: step 6987, loss 0.517471.
Train: 2018-08-09T01:40:17.213428: step 6988, loss 0.532888.
Train: 2018-08-09T01:40:20.296625: step 6989, loss 0.534607.
Train: 2018-08-09T01:40:23.425945: step 6990, loss 0.502257.
Test: 2018-08-09T01:40:44.399708: step 6990, loss 0.552655.
Train: 2018-08-09T01:40:47.520004: step 6991, loss 0.563683.
Train: 2018-08-09T01:40:50.637292: step 6992, loss 0.609914.
Train: 2018-08-09T01:40:53.727508: step 6993, loss 0.565085.
Train: 2018-08-09T01:40:56.838780: step 6994, loss 0.563219.
Train: 2018-08-09T01:40:59.897914: step 6995, loss 0.532559.
Train: 2018-08-09T01:41:02.999159: step 6996, loss 0.579074.
Train: 2018-08-09T01:41:06.097396: step 6997, loss 0.501177.
Train: 2018-08-09T01:41:09.151516: step 6998, loss 0.609802.
Train: 2018-08-09T01:41:12.255770: step 6999, loss 0.625698.
Train: 2018-08-09T01:41:15.353005: step 7000, loss 0.438647.
Test: 2018-08-09T01:41:36.329776: step 7000, loss 0.550254.
Train: 2018-08-09T01:41:41.283948: step 7001, loss 0.594085.
Train: 2018-08-09T01:41:44.375166: step 7002, loss 0.484905.
Train: 2018-08-09T01:41:47.460369: step 7003, loss 0.51506.
Train: 2018-08-09T01:41:50.523513: step 7004, loss 0.594425.
Train: 2018-08-09T01:41:53.621750: step 7005, loss 0.627723.
Train: 2018-08-09T01:41:56.699935: step 7006, loss 0.645146.
Train: 2018-08-09T01:41:59.791153: step 7007, loss 0.561533.
Train: 2018-08-09T01:42:02.872345: step 7008, loss 0.593077.
Train: 2018-08-09T01:42:05.953537: step 7009, loss 0.562348.
Train: 2018-08-09T01:42:09.046761: step 7010, loss 0.562552.
Test: 2018-08-09T01:42:29.985432: step 7010, loss 0.548198.
Train: 2018-08-09T01:42:33.084671: step 7011, loss 0.562876.
Train: 2018-08-09T01:42:36.191933: step 7012, loss 0.659608.
Train: 2018-08-09T01:42:39.278138: step 7013, loss 0.499035.
Train: 2018-08-09T01:42:42.350306: step 7014, loss 0.53026.
Train: 2018-08-09T01:42:45.417461: step 7015, loss 0.593451.
Train: 2018-08-09T01:42:48.526727: step 7016, loss 0.596047.
Train: 2018-08-09T01:42:51.640005: step 7017, loss 0.530542.
Train: 2018-08-09T01:42:54.736237: step 7018, loss 0.578281.
Train: 2018-08-09T01:42:57.821440: step 7019, loss 0.497447.
Train: 2018-08-09T01:43:00.906642: step 7020, loss 0.482065.
Test: 2018-08-09T01:43:21.854337: step 7020, loss 0.550268.
Train: 2018-08-09T01:43:25.026771: step 7021, loss 0.560305.
Train: 2018-08-09T01:43:28.113979: step 7022, loss 0.479263.
Train: 2018-08-09T01:43:31.198179: step 7023, loss 0.577993.
Train: 2018-08-09T01:43:34.314465: step 7024, loss 0.651349.
Train: 2018-08-09T01:43:37.401673: step 7025, loss 0.580217.
Train: 2018-08-09T01:43:40.502918: step 7026, loss 0.611799.
Train: 2018-08-09T01:43:43.597144: step 7027, loss 0.578173.
Train: 2018-08-09T01:43:46.668310: step 7028, loss 0.662943.
Train: 2018-08-09T01:43:49.761534: step 7029, loss 0.614587.
Train: 2018-08-09T01:43:52.853755: step 7030, loss 0.543242.
Test: 2018-08-09T01:44:13.810474: step 7030, loss 0.547468.
Train: 2018-08-09T01:44:16.916732: step 7031, loss 0.530863.
Train: 2018-08-09T01:44:19.994916: step 7032, loss 0.691025.
Train: 2018-08-09T01:44:23.113207: step 7033, loss 0.547486.
Train: 2018-08-09T01:44:26.208436: step 7034, loss 0.610446.
Train: 2018-08-09T01:44:29.308679: step 7035, loss 0.500038.
Train: 2018-08-09T01:44:32.388868: step 7036, loss 0.546487.
Train: 2018-08-09T01:44:35.472066: step 7037, loss 0.628265.
Train: 2018-08-09T01:44:38.562282: step 7038, loss 0.531984.
Train: 2018-08-09T01:44:41.656508: step 7039, loss 0.564172.
Train: 2018-08-09T01:44:44.749732: step 7040, loss 0.455099.
Test: 2018-08-09T01:45:05.718483: step 7040, loss 0.548726.
Train: 2018-08-09T01:45:08.807696: step 7041, loss 0.485072.
Train: 2018-08-09T01:45:11.945037: step 7042, loss 0.532387.
Train: 2018-08-09T01:45:14.996149: step 7043, loss 0.609953.
Train: 2018-08-09T01:45:18.119453: step 7044, loss 0.57835.
Train: 2018-08-09T01:45:21.223707: step 7045, loss 0.594017.
Train: 2018-08-09T01:45:24.319939: step 7046, loss 0.500343.
Train: 2018-08-09T01:45:27.426197: step 7047, loss 0.579107.
Train: 2018-08-09T01:45:30.505384: step 7048, loss 0.51511.
Train: 2018-08-09T01:45:33.587579: step 7049, loss 0.546957.
Train: 2018-08-09T01:45:36.690829: step 7050, loss 0.513532.
Test: 2018-08-09T01:45:57.658577: step 7050, loss 0.550577.
Train: 2018-08-09T01:46:00.797924: step 7051, loss 0.529774.
Train: 2018-08-09T01:46:03.895158: step 7052, loss 0.612866.
Train: 2018-08-09T01:46:06.986377: step 7053, loss 0.512739.
Train: 2018-08-09T01:46:10.113692: step 7054, loss 0.531538.
Train: 2018-08-09T01:46:13.199897: step 7055, loss 0.613881.
Train: 2018-08-09T01:46:16.277078: step 7056, loss 0.563519.
Train: 2018-08-09T01:46:19.393364: step 7057, loss 0.579621.
Train: 2018-08-09T01:46:22.507644: step 7058, loss 0.428552.
Train: 2018-08-09T01:46:25.600867: step 7059, loss 0.528698.
Train: 2018-08-09T01:46:28.704118: step 7060, loss 0.528441.
Test: 2018-08-09T01:46:49.691919: step 7060, loss 0.546919.
Train: 2018-08-09T01:46:52.785143: step 7061, loss 0.540983.
Train: 2018-08-09T01:46:55.899423: step 7062, loss 0.532064.
Train: 2018-08-09T01:46:59.011698: step 7063, loss 0.596296.
Train: 2018-08-09T01:47:02.091887: step 7064, loss 0.582829.
Train: 2018-08-09T01:47:05.204162: step 7065, loss 0.500622.
Train: 2018-08-09T01:47:08.296383: step 7066, loss 0.653941.
Train: 2018-08-09T01:47:11.396626: step 7067, loss 0.546091.
Train: 2018-08-09T01:47:14.551013: step 7068, loss 0.50731.
Train: 2018-08-09T01:47:17.651255: step 7069, loss 0.611555.
Train: 2018-08-09T01:47:20.738463: step 7070, loss 0.540392.
Test: 2018-08-09T01:47:41.730275: step 7070, loss 0.547334.
Train: 2018-08-09T01:47:44.813472: step 7071, loss 0.545833.
Train: 2018-08-09T01:47:47.899677: step 7072, loss 0.585382.
Train: 2018-08-09T01:47:51.016965: step 7073, loss 0.443483.
Train: 2018-08-09T01:47:54.130243: step 7074, loss 0.578558.
Train: 2018-08-09T01:47:57.234496: step 7075, loss 0.679617.
Train: 2018-08-09T01:48:00.301651: step 7076, loss 0.542975.
Train: 2018-08-09T01:48:03.380838: step 7077, loss 0.616105.
Train: 2018-08-09T01:48:06.453006: step 7078, loss 0.480064.
Train: 2018-08-09T01:48:09.533195: step 7079, loss 0.479574.
Train: 2018-08-09T01:48:12.606366: step 7080, loss 0.595493.
Test: 2018-08-09T01:48:33.609207: step 7080, loss 0.548876.
Train: 2018-08-09T01:48:36.730505: step 7081, loss 0.595686.
Train: 2018-08-09T01:48:39.888903: step 7082, loss 0.462958.
Train: 2018-08-09T01:48:42.967087: step 7083, loss 0.561682.
Train: 2018-08-09T01:48:46.085377: step 7084, loss 0.546256.
Train: 2018-08-09T01:48:49.167572: step 7085, loss 0.464089.
Train: 2018-08-09T01:48:52.257788: step 7086, loss 0.528775.
Train: 2018-08-09T01:48:55.354020: step 7087, loss 0.528592.
Train: 2018-08-09T01:48:58.458273: step 7088, loss 0.545961.
Train: 2018-08-09T01:49:01.537460: step 7089, loss 0.545434.
Train: 2018-08-09T01:49:04.661767: step 7090, loss 0.579005.
Test: 2018-08-09T01:49:25.652576: step 7090, loss 0.549157.
Train: 2018-08-09T01:49:28.756829: step 7091, loss 0.62997.
Train: 2018-08-09T01:49:31.885146: step 7092, loss 0.596298.
Train: 2018-08-09T01:49:34.979373: step 7093, loss 0.596264.
Train: 2018-08-09T01:49:38.084629: step 7094, loss 0.49521.
Train: 2018-08-09T01:49:41.206930: step 7095, loss 0.512277.
Train: 2018-08-09T01:49:44.276090: step 7096, loss 0.579488.
Train: 2018-08-09T01:49:47.380344: step 7097, loss 0.51219.
Train: 2018-08-09T01:49:50.467551: step 7098, loss 0.460644.
Train: 2018-08-09T01:49:53.549746: step 7099, loss 0.545395.
Train: 2018-08-09T01:49:56.643973: step 7100, loss 0.612731.
Test: 2018-08-09T01:50:17.643806: step 7100, loss 0.548364.
Train: 2018-08-09T01:50:22.442564: step 7101, loss 0.596173.
Train: 2018-08-09T01:50:25.543809: step 7102, loss 0.476095.
Train: 2018-08-09T01:50:28.664106: step 7103, loss 0.561117.
Train: 2018-08-09T01:50:31.761340: step 7104, loss 0.614192.
Train: 2018-08-09T01:50:34.868601: step 7105, loss 0.526897.
Train: 2018-08-09T01:50:37.976866: step 7106, loss 0.681508.
Train: 2018-08-09T01:50:41.053044: step 7107, loss 0.561548.
Train: 2018-08-09T01:50:44.133234: step 7108, loss 0.613447.
Train: 2018-08-09T01:50:47.177327: step 7109, loss 0.509536.
Train: 2018-08-09T01:50:50.260524: step 7110, loss 0.545858.
Test: 2018-08-09T01:51:11.224262: step 7110, loss 0.548259.
Train: 2018-08-09T01:51:14.339544: step 7111, loss 0.494224.
Train: 2018-08-09T01:51:17.415723: step 7112, loss 0.59536.
Train: 2018-08-09T01:51:20.494909: step 7113, loss 0.49481.
Train: 2018-08-09T01:51:23.579109: step 7114, loss 0.546963.
Train: 2018-08-09T01:51:26.668323: step 7115, loss 0.525414.
Train: 2018-08-09T01:51:29.782603: step 7116, loss 0.576501.
Train: 2018-08-09T01:51:32.917939: step 7117, loss 0.529374.
Train: 2018-08-09T01:51:36.013168: step 7118, loss 0.491043.
Train: 2018-08-09T01:51:39.056259: step 7119, loss 0.598438.
Train: 2018-08-09T01:51:42.134443: step 7120, loss 0.544816.
Test: 2018-08-09T01:52:03.098180: step 7120, loss 0.549297.
Train: 2018-08-09T01:52:06.193409: step 7121, loss 0.510592.
Train: 2018-08-09T01:52:09.274601: step 7122, loss 0.529106.
Train: 2018-08-09T01:52:12.365820: step 7123, loss 0.528636.
Train: 2018-08-09T01:52:15.471076: step 7124, loss 0.579059.
Train: 2018-08-09T01:52:18.589367: step 7125, loss 0.61782.
Train: 2018-08-09T01:52:21.664542: step 7126, loss 0.542601.
Train: 2018-08-09T01:52:24.757767: step 7127, loss 0.50836.
Train: 2018-08-09T01:52:27.843972: step 7128, loss 0.565073.
Train: 2018-08-09T01:52:30.938199: step 7129, loss 0.630033.
Train: 2018-08-09T01:52:34.003348: step 7130, loss 0.615369.
Test: 2018-08-09T01:52:54.991149: step 7130, loss 0.546075.
Train: 2018-08-09T01:52:58.075349: step 7131, loss 0.51146.
Train: 2018-08-09T01:53:01.161554: step 7132, loss 0.526325.
Train: 2018-08-09T01:53:04.244751: step 7133, loss 0.565573.
Train: 2018-08-09T01:53:07.344994: step 7134, loss 0.562175.
Train: 2018-08-09T01:53:10.452255: step 7135, loss 0.559843.
Train: 2018-08-09T01:53:13.558514: step 7136, loss 0.597358.
Train: 2018-08-09T01:53:16.653744: step 7137, loss 0.647081.
Train: 2018-08-09T01:53:19.768024: step 7138, loss 0.529226.
Train: 2018-08-09T01:53:22.852224: step 7139, loss 0.577578.
Train: 2018-08-09T01:53:25.956477: step 7140, loss 0.749324.
Test: 2018-08-09T01:53:46.958316: step 7140, loss 0.549428.
Train: 2018-08-09T01:53:50.059560: step 7141, loss 0.628042.
Train: 2018-08-09T01:53:53.189883: step 7142, loss 0.546778.
Train: 2018-08-09T01:53:56.278094: step 7143, loss 0.513987.
Train: 2018-08-09T01:53:59.341238: step 7144, loss 0.482002.
Train: 2018-08-09T01:54:02.446494: step 7145, loss 0.529503.
Train: 2018-08-09T01:54:05.509638: step 7146, loss 0.643129.
Train: 2018-08-09T01:54:08.600857: step 7147, loss 0.514751.
Train: 2018-08-09T01:54:11.713131: step 7148, loss 0.578974.
Train: 2018-08-09T01:54:14.826409: step 7149, loss 0.62697.
Train: 2018-08-09T01:54:17.893563: step 7150, loss 0.595129.
Test: 2018-08-09T01:54:38.883370: step 7150, loss 0.5499.
Train: 2018-08-09T01:54:41.973586: step 7151, loss 0.483701.
Train: 2018-08-09T01:54:45.079844: step 7152, loss 0.642323.
Train: 2018-08-09T01:54:48.170060: step 7153, loss 0.499742.
Train: 2018-08-09T01:54:51.281332: step 7154, loss 0.642142.
Train: 2018-08-09T01:54:54.382578: step 7155, loss 0.515762.
Train: 2018-08-09T01:54:57.476804: step 7156, loss 0.610405.
Train: 2018-08-09T01:55:00.556994: step 7157, loss 0.500199.
Train: 2018-08-09T01:55:03.634175: step 7158, loss 0.62603.
Train: 2018-08-09T01:55:06.736423: step 7159, loss 0.563154.
Train: 2018-08-09T01:55:09.850703: step 7160, loss 0.59458.
Test: 2018-08-09T01:55:30.859560: step 7160, loss 0.549684.
Train: 2018-08-09T01:55:33.943760: step 7161, loss 0.578862.
Train: 2018-08-09T01:55:37.044003: step 7162, loss 0.500546.
Train: 2018-08-09T01:55:40.140234: step 7163, loss 0.625924.
Train: 2018-08-09T01:55:43.234461: step 7164, loss 0.469219.
Train: 2018-08-09T01:55:46.347738: step 7165, loss 0.594582.
Train: 2018-08-09T01:55:49.431939: step 7166, loss 0.484608.
Train: 2018-08-09T01:55:52.545216: step 7167, loss 0.563064.
Train: 2018-08-09T01:55:55.662504: step 7168, loss 0.436543.
Train: 2018-08-09T01:55:58.753723: step 7169, loss 0.514814.
Train: 2018-08-09T01:56:01.846947: step 7170, loss 0.498165.
Test: 2018-08-09T01:56:22.880871: step 7170, loss 0.550986.
Train: 2018-08-09T01:56:26.004175: step 7171, loss 0.612376.
Train: 2018-08-09T01:56:29.086369: step 7172, loss 0.547991.
Train: 2018-08-09T01:56:32.185609: step 7173, loss 0.513085.
Train: 2018-08-09T01:56:35.301894: step 7174, loss 0.514353.
Train: 2018-08-09T01:56:38.384089: step 7175, loss 0.658062.
Train: 2018-08-09T01:56:41.470294: step 7176, loss 0.5597.
Train: 2018-08-09T01:56:44.578558: step 7177, loss 0.64835.
Train: 2018-08-09T01:56:47.637691: step 7178, loss 0.515735.
Train: 2018-08-09T01:56:50.742948: step 7179, loss 0.543452.
Train: 2018-08-09T01:56:53.837174: step 7180, loss 0.564174.
Test: 2018-08-09T01:57:14.818959: step 7180, loss 0.546507.
Train: 2018-08-09T01:57:17.903159: step 7181, loss 0.580844.
Train: 2018-08-09T01:57:21.003401: step 7182, loss 0.66446.
Train: 2018-08-09T01:57:24.060530: step 7183, loss 0.479076.
Train: 2018-08-09T01:57:27.154757: step 7184, loss 0.545386.
Train: 2018-08-09T01:57:30.257005: step 7185, loss 0.58058.
Train: 2018-08-09T01:57:33.366271: step 7186, loss 0.628621.
Train: 2018-08-09T01:57:36.481554: step 7187, loss 0.431219.
Train: 2018-08-09T01:57:39.553722: step 7188, loss 0.52985.
Train: 2018-08-09T01:57:42.647949: step 7189, loss 0.562626.
Train: 2018-08-09T01:57:45.750196: step 7190, loss 0.529597.
Test: 2018-08-09T01:58:06.731982: step 7190, loss 0.547643.
Train: 2018-08-09T01:58:09.813173: step 7191, loss 0.463387.
Train: 2018-08-09T01:58:12.942493: step 7192, loss 0.644679.
Train: 2018-08-09T01:58:16.026693: step 7193, loss 0.512575.
Train: 2018-08-09T01:58:19.131949: step 7194, loss 0.563774.
Train: 2018-08-09T01:58:22.236203: step 7195, loss 0.662133.
Train: 2018-08-09T01:58:25.335443: step 7196, loss 0.511488.
Train: 2018-08-09T01:58:28.408614: step 7197, loss 0.528086.
Train: 2018-08-09T01:58:31.498830: step 7198, loss 0.529178.
Train: 2018-08-09T01:58:34.583030: step 7199, loss 0.443299.
Train: 2018-08-09T01:58:37.688285: step 7200, loss 0.547.
Test: 2018-08-09T01:58:58.648012: step 7200, loss 0.545974.
Train: 2018-08-09T01:59:03.560072: step 7201, loss 0.567772.
Train: 2018-08-09T01:59:06.657306: step 7202, loss 0.546803.
Train: 2018-08-09T01:59:09.778605: step 7203, loss 0.5797.
Train: 2018-08-09T01:59:12.913941: step 7204, loss 0.646924.
Train: 2018-08-09T01:59:16.036242: step 7205, loss 0.545087.
Train: 2018-08-09T01:59:19.131472: step 7206, loss 0.561967.
Train: 2018-08-09T01:59:22.215672: step 7207, loss 0.52925.
Train: 2018-08-09T01:59:25.297866: step 7208, loss 0.409747.
Train: 2018-08-09T01:59:28.394098: step 7209, loss 0.5116.
Train: 2018-08-09T01:59:31.496346: step 7210, loss 0.630634.
Test: 2018-08-09T01:59:52.482142: step 7210, loss 0.547682.
Train: 2018-08-09T01:59:55.557318: step 7211, loss 0.579894.
Train: 2018-08-09T01:59:58.655555: step 7212, loss 0.578908.
Train: 2018-08-09T02:00:01.779862: step 7213, loss 0.646986.
Train: 2018-08-09T02:00:04.860051: step 7214, loss 0.578761.
Train: 2018-08-09T02:00:07.948262: step 7215, loss 0.510276.
Train: 2018-08-09T02:00:11.044494: step 7216, loss 0.510199.
Train: 2018-08-09T02:00:14.167798: step 7217, loss 0.648889.
Train: 2018-08-09T02:00:17.235955: step 7218, loss 0.613249.
Train: 2018-08-09T02:00:20.334193: step 7219, loss 0.64888.
Train: 2018-08-09T02:00:23.442457: step 7220, loss 0.546078.
Test: 2018-08-09T02:00:44.427250: step 7220, loss 0.548333.
Train: 2018-08-09T02:00:47.527492: step 7221, loss 0.595332.
Train: 2018-08-09T02:00:50.631746: step 7222, loss 0.545162.
Train: 2018-08-09T02:00:53.737002: step 7223, loss 0.547821.
Train: 2018-08-09T02:00:56.661778: step 7224, loss 0.509816.
Train: 2018-08-09T02:00:59.751994: step 7225, loss 0.380793.
Train: 2018-08-09T02:01:02.872290: step 7226, loss 0.562645.
Train: 2018-08-09T02:01:05.961503: step 7227, loss 0.497258.
Train: 2018-08-09T02:01:09.027655: step 7228, loss 0.578786.
Train: 2018-08-09T02:01:12.119876: step 7229, loss 0.545209.
Train: 2018-08-09T02:01:15.210093: step 7230, loss 0.562832.
Test: 2018-08-09T02:01:36.208923: step 7230, loss 0.548724.
Train: 2018-08-09T02:01:39.340248: step 7231, loss 0.596257.
Train: 2018-08-09T02:01:42.422443: step 7232, loss 0.545859.
Train: 2018-08-09T02:01:45.513661: step 7233, loss 0.578831.
Train: 2018-08-09T02:01:48.584827: step 7234, loss 0.67899.
Train: 2018-08-09T02:01:51.661005: step 7235, loss 0.612233.
Train: 2018-08-09T02:01:54.765259: step 7236, loss 0.546078.
Train: 2018-08-09T02:01:57.880541: step 7237, loss 0.57897.
Train: 2018-08-09T02:02:00.956720: step 7238, loss 0.562576.
Train: 2018-08-09T02:02:04.026883: step 7239, loss 0.529842.
Train: 2018-08-09T02:02:07.098048: step 7240, loss 0.595445.
Test: 2018-08-09T02:02:28.076826: step 7240, loss 0.548473.
Train: 2018-08-09T02:02:31.168044: step 7241, loss 0.480687.
Train: 2018-08-09T02:02:34.314409: step 7242, loss 0.628317.
Train: 2018-08-09T02:02:37.443729: step 7243, loss 0.529906.
Train: 2018-08-09T02:02:40.526926: step 7244, loss 0.578917.
Train: 2018-08-09T02:02:43.623158: step 7245, loss 0.529949.
Train: 2018-08-09T02:02:46.729417: step 7246, loss 0.513631.
Train: 2018-08-09T02:02:49.854727: step 7247, loss 0.562635.
Train: 2018-08-09T02:02:52.943940: step 7248, loss 0.59528.
Train: 2018-08-09T02:02:56.084289: step 7249, loss 0.497208.
Train: 2018-08-09T02:02:59.151444: step 7250, loss 0.595301.
Test: 2018-08-09T02:03:20.130221: step 7250, loss 0.548477.
Train: 2018-08-09T02:03:23.210410: step 7251, loss 0.693548.
Train: 2018-08-09T02:03:26.284583: step 7252, loss 0.578952.
Train: 2018-08-09T02:03:29.374799: step 7253, loss 0.69306.
Train: 2018-08-09T02:03:32.481058: step 7254, loss 0.546413.
Train: 2018-08-09T02:03:35.589322: step 7255, loss 0.497988.
Train: 2018-08-09T02:03:38.751730: step 7256, loss 0.643515.
Train: 2018-08-09T02:03:41.831919: step 7257, loss 0.578848.
Train: 2018-08-09T02:03:44.922135: step 7258, loss 0.498595.
Train: 2018-08-09T02:03:48.002325: step 7259, loss 0.514739.
Train: 2018-08-09T02:03:51.087528: step 7260, loss 0.594944.
Test: 2018-08-09T02:04:12.080342: step 7260, loss 0.550862.
Train: 2018-08-09T02:04:15.170558: step 7261, loss 0.57887.
Train: 2018-08-09T02:04:18.236710: step 7262, loss 0.626868.
Train: 2018-08-09T02:04:21.335950: step 7263, loss 0.57886.
Train: 2018-08-09T02:04:24.483318: step 7264, loss 0.467302.
Train: 2018-08-09T02:04:27.580552: step 7265, loss 0.515127.
Train: 2018-08-09T02:04:30.666758: step 7266, loss 0.578865.
Train: 2018-08-09T02:04:33.772014: step 7267, loss 0.61078.
Train: 2018-08-09T02:04:36.898326: step 7268, loss 0.54694.
Train: 2018-08-09T02:04:39.996563: step 7269, loss 0.53098.
Train: 2018-08-09T02:04:43.095803: step 7270, loss 0.578859.
Test: 2018-08-09T02:05:04.034474: step 7270, loss 0.548506.
Train: 2018-08-09T02:05:07.104636: step 7271, loss 0.530911.
Train: 2018-08-09T02:05:10.213903: step 7272, loss 0.610872.
Train: 2018-08-09T02:05:13.319159: step 7273, loss 0.562852.
Train: 2018-08-09T02:05:16.408372: step 7274, loss 0.562854.
Train: 2018-08-09T02:05:19.483548: step 7275, loss 0.578891.
Train: 2018-08-09T02:05:22.585796: step 7276, loss 0.594885.
Train: 2018-08-09T02:05:25.685036: step 7277, loss 0.594892.
Train: 2018-08-09T02:05:28.774249: step 7278, loss 0.578881.
Train: 2018-08-09T02:05:31.904572: step 7279, loss 0.546919.
Train: 2018-08-09T02:05:34.994788: step 7280, loss 0.578871.
Test: 2018-08-09T02:05:56.002643: step 7280, loss 0.547945.
Train: 2018-08-09T02:05:59.052752: step 7281, loss 0.562896.
Train: 2018-08-09T02:06:02.147981: step 7282, loss 0.546973.
Train: 2018-08-09T02:06:05.231178: step 7283, loss 0.515058.
Train: 2018-08-09T02:06:08.308360: step 7284, loss 0.499042.
Train: 2018-08-09T02:06:11.397573: step 7285, loss 0.6109.
Train: 2018-08-09T02:06:14.505837: step 7286, loss 0.466789.
Train: 2018-08-09T02:06:17.617109: step 7287, loss 0.466472.
Train: 2018-08-09T02:06:20.715346: step 7288, loss 0.578877.
Train: 2018-08-09T02:06:23.854693: step 7289, loss 0.611273.
Train: 2018-08-09T02:06:26.943907: step 7290, loss 0.578955.
Test: 2018-08-09T02:06:47.942737: step 7290, loss 0.550512.
Train: 2018-08-09T02:06:51.070051: step 7291, loss 0.660123.
Train: 2018-08-09T02:06:54.226443: step 7292, loss 0.595146.
Train: 2018-08-09T02:06:57.338718: step 7293, loss 0.562689.
Train: 2018-08-09T02:07:00.460017: step 7294, loss 0.53023.
Train: 2018-08-09T02:07:03.562265: step 7295, loss 0.611349.
Train: 2018-08-09T02:07:06.671531: step 7296, loss 0.449212.
Train: 2018-08-09T02:07:09.778792: step 7297, loss 0.513963.
Train: 2018-08-09T02:07:12.856977: step 7298, loss 0.546379.
Train: 2018-08-09T02:07:15.926137: step 7299, loss 0.644158.
Train: 2018-08-09T02:07:19.013345: step 7300, loss 0.464741.
Test: 2018-08-09T02:07:40.040250: step 7300, loss 0.54851.
Train: 2018-08-09T02:07:44.897163: step 7301, loss 0.562593.
Train: 2018-08-09T02:07:48.006429: step 7302, loss 0.5298.
Train: 2018-08-09T02:07:51.058544: step 7303, loss 0.578967.
Train: 2018-08-09T02:07:54.115672: step 7304, loss 0.51317.
Train: 2018-08-09T02:07:57.242987: step 7305, loss 0.512986.
Train: 2018-08-09T02:08:00.349245: step 7306, loss 0.661801.
Train: 2018-08-09T02:08:03.449488: step 7307, loss 0.496315.
Train: 2018-08-09T02:08:06.547726: step 7308, loss 0.496054.
Train: 2018-08-09T02:08:09.639947: step 7309, loss 0.495864.
Train: 2018-08-09T02:08:12.721139: step 7310, loss 0.612514.
Test: 2018-08-09T02:08:33.708940: step 7310, loss 0.547305.
Train: 2018-08-09T02:08:36.820212: step 7311, loss 0.612662.
Train: 2018-08-09T02:08:39.935494: step 7312, loss 0.612939.
Train: 2018-08-09T02:08:43.024708: step 7313, loss 0.495317.
Train: 2018-08-09T02:08:46.080833: step 7314, loss 0.646173.
Train: 2018-08-09T02:08:49.165033: step 7315, loss 0.512148.
Train: 2018-08-09T02:08:52.270289: step 7316, loss 0.579118.
Train: 2018-08-09T02:08:55.396601: step 7317, loss 0.663107.
Train: 2018-08-09T02:08:58.480801: step 7318, loss 0.462228.
Train: 2018-08-09T02:09:01.557983: step 7319, loss 0.679407.
Train: 2018-08-09T02:09:04.670257: step 7320, loss 0.512447.
Test: 2018-08-09T02:09:25.688139: step 7320, loss 0.548104.
Train: 2018-08-09T02:09:28.793394: step 7321, loss 0.495834.
Train: 2018-08-09T02:09:31.883610: step 7322, loss 0.529167.
Train: 2018-08-09T02:09:34.968813: step 7323, loss 0.562521.
Train: 2018-08-09T02:09:38.122197: step 7324, loss 0.59559.
Train: 2018-08-09T02:09:41.200381: step 7325, loss 0.595832.
Train: 2018-08-09T02:09:44.290597: step 7326, loss 0.479139.
Train: 2018-08-09T02:09:47.392845: step 7327, loss 0.579366.
Train: 2018-08-09T02:09:50.492085: step 7328, loss 0.512383.
Train: 2018-08-09T02:09:53.596338: step 7329, loss 0.579011.
Train: 2018-08-09T02:09:56.697583: step 7330, loss 0.528784.
Test: 2018-08-09T02:10:17.701428: step 7330, loss 0.547987.
Train: 2018-08-09T02:10:20.774598: step 7331, loss 0.51231.
Train: 2018-08-09T02:10:23.855790: step 7332, loss 0.52921.
Train: 2018-08-09T02:10:26.920939: step 7333, loss 0.595886.
Train: 2018-08-09T02:10:30.031208: step 7334, loss 0.579349.
Train: 2018-08-09T02:10:33.125435: step 7335, loss 0.495297.
Train: 2018-08-09T02:10:36.199608: step 7336, loss 0.545529.
Train: 2018-08-09T02:10:39.304865: step 7337, loss 0.545427.
Train: 2018-08-09T02:10:42.382046: step 7338, loss 0.512001.
Train: 2018-08-09T02:10:45.509361: step 7339, loss 0.511996.
Train: 2018-08-09T02:10:48.620633: step 7340, loss 0.545182.
Test: 2018-08-09T02:11:09.637511: step 7340, loss 0.546384.
Train: 2018-08-09T02:11:12.737753: step 7341, loss 0.613099.
Train: 2018-08-09T02:11:15.864065: step 7342, loss 0.665292.
Train: 2018-08-09T02:11:18.953279: step 7343, loss 0.629629.
Train: 2018-08-09T02:11:22.025447: step 7344, loss 0.596028.
Train: 2018-08-09T02:11:25.127695: step 7345, loss 0.697241.
Train: 2018-08-09T02:11:28.224930: step 7346, loss 0.629638.
Train: 2018-08-09T02:11:31.299103: step 7347, loss 0.429101.
Train: 2018-08-09T02:11:34.360242: step 7348, loss 0.595703.
Train: 2018-08-09T02:11:37.453465: step 7349, loss 0.51267.
Train: 2018-08-09T02:11:40.543682: step 7350, loss 0.463077.
Test: 2018-08-09T02:12:01.538501: step 7350, loss 0.547557.
Train: 2018-08-09T02:12:04.674840: step 7351, loss 0.496193.
Train: 2018-08-09T02:12:07.744000: step 7352, loss 0.496056.
Train: 2018-08-09T02:12:10.850258: step 7353, loss 0.612339.
Train: 2018-08-09T02:12:13.963536: step 7354, loss 0.629215.
Train: 2018-08-09T02:12:17.059768: step 7355, loss 0.512573.
Train: 2018-08-09T02:12:20.121909: step 7356, loss 0.546092.
Train: 2018-08-09T02:12:23.193075: step 7357, loss 0.6457.
Train: 2018-08-09T02:12:26.277275: step 7358, loss 0.595654.
Train: 2018-08-09T02:12:29.362477: step 7359, loss 0.595648.
Train: 2018-08-09T02:12:32.476757: step 7360, loss 0.51289.
Test: 2018-08-09T02:12:53.493636: step 7360, loss 0.552653.
Train: 2018-08-09T02:12:56.563798: step 7361, loss 0.628475.
Train: 2018-08-09T02:12:59.661033: step 7362, loss 0.496744.
Train: 2018-08-09T02:13:02.725179: step 7363, loss 0.513115.
Train: 2018-08-09T02:13:05.829433: step 7364, loss 0.628358.
Train: 2018-08-09T02:13:08.935691: step 7365, loss 0.546149.
Train: 2018-08-09T02:13:12.034932: step 7366, loss 0.496825.
Train: 2018-08-09T02:13:15.103089: step 7367, loss 0.496852.
Train: 2018-08-09T02:13:18.592366: step 7368, loss 0.562488.
Train: 2018-08-09T02:13:21.673558: step 7369, loss 0.595451.
Train: 2018-08-09T02:13:24.768787: step 7370, loss 0.644976.
Test: 2018-08-09T02:13:45.786669: step 7370, loss 0.547692.
Train: 2018-08-09T02:13:48.868863: step 7371, loss 0.57901.
Train: 2018-08-09T02:13:51.974119: step 7372, loss 0.677746.
Train: 2018-08-09T02:13:55.059322: step 7373, loss 0.595345.
Train: 2018-08-09T02:13:58.137505: step 7374, loss 0.578926.
Train: 2018-08-09T02:14:01.242762: step 7375, loss 0.578997.
Train: 2018-08-09T02:14:04.360050: step 7376, loss 0.578868.
Train: 2018-08-09T02:14:07.496388: step 7377, loss 0.481915.
Train: 2018-08-09T02:14:10.643756: step 7378, loss 0.627299.
Train: 2018-08-09T02:14:13.736980: step 7379, loss 0.643412.
Train: 2018-08-09T02:14:16.816167: step 7380, loss 0.594919.
Test: 2018-08-09T02:14:37.839062: step 7380, loss 0.550923.
Train: 2018-08-09T02:14:40.947325: step 7381, loss 0.515133.
Train: 2018-08-09T02:14:44.035536: step 7382, loss 0.546902.
Train: 2018-08-09T02:14:47.105698: step 7383, loss 0.610826.
Train: 2018-08-09T02:14:50.207946: step 7384, loss 0.562944.
Train: 2018-08-09T02:14:53.298163: step 7385, loss 0.51528.
Train: 2018-08-09T02:14:56.391386: step 7386, loss 0.4677.
Train: 2018-08-09T02:14:59.489624: step 7387, loss 0.674244.
Train: 2018-08-09T02:15:02.564800: step 7388, loss 0.594751.
Train: 2018-08-09T02:15:05.647997: step 7389, loss 0.531279.
Train: 2018-08-09T02:15:08.765285: step 7390, loss 0.499615.
Test: 2018-08-09T02:15:29.761108: step 7390, loss 0.549328.
Train: 2018-08-09T02:15:32.843302: step 7391, loss 0.48367.
Train: 2018-08-09T02:15:35.953572: step 7392, loss 0.531144.
Train: 2018-08-09T02:15:39.054817: step 7393, loss 0.451194.
Train: 2018-08-09T02:15:42.154057: step 7394, loss 0.594827.
Train: 2018-08-09T02:15:45.221211: step 7395, loss 0.4983.
Train: 2018-08-09T02:15:48.288366: step 7396, loss 0.432583.
Train: 2018-08-09T02:15:51.380588: step 7397, loss 0.447355.
Train: 2018-08-09T02:15:54.486846: step 7398, loss 0.54726.
Train: 2018-08-09T02:15:57.579068: step 7399, loss 0.613255.
Train: 2018-08-09T02:16:00.694350: step 7400, loss 0.579358.
Test: 2018-08-09T02:16:21.691175: step 7400, loss 0.548673.
Train: 2018-08-09T02:16:26.552099: step 7401, loss 0.510461.
Train: 2018-08-09T02:16:29.634293: step 7402, loss 0.613148.
Train: 2018-08-09T02:16:32.715485: step 7403, loss 0.527135.
Train: 2018-08-09T02:16:35.813723: step 7404, loss 0.561288.
Train: 2018-08-09T02:16:38.922989: step 7405, loss 0.601577.
Train: 2018-08-09T02:16:41.989142: step 7406, loss 0.564719.
Train: 2018-08-09T02:16:45.071336: step 7407, loss 0.544138.
Train: 2018-08-09T02:16:48.141499: step 7408, loss 0.563126.
Train: 2018-08-09T02:16:51.237731: step 7409, loss 0.561897.
Train: 2018-08-09T02:16:54.319926: step 7410, loss 0.665527.
Test: 2018-08-09T02:17:15.285668: step 7410, loss 0.547037.
Train: 2018-08-09T02:17:18.410977: step 7411, loss 0.596485.
Train: 2018-08-09T02:17:21.548319: step 7412, loss 0.562946.
Train: 2018-08-09T02:17:24.616476: step 7413, loss 0.528598.
Train: 2018-08-09T02:17:27.715716: step 7414, loss 0.545588.
Train: 2018-08-09T02:17:30.805932: step 7415, loss 0.512149.
Train: 2018-08-09T02:17:33.904169: step 7416, loss 0.528833.
Train: 2018-08-09T02:17:37.053543: step 7417, loss 0.714087.
Train: 2018-08-09T02:17:40.130724: step 7418, loss 0.562153.
Train: 2018-08-09T02:17:43.190860: step 7419, loss 0.478755.
Train: 2018-08-09T02:17:46.293108: step 7420, loss 0.495694.
Test: 2018-08-09T02:18:07.301965: step 7420, loss 0.549924.
Train: 2018-08-09T02:18:10.385162: step 7421, loss 0.695903.
Train: 2018-08-09T02:18:13.441288: step 7422, loss 0.445891.
Train: 2018-08-09T02:18:16.552560: step 7423, loss 0.562863.
Train: 2018-08-09T02:18:19.625730: step 7424, loss 0.495105.
Train: 2018-08-09T02:18:22.700906: step 7425, loss 0.613162.
Train: 2018-08-09T02:18:25.771069: step 7426, loss 0.562467.
Train: 2018-08-09T02:18:28.870309: step 7427, loss 0.561959.
Train: 2018-08-09T02:18:31.938467: step 7428, loss 0.612528.
Train: 2018-08-09T02:18:35.043722: step 7429, loss 0.562406.
Train: 2018-08-09T02:18:38.195101: step 7430, loss 0.562667.
Test: 2018-08-09T02:18:59.192929: step 7430, loss 0.550676.
Train: 2018-08-09T02:19:02.276126: step 7431, loss 0.562717.
Train: 2018-08-09T02:19:05.365339: step 7432, loss 0.513353.
Train: 2018-08-09T02:19:08.483630: step 7433, loss 0.528747.
Train: 2018-08-09T02:19:11.614956: step 7434, loss 0.478973.
Train: 2018-08-09T02:19:14.726227: step 7435, loss 0.463488.
Train: 2018-08-09T02:19:17.840507: step 7436, loss 0.579678.
Train: 2018-08-09T02:19:20.925710: step 7437, loss 0.646126.
Train: 2018-08-09T02:19:23.996875: step 7438, loss 0.595517.
Train: 2018-08-09T02:19:27.065033: step 7439, loss 0.562663.
Train: 2018-08-09T02:19:30.152241: step 7440, loss 0.546292.
Test: 2018-08-09T02:19:51.150069: step 7440, loss 0.547293.
Train: 2018-08-09T02:19:54.250311: step 7441, loss 0.578451.
Train: 2018-08-09T02:19:57.382639: step 7442, loss 0.562234.
Train: 2018-08-09T02:20:00.472855: step 7443, loss 0.578652.
Train: 2018-08-09T02:20:03.552042: step 7444, loss 0.512911.
Train: 2018-08-09T02:20:06.652284: step 7445, loss 0.494777.
Train: 2018-08-09T02:20:09.772580: step 7446, loss 0.612929.
Train: 2018-08-09T02:20:12.887863: step 7447, loss 0.547212.
Train: 2018-08-09T02:20:16.009162: step 7448, loss 0.544728.
Train: 2018-08-09T02:20:19.086343: step 7449, loss 0.580088.
Train: 2018-08-09T02:20:22.184580: step 7450, loss 0.645445.
Test: 2018-08-09T02:20:43.181405: step 7450, loss 0.548661.
Train: 2018-08-09T02:20:46.282650: step 7451, loss 0.512369.
Train: 2018-08-09T02:20:49.364845: step 7452, loss 0.629489.
Train: 2018-08-09T02:20:52.476117: step 7453, loss 0.496261.
Train: 2018-08-09T02:20:55.565330: step 7454, loss 0.545771.
Train: 2018-08-09T02:20:58.689637: step 7455, loss 0.562307.
Train: 2018-08-09T02:21:01.799907: step 7456, loss 0.695024.
Train: 2018-08-09T02:21:04.885109: step 7457, loss 0.595706.
Train: 2018-08-09T02:21:07.995379: step 7458, loss 0.447353.
Train: 2018-08-09T02:21:11.085595: step 7459, loss 0.595675.
Train: 2018-08-09T02:21:14.203885: step 7460, loss 0.579231.
Test: 2018-08-09T02:21:35.223772: step 7460, loss 0.547216.
Train: 2018-08-09T02:21:38.313987: step 7461, loss 0.546211.
Train: 2018-08-09T02:21:41.372118: step 7462, loss 0.61168.
Train: 2018-08-09T02:21:44.496425: step 7463, loss 0.676923.
Train: 2018-08-09T02:21:47.617724: step 7464, loss 0.51381.
Train: 2018-08-09T02:21:50.695908: step 7465, loss 0.481469.
Train: 2018-08-09T02:21:53.800161: step 7466, loss 0.481512.
Train: 2018-08-09T02:21:56.903412: step 7467, loss 0.530158.
Train: 2018-08-09T02:22:00.022705: step 7468, loss 0.497561.
Train: 2018-08-09T02:22:03.129966: step 7469, loss 0.644123.
Train: 2018-08-09T02:22:06.190102: step 7470, loss 0.546071.
Test: 2018-08-09T02:22:27.205978: step 7470, loss 0.550332.
Train: 2018-08-09T02:22:30.335298: step 7471, loss 0.595662.
Train: 2018-08-09T02:22:33.453588: step 7472, loss 0.513529.
Train: 2018-08-09T02:22:36.559847: step 7473, loss 0.628102.
Train: 2018-08-09T02:22:39.679140: step 7474, loss 0.644507.
Train: 2018-08-09T02:22:42.779383: step 7475, loss 0.530103.
Train: 2018-08-09T02:22:45.896671: step 7476, loss 0.59533.
Train: 2018-08-09T02:22:48.975858: step 7477, loss 0.627722.
Train: 2018-08-09T02:22:52.071087: step 7478, loss 0.578896.
Train: 2018-08-09T02:22:55.171330: step 7479, loss 0.595061.
Train: 2018-08-09T02:22:58.286613: step 7480, loss 0.530486.
Test: 2018-08-09T02:23:19.294467: step 7480, loss 0.548897.
Train: 2018-08-09T02:23:22.416768: step 7481, loss 0.594995.
Train: 2018-08-09T02:23:25.536061: step 7482, loss 0.659272.
Train: 2018-08-09T02:23:28.633296: step 7483, loss 0.498762.
Train: 2018-08-09T02:23:31.743565: step 7484, loss 0.562904.
Train: 2018-08-09T02:23:34.850827: step 7485, loss 0.531021.
Train: 2018-08-09T02:23:37.964104: step 7486, loss 0.578872.
Train: 2018-08-09T02:23:41.052315: step 7487, loss 0.531014.
Train: 2018-08-09T02:23:44.159576: step 7488, loss 0.658572.
Train: 2018-08-09T02:23:47.264832: step 7489, loss 0.483447.
Train: 2018-08-09T02:23:50.346024: step 7490, loss 0.578851.
Test: 2018-08-09T02:24:11.401004: step 7490, loss 0.548665.
Train: 2018-08-09T02:24:14.494228: step 7491, loss 0.515263.
Train: 2018-08-09T02:24:17.652625: step 7492, loss 0.57886.
Train: 2018-08-09T02:24:20.757881: step 7493, loss 0.547021.
Train: 2018-08-09T02:24:23.828044: step 7494, loss 0.451428.
Train: 2018-08-09T02:24:26.933299: step 7495, loss 0.514962.
Train: 2018-08-09T02:24:30.031537: step 7496, loss 0.562836.
Train: 2018-08-09T02:24:33.122756: step 7497, loss 0.482341.
Train: 2018-08-09T02:24:36.224001: step 7498, loss 0.611097.
Train: 2018-08-09T02:24:39.296169: step 7499, loss 0.595177.
Train: 2018-08-09T02:24:42.400423: step 7500, loss 0.497394.
Test: 2018-08-09T02:25:03.388224: step 7500, loss 0.550358.
Train: 2018-08-09T02:25:08.229094: step 7501, loss 0.611306.
Train: 2018-08-09T02:25:11.323320: step 7502, loss 0.693973.
Train: 2018-08-09T02:25:14.395489: step 7503, loss 0.595699.
Train: 2018-08-09T02:25:17.471667: step 7504, loss 0.611989.
Train: 2018-08-09T02:25:20.533808: step 7505, loss 0.562579.
Train: 2018-08-09T02:25:23.631043: step 7506, loss 0.530214.
Train: 2018-08-09T02:25:26.707222: step 7507, loss 0.627522.
Train: 2018-08-09T02:25:29.755326: step 7508, loss 0.562714.
Train: 2018-08-09T02:25:32.849553: step 7509, loss 0.627335.
Train: 2018-08-09T02:25:35.927737: step 7510, loss 0.627204.
Test: 2018-08-09T02:25:56.923559: step 7510, loss 0.548985.
Train: 2018-08-09T02:26:00.004751: step 7511, loss 0.578857.
Train: 2018-08-09T02:26:03.094967: step 7512, loss 0.578861.
Train: 2018-08-09T02:26:06.188191: step 7513, loss 0.451192.
Train: 2018-08-09T02:26:09.281415: step 7514, loss 0.546907.
Train: 2018-08-09T02:26:12.358596: step 7515, loss 0.54704.
Train: 2018-08-09T02:26:15.452823: step 7516, loss 0.610942.
Train: 2018-08-09T02:26:18.544042: step 7517, loss 0.498906.
Train: 2018-08-09T02:26:21.646290: step 7518, loss 0.674738.
Train: 2018-08-09T02:26:24.766586: step 7519, loss 0.658659.
Train: 2018-08-09T02:26:27.885879: step 7520, loss 0.594724.
Test: 2018-08-09T02:26:48.889723: step 7520, loss 0.549415.
Train: 2018-08-09T02:26:51.960888: step 7521, loss 0.610473.
Train: 2018-08-09T02:26:55.050101: step 7522, loss 0.500099.
Train: 2018-08-09T02:26:58.152349: step 7523, loss 0.594587.
Train: 2018-08-09T02:27:01.254597: step 7524, loss 0.53181.
Train: 2018-08-09T02:27:04.157315: step 7525, loss 0.479612.
Train: 2018-08-09T02:27:07.253547: step 7526, loss 0.547503.
Train: 2018-08-09T02:27:10.358803: step 7527, loss 0.594572.
Train: 2018-08-09T02:27:13.447013: step 7528, loss 0.531747.
Train: 2018-08-09T02:27:16.552270: step 7529, loss 0.578874.
Train: 2018-08-09T02:27:19.647499: step 7530, loss 0.547391.
Test: 2018-08-09T02:27:40.634298: step 7530, loss 0.551333.
Train: 2018-08-09T02:27:43.733537: step 7531, loss 0.563136.
Train: 2018-08-09T02:27:46.829769: step 7532, loss 0.547327.
Train: 2018-08-09T02:27:49.924998: step 7533, loss 0.642041.
Train: 2018-08-09T02:27:53.026244: step 7534, loss 0.57888.
Train: 2018-08-09T02:27:56.139521: step 7535, loss 0.642037.
Train: 2018-08-09T02:27:59.219711: step 7536, loss 0.389616.
Train: 2018-08-09T02:28:02.315943: step 7537, loss 0.515637.
Train: 2018-08-09T02:28:05.410169: step 7538, loss 0.594709.
Train: 2018-08-09T02:28:08.485345: step 7539, loss 0.610631.
Train: 2018-08-09T02:28:11.554505: step 7540, loss 0.594751.
Test: 2018-08-09T02:28:32.556343: step 7540, loss 0.551054.
Train: 2018-08-09T02:28:35.661599: step 7541, loss 0.578853.
Train: 2018-08-09T02:28:38.797938: step 7542, loss 0.658418.
Train: 2018-08-09T02:28:41.889157: step 7543, loss 0.594746.
Train: 2018-08-09T02:28:44.968343: step 7544, loss 0.578878.
Train: 2018-08-09T02:28:48.053546: step 7545, loss 0.54723.
Train: 2018-08-09T02:28:51.137746: step 7546, loss 0.594655.
Train: 2018-08-09T02:28:54.257039: step 7547, loss 0.642117.
Train: 2018-08-09T02:28:57.330210: step 7548, loss 0.578871.
Train: 2018-08-09T02:29:00.437472: step 7549, loss 0.500326.
Train: 2018-08-09T02:29:03.561778: step 7550, loss 0.657353.
Test: 2018-08-09T02:29:24.561611: step 7550, loss 0.550923.
Train: 2018-08-09T02:29:27.664862: step 7551, loss 0.516255.
Train: 2018-08-09T02:29:30.771120: step 7552, loss 0.516342.
Train: 2018-08-09T02:29:33.865347: step 7553, loss 0.4694.
Train: 2018-08-09T02:29:36.967595: step 7554, loss 0.547573.
Train: 2018-08-09T02:29:40.038760: step 7555, loss 0.531767.
Train: 2018-08-09T02:29:43.131984: step 7556, loss 0.563115.
Train: 2018-08-09T02:29:46.204153: step 7557, loss 0.610461.
Train: 2018-08-09T02:29:49.291361: step 7558, loss 0.563072.
Train: 2018-08-09T02:29:52.381577: step 7559, loss 0.547224.
Train: 2018-08-09T02:29:55.483825: step 7560, loss 0.563023.
Test: 2018-08-09T02:30:16.477642: step 7560, loss 0.54811.
Train: 2018-08-09T02:30:19.634033: step 7561, loss 0.578859.
Train: 2018-08-09T02:30:22.724250: step 7562, loss 0.578857.
Train: 2018-08-09T02:30:25.820481: step 7563, loss 0.578855.
Train: 2018-08-09T02:30:28.933759: step 7564, loss 0.547015.
Train: 2018-08-09T02:30:32.014951: step 7565, loss 0.546999.
Train: 2018-08-09T02:30:35.116196: step 7566, loss 0.594805.
Train: 2018-08-09T02:30:38.210423: step 7567, loss 0.626758.
Train: 2018-08-09T02:30:41.327711: step 7568, loss 0.483144.
Train: 2018-08-09T02:30:44.411911: step 7569, loss 0.498976.
Train: 2018-08-09T02:30:47.537220: step 7570, loss 0.530835.
Test: 2018-08-09T02:31:08.515997: step 7570, loss 0.548378.
Train: 2018-08-09T02:31:11.622256: step 7571, loss 0.643103.
Train: 2018-08-09T02:31:14.732525: step 7572, loss 0.450312.
Train: 2018-08-09T02:31:17.835775: step 7573, loss 0.514402.
Train: 2018-08-09T02:31:20.918973: step 7574, loss 0.498011.
Train: 2018-08-09T02:31:24.007184: step 7575, loss 0.513907.
Train: 2018-08-09T02:31:27.099405: step 7576, loss 0.562587.
Train: 2018-08-09T02:31:30.215690: step 7577, loss 0.529752.
Train: 2018-08-09T02:31:33.282845: step 7578, loss 0.595503.
Train: 2018-08-09T02:31:36.378074: step 7579, loss 0.595669.
Train: 2018-08-09T02:31:39.468290: step 7580, loss 0.56251.
Test: 2018-08-09T02:32:00.460102: step 7580, loss 0.545624.
Train: 2018-08-09T02:32:03.553326: step 7581, loss 0.51283.
Train: 2018-08-09T02:32:06.664598: step 7582, loss 0.479212.
Train: 2018-08-09T02:32:09.735763: step 7583, loss 0.428844.
Train: 2018-08-09T02:32:12.816955: step 7584, loss 0.562364.
Train: 2018-08-09T02:32:15.911182: step 7585, loss 0.528552.
Train: 2018-08-09T02:32:18.987361: step 7586, loss 0.59633.
Train: 2018-08-09T02:32:22.085598: step 7587, loss 0.630895.
Train: 2018-08-09T02:32:25.157766: step 7588, loss 0.511179.
Train: 2018-08-09T02:32:28.253998: step 7589, loss 0.699445.
Train: 2018-08-09T02:32:31.346220: step 7590, loss 0.494194.
Test: 2018-08-09T02:32:52.355077: step 7590, loss 0.547611.
Train: 2018-08-09T02:32:55.495426: step 7591, loss 0.562253.
Train: 2018-08-09T02:32:58.601684: step 7592, loss 0.545167.
Train: 2018-08-09T02:33:01.676860: step 7593, loss 0.42547.
Train: 2018-08-09T02:33:04.790138: step 7594, loss 0.528128.
Train: 2018-08-09T02:33:07.864311: step 7595, loss 0.613892.
Train: 2018-08-09T02:33:10.928458: step 7596, loss 0.614243.
Train: 2018-08-09T02:33:14.017671: step 7597, loss 0.614312.
Train: 2018-08-09T02:33:17.119919: step 7598, loss 0.596545.
Train: 2018-08-09T02:33:20.200109: step 7599, loss 0.630771.
Train: 2018-08-09T02:33:23.292330: step 7600, loss 0.665083.
Test: 2018-08-09T02:33:44.374382: step 7600, loss 0.548969.
Train: 2018-08-09T02:33:49.303486: step 7601, loss 0.562365.
Train: 2018-08-09T02:33:52.374652: step 7602, loss 0.511534.
Train: 2018-08-09T02:33:55.476900: step 7603, loss 0.56236.
Train: 2018-08-09T02:33:58.609228: step 7604, loss 0.545482.
Train: 2018-08-09T02:34:01.697439: step 7605, loss 0.545572.
Train: 2018-08-09T02:34:04.781639: step 7606, loss 0.562446.
Train: 2018-08-09T02:34:07.873860: step 7607, loss 0.495619.
Train: 2018-08-09T02:34:10.960065: step 7608, loss 0.545668.
Train: 2018-08-09T02:34:14.064318: step 7609, loss 0.579266.
Train: 2018-08-09T02:34:17.154535: step 7610, loss 0.595809.
Test: 2018-08-09T02:34:38.130303: step 7610, loss 0.546782.
Train: 2018-08-09T02:34:41.218514: step 7611, loss 0.529018.
Train: 2018-08-09T02:34:44.310736: step 7612, loss 0.429008.
Train: 2018-08-09T02:34:47.406968: step 7613, loss 0.579116.
Train: 2018-08-09T02:34:50.504202: step 7614, loss 0.512239.
Train: 2018-08-09T02:34:53.621490: step 7615, loss 0.562389.
Train: 2018-08-09T02:34:56.681626: step 7616, loss 0.545625.
Train: 2018-08-09T02:34:59.794904: step 7617, loss 0.595984.
Train: 2018-08-09T02:35:02.885120: step 7618, loss 0.579185.
Train: 2018-08-09T02:35:05.977341: step 7619, loss 0.411179.
Train: 2018-08-09T02:35:09.043493: step 7620, loss 0.579216.
Test: 2018-08-09T02:35:30.046334: step 7620, loss 0.547193.
Train: 2018-08-09T02:35:33.179664: step 7621, loss 0.545495.
Train: 2018-08-09T02:35:36.279907: step 7622, loss 0.613064.
Train: 2018-08-09T02:35:39.400203: step 7623, loss 0.613111.
Train: 2018-08-09T02:35:42.505459: step 7624, loss 0.528534.
Train: 2018-08-09T02:35:45.601691: step 7625, loss 0.629937.
Train: 2018-08-09T02:35:48.651800: step 7626, loss 0.613117.
Train: 2018-08-09T02:35:51.745025: step 7627, loss 0.612906.
Train: 2018-08-09T02:35:54.834238: step 7628, loss 0.730249.
Train: 2018-08-09T02:35:57.942502: step 7629, loss 0.579685.
Train: 2018-08-09T02:36:01.047758: step 7630, loss 0.628493.
Test: 2018-08-09T02:36:22.067644: step 7630, loss 0.548998.
Train: 2018-08-09T02:36:25.162873: step 7631, loss 0.562744.
Train: 2018-08-09T02:36:28.273143: step 7632, loss 0.595441.
Train: 2018-08-09T02:36:31.418505: step 7633, loss 0.51433.
Train: 2018-08-09T02:36:34.528775: step 7634, loss 0.562679.
Train: 2018-08-09T02:36:37.651076: step 7635, loss 0.513908.
Train: 2018-08-09T02:36:40.720236: step 7636, loss 0.611311.
Train: 2018-08-09T02:36:43.803433: step 7637, loss 0.5465.
Train: 2018-08-09T02:36:46.899665: step 7638, loss 0.627355.
Train: 2018-08-09T02:36:49.985871: step 7639, loss 0.578926.
Train: 2018-08-09T02:36:53.072076: step 7640, loss 0.563028.
Test: 2018-08-09T02:37:14.070907: step 7640, loss 0.549697.
Train: 2018-08-09T02:37:17.146082: step 7641, loss 0.626876.
Train: 2018-08-09T02:37:20.672458: step 7642, loss 0.547009.
Train: 2018-08-09T02:37:23.783730: step 7643, loss 0.515325.
Train: 2018-08-09T02:37:26.881967: step 7644, loss 0.610626.
Train: 2018-08-09T02:37:29.969176: step 7645, loss 0.499621.
Train: 2018-08-09T02:37:33.050367: step 7646, loss 0.515531.
Train: 2018-08-09T02:37:36.138578: step 7647, loss 0.578855.
Train: 2018-08-09T02:37:39.223781: step 7648, loss 0.578852.
Train: 2018-08-09T02:37:42.308984: step 7649, loss 0.515471.
Train: 2018-08-09T02:37:45.402207: step 7650, loss 0.658154.
Test: 2018-08-09T02:38:06.417081: step 7650, loss 0.550548.
Train: 2018-08-09T02:38:09.531360: step 7651, loss 0.547123.
Train: 2018-08-09T02:38:12.596510: step 7652, loss 0.547087.
Train: 2018-08-09T02:38:15.671686: step 7653, loss 0.51525.
Train: 2018-08-09T02:38:18.758894: step 7654, loss 0.562852.
Train: 2018-08-09T02:38:21.864150: step 7655, loss 0.530836.
Train: 2018-08-09T02:38:24.951358: step 7656, loss 0.595685.
Train: 2018-08-09T02:38:28.040571: step 7657, loss 0.513832.
Train: 2018-08-09T02:38:31.135800: step 7658, loss 0.594738.
Train: 2018-08-09T02:38:34.198945: step 7659, loss 0.593696.
Train: 2018-08-09T02:38:37.294174: step 7660, loss 0.579133.
Test: 2018-08-09T02:38:58.308044: step 7660, loss 0.549439.
Train: 2018-08-09T02:39:01.423327: step 7661, loss 0.49737.
Train: 2018-08-09T02:39:04.512540: step 7662, loss 0.597346.
Train: 2018-08-09T02:39:07.600751: step 7663, loss 0.548734.
Train: 2018-08-09T02:39:10.681943: step 7664, loss 0.628474.
Train: 2018-08-09T02:39:13.790207: step 7665, loss 0.611824.
Train: 2018-08-09T02:39:16.894460: step 7666, loss 0.613238.
Train: 2018-08-09T02:39:19.977658: step 7667, loss 0.562946.
Train: 2018-08-09T02:39:23.110988: step 7668, loss 0.595582.
Train: 2018-08-09T02:39:26.166111: step 7669, loss 0.53116.
Train: 2018-08-09T02:39:29.251313: step 7670, loss 0.51561.
Test: 2018-08-09T02:39:50.229088: step 7670, loss 0.547639.
Train: 2018-08-09T02:39:53.301256: step 7671, loss 0.57887.
Train: 2018-08-09T02:39:56.393477: step 7672, loss 0.59468.
Train: 2018-08-09T02:39:59.460632: step 7673, loss 0.484121.
Train: 2018-08-09T02:40:02.575914: step 7674, loss 0.578896.
Train: 2018-08-09T02:40:05.662120: step 7675, loss 0.610509.
Train: 2018-08-09T02:40:08.729275: step 7676, loss 0.483999.
Train: 2018-08-09T02:40:11.802445: step 7677, loss 0.610504.
Train: 2018-08-09T02:40:14.885643: step 7678, loss 0.594737.
Train: 2018-08-09T02:40:17.991901: step 7679, loss 0.658026.
Train: 2018-08-09T02:40:21.053040: step 7680, loss 0.578858.
Test: 2018-08-09T02:40:42.066911: step 7680, loss 0.550693.
Train: 2018-08-09T02:40:45.194225: step 7681, loss 0.484212.
Train: 2018-08-09T02:40:48.288451: step 7682, loss 0.452638.
Train: 2018-08-09T02:40:51.421782: step 7683, loss 0.531421.
Train: 2018-08-09T02:40:54.533054: step 7684, loss 0.642283.
Train: 2018-08-09T02:40:57.647334: step 7685, loss 0.594732.
Train: 2018-08-09T02:41:00.713486: step 7686, loss 0.547099.
Train: 2018-08-09T02:41:03.821750: step 7687, loss 0.594752.
Train: 2018-08-09T02:41:06.916980: step 7688, loss 0.64245.
Train: 2018-08-09T02:41:10.016219: step 7689, loss 0.51534.
Train: 2018-08-09T02:41:13.129497: step 7690, loss 0.547094.
Test: 2018-08-09T02:41:34.112285: step 7690, loss 0.54989.
Train: 2018-08-09T02:41:37.184452: step 7691, loss 0.483531.
Train: 2018-08-09T02:41:40.289708: step 7692, loss 0.483371.
Train: 2018-08-09T02:41:43.398975: step 7693, loss 0.562891.
Train: 2018-08-09T02:41:46.478162: step 7694, loss 0.594941.
Train: 2018-08-09T02:41:49.583418: step 7695, loss 0.530627.
Train: 2018-08-09T02:41:52.685666: step 7696, loss 0.595003.
Train: 2018-08-09T02:41:55.793930: step 7697, loss 0.594922.
Train: 2018-08-09T02:41:58.856071: step 7698, loss 0.514292.
Train: 2018-08-09T02:42:01.934255: step 7699, loss 0.676169.
Train: 2018-08-09T02:42:04.981357: step 7700, loss 0.643573.
Test: 2018-08-09T02:42:25.961137: step 7700, loss 0.548897.
Train: 2018-08-09T02:42:30.805015: step 7701, loss 0.482244.
Train: 2018-08-09T02:42:33.931327: step 7702, loss 0.578731.
Train: 2018-08-09T02:42:37.046609: step 7703, loss 0.530553.
Train: 2018-08-09T02:42:40.164900: step 7704, loss 0.578979.
Train: 2018-08-09T02:42:43.203980: step 7705, loss 0.546579.
Train: 2018-08-09T02:42:46.281161: step 7706, loss 0.562832.
Train: 2018-08-09T02:42:49.396444: step 7707, loss 0.530469.
Train: 2018-08-09T02:42:52.483652: step 7708, loss 0.546602.
Train: 2018-08-09T02:42:55.551810: step 7709, loss 0.4817.
Train: 2018-08-09T02:42:58.640020: step 7710, loss 0.562554.
Test: 2018-08-09T02:43:19.611779: step 7710, loss 0.54977.
Train: 2018-08-09T02:43:22.731072: step 7711, loss 0.579003.
Train: 2018-08-09T02:43:25.839336: step 7712, loss 0.562897.
Train: 2018-08-09T02:43:28.925541: step 7713, loss 0.628045.
Train: 2018-08-09T02:43:32.001720: step 7714, loss 0.529965.
Train: 2018-08-09T02:43:35.106976: step 7715, loss 0.497.
Train: 2018-08-09T02:43:38.193181: step 7716, loss 0.562224.
Train: 2018-08-09T02:43:41.409733: step 7717, loss 0.595419.
Train: 2018-08-09T02:43:44.492931: step 7718, loss 0.496553.
Train: 2018-08-09T02:43:47.586154: step 7719, loss 0.496294.
Train: 2018-08-09T02:43:50.695421: step 7720, loss 0.679916.
Test: 2018-08-09T02:44:11.713302: step 7720, loss 0.549332.
Train: 2018-08-09T02:44:14.817555: step 7721, loss 0.562176.
Train: 2018-08-09T02:44:17.928827: step 7722, loss 0.412451.
Train: 2018-08-09T02:44:21.019043: step 7723, loss 0.629416.
Train: 2018-08-09T02:44:24.096225: step 7724, loss 0.595777.
Train: 2018-08-09T02:44:27.180425: step 7725, loss 0.545763.
Train: 2018-08-09T02:44:30.256604: step 7726, loss 0.511739.
Train: 2018-08-09T02:44:33.333785: step 7727, loss 0.478033.
Train: 2018-08-09T02:44:36.437036: step 7728, loss 0.57987.
Train: 2018-08-09T02:44:39.519230: step 7729, loss 0.529788.
Train: 2018-08-09T02:44:42.596412: step 7730, loss 0.64779.
Test: 2018-08-09T02:45:03.602261: step 7730, loss 0.547007.
Train: 2018-08-09T02:45:06.682450: step 7731, loss 0.596633.
Train: 2018-08-09T02:45:09.779684: step 7732, loss 0.511574.
Train: 2018-08-09T02:45:12.892962: step 7733, loss 0.563379.
Train: 2018-08-09T02:45:15.967135: step 7734, loss 0.613502.
Train: 2018-08-09T02:45:19.022258: step 7735, loss 0.494779.
Train: 2018-08-09T02:45:22.115482: step 7736, loss 0.512351.
Train: 2018-08-09T02:45:25.204695: step 7737, loss 0.411809.
Train: 2018-08-09T02:45:28.285887: step 7738, loss 0.494248.
Train: 2018-08-09T02:45:31.352039: step 7739, loss 0.4096.
Train: 2018-08-09T02:45:34.458298: step 7740, loss 0.493782.
Test: 2018-08-09T02:45:55.458131: step 7740, loss 0.546758.
Train: 2018-08-09T02:45:58.570406: step 7741, loss 0.527924.
Train: 2018-08-09T02:46:01.720781: step 7742, loss 0.562553.
Train: 2018-08-09T02:46:04.799968: step 7743, loss 0.581825.
Train: 2018-08-09T02:46:07.914248: step 7744, loss 0.434819.
Train: 2018-08-09T02:46:10.985413: step 7745, loss 0.508127.
Train: 2018-08-09T02:46:14.085656: step 7746, loss 0.692665.
Train: 2018-08-09T02:46:17.188907: step 7747, loss 0.487076.
Train: 2018-08-09T02:46:20.278120: step 7748, loss 0.625462.
Train: 2018-08-09T02:46:23.382374: step 7749, loss 0.420219.
Train: 2018-08-09T02:46:26.502670: step 7750, loss 0.528538.
Test: 2018-08-09T02:46:47.573692: step 7750, loss 0.547063.
Train: 2018-08-09T02:46:50.670926: step 7751, loss 0.616938.
Train: 2018-08-09T02:46:53.789217: step 7752, loss 0.56238.
Train: 2018-08-09T02:46:56.895476: step 7753, loss 0.488749.
Train: 2018-08-09T02:46:59.996721: step 7754, loss 0.527257.
Train: 2018-08-09T02:47:03.089945: step 7755, loss 0.470505.
Train: 2018-08-09T02:47:06.183169: step 7756, loss 0.526333.
Train: 2018-08-09T02:47:09.286420: step 7757, loss 0.453053.
Train: 2018-08-09T02:47:12.378641: step 7758, loss 0.637692.
Train: 2018-08-09T02:47:15.422734: step 7759, loss 0.692142.
Train: 2018-08-09T02:47:18.515958: step 7760, loss 0.508556.
Test: 2018-08-09T02:47:39.525819: step 7760, loss 0.547752.
Train: 2018-08-09T02:47:42.616034: step 7761, loss 0.507799.
Train: 2018-08-09T02:47:45.705247: step 7762, loss 0.561551.
Train: 2018-08-09T02:47:48.791453: step 7763, loss 0.491499.
Train: 2018-08-09T02:47:51.903727: step 7764, loss 0.56173.
Train: 2018-08-09T02:47:55.001965: step 7765, loss 0.528274.
Train: 2018-08-09T02:47:58.147327: step 7766, loss 0.471947.
Train: 2018-08-09T02:48:01.239549: step 7767, loss 0.543625.
Train: 2018-08-09T02:48:04.391930: step 7768, loss 0.434778.
Train: 2018-08-09T02:48:07.485154: step 7769, loss 0.652618.
Train: 2018-08-09T02:48:10.586399: step 7770, loss 0.599324.
Test: 2018-08-09T02:48:31.574201: step 7770, loss 0.548452.
Train: 2018-08-09T02:48:34.860939: step 7771, loss 0.67167.
Train: 2018-08-09T02:48:37.957171: step 7772, loss 0.563756.
Train: 2018-08-09T02:48:41.124592: step 7773, loss 0.527285.
Train: 2018-08-09T02:48:44.225837: step 7774, loss 0.616922.
Train: 2018-08-09T02:48:47.358165: step 7775, loss 0.545239.
Train: 2018-08-09T02:48:50.457405: step 7776, loss 0.650197.
Train: 2018-08-09T02:48:53.565669: step 7777, loss 0.736527.
Train: 2018-08-09T02:48:56.638840: step 7778, loss 0.579205.
Train: 2018-08-09T02:48:59.733067: step 7779, loss 0.595707.
Train: 2018-08-09T02:49:02.841331: step 7780, loss 0.563006.
Test: 2018-08-09T02:49:23.856204: step 7780, loss 0.549816.
Train: 2018-08-09T02:49:26.968478: step 7781, loss 0.561694.
Train: 2018-08-09T02:49:30.076742: step 7782, loss 0.629037.
Train: 2018-08-09T02:49:33.179993: step 7783, loss 0.413936.
Train: 2018-08-09T02:49:36.269206: step 7784, loss 0.578941.
Train: 2018-08-09T02:49:39.352404: step 7785, loss 0.613008.
Train: 2018-08-09T02:49:42.449638: step 7786, loss 0.562348.
Train: 2018-08-09T02:49:45.536846: step 7787, loss 0.595683.
Train: 2018-08-09T02:49:48.637089: step 7788, loss 0.513368.
Train: 2018-08-09T02:49:51.761396: step 7789, loss 0.51353.
Train: 2018-08-09T02:49:54.842588: step 7790, loss 0.62799.
Test: 2018-08-09T02:50:15.850442: step 7790, loss 0.551032.
Train: 2018-08-09T02:50:18.967730: step 7791, loss 0.578944.
Train: 2018-08-09T02:50:22.086021: step 7792, loss 0.562641.
Train: 2018-08-09T02:50:25.170220: step 7793, loss 0.643827.
Train: 2018-08-09T02:50:28.259434: step 7794, loss 0.530367.
Train: 2018-08-09T02:50:31.334610: step 7795, loss 0.611147.
Train: 2018-08-09T02:50:34.442874: step 7796, loss 0.594987.
Train: 2018-08-09T02:50:37.518050: step 7797, loss 0.546808.
Train: 2018-08-09T02:50:40.608266: step 7798, loss 0.594867.
Train: 2018-08-09T02:50:43.744605: step 7799, loss 0.626718.
Train: 2018-08-09T02:50:46.793712: step 7800, loss 0.515283.
Test: 2018-08-09T02:51:07.784520: step 7800, loss 0.550537.
Train: 2018-08-09T02:51:12.663492: step 7801, loss 0.594724.
Train: 2018-08-09T02:51:15.739671: step 7802, loss 0.563034.
Train: 2018-08-09T02:51:18.827882: step 7803, loss 0.626208.
Train: 2018-08-09T02:51:21.930129: step 7804, loss 0.563121.
Train: 2018-08-09T02:51:25.033380: step 7805, loss 0.578868.
Train: 2018-08-09T02:51:28.071457: step 7806, loss 0.516236.
Train: 2018-08-09T02:51:31.163679: step 7807, loss 0.641451.
Train: 2018-08-09T02:51:34.243868: step 7808, loss 0.578893.
Train: 2018-08-09T02:51:37.321050: step 7809, loss 0.547735.
Train: 2018-08-09T02:51:40.423298: step 7810, loss 0.641179.
Test: 2018-08-09T02:52:01.400070: step 7810, loss 0.550618.
Train: 2018-08-09T02:52:04.461208: step 7811, loss 0.54789.
Train: 2018-08-09T02:52:07.574485: step 7812, loss 0.609893.
Train: 2018-08-09T02:52:10.667709: step 7813, loss 0.656226.
Train: 2018-08-09T02:52:13.757925: step 7814, loss 0.609734.
Train: 2018-08-09T02:52:16.845134: step 7815, loss 0.517693.
Train: 2018-08-09T02:52:19.948384: step 7816, loss 0.548431.
Train: 2018-08-09T02:52:23.052637: step 7817, loss 0.655328.
Train: 2018-08-09T02:52:26.127813: step 7818, loss 0.533395.
Train: 2018-08-09T02:52:29.189955: step 7819, loss 0.639801.
Train: 2018-08-09T02:52:32.283179: step 7820, loss 0.563942.
Test: 2018-08-09T02:52:53.326127: step 7820, loss 0.551553.
Train: 2018-08-09T02:52:56.432385: step 7821, loss 0.609323.
Train: 2018-08-09T02:52:59.541651: step 7822, loss 0.564065.
Train: 2018-08-09T02:53:02.610812: step 7823, loss 0.4739.
Train: 2018-08-09T02:53:05.724089: step 7824, loss 0.579148.
Train: 2018-08-09T02:53:08.808289: step 7825, loss 0.59419.
Train: 2018-08-09T02:53:11.721033: step 7826, loss 0.564102.
Train: 2018-08-09T02:53:14.832305: step 7827, loss 0.60923.
Train: 2018-08-09T02:53:17.931545: step 7828, loss 0.609225.
Train: 2018-08-09T02:53:21.003713: step 7829, loss 0.549143.
Train: 2018-08-09T02:53:24.097940: step 7830, loss 0.489017.
Test: 2018-08-09T02:53:45.097773: step 7830, loss 0.550568.
Train: 2018-08-09T02:53:48.263189: step 7831, loss 0.549053.
Train: 2018-08-09T02:53:51.372455: step 7832, loss 0.624363.
Train: 2018-08-09T02:53:54.457658: step 7833, loss 0.579114.
Train: 2018-08-09T02:53:57.569933: step 7834, loss 0.548888.
Train: 2018-08-09T02:54:00.661151: step 7835, loss 0.503443.
Train: 2018-08-09T02:54:03.755378: step 7836, loss 0.685262.
Train: 2018-08-09T02:54:06.846597: step 7837, loss 0.533531.
Train: 2018-08-09T02:54:09.933805: step 7838, loss 0.609445.
Train: 2018-08-09T02:54:13.029034: step 7839, loss 0.503037.
Train: 2018-08-09T02:54:16.143314: step 7840, loss 0.44195.
Test: 2018-08-09T02:54:37.150166: step 7840, loss 0.553416.
Train: 2018-08-09T02:54:40.231358: step 7841, loss 0.517793.
Train: 2018-08-09T02:54:43.369702: step 7842, loss 0.532839.
Train: 2018-08-09T02:54:46.447886: step 7843, loss 0.563453.
Train: 2018-08-09T02:54:49.547126: step 7844, loss 0.547826.
Train: 2018-08-09T02:54:52.635336: step 7845, loss 0.56325.
Train: 2018-08-09T02:54:55.704496: step 7846, loss 0.594553.
Train: 2018-08-09T02:54:58.789699: step 7847, loss 0.468357.
Train: 2018-08-09T02:55:01.865878: step 7848, loss 0.499351.
Train: 2018-08-09T02:55:04.953086: step 7849, loss 0.57895.
Train: 2018-08-09T02:55:08.061350: step 7850, loss 0.481751.
Test: 2018-08-09T02:55:29.087252: step 7850, loss 0.550219.
Train: 2018-08-09T02:55:32.161425: step 7851, loss 0.512578.
Train: 2018-08-09T02:55:35.261668: step 7852, loss 0.546462.
Train: 2018-08-09T02:55:38.353889: step 7853, loss 0.526188.
Train: 2018-08-09T02:55:41.448116: step 7854, loss 0.56297.
Train: 2018-08-09T02:55:44.547356: step 7855, loss 0.638413.
Train: 2018-08-09T02:55:47.649604: step 7856, loss 0.523766.
Train: 2018-08-09T02:55:50.729793: step 7857, loss 0.614493.
Train: 2018-08-09T02:55:53.822015: step 7858, loss 0.599294.
Train: 2018-08-09T02:55:56.900199: step 7859, loss 0.603715.
Train: 2018-08-09T02:56:00.000442: step 7860, loss 0.579947.
Test: 2018-08-09T02:56:21.023336: step 7860, loss 0.548342.
Train: 2018-08-09T02:56:24.157669: step 7861, loss 0.530827.
Train: 2018-08-09T02:56:27.237858: step 7862, loss 0.495722.
Train: 2018-08-09T02:56:30.325066: step 7863, loss 0.595704.
Train: 2018-08-09T02:56:33.430322: step 7864, loss 0.528726.
Train: 2018-08-09T02:56:36.531568: step 7865, loss 0.495461.
Train: 2018-08-09T02:56:39.649858: step 7866, loss 0.544485.
Train: 2018-08-09T02:56:42.760128: step 7867, loss 0.731193.
Train: 2018-08-09T02:56:45.895464: step 7868, loss 0.562773.
Train: 2018-08-09T02:56:49.007738: step 7869, loss 0.52966.
Train: 2018-08-09T02:56:52.086925: step 7870, loss 0.461228.
Test: 2018-08-09T02:57:13.099793: step 7870, loss 0.545927.
Train: 2018-08-09T02:57:16.169955: step 7871, loss 0.510664.
Train: 2018-08-09T02:57:19.292256: step 7872, loss 0.630268.
Train: 2018-08-09T02:57:22.366430: step 7873, loss 0.545765.
Train: 2018-08-09T02:57:25.416539: step 7874, loss 0.631233.
Train: 2018-08-09T02:57:28.532825: step 7875, loss 0.600314.
Train: 2018-08-09T02:57:31.603990: step 7876, loss 0.61238.
Train: 2018-08-09T02:57:34.668137: step 7877, loss 0.545945.
Train: 2018-08-09T02:57:37.773393: step 7878, loss 0.596517.
Train: 2018-08-09T02:57:40.869625: step 7879, loss 0.628219.
Train: 2018-08-09T02:57:43.999947: step 7880, loss 0.57912.
Test: 2018-08-09T02:58:05.015824: step 7880, loss 0.547041.
Train: 2018-08-09T02:58:08.117068: step 7881, loss 0.54672.
Train: 2018-08-09T02:58:11.218314: step 7882, loss 0.627203.
Train: 2018-08-09T02:58:14.327580: step 7883, loss 0.578888.
Train: 2018-08-09T02:58:17.448879: step 7884, loss 0.482984.
Train: 2018-08-09T02:58:20.542103: step 7885, loss 0.546842.
Train: 2018-08-09T02:58:23.669418: step 7886, loss 0.578956.
Train: 2018-08-09T02:58:26.795730: step 7887, loss 0.499088.
Train: 2018-08-09T02:58:29.895972: step 7888, loss 0.562818.
Train: 2018-08-09T02:58:32.971148: step 7889, loss 0.562794.
Train: 2018-08-09T02:58:36.068383: step 7890, loss 0.514936.
Test: 2018-08-09T02:58:57.133389: step 7890, loss 0.549607.
Train: 2018-08-09T02:59:00.304821: step 7891, loss 0.546863.
Train: 2018-08-09T02:59:03.532402: step 7892, loss 0.627126.
Train: 2018-08-09T02:59:06.613594: step 7893, loss 0.659177.
Train: 2018-08-09T02:59:09.749933: step 7894, loss 0.626896.
Train: 2018-08-09T02:59:12.874240: step 7895, loss 0.59481.
Train: 2018-08-09T02:59:15.957437: step 7896, loss 0.562883.
Train: 2018-08-09T02:59:19.033616: step 7897, loss 0.5947.
Train: 2018-08-09T02:59:22.144888: step 7898, loss 0.642297.
Train: 2018-08-09T02:59:25.244128: step 7899, loss 0.547043.
Train: 2018-08-09T02:59:28.324317: step 7900, loss 0.51582.
Test: 2018-08-09T02:59:49.317132: step 7900, loss 0.5488.
Train: 2018-08-09T02:59:54.205127: step 7901, loss 0.531398.
Train: 2018-08-09T02:59:57.279300: step 7902, loss 0.467678.
Train: 2018-08-09T03:00:00.364503: step 7903, loss 0.451749.
Train: 2018-08-09T03:00:03.423637: step 7904, loss 0.562644.
Train: 2018-08-09T03:00:06.511847: step 7905, loss 0.513671.
Train: 2018-08-09T03:00:09.582010: step 7906, loss 0.570193.
Train: 2018-08-09T03:00:12.704311: step 7907, loss 0.481773.
Train: 2018-08-09T03:00:15.800543: step 7908, loss 0.512689.
Train: 2018-08-09T03:00:18.917831: step 7909, loss 0.477775.
Train: 2018-08-09T03:00:21.993007: step 7910, loss 0.580346.
Test: 2018-08-09T03:00:42.996852: step 7910, loss 0.545974.
Train: 2018-08-09T03:00:46.107120: step 7911, loss 0.543813.
Train: 2018-08-09T03:00:49.184301: step 7912, loss 0.565916.
Train: 2018-08-09T03:00:52.309611: step 7913, loss 0.578101.
Train: 2018-08-09T03:00:55.387795: step 7914, loss 0.561585.
Train: 2018-08-09T03:00:58.498064: step 7915, loss 0.544315.
Train: 2018-08-09T03:01:01.625379: step 7916, loss 0.546144.
Train: 2018-08-09T03:01:04.758709: step 7917, loss 0.610933.
Train: 2018-08-09T03:01:07.846920: step 7918, loss 0.582422.
Train: 2018-08-09T03:01:10.923099: step 7919, loss 0.546914.
Train: 2018-08-09T03:01:14.014318: step 7920, loss 0.512681.
Test: 2018-08-09T03:01:35.024177: step 7920, loss 0.548923.
Train: 2018-08-09T03:01:38.114394: step 7921, loss 0.581659.
Train: 2018-08-09T03:01:41.209622: step 7922, loss 0.613815.
Train: 2018-08-09T03:01:44.320894: step 7923, loss 0.547699.
Train: 2018-08-09T03:01:47.418129: step 7924, loss 0.629097.
Train: 2018-08-09T03:01:50.530404: step 7925, loss 0.66103.
Train: 2018-08-09T03:01:53.593548: step 7926, loss 0.644128.
Train: 2018-08-09T03:01:56.693790: step 7927, loss 0.514025.
Train: 2018-08-09T03:01:59.814086: step 7928, loss 0.498095.
Train: 2018-08-09T03:02:02.912324: step 7929, loss 0.562722.
Train: 2018-08-09T03:02:05.990508: step 7930, loss 0.498293.
Test: 2018-08-09T03:02:27.019418: step 7930, loss 0.547651.
Train: 2018-08-09T03:02:30.105623: step 7931, loss 0.530469.
Train: 2018-08-09T03:02:33.205866: step 7932, loss 0.530448.
Train: 2018-08-09T03:02:36.317138: step 7933, loss 0.546646.
Train: 2018-08-09T03:02:39.401338: step 7934, loss 0.627403.
Train: 2018-08-09T03:02:42.493559: step 7935, loss 0.627414.
Train: 2018-08-09T03:02:45.618869: step 7936, loss 0.514284.
Train: 2018-08-09T03:02:48.717106: step 7937, loss 0.530404.
Train: 2018-08-09T03:02:51.803311: step 7938, loss 0.514214.
Train: 2018-08-09T03:02:54.901549: step 7939, loss 0.562689.
Train: 2018-08-09T03:02:58.031871: step 7940, loss 0.595118.
Test: 2018-08-09T03:03:19.089859: step 7940, loss 0.549315.
Train: 2018-08-09T03:03:22.184086: step 7941, loss 0.562665.
Train: 2018-08-09T03:03:25.320424: step 7942, loss 0.611399.
Train: 2018-08-09T03:03:28.397606: step 7943, loss 0.513981.
Train: 2018-08-09T03:03:31.505870: step 7944, loss 0.595186.
Train: 2018-08-09T03:03:34.595083: step 7945, loss 0.513944.
Train: 2018-08-09T03:03:37.677278: step 7946, loss 0.578938.
Train: 2018-08-09T03:03:40.834672: step 7947, loss 0.562657.
Train: 2018-08-09T03:03:43.920878: step 7948, loss 0.513847.
Train: 2018-08-09T03:03:47.006080: step 7949, loss 0.562622.
Train: 2018-08-09T03:03:50.104318: step 7950, loss 0.481079.
Test: 2018-08-09T03:04:11.166316: step 7950, loss 0.548491.
Train: 2018-08-09T03:04:14.269566: step 7951, loss 0.578996.
Train: 2018-08-09T03:04:17.369809: step 7952, loss 0.529866.
Train: 2018-08-09T03:04:20.470052: step 7953, loss 0.661157.
Train: 2018-08-09T03:04:23.559265: step 7954, loss 0.562534.
Train: 2018-08-09T03:04:26.721673: step 7955, loss 0.562502.
Train: 2018-08-09T03:04:29.792838: step 7956, loss 0.579027.
Train: 2018-08-09T03:04:32.894084: step 7957, loss 0.431194.
Train: 2018-08-09T03:04:36.022401: step 7958, loss 0.595359.
Train: 2018-08-09T03:04:39.136681: step 7959, loss 0.562484.
Train: 2018-08-09T03:04:42.233916: step 7960, loss 0.628563.
Test: 2018-08-09T03:05:03.270848: step 7960, loss 0.548893.
Train: 2018-08-09T03:05:06.379111: step 7961, loss 0.578956.
Train: 2018-08-09T03:05:09.503418: step 7962, loss 0.579078.
Train: 2018-08-09T03:05:12.632738: step 7963, loss 0.595465.
Train: 2018-08-09T03:05:15.716938: step 7964, loss 0.579.
Train: 2018-08-09T03:05:18.796125: step 7965, loss 0.579011.
Train: 2018-08-09T03:05:21.889349: step 7966, loss 0.595345.
Train: 2018-08-09T03:05:24.993602: step 7967, loss 0.562529.
Train: 2018-08-09T03:05:28.135957: step 7968, loss 0.578976.
Train: 2018-08-09T03:05:31.280317: step 7969, loss 0.546384.
Train: 2018-08-09T03:05:34.398607: step 7970, loss 0.62778.
Test: 2018-08-09T03:05:55.440552: step 7970, loss 0.548056.
Train: 2018-08-09T03:05:58.533776: step 7971, loss 0.61132.
Train: 2018-08-09T03:06:01.648056: step 7972, loss 0.497883.
Train: 2018-08-09T03:06:04.735264: step 7973, loss 0.611225.
Train: 2018-08-09T03:06:07.853555: step 7974, loss 0.594982.
Train: 2018-08-09T03:06:10.956806: step 7975, loss 0.53055.
Train: 2018-08-09T03:06:14.059053: step 7976, loss 0.546748.
Train: 2018-08-09T03:06:17.137237: step 7977, loss 0.578906.
Train: 2018-08-09T03:06:20.254526: step 7978, loss 0.578913.
Train: 2018-08-09T03:06:23.350758: step 7979, loss 0.563037.
Train: 2018-08-09T03:06:26.420920: step 7980, loss 0.514988.
Test: 2018-08-09T03:06:47.548092: step 7980, loss 0.55332.
Train: 2018-08-09T03:06:50.669390: step 7981, loss 0.658828.
Train: 2018-08-09T03:06:53.790689: step 7982, loss 0.514991.
Train: 2018-08-09T03:06:56.856841: step 7983, loss 0.53098.
Train: 2018-08-09T03:06:59.943046: step 7984, loss 0.626794.
Train: 2018-08-09T03:07:03.022233: step 7985, loss 0.514966.
Train: 2018-08-09T03:07:06.136513: step 7986, loss 0.626522.
Train: 2018-08-09T03:07:09.208681: step 7987, loss 0.674339.
Train: 2018-08-09T03:07:12.307921: step 7988, loss 0.594802.
Train: 2018-08-09T03:07:15.387108: step 7989, loss 0.547022.
Train: 2018-08-09T03:07:18.502391: step 7990, loss 0.547112.
Test: 2018-08-09T03:07:39.548346: step 7990, loss 0.550085.
Train: 2018-08-09T03:07:42.665634: step 7991, loss 0.499526.
Train: 2018-08-09T03:07:45.848095: step 7992, loss 0.595067.
Train: 2018-08-09T03:07:48.945330: step 7993, loss 0.611487.
Train: 2018-08-09T03:07:52.036549: step 7994, loss 0.515709.
Train: 2018-08-09T03:07:55.115735: step 7995, loss 0.59542.
Train: 2018-08-09T03:07:58.219989: step 7996, loss 0.626021.
Train: 2018-08-09T03:08:01.317223: step 7997, loss 0.547076.
Train: 2018-08-09T03:08:04.409445: step 7998, loss 0.610088.
Train: 2018-08-09T03:08:07.489634: step 7999, loss 0.547672.
Train: 2018-08-09T03:08:10.601909: step 8000, loss 0.547339.
Test: 2018-08-09T03:08:31.625806: step 8000, loss 0.550921.
Train: 2018-08-09T03:08:36.458655: step 8001, loss 0.516418.
Train: 2018-08-09T03:08:39.543857: step 8002, loss 0.610062.
Train: 2018-08-09T03:08:42.656132: step 8003, loss 0.68872.
Train: 2018-08-09T03:08:45.763393: step 8004, loss 0.500871.
Train: 2018-08-09T03:08:48.853609: step 8005, loss 0.547435.
Train: 2018-08-09T03:08:51.945831: step 8006, loss 0.610114.
Train: 2018-08-09T03:08:55.033039: step 8007, loss 0.610376.
Train: 2018-08-09T03:08:58.118242: step 8008, loss 0.54779.
Train: 2018-08-09T03:09:01.222495: step 8009, loss 0.610101.
Train: 2018-08-09T03:09:04.302684: step 8010, loss 0.501415.
Test: 2018-08-09T03:09:25.315552: step 8010, loss 0.551242.
Train: 2018-08-09T03:09:28.445874: step 8011, loss 0.609975.
Train: 2018-08-09T03:09:31.583216: step 8012, loss 0.594445.
Train: 2018-08-09T03:09:34.697496: step 8013, loss 0.486009.
Train: 2018-08-09T03:09:37.801749: step 8014, loss 0.547872.
Train: 2018-08-09T03:09:40.916029: step 8015, loss 0.532475.
Train: 2018-08-09T03:09:44.020282: step 8016, loss 0.531991.
Train: 2018-08-09T03:09:47.127544: step 8017, loss 0.532751.
Train: 2018-08-09T03:09:50.211744: step 8018, loss 0.563088.
Train: 2018-08-09T03:09:53.317000: step 8019, loss 0.499914.
Train: 2018-08-09T03:09:56.399194: step 8020, loss 0.546347.
Test: 2018-08-09T03:10:17.422089: step 8020, loss 0.552031.
Train: 2018-08-09T03:10:20.509297: step 8021, loss 0.577882.
Train: 2018-08-09T03:10:23.617561: step 8022, loss 0.563089.
Train: 2018-08-09T03:10:26.735851: step 8023, loss 0.592652.
Train: 2018-08-09T03:10:29.841107: step 8024, loss 0.531134.
Train: 2018-08-09T03:10:32.933329: step 8025, loss 0.677443.
Train: 2018-08-09T03:10:36.013518: step 8026, loss 0.434428.
Train: 2018-08-09T03:10:39.102731: step 8027, loss 0.490708.
Train: 2018-08-09T03:10:42.183923: step 8028, loss 0.525862.
Train: 2018-08-09T03:10:45.282161: step 8029, loss 0.692197.
Train: 2018-08-09T03:10:48.394435: step 8030, loss 0.497749.
Test: 2018-08-09T03:11:09.419335: step 8030, loss 0.549135.
Train: 2018-08-09T03:11:12.510553: step 8031, loss 0.610061.
Train: 2018-08-09T03:11:15.626839: step 8032, loss 0.610677.
Train: 2018-08-09T03:11:18.719060: step 8033, loss 0.642795.
Train: 2018-08-09T03:11:21.815292: step 8034, loss 0.563306.
Train: 2018-08-09T03:11:24.902500: step 8035, loss 0.51216.
Train: 2018-08-09T03:11:28.016780: step 8036, loss 0.614666.
Train: 2018-08-09T03:11:31.092959: step 8037, loss 0.585056.
Train: 2018-08-09T03:11:34.184178: step 8038, loss 0.626667.
Train: 2018-08-09T03:11:37.302468: step 8039, loss 0.58005.
Train: 2018-08-09T03:11:40.414743: step 8040, loss 0.640373.
Test: 2018-08-09T03:12:01.444656: step 8040, loss 0.552894.
Train: 2018-08-09T03:12:04.545901: step 8041, loss 0.534599.
Train: 2018-08-09T03:12:07.674218: step 8042, loss 0.639706.
Train: 2018-08-09T03:12:10.771453: step 8043, loss 0.526115.
Train: 2018-08-09T03:12:13.873701: step 8044, loss 0.661843.
Train: 2018-08-09T03:12:16.937848: step 8045, loss 0.501689.
Train: 2018-08-09T03:12:20.026058: step 8046, loss 0.563617.
Train: 2018-08-09T03:12:23.099229: step 8047, loss 0.609984.
Train: 2018-08-09T03:12:26.203483: step 8048, loss 0.547814.
Train: 2018-08-09T03:12:29.287683: step 8049, loss 0.609566.
Train: 2018-08-09T03:12:32.371883: step 8050, loss 0.56087.
Test: 2018-08-09T03:12:53.404804: step 8050, loss 0.548362.
Train: 2018-08-09T03:12:56.521089: step 8051, loss 0.614841.
Train: 2018-08-09T03:12:59.632361: step 8052, loss 0.531114.
Train: 2018-08-09T03:13:02.730598: step 8053, loss 0.501466.
Train: 2018-08-09T03:13:05.824825: step 8054, loss 0.38841.
Train: 2018-08-09T03:13:08.930081: step 8055, loss 0.589391.
Train: 2018-08-09T03:13:12.032329: step 8056, loss 0.545171.
Train: 2018-08-09T03:13:15.121542: step 8057, loss 0.636622.
Train: 2018-08-09T03:13:18.215769: step 8058, loss 0.564492.
Train: 2018-08-09T03:13:21.284929: step 8059, loss 0.535242.
Train: 2018-08-09T03:13:24.355092: step 8060, loss 0.486059.
Test: 2018-08-09T03:13:45.377986: step 8060, loss 0.548847.
Train: 2018-08-09T03:13:48.484244: step 8061, loss 0.562969.
Train: 2018-08-09T03:13:51.567442: step 8062, loss 0.593592.
Train: 2018-08-09T03:13:54.654650: step 8063, loss 0.56314.
Train: 2018-08-09T03:13:57.744866: step 8064, loss 0.611052.
Train: 2018-08-09T03:14:00.847114: step 8065, loss 0.546183.
Train: 2018-08-09T03:14:03.940338: step 8066, loss 0.559911.
Train: 2018-08-09T03:14:07.039578: step 8067, loss 0.684632.
Train: 2018-08-09T03:14:10.156866: step 8068, loss 0.622535.
Train: 2018-08-09T03:14:13.262122: step 8069, loss 0.514889.
Train: 2018-08-09T03:14:16.346322: step 8070, loss 0.515093.
Test: 2018-08-09T03:14:37.360193: step 8070, loss 0.54794.
Train: 2018-08-09T03:14:40.492520: step 8071, loss 0.552377.
Train: 2018-08-09T03:14:43.559675: step 8072, loss 0.488762.
Train: 2018-08-09T03:14:46.672952: step 8073, loss 0.551692.
Train: 2018-08-09T03:14:49.787232: step 8074, loss 0.49976.
Train: 2018-08-09T03:14:52.888477: step 8075, loss 0.491369.
Train: 2018-08-09T03:14:55.998747: step 8076, loss 0.600022.
Train: 2018-08-09T03:14:59.092973: step 8077, loss 0.601755.
Train: 2018-08-09T03:15:02.183189: step 8078, loss 0.507264.
Train: 2018-08-09T03:15:05.268392: step 8079, loss 0.517871.
Train: 2018-08-09T03:15:08.352592: step 8080, loss 0.596736.
Test: 2018-08-09T03:15:29.394537: step 8080, loss 0.551861.
Train: 2018-08-09T03:15:32.470716: step 8081, loss 0.486048.
Train: 2018-08-09T03:15:35.564942: step 8082, loss 0.456016.
Train: 2018-08-09T03:15:38.669196: step 8083, loss 0.498945.
Train: 2018-08-09T03:15:41.744372: step 8084, loss 0.554929.
Train: 2018-08-09T03:15:44.823558: step 8085, loss 0.607003.
Train: 2018-08-09T03:15:47.930820: step 8086, loss 0.527533.
Train: 2018-08-09T03:15:51.018028: step 8087, loss 0.639349.
Train: 2018-08-09T03:15:54.100223: step 8088, loss 0.627699.
Train: 2018-08-09T03:15:57.214503: step 8089, loss 0.592511.
Train: 2018-08-09T03:16:00.298703: step 8090, loss 0.520216.
Test: 2018-08-09T03:16:21.315581: step 8090, loss 0.547267.
Train: 2018-08-09T03:16:24.383738: step 8091, loss 0.667042.
Train: 2018-08-09T03:16:27.537122: step 8092, loss 0.611851.
Train: 2018-08-09T03:16:30.656415: step 8093, loss 0.487746.
Train: 2018-08-09T03:16:33.756658: step 8094, loss 0.56416.
Train: 2018-08-09T03:16:36.915055: step 8095, loss 0.449641.
Train: 2018-08-09T03:16:40.010285: step 8096, loss 0.627842.
Train: 2018-08-09T03:16:43.099498: step 8097, loss 0.613702.
Train: 2018-08-09T03:16:46.196733: step 8098, loss 0.558217.
Train: 2018-08-09T03:16:49.287952: step 8099, loss 0.593863.
Train: 2018-08-09T03:16:52.367138: step 8100, loss 0.617108.
Test: 2018-08-09T03:17:13.398054: step 8100, loss 0.550472.
Train: 2018-08-09T03:17:18.242935: step 8101, loss 0.516236.
Train: 2018-08-09T03:17:21.327135: step 8102, loss 0.58045.
Train: 2018-08-09T03:17:24.426375: step 8103, loss 0.518315.
Train: 2018-08-09T03:17:27.530628: step 8104, loss 0.685884.
Train: 2018-08-09T03:17:30.609815: step 8105, loss 0.505917.
Train: 2018-08-09T03:17:33.711060: step 8106, loss 0.559182.
Train: 2018-08-09T03:17:36.818322: step 8107, loss 0.49944.
Train: 2018-08-09T03:17:39.933604: step 8108, loss 0.55218.
Train: 2018-08-09T03:17:42.993740: step 8109, loss 0.494257.
Train: 2018-08-09T03:17:46.087967: step 8110, loss 0.628673.
Test: 2018-08-09T03:18:07.127907: step 8110, loss 0.547601.
Train: 2018-08-09T03:18:10.218123: step 8111, loss 0.628474.
Train: 2018-08-09T03:18:13.320370: step 8112, loss 0.577793.
Train: 2018-08-09T03:18:16.423621: step 8113, loss 0.510341.
Train: 2018-08-09T03:18:19.490776: step 8114, loss 0.601319.
Train: 2018-08-09T03:18:22.585002: step 8115, loss 0.524221.
Train: 2018-08-09T03:18:25.693267: step 8116, loss 0.54332.
Train: 2018-08-09T03:18:28.847653: step 8117, loss 0.539123.
Train: 2018-08-09T03:18:31.966946: step 8118, loss 0.63652.
Train: 2018-08-09T03:18:35.044128: step 8119, loss 0.559628.
Train: 2018-08-09T03:18:38.179464: step 8120, loss 0.535518.
Test: 2018-08-09T03:18:59.223415: step 8120, loss 0.552001.
Train: 2018-08-09T03:19:02.316638: step 8121, loss 0.58145.
Train: 2018-08-09T03:19:05.408859: step 8122, loss 0.592626.
Train: 2018-08-09T03:19:08.503086: step 8123, loss 0.57442.
Train: 2018-08-09T03:19:11.609345: step 8124, loss 0.691204.
Train: 2018-08-09T03:19:14.687529: step 8125, loss 0.529247.
Train: 2018-08-09T03:19:17.787771: step 8126, loss 0.547894.
Train: 2018-08-09T03:19:20.723577: step 8127, loss 0.628015.
Train: 2018-08-09T03:19:23.828833: step 8128, loss 0.652956.
Train: 2018-08-09T03:19:26.928073: step 8129, loss 0.539117.
Train: 2018-08-09T03:19:30.016284: step 8130, loss 0.530101.
Test: 2018-08-09T03:19:51.033162: step 8130, loss 0.549167.
Train: 2018-08-09T03:19:54.117370: step 8131, loss 0.564332.
Train: 2018-08-09T03:19:57.224623: step 8132, loss 0.433547.
Train: 2018-08-09T03:20:00.327874: step 8133, loss 0.571444.
Train: 2018-08-09T03:20:03.433130: step 8134, loss 0.628307.
Train: 2018-08-09T03:20:06.535378: step 8135, loss 0.55186.
Train: 2018-08-09T03:20:09.614565: step 8136, loss 0.579164.
Train: 2018-08-09T03:20:12.728844: step 8137, loss 0.518299.
Train: 2018-08-09T03:20:15.837108: step 8138, loss 0.518197.
Train: 2018-08-09T03:20:18.922311: step 8139, loss 0.594183.
Train: 2018-08-09T03:20:22.002500: step 8140, loss 0.519047.
Test: 2018-08-09T03:20:43.044446: step 8140, loss 0.549215.
Train: 2018-08-09T03:20:46.114608: step 8141, loss 0.609839.
Train: 2018-08-09T03:20:49.278019: step 8142, loss 0.472246.
Train: 2018-08-09T03:20:52.375253: step 8143, loss 0.641004.
Train: 2018-08-09T03:20:55.484520: step 8144, loss 0.609909.
Train: 2018-08-09T03:20:58.580752: step 8145, loss 0.595063.
Train: 2018-08-09T03:21:01.694029: step 8146, loss 0.580526.
Train: 2018-08-09T03:21:04.795275: step 8147, loss 0.702662.
Train: 2018-08-09T03:21:07.909555: step 8148, loss 0.503644.
Train: 2018-08-09T03:21:10.983728: step 8149, loss 0.531824.
Train: 2018-08-09T03:21:14.079960: step 8150, loss 0.64.
Test: 2018-08-09T03:21:35.097841: step 8150, loss 0.550898.
Train: 2018-08-09T03:21:38.149955: step 8151, loss 0.655917.
Train: 2018-08-09T03:21:41.234156: step 8152, loss 0.625353.
Train: 2018-08-09T03:21:44.324372: step 8153, loss 0.547399.
Train: 2018-08-09T03:21:47.427622: step 8154, loss 0.487856.
Train: 2018-08-09T03:21:50.513827: step 8155, loss 0.487077.
Train: 2018-08-09T03:21:53.607052: step 8156, loss 0.592991.
Train: 2018-08-09T03:21:56.688244: step 8157, loss 0.454866.
Train: 2018-08-09T03:21:59.794502: step 8158, loss 0.543376.
Train: 2018-08-09T03:22:02.883716: step 8159, loss 0.604159.
Train: 2018-08-09T03:22:05.974935: step 8160, loss 0.514742.
Test: 2018-08-09T03:22:27.018885: step 8160, loss 0.547266.
Train: 2018-08-09T03:22:30.138178: step 8161, loss 0.482972.
Train: 2018-08-09T03:22:33.229397: step 8162, loss 0.568656.
Train: 2018-08-09T03:22:36.339666: step 8163, loss 0.568396.
Train: 2018-08-09T03:22:39.438906: step 8164, loss 0.5803.
Train: 2018-08-09T03:22:42.551181: step 8165, loss 0.577508.
Train: 2018-08-09T03:22:45.652426: step 8166, loss 0.546193.
Train: 2018-08-09T03:22:48.757682: step 8167, loss 0.627001.
Train: 2018-08-09T03:22:51.876975: step 8168, loss 0.575063.
Train: 2018-08-09T03:22:54.960173: step 8169, loss 0.64099.
Train: 2018-08-09T03:22:58.076458: step 8170, loss 0.63271.
Test: 2018-08-09T03:23:19.132441: step 8170, loss 0.549569.
Train: 2018-08-09T03:23:22.197589: step 8171, loss 0.626361.
Train: 2018-08-09T03:23:25.278781: step 8172, loss 0.520652.
Train: 2018-08-09T03:23:28.380027: step 8173, loss 0.609523.
Train: 2018-08-09T03:23:31.500323: step 8174, loss 0.48651.
Train: 2018-08-09T03:23:34.616608: step 8175, loss 0.516367.
Train: 2018-08-09T03:23:37.690782: step 8176, loss 0.674317.
Train: 2018-08-09T03:23:40.812080: step 8177, loss 0.579601.
Train: 2018-08-09T03:23:43.892270: step 8178, loss 0.609961.
Train: 2018-08-09T03:23:47.001536: step 8179, loss 0.61041.
Train: 2018-08-09T03:23:50.078718: step 8180, loss 0.626171.
Test: 2018-08-09T03:24:11.089580: step 8180, loss 0.549367.
Train: 2018-08-09T03:24:14.186814: step 8181, loss 0.531648.
Train: 2018-08-09T03:24:17.288060: step 8182, loss 0.485726.
Train: 2018-08-09T03:24:20.378276: step 8183, loss 0.609663.
Train: 2018-08-09T03:24:23.493558: step 8184, loss 0.64173.
Train: 2018-08-09T03:24:26.616863: step 8185, loss 0.468957.
Train: 2018-08-09T03:24:29.724124: step 8186, loss 0.562026.
Train: 2018-08-09T03:24:32.837401: step 8187, loss 0.563288.
Train: 2018-08-09T03:24:35.930625: step 8188, loss 0.563875.
Train: 2018-08-09T03:24:39.029865: step 8189, loss 0.513673.
Train: 2018-08-09T03:24:42.119078: step 8190, loss 0.464875.
Test: 2018-08-09T03:25:03.163029: step 8190, loss 0.549813.
Train: 2018-08-09T03:25:06.308391: step 8191, loss 0.64121.
Train: 2018-08-09T03:25:09.427685: step 8192, loss 0.548471.
Train: 2018-08-09T03:25:12.551991: step 8193, loss 0.581877.
Train: 2018-08-09T03:25:15.659253: step 8194, loss 0.579586.
Train: 2018-08-09T03:25:18.756487: step 8195, loss 0.55963.
Train: 2018-08-09T03:25:21.845701: step 8196, loss 0.507872.
Train: 2018-08-09T03:25:24.965997: step 8197, loss 0.509049.
Train: 2018-08-09T03:25:28.073258: step 8198, loss 0.588063.
Train: 2018-08-09T03:25:31.196562: step 8199, loss 0.482955.
Train: 2018-08-09T03:25:34.285775: step 8200, loss 0.437597.
Test: 2018-08-09T03:25:55.360809: step 8200, loss 0.548549.
Train: 2018-08-09T03:26:00.267855: step 8201, loss 0.656517.
Train: 2018-08-09T03:26:03.320972: step 8202, loss 0.594697.
Train: 2018-08-09T03:26:06.425225: step 8203, loss 0.434543.
Train: 2018-08-09T03:26:09.497393: step 8204, loss 0.526935.
Train: 2018-08-09T03:26:12.562543: step 8205, loss 0.606798.
Train: 2018-08-09T03:26:15.665794: step 8206, loss 0.516698.
Train: 2018-08-09T03:26:18.753001: step 8207, loss 0.508994.
Train: 2018-08-09T03:26:21.831186: step 8208, loss 0.711288.
Train: 2018-08-09T03:26:24.951482: step 8209, loss 0.619215.
Train: 2018-08-09T03:26:28.051724: step 8210, loss 0.612901.
Test: 2018-08-09T03:26:49.127760: step 8210, loss 0.557001.
Train: 2018-08-09T03:26:52.183885: step 8211, loss 0.628307.
Train: 2018-08-09T03:26:55.266080: step 8212, loss 0.554872.
Train: 2018-08-09T03:26:58.369330: step 8213, loss 0.567009.
Train: 2018-08-09T03:27:01.474586: step 8214, loss 0.595858.
Train: 2018-08-09T03:27:04.560792: step 8215, loss 0.485115.
Train: 2018-08-09T03:27:07.657024: step 8216, loss 0.440641.
Train: 2018-08-09T03:27:10.772306: step 8217, loss 0.734777.
Train: 2018-08-09T03:27:13.882576: step 8218, loss 0.517346.
Train: 2018-08-09T03:27:16.970786: step 8219, loss 0.602957.
Train: 2018-08-09T03:27:20.055989: step 8220, loss 0.515596.
Test: 2018-08-09T03:27:41.077881: step 8220, loss 0.550125.
Train: 2018-08-09T03:27:44.139019: step 8221, loss 0.531851.
Train: 2018-08-09T03:27:47.244275: step 8222, loss 0.562505.
Train: 2018-08-09T03:27:50.325467: step 8223, loss 0.564186.
Train: 2018-08-09T03:27:53.406659: step 8224, loss 0.579191.
Train: 2018-08-09T03:27:56.526955: step 8225, loss 0.624636.
Train: 2018-08-09T03:27:59.616169: step 8226, loss 0.610803.
Train: 2018-08-09T03:28:02.731451: step 8227, loss 0.61111.
Train: 2018-08-09T03:28:05.841721: step 8228, loss 0.533739.
Train: 2018-08-09T03:28:08.938955: step 8229, loss 0.549587.
Train: 2018-08-09T03:28:12.036190: step 8230, loss 0.641336.
Test: 2018-08-09T03:28:33.073122: step 8230, loss 0.551085.
Train: 2018-08-09T03:28:36.155316: step 8231, loss 0.563621.
Train: 2018-08-09T03:28:39.265585: step 8232, loss 0.579202.
Train: 2018-08-09T03:28:42.354799: step 8233, loss 0.532371.
Train: 2018-08-09T03:28:45.445015: step 8234, loss 0.580607.
Train: 2018-08-09T03:28:48.587370: step 8235, loss 0.547649.
Train: 2018-08-09T03:28:51.674578: step 8236, loss 0.502595.
Train: 2018-08-09T03:28:54.768804: step 8237, loss 0.532448.
Train: 2018-08-09T03:28:57.845986: step 8238, loss 0.594676.
Train: 2018-08-09T03:29:00.924170: step 8239, loss 0.578964.
Train: 2018-08-09T03:29:04.029426: step 8240, loss 0.530742.
Test: 2018-08-09T03:29:25.070369: step 8240, loss 0.547817.
Train: 2018-08-09T03:29:28.186653: step 8241, loss 0.499099.
Train: 2018-08-09T03:29:31.298928: step 8242, loss 0.579045.
Train: 2018-08-09T03:29:34.392152: step 8243, loss 0.530792.
Train: 2018-08-09T03:29:37.485376: step 8244, loss 0.481352.
Train: 2018-08-09T03:29:40.596648: step 8245, loss 0.579183.
Train: 2018-08-09T03:29:43.661797: step 8246, loss 0.579767.
Train: 2018-08-09T03:29:46.779085: step 8247, loss 0.579445.
Train: 2018-08-09T03:29:49.867296: step 8248, loss 0.578435.
Train: 2018-08-09T03:29:52.991603: step 8249, loss 0.464457.
Train: 2018-08-09T03:29:56.068784: step 8250, loss 0.545922.
Test: 2018-08-09T03:30:17.122762: step 8250, loss 0.547515.
Train: 2018-08-09T03:30:20.260102: step 8251, loss 0.629047.
Train: 2018-08-09T03:30:23.364356: step 8252, loss 0.612437.
Train: 2018-08-09T03:30:26.480641: step 8253, loss 0.628936.
Train: 2018-08-09T03:30:29.581886: step 8254, loss 0.579063.
Train: 2018-08-09T03:30:32.679121: step 8255, loss 0.661174.
Train: 2018-08-09T03:30:35.788387: step 8256, loss 0.561925.
Train: 2018-08-09T03:30:38.879606: step 8257, loss 0.54652.
Train: 2018-08-09T03:30:41.996894: step 8258, loss 0.519898.
Train: 2018-08-09T03:30:45.060038: step 8259, loss 0.561596.
Train: 2018-08-09T03:30:48.151257: step 8260, loss 0.64196.
Test: 2018-08-09T03:31:09.179165: step 8260, loss 0.550023.
Train: 2018-08-09T03:31:12.302468: step 8261, loss 0.547358.
Train: 2018-08-09T03:31:15.392684: step 8262, loss 0.548339.
Train: 2018-08-09T03:31:18.470868: step 8263, loss 0.497524.
Train: 2018-08-09T03:31:21.565095: step 8264, loss 0.515749.
Train: 2018-08-09T03:31:24.696420: step 8265, loss 0.546239.
Train: 2018-08-09T03:31:27.811703: step 8266, loss 0.563802.
Train: 2018-08-09T03:31:30.918964: step 8267, loss 0.661125.
Train: 2018-08-09T03:31:34.049287: step 8268, loss 0.562792.
Train: 2018-08-09T03:31:37.149530: step 8269, loss 0.497194.
Train: 2018-08-09T03:31:40.243756: step 8270, loss 0.728902.
Test: 2018-08-09T03:32:01.306758: step 8270, loss 0.549212.
Train: 2018-08-09T03:32:04.397976: step 8271, loss 0.660571.
Train: 2018-08-09T03:32:07.516267: step 8272, loss 0.547054.
Train: 2018-08-09T03:32:10.611496: step 8273, loss 0.57971.
Train: 2018-08-09T03:32:13.706725: step 8274, loss 0.499043.
Train: 2018-08-09T03:32:16.814989: step 8275, loss 0.626603.
Train: 2018-08-09T03:32:19.926261: step 8276, loss 0.642521.
Train: 2018-08-09T03:32:23.021491: step 8277, loss 0.579075.
Train: 2018-08-09T03:32:26.112709: step 8278, loss 0.499716.
Train: 2018-08-09T03:32:29.197912: step 8279, loss 0.578078.
Train: 2018-08-09T03:32:32.297152: step 8280, loss 0.515722.
Test: 2018-08-09T03:32:53.489497: step 8280, loss 0.550101.
Train: 2018-08-09T03:32:56.656918: step 8281, loss 0.594054.
Train: 2018-08-09T03:32:59.763177: step 8282, loss 0.579274.
Train: 2018-08-09T03:33:02.875451: step 8283, loss 0.657548.
Train: 2018-08-09T03:33:05.984718: step 8284, loss 0.531368.
Train: 2018-08-09T03:33:09.073931: step 8285, loss 0.563826.
Train: 2018-08-09T03:33:12.164147: step 8286, loss 0.547571.
Train: 2018-08-09T03:33:15.269404: step 8287, loss 0.563808.
Train: 2018-08-09T03:33:18.350595: step 8288, loss 0.657125.
Train: 2018-08-09T03:33:21.436801: step 8289, loss 0.547865.
Train: 2018-08-09T03:33:24.534036: step 8290, loss 0.532171.
Test: 2018-08-09T03:33:45.645165: step 8290, loss 0.549161.
Train: 2018-08-09T03:33:48.743402: step 8291, loss 0.672923.
Train: 2018-08-09T03:33:51.882748: step 8292, loss 0.516401.
Train: 2018-08-09T03:33:54.990010: step 8293, loss 0.563311.
Train: 2018-08-09T03:33:58.092258: step 8294, loss 0.578696.
Train: 2018-08-09T03:34:01.205535: step 8295, loss 0.547732.
Train: 2018-08-09T03:34:04.311793: step 8296, loss 0.453718.
Train: 2018-08-09T03:34:07.434095: step 8297, loss 0.594659.
Train: 2018-08-09T03:34:10.552386: step 8298, loss 0.515738.
Train: 2018-08-09T03:34:13.634580: step 8299, loss 0.515785.
Train: 2018-08-09T03:34:16.738834: step 8300, loss 0.579571.
Test: 2018-08-09T03:34:37.751701: step 8300, loss 0.549776.
Train: 2018-08-09T03:34:42.908411: step 8301, loss 0.498747.
Train: 2018-08-09T03:34:46.006649: step 8302, loss 0.514845.
Train: 2018-08-09T03:34:49.106891: step 8303, loss 0.626704.
Train: 2018-08-09T03:34:52.214153: step 8304, loss 0.481852.
Train: 2018-08-09T03:34:55.312390: step 8305, loss 0.529015.
Train: 2018-08-09T03:34:58.407619: step 8306, loss 0.430672.
Train: 2018-08-09T03:35:01.515884: step 8307, loss 0.611791.
Train: 2018-08-09T03:35:04.584041: step 8308, loss 0.54614.
Train: 2018-08-09T03:35:07.664230: step 8309, loss 0.52401.
Train: 2018-08-09T03:35:10.743417: step 8310, loss 0.596348.
Test: 2018-08-09T03:35:31.771324: step 8310, loss 0.547786.
Train: 2018-08-09T03:35:34.905657: step 8311, loss 0.543416.
Train: 2018-08-09T03:35:38.010913: step 8312, loss 0.612411.
Train: 2018-08-09T03:35:41.074058: step 8313, loss 0.565992.
Train: 2018-08-09T03:35:44.167282: step 8314, loss 0.661163.
Train: 2018-08-09T03:35:47.276548: step 8315, loss 0.503214.
Train: 2018-08-09T03:35:50.380802: step 8316, loss 0.540895.
Train: 2018-08-09T03:35:53.528170: step 8317, loss 0.652325.
Train: 2018-08-09T03:35:56.627410: step 8318, loss 0.597344.
Train: 2018-08-09T03:35:59.708602: step 8319, loss 0.529198.
Train: 2018-08-09T03:36:02.783778: step 8320, loss 0.514306.
Test: 2018-08-09T03:36:23.805670: step 8320, loss 0.550355.
Train: 2018-08-09T03:36:26.920952: step 8321, loss 0.561922.
Train: 2018-08-09T03:36:29.989109: step 8322, loss 0.432284.
Train: 2018-08-09T03:36:33.061277: step 8323, loss 0.43253.
Train: 2018-08-09T03:36:36.129435: step 8324, loss 0.596697.
Train: 2018-08-09T03:36:39.224664: step 8325, loss 0.463297.
Train: 2018-08-09T03:36:42.308864: step 8326, loss 0.529145.
Train: 2018-08-09T03:36:45.426152: step 8327, loss 0.528864.
Train: 2018-08-09T03:36:48.536421: step 8328, loss 0.613284.
Train: 2018-08-09T03:36:51.625635: step 8329, loss 0.563119.
Train: 2018-08-09T03:36:54.735904: step 8330, loss 0.545602.
Test: 2018-08-09T03:37:15.797903: step 8330, loss 0.549727.
Train: 2018-08-09T03:37:18.917196: step 8331, loss 0.528083.
Train: 2018-08-09T03:37:22.014430: step 8332, loss 0.630262.
Train: 2018-08-09T03:37:25.089606: step 8333, loss 0.511686.
Train: 2018-08-09T03:37:28.208900: step 8334, loss 0.562651.
Train: 2018-08-09T03:37:31.304129: step 8335, loss 0.579091.
Train: 2018-08-09T03:37:34.429438: step 8336, loss 0.544996.
Train: 2018-08-09T03:37:37.518651: step 8337, loss 0.477511.
Train: 2018-08-09T03:37:40.615886: step 8338, loss 0.613434.
Train: 2018-08-09T03:37:43.719137: step 8339, loss 0.597348.
Train: 2018-08-09T03:37:46.824393: step 8340, loss 0.562248.
Test: 2018-08-09T03:38:07.888397: step 8340, loss 0.550916.
Train: 2018-08-09T03:38:11.015711: step 8341, loss 0.545605.
Train: 2018-08-09T03:38:14.138012: step 8342, loss 0.528278.
Train: 2018-08-09T03:38:17.230234: step 8343, loss 0.545282.
Train: 2018-08-09T03:38:20.331479: step 8344, loss 0.511125.
Train: 2018-08-09T03:38:23.391615: step 8345, loss 0.59637.
Train: 2018-08-09T03:38:26.507900: step 8346, loss 0.528377.
Train: 2018-08-09T03:38:29.586085: step 8347, loss 0.528111.
Train: 2018-08-09T03:38:32.697357: step 8348, loss 0.562573.
Train: 2018-08-09T03:38:35.781556: step 8349, loss 0.511083.
Train: 2018-08-09T03:38:38.869767: step 8350, loss 0.579625.
Test: 2018-08-09T03:38:59.943798: step 8350, loss 0.54697.
Train: 2018-08-09T03:39:03.039027: step 8351, loss 0.613885.
Train: 2018-08-09T03:39:06.144283: step 8352, loss 0.562348.
Train: 2018-08-09T03:39:09.251544: step 8353, loss 0.52812.
Train: 2018-08-09T03:39:12.359808: step 8354, loss 0.528198.
Train: 2018-08-09T03:39:15.472083: step 8355, loss 0.596173.
Train: 2018-08-09T03:39:18.559291: step 8356, loss 0.460069.
Train: 2018-08-09T03:39:21.627448: step 8357, loss 0.49391.
Train: 2018-08-09T03:39:24.717664: step 8358, loss 0.561772.
Train: 2018-08-09T03:39:27.792840: step 8359, loss 0.579078.
Train: 2018-08-09T03:39:30.875035: step 8360, loss 0.545527.
Test: 2018-08-09T03:39:51.951071: step 8360, loss 0.547433.
Train: 2018-08-09T03:39:55.019228: step 8361, loss 0.492736.
Train: 2018-08-09T03:39:58.100420: step 8362, loss 0.544221.
Train: 2018-08-09T03:40:01.198657: step 8363, loss 0.563074.
Train: 2018-08-09T03:40:04.295892: step 8364, loss 0.647858.
Train: 2018-08-09T03:40:07.391121: step 8365, loss 0.5796.
Train: 2018-08-09T03:40:10.476324: step 8366, loss 0.579724.
Train: 2018-08-09T03:40:13.596620: step 8367, loss 0.667153.
Train: 2018-08-09T03:40:16.704884: step 8368, loss 0.614344.
Train: 2018-08-09T03:40:19.798108: step 8369, loss 0.528365.
Train: 2018-08-09T03:40:22.897348: step 8370, loss 0.595241.
Test: 2018-08-09T03:40:43.954333: step 8370, loss 0.549143.
Train: 2018-08-09T03:40:47.056581: step 8371, loss 0.595666.
Train: 2018-08-09T03:40:50.165847: step 8372, loss 0.577022.
Train: 2018-08-09T03:40:53.278122: step 8373, loss 0.527408.
Train: 2018-08-09T03:40:56.374354: step 8374, loss 0.514055.
Train: 2018-08-09T03:40:59.470586: step 8375, loss 0.563179.
Train: 2018-08-09T03:41:02.575842: step 8376, loss 0.529573.
Train: 2018-08-09T03:41:05.694133: step 8377, loss 0.690632.
Train: 2018-08-09T03:41:08.803399: step 8378, loss 0.497414.
Train: 2018-08-09T03:41:11.907653: step 8379, loss 0.530383.
Train: 2018-08-09T03:41:15.001879: step 8380, loss 0.531509.
Test: 2018-08-09T03:41:36.055856: step 8380, loss 0.549472.
Train: 2018-08-09T03:41:39.142061: step 8381, loss 0.640609.
Train: 2018-08-09T03:41:42.212224: step 8382, loss 0.543285.
Train: 2018-08-09T03:41:45.320489: step 8383, loss 0.594652.
Train: 2018-08-09T03:41:48.418726: step 8384, loss 0.548019.
Train: 2018-08-09T03:41:51.543032: step 8385, loss 0.513828.
Train: 2018-08-09T03:41:54.607179: step 8386, loss 0.561921.
Train: 2018-08-09T03:41:57.713438: step 8387, loss 0.593712.
Train: 2018-08-09T03:42:00.805659: step 8388, loss 0.548562.
Train: 2018-08-09T03:42:03.929966: step 8389, loss 0.496018.
Train: 2018-08-09T03:42:07.033216: step 8390, loss 0.629438.
Test: 2018-08-09T03:42:28.057114: step 8390, loss 0.551497.
Train: 2018-08-09T03:42:31.143318: step 8391, loss 0.482909.
Train: 2018-08-09T03:42:34.272638: step 8392, loss 0.484435.
Train: 2018-08-09T03:42:37.378897: step 8393, loss 0.594657.
Train: 2018-08-09T03:42:40.462094: step 8394, loss 0.625331.
Train: 2018-08-09T03:42:43.541281: step 8395, loss 0.483286.
Train: 2018-08-09T03:42:46.633503: step 8396, loss 0.642926.
Train: 2018-08-09T03:42:49.734748: step 8397, loss 0.659959.
Train: 2018-08-09T03:42:52.852036: step 8398, loss 0.497302.
Train: 2018-08-09T03:42:55.928215: step 8399, loss 0.644563.
Train: 2018-08-09T03:42:59.039487: step 8400, loss 0.561848.
Test: 2018-08-09T03:43:20.057368: step 8400, loss 0.546801.
Train: 2018-08-09T03:43:24.863145: step 8401, loss 0.57719.
Train: 2018-08-09T03:43:27.930299: step 8402, loss 0.481311.
Train: 2018-08-09T03:43:31.016505: step 8403, loss 0.529439.
Train: 2018-08-09T03:43:34.128779: step 8404, loss 0.595115.
Train: 2018-08-09T03:43:37.215987: step 8405, loss 0.596461.
Train: 2018-08-09T03:43:40.309211: step 8406, loss 0.482962.
Train: 2018-08-09T03:43:43.379374: step 8407, loss 0.546531.
Train: 2018-08-09T03:43:46.472598: step 8408, loss 0.595532.
Train: 2018-08-09T03:43:49.585876: step 8409, loss 0.562614.
Train: 2018-08-09T03:43:52.685115: step 8410, loss 0.54529.
Test: 2018-08-09T03:44:13.765161: step 8410, loss 0.548896.
Train: 2018-08-09T03:44:16.850364: step 8411, loss 0.496382.
Train: 2018-08-09T03:44:19.946596: step 8412, loss 0.530084.
Train: 2018-08-09T03:44:23.033804: step 8413, loss 0.695573.
Train: 2018-08-09T03:44:26.127028: step 8414, loss 0.645492.
Train: 2018-08-09T03:44:29.210225: step 8415, loss 0.562653.
Train: 2018-08-09T03:44:32.303450: step 8416, loss 0.579518.
Train: 2018-08-09T03:44:35.425751: step 8417, loss 0.595613.
Train: 2018-08-09T03:44:38.507946: step 8418, loss 0.546016.
Train: 2018-08-09T03:44:41.597159: step 8419, loss 0.628508.
Train: 2018-08-09T03:44:44.657295: step 8420, loss 0.578265.
Test: 2018-08-09T03:45:05.706259: step 8420, loss 0.547791.
Train: 2018-08-09T03:45:08.789456: step 8421, loss 0.480958.
Train: 2018-08-09T03:45:11.890701: step 8422, loss 0.660326.
Train: 2018-08-09T03:45:14.974901: step 8423, loss 0.562962.
Train: 2018-08-09T03:45:18.035037: step 8424, loss 0.628266.
Train: 2018-08-09T03:45:21.121243: step 8425, loss 0.546568.
Train: 2018-08-09T03:45:24.198424: step 8426, loss 0.627638.
Train: 2018-08-09T03:45:27.283627: step 8427, loss 0.530589.
Train: 2018-08-09T03:45:30.197374: step 8428, loss 0.562711.
Train: 2018-08-09T03:45:33.262523: step 8429, loss 0.499156.
Train: 2018-08-09T03:45:36.325667: step 8430, loss 0.498909.
Test: 2018-08-09T03:45:57.378642: step 8430, loss 0.547877.
Train: 2018-08-09T03:46:00.477881: step 8431, loss 0.610857.
Train: 2018-08-09T03:46:03.580129: step 8432, loss 0.499265.
Train: 2018-08-09T03:46:06.667337: step 8433, loss 0.562934.
Train: 2018-08-09T03:46:09.762566: step 8434, loss 0.498872.
Train: 2018-08-09T03:46:12.849775: step 8435, loss 0.56288.
Train: 2018-08-09T03:46:15.927959: step 8436, loss 0.53077.
Train: 2018-08-09T03:46:19.021183: step 8437, loss 0.578955.
Train: 2018-08-09T03:46:22.124434: step 8438, loss 0.466077.
Train: 2018-08-09T03:46:25.218660: step 8439, loss 0.514205.
Train: 2018-08-09T03:46:28.335948: step 8440, loss 0.676366.
Test: 2018-08-09T03:46:49.424016: step 8440, loss 0.547433.
Train: 2018-08-09T03:46:52.508215: step 8441, loss 0.578891.
Train: 2018-08-09T03:46:55.628511: step 8442, loss 0.416243.
Train: 2018-08-09T03:46:58.717725: step 8443, loss 0.611571.
Train: 2018-08-09T03:47:01.776858: step 8444, loss 0.464416.
Train: 2018-08-09T03:47:04.897154: step 8445, loss 0.562394.
Train: 2018-08-09T03:47:08.008426: step 8446, loss 0.529513.
Train: 2018-08-09T03:47:11.101650: step 8447, loss 0.59551.
Train: 2018-08-09T03:47:14.197882: step 8448, loss 0.56252.
Train: 2018-08-09T03:47:17.257016: step 8449, loss 0.545649.
Train: 2018-08-09T03:47:20.360266: step 8450, loss 0.462249.
Test: 2018-08-09T03:47:41.400206: step 8450, loss 0.550473.
Train: 2018-08-09T03:47:44.503456: step 8451, loss 0.561656.
Train: 2018-08-09T03:47:47.617736: step 8452, loss 0.511667.
Train: 2018-08-09T03:47:50.731014: step 8453, loss 0.612619.
Train: 2018-08-09T03:47:53.818222: step 8454, loss 0.562836.
Train: 2018-08-09T03:47:56.961579: step 8455, loss 0.476414.
Train: 2018-08-09T03:48:00.040766: step 8456, loss 0.68321.
Train: 2018-08-09T03:48:03.120955: step 8457, loss 0.596868.
Train: 2018-08-09T03:48:06.225208: step 8458, loss 0.563082.
Train: 2018-08-09T03:48:09.306400: step 8459, loss 0.545043.
Train: 2018-08-09T03:48:12.385587: step 8460, loss 0.4415.
Test: 2018-08-09T03:48:33.437559: step 8460, loss 0.546891.
Train: 2018-08-09T03:48:36.559860: step 8461, loss 0.596861.
Train: 2018-08-09T03:48:39.709233: step 8462, loss 0.527693.
Train: 2018-08-09T03:48:42.808473: step 8463, loss 0.580325.
Train: 2018-08-09T03:48:45.906711: step 8464, loss 0.579264.
Train: 2018-08-09T03:48:49.007956: step 8465, loss 0.631119.
Train: 2018-08-09T03:48:52.109201: step 8466, loss 0.510591.
Train: 2018-08-09T03:48:55.184377: step 8467, loss 0.613234.
Train: 2018-08-09T03:48:58.283617: step 8468, loss 0.59709.
Train: 2018-08-09T03:49:01.374836: step 8469, loss 0.630426.
Train: 2018-08-09T03:49:04.491121: step 8470, loss 0.545516.
Test: 2018-08-09T03:49:25.544096: step 8470, loss 0.547841.
Train: 2018-08-09T03:49:28.632307: step 8471, loss 0.579046.
Train: 2018-08-09T03:49:31.726533: step 8472, loss 0.663721.
Train: 2018-08-09T03:49:34.831789: step 8473, loss 0.529316.
Train: 2018-08-09T03:49:37.921002: step 8474, loss 0.512593.
Train: 2018-08-09T03:49:41.005202: step 8475, loss 0.628966.
Train: 2018-08-09T03:49:44.135525: step 8476, loss 0.595556.
Train: 2018-08-09T03:49:47.222733: step 8477, loss 0.644706.
Train: 2018-08-09T03:49:50.290890: step 8478, loss 0.447942.
Train: 2018-08-09T03:49:53.367069: step 8479, loss 0.595444.
Train: 2018-08-09T03:49:56.449264: step 8480, loss 0.497556.
Test: 2018-08-09T03:50:17.501235: step 8480, loss 0.547446.
Train: 2018-08-09T03:50:20.560368: step 8481, loss 0.546511.
Train: 2018-08-09T03:50:23.651587: step 8482, loss 0.530204.
Train: 2018-08-09T03:50:26.736790: step 8483, loss 0.513961.
Train: 2018-08-09T03:50:29.825001: step 8484, loss 0.595136.
Train: 2018-08-09T03:50:32.920230: step 8485, loss 0.465277.
Train: 2018-08-09T03:50:36.022478: step 8486, loss 0.416259.
Train: 2018-08-09T03:50:39.113697: step 8487, loss 0.57895.
Train: 2018-08-09T03:50:42.226974: step 8488, loss 0.579067.
Train: 2018-08-09T03:50:45.313179: step 8489, loss 0.562451.
Train: 2018-08-09T03:50:48.428462: step 8490, loss 0.529391.
Test: 2018-08-09T03:51:09.498482: step 8490, loss 0.546373.
Train: 2018-08-09T03:51:12.615769: step 8491, loss 0.579108.
Train: 2018-08-09T03:51:15.729047: step 8492, loss 0.52912.
Train: 2018-08-09T03:51:18.826281: step 8493, loss 0.479373.
Train: 2018-08-09T03:51:21.908476: step 8494, loss 0.612047.
Train: 2018-08-09T03:51:24.998692: step 8495, loss 0.696421.
Train: 2018-08-09T03:51:28.075874: step 8496, loss 0.561889.
Train: 2018-08-09T03:51:31.148042: step 8497, loss 0.562346.
Train: 2018-08-09T03:51:34.234247: step 8498, loss 0.596281.
Train: 2018-08-09T03:51:37.313434: step 8499, loss 0.562775.
Train: 2018-08-09T03:51:40.370562: step 8500, loss 0.629036.
Test: 2018-08-09T03:52:01.441584: step 8500, loss 0.54753.
Train: 2018-08-09T03:52:06.306518: step 8501, loss 0.545978.
Train: 2018-08-09T03:52:09.404755: step 8502, loss 0.528998.
Train: 2018-08-09T03:52:12.497980: step 8503, loss 0.56276.
Train: 2018-08-09T03:52:15.598222: step 8504, loss 0.52946.
Train: 2018-08-09T03:52:18.684428: step 8505, loss 0.529751.
Train: 2018-08-09T03:52:21.780660: step 8506, loss 0.513217.
Train: 2018-08-09T03:52:24.845809: step 8507, loss 0.694831.
Train: 2018-08-09T03:52:27.929006: step 8508, loss 0.52952.
Train: 2018-08-09T03:52:30.987137: step 8509, loss 0.579036.
Train: 2018-08-09T03:52:34.062313: step 8510, loss 0.512825.
Test: 2018-08-09T03:52:55.097240: step 8510, loss 0.549557.
Train: 2018-08-09T03:52:58.198485: step 8511, loss 0.595834.
Train: 2018-08-09T03:53:01.291709: step 8512, loss 0.496502.
Train: 2018-08-09T03:53:04.362874: step 8513, loss 0.611927.
Train: 2018-08-09T03:53:07.445069: step 8514, loss 0.546049.
Train: 2018-08-09T03:53:10.546314: step 8515, loss 0.644797.
Train: 2018-08-09T03:53:13.620487: step 8516, loss 0.545892.
Train: 2018-08-09T03:53:16.700677: step 8517, loss 0.611912.
Train: 2018-08-09T03:53:19.787885: step 8518, loss 0.545954.
Train: 2018-08-09T03:53:22.854037: step 8519, loss 0.529736.
Train: 2018-08-09T03:53:25.911165: step 8520, loss 0.513589.
Test: 2018-08-09T03:53:46.933057: step 8520, loss 0.551544.
Train: 2018-08-09T03:53:50.064382: step 8521, loss 0.447604.
Train: 2018-08-09T03:53:53.153595: step 8522, loss 0.56247.
Train: 2018-08-09T03:53:56.219747: step 8523, loss 0.546338.
Train: 2018-08-09T03:53:59.295926: step 8524, loss 0.496503.
Train: 2018-08-09T03:54:02.386142: step 8525, loss 0.562484.
Train: 2018-08-09T03:54:05.480369: step 8526, loss 0.429193.
Train: 2018-08-09T03:54:08.558553: step 8527, loss 0.629236.
Train: 2018-08-09T03:54:11.661803: step 8528, loss 0.663318.
Train: 2018-08-09T03:54:14.752019: step 8529, loss 0.529195.
Train: 2018-08-09T03:54:17.847249: step 8530, loss 0.612692.
Test: 2018-08-09T03:54:38.871146: step 8530, loss 0.550437.
Train: 2018-08-09T03:54:41.957351: step 8531, loss 0.529143.
Train: 2018-08-09T03:54:45.032527: step 8532, loss 0.528939.
Train: 2018-08-09T03:54:48.110711: step 8533, loss 0.561412.
Train: 2018-08-09T03:54:51.193908: step 8534, loss 0.57906.
Train: 2018-08-09T03:54:54.248028: step 8535, loss 0.596017.
Train: 2018-08-09T03:54:57.291119: step 8536, loss 0.561953.
Train: 2018-08-09T03:55:00.362284: step 8537, loss 0.512191.
Train: 2018-08-09T03:55:03.435455: step 8538, loss 0.494956.
Train: 2018-08-09T03:55:06.537703: step 8539, loss 0.578761.
Train: 2018-08-09T03:55:09.602853: step 8540, loss 0.613381.
Test: 2018-08-09T03:55:30.597672: step 8540, loss 0.549102.
Train: 2018-08-09T03:55:33.638757: step 8541, loss 0.663587.
Train: 2018-08-09T03:55:36.735992: step 8542, loss 0.764653.
Train: 2018-08-09T03:55:39.829217: step 8543, loss 0.545825.
Train: 2018-08-09T03:55:42.919432: step 8544, loss 0.396565.
Train: 2018-08-09T03:55:46.023685: step 8545, loss 0.462729.
Train: 2018-08-09T03:55:49.111896: step 8546, loss 0.51298.
Train: 2018-08-09T03:55:52.200107: step 8547, loss 0.545486.
Train: 2018-08-09T03:55:55.273277: step 8548, loss 0.579298.
Train: 2018-08-09T03:55:58.363494: step 8549, loss 0.578773.
Train: 2018-08-09T03:56:01.407587: step 8550, loss 0.52845.
Test: 2018-08-09T03:56:22.442515: step 8550, loss 0.549991.
Train: 2018-08-09T03:56:25.553785: step 8551, loss 0.579547.
Train: 2018-08-09T03:56:28.633975: step 8552, loss 0.495561.
Train: 2018-08-09T03:56:31.719177: step 8553, loss 0.595831.
Train: 2018-08-09T03:56:34.784326: step 8554, loss 0.512393.
Train: 2018-08-09T03:56:37.871535: step 8555, loss 0.528585.
Train: 2018-08-09T03:56:40.927660: step 8556, loss 0.562239.
Train: 2018-08-09T03:56:44.021887: step 8557, loss 0.645352.
Train: 2018-08-09T03:56:47.111100: step 8558, loss 0.529283.
Train: 2018-08-09T03:56:50.218361: step 8559, loss 0.460848.
Train: 2018-08-09T03:56:53.278497: step 8560, loss 0.664613.
Test: 2018-08-09T03:57:14.316432: step 8560, loss 0.549094.
Train: 2018-08-09T03:57:17.390605: step 8561, loss 0.561458.
Train: 2018-08-09T03:57:20.487840: step 8562, loss 0.411457.
Train: 2018-08-09T03:57:23.604125: step 8563, loss 0.461038.
Train: 2018-08-09T03:57:26.689328: step 8564, loss 0.581674.
Train: 2018-08-09T03:57:29.779544: step 8565, loss 0.613114.
Train: 2018-08-09T03:57:32.855722: step 8566, loss 0.442463.
Train: 2018-08-09T03:57:35.905832: step 8567, loss 0.595924.
Train: 2018-08-09T03:57:39.020112: step 8568, loss 0.510562.
Train: 2018-08-09T03:57:42.105314: step 8569, loss 0.492219.
Train: 2018-08-09T03:57:45.163445: step 8570, loss 0.475831.
Test: 2018-08-09T03:58:06.228452: step 8570, loss 0.549992.
Train: 2018-08-09T03:58:09.322678: step 8571, loss 0.632948.
Train: 2018-08-09T03:58:12.395849: step 8572, loss 0.528718.
Train: 2018-08-09T03:58:15.527174: step 8573, loss 0.582648.
Train: 2018-08-09T03:58:18.612386: step 8574, loss 0.529414.
Train: 2018-08-09T03:58:21.683542: step 8575, loss 0.614069.
Train: 2018-08-09T03:58:24.752702: step 8576, loss 0.579911.
Train: 2018-08-09T03:58:27.842918: step 8577, loss 0.596713.
Train: 2018-08-09T03:58:30.938147: step 8578, loss 0.631803.
Train: 2018-08-09T03:58:33.993270: step 8579, loss 0.597151.
Train: 2018-08-09T03:58:37.075465: step 8580, loss 0.528127.
Test: 2018-08-09T03:58:58.113400: step 8580, loss 0.548197.
Train: 2018-08-09T03:59:01.203615: step 8581, loss 0.614586.
Train: 2018-08-09T03:59:04.274780: step 8582, loss 0.51092.
Train: 2018-08-09T03:59:07.316868: step 8583, loss 0.392236.
Train: 2018-08-09T03:59:10.384023: step 8584, loss 0.579849.
Train: 2018-08-09T03:59:13.481258: step 8585, loss 0.545138.
Train: 2018-08-09T03:59:16.559442: step 8586, loss 0.511391.
Train: 2018-08-09T03:59:19.627599: step 8587, loss 0.528439.
Train: 2018-08-09T03:59:22.721826: step 8588, loss 0.528412.
Train: 2018-08-09T03:59:25.764917: step 8589, loss 0.528238.
Train: 2018-08-09T03:59:28.840093: step 8590, loss 0.596461.
Test: 2018-08-09T03:59:49.894070: step 8590, loss 0.548279.
Train: 2018-08-09T03:59:52.968243: step 8591, loss 0.54532.
Train: 2018-08-09T03:59:56.090544: step 8592, loss 0.562503.
Train: 2018-08-09T03:59:59.163715: step 8593, loss 0.61348.
Train: 2018-08-09T04:00:02.297045: step 8594, loss 0.613615.
Train: 2018-08-09T04:00:05.379240: step 8595, loss 0.579409.
Train: 2018-08-09T04:00:08.438373: step 8596, loss 0.579369.
Train: 2018-08-09T04:00:11.508536: step 8597, loss 0.596475.
Train: 2018-08-09T04:00:14.618805: step 8598, loss 0.630014.
Train: 2018-08-09T04:00:17.676936: step 8599, loss 0.596155.
Train: 2018-08-09T04:00:20.771163: step 8600, loss 0.511877.
Test: 2018-08-09T04:00:41.808095: step 8600, loss 0.547929.
Train: 2018-08-09T04:00:46.677040: step 8601, loss 0.511996.
Train: 2018-08-09T04:00:49.779288: step 8602, loss 0.528842.
Train: 2018-08-09T04:00:52.850453: step 8603, loss 0.545616.
Train: 2018-08-09T04:00:55.926632: step 8604, loss 0.495514.
Train: 2018-08-09T04:00:59.008826: step 8605, loss 0.512264.
Train: 2018-08-09T04:01:02.084002: step 8606, loss 0.579264.
Train: 2018-08-09T04:01:05.154165: step 8607, loss 0.646144.
Train: 2018-08-09T04:01:08.222322: step 8608, loss 0.629418.
Train: 2018-08-09T04:01:11.316549: step 8609, loss 0.512348.
Train: 2018-08-09T04:01:14.387715: step 8610, loss 0.695899.
Test: 2018-08-09T04:01:35.393564: step 8610, loss 0.550637.
Train: 2018-08-09T04:01:38.571011: step 8611, loss 0.579106.
Train: 2018-08-09T04:01:41.661227: step 8612, loss 0.463029.
Train: 2018-08-09T04:01:44.735401: step 8613, loss 0.595482.
Train: 2018-08-09T04:01:47.806566: step 8614, loss 0.57905.
Train: 2018-08-09T04:01:50.865699: step 8615, loss 0.496574.
Train: 2018-08-09T04:01:53.945889: step 8616, loss 0.595472.
Train: 2018-08-09T04:01:57.077214: step 8617, loss 0.611903.
Train: 2018-08-09T04:02:00.136348: step 8618, loss 0.578997.
Train: 2018-08-09T04:02:03.228569: step 8619, loss 0.44789.
Train: 2018-08-09T04:02:06.280684: step 8620, loss 0.562583.
Test: 2018-08-09T04:02:27.324634: step 8620, loss 0.54722.
Train: 2018-08-09T04:02:30.408834: step 8621, loss 0.546183.
Train: 2018-08-09T04:02:33.493034: step 8622, loss 0.513403.
Train: 2018-08-09T04:02:36.568210: step 8623, loss 0.480538.
Train: 2018-08-09T04:02:39.634362: step 8624, loss 0.529657.
Train: 2018-08-09T04:02:42.747639: step 8625, loss 0.49658.
Train: 2018-08-09T04:02:45.850890: step 8626, loss 0.6121.
Train: 2018-08-09T04:02:48.925063: step 8627, loss 0.545902.
Train: 2018-08-09T04:02:51.985199: step 8628, loss 0.595693.
Train: 2018-08-09T04:02:55.081431: step 8629, loss 0.512554.
Train: 2018-08-09T04:02:58.186687: step 8630, loss 0.579073.
Test: 2018-08-09T04:03:19.242670: step 8630, loss 0.547424.
Train: 2018-08-09T04:03:22.352939: step 8631, loss 0.595794.
Train: 2018-08-09T04:03:25.481256: step 8632, loss 0.529006.
Train: 2018-08-09T04:03:28.565456: step 8633, loss 0.562414.
Train: 2018-08-09T04:03:31.662691: step 8634, loss 0.579116.
Train: 2018-08-09T04:03:34.754912: step 8635, loss 0.545663.
Train: 2018-08-09T04:03:37.853150: step 8636, loss 0.579199.
Train: 2018-08-09T04:03:40.983472: step 8637, loss 0.495468.
Train: 2018-08-09T04:03:44.069677: step 8638, loss 0.595963.
Train: 2018-08-09T04:03:47.106752: step 8639, loss 0.612682.
Train: 2018-08-09T04:03:50.191955: step 8640, loss 0.679681.
Test: 2018-08-09T04:04:11.257965: step 8640, loss 0.551816.
Train: 2018-08-09T04:04:14.347177: step 8641, loss 0.562385.
Train: 2018-08-09T04:04:17.473489: step 8642, loss 0.645762.
Train: 2018-08-09T04:04:20.549668: step 8643, loss 0.662096.
Train: 2018-08-09T04:04:23.598775: step 8644, loss 0.562503.
Train: 2018-08-09T04:04:26.715060: step 8645, loss 0.628345.
Train: 2018-08-09T04:04:29.791239: step 8646, loss 0.578946.
Train: 2018-08-09T04:04:32.890478: step 8647, loss 0.595215.
Train: 2018-08-09T04:04:35.951617: step 8648, loss 0.611316.
Train: 2018-08-09T04:04:39.041834: step 8649, loss 0.514375.
Train: 2018-08-09T04:04:42.139068: step 8650, loss 0.675246.
Test: 2018-08-09T04:05:03.196053: step 8650, loss 0.550936.
Train: 2018-08-09T04:05:06.273234: step 8651, loss 0.530934.
Train: 2018-08-09T04:05:09.348410: step 8652, loss 0.54704.
Train: 2018-08-09T04:05:12.439629: step 8653, loss 0.578857.
Train: 2018-08-09T04:05:15.531850: step 8654, loss 0.563058.
Train: 2018-08-09T04:05:18.612039: step 8655, loss 0.594618.
Train: 2018-08-09T04:05:21.677189: step 8656, loss 0.610294.
Train: 2018-08-09T04:05:24.749357: step 8657, loss 0.672809.
Train: 2018-08-09T04:05:27.841578: step 8658, loss 0.563316.
Train: 2018-08-09T04:05:30.927784: step 8659, loss 0.640958.
Train: 2018-08-09T04:05:34.067130: step 8660, loss 0.455498.
Test: 2018-08-09T04:05:55.139155: step 8660, loss 0.551469.
Train: 2018-08-09T04:05:58.240400: step 8661, loss 0.501984.
Train: 2018-08-09T04:06:01.322595: step 8662, loss 0.548194.
Train: 2018-08-09T04:06:04.422838: step 8663, loss 0.609731.
Train: 2018-08-09T04:06:07.485982: step 8664, loss 0.62506.
Train: 2018-08-09T04:06:10.558150: step 8665, loss 0.548297.
Train: 2018-08-09T04:06:13.637337: step 8666, loss 0.502355.
Train: 2018-08-09T04:06:16.725547: step 8667, loss 0.624992.
Train: 2018-08-09T04:06:19.831806: step 8668, loss 0.548327.
Train: 2018-08-09T04:06:22.917009: step 8669, loss 0.609646.
Train: 2018-08-09T04:06:25.971129: step 8670, loss 0.640277.
Test: 2018-08-09T04:06:47.076242: step 8670, loss 0.551684.
Train: 2018-08-09T04:06:50.182500: step 8671, loss 0.517813.
Train: 2018-08-09T04:06:53.273719: step 8672, loss 0.655455.
Train: 2018-08-09T04:06:56.371956: step 8673, loss 0.563754.
Train: 2018-08-09T04:06:59.440113: step 8674, loss 0.624751.
Train: 2018-08-09T04:07:02.515290: step 8675, loss 0.563837.
Train: 2018-08-09T04:07:05.596482: step 8676, loss 0.518327.
Train: 2018-08-09T04:07:08.679679: step 8677, loss 0.548708.
Train: 2018-08-09T04:07:11.753852: step 8678, loss 0.57906.
Train: 2018-08-09T04:07:14.834042: step 8679, loss 0.639794.
Train: 2018-08-09T04:07:17.908215: step 8680, loss 0.594236.
Test: 2018-08-09T04:07:38.940133: step 8680, loss 0.551451.
Train: 2018-08-09T04:07:42.023330: step 8681, loss 0.548777.
Train: 2018-08-09T04:07:45.098507: step 8682, loss 0.609369.
Train: 2018-08-09T04:07:48.204765: step 8683, loss 0.548838.
Train: 2018-08-09T04:07:51.277936: step 8684, loss 0.624461.
Train: 2018-08-09T04:07:54.366147: step 8685, loss 0.579108.
Train: 2018-08-09T04:07:57.447338: step 8686, loss 0.56403.
Train: 2018-08-09T04:08:00.544573: step 8687, loss 0.54897.
Train: 2018-08-09T04:08:03.620752: step 8688, loss 0.669592.
Train: 2018-08-09T04:08:06.674872: step 8689, loss 0.549039.
Train: 2018-08-09T04:08:09.755061: step 8690, loss 0.549079.
Test: 2018-08-09T04:08:30.785977: step 8690, loss 0.553448.
Train: 2018-08-09T04:08:33.848118: step 8691, loss 0.54909.
Train: 2018-08-09T04:08:36.972425: step 8692, loss 0.549075.
Train: 2018-08-09T04:08:40.028550: step 8693, loss 0.699558.
Train: 2018-08-09T04:08:43.137817: step 8694, loss 0.669336.
Train: 2018-08-09T04:08:46.218006: step 8695, loss 0.594173.
Train: 2018-08-09T04:08:49.300201: step 8696, loss 0.564284.
Train: 2018-08-09T04:08:52.361340: step 8697, loss 0.504735.
Train: 2018-08-09T04:08:55.457572: step 8698, loss 0.549471.
Train: 2018-08-09T04:08:58.562828: step 8699, loss 0.594153.
Train: 2018-08-09T04:09:01.617951: step 8700, loss 0.653724.
Test: 2018-08-09T04:09:22.756152: step 8700, loss 0.55385.
Train: 2018-08-09T04:09:27.591006: step 8701, loss 0.5198.
Train: 2018-08-09T04:09:30.682224: step 8702, loss 0.57928.
Train: 2018-08-09T04:09:33.778456: step 8703, loss 0.534683.
Train: 2018-08-09T04:09:36.885718: step 8704, loss 0.519753.
Train: 2018-08-09T04:09:39.950867: step 8705, loss 0.474879.
Train: 2018-08-09T04:09:43.004987: step 8706, loss 0.594169.
Train: 2018-08-09T04:09:46.095203: step 8707, loss 0.519096.
Train: 2018-08-09T04:09:49.177398: step 8708, loss 0.518806.
Train: 2018-08-09T04:09:52.238537: step 8709, loss 0.548771.
Train: 2018-08-09T04:09:55.316721: step 8710, loss 0.624718.
Test: 2018-08-09T04:10:16.325578: step 8710, loss 0.549985.
Train: 2018-08-09T04:10:19.398748: step 8711, loss 0.624852.
Train: 2018-08-09T04:10:22.487962: step 8712, loss 0.548358.
Train: 2018-08-09T04:10:25.570156: step 8713, loss 0.609684.
Train: 2018-08-09T04:10:28.685439: step 8714, loss 0.517437.
Train: 2018-08-09T04:10:31.779665: step 8715, loss 0.548104.
Train: 2018-08-09T04:10:34.853839: step 8716, loss 0.578931.
Train: 2018-08-09T04:10:37.957090: step 8717, loss 0.594421.
Train: 2018-08-09T04:10:41.032266: step 8718, loss 0.563379.
Train: 2018-08-09T04:10:44.117468: step 8719, loss 0.656697.
Train: 2018-08-09T04:10:47.185626: step 8720, loss 0.501098.
Test: 2018-08-09T04:11:08.220552: step 8720, loss 0.551659.
Train: 2018-08-09T04:11:11.311771: step 8721, loss 0.563313.
Train: 2018-08-09T04:11:14.402989: step 8722, loss 0.563285.
Train: 2018-08-09T04:11:17.479168: step 8723, loss 0.563258.
Train: 2018-08-09T04:11:20.543315: step 8724, loss 0.469323.
Train: 2018-08-09T04:11:23.599440: step 8725, loss 0.51607.
Train: 2018-08-09T04:11:26.705699: step 8726, loss 0.578865.
Train: 2018-08-09T04:11:29.789899: step 8727, loss 0.499799.
Train: 2018-08-09T04:11:32.873096: step 8728, loss 0.626493.
Train: 2018-08-09T04:11:35.793861: step 8729, loss 0.57992.
Train: 2018-08-09T04:11:38.836952: step 8730, loss 0.642689.
Test: 2018-08-09T04:11:59.898951: step 8730, loss 0.549138.
Train: 2018-08-09T04:12:02.986159: step 8731, loss 0.546924.
Train: 2018-08-09T04:12:06.068353: step 8732, loss 0.546893.
Train: 2018-08-09T04:12:09.167593: step 8733, loss 0.466833.
Train: 2018-08-09T04:12:12.265830: step 8734, loss 0.627018.
Train: 2018-08-09T04:12:15.367076: step 8735, loss 0.627091.
Train: 2018-08-09T04:12:18.450273: step 8736, loss 0.482408.
Train: 2018-08-09T04:12:21.541492: step 8737, loss 0.546671.
Train: 2018-08-09T04:12:24.617671: step 8738, loss 0.562743.
Train: 2018-08-09T04:12:27.706884: step 8739, loss 0.514232.
Train: 2018-08-09T04:12:30.791084: step 8740, loss 0.562691.
Test: 2018-08-09T04:12:51.841051: step 8740, loss 0.548687.
Train: 2018-08-09T04:12:54.938285: step 8741, loss 0.546435.
Train: 2018-08-09T04:12:58.000426: step 8742, loss 0.497561.
Train: 2018-08-09T04:13:01.072594: step 8743, loss 0.52997.
Train: 2018-08-09T04:13:04.126714: step 8744, loss 0.5462.
Train: 2018-08-09T04:13:07.189858: step 8745, loss 0.513261.
Train: 2018-08-09T04:13:10.260021: step 8746, loss 0.546025.
Train: 2018-08-09T04:13:13.348232: step 8747, loss 0.545934.
Train: 2018-08-09T04:13:16.406362: step 8748, loss 0.628874.
Train: 2018-08-09T04:13:19.532674: step 8749, loss 0.529172.
Train: 2018-08-09T04:13:22.579776: step 8750, loss 0.579101.
Test: 2018-08-09T04:13:43.601668: step 8750, loss 0.546773.
Train: 2018-08-09T04:13:46.667819: step 8751, loss 0.512324.
Train: 2018-08-09T04:13:49.750014: step 8752, loss 0.579141.
Train: 2018-08-09T04:13:52.823185: step 8753, loss 0.528877.
Train: 2018-08-09T04:13:55.926435: step 8754, loss 0.595972.
Train: 2018-08-09T04:13:59.013644: step 8755, loss 0.49515.
Train: 2018-08-09T04:14:02.090825: step 8756, loss 0.579216.
Train: 2018-08-09T04:14:05.167004: step 8757, loss 0.461201.
Train: 2018-08-09T04:14:08.269252: step 8758, loss 0.494738.
Train: 2018-08-09T04:14:11.343425: step 8759, loss 0.54539.
Train: 2018-08-09T04:14:14.409577: step 8760, loss 0.494293.
Test: 2018-08-09T04:14:35.450520: step 8760, loss 0.548277.
Train: 2018-08-09T04:14:38.532714: step 8761, loss 0.579412.
Train: 2018-08-09T04:14:41.610898: step 8762, loss 0.647949.
Train: 2018-08-09T04:14:44.706127: step 8763, loss 0.613759.
Train: 2018-08-09T04:14:47.829431: step 8764, loss 0.545193.
Train: 2018-08-09T04:14:50.932682: step 8765, loss 0.648052.
Train: 2018-08-09T04:14:54.022898: step 8766, loss 0.596575.
Train: 2018-08-09T04:14:57.107098: step 8767, loss 0.459818.
Train: 2018-08-09T04:15:00.237420: step 8768, loss 0.545257.
Train: 2018-08-09T04:15:03.339668: step 8769, loss 0.494031.
Train: 2018-08-09T04:15:06.404818: step 8770, loss 0.613602.
Test: 2018-08-09T04:15:27.420694: step 8770, loss 0.54827.
Train: 2018-08-09T04:15:30.511912: step 8771, loss 0.545255.
Train: 2018-08-09T04:15:33.593104: step 8772, loss 0.579417.
Train: 2018-08-09T04:15:36.683321: step 8773, loss 0.528201.
Train: 2018-08-09T04:15:39.749472: step 8774, loss 0.66473.
Train: 2018-08-09T04:15:42.809608: step 8775, loss 0.579375.
Train: 2018-08-09T04:15:45.929904: step 8776, loss 0.494359.
Train: 2018-08-09T04:15:49.017112: step 8777, loss 0.613286.
Train: 2018-08-09T04:15:52.083264: step 8778, loss 0.545405.
Train: 2018-08-09T04:15:55.149416: step 8779, loss 0.494669.
Train: 2018-08-09T04:15:58.222587: step 8780, loss 0.477788.
Test: 2018-08-09T04:16:19.279572: step 8780, loss 0.548417.
Train: 2018-08-09T04:16:22.347729: step 8781, loss 0.494649.
Train: 2018-08-09T04:16:25.424910: step 8782, loss 0.613214.
Train: 2018-08-09T04:16:28.465996: step 8783, loss 0.545388.
Train: 2018-08-09T04:16:31.572254: step 8784, loss 0.579322.
Train: 2018-08-09T04:16:34.648433: step 8785, loss 0.545375.
Train: 2018-08-09T04:16:37.742660: step 8786, loss 0.681193.
Train: 2018-08-09T04:16:40.819841: step 8787, loss 0.647092.
Train: 2018-08-09T04:16:43.914068: step 8788, loss 0.562363.
Train: 2018-08-09T04:16:46.983228: step 8789, loss 0.579222.
Train: 2018-08-09T04:16:50.112548: step 8790, loss 0.579185.
Test: 2018-08-09T04:17:11.190589: step 8790, loss 0.549244.
Train: 2018-08-09T04:17:14.292837: step 8791, loss 0.495414.
Train: 2018-08-09T04:17:17.399095: step 8792, loss 0.662715.
Train: 2018-08-09T04:17:20.458229: step 8793, loss 0.612419.
Train: 2018-08-09T04:17:23.546440: step 8794, loss 0.579058.
Train: 2018-08-09T04:17:26.615600: step 8795, loss 0.645164.
Train: 2018-08-09T04:17:29.697794: step 8796, loss 0.611896.
Train: 2018-08-09T04:17:32.797034: step 8797, loss 0.52984.
Train: 2018-08-09T04:17:35.883240: step 8798, loss 0.660434.
Train: 2018-08-09T04:17:38.954405: step 8799, loss 0.578901.
Train: 2018-08-09T04:17:42.039608: step 8800, loss 0.49823.
Test: 2018-08-09T04:18:03.151739: step 8800, loss 0.550169.
Train: 2018-08-09T04:18:08.013666: step 8801, loss 0.578872.
Train: 2018-08-09T04:18:11.090847: step 8802, loss 0.482749.
Train: 2018-08-09T04:18:14.190087: step 8803, loss 0.562869.
Train: 2018-08-09T04:18:17.286319: step 8804, loss 0.514975.
Train: 2018-08-09T04:18:20.358487: step 8805, loss 0.546929.
Train: 2018-08-09T04:18:23.427647: step 8806, loss 0.53096.
Train: 2018-08-09T04:18:26.486780: step 8807, loss 0.642772.
Train: 2018-08-09T04:18:29.554938: step 8808, loss 0.594827.
Train: 2018-08-09T04:18:32.647159: step 8809, loss 0.610757.
Train: 2018-08-09T04:18:35.698271: step 8810, loss 0.531096.
Test: 2018-08-09T04:18:56.836472: step 8810, loss 0.54865.
Train: 2018-08-09T04:18:59.921675: step 8811, loss 0.515234.
Train: 2018-08-09T04:19:03.000861: step 8812, loss 0.70612.
Train: 2018-08-09T04:19:06.079045: step 8813, loss 0.547117.
Train: 2018-08-09T04:19:09.147203: step 8814, loss 0.499642.
Train: 2018-08-09T04:19:12.264491: step 8815, loss 0.436314.
Train: 2018-08-09T04:19:15.345683: step 8816, loss 0.562988.
Train: 2018-08-09T04:19:18.447931: step 8817, loss 0.642464.
Train: 2018-08-09T04:19:21.555192: step 8818, loss 0.642493.
Train: 2018-08-09T04:19:24.637387: step 8819, loss 0.610648.
Train: 2018-08-09T04:19:27.725598: step 8820, loss 0.499509.
Test: 2018-08-09T04:19:48.770551: step 8820, loss 0.548724.
Train: 2018-08-09T04:19:51.884830: step 8821, loss 0.562991.
Train: 2018-08-09T04:19:54.967025: step 8822, loss 0.658191.
Train: 2018-08-09T04:19:58.046212: step 8823, loss 0.642223.
Train: 2018-08-09T04:20:01.125398: step 8824, loss 0.610457.
Train: 2018-08-09T04:20:04.196564: step 8825, loss 0.578866.
Train: 2018-08-09T04:20:07.247676: step 8826, loss 0.484674.
Train: 2018-08-09T04:20:10.343908: step 8827, loss 0.578876.
Train: 2018-08-09T04:20:13.442145: step 8828, loss 0.531891.
Train: 2018-08-09T04:20:16.531359: step 8829, loss 0.57888.
Train: 2018-08-09T04:20:19.617564: step 8830, loss 0.547583.
Test: 2018-08-09T04:20:40.658506: step 8830, loss 0.550343.
Train: 2018-08-09T04:20:43.701597: step 8831, loss 0.469332.
Train: 2018-08-09T04:20:46.815877: step 8832, loss 0.578876.
Train: 2018-08-09T04:20:49.928151: step 8833, loss 0.484651.
Train: 2018-08-09T04:20:53.001322: step 8834, loss 0.563115.
Train: 2018-08-09T04:20:56.084519: step 8835, loss 0.594651.
Train: 2018-08-09T04:20:59.140645: step 8836, loss 0.578869.
Train: 2018-08-09T04:21:02.216824: step 8837, loss 0.547158.
Train: 2018-08-09T04:21:05.297013: step 8838, loss 0.626507.
Train: 2018-08-09T04:21:08.384221: step 8839, loss 0.547079.
Train: 2018-08-09T04:21:11.497499: step 8840, loss 0.499322.
Test: 2018-08-09T04:21:32.532425: step 8840, loss 0.5516.
Train: 2018-08-09T04:21:35.601584: step 8841, loss 0.562919.
Train: 2018-08-09T04:21:38.784045: step 8842, loss 0.658719.
Train: 2018-08-09T04:21:41.865238: step 8843, loss 0.562887.
Train: 2018-08-09T04:21:44.928382: step 8844, loss 0.546905.
Train: 2018-08-09T04:21:48.000550: step 8845, loss 0.642811.
Train: 2018-08-09T04:21:51.113827: step 8846, loss 0.578861.
Train: 2018-08-09T04:21:54.197025: step 8847, loss 0.515021.
Train: 2018-08-09T04:21:57.229086: step 8848, loss 0.578861.
Train: 2018-08-09T04:22:00.277190: step 8849, loss 0.515023.
Train: 2018-08-09T04:22:03.339331: step 8850, loss 0.514973.
Test: 2018-08-09T04:22:24.395314: step 8850, loss 0.552715.
Train: 2018-08-09T04:22:27.536666: step 8851, loss 0.514871.
Train: 2018-08-09T04:22:30.665985: step 8852, loss 0.562831.
Train: 2018-08-09T04:22:33.748180: step 8853, loss 0.627084.
Train: 2018-08-09T04:22:36.832380: step 8854, loss 0.546703.
Train: 2018-08-09T04:22:39.932623: step 8855, loss 0.562773.
Train: 2018-08-09T04:22:43.051916: step 8856, loss 0.498275.
Train: 2018-08-09T04:22:46.132105: step 8857, loss 0.530418.
Train: 2018-08-09T04:22:49.215303: step 8858, loss 0.578893.
Train: 2018-08-09T04:22:52.326575: step 8859, loss 0.53022.
Train: 2018-08-09T04:22:55.424812: step 8860, loss 0.53012.
Test: 2018-08-09T04:23:16.472774: step 8860, loss 0.546722.
Train: 2018-08-09T04:23:19.548952: step 8861, loss 0.56262.
Train: 2018-08-09T04:23:22.619115: step 8862, loss 0.758768.
Train: 2018-08-09T04:23:25.687272: step 8863, loss 0.529942.
Train: 2018-08-09T04:23:28.757435: step 8864, loss 0.693188.
Train: 2018-08-09T04:23:31.836621: step 8865, loss 0.481242.
Train: 2018-08-09T04:23:34.931851: step 8866, loss 0.578912.
Train: 2018-08-09T04:23:38.040119: step 8867, loss 0.611395.
Train: 2018-08-09T04:23:41.167429: step 8868, loss 0.514046.
Train: 2018-08-09T04:23:44.252632: step 8869, loss 0.611302.
Train: 2018-08-09T04:23:47.294720: step 8870, loss 0.65977.
Test: 2018-08-09T04:24:08.368751: step 8870, loss 0.55069.
Train: 2018-08-09T04:24:11.455958: step 8871, loss 0.546622.
Train: 2018-08-09T04:24:14.553193: step 8872, loss 0.643233.
Train: 2018-08-09T04:24:17.623355: step 8873, loss 0.594905.
Train: 2018-08-09T04:24:20.722596: step 8874, loss 0.49897.
Train: 2018-08-09T04:24:23.790753: step 8875, loss 0.578866.
Train: 2018-08-09T04:24:26.858910: step 8876, loss 0.499296.
Train: 2018-08-09T04:24:29.925063: step 8877, loss 0.531145.
Train: 2018-08-09T04:24:32.999236: step 8878, loss 0.562966.
Train: 2018-08-09T04:24:36.082433: step 8879, loss 0.515242.
Train: 2018-08-09T04:24:39.151593: step 8880, loss 0.674388.
Test: 2018-08-09T04:25:00.218605: step 8880, loss 0.549253.
Train: 2018-08-09T04:25:03.310826: step 8881, loss 0.451616.
Train: 2018-08-09T04:25:06.397031: step 8882, loss 0.626644.
Train: 2018-08-09T04:25:09.493263: step 8883, loss 0.53108.
Train: 2018-08-09T04:25:12.591500: step 8884, loss 0.562916.
Train: 2018-08-09T04:25:15.671690: step 8885, loss 0.546963.
Train: 2018-08-09T04:25:18.759901: step 8886, loss 0.514993.
Train: 2018-08-09T04:25:21.869167: step 8887, loss 0.482893.
Train: 2018-08-09T04:25:24.952365: step 8888, loss 0.594913.
Train: 2018-08-09T04:25:28.030549: step 8889, loss 0.578874.
Train: 2018-08-09T04:25:31.104722: step 8890, loss 0.482217.
Test: 2018-08-09T04:25:52.145665: step 8890, loss 0.550031.
Train: 2018-08-09T04:25:55.219838: step 8891, loss 0.562728.
Train: 2018-08-09T04:25:58.354171: step 8892, loss 0.627511.
Train: 2018-08-09T04:26:01.442382: step 8893, loss 0.692492.
Train: 2018-08-09T04:26:04.533600: step 8894, loss 0.530262.
Train: 2018-08-09T04:26:07.615795: step 8895, loss 0.627525.
Train: 2018-08-09T04:26:10.684955: step 8896, loss 0.595082.
Train: 2018-08-09T04:26:13.769155: step 8897, loss 0.514242.
Train: 2018-08-09T04:26:16.850347: step 8898, loss 0.772699.
Train: 2018-08-09T04:26:19.900456: step 8899, loss 0.498446.
Train: 2018-08-09T04:26:22.982651: step 8900, loss 0.643048.
Test: 2018-08-09T04:26:44.020586: step 8900, loss 0.548499.
Train: 2018-08-09T04:26:48.875493: step 8901, loss 0.514912.
Train: 2018-08-09T04:26:51.983757: step 8902, loss 0.59481.
Train: 2018-08-09T04:26:55.070965: step 8903, loss 0.578859.
Train: 2018-08-09T04:26:58.156168: step 8904, loss 0.562989.
Train: 2018-08-09T04:27:01.237360: step 8905, loss 0.594694.
Train: 2018-08-09T04:27:04.332589: step 8906, loss 0.547268.
Train: 2018-08-09T04:27:07.383701: step 8907, loss 0.610402.
Train: 2018-08-09T04:27:10.447848: step 8908, loss 0.563136.
Train: 2018-08-09T04:27:13.533051: step 8909, loss 0.437553.
Train: 2018-08-09T04:27:16.598200: step 8910, loss 0.578871.
Test: 2018-08-09T04:27:37.615079: step 8910, loss 0.550802.
Train: 2018-08-09T04:27:40.719332: step 8911, loss 0.453123.
Train: 2018-08-09T04:27:43.791500: step 8912, loss 0.515838.
Train: 2018-08-09T04:27:46.871689: step 8913, loss 0.547248.
Train: 2018-08-09T04:27:49.931825: step 8914, loss 0.499571.
Train: 2018-08-09T04:27:52.973913: step 8915, loss 0.562936.
Train: 2018-08-09T04:27:56.083180: step 8916, loss 0.498951.
Train: 2018-08-09T04:27:59.180415: step 8917, loss 0.675197.
Train: 2018-08-09T04:28:02.256593: step 8918, loss 0.546697.
Train: 2018-08-09T04:28:05.319737: step 8919, loss 0.611135.
Train: 2018-08-09T04:28:08.383884: step 8920, loss 0.562737.
Test: 2018-08-09T04:28:29.421818: step 8920, loss 0.550018.
Train: 2018-08-09T04:28:32.521058: step 8921, loss 0.498051.
Train: 2018-08-09T04:28:35.614282: step 8922, loss 0.530287.
Train: 2018-08-09T04:28:38.690461: step 8923, loss 0.546419.
Train: 2018-08-09T04:28:41.766639: step 8924, loss 0.562635.
Train: 2018-08-09T04:28:44.859863: step 8925, loss 0.595257.
Train: 2018-08-09T04:28:47.983167: step 8926, loss 0.61164.
Train: 2018-08-09T04:28:51.066365: step 8927, loss 0.46444.
Train: 2018-08-09T04:28:54.169616: step 8928, loss 0.496997.
Train: 2018-08-09T04:28:57.224738: step 8929, loss 0.414576.
Train: 2018-08-09T04:29:00.329994: step 8930, loss 0.595539.
Test: 2018-08-09T04:29:21.382969: step 8930, loss 0.548805.
Train: 2018-08-09T04:29:24.440096: step 8931, loss 0.545879.
Train: 2018-08-09T04:29:27.534323: step 8932, loss 0.545791.
Train: 2018-08-09T04:29:30.621531: step 8933, loss 0.645944.
Train: 2018-08-09T04:29:33.705731: step 8934, loss 0.579142.
Train: 2018-08-09T04:29:36.785920: step 8935, loss 0.478625.
Train: 2018-08-09T04:29:39.891177: step 8936, loss 0.478421.
Train: 2018-08-09T04:29:42.974374: step 8937, loss 0.545526.
Train: 2018-08-09T04:29:46.047545: step 8938, loss 0.66376.
Train: 2018-08-09T04:29:49.126731: step 8939, loss 0.579277.
Train: 2018-08-09T04:29:52.203913: step 8940, loss 0.579285.
Test: 2018-08-09T04:30:13.247863: step 8940, loss 0.546494.
Train: 2018-08-09T04:30:16.347103: step 8941, loss 0.562358.
Train: 2018-08-09T04:30:19.434311: step 8942, loss 0.545427.
Train: 2018-08-09T04:30:22.468378: step 8943, loss 0.528492.
Train: 2018-08-09T04:30:25.553580: step 8944, loss 0.562355.
Train: 2018-08-09T04:30:28.646804: step 8945, loss 0.613189.
Train: 2018-08-09T04:30:31.722983: step 8946, loss 0.511554.
Train: 2018-08-09T04:30:34.787130: step 8947, loss 0.579291.
Train: 2018-08-09T04:30:37.855287: step 8948, loss 0.511566.
Train: 2018-08-09T04:30:40.941493: step 8949, loss 0.596227.
Train: 2018-08-09T04:30:44.023687: step 8950, loss 0.663937.
Test: 2018-08-09T04:31:05.065632: step 8950, loss 0.550365.
Train: 2018-08-09T04:31:08.121757: step 8951, loss 0.596156.
Train: 2018-08-09T04:31:11.220997: step 8952, loss 0.528661.
Train: 2018-08-09T04:31:14.311213: step 8953, loss 0.478253.
Train: 2018-08-09T04:31:17.387392: step 8954, loss 0.612836.
Train: 2018-08-09T04:31:20.450536: step 8955, loss 0.528795.
Train: 2018-08-09T04:31:23.519696: step 8956, loss 0.612744.
Train: 2018-08-09T04:31:26.593869: step 8957, loss 0.579157.
Train: 2018-08-09T04:31:29.661024: step 8958, loss 0.545679.
Train: 2018-08-09T04:31:32.742216: step 8959, loss 0.545711.
Train: 2018-08-09T04:31:35.847472: step 8960, loss 0.529048.
Test: 2018-08-09T04:31:56.939551: step 8960, loss 0.550583.
Train: 2018-08-09T04:32:00.041798: step 8961, loss 0.562427.
Train: 2018-08-09T04:32:03.136025: step 8962, loss 0.512423.
Train: 2018-08-09T04:32:06.237270: step 8963, loss 0.695808.
Train: 2018-08-09T04:32:09.308436: step 8964, loss 0.59572.
Train: 2018-08-09T04:32:12.382609: step 8965, loss 0.54586.
Train: 2018-08-09T04:32:15.485860: step 8966, loss 0.545907.
Train: 2018-08-09T04:32:18.561036: step 8967, loss 0.628648.
Train: 2018-08-09T04:32:21.685342: step 8968, loss 0.628503.
Train: 2018-08-09T04:32:24.784582: step 8969, loss 0.496764.
Train: 2018-08-09T04:32:27.887833: step 8970, loss 0.611787.
Test: 2018-08-09T04:32:48.957853: step 8970, loss 0.549722.
Train: 2018-08-09T04:32:52.058095: step 8971, loss 0.611683.
Train: 2018-08-09T04:32:55.183405: step 8972, loss 0.57893.
Train: 2018-08-09T04:32:58.282645: step 8973, loss 0.676493.
Train: 2018-08-09T04:33:01.359826: step 8974, loss 0.497961.
Train: 2018-08-09T04:33:04.468090: step 8975, loss 0.562747.
Train: 2018-08-09T04:33:07.532237: step 8976, loss 0.530604.
Train: 2018-08-09T04:33:10.592373: step 8977, loss 0.482517.
Train: 2018-08-09T04:33:13.637469: step 8978, loss 0.562816.
Train: 2018-08-09T04:33:16.702618: step 8979, loss 0.562819.
Train: 2018-08-09T04:33:19.771778: step 8980, loss 0.546775.
Test: 2018-08-09T04:33:40.905969: step 8980, loss 0.546567.
Train: 2018-08-09T04:33:43.955075: step 8981, loss 0.49862.
Train: 2018-08-09T04:33:47.016214: step 8982, loss 0.562801.
Train: 2018-08-09T04:33:50.122472: step 8983, loss 0.530602.
Train: 2018-08-09T04:33:53.188624: step 8984, loss 0.498288.
Train: 2018-08-09T04:33:56.271822: step 8985, loss 0.401117.
Train: 2018-08-09T04:33:59.356022: step 8986, loss 0.578908.
Train: 2018-08-09T04:34:02.456265: step 8987, loss 0.611557.
Train: 2018-08-09T04:34:05.538459: step 8988, loss 0.497129.
Train: 2018-08-09T04:34:08.649731: step 8989, loss 0.611825.
Train: 2018-08-09T04:34:11.752982: step 8990, loss 0.644867.
Test: 2018-08-09T04:34:32.830020: step 8990, loss 0.548939.
Train: 2018-08-09T04:34:35.931265: step 8991, loss 0.54603.
Train: 2018-08-09T04:34:39.037524: step 8992, loss 0.579008.
Train: 2018-08-09T04:34:42.153809: step 8993, loss 0.562499.
Train: 2018-08-09T04:34:45.227983: step 8994, loss 0.545971.
Train: 2018-08-09T04:34:48.315191: step 8995, loss 0.545953.
Train: 2018-08-09T04:34:51.417439: step 8996, loss 0.562482.
Train: 2018-08-09T04:34:54.486599: step 8997, loss 0.529353.
Train: 2018-08-09T04:34:57.583833: step 8998, loss 0.595628.
Train: 2018-08-09T04:35:00.670039: step 8999, loss 0.545877.
Train: 2018-08-09T04:35:03.745215: step 9000, loss 0.612251.
Test: 2018-08-09T04:35:24.824259: step 9000, loss 0.548171.
Train: 2018-08-09T04:35:29.658110: step 9001, loss 0.678604.
Train: 2018-08-09T04:35:32.733286: step 9002, loss 0.529369.
Train: 2018-08-09T04:35:35.803449: step 9003, loss 0.579021.
Train: 2018-08-09T04:35:38.923745: step 9004, loss 0.579008.
Train: 2018-08-09T04:35:41.997918: step 9005, loss 0.595466.
Train: 2018-08-09T04:35:45.065073: step 9006, loss 0.546105.
Train: 2018-08-09T04:35:48.143257: step 9007, loss 0.628187.
Train: 2018-08-09T04:35:51.193366: step 9008, loss 0.546221.
Train: 2018-08-09T04:35:54.236457: step 9009, loss 0.578935.
Train: 2018-08-09T04:35:57.332689: step 9010, loss 0.546339.
Test: 2018-08-09T04:36:18.372628: step 9010, loss 0.548642.
Train: 2018-08-09T04:36:21.425746: step 9011, loss 0.578914.
Train: 2018-08-09T04:36:24.510949: step 9012, loss 0.546437.
Train: 2018-08-09T04:36:27.594146: step 9013, loss 0.530263.
Train: 2018-08-09T04:36:30.686367: step 9014, loss 0.562696.
Train: 2018-08-09T04:36:33.778589: step 9015, loss 0.643664.
Train: 2018-08-09T04:36:36.832709: step 9016, loss 0.627376.
Train: 2018-08-09T04:36:39.936962: step 9017, loss 0.482167.
Train: 2018-08-09T04:36:43.016149: step 9018, loss 0.498367.
Train: 2018-08-09T04:36:46.079293: step 9019, loss 0.514457.
Train: 2018-08-09T04:36:49.133413: step 9020, loss 0.514393.
Test: 2018-08-09T04:37:10.269609: step 9020, loss 0.549434.
Train: 2018-08-09T04:37:13.320720: step 9021, loss 0.611187.
Train: 2018-08-09T04:37:16.411939: step 9022, loss 0.514232.
Train: 2018-08-09T04:37:19.487115: step 9023, loss 0.465565.
Train: 2018-08-09T04:37:22.572318: step 9024, loss 0.530188.
Train: 2018-08-09T04:37:25.658523: step 9025, loss 0.530042.
Train: 2018-08-09T04:37:28.743726: step 9026, loss 0.464485.
Train: 2018-08-09T04:37:31.837952: step 9027, loss 0.513258.
Train: 2018-08-09T04:37:34.936190: step 9028, loss 0.612035.
Train: 2018-08-09T04:37:38.030417: step 9029, loss 0.579044.
Train: 2018-08-09T04:37:40.940153: step 9030, loss 0.615638.
Test: 2018-08-09T04:38:02.014183: step 9030, loss 0.550614.
Train: 2018-08-09T04:38:05.111417: step 9031, loss 0.595738.
Train: 2018-08-09T04:38:08.187596: step 9032, loss 0.562431.
Train: 2018-08-09T04:38:11.243721: step 9033, loss 0.512386.
Train: 2018-08-09T04:38:14.322908: step 9034, loss 0.612528.
Train: 2018-08-09T04:38:17.407108: step 9035, loss 0.562415.
Train: 2018-08-09T04:38:20.482284: step 9036, loss 0.545698.
Train: 2018-08-09T04:38:23.544426: step 9037, loss 0.545689.
Train: 2018-08-09T04:38:26.622610: step 9038, loss 0.495482.
Train: 2018-08-09T04:38:29.697786: step 9039, loss 0.528888.
Train: 2018-08-09T04:38:32.789005: step 9040, loss 0.528821.
Test: 2018-08-09T04:38:53.868048: step 9040, loss 0.551711.
Train: 2018-08-09T04:38:56.941218: step 9041, loss 0.545563.
Train: 2018-08-09T04:39:00.060512: step 9042, loss 0.629777.
Train: 2018-08-09T04:39:03.162760: step 9043, loss 0.52865.
Train: 2018-08-09T04:39:06.250971: step 9044, loss 0.579243.
Train: 2018-08-09T04:39:09.348205: step 9045, loss 0.477947.
Train: 2018-08-09T04:39:12.449451: step 9046, loss 0.613091.
Train: 2018-08-09T04:39:15.526632: step 9047, loss 0.545439.
Train: 2018-08-09T04:39:18.605819: step 9048, loss 0.579286.
Train: 2018-08-09T04:39:21.702051: step 9049, loss 0.528494.
Train: 2018-08-09T04:39:24.794272: step 9050, loss 0.748694.
Test: 2018-08-09T04:39:45.855268: step 9050, loss 0.547808.
Train: 2018-08-09T04:39:48.922422: step 9051, loss 0.511671.
Train: 2018-08-09T04:39:52.017651: step 9052, loss 0.579238.
Train: 2018-08-09T04:39:55.085809: step 9053, loss 0.478184.
Train: 2018-08-09T04:39:58.162990: step 9054, loss 0.579209.
Train: 2018-08-09T04:40:01.239169: step 9055, loss 0.528748.
Train: 2018-08-09T04:40:04.309331: step 9056, loss 0.612818.
Train: 2018-08-09T04:40:07.390524: step 9057, loss 0.612765.
Train: 2018-08-09T04:40:10.462692: step 9058, loss 0.52888.
Train: 2018-08-09T04:40:13.555916: step 9059, loss 0.612617.
Train: 2018-08-09T04:40:16.645129: step 9060, loss 0.696036.
Test: 2018-08-09T04:40:37.704120: step 9060, loss 0.550003.
Train: 2018-08-09T04:40:40.835445: step 9061, loss 0.545809.
Train: 2018-08-09T04:40:43.924658: step 9062, loss 0.512739.
Train: 2018-08-09T04:40:46.993818: step 9063, loss 0.579025.
Train: 2018-08-09T04:40:50.079021: step 9064, loss 0.611996.
Train: 2018-08-09T04:40:53.144170: step 9065, loss 0.546091.
Train: 2018-08-09T04:40:56.229373: step 9066, loss 0.496948.
Train: 2018-08-09T04:40:59.306554: step 9067, loss 0.497042.
Train: 2018-08-09T04:41:02.426850: step 9068, loss 0.546192.
Train: 2018-08-09T04:41:05.518069: step 9069, loss 0.644494.
Train: 2018-08-09T04:41:08.613298: step 9070, loss 0.480756.
Test: 2018-08-09T04:41:29.677302: step 9070, loss 0.550956.
Train: 2018-08-09T04:41:32.739443: step 9071, loss 0.464364.
Train: 2018-08-09T04:41:35.849712: step 9072, loss 0.562565.
Train: 2018-08-09T04:41:38.939928: step 9073, loss 0.562551.
Train: 2018-08-09T04:41:42.013099: step 9074, loss 0.611882.
Train: 2018-08-09T04:41:45.090280: step 9075, loss 0.529615.
Train: 2018-08-09T04:41:48.178491: step 9076, loss 0.546049.
Train: 2018-08-09T04:41:51.240632: step 9077, loss 0.529534.
Train: 2018-08-09T04:41:54.333856: step 9078, loss 0.512961.
Train: 2018-08-09T04:41:57.431091: step 9079, loss 0.512843.
Train: 2018-08-09T04:42:00.520305: step 9080, loss 0.67859.
Test: 2018-08-09T04:42:21.585310: step 9080, loss 0.550048.
Train: 2018-08-09T04:42:24.656476: step 9081, loss 0.529268.
Train: 2018-08-09T04:42:27.709593: step 9082, loss 0.612284.
Train: 2018-08-09T04:42:30.817857: step 9083, loss 0.612279.
Train: 2018-08-09T04:42:33.902057: step 9084, loss 0.54587.
Train: 2018-08-09T04:42:36.994279: step 9085, loss 0.512725.
Train: 2018-08-09T04:42:40.109561: step 9086, loss 0.562466.
Train: 2018-08-09T04:42:43.197772: step 9087, loss 0.496129.
Train: 2018-08-09T04:42:46.286986: step 9088, loss 0.529255.
Train: 2018-08-09T04:42:49.356146: step 9089, loss 0.479322.
Train: 2018-08-09T04:42:52.447364: step 9090, loss 0.495763.
Test: 2018-08-09T04:43:13.495325: step 9090, loss 0.549905.
Train: 2018-08-09T04:43:16.590554: step 9091, loss 0.528969.
Train: 2018-08-09T04:43:19.684781: step 9092, loss 0.57917.
Train: 2018-08-09T04:43:22.772992: step 9093, loss 0.495095.
Train: 2018-08-09T04:43:25.866216: step 9094, loss 0.54549.
Train: 2018-08-09T04:43:28.950416: step 9095, loss 0.647004.
Train: 2018-08-09T04:43:32.029602: step 9096, loss 0.47759.
Train: 2018-08-09T04:43:35.097760: step 9097, loss 0.579339.
Train: 2018-08-09T04:43:38.194994: step 9098, loss 0.562343.
Train: 2018-08-09T04:43:41.289221: step 9099, loss 0.56234.
Train: 2018-08-09T04:43:44.369411: step 9100, loss 0.630598.
Test: 2018-08-09T04:44:05.437425: step 9100, loss 0.548934.
Train: 2018-08-09T04:44:10.300354: step 9101, loss 0.562339.
Train: 2018-08-09T04:44:13.372522: step 9102, loss 0.460007.
Train: 2018-08-09T04:44:16.443687: step 9103, loss 0.562338.
Train: 2018-08-09T04:44:19.510842: step 9104, loss 0.579425.
Train: 2018-08-09T04:44:22.625122: step 9105, loss 0.664896.
Train: 2018-08-09T04:44:25.715338: step 9106, loss 0.613545.
Train: 2018-08-09T04:44:28.772466: step 9107, loss 0.630464.
Train: 2018-08-09T04:44:31.846639: step 9108, loss 0.562349.
Train: 2018-08-09T04:44:34.928834: step 9109, loss 0.494658.
Train: 2018-08-09T04:44:37.969920: step 9110, loss 0.461.
Test: 2018-08-09T04:44:59.026905: step 9110, loss 0.547816.
Train: 2018-08-09T04:45:02.072000: step 9111, loss 0.562364.
Train: 2018-08-09T04:45:05.148179: step 9112, loss 0.528595.
Train: 2018-08-09T04:45:08.242406: step 9113, loss 0.663687.
Train: 2018-08-09T04:45:11.324600: step 9114, loss 0.545511.
Train: 2018-08-09T04:45:14.414817: step 9115, loss 0.629731.
Train: 2018-08-09T04:45:17.509043: step 9116, loss 0.478388.
Train: 2018-08-09T04:45:20.622320: step 9117, loss 0.528821.
Train: 2018-08-09T04:45:23.718552: step 9118, loss 0.528836.
Train: 2018-08-09T04:45:26.800747: step 9119, loss 0.545613.
Train: 2018-08-09T04:45:29.884947: step 9120, loss 0.562391.
Test: 2018-08-09T04:45:50.925890: step 9120, loss 0.547932.
Train: 2018-08-09T04:45:54.114367: step 9121, loss 0.562391.
Train: 2018-08-09T04:45:57.187537: step 9122, loss 0.679889.
Train: 2018-08-09T04:46:00.264718: step 9123, loss 0.411625.
Train: 2018-08-09T04:46:03.331873: step 9124, loss 0.495352.
Train: 2018-08-09T04:46:06.410057: step 9125, loss 0.612746.
Train: 2018-08-09T04:46:09.478215: step 9126, loss 0.512014.
Train: 2018-08-09T04:46:12.586479: step 9127, loss 0.495149.
Train: 2018-08-09T04:46:15.668674: step 9128, loss 0.528696.
Train: 2018-08-09T04:46:18.763903: step 9129, loss 0.629867.
Train: 2018-08-09T04:46:21.866151: step 9130, loss 0.511704.
Test: 2018-08-09T04:46:42.901077: step 9130, loss 0.550354.
Train: 2018-08-09T04:46:45.993298: step 9131, loss 0.528543.
Train: 2018-08-09T04:46:49.104570: step 9132, loss 0.494616.
Train: 2018-08-09T04:46:52.165709: step 9133, loss 0.545376.
Train: 2018-08-09T04:46:55.313077: step 9134, loss 0.562344.
Train: 2018-08-09T04:46:58.401288: step 9135, loss 0.528258.
Train: 2018-08-09T04:47:01.501530: step 9136, loss 0.596489.
Train: 2018-08-09T04:47:04.578712: step 9137, loss 0.545243.
Train: 2018-08-09T04:47:07.689984: step 9138, loss 0.545223.
Train: 2018-08-09T04:47:10.780200: step 9139, loss 0.493811.
Train: 2018-08-09T04:47:13.844346: step 9140, loss 0.613821.
Test: 2018-08-09T04:47:34.891305: step 9140, loss 0.551439.
Train: 2018-08-09T04:47:37.974502: step 9141, loss 0.596683.
Train: 2018-08-09T04:47:41.085774: step 9142, loss 0.579507.
Train: 2018-08-09T04:47:44.136886: step 9143, loss 0.579498.
Train: 2018-08-09T04:47:47.211059: step 9144, loss 0.630927.
Train: 2018-08-09T04:47:50.272198: step 9145, loss 0.476768.
Train: 2018-08-09T04:47:53.360409: step 9146, loss 0.59654.
Train: 2018-08-09T04:47:56.519809: step 9147, loss 0.613575.
Train: 2018-08-09T04:47:59.674195: step 9148, loss 0.647555.
Train: 2018-08-09T04:48:02.896763: step 9149, loss 0.66426.
Train: 2018-08-09T04:48:05.991993: step 9150, loss 0.596171.
Test: 2018-08-09T04:48:27.062012: step 9150, loss 0.546619.
Train: 2018-08-09T04:48:30.173284: step 9151, loss 0.528737.
Train: 2018-08-09T04:48:33.215372: step 9152, loss 0.528893.
Train: 2018-08-09T04:48:36.307593: step 9153, loss 0.562419.
Train: 2018-08-09T04:48:39.489052: step 9154, loss 0.562438.
Train: 2018-08-09T04:48:42.586287: step 9155, loss 0.57906.
Train: 2018-08-09T04:48:45.671490: step 9156, loss 0.711483.
Train: 2018-08-09T04:48:48.795796: step 9157, loss 0.562521.
Train: 2018-08-09T04:48:51.887015: step 9158, loss 0.529768.
Train: 2018-08-09T04:48:54.931108: step 9159, loss 0.497246.
Train: 2018-08-09T04:48:58.014306: step 9160, loss 0.578926.
Test: 2018-08-09T04:49:19.084325: step 9160, loss 0.550483.
Train: 2018-08-09T04:49:22.152482: step 9161, loss 0.530116.
Train: 2018-08-09T04:49:25.225653: step 9162, loss 0.562666.
Train: 2018-08-09T04:49:28.311859: step 9163, loss 0.481578.
Train: 2018-08-09T04:49:31.415109: step 9164, loss 0.497784.
Train: 2018-08-09T04:49:34.492290: step 9165, loss 0.660131.
Train: 2018-08-09T04:49:37.585514: step 9166, loss 0.692565.
Train: 2018-08-09T04:49:40.650664: step 9167, loss 0.5627.
Train: 2018-08-09T04:49:43.768954: step 9168, loss 0.627362.
Train: 2018-08-09T04:49:46.848141: step 9169, loss 0.546662.
Train: 2018-08-09T04:49:49.878197: step 9170, loss 0.611006.
Test: 2018-08-09T04:50:10.941198: step 9170, loss 0.549654.
Train: 2018-08-09T04:50:14.037430: step 9171, loss 0.610903.
Train: 2018-08-09T04:50:17.136670: step 9172, loss 0.594824.
Train: 2018-08-09T04:50:20.194801: step 9173, loss 0.594764.
Train: 2018-08-09T04:50:23.301059: step 9174, loss 0.563013.
Train: 2018-08-09T04:50:26.413334: step 9175, loss 0.626245.
Train: 2018-08-09T04:50:29.500542: step 9176, loss 0.610332.
Train: 2018-08-09T04:50:32.565692: step 9177, loss 0.51622.
Train: 2018-08-09T04:50:35.626830: step 9178, loss 0.500788.
Train: 2018-08-09T04:50:38.701004: step 9179, loss 0.532093.
Train: 2018-08-09T04:50:41.804254: step 9180, loss 0.578894.
Test: 2018-08-09T04:51:02.878284: step 9180, loss 0.548708.
Train: 2018-08-09T04:51:05.956468: step 9181, loss 0.516556.
Train: 2018-08-09T04:51:09.035655: step 9182, loss 0.532112.
Train: 2018-08-09T04:51:12.111834: step 9183, loss 0.500824.
Train: 2018-08-09T04:51:15.179991: step 9184, loss 0.531929.
Train: 2018-08-09T04:51:18.229098: step 9185, loss 0.547482.
Train: 2018-08-09T04:51:21.310290: step 9186, loss 0.578867.
Train: 2018-08-09T04:51:24.387471: step 9187, loss 0.673563.
Train: 2018-08-09T04:51:27.478690: step 9188, loss 0.5157.
Train: 2018-08-09T04:51:30.576927: step 9189, loss 0.626298.
Train: 2018-08-09T04:51:33.678173: step 9190, loss 0.563045.
Test: 2018-08-09T04:51:54.723126: step 9190, loss 0.547617.
Train: 2018-08-09T04:51:57.812339: step 9191, loss 0.43647.
Train: 2018-08-09T04:52:00.946672: step 9192, loss 0.531271.
Train: 2018-08-09T04:52:03.988760: step 9193, loss 0.62659.
Train: 2018-08-09T04:52:07.065941: step 9194, loss 0.499172.
Train: 2018-08-09T04:52:10.078952: step 9195, loss 0.610823.
Train: 2018-08-09T04:52:13.145104: step 9196, loss 0.610879.
Train: 2018-08-09T04:52:16.218275: step 9197, loss 0.530806.
Train: 2018-08-09T04:52:19.302475: step 9198, loss 0.707191.
Train: 2018-08-09T04:52:22.399710: step 9199, loss 0.691009.
Train: 2018-08-09T04:52:25.475889: step 9200, loss 0.610798.
Test: 2018-08-09T04:52:46.525855: step 9200, loss 0.548041.
Train: 2018-08-09T04:52:51.382768: step 9201, loss 0.562949.
Train: 2018-08-09T04:52:54.491032: step 9202, loss 0.563.
Train: 2018-08-09T04:52:57.574229: step 9203, loss 0.610489.
Train: 2018-08-09T04:53:00.650408: step 9204, loss 0.484288.
Train: 2018-08-09T04:53:03.734608: step 9205, loss 0.500165.
Train: 2018-08-09T04:53:06.831843: step 9206, loss 0.626087.
Train: 2018-08-09T04:53:09.935093: step 9207, loss 0.547421.
Train: 2018-08-09T04:53:13.017288: step 9208, loss 0.641735.
Train: 2018-08-09T04:53:16.070405: step 9209, loss 0.563185.
Train: 2018-08-09T04:53:19.158616: step 9210, loss 0.578878.
Test: 2018-08-09T04:53:40.197553: step 9210, loss 0.550943.
Train: 2018-08-09T04:53:43.320857: step 9211, loss 0.578882.
Train: 2018-08-09T04:53:46.380993: step 9212, loss 0.563262.
Train: 2018-08-09T04:53:49.462185: step 9213, loss 0.532067.
Train: 2018-08-09T04:53:52.518310: step 9214, loss 0.563287.
Train: 2018-08-09T04:53:55.609529: step 9215, loss 0.672509.
Train: 2018-08-09T04:53:58.693729: step 9216, loss 0.563324.
Train: 2018-08-09T04:54:01.809012: step 9217, loss 0.547803.
Train: 2018-08-09T04:54:04.895217: step 9218, loss 0.563371.
Train: 2018-08-09T04:54:07.976409: step 9219, loss 0.532328.
Train: 2018-08-09T04:54:11.065622: step 9220, loss 0.501257.
Test: 2018-08-09T04:54:32.115589: step 9220, loss 0.548776.
Train: 2018-08-09T04:54:35.204802: step 9221, loss 0.547796.
Train: 2018-08-09T04:54:38.310058: step 9222, loss 0.578896.
Train: 2018-08-09T04:54:41.401277: step 9223, loss 0.469653.
Train: 2018-08-09T04:54:44.465423: step 9224, loss 0.563225.
Train: 2018-08-09T04:54:47.545613: step 9225, loss 0.484654.
Train: 2018-08-09T04:54:50.601738: step 9226, loss 0.594634.
Train: 2018-08-09T04:54:53.701980: step 9227, loss 0.48392.
Train: 2018-08-09T04:54:56.776154: step 9228, loss 0.562963.
Train: 2018-08-09T04:54:59.872386: step 9229, loss 0.594822.
Train: 2018-08-09T04:55:02.934528: step 9230, loss 0.482784.
Test: 2018-08-09T04:55:23.986499: step 9230, loss 0.548328.
Train: 2018-08-09T04:55:27.066688: step 9231, loss 0.643207.
Train: 2018-08-09T04:55:30.163923: step 9232, loss 0.466005.
Train: 2018-08-09T04:55:33.242107: step 9233, loss 0.676032.
Train: 2018-08-09T04:55:36.340344: step 9234, loss 0.627553.
Train: 2018-08-09T04:55:39.388449: step 9235, loss 0.692479.
Train: 2018-08-09T04:55:42.436552: step 9236, loss 0.530304.
Train: 2018-08-09T04:55:45.535793: step 9237, loss 0.562711.
Train: 2018-08-09T04:55:48.644057: step 9238, loss 0.578889.
Train: 2018-08-09T04:55:51.734273: step 9239, loss 0.578886.
Train: 2018-08-09T04:55:54.821480: step 9240, loss 0.659548.
Test: 2018-08-09T04:56:15.856407: step 9240, loss 0.548315.
Train: 2018-08-09T04:56:18.940607: step 9241, loss 0.562784.
Train: 2018-08-09T04:56:22.040849: step 9242, loss 0.62703.
Train: 2018-08-09T04:56:25.137082: step 9243, loss 0.594867.
Train: 2018-08-09T04:56:28.221281: step 9244, loss 0.451253.
Train: 2018-08-09T04:56:31.314506: step 9245, loss 0.467311.
Train: 2018-08-09T04:56:34.400711: step 9246, loss 0.57886.
Train: 2018-08-09T04:56:37.491930: step 9247, loss 0.467152.
Train: 2018-08-09T04:56:40.566103: step 9248, loss 0.498892.
Train: 2018-08-09T04:56:43.670356: step 9249, loss 0.514689.
Train: 2018-08-09T04:56:46.744530: step 9250, loss 0.594981.
Test: 2018-08-09T04:57:07.876715: step 9250, loss 0.547603.
Train: 2018-08-09T04:57:10.958909: step 9251, loss 0.546585.
Train: 2018-08-09T04:57:14.042106: step 9252, loss 0.611289.
Train: 2018-08-09T04:57:17.097229: step 9253, loss 0.530227.
Train: 2018-08-09T04:57:20.178421: step 9254, loss 0.660216.
Train: 2018-08-09T04:57:23.253597: step 9255, loss 0.448787.
Train: 2018-08-09T04:57:26.337797: step 9256, loss 0.51372.
Train: 2018-08-09T04:57:29.415981: step 9257, loss 0.480861.
Train: 2018-08-09T04:57:32.510208: step 9258, loss 0.529739.
Train: 2018-08-09T04:57:35.591400: step 9259, loss 0.578994.
Train: 2018-08-09T04:57:38.663568: step 9260, loss 0.562493.
Test: 2018-08-09T04:57:59.722558: step 9260, loss 0.549449.
Train: 2018-08-09T04:58:02.825809: step 9261, loss 0.595617.
Train: 2018-08-09T04:58:05.903993: step 9262, loss 0.579062.
Train: 2018-08-09T04:58:08.972150: step 9263, loss 0.529186.
Train: 2018-08-09T04:58:12.065374: step 9264, loss 0.579093.
Train: 2018-08-09T04:58:15.142555: step 9265, loss 0.562426.
Train: 2018-08-09T04:58:18.235780: step 9266, loss 0.529026.
Train: 2018-08-09T04:58:21.334017: step 9267, loss 0.528973.
Train: 2018-08-09T04:58:24.431252: step 9268, loss 0.512161.
Train: 2018-08-09T04:58:27.493393: step 9269, loss 0.562391.
Train: 2018-08-09T04:58:30.584612: step 9270, loss 0.511937.
Test: 2018-08-09T04:58:51.615528: step 9270, loss 0.549767.
Train: 2018-08-09T04:58:54.758884: step 9271, loss 0.427536.
Train: 2018-08-09T04:58:57.858124: step 9272, loss 0.545435.
Train: 2018-08-09T04:59:00.966389: step 9273, loss 0.647281.
Train: 2018-08-09T04:59:04.060615: step 9274, loss 0.528304.
Train: 2018-08-09T04:59:07.162863: step 9275, loss 0.613507.
Train: 2018-08-09T04:59:10.248066: step 9276, loss 0.613555.
Train: 2018-08-09T04:59:13.309204: step 9277, loss 0.545267.
Train: 2018-08-09T04:59:16.409447: step 9278, loss 0.545266.
Train: 2018-08-09T04:59:19.502671: step 9279, loss 0.562338.
Train: 2018-08-09T04:59:22.583863: step 9280, loss 0.613559.
Test: 2018-08-09T04:59:43.645862: step 9280, loss 0.550872.
Train: 2018-08-09T04:59:46.715021: step 9281, loss 0.511168.
Train: 2018-08-09T04:59:49.798219: step 9282, loss 0.579393.
Train: 2018-08-09T04:59:52.899464: step 9283, loss 0.579383.
Train: 2018-08-09T04:59:55.970629: step 9284, loss 0.596392.
Train: 2018-08-09T04:59:59.039790: step 9285, loss 0.477355.
Train: 2018-08-09T05:00:02.119979: step 9286, loss 0.545353.
Train: 2018-08-09T05:00:05.184126: step 9287, loss 0.562347.
Train: 2018-08-09T05:00:08.274342: step 9288, loss 0.66427.
Train: 2018-08-09T05:00:11.383608: step 9289, loss 0.596261.
Train: 2018-08-09T05:00:14.451766: step 9290, loss 0.494704.
Test: 2018-08-09T05:00:35.519780: step 9290, loss 0.550368.
Train: 2018-08-09T05:00:38.581921: step 9291, loss 0.444114.
Train: 2018-08-09T05:00:41.749342: step 9292, loss 0.579264.
Train: 2018-08-09T05:00:44.818503: step 9293, loss 0.613074.
Train: 2018-08-09T05:00:47.907716: step 9294, loss 0.528582.
Train: 2018-08-09T05:00:51.001943: step 9295, loss 0.613022.
Train: 2018-08-09T05:00:54.089150: step 9296, loss 0.494908.
Train: 2018-08-09T05:00:57.165329: step 9297, loss 0.528645.
Train: 2018-08-09T05:01:00.276601: step 9298, loss 0.646706.
Train: 2018-08-09T05:01:03.338743: step 9299, loss 0.579221.
Train: 2018-08-09T05:01:06.432969: step 9300, loss 0.579203.
Test: 2018-08-09T05:01:27.493965: step 9300, loss 0.547922.
Train: 2018-08-09T05:01:32.316787: step 9301, loss 0.579183.
Train: 2018-08-09T05:01:35.404998: step 9302, loss 0.562397.
Train: 2018-08-09T05:01:38.510254: step 9303, loss 0.512207.
Train: 2018-08-09T05:01:41.580417: step 9304, loss 0.545694.
Train: 2018-08-09T05:01:44.641555: step 9305, loss 0.59583.
Train: 2018-08-09T05:01:47.759846: step 9306, loss 0.562423.
Train: 2018-08-09T05:01:50.838030: step 9307, loss 0.595766.
Train: 2018-08-09T05:01:53.940278: step 9308, loss 0.579082.
Train: 2018-08-09T05:01:57.051550: step 9309, loss 0.612283.
Train: 2018-08-09T05:02:00.118705: step 9310, loss 0.695016.
Test: 2018-08-09T05:02:21.151626: step 9310, loss 0.547678.
Train: 2018-08-09T05:02:24.245852: step 9311, loss 0.529521.
Train: 2018-08-09T05:02:27.338074: step 9312, loss 0.562541.
Train: 2018-08-09T05:02:30.418263: step 9313, loss 0.513422.
Train: 2018-08-09T05:02:33.497450: step 9314, loss 0.660686.
Train: 2018-08-09T05:02:36.581650: step 9315, loss 0.562631.
Train: 2018-08-09T05:02:39.659834: step 9316, loss 0.562668.
Train: 2018-08-09T05:02:42.768098: step 9317, loss 0.546509.
Train: 2018-08-09T05:02:45.850293: step 9318, loss 0.498107.
Train: 2018-08-09T05:02:48.956551: step 9319, loss 0.659588.
Train: 2018-08-09T05:02:52.044762: step 9320, loss 0.51447.
Test: 2018-08-09T05:03:13.132830: step 9320, loss 0.548334.
Train: 2018-08-09T05:03:16.222043: step 9321, loss 0.498473.
Train: 2018-08-09T05:03:19.314264: step 9322, loss 0.562794.
Train: 2018-08-09T05:03:22.394453: step 9323, loss 0.562794.
Train: 2018-08-09T05:03:25.477651: step 9324, loss 0.578873.
Train: 2018-08-09T05:03:28.565862: step 9325, loss 0.562795.
Train: 2018-08-09T05:03:31.631011: step 9326, loss 0.418118.
Train: 2018-08-09T05:03:34.739275: step 9327, loss 0.643336.
Train: 2018-08-09T05:03:37.858568: step 9328, loss 0.578881.
Train: 2018-08-09T05:03:41.019973: step 9329, loss 0.611156.
Train: 2018-08-09T05:03:44.125229: step 9330, loss 0.611145.
Test: 2018-08-09T05:04:05.184220: step 9330, loss 0.547055.
Train: 2018-08-09T05:04:08.128046: step 9331, loss 0.545574.
Train: 2018-08-09T05:04:11.239318: step 9332, loss 0.546667.
Train: 2018-08-09T05:04:14.344574: step 9333, loss 0.51447.
Train: 2018-08-09T05:04:17.426769: step 9334, loss 0.643327.
Train: 2018-08-09T05:04:20.496932: step 9335, loss 0.530575.
Train: 2018-08-09T05:04:23.578124: step 9336, loss 0.498378.
Train: 2018-08-09T05:04:26.656308: step 9337, loss 0.578879.
Train: 2018-08-09T05:04:29.773596: step 9338, loss 0.562752.
Train: 2018-08-09T05:04:32.851780: step 9339, loss 0.611165.
Train: 2018-08-09T05:04:35.941996: step 9340, loss 0.546607.
Test: 2018-08-09T05:04:57.009008: step 9340, loss 0.548229.
Train: 2018-08-09T05:05:00.096215: step 9341, loss 0.482036.
Train: 2018-08-09T05:05:03.216511: step 9342, loss 0.611222.
Train: 2018-08-09T05:05:06.312743: step 9343, loss 0.562715.
Train: 2018-08-09T05:05:09.377893: step 9344, loss 0.562707.
Train: 2018-08-09T05:05:12.468109: step 9345, loss 0.595091.
Train: 2018-08-09T05:05:15.581386: step 9346, loss 0.49792.
Train: 2018-08-09T05:05:18.664584: step 9347, loss 0.530261.
Train: 2018-08-09T05:05:21.710682: step 9348, loss 0.562669.
Train: 2018-08-09T05:05:24.779842: step 9349, loss 0.497608.
Train: 2018-08-09T05:05:27.871061: step 9350, loss 0.578925.
Test: 2018-08-09T05:05:48.980184: step 9350, loss 0.549158.
Train: 2018-08-09T05:05:52.034304: step 9351, loss 0.611595.
Train: 2018-08-09T05:05:55.126526: step 9352, loss 0.660659.
Train: 2018-08-09T05:05:58.195686: step 9353, loss 0.529947.
Train: 2018-08-09T05:06:01.304952: step 9354, loss 0.644237.
Train: 2018-08-09T05:06:04.392160: step 9355, loss 0.644126.
Train: 2018-08-09T05:06:07.461320: step 9356, loss 0.465127.
Train: 2018-08-09T05:06:10.507419: step 9357, loss 0.562667.
Train: 2018-08-09T05:06:13.597635: step 9358, loss 0.611362.
Train: 2018-08-09T05:06:16.701889: step 9359, loss 0.481669.
Train: 2018-08-09T05:06:19.796115: step 9360, loss 0.562693.
Test: 2018-08-09T05:06:40.836055: step 9360, loss 0.548119.
Train: 2018-08-09T05:06:43.971391: step 9361, loss 0.481658.
Train: 2018-08-09T05:06:47.050578: step 9362, loss 0.660055.
Train: 2018-08-09T05:06:50.131770: step 9363, loss 0.643803.
Train: 2018-08-09T05:06:53.231009: step 9364, loss 0.578897.
Train: 2018-08-09T05:06:56.316212: step 9365, loss 0.401007.
Train: 2018-08-09T05:06:59.494663: step 9366, loss 0.546513.
Train: 2018-08-09T05:07:02.559812: step 9367, loss 0.530261.
Train: 2018-08-09T05:07:05.698156: step 9368, loss 0.546423.
Train: 2018-08-09T05:07:08.788372: step 9369, loss 0.49755.
Train: 2018-08-09T05:07:11.880594: step 9370, loss 0.497337.
Test: 2018-08-09T05:07:32.911509: step 9370, loss 0.548469.
Train: 2018-08-09T05:07:35.985682: step 9371, loss 0.611708.
Train: 2018-08-09T05:07:39.083920: step 9372, loss 0.611802.
Train: 2018-08-09T05:07:42.178146: step 9373, loss 0.578978.
Train: 2018-08-09T05:07:45.254325: step 9374, loss 0.595438.
Train: 2018-08-09T05:07:48.327496: step 9375, loss 0.628358.
Train: 2018-08-09T05:07:51.403674: step 9376, loss 0.463882.
Train: 2018-08-09T05:07:54.520962: step 9377, loss 0.546075.
Train: 2018-08-09T05:07:57.604160: step 9378, loss 0.694287.
Train: 2018-08-09T05:08:00.709416: step 9379, loss 0.61188.
Train: 2018-08-09T05:08:03.778576: step 9380, loss 0.513306.
Test: 2018-08-09T05:08:24.843583: step 9380, loss 0.548439.
Train: 2018-08-09T05:08:27.938811: step 9381, loss 0.562563.
Train: 2018-08-09T05:08:31.020003: step 9382, loss 0.562573.
Train: 2018-08-09T05:08:34.113227: step 9383, loss 0.513478.
Train: 2018-08-09T05:08:37.202441: step 9384, loss 0.546214.
Train: 2018-08-09T05:08:40.286641: step 9385, loss 0.513464.
Train: 2018-08-09T05:08:43.331737: step 9386, loss 0.677287.
Train: 2018-08-09T05:08:46.430977: step 9387, loss 0.546207.
Train: 2018-08-09T05:08:49.531219: step 9388, loss 0.595309.
Train: 2018-08-09T05:08:52.605393: step 9389, loss 0.513566.
Train: 2018-08-09T05:08:55.676558: step 9390, loss 0.497238.
Test: 2018-08-09T05:09:16.774653: step 9390, loss 0.547885.
Train: 2018-08-09T05:09:19.887929: step 9391, loss 0.644362.
Train: 2018-08-09T05:09:23.006220: step 9392, loss 0.431833.
Train: 2018-08-09T05:09:26.067359: step 9393, loss 0.578951.
Train: 2018-08-09T05:09:29.137522: step 9394, loss 0.562567.
Train: 2018-08-09T05:09:32.198661: step 9395, loss 0.578965.
Train: 2018-08-09T05:09:35.312940: step 9396, loss 0.513294.
Train: 2018-08-09T05:09:38.356031: step 9397, loss 0.529655.
Train: 2018-08-09T05:09:41.458279: step 9398, loss 0.529583.
Train: 2018-08-09T05:09:44.538469: step 9399, loss 0.529498.
Train: 2018-08-09T05:09:47.623671: step 9400, loss 0.579027.
Test: 2018-08-09T05:10:08.676646: step 9400, loss 0.548824.
Train: 2018-08-09T05:10:13.582689: step 9401, loss 0.496186.
Train: 2018-08-09T05:10:16.697972: step 9402, loss 0.545838.
Train: 2018-08-09T05:10:19.765127: step 9403, loss 0.512466.
Train: 2018-08-09T05:10:22.844313: step 9404, loss 0.629241.
Train: 2018-08-09T05:10:25.924503: step 9405, loss 0.595871.
Train: 2018-08-09T05:10:29.009705: step 9406, loss 0.47868.
Train: 2018-08-09T05:10:32.085884: step 9407, loss 0.47851.
Train: 2018-08-09T05:10:35.177103: step 9408, loss 0.56238.
Train: 2018-08-09T05:10:38.245260: step 9409, loss 0.62984.
Train: 2018-08-09T05:10:41.298378: step 9410, loss 0.511706.
Test: 2018-08-09T05:11:02.309240: step 9410, loss 0.54907.
Train: 2018-08-09T05:11:05.373386: step 9411, loss 0.443962.
Train: 2018-08-09T05:11:08.457586: step 9412, loss 0.579317.
Train: 2018-08-09T05:11:11.530757: step 9413, loss 0.562344.
Train: 2018-08-09T05:11:14.611949: step 9414, loss 0.528254.
Train: 2018-08-09T05:11:17.672085: step 9415, loss 0.545257.
Train: 2018-08-09T05:11:20.768317: step 9416, loss 0.596567.
Train: 2018-08-09T05:11:23.876581: step 9417, loss 0.682285.
Train: 2018-08-09T05:11:26.984845: step 9418, loss 0.665056.
Train: 2018-08-09T05:11:30.086090: step 9419, loss 0.664798.
Train: 2018-08-09T05:11:33.149235: step 9420, loss 0.664397.
Test: 2018-08-09T05:11:54.185164: step 9420, loss 0.549065.
Train: 2018-08-09T05:11:57.282398: step 9421, loss 0.52852.
Train: 2018-08-09T05:12:00.384646: step 9422, loss 0.579219.
Train: 2018-08-09T05:12:03.485891: step 9423, loss 0.61271.
Train: 2018-08-09T05:12:06.567083: step 9424, loss 0.579113.
Train: 2018-08-09T05:12:09.640254: step 9425, loss 0.529228.
Train: 2018-08-09T05:12:12.714427: step 9426, loss 0.62868.
Train: 2018-08-09T05:12:15.783588: step 9427, loss 0.611943.
Train: 2018-08-09T05:12:18.846732: step 9428, loss 0.529779.
Train: 2018-08-09T05:12:21.948980: step 9429, loss 0.464634.
Train: 2018-08-09T05:12:25.013126: step 9430, loss 0.546331.
Test: 2018-08-09T05:12:46.038026: step 9430, loss 0.548013.
Train: 2018-08-09T05:12:49.120221: step 9431, loss 0.513828.
Train: 2018-08-09T05:12:52.208431: step 9432, loss 0.643968.
Train: 2018-08-09T05:12:55.282604: step 9433, loss 0.59514.
Train: 2018-08-09T05:12:58.400895: step 9434, loss 0.562698.
Train: 2018-08-09T05:13:01.479079: step 9435, loss 0.530386.
Train: 2018-08-09T05:13:04.586341: step 9436, loss 0.595034.
Train: 2018-08-09T05:13:07.683575: step 9437, loss 0.675623.
Train: 2018-08-09T05:13:10.766773: step 9438, loss 0.578872.
Train: 2018-08-09T05:13:13.841948: step 9439, loss 0.514784.
Train: 2018-08-09T05:13:16.898074: step 9440, loss 0.594851.
Test: 2018-08-09T05:13:37.953054: step 9440, loss 0.550373.
Train: 2018-08-09T05:13:41.035248: step 9441, loss 0.57886.
Train: 2018-08-09T05:13:44.109421: step 9442, loss 0.578859.
Train: 2018-08-09T05:13:47.197632: step 9443, loss 0.531211.
Train: 2018-08-09T05:13:50.285843: step 9444, loss 0.531275.
Train: 2018-08-09T05:13:53.382075: step 9445, loss 0.563006.
Train: 2018-08-09T05:13:56.481315: step 9446, loss 0.594706.
Train: 2018-08-09T05:13:59.569526: step 9447, loss 0.531357.
Train: 2018-08-09T05:14:02.649715: step 9448, loss 0.642193.
Train: 2018-08-09T05:14:05.734918: step 9449, loss 0.499799.
Train: 2018-08-09T05:14:08.824131: step 9450, loss 0.578861.
Test: 2018-08-09T05:14:29.897159: step 9450, loss 0.547638.
Train: 2018-08-09T05:14:32.987374: step 9451, loss 0.483996.
Train: 2018-08-09T05:14:36.057537: step 9452, loss 0.594692.
Train: 2018-08-09T05:14:39.139732: step 9453, loss 0.515478.
Train: 2018-08-09T05:14:42.223932: step 9454, loss 0.626474.
Train: 2018-08-09T05:14:45.295097: step 9455, loss 0.499462.
Train: 2018-08-09T05:14:48.359244: step 9456, loss 0.531142.
Train: 2018-08-09T05:14:51.413364: step 9457, loss 0.56292.
Train: 2018-08-09T05:14:54.502577: step 9458, loss 0.514978.
Train: 2018-08-09T05:14:57.576751: step 9459, loss 0.466769.
Train: 2018-08-09T05:15:00.652929: step 9460, loss 0.659272.
Test: 2018-08-09T05:15:21.733978: step 9460, loss 0.550713.
Train: 2018-08-09T05:15:24.820184: step 9461, loss 0.611105.
Train: 2018-08-09T05:15:27.911402: step 9462, loss 0.643408.
Train: 2018-08-09T05:15:30.994600: step 9463, loss 0.562754.
Train: 2018-08-09T05:15:34.068773: step 9464, loss 0.562757.
Train: 2018-08-09T05:15:37.123896: step 9465, loss 0.53052.
Train: 2018-08-09T05:15:40.207093: step 9466, loss 0.611135.
Train: 2018-08-09T05:15:43.294301: step 9467, loss 0.482156.
Train: 2018-08-09T05:15:46.395546: step 9468, loss 0.514333.
Train: 2018-08-09T05:15:49.485763: step 9469, loss 0.514223.
Train: 2018-08-09T05:15:52.588011: step 9470, loss 0.595105.
Test: 2018-08-09T05:16:13.657028: step 9470, loss 0.55053.
Train: 2018-08-09T05:16:16.745238: step 9471, loss 0.676309.
Train: 2018-08-09T05:16:19.845481: step 9472, loss 0.53022.
Train: 2018-08-09T05:16:22.922662: step 9473, loss 0.595137.
Train: 2018-08-09T05:16:26.008867: step 9474, loss 0.627585.
Train: 2018-08-09T05:16:29.087051: step 9475, loss 0.514077.
Train: 2018-08-09T05:16:32.159220: step 9476, loss 0.627495.
Train: 2018-08-09T05:16:35.235398: step 9477, loss 0.562715.
Train: 2018-08-09T05:16:38.316590: step 9478, loss 0.578887.
Train: 2018-08-09T05:16:41.390764: step 9479, loss 0.514342.
Train: 2018-08-09T05:16:44.481982: step 9480, loss 0.530493.
Test: 2018-08-09T05:17:05.601133: step 9480, loss 0.548242.
Train: 2018-08-09T05:17:08.719423: step 9481, loss 0.514347.
Train: 2018-08-09T05:17:11.787580: step 9482, loss 0.691945.
Train: 2018-08-09T05:17:14.883812: step 9483, loss 0.498218.
Train: 2018-08-09T05:17:17.957986: step 9484, loss 0.578882.
Train: 2018-08-09T05:17:21.027146: step 9485, loss 0.514355.
Train: 2018-08-09T05:17:24.094300: step 9486, loss 0.530454.
Train: 2018-08-09T05:17:27.179503: step 9487, loss 0.611214.
Train: 2018-08-09T05:17:30.237634: step 9488, loss 0.611226.
Train: 2018-08-09T05:17:33.301781: step 9489, loss 0.708168.
Train: 2018-08-09T05:17:36.392999: step 9490, loss 0.594993.
Test: 2018-08-09T05:17:57.480065: step 9490, loss 0.547146.
Train: 2018-08-09T05:18:00.539197: step 9491, loss 0.546741.
Train: 2018-08-09T05:18:03.647461: step 9492, loss 0.562841.
Train: 2018-08-09T05:18:06.728653: step 9493, loss 0.562872.
Train: 2018-08-09T05:18:09.840928: step 9494, loss 0.578861.
Train: 2018-08-09T05:18:12.924125: step 9495, loss 0.515134.
Train: 2018-08-09T05:18:16.024368: step 9496, loss 0.56294.
Train: 2018-08-09T05:18:19.107565: step 9497, loss 0.594768.
Train: 2018-08-09T05:18:22.226859: step 9498, loss 0.53118.
Train: 2018-08-09T05:18:25.304040: step 9499, loss 0.610636.
Train: 2018-08-09T05:18:28.387238: step 9500, loss 0.562985.
Test: 2018-08-09T05:18:49.510399: step 9500, loss 0.552335.
Train: 2018-08-09T05:18:54.449530: step 9501, loss 0.547137.
Train: 2018-08-09T05:18:57.519693: step 9502, loss 0.53129.
Train: 2018-08-09T05:19:00.592863: step 9503, loss 0.531273.
Train: 2018-08-09T05:19:03.671047: step 9504, loss 0.547103.
Train: 2018-08-09T05:19:06.750234: step 9505, loss 0.499367.
Train: 2018-08-09T05:19:09.846466: step 9506, loss 0.610729.
Train: 2018-08-09T05:19:12.950719: step 9507, loss 0.515031.
Train: 2018-08-09T05:19:16.022888: step 9508, loss 0.54688.
Train: 2018-08-09T05:19:19.111098: step 9509, loss 0.546809.
Train: 2018-08-09T05:19:22.180258: step 9510, loss 0.498544.
Test: 2018-08-09T05:19:43.210172: step 9510, loss 0.548876.
Train: 2018-08-09T05:19:46.311416: step 9511, loss 0.530519.
Train: 2018-08-09T05:19:49.401632: step 9512, loss 0.530363.
Train: 2018-08-09T05:19:52.490846: step 9513, loss 0.562673.
Train: 2018-08-09T05:19:55.591088: step 9514, loss 0.709278.
Train: 2018-08-09T05:19:58.631171: step 9515, loss 0.660438.
Train: 2018-08-09T05:20:01.714369: step 9516, loss 0.562637.
Train: 2018-08-09T05:20:04.812606: step 9517, loss 0.578915.
Train: 2018-08-09T05:20:07.887782: step 9518, loss 0.562661.
Train: 2018-08-09T05:20:10.972985: step 9519, loss 0.481515.
Train: 2018-08-09T05:20:14.078241: step 9520, loss 0.611386.
Test: 2018-08-09T05:20:35.127205: step 9520, loss 0.549304.
Train: 2018-08-09T05:20:38.229452: step 9521, loss 0.49774.
Train: 2018-08-09T05:20:41.333706: step 9522, loss 0.530173.
Train: 2018-08-09T05:20:44.432945: step 9523, loss 0.546384.
Train: 2018-08-09T05:20:47.493082: step 9524, loss 0.59521.
Train: 2018-08-09T05:20:50.578284: step 9525, loss 0.448527.
Train: 2018-08-09T05:20:53.652457: step 9526, loss 0.546257.
Train: 2018-08-09T05:20:56.728636: step 9527, loss 0.529809.
Train: 2018-08-09T05:20:59.811834: step 9528, loss 0.529692.
Train: 2018-08-09T05:21:02.898039: step 9529, loss 0.59547.
Train: 2018-08-09T05:21:05.977226: step 9530, loss 0.529492.
Test: 2018-08-09T05:21:27.037219: step 9530, loss 0.548224.
Train: 2018-08-09T05:21:30.136458: step 9531, loss 0.479722.
Train: 2018-08-09T05:21:33.199602: step 9532, loss 0.545847.
Train: 2018-08-09T05:21:36.282800: step 9533, loss 0.595761.
Train: 2018-08-09T05:21:39.346947: step 9534, loss 0.462195.
Train: 2018-08-09T05:21:42.449195: step 9535, loss 0.595925.
Train: 2018-08-09T05:21:45.552445: step 9536, loss 0.646421.
Train: 2018-08-09T05:21:48.639653: step 9537, loss 0.629671.
Train: 2018-08-09T05:21:51.719843: step 9538, loss 0.495113.
Train: 2018-08-09T05:21:54.807051: step 9539, loss 0.663346.
Train: 2018-08-09T05:21:57.897267: step 9540, loss 0.545575.
Test: 2018-08-09T05:22:18.968289: step 9540, loss 0.547286.
Train: 2018-08-09T05:22:22.087582: step 9541, loss 0.545593.
Train: 2018-08-09T05:22:25.201862: step 9542, loss 0.579176.
Train: 2018-08-09T05:22:28.268014: step 9543, loss 0.679783.
Train: 2018-08-09T05:22:31.353217: step 9544, loss 0.51224.
Train: 2018-08-09T05:22:34.437417: step 9545, loss 0.612502.
Train: 2018-08-09T05:22:37.492540: step 9546, loss 0.412566.
Train: 2018-08-09T05:22:40.602809: step 9547, loss 0.629051.
Train: 2018-08-09T05:22:43.658934: step 9548, loss 0.462631.
Train: 2018-08-09T05:22:46.777225: step 9549, loss 0.662306.
Train: 2018-08-09T05:22:49.881478: step 9550, loss 0.529198.
Test: 2018-08-09T05:23:10.988597: step 9550, loss 0.548769.
Train: 2018-08-09T05:23:14.063772: step 9551, loss 0.545837.
Train: 2018-08-09T05:23:17.124912: step 9552, loss 0.662112.
Train: 2018-08-09T05:23:20.223149: step 9553, loss 0.529314.
Train: 2018-08-09T05:23:23.329407: step 9554, loss 0.545923.
Train: 2018-08-09T05:23:26.396562: step 9555, loss 0.595567.
Train: 2018-08-09T05:23:29.482767: step 9556, loss 0.579014.
Train: 2018-08-09T05:23:32.577997: step 9557, loss 0.546022.
Train: 2018-08-09T05:23:35.683253: step 9558, loss 0.611931.
Train: 2018-08-09T05:23:38.745394: step 9559, loss 0.595416.
Train: 2018-08-09T05:23:41.829594: step 9560, loss 0.611764.
Test: 2018-08-09T05:24:02.909640: step 9560, loss 0.549743.
Train: 2018-08-09T05:24:05.991835: step 9561, loss 0.562589.
Train: 2018-08-09T05:24:09.086061: step 9562, loss 0.546301.
Train: 2018-08-09T05:24:12.200342: step 9563, loss 0.497528.
Train: 2018-08-09T05:24:15.291560: step 9564, loss 0.595199.
Train: 2018-08-09T05:24:18.334651: step 9565, loss 0.708858.
Train: 2018-08-09T05:24:21.382755: step 9566, loss 0.643626.
Train: 2018-08-09T05:24:24.482998: step 9567, loss 0.449979.
Train: 2018-08-09T05:24:27.702558: step 9568, loss 0.514551.
Train: 2018-08-09T05:24:30.801798: step 9569, loss 0.578876.
Train: 2018-08-09T05:24:33.914072: step 9570, loss 0.530733.
Test: 2018-08-09T05:24:54.957021: step 9570, loss 0.549622.
Train: 2018-08-09T05:24:58.068292: step 9571, loss 0.627019.
Train: 2018-08-09T05:25:01.169537: step 9572, loss 0.514804.
Train: 2018-08-09T05:25:04.255742: step 9573, loss 0.48283.
Train: 2018-08-09T05:25:07.390076: step 9574, loss 0.610925.
Train: 2018-08-09T05:25:10.461241: step 9575, loss 0.594885.
Train: 2018-08-09T05:25:13.571511: step 9576, loss 0.65897.
Train: 2018-08-09T05:25:16.661726: step 9577, loss 0.498924.
Train: 2018-08-09T05:25:19.732892: step 9578, loss 0.467003.
Train: 2018-08-09T05:25:22.808068: step 9579, loss 0.514863.
Train: 2018-08-09T05:25:25.912321: step 9580, loss 0.498704.
Test: 2018-08-09T05:25:47.007408: step 9580, loss 0.548334.
Train: 2018-08-09T05:25:50.087597: step 9581, loss 0.498471.
Train: 2018-08-09T05:25:53.150741: step 9582, loss 0.514318.
Train: 2018-08-09T05:25:56.222909: step 9583, loss 0.59511.
Train: 2018-08-09T05:25:59.304101: step 9584, loss 0.530123.
Train: 2018-08-09T05:26:02.386295: step 9585, loss 0.644216.
Train: 2018-08-09T05:26:05.461471: step 9586, loss 0.578943.
Train: 2018-08-09T05:26:08.549682: step 9587, loss 0.546206.
Train: 2018-08-09T05:26:11.640913: step 9588, loss 0.578961.
Train: 2018-08-09T05:26:14.740141: step 9589, loss 0.578967.
Train: 2018-08-09T05:26:17.808298: step 9590, loss 0.595396.
Test: 2018-08-09T05:26:38.903385: step 9590, loss 0.547159.
Train: 2018-08-09T05:26:41.985579: step 9591, loss 0.61182.
Train: 2018-08-09T05:26:45.081811: step 9592, loss 0.447681.
Train: 2018-08-09T05:26:48.148966: step 9593, loss 0.546112.
Train: 2018-08-09T05:26:51.225145: step 9594, loss 0.595426.
Train: 2018-08-09T05:26:54.317366: step 9595, loss 0.546049.
Train: 2018-08-09T05:26:57.404574: step 9596, loss 0.529579.
Train: 2018-08-09T05:27:00.553947: step 9597, loss 0.529531.
Train: 2018-08-09T05:27:03.635139: step 9598, loss 0.496442.
Train: 2018-08-09T05:27:06.712320: step 9599, loss 0.595566.
Train: 2018-08-09T05:27:09.784489: step 9600, loss 0.562474.
Test: 2018-08-09T05:27:30.897623: step 9600, loss 0.549396.
Train: 2018-08-09T05:27:35.765565: step 9601, loss 0.512604.
Train: 2018-08-09T05:27:38.836730: step 9602, loss 0.512571.
Train: 2018-08-09T05:27:41.923939: step 9603, loss 0.629154.
Train: 2018-08-09T05:27:44.980064: step 9604, loss 0.595831.
Train: 2018-08-09T05:27:48.082312: step 9605, loss 0.528995.
Train: 2018-08-09T05:27:51.197595: step 9606, loss 0.528992.
Train: 2018-08-09T05:27:54.307864: step 9607, loss 0.562404.
Train: 2018-08-09T05:27:57.387051: step 9608, loss 0.713142.
Train: 2018-08-09T05:28:00.459219: step 9609, loss 0.495547.
Train: 2018-08-09T05:28:03.557456: step 9610, loss 0.595832.
Test: 2018-08-09T05:28:24.606420: step 9610, loss 0.54994.
Train: 2018-08-09T05:28:27.689617: step 9611, loss 0.545732.
Train: 2018-08-09T05:28:30.780835: step 9612, loss 0.545751.
Train: 2018-08-09T05:28:33.869046: step 9613, loss 0.512427.
Train: 2018-08-09T05:28:36.935198: step 9614, loss 0.545757.
Train: 2018-08-09T05:28:40.002353: step 9615, loss 0.545748.
Train: 2018-08-09T05:28:43.090564: step 9616, loss 0.545736.
Train: 2018-08-09T05:28:46.199830: step 9617, loss 0.529022.
Train: 2018-08-09T05:28:49.274004: step 9618, loss 0.629277.
Train: 2018-08-09T05:28:52.368230: step 9619, loss 0.562414.
Train: 2018-08-09T05:28:55.439396: step 9620, loss 0.529.
Test: 2018-08-09T05:29:16.569576: step 9620, loss 0.547389.
Train: 2018-08-09T05:29:19.662799: step 9621, loss 0.64597.
Train: 2018-08-09T05:29:22.743991: step 9622, loss 0.529041.
Train: 2018-08-09T05:29:25.843231: step 9623, loss 0.579106.
Train: 2018-08-09T05:29:28.964530: step 9624, loss 0.579096.
Train: 2018-08-09T05:29:32.047727: step 9625, loss 0.579084.
Train: 2018-08-09T05:29:35.152983: step 9626, loss 0.512585.
Train: 2018-08-09T05:29:38.244202: step 9627, loss 0.612296.
Train: 2018-08-09T05:29:41.335421: step 9628, loss 0.562469.
Train: 2018-08-09T05:29:44.447695: step 9629, loss 0.446494.
Train: 2018-08-09T05:29:47.523874: step 9630, loss 0.612208.
Test: 2018-08-09T05:30:08.644028: step 9630, loss 0.550075.
Train: 2018-08-09T05:30:11.788387: step 9631, loss 0.56247.
Train: 2018-08-09T05:30:14.694112: step 9632, loss 0.580149.
Train: 2018-08-09T05:30:17.817416: step 9633, loss 0.595591.
Train: 2018-08-09T05:30:20.901616: step 9634, loss 0.479799.
Train: 2018-08-09T05:30:23.991832: step 9635, loss 0.562492.
Train: 2018-08-09T05:30:27.101099: step 9636, loss 0.529389.
Train: 2018-08-09T05:30:30.176275: step 9637, loss 0.645267.
Train: 2018-08-09T05:30:33.258470: step 9638, loss 0.479762.
Train: 2018-08-09T05:30:36.348686: step 9639, loss 0.545926.
Train: 2018-08-09T05:30:39.411830: step 9640, loss 0.529345.
Test: 2018-08-09T05:31:00.507919: step 9640, loss 0.549437.
Train: 2018-08-09T05:31:03.609164: step 9641, loss 0.562466.
Train: 2018-08-09T05:31:06.686345: step 9642, loss 0.512663.
Train: 2018-08-09T05:31:09.775558: step 9643, loss 0.462699.
Train: 2018-08-09T05:31:12.898862: step 9644, loss 0.512408.
Train: 2018-08-09T05:31:15.985068: step 9645, loss 0.545679.
Train: 2018-08-09T05:31:19.067263: step 9646, loss 0.579174.
Train: 2018-08-09T05:31:22.155473: step 9647, loss 0.663325.
Train: 2018-08-09T05:31:25.244687: step 9648, loss 0.697044.
Train: 2018-08-09T05:31:28.334903: step 9649, loss 0.495167.
Train: 2018-08-09T05:31:31.400052: step 9650, loss 0.64636.
Test: 2018-08-09T05:31:52.473080: step 9650, loss 0.549228.
Train: 2018-08-09T05:31:55.552266: step 9651, loss 0.595921.
Train: 2018-08-09T05:31:58.635463: step 9652, loss 0.662741.
Train: 2018-08-09T05:32:01.729690: step 9653, loss 0.529119.
Train: 2018-08-09T05:32:04.828930: step 9654, loss 0.545847.
Train: 2018-08-09T05:32:07.929173: step 9655, loss 0.545907.
Train: 2018-08-09T05:32:11.012370: step 9656, loss 0.595555.
Train: 2018-08-09T05:32:14.080528: step 9657, loss 0.513034.
Train: 2018-08-09T05:32:17.173752: step 9658, loss 0.529586.
Train: 2018-08-09T05:32:20.250933: step 9659, loss 0.480246.
Train: 2018-08-09T05:32:23.332125: step 9660, loss 0.529587.
Test: 2018-08-09T05:32:44.385099: step 9660, loss 0.54769.
Train: 2018-08-09T05:32:47.487347: step 9661, loss 0.513052.
Train: 2018-08-09T05:32:50.568539: step 9662, loss 0.545995.
Train: 2018-08-09T05:32:53.647726: step 9663, loss 0.512805.
Train: 2018-08-09T05:32:56.773035: step 9664, loss 0.595678.
Train: 2018-08-09T05:32:59.858238: step 9665, loss 0.662174.
Train: 2018-08-09T05:33:02.945446: step 9666, loss 0.595639.
Train: 2018-08-09T05:33:06.051705: step 9667, loss 0.579062.
Train: 2018-08-09T05:33:09.155958: step 9668, loss 0.678463.
Train: 2018-08-09T05:33:12.251187: step 9669, loss 0.579011.
Train: 2018-08-09T05:33:15.333382: step 9670, loss 0.496669.
Test: 2018-08-09T05:33:36.405408: step 9670, loss 0.548383.
Train: 2018-08-09T05:33:39.552775: step 9671, loss 0.562541.
Train: 2018-08-09T05:33:42.621935: step 9672, loss 0.496906.
Train: 2018-08-09T05:33:45.719169: step 9673, loss 0.496929.
Train: 2018-08-09T05:33:48.817407: step 9674, loss 0.529711.
Train: 2018-08-09T05:33:51.925671: step 9675, loss 0.595418.
Train: 2018-08-09T05:33:55.001849: step 9676, loss 0.59543.
Train: 2018-08-09T05:33:58.097079: step 9677, loss 0.529642.
Train: 2018-08-09T05:34:01.196319: step 9678, loss 0.595438.
Train: 2018-08-09T05:34:04.313607: step 9679, loss 0.628334.
Train: 2018-08-09T05:34:07.387781: step 9680, loss 0.480399.
Test: 2018-08-09T05:34:28.457800: step 9680, loss 0.550876.
Train: 2018-08-09T05:34:31.572080: step 9681, loss 0.677566.
Train: 2018-08-09T05:34:34.662296: step 9682, loss 0.480545.
Train: 2018-08-09T05:34:37.746496: step 9683, loss 0.496968.
Train: 2018-08-09T05:34:40.843730: step 9684, loss 0.611791.
Train: 2018-08-09T05:34:43.923920: step 9685, loss 0.496914.
Train: 2018-08-09T05:34:47.039202: step 9686, loss 0.54612.
Train: 2018-08-09T05:34:50.135434: step 9687, loss 0.562536.
Train: 2018-08-09T05:34:53.211613: step 9688, loss 0.644829.
Train: 2018-08-09T05:34:56.289797: step 9689, loss 0.496724.
Train: 2018-08-09T05:34:59.356952: step 9690, loss 0.578989.
Test: 2018-08-09T05:35:20.439004: step 9690, loss 0.551455.
Train: 2018-08-09T05:35:23.540249: step 9691, loss 0.562525.
Train: 2018-08-09T05:35:26.661547: step 9692, loss 0.611928.
Train: 2018-08-09T05:35:29.766803: step 9693, loss 0.546072.
Train: 2018-08-09T05:35:32.828945: step 9694, loss 0.694139.
Train: 2018-08-09T05:35:35.955257: step 9695, loss 0.578966.
Train: 2018-08-09T05:35:39.036449: step 9696, loss 0.611684.
Train: 2018-08-09T05:35:42.124659: step 9697, loss 0.546298.
Train: 2018-08-09T05:35:45.222897: step 9698, loss 0.578918.
Train: 2018-08-09T05:35:48.297070: step 9699, loss 0.54644.
Train: 2018-08-09T05:35:51.380268: step 9700, loss 0.611297.
Test: 2018-08-09T05:36:12.482373: step 9700, loss 0.548813.
Train: 2018-08-09T05:36:17.280128: step 9701, loss 0.611202.
Train: 2018-08-09T05:36:20.376360: step 9702, loss 0.546664.
Train: 2018-08-09T05:36:23.498661: step 9703, loss 0.578871.
Train: 2018-08-09T05:36:26.547768: step 9704, loss 0.56284.
Train: 2018-08-09T05:36:29.620939: step 9705, loss 0.49891.
Train: 2018-08-09T05:36:32.696115: step 9706, loss 0.498973.
Train: 2018-08-09T05:36:35.772294: step 9707, loss 0.546892.
Train: 2018-08-09T05:36:38.846467: step 9708, loss 0.514874.
Train: 2018-08-09T05:36:41.960747: step 9709, loss 0.578865.
Train: 2018-08-09T05:36:45.035923: step 9710, loss 0.530742.
Test: 2018-08-09T05:37:06.162092: step 9710, loss 0.548961.
Train: 2018-08-09T05:37:09.256319: step 9711, loss 0.594941.
Train: 2018-08-09T05:37:12.332497: step 9712, loss 0.498444.
Train: 2018-08-09T05:37:15.447780: step 9713, loss 0.530522.
Train: 2018-08-09T05:37:18.536993: step 9714, loss 0.530411.
Train: 2018-08-09T05:37:21.610164: step 9715, loss 0.481674.
Train: 2018-08-09T05:37:24.697372: step 9716, loss 0.530115.
Train: 2018-08-09T05:37:27.823684: step 9717, loss 0.595269.
Train: 2018-08-09T05:37:30.915905: step 9718, loss 0.54619.
Train: 2018-08-09T05:37:33.989076: step 9719, loss 0.611839.
Train: 2018-08-09T05:37:37.059239: step 9720, loss 0.562526.
Test: 2018-08-09T05:37:58.116224: step 9720, loss 0.549556.
Train: 2018-08-09T05:38:01.281639: step 9721, loss 0.562512.
Train: 2018-08-09T05:38:04.364837: step 9722, loss 0.595526.
Train: 2018-08-09T05:38:07.436002: step 9723, loss 0.529443.
Train: 2018-08-09T05:38:10.537247: step 9724, loss 0.479765.
Train: 2018-08-09T05:38:13.639495: step 9725, loss 0.479558.
Train: 2018-08-09T05:38:16.715675: step 9726, loss 0.529168.
Train: 2018-08-09T05:38:19.797869: step 9727, loss 0.579116.
Train: 2018-08-09T05:38:22.865024: step 9728, loss 0.579147.
Train: 2018-08-09T05:38:25.973288: step 9729, loss 0.49529.
Train: 2018-08-09T05:38:29.031418: step 9730, loss 0.528732.
Test: 2018-08-09T05:38:50.073364: step 9730, loss 0.549109.
Train: 2018-08-09T05:38:53.155558: step 9731, loss 0.596116.
Train: 2018-08-09T05:38:56.261816: step 9732, loss 0.579268.
Train: 2018-08-09T05:38:59.345014: step 9733, loss 0.51156.
Train: 2018-08-09T05:39:02.432222: step 9734, loss 0.528417.
Train: 2018-08-09T05:39:05.533467: step 9735, loss 0.630358.
Train: 2018-08-09T05:39:08.620675: step 9736, loss 0.511298.
Train: 2018-08-09T05:39:11.734955: step 9737, loss 0.664563.
Train: 2018-08-09T05:39:14.826174: step 9738, loss 0.562341.
Train: 2018-08-09T05:39:17.911377: step 9739, loss 0.494296.
Train: 2018-08-09T05:39:20.970510: step 9740, loss 0.494274.
Test: 2018-08-09T05:39:42.002428: step 9740, loss 0.547023.
Train: 2018-08-09T05:39:45.090639: step 9741, loss 0.460121.
Train: 2018-08-09T05:39:48.210935: step 9742, loss 0.562338.
Train: 2018-08-09T05:39:51.288116: step 9743, loss 0.613674.
Train: 2018-08-09T05:39:54.359282: step 9744, loss 0.528081.
Train: 2018-08-09T05:39:57.447492: step 9745, loss 0.510895.
Train: 2018-08-09T05:40:00.511639: step 9746, loss 0.613858.
Train: 2018-08-09T05:40:03.625919: step 9747, loss 0.52797.
Train: 2018-08-09T05:40:06.729170: step 9748, loss 0.596727.
Train: 2018-08-09T05:40:09.835428: step 9749, loss 0.596727.
Train: 2018-08-09T05:40:12.931660: step 9750, loss 0.648252.
Test: 2018-08-09T05:40:33.985638: step 9750, loss 0.547566.
Train: 2018-08-09T05:40:37.100920: step 9751, loss 0.648071.
Train: 2018-08-09T05:40:40.170080: step 9752, loss 0.596516.
Train: 2018-08-09T05:40:43.231219: step 9753, loss 0.562343.
Train: 2018-08-09T05:40:46.286341: step 9754, loss 0.545385.
Train: 2018-08-09T05:40:49.367533: step 9755, loss 0.613098.
Train: 2018-08-09T05:40:52.476800: step 9756, loss 0.562376.
Train: 2018-08-09T05:40:55.566013: step 9757, loss 0.579184.
Train: 2018-08-09T05:40:58.632165: step 9758, loss 0.646079.
Train: 2018-08-09T05:41:01.719374: step 9759, loss 0.662383.
Train: 2018-08-09T05:41:04.821621: step 9760, loss 0.529353.
Test: 2018-08-09T05:41:25.883620: step 9760, loss 0.546449.
Train: 2018-08-09T05:41:28.967819: step 9761, loss 0.496586.
Train: 2018-08-09T05:41:32.068062: step 9762, loss 0.529688.
Train: 2018-08-09T05:41:35.153265: step 9763, loss 0.497016.
Train: 2018-08-09T05:41:38.238468: step 9764, loss 0.497102.
Train: 2018-08-09T05:41:41.342721: step 9765, loss 0.628061.
Train: 2018-08-09T05:41:44.404862: step 9766, loss 0.578944.
Train: 2018-08-09T05:41:47.473020: step 9767, loss 0.644267.
Train: 2018-08-09T05:41:50.586297: step 9768, loss 0.530046.
Train: 2018-08-09T05:41:53.668492: step 9769, loss 0.627708.
Train: 2018-08-09T05:41:56.731636: step 9770, loss 0.578902.
Test: 2018-08-09T05:42:17.833741: step 9770, loss 0.549387.
Train: 2018-08-09T05:42:20.885855: step 9771, loss 0.530352.
Train: 2018-08-09T05:42:23.975069: step 9772, loss 0.481978.
Train: 2018-08-09T05:42:27.055258: step 9773, loss 0.595034.
Train: 2018-08-09T05:42:30.139458: step 9774, loss 0.498197.
Train: 2018-08-09T05:42:33.214634: step 9775, loss 0.546593.
Train: 2018-08-09T05:42:36.279783: step 9776, loss 0.595046.
Train: 2018-08-09T05:42:39.359973: step 9777, loss 0.578888.
Train: 2018-08-09T05:42:42.454199: step 9778, loss 0.530407.
Train: 2018-08-09T05:42:45.527370: step 9779, loss 0.595058.
Train: 2018-08-09T05:42:48.643655: step 9780, loss 0.675897.
Test: 2018-08-09T05:43:09.727713: step 9780, loss 0.549457.
Train: 2018-08-09T05:43:12.841992: step 9781, loss 0.465926.
Train: 2018-08-09T05:43:15.952261: step 9782, loss 0.691833.
Train: 2018-08-09T05:43:19.034456: step 9783, loss 0.530575.
Train: 2018-08-09T05:43:22.119659: step 9784, loss 0.418093.
Train: 2018-08-09T05:43:25.189821: step 9785, loss 0.578873.
Train: 2018-08-09T05:43:28.263995: step 9786, loss 0.594996.
Train: 2018-08-09T05:43:31.351203: step 9787, loss 0.595001.
Train: 2018-08-09T05:43:34.436406: step 9788, loss 0.482188.
Train: 2018-08-09T05:43:37.541662: step 9789, loss 0.530478.
Train: 2018-08-09T05:43:40.617840: step 9790, loss 0.643545.
Test: 2018-08-09T05:44:01.705909: step 9790, loss 0.548189.
Train: 2018-08-09T05:44:04.816177: step 9791, loss 0.643546.
Train: 2018-08-09T05:44:07.982596: step 9792, loss 0.578885.
Train: 2018-08-09T05:44:11.066796: step 9793, loss 0.627254.
Train: 2018-08-09T05:44:14.142974: step 9794, loss 0.546696.
Train: 2018-08-09T05:44:17.245223: step 9795, loss 0.594932.
Train: 2018-08-09T05:44:20.353487: step 9796, loss 0.659015.
Train: 2018-08-09T05:44:23.434678: step 9797, loss 0.51496.
Train: 2018-08-09T05:44:26.549961: step 9798, loss 0.5948.
Train: 2018-08-09T05:44:29.610097: step 9799, loss 0.547054.
Train: 2018-08-09T05:44:32.716356: step 9800, loss 0.610605.
Test: 2018-08-09T05:44:53.790386: step 9800, loss 0.548785.
Train: 2018-08-09T05:44:58.666350: step 9801, loss 0.531352.
Train: 2018-08-09T05:45:01.751553: step 9802, loss 0.610486.
Train: 2018-08-09T05:45:04.854803: step 9803, loss 0.610424.
Train: 2018-08-09T05:45:07.963067: step 9804, loss 0.641827.
Train: 2018-08-09T05:45:11.034233: step 9805, loss 0.610242.
Train: 2018-08-09T05:45:14.148513: step 9806, loss 0.532023.
Train: 2018-08-09T05:45:17.216670: step 9807, loss 0.56332.
Train: 2018-08-09T05:45:20.328945: step 9808, loss 0.516747.
Train: 2018-08-09T05:45:23.396099: step 9809, loss 0.563389.
Train: 2018-08-09T05:45:26.464257: step 9810, loss 0.501363.
Test: 2018-08-09T05:45:47.510213: step 9810, loss 0.550612.
Train: 2018-08-09T05:45:50.670615: step 9811, loss 0.625473.
Train: 2018-08-09T05:45:53.774868: step 9812, loss 0.501347.
Train: 2018-08-09T05:45:56.838012: step 9813, loss 0.578911.
Train: 2018-08-09T05:45:59.931236: step 9814, loss 0.501206.
Train: 2018-08-09T05:46:03.003404: step 9815, loss 0.672328.
Train: 2018-08-09T05:46:06.084596: step 9816, loss 0.578899.
Train: 2018-08-09T05:46:09.176818: step 9817, loss 0.532191.
Train: 2018-08-09T05:46:12.269039: step 9818, loss 0.407521.
Train: 2018-08-09T05:46:15.388332: step 9819, loss 0.59452.
Train: 2018-08-09T05:46:18.486570: step 9820, loss 0.610234.
Test: 2018-08-09T05:46:39.559598: step 9820, loss 0.551417.
Train: 2018-08-09T05:46:42.628757: step 9821, loss 0.610287.
Train: 2018-08-09T05:46:45.694909: step 9822, loss 0.547425.
Train: 2018-08-09T05:46:48.767077: step 9823, loss 0.626093.
Train: 2018-08-09T05:46:51.871330: step 9824, loss 0.6261.
Train: 2018-08-09T05:46:54.962549: step 9825, loss 0.453023.
Train: 2018-08-09T05:46:58.051763: step 9826, loss 0.610369.
Train: 2018-08-09T05:47:01.134960: step 9827, loss 0.578865.
Train: 2018-08-09T05:47:04.258264: step 9828, loss 0.484284.
Train: 2018-08-09T05:47:07.360512: step 9829, loss 0.594654.
Train: 2018-08-09T05:47:10.442707: step 9830, loss 0.578861.
Test: 2018-08-09T05:47:31.534785: step 9830, loss 0.550604.
Train: 2018-08-09T05:47:34.623998: step 9831, loss 0.610507.
Train: 2018-08-09T05:47:37.695164: step 9832, loss 0.594685.
Train: 2018-08-09T05:47:40.760313: step 9833, loss 0.547223.
Train: 2018-08-09T05:47:43.846518: step 9834, loss 0.626318.
Train: 2018-08-09T05:47:46.920692: step 9835, loss 0.499841.
Train: 2018-08-09T05:47:50.009905: step 9836, loss 0.642101.
Train: 2018-08-09T05:47:53.084078: step 9837, loss 0.673632.
Train: 2018-08-09T05:47:56.211393: step 9838, loss 0.531608.
Train: 2018-08-09T05:47:59.268521: step 9839, loss 0.61032.
Train: 2018-08-09T05:48:02.357734: step 9840, loss 0.547496.
Test: 2018-08-09T05:48:23.445802: step 9840, loss 0.550314.
Train: 2018-08-09T05:48:26.565095: step 9841, loss 0.516221.
Train: 2018-08-09T05:48:29.690404: step 9842, loss 0.437966.
Train: 2018-08-09T05:48:32.796663: step 9843, loss 0.500434.
Train: 2018-08-09T05:48:35.900916: step 9844, loss 0.547396.
Train: 2018-08-09T05:48:39.038258: step 9845, loss 0.610434.
Train: 2018-08-09T05:48:42.137498: step 9846, loss 0.563042.
Train: 2018-08-09T05:48:45.219693: step 9847, loss 0.61056.
Train: 2018-08-09T05:48:48.287850: step 9848, loss 0.578859.
Train: 2018-08-09T05:48:51.405138: step 9849, loss 0.65826.
Train: 2018-08-09T05:48:54.521423: step 9850, loss 0.562992.
Test: 2018-08-09T05:49:15.642579: step 9850, loss 0.551144.
Train: 2018-08-09T05:49:18.711739: step 9851, loss 0.563003.
Train: 2018-08-09T05:49:21.806968: step 9852, loss 0.610556.
Train: 2018-08-09T05:49:24.891168: step 9853, loss 0.56303.
Train: 2018-08-09T05:49:27.982387: step 9854, loss 0.547231.
Train: 2018-08-09T05:49:31.065584: step 9855, loss 0.594669.
Train: 2018-08-09T05:49:34.139757: step 9856, loss 0.531479.
Train: 2018-08-09T05:49:37.232982: step 9857, loss 0.594655.
Train: 2018-08-09T05:49:40.336232: step 9858, loss 0.484154.
Train: 2018-08-09T05:49:43.421435: step 9859, loss 0.547258.
Train: 2018-08-09T05:49:46.507640: step 9860, loss 0.673798.
Test: 2018-08-09T05:50:07.598716: step 9860, loss 0.550026.
Train: 2018-08-09T05:50:10.678905: step 9861, loss 0.547236.
Train: 2018-08-09T05:50:13.773132: step 9862, loss 0.610479.
Train: 2018-08-09T05:50:16.843294: step 9863, loss 0.578861.
Train: 2018-08-09T05:50:19.944540: step 9864, loss 0.626206.
Train: 2018-08-09T05:50:22.990639: step 9865, loss 0.531609.
Train: 2018-08-09T05:50:26.060801: step 9866, loss 0.578869.
Train: 2018-08-09T05:50:29.166057: step 9867, loss 0.594592.
Train: 2018-08-09T05:50:32.290364: step 9868, loss 0.610274.
Train: 2018-08-09T05:50:35.360527: step 9869, loss 0.48486.
Train: 2018-08-09T05:50:38.451745: step 9870, loss 0.547542.
Test: 2018-08-09T05:50:59.509733: step 9870, loss 0.550892.
Train: 2018-08-09T05:51:02.638050: step 9871, loss 0.500524.
Train: 2018-08-09T05:51:05.725258: step 9872, loss 0.500393.
Train: 2018-08-09T05:51:08.831517: step 9873, loss 0.563118.
Train: 2018-08-09T05:51:11.938778: step 9874, loss 0.578881.
Train: 2018-08-09T05:51:15.026989: step 9875, loss 0.578877.
Train: 2018-08-09T05:51:18.131242: step 9876, loss 0.578856.
Train: 2018-08-09T05:51:21.209426: step 9877, loss 0.563005.
Train: 2018-08-09T05:51:24.281594: step 9878, loss 0.515298.
Train: 2018-08-09T05:51:27.338722: step 9879, loss 0.515207.
Train: 2018-08-09T05:51:30.431946: step 9880, loss 0.578898.
Test: 2018-08-09T05:51:51.489934: step 9880, loss 0.547258.
Train: 2018-08-09T05:51:54.575136: step 9881, loss 0.610881.
Train: 2018-08-09T05:51:57.681395: step 9882, loss 0.530836.
Train: 2018-08-09T05:52:00.753563: step 9883, loss 0.562795.
Train: 2018-08-09T05:52:03.850798: step 9884, loss 0.546721.
Train: 2018-08-09T05:52:06.948032: step 9885, loss 0.562758.
Train: 2018-08-09T05:52:10.021203: step 9886, loss 0.643408.
Train: 2018-08-09T05:52:13.097382: step 9887, loss 0.530476.
Train: 2018-08-09T05:52:16.200632: step 9888, loss 0.546571.
Train: 2018-08-09T05:52:19.298870: step 9889, loss 0.659711.
Train: 2018-08-09T05:52:22.343966: step 9890, loss 0.578853.
Test: 2018-08-09T05:52:43.430029: step 9890, loss 0.546385.
Train: 2018-08-09T05:52:46.516233: step 9891, loss 0.562722.
Train: 2018-08-09T05:52:49.616476: step 9892, loss 0.530521.
Train: 2018-08-09T05:52:52.713711: step 9893, loss 0.514309.
Train: 2018-08-09T05:52:55.798913: step 9894, loss 0.611103.
Train: 2018-08-09T05:52:58.913193: step 9895, loss 0.400932.
Train: 2018-08-09T05:53:02.004412: step 9896, loss 0.660071.
Train: 2018-08-09T05:53:05.089615: step 9897, loss 0.529457.
Train: 2018-08-09T05:53:08.214924: step 9898, loss 0.579042.
Train: 2018-08-09T05:53:11.306143: step 9899, loss 0.529328.
Train: 2018-08-09T05:53:14.402375: step 9900, loss 0.5447.
Test: 2018-08-09T05:53:35.466379: step 9900, loss 0.550516.
Train: 2018-08-09T05:53:40.269147: step 9901, loss 0.513923.
Train: 2018-08-09T05:53:43.369390: step 9902, loss 0.447463.
Train: 2018-08-09T05:53:46.437547: step 9903, loss 0.597975.
Train: 2018-08-09T05:53:49.535785: step 9904, loss 0.512978.
Train: 2018-08-09T05:53:52.602940: step 9905, loss 0.496012.
Train: 2018-08-09T05:53:55.703182: step 9906, loss 0.612432.
Train: 2018-08-09T05:53:58.786379: step 9907, loss 0.491571.
Train: 2018-08-09T05:54:01.860553: step 9908, loss 0.599484.
Train: 2018-08-09T05:54:04.933724: step 9909, loss 0.546644.
Train: 2018-08-09T05:54:08.021934: step 9910, loss 0.56249.
Test: 2018-08-09T05:54:29.094962: step 9910, loss 0.548183.
Train: 2018-08-09T05:54:32.129028: step 9911, loss 0.546561.
Train: 2018-08-09T05:54:35.229271: step 9912, loss 0.527975.
Train: 2018-08-09T05:54:38.337535: step 9913, loss 0.647496.
Train: 2018-08-09T05:54:41.432765: step 9914, loss 0.596165.
Train: 2018-08-09T05:54:44.520975: step 9915, loss 0.629315.
Train: 2018-08-09T05:54:47.599159: step 9916, loss 0.545811.
Train: 2018-08-09T05:54:50.740511: step 9917, loss 0.529055.
Train: 2018-08-09T05:54:53.828722: step 9918, loss 0.495717.
Train: 2018-08-09T05:54:56.889861: step 9919, loss 0.429463.
Train: 2018-08-09T05:54:59.960023: step 9920, loss 0.562405.
Test: 2018-08-09T05:55:21.014001: step 9920, loss 0.549314.
Train: 2018-08-09T05:55:24.104216: step 9921, loss 0.612499.
Train: 2018-08-09T05:55:27.225515: step 9922, loss 0.562387.
Train: 2018-08-09T05:55:30.323752: step 9923, loss 0.579145.
Train: 2018-08-09T05:55:33.398928: step 9924, loss 0.495609.
Train: 2018-08-09T05:55:36.473102: step 9925, loss 0.595862.
Train: 2018-08-09T05:55:39.565323: step 9926, loss 0.512225.
Train: 2018-08-09T05:55:42.667571: step 9927, loss 0.595897.
Train: 2018-08-09T05:55:45.731718: step 9928, loss 0.411653.
Train: 2018-08-09T05:55:48.829955: step 9929, loss 0.478423.
Train: 2018-08-09T05:55:51.919168: step 9930, loss 0.511815.
Test: 2018-08-09T05:56:13.019268: step 9930, loss 0.547787.
Train: 2018-08-09T05:56:16.148588: step 9931, loss 0.494691.
Train: 2018-08-09T05:56:19.234793: step 9932, loss 0.579338.
Train: 2018-08-09T05:56:22.144529: step 9933, loss 0.671463.
Train: 2018-08-09T05:56:25.232740: step 9934, loss 0.596482.
Train: 2018-08-09T05:56:28.325964: step 9935, loss 0.630658.
Train: 2018-08-09T05:56:31.403145: step 9936, loss 0.477.
Train: 2018-08-09T05:56:34.467292: step 9937, loss 0.613563.
Train: 2018-08-09T05:56:37.585583: step 9938, loss 0.494112.
Train: 2018-08-09T05:56:40.661762: step 9939, loss 0.630648.
Train: 2018-08-09T05:56:43.724905: step 9940, loss 0.647641.
Test: 2018-08-09T05:57:04.813976: step 9940, loss 0.551544.
Train: 2018-08-09T05:57:07.945312: step 9941, loss 0.528309.
Train: 2018-08-09T05:57:11.063591: step 9942, loss 0.494373.
Train: 2018-08-09T05:57:14.120720: step 9943, loss 0.528376.
Train: 2018-08-09T05:57:17.200909: step 9944, loss 0.52838.
Train: 2018-08-09T05:57:20.294133: step 9945, loss 0.562347.
Train: 2018-08-09T05:57:23.394376: step 9946, loss 0.596326.
Train: 2018-08-09T05:57:26.492613: step 9947, loss 0.596307.
Train: 2018-08-09T05:57:29.590850: step 9948, loss 0.630188.
Train: 2018-08-09T05:57:32.680064: step 9949, loss 0.562358.
Train: 2018-08-09T05:57:35.763261: step 9950, loss 0.629902.
Test: 2018-08-09T05:57:56.845313: step 9950, loss 0.54979.
Train: 2018-08-09T05:57:59.930515: step 9951, loss 0.495053.
Train: 2018-08-09T05:58:03.002683: step 9952, loss 0.545587.
Train: 2018-08-09T05:58:06.066830: step 9953, loss 0.512074.
Train: 2018-08-09T05:58:09.171083: step 9954, loss 0.528875.
Train: 2018-08-09T05:58:12.230217: step 9955, loss 0.528882.
Train: 2018-08-09T05:58:15.339484: step 9956, loss 0.612686.
Train: 2018-08-09T05:58:18.415662: step 9957, loss 0.595911.
Train: 2018-08-09T05:58:21.516908: step 9958, loss 0.629343.
Train: 2018-08-09T05:58:24.633193: step 9959, loss 0.595807.
Train: 2018-08-09T05:58:27.699345: step 9960, loss 0.512495.
Test: 2018-08-09T05:58:48.790421: step 9960, loss 0.549391.
Train: 2018-08-09T05:58:51.912721: step 9961, loss 0.628929.
Train: 2018-08-09T05:58:55.009956: step 9962, loss 0.612199.
Train: 2018-08-09T05:58:58.093154: step 9963, loss 0.545973.
Train: 2018-08-09T05:59:01.199412: step 9964, loss 0.562518.
Train: 2018-08-09T05:59:04.267570: step 9965, loss 0.51323.
Train: 2018-08-09T05:59:07.348762: step 9966, loss 0.562554.
Train: 2018-08-09T05:59:10.429954: step 9967, loss 0.497004.
Train: 2018-08-09T05:59:13.561279: step 9968, loss 0.562569.
Train: 2018-08-09T05:59:16.662525: step 9969, loss 0.578956.
Train: 2018-08-09T05:59:19.725669: step 9970, loss 0.677226.
Test: 2018-08-09T05:59:40.806718: step 9970, loss 0.549762.
Train: 2018-08-09T05:59:43.887909: step 9971, loss 0.546283.
Train: 2018-08-09T05:59:46.964088: step 9972, loss 0.578931.
Train: 2018-08-09T05:59:50.063328: step 9973, loss 0.660324.
Train: 2018-08-09T05:59:53.162568: step 9974, loss 0.660037.
Train: 2018-08-09T05:59:56.246768: step 9975, loss 0.62734.
Train: 2018-08-09T05:59:59.328963: step 9976, loss 0.62707.
Train: 2018-08-09T06:00:02.436224: step 9977, loss 0.562887.
Train: 2018-08-09T06:00:05.527443: step 9978, loss 0.594749.
Train: 2018-08-09T06:00:08.616656: step 9979, loss 0.578861.
Train: 2018-08-09T06:00:11.704867: step 9980, loss 0.484472.
Test: 2018-08-09T06:00:32.797948: step 9980, loss 0.549079.
Train: 2018-08-09T06:00:35.899193: step 9981, loss 0.594563.
Train: 2018-08-09T06:00:39.008460: step 9982, loss 0.469386.
Train: 2018-08-09T06:00:42.082633: step 9983, loss 0.516366.
Train: 2018-08-09T06:00:45.173851: step 9984, loss 0.625786.
Train: 2018-08-09T06:00:48.267076: step 9985, loss 0.688253.
Train: 2018-08-09T06:00:51.358294: step 9986, loss 0.532154.
Train: 2018-08-09T06:00:54.445502: step 9987, loss 0.532242.
Train: 2018-08-09T06:00:57.553766: step 9988, loss 0.516739.
Train: 2018-08-09T06:01:00.646990: step 9989, loss 0.61001.
Train: 2018-08-09T06:01:03.757259: step 9990, loss 0.578929.
Test: 2018-08-09T06:01:24.843322: step 9990, loss 0.551173.
Train: 2018-08-09T06:01:27.980663: step 9991, loss 0.594438.
Train: 2018-08-09T06:01:31.105972: step 9992, loss 0.547875.
Train: 2018-08-09T06:01:34.203207: step 9993, loss 0.625456.
Train: 2018-08-09T06:01:37.240282: step 9994, loss 0.57892.
Train: 2018-08-09T06:01:40.290391: step 9995, loss 0.563444.
Train: 2018-08-09T06:01:43.391636: step 9996, loss 0.563466.
Train: 2018-08-09T06:01:46.462802: step 9997, loss 0.501653.
Train: 2018-08-09T06:01:49.537978: step 9998, loss 0.486112.
Train: 2018-08-09T06:01:52.603127: step 9999, loss 0.609927.
Train: 2018-08-09T06:01:55.691338: step 10000, loss 0.501235.
Test: 2018-08-09T06:02:16.774393: step 10000, loss 0.549895.
Train: 2018-08-09T06:02:21.712521: step 10001, loss 0.610047.
Train: 2018-08-09T06:02:24.808753: step 10002, loss 0.469607.
Train: 2018-08-09T06:02:27.909999: step 10003, loss 0.422135.
Train: 2018-08-09T06:02:30.981164: step 10004, loss 0.547229.
Train: 2018-08-09T06:02:34.069375: step 10005, loss 0.483472.
Train: 2018-08-09T06:02:37.153575: step 10006, loss 0.612076.
Train: 2018-08-09T06:02:40.234767: step 10007, loss 0.627863.
Train: 2018-08-09T06:02:43.305932: step 10008, loss 0.642931.
Train: 2018-08-09T06:02:46.365066: step 10009, loss 0.659829.
Train: 2018-08-09T06:02:49.458289: step 10010, loss 0.578884.
Test: 2018-08-09T06:03:10.577440: step 10010, loss 0.551332.
Train: 2018-08-09T06:03:13.680690: step 10011, loss 0.498269.
Train: 2018-08-09T06:03:16.764890: step 10012, loss 0.530565.
Train: 2018-08-09T06:03:19.838061: step 10013, loss 0.594988.
Train: 2018-08-09T06:03:22.915242: step 10014, loss 0.530479.
Train: 2018-08-09T06:03:26.001448: step 10015, loss 0.54661.
Train: 2018-08-09T06:03:29.078629: step 10016, loss 0.546489.
Train: 2018-08-09T06:03:32.164834: step 10017, loss 0.578931.
Train: 2018-08-09T06:03:35.233994: step 10018, loss 0.59512.
Train: 2018-08-09T06:03:38.407432: step 10019, loss 0.514024.
Train: 2018-08-09T06:03:41.510682: step 10020, loss 0.530139.
Test: 2018-08-09T06:04:02.606771: step 10020, loss 0.549227.
Train: 2018-08-09T06:04:05.721051: step 10021, loss 0.497502.
Train: 2018-08-09T06:04:08.833326: step 10022, loss 0.562618.
Train: 2018-08-09T06:04:11.947606: step 10023, loss 0.57896.
Train: 2018-08-09T06:04:15.059880: step 10024, loss 0.578966.
Train: 2018-08-09T06:04:18.136059: step 10025, loss 0.546086.
Train: 2018-08-09T06:04:21.236302: step 10026, loss 0.59546.
Train: 2018-08-09T06:04:24.328523: step 10027, loss 0.59542.
Train: 2018-08-09T06:04:27.395678: step 10028, loss 0.59553.
Train: 2018-08-09T06:04:30.537030: step 10029, loss 0.562457.
Train: 2018-08-09T06:04:33.614211: step 10030, loss 0.513255.
Test: 2018-08-09T06:04:54.708295: step 10030, loss 0.549604.
Train: 2018-08-09T06:04:57.775449: step 10031, loss 0.529543.
Train: 2018-08-09T06:05:00.859650: step 10032, loss 0.529687.
Train: 2018-08-09T06:05:03.939839: step 10033, loss 0.529494.
Train: 2018-08-09T06:05:07.017020: step 10034, loss 0.611944.
Train: 2018-08-09T06:05:10.111247: step 10035, loss 0.529441.
Train: 2018-08-09T06:05:13.190433: step 10036, loss 0.529522.
Train: 2018-08-09T06:05:16.245556: step 10037, loss 0.56254.
Train: 2018-08-09T06:05:19.355826: step 10038, loss 0.612299.
Train: 2018-08-09T06:05:22.433007: step 10039, loss 0.529389.
Train: 2018-08-09T06:05:25.560322: step 10040, loss 0.562404.
Test: 2018-08-09T06:05:46.701531: step 10040, loss 0.54752.
Train: 2018-08-09T06:05:49.786733: step 10041, loss 0.545869.
Train: 2018-08-09T06:05:52.914048: step 10042, loss 0.69536.
Train: 2018-08-09T06:05:55.997245: step 10043, loss 0.595504.
Train: 2018-08-09T06:05:59.126565: step 10044, loss 0.562461.
Train: 2018-08-09T06:06:02.197730: step 10045, loss 0.480085.
Train: 2018-08-09T06:06:05.280928: step 10046, loss 0.628549.
Train: 2018-08-09T06:06:08.370141: step 10047, loss 0.644953.
Train: 2018-08-09T06:06:11.471387: step 10048, loss 0.628203.
Train: 2018-08-09T06:06:14.602712: step 10049, loss 0.48077.
Train: 2018-08-09T06:06:17.688917: step 10050, loss 0.611647.
Test: 2018-08-09T06:06:38.764953: step 10050, loss 0.5486.
Train: 2018-08-09T06:06:41.903296: step 10051, loss 0.59519.
Train: 2018-08-09T06:06:44.983486: step 10052, loss 0.660121.
Train: 2018-08-09T06:06:48.083728: step 10053, loss 0.465647.
Train: 2018-08-09T06:06:51.166926: step 10054, loss 0.594949.
Train: 2018-08-09T06:06:54.251126: step 10055, loss 0.498325.
Train: 2018-08-09T06:06:57.313267: step 10056, loss 0.498442.
Train: 2018-08-09T06:07:00.420529: step 10057, loss 0.530664.
Train: 2018-08-09T06:07:03.498713: step 10058, loss 0.546708.
Train: 2018-08-09T06:07:06.595948: step 10059, loss 0.49826.
Train: 2018-08-09T06:07:09.738302: step 10060, loss 0.6274.
Test: 2018-08-09T06:07:30.817346: step 10060, loss 0.548774.
Train: 2018-08-09T06:07:33.924607: step 10061, loss 0.530348.
Train: 2018-08-09T06:07:37.003794: step 10062, loss 0.546492.
Train: 2018-08-09T06:07:40.118073: step 10063, loss 0.48154.
Train: 2018-08-09T06:07:43.184225: step 10064, loss 0.513836.
Train: 2018-08-09T06:07:46.233332: step 10065, loss 0.51363.
Train: 2018-08-09T06:07:49.345607: step 10066, loss 0.464221.
Train: 2018-08-09T06:07:52.425796: step 10067, loss 0.611972.
Train: 2018-08-09T06:07:55.535063: step 10068, loss 0.430131.
Train: 2018-08-09T06:07:58.610239: step 10069, loss 0.629263.
Train: 2018-08-09T06:08:01.708476: step 10070, loss 0.612484.
Test: 2018-08-09T06:08:22.800555: step 10070, loss 0.548626.
Train: 2018-08-09T06:08:25.899794: step 10071, loss 0.512208.
Train: 2018-08-09T06:08:28.991013: step 10072, loss 0.562436.
Train: 2018-08-09T06:08:32.083234: step 10073, loss 0.579204.
Train: 2018-08-09T06:08:35.182475: step 10074, loss 0.61285.
Train: 2018-08-09T06:08:38.269683: step 10075, loss 0.511834.
Train: 2018-08-09T06:08:41.364912: step 10076, loss 0.511566.
Train: 2018-08-09T06:08:44.462146: step 10077, loss 0.511546.
Train: 2018-08-09T06:08:47.549354: step 10078, loss 0.596314.
Train: 2018-08-09T06:08:50.646589: step 10079, loss 0.544961.
Train: 2018-08-09T06:08:53.733797: step 10080, loss 0.666178.
Test: 2018-08-09T06:09:14.821865: step 10080, loss 0.546934.
Train: 2018-08-09T06:09:17.971238: step 10081, loss 0.579791.
Train: 2018-08-09T06:09:21.051427: step 10082, loss 0.493913.
Train: 2018-08-09T06:09:24.144651: step 10083, loss 0.630558.
Train: 2018-08-09T06:09:27.220830: step 10084, loss 0.579485.
Train: 2018-08-09T06:09:30.325083: step 10085, loss 0.494627.
Train: 2018-08-09T06:09:33.431342: step 10086, loss 0.528546.
Train: 2018-08-09T06:09:36.538603: step 10087, loss 0.511647.
Train: 2018-08-09T06:09:39.648873: step 10088, loss 0.5116.
Train: 2018-08-09T06:09:42.727057: step 10089, loss 0.4607.
Train: 2018-08-09T06:09:45.845347: step 10090, loss 0.596339.
Test: 2018-08-09T06:10:06.943442: step 10090, loss 0.548968.
Train: 2018-08-09T06:10:10.037668: step 10091, loss 0.579352.
Train: 2018-08-09T06:10:13.195063: step 10092, loss 0.426021.
Train: 2018-08-09T06:10:16.272244: step 10093, loss 0.511022.
Train: 2018-08-09T06:10:19.353436: step 10094, loss 0.579583.
Train: 2018-08-09T06:10:22.440644: step 10095, loss 0.527897.
Train: 2018-08-09T06:10:25.559938: step 10096, loss 0.527968.
Train: 2018-08-09T06:10:28.648148: step 10097, loss 0.544716.
Train: 2018-08-09T06:10:31.731346: step 10098, loss 0.527214.
Train: 2018-08-09T06:10:34.825572: step 10099, loss 0.580181.
Train: 2018-08-09T06:10:37.924812: step 10100, loss 0.509547.
Test: 2018-08-09T06:10:58.996838: step 10100, loss 0.54658.
Train: 2018-08-09T06:11:03.858764: step 10101, loss 0.52731.
Train: 2018-08-09T06:11:06.957001: step 10102, loss 0.508507.
Train: 2018-08-09T06:11:10.052230: step 10103, loss 0.632474.
Train: 2018-08-09T06:11:13.142446: step 10104, loss 0.582203.
Train: 2018-08-09T06:11:16.196566: step 10105, loss 0.581465.
Train: 2018-08-09T06:11:19.288788: step 10106, loss 0.492016.
Train: 2018-08-09T06:11:22.392038: step 10107, loss 0.510396.
Train: 2018-08-09T06:11:25.508324: step 10108, loss 0.527594.
Train: 2018-08-09T06:11:28.583500: step 10109, loss 0.615881.
Train: 2018-08-09T06:11:31.706804: step 10110, loss 0.562835.
Test: 2018-08-09T06:11:52.749752: step 10110, loss 0.550544.
Train: 2018-08-09T06:11:55.871050: step 10111, loss 0.686834.
Train: 2018-08-09T06:11:58.946226: step 10112, loss 0.527683.
Train: 2018-08-09T06:12:02.014383: step 10113, loss 0.562395.
Train: 2018-08-09T06:12:05.121645: step 10114, loss 0.562408.
Train: 2018-08-09T06:12:08.231914: step 10115, loss 0.562339.
Train: 2018-08-09T06:12:11.325138: step 10116, loss 0.596963.
Train: 2018-08-09T06:12:14.451450: step 10117, loss 0.665913.
Train: 2018-08-09T06:12:17.527629: step 10118, loss 0.57953.
Train: 2018-08-09T06:12:20.603807: step 10119, loss 0.579446.
Train: 2018-08-09T06:12:23.688008: step 10120, loss 0.698697.
Test: 2018-08-09T06:12:44.776076: step 10120, loss 0.547765.
Train: 2018-08-09T06:12:47.837213: step 10121, loss 0.579306.
Train: 2018-08-09T06:12:50.944475: step 10122, loss 0.461339.
Train: 2018-08-09T06:12:54.038702: step 10123, loss 0.54562.
Train: 2018-08-09T06:12:57.122901: step 10124, loss 0.545682.
Train: 2018-08-09T06:13:00.204093: step 10125, loss 0.562409.
Train: 2018-08-09T06:13:03.310352: step 10126, loss 0.612367.
Train: 2018-08-09T06:13:06.396558: step 10127, loss 0.512766.
Train: 2018-08-09T06:13:09.518859: step 10128, loss 0.512843.
Train: 2018-08-09T06:13:12.613086: step 10129, loss 0.612059.
Train: 2018-08-09T06:13:15.675227: step 10130, loss 0.463538.
Test: 2018-08-09T06:13:36.745247: step 10130, loss 0.547055.
Train: 2018-08-09T06:13:39.819420: step 10131, loss 0.611981.
Train: 2018-08-09T06:13:42.896601: step 10132, loss 0.546003.
Train: 2018-08-09T06:13:45.968769: step 10133, loss 0.595467.
Train: 2018-08-09T06:13:49.049961: step 10134, loss 0.612014.
Train: 2018-08-09T06:13:52.109095: step 10135, loss 0.513275.
Train: 2018-08-09T06:13:55.202319: step 10136, loss 0.628111.
Train: 2018-08-09T06:13:58.280503: step 10137, loss 0.448022.
Train: 2018-08-09T06:14:01.364703: step 10138, loss 0.546173.
Train: 2018-08-09T06:14:04.442887: step 10139, loss 0.464309.
Train: 2018-08-09T06:14:07.538116: step 10140, loss 0.595439.
Test: 2018-08-09T06:14:28.635208: step 10140, loss 0.548372.
Train: 2018-08-09T06:14:31.749487: step 10141, loss 0.529499.
Train: 2018-08-09T06:14:34.851735: step 10142, loss 0.644851.
Train: 2018-08-09T06:14:37.953984: step 10143, loss 0.480431.
Train: 2018-08-09T06:14:41.043197: step 10144, loss 0.562458.
Train: 2018-08-09T06:14:44.125391: step 10145, loss 0.595531.
Train: 2018-08-09T06:14:47.205581: step 10146, loss 0.546032.
Train: 2018-08-09T06:14:50.312843: step 10147, loss 0.463193.
Train: 2018-08-09T06:14:53.409074: step 10148, loss 0.512686.
Train: 2018-08-09T06:14:56.535386: step 10149, loss 0.595785.
Train: 2018-08-09T06:14:59.647661: step 10150, loss 0.495419.
Test: 2018-08-09T06:15:20.739740: step 10150, loss 0.549242.
Train: 2018-08-09T06:15:23.841987: step 10151, loss 0.595675.
Train: 2018-08-09T06:15:26.942230: step 10152, loss 0.529314.
Train: 2018-08-09T06:15:30.019411: step 10153, loss 0.528353.
Train: 2018-08-09T06:15:33.117648: step 10154, loss 0.51079.
Train: 2018-08-09T06:15:36.216888: step 10155, loss 0.511117.
Train: 2018-08-09T06:15:39.300086: step 10156, loss 0.650575.
Train: 2018-08-09T06:15:42.380275: step 10157, loss 0.495315.
Train: 2018-08-09T06:15:45.469489: step 10158, loss 0.545929.
Train: 2018-08-09T06:15:48.539651: step 10159, loss 0.614467.
Train: 2018-08-09T06:15:51.635883: step 10160, loss 0.512036.
Test: 2018-08-09T06:16:12.761050: step 10160, loss 0.548264.
Train: 2018-08-09T06:16:15.851265: step 10161, loss 0.459738.
Train: 2018-08-09T06:16:18.943487: step 10162, loss 0.562333.
Train: 2018-08-09T06:16:22.016657: step 10163, loss 0.528392.
Train: 2018-08-09T06:16:25.106873: step 10164, loss 0.562775.
Train: 2018-08-09T06:16:28.206114: step 10165, loss 0.596626.
Train: 2018-08-09T06:16:31.327412: step 10166, loss 0.596614.
Train: 2018-08-09T06:16:34.420636: step 10167, loss 0.528139.
Train: 2018-08-09T06:16:37.522884: step 10168, loss 0.493717.
Train: 2018-08-09T06:16:40.610092: step 10169, loss 0.630962.
Train: 2018-08-09T06:16:43.709332: step 10170, loss 0.493656.
Test: 2018-08-09T06:17:04.777346: step 10170, loss 0.546241.
Train: 2018-08-09T06:17:07.909674: step 10171, loss 0.716892.
Train: 2018-08-09T06:17:11.064061: step 10172, loss 0.579485.
Train: 2018-08-09T06:17:14.166309: step 10173, loss 0.442587.
Train: 2018-08-09T06:17:17.248504: step 10174, loss 0.528113.
Train: 2018-08-09T06:17:20.326688: step 10175, loss 0.63082.
Train: 2018-08-09T06:17:23.399859: step 10176, loss 0.698949.
Train: 2018-08-09T06:17:26.502106: step 10177, loss 0.579323.
Train: 2018-08-09T06:17:29.606360: step 10178, loss 0.613381.
Train: 2018-08-09T06:17:32.689557: step 10179, loss 0.62995.
Train: 2018-08-09T06:17:35.772754: step 10180, loss 0.511934.
Test: 2018-08-09T06:17:56.910956: step 10180, loss 0.548596.
Train: 2018-08-09T06:17:59.959059: step 10181, loss 0.5624.
Train: 2018-08-09T06:18:03.039249: step 10182, loss 0.562418.
Train: 2018-08-09T06:18:06.112419: step 10183, loss 0.512488.
Train: 2018-08-09T06:18:09.223691: step 10184, loss 0.3963.
Train: 2018-08-09T06:18:12.294856: step 10185, loss 0.529193.
Train: 2018-08-09T06:18:15.359003: step 10186, loss 0.629022.
Train: 2018-08-09T06:18:18.426158: step 10187, loss 0.579084.
Train: 2018-08-09T06:18:21.535425: step 10188, loss 0.495901.
Train: 2018-08-09T06:18:24.634665: step 10189, loss 0.462568.
Train: 2018-08-09T06:18:27.725883: step 10190, loss 0.562426.
Test: 2018-08-09T06:18:48.903189: step 10190, loss 0.547392.
Train: 2018-08-09T06:18:51.983377: step 10191, loss 0.529.
Train: 2018-08-09T06:18:55.109690: step 10192, loss 0.679596.
Train: 2018-08-09T06:18:58.173836: step 10193, loss 0.545666.
Train: 2018-08-09T06:19:01.264052: step 10194, loss 0.562404.
Train: 2018-08-09T06:19:04.340232: step 10195, loss 0.646101.
Train: 2018-08-09T06:19:07.452506: step 10196, loss 0.612554.
Train: 2018-08-09T06:19:10.562775: step 10197, loss 0.629139.
Train: 2018-08-09T06:19:13.651988: step 10198, loss 0.595702.
Train: 2018-08-09T06:19:16.749223: step 10199, loss 0.595602.
Train: 2018-08-09T06:19:19.849466: step 10200, loss 0.645018.
Test: 2018-08-09T06:19:40.945555: step 10200, loss 0.548398.
Train: 2018-08-09T06:19:45.805475: step 10201, loss 0.579005.
Train: 2018-08-09T06:19:48.908726: step 10202, loss 0.480867.
Train: 2018-08-09T06:19:51.964852: step 10203, loss 0.53003.
Train: 2018-08-09T06:19:55.032006: step 10204, loss 0.611474.
Train: 2018-08-09T06:19:58.112196: step 10205, loss 0.578907.
Train: 2018-08-09T06:20:01.216449: step 10206, loss 0.514117.
Train: 2018-08-09T06:20:04.315689: step 10207, loss 0.530376.
Train: 2018-08-09T06:20:07.403900: step 10208, loss 0.546569.
Train: 2018-08-09T06:20:10.492110: step 10209, loss 0.595039.
Train: 2018-08-09T06:20:13.592353: step 10210, loss 0.530466.
Test: 2018-08-09T06:20:34.694458: step 10210, loss 0.551288.
Train: 2018-08-09T06:20:37.769634: step 10211, loss 0.578883.
Train: 2018-08-09T06:20:40.843807: step 10212, loss 0.578882.
Train: 2018-08-09T06:20:43.941042: step 10213, loss 0.562759.
Train: 2018-08-09T06:20:47.034266: step 10214, loss 0.59499.
Train: 2018-08-09T06:20:50.113453: step 10215, loss 0.562778.
Train: 2018-08-09T06:20:53.222719: step 10216, loss 0.594958.
Train: 2018-08-09T06:20:56.359058: step 10217, loss 0.482483.
Train: 2018-08-09T06:20:59.455290: step 10218, loss 0.530662.
Train: 2018-08-09T06:21:02.538487: step 10219, loss 0.546705.
Train: 2018-08-09T06:21:05.610656: step 10220, loss 0.627185.
Test: 2018-08-09T06:21:26.740835: step 10220, loss 0.550122.
Train: 2018-08-09T06:21:29.856117: step 10221, loss 0.578876.
Train: 2018-08-09T06:21:32.968392: step 10222, loss 0.546678.
Train: 2018-08-09T06:21:36.069637: step 10223, loss 0.578874.
Train: 2018-08-09T06:21:39.184920: step 10224, loss 0.546676.
Train: 2018-08-09T06:21:42.283157: step 10225, loss 0.594982.
Train: 2018-08-09T06:21:45.384403: step 10226, loss 0.643259.
Train: 2018-08-09T06:21:48.497680: step 10227, loss 0.514595.
Train: 2018-08-09T06:21:51.594915: step 10228, loss 0.594931.
Train: 2018-08-09T06:21:54.669088: step 10229, loss 0.675137.
Train: 2018-08-09T06:21:57.764317: step 10230, loss 0.642866.
Test: 2018-08-09T06:22:18.873441: step 10230, loss 0.549796.
Train: 2018-08-09T06:22:22.023817: step 10231, loss 0.610742.
Train: 2018-08-09T06:22:25.116038: step 10232, loss 0.626471.
Train: 2018-08-09T06:22:28.195225: step 10233, loss 0.452517.
Train: 2018-08-09T06:22:31.094934: step 10234, loss 0.613531.
Train: 2018-08-09T06:22:34.204201: step 10235, loss 0.626009.
Train: 2018-08-09T06:22:37.265340: step 10236, loss 0.547566.
Train: 2018-08-09T06:22:40.351546: step 10237, loss 0.547661.
Train: 2018-08-09T06:22:43.432737: step 10238, loss 0.625643.
Train: 2018-08-09T06:22:46.527967: step 10239, loss 0.532292.
Train: 2018-08-09T06:22:49.605148: step 10240, loss 0.547894.
Test: 2018-08-09T06:23:10.715274: step 10240, loss 0.550668.
Train: 2018-08-09T06:23:13.791453: step 10241, loss 0.532441.
Train: 2018-08-09T06:23:16.913754: step 10242, loss 0.516964.
Train: 2018-08-09T06:23:20.031042: step 10243, loss 0.501405.
Train: 2018-08-09T06:23:23.110229: step 10244, loss 0.532304.
Train: 2018-08-09T06:23:26.191421: step 10245, loss 0.516594.
Train: 2018-08-09T06:23:29.298682: step 10246, loss 0.594514.
Train: 2018-08-09T06:23:32.415970: step 10247, loss 0.610218.
Train: 2018-08-09T06:23:35.525237: step 10248, loss 0.578873.
Train: 2018-08-09T06:23:38.632498: step 10249, loss 0.610305.
Train: 2018-08-09T06:23:41.741765: step 10250, loss 0.484515.
Test: 2018-08-09T06:24:02.832840: step 10250, loss 0.54894.
Train: 2018-08-09T06:24:05.918043: step 10251, loss 0.547351.
Train: 2018-08-09T06:24:09.015278: step 10252, loss 0.531485.
Train: 2018-08-09T06:24:12.120534: step 10253, loss 0.578859.
Train: 2018-08-09T06:24:15.195710: step 10254, loss 0.51538.
Train: 2018-08-09T06:24:18.275899: step 10255, loss 0.531108.
Train: 2018-08-09T06:24:21.365112: step 10256, loss 0.530954.
Train: 2018-08-09T06:24:24.441291: step 10257, loss 0.658998.
Train: 2018-08-09T06:24:27.541534: step 10258, loss 0.546764.
Train: 2018-08-09T06:24:30.655814: step 10259, loss 0.562791.
Train: 2018-08-09T06:24:33.776110: step 10260, loss 0.611094.
Test: 2018-08-09T06:24:54.917319: step 10260, loss 0.550094.
Train: 2018-08-09T06:24:58.057668: step 10261, loss 0.546641.
Train: 2018-08-09T06:25:01.159916: step 10262, loss 0.562747.
Train: 2018-08-09T06:25:04.258153: step 10263, loss 0.562736.
Train: 2018-08-09T06:25:07.351377: step 10264, loss 0.546561.
Train: 2018-08-09T06:25:10.460644: step 10265, loss 0.578892.
Train: 2018-08-09T06:25:13.568908: step 10266, loss 0.595089.
Train: 2018-08-09T06:25:16.665140: step 10267, loss 0.481727.
Train: 2018-08-09T06:25:19.791452: step 10268, loss 0.530243.
Train: 2018-08-09T06:25:22.895705: step 10269, loss 0.578912.
Train: 2018-08-09T06:25:26.002967: step 10270, loss 0.660291.
Test: 2018-08-09T06:25:47.120112: step 10270, loss 0.549863.
Train: 2018-08-09T06:25:50.195287: step 10271, loss 0.611455.
Train: 2018-08-09T06:25:53.292522: step 10272, loss 0.578911.
Train: 2018-08-09T06:25:56.383741: step 10273, loss 0.546443.
Train: 2018-08-09T06:25:59.452901: step 10274, loss 0.546466.
Train: 2018-08-09T06:26:02.559159: step 10275, loss 0.61132.
Train: 2018-08-09T06:26:05.652383: step 10276, loss 0.562704.
Train: 2018-08-09T06:26:08.766663: step 10277, loss 0.481851.
Train: 2018-08-09T06:26:11.867909: step 10278, loss 0.562712.
Train: 2018-08-09T06:26:14.971159: step 10279, loss 0.465581.
Train: 2018-08-09T06:26:18.062378: step 10280, loss 0.611344.
Test: 2018-08-09T06:26:39.147438: step 10280, loss 0.548682.
Train: 2018-08-09T06:26:42.189525: step 10281, loss 0.513954.
Train: 2018-08-09T06:26:45.296787: step 10282, loss 0.61145.
Train: 2018-08-09T06:26:48.378981: step 10283, loss 0.611479.
Train: 2018-08-09T06:26:51.464184: step 10284, loss 0.644032.
Train: 2018-08-09T06:26:54.554400: step 10285, loss 0.546402.
Train: 2018-08-09T06:26:57.663667: step 10286, loss 0.611383.
Train: 2018-08-09T06:27:00.725808: step 10287, loss 0.5789.
Train: 2018-08-09T06:27:03.801987: step 10288, loss 0.578892.
Train: 2018-08-09T06:27:06.860118: step 10289, loss 0.562735.
Train: 2018-08-09T06:27:09.972393: step 10290, loss 0.498265.
Test: 2018-08-09T06:27:31.164737: step 10290, loss 0.549488.
Train: 2018-08-09T06:27:34.328148: step 10291, loss 0.562762.
Train: 2018-08-09T06:27:37.451452: step 10292, loss 0.659444.
Train: 2018-08-09T06:27:40.548686: step 10293, loss 0.56279.
Train: 2018-08-09T06:27:43.656950: step 10294, loss 0.482527.
Train: 2018-08-09T06:27:46.766217: step 10295, loss 0.530698.
Train: 2018-08-09T06:27:49.852423: step 10296, loss 0.627072.
Train: 2018-08-09T06:27:52.939630: step 10297, loss 0.578868.
Train: 2018-08-09T06:27:56.044886: step 10298, loss 0.562821.
Train: 2018-08-09T06:27:59.128084: step 10299, loss 0.562827.
Train: 2018-08-09T06:28:02.220305: step 10300, loss 0.610925.
Test: 2018-08-09T06:28:23.342464: step 10300, loss 0.547847.
Train: 2018-08-09T06:28:28.205392: step 10301, loss 0.578871.
Train: 2018-08-09T06:28:31.283576: step 10302, loss 0.530883.
Train: 2018-08-09T06:28:34.338699: step 10303, loss 0.594847.
Train: 2018-08-09T06:28:37.427912: step 10304, loss 0.658719.
Train: 2018-08-09T06:28:40.510107: step 10305, loss 0.562932.
Train: 2018-08-09T06:28:43.603331: step 10306, loss 0.547067.
Train: 2018-08-09T06:28:46.719616: step 10307, loss 0.547116.
Train: 2018-08-09T06:28:49.801811: step 10308, loss 0.483727.
Train: 2018-08-09T06:28:52.867963: step 10309, loss 0.562993.
Train: 2018-08-09T06:28:55.973219: step 10310, loss 0.562983.
Test: 2018-08-09T06:29:17.081340: step 10310, loss 0.548687.
Train: 2018-08-09T06:29:20.186596: step 10311, loss 0.610632.
Train: 2018-08-09T06:29:23.253751: step 10312, loss 0.531204.
Train: 2018-08-09T06:29:26.351988: step 10313, loss 0.562965.
Train: 2018-08-09T06:29:29.457244: step 10314, loss 0.515246.
Train: 2018-08-09T06:29:32.525401: step 10315, loss 0.674418.
Train: 2018-08-09T06:29:35.651713: step 10316, loss 0.435599.
Train: 2018-08-09T06:29:38.742932: step 10317, loss 0.626701.
Train: 2018-08-09T06:29:41.823122: step 10318, loss 0.57886.
Train: 2018-08-09T06:29:44.928377: step 10319, loss 0.626748.
Train: 2018-08-09T06:29:48.021602: step 10320, loss 0.57886.
Test: 2018-08-09T06:30:09.112677: step 10320, loss 0.547988.
Train: 2018-08-09T06:30:12.240994: step 10321, loss 0.546984.
Train: 2018-08-09T06:30:15.320181: step 10322, loss 0.467339.
Train: 2018-08-09T06:30:18.407389: step 10323, loss 0.562906.
Train: 2018-08-09T06:30:21.495600: step 10324, loss 0.54691.
Train: 2018-08-09T06:30:24.583810: step 10325, loss 0.578863.
Train: 2018-08-09T06:30:27.698090: step 10326, loss 0.562845.
Train: 2018-08-09T06:30:30.781288: step 10327, loss 0.54679.
Train: 2018-08-09T06:30:33.871504: step 10328, loss 0.498565.
Train: 2018-08-09T06:30:36.967736: step 10329, loss 0.530576.
Train: 2018-08-09T06:30:40.049931: step 10330, loss 0.578884.
Test: 2018-08-09T06:31:01.164068: step 10330, loss 0.548774.
Train: 2018-08-09T06:31:04.233227: step 10331, loss 0.627434.
Train: 2018-08-09T06:31:07.305395: step 10332, loss 0.562702.
Train: 2018-08-09T06:31:10.422683: step 10333, loss 0.627529.
Train: 2018-08-09T06:31:13.533956: step 10334, loss 0.562694.
Train: 2018-08-09T06:31:16.630187: step 10335, loss 0.562695.
Train: 2018-08-09T06:31:19.721406: step 10336, loss 0.514105.
Train: 2018-08-09T06:31:22.816635: step 10337, loss 0.562689.
Train: 2018-08-09T06:31:25.894819: step 10338, loss 0.514025.
Train: 2018-08-09T06:31:28.978017: step 10339, loss 0.611392.
Train: 2018-08-09T06:31:32.073246: step 10340, loss 0.51391.
Test: 2018-08-09T06:31:53.161314: step 10340, loss 0.549246.
Train: 2018-08-09T06:31:56.266570: step 10341, loss 0.562645.
Train: 2018-08-09T06:31:59.395889: step 10342, loss 0.611501.
Train: 2018-08-09T06:32:02.494127: step 10343, loss 0.595215.
Train: 2018-08-09T06:32:05.597378: step 10344, loss 0.562634.
Train: 2018-08-09T06:32:08.689599: step 10345, loss 0.627763.
Train: 2018-08-09T06:32:11.805884: step 10346, loss 0.643946.
Train: 2018-08-09T06:32:14.894095: step 10347, loss 0.497824.
Train: 2018-08-09T06:32:17.990327: step 10348, loss 0.53031.
Train: 2018-08-09T06:32:21.063498: step 10349, loss 0.627454.
Train: 2018-08-09T06:32:24.165746: step 10350, loss 0.562726.
Test: 2018-08-09T06:32:45.248800: step 10350, loss 0.553725.
Train: 2018-08-09T06:32:48.363080: step 10351, loss 0.578883.
Train: 2018-08-09T06:32:51.444272: step 10352, loss 0.627225.
Train: 2018-08-09T06:32:54.558552: step 10353, loss 0.498489.
Train: 2018-08-09T06:32:57.699904: step 10354, loss 0.610991.
Train: 2018-08-09T06:33:00.784104: step 10355, loss 0.578867.
Train: 2018-08-09T06:33:03.882341: step 10356, loss 0.530847.
Train: 2018-08-09T06:33:06.979576: step 10357, loss 0.53089.
Train: 2018-08-09T06:33:10.040715: step 10358, loss 0.498924.
Train: 2018-08-09T06:33:13.145971: step 10359, loss 0.626875.
Train: 2018-08-09T06:33:16.212122: step 10360, loss 0.594865.
Test: 2018-08-09T06:33:37.289161: step 10360, loss 0.550306.
Train: 2018-08-09T06:33:40.466609: step 10361, loss 0.610847.
Train: 2018-08-09T06:33:43.567854: step 10362, loss 0.483037.
Train: 2018-08-09T06:33:46.664086: step 10363, loss 0.54691.
Train: 2018-08-09T06:33:49.717203: step 10364, loss 0.594848.
Train: 2018-08-09T06:33:52.789371: step 10365, loss 0.450968.
Train: 2018-08-09T06:33:55.863545: step 10366, loss 0.626927.
Train: 2018-08-09T06:33:58.935713: step 10367, loss 0.530764.
Train: 2018-08-09T06:34:02.077065: step 10368, loss 0.562813.
Train: 2018-08-09T06:34:05.176305: step 10369, loss 0.546717.
Train: 2018-08-09T06:34:08.279555: step 10370, loss 0.514471.
Test: 2018-08-09T06:34:29.393693: step 10370, loss 0.547624.
Train: 2018-08-09T06:34:32.471876: step 10371, loss 0.530463.
Train: 2018-08-09T06:34:35.576130: step 10372, loss 0.546529.
Train: 2018-08-09T06:34:38.678378: step 10373, loss 0.546449.
Train: 2018-08-09T06:34:41.783633: step 10374, loss 0.530093.
Train: 2018-08-09T06:34:44.859812: step 10375, loss 0.464685.
Train: 2018-08-09T06:34:47.953036: step 10376, loss 0.578878.
Train: 2018-08-09T06:34:51.044255: step 10377, loss 0.562677.
Train: 2018-08-09T06:34:54.137479: step 10378, loss 0.546143.
Train: 2018-08-09T06:34:57.235716: step 10379, loss 0.496363.
Train: 2018-08-09T06:35:00.303874: step 10380, loss 0.52912.
Test: 2018-08-09T06:35:21.389936: step 10380, loss 0.545513.
Train: 2018-08-09T06:35:24.548333: step 10381, loss 0.562385.
Train: 2018-08-09T06:35:27.647573: step 10382, loss 0.67963.
Train: 2018-08-09T06:35:30.737789: step 10383, loss 0.461789.
Train: 2018-08-09T06:35:33.822992: step 10384, loss 0.528786.
Train: 2018-08-09T06:35:36.916216: step 10385, loss 0.61308.
Train: 2018-08-09T06:35:40.006432: step 10386, loss 0.596148.
Train: 2018-08-09T06:35:43.112690: step 10387, loss 0.511766.
Train: 2018-08-09T06:35:46.206917: step 10388, loss 0.528592.
Train: 2018-08-09T06:35:49.327213: step 10389, loss 0.697654.
Train: 2018-08-09T06:35:52.410411: step 10390, loss 0.545466.
Test: 2018-08-09T06:36:13.518532: step 10390, loss 0.547183.
Train: 2018-08-09T06:36:16.610753: step 10391, loss 0.51171.
Train: 2018-08-09T06:36:19.741075: step 10392, loss 0.562365.
Train: 2018-08-09T06:36:22.842321: step 10393, loss 0.461053.
Train: 2018-08-09T06:36:25.981667: step 10394, loss 0.54545.
Train: 2018-08-09T06:36:29.067873: step 10395, loss 0.545419.
Train: 2018-08-09T06:36:32.184158: step 10396, loss 0.579319.
Train: 2018-08-09T06:36:35.280390: step 10397, loss 0.409587.
Train: 2018-08-09T06:36:38.357571: step 10398, loss 0.528264.
Train: 2018-08-09T06:36:41.430742: step 10399, loss 0.579488.
Train: 2018-08-09T06:36:44.528979: step 10400, loss 0.528064.
Test: 2018-08-09T06:37:05.643116: step 10400, loss 0.548841.
Train: 2018-08-09T06:37:10.507048: step 10401, loss 0.562403.
Train: 2018-08-09T06:37:13.649403: step 10402, loss 0.545125.
Train: 2018-08-09T06:37:16.733603: step 10403, loss 0.579558.
Train: 2018-08-09T06:37:19.844875: step 10404, loss 0.562354.
Train: 2018-08-09T06:37:22.948126: step 10405, loss 0.493307.
Train: 2018-08-09T06:37:26.023301: step 10406, loss 0.596913.
Train: 2018-08-09T06:37:29.145603: step 10407, loss 0.666137.
Train: 2018-08-09T06:37:32.232811: step 10408, loss 0.49322.
Train: 2018-08-09T06:37:35.311997: step 10409, loss 0.631446.
Train: 2018-08-09T06:37:38.359099: step 10410, loss 0.510575.
Test: 2018-08-09T06:37:59.408063: step 10410, loss 0.549443.
Train: 2018-08-09T06:38:02.537382: step 10411, loss 0.648532.
Train: 2018-08-09T06:38:05.615566: step 10412, loss 0.527939.
Train: 2018-08-09T06:38:08.709793: step 10413, loss 0.648217.
Train: 2018-08-09T06:38:11.810036: step 10414, loss 0.613705.
Train: 2018-08-09T06:38:14.903260: step 10415, loss 0.494093.
Train: 2018-08-09T06:38:17.988462: step 10416, loss 0.562365.
Train: 2018-08-09T06:38:21.106753: step 10417, loss 0.545397.
Train: 2018-08-09T06:38:24.203988: step 10418, loss 0.596276.
Train: 2018-08-09T06:38:27.302225: step 10419, loss 0.629998.
Train: 2018-08-09T06:38:30.370383: step 10420, loss 0.596083.
Test: 2018-08-09T06:38:51.472487: step 10420, loss 0.546659.
Train: 2018-08-09T06:38:54.543652: step 10421, loss 0.545617.
Train: 2018-08-09T06:38:57.654925: step 10422, loss 0.579148.
Train: 2018-08-09T06:39:00.749152: step 10423, loss 0.579127.
Train: 2018-08-09T06:39:03.840370: step 10424, loss 0.579072.
Train: 2018-08-09T06:39:06.973701: step 10425, loss 0.562468.
Train: 2018-08-09T06:39:10.082967: step 10426, loss 0.562493.
Train: 2018-08-09T06:39:13.174186: step 10427, loss 0.513078.
Train: 2018-08-09T06:39:16.274429: step 10428, loss 0.578984.
Train: 2018-08-09T06:39:19.378682: step 10429, loss 0.62823.
Train: 2018-08-09T06:39:22.455863: step 10430, loss 0.578951.
Test: 2018-08-09T06:39:43.556966: step 10430, loss 0.549779.
Train: 2018-08-09T06:39:46.605070: step 10431, loss 0.513627.
Train: 2018-08-09T06:39:49.706315: step 10432, loss 0.644119.
Train: 2018-08-09T06:39:52.812573: step 10433, loss 0.57891.
Train: 2018-08-09T06:39:55.904795: step 10434, loss 0.627505.
Train: 2018-08-09T06:39:58.993005: step 10435, loss 0.530458.
Train: 2018-08-09T06:40:02.094251: step 10436, loss 0.61107.
Train: 2018-08-09T06:40:05.161406: step 10437, loss 0.530736.
Train: 2018-08-09T06:40:08.244603: step 10438, loss 0.498832.
Train: 2018-08-09T06:40:11.336824: step 10439, loss 0.594854.
Train: 2018-08-09T06:40:14.447094: step 10440, loss 0.56289.
Test: 2018-08-09T06:40:35.571257: step 10440, loss 0.54796.
Train: 2018-08-09T06:40:38.706593: step 10441, loss 0.562907.
Train: 2018-08-09T06:40:41.889054: step 10442, loss 0.515107.
Train: 2018-08-09T06:40:44.977265: step 10443, loss 0.658559.
Train: 2018-08-09T06:40:48.086532: step 10444, loss 0.610691.
Train: 2018-08-09T06:40:51.172737: step 10445, loss 0.515332.
Train: 2018-08-09T06:40:54.275988: step 10446, loss 0.547125.
Train: 2018-08-09T06:40:57.357180: step 10447, loss 0.594719.
Train: 2018-08-09T06:41:00.459428: step 10448, loss 0.515481.
Train: 2018-08-09T06:41:03.516556: step 10449, loss 0.531316.
Train: 2018-08-09T06:41:06.624820: step 10450, loss 0.515417.
Test: 2018-08-09T06:41:27.753997: step 10450, loss 0.549285.
Train: 2018-08-09T06:41:30.841205: step 10451, loss 0.531192.
Train: 2018-08-09T06:41:33.931421: step 10452, loss 0.531088.
Train: 2018-08-09T06:41:37.026650: step 10453, loss 0.530947.
Train: 2018-08-09T06:41:40.089794: step 10454, loss 0.482754.
Train: 2018-08-09T06:41:43.182015: step 10455, loss 0.594967.
Train: 2018-08-09T06:41:46.255186: step 10456, loss 0.595045.
Train: 2018-08-09T06:41:49.337381: step 10457, loss 0.562674.
Train: 2018-08-09T06:41:52.428600: step 10458, loss 0.546472.
Train: 2018-08-09T06:41:55.523829: step 10459, loss 0.530188.
Train: 2018-08-09T06:41:58.603015: step 10460, loss 0.578951.
Test: 2018-08-09T06:42:19.713142: step 10460, loss 0.550982.
Train: 2018-08-09T06:42:22.795336: step 10461, loss 0.529778.
Train: 2018-08-09T06:42:25.905605: step 10462, loss 0.579116.
Train: 2018-08-09T06:42:28.971757: step 10463, loss 0.447749.
Train: 2018-08-09T06:42:32.034902: step 10464, loss 0.513032.
Train: 2018-08-09T06:42:35.090024: step 10465, loss 0.61239.
Train: 2018-08-09T06:42:38.199291: step 10466, loss 0.645479.
Train: 2018-08-09T06:42:41.284494: step 10467, loss 0.512594.
Train: 2018-08-09T06:42:44.402785: step 10468, loss 0.595728.
Train: 2018-08-09T06:42:47.493000: step 10469, loss 0.512549.
Train: 2018-08-09T06:42:50.582214: step 10470, loss 0.512484.
Test: 2018-08-09T06:43:11.708383: step 10470, loss 0.547426.
Train: 2018-08-09T06:43:14.864774: step 10471, loss 0.512373.
Train: 2018-08-09T06:43:17.954990: step 10472, loss 0.545633.
Train: 2018-08-09T06:43:21.051222: step 10473, loss 0.52881.
Train: 2018-08-09T06:43:24.137428: step 10474, loss 0.596134.
Train: 2018-08-09T06:43:27.254716: step 10475, loss 0.528422.
Train: 2018-08-09T06:43:30.346937: step 10476, loss 0.562283.
Train: 2018-08-09T06:43:33.426124: step 10477, loss 0.630518.
Train: 2018-08-09T06:43:36.537396: step 10478, loss 0.613467.
Train: 2018-08-09T06:43:39.641649: step 10479, loss 0.511468.
Train: 2018-08-09T06:43:42.719833: step 10480, loss 0.596288.
Test: 2018-08-09T06:44:03.807901: step 10480, loss 0.54844.
Train: 2018-08-09T06:44:06.934213: step 10481, loss 0.596177.
Train: 2018-08-09T06:44:10.034455: step 10482, loss 0.528601.
Train: 2018-08-09T06:44:13.124671: step 10483, loss 0.612875.
Train: 2018-08-09T06:44:16.194834: step 10484, loss 0.511769.
Train: 2018-08-09T06:44:19.286053: step 10485, loss 0.444473.
Train: 2018-08-09T06:44:22.416376: step 10486, loss 0.545635.
Train: 2018-08-09T06:44:25.507594: step 10487, loss 0.528412.
Train: 2018-08-09T06:44:28.593799: step 10488, loss 0.528372.
Train: 2018-08-09T06:44:31.644912: step 10489, loss 0.595371.
Train: 2018-08-09T06:44:34.745154: step 10490, loss 0.476279.
Test: 2018-08-09T06:44:55.845254: step 10490, loss 0.547408.
Train: 2018-08-09T06:44:58.914414: step 10491, loss 0.596499.
Train: 2018-08-09T06:45:02.021675: step 10492, loss 0.598004.
Train: 2018-08-09T06:45:05.092840: step 10493, loss 0.635283.
Train: 2018-08-09T06:45:08.199099: step 10494, loss 0.751049.
Train: 2018-08-09T06:45:11.299342: step 10495, loss 0.493062.
Train: 2018-08-09T06:45:14.385547: step 10496, loss 0.56133.
Train: 2018-08-09T06:45:17.486792: step 10497, loss 0.494617.
Train: 2018-08-09T06:45:20.574001: step 10498, loss 0.528879.
Train: 2018-08-09T06:45:23.660206: step 10499, loss 0.563364.
Train: 2018-08-09T06:45:26.754433: step 10500, loss 0.546229.
Test: 2018-08-09T06:45:47.874586: step 10500, loss 0.546054.
Train: 2018-08-09T06:45:52.904960: step 10501, loss 0.713281.
Train: 2018-08-09T06:45:56.011218: step 10502, loss 0.579122.
Train: 2018-08-09T06:45:59.082383: step 10503, loss 0.529372.
Train: 2018-08-09T06:46:02.181624: step 10504, loss 0.496352.
Train: 2018-08-09T06:46:05.237749: step 10505, loss 0.628426.
Train: 2018-08-09T06:46:08.326962: step 10506, loss 0.611974.
Train: 2018-08-09T06:46:11.423194: step 10507, loss 0.627948.
Train: 2018-08-09T06:46:14.531458: step 10508, loss 0.562893.
Train: 2018-08-09T06:46:17.606635: step 10509, loss 0.449004.
Train: 2018-08-09T06:46:20.695848: step 10510, loss 0.448947.
Test: 2018-08-09T06:46:41.750828: step 10510, loss 0.546142.
Train: 2018-08-09T06:46:44.845054: step 10511, loss 0.481359.
Train: 2018-08-09T06:46:47.940283: step 10512, loss 0.644481.
Train: 2018-08-09T06:46:51.010446: step 10513, loss 0.480825.
Train: 2018-08-09T06:46:54.083617: step 10514, loss 0.595383.
Train: 2018-08-09T06:46:57.179849: step 10515, loss 0.628196.
Train: 2018-08-09T06:47:00.228955: step 10516, loss 0.54614.
Train: 2018-08-09T06:47:03.331203: step 10517, loss 0.611814.
Train: 2018-08-09T06:47:06.419414: step 10518, loss 0.496943.
Train: 2018-08-09T06:47:09.521662: step 10519, loss 0.644629.
Train: 2018-08-09T06:47:12.601851: step 10520, loss 0.546172.
Test: 2018-08-09T06:47:33.884437: step 10520, loss 0.549701.
Train: 2018-08-09T06:47:36.977660: step 10521, loss 0.611718.
Train: 2018-08-09T06:47:40.078905: step 10522, loss 0.529873.
Train: 2018-08-09T06:47:43.213239: step 10523, loss 0.595291.
Train: 2018-08-09T06:47:46.303455: step 10524, loss 0.562606.
Train: 2018-08-09T06:47:49.381639: step 10525, loss 0.513684.
Train: 2018-08-09T06:47:52.476868: step 10526, loss 0.595237.
Train: 2018-08-09T06:47:55.572098: step 10527, loss 0.513735.
Train: 2018-08-09T06:47:58.709439: step 10528, loss 0.530024.
Train: 2018-08-09T06:48:01.810684: step 10529, loss 0.578929.
Train: 2018-08-09T06:48:04.910927: step 10530, loss 0.627889.
Test: 2018-08-09T06:48:26.023059: step 10530, loss 0.547958.
Train: 2018-08-09T06:48:29.149370: step 10531, loss 0.595234.
Train: 2018-08-09T06:48:32.256632: step 10532, loss 0.627781.
Train: 2018-08-09T06:48:35.354869: step 10533, loss 0.53016.
Train: 2018-08-09T06:48:38.518280: step 10534, loss 0.578904.
Train: 2018-08-09T06:48:41.434032: step 10535, loss 0.631823.
Train: 2018-08-09T06:48:44.526253: step 10536, loss 0.562727.
Train: 2018-08-09T06:48:47.623488: step 10537, loss 0.659496.
Train: 2018-08-09T06:48:50.707688: step 10538, loss 0.53068.
Train: 2018-08-09T06:48:53.832997: step 10539, loss 0.530819.
Train: 2018-08-09T06:48:56.935245: step 10540, loss 0.610856.
Test: 2018-08-09T06:49:18.077457: step 10540, loss 0.550975.
Train: 2018-08-09T06:49:21.102499: step 10541, loss 0.562906.
Train: 2018-08-09T06:49:24.228811: step 10542, loss 0.515177.
Train: 2018-08-09T06:49:27.328051: step 10543, loss 0.610685.
Train: 2018-08-09T06:49:30.439323: step 10544, loss 0.563005.
Train: 2018-08-09T06:49:33.512494: step 10545, loss 0.626433.
Train: 2018-08-09T06:49:36.603713: step 10546, loss 0.594685.
Train: 2018-08-09T06:49:39.701950: step 10547, loss 0.563092.
Train: 2018-08-09T06:49:42.805201: step 10548, loss 0.594628.
Train: 2018-08-09T06:49:45.915470: step 10549, loss 0.547452.
Train: 2018-08-09T06:49:48.983634: step 10550, loss 0.751419.
Test: 2018-08-09T06:50:10.089743: step 10550, loss 0.55043.
Train: 2018-08-09T06:50:13.196002: step 10551, loss 0.610104.
Train: 2018-08-09T06:50:16.323316: step 10552, loss 0.59445.
Train: 2018-08-09T06:50:19.385457: step 10553, loss 0.563489.
Train: 2018-08-09T06:50:22.473668: step 10554, loss 0.50205.
Train: 2018-08-09T06:50:25.565890: step 10555, loss 0.625016.
Train: 2018-08-09T06:50:28.682175: step 10556, loss 0.502514.
Train: 2018-08-09T06:50:31.766375: step 10557, loss 0.624838.
Train: 2018-08-09T06:50:34.845562: step 10558, loss 0.533301.
Train: 2018-08-09T06:50:37.965858: step 10559, loss 0.579036.
Train: 2018-08-09T06:50:41.059082: step 10560, loss 0.609461.
Test: 2018-08-09T06:51:02.150158: step 10560, loss 0.54907.
Train: 2018-08-09T06:51:05.240373: step 10561, loss 0.54869.
Train: 2018-08-09T06:51:08.363677: step 10562, loss 0.488031.
Train: 2018-08-09T06:51:11.462917: step 10563, loss 0.457548.
Train: 2018-08-09T06:51:14.530072: step 10564, loss 0.518075.
Train: 2018-08-09T06:51:17.620288: step 10565, loss 0.563692.
Train: 2018-08-09T06:51:20.696467: step 10566, loss 0.517519.
Train: 2018-08-09T06:51:23.797712: step 10567, loss 0.609814.
Train: 2018-08-09T06:51:26.945080: step 10568, loss 0.625396.
Train: 2018-08-09T06:51:30.014240: step 10569, loss 0.516818.
Train: 2018-08-09T06:51:33.089416: step 10570, loss 0.532179.
Test: 2018-08-09T06:51:54.181495: step 10570, loss 0.549796.
Train: 2018-08-09T06:51:57.264691: step 10571, loss 0.625754.
Train: 2018-08-09T06:52:00.338865: step 10572, loss 0.484925.
Train: 2018-08-09T06:52:03.428078: step 10573, loss 0.531724.
Train: 2018-08-09T06:52:06.516289: step 10574, loss 0.578902.
Train: 2018-08-09T06:52:09.622547: step 10575, loss 0.594742.
Train: 2018-08-09T06:52:12.741841: step 10576, loss 0.62646.
Train: 2018-08-09T06:52:15.847097: step 10577, loss 0.578843.
Train: 2018-08-09T06:52:18.935307: step 10578, loss 0.562948.
Train: 2018-08-09T06:52:22.041566: step 10579, loss 0.499237.
Train: 2018-08-09T06:52:25.108721: step 10580, loss 0.59481.
Test: 2018-08-09T06:52:46.202805: step 10580, loss 0.550338.
Train: 2018-08-09T06:52:49.267954: step 10581, loss 0.514967.
Train: 2018-08-09T06:52:52.353156: step 10582, loss 0.482841.
Train: 2018-08-09T06:52:55.470444: step 10583, loss 0.546622.
Train: 2018-08-09T06:52:58.569684: step 10584, loss 0.595143.
Train: 2018-08-09T06:53:01.642855: step 10585, loss 0.497956.
Train: 2018-08-09T06:53:04.714020: step 10586, loss 0.529924.
Train: 2018-08-09T06:53:07.807244: step 10587, loss 0.57881.
Train: 2018-08-09T06:53:10.908490: step 10588, loss 0.398734.
Train: 2018-08-09T06:53:14.015751: step 10589, loss 0.463287.
Train: 2018-08-09T06:53:17.102959: step 10590, loss 0.562652.
Test: 2018-08-09T06:53:38.170974: step 10590, loss 0.546609.
Train: 2018-08-09T06:53:41.327365: step 10591, loss 0.596991.
Train: 2018-08-09T06:53:44.452674: step 10592, loss 0.58018.
Train: 2018-08-09T06:53:47.531861: step 10593, loss 0.596251.
Train: 2018-08-09T06:53:50.602024: step 10594, loss 0.459697.
Train: 2018-08-09T06:53:53.694245: step 10595, loss 0.579905.
Train: 2018-08-09T06:53:56.785464: step 10596, loss 0.54619.
Train: 2018-08-09T06:53:59.901749: step 10597, loss 0.441613.
Train: 2018-08-09T06:54:02.987955: step 10598, loss 0.562576.
Train: 2018-08-09T06:54:06.059120: step 10599, loss 0.527799.
Train: 2018-08-09T06:54:09.132291: step 10600, loss 0.545801.
Test: 2018-08-09T06:54:30.250438: step 10600, loss 0.546649.
Train: 2018-08-09T06:54:35.150466: step 10601, loss 0.738733.
Train: 2018-08-09T06:54:38.229653: step 10602, loss 0.666703.
Train: 2018-08-09T06:54:41.292797: step 10603, loss 0.596202.
Train: 2018-08-09T06:54:44.368975: step 10604, loss 0.613369.
Train: 2018-08-09T06:54:47.464205: step 10605, loss 0.47624.
Train: 2018-08-09T06:54:50.539381: step 10606, loss 0.511187.
Train: 2018-08-09T06:54:53.631602: step 10607, loss 0.46041.
Train: 2018-08-09T06:54:56.730842: step 10608, loss 0.596588.
Train: 2018-08-09T06:54:59.832087: step 10609, loss 0.579094.
Train: 2018-08-09T06:55:02.936341: step 10610, loss 0.579537.
Test: 2018-08-09T06:55:24.019395: step 10610, loss 0.547708.
Train: 2018-08-09T06:55:27.096576: step 10611, loss 0.579413.
Train: 2018-08-09T06:55:30.196819: step 10612, loss 0.664104.
Train: 2018-08-09T06:55:33.281019: step 10613, loss 0.494777.
Train: 2018-08-09T06:55:36.376248: step 10614, loss 0.579268.
Train: 2018-08-09T06:55:39.450422: step 10615, loss 0.528676.
Train: 2018-08-09T06:55:42.548659: step 10616, loss 0.612845.
Train: 2018-08-09T06:55:45.669958: step 10617, loss 0.512032.
Train: 2018-08-09T06:55:48.767192: step 10618, loss 0.495333.
Train: 2018-08-09T06:55:51.854400: step 10619, loss 0.713281.
Train: 2018-08-09T06:55:54.961662: step 10620, loss 0.562434.
Test: 2018-08-09T06:56:16.093847: step 10620, loss 0.549952.
Train: 2018-08-09T06:56:19.205119: step 10621, loss 0.462348.
Train: 2018-08-09T06:56:22.293329: step 10622, loss 0.562428.
Train: 2018-08-09T06:56:25.421647: step 10623, loss 0.495798.
Train: 2018-08-09T06:56:28.508855: step 10624, loss 0.579108.
Train: 2018-08-09T06:56:31.596063: step 10625, loss 0.612421.
Train: 2018-08-09T06:56:34.678259: step 10626, loss 0.562468.
Train: 2018-08-09T06:56:37.767471: step 10627, loss 0.529158.
Train: 2018-08-09T06:56:40.851671: step 10628, loss 0.64564.
Train: 2018-08-09T06:56:43.960937: step 10629, loss 0.662027.
Train: 2018-08-09T06:56:47.041127: step 10630, loss 0.529292.
Test: 2018-08-09T06:57:08.159274: step 10630, loss 0.548939.
Train: 2018-08-09T06:57:11.231442: step 10631, loss 0.578979.
Train: 2018-08-09T06:57:14.325669: step 10632, loss 0.611886.
Train: 2018-08-09T06:57:17.427917: step 10633, loss 0.529972.
Train: 2018-08-09T06:57:20.540191: step 10634, loss 0.546405.
Train: 2018-08-09T06:57:23.648456: step 10635, loss 0.67672.
Train: 2018-08-09T06:57:26.762735: step 10636, loss 0.595367.
Train: 2018-08-09T06:57:29.836909: step 10637, loss 0.546481.
Train: 2018-08-09T06:57:32.893034: step 10638, loss 0.578914.
Train: 2018-08-09T06:57:35.986258: step 10639, loss 0.514468.
Train: 2018-08-09T06:57:39.075472: step 10640, loss 0.643253.
Test: 2018-08-09T06:58:00.210665: step 10640, loss 0.548385.
Train: 2018-08-09T06:58:03.275814: step 10641, loss 0.530715.
Train: 2018-08-09T06:58:06.383075: step 10642, loss 0.562845.
Train: 2018-08-09T06:58:09.487328: step 10643, loss 0.562871.
Train: 2018-08-09T06:58:12.597597: step 10644, loss 0.610801.
Train: 2018-08-09T06:58:15.685808: step 10645, loss 0.594795.
Train: 2018-08-09T06:58:18.779032: step 10646, loss 0.547064.
Train: 2018-08-09T06:58:21.871254: step 10647, loss 0.57886.
Train: 2018-08-09T06:58:24.968488: step 10648, loss 0.705575.
Train: 2018-08-09T06:58:28.059707: step 10649, loss 0.594639.
Train: 2018-08-09T06:58:31.164963: step 10650, loss 0.531742.
Test: 2018-08-09T06:58:52.278098: step 10650, loss 0.549726.
Train: 2018-08-09T06:58:55.437497: step 10651, loss 0.500566.
Train: 2018-08-09T06:58:58.553782: step 10652, loss 0.516321.
Train: 2018-08-09T06:59:01.630964: step 10653, loss 0.672703.
Train: 2018-08-09T06:59:04.753265: step 10654, loss 0.610099.
Train: 2018-08-09T06:59:07.853508: step 10655, loss 0.610029.
Train: 2018-08-09T06:59:10.954753: step 10656, loss 0.563397.
Train: 2018-08-09T06:59:14.033940: step 10657, loss 0.625361.
Train: 2018-08-09T06:59:17.140198: step 10658, loss 0.51723.
Train: 2018-08-09T06:59:20.236431: step 10659, loss 0.563554.
Train: 2018-08-09T06:59:23.341687: step 10660, loss 0.548207.
Test: 2018-08-09T06:59:44.489915: step 10660, loss 0.549204.
Train: 2018-08-09T06:59:47.589154: step 10661, loss 0.548236.
Train: 2018-08-09T06:59:50.694410: step 10662, loss 0.640424.
Train: 2018-08-09T06:59:53.804679: step 10663, loss 0.517608.
Train: 2018-08-09T06:59:56.895898: step 10664, loss 0.655682.
Train: 2018-08-09T06:59:59.990125: step 10665, loss 0.54836.
Train: 2018-08-09T07:00:03.084352: step 10666, loss 0.640201.
Train: 2018-08-09T07:00:06.175570: step 10667, loss 0.609552.
Train: 2018-08-09T07:00:09.320933: step 10668, loss 0.53334.
Train: 2018-08-09T07:00:12.413154: step 10669, loss 0.442142.
Train: 2018-08-09T07:00:15.486325: step 10670, loss 0.533324.
Test: 2018-08-09T07:00:36.625529: step 10670, loss 0.550019.
Train: 2018-08-09T07:00:39.729781: step 10671, loss 0.640084.
Train: 2018-08-09T07:00:42.832029: step 10672, loss 0.548449.
Train: 2018-08-09T07:00:45.920240: step 10673, loss 0.471935.
Train: 2018-08-09T07:00:48.987395: step 10674, loss 0.548278.
Train: 2018-08-09T07:00:52.056555: step 10675, loss 0.532753.
Train: 2018-08-09T07:00:55.141758: step 10676, loss 0.563471.
Train: 2018-08-09T07:00:58.231974: step 10677, loss 0.563348.
Train: 2018-08-09T07:01:01.352270: step 10678, loss 0.532164.
Train: 2018-08-09T07:01:04.415414: step 10679, loss 0.594261.
Train: 2018-08-09T07:01:07.515657: step 10680, loss 0.641319.
Test: 2018-08-09T07:01:28.651853: step 10680, loss 0.55006.
Train: 2018-08-09T07:01:31.775156: step 10681, loss 0.54713.
Train: 2018-08-09T07:01:34.864369: step 10682, loss 0.451752.
Train: 2018-08-09T07:01:37.960601: step 10683, loss 0.563618.
Train: 2018-08-09T07:01:41.014721: step 10684, loss 0.417904.
Train: 2018-08-09T07:01:44.093908: step 10685, loss 0.611869.
Train: 2018-08-09T07:01:47.205180: step 10686, loss 0.566229.
Train: 2018-08-09T07:01:50.299406: step 10687, loss 0.579908.
Train: 2018-08-09T07:01:53.397644: step 10688, loss 0.561113.
Train: 2018-08-09T07:01:56.489865: step 10689, loss 0.509203.
Train: 2018-08-09T07:01:59.596124: step 10690, loss 0.593501.
Test: 2018-08-09T07:02:20.744351: step 10690, loss 0.54554.
Train: 2018-08-09T07:02:23.835570: step 10691, loss 0.532268.
Train: 2018-08-09T07:02:26.982938: step 10692, loss 0.569091.
Train: 2018-08-09T07:02:30.079170: step 10693, loss 0.597151.
Train: 2018-08-09T07:02:33.164373: step 10694, loss 0.614666.
Train: 2018-08-09T07:02:36.225512: step 10695, loss 0.594042.
Train: 2018-08-09T07:02:39.335781: step 10696, loss 0.530553.
Train: 2018-08-09T07:02:42.407957: step 10697, loss 0.579874.
Train: 2018-08-09T07:02:45.510197: step 10698, loss 0.513283.
Train: 2018-08-09T07:02:48.586376: step 10699, loss 0.627865.
Train: 2018-08-09T07:02:51.676592: step 10700, loss 0.481134.
Test: 2018-08-09T07:03:12.829833: step 10700, loss 0.548676.
Train: 2018-08-09T07:03:17.700783: step 10701, loss 0.465193.
Train: 2018-08-09T07:03:20.775959: step 10702, loss 0.546298.
Train: 2018-08-09T07:03:23.890239: step 10703, loss 0.464855.
Train: 2018-08-09T07:03:27.015548: step 10704, loss 0.530048.
Train: 2018-08-09T07:03:30.104761: step 10705, loss 0.56245.
Train: 2018-08-09T07:03:33.215031: step 10706, loss 0.480008.
Train: 2018-08-09T07:03:36.291209: step 10707, loss 0.579192.
Train: 2018-08-09T07:03:39.497735: step 10708, loss 0.512577.
Train: 2018-08-09T07:03:42.602991: step 10709, loss 0.612534.
Train: 2018-08-09T07:03:45.698220: step 10710, loss 0.612606.
Test: 2018-08-09T07:04:06.806341: step 10710, loss 0.547346.
Train: 2018-08-09T07:04:09.962733: step 10711, loss 0.646122.
Train: 2018-08-09T07:04:13.042922: step 10712, loss 0.646048.
Train: 2018-08-09T07:04:16.115090: step 10713, loss 0.562465.
Train: 2018-08-09T07:04:19.204304: step 10714, loss 0.529043.
Train: 2018-08-09T07:04:22.321591: step 10715, loss 0.662422.
Train: 2018-08-09T07:04:25.399775: step 10716, loss 0.479345.
Train: 2018-08-09T07:04:28.533106: step 10717, loss 0.479429.
Train: 2018-08-09T07:04:31.680474: step 10718, loss 0.562451.
Train: 2018-08-09T07:04:34.735597: step 10719, loss 0.529213.
Train: 2018-08-09T07:04:37.839850: step 10720, loss 0.512549.
Test: 2018-08-09T07:04:58.940952: step 10720, loss 0.54682.
Train: 2018-08-09T07:05:02.021142: step 10721, loss 0.645747.
Train: 2018-08-09T07:05:05.118376: step 10722, loss 0.579103.
Train: 2018-08-09T07:05:08.206587: step 10723, loss 0.529157.
Train: 2018-08-09T07:05:11.301816: step 10724, loss 0.545791.
Train: 2018-08-09T07:05:14.403062: step 10725, loss 0.529111.
Train: 2018-08-09T07:05:17.490270: step 10726, loss 0.512448.
Train: 2018-08-09T07:05:20.589510: step 10727, loss 0.595701.
Train: 2018-08-09T07:05:23.678723: step 10728, loss 0.662831.
Train: 2018-08-09T07:05:26.804032: step 10729, loss 0.562202.
Train: 2018-08-09T07:05:29.898259: step 10730, loss 0.629282.
Test: 2018-08-09T07:05:51.078572: step 10730, loss 0.546218.
Train: 2018-08-09T07:05:54.164777: step 10731, loss 0.579098.
Train: 2018-08-09T07:05:57.268028: step 10732, loss 0.5957.
Train: 2018-08-09T07:06:00.378297: step 10733, loss 0.47982.
Train: 2018-08-09T07:06:03.476534: step 10734, loss 0.562501.
Train: 2018-08-09T07:06:06.569759: step 10735, loss 0.61199.
Train: 2018-08-09T07:06:09.661980: step 10736, loss 0.430818.
Train: 2018-08-09T07:06:12.758212: step 10737, loss 0.513104.
Train: 2018-08-09T07:06:15.881516: step 10738, loss 0.529533.
Train: 2018-08-09T07:06:18.990783: step 10739, loss 0.595528.
Train: 2018-08-09T07:06:22.104060: step 10740, loss 0.463287.
Test: 2018-08-09T07:06:43.249280: step 10740, loss 0.551329.
Train: 2018-08-09T07:06:46.347517: step 10741, loss 0.545894.
Train: 2018-08-09T07:06:49.474831: step 10742, loss 0.54581.
Train: 2018-08-09T07:06:52.545997: step 10743, loss 0.56242.
Train: 2018-08-09T07:06:55.612148: step 10744, loss 0.512356.
Train: 2018-08-09T07:06:58.705373: step 10745, loss 0.529111.
Train: 2018-08-09T07:07:01.823664: step 10746, loss 0.596372.
Train: 2018-08-09T07:07:04.912877: step 10747, loss 0.579308.
Train: 2018-08-09T07:07:08.016127: step 10748, loss 0.52874.
Train: 2018-08-09T07:07:11.098322: step 10749, loss 0.494948.
Train: 2018-08-09T07:07:14.179514: step 10750, loss 0.697721.
Test: 2018-08-09T07:07:35.400936: step 10750, loss 0.549735.
Train: 2018-08-09T07:07:38.495163: step 10751, loss 0.596162.
Train: 2018-08-09T07:07:41.588387: step 10752, loss 0.545499.
Train: 2018-08-09T07:07:44.669579: step 10753, loss 0.612926.
Train: 2018-08-09T07:07:47.749768: step 10754, loss 0.612842.
Train: 2018-08-09T07:07:50.835974: step 10755, loss 0.428112.
Train: 2018-08-09T07:07:53.911149: step 10756, loss 0.495241.
Train: 2018-08-09T07:07:56.970283: step 10757, loss 0.57919.
Train: 2018-08-09T07:08:00.087571: step 10758, loss 0.663251.
Train: 2018-08-09T07:08:03.173776: step 10759, loss 0.495233.
Train: 2018-08-09T07:08:06.269006: step 10760, loss 0.579172.
Test: 2018-08-09T07:08:27.407207: step 10760, loss 0.549211.
Train: 2018-08-09T07:08:30.521487: step 10761, loss 0.545617.
Train: 2018-08-09T07:08:33.601676: step 10762, loss 0.495317.
Train: 2018-08-09T07:08:36.698910: step 10763, loss 0.595961.
Train: 2018-08-09T07:08:39.808177: step 10764, loss 0.646318.
Train: 2018-08-09T07:08:42.896388: step 10765, loss 0.612678.
Train: 2018-08-09T07:08:46.012673: step 10766, loss 0.528958.
Train: 2018-08-09T07:08:49.091860: step 10767, loss 0.529025.
Train: 2018-08-09T07:08:52.231207: step 10768, loss 0.662499.
Train: 2018-08-09T07:08:55.319417: step 10769, loss 0.645524.
Train: 2018-08-09T07:08:58.402615: step 10770, loss 0.529198.
Test: 2018-08-09T07:09:19.530789: step 10770, loss 0.549566.
Train: 2018-08-09T07:09:22.664119: step 10771, loss 0.562687.
Train: 2018-08-09T07:09:25.757343: step 10772, loss 0.57906.
Train: 2018-08-09T07:09:28.869618: step 10773, loss 0.530027.
Train: 2018-08-09T07:09:31.984901: step 10774, loss 0.578931.
Train: 2018-08-09T07:09:35.075117: step 10775, loss 0.628025.
Train: 2018-08-09T07:09:38.177365: step 10776, loss 0.529941.
Train: 2018-08-09T07:09:41.274599: step 10777, loss 0.481143.
Train: 2018-08-09T07:09:44.371834: step 10778, loss 0.530073.
Train: 2018-08-09T07:09:47.468066: step 10779, loss 0.513741.
Train: 2018-08-09T07:09:50.572319: step 10780, loss 0.546234.
Test: 2018-08-09T07:10:11.700494: step 10780, loss 0.549125.
Train: 2018-08-09T07:10:14.756619: step 10781, loss 0.546244.
Train: 2018-08-09T07:10:17.861875: step 10782, loss 0.546138.
Train: 2018-08-09T07:10:20.983174: step 10783, loss 0.480591.
Train: 2018-08-09T07:10:24.067373: step 10784, loss 0.579105.
Train: 2018-08-09T07:10:27.169622: step 10785, loss 0.546002.
Train: 2018-08-09T07:10:30.292926: step 10786, loss 0.529086.
Train: 2018-08-09T07:10:33.401190: step 10787, loss 0.595724.
Train: 2018-08-09T07:10:36.492408: step 10788, loss 0.512172.
Train: 2018-08-09T07:10:39.559563: step 10789, loss 0.444829.
Train: 2018-08-09T07:10:42.669832: step 10790, loss 0.544976.
Test: 2018-08-09T07:11:03.780961: step 10790, loss 0.550775.
Train: 2018-08-09T07:11:06.871177: step 10791, loss 0.4765.
Train: 2018-08-09T07:11:10.006513: step 10792, loss 0.545565.
Train: 2018-08-09T07:11:13.108761: step 10793, loss 0.462067.
Train: 2018-08-09T07:11:16.185942: step 10794, loss 0.656257.
Train: 2018-08-09T07:11:19.302228: step 10795, loss 0.612779.
Train: 2018-08-09T07:11:22.402471: step 10796, loss 0.510923.
Train: 2018-08-09T07:11:25.482660: step 10797, loss 0.632687.
Train: 2018-08-09T07:11:28.568865: step 10798, loss 0.667162.
Train: 2018-08-09T07:11:31.664094: step 10799, loss 0.54605.
Train: 2018-08-09T07:11:34.771356: step 10800, loss 0.580732.
Test: 2018-08-09T07:11:55.884491: step 10800, loss 0.548342.
Train: 2018-08-09T07:12:00.867739: step 10801, loss 0.578545.
Train: 2018-08-09T07:12:03.951939: step 10802, loss 0.630016.
Train: 2018-08-09T07:12:07.043157: step 10803, loss 0.680244.
Train: 2018-08-09T07:12:10.115326: step 10804, loss 0.612576.
Train: 2018-08-09T07:12:13.208550: step 10805, loss 0.545843.
Train: 2018-08-09T07:12:16.312803: step 10806, loss 0.595595.
Train: 2018-08-09T07:12:19.410038: step 10807, loss 0.62844.
Train: 2018-08-09T07:12:22.500254: step 10808, loss 0.578959.
Train: 2018-08-09T07:12:25.559387: step 10809, loss 0.56262.
Train: 2018-08-09T07:12:28.641582: step 10810, loss 0.54644.
Test: 2018-08-09T07:12:49.722631: step 10810, loss 0.54696.
Train: 2018-08-09T07:12:52.793796: step 10811, loss 0.514214.
Train: 2018-08-09T07:12:55.890028: step 10812, loss 0.514383.
Train: 2018-08-09T07:12:58.989268: step 10813, loss 0.466174.
Train: 2018-08-09T07:13:02.103548: step 10814, loss 0.530559.
Train: 2018-08-09T07:13:05.168697: step 10815, loss 0.546626.
Train: 2018-08-09T07:13:08.254903: step 10816, loss 0.594994.
Train: 2018-08-09T07:13:11.360159: step 10817, loss 0.530407.
Train: 2018-08-09T07:13:14.481457: step 10818, loss 0.497881.
Train: 2018-08-09T07:13:17.567663: step 10819, loss 0.627676.
Train: 2018-08-09T07:13:20.676930: step 10820, loss 0.595204.
Test: 2018-08-09T07:13:41.804101: step 10820, loss 0.549785.
Train: 2018-08-09T07:13:44.912365: step 10821, loss 0.562313.
Train: 2018-08-09T07:13:48.012607: step 10822, loss 0.496829.
Train: 2018-08-09T07:13:51.092797: step 10823, loss 0.495978.
Train: 2018-08-09T07:13:54.192037: step 10824, loss 0.511949.
Train: 2018-08-09T07:13:57.272226: step 10825, loss 0.59588.
Train: 2018-08-09T07:14:00.366453: step 10826, loss 0.648541.
Train: 2018-08-09T07:14:03.438621: step 10827, loss 0.463291.
Train: 2018-08-09T07:14:06.543877: step 10828, loss 0.632844.
Train: 2018-08-09T07:14:09.610029: step 10829, loss 0.544747.
Train: 2018-08-09T07:14:12.721301: step 10830, loss 0.594308.
Test: 2018-08-09T07:14:33.847470: step 10830, loss 0.545927.
Train: 2018-08-09T07:14:36.969771: step 10831, loss 0.647344.
Train: 2018-08-09T07:14:40.087059: step 10832, loss 0.579934.
Train: 2018-08-09T07:14:43.163238: step 10833, loss 0.56291.
Train: 2018-08-09T07:14:46.214350: step 10834, loss 0.628512.
Train: 2018-08-09T07:14:49.286518: step 10835, loss 0.546088.
Train: 2018-08-09T07:14:52.192243: step 10836, loss 0.476275.
Train: 2018-08-09T07:14:55.267419: step 10837, loss 0.562706.
Train: 2018-08-09T07:14:58.356633: step 10838, loss 0.530453.
Train: 2018-08-09T07:15:01.449857: step 10839, loss 0.514299.
Train: 2018-08-09T07:15:04.548094: step 10840, loss 0.514241.
Test: 2018-08-09T07:15:25.666242: step 10840, loss 0.550596.
Train: 2018-08-09T07:15:28.759466: step 10841, loss 0.562706.
Train: 2018-08-09T07:15:31.904828: step 10842, loss 0.51404.
Train: 2018-08-09T07:15:35.015097: step 10843, loss 0.578911.
Train: 2018-08-09T07:15:38.108321: step 10844, loss 0.481227.
Train: 2018-08-09T07:15:41.194527: step 10845, loss 0.562604.
Train: 2018-08-09T07:15:44.286748: step 10846, loss 0.644452.
Train: 2018-08-09T07:15:47.382980: step 10847, loss 0.628131.
Train: 2018-08-09T07:15:50.485228: step 10848, loss 0.529781.
Train: 2018-08-09T07:15:53.539348: step 10849, loss 0.546168.
Train: 2018-08-09T07:15:56.620540: step 10850, loss 0.546151.
Test: 2018-08-09T07:16:17.761750: step 10850, loss 0.548403.
Train: 2018-08-09T07:16:20.874024: step 10851, loss 0.579002.
Train: 2018-08-09T07:16:23.982288: step 10852, loss 0.595346.
Train: 2018-08-09T07:16:27.077517: step 10853, loss 0.644672.
Train: 2018-08-09T07:16:30.185781: step 10854, loss 0.611793.
Train: 2018-08-09T07:16:33.251933: step 10855, loss 0.578955.
Train: 2018-08-09T07:16:36.354181: step 10856, loss 0.497351.
Train: 2018-08-09T07:16:39.436376: step 10857, loss 0.595214.
Train: 2018-08-09T07:16:42.578730: step 10858, loss 0.49757.
Train: 2018-08-09T07:16:45.662931: step 10859, loss 0.530109.
Train: 2018-08-09T07:16:48.740112: step 10860, loss 0.481236.
Test: 2018-08-09T07:17:09.859262: step 10860, loss 0.549181.
Train: 2018-08-09T07:17:12.985574: step 10861, loss 0.530001.
Train: 2018-08-09T07:17:16.109881: step 10862, loss 0.529806.
Train: 2018-08-09T07:17:19.185056: step 10863, loss 0.496687.
Train: 2018-08-09T07:17:22.325406: step 10864, loss 0.595677.
Train: 2018-08-09T07:17:25.405595: step 10865, loss 0.579191.
Train: 2018-08-09T07:17:28.445678: step 10866, loss 0.562462.
Train: 2018-08-09T07:17:31.532886: step 10867, loss 0.495847.
Train: 2018-08-09T07:17:34.643155: step 10868, loss 0.54607.
Train: 2018-08-09T07:17:37.729361: step 10869, loss 0.595379.
Train: 2018-08-09T07:17:40.832611: step 10870, loss 0.628374.
Test: 2018-08-09T07:18:02.004903: step 10870, loss 0.547795.
Train: 2018-08-09T07:18:05.134223: step 10871, loss 0.663722.
Train: 2018-08-09T07:18:08.233463: step 10872, loss 0.562258.
Train: 2018-08-09T07:18:11.314655: step 10873, loss 0.629695.
Train: 2018-08-09T07:18:14.423922: step 10874, loss 0.513049.
Train: 2018-08-09T07:18:17.543215: step 10875, loss 0.430268.
Train: 2018-08-09T07:18:20.639447: step 10876, loss 0.628652.
Train: 2018-08-09T07:18:23.715626: step 10877, loss 0.595511.
Train: 2018-08-09T07:18:26.843943: step 10878, loss 0.546034.
Train: 2018-08-09T07:18:29.929153: step 10879, loss 0.546057.
Train: 2018-08-09T07:18:33.038412: step 10880, loss 0.546064.
Test: 2018-08-09T07:18:54.244795: step 10880, loss 0.548349.
Train: 2018-08-09T07:18:57.338018: step 10881, loss 0.496669.
Train: 2018-08-09T07:19:00.463328: step 10882, loss 0.562515.
Train: 2018-08-09T07:19:03.531485: step 10883, loss 0.463468.
Train: 2018-08-09T07:19:06.615685: step 10884, loss 0.545758.
Train: 2018-08-09T07:19:09.697880: step 10885, loss 0.545907.
Train: 2018-08-09T07:19:12.797120: step 10886, loss 0.645691.
Train: 2018-08-09T07:19:15.901373: step 10887, loss 0.529218.
Train: 2018-08-09T07:19:19.000613: step 10888, loss 0.495761.
Train: 2018-08-09T07:19:22.088824: step 10889, loss 0.595414.
Train: 2018-08-09T07:19:25.178037: step 10890, loss 0.56173.
Test: 2018-08-09T07:19:46.303204: step 10890, loss 0.550299.
Train: 2018-08-09T07:19:49.420491: step 10891, loss 0.596895.
Train: 2018-08-09T07:19:52.552819: step 10892, loss 0.596443.
Train: 2018-08-09T07:19:55.650054: step 10893, loss 0.579253.
Train: 2018-08-09T07:19:58.743278: step 10894, loss 0.495378.
Train: 2018-08-09T07:20:01.859563: step 10895, loss 0.596122.
Train: 2018-08-09T07:20:04.984872: step 10896, loss 0.51269.
Train: 2018-08-09T07:20:08.075088: step 10897, loss 0.629482.
Train: 2018-08-09T07:20:11.176334: step 10898, loss 0.695514.
Train: 2018-08-09T07:20:14.255521: step 10899, loss 0.545887.
Train: 2018-08-09T07:20:17.365791: step 10900, loss 0.512882.
Test: 2018-08-09T07:20:38.476919: step 10900, loss 0.54766.
Train: 2018-08-09T07:20:43.370930: step 10901, loss 0.496467.
Train: 2018-08-09T07:20:46.459141: step 10902, loss 0.529518.
Train: 2018-08-09T07:20:49.555373: step 10903, loss 0.480036.
Train: 2018-08-09T07:20:52.650603: step 10904, loss 0.529432.
Train: 2018-08-09T07:20:55.736808: step 10905, loss 0.562474.
Train: 2018-08-09T07:20:58.803963: step 10906, loss 0.562465.
Train: 2018-08-09T07:21:01.920248: step 10907, loss 0.479421.
Train: 2018-08-09T07:21:04.995424: step 10908, loss 0.61239.
Train: 2018-08-09T07:21:08.087645: step 10909, loss 0.579112.
Train: 2018-08-09T07:21:11.152795: step 10910, loss 0.512361.
Test: 2018-08-09T07:21:32.273950: step 10910, loss 0.548652.
Train: 2018-08-09T07:21:35.346118: step 10911, loss 0.595841.
Train: 2018-08-09T07:21:38.452377: step 10912, loss 0.562406.
Train: 2018-08-09T07:21:41.546603: step 10913, loss 0.428533.
Train: 2018-08-09T07:21:44.660883: step 10914, loss 0.579177.
Train: 2018-08-09T07:21:47.728038: step 10915, loss 0.545571.
Train: 2018-08-09T07:21:50.817252: step 10916, loss 0.478217.
Train: 2018-08-09T07:21:54.014753: step 10917, loss 0.511665.
Train: 2018-08-09T07:21:57.085918: step 10918, loss 0.664103.
Train: 2018-08-09T07:22:00.165105: step 10919, loss 0.545408.
Train: 2018-08-09T07:22:03.252313: step 10920, loss 0.613392.
Test: 2018-08-09T07:22:24.392519: step 10920, loss 0.545762.
Train: 2018-08-09T07:22:27.508804: step 10921, loss 0.477266.
Train: 2018-08-09T07:22:30.616066: step 10922, loss 0.562352.
Train: 2018-08-09T07:22:33.742378: step 10923, loss 0.613554.
Train: 2018-08-09T07:22:36.813543: step 10924, loss 0.596541.
Train: 2018-08-09T07:22:39.870671: step 10925, loss 0.494057.
Train: 2018-08-09T07:22:43.006007: step 10926, loss 0.56239.
Train: 2018-08-09T07:22:46.124298: step 10927, loss 0.5624.
Train: 2018-08-09T07:22:49.239580: step 10928, loss 0.562362.
Train: 2018-08-09T07:22:52.303727: step 10929, loss 0.579474.
Train: 2018-08-09T07:22:55.374892: step 10930, loss 0.443285.
Test: 2018-08-09T07:23:16.510086: step 10930, loss 0.548964.
Train: 2018-08-09T07:23:19.645421: step 10931, loss 0.715471.
Train: 2018-08-09T07:23:22.731626: step 10932, loss 0.494398.
Train: 2018-08-09T07:23:25.805800: step 10933, loss 0.596426.
Train: 2018-08-09T07:23:28.930107: step 10934, loss 0.664091.
Train: 2018-08-09T07:23:32.031352: step 10935, loss 0.528511.
Train: 2018-08-09T07:23:35.117557: step 10936, loss 0.579295.
Train: 2018-08-09T07:23:38.221810: step 10937, loss 0.528731.
Train: 2018-08-09T07:23:41.317040: step 10938, loss 0.545676.
Train: 2018-08-09T07:23:44.386200: step 10939, loss 0.528829.
Train: 2018-08-09T07:23:47.488448: step 10940, loss 0.663062.
Test: 2018-08-09T07:24:08.604591: step 10940, loss 0.551782.
Train: 2018-08-09T07:24:11.685782: step 10941, loss 0.512176.
Train: 2018-08-09T07:24:14.808083: step 10942, loss 0.579132.
Train: 2018-08-09T07:24:17.900305: step 10943, loss 0.579116.
Train: 2018-08-09T07:24:20.981497: step 10944, loss 0.462441.
Train: 2018-08-09T07:24:24.087755: step 10945, loss 0.545778.
Train: 2018-08-09T07:24:27.174963: step 10946, loss 0.529101.
Train: 2018-08-09T07:24:30.260166: step 10947, loss 0.61246.
Train: 2018-08-09T07:24:33.351385: step 10948, loss 0.57909.
Train: 2018-08-09T07:24:36.430571: step 10949, loss 0.479191.
Train: 2018-08-09T07:24:39.530814: step 10950, loss 0.579107.
Test: 2018-08-09T07:25:00.645954: step 10950, loss 0.550598.
Train: 2018-08-09T07:25:03.819391: step 10951, loss 0.645762.
Train: 2018-08-09T07:25:06.893564: step 10952, loss 0.545796.
Train: 2018-08-09T07:25:10.002831: step 10953, loss 0.529186.
Train: 2018-08-09T07:25:13.109089: step 10954, loss 0.595698.
Train: 2018-08-09T07:25:16.222367: step 10955, loss 0.562453.
Train: 2018-08-09T07:25:19.299548: step 10956, loss 0.628839.
Train: 2018-08-09T07:25:22.385754: step 10957, loss 0.562468.
Train: 2018-08-09T07:25:25.497026: step 10958, loss 0.579034.
Train: 2018-08-09T07:25:28.578218: step 10959, loss 0.612012.
Train: 2018-08-09T07:25:31.668434: step 10960, loss 0.62835.
Test: 2018-08-09T07:25:52.807637: step 10960, loss 0.5503.
Train: 2018-08-09T07:25:55.921917: step 10961, loss 0.578961.
Train: 2018-08-09T07:25:59.033189: step 10962, loss 0.529918.
Train: 2018-08-09T07:26:02.146466: step 10963, loss 0.72561.
Train: 2018-08-09T07:26:05.264757: step 10964, loss 0.562685.
Train: 2018-08-09T07:26:08.361992: step 10965, loss 0.482033.
Train: 2018-08-09T07:26:11.455216: step 10966, loss 0.594974.
Train: 2018-08-09T07:26:14.551448: step 10967, loss 0.498615.
Train: 2018-08-09T07:26:17.697813: step 10968, loss 0.498731.
Train: 2018-08-09T07:26:20.818109: step 10969, loss 0.498742.
Train: 2018-08-09T07:26:23.927376: step 10970, loss 0.594909.
Test: 2018-08-09T07:26:45.040510: step 10970, loss 0.548389.
Train: 2018-08-09T07:26:48.131728: step 10971, loss 0.562821.
Train: 2018-08-09T07:26:51.251022: step 10972, loss 0.562811.
Train: 2018-08-09T07:26:54.364299: step 10973, loss 0.49855.
Train: 2018-08-09T07:26:57.464542: step 10974, loss 0.514515.
Train: 2018-08-09T07:27:00.552753: step 10975, loss 0.546635.
Train: 2018-08-09T07:27:03.662019: step 10976, loss 0.53038.
Train: 2018-08-09T07:27:06.756246: step 10977, loss 0.54645.
Train: 2018-08-09T07:27:09.878547: step 10978, loss 0.627716.
Train: 2018-08-09T07:27:13.005862: step 10979, loss 0.644056.
Train: 2018-08-09T07:27:16.084046: step 10980, loss 0.627718.
Test: 2018-08-09T07:27:37.273383: step 10980, loss 0.550488.
Train: 2018-08-09T07:27:40.417742: step 10981, loss 0.497612.
Train: 2018-08-09T07:27:43.512972: step 10982, loss 0.627743.
Train: 2018-08-09T07:27:46.637278: step 10983, loss 0.611412.
Train: 2018-08-09T07:27:49.703431: step 10984, loss 0.53031.
Train: 2018-08-09T07:27:52.820719: step 10985, loss 0.497968.
Train: 2018-08-09T07:27:55.913943: step 10986, loss 0.578878.
Train: 2018-08-09T07:27:58.981097: step 10987, loss 0.578877.
Train: 2018-08-09T07:28:02.074321: step 10988, loss 0.530329.
Train: 2018-08-09T07:28:05.190607: step 10989, loss 0.530319.
Train: 2018-08-09T07:28:08.311905: step 10990, loss 0.530281.
Test: 2018-08-09T07:28:29.457125: step 10990, loss 0.549309.
Train: 2018-08-09T07:28:32.556365: step 10991, loss 0.497744.
Train: 2018-08-09T07:28:35.772917: step 10992, loss 0.56261.
Train: 2018-08-09T07:28:38.859122: step 10993, loss 0.578988.
Train: 2018-08-09T07:28:41.974404: step 10994, loss 0.562563.
Train: 2018-08-09T07:28:45.070637: step 10995, loss 0.480732.
Train: 2018-08-09T07:28:48.184917: step 10996, loss 0.611721.
Train: 2018-08-09T07:28:51.294183: step 10997, loss 0.529657.
Train: 2018-08-09T07:28:54.360335: step 10998, loss 0.578931.
Train: 2018-08-09T07:28:57.448546: step 10999, loss 0.628515.
Train: 2018-08-09T07:29:00.556811: step 11000, loss 0.496032.
Test: 2018-08-09T07:29:21.694008: step 11000, loss 0.546888.
Train: 2018-08-09T07:29:26.501791: step 11001, loss 0.495206.
Train: 2018-08-09T07:29:29.596017: step 11002, loss 0.562238.
Train: 2018-08-09T07:29:32.684228: step 11003, loss 0.512854.
Train: 2018-08-09T07:29:35.789484: step 11004, loss 0.579586.
Train: 2018-08-09T07:29:38.856639: step 11005, loss 0.528959.
Train: 2018-08-09T07:29:41.979943: step 11006, loss 0.545261.
Train: 2018-08-09T07:29:45.055119: step 11007, loss 0.457337.
Train: 2018-08-09T07:29:48.140321: step 11008, loss 0.559832.
Train: 2018-08-09T07:29:51.234548: step 11009, loss 0.709038.
Train: 2018-08-09T07:29:54.322759: step 11010, loss 0.634552.
Test: 2018-08-09T07:30:15.425867: step 11010, loss 0.549289.
Train: 2018-08-09T07:30:18.563207: step 11011, loss 0.63436.
Train: 2018-08-09T07:30:21.657434: step 11012, loss 0.613245.
Train: 2018-08-09T07:30:24.762690: step 11013, loss 0.595432.
Train: 2018-08-09T07:30:27.866943: step 11014, loss 0.561417.
Train: 2018-08-09T07:30:30.984231: step 11015, loss 0.662464.
Train: 2018-08-09T07:30:34.046373: step 11016, loss 0.496684.
Train: 2018-08-09T07:30:37.188728: step 11017, loss 0.628165.
Train: 2018-08-09T07:30:40.302005: step 11018, loss 0.579029.
Train: 2018-08-09T07:30:43.389213: step 11019, loss 0.546544.
Train: 2018-08-09T07:30:46.449349: step 11020, loss 0.756456.
Test: 2018-08-09T07:31:07.557470: step 11020, loss 0.549034.
Train: 2018-08-09T07:31:10.625627: step 11021, loss 0.594836.
Train: 2018-08-09T07:31:13.719854: step 11022, loss 0.578874.
Train: 2018-08-09T07:31:16.792022: step 11023, loss 0.578791.
Train: 2018-08-09T07:31:19.885246: step 11024, loss 0.61027.
Train: 2018-08-09T07:31:22.991505: step 11025, loss 0.408065.
Train: 2018-08-09T07:31:26.101774: step 11026, loss 0.53199.
Train: 2018-08-09T07:31:29.187979: step 11027, loss 0.62578.
Train: 2018-08-09T07:31:32.258142: step 11028, loss 0.578889.
Train: 2018-08-09T07:31:35.347355: step 11029, loss 0.594483.
Train: 2018-08-09T07:31:38.445593: step 11030, loss 0.547808.
Test: 2018-08-09T07:31:59.561735: step 11030, loss 0.551186.
Train: 2018-08-09T07:32:02.631898: step 11031, loss 0.656543.
Train: 2018-08-09T07:32:05.744172: step 11032, loss 0.486031.
Train: 2018-08-09T07:32:08.835391: step 11033, loss 0.501586.
Train: 2018-08-09T07:32:11.917585: step 11034, loss 0.532493.
Train: 2018-08-09T07:32:15.034873: step 11035, loss 0.578921.
Train: 2018-08-09T07:32:18.122081: step 11036, loss 0.57891.
Train: 2018-08-09T07:32:21.225332: step 11037, loss 0.609959.
Train: 2018-08-09T07:32:24.327580: step 11038, loss 0.5168.
Train: 2018-08-09T07:32:27.398745: step 11039, loss 0.594473.
Train: 2018-08-09T07:32:30.486956: step 11040, loss 0.594468.
Test: 2018-08-09T07:32:51.581040: step 11040, loss 0.548743.
Train: 2018-08-09T07:32:54.670253: step 11041, loss 0.547785.
Train: 2018-08-09T07:32:57.850709: step 11042, loss 0.594456.
Train: 2018-08-09T07:33:00.936914: step 11043, loss 0.485393.
Train: 2018-08-09T07:33:04.045178: step 11044, loss 0.563109.
Train: 2018-08-09T07:33:07.124365: step 11045, loss 0.453277.
Train: 2018-08-09T07:33:10.211573: step 11046, loss 0.562856.
Train: 2018-08-09T07:33:13.326855: step 11047, loss 0.627825.
Train: 2018-08-09T07:33:16.461189: step 11048, loss 0.610185.
Train: 2018-08-09T07:33:19.545389: step 11049, loss 0.612154.
Train: 2018-08-09T07:33:22.657664: step 11050, loss 0.530395.
Test: 2018-08-09T07:33:43.814915: step 11050, loss 0.549578.
Train: 2018-08-09T07:33:46.913153: step 11051, loss 0.546805.
Train: 2018-08-09T07:33:49.993342: step 11052, loss 0.483592.
Train: 2018-08-09T07:33:53.084560: step 11053, loss 0.498352.
Train: 2018-08-09T07:33:56.165752: step 11054, loss 0.545114.
Train: 2018-08-09T07:33:59.235915: step 11055, loss 0.54974.
Train: 2018-08-09T07:34:02.377267: step 11056, loss 0.580387.
Train: 2018-08-09T07:34:05.457457: step 11057, loss 0.513742.
Train: 2018-08-09T07:34:08.588782: step 11058, loss 0.528928.
Train: 2018-08-09T07:34:11.706070: step 11059, loss 0.545633.
Train: 2018-08-09T07:34:14.783251: step 11060, loss 0.531245.
Test: 2018-08-09T07:34:35.887362: step 11060, loss 0.548054.
Train: 2018-08-09T07:34:38.978580: step 11061, loss 0.579344.
Train: 2018-08-09T07:34:42.063783: step 11062, loss 0.594139.
Train: 2018-08-09T07:34:45.159012: step 11063, loss 0.494604.
Train: 2018-08-09T07:34:48.292343: step 11064, loss 0.495925.
Train: 2018-08-09T07:34:51.369524: step 11065, loss 0.54302.
Train: 2018-08-09T07:34:54.495836: step 11066, loss 0.513011.
Train: 2018-08-09T07:34:57.583044: step 11067, loss 0.612658.
Train: 2018-08-09T07:35:00.692311: step 11068, loss 0.580531.
Train: 2018-08-09T07:35:03.793556: step 11069, loss 0.540467.
Train: 2018-08-09T07:35:06.880764: step 11070, loss 0.656111.
Test: 2018-08-09T07:35:27.982869: step 11070, loss 0.547984.
Train: 2018-08-09T07:35:31.092136: step 11071, loss 0.491612.
Train: 2018-08-09T07:35:34.184357: step 11072, loss 0.673414.
Train: 2018-08-09T07:35:37.275576: step 11073, loss 0.645992.
Train: 2018-08-09T07:35:40.384842: step 11074, loss 0.578939.
Train: 2018-08-09T07:35:43.485085: step 11075, loss 0.542974.
Train: 2018-08-09T07:35:46.567280: step 11076, loss 0.56142.
Train: 2018-08-09T07:35:49.654488: step 11077, loss 0.497064.
Train: 2018-08-09T07:35:52.738688: step 11078, loss 0.495886.
Train: 2018-08-09T07:35:55.823891: step 11079, loss 0.578987.
Train: 2018-08-09T07:35:58.920123: step 11080, loss 0.563166.
Test: 2018-08-09T07:36:20.030249: step 11080, loss 0.549528.
Train: 2018-08-09T07:36:23.128486: step 11081, loss 0.579365.
Train: 2018-08-09T07:36:26.261816: step 11082, loss 0.546085.
Train: 2018-08-09T07:36:29.347019: step 11083, loss 0.546745.
Train: 2018-08-09T07:36:32.422195: step 11084, loss 0.562465.
Train: 2018-08-09T07:36:35.537478: step 11085, loss 0.546543.
Train: 2018-08-09T07:36:38.661785: step 11086, loss 0.562415.
Train: 2018-08-09T07:36:41.767041: step 11087, loss 0.54635.
Train: 2018-08-09T07:36:44.872296: step 11088, loss 0.512966.
Train: 2018-08-09T07:36:47.960507: step 11089, loss 0.562849.
Train: 2018-08-09T07:36:51.040697: step 11090, loss 0.529105.
Test: 2018-08-09T07:37:12.144807: step 11090, loss 0.549332.
Train: 2018-08-09T07:37:15.247055: step 11091, loss 0.694955.
Train: 2018-08-09T07:37:18.388407: step 11092, loss 0.545678.
Train: 2018-08-09T07:37:21.519732: step 11093, loss 0.595573.
Train: 2018-08-09T07:37:24.652060: step 11094, loss 0.478483.
Train: 2018-08-09T07:37:27.768345: step 11095, loss 0.563003.
Train: 2018-08-09T07:37:30.853548: step 11096, loss 0.547046.
Train: 2018-08-09T07:37:33.965823: step 11097, loss 0.612059.
Train: 2018-08-09T07:37:37.064060: step 11098, loss 0.611107.
Train: 2018-08-09T07:37:40.169316: step 11099, loss 0.52889.
Train: 2018-08-09T07:37:43.248503: step 11100, loss 0.643758.
Test: 2018-08-09T07:38:04.344592: step 11100, loss 0.546881.
Train: 2018-08-09T07:38:09.208523: step 11101, loss 0.611431.
Train: 2018-08-09T07:38:12.308766: step 11102, loss 0.548139.
Train: 2018-08-09T07:38:15.397979: step 11103, loss 0.497159.
Train: 2018-08-09T07:38:18.483182: step 11104, loss 0.530416.
Train: 2018-08-09T07:38:21.580417: step 11105, loss 0.627605.
Train: 2018-08-09T07:38:24.681662: step 11106, loss 0.611074.
Train: 2018-08-09T07:38:27.750822: step 11107, loss 0.546877.
Train: 2018-08-09T07:38:30.844046: step 11108, loss 0.514294.
Train: 2018-08-09T07:38:33.951307: step 11109, loss 0.562799.
Train: 2018-08-09T07:38:37.024478: step 11110, loss 0.659184.
Test: 2018-08-09T07:38:58.099512: step 11110, loss 0.547761.
Train: 2018-08-09T07:39:01.204767: step 11111, loss 0.691754.
Train: 2018-08-09T07:39:04.327068: step 11112, loss 0.690459.
Train: 2018-08-09T07:39:07.407257: step 11113, loss 0.547168.
Train: 2018-08-09T07:39:10.534572: step 11114, loss 0.531613.
Train: 2018-08-09T07:39:13.634815: step 11115, loss 0.531798.
Train: 2018-08-09T07:39:16.752103: step 11116, loss 0.453685.
Train: 2018-08-09T07:39:19.903482: step 11117, loss 0.657106.
Train: 2018-08-09T07:39:23.042828: step 11118, loss 0.578878.
Train: 2018-08-09T07:39:26.169140: step 11119, loss 0.625667.
Train: 2018-08-09T07:39:29.242311: step 11120, loss 0.687794.
Test: 2018-08-09T07:39:50.338400: step 11120, loss 0.548349.
Train: 2018-08-09T07:39:53.427613: step 11121, loss 0.609895.
Train: 2018-08-09T07:39:56.558938: step 11122, loss 0.532708.
Train: 2018-08-09T07:39:59.687256: step 11123, loss 0.625039.
Train: 2018-08-09T07:40:02.825600: step 11124, loss 0.56371.
Train: 2018-08-09T07:40:05.938877: step 11125, loss 0.640009.
Train: 2018-08-09T07:40:09.024079: step 11126, loss 0.548705.
Train: 2018-08-09T07:40:12.155405: step 11127, loss 0.563961.
Train: 2018-08-09T07:40:15.246624: step 11128, loss 0.639471.
Train: 2018-08-09T07:40:18.343858: step 11129, loss 0.50398.
Train: 2018-08-09T07:40:21.447109: step 11130, loss 0.459101.
Test: 2018-08-09T07:40:42.550217: step 11130, loss 0.551769.
Train: 2018-08-09T07:40:45.668507: step 11131, loss 0.489031.
Train: 2018-08-09T07:40:48.749699: step 11132, loss 0.488769.
Train: 2018-08-09T07:40:51.844929: step 11133, loss 0.503476.
Train: 2018-08-09T07:40:54.945171: step 11134, loss 0.594246.
Train: 2018-08-09T07:40:57.995281: step 11135, loss 0.563728.
Train: 2018-08-09T07:41:01.108558: step 11136, loss 0.578982.
Train: 2018-08-09T07:41:04.010273: step 11137, loss 0.547142.
Train: 2018-08-09T07:41:07.103497: step 11138, loss 0.578828.
Train: 2018-08-09T07:41:10.169649: step 11139, loss 0.609948.
Train: 2018-08-09T07:41:13.252846: step 11140, loss 0.516391.
Test: 2018-08-09T07:41:34.359965: step 11140, loss 0.548466.
Train: 2018-08-09T07:41:37.447172: step 11141, loss 0.531588.
Train: 2018-08-09T07:41:40.583511: step 11142, loss 0.593472.
Train: 2018-08-09T07:41:43.669716: step 11143, loss 0.611751.
Train: 2018-08-09T07:41:46.773970: step 11144, loss 0.693093.
Train: 2018-08-09T07:41:49.868196: step 11145, loss 0.530251.
Train: 2018-08-09T07:41:52.988492: step 11146, loss 0.532721.
Train: 2018-08-09T07:41:56.123828: step 11147, loss 0.610753.
Train: 2018-08-09T07:41:59.229084: step 11148, loss 0.626573.
Train: 2018-08-09T07:42:02.330330: step 11149, loss 0.515807.
Train: 2018-08-09T07:42:05.421548: step 11150, loss 0.516091.
Test: 2018-08-09T07:42:26.539696: step 11150, loss 0.550825.
Train: 2018-08-09T07:42:29.630914: step 11151, loss 0.594565.
Train: 2018-08-09T07:42:32.712106: step 11152, loss 0.531776.
Train: 2018-08-09T07:42:35.849448: step 11153, loss 0.594582.
Train: 2018-08-09T07:42:38.955706: step 11154, loss 0.484599.
Train: 2018-08-09T07:42:42.098061: step 11155, loss 0.531632.
Train: 2018-08-09T07:42:45.183264: step 11156, loss 0.594654.
Train: 2018-08-09T07:42:48.271474: step 11157, loss 0.452332.
Train: 2018-08-09T07:42:51.362693: step 11158, loss 0.642405.
Train: 2018-08-09T07:42:54.462936: step 11159, loss 0.562961.
Train: 2018-08-09T07:42:57.537109: step 11160, loss 0.594778.
Test: 2018-08-09T07:43:18.619161: step 11160, loss 0.549157.
Train: 2018-08-09T07:43:21.723414: step 11161, loss 0.483114.
Train: 2018-08-09T07:43:24.846718: step 11162, loss 0.562879.
Train: 2018-08-09T07:43:27.938939: step 11163, loss 0.61094.
Train: 2018-08-09T07:43:31.023139: step 11164, loss 0.482497.
Train: 2018-08-09T07:43:34.144438: step 11165, loss 0.514412.
Train: 2018-08-09T07:43:37.278771: step 11166, loss 0.530427.
Train: 2018-08-09T07:43:40.385030: step 11167, loss 0.530236.
Train: 2018-08-09T07:43:43.519363: step 11168, loss 0.578854.
Train: 2018-08-09T07:43:46.611585: step 11169, loss 0.562646.
Train: 2018-08-09T07:43:49.742910: step 11170, loss 0.677365.
Test: 2018-08-09T07:44:10.863063: step 11170, loss 0.548433.
Train: 2018-08-09T07:44:13.979348: step 11171, loss 0.46414.
Train: 2018-08-09T07:44:17.050513: step 11172, loss 0.529737.
Train: 2018-08-09T07:44:20.127695: step 11173, loss 0.562493.
Train: 2018-08-09T07:44:23.236961: step 11174, loss 0.562473.
Train: 2018-08-09T07:44:26.333194: step 11175, loss 0.579016.
Train: 2018-08-09T07:44:29.440455: step 11176, loss 0.562409.
Train: 2018-08-09T07:44:32.554735: step 11177, loss 0.595963.
Train: 2018-08-09T07:44:35.653975: step 11178, loss 0.56256.
Train: 2018-08-09T07:44:38.759231: step 11179, loss 0.445944.
Train: 2018-08-09T07:44:41.856465: step 11180, loss 0.57942.
Test: 2018-08-09T07:45:02.988651: step 11180, loss 0.546789.
Train: 2018-08-09T07:45:06.091901: step 11181, loss 0.412149.
Train: 2018-08-09T07:45:09.181114: step 11182, loss 0.67937.
Train: 2018-08-09T07:45:12.264312: step 11183, loss 0.612489.
Train: 2018-08-09T07:45:15.357535: step 11184, loss 0.42805.
Train: 2018-08-09T07:45:18.459783: step 11185, loss 0.393879.
Train: 2018-08-09T07:45:21.554010: step 11186, loss 0.493853.
Train: 2018-08-09T07:45:24.662274: step 11187, loss 0.544455.
Train: 2018-08-09T07:45:27.756501: step 11188, loss 0.615648.
Train: 2018-08-09T07:45:30.842706: step 11189, loss 0.612184.
Train: 2018-08-09T07:45:33.932923: step 11190, loss 0.564952.
Test: 2018-08-09T07:45:55.017982: step 11190, loss 0.54662.
Train: 2018-08-09T07:45:58.202448: step 11191, loss 0.473257.
Train: 2018-08-09T07:46:01.301688: step 11192, loss 0.704755.
Train: 2018-08-09T07:46:04.412960: step 11193, loss 0.634512.
Train: 2018-08-09T07:46:07.533256: step 11194, loss 0.526814.
Train: 2018-08-09T07:46:10.628486: step 11195, loss 0.475021.
Train: 2018-08-09T07:46:13.725720: step 11196, loss 0.562099.
Train: 2018-08-09T07:46:16.819947: step 11197, loss 0.598405.
Train: 2018-08-09T07:46:19.890110: step 11198, loss 0.561993.
Train: 2018-08-09T07:46:23.012411: step 11199, loss 0.459544.
Train: 2018-08-09T07:46:26.112654: step 11200, loss 0.52803.
Test: 2018-08-09T07:46:47.222781: step 11200, loss 0.546878.
Train: 2018-08-09T07:46:52.160909: step 11201, loss 0.631125.
Train: 2018-08-09T07:46:55.248117: step 11202, loss 0.579423.
Train: 2018-08-09T07:46:58.320285: step 11203, loss 0.545327.
Train: 2018-08-09T07:47:01.407493: step 11204, loss 0.579196.
Train: 2018-08-09T07:47:04.501720: step 11205, loss 0.647874.
Train: 2018-08-09T07:47:07.604970: step 11206, loss 0.562456.
Train: 2018-08-09T07:47:10.684157: step 11207, loss 0.579334.
Train: 2018-08-09T07:47:13.780389: step 11208, loss 0.545475.
Train: 2018-08-09T07:47:16.887651: step 11209, loss 0.59609.
Train: 2018-08-09T07:47:20.004938: step 11210, loss 0.528758.
Test: 2018-08-09T07:47:41.170212: step 11210, loss 0.546674.
Train: 2018-08-09T07:47:44.255414: step 11211, loss 0.629503.
Train: 2018-08-09T07:47:47.341619: step 11212, loss 0.595859.
Train: 2018-08-09T07:47:50.414790: step 11213, loss 0.612455.
Train: 2018-08-09T07:47:53.524057: step 11214, loss 0.51262.
Train: 2018-08-09T07:47:56.675435: step 11215, loss 0.562481.
Train: 2018-08-09T07:47:59.760638: step 11216, loss 0.463344.
Train: 2018-08-09T07:48:02.879931: step 11217, loss 0.612045.
Train: 2018-08-09T07:48:05.998222: step 11218, loss 0.595494.
Train: 2018-08-09T07:48:09.134561: step 11219, loss 0.578989.
Train: 2018-08-09T07:48:12.199710: step 11220, loss 0.54612.
Test: 2018-08-09T07:48:33.315852: step 11220, loss 0.550915.
Train: 2018-08-09T07:48:36.457204: step 11221, loss 0.628164.
Train: 2018-08-09T07:48:39.638663: step 11222, loss 0.562583.
Train: 2018-08-09T07:48:42.758959: step 11223, loss 0.546285.
Train: 2018-08-09T07:48:45.847169: step 11224, loss 0.578929.
Train: 2018-08-09T07:48:48.984511: step 11225, loss 0.448788.
Train: 2018-08-09T07:48:52.081745: step 11226, loss 0.546373.
Train: 2018-08-09T07:48:55.188004: step 11227, loss 0.497542.
Train: 2018-08-09T07:48:58.317324: step 11228, loss 0.562611.
Train: 2018-08-09T07:49:01.417567: step 11229, loss 0.611547.
Train: 2018-08-09T07:49:04.528839: step 11230, loss 0.529751.
Test: 2018-08-09T07:49:25.635957: step 11230, loss 0.549696.
Train: 2018-08-09T07:49:28.716146: step 11231, loss 0.562511.
Train: 2018-08-09T07:49:31.814384: step 11232, loss 0.57921.
Train: 2018-08-09T07:49:34.906605: step 11233, loss 0.463911.
Train: 2018-08-09T07:49:38.015872: step 11234, loss 0.59503.
Train: 2018-08-09T07:49:41.109095: step 11235, loss 0.51363.
Train: 2018-08-09T07:49:44.235407: step 11236, loss 0.595536.
Train: 2018-08-09T07:49:47.338658: step 11237, loss 0.578806.
Train: 2018-08-09T07:49:50.465973: step 11238, loss 0.612881.
Train: 2018-08-09T07:49:53.565213: step 11239, loss 0.578761.
Train: 2018-08-09T07:49:56.633370: step 11240, loss 0.595119.
Test: 2018-08-09T07:50:17.718430: step 11240, loss 0.546969.
Train: 2018-08-09T07:50:20.800624: step 11241, loss 0.545921.
Train: 2018-08-09T07:50:23.949997: step 11242, loss 0.578685.
Train: 2018-08-09T07:50:27.038208: step 11243, loss 0.595593.
Train: 2018-08-09T07:50:30.127422: step 11244, loss 0.56312.
Train: 2018-08-09T07:50:33.228667: step 11245, loss 0.496939.
Train: 2018-08-09T07:50:36.334926: step 11246, loss 0.545863.
Train: 2018-08-09T07:50:39.442187: step 11247, loss 0.546394.
Train: 2018-08-09T07:50:42.540424: step 11248, loss 0.579058.
Train: 2018-08-09T07:50:45.659718: step 11249, loss 0.72661.
Train: 2018-08-09T07:50:48.755950: step 11250, loss 0.644265.
Test: 2018-08-09T07:51:09.869084: step 11250, loss 0.548066.
Train: 2018-08-09T07:51:13.006425: step 11251, loss 0.513948.
Train: 2018-08-09T07:51:16.112684: step 11252, loss 0.530307.
Train: 2018-08-09T07:51:19.229972: step 11253, loss 0.546556.
Train: 2018-08-09T07:51:22.348262: step 11254, loss 0.611174.
Train: 2018-08-09T07:51:25.447503: step 11255, loss 0.562765.
Train: 2018-08-09T07:51:28.535713: step 11256, loss 0.482371.
Train: 2018-08-09T07:51:31.637961: step 11257, loss 0.67536.
Train: 2018-08-09T07:51:34.725169: step 11258, loss 0.562821.
Train: 2018-08-09T07:51:37.817391: step 11259, loss 0.498763.
Train: 2018-08-09T07:51:40.911617: step 11260, loss 0.482781.
Test: 2018-08-09T07:52:02.007706: step 11260, loss 0.547812.
Train: 2018-08-09T07:52:05.081879: step 11261, loss 0.626964.
Train: 2018-08-09T07:52:08.205184: step 11262, loss 0.466648.
Train: 2018-08-09T07:52:11.311442: step 11263, loss 0.546751.
Train: 2018-08-09T07:52:14.425722: step 11264, loss 0.611061.
Train: 2018-08-09T07:52:17.500898: step 11265, loss 0.514458.
Train: 2018-08-09T07:52:20.590112: step 11266, loss 0.578852.
Train: 2018-08-09T07:52:23.694365: step 11267, loss 0.611246.
Train: 2018-08-09T07:52:26.812656: step 11268, loss 0.514229.
Train: 2018-08-09T07:52:29.929943: step 11269, loss 0.676004.
Train: 2018-08-09T07:52:33.032191: step 11270, loss 0.643568.
Test: 2018-08-09T07:52:54.150339: step 11270, loss 0.549477.
Train: 2018-08-09T07:52:57.260608: step 11271, loss 0.611117.
Train: 2018-08-09T07:53:00.345811: step 11272, loss 0.562786.
Train: 2018-08-09T07:53:03.427003: step 11273, loss 0.627004.
Train: 2018-08-09T07:53:06.507192: step 11274, loss 0.53089.
Train: 2018-08-09T07:53:09.626485: step 11275, loss 0.562907.
Train: 2018-08-09T07:53:12.720712: step 11276, loss 0.515167.
Train: 2018-08-09T07:53:15.834992: step 11277, loss 0.578859.
Train: 2018-08-09T07:53:18.935235: step 11278, loss 0.578858.
Train: 2018-08-09T07:53:22.035478: step 11279, loss 0.578858.
Train: 2018-08-09T07:53:25.125694: step 11280, loss 0.515415.
Test: 2018-08-09T07:53:46.224791: step 11280, loss 0.549937.
Train: 2018-08-09T07:53:49.337065: step 11281, loss 0.499558.
Train: 2018-08-09T07:53:52.458364: step 11282, loss 0.658252.
Train: 2018-08-09T07:53:55.553593: step 11283, loss 0.54712.
Train: 2018-08-09T07:53:58.664865: step 11284, loss 0.562981.
Train: 2018-08-09T07:54:01.791177: step 11285, loss 0.594722.
Train: 2018-08-09T07:54:04.864348: step 11286, loss 0.626432.
Train: 2018-08-09T07:54:07.974617: step 11287, loss 0.626356.
Train: 2018-08-09T07:54:11.071852: step 11288, loss 0.594608.
Train: 2018-08-09T07:54:14.188137: step 11289, loss 0.515879.
Train: 2018-08-09T07:54:17.258300: step 11290, loss 0.625939.
Test: 2018-08-09T07:54:38.348373: step 11290, loss 0.548541.
Train: 2018-08-09T07:54:41.467666: step 11291, loss 0.641634.
Train: 2018-08-09T07:54:44.612026: step 11292, loss 0.547623.
Train: 2018-08-09T07:54:47.723298: step 11293, loss 0.485836.
Train: 2018-08-09T07:54:50.801482: step 11294, loss 0.578985.
Train: 2018-08-09T07:54:53.892701: step 11295, loss 0.61029.
Train: 2018-08-09T07:54:57.001967: step 11296, loss 0.563285.
Train: 2018-08-09T07:55:00.105218: step 11297, loss 0.547721.
Train: 2018-08-09T07:55:03.202453: step 11298, loss 0.485469.
Train: 2018-08-09T07:55:06.308711: step 11299, loss 0.531996.
Train: 2018-08-09T07:55:09.403941: step 11300, loss 0.563331.
Test: 2018-08-09T07:55:30.504041: step 11300, loss 0.550242.
Train: 2018-08-09T07:55:35.301797: step 11301, loss 0.578907.
Train: 2018-08-09T07:55:38.385996: step 11302, loss 0.515949.
Train: 2018-08-09T07:55:41.464180: step 11303, loss 0.563641.
Train: 2018-08-09T07:55:44.551388: step 11304, loss 0.484315.
Train: 2018-08-09T07:55:47.624559: step 11305, loss 0.610814.
Train: 2018-08-09T07:55:50.723799: step 11306, loss 0.546781.
Train: 2018-08-09T07:55:53.837076: step 11307, loss 0.499346.
Train: 2018-08-09T07:55:56.934311: step 11308, loss 0.594815.
Train: 2018-08-09T07:56:00.032548: step 11309, loss 0.482316.
Train: 2018-08-09T07:56:03.138807: step 11310, loss 0.481793.
Test: 2018-08-09T07:56:24.192784: step 11310, loss 0.551544.
Train: 2018-08-09T07:56:27.329122: step 11311, loss 0.595183.
Train: 2018-08-09T07:56:30.393269: step 11312, loss 0.694714.
Train: 2018-08-09T07:56:33.469448: step 11313, loss 0.5464.
Train: 2018-08-09T07:56:36.571696: step 11314, loss 0.62791.
Train: 2018-08-09T07:56:39.690989: step 11315, loss 0.480553.
Train: 2018-08-09T07:56:42.787221: step 11316, loss 0.693433.
Train: 2018-08-09T07:56:45.898493: step 11317, loss 0.56188.
Train: 2018-08-09T07:56:48.976677: step 11318, loss 0.610778.
Train: 2018-08-09T07:56:52.061880: step 11319, loss 0.579555.
Train: 2018-08-09T07:56:55.166133: step 11320, loss 0.562563.
Test: 2018-08-09T07:57:16.248185: step 11320, loss 0.55115.
Train: 2018-08-09T07:57:19.337398: step 11321, loss 0.562891.
Train: 2018-08-09T07:57:22.422600: step 11322, loss 0.530553.
Train: 2018-08-09T07:57:25.563952: step 11323, loss 0.594862.
Train: 2018-08-09T07:57:28.744408: step 11324, loss 0.481319.
Train: 2018-08-09T07:57:31.850667: step 11325, loss 0.627777.
Train: 2018-08-09T07:57:34.922835: step 11326, loss 0.595336.
Train: 2018-08-09T07:57:37.996006: step 11327, loss 0.498409.
Train: 2018-08-09T07:57:41.074190: step 11328, loss 0.529987.
Train: 2018-08-09T07:57:44.165409: step 11329, loss 0.611277.
Train: 2018-08-09T07:57:47.255625: step 11330, loss 0.562481.
Test: 2018-08-09T07:58:08.371767: step 11330, loss 0.550479.
Train: 2018-08-09T07:58:11.492063: step 11331, loss 0.48053.
Train: 2018-08-09T07:58:14.596316: step 11332, loss 0.481989.
Train: 2018-08-09T07:58:17.698564: step 11333, loss 0.398244.
Train: 2018-08-09T07:58:20.789783: step 11334, loss 0.599615.
Train: 2018-08-09T07:58:23.884010: step 11335, loss 0.513721.
Train: 2018-08-09T07:58:26.989265: step 11336, loss 0.59601.
Train: 2018-08-09T07:58:30.068452: step 11337, loss 0.560726.
Train: 2018-08-09T07:58:33.161676: step 11338, loss 0.513542.
Train: 2018-08-09T07:58:36.261919: step 11339, loss 0.57988.
Train: 2018-08-09T07:58:39.381212: step 11340, loss 0.58111.
Test: 2018-08-09T07:59:00.465269: step 11340, loss 0.547026.
Train: 2018-08-09T07:59:03.622663: step 11341, loss 0.458711.
Train: 2018-08-09T07:59:06.755994: step 11342, loss 0.561867.
Train: 2018-08-09T07:59:09.852226: step 11343, loss 0.49549.
Train: 2018-08-09T07:59:12.938432: step 11344, loss 0.597417.
Train: 2018-08-09T07:59:16.050706: step 11345, loss 0.631586.
Train: 2018-08-09T07:59:19.154960: step 11346, loss 0.648735.
Train: 2018-08-09T07:59:22.265229: step 11347, loss 0.494154.
Train: 2018-08-09T07:59:25.388533: step 11348, loss 0.460266.
Train: 2018-08-09T07:59:28.512840: step 11349, loss 0.628936.
Train: 2018-08-09T07:59:31.610074: step 11350, loss 0.528928.
Test: 2018-08-09T07:59:52.715187: step 11350, loss 0.550308.
Train: 2018-08-09T07:59:55.819441: step 11351, loss 0.596187.
Train: 2018-08-09T07:59:58.916675: step 11352, loss 0.681386.
Train: 2018-08-09T08:00:02.032960: step 11353, loss 0.512557.
Train: 2018-08-09T08:00:05.141224: step 11354, loss 0.579473.
Train: 2018-08-09T08:00:08.235451: step 11355, loss 0.512731.
Train: 2018-08-09T08:00:11.328675: step 11356, loss 0.545538.
Train: 2018-08-09T08:00:14.453984: step 11357, loss 0.546224.
Train: 2018-08-09T08:00:17.537182: step 11358, loss 0.512629.
Train: 2018-08-09T08:00:20.653467: step 11359, loss 0.529216.
Train: 2018-08-09T08:00:23.799832: step 11360, loss 0.595683.
Test: 2018-08-09T08:00:44.936028: step 11360, loss 0.548757.
Train: 2018-08-09T08:00:48.039278: step 11361, loss 0.612323.
Train: 2018-08-09T08:00:51.159574: step 11362, loss 0.562454.
Train: 2018-08-09T08:00:54.269844: step 11363, loss 0.479446.
Train: 2018-08-09T08:00:57.364070: step 11364, loss 0.645527.
Train: 2018-08-09T08:01:00.469326: step 11365, loss 0.612266.
Train: 2018-08-09T08:01:03.558540: step 11366, loss 0.6122.
Train: 2018-08-09T08:01:06.673822: step 11367, loss 0.595566.
Train: 2018-08-09T08:01:09.819185: step 11368, loss 0.51302.
Train: 2018-08-09T08:01:12.918425: step 11369, loss 0.546054.
Train: 2018-08-09T08:01:16.001623: step 11370, loss 0.480282.
Test: 2018-08-09T08:01:37.138821: step 11370, loss 0.548358.
Train: 2018-08-09T08:01:40.260119: step 11371, loss 0.562531.
Train: 2018-08-09T08:01:43.352341: step 11372, loss 0.62836.
Train: 2018-08-09T08:01:46.444562: step 11373, loss 0.578977.
Train: 2018-08-09T08:01:49.549818: step 11374, loss 0.529698.
Train: 2018-08-09T08:01:52.623991: step 11375, loss 0.595384.
Train: 2018-08-09T08:01:55.723231: step 11376, loss 0.562574.
Train: 2018-08-09T08:01:58.796402: step 11377, loss 0.513414.
Train: 2018-08-09T08:02:01.919706: step 11378, loss 0.513417.
Train: 2018-08-09T08:02:05.069079: step 11379, loss 0.513374.
Train: 2018-08-09T08:02:08.266581: step 11380, loss 0.513216.
Test: 2018-08-09T08:02:29.683523: step 11380, loss 0.548314.
Train: 2018-08-09T08:02:32.795797: step 11381, loss 0.579057.
Train: 2018-08-09T08:02:35.909074: step 11382, loss 0.678144.
Train: 2018-08-09T08:02:39.029370: step 11383, loss 0.529468.
Train: 2018-08-09T08:02:42.126605: step 11384, loss 0.612045.
Train: 2018-08-09T08:02:45.197770: step 11385, loss 0.529537.
Train: 2018-08-09T08:02:48.310045: step 11386, loss 0.595488.
Train: 2018-08-09T08:02:51.400261: step 11387, loss 0.447273.
Train: 2018-08-09T08:02:54.504514: step 11388, loss 0.645101.
Train: 2018-08-09T08:02:57.648874: step 11389, loss 0.644892.
Train: 2018-08-09T08:03:00.764157: step 11390, loss 0.595517.
Test: 2018-08-09T08:03:21.878294: step 11390, loss 0.545966.
Train: 2018-08-09T08:03:25.003603: step 11391, loss 0.578967.
Train: 2018-08-09T08:03:28.149968: step 11392, loss 0.529888.
Train: 2018-08-09T08:03:31.242190: step 11393, loss 0.644244.
Train: 2018-08-09T08:03:34.338422: step 11394, loss 0.530071.
Train: 2018-08-09T08:03:37.463731: step 11395, loss 0.465149.
Train: 2018-08-09T08:03:40.626139: step 11396, loss 0.627664.
Train: 2018-08-09T08:03:43.756462: step 11397, loss 0.530206.
Train: 2018-08-09T08:03:46.875755: step 11398, loss 0.627583.
Train: 2018-08-09T08:03:49.970984: step 11399, loss 0.65991.
Train: 2018-08-09T08:03:53.072230: step 11400, loss 0.562731.
Test: 2018-08-09T08:04:14.198399: step 11400, loss 0.547668.
Train: 2018-08-09T08:04:19.049296: step 11401, loss 0.611104.
Train: 2018-08-09T08:04:22.146530: step 11402, loss 0.514621.
Train: 2018-08-09T08:04:25.272842: step 11403, loss 0.562836.
Train: 2018-08-09T08:04:28.375090: step 11404, loss 0.610869.
Train: 2018-08-09T08:04:31.500400: step 11405, loss 0.530963.
Train: 2018-08-09T08:04:34.591618: step 11406, loss 0.578857.
Train: 2018-08-09T08:04:37.685845: step 11407, loss 0.483329.
Train: 2018-08-09T08:04:40.758013: step 11408, loss 0.578878.
Train: 2018-08-09T08:04:43.891344: step 11409, loss 0.403638.
Train: 2018-08-09T08:04:46.975544: step 11410, loss 0.498777.
Test: 2018-08-09T08:05:08.061607: step 11410, loss 0.551307.
Train: 2018-08-09T08:05:11.154830: step 11411, loss 0.57844.
Train: 2018-08-09T08:05:14.269110: step 11412, loss 0.49706.
Train: 2018-08-09T08:05:17.382387: step 11413, loss 0.563952.
Train: 2018-08-09T08:05:20.469595: step 11414, loss 0.545449.
Train: 2018-08-09T08:05:23.593902: step 11415, loss 0.494646.
Train: 2018-08-09T08:05:26.685121: step 11416, loss 0.545021.
Train: 2018-08-09T08:05:29.804414: step 11417, loss 0.56266.
Train: 2018-08-09T08:05:32.974843: step 11418, loss 0.57537.
Train: 2018-08-09T08:05:36.062051: step 11419, loss 0.419312.
Train: 2018-08-09T08:05:39.134219: step 11420, loss 0.6144.
Test: 2018-08-09T08:06:00.209253: step 11420, loss 0.545871.
Train: 2018-08-09T08:06:03.292449: step 11421, loss 0.592267.
Train: 2018-08-09T08:06:06.375647: step 11422, loss 0.53227.
Train: 2018-08-09T08:06:09.486919: step 11423, loss 0.438157.
Train: 2018-08-09T08:06:12.600196: step 11424, loss 0.611154.
Train: 2018-08-09T08:06:15.713474: step 11425, loss 0.682156.
Train: 2018-08-09T08:06:18.839785: step 11426, loss 0.700416.
Train: 2018-08-09T08:06:21.941031: step 11427, loss 0.543353.
Train: 2018-08-09T08:06:25.061327: step 11428, loss 0.477724.
Train: 2018-08-09T08:06:28.151543: step 11429, loss 0.595006.
Train: 2018-08-09T08:06:31.224714: step 11430, loss 0.615661.
Test: 2018-08-09T08:06:52.331833: step 11430, loss 0.547636.
Train: 2018-08-09T08:06:55.458144: step 11431, loss 0.629272.
Train: 2018-08-09T08:06:58.561394: step 11432, loss 0.678102.
Train: 2018-08-09T08:07:01.667653: step 11433, loss 0.61148.
Train: 2018-08-09T08:07:04.747842: step 11434, loss 0.594901.
Train: 2018-08-09T08:07:07.866133: step 11435, loss 0.499177.
Train: 2018-08-09T08:07:10.973394: step 11436, loss 0.531229.
Train: 2018-08-09T08:07:14.033530: step 11437, loss 0.515542.
Train: 2018-08-09T08:07:16.945272: step 11438, loss 0.630485.
Train: 2018-08-09T08:07:20.059552: step 11439, loss 0.547347.
Train: 2018-08-09T08:07:23.144754: step 11440, loss 0.515949.
Test: 2018-08-09T08:07:44.326070: step 11440, loss 0.550804.
Train: 2018-08-09T08:07:47.422302: step 11441, loss 0.626023.
Train: 2018-08-09T08:07:50.592731: step 11442, loss 0.594565.
Train: 2018-08-09T08:07:53.690969: step 11443, loss 0.531899.
Train: 2018-08-09T08:07:56.793217: step 11444, loss 0.469376.
Train: 2018-08-09T08:07:59.890451: step 11445, loss 0.484931.
Train: 2018-08-09T08:08:02.991696: step 11446, loss 0.516093.
Train: 2018-08-09T08:08:06.080910: step 11447, loss 0.594615.
Train: 2018-08-09T08:08:09.189174: step 11448, loss 0.531516.
Train: 2018-08-09T08:08:12.289417: step 11449, loss 0.563037.
Train: 2018-08-09T08:08:15.405702: step 11450, loss 0.610591.
Test: 2018-08-09T08:08:36.538890: step 11450, loss 0.548064.
Train: 2018-08-09T08:08:39.647153: step 11451, loss 0.56296.
Train: 2018-08-09T08:08:42.751407: step 11452, loss 0.610705.
Train: 2018-08-09T08:08:45.826583: step 11453, loss 0.48325.
Train: 2018-08-09T08:08:48.945876: step 11454, loss 0.578853.
Train: 2018-08-09T08:08:52.033084: step 11455, loss 0.514846.
Train: 2018-08-09T08:08:55.114276: step 11456, loss 0.546802.
Train: 2018-08-09T08:08:58.204492: step 11457, loss 0.498226.
Train: 2018-08-09T08:09:01.321780: step 11458, loss 0.595157.
Train: 2018-08-09T08:09:04.382919: step 11459, loss 0.448771.
Train: 2018-08-09T08:09:07.505220: step 11460, loss 0.594647.
Test: 2018-08-09T08:09:28.672499: step 11460, loss 0.54874.
Train: 2018-08-09T08:09:31.791792: step 11461, loss 0.578309.
Train: 2018-08-09T08:09:34.907074: step 11462, loss 0.596665.
Train: 2018-08-09T08:09:37.985258: step 11463, loss 0.510939.
Train: 2018-08-09T08:09:41.112573: step 11464, loss 0.527681.
Train: 2018-08-09T08:09:44.215823: step 11465, loss 0.50641.
Train: 2018-08-09T08:09:47.290999: step 11466, loss 0.630596.
Train: 2018-08-09T08:09:50.393248: step 11467, loss 0.675983.
Train: 2018-08-09T08:09:53.516552: step 11468, loss 0.50725.
Train: 2018-08-09T08:09:56.616794: step 11469, loss 0.583562.
Train: 2018-08-09T08:09:59.738093: step 11470, loss 0.64901.
Test: 2018-08-09T08:10:20.833179: step 11470, loss 0.548149.
Train: 2018-08-09T08:10:23.930413: step 11471, loss 0.563699.
Train: 2018-08-09T08:10:27.027648: step 11472, loss 0.562515.
Train: 2018-08-09T08:10:30.173011: step 11473, loss 0.529801.
Train: 2018-08-09T08:10:33.249190: step 11474, loss 0.595714.
Train: 2018-08-09T08:10:36.370489: step 11475, loss 0.644503.
Train: 2018-08-09T08:10:39.473739: step 11476, loss 0.643839.
Train: 2018-08-09T08:10:42.566963: step 11477, loss 0.595293.
Train: 2018-08-09T08:10:45.674225: step 11478, loss 0.515906.
Train: 2018-08-09T08:10:48.780483: step 11479, loss 0.514855.
Train: 2018-08-09T08:10:51.858667: step 11480, loss 0.579006.
Test: 2018-08-09T08:11:12.992857: step 11480, loss 0.550398.
Train: 2018-08-09T08:11:16.082070: step 11481, loss 0.62663.
Train: 2018-08-09T08:11:19.181311: step 11482, loss 0.547001.
Train: 2018-08-09T08:11:22.298598: step 11483, loss 0.499498.
Train: 2018-08-09T08:11:25.399844: step 11484, loss 0.578873.
Train: 2018-08-09T08:11:28.499084: step 11485, loss 0.547522.
Train: 2018-08-09T08:11:31.613364: step 11486, loss 0.594608.
Train: 2018-08-09T08:11:34.716615: step 11487, loss 0.46778.
Train: 2018-08-09T08:11:37.835908: step 11488, loss 0.467306.
Train: 2018-08-09T08:11:40.931137: step 11489, loss 0.514386.
Train: 2018-08-09T08:11:44.046420: step 11490, loss 0.496098.
Test: 2018-08-09T08:12:05.199662: step 11490, loss 0.549427.
Train: 2018-08-09T08:12:08.335999: step 11491, loss 0.544267.
Train: 2018-08-09T08:12:11.426215: step 11492, loss 0.508509.
Train: 2018-08-09T08:12:14.524452: step 11493, loss 0.607378.
Train: 2018-08-09T08:12:17.629709: step 11494, loss 0.622726.
Train: 2018-08-09T08:12:20.743989: step 11495, loss 0.581359.
Train: 2018-08-09T08:12:23.852252: step 11496, loss 0.530489.
Train: 2018-08-09T08:12:26.975557: step 11497, loss 0.581156.
Train: 2018-08-09T08:12:30.072791: step 11498, loss 0.581944.
Train: 2018-08-09T08:12:33.146965: step 11499, loss 0.56145.
Train: 2018-08-09T08:12:36.229159: step 11500, loss 0.512626.
Test: 2018-08-09T08:12:57.362347: step 11500, loss 0.548427.
Train: 2018-08-09T08:13:02.212241: step 11501, loss 0.566833.
Train: 2018-08-09T08:13:05.326521: step 11502, loss 0.497958.
Train: 2018-08-09T08:13:08.420748: step 11503, loss 0.579103.
Train: 2018-08-09T08:13:11.523999: step 11504, loss 0.627989.
Train: 2018-08-09T08:13:14.625244: step 11505, loss 0.643775.
Train: 2018-08-09T08:13:17.721476: step 11506, loss 0.611124.
Train: 2018-08-09T08:13:20.833751: step 11507, loss 0.579072.
Train: 2018-08-09T08:13:23.923967: step 11508, loss 0.611507.
Train: 2018-08-09T08:13:27.005159: step 11509, loss 0.578812.
Train: 2018-08-09T08:13:30.114425: step 11510, loss 0.546738.
Test: 2018-08-09T08:13:51.227560: step 11510, loss 0.549016.
Train: 2018-08-09T08:13:54.326799: step 11511, loss 0.594859.
Train: 2018-08-09T08:13:57.395960: step 11512, loss 0.530876.
Train: 2018-08-09T08:14:00.504224: step 11513, loss 0.610803.
Train: 2018-08-09T08:14:03.592434: step 11514, loss 0.594794.
Train: 2018-08-09T08:14:06.715738: step 11515, loss 0.499432.
Train: 2018-08-09T08:14:09.871127: step 11516, loss 0.626511.
Train: 2018-08-09T08:14:13.032533: step 11517, loss 0.531298.
Train: 2018-08-09T08:14:16.143805: step 11518, loss 0.515518.
Train: 2018-08-09T08:14:19.231013: step 11519, loss 0.578874.
Train: 2018-08-09T08:14:22.302178: step 11520, loss 0.642235.
Test: 2018-08-09T08:14:43.422331: step 11520, loss 0.550623.
Train: 2018-08-09T08:14:46.550648: step 11521, loss 0.515612.
Train: 2018-08-09T08:14:49.634848: step 11522, loss 0.563103.
Train: 2018-08-09T08:14:52.745118: step 11523, loss 0.594674.
Train: 2018-08-09T08:14:55.840347: step 11524, loss 0.594654.
Train: 2018-08-09T08:14:58.935576: step 11525, loss 0.531525.
Train: 2018-08-09T08:15:02.051862: step 11526, loss 0.610416.
Train: 2018-08-09T08:15:05.126035: step 11527, loss 0.594627.
Train: 2018-08-09T08:15:08.252347: step 11528, loss 0.563122.
Train: 2018-08-09T08:15:11.361614: step 11529, loss 0.610331.
Train: 2018-08-09T08:15:14.483915: step 11530, loss 0.610278.
Test: 2018-08-09T08:15:35.641166: step 11530, loss 0.54912.
Train: 2018-08-09T08:15:38.755446: step 11531, loss 0.516203.
Train: 2018-08-09T08:15:41.876745: step 11532, loss 0.610189.
Train: 2018-08-09T08:15:44.980998: step 11533, loss 0.531994.
Train: 2018-08-09T08:15:48.106308: step 11534, loss 0.500785.
Train: 2018-08-09T08:15:51.192513: step 11535, loss 0.51638.
Train: 2018-08-09T08:15:54.290750: step 11536, loss 0.594514.
Train: 2018-08-09T08:15:57.392998: step 11537, loss 0.51613.
Train: 2018-08-09T08:16:00.498254: step 11538, loss 0.610308.
Train: 2018-08-09T08:16:03.627575: step 11539, loss 0.515928.
Train: 2018-08-09T08:16:06.727817: step 11540, loss 0.484183.
Test: 2018-08-09T08:16:27.874040: step 11540, loss 0.552307.
Train: 2018-08-09T08:16:30.951220: step 11541, loss 0.642315.
Train: 2018-08-09T08:16:34.054471: step 11542, loss 0.515513.
Train: 2018-08-09T08:16:37.155716: step 11543, loss 0.594965.
Train: 2018-08-09T08:16:40.258968: step 11544, loss 0.546935.
Train: 2018-08-09T08:16:43.358207: step 11545, loss 0.498876.
Train: 2018-08-09T08:16:46.430375: step 11546, loss 0.594762.
Train: 2018-08-09T08:16:49.518586: step 11547, loss 0.578613.
Train: 2018-08-09T08:16:52.609805: step 11548, loss 0.578428.
Train: 2018-08-09T08:16:55.729098: step 11549, loss 0.628511.
Train: 2018-08-09T08:16:58.842375: step 11550, loss 0.627441.
Test: 2018-08-09T08:17:19.972555: step 11550, loss 0.550129.
Train: 2018-08-09T08:17:23.068786: step 11551, loss 0.594875.
Train: 2018-08-09T08:17:26.165019: step 11552, loss 0.594601.
Train: 2018-08-09T08:17:29.276291: step 11553, loss 0.61123.
Train: 2018-08-09T08:17:32.425664: step 11554, loss 0.482605.
Train: 2018-08-09T08:17:35.529917: step 11555, loss 0.498578.
Train: 2018-08-09T08:17:38.642192: step 11556, loss 0.481087.
Train: 2018-08-09T08:17:41.736419: step 11557, loss 0.56038.
Train: 2018-08-09T08:17:44.853707: step 11558, loss 0.508936.
Train: 2018-08-09T08:17:47.956957: step 11559, loss 0.521569.
Train: 2018-08-09T08:17:51.058203: step 11560, loss 0.540013.
Test: 2018-08-09T08:18:12.194398: step 11560, loss 0.54846.
Train: 2018-08-09T08:18:15.282609: step 11561, loss 0.559991.
Train: 2018-08-09T08:18:18.382851: step 11562, loss 0.579494.
Train: 2018-08-09T08:18:21.478081: step 11563, loss 0.578377.
Train: 2018-08-09T08:18:24.554260: step 11564, loss 0.628076.
Train: 2018-08-09T08:18:27.670545: step 11565, loss 0.456941.
Train: 2018-08-09T08:18:30.763769: step 11566, loss 0.644989.
Train: 2018-08-09T08:18:33.860001: step 11567, loss 0.627759.
Train: 2018-08-09T08:18:37.010377: step 11568, loss 0.530977.
Train: 2018-08-09T08:18:40.206875: step 11569, loss 0.563008.
Train: 2018-08-09T08:18:43.301102: step 11570, loss 0.594772.
Test: 2018-08-09T08:19:04.432285: step 11570, loss 0.548888.
Train: 2018-08-09T08:19:07.511471: step 11571, loss 0.595146.
Train: 2018-08-09T08:19:10.629761: step 11572, loss 0.483585.
Train: 2018-08-09T08:19:13.712959: step 11573, loss 0.530695.
Train: 2018-08-09T08:19:16.798162: step 11574, loss 0.570397.
Train: 2018-08-09T08:19:19.879353: step 11575, loss 0.628083.
Train: 2018-08-09T08:19:23.007671: step 11576, loss 0.61737.
Train: 2018-08-09T08:19:26.101898: step 11577, loss 0.586601.
Train: 2018-08-09T08:19:29.208157: step 11578, loss 0.52901.
Train: 2018-08-09T08:19:32.302383: step 11579, loss 0.596948.
Train: 2018-08-09T08:19:35.408642: step 11580, loss 0.56205.
Test: 2018-08-09T08:19:56.528795: step 11580, loss 0.548549.
Train: 2018-08-09T08:19:59.630040: step 11581, loss 0.476905.
Train: 2018-08-09T08:20:02.752341: step 11582, loss 0.516102.
Train: 2018-08-09T08:20:05.862610: step 11583, loss 0.506395.
Train: 2018-08-09T08:20:08.954832: step 11584, loss 0.656896.
Train: 2018-08-09T08:20:12.061090: step 11585, loss 0.663413.
Train: 2018-08-09T08:20:15.160330: step 11586, loss 0.798582.
Train: 2018-08-09T08:20:18.271602: step 11587, loss 0.57226.
Train: 2018-08-09T08:20:21.361819: step 11588, loss 0.518114.
Train: 2018-08-09T08:20:24.495149: step 11589, loss 0.524152.
Train: 2018-08-09T08:20:27.622464: step 11590, loss 0.615736.
Test: 2018-08-09T08:20:48.763673: step 11590, loss 0.548579.
Train: 2018-08-09T08:20:51.840854: step 11591, loss 0.50915.
Train: 2018-08-09T08:20:54.978195: step 11592, loss 0.441086.
Train: 2018-08-09T08:20:58.066406: step 11593, loss 0.679357.
Train: 2018-08-09T08:21:01.179683: step 11594, loss 0.459649.
Train: 2018-08-09T08:21:04.296971: step 11595, loss 0.493479.
Train: 2018-08-09T08:21:07.407241: step 11596, loss 0.426383.
Train: 2018-08-09T08:21:10.530545: step 11597, loss 0.886954.
Train: 2018-08-09T08:21:13.628782: step 11598, loss 0.535564.
Train: 2018-08-09T08:21:16.744065: step 11599, loss 0.527771.
Train: 2018-08-09T08:21:19.819241: step 11600, loss 0.418832.
Test: 2018-08-09T08:21:40.956439: step 11600, loss 0.5631.
Train: 2018-08-09T08:21:45.854461: step 11601, loss 0.63605.
Train: 2018-08-09T08:21:48.945680: step 11602, loss 0.542115.
Train: 2018-08-09T08:21:52.051939: step 11603, loss 0.510548.
Train: 2018-08-09T08:21:55.182261: step 11604, loss 0.545844.
Train: 2018-08-09T08:21:58.275485: step 11605, loss 0.585647.
Train: 2018-08-09T08:22:01.366704: step 11606, loss 0.529049.
Train: 2018-08-09T08:22:04.472962: step 11607, loss 0.566558.
Train: 2018-08-09T08:22:07.591253: step 11608, loss 0.529283.
Train: 2018-08-09T08:22:10.706536: step 11609, loss 0.545914.
Train: 2018-08-09T08:22:13.827834: step 11610, loss 0.57882.
Test: 2018-08-09T08:22:34.934953: step 11610, loss 0.552655.
Train: 2018-08-09T08:22:38.070288: step 11611, loss 0.617887.
Train: 2018-08-09T08:22:41.200611: step 11612, loss 0.549317.
Train: 2018-08-09T08:22:44.285814: step 11613, loss 0.517567.
Train: 2018-08-09T08:22:47.375027: step 11614, loss 0.609448.
Train: 2018-08-09T08:22:50.472262: step 11615, loss 0.545704.
Train: 2018-08-09T08:22:53.571502: step 11616, loss 0.665486.
Train: 2018-08-09T08:22:56.648683: step 11617, loss 0.584134.
Train: 2018-08-09T08:22:59.758953: step 11618, loss 0.552474.
Train: 2018-08-09T08:23:02.851174: step 11619, loss 0.46211.
Train: 2018-08-09T08:23:05.964451: step 11620, loss 0.896672.
Test: 2018-08-09T08:23:27.092626: step 11620, loss 0.56519.
Train: 2018-08-09T08:23:30.156772: step 11621, loss 0.620394.
Train: 2018-08-09T08:23:33.242978: step 11622, loss 0.633628.
Train: 2018-08-09T08:23:36.324169: step 11623, loss 0.547794.
Train: 2018-08-09T08:23:39.435441: step 11624, loss 0.596148.
Train: 2018-08-09T08:23:42.544708: step 11625, loss 0.549384.
Train: 2018-08-09T08:23:45.650967: step 11626, loss 0.579375.
Train: 2018-08-09T08:23:48.765247: step 11627, loss 0.43995.
Train: 2018-08-09T08:23:51.850450: step 11628, loss 0.449883.
Train: 2018-08-09T08:23:54.971748: step 11629, loss 0.514234.
Train: 2018-08-09T08:23:58.080012: step 11630, loss 0.665085.
Test: 2018-08-09T08:24:19.245286: step 11630, loss 0.549384.
Train: 2018-08-09T08:24:22.338509: step 11631, loss 0.645186.
Train: 2018-08-09T08:24:25.423712: step 11632, loss 0.597174.
Train: 2018-08-09T08:24:28.541000: step 11633, loss 0.56845.
Train: 2018-08-09T08:24:31.655280: step 11634, loss 0.494128.
Train: 2018-08-09T08:24:34.768557: step 11635, loss 0.625556.
Train: 2018-08-09T08:24:37.886848: step 11636, loss 0.635756.
Train: 2018-08-09T08:24:41.006141: step 11637, loss 0.688628.
Train: 2018-08-09T08:24:44.088336: step 11638, loss 0.59854.
Train: 2018-08-09T08:24:47.188578: step 11639, loss 0.476164.
Train: 2018-08-09T08:24:50.290826: step 11640, loss 0.740129.
Test: 2018-08-09T08:25:11.453091: step 11640, loss 0.548261.
Train: 2018-08-09T08:25:14.607478: step 11641, loss 0.56233.
Train: 2018-08-09T08:25:17.821021: step 11642, loss 0.528978.
Train: 2018-08-09T08:25:20.926278: step 11643, loss 0.532845.
Train: 2018-08-09T08:25:24.034542: step 11644, loss 0.666657.
Train: 2018-08-09T08:25:27.150827: step 11645, loss 0.590368.
Train: 2018-08-09T08:25:30.266110: step 11646, loss 0.593498.
Train: 2018-08-09T08:25:33.380389: step 11647, loss 0.510785.
Train: 2018-08-09T08:25:36.486648: step 11648, loss 0.498902.
Train: 2018-08-09T08:25:39.623990: step 11649, loss 0.513804.
Train: 2018-08-09T08:25:42.756318: step 11650, loss 0.567059.
Test: 2018-08-09T08:26:03.898529: step 11650, loss 0.550628.
Train: 2018-08-09T08:26:07.024841: step 11651, loss 0.531126.
Train: 2018-08-09T08:26:10.144134: step 11652, loss 0.570731.
Train: 2018-08-09T08:26:13.274457: step 11653, loss 0.593412.
Train: 2018-08-09T08:26:16.388737: step 11654, loss 0.584656.
Train: 2018-08-09T08:26:19.523070: step 11655, loss 0.508951.
Train: 2018-08-09T08:26:22.643366: step 11656, loss 0.545329.
Train: 2018-08-09T08:26:25.780707: step 11657, loss 0.80452.
Train: 2018-08-09T08:26:28.896993: step 11658, loss 0.47596.
Train: 2018-08-09T08:26:31.993225: step 11659, loss 0.637392.
Train: 2018-08-09T08:26:35.104497: step 11660, loss 0.424375.
Test: 2018-08-09T08:26:56.263754: step 11660, loss 0.548545.
Train: 2018-08-09T08:26:59.390066: step 11661, loss 0.381465.
Train: 2018-08-09T08:27:02.509359: step 11662, loss 0.595386.
Train: 2018-08-09T08:27:05.644695: step 11663, loss 0.602406.
Train: 2018-08-09T08:27:08.760980: step 11664, loss 0.54816.
Train: 2018-08-09T08:27:11.893308: step 11665, loss 0.657754.
Train: 2018-08-09T08:27:14.993551: step 11666, loss 0.7722.
Train: 2018-08-09T08:27:18.115852: step 11667, loss 0.652996.
Train: 2018-08-09T08:27:21.221108: step 11668, loss 0.494997.
Train: 2018-08-09T08:27:24.340402: step 11669, loss 0.544321.
Train: 2018-08-09T08:27:27.492783: step 11670, loss 0.526318.
Test: 2018-08-09T08:27:48.728242: step 11670, loss 0.545684.
Train: 2018-08-09T08:27:51.859567: step 11671, loss 0.593644.
Train: 2018-08-09T08:27:54.994903: step 11672, loss 0.705727.
Train: 2018-08-09T08:27:58.140266: step 11673, loss 0.546094.
Train: 2018-08-09T08:28:01.252541: step 11674, loss 0.543279.
Train: 2018-08-09T08:28:04.343759: step 11675, loss 0.716121.
Train: 2018-08-09T08:28:07.491127: step 11676, loss 0.503554.
Train: 2018-08-09T08:28:10.603402: step 11677, loss 0.578234.
Train: 2018-08-09T08:28:13.723698: step 11678, loss 0.50731.
Train: 2018-08-09T08:28:16.813914: step 11679, loss 0.556183.
Train: 2018-08-09T08:28:19.931202: step 11680, loss 0.594801.
Test: 2018-08-09T08:28:41.121542: step 11680, loss 0.549987.
Train: 2018-08-09T08:28:44.183683: step 11681, loss 0.577735.
Train: 2018-08-09T08:28:47.288939: step 11682, loss 0.548088.
Train: 2018-08-09T08:28:50.402216: step 11683, loss 0.703591.
Train: 2018-08-09T08:28:53.548582: step 11684, loss 0.536917.
Train: 2018-08-09T08:28:56.657848: step 11685, loss 0.472702.
Train: 2018-08-09T08:28:59.756086: step 11686, loss 0.561906.
Train: 2018-08-09T08:29:02.874376: step 11687, loss 0.544986.
Train: 2018-08-09T08:29:06.009712: step 11688, loss 0.507611.
Train: 2018-08-09T08:29:09.141037: step 11689, loss 0.649872.
Train: 2018-08-09T08:29:12.235264: step 11690, loss 0.61266.
Test: 2018-08-09T08:29:33.385498: step 11690, loss 0.549179.
Train: 2018-08-09T08:29:36.491756: step 11691, loss 0.563095.
Train: 2018-08-09T08:29:39.650153: step 11692, loss 0.683416.
Train: 2018-08-09T08:29:42.802534: step 11693, loss 0.632752.
Train: 2018-08-09T08:29:45.908793: step 11694, loss 0.533321.
Train: 2018-08-09T08:29:49.018059: step 11695, loss 0.666398.
Train: 2018-08-09T08:29:52.121310: step 11696, loss 0.57907.
Train: 2018-08-09T08:29:55.238598: step 11697, loss 0.521299.
Train: 2018-08-09T08:29:58.358894: step 11698, loss 0.679757.
Train: 2018-08-09T08:30:01.458134: step 11699, loss 0.565125.
Train: 2018-08-09T08:30:04.560382: step 11700, loss 0.593279.
Test: 2018-08-09T08:30:25.745708: step 11700, loss 0.553201.
Train: 2018-08-09T08:30:30.636712: step 11701, loss 0.531844.
Train: 2018-08-09T08:30:33.737957: step 11702, loss 0.598246.
Train: 2018-08-09T08:30:36.877304: step 11703, loss 0.529931.
Train: 2018-08-09T08:30:40.015648: step 11704, loss 0.627499.
Train: 2018-08-09T08:30:43.115891: step 11705, loss 0.59291.
Train: 2018-08-09T08:30:46.242203: step 11706, loss 0.549297.
Train: 2018-08-09T08:30:49.364504: step 11707, loss 0.730552.
Train: 2018-08-09T08:30:52.464746: step 11708, loss 0.623564.
Train: 2018-08-09T08:30:55.586045: step 11709, loss 0.563382.
Train: 2018-08-09T08:30:58.673253: step 11710, loss 0.564811.
Test: 2018-08-09T08:31:19.813460: step 11710, loss 0.552054.
Train: 2018-08-09T08:31:22.924731: step 11711, loss 0.535787.
Train: 2018-08-09T08:31:26.046030: step 11712, loss 0.518706.
Train: 2018-08-09T08:31:29.156299: step 11713, loss 0.53135.
Train: 2018-08-09T08:31:32.262558: step 11714, loss 0.468677.
Train: 2018-08-09T08:31:35.377841: step 11715, loss 0.647477.
Train: 2018-08-09T08:31:38.482094: step 11716, loss 0.55525.
Train: 2018-08-09T08:31:41.647510: step 11717, loss 0.597078.
Train: 2018-08-09T08:31:44.771817: step 11718, loss 0.51944.
Train: 2018-08-09T08:31:47.878075: step 11719, loss 0.504247.
Train: 2018-08-09T08:31:51.022435: step 11720, loss 0.502774.
Test: 2018-08-09T08:32:12.168658: step 11720, loss 0.550992.
Train: 2018-08-09T08:32:15.296975: step 11721, loss 0.579281.
Train: 2018-08-09T08:32:18.415265: step 11722, loss 0.533209.
Train: 2018-08-09T08:32:21.527540: step 11723, loss 0.517339.
Train: 2018-08-09T08:32:24.670897: step 11724, loss 0.532544.
Train: 2018-08-09T08:32:27.806233: step 11725, loss 0.612293.
Train: 2018-08-09T08:32:30.940567: step 11726, loss 0.595556.
Train: 2018-08-09T08:32:34.063871: step 11727, loss 0.594655.
Train: 2018-08-09T08:32:37.197201: step 11728, loss 0.625686.
Train: 2018-08-09T08:32:40.314489: step 11729, loss 0.578618.
Train: 2018-08-09T08:32:43.435788: step 11730, loss 0.608791.
Test: 2018-08-09T08:33:04.607077: step 11730, loss 0.549494.
Train: 2018-08-09T08:33:07.768482: step 11731, loss 0.486591.
Train: 2018-08-09T08:33:10.893791: step 11732, loss 0.57934.
Train: 2018-08-09T08:33:14.012082: step 11733, loss 0.65748.
Train: 2018-08-09T08:33:17.114330: step 11734, loss 0.56358.
Train: 2018-08-09T08:33:20.230615: step 11735, loss 0.624756.
Train: 2018-08-09T08:33:23.338879: step 11736, loss 0.517071.
Train: 2018-08-09T08:33:26.472210: step 11737, loss 0.598035.
Train: 2018-08-09T08:33:29.639631: step 11738, loss 0.608391.
Train: 2018-08-09T08:33:32.591479: step 11739, loss 0.535217.
Train: 2018-08-09T08:33:35.708768: step 11740, loss 0.625721.
Test: 2018-08-09T08:33:56.913144: step 11740, loss 0.549905.
Train: 2018-08-09T08:34:00.022411: step 11741, loss 0.547391.
Train: 2018-08-09T08:34:03.161757: step 11742, loss 0.516752.
Train: 2018-08-09T08:34:06.286064: step 11743, loss 0.611662.
Train: 2018-08-09T08:34:09.435437: step 11744, loss 0.49956.
Train: 2018-08-09T08:34:12.529664: step 11745, loss 0.611353.
Train: 2018-08-09T08:34:15.629907: step 11746, loss 0.640844.
Train: 2018-08-09T08:34:18.728144: step 11747, loss 0.640383.
Train: 2018-08-09T08:34:21.845432: step 11748, loss 0.608717.
Train: 2018-08-09T08:34:24.948683: step 11749, loss 0.517578.
Train: 2018-08-09T08:34:28.093043: step 11750, loss 0.5468.
Test: 2018-08-09T08:34:49.282380: step 11750, loss 0.549916.
Train: 2018-08-09T08:34:52.402675: step 11751, loss 0.550455.
Train: 2018-08-09T08:34:55.530992: step 11752, loss 0.476937.
Train: 2018-08-09T08:34:58.640259: step 11753, loss 0.614578.
Train: 2018-08-09T08:35:01.774593: step 11754, loss 0.703674.
Train: 2018-08-09T08:35:04.857790: step 11755, loss 0.53242.
Train: 2018-08-09T08:35:07.996134: step 11756, loss 0.532309.
Train: 2018-08-09T08:35:11.139491: step 11757, loss 0.57822.
Train: 2018-08-09T08:35:14.252769: step 11758, loss 0.502051.
Train: 2018-08-09T08:35:17.385097: step 11759, loss 0.562505.
Train: 2018-08-09T08:35:20.486342: step 11760, loss 0.564476.
Test: 2018-08-09T08:35:41.644597: step 11760, loss 0.551374.
Train: 2018-08-09T08:35:44.804999: step 11761, loss 0.61008.
Train: 2018-08-09T08:35:47.927300: step 11762, loss 0.50171.
Train: 2018-08-09T08:35:51.016513: step 11763, loss 0.51641.
Train: 2018-08-09T08:35:54.133801: step 11764, loss 0.564689.
Train: 2018-08-09T08:35:57.261116: step 11765, loss 0.686882.
Train: 2018-08-09T08:36:00.358351: step 11766, loss 0.549258.
Train: 2018-08-09T08:36:03.493686: step 11767, loss 0.594353.
Train: 2018-08-09T08:36:06.619999: step 11768, loss 0.56393.
Train: 2018-08-09T08:36:09.706204: step 11769, loss 0.611638.
Train: 2018-08-09T08:36:12.814468: step 11770, loss 0.54813.
Test: 2018-08-09T08:36:33.992775: step 11770, loss 0.552237.
Train: 2018-08-09T08:36:37.096026: step 11771, loss 0.605819.
Train: 2018-08-09T08:36:40.222338: step 11772, loss 0.5786.
Train: 2018-08-09T08:36:43.347647: step 11773, loss 0.513035.
Train: 2018-08-09T08:36:46.466941: step 11774, loss 0.613163.
Train: 2018-08-09T08:36:49.597263: step 11775, loss 0.548519.
Train: 2018-08-09T08:36:52.718562: step 11776, loss 0.595539.
Train: 2018-08-09T08:36:55.831839: step 11777, loss 0.562507.
Train: 2018-08-09T08:36:58.988231: step 11778, loss 0.660106.
Train: 2018-08-09T08:37:02.086469: step 11779, loss 0.609951.
Train: 2018-08-09T08:37:05.222807: step 11780, loss 0.532079.
Test: 2018-08-09T08:37:26.385072: step 11780, loss 0.549275.
Train: 2018-08-09T08:37:29.532440: step 11781, loss 0.594195.
Train: 2018-08-09T08:37:32.650731: step 11782, loss 0.703637.
Train: 2018-08-09T08:37:35.847229: step 11783, loss 0.60969.
Train: 2018-08-09T08:37:38.971536: step 11784, loss 0.563423.
Train: 2018-08-09T08:37:42.113890: step 11785, loss 0.517486.
Train: 2018-08-09T08:37:45.221152: step 11786, loss 0.502096.
Train: 2018-08-09T08:37:48.338440: step 11787, loss 0.563371.
Train: 2018-08-09T08:37:51.465754: step 11788, loss 0.440522.
Train: 2018-08-09T08:37:54.584045: step 11789, loss 0.517281.
Train: 2018-08-09T08:37:57.700330: step 11790, loss 0.516933.
Test: 2018-08-09T08:38:18.880644: step 11790, loss 0.549288.
Train: 2018-08-09T08:38:22.049067: step 11791, loss 0.547736.
Train: 2018-08-09T08:38:25.195432: step 11792, loss 0.500403.
Train: 2018-08-09T08:38:28.324752: step 11793, loss 0.531541.
Train: 2018-08-09T08:38:31.451064: step 11794, loss 0.62658.
Train: 2018-08-09T08:38:34.572363: step 11795, loss 0.562221.
Train: 2018-08-09T08:38:37.694664: step 11796, loss 0.610409.
Train: 2018-08-09T08:38:40.850053: step 11797, loss 0.546686.
Train: 2018-08-09T08:38:43.975363: step 11798, loss 0.579134.
Train: 2018-08-09T08:38:47.091648: step 11799, loss 0.579374.
Train: 2018-08-09T08:38:50.215955: step 11800, loss 0.498394.
Test: 2018-08-09T08:39:11.398274: step 11800, loss 0.550531.
Train: 2018-08-09T08:39:16.319357: step 11801, loss 0.578205.
Train: 2018-08-09T08:39:19.416591: step 11802, loss 0.595738.
Train: 2018-08-09T08:39:22.551927: step 11803, loss 0.496975.
Train: 2018-08-09T08:39:25.642143: step 11804, loss 0.463961.
Train: 2018-08-09T08:39:28.764445: step 11805, loss 0.496642.
Train: 2018-08-09T08:39:31.904794: step 11806, loss 0.578486.
Train: 2018-08-09T08:39:35.056173: step 11807, loss 0.511859.
Train: 2018-08-09T08:39:38.156415: step 11808, loss 0.562538.
Train: 2018-08-09T08:39:41.273703: step 11809, loss 0.647237.
Train: 2018-08-09T08:39:44.389989: step 11810, loss 0.64598.
Test: 2018-08-09T08:40:05.513150: step 11810, loss 0.547799.
Train: 2018-08-09T08:40:08.659515: step 11811, loss 0.44323.
Train: 2018-08-09T08:40:11.770787: step 11812, loss 0.580545.
Train: 2018-08-09T08:40:14.887072: step 11813, loss 0.562427.
Train: 2018-08-09T08:40:18.011379: step 11814, loss 0.493791.
Train: 2018-08-09T08:40:21.117637: step 11815, loss 0.492703.
Train: 2018-08-09T08:40:24.239939: step 11816, loss 0.649367.
Train: 2018-08-09T08:40:27.417387: step 11817, loss 0.529257.
Train: 2018-08-09T08:40:30.550717: step 11818, loss 0.614545.
Train: 2018-08-09T08:40:33.659984: step 11819, loss 0.597364.
Train: 2018-08-09T08:40:36.788301: step 11820, loss 0.596075.
Test: 2018-08-09T08:40:57.904444: step 11820, loss 0.548295.
Train: 2018-08-09T08:41:01.046798: step 11821, loss 0.527297.
Train: 2018-08-09T08:41:04.165088: step 11822, loss 0.595888.
Train: 2018-08-09T08:41:07.288393: step 11823, loss 0.494527.
Train: 2018-08-09T08:41:10.419718: step 11824, loss 0.544729.
Train: 2018-08-09T08:41:13.563075: step 11825, loss 0.44466.
Train: 2018-08-09T08:41:16.643265: step 11826, loss 0.578851.
Train: 2018-08-09T08:41:19.755539: step 11827, loss 0.562796.
Train: 2018-08-09T08:41:22.882854: step 11828, loss 0.631326.
Train: 2018-08-09T08:41:26.021198: step 11829, loss 0.561828.
Train: 2018-08-09T08:41:29.169569: step 11830, loss 0.631425.
Test: 2018-08-09T08:41:50.335844: step 11830, loss 0.547176.
Train: 2018-08-09T08:41:53.423052: step 11831, loss 0.460201.
Train: 2018-08-09T08:41:56.554377: step 11832, loss 0.544695.
Train: 2018-08-09T08:41:59.704753: step 11833, loss 0.510778.
Train: 2018-08-09T08:42:02.832068: step 11834, loss 0.630138.
Train: 2018-08-09T08:42:05.978433: step 11835, loss 0.562182.
Train: 2018-08-09T08:42:09.109758: step 11836, loss 0.562387.
Train: 2018-08-09T08:42:12.248102: step 11837, loss 0.495544.
Train: 2018-08-09T08:42:15.356367: step 11838, loss 0.578672.
Train: 2018-08-09T08:42:18.486689: step 11839, loss 0.494019.
Train: 2018-08-09T08:42:21.600969: step 11840, loss 0.527435.
Test: 2018-08-09T08:42:42.750199: step 11840, loss 0.549639.
Train: 2018-08-09T08:42:45.860468: step 11841, loss 0.578136.
Train: 2018-08-09T08:42:49.004829: step 11842, loss 0.597725.
Train: 2018-08-09T08:42:52.140164: step 11843, loss 0.649934.
Train: 2018-08-09T08:42:55.277506: step 11844, loss 0.648346.
Train: 2018-08-09T08:42:58.390783: step 11845, loss 0.511499.
Train: 2018-08-09T08:43:01.502055: step 11846, loss 0.494669.
Train: 2018-08-09T08:43:04.646415: step 11847, loss 0.545805.
Train: 2018-08-09T08:43:07.786764: step 11848, loss 0.545666.
Train: 2018-08-09T08:43:10.920095: step 11849, loss 0.645639.
Train: 2018-08-09T08:43:14.029362: step 11850, loss 0.529215.
Test: 2018-08-09T08:43:35.196640: step 11850, loss 0.549964.
Train: 2018-08-09T08:43:38.367069: step 11851, loss 0.56128.
Train: 2018-08-09T08:43:41.508421: step 11852, loss 0.528258.
Train: 2018-08-09T08:43:44.640749: step 11853, loss 0.62821.
Train: 2018-08-09T08:43:47.746005: step 11854, loss 0.54469.
Train: 2018-08-09T08:43:50.882344: step 11855, loss 0.562399.
Train: 2018-08-09T08:43:54.026704: step 11856, loss 0.562863.
Train: 2018-08-09T08:43:57.118925: step 11857, loss 0.595781.
Train: 2018-08-09T08:44:00.252256: step 11858, loss 0.545684.
Train: 2018-08-09T08:44:03.355506: step 11859, loss 0.595433.
Train: 2018-08-09T08:44:06.487834: step 11860, loss 0.463411.
Test: 2018-08-09T08:44:27.685193: step 11860, loss 0.548265.
Train: 2018-08-09T08:44:30.797467: step 11861, loss 0.512649.
Train: 2018-08-09T08:44:33.941827: step 11862, loss 0.679763.
Train: 2018-08-09T08:44:37.046080: step 11863, loss 0.561609.
Train: 2018-08-09T08:44:40.189437: step 11864, loss 0.563432.
Train: 2018-08-09T08:44:43.322768: step 11865, loss 0.448223.
Train: 2018-08-09T08:44:46.447075: step 11866, loss 0.578302.
Train: 2018-08-09T08:44:49.613494: step 11867, loss 0.461615.
Train: 2018-08-09T08:44:52.729779: step 11868, loss 0.579615.
Train: 2018-08-09T08:44:55.843056: step 11869, loss 0.529076.
Train: 2018-08-09T08:44:58.967363: step 11870, loss 0.612449.
Test: 2018-08-09T08:45:20.154695: step 11870, loss 0.547995.
Train: 2018-08-09T08:45:23.290030: step 11871, loss 0.596261.
Train: 2018-08-09T08:45:26.447425: step 11872, loss 0.677925.
Train: 2018-08-09T08:45:29.593790: step 11873, loss 0.631835.
Train: 2018-08-09T08:45:32.715089: step 11874, loss 0.512243.
Train: 2018-08-09T08:45:35.836387: step 11875, loss 0.480624.
Train: 2018-08-09T08:45:38.987766: step 11876, loss 0.529165.
Train: 2018-08-09T08:45:42.129118: step 11877, loss 0.613425.
Train: 2018-08-09T08:45:45.260443: step 11878, loss 0.628723.
Train: 2018-08-09T08:45:48.364697: step 11879, loss 0.593121.
Train: 2018-08-09T08:45:51.493014: step 11880, loss 0.511408.
Test: 2018-08-09T08:46:12.666309: step 11880, loss 0.54725.
Train: 2018-08-09T08:46:15.805655: step 11881, loss 0.562929.
Train: 2018-08-09T08:46:18.915924: step 11882, loss 0.580389.
Train: 2018-08-09T08:46:22.047249: step 11883, loss 0.62765.
Train: 2018-08-09T08:46:25.197625: step 11884, loss 0.547184.
Train: 2018-08-09T08:46:28.331959: step 11885, loss 0.499498.
Train: 2018-08-09T08:46:31.422175: step 11886, loss 0.512827.
Train: 2018-08-09T08:46:34.541468: step 11887, loss 0.513097.
Train: 2018-08-09T08:46:37.667780: step 11888, loss 0.562313.
Train: 2018-08-09T08:46:40.789079: step 11889, loss 0.546198.
Train: 2018-08-09T08:46:43.917396: step 11890, loss 0.529391.
Test: 2018-08-09T08:47:05.076653: step 11890, loss 0.548482.
Train: 2018-08-09T08:47:08.184917: step 11891, loss 0.644637.
Train: 2018-08-09T08:47:11.293181: step 11892, loss 0.675824.
Train: 2018-08-09T08:47:14.415482: step 11893, loss 0.514216.
Train: 2018-08-09T08:47:17.544802: step 11894, loss 0.498765.
Train: 2018-08-09T08:47:20.693173: step 11895, loss 0.514262.
Train: 2018-08-09T08:47:23.842546: step 11896, loss 0.579556.
Train: 2018-08-09T08:47:26.989914: step 11897, loss 0.562976.
Train: 2018-08-09T08:47:30.130263: step 11898, loss 0.563036.
Train: 2018-08-09T08:47:33.247551: step 11899, loss 0.563768.
Train: 2018-08-09T08:47:36.395922: step 11900, loss 0.596988.
Test: 2018-08-09T08:47:57.676502: step 11900, loss 0.549742.
Train: 2018-08-09T08:48:02.756006: step 11901, loss 0.627674.
Train: 2018-08-09T08:48:05.889337: step 11902, loss 0.514026.
Train: 2018-08-09T08:48:08.996598: step 11903, loss 0.611043.
Train: 2018-08-09T08:48:12.093833: step 11904, loss 0.449348.
Train: 2018-08-09T08:48:15.174022: step 11905, loss 0.626078.
Train: 2018-08-09T08:48:18.276270: step 11906, loss 0.629149.
Train: 2018-08-09T08:48:21.383531: step 11907, loss 0.579213.
Train: 2018-08-09T08:48:24.514857: step 11908, loss 0.547031.
Train: 2018-08-09T08:48:27.640166: step 11909, loss 0.433887.
Train: 2018-08-09T08:48:30.781518: step 11910, loss 0.660249.
Test: 2018-08-09T08:48:52.020989: step 11910, loss 0.549464.
Train: 2018-08-09T08:48:55.154319: step 11911, loss 0.595169.
Train: 2018-08-09T08:48:58.292663: step 11912, loss 0.498462.
Train: 2018-08-09T08:49:01.440031: step 11913, loss 0.643867.
Train: 2018-08-09T08:49:04.573361: step 11914, loss 0.482605.
Train: 2018-08-09T08:49:07.733764: step 11915, loss 0.594703.
Train: 2018-08-09T08:49:10.904193: step 11916, loss 0.48264.
Train: 2018-08-09T08:49:14.064596: step 11917, loss 0.546745.
Train: 2018-08-09T08:49:17.202940: step 11918, loss 0.530865.
Train: 2018-08-09T08:49:20.351311: step 11919, loss 0.546388.
Train: 2018-08-09T08:49:23.477623: step 11920, loss 0.514283.
Test: 2018-08-09T08:49:44.636880: step 11920, loss 0.548642.
Train: 2018-08-09T08:49:47.767202: step 11921, loss 0.480976.
Train: 2018-08-09T08:49:50.891509: step 11922, loss 0.529905.
Train: 2018-08-09T08:49:54.032860: step 11923, loss 0.511662.
Train: 2018-08-09T08:49:57.190255: step 11924, loss 0.680749.
Train: 2018-08-09T08:50:00.320578: step 11925, loss 0.595984.
Train: 2018-08-09T08:50:03.417812: step 11926, loss 0.661892.
Train: 2018-08-09T08:50:06.561170: step 11927, loss 0.578893.
Train: 2018-08-09T08:50:09.698511: step 11928, loss 0.529886.
Train: 2018-08-09T08:50:12.809783: step 11929, loss 0.562273.
Train: 2018-08-09T08:50:15.931082: step 11930, loss 0.529904.
Test: 2018-08-09T08:50:37.148494: step 11930, loss 0.547639.
Train: 2018-08-09T08:50:40.236704: step 11931, loss 0.579463.
Train: 2018-08-09T08:50:43.318898: step 11932, loss 0.513498.
Train: 2018-08-09T08:50:46.437189: step 11933, loss 0.595365.
Train: 2018-08-09T08:50:49.530413: step 11934, loss 0.661579.
Train: 2018-08-09T08:50:52.648704: step 11935, loss 0.530077.
Train: 2018-08-09T08:50:55.765992: step 11936, loss 0.645105.
Train: 2018-08-09T08:50:58.901328: step 11937, loss 0.480481.
Train: 2018-08-09T08:51:02.040675: step 11938, loss 0.676626.
Train: 2018-08-09T08:51:05.153952: step 11939, loss 0.530188.
Train: 2018-08-09T08:51:08.285277: step 11940, loss 0.529978.
Test: 2018-08-09T08:51:29.448545: step 11940, loss 0.546249.
Train: 2018-08-09T08:51:32.599923: step 11941, loss 0.579026.
Train: 2018-08-09T08:51:35.729243: step 11942, loss 0.497825.
Train: 2018-08-09T08:51:38.859566: step 11943, loss 0.692204.
Train: 2018-08-09T08:51:42.012950: step 11944, loss 0.59494.
Train: 2018-08-09T08:51:45.147283: step 11945, loss 0.530218.
Train: 2018-08-09T08:51:48.271590: step 11946, loss 0.482051.
Train: 2018-08-09T08:51:51.402915: step 11947, loss 0.692001.
Train: 2018-08-09T08:51:54.529227: step 11948, loss 0.56296.
Train: 2018-08-09T08:51:57.657544: step 11949, loss 0.578927.
Train: 2018-08-09T08:52:00.753776: step 11950, loss 0.482929.
Test: 2018-08-09T08:52:21.906015: step 11950, loss 0.552115.
Train: 2018-08-09T08:52:25.009265: step 11951, loss 0.611302.
Train: 2018-08-09T08:52:28.119534: step 11952, loss 0.499167.
Train: 2018-08-09T08:52:31.257878: step 11953, loss 0.562934.
Train: 2018-08-09T08:52:34.383188: step 11954, loss 0.53124.
Train: 2018-08-09T08:52:37.512507: step 11955, loss 0.562982.
Train: 2018-08-09T08:52:40.642830: step 11956, loss 0.562567.
Train: 2018-08-09T08:52:43.774156: step 11957, loss 0.546684.
Train: 2018-08-09T08:52:46.879412: step 11958, loss 0.562682.
Train: 2018-08-09T08:52:49.981660: step 11959, loss 0.498513.
Train: 2018-08-09T08:52:53.078895: step 11960, loss 0.578693.
Test: 2018-08-09T08:53:14.219101: step 11960, loss 0.550703.
Train: 2018-08-09T08:53:17.308314: step 11961, loss 0.610786.
Train: 2018-08-09T08:53:20.440642: step 11962, loss 0.57901.
Train: 2018-08-09T08:53:23.591018: step 11963, loss 0.547029.
Train: 2018-08-09T08:53:26.708306: step 11964, loss 0.5463.
Train: 2018-08-09T08:53:29.846650: step 11965, loss 0.498354.
Train: 2018-08-09T08:53:32.966946: step 11966, loss 0.546628.
Train: 2018-08-09T08:53:36.101279: step 11967, loss 0.529958.
Train: 2018-08-09T08:53:39.215559: step 11968, loss 0.627998.
Train: 2018-08-09T08:53:42.314799: step 11969, loss 0.578146.
Train: 2018-08-09T08:53:45.405015: step 11970, loss 0.578939.
Test: 2018-08-09T08:54:06.547227: step 11970, loss 0.549133.
Train: 2018-08-09T08:54:09.715650: step 11971, loss 0.481759.
Train: 2018-08-09T08:54:12.843968: step 11972, loss 0.578486.
Train: 2018-08-09T08:54:15.965266: step 11973, loss 0.513983.
Train: 2018-08-09T08:54:19.075536: step 11974, loss 0.562838.
Train: 2018-08-09T08:54:22.155725: step 11975, loss 0.596023.
Train: 2018-08-09T08:54:25.257973: step 11976, loss 0.513602.
Train: 2018-08-09T08:54:28.361224: step 11977, loss 0.579334.
Train: 2018-08-09T08:54:31.496560: step 11978, loss 0.562655.
Train: 2018-08-09T08:54:34.602818: step 11979, loss 0.496803.
Train: 2018-08-09T08:54:37.714091: step 11980, loss 0.562868.
Test: 2018-08-09T08:54:58.862318: step 11980, loss 0.545717.
Train: 2018-08-09T08:55:01.995648: step 11981, loss 0.529319.
Train: 2018-08-09T08:55:05.089875: step 11982, loss 0.562233.
Train: 2018-08-09T08:55:08.193126: step 11983, loss 0.628562.
Train: 2018-08-09T08:55:11.339491: step 11984, loss 0.579109.
Train: 2018-08-09T08:55:14.466806: step 11985, loss 0.612765.
Train: 2018-08-09T08:55:17.596126: step 11986, loss 0.529803.
Train: 2018-08-09T08:55:20.723440: step 11987, loss 0.562664.
Train: 2018-08-09T08:55:23.838723: step 11988, loss 0.562236.
Train: 2018-08-09T08:55:26.935958: step 11989, loss 0.545975.
Train: 2018-08-09T08:55:30.027176: step 11990, loss 0.512781.
Test: 2018-08-09T08:55:51.206486: step 11990, loss 0.548233.
Train: 2018-08-09T08:55:54.305726: step 11991, loss 0.562212.
Train: 2018-08-09T08:55:57.470140: step 11992, loss 0.529033.
Train: 2018-08-09T08:56:00.600462: step 11993, loss 0.595513.
Train: 2018-08-09T08:56:03.718753: step 11994, loss 0.628652.
Train: 2018-08-09T08:56:06.836041: step 11995, loss 0.562311.
Train: 2018-08-09T08:56:09.968369: step 11996, loss 0.545956.
Train: 2018-08-09T08:56:13.062596: step 11997, loss 0.612138.
Train: 2018-08-09T08:56:16.178881: step 11998, loss 0.628758.
Train: 2018-08-09T08:56:19.303188: step 11999, loss 0.546293.
Train: 2018-08-09T08:56:22.404433: step 12000, loss 0.513068.
Test: 2018-08-09T08:56:43.572714: step 12000, loss 0.547744.
Train: 2018-08-09T08:56:48.557968: step 12001, loss 0.546053.
Train: 2018-08-09T08:56:51.664227: step 12002, loss 0.529592.
Train: 2018-08-09T08:56:54.761462: step 12003, loss 0.611881.
Train: 2018-08-09T08:56:57.881757: step 12004, loss 0.562988.
Train: 2018-08-09T08:57:01.022107: step 12005, loss 0.661009.
Train: 2018-08-09T08:57:04.116333: step 12006, loss 0.546204.
Train: 2018-08-09T08:57:07.223595: step 12007, loss 0.497371.
Train: 2018-08-09T08:57:10.325843: step 12008, loss 0.595166.
Train: 2018-08-09T08:57:13.420070: step 12009, loss 0.481006.
Train: 2018-08-09T08:57:16.539363: step 12010, loss 0.415514.
Test: 2018-08-09T08:57:37.713660: step 12010, loss 0.550339.
Train: 2018-08-09T08:57:40.826937: step 12011, loss 0.562526.
Train: 2018-08-09T08:57:43.941217: step 12012, loss 0.710304.
Train: 2018-08-09T08:57:47.054494: step 12013, loss 0.5459.
Train: 2018-08-09T08:57:50.149724: step 12014, loss 0.546429.
Train: 2018-08-09T08:57:53.230916: step 12015, loss 0.529787.
Train: 2018-08-09T08:57:56.370262: step 12016, loss 0.546204.
Train: 2018-08-09T08:57:59.514622: step 12017, loss 0.644738.
Train: 2018-08-09T08:58:02.604838: step 12018, loss 0.513403.
Train: 2018-08-09T08:58:05.675001: step 12019, loss 0.562482.
Train: 2018-08-09T08:58:08.824374: step 12020, loss 0.578944.
Test: 2018-08-09T08:58:29.981626: step 12020, loss 0.549614.
Train: 2018-08-09T08:58:33.061815: step 12021, loss 0.661362.
Train: 2018-08-09T08:58:36.166068: step 12022, loss 0.579141.
Train: 2018-08-09T08:58:39.288369: step 12023, loss 0.562838.
Train: 2018-08-09T08:58:42.369562: step 12024, loss 0.529642.
Train: 2018-08-09T08:58:45.470807: step 12025, loss 0.546278.
Train: 2018-08-09T08:58:48.571050: step 12026, loss 0.49711.
Train: 2018-08-09T08:58:51.681319: step 12027, loss 0.562628.
Train: 2018-08-09T08:58:54.740452: step 12028, loss 0.513532.
Train: 2018-08-09T08:58:57.851724: step 12029, loss 0.57898.
Train: 2018-08-09T08:59:00.960991: step 12030, loss 0.529895.
Test: 2018-08-09T08:59:22.124259: step 12030, loss 0.552161.
Train: 2018-08-09T08:59:25.222496: step 12031, loss 0.529962.
Train: 2018-08-09T08:59:28.356829: step 12032, loss 0.579103.
Train: 2018-08-09T08:59:31.475120: step 12033, loss 0.48041.
Train: 2018-08-09T08:59:34.574360: step 12034, loss 0.611705.
Train: 2018-08-09T08:59:37.670592: step 12035, loss 0.529508.
Train: 2018-08-09T08:59:40.760808: step 12036, loss 0.612038.
Train: 2018-08-09T08:59:43.855035: step 12037, loss 0.56244.
Train: 2018-08-09T08:59:46.939235: step 12038, loss 0.562487.
Train: 2018-08-09T08:59:50.042485: step 12039, loss 0.57915.
Train: 2018-08-09T08:59:52.953224: step 12040, loss 0.633075.
Test: 2018-08-09T09:00:14.097441: step 12040, loss 0.548873.
Train: 2018-08-09T09:00:17.188659: step 12041, loss 0.595501.
Train: 2018-08-09T09:00:20.289905: step 12042, loss 0.595359.
Train: 2018-08-09T09:00:23.397166: step 12043, loss 0.529782.
Train: 2018-08-09T09:00:26.496406: step 12044, loss 0.595524.
Train: 2018-08-09T09:00:29.571582: step 12045, loss 0.562486.
Train: 2018-08-09T09:00:32.710936: step 12046, loss 0.595173.
Train: 2018-08-09T09:00:35.830222: step 12047, loss 0.529744.
Train: 2018-08-09T09:00:38.915425: step 12048, loss 0.59506.
Train: 2018-08-09T09:00:42.000628: step 12049, loss 0.480901.
Train: 2018-08-09T09:00:45.070790: step 12050, loss 0.513613.
Test: 2018-08-09T09:01:06.212000: step 12050, loss 0.548534.
Train: 2018-08-09T09:01:09.289180: step 12051, loss 0.530206.
Train: 2018-08-09T09:01:12.363354: step 12052, loss 0.545999.
Train: 2018-08-09T09:01:15.447554: step 12053, loss 0.480562.
Train: 2018-08-09T09:01:18.552810: step 12054, loss 0.480286.
Train: 2018-08-09T09:01:21.634002: step 12055, loss 0.512634.
Train: 2018-08-09T09:01:24.703162: step 12056, loss 0.529636.
Train: 2018-08-09T09:01:27.833484: step 12057, loss 0.595689.
Train: 2018-08-09T09:01:30.947764: step 12058, loss 0.57913.
Train: 2018-08-09T09:01:34.029959: step 12059, loss 0.645673.
Train: 2018-08-09T09:01:37.097114: step 12060, loss 0.49599.
Test: 2018-08-09T09:01:58.248350: step 12060, loss 0.547945.
Train: 2018-08-09T09:02:01.377669: step 12061, loss 0.646474.
Train: 2018-08-09T09:02:04.450840: step 12062, loss 0.562647.
Train: 2018-08-09T09:02:07.547072: step 12063, loss 0.596184.
Train: 2018-08-09T09:02:10.645309: step 12064, loss 0.629125.
Train: 2018-08-09T09:02:13.762597: step 12065, loss 0.54579.
Train: 2018-08-09T09:02:16.876877: step 12066, loss 0.579247.
Train: 2018-08-09T09:02:19.957066: step 12067, loss 0.563093.
Train: 2018-08-09T09:02:23.086387: step 12068, loss 0.628872.
Train: 2018-08-09T09:02:26.203675: step 12069, loss 0.611996.
Train: 2018-08-09T09:02:29.277848: step 12070, loss 0.611775.
Test: 2018-08-09T09:02:50.446129: step 12070, loss 0.549578.
Train: 2018-08-09T09:02:53.538350: step 12071, loss 0.545892.
Train: 2018-08-09T09:02:56.754902: step 12072, loss 0.562344.
Train: 2018-08-09T09:02:59.920318: step 12073, loss 0.595353.
Train: 2018-08-09T09:03:03.013542: step 12074, loss 0.644186.
Train: 2018-08-09T09:03:06.131832: step 12075, loss 0.627741.
Train: 2018-08-09T09:03:09.213025: step 12076, loss 0.562475.
Train: 2018-08-09T09:03:12.284190: step 12077, loss 0.546719.
Train: 2018-08-09T09:03:15.398470: step 12078, loss 0.595175.
Train: 2018-08-09T09:03:18.482670: step 12079, loss 0.578764.
Train: 2018-08-09T09:03:21.577899: step 12080, loss 0.435241.
Test: 2018-08-09T09:03:42.820378: step 12080, loss 0.547991.
Train: 2018-08-09T09:03:45.910593: step 12081, loss 0.515004.
Train: 2018-08-09T09:03:49.002815: step 12082, loss 0.626538.
Train: 2018-08-09T09:03:52.091025: step 12083, loss 0.499234.
Train: 2018-08-09T09:03:55.184249: step 12084, loss 0.562931.
Train: 2018-08-09T09:03:58.261431: step 12085, loss 0.499237.
Train: 2018-08-09T09:04:01.386740: step 12086, loss 0.626794.
Train: 2018-08-09T09:04:04.472945: step 12087, loss 0.610956.
Train: 2018-08-09T09:04:07.551129: step 12088, loss 0.563212.
Train: 2018-08-09T09:04:10.665409: step 12089, loss 0.483166.
Train: 2018-08-09T09:04:13.740586: step 12090, loss 0.547026.
Test: 2018-08-09T09:04:34.961005: step 12090, loss 0.547282.
Train: 2018-08-09T09:04:38.052223: step 12091, loss 0.562809.
Train: 2018-08-09T09:04:41.207613: step 12092, loss 0.54689.
Train: 2018-08-09T09:04:44.308858: step 12093, loss 0.610806.
Train: 2018-08-09T09:04:47.408098: step 12094, loss 0.54691.
Train: 2018-08-09T09:04:50.504330: step 12095, loss 0.594625.
Train: 2018-08-09T09:04:53.581511: step 12096, loss 0.627006.
Train: 2018-08-09T09:04:56.672730: step 12097, loss 0.562725.
Train: 2018-08-09T09:04:59.775981: step 12098, loss 0.546715.
Train: 2018-08-09T09:05:02.867200: step 12099, loss 0.514534.
Train: 2018-08-09T09:05:05.982482: step 12100, loss 0.546837.
Test: 2018-08-09T09:05:27.129707: step 12100, loss 0.548359.
Train: 2018-08-09T09:05:32.087889: step 12101, loss 0.498249.
Train: 2018-08-09T09:05:35.194148: step 12102, loss 0.466276.
Train: 2018-08-09T09:05:38.283361: step 12103, loss 0.530354.
Train: 2018-08-09T09:05:41.378591: step 12104, loss 0.611436.
Train: 2018-08-09T09:05:44.431708: step 12105, loss 0.611269.
Train: 2018-08-09T09:05:47.534959: step 12106, loss 0.546178.
Train: 2018-08-09T09:05:50.643223: step 12107, loss 0.677449.
Train: 2018-08-09T09:05:53.698346: step 12108, loss 0.627829.
Train: 2018-08-09T09:05:56.812626: step 12109, loss 0.578688.
Train: 2018-08-09T09:05:59.893818: step 12110, loss 0.62807.
Test: 2018-08-09T09:06:21.039038: step 12110, loss 0.546268.
Train: 2018-08-09T09:06:24.113210: step 12111, loss 0.481332.
Train: 2018-08-09T09:06:27.204429: step 12112, loss 0.562564.
Train: 2018-08-09T09:06:30.287626: step 12113, loss 0.675997.
Train: 2018-08-09T09:06:33.394888: step 12114, loss 0.611301.
Train: 2018-08-09T09:06:36.511173: step 12115, loss 0.65966.
Train: 2018-08-09T09:06:39.612418: step 12116, loss 0.594703.
Train: 2018-08-09T09:06:42.726698: step 12117, loss 0.514841.
Train: 2018-08-09T09:06:45.846994: step 12118, loss 0.546699.
Train: 2018-08-09T09:06:48.948240: step 12119, loss 0.61051.
Train: 2018-08-09T09:06:52.025421: step 12120, loss 0.594864.
Test: 2018-08-09T09:07:13.187686: step 12120, loss 0.549479.
Train: 2018-08-09T09:07:16.313998: step 12121, loss 0.547302.
Train: 2018-08-09T09:07:19.400203: step 12122, loss 0.578898.
Train: 2018-08-09T09:07:22.477385: step 12123, loss 0.500054.
Train: 2018-08-09T09:07:25.584646: step 12124, loss 0.610084.
Train: 2018-08-09T09:07:28.662830: step 12125, loss 0.547592.
Train: 2018-08-09T09:07:31.743019: step 12126, loss 0.531588.
Train: 2018-08-09T09:07:34.851283: step 12127, loss 0.500497.
Train: 2018-08-09T09:07:37.935483: step 12128, loss 0.515738.
Train: 2018-08-09T09:07:41.087865: step 12129, loss 0.531719.
Train: 2018-08-09T09:07:44.148001: step 12130, loss 0.547209.
Test: 2018-08-09T09:08:05.301242: step 12130, loss 0.5494.
Train: 2018-08-09T09:08:08.392460: step 12131, loss 0.547524.
Train: 2018-08-09T09:08:11.461620: step 12132, loss 0.578857.
Train: 2018-08-09T09:08:14.568882: step 12133, loss 0.531115.
Train: 2018-08-09T09:08:17.638041: step 12134, loss 0.594482.
Train: 2018-08-09T09:08:20.713217: step 12135, loss 0.626223.
Train: 2018-08-09T09:08:23.788394: step 12136, loss 0.56304.
Train: 2018-08-09T09:08:26.890641: step 12137, loss 0.610781.
Train: 2018-08-09T09:08:30.013945: step 12138, loss 0.514603.
Train: 2018-08-09T09:08:33.073079: step 12139, loss 0.49856.
Train: 2018-08-09T09:08:36.185354: step 12140, loss 0.530992.
Test: 2018-08-09T09:08:57.327565: step 12140, loss 0.549485.
Train: 2018-08-09T09:09:00.423797: step 12141, loss 0.563142.
Train: 2018-08-09T09:09:03.636338: step 12142, loss 0.514211.
Train: 2018-08-09T09:09:06.727557: step 12143, loss 0.628444.
Train: 2018-08-09T09:09:09.847853: step 12144, loss 0.51504.
Train: 2018-08-09T09:09:12.937066: step 12145, loss 0.562865.
Train: 2018-08-09T09:09:15.994194: step 12146, loss 0.579751.
Train: 2018-08-09T09:09:19.098448: step 12147, loss 0.530511.
Train: 2018-08-09T09:09:22.188664: step 12148, loss 0.545818.
Train: 2018-08-09T09:09:25.302944: step 12149, loss 0.562009.
Train: 2018-08-09T09:09:28.379122: step 12150, loss 0.562858.
Test: 2018-08-09T09:09:49.542391: step 12150, loss 0.547866.
Train: 2018-08-09T09:09:52.659678: step 12151, loss 0.431462.
Train: 2018-08-09T09:09:55.733851: step 12152, loss 0.562778.
Train: 2018-08-09T09:09:58.847128: step 12153, loss 0.463582.
Train: 2018-08-09T09:10:01.915286: step 12154, loss 0.694854.
Train: 2018-08-09T09:10:04.988456: step 12155, loss 0.496329.
Train: 2018-08-09T09:10:08.080678: step 12156, loss 0.512663.
Train: 2018-08-09T09:10:11.134798: step 12157, loss 0.545489.
Train: 2018-08-09T09:10:14.224011: step 12158, loss 0.579235.
Train: 2018-08-09T09:10:17.319240: step 12159, loss 0.562475.
Train: 2018-08-09T09:10:20.396422: step 12160, loss 0.578864.
Test: 2018-08-09T09:10:41.571722: step 12160, loss 0.54666.
Train: 2018-08-09T09:10:44.663943: step 12161, loss 0.478441.
Train: 2018-08-09T09:10:47.743129: step 12162, loss 0.57887.
Train: 2018-08-09T09:10:50.837356: step 12163, loss 0.511881.
Train: 2018-08-09T09:10:53.928575: step 12164, loss 0.528753.
Train: 2018-08-09T09:10:57.032828: step 12165, loss 0.63071.
Train: 2018-08-09T09:11:00.090959: step 12166, loss 0.562126.
Train: 2018-08-09T09:11:03.165132: step 12167, loss 0.461281.
Train: 2018-08-09T09:11:06.288436: step 12168, loss 0.630488.
Train: 2018-08-09T09:11:09.370631: step 12169, loss 0.579334.
Train: 2018-08-09T09:11:12.448815: step 12170, loss 0.613216.
Test: 2018-08-09T09:11:33.592030: step 12170, loss 0.549001.
Train: 2018-08-09T09:11:36.718341: step 12171, loss 0.596591.
Train: 2018-08-09T09:11:39.799533: step 12172, loss 0.494918.
Train: 2018-08-09T09:11:42.888746: step 12173, loss 0.494453.
Train: 2018-08-09T09:11:46.010045: step 12174, loss 0.54536.
Train: 2018-08-09T09:11:49.108282: step 12175, loss 0.494373.
Train: 2018-08-09T09:11:52.225570: step 12176, loss 0.59654.
Train: 2018-08-09T09:11:55.301749: step 12177, loss 0.579769.
Train: 2018-08-09T09:11:58.390962: step 12178, loss 0.528307.
Train: 2018-08-09T09:12:01.497221: step 12179, loss 0.682165.
Train: 2018-08-09T09:12:04.586435: step 12180, loss 0.528383.
Test: 2018-08-09T09:12:25.743686: step 12180, loss 0.548398.
Train: 2018-08-09T09:12:28.853955: step 12181, loss 0.612921.
Train: 2018-08-09T09:12:31.960214: step 12182, loss 0.579012.
Train: 2018-08-09T09:12:35.075496: step 12183, loss 0.579175.
Train: 2018-08-09T09:12:38.131622: step 12184, loss 0.444592.
Train: 2018-08-09T09:12:41.216824: step 12185, loss 0.595822.
Train: 2018-08-09T09:12:44.280971: step 12186, loss 0.697148.
Train: 2018-08-09T09:12:47.381214: step 12187, loss 0.579317.
Train: 2018-08-09T09:12:50.434331: step 12188, loss 0.529028.
Train: 2018-08-09T09:12:53.522542: step 12189, loss 0.562407.
Train: 2018-08-09T09:12:56.611755: step 12190, loss 0.512501.
Test: 2018-08-09T09:13:17.770010: step 12190, loss 0.548764.
Train: 2018-08-09T09:13:20.863243: step 12191, loss 0.645569.
Train: 2018-08-09T09:13:24.027647: step 12192, loss 0.562453.
Train: 2018-08-09T09:13:27.102823: step 12193, loss 0.628633.
Train: 2018-08-09T09:13:30.177999: step 12194, loss 0.546051.
Train: 2018-08-09T09:13:33.260193: step 12195, loss 0.529703.
Train: 2018-08-09T09:13:36.370463: step 12196, loss 0.562637.
Train: 2018-08-09T09:13:39.448647: step 12197, loss 0.67733.
Train: 2018-08-09T09:13:42.549892: step 12198, loss 0.660717.
Train: 2018-08-09T09:13:45.624065: step 12199, loss 0.595171.
Train: 2018-08-09T09:13:48.719295: step 12200, loss 0.497932.
Test: 2018-08-09T09:14:09.845464: step 12200, loss 0.54945.
Train: 2018-08-09T09:14:14.694355: step 12201, loss 0.595054.
Train: 2018-08-09T09:14:17.776550: step 12202, loss 0.498444.
Train: 2018-08-09T09:14:20.857742: step 12203, loss 0.546753.
Train: 2018-08-09T09:14:23.944950: step 12204, loss 0.4666.
Train: 2018-08-09T09:14:27.023134: step 12205, loss 0.530735.
Train: 2018-08-09T09:14:30.103324: step 12206, loss 0.627044.
Train: 2018-08-09T09:14:33.182510: step 12207, loss 0.546755.
Train: 2018-08-09T09:14:36.242646: step 12208, loss 0.498576.
Train: 2018-08-09T09:14:39.292756: step 12209, loss 0.643182.
Train: 2018-08-09T09:14:42.364924: step 12210, loss 0.530646.
Test: 2018-08-09T09:15:03.494101: step 12210, loss 0.550154.
Train: 2018-08-09T09:15:06.625426: step 12211, loss 0.530622.
Train: 2018-08-09T09:15:09.676538: step 12212, loss 0.562777.
Train: 2018-08-09T09:15:12.770764: step 12213, loss 0.57888.
Train: 2018-08-09T09:15:15.866996: step 12214, loss 0.482155.
Train: 2018-08-09T09:15:18.949191: step 12215, loss 0.643486.
Train: 2018-08-09T09:15:22.042415: step 12216, loss 0.643511.
Train: 2018-08-09T09:15:25.176748: step 12217, loss 0.53045.
Train: 2018-08-09T09:15:28.257941: step 12218, loss 0.611164.
Train: 2018-08-09T09:15:31.334119: step 12219, loss 0.530501.
Train: 2018-08-09T09:15:34.400271: step 12220, loss 0.578874.
Test: 2018-08-09T09:15:55.567550: step 12220, loss 0.546447.
Train: 2018-08-09T09:15:58.656763: step 12221, loss 0.530532.
Train: 2018-08-09T09:16:01.696846: step 12222, loss 0.659469.
Train: 2018-08-09T09:16:04.819147: step 12223, loss 0.643258.
Train: 2018-08-09T09:16:07.904350: step 12224, loss 0.627034.
Train: 2018-08-09T09:16:10.993563: step 12225, loss 0.578861.
Train: 2018-08-09T09:16:14.054701: step 12226, loss 0.483086.
Train: 2018-08-09T09:16:17.132886: step 12227, loss 0.546976.
Train: 2018-08-09T09:16:20.198035: step 12228, loss 0.483294.
Train: 2018-08-09T09:16:23.280230: step 12229, loss 0.546983.
Train: 2018-08-09T09:16:26.353400: step 12230, loss 0.594807.
Test: 2018-08-09T09:16:47.526695: step 12230, loss 0.549168.
Train: 2018-08-09T09:16:50.620921: step 12231, loss 0.562908.
Train: 2018-08-09T09:16:53.706124: step 12232, loss 0.419294.
Train: 2018-08-09T09:16:56.783305: step 12233, loss 0.562862.
Train: 2018-08-09T09:16:59.875526: step 12234, loss 0.659041.
Train: 2018-08-09T09:17:02.978777: step 12235, loss 0.514697.
Train: 2018-08-09T09:17:06.064982: step 12236, loss 0.49852.
Train: 2018-08-09T09:17:09.128127: step 12237, loss 0.530549.
Train: 2018-08-09T09:17:12.195282: step 12238, loss 0.546592.
Train: 2018-08-09T09:17:15.280484: step 12239, loss 0.627453.
Train: 2018-08-09T09:17:18.392759: step 12240, loss 0.54647.
Test: 2018-08-09T09:17:39.524944: step 12240, loss 0.548685.
Train: 2018-08-09T09:17:42.679330: step 12241, loss 0.611368.
Train: 2018-08-09T09:17:45.794613: step 12242, loss 0.578911.
Train: 2018-08-09T09:17:48.864775: step 12243, loss 0.481409.
Train: 2018-08-09T09:17:51.940954: step 12244, loss 0.59518.
Train: 2018-08-09T09:17:55.022146: step 12245, loss 0.562631.
Train: 2018-08-09T09:17:58.144447: step 12246, loss 0.611526.
Train: 2018-08-09T09:18:01.234663: step 12247, loss 0.432181.
Train: 2018-08-09T09:18:04.318863: step 12248, loss 0.513582.
Train: 2018-08-09T09:18:07.395042: step 12249, loss 0.529816.
Train: 2018-08-09T09:18:10.424095: step 12250, loss 0.578974.
Test: 2018-08-09T09:18:31.598393: step 12250, loss 0.552089.
Train: 2018-08-09T09:18:34.707659: step 12251, loss 0.595426.
Train: 2018-08-09T09:18:37.837981: step 12252, loss 0.546008.
Train: 2018-08-09T09:18:40.999387: step 12253, loss 0.512959.
Train: 2018-08-09T09:18:44.088600: step 12254, loss 0.562484.
Train: 2018-08-09T09:18:47.146731: step 12255, loss 0.479649.
Train: 2018-08-09T09:18:50.252990: step 12256, loss 0.645462.
Train: 2018-08-09T09:18:53.329168: step 12257, loss 0.529155.
Train: 2018-08-09T09:18:56.406350: step 12258, loss 0.562457.
Train: 2018-08-09T09:18:59.494560: step 12259, loss 0.545749.
Train: 2018-08-09T09:19:02.607838: step 12260, loss 0.529035.
Test: 2018-08-09T09:19:23.787148: step 12260, loss 0.548633.
Train: 2018-08-09T09:19:26.865332: step 12261, loss 0.528952.
Train: 2018-08-09T09:19:29.947526: step 12262, loss 0.512183.
Train: 2018-08-09T09:19:33.033732: step 12263, loss 0.612836.
Train: 2018-08-09T09:19:36.126956: step 12264, loss 0.49512.
Train: 2018-08-09T09:19:39.227198: step 12265, loss 0.444388.
Train: 2018-08-09T09:19:42.316412: step 12266, loss 0.596126.
Train: 2018-08-09T09:19:45.409636: step 12267, loss 0.579297.
Train: 2018-08-09T09:19:48.527926: step 12268, loss 0.596313.
Train: 2018-08-09T09:19:51.639199: step 12269, loss 0.494436.
Train: 2018-08-09T09:19:54.739441: step 12270, loss 0.766649.
Test: 2018-08-09T09:20:15.907723: step 12270, loss 0.54771.
Train: 2018-08-09T09:20:19.023004: step 12271, loss 0.579381.
Train: 2018-08-09T09:20:22.098181: step 12272, loss 0.630202.
Train: 2018-08-09T09:20:25.150295: step 12273, loss 0.545441.
Train: 2018-08-09T09:20:28.249535: step 12274, loss 0.646843.
Train: 2018-08-09T09:20:31.353789: step 12275, loss 0.629671.
Train: 2018-08-09T09:20:34.457039: step 12276, loss 0.562407.
Train: 2018-08-09T09:20:37.586359: step 12277, loss 0.579116.
Train: 2018-08-09T09:20:40.689610: step 12278, loss 0.529177.
Train: 2018-08-09T09:20:43.768797: step 12279, loss 0.628813.
Train: 2018-08-09T09:20:46.868037: step 12280, loss 0.579021.
Test: 2018-08-09T09:21:08.010248: step 12280, loss 0.551447.
Train: 2018-08-09T09:21:11.096453: step 12281, loss 0.546049.
Train: 2018-08-09T09:21:14.164611: step 12282, loss 0.578972.
Train: 2018-08-09T09:21:17.236779: step 12283, loss 0.447938.
Train: 2018-08-09T09:21:20.305939: step 12284, loss 0.578946.
Train: 2018-08-09T09:21:23.391141: step 12285, loss 0.448204.
Train: 2018-08-09T09:21:26.488376: step 12286, loss 0.546239.
Train: 2018-08-09T09:21:29.555531: step 12287, loss 0.595315.
Train: 2018-08-09T09:21:32.623688: step 12288, loss 0.431617.
Train: 2018-08-09T09:21:35.708891: step 12289, loss 0.52975.
Train: 2018-08-09T09:21:38.801112: step 12290, loss 0.546095.
Test: 2018-08-09T09:21:59.956359: step 12290, loss 0.546452.
Train: 2018-08-09T09:22:03.028527: step 12291, loss 0.595477.
Train: 2018-08-09T09:22:06.155841: step 12292, loss 0.545997.
Train: 2018-08-09T09:22:09.263102: step 12293, loss 0.512893.
Train: 2018-08-09T09:22:12.362342: step 12294, loss 0.595608.
Train: 2018-08-09T09:22:15.435513: step 12295, loss 0.529284.
Train: 2018-08-09T09:22:18.530742: step 12296, loss 0.562452.
Train: 2018-08-09T09:22:21.624969: step 12297, loss 0.595723.
Train: 2018-08-09T09:22:24.720198: step 12298, loss 0.579089.
Train: 2018-08-09T09:22:27.817433: step 12299, loss 0.529116.
Train: 2018-08-09T09:22:30.925697: step 12300, loss 0.4624.
Test: 2018-08-09T09:22:52.072922: step 12300, loss 0.548658.
Train: 2018-08-09T09:22:56.880704: step 12301, loss 0.579133.
Train: 2018-08-09T09:22:59.970920: step 12302, loss 0.629329.
Train: 2018-08-09T09:23:03.054118: step 12303, loss 0.562402.
Train: 2018-08-09T09:23:06.143331: step 12304, loss 0.52893.
Train: 2018-08-09T09:23:09.202464: step 12305, loss 0.595898.
Train: 2018-08-09T09:23:12.282654: step 12306, loss 0.545658.
Train: 2018-08-09T09:23:15.368859: step 12307, loss 0.629394.
Train: 2018-08-09T09:23:18.481134: step 12308, loss 0.528942.
Train: 2018-08-09T09:23:21.528235: step 12309, loss 0.512234.
Train: 2018-08-09T09:23:24.635497: step 12310, loss 0.662782.
Test: 2018-08-09T09:23:45.761666: step 12310, loss 0.547393.
Train: 2018-08-09T09:23:48.808767: step 12311, loss 0.562416.
Train: 2018-08-09T09:23:51.903996: step 12312, loss 0.629173.
Train: 2018-08-09T09:23:54.984186: step 12313, loss 0.545784.
Train: 2018-08-09T09:23:58.087436: step 12314, loss 0.595696.
Train: 2018-08-09T09:24:01.183668: step 12315, loss 0.529285.
Train: 2018-08-09T09:24:04.255836: step 12316, loss 0.54591.
Train: 2018-08-09T09:24:07.356079: step 12317, loss 0.4963.
Train: 2018-08-09T09:24:10.460332: step 12318, loss 0.545939.
Train: 2018-08-09T09:24:13.562580: step 12319, loss 0.512848.
Train: 2018-08-09T09:24:16.639762: step 12320, loss 0.562473.
Test: 2018-08-09T09:24:37.773953: step 12320, loss 0.548828.
Train: 2018-08-09T09:24:40.852136: step 12321, loss 0.645307.
Train: 2018-08-09T09:24:43.947365: step 12322, loss 0.446585.
Train: 2018-08-09T09:24:47.039586: step 12323, loss 0.562457.
Train: 2018-08-09T09:24:50.118773: step 12324, loss 0.545848.
Train: 2018-08-09T09:24:53.246088: step 12325, loss 0.562477.
Train: 2018-08-09T09:24:56.323269: step 12326, loss 0.579101.
Train: 2018-08-09T09:24:59.389421: step 12327, loss 0.595738.
Train: 2018-08-09T09:25:02.482645: step 12328, loss 0.462711.
Train: 2018-08-09T09:25:05.574867: step 12329, loss 0.645677.
Train: 2018-08-09T09:25:08.646032: step 12330, loss 0.57908.
Test: 2018-08-09T09:25:29.781225: step 12330, loss 0.547497.
Train: 2018-08-09T09:25:32.903526: step 12331, loss 0.595699.
Train: 2018-08-09T09:25:36.000761: step 12332, loss 0.52923.
Train: 2018-08-09T09:25:39.070923: step 12333, loss 0.562461.
Train: 2018-08-09T09:25:42.157129: step 12334, loss 0.595657.
Train: 2018-08-09T09:25:45.257372: step 12335, loss 0.562466.
Train: 2018-08-09T09:25:48.348590: step 12336, loss 0.463059.
Train: 2018-08-09T09:25:51.441814: step 12337, loss 0.661924.
Train: 2018-08-09T09:25:54.539049: step 12338, loss 0.628719.
Train: 2018-08-09T09:25:57.638289: step 12339, loss 0.612088.
Train: 2018-08-09T09:26:00.730510: step 12340, loss 0.529535.
Test: 2018-08-09T09:26:21.865703: step 12340, loss 0.548346.
Train: 2018-08-09T09:26:24.766415: step 12341, loss 0.580095.
Train: 2018-08-09T09:26:27.886711: step 12342, loss 0.431093.
Train: 2018-08-09T09:26:31.014026: step 12343, loss 0.546101.
Train: 2018-08-09T09:26:34.098226: step 12344, loss 0.595428.
Train: 2018-08-09T09:26:37.185434: step 12345, loss 0.611844.
Train: 2018-08-09T09:26:40.260610: step 12346, loss 0.578987.
Train: 2018-08-09T09:26:43.322751: step 12347, loss 0.562559.
Train: 2018-08-09T09:26:46.372861: step 12348, loss 0.513391.
Train: 2018-08-09T09:26:49.417957: step 12349, loss 0.546185.
Train: 2018-08-09T09:26:52.511181: step 12350, loss 0.431441.
Test: 2018-08-09T09:27:13.625318: step 12350, loss 0.548401.
Train: 2018-08-09T09:27:16.727565: step 12351, loss 0.677516.
Train: 2018-08-09T09:27:19.832821: step 12352, loss 0.546115.
Train: 2018-08-09T09:27:22.914013: step 12353, loss 0.546113.
Train: 2018-08-09T09:27:26.005232: step 12354, loss 0.578982.
Train: 2018-08-09T09:27:29.080408: step 12355, loss 0.595426.
Train: 2018-08-09T09:27:32.164608: step 12356, loss 0.480345.
Train: 2018-08-09T09:27:35.222739: step 12357, loss 0.54607.
Train: 2018-08-09T09:27:38.316966: step 12358, loss 0.628458.
Train: 2018-08-09T09:27:41.375096: step 12359, loss 0.46369.
Train: 2018-08-09T09:27:44.498400: step 12360, loss 0.562587.
Test: 2018-08-09T09:28:05.587471: step 12360, loss 0.547653.
Train: 2018-08-09T09:28:08.690721: step 12361, loss 0.496463.
Train: 2018-08-09T09:28:11.771913: step 12362, loss 0.579075.
Train: 2018-08-09T09:28:14.855110: step 12363, loss 0.54585.
Train: 2018-08-09T09:28:17.940313: step 12364, loss 0.595636.
Train: 2018-08-09T09:28:21.031532: step 12365, loss 0.579003.
Train: 2018-08-09T09:28:24.112724: step 12366, loss 0.612295.
Train: 2018-08-09T09:28:27.178876: step 12367, loss 0.545809.
Train: 2018-08-09T09:28:30.263076: step 12368, loss 0.545788.
Train: 2018-08-09T09:28:33.338252: step 12369, loss 0.529177.
Train: 2018-08-09T09:28:36.413428: step 12370, loss 0.529227.
Test: 2018-08-09T09:28:57.522552: step 12370, loss 0.547454.
Train: 2018-08-09T09:29:00.604746: step 12371, loss 0.612306.
Train: 2018-08-09T09:29:03.706994: step 12372, loss 0.49589.
Train: 2018-08-09T09:29:06.796208: step 12373, loss 0.478474.
Train: 2018-08-09T09:29:09.915501: step 12374, loss 0.561534.
Train: 2018-08-09T09:29:13.020757: step 12375, loss 0.545315.
Train: 2018-08-09T09:29:16.117992: step 12376, loss 0.564307.
Train: 2018-08-09T09:29:19.224250: step 12377, loss 0.546134.
Train: 2018-08-09T09:29:22.292407: step 12378, loss 0.476744.
Train: 2018-08-09T09:29:25.376607: step 12379, loss 0.562201.
Train: 2018-08-09T09:29:28.491890: step 12380, loss 0.544593.
Test: 2018-08-09T09:29:49.534838: step 12380, loss 0.549413.
Train: 2018-08-09T09:29:52.636083: step 12381, loss 0.55994.
Train: 2018-08-09T09:29:55.697222: step 12382, loss 0.56194.
Train: 2018-08-09T09:29:58.790446: step 12383, loss 0.543121.
Train: 2018-08-09T09:30:01.868630: step 12384, loss 0.56721.
Train: 2018-08-09T09:30:04.964862: step 12385, loss 0.579869.
Train: 2018-08-09T09:30:08.052070: step 12386, loss 0.558378.
Train: 2018-08-09T09:30:11.127246: step 12387, loss 0.614753.
Train: 2018-08-09T09:30:14.198411: step 12388, loss 0.529213.
Train: 2018-08-09T09:30:17.269577: step 12389, loss 0.636488.
Train: 2018-08-09T09:30:20.368817: step 12390, loss 0.583151.
Test: 2018-08-09T09:30:41.425802: step 12390, loss 0.550147.
Train: 2018-08-09T09:30:44.576177: step 12391, loss 0.664785.
Train: 2018-08-09T09:30:47.646340: step 12392, loss 0.563567.
Train: 2018-08-09T09:30:50.695447: step 12393, loss 0.545752.
Train: 2018-08-09T09:30:53.790676: step 12394, loss 0.61222.
Train: 2018-08-09T09:30:56.868860: step 12395, loss 0.479528.
Train: 2018-08-09T09:30:59.917967: step 12396, loss 0.52938.
Train: 2018-08-09T09:31:03.009186: step 12397, loss 0.56249.
Train: 2018-08-09T09:31:06.084362: step 12398, loss 0.628518.
Train: 2018-08-09T09:31:09.146503: step 12399, loss 0.694321.
Train: 2018-08-09T09:31:12.211652: step 12400, loss 0.59537.
Test: 2018-08-09T09:31:33.209480: step 12400, loss 0.549132.
Train: 2018-08-09T09:31:38.240857: step 12401, loss 0.562591.
Train: 2018-08-09T09:31:41.324054: step 12402, loss 0.546339.
Train: 2018-08-09T09:31:44.390206: step 12403, loss 0.611406.
Train: 2018-08-09T09:31:47.477415: step 12404, loss 0.546525.
Train: 2018-08-09T09:31:50.548580: step 12405, loss 0.482022.
Train: 2018-08-09T09:31:53.563596: step 12406, loss 0.562757.
Train: 2018-08-09T09:31:56.648799: step 12407, loss 0.562778.
Train: 2018-08-09T09:31:59.712945: step 12408, loss 0.562794.
Train: 2018-08-09T09:32:02.795140: step 12409, loss 0.59498.
Train: 2018-08-09T09:32:05.886359: step 12410, loss 0.466483.
Test: 2018-08-09T09:32:26.827034: step 12410, loss 0.54715.
Train: 2018-08-09T09:32:29.911234: step 12411, loss 0.546739.
Train: 2018-08-09T09:32:33.000447: step 12412, loss 0.418122.
Train: 2018-08-09T09:32:36.076626: step 12413, loss 0.498274.
Train: 2018-08-09T09:32:39.188901: step 12414, loss 0.578877.
Train: 2018-08-09T09:32:42.242018: step 12415, loss 0.578936.
Train: 2018-08-09T09:32:45.321205: step 12416, loss 0.497522.
Train: 2018-08-09T09:32:48.440498: step 12417, loss 0.480915.
Train: 2018-08-09T09:32:51.512666: step 12418, loss 0.595412.
Train: 2018-08-09T09:32:54.565784: step 12419, loss 0.513115.
Train: 2018-08-09T09:32:57.677056: step 12420, loss 0.544958.
Test: 2018-08-09T09:33:18.654831: step 12420, loss 0.549873.
Train: 2018-08-09T09:33:21.744043: step 12421, loss 0.562988.
Train: 2018-08-09T09:33:24.817214: step 12422, loss 0.459955.
Train: 2018-08-09T09:33:27.888379: step 12423, loss 0.495202.
Train: 2018-08-09T09:33:30.978595: step 12424, loss 0.581425.
Train: 2018-08-09T09:33:34.052769: step 12425, loss 0.616291.
Train: 2018-08-09T09:33:37.138974: step 12426, loss 0.667187.
Train: 2018-08-09T09:33:40.281329: step 12427, loss 0.646748.
Train: 2018-08-09T09:33:43.396611: step 12428, loss 0.579968.
Train: 2018-08-09T09:33:46.484822: step 12429, loss 0.665367.
Train: 2018-08-09T09:33:49.550974: step 12430, loss 0.595737.
Test: 2018-08-09T09:34:10.488642: step 12430, loss 0.547935.
Train: 2018-08-09T09:34:13.556799: step 12431, loss 0.445124.
Train: 2018-08-09T09:34:16.631975: step 12432, loss 0.562406.
Train: 2018-08-09T09:34:19.713167: step 12433, loss 0.645812.
Train: 2018-08-09T09:34:22.809399: step 12434, loss 0.54581.
Train: 2018-08-09T09:34:25.889589: step 12435, loss 0.545911.
Train: 2018-08-09T09:34:28.947719: step 12436, loss 0.545943.
Train: 2018-08-09T09:34:32.019887: step 12437, loss 0.545963.
Train: 2018-08-09T09:34:35.094060: step 12438, loss 0.545988.
Train: 2018-08-09T09:34:38.149183: step 12439, loss 0.545997.
Train: 2018-08-09T09:34:41.241405: step 12440, loss 0.595512.
Test: 2018-08-09T09:35:02.181078: step 12440, loss 0.551435.
Train: 2018-08-09T09:35:05.283326: step 12441, loss 0.562513.
Train: 2018-08-09T09:35:08.379558: step 12442, loss 0.66134.
Train: 2018-08-09T09:35:11.435683: step 12443, loss 0.628272.
Train: 2018-08-09T09:35:14.522891: step 12444, loss 0.529797.
Train: 2018-08-09T09:35:17.583027: step 12445, loss 0.611644.
Train: 2018-08-09T09:35:20.650182: step 12446, loss 0.513705.
Train: 2018-08-09T09:35:23.726360: step 12447, loss 0.562639.
Train: 2018-08-09T09:35:26.805547: step 12448, loss 0.595166.
Train: 2018-08-09T09:35:29.882729: step 12449, loss 0.562681.
Train: 2018-08-09T09:35:32.972945: step 12450, loss 0.627476.
Test: 2018-08-09T09:35:53.887552: step 12450, loss 0.548815.
Train: 2018-08-09T09:35:57.055975: step 12451, loss 0.578885.
Train: 2018-08-09T09:36:00.111097: step 12452, loss 0.498294.
Train: 2018-08-09T09:36:03.190284: step 12453, loss 0.562777.
Train: 2018-08-09T09:36:06.296543: step 12454, loss 0.546713.
Train: 2018-08-09T09:36:09.353671: step 12455, loss 0.594939.
Train: 2018-08-09T09:36:12.413807: step 12456, loss 0.546755.
Train: 2018-08-09T09:36:15.471938: step 12457, loss 0.482591.
Train: 2018-08-09T09:36:18.565162: step 12458, loss 0.562793.
Train: 2018-08-09T09:36:21.645351: step 12459, loss 0.594942.
Train: 2018-08-09T09:36:24.730554: step 12460, loss 0.643192.
Test: 2018-08-09T09:36:45.650174: step 12460, loss 0.549585.
Train: 2018-08-09T09:36:48.706299: step 12461, loss 0.594913.
Train: 2018-08-09T09:36:51.804536: step 12462, loss 0.514712.
Train: 2018-08-09T09:36:54.897760: step 12463, loss 0.626943.
Train: 2018-08-09T09:36:57.985971: step 12464, loss 0.562881.
Train: 2018-08-09T09:37:01.048112: step 12465, loss 0.59487.
Train: 2018-08-09T09:37:04.107246: step 12466, loss 0.594834.
Train: 2018-08-09T09:37:07.184427: step 12467, loss 0.610779.
Train: 2018-08-09T09:37:10.271635: step 12468, loss 0.578861.
Train: 2018-08-09T09:37:13.351824: step 12469, loss 0.562979.
Train: 2018-08-09T09:37:16.439032: step 12470, loss 0.547158.
Test: 2018-08-09T09:37:37.324561: step 12470, loss 0.54881.
Train: 2018-08-09T09:37:40.394724: step 12471, loss 0.626339.
Train: 2018-08-09T09:37:43.476918: step 12472, loss 0.610456.
Train: 2018-08-09T09:37:46.608244: step 12473, loss 0.563129.
Train: 2018-08-09T09:37:49.674396: step 12474, loss 0.468867.
Train: 2018-08-09T09:37:52.745561: step 12475, loss 0.468893.
Train: 2018-08-09T09:37:55.808706: step 12476, loss 0.64178.
Train: 2018-08-09T09:37:58.899924: step 12477, loss 0.56313.
Train: 2018-08-09T09:38:02.018215: step 12478, loss 0.484462.
Train: 2018-08-09T09:38:05.083364: step 12479, loss 0.500063.
Train: 2018-08-09T09:38:08.160545: step 12480, loss 0.594639.
Test: 2018-08-09T09:38:29.027024: step 12480, loss 0.549397.
Train: 2018-08-09T09:38:32.109218: step 12481, loss 0.594685.
Train: 2018-08-09T09:38:35.173365: step 12482, loss 0.658106.
Train: 2018-08-09T09:38:38.244530: step 12483, loss 0.48381.
Train: 2018-08-09T09:38:41.317701: step 12484, loss 0.57887.
Train: 2018-08-09T09:38:44.354776: step 12485, loss 0.610628.
Train: 2018-08-09T09:38:47.441984: step 12486, loss 0.626522.
Train: 2018-08-09T09:38:50.530195: step 12487, loss 0.515412.
Train: 2018-08-09T09:38:53.600358: step 12488, loss 0.531257.
Train: 2018-08-09T09:38:56.667512: step 12489, loss 0.610611.
Train: 2018-08-09T09:38:59.740683: step 12490, loss 0.547094.
Test: 2018-08-09T09:39:20.606159: step 12490, loss 0.551101.
Train: 2018-08-09T09:39:23.669302: step 12491, loss 0.531231.
Train: 2018-08-09T09:39:26.757513: step 12492, loss 0.562971.
Train: 2018-08-09T09:39:29.804614: step 12493, loss 0.706093.
Train: 2018-08-09T09:39:32.881796: step 12494, loss 0.515324.
Train: 2018-08-09T09:39:35.946945: step 12495, loss 0.578843.
Train: 2018-08-09T09:39:39.015103: step 12496, loss 0.531222.
Train: 2018-08-09T09:39:42.098300: step 12497, loss 0.53123.
Train: 2018-08-09T09:39:45.151417: step 12498, loss 0.594779.
Train: 2018-08-09T09:39:48.205537: step 12499, loss 0.610619.
Train: 2018-08-09T09:39:51.265674: step 12500, loss 0.515369.
Test: 2018-08-09T09:40:12.114104: step 12500, loss 0.55049.
Train: 2018-08-09T09:40:16.987060: step 12501, loss 0.626499.
Train: 2018-08-09T09:40:20.031153: step 12502, loss 0.562969.
Train: 2018-08-09T09:40:23.101316: step 12503, loss 0.562999.
Train: 2018-08-09T09:40:26.167468: step 12504, loss 0.57885.
Train: 2018-08-09T09:40:29.216575: step 12505, loss 0.562984.
Train: 2018-08-09T09:40:32.242620: step 12506, loss 0.547135.
Train: 2018-08-09T09:40:35.279695: step 12507, loss 0.562989.
Train: 2018-08-09T09:40:38.354871: step 12508, loss 0.531272.
Train: 2018-08-09T09:40:41.388937: step 12509, loss 0.626476.
Train: 2018-08-09T09:40:44.462108: step 12510, loss 0.531227.
Test: 2018-08-09T09:41:05.298507: step 12510, loss 0.552306.
Train: 2018-08-09T09:41:08.382706: step 12511, loss 0.610564.
Train: 2018-08-09T09:41:11.440837: step 12512, loss 0.610596.
Train: 2018-08-09T09:41:14.494957: step 12513, loss 0.658128.
Train: 2018-08-09T09:41:17.560107: step 12514, loss 0.483914.
Train: 2018-08-09T09:41:20.625256: step 12515, loss 0.594716.
Train: 2018-08-09T09:41:23.700432: step 12516, loss 0.547269.
Train: 2018-08-09T09:41:26.814712: step 12517, loss 0.642076.
Train: 2018-08-09T09:41:29.902923: step 12518, loss 0.547331.
Train: 2018-08-09T09:41:32.960051: step 12519, loss 0.578872.
Train: 2018-08-09T09:41:35.999131: step 12520, loss 0.531698.
Test: 2018-08-09T09:41:56.845556: step 12520, loss 0.551984.
Train: 2018-08-09T09:41:59.901681: step 12521, loss 0.531678.
Train: 2018-08-09T09:42:02.985881: step 12522, loss 0.625969.
Train: 2018-08-09T09:42:06.044012: step 12523, loss 0.578851.
Train: 2018-08-09T09:42:09.128212: step 12524, loss 0.610288.
Train: 2018-08-09T09:42:12.174311: step 12525, loss 0.610265.
Train: 2018-08-09T09:42:15.240463: step 12526, loss 0.578891.
Train: 2018-08-09T09:42:18.305612: step 12527, loss 0.532025.
Train: 2018-08-09T09:42:21.393823: step 12528, loss 0.563265.
Train: 2018-08-09T09:42:24.462983: step 12529, loss 0.422796.
Train: 2018-08-09T09:42:27.510084: step 12530, loss 0.563287.
Test: 2018-08-09T09:42:48.358515: step 12530, loss 0.551498.
Train: 2018-08-09T09:42:51.415642: step 12531, loss 0.610218.
Train: 2018-08-09T09:42:54.504856: step 12532, loss 0.500445.
Train: 2018-08-09T09:42:57.584042: step 12533, loss 0.563139.
Train: 2018-08-09T09:43:00.652200: step 12534, loss 0.531638.
Train: 2018-08-09T09:43:03.713339: step 12535, loss 0.578853.
Train: 2018-08-09T09:43:06.800547: step 12536, loss 0.531435.
Train: 2018-08-09T09:43:09.858677: step 12537, loss 0.531271.
Train: 2018-08-09T09:43:12.945885: step 12538, loss 0.578841.
Train: 2018-08-09T09:43:16.015045: step 12539, loss 0.594829.
Train: 2018-08-09T09:43:19.092227: step 12540, loss 0.5629.
Test: 2018-08-09T09:43:39.947677: step 12540, loss 0.548473.
Train: 2018-08-09T09:43:43.039897: step 12541, loss 0.658952.
Train: 2018-08-09T09:43:46.146156: step 12542, loss 0.530863.
Train: 2018-08-09T09:43:49.212308: step 12543, loss 0.530845.
Train: 2018-08-09T09:43:52.274449: step 12544, loss 0.56289.
Train: 2018-08-09T09:43:55.319545: step 12545, loss 0.546828.
Train: 2018-08-09T09:43:58.392716: step 12546, loss 0.562774.
Train: 2018-08-09T09:44:01.484937: step 12547, loss 0.562786.
Train: 2018-08-09T09:44:04.544071: step 12548, loss 0.482328.
Train: 2018-08-09T09:44:07.636292: step 12549, loss 0.546693.
Train: 2018-08-09T09:44:10.725505: step 12550, loss 0.595012.
Test: 2018-08-09T09:44:31.564912: step 12550, loss 0.548759.
Train: 2018-08-09T09:44:34.661144: step 12551, loss 0.562728.
Train: 2018-08-09T09:44:37.753365: step 12552, loss 0.562745.
Train: 2018-08-09T09:44:40.814504: step 12553, loss 0.627656.
Train: 2018-08-09T09:44:43.871632: step 12554, loss 0.562747.
Train: 2018-08-09T09:44:46.953826: step 12555, loss 0.546523.
Train: 2018-08-09T09:44:50.007947: step 12556, loss 0.660209.
Train: 2018-08-09T09:44:53.102173: step 12557, loss 0.595053.
Train: 2018-08-09T09:44:56.189381: step 12558, loss 0.514138.
Train: 2018-08-09T09:44:59.264557: step 12559, loss 0.627423.
Train: 2018-08-09T09:45:02.340736: step 12560, loss 0.578802.
Test: 2018-08-09T09:45:23.182148: step 12560, loss 0.550727.
Train: 2018-08-09T09:45:26.239276: step 12561, loss 0.578959.
Train: 2018-08-09T09:45:29.322473: step 12562, loss 0.643414.
Train: 2018-08-09T09:45:32.392636: step 12563, loss 0.466826.
Train: 2018-08-09T09:45:35.457785: step 12564, loss 0.578882.
Train: 2018-08-09T09:45:38.540982: step 12565, loss 0.562858.
Train: 2018-08-09T09:45:41.643230: step 12566, loss 0.562858.
Train: 2018-08-09T09:45:44.710385: step 12567, loss 0.562859.
Train: 2018-08-09T09:45:47.789572: step 12568, loss 0.578878.
Train: 2018-08-09T09:45:50.844695: step 12569, loss 0.498873.
Train: 2018-08-09T09:45:53.918868: step 12570, loss 0.466815.
Test: 2018-08-09T09:46:14.744237: step 12570, loss 0.549016.
Train: 2018-08-09T09:46:17.863530: step 12571, loss 0.530756.
Train: 2018-08-09T09:46:20.916648: step 12572, loss 0.546725.
Train: 2018-08-09T09:46:24.004858: step 12573, loss 0.514431.
Train: 2018-08-09T09:46:27.092066: step 12574, loss 0.595054.
Train: 2018-08-09T09:46:30.118112: step 12575, loss 0.562698.
Train: 2018-08-09T09:46:33.169224: step 12576, loss 0.595123.
Train: 2018-08-09T09:46:36.237381: step 12577, loss 0.513922.
Train: 2018-08-09T09:46:39.299523: step 12578, loss 0.497524.
Train: 2018-08-09T09:46:42.345621: step 12579, loss 0.513639.
Train: 2018-08-09T09:46:45.424808: step 12580, loss 0.529822.
Test: 2018-08-09T09:47:06.280258: step 12580, loss 0.549012.
Train: 2018-08-09T09:47:09.332371: step 12581, loss 0.546111.
Train: 2018-08-09T09:47:12.414566: step 12582, loss 0.562483.
Train: 2018-08-09T09:47:15.481721: step 12583, loss 0.512891.
Train: 2018-08-09T09:47:18.559905: step 12584, loss 0.579052.
Train: 2018-08-09T09:47:21.632073: step 12585, loss 0.512451.
Train: 2018-08-09T09:47:24.680177: step 12586, loss 0.528995.
Train: 2018-08-09T09:47:27.752345: step 12587, loss 0.528813.
Train: 2018-08-09T09:47:30.826519: step 12588, loss 0.528598.
Train: 2018-08-09T09:47:33.900692: step 12589, loss 0.493854.
Train: 2018-08-09T09:47:36.956818: step 12590, loss 0.648227.
Test: 2018-08-09T09:47:57.893482: step 12590, loss 0.545538.
Train: 2018-08-09T09:48:00.977683: step 12591, loss 0.579588.
Train: 2018-08-09T09:48:04.100986: step 12592, loss 0.527973.
Train: 2018-08-09T09:48:07.165133: step 12593, loss 0.49397.
Train: 2018-08-09T09:48:10.209226: step 12594, loss 0.615201.
Train: 2018-08-09T09:48:13.271368: step 12595, loss 0.562241.
Train: 2018-08-09T09:48:16.308443: step 12596, loss 0.615257.
Train: 2018-08-09T09:48:19.388632: step 12597, loss 0.613659.
Train: 2018-08-09T09:48:22.425707: step 12598, loss 0.579029.
Train: 2018-08-09T09:48:25.487848: step 12599, loss 0.56227.
Train: 2018-08-09T09:48:28.544976: step 12600, loss 0.460365.
Test: 2018-08-09T09:48:49.468607: step 12600, loss 0.547717.
Train: 2018-08-09T09:48:54.299450: step 12601, loss 0.528381.
Train: 2018-08-09T09:48:57.356578: step 12602, loss 0.528349.
Train: 2018-08-09T09:49:00.419722: step 12603, loss 0.477398.
Train: 2018-08-09T09:49:03.516957: step 12604, loss 0.545285.
Train: 2018-08-09T09:49:06.566064: step 12605, loss 0.545307.
Train: 2018-08-09T09:49:09.653272: step 12606, loss 0.528178.
Train: 2018-08-09T09:49:12.736470: step 12607, loss 0.596501.
Train: 2018-08-09T09:49:15.807634: step 12608, loss 0.579441.
Train: 2018-08-09T09:49:18.869776: step 12609, loss 0.545195.
Train: 2018-08-09T09:49:21.932920: step 12610, loss 0.4594.
Test: 2018-08-09T09:49:42.738236: step 12610, loss 0.547542.
Train: 2018-08-09T09:49:45.791353: step 12611, loss 0.527981.
Train: 2018-08-09T09:49:48.852492: step 12612, loss 0.631179.
Train: 2018-08-09T09:49:51.899593: step 12613, loss 0.545071.
Train: 2018-08-09T09:49:54.927644: step 12614, loss 0.579542.
Train: 2018-08-09T09:49:57.983769: step 12615, loss 0.631244.
Train: 2018-08-09T09:50:01.045910: step 12616, loss 0.527923.
Train: 2018-08-09T09:50:04.126100: step 12617, loss 0.613926.
Train: 2018-08-09T09:50:07.191249: step 12618, loss 0.545078.
Train: 2018-08-09T09:50:10.271439: step 12619, loss 0.596642.
Train: 2018-08-09T09:50:13.301495: step 12620, loss 0.511019.
Test: 2018-08-09T09:50:34.120848: step 12620, loss 0.548906.
Train: 2018-08-09T09:50:37.128845: step 12621, loss 0.579461.
Train: 2018-08-09T09:50:40.207029: step 12622, loss 0.476955.
Train: 2018-08-09T09:50:43.283208: step 12623, loss 0.425801.
Train: 2018-08-09T09:50:46.332315: step 12624, loss 0.545282.
Train: 2018-08-09T09:50:49.367384: step 12625, loss 0.579405.
Train: 2018-08-09T09:50:52.436544: step 12626, loss 0.665196.
Train: 2018-08-09T09:50:55.527763: step 12627, loss 0.528194.
Train: 2018-08-09T09:50:58.580880: step 12628, loss 0.562318.
Train: 2018-08-09T09:51:01.662072: step 12629, loss 0.579526.
Train: 2018-08-09T09:51:04.734240: step 12630, loss 0.579397.
Test: 2018-08-09T09:51:25.535546: step 12630, loss 0.54763.
Train: 2018-08-09T09:51:28.633782: step 12631, loss 0.562254.
Train: 2018-08-09T09:51:31.716980: step 12632, loss 0.562359.
Train: 2018-08-09T09:51:34.807196: step 12633, loss 0.596448.
Train: 2018-08-09T09:51:37.865326: step 12634, loss 0.715392.
Train: 2018-08-09T09:51:40.930476: step 12635, loss 0.376022.
Train: 2018-08-09T09:51:43.994623: step 12636, loss 0.663899.
Train: 2018-08-09T09:51:47.029692: step 12637, loss 0.646723.
Train: 2018-08-09T09:51:50.110884: step 12638, loss 0.629566.
Train: 2018-08-09T09:51:53.190071: step 12639, loss 0.5791.
Train: 2018-08-09T09:51:56.241183: step 12640, loss 0.595335.
Test: 2018-08-09T09:52:17.037475: step 12640, loss 0.548879.
Train: 2018-08-09T09:52:20.096608: step 12641, loss 0.562366.
Train: 2018-08-09T09:52:23.000328: step 12642, loss 0.598399.
Train: 2018-08-09T09:52:26.069488: step 12643, loss 0.497601.
Train: 2018-08-09T09:52:29.109571: step 12644, loss 0.513497.
Train: 2018-08-09T09:52:32.175723: step 12645, loss 0.546561.
Train: 2018-08-09T09:52:35.220819: step 12646, loss 0.546345.
Train: 2018-08-09T09:52:38.271931: step 12647, loss 0.56263.
Train: 2018-08-09T09:52:41.350115: step 12648, loss 0.497137.
Train: 2018-08-09T09:52:44.386187: step 12649, loss 0.578951.
Train: 2018-08-09T09:52:47.461363: step 12650, loss 0.480679.
Test: 2018-08-09T09:53:08.263671: step 12650, loss 0.549064.
Train: 2018-08-09T09:53:11.317791: step 12651, loss 0.644554.
Train: 2018-08-09T09:53:14.397980: step 12652, loss 0.513369.
Train: 2018-08-09T09:53:17.462127: step 12653, loss 0.57898.
Train: 2018-08-09T09:53:20.512236: step 12654, loss 0.562545.
Train: 2018-08-09T09:53:23.582399: step 12655, loss 0.611801.
Train: 2018-08-09T09:53:26.635516: step 12656, loss 0.677368.
Train: 2018-08-09T09:53:29.692645: step 12657, loss 0.578941.
Train: 2018-08-09T09:53:32.733730: step 12658, loss 0.627864.
Train: 2018-08-09T09:53:35.790858: step 12659, loss 0.644037.
Train: 2018-08-09T09:53:38.865031: step 12660, loss 0.595101.
Test: 2018-08-09T09:53:59.647286: step 12660, loss 0.54824.
Train: 2018-08-09T09:54:02.734493: step 12661, loss 0.578883.
Train: 2018-08-09T09:54:05.811675: step 12662, loss 0.450295.
Train: 2018-08-09T09:54:08.873816: step 12663, loss 0.594912.
Train: 2018-08-09T09:54:11.930945: step 12664, loss 0.498802.
Train: 2018-08-09T09:54:14.987070: step 12665, loss 0.610866.
Train: 2018-08-09T09:54:18.043195: step 12666, loss 0.546904.
Train: 2018-08-09T09:54:21.079267: step 12667, loss 0.61079.
Train: 2018-08-09T09:54:24.160459: step 12668, loss 0.531036.
Train: 2018-08-09T09:54:27.223603: step 12669, loss 0.594788.
Train: 2018-08-09T09:54:30.262683: step 12670, loss 0.690238.
Test: 2018-08-09T09:54:51.053962: step 12670, loss 0.549329.
Train: 2018-08-09T09:54:54.104071: step 12671, loss 0.578859.
Train: 2018-08-09T09:54:57.155183: step 12672, loss 0.547219.
Train: 2018-08-09T09:55:00.223341: step 12673, loss 0.54729.
Train: 2018-08-09T09:55:03.260415: step 12674, loss 0.51582.
Train: 2018-08-09T09:55:06.311527: step 12675, loss 0.531609.
Train: 2018-08-09T09:55:09.349605: step 12676, loss 0.531603.
Train: 2018-08-09T09:55:12.376653: step 12677, loss 0.500033.
Train: 2018-08-09T09:55:15.458848: step 12678, loss 0.468291.
Train: 2018-08-09T09:55:18.502941: step 12679, loss 0.49961.
Train: 2018-08-09T09:55:21.571099: step 12680, loss 0.547024.
Test: 2018-08-09T09:55:42.360372: step 12680, loss 0.547864.
Train: 2018-08-09T09:55:45.423515: step 12681, loss 0.594712.
Train: 2018-08-09T09:55:48.495683: step 12682, loss 0.530639.
Train: 2018-08-09T09:55:51.537771: step 12683, loss 0.56208.
Train: 2018-08-09T09:55:54.596905: step 12684, loss 0.513498.
Train: 2018-08-09T09:55:57.676092: step 12685, loss 0.514367.
Train: 2018-08-09T09:56:00.730211: step 12686, loss 0.54676.
Train: 2018-08-09T09:56:03.764278: step 12687, loss 0.546068.
Train: 2018-08-09T09:56:06.807369: step 12688, loss 0.646929.
Train: 2018-08-09T09:56:09.884550: step 12689, loss 0.73136.
Train: 2018-08-09T09:56:12.971758: step 12690, loss 0.562292.
Test: 2018-08-09T09:56:33.769053: step 12690, loss 0.549485.
Train: 2018-08-09T09:56:36.871301: step 12691, loss 0.59572.
Train: 2018-08-09T09:56:39.946477: step 12692, loss 0.562549.
Train: 2018-08-09T09:56:43.028672: step 12693, loss 0.546739.
Train: 2018-08-09T09:56:46.096829: step 12694, loss 0.432798.
Train: 2018-08-09T09:56:49.139920: step 12695, loss 0.530106.
Train: 2018-08-09T09:56:52.181005: step 12696, loss 0.660488.
Train: 2018-08-09T09:56:55.244149: step 12697, loss 0.578927.
Train: 2018-08-09T09:56:58.316317: step 12698, loss 0.562625.
Train: 2018-08-09T09:57:01.368432: step 12699, loss 0.595224.
Train: 2018-08-09T09:57:04.428568: step 12700, loss 0.578923.
Test: 2018-08-09T09:57:25.218844: step 12700, loss 0.550479.
Train: 2018-08-09T09:57:30.021613: step 12701, loss 0.578916.
Train: 2018-08-09T09:57:33.075733: step 12702, loss 0.546403.
Train: 2018-08-09T09:57:36.138877: step 12703, loss 0.530181.
Train: 2018-08-09T09:57:39.180965: step 12704, loss 0.54643.
Train: 2018-08-09T09:57:42.264162: step 12705, loss 0.48147.
Train: 2018-08-09T09:57:45.338336: step 12706, loss 0.578913.
Train: 2018-08-09T09:57:48.403485: step 12707, loss 0.578922.
Train: 2018-08-09T09:57:51.528794: step 12708, loss 0.530056.
Train: 2018-08-09T09:57:54.574893: step 12709, loss 0.546324.
Train: 2018-08-09T09:57:57.661099: step 12710, loss 0.611548.
Test: 2018-08-09T09:58:18.450372: step 12710, loss 0.548529.
Train: 2018-08-09T09:58:21.496470: step 12711, loss 0.611629.
Train: 2018-08-09T09:58:24.602729: step 12712, loss 0.562573.
Train: 2018-08-09T09:58:27.662865: step 12713, loss 0.448317.
Train: 2018-08-09T09:58:30.719993: step 12714, loss 0.546274.
Train: 2018-08-09T09:58:33.774113: step 12715, loss 0.546149.
Train: 2018-08-09T09:58:36.853300: step 12716, loss 0.644873.
Train: 2018-08-09T09:58:39.904412: step 12717, loss 0.546348.
Train: 2018-08-09T09:58:43.014681: step 12718, loss 0.660922.
Train: 2018-08-09T09:58:46.061783: step 12719, loss 0.595317.
Train: 2018-08-09T09:58:49.133950: step 12720, loss 0.628.
Test: 2018-08-09T09:59:09.961325: step 12720, loss 0.550416.
Train: 2018-08-09T09:59:13.012437: step 12721, loss 0.529991.
Train: 2018-08-09T09:59:16.083602: step 12722, loss 0.595207.
Train: 2018-08-09T09:59:19.148752: step 12723, loss 0.611418.
Train: 2018-08-09T09:59:22.201869: step 12724, loss 0.562686.
Train: 2018-08-09T09:59:25.275040: step 12725, loss 0.498007.
Train: 2018-08-09T09:59:28.328157: step 12726, loss 0.514247.
Train: 2018-08-09T09:59:31.366234: step 12727, loss 0.530414.
Train: 2018-08-09T09:59:34.387267: step 12728, loss 0.546557.
Train: 2018-08-09T09:59:37.462442: step 12729, loss 0.562718.
Train: 2018-08-09T09:59:40.502525: step 12730, loss 0.54652.
Test: 2018-08-09T10:00:01.345943: step 12730, loss 0.549356.
Train: 2018-08-09T10:00:04.399060: step 12731, loss 0.562695.
Train: 2018-08-09T10:00:07.476241: step 12732, loss 0.546475.
Train: 2018-08-09T10:00:10.556430: step 12733, loss 0.562668.
Train: 2018-08-09T10:00:13.604534: step 12734, loss 0.627632.
Train: 2018-08-09T10:00:16.680713: step 12735, loss 0.611379.
Train: 2018-08-09T10:00:19.741852: step 12736, loss 0.578885.
Train: 2018-08-09T10:00:22.803993: step 12737, loss 0.481676.
Train: 2018-08-09T10:00:25.854103: step 12738, loss 0.530265.
Train: 2018-08-09T10:00:28.938303: step 12739, loss 0.595119.
Train: 2018-08-09T10:00:32.007463: step 12740, loss 0.56267.
Test: 2018-08-09T10:00:52.828821: step 12740, loss 0.546845.
Train: 2018-08-09T10:00:55.891965: step 12741, loss 0.530213.
Train: 2018-08-09T10:00:59.017274: step 12742, loss 0.59517.
Train: 2018-08-09T10:01:02.060365: step 12743, loss 0.578922.
Train: 2018-08-09T10:01:05.117493: step 12744, loss 0.562678.
Train: 2018-08-09T10:01:08.178632: step 12745, loss 0.595177.
Train: 2018-08-09T10:01:11.235760: step 12746, loss 0.5789.
Train: 2018-08-09T10:01:14.319960: step 12747, loss 0.54645.
Train: 2018-08-09T10:01:17.356032: step 12748, loss 0.595114.
Train: 2018-08-09T10:01:20.410152: step 12749, loss 0.546491.
Train: 2018-08-09T10:01:23.475302: step 12750, loss 0.514107.
Test: 2018-08-09T10:01:44.300671: step 12750, loss 0.548734.
Train: 2018-08-09T10:01:47.380860: step 12751, loss 0.546479.
Train: 2018-08-09T10:01:50.428964: step 12752, loss 0.514056.
Train: 2018-08-09T10:01:53.509153: step 12753, loss 0.578874.
Train: 2018-08-09T10:01:56.615412: step 12754, loss 0.513926.
Train: 2018-08-09T10:01:59.683569: step 12755, loss 0.595195.
Train: 2018-08-09T10:02:02.771780: step 12756, loss 0.54634.
Train: 2018-08-09T10:02:05.850967: step 12757, loss 0.464675.
Train: 2018-08-09T10:02:08.912105: step 12758, loss 0.480772.
Train: 2018-08-09T10:02:12.000316: step 12759, loss 0.595455.
Train: 2018-08-09T10:02:15.049423: step 12760, loss 0.52955.
Test: 2018-08-09T10:02:35.910888: step 12760, loss 0.548194.
Train: 2018-08-09T10:02:38.972026: step 12761, loss 0.529082.
Train: 2018-08-09T10:02:42.055224: step 12762, loss 0.528918.
Train: 2018-08-09T10:02:45.138421: step 12763, loss 0.512721.
Train: 2018-08-09T10:02:48.217608: step 12764, loss 0.630127.
Train: 2018-08-09T10:02:51.274736: step 12765, loss 0.545629.
Train: 2018-08-09T10:02:54.354925: step 12766, loss 0.562677.
Train: 2018-08-09T10:02:57.453163: step 12767, loss 0.510655.
Train: 2018-08-09T10:03:00.562429: step 12768, loss 0.561505.
Train: 2018-08-09T10:03:03.613542: step 12769, loss 0.630243.
Train: 2018-08-09T10:03:06.692728: step 12770, loss 0.510713.
Test: 2018-08-09T10:03:27.609340: step 12770, loss 0.546135.
Train: 2018-08-09T10:03:30.659449: step 12771, loss 0.582122.
Train: 2018-08-09T10:03:33.745654: step 12772, loss 0.650695.
Train: 2018-08-09T10:03:36.837876: step 12773, loss 0.528714.
Train: 2018-08-09T10:03:39.980230: step 12774, loss 0.477607.
Train: 2018-08-09T10:03:43.067438: step 12775, loss 0.477171.
Train: 2018-08-09T10:03:46.116545: step 12776, loss 0.562341.
Train: 2018-08-09T10:03:49.173673: step 12777, loss 0.595889.
Train: 2018-08-09T10:03:52.262886: step 12778, loss 0.545678.
Train: 2018-08-09T10:03:55.306980: step 12779, loss 0.562234.
Train: 2018-08-09T10:03:58.409228: step 12780, loss 0.528699.
Test: 2018-08-09T10:04:19.316816: step 12780, loss 0.546491.
Train: 2018-08-09T10:04:22.399010: step 12781, loss 0.630206.
Train: 2018-08-09T10:04:25.472181: step 12782, loss 0.47797.
Train: 2018-08-09T10:04:28.556381: step 12783, loss 0.680774.
Train: 2018-08-09T10:04:31.627546: step 12784, loss 0.528659.
Train: 2018-08-09T10:04:34.742829: step 12785, loss 0.56238.
Train: 2018-08-09T10:04:37.810986: step 12786, loss 0.461374.
Train: 2018-08-09T10:04:40.886162: step 12787, loss 0.478173.
Train: 2018-08-09T10:04:44.003450: step 12788, loss 0.562356.
Train: 2018-08-09T10:04:47.067597: step 12789, loss 0.579258.
Train: 2018-08-09T10:04:50.160821: step 12790, loss 0.579312.
Test: 2018-08-09T10:05:11.025295: step 12790, loss 0.552907.
Train: 2018-08-09T10:05:14.076406: step 12791, loss 0.562374.
Train: 2018-08-09T10:05:17.188681: step 12792, loss 0.61308.
Train: 2018-08-09T10:05:20.263858: step 12793, loss 0.477797.
Train: 2018-08-09T10:05:23.329006: step 12794, loss 0.613144.
Train: 2018-08-09T10:05:26.406187: step 12795, loss 0.443959.
Train: 2018-08-09T10:05:29.452286: step 12796, loss 0.579307.
Train: 2018-08-09T10:05:32.602662: step 12797, loss 0.613222.
Train: 2018-08-09T10:05:35.686862: step 12798, loss 0.51154.
Train: 2018-08-09T10:05:38.754017: step 12799, loss 0.545393.
Train: 2018-08-09T10:05:41.810142: step 12800, loss 0.630239.
Test: 2018-08-09T10:06:02.703693: step 12800, loss 0.548393.
Train: 2018-08-09T10:06:07.537544: step 12801, loss 0.596274.
Train: 2018-08-09T10:06:10.616731: step 12802, loss 0.528506.
Train: 2018-08-09T10:06:13.696921: step 12803, loss 0.545443.
Train: 2018-08-09T10:06:16.756054: step 12804, loss 0.579271.
Train: 2018-08-09T10:06:19.823209: step 12805, loss 0.596153.
Train: 2018-08-09T10:06:22.903398: step 12806, loss 0.629859.
Train: 2018-08-09T10:06:25.979577: step 12807, loss 0.562377.
Train: 2018-08-09T10:06:29.032694: step 12808, loss 0.579185.
Train: 2018-08-09T10:06:32.108873: step 12809, loss 0.612678.
Train: 2018-08-09T10:06:35.170011: step 12810, loss 0.545703.
Test: 2018-08-09T10:06:56.056544: step 12810, loss 0.54933.
Train: 2018-08-09T10:06:59.166812: step 12811, loss 0.595772.
Train: 2018-08-09T10:07:02.231962: step 12812, loss 0.412818.
Train: 2018-08-09T10:07:05.274050: step 12813, loss 0.645548.
Train: 2018-08-09T10:07:08.328170: step 12814, loss 0.562462.
Train: 2018-08-09T10:07:11.401340: step 12815, loss 0.562473.
Train: 2018-08-09T10:07:14.491556: step 12816, loss 0.64521.
Train: 2018-08-09T10:07:17.604834: step 12817, loss 0.595511.
Train: 2018-08-09T10:07:20.663967: step 12818, loss 0.611901.
Train: 2018-08-09T10:07:23.719090: step 12819, loss 0.628171.
Train: 2018-08-09T10:07:26.802287: step 12820, loss 0.578938.
Test: 2018-08-09T10:07:47.697843: step 12820, loss 0.54801.
Train: 2018-08-09T10:07:50.781041: step 12821, loss 0.497547.
Train: 2018-08-09T10:07:53.883288: step 12822, loss 0.465269.
Train: 2018-08-09T10:07:56.948438: step 12823, loss 0.578903.
Train: 2018-08-09T10:08:00.035646: step 12824, loss 0.465446.
Train: 2018-08-09T10:08:03.114833: step 12825, loss 0.530244.
Train: 2018-08-09T10:08:06.188003: step 12826, loss 0.660097.
Train: 2018-08-09T10:08:09.250145: step 12827, loss 0.578905.
Train: 2018-08-09T10:08:12.351390: step 12828, loss 0.562681.
Train: 2018-08-09T10:08:15.443611: step 12829, loss 0.530264.
Train: 2018-08-09T10:08:18.521795: step 12830, loss 0.530263.
Test: 2018-08-09T10:08:39.400306: step 12830, loss 0.546255.
Train: 2018-08-09T10:08:42.469465: step 12831, loss 0.643787.
Train: 2018-08-09T10:08:45.562690: step 12832, loss 0.611318.
Train: 2018-08-09T10:08:48.638868: step 12833, loss 0.497961.
Train: 2018-08-09T10:08:51.724071: step 12834, loss 0.514162.
Train: 2018-08-09T10:08:54.795236: step 12835, loss 0.62747.
Train: 2018-08-09T10:08:57.872418: step 12836, loss 0.514156.
Train: 2018-08-09T10:09:00.960629: step 12837, loss 0.627464.
Train: 2018-08-09T10:09:04.020764: step 12838, loss 0.530354.
Train: 2018-08-09T10:09:07.073882: step 12839, loss 0.59507.
Train: 2018-08-09T10:09:10.109954: step 12840, loss 0.56272.
Test: 2018-08-09T10:09:30.990470: step 12840, loss 0.549415.
Train: 2018-08-09T10:09:34.078680: step 12841, loss 0.627376.
Train: 2018-08-09T10:09:37.215019: step 12842, loss 0.611165.
Train: 2018-08-09T10:09:40.276158: step 12843, loss 0.578878.
Train: 2018-08-09T10:09:43.319248: step 12844, loss 0.611027.
Train: 2018-08-09T10:09:46.407459: step 12845, loss 0.610942.
Train: 2018-08-09T10:09:49.489654: step 12846, loss 0.546882.
Train: 2018-08-09T10:09:52.570846: step 12847, loss 0.690529.
Train: 2018-08-09T10:09:55.662064: step 12848, loss 0.578859.
Train: 2018-08-09T10:09:58.742254: step 12849, loss 0.499724.
Train: 2018-08-09T10:10:01.833472: step 12850, loss 0.563072.
Test: 2018-08-09T10:10:22.803226: step 12850, loss 0.550129.
Train: 2018-08-09T10:10:25.878401: step 12851, loss 0.594624.
Train: 2018-08-09T10:10:28.964607: step 12852, loss 0.594593.
Train: 2018-08-09T10:10:32.052817: step 12853, loss 0.641622.
Train: 2018-08-09T10:10:35.126991: step 12854, loss 0.516344.
Train: 2018-08-09T10:10:38.200161: step 12855, loss 0.563288.
Train: 2018-08-09T10:10:41.273332: step 12856, loss 0.516592.
Train: 2018-08-09T10:10:44.336476: step 12857, loss 0.563333.
Train: 2018-08-09T10:10:47.418671: step 12858, loss 0.56334.
Train: 2018-08-09T10:10:50.472791: step 12859, loss 0.625579.
Train: 2018-08-09T10:10:53.559999: step 12860, loss 0.594449.
Test: 2018-08-09T10:11:14.461571: step 12860, loss 0.551193.
Train: 2018-08-09T10:11:17.561813: step 12861, loss 0.56339.
Train: 2018-08-09T10:11:20.643005: step 12862, loss 0.470364.
Train: 2018-08-09T10:11:23.711163: step 12863, loss 0.578913.
Train: 2018-08-09T10:11:26.805389: step 12864, loss 0.501254.
Train: 2018-08-09T10:11:29.884576: step 12865, loss 0.563342.
Train: 2018-08-09T10:11:32.969779: step 12866, loss 0.563308.
Train: 2018-08-09T10:11:36.057989: step 12867, loss 0.578889.
Train: 2018-08-09T10:11:39.167256: step 12868, loss 0.563246.
Train: 2018-08-09T10:11:42.235413: step 12869, loss 0.50057.
Train: 2018-08-09T10:11:45.304574: step 12870, loss 0.578872.
Test: 2018-08-09T10:12:06.200130: step 12870, loss 0.55196.
Train: 2018-08-09T10:12:09.327444: step 12871, loss 0.594603.
Train: 2018-08-09T10:12:12.381564: step 12872, loss 0.563107.
Train: 2018-08-09T10:12:15.471780: step 12873, loss 0.626212.
Train: 2018-08-09T10:12:18.520886: step 12874, loss 0.578862.
Train: 2018-08-09T10:12:21.595060: step 12875, loss 0.594654.
Train: 2018-08-09T10:12:24.702321: step 12876, loss 0.594651.
Train: 2018-08-09T10:12:27.778500: step 12877, loss 0.531525.
Train: 2018-08-09T10:12:30.864705: step 12878, loss 0.563082.
Train: 2018-08-09T10:12:33.975977: step 12879, loss 0.563078.
Train: 2018-08-09T10:12:37.007036: step 12880, loss 0.484127.
Test: 2018-08-09T10:12:57.908608: step 12880, loss 0.550618.
Train: 2018-08-09T10:13:00.967741: step 12881, loss 0.54723.
Train: 2018-08-09T10:13:04.031888: step 12882, loss 0.483787.
Train: 2018-08-09T10:13:07.088013: step 12883, loss 0.642433.
Train: 2018-08-09T10:13:10.129098: step 12884, loss 0.626625.
Train: 2018-08-09T10:13:13.210290: step 12885, loss 0.562934.
Train: 2018-08-09T10:13:16.455920: step 12886, loss 0.610725.
Train: 2018-08-09T10:13:19.896066: step 12887, loss 0.499213.
Train: 2018-08-09T10:13:23.226922: step 12888, loss 0.483201.
Train: 2018-08-09T10:13:26.983911: step 12889, loss 0.578862.
Train: 2018-08-09T10:13:30.737892: step 12890, loss 0.530848.
