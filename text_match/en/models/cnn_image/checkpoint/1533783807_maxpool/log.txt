Train: 2018-08-09T11:03:34.331106: step 1, loss 0.708884.
Train: 2018-08-09T11:03:34.456108: step 2, loss 1.66116.
Train: 2018-08-09T11:03:34.549835: step 3, loss 0.906012.
Train: 2018-08-09T11:03:34.659185: step 4, loss 0.702683.
Train: 2018-08-09T11:03:34.768503: step 5, loss 0.70613.
Train: 2018-08-09T11:03:34.877883: step 6, loss 0.666766.
Train: 2018-08-09T11:03:34.987201: step 7, loss 0.5858.
Train: 2018-08-09T11:03:35.080961: step 8, loss 0.650364.
Train: 2018-08-09T11:03:35.190311: step 9, loss 0.497809.
Train: 2018-08-09T11:03:35.299658: step 10, loss 0.58869.
Test: 2018-08-09T11:03:35.911165: step 10, loss 0.687043.
Train: 2018-08-09T11:03:36.020545: step 11, loss 0.572359.
Train: 2018-08-09T11:03:36.129895: step 12, loss 0.530113.
Train: 2018-08-09T11:03:36.223623: step 13, loss 0.531163.
Train: 2018-08-09T11:03:36.332942: step 14, loss 0.50886.
Train: 2018-08-09T11:03:36.426670: step 15, loss 0.578725.
Train: 2018-08-09T11:03:36.536020: step 16, loss 0.487491.
Train: 2018-08-09T11:03:36.629777: step 17, loss 0.612269.
Train: 2018-08-09T11:03:36.739126: step 18, loss 0.75108.
Train: 2018-08-09T11:03:36.848475: step 19, loss 0.359089.
Train: 2018-08-09T11:03:36.942207: step 20, loss 0.582585.
Test: 2018-08-09T11:03:37.442090: step 20, loss 0.646191.
Train: 2018-08-09T11:03:37.535784: step 21, loss 0.600274.
Train: 2018-08-09T11:03:37.629541: step 22, loss 0.618021.
Train: 2018-08-09T11:03:37.738891: step 23, loss 0.534326.
Train: 2018-08-09T11:03:37.848209: step 24, loss 0.535555.
Train: 2018-08-09T11:03:37.944354: step 25, loss 0.541178.
Train: 2018-08-09T11:03:38.053703: step 26, loss 0.469092.
Train: 2018-08-09T11:03:38.147431: step 27, loss 0.453093.
Train: 2018-08-09T11:03:38.256810: step 28, loss 0.584593.
Train: 2018-08-09T11:03:38.350538: step 29, loss 0.710967.
Train: 2018-08-09T11:03:38.467617: step 30, loss 0.422156.
Test: 2018-08-09T11:03:38.951848: step 30, loss 0.656564.
Train: 2018-08-09T11:03:39.061196: step 31, loss 0.590544.
Train: 2018-08-09T11:03:39.154954: step 32, loss 0.486986.
Train: 2018-08-09T11:03:39.264303: step 33, loss 0.532713.
Train: 2018-08-09T11:03:39.358034: step 34, loss 0.554096.
Train: 2018-08-09T11:03:39.467351: step 35, loss 0.547537.
Train: 2018-08-09T11:03:39.576700: step 36, loss 0.632982.
Train: 2018-08-09T11:03:39.670427: step 37, loss 0.45061.
Train: 2018-08-09T11:03:39.779776: step 38, loss 0.388298.
Train: 2018-08-09T11:03:39.873535: step 39, loss 0.532207.
Train: 2018-08-09T11:03:39.976487: step 40, loss 0.606725.
Test: 2018-08-09T11:03:40.476340: step 40, loss 0.64276.
Train: 2018-08-09T11:03:40.570096: step 41, loss 0.577648.
Train: 2018-08-09T11:03:40.679446: step 42, loss 0.541586.
Train: 2018-08-09T11:03:40.782007: step 43, loss 0.543861.
Train: 2018-08-09T11:03:40.875735: step 44, loss 0.559414.
Train: 2018-08-09T11:03:40.985086: step 45, loss 0.588276.
Train: 2018-08-09T11:03:41.078813: step 46, loss 0.532878.
Train: 2018-08-09T11:03:41.188162: step 47, loss 0.60016.
Train: 2018-08-09T11:03:41.297511: step 48, loss 0.587105.
Train: 2018-08-09T11:03:41.391209: step 49, loss 0.65014.
Train: 2018-08-09T11:03:41.500559: step 50, loss 0.556801.
Test: 2018-08-09T11:03:42.000442: step 50, loss 0.636701.
Train: 2018-08-09T11:03:42.094202: step 51, loss 0.5955.
Train: 2018-08-09T11:03:42.203547: step 52, loss 0.51239.
Train: 2018-08-09T11:03:42.297275: step 53, loss 0.574729.
Train: 2018-08-09T11:03:42.406626: step 54, loss 0.518638.
Train: 2018-08-09T11:03:42.515974: step 55, loss 0.482506.
Train: 2018-08-09T11:03:42.609673: step 56, loss 0.639162.
Train: 2018-08-09T11:03:42.719022: step 57, loss 0.544223.
Train: 2018-08-09T11:03:42.828401: step 58, loss 0.527552.
Train: 2018-08-09T11:03:42.923471: step 59, loss 0.613839.
Train: 2018-08-09T11:03:43.032816: step 60, loss 0.577708.
Test: 2018-08-09T11:03:43.532670: step 60, loss 0.628626.
Train: 2018-08-09T11:03:43.626397: step 61, loss 0.539379.
Train: 2018-08-09T11:03:43.735776: step 62, loss 0.591251.
Train: 2018-08-09T11:03:43.845125: step 63, loss 0.628954.
Train: 2018-08-09T11:03:43.938823: step 64, loss 0.585742.
Train: 2018-08-09T11:03:44.048173: step 65, loss 0.489236.
Train: 2018-08-09T11:03:44.159135: step 66, loss 0.485168.
Train: 2018-08-09T11:03:44.252894: step 67, loss 0.713041.
Train: 2018-08-09T11:03:44.362243: step 68, loss 0.588145.
Train: 2018-08-09T11:03:44.471587: step 69, loss 0.617357.
Train: 2018-08-09T11:03:44.565320: step 70, loss 0.565378.
Test: 2018-08-09T11:03:45.065174: step 70, loss 0.626093.
Train: 2018-08-09T11:03:45.158925: step 71, loss 0.545273.
Train: 2018-08-09T11:03:45.268280: step 72, loss 0.542321.
Train: 2018-08-09T11:03:45.362007: step 73, loss 0.575231.
Train: 2018-08-09T11:03:45.471357: step 74, loss 0.560909.
Train: 2018-08-09T11:03:45.580710: step 75, loss 0.465894.
Train: 2018-08-09T11:03:45.674434: step 76, loss 0.523439.
Train: 2018-08-09T11:03:45.783783: step 77, loss 0.461679.
Train: 2018-08-09T11:03:45.893133: step 78, loss 0.646863.
Train: 2018-08-09T11:03:45.986830: step 79, loss 0.43997.
Train: 2018-08-09T11:03:46.096211: step 80, loss 0.570805.
Test: 2018-08-09T11:03:46.596064: step 80, loss 0.615725.
Train: 2018-08-09T11:03:46.689790: step 81, loss 0.605981.
Train: 2018-08-09T11:03:46.799139: step 82, loss 0.451082.
Train: 2018-08-09T11:03:46.892897: step 83, loss 0.504311.
Train: 2018-08-09T11:03:47.004620: step 84, loss 0.6112.
Train: 2018-08-09T11:03:47.113998: step 85, loss 0.505073.
Train: 2018-08-09T11:03:47.207727: step 86, loss 0.587732.
Train: 2018-08-09T11:03:47.317075: step 87, loss 0.459373.
Train: 2018-08-09T11:03:47.426425: step 88, loss 0.584924.
Train: 2018-08-09T11:03:47.520152: step 89, loss 0.472705.
Train: 2018-08-09T11:03:47.629472: step 90, loss 0.425996.
Test: 2018-08-09T11:03:48.129354: step 90, loss 0.591987.
Train: 2018-08-09T11:03:48.223082: step 91, loss 0.590399.
Train: 2018-08-09T11:03:48.332432: step 92, loss 0.551595.
Train: 2018-08-09T11:03:48.441810: step 93, loss 0.433031.
Train: 2018-08-09T11:03:48.535509: step 94, loss 0.531624.
Train: 2018-08-09T11:03:48.644888: step 95, loss 0.483029.
Train: 2018-08-09T11:03:48.754237: step 96, loss 0.441917.
Train: 2018-08-09T11:03:48.863586: step 97, loss 0.559884.
Train: 2018-08-09T11:03:48.959592: step 98, loss 0.628081.
Train: 2018-08-09T11:03:49.068942: step 99, loss 0.591934.
Train: 2018-08-09T11:03:49.162664: step 100, loss 0.516341.
Test: 2018-08-09T11:03:49.662522: step 100, loss 0.600471.
Train: 2018-08-09T11:03:50.318616: step 101, loss 0.51318.
Train: 2018-08-09T11:03:50.427996: step 102, loss 0.514968.
Train: 2018-08-09T11:03:50.521724: step 103, loss 0.508044.
Train: 2018-08-09T11:03:50.631073: step 104, loss 0.5183.
Train: 2018-08-09T11:03:50.740423: step 105, loss 0.564583.
Train: 2018-08-09T11:03:50.849773: step 106, loss 0.472724.
Train: 2018-08-09T11:03:50.944854: step 107, loss 0.530024.
Train: 2018-08-09T11:03:51.054173: step 108, loss 0.602673.
Train: 2018-08-09T11:03:51.163547: step 109, loss 0.657353.
Train: 2018-08-09T11:03:51.257251: step 110, loss 0.607373.
Test: 2018-08-09T11:03:51.757134: step 110, loss 0.597382.
Train: 2018-08-09T11:03:51.866512: step 111, loss 0.472208.
Train: 2018-08-09T11:03:51.975861: step 112, loss 0.520738.
Train: 2018-08-09T11:03:52.069588: step 113, loss 0.628158.
Train: 2018-08-09T11:03:52.178938: step 114, loss 0.542713.
Train: 2018-08-09T11:03:52.288258: step 115, loss 0.541939.
Train: 2018-08-09T11:03:52.397634: step 116, loss 0.508201.
Train: 2018-08-09T11:03:52.506956: step 117, loss 0.550457.
Train: 2018-08-09T11:03:52.600714: step 118, loss 0.521198.
Train: 2018-08-09T11:03:52.710033: step 119, loss 0.554005.
Train: 2018-08-09T11:03:52.819412: step 120, loss 0.481666.
Test: 2018-08-09T11:03:53.305384: step 120, loss 0.567303.
Train: 2018-08-09T11:03:53.423414: step 121, loss 0.582065.
Train: 2018-08-09T11:03:53.517112: step 122, loss 0.592299.
Train: 2018-08-09T11:03:53.626461: step 123, loss 0.566885.
Train: 2018-08-09T11:03:53.735809: step 124, loss 0.555565.
Train: 2018-08-09T11:03:53.829573: step 125, loss 0.474923.
Train: 2018-08-09T11:03:53.938918: step 126, loss 0.604381.
Train: 2018-08-09T11:03:54.048237: step 127, loss 0.518219.
Train: 2018-08-09T11:03:54.141994: step 128, loss 0.533926.
Train: 2018-08-09T11:03:54.251314: step 129, loss 0.511272.
Train: 2018-08-09T11:03:54.360693: step 130, loss 0.546237.
Test: 2018-08-09T11:03:54.860546: step 130, loss 0.565614.
Train: 2018-08-09T11:03:54.962501: step 131, loss 0.544186.
Train: 2018-08-09T11:03:55.056198: step 132, loss 0.62887.
Train: 2018-08-09T11:03:55.165582: step 133, loss 0.514453.
Train: 2018-08-09T11:03:55.274923: step 134, loss 0.619615.
Train: 2018-08-09T11:03:55.384270: step 135, loss 0.561811.
Train: 2018-08-09T11:03:55.478005: step 136, loss 0.634887.
Train: 2018-08-09T11:03:55.587354: step 137, loss 0.467249.
Train: 2018-08-09T11:03:55.696698: step 138, loss 0.653952.
Train: 2018-08-09T11:03:55.790402: step 139, loss 0.522541.
Train: 2018-08-09T11:03:55.899751: step 140, loss 0.499124.
Test: 2018-08-09T11:03:56.399632: step 140, loss 0.569084.
Train: 2018-08-09T11:03:56.493361: step 141, loss 0.577987.
Train: 2018-08-09T11:03:56.602740: step 142, loss 0.651306.
Train: 2018-08-09T11:03:56.712090: step 143, loss 0.5041.
Train: 2018-08-09T11:03:56.821441: step 144, loss 0.53146.
Train: 2018-08-09T11:03:56.915167: step 145, loss 0.553739.
Train: 2018-08-09T11:03:57.024515: step 146, loss 0.47746.
Train: 2018-08-09T11:03:57.133864: step 147, loss 0.595072.
Train: 2018-08-09T11:03:57.243214: step 148, loss 0.643143.
Train: 2018-08-09T11:03:57.336942: step 149, loss 0.543555.
Train: 2018-08-09T11:03:57.446295: step 150, loss 0.726321.
Test: 2018-08-09T11:03:57.948774: step 150, loss 0.570707.
Train: 2018-08-09T11:03:58.042499: step 151, loss 0.533186.
Train: 2018-08-09T11:03:58.151847: step 152, loss 0.675569.
Train: 2018-08-09T11:03:58.261228: step 153, loss 0.553683.
Train: 2018-08-09T11:03:58.370577: step 154, loss 0.621918.
Train: 2018-08-09T11:03:58.479926: step 155, loss 0.520472.
Train: 2018-08-09T11:03:58.573654: step 156, loss 0.512221.
Train: 2018-08-09T11:03:58.683004: step 157, loss 0.496987.
Train: 2018-08-09T11:03:58.792353: step 158, loss 0.617653.
Train: 2018-08-09T11:03:58.901703: step 159, loss 0.52366.
Train: 2018-08-09T11:03:58.995429: step 160, loss 0.534868.
Test: 2018-08-09T11:03:59.495283: step 160, loss 0.558881.
Train: 2018-08-09T11:03:59.604660: step 161, loss 0.638201.
Train: 2018-08-09T11:03:59.698389: step 162, loss 0.525443.
Train: 2018-08-09T11:03:59.807738: step 163, loss 0.520951.
Train: 2018-08-09T11:03:59.918469: step 164, loss 0.596719.
Train: 2018-08-09T11:04:00.027819: step 165, loss 0.697681.
Train: 2018-08-09T11:04:00.137198: step 166, loss 0.470755.
Train: 2018-08-09T11:04:00.246517: step 167, loss 0.499153.
Train: 2018-08-09T11:04:00.340276: step 168, loss 0.5918.
Train: 2018-08-09T11:04:00.449624: step 169, loss 0.429631.
Train: 2018-08-09T11:04:00.558973: step 170, loss 0.667331.
Test: 2018-08-09T11:04:01.043205: step 170, loss 0.552361.
Train: 2018-08-09T11:04:01.152586: step 171, loss 0.549526.
Train: 2018-08-09T11:04:01.261933: step 172, loss 0.480896.
Train: 2018-08-09T11:04:01.371283: step 173, loss 0.670762.
Train: 2018-08-09T11:04:01.480630: step 174, loss 0.535811.
Train: 2018-08-09T11:04:01.574330: step 175, loss 0.674171.
Train: 2018-08-09T11:04:01.683708: step 176, loss 0.488857.
Train: 2018-08-09T11:04:01.793057: step 177, loss 0.545327.
Train: 2018-08-09T11:04:01.902407: step 178, loss 0.56291.
Train: 2018-08-09T11:04:01.998478: step 179, loss 0.608452.
Train: 2018-08-09T11:04:02.107827: step 180, loss 0.531489.
Test: 2018-08-09T11:04:02.607680: step 180, loss 0.558318.
Train: 2018-08-09T11:04:02.717029: step 181, loss 0.619232.
Train: 2018-08-09T11:04:02.810787: step 182, loss 0.601712.
Train: 2018-08-09T11:04:02.935753: step 183, loss 0.574316.
Train: 2018-08-09T11:04:03.029456: step 184, loss 0.527716.
Train: 2018-08-09T11:04:03.138835: step 185, loss 0.669662.
Train: 2018-08-09T11:04:03.248183: step 186, loss 0.556135.
Train: 2018-08-09T11:04:03.357504: step 187, loss 0.678031.
Train: 2018-08-09T11:04:03.451231: step 188, loss 0.63966.
Train: 2018-08-09T11:04:03.560610: step 189, loss 0.493923.
Train: 2018-08-09T11:04:03.669960: step 190, loss 0.568195.
Test: 2018-08-09T11:04:04.170410: step 190, loss 0.558622.
Train: 2018-08-09T11:04:04.279788: step 191, loss 0.550207.
Train: 2018-08-09T11:04:04.389139: step 192, loss 0.560717.
Train: 2018-08-09T11:04:04.498481: step 193, loss 0.57217.
Train: 2018-08-09T11:04:04.607836: step 194, loss 0.583937.
Train: 2018-08-09T11:04:04.717185: step 195, loss 0.490891.
Train: 2018-08-09T11:04:04.842127: step 196, loss 0.59446.
Train: 2018-08-09T11:04:04.951505: step 197, loss 0.51425.
Train: 2018-08-09T11:04:05.045233: step 198, loss 0.656156.
Train: 2018-08-09T11:04:05.154582: step 199, loss 0.603322.
Train: 2018-08-09T11:04:05.263931: step 200, loss 0.480851.
Test: 2018-08-09T11:04:05.763784: step 200, loss 0.549485.
Train: 2018-08-09T11:04:06.357423: step 201, loss 0.551356.
Train: 2018-08-09T11:04:06.466773: step 202, loss 0.556612.
Train: 2018-08-09T11:04:06.560501: step 203, loss 0.493012.
Train: 2018-08-09T11:04:06.669821: step 204, loss 0.45283.
Train: 2018-08-09T11:04:06.779199: step 205, loss 0.524969.
Train: 2018-08-09T11:04:06.888551: step 206, loss 0.549015.
Train: 2018-08-09T11:04:06.997897: step 207, loss 0.614192.
Train: 2018-08-09T11:04:07.107247: step 208, loss 0.489291.
Train: 2018-08-09T11:04:07.200975: step 209, loss 0.584668.
Train: 2018-08-09T11:04:07.310325: step 210, loss 0.511657.
Test: 2018-08-09T11:04:07.810178: step 210, loss 0.548875.
Train: 2018-08-09T11:04:07.919556: step 211, loss 0.523119.
Train: 2018-08-09T11:04:08.028905: step 212, loss 0.570285.
Train: 2018-08-09T11:04:08.138255: step 213, loss 0.526043.
Train: 2018-08-09T11:04:08.247572: step 214, loss 0.578686.
Train: 2018-08-09T11:04:08.341332: step 215, loss 0.595125.
Train: 2018-08-09T11:04:08.450683: step 216, loss 0.498481.
Train: 2018-08-09T11:04:08.560000: step 217, loss 0.650054.
Train: 2018-08-09T11:04:08.669529: step 218, loss 0.618896.
Train: 2018-08-09T11:04:08.778912: step 219, loss 0.570068.
Train: 2018-08-09T11:04:08.888258: step 220, loss 0.571089.
Test: 2018-08-09T11:04:09.374793: step 220, loss 0.559008.
Train: 2018-08-09T11:04:09.484142: step 221, loss 0.597329.
Train: 2018-08-09T11:04:09.593492: step 222, loss 0.553592.
Train: 2018-08-09T11:04:09.702873: step 223, loss 0.602346.
Train: 2018-08-09T11:04:09.812191: step 224, loss 0.588522.
Train: 2018-08-09T11:04:09.921569: step 225, loss 0.539397.
Train: 2018-08-09T11:04:10.030919: step 226, loss 0.54431.
Train: 2018-08-09T11:04:10.140268: step 227, loss 0.64745.
Train: 2018-08-09T11:04:10.249616: step 228, loss 0.658041.
Train: 2018-08-09T11:04:10.358966: step 229, loss 0.595658.
Train: 2018-08-09T11:04:10.452695: step 230, loss 0.554854.
Test: 2018-08-09T11:04:10.958245: step 230, loss 0.557959.
Train: 2018-08-09T11:04:11.063394: step 231, loss 0.572597.
Train: 2018-08-09T11:04:11.172746: step 232, loss 0.559244.
Train: 2018-08-09T11:04:11.266471: step 233, loss 0.577679.
Train: 2018-08-09T11:04:11.375823: step 234, loss 0.635171.
Train: 2018-08-09T11:04:11.485140: step 235, loss 0.534936.
Train: 2018-08-09T11:04:11.594519: step 236, loss 0.59589.
Train: 2018-08-09T11:04:11.703862: step 237, loss 0.663162.
Train: 2018-08-09T11:04:11.813217: step 238, loss 0.569306.
Train: 2018-08-09T11:04:11.922569: step 239, loss 0.519428.
Train: 2018-08-09T11:04:12.031886: step 240, loss 0.534429.
Test: 2018-08-09T11:04:12.516148: step 240, loss 0.550385.
Train: 2018-08-09T11:04:12.625496: step 241, loss 0.545668.
Train: 2018-08-09T11:04:12.750497: step 242, loss 0.576242.
Train: 2018-08-09T11:04:12.844225: step 243, loss 0.614571.
Train: 2018-08-09T11:04:12.953568: step 244, loss 0.56668.
Train: 2018-08-09T11:04:13.062923: step 245, loss 0.433144.
Train: 2018-08-09T11:04:13.172275: step 246, loss 0.570076.
Train: 2018-08-09T11:04:13.281621: step 247, loss 0.600658.
Train: 2018-08-09T11:04:13.390941: step 248, loss 0.533442.
Train: 2018-08-09T11:04:13.500290: step 249, loss 0.52991.
Train: 2018-08-09T11:04:13.609639: step 250, loss 0.554407.
Test: 2018-08-09T11:04:14.111974: step 250, loss 0.548787.
Train: 2018-08-09T11:04:14.205733: step 251, loss 0.468539.
Train: 2018-08-09T11:04:14.315081: step 252, loss 0.551559.
Train: 2018-08-09T11:04:14.424430: step 253, loss 0.581896.
Train: 2018-08-09T11:04:14.530802: step 254, loss 0.632897.
Train: 2018-08-09T11:04:14.640151: step 255, loss 0.50384.
Train: 2018-08-09T11:04:14.749500: step 256, loss 0.503913.
Train: 2018-08-09T11:04:14.858819: step 257, loss 0.538767.
Train: 2018-08-09T11:04:14.968199: step 258, loss 0.458203.
Train: 2018-08-09T11:04:15.077548: step 259, loss 0.539602.
Train: 2018-08-09T11:04:15.186898: step 260, loss 0.550591.
Test: 2018-08-09T11:04:15.686755: step 260, loss 0.546602.
Train: 2018-08-09T11:04:15.796129: step 261, loss 0.466361.
Train: 2018-08-09T11:04:15.906748: step 262, loss 0.519067.
Train: 2018-08-09T11:04:16.004643: step 263, loss 0.562102.
Train: 2018-08-09T11:04:16.113993: step 264, loss 0.675966.
Train: 2018-08-09T11:04:16.223336: step 265, loss 0.521846.
Train: 2018-08-09T11:04:16.332691: step 266, loss 0.610047.
Train: 2018-08-09T11:04:16.442040: step 267, loss 0.526474.
Train: 2018-08-09T11:04:16.551360: step 268, loss 0.562317.
Train: 2018-08-09T11:04:16.660741: step 269, loss 0.538153.
Train: 2018-08-09T11:04:16.770088: step 270, loss 0.651421.
Test: 2018-08-09T11:04:17.269941: step 270, loss 0.554613.
Train: 2018-08-09T11:04:17.379313: step 271, loss 0.573888.
Train: 2018-08-09T11:04:17.473048: step 272, loss 0.532272.
Train: 2018-08-09T11:04:17.582397: step 273, loss 0.525689.
Train: 2018-08-09T11:04:17.691746: step 274, loss 0.599558.
Train: 2018-08-09T11:04:17.801095: step 275, loss 0.542906.
Train: 2018-08-09T11:04:17.910447: step 276, loss 0.62879.
Train: 2018-08-09T11:04:18.019794: step 277, loss 0.532197.
Train: 2018-08-09T11:04:18.129143: step 278, loss 0.543343.
Train: 2018-08-09T11:04:18.238492: step 279, loss 0.624687.
Train: 2018-08-09T11:04:18.347841: step 280, loss 0.548275.
Test: 2018-08-09T11:04:18.832073: step 280, loss 0.552748.
Train: 2018-08-09T11:04:18.942836: step 281, loss 0.628167.
Train: 2018-08-09T11:04:19.052155: step 282, loss 0.548623.
Train: 2018-08-09T11:04:19.161534: step 283, loss 0.588191.
Train: 2018-08-09T11:04:19.270883: step 284, loss 0.532365.
Train: 2018-08-09T11:04:19.380232: step 285, loss 0.524479.
Train: 2018-08-09T11:04:19.489582: step 286, loss 0.51958.
Train: 2018-08-09T11:04:19.598931: step 287, loss 0.542206.
Train: 2018-08-09T11:04:19.708250: step 288, loss 0.492244.
Train: 2018-08-09T11:04:19.817630: step 289, loss 0.45784.
Train: 2018-08-09T11:04:19.926949: step 290, loss 0.556853.
Test: 2018-08-09T11:04:20.426831: step 290, loss 0.548151.
Train: 2018-08-09T11:04:20.536180: step 291, loss 0.611913.
Train: 2018-08-09T11:04:20.645530: step 292, loss 0.514902.
Train: 2018-08-09T11:04:20.754909: step 293, loss 0.554258.
Train: 2018-08-09T11:04:20.864257: step 294, loss 0.503198.
Train: 2018-08-09T11:04:20.975911: step 295, loss 0.485392.
Train: 2018-08-09T11:04:21.069610: step 296, loss 0.509005.
Train: 2018-08-09T11:04:21.194582: step 297, loss 0.57182.
Train: 2018-08-09T11:04:21.303961: step 298, loss 0.58828.
Train: 2018-08-09T11:04:21.413309: step 299, loss 0.553942.
Train: 2018-08-09T11:04:21.522657: step 300, loss 0.594592.
Test: 2018-08-09T11:04:22.006889: step 300, loss 0.550315.
Train: 2018-08-09T11:04:22.787984: step 301, loss 0.487179.
Train: 2018-08-09T11:04:22.897334: step 302, loss 0.584303.
Train: 2018-08-09T11:04:23.008091: step 303, loss 0.582127.
Train: 2018-08-09T11:04:23.117441: step 304, loss 0.666931.
Train: 2018-08-09T11:04:23.226794: step 305, loss 0.502575.
Train: 2018-08-09T11:04:23.336139: step 306, loss 0.470101.
Train: 2018-08-09T11:04:23.445488: step 307, loss 0.530948.
Train: 2018-08-09T11:04:23.554837: step 308, loss 0.564517.
Train: 2018-08-09T11:04:23.664157: step 309, loss 0.472394.
Train: 2018-08-09T11:04:23.773539: step 310, loss 0.661529.
Test: 2018-08-09T11:04:24.269455: step 310, loss 0.548598.
Train: 2018-08-09T11:04:24.378803: step 311, loss 0.565825.
Train: 2018-08-09T11:04:24.488152: step 312, loss 0.535517.
Train: 2018-08-09T11:04:24.597531: step 313, loss 0.607449.
Train: 2018-08-09T11:04:24.706880: step 314, loss 0.589829.
Train: 2018-08-09T11:04:24.816230: step 315, loss 0.595669.
Train: 2018-08-09T11:04:24.916275: step 316, loss 0.593601.
Train: 2018-08-09T11:04:25.025656: step 317, loss 0.576046.
Train: 2018-08-09T11:04:25.135004: step 318, loss 0.619125.
Train: 2018-08-09T11:04:25.244354: step 319, loss 0.633376.
Train: 2018-08-09T11:04:25.353703: step 320, loss 0.540221.
Test: 2018-08-09T11:04:25.853557: step 320, loss 0.561996.
Train: 2018-08-09T11:04:25.962934: step 321, loss 0.594113.
Train: 2018-08-09T11:04:26.072254: step 322, loss 0.541861.
Train: 2018-08-09T11:04:26.181633: step 323, loss 0.587209.
Train: 2018-08-09T11:04:26.290983: step 324, loss 0.545497.
Train: 2018-08-09T11:04:26.400332: step 325, loss 0.564175.
Train: 2018-08-09T11:04:26.509650: step 326, loss 0.578136.
Train: 2018-08-09T11:04:26.619029: step 327, loss 0.525696.
Train: 2018-08-09T11:04:26.712758: step 328, loss 0.626982.
Train: 2018-08-09T11:04:26.822107: step 329, loss 0.492294.
Train: 2018-08-09T11:04:26.931456: step 330, loss 0.515531.
Test: 2018-08-09T11:04:27.431308: step 330, loss 0.547289.
Train: 2018-08-09T11:04:27.540658: step 331, loss 0.53712.
Train: 2018-08-09T11:04:27.650007: step 332, loss 0.517793.
Train: 2018-08-09T11:04:27.759386: step 333, loss 0.581288.
Train: 2018-08-09T11:04:27.868735: step 334, loss 0.479208.
Train: 2018-08-09T11:04:27.980439: step 335, loss 0.500039.
Train: 2018-08-09T11:04:28.074138: step 336, loss 0.575703.
Train: 2018-08-09T11:04:28.183486: step 337, loss 0.403247.
Train: 2018-08-09T11:04:28.292866: step 338, loss 0.441892.
Train: 2018-08-09T11:04:28.402216: step 339, loss 0.601274.
Train: 2018-08-09T11:04:28.511568: step 340, loss 0.538662.
Test: 2018-08-09T11:04:29.011451: step 340, loss 0.546367.
Train: 2018-08-09T11:04:29.120766: step 341, loss 0.524714.
Train: 2018-08-09T11:04:29.230145: step 342, loss 0.435635.
Train: 2018-08-09T11:04:29.339495: step 343, loss 0.561355.
Train: 2018-08-09T11:04:29.448843: step 344, loss 0.647848.
Train: 2018-08-09T11:04:29.558192: step 345, loss 0.586641.
Train: 2018-08-09T11:04:29.667543: step 346, loss 0.579638.
Train: 2018-08-09T11:04:29.776891: step 347, loss 0.536098.
Train: 2018-08-09T11:04:29.886244: step 348, loss 0.591044.
Train: 2018-08-09T11:04:29.997885: step 349, loss 0.50519.
Train: 2018-08-09T11:04:30.107234: step 350, loss 0.557994.
Test: 2018-08-09T11:04:30.607116: step 350, loss 0.555892.
Train: 2018-08-09T11:04:30.700837: step 351, loss 0.653151.
Train: 2018-08-09T11:04:30.810193: step 352, loss 0.456941.
Train: 2018-08-09T11:04:30.919544: step 353, loss 0.535554.
Train: 2018-08-09T11:04:31.028892: step 354, loss 0.646573.
Train: 2018-08-09T11:04:31.138240: step 355, loss 0.535163.
Train: 2018-08-09T11:04:31.247589: step 356, loss 0.570989.
Train: 2018-08-09T11:04:31.356909: step 357, loss 0.466719.
Train: 2018-08-09T11:04:31.466287: step 358, loss 0.403433.
Train: 2018-08-09T11:04:31.575607: step 359, loss 0.528189.
Train: 2018-08-09T11:04:31.684986: step 360, loss 0.530784.
Test: 2018-08-09T11:04:32.171452: step 360, loss 0.553184.
Train: 2018-08-09T11:04:32.280830: step 361, loss 0.577268.
Train: 2018-08-09T11:04:32.405771: step 362, loss 0.624477.
Train: 2018-08-09T11:04:32.499532: step 363, loss 0.423127.
Train: 2018-08-09T11:04:32.608877: step 364, loss 0.519207.
Train: 2018-08-09T11:04:32.718227: step 365, loss 0.422362.
Train: 2018-08-09T11:04:32.827576: step 366, loss 0.501161.
Train: 2018-08-09T11:04:32.936926: step 367, loss 0.502783.
Train: 2018-08-09T11:04:33.046245: step 368, loss 0.492091.
Train: 2018-08-09T11:04:33.155624: step 369, loss 0.540615.
Train: 2018-08-09T11:04:33.264973: step 370, loss 0.502058.
Test: 2018-08-09T11:04:33.749204: step 370, loss 0.546429.
Train: 2018-08-09T11:04:33.858553: step 371, loss 0.548627.
Train: 2018-08-09T11:04:33.968525: step 372, loss 0.554312.
Train: 2018-08-09T11:04:34.077874: step 373, loss 0.576971.
Train: 2018-08-09T11:04:34.187224: step 374, loss 0.547514.
Train: 2018-08-09T11:04:34.296572: step 375, loss 0.544705.
Train: 2018-08-09T11:04:34.405893: step 376, loss 0.571116.
Train: 2018-08-09T11:04:34.515274: step 377, loss 0.510271.
Train: 2018-08-09T11:04:34.608970: step 378, loss 0.623545.
Train: 2018-08-09T11:04:34.718348: step 379, loss 0.518785.
Train: 2018-08-09T11:04:34.827699: step 380, loss 0.602044.
Test: 2018-08-09T11:04:35.327552: step 380, loss 0.556487.
Train: 2018-08-09T11:04:35.436929: step 381, loss 0.52253.
Train: 2018-08-09T11:04:35.546279: step 382, loss 0.531611.
Train: 2018-08-09T11:04:35.655628: step 383, loss 0.508973.
Train: 2018-08-09T11:04:35.764980: step 384, loss 0.532811.
Train: 2018-08-09T11:04:35.858704: step 385, loss 0.523705.
Train: 2018-08-09T11:04:35.968054: step 386, loss 0.481893.
Train: 2018-08-09T11:04:36.077403: step 387, loss 0.494441.
Train: 2018-08-09T11:04:36.186723: step 388, loss 0.516685.
Train: 2018-08-09T11:04:36.296103: step 389, loss 0.440128.
Train: 2018-08-09T11:04:36.421072: step 390, loss 0.579547.
Test: 2018-08-09T11:04:36.907623: step 390, loss 0.550102.
Train: 2018-08-09T11:04:37.016974: step 391, loss 0.441647.
Train: 2018-08-09T11:04:37.126352: step 392, loss 0.641553.
Train: 2018-08-09T11:04:37.235701: step 393, loss 0.428311.
Train: 2018-08-09T11:04:37.345053: step 394, loss 0.583719.
Train: 2018-08-09T11:04:37.438779: step 395, loss 0.564809.
Train: 2018-08-09T11:04:37.548098: step 396, loss 0.696515.
Train: 2018-08-09T11:04:37.657478: step 397, loss 0.46591.
Train: 2018-08-09T11:04:37.766797: step 398, loss 0.466823.
Train: 2018-08-09T11:04:37.876176: step 399, loss 0.48372.
Train: 2018-08-09T11:04:37.985494: step 400, loss 0.638112.
Test: 2018-08-09T11:04:38.469755: step 400, loss 0.551439.
Train: 2018-08-09T11:04:39.060261: step 401, loss 0.49045.
Train: 2018-08-09T11:04:39.169577: step 402, loss 0.557951.
Train: 2018-08-09T11:04:39.278956: step 403, loss 0.592447.
Train: 2018-08-09T11:04:39.388306: step 404, loss 0.592674.
Train: 2018-08-09T11:04:39.497625: step 405, loss 0.574008.
Train: 2018-08-09T11:04:39.591383: step 406, loss 0.544478.
Train: 2018-08-09T11:04:39.700703: step 407, loss 0.517862.
Train: 2018-08-09T11:04:39.810082: step 408, loss 0.643933.
Train: 2018-08-09T11:04:39.919431: step 409, loss 0.537337.
Train: 2018-08-09T11:04:40.028779: step 410, loss 0.422882.
Test: 2018-08-09T11:04:40.513036: step 410, loss 0.549687.
Train: 2018-08-09T11:04:40.622390: step 411, loss 0.693534.
Train: 2018-08-09T11:04:40.731740: step 412, loss 0.537461.
Train: 2018-08-09T11:04:40.841089: step 413, loss 0.546926.
Train: 2018-08-09T11:04:40.950408: step 414, loss 0.648028.
Train: 2018-08-09T11:04:41.044169: step 415, loss 0.678252.
Train: 2018-08-09T11:04:41.153519: step 416, loss 0.526255.
Train: 2018-08-09T11:04:41.265519: step 417, loss 0.53358.
Train: 2018-08-09T11:04:41.374873: step 418, loss 0.491637.
Train: 2018-08-09T11:04:41.484188: step 419, loss 0.708382.
Train: 2018-08-09T11:04:41.577916: step 420, loss 0.529107.
Test: 2018-08-09T11:04:42.077800: step 420, loss 0.558192.
Train: 2018-08-09T11:04:42.187178: step 421, loss 0.505856.
Train: 2018-08-09T11:04:42.296527: step 422, loss 0.602423.
Train: 2018-08-09T11:04:42.390224: step 423, loss 0.587835.
Train: 2018-08-09T11:04:42.499573: step 424, loss 0.504458.
Train: 2018-08-09T11:04:42.608953: step 425, loss 0.520146.
Train: 2018-08-09T11:04:42.718303: step 426, loss 0.526512.
Train: 2018-08-09T11:04:42.827652: step 427, loss 0.585562.
Train: 2018-08-09T11:04:42.935750: step 428, loss 0.560505.
Train: 2018-08-09T11:04:43.029508: step 429, loss 0.527908.
Train: 2018-08-09T11:04:43.138858: step 430, loss 0.518645.
Test: 2018-08-09T11:04:43.638709: step 430, loss 0.548996.
Train: 2018-08-09T11:04:43.732468: step 431, loss 0.567207.
Train: 2018-08-09T11:04:43.841816: step 432, loss 0.603985.
Train: 2018-08-09T11:04:43.951170: step 433, loss 0.54303.
Train: 2018-08-09T11:04:44.060515: step 434, loss 0.591444.
Train: 2018-08-09T11:04:44.154243: step 435, loss 0.513558.
Train: 2018-08-09T11:04:44.263563: step 436, loss 0.492604.
Train: 2018-08-09T11:04:44.372942: step 437, loss 0.52188.
Train: 2018-08-09T11:04:44.482291: step 438, loss 0.597314.
Train: 2018-08-09T11:04:44.591640: step 439, loss 0.605632.
Train: 2018-08-09T11:04:44.685368: step 440, loss 0.50548.
Test: 2018-08-09T11:04:45.183217: step 440, loss 0.55332.
Train: 2018-08-09T11:04:45.292596: step 441, loss 0.610558.
Train: 2018-08-09T11:04:45.401945: step 442, loss 0.49276.
Train: 2018-08-09T11:04:45.511295: step 443, loss 0.434174.
Train: 2018-08-09T11:04:45.604993: step 444, loss 0.42107.
Train: 2018-08-09T11:04:45.714371: step 445, loss 0.473044.
Train: 2018-08-09T11:04:45.823723: step 446, loss 0.592697.
Train: 2018-08-09T11:04:45.933071: step 447, loss 0.673926.
Train: 2018-08-09T11:04:46.042389: step 448, loss 0.506401.
Train: 2018-08-09T11:04:46.136116: step 449, loss 0.512832.
Train: 2018-08-09T11:04:46.245467: step 450, loss 0.431358.
Test: 2018-08-09T11:04:46.745350: step 450, loss 0.547534.
Train: 2018-08-09T11:04:46.854697: step 451, loss 0.654783.
Train: 2018-08-09T11:04:46.948427: step 452, loss 0.500348.
Train: 2018-08-09T11:04:47.057805: step 453, loss 0.544787.
Train: 2018-08-09T11:04:47.167125: step 454, loss 0.537758.
Train: 2018-08-09T11:04:47.276473: step 455, loss 0.536428.
Train: 2018-08-09T11:04:47.385853: step 456, loss 0.566603.
Train: 2018-08-09T11:04:47.495222: step 457, loss 0.54769.
Train: 2018-08-09T11:04:47.588930: step 458, loss 0.542082.
Train: 2018-08-09T11:04:47.698281: step 459, loss 0.647594.
Train: 2018-08-09T11:04:47.807598: step 460, loss 0.488417.
Test: 2018-08-09T11:04:48.308986: step 460, loss 0.549639.
Train: 2018-08-09T11:04:48.402696: step 461, loss 0.519738.
Train: 2018-08-09T11:04:48.512046: step 462, loss 0.516108.
Train: 2018-08-09T11:04:48.621395: step 463, loss 0.528751.
Train: 2018-08-09T11:04:48.730746: step 464, loss 0.522607.
Train: 2018-08-09T11:04:48.840088: step 465, loss 0.56599.
Train: 2018-08-09T11:04:48.949437: step 466, loss 0.529753.
Train: 2018-08-09T11:04:49.043171: step 467, loss 0.723229.
Train: 2018-08-09T11:04:49.152514: step 468, loss 0.458272.
Train: 2018-08-09T11:04:49.261869: step 469, loss 0.563976.
Train: 2018-08-09T11:04:49.371197: step 470, loss 0.597053.
Test: 2018-08-09T11:04:49.855480: step 470, loss 0.549468.
Train: 2018-08-09T11:04:49.966341: step 471, loss 0.592639.
Train: 2018-08-09T11:04:50.075689: step 472, loss 0.545905.
Train: 2018-08-09T11:04:50.169417: step 473, loss 0.57624.
Train: 2018-08-09T11:04:50.278766: step 474, loss 0.608133.
Train: 2018-08-09T11:04:50.388116: step 475, loss 0.525103.
Train: 2018-08-09T11:04:50.497468: step 476, loss 0.550499.
Train: 2018-08-09T11:04:50.591193: step 477, loss 0.467322.
Train: 2018-08-09T11:04:50.700512: step 478, loss 0.561685.
Train: 2018-08-09T11:04:50.809891: step 479, loss 0.476719.
Train: 2018-08-09T11:04:50.919240: step 480, loss 0.636513.
Test: 2018-08-09T11:04:51.419107: step 480, loss 0.547746.
Train: 2018-08-09T11:04:51.512853: step 481, loss 0.578478.
Train: 2018-08-09T11:04:51.622203: step 482, loss 0.486702.
Train: 2018-08-09T11:04:51.731548: step 483, loss 0.66408.
Train: 2018-08-09T11:04:51.840898: step 484, loss 0.66079.
Train: 2018-08-09T11:04:51.937139: step 485, loss 0.574293.
Train: 2018-08-09T11:04:52.046459: step 486, loss 0.585907.
Train: 2018-08-09T11:04:52.155837: step 487, loss 0.506352.
Train: 2018-08-09T11:04:52.249565: step 488, loss 0.521186.
Train: 2018-08-09T11:04:52.358915: step 489, loss 0.639697.
Train: 2018-08-09T11:04:52.468264: step 490, loss 0.558159.
Test: 2018-08-09T11:04:52.968148: step 490, loss 0.546647.
Train: 2018-08-09T11:04:53.077496: step 491, loss 0.508342.
Train: 2018-08-09T11:04:53.186845: step 492, loss 0.570194.
Train: 2018-08-09T11:04:53.280544: step 493, loss 0.570092.
Train: 2018-08-09T11:04:53.389924: step 494, loss 0.563226.
Train: 2018-08-09T11:04:53.499265: step 495, loss 0.596261.
Train: 2018-08-09T11:04:53.608591: step 496, loss 0.577462.
Train: 2018-08-09T11:04:53.717972: step 497, loss 0.599378.
Train: 2018-08-09T11:04:53.811692: step 498, loss 0.65315.
Train: 2018-08-09T11:04:53.921788: step 499, loss 0.48316.
Train: 2018-08-09T11:04:54.031137: step 500, loss 0.577863.
Test: 2018-08-09T11:04:54.538131: step 500, loss 0.551655.
Train: 2018-08-09T11:04:55.100523: step 501, loss 0.600484.
Train: 2018-08-09T11:04:55.209872: step 502, loss 0.559519.
Train: 2018-08-09T11:04:55.303602: step 503, loss 0.586238.
Train: 2018-08-09T11:04:55.412949: step 504, loss 0.61291.
Train: 2018-08-09T11:04:55.522299: step 505, loss 0.539998.
Train: 2018-08-09T11:04:55.631648: step 506, loss 0.606809.
Train: 2018-08-09T11:04:55.740998: step 507, loss 0.59676.
Train: 2018-08-09T11:04:55.834726: step 508, loss 0.521796.
Train: 2018-08-09T11:04:55.944076: step 509, loss 0.549678.
Train: 2018-08-09T11:04:56.053418: step 510, loss 0.561047.
Test: 2018-08-09T11:04:56.553276: step 510, loss 0.551719.
Train: 2018-08-09T11:04:56.662649: step 511, loss 0.550203.
Train: 2018-08-09T11:04:56.756355: step 512, loss 0.562208.
Train: 2018-08-09T11:04:56.865733: step 513, loss 0.577694.
Train: 2018-08-09T11:04:56.977340: step 514, loss 0.54955.
Train: 2018-08-09T11:04:57.086690: step 515, loss 0.562848.
Train: 2018-08-09T11:04:57.196038: step 516, loss 0.474328.
Train: 2018-08-09T11:04:57.305357: step 517, loss 0.583539.
Train: 2018-08-09T11:04:57.399086: step 518, loss 0.598749.
Train: 2018-08-09T11:04:57.508464: step 519, loss 0.510206.
Train: 2018-08-09T11:04:57.617814: step 520, loss 0.550189.
Test: 2018-08-09T11:04:58.117670: step 520, loss 0.547124.
Train: 2018-08-09T11:04:58.227045: step 521, loss 0.459754.
Train: 2018-08-09T11:04:58.320744: step 522, loss 0.486518.
Train: 2018-08-09T11:04:58.430123: step 523, loss 0.742876.
Train: 2018-08-09T11:04:58.539473: step 524, loss 0.716297.
Train: 2018-08-09T11:04:58.648821: step 525, loss 0.573509.
Train: 2018-08-09T11:04:58.758170: step 526, loss 0.642927.
Train: 2018-08-09T11:04:58.867490: step 527, loss 0.558251.
Train: 2018-08-09T11:04:58.979105: step 528, loss 0.534927.
Train: 2018-08-09T11:04:59.088454: step 529, loss 0.510394.
Train: 2018-08-09T11:04:59.197804: step 530, loss 0.6255.
Test: 2018-08-09T11:04:59.682035: step 530, loss 0.556093.
Train: 2018-08-09T11:04:59.791414: step 531, loss 0.545054.
Train: 2018-08-09T11:04:59.900766: step 532, loss 0.513814.
Train: 2018-08-09T11:05:00.010113: step 533, loss 0.471037.
Train: 2018-08-09T11:05:00.119431: step 534, loss 0.566548.
Train: 2018-08-09T11:05:00.228810: step 535, loss 0.579026.
Train: 2018-08-09T11:05:00.338161: step 536, loss 0.547729.
Train: 2018-08-09T11:05:00.431888: step 537, loss 0.579112.
Train: 2018-08-09T11:05:00.541238: step 538, loss 0.502179.
Train: 2018-08-09T11:05:00.650589: step 539, loss 0.596796.
Train: 2018-08-09T11:05:00.759935: step 540, loss 0.656647.
Test: 2018-08-09T11:05:01.261162: step 540, loss 0.550072.
Train: 2018-08-09T11:05:01.370510: step 541, loss 0.635172.
Train: 2018-08-09T11:05:01.464242: step 542, loss 0.550472.
Train: 2018-08-09T11:05:01.573587: step 543, loss 0.553002.
Train: 2018-08-09T11:05:01.682906: step 544, loss 0.5139.
Train: 2018-08-09T11:05:01.792258: step 545, loss 0.529388.
Train: 2018-08-09T11:05:01.901635: step 546, loss 0.549711.
Train: 2018-08-09T11:05:02.010984: step 547, loss 0.599963.
Train: 2018-08-09T11:05:02.120304: step 548, loss 0.679823.
Train: 2018-08-09T11:05:02.229653: step 549, loss 0.575969.
Train: 2018-08-09T11:05:02.339036: step 550, loss 0.613169.
Test: 2018-08-09T11:05:02.823292: step 550, loss 0.549166.
Train: 2018-08-09T11:05:02.934014: step 551, loss 0.610733.
Train: 2018-08-09T11:05:03.043363: step 552, loss 0.626186.
Train: 2018-08-09T11:05:03.152710: step 553, loss 0.520345.
Train: 2018-08-09T11:05:03.262059: step 554, loss 0.559586.
Train: 2018-08-09T11:05:03.371409: step 555, loss 0.637232.
Train: 2018-08-09T11:05:03.480758: step 556, loss 0.537146.
Train: 2018-08-09T11:05:03.574486: step 557, loss 0.57885.
Train: 2018-08-09T11:05:03.683835: step 558, loss 0.654619.
Train: 2018-08-09T11:05:03.793155: step 559, loss 0.653492.
Train: 2018-08-09T11:05:03.902534: step 560, loss 0.482971.
Test: 2018-08-09T11:05:04.402410: step 560, loss 0.55482.
Train: 2018-08-09T11:05:04.511735: step 561, loss 0.553929.
Train: 2018-08-09T11:05:04.621114: step 562, loss 0.555904.
Train: 2018-08-09T11:05:04.730432: step 563, loss 0.539531.
Train: 2018-08-09T11:05:04.839782: step 564, loss 0.630855.
Train: 2018-08-09T11:05:04.951491: step 565, loss 0.581701.
Train: 2018-08-09T11:05:05.060871: step 566, loss 0.533471.
Train: 2018-08-09T11:05:05.170223: step 567, loss 0.554724.
Train: 2018-08-09T11:05:05.279569: step 568, loss 0.522448.
Train: 2018-08-09T11:05:05.388918: step 569, loss 0.643581.
Train: 2018-08-09T11:05:05.498238: step 570, loss 0.56408.
Test: 2018-08-09T11:05:05.998121: step 570, loss 0.550385.
Train: 2018-08-09T11:05:06.107494: step 571, loss 0.643018.
Train: 2018-08-09T11:05:06.216819: step 572, loss 0.576761.
Train: 2018-08-09T11:05:06.326198: step 573, loss 0.639523.
Train: 2018-08-09T11:05:06.419928: step 574, loss 0.59453.
Train: 2018-08-09T11:05:06.529274: step 575, loss 0.607082.
Train: 2018-08-09T11:05:06.638623: step 576, loss 0.544285.
Train: 2018-08-09T11:05:06.747942: step 577, loss 0.529505.
Train: 2018-08-09T11:05:06.857323: step 578, loss 0.582014.
Train: 2018-08-09T11:05:06.968147: step 579, loss 0.587081.
Train: 2018-08-09T11:05:07.077496: step 580, loss 0.478195.
Test: 2018-08-09T11:05:07.561752: step 580, loss 0.550883.
Train: 2018-08-09T11:05:07.671106: step 581, loss 0.641358.
Train: 2018-08-09T11:05:07.780455: step 582, loss 0.548459.
Train: 2018-08-09T11:05:07.889799: step 583, loss 0.56489.
Train: 2018-08-09T11:05:07.999153: step 584, loss 0.551081.
Train: 2018-08-09T11:05:08.108503: step 585, loss 0.538221.
Train: 2018-08-09T11:05:08.217857: step 586, loss 0.640388.
Train: 2018-08-09T11:05:08.311582: step 587, loss 0.595626.
Train: 2018-08-09T11:05:08.420930: step 588, loss 0.488292.
Train: 2018-08-09T11:05:08.530279: step 589, loss 0.54432.
Train: 2018-08-09T11:05:08.639628: step 590, loss 0.483874.
Test: 2018-08-09T11:05:09.140086: step 590, loss 0.549447.
Train: 2018-08-09T11:05:09.245860: step 591, loss 0.50521.
Train: 2018-08-09T11:05:09.355209: step 592, loss 0.549363.
Train: 2018-08-09T11:05:09.448937: step 593, loss 0.682872.
Train: 2018-08-09T11:05:09.558255: step 594, loss 0.623253.
Train: 2018-08-09T11:05:09.667635: step 595, loss 0.573709.
Train: 2018-08-09T11:05:09.776985: step 596, loss 0.608815.
Train: 2018-08-09T11:05:09.886335: step 597, loss 0.544453.
Train: 2018-08-09T11:05:09.995683: step 598, loss 0.608196.
Train: 2018-08-09T11:05:10.089411: step 599, loss 0.546562.
Train: 2018-08-09T11:05:10.198760: step 600, loss 0.553327.
Test: 2018-08-09T11:05:10.698613: step 600, loss 0.547889.
Train: 2018-08-09T11:05:11.323466: step 601, loss 0.606345.
Train: 2018-08-09T11:05:11.417223: step 602, loss 0.536415.
Train: 2018-08-09T11:05:11.529982: step 603, loss 0.422413.
Train: 2018-08-09T11:05:11.639329: step 604, loss 0.512583.
Train: 2018-08-09T11:05:11.748676: step 605, loss 0.518252.
Train: 2018-08-09T11:05:11.858027: step 606, loss 0.536289.
Train: 2018-08-09T11:05:11.965133: step 607, loss 0.572061.
Train: 2018-08-09T11:05:12.074511: step 608, loss 0.554526.
Train: 2018-08-09T11:05:12.183831: step 609, loss 0.542692.
Train: 2018-08-09T11:05:12.277589: step 610, loss 0.506505.
Test: 2018-08-09T11:05:12.777466: step 610, loss 0.546596.
Train: 2018-08-09T11:05:12.886824: step 611, loss 0.427405.
Train: 2018-08-09T11:05:12.996169: step 612, loss 0.565896.
Train: 2018-08-09T11:05:13.105518: step 613, loss 0.544677.
Train: 2018-08-09T11:05:13.214837: step 614, loss 0.564965.
Train: 2018-08-09T11:05:13.324217: step 615, loss 0.640082.
Train: 2018-08-09T11:05:13.433536: step 616, loss 0.581579.
Train: 2018-08-09T11:05:13.527294: step 617, loss 0.561836.
Train: 2018-08-09T11:05:13.636646: step 618, loss 0.616884.
Train: 2018-08-09T11:05:13.745993: step 619, loss 0.527858.
Train: 2018-08-09T11:05:13.855311: step 620, loss 0.496568.
Test: 2018-08-09T11:05:14.339597: step 620, loss 0.547808.
Train: 2018-08-09T11:05:14.448947: step 621, loss 0.639667.
Train: 2018-08-09T11:05:14.558301: step 622, loss 0.616111.
Train: 2018-08-09T11:05:14.667651: step 623, loss 0.620373.
Train: 2018-08-09T11:05:14.761349: step 624, loss 0.667031.
Train: 2018-08-09T11:05:14.870697: step 625, loss 0.484677.
Train: 2018-08-09T11:05:14.981507: step 626, loss 0.599629.
Train: 2018-08-09T11:05:15.090872: step 627, loss 0.613949.
Train: 2018-08-09T11:05:15.193213: step 628, loss 0.498169.
Train: 2018-08-09T11:05:15.302565: step 629, loss 0.606712.
Train: 2018-08-09T11:05:15.411911: step 630, loss 0.470019.
Test: 2018-08-09T11:05:15.911798: step 630, loss 0.553849.
Train: 2018-08-09T11:05:16.005521: step 631, loss 0.52799.
Train: 2018-08-09T11:05:16.114872: step 632, loss 0.65986.
Train: 2018-08-09T11:05:16.224190: step 633, loss 0.514958.
Train: 2018-08-09T11:05:16.333570: step 634, loss 0.567728.
Train: 2018-08-09T11:05:16.427266: step 635, loss 0.483792.
Train: 2018-08-09T11:05:16.536646: step 636, loss 0.570233.
Train: 2018-08-09T11:05:16.645996: step 637, loss 0.605483.
Train: 2018-08-09T11:05:16.755346: step 638, loss 0.482338.
Train: 2018-08-09T11:05:16.849076: step 639, loss 0.545539.
Train: 2018-08-09T11:05:16.950990: step 640, loss 0.714182.
Test: 2018-08-09T11:05:17.450872: step 640, loss 0.547275.
Train: 2018-08-09T11:05:17.560251: step 641, loss 0.581283.
Train: 2018-08-09T11:05:17.669600: step 642, loss 0.645379.
Train: 2018-08-09T11:05:17.763328: step 643, loss 0.644609.
Train: 2018-08-09T11:05:17.872677: step 644, loss 0.575408.
Train: 2018-08-09T11:05:17.981997: step 645, loss 0.544328.
Train: 2018-08-09T11:05:18.075754: step 646, loss 0.578053.
Train: 2018-08-09T11:05:18.185075: step 647, loss 0.544611.
Train: 2018-08-09T11:05:18.294453: step 648, loss 0.606837.
Train: 2018-08-09T11:05:18.388180: step 649, loss 0.550714.
Train: 2018-08-09T11:05:18.497530: step 650, loss 0.561648.
Test: 2018-08-09T11:05:18.997385: step 650, loss 0.550745.
Train: 2018-08-09T11:05:19.091109: step 651, loss 0.483912.
Train: 2018-08-09T11:05:19.200459: step 652, loss 0.595808.
Train: 2018-08-09T11:05:19.309839: step 653, loss 0.564255.
Train: 2018-08-09T11:05:19.419157: step 654, loss 0.588048.
Train: 2018-08-09T11:05:19.512886: step 655, loss 0.531369.
Train: 2018-08-09T11:05:19.622265: step 656, loss 0.518523.
Train: 2018-08-09T11:05:19.731583: step 657, loss 0.718665.
Train: 2018-08-09T11:05:19.825342: step 658, loss 0.57821.
Train: 2018-08-09T11:05:19.936146: step 659, loss 0.586389.
Train: 2018-08-09T11:05:20.045496: step 660, loss 0.574486.
Test: 2018-08-09T11:05:20.529751: step 660, loss 0.548284.
Train: 2018-08-09T11:05:20.639106: step 661, loss 0.630099.
Train: 2018-08-09T11:05:20.748454: step 662, loss 0.573353.
Train: 2018-08-09T11:05:20.842152: step 663, loss 0.519575.
Train: 2018-08-09T11:05:20.951501: step 664, loss 0.649551.
Train: 2018-08-09T11:05:21.060881: step 665, loss 0.568891.
Train: 2018-08-09T11:05:21.154610: step 666, loss 0.534551.
Train: 2018-08-09T11:05:21.263931: step 667, loss 0.48704.
Train: 2018-08-09T11:05:21.373307: step 668, loss 0.521796.
Train: 2018-08-09T11:05:21.467035: step 669, loss 0.490112.
Train: 2018-08-09T11:05:21.576354: step 670, loss 0.535966.
Test: 2018-08-09T11:05:22.061259: step 670, loss 0.551027.
Train: 2018-08-09T11:05:22.170638: step 671, loss 0.531613.
Train: 2018-08-09T11:05:22.279987: step 672, loss 0.574624.
Train: 2018-08-09T11:05:22.389339: step 673, loss 0.565767.
Train: 2018-08-09T11:05:22.483063: step 674, loss 0.513404.
Train: 2018-08-09T11:05:22.592414: step 675, loss 0.528697.
Train: 2018-08-09T11:05:22.686140: step 676, loss 0.579723.
Train: 2018-08-09T11:05:22.795461: step 677, loss 0.629868.
Train: 2018-08-09T11:05:22.904810: step 678, loss 0.558169.
Train: 2018-08-09T11:05:22.998567: step 679, loss 0.557426.
Train: 2018-08-09T11:05:23.107917: step 680, loss 0.493681.
Test: 2018-08-09T11:05:23.607768: step 680, loss 0.548728.
Train: 2018-08-09T11:05:23.701526: step 681, loss 0.581493.
Train: 2018-08-09T11:05:23.810877: step 682, loss 0.552052.
Train: 2018-08-09T11:05:23.904604: step 683, loss 0.593967.
Train: 2018-08-09T11:05:24.013953: step 684, loss 0.635834.
Train: 2018-08-09T11:05:24.107682: step 685, loss 0.54125.
Train: 2018-08-09T11:05:24.217032: step 686, loss 0.601782.
Train: 2018-08-09T11:05:24.326378: step 687, loss 0.521619.
Train: 2018-08-09T11:05:24.420107: step 688, loss 0.527531.
Train: 2018-08-09T11:05:24.529426: step 689, loss 0.414814.
Train: 2018-08-09T11:05:24.623184: step 690, loss 0.567788.
Test: 2018-08-09T11:05:25.132357: step 690, loss 0.547133.
Train: 2018-08-09T11:05:25.226115: step 691, loss 0.561505.
Train: 2018-08-09T11:05:25.335434: step 692, loss 0.578146.
Train: 2018-08-09T11:05:25.429192: step 693, loss 0.526784.
Train: 2018-08-09T11:05:25.538545: step 694, loss 0.545393.
Train: 2018-08-09T11:05:25.632267: step 695, loss 0.562691.
Train: 2018-08-09T11:05:25.741619: step 696, loss 0.540676.
Train: 2018-08-09T11:05:25.835347: step 697, loss 0.593538.
Train: 2018-08-09T11:05:25.944696: step 698, loss 0.479613.
Train: 2018-08-09T11:05:26.038393: step 699, loss 0.52457.
Train: 2018-08-09T11:05:26.147773: step 700, loss 0.58708.
Test: 2018-08-09T11:05:26.647626: step 700, loss 0.546731.
Train: 2018-08-09T11:05:27.210023: step 701, loss 0.564148.
Train: 2018-08-09T11:05:27.303745: step 702, loss 0.440223.
Train: 2018-08-09T11:05:27.413105: step 703, loss 0.60129.
Train: 2018-08-09T11:05:27.506828: step 704, loss 0.652367.
Train: 2018-08-09T11:05:27.616177: step 705, loss 0.524699.
Train: 2018-08-09T11:05:27.709876: step 706, loss 0.588066.
Train: 2018-08-09T11:05:27.834845: step 707, loss 0.559279.
Train: 2018-08-09T11:05:27.944225: step 708, loss 0.54117.
Train: 2018-08-09T11:05:28.037956: step 709, loss 0.468359.
Train: 2018-08-09T11:05:28.147302: step 710, loss 0.493047.
Test: 2018-08-09T11:05:28.647155: step 710, loss 0.546814.
Train: 2018-08-09T11:05:28.787745: step 711, loss 0.58246.
Train: 2018-08-09T11:05:28.897127: step 712, loss 0.581693.
Train: 2018-08-09T11:05:28.992201: step 713, loss 0.561808.
Train: 2018-08-09T11:05:29.101550: step 714, loss 0.671278.
Train: 2018-08-09T11:05:29.195247: step 715, loss 0.562784.
Train: 2018-08-09T11:05:29.289006: step 716, loss 0.558744.
Train: 2018-08-09T11:05:29.398355: step 717, loss 0.658775.
Train: 2018-08-09T11:05:29.492053: step 718, loss 0.724052.
Train: 2018-08-09T11:05:29.601403: step 719, loss 0.465197.
Train: 2018-08-09T11:05:29.695160: step 720, loss 0.583113.
Test: 2018-08-09T11:05:30.195047: step 720, loss 0.548307.
Train: 2018-08-09T11:05:30.288765: step 721, loss 0.574273.
Train: 2018-08-09T11:05:30.382498: step 722, loss 0.707175.
Train: 2018-08-09T11:05:30.491843: step 723, loss 0.596399.
Train: 2018-08-09T11:05:30.585576: step 724, loss 0.563498.
Train: 2018-08-09T11:05:30.679303: step 725, loss 0.60996.
Train: 2018-08-09T11:05:30.773031: step 726, loss 0.524682.
Train: 2018-08-09T11:05:30.882381: step 727, loss 0.569142.
Train: 2018-08-09T11:05:30.978424: step 728, loss 0.547105.
Train: 2018-08-09T11:05:31.072122: step 729, loss 0.612521.
Train: 2018-08-09T11:05:31.165880: step 730, loss 0.507645.
Test: 2018-08-09T11:05:31.665732: step 730, loss 0.551402.
Train: 2018-08-09T11:05:31.759460: step 731, loss 0.535615.
Train: 2018-08-09T11:05:31.868811: step 732, loss 0.620151.
Train: 2018-08-09T11:05:31.962567: step 733, loss 0.476132.
Train: 2018-08-09T11:05:32.056299: step 734, loss 0.672557.
Train: 2018-08-09T11:05:32.150023: step 735, loss 0.620053.
Train: 2018-08-09T11:05:32.243751: step 736, loss 0.655629.
Train: 2018-08-09T11:05:32.353069: step 737, loss 0.611321.
Train: 2018-08-09T11:05:32.446831: step 738, loss 0.636152.
Train: 2018-08-09T11:05:32.540557: step 739, loss 0.608338.
Train: 2018-08-09T11:05:32.634283: step 740, loss 0.566033.
Test: 2018-08-09T11:05:33.136415: step 740, loss 0.552154.
Train: 2018-08-09T11:05:33.230174: step 741, loss 0.535882.
Train: 2018-08-09T11:05:33.323901: step 742, loss 0.638537.
Train: 2018-08-09T11:05:33.417600: step 743, loss 0.567957.
Train: 2018-08-09T11:05:33.511359: step 744, loss 0.523748.
Train: 2018-08-09T11:05:33.605085: step 745, loss 0.496202.
Train: 2018-08-09T11:05:33.698814: step 746, loss 0.552221.
Train: 2018-08-09T11:05:33.792540: step 747, loss 0.492505.
Train: 2018-08-09T11:05:33.886271: step 748, loss 0.507092.
Train: 2018-08-09T11:05:33.979996: step 749, loss 0.591789.
Train: 2018-08-09T11:05:34.073724: step 750, loss 0.705563.
Test: 2018-08-09T11:05:34.573576: step 750, loss 0.551406.
Train: 2018-08-09T11:05:34.667334: step 751, loss 0.52002.
Train: 2018-08-09T11:05:34.761065: step 752, loss 0.49062.
Train: 2018-08-09T11:05:34.854790: step 753, loss 0.546676.
Train: 2018-08-09T11:05:34.950023: step 754, loss 0.599927.
Train: 2018-08-09T11:05:35.043720: step 755, loss 0.502824.
Train: 2018-08-09T11:05:35.137479: step 756, loss 0.609548.
Train: 2018-08-09T11:05:35.231209: step 757, loss 0.580495.
Train: 2018-08-09T11:05:35.324934: step 758, loss 0.529229.
Train: 2018-08-09T11:05:35.418662: step 759, loss 0.487313.
Train: 2018-08-09T11:05:35.496769: step 760, loss 0.482568.
Test: 2018-08-09T11:05:35.996622: step 760, loss 0.548037.
Train: 2018-08-09T11:05:36.090350: step 761, loss 0.611703.
Train: 2018-08-09T11:05:36.184076: step 762, loss 0.593643.
Train: 2018-08-09T11:05:36.277835: step 763, loss 0.651471.
Train: 2018-08-09T11:05:36.371536: step 764, loss 0.581802.
Train: 2018-08-09T11:05:36.449669: step 765, loss 0.528201.
Train: 2018-08-09T11:05:36.543398: step 766, loss 0.576008.
Train: 2018-08-09T11:05:36.637128: step 767, loss 0.477841.
Train: 2018-08-09T11:05:36.730823: step 768, loss 0.593707.
Train: 2018-08-09T11:05:36.824551: step 769, loss 0.513947.
Train: 2018-08-09T11:05:36.902658: step 770, loss 0.491982.
Test: 2018-08-09T11:05:37.403988: step 770, loss 0.547517.
Train: 2018-08-09T11:05:37.497747: step 771, loss 0.436794.
Train: 2018-08-09T11:05:37.575853: step 772, loss 0.489914.
Train: 2018-08-09T11:05:37.669581: step 773, loss 0.547084.
Train: 2018-08-09T11:05:37.763279: step 774, loss 0.56678.
Train: 2018-08-09T11:05:37.857007: step 775, loss 0.650686.
Train: 2018-08-09T11:05:37.935113: step 776, loss 0.511724.
Train: 2018-08-09T11:05:38.028873: step 777, loss 0.511667.
Train: 2018-08-09T11:05:38.122569: step 778, loss 0.641779.
Train: 2018-08-09T11:05:38.200676: step 779, loss 0.509084.
Train: 2018-08-09T11:05:38.294433: step 780, loss 0.585781.
Test: 2018-08-09T11:05:38.794290: step 780, loss 0.546411.
Train: 2018-08-09T11:05:38.872422: step 781, loss 0.530279.
Train: 2018-08-09T11:05:38.967544: step 782, loss 0.582242.
Train: 2018-08-09T11:05:39.045646: step 783, loss 0.564896.
Train: 2018-08-09T11:05:39.139355: step 784, loss 0.598418.
Train: 2018-08-09T11:05:39.233106: step 785, loss 0.660919.
Train: 2018-08-09T11:05:39.311183: step 786, loss 0.566671.
Train: 2018-08-09T11:05:39.404943: step 787, loss 0.636398.
Train: 2018-08-09T11:05:39.494212: step 788, loss 0.633882.
Train: 2018-08-09T11:05:39.570840: step 789, loss 0.539884.
Train: 2018-08-09T11:05:39.664570: step 790, loss 0.450954.
Test: 2018-08-09T11:05:40.148798: step 790, loss 0.550991.
Train: 2018-08-09T11:05:40.242525: step 791, loss 0.54908.
Train: 2018-08-09T11:05:40.320661: step 792, loss 0.521181.
Train: 2018-08-09T11:05:40.414393: step 793, loss 0.530622.
Train: 2018-08-09T11:05:40.492496: step 794, loss 0.676736.
Train: 2018-08-09T11:05:40.582084: step 795, loss 0.579666.
Train: 2018-08-09T11:05:40.660216: step 796, loss 0.60881.
Train: 2018-08-09T11:05:40.753948: step 797, loss 0.5754.
Train: 2018-08-09T11:05:40.832056: step 798, loss 0.581988.
Train: 2018-08-09T11:05:40.921397: step 799, loss 0.579709.
Train: 2018-08-09T11:05:40.999534: step 800, loss 0.582532.
Test: 2018-08-09T11:05:41.499411: step 800, loss 0.55228.
Train: 2018-08-09T11:05:42.058049: step 801, loss 0.580496.
Train: 2018-08-09T11:05:42.151784: step 802, loss 0.499747.
Train: 2018-08-09T11:05:42.229889: step 803, loss 0.62663.
Train: 2018-08-09T11:05:42.323616: step 804, loss 0.59266.
Train: 2018-08-09T11:05:42.401722: step 805, loss 0.536305.
Train: 2018-08-09T11:05:42.479830: step 806, loss 0.575455.
Train: 2018-08-09T11:05:42.573526: step 807, loss 0.520529.
Train: 2018-08-09T11:05:42.651633: step 808, loss 0.640444.
Train: 2018-08-09T11:05:42.729740: step 809, loss 0.610524.
Train: 2018-08-09T11:05:42.823499: step 810, loss 0.521666.
Test: 2018-08-09T11:05:43.307730: step 810, loss 0.552783.
Train: 2018-08-09T11:05:43.385835: step 811, loss 0.504968.
Train: 2018-08-09T11:05:43.479594: step 812, loss 0.518176.
Train: 2018-08-09T11:05:43.557696: step 813, loss 0.645501.
Train: 2018-08-09T11:05:43.635809: step 814, loss 0.503529.
Train: 2018-08-09T11:05:43.713907: step 815, loss 0.457376.
Train: 2018-08-09T11:05:43.807638: step 816, loss 0.535957.
Train: 2018-08-09T11:05:43.885718: step 817, loss 0.620346.
Train: 2018-08-09T11:05:43.966202: step 818, loss 0.593062.
Train: 2018-08-09T11:05:44.044279: step 819, loss 0.687829.
Train: 2018-08-09T11:05:44.122417: step 820, loss 0.517591.
Test: 2018-08-09T11:05:44.622268: step 820, loss 0.549268.
Train: 2018-08-09T11:05:44.700374: step 821, loss 0.642013.
Train: 2018-08-09T11:05:44.778516: step 822, loss 0.583274.
Train: 2018-08-09T11:05:44.872239: step 823, loss 0.630829.
Train: 2018-08-09T11:05:44.950345: step 824, loss 0.567578.
Train: 2018-08-09T11:05:45.028453: step 825, loss 0.563426.
Train: 2018-08-09T11:05:45.106561: step 826, loss 0.597052.
Train: 2018-08-09T11:05:45.184635: step 827, loss 0.517464.
Train: 2018-08-09T11:05:45.262772: step 828, loss 0.564171.
Train: 2018-08-09T11:05:45.356495: step 829, loss 0.533613.
Train: 2018-08-09T11:05:45.434606: step 830, loss 0.547559.
Test: 2018-08-09T11:05:45.935002: step 830, loss 0.551321.
Train: 2018-08-09T11:05:46.013138: step 831, loss 0.517647.
Train: 2018-08-09T11:05:46.091213: step 832, loss 0.575648.
Train: 2018-08-09T11:05:46.169322: step 833, loss 0.596246.
Train: 2018-08-09T11:05:46.247458: step 834, loss 0.592607.
Train: 2018-08-09T11:05:46.325564: step 835, loss 0.626859.
Train: 2018-08-09T11:05:46.403646: step 836, loss 0.545368.
Train: 2018-08-09T11:05:46.481777: step 837, loss 0.626682.
Train: 2018-08-09T11:05:46.559853: step 838, loss 0.467478.
Train: 2018-08-09T11:05:46.653583: step 839, loss 0.597952.
Train: 2018-08-09T11:05:46.731688: step 840, loss 0.455277.
Test: 2018-08-09T11:05:47.231571: step 840, loss 0.548251.
Train: 2018-08-09T11:05:47.309707: step 841, loss 0.530785.
Train: 2018-08-09T11:05:47.387809: step 842, loss 0.594816.
Train: 2018-08-09T11:05:47.465921: step 843, loss 0.531653.
Train: 2018-08-09T11:05:47.543996: step 844, loss 0.593097.
Train: 2018-08-09T11:05:47.622134: step 845, loss 0.531159.
Train: 2018-08-09T11:05:47.700241: step 846, loss 0.57995.
Train: 2018-08-09T11:05:47.778346: step 847, loss 0.51685.
Train: 2018-08-09T11:05:47.856453: step 848, loss 0.546232.
Train: 2018-08-09T11:05:47.934558: step 849, loss 0.582608.
Train: 2018-08-09T11:05:48.012637: step 850, loss 0.526828.
Test: 2018-08-09T11:05:48.512518: step 850, loss 0.550363.
Train: 2018-08-09T11:05:48.590655: step 851, loss 0.479799.
Train: 2018-08-09T11:05:48.668764: step 852, loss 0.47609.
Train: 2018-08-09T11:05:48.746875: step 853, loss 0.528449.
Train: 2018-08-09T11:05:48.824975: step 854, loss 0.530264.
Train: 2018-08-09T11:05:48.903085: step 855, loss 0.471574.
Train: 2018-08-09T11:05:48.982653: step 856, loss 0.642337.
Train: 2018-08-09T11:05:49.060761: step 857, loss 0.553307.
Train: 2018-08-09T11:05:49.138868: step 858, loss 0.563912.
Train: 2018-08-09T11:05:49.232594: step 859, loss 0.507925.
Train: 2018-08-09T11:05:49.295082: step 860, loss 0.564513.
Test: 2018-08-09T11:05:49.794964: step 860, loss 0.547071.
Train: 2018-08-09T11:05:49.873068: step 861, loss 0.599112.
Train: 2018-08-09T11:05:49.951178: step 862, loss 0.45765.
Train: 2018-08-09T11:05:50.029280: step 863, loss 0.649199.
Train: 2018-08-09T11:05:50.107387: step 864, loss 0.629344.
Train: 2018-08-09T11:05:50.201118: step 865, loss 0.609522.
Train: 2018-08-09T11:05:50.279233: step 866, loss 0.552852.
Train: 2018-08-09T11:05:50.357331: step 867, loss 0.578661.
Train: 2018-08-09T11:05:50.435407: step 868, loss 0.507651.
Train: 2018-08-09T11:05:50.513548: step 869, loss 0.562987.
Train: 2018-08-09T11:05:50.591620: step 870, loss 0.491175.
Test: 2018-08-09T11:05:51.092949: step 870, loss 0.546557.
Train: 2018-08-09T11:05:51.171056: step 871, loss 0.580733.
Train: 2018-08-09T11:05:51.233540: step 872, loss 0.582119.
Train: 2018-08-09T11:05:51.327270: step 873, loss 0.611212.
Train: 2018-08-09T11:05:51.405375: step 874, loss 0.620774.
Train: 2018-08-09T11:05:51.483512: step 875, loss 0.528263.
Train: 2018-08-09T11:05:51.561589: step 876, loss 0.463943.
Train: 2018-08-09T11:05:51.639727: step 877, loss 0.579806.
Train: 2018-08-09T11:05:51.717831: step 878, loss 0.630247.
Train: 2018-08-09T11:05:51.795939: step 879, loss 0.597329.
Train: 2018-08-09T11:05:51.874039: step 880, loss 0.599245.
Test: 2018-08-09T11:05:52.373897: step 880, loss 0.551094.
Train: 2018-08-09T11:05:52.452034: step 881, loss 0.578751.
Train: 2018-08-09T11:05:52.530109: step 882, loss 0.56039.
Train: 2018-08-09T11:05:52.608251: step 883, loss 0.482839.
Train: 2018-08-09T11:05:52.686322: step 884, loss 0.481508.
Train: 2018-08-09T11:05:52.764461: step 885, loss 0.530467.
Train: 2018-08-09T11:05:52.842537: step 886, loss 0.598556.
Train: 2018-08-09T11:05:52.921312: step 887, loss 0.468763.
Train: 2018-08-09T11:05:52.999450: step 888, loss 0.482892.
Train: 2018-08-09T11:05:53.077527: step 889, loss 0.563233.
Train: 2018-08-09T11:05:53.155662: step 890, loss 0.577679.
Test: 2018-08-09T11:05:53.655542: step 890, loss 0.545096.
Train: 2018-08-09T11:05:53.733620: step 891, loss 0.459034.
Train: 2018-08-09T11:05:53.811728: step 892, loss 0.562705.
Train: 2018-08-09T11:05:53.889835: step 893, loss 0.52367.
Train: 2018-08-09T11:05:53.966896: step 894, loss 0.614501.
Train: 2018-08-09T11:05:54.045003: step 895, loss 0.53095.
Train: 2018-08-09T11:05:54.123105: step 896, loss 0.527982.
Train: 2018-08-09T11:05:54.201218: step 897, loss 0.651381.
Train: 2018-08-09T11:05:54.279323: step 898, loss 0.668328.
Train: 2018-08-09T11:05:54.371644: step 899, loss 0.734105.
Train: 2018-08-09T11:05:54.445251: step 900, loss 0.597912.
Test: 2018-08-09T11:05:54.929518: step 900, loss 0.547905.
Train: 2018-08-09T11:05:55.491884: step 901, loss 0.511942.
Train: 2018-08-09T11:05:55.569981: step 902, loss 0.613242.
Train: 2018-08-09T11:05:55.648094: step 903, loss 0.548136.
Train: 2018-08-09T11:05:55.726198: step 904, loss 0.561947.
Train: 2018-08-09T11:05:55.803350: step 905, loss 0.532378.
Train: 2018-08-09T11:05:55.884272: step 906, loss 0.547703.
Train: 2018-08-09T11:05:55.967659: step 907, loss 0.548032.
Train: 2018-08-09T11:05:56.045796: step 908, loss 0.546839.
Train: 2018-08-09T11:05:56.123900: step 909, loss 0.484527.
Train: 2018-08-09T11:05:56.202009: step 910, loss 0.533224.
Test: 2018-08-09T11:05:56.701880: step 910, loss 0.549521.
Train: 2018-08-09T11:05:56.779998: step 911, loss 0.53326.
Train: 2018-08-09T11:05:56.858105: step 912, loss 0.595166.
Train: 2018-08-09T11:05:56.936211: step 913, loss 0.5934.
Train: 2018-08-09T11:05:57.014318: step 914, loss 0.496168.
Train: 2018-08-09T11:05:57.092425: step 915, loss 0.530761.
Train: 2018-08-09T11:05:57.170527: step 916, loss 0.546039.
Train: 2018-08-09T11:05:57.248639: step 917, loss 0.497748.
Train: 2018-08-09T11:05:57.326714: step 918, loss 0.490691.
Train: 2018-08-09T11:05:57.404820: step 919, loss 0.599792.
Train: 2018-08-09T11:05:57.482927: step 920, loss 0.494092.
Test: 2018-08-09T11:05:57.982809: step 920, loss 0.54995.
Train: 2018-08-09T11:05:58.060946: step 921, loss 0.508511.
Train: 2018-08-09T11:05:58.139053: step 922, loss 0.522698.
Train: 2018-08-09T11:05:58.217156: step 923, loss 0.494996.
Train: 2018-08-09T11:05:58.295266: step 924, loss 0.643523.
Train: 2018-08-09T11:05:58.373374: step 925, loss 0.544641.
Train: 2018-08-09T11:05:58.451477: step 926, loss 0.525619.
Train: 2018-08-09T11:05:58.529586: step 927, loss 0.619606.
Train: 2018-08-09T11:05:58.607692: step 928, loss 0.510197.
Train: 2018-08-09T11:05:58.685800: step 929, loss 0.547996.
Train: 2018-08-09T11:05:58.763905: step 930, loss 0.550633.
Test: 2018-08-09T11:05:59.266177: step 930, loss 0.547233.
Train: 2018-08-09T11:05:59.328663: step 931, loss 0.455306.
Train: 2018-08-09T11:05:59.406770: step 932, loss 0.579421.
Train: 2018-08-09T11:05:59.500532: step 933, loss 0.670586.
Train: 2018-08-09T11:05:59.563012: step 934, loss 0.461938.
Train: 2018-08-09T11:05:59.656709: step 935, loss 0.565893.
Train: 2018-08-09T11:05:59.734867: step 936, loss 0.682523.
Train: 2018-08-09T11:05:59.812956: step 937, loss 0.633327.
Train: 2018-08-09T11:05:59.891056: step 938, loss 0.653398.
Train: 2018-08-09T11:05:59.969138: step 939, loss 0.563143.
Train: 2018-08-09T11:06:00.047274: step 940, loss 0.546541.
Test: 2018-08-09T11:06:00.531529: step 940, loss 0.54857.
Train: 2018-08-09T11:06:00.609610: step 941, loss 0.6282.
Train: 2018-08-09T11:06:00.687747: step 942, loss 0.609841.
Train: 2018-08-09T11:06:00.765852: step 943, loss 0.547364.
Train: 2018-08-09T11:06:00.843961: step 944, loss 0.579494.
Train: 2018-08-09T11:06:00.924390: step 945, loss 0.53238.
Train: 2018-08-09T11:06:01.002482: step 946, loss 0.501117.
Train: 2018-08-09T11:06:01.080618: step 947, loss 0.563549.
Train: 2018-08-09T11:06:01.158694: step 948, loss 0.499827.
Train: 2018-08-09T11:06:01.236803: step 949, loss 0.515689.
Train: 2018-08-09T11:06:01.314940: step 950, loss 0.500618.
Test: 2018-08-09T11:06:01.830411: step 950, loss 0.551401.
Train: 2018-08-09T11:06:01.892897: step 951, loss 0.703755.
Train: 2018-08-09T11:06:01.986624: step 952, loss 0.580625.
Train: 2018-08-09T11:06:02.064764: step 953, loss 0.561835.
Train: 2018-08-09T11:06:02.142869: step 954, loss 0.610616.
Train: 2018-08-09T11:06:02.220975: step 955, loss 0.483997.
Train: 2018-08-09T11:06:02.299085: step 956, loss 0.578647.
Train: 2018-08-09T11:06:02.377182: step 957, loss 0.513856.
Train: 2018-08-09T11:06:02.455289: step 958, loss 0.686897.
Train: 2018-08-09T11:06:02.548994: step 959, loss 0.562719.
Train: 2018-08-09T11:06:02.627099: step 960, loss 0.515223.
Test: 2018-08-09T11:06:03.112676: step 960, loss 0.549463.
Train: 2018-08-09T11:06:03.190780: step 961, loss 0.469816.
Train: 2018-08-09T11:06:03.268888: step 962, loss 0.561427.
Train: 2018-08-09T11:06:03.346995: step 963, loss 0.466475.
Train: 2018-08-09T11:06:03.425128: step 964, loss 0.609531.
Train: 2018-08-09T11:06:03.518828: step 965, loss 0.56393.
Train: 2018-08-09T11:06:03.596936: step 966, loss 0.579957.
Train: 2018-08-09T11:06:03.675042: step 967, loss 0.565053.
Train: 2018-08-09T11:06:03.753173: step 968, loss 0.481514.
Train: 2018-08-09T11:06:03.831291: step 969, loss 0.614539.
Train: 2018-08-09T11:06:03.905991: step 970, loss 0.545085.
Test: 2018-08-09T11:06:04.405875: step 970, loss 0.548215.
Train: 2018-08-09T11:06:04.484010: step 971, loss 0.611776.
Train: 2018-08-09T11:06:04.562117: step 972, loss 0.629662.
Train: 2018-08-09T11:06:04.640195: step 973, loss 0.578179.
Train: 2018-08-09T11:06:04.718330: step 974, loss 0.549613.
Train: 2018-08-09T11:06:04.796438: step 975, loss 0.613901.
Train: 2018-08-09T11:06:04.874545: step 976, loss 0.530211.
Train: 2018-08-09T11:06:04.956939: step 977, loss 0.596468.
Train: 2018-08-09T11:06:05.035058: step 978, loss 0.531875.
Train: 2018-08-09T11:06:05.113162: step 979, loss 0.544777.
Train: 2018-08-09T11:06:05.191241: step 980, loss 0.592685.
Test: 2018-08-09T11:06:05.691123: step 980, loss 0.549231.
Train: 2018-08-09T11:06:05.769230: step 981, loss 0.497905.
Train: 2018-08-09T11:06:05.847335: step 982, loss 0.595124.
Train: 2018-08-09T11:06:05.925475: step 983, loss 0.545536.
Train: 2018-08-09T11:06:06.003550: step 984, loss 0.49762.
Train: 2018-08-09T11:06:06.081656: step 985, loss 0.529369.
Train: 2018-08-09T11:06:06.159764: step 986, loss 0.711297.
Train: 2018-08-09T11:06:06.237870: step 987, loss 0.563245.
Train: 2018-08-09T11:06:06.316005: step 988, loss 0.546923.
Train: 2018-08-09T11:06:06.394082: step 989, loss 0.596339.
Train: 2018-08-09T11:06:06.472218: step 990, loss 0.658457.
Test: 2018-08-09T11:06:06.972097: step 990, loss 0.547578.
Train: 2018-08-09T11:06:07.050178: step 991, loss 0.609126.
Train: 2018-08-09T11:06:07.128314: step 992, loss 0.544453.
Train: 2018-08-09T11:06:07.206423: step 993, loss 0.611597.
Train: 2018-08-09T11:06:07.284523: step 994, loss 0.501256.
Train: 2018-08-09T11:06:07.362636: step 995, loss 0.518485.
Train: 2018-08-09T11:06:07.456334: step 996, loss 0.499722.
Train: 2018-08-09T11:06:07.534441: step 997, loss 0.642326.
Train: 2018-08-09T11:06:07.612547: step 998, loss 0.578409.
Train: 2018-08-09T11:06:07.690684: step 999, loss 0.580284.
Train: 2018-08-09T11:06:07.768788: step 1000, loss 0.498736.
Test: 2018-08-09T11:06:08.271126: step 1000, loss 0.549408.
Train: 2018-08-09T11:06:08.849120: step 1001, loss 0.547627.
Train: 2018-08-09T11:06:08.927220: step 1002, loss 0.595931.
Train: 2018-08-09T11:06:09.020952: step 1003, loss 0.624369.
Train: 2018-08-09T11:06:09.099028: step 1004, loss 0.577807.
Train: 2018-08-09T11:06:09.177166: step 1005, loss 0.658239.
Train: 2018-08-09T11:06:09.255272: step 1006, loss 0.582318.
Train: 2018-08-09T11:06:09.333378: step 1007, loss 0.639938.
Train: 2018-08-09T11:06:09.411487: step 1008, loss 0.593159.
Train: 2018-08-09T11:06:09.489593: step 1009, loss 0.532581.
Train: 2018-08-09T11:06:09.567693: step 1010, loss 0.56444.
Test: 2018-08-09T11:06:10.053390: step 1010, loss 0.552619.
Train: 2018-08-09T11:06:10.131497: step 1011, loss 0.533775.
Train: 2018-08-09T11:06:10.209574: step 1012, loss 0.439602.
Train: 2018-08-09T11:06:10.287699: step 1013, loss 0.59388.
Train: 2018-08-09T11:06:10.365824: step 1014, loss 0.592348.
Train: 2018-08-09T11:06:10.459514: step 1015, loss 0.515463.
Train: 2018-08-09T11:06:10.537652: step 1016, loss 0.624091.
Train: 2018-08-09T11:06:10.615757: step 1017, loss 0.470369.
Train: 2018-08-09T11:06:10.693834: step 1018, loss 0.62717.
Train: 2018-08-09T11:06:10.771971: step 1019, loss 0.626361.
Train: 2018-08-09T11:06:10.850046: step 1020, loss 0.611101.
Test: 2018-08-09T11:06:11.349930: step 1020, loss 0.552088.
Train: 2018-08-09T11:06:11.428036: step 1021, loss 0.563705.
Train: 2018-08-09T11:06:11.506142: step 1022, loss 0.532512.
Train: 2018-08-09T11:06:11.584249: step 1023, loss 0.516071.
Train: 2018-08-09T11:06:11.662355: step 1024, loss 0.563261.
Train: 2018-08-09T11:06:11.740492: step 1025, loss 0.515743.
Train: 2018-08-09T11:06:11.818570: step 1026, loss 0.515348.
Train: 2018-08-09T11:06:11.896705: step 1027, loss 0.532262.
Train: 2018-08-09T11:06:11.976219: step 1028, loss 0.62811.
Train: 2018-08-09T11:06:12.054358: step 1029, loss 0.434654.
Train: 2018-08-09T11:06:12.132466: step 1030, loss 0.608889.
Test: 2018-08-09T11:06:12.632100: step 1030, loss 0.546889.
Train: 2018-08-09T11:06:12.710238: step 1031, loss 0.549019.
Train: 2018-08-09T11:06:12.772723: step 1032, loss 0.481519.
Train: 2018-08-09T11:06:12.866423: step 1033, loss 0.596492.
Train: 2018-08-09T11:06:12.944526: step 1034, loss 0.545601.
Train: 2018-08-09T11:06:13.022663: step 1035, loss 0.593284.
Train: 2018-08-09T11:06:13.100742: step 1036, loss 0.546792.
Train: 2018-08-09T11:06:13.178877: step 1037, loss 0.494829.
Train: 2018-08-09T11:06:13.256982: step 1038, loss 0.547478.
Train: 2018-08-09T11:06:13.335090: step 1039, loss 0.546883.
Train: 2018-08-09T11:06:13.413196: step 1040, loss 0.545911.
Test: 2018-08-09T11:06:13.915382: step 1040, loss 0.547084.
Train: 2018-08-09T11:06:13.993490: step 1041, loss 0.52705.
Train: 2018-08-09T11:06:14.071626: step 1042, loss 0.611478.
Train: 2018-08-09T11:06:14.149730: step 1043, loss 0.478632.
Train: 2018-08-09T11:06:14.227840: step 1044, loss 0.63361.
Train: 2018-08-09T11:06:14.305946: step 1045, loss 0.666397.
Train: 2018-08-09T11:06:14.384049: step 1046, loss 0.492382.
Train: 2018-08-09T11:06:14.462130: step 1047, loss 0.596978.
Train: 2018-08-09T11:06:14.540267: step 1048, loss 0.562756.
Train: 2018-08-09T11:06:14.618368: step 1049, loss 0.628989.
Train: 2018-08-09T11:06:14.696480: step 1050, loss 0.547873.
Test: 2018-08-09T11:06:15.180711: step 1050, loss 0.547081.
Train: 2018-08-09T11:06:15.258846: step 1051, loss 0.594867.
Train: 2018-08-09T11:06:15.352544: step 1052, loss 0.49255.
Train: 2018-08-09T11:06:15.430651: step 1053, loss 0.716159.
Train: 2018-08-09T11:06:15.508788: step 1054, loss 0.527381.
Train: 2018-08-09T11:06:15.586894: step 1055, loss 0.447624.
Train: 2018-08-09T11:06:15.665003: step 1056, loss 0.498463.
Train: 2018-08-09T11:06:15.743109: step 1057, loss 0.575818.
Train: 2018-08-09T11:06:15.821214: step 1058, loss 0.56317.
Train: 2018-08-09T11:06:15.899309: step 1059, loss 0.479616.
Train: 2018-08-09T11:06:15.982874: step 1060, loss 0.629209.
Test: 2018-08-09T11:06:16.475548: step 1060, loss 0.547408.
Train: 2018-08-09T11:06:16.553654: step 1061, loss 0.56334.
Train: 2018-08-09T11:06:16.631791: step 1062, loss 0.59486.
Train: 2018-08-09T11:06:16.709870: step 1063, loss 0.594347.
Train: 2018-08-09T11:06:16.788004: step 1064, loss 0.496997.
Train: 2018-08-09T11:06:16.866105: step 1065, loss 0.678921.
Train: 2018-08-09T11:06:16.944188: step 1066, loss 0.61121.
Train: 2018-08-09T11:06:17.022324: step 1067, loss 0.464062.
Train: 2018-08-09T11:06:17.100432: step 1068, loss 0.481331.
Train: 2018-08-09T11:06:17.178508: step 1069, loss 0.51362.
Train: 2018-08-09T11:06:17.256643: step 1070, loss 0.531072.
Test: 2018-08-09T11:06:17.756497: step 1070, loss 0.550109.
Train: 2018-08-09T11:06:17.834602: step 1071, loss 0.562173.
Train: 2018-08-09T11:06:17.916035: step 1072, loss 0.543458.
Train: 2018-08-09T11:06:17.980625: step 1073, loss 0.52852.
Train: 2018-08-09T11:06:18.058732: step 1074, loss 0.630198.
Train: 2018-08-09T11:06:18.136839: step 1075, loss 0.628805.
Train: 2018-08-09T11:06:18.214915: step 1076, loss 0.561379.
Train: 2018-08-09T11:06:18.293048: step 1077, loss 0.594805.
Train: 2018-08-09T11:06:18.371158: step 1078, loss 0.659731.
Train: 2018-08-09T11:06:18.464882: step 1079, loss 0.463643.
Train: 2018-08-09T11:06:18.542994: step 1080, loss 0.577255.
Test: 2018-08-09T11:06:19.027223: step 1080, loss 0.54766.
Train: 2018-08-09T11:06:19.105363: step 1081, loss 0.496066.
Train: 2018-08-09T11:06:19.183467: step 1082, loss 0.49656.
Train: 2018-08-09T11:06:19.261546: step 1083, loss 0.544618.
Train: 2018-08-09T11:06:19.339682: step 1084, loss 0.497961.
Train: 2018-08-09T11:06:19.417793: step 1085, loss 0.54375.
Train: 2018-08-09T11:06:19.495864: step 1086, loss 0.630297.
Train: 2018-08-09T11:06:19.573994: step 1087, loss 0.64822.
Train: 2018-08-09T11:06:19.652106: step 1088, loss 0.545725.
Train: 2018-08-09T11:06:19.730184: step 1089, loss 0.560929.
Train: 2018-08-09T11:06:19.808321: step 1090, loss 0.545222.
Test: 2018-08-09T11:06:20.308203: step 1090, loss 0.548624.
Train: 2018-08-09T11:06:20.386302: step 1091, loss 0.530112.
Train: 2018-08-09T11:06:20.464415: step 1092, loss 0.581929.
Train: 2018-08-09T11:06:20.542491: step 1093, loss 0.729766.
Train: 2018-08-09T11:06:20.620597: step 1094, loss 0.626066.
Train: 2018-08-09T11:06:20.698740: step 1095, loss 0.696769.
Train: 2018-08-09T11:06:20.776844: step 1096, loss 0.577669.
Train: 2018-08-09T11:06:20.854947: step 1097, loss 0.611691.
Train: 2018-08-09T11:06:20.934424: step 1098, loss 0.561486.
Train: 2018-08-09T11:06:21.012500: step 1099, loss 0.529313.
Train: 2018-08-09T11:06:21.090636: step 1100, loss 0.611394.
Test: 2018-08-09T11:06:21.590489: step 1100, loss 0.54793.
Train: 2018-08-09T11:06:22.152854: step 1101, loss 0.609611.
Train: 2018-08-09T11:06:22.230992: step 1102, loss 0.594765.
Train: 2018-08-09T11:06:22.309100: step 1103, loss 0.579898.
Train: 2018-08-09T11:06:22.387210: step 1104, loss 0.562645.
Train: 2018-08-09T11:06:22.465312: step 1105, loss 0.621842.
Train: 2018-08-09T11:06:22.543388: step 1106, loss 0.548803.
Train: 2018-08-09T11:06:22.621525: step 1107, loss 0.579113.
Train: 2018-08-09T11:06:22.699601: step 1108, loss 0.701101.
Train: 2018-08-09T11:06:22.777737: step 1109, loss 0.549257.
Train: 2018-08-09T11:06:22.855844: step 1110, loss 0.506585.
Test: 2018-08-09T11:06:23.358041: step 1110, loss 0.551152.
Train: 2018-08-09T11:06:23.436184: step 1111, loss 0.489914.
Train: 2018-08-09T11:06:23.514285: step 1112, loss 0.550339.
Train: 2018-08-09T11:06:23.592393: step 1113, loss 0.639766.
Train: 2018-08-09T11:06:23.670492: step 1114, loss 0.53509.
Train: 2018-08-09T11:06:23.748604: step 1115, loss 0.550492.
Train: 2018-08-09T11:06:23.826682: step 1116, loss 0.608642.
Train: 2018-08-09T11:06:23.904819: step 1117, loss 0.610175.
Train: 2018-08-09T11:06:23.982924: step 1118, loss 0.504922.
Train: 2018-08-09T11:06:24.061001: step 1119, loss 0.53537.
Train: 2018-08-09T11:06:24.139107: step 1120, loss 0.593753.
Test: 2018-08-09T11:06:24.638991: step 1120, loss 0.550147.
Train: 2018-08-09T11:06:24.717126: step 1121, loss 0.533664.
Train: 2018-08-09T11:06:24.795239: step 1122, loss 0.565839.
Train: 2018-08-09T11:06:24.873341: step 1123, loss 0.519054.
Train: 2018-08-09T11:06:24.952132: step 1124, loss 0.518135.
Train: 2018-08-09T11:06:25.030268: step 1125, loss 0.57912.
Train: 2018-08-09T11:06:25.108376: step 1126, loss 0.626065.
Train: 2018-08-09T11:06:25.186452: step 1127, loss 0.562893.
Train: 2018-08-09T11:06:25.264558: step 1128, loss 0.65528.
Train: 2018-08-09T11:06:25.342691: step 1129, loss 0.580982.
Train: 2018-08-09T11:06:25.420772: step 1130, loss 0.577698.
Test: 2018-08-09T11:06:25.905057: step 1130, loss 0.551408.
Train: 2018-08-09T11:06:25.983169: step 1131, loss 0.548993.
Train: 2018-08-09T11:06:26.061276: step 1132, loss 0.439636.
Train: 2018-08-09T11:06:26.139355: step 1133, loss 0.50139.
Train: 2018-08-09T11:06:26.217489: step 1134, loss 0.56425.
Train: 2018-08-09T11:06:26.295566: step 1135, loss 0.514352.
Train: 2018-08-09T11:06:26.373703: step 1136, loss 0.467642.
Train: 2018-08-09T11:06:26.456865: step 1137, loss 0.562073.
Train: 2018-08-09T11:06:26.534978: step 1138, loss 0.531648.
Train: 2018-08-09T11:06:26.613084: step 1139, loss 0.642901.
Train: 2018-08-09T11:06:26.691192: step 1140, loss 0.627705.
Test: 2018-08-09T11:06:27.191043: step 1140, loss 0.549073.
Train: 2018-08-09T11:06:27.269174: step 1141, loss 0.448294.
Train: 2018-08-09T11:06:27.347257: step 1142, loss 0.582012.
Train: 2018-08-09T11:06:27.425389: step 1143, loss 0.575366.
Train: 2018-08-09T11:06:27.503502: step 1144, loss 0.576881.
Train: 2018-08-09T11:06:27.581576: step 1145, loss 0.52911.
Train: 2018-08-09T11:06:27.659685: step 1146, loss 0.594381.
Train: 2018-08-09T11:06:27.737819: step 1147, loss 0.528003.
Train: 2018-08-09T11:06:27.815897: step 1148, loss 0.596093.
Train: 2018-08-09T11:06:27.894004: step 1149, loss 0.546174.
Train: 2018-08-09T11:06:27.972110: step 1150, loss 0.444476.
Test: 2018-08-09T11:06:28.471992: step 1150, loss 0.545762.
Train: 2018-08-09T11:06:28.550128: step 1151, loss 0.560134.
Train: 2018-08-09T11:06:28.628237: step 1152, loss 0.460925.
Train: 2018-08-09T11:06:28.706340: step 1153, loss 0.613959.
Train: 2018-08-09T11:06:28.784418: step 1154, loss 0.580591.
Train: 2018-08-09T11:06:28.862554: step 1155, loss 0.513826.
Train: 2018-08-09T11:06:28.940658: step 1156, loss 0.547344.
Train: 2018-08-09T11:06:29.018768: step 1157, loss 0.547076.
Train: 2018-08-09T11:06:29.096874: step 1158, loss 0.546133.
Train: 2018-08-09T11:06:29.174977: step 1159, loss 0.602748.
Train: 2018-08-09T11:06:29.253057: step 1160, loss 0.618182.
Test: 2018-08-09T11:06:29.752941: step 1160, loss 0.546717.
Train: 2018-08-09T11:06:29.831077: step 1161, loss 0.57741.
Train: 2018-08-09T11:06:29.911575: step 1162, loss 0.56148.
Train: 2018-08-09T11:06:29.989683: step 1163, loss 0.513008.
Train: 2018-08-09T11:06:30.067788: step 1164, loss 0.594914.
Train: 2018-08-09T11:06:30.145924: step 1165, loss 0.647898.
Train: 2018-08-09T11:06:30.224002: step 1166, loss 0.59401.
Train: 2018-08-09T11:06:30.302139: step 1167, loss 0.615215.
Train: 2018-08-09T11:06:30.380244: step 1168, loss 0.613628.
Train: 2018-08-09T11:06:30.458353: step 1169, loss 0.513009.
Train: 2018-08-09T11:06:30.536459: step 1170, loss 0.56228.
Test: 2018-08-09T11:06:31.036310: step 1170, loss 0.549536.
Train: 2018-08-09T11:06:31.114447: step 1171, loss 0.529766.
Train: 2018-08-09T11:06:31.192554: step 1172, loss 0.531395.
Train: 2018-08-09T11:06:31.270658: step 1173, loss 0.596293.
Train: 2018-08-09T11:06:31.348737: step 1174, loss 0.61426.
Train: 2018-08-09T11:06:31.426874: step 1175, loss 0.628976.
Train: 2018-08-09T11:06:31.504951: step 1176, loss 0.417712.
Train: 2018-08-09T11:06:31.583087: step 1177, loss 0.576592.
Train: 2018-08-09T11:06:31.661194: step 1178, loss 0.579234.
Train: 2018-08-09T11:06:31.739304: step 1179, loss 0.674775.
Train: 2018-08-09T11:06:31.817401: step 1180, loss 0.51476.
Test: 2018-08-09T11:06:32.319543: step 1180, loss 0.548001.
Train: 2018-08-09T11:06:32.397678: step 1181, loss 0.548109.
Train: 2018-08-09T11:06:32.475755: step 1182, loss 0.499632.
Train: 2018-08-09T11:06:32.553863: step 1183, loss 0.404318.
Train: 2018-08-09T11:06:32.631999: step 1184, loss 0.57876.
Train: 2018-08-09T11:06:32.710106: step 1185, loss 0.677023.
Train: 2018-08-09T11:06:32.788199: step 1186, loss 0.546784.
Train: 2018-08-09T11:06:32.866288: step 1187, loss 0.563702.
Train: 2018-08-09T11:06:32.944395: step 1188, loss 0.64237.
Train: 2018-08-09T11:06:33.022546: step 1189, loss 0.562058.
Train: 2018-08-09T11:06:33.100638: step 1190, loss 0.514862.
Test: 2018-08-09T11:06:33.600522: step 1190, loss 0.550233.
Train: 2018-08-09T11:06:33.678628: step 1191, loss 0.563027.
Train: 2018-08-09T11:06:33.756733: step 1192, loss 0.548836.
Train: 2018-08-09T11:06:33.834810: step 1193, loss 0.577567.
Train: 2018-08-09T11:06:33.914265: step 1194, loss 0.532663.
Train: 2018-08-09T11:06:33.992372: step 1195, loss 0.610975.
Train: 2018-08-09T11:06:34.070505: step 1196, loss 0.497705.
Train: 2018-08-09T11:06:34.148616: step 1197, loss 0.530664.
Train: 2018-08-09T11:06:34.226721: step 1198, loss 0.627176.
Train: 2018-08-09T11:06:34.304834: step 1199, loss 0.514743.
Train: 2018-08-09T11:06:34.382937: step 1200, loss 0.561799.
Test: 2018-08-09T11:06:34.867191: step 1200, loss 0.545673.
Train: 2018-08-09T11:06:35.398322: step 1201, loss 0.578942.
Train: 2018-08-09T11:06:35.476397: step 1202, loss 0.530237.
Train: 2018-08-09T11:06:35.554529: step 1203, loss 0.612433.
Train: 2018-08-09T11:06:35.632642: step 1204, loss 0.543622.
Train: 2018-08-09T11:06:35.710716: step 1205, loss 0.578573.
Train: 2018-08-09T11:06:35.788859: step 1206, loss 0.596462.
Train: 2018-08-09T11:06:35.866960: step 1207, loss 0.563431.
Train: 2018-08-09T11:06:35.947370: step 1208, loss 0.547518.
Train: 2018-08-09T11:06:36.025481: step 1209, loss 0.660538.
Train: 2018-08-09T11:06:36.103583: step 1210, loss 0.580186.
Test: 2018-08-09T11:06:36.603437: step 1210, loss 0.550012.
Train: 2018-08-09T11:06:36.681573: step 1211, loss 0.433635.
Train: 2018-08-09T11:06:36.759681: step 1212, loss 0.5303.
Train: 2018-08-09T11:06:36.837788: step 1213, loss 0.581694.
Train: 2018-08-09T11:06:36.915861: step 1214, loss 0.593286.
Train: 2018-08-09T11:06:36.993969: step 1215, loss 0.577924.
Train: 2018-08-09T11:06:37.072107: step 1216, loss 0.609426.
Train: 2018-08-09T11:06:37.150181: step 1217, loss 0.482956.
Train: 2018-08-09T11:06:37.228287: step 1218, loss 0.611341.
Train: 2018-08-09T11:06:37.306429: step 1219, loss 0.513121.
Train: 2018-08-09T11:06:37.384531: step 1220, loss 0.54756.
Test: 2018-08-09T11:06:37.884392: step 1220, loss 0.549302.
Train: 2018-08-09T11:06:37.949203: step 1221, loss 0.6287.
Train: 2018-08-09T11:06:38.027302: step 1222, loss 0.594906.
Train: 2018-08-09T11:06:38.105413: step 1223, loss 0.545979.
Train: 2018-08-09T11:06:38.183491: step 1224, loss 0.578282.
Train: 2018-08-09T11:06:38.261596: step 1225, loss 0.629203.
Train: 2018-08-09T11:06:38.355354: step 1226, loss 0.482583.
Train: 2018-08-09T11:06:38.433430: step 1227, loss 0.545654.
Train: 2018-08-09T11:06:38.511567: step 1228, loss 0.562623.
Train: 2018-08-09T11:06:38.589675: step 1229, loss 0.593798.
Train: 2018-08-09T11:06:38.667780: step 1230, loss 0.577597.
Test: 2018-08-09T11:06:39.152015: step 1230, loss 0.549463.
Train: 2018-08-09T11:06:39.230141: step 1231, loss 0.530841.
Train: 2018-08-09T11:06:39.308254: step 1232, loss 0.578229.
Train: 2018-08-09T11:06:39.386364: step 1233, loss 0.578661.
Train: 2018-08-09T11:06:39.464437: step 1234, loss 0.51469.
Train: 2018-08-09T11:06:39.542574: step 1235, loss 0.514332.
Train: 2018-08-09T11:06:39.620652: step 1236, loss 0.563894.
Train: 2018-08-09T11:06:39.698787: step 1237, loss 0.562818.
Train: 2018-08-09T11:06:39.776890: step 1238, loss 0.53003.
Train: 2018-08-09T11:06:39.855003: step 1239, loss 0.532102.
Train: 2018-08-09T11:06:39.935529: step 1240, loss 0.594393.
Test: 2018-08-09T11:06:40.435792: step 1240, loss 0.550969.
Train: 2018-08-09T11:06:40.513898: step 1241, loss 0.628467.
Train: 2018-08-09T11:06:40.592004: step 1242, loss 0.547004.
Train: 2018-08-09T11:06:40.670114: step 1243, loss 0.644727.
Train: 2018-08-09T11:06:40.748219: step 1244, loss 0.545463.
Train: 2018-08-09T11:06:40.826358: step 1245, loss 0.528534.
Train: 2018-08-09T11:06:40.904432: step 1246, loss 0.482157.
Train: 2018-08-09T11:06:40.982562: step 1247, loss 0.545978.
Train: 2018-08-09T11:06:41.060644: step 1248, loss 0.497214.
Train: 2018-08-09T11:06:41.138782: step 1249, loss 0.563128.
Train: 2018-08-09T11:06:41.216889: step 1250, loss 0.577685.
Test: 2018-08-09T11:06:41.716741: step 1250, loss 0.547023.
Train: 2018-08-09T11:06:41.794877: step 1251, loss 0.596653.
Train: 2018-08-09T11:06:41.872984: step 1252, loss 0.481356.
Train: 2018-08-09T11:06:41.951061: step 1253, loss 0.56323.
Train: 2018-08-09T11:06:42.029167: step 1254, loss 0.579581.
Train: 2018-08-09T11:06:42.107274: step 1255, loss 0.496873.
Train: 2018-08-09T11:06:42.185411: step 1256, loss 0.646124.
Train: 2018-08-09T11:06:42.263487: step 1257, loss 0.530177.
Train: 2018-08-09T11:06:42.341623: step 1258, loss 0.546647.
Train: 2018-08-09T11:06:42.419700: step 1259, loss 0.531843.
Train: 2018-08-09T11:06:42.497840: step 1260, loss 0.613839.
Test: 2018-08-09T11:06:42.985736: step 1260, loss 0.546742.
Train: 2018-08-09T11:06:43.063828: step 1261, loss 0.513023.
Train: 2018-08-09T11:06:43.141959: step 1262, loss 0.678534.
Train: 2018-08-09T11:06:43.220047: step 1263, loss 0.561219.
Train: 2018-08-09T11:06:43.298177: step 1264, loss 0.64567.
Train: 2018-08-09T11:06:43.376253: step 1265, loss 0.580815.
Train: 2018-08-09T11:06:43.454399: step 1266, loss 0.645588.
Train: 2018-08-09T11:06:43.532469: step 1267, loss 0.496842.
Train: 2018-08-09T11:06:43.610604: step 1268, loss 0.545758.
Train: 2018-08-09T11:06:43.688706: step 1269, loss 0.51367.
Train: 2018-08-09T11:06:43.766816: step 1270, loss 0.530544.
Test: 2018-08-09T11:06:44.264366: step 1270, loss 0.549199.
Train: 2018-08-09T11:06:44.342473: step 1271, loss 0.529721.
Train: 2018-08-09T11:06:44.420579: step 1272, loss 0.546688.
Train: 2018-08-09T11:06:44.498712: step 1273, loss 0.497904.
Train: 2018-08-09T11:06:44.576792: step 1274, loss 0.465717.
Train: 2018-08-09T11:06:44.654898: step 1275, loss 0.547142.
Train: 2018-08-09T11:06:44.733040: step 1276, loss 0.512475.
Train: 2018-08-09T11:06:44.811143: step 1277, loss 0.49599.
Train: 2018-08-09T11:06:44.889218: step 1278, loss 0.59512.
Train: 2018-08-09T11:06:44.967357: step 1279, loss 0.597336.
Train: 2018-08-09T11:06:45.045462: step 1280, loss 0.796149.
Test: 2018-08-09T11:06:45.545314: step 1280, loss 0.547401.
Train: 2018-08-09T11:06:45.623450: step 1281, loss 0.628822.
Train: 2018-08-09T11:06:45.701557: step 1282, loss 0.546403.
Train: 2018-08-09T11:06:45.779663: step 1283, loss 0.612092.
Train: 2018-08-09T11:06:45.857770: step 1284, loss 0.497496.
Train: 2018-08-09T11:06:45.935877: step 1285, loss 0.528122.
Train: 2018-08-09T11:06:46.013988: step 1286, loss 0.514243.
Train: 2018-08-09T11:06:46.092091: step 1287, loss 0.57903.
Train: 2018-08-09T11:06:46.170199: step 1288, loss 0.547144.
Train: 2018-08-09T11:06:46.248304: step 1289, loss 0.514147.
Train: 2018-08-09T11:06:46.326410: step 1290, loss 0.447539.
Test: 2018-08-09T11:06:46.825835: step 1290, loss 0.547714.
Train: 2018-08-09T11:06:46.903975: step 1291, loss 0.545691.
Train: 2018-08-09T11:06:46.982049: step 1292, loss 0.563844.
Train: 2018-08-09T11:06:47.060157: step 1293, loss 0.446763.
Train: 2018-08-09T11:06:47.138262: step 1294, loss 0.528808.
Train: 2018-08-09T11:06:47.216371: step 1295, loss 0.528915.
Train: 2018-08-09T11:06:47.294477: step 1296, loss 0.645942.
Train: 2018-08-09T11:06:47.372582: step 1297, loss 0.512607.
Train: 2018-08-09T11:06:47.450720: step 1298, loss 0.647929.
Train: 2018-08-09T11:06:47.528797: step 1299, loss 0.596695.
Train: 2018-08-09T11:06:47.606932: step 1300, loss 0.595306.
Test: 2018-08-09T11:06:48.091164: step 1300, loss 0.549085.
Train: 2018-08-09T11:06:48.653563: step 1301, loss 0.596514.
Train: 2018-08-09T11:06:48.731667: step 1302, loss 0.528179.
Train: 2018-08-09T11:06:48.809745: step 1303, loss 0.494281.
Train: 2018-08-09T11:06:48.887880: step 1304, loss 0.477316.
Train: 2018-08-09T11:06:48.968357: step 1305, loss 0.580893.
Train: 2018-08-09T11:06:49.046431: step 1306, loss 0.545321.
Train: 2018-08-09T11:06:49.124566: step 1307, loss 0.597165.
Train: 2018-08-09T11:06:49.202673: step 1308, loss 0.647241.
Train: 2018-08-09T11:06:49.280775: step 1309, loss 0.613044.
Train: 2018-08-09T11:06:49.358886: step 1310, loss 0.529356.
Test: 2018-08-09T11:06:49.858738: step 1310, loss 0.548529.
Train: 2018-08-09T11:06:49.936869: step 1311, loss 0.596297.
Train: 2018-08-09T11:06:50.014981: step 1312, loss 0.612568.
Train: 2018-08-09T11:06:50.093088: step 1313, loss 0.495468.
Train: 2018-08-09T11:06:50.171198: step 1314, loss 0.579157.
Train: 2018-08-09T11:06:50.249270: step 1315, loss 0.59521.
Train: 2018-08-09T11:06:50.327379: step 1316, loss 0.529908.
Train: 2018-08-09T11:06:50.405514: step 1317, loss 0.579377.
Train: 2018-08-09T11:06:50.483622: step 1318, loss 0.496957.
Train: 2018-08-09T11:06:50.561733: step 1319, loss 0.529421.
Train: 2018-08-09T11:06:50.639834: step 1320, loss 0.547394.
Test: 2018-08-09T11:06:51.124659: step 1320, loss 0.548902.
Train: 2018-08-09T11:06:51.202770: step 1321, loss 0.578853.
Train: 2018-08-09T11:06:51.280876: step 1322, loss 0.495822.
Train: 2018-08-09T11:06:51.358955: step 1323, loss 0.529095.
Train: 2018-08-09T11:06:51.437087: step 1324, loss 0.513634.
Train: 2018-08-09T11:06:51.515197: step 1325, loss 0.59646.
Train: 2018-08-09T11:06:51.608907: step 1326, loss 0.562413.
Train: 2018-08-09T11:06:51.671381: step 1327, loss 0.511596.
Train: 2018-08-09T11:06:51.749487: step 1328, loss 0.478974.
Train: 2018-08-09T11:06:51.827594: step 1329, loss 0.596283.
Train: 2018-08-09T11:06:51.905699: step 1330, loss 0.663177.
Test: 2018-08-09T11:06:52.405583: step 1330, loss 0.547947.
Train: 2018-08-09T11:06:52.483719: step 1331, loss 0.579623.
Train: 2018-08-09T11:06:52.561796: step 1332, loss 0.613252.
Train: 2018-08-09T11:06:52.639933: step 1333, loss 0.595502.
Train: 2018-08-09T11:06:52.718039: step 1334, loss 0.579379.
Train: 2018-08-09T11:06:52.796116: step 1335, loss 0.661991.
Train: 2018-08-09T11:06:52.874253: step 1336, loss 0.562735.
Train: 2018-08-09T11:06:52.952358: step 1337, loss 0.546146.
Train: 2018-08-09T11:06:53.030465: step 1338, loss 0.645646.
Train: 2018-08-09T11:06:53.108568: step 1339, loss 0.546252.
Train: 2018-08-09T11:06:53.186649: step 1340, loss 0.660235.
Test: 2018-08-09T11:06:53.686531: step 1340, loss 0.546297.
Train: 2018-08-09T11:06:53.764661: step 1341, loss 0.626252.
Train: 2018-08-09T11:06:53.842776: step 1342, loss 0.498691.
Train: 2018-08-09T11:06:53.922261: step 1343, loss 0.546725.
Train: 2018-08-09T11:06:54.000338: step 1344, loss 0.514326.
Train: 2018-08-09T11:06:54.078473: step 1345, loss 0.483186.
Train: 2018-08-09T11:06:54.156550: step 1346, loss 0.563706.
Train: 2018-08-09T11:06:54.234686: step 1347, loss 0.562359.
Train: 2018-08-09T11:06:54.312794: step 1348, loss 0.515146.
Train: 2018-08-09T11:06:54.390903: step 1349, loss 0.578403.
Train: 2018-08-09T11:06:54.468976: step 1350, loss 0.482989.
Test: 2018-08-09T11:06:54.953262: step 1350, loss 0.549636.
Train: 2018-08-09T11:06:55.031374: step 1351, loss 0.611022.
Train: 2018-08-09T11:06:55.109480: step 1352, loss 0.595499.
Train: 2018-08-09T11:06:55.203178: step 1353, loss 0.627272.
Train: 2018-08-09T11:06:55.265664: step 1354, loss 0.643253.
Train: 2018-08-09T11:06:55.343771: step 1355, loss 0.482667.
Train: 2018-08-09T11:06:55.437501: step 1356, loss 0.706367.
Train: 2018-08-09T11:06:55.500016: step 1357, loss 0.595071.
Train: 2018-08-09T11:06:55.578121: step 1358, loss 0.530839.
Train: 2018-08-09T11:06:55.671849: step 1359, loss 0.483467.
Train: 2018-08-09T11:06:55.734304: step 1360, loss 0.531434.
Test: 2018-08-09T11:06:56.235600: step 1360, loss 0.548658.
Train: 2018-08-09T11:06:56.313729: step 1361, loss 0.515227.
Train: 2018-08-09T11:06:56.391831: step 1362, loss 0.594793.
Train: 2018-08-09T11:06:56.469914: step 1363, loss 0.467437.
Train: 2018-08-09T11:06:56.548020: step 1364, loss 0.466763.
Train: 2018-08-09T11:06:56.626156: step 1365, loss 0.579051.
Train: 2018-08-09T11:06:56.704234: step 1366, loss 0.498445.
Train: 2018-08-09T11:06:56.782369: step 1367, loss 0.513961.
Train: 2018-08-09T11:06:56.860447: step 1368, loss 0.561655.
Train: 2018-08-09T11:06:56.938553: step 1369, loss 0.480575.
Train: 2018-08-09T11:06:57.016658: step 1370, loss 0.546281.
Test: 2018-08-09T11:06:57.515528: step 1370, loss 0.550694.
Train: 2018-08-09T11:06:57.593670: step 1371, loss 0.54625.
Train: 2018-08-09T11:06:57.671769: step 1372, loss 0.595341.
Train: 2018-08-09T11:06:57.749846: step 1373, loss 0.580048.
Train: 2018-08-09T11:06:57.827982: step 1374, loss 0.578687.
Train: 2018-08-09T11:06:57.909457: step 1375, loss 0.562872.
Train: 2018-08-09T11:06:57.987565: step 1376, loss 0.595598.
Train: 2018-08-09T11:06:58.065701: step 1377, loss 0.477939.
Train: 2018-08-09T11:06:58.143808: step 1378, loss 0.562148.
Train: 2018-08-09T11:06:58.221910: step 1379, loss 0.528504.
Train: 2018-08-09T11:06:58.300020: step 1380, loss 0.51094.
Test: 2018-08-09T11:06:58.784278: step 1380, loss 0.549573.
Train: 2018-08-09T11:06:58.862388: step 1381, loss 0.596416.
Train: 2018-08-09T11:06:58.940495: step 1382, loss 0.528573.
Train: 2018-08-09T11:06:59.018573: step 1383, loss 0.562783.
Train: 2018-08-09T11:06:59.096708: step 1384, loss 0.563116.
Train: 2018-08-09T11:06:59.174821: step 1385, loss 0.510176.
Train: 2018-08-09T11:06:59.252925: step 1386, loss 0.613756.
Train: 2018-08-09T11:06:59.331028: step 1387, loss 0.510518.
Train: 2018-08-09T11:06:59.409105: step 1388, loss 0.563016.
Train: 2018-08-09T11:06:59.487237: step 1389, loss 0.632025.
Train: 2018-08-09T11:06:59.565343: step 1390, loss 0.579622.
Test: 2018-08-09T11:07:00.066504: step 1390, loss 0.549418.
Train: 2018-08-09T11:07:00.144609: step 1391, loss 0.544855.
Train: 2018-08-09T11:07:00.222684: step 1392, loss 0.51127.
Train: 2018-08-09T11:07:00.300823: step 1393, loss 0.581273.
Train: 2018-08-09T11:07:00.378929: step 1394, loss 0.564043.
Train: 2018-08-09T11:07:00.457034: step 1395, loss 0.459522.
Train: 2018-08-09T11:07:00.535164: step 1396, loss 0.612824.
Train: 2018-08-09T11:07:00.613217: step 1397, loss 0.614982.
Train: 2018-08-09T11:07:00.691355: step 1398, loss 0.562215.
Train: 2018-08-09T11:07:00.769456: step 1399, loss 0.579765.
Train: 2018-08-09T11:07:00.847537: step 1400, loss 0.544809.
Test: 2018-08-09T11:07:01.347420: step 1400, loss 0.548266.
Train: 2018-08-09T11:07:01.912068: step 1401, loss 0.529812.
Train: 2018-08-09T11:07:01.990207: step 1402, loss 0.47714.
Train: 2018-08-09T11:07:02.083935: step 1403, loss 0.581136.
Train: 2018-08-09T11:07:02.162044: step 1404, loss 0.494115.
Train: 2018-08-09T11:07:02.240146: step 1405, loss 0.494753.
Train: 2018-08-09T11:07:02.318227: step 1406, loss 0.545563.
Train: 2018-08-09T11:07:02.396360: step 1407, loss 0.699664.
Train: 2018-08-09T11:07:02.474436: step 1408, loss 0.546478.
Train: 2018-08-09T11:07:02.552545: step 1409, loss 0.579572.
Train: 2018-08-09T11:07:02.630681: step 1410, loss 0.612631.
Test: 2018-08-09T11:07:03.114911: step 1410, loss 0.546542.
Train: 2018-08-09T11:07:03.193047: step 1411, loss 0.612119.
Train: 2018-08-09T11:07:03.271153: step 1412, loss 0.578453.
Train: 2018-08-09T11:07:03.349264: step 1413, loss 0.546135.
Train: 2018-08-09T11:07:03.427338: step 1414, loss 0.579171.
Train: 2018-08-09T11:07:03.505444: step 1415, loss 0.561609.
Train: 2018-08-09T11:07:03.583552: step 1416, loss 0.578968.
Train: 2018-08-09T11:07:03.661656: step 1417, loss 0.512569.
Train: 2018-08-09T11:07:03.739795: step 1418, loss 0.546349.
Train: 2018-08-09T11:07:03.817871: step 1419, loss 0.727696.
Train: 2018-08-09T11:07:03.896007: step 1420, loss 0.563118.
Test: 2018-08-09T11:07:04.397350: step 1420, loss 0.548462.
Train: 2018-08-09T11:07:04.475457: step 1421, loss 0.529487.
Train: 2018-08-09T11:07:04.553564: step 1422, loss 0.562887.
Train: 2018-08-09T11:07:04.631704: step 1423, loss 0.514012.
Train: 2018-08-09T11:07:04.709777: step 1424, loss 0.627357.
Train: 2018-08-09T11:07:04.787915: step 1425, loss 0.643303.
Train: 2018-08-09T11:07:04.881645: step 1426, loss 0.611406.
Train: 2018-08-09T11:07:04.959742: step 1427, loss 0.643118.
Train: 2018-08-09T11:07:05.037824: step 1428, loss 0.626642.
Train: 2018-08-09T11:07:05.115960: step 1429, loss 0.594973.
Train: 2018-08-09T11:07:05.194038: step 1430, loss 0.531046.
Test: 2018-08-09T11:07:05.693919: step 1430, loss 0.549567.
Train: 2018-08-09T11:07:05.772057: step 1431, loss 0.6262.
Train: 2018-08-09T11:07:05.850157: step 1432, loss 0.578354.
Train: 2018-08-09T11:07:05.929667: step 1433, loss 0.548447.
Train: 2018-08-09T11:07:06.007772: step 1434, loss 0.501154.
Train: 2018-08-09T11:07:06.085879: step 1435, loss 0.547949.
Train: 2018-08-09T11:07:06.164011: step 1436, loss 0.594509.
Train: 2018-08-09T11:07:06.242091: step 1437, loss 0.548091.
Train: 2018-08-09T11:07:06.320230: step 1438, loss 0.548199.
Train: 2018-08-09T11:07:06.398306: step 1439, loss 0.64024.
Train: 2018-08-09T11:07:06.476411: step 1440, loss 0.502264.
Test: 2018-08-09T11:07:06.976294: step 1440, loss 0.549098.
Train: 2018-08-09T11:07:07.054434: step 1441, loss 0.517311.
Train: 2018-08-09T11:07:07.132508: step 1442, loss 0.532723.
Train: 2018-08-09T11:07:07.210641: step 1443, loss 0.56314.
Train: 2018-08-09T11:07:07.288750: step 1444, loss 0.594573.
Train: 2018-08-09T11:07:07.366828: step 1445, loss 0.516771.
Train: 2018-08-09T11:07:07.444966: step 1446, loss 0.579285.
Train: 2018-08-09T11:07:07.523042: step 1447, loss 0.578722.
Train: 2018-08-09T11:07:07.601146: step 1448, loss 0.579719.
Train: 2018-08-09T11:07:07.679280: step 1449, loss 0.625754.
Train: 2018-08-09T11:07:07.757390: step 1450, loss 0.65712.
Test: 2018-08-09T11:07:08.243922: step 1450, loss 0.54983.
Train: 2018-08-09T11:07:08.322056: step 1451, loss 0.671845.
Train: 2018-08-09T11:07:08.400167: step 1452, loss 0.517017.
Train: 2018-08-09T11:07:08.478267: step 1453, loss 0.656579.
Train: 2018-08-09T11:07:08.556380: step 1454, loss 0.594248.
Train: 2018-08-09T11:07:08.634484: step 1455, loss 0.548149.
Train: 2018-08-09T11:07:08.712560: step 1456, loss 0.548236.
Train: 2018-08-09T11:07:08.790700: step 1457, loss 0.579145.
Train: 2018-08-09T11:07:08.868804: step 1458, loss 0.532951.
Train: 2018-08-09T11:07:08.946909: step 1459, loss 0.594334.
Train: 2018-08-09T11:07:09.025018: step 1460, loss 0.579049.
Test: 2018-08-09T11:07:09.524869: step 1460, loss 0.54996.
Train: 2018-08-09T11:07:09.603006: step 1461, loss 0.609535.
Train: 2018-08-09T11:07:09.681112: step 1462, loss 0.533121.
Train: 2018-08-09T11:07:09.759221: step 1463, loss 0.609206.
Train: 2018-08-09T11:07:09.837325: step 1464, loss 0.640072.
Train: 2018-08-09T11:07:09.916751: step 1465, loss 0.518294.
Train: 2018-08-09T11:07:09.994860: step 1466, loss 0.594364.
Train: 2018-08-09T11:07:10.072965: step 1467, loss 0.563823.
Train: 2018-08-09T11:07:10.151072: step 1468, loss 0.578887.
Train: 2018-08-09T11:07:10.229210: step 1469, loss 0.473089.
Train: 2018-08-09T11:07:10.307285: step 1470, loss 0.533432.
Test: 2018-08-09T11:07:10.804548: step 1470, loss 0.552961.
Train: 2018-08-09T11:07:10.882685: step 1471, loss 0.548525.
Train: 2018-08-09T11:07:10.960793: step 1472, loss 0.594318.
Train: 2018-08-09T11:07:11.038900: step 1473, loss 0.594324.
Train: 2018-08-09T11:07:11.117004: step 1474, loss 0.594347.
Train: 2018-08-09T11:07:11.195111: step 1475, loss 0.579061.
Train: 2018-08-09T11:07:11.273214: step 1476, loss 0.625243.
Train: 2018-08-09T11:07:11.351294: step 1477, loss 0.532627.
Train: 2018-08-09T11:07:11.445022: step 1478, loss 0.486735.
Train: 2018-08-09T11:07:11.523159: step 1479, loss 0.532911.
Train: 2018-08-09T11:07:11.601234: step 1480, loss 0.579275.
Test: 2018-08-09T11:07:12.089989: step 1480, loss 0.551817.
Train: 2018-08-09T11:07:12.168125: step 1481, loss 0.517023.
Train: 2018-08-09T11:07:12.246233: step 1482, loss 0.641362.
Train: 2018-08-09T11:07:12.324309: step 1483, loss 0.671908.
Train: 2018-08-09T11:07:12.402416: step 1484, loss 0.579327.
Train: 2018-08-09T11:07:12.480552: step 1485, loss 0.56299.
Train: 2018-08-09T11:07:12.558662: step 1486, loss 0.516272.
Train: 2018-08-09T11:07:12.636763: step 1487, loss 0.532222.
Train: 2018-08-09T11:07:12.714872: step 1488, loss 0.578602.
Train: 2018-08-09T11:07:12.792950: step 1489, loss 0.656908.
Train: 2018-08-09T11:07:12.871086: step 1490, loss 0.501086.
Test: 2018-08-09T11:07:13.370005: step 1490, loss 0.549175.
Train: 2018-08-09T11:07:13.448142: step 1491, loss 0.469474.
Train: 2018-08-09T11:07:13.526248: step 1492, loss 0.484987.
Train: 2018-08-09T11:07:13.604358: step 1493, loss 0.562675.
Train: 2018-08-09T11:07:13.682461: step 1494, loss 0.483996.
Train: 2018-08-09T11:07:13.760571: step 1495, loss 0.515083.
Train: 2018-08-09T11:07:13.838645: step 1496, loss 0.61093.
Train: 2018-08-09T11:07:13.918469: step 1497, loss 0.562968.
Train: 2018-08-09T11:07:13.996607: step 1498, loss 0.530326.
Train: 2018-08-09T11:07:14.074701: step 1499, loss 0.660461.
Train: 2018-08-09T11:07:14.152820: step 1500, loss 0.546104.
Test: 2018-08-09T11:07:14.652672: step 1500, loss 0.549204.
Train: 2018-08-09T11:07:15.215070: step 1501, loss 0.513656.
Train: 2018-08-09T11:07:15.293182: step 1502, loss 0.497334.
Train: 2018-08-09T11:07:15.371252: step 1503, loss 0.497465.
Train: 2018-08-09T11:07:15.449389: step 1504, loss 0.529241.
Train: 2018-08-09T11:07:15.527496: step 1505, loss 0.544177.
Train: 2018-08-09T11:07:15.605604: step 1506, loss 0.494859.
Train: 2018-08-09T11:07:15.683710: step 1507, loss 0.495794.
Train: 2018-08-09T11:07:15.761818: step 1508, loss 0.478601.
Train: 2018-08-09T11:07:15.839892: step 1509, loss 0.630718.
Train: 2018-08-09T11:07:15.920411: step 1510, loss 0.476734.
Test: 2018-08-09T11:07:16.420294: step 1510, loss 0.545588.
Train: 2018-08-09T11:07:16.498430: step 1511, loss 0.613256.
Train: 2018-08-09T11:07:16.576537: step 1512, loss 0.579692.
Train: 2018-08-09T11:07:16.654614: step 1513, loss 0.648133.
Train: 2018-08-09T11:07:16.732750: step 1514, loss 0.509612.
Train: 2018-08-09T11:07:16.810858: step 1515, loss 0.63109.
Train: 2018-08-09T11:07:16.888935: step 1516, loss 0.598589.
Train: 2018-08-09T11:07:16.971595: step 1517, loss 0.578405.
Train: 2018-08-09T11:07:17.049730: step 1518, loss 0.649858.
Train: 2018-08-09T11:07:17.127832: step 1519, loss 0.545518.
Train: 2018-08-09T11:07:17.205943: step 1520, loss 0.424365.
Test: 2018-08-09T11:07:17.690181: step 1520, loss 0.550796.
Train: 2018-08-09T11:07:17.768310: step 1521, loss 0.527528.
Train: 2018-08-09T11:07:17.846411: step 1522, loss 0.476181.
Train: 2018-08-09T11:07:17.937856: step 1523, loss 0.528788.
Train: 2018-08-09T11:07:18.015999: step 1524, loss 0.546719.
Train: 2018-08-09T11:07:18.094068: step 1525, loss 0.52741.
Train: 2018-08-09T11:07:18.172176: step 1526, loss 0.61532.
Train: 2018-08-09T11:07:18.250282: step 1527, loss 0.632135.
Train: 2018-08-09T11:07:18.328359: step 1528, loss 0.44168.
Train: 2018-08-09T11:07:18.406492: step 1529, loss 0.57873.
Train: 2018-08-09T11:07:18.484603: step 1530, loss 0.614744.
Test: 2018-08-09T11:07:18.984494: step 1530, loss 0.547471.
Train: 2018-08-09T11:07:19.062590: step 1531, loss 0.458022.
Train: 2018-08-09T11:07:19.140698: step 1532, loss 0.596887.
Train: 2018-08-09T11:07:19.218804: step 1533, loss 0.647189.
Train: 2018-08-09T11:07:19.296911: step 1534, loss 0.578308.
Train: 2018-08-09T11:07:19.375017: step 1535, loss 0.561294.
Train: 2018-08-09T11:07:19.453094: step 1536, loss 0.562947.
Train: 2018-08-09T11:07:19.531231: step 1537, loss 0.493258.
Train: 2018-08-09T11:07:19.609336: step 1538, loss 0.493829.
Train: 2018-08-09T11:07:19.687414: step 1539, loss 0.493156.
Train: 2018-08-09T11:07:19.765519: step 1540, loss 0.613117.
Test: 2018-08-09T11:07:20.265403: step 1540, loss 0.54754.
Train: 2018-08-09T11:07:20.327917: step 1541, loss 0.614085.
Train: 2018-08-09T11:07:20.421669: step 1542, loss 0.596415.
Train: 2018-08-09T11:07:20.499722: step 1543, loss 0.561429.
Train: 2018-08-09T11:07:20.577858: step 1544, loss 0.512002.
Train: 2018-08-09T11:07:20.655934: step 1545, loss 0.510148.
Train: 2018-08-09T11:07:20.734072: step 1546, loss 0.528524.
Train: 2018-08-09T11:07:20.812147: step 1547, loss 0.477171.
Train: 2018-08-09T11:07:20.890286: step 1548, loss 0.476696.
Train: 2018-08-09T11:07:20.969759: step 1549, loss 0.646732.
Train: 2018-08-09T11:07:21.047894: step 1550, loss 0.614082.
Test: 2018-08-09T11:07:21.532150: step 1550, loss 0.548794.
Train: 2018-08-09T11:07:21.610262: step 1551, loss 0.52782.
Train: 2018-08-09T11:07:21.688370: step 1552, loss 0.545215.
Train: 2018-08-09T11:07:21.782101: step 1553, loss 0.615552.
Train: 2018-08-09T11:07:21.860172: step 1554, loss 0.613662.
Train: 2018-08-09T11:07:21.922659: step 1555, loss 0.477514.
Train: 2018-08-09T11:07:22.016417: step 1556, loss 0.49326.
Train: 2018-08-09T11:07:22.094523: step 1557, loss 0.595915.
Train: 2018-08-09T11:07:22.172629: step 1558, loss 0.664988.
Train: 2018-08-09T11:07:22.250737: step 1559, loss 0.579089.
Train: 2018-08-09T11:07:22.328845: step 1560, loss 0.596665.
Test: 2018-08-09T11:07:22.813106: step 1560, loss 0.550925.
Train: 2018-08-09T11:07:22.891211: step 1561, loss 0.562416.
Train: 2018-08-09T11:07:22.970645: step 1562, loss 0.495153.
Train: 2018-08-09T11:07:23.048724: step 1563, loss 0.528809.
Train: 2018-08-09T11:07:23.126858: step 1564, loss 0.561732.
Train: 2018-08-09T11:07:23.204965: step 1565, loss 0.613517.
Train: 2018-08-09T11:07:23.283066: step 1566, loss 0.679958.
Train: 2018-08-09T11:07:23.361177: step 1567, loss 0.612388.
Train: 2018-08-09T11:07:23.439284: step 1568, loss 0.579128.
Train: 2018-08-09T11:07:23.517362: step 1569, loss 0.56289.
Train: 2018-08-09T11:07:23.595497: step 1570, loss 0.446686.
Test: 2018-08-09T11:07:24.095350: step 1570, loss 0.548371.
Train: 2018-08-09T11:07:24.173500: step 1571, loss 0.580174.
Train: 2018-08-09T11:07:24.251563: step 1572, loss 0.563386.
Train: 2018-08-09T11:07:24.329695: step 1573, loss 0.563027.
Train: 2018-08-09T11:07:24.407775: step 1574, loss 0.546579.
Train: 2018-08-09T11:07:24.485913: step 1575, loss 0.497046.
Train: 2018-08-09T11:07:24.563991: step 1576, loss 0.546393.
Train: 2018-08-09T11:07:24.642126: step 1577, loss 0.546408.
Train: 2018-08-09T11:07:24.720233: step 1578, loss 0.530185.
Train: 2018-08-09T11:07:24.798313: step 1579, loss 0.546648.
Train: 2018-08-09T11:07:24.876415: step 1580, loss 0.611675.
Test: 2018-08-09T11:07:25.378601: step 1580, loss 0.549103.
Train: 2018-08-09T11:07:25.441084: step 1581, loss 0.595605.
Train: 2018-08-09T11:07:25.519225: step 1582, loss 0.497688.
Train: 2018-08-09T11:07:25.612919: step 1583, loss 0.562705.
Train: 2018-08-09T11:07:25.691025: step 1584, loss 0.431753.
Train: 2018-08-09T11:07:25.769163: step 1585, loss 0.546494.
Train: 2018-08-09T11:07:25.847272: step 1586, loss 0.612392.
Train: 2018-08-09T11:07:25.925345: step 1587, loss 0.661937.
Train: 2018-08-09T11:07:26.003482: step 1588, loss 0.596398.
Train: 2018-08-09T11:07:26.081570: step 1589, loss 0.595405.
Train: 2018-08-09T11:07:26.159695: step 1590, loss 0.562641.
Test: 2018-08-09T11:07:26.659547: step 1590, loss 0.548425.
Train: 2018-08-09T11:07:26.722063: step 1591, loss 0.579034.
Train: 2018-08-09T11:07:26.800174: step 1592, loss 0.595172.
Train: 2018-08-09T11:07:26.893898: step 1593, loss 0.59525.
Train: 2018-08-09T11:07:26.958660: step 1594, loss 0.497425.
Train: 2018-08-09T11:07:27.036768: step 1595, loss 0.513767.
Train: 2018-08-09T11:07:27.130496: step 1596, loss 0.546248.
Train: 2018-08-09T11:07:27.208631: step 1597, loss 0.546564.
Train: 2018-08-09T11:07:27.286709: step 1598, loss 0.627316.
Train: 2018-08-09T11:07:27.364817: step 1599, loss 0.64424.
Train: 2018-08-09T11:07:27.442952: step 1600, loss 0.546224.
Test: 2018-08-09T11:07:27.932784: step 1600, loss 0.549325.
Train: 2018-08-09T11:07:28.510802: step 1601, loss 0.578855.
Train: 2018-08-09T11:07:28.620152: step 1602, loss 0.530327.
Train: 2018-08-09T11:07:28.698252: step 1603, loss 0.514133.
Train: 2018-08-09T11:07:28.760743: step 1604, loss 0.659767.
Train: 2018-08-09T11:07:28.838849: step 1605, loss 0.611233.
Train: 2018-08-09T11:07:28.932548: step 1606, loss 0.610908.
Train: 2018-08-09T11:07:29.010684: step 1607, loss 0.579165.
Train: 2018-08-09T11:07:29.088791: step 1608, loss 0.531043.
Train: 2018-08-09T11:07:29.166868: step 1609, loss 0.579364.
Train: 2018-08-09T11:07:29.244973: step 1610, loss 0.642731.
Test: 2018-08-09T11:07:29.729235: step 1610, loss 0.552277.
Train: 2018-08-09T11:07:29.885447: step 1611, loss 0.546935.
Train: 2018-08-09T11:07:29.963556: step 1612, loss 0.753058.
Train: 2018-08-09T11:07:30.041691: step 1613, loss 0.436985.
Train: 2018-08-09T11:07:30.119798: step 1614, loss 0.516.
Train: 2018-08-09T11:07:30.197907: step 1615, loss 0.500319.
Train: 2018-08-09T11:07:30.276006: step 1616, loss 0.532072.
Train: 2018-08-09T11:07:30.354088: step 1617, loss 0.515986.
Train: 2018-08-09T11:07:30.447816: step 1618, loss 0.578955.
Train: 2018-08-09T11:07:30.525921: step 1619, loss 0.468344.
Train: 2018-08-09T11:07:30.604062: step 1620, loss 0.642277.
Test: 2018-08-09T11:07:31.089637: step 1620, loss 0.550532.
Train: 2018-08-09T11:07:31.167773: step 1621, loss 0.594712.
Train: 2018-08-09T11:07:31.245879: step 1622, loss 0.515165.
Train: 2018-08-09T11:07:31.323957: step 1623, loss 0.515209.
Train: 2018-08-09T11:07:31.402062: step 1624, loss 0.626873.
Train: 2018-08-09T11:07:31.480169: step 1625, loss 0.562869.
Train: 2018-08-09T11:07:31.558304: step 1626, loss 0.45101.
Train: 2018-08-09T11:07:31.636412: step 1627, loss 0.562448.
Train: 2018-08-09T11:07:31.714519: step 1628, loss 0.531214.
Train: 2018-08-09T11:07:31.792597: step 1629, loss 0.497577.
Train: 2018-08-09T11:07:31.870704: step 1630, loss 0.529671.
Test: 2018-08-09T11:07:32.370585: step 1630, loss 0.549475.
Train: 2018-08-09T11:07:32.448723: step 1631, loss 0.512893.
Train: 2018-08-09T11:07:32.526828: step 1632, loss 0.563555.
Train: 2018-08-09T11:07:32.604920: step 1633, loss 0.545111.
Train: 2018-08-09T11:07:32.683011: step 1634, loss 0.580977.
Train: 2018-08-09T11:07:32.761147: step 1635, loss 0.546098.
Train: 2018-08-09T11:07:32.839275: step 1636, loss 0.616273.
Train: 2018-08-09T11:07:32.918755: step 1637, loss 0.445113.
Train: 2018-08-09T11:07:32.996863: step 1638, loss 0.510634.
Train: 2018-08-09T11:07:33.090621: step 1639, loss 0.425991.
Train: 2018-08-09T11:07:33.168695: step 1640, loss 0.475675.
Test: 2018-08-09T11:07:33.652958: step 1640, loss 0.545442.
Train: 2018-08-09T11:07:33.731087: step 1641, loss 0.566449.
Train: 2018-08-09T11:07:33.809200: step 1642, loss 0.561258.
Train: 2018-08-09T11:07:33.887305: step 1643, loss 0.578016.
Train: 2018-08-09T11:07:33.965407: step 1644, loss 0.546008.
Train: 2018-08-09T11:07:34.043491: step 1645, loss 0.436638.
Train: 2018-08-09T11:07:34.121631: step 1646, loss 0.688724.
Train: 2018-08-09T11:07:34.199734: step 1647, loss 0.544262.
Train: 2018-08-09T11:07:34.277841: step 1648, loss 0.580174.
Train: 2018-08-09T11:07:34.355917: step 1649, loss 0.564.
Train: 2018-08-09T11:07:34.434048: step 1650, loss 0.490818.
Test: 2018-08-09T11:07:34.936288: step 1650, loss 0.547125.
Train: 2018-08-09T11:07:35.014393: step 1651, loss 0.568044.
Train: 2018-08-09T11:07:35.092499: step 1652, loss 0.508496.
Train: 2018-08-09T11:07:35.170608: step 1653, loss 0.559752.
Train: 2018-08-09T11:07:35.248743: step 1654, loss 0.599775.
Train: 2018-08-09T11:07:35.326850: step 1655, loss 0.561635.
Train: 2018-08-09T11:07:35.404929: step 1656, loss 0.561054.
Train: 2018-08-09T11:07:35.483064: step 1657, loss 0.578363.
Train: 2018-08-09T11:07:35.561170: step 1658, loss 0.544034.
Train: 2018-08-09T11:07:35.639277: step 1659, loss 0.579536.
Train: 2018-08-09T11:07:35.717378: step 1660, loss 0.737382.
Test: 2018-08-09T11:07:36.217266: step 1660, loss 0.548015.
Train: 2018-08-09T11:07:36.295342: step 1661, loss 0.596704.
Train: 2018-08-09T11:07:36.373478: step 1662, loss 0.632678.
Train: 2018-08-09T11:07:36.451586: step 1663, loss 0.493699.
Train: 2018-08-09T11:07:36.529691: step 1664, loss 0.59646.
Train: 2018-08-09T11:07:36.607767: step 1665, loss 0.562926.
Train: 2018-08-09T11:07:36.685901: step 1666, loss 0.612509.
Train: 2018-08-09T11:07:36.764013: step 1667, loss 0.57892.
Train: 2018-08-09T11:07:36.842112: step 1668, loss 0.595519.
Train: 2018-08-09T11:07:36.920907: step 1669, loss 0.52923.
Train: 2018-08-09T11:07:36.999043: step 1670, loss 0.496714.
Test: 2018-08-09T11:07:37.498897: step 1670, loss 0.548418.
Train: 2018-08-09T11:07:37.577033: step 1671, loss 0.447941.
Train: 2018-08-09T11:07:37.655108: step 1672, loss 0.529684.
Train: 2018-08-09T11:07:37.733248: step 1673, loss 0.579142.
Train: 2018-08-09T11:07:37.811323: step 1674, loss 0.612019.
Train: 2018-08-09T11:07:37.889459: step 1675, loss 0.628796.
Train: 2018-08-09T11:07:37.967562: step 1676, loss 0.676549.
Train: 2018-08-09T11:07:38.045673: step 1677, loss 0.530206.
Train: 2018-08-09T11:07:38.123779: step 1678, loss 0.546767.
Train: 2018-08-09T11:07:38.201888: step 1679, loss 0.578895.
Train: 2018-08-09T11:07:38.279961: step 1680, loss 0.563042.
Test: 2018-08-09T11:07:38.779844: step 1680, loss 0.549412.
Train: 2018-08-09T11:07:38.857950: step 1681, loss 0.547118.
Train: 2018-08-09T11:07:38.936087: step 1682, loss 0.594792.
Train: 2018-08-09T11:07:39.014191: step 1683, loss 0.627171.
Train: 2018-08-09T11:07:39.092301: step 1684, loss 0.578329.
Train: 2018-08-09T11:07:39.170403: step 1685, loss 0.642955.
Train: 2018-08-09T11:07:39.248511: step 1686, loss 0.56274.
Train: 2018-08-09T11:07:39.326615: step 1687, loss 0.547113.
Train: 2018-08-09T11:07:39.404727: step 1688, loss 0.515956.
Train: 2018-08-09T11:07:39.482830: step 1689, loss 0.594606.
Train: 2018-08-09T11:07:39.560911: step 1690, loss 0.563379.
Test: 2018-08-09T11:07:40.063110: step 1690, loss 0.549498.
Train: 2018-08-09T11:07:40.141241: step 1691, loss 0.515672.
Train: 2018-08-09T11:07:40.219319: step 1692, loss 0.515885.
Train: 2018-08-09T11:07:40.297454: step 1693, loss 0.625815.
Train: 2018-08-09T11:07:40.375561: step 1694, loss 0.595025.
Train: 2018-08-09T11:07:40.453668: step 1695, loss 0.642605.
Train: 2018-08-09T11:07:40.531771: step 1696, loss 0.594268.
Train: 2018-08-09T11:07:40.609881: step 1697, loss 0.689888.
Train: 2018-08-09T11:07:40.687957: step 1698, loss 0.485135.
Train: 2018-08-09T11:07:40.766066: step 1699, loss 0.672759.
Train: 2018-08-09T11:07:40.844201: step 1700, loss 0.501587.
Test: 2018-08-09T11:07:41.343189: step 1700, loss 0.54913.
Train: 2018-08-09T11:07:41.939064: step 1701, loss 0.563448.
Train: 2018-08-09T11:07:42.017172: step 1702, loss 0.532811.
Train: 2018-08-09T11:07:42.095246: step 1703, loss 0.548281.
Train: 2018-08-09T11:07:42.189005: step 1704, loss 0.471165.
Train: 2018-08-09T11:07:42.267111: step 1705, loss 0.485926.
Train: 2018-08-09T11:07:42.345189: step 1706, loss 0.579292.
Train: 2018-08-09T11:07:42.423327: step 1707, loss 0.688641.
Train: 2018-08-09T11:07:42.501402: step 1708, loss 0.547415.
Train: 2018-08-09T11:07:42.579536: step 1709, loss 0.579181.
Train: 2018-08-09T11:07:42.657642: step 1710, loss 0.500516.
Test: 2018-08-09T11:07:43.141900: step 1710, loss 0.549728.
Train: 2018-08-09T11:07:43.220012: step 1711, loss 0.547455.
Train: 2018-08-09T11:07:43.298121: step 1712, loss 0.485089.
Train: 2018-08-09T11:07:43.377908: step 1713, loss 0.626868.
Train: 2018-08-09T11:07:43.456015: step 1714, loss 0.656934.
Train: 2018-08-09T11:07:43.534122: step 1715, loss 0.481979.
Train: 2018-08-09T11:07:43.612242: step 1716, loss 0.513169.
Train: 2018-08-09T11:07:43.690334: step 1717, loss 0.510086.
Train: 2018-08-09T11:07:43.768441: step 1718, loss 0.55765.
Train: 2018-08-09T11:07:43.846519: step 1719, loss 0.531812.
Train: 2018-08-09T11:07:43.924654: step 1720, loss 0.496916.
Test: 2018-08-09T11:07:44.424507: step 1720, loss 0.546496.
Train: 2018-08-09T11:07:44.502637: step 1721, loss 0.552031.
Train: 2018-08-09T11:07:44.580749: step 1722, loss 0.606913.
Train: 2018-08-09T11:07:44.658856: step 1723, loss 0.464937.
Train: 2018-08-09T11:07:44.736958: step 1724, loss 0.508529.
Train: 2018-08-09T11:07:44.815064: step 1725, loss 0.545673.
Train: 2018-08-09T11:07:44.908767: step 1726, loss 0.542283.
Train: 2018-08-09T11:07:44.986898: step 1727, loss 0.549324.
Train: 2018-08-09T11:07:45.065010: step 1728, loss 0.629357.
Train: 2018-08-09T11:07:45.143087: step 1729, loss 0.567272.
Train: 2018-08-09T11:07:45.221192: step 1730, loss 0.62972.
Test: 2018-08-09T11:07:45.721103: step 1730, loss 0.545008.
Train: 2018-08-09T11:07:45.783590: step 1731, loss 0.636618.
Train: 2018-08-09T11:07:45.877289: step 1732, loss 0.677408.
Train: 2018-08-09T11:07:45.957744: step 1733, loss 0.541193.
Train: 2018-08-09T11:07:46.020236: step 1734, loss 0.595367.
Train: 2018-08-09T11:07:46.098340: step 1735, loss 0.625893.
Train: 2018-08-09T11:07:46.192039: step 1736, loss 0.513515.
Train: 2018-08-09T11:07:46.270176: step 1737, loss 0.479185.
Train: 2018-08-09T11:07:46.348255: step 1738, loss 0.497903.
Train: 2018-08-09T11:07:46.426394: step 1739, loss 0.708283.
Train: 2018-08-09T11:07:46.488873: step 1740, loss 0.500378.
Test: 2018-08-09T11:07:46.988750: step 1740, loss 0.551223.
Train: 2018-08-09T11:07:47.066864: step 1741, loss 0.547177.
Train: 2018-08-09T11:07:47.144964: step 1742, loss 0.595156.
Train: 2018-08-09T11:07:47.223077: step 1743, loss 0.594655.
Train: 2018-08-09T11:07:47.301182: step 1744, loss 0.437713.
Train: 2018-08-09T11:07:47.379289: step 1745, loss 0.485581.
Train: 2018-08-09T11:07:47.470580: step 1746, loss 0.563226.
Train: 2018-08-09T11:07:47.538251: step 1747, loss 0.563439.
Train: 2018-08-09T11:07:47.616357: step 1748, loss 0.547217.
Train: 2018-08-09T11:07:47.694465: step 1749, loss 0.578882.
Train: 2018-08-09T11:07:47.772601: step 1750, loss 0.610551.
Test: 2018-08-09T11:07:48.272453: step 1750, loss 0.550023.
Train: 2018-08-09T11:07:48.350559: step 1751, loss 0.531608.
Train: 2018-08-09T11:07:48.428697: step 1752, loss 0.594422.
Train: 2018-08-09T11:07:48.506774: step 1753, loss 0.578613.
Train: 2018-08-09T11:07:48.584909: step 1754, loss 0.515403.
Train: 2018-08-09T11:07:48.663016: step 1755, loss 0.562396.
Train: 2018-08-09T11:07:48.741093: step 1756, loss 0.627182.
Train: 2018-08-09T11:07:48.819199: step 1757, loss 0.611159.
Train: 2018-08-09T11:07:48.897336: step 1758, loss 0.562726.
Train: 2018-08-09T11:07:48.975413: step 1759, loss 0.594699.
Train: 2018-08-09T11:07:49.053550: step 1760, loss 0.499367.
Test: 2018-08-09T11:07:49.553433: step 1760, loss 0.549821.
Train: 2018-08-09T11:07:49.631538: step 1761, loss 0.451436.
Train: 2018-08-09T11:07:49.709644: step 1762, loss 0.514841.
Train: 2018-08-09T11:07:49.787723: step 1763, loss 0.562487.
Train: 2018-08-09T11:07:49.865859: step 1764, loss 0.578863.
Train: 2018-08-09T11:07:49.946393: step 1765, loss 0.580043.
Train: 2018-08-09T11:07:50.024504: step 1766, loss 0.562195.
Train: 2018-08-09T11:07:50.102612: step 1767, loss 0.591972.
Train: 2018-08-09T11:07:50.180719: step 1768, loss 0.545503.
Train: 2018-08-09T11:07:50.258827: step 1769, loss 0.511992.
Train: 2018-08-09T11:07:50.336931: step 1770, loss 0.60394.
Test: 2018-08-09T11:07:50.833673: step 1770, loss 0.548351.
Train: 2018-08-09T11:07:50.911780: step 1771, loss 0.599637.
Train: 2018-08-09T11:07:51.005507: step 1772, loss 0.596676.
Train: 2018-08-09T11:07:51.099240: step 1773, loss 0.632463.
Train: 2018-08-09T11:07:51.177341: step 1774, loss 0.546027.
Train: 2018-08-09T11:07:51.255449: step 1775, loss 0.577137.
Train: 2018-08-09T11:07:51.333555: step 1776, loss 0.497777.
Train: 2018-08-09T11:07:51.411663: step 1777, loss 0.580379.
Train: 2018-08-09T11:07:51.505389: step 1778, loss 0.72444.
Train: 2018-08-09T11:07:51.599117: step 1779, loss 0.611151.
Train: 2018-08-09T11:07:51.677224: step 1780, loss 0.56315.
Test: 2018-08-09T11:07:52.181341: step 1780, loss 0.549392.
Train: 2018-08-09T11:07:52.259442: step 1781, loss 0.578585.
Train: 2018-08-09T11:07:52.337524: step 1782, loss 0.594462.
Train: 2018-08-09T11:07:52.415655: step 1783, loss 0.4376.
Train: 2018-08-09T11:07:52.493737: step 1784, loss 0.563155.
Train: 2018-08-09T11:07:52.571872: step 1785, loss 0.578831.
Train: 2018-08-09T11:07:52.649975: step 1786, loss 0.594643.
Train: 2018-08-09T11:07:52.743676: step 1787, loss 0.704564.
Train: 2018-08-09T11:07:52.821810: step 1788, loss 0.563092.
Train: 2018-08-09T11:07:52.899922: step 1789, loss 0.641301.
Train: 2018-08-09T11:07:52.977997: step 1790, loss 0.594451.
Test: 2018-08-09T11:07:53.477880: step 1790, loss 0.550053.
Train: 2018-08-09T11:07:53.555986: step 1791, loss 0.578845.
Train: 2018-08-09T11:07:53.634122: step 1792, loss 0.578878.
Train: 2018-08-09T11:07:53.712230: step 1793, loss 0.578962.
Train: 2018-08-09T11:07:53.790336: step 1794, loss 0.625032.
Train: 2018-08-09T11:07:53.868436: step 1795, loss 0.609668.
Train: 2018-08-09T11:07:53.946520: step 1796, loss 0.579025.
Train: 2018-08-09T11:07:54.024626: step 1797, loss 0.563927.
Train: 2018-08-09T11:07:54.118354: step 1798, loss 0.579149.
Train: 2018-08-09T11:07:54.196490: step 1799, loss 0.579156.
Train: 2018-08-09T11:07:54.274567: step 1800, loss 0.579054.
Test: 2018-08-09T11:07:54.774449: step 1800, loss 0.551176.
Train: 2018-08-09T11:07:55.323793: step 1801, loss 0.488911.
Train: 2018-08-09T11:07:55.417545: step 1802, loss 0.519028.
Train: 2018-08-09T11:07:55.495631: step 1803, loss 0.503836.
Train: 2018-08-09T11:07:55.573734: step 1804, loss 0.533675.
Train: 2018-08-09T11:07:55.667475: step 1805, loss 0.579216.
Train: 2018-08-09T11:07:55.745599: step 1806, loss 0.466465.
Train: 2018-08-09T11:07:55.823705: step 1807, loss 0.563597.
Train: 2018-08-09T11:07:55.901781: step 1808, loss 0.564056.
Train: 2018-08-09T11:07:55.995534: step 1809, loss 0.656533.
Train: 2018-08-09T11:07:56.073617: step 1810, loss 0.640476.
Test: 2018-08-09T11:07:56.573498: step 1810, loss 0.550145.
Train: 2018-08-09T11:07:56.636008: step 1811, loss 0.609785.
Train: 2018-08-09T11:07:56.729710: step 1812, loss 0.532722.
Train: 2018-08-09T11:07:56.807819: step 1813, loss 0.594428.
Train: 2018-08-09T11:07:56.885955: step 1814, loss 0.54797.
Train: 2018-08-09T11:07:56.950704: step 1815, loss 0.548021.
Train: 2018-08-09T11:07:57.044403: step 1816, loss 0.532593.
Train: 2018-08-09T11:07:57.122538: step 1817, loss 0.486012.
Train: 2018-08-09T11:07:57.200646: step 1818, loss 0.578928.
Train: 2018-08-09T11:07:57.278748: step 1819, loss 0.563299.
Train: 2018-08-09T11:07:57.356860: step 1820, loss 0.547635.
Test: 2018-08-09T11:07:57.856710: step 1820, loss 0.550263.
Train: 2018-08-09T11:07:57.934847: step 1821, loss 0.594443.
Train: 2018-08-09T11:07:58.012954: step 1822, loss 0.563014.
Train: 2018-08-09T11:07:58.091031: step 1823, loss 0.594686.
Train: 2018-08-09T11:07:58.169137: step 1824, loss 0.641995.
Train: 2018-08-09T11:07:58.247273: step 1825, loss 0.594477.
Train: 2018-08-09T11:07:58.325350: step 1826, loss 0.610579.
Train: 2018-08-09T11:07:58.403487: step 1827, loss 0.72078.
Train: 2018-08-09T11:07:58.481596: step 1828, loss 0.56328.
Train: 2018-08-09T11:07:58.559680: step 1829, loss 0.563318.
Train: 2018-08-09T11:07:58.645430: step 1830, loss 0.500826.
Test: 2018-08-09T11:07:59.139937: step 1830, loss 0.548131.
Train: 2018-08-09T11:07:59.202452: step 1831, loss 0.485408.
Train: 2018-08-09T11:07:59.296149: step 1832, loss 0.532217.
Train: 2018-08-09T11:07:59.374286: step 1833, loss 0.516463.
Train: 2018-08-09T11:07:59.452392: step 1834, loss 0.547626.
Train: 2018-08-09T11:07:59.530500: step 1835, loss 0.531828.
Train: 2018-08-09T11:07:59.608602: step 1836, loss 0.578802.
Train: 2018-08-09T11:07:59.686710: step 1837, loss 0.563174.
Train: 2018-08-09T11:07:59.764819: step 1838, loss 0.610663.
Train: 2018-08-09T11:07:59.842897: step 1839, loss 0.547315.
Train: 2018-08-09T11:07:59.921032: step 1840, loss 0.705473.
Test: 2018-08-09T11:08:00.420909: step 1840, loss 0.549397.
Train: 2018-08-09T11:08:00.499021: step 1841, loss 0.468148.
Train: 2018-08-09T11:08:00.577123: step 1842, loss 0.626337.
Train: 2018-08-09T11:08:00.655235: step 1843, loss 0.547176.
Train: 2018-08-09T11:08:00.733341: step 1844, loss 0.547199.
Train: 2018-08-09T11:08:00.811449: step 1845, loss 0.673916.
Train: 2018-08-09T11:08:00.889555: step 1846, loss 0.594676.
Train: 2018-08-09T11:08:00.967660: step 1847, loss 0.499786.
Train: 2018-08-09T11:08:01.045738: step 1848, loss 0.563072.
Train: 2018-08-09T11:08:01.123899: step 1849, loss 0.531454.
Train: 2018-08-09T11:08:01.201980: step 1850, loss 0.610441.
Test: 2018-08-09T11:08:01.701858: step 1850, loss 0.548833.
Train: 2018-08-09T11:08:01.779953: step 1851, loss 0.626287.
Train: 2018-08-09T11:08:01.858074: step 1852, loss 0.484065.
Train: 2018-08-09T11:08:01.937540: step 1853, loss 0.578714.
Train: 2018-08-09T11:08:02.015647: step 1854, loss 0.547325.
Train: 2018-08-09T11:08:02.093755: step 1855, loss 0.578991.
Train: 2018-08-09T11:08:02.171856: step 1856, loss 0.626815.
Train: 2018-08-09T11:08:02.249936: step 1857, loss 0.578658.
Train: 2018-08-09T11:08:02.328078: step 1858, loss 0.547137.
Train: 2018-08-09T11:08:02.406152: step 1859, loss 0.610551.
Train: 2018-08-09T11:08:02.484286: step 1860, loss 0.547213.
Test: 2018-08-09T11:08:02.984164: step 1860, loss 0.548223.
Train: 2018-08-09T11:08:03.062275: step 1861, loss 0.625866.
Train: 2018-08-09T11:08:03.140376: step 1862, loss 0.562935.
Train: 2018-08-09T11:08:03.218459: step 1863, loss 0.484294.
Train: 2018-08-09T11:08:03.296595: step 1864, loss 0.626574.
Train: 2018-08-09T11:08:03.374702: step 1865, loss 0.57877.
Train: 2018-08-09T11:08:03.452780: step 1866, loss 0.673683.
Train: 2018-08-09T11:08:03.530915: step 1867, loss 0.500236.
Train: 2018-08-09T11:08:03.609021: step 1868, loss 0.516096.
Train: 2018-08-09T11:08:03.687127: step 1869, loss 0.531884.
Train: 2018-08-09T11:08:03.765204: step 1870, loss 0.689237.
Test: 2018-08-09T11:08:04.267459: step 1870, loss 0.551411.
Train: 2018-08-09T11:08:04.345589: step 1871, loss 0.641574.
Train: 2018-08-09T11:08:04.423671: step 1872, loss 0.672622.
Train: 2018-08-09T11:08:04.501779: step 1873, loss 0.563451.
Train: 2018-08-09T11:08:04.579919: step 1874, loss 0.563463.
Train: 2018-08-09T11:08:04.658017: step 1875, loss 0.501795.
Train: 2018-08-09T11:08:04.751721: step 1876, loss 0.702205.
Train: 2018-08-09T11:08:04.829856: step 1877, loss 0.579053.
Train: 2018-08-09T11:08:04.907962: step 1878, loss 0.563687.
Train: 2018-08-09T11:08:04.986040: step 1879, loss 0.45691.
Train: 2018-08-09T11:08:05.064146: step 1880, loss 0.563754.
Test: 2018-08-09T11:08:05.564029: step 1880, loss 0.550042.
Train: 2018-08-09T11:08:05.642165: step 1881, loss 0.563723.
Train: 2018-08-09T11:08:05.720272: step 1882, loss 0.609538.
Train: 2018-08-09T11:08:05.798379: step 1883, loss 0.594214.
Train: 2018-08-09T11:08:05.876454: step 1884, loss 0.518039.
Train: 2018-08-09T11:08:05.956090: step 1885, loss 0.594306.
Train: 2018-08-09T11:08:06.034169: step 1886, loss 0.579006.
Train: 2018-08-09T11:08:06.112303: step 1887, loss 0.563758.
Train: 2018-08-09T11:08:06.190410: step 1888, loss 0.594291.
Train: 2018-08-09T11:08:06.268513: step 1889, loss 0.472122.
Train: 2018-08-09T11:08:06.346622: step 1890, loss 0.609603.
Test: 2018-08-09T11:08:06.846476: step 1890, loss 0.54932.
Train: 2018-08-09T11:08:06.924612: step 1891, loss 0.609628.
Train: 2018-08-09T11:08:07.002720: step 1892, loss 0.54837.
Train: 2018-08-09T11:08:07.080796: step 1893, loss 0.517652.
Train: 2018-08-09T11:08:07.158932: step 1894, loss 0.517463.
Train: 2018-08-09T11:08:07.237039: step 1895, loss 0.609773.
Train: 2018-08-09T11:08:07.315145: step 1896, loss 0.54812.
Train: 2018-08-09T11:08:07.393222: step 1897, loss 0.532447.
Train: 2018-08-09T11:08:07.471328: step 1898, loss 0.578927.
Train: 2018-08-09T11:08:07.549461: step 1899, loss 0.610084.
Train: 2018-08-09T11:08:07.643192: step 1900, loss 0.625668.
Test: 2018-08-09T11:08:08.128810: step 1900, loss 0.551672.
Train: 2018-08-09T11:08:08.675560: step 1901, loss 0.57887.
Train: 2018-08-09T11:08:08.753637: step 1902, loss 0.56333.
Train: 2018-08-09T11:08:08.847391: step 1903, loss 0.750346.
Train: 2018-08-09T11:08:08.925502: step 1904, loss 0.609982.
Train: 2018-08-09T11:08:09.003602: step 1905, loss 0.594407.
Train: 2018-08-09T11:08:09.081685: step 1906, loss 0.501737.
Train: 2018-08-09T11:08:09.159821: step 1907, loss 0.548119.
Train: 2018-08-09T11:08:09.237898: step 1908, loss 0.501962.
Train: 2018-08-09T11:08:09.316036: step 1909, loss 0.594363.
Train: 2018-08-09T11:08:09.394137: step 1910, loss 0.548119.
Test: 2018-08-09T11:08:09.893993: step 1910, loss 0.549678.
Train: 2018-08-09T11:08:09.958893: step 1911, loss 0.656026.
Train: 2018-08-09T11:08:10.037000: step 1912, loss 0.517375.
Train: 2018-08-09T11:08:10.115106: step 1913, loss 0.655924.
Train: 2018-08-09T11:08:10.193215: step 1914, loss 0.532811.
Train: 2018-08-09T11:08:10.271323: step 1915, loss 0.594299.
Train: 2018-08-09T11:08:10.365044: step 1916, loss 0.517633.
Train: 2018-08-09T11:08:10.443125: step 1917, loss 0.548262.
Train: 2018-08-09T11:08:10.521265: step 1918, loss 0.563532.
Train: 2018-08-09T11:08:10.599339: step 1919, loss 0.625176.
Train: 2018-08-09T11:08:10.677474: step 1920, loss 0.563522.
Test: 2018-08-09T11:08:11.177326: step 1920, loss 0.550244.
Train: 2018-08-09T11:08:11.255462: step 1921, loss 0.686765.
Train: 2018-08-09T11:08:11.333570: step 1922, loss 0.471218.
Train: 2018-08-09T11:08:11.411672: step 1923, loss 0.502001.
Train: 2018-08-09T11:08:11.493925: step 1924, loss 0.578653.
Train: 2018-08-09T11:08:11.568523: step 1925, loss 0.454352.
Train: 2018-08-09T11:08:11.646661: step 1926, loss 0.562169.
Train: 2018-08-09T11:08:11.724738: step 1927, loss 0.562545.
Train: 2018-08-09T11:08:11.802873: step 1928, loss 0.563215.
Train: 2018-08-09T11:08:11.880981: step 1929, loss 0.643945.
Train: 2018-08-09T11:08:11.959087: step 1930, loss 0.545343.
Test: 2018-08-09T11:08:12.458940: step 1930, loss 0.547495.
Train: 2018-08-09T11:08:12.537076: step 1931, loss 0.560501.
Train: 2018-08-09T11:08:12.615183: step 1932, loss 0.481498.
Train: 2018-08-09T11:08:12.693260: step 1933, loss 0.514437.
Train: 2018-08-09T11:08:12.771395: step 1934, loss 0.578083.
Train: 2018-08-09T11:08:12.849473: step 1935, loss 0.547523.
Train: 2018-08-09T11:08:12.927612: step 1936, loss 0.499398.
Train: 2018-08-09T11:08:13.005684: step 1937, loss 0.46024.
Train: 2018-08-09T11:08:13.083822: step 1938, loss 0.562429.
Train: 2018-08-09T11:08:13.161900: step 1939, loss 0.547648.
Train: 2018-08-09T11:08:13.240035: step 1940, loss 0.688912.
Test: 2018-08-09T11:08:13.739887: step 1940, loss 0.545205.
Train: 2018-08-09T11:08:13.821603: step 1941, loss 0.530722.
Train: 2018-08-09T11:08:13.899739: step 1942, loss 0.533867.
Train: 2018-08-09T11:08:13.977850: step 1943, loss 0.597037.
Train: 2018-08-09T11:08:14.055953: step 1944, loss 0.562264.
Train: 2018-08-09T11:08:14.134061: step 1945, loss 0.427826.
Train: 2018-08-09T11:08:14.212168: step 1946, loss 0.632173.
Train: 2018-08-09T11:08:14.290272: step 1947, loss 0.580791.
Train: 2018-08-09T11:08:14.368349: step 1948, loss 0.612614.
Train: 2018-08-09T11:08:14.446487: step 1949, loss 0.562973.
Train: 2018-08-09T11:08:14.524593: step 1950, loss 0.661026.
Test: 2018-08-09T11:08:15.024447: step 1950, loss 0.550109.
Train: 2018-08-09T11:08:15.102576: step 1951, loss 0.513568.
Train: 2018-08-09T11:08:15.180683: step 1952, loss 0.562602.
Train: 2018-08-09T11:08:15.258766: step 1953, loss 0.626778.
Train: 2018-08-09T11:08:15.336872: step 1954, loss 0.578841.
Train: 2018-08-09T11:08:15.415009: step 1955, loss 0.626558.
Train: 2018-08-09T11:08:15.493113: step 1956, loss 0.578841.
Train: 2018-08-09T11:08:15.571191: step 1957, loss 0.499731.
Train: 2018-08-09T11:08:15.649297: step 1958, loss 0.578914.
Train: 2018-08-09T11:08:15.743050: step 1959, loss 0.452616.
Train: 2018-08-09T11:08:15.821162: step 1960, loss 0.563028.
Test: 2018-08-09T11:08:16.307812: step 1960, loss 0.551205.
Train: 2018-08-09T11:08:16.385924: step 1961, loss 0.59466.
Train: 2018-08-09T11:08:16.464001: step 1962, loss 0.515491.
Train: 2018-08-09T11:08:16.542138: step 1963, loss 0.531306.
Train: 2018-08-09T11:08:16.635835: step 1964, loss 0.562951.
Train: 2018-08-09T11:08:16.713971: step 1965, loss 0.57887.
Train: 2018-08-09T11:08:16.792078: step 1966, loss 0.578866.
Train: 2018-08-09T11:08:16.870154: step 1967, loss 0.610738.
Train: 2018-08-09T11:08:16.948261: step 1968, loss 0.498971.
Train: 2018-08-09T11:08:17.026399: step 1969, loss 0.594807.
Train: 2018-08-09T11:08:17.104473: step 1970, loss 0.610616.
Test: 2018-08-09T11:08:17.604357: step 1970, loss 0.552103.
Train: 2018-08-09T11:08:17.682463: step 1971, loss 0.578847.
Train: 2018-08-09T11:08:17.760600: step 1972, loss 0.562827.
Train: 2018-08-09T11:08:17.838703: step 1973, loss 0.546904.
Train: 2018-08-09T11:08:17.918196: step 1974, loss 0.59487.
Train: 2018-08-09T11:08:18.003486: step 1975, loss 0.530951.
Train: 2018-08-09T11:08:18.072408: step 1976, loss 0.562809.
Train: 2018-08-09T11:08:18.150518: step 1977, loss 0.578892.
Train: 2018-08-09T11:08:18.228595: step 1978, loss 0.514727.
Train: 2018-08-09T11:08:18.306733: step 1979, loss 0.562769.
Train: 2018-08-09T11:08:18.384839: step 1980, loss 0.546742.
Test: 2018-08-09T11:08:18.884690: step 1980, loss 0.549478.
Train: 2018-08-09T11:08:18.962797: step 1981, loss 0.482536.
Train: 2018-08-09T11:08:19.040936: step 1982, loss 0.53062.
Train: 2018-08-09T11:08:19.119042: step 1983, loss 0.448962.
Train: 2018-08-09T11:08:19.197143: step 1984, loss 0.595434.
Train: 2018-08-09T11:08:19.275254: step 1985, loss 0.529147.
Train: 2018-08-09T11:08:19.353356: step 1986, loss 0.446595.
Train: 2018-08-09T11:08:19.431466: step 1987, loss 0.544776.
Train: 2018-08-09T11:08:19.509545: step 1988, loss 0.459655.
Train: 2018-08-09T11:08:19.603271: step 1989, loss 0.633405.
Train: 2018-08-09T11:08:19.681402: step 1990, loss 0.549215.
Test: 2018-08-09T11:08:20.170170: step 1990, loss 0.548566.
Train: 2018-08-09T11:08:20.248276: step 1991, loss 0.630978.
Train: 2018-08-09T11:08:20.326384: step 1992, loss 0.421585.
Train: 2018-08-09T11:08:20.404524: step 1993, loss 0.421978.
Train: 2018-08-09T11:08:20.482626: step 1994, loss 0.720095.
Train: 2018-08-09T11:08:20.560733: step 1995, loss 0.564324.
Train: 2018-08-09T11:08:20.638810: step 1996, loss 0.597002.
Train: 2018-08-09T11:08:20.716915: step 1997, loss 0.523295.
Train: 2018-08-09T11:08:20.795048: step 1998, loss 0.488488.
Train: 2018-08-09T11:08:20.873163: step 1999, loss 0.665434.
Train: 2018-08-09T11:08:20.951236: step 2000, loss 0.47707.
Test: 2018-08-09T11:08:21.451119: step 2000, loss 0.545718.
Train: 2018-08-09T11:08:22.014562: step 2001, loss 0.619202.
Train: 2018-08-09T11:08:22.092668: step 2002, loss 0.544091.
Train: 2018-08-09T11:08:22.186427: step 2003, loss 0.581266.
Train: 2018-08-09T11:08:22.264503: step 2004, loss 0.565031.
Train: 2018-08-09T11:08:22.342613: step 2005, loss 0.57909.
Train: 2018-08-09T11:08:22.420746: step 2006, loss 0.54644.
Train: 2018-08-09T11:08:22.498853: step 2007, loss 0.49275.
Train: 2018-08-09T11:08:22.576959: step 2008, loss 0.511582.
Train: 2018-08-09T11:08:22.655063: step 2009, loss 0.630148.
Train: 2018-08-09T11:08:22.733174: step 2010, loss 0.579442.
Test: 2018-08-09T11:08:23.233052: step 2010, loss 0.545695.
Train: 2018-08-09T11:08:23.295510: step 2011, loss 0.442795.
Train: 2018-08-09T11:08:23.389267: step 2012, loss 0.563503.
Train: 2018-08-09T11:08:23.467374: step 2013, loss 0.594591.
Train: 2018-08-09T11:08:23.545451: step 2014, loss 0.648439.
Train: 2018-08-09T11:08:23.623557: step 2015, loss 0.565302.
Train: 2018-08-09T11:08:23.701696: step 2016, loss 0.512894.
Train: 2018-08-09T11:08:23.779801: step 2017, loss 0.679791.
Train: 2018-08-09T11:08:23.857904: step 2018, loss 0.595034.
Train: 2018-08-09T11:08:23.936014: step 2019, loss 0.529192.
Train: 2018-08-09T11:08:24.014116: step 2020, loss 0.529575.
Test: 2018-08-09T11:08:24.513972: step 2020, loss 0.552062.
Train: 2018-08-09T11:08:24.592109: step 2021, loss 0.562544.
Train: 2018-08-09T11:08:24.670215: step 2022, loss 0.513118.
Train: 2018-08-09T11:08:24.748324: step 2023, loss 0.579129.
Train: 2018-08-09T11:08:24.826429: step 2024, loss 0.611746.
Train: 2018-08-09T11:08:24.906032: step 2025, loss 0.480687.
Train: 2018-08-09T11:08:24.984141: step 2026, loss 0.579137.
Train: 2018-08-09T11:08:25.062245: step 2027, loss 0.562645.
Train: 2018-08-09T11:08:25.140383: step 2028, loss 0.595227.
Train: 2018-08-09T11:08:25.218491: step 2029, loss 0.530035.
Train: 2018-08-09T11:08:25.296566: step 2030, loss 0.497382.
Test: 2018-08-09T11:08:25.796449: step 2030, loss 0.549182.
Train: 2018-08-09T11:08:25.874585: step 2031, loss 0.481021.
Train: 2018-08-09T11:08:25.952662: step 2032, loss 0.431776.
Train: 2018-08-09T11:08:26.030768: step 2033, loss 0.726719.
Train: 2018-08-09T11:08:26.108874: step 2034, loss 0.529642.
Train: 2018-08-09T11:08:26.187013: step 2035, loss 0.480308.
Train: 2018-08-09T11:08:26.265089: step 2036, loss 0.677959.
Train: 2018-08-09T11:08:26.343193: step 2037, loss 0.529651.
Train: 2018-08-09T11:08:26.421331: step 2038, loss 0.46349.
Train: 2018-08-09T11:08:26.499409: step 2039, loss 0.512841.
Train: 2018-08-09T11:08:26.577513: step 2040, loss 0.545872.
Test: 2018-08-09T11:08:27.079845: step 2040, loss 0.549377.
Train: 2018-08-09T11:08:27.157951: step 2041, loss 0.629085.
Train: 2018-08-09T11:08:27.236087: step 2042, loss 0.495787.
Train: 2018-08-09T11:08:27.314167: step 2043, loss 0.478978.
Train: 2018-08-09T11:08:27.392271: step 2044, loss 0.528851.
Train: 2018-08-09T11:08:27.470412: step 2045, loss 0.596005.
Train: 2018-08-09T11:08:27.548511: step 2046, loss 0.528756.
Train: 2018-08-09T11:08:27.626625: step 2047, loss 0.528654.
Train: 2018-08-09T11:08:27.704727: step 2048, loss 0.545395.
Train: 2018-08-09T11:08:27.782804: step 2049, loss 0.528322.
Train: 2018-08-09T11:08:27.860941: step 2050, loss 0.545258.
Test: 2018-08-09T11:08:28.360820: step 2050, loss 0.54891.
Train: 2018-08-09T11:08:28.438929: step 2051, loss 0.579413.
Train: 2018-08-09T11:08:28.517005: step 2052, loss 0.528105.
Train: 2018-08-09T11:08:28.595148: step 2053, loss 0.665423.
Train: 2018-08-09T11:08:28.673219: step 2054, loss 0.648822.
Train: 2018-08-09T11:08:28.751356: step 2055, loss 0.493927.
Train: 2018-08-09T11:08:28.829461: step 2056, loss 0.459826.
Train: 2018-08-09T11:08:28.908906: step 2057, loss 0.579412.
Train: 2018-08-09T11:08:28.987014: step 2058, loss 0.647846.
Train: 2018-08-09T11:08:29.080774: step 2059, loss 0.511065.
Train: 2018-08-09T11:08:29.158847: step 2060, loss 0.494054.
Test: 2018-08-09T11:08:29.638142: step 2060, loss 0.546335.
Train: 2018-08-09T11:08:29.716257: step 2061, loss 0.545348.
Train: 2018-08-09T11:08:29.794361: step 2062, loss 0.596609.
Train: 2018-08-09T11:08:29.872472: step 2063, loss 0.459822.
Train: 2018-08-09T11:08:29.950573: step 2064, loss 0.562285.
Train: 2018-08-09T11:08:30.028680: step 2065, loss 0.528107.
Train: 2018-08-09T11:08:30.122409: step 2066, loss 0.49378.
Train: 2018-08-09T11:08:30.200516: step 2067, loss 0.579477.
Train: 2018-08-09T11:08:30.278621: step 2068, loss 0.459119.
Train: 2018-08-09T11:08:30.356724: step 2069, loss 0.665767.
Train: 2018-08-09T11:08:30.434834: step 2070, loss 0.510618.
Test: 2018-08-09T11:08:30.934686: step 2070, loss 0.546814.
Train: 2018-08-09T11:08:31.012818: step 2071, loss 0.441482.
Train: 2018-08-09T11:08:31.090900: step 2072, loss 0.562325.
Train: 2018-08-09T11:08:31.169006: step 2073, loss 0.562381.
Train: 2018-08-09T11:08:31.247143: step 2074, loss 0.579752.
Train: 2018-08-09T11:08:31.325249: step 2075, loss 0.510193.
Train: 2018-08-09T11:08:31.403354: step 2076, loss 0.492658.
Train: 2018-08-09T11:08:31.481463: step 2077, loss 0.54494.
Train: 2018-08-09T11:08:31.559538: step 2078, loss 0.527364.
Train: 2018-08-09T11:08:31.637672: step 2079, loss 0.527345.
Train: 2018-08-09T11:08:31.715784: step 2080, loss 0.579983.
Test: 2018-08-09T11:08:32.215635: step 2080, loss 0.547261.
Train: 2018-08-09T11:08:32.293772: step 2081, loss 0.544778.
Train: 2018-08-09T11:08:32.371848: step 2082, loss 0.615192.
Train: 2018-08-09T11:08:32.449982: step 2083, loss 0.544861.
Train: 2018-08-09T11:08:32.528090: step 2084, loss 0.544799.
Train: 2018-08-09T11:08:32.606168: step 2085, loss 0.632679.
Train: 2018-08-09T11:08:32.684305: step 2086, loss 0.562461.
Train: 2018-08-09T11:08:32.762380: step 2087, loss 0.544849.
Train: 2018-08-09T11:08:32.856109: step 2088, loss 0.650071.
Train: 2018-08-09T11:08:32.935739: step 2089, loss 0.562369.
Train: 2018-08-09T11:08:33.013845: step 2090, loss 0.492663.
Test: 2018-08-09T11:08:33.498101: step 2090, loss 0.547365.
Train: 2018-08-09T11:08:33.576215: step 2091, loss 0.597114.
Train: 2018-08-09T11:08:33.654319: step 2092, loss 0.45815.
Train: 2018-08-09T11:08:33.732397: step 2093, loss 0.475563.
Train: 2018-08-09T11:08:33.810533: step 2094, loss 0.614439.
Train: 2018-08-09T11:08:33.888639: step 2095, loss 0.597069.
Train: 2018-08-09T11:08:33.982337: step 2096, loss 0.597031.
Train: 2018-08-09T11:08:34.060473: step 2097, loss 0.596932.
Train: 2018-08-09T11:08:34.138550: step 2098, loss 0.49329.
Train: 2018-08-09T11:08:34.216657: step 2099, loss 0.493377.
Train: 2018-08-09T11:08:34.294794: step 2100, loss 0.579571.
Test: 2018-08-09T11:08:34.794646: step 2100, loss 0.548807.
Train: 2018-08-09T11:08:35.388749: step 2101, loss 0.631207.
Train: 2018-08-09T11:08:35.466863: step 2102, loss 0.596708.
Train: 2018-08-09T11:08:35.544966: step 2103, loss 0.596602.
Train: 2018-08-09T11:08:35.623042: step 2104, loss 0.699114.
Train: 2018-08-09T11:08:35.701149: step 2105, loss 0.54528.
Train: 2018-08-09T11:08:35.794903: step 2106, loss 0.664019.
Train: 2018-08-09T11:08:35.857392: step 2107, loss 0.43656.
Train: 2018-08-09T11:08:35.951089: step 2108, loss 0.461613.
Train: 2018-08-09T11:08:36.029197: step 2109, loss 0.579156.
Train: 2018-08-09T11:08:36.107333: step 2110, loss 0.595898.
Test: 2018-08-09T11:08:36.591564: step 2110, loss 0.548018.
Train: 2018-08-09T11:08:36.669701: step 2111, loss 0.445391.
Train: 2018-08-09T11:08:36.747801: step 2112, loss 0.579137.
Train: 2018-08-09T11:08:36.825913: step 2113, loss 0.662698.
Train: 2018-08-09T11:08:36.904021: step 2114, loss 0.562399.
Train: 2018-08-09T11:08:36.983660: step 2115, loss 0.629019.
Train: 2018-08-09T11:08:37.061768: step 2116, loss 0.612236.
Train: 2018-08-09T11:08:37.139873: step 2117, loss 0.496369.
Train: 2018-08-09T11:08:37.218010: step 2118, loss 0.545982.
Train: 2018-08-09T11:08:37.296086: step 2119, loss 0.529558.
Train: 2018-08-09T11:08:37.374193: step 2120, loss 0.562554.
Test: 2018-08-09T11:08:37.874101: step 2120, loss 0.545891.
Train: 2018-08-09T11:08:37.952211: step 2121, loss 0.562543.
Train: 2018-08-09T11:08:38.030317: step 2122, loss 0.513266.
Train: 2018-08-09T11:08:38.108420: step 2123, loss 0.578968.
Train: 2018-08-09T11:08:38.202122: step 2124, loss 0.595381.
Train: 2018-08-09T11:08:38.280257: step 2125, loss 0.562552.
Train: 2018-08-09T11:08:38.358335: step 2126, loss 0.595356.
Train: 2018-08-09T11:08:38.436472: step 2127, loss 0.546165.
Train: 2018-08-09T11:08:38.514549: step 2128, loss 0.611662.
Train: 2018-08-09T11:08:38.592686: step 2129, loss 0.529919.
Train: 2018-08-09T11:08:38.670792: step 2130, loss 0.497353.
Test: 2018-08-09T11:08:39.155678: step 2130, loss 0.549166.
Train: 2018-08-09T11:08:39.233784: step 2131, loss 0.578926.
Train: 2018-08-09T11:08:39.311889: step 2132, loss 0.595241.
Train: 2018-08-09T11:08:39.390022: step 2133, loss 0.464733.
Train: 2018-08-09T11:08:39.468103: step 2134, loss 0.59528.
Train: 2018-08-09T11:08:39.546240: step 2135, loss 0.546294.
Train: 2018-08-09T11:08:39.639968: step 2136, loss 0.513519.
Train: 2018-08-09T11:08:39.718076: step 2137, loss 0.447974.
Train: 2018-08-09T11:08:39.796180: step 2138, loss 0.529714.
Train: 2018-08-09T11:08:39.874258: step 2139, loss 0.59543.
Train: 2018-08-09T11:08:39.952363: step 2140, loss 0.562549.
Test: 2018-08-09T11:08:40.452273: step 2140, loss 0.545723.
Train: 2018-08-09T11:08:40.530383: step 2141, loss 0.628675.
Train: 2018-08-09T11:08:40.608490: step 2142, loss 0.562476.
Train: 2018-08-09T11:08:40.686596: step 2143, loss 0.579019.
Train: 2018-08-09T11:08:40.764702: step 2144, loss 0.545907.
Train: 2018-08-09T11:08:40.842810: step 2145, loss 0.579065.
Train: 2018-08-09T11:08:40.920889: step 2146, loss 0.479579.
Train: 2018-08-09T11:08:40.999023: step 2147, loss 0.579057.
Train: 2018-08-09T11:08:41.077100: step 2148, loss 0.495976.
Train: 2018-08-09T11:08:41.155233: step 2149, loss 0.6124.
Train: 2018-08-09T11:08:41.233343: step 2150, loss 0.62912.
Test: 2018-08-09T11:08:41.733231: step 2150, loss 0.545574.
Train: 2018-08-09T11:08:41.811300: step 2151, loss 0.645731.
Train: 2018-08-09T11:08:41.889438: step 2152, loss 0.545823.
Train: 2018-08-09T11:08:41.979206: step 2153, loss 0.462846.
Train: 2018-08-09T11:08:42.057794: step 2154, loss 0.595684.
Train: 2018-08-09T11:08:42.120248: step 2155, loss 0.545879.
Train: 2018-08-09T11:08:42.214002: step 2156, loss 0.595664.
Train: 2018-08-09T11:08:42.292084: step 2157, loss 0.66196.
Train: 2018-08-09T11:08:42.370220: step 2158, loss 0.54596.
Train: 2018-08-09T11:08:42.448326: step 2159, loss 0.628534.
Train: 2018-08-09T11:08:42.526431: step 2160, loss 0.579012.
Test: 2018-08-09T11:08:43.010698: step 2160, loss 0.548412.
Train: 2018-08-09T11:08:43.088801: step 2161, loss 0.480474.
Train: 2018-08-09T11:08:43.182512: step 2162, loss 0.447785.
Train: 2018-08-09T11:08:43.260606: step 2163, loss 0.529759.
Train: 2018-08-09T11:08:43.338712: step 2164, loss 0.578968.
Train: 2018-08-09T11:08:43.416850: step 2165, loss 0.529653.
Train: 2018-08-09T11:08:43.494927: step 2166, loss 0.52962.
Train: 2018-08-09T11:08:43.573062: step 2167, loss 0.463615.
Train: 2018-08-09T11:08:43.651167: step 2168, loss 0.545948.
Train: 2018-08-09T11:08:43.729247: step 2169, loss 0.562452.
Train: 2018-08-09T11:08:43.807350: step 2170, loss 0.52922.
Test: 2018-08-09T11:08:44.310945: step 2170, loss 0.546817.
Train: 2018-08-09T11:08:44.389082: step 2171, loss 0.445766.
Train: 2018-08-09T11:08:44.467189: step 2172, loss 0.579139.
Train: 2018-08-09T11:08:44.545299: step 2173, loss 0.461656.
Train: 2018-08-09T11:08:44.623403: step 2174, loss 0.596107.
Train: 2018-08-09T11:08:44.701509: step 2175, loss 0.613149.
Train: 2018-08-09T11:08:44.779587: step 2176, loss 0.545378.
Train: 2018-08-09T11:08:44.857717: step 2177, loss 0.613312.
Train: 2018-08-09T11:08:44.935829: step 2178, loss 0.613406.
Train: 2018-08-09T11:08:45.013938: step 2179, loss 0.511317.
Train: 2018-08-09T11:08:45.092011: step 2180, loss 0.562365.
Test: 2018-08-09T11:08:45.591901: step 2180, loss 0.547037.
Train: 2018-08-09T11:08:45.670000: step 2181, loss 0.64747.
Train: 2018-08-09T11:08:45.748138: step 2182, loss 0.562344.
Train: 2018-08-09T11:08:45.826244: step 2183, loss 0.596321.
Train: 2018-08-09T11:08:45.916591: step 2184, loss 0.562352.
Train: 2018-08-09T11:08:45.980128: step 2185, loss 0.528518.
Train: 2018-08-09T11:08:46.073882: step 2186, loss 0.545452.
Train: 2018-08-09T11:08:46.151963: step 2187, loss 0.461012.
Train: 2018-08-09T11:08:46.230099: step 2188, loss 0.596166.
Train: 2018-08-09T11:08:46.308206: step 2189, loss 0.629968.
Train: 2018-08-09T11:08:46.386281: step 2190, loss 0.545465.
Test: 2018-08-09T11:08:46.886163: step 2190, loss 0.547206.
Train: 2018-08-09T11:08:46.964301: step 2191, loss 0.612968.
Train: 2018-08-09T11:08:47.042377: step 2192, loss 0.528728.
Train: 2018-08-09T11:08:47.120487: step 2193, loss 0.596019.
Train: 2018-08-09T11:08:47.198621: step 2194, loss 0.528839.
Train: 2018-08-09T11:08:47.276696: step 2195, loss 0.646189.
Train: 2018-08-09T11:08:47.354835: step 2196, loss 0.579125.
Train: 2018-08-09T11:08:47.432912: step 2197, loss 0.529078.
Train: 2018-08-09T11:08:47.511017: step 2198, loss 0.595778.
Train: 2018-08-09T11:08:47.589149: step 2199, loss 0.54593.
Train: 2018-08-09T11:08:47.667261: step 2200, loss 0.579052.
Test: 2018-08-09T11:08:48.167113: step 2200, loss 0.547592.
Train: 2018-08-09T11:08:48.745945: step 2201, loss 0.628701.
Train: 2018-08-09T11:08:48.824021: step 2202, loss 0.645036.
Train: 2018-08-09T11:08:48.902155: step 2203, loss 0.611869.
Train: 2018-08-09T11:08:48.980266: step 2204, loss 0.611693.
Train: 2018-08-09T11:08:49.058372: step 2205, loss 0.578927.
Train: 2018-08-09T11:08:49.136486: step 2206, loss 0.546468.
Train: 2018-08-09T11:08:49.214585: step 2207, loss 0.514271.
Train: 2018-08-09T11:08:49.292692: step 2208, loss 0.59499.
Train: 2018-08-09T11:08:49.370770: step 2209, loss 0.546741.
Train: 2018-08-09T11:08:49.448905: step 2210, loss 0.594896.
Test: 2018-08-09T11:08:49.948760: step 2210, loss 0.549104.
Train: 2018-08-09T11:08:50.026893: step 2211, loss 0.578866.
Train: 2018-08-09T11:08:50.104995: step 2212, loss 0.562911.
Train: 2018-08-09T11:08:50.183081: step 2213, loss 0.594771.
Train: 2018-08-09T11:08:50.261218: step 2214, loss 0.547109.
Train: 2018-08-09T11:08:50.339290: step 2215, loss 0.642243.
Train: 2018-08-09T11:08:50.417398: step 2216, loss 0.499848.
Train: 2018-08-09T11:08:50.511165: step 2217, loss 0.563051.
Train: 2018-08-09T11:08:50.589262: step 2218, loss 0.500105.
Train: 2018-08-09T11:08:50.667369: step 2219, loss 0.563043.
Train: 2018-08-09T11:08:50.745474: step 2220, loss 0.499978.
Test: 2018-08-09T11:08:51.247636: step 2220, loss 0.549994.
Train: 2018-08-09T11:08:51.310121: step 2221, loss 0.547298.
Train: 2018-08-09T11:08:51.388257: step 2222, loss 0.562894.
Train: 2018-08-09T11:08:51.481981: step 2223, loss 0.546869.
Train: 2018-08-09T11:08:51.560061: step 2224, loss 0.626833.
Train: 2018-08-09T11:08:51.638167: step 2225, loss 0.595056.
Train: 2018-08-09T11:08:51.716275: step 2226, loss 0.547232.
Train: 2018-08-09T11:08:51.794382: step 2227, loss 0.482803.
Train: 2018-08-09T11:08:51.872518: step 2228, loss 0.610678.
Train: 2018-08-09T11:08:51.950596: step 2229, loss 0.595034.
Train: 2018-08-09T11:08:52.028701: step 2230, loss 0.514492.
Test: 2018-08-09T11:08:52.528583: step 2230, loss 0.548349.
Train: 2018-08-09T11:08:52.606727: step 2231, loss 0.530504.
Train: 2018-08-09T11:08:52.684826: step 2232, loss 0.595191.
Train: 2018-08-09T11:08:52.762935: step 2233, loss 0.659536.
Train: 2018-08-09T11:08:52.841040: step 2234, loss 0.466049.
Train: 2018-08-09T11:08:52.921405: step 2235, loss 0.56244.
Train: 2018-08-09T11:08:52.999540: step 2236, loss 0.579749.
Train: 2018-08-09T11:08:53.077620: step 2237, loss 0.676875.
Train: 2018-08-09T11:08:53.155759: step 2238, loss 0.578427.
Train: 2018-08-09T11:08:53.233833: step 2239, loss 0.482641.
Train: 2018-08-09T11:08:53.311939: step 2240, loss 0.675332.
Test: 2018-08-09T11:08:53.811871: step 2240, loss 0.549671.
Train: 2018-08-09T11:08:53.889928: step 2241, loss 0.64276.
Train: 2018-08-09T11:08:53.968035: step 2242, loss 0.515146.
Train: 2018-08-09T11:08:54.046170: step 2243, loss 0.483476.
Train: 2018-08-09T11:08:54.124279: step 2244, loss 0.578858.
Train: 2018-08-09T11:08:54.202384: step 2245, loss 0.515298.
Train: 2018-08-09T11:08:54.280490: step 2246, loss 0.753787.
Train: 2018-08-09T11:08:54.358598: step 2247, loss 0.531274.
Train: 2018-08-09T11:08:54.436707: step 2248, loss 0.673876.
Train: 2018-08-09T11:08:54.514815: step 2249, loss 0.594647.
Train: 2018-08-09T11:08:54.592918: step 2250, loss 0.484484.
Test: 2018-08-09T11:08:55.093675: step 2250, loss 0.548449.
Train: 2018-08-09T11:08:55.171748: step 2251, loss 0.516047.
Train: 2018-08-09T11:08:55.249885: step 2252, loss 0.594574.
Train: 2018-08-09T11:08:55.327988: step 2253, loss 0.625934.
Train: 2018-08-09T11:08:55.406068: step 2254, loss 0.531899.
Train: 2018-08-09T11:08:55.484210: step 2255, loss 0.547585.
Train: 2018-08-09T11:08:55.577928: step 2256, loss 0.485025.
Train: 2018-08-09T11:08:55.656039: step 2257, loss 0.578879.
Train: 2018-08-09T11:08:55.734151: step 2258, loss 0.516163.
Train: 2018-08-09T11:08:55.812247: step 2259, loss 0.704535.
Train: 2018-08-09T11:08:55.890328: step 2260, loss 0.563177.
Test: 2018-08-09T11:08:56.374591: step 2260, loss 0.550855.
Train: 2018-08-09T11:08:56.452722: step 2261, loss 0.531803.
Train: 2018-08-09T11:08:56.530837: step 2262, loss 0.59457.
Train: 2018-08-09T11:08:56.608936: step 2263, loss 0.578875.
Train: 2018-08-09T11:08:56.687042: step 2264, loss 0.61025.
Train: 2018-08-09T11:08:56.780744: step 2265, loss 0.657248.
Train: 2018-08-09T11:08:56.858882: step 2266, loss 0.563246.
Train: 2018-08-09T11:08:56.936989: step 2267, loss 0.547678.
Train: 2018-08-09T11:08:57.015108: step 2268, loss 0.516551.
Train: 2018-08-09T11:08:57.093197: step 2269, loss 0.500978.
Train: 2018-08-09T11:08:57.171307: step 2270, loss 0.594491.
Test: 2018-08-09T11:08:57.671177: step 2270, loss 0.553967.
Train: 2018-08-09T11:08:57.749296: step 2271, loss 0.57889.
Train: 2018-08-09T11:08:57.827403: step 2272, loss 0.672587.
Train: 2018-08-09T11:08:57.906891: step 2273, loss 0.46973.
Train: 2018-08-09T11:08:57.985029: step 2274, loss 0.53208.
Train: 2018-08-09T11:08:58.063135: step 2275, loss 0.547641.
Train: 2018-08-09T11:08:58.141212: step 2276, loss 0.547587.
Train: 2018-08-09T11:08:58.219349: step 2277, loss 0.594553.
Train: 2018-08-09T11:08:58.297454: step 2278, loss 0.688736.
Train: 2018-08-09T11:08:58.375562: step 2279, loss 0.625916.
Train: 2018-08-09T11:08:58.453668: step 2280, loss 0.563231.
Test: 2018-08-09T11:08:58.953521: step 2280, loss 0.549798.
Train: 2018-08-09T11:08:59.031657: step 2281, loss 0.531999.
Train: 2018-08-09T11:08:59.109734: step 2282, loss 0.469565.
Train: 2018-08-09T11:08:59.187871: step 2283, loss 0.5007.
Train: 2018-08-09T11:08:59.265946: step 2284, loss 0.625898.
Train: 2018-08-09T11:08:59.344053: step 2285, loss 0.594565.
Train: 2018-08-09T11:08:59.422191: step 2286, loss 0.578874.
Train: 2018-08-09T11:08:59.500296: step 2287, loss 0.610294.
Train: 2018-08-09T11:08:59.578402: step 2288, loss 0.484636.
Train: 2018-08-09T11:08:59.656510: step 2289, loss 0.594594.
Train: 2018-08-09T11:08:59.734588: step 2290, loss 0.594606.
Test: 2018-08-09T11:09:00.238868: step 2290, loss 0.550163.
Train: 2018-08-09T11:09:00.317001: step 2291, loss 0.563127.
Train: 2018-08-09T11:09:00.395108: step 2292, loss 0.547376.
Train: 2018-08-09T11:09:00.473215: step 2293, loss 0.515838.
Train: 2018-08-09T11:09:00.551326: step 2294, loss 0.5473.
Train: 2018-08-09T11:09:00.629403: step 2295, loss 0.578862.
Train: 2018-08-09T11:09:00.707510: step 2296, loss 0.578857.
Train: 2018-08-09T11:09:00.785615: step 2297, loss 0.594709.
Train: 2018-08-09T11:09:00.863721: step 2298, loss 0.515407.
Train: 2018-08-09T11:09:00.941862: step 2299, loss 0.578856.
Train: 2018-08-09T11:09:01.019966: step 2300, loss 0.483435.
Test: 2018-08-09T11:09:01.519818: step 2300, loss 0.549184.
Train: 2018-08-09T11:09:02.113459: step 2301, loss 0.499144.
Train: 2018-08-09T11:09:02.191534: step 2302, loss 0.610856.
Train: 2018-08-09T11:09:02.269642: step 2303, loss 0.578867.
Train: 2018-08-09T11:09:02.347747: step 2304, loss 0.530679.
Train: 2018-08-09T11:09:02.425853: step 2305, loss 0.450071.
Train: 2018-08-09T11:09:02.503961: step 2306, loss 0.514229.
Train: 2018-08-09T11:09:02.582066: step 2307, loss 0.497723.
Train: 2018-08-09T11:09:02.660204: step 2308, loss 0.595246.
Train: 2018-08-09T11:09:02.738312: step 2309, loss 0.644479.
Train: 2018-08-09T11:09:02.816388: step 2310, loss 0.513299.
Test: 2018-08-09T11:09:03.316269: step 2310, loss 0.548967.
Train: 2018-08-09T11:09:03.394405: step 2311, loss 0.611906.
Train: 2018-08-09T11:09:03.472513: step 2312, loss 0.447079.
Train: 2018-08-09T11:09:03.550620: step 2313, loss 0.529399.
Train: 2018-08-09T11:09:03.628725: step 2314, loss 0.579064.
Train: 2018-08-09T11:09:03.706832: step 2315, loss 0.695668.
Train: 2018-08-09T11:09:03.784911: step 2316, loss 0.645702.
Train: 2018-08-09T11:09:03.863015: step 2317, loss 0.545808.
Train: 2018-08-09T11:09:03.942559: step 2318, loss 0.562452.
Train: 2018-08-09T11:09:04.020672: step 2319, loss 0.595672.
Train: 2018-08-09T11:09:04.098777: step 2320, loss 0.595638.
Test: 2018-08-09T11:09:04.598655: step 2320, loss 0.548219.
Train: 2018-08-09T11:09:04.676762: step 2321, loss 0.463138.
Train: 2018-08-09T11:09:04.754843: step 2322, loss 0.612136.
Train: 2018-08-09T11:09:04.848602: step 2323, loss 0.529405.
Train: 2018-08-09T11:09:04.926707: step 2324, loss 0.562491.
Train: 2018-08-09T11:09:05.004814: step 2325, loss 0.595553.
Train: 2018-08-09T11:09:05.082923: step 2326, loss 0.645063.
Train: 2018-08-09T11:09:05.160998: step 2327, loss 0.546043.
Train: 2018-08-09T11:09:05.239103: step 2328, loss 0.546078.
Train: 2018-08-09T11:09:05.332863: step 2329, loss 0.562544.
Train: 2018-08-09T11:09:05.410969: step 2330, loss 0.562568.
Test: 2018-08-09T11:09:05.895199: step 2330, loss 0.548454.
Train: 2018-08-09T11:09:05.973306: step 2331, loss 0.464237.
Train: 2018-08-09T11:09:06.051442: step 2332, loss 0.513355.
Train: 2018-08-09T11:09:06.129550: step 2333, loss 0.562537.
Train: 2018-08-09T11:09:06.207657: step 2334, loss 0.562528.
Train: 2018-08-09T11:09:06.285762: step 2335, loss 0.546075.
Train: 2018-08-09T11:09:06.379485: step 2336, loss 0.546046.
Train: 2018-08-09T11:09:06.457598: step 2337, loss 0.562535.
Train: 2018-08-09T11:09:06.535704: step 2338, loss 0.579001.
Train: 2018-08-09T11:09:06.613811: step 2339, loss 0.545958.
Train: 2018-08-09T11:09:06.691918: step 2340, loss 0.612117.
Test: 2018-08-09T11:09:07.176148: step 2340, loss 0.550748.
Train: 2018-08-09T11:09:07.254284: step 2341, loss 0.545948.
Train: 2018-08-09T11:09:07.332360: step 2342, loss 0.562469.
Train: 2018-08-09T11:09:07.426113: step 2343, loss 0.545953.
Train: 2018-08-09T11:09:07.504223: step 2344, loss 0.545956.
Train: 2018-08-09T11:09:07.566713: step 2345, loss 0.529383.
Train: 2018-08-09T11:09:07.660408: step 2346, loss 0.512777.
Train: 2018-08-09T11:09:07.738516: step 2347, loss 0.628791.
Train: 2018-08-09T11:09:07.816652: step 2348, loss 0.562482.
Train: 2018-08-09T11:09:07.894729: step 2349, loss 0.579042.
Train: 2018-08-09T11:09:07.974300: step 2350, loss 0.628831.
Test: 2018-08-09T11:09:08.474154: step 2350, loss 0.549449.
Train: 2018-08-09T11:09:08.552292: step 2351, loss 0.628757.
Train: 2018-08-09T11:09:08.630396: step 2352, loss 0.529422.
Train: 2018-08-09T11:09:08.708474: step 2353, loss 0.595538.
Train: 2018-08-09T11:09:08.786609: step 2354, loss 0.562526.
Train: 2018-08-09T11:09:08.864686: step 2355, loss 0.562549.
Train: 2018-08-09T11:09:08.942824: step 2356, loss 0.562574.
Train: 2018-08-09T11:09:09.020900: step 2357, loss 0.562578.
Train: 2018-08-09T11:09:09.099036: step 2358, loss 0.497085.
Train: 2018-08-09T11:09:09.177139: step 2359, loss 0.595305.
Train: 2018-08-09T11:09:09.255219: step 2360, loss 0.595289.
Test: 2018-08-09T11:09:09.755128: step 2360, loss 0.54915.
Train: 2018-08-09T11:09:09.833232: step 2361, loss 0.660596.
Train: 2018-08-09T11:09:09.913619: step 2362, loss 0.6278.
Train: 2018-08-09T11:09:09.991739: step 2363, loss 0.513974.
Train: 2018-08-09T11:09:10.069863: step 2364, loss 0.497926.
Train: 2018-08-09T11:09:10.147969: step 2365, loss 0.562729.
Train: 2018-08-09T11:09:10.226047: step 2366, loss 0.498052.
Train: 2018-08-09T11:09:10.304152: step 2367, loss 0.578897.
Train: 2018-08-09T11:09:10.382259: step 2368, loss 0.578873.
Train: 2018-08-09T11:09:10.460367: step 2369, loss 0.498033.
Train: 2018-08-09T11:09:10.538502: step 2370, loss 0.46555.
Test: 2018-08-09T11:09:11.038368: step 2370, loss 0.549892.
Train: 2018-08-09T11:09:11.116494: step 2371, loss 0.59508.
Train: 2018-08-09T11:09:11.194568: step 2372, loss 0.562309.
Train: 2018-08-09T11:09:11.272703: step 2373, loss 0.513723.
Train: 2018-08-09T11:09:11.350780: step 2374, loss 0.545748.
Train: 2018-08-09T11:09:11.428917: step 2375, loss 0.514.
Train: 2018-08-09T11:09:11.522617: step 2376, loss 0.679825.
Train: 2018-08-09T11:09:11.585133: step 2377, loss 0.49453.
Train: 2018-08-09T11:09:11.678829: step 2378, loss 0.578362.
Train: 2018-08-09T11:09:11.756966: step 2379, loss 0.512746.
Train: 2018-08-09T11:09:11.835041: step 2380, loss 0.476979.
Test: 2018-08-09T11:09:12.321590: step 2380, loss 0.547697.
Train: 2018-08-09T11:09:12.399726: step 2381, loss 0.697013.
Train: 2018-08-09T11:09:12.477832: step 2382, loss 0.579731.
Train: 2018-08-09T11:09:12.561683: step 2383, loss 0.577663.
Train: 2018-08-09T11:09:12.639793: step 2384, loss 0.560414.
Train: 2018-08-09T11:09:12.717871: step 2385, loss 0.525936.
Train: 2018-08-09T11:09:12.795976: step 2386, loss 0.578946.
Train: 2018-08-09T11:09:12.874084: step 2387, loss 0.546481.
Train: 2018-08-09T11:09:12.952221: step 2388, loss 0.558957.
Train: 2018-08-09T11:09:13.030298: step 2389, loss 0.496045.
Train: 2018-08-09T11:09:13.108403: step 2390, loss 0.457152.
Test: 2018-08-09T11:09:13.608286: step 2390, loss 0.546548.
Train: 2018-08-09T11:09:13.686393: step 2391, loss 0.569029.
Train: 2018-08-09T11:09:13.764499: step 2392, loss 0.52336.
Train: 2018-08-09T11:09:13.842637: step 2393, loss 0.620924.
Train: 2018-08-09T11:09:13.932036: step 2394, loss 0.509945.
Train: 2018-08-09T11:09:14.010149: step 2395, loss 0.595345.
Train: 2018-08-09T11:09:14.088257: step 2396, loss 0.494725.
Train: 2018-08-09T11:09:14.166333: step 2397, loss 0.596229.
Train: 2018-08-09T11:09:14.244438: step 2398, loss 0.650398.
Train: 2018-08-09T11:09:14.322572: step 2399, loss 0.528864.
Train: 2018-08-09T11:09:14.400683: step 2400, loss 0.563197.
Test: 2018-08-09T11:09:14.892632: step 2400, loss 0.547067.
Train: 2018-08-09T11:09:15.501868: step 2401, loss 0.613403.
Train: 2018-08-09T11:09:15.579978: step 2402, loss 0.578566.
Train: 2018-08-09T11:09:15.658086: step 2403, loss 0.49641.
Train: 2018-08-09T11:09:15.736188: step 2404, loss 0.530073.
Train: 2018-08-09T11:09:15.814295: step 2405, loss 0.561928.
Train: 2018-08-09T11:09:15.908023: step 2406, loss 0.512951.
Train: 2018-08-09T11:09:15.986128: step 2407, loss 0.530624.
Train: 2018-08-09T11:09:16.048614: step 2408, loss 0.51031.
Train: 2018-08-09T11:09:16.142312: step 2409, loss 0.546068.
Train: 2018-08-09T11:09:16.220448: step 2410, loss 0.546343.
Test: 2018-08-09T11:09:16.704705: step 2410, loss 0.550834.
Train: 2018-08-09T11:09:16.782816: step 2411, loss 0.562515.
Train: 2018-08-09T11:09:16.860922: step 2412, loss 0.562568.
Train: 2018-08-09T11:09:16.941358: step 2413, loss 0.595397.
Train: 2018-08-09T11:09:17.019469: step 2414, loss 0.513001.
Train: 2018-08-09T11:09:17.097545: step 2415, loss 0.479901.
Train: 2018-08-09T11:09:17.175678: step 2416, loss 0.612022.
Train: 2018-08-09T11:09:17.253790: step 2417, loss 0.579025.
Train: 2018-08-09T11:09:17.331895: step 2418, loss 0.562481.
Train: 2018-08-09T11:09:17.425625: step 2419, loss 0.52925.
Train: 2018-08-09T11:09:17.503731: step 2420, loss 0.545826.
Test: 2018-08-09T11:09:17.987960: step 2420, loss 0.551892.
Train: 2018-08-09T11:09:18.066093: step 2421, loss 0.579075.
Train: 2018-08-09T11:09:18.144204: step 2422, loss 0.612407.
Train: 2018-08-09T11:09:18.237956: step 2423, loss 0.479229.
Train: 2018-08-09T11:09:18.300387: step 2424, loss 0.612421.
Train: 2018-08-09T11:09:18.378525: step 2425, loss 0.54572.
Train: 2018-08-09T11:09:18.472223: step 2426, loss 0.512428.
Train: 2018-08-09T11:09:18.550358: step 2427, loss 0.562364.
Train: 2018-08-09T11:09:18.628466: step 2428, loss 0.59583.
Train: 2018-08-09T11:09:18.706543: step 2429, loss 0.495678.
Train: 2018-08-09T11:09:18.784679: step 2430, loss 0.528961.
Test: 2018-08-09T11:09:19.277287: step 2430, loss 0.549883.
Train: 2018-08-09T11:09:19.355392: step 2431, loss 0.512206.
Train: 2018-08-09T11:09:19.433530: step 2432, loss 0.545674.
Train: 2018-08-09T11:09:19.511638: step 2433, loss 0.511967.
Train: 2018-08-09T11:09:19.589712: step 2434, loss 0.680258.
Train: 2018-08-09T11:09:19.667819: step 2435, loss 0.528572.
Train: 2018-08-09T11:09:19.745928: step 2436, loss 0.562375.
Train: 2018-08-09T11:09:19.824061: step 2437, loss 0.461126.
Train: 2018-08-09T11:09:19.902170: step 2438, loss 0.477802.
Train: 2018-08-09T11:09:19.995893: step 2439, loss 0.596188.
Train: 2018-08-09T11:09:20.073973: step 2440, loss 0.511339.
Test: 2018-08-09T11:09:20.558260: step 2440, loss 0.549605.
Train: 2018-08-09T11:09:20.636342: step 2441, loss 0.46005.
Train: 2018-08-09T11:09:20.714472: step 2442, loss 0.545519.
Train: 2018-08-09T11:09:20.808214: step 2443, loss 0.527887.
Train: 2018-08-09T11:09:20.870695: step 2444, loss 0.545361.
Train: 2018-08-09T11:09:20.964421: step 2445, loss 0.45845.
Train: 2018-08-09T11:09:21.042496: step 2446, loss 0.631915.
Train: 2018-08-09T11:09:21.120632: step 2447, loss 0.579814.
Train: 2018-08-09T11:09:21.198739: step 2448, loss 0.631644.
Train: 2018-08-09T11:09:21.276846: step 2449, loss 0.440511.
Train: 2018-08-09T11:09:21.354922: step 2450, loss 0.527771.
Test: 2018-08-09T11:09:21.854804: step 2450, loss 0.545331.
Train: 2018-08-09T11:09:21.932910: step 2451, loss 0.52772.
Train: 2018-08-09T11:09:22.011048: step 2452, loss 0.666918.
Train: 2018-08-09T11:09:22.089152: step 2453, loss 0.615036.
Train: 2018-08-09T11:09:22.167261: step 2454, loss 0.510366.
Train: 2018-08-09T11:09:22.245337: step 2455, loss 0.56243.
Train: 2018-08-09T11:09:22.323474: step 2456, loss 0.562642.
Train: 2018-08-09T11:09:22.401580: step 2457, loss 0.457725.
Train: 2018-08-09T11:09:22.479657: step 2458, loss 0.597189.
Train: 2018-08-09T11:09:22.557798: step 2459, loss 0.475165.
Train: 2018-08-09T11:09:22.635900: step 2460, loss 0.5448.
Test: 2018-08-09T11:09:23.138150: step 2460, loss 0.547318.
Train: 2018-08-09T11:09:23.216223: step 2461, loss 0.597334.
Train: 2018-08-09T11:09:23.294361: step 2462, loss 0.509764.
Train: 2018-08-09T11:09:23.372468: step 2463, loss 0.719625.
Train: 2018-08-09T11:09:23.450542: step 2464, loss 0.475326.
Train: 2018-08-09T11:09:23.528681: step 2465, loss 0.579692.
Train: 2018-08-09T11:09:23.606760: step 2466, loss 0.510096.
Train: 2018-08-09T11:09:23.684893: step 2467, loss 0.631819.
Train: 2018-08-09T11:09:23.763001: step 2468, loss 0.562335.
Train: 2018-08-09T11:09:23.841106: step 2469, loss 0.544854.
Train: 2018-08-09T11:09:23.919213: step 2470, loss 0.527729.
Test: 2018-08-09T11:09:24.419078: step 2470, loss 0.546149.
Train: 2018-08-09T11:09:24.497172: step 2471, loss 0.527849.
Train: 2018-08-09T11:09:24.575277: step 2472, loss 0.596822.
Train: 2018-08-09T11:09:24.653417: step 2473, loss 0.510543.
Train: 2018-08-09T11:09:24.731521: step 2474, loss 0.510807.
Train: 2018-08-09T11:09:24.809628: step 2475, loss 0.614104.
Train: 2018-08-09T11:09:24.887705: step 2476, loss 0.596955.
Train: 2018-08-09T11:09:24.968152: step 2477, loss 0.59694.
Train: 2018-08-09T11:09:25.046259: step 2478, loss 0.579559.
Train: 2018-08-09T11:09:25.124364: step 2479, loss 0.579362.
Train: 2018-08-09T11:09:25.202472: step 2480, loss 0.596586.
Test: 2018-08-09T11:09:25.702324: step 2480, loss 0.54834.
Train: 2018-08-09T11:09:25.780430: step 2481, loss 0.579316.
Train: 2018-08-09T11:09:25.858538: step 2482, loss 0.697923.
Train: 2018-08-09T11:09:25.936675: step 2483, loss 0.51179.
Train: 2018-08-09T11:09:26.014780: step 2484, loss 0.57923.
Train: 2018-08-09T11:09:26.092858: step 2485, loss 0.646022.
Train: 2018-08-09T11:09:26.170995: step 2486, loss 0.479257.
Train: 2018-08-09T11:09:26.249100: step 2487, loss 0.562491.
Train: 2018-08-09T11:09:26.327207: step 2488, loss 0.612077.
Train: 2018-08-09T11:09:26.420936: step 2489, loss 0.595475.
Train: 2018-08-09T11:09:26.499042: step 2490, loss 0.562574.
Test: 2018-08-09T11:09:26.983905: step 2490, loss 0.547891.
Train: 2018-08-09T11:09:27.062008: step 2491, loss 0.627993.
Train: 2018-08-09T11:09:27.140117: step 2492, loss 0.530126.
Train: 2018-08-09T11:09:27.233837: step 2493, loss 0.497772.
Train: 2018-08-09T11:09:27.311919: step 2494, loss 0.659909.
Train: 2018-08-09T11:09:27.390058: step 2495, loss 0.530425.
Train: 2018-08-09T11:09:27.468164: step 2496, loss 0.562759.
Train: 2018-08-09T11:09:27.546269: step 2497, loss 0.611038.
Train: 2018-08-09T11:09:27.624375: step 2498, loss 0.610915.
Train: 2018-08-09T11:09:27.702453: step 2499, loss 0.642748.
Train: 2018-08-09T11:09:27.780588: step 2500, loss 0.419841.
Test: 2018-08-09T11:09:28.280442: step 2500, loss 0.549901.
Train: 2018-08-09T11:09:28.827217: step 2501, loss 0.483512.
Train: 2018-08-09T11:09:28.936567: step 2502, loss 0.674232.
Train: 2018-08-09T11:09:29.014675: step 2503, loss 0.562894.
Train: 2018-08-09T11:09:29.092779: step 2504, loss 0.531274.
Train: 2018-08-09T11:09:29.170855: step 2505, loss 0.531264.
Train: 2018-08-09T11:09:29.248991: step 2506, loss 0.563103.
Train: 2018-08-09T11:09:29.327099: step 2507, loss 0.56303.
Train: 2018-08-09T11:09:29.405206: step 2508, loss 0.49946.
Train: 2018-08-09T11:09:29.483315: step 2509, loss 0.690253.
Train: 2018-08-09T11:09:29.561419: step 2510, loss 0.594743.
Test: 2018-08-09T11:09:30.062832: step 2510, loss 0.5493.
Train: 2018-08-09T11:09:30.172210: step 2511, loss 0.626505.
Train: 2018-08-09T11:09:30.250316: step 2512, loss 0.452071.
Train: 2018-08-09T11:09:30.328392: step 2513, loss 0.578861.
Train: 2018-08-09T11:09:30.406530: step 2514, loss 0.642266.
Train: 2018-08-09T11:09:30.484631: step 2515, loss 0.547197.
Train: 2018-08-09T11:09:30.562714: step 2516, loss 0.499753.
Train: 2018-08-09T11:09:30.640850: step 2517, loss 0.610522.
Train: 2018-08-09T11:09:30.718959: step 2518, loss 0.531378.
Train: 2018-08-09T11:09:30.797033: step 2519, loss 0.563025.
Train: 2018-08-09T11:09:30.875170: step 2520, loss 0.563016.
Test: 2018-08-09T11:09:31.370170: step 2520, loss 0.549351.
Train: 2018-08-09T11:09:31.448280: step 2521, loss 0.610566.
Train: 2018-08-09T11:09:31.526382: step 2522, loss 0.547158.
Train: 2018-08-09T11:09:31.604466: step 2523, loss 0.49959.
Train: 2018-08-09T11:09:31.682600: step 2524, loss 0.626486.
Train: 2018-08-09T11:09:31.776310: step 2525, loss 0.515337.
Train: 2018-08-09T11:09:31.854440: step 2526, loss 0.483464.
Train: 2018-08-09T11:09:31.932512: step 2527, loss 0.626678.
Train: 2018-08-09T11:09:32.010618: step 2528, loss 0.562907.
Train: 2018-08-09T11:09:32.088753: step 2529, loss 0.514928.
Train: 2018-08-09T11:09:32.166861: step 2530, loss 0.578861.
Test: 2018-08-09T11:09:32.666714: step 2530, loss 0.550823.
Train: 2018-08-09T11:09:32.744821: step 2531, loss 0.611001.
Train: 2018-08-09T11:09:32.822928: step 2532, loss 0.578869.
Train: 2018-08-09T11:09:32.901060: step 2533, loss 0.578858.
Train: 2018-08-09T11:09:32.979173: step 2534, loss 0.514643.
Train: 2018-08-09T11:09:33.057276: step 2535, loss 0.594951.
Train: 2018-08-09T11:09:33.135355: step 2536, loss 0.627102.
Train: 2018-08-09T11:09:33.213490: step 2537, loss 0.707393.
Train: 2018-08-09T11:09:33.291596: step 2538, loss 0.562853.
Train: 2018-08-09T11:09:33.369707: step 2539, loss 0.530945.
Train: 2018-08-09T11:09:33.447784: step 2540, loss 0.674523.
Test: 2018-08-09T11:09:33.950042: step 2540, loss 0.549887.
Train: 2018-08-09T11:09:34.028178: step 2541, loss 0.547085.
Train: 2018-08-09T11:09:34.106284: step 2542, loss 0.547168.
Train: 2018-08-09T11:09:34.184392: step 2543, loss 0.56305.
Train: 2018-08-09T11:09:34.262500: step 2544, loss 0.610433.
Train: 2018-08-09T11:09:34.340605: step 2545, loss 0.610361.
Train: 2018-08-09T11:09:34.418712: step 2546, loss 0.65739.
Train: 2018-08-09T11:09:34.496787: step 2547, loss 0.547604.
Train: 2018-08-09T11:09:34.574893: step 2548, loss 0.532127.
Train: 2018-08-09T11:09:34.653033: step 2549, loss 0.578903.
Train: 2018-08-09T11:09:34.731106: step 2550, loss 0.578912.
Test: 2018-08-09T11:09:35.231037: step 2550, loss 0.549499.
Train: 2018-08-09T11:09:35.309125: step 2551, loss 0.470474.
Train: 2018-08-09T11:09:35.387235: step 2552, loss 0.594416.
Train: 2018-08-09T11:09:35.465310: step 2553, loss 0.50147.
Train: 2018-08-09T11:09:35.543447: step 2554, loss 0.547902.
Train: 2018-08-09T11:09:35.621523: step 2555, loss 0.51679.
Train: 2018-08-09T11:09:35.699660: step 2556, loss 0.6723.
Train: 2018-08-09T11:09:35.777765: step 2557, loss 0.563326.
Train: 2018-08-09T11:09:35.855874: step 2558, loss 0.641215.
Train: 2018-08-09T11:09:35.936411: step 2559, loss 0.501056.
Train: 2018-08-09T11:09:36.014520: step 2560, loss 0.641205.
Test: 2018-08-09T11:09:36.514401: step 2560, loss 0.549332.
Train: 2018-08-09T11:09:36.592476: step 2561, loss 0.532166.
Train: 2018-08-09T11:09:36.670581: step 2562, loss 0.64121.
Train: 2018-08-09T11:09:36.748717: step 2563, loss 0.594461.
Train: 2018-08-09T11:09:36.826794: step 2564, loss 0.563367.
Train: 2018-08-09T11:09:36.904903: step 2565, loss 0.470175.
Train: 2018-08-09T11:09:36.983042: step 2566, loss 0.563462.
Train: 2018-08-09T11:09:37.061116: step 2567, loss 0.500716.
Train: 2018-08-09T11:09:37.139255: step 2568, loss 0.610378.
Train: 2018-08-09T11:09:37.217330: step 2569, loss 0.547563.
Train: 2018-08-09T11:09:37.295465: step 2570, loss 0.641861.
Test: 2018-08-09T11:09:37.795317: step 2570, loss 0.551291.
Train: 2018-08-09T11:09:37.873453: step 2571, loss 0.53129.
Train: 2018-08-09T11:09:37.952196: step 2572, loss 0.627656.
Train: 2018-08-09T11:09:38.030331: step 2573, loss 0.500978.
Train: 2018-08-09T11:09:38.108440: step 2574, loss 0.500051.
Train: 2018-08-09T11:09:38.186550: step 2575, loss 0.594947.
Train: 2018-08-09T11:09:38.264654: step 2576, loss 0.531783.
Train: 2018-08-09T11:09:38.342761: step 2577, loss 0.57883.
Train: 2018-08-09T11:09:38.420867: step 2578, loss 0.563167.
Train: 2018-08-09T11:09:38.514591: step 2579, loss 0.594658.
Train: 2018-08-09T11:09:38.592702: step 2580, loss 0.515702.
Test: 2018-08-09T11:09:39.076964: step 2580, loss 0.548731.
Train: 2018-08-09T11:09:39.155070: step 2581, loss 0.658112.
Train: 2018-08-09T11:09:39.233175: step 2582, loss 0.483769.
Train: 2018-08-09T11:09:39.311252: step 2583, loss 0.610559.
Train: 2018-08-09T11:09:39.389389: step 2584, loss 0.61076.
Train: 2018-08-09T11:09:39.467496: step 2585, loss 0.499413.
Train: 2018-08-09T11:09:39.561196: step 2586, loss 0.57883.
Train: 2018-08-09T11:09:39.639333: step 2587, loss 0.563018.
Train: 2018-08-09T11:09:39.717409: step 2588, loss 0.578869.
Train: 2018-08-09T11:09:39.795541: step 2589, loss 0.61068.
Train: 2018-08-09T11:09:39.873621: step 2590, loss 0.531138.
Test: 2018-08-09T11:09:40.373504: step 2590, loss 0.546815.
Train: 2018-08-09T11:09:40.451639: step 2591, loss 0.578831.
Train: 2018-08-09T11:09:40.529745: step 2592, loss 0.610704.
Train: 2018-08-09T11:09:40.607853: step 2593, loss 0.642549.
Train: 2018-08-09T11:09:40.685961: step 2594, loss 0.53121.
Train: 2018-08-09T11:09:40.764065: step 2595, loss 0.531259.
Train: 2018-08-09T11:09:40.842168: step 2596, loss 0.562995.
Train: 2018-08-09T11:09:40.921754: step 2597, loss 0.483688.
Train: 2018-08-09T11:09:40.999861: step 2598, loss 0.626511.
Train: 2018-08-09T11:09:41.077963: step 2599, loss 0.531194.
Train: 2018-08-09T11:09:41.156043: step 2600, loss 0.467543.
Test: 2018-08-09T11:09:41.655926: step 2600, loss 0.548582.
Train: 2018-08-09T11:09:42.202701: step 2601, loss 0.46726.
Train: 2018-08-09T11:09:42.280808: step 2602, loss 0.578864.
Train: 2018-08-09T11:09:42.358886: step 2603, loss 0.594932.
Train: 2018-08-09T11:09:42.436991: step 2604, loss 0.594979.
Train: 2018-08-09T11:09:42.515128: step 2605, loss 0.595013.
Train: 2018-08-09T11:09:42.608826: step 2606, loss 0.578885.
Train: 2018-08-09T11:09:42.686933: step 2607, loss 0.659697.
Train: 2018-08-09T11:09:42.765070: step 2608, loss 0.611179.
Train: 2018-08-09T11:09:42.843148: step 2609, loss 0.530517.
Train: 2018-08-09T11:09:42.921854: step 2610, loss 0.594985.
Test: 2018-08-09T11:09:43.408092: step 2610, loss 0.54954.
Train: 2018-08-09T11:09:43.486200: step 2611, loss 0.498441.
Train: 2018-08-09T11:09:43.564337: step 2612, loss 0.530614.
Train: 2018-08-09T11:09:43.658059: step 2613, loss 0.482301.
Train: 2018-08-09T11:09:43.720519: step 2614, loss 0.514372.
Train: 2018-08-09T11:09:43.814248: step 2615, loss 0.611228.
Train: 2018-08-09T11:09:43.892385: step 2616, loss 0.578896.
Train: 2018-08-09T11:09:43.970485: step 2617, loss 0.530257.
Train: 2018-08-09T11:09:44.048598: step 2618, loss 0.562665.
Train: 2018-08-09T11:09:44.126705: step 2619, loss 0.481333.
Train: 2018-08-09T11:09:44.204780: step 2620, loss 0.578938.
Test: 2018-08-09T11:09:44.704663: step 2620, loss 0.546046.
Train: 2018-08-09T11:09:44.782799: step 2621, loss 0.529904.
Train: 2018-08-09T11:09:44.860906: step 2622, loss 0.546194.
Train: 2018-08-09T11:09:44.938982: step 2623, loss 0.480418.
Train: 2018-08-09T11:09:45.017118: step 2624, loss 0.628461.
Train: 2018-08-09T11:09:45.095203: step 2625, loss 0.545948.
Train: 2018-08-09T11:09:45.173329: step 2626, loss 0.562472.
Train: 2018-08-09T11:09:45.251409: step 2627, loss 0.612244.
Train: 2018-08-09T11:09:45.329514: step 2628, loss 0.612207.
Train: 2018-08-09T11:09:45.411294: step 2629, loss 0.496116.
Train: 2018-08-09T11:09:45.489401: step 2630, loss 0.562443.
Test: 2018-08-09T11:09:45.989252: step 2630, loss 0.546877.
Train: 2018-08-09T11:09:46.067388: step 2631, loss 0.495991.
Train: 2018-08-09T11:09:46.145497: step 2632, loss 0.562448.
Train: 2018-08-09T11:09:46.223572: step 2633, loss 0.562467.
Train: 2018-08-09T11:09:46.301708: step 2634, loss 0.462294.
Train: 2018-08-09T11:09:46.379817: step 2635, loss 0.612582.
Train: 2018-08-09T11:09:46.457893: step 2636, loss 0.562431.
Train: 2018-08-09T11:09:46.535998: step 2637, loss 0.612719.
Train: 2018-08-09T11:09:46.614133: step 2638, loss 0.663015.
Train: 2018-08-09T11:09:46.707833: step 2639, loss 0.528899.
Train: 2018-08-09T11:09:46.785938: step 2640, loss 0.629348.
Test: 2018-08-09T11:09:47.285822: step 2640, loss 0.549931.
Train: 2018-08-09T11:09:47.348307: step 2641, loss 0.62921.
Train: 2018-08-09T11:09:47.442034: step 2642, loss 0.579081.
Train: 2018-08-09T11:09:47.520143: step 2643, loss 0.545868.
Train: 2018-08-09T11:09:47.598280: step 2644, loss 0.56248.
Train: 2018-08-09T11:09:47.676384: step 2645, loss 0.479937.
Train: 2018-08-09T11:09:47.754491: step 2646, loss 0.529509.
Train: 2018-08-09T11:09:47.832598: step 2647, loss 0.496529.
Train: 2018-08-09T11:09:47.910703: step 2648, loss 0.496467.
Train: 2018-08-09T11:09:47.988811: step 2649, loss 0.595563.
Train: 2018-08-09T11:09:48.066886: step 2650, loss 0.52937.
Test: 2018-08-09T11:09:48.566769: step 2650, loss 0.548817.
Train: 2018-08-09T11:09:48.644909: step 2651, loss 0.67851.
Train: 2018-08-09T11:09:48.723012: step 2652, loss 0.545907.
Train: 2018-08-09T11:09:48.801120: step 2653, loss 0.595597.
Train: 2018-08-09T11:09:48.879226: step 2654, loss 0.512855.
Train: 2018-08-09T11:09:48.959626: step 2655, loss 0.661734.
Train: 2018-08-09T11:09:49.037731: step 2656, loss 0.595525.
Train: 2018-08-09T11:09:49.115811: step 2657, loss 0.529565.
Train: 2018-08-09T11:09:49.193917: step 2658, loss 0.661247.
Train: 2018-08-09T11:09:49.272057: step 2659, loss 0.529751.
Train: 2018-08-09T11:09:49.350160: step 2660, loss 0.611687.
Test: 2018-08-09T11:09:49.854655: step 2660, loss 0.547315.
Train: 2018-08-09T11:09:49.932761: step 2661, loss 0.480993.
Train: 2018-08-09T11:09:50.010899: step 2662, loss 0.546318.
Train: 2018-08-09T11:09:50.088977: step 2663, loss 0.644094.
Train: 2018-08-09T11:09:50.167082: step 2664, loss 0.497614.
Train: 2018-08-09T11:09:50.245219: step 2665, loss 0.51392.
Train: 2018-08-09T11:09:50.323329: step 2666, loss 0.54641.
Train: 2018-08-09T11:09:50.401432: step 2667, loss 0.578908.
Train: 2018-08-09T11:09:50.479532: step 2668, loss 0.611455.
Train: 2018-08-09T11:09:50.557644: step 2669, loss 0.546408.
Train: 2018-08-09T11:09:50.635722: step 2670, loss 0.497685.
Test: 2018-08-09T11:09:51.135603: step 2670, loss 0.547422.
Train: 2018-08-09T11:09:51.213709: step 2671, loss 0.595162.
Train: 2018-08-09T11:09:51.291847: step 2672, loss 0.627717.
Train: 2018-08-09T11:09:51.369958: step 2673, loss 0.595165.
Train: 2018-08-09T11:09:51.448031: step 2674, loss 0.562681.
Train: 2018-08-09T11:09:51.526161: step 2675, loss 0.578897.
Train: 2018-08-09T11:09:51.619866: step 2676, loss 0.643616.
Train: 2018-08-09T11:09:51.697970: step 2677, loss 0.482044.
Train: 2018-08-09T11:09:51.776109: step 2678, loss 0.546629.
Train: 2018-08-09T11:09:51.854215: step 2679, loss 0.594998.
Train: 2018-08-09T11:09:51.932321: step 2680, loss 0.643289.
Test: 2018-08-09T11:09:52.432175: step 2680, loss 0.548355.
Train: 2018-08-09T11:09:52.510308: step 2681, loss 0.434255.
Train: 2018-08-09T11:09:52.588386: step 2682, loss 0.643165.
Train: 2018-08-09T11:09:52.666526: step 2683, loss 0.627041.
Train: 2018-08-09T11:09:52.744630: step 2684, loss 0.54682.
Train: 2018-08-09T11:09:52.822707: step 2685, loss 0.562864.
Train: 2018-08-09T11:09:52.900813: step 2686, loss 0.578853.
Train: 2018-08-09T11:09:52.981191: step 2687, loss 0.610778.
Train: 2018-08-09T11:09:53.059300: step 2688, loss 0.467356.
Train: 2018-08-09T11:09:53.137439: step 2689, loss 0.483291.
Train: 2018-08-09T11:09:53.215536: step 2690, loss 0.562907.
Test: 2018-08-09T11:09:53.715395: step 2690, loss 0.547303.
Train: 2018-08-09T11:09:53.793500: step 2691, loss 0.482977.
Train: 2018-08-09T11:09:53.871636: step 2692, loss 0.610917.
Train: 2018-08-09T11:09:53.949739: step 2693, loss 0.530639.
Train: 2018-08-09T11:09:54.027850: step 2694, loss 0.578985.
Train: 2018-08-09T11:09:54.105956: step 2695, loss 0.611329.
Train: 2018-08-09T11:09:54.184067: step 2696, loss 0.49821.
Train: 2018-08-09T11:09:54.262170: step 2697, loss 0.546449.
Train: 2018-08-09T11:09:54.340247: step 2698, loss 0.562726.
Train: 2018-08-09T11:09:54.418354: step 2699, loss 0.546301.
Train: 2018-08-09T11:09:54.496496: step 2700, loss 0.643922.
Test: 2018-08-09T11:09:54.997776: step 2700, loss 0.548579.
Train: 2018-08-09T11:09:55.575765: step 2701, loss 0.562608.
Train: 2018-08-09T11:09:55.653871: step 2702, loss 0.480953.
Train: 2018-08-09T11:09:55.747624: step 2703, loss 0.480507.
Train: 2018-08-09T11:09:55.825735: step 2704, loss 0.430495.
Train: 2018-08-09T11:09:55.903841: step 2705, loss 0.611868.
Train: 2018-08-09T11:09:55.981950: step 2706, loss 0.510051.
Train: 2018-08-09T11:09:56.060055: step 2707, loss 0.543181.
Train: 2018-08-09T11:09:56.138157: step 2708, loss 0.566104.
Train: 2018-08-09T11:09:56.216268: step 2709, loss 0.505144.
Train: 2018-08-09T11:09:56.294343: step 2710, loss 0.541638.
Test: 2018-08-09T11:09:56.794226: step 2710, loss 0.547063.
Train: 2018-08-09T11:09:56.872363: step 2711, loss 0.640923.
Train: 2018-08-09T11:09:56.951795: step 2712, loss 0.601601.
Train: 2018-08-09T11:09:57.029887: step 2713, loss 0.597641.
Train: 2018-08-09T11:09:57.107999: step 2714, loss 0.687126.
Train: 2018-08-09T11:09:57.186106: step 2715, loss 0.508286.
Train: 2018-08-09T11:09:57.264184: step 2716, loss 0.547007.
Train: 2018-08-09T11:09:57.342319: step 2717, loss 0.54342.
Train: 2018-08-09T11:09:57.420425: step 2718, loss 0.57815.
Train: 2018-08-09T11:09:57.498537: step 2719, loss 0.679718.
Train: 2018-08-09T11:09:57.576639: step 2720, loss 0.562393.
Test: 2018-08-09T11:09:58.076515: step 2720, loss 0.54706.
Train: 2018-08-09T11:09:58.154627: step 2721, loss 0.579031.
Train: 2018-08-09T11:09:58.232735: step 2722, loss 0.595244.
Train: 2018-08-09T11:09:58.310814: step 2723, loss 0.595389.
Train: 2018-08-09T11:09:58.388947: step 2724, loss 0.530138.
Train: 2018-08-09T11:09:58.467024: step 2725, loss 0.562661.
Train: 2018-08-09T11:09:58.545161: step 2726, loss 0.562703.
Train: 2018-08-09T11:09:58.623238: step 2727, loss 0.691109.
Train: 2018-08-09T11:09:58.701373: step 2728, loss 0.578362.
Train: 2018-08-09T11:09:58.779481: step 2729, loss 0.499669.
Train: 2018-08-09T11:09:58.857587: step 2730, loss 0.563133.
Test: 2018-08-09T11:09:59.358079: step 2730, loss 0.550869.
Train: 2018-08-09T11:09:59.436216: step 2731, loss 0.563011.
Train: 2018-08-09T11:09:59.514323: step 2732, loss 0.532072.
Train: 2018-08-09T11:09:59.592400: step 2733, loss 0.532351.
Train: 2018-08-09T11:09:59.670535: step 2734, loss 0.499876.
Train: 2018-08-09T11:09:59.748611: step 2735, loss 0.547126.
Train: 2018-08-09T11:09:59.826754: step 2736, loss 0.626492.
Train: 2018-08-09T11:09:59.904855: step 2737, loss 0.515025.
Train: 2018-08-09T11:09:59.982931: step 2738, loss 0.562859.
Train: 2018-08-09T11:10:00.061091: step 2739, loss 0.450664.
Train: 2018-08-09T11:10:00.139175: step 2740, loss 0.530656.
Test: 2018-08-09T11:10:00.639052: step 2740, loss 0.549201.
Train: 2018-08-09T11:10:00.717158: step 2741, loss 0.498464.
Train: 2018-08-09T11:10:00.795270: step 2742, loss 0.514144.
Train: 2018-08-09T11:10:00.873349: step 2743, loss 0.658088.
Train: 2018-08-09T11:10:00.951484: step 2744, loss 0.560537.
Train: 2018-08-09T11:10:01.029590: step 2745, loss 0.613635.
Train: 2018-08-09T11:10:01.107699: step 2746, loss 0.423635.
Train: 2018-08-09T11:10:01.185804: step 2747, loss 0.439359.
Train: 2018-08-09T11:10:01.279532: step 2748, loss 0.604949.
Train: 2018-08-09T11:10:01.357640: step 2749, loss 0.547744.
Train: 2018-08-09T11:10:01.435715: step 2750, loss 0.583658.
Test: 2018-08-09T11:10:01.922226: step 2750, loss 0.547752.
Train: 2018-08-09T11:10:01.999377: step 2751, loss 0.566869.
Train: 2018-08-09T11:10:02.077484: step 2752, loss 0.62109.
Train: 2018-08-09T11:10:02.155591: step 2753, loss 0.598932.
Train: 2018-08-09T11:10:02.249287: step 2754, loss 0.582369.
Train: 2018-08-09T11:10:02.327424: step 2755, loss 0.559323.
Train: 2018-08-09T11:10:02.405526: step 2756, loss 0.406913.
Train: 2018-08-09T11:10:02.483638: step 2757, loss 0.544578.
Train: 2018-08-09T11:10:02.561713: step 2758, loss 0.54746.
Train: 2018-08-09T11:10:02.639850: step 2759, loss 0.519608.
Train: 2018-08-09T11:10:02.717956: step 2760, loss 0.577037.
Test: 2018-08-09T11:10:03.217840: step 2760, loss 0.547778.
Train: 2018-08-09T11:10:03.295918: step 2761, loss 0.529722.
Train: 2018-08-09T11:10:03.374053: step 2762, loss 0.565957.
Train: 2018-08-09T11:10:03.452130: step 2763, loss 0.546184.
Train: 2018-08-09T11:10:03.530235: step 2764, loss 0.580519.
Train: 2018-08-09T11:10:03.608372: step 2765, loss 0.545523.
Train: 2018-08-09T11:10:03.686481: step 2766, loss 0.493114.
Train: 2018-08-09T11:10:03.764586: step 2767, loss 0.549327.
Train: 2018-08-09T11:10:03.842695: step 2768, loss 0.62064.
Train: 2018-08-09T11:10:03.923208: step 2769, loss 0.557181.
Train: 2018-08-09T11:10:04.001313: step 2770, loss 0.479781.
Test: 2018-08-09T11:10:04.501190: step 2770, loss 0.548438.
Train: 2018-08-09T11:10:04.579295: step 2771, loss 0.563315.
Train: 2018-08-09T11:10:04.657409: step 2772, loss 0.543164.
Train: 2018-08-09T11:10:04.735513: step 2773, loss 0.647085.
Train: 2018-08-09T11:10:04.829214: step 2774, loss 0.661741.
Train: 2018-08-09T11:10:04.907349: step 2775, loss 0.645967.
Train: 2018-08-09T11:10:04.985459: step 2776, loss 0.546478.
Train: 2018-08-09T11:10:05.063561: step 2777, loss 0.449335.
Train: 2018-08-09T11:10:05.141668: step 2778, loss 0.611178.
Train: 2018-08-09T11:10:05.219746: step 2779, loss 0.643226.
Train: 2018-08-09T11:10:05.297881: step 2780, loss 0.59488.
Test: 2018-08-09T11:10:05.797734: step 2780, loss 0.549748.
Train: 2018-08-09T11:10:05.875871: step 2781, loss 0.546947.
Train: 2018-08-09T11:10:05.956209: step 2782, loss 0.531086.
Train: 2018-08-09T11:10:06.034318: step 2783, loss 0.578863.
Train: 2018-08-09T11:10:06.112423: step 2784, loss 0.531227.
Train: 2018-08-09T11:10:06.190559: step 2785, loss 0.594723.
Train: 2018-08-09T11:10:06.268667: step 2786, loss 0.547168.
Train: 2018-08-09T11:10:06.362364: step 2787, loss 0.578859.
Train: 2018-08-09T11:10:06.440478: step 2788, loss 0.563038.
Train: 2018-08-09T11:10:06.518608: step 2789, loss 0.610486.
Train: 2018-08-09T11:10:06.596683: step 2790, loss 0.56307.
Test: 2018-08-09T11:10:07.096566: step 2790, loss 0.547111.
Train: 2018-08-09T11:10:07.174702: step 2791, loss 0.468426.
Train: 2018-08-09T11:10:07.252779: step 2792, loss 0.657817.
Train: 2018-08-09T11:10:07.330919: step 2793, loss 0.547306.
Train: 2018-08-09T11:10:07.409022: step 2794, loss 0.641962.
Train: 2018-08-09T11:10:07.487131: step 2795, loss 0.531615.
Train: 2018-08-09T11:10:07.580860: step 2796, loss 0.547388.
Train: 2018-08-09T11:10:07.658986: step 2797, loss 0.547395.
Train: 2018-08-09T11:10:07.737072: step 2798, loss 0.578868.
Train: 2018-08-09T11:10:07.815172: step 2799, loss 0.626091.
Train: 2018-08-09T11:10:07.893283: step 2800, loss 0.657501.
Test: 2018-08-09T11:10:08.379883: step 2800, loss 0.550859.
Train: 2018-08-09T11:10:08.973491: step 2801, loss 0.500431.
Train: 2018-08-09T11:10:09.051598: step 2802, loss 0.563202.
Train: 2018-08-09T11:10:09.129675: step 2803, loss 0.484892.
Train: 2018-08-09T11:10:09.207807: step 2804, loss 0.688633.
Train: 2018-08-09T11:10:09.285913: step 2805, loss 0.610204.
Train: 2018-08-09T11:10:09.364028: step 2806, loss 0.594516.
Train: 2018-08-09T11:10:09.442101: step 2807, loss 0.516492.
Train: 2018-08-09T11:10:09.520238: step 2808, loss 0.547718.
Train: 2018-08-09T11:10:09.598340: step 2809, loss 0.454226.
Train: 2018-08-09T11:10:09.692043: step 2810, loss 0.547662.
Test: 2018-08-09T11:10:10.177693: step 2810, loss 0.549164.
Train: 2018-08-09T11:10:10.255774: step 2811, loss 0.547586.
Train: 2018-08-09T11:10:10.333905: step 2812, loss 0.516135.
Train: 2018-08-09T11:10:10.427634: step 2813, loss 0.578868.
Train: 2018-08-09T11:10:10.505745: step 2814, loss 0.578864.
Train: 2018-08-09T11:10:10.583852: step 2815, loss 0.531427.
Train: 2018-08-09T11:10:10.661960: step 2816, loss 0.626421.
Train: 2018-08-09T11:10:10.740036: step 2817, loss 0.61061.
Train: 2018-08-09T11:10:10.818172: step 2818, loss 0.562974.
Train: 2018-08-09T11:10:10.896277: step 2819, loss 0.515285.
Train: 2018-08-09T11:10:10.974385: step 2820, loss 0.594775.
Test: 2018-08-09T11:10:11.474238: step 2820, loss 0.551016.
Train: 2018-08-09T11:10:11.552343: step 2821, loss 0.547.
Train: 2018-08-09T11:10:11.630480: step 2822, loss 0.531018.
Train: 2018-08-09T11:10:11.708592: step 2823, loss 0.530941.
Train: 2018-08-09T11:10:11.786694: step 2824, loss 0.594871.
Train: 2018-08-09T11:10:11.864801: step 2825, loss 0.578866.
Train: 2018-08-09T11:10:11.945198: step 2826, loss 0.530728.
Train: 2018-08-09T11:10:12.023308: step 2827, loss 0.546728.
Train: 2018-08-09T11:10:12.101414: step 2828, loss 0.466179.
Train: 2018-08-09T11:10:12.179522: step 2829, loss 0.578886.
Train: 2018-08-09T11:10:12.257628: step 2830, loss 0.5627.
Test: 2018-08-09T11:10:12.757481: step 2830, loss 0.548073.
Train: 2018-08-09T11:10:12.835617: step 2831, loss 0.595143.
Train: 2018-08-09T11:10:12.913724: step 2832, loss 0.530128.
Train: 2018-08-09T11:10:12.991801: step 2833, loss 0.676696.
Train: 2018-08-09T11:10:13.069936: step 2834, loss 0.644099.
Train: 2018-08-09T11:10:13.148013: step 2835, loss 0.578917.
Train: 2018-08-09T11:10:13.226150: step 2836, loss 0.578909.
Train: 2018-08-09T11:10:13.304258: step 2837, loss 0.562683.
Train: 2018-08-09T11:10:13.382363: step 2838, loss 0.546506.
Train: 2018-08-09T11:10:13.460440: step 2839, loss 0.514178.
Train: 2018-08-09T11:10:13.538548: step 2840, loss 0.53036.
Test: 2018-08-09T11:10:14.039941: step 2840, loss 0.548154.
Train: 2018-08-09T11:10:14.118080: step 2841, loss 0.562706.
Train: 2018-08-09T11:10:14.196153: step 2842, loss 0.578898.
Train: 2018-08-09T11:10:14.274278: step 2843, loss 0.44932.
Train: 2018-08-09T11:10:14.352397: step 2844, loss 0.530213.
Train: 2018-08-09T11:10:14.430473: step 2845, loss 0.595186.
Train: 2018-08-09T11:10:14.508585: step 2846, loss 0.546326.
Train: 2018-08-09T11:10:14.586712: step 2847, loss 0.52995.
Train: 2018-08-09T11:10:14.664823: step 2848, loss 0.480767.
Train: 2018-08-09T11:10:14.758552: step 2849, loss 0.578972.
Train: 2018-08-09T11:10:14.836629: step 2850, loss 0.57898.
Test: 2018-08-09T11:10:15.320920: step 2850, loss 0.547053.
Train: 2018-08-09T11:10:15.399026: step 2851, loss 0.578989.
Train: 2018-08-09T11:10:15.477129: step 2852, loss 0.479882.
Train: 2018-08-09T11:10:15.570863: step 2853, loss 0.645303.
Train: 2018-08-09T11:10:15.648937: step 2854, loss 0.579059.
Train: 2018-08-09T11:10:15.727073: step 2855, loss 0.37989.
Train: 2018-08-09T11:10:15.805180: step 2856, loss 0.495802.
Train: 2018-08-09T11:10:15.883286: step 2857, loss 0.462.
Train: 2018-08-09T11:10:15.959348: step 2858, loss 0.663449.
Train: 2018-08-09T11:10:16.037487: step 2859, loss 0.444258.
Train: 2018-08-09T11:10:16.115592: step 2860, loss 0.579498.
Test: 2018-08-09T11:10:16.615444: step 2860, loss 0.547685.
Train: 2018-08-09T11:10:16.693550: step 2861, loss 0.562244.
Train: 2018-08-09T11:10:16.771688: step 2862, loss 0.494101.
Train: 2018-08-09T11:10:16.849770: step 2863, loss 0.545054.
Train: 2018-08-09T11:10:16.931231: step 2864, loss 0.579399.
Train: 2018-08-09T11:10:17.009338: step 2865, loss 0.56223.
Train: 2018-08-09T11:10:17.087445: step 2866, loss 0.579687.
Train: 2018-08-09T11:10:17.181171: step 2867, loss 0.527767.
Train: 2018-08-09T11:10:17.259303: step 2868, loss 0.52765.
Train: 2018-08-09T11:10:17.337416: step 2869, loss 0.562387.
Train: 2018-08-09T11:10:17.415522: step 2870, loss 0.580173.
Test: 2018-08-09T11:10:17.915376: step 2870, loss 0.54603.
Train: 2018-08-09T11:10:17.993511: step 2871, loss 0.527566.
Train: 2018-08-09T11:10:18.071617: step 2872, loss 0.527498.
Train: 2018-08-09T11:10:18.149696: step 2873, loss 0.544561.
Train: 2018-08-09T11:10:18.227800: step 2874, loss 0.492392.
Train: 2018-08-09T11:10:18.305906: step 2875, loss 0.474661.
Train: 2018-08-09T11:10:18.384044: step 2876, loss 0.491839.
Train: 2018-08-09T11:10:18.462150: step 2877, loss 0.615479.
Train: 2018-08-09T11:10:18.540257: step 2878, loss 0.599271.
Train: 2018-08-09T11:10:18.618333: step 2879, loss 0.526967.
Train: 2018-08-09T11:10:18.696439: step 2880, loss 0.598119.
Test: 2018-08-09T11:10:19.198825: step 2880, loss 0.549183.
Train: 2018-08-09T11:10:19.276900: step 2881, loss 0.652266.
Train: 2018-08-09T11:10:19.355038: step 2882, loss 0.491632.
Train: 2018-08-09T11:10:19.433142: step 2883, loss 0.545895.
Train: 2018-08-09T11:10:19.511251: step 2884, loss 0.474658.
Train: 2018-08-09T11:10:19.589357: step 2885, loss 0.509792.
Train: 2018-08-09T11:10:19.683056: step 2886, loss 0.4571.
Train: 2018-08-09T11:10:19.761161: step 2887, loss 0.544745.
Train: 2018-08-09T11:10:19.839299: step 2888, loss 0.544807.
Train: 2018-08-09T11:10:19.917401: step 2889, loss 0.527192.
Train: 2018-08-09T11:10:19.995506: step 2890, loss 0.562411.
Test: 2018-08-09T11:10:20.479743: step 2890, loss 0.546546.
Train: 2018-08-09T11:10:20.555815: step 2891, loss 0.633099.
Train: 2018-08-09T11:10:20.649513: step 2892, loss 0.668398.
Train: 2018-08-09T11:10:20.727619: step 2893, loss 0.562416.
Train: 2018-08-09T11:10:20.805756: step 2894, loss 0.50963.
Train: 2018-08-09T11:10:20.883833: step 2895, loss 0.509694.
Train: 2018-08-09T11:10:20.966307: step 2896, loss 0.632615.
Train: 2018-08-09T11:10:21.044377: step 2897, loss 0.509817.
Train: 2018-08-09T11:10:21.122484: step 2898, loss 0.632388.
Train: 2018-08-09T11:10:21.200591: step 2899, loss 0.527449.
Train: 2018-08-09T11:10:21.278730: step 2900, loss 0.579773.
Test: 2018-08-09T11:10:21.778605: step 2900, loss 0.547383.
Train: 2018-08-09T11:10:22.309735: step 2901, loss 0.544952.
Train: 2018-08-09T11:10:22.387842: step 2902, loss 0.596969.
Train: 2018-08-09T11:10:22.465919: step 2903, loss 0.527786.
Train: 2018-08-09T11:10:22.544059: step 2904, loss 0.510416.
Train: 2018-08-09T11:10:22.622161: step 2905, loss 0.614072.
Train: 2018-08-09T11:10:22.700239: step 2906, loss 0.579562.
Train: 2018-08-09T11:10:22.778376: step 2907, loss 0.596871.
Train: 2018-08-09T11:10:22.856452: step 2908, loss 0.596665.
Train: 2018-08-09T11:10:22.935955: step 2909, loss 0.494143.
Train: 2018-08-09T11:10:23.014059: step 2910, loss 0.630534.
Test: 2018-08-09T11:10:23.513912: step 2910, loss 0.54834.
Train: 2018-08-09T11:10:23.592018: step 2911, loss 0.579358.
Train: 2018-08-09T11:10:23.670156: step 2912, loss 0.630155.
Train: 2018-08-09T11:10:23.748234: step 2913, loss 0.579246.
Train: 2018-08-09T11:10:23.826338: step 2914, loss 0.545558.
Train: 2018-08-09T11:10:23.904474: step 2915, loss 0.629446.
Train: 2018-08-09T11:10:23.982552: step 2916, loss 0.72929.
Train: 2018-08-09T11:10:24.060670: step 2917, loss 0.529319.
Train: 2018-08-09T11:10:24.138796: step 2918, loss 0.513063.
Train: 2018-08-09T11:10:24.216902: step 2919, loss 0.546139.
Train: 2018-08-09T11:10:24.294978: step 2920, loss 0.595301.
Test: 2018-08-09T11:10:24.794892: step 2920, loss 0.549211.
Train: 2018-08-09T11:10:24.872967: step 2921, loss 0.481158.
Train: 2018-08-09T11:10:24.952475: step 2922, loss 0.546391.
Train: 2018-08-09T11:10:25.030584: step 2923, loss 0.676324.
Train: 2018-08-09T11:10:25.108684: step 2924, loss 0.562711.
Train: 2018-08-09T11:10:25.186792: step 2925, loss 0.514345.
Train: 2018-08-09T11:10:25.264876: step 2926, loss 0.739929.
Train: 2018-08-09T11:10:25.358601: step 2927, loss 0.594897.
Train: 2018-08-09T11:10:25.436707: step 2928, loss 0.546947.
Train: 2018-08-09T11:10:25.514814: step 2929, loss 0.610649.
Train: 2018-08-09T11:10:25.592944: step 2930, loss 0.499718.
Test: 2018-08-09T11:10:26.092804: step 2930, loss 0.54888.
Train: 2018-08-09T11:10:26.170939: step 2931, loss 0.626223.
Train: 2018-08-09T11:10:26.249015: step 2932, loss 0.51593.
Train: 2018-08-09T11:10:26.327122: step 2933, loss 0.594574.
Train: 2018-08-09T11:10:26.405262: step 2934, loss 0.610214.
Train: 2018-08-09T11:10:26.483365: step 2935, loss 0.453885.
Train: 2018-08-09T11:10:26.561468: step 2936, loss 0.563266.
Train: 2018-08-09T11:10:26.639583: step 2937, loss 0.500793.
Train: 2018-08-09T11:10:26.717655: step 2938, loss 0.594522.
Train: 2018-08-09T11:10:26.795794: step 2939, loss 0.531937.
Train: 2018-08-09T11:10:26.873868: step 2940, loss 0.563207.
Test: 2018-08-09T11:10:27.374515: step 2940, loss 0.551449.
Train: 2018-08-09T11:10:27.452619: step 2941, loss 0.578872.
Train: 2018-08-09T11:10:27.530727: step 2942, loss 0.547456.
Train: 2018-08-09T11:10:27.608834: step 2943, loss 0.468744.
Train: 2018-08-09T11:10:27.686969: step 2944, loss 0.594646.
Train: 2018-08-09T11:10:27.765076: step 2945, loss 0.626334.
Train: 2018-08-09T11:10:27.843184: step 2946, loss 0.547271.
Train: 2018-08-09T11:10:27.921290: step 2947, loss 0.578853.
Train: 2018-08-09T11:10:27.999396: step 2948, loss 0.59473.
Train: 2018-08-09T11:10:28.093124: step 2949, loss 0.594733.
Train: 2018-08-09T11:10:28.155579: step 2950, loss 0.642391.
Test: 2018-08-09T11:10:28.655492: step 2950, loss 0.548748.
Train: 2018-08-09T11:10:28.733568: step 2951, loss 0.563001.
Train: 2018-08-09T11:10:28.811705: step 2952, loss 0.531349.
Train: 2018-08-09T11:10:28.905426: step 2953, loss 0.610524.
Train: 2018-08-09T11:10:28.983533: step 2954, loss 0.563045.
Train: 2018-08-09T11:10:29.061615: step 2955, loss 0.515648.
Train: 2018-08-09T11:10:29.139735: step 2956, loss 0.547246.
Train: 2018-08-09T11:10:29.217859: step 2957, loss 0.452323.
Train: 2018-08-09T11:10:29.295967: step 2958, loss 0.515423.
Train: 2018-08-09T11:10:29.374076: step 2959, loss 0.435655.
Train: 2018-08-09T11:10:29.452179: step 2960, loss 0.498885.
Test: 2018-08-09T11:10:29.953428: step 2960, loss 0.548925.
Train: 2018-08-09T11:10:30.031531: step 2961, loss 0.498423.
Train: 2018-08-09T11:10:30.109644: step 2962, loss 0.64367.
Train: 2018-08-09T11:10:30.187751: step 2963, loss 0.578895.
Train: 2018-08-09T11:10:30.265827: step 2964, loss 0.595152.
Train: 2018-08-09T11:10:30.343963: step 2965, loss 0.595373.
Train: 2018-08-09T11:10:30.422042: step 2966, loss 0.595302.
Train: 2018-08-09T11:10:30.500179: step 2967, loss 0.513061.
Train: 2018-08-09T11:10:30.578283: step 2968, loss 0.595883.
Train: 2018-08-09T11:10:30.656362: step 2969, loss 0.62769.
Train: 2018-08-09T11:10:30.734500: step 2970, loss 0.579063.
Test: 2018-08-09T11:10:31.234348: step 2970, loss 0.551074.
Train: 2018-08-09T11:10:31.312485: step 2971, loss 0.545543.
Train: 2018-08-09T11:10:31.390592: step 2972, loss 0.594737.
Train: 2018-08-09T11:10:31.468697: step 2973, loss 0.641901.
Train: 2018-08-09T11:10:31.546776: step 2974, loss 0.563703.
Train: 2018-08-09T11:10:31.624882: step 2975, loss 0.598293.
Train: 2018-08-09T11:10:31.718640: step 2976, loss 0.467075.
Train: 2018-08-09T11:10:31.796715: step 2977, loss 0.534615.
Train: 2018-08-09T11:10:31.874853: step 2978, loss 0.547517.
Train: 2018-08-09T11:10:31.955303: step 2979, loss 0.595147.
Train: 2018-08-09T11:10:32.033403: step 2980, loss 0.627944.
Test: 2018-08-09T11:10:32.533286: step 2980, loss 0.550969.
Train: 2018-08-09T11:10:32.611417: step 2981, loss 0.546351.
Train: 2018-08-09T11:10:32.689498: step 2982, loss 0.578941.
Train: 2018-08-09T11:10:32.767606: step 2983, loss 0.464302.
Train: 2018-08-09T11:10:32.845742: step 2984, loss 0.56256.
Train: 2018-08-09T11:10:32.924402: step 2985, loss 0.611793.
Train: 2018-08-09T11:10:33.002479: step 2986, loss 0.513301.
Train: 2018-08-09T11:10:33.080614: step 2987, loss 0.496858.
Train: 2018-08-09T11:10:33.158722: step 2988, loss 0.49664.
Train: 2018-08-09T11:10:33.236832: step 2989, loss 0.529506.
Train: 2018-08-09T11:10:33.314935: step 2990, loss 0.595936.
Test: 2018-08-09T11:10:33.814787: step 2990, loss 0.548782.
Train: 2018-08-09T11:10:33.892923: step 2991, loss 0.62911.
Train: 2018-08-09T11:10:33.971000: step 2992, loss 0.545845.
Train: 2018-08-09T11:10:34.049137: step 2993, loss 0.595657.
Train: 2018-08-09T11:10:34.127238: step 2994, loss 0.545865.
Train: 2018-08-09T11:10:34.205347: step 2995, loss 0.579058.
Train: 2018-08-09T11:10:34.299078: step 2996, loss 0.56246.
Train: 2018-08-09T11:10:34.377184: step 2997, loss 0.56246.
Train: 2018-08-09T11:10:34.455290: step 2998, loss 0.562462.
Train: 2018-08-09T11:10:34.533393: step 2999, loss 0.562464.
Train: 2018-08-09T11:10:34.611503: step 3000, loss 0.57905.
Test: 2018-08-09T11:10:35.111355: step 3000, loss 0.547567.
Train: 2018-08-09T11:10:35.704996: step 3001, loss 0.628768.
Train: 2018-08-09T11:10:35.783072: step 3002, loss 0.612124.
Train: 2018-08-09T11:10:35.876824: step 3003, loss 0.562502.
Train: 2018-08-09T11:10:35.941698: step 3004, loss 0.727242.
Train: 2018-08-09T11:10:36.019803: step 3005, loss 0.496986.
Train: 2018-08-09T11:10:36.113503: step 3006, loss 0.709683.
Train: 2018-08-09T11:10:36.191638: step 3007, loss 0.513887.
Train: 2018-08-09T11:10:36.269744: step 3008, loss 0.62747.
Train: 2018-08-09T11:10:36.347855: step 3009, loss 0.594995.
Train: 2018-08-09T11:10:36.425957: step 3010, loss 0.528608.
Test: 2018-08-09T11:10:36.910219: step 3010, loss 0.548522.
Train: 2018-08-09T11:10:37.003947: step 3011, loss 0.642759.
Train: 2018-08-09T11:10:37.082053: step 3012, loss 0.563075.
Train: 2018-08-09T11:10:37.160162: step 3013, loss 0.547189.
Train: 2018-08-09T11:10:37.238267: step 3014, loss 0.594665.
Train: 2018-08-09T11:10:37.316373: step 3015, loss 0.516005.
Train: 2018-08-09T11:10:37.394450: step 3016, loss 0.625983.
Train: 2018-08-09T11:10:37.472586: step 3017, loss 0.500569.
Train: 2018-08-09T11:10:37.550693: step 3018, loss 0.485027.
Train: 2018-08-09T11:10:37.628795: step 3019, loss 0.578879.
Train: 2018-08-09T11:10:37.706906: step 3020, loss 0.531914.
Test: 2018-08-09T11:10:38.209230: step 3020, loss 0.549708.
Train: 2018-08-09T11:10:38.287361: step 3021, loss 0.484871.
Train: 2018-08-09T11:10:38.365472: step 3022, loss 0.626009.
Train: 2018-08-09T11:10:38.443549: step 3023, loss 0.515935.
Train: 2018-08-09T11:10:38.521685: step 3024, loss 0.563153.
Train: 2018-08-09T11:10:38.615416: step 3025, loss 0.578875.
Train: 2018-08-09T11:10:38.693515: step 3026, loss 0.515594.
Train: 2018-08-09T11:10:38.771627: step 3027, loss 0.610567.
Train: 2018-08-09T11:10:38.849733: step 3028, loss 0.563011.
Train: 2018-08-09T11:10:38.927838: step 3029, loss 0.578859.
Train: 2018-08-09T11:10:39.005950: step 3030, loss 0.467481.
Test: 2018-08-09T11:10:39.490177: step 3030, loss 0.550973.
Train: 2018-08-09T11:10:39.568314: step 3031, loss 0.578863.
Train: 2018-08-09T11:10:39.646420: step 3032, loss 0.578865.
Train: 2018-08-09T11:10:39.740148: step 3033, loss 0.514787.
Train: 2018-08-09T11:10:39.818255: step 3034, loss 0.53069.
Train: 2018-08-09T11:10:39.896333: step 3035, loss 0.498343.
Train: 2018-08-09T11:10:39.975756: step 3036, loss 0.562708.
Train: 2018-08-09T11:10:40.053892: step 3037, loss 0.5789.
Train: 2018-08-09T11:10:40.131968: step 3038, loss 0.578863.
Train: 2018-08-09T11:10:40.210102: step 3039, loss 0.546155.
Train: 2018-08-09T11:10:40.288212: step 3040, loss 0.62824.
Test: 2018-08-09T11:10:40.788066: step 3040, loss 0.547777.
Train: 2018-08-09T11:10:40.866201: step 3041, loss 0.546311.
Train: 2018-08-09T11:10:40.944308: step 3042, loss 0.54629.
Train: 2018-08-09T11:10:41.022419: step 3043, loss 0.480244.
Train: 2018-08-09T11:10:41.100490: step 3044, loss 0.496697.
Train: 2018-08-09T11:10:41.178628: step 3045, loss 0.496454.
Train: 2018-08-09T11:10:41.256732: step 3046, loss 0.529273.
Train: 2018-08-09T11:10:41.334811: step 3047, loss 0.646358.
Train: 2018-08-09T11:10:41.412947: step 3048, loss 0.562134.
Train: 2018-08-09T11:10:41.491054: step 3049, loss 0.461158.
Train: 2018-08-09T11:10:41.584787: step 3050, loss 0.630201.
Test: 2018-08-09T11:10:42.070261: step 3050, loss 0.547775.
Train: 2018-08-09T11:10:42.148393: step 3051, loss 0.681023.
Train: 2018-08-09T11:10:42.226473: step 3052, loss 0.444015.
Train: 2018-08-09T11:10:42.320227: step 3053, loss 0.56238.
Train: 2018-08-09T11:10:42.398339: step 3054, loss 0.597078.
Train: 2018-08-09T11:10:42.476445: step 3055, loss 0.511676.
Train: 2018-08-09T11:10:42.554533: step 3056, loss 0.562429.
Train: 2018-08-09T11:10:42.632629: step 3057, loss 0.663492.
Train: 2018-08-09T11:10:42.710764: step 3058, loss 0.562289.
Train: 2018-08-09T11:10:42.788843: step 3059, loss 0.629599.
Train: 2018-08-09T11:10:42.866981: step 3060, loss 0.579155.
Test: 2018-08-09T11:10:43.366837: step 3060, loss 0.547998.
Train: 2018-08-09T11:10:43.444966: step 3061, loss 0.57884.
Train: 2018-08-09T11:10:43.523073: step 3062, loss 0.679226.
Train: 2018-08-09T11:10:43.601179: step 3063, loss 0.579079.
Train: 2018-08-09T11:10:43.679288: step 3064, loss 0.612084.
Train: 2018-08-09T11:10:43.757362: step 3065, loss 0.529639.
Train: 2018-08-09T11:10:43.835503: step 3066, loss 0.710029.
Train: 2018-08-09T11:10:43.915878: step 3067, loss 0.660372.
Train: 2018-08-09T11:10:43.993986: step 3068, loss 0.530358.
Train: 2018-08-09T11:10:44.072094: step 3069, loss 0.530611.
Train: 2018-08-09T11:10:44.150230: step 3070, loss 0.65895.
Test: 2018-08-09T11:10:44.644174: step 3070, loss 0.54801.
Train: 2018-08-09T11:10:44.722309: step 3071, loss 0.467377.
Train: 2018-08-09T11:10:44.800420: step 3072, loss 0.467737.
Train: 2018-08-09T11:10:44.894116: step 3073, loss 0.578861.
Train: 2018-08-09T11:10:44.972251: step 3074, loss 0.563024.
Train: 2018-08-09T11:10:45.050326: step 3075, loss 0.547222.
Train: 2018-08-09T11:10:45.128434: step 3076, loss 0.515617.
Train: 2018-08-09T11:10:45.206540: step 3077, loss 0.452275.
Train: 2018-08-09T11:10:45.284647: step 3078, loss 0.48362.
Train: 2018-08-09T11:10:45.362756: step 3079, loss 0.546944.
Train: 2018-08-09T11:10:45.440891: step 3080, loss 0.5308.
Test: 2018-08-09T11:10:45.933487: step 3080, loss 0.548784.
Train: 2018-08-09T11:10:46.011624: step 3081, loss 0.481863.
Train: 2018-08-09T11:10:46.105351: step 3082, loss 0.496977.
Train: 2018-08-09T11:10:46.183459: step 3083, loss 0.510721.
Train: 2018-08-09T11:10:46.261565: step 3084, loss 0.544873.
Train: 2018-08-09T11:10:46.339671: step 3085, loss 0.629575.
Train: 2018-08-09T11:10:46.417748: step 3086, loss 0.585748.
Train: 2018-08-09T11:10:46.495853: step 3087, loss 0.6047.
Train: 2018-08-09T11:10:46.569081: step 3088, loss 0.56223.
Train: 2018-08-09T11:10:46.662822: step 3089, loss 0.577304.
Train: 2018-08-09T11:10:46.740946: step 3090, loss 0.523261.
Test: 2018-08-09T11:10:47.225178: step 3090, loss 0.546861.
Train: 2018-08-09T11:10:47.303313: step 3091, loss 0.476047.
Train: 2018-08-09T11:10:47.381423: step 3092, loss 0.495298.
Train: 2018-08-09T11:10:47.459499: step 3093, loss 0.495494.
Train: 2018-08-09T11:10:47.537603: step 3094, loss 0.494409.
Train: 2018-08-09T11:10:47.631332: step 3095, loss 0.441007.
Train: 2018-08-09T11:10:47.709474: step 3096, loss 0.543454.
Train: 2018-08-09T11:10:47.787545: step 3097, loss 0.575582.
Train: 2018-08-09T11:10:47.865681: step 3098, loss 0.548583.
Train: 2018-08-09T11:10:47.943788: step 3099, loss 0.522819.
Train: 2018-08-09T11:10:48.021895: step 3100, loss 0.597329.
Test: 2018-08-09T11:10:48.521747: step 3100, loss 0.5471.
Train: 2018-08-09T11:10:49.054471: step 3101, loss 0.505427.
Train: 2018-08-09T11:10:49.132578: step 3102, loss 0.46141.
Train: 2018-08-09T11:10:49.210688: step 3103, loss 0.45388.
Train: 2018-08-09T11:10:49.288792: step 3104, loss 0.425479.
Train: 2018-08-09T11:10:49.366896: step 3105, loss 0.520714.
Train: 2018-08-09T11:10:49.460626: step 3106, loss 0.711241.
Train: 2018-08-09T11:10:49.538703: step 3107, loss 0.558043.
Train: 2018-08-09T11:10:49.616842: step 3108, loss 0.516286.
Train: 2018-08-09T11:10:49.694944: step 3109, loss 0.504419.
Train: 2018-08-09T11:10:49.773052: step 3110, loss 0.55222.
Test: 2018-08-09T11:10:50.272904: step 3110, loss 0.546433.
Train: 2018-08-09T11:10:50.351041: step 3111, loss 0.508946.
Train: 2018-08-09T11:10:50.429147: step 3112, loss 0.697227.
Train: 2018-08-09T11:10:50.507255: step 3113, loss 0.691151.
Train: 2018-08-09T11:10:50.585360: step 3114, loss 0.582578.
Train: 2018-08-09T11:10:50.663467: step 3115, loss 0.650738.
Train: 2018-08-09T11:10:50.741573: step 3116, loss 0.545858.
Train: 2018-08-09T11:10:50.819681: step 3117, loss 0.545301.
Train: 2018-08-09T11:10:50.897791: step 3118, loss 0.578805.
Train: 2018-08-09T11:10:50.977234: step 3119, loss 0.512083.
Train: 2018-08-09T11:10:51.055337: step 3120, loss 0.51223.
Test: 2018-08-09T11:10:51.551635: step 3120, loss 0.548043.
Train: 2018-08-09T11:10:51.629705: step 3121, loss 0.629159.
Train: 2018-08-09T11:10:51.707812: step 3122, loss 0.529166.
Train: 2018-08-09T11:10:51.785919: step 3123, loss 0.678735.
Train: 2018-08-09T11:10:51.864055: step 3124, loss 0.628665.
Train: 2018-08-09T11:10:51.942130: step 3125, loss 0.677784.
Train: 2018-08-09T11:10:52.020271: step 3126, loss 0.595321.
Train: 2018-08-09T11:10:52.098344: step 3127, loss 0.611446.
Train: 2018-08-09T11:10:52.176477: step 3128, loss 0.595047.
Train: 2018-08-09T11:10:52.270180: step 3129, loss 0.546707.
Train: 2018-08-09T11:10:52.348284: step 3130, loss 0.547054.
Test: 2018-08-09T11:10:52.832572: step 3130, loss 0.550467.
Train: 2018-08-09T11:10:52.915954: step 3131, loss 0.594737.
Train: 2018-08-09T11:10:52.994091: step 3132, loss 0.531427.
Train: 2018-08-09T11:10:53.072200: step 3133, loss 0.578856.
Train: 2018-08-09T11:10:53.150305: step 3134, loss 0.468776.
Train: 2018-08-09T11:10:53.244004: step 3135, loss 0.594616.
Train: 2018-08-09T11:10:53.322142: step 3136, loss 0.578882.
Train: 2018-08-09T11:10:53.400215: step 3137, loss 0.469158.
Train: 2018-08-09T11:10:53.478322: step 3138, loss 0.43769.
Train: 2018-08-09T11:10:53.556456: step 3139, loss 0.610373.
Train: 2018-08-09T11:10:53.634536: step 3140, loss 0.515746.
Test: 2018-08-09T11:10:54.119203: step 3140, loss 0.550594.
Train: 2018-08-09T11:10:54.212894: step 3141, loss 0.562994.
Train: 2018-08-09T11:10:54.291030: step 3142, loss 0.483686.
Train: 2018-08-09T11:10:54.373666: step 3143, loss 0.530622.
Train: 2018-08-09T11:10:54.443261: step 3144, loss 0.611052.
Train: 2018-08-09T11:10:54.521370: step 3145, loss 0.676869.
Train: 2018-08-09T11:10:54.599506: step 3146, loss 0.595045.
Train: 2018-08-09T11:10:54.677612: step 3147, loss 0.514642.
Train: 2018-08-09T11:10:54.755688: step 3148, loss 0.594707.
Train: 2018-08-09T11:10:54.849441: step 3149, loss 0.626859.
Train: 2018-08-09T11:10:54.927548: step 3150, loss 0.530618.
Test: 2018-08-09T11:10:55.411809: step 3150, loss 0.550153.
Train: 2018-08-09T11:10:55.489920: step 3151, loss 0.530611.
Train: 2018-08-09T11:10:55.568028: step 3152, loss 0.59491.
Train: 2018-08-09T11:10:55.646104: step 3153, loss 0.530297.
Train: 2018-08-09T11:10:55.739830: step 3154, loss 0.563197.
Train: 2018-08-09T11:10:55.817968: step 3155, loss 0.577889.
Train: 2018-08-09T11:10:55.896268: step 3156, loss 0.611243.
Train: 2018-08-09T11:10:55.964127: step 3157, loss 0.595041.
Train: 2018-08-09T11:10:56.057861: step 3158, loss 0.660679.
Train: 2018-08-09T11:10:56.135961: step 3159, loss 0.498267.
Train: 2018-08-09T11:10:56.214044: step 3160, loss 0.594788.
Test: 2018-08-09T11:10:56.698330: step 3160, loss 0.549022.
Train: 2018-08-09T11:10:56.792034: step 3161, loss 0.579478.
Train: 2018-08-09T11:10:56.870139: step 3162, loss 0.675074.
Train: 2018-08-09T11:10:56.948277: step 3163, loss 0.49975.
Train: 2018-08-09T11:10:57.026382: step 3164, loss 0.578784.
Train: 2018-08-09T11:10:57.104459: step 3165, loss 0.594376.
Train: 2018-08-09T11:10:57.182567: step 3166, loss 0.594294.
Train: 2018-08-09T11:10:57.260709: step 3167, loss 0.549272.
Train: 2018-08-09T11:10:57.338809: step 3168, loss 0.531913.
Train: 2018-08-09T11:10:57.416912: step 3169, loss 0.563367.
Train: 2018-08-09T11:10:57.495023: step 3170, loss 0.641398.
Test: 2018-08-09T11:10:57.994874: step 3170, loss 0.552118.
Train: 2018-08-09T11:10:58.073005: step 3171, loss 0.563305.
Train: 2018-08-09T11:10:58.151117: step 3172, loss 0.594553.
Train: 2018-08-09T11:10:58.229225: step 3173, loss 0.563278.
Train: 2018-08-09T11:10:58.307301: step 3174, loss 0.532119.
Train: 2018-08-09T11:10:58.385445: step 3175, loss 0.469725.
Train: 2018-08-09T11:10:58.463516: step 3176, loss 0.578914.
Train: 2018-08-09T11:10:58.541651: step 3177, loss 0.59455.
Train: 2018-08-09T11:10:58.635350: step 3178, loss 0.531927.
Train: 2018-08-09T11:10:58.713489: step 3179, loss 0.594535.
Train: 2018-08-09T11:10:58.791592: step 3180, loss 0.531841.
Test: 2018-08-09T11:10:59.293816: step 3180, loss 0.552014.
Train: 2018-08-09T11:10:59.371945: step 3181, loss 0.531757.
Train: 2018-08-09T11:10:59.450028: step 3182, loss 0.578861.
Train: 2018-08-09T11:10:59.528136: step 3183, loss 0.547327.
Train: 2018-08-09T11:10:59.606240: step 3184, loss 0.51569.
Train: 2018-08-09T11:10:59.684347: step 3185, loss 0.658102.
Train: 2018-08-09T11:10:59.762487: step 3186, loss 0.51546.
Train: 2018-08-09T11:10:59.840592: step 3187, loss 0.642395.
Train: 2018-08-09T11:10:59.918668: step 3188, loss 0.594823.
Train: 2018-08-09T11:10:59.996805: step 3189, loss 0.578847.
Train: 2018-08-09T11:11:00.074910: step 3190, loss 0.515428.
Test: 2018-08-09T11:11:00.574763: step 3190, loss 0.551118.
Train: 2018-08-09T11:11:00.652899: step 3191, loss 0.594714.
Train: 2018-08-09T11:11:00.731007: step 3192, loss 0.674065.
Train: 2018-08-09T11:11:00.809085: step 3193, loss 0.499657.
Train: 2018-08-09T11:11:00.887190: step 3194, loss 0.563054.
Train: 2018-08-09T11:11:00.967611: step 3195, loss 0.626361.
Train: 2018-08-09T11:11:01.045688: step 3196, loss 0.531443.
Train: 2018-08-09T11:11:01.123795: step 3197, loss 0.57886.
Train: 2018-08-09T11:11:01.201930: step 3198, loss 0.452497.
Train: 2018-08-09T11:11:01.295634: step 3199, loss 0.563046.
Train: 2018-08-09T11:11:01.373764: step 3200, loss 0.56301.
Test: 2018-08-09T11:11:01.858021: step 3200, loss 0.549922.
Train: 2018-08-09T11:11:02.420393: step 3201, loss 0.547115.
Train: 2018-08-09T11:11:02.498478: step 3202, loss 0.499387.
Train: 2018-08-09T11:11:02.576614: step 3203, loss 0.610738.
Train: 2018-08-09T11:11:02.654713: step 3204, loss 0.578858.
Train: 2018-08-09T11:11:02.732815: step 3205, loss 0.562867.
Train: 2018-08-09T11:11:02.826549: step 3206, loss 0.674915.
Train: 2018-08-09T11:11:02.907926: step 3207, loss 0.690843.
Train: 2018-08-09T11:11:02.983454: step 3208, loss 0.626687.
Train: 2018-08-09T11:11:03.061567: step 3209, loss 0.531192.
Train: 2018-08-09T11:11:03.139670: step 3210, loss 0.626331.
Test: 2018-08-09T11:11:03.639554: step 3210, loss 0.549502.
Train: 2018-08-09T11:11:03.717651: step 3211, loss 0.515705.
Train: 2018-08-09T11:11:03.803019: step 3212, loss 0.531863.
Train: 2018-08-09T11:11:03.885662: step 3213, loss 0.500268.
Train: 2018-08-09T11:11:03.964243: step 3214, loss 0.484319.
Train: 2018-08-09T11:11:04.042355: step 3215, loss 0.51573.
Train: 2018-08-09T11:11:04.120466: step 3216, loss 0.563064.
Train: 2018-08-09T11:11:04.198571: step 3217, loss 0.547127.
Train: 2018-08-09T11:11:04.276674: step 3218, loss 0.594787.
Train: 2018-08-09T11:11:04.354780: step 3219, loss 0.578886.
Train: 2018-08-09T11:11:04.432857: step 3220, loss 0.546936.
Test: 2018-08-09T11:11:04.932741: step 3220, loss 0.550916.
Train: 2018-08-09T11:11:05.010877: step 3221, loss 0.514909.
Train: 2018-08-09T11:11:05.088983: step 3222, loss 0.675026.
Train: 2018-08-09T11:11:05.167092: step 3223, loss 0.642991.
Train: 2018-08-09T11:11:05.245196: step 3224, loss 0.498793.
Train: 2018-08-09T11:11:05.323275: step 3225, loss 0.562837.
Train: 2018-08-09T11:11:05.417026: step 3226, loss 0.530799.
Train: 2018-08-09T11:11:05.495109: step 3227, loss 0.691128.
Train: 2018-08-09T11:11:05.573215: step 3228, loss 0.466787.
Train: 2018-08-09T11:11:05.651352: step 3229, loss 0.546826.
Train: 2018-08-09T11:11:05.729457: step 3230, loss 0.594895.
Test: 2018-08-09T11:11:06.230769: step 3230, loss 0.550233.
Train: 2018-08-09T11:11:06.308875: step 3231, loss 0.578885.
Train: 2018-08-09T11:11:06.386983: step 3232, loss 0.562832.
Train: 2018-08-09T11:11:06.465090: step 3233, loss 0.530762.
Train: 2018-08-09T11:11:06.543195: step 3234, loss 0.5949.
Train: 2018-08-09T11:11:06.621301: step 3235, loss 0.626996.
Train: 2018-08-09T11:11:06.699380: step 3236, loss 0.562868.
Train: 2018-08-09T11:11:06.777484: step 3237, loss 0.530831.
Train: 2018-08-09T11:11:06.855590: step 3238, loss 0.466774.
Train: 2018-08-09T11:11:06.933714: step 3239, loss 0.594931.
Train: 2018-08-09T11:11:07.011835: step 3240, loss 0.594909.
Test: 2018-08-09T11:11:07.511687: step 3240, loss 0.550766.
Train: 2018-08-09T11:11:07.589823: step 3241, loss 0.611151.
Train: 2018-08-09T11:11:07.667899: step 3242, loss 0.466472.
Train: 2018-08-09T11:11:07.746008: step 3243, loss 0.546706.
Train: 2018-08-09T11:11:07.824113: step 3244, loss 0.578887.
Train: 2018-08-09T11:11:07.902220: step 3245, loss 0.482149.
Train: 2018-08-09T11:11:07.982639: step 3246, loss 0.595044.
Train: 2018-08-09T11:11:08.060715: step 3247, loss 0.562733.
Train: 2018-08-09T11:11:08.138852: step 3248, loss 0.611334.
Train: 2018-08-09T11:11:08.216932: step 3249, loss 0.611338.
Train: 2018-08-09T11:11:08.310711: step 3250, loss 0.69237.
Test: 2018-08-09T11:11:08.794919: step 3250, loss 0.551844.
Train: 2018-08-09T11:11:08.873050: step 3251, loss 0.562719.
Train: 2018-08-09T11:11:08.951162: step 3252, loss 0.546618.
Train: 2018-08-09T11:11:09.029264: step 3253, loss 0.611091.
Train: 2018-08-09T11:11:09.107345: step 3254, loss 0.594959.
Train: 2018-08-09T11:11:09.185452: step 3255, loss 0.578862.
Train: 2018-08-09T11:11:09.279210: step 3256, loss 0.530887.
Train: 2018-08-09T11:11:09.357316: step 3257, loss 0.626758.
Train: 2018-08-09T11:11:09.435422: step 3258, loss 0.610714.
Train: 2018-08-09T11:11:09.513531: step 3259, loss 0.531216.
Train: 2018-08-09T11:11:09.591617: step 3260, loss 0.563011.
Test: 2018-08-09T11:11:10.093769: step 3260, loss 0.549413.
Train: 2018-08-09T11:11:10.171904: step 3261, loss 0.62632.
Train: 2018-08-09T11:11:10.249981: step 3262, loss 0.578862.
Train: 2018-08-09T11:11:10.328088: step 3263, loss 0.547386.
Train: 2018-08-09T11:11:10.406224: step 3264, loss 0.531735.
Train: 2018-08-09T11:11:10.484331: step 3265, loss 0.610265.
Train: 2018-08-09T11:11:10.562436: step 3266, loss 0.563197.
Train: 2018-08-09T11:11:10.640548: step 3267, loss 0.594533.
Train: 2018-08-09T11:11:10.718654: step 3268, loss 0.547627.
Train: 2018-08-09T11:11:10.796758: step 3269, loss 0.610115.
Train: 2018-08-09T11:11:10.874863: step 3270, loss 0.563294.
Test: 2018-08-09T11:11:11.374747: step 3270, loss 0.549904.
Train: 2018-08-09T11:11:11.452822: step 3271, loss 0.454289.
Train: 2018-08-09T11:11:11.530959: step 3272, loss 0.672475.
Train: 2018-08-09T11:11:11.609041: step 3273, loss 0.547716.
Train: 2018-08-09T11:11:11.687142: step 3274, loss 0.516577.
Train: 2018-08-09T11:11:11.765279: step 3275, loss 0.563326.
Train: 2018-08-09T11:11:11.859008: step 3276, loss 0.625687.
Train: 2018-08-09T11:11:11.937816: step 3277, loss 0.485247.
Train: 2018-08-09T11:11:12.015898: step 3278, loss 0.610108.
Train: 2018-08-09T11:11:12.094004: step 3279, loss 0.484934.
Train: 2018-08-09T11:11:12.172111: step 3280, loss 0.484517.
Test: 2018-08-09T11:11:12.672021: step 3280, loss 0.549992.
Train: 2018-08-09T11:11:12.750098: step 3281, loss 0.594687.
Train: 2018-08-09T11:11:12.828239: step 3282, loss 0.611059.
Train: 2018-08-09T11:11:12.906313: step 3283, loss 0.54721.
Train: 2018-08-09T11:11:12.984419: step 3284, loss 0.690908.
Train: 2018-08-09T11:11:13.062525: step 3285, loss 0.531308.
Train: 2018-08-09T11:11:13.140664: step 3286, loss 0.578649.
Train: 2018-08-09T11:11:13.218771: step 3287, loss 0.578902.
Train: 2018-08-09T11:11:13.296871: step 3288, loss 0.499575.
Train: 2018-08-09T11:11:13.374986: step 3289, loss 0.51531.
Train: 2018-08-09T11:11:13.453088: step 3290, loss 0.658244.
Test: 2018-08-09T11:11:13.952966: step 3290, loss 0.551614.
Train: 2018-08-09T11:11:14.031077: step 3291, loss 0.595006.
Train: 2018-08-09T11:11:14.109184: step 3292, loss 0.54695.
Train: 2018-08-09T11:11:14.202913: step 3293, loss 0.451402.
Train: 2018-08-09T11:11:14.281021: step 3294, loss 0.53046.
Train: 2018-08-09T11:11:14.359107: step 3295, loss 0.628666.
Train: 2018-08-09T11:11:14.437233: step 3296, loss 0.497848.
Train: 2018-08-09T11:11:14.515338: step 3297, loss 0.497783.
Train: 2018-08-09T11:11:14.593445: step 3298, loss 0.546383.
Train: 2018-08-09T11:11:14.671523: step 3299, loss 0.578456.
Train: 2018-08-09T11:11:14.749662: step 3300, loss 0.560286.
Test: 2018-08-09T11:11:15.234859: step 3300, loss 0.54742.
Train: 2018-08-09T11:11:15.797255: step 3301, loss 0.530791.
Train: 2018-08-09T11:11:15.875362: step 3302, loss 0.610074.
Train: 2018-08-09T11:11:15.953470: step 3303, loss 0.614686.
Train: 2018-08-09T11:11:16.047191: step 3304, loss 0.498181.
Train: 2018-08-09T11:11:16.125303: step 3305, loss 0.57807.
Train: 2018-08-09T11:11:16.203380: step 3306, loss 0.426485.
Train: 2018-08-09T11:11:16.281516: step 3307, loss 0.493837.
Train: 2018-08-09T11:11:16.359593: step 3308, loss 0.471312.
Train: 2018-08-09T11:11:16.437700: step 3309, loss 0.616091.
Train: 2018-08-09T11:11:16.515839: step 3310, loss 0.646679.
Test: 2018-08-09T11:11:17.017069: step 3310, loss 0.546464.
Train: 2018-08-09T11:11:17.079528: step 3311, loss 0.556362.
Train: 2018-08-09T11:11:17.173286: step 3312, loss 0.564424.
Train: 2018-08-09T11:11:17.255245: step 3313, loss 0.63203.
Train: 2018-08-09T11:11:17.333349: step 3314, loss 0.563057.
Train: 2018-08-09T11:11:17.411455: step 3315, loss 0.579712.
Train: 2018-08-09T11:11:17.489565: step 3316, loss 0.680944.
Train: 2018-08-09T11:11:17.567699: step 3317, loss 0.547234.
Train: 2018-08-09T11:11:17.645806: step 3318, loss 0.661817.
Train: 2018-08-09T11:11:17.723885: step 3319, loss 0.674096.
Train: 2018-08-09T11:11:17.801989: step 3320, loss 0.450637.
Test: 2018-08-09T11:11:18.301873: step 3320, loss 0.549866.
Train: 2018-08-09T11:11:18.380008: step 3321, loss 0.499167.
Train: 2018-08-09T11:11:18.458117: step 3322, loss 0.594341.
Train: 2018-08-09T11:11:18.536222: step 3323, loss 0.53216.
Train: 2018-08-09T11:11:18.614328: step 3324, loss 0.516166.
Train: 2018-08-09T11:11:18.692434: step 3325, loss 0.531008.
Train: 2018-08-09T11:11:18.770512: step 3326, loss 0.642775.
Train: 2018-08-09T11:11:18.848647: step 3327, loss 0.626904.
Train: 2018-08-09T11:11:18.926754: step 3328, loss 0.515075.
Train: 2018-08-09T11:11:19.004857: step 3329, loss 0.531083.
Train: 2018-08-09T11:11:19.082970: step 3330, loss 0.594795.
Test: 2018-08-09T11:11:19.582819: step 3330, loss 0.551012.
Train: 2018-08-09T11:11:19.660956: step 3331, loss 0.642598.
Train: 2018-08-09T11:11:19.739063: step 3332, loss 0.499328.
Train: 2018-08-09T11:11:19.817141: step 3333, loss 0.547054.
Train: 2018-08-09T11:11:19.895277: step 3334, loss 0.49934.
Train: 2018-08-09T11:11:19.988973: step 3335, loss 0.578859.
Train: 2018-08-09T11:11:20.067111: step 3336, loss 0.515099.
Train: 2018-08-09T11:11:20.145188: step 3337, loss 0.594832.
Train: 2018-08-09T11:11:20.223293: step 3338, loss 0.514916.
Train: 2018-08-09T11:11:20.301434: step 3339, loss 0.54682.
Train: 2018-08-09T11:11:20.379537: step 3340, loss 0.643073.
Test: 2018-08-09T11:11:20.879390: step 3340, loss 0.550183.
Train: 2018-08-09T11:11:20.959972: step 3341, loss 0.659218.
Train: 2018-08-09T11:11:21.038108: step 3342, loss 0.530766.
Train: 2018-08-09T11:11:21.116237: step 3343, loss 0.514733.
Train: 2018-08-09T11:11:21.194323: step 3344, loss 0.482616.
Train: 2018-08-09T11:11:21.272397: step 3345, loss 0.498509.
Train: 2018-08-09T11:11:21.350506: step 3346, loss 0.530524.
Train: 2018-08-09T11:11:21.428641: step 3347, loss 0.530381.
Train: 2018-08-09T11:11:21.506748: step 3348, loss 0.497779.
Train: 2018-08-09T11:11:21.584857: step 3349, loss 0.611508.
Train: 2018-08-09T11:11:21.662961: step 3350, loss 0.513579.
Test: 2018-08-09T11:11:22.156813: step 3350, loss 0.552161.
Train: 2018-08-09T11:11:22.234951: step 3351, loss 0.578959.
Train: 2018-08-09T11:11:22.313057: step 3352, loss 0.562537.
Train: 2018-08-09T11:11:22.406774: step 3353, loss 0.562514.
Train: 2018-08-09T11:11:22.484891: step 3354, loss 0.496427.
Train: 2018-08-09T11:11:22.562997: step 3355, loss 0.678431.
Train: 2018-08-09T11:11:22.641075: step 3356, loss 0.645401.
Train: 2018-08-09T11:11:22.719180: step 3357, loss 0.529337.
Train: 2018-08-09T11:11:22.797317: step 3358, loss 0.579039.
Train: 2018-08-09T11:11:22.875395: step 3359, loss 0.496245.
Train: 2018-08-09T11:11:22.960836: step 3360, loss 0.463058.
Test: 2018-08-09T11:11:23.460688: step 3360, loss 0.548779.
Train: 2018-08-09T11:11:23.538820: step 3361, loss 0.562441.
Train: 2018-08-09T11:11:23.616931: step 3362, loss 0.51252.
Train: 2018-08-09T11:11:23.695033: step 3363, loss 0.645898.
Train: 2018-08-09T11:11:23.773145: step 3364, loss 0.462219.
Train: 2018-08-09T11:11:23.851221: step 3365, loss 0.595948.
Train: 2018-08-09T11:11:23.929328: step 3366, loss 0.579308.
Train: 2018-08-09T11:11:24.007434: step 3367, loss 0.528859.
Train: 2018-08-09T11:11:24.085571: step 3368, loss 0.478444.
Train: 2018-08-09T11:11:24.163648: step 3369, loss 0.612891.
Train: 2018-08-09T11:11:24.241783: step 3370, loss 0.629542.
Test: 2018-08-09T11:11:24.741638: step 3370, loss 0.549098.
Train: 2018-08-09T11:11:24.819773: step 3371, loss 0.528626.
Train: 2018-08-09T11:11:24.897880: step 3372, loss 0.646295.
Train: 2018-08-09T11:11:24.975957: step 3373, loss 0.511206.
Train: 2018-08-09T11:11:25.054092: step 3374, loss 0.597945.
Train: 2018-08-09T11:11:25.132170: step 3375, loss 0.529152.
Train: 2018-08-09T11:11:25.210277: step 3376, loss 0.494835.
Train: 2018-08-09T11:11:25.288412: step 3377, loss 0.57897.
Train: 2018-08-09T11:11:25.366519: step 3378, loss 0.579347.
Train: 2018-08-09T11:11:25.460247: step 3379, loss 0.562871.
Train: 2018-08-09T11:11:25.538354: step 3380, loss 0.613079.
Test: 2018-08-09T11:11:26.025106: step 3380, loss 0.547958.
Train: 2018-08-09T11:11:26.103183: step 3381, loss 0.545918.
Train: 2018-08-09T11:11:26.181322: step 3382, loss 0.545728.
Train: 2018-08-09T11:11:26.259427: step 3383, loss 0.562436.
Train: 2018-08-09T11:11:26.337529: step 3384, loss 0.462338.
Train: 2018-08-09T11:11:26.415638: step 3385, loss 0.595837.
Train: 2018-08-09T11:11:26.509362: step 3386, loss 0.495622.
Train: 2018-08-09T11:11:26.587475: step 3387, loss 0.545698.
Train: 2018-08-09T11:11:26.665549: step 3388, loss 0.595876.
Train: 2018-08-09T11:11:26.743691: step 3389, loss 0.729823.
Train: 2018-08-09T11:11:26.821762: step 3390, loss 0.57912.
Test: 2018-08-09T11:11:27.321646: step 3390, loss 0.548713.
Train: 2018-08-09T11:11:27.399751: step 3391, loss 0.595754.
Train: 2018-08-09T11:11:27.477859: step 3392, loss 0.496001.
Train: 2018-08-09T11:11:27.555965: step 3393, loss 0.628811.
Train: 2018-08-09T11:11:27.634097: step 3394, loss 0.612118.
Train: 2018-08-09T11:11:27.712210: step 3395, loss 0.611991.
Train: 2018-08-09T11:11:27.790286: step 3396, loss 0.513249.
Train: 2018-08-09T11:11:27.868421: step 3397, loss 0.529789.
Train: 2018-08-09T11:11:27.948839: step 3398, loss 0.546225.
Train: 2018-08-09T11:11:28.042589: step 3399, loss 0.611616.
Train: 2018-08-09T11:11:28.120694: step 3400, loss 0.562622.
Test: 2018-08-09T11:11:28.604935: step 3400, loss 0.548007.
Train: 2018-08-09T11:11:29.198544: step 3401, loss 0.49754.
Train: 2018-08-09T11:11:29.307895: step 3402, loss 0.562648.
Train: 2018-08-09T11:11:29.386001: step 3403, loss 0.530135.
Train: 2018-08-09T11:11:29.464076: step 3404, loss 0.643961.
Train: 2018-08-09T11:11:29.542213: step 3405, loss 0.578906.
Train: 2018-08-09T11:11:29.635942: step 3406, loss 0.595119.
Train: 2018-08-09T11:11:29.714017: step 3407, loss 0.611273.
Train: 2018-08-09T11:11:29.792154: step 3408, loss 0.643487.
Train: 2018-08-09T11:11:29.870263: step 3409, loss 0.59497.
Train: 2018-08-09T11:11:29.949706: step 3410, loss 0.546798.
Test: 2018-08-09T11:11:30.449558: step 3410, loss 0.548498.
Train: 2018-08-09T11:11:30.574559: step 3411, loss 0.514911.
Train: 2018-08-09T11:11:30.652666: step 3412, loss 0.54694.
Train: 2018-08-09T11:11:30.730772: step 3413, loss 0.578861.
Train: 2018-08-09T11:11:30.808879: step 3414, loss 0.578858.
Train: 2018-08-09T11:11:30.886956: step 3415, loss 0.531157.
Train: 2018-08-09T11:11:30.965093: step 3416, loss 0.562967.
Train: 2018-08-09T11:11:31.043199: step 3417, loss 0.54706.
Train: 2018-08-09T11:11:31.136921: step 3418, loss 0.499399.
Train: 2018-08-09T11:11:31.215003: step 3419, loss 0.515233.
Train: 2018-08-09T11:11:31.293110: step 3420, loss 0.562838.
Test: 2018-08-09T11:11:31.793023: step 3420, loss 0.547266.
Train: 2018-08-09T11:11:31.871098: step 3421, loss 0.498935.
Train: 2018-08-09T11:11:31.951513: step 3422, loss 0.611104.
Train: 2018-08-09T11:11:32.029651: step 3423, loss 0.578701.
Train: 2018-08-09T11:11:32.107756: step 3424, loss 0.546874.
Train: 2018-08-09T11:11:32.185833: step 3425, loss 0.498106.
Train: 2018-08-09T11:11:32.263940: step 3426, loss 0.530052.
Train: 2018-08-09T11:11:32.342076: step 3427, loss 0.513716.
Train: 2018-08-09T11:11:32.420179: step 3428, loss 0.529528.
Train: 2018-08-09T11:11:32.513913: step 3429, loss 0.512443.
Train: 2018-08-09T11:11:32.591987: step 3430, loss 0.511338.
Test: 2018-08-09T11:11:33.091900: step 3430, loss 0.546743.
Train: 2018-08-09T11:11:33.170006: step 3431, loss 0.527839.
Train: 2018-08-09T11:11:33.248107: step 3432, loss 0.566119.
Train: 2018-08-09T11:11:33.326190: step 3433, loss 0.381849.
Train: 2018-08-09T11:11:33.404295: step 3434, loss 0.499259.
Train: 2018-08-09T11:11:33.482432: step 3435, loss 0.487331.
Train: 2018-08-09T11:11:33.560534: step 3436, loss 0.463561.
Train: 2018-08-09T11:11:33.638647: step 3437, loss 0.604796.
Train: 2018-08-09T11:11:33.716746: step 3438, loss 0.504241.
Train: 2018-08-09T11:11:33.794860: step 3439, loss 0.46672.
Train: 2018-08-09T11:11:33.872967: step 3440, loss 0.490979.
Test: 2018-08-09T11:11:34.373549: step 3440, loss 0.554692.
Train: 2018-08-09T11:11:34.451679: step 3441, loss 0.569103.
Train: 2018-08-09T11:11:34.529786: step 3442, loss 0.645254.
Train: 2018-08-09T11:11:34.607896: step 3443, loss 0.589834.
Train: 2018-08-09T11:11:34.701591: step 3444, loss 0.58865.
Train: 2018-08-09T11:11:34.782325: step 3445, loss 0.643776.
Train: 2018-08-09T11:11:34.848961: step 3446, loss 0.497147.
Train: 2018-08-09T11:11:34.927065: step 3447, loss 0.507003.
Train: 2018-08-09T11:11:35.005147: step 3448, loss 0.493881.
Train: 2018-08-09T11:11:35.098908: step 3449, loss 0.733369.
Train: 2018-08-09T11:11:35.176981: step 3450, loss 0.490211.
Test: 2018-08-09T11:11:35.661266: step 3450, loss 0.548211.
Train: 2018-08-09T11:11:35.739347: step 3451, loss 0.614812.
Train: 2018-08-09T11:11:35.817480: step 3452, loss 0.614591.
Train: 2018-08-09T11:11:35.895563: step 3453, loss 0.527774.
Train: 2018-08-09T11:11:35.989289: step 3454, loss 0.628242.
Train: 2018-08-09T11:11:36.067396: step 3455, loss 0.563058.
Train: 2018-08-09T11:11:36.145534: step 3456, loss 0.578743.
Train: 2018-08-09T11:11:36.223608: step 3457, loss 0.61155.
Train: 2018-08-09T11:11:36.301746: step 3458, loss 0.498087.
Train: 2018-08-09T11:11:36.379822: step 3459, loss 0.482257.
Train: 2018-08-09T11:11:36.457959: step 3460, loss 0.546701.
Test: 2018-08-09T11:11:36.959211: step 3460, loss 0.552597.
Train: 2018-08-09T11:11:37.037347: step 3461, loss 0.54672.
Train: 2018-08-09T11:11:37.115454: step 3462, loss 0.675307.
Train: 2018-08-09T11:11:37.193556: step 3463, loss 0.610942.
Train: 2018-08-09T11:11:37.271637: step 3464, loss 0.546879.
Train: 2018-08-09T11:11:37.349767: step 3465, loss 0.499067.
Train: 2018-08-09T11:11:37.427852: step 3466, loss 0.594807.
Train: 2018-08-09T11:11:37.505989: step 3467, loss 0.626649.
Train: 2018-08-09T11:11:37.599684: step 3468, loss 0.56296.
Train: 2018-08-09T11:11:37.677822: step 3469, loss 0.594728.
Train: 2018-08-09T11:11:37.755929: step 3470, loss 0.563014.
Test: 2018-08-09T11:11:38.240193: step 3470, loss 0.548823.
Train: 2018-08-09T11:11:38.318295: step 3471, loss 0.61048.
Train: 2018-08-09T11:11:38.396372: step 3472, loss 0.531508.
Train: 2018-08-09T11:11:38.490109: step 3473, loss 0.452764.
Train: 2018-08-09T11:11:38.568222: step 3474, loss 0.499927.
Train: 2018-08-09T11:11:38.646313: step 3475, loss 0.562963.
Train: 2018-08-09T11:11:38.724450: step 3476, loss 0.530982.
Train: 2018-08-09T11:11:38.802551: step 3477, loss 0.563655.
Train: 2018-08-09T11:11:38.880633: step 3478, loss 0.48289.
Train: 2018-08-09T11:11:38.961104: step 3479, loss 0.48163.
Train: 2018-08-09T11:11:39.039210: step 3480, loss 0.514413.
Test: 2018-08-09T11:11:39.539087: step 3480, loss 0.548818.
Train: 2018-08-09T11:11:39.617199: step 3481, loss 0.545726.
Train: 2018-08-09T11:11:39.695305: step 3482, loss 0.610982.
Train: 2018-08-09T11:11:39.773416: step 3483, loss 0.526927.
Train: 2018-08-09T11:11:39.851489: step 3484, loss 0.53076.
Train: 2018-08-09T11:11:39.929628: step 3485, loss 0.580526.
Train: 2018-08-09T11:11:40.007733: step 3486, loss 0.528029.
Train: 2018-08-09T11:11:40.085838: step 3487, loss 0.542434.
Train: 2018-08-09T11:11:40.163945: step 3488, loss 0.546926.
Train: 2018-08-09T11:11:40.257674: step 3489, loss 0.672304.
Train: 2018-08-09T11:11:40.335780: step 3490, loss 0.542477.
Test: 2018-08-09T11:11:40.820010: step 3490, loss 0.546077.
Train: 2018-08-09T11:11:40.898147: step 3491, loss 0.68256.
Train: 2018-08-09T11:11:40.978514: step 3492, loss 0.582612.
Train: 2018-08-09T11:11:41.056622: step 3493, loss 0.429647.
Train: 2018-08-09T11:11:41.134727: step 3494, loss 0.629307.
Train: 2018-08-09T11:11:41.212835: step 3495, loss 0.495977.
Train: 2018-08-09T11:11:41.306592: step 3496, loss 0.479651.
Train: 2018-08-09T11:11:41.384698: step 3497, loss 0.661055.
Train: 2018-08-09T11:11:41.462806: step 3498, loss 0.447368.
Train: 2018-08-09T11:11:41.540883: step 3499, loss 0.546099.
Train: 2018-08-09T11:11:41.619018: step 3500, loss 0.578934.
Test: 2018-08-09T11:11:42.118900: step 3500, loss 0.547078.
Train: 2018-08-09T11:11:42.696891: step 3501, loss 0.545954.
Train: 2018-08-09T11:11:42.774995: step 3502, loss 0.463636.
Train: 2018-08-09T11:11:42.868694: step 3503, loss 0.57904.
Train: 2018-08-09T11:11:42.949214: step 3504, loss 0.579024.
Train: 2018-08-09T11:11:43.027320: step 3505, loss 0.612386.
Train: 2018-08-09T11:11:43.105400: step 3506, loss 0.529387.
Train: 2018-08-09T11:11:43.183534: step 3507, loss 0.545722.
Train: 2018-08-09T11:11:43.261640: step 3508, loss 0.595828.
Train: 2018-08-09T11:11:43.339743: step 3509, loss 0.562417.
Train: 2018-08-09T11:11:43.417851: step 3510, loss 0.495964.
Test: 2018-08-09T11:11:43.917723: step 3510, loss 0.548096.
Train: 2018-08-09T11:11:43.995843: step 3511, loss 0.562416.
Train: 2018-08-09T11:11:44.073919: step 3512, loss 0.529062.
Train: 2018-08-09T11:11:44.152058: step 3513, loss 0.478933.
Train: 2018-08-09T11:11:44.230163: step 3514, loss 0.562157.
Train: 2018-08-09T11:11:44.308268: step 3515, loss 0.546036.
Train: 2018-08-09T11:11:44.386376: step 3516, loss 0.477951.
Train: 2018-08-09T11:11:44.480104: step 3517, loss 0.510856.
Train: 2018-08-09T11:11:44.558210: step 3518, loss 0.544486.
Train: 2018-08-09T11:11:44.636313: step 3519, loss 0.420302.
Train: 2018-08-09T11:11:44.714395: step 3520, loss 0.561632.
Test: 2018-08-09T11:11:45.215741: step 3520, loss 0.545693.
Train: 2018-08-09T11:11:45.293877: step 3521, loss 0.659231.
Train: 2018-08-09T11:11:45.371984: step 3522, loss 0.579125.
Train: 2018-08-09T11:11:45.450093: step 3523, loss 0.583222.
Train: 2018-08-09T11:11:45.528198: step 3524, loss 0.563071.
Train: 2018-08-09T11:11:45.609192: step 3525, loss 0.601052.
Train: 2018-08-09T11:11:45.687296: step 3526, loss 0.525816.
Train: 2018-08-09T11:11:45.780995: step 3527, loss 0.682159.
Train: 2018-08-09T11:11:45.859126: step 3528, loss 0.527448.
Train: 2018-08-09T11:11:45.937238: step 3529, loss 0.493609.
Train: 2018-08-09T11:11:46.015345: step 3530, loss 0.667306.
Test: 2018-08-09T11:11:46.499606: step 3530, loss 0.548277.
Train: 2018-08-09T11:11:46.577713: step 3531, loss 0.510866.
Train: 2018-08-09T11:11:46.655788: step 3532, loss 0.579532.
Train: 2018-08-09T11:11:46.749547: step 3533, loss 0.545058.
Train: 2018-08-09T11:11:46.827623: step 3534, loss 0.494948.
Train: 2018-08-09T11:11:46.908961: step 3535, loss 0.51182.
Train: 2018-08-09T11:11:46.981482: step 3536, loss 0.595932.
Train: 2018-08-09T11:11:47.059618: step 3537, loss 0.562214.
Train: 2018-08-09T11:11:47.137692: step 3538, loss 0.494917.
Train: 2018-08-09T11:11:47.215827: step 3539, loss 0.629479.
Train: 2018-08-09T11:11:47.309551: step 3540, loss 0.612854.
Test: 2018-08-09T11:11:47.793790: step 3540, loss 0.548548.
Train: 2018-08-09T11:11:47.877539: step 3541, loss 0.62962.
Train: 2018-08-09T11:11:47.955677: step 3542, loss 0.545644.
Train: 2018-08-09T11:11:48.033755: step 3543, loss 0.629364.
Train: 2018-08-09T11:11:48.111884: step 3544, loss 0.579172.
Train: 2018-08-09T11:11:48.189965: step 3545, loss 0.612147.
Train: 2018-08-09T11:11:48.283731: step 3546, loss 0.562476.
Train: 2018-08-09T11:11:48.361800: step 3547, loss 0.710696.
Train: 2018-08-09T11:11:48.439908: step 3548, loss 0.546198.
Train: 2018-08-09T11:11:48.518045: step 3549, loss 0.562511.
Train: 2018-08-09T11:11:48.596150: step 3550, loss 0.611102.
Test: 2018-08-09T11:11:49.096030: step 3550, loss 0.551498.
Train: 2018-08-09T11:11:49.174109: step 3551, loss 0.611208.
Train: 2018-08-09T11:11:49.252240: step 3552, loss 0.531537.
Train: 2018-08-09T11:11:49.330349: step 3553, loss 0.626626.
Train: 2018-08-09T11:11:49.408430: step 3554, loss 0.547412.
Train: 2018-08-09T11:11:49.486569: step 3555, loss 0.578944.
Train: 2018-08-09T11:11:49.564643: step 3556, loss 0.547271.
Train: 2018-08-09T11:11:49.642749: step 3557, loss 0.594894.
Train: 2018-08-09T11:11:49.720885: step 3558, loss 0.610282.
Train: 2018-08-09T11:11:49.814585: step 3559, loss 0.500269.
Train: 2018-08-09T11:11:49.892720: step 3560, loss 0.673082.
Test: 2018-08-09T11:11:50.379397: step 3560, loss 0.550899.
Train: 2018-08-09T11:11:50.457514: step 3561, loss 0.578904.
Train: 2018-08-09T11:11:50.535641: step 3562, loss 0.610059.
Train: 2018-08-09T11:11:50.629339: step 3563, loss 0.501329.
Train: 2018-08-09T11:11:50.707444: step 3564, loss 0.594107.
Train: 2018-08-09T11:11:50.785583: step 3565, loss 0.61026.
Train: 2018-08-09T11:11:50.863710: step 3566, loss 0.517335.
Train: 2018-08-09T11:11:50.941795: step 3567, loss 0.532453.
Train: 2018-08-09T11:11:51.019901: step 3568, loss 0.625034.
Train: 2018-08-09T11:11:51.098009: step 3569, loss 0.625526.
Train: 2018-08-09T11:11:51.176115: step 3570, loss 0.563852.
Test: 2018-08-09T11:11:51.675967: step 3570, loss 0.54987.
Train: 2018-08-09T11:11:51.754103: step 3571, loss 0.456412.
Train: 2018-08-09T11:11:51.832210: step 3572, loss 0.487095.
Train: 2018-08-09T11:11:51.912720: step 3573, loss 0.578656.
Train: 2018-08-09T11:11:51.990857: step 3574, loss 0.532628.
Train: 2018-08-09T11:11:52.068966: step 3575, loss 0.594169.
Train: 2018-08-09T11:11:52.162663: step 3576, loss 0.625599.
Train: 2018-08-09T11:11:52.240769: step 3577, loss 0.563819.
Train: 2018-08-09T11:11:52.318907: step 3578, loss 0.547898.
Train: 2018-08-09T11:11:52.397013: step 3579, loss 0.57893.
Train: 2018-08-09T11:11:52.475119: step 3580, loss 0.594887.
Test: 2018-08-09T11:11:52.962969: step 3580, loss 0.548393.
Train: 2018-08-09T11:11:53.056702: step 3581, loss 0.627398.
Train: 2018-08-09T11:11:53.134808: step 3582, loss 0.578681.
Train: 2018-08-09T11:11:53.212885: step 3583, loss 0.516667.
Train: 2018-08-09T11:11:53.291016: step 3584, loss 0.610412.
Train: 2018-08-09T11:11:53.369129: step 3585, loss 0.547832.
Train: 2018-08-09T11:11:53.447235: step 3586, loss 0.703202.
Train: 2018-08-09T11:11:53.525341: step 3587, loss 0.640834.
Train: 2018-08-09T11:11:53.603448: step 3588, loss 0.51705.
Train: 2018-08-09T11:11:53.681550: step 3589, loss 0.594408.
Train: 2018-08-09T11:11:53.759630: step 3590, loss 0.609672.
Test: 2018-08-09T11:11:54.259513: step 3590, loss 0.55048.
Train: 2018-08-09T11:11:54.337619: step 3591, loss 0.563636.
Train: 2018-08-09T11:11:54.415758: step 3592, loss 0.624893.
Train: 2018-08-09T11:11:54.493833: step 3593, loss 0.533365.
Train: 2018-08-09T11:11:54.571970: step 3594, loss 0.487784.
Train: 2018-08-09T11:11:54.650077: step 3595, loss 0.6247.
Train: 2018-08-09T11:11:54.743803: step 3596, loss 0.639859.
Train: 2018-08-09T11:11:54.821911: step 3597, loss 0.548698.
Train: 2018-08-09T11:11:54.899987: step 3598, loss 0.609395.
Train: 2018-08-09T11:11:54.978094: step 3599, loss 0.563938.
Train: 2018-08-09T11:11:55.056230: step 3600, loss 0.654714.
Test: 2018-08-09T11:11:55.556084: step 3600, loss 0.550469.
Train: 2018-08-09T11:11:56.135581: step 3601, loss 0.639455.
Train: 2018-08-09T11:11:56.213656: step 3602, loss 0.564089.
Train: 2018-08-09T11:11:56.291794: step 3603, loss 0.519183.
Train: 2018-08-09T11:11:56.369900: step 3604, loss 0.54925.
Train: 2018-08-09T11:11:56.448008: step 3605, loss 0.504437.
Train: 2018-08-09T11:11:56.526115: step 3606, loss 0.564154.
Train: 2018-08-09T11:11:56.604189: step 3607, loss 0.53408.
Train: 2018-08-09T11:11:56.682326: step 3608, loss 0.458401.
Train: 2018-08-09T11:11:56.760404: step 3609, loss 0.579046.
Train: 2018-08-09T11:11:56.838541: step 3610, loss 0.610522.
Test: 2018-08-09T11:11:57.338417: step 3610, loss 0.552057.
Train: 2018-08-09T11:11:57.416498: step 3611, loss 0.564059.
Train: 2018-08-09T11:11:57.494629: step 3612, loss 0.662479.
Train: 2018-08-09T11:11:57.572745: step 3613, loss 0.640414.
Train: 2018-08-09T11:11:57.650848: step 3614, loss 0.609363.
Train: 2018-08-09T11:11:57.728957: step 3615, loss 0.609342.
Train: 2018-08-09T11:11:57.822684: step 3616, loss 0.578814.
Train: 2018-08-09T11:11:57.900759: step 3617, loss 0.56429.
Train: 2018-08-09T11:11:57.965607: step 3618, loss 0.563904.
Train: 2018-08-09T11:11:58.059306: step 3619, loss 0.533242.
Train: 2018-08-09T11:11:58.137412: step 3620, loss 0.54798.
Test: 2018-08-09T11:11:58.637294: step 3620, loss 0.550457.
Train: 2018-08-09T11:11:58.715431: step 3621, loss 0.502115.
Train: 2018-08-09T11:11:58.793539: step 3622, loss 0.578194.
Train: 2018-08-09T11:11:58.871615: step 3623, loss 0.610093.
Train: 2018-08-09T11:11:58.949745: step 3624, loss 0.500471.
Train: 2018-08-09T11:11:59.027826: step 3625, loss 0.502761.
Train: 2018-08-09T11:11:59.105962: step 3626, loss 0.562957.
Train: 2018-08-09T11:11:59.184071: step 3627, loss 0.580271.
Train: 2018-08-09T11:11:59.262177: step 3628, loss 0.580817.
Train: 2018-08-09T11:11:59.355905: step 3629, loss 0.546475.
Train: 2018-08-09T11:11:59.434006: step 3630, loss 0.648133.
Test: 2018-08-09T11:11:59.920601: step 3630, loss 0.548875.
Train: 2018-08-09T11:11:59.998733: step 3631, loss 0.584292.
Train: 2018-08-09T11:12:00.092437: step 3632, loss 0.609021.
Train: 2018-08-09T11:12:00.170543: step 3633, loss 0.530186.
Train: 2018-08-09T11:12:00.248680: step 3634, loss 0.547882.
Train: 2018-08-09T11:12:00.326786: step 3635, loss 0.564279.
Train: 2018-08-09T11:12:00.404889: step 3636, loss 0.532676.
Train: 2018-08-09T11:12:00.482969: step 3637, loss 0.68786.
Train: 2018-08-09T11:12:00.561107: step 3638, loss 0.501495.
Train: 2018-08-09T11:12:00.639215: step 3639, loss 0.60991.
Train: 2018-08-09T11:12:00.717290: step 3640, loss 0.501534.
Test: 2018-08-09T11:12:01.217173: step 3640, loss 0.550661.
Train: 2018-08-09T11:12:01.295309: step 3641, loss 0.501449.
Train: 2018-08-09T11:12:01.389007: step 3642, loss 0.53232.
Train: 2018-08-09T11:12:01.467145: step 3643, loss 0.563309.
Train: 2018-08-09T11:12:01.545250: step 3644, loss 0.578852.
Train: 2018-08-09T11:12:01.623325: step 3645, loss 0.500488.
Train: 2018-08-09T11:12:01.701434: step 3646, loss 0.563085.
Train: 2018-08-09T11:12:01.779570: step 3647, loss 0.562876.
Train: 2018-08-09T11:12:01.857675: step 3648, loss 0.514992.
Train: 2018-08-09T11:12:01.936425: step 3649, loss 0.465976.
Train: 2018-08-09T11:12:02.014496: step 3650, loss 0.530682.
Test: 2018-08-09T11:12:02.514377: step 3650, loss 0.548803.
Train: 2018-08-09T11:12:02.592483: step 3651, loss 0.64287.
Train: 2018-08-09T11:12:02.670590: step 3652, loss 0.510526.
Train: 2018-08-09T11:12:02.748699: step 3653, loss 0.438555.
Train: 2018-08-09T11:12:02.826828: step 3654, loss 0.615689.
Train: 2018-08-09T11:12:02.920562: step 3655, loss 0.65391.
Train: 2018-08-09T11:12:02.998639: step 3656, loss 0.562855.
Train: 2018-08-09T11:12:03.076745: step 3657, loss 0.578978.
Train: 2018-08-09T11:12:03.154882: step 3658, loss 0.496037.
Train: 2018-08-09T11:12:03.232999: step 3659, loss 0.471158.
Train: 2018-08-09T11:12:03.311065: step 3660, loss 0.516252.
Test: 2018-08-09T11:12:03.810947: step 3660, loss 0.548518.
Train: 2018-08-09T11:12:03.889054: step 3661, loss 0.545458.
Train: 2018-08-09T11:12:03.967159: step 3662, loss 0.567042.
Train: 2018-08-09T11:12:04.045293: step 3663, loss 0.507342.
Train: 2018-08-09T11:12:04.123372: step 3664, loss 0.49766.
Train: 2018-08-09T11:12:04.201482: step 3665, loss 0.584978.
Train: 2018-08-09T11:12:04.279619: step 3666, loss 0.576529.
Train: 2018-08-09T11:12:04.357723: step 3667, loss 0.438902.
Train: 2018-08-09T11:12:04.451445: step 3668, loss 0.711635.
Train: 2018-08-09T11:12:04.529528: step 3669, loss 0.545654.
Train: 2018-08-09T11:12:04.607664: step 3670, loss 0.646365.
Test: 2018-08-09T11:12:05.093328: step 3670, loss 0.549482.
Train: 2018-08-09T11:12:05.171434: step 3671, loss 0.632923.
Train: 2018-08-09T11:12:05.249539: step 3672, loss 0.597673.
Train: 2018-08-09T11:12:05.343268: step 3673, loss 0.561026.
Train: 2018-08-09T11:12:05.421402: step 3674, loss 0.661279.
Train: 2018-08-09T11:12:05.499505: step 3675, loss 0.511955.
Train: 2018-08-09T11:12:05.577619: step 3676, loss 0.512142.
Train: 2018-08-09T11:12:05.671348: step 3677, loss 0.56283.
Train: 2018-08-09T11:12:05.749426: step 3678, loss 0.578885.
Train: 2018-08-09T11:12:05.833169: step 3679, loss 0.561772.
Train: 2018-08-09T11:12:05.911276: step 3680, loss 0.561721.
Test: 2018-08-09T11:12:06.411127: step 3680, loss 0.549185.
Train: 2018-08-09T11:12:06.489233: step 3681, loss 0.642344.
Train: 2018-08-09T11:12:06.567370: step 3682, loss 0.645572.
Train: 2018-08-09T11:12:06.645473: step 3683, loss 0.628245.
Train: 2018-08-09T11:12:06.723552: step 3684, loss 0.660168.
Train: 2018-08-09T11:12:06.801689: step 3685, loss 0.514979.
Train: 2018-08-09T11:12:06.879797: step 3686, loss 0.594167.
Train: 2018-08-09T11:12:06.957874: step 3687, loss 0.516137.
Train: 2018-08-09T11:12:07.035980: step 3688, loss 0.594746.
Train: 2018-08-09T11:12:07.129738: step 3689, loss 0.609953.
Train: 2018-08-09T11:12:07.207813: step 3690, loss 0.485536.
Test: 2018-08-09T11:12:07.692105: step 3690, loss 0.550022.
Train: 2018-08-09T11:12:07.770181: step 3691, loss 0.578913.
Train: 2018-08-09T11:12:07.848322: step 3692, loss 0.578731.
Train: 2018-08-09T11:12:07.942017: step 3693, loss 0.578957.
Train: 2018-08-09T11:12:08.020123: step 3694, loss 0.517076.
Train: 2018-08-09T11:12:08.098259: step 3695, loss 0.439926.
Train: 2018-08-09T11:12:08.176368: step 3696, loss 0.640799.
Train: 2018-08-09T11:12:08.254472: step 3697, loss 0.501446.
Train: 2018-08-09T11:12:08.332549: step 3698, loss 0.501057.
Train: 2018-08-09T11:12:08.410686: step 3699, loss 0.610111.
Train: 2018-08-09T11:12:08.488793: step 3700, loss 0.578858.
Test: 2018-08-09T11:12:08.990999: step 3700, loss 0.549673.
Train: 2018-08-09T11:12:09.568991: step 3701, loss 0.610002.
Train: 2018-08-09T11:12:09.647100: step 3702, loss 0.531519.
Train: 2018-08-09T11:12:09.725175: step 3703, loss 0.594865.
Train: 2018-08-09T11:12:09.803312: step 3704, loss 0.531711.
Train: 2018-08-09T11:12:09.881418: step 3705, loss 0.610938.
Train: 2018-08-09T11:12:09.959526: step 3706, loss 0.531192.
Train: 2018-08-09T11:12:10.037601: step 3707, loss 0.562546.
Train: 2018-08-09T11:12:10.115733: step 3708, loss 0.626418.
Train: 2018-08-09T11:12:10.193816: step 3709, loss 0.514983.
Train: 2018-08-09T11:12:10.271953: step 3710, loss 0.610304.
Test: 2018-08-09T11:12:10.771814: step 3710, loss 0.549651.
Train: 2018-08-09T11:12:10.849942: step 3711, loss 0.435209.
Train: 2018-08-09T11:12:10.930268: step 3712, loss 0.707996.
Train: 2018-08-09T11:12:11.008386: step 3713, loss 0.562178.
Train: 2018-08-09T11:12:11.086480: step 3714, loss 0.563504.
Train: 2018-08-09T11:12:11.180213: step 3715, loss 0.579575.
Train: 2018-08-09T11:12:11.258316: step 3716, loss 0.58118.
Train: 2018-08-09T11:12:11.336423: step 3717, loss 0.579355.
Train: 2018-08-09T11:12:11.414559: step 3718, loss 0.562762.
Train: 2018-08-09T11:12:11.492670: step 3719, loss 0.579067.
Train: 2018-08-09T11:12:11.570773: step 3720, loss 0.594915.
Test: 2018-08-09T11:12:12.070624: step 3720, loss 0.549454.
Train: 2018-08-09T11:12:12.148761: step 3721, loss 0.499674.
Train: 2018-08-09T11:12:12.226867: step 3722, loss 0.484202.
Train: 2018-08-09T11:12:12.304945: step 3723, loss 0.483983.
Train: 2018-08-09T11:12:12.383050: step 3724, loss 0.562924.
Train: 2018-08-09T11:12:12.461156: step 3725, loss 0.61056.
Train: 2018-08-09T11:12:12.539290: step 3726, loss 0.610975.
Train: 2018-08-09T11:12:12.617401: step 3727, loss 0.59459.
Train: 2018-08-09T11:12:12.695508: step 3728, loss 0.562873.
Train: 2018-08-09T11:12:12.789236: step 3729, loss 0.579065.
Train: 2018-08-09T11:12:12.867341: step 3730, loss 0.658618.
Test: 2018-08-09T11:12:13.354015: step 3730, loss 0.550416.
Train: 2018-08-09T11:12:13.432086: step 3731, loss 0.658571.
Train: 2018-08-09T11:12:13.510227: step 3732, loss 0.6105.
Train: 2018-08-09T11:12:13.603921: step 3733, loss 0.562991.
Train: 2018-08-09T11:12:13.682028: step 3734, loss 0.563299.
Train: 2018-08-09T11:12:13.760135: step 3735, loss 0.578899.
Train: 2018-08-09T11:12:13.838272: step 3736, loss 0.51599.
Train: 2018-08-09T11:12:13.916377: step 3737, loss 0.516235.
Train: 2018-08-09T11:12:13.994484: step 3738, loss 0.437992.
Train: 2018-08-09T11:12:14.072591: step 3739, loss 0.62598.
Train: 2018-08-09T11:12:14.150698: step 3740, loss 0.500251.
Test: 2018-08-09T11:12:14.650596: step 3740, loss 0.549531.
Train: 2018-08-09T11:12:14.728657: step 3741, loss 0.641758.
Train: 2018-08-09T11:12:14.806763: step 3742, loss 0.53153.
Train: 2018-08-09T11:12:14.884870: step 3743, loss 0.515558.
Train: 2018-08-09T11:12:14.965362: step 3744, loss 0.610477.
Train: 2018-08-09T11:12:15.043465: step 3745, loss 0.578876.
Train: 2018-08-09T11:12:15.121576: step 3746, loss 0.578891.
Train: 2018-08-09T11:12:15.199679: step 3747, loss 0.642582.
Train: 2018-08-09T11:12:15.277786: step 3748, loss 0.562902.
Train: 2018-08-09T11:12:15.371483: step 3749, loss 0.594627.
Train: 2018-08-09T11:12:15.449619: step 3750, loss 0.515282.
Test: 2018-08-09T11:12:15.933867: step 3750, loss 0.548679.
Train: 2018-08-09T11:12:16.011957: step 3751, loss 0.531038.
Train: 2018-08-09T11:12:16.090063: step 3752, loss 0.610805.
Train: 2018-08-09T11:12:16.183822: step 3753, loss 0.547036.
Train: 2018-08-09T11:12:16.261675: step 3754, loss 0.579471.
Train: 2018-08-09T11:12:16.339812: step 3755, loss 0.563558.
Train: 2018-08-09T11:12:16.417915: step 3756, loss 0.499052.
Train: 2018-08-09T11:12:16.495997: step 3757, loss 0.483309.
Train: 2018-08-09T11:12:16.574131: step 3758, loss 0.610989.
Train: 2018-08-09T11:12:16.652209: step 3759, loss 0.530694.
Train: 2018-08-09T11:12:16.730345: step 3760, loss 0.64345.
Test: 2018-08-09T11:12:17.231674: step 3760, loss 0.548994.
Train: 2018-08-09T11:12:17.309810: step 3761, loss 0.610957.
Train: 2018-08-09T11:12:17.387887: step 3762, loss 0.562872.
Train: 2018-08-09T11:12:17.466025: step 3763, loss 0.546903.
Train: 2018-08-09T11:12:17.544130: step 3764, loss 0.562924.
Train: 2018-08-09T11:12:17.622236: step 3765, loss 0.562959.
Train: 2018-08-09T11:12:17.700314: step 3766, loss 0.482845.
Train: 2018-08-09T11:12:17.778439: step 3767, loss 0.514754.
Train: 2018-08-09T11:12:17.856556: step 3768, loss 0.64313.
Train: 2018-08-09T11:12:17.934663: step 3769, loss 0.546639.
Train: 2018-08-09T11:12:18.028391: step 3770, loss 0.466182.
Test: 2018-08-09T11:12:18.512652: step 3770, loss 0.548821.
Train: 2018-08-09T11:12:18.592445: step 3771, loss 0.562715.
Train: 2018-08-09T11:12:18.670552: step 3772, loss 0.595186.
Train: 2018-08-09T11:12:18.764300: step 3773, loss 0.497811.
Train: 2018-08-09T11:12:18.826735: step 3774, loss 0.546562.
Train: 2018-08-09T11:12:18.919274: step 3775, loss 0.546553.
Train: 2018-08-09T11:12:18.997384: step 3776, loss 0.562533.
Train: 2018-08-09T11:12:19.075519: step 3777, loss 0.546088.
Train: 2018-08-09T11:12:19.153625: step 3778, loss 0.579088.
Train: 2018-08-09T11:12:19.231703: step 3779, loss 0.480036.
Train: 2018-08-09T11:12:19.309839: step 3780, loss 0.463149.
Test: 2018-08-09T11:12:19.809690: step 3780, loss 0.551294.
Train: 2018-08-09T11:12:19.887823: step 3781, loss 0.562451.
Train: 2018-08-09T11:12:19.965903: step 3782, loss 0.562252.
Train: 2018-08-09T11:12:20.044041: step 3783, loss 0.545862.
Train: 2018-08-09T11:12:20.122147: step 3784, loss 0.528893.
Train: 2018-08-09T11:12:20.200253: step 3785, loss 0.545099.
Train: 2018-08-09T11:12:20.293977: step 3786, loss 0.630597.
Train: 2018-08-09T11:12:20.372089: step 3787, loss 0.54496.
Train: 2018-08-09T11:12:20.450165: step 3788, loss 0.442756.
Train: 2018-08-09T11:12:20.528302: step 3789, loss 0.493436.
Train: 2018-08-09T11:12:20.606408: step 3790, loss 0.526679.
Test: 2018-08-09T11:12:21.090668: step 3790, loss 0.546707.
Train: 2018-08-09T11:12:21.168775: step 3791, loss 0.613185.
Train: 2018-08-09T11:12:21.262503: step 3792, loss 0.527861.
Train: 2018-08-09T11:12:21.340610: step 3793, loss 0.507689.
Train: 2018-08-09T11:12:21.418718: step 3794, loss 0.509439.
Train: 2018-08-09T11:12:21.496792: step 3795, loss 0.598702.
Train: 2018-08-09T11:12:21.574933: step 3796, loss 0.63738.
Train: 2018-08-09T11:12:21.653037: step 3797, loss 0.511076.
Train: 2018-08-09T11:12:21.731142: step 3798, loss 0.580325.
Train: 2018-08-09T11:12:21.809251: step 3799, loss 0.543591.
Train: 2018-08-09T11:12:21.902971: step 3800, loss 0.579426.
Test: 2018-08-09T11:12:22.389544: step 3800, loss 0.551197.
Train: 2018-08-09T11:12:22.936257: step 3801, loss 0.561385.
Train: 2018-08-09T11:12:23.014396: step 3802, loss 0.581067.
Train: 2018-08-09T11:12:23.092472: step 3803, loss 0.475115.
Train: 2018-08-09T11:12:23.170608: step 3804, loss 0.614551.
Train: 2018-08-09T11:12:23.264331: step 3805, loss 0.457254.
Train: 2018-08-09T11:12:23.342414: step 3806, loss 0.597924.
Train: 2018-08-09T11:12:23.420549: step 3807, loss 0.543914.
Train: 2018-08-09T11:12:23.502215: step 3808, loss 0.476073.
Train: 2018-08-09T11:12:23.580351: step 3809, loss 0.59757.
Train: 2018-08-09T11:12:23.658429: step 3810, loss 0.59733.
Test: 2018-08-09T11:12:24.155941: step 3810, loss 0.546711.
Train: 2018-08-09T11:12:24.234048: step 3811, loss 0.64905.
Train: 2018-08-09T11:12:24.312155: step 3812, loss 0.527343.
Train: 2018-08-09T11:12:24.390294: step 3813, loss 0.509894.
Train: 2018-08-09T11:12:24.468396: step 3814, loss 0.614424.
Train: 2018-08-09T11:12:24.546473: step 3815, loss 0.423843.
Train: 2018-08-09T11:12:24.640231: step 3816, loss 0.54435.
Train: 2018-08-09T11:12:24.718308: step 3817, loss 0.492664.
Train: 2018-08-09T11:12:24.796446: step 3818, loss 0.562919.
Train: 2018-08-09T11:12:24.874560: step 3819, loss 0.562562.
Train: 2018-08-09T11:12:24.952657: step 3820, loss 0.527608.
Test: 2018-08-09T11:12:25.452539: step 3820, loss 0.549372.
Train: 2018-08-09T11:12:25.530646: step 3821, loss 0.681391.
Train: 2018-08-09T11:12:25.608752: step 3822, loss 0.595681.
Train: 2018-08-09T11:12:25.686861: step 3823, loss 0.560535.
Train: 2018-08-09T11:12:25.764934: step 3824, loss 0.563592.
Train: 2018-08-09T11:12:25.843066: step 3825, loss 0.665522.
Train: 2018-08-09T11:12:25.921183: step 3826, loss 0.408087.
Train: 2018-08-09T11:12:25.999285: step 3827, loss 0.580244.
Train: 2018-08-09T11:12:26.077391: step 3828, loss 0.562401.
Train: 2018-08-09T11:12:26.155470: step 3829, loss 0.493335.
Train: 2018-08-09T11:12:26.233605: step 3830, loss 0.579559.
Test: 2018-08-09T11:12:26.733457: step 3830, loss 0.547557.
Train: 2018-08-09T11:12:26.811563: step 3831, loss 0.599298.
Train: 2018-08-09T11:12:26.889671: step 3832, loss 0.59682.
Train: 2018-08-09T11:12:26.985336: step 3833, loss 0.544264.
Train: 2018-08-09T11:12:27.063443: step 3834, loss 0.649847.
Train: 2018-08-09T11:12:27.141519: step 3835, loss 0.528361.
Train: 2018-08-09T11:12:27.235277: step 3836, loss 0.544607.
Train: 2018-08-09T11:12:27.313383: step 3837, loss 0.444584.
Train: 2018-08-09T11:12:27.391490: step 3838, loss 0.513329.
Train: 2018-08-09T11:12:27.485190: step 3839, loss 0.512649.
Train: 2018-08-09T11:12:27.563324: step 3840, loss 0.428804.
Test: 2018-08-09T11:12:28.047581: step 3840, loss 0.551061.
Train: 2018-08-09T11:12:28.141307: step 3841, loss 0.577875.
Train: 2018-08-09T11:12:28.219389: step 3842, loss 0.578279.
Train: 2018-08-09T11:12:28.297502: step 3843, loss 0.579984.
Train: 2018-08-09T11:12:28.375633: step 3844, loss 0.6485.
Train: 2018-08-09T11:12:28.453745: step 3845, loss 0.577956.
Train: 2018-08-09T11:12:28.531819: step 3846, loss 0.562206.
Train: 2018-08-09T11:12:28.609953: step 3847, loss 0.578508.
Train: 2018-08-09T11:12:28.688059: step 3848, loss 0.629119.
Train: 2018-08-09T11:12:28.766167: step 3849, loss 0.562395.
Train: 2018-08-09T11:12:28.844242: step 3850, loss 0.56101.
Test: 2018-08-09T11:12:29.344150: step 3850, loss 0.546045.
Train: 2018-08-09T11:12:29.422231: step 3851, loss 0.54287.
Train: 2018-08-09T11:12:29.500339: step 3852, loss 0.563822.
Train: 2018-08-09T11:12:29.594094: step 3853, loss 0.629775.
Train: 2018-08-09T11:12:29.672202: step 3854, loss 0.698251.
Train: 2018-08-09T11:12:29.750309: step 3855, loss 0.577922.
Train: 2018-08-09T11:12:29.828413: step 3856, loss 0.546248.
Train: 2018-08-09T11:12:29.906522: step 3857, loss 0.545158.
Train: 2018-08-09T11:12:29.984598: step 3858, loss 0.612763.
Train: 2018-08-09T11:12:30.062739: step 3859, loss 0.580784.
Train: 2018-08-09T11:12:30.140842: step 3860, loss 0.692691.
Test: 2018-08-09T11:12:30.640695: step 3860, loss 0.550725.
Train: 2018-08-09T11:12:30.718801: step 3861, loss 0.643649.
Train: 2018-08-09T11:12:30.796937: step 3862, loss 0.514545.
Train: 2018-08-09T11:12:30.875045: step 3863, loss 0.467755.
Train: 2018-08-09T11:12:30.955469: step 3864, loss 0.483856.
Train: 2018-08-09T11:12:31.033577: step 3865, loss 0.562628.
Train: 2018-08-09T11:12:31.111655: step 3866, loss 0.626187.
Train: 2018-08-09T11:12:31.189789: step 3867, loss 0.626144.
Train: 2018-08-09T11:12:31.267898: step 3868, loss 0.578906.
Train: 2018-08-09T11:12:31.361618: step 3869, loss 0.610221.
Train: 2018-08-09T11:12:31.439700: step 3870, loss 0.547766.
Test: 2018-08-09T11:12:31.923961: step 3870, loss 0.549832.
Train: 2018-08-09T11:12:32.002097: step 3871, loss 0.563333.
Train: 2018-08-09T11:12:32.080204: step 3872, loss 0.610165.
Train: 2018-08-09T11:12:32.173933: step 3873, loss 0.4856.
Train: 2018-08-09T11:12:32.252008: step 3874, loss 0.439032.
Train: 2018-08-09T11:12:32.330116: step 3875, loss 0.578864.
Train: 2018-08-09T11:12:32.408253: step 3876, loss 0.594799.
Train: 2018-08-09T11:12:32.486328: step 3877, loss 0.594454.
Train: 2018-08-09T11:12:32.564479: step 3878, loss 0.516333.
Train: 2018-08-09T11:12:32.642567: step 3879, loss 0.547496.
Train: 2018-08-09T11:12:32.720678: step 3880, loss 0.562686.
Test: 2018-08-09T11:12:33.222854: step 3880, loss 0.55056.
Train: 2018-08-09T11:12:33.300958: step 3881, loss 0.515406.
Train: 2018-08-09T11:12:33.379096: step 3882, loss 0.481634.
Train: 2018-08-09T11:12:33.457203: step 3883, loss 0.512459.
Train: 2018-08-09T11:12:33.535309: step 3884, loss 0.509324.
Train: 2018-08-09T11:12:33.613418: step 3885, loss 0.575319.
Train: 2018-08-09T11:12:33.707138: step 3886, loss 0.587513.
Train: 2018-08-09T11:12:33.785250: step 3887, loss 0.491331.
Train: 2018-08-09T11:12:33.863327: step 3888, loss 0.608581.
Train: 2018-08-09T11:12:33.941465: step 3889, loss 0.639906.
Train: 2018-08-09T11:12:34.019571: step 3890, loss 0.61738.
Test: 2018-08-09T11:12:34.519454: step 3890, loss 0.544489.
Train: 2018-08-09T11:12:34.597558: step 3891, loss 0.595813.
Train: 2018-08-09T11:12:34.675665: step 3892, loss 0.492302.
Train: 2018-08-09T11:12:34.753742: step 3893, loss 0.594065.
Train: 2018-08-09T11:12:34.831879: step 3894, loss 0.541724.
Train: 2018-08-09T11:12:34.911453: step 3895, loss 0.714104.
Train: 2018-08-09T11:12:34.989561: step 3896, loss 0.464706.
Train: 2018-08-09T11:12:35.067696: step 3897, loss 0.593214.
Train: 2018-08-09T11:12:35.145803: step 3898, loss 0.644233.
Train: 2018-08-09T11:12:35.223915: step 3899, loss 0.464936.
Train: 2018-08-09T11:12:35.302016: step 3900, loss 0.526746.
Test: 2018-08-09T11:12:35.801868: step 3900, loss 0.548342.
Train: 2018-08-09T11:12:36.411100: step 3901, loss 0.627793.
Train: 2018-08-09T11:12:36.489237: step 3902, loss 0.512759.
Train: 2018-08-09T11:12:36.567344: step 3903, loss 0.630267.
Train: 2018-08-09T11:12:36.645451: step 3904, loss 0.593147.
Train: 2018-08-09T11:12:36.723560: step 3905, loss 0.51091.
Train: 2018-08-09T11:12:36.811075: step 3906, loss 0.547943.
Train: 2018-08-09T11:12:36.878742: step 3907, loss 0.593313.
Train: 2018-08-09T11:12:36.959128: step 3908, loss 0.61064.
Train: 2018-08-09T11:12:37.037265: step 3909, loss 0.615335.
Train: 2018-08-09T11:12:37.115340: step 3910, loss 0.483217.
Test: 2018-08-09T11:12:37.615223: step 3910, loss 0.549769.
Train: 2018-08-09T11:12:37.693354: step 3911, loss 0.5967.
Train: 2018-08-09T11:12:37.787087: step 3912, loss 0.627412.
Train: 2018-08-09T11:12:37.865188: step 3913, loss 0.564247.
Train: 2018-08-09T11:12:37.943270: step 3914, loss 0.704883.
Train: 2018-08-09T11:12:38.021379: step 3915, loss 0.578393.
Train: 2018-08-09T11:12:38.099513: step 3916, loss 0.516932.
Train: 2018-08-09T11:12:38.177620: step 3917, loss 0.502023.
Train: 2018-08-09T11:12:38.255696: step 3918, loss 0.517571.
Train: 2018-08-09T11:12:38.349425: step 3919, loss 0.502563.
Train: 2018-08-09T11:12:38.427561: step 3920, loss 0.579081.
Test: 2018-08-09T11:12:38.914096: step 3920, loss 0.55203.
Train: 2018-08-09T11:12:38.992233: step 3921, loss 0.656173.
Train: 2018-08-09T11:12:39.070310: step 3922, loss 0.409685.
Train: 2018-08-09T11:12:39.164051: step 3923, loss 0.563605.
Train: 2018-08-09T11:12:39.242144: step 3924, loss 0.579022.
Train: 2018-08-09T11:12:39.320281: step 3925, loss 0.609994.
Train: 2018-08-09T11:12:39.398383: step 3926, loss 0.594449.
Train: 2018-08-09T11:12:39.476496: step 3927, loss 0.594434.
Train: 2018-08-09T11:12:39.554600: step 3928, loss 0.609963.
Train: 2018-08-09T11:12:39.632678: step 3929, loss 0.532355.
Train: 2018-08-09T11:12:39.710783: step 3930, loss 0.516818.
Test: 2018-08-09T11:12:40.210691: step 3930, loss 0.549958.
Train: 2018-08-09T11:12:40.288772: step 3931, loss 0.734427.
Train: 2018-08-09T11:12:40.366910: step 3932, loss 0.53234.
Train: 2018-08-09T11:12:40.445020: step 3933, loss 0.578916.
Train: 2018-08-09T11:12:40.523092: step 3934, loss 0.516875.
Train: 2018-08-09T11:12:40.601199: step 3935, loss 0.547889.
Train: 2018-08-09T11:12:40.694953: step 3936, loss 0.501205.
Train: 2018-08-09T11:12:40.773063: step 3937, loss 0.594384.
Train: 2018-08-09T11:12:40.851166: step 3938, loss 0.50071.
Train: 2018-08-09T11:12:40.930685: step 3939, loss 0.547713.
Train: 2018-08-09T11:12:41.008790: step 3940, loss 0.452198.
Test: 2018-08-09T11:12:41.508642: step 3940, loss 0.550349.
Train: 2018-08-09T11:12:41.586748: step 3941, loss 0.659045.
Train: 2018-08-09T11:12:41.664886: step 3942, loss 0.482604.
Train: 2018-08-09T11:12:41.742964: step 3943, loss 0.596442.
Train: 2018-08-09T11:12:41.821068: step 3944, loss 0.661728.
Train: 2018-08-09T11:12:41.899175: step 3945, loss 0.561593.
Train: 2018-08-09T11:12:41.977308: step 3946, loss 0.579676.
Train: 2018-08-09T11:12:42.055419: step 3947, loss 0.57878.
Train: 2018-08-09T11:12:42.133526: step 3948, loss 0.547314.
Train: 2018-08-09T11:12:42.227225: step 3949, loss 0.514886.
Train: 2018-08-09T11:12:42.305331: step 3950, loss 0.514897.
Test: 2018-08-09T11:12:42.789622: step 3950, loss 0.548404.
Train: 2018-08-09T11:12:42.867728: step 3951, loss 0.530758.
Train: 2018-08-09T11:12:42.947213: step 3952, loss 0.627133.
Train: 2018-08-09T11:12:43.040912: step 3953, loss 0.562535.
Train: 2018-08-09T11:12:43.119016: step 3954, loss 0.481615.
Train: 2018-08-09T11:12:43.197154: step 3955, loss 0.481687.
Train: 2018-08-09T11:12:43.275257: step 3956, loss 0.561942.
Train: 2018-08-09T11:12:43.353368: step 3957, loss 0.510839.
Train: 2018-08-09T11:12:43.431444: step 3958, loss 0.580448.
Train: 2018-08-09T11:12:43.509550: step 3959, loss 0.492933.
Train: 2018-08-09T11:12:43.587687: step 3960, loss 0.633844.
Test: 2018-08-09T11:12:44.087540: step 3960, loss 0.547849.
Train: 2018-08-09T11:12:44.181267: step 3961, loss 0.561341.
Train: 2018-08-09T11:12:44.259373: step 3962, loss 0.68076.
Train: 2018-08-09T11:12:44.337485: step 3963, loss 0.479209.
Train: 2018-08-09T11:12:44.415586: step 3964, loss 0.546542.
Train: 2018-08-09T11:12:44.493694: step 3965, loss 0.702794.
Train: 2018-08-09T11:12:44.571832: step 3966, loss 0.705064.
Train: 2018-08-09T11:12:44.649938: step 3967, loss 0.629421.
Train: 2018-08-09T11:12:44.728013: step 3968, loss 0.464057.
Train: 2018-08-09T11:12:44.821772: step 3969, loss 0.465078.
Train: 2018-08-09T11:12:44.899878: step 3970, loss 0.595046.
Test: 2018-08-09T11:12:45.385536: step 3970, loss 0.550647.
Train: 2018-08-09T11:12:45.463672: step 3971, loss 0.643341.
Train: 2018-08-09T11:12:45.541779: step 3972, loss 0.578849.
Train: 2018-08-09T11:12:45.635511: step 3973, loss 0.578867.
Train: 2018-08-09T11:12:45.713584: step 3974, loss 0.4989.
Train: 2018-08-09T11:12:45.791690: step 3975, loss 0.610817.
Train: 2018-08-09T11:12:45.869825: step 3976, loss 0.57886.
Train: 2018-08-09T11:12:45.947933: step 3977, loss 0.531095.
Train: 2018-08-09T11:12:46.026009: step 3978, loss 0.626577.
Train: 2018-08-09T11:12:46.104168: step 3979, loss 0.531232.
Train: 2018-08-09T11:12:46.182248: step 3980, loss 0.531282.
Test: 2018-08-09T11:12:46.682130: step 3980, loss 0.550548.
Train: 2018-08-09T11:12:46.760244: step 3981, loss 0.483735.
Train: 2018-08-09T11:12:46.838319: step 3982, loss 0.642347.
Train: 2018-08-09T11:12:46.917868: step 3983, loss 0.531243.
Train: 2018-08-09T11:12:47.006645: step 3984, loss 0.578885.
Train: 2018-08-09T11:12:47.084753: step 3985, loss 0.578913.
Train: 2018-08-09T11:12:47.162829: step 3986, loss 0.626436.
Train: 2018-08-09T11:12:47.240936: step 3987, loss 0.610466.
Train: 2018-08-09T11:12:47.319072: step 3988, loss 0.547448.
Train: 2018-08-09T11:12:47.397180: step 3989, loss 0.547523.
Train: 2018-08-09T11:12:47.475254: step 3990, loss 0.594757.
Test: 2018-08-09T11:12:47.975166: step 3990, loss 0.550044.
Train: 2018-08-09T11:12:48.053274: step 3991, loss 0.62627.
Train: 2018-08-09T11:12:48.131381: step 3992, loss 0.578857.
Train: 2018-08-09T11:12:48.209457: step 3993, loss 0.563129.
Train: 2018-08-09T11:12:48.287563: step 3994, loss 0.626012.
Train: 2018-08-09T11:12:48.381292: step 3995, loss 0.641593.
Train: 2018-08-09T11:12:48.459398: step 3996, loss 0.641385.
Train: 2018-08-09T11:12:48.537535: step 3997, loss 0.501077.
Train: 2018-08-09T11:12:48.615611: step 3998, loss 0.455003.
Train: 2018-08-09T11:12:48.693744: step 3999, loss 0.56339.
Train: 2018-08-09T11:12:48.771854: step 4000, loss 0.640999.
Test: 2018-08-09T11:12:49.262848: step 4000, loss 0.550046.
Train: 2018-08-09T11:12:49.806642: step 4001, loss 0.485852.
Train: 2018-08-09T11:12:49.900340: step 4002, loss 0.563421.
Train: 2018-08-09T11:12:49.978474: step 4003, loss 0.578918.
Train: 2018-08-09T11:12:50.056582: step 4004, loss 0.516729.
Train: 2018-08-09T11:12:50.134658: step 4005, loss 0.578911.
Train: 2018-08-09T11:12:50.212797: step 4006, loss 0.516553.
Train: 2018-08-09T11:12:50.290903: step 4007, loss 0.422592.
Train: 2018-08-09T11:12:50.369009: step 4008, loss 0.547478.
Train: 2018-08-09T11:12:50.447112: step 4009, loss 0.56306.
Train: 2018-08-09T11:12:50.540844: step 4010, loss 0.562771.
Test: 2018-08-09T11:12:51.040696: step 4010, loss 0.549686.
Train: 2018-08-09T11:12:51.118826: step 4011, loss 0.482729.
Train: 2018-08-09T11:12:51.196938: step 4012, loss 0.579036.
Train: 2018-08-09T11:12:51.275050: step 4013, loss 0.481183.
Train: 2018-08-09T11:12:51.353158: step 4014, loss 0.579738.
Train: 2018-08-09T11:12:51.431258: step 4015, loss 0.613367.
Train: 2018-08-09T11:12:51.509335: step 4016, loss 0.616688.
Train: 2018-08-09T11:12:51.587467: step 4017, loss 0.495643.
Train: 2018-08-09T11:12:51.665579: step 4018, loss 0.496068.
Train: 2018-08-09T11:12:51.743682: step 4019, loss 0.562652.
Train: 2018-08-09T11:12:51.821786: step 4020, loss 0.528296.
Test: 2018-08-09T11:12:52.323989: step 4020, loss 0.54905.
Train: 2018-08-09T11:12:52.402127: step 4021, loss 0.511531.
Train: 2018-08-09T11:12:52.480232: step 4022, loss 0.528522.
Train: 2018-08-09T11:12:52.558342: step 4023, loss 0.524674.
Train: 2018-08-09T11:12:52.636441: step 4024, loss 0.561265.
Train: 2018-08-09T11:12:52.714553: step 4025, loss 0.636249.
Train: 2018-08-09T11:12:52.808282: step 4026, loss 0.528763.
Train: 2018-08-09T11:12:52.886358: step 4027, loss 0.582153.
Train: 2018-08-09T11:12:52.964494: step 4028, loss 0.474787.
Train: 2018-08-09T11:12:53.042603: step 4029, loss 0.597453.
Train: 2018-08-09T11:12:53.120677: step 4030, loss 0.47343.
Test: 2018-08-09T11:12:53.604938: step 4030, loss 0.547114.
Train: 2018-08-09T11:12:53.683075: step 4031, loss 0.488612.
Train: 2018-08-09T11:12:53.761181: step 4032, loss 0.635981.
Train: 2018-08-09T11:12:53.854910: step 4033, loss 0.49037.
Train: 2018-08-09T11:12:53.934449: step 4034, loss 0.453026.
Train: 2018-08-09T11:12:54.012526: step 4035, loss 0.525071.
Train: 2018-08-09T11:12:54.090631: step 4036, loss 0.586154.
Train: 2018-08-09T11:12:54.168770: step 4037, loss 0.545939.
Train: 2018-08-09T11:12:54.246876: step 4038, loss 0.671294.
Train: 2018-08-09T11:12:54.321980: step 4039, loss 0.543881.
Train: 2018-08-09T11:12:54.400088: step 4040, loss 0.533499.
Test: 2018-08-09T11:12:54.899995: step 4040, loss 0.550499.
Train: 2018-08-09T11:12:54.978108: step 4041, loss 0.778307.
Train: 2018-08-09T11:12:55.056216: step 4042, loss 0.581558.
Train: 2018-08-09T11:12:55.134321: step 4043, loss 0.560872.
Train: 2018-08-09T11:12:55.228019: step 4044, loss 0.630168.
Train: 2018-08-09T11:12:55.306157: step 4045, loss 0.54709.
Train: 2018-08-09T11:12:55.384261: step 4046, loss 0.511818.
Train: 2018-08-09T11:12:55.462362: step 4047, loss 0.644251.
Train: 2018-08-09T11:12:55.540444: step 4048, loss 0.661404.
Train: 2018-08-09T11:12:55.618580: step 4049, loss 0.627686.
Train: 2018-08-09T11:12:55.696687: step 4050, loss 0.530202.
Test: 2018-08-09T11:12:56.201021: step 4050, loss 0.550802.
Train: 2018-08-09T11:12:56.279126: step 4051, loss 0.498619.
Train: 2018-08-09T11:12:56.357258: step 4052, loss 0.610702.
Train: 2018-08-09T11:12:56.435367: step 4053, loss 0.562929.
Train: 2018-08-09T11:12:56.513476: step 4054, loss 0.626335.
Train: 2018-08-09T11:12:56.591583: step 4055, loss 0.562918.
Train: 2018-08-09T11:12:56.685281: step 4056, loss 0.579019.
Train: 2018-08-09T11:12:56.763417: step 4057, loss 0.563139.
Train: 2018-08-09T11:12:56.841493: step 4058, loss 0.578967.
Train: 2018-08-09T11:12:56.919628: step 4059, loss 0.656603.
Train: 2018-08-09T11:12:56.997737: step 4060, loss 0.547891.
Test: 2018-08-09T11:12:57.497589: step 4060, loss 0.551424.
Train: 2018-08-09T11:12:57.575727: step 4061, loss 0.594207.
Train: 2018-08-09T11:12:57.653833: step 4062, loss 0.578847.
Train: 2018-08-09T11:12:57.731910: step 4063, loss 0.563943.
Train: 2018-08-09T11:12:57.810048: step 4064, loss 0.563661.
Train: 2018-08-09T11:12:57.888153: step 4065, loss 0.593917.
Train: 2018-08-09T11:12:57.966260: step 4066, loss 0.548128.
Train: 2018-08-09T11:12:58.044366: step 4067, loss 0.53361.
Train: 2018-08-09T11:12:58.122472: step 4068, loss 0.594982.
Train: 2018-08-09T11:12:58.200580: step 4069, loss 0.563594.
Train: 2018-08-09T11:12:58.278655: step 4070, loss 0.655242.
Test: 2018-08-09T11:12:58.794159: step 4070, loss 0.550752.
Train: 2018-08-09T11:12:58.872265: step 4071, loss 0.62525.
Train: 2018-08-09T11:12:58.952686: step 4072, loss 0.548504.
Train: 2018-08-09T11:12:59.030794: step 4073, loss 0.594257.
Train: 2018-08-09T11:12:59.108869: step 4074, loss 0.503501.
Train: 2018-08-09T11:12:59.187000: step 4075, loss 0.594262.
Train: 2018-08-09T11:12:59.265114: step 4076, loss 0.624521.
Train: 2018-08-09T11:12:59.343218: step 4077, loss 0.594232.
Train: 2018-08-09T11:12:59.421294: step 4078, loss 0.563933.
Train: 2018-08-09T11:12:59.499403: step 4079, loss 0.503487.
Train: 2018-08-09T11:12:59.593128: step 4080, loss 0.563931.
Test: 2018-08-09T11:13:00.077421: step 4080, loss 0.55028.
Train: 2018-08-09T11:13:00.155527: step 4081, loss 0.609486.
Train: 2018-08-09T11:13:00.233604: step 4082, loss 0.56386.
Train: 2018-08-09T11:13:00.327362: step 4083, loss 0.548641.
Train: 2018-08-09T11:13:00.405468: step 4084, loss 0.502825.
Train: 2018-08-09T11:13:00.483577: step 4085, loss 0.670725.
Train: 2018-08-09T11:13:00.561682: step 4086, loss 0.518173.
Train: 2018-08-09T11:13:00.639788: step 4087, loss 0.51743.
Train: 2018-08-09T11:13:00.717895: step 4088, loss 0.517508.
Train: 2018-08-09T11:13:00.795972: step 4089, loss 0.548673.
Train: 2018-08-09T11:13:00.874109: step 4090, loss 0.594435.
Test: 2018-08-09T11:13:01.374710: step 4090, loss 0.55064.
Train: 2018-08-09T11:13:01.452815: step 4091, loss 0.530197.
Train: 2018-08-09T11:13:01.530923: step 4092, loss 0.448701.
Train: 2018-08-09T11:13:01.609059: step 4093, loss 0.61051.
Train: 2018-08-09T11:13:01.687166: step 4094, loss 0.628693.
Train: 2018-08-09T11:13:01.765275: step 4095, loss 0.576906.
Train: 2018-08-09T11:13:01.858972: step 4096, loss 0.634562.
Train: 2018-08-09T11:13:01.937107: step 4097, loss 0.596723.
Train: 2018-08-09T11:13:02.015184: step 4098, loss 0.478737.
Train: 2018-08-09T11:13:02.093324: step 4099, loss 0.613921.
Train: 2018-08-09T11:13:02.171396: step 4100, loss 0.594619.
Test: 2018-08-09T11:13:02.671279: step 4100, loss 0.547008.
Train: 2018-08-09T11:13:03.218055: step 4101, loss 0.545801.
Train: 2018-08-09T11:13:03.296165: step 4102, loss 0.581047.
Train: 2018-08-09T11:13:03.374269: step 4103, loss 0.49617.
Train: 2018-08-09T11:13:03.452374: step 4104, loss 0.54315.
Train: 2018-08-09T11:13:03.530476: step 4105, loss 0.596338.
Train: 2018-08-09T11:13:03.608558: step 4106, loss 0.526418.
Train: 2018-08-09T11:13:03.686694: step 4107, loss 0.530481.
Train: 2018-08-09T11:13:03.764804: step 4108, loss 0.549898.
Train: 2018-08-09T11:13:03.858530: step 4109, loss 0.401085.
Train: 2018-08-09T11:13:03.923456: step 4110, loss 0.602814.
Test: 2018-08-09T11:13:04.423338: step 4110, loss 0.54825.
Train: 2018-08-09T11:13:04.501444: step 4111, loss 0.582382.
Train: 2018-08-09T11:13:04.579550: step 4112, loss 0.588657.
Train: 2018-08-09T11:13:04.657655: step 4113, loss 0.527136.
Train: 2018-08-09T11:13:04.751354: step 4114, loss 0.530935.
Train: 2018-08-09T11:13:04.829491: step 4115, loss 0.558622.
Train: 2018-08-09T11:13:04.907600: step 4116, loss 0.531849.
Train: 2018-08-09T11:13:04.985704: step 4117, loss 0.62265.
Train: 2018-08-09T11:13:05.063782: step 4118, loss 0.548953.
Train: 2018-08-09T11:13:05.141920: step 4119, loss 0.547598.
Train: 2018-08-09T11:13:05.220024: step 4120, loss 0.499413.
Test: 2018-08-09T11:13:05.719876: step 4120, loss 0.548435.
Train: 2018-08-09T11:13:05.798009: step 4121, loss 0.577339.
Train: 2018-08-09T11:13:05.891747: step 4122, loss 0.545637.
Train: 2018-08-09T11:13:05.972139: step 4123, loss 0.563777.
Train: 2018-08-09T11:13:06.050215: step 4124, loss 0.495801.
Train: 2018-08-09T11:13:06.128352: step 4125, loss 0.559959.
Train: 2018-08-09T11:13:06.206429: step 4126, loss 0.521981.
Train: 2018-08-09T11:13:06.284566: step 4127, loss 0.533157.
Train: 2018-08-09T11:13:06.362671: step 4128, loss 0.550345.
Train: 2018-08-09T11:13:06.440778: step 4129, loss 0.473641.
Train: 2018-08-09T11:13:06.518855: step 4130, loss 0.507721.
Test: 2018-08-09T11:13:07.018767: step 4130, loss 0.546369.
Train: 2018-08-09T11:13:07.096873: step 4131, loss 0.643297.
Train: 2018-08-09T11:13:07.174978: step 4132, loss 0.471477.
Train: 2018-08-09T11:13:07.253057: step 4133, loss 0.527383.
Train: 2018-08-09T11:13:07.346784: step 4134, loss 0.617244.
Train: 2018-08-09T11:13:07.424922: step 4135, loss 0.600132.
Train: 2018-08-09T11:13:07.503029: step 4136, loss 0.604499.
Train: 2018-08-09T11:13:07.581128: step 4137, loss 0.563367.
Train: 2018-08-09T11:13:07.659244: step 4138, loss 0.628429.
Train: 2018-08-09T11:13:07.737318: step 4139, loss 0.546995.
Train: 2018-08-09T11:13:07.815458: step 4140, loss 0.66082.
Test: 2018-08-09T11:13:08.316710: step 4140, loss 0.550281.
Train: 2018-08-09T11:13:08.394840: step 4141, loss 0.5143.
Train: 2018-08-09T11:13:08.472952: step 4142, loss 0.563054.
Train: 2018-08-09T11:13:08.551030: step 4143, loss 0.467005.
Train: 2018-08-09T11:13:08.629165: step 4144, loss 0.482613.
Train: 2018-08-09T11:13:08.707275: step 4145, loss 0.546772.
Train: 2018-08-09T11:13:08.801022: step 4146, loss 0.594966.
Train: 2018-08-09T11:13:08.879106: step 4147, loss 0.562905.
Train: 2018-08-09T11:13:08.957183: step 4148, loss 0.642901.
Train: 2018-08-09T11:13:09.035321: step 4149, loss 0.546777.
Train: 2018-08-09T11:13:09.113426: step 4150, loss 0.643346.
Test: 2018-08-09T11:13:09.597658: step 4150, loss 0.548406.
Train: 2018-08-09T11:13:09.691417: step 4151, loss 0.562802.
Train: 2018-08-09T11:13:09.769521: step 4152, loss 0.675018.
Train: 2018-08-09T11:13:09.847629: step 4153, loss 0.562889.
Train: 2018-08-09T11:13:09.927967: step 4154, loss 0.48329.
Train: 2018-08-09T11:13:10.006103: step 4155, loss 0.547117.
Train: 2018-08-09T11:13:10.084208: step 4156, loss 0.562924.
Train: 2018-08-09T11:13:10.162316: step 4157, loss 0.499382.
Train: 2018-08-09T11:13:10.240422: step 4158, loss 0.578871.
Train: 2018-08-09T11:13:10.318528: step 4159, loss 0.562902.
Train: 2018-08-09T11:13:10.396635: step 4160, loss 0.610685.
Test: 2018-08-09T11:13:10.896488: step 4160, loss 0.55101.
Train: 2018-08-09T11:13:10.974624: step 4161, loss 0.690361.
Train: 2018-08-09T11:13:11.052731: step 4162, loss 0.515207.
Train: 2018-08-09T11:13:11.130840: step 4163, loss 0.562942.
Train: 2018-08-09T11:13:11.208947: step 4164, loss 0.578894.
Train: 2018-08-09T11:13:11.287051: step 4165, loss 0.594654.
Train: 2018-08-09T11:13:11.380749: step 4166, loss 0.547232.
Train: 2018-08-09T11:13:11.458889: step 4167, loss 0.562764.
Train: 2018-08-09T11:13:11.536961: step 4168, loss 0.562974.
Train: 2018-08-09T11:13:11.615099: step 4169, loss 0.467905.
Train: 2018-08-09T11:13:11.693174: step 4170, loss 0.499462.
Test: 2018-08-09T11:13:12.193628: step 4170, loss 0.549733.
Train: 2018-08-09T11:13:12.271739: step 4171, loss 0.530591.
Train: 2018-08-09T11:13:12.349846: step 4172, loss 0.627946.
Train: 2018-08-09T11:13:12.427923: step 4173, loss 0.465186.
Train: 2018-08-09T11:13:12.506028: step 4174, loss 0.545368.
Train: 2018-08-09T11:13:12.584166: step 4175, loss 0.561484.
Train: 2018-08-09T11:13:12.662268: step 4176, loss 0.594308.
Train: 2018-08-09T11:13:12.740381: step 4177, loss 0.581534.
Train: 2018-08-09T11:13:12.818456: step 4178, loss 0.595528.
Train: 2018-08-09T11:13:12.912214: step 4179, loss 0.682719.
Train: 2018-08-09T11:13:12.990322: step 4180, loss 0.581034.
Test: 2018-08-09T11:13:13.474552: step 4180, loss 0.54752.
Train: 2018-08-09T11:13:13.552659: step 4181, loss 0.445429.
Train: 2018-08-09T11:13:13.630789: step 4182, loss 0.511731.
Train: 2018-08-09T11:13:13.724538: step 4183, loss 0.56456.
Train: 2018-08-09T11:13:13.802599: step 4184, loss 0.59691.
Train: 2018-08-09T11:13:13.880735: step 4185, loss 0.579741.
Train: 2018-08-09T11:13:13.961117: step 4186, loss 0.595517.
Train: 2018-08-09T11:13:14.039193: step 4187, loss 0.660894.
Train: 2018-08-09T11:13:14.117299: step 4188, loss 0.530396.
Train: 2018-08-09T11:13:14.195408: step 4189, loss 0.611516.
Train: 2018-08-09T11:13:14.273542: step 4190, loss 0.514346.
Test: 2018-08-09T11:13:14.773395: step 4190, loss 0.550265.
Train: 2018-08-09T11:13:14.851531: step 4191, loss 0.562651.
Train: 2018-08-09T11:13:14.929632: step 4192, loss 0.6427.
Train: 2018-08-09T11:13:15.007715: step 4193, loss 0.499418.
Train: 2018-08-09T11:13:15.085851: step 4194, loss 0.563125.
Train: 2018-08-09T11:13:15.163957: step 4195, loss 0.626335.
Train: 2018-08-09T11:13:15.257695: step 4196, loss 0.547204.
Train: 2018-08-09T11:13:15.335792: step 4197, loss 0.483637.
Train: 2018-08-09T11:13:15.413898: step 4198, loss 0.563004.
Train: 2018-08-09T11:13:15.492008: step 4199, loss 0.626334.
Train: 2018-08-09T11:13:15.570112: step 4200, loss 0.515417.
Test: 2018-08-09T11:13:16.056757: step 4200, loss 0.549251.
Train: 2018-08-09T11:13:16.634777: step 4201, loss 0.578882.
Train: 2018-08-09T11:13:16.712852: step 4202, loss 0.451449.
Train: 2018-08-09T11:13:16.790959: step 4203, loss 0.5312.
Train: 2018-08-09T11:13:16.884715: step 4204, loss 0.547063.
Train: 2018-08-09T11:13:16.962793: step 4205, loss 0.51463.
Train: 2018-08-09T11:13:17.040899: step 4206, loss 0.43339.
Train: 2018-08-09T11:13:17.119036: step 4207, loss 0.611834.
Train: 2018-08-09T11:13:17.197112: step 4208, loss 0.52933.
Train: 2018-08-09T11:13:17.275252: step 4209, loss 0.562326.
Train: 2018-08-09T11:13:17.353356: step 4210, loss 0.530938.
Test: 2018-08-09T11:13:17.859967: step 4210, loss 0.548584.
Train: 2018-08-09T11:13:17.932761: step 4211, loss 0.529328.
Train: 2018-08-09T11:13:18.010867: step 4212, loss 0.631424.
Train: 2018-08-09T11:13:18.088974: step 4213, loss 0.613644.
Train: 2018-08-09T11:13:18.167075: step 4214, loss 0.563645.
Train: 2018-08-09T11:13:18.260808: step 4215, loss 0.496034.
Train: 2018-08-09T11:13:18.338916: step 4216, loss 0.629261.
Train: 2018-08-09T11:13:18.416991: step 4217, loss 0.661861.
Train: 2018-08-09T11:13:18.495131: step 4218, loss 0.578842.
Train: 2018-08-09T11:13:18.573239: step 4219, loss 0.496533.
Train: 2018-08-09T11:13:18.651341: step 4220, loss 0.628535.
Test: 2018-08-09T11:13:19.151194: step 4220, loss 0.55209.
Train: 2018-08-09T11:13:19.229330: step 4221, loss 0.677668.
Train: 2018-08-09T11:13:19.307438: step 4222, loss 0.578946.
Train: 2018-08-09T11:13:19.385545: step 4223, loss 0.64434.
Train: 2018-08-09T11:13:19.463644: step 4224, loss 0.578917.
Train: 2018-08-09T11:13:19.541756: step 4225, loss 0.562695.
Train: 2018-08-09T11:13:19.635500: step 4226, loss 0.449728.
Train: 2018-08-09T11:13:19.713591: step 4227, loss 0.595003.
Train: 2018-08-09T11:13:19.791697: step 4228, loss 0.578875.
Train: 2018-08-09T11:13:19.869801: step 4229, loss 0.482464.
Train: 2018-08-09T11:13:19.947914: step 4230, loss 0.578871.
Test: 2018-08-09T11:13:20.440763: step 4230, loss 0.552014.
Train: 2018-08-09T11:13:20.518869: step 4231, loss 0.498573.
Train: 2018-08-09T11:13:20.596976: step 4232, loss 0.562799.
Train: 2018-08-09T11:13:20.675114: step 4233, loss 0.562788.
Train: 2018-08-09T11:13:20.768835: step 4234, loss 0.578876.
Train: 2018-08-09T11:13:20.846947: step 4235, loss 0.530562.
Train: 2018-08-09T11:13:20.918801: step 4236, loss 0.643364.
Train: 2018-08-09T11:13:20.996938: step 4237, loss 0.611104.
Train: 2018-08-09T11:13:21.075014: step 4238, loss 0.434012.
Train: 2018-08-09T11:13:21.153123: step 4239, loss 0.530533.
Train: 2018-08-09T11:13:21.231259: step 4240, loss 0.465879.
Test: 2018-08-09T11:13:21.731136: step 4240, loss 0.548132.
Train: 2018-08-09T11:13:21.809242: step 4241, loss 0.497863.
Train: 2018-08-09T11:13:21.887323: step 4242, loss 0.59526.
Train: 2018-08-09T11:13:21.965433: step 4243, loss 0.693153.
Train: 2018-08-09T11:13:22.059188: step 4244, loss 0.69333.
Train: 2018-08-09T11:13:22.137295: step 4245, loss 0.644075.
Train: 2018-08-09T11:13:22.215372: step 4246, loss 0.562675.
Train: 2018-08-09T11:13:22.293508: step 4247, loss 0.546499.
Train: 2018-08-09T11:13:22.371615: step 4248, loss 0.530387.
Train: 2018-08-09T11:13:22.449693: step 4249, loss 0.627286.
Train: 2018-08-09T11:13:22.527828: step 4250, loss 0.466181.
Test: 2018-08-09T11:13:23.027705: step 4250, loss 0.54891.
Train: 2018-08-09T11:13:23.105812: step 4251, loss 0.450099.
Train: 2018-08-09T11:13:23.183923: step 4252, loss 0.675718.
Train: 2018-08-09T11:13:23.277648: step 4253, loss 0.64342.
Train: 2018-08-09T11:13:23.355728: step 4254, loss 0.546664.
Train: 2018-08-09T11:13:23.433864: step 4255, loss 0.627149.
Train: 2018-08-09T11:13:23.511972: step 4256, loss 0.578872.
Train: 2018-08-09T11:13:23.590048: step 4257, loss 0.466707.
Train: 2018-08-09T11:13:23.668153: step 4258, loss 0.46673.
Train: 2018-08-09T11:13:23.746292: step 4259, loss 0.482602.
Train: 2018-08-09T11:13:23.824368: step 4260, loss 0.562792.
Test: 2018-08-09T11:13:24.325572: step 4260, loss 0.549461.
Train: 2018-08-09T11:13:24.403707: step 4261, loss 0.627283.
Train: 2018-08-09T11:13:24.481783: step 4262, loss 0.56274.
Train: 2018-08-09T11:13:24.559891: step 4263, loss 0.643589.
Train: 2018-08-09T11:13:24.637997: step 4264, loss 0.530393.
Train: 2018-08-09T11:13:24.716130: step 4265, loss 0.611238.
Train: 2018-08-09T11:13:24.809861: step 4266, loss 0.433421.
Train: 2018-08-09T11:13:24.887968: step 4267, loss 0.562699.
Train: 2018-08-09T11:13:24.966077: step 4268, loss 0.546457.
Train: 2018-08-09T11:13:25.044182: step 4269, loss 0.692626.
Train: 2018-08-09T11:13:25.122257: step 4270, loss 0.513966.
Test: 2018-08-09T11:13:25.609100: step 4270, loss 0.550519.
Train: 2018-08-09T11:13:25.702859: step 4271, loss 0.611382.
Train: 2018-08-09T11:13:25.780966: step 4272, loss 0.595141.
Train: 2018-08-09T11:13:25.859043: step 4273, loss 0.546452.
Train: 2018-08-09T11:13:25.935846: step 4274, loss 0.530271.
Train: 2018-08-09T11:13:26.013919: step 4275, loss 0.578917.
Train: 2018-08-09T11:13:26.092053: step 4276, loss 0.514024.
Train: 2018-08-09T11:13:26.170162: step 4277, loss 0.562678.
Train: 2018-08-09T11:13:26.248269: step 4278, loss 0.578948.
Train: 2018-08-09T11:13:26.341991: step 4279, loss 0.57882.
Train: 2018-08-09T11:13:26.420103: step 4280, loss 0.611545.
Test: 2018-08-09T11:13:26.919956: step 4280, loss 0.551142.
Train: 2018-08-09T11:13:26.998086: step 4281, loss 0.49789.
Train: 2018-08-09T11:13:27.076169: step 4282, loss 0.578875.
Train: 2018-08-09T11:13:27.154276: step 4283, loss 0.562701.
Train: 2018-08-09T11:13:27.232406: step 4284, loss 0.562911.
Train: 2018-08-09T11:13:27.310488: step 4285, loss 0.530102.
Train: 2018-08-09T11:13:27.388621: step 4286, loss 0.530164.
Train: 2018-08-09T11:13:27.466732: step 4287, loss 0.562746.
Train: 2018-08-09T11:13:27.544838: step 4288, loss 0.546413.
Train: 2018-08-09T11:13:27.638568: step 4289, loss 0.513729.
Train: 2018-08-09T11:13:27.716673: step 4290, loss 0.513665.
Test: 2018-08-09T11:13:28.216525: step 4290, loss 0.550335.
Train: 2018-08-09T11:13:28.294686: step 4291, loss 0.710029.
Train: 2018-08-09T11:13:28.372768: step 4292, loss 0.529887.
Train: 2018-08-09T11:13:28.450845: step 4293, loss 0.464439.
Train: 2018-08-09T11:13:28.528951: step 4294, loss 0.578956.
Train: 2018-08-09T11:13:28.607057: step 4295, loss 0.546139.
Train: 2018-08-09T11:13:28.685166: step 4296, loss 0.51322.
Train: 2018-08-09T11:13:28.763270: step 4297, loss 0.628408.
Train: 2018-08-09T11:13:28.841408: step 4298, loss 0.579023.
Train: 2018-08-09T11:13:28.919486: step 4299, loss 0.546017.
Train: 2018-08-09T11:13:28.997621: step 4300, loss 0.579015.
Test: 2018-08-09T11:13:29.497473: step 4300, loss 0.549529.
Train: 2018-08-09T11:13:30.028628: step 4301, loss 0.496463.
Train: 2018-08-09T11:13:30.137975: step 4302, loss 0.529432.
Train: 2018-08-09T11:13:30.216083: step 4303, loss 0.496249.
Train: 2018-08-09T11:13:30.294191: step 4304, loss 0.612311.
Train: 2018-08-09T11:13:30.372296: step 4305, loss 0.612349.
Train: 2018-08-09T11:13:30.450400: step 4306, loss 0.495965.
Train: 2018-08-09T11:13:30.544102: step 4307, loss 0.595735.
Train: 2018-08-09T11:13:30.622238: step 4308, loss 0.479183.
Train: 2018-08-09T11:13:30.700345: step 4309, loss 0.545749.
Train: 2018-08-09T11:13:30.778422: step 4310, loss 0.528998.
Test: 2018-08-09T11:13:31.262683: step 4310, loss 0.54925.
Train: 2018-08-09T11:13:31.387653: step 4311, loss 0.52892.
Train: 2018-08-09T11:13:31.465789: step 4312, loss 0.545612.
Train: 2018-08-09T11:13:31.543895: step 4313, loss 0.64646.
Train: 2018-08-09T11:13:31.622002: step 4314, loss 0.545555.
Train: 2018-08-09T11:13:31.700109: step 4315, loss 0.562377.
Train: 2018-08-09T11:13:31.778223: step 4316, loss 0.646592.
Train: 2018-08-09T11:13:31.856322: step 4317, loss 0.646515.
Train: 2018-08-09T11:13:31.934433: step 4318, loss 0.646321.
Train: 2018-08-09T11:13:32.012507: step 4319, loss 0.595865.
Train: 2018-08-09T11:13:32.090611: step 4320, loss 0.545766.
Test: 2018-08-09T11:13:32.606116: step 4320, loss 0.548775.
Train: 2018-08-09T11:13:32.684256: step 4321, loss 0.512624.
Train: 2018-08-09T11:13:32.762358: step 4322, loss 0.429865.
Train: 2018-08-09T11:13:32.840467: step 4323, loss 0.579057.
Train: 2018-08-09T11:13:32.921930: step 4324, loss 0.545889.
Train: 2018-08-09T11:13:33.000069: step 4325, loss 0.661947.
Train: 2018-08-09T11:13:33.078171: step 4326, loss 0.595604.
Train: 2018-08-09T11:13:33.156281: step 4327, loss 0.612028.
Train: 2018-08-09T11:13:33.234387: step 4328, loss 0.628428.
Train: 2018-08-09T11:13:33.312495: step 4329, loss 0.562553.
Train: 2018-08-09T11:13:33.390602: step 4330, loss 0.562563.
Test: 2018-08-09T11:13:33.890452: step 4330, loss 0.550418.
Train: 2018-08-09T11:13:33.968589: step 4331, loss 0.546313.
Train: 2018-08-09T11:13:34.046665: step 4332, loss 0.464991.
Train: 2018-08-09T11:13:34.124801: step 4333, loss 0.643991.
Train: 2018-08-09T11:13:34.202908: step 4334, loss 0.530216.
Train: 2018-08-09T11:13:34.281016: step 4335, loss 0.513986.
Train: 2018-08-09T11:13:34.374714: step 4336, loss 0.61137.
Train: 2018-08-09T11:13:34.452820: step 4337, loss 0.530305.
Train: 2018-08-09T11:13:34.530956: step 4338, loss 0.595066.
Train: 2018-08-09T11:13:34.609066: step 4339, loss 0.595143.
Train: 2018-08-09T11:13:34.687140: step 4340, loss 0.611206.
Test: 2018-08-09T11:13:35.187893: step 4340, loss 0.551285.
Train: 2018-08-09T11:13:35.266027: step 4341, loss 0.627298.
Train: 2018-08-09T11:13:35.344136: step 4342, loss 0.562768.
Train: 2018-08-09T11:13:35.422218: step 4343, loss 0.514669.
Train: 2018-08-09T11:13:35.500317: step 4344, loss 0.67506.
Train: 2018-08-09T11:13:35.578424: step 4345, loss 0.498955.
Train: 2018-08-09T11:13:35.656562: step 4346, loss 0.483121.
Train: 2018-08-09T11:13:35.750260: step 4347, loss 0.546948.
Train: 2018-08-09T11:13:35.828402: step 4348, loss 0.514998.
Train: 2018-08-09T11:13:35.906483: step 4349, loss 0.498929.
Train: 2018-08-09T11:13:35.984578: step 4350, loss 0.562831.
Test: 2018-08-09T11:13:36.484461: step 4350, loss 0.550788.
Train: 2018-08-09T11:13:36.546971: step 4351, loss 0.627024.
Train: 2018-08-09T11:13:36.640704: step 4352, loss 0.498509.
Train: 2018-08-09T11:13:36.718814: step 4353, loss 0.514413.
Train: 2018-08-09T11:13:36.796887: step 4354, loss 0.627689.
Train: 2018-08-09T11:13:36.874993: step 4355, loss 0.48173.
Train: 2018-08-09T11:13:36.953103: step 4356, loss 0.595136.
Train: 2018-08-09T11:13:37.031237: step 4357, loss 0.513983.
Train: 2018-08-09T11:13:37.109344: step 4358, loss 0.562668.
Train: 2018-08-09T11:13:37.187420: step 4359, loss 0.530036.
Train: 2018-08-09T11:13:37.281179: step 4360, loss 0.595303.
Test: 2018-08-09T11:13:37.765409: step 4360, loss 0.549709.
Train: 2018-08-09T11:13:37.843548: step 4361, loss 0.54622.
Train: 2018-08-09T11:13:37.923997: step 4362, loss 0.529767.
Train: 2018-08-09T11:13:38.002104: step 4363, loss 0.529659.
Train: 2018-08-09T11:13:38.095831: step 4364, loss 0.496621.
Train: 2018-08-09T11:13:38.173944: step 4365, loss 0.612064.
Train: 2018-08-09T11:13:38.252046: step 4366, loss 0.529338.
Train: 2018-08-09T11:13:38.330160: step 4367, loss 0.628868.
Train: 2018-08-09T11:13:38.408263: step 4368, loss 0.479335.
Train: 2018-08-09T11:13:38.486341: step 4369, loss 0.545825.
Train: 2018-08-09T11:13:38.564446: step 4370, loss 0.495629.
Test: 2018-08-09T11:13:39.059537: step 4370, loss 0.549872.
Train: 2018-08-09T11:13:39.137675: step 4371, loss 0.629471.
Train: 2018-08-09T11:13:39.231397: step 4372, loss 0.57912.
Train: 2018-08-09T11:13:39.309510: step 4373, loss 0.612702.
Train: 2018-08-09T11:13:39.387586: step 4374, loss 0.612759.
Train: 2018-08-09T11:13:39.465722: step 4375, loss 0.612651.
Train: 2018-08-09T11:13:39.543831: step 4376, loss 0.545648.
Train: 2018-08-09T11:13:39.621935: step 4377, loss 0.4956.
Train: 2018-08-09T11:13:39.700042: step 4378, loss 0.512183.
Train: 2018-08-09T11:13:39.778121: step 4379, loss 0.478944.
Train: 2018-08-09T11:13:39.871870: step 4380, loss 0.529225.
Test: 2018-08-09T11:13:40.356132: step 4380, loss 0.54598.
Train: 2018-08-09T11:13:40.434243: step 4381, loss 0.646575.
Train: 2018-08-09T11:13:40.527971: step 4382, loss 0.545641.
Train: 2018-08-09T11:13:40.606048: step 4383, loss 0.73042.
Train: 2018-08-09T11:13:40.684156: step 4384, loss 0.562397.
Train: 2018-08-09T11:13:40.762293: step 4385, loss 0.595785.
Train: 2018-08-09T11:13:40.840368: step 4386, loss 0.545807.
Train: 2018-08-09T11:13:40.918504: step 4387, loss 0.595658.
Train: 2018-08-09T11:13:40.996581: step 4388, loss 0.496214.
Train: 2018-08-09T11:13:41.074689: step 4389, loss 0.595587.
Train: 2018-08-09T11:13:41.168446: step 4390, loss 0.57903.
Test: 2018-08-09T11:13:41.652707: step 4390, loss 0.547694.
Train: 2018-08-09T11:13:41.730813: step 4391, loss 0.661417.
Train: 2018-08-09T11:13:41.808890: step 4392, loss 0.628252.
Train: 2018-08-09T11:13:41.902649: step 4393, loss 0.562587.
Train: 2018-08-09T11:13:41.967437: step 4394, loss 0.66041.
Train: 2018-08-09T11:13:42.045559: step 4395, loss 0.578901.
Train: 2018-08-09T11:13:42.139301: step 4396, loss 0.562751.
Train: 2018-08-09T11:13:42.217408: step 4397, loss 0.627072.
Train: 2018-08-09T11:13:42.295488: step 4398, loss 0.546891.
Train: 2018-08-09T11:13:42.373591: step 4399, loss 0.610721.
Train: 2018-08-09T11:13:42.451729: step 4400, loss 0.436148.
Test: 2018-08-09T11:13:42.951580: step 4400, loss 0.547595.
Train: 2018-08-09T11:13:43.482734: step 4401, loss 0.594695.
Train: 2018-08-09T11:13:43.560844: step 4402, loss 0.578865.
Train: 2018-08-09T11:13:43.654542: step 4403, loss 0.436912.
Train: 2018-08-09T11:13:43.732648: step 4404, loss 0.531516.
Train: 2018-08-09T11:13:43.810757: step 4405, loss 0.468209.
Train: 2018-08-09T11:13:43.888861: step 4406, loss 0.515454.
Train: 2018-08-09T11:13:43.967644: step 4407, loss 0.67441.
Train: 2018-08-09T11:13:44.045750: step 4408, loss 0.5468.
Train: 2018-08-09T11:13:44.123853: step 4409, loss 0.611025.
Train: 2018-08-09T11:13:44.201963: step 4410, loss 0.530814.
Test: 2018-08-09T11:13:44.701815: step 4410, loss 0.550283.
Train: 2018-08-09T11:13:44.779920: step 4411, loss 0.451081.
Train: 2018-08-09T11:13:44.858058: step 4412, loss 0.562765.
Train: 2018-08-09T11:13:44.936165: step 4413, loss 0.514609.
Train: 2018-08-09T11:13:45.014240: step 4414, loss 0.545902.
Train: 2018-08-09T11:13:45.092377: step 4415, loss 0.531391.
Train: 2018-08-09T11:13:45.170455: step 4416, loss 0.529432.
Train: 2018-08-09T11:13:45.264181: step 4417, loss 0.562833.
Train: 2018-08-09T11:13:45.342318: step 4418, loss 0.578892.
Train: 2018-08-09T11:13:45.420396: step 4419, loss 0.463756.
Train: 2018-08-09T11:13:45.498526: step 4420, loss 0.578601.
Test: 2018-08-09T11:13:45.982763: step 4420, loss 0.550513.
Train: 2018-08-09T11:13:46.060869: step 4421, loss 0.579553.
Train: 2018-08-09T11:13:46.138975: step 4422, loss 0.629785.
Train: 2018-08-09T11:13:46.232735: step 4423, loss 0.529154.
Train: 2018-08-09T11:13:46.310840: step 4424, loss 0.511522.
Train: 2018-08-09T11:13:46.388918: step 4425, loss 0.528846.
Train: 2018-08-09T11:13:46.467050: step 4426, loss 0.527922.
Train: 2018-08-09T11:13:46.545161: step 4427, loss 0.578944.
Train: 2018-08-09T11:13:46.623267: step 4428, loss 0.596437.
Train: 2018-08-09T11:13:46.701346: step 4429, loss 0.562893.
Train: 2018-08-09T11:13:46.779481: step 4430, loss 0.595585.
Test: 2018-08-09T11:13:47.280760: step 4430, loss 0.548071.
Train: 2018-08-09T11:13:47.358897: step 4431, loss 0.613752.
Train: 2018-08-09T11:13:47.437006: step 4432, loss 0.562336.
Train: 2018-08-09T11:13:47.515111: step 4433, loss 0.526944.
Train: 2018-08-09T11:13:47.593216: step 4434, loss 0.579566.
Train: 2018-08-09T11:13:47.686914: step 4435, loss 0.580517.
Train: 2018-08-09T11:13:47.765046: step 4436, loss 0.477884.
Train: 2018-08-09T11:13:47.843157: step 4437, loss 0.544676.
Train: 2018-08-09T11:13:47.921265: step 4438, loss 0.699405.
Train: 2018-08-09T11:13:47.999372: step 4439, loss 0.511813.
Train: 2018-08-09T11:13:48.077448: step 4440, loss 0.544822.
Test: 2018-08-09T11:13:48.571376: step 4440, loss 0.547994.
Train: 2018-08-09T11:13:48.649512: step 4441, loss 0.562896.
Train: 2018-08-09T11:13:48.727618: step 4442, loss 0.646557.
Train: 2018-08-09T11:13:48.805696: step 4443, loss 0.611519.
Train: 2018-08-09T11:13:48.899422: step 4444, loss 0.644885.
Train: 2018-08-09T11:13:48.977560: step 4445, loss 0.759832.
Train: 2018-08-09T11:13:49.055666: step 4446, loss 0.51388.
Train: 2018-08-09T11:13:49.133777: step 4447, loss 0.498013.
Train: 2018-08-09T11:13:49.211848: step 4448, loss 0.562768.
Train: 2018-08-09T11:13:49.289956: step 4449, loss 0.498585.
Train: 2018-08-09T11:13:49.368093: step 4450, loss 0.482708.
Test: 2018-08-09T11:13:49.867970: step 4450, loss 0.549043.
Train: 2018-08-09T11:13:49.946051: step 4451, loss 0.626929.
Train: 2018-08-09T11:13:50.024187: step 4452, loss 0.498857.
Train: 2018-08-09T11:13:50.102290: step 4453, loss 0.562863.
Train: 2018-08-09T11:13:50.196017: step 4454, loss 0.562865.
Train: 2018-08-09T11:13:50.274099: step 4455, loss 0.498837.
Train: 2018-08-09T11:13:50.352206: step 4456, loss 0.562833.
Train: 2018-08-09T11:13:50.430342: step 4457, loss 0.578837.
Train: 2018-08-09T11:13:50.508448: step 4458, loss 0.514531.
Train: 2018-08-09T11:13:50.586540: step 4459, loss 0.562503.
Train: 2018-08-09T11:13:50.664631: step 4460, loss 0.514089.
Test: 2018-08-09T11:13:51.167384: step 4460, loss 0.549138.
Train: 2018-08-09T11:13:51.245515: step 4461, loss 0.514172.
Train: 2018-08-09T11:13:51.323596: step 4462, loss 0.494627.
Train: 2018-08-09T11:13:51.401705: step 4463, loss 0.5969.
Train: 2018-08-09T11:13:51.479844: step 4464, loss 0.611793.
Train: 2018-08-09T11:13:51.557942: step 4465, loss 0.59836.
Train: 2018-08-09T11:13:51.636049: step 4466, loss 0.595873.
Train: 2018-08-09T11:13:51.725641: step 4467, loss 0.626337.
Train: 2018-08-09T11:13:51.803772: step 4468, loss 0.511521.
Train: 2018-08-09T11:13:51.881882: step 4469, loss 0.463251.
Train: 2018-08-09T11:13:51.959990: step 4470, loss 0.597922.
Test: 2018-08-09T11:13:52.459867: step 4470, loss 0.547258.
Train: 2018-08-09T11:13:52.537949: step 4471, loss 0.54441.
Train: 2018-08-09T11:13:52.616085: step 4472, loss 0.596919.
Train: 2018-08-09T11:13:52.694164: step 4473, loss 0.562282.
Train: 2018-08-09T11:13:52.772302: step 4474, loss 0.528842.
Train: 2018-08-09T11:13:52.866022: step 4475, loss 0.577825.
Train: 2018-08-09T11:13:52.933836: step 4476, loss 0.545073.
Train: 2018-08-09T11:13:53.011951: step 4477, loss 0.595654.
Train: 2018-08-09T11:13:53.090055: step 4478, loss 0.646873.
Train: 2018-08-09T11:13:53.183753: step 4479, loss 0.610793.
Train: 2018-08-09T11:13:53.261889: step 4480, loss 0.447473.
Test: 2018-08-09T11:13:53.746145: step 4480, loss 0.548944.
Train: 2018-08-09T11:13:53.824259: step 4481, loss 0.530656.
Train: 2018-08-09T11:13:53.917978: step 4482, loss 0.547234.
Train: 2018-08-09T11:13:53.996092: step 4483, loss 0.59423.
Train: 2018-08-09T11:13:54.074167: step 4484, loss 0.513724.
Train: 2018-08-09T11:13:54.152304: step 4485, loss 0.594792.
Train: 2018-08-09T11:13:54.230412: step 4486, loss 0.595185.
Train: 2018-08-09T11:13:54.308488: step 4487, loss 0.5461.
Train: 2018-08-09T11:13:54.386595: step 4488, loss 0.661423.
Train: 2018-08-09T11:13:54.464732: step 4489, loss 0.497509.
Train: 2018-08-09T11:13:54.558458: step 4490, loss 0.52999.
Test: 2018-08-09T11:13:55.042689: step 4490, loss 0.548654.
Train: 2018-08-09T11:13:55.120796: step 4491, loss 0.562924.
Train: 2018-08-09T11:13:55.214555: step 4492, loss 0.513795.
Train: 2018-08-09T11:13:55.292663: step 4493, loss 0.594863.
Train: 2018-08-09T11:13:55.370767: step 4494, loss 0.578619.
Train: 2018-08-09T11:13:55.448874: step 4495, loss 0.513299.
Train: 2018-08-09T11:13:55.526978: step 4496, loss 0.480495.
Train: 2018-08-09T11:13:55.605087: step 4497, loss 0.595645.
Train: 2018-08-09T11:13:55.683194: step 4498, loss 0.514041.
Train: 2018-08-09T11:13:55.761287: step 4499, loss 0.56348.
Train: 2018-08-09T11:13:55.839407: step 4500, loss 0.546071.
Test: 2018-08-09T11:13:56.348251: step 4500, loss 0.547499.
Train: 2018-08-09T11:13:56.941893: step 4501, loss 0.662399.
Train: 2018-08-09T11:13:57.019998: step 4502, loss 0.512817.
Train: 2018-08-09T11:13:57.098106: step 4503, loss 0.496877.
Train: 2018-08-09T11:13:57.176183: step 4504, loss 0.661794.
Train: 2018-08-09T11:13:57.254317: step 4505, loss 0.594652.
Train: 2018-08-09T11:13:57.348049: step 4506, loss 0.627523.
Train: 2018-08-09T11:13:57.426153: step 4507, loss 0.578888.
Train: 2018-08-09T11:13:57.504259: step 4508, loss 0.530204.
Train: 2018-08-09T11:13:57.582367: step 4509, loss 0.513473.
Train: 2018-08-09T11:13:57.660472: step 4510, loss 0.513577.
Test: 2018-08-09T11:13:58.153272: step 4510, loss 0.548854.
Train: 2018-08-09T11:13:58.246978: step 4511, loss 0.577508.
Train: 2018-08-09T11:13:58.325081: step 4512, loss 0.628647.
Train: 2018-08-09T11:13:58.403188: step 4513, loss 0.462565.
Train: 2018-08-09T11:13:58.481325: step 4514, loss 0.529591.
Train: 2018-08-09T11:13:58.559433: step 4515, loss 0.563131.
Train: 2018-08-09T11:13:58.637534: step 4516, loss 0.498107.
Train: 2018-08-09T11:13:58.715640: step 4517, loss 0.596128.
Train: 2018-08-09T11:13:58.793750: step 4518, loss 0.49746.
Train: 2018-08-09T11:13:58.871828: step 4519, loss 0.644606.
Train: 2018-08-09T11:13:58.949933: step 4520, loss 0.613012.
Test: 2018-08-09T11:13:59.449842: step 4520, loss 0.547475.
Train: 2018-08-09T11:13:59.527948: step 4521, loss 0.594229.
Train: 2018-08-09T11:13:59.606060: step 4522, loss 0.479771.
Train: 2018-08-09T11:13:59.684168: step 4523, loss 0.480311.
Train: 2018-08-09T11:13:59.762272: step 4524, loss 0.512668.
Train: 2018-08-09T11:13:59.840349: step 4525, loss 0.563074.
Train: 2018-08-09T11:13:59.918457: step 4526, loss 0.513992.
Train: 2018-08-09T11:14:00.012187: step 4527, loss 0.495375.
Train: 2018-08-09T11:14:00.090320: step 4528, loss 0.497182.
Train: 2018-08-09T11:14:00.184021: step 4529, loss 0.561715.
Train: 2018-08-09T11:14:00.262126: step 4530, loss 0.56219.
Test: 2018-08-09T11:14:00.762033: step 4530, loss 0.544484.
Train: 2018-08-09T11:14:00.840140: step 4531, loss 0.562312.
Train: 2018-08-09T11:14:00.920517: step 4532, loss 0.494479.
Train: 2018-08-09T11:14:00.998625: step 4533, loss 0.561173.
Train: 2018-08-09T11:14:01.076764: step 4534, loss 0.578499.
Train: 2018-08-09T11:14:01.154839: step 4535, loss 0.525132.
Train: 2018-08-09T11:14:01.248566: step 4536, loss 0.489274.
Train: 2018-08-09T11:14:01.326673: step 4537, loss 0.52402.
Train: 2018-08-09T11:14:01.404810: step 4538, loss 0.662247.
Train: 2018-08-09T11:14:01.482887: step 4539, loss 0.65835.
Train: 2018-08-09T11:14:01.560992: step 4540, loss 0.662117.
Test: 2018-08-09T11:14:02.060876: step 4540, loss 0.549138.
Train: 2018-08-09T11:14:02.139013: step 4541, loss 0.562088.
Train: 2018-08-09T11:14:02.217088: step 4542, loss 0.599282.
Train: 2018-08-09T11:14:02.295228: step 4543, loss 0.49338.
Train: 2018-08-09T11:14:02.373300: step 4544, loss 0.578239.
Train: 2018-08-09T11:14:02.451439: step 4545, loss 0.541047.
Train: 2018-08-09T11:14:02.529545: step 4546, loss 0.580388.
Train: 2018-08-09T11:14:02.607651: step 4547, loss 0.560619.
Train: 2018-08-09T11:14:02.701349: step 4548, loss 0.580274.
Train: 2018-08-09T11:14:02.779486: step 4549, loss 0.563203.
Train: 2018-08-09T11:14:02.857563: step 4550, loss 0.529549.
Test: 2018-08-09T11:14:03.342619: step 4550, loss 0.545475.
Train: 2018-08-09T11:14:03.420756: step 4551, loss 0.62838.
Train: 2018-08-09T11:14:03.514484: step 4552, loss 0.596055.
Train: 2018-08-09T11:14:03.592590: step 4553, loss 0.528619.
Train: 2018-08-09T11:14:03.670696: step 4554, loss 0.613107.
Train: 2018-08-09T11:14:03.748804: step 4555, loss 0.531127.
Train: 2018-08-09T11:14:03.826883: step 4556, loss 0.594918.
Train: 2018-08-09T11:14:03.904987: step 4557, loss 0.56298.
Train: 2018-08-09T11:14:03.983124: step 4558, loss 0.562826.
Train: 2018-08-09T11:14:04.061231: step 4559, loss 0.514426.
Train: 2018-08-09T11:14:04.154953: step 4560, loss 0.48273.
Test: 2018-08-09T11:14:04.639219: step 4560, loss 0.549605.
Train: 2018-08-09T11:14:04.732947: step 4561, loss 0.643027.
Train: 2018-08-09T11:14:04.811052: step 4562, loss 0.562829.
Train: 2018-08-09T11:14:04.889161: step 4563, loss 0.62683.
Train: 2018-08-09T11:14:04.967266: step 4564, loss 0.674728.
Train: 2018-08-09T11:14:05.045373: step 4565, loss 0.547023.
Train: 2018-08-09T11:14:05.139101: step 4566, loss 0.610596.
Train: 2018-08-09T11:14:05.217178: step 4567, loss 0.563029.
Train: 2018-08-09T11:14:05.295315: step 4568, loss 0.563079.
Train: 2018-08-09T11:14:05.373422: step 4569, loss 0.578868.
Train: 2018-08-09T11:14:05.467151: step 4570, loss 0.673137.
Test: 2018-08-09T11:14:05.953753: step 4570, loss 0.549752.
Train: 2018-08-09T11:14:06.047451: step 4571, loss 0.53193.
Train: 2018-08-09T11:14:06.125591: step 4572, loss 0.485261.
Train: 2018-08-09T11:14:06.203690: step 4573, loss 0.578896.
Train: 2018-08-09T11:14:06.281770: step 4574, loss 0.563311.
Train: 2018-08-09T11:14:06.359878: step 4575, loss 0.547768.
Train: 2018-08-09T11:14:06.438014: step 4576, loss 0.672329.
Train: 2018-08-09T11:14:06.516120: step 4577, loss 0.625541.
Train: 2018-08-09T11:14:06.609848: step 4578, loss 0.625411.
Train: 2018-08-09T11:14:06.687955: step 4579, loss 0.594383.
Train: 2018-08-09T11:14:06.766061: step 4580, loss 0.578971.
Test: 2018-08-09T11:14:07.265938: step 4580, loss 0.551003.
Train: 2018-08-09T11:14:07.344020: step 4581, loss 0.578967.
Train: 2018-08-09T11:14:07.422157: step 4582, loss 0.594245.
Train: 2018-08-09T11:14:07.500260: step 4583, loss 0.578942.
Train: 2018-08-09T11:14:07.578364: step 4584, loss 0.548753.
Train: 2018-08-09T11:14:07.672068: step 4585, loss 0.503315.
Train: 2018-08-09T11:14:07.750208: step 4586, loss 0.654848.
Train: 2018-08-09T11:14:07.828281: step 4587, loss 0.563911.
Train: 2018-08-09T11:14:07.908789: step 4588, loss 0.533715.
Train: 2018-08-09T11:14:07.986898: step 4589, loss 0.518629.
Train: 2018-08-09T11:14:08.065003: step 4590, loss 0.488182.
Test: 2018-08-09T11:14:08.564886: step 4590, loss 0.552464.
Train: 2018-08-09T11:14:08.643022: step 4591, loss 0.594194.
Train: 2018-08-09T11:14:08.721130: step 4592, loss 0.609687.
Train: 2018-08-09T11:14:08.799236: step 4593, loss 0.609551.
Train: 2018-08-09T11:14:08.877342: step 4594, loss 0.517986.
Train: 2018-08-09T11:14:08.955450: step 4595, loss 0.456665.
Train: 2018-08-09T11:14:09.033556: step 4596, loss 0.502228.
Train: 2018-08-09T11:14:09.111662: step 4597, loss 0.671345.
Train: 2018-08-09T11:14:09.205360: step 4598, loss 0.547653.
Train: 2018-08-09T11:14:09.283467: step 4599, loss 0.469585.
Train: 2018-08-09T11:14:09.361603: step 4600, loss 0.544993.
Test: 2018-08-09T11:14:09.861456: step 4600, loss 0.551343.
Train: 2018-08-09T11:14:10.460121: step 4601, loss 0.614577.
Train: 2018-08-09T11:14:10.538259: step 4602, loss 0.596694.
Train: 2018-08-09T11:14:10.616360: step 4603, loss 0.547135.
Train: 2018-08-09T11:14:10.694470: step 4604, loss 0.546117.
Train: 2018-08-09T11:14:10.772571: step 4605, loss 0.500398.
Train: 2018-08-09T11:14:10.866276: step 4606, loss 0.562575.
Train: 2018-08-09T11:14:10.944406: step 4607, loss 0.629498.
Train: 2018-08-09T11:14:11.022513: step 4608, loss 0.596566.
Train: 2018-08-09T11:14:11.100597: step 4609, loss 0.691227.
Train: 2018-08-09T11:14:11.178700: step 4610, loss 0.466918.
Test: 2018-08-09T11:14:11.678616: step 4610, loss 0.549811.
Train: 2018-08-09T11:14:11.756714: step 4611, loss 0.467311.
Train: 2018-08-09T11:14:11.834826: step 4612, loss 0.579.
Train: 2018-08-09T11:14:11.912936: step 4613, loss 0.562867.
Train: 2018-08-09T11:14:11.991040: step 4614, loss 0.626795.
Train: 2018-08-09T11:14:12.069116: step 4615, loss 0.466982.
Train: 2018-08-09T11:14:12.147251: step 4616, loss 0.482728.
Train: 2018-08-09T11:14:12.240950: step 4617, loss 0.562738.
Train: 2018-08-09T11:14:12.319057: step 4618, loss 0.498153.
Train: 2018-08-09T11:14:12.397192: step 4619, loss 0.595183.
Train: 2018-08-09T11:14:12.475301: step 4620, loss 0.595202.
Test: 2018-08-09T11:14:12.975154: step 4620, loss 0.54733.
Train: 2018-08-09T11:14:13.053284: step 4621, loss 0.546418.
Train: 2018-08-09T11:14:13.131396: step 4622, loss 0.627975.
Train: 2018-08-09T11:14:13.209474: step 4623, loss 0.628269.
Train: 2018-08-09T11:14:13.287611: step 4624, loss 0.627963.
Train: 2018-08-09T11:14:13.365716: step 4625, loss 0.513757.
Train: 2018-08-09T11:14:13.459445: step 4626, loss 0.546318.
Train: 2018-08-09T11:14:13.537552: step 4627, loss 0.578958.
Train: 2018-08-09T11:14:13.615657: step 4628, loss 0.611444.
Train: 2018-08-09T11:14:13.693765: step 4629, loss 0.57891.
Train: 2018-08-09T11:14:13.771840: step 4630, loss 0.530228.
Test: 2018-08-09T11:14:14.273117: step 4630, loss 0.547491.
Train: 2018-08-09T11:14:14.351222: step 4631, loss 0.595116.
Train: 2018-08-09T11:14:14.429305: step 4632, loss 0.692289.
Train: 2018-08-09T11:14:14.507412: step 4633, loss 0.514289.
Train: 2018-08-09T11:14:14.585544: step 4634, loss 0.514406.
Train: 2018-08-09T11:14:14.663654: step 4635, loss 0.51446.
Train: 2018-08-09T11:14:14.741732: step 4636, loss 0.675509.
Train: 2018-08-09T11:14:14.819867: step 4637, loss 0.594949.
Train: 2018-08-09T11:14:14.897973: step 4638, loss 0.594911.
Train: 2018-08-09T11:14:14.991674: step 4639, loss 0.562858.
Train: 2018-08-09T11:14:15.069809: step 4640, loss 0.594832.
Test: 2018-08-09T11:14:15.554040: step 4640, loss 0.549805.
Train: 2018-08-09T11:14:15.632176: step 4641, loss 0.56293.
Train: 2018-08-09T11:14:15.725904: step 4642, loss 0.515258.
Train: 2018-08-09T11:14:15.803982: step 4643, loss 0.515311.
Train: 2018-08-09T11:14:15.882117: step 4644, loss 0.531192.
Train: 2018-08-09T11:14:15.962638: step 4645, loss 0.531158.
Train: 2018-08-09T11:14:16.040746: step 4646, loss 0.610705.
Train: 2018-08-09T11:14:16.118822: step 4647, loss 0.531071.
Train: 2018-08-09T11:14:16.196958: step 4648, loss 0.49913.
Train: 2018-08-09T11:14:16.275073: step 4649, loss 0.546899.
Train: 2018-08-09T11:14:16.353141: step 4650, loss 0.466719.
Test: 2018-08-09T11:14:16.853024: step 4650, loss 0.55015.
Train: 2018-08-09T11:14:16.946781: step 4651, loss 0.498455.
Train: 2018-08-09T11:14:17.024857: step 4652, loss 0.611183.
Train: 2018-08-09T11:14:17.102965: step 4653, loss 0.562702.
Train: 2018-08-09T11:14:17.181103: step 4654, loss 0.497558.
Train: 2018-08-09T11:14:17.259207: step 4655, loss 0.546301.
Train: 2018-08-09T11:14:17.337285: step 4656, loss 0.595371.
Train: 2018-08-09T11:14:17.415420: step 4657, loss 0.545797.
Train: 2018-08-09T11:14:17.493527: step 4658, loss 0.513045.
Train: 2018-08-09T11:14:17.571635: step 4659, loss 0.595812.
Train: 2018-08-09T11:14:17.665331: step 4660, loss 0.529112.
Test: 2018-08-09T11:14:18.151919: step 4660, loss 0.547832.
Train: 2018-08-09T11:14:18.230013: step 4661, loss 0.610741.
Train: 2018-08-09T11:14:18.308120: step 4662, loss 0.528844.
Train: 2018-08-09T11:14:18.401879: step 4663, loss 0.584172.
Train: 2018-08-09T11:14:18.479955: step 4664, loss 0.561909.
Train: 2018-08-09T11:14:18.558092: step 4665, loss 0.497307.
Train: 2018-08-09T11:14:18.636199: step 4666, loss 0.580294.
Train: 2018-08-09T11:14:18.714275: step 4667, loss 0.630337.
Train: 2018-08-09T11:14:18.792382: step 4668, loss 0.561683.
Train: 2018-08-09T11:14:18.870519: step 4669, loss 0.52905.
Train: 2018-08-09T11:14:18.948625: step 4670, loss 0.595909.
Test: 2018-08-09T11:14:19.453132: step 4670, loss 0.549308.
Train: 2018-08-09T11:14:19.531269: step 4671, loss 0.529006.
Train: 2018-08-09T11:14:19.609344: step 4672, loss 0.645764.
Train: 2018-08-09T11:14:19.687476: step 4673, loss 0.479227.
Train: 2018-08-09T11:14:19.765596: step 4674, loss 0.495882.
Train: 2018-08-09T11:14:19.859285: step 4675, loss 0.512397.
Train: 2018-08-09T11:14:19.934115: step 4676, loss 0.579003.
Train: 2018-08-09T11:14:20.012191: step 4677, loss 0.545766.
Train: 2018-08-09T11:14:20.090297: step 4678, loss 0.64594.
Train: 2018-08-09T11:14:20.168405: step 4679, loss 0.5959.
Train: 2018-08-09T11:14:20.246510: step 4680, loss 0.546151.
Test: 2018-08-09T11:14:20.746394: step 4680, loss 0.550482.
Train: 2018-08-09T11:14:20.824529: step 4681, loss 0.478639.
Train: 2018-08-09T11:14:20.902635: step 4682, loss 0.578589.
Train: 2018-08-09T11:14:20.980747: step 4683, loss 0.562549.
Train: 2018-08-09T11:14:21.058818: step 4684, loss 0.528964.
Train: 2018-08-09T11:14:21.152546: step 4685, loss 0.579407.
Train: 2018-08-09T11:14:21.230654: step 4686, loss 0.595583.
Train: 2018-08-09T11:14:21.308790: step 4687, loss 0.528027.
Train: 2018-08-09T11:14:21.386896: step 4688, loss 0.512473.
Train: 2018-08-09T11:14:21.465002: step 4689, loss 0.579228.
Train: 2018-08-09T11:14:21.543080: step 4690, loss 0.629668.
Test: 2018-08-09T11:14:22.034000: step 4690, loss 0.549084.
Train: 2018-08-09T11:14:22.112106: step 4691, loss 0.596846.
Train: 2018-08-09T11:14:22.190243: step 4692, loss 0.544992.
Train: 2018-08-09T11:14:22.283940: step 4693, loss 0.579406.
Train: 2018-08-09T11:14:22.362078: step 4694, loss 0.578829.
Train: 2018-08-09T11:14:22.440184: step 4695, loss 0.629469.
Train: 2018-08-09T11:14:22.518291: step 4696, loss 0.612218.
Train: 2018-08-09T11:14:22.596368: step 4697, loss 0.562555.
Train: 2018-08-09T11:14:22.674474: step 4698, loss 0.513378.
Train: 2018-08-09T11:14:22.752582: step 4699, loss 0.628064.
Train: 2018-08-09T11:14:22.830687: step 4700, loss 0.546302.
Test: 2018-08-09T11:14:23.340846: step 4700, loss 0.546154.
Train: 2018-08-09T11:14:23.903211: step 4701, loss 0.5138.
Train: 2018-08-09T11:14:23.981352: step 4702, loss 0.53014.
Train: 2018-08-09T11:14:24.059451: step 4703, loss 0.497634.
Train: 2018-08-09T11:14:24.137560: step 4704, loss 0.595248.
Train: 2018-08-09T11:14:24.215668: step 4705, loss 0.725357.
Train: 2018-08-09T11:14:24.309414: step 4706, loss 0.514023.
Train: 2018-08-09T11:14:24.387473: step 4707, loss 0.546523.
Train: 2018-08-09T11:14:24.465605: step 4708, loss 0.643551.
Train: 2018-08-09T11:14:24.543716: step 4709, loss 0.530498.
Train: 2018-08-09T11:14:24.621823: step 4710, loss 0.466168.
Test: 2018-08-09T11:14:25.121675: step 4710, loss 0.548289.
Train: 2018-08-09T11:14:25.199812: step 4711, loss 0.578879.
Train: 2018-08-09T11:14:25.277922: step 4712, loss 0.594983.
Train: 2018-08-09T11:14:25.356026: step 4713, loss 0.417892.
Train: 2018-08-09T11:14:25.434101: step 4714, loss 0.578882.
Train: 2018-08-09T11:14:25.512239: step 4715, loss 0.595047.
Train: 2018-08-09T11:14:25.605967: step 4716, loss 0.498018.
Train: 2018-08-09T11:14:25.684043: step 4717, loss 0.514075.
Train: 2018-08-09T11:14:25.762180: step 4718, loss 0.562661.
Train: 2018-08-09T11:14:25.840256: step 4719, loss 0.578925.
Train: 2018-08-09T11:14:25.919782: step 4720, loss 0.578931.
Test: 2018-08-09T11:14:26.419666: step 4720, loss 0.547906.
Train: 2018-08-09T11:14:26.497802: step 4721, loss 0.546259.
Train: 2018-08-09T11:14:26.575909: step 4722, loss 0.448024.
Train: 2018-08-09T11:14:26.654017: step 4723, loss 0.611807.
Train: 2018-08-09T11:14:26.732115: step 4724, loss 0.562534.
Train: 2018-08-09T11:14:26.810197: step 4725, loss 0.595477.
Train: 2018-08-09T11:14:26.888331: step 4726, loss 0.546003.
Train: 2018-08-09T11:14:26.977046: step 4727, loss 0.579.
Train: 2018-08-09T11:14:27.055138: step 4728, loss 0.595495.
Train: 2018-08-09T11:14:27.133244: step 4729, loss 0.545908.
Train: 2018-08-09T11:14:27.211353: step 4730, loss 0.579026.
Test: 2018-08-09T11:14:27.711203: step 4730, loss 0.548311.
Train: 2018-08-09T11:14:27.789340: step 4731, loss 0.644867.
Train: 2018-08-09T11:14:27.867416: step 4732, loss 0.661192.
Train: 2018-08-09T11:14:27.952894: step 4733, loss 0.482661.
Train: 2018-08-09T11:14:28.031003: step 4734, loss 0.644809.
Train: 2018-08-09T11:14:28.109079: step 4735, loss 0.465235.
Train: 2018-08-09T11:14:28.187214: step 4736, loss 0.611484.
Train: 2018-08-09T11:14:28.265325: step 4737, loss 0.399049.
Train: 2018-08-09T11:14:28.343428: step 4738, loss 0.693783.
Train: 2018-08-09T11:14:28.421538: step 4739, loss 0.611742.
Train: 2018-08-09T11:14:28.515234: step 4740, loss 0.611667.
Test: 2018-08-09T11:14:29.015117: step 4740, loss 0.547928.
Train: 2018-08-09T11:14:29.093252: step 4741, loss 0.578936.
Train: 2018-08-09T11:14:29.171329: step 4742, loss 0.497506.
Train: 2018-08-09T11:14:29.249466: step 4743, loss 0.595178.
Train: 2018-08-09T11:14:29.327542: step 4744, loss 0.611398.
Train: 2018-08-09T11:14:29.405678: step 4745, loss 0.514073.
Train: 2018-08-09T11:14:29.483786: step 4746, loss 0.481728.
Train: 2018-08-09T11:14:29.561861: step 4747, loss 0.514076.
Train: 2018-08-09T11:14:29.640001: step 4748, loss 0.643805.
Train: 2018-08-09T11:14:29.733727: step 4749, loss 0.562667.
Train: 2018-08-09T11:14:29.811802: step 4750, loss 0.546503.
Test: 2018-08-09T11:14:30.296065: step 4750, loss 0.548097.
Train: 2018-08-09T11:14:30.374200: step 4751, loss 0.432926.
Train: 2018-08-09T11:14:30.452279: step 4752, loss 0.513859.
Train: 2018-08-09T11:14:30.546038: step 4753, loss 0.595246.
Train: 2018-08-09T11:14:30.624142: step 4754, loss 0.5952.
Train: 2018-08-09T11:14:30.702217: step 4755, loss 0.562542.
Train: 2018-08-09T11:14:30.780356: step 4756, loss 0.611643.
Train: 2018-08-09T11:14:30.858462: step 4757, loss 0.562634.
Train: 2018-08-09T11:14:30.938851: step 4758, loss 0.562555.
Train: 2018-08-09T11:14:31.016956: step 4759, loss 0.660796.
Train: 2018-08-09T11:14:31.095063: step 4760, loss 0.644469.
Test: 2018-08-09T11:14:31.594940: step 4760, loss 0.548504.
Train: 2018-08-09T11:14:31.673051: step 4761, loss 0.677065.
Train: 2018-08-09T11:14:31.751153: step 4762, loss 0.578919.
Train: 2018-08-09T11:14:31.844883: step 4763, loss 0.51396.
Train: 2018-08-09T11:14:31.922963: step 4764, loss 0.562614.
Train: 2018-08-09T11:14:32.001100: step 4765, loss 0.562684.
Train: 2018-08-09T11:14:32.079202: step 4766, loss 0.482389.
Train: 2018-08-09T11:14:32.157307: step 4767, loss 0.482165.
Train: 2018-08-09T11:14:32.235419: step 4768, loss 0.417593.
Train: 2018-08-09T11:14:32.313496: step 4769, loss 0.611659.
Train: 2018-08-09T11:14:32.391636: step 4770, loss 0.496131.
Test: 2018-08-09T11:14:32.891486: step 4770, loss 0.550191.
Train: 2018-08-09T11:14:32.970994: step 4771, loss 0.495739.
Train: 2018-08-09T11:14:33.049107: step 4772, loss 0.5323.
Train: 2018-08-09T11:14:33.127184: step 4773, loss 0.530635.
Train: 2018-08-09T11:14:33.205320: step 4774, loss 0.617688.
Train: 2018-08-09T11:14:33.283397: step 4775, loss 0.527841.
Train: 2018-08-09T11:14:33.361538: step 4776, loss 0.612389.
Train: 2018-08-09T11:14:33.455261: step 4777, loss 0.528182.
Train: 2018-08-09T11:14:33.533368: step 4778, loss 0.631186.
Train: 2018-08-09T11:14:33.611478: step 4779, loss 0.544501.
Train: 2018-08-09T11:14:33.689581: step 4780, loss 0.594645.
Test: 2018-08-09T11:14:34.189433: step 4780, loss 0.547916.
Train: 2018-08-09T11:14:34.267569: step 4781, loss 0.578908.
Train: 2018-08-09T11:14:34.345677: step 4782, loss 0.578247.
Train: 2018-08-09T11:14:34.423785: step 4783, loss 0.545019.
Train: 2018-08-09T11:14:34.501890: step 4784, loss 0.578509.
Train: 2018-08-09T11:14:34.579967: step 4785, loss 0.579332.
Train: 2018-08-09T11:14:34.658073: step 4786, loss 0.561157.
Train: 2018-08-09T11:14:34.736180: step 4787, loss 0.612528.
Train: 2018-08-09T11:14:34.829937: step 4788, loss 0.644027.
Train: 2018-08-09T11:14:34.910368: step 4789, loss 0.594721.
Train: 2018-08-09T11:14:34.988501: step 4790, loss 0.529487.
Test: 2018-08-09T11:14:35.488358: step 4790, loss 0.548883.
Train: 2018-08-09T11:14:35.566463: step 4791, loss 0.547166.
Train: 2018-08-09T11:14:35.644600: step 4792, loss 0.595513.
Train: 2018-08-09T11:14:35.722681: step 4793, loss 0.579469.
Train: 2018-08-09T11:14:35.800784: step 4794, loss 0.495548.
Train: 2018-08-09T11:14:35.878890: step 4795, loss 0.59529.
Train: 2018-08-09T11:14:35.957031: step 4796, loss 0.496626.
Train: 2018-08-09T11:14:36.035128: step 4797, loss 0.545382.
Train: 2018-08-09T11:14:36.113253: step 4798, loss 0.612277.
Train: 2018-08-09T11:14:36.206944: step 4799, loss 0.561064.
Train: 2018-08-09T11:14:36.285076: step 4800, loss 0.610902.
Test: 2018-08-09T11:14:36.769305: step 4800, loss 0.549595.
Train: 2018-08-09T11:14:37.302061: step 4801, loss 0.580513.
Train: 2018-08-09T11:14:37.380167: step 4802, loss 0.577482.
Train: 2018-08-09T11:14:37.458305: step 4803, loss 0.629246.
Train: 2018-08-09T11:14:37.536414: step 4804, loss 0.578997.
Train: 2018-08-09T11:14:37.614518: step 4805, loss 0.531639.
Train: 2018-08-09T11:14:37.708246: step 4806, loss 0.496251.
Train: 2018-08-09T11:14:37.786353: step 4807, loss 0.482014.
Train: 2018-08-09T11:14:37.864428: step 4808, loss 0.498555.
Train: 2018-08-09T11:14:37.942566: step 4809, loss 0.529626.
Train: 2018-08-09T11:14:38.020672: step 4810, loss 0.460981.
Test: 2018-08-09T11:14:38.520525: step 4810, loss 0.546041.
Train: 2018-08-09T11:14:38.598661: step 4811, loss 0.532927.
Train: 2018-08-09T11:14:38.676772: step 4812, loss 0.628225.
Train: 2018-08-09T11:14:38.754845: step 4813, loss 0.541158.
Train: 2018-08-09T11:14:38.832950: step 4814, loss 0.493447.
Train: 2018-08-09T11:14:38.912445: step 4815, loss 0.533023.
Train: 2018-08-09T11:14:39.006199: step 4816, loss 0.559468.
Train: 2018-08-09T11:14:39.084310: step 4817, loss 0.687512.
Train: 2018-08-09T11:14:39.162417: step 4818, loss 0.614966.
Train: 2018-08-09T11:14:39.240495: step 4819, loss 0.600311.
Train: 2018-08-09T11:14:39.318600: step 4820, loss 0.543973.
Test: 2018-08-09T11:14:39.818506: step 4820, loss 0.549399.
Train: 2018-08-09T11:14:39.896618: step 4821, loss 0.557952.
Train: 2018-08-09T11:14:39.974725: step 4822, loss 0.565266.
Train: 2018-08-09T11:14:40.052832: step 4823, loss 0.595944.
Train: 2018-08-09T11:14:40.130939: step 4824, loss 0.550439.
Train: 2018-08-09T11:14:40.209016: step 4825, loss 0.583859.
Train: 2018-08-09T11:14:40.302774: step 4826, loss 0.595507.
Train: 2018-08-09T11:14:40.380849: step 4827, loss 0.547228.
Train: 2018-08-09T11:14:40.458955: step 4828, loss 0.563506.
Train: 2018-08-09T11:14:40.537094: step 4829, loss 0.657937.
Train: 2018-08-09T11:14:40.615194: step 4830, loss 0.56283.
Test: 2018-08-09T11:14:41.115052: step 4830, loss 0.551281.
Train: 2018-08-09T11:14:41.193189: step 4831, loss 0.610662.
Train: 2018-08-09T11:14:41.266267: step 4832, loss 0.547397.
Train: 2018-08-09T11:14:41.344378: step 4833, loss 0.594585.
Train: 2018-08-09T11:14:41.438106: step 4834, loss 0.610153.
Train: 2018-08-09T11:14:41.516213: step 4835, loss 0.594468.
Train: 2018-08-09T11:14:41.594291: step 4836, loss 0.594434.
Train: 2018-08-09T11:14:41.672426: step 4837, loss 0.532521.
Train: 2018-08-09T11:14:41.750502: step 4838, loss 0.54806.
Train: 2018-08-09T11:14:41.828611: step 4839, loss 0.594358.
Train: 2018-08-09T11:14:41.906742: step 4840, loss 0.548181.
Test: 2018-08-09T11:14:42.406625: step 4840, loss 0.552074.
Train: 2018-08-09T11:14:42.484729: step 4841, loss 0.563592.
Train: 2018-08-09T11:14:42.562841: step 4842, loss 0.517493.
Train: 2018-08-09T11:14:42.640944: step 4843, loss 0.578949.
Train: 2018-08-09T11:14:42.719055: step 4844, loss 0.54812.
Train: 2018-08-09T11:14:42.812785: step 4845, loss 0.548202.
Train: 2018-08-09T11:14:42.890889: step 4846, loss 0.594236.
Train: 2018-08-09T11:14:42.960643: step 4847, loss 0.516729.
Train: 2018-08-09T11:14:43.038719: step 4848, loss 0.500652.
Train: 2018-08-09T11:14:43.132480: step 4849, loss 0.59584.
Train: 2018-08-09T11:14:43.210584: step 4850, loss 0.577496.
Test: 2018-08-09T11:14:43.694839: step 4850, loss 0.547943.
Train: 2018-08-09T11:14:43.772951: step 4851, loss 0.550194.
Train: 2018-08-09T11:14:43.866648: step 4852, loss 0.674049.
Train: 2018-08-09T11:14:43.944781: step 4853, loss 0.578212.
Train: 2018-08-09T11:14:44.022887: step 4854, loss 0.498293.
Train: 2018-08-09T11:14:44.101000: step 4855, loss 0.531837.
Train: 2018-08-09T11:14:44.179077: step 4856, loss 0.515219.
Train: 2018-08-09T11:14:44.257212: step 4857, loss 0.562811.
Train: 2018-08-09T11:14:44.335290: step 4858, loss 0.51556.
Train: 2018-08-09T11:14:44.413429: step 4859, loss 0.612638.
Train: 2018-08-09T11:14:44.507124: step 4860, loss 0.577962.
Test: 2018-08-09T11:14:44.991415: step 4860, loss 0.549956.
Train: 2018-08-09T11:14:45.085137: step 4861, loss 0.49568.
Train: 2018-08-09T11:14:45.163220: step 4862, loss 0.659196.
Train: 2018-08-09T11:14:45.241327: step 4863, loss 0.599251.
Train: 2018-08-09T11:14:45.319432: step 4864, loss 0.661925.
Train: 2018-08-09T11:14:45.397567: step 4865, loss 0.498627.
Train: 2018-08-09T11:14:45.475671: step 4866, loss 0.546995.
Train: 2018-08-09T11:14:45.569409: step 4867, loss 0.529395.
Train: 2018-08-09T11:14:45.647512: step 4868, loss 0.562717.
Train: 2018-08-09T11:14:45.725612: step 4869, loss 0.516703.
Train: 2018-08-09T11:14:45.803723: step 4870, loss 0.611479.
Test: 2018-08-09T11:14:46.290362: step 4870, loss 0.551527.
Train: 2018-08-09T11:14:46.384120: step 4871, loss 0.563049.
Train: 2018-08-09T11:14:46.462226: step 4872, loss 0.515179.
Train: 2018-08-09T11:14:46.540334: step 4873, loss 0.658393.
Train: 2018-08-09T11:14:46.618440: step 4874, loss 0.54704.
Train: 2018-08-09T11:14:46.696548: step 4875, loss 0.642756.
Train: 2018-08-09T11:14:46.774653: step 4876, loss 0.515229.
Train: 2018-08-09T11:14:46.852759: step 4877, loss 0.562905.
Train: 2018-08-09T11:14:46.930835: step 4878, loss 0.531274.
Train: 2018-08-09T11:14:47.008973: step 4879, loss 0.547133.
Train: 2018-08-09T11:14:47.087050: step 4880, loss 0.53127.
Test: 2018-08-09T11:14:47.586933: step 4880, loss 0.54928.
Train: 2018-08-09T11:14:47.665068: step 4881, loss 0.562934.
Train: 2018-08-09T11:14:47.743174: step 4882, loss 0.547009.
Train: 2018-08-09T11:14:47.821277: step 4883, loss 0.59482.
Train: 2018-08-09T11:14:47.916425: step 4884, loss 0.531.
Train: 2018-08-09T11:14:47.994566: step 4885, loss 0.546865.
Train: 2018-08-09T11:14:48.072640: step 4886, loss 0.57885.
Train: 2018-08-09T11:14:48.150776: step 4887, loss 0.59486.
Train: 2018-08-09T11:14:48.228852: step 4888, loss 0.498643.
Train: 2018-08-09T11:14:48.306985: step 4889, loss 0.466323.
Train: 2018-08-09T11:14:48.385064: step 4890, loss 0.546618.
Test: 2018-08-09T11:14:48.884947: step 4890, loss 0.549346.
Train: 2018-08-09T11:14:48.963055: step 4891, loss 0.497876.
Train: 2018-08-09T11:14:49.041160: step 4892, loss 0.497376.
Train: 2018-08-09T11:14:49.134888: step 4893, loss 0.612128.
Train: 2018-08-09T11:14:49.213025: step 4894, loss 0.529629.
Train: 2018-08-09T11:14:49.291132: step 4895, loss 0.62913.
Train: 2018-08-09T11:14:49.369243: step 4896, loss 0.529007.
Train: 2018-08-09T11:14:49.447345: step 4897, loss 0.562495.
Train: 2018-08-09T11:14:49.525452: step 4898, loss 0.612201.
Train: 2018-08-09T11:14:49.603530: step 4899, loss 0.495557.
Train: 2018-08-09T11:14:49.681665: step 4900, loss 0.496153.
Test: 2018-08-09T11:14:50.192365: step 4900, loss 0.548544.
Train: 2018-08-09T11:14:50.770907: step 4901, loss 0.646469.
Train: 2018-08-09T11:14:50.849014: step 4902, loss 0.61257.
Train: 2018-08-09T11:14:50.927152: step 4903, loss 0.562909.
Train: 2018-08-09T11:14:51.020880: step 4904, loss 0.512161.
Train: 2018-08-09T11:14:51.098955: step 4905, loss 0.629095.
Train: 2018-08-09T11:14:51.177063: step 4906, loss 0.629463.
Train: 2018-08-09T11:14:51.255198: step 4907, loss 0.495876.
Train: 2018-08-09T11:14:51.333276: step 4908, loss 0.512605.
Train: 2018-08-09T11:14:51.411392: step 4909, loss 0.56245.
Train: 2018-08-09T11:14:51.489518: step 4910, loss 0.562479.
Test: 2018-08-09T11:14:51.996758: step 4910, loss 0.548772.
Train: 2018-08-09T11:14:52.074833: step 4911, loss 0.529225.
Train: 2018-08-09T11:14:52.152940: step 4912, loss 0.645548.
Train: 2018-08-09T11:14:52.231048: step 4913, loss 0.545853.
Train: 2018-08-09T11:14:52.309155: step 4914, loss 0.512691.
Train: 2018-08-09T11:14:52.387291: step 4915, loss 0.579053.
Train: 2018-08-09T11:14:52.480989: step 4916, loss 0.59565.
Train: 2018-08-09T11:14:52.555547: step 4917, loss 0.529316.
Train: 2018-08-09T11:14:52.633649: step 4918, loss 0.545894.
Train: 2018-08-09T11:14:52.711726: step 4919, loss 0.51275.
Train: 2018-08-09T11:14:52.789863: step 4920, loss 0.628813.
Test: 2018-08-09T11:14:53.289714: step 4920, loss 0.549443.
Train: 2018-08-09T11:14:53.367853: step 4921, loss 0.463003.
Train: 2018-08-09T11:14:53.445957: step 4922, loss 0.579058.
Train: 2018-08-09T11:14:53.524068: step 4923, loss 0.595669.
Train: 2018-08-09T11:14:53.617799: step 4924, loss 0.446192.
Train: 2018-08-09T11:14:53.695869: step 4925, loss 0.529159.
Train: 2018-08-09T11:14:53.774009: step 4926, loss 0.495717.
Train: 2018-08-09T11:14:53.852112: step 4927, loss 0.595848.
Train: 2018-08-09T11:14:53.930188: step 4928, loss 0.461803.
Train: 2018-08-09T11:14:54.008326: step 4929, loss 0.545506.
Train: 2018-08-09T11:14:54.086427: step 4930, loss 0.646904.
Test: 2018-08-09T11:14:54.586285: step 4930, loss 0.545867.
Train: 2018-08-09T11:14:54.664391: step 4931, loss 0.579243.
Train: 2018-08-09T11:14:54.742528: step 4932, loss 0.579259.
Train: 2018-08-09T11:14:54.836226: step 4933, loss 0.596238.
Train: 2018-08-09T11:14:54.915738: step 4934, loss 0.49456.
Train: 2018-08-09T11:14:54.993876: step 4935, loss 0.579371.
Train: 2018-08-09T11:14:55.071984: step 4936, loss 0.562324.
Train: 2018-08-09T11:14:55.150059: step 4937, loss 0.528485.
Train: 2018-08-09T11:14:55.228196: step 4938, loss 0.494638.
Train: 2018-08-09T11:14:55.306300: step 4939, loss 0.562497.
Train: 2018-08-09T11:14:55.400001: step 4940, loss 0.579597.
Test: 2018-08-09T11:14:55.884261: step 4940, loss 0.550285.
Train: 2018-08-09T11:14:55.962367: step 4941, loss 0.545376.
Train: 2018-08-09T11:14:56.056127: step 4942, loss 0.579326.
Train: 2018-08-09T11:14:56.134233: step 4943, loss 0.579339.
Train: 2018-08-09T11:14:56.212339: step 4944, loss 0.47753.
Train: 2018-08-09T11:14:56.290444: step 4945, loss 0.511413.
Train: 2018-08-09T11:14:56.368554: step 4946, loss 0.5453.
Train: 2018-08-09T11:14:56.446629: step 4947, loss 0.51126.
Train: 2018-08-09T11:14:56.524764: step 4948, loss 0.596401.
Train: 2018-08-09T11:14:56.618472: step 4949, loss 0.528286.
Train: 2018-08-09T11:14:56.696599: step 4950, loss 0.665104.
Test: 2018-08-09T11:14:57.182232: step 4950, loss 0.546954.
Train: 2018-08-09T11:14:57.260370: step 4951, loss 0.64768.
Train: 2018-08-09T11:14:57.338476: step 4952, loss 0.545318.
Train: 2018-08-09T11:14:57.432206: step 4953, loss 0.647514.
Train: 2018-08-09T11:14:57.510310: step 4954, loss 0.613284.
Train: 2018-08-09T11:14:57.588417: step 4955, loss 0.748295.
Train: 2018-08-09T11:14:57.666494: step 4956, loss 0.512009.
Train: 2018-08-09T11:14:57.744601: step 4957, loss 0.478867.
Train: 2018-08-09T11:14:57.822706: step 4958, loss 0.645664.
Train: 2018-08-09T11:14:57.900485: step 4959, loss 0.529295.
Train: 2018-08-09T11:14:57.978622: step 4960, loss 0.562513.
Test: 2018-08-09T11:14:58.478483: step 4960, loss 0.551479.
Train: 2018-08-09T11:14:58.556582: step 4961, loss 0.546201.
Train: 2018-08-09T11:14:58.634688: step 4962, loss 0.546067.
Train: 2018-08-09T11:14:58.712826: step 4963, loss 0.611705.
Train: 2018-08-09T11:14:58.790933: step 4964, loss 0.562641.
Train: 2018-08-09T11:14:58.884628: step 4965, loss 0.595224.
Train: 2018-08-09T11:14:58.965015: step 4966, loss 0.595149.
Train: 2018-08-09T11:14:59.043121: step 4967, loss 0.659949.
Train: 2018-08-09T11:14:59.121198: step 4968, loss 0.611166.
Train: 2018-08-09T11:14:59.199305: step 4969, loss 0.514642.
Train: 2018-08-09T11:14:59.277412: step 4970, loss 0.610876.
Test: 2018-08-09T11:14:59.777293: step 4970, loss 0.551594.
Train: 2018-08-09T11:14:59.855430: step 4971, loss 0.546971.
Train: 2018-08-09T11:14:59.933506: step 4972, loss 0.515286.
Train: 2018-08-09T11:15:00.011645: step 4973, loss 0.57886.
Train: 2018-08-09T11:15:00.089751: step 4974, loss 0.531352.
Train: 2018-08-09T11:15:00.167857: step 4975, loss 0.578864.
Train: 2018-08-09T11:15:00.261585: step 4976, loss 0.578864.
Train: 2018-08-09T11:15:00.339691: step 4977, loss 0.594647.
Train: 2018-08-09T11:15:00.417798: step 4978, loss 0.531575.
Train: 2018-08-09T11:15:00.495875: step 4979, loss 0.484346.
Train: 2018-08-09T11:15:00.574012: step 4980, loss 0.626193.
Test: 2018-08-09T11:15:01.076224: step 4980, loss 0.551886.
Train: 2018-08-09T11:15:01.154334: step 4981, loss 0.657734.
Train: 2018-08-09T11:15:01.232442: step 4982, loss 0.641853.
Train: 2018-08-09T11:15:01.310520: step 4983, loss 0.500383.
Train: 2018-08-09T11:15:01.388654: step 4984, loss 0.594547.
Train: 2018-08-09T11:15:01.466762: step 4985, loss 0.578899.
Train: 2018-08-09T11:15:01.560489: step 4986, loss 0.594513.
Train: 2018-08-09T11:15:01.638565: step 4987, loss 0.563294.
Train: 2018-08-09T11:15:01.716704: step 4988, loss 0.625625.
Train: 2018-08-09T11:15:01.794780: step 4989, loss 0.516746.
Train: 2018-08-09T11:15:01.872916: step 4990, loss 0.470237.
Test: 2018-08-09T11:15:02.372767: step 4990, loss 0.549393.
Train: 2018-08-09T11:15:02.450904: step 4991, loss 0.547825.
Train: 2018-08-09T11:15:02.529011: step 4992, loss 0.578901.
Train: 2018-08-09T11:15:02.607088: step 4993, loss 0.578894.
Train: 2018-08-09T11:15:02.685224: step 4994, loss 0.688052.
Train: 2018-08-09T11:15:02.778946: step 4995, loss 0.547754.
Train: 2018-08-09T11:15:02.857028: step 4996, loss 0.532226.
Train: 2018-08-09T11:15:02.936610: step 4997, loss 0.563335.
Train: 2018-08-09T11:15:03.014722: step 4998, loss 0.547749.
Train: 2018-08-09T11:15:03.092840: step 4999, loss 0.625625.
Train: 2018-08-09T11:15:03.170935: step 5000, loss 0.578878.
Test: 2018-08-09T11:15:03.670813: step 5000, loss 0.548111.
Train: 2018-08-09T11:15:04.233154: step 5001, loss 0.532126.
Train: 2018-08-09T11:15:04.311261: step 5002, loss 0.531934.
Train: 2018-08-09T11:15:04.389399: step 5003, loss 0.579025.
Train: 2018-08-09T11:15:04.467501: step 5004, loss 0.610576.
Train: 2018-08-09T11:15:04.561227: step 5005, loss 0.563385.
Train: 2018-08-09T11:15:04.639310: step 5006, loss 0.578789.
Train: 2018-08-09T11:15:04.717417: step 5007, loss 0.531949.
Train: 2018-08-09T11:15:04.795553: step 5008, loss 0.579004.
Train: 2018-08-09T11:15:04.873661: step 5009, loss 0.51647.
Train: 2018-08-09T11:15:04.952455: step 5010, loss 0.610216.
Test: 2018-08-09T11:15:05.452306: step 5010, loss 0.549716.
Train: 2018-08-09T11:15:05.530414: step 5011, loss 0.610219.
Train: 2018-08-09T11:15:05.624140: step 5012, loss 0.610205.
Train: 2018-08-09T11:15:05.702279: step 5013, loss 0.453682.
Train: 2018-08-09T11:15:05.780378: step 5014, loss 0.672914.
Train: 2018-08-09T11:15:05.858490: step 5015, loss 0.53188.
Train: 2018-08-09T11:15:05.936593: step 5016, loss 0.59454.
Train: 2018-08-09T11:15:06.014674: step 5017, loss 0.437871.
Train: 2018-08-09T11:15:06.092779: step 5018, loss 0.547498.
Train: 2018-08-09T11:15:06.186542: step 5019, loss 0.547314.
Train: 2018-08-09T11:15:06.264645: step 5020, loss 0.499903.
Test: 2018-08-09T11:15:06.764549: step 5020, loss 0.550511.
Train: 2018-08-09T11:15:06.842604: step 5021, loss 0.531191.
Train: 2018-08-09T11:15:06.920740: step 5022, loss 0.56274.
Train: 2018-08-09T11:15:06.998819: step 5023, loss 0.57912.
Train: 2018-08-09T11:15:07.076954: step 5024, loss 0.547336.
Train: 2018-08-09T11:15:07.155060: step 5025, loss 0.563008.
Train: 2018-08-09T11:15:07.233168: step 5026, loss 0.612061.
Train: 2018-08-09T11:15:07.311273: step 5027, loss 0.498275.
Train: 2018-08-09T11:15:07.389380: step 5028, loss 0.514012.
Train: 2018-08-09T11:15:07.483109: step 5029, loss 0.530272.
Train: 2018-08-09T11:15:07.561214: step 5030, loss 0.546288.
Test: 2018-08-09T11:15:08.062376: step 5030, loss 0.548513.
Train: 2018-08-09T11:15:08.140513: step 5031, loss 0.48079.
Train: 2018-08-09T11:15:08.218589: step 5032, loss 0.513185.
Train: 2018-08-09T11:15:08.296697: step 5033, loss 0.446124.
Train: 2018-08-09T11:15:08.374803: step 5034, loss 0.613619.
Train: 2018-08-09T11:15:08.452909: step 5035, loss 0.494152.
Train: 2018-08-09T11:15:08.531048: step 5036, loss 0.768023.
Train: 2018-08-09T11:15:08.624745: step 5037, loss 0.663311.
Train: 2018-08-09T11:15:08.702884: step 5038, loss 0.561197.
Train: 2018-08-09T11:15:08.780958: step 5039, loss 0.613367.
Train: 2018-08-09T11:15:08.859064: step 5040, loss 0.59734.
Test: 2018-08-09T11:15:09.358953: step 5040, loss 0.54984.
Train: 2018-08-09T11:15:09.437082: step 5041, loss 0.495389.
Train: 2018-08-09T11:15:09.515190: step 5042, loss 0.528948.
Train: 2018-08-09T11:15:09.593307: step 5043, loss 0.495721.
Train: 2018-08-09T11:15:09.671403: step 5044, loss 0.595887.
Train: 2018-08-09T11:15:09.749479: step 5045, loss 0.479256.
Train: 2018-08-09T11:15:09.827616: step 5046, loss 0.529042.
Train: 2018-08-09T11:15:09.908116: step 5047, loss 0.445587.
Train: 2018-08-09T11:15:10.001844: step 5048, loss 0.529008.
Train: 2018-08-09T11:15:10.079982: step 5049, loss 0.629546.
Train: 2018-08-09T11:15:10.158057: step 5050, loss 0.545573.
Test: 2018-08-09T11:15:10.642338: step 5050, loss 0.547855.
Train: 2018-08-09T11:15:10.720455: step 5051, loss 0.444419.
Train: 2018-08-09T11:15:10.814152: step 5052, loss 0.528508.
Train: 2018-08-09T11:15:10.892295: step 5053, loss 0.579172.
Train: 2018-08-09T11:15:10.970397: step 5054, loss 0.494452.
Train: 2018-08-09T11:15:11.064124: step 5055, loss 0.544949.
Train: 2018-08-09T11:15:11.142231: step 5056, loss 0.579903.
Train: 2018-08-09T11:15:11.220338: step 5057, loss 0.562087.
Train: 2018-08-09T11:15:11.298445: step 5058, loss 0.42438.
Train: 2018-08-09T11:15:11.376549: step 5059, loss 0.596642.
Train: 2018-08-09T11:15:11.454653: step 5060, loss 0.632224.
Test: 2018-08-09T11:15:11.956855: step 5060, loss 0.549971.
Train: 2018-08-09T11:15:12.034992: step 5061, loss 0.562124.
Train: 2018-08-09T11:15:12.113097: step 5062, loss 0.544152.
Train: 2018-08-09T11:15:12.191176: step 5063, loss 0.528091.
Train: 2018-08-09T11:15:12.269282: step 5064, loss 0.544298.
Train: 2018-08-09T11:15:12.347421: step 5065, loss 0.597951.
Train: 2018-08-09T11:15:12.430270: step 5066, loss 0.4752.
Train: 2018-08-09T11:15:12.508373: step 5067, loss 0.526721.
Train: 2018-08-09T11:15:12.586480: step 5068, loss 0.491399.
Train: 2018-08-09T11:15:12.680209: step 5069, loss 0.561445.
Train: 2018-08-09T11:15:12.758285: step 5070, loss 0.579506.
Test: 2018-08-09T11:15:13.258167: step 5070, loss 0.547763.
Train: 2018-08-09T11:15:13.336273: step 5071, loss 0.49005.
Train: 2018-08-09T11:15:13.414381: step 5072, loss 0.584629.
Train: 2018-08-09T11:15:13.492521: step 5073, loss 0.561874.
Train: 2018-08-09T11:15:13.570624: step 5074, loss 0.600654.
Train: 2018-08-09T11:15:13.648701: step 5075, loss 0.601572.
Train: 2018-08-09T11:15:13.742461: step 5076, loss 0.527694.
Train: 2018-08-09T11:15:13.820534: step 5077, loss 0.544193.
Train: 2018-08-09T11:15:13.898672: step 5078, loss 0.631806.
Train: 2018-08-09T11:15:13.976749: step 5079, loss 0.579492.
Train: 2018-08-09T11:15:14.054885: step 5080, loss 0.579827.
Test: 2018-08-09T11:15:14.554754: step 5080, loss 0.548802.
Train: 2018-08-09T11:15:14.632878: step 5081, loss 0.613853.
Train: 2018-08-09T11:15:14.710950: step 5082, loss 0.63088.
Train: 2018-08-09T11:15:14.804680: step 5083, loss 0.494084.
Train: 2018-08-09T11:15:14.882810: step 5084, loss 0.494302.
Train: 2018-08-09T11:15:14.960922: step 5085, loss 0.494438.
Train: 2018-08-09T11:15:15.039029: step 5086, loss 0.545388.
Train: 2018-08-09T11:15:15.117134: step 5087, loss 0.528448.
Train: 2018-08-09T11:15:15.195241: step 5088, loss 0.562353.
Train: 2018-08-09T11:15:15.273344: step 5089, loss 0.562354.
Train: 2018-08-09T11:15:15.351454: step 5090, loss 0.630102.
Test: 2018-08-09T11:15:15.851307: step 5090, loss 0.547795.
Train: 2018-08-09T11:15:15.930754: step 5091, loss 0.51163.
Train: 2018-08-09T11:15:16.008835: step 5092, loss 0.613055.
Train: 2018-08-09T11:15:16.102595: step 5093, loss 0.579241.
Train: 2018-08-09T11:15:16.180669: step 5094, loss 0.579216.
Train: 2018-08-09T11:15:16.258807: step 5095, loss 0.545579.
Train: 2018-08-09T11:15:16.336914: step 5096, loss 0.562394.
Train: 2018-08-09T11:15:16.415020: step 5097, loss 0.595902.
Train: 2018-08-09T11:15:16.493127: step 5098, loss 0.595844.
Train: 2018-08-09T11:15:16.571234: step 5099, loss 0.595774.
Train: 2018-08-09T11:15:16.649311: step 5100, loss 0.545823.
Test: 2018-08-09T11:15:17.149193: step 5100, loss 0.54818.
Train: 2018-08-09T11:15:17.711560: step 5101, loss 0.54588.
Train: 2018-08-09T11:15:17.789665: step 5102, loss 0.529376.
Train: 2018-08-09T11:15:17.883424: step 5103, loss 0.579015.
Train: 2018-08-09T11:15:17.963896: step 5104, loss 0.529487.
Train: 2018-08-09T11:15:18.042004: step 5105, loss 0.496532.
Train: 2018-08-09T11:15:18.120139: step 5106, loss 0.579017.
Train: 2018-08-09T11:15:18.198247: step 5107, loss 0.562508.
Train: 2018-08-09T11:15:18.276323: step 5108, loss 0.694506.
Train: 2018-08-09T11:15:18.354464: step 5109, loss 0.578991.
Train: 2018-08-09T11:15:18.432567: step 5110, loss 0.546134.
Test: 2018-08-09T11:15:18.932420: step 5110, loss 0.549695.
Train: 2018-08-09T11:15:19.010525: step 5111, loss 0.578957.
Train: 2018-08-09T11:15:19.088662: step 5112, loss 0.56259.
Train: 2018-08-09T11:15:19.182385: step 5113, loss 0.611581.
Train: 2018-08-09T11:15:19.260500: step 5114, loss 0.562637.
Train: 2018-08-09T11:15:19.338573: step 5115, loss 0.546409.
Train: 2018-08-09T11:15:19.416685: step 5116, loss 0.578904.
Train: 2018-08-09T11:15:19.494816: step 5117, loss 0.510896.
Train: 2018-08-09T11:15:19.572924: step 5118, loss 0.595075.
Train: 2018-08-09T11:15:19.651024: step 5119, loss 0.627396.
Train: 2018-08-09T11:15:19.729105: step 5120, loss 0.627263.
Test: 2018-08-09T11:15:20.231300: step 5120, loss 0.547123.
Train: 2018-08-09T11:15:20.309406: step 5121, loss 0.57887.
Train: 2018-08-09T11:15:20.387542: step 5122, loss 0.626954.
Train: 2018-08-09T11:15:20.465627: step 5123, loss 0.610801.
Train: 2018-08-09T11:15:20.543726: step 5124, loss 0.658379.
Train: 2018-08-09T11:15:20.621864: step 5125, loss 0.531403.
Train: 2018-08-09T11:15:20.715561: step 5126, loss 0.500097.
Train: 2018-08-09T11:15:20.793666: step 5127, loss 0.500291.
Train: 2018-08-09T11:15:20.871805: step 5128, loss 0.641693.
Train: 2018-08-09T11:15:20.949880: step 5129, loss 0.437857.
Train: 2018-08-09T11:15:21.027986: step 5130, loss 0.531838.
Test: 2018-08-09T11:15:21.525475: step 5130, loss 0.547277.
Train: 2018-08-09T11:15:21.603613: step 5131, loss 0.484685.
Train: 2018-08-09T11:15:21.681720: step 5132, loss 0.578843.
Train: 2018-08-09T11:15:21.775448: step 5133, loss 0.626215.
Train: 2018-08-09T11:15:21.853548: step 5134, loss 0.610479.
Train: 2018-08-09T11:15:21.920472: step 5135, loss 0.626284.
Train: 2018-08-09T11:15:22.014232: step 5136, loss 0.578868.
Train: 2018-08-09T11:15:22.092337: step 5137, loss 0.610388.
Train: 2018-08-09T11:15:22.170444: step 5138, loss 0.500187.
Train: 2018-08-09T11:15:22.248549: step 5139, loss 0.563134.
Train: 2018-08-09T11:15:22.326628: step 5140, loss 0.484464.
Test: 2018-08-09T11:15:22.826510: step 5140, loss 0.551916.
Train: 2018-08-09T11:15:22.904645: step 5141, loss 0.594624.
Train: 2018-08-09T11:15:22.982753: step 5142, loss 0.547315.
Train: 2018-08-09T11:15:23.060855: step 5143, loss 0.515683.
Train: 2018-08-09T11:15:23.138967: step 5144, loss 0.515547.
Train: 2018-08-09T11:15:23.217072: step 5145, loss 0.54711.
Train: 2018-08-09T11:15:23.295176: step 5146, loss 0.642544.
Train: 2018-08-09T11:15:23.388461: step 5147, loss 0.515087.
Train: 2018-08-09T11:15:23.466138: step 5148, loss 0.514953.
Train: 2018-08-09T11:15:23.544216: step 5149, loss 0.562839.
Train: 2018-08-09T11:15:23.622352: step 5150, loss 0.594939.
Test: 2018-08-09T11:15:24.110035: step 5150, loss 0.547089.
Train: 2018-08-09T11:15:24.188144: step 5151, loss 0.514485.
Train: 2018-08-09T11:15:24.281841: step 5152, loss 0.530465.
Train: 2018-08-09T11:15:24.359979: step 5153, loss 0.497933.
Train: 2018-08-09T11:15:24.438054: step 5154, loss 0.562696.
Train: 2018-08-09T11:15:24.516191: step 5155, loss 0.513688.
Train: 2018-08-09T11:15:24.594294: step 5156, loss 0.497081.
Train: 2018-08-09T11:15:24.672373: step 5157, loss 0.562578.
Train: 2018-08-09T11:15:24.750481: step 5158, loss 0.545888.
Train: 2018-08-09T11:15:24.828619: step 5159, loss 0.645484.
Train: 2018-08-09T11:15:24.906694: step 5160, loss 0.512416.
Test: 2018-08-09T11:15:25.406607: step 5160, loss 0.549258.
Train: 2018-08-09T11:15:25.484714: step 5161, loss 0.529009.
Train: 2018-08-09T11:15:25.562819: step 5162, loss 0.528558.
Train: 2018-08-09T11:15:25.656519: step 5163, loss 0.494183.
Train: 2018-08-09T11:15:25.734658: step 5164, loss 0.512085.
Train: 2018-08-09T11:15:25.812755: step 5165, loss 0.617915.
Train: 2018-08-09T11:15:25.890869: step 5166, loss 0.594966.
Train: 2018-08-09T11:15:25.969604: step 5167, loss 0.544452.
Train: 2018-08-09T11:15:26.047742: step 5168, loss 0.511355.
Train: 2018-08-09T11:15:26.125851: step 5169, loss 0.527769.
Train: 2018-08-09T11:15:26.203954: step 5170, loss 0.526535.
Test: 2018-08-09T11:15:26.703807: step 5170, loss 0.546045.
Train: 2018-08-09T11:15:26.781944: step 5171, loss 0.614098.
Train: 2018-08-09T11:15:26.860049: step 5172, loss 0.597598.
Train: 2018-08-09T11:15:26.938157: step 5173, loss 0.580082.
Train: 2018-08-09T11:15:27.031878: step 5174, loss 0.51017.
Train: 2018-08-09T11:15:27.109991: step 5175, loss 0.56213.
Train: 2018-08-09T11:15:27.188098: step 5176, loss 0.597446.
Train: 2018-08-09T11:15:27.266174: step 5177, loss 0.597411.
Train: 2018-08-09T11:15:27.344310: step 5178, loss 0.612594.
Train: 2018-08-09T11:15:27.422418: step 5179, loss 0.511623.
Train: 2018-08-09T11:15:27.500523: step 5180, loss 0.630441.
Test: 2018-08-09T11:15:28.000401: step 5180, loss 0.549012.
Train: 2018-08-09T11:15:28.078512: step 5181, loss 0.614111.
Train: 2018-08-09T11:15:28.156589: step 5182, loss 0.646774.
Train: 2018-08-09T11:15:28.234721: step 5183, loss 0.545609.
Train: 2018-08-09T11:15:28.312832: step 5184, loss 0.612484.
Train: 2018-08-09T11:15:28.390908: step 5185, loss 0.562452.
Train: 2018-08-09T11:15:28.484667: step 5186, loss 0.545935.
Train: 2018-08-09T11:15:28.562742: step 5187, loss 0.595491.
Train: 2018-08-09T11:15:28.640881: step 5188, loss 0.529692.
Train: 2018-08-09T11:15:28.718987: step 5189, loss 0.464286.
Train: 2018-08-09T11:15:28.797093: step 5190, loss 0.611689.
Test: 2018-08-09T11:15:29.291988: step 5190, loss 0.547284.
Train: 2018-08-09T11:15:29.370124: step 5191, loss 0.595277.
Train: 2018-08-09T11:15:29.448231: step 5192, loss 0.513685.
Train: 2018-08-09T11:15:29.541964: step 5193, loss 0.54635.
Train: 2018-08-09T11:15:29.620035: step 5194, loss 0.497486.
Train: 2018-08-09T11:15:29.698141: step 5195, loss 0.578939.
Train: 2018-08-09T11:15:29.776277: step 5196, loss 0.644139.
Train: 2018-08-09T11:15:29.854386: step 5197, loss 0.611476.
Train: 2018-08-09T11:15:29.932493: step 5198, loss 0.595155.
Train: 2018-08-09T11:15:30.010569: step 5199, loss 0.562689.
Train: 2018-08-09T11:15:30.088706: step 5200, loss 0.595065.
Test: 2018-08-09T11:15:30.635446: step 5200, loss 0.550069.
Train: 2018-08-09T11:15:31.136741: step 5201, loss 0.530485.
Train: 2018-08-09T11:15:31.214878: step 5202, loss 0.433879.
Train: 2018-08-09T11:15:31.308576: step 5203, loss 0.56276.
Train: 2018-08-09T11:15:31.386712: step 5204, loss 0.627311.
Train: 2018-08-09T11:15:31.464819: step 5205, loss 0.530471.
Train: 2018-08-09T11:15:31.542921: step 5206, loss 0.546598.
Train: 2018-08-09T11:15:31.621001: step 5207, loss 0.498122.
Train: 2018-08-09T11:15:31.699140: step 5208, loss 0.578892.
Train: 2018-08-09T11:15:31.777217: step 5209, loss 0.514091.
Train: 2018-08-09T11:15:31.870973: step 5210, loss 0.513967.
Test: 2018-08-09T11:15:32.370825: step 5210, loss 0.546773.
Train: 2018-08-09T11:15:32.495796: step 5211, loss 0.644031.
Train: 2018-08-09T11:15:32.573933: step 5212, loss 0.595218.
Train: 2018-08-09T11:15:32.652039: step 5213, loss 0.513724.
Train: 2018-08-09T11:15:32.730147: step 5214, loss 0.562613.
Train: 2018-08-09T11:15:32.808252: step 5215, loss 0.529929.
Train: 2018-08-09T11:15:32.901981: step 5216, loss 0.480782.
Train: 2018-08-09T11:15:32.966726: step 5217, loss 0.546152.
Train: 2018-08-09T11:15:33.060455: step 5218, loss 0.48029.
Train: 2018-08-09T11:15:33.138562: step 5219, loss 0.562504.
Train: 2018-08-09T11:15:33.216642: step 5220, loss 0.512782.
Test: 2018-08-09T11:15:33.716526: step 5220, loss 0.548126.
Train: 2018-08-09T11:15:33.794662: step 5221, loss 0.579083.
Train: 2018-08-09T11:15:33.872739: step 5222, loss 0.612464.
Train: 2018-08-09T11:15:33.950847: step 5223, loss 0.579124.
Train: 2018-08-09T11:15:34.028982: step 5224, loss 0.579142.
Train: 2018-08-09T11:15:34.107059: step 5225, loss 0.595886.
Train: 2018-08-09T11:15:34.185197: step 5226, loss 0.528918.
Train: 2018-08-09T11:15:34.278892: step 5227, loss 0.595905.
Train: 2018-08-09T11:15:34.357000: step 5228, loss 0.545656.
Train: 2018-08-09T11:15:34.435138: step 5229, loss 0.528907.
Train: 2018-08-09T11:15:34.513243: step 5230, loss 0.62942.
Test: 2018-08-09T11:15:35.015359: step 5230, loss 0.54925.
Train: 2018-08-09T11:15:35.093465: step 5231, loss 0.595888.
Train: 2018-08-09T11:15:35.171602: step 5232, loss 0.495541.
Train: 2018-08-09T11:15:35.249681: step 5233, loss 0.595846.
Train: 2018-08-09T11:15:35.327812: step 5234, loss 0.562418.
Train: 2018-08-09T11:15:35.405918: step 5235, loss 0.595801.
Train: 2018-08-09T11:15:35.499651: step 5236, loss 0.495775.
Train: 2018-08-09T11:15:35.577758: step 5237, loss 0.495792.
Train: 2018-08-09T11:15:35.655863: step 5238, loss 0.495735.
Train: 2018-08-09T11:15:35.733971: step 5239, loss 0.562416.
Train: 2018-08-09T11:15:35.812079: step 5240, loss 0.545683.
Test: 2018-08-09T11:15:36.311930: step 5240, loss 0.548603.
Train: 2018-08-09T11:15:36.390065: step 5241, loss 0.562393.
Train: 2018-08-09T11:15:36.483764: step 5242, loss 0.52885.
Train: 2018-08-09T11:15:36.561877: step 5243, loss 0.427967.
Train: 2018-08-09T11:15:36.639977: step 5244, loss 0.511794.
Train: 2018-08-09T11:15:36.733704: step 5245, loss 0.596194.
Train: 2018-08-09T11:15:36.811811: step 5246, loss 0.596278.
Train: 2018-08-09T11:15:36.889948: step 5247, loss 0.494226.
Train: 2018-08-09T11:15:36.968606: step 5248, loss 0.545126.
Train: 2018-08-09T11:15:37.046684: step 5249, loss 0.493825.
Train: 2018-08-09T11:15:37.124790: step 5250, loss 0.562761.
Test: 2018-08-09T11:15:37.624672: step 5250, loss 0.545462.
Train: 2018-08-09T11:15:37.702809: step 5251, loss 0.563011.
Train: 2018-08-09T11:15:37.780885: step 5252, loss 0.614139.
Train: 2018-08-09T11:15:37.858992: step 5253, loss 0.44142.
Train: 2018-08-09T11:15:37.937129: step 5254, loss 0.510203.
Train: 2018-08-09T11:15:38.030851: step 5255, loss 0.631852.
Train: 2018-08-09T11:15:38.108933: step 5256, loss 0.510257.
Train: 2018-08-09T11:15:38.187075: step 5257, loss 0.510451.
Train: 2018-08-09T11:15:38.265176: step 5258, loss 0.527332.
Train: 2018-08-09T11:15:38.343252: step 5259, loss 0.50974.
Train: 2018-08-09T11:15:38.421384: step 5260, loss 0.509954.
Test: 2018-08-09T11:15:38.921246: step 5260, loss 0.54856.
Train: 2018-08-09T11:15:38.999372: step 5261, loss 0.545121.
Train: 2018-08-09T11:15:39.077485: step 5262, loss 0.544165.
Train: 2018-08-09T11:15:39.155565: step 5263, loss 0.544666.
Train: 2018-08-09T11:15:39.249289: step 5264, loss 0.68914.
Train: 2018-08-09T11:15:39.327396: step 5265, loss 0.492307.
Train: 2018-08-09T11:15:39.405533: step 5266, loss 0.580278.
Train: 2018-08-09T11:15:39.483609: step 5267, loss 0.61522.
Train: 2018-08-09T11:15:39.561740: step 5268, loss 0.597733.
Train: 2018-08-09T11:15:39.639853: step 5269, loss 0.650272.
Train: 2018-08-09T11:15:39.717953: step 5270, loss 0.492385.
Test: 2018-08-09T11:15:40.220107: step 5270, loss 0.547995.
Train: 2018-08-09T11:15:40.298213: step 5271, loss 0.597272.
Train: 2018-08-09T11:15:40.376320: step 5272, loss 0.52756.
Train: 2018-08-09T11:15:40.470082: step 5273, loss 0.475556.
Train: 2018-08-09T11:15:40.548184: step 5274, loss 0.440955.
Train: 2018-08-09T11:15:40.626292: step 5275, loss 0.544993.
Train: 2018-08-09T11:15:40.704410: step 5276, loss 0.579718.
Train: 2018-08-09T11:15:40.782476: step 5277, loss 0.562351.
Train: 2018-08-09T11:15:40.860581: step 5278, loss 0.597088.
Train: 2018-08-09T11:15:40.938691: step 5279, loss 0.510285.
Train: 2018-08-09T11:15:41.032445: step 5280, loss 0.649105.
Test: 2018-08-09T11:15:41.516677: step 5280, loss 0.545459.
Train: 2018-08-09T11:15:41.610406: step 5281, loss 0.631631.
Train: 2018-08-09T11:15:41.688541: step 5282, loss 0.596886.
Train: 2018-08-09T11:15:41.766618: step 5283, loss 0.613985.
Train: 2018-08-09T11:15:41.844723: step 5284, loss 0.596632.
Train: 2018-08-09T11:15:41.924323: step 5285, loss 0.596494.
Train: 2018-08-09T11:15:42.002398: step 5286, loss 0.630334.
Train: 2018-08-09T11:15:42.080534: step 5287, loss 0.731425.
Train: 2018-08-09T11:15:42.158612: step 5288, loss 0.562389.
Train: 2018-08-09T11:15:42.252369: step 5289, loss 0.612411.
Train: 2018-08-09T11:15:42.330475: step 5290, loss 0.628633.
Test: 2018-08-09T11:15:42.814731: step 5290, loss 0.548428.
Train: 2018-08-09T11:15:42.892842: step 5291, loss 0.496918.
Train: 2018-08-09T11:15:42.986540: step 5292, loss 0.530013.
Train: 2018-08-09T11:15:43.064682: step 5293, loss 0.643833.
Train: 2018-08-09T11:15:43.142784: step 5294, loss 0.465908.
Train: 2018-08-09T11:15:43.220861: step 5295, loss 0.498479.
Train: 2018-08-09T11:15:43.298996: step 5296, loss 0.594923.
Train: 2018-08-09T11:15:43.377104: step 5297, loss 0.594882.
Train: 2018-08-09T11:15:43.455210: step 5298, loss 0.578842.
Train: 2018-08-09T11:15:43.548941: step 5299, loss 0.642631.
Train: 2018-08-09T11:15:43.623976: step 5300, loss 0.562973.
Test: 2018-08-09T11:15:44.112547: step 5300, loss 0.549385.
Train: 2018-08-09T11:15:44.737400: step 5301, loss 0.420518.
Train: 2018-08-09T11:15:44.815506: step 5302, loss 0.610551.
Train: 2018-08-09T11:15:44.909234: step 5303, loss 0.531363.
Train: 2018-08-09T11:15:44.987311: step 5304, loss 0.578871.
Train: 2018-08-09T11:15:45.065447: step 5305, loss 0.657979.
Train: 2018-08-09T11:15:45.143554: step 5306, loss 0.626091.
Train: 2018-08-09T11:15:45.221663: step 5307, loss 0.641951.
Train: 2018-08-09T11:15:45.299768: step 5308, loss 0.594482.
Train: 2018-08-09T11:15:45.393466: step 5309, loss 0.578838.
Train: 2018-08-09T11:15:45.471572: step 5310, loss 0.563316.
Test: 2018-08-09T11:15:45.955833: step 5310, loss 0.551753.
Train: 2018-08-09T11:15:46.049584: step 5311, loss 0.547827.
Train: 2018-08-09T11:15:46.127697: step 5312, loss 0.53242.
Train: 2018-08-09T11:15:46.205805: step 5313, loss 0.563549.
Train: 2018-08-09T11:15:46.283911: step 5314, loss 0.594369.
Train: 2018-08-09T11:15:46.361987: step 5315, loss 0.54787.
Train: 2018-08-09T11:15:46.440095: step 5316, loss 0.517138.
Train: 2018-08-09T11:15:46.518230: step 5317, loss 0.548.
Train: 2018-08-09T11:15:46.596306: step 5318, loss 0.563401.
Train: 2018-08-09T11:15:46.690036: step 5319, loss 0.516173.
Train: 2018-08-09T11:15:46.768141: step 5320, loss 0.578984.
Test: 2018-08-09T11:15:47.254812: step 5320, loss 0.550149.
Train: 2018-08-09T11:15:47.332948: step 5321, loss 0.516364.
Train: 2018-08-09T11:15:47.426676: step 5322, loss 0.531301.
Train: 2018-08-09T11:15:47.504779: step 5323, loss 0.686279.
Train: 2018-08-09T11:15:47.582889: step 5324, loss 0.546664.
Train: 2018-08-09T11:15:47.660965: step 5325, loss 0.61304.
Train: 2018-08-09T11:15:47.754725: step 5326, loss 0.611735.
Train: 2018-08-09T11:15:47.832831: step 5327, loss 0.657136.
Train: 2018-08-09T11:15:47.910906: step 5328, loss 0.594887.
Train: 2018-08-09T11:15:47.989046: step 5329, loss 0.594025.
Train: 2018-08-09T11:15:48.067120: step 5330, loss 0.595995.
Test: 2018-08-09T11:15:48.567034: step 5330, loss 0.550123.
Train: 2018-08-09T11:15:48.645109: step 5331, loss 0.547375.
Train: 2018-08-09T11:15:48.723246: step 5332, loss 0.579276.
Train: 2018-08-09T11:15:48.816944: step 5333, loss 0.608235.
Train: 2018-08-09T11:15:48.895051: step 5334, loss 0.610372.
Train: 2018-08-09T11:15:48.974555: step 5335, loss 0.59432.
Train: 2018-08-09T11:15:49.052692: step 5336, loss 0.5025.
Train: 2018-08-09T11:15:49.130769: step 5337, loss 0.488073.
Train: 2018-08-09T11:15:49.208904: step 5338, loss 0.502719.
Train: 2018-08-09T11:15:49.286993: step 5339, loss 0.533246.
Train: 2018-08-09T11:15:49.365118: step 5340, loss 0.686265.
Test: 2018-08-09T11:15:49.864971: step 5340, loss 0.550907.
Train: 2018-08-09T11:15:49.943107: step 5341, loss 0.564182.
Train: 2018-08-09T11:15:50.021214: step 5342, loss 0.548395.
Train: 2018-08-09T11:15:50.114911: step 5343, loss 0.486846.
Train: 2018-08-09T11:15:50.193018: step 5344, loss 0.470924.
Train: 2018-08-09T11:15:50.271155: step 5345, loss 0.515582.
Train: 2018-08-09T11:15:50.349262: step 5346, loss 0.625591.
Train: 2018-08-09T11:15:50.427371: step 5347, loss 0.610845.
Train: 2018-08-09T11:15:50.505476: step 5348, loss 0.579002.
Train: 2018-08-09T11:15:50.599172: step 5349, loss 0.58308.
Train: 2018-08-09T11:15:50.677309: step 5350, loss 0.596137.
Test: 2018-08-09T11:15:51.162929: step 5350, loss 0.548597.
Train: 2018-08-09T11:15:51.241010: step 5351, loss 0.578032.
Train: 2018-08-09T11:15:51.319116: step 5352, loss 0.547939.
Train: 2018-08-09T11:15:51.412877: step 5353, loss 0.610264.
Train: 2018-08-09T11:15:51.490981: step 5354, loss 0.609965.
Train: 2018-08-09T11:15:51.569088: step 5355, loss 0.563789.
Train: 2018-08-09T11:15:51.647166: step 5356, loss 0.562524.
Train: 2018-08-09T11:15:51.725301: step 5357, loss 0.641498.
Train: 2018-08-09T11:15:51.803378: step 5358, loss 0.516404.
Train: 2018-08-09T11:15:51.881484: step 5359, loss 0.704351.
Train: 2018-08-09T11:15:51.959622: step 5360, loss 0.579394.
Test: 2018-08-09T11:15:52.467104: step 5360, loss 0.549504.
Train: 2018-08-09T11:15:52.545210: step 5361, loss 0.501738.
Train: 2018-08-09T11:15:52.623315: step 5362, loss 0.547997.
Train: 2018-08-09T11:15:52.701453: step 5363, loss 0.563474.
Train: 2018-08-09T11:15:52.795152: step 5364, loss 0.578903.
Train: 2018-08-09T11:15:52.873287: step 5365, loss 0.563473.
Train: 2018-08-09T11:15:52.945056: step 5366, loss 0.563553.
Train: 2018-08-09T11:15:53.023192: step 5367, loss 0.578988.
Train: 2018-08-09T11:15:53.101298: step 5368, loss 0.517291.
Train: 2018-08-09T11:15:53.194996: step 5369, loss 0.517193.
Train: 2018-08-09T11:15:53.273103: step 5370, loss 0.609859.
Test: 2018-08-09T11:15:53.772986: step 5370, loss 0.550669.
Train: 2018-08-09T11:15:53.851093: step 5371, loss 0.594397.
Train: 2018-08-09T11:15:53.929229: step 5372, loss 0.563403.
Train: 2018-08-09T11:15:54.007338: step 5373, loss 0.609949.
Train: 2018-08-09T11:15:54.101032: step 5374, loss 0.578983.
Train: 2018-08-09T11:15:54.179169: step 5375, loss 0.532356.
Train: 2018-08-09T11:15:54.257276: step 5376, loss 0.547859.
Train: 2018-08-09T11:15:54.328518: step 5377, loss 0.656645.
Train: 2018-08-09T11:15:54.420155: step 5378, loss 0.485694.
Train: 2018-08-09T11:15:54.498296: step 5379, loss 0.469963.
Train: 2018-08-09T11:15:54.576368: step 5380, loss 0.625907.
Test: 2018-08-09T11:15:55.076281: step 5380, loss 0.548583.
Train: 2018-08-09T11:15:55.154387: step 5381, loss 0.563189.
Train: 2018-08-09T11:15:55.232466: step 5382, loss 0.563253.
Train: 2018-08-09T11:15:55.310573: step 5383, loss 0.610206.
Train: 2018-08-09T11:15:55.388706: step 5384, loss 0.563198.
Train: 2018-08-09T11:15:55.482435: step 5385, loss 0.641705.
Train: 2018-08-09T11:15:55.560512: step 5386, loss 0.626002.
Train: 2018-08-09T11:15:55.638650: step 5387, loss 0.563203.
Train: 2018-08-09T11:15:55.716724: step 5388, loss 0.594531.
Train: 2018-08-09T11:15:55.794831: step 5389, loss 0.563272.
Train: 2018-08-09T11:15:55.888589: step 5390, loss 0.547634.
Test: 2018-08-09T11:15:56.372850: step 5390, loss 0.552223.
Train: 2018-08-09T11:15:56.450951: step 5391, loss 0.578899.
Train: 2018-08-09T11:15:56.544678: step 5392, loss 0.438631.
Train: 2018-08-09T11:15:56.622762: step 5393, loss 0.469509.
Train: 2018-08-09T11:15:56.700897: step 5394, loss 0.547512.
Train: 2018-08-09T11:15:56.778973: step 5395, loss 0.578849.
Train: 2018-08-09T11:15:56.857081: step 5396, loss 0.499867.
Train: 2018-08-09T11:15:56.935213: step 5397, loss 0.626583.
Train: 2018-08-09T11:15:57.013294: step 5398, loss 0.62639.
Train: 2018-08-09T11:15:57.107052: step 5399, loss 0.578946.
Train: 2018-08-09T11:15:57.185159: step 5400, loss 0.642661.
Test: 2018-08-09T11:15:57.685011: step 5400, loss 0.547943.
Train: 2018-08-09T11:15:58.250113: step 5401, loss 0.514888.
Train: 2018-08-09T11:15:58.328220: step 5402, loss 0.45118.
Train: 2018-08-09T11:15:58.406329: step 5403, loss 0.450121.
Train: 2018-08-09T11:15:58.500054: step 5404, loss 0.708115.
Train: 2018-08-09T11:15:58.578161: step 5405, loss 0.580068.
Train: 2018-08-09T11:15:58.656264: step 5406, loss 0.54642.
Train: 2018-08-09T11:15:58.734345: step 5407, loss 0.48151.
Train: 2018-08-09T11:15:58.812480: step 5408, loss 0.497291.
Train: 2018-08-09T11:15:58.890588: step 5409, loss 0.562057.
Train: 2018-08-09T11:15:58.968664: step 5410, loss 0.462457.
Test: 2018-08-09T11:15:59.468577: step 5410, loss 0.546747.
Train: 2018-08-09T11:15:59.546652: step 5411, loss 0.578201.
Train: 2018-08-09T11:15:59.640380: step 5412, loss 0.52695.
Train: 2018-08-09T11:15:59.718488: step 5413, loss 0.530898.
Train: 2018-08-09T11:15:59.796623: step 5414, loss 0.582061.
Train: 2018-08-09T11:15:59.869675: step 5415, loss 0.575856.
Train: 2018-08-09T11:15:59.955051: step 5416, loss 0.630081.
Train: 2018-08-09T11:16:00.033157: step 5417, loss 0.423132.
Train: 2018-08-09T11:16:00.111263: step 5418, loss 0.54283.
Train: 2018-08-09T11:16:00.204994: step 5419, loss 0.590177.
Train: 2018-08-09T11:16:00.283098: step 5420, loss 0.52754.
Test: 2018-08-09T11:16:00.767360: step 5420, loss 0.546372.
Train: 2018-08-09T11:16:00.845460: step 5421, loss 0.569943.
Train: 2018-08-09T11:16:00.923571: step 5422, loss 0.454628.
Train: 2018-08-09T11:16:01.017270: step 5423, loss 0.59924.
Train: 2018-08-09T11:16:01.095406: step 5424, loss 0.598313.
Train: 2018-08-09T11:16:01.173513: step 5425, loss 0.478557.
Train: 2018-08-09T11:16:01.251608: step 5426, loss 0.565146.
Train: 2018-08-09T11:16:01.345317: step 5427, loss 0.557409.
Train: 2018-08-09T11:16:01.423423: step 5428, loss 0.471559.
Train: 2018-08-09T11:16:01.501566: step 5429, loss 0.565172.
Train: 2018-08-09T11:16:01.579668: step 5430, loss 0.584996.
Test: 2018-08-09T11:16:02.079519: step 5430, loss 0.547229.
Train: 2018-08-09T11:16:02.157625: step 5431, loss 0.577701.
Train: 2018-08-09T11:16:02.235762: step 5432, loss 0.561295.
Train: 2018-08-09T11:16:02.313865: step 5433, loss 0.595275.
Train: 2018-08-09T11:16:02.391945: step 5434, loss 0.561781.
Train: 2018-08-09T11:16:02.470083: step 5435, loss 0.598466.
Train: 2018-08-09T11:16:02.563811: step 5436, loss 0.583718.
Train: 2018-08-09T11:16:02.641917: step 5437, loss 0.698972.
Train: 2018-08-09T11:16:02.719993: step 5438, loss 0.610959.
Train: 2018-08-09T11:16:02.798100: step 5439, loss 0.512975.
Train: 2018-08-09T11:16:02.876239: step 5440, loss 0.562474.
Test: 2018-08-09T11:16:03.378383: step 5440, loss 0.550337.
Train: 2018-08-09T11:16:03.456477: step 5441, loss 0.611817.
Train: 2018-08-09T11:16:03.534555: step 5442, loss 0.562833.
Train: 2018-08-09T11:16:03.612662: step 5443, loss 0.611194.
Train: 2018-08-09T11:16:03.690798: step 5444, loss 0.611064.
Train: 2018-08-09T11:16:03.768876: step 5445, loss 0.482786.
Train: 2018-08-09T11:16:03.863946: step 5446, loss 0.467055.
Train: 2018-08-09T11:16:03.935891: step 5447, loss 0.483039.
Train: 2018-08-09T11:16:04.013998: step 5448, loss 0.610871.
Train: 2018-08-09T11:16:04.092075: step 5449, loss 0.498783.
Train: 2018-08-09T11:16:04.170211: step 5450, loss 0.56268.
Test: 2018-08-09T11:16:04.670063: step 5450, loss 0.548867.
Train: 2018-08-09T11:16:04.748203: step 5451, loss 0.498536.
Train: 2018-08-09T11:16:04.841928: step 5452, loss 0.661139.
Train: 2018-08-09T11:16:04.910704: step 5453, loss 0.547029.
Train: 2018-08-09T11:16:04.988842: step 5454, loss 0.514516.
Train: 2018-08-09T11:16:05.082569: step 5455, loss 0.51449.
Train: 2018-08-09T11:16:05.160679: step 5456, loss 0.578891.
Train: 2018-08-09T11:16:05.238777: step 5457, loss 0.59505.
Train: 2018-08-09T11:16:05.316891: step 5458, loss 0.578864.
Train: 2018-08-09T11:16:05.394996: step 5459, loss 0.56269.
Train: 2018-08-09T11:16:05.473071: step 5460, loss 0.562687.
Test: 2018-08-09T11:16:05.972955: step 5460, loss 0.548706.
Train: 2018-08-09T11:16:06.051060: step 5461, loss 0.530175.
Train: 2018-08-09T11:16:06.144790: step 5462, loss 0.562779.
Train: 2018-08-09T11:16:06.222928: step 5463, loss 0.611492.
Train: 2018-08-09T11:16:06.301001: step 5464, loss 0.530039.
Train: 2018-08-09T11:16:06.379109: step 5465, loss 0.64403.
Train: 2018-08-09T11:16:06.457249: step 5466, loss 0.513932.
Train: 2018-08-09T11:16:06.550974: step 5467, loss 0.611363.
Train: 2018-08-09T11:16:06.629080: step 5468, loss 0.57894.
Train: 2018-08-09T11:16:06.707187: step 5469, loss 0.465268.
Train: 2018-08-09T11:16:06.785288: step 5470, loss 0.54636.
Test: 2018-08-09T11:16:07.285146: step 5470, loss 0.547998.
Train: 2018-08-09T11:16:07.363283: step 5471, loss 0.530073.
Train: 2018-08-09T11:16:07.441388: step 5472, loss 0.529816.
Train: 2018-08-09T11:16:07.519491: step 5473, loss 0.644918.
Train: 2018-08-09T11:16:07.597572: step 5474, loss 0.496567.
Train: 2018-08-09T11:16:07.675677: step 5475, loss 0.628484.
Train: 2018-08-09T11:16:07.769406: step 5476, loss 0.512782.
Train: 2018-08-09T11:16:07.847512: step 5477, loss 0.628404.
Train: 2018-08-09T11:16:07.927899: step 5478, loss 0.513145.
Train: 2018-08-09T11:16:08.006007: step 5479, loss 0.561055.
Train: 2018-08-09T11:16:08.084143: step 5480, loss 0.580092.
Test: 2018-08-09T11:16:08.584002: step 5480, loss 0.550035.
Train: 2018-08-09T11:16:08.662131: step 5481, loss 0.597416.
Train: 2018-08-09T11:16:08.740237: step 5482, loss 0.611145.
Train: 2018-08-09T11:16:08.818320: step 5483, loss 0.561352.
Train: 2018-08-09T11:16:08.912072: step 5484, loss 0.579188.
Train: 2018-08-09T11:16:08.990179: step 5485, loss 0.57829.
Train: 2018-08-09T11:16:09.068257: step 5486, loss 0.62772.
Train: 2018-08-09T11:16:09.146361: step 5487, loss 0.612634.
Train: 2018-08-09T11:16:09.224470: step 5488, loss 0.578943.
Train: 2018-08-09T11:16:09.318227: step 5489, loss 0.627813.
Train: 2018-08-09T11:16:09.396302: step 5490, loss 0.563053.
Test: 2018-08-09T11:16:09.896185: step 5490, loss 0.547841.
Train: 2018-08-09T11:16:09.961133: step 5491, loss 0.658702.
Train: 2018-08-09T11:16:10.054864: step 5492, loss 0.499275.
Train: 2018-08-09T11:16:10.132968: step 5493, loss 0.610485.
Train: 2018-08-09T11:16:10.211074: step 5494, loss 0.56318.
Train: 2018-08-09T11:16:10.289183: step 5495, loss 0.46884.
Train: 2018-08-09T11:16:10.367257: step 5496, loss 0.563199.
Train: 2018-08-09T11:16:10.445390: step 5497, loss 0.516004.
Train: 2018-08-09T11:16:10.523500: step 5498, loss 0.515977.
Train: 2018-08-09T11:16:10.617198: step 5499, loss 0.547337.
Train: 2018-08-09T11:16:10.695335: step 5500, loss 0.641961.
Test: 2018-08-09T11:16:11.195188: step 5500, loss 0.551274.
Train: 2018-08-09T11:16:11.757555: step 5501, loss 0.610475.
Train: 2018-08-09T11:16:11.835691: step 5502, loss 0.468444.
Train: 2018-08-09T11:16:11.915186: step 5503, loss 0.515675.
Train: 2018-08-09T11:16:12.008914: step 5504, loss 0.578902.
Train: 2018-08-09T11:16:12.087021: step 5505, loss 0.626465.
Train: 2018-08-09T11:16:12.165129: step 5506, loss 0.531281.
Train: 2018-08-09T11:16:12.243265: step 5507, loss 0.515325.
Train: 2018-08-09T11:16:12.321371: step 5508, loss 0.562949.
Train: 2018-08-09T11:16:12.399483: step 5509, loss 0.594818.
Train: 2018-08-09T11:16:12.477580: step 5510, loss 0.546942.
Test: 2018-08-09T11:16:12.977436: step 5510, loss 0.549102.
Train: 2018-08-09T11:16:13.055544: step 5511, loss 0.514943.
Train: 2018-08-09T11:16:13.133650: step 5512, loss 0.562831.
Train: 2018-08-09T11:16:13.227404: step 5513, loss 0.530706.
Train: 2018-08-09T11:16:13.305514: step 5514, loss 0.594991.
Train: 2018-08-09T11:16:13.383619: step 5515, loss 0.546612.
Train: 2018-08-09T11:16:13.461699: step 5516, loss 0.562728.
Train: 2018-08-09T11:16:13.539834: step 5517, loss 0.530325.
Train: 2018-08-09T11:16:13.617941: step 5518, loss 0.514042.
Train: 2018-08-09T11:16:13.696048: step 5519, loss 0.562662.
Train: 2018-08-09T11:16:13.789744: step 5520, loss 0.497389.
Test: 2018-08-09T11:16:14.276323: step 5520, loss 0.54849.
Train: 2018-08-09T11:16:14.354461: step 5521, loss 0.66072.
Train: 2018-08-09T11:16:14.448158: step 5522, loss 0.595338.
Train: 2018-08-09T11:16:14.526289: step 5523, loss 0.529826.
Train: 2018-08-09T11:16:14.604370: step 5524, loss 0.628185.
Train: 2018-08-09T11:16:14.682478: step 5525, loss 0.579059.
Train: 2018-08-09T11:16:14.760610: step 5526, loss 0.562485.
Train: 2018-08-09T11:16:14.855155: step 5527, loss 0.56254.
Train: 2018-08-09T11:16:14.925736: step 5528, loss 0.546174.
Train: 2018-08-09T11:16:15.003844: step 5529, loss 0.595318.
Train: 2018-08-09T11:16:15.097594: step 5530, loss 0.546255.
Test: 2018-08-09T11:16:15.581833: step 5530, loss 0.549092.
Train: 2018-08-09T11:16:15.659968: step 5531, loss 0.611701.
Train: 2018-08-09T11:16:15.738074: step 5532, loss 0.595312.
Train: 2018-08-09T11:16:15.831803: step 5533, loss 0.562558.
Train: 2018-08-09T11:16:15.912107: step 5534, loss 0.562584.
Train: 2018-08-09T11:16:15.979628: step 5535, loss 0.513802.
Train: 2018-08-09T11:16:16.073357: step 5536, loss 0.62774.
Train: 2018-08-09T11:16:16.151493: step 5537, loss 0.400113.
Train: 2018-08-09T11:16:16.229570: step 5538, loss 0.546308.
Train: 2018-08-09T11:16:16.307708: step 5539, loss 0.595263.
Train: 2018-08-09T11:16:16.385783: step 5540, loss 0.529941.
Test: 2018-08-09T11:16:16.885666: step 5540, loss 0.550332.
Train: 2018-08-09T11:16:16.963771: step 5541, loss 0.513493.
Train: 2018-08-09T11:16:17.041903: step 5542, loss 0.529958.
Train: 2018-08-09T11:16:17.119986: step 5543, loss 0.562652.
Train: 2018-08-09T11:16:17.213745: step 5544, loss 0.562485.
Train: 2018-08-09T11:16:17.291820: step 5545, loss 0.44685.
Train: 2018-08-09T11:16:17.369928: step 5546, loss 0.49606.
Train: 2018-08-09T11:16:17.448063: step 5547, loss 0.595702.
Train: 2018-08-09T11:16:17.526171: step 5548, loss 0.629296.
Train: 2018-08-09T11:16:17.604247: step 5549, loss 0.645872.
Train: 2018-08-09T11:16:17.682352: step 5550, loss 0.59582.
Test: 2018-08-09T11:16:18.182266: step 5550, loss 0.549815.
Train: 2018-08-09T11:16:18.260371: step 5551, loss 0.528705.
Train: 2018-08-09T11:16:18.338478: step 5552, loss 0.596198.
Train: 2018-08-09T11:16:18.416587: step 5553, loss 0.579664.
Train: 2018-08-09T11:16:18.510282: step 5554, loss 0.56184.
Train: 2018-08-09T11:16:18.588419: step 5555, loss 0.545715.
Train: 2018-08-09T11:16:18.666496: step 5556, loss 0.545548.
Train: 2018-08-09T11:16:18.744602: step 5557, loss 0.529078.
Train: 2018-08-09T11:16:18.822739: step 5558, loss 0.545671.
Train: 2018-08-09T11:16:18.900847: step 5559, loss 0.629376.
Train: 2018-08-09T11:16:18.980411: step 5560, loss 0.579063.
Test: 2018-08-09T11:16:19.480263: step 5560, loss 0.548079.
Train: 2018-08-09T11:16:19.558401: step 5561, loss 0.49539.
Train: 2018-08-09T11:16:19.636476: step 5562, loss 0.679598.
Train: 2018-08-09T11:16:19.730236: step 5563, loss 0.595939.
Train: 2018-08-09T11:16:19.808341: step 5564, loss 0.578934.
Train: 2018-08-09T11:16:19.886446: step 5565, loss 0.496607.
Train: 2018-08-09T11:16:19.964525: step 5566, loss 0.480217.
Train: 2018-08-09T11:16:20.042660: step 5567, loss 0.496609.
Train: 2018-08-09T11:16:20.120767: step 5568, loss 0.677919.
Train: 2018-08-09T11:16:20.214491: step 5569, loss 0.480242.
Train: 2018-08-09T11:16:20.292602: step 5570, loss 0.579099.
Test: 2018-08-09T11:16:20.792460: step 5570, loss 0.548337.
Train: 2018-08-09T11:16:20.870559: step 5571, loss 0.529469.
Train: 2018-08-09T11:16:20.949966: step 5572, loss 0.595645.
Train: 2018-08-09T11:16:21.028073: step 5573, loss 0.628291.
Train: 2018-08-09T11:16:21.106209: step 5574, loss 0.595558.
Train: 2018-08-09T11:16:21.184316: step 5575, loss 0.693923.
Train: 2018-08-09T11:16:21.262394: step 5576, loss 0.546267.
Train: 2018-08-09T11:16:21.340498: step 5577, loss 0.56265.
Train: 2018-08-09T11:16:21.418635: step 5578, loss 0.513922.
Train: 2018-08-09T11:16:21.512333: step 5579, loss 0.465374.
Train: 2018-08-09T11:16:21.590470: step 5580, loss 0.611345.
Test: 2018-08-09T11:16:22.090347: step 5580, loss 0.549338.
Train: 2018-08-09T11:16:22.168429: step 5581, loss 0.530266.
Train: 2018-08-09T11:16:22.246565: step 5582, loss 0.5789.
Train: 2018-08-09T11:16:22.324643: step 5583, loss 0.530275.
Train: 2018-08-09T11:16:22.402778: step 5584, loss 0.432973.
Train: 2018-08-09T11:16:22.480885: step 5585, loss 0.546399.
Train: 2018-08-09T11:16:22.574610: step 5586, loss 0.660422.
Train: 2018-08-09T11:16:22.652721: step 5587, loss 0.611551.
Train: 2018-08-09T11:16:22.730826: step 5588, loss 0.611542.
Train: 2018-08-09T11:16:22.808904: step 5589, loss 0.546342.
Train: 2018-08-09T11:16:22.887043: step 5590, loss 0.595198.
Test: 2018-08-09T11:16:23.390672: step 5590, loss 0.547416.
Train: 2018-08-09T11:16:23.468803: step 5591, loss 0.676478.
Train: 2018-08-09T11:16:23.546916: step 5592, loss 0.611325.
Train: 2018-08-09T11:16:23.624993: step 5593, loss 0.481951.
Train: 2018-08-09T11:16:23.703123: step 5594, loss 0.530495.
Train: 2018-08-09T11:16:23.781205: step 5595, loss 0.627216.
Train: 2018-08-09T11:16:23.874970: step 5596, loss 0.627118.
Train: 2018-08-09T11:16:23.953070: step 5597, loss 0.562827.
Train: 2018-08-09T11:16:24.031176: step 5598, loss 0.674809.
Train: 2018-08-09T11:16:24.109286: step 5599, loss 0.531126.
Train: 2018-08-09T11:16:24.187390: step 5600, loss 0.53128.
Test: 2018-08-09T11:16:24.687267: step 5600, loss 0.551768.
Train: 2018-08-09T11:16:25.283907: step 5601, loss 0.483798.
Train: 2018-08-09T11:16:25.362013: step 5602, loss 0.610545.
Train: 2018-08-09T11:16:25.440120: step 5603, loss 0.594686.
Train: 2018-08-09T11:16:25.518228: step 5604, loss 0.657878.
Train: 2018-08-09T11:16:25.596332: step 5605, loss 0.563108.
Train: 2018-08-09T11:16:25.690031: step 5606, loss 0.531713.
Train: 2018-08-09T11:16:25.768168: step 5607, loss 0.469006.
Train: 2018-08-09T11:16:25.846274: step 5608, loss 0.673094.
Train: 2018-08-09T11:16:25.924353: step 5609, loss 0.500468.
Train: 2018-08-09T11:16:26.002487: step 5610, loss 0.531829.
Test: 2018-08-09T11:16:26.502364: step 5610, loss 0.549661.
Train: 2018-08-09T11:16:26.580470: step 5611, loss 0.484707.
Train: 2018-08-09T11:16:26.658583: step 5612, loss 0.64179.
Train: 2018-08-09T11:16:26.736685: step 5613, loss 0.468674.
Train: 2018-08-09T11:16:26.814764: step 5614, loss 0.484172.
Train: 2018-08-09T11:16:26.892871: step 5615, loss 0.563022.
Train: 2018-08-09T11:16:26.986602: step 5616, loss 0.658355.
Train: 2018-08-09T11:16:27.064738: step 5617, loss 0.562936.
Train: 2018-08-09T11:16:27.142844: step 5618, loss 0.642645.
Train: 2018-08-09T11:16:27.220946: step 5619, loss 0.451287.
Train: 2018-08-09T11:16:27.299056: step 5620, loss 0.514941.
Test: 2018-08-09T11:16:27.798908: step 5620, loss 0.549643.
Train: 2018-08-09T11:16:27.877014: step 5621, loss 0.498739.
Train: 2018-08-09T11:16:27.956505: step 5622, loss 0.691508.
Train: 2018-08-09T11:16:28.050236: step 5623, loss 0.594981.
Train: 2018-08-09T11:16:28.128310: step 5624, loss 0.482222.
Train: 2018-08-09T11:16:28.206416: step 5625, loss 0.67572.
Train: 2018-08-09T11:16:28.284525: step 5626, loss 0.643424.
Train: 2018-08-09T11:16:28.362660: step 5627, loss 0.530555.
Train: 2018-08-09T11:16:28.440768: step 5628, loss 0.530598.
Train: 2018-08-09T11:16:28.518874: step 5629, loss 0.53059.
Train: 2018-08-09T11:16:28.596949: step 5630, loss 0.578877.
Test: 2018-08-09T11:16:29.096863: step 5630, loss 0.548284.
Train: 2018-08-09T11:16:29.174938: step 5631, loss 0.530551.
Train: 2018-08-09T11:16:29.253070: step 5632, loss 0.595002.
Train: 2018-08-09T11:16:29.346799: step 5633, loss 0.54663.
Train: 2018-08-09T11:16:29.424908: step 5634, loss 0.578882.
Train: 2018-08-09T11:16:29.502986: step 5635, loss 0.514334.
Train: 2018-08-09T11:16:29.581104: step 5636, loss 0.514267.
Train: 2018-08-09T11:16:29.659199: step 5637, loss 0.595077.
Train: 2018-08-09T11:16:29.737331: step 5638, loss 0.611305.
Train: 2018-08-09T11:16:29.831035: step 5639, loss 0.530282.
Train: 2018-08-09T11:16:29.910575: step 5640, loss 0.546466.
Test: 2018-08-09T11:16:30.394853: step 5640, loss 0.548688.
Train: 2018-08-09T11:16:30.488551: step 5641, loss 0.513966.
Train: 2018-08-09T11:16:30.566688: step 5642, loss 0.676498.
Train: 2018-08-09T11:16:30.644798: step 5643, loss 0.530142.
Train: 2018-08-09T11:16:30.722902: step 5644, loss 0.676476.
Train: 2018-08-09T11:16:30.801008: step 5645, loss 0.562675.
Train: 2018-08-09T11:16:30.892305: step 5646, loss 0.562691.
Train: 2018-08-09T11:16:30.961972: step 5647, loss 0.562706.
Train: 2018-08-09T11:16:31.040077: step 5648, loss 0.546557.
Train: 2018-08-09T11:16:31.118218: step 5649, loss 0.627351.
Train: 2018-08-09T11:16:31.196291: step 5650, loss 0.562754.
Test: 2018-08-09T11:16:31.696173: step 5650, loss 0.548298.
Train: 2018-08-09T11:16:31.774307: step 5651, loss 0.530575.
Train: 2018-08-09T11:16:31.868038: step 5652, loss 0.530605.
Train: 2018-08-09T11:16:31.946145: step 5653, loss 0.546685.
Train: 2018-08-09T11:16:32.024252: step 5654, loss 0.514445.
Train: 2018-08-09T11:16:32.102328: step 5655, loss 0.59498.
Train: 2018-08-09T11:16:32.180465: step 5656, loss 0.546479.
Train: 2018-08-09T11:16:32.258570: step 5657, loss 0.61092.
Train: 2018-08-09T11:16:32.336674: step 5658, loss 0.611565.
Train: 2018-08-09T11:16:32.430406: step 5659, loss 0.497455.
Train: 2018-08-09T11:16:32.508511: step 5660, loss 0.562737.
Test: 2018-08-09T11:16:33.008365: step 5660, loss 0.54909.
Train: 2018-08-09T11:16:33.086471: step 5661, loss 0.579485.
Train: 2018-08-09T11:16:33.164576: step 5662, loss 0.563414.
Train: 2018-08-09T11:16:33.242717: step 5663, loss 0.579076.
Train: 2018-08-09T11:16:33.320820: step 5664, loss 0.594651.
Train: 2018-08-09T11:16:33.398923: step 5665, loss 0.595094.
Train: 2018-08-09T11:16:33.492639: step 5666, loss 0.546994.
Train: 2018-08-09T11:16:33.570731: step 5667, loss 0.611333.
Train: 2018-08-09T11:16:33.648870: step 5668, loss 0.530692.
Train: 2018-08-09T11:16:33.726946: step 5669, loss 0.530715.
Train: 2018-08-09T11:16:33.805081: step 5670, loss 0.466718.
Test: 2018-08-09T11:16:34.306388: step 5670, loss 0.549001.
Train: 2018-08-09T11:16:34.384495: step 5671, loss 0.530727.
Train: 2018-08-09T11:16:34.462631: step 5672, loss 0.627115.
Train: 2018-08-09T11:16:34.540740: step 5673, loss 0.61106.
Train: 2018-08-09T11:16:34.618814: step 5674, loss 0.546692.
Train: 2018-08-09T11:16:34.696954: step 5675, loss 0.578875.
Train: 2018-08-09T11:16:34.790679: step 5676, loss 0.498394.
Train: 2018-08-09T11:16:34.868786: step 5677, loss 0.627226.
Train: 2018-08-09T11:16:34.946862: step 5678, loss 0.643341.
Train: 2018-08-09T11:16:35.025000: step 5679, loss 0.450126.
Train: 2018-08-09T11:16:35.103105: step 5680, loss 0.466133.
Test: 2018-08-09T11:16:35.602959: step 5680, loss 0.551885.
Train: 2018-08-09T11:16:35.681094: step 5681, loss 0.546593.
Train: 2018-08-09T11:16:35.759204: step 5682, loss 0.481772.
Train: 2018-08-09T11:16:35.837309: step 5683, loss 0.546426.
Train: 2018-08-09T11:16:35.917719: step 5684, loss 0.5463.
Train: 2018-08-09T11:16:36.011478: step 5685, loss 0.546191.
Train: 2018-08-09T11:16:36.089554: step 5686, loss 0.480381.
Train: 2018-08-09T11:16:36.167662: step 5687, loss 0.546028.
Train: 2018-08-09T11:16:36.245798: step 5688, loss 0.512397.
Train: 2018-08-09T11:16:36.323875: step 5689, loss 0.529025.
Train: 2018-08-09T11:16:36.401982: step 5690, loss 0.528963.
Test: 2018-08-09T11:16:36.901863: step 5690, loss 0.548282.
Train: 2018-08-09T11:16:36.979974: step 5691, loss 0.57787.
Train: 2018-08-09T11:16:37.073698: step 5692, loss 0.476894.
Train: 2018-08-09T11:16:37.151833: step 5693, loss 0.585487.
Train: 2018-08-09T11:16:37.229912: step 5694, loss 0.527996.
Train: 2018-08-09T11:16:37.308049: step 5695, loss 0.509887.
Train: 2018-08-09T11:16:37.386159: step 5696, loss 0.614396.
Train: 2018-08-09T11:16:37.464262: step 5697, loss 0.438763.
Train: 2018-08-09T11:16:37.557989: step 5698, loss 0.667417.
Train: 2018-08-09T11:16:37.636066: step 5699, loss 0.630866.
Train: 2018-08-09T11:16:37.714173: step 5700, loss 0.648767.
Test: 2018-08-09T11:16:38.200940: step 5700, loss 0.545994.
Train: 2018-08-09T11:16:38.747692: step 5701, loss 0.615814.
Train: 2018-08-09T11:16:38.825798: step 5702, loss 0.510047.
Train: 2018-08-09T11:16:38.903906: step 5703, loss 0.493419.
Train: 2018-08-09T11:16:38.982012: step 5704, loss 0.615285.
Train: 2018-08-09T11:16:39.060119: step 5705, loss 0.54453.
Train: 2018-08-09T11:16:39.153847: step 5706, loss 0.579268.
Train: 2018-08-09T11:16:39.231922: step 5707, loss 0.630226.
Train: 2018-08-09T11:16:39.310059: step 5708, loss 0.596154.
Train: 2018-08-09T11:16:39.388144: step 5709, loss 0.528393.
Train: 2018-08-09T11:16:39.466272: step 5710, loss 0.494701.
Test: 2018-08-09T11:16:39.952898: step 5710, loss 0.549768.
Train: 2018-08-09T11:16:40.046598: step 5711, loss 0.494859.
Train: 2018-08-09T11:16:40.124733: step 5712, loss 0.478181.
Train: 2018-08-09T11:16:40.202841: step 5713, loss 0.57908.
Train: 2018-08-09T11:16:40.280946: step 5714, loss 0.596498.
Train: 2018-08-09T11:16:40.359053: step 5715, loss 0.579372.
Train: 2018-08-09T11:16:40.437162: step 5716, loss 0.49477.
Train: 2018-08-09T11:16:40.530887: step 5717, loss 0.562753.
Train: 2018-08-09T11:16:40.608993: step 5718, loss 0.545609.
Train: 2018-08-09T11:16:40.687101: step 5719, loss 0.634447.
Train: 2018-08-09T11:16:40.765207: step 5720, loss 0.612968.
Test: 2018-08-09T11:16:41.265099: step 5720, loss 0.549171.
Train: 2018-08-09T11:16:41.343166: step 5721, loss 0.528771.
Train: 2018-08-09T11:16:41.421302: step 5722, loss 0.495223.
Train: 2018-08-09T11:16:41.499411: step 5723, loss 0.49525.
Train: 2018-08-09T11:16:41.577515: step 5724, loss 0.646386.
Train: 2018-08-09T11:16:41.655622: step 5725, loss 0.579179.
Train: 2018-08-09T11:16:41.733729: step 5726, loss 0.61272.
Train: 2018-08-09T11:16:41.827457: step 5727, loss 0.562403.
Train: 2018-08-09T11:16:41.907785: step 5728, loss 0.562418.
Train: 2018-08-09T11:16:41.984308: step 5729, loss 0.51237.
Train: 2018-08-09T11:16:42.062384: step 5730, loss 0.545757.
Test: 2018-08-09T11:16:42.562267: step 5730, loss 0.548077.
Train: 2018-08-09T11:16:42.640403: step 5731, loss 0.679087.
Train: 2018-08-09T11:16:42.718510: step 5732, loss 0.562451.
Train: 2018-08-09T11:16:42.796598: step 5733, loss 0.612223.
Train: 2018-08-09T11:16:42.874692: step 5734, loss 0.595573.
Train: 2018-08-09T11:16:42.952832: step 5735, loss 0.546039.
Train: 2018-08-09T11:16:43.046528: step 5736, loss 0.611867.
Train: 2018-08-09T11:16:43.124635: step 5737, loss 0.546179.
Train: 2018-08-09T11:16:43.202771: step 5738, loss 0.497213.
Train: 2018-08-09T11:16:43.280848: step 5739, loss 0.627905.
Train: 2018-08-09T11:16:43.374575: step 5740, loss 0.578921.
Test: 2018-08-09T11:16:43.874483: step 5740, loss 0.551739.
Train: 2018-08-09T11:16:43.952594: step 5741, loss 0.513919.
Train: 2018-08-09T11:16:44.030670: step 5742, loss 0.660039.
Train: 2018-08-09T11:16:44.108778: step 5743, loss 0.530348.
Train: 2018-08-09T11:16:44.186914: step 5744, loss 0.562735.
Train: 2018-08-09T11:16:44.265020: step 5745, loss 0.691736.
Train: 2018-08-09T11:16:44.358749: step 5746, loss 0.514622.
Train: 2018-08-09T11:16:44.436854: step 5747, loss 0.530799.
Train: 2018-08-09T11:16:44.514961: step 5748, loss 0.530873.
Train: 2018-08-09T11:16:44.593064: step 5749, loss 0.578862.
Train: 2018-08-09T11:16:44.671148: step 5750, loss 0.499023.
Test: 2018-08-09T11:16:45.173654: step 5750, loss 0.550336.
Train: 2018-08-09T11:16:45.251760: step 5751, loss 0.578862.
Train: 2018-08-09T11:16:45.329836: step 5752, loss 0.594838.
Train: 2018-08-09T11:16:45.407969: step 5753, loss 0.530948.
Train: 2018-08-09T11:16:45.486081: step 5754, loss 0.610816.
Train: 2018-08-09T11:16:45.564187: step 5755, loss 0.578862.
Train: 2018-08-09T11:16:45.642295: step 5756, loss 0.546946.
Train: 2018-08-09T11:16:45.735989: step 5757, loss 0.451229.
Train: 2018-08-09T11:16:45.814097: step 5758, loss 0.514927.
Train: 2018-08-09T11:16:45.892235: step 5759, loss 0.578866.
Train: 2018-08-09T11:16:45.970309: step 5760, loss 0.546751.
Test: 2018-08-09T11:16:46.461113: step 5760, loss 0.548307.
Train: 2018-08-09T11:16:46.539248: step 5761, loss 0.56278.
Train: 2018-08-09T11:16:46.632976: step 5762, loss 0.546623.
Train: 2018-08-09T11:16:46.711083: step 5763, loss 0.530395.
Train: 2018-08-09T11:16:46.789189: step 5764, loss 0.530281.
Train: 2018-08-09T11:16:46.867295: step 5765, loss 0.595163.
Train: 2018-08-09T11:16:46.945386: step 5766, loss 0.497486.
Train: 2018-08-09T11:16:47.023509: step 5767, loss 0.578935.
Train: 2018-08-09T11:16:47.101586: step 5768, loss 0.513464.
Train: 2018-08-09T11:16:47.195372: step 5769, loss 0.693967.
Train: 2018-08-09T11:16:47.273451: step 5770, loss 0.480405.
Test: 2018-08-09T11:16:47.773333: step 5770, loss 0.548976.
Train: 2018-08-09T11:16:47.851439: step 5771, loss 0.496697.
Train: 2018-08-09T11:16:47.929515: step 5772, loss 0.529491.
Train: 2018-08-09T11:16:48.007623: step 5773, loss 0.529366.
Train: 2018-08-09T11:16:48.085758: step 5774, loss 0.462748.
Train: 2018-08-09T11:16:48.179456: step 5775, loss 0.512357.
Train: 2018-08-09T11:16:48.257589: step 5776, loss 0.545635.
Train: 2018-08-09T11:16:48.335670: step 5777, loss 0.494433.
Train: 2018-08-09T11:16:48.413776: step 5778, loss 0.493751.
Train: 2018-08-09T11:16:48.491914: step 5779, loss 0.667034.
Train: 2018-08-09T11:16:48.569989: step 5780, loss 0.579843.
Test: 2018-08-09T11:16:49.071533: step 5780, loss 0.546699.
Train: 2018-08-09T11:16:49.149663: step 5781, loss 0.561653.
Train: 2018-08-09T11:16:49.227746: step 5782, loss 0.578216.
Train: 2018-08-09T11:16:49.321474: step 5783, loss 0.528536.
Train: 2018-08-09T11:16:49.399610: step 5784, loss 0.546616.
Train: 2018-08-09T11:16:49.477685: step 5785, loss 0.545648.
Train: 2018-08-09T11:16:49.555824: step 5786, loss 0.561678.
Train: 2018-08-09T11:16:49.633899: step 5787, loss 0.68386.
Train: 2018-08-09T11:16:49.712036: step 5788, loss 0.562643.
Train: 2018-08-09T11:16:49.790139: step 5789, loss 0.493763.
Train: 2018-08-09T11:16:49.883841: step 5790, loss 0.528156.
Test: 2018-08-09T11:16:50.368125: step 5790, loss 0.548252.
Train: 2018-08-09T11:16:50.446209: step 5791, loss 0.54524.
Train: 2018-08-09T11:16:50.539960: step 5792, loss 0.476824.
Train: 2018-08-09T11:16:50.618073: step 5793, loss 0.545221.
Train: 2018-08-09T11:16:50.696178: step 5794, loss 0.596609.
Train: 2018-08-09T11:16:50.774285: step 5795, loss 0.57948.
Train: 2018-08-09T11:16:50.852391: step 5796, loss 0.562331.
Train: 2018-08-09T11:16:50.931835: step 5797, loss 0.68229.
Train: 2018-08-09T11:16:51.009947: step 5798, loss 0.425559.
Train: 2018-08-09T11:16:51.088024: step 5799, loss 0.613612.
Train: 2018-08-09T11:16:51.181782: step 5800, loss 0.647735.
Test: 2018-08-09T11:16:51.666013: step 5800, loss 0.545738.
Train: 2018-08-09T11:16:52.193086: step 5801, loss 0.545301.
Train: 2018-08-09T11:16:52.271192: step 5802, loss 0.52836.
Train: 2018-08-09T11:16:52.364921: step 5803, loss 0.630228.
Train: 2018-08-09T11:16:52.443028: step 5804, loss 0.646976.
Train: 2018-08-09T11:16:52.521134: step 5805, loss 0.579223.
Train: 2018-08-09T11:16:52.599237: step 5806, loss 0.629573.
Train: 2018-08-09T11:16:52.677316: step 5807, loss 0.595845.
Train: 2018-08-09T11:16:52.755424: step 5808, loss 0.562445.
Train: 2018-08-09T11:16:52.833561: step 5809, loss 0.595598.
Train: 2018-08-09T11:16:52.918090: step 5810, loss 0.546029.
Test: 2018-08-09T11:16:53.418004: step 5810, loss 0.54965.
Train: 2018-08-09T11:16:53.496110: step 5811, loss 0.562551.
Train: 2018-08-09T11:16:53.574185: step 5812, loss 0.611667.
Train: 2018-08-09T11:16:53.667953: step 5813, loss 0.595216.
Train: 2018-08-09T11:16:53.746021: step 5814, loss 0.578901.
Train: 2018-08-09T11:16:53.824127: step 5815, loss 0.578887.
Train: 2018-08-09T11:16:53.907094: step 5816, loss 0.65935.
Train: 2018-08-09T11:16:53.985230: step 5817, loss 0.562854.
Train: 2018-08-09T11:16:54.063307: step 5818, loss 0.594796.
Train: 2018-08-09T11:16:54.141444: step 5819, loss 0.562999.
Train: 2018-08-09T11:16:54.219550: step 5820, loss 0.547272.
Test: 2018-08-09T11:16:54.719402: step 5820, loss 0.550756.
Train: 2018-08-09T11:16:54.797508: step 5821, loss 0.610352.
Train: 2018-08-09T11:16:54.891236: step 5822, loss 0.547508.
Train: 2018-08-09T11:16:54.969344: step 5823, loss 0.500695.
Train: 2018-08-09T11:16:55.047480: step 5824, loss 0.563271.
Train: 2018-08-09T11:16:55.125584: step 5825, loss 0.610096.
Train: 2018-08-09T11:16:55.203698: step 5826, loss 0.610052.
Train: 2018-08-09T11:16:55.281800: step 5827, loss 0.547822.
Train: 2018-08-09T11:16:55.375529: step 5828, loss 0.563392.
Train: 2018-08-09T11:16:55.453630: step 5829, loss 0.563416.
Train: 2018-08-09T11:16:55.531742: step 5830, loss 0.532453.
Test: 2018-08-09T11:16:56.031593: step 5830, loss 0.550085.
Train: 2018-08-09T11:16:56.109734: step 5831, loss 0.51696.
Train: 2018-08-09T11:16:56.187837: step 5832, loss 0.485851.
Train: 2018-08-09T11:16:56.269566: step 5833, loss 0.501134.
Train: 2018-08-09T11:16:56.347669: step 5834, loss 0.563266.
Train: 2018-08-09T11:16:56.425744: step 5835, loss 0.594558.
Train: 2018-08-09T11:16:56.503880: step 5836, loss 0.547436.
Train: 2018-08-09T11:16:56.597579: step 5837, loss 0.547254.
Train: 2018-08-09T11:16:56.675716: step 5838, loss 0.531279.
Train: 2018-08-09T11:16:56.753820: step 5839, loss 0.515089.
Train: 2018-08-09T11:16:56.831931: step 5840, loss 0.59474.
Test: 2018-08-09T11:16:57.331782: step 5840, loss 0.547738.
Train: 2018-08-09T11:16:57.409914: step 5841, loss 0.514444.
Train: 2018-08-09T11:16:57.488021: step 5842, loss 0.481646.
Train: 2018-08-09T11:16:57.566104: step 5843, loss 0.497478.
Train: 2018-08-09T11:16:57.644240: step 5844, loss 0.546567.
Train: 2018-08-09T11:16:57.737966: step 5845, loss 0.494283.
Train: 2018-08-09T11:16:57.816074: step 5846, loss 0.5947.
Train: 2018-08-09T11:16:57.894180: step 5847, loss 0.510593.
Train: 2018-08-09T11:16:57.972255: step 5848, loss 0.599655.
Train: 2018-08-09T11:16:58.050362: step 5849, loss 0.562334.
Train: 2018-08-09T11:16:58.128499: step 5850, loss 0.601512.
Test: 2018-08-09T11:16:58.628379: step 5850, loss 0.548441.
Train: 2018-08-09T11:16:58.706482: step 5851, loss 0.510449.
Train: 2018-08-09T11:16:58.800216: step 5852, loss 0.526146.
Train: 2018-08-09T11:16:58.878322: step 5853, loss 0.510271.
Train: 2018-08-09T11:16:58.956398: step 5854, loss 0.55126.
Train: 2018-08-09T11:16:59.034517: step 5855, loss 0.617063.
Train: 2018-08-09T11:16:59.112638: step 5856, loss 0.494288.
Train: 2018-08-09T11:16:59.190748: step 5857, loss 0.440728.
Train: 2018-08-09T11:16:59.284472: step 5858, loss 0.617553.
Train: 2018-08-09T11:16:59.362583: step 5859, loss 0.546678.
Train: 2018-08-09T11:16:59.440691: step 5860, loss 0.562791.
Test: 2018-08-09T11:16:59.941928: step 5860, loss 0.548334.
Train: 2018-08-09T11:17:00.020060: step 5861, loss 0.511722.
Train: 2018-08-09T11:17:00.098141: step 5862, loss 0.614245.
Train: 2018-08-09T11:17:00.176282: step 5863, loss 0.612999.
Train: 2018-08-09T11:17:00.254356: step 5864, loss 0.511747.
Train: 2018-08-09T11:17:00.332494: step 5865, loss 0.562428.
Train: 2018-08-09T11:17:00.426223: step 5866, loss 0.612898.
Train: 2018-08-09T11:17:00.504322: step 5867, loss 0.52871.
Train: 2018-08-09T11:17:00.582434: step 5868, loss 0.545597.
Train: 2018-08-09T11:17:00.660514: step 5869, loss 0.528719.
Train: 2018-08-09T11:17:00.738646: step 5870, loss 0.59602.
Test: 2018-08-09T11:17:01.238529: step 5870, loss 0.545993.
Train: 2018-08-09T11:17:01.316637: step 5871, loss 0.595984.
Train: 2018-08-09T11:17:01.394742: step 5872, loss 0.478446.
Train: 2018-08-09T11:17:01.488440: step 5873, loss 0.495132.
Train: 2018-08-09T11:17:01.566576: step 5874, loss 0.478059.
Train: 2018-08-09T11:17:01.644666: step 5875, loss 0.562687.
Train: 2018-08-09T11:17:01.722761: step 5876, loss 0.545574.
Train: 2018-08-09T11:17:01.800896: step 5877, loss 0.562244.
Train: 2018-08-09T11:17:01.878997: step 5878, loss 0.562304.
Train: 2018-08-09T11:17:01.974566: step 5879, loss 0.698371.
Train: 2018-08-09T11:17:02.052685: step 5880, loss 0.545663.
Test: 2018-08-09T11:17:02.552537: step 5880, loss 0.549652.
Train: 2018-08-09T11:17:02.630674: step 5881, loss 0.613488.
Train: 2018-08-09T11:17:02.708751: step 5882, loss 0.646881.
Train: 2018-08-09T11:17:02.786891: step 5883, loss 0.562372.
Train: 2018-08-09T11:17:02.864993: step 5884, loss 0.595969.
Train: 2018-08-09T11:17:02.943100: step 5885, loss 0.629344.
Train: 2018-08-09T11:17:03.021178: step 5886, loss 0.529104.
Train: 2018-08-09T11:17:03.114905: step 5887, loss 0.612281.
Train: 2018-08-09T11:17:03.193041: step 5888, loss 0.628659.
Train: 2018-08-09T11:17:03.271148: step 5889, loss 0.562514.
Train: 2018-08-09T11:17:03.349254: step 5890, loss 0.726453.
Test: 2018-08-09T11:17:03.849106: step 5890, loss 0.550481.
Train: 2018-08-09T11:17:03.928762: step 5891, loss 0.660374.
Train: 2018-08-09T11:17:04.006839: step 5892, loss 0.546665.
Train: 2018-08-09T11:17:04.084948: step 5893, loss 0.610583.
Train: 2018-08-09T11:17:04.163081: step 5894, loss 0.562956.
Train: 2018-08-09T11:17:04.241187: step 5895, loss 0.610283.
Train: 2018-08-09T11:17:04.319290: step 5896, loss 0.625085.
Train: 2018-08-09T11:17:04.397402: step 5897, loss 0.533919.
Train: 2018-08-09T11:17:04.491129: step 5898, loss 0.638856.
Train: 2018-08-09T11:17:04.569206: step 5899, loss 0.594992.
Train: 2018-08-09T11:17:04.647342: step 5900, loss 0.564573.
Test: 2018-08-09T11:17:05.147194: step 5900, loss 0.551506.
Train: 2018-08-09T11:17:05.693966: step 5901, loss 0.566853.
Train: 2018-08-09T11:17:05.787702: step 5902, loss 0.549382.
Train: 2018-08-09T11:17:05.865806: step 5903, loss 0.624966.
Train: 2018-08-09T11:17:05.945311: step 5904, loss 0.533356.
Train: 2018-08-09T11:17:06.023419: step 5905, loss 0.533766.
Train: 2018-08-09T11:17:06.101495: step 5906, loss 0.488669.
Train: 2018-08-09T11:17:06.179635: step 5907, loss 0.533716.
Train: 2018-08-09T11:17:06.257732: step 5908, loss 0.503313.
Train: 2018-08-09T11:17:06.351466: step 5909, loss 0.563853.
Train: 2018-08-09T11:17:06.429572: step 5910, loss 0.5028.
Test: 2018-08-09T11:17:06.913805: step 5910, loss 0.549341.
Train: 2018-08-09T11:17:06.991934: step 5911, loss 0.487141.
Train: 2018-08-09T11:17:07.085662: step 5912, loss 0.563593.
Train: 2018-08-09T11:17:07.163746: step 5913, loss 0.501535.
Train: 2018-08-09T11:17:07.241884: step 5914, loss 0.485486.
Train: 2018-08-09T11:17:07.319957: step 5915, loss 0.625959.
Train: 2018-08-09T11:17:07.398095: step 5916, loss 0.515695.
Train: 2018-08-09T11:17:07.491825: step 5917, loss 0.578813.
Train: 2018-08-09T11:17:07.569929: step 5918, loss 0.562985.
Train: 2018-08-09T11:17:07.648025: step 5919, loss 0.530529.
Train: 2018-08-09T11:17:07.726111: step 5920, loss 0.562572.
Test: 2018-08-09T11:17:08.228494: step 5920, loss 0.549853.
Train: 2018-08-09T11:17:08.306626: step 5921, loss 0.660084.
Train: 2018-08-09T11:17:08.384707: step 5922, loss 0.43214.
Train: 2018-08-09T11:17:08.462845: step 5923, loss 0.595396.
Train: 2018-08-09T11:17:08.540920: step 5924, loss 0.512767.
Train: 2018-08-09T11:17:08.619026: step 5925, loss 0.545274.
Train: 2018-08-09T11:17:08.712788: step 5926, loss 0.597643.
Train: 2018-08-09T11:17:08.790860: step 5927, loss 0.509369.
Train: 2018-08-09T11:17:08.868997: step 5928, loss 0.68297.
Train: 2018-08-09T11:17:08.947106: step 5929, loss 0.52796.
Train: 2018-08-09T11:17:09.025211: step 5930, loss 0.613097.
Test: 2018-08-09T11:17:09.525067: step 5930, loss 0.547561.
Train: 2018-08-09T11:17:09.603194: step 5931, loss 0.612122.
Train: 2018-08-09T11:17:09.681277: step 5932, loss 0.528138.
Train: 2018-08-09T11:17:09.775033: step 5933, loss 0.612284.
Train: 2018-08-09T11:17:09.853142: step 5934, loss 0.580755.
Train: 2018-08-09T11:17:09.932612: step 5935, loss 0.63248.
Train: 2018-08-09T11:17:10.010713: step 5936, loss 0.57891.
Train: 2018-08-09T11:17:10.088824: step 5937, loss 0.562577.
Train: 2018-08-09T11:17:10.166930: step 5938, loss 0.546172.
Train: 2018-08-09T11:17:10.245037: step 5939, loss 0.546311.
Train: 2018-08-09T11:17:10.323114: step 5940, loss 0.546326.
Test: 2018-08-09T11:17:10.822996: step 5940, loss 0.548108.
Train: 2018-08-09T11:17:10.916724: step 5941, loss 0.578864.
Train: 2018-08-09T11:17:10.994860: step 5942, loss 0.578873.
Train: 2018-08-09T11:17:11.072963: step 5943, loss 0.6111.
Train: 2018-08-09T11:17:11.151074: step 5944, loss 0.498575.
Train: 2018-08-09T11:17:11.229179: step 5945, loss 0.594927.
Train: 2018-08-09T11:17:11.307285: step 5946, loss 0.642999.
Train: 2018-08-09T11:17:11.385394: step 5947, loss 0.594853.
Train: 2018-08-09T11:17:11.463468: step 5948, loss 0.610746.
Train: 2018-08-09T11:17:11.557199: step 5949, loss 0.419918.
Train: 2018-08-09T11:17:11.635334: step 5950, loss 0.594745.
Test: 2018-08-09T11:17:12.120951: step 5950, loss 0.551103.
Train: 2018-08-09T11:17:12.214678: step 5951, loss 0.59474.
Train: 2018-08-09T11:17:12.292814: step 5952, loss 0.547136.
Train: 2018-08-09T11:17:12.370891: step 5953, loss 0.547164.
Train: 2018-08-09T11:17:12.448998: step 5954, loss 0.578855.
Train: 2018-08-09T11:17:12.527134: step 5955, loss 0.531317.
Train: 2018-08-09T11:17:12.605239: step 5956, loss 0.531301.
Train: 2018-08-09T11:17:12.683347: step 5957, loss 0.56298.
Train: 2018-08-09T11:17:12.777071: step 5958, loss 0.578869.
Train: 2018-08-09T11:17:12.855151: step 5959, loss 0.515172.
Train: 2018-08-09T11:17:12.933260: step 5960, loss 0.674472.
Test: 2018-08-09T11:17:13.433141: step 5960, loss 0.550364.
Train: 2018-08-09T11:17:13.511277: step 5961, loss 0.594932.
Train: 2018-08-09T11:17:13.589354: step 5962, loss 0.531112.
Train: 2018-08-09T11:17:13.683083: step 5963, loss 0.626569.
Train: 2018-08-09T11:17:13.761219: step 5964, loss 0.57887.
Train: 2018-08-09T11:17:13.839325: step 5965, loss 0.59471.
Train: 2018-08-09T11:17:13.917893: step 5966, loss 0.531322.
Train: 2018-08-09T11:17:13.996031: step 5967, loss 0.499817.
Train: 2018-08-09T11:17:14.074137: step 5968, loss 0.689671.
Train: 2018-08-09T11:17:14.152248: step 5969, loss 0.610497.
Train: 2018-08-09T11:17:14.230350: step 5970, loss 0.547371.
Test: 2018-08-09T11:17:14.730228: step 5970, loss 0.551357.
Train: 2018-08-09T11:17:14.823955: step 5971, loss 0.547388.
Train: 2018-08-09T11:17:14.902038: step 5972, loss 0.484522.
Train: 2018-08-09T11:17:14.980175: step 5973, loss 0.578866.
Train: 2018-08-09T11:17:15.058282: step 5974, loss 0.468524.
Train: 2018-08-09T11:17:15.136387: step 5975, loss 0.610447.
Train: 2018-08-09T11:17:15.214494: step 5976, loss 0.48379.
Train: 2018-08-09T11:17:15.292570: step 5977, loss 0.451499.
Train: 2018-08-09T11:17:15.386328: step 5978, loss 0.466859.
Train: 2018-08-09T11:17:15.464430: step 5979, loss 0.579582.
Train: 2018-08-09T11:17:15.542541: step 5980, loss 0.496717.
Test: 2018-08-09T11:17:16.044640: step 5980, loss 0.546165.
Train: 2018-08-09T11:17:16.122776: step 5981, loss 0.561587.
Train: 2018-08-09T11:17:16.200883: step 5982, loss 0.598207.
Train: 2018-08-09T11:17:16.278990: step 5983, loss 0.530281.
Train: 2018-08-09T11:17:16.372718: step 5984, loss 0.508287.
Train: 2018-08-09T11:17:16.450824: step 5985, loss 0.597373.
Train: 2018-08-09T11:17:16.528931: step 5986, loss 0.562483.
Train: 2018-08-09T11:17:16.607038: step 5987, loss 0.631763.
Train: 2018-08-09T11:17:16.685144: step 5988, loss 0.527767.
Train: 2018-08-09T11:17:16.763222: step 5989, loss 0.459447.
Train: 2018-08-09T11:17:16.841358: step 5990, loss 0.509528.
Test: 2018-08-09T11:17:17.341210: step 5990, loss 0.547239.
Train: 2018-08-09T11:17:17.429782: step 5991, loss 0.630686.
Train: 2018-08-09T11:17:17.495427: step 5992, loss 0.495399.
Train: 2018-08-09T11:17:17.589161: step 5993, loss 0.670616.
Train: 2018-08-09T11:17:17.667266: step 5994, loss 0.509312.
Train: 2018-08-09T11:17:17.745373: step 5995, loss 0.56184.
Train: 2018-08-09T11:17:17.823480: step 5996, loss 0.529106.
Train: 2018-08-09T11:17:17.901557: step 5997, loss 0.545286.
Train: 2018-08-09T11:17:17.983038: step 5998, loss 0.508328.
Train: 2018-08-09T11:17:18.061116: step 5999, loss 0.613538.
Train: 2018-08-09T11:17:18.154875: step 6000, loss 0.508066.
Test: 2018-08-09T11:17:18.654724: step 6000, loss 0.545882.
Train: 2018-08-09T11:17:19.201501: step 6001, loss 0.578325.
Train: 2018-08-09T11:17:19.279578: step 6002, loss 0.616223.
Train: 2018-08-09T11:17:19.373307: step 6003, loss 0.510307.
Train: 2018-08-09T11:17:19.451442: step 6004, loss 0.688225.
Train: 2018-08-09T11:17:19.529548: step 6005, loss 0.528781.
Train: 2018-08-09T11:17:19.607626: step 6006, loss 0.495833.
Train: 2018-08-09T11:17:19.685762: step 6007, loss 0.595858.
Train: 2018-08-09T11:17:19.763838: step 6008, loss 0.461235.
Train: 2018-08-09T11:17:19.857586: step 6009, loss 0.647277.
Train: 2018-08-09T11:17:19.936372: step 6010, loss 0.52767.
Test: 2018-08-09T11:17:20.436259: step 6010, loss 0.54673.
Train: 2018-08-09T11:17:20.514366: step 6011, loss 0.662721.
Train: 2018-08-09T11:17:20.592471: step 6012, loss 0.528686.
Train: 2018-08-09T11:17:20.670574: step 6013, loss 0.694835.
Train: 2018-08-09T11:17:20.748688: step 6014, loss 0.562888.
Train: 2018-08-09T11:17:20.826792: step 6015, loss 0.529923.
Train: 2018-08-09T11:17:20.920491: step 6016, loss 0.627743.
Train: 2018-08-09T11:17:20.998596: step 6017, loss 0.562602.
Train: 2018-08-09T11:17:21.076703: step 6018, loss 0.611248.
Train: 2018-08-09T11:17:21.154840: step 6019, loss 0.562819.
Train: 2018-08-09T11:17:21.232916: step 6020, loss 0.477427.
Test: 2018-08-09T11:17:21.732798: step 6020, loss 0.549096.
Train: 2018-08-09T11:17:21.810905: step 6021, loss 0.514901.
Train: 2018-08-09T11:17:21.889042: step 6022, loss 0.562879.
Train: 2018-08-09T11:17:21.982744: step 6023, loss 0.626843.
Train: 2018-08-09T11:17:22.060876: step 6024, loss 0.562893.
Train: 2018-08-09T11:17:22.138952: step 6025, loss 0.531025.
Train: 2018-08-09T11:17:22.217084: step 6026, loss 0.531029.
Train: 2018-08-09T11:17:22.295196: step 6027, loss 0.531007.
Train: 2018-08-09T11:17:22.388894: step 6028, loss 0.45112.
Train: 2018-08-09T11:17:22.467034: step 6029, loss 0.546847.
Train: 2018-08-09T11:17:22.545107: step 6030, loss 0.53065.
Test: 2018-08-09T11:17:23.046514: step 6030, loss 0.550038.
Train: 2018-08-09T11:17:23.124651: step 6031, loss 0.54661.
Train: 2018-08-09T11:17:23.202757: step 6032, loss 0.627596.
Train: 2018-08-09T11:17:23.280871: step 6033, loss 0.51389.
Train: 2018-08-09T11:17:23.358970: step 6034, loss 0.529986.
Train: 2018-08-09T11:17:23.437077: step 6035, loss 0.480331.
Train: 2018-08-09T11:17:23.530809: step 6036, loss 0.529892.
Train: 2018-08-09T11:17:23.608913: step 6037, loss 0.511934.
Train: 2018-08-09T11:17:23.687018: step 6038, loss 0.5279.
Train: 2018-08-09T11:17:23.765123: step 6039, loss 0.509735.
Train: 2018-08-09T11:17:23.843231: step 6040, loss 0.618139.
Test: 2018-08-09T11:17:24.343084: step 6040, loss 0.549164.
Train: 2018-08-09T11:17:24.421221: step 6041, loss 0.564209.
Train: 2018-08-09T11:17:24.499327: step 6042, loss 0.619974.
Train: 2018-08-09T11:17:24.577434: step 6043, loss 0.665828.
Train: 2018-08-09T11:17:24.655544: step 6044, loss 0.647934.
Train: 2018-08-09T11:17:24.749267: step 6045, loss 0.561815.
Train: 2018-08-09T11:17:24.827345: step 6046, loss 0.4954.
Train: 2018-08-09T11:17:24.907148: step 6047, loss 0.512841.
Train: 2018-08-09T11:17:24.985288: step 6048, loss 0.512315.
Train: 2018-08-09T11:17:25.063395: step 6049, loss 0.579144.
Train: 2018-08-09T11:17:25.141499: step 6050, loss 0.562391.
Test: 2018-08-09T11:17:25.641351: step 6050, loss 0.550653.
Train: 2018-08-09T11:17:25.719481: step 6051, loss 0.446139.
Train: 2018-08-09T11:17:25.797593: step 6052, loss 0.562343.
Train: 2018-08-09T11:17:25.875701: step 6053, loss 0.529103.
Train: 2018-08-09T11:17:25.969398: step 6054, loss 0.528747.
Train: 2018-08-09T11:17:26.047534: step 6055, loss 0.713185.
Train: 2018-08-09T11:17:26.125647: step 6056, loss 0.629372.
Train: 2018-08-09T11:17:26.203748: step 6057, loss 0.629772.
Train: 2018-08-09T11:17:26.281855: step 6058, loss 0.579179.
Train: 2018-08-09T11:17:26.359958: step 6059, loss 0.495672.
Train: 2018-08-09T11:17:26.453683: step 6060, loss 0.645313.
Test: 2018-08-09T11:17:26.939287: step 6060, loss 0.548167.
Train: 2018-08-09T11:17:27.017422: step 6061, loss 0.761451.
Train: 2018-08-09T11:17:27.111144: step 6062, loss 0.480227.
Train: 2018-08-09T11:17:27.189258: step 6063, loss 0.529615.
Train: 2018-08-09T11:17:27.269002: step 6064, loss 0.513441.
Train: 2018-08-09T11:17:27.347138: step 6065, loss 0.529992.
Train: 2018-08-09T11:17:27.425244: step 6066, loss 0.529962.
Train: 2018-08-09T11:17:27.503321: step 6067, loss 0.562074.
Train: 2018-08-09T11:17:27.581458: step 6068, loss 0.61142.
Train: 2018-08-09T11:17:27.675189: step 6069, loss 0.513404.
Train: 2018-08-09T11:17:27.753292: step 6070, loss 0.513285.
Test: 2018-08-09T11:17:28.253145: step 6070, loss 0.551296.
Train: 2018-08-09T11:17:28.331276: step 6071, loss 0.478411.
Train: 2018-08-09T11:17:28.409387: step 6072, loss 0.613063.
Train: 2018-08-09T11:17:28.487490: step 6073, loss 0.527632.
Train: 2018-08-09T11:17:28.565601: step 6074, loss 0.543302.
Train: 2018-08-09T11:17:28.643678: step 6075, loss 0.579173.
Train: 2018-08-09T11:17:28.737437: step 6076, loss 0.548412.
Train: 2018-08-09T11:17:28.815542: step 6077, loss 0.418095.
Train: 2018-08-09T11:17:28.893648: step 6078, loss 0.487141.
Train: 2018-08-09T11:17:28.971756: step 6079, loss 0.493072.
Train: 2018-08-09T11:17:29.049831: step 6080, loss 0.501454.
Test: 2018-08-09T11:17:29.549766: step 6080, loss 0.550093.
Train: 2018-08-09T11:17:29.627846: step 6081, loss 0.418106.
Train: 2018-08-09T11:17:29.705957: step 6082, loss 0.718802.
Train: 2018-08-09T11:17:29.799680: step 6083, loss 0.614888.
Train: 2018-08-09T11:17:29.877792: step 6084, loss 0.552994.
Train: 2018-08-09T11:17:29.955867: step 6085, loss 0.672084.
Train: 2018-08-09T11:17:30.034004: step 6086, loss 0.629306.
Train: 2018-08-09T11:17:30.112081: step 6087, loss 0.544995.
Train: 2018-08-09T11:17:30.237052: step 6088, loss 0.612881.
Train: 2018-08-09T11:17:30.315190: step 6089, loss 0.529917.
Train: 2018-08-09T11:17:30.393295: step 6090, loss 0.530235.
Test: 2018-08-09T11:17:30.893148: step 6090, loss 0.551132.
Train: 2018-08-09T11:17:30.972603: step 6091, loss 0.611071.
Train: 2018-08-09T11:17:31.050711: step 6092, loss 0.562078.
Train: 2018-08-09T11:17:31.128786: step 6093, loss 0.562498.
Train: 2018-08-09T11:17:31.206892: step 6094, loss 0.513921.
Train: 2018-08-09T11:17:31.285032: step 6095, loss 0.497659.
Train: 2018-08-09T11:17:31.378760: step 6096, loss 0.562765.
Train: 2018-08-09T11:17:31.456864: step 6097, loss 0.481547.
Train: 2018-08-09T11:17:31.534966: step 6098, loss 0.465808.
Train: 2018-08-09T11:17:31.613080: step 6099, loss 0.594799.
Train: 2018-08-09T11:17:31.691185: step 6100, loss 0.594019.
Test: 2018-08-09T11:17:32.191061: step 6100, loss 0.549733.
Train: 2018-08-09T11:17:32.800299: step 6101, loss 0.660962.
Train: 2018-08-09T11:17:32.878404: step 6102, loss 0.611757.
Train: 2018-08-09T11:17:32.974420: step 6103, loss 0.59572.
Train: 2018-08-09T11:17:33.048554: step 6104, loss 0.676215.
Train: 2018-08-09T11:17:33.126685: step 6105, loss 0.610731.
Train: 2018-08-09T11:17:33.220397: step 6106, loss 0.610952.
Train: 2018-08-09T11:17:33.298519: step 6107, loss 0.610979.
Train: 2018-08-09T11:17:33.376603: step 6108, loss 0.563081.
Train: 2018-08-09T11:17:33.454740: step 6109, loss 0.483966.
Train: 2018-08-09T11:17:33.532847: step 6110, loss 0.515773.
Test: 2018-08-09T11:17:34.032714: step 6110, loss 0.548892.
Train: 2018-08-09T11:17:34.142071: step 6111, loss 0.563274.
Train: 2018-08-09T11:17:34.220183: step 6112, loss 0.657701.
Train: 2018-08-09T11:17:34.298259: step 6113, loss 0.626256.
Train: 2018-08-09T11:17:34.376366: step 6114, loss 0.500488.
Train: 2018-08-09T11:17:34.454503: step 6115, loss 0.672793.
Train: 2018-08-09T11:17:34.548203: step 6116, loss 0.547641.
Train: 2018-08-09T11:17:34.626338: step 6117, loss 0.501103.
Train: 2018-08-09T11:17:34.704445: step 6118, loss 0.532206.
Train: 2018-08-09T11:17:34.782522: step 6119, loss 0.438784.
Train: 2018-08-09T11:17:34.860658: step 6120, loss 0.547222.
Test: 2018-08-09T11:17:35.365937: step 6120, loss 0.548789.
Train: 2018-08-09T11:17:35.444071: step 6121, loss 0.657918.
Train: 2018-08-09T11:17:35.522148: step 6122, loss 0.514949.
Train: 2018-08-09T11:17:35.600257: step 6123, loss 0.547719.
Train: 2018-08-09T11:17:35.678361: step 6124, loss 0.532246.
Train: 2018-08-09T11:17:35.756499: step 6125, loss 0.563392.
Train: 2018-08-09T11:17:35.834606: step 6126, loss 0.580279.
Train: 2018-08-09T11:17:35.928333: step 6127, loss 0.644815.
Train: 2018-08-09T11:17:36.006440: step 6128, loss 0.466157.
Train: 2018-08-09T11:17:36.084547: step 6129, loss 0.593866.
Train: 2018-08-09T11:17:36.162653: step 6130, loss 0.577645.
Test: 2018-08-09T11:17:36.662504: step 6130, loss 0.548823.
Train: 2018-08-09T11:17:36.740641: step 6131, loss 0.562944.
Train: 2018-08-09T11:17:36.818750: step 6132, loss 0.465199.
Train: 2018-08-09T11:17:36.896828: step 6133, loss 0.611106.
Train: 2018-08-09T11:17:36.974961: step 6134, loss 0.577593.
Train: 2018-08-09T11:17:37.053069: step 6135, loss 0.528571.
Train: 2018-08-09T11:17:37.131176: step 6136, loss 0.579095.
Train: 2018-08-09T11:17:37.224902: step 6137, loss 0.579667.
Train: 2018-08-09T11:17:37.302979: step 6138, loss 0.614118.
Train: 2018-08-09T11:17:37.381087: step 6139, loss 0.598391.
Train: 2018-08-09T11:17:37.459222: step 6140, loss 0.532732.
Test: 2018-08-09T11:17:37.961511: step 6140, loss 0.548289.
Train: 2018-08-09T11:17:38.039623: step 6141, loss 0.481308.
Train: 2018-08-09T11:17:38.117699: step 6142, loss 0.49697.
Train: 2018-08-09T11:17:38.211452: step 6143, loss 0.529237.
Train: 2018-08-09T11:17:38.289564: step 6144, loss 0.645552.
Train: 2018-08-09T11:17:38.367671: step 6145, loss 0.580218.
Train: 2018-08-09T11:17:38.445748: step 6146, loss 0.627879.
Train: 2018-08-09T11:17:38.523853: step 6147, loss 0.545491.
Train: 2018-08-09T11:17:38.601990: step 6148, loss 0.530342.
Train: 2018-08-09T11:17:38.695713: step 6149, loss 0.546285.
Train: 2018-08-09T11:17:38.773794: step 6150, loss 0.514466.
Test: 2018-08-09T11:17:39.258055: step 6150, loss 0.549311.
Train: 2018-08-09T11:17:39.336192: step 6151, loss 0.546928.
Train: 2018-08-09T11:17:39.414299: step 6152, loss 0.546293.
Train: 2018-08-09T11:17:39.508024: step 6153, loss 0.57864.
Train: 2018-08-09T11:17:39.586103: step 6154, loss 0.546843.
Train: 2018-08-09T11:17:39.664239: step 6155, loss 0.578152.
Train: 2018-08-09T11:17:39.742318: step 6156, loss 0.595443.
Train: 2018-08-09T11:17:39.820452: step 6157, loss 0.480562.
Train: 2018-08-09T11:17:39.898559: step 6158, loss 0.59568.
Train: 2018-08-09T11:17:39.978998: step 6159, loss 0.529899.
Train: 2018-08-09T11:17:40.057135: step 6160, loss 0.595675.
Test: 2018-08-09T11:17:40.556988: step 6160, loss 0.547661.
Train: 2018-08-09T11:17:40.635123: step 6161, loss 0.595595.
Train: 2018-08-09T11:17:40.713201: step 6162, loss 0.513433.
Train: 2018-08-09T11:17:40.806965: step 6163, loss 0.562664.
Train: 2018-08-09T11:17:40.885064: step 6164, loss 0.54659.
Train: 2018-08-09T11:17:40.963171: step 6165, loss 0.545885.
Train: 2018-08-09T11:17:41.041279: step 6166, loss 0.544967.
Train: 2018-08-09T11:17:41.119384: step 6167, loss 0.478798.
Train: 2018-08-09T11:17:41.197490: step 6168, loss 0.681828.
Train: 2018-08-09T11:17:41.275605: step 6169, loss 0.545289.
Train: 2018-08-09T11:17:41.353704: step 6170, loss 0.579019.
Test: 2018-08-09T11:17:41.853581: step 6170, loss 0.548655.
Train: 2018-08-09T11:17:41.933090: step 6171, loss 0.645641.
Train: 2018-08-09T11:17:42.011196: step 6172, loss 0.710329.
Train: 2018-08-09T11:17:42.104919: step 6173, loss 0.546695.
Train: 2018-08-09T11:17:42.183031: step 6174, loss 0.513648.
Train: 2018-08-09T11:17:42.261137: step 6175, loss 0.546245.
Train: 2018-08-09T11:17:42.339240: step 6176, loss 0.61229.
Train: 2018-08-09T11:17:42.417350: step 6177, loss 0.595095.
Train: 2018-08-09T11:17:42.495457: step 6178, loss 0.515134.
Train: 2018-08-09T11:17:42.589186: step 6179, loss 0.515382.
Train: 2018-08-09T11:17:42.667291: step 6180, loss 0.530822.
Test: 2018-08-09T11:17:43.167144: step 6180, loss 0.549781.
Train: 2018-08-09T11:17:43.245280: step 6181, loss 0.531097.
Train: 2018-08-09T11:17:43.323388: step 6182, loss 0.594839.
Train: 2018-08-09T11:17:43.401465: step 6183, loss 0.594715.
Train: 2018-08-09T11:17:43.479573: step 6184, loss 0.642766.
Train: 2018-08-09T11:17:43.557707: step 6185, loss 0.547074.
Train: 2018-08-09T11:17:43.651405: step 6186, loss 0.610707.
Train: 2018-08-09T11:17:43.729541: step 6187, loss 0.515424.
Train: 2018-08-09T11:17:43.807642: step 6188, loss 0.59471.
Train: 2018-08-09T11:17:43.885757: step 6189, loss 0.642189.
Train: 2018-08-09T11:17:43.966134: step 6190, loss 0.484098.
Test: 2018-08-09T11:17:44.466015: step 6190, loss 0.548881.
Train: 2018-08-09T11:17:44.544147: step 6191, loss 0.56307.
Train: 2018-08-09T11:17:44.622259: step 6192, loss 0.578852.
Train: 2018-08-09T11:17:44.700369: step 6193, loss 0.594654.
Train: 2018-08-09T11:17:44.778473: step 6194, loss 0.610394.
Train: 2018-08-09T11:17:44.872201: step 6195, loss 0.500126.
Train: 2018-08-09T11:17:44.950310: step 6196, loss 0.452845.
Train: 2018-08-09T11:17:45.028384: step 6197, loss 0.484095.
Train: 2018-08-09T11:17:45.106522: step 6198, loss 0.642317.
Train: 2018-08-09T11:17:45.184625: step 6199, loss 0.483395.
Train: 2018-08-09T11:17:45.262703: step 6200, loss 0.466659.
Test: 2018-08-09T11:17:45.762586: step 6200, loss 0.550023.
Train: 2018-08-09T11:17:46.326590: step 6201, loss 0.626745.
Train: 2018-08-09T11:17:46.404667: step 6202, loss 0.465244.
Train: 2018-08-09T11:17:46.482773: step 6203, loss 0.544725.
Train: 2018-08-09T11:17:46.560880: step 6204, loss 0.64748.
Train: 2018-08-09T11:17:46.638985: step 6205, loss 0.579904.
Train: 2018-08-09T11:17:46.717119: step 6206, loss 0.529521.
Train: 2018-08-09T11:17:46.810820: step 6207, loss 0.668361.
Train: 2018-08-09T11:17:46.888957: step 6208, loss 0.612234.
Train: 2018-08-09T11:17:46.967034: step 6209, loss 0.496035.
Train: 2018-08-09T11:17:47.045170: step 6210, loss 0.543971.
Test: 2018-08-09T11:17:47.545030: step 6210, loss 0.550837.
Train: 2018-08-09T11:17:47.623128: step 6211, loss 0.496419.
Train: 2018-08-09T11:17:47.701267: step 6212, loss 0.595472.
Train: 2018-08-09T11:17:47.779343: step 6213, loss 0.463721.
Train: 2018-08-09T11:17:47.873094: step 6214, loss 0.628661.
Train: 2018-08-09T11:17:47.951766: step 6215, loss 0.497513.
Train: 2018-08-09T11:17:48.029904: step 6216, loss 0.613617.
Train: 2018-08-09T11:17:48.108010: step 6217, loss 0.546855.
Train: 2018-08-09T11:17:48.186118: step 6218, loss 0.509401.
Train: 2018-08-09T11:17:48.264219: step 6219, loss 0.560039.
Train: 2018-08-09T11:17:48.342330: step 6220, loss 0.473707.
Test: 2018-08-09T11:17:48.840188: step 6220, loss 0.548516.
Train: 2018-08-09T11:17:48.918275: step 6221, loss 0.52858.
Train: 2018-08-09T11:17:48.996380: step 6222, loss 0.705155.
Train: 2018-08-09T11:17:49.090111: step 6223, loss 0.58108.
Train: 2018-08-09T11:17:49.168217: step 6224, loss 0.584314.
Train: 2018-08-09T11:17:49.246326: step 6225, loss 0.545112.
Train: 2018-08-09T11:17:49.324424: step 6226, loss 0.544259.
Train: 2018-08-09T11:17:49.402535: step 6227, loss 0.628464.
Train: 2018-08-09T11:17:49.480642: step 6228, loss 0.547051.
Train: 2018-08-09T11:17:49.574366: step 6229, loss 0.710744.
Train: 2018-08-09T11:17:49.652477: step 6230, loss 0.546559.
Test: 2018-08-09T11:17:50.152343: step 6230, loss 0.547455.
Train: 2018-08-09T11:17:50.230435: step 6231, loss 0.498381.
Train: 2018-08-09T11:17:50.308572: step 6232, loss 0.497823.
Train: 2018-08-09T11:17:50.386667: step 6233, loss 0.546719.
Train: 2018-08-09T11:17:50.464755: step 6234, loss 0.514216.
Train: 2018-08-09T11:17:50.542893: step 6235, loss 0.643398.
Train: 2018-08-09T11:17:50.636591: step 6236, loss 0.627442.
Train: 2018-08-09T11:17:50.714729: step 6237, loss 0.546642.
Train: 2018-08-09T11:17:50.792834: step 6238, loss 0.562709.
Train: 2018-08-09T11:17:50.870946: step 6239, loss 0.594826.
Train: 2018-08-09T11:17:50.951440: step 6240, loss 0.578859.
Test: 2018-08-09T11:17:51.451292: step 6240, loss 0.548464.
Train: 2018-08-09T11:17:51.529428: step 6241, loss 0.530839.
Train: 2018-08-09T11:17:51.607535: step 6242, loss 0.562814.
Train: 2018-08-09T11:17:51.685637: step 6243, loss 0.530929.
Train: 2018-08-09T11:17:51.779370: step 6244, loss 0.482888.
Train: 2018-08-09T11:17:51.857445: step 6245, loss 0.562879.
Train: 2018-08-09T11:17:51.935583: step 6246, loss 0.482517.
Train: 2018-08-09T11:17:52.013689: step 6247, loss 0.530253.
Train: 2018-08-09T11:17:52.091795: step 6248, loss 0.546236.
Train: 2018-08-09T11:17:52.169883: step 6249, loss 0.579106.
Train: 2018-08-09T11:17:52.248009: step 6250, loss 0.594269.
Test: 2018-08-09T11:17:52.747861: step 6250, loss 0.545325.
Train: 2018-08-09T11:17:52.841614: step 6251, loss 0.562161.
Train: 2018-08-09T11:17:52.921147: step 6252, loss 0.490565.
Train: 2018-08-09T11:17:52.999257: step 6253, loss 0.528037.
Train: 2018-08-09T11:17:53.077386: step 6254, loss 0.580081.
Train: 2018-08-09T11:17:53.155498: step 6255, loss 0.581252.
Train: 2018-08-09T11:17:53.233609: step 6256, loss 0.577094.
Train: 2018-08-09T11:17:53.311683: step 6257, loss 0.691554.
Train: 2018-08-09T11:17:53.405443: step 6258, loss 0.527655.
Train: 2018-08-09T11:17:53.483549: step 6259, loss 0.659487.
Train: 2018-08-09T11:17:53.561621: step 6260, loss 0.5301.
Test: 2018-08-09T11:17:54.061535: step 6260, loss 0.548259.
Train: 2018-08-09T11:17:54.139640: step 6261, loss 0.481659.
Train: 2018-08-09T11:17:54.217745: step 6262, loss 0.611087.
Train: 2018-08-09T11:17:54.295855: step 6263, loss 0.562716.
Train: 2018-08-09T11:17:54.389582: step 6264, loss 0.481524.
Train: 2018-08-09T11:17:54.467689: step 6265, loss 0.562731.
Train: 2018-08-09T11:17:54.545765: step 6266, loss 0.579116.
Train: 2018-08-09T11:17:54.623902: step 6267, loss 0.562661.
Train: 2018-08-09T11:17:54.701980: step 6268, loss 0.611162.
Train: 2018-08-09T11:17:54.795733: step 6269, loss 0.546643.
Train: 2018-08-09T11:17:54.873846: step 6270, loss 0.546646.
Test: 2018-08-09T11:17:55.358860: step 6270, loss 0.551328.
Train: 2018-08-09T11:17:55.452611: step 6271, loss 0.627201.
Train: 2018-08-09T11:17:55.530693: step 6272, loss 0.546692.
Train: 2018-08-09T11:17:55.608801: step 6273, loss 0.53063.
Train: 2018-08-09T11:17:55.686908: step 6274, loss 0.627104.
Train: 2018-08-09T11:17:55.765014: step 6275, loss 0.578872.
Train: 2018-08-09T11:17:55.858773: step 6276, loss 0.562835.
Train: 2018-08-09T11:17:55.936968: step 6277, loss 0.514768.
Train: 2018-08-09T11:17:56.015105: step 6278, loss 0.578862.
Train: 2018-08-09T11:17:56.093208: step 6279, loss 0.723062.
Train: 2018-08-09T11:17:56.171318: step 6280, loss 0.562918.
Test: 2018-08-09T11:17:56.671171: step 6280, loss 0.548623.
Train: 2018-08-09T11:17:56.749306: step 6281, loss 0.435571.
Train: 2018-08-09T11:17:56.827418: step 6282, loss 0.562937.
Train: 2018-08-09T11:17:56.921142: step 6283, loss 0.626683.
Train: 2018-08-09T11:17:56.999217: step 6284, loss 0.531079.
Train: 2018-08-09T11:17:57.077354: step 6285, loss 0.483308.
Train: 2018-08-09T11:17:57.155431: step 6286, loss 0.483111.
Train: 2018-08-09T11:17:57.233568: step 6287, loss 0.530783.
Train: 2018-08-09T11:17:57.311674: step 6288, loss 0.610986.
Train: 2018-08-09T11:17:57.405373: step 6289, loss 0.643638.
Train: 2018-08-09T11:17:57.483510: step 6290, loss 0.627209.
Test: 2018-08-09T11:17:57.969088: step 6290, loss 0.549524.
Train: 2018-08-09T11:17:58.047225: step 6291, loss 0.546549.
Train: 2018-08-09T11:17:58.125332: step 6292, loss 0.578818.
Train: 2018-08-09T11:17:58.219057: step 6293, loss 0.610984.
Train: 2018-08-09T11:17:58.298852: step 6294, loss 0.643172.
Train: 2018-08-09T11:17:58.376957: step 6295, loss 0.514755.
Train: 2018-08-09T11:17:58.455061: step 6296, loss 0.530925.
Train: 2018-08-09T11:17:58.533170: step 6297, loss 0.610887.
Train: 2018-08-09T11:17:58.611247: step 6298, loss 0.594721.
Train: 2018-08-09T11:17:58.705006: step 6299, loss 0.594736.
Train: 2018-08-09T11:17:58.783112: step 6300, loss 0.563011.
Test: 2018-08-09T11:17:59.282980: step 6300, loss 0.548075.
Train: 2018-08-09T11:17:59.829740: step 6301, loss 0.515263.
Train: 2018-08-09T11:17:59.907847: step 6302, loss 0.578873.
Train: 2018-08-09T11:17:59.985959: step 6303, loss 0.547265.
Train: 2018-08-09T11:18:00.064055: step 6304, loss 0.610544.
Train: 2018-08-09T11:18:00.157783: step 6305, loss 0.626354.
Train: 2018-08-09T11:18:00.235891: step 6306, loss 0.579104.
Train: 2018-08-09T11:18:00.314002: step 6307, loss 0.563242.
Train: 2018-08-09T11:18:00.392077: step 6308, loss 0.547405.
Train: 2018-08-09T11:18:00.470219: step 6309, loss 0.641732.
Train: 2018-08-09T11:18:00.548321: step 6310, loss 0.468815.
Test: 2018-08-09T11:18:01.048198: step 6310, loss 0.548385.
Train: 2018-08-09T11:18:01.126309: step 6311, loss 0.594523.
Train: 2018-08-09T11:18:01.220032: step 6312, loss 0.594432.
Train: 2018-08-09T11:18:01.298147: step 6313, loss 0.531503.
Train: 2018-08-09T11:18:01.376251: step 6314, loss 0.67335.
Train: 2018-08-09T11:18:01.454358: step 6315, loss 0.608534.
Train: 2018-08-09T11:18:01.532460: step 6316, loss 0.57893.
Train: 2018-08-09T11:18:01.610572: step 6317, loss 0.563133.
Train: 2018-08-09T11:18:01.704299: step 6318, loss 0.532878.
Train: 2018-08-09T11:18:01.782406: step 6319, loss 0.580957.
Train: 2018-08-09T11:18:01.860511: step 6320, loss 0.533167.
Test: 2018-08-09T11:18:02.347106: step 6320, loss 0.554163.
Train: 2018-08-09T11:18:02.425208: step 6321, loss 0.545391.
Train: 2018-08-09T11:18:02.503317: step 6322, loss 0.609212.
Train: 2018-08-09T11:18:02.597013: step 6323, loss 0.563636.
Train: 2018-08-09T11:18:02.675149: step 6324, loss 0.627236.
Train: 2018-08-09T11:18:02.753226: step 6325, loss 0.563588.
Train: 2018-08-09T11:18:02.831364: step 6326, loss 0.548995.
Train: 2018-08-09T11:18:02.909469: step 6327, loss 0.60967.
Train: 2018-08-09T11:18:02.987573: step 6328, loss 0.517569.
Train: 2018-08-09T11:18:03.081304: step 6329, loss 0.548293.
Train: 2018-08-09T11:18:03.159410: step 6330, loss 0.502189.
Test: 2018-08-09T11:18:03.659263: step 6330, loss 0.551481.
Train: 2018-08-09T11:18:03.737401: step 6331, loss 0.563577.
Train: 2018-08-09T11:18:03.815505: step 6332, loss 0.609779.
Train: 2018-08-09T11:18:03.893616: step 6333, loss 0.563503.
Train: 2018-08-09T11:18:03.974032: step 6334, loss 0.656206.
Train: 2018-08-09T11:18:04.052164: step 6335, loss 0.563493.
Train: 2018-08-09T11:18:04.148034: step 6336, loss 0.702476.
Train: 2018-08-09T11:18:04.215016: step 6337, loss 0.440375.
Train: 2018-08-09T11:18:04.293120: step 6338, loss 0.609767.
Train: 2018-08-09T11:18:04.386844: step 6339, loss 0.62513.
Train: 2018-08-09T11:18:04.464962: step 6340, loss 0.517473.
Test: 2018-08-09T11:18:04.964807: step 6340, loss 0.55152.
Train: 2018-08-09T11:18:05.042937: step 6341, loss 0.625082.
Train: 2018-08-09T11:18:05.121019: step 6342, loss 0.486853.
Train: 2018-08-09T11:18:05.199156: step 6343, loss 0.625066.
Train: 2018-08-09T11:18:05.277259: step 6344, loss 0.563609.
Train: 2018-08-09T11:18:05.370991: step 6345, loss 0.640416.
Train: 2018-08-09T11:18:05.449099: step 6346, loss 0.609666.
Train: 2018-08-09T11:18:05.527204: step 6347, loss 0.578991.
Train: 2018-08-09T11:18:05.605281: step 6348, loss 0.563711.
Train: 2018-08-09T11:18:05.699010: step 6349, loss 0.609554.
Train: 2018-08-09T11:18:05.777145: step 6350, loss 0.548534.
Test: 2018-08-09T11:18:06.276998: step 6350, loss 0.549527.
Train: 2018-08-09T11:18:06.355105: step 6351, loss 0.502869.
Train: 2018-08-09T11:18:06.433242: step 6352, loss 0.563776.
Train: 2018-08-09T11:18:06.511348: step 6353, loss 0.53329.
Train: 2018-08-09T11:18:06.589457: step 6354, loss 0.578973.
Train: 2018-08-09T11:18:06.683182: step 6355, loss 0.640258.
Train: 2018-08-09T11:18:06.761289: step 6356, loss 0.686141.
Train: 2018-08-09T11:18:06.839398: step 6357, loss 0.487435.
Train: 2018-08-09T11:18:06.917502: step 6358, loss 0.563758.
Train: 2018-08-09T11:18:06.995580: step 6359, loss 0.533233.
Train: 2018-08-09T11:18:07.073715: step 6360, loss 0.563735.
Test: 2018-08-09T11:18:07.573567: step 6360, loss 0.552272.
Train: 2018-08-09T11:18:07.651693: step 6361, loss 0.517832.
Train: 2018-08-09T11:18:07.729810: step 6362, loss 0.563661.
Train: 2018-08-09T11:18:07.823539: step 6363, loss 0.594329.
Train: 2018-08-09T11:18:07.901645: step 6364, loss 0.548192.
Train: 2018-08-09T11:18:07.982163: step 6365, loss 0.486473.
Train: 2018-08-09T11:18:08.060243: step 6366, loss 0.563458.
Train: 2018-08-09T11:18:08.138355: step 6367, loss 0.625491.
Train: 2018-08-09T11:18:08.216482: step 6368, loss 0.625547.
Train: 2018-08-09T11:18:08.310205: step 6369, loss 0.532236.
Train: 2018-08-09T11:18:08.388317: step 6370, loss 0.516595.
Test: 2018-08-09T11:18:08.872548: step 6370, loss 0.550398.
Train: 2018-08-09T11:18:08.950679: step 6371, loss 0.547639.
Train: 2018-08-09T11:18:09.028760: step 6372, loss 0.594529.
Train: 2018-08-09T11:18:09.122490: step 6373, loss 0.594599.
Train: 2018-08-09T11:18:09.200600: step 6374, loss 0.578873.
Train: 2018-08-09T11:18:09.278734: step 6375, loss 0.531658.
Train: 2018-08-09T11:18:09.356809: step 6376, loss 0.515792.
Train: 2018-08-09T11:18:09.434916: step 6377, loss 0.531393.
Train: 2018-08-09T11:18:09.513052: step 6378, loss 0.562976.
Train: 2018-08-09T11:18:09.606781: step 6379, loss 0.530863.
Train: 2018-08-09T11:18:09.684887: step 6380, loss 0.514938.
Test: 2018-08-09T11:18:10.186105: step 6380, loss 0.551251.
Train: 2018-08-09T11:18:10.264242: step 6381, loss 0.497688.
Train: 2018-08-09T11:18:10.342347: step 6382, loss 0.562841.
Train: 2018-08-09T11:18:10.420449: step 6383, loss 0.595252.
Train: 2018-08-09T11:18:10.498559: step 6384, loss 0.562121.
Train: 2018-08-09T11:18:10.576666: step 6385, loss 0.577033.
Train: 2018-08-09T11:18:10.670395: step 6386, loss 0.577903.
Train: 2018-08-09T11:18:10.748501: step 6387, loss 0.495137.
Train: 2018-08-09T11:18:10.826608: step 6388, loss 0.566926.
Train: 2018-08-09T11:18:10.904714: step 6389, loss 0.463239.
Train: 2018-08-09T11:18:10.982790: step 6390, loss 0.649127.
Test: 2018-08-09T11:18:11.482703: step 6390, loss 0.546647.
Train: 2018-08-09T11:18:11.560810: step 6391, loss 0.406317.
Train: 2018-08-09T11:18:11.638915: step 6392, loss 0.429078.
Train: 2018-08-09T11:18:11.732644: step 6393, loss 0.548482.
Train: 2018-08-09T11:18:11.810750: step 6394, loss 0.512626.
Train: 2018-08-09T11:18:11.888857: step 6395, loss 0.527999.
Train: 2018-08-09T11:18:11.967614: step 6396, loss 0.560698.
Train: 2018-08-09T11:18:12.045717: step 6397, loss 0.616121.
Train: 2018-08-09T11:18:12.123793: step 6398, loss 0.546229.
Train: 2018-08-09T11:18:12.217552: step 6399, loss 0.599171.
Train: 2018-08-09T11:18:12.295658: step 6400, loss 0.57915.
Test: 2018-08-09T11:18:12.795512: step 6400, loss 0.547216.
Train: 2018-08-09T11:18:13.326666: step 6401, loss 0.580775.
Train: 2018-08-09T11:18:13.404772: step 6402, loss 0.511023.
Train: 2018-08-09T11:18:13.482850: step 6403, loss 0.615619.
Train: 2018-08-09T11:18:13.560989: step 6404, loss 0.613842.
Train: 2018-08-09T11:18:13.639093: step 6405, loss 0.510362.
Train: 2018-08-09T11:18:13.732824: step 6406, loss 0.647165.
Train: 2018-08-09T11:18:13.810927: step 6407, loss 0.560931.
Train: 2018-08-09T11:18:13.889002: step 6408, loss 0.563062.
Train: 2018-08-09T11:18:13.967110: step 6409, loss 0.530163.
Train: 2018-08-09T11:18:14.045241: step 6410, loss 0.512584.
Test: 2018-08-09T11:18:14.545100: step 6410, loss 0.549298.
Train: 2018-08-09T11:18:14.623234: step 6411, loss 0.529114.
Train: 2018-08-09T11:18:14.701312: step 6412, loss 0.529328.
Train: 2018-08-09T11:18:14.779445: step 6413, loss 0.612297.
Train: 2018-08-09T11:18:14.857558: step 6414, loss 0.612506.
Train: 2018-08-09T11:18:14.937975: step 6415, loss 0.595902.
Train: 2018-08-09T11:18:15.031675: step 6416, loss 0.529292.
Train: 2018-08-09T11:18:15.109812: step 6417, loss 0.463554.
Train: 2018-08-09T11:18:15.187916: step 6418, loss 0.529397.
Train: 2018-08-09T11:18:15.265993: step 6419, loss 0.512351.
Train: 2018-08-09T11:18:15.344129: step 6420, loss 0.66189.
Test: 2018-08-09T11:18:15.844010: step 6420, loss 0.550528.
Train: 2018-08-09T11:18:15.922119: step 6421, loss 0.612826.
Train: 2018-08-09T11:18:16.015846: step 6422, loss 0.579723.
Train: 2018-08-09T11:18:16.093953: step 6423, loss 0.496532.
Train: 2018-08-09T11:18:16.172058: step 6424, loss 0.513345.
Train: 2018-08-09T11:18:16.250171: step 6425, loss 0.496012.
Train: 2018-08-09T11:18:16.328242: step 6426, loss 0.611928.
Train: 2018-08-09T11:18:16.406379: step 6427, loss 0.496277.
Train: 2018-08-09T11:18:16.500082: step 6428, loss 0.529835.
Train: 2018-08-09T11:18:16.578214: step 6429, loss 0.562497.
Train: 2018-08-09T11:18:16.656315: step 6430, loss 0.579212.
Test: 2018-08-09T11:18:17.158465: step 6430, loss 0.54809.
Train: 2018-08-09T11:18:17.236595: step 6431, loss 0.429267.
Train: 2018-08-09T11:18:17.314676: step 6432, loss 0.596086.
Train: 2018-08-09T11:18:17.408405: step 6433, loss 0.562318.
Train: 2018-08-09T11:18:17.486541: step 6434, loss 0.663276.
Train: 2018-08-09T11:18:17.564617: step 6435, loss 0.495494.
Train: 2018-08-09T11:18:17.642760: step 6436, loss 0.545671.
Train: 2018-08-09T11:18:17.720830: step 6437, loss 0.579153.
Train: 2018-08-09T11:18:17.798937: step 6438, loss 0.579156.
Train: 2018-08-09T11:18:17.877078: step 6439, loss 0.595918.
Train: 2018-08-09T11:18:17.970805: step 6440, loss 0.562404.
Test: 2018-08-09T11:18:18.455058: step 6440, loss 0.547991.
Train: 2018-08-09T11:18:18.533169: step 6441, loss 0.629349.
Train: 2018-08-09T11:18:18.611276: step 6442, loss 0.545708.
Train: 2018-08-09T11:18:18.705001: step 6443, loss 0.512377.
Train: 2018-08-09T11:18:18.783081: step 6444, loss 0.545755.
Train: 2018-08-09T11:18:18.861187: step 6445, loss 0.645794.
Train: 2018-08-09T11:18:18.941748: step 6446, loss 0.629022.
Train: 2018-08-09T11:18:19.019885: step 6447, loss 0.595658.
Train: 2018-08-09T11:18:19.097987: step 6448, loss 0.545928.
Train: 2018-08-09T11:18:19.176094: step 6449, loss 0.545992.
Train: 2018-08-09T11:18:19.254205: step 6450, loss 0.611951.
Test: 2018-08-09T11:18:19.754057: step 6450, loss 0.550255.
Train: 2018-08-09T11:18:19.832163: step 6451, loss 0.496827.
Train: 2018-08-09T11:18:19.925922: step 6452, loss 0.611785.
Train: 2018-08-09T11:18:20.003998: step 6453, loss 0.480705.
Train: 2018-08-09T11:18:20.082138: step 6454, loss 0.529848.
Train: 2018-08-09T11:18:20.168186: step 6455, loss 0.562579.
Train: 2018-08-09T11:18:20.246292: step 6456, loss 0.611686.
Train: 2018-08-09T11:18:20.324368: step 6457, loss 0.546231.
Train: 2018-08-09T11:18:20.402504: step 6458, loss 0.677046.
Train: 2018-08-09T11:18:20.480612: step 6459, loss 0.578929.
Train: 2018-08-09T11:18:20.574340: step 6460, loss 0.513828.
Test: 2018-08-09T11:18:21.058571: step 6460, loss 0.55112.
Train: 2018-08-09T11:18:21.152317: step 6461, loss 0.448903.
Train: 2018-08-09T11:18:21.230429: step 6462, loss 0.64397.
Train: 2018-08-09T11:18:21.308536: step 6463, loss 0.62768.
Train: 2018-08-09T11:18:21.386647: step 6464, loss 0.49779.
Train: 2018-08-09T11:18:21.464755: step 6465, loss 0.562674.
Train: 2018-08-09T11:18:21.542856: step 6466, loss 0.546485.
Train: 2018-08-09T11:18:21.636589: step 6467, loss 0.481613.
Train: 2018-08-09T11:18:21.714666: step 6468, loss 0.578909.
Train: 2018-08-09T11:18:21.792803: step 6469, loss 0.530103.
Train: 2018-08-09T11:18:21.870910: step 6470, loss 0.530007.
Test: 2018-08-09T11:18:22.370762: step 6470, loss 0.550379.
Train: 2018-08-09T11:18:22.448899: step 6471, loss 0.497213.
Train: 2018-08-09T11:18:22.526973: step 6472, loss 0.628386.
Train: 2018-08-09T11:18:22.605080: step 6473, loss 0.496765.
Train: 2018-08-09T11:18:22.698809: step 6474, loss 0.661456.
Train: 2018-08-09T11:18:22.776945: step 6475, loss 0.529747.
Train: 2018-08-09T11:18:22.855048: step 6476, loss 0.578971.
Train: 2018-08-09T11:18:22.934488: step 6477, loss 0.545897.
Train: 2018-08-09T11:18:23.012566: step 6478, loss 0.512818.
Train: 2018-08-09T11:18:23.090703: step 6479, loss 0.628991.
Train: 2018-08-09T11:18:23.184429: step 6480, loss 0.546209.
Test: 2018-08-09T11:18:23.684283: step 6480, loss 0.550119.
Train: 2018-08-09T11:18:23.762412: step 6481, loss 0.57908.
Train: 2018-08-09T11:18:23.840524: step 6482, loss 0.628679.
Train: 2018-08-09T11:18:23.918632: step 6483, loss 0.57911.
Train: 2018-08-09T11:18:23.996733: step 6484, loss 0.562528.
Train: 2018-08-09T11:18:24.074844: step 6485, loss 0.562585.
Train: 2018-08-09T11:18:24.168568: step 6486, loss 0.611758.
Train: 2018-08-09T11:18:24.246649: step 6487, loss 0.513498.
Train: 2018-08-09T11:18:24.324785: step 6488, loss 0.464524.
Train: 2018-08-09T11:18:24.402863: step 6489, loss 0.529872.
Train: 2018-08-09T11:18:24.480969: step 6490, loss 0.660822.
Test: 2018-08-09T11:18:24.982152: step 6490, loss 0.550951.
Train: 2018-08-09T11:18:25.060294: step 6491, loss 0.562572.
Train: 2018-08-09T11:18:25.138397: step 6492, loss 0.497115.
Train: 2018-08-09T11:18:25.232129: step 6493, loss 0.529767.
Train: 2018-08-09T11:18:25.310202: step 6494, loss 0.546168.
Train: 2018-08-09T11:18:25.388337: step 6495, loss 0.644658.
Train: 2018-08-09T11:18:25.466415: step 6496, loss 0.54623.
Train: 2018-08-09T11:18:25.544550: step 6497, loss 0.595528.
Train: 2018-08-09T11:18:25.622657: step 6498, loss 0.464019.
Train: 2018-08-09T11:18:25.716379: step 6499, loss 0.546119.
Train: 2018-08-09T11:18:25.794492: step 6500, loss 0.61191.
Test: 2018-08-09T11:18:26.278753: step 6500, loss 0.547108.
Train: 2018-08-09T11:18:26.856711: step 6501, loss 0.628331.
Train: 2018-08-09T11:18:26.936227: step 6502, loss 0.578946.
Train: 2018-08-09T11:18:27.012395: step 6503, loss 0.562505.
Train: 2018-08-09T11:18:27.090530: step 6504, loss 0.6447.
Train: 2018-08-09T11:18:27.168606: step 6505, loss 0.578954.
Train: 2018-08-09T11:18:27.262367: step 6506, loss 0.481058.
Train: 2018-08-09T11:18:27.340442: step 6507, loss 0.611517.
Train: 2018-08-09T11:18:27.418548: step 6508, loss 0.611468.
Train: 2018-08-09T11:18:27.496686: step 6509, loss 0.481485.
Train: 2018-08-09T11:18:27.574792: step 6510, loss 0.530225.
Test: 2018-08-09T11:18:28.074678: step 6510, loss 0.549925.
Train: 2018-08-09T11:18:28.152751: step 6511, loss 0.513991.
Train: 2018-08-09T11:18:28.230887: step 6512, loss 0.57891.
Train: 2018-08-09T11:18:28.324621: step 6513, loss 0.56266.
Train: 2018-08-09T11:18:28.402692: step 6514, loss 0.627695.
Train: 2018-08-09T11:18:28.480827: step 6515, loss 0.497658.
Train: 2018-08-09T11:18:28.558906: step 6516, loss 0.497604.
Train: 2018-08-09T11:18:28.637041: step 6517, loss 0.497482.
Train: 2018-08-09T11:18:28.715142: step 6518, loss 0.52995.
Train: 2018-08-09T11:18:28.793224: step 6519, loss 0.529774.
Train: 2018-08-09T11:18:28.886982: step 6520, loss 0.430998.
Test: 2018-08-09T11:18:29.389555: step 6520, loss 0.547608.
Train: 2018-08-09T11:18:29.467660: step 6521, loss 0.579073.
Train: 2018-08-09T11:18:29.545767: step 6522, loss 0.562513.
Train: 2018-08-09T11:18:29.623906: step 6523, loss 0.462299.
Train: 2018-08-09T11:18:29.702012: step 6524, loss 0.562753.
Train: 2018-08-09T11:18:29.780118: step 6525, loss 0.596393.
Train: 2018-08-09T11:18:29.873846: step 6526, loss 0.494616.
Train: 2018-08-09T11:18:29.951953: step 6527, loss 0.596452.
Train: 2018-08-09T11:18:30.030059: step 6528, loss 0.596459.
Train: 2018-08-09T11:18:30.108135: step 6529, loss 0.562237.
Train: 2018-08-09T11:18:30.186271: step 6530, loss 0.613675.
Test: 2018-08-09T11:18:30.686124: step 6530, loss 0.549591.
Train: 2018-08-09T11:18:30.764260: step 6531, loss 0.613544.
Train: 2018-08-09T11:18:30.842368: step 6532, loss 0.664272.
Train: 2018-08-09T11:18:30.922188: step 6533, loss 0.477574.
Train: 2018-08-09T11:18:31.000326: step 6534, loss 0.630041.
Train: 2018-08-09T11:18:31.094047: step 6535, loss 0.613027.
Train: 2018-08-09T11:18:31.172155: step 6536, loss 0.495005.
Train: 2018-08-09T11:18:31.250266: step 6537, loss 0.680069.
Train: 2018-08-09T11:18:31.328374: step 6538, loss 0.595914.
Train: 2018-08-09T11:18:31.406483: step 6539, loss 0.562437.
Train: 2018-08-09T11:18:31.500202: step 6540, loss 0.51257.
Test: 2018-08-09T11:18:31.984463: step 6540, loss 0.547546.
Train: 2018-08-09T11:18:32.078166: step 6541, loss 0.529283.
Train: 2018-08-09T11:18:32.156274: step 6542, loss 0.54591.
Train: 2018-08-09T11:18:32.234410: step 6543, loss 0.512848.
Train: 2018-08-09T11:18:32.312516: step 6544, loss 0.595564.
Train: 2018-08-09T11:18:32.390623: step 6545, loss 0.579017.
Train: 2018-08-09T11:18:32.468731: step 6546, loss 0.546.
Train: 2018-08-09T11:18:32.546805: step 6547, loss 0.546021.
Train: 2018-08-09T11:18:32.640564: step 6548, loss 0.595476.
Train: 2018-08-09T11:18:32.718671: step 6549, loss 0.562523.
Train: 2018-08-09T11:18:32.796772: step 6550, loss 0.54609.
Test: 2018-08-09T11:18:33.296629: step 6550, loss 0.546518.
Train: 2018-08-09T11:18:33.374766: step 6551, loss 0.496788.
Train: 2018-08-09T11:18:33.452872: step 6552, loss 0.595431.
Train: 2018-08-09T11:18:33.530951: step 6553, loss 0.595409.
Train: 2018-08-09T11:18:33.624676: step 6554, loss 0.56254.
Train: 2018-08-09T11:18:33.702783: step 6555, loss 0.611823.
Train: 2018-08-09T11:18:33.780891: step 6556, loss 0.513366.
Train: 2018-08-09T11:18:33.859027: step 6557, loss 0.496994.
Train: 2018-08-09T11:18:33.938565: step 6558, loss 0.546149.
Train: 2018-08-09T11:18:34.016672: step 6559, loss 0.562591.
Train: 2018-08-09T11:18:34.094809: step 6560, loss 0.546106.
Test: 2018-08-09T11:18:34.594686: step 6560, loss 0.545872.
Train: 2018-08-09T11:18:34.672802: step 6561, loss 0.562523.
Train: 2018-08-09T11:18:34.750904: step 6562, loss 0.463695.
Train: 2018-08-09T11:18:34.844634: step 6563, loss 0.578985.
Train: 2018-08-09T11:18:34.922743: step 6564, loss 0.512734.
Train: 2018-08-09T11:18:35.000845: step 6565, loss 0.529359.
Train: 2018-08-09T11:18:35.078949: step 6566, loss 0.545858.
Train: 2018-08-09T11:18:35.157061: step 6567, loss 0.646156.
Train: 2018-08-09T11:18:35.235136: step 6568, loss 0.529021.
Train: 2018-08-09T11:18:35.317891: step 6569, loss 0.629276.
Train: 2018-08-09T11:18:35.411651: step 6570, loss 0.545843.
Test: 2018-08-09T11:18:35.895294: step 6570, loss 0.549853.
Train: 2018-08-09T11:18:35.989023: step 6571, loss 0.528623.
Train: 2018-08-09T11:18:36.067133: step 6572, loss 0.646711.
Train: 2018-08-09T11:18:36.145210: step 6573, loss 0.612512.
Train: 2018-08-09T11:18:36.223316: step 6574, loss 0.478988.
Train: 2018-08-09T11:18:36.301423: step 6575, loss 0.495601.
Train: 2018-08-09T11:18:36.395181: step 6576, loss 0.646229.
Train: 2018-08-09T11:18:36.473257: step 6577, loss 0.562425.
Train: 2018-08-09T11:18:36.551394: step 6578, loss 0.579358.
Train: 2018-08-09T11:18:36.629497: step 6579, loss 0.545663.
Train: 2018-08-09T11:18:36.707577: step 6580, loss 0.529173.
Test: 2018-08-09T11:18:37.207478: step 6580, loss 0.547532.
Train: 2018-08-09T11:18:37.285591: step 6581, loss 0.54592.
Train: 2018-08-09T11:18:37.363684: step 6582, loss 0.628734.
Train: 2018-08-09T11:18:37.457402: step 6583, loss 0.628884.
Train: 2018-08-09T11:18:37.535532: step 6584, loss 0.595594.
Train: 2018-08-09T11:18:37.613644: step 6585, loss 0.546036.
Train: 2018-08-09T11:18:37.691722: step 6586, loss 0.61186.
Train: 2018-08-09T11:18:37.769858: step 6587, loss 0.562564.
Train: 2018-08-09T11:18:37.847933: step 6588, loss 0.529889.
Train: 2018-08-09T11:18:37.928473: step 6589, loss 0.513655.
Train: 2018-08-09T11:18:38.006549: step 6590, loss 0.546318.
Test: 2018-08-09T11:18:38.506462: step 6590, loss 0.549814.
Train: 2018-08-09T11:18:38.584568: step 6591, loss 0.578934.
Train: 2018-08-09T11:18:38.662675: step 6592, loss 0.513762.
Train: 2018-08-09T11:18:38.756403: step 6593, loss 0.54633.
Train: 2018-08-09T11:18:38.834481: step 6594, loss 0.464787.
Train: 2018-08-09T11:18:38.912616: step 6595, loss 0.644305.
Train: 2018-08-09T11:18:38.990718: step 6596, loss 0.546227.
Train: 2018-08-09T11:18:39.084446: step 6597, loss 0.578975.
Train: 2018-08-09T11:18:39.162558: step 6598, loss 0.660784.
Train: 2018-08-09T11:18:39.240668: step 6599, loss 0.595297.
Train: 2018-08-09T11:18:39.318739: step 6600, loss 0.562585.
Test: 2018-08-09T11:18:39.818674: step 6600, loss 0.550431.
Train: 2018-08-09T11:18:40.413876: step 6601, loss 0.529996.
Train: 2018-08-09T11:18:40.491981: step 6602, loss 0.595317.
Train: 2018-08-09T11:18:40.570087: step 6603, loss 0.562616.
Train: 2018-08-09T11:18:40.663785: step 6604, loss 0.611539.
Train: 2018-08-09T11:18:40.741924: step 6605, loss 0.546517.
Train: 2018-08-09T11:18:40.820030: step 6606, loss 0.562697.
Train: 2018-08-09T11:18:40.898136: step 6607, loss 0.643631.
Train: 2018-08-09T11:18:40.976212: step 6608, loss 0.595057.
Train: 2018-08-09T11:18:41.054353: step 6609, loss 0.450258.
Train: 2018-08-09T11:18:41.148076: step 6610, loss 0.594933.
Test: 2018-08-09T11:18:41.632339: step 6610, loss 0.548992.
Train: 2018-08-09T11:18:41.710449: step 6611, loss 0.482562.
Train: 2018-08-09T11:18:41.788546: step 6612, loss 0.578868.
Train: 2018-08-09T11:18:41.882282: step 6613, loss 0.611009.
Train: 2018-08-09T11:18:41.962785: step 6614, loss 0.594929.
Train: 2018-08-09T11:18:42.040891: step 6615, loss 0.562825.
Train: 2018-08-09T11:18:42.118995: step 6616, loss 0.546804.
Train: 2018-08-09T11:18:42.197074: step 6617, loss 0.530778.
Train: 2018-08-09T11:18:42.275211: step 6618, loss 0.659024.
Train: 2018-08-09T11:18:42.353288: step 6619, loss 0.562864.
Train: 2018-08-09T11:18:42.447045: step 6620, loss 0.642821.
Test: 2018-08-09T11:18:42.931283: step 6620, loss 0.549166.
Train: 2018-08-09T11:18:43.009384: step 6621, loss 0.546958.
Train: 2018-08-09T11:18:43.087519: step 6622, loss 0.528958.
Train: 2018-08-09T11:18:43.181249: step 6623, loss 0.499313.
Train: 2018-08-09T11:18:43.259354: step 6624, loss 0.547031.
Train: 2018-08-09T11:18:43.337431: step 6625, loss 0.499199.
Train: 2018-08-09T11:18:43.431161: step 6626, loss 0.562907.
Train: 2018-08-09T11:18:43.509296: step 6627, loss 0.610837.
Train: 2018-08-09T11:18:43.587371: step 6628, loss 0.418785.
Train: 2018-08-09T11:18:43.665504: step 6629, loss 0.482468.
Train: 2018-08-09T11:18:43.743615: step 6630, loss 0.594903.
Test: 2018-08-09T11:18:44.245872: step 6630, loss 0.548081.
Train: 2018-08-09T11:18:44.324004: step 6631, loss 0.594965.
Train: 2018-08-09T11:18:44.402116: step 6632, loss 0.643944.
Train: 2018-08-09T11:18:44.480195: step 6633, loss 0.49685.
Train: 2018-08-09T11:18:44.573944: step 6634, loss 0.57836.
Train: 2018-08-09T11:18:44.652028: step 6635, loss 0.446823.
Train: 2018-08-09T11:18:44.730164: step 6636, loss 0.511165.
Train: 2018-08-09T11:18:44.808239: step 6637, loss 0.630107.
Train: 2018-08-09T11:18:44.886377: step 6638, loss 0.542177.
Train: 2018-08-09T11:18:44.964479: step 6639, loss 0.590048.
Train: 2018-08-09T11:18:45.058181: step 6640, loss 0.452529.
Test: 2018-08-09T11:18:45.558089: step 6640, loss 0.547061.
Train: 2018-08-09T11:18:45.636170: step 6641, loss 0.450846.
Train: 2018-08-09T11:18:45.714300: step 6642, loss 0.422025.
Train: 2018-08-09T11:18:45.792383: step 6643, loss 0.506922.
Train: 2018-08-09T11:18:45.870519: step 6644, loss 0.658125.
Train: 2018-08-09T11:18:45.950880: step 6645, loss 0.578694.
Train: 2018-08-09T11:18:46.044608: step 6646, loss 0.602257.
Train: 2018-08-09T11:18:46.122744: step 6647, loss 0.504577.
Train: 2018-08-09T11:18:46.200820: step 6648, loss 0.543126.
Train: 2018-08-09T11:18:46.278929: step 6649, loss 0.627601.
Train: 2018-08-09T11:18:46.357065: step 6650, loss 0.485606.
Test: 2018-08-09T11:18:46.872544: step 6650, loss 0.546733.
Train: 2018-08-09T11:18:46.950644: step 6651, loss 0.614944.
Train: 2018-08-09T11:18:47.028751: step 6652, loss 0.543354.
Train: 2018-08-09T11:18:47.106870: step 6653, loss 0.478581.
Train: 2018-08-09T11:18:47.184994: step 6654, loss 0.6618.
Train: 2018-08-09T11:18:47.263103: step 6655, loss 0.545451.
Train: 2018-08-09T11:18:47.356799: step 6656, loss 0.578402.
Train: 2018-08-09T11:18:47.434935: step 6657, loss 0.632826.
Train: 2018-08-09T11:18:47.513042: step 6658, loss 0.563445.
Train: 2018-08-09T11:18:47.591118: step 6659, loss 0.630657.
Train: 2018-08-09T11:18:47.669224: step 6660, loss 0.478423.
Test: 2018-08-09T11:18:48.171497: step 6660, loss 0.547389.
Train: 2018-08-09T11:18:48.249634: step 6661, loss 0.529104.
Train: 2018-08-09T11:18:48.327739: step 6662, loss 0.477462.
Train: 2018-08-09T11:18:48.421470: step 6663, loss 0.461152.
Train: 2018-08-09T11:18:48.499546: step 6664, loss 0.57726.
Train: 2018-08-09T11:18:48.577685: step 6665, loss 0.495941.
Train: 2018-08-09T11:18:48.655790: step 6666, loss 0.530404.
Train: 2018-08-09T11:18:48.733864: step 6667, loss 0.596182.
Train: 2018-08-09T11:18:48.812002: step 6668, loss 0.630275.
Train: 2018-08-09T11:18:48.905747: step 6669, loss 0.54539.
Train: 2018-08-09T11:18:48.983830: step 6670, loss 0.580844.
Test: 2018-08-09T11:18:49.483720: step 6670, loss 0.547808.
Train: 2018-08-09T11:18:49.561826: step 6671, loss 0.647507.
Train: 2018-08-09T11:18:49.639902: step 6672, loss 0.564709.
Train: 2018-08-09T11:18:49.718008: step 6673, loss 0.594978.
Train: 2018-08-09T11:18:49.796145: step 6674, loss 0.529881.
Train: 2018-08-09T11:18:49.874221: step 6675, loss 0.56228.
Train: 2018-08-09T11:18:49.953048: step 6676, loss 0.578515.
Train: 2018-08-09T11:18:50.046803: step 6677, loss 0.59632.
Train: 2018-08-09T11:18:50.124912: step 6678, loss 0.595644.
Train: 2018-08-09T11:18:50.203019: step 6679, loss 0.612018.
Train: 2018-08-09T11:18:50.281097: step 6680, loss 0.595017.
Test: 2018-08-09T11:18:50.786990: step 6680, loss 0.547428.
Train: 2018-08-09T11:18:50.865121: step 6681, loss 0.62782.
Train: 2018-08-09T11:18:50.943202: step 6682, loss 0.691964.
Train: 2018-08-09T11:18:51.021342: step 6683, loss 0.610971.
Train: 2018-08-09T11:18:51.099416: step 6684, loss 0.515033.
Train: 2018-08-09T11:18:51.193174: step 6685, loss 0.642317.
Train: 2018-08-09T11:18:51.271251: step 6686, loss 0.547006.
Train: 2018-08-09T11:18:51.349387: step 6687, loss 0.610219.
Train: 2018-08-09T11:18:51.427464: step 6688, loss 0.594094.
Train: 2018-08-09T11:18:51.510222: step 6689, loss 0.625399.
Train: 2018-08-09T11:18:51.588358: step 6690, loss 0.518578.
Test: 2018-08-09T11:18:52.088228: step 6690, loss 0.552488.
Train: 2018-08-09T11:18:52.166347: step 6691, loss 0.625029.
Train: 2018-08-09T11:18:52.244455: step 6692, loss 0.533264.
Train: 2018-08-09T11:18:52.322561: step 6693, loss 0.609482.
Train: 2018-08-09T11:18:52.416288: step 6694, loss 0.533936.
Train: 2018-08-09T11:18:52.494396: step 6695, loss 0.640284.
Train: 2018-08-09T11:18:52.572502: step 6696, loss 0.518419.
Train: 2018-08-09T11:18:52.650609: step 6697, loss 0.472764.
Train: 2018-08-09T11:18:52.728717: step 6698, loss 0.594266.
Train: 2018-08-09T11:18:52.806822: step 6699, loss 0.518165.
Train: 2018-08-09T11:18:52.884929: step 6700, loss 0.56378.
Test: 2018-08-09T11:18:53.384781: step 6700, loss 0.552877.
Train: 2018-08-09T11:18:53.994043: step 6701, loss 0.624649.
Train: 2018-08-09T11:18:54.072143: step 6702, loss 0.548429.
Train: 2018-08-09T11:18:54.150251: step 6703, loss 0.594271.
Train: 2018-08-09T11:18:54.228332: step 6704, loss 0.532652.
Train: 2018-08-09T11:18:54.306463: step 6705, loss 0.501158.
Train: 2018-08-09T11:18:54.400197: step 6706, loss 0.625479.
Train: 2018-08-09T11:18:54.478272: step 6707, loss 0.548546.
Train: 2018-08-09T11:18:54.556379: step 6708, loss 0.515997.
Train: 2018-08-09T11:18:54.634488: step 6709, loss 0.483806.
Train: 2018-08-09T11:18:54.712593: step 6710, loss 0.561316.
Test: 2018-08-09T11:18:55.213806: step 6710, loss 0.548759.
Train: 2018-08-09T11:18:55.291942: step 6711, loss 0.591965.
Train: 2018-08-09T11:18:55.370019: step 6712, loss 0.528439.
Train: 2018-08-09T11:18:55.448155: step 6713, loss 0.520182.
Train: 2018-08-09T11:18:55.541854: step 6714, loss 0.588086.
Train: 2018-08-09T11:18:55.619990: step 6715, loss 0.540161.
Train: 2018-08-09T11:18:55.698068: step 6716, loss 0.54266.
Train: 2018-08-09T11:18:55.776174: step 6717, loss 0.656857.
Train: 2018-08-09T11:18:55.854310: step 6718, loss 0.654449.
Train: 2018-08-09T11:18:55.932420: step 6719, loss 0.623307.
Train: 2018-08-09T11:18:56.026115: step 6720, loss 0.657894.
Test: 2018-08-09T11:18:56.510376: step 6720, loss 0.54911.
Train: 2018-08-09T11:18:56.604128: step 6721, loss 0.514771.
Train: 2018-08-09T11:18:56.682241: step 6722, loss 0.548159.
Train: 2018-08-09T11:18:56.760317: step 6723, loss 0.578333.
Train: 2018-08-09T11:18:56.838456: step 6724, loss 0.610109.
Train: 2018-08-09T11:18:56.918814: step 6725, loss 0.595339.
Train: 2018-08-09T11:18:56.996948: step 6726, loss 0.532616.
Train: 2018-08-09T11:18:57.075058: step 6727, loss 0.594112.
Train: 2018-08-09T11:18:57.153165: step 6728, loss 0.547947.
Train: 2018-08-09T11:18:57.246894: step 6729, loss 0.563562.
Train: 2018-08-09T11:18:57.324970: step 6730, loss 0.578968.
Test: 2018-08-09T11:18:57.809230: step 6730, loss 0.549857.
Train: 2018-08-09T11:18:57.887336: step 6731, loss 0.502284.
Train: 2018-08-09T11:18:57.981089: step 6732, loss 0.502279.
Train: 2018-08-09T11:18:58.062295: step 6733, loss 0.548162.
Train: 2018-08-09T11:18:58.133312: step 6734, loss 0.57892.
Train: 2018-08-09T11:18:58.211426: step 6735, loss 0.625238.
Train: 2018-08-09T11:18:58.305118: step 6736, loss 0.54809.
Train: 2018-08-09T11:18:58.383224: step 6737, loss 0.563408.
Train: 2018-08-09T11:18:58.461332: step 6738, loss 0.53246.
Train: 2018-08-09T11:18:58.539467: step 6739, loss 0.609995.
Train: 2018-08-09T11:18:58.617573: step 6740, loss 0.516685.
Test: 2018-08-09T11:18:59.122813: step 6740, loss 0.551638.
Train: 2018-08-09T11:18:59.200888: step 6741, loss 0.594472.
Train: 2018-08-09T11:18:59.279026: step 6742, loss 0.516421.
Train: 2018-08-09T11:18:59.357134: step 6743, loss 0.516214.
Train: 2018-08-09T11:18:59.435209: step 6744, loss 0.563197.
Train: 2018-08-09T11:18:59.513345: step 6745, loss 0.56309.
Train: 2018-08-09T11:18:59.591452: step 6746, loss 0.531442.
Train: 2018-08-09T11:18:59.685164: step 6747, loss 0.578961.
Train: 2018-08-09T11:18:59.763256: step 6748, loss 0.578859.
Train: 2018-08-09T11:18:59.841364: step 6749, loss 0.546916.
Train: 2018-08-09T11:18:59.919499: step 6750, loss 0.546632.
Test: 2018-08-09T11:19:00.416636: step 6750, loss 0.548843.
Train: 2018-08-09T11:19:00.494773: step 6751, loss 0.6272.
Train: 2018-08-09T11:19:00.572880: step 6752, loss 0.514529.
Train: 2018-08-09T11:19:00.666608: step 6753, loss 0.579281.
Train: 2018-08-09T11:19:00.744685: step 6754, loss 0.563169.
Train: 2018-08-09T11:19:00.822821: step 6755, loss 0.578897.
Train: 2018-08-09T11:19:00.900897: step 6756, loss 0.546552.
Train: 2018-08-09T11:19:00.979003: step 6757, loss 0.595467.
Train: 2018-08-09T11:19:01.057142: step 6758, loss 0.578679.
Train: 2018-08-09T11:19:01.135247: step 6759, loss 0.59508.
Train: 2018-08-09T11:19:01.228975: step 6760, loss 0.530171.
Test: 2018-08-09T11:19:01.728859: step 6760, loss 0.549267.
Train: 2018-08-09T11:19:01.806964: step 6761, loss 0.514001.
Train: 2018-08-09T11:19:01.885070: step 6762, loss 0.545788.
Train: 2018-08-09T11:19:01.967493: step 6763, loss 0.529428.
Train: 2018-08-09T11:19:02.045598: step 6764, loss 0.645706.
Train: 2018-08-09T11:19:02.123705: step 6765, loss 0.577653.
Train: 2018-08-09T11:19:02.201795: step 6766, loss 0.512002.
Train: 2018-08-09T11:19:02.279914: step 6767, loss 0.597294.
Train: 2018-08-09T11:19:02.357995: step 6768, loss 0.495371.
Train: 2018-08-09T11:19:02.451724: step 6769, loss 0.5967.
Train: 2018-08-09T11:19:02.529858: step 6770, loss 0.630513.
Test: 2018-08-09T11:19:03.029726: step 6770, loss 0.547965.
Train: 2018-08-09T11:19:03.107848: step 6771, loss 0.561178.
Train: 2018-08-09T11:19:03.185923: step 6772, loss 0.478137.
Train: 2018-08-09T11:19:03.264033: step 6773, loss 0.47843.
Train: 2018-08-09T11:19:03.342168: step 6774, loss 0.562032.
Train: 2018-08-09T11:19:03.435915: step 6775, loss 0.547772.
Train: 2018-08-09T11:19:03.514002: step 6776, loss 0.545168.
Train: 2018-08-09T11:19:03.592079: step 6777, loss 0.600028.
Train: 2018-08-09T11:19:03.670184: step 6778, loss 0.6539.
Train: 2018-08-09T11:19:03.748322: step 6779, loss 0.564938.
Train: 2018-08-09T11:19:03.826428: step 6780, loss 0.59581.
Test: 2018-08-09T11:19:04.326281: step 6780, loss 0.548665.
Train: 2018-08-09T11:19:04.404388: step 6781, loss 0.497273.
Train: 2018-08-09T11:19:04.482525: step 6782, loss 0.562432.
Train: 2018-08-09T11:19:04.560633: step 6783, loss 0.578315.
Train: 2018-08-09T11:19:04.638738: step 6784, loss 0.513817.
Train: 2018-08-09T11:19:04.732466: step 6785, loss 0.561897.
Train: 2018-08-09T11:19:04.810542: step 6786, loss 0.613143.
Train: 2018-08-09T11:19:04.888678: step 6787, loss 0.481758.
Train: 2018-08-09T11:19:04.968214: step 6788, loss 0.578407.
Train: 2018-08-09T11:19:05.046320: step 6789, loss 0.562679.
Train: 2018-08-09T11:19:05.124457: step 6790, loss 0.66028.
Test: 2018-08-09T11:19:05.624310: step 6790, loss 0.546768.
Train: 2018-08-09T11:19:05.702441: step 6791, loss 0.546719.
Train: 2018-08-09T11:19:05.780553: step 6792, loss 0.53036.
Train: 2018-08-09T11:19:05.874280: step 6793, loss 0.54661.
Train: 2018-08-09T11:19:05.952389: step 6794, loss 0.595372.
Train: 2018-08-09T11:19:06.030489: step 6795, loss 0.562708.
Train: 2018-08-09T11:19:06.108570: step 6796, loss 0.530023.
Train: 2018-08-09T11:19:06.186677: step 6797, loss 0.643912.
Train: 2018-08-09T11:19:06.280406: step 6798, loss 0.676286.
Train: 2018-08-09T11:19:06.358541: step 6799, loss 0.514446.
Train: 2018-08-09T11:19:06.436646: step 6800, loss 0.546768.
Test: 2018-08-09T11:19:06.934504: step 6800, loss 0.547215.
Train: 2018-08-09T11:19:07.496871: step 6801, loss 0.482757.
Train: 2018-08-09T11:19:07.575010: step 6802, loss 0.723148.
Train: 2018-08-09T11:19:07.653115: step 6803, loss 0.562959.
Train: 2018-08-09T11:19:07.731222: step 6804, loss 0.531053.
Train: 2018-08-09T11:19:07.809312: step 6805, loss 0.626607.
Train: 2018-08-09T11:19:07.887438: step 6806, loss 0.578862.
Train: 2018-08-09T11:19:07.965541: step 6807, loss 0.53133.
Train: 2018-08-09T11:19:08.043651: step 6808, loss 0.594684.
Train: 2018-08-09T11:19:08.137345: step 6809, loss 0.594661.
Train: 2018-08-09T11:19:08.215483: step 6810, loss 0.515792.
Test: 2018-08-09T11:19:08.699739: step 6810, loss 0.550133.
Train: 2018-08-09T11:19:08.777850: step 6811, loss 0.610374.
Train: 2018-08-09T11:19:08.871578: step 6812, loss 0.500193.
Train: 2018-08-09T11:19:08.951945: step 6813, loss 0.500187.
Train: 2018-08-09T11:19:09.030022: step 6814, loss 0.594625.
Train: 2018-08-09T11:19:09.108129: step 6815, loss 0.484238.
Train: 2018-08-09T11:19:09.186235: step 6816, loss 0.515645.
Train: 2018-08-09T11:19:09.264371: step 6817, loss 0.59474.
Train: 2018-08-09T11:19:09.342481: step 6818, loss 0.59473.
Train: 2018-08-09T11:19:09.436178: step 6819, loss 0.59477.
Train: 2018-08-09T11:19:09.514314: step 6820, loss 0.59482.
Test: 2018-08-09T11:19:10.014190: step 6820, loss 0.547976.
Train: 2018-08-09T11:19:10.092301: step 6821, loss 0.514987.
Train: 2018-08-09T11:19:10.170412: step 6822, loss 0.546918.
Train: 2018-08-09T11:19:10.248515: step 6823, loss 0.498839.
Train: 2018-08-09T11:19:10.326592: step 6824, loss 0.530475.
Train: 2018-08-09T11:19:10.404698: step 6825, loss 0.562978.
Train: 2018-08-09T11:19:10.498457: step 6826, loss 0.563272.
Train: 2018-08-09T11:19:10.576532: step 6827, loss 0.579563.
Train: 2018-08-09T11:19:10.654669: step 6828, loss 0.595172.
Train: 2018-08-09T11:19:10.732777: step 6829, loss 0.578711.
Train: 2018-08-09T11:19:10.810851: step 6830, loss 0.611687.
Test: 2018-08-09T11:19:11.311364: step 6830, loss 0.55051.
Train: 2018-08-09T11:19:11.389445: step 6831, loss 0.660061.
Train: 2018-08-09T11:19:11.467577: step 6832, loss 0.659868.
Train: 2018-08-09T11:19:11.561309: step 6833, loss 0.578885.
Train: 2018-08-09T11:19:11.639385: step 6834, loss 0.562803.
Train: 2018-08-09T11:19:11.717492: step 6835, loss 0.610907.
Train: 2018-08-09T11:19:11.795629: step 6836, loss 0.642717.
Train: 2018-08-09T11:19:11.873736: step 6837, loss 0.562976.
Train: 2018-08-09T11:19:11.951842: step 6838, loss 0.578869.
Train: 2018-08-09T11:19:12.029953: step 6839, loss 0.531557.
Train: 2018-08-09T11:19:12.108056: step 6840, loss 0.547415.
Test: 2018-08-09T11:19:12.607933: step 6840, loss 0.549646.
Train: 2018-08-09T11:19:12.686048: step 6841, loss 0.53177.
Train: 2018-08-09T11:19:12.779741: step 6842, loss 0.657305.
Train: 2018-08-09T11:19:12.857878: step 6843, loss 0.641472.
Train: 2018-08-09T11:19:12.935985: step 6844, loss 0.547707.
Train: 2018-08-09T11:19:13.014092: step 6845, loss 0.609991.
Train: 2018-08-09T11:19:13.092201: step 6846, loss 0.625422.
Train: 2018-08-09T11:19:13.170305: step 6847, loss 0.532624.
Train: 2018-08-09T11:19:13.248382: step 6848, loss 0.655957.
Train: 2018-08-09T11:19:13.342110: step 6849, loss 0.578986.
Train: 2018-08-09T11:19:13.420246: step 6850, loss 0.548451.
Test: 2018-08-09T11:19:13.922389: step 6850, loss 0.552398.
Train: 2018-08-09T11:19:14.000525: step 6851, loss 0.579026.
Train: 2018-08-09T11:19:14.078633: step 6852, loss 0.609448.
Train: 2018-08-09T11:19:14.156738: step 6853, loss 0.548766.
Train: 2018-08-09T11:19:14.234844: step 6854, loss 0.579095.
Train: 2018-08-09T11:19:14.312950: step 6855, loss 0.594208.
Train: 2018-08-09T11:19:14.391027: step 6856, loss 0.564064.
Train: 2018-08-09T11:19:14.469166: step 6857, loss 0.518954.
Train: 2018-08-09T11:19:14.547266: step 6858, loss 0.53399.
Train: 2018-08-09T11:19:14.625377: step 6859, loss 0.503814.
Train: 2018-08-09T11:19:14.719105: step 6860, loss 0.518585.
Test: 2018-08-09T11:19:15.218957: step 6860, loss 0.54907.
Train: 2018-08-09T11:19:15.297094: step 6861, loss 0.533637.
Train: 2018-08-09T11:19:15.375170: step 6862, loss 0.548639.
Train: 2018-08-09T11:19:15.453309: step 6863, loss 0.594627.
Train: 2018-08-09T11:19:15.531383: step 6864, loss 0.578863.
Train: 2018-08-09T11:19:15.609521: step 6865, loss 0.578697.
Train: 2018-08-09T11:19:15.703219: step 6866, loss 0.501112.
Train: 2018-08-09T11:19:15.781325: step 6867, loss 0.531384.
Train: 2018-08-09T11:19:15.859456: step 6868, loss 0.531676.
Train: 2018-08-09T11:19:15.938911: step 6869, loss 0.450128.
Train: 2018-08-09T11:19:16.017020: step 6870, loss 0.460443.
Test: 2018-08-09T11:19:16.516868: step 6870, loss 0.547264.
Train: 2018-08-09T11:19:16.595005: step 6871, loss 0.503833.
Train: 2018-08-09T11:19:16.673112: step 6872, loss 0.5629.
Train: 2018-08-09T11:19:16.751215: step 6873, loss 0.500709.
Train: 2018-08-09T11:19:16.844946: step 6874, loss 0.465856.
Train: 2018-08-09T11:19:16.923052: step 6875, loss 0.534216.
Train: 2018-08-09T11:19:17.001129: step 6876, loss 0.571751.
Train: 2018-08-09T11:19:17.079265: step 6877, loss 0.612089.
Train: 2018-08-09T11:19:17.157372: step 6878, loss 0.583358.
Train: 2018-08-09T11:19:17.235482: step 6879, loss 0.483062.
Train: 2018-08-09T11:19:17.313585: step 6880, loss 0.561736.
Test: 2018-08-09T11:19:17.813438: step 6880, loss 0.546243.
Train: 2018-08-09T11:19:17.907468: step 6881, loss 0.526564.
Train: 2018-08-09T11:19:17.972542: step 6882, loss 0.585055.
Train: 2018-08-09T11:19:18.066270: step 6883, loss 0.581046.
Train: 2018-08-09T11:19:18.144345: step 6884, loss 0.543244.
Train: 2018-08-09T11:19:18.222483: step 6885, loss 0.495249.
Train: 2018-08-09T11:19:18.300591: step 6886, loss 0.463342.
Train: 2018-08-09T11:19:18.378666: step 6887, loss 0.577869.
Train: 2018-08-09T11:19:18.456802: step 6888, loss 0.598192.
Train: 2018-08-09T11:19:18.550501: step 6889, loss 0.611762.
Train: 2018-08-09T11:19:18.628637: step 6890, loss 0.544518.
Test: 2018-08-09T11:19:19.128489: step 6890, loss 0.549227.
Train: 2018-08-09T11:19:19.206620: step 6891, loss 0.595081.
Train: 2018-08-09T11:19:19.284733: step 6892, loss 0.544863.
Train: 2018-08-09T11:19:19.362839: step 6893, loss 0.526918.
Train: 2018-08-09T11:19:19.440915: step 6894, loss 0.613745.
Train: 2018-08-09T11:19:19.519054: step 6895, loss 0.528771.
Train: 2018-08-09T11:19:19.597159: step 6896, loss 0.511492.
Train: 2018-08-09T11:19:19.690886: step 6897, loss 0.561804.
Train: 2018-08-09T11:19:19.768995: step 6898, loss 0.492703.
Train: 2018-08-09T11:19:19.847101: step 6899, loss 0.509413.
Train: 2018-08-09T11:19:19.926692: step 6900, loss 0.492056.
Test: 2018-08-09T11:19:20.426551: step 6900, loss 0.550498.
Train: 2018-08-09T11:19:20.988941: step 6901, loss 0.600372.
Train: 2018-08-09T11:19:21.067048: step 6902, loss 0.565257.
Train: 2018-08-09T11:19:21.160770: step 6903, loss 0.494754.
Train: 2018-08-09T11:19:21.238852: step 6904, loss 0.695607.
Train: 2018-08-09T11:19:21.316989: step 6905, loss 0.652545.
Train: 2018-08-09T11:19:21.395098: step 6906, loss 0.627635.
Train: 2018-08-09T11:19:21.473203: step 6907, loss 0.660947.
Train: 2018-08-09T11:19:21.551279: step 6908, loss 0.513673.
Train: 2018-08-09T11:19:21.645008: step 6909, loss 0.529953.
Train: 2018-08-09T11:19:21.723127: step 6910, loss 0.497428.
Test: 2018-08-09T11:19:22.208865: step 6910, loss 0.548418.
Train: 2018-08-09T11:19:22.302589: step 6911, loss 0.612001.
Train: 2018-08-09T11:19:22.380667: step 6912, loss 0.628583.
Train: 2018-08-09T11:19:22.458773: step 6913, loss 0.660269.
Train: 2018-08-09T11:19:22.536909: step 6914, loss 0.578811.
Train: 2018-08-09T11:19:22.615016: step 6915, loss 0.434744.
Train: 2018-08-09T11:19:22.693122: step 6916, loss 0.499294.
Train: 2018-08-09T11:19:22.771232: step 6917, loss 0.498626.
Train: 2018-08-09T11:19:22.849305: step 6918, loss 0.418647.
Train: 2018-08-09T11:19:22.943693: step 6919, loss 0.610936.
Train: 2018-08-09T11:19:23.021831: step 6920, loss 0.562491.
Test: 2018-08-09T11:19:23.506061: step 6920, loss 0.546162.
Train: 2018-08-09T11:19:23.584198: step 6921, loss 0.546559.
Train: 2018-08-09T11:19:23.677896: step 6922, loss 0.62731.
Train: 2018-08-09T11:19:23.756032: step 6923, loss 0.64977.
Train: 2018-08-09T11:19:23.834139: step 6924, loss 0.545168.
Train: 2018-08-09T11:19:23.912215: step 6925, loss 0.52953.
Train: 2018-08-09T11:19:23.990352: step 6926, loss 0.445037.
Train: 2018-08-09T11:19:24.068453: step 6927, loss 0.582138.
Train: 2018-08-09T11:19:24.162186: step 6928, loss 0.565912.
Train: 2018-08-09T11:19:24.240294: step 6929, loss 0.476427.
Train: 2018-08-09T11:19:24.318369: step 6930, loss 0.564102.
Test: 2018-08-09T11:19:24.818254: step 6930, loss 0.547576.
Train: 2018-08-09T11:19:24.896389: step 6931, loss 0.686835.
Train: 2018-08-09T11:19:24.974495: step 6932, loss 0.563545.
Train: 2018-08-09T11:19:25.052597: step 6933, loss 0.545802.
Train: 2018-08-09T11:19:25.146300: step 6934, loss 0.562017.
Train: 2018-08-09T11:19:25.224436: step 6935, loss 0.578622.
Train: 2018-08-09T11:19:25.302514: step 6936, loss 0.580982.
Train: 2018-08-09T11:19:25.380650: step 6937, loss 0.49674.
Train: 2018-08-09T11:19:25.458725: step 6938, loss 0.628159.
Train: 2018-08-09T11:19:25.536858: step 6939, loss 0.513282.
Train: 2018-08-09T11:19:25.630561: step 6940, loss 0.726451.
Test: 2018-08-09T11:19:26.116163: step 6940, loss 0.550504.
Train: 2018-08-09T11:19:26.194299: step 6941, loss 0.530252.
Train: 2018-08-09T11:19:26.272405: step 6942, loss 0.659988.
Train: 2018-08-09T11:19:26.366134: step 6943, loss 0.65969.
Train: 2018-08-09T11:19:26.444243: step 6944, loss 0.578937.
Train: 2018-08-09T11:19:26.522347: step 6945, loss 0.547023.
Train: 2018-08-09T11:19:26.600454: step 6946, loss 0.721572.
Train: 2018-08-09T11:19:26.678529: step 6947, loss 0.500243.
Train: 2018-08-09T11:19:26.756635: step 6948, loss 0.516173.
Train: 2018-08-09T11:19:26.850395: step 6949, loss 0.578949.
Train: 2018-08-09T11:19:26.928500: step 6950, loss 0.656942.
Test: 2018-08-09T11:19:27.412766: step 6950, loss 0.548214.
Train: 2018-08-09T11:19:27.490868: step 6951, loss 0.532266.
Train: 2018-08-09T11:19:27.584595: step 6952, loss 0.594412.
Train: 2018-08-09T11:19:27.662721: step 6953, loss 0.687139.
Train: 2018-08-09T11:19:27.740811: step 6954, loss 0.578963.
Train: 2018-08-09T11:19:27.818915: step 6955, loss 0.578938.
Train: 2018-08-09T11:19:27.896994: step 6956, loss 0.578969.
Train: 2018-08-09T11:19:27.975133: step 6957, loss 0.548646.
Train: 2018-08-09T11:19:28.068857: step 6958, loss 0.594096.
Train: 2018-08-09T11:19:28.146965: step 6959, loss 0.564165.
Train: 2018-08-09T11:19:28.225087: step 6960, loss 0.594105.
Test: 2018-08-09T11:19:28.724922: step 6960, loss 0.553312.
Train: 2018-08-09T11:19:28.803028: step 6961, loss 0.563886.
Train: 2018-08-09T11:19:28.881165: step 6962, loss 0.549492.
Train: 2018-08-09T11:19:28.959274: step 6963, loss 0.503895.
Train: 2018-08-09T11:19:29.037349: step 6964, loss 0.579698.
Train: 2018-08-09T11:19:29.131106: step 6965, loss 0.654532.
Train: 2018-08-09T11:19:29.206218: step 6966, loss 0.624446.
Train: 2018-08-09T11:19:29.284356: step 6967, loss 0.489022.
Train: 2018-08-09T11:19:29.362431: step 6968, loss 0.579105.
Train: 2018-08-09T11:19:29.440569: step 6969, loss 0.503999.
Train: 2018-08-09T11:19:29.534266: step 6970, loss 0.594226.
Test: 2018-08-09T11:19:30.022753: step 6970, loss 0.549899.
Train: 2018-08-09T11:19:30.100890: step 6971, loss 0.639431.
Train: 2018-08-09T11:19:30.178996: step 6972, loss 0.609224.
Train: 2018-08-09T11:19:30.303966: step 6973, loss 0.579284.
Train: 2018-08-09T11:19:30.382043: step 6974, loss 0.594211.
Train: 2018-08-09T11:19:30.475802: step 6975, loss 0.609205.
Train: 2018-08-09T11:19:30.553880: step 6976, loss 0.609249.
Train: 2018-08-09T11:19:30.632014: step 6977, loss 0.549282.
Train: 2018-08-09T11:19:30.710122: step 6978, loss 0.474499.
Train: 2018-08-09T11:19:30.788228: step 6979, loss 0.639123.
Train: 2018-08-09T11:19:30.866335: step 6980, loss 0.51923.
Test: 2018-08-09T11:19:31.366186: step 6980, loss 0.551794.
Train: 2018-08-09T11:19:31.444323: step 6981, loss 0.639218.
Train: 2018-08-09T11:19:31.536152: step 6982, loss 0.564141.
Train: 2018-08-09T11:19:31.614255: step 6983, loss 0.563985.
Train: 2018-08-09T11:19:31.692361: step 6984, loss 0.549078.
Train: 2018-08-09T11:19:31.770468: step 6985, loss 0.594151.
Train: 2018-08-09T11:19:31.848572: step 6986, loss 0.639693.
Train: 2018-08-09T11:19:31.942303: step 6987, loss 0.518791.
Train: 2018-08-09T11:19:32.020406: step 6988, loss 0.533847.
Train: 2018-08-09T11:19:32.098517: step 6989, loss 0.533447.
Train: 2018-08-09T11:19:32.176623: step 6990, loss 0.503143.
Test: 2018-08-09T11:19:32.676478: step 6990, loss 0.552717.
Train: 2018-08-09T11:19:32.754581: step 6991, loss 0.563774.
Train: 2018-08-09T11:19:32.832719: step 6992, loss 0.61075.
Train: 2018-08-09T11:19:32.915319: step 6993, loss 0.563894.
Train: 2018-08-09T11:19:32.993457: step 6994, loss 0.562962.
Train: 2018-08-09T11:19:33.071563: step 6995, loss 0.532421.
Train: 2018-08-09T11:19:33.149668: step 6996, loss 0.578929.
Train: 2018-08-09T11:19:33.243366: step 6997, loss 0.500877.
Train: 2018-08-09T11:19:33.321473: step 6998, loss 0.611926.
Train: 2018-08-09T11:19:33.399611: step 6999, loss 0.627.
Train: 2018-08-09T11:19:33.477717: step 7000, loss 0.436611.
Test: 2018-08-09T11:19:33.977569: step 7000, loss 0.549893.
Train: 2018-08-09T11:19:34.508724: step 7001, loss 0.593899.
Train: 2018-08-09T11:19:34.602452: step 7002, loss 0.481539.
Train: 2018-08-09T11:19:34.680558: step 7003, loss 0.513446.
Train: 2018-08-09T11:19:34.758665: step 7004, loss 0.595424.
Train: 2018-08-09T11:19:34.836771: step 7005, loss 0.632182.
Train: 2018-08-09T11:19:34.916350: step 7006, loss 0.649019.
Train: 2018-08-09T11:19:34.994487: step 7007, loss 0.562482.
Train: 2018-08-09T11:19:35.072594: step 7008, loss 0.59274.
Train: 2018-08-09T11:19:35.150701: step 7009, loss 0.564971.
Train: 2018-08-09T11:19:35.244429: step 7010, loss 0.561449.
Test: 2018-08-09T11:19:35.728694: step 7010, loss 0.548105.
Train: 2018-08-09T11:19:35.869281: step 7011, loss 0.563291.
Train: 2018-08-09T11:19:35.947388: step 7012, loss 0.659357.
Train: 2018-08-09T11:19:36.025494: step 7013, loss 0.498579.
Train: 2018-08-09T11:19:36.103601: step 7014, loss 0.532293.
Train: 2018-08-09T11:19:36.181708: step 7015, loss 0.595436.
Train: 2018-08-09T11:19:36.275436: step 7016, loss 0.594367.
Train: 2018-08-09T11:19:36.353542: step 7017, loss 0.530874.
Train: 2018-08-09T11:19:36.431648: step 7018, loss 0.578498.
Train: 2018-08-09T11:19:36.509759: step 7019, loss 0.499602.
Train: 2018-08-09T11:19:36.587863: step 7020, loss 0.483709.
Test: 2018-08-09T11:19:37.089973: step 7020, loss 0.551105.
Train: 2018-08-09T11:19:37.168110: step 7021, loss 0.563626.
Train: 2018-08-09T11:19:37.246217: step 7022, loss 0.483011.
Train: 2018-08-09T11:19:37.324293: step 7023, loss 0.578707.
Train: 2018-08-09T11:19:37.402399: step 7024, loss 0.642568.
Train: 2018-08-09T11:19:37.480505: step 7025, loss 0.578968.
Train: 2018-08-09T11:19:37.558643: step 7026, loss 0.611368.
Train: 2018-08-09T11:19:37.636751: step 7027, loss 0.577771.
Train: 2018-08-09T11:19:37.722531: step 7028, loss 0.660693.
Train: 2018-08-09T11:19:37.800633: step 7029, loss 0.610643.
Train: 2018-08-09T11:19:37.894366: step 7030, loss 0.547664.
Test: 2018-08-09T11:19:38.378598: step 7030, loss 0.547879.
Train: 2018-08-09T11:19:38.472349: step 7031, loss 0.530841.
Train: 2018-08-09T11:19:38.550461: step 7032, loss 0.691057.
Train: 2018-08-09T11:19:38.628564: step 7033, loss 0.547615.
Train: 2018-08-09T11:19:38.706675: step 7034, loss 0.610186.
Train: 2018-08-09T11:19:38.784750: step 7035, loss 0.500244.
Train: 2018-08-09T11:19:38.862889: step 7036, loss 0.547443.
Train: 2018-08-09T11:19:38.950357: step 7037, loss 0.626035.
Train: 2018-08-09T11:19:39.028465: step 7038, loss 0.531927.
Train: 2018-08-09T11:19:39.106597: step 7039, loss 0.56322.
Train: 2018-08-09T11:19:39.184678: step 7040, loss 0.453951.
Test: 2018-08-09T11:19:39.684560: step 7040, loss 0.548577.
Train: 2018-08-09T11:19:39.762667: step 7041, loss 0.48503.
Train: 2018-08-09T11:19:39.840803: step 7042, loss 0.531806.
Train: 2018-08-09T11:19:39.918910: step 7043, loss 0.610392.
Train: 2018-08-09T11:19:39.997016: step 7044, loss 0.578834.
Train: 2018-08-09T11:19:40.090739: step 7045, loss 0.59464.
Train: 2018-08-09T11:19:40.168846: step 7046, loss 0.499576.
Train: 2018-08-09T11:19:40.246959: step 7047, loss 0.578933.
Train: 2018-08-09T11:19:40.325064: step 7048, loss 0.515075.
Train: 2018-08-09T11:19:40.418790: step 7049, loss 0.54675.
Train: 2018-08-09T11:19:40.496898: step 7050, loss 0.514427.
Test: 2018-08-09T11:19:40.981129: step 7050, loss 0.55056.
Train: 2018-08-09T11:19:41.074887: step 7051, loss 0.530064.
Train: 2018-08-09T11:19:41.152964: step 7052, loss 0.611243.
Train: 2018-08-09T11:19:41.231101: step 7053, loss 0.512847.
Train: 2018-08-09T11:19:41.309177: step 7054, loss 0.528338.
Train: 2018-08-09T11:19:41.387283: step 7055, loss 0.614321.
Train: 2018-08-09T11:19:41.465417: step 7056, loss 0.562601.
Train: 2018-08-09T11:19:41.559149: step 7057, loss 0.578814.
Train: 2018-08-09T11:19:41.637254: step 7058, loss 0.423374.
Train: 2018-08-09T11:19:41.715333: step 7059, loss 0.529846.
Train: 2018-08-09T11:19:41.793468: step 7060, loss 0.523125.
Test: 2018-08-09T11:19:42.295686: step 7060, loss 0.546374.
Train: 2018-08-09T11:19:42.373767: step 7061, loss 0.546556.
Train: 2018-08-09T11:19:42.451908: step 7062, loss 0.519906.
Train: 2018-08-09T11:19:42.530012: step 7063, loss 0.604518.
Train: 2018-08-09T11:19:42.608088: step 7064, loss 0.579757.
Train: 2018-08-09T11:19:42.686220: step 7065, loss 0.5069.
Train: 2018-08-09T11:19:42.779924: step 7066, loss 0.659031.
Train: 2018-08-09T11:19:42.858053: step 7067, loss 0.544324.
Train: 2018-08-09T11:19:42.936165: step 7068, loss 0.507008.
Train: 2018-08-09T11:19:43.014243: step 7069, loss 0.59434.
Train: 2018-08-09T11:19:43.092349: step 7070, loss 0.541807.
Test: 2018-08-09T11:19:43.592231: step 7070, loss 0.547257.
Train: 2018-08-09T11:19:43.670337: step 7071, loss 0.54039.
Train: 2018-08-09T11:19:43.748444: step 7072, loss 0.58139.
Train: 2018-08-09T11:19:43.826581: step 7073, loss 0.438139.
Train: 2018-08-09T11:19:43.920903: step 7074, loss 0.576762.
Train: 2018-08-09T11:19:43.999043: step 7075, loss 0.687302.
Train: 2018-08-09T11:19:44.077147: step 7076, loss 0.542124.
Train: 2018-08-09T11:19:44.155253: step 7077, loss 0.612689.
Train: 2018-08-09T11:19:44.233360: step 7078, loss 0.480938.
Train: 2018-08-09T11:19:44.311471: step 7079, loss 0.477592.
Train: 2018-08-09T11:19:44.389573: step 7080, loss 0.589038.
Test: 2018-08-09T11:19:44.889450: step 7080, loss 0.54807.
Train: 2018-08-09T11:19:44.967556: step 7081, loss 0.602295.
Train: 2018-08-09T11:19:45.045668: step 7082, loss 0.455276.
Train: 2018-08-09T11:19:45.139396: step 7083, loss 0.568808.
Train: 2018-08-09T11:19:45.217506: step 7084, loss 0.55242.
Train: 2018-08-09T11:19:45.295603: step 7085, loss 0.462193.
Train: 2018-08-09T11:19:45.373719: step 7086, loss 0.52893.
Train: 2018-08-09T11:19:45.451822: step 7087, loss 0.531277.
Train: 2018-08-09T11:19:45.529899: step 7088, loss 0.547233.
Train: 2018-08-09T11:19:45.623658: step 7089, loss 0.542922.
Train: 2018-08-09T11:19:45.701764: step 7090, loss 0.578652.
Test: 2018-08-09T11:19:46.201616: step 7090, loss 0.548795.
Train: 2018-08-09T11:19:46.279723: step 7091, loss 0.630109.
Train: 2018-08-09T11:19:46.357829: step 7092, loss 0.596333.
Train: 2018-08-09T11:19:46.435969: step 7093, loss 0.598847.
Train: 2018-08-09T11:19:46.514073: step 7094, loss 0.498594.
Train: 2018-08-09T11:19:46.607794: step 7095, loss 0.513047.
Train: 2018-08-09T11:19:46.685907: step 7096, loss 0.580794.
Train: 2018-08-09T11:19:46.764013: step 7097, loss 0.512886.
Train: 2018-08-09T11:19:46.842123: step 7098, loss 0.463202.
Train: 2018-08-09T11:19:46.921655: step 7099, loss 0.546306.
Train: 2018-08-09T11:19:46.999765: step 7100, loss 0.615184.
Test: 2018-08-09T11:19:47.499618: step 7100, loss 0.548626.
Train: 2018-08-09T11:19:48.077636: step 7101, loss 0.595693.
Train: 2018-08-09T11:19:48.155743: step 7102, loss 0.478232.
Train: 2018-08-09T11:19:48.233849: step 7103, loss 0.562946.
Train: 2018-08-09T11:19:48.311956: step 7104, loss 0.614117.
Train: 2018-08-09T11:19:48.390063: step 7105, loss 0.528584.
Train: 2018-08-09T11:19:48.483786: step 7106, loss 0.678506.
Train: 2018-08-09T11:19:48.561896: step 7107, loss 0.561763.
Train: 2018-08-09T11:19:48.640004: step 7108, loss 0.612519.
Train: 2018-08-09T11:19:48.718106: step 7109, loss 0.512904.
Train: 2018-08-09T11:19:48.796217: step 7110, loss 0.5461.
Test: 2018-08-09T11:19:49.297611: step 7110, loss 0.54904.
Train: 2018-08-09T11:19:49.375716: step 7111, loss 0.496752.
Train: 2018-08-09T11:19:49.453822: step 7112, loss 0.59553.
Train: 2018-08-09T11:19:49.547520: step 7113, loss 0.496279.
Train: 2018-08-09T11:19:49.625656: step 7114, loss 0.546735.
Train: 2018-08-09T11:19:49.703763: step 7115, loss 0.529038.
Train: 2018-08-09T11:19:49.781841: step 7116, loss 0.57994.
Train: 2018-08-09T11:19:49.859976: step 7117, loss 0.529948.
Train: 2018-08-09T11:19:49.938086: step 7118, loss 0.495397.
Train: 2018-08-09T11:19:50.016187: step 7119, loss 0.597968.
Train: 2018-08-09T11:19:50.109917: step 7120, loss 0.544793.
Test: 2018-08-09T11:19:50.594181: step 7120, loss 0.549726.
Train: 2018-08-09T11:19:50.687907: step 7121, loss 0.511246.
Train: 2018-08-09T11:19:50.766022: step 7122, loss 0.525996.
Train: 2018-08-09T11:19:50.844089: step 7123, loss 0.52811.
Train: 2018-08-09T11:19:50.923584: step 7124, loss 0.580318.
Train: 2018-08-09T11:19:51.001692: step 7125, loss 0.615757.
Train: 2018-08-09T11:19:51.079795: step 7126, loss 0.543695.
Train: 2018-08-09T11:19:51.157904: step 7127, loss 0.510925.
Train: 2018-08-09T11:19:51.251633: step 7128, loss 0.562709.
Train: 2018-08-09T11:19:51.329739: step 7129, loss 0.63537.
Train: 2018-08-09T11:19:51.407845: step 7130, loss 0.614684.
Test: 2018-08-09T11:19:51.892076: step 7130, loss 0.5461.
Train: 2018-08-09T11:19:51.970184: step 7131, loss 0.51226.
Train: 2018-08-09T11:19:52.063940: step 7132, loss 0.529528.
Train: 2018-08-09T11:19:52.142048: step 7133, loss 0.562482.
Train: 2018-08-09T11:19:52.220123: step 7134, loss 0.562878.
Train: 2018-08-09T11:19:52.298260: step 7135, loss 0.560393.
Train: 2018-08-09T11:19:52.376339: step 7136, loss 0.594381.
Train: 2018-08-09T11:19:52.454475: step 7137, loss 0.646489.
Train: 2018-08-09T11:19:52.548195: step 7138, loss 0.529037.
Train: 2018-08-09T11:19:52.626310: step 7139, loss 0.577627.
Train: 2018-08-09T11:19:52.704415: step 7140, loss 0.743984.
Test: 2018-08-09T11:19:53.205621: step 7140, loss 0.549762.
Train: 2018-08-09T11:19:53.283759: step 7141, loss 0.628863.
Train: 2018-08-09T11:19:53.361864: step 7142, loss 0.546411.
Train: 2018-08-09T11:19:53.439966: step 7143, loss 0.514643.
Train: 2018-08-09T11:19:53.518077: step 7144, loss 0.484319.
Train: 2018-08-09T11:19:53.596183: step 7145, loss 0.531694.
Train: 2018-08-09T11:19:53.689906: step 7146, loss 0.641985.
Train: 2018-08-09T11:19:53.768018: step 7147, loss 0.51508.
Train: 2018-08-09T11:19:53.846125: step 7148, loss 0.578884.
Train: 2018-08-09T11:19:53.924232: step 7149, loss 0.626836.
Train: 2018-08-09T11:19:54.002308: step 7150, loss 0.594916.
Test: 2018-08-09T11:19:54.499118: step 7150, loss 0.550063.
Train: 2018-08-09T11:19:54.577254: step 7151, loss 0.484123.
Train: 2018-08-09T11:19:54.655364: step 7152, loss 0.642195.
Train: 2018-08-09T11:19:54.749060: step 7153, loss 0.500149.
Train: 2018-08-09T11:19:54.827190: step 7154, loss 0.641848.
Train: 2018-08-09T11:19:54.910625: step 7155, loss 0.516081.
Train: 2018-08-09T11:19:54.988734: step 7156, loss 0.610261.
Train: 2018-08-09T11:19:55.066871: step 7157, loss 0.500532.
Train: 2018-08-09T11:19:55.144976: step 7158, loss 0.625822.
Train: 2018-08-09T11:19:55.223087: step 7159, loss 0.563237.
Train: 2018-08-09T11:19:55.301158: step 7160, loss 0.594489.
Test: 2018-08-09T11:19:55.801072: step 7160, loss 0.549734.
Train: 2018-08-09T11:19:55.879148: step 7161, loss 0.578955.
Train: 2018-08-09T11:19:55.957284: step 7162, loss 0.500625.
Train: 2018-08-09T11:19:56.035388: step 7163, loss 0.625937.
Train: 2018-08-09T11:19:56.129120: step 7164, loss 0.469219.
Train: 2018-08-09T11:19:56.207230: step 7165, loss 0.594713.
Train: 2018-08-09T11:19:56.285305: step 7166, loss 0.48457.
Train: 2018-08-09T11:19:56.363439: step 7167, loss 0.562959.
Train: 2018-08-09T11:19:56.441515: step 7168, loss 0.436122.
Train: 2018-08-09T11:19:56.519648: step 7169, loss 0.514557.
Train: 2018-08-09T11:19:56.613380: step 7170, loss 0.496706.
Test: 2018-08-09T11:19:57.097612: step 7170, loss 0.550606.
Train: 2018-08-09T11:19:57.175748: step 7171, loss 0.613011.
Train: 2018-08-09T11:19:57.269444: step 7172, loss 0.546305.
Train: 2018-08-09T11:19:57.347582: step 7173, loss 0.511661.
Train: 2018-08-09T11:19:57.425689: step 7174, loss 0.505746.
Train: 2018-08-09T11:19:57.503790: step 7175, loss 0.670039.
Train: 2018-08-09T11:19:57.581900: step 7176, loss 0.564356.
Train: 2018-08-09T11:19:57.675630: step 7177, loss 0.655357.
Train: 2018-08-09T11:19:57.753736: step 7178, loss 0.512555.
Train: 2018-08-09T11:19:57.831844: step 7179, loss 0.548414.
Train: 2018-08-09T11:19:57.912232: step 7180, loss 0.563361.
Test: 2018-08-09T11:19:58.412141: step 7180, loss 0.546736.
Train: 2018-08-09T11:19:58.490253: step 7181, loss 0.580115.
Train: 2018-08-09T11:19:58.568353: step 7182, loss 0.660797.
Train: 2018-08-09T11:19:58.646466: step 7183, loss 0.480456.
Train: 2018-08-09T11:19:58.740194: step 7184, loss 0.546247.
Train: 2018-08-09T11:19:58.818271: step 7185, loss 0.578946.
Train: 2018-08-09T11:19:58.896408: step 7186, loss 0.627821.
Train: 2018-08-09T11:19:58.974514: step 7187, loss 0.432582.
Train: 2018-08-09T11:19:59.052619: step 7188, loss 0.529943.
Train: 2018-08-09T11:19:59.130727: step 7189, loss 0.56255.
Train: 2018-08-09T11:19:59.208803: step 7190, loss 0.530136.
Test: 2018-08-09T11:19:59.708685: step 7190, loss 0.547886.
Train: 2018-08-09T11:19:59.786821: step 7191, loss 0.464765.
Train: 2018-08-09T11:19:59.864897: step 7192, loss 0.644302.
Train: 2018-08-09T11:19:59.944330: step 7193, loss 0.513278.
Train: 2018-08-09T11:20:00.038086: step 7194, loss 0.562312.
Train: 2018-08-09T11:20:00.116193: step 7195, loss 0.661845.
Train: 2018-08-09T11:20:00.194303: step 7196, loss 0.51332.
Train: 2018-08-09T11:20:00.272406: step 7197, loss 0.528952.
Train: 2018-08-09T11:20:00.356137: step 7198, loss 0.529295.
Train: 2018-08-09T11:20:00.434245: step 7199, loss 0.446286.
Train: 2018-08-09T11:20:00.512351: step 7200, loss 0.5458.
Test: 2018-08-09T11:20:01.012245: step 7200, loss 0.546741.
Train: 2018-08-09T11:20:01.574600: step 7201, loss 0.563026.
Train: 2018-08-09T11:20:01.668329: step 7202, loss 0.54514.
Train: 2018-08-09T11:20:01.746464: step 7203, loss 0.579147.
Train: 2018-08-09T11:20:01.824571: step 7204, loss 0.646377.
Train: 2018-08-09T11:20:01.902678: step 7205, loss 0.545414.
Train: 2018-08-09T11:20:01.980785: step 7206, loss 0.562526.
Train: 2018-08-09T11:20:02.058889: step 7207, loss 0.528073.
Train: 2018-08-09T11:20:02.137001: step 7208, loss 0.409431.
Train: 2018-08-09T11:20:02.230726: step 7209, loss 0.511734.
Train: 2018-08-09T11:20:02.308804: step 7210, loss 0.63201.
Test: 2018-08-09T11:20:02.810323: step 7210, loss 0.547461.
Train: 2018-08-09T11:20:02.888461: step 7211, loss 0.579145.
Train: 2018-08-09T11:20:02.966562: step 7212, loss 0.580341.
Train: 2018-08-09T11:20:03.044674: step 7213, loss 0.649078.
Train: 2018-08-09T11:20:03.122781: step 7214, loss 0.578355.
Train: 2018-08-09T11:20:03.200887: step 7215, loss 0.510179.
Train: 2018-08-09T11:20:03.278995: step 7216, loss 0.510884.
Train: 2018-08-09T11:20:03.372692: step 7217, loss 0.646511.
Train: 2018-08-09T11:20:03.450828: step 7218, loss 0.614379.
Train: 2018-08-09T11:20:03.528905: step 7219, loss 0.648238.
Train: 2018-08-09T11:20:03.607010: step 7220, loss 0.545745.
Test: 2018-08-09T11:20:04.106895: step 7220, loss 0.548542.
Train: 2018-08-09T11:20:04.185030: step 7221, loss 0.596334.
Train: 2018-08-09T11:20:04.263139: step 7222, loss 0.545186.
Train: 2018-08-09T11:20:04.356865: step 7223, loss 0.546126.
Train: 2018-08-09T11:20:04.434972: step 7224, loss 0.508552.
Train: 2018-08-09T11:20:04.513078: step 7225, loss 0.380485.
Train: 2018-08-09T11:20:04.591182: step 7226, loss 0.562041.
Train: 2018-08-09T11:20:04.669291: step 7227, loss 0.49582.
Train: 2018-08-09T11:20:04.747398: step 7228, loss 0.578847.
Train: 2018-08-09T11:20:04.825499: step 7229, loss 0.546728.
Train: 2018-08-09T11:20:04.903610: step 7230, loss 0.562403.
Test: 2018-08-09T11:20:05.403463: step 7230, loss 0.548473.
Train: 2018-08-09T11:20:05.497215: step 7231, loss 0.595852.
Train: 2018-08-09T11:20:05.575298: step 7232, loss 0.545113.
Train: 2018-08-09T11:20:05.653405: step 7233, loss 0.580107.
Train: 2018-08-09T11:20:05.731540: step 7234, loss 0.68005.
Train: 2018-08-09T11:20:05.825269: step 7235, loss 0.612574.
Train: 2018-08-09T11:20:05.903346: step 7236, loss 0.546112.
Train: 2018-08-09T11:20:05.983758: step 7237, loss 0.579137.
Train: 2018-08-09T11:20:06.061864: step 7238, loss 0.562173.
Train: 2018-08-09T11:20:06.139970: step 7239, loss 0.529486.
Train: 2018-08-09T11:20:06.218077: step 7240, loss 0.595035.
Test: 2018-08-09T11:20:06.717953: step 7240, loss 0.548226.
Train: 2018-08-09T11:20:06.796068: step 7241, loss 0.479579.
Train: 2018-08-09T11:20:06.874172: step 7242, loss 0.628896.
Train: 2018-08-09T11:20:06.967900: step 7243, loss 0.52924.
Train: 2018-08-09T11:20:07.045976: step 7244, loss 0.578767.
Train: 2018-08-09T11:20:07.124113: step 7245, loss 0.529475.
Train: 2018-08-09T11:20:07.202190: step 7246, loss 0.512793.
Train: 2018-08-09T11:20:07.280296: step 7247, loss 0.562513.
Train: 2018-08-09T11:20:07.374048: step 7248, loss 0.595001.
Train: 2018-08-09T11:20:07.452161: step 7249, loss 0.496325.
Train: 2018-08-09T11:20:07.530267: step 7250, loss 0.596575.
Test: 2018-08-09T11:20:08.030870: step 7250, loss 0.54816.
Train: 2018-08-09T11:20:08.109007: step 7251, loss 0.695631.
Train: 2018-08-09T11:20:08.187113: step 7252, loss 0.578532.
Train: 2018-08-09T11:20:08.265190: step 7253, loss 0.694161.
Train: 2018-08-09T11:20:08.343330: step 7254, loss 0.545974.
Train: 2018-08-09T11:20:08.421435: step 7255, loss 0.49814.
Train: 2018-08-09T11:20:08.499540: step 7256, loss 0.644445.
Train: 2018-08-09T11:20:08.593271: step 7257, loss 0.579234.
Train: 2018-08-09T11:20:08.671375: step 7258, loss 0.498496.
Train: 2018-08-09T11:20:08.749452: step 7259, loss 0.514685.
Train: 2018-08-09T11:20:08.827587: step 7260, loss 0.594871.
Test: 2018-08-09T11:20:09.321355: step 7260, loss 0.550923.
Train: 2018-08-09T11:20:09.399474: step 7261, loss 0.578883.
Train: 2018-08-09T11:20:09.493204: step 7262, loss 0.626721.
Train: 2018-08-09T11:20:09.571314: step 7263, loss 0.578704.
Train: 2018-08-09T11:20:09.649387: step 7264, loss 0.467694.
Train: 2018-08-09T11:20:09.727524: step 7265, loss 0.515395.
Train: 2018-08-09T11:20:09.805627: step 7266, loss 0.57858.
Train: 2018-08-09T11:20:09.883737: step 7267, loss 0.610726.
Train: 2018-08-09T11:20:09.977435: step 7268, loss 0.546824.
Train: 2018-08-09T11:20:10.055566: step 7269, loss 0.531047.
Train: 2018-08-09T11:20:10.133679: step 7270, loss 0.579543.
Test: 2018-08-09T11:20:10.633556: step 7270, loss 0.548153.
Train: 2018-08-09T11:20:10.711661: step 7271, loss 0.529581.
Train: 2018-08-09T11:20:10.789773: step 7272, loss 0.611325.
Train: 2018-08-09T11:20:10.867881: step 7273, loss 0.560538.
Train: 2018-08-09T11:20:10.945958: step 7274, loss 0.562566.
Train: 2018-08-09T11:20:11.024093: step 7275, loss 0.577619.
Train: 2018-08-09T11:20:11.102201: step 7276, loss 0.59159.
Train: 2018-08-09T11:20:11.180307: step 7277, loss 0.594438.
Train: 2018-08-09T11:20:11.274034: step 7278, loss 0.581038.
Train: 2018-08-09T11:20:11.352110: step 7279, loss 0.543206.
Train: 2018-08-09T11:20:11.430248: step 7280, loss 0.583154.
Test: 2018-08-09T11:20:11.930102: step 7280, loss 0.546157.
Train: 2018-08-09T11:20:12.008237: step 7281, loss 0.567649.
Train: 2018-08-09T11:20:12.086345: step 7282, loss 0.547747.
Train: 2018-08-09T11:20:12.164420: step 7283, loss 0.510983.
Train: 2018-08-09T11:20:12.242556: step 7284, loss 0.492581.
Train: 2018-08-09T11:20:12.320663: step 7285, loss 0.616157.
Train: 2018-08-09T11:20:12.398769: step 7286, loss 0.456947.
Train: 2018-08-09T11:20:12.476877: step 7287, loss 0.462481.
Train: 2018-08-09T11:20:12.570605: step 7288, loss 0.580043.
Train: 2018-08-09T11:20:12.648712: step 7289, loss 0.619441.
Train: 2018-08-09T11:20:12.726818: step 7290, loss 0.577081.
Test: 2018-08-09T11:20:13.213320: step 7290, loss 0.549259.
Train: 2018-08-09T11:20:13.307046: step 7291, loss 0.669338.
Train: 2018-08-09T11:20:13.385179: step 7292, loss 0.591018.
Train: 2018-08-09T11:20:13.463290: step 7293, loss 0.565625.
Train: 2018-08-09T11:20:13.541396: step 7294, loss 0.52839.
Train: 2018-08-09T11:20:13.619472: step 7295, loss 0.614487.
Train: 2018-08-09T11:20:13.697605: step 7296, loss 0.448517.
Train: 2018-08-09T11:20:13.791337: step 7297, loss 0.516975.
Train: 2018-08-09T11:20:13.869414: step 7298, loss 0.545827.
Train: 2018-08-09T11:20:13.947521: step 7299, loss 0.647396.
Train: 2018-08-09T11:20:14.025657: step 7300, loss 0.468042.
Test: 2018-08-09T11:20:14.525550: step 7300, loss 0.549021.
Train: 2018-08-09T11:20:15.089279: step 7301, loss 0.563435.
Train: 2018-08-09T11:20:15.167415: step 7302, loss 0.531517.
Train: 2018-08-09T11:20:15.245491: step 7303, loss 0.577966.
Train: 2018-08-09T11:20:15.323628: step 7304, loss 0.515544.
Train: 2018-08-09T11:20:15.401734: step 7305, loss 0.514961.
Train: 2018-08-09T11:20:15.479841: step 7306, loss 0.65792.
Train: 2018-08-09T11:20:15.573569: step 7307, loss 0.498976.
Train: 2018-08-09T11:20:15.651674: step 7308, loss 0.499209.
Train: 2018-08-09T11:20:15.729782: step 7309, loss 0.49869.
Train: 2018-08-09T11:20:15.807890: step 7310, loss 0.610811.
Test: 2018-08-09T11:20:16.307740: step 7310, loss 0.548154.
Train: 2018-08-09T11:20:16.385877: step 7311, loss 0.611392.
Train: 2018-08-09T11:20:16.463984: step 7312, loss 0.611374.
Train: 2018-08-09T11:20:16.542092: step 7313, loss 0.497172.
Train: 2018-08-09T11:20:16.635789: step 7314, loss 0.644691.
Train: 2018-08-09T11:20:16.713897: step 7315, loss 0.512352.
Train: 2018-08-09T11:20:16.792032: step 7316, loss 0.578406.
Train: 2018-08-09T11:20:16.870132: step 7317, loss 0.659117.
Train: 2018-08-09T11:20:16.949554: step 7318, loss 0.46447.
Train: 2018-08-09T11:20:17.027664: step 7319, loss 0.677073.
Train: 2018-08-09T11:20:17.105768: step 7320, loss 0.514276.
Test: 2018-08-09T11:20:17.605679: step 7320, loss 0.548567.
Train: 2018-08-09T11:20:17.683726: step 7321, loss 0.496941.
Train: 2018-08-09T11:20:17.777455: step 7322, loss 0.530954.
Train: 2018-08-09T11:20:17.855563: step 7323, loss 0.562847.
Train: 2018-08-09T11:20:17.933698: step 7324, loss 0.595298.
Train: 2018-08-09T11:20:18.011775: step 7325, loss 0.595921.
Train: 2018-08-09T11:20:18.089913: step 7326, loss 0.481883.
Train: 2018-08-09T11:20:18.183610: step 7327, loss 0.577628.
Train: 2018-08-09T11:20:18.261746: step 7328, loss 0.512042.
Train: 2018-08-09T11:20:18.339853: step 7329, loss 0.579563.
Train: 2018-08-09T11:20:18.417929: step 7330, loss 0.529293.
Test: 2018-08-09T11:20:18.919338: step 7330, loss 0.548133.
Train: 2018-08-09T11:20:18.997470: step 7331, loss 0.513405.
Train: 2018-08-09T11:20:19.075551: step 7332, loss 0.529546.
Train: 2018-08-09T11:20:19.153690: step 7333, loss 0.595452.
Train: 2018-08-09T11:20:19.231795: step 7334, loss 0.577775.
Train: 2018-08-09T11:20:19.309873: step 7335, loss 0.496707.
Train: 2018-08-09T11:20:19.403625: step 7336, loss 0.544487.
Train: 2018-08-09T11:20:19.481737: step 7337, loss 0.545683.
Train: 2018-08-09T11:20:19.559846: step 7338, loss 0.511676.
Train: 2018-08-09T11:20:19.637951: step 7339, loss 0.511217.
Train: 2018-08-09T11:20:19.716056: step 7340, loss 0.542446.
Test: 2018-08-09T11:20:20.215908: step 7340, loss 0.54603.
Train: 2018-08-09T11:20:20.294015: step 7341, loss 0.612599.
Train: 2018-08-09T11:20:20.372147: step 7342, loss 0.669626.
Train: 2018-08-09T11:20:20.465880: step 7343, loss 0.63496.
Train: 2018-08-09T11:20:20.543956: step 7344, loss 0.59355.
Train: 2018-08-09T11:20:20.622092: step 7345, loss 0.694602.
Train: 2018-08-09T11:20:20.700195: step 7346, loss 0.627591.
Train: 2018-08-09T11:20:20.778306: step 7347, loss 0.431603.
Train: 2018-08-09T11:20:20.856383: step 7348, loss 0.595949.
Train: 2018-08-09T11:20:20.951539: step 7349, loss 0.512665.
Train: 2018-08-09T11:20:21.029649: step 7350, loss 0.462354.
Test: 2018-08-09T11:20:21.513881: step 7350, loss 0.547595.
Train: 2018-08-09T11:20:21.592019: step 7351, loss 0.497814.
Train: 2018-08-09T11:20:21.685745: step 7352, loss 0.496366.
Train: 2018-08-09T11:20:21.763821: step 7353, loss 0.613045.
Train: 2018-08-09T11:20:21.841959: step 7354, loss 0.6293.
Train: 2018-08-09T11:20:21.920035: step 7355, loss 0.513644.
Train: 2018-08-09T11:20:21.998142: step 7356, loss 0.546042.
Train: 2018-08-09T11:20:22.076279: step 7357, loss 0.646994.
Train: 2018-08-09T11:20:22.169976: step 7358, loss 0.59608.
Train: 2018-08-09T11:20:22.248113: step 7359, loss 0.594085.
Train: 2018-08-09T11:20:22.326220: step 7360, loss 0.51447.
Test: 2018-08-09T11:20:22.826097: step 7360, loss 0.552913.
Train: 2018-08-09T11:20:22.904207: step 7361, loss 0.627982.
Train: 2018-08-09T11:20:22.983659: step 7362, loss 0.49877.
Train: 2018-08-09T11:20:23.061767: step 7363, loss 0.51377.
Train: 2018-08-09T11:20:23.139873: step 7364, loss 0.626961.
Train: 2018-08-09T11:20:23.217983: step 7365, loss 0.546547.
Train: 2018-08-09T11:20:23.296081: step 7366, loss 0.498253.
Train: 2018-08-09T11:20:23.389783: step 7367, loss 0.498026.
Train: 2018-08-09T11:20:23.467924: step 7368, loss 0.562791.
Train: 2018-08-09T11:20:23.546027: step 7369, loss 0.595386.
Train: 2018-08-09T11:20:23.624134: step 7370, loss 0.643735.
Test: 2018-08-09T11:20:24.124024: step 7370, loss 0.548162.
Train: 2018-08-09T11:20:24.202122: step 7371, loss 0.57816.
Train: 2018-08-09T11:20:24.280228: step 7372, loss 0.67591.
Train: 2018-08-09T11:20:24.358331: step 7373, loss 0.595012.
Train: 2018-08-09T11:20:24.436442: step 7374, loss 0.578872.
Train: 2018-08-09T11:20:24.530170: step 7375, loss 0.579037.
Train: 2018-08-09T11:20:24.608278: step 7376, loss 0.579814.
Train: 2018-08-09T11:20:24.686383: step 7377, loss 0.483865.
Train: 2018-08-09T11:20:24.764491: step 7378, loss 0.626016.
Train: 2018-08-09T11:20:24.842601: step 7379, loss 0.641249.
Train: 2018-08-09T11:20:24.921412: step 7380, loss 0.594815.
Test: 2018-08-09T11:20:25.421295: step 7380, loss 0.551177.
Train: 2018-08-09T11:20:25.499400: step 7381, loss 0.5152.
Train: 2018-08-09T11:20:25.577539: step 7382, loss 0.549053.
Train: 2018-08-09T11:20:25.671267: step 7383, loss 0.610803.
Train: 2018-08-09T11:20:25.749343: step 7384, loss 0.563932.
Train: 2018-08-09T11:20:25.832666: step 7385, loss 0.515693.
Train: 2018-08-09T11:20:25.912190: step 7386, loss 0.467441.
Train: 2018-08-09T11:20:25.990292: step 7387, loss 0.672334.
Train: 2018-08-09T11:20:26.068404: step 7388, loss 0.597734.
Train: 2018-08-09T11:20:26.146510: step 7389, loss 0.530638.
Train: 2018-08-09T11:20:26.224612: step 7390, loss 0.496968.
Test: 2018-08-09T11:20:26.724469: step 7390, loss 0.548783.
Train: 2018-08-09T11:20:26.802575: step 7391, loss 0.48192.
Train: 2018-08-09T11:20:26.880682: step 7392, loss 0.531334.
Train: 2018-08-09T11:20:26.974445: step 7393, loss 0.445247.
Train: 2018-08-09T11:20:27.052547: step 7394, loss 0.601372.
Train: 2018-08-09T11:20:27.130653: step 7395, loss 0.495145.
Train: 2018-08-09T11:20:27.208759: step 7396, loss 0.430197.
Train: 2018-08-09T11:20:27.286865: step 7397, loss 0.442609.
Train: 2018-08-09T11:20:27.364941: step 7398, loss 0.553971.
Train: 2018-08-09T11:20:27.458696: step 7399, loss 0.632308.
Train: 2018-08-09T11:20:27.536807: step 7400, loss 0.576548.
Test: 2018-08-09T11:20:28.023275: step 7400, loss 0.548512.
Train: 2018-08-09T11:20:28.585610: step 7401, loss 0.503629.
Train: 2018-08-09T11:20:28.663718: step 7402, loss 0.618975.
Train: 2018-08-09T11:20:28.757472: step 7403, loss 0.525719.
Train: 2018-08-09T11:20:28.835582: step 7404, loss 0.5593.
Train: 2018-08-09T11:20:28.913688: step 7405, loss 0.594363.
Train: 2018-08-09T11:20:28.991794: step 7406, loss 0.562057.
Train: 2018-08-09T11:20:29.069870: step 7407, loss 0.546603.
Train: 2018-08-09T11:20:29.148009: step 7408, loss 0.561499.
Train: 2018-08-09T11:20:29.241737: step 7409, loss 0.562162.
Train: 2018-08-09T11:20:29.319843: step 7410, loss 0.670766.
Test: 2018-08-09T11:20:29.804083: step 7410, loss 0.546978.
Train: 2018-08-09T11:20:29.882212: step 7411, loss 0.59851.
Train: 2018-08-09T11:20:29.961818: step 7412, loss 0.56094.
Train: 2018-08-09T11:20:30.055546: step 7413, loss 0.529968.
Train: 2018-08-09T11:20:30.133647: step 7414, loss 0.547048.
Train: 2018-08-09T11:20:30.211759: step 7415, loss 0.512875.
Train: 2018-08-09T11:20:30.289836: step 7416, loss 0.529266.
Train: 2018-08-09T11:20:30.367972: step 7417, loss 0.710699.
Train: 2018-08-09T11:20:30.461670: step 7418, loss 0.56215.
Train: 2018-08-09T11:20:30.539806: step 7419, loss 0.481628.
Train: 2018-08-09T11:20:30.617913: step 7420, loss 0.497868.
Test: 2018-08-09T11:20:31.117797: step 7420, loss 0.55053.
Train: 2018-08-09T11:20:31.195902: step 7421, loss 0.692427.
Train: 2018-08-09T11:20:31.274009: step 7422, loss 0.44932.
Train: 2018-08-09T11:20:31.352112: step 7423, loss 0.562721.
Train: 2018-08-09T11:20:31.445837: step 7424, loss 0.497948.
Train: 2018-08-09T11:20:31.529477: step 7425, loss 0.611301.
Train: 2018-08-09T11:20:31.598162: step 7426, loss 0.562648.
Train: 2018-08-09T11:20:31.676267: step 7427, loss 0.5627.
Train: 2018-08-09T11:20:31.770025: step 7428, loss 0.611376.
Train: 2018-08-09T11:20:31.848126: step 7429, loss 0.562672.
Train: 2018-08-09T11:20:31.931796: step 7430, loss 0.562668.
Test: 2018-08-09T11:20:32.416052: step 7430, loss 0.551158.
Train: 2018-08-09T11:20:32.509785: step 7431, loss 0.562624.
Train: 2018-08-09T11:20:32.587891: step 7432, loss 0.513977.
Train: 2018-08-09T11:20:32.665969: step 7433, loss 0.530131.
Train: 2018-08-09T11:20:32.744104: step 7434, loss 0.481208.
Train: 2018-08-09T11:20:32.822211: step 7435, loss 0.463556.
Train: 2018-08-09T11:20:32.900322: step 7436, loss 0.578636.
Train: 2018-08-09T11:20:32.994045: step 7437, loss 0.64621.
Train: 2018-08-09T11:20:33.072154: step 7438, loss 0.596598.
Train: 2018-08-09T11:20:33.150262: step 7439, loss 0.559142.
Train: 2018-08-09T11:20:33.228334: step 7440, loss 0.544767.
Test: 2018-08-09T11:20:33.728220: step 7440, loss 0.546754.
Train: 2018-08-09T11:20:33.806354: step 7441, loss 0.581608.
Train: 2018-08-09T11:20:33.880471: step 7442, loss 0.565494.
Train: 2018-08-09T11:20:33.974195: step 7443, loss 0.581319.
Train: 2018-08-09T11:20:34.052305: step 7444, loss 0.512583.
Train: 2018-08-09T11:20:34.130412: step 7445, loss 0.493772.
Train: 2018-08-09T11:20:34.208522: step 7446, loss 0.615601.
Train: 2018-08-09T11:20:34.286625: step 7447, loss 0.545096.
Train: 2018-08-09T11:20:34.364732: step 7448, loss 0.544945.
Train: 2018-08-09T11:20:34.458455: step 7449, loss 0.578728.
Train: 2018-08-09T11:20:34.536535: step 7450, loss 0.646133.
Test: 2018-08-09T11:20:35.026199: step 7450, loss 0.548617.
Train: 2018-08-09T11:20:35.104308: step 7451, loss 0.512574.
Train: 2018-08-09T11:20:35.198032: step 7452, loss 0.629982.
Train: 2018-08-09T11:20:35.276134: step 7453, loss 0.495997.
Train: 2018-08-09T11:20:35.354249: step 7454, loss 0.544858.
Train: 2018-08-09T11:20:35.432347: step 7455, loss 0.562368.
Train: 2018-08-09T11:20:35.526081: step 7456, loss 0.693942.
Train: 2018-08-09T11:20:35.604191: step 7457, loss 0.595231.
Train: 2018-08-09T11:20:35.682294: step 7458, loss 0.447813.
Train: 2018-08-09T11:20:35.760421: step 7459, loss 0.594658.
Train: 2018-08-09T11:20:35.838478: step 7460, loss 0.57913.
Test: 2018-08-09T11:20:36.338359: step 7460, loss 0.547252.
Train: 2018-08-09T11:20:36.416467: step 7461, loss 0.54602.
Train: 2018-08-09T11:20:36.494597: step 7462, loss 0.612039.
Train: 2018-08-09T11:20:36.588300: step 7463, loss 0.676937.
Train: 2018-08-09T11:20:36.666407: step 7464, loss 0.51389.
Train: 2018-08-09T11:20:36.744538: step 7465, loss 0.48313.
Train: 2018-08-09T11:20:36.822654: step 7466, loss 0.481469.
Train: 2018-08-09T11:20:36.900757: step 7467, loss 0.530215.
Train: 2018-08-09T11:20:36.978867: step 7468, loss 0.498274.
Train: 2018-08-09T11:20:37.072594: step 7469, loss 0.643932.
Train: 2018-08-09T11:20:37.150698: step 7470, loss 0.547288.
Test: 2018-08-09T11:20:37.634961: step 7470, loss 0.550442.
Train: 2018-08-09T11:20:37.728655: step 7471, loss 0.596197.
Train: 2018-08-09T11:20:37.806793: step 7472, loss 0.513341.
Train: 2018-08-09T11:20:37.884896: step 7473, loss 0.628334.
Train: 2018-08-09T11:20:37.964380: step 7474, loss 0.64393.
Train: 2018-08-09T11:20:38.042455: step 7475, loss 0.531241.
Train: 2018-08-09T11:20:38.120562: step 7476, loss 0.594998.
Train: 2018-08-09T11:20:38.214321: step 7477, loss 0.627146.
Train: 2018-08-09T11:20:38.292425: step 7478, loss 0.578881.
Train: 2018-08-09T11:20:38.370531: step 7479, loss 0.59488.
Train: 2018-08-09T11:20:38.448638: step 7480, loss 0.530992.
Test: 2018-08-09T11:20:38.948491: step 7480, loss 0.54921.
Train: 2018-08-09T11:20:39.026627: step 7481, loss 0.594785.
Train: 2018-08-09T11:20:39.104734: step 7482, loss 0.65835.
Train: 2018-08-09T11:20:39.182841: step 7483, loss 0.499629.
Train: 2018-08-09T11:20:39.260947: step 7484, loss 0.563041.
Train: 2018-08-09T11:20:39.339056: step 7485, loss 0.531475.
Train: 2018-08-09T11:20:39.432781: step 7486, loss 0.578863.
Train: 2018-08-09T11:20:39.510888: step 7487, loss 0.531535.
Train: 2018-08-09T11:20:39.588997: step 7488, loss 0.657752.
Train: 2018-08-09T11:20:39.667073: step 7489, loss 0.484352.
Train: 2018-08-09T11:20:39.745208: step 7490, loss 0.578871.
Test: 2018-08-09T11:20:40.247155: step 7490, loss 0.54894.
Train: 2018-08-09T11:20:40.325267: step 7491, loss 0.515824.
Train: 2018-08-09T11:20:40.403375: step 7492, loss 0.578876.
Train: 2018-08-09T11:20:40.481480: step 7493, loss 0.547305.
Train: 2018-08-09T11:20:40.559586: step 7494, loss 0.452442.
Train: 2018-08-09T11:20:40.637693: step 7495, loss 0.515438.
Train: 2018-08-09T11:20:40.731422: step 7496, loss 0.562987.
Train: 2018-08-09T11:20:40.809498: step 7497, loss 0.483018.
Train: 2018-08-09T11:20:40.887634: step 7498, loss 0.611002.
Train: 2018-08-09T11:20:40.965712: step 7499, loss 0.594997.
Train: 2018-08-09T11:20:41.043816: step 7500, loss 0.498155.
Test: 2018-08-09T11:20:41.543700: step 7500, loss 0.550577.
Train: 2018-08-09T11:20:42.074868: step 7501, loss 0.611263.
Train: 2018-08-09T11:20:42.152963: step 7502, loss 0.692645.
Train: 2018-08-09T11:20:42.231067: step 7503, loss 0.595177.
Train: 2018-08-09T11:20:42.309144: step 7504, loss 0.61133.
Train: 2018-08-09T11:20:42.387283: step 7505, loss 0.562715.
Train: 2018-08-09T11:20:42.465388: step 7506, loss 0.530428.
Train: 2018-08-09T11:20:42.543494: step 7507, loss 0.627316.
Train: 2018-08-09T11:20:42.621601: step 7508, loss 0.562761.
Train: 2018-08-09T11:20:42.715299: step 7509, loss 0.627163.
Train: 2018-08-09T11:20:42.793404: step 7510, loss 0.627044.
Test: 2018-08-09T11:20:43.293288: step 7510, loss 0.549068.
Train: 2018-08-09T11:20:43.371393: step 7511, loss 0.578865.
Train: 2018-08-09T11:20:43.449530: step 7512, loss 0.57886.
Train: 2018-08-09T11:20:43.527607: step 7513, loss 0.451566.
Train: 2018-08-09T11:20:43.605743: step 7514, loss 0.547055.
Train: 2018-08-09T11:20:43.683851: step 7515, loss 0.547058.
Train: 2018-08-09T11:20:43.777576: step 7516, loss 0.610666.
Train: 2018-08-09T11:20:43.855685: step 7517, loss 0.499383.
Train: 2018-08-09T11:20:43.935137: step 7518, loss 0.674299.
Train: 2018-08-09T11:20:44.013244: step 7519, loss 0.658298.
Train: 2018-08-09T11:20:44.091355: step 7520, loss 0.594704.
Test: 2018-08-09T11:20:44.591210: step 7520, loss 0.549455.
Train: 2018-08-09T11:20:44.669313: step 7521, loss 0.610457.
Train: 2018-08-09T11:20:44.747421: step 7522, loss 0.50014.
Train: 2018-08-09T11:20:44.841179: step 7523, loss 0.594589.
Train: 2018-08-09T11:20:44.919285: step 7524, loss 0.531818.
Train: 2018-08-09T11:20:44.997392: step 7525, loss 0.47963.
Train: 2018-08-09T11:20:45.075496: step 7526, loss 0.547511.
Train: 2018-08-09T11:20:45.153575: step 7527, loss 0.594573.
Train: 2018-08-09T11:20:45.231713: step 7528, loss 0.531752.
Train: 2018-08-09T11:20:45.309820: step 7529, loss 0.578868.
Train: 2018-08-09T11:20:45.403516: step 7530, loss 0.547386.
Test: 2018-08-09T11:20:45.903399: step 7530, loss 0.551317.
Train: 2018-08-09T11:20:45.967335: step 7531, loss 0.563103.
Train: 2018-08-09T11:20:46.061093: step 7532, loss 0.547301.
Train: 2018-08-09T11:20:46.139202: step 7533, loss 0.642087.
Train: 2018-08-09T11:20:46.217306: step 7534, loss 0.578855.
Train: 2018-08-09T11:20:46.295383: step 7535, loss 0.642098.
Train: 2018-08-09T11:20:46.373491: step 7536, loss 0.389446.
Train: 2018-08-09T11:20:46.451626: step 7537, loss 0.515605.
Train: 2018-08-09T11:20:46.529733: step 7538, loss 0.5947.
Train: 2018-08-09T11:20:46.623461: step 7539, loss 0.610642.
Train: 2018-08-09T11:20:46.701568: step 7540, loss 0.594802.
Test: 2018-08-09T11:20:47.201420: step 7540, loss 0.55102.
Train: 2018-08-09T11:20:47.279557: step 7541, loss 0.578849.
Train: 2018-08-09T11:20:47.357664: step 7542, loss 0.658483.
Train: 2018-08-09T11:20:47.435740: step 7543, loss 0.594774.
Train: 2018-08-09T11:20:47.513876: step 7544, loss 0.578901.
Train: 2018-08-09T11:20:47.591952: step 7545, loss 0.547161.
Train: 2018-08-09T11:20:47.670089: step 7546, loss 0.594692.
Train: 2018-08-09T11:20:47.748167: step 7547, loss 0.64206.
Train: 2018-08-09T11:20:47.826303: step 7548, loss 0.578867.
Train: 2018-08-09T11:20:47.920784: step 7549, loss 0.500279.
Train: 2018-08-09T11:20:47.998921: step 7550, loss 0.657384.
Test: 2018-08-09T11:20:48.498775: step 7550, loss 0.550909.
Train: 2018-08-09T11:20:48.576905: step 7551, loss 0.516227.
Train: 2018-08-09T11:20:48.655017: step 7552, loss 0.516306.
Train: 2018-08-09T11:20:48.733124: step 7553, loss 0.469382.
Train: 2018-08-09T11:20:48.811230: step 7554, loss 0.547532.
Train: 2018-08-09T11:20:48.889341: step 7555, loss 0.531758.
Train: 2018-08-09T11:20:48.983065: step 7556, loss 0.563109.
Train: 2018-08-09T11:20:49.061171: step 7557, loss 0.61043.
Train: 2018-08-09T11:20:49.139282: step 7558, loss 0.563036.
Train: 2018-08-09T11:20:49.217386: step 7559, loss 0.547181.
Train: 2018-08-09T11:20:49.295461: step 7560, loss 0.56293.
Test: 2018-08-09T11:20:49.795369: step 7560, loss 0.548032.
Train: 2018-08-09T11:20:49.873480: step 7561, loss 0.578863.
Train: 2018-08-09T11:20:49.951586: step 7562, loss 0.57876.
Train: 2018-08-09T11:20:50.029695: step 7563, loss 0.578837.
Train: 2018-08-09T11:20:50.107799: step 7564, loss 0.546317.
Train: 2018-08-09T11:20:50.201527: step 7565, loss 0.547592.
Train: 2018-08-09T11:20:50.279606: step 7566, loss 0.594738.
Train: 2018-08-09T11:20:50.357741: step 7567, loss 0.62724.
Train: 2018-08-09T11:20:50.435841: step 7568, loss 0.482212.
Train: 2018-08-09T11:20:50.513925: step 7569, loss 0.497805.
Train: 2018-08-09T11:20:50.592060: step 7570, loss 0.530994.
Test: 2018-08-09T11:20:51.094487: step 7570, loss 0.547827.
Train: 2018-08-09T11:20:51.172592: step 7571, loss 0.645672.
Train: 2018-08-09T11:20:51.250699: step 7572, loss 0.448744.
Train: 2018-08-09T11:20:51.328777: step 7573, loss 0.51355.
Train: 2018-08-09T11:20:51.406882: step 7574, loss 0.496911.
Train: 2018-08-09T11:20:51.485013: step 7575, loss 0.508722.
Train: 2018-08-09T11:20:51.578716: step 7576, loss 0.563124.
Train: 2018-08-09T11:20:51.656853: step 7577, loss 0.524314.
Train: 2018-08-09T11:20:51.734960: step 7578, loss 0.59502.
Train: 2018-08-09T11:20:51.813072: step 7579, loss 0.599409.
Train: 2018-08-09T11:20:51.891173: step 7580, loss 0.556934.
Test: 2018-08-09T11:20:52.391025: step 7580, loss 0.544287.
Train: 2018-08-09T11:20:52.469164: step 7581, loss 0.503618.
Train: 2018-08-09T11:20:52.547269: step 7582, loss 0.470971.
Train: 2018-08-09T11:20:52.625376: step 7583, loss 0.416614.
Train: 2018-08-09T11:20:52.703481: step 7584, loss 0.56745.
Train: 2018-08-09T11:20:52.797209: step 7585, loss 0.534513.
Train: 2018-08-09T11:20:52.875315: step 7586, loss 0.606969.
Train: 2018-08-09T11:20:52.955799: step 7587, loss 0.645507.
Train: 2018-08-09T11:20:53.033937: step 7588, loss 0.499249.
Train: 2018-08-09T11:20:53.112044: step 7589, loss 0.705739.
Train: 2018-08-09T11:20:53.190119: step 7590, loss 0.484286.
Test: 2018-08-09T11:20:53.690002: step 7590, loss 0.547066.
Train: 2018-08-09T11:20:53.783731: step 7591, loss 0.560612.
Train: 2018-08-09T11:20:53.861836: step 7592, loss 0.544717.
Train: 2018-08-09T11:20:53.939943: step 7593, loss 0.415534.
Train: 2018-08-09T11:20:54.018083: step 7594, loss 0.524263.
Train: 2018-08-09T11:20:54.096157: step 7595, loss 0.616731.
Train: 2018-08-09T11:20:54.189916: step 7596, loss 0.614745.
Train: 2018-08-09T11:20:54.267990: step 7597, loss 0.618203.
Train: 2018-08-09T11:20:54.346127: step 7598, loss 0.592884.
Train: 2018-08-09T11:20:54.424235: step 7599, loss 0.629952.
Train: 2018-08-09T11:20:54.502340: step 7600, loss 0.664831.
Test: 2018-08-09T11:20:55.004512: step 7600, loss 0.549366.
Train: 2018-08-09T11:20:55.566910: step 7601, loss 0.562495.
Train: 2018-08-09T11:20:55.645018: step 7602, loss 0.514509.
Train: 2018-08-09T11:20:55.723095: step 7603, loss 0.563472.
Train: 2018-08-09T11:20:55.816852: step 7604, loss 0.546101.
Train: 2018-08-09T11:20:55.896257: step 7605, loss 0.548386.
Train: 2018-08-09T11:20:55.974362: step 7606, loss 0.562186.
Train: 2018-08-09T11:20:56.052471: step 7607, loss 0.500239.
Train: 2018-08-09T11:20:56.130569: step 7608, loss 0.546969.
Train: 2018-08-09T11:20:56.208683: step 7609, loss 0.57893.
Train: 2018-08-09T11:20:56.302403: step 7610, loss 0.594737.
Test: 2018-08-09T11:20:56.786640: step 7610, loss 0.548455.
Train: 2018-08-09T11:20:56.864776: step 7611, loss 0.531955.
Train: 2018-08-09T11:20:56.942854: step 7612, loss 0.438124.
Train: 2018-08-09T11:20:57.036582: step 7613, loss 0.579123.
Train: 2018-08-09T11:20:57.114717: step 7614, loss 0.515266.
Train: 2018-08-09T11:20:57.192825: step 7615, loss 0.563129.
Train: 2018-08-09T11:20:57.270928: step 7616, loss 0.547106.
Train: 2018-08-09T11:20:57.353750: step 7617, loss 0.595157.
Train: 2018-08-09T11:20:57.431887: step 7618, loss 0.578843.
Train: 2018-08-09T11:20:57.509966: step 7619, loss 0.418885.
Train: 2018-08-09T11:20:57.603698: step 7620, loss 0.578859.
Test: 2018-08-09T11:20:58.087978: step 7620, loss 0.548101.
Train: 2018-08-09T11:20:58.166058: step 7621, loss 0.545901.
Train: 2018-08-09T11:20:58.259817: step 7622, loss 0.611217.
Train: 2018-08-09T11:20:58.337924: step 7623, loss 0.610969.
Train: 2018-08-09T11:20:58.416031: step 7624, loss 0.531781.
Train: 2018-08-09T11:20:58.494137: step 7625, loss 0.631447.
Train: 2018-08-09T11:20:58.572214: step 7626, loss 0.611406.
Train: 2018-08-09T11:20:58.650351: step 7627, loss 0.61413.
Train: 2018-08-09T11:20:58.728457: step 7628, loss 0.72564.
Train: 2018-08-09T11:20:58.822186: step 7629, loss 0.578982.
Train: 2018-08-09T11:20:58.900262: step 7630, loss 0.626726.
Test: 2018-08-09T11:20:59.384522: step 7630, loss 0.54989.
Train: 2018-08-09T11:20:59.462659: step 7631, loss 0.563017.
Train: 2018-08-09T11:20:59.556387: step 7632, loss 0.594318.
Train: 2018-08-09T11:20:59.634495: step 7633, loss 0.516032.
Train: 2018-08-09T11:20:59.712570: step 7634, loss 0.563155.
Train: 2018-08-09T11:20:59.790707: step 7635, loss 0.516325.
Train: 2018-08-09T11:20:59.868784: step 7636, loss 0.610185.
Train: 2018-08-09T11:20:59.946920: step 7637, loss 0.547668.
Train: 2018-08-09T11:21:00.040648: step 7638, loss 0.625665.
Train: 2018-08-09T11:21:00.118725: step 7639, loss 0.578897.
Train: 2018-08-09T11:21:00.196861: step 7640, loss 0.563375.
Test: 2018-08-09T11:21:00.696713: step 7640, loss 0.550639.
Train: 2018-08-09T11:21:00.774851: step 7641, loss 0.62544.
Train: 2018-08-09T11:21:00.852956: step 7642, loss 0.547983.
Train: 2018-08-09T11:21:00.933433: step 7643, loss 0.517147.
Train: 2018-08-09T11:21:01.011569: step 7644, loss 0.609814.
Train: 2018-08-09T11:21:01.089674: step 7645, loss 0.501811.
Train: 2018-08-09T11:21:01.183373: step 7646, loss 0.51719.
Train: 2018-08-09T11:21:01.261503: step 7647, loss 0.578936.
Train: 2018-08-09T11:21:01.339586: step 7648, loss 0.578932.
Train: 2018-08-09T11:21:01.417724: step 7649, loss 0.516933.
Train: 2018-08-09T11:21:01.495827: step 7650, loss 0.656534.
Test: 2018-08-09T11:21:01.995714: step 7650, loss 0.551159.
Train: 2018-08-09T11:21:02.073819: step 7651, loss 0.547843.
Train: 2018-08-09T11:21:02.167515: step 7652, loss 0.547782.
Train: 2018-08-09T11:21:02.245655: step 7653, loss 0.516601.
Train: 2018-08-09T11:21:02.323754: step 7654, loss 0.563249.
Train: 2018-08-09T11:21:02.401836: step 7655, loss 0.53197.
Train: 2018-08-09T11:21:02.479976: step 7656, loss 0.594459.
Train: 2018-08-09T11:21:02.573669: step 7657, loss 0.516.
Train: 2018-08-09T11:21:02.651776: step 7658, loss 0.594835.
Train: 2018-08-09T11:21:02.734073: step 7659, loss 0.595018.
Train: 2018-08-09T11:21:02.807712: step 7660, loss 0.578528.
Test: 2018-08-09T11:21:03.294421: step 7660, loss 0.550519.
Train: 2018-08-09T11:21:03.388124: step 7661, loss 0.499405.
Train: 2018-08-09T11:21:03.466229: step 7662, loss 0.595016.
Train: 2018-08-09T11:21:03.544368: step 7663, loss 0.547514.
Train: 2018-08-09T11:21:03.622473: step 7664, loss 0.626529.
Train: 2018-08-09T11:21:03.700579: step 7665, loss 0.610648.
Train: 2018-08-09T11:21:03.794280: step 7666, loss 0.610852.
Train: 2018-08-09T11:21:03.877099: step 7667, loss 0.563145.
Train: 2018-08-09T11:21:03.944742: step 7668, loss 0.59486.
Train: 2018-08-09T11:21:04.038502: step 7669, loss 0.531302.
Train: 2018-08-09T11:21:04.116578: step 7670, loss 0.515447.
Test: 2018-08-09T11:21:04.616465: step 7670, loss 0.547541.
Train: 2018-08-09T11:21:04.694598: step 7671, loss 0.578958.
Train: 2018-08-09T11:21:04.772673: step 7672, loss 0.594707.
Train: 2018-08-09T11:21:04.850811: step 7673, loss 0.483669.
Train: 2018-08-09T11:21:04.936243: step 7674, loss 0.578911.
Train: 2018-08-09T11:21:05.014349: step 7675, loss 0.610778.
Train: 2018-08-09T11:21:05.105179: step 7676, loss 0.483537.
Train: 2018-08-09T11:21:05.183316: step 7677, loss 0.610723.
Train: 2018-08-09T11:21:05.261391: step 7678, loss 0.594687.
Train: 2018-08-09T11:21:05.339499: step 7679, loss 0.658796.
Train: 2018-08-09T11:21:05.417604: step 7680, loss 0.578882.
Test: 2018-08-09T11:21:05.917496: step 7680, loss 0.550495.
Train: 2018-08-09T11:21:05.995594: step 7681, loss 0.483542.
Train: 2018-08-09T11:21:06.073730: step 7682, loss 0.451783.
Train: 2018-08-09T11:21:06.167458: step 7683, loss 0.53108.
Train: 2018-08-09T11:21:06.245565: step 7684, loss 0.642872.
Train: 2018-08-09T11:21:06.323640: step 7685, loss 0.59483.
Train: 2018-08-09T11:21:06.401748: step 7686, loss 0.546786.
Train: 2018-08-09T11:21:06.479901: step 7687, loss 0.594894.
Train: 2018-08-09T11:21:06.557962: step 7688, loss 0.642848.
Train: 2018-08-09T11:21:06.651690: step 7689, loss 0.514917.
Train: 2018-08-09T11:21:06.729825: step 7690, loss 0.546953.
Test: 2018-08-09T11:21:07.229686: step 7690, loss 0.549709.
Train: 2018-08-09T11:21:07.307814: step 7691, loss 0.482964.
Train: 2018-08-09T11:21:07.385921: step 7692, loss 0.482642.
Train: 2018-08-09T11:21:07.463998: step 7693, loss 0.562916.
Train: 2018-08-09T11:21:07.557725: step 7694, loss 0.595077.
Train: 2018-08-09T11:21:07.635856: step 7695, loss 0.530564.
Train: 2018-08-09T11:21:07.713970: step 7696, loss 0.5951.
Train: 2018-08-09T11:21:07.792046: step 7697, loss 0.59536.
Train: 2018-08-09T11:21:07.870182: step 7698, loss 0.513927.
Train: 2018-08-09T11:21:07.949736: step 7699, loss 0.676448.
Train: 2018-08-09T11:21:08.027847: step 7700, loss 0.643992.
Test: 2018-08-09T11:21:08.527699: step 7700, loss 0.548724.
Train: 2018-08-09T11:21:09.074476: step 7701, loss 0.481548.
Train: 2018-08-09T11:21:09.152552: step 7702, loss 0.578915.
Train: 2018-08-09T11:21:09.230660: step 7703, loss 0.530353.
Train: 2018-08-09T11:21:09.324387: step 7704, loss 0.578754.
Train: 2018-08-09T11:21:09.402523: step 7705, loss 0.546376.
Train: 2018-08-09T11:21:09.480631: step 7706, loss 0.562966.
Train: 2018-08-09T11:21:09.558736: step 7707, loss 0.530209.
Train: 2018-08-09T11:21:09.636843: step 7708, loss 0.546025.
Train: 2018-08-09T11:21:09.714919: step 7709, loss 0.481167.
Train: 2018-08-09T11:21:09.793056: step 7710, loss 0.562249.
Test: 2018-08-09T11:21:10.294295: step 7710, loss 0.549451.
Train: 2018-08-09T11:21:10.372369: step 7711, loss 0.579366.
Train: 2018-08-09T11:21:10.450506: step 7712, loss 0.560619.
Train: 2018-08-09T11:21:10.544235: step 7713, loss 0.62919.
Train: 2018-08-09T11:21:10.622310: step 7714, loss 0.53031.
Train: 2018-08-09T11:21:10.700418: step 7715, loss 0.493905.
Train: 2018-08-09T11:21:10.778525: step 7716, loss 0.563116.
Train: 2018-08-09T11:21:10.856661: step 7717, loss 0.599512.
Train: 2018-08-09T11:21:10.950358: step 7718, loss 0.494476.
Train: 2018-08-09T11:21:11.028500: step 7719, loss 0.491488.
Train: 2018-08-09T11:21:11.106602: step 7720, loss 0.679744.
Test: 2018-08-09T11:21:11.593372: step 7720, loss 0.548911.
Train: 2018-08-09T11:21:11.671509: step 7721, loss 0.565509.
Train: 2018-08-09T11:21:11.749616: step 7722, loss 0.410432.
Train: 2018-08-09T11:21:11.843347: step 7723, loss 0.630897.
Train: 2018-08-09T11:21:11.920226: step 7724, loss 0.596946.
Train: 2018-08-09T11:21:11.998364: step 7725, loss 0.545167.
Train: 2018-08-09T11:21:12.076473: step 7726, loss 0.507516.
Train: 2018-08-09T11:21:12.154577: step 7727, loss 0.479272.
Train: 2018-08-09T11:21:12.248280: step 7728, loss 0.580138.
Train: 2018-08-09T11:21:12.326411: step 7729, loss 0.532635.
Train: 2018-08-09T11:21:12.404488: step 7730, loss 0.646362.
Test: 2018-08-09T11:21:12.904369: step 7730, loss 0.547005.
Train: 2018-08-09T11:21:12.982476: step 7731, loss 0.595836.
Train: 2018-08-09T11:21:13.060582: step 7732, loss 0.513296.
Train: 2018-08-09T11:21:13.138718: step 7733, loss 0.560978.
Train: 2018-08-09T11:21:13.216828: step 7734, loss 0.613148.
Train: 2018-08-09T11:21:13.294902: step 7735, loss 0.496172.
Train: 2018-08-09T11:21:13.388661: step 7736, loss 0.510522.
Train: 2018-08-09T11:21:13.466769: step 7737, loss 0.41015.
Train: 2018-08-09T11:21:13.544874: step 7738, loss 0.496012.
Train: 2018-08-09T11:21:13.622976: step 7739, loss 0.409574.
Train: 2018-08-09T11:21:13.701086: step 7740, loss 0.490764.
Test: 2018-08-09T11:21:14.200941: step 7740, loss 0.546621.
Train: 2018-08-09T11:21:14.279075: step 7741, loss 0.525334.
Train: 2018-08-09T11:21:14.357153: step 7742, loss 0.561271.
Train: 2018-08-09T11:21:14.435292: step 7743, loss 0.587361.
Train: 2018-08-09T11:21:14.528986: step 7744, loss 0.440161.
Train: 2018-08-09T11:21:14.607124: step 7745, loss 0.505582.
Train: 2018-08-09T11:21:14.685203: step 7746, loss 0.686155.
Train: 2018-08-09T11:21:14.763307: step 7747, loss 0.484996.
Train: 2018-08-09T11:21:14.841446: step 7748, loss 0.623066.
Train: 2018-08-09T11:21:14.919552: step 7749, loss 0.412715.
Train: 2018-08-09T11:21:15.013247: step 7750, loss 0.528866.
Test: 2018-08-09T11:21:15.497509: step 7750, loss 0.54716.
Train: 2018-08-09T11:21:15.575615: step 7751, loss 0.620156.
Train: 2018-08-09T11:21:15.653752: step 7752, loss 0.563889.
Train: 2018-08-09T11:21:15.747477: step 7753, loss 0.487843.
Train: 2018-08-09T11:21:15.825556: step 7754, loss 0.526351.
Train: 2018-08-09T11:21:15.903664: step 7755, loss 0.470794.
Train: 2018-08-09T11:21:15.981797: step 7756, loss 0.533876.
Train: 2018-08-09T11:21:16.059876: step 7757, loss 0.459959.
Train: 2018-08-09T11:21:16.153604: step 7758, loss 0.638735.
Train: 2018-08-09T11:21:16.231712: step 7759, loss 0.690194.
Train: 2018-08-09T11:21:16.309847: step 7760, loss 0.5117.
Test: 2018-08-09T11:21:16.809725: step 7760, loss 0.547821.
Train: 2018-08-09T11:21:16.887836: step 7761, loss 0.508419.
Train: 2018-08-09T11:21:16.968220: step 7762, loss 0.56171.
Train: 2018-08-09T11:21:17.046326: step 7763, loss 0.489901.
Train: 2018-08-09T11:21:17.124463: step 7764, loss 0.563812.
Train: 2018-08-09T11:21:17.202540: step 7765, loss 0.526021.
Train: 2018-08-09T11:21:17.280673: step 7766, loss 0.473777.
Train: 2018-08-09T11:21:17.374404: step 7767, loss 0.544483.
Train: 2018-08-09T11:21:17.452510: step 7768, loss 0.439247.
Train: 2018-08-09T11:21:17.530618: step 7769, loss 0.6507.
Train: 2018-08-09T11:21:17.608724: step 7770, loss 0.598123.
Test: 2018-08-09T11:21:18.108576: step 7770, loss 0.548474.
Train: 2018-08-09T11:21:18.186713: step 7771, loss 0.670691.
Train: 2018-08-09T11:21:18.264788: step 7772, loss 0.563427.
Train: 2018-08-09T11:21:18.342928: step 7773, loss 0.527048.
Train: 2018-08-09T11:21:18.436655: step 7774, loss 0.615526.
Train: 2018-08-09T11:21:18.514760: step 7775, loss 0.545187.
Train: 2018-08-09T11:21:18.592837: step 7776, loss 0.648696.
Train: 2018-08-09T11:21:18.670973: step 7777, loss 0.734135.
Train: 2018-08-09T11:21:18.749080: step 7778, loss 0.579365.
Train: 2018-08-09T11:21:18.827192: step 7779, loss 0.596253.
Train: 2018-08-09T11:21:18.920568: step 7780, loss 0.562376.
Test: 2018-08-09T11:21:19.405859: step 7780, loss 0.549896.
Train: 2018-08-09T11:21:19.483997: step 7781, loss 0.562302.
Train: 2018-08-09T11:21:19.562102: step 7782, loss 0.629046.
Train: 2018-08-09T11:21:19.655830: step 7783, loss 0.413797.
Train: 2018-08-09T11:21:19.733937: step 7784, loss 0.579078.
Train: 2018-08-09T11:21:19.812043: step 7785, loss 0.611931.
Train: 2018-08-09T11:21:19.890122: step 7786, loss 0.562593.
Train: 2018-08-09T11:21:19.983878: step 7787, loss 0.595332.
Train: 2018-08-09T11:21:20.061984: step 7788, loss 0.513696.
Train: 2018-08-09T11:21:20.140090: step 7789, loss 0.513809.
Train: 2018-08-09T11:21:20.218166: step 7790, loss 0.627685.
Test: 2018-08-09T11:21:20.718049: step 7790, loss 0.551159.
Train: 2018-08-09T11:21:20.796185: step 7791, loss 0.578907.
Train: 2018-08-09T11:21:20.874293: step 7792, loss 0.562705.
Train: 2018-08-09T11:21:20.952396: step 7793, loss 0.643517.
Train: 2018-08-09T11:21:21.046127: step 7794, loss 0.530556.
Train: 2018-08-09T11:21:21.124234: step 7795, loss 0.611006.
Train: 2018-08-09T11:21:21.202340: step 7796, loss 0.59489.
Train: 2018-08-09T11:21:21.280447: step 7797, loss 0.546902.
Train: 2018-08-09T11:21:21.358554: step 7798, loss 0.594819.
Train: 2018-08-09T11:21:21.436660: step 7799, loss 0.626524.
Train: 2018-08-09T11:21:21.514735: step 7800, loss 0.515556.
Test: 2018-08-09T11:21:22.016007: step 7800, loss 0.550623.
Train: 2018-08-09T11:21:22.594027: step 7801, loss 0.594665.
Train: 2018-08-09T11:21:22.672103: step 7802, loss 0.563084.
Train: 2018-08-09T11:21:22.765859: step 7803, loss 0.626116.
Train: 2018-08-09T11:21:22.843965: step 7804, loss 0.563181.
Train: 2018-08-09T11:21:22.922042: step 7805, loss 0.57887.
Train: 2018-08-09T11:21:23.000181: step 7806, loss 0.516359.
Train: 2018-08-09T11:21:23.078285: step 7807, loss 0.641361.
Train: 2018-08-09T11:21:23.156363: step 7808, loss 0.578886.
Train: 2018-08-09T11:21:23.250121: step 7809, loss 0.547805.
Train: 2018-08-09T11:21:23.328227: step 7810, loss 0.64103.
Test: 2018-08-09T11:21:23.828080: step 7810, loss 0.550671.
Train: 2018-08-09T11:21:23.907535: step 7811, loss 0.54794.
Train: 2018-08-09T11:21:23.985643: step 7812, loss 0.609864.
Train: 2018-08-09T11:21:24.063781: step 7813, loss 0.656164.
Train: 2018-08-09T11:21:24.141855: step 7814, loss 0.609698.
Train: 2018-08-09T11:21:24.219961: step 7815, loss 0.517697.
Train: 2018-08-09T11:21:24.298070: step 7816, loss 0.548415.
Train: 2018-08-09T11:21:24.391826: step 7817, loss 0.655294.
Train: 2018-08-09T11:21:24.469933: step 7818, loss 0.533378.
Train: 2018-08-09T11:21:24.548040: step 7819, loss 0.639742.
Train: 2018-08-09T11:21:24.626146: step 7820, loss 0.563934.
Test: 2018-08-09T11:21:25.125999: step 7820, loss 0.551491.
Train: 2018-08-09T11:21:25.204134: step 7821, loss 0.609471.
Train: 2018-08-09T11:21:25.282211: step 7822, loss 0.564139.
Train: 2018-08-09T11:21:25.360349: step 7823, loss 0.473727.
Train: 2018-08-09T11:21:25.454046: step 7824, loss 0.579019.
Train: 2018-08-09T11:21:25.532152: step 7825, loss 0.593927.
Train: 2018-08-09T11:21:25.610304: step 7826, loss 0.563809.
Train: 2018-08-09T11:21:25.688396: step 7827, loss 0.609399.
Train: 2018-08-09T11:21:25.766473: step 7828, loss 0.609393.
Train: 2018-08-09T11:21:25.844610: step 7829, loss 0.548533.
Train: 2018-08-09T11:21:25.924078: step 7830, loss 0.488347.
Test: 2018-08-09T11:21:26.439582: step 7830, loss 0.550057.
Train: 2018-08-09T11:21:26.517722: step 7831, loss 0.54852.
Train: 2018-08-09T11:21:26.595796: step 7832, loss 0.626315.
Train: 2018-08-09T11:21:26.673932: step 7833, loss 0.577285.
Train: 2018-08-09T11:21:26.752037: step 7834, loss 0.548282.
Train: 2018-08-09T11:21:26.830115: step 7835, loss 0.501645.
Train: 2018-08-09T11:21:26.923876: step 7836, loss 0.686857.
Train: 2018-08-09T11:21:27.001979: step 7837, loss 0.534419.
Train: 2018-08-09T11:21:27.080087: step 7838, loss 0.609919.
Train: 2018-08-09T11:21:27.158189: step 7839, loss 0.500771.
Train: 2018-08-09T11:21:27.236300: step 7840, loss 0.438013.
Test: 2018-08-09T11:21:27.736185: step 7840, loss 0.552372.
Train: 2018-08-09T11:21:27.814258: step 7841, loss 0.51739.
Train: 2018-08-09T11:21:27.909349: step 7842, loss 0.531716.
Train: 2018-08-09T11:21:27.987487: step 7843, loss 0.561511.
Train: 2018-08-09T11:21:28.065593: step 7844, loss 0.544901.
Train: 2018-08-09T11:21:28.143700: step 7845, loss 0.561461.
Train: 2018-08-09T11:21:28.221776: step 7846, loss 0.597442.
Train: 2018-08-09T11:21:28.299912: step 7847, loss 0.466541.
Train: 2018-08-09T11:21:28.393635: step 7848, loss 0.486833.
Train: 2018-08-09T11:21:28.471747: step 7849, loss 0.597371.
Train: 2018-08-09T11:21:28.549853: step 7850, loss 0.472802.
Test: 2018-08-09T11:21:29.043304: step 7850, loss 0.549174.
Train: 2018-08-09T11:21:29.121439: step 7851, loss 0.510925.
Train: 2018-08-09T11:21:29.199542: step 7852, loss 0.547413.
Train: 2018-08-09T11:21:29.277658: step 7853, loss 0.527151.
Train: 2018-08-09T11:21:29.371349: step 7854, loss 0.563471.
Train: 2018-08-09T11:21:29.449487: step 7855, loss 0.63691.
Train: 2018-08-09T11:21:29.527563: step 7856, loss 0.531529.
Train: 2018-08-09T11:21:29.605670: step 7857, loss 0.617639.
Train: 2018-08-09T11:21:29.683807: step 7858, loss 0.594265.
Train: 2018-08-09T11:21:29.761913: step 7859, loss 0.593997.
Train: 2018-08-09T11:21:29.855641: step 7860, loss 0.577386.
Test: 2018-08-09T11:21:30.339873: step 7860, loss 0.54882.
Train: 2018-08-09T11:21:30.418008: step 7861, loss 0.528904.
Train: 2018-08-09T11:21:30.511707: step 7862, loss 0.496016.
Train: 2018-08-09T11:21:30.605464: step 7863, loss 0.593539.
Train: 2018-08-09T11:21:30.683571: step 7864, loss 0.528881.
Train: 2018-08-09T11:21:30.761678: step 7865, loss 0.49476.
Train: 2018-08-09T11:21:30.839754: step 7866, loss 0.546145.
Train: 2018-08-09T11:21:30.917864: step 7867, loss 0.729929.
Train: 2018-08-09T11:21:31.011621: step 7868, loss 0.564948.
Train: 2018-08-09T11:21:31.089726: step 7869, loss 0.528615.
Train: 2018-08-09T11:21:31.167833: step 7870, loss 0.462991.
Test: 2018-08-09T11:21:31.667684: step 7870, loss 0.546572.
Train: 2018-08-09T11:21:31.745790: step 7871, loss 0.513486.
Train: 2018-08-09T11:21:31.823898: step 7872, loss 0.63086.
Train: 2018-08-09T11:21:31.902003: step 7873, loss 0.545992.
Train: 2018-08-09T11:21:31.981568: step 7874, loss 0.629167.
Train: 2018-08-09T11:21:32.059672: step 7875, loss 0.595074.
Train: 2018-08-09T11:21:32.137784: step 7876, loss 0.612048.
Train: 2018-08-09T11:21:32.231501: step 7877, loss 0.546465.
Train: 2018-08-09T11:21:32.309606: step 7878, loss 0.595677.
Train: 2018-08-09T11:21:32.387720: step 7879, loss 0.625182.
Train: 2018-08-09T11:21:32.465825: step 7880, loss 0.579083.
Test: 2018-08-09T11:21:32.965678: step 7880, loss 0.547392.
Train: 2018-08-09T11:21:33.043815: step 7881, loss 0.546491.
Train: 2018-08-09T11:21:33.121921: step 7882, loss 0.62605.
Train: 2018-08-09T11:21:33.199999: step 7883, loss 0.579479.
Train: 2018-08-09T11:21:33.293756: step 7884, loss 0.484325.
Train: 2018-08-09T11:21:33.371862: step 7885, loss 0.546996.
Train: 2018-08-09T11:21:33.449973: step 7886, loss 0.579199.
Train: 2018-08-09T11:21:33.528075: step 7887, loss 0.501575.
Train: 2018-08-09T11:21:33.606185: step 7888, loss 0.562671.
Train: 2018-08-09T11:21:33.684258: step 7889, loss 0.563029.
Train: 2018-08-09T11:21:33.762395: step 7890, loss 0.514257.
Test: 2018-08-09T11:21:34.266416: step 7890, loss 0.549482.
Train: 2018-08-09T11:21:34.344553: step 7891, loss 0.546548.
Train: 2018-08-09T11:21:34.422660: step 7892, loss 0.628521.
Train: 2018-08-09T11:21:34.500738: step 7893, loss 0.662391.
Train: 2018-08-09T11:21:34.578844: step 7894, loss 0.626731.
Train: 2018-08-09T11:21:34.672601: step 7895, loss 0.59457.
Train: 2018-08-09T11:21:34.750678: step 7896, loss 0.56301.
Train: 2018-08-09T11:21:34.828784: step 7897, loss 0.594459.
Train: 2018-08-09T11:21:34.906920: step 7898, loss 0.642093.
Train: 2018-08-09T11:21:34.985027: step 7899, loss 0.547842.
Train: 2018-08-09T11:21:35.063134: step 7900, loss 0.517373.
Test: 2018-08-09T11:21:35.563010: step 7900, loss 0.549427.
Train: 2018-08-09T11:21:36.170070: step 7901, loss 0.532155.
Train: 2018-08-09T11:21:36.248178: step 7902, loss 0.470827.
Train: 2018-08-09T11:21:36.329424: step 7903, loss 0.454747.
Train: 2018-08-09T11:21:36.407557: step 7904, loss 0.562734.
Train: 2018-08-09T11:21:36.485671: step 7905, loss 0.515536.
Train: 2018-08-09T11:21:36.579396: step 7906, loss 0.563685.
Train: 2018-08-09T11:21:36.657502: step 7907, loss 0.482601.
Train: 2018-08-09T11:21:36.735612: step 7908, loss 0.515149.
Train: 2018-08-09T11:21:36.813716: step 7909, loss 0.480489.
Train: 2018-08-09T11:21:36.891821: step 7910, loss 0.579608.
Test: 2018-08-09T11:21:37.391698: step 7910, loss 0.546411.
Train: 2018-08-09T11:21:37.501024: step 7911, loss 0.547536.
Train: 2018-08-09T11:21:37.579130: step 7912, loss 0.561099.
Train: 2018-08-09T11:21:37.657237: step 7913, loss 0.581328.
Train: 2018-08-09T11:21:37.750964: step 7914, loss 0.561432.
Train: 2018-08-09T11:21:37.829102: step 7915, loss 0.545992.
Train: 2018-08-09T11:21:37.907208: step 7916, loss 0.545068.
Train: 2018-08-09T11:21:37.985284: step 7917, loss 0.617229.
Train: 2018-08-09T11:21:38.063421: step 7918, loss 0.580838.
Train: 2018-08-09T11:21:38.141498: step 7919, loss 0.546098.
Train: 2018-08-09T11:21:38.219604: step 7920, loss 0.51383.
Test: 2018-08-09T11:21:38.719487: step 7920, loss 0.549011.
Train: 2018-08-09T11:21:38.797593: step 7921, loss 0.579005.
Train: 2018-08-09T11:21:38.875729: step 7922, loss 0.613562.
Train: 2018-08-09T11:21:38.970879: step 7923, loss 0.547309.
Train: 2018-08-09T11:21:39.048962: step 7924, loss 0.63108.
Train: 2018-08-09T11:21:39.127067: step 7925, loss 0.663743.
Train: 2018-08-09T11:21:39.205207: step 7926, loss 0.645473.
Train: 2018-08-09T11:21:39.283311: step 7927, loss 0.513969.
Train: 2018-08-09T11:21:39.361418: step 7928, loss 0.498345.
Train: 2018-08-09T11:21:39.455148: step 7929, loss 0.562526.
Train: 2018-08-09T11:21:39.533252: step 7930, loss 0.497518.
Test: 2018-08-09T11:21:40.033105: step 7930, loss 0.547462.
Train: 2018-08-09T11:21:40.111241: step 7931, loss 0.530146.
Train: 2018-08-09T11:21:40.189347: step 7932, loss 0.529806.
Train: 2018-08-09T11:21:40.267456: step 7933, loss 0.546514.
Train: 2018-08-09T11:21:40.345531: step 7934, loss 0.627821.
Train: 2018-08-09T11:21:40.423663: step 7935, loss 0.627814.
Train: 2018-08-09T11:21:40.517399: step 7936, loss 0.514073.
Train: 2018-08-09T11:21:40.595502: step 7937, loss 0.530253.
Train: 2018-08-09T11:21:40.673578: step 7938, loss 0.514099.
Train: 2018-08-09T11:21:40.751717: step 7939, loss 0.562434.
Train: 2018-08-09T11:21:40.829792: step 7940, loss 0.594763.
Test: 2018-08-09T11:21:41.331109: step 7940, loss 0.549119.
Train: 2018-08-09T11:21:41.409215: step 7941, loss 0.563572.
Train: 2018-08-09T11:21:41.487350: step 7942, loss 0.612034.
Train: 2018-08-09T11:21:41.581080: step 7943, loss 0.513623.
Train: 2018-08-09T11:21:41.659187: step 7944, loss 0.595498.
Train: 2018-08-09T11:21:41.737264: step 7945, loss 0.513744.
Train: 2018-08-09T11:21:41.815372: step 7946, loss 0.578798.
Train: 2018-08-09T11:21:41.893507: step 7947, loss 0.562594.
Train: 2018-08-09T11:21:41.971613: step 7948, loss 0.513799.
Train: 2018-08-09T11:21:42.065311: step 7949, loss 0.562749.
Train: 2018-08-09T11:21:42.143447: step 7950, loss 0.480726.
Test: 2018-08-09T11:21:42.643301: step 7950, loss 0.54842.
Train: 2018-08-09T11:21:42.721436: step 7951, loss 0.578759.
Train: 2018-08-09T11:21:42.799544: step 7952, loss 0.529097.
Train: 2018-08-09T11:21:42.877527: step 7953, loss 0.663102.
Train: 2018-08-09T11:21:42.957938: step 7954, loss 0.562885.
Train: 2018-08-09T11:21:43.036041: step 7955, loss 0.561793.
Train: 2018-08-09T11:21:43.129769: step 7956, loss 0.578968.
Train: 2018-08-09T11:21:43.207848: step 7957, loss 0.430151.
Train: 2018-08-09T11:21:43.285956: step 7958, loss 0.595288.
Train: 2018-08-09T11:21:43.364088: step 7959, loss 0.562211.
Train: 2018-08-09T11:21:43.457819: step 7960, loss 0.629342.
Test: 2018-08-09T11:21:43.942051: step 7960, loss 0.548581.
Train: 2018-08-09T11:21:44.020188: step 7961, loss 0.579778.
Train: 2018-08-09T11:21:44.113936: step 7962, loss 0.578943.
Train: 2018-08-09T11:21:44.192022: step 7963, loss 0.595241.
Train: 2018-08-09T11:21:44.270129: step 7964, loss 0.579132.
Train: 2018-08-09T11:21:44.348236: step 7965, loss 0.579258.
Train: 2018-08-09T11:21:44.426346: step 7966, loss 0.595841.
Train: 2018-08-09T11:21:44.504449: step 7967, loss 0.562534.
Train: 2018-08-09T11:21:44.598176: step 7968, loss 0.578984.
Train: 2018-08-09T11:21:44.676253: step 7969, loss 0.54625.
Train: 2018-08-09T11:21:44.754360: step 7970, loss 0.627448.
Test: 2018-08-09T11:21:45.256888: step 7970, loss 0.548221.
Train: 2018-08-09T11:21:45.334970: step 7971, loss 0.61129.
Train: 2018-08-09T11:21:45.413077: step 7972, loss 0.498494.
Train: 2018-08-09T11:21:45.491153: step 7973, loss 0.611013.
Train: 2018-08-09T11:21:45.569290: step 7974, loss 0.594813.
Train: 2018-08-09T11:21:45.647396: step 7975, loss 0.530938.
Train: 2018-08-09T11:21:45.741125: step 7976, loss 0.546956.
Train: 2018-08-09T11:21:45.819201: step 7977, loss 0.578789.
Train: 2018-08-09T11:21:45.897337: step 7978, loss 0.578722.
Train: 2018-08-09T11:21:45.975442: step 7979, loss 0.563042.
Train: 2018-08-09T11:21:46.053536: step 7980, loss 0.515234.
Test: 2018-08-09T11:21:46.553428: step 7980, loss 0.553444.
Train: 2018-08-09T11:21:46.631539: step 7981, loss 0.658486.
Train: 2018-08-09T11:21:46.709616: step 7982, loss 0.515094.
Train: 2018-08-09T11:21:46.803374: step 7983, loss 0.531383.
Train: 2018-08-09T11:21:46.881481: step 7984, loss 0.626366.
Train: 2018-08-09T11:21:46.961881: step 7985, loss 0.515469.
Train: 2018-08-09T11:21:47.039987: step 7986, loss 0.626777.
Train: 2018-08-09T11:21:47.118092: step 7987, loss 0.674005.
Train: 2018-08-09T11:21:47.196198: step 7988, loss 0.594457.
Train: 2018-08-09T11:21:47.274309: step 7989, loss 0.547452.
Train: 2018-08-09T11:21:47.368033: step 7990, loss 0.547086.
Test: 2018-08-09T11:21:47.867885: step 7990, loss 0.550195.
Train: 2018-08-09T11:21:47.946021: step 7991, loss 0.500373.
Train: 2018-08-09T11:21:48.024127: step 7992, loss 0.594428.
Train: 2018-08-09T11:21:48.102231: step 7993, loss 0.610005.
Train: 2018-08-09T11:21:48.180341: step 7994, loss 0.515933.
Train: 2018-08-09T11:21:48.274042: step 7995, loss 0.595293.
Train: 2018-08-09T11:21:48.352171: step 7996, loss 0.626529.
Train: 2018-08-09T11:21:48.430283: step 7997, loss 0.547511.
Train: 2018-08-09T11:21:48.508360: step 7998, loss 0.610389.
Train: 2018-08-09T11:21:48.586492: step 7999, loss 0.547903.
Train: 2018-08-09T11:21:48.664599: step 8000, loss 0.547777.
Test: 2018-08-09T11:21:49.166815: step 8000, loss 0.551038.
Train: 2018-08-09T11:21:49.791698: step 8001, loss 0.516564.
Train: 2018-08-09T11:21:49.869775: step 8002, loss 0.610172.
Train: 2018-08-09T11:21:49.947912: step 8003, loss 0.68807.
Train: 2018-08-09T11:21:50.026019: step 8004, loss 0.500879.
Train: 2018-08-09T11:21:50.104124: step 8005, loss 0.547813.
Train: 2018-08-09T11:21:50.182228: step 8006, loss 0.610013.
Train: 2018-08-09T11:21:50.275960: step 8007, loss 0.609517.
Train: 2018-08-09T11:21:50.354037: step 8008, loss 0.54815.
Train: 2018-08-09T11:21:50.432174: step 8009, loss 0.609752.
Train: 2018-08-09T11:21:50.510282: step 8010, loss 0.500116.
Test: 2018-08-09T11:21:51.011529: step 8010, loss 0.550955.
Train: 2018-08-09T11:21:51.089660: step 8011, loss 0.609364.
Train: 2018-08-09T11:21:51.167743: step 8012, loss 0.594237.
Train: 2018-08-09T11:21:51.257434: step 8013, loss 0.482296.
Train: 2018-08-09T11:21:51.335511: step 8014, loss 0.549041.
Train: 2018-08-09T11:21:51.413617: step 8015, loss 0.531107.
Train: 2018-08-09T11:21:51.491750: step 8016, loss 0.532985.
Train: 2018-08-09T11:21:51.569831: step 8017, loss 0.52871.
Train: 2018-08-09T11:21:51.663558: step 8018, loss 0.562362.
Train: 2018-08-09T11:21:51.741695: step 8019, loss 0.492685.
Train: 2018-08-09T11:21:51.819801: step 8020, loss 0.544481.
Test: 2018-08-09T11:21:52.319685: step 8020, loss 0.550594.
Train: 2018-08-09T11:21:52.397785: step 8021, loss 0.577453.
Train: 2018-08-09T11:21:52.475867: step 8022, loss 0.564234.
Train: 2018-08-09T11:21:52.553976: step 8023, loss 0.608307.
Train: 2018-08-09T11:21:52.647702: step 8024, loss 0.535926.
Train: 2018-08-09T11:21:52.725807: step 8025, loss 0.686217.
Train: 2018-08-09T11:21:52.803945: step 8026, loss 0.450187.
Train: 2018-08-09T11:21:52.882052: step 8027, loss 0.495399.
Train: 2018-08-09T11:21:52.965618: step 8028, loss 0.523964.
Train: 2018-08-09T11:21:53.043725: step 8029, loss 0.679862.
Train: 2018-08-09T11:21:53.121800: step 8030, loss 0.497066.
Test: 2018-08-09T11:21:53.621685: step 8030, loss 0.550307.
Train: 2018-08-09T11:21:53.699791: step 8031, loss 0.614197.
Train: 2018-08-09T11:21:53.777926: step 8032, loss 0.596739.
Train: 2018-08-09T11:21:53.856029: step 8033, loss 0.62603.
Train: 2018-08-09T11:21:53.949730: step 8034, loss 0.562387.
Train: 2018-08-09T11:21:54.027838: step 8035, loss 0.514909.
Train: 2018-08-09T11:21:54.105974: step 8036, loss 0.61116.
Train: 2018-08-09T11:21:54.184052: step 8037, loss 0.57885.
Train: 2018-08-09T11:21:54.262190: step 8038, loss 0.626137.
Train: 2018-08-09T11:21:54.355909: step 8039, loss 0.578893.
Train: 2018-08-09T11:21:54.433993: step 8040, loss 0.641524.
Test: 2018-08-09T11:21:54.933875: step 8040, loss 0.551622.
Train: 2018-08-09T11:21:55.012011: step 8041, loss 0.532262.
Train: 2018-08-09T11:21:55.090118: step 8042, loss 0.641114.
Train: 2018-08-09T11:21:55.168225: step 8043, loss 0.516954.
Train: 2018-08-09T11:21:55.246336: step 8044, loss 0.656227.
Train: 2018-08-09T11:21:55.324407: step 8045, loss 0.501937.
Train: 2018-08-09T11:21:55.418137: step 8046, loss 0.563581.
Train: 2018-08-09T11:21:55.496243: step 8047, loss 0.6097.
Train: 2018-08-09T11:21:55.574379: step 8048, loss 0.548275.
Train: 2018-08-09T11:21:55.652456: step 8049, loss 0.609625.
Train: 2018-08-09T11:21:55.730592: step 8050, loss 0.563672.
Test: 2018-08-09T11:21:56.232830: step 8050, loss 0.549933.
Train: 2018-08-09T11:21:56.310935: step 8051, loss 0.609664.
Train: 2018-08-09T11:21:56.389043: step 8052, loss 0.533195.
Train: 2018-08-09T11:21:56.482772: step 8053, loss 0.502744.
Train: 2018-08-09T11:21:56.560907: step 8054, loss 0.395691.
Train: 2018-08-09T11:21:56.639014: step 8055, loss 0.594252.
Train: 2018-08-09T11:21:56.717116: step 8056, loss 0.547996.
Train: 2018-08-09T11:21:56.795202: step 8057, loss 0.625316.
Train: 2018-08-09T11:21:56.888924: step 8058, loss 0.563147.
Train: 2018-08-09T11:21:56.967031: step 8059, loss 0.532105.
Train: 2018-08-09T11:21:57.045167: step 8060, loss 0.482992.
Test: 2018-08-09T11:21:57.529400: step 8060, loss 0.548306.
Train: 2018-08-09T11:21:57.607535: step 8061, loss 0.564134.
Train: 2018-08-09T11:21:57.701232: step 8062, loss 0.595816.
Train: 2018-08-09T11:21:57.779340: step 8063, loss 0.562587.
Train: 2018-08-09T11:21:57.857477: step 8064, loss 0.613985.
Train: 2018-08-09T11:21:57.936166: step 8065, loss 0.545579.
Train: 2018-08-09T11:21:58.014266: step 8066, loss 0.563804.
Train: 2018-08-09T11:21:58.107997: step 8067, loss 0.64424.
Train: 2018-08-09T11:21:58.186075: step 8068, loss 0.627089.
Train: 2018-08-09T11:21:58.264211: step 8069, loss 0.49791.
Train: 2018-08-09T11:21:58.342317: step 8070, loss 0.530398.
Test: 2018-08-09T11:21:58.842200: step 8070, loss 0.549511.
Train: 2018-08-09T11:21:58.920307: step 8071, loss 0.545881.
Train: 2018-08-09T11:21:58.998413: step 8072, loss 0.498043.
Train: 2018-08-09T11:21:59.076515: step 8073, loss 0.495336.
Train: 2018-08-09T11:21:59.154626: step 8074, loss 0.495345.
Train: 2018-08-09T11:21:59.232733: step 8075, loss 0.495782.
Train: 2018-08-09T11:21:59.326459: step 8076, loss 0.60829.
Train: 2018-08-09T11:21:59.404568: step 8077, loss 0.596527.
Train: 2018-08-09T11:21:59.482645: step 8078, loss 0.505745.
Train: 2018-08-09T11:21:59.560751: step 8079, loss 0.504053.
Train: 2018-08-09T11:21:59.638887: step 8080, loss 0.596092.
Test: 2018-08-09T11:22:00.151053: step 8080, loss 0.552443.
Train: 2018-08-09T11:22:00.230093: step 8081, loss 0.47619.
Train: 2018-08-09T11:22:00.308199: step 8082, loss 0.474899.
Train: 2018-08-09T11:22:00.386304: step 8083, loss 0.501612.
Train: 2018-08-09T11:22:00.464933: step 8084, loss 0.539343.
Train: 2018-08-09T11:22:00.558660: step 8085, loss 0.629157.
Train: 2018-08-09T11:22:00.636796: step 8086, loss 0.387804.
Train: 2018-08-09T11:22:00.714901: step 8087, loss 0.590691.
Train: 2018-08-09T11:22:00.808630: step 8088, loss 0.644602.
Train: 2018-08-09T11:22:00.886738: step 8089, loss 0.593892.
Train: 2018-08-09T11:22:00.964845: step 8090, loss 0.498039.
Test: 2018-08-09T11:22:01.464697: step 8090, loss 0.546444.
Train: 2018-08-09T11:22:01.542833: step 8091, loss 0.609809.
Train: 2018-08-09T11:22:01.620911: step 8092, loss 0.611643.
Train: 2018-08-09T11:22:01.699043: step 8093, loss 0.480803.
Train: 2018-08-09T11:22:01.792775: step 8094, loss 0.560614.
Train: 2018-08-09T11:22:01.870882: step 8095, loss 0.4497.
Train: 2018-08-09T11:22:01.948959: step 8096, loss 0.612274.
Train: 2018-08-09T11:22:02.027094: step 8097, loss 0.612916.
Train: 2018-08-09T11:22:02.105201: step 8098, loss 0.560751.
Train: 2018-08-09T11:22:02.183277: step 8099, loss 0.59537.
Train: 2018-08-09T11:22:02.261415: step 8100, loss 0.613366.
Test: 2018-08-09T11:22:02.761275: step 8100, loss 0.547149.
Train: 2018-08-09T11:22:03.325336: step 8101, loss 0.51318.
Train: 2018-08-09T11:22:03.403412: step 8102, loss 0.579447.
Train: 2018-08-09T11:22:03.481520: step 8103, loss 0.513079.
Train: 2018-08-09T11:22:03.559656: step 8104, loss 0.66353.
Train: 2018-08-09T11:22:03.637766: step 8105, loss 0.500258.
Train: 2018-08-09T11:22:03.731462: step 8106, loss 0.562335.
Train: 2018-08-09T11:22:03.809598: step 8107, loss 0.499144.
Train: 2018-08-09T11:22:03.887673: step 8108, loss 0.547541.
Train: 2018-08-09T11:22:03.965812: step 8109, loss 0.498318.
Train: 2018-08-09T11:22:04.043918: step 8110, loss 0.626778.
Test: 2018-08-09T11:22:04.543770: step 8110, loss 0.548709.
Train: 2018-08-09T11:22:04.621906: step 8111, loss 0.62635.
Train: 2018-08-09T11:22:04.699993: step 8112, loss 0.577758.
Train: 2018-08-09T11:22:04.793711: step 8113, loss 0.51409.
Train: 2018-08-09T11:22:04.871816: step 8114, loss 0.576775.
Train: 2018-08-09T11:22:04.951269: step 8115, loss 0.527751.
Train: 2018-08-09T11:22:05.029377: step 8116, loss 0.552871.
Train: 2018-08-09T11:22:05.107514: step 8117, loss 0.553239.
Train: 2018-08-09T11:22:05.185621: step 8118, loss 0.629341.
Train: 2018-08-09T11:22:05.270358: step 8119, loss 0.563868.
Train: 2018-08-09T11:22:05.348466: step 8120, loss 0.546471.
Test: 2018-08-09T11:22:05.848348: step 8120, loss 0.553098.
Train: 2018-08-09T11:22:05.942076: step 8121, loss 0.578841.
Train: 2018-08-09T11:22:06.020213: step 8122, loss 0.579374.
Train: 2018-08-09T11:22:06.098320: step 8123, loss 0.562623.
Train: 2018-08-09T11:22:06.176425: step 8124, loss 0.692912.
Train: 2018-08-09T11:22:06.254529: step 8125, loss 0.515121.
Train: 2018-08-09T11:22:06.348259: step 8126, loss 0.547069.
Train: 2018-08-09T11:22:06.426366: step 8127, loss 0.630121.
Train: 2018-08-09T11:22:06.504442: step 8128, loss 0.656947.
Train: 2018-08-09T11:22:06.582582: step 8129, loss 0.531834.
Train: 2018-08-09T11:22:06.660687: step 8130, loss 0.532571.
Test: 2018-08-09T11:22:07.154599: step 8130, loss 0.550963.
Train: 2018-08-09T11:22:07.232703: step 8131, loss 0.563303.
Train: 2018-08-09T11:22:07.310842: step 8132, loss 0.439071.
Train: 2018-08-09T11:22:07.404539: step 8133, loss 0.485261.
Train: 2018-08-09T11:22:07.482678: step 8134, loss 0.625625.
Train: 2018-08-09T11:22:07.567254: step 8135, loss 0.546791.
Train: 2018-08-09T11:22:07.645545: step 8136, loss 0.578524.
Train: 2018-08-09T11:22:07.723621: step 8137, loss 0.516046.
Train: 2018-08-09T11:22:07.801731: step 8138, loss 0.513053.
Train: 2018-08-09T11:22:07.879835: step 8139, loss 0.595369.
Train: 2018-08-09T11:22:07.957942: step 8140, loss 0.513359.
Test: 2018-08-09T11:22:08.457824: step 8140, loss 0.546481.
Train: 2018-08-09T11:22:08.535960: step 8141, loss 0.614419.
Train: 2018-08-09T11:22:08.614068: step 8142, loss 0.462779.
Train: 2018-08-09T11:22:08.692174: step 8143, loss 0.645616.
Train: 2018-08-09T11:22:08.785871: step 8144, loss 0.612657.
Train: 2018-08-09T11:22:08.864011: step 8145, loss 0.595067.
Train: 2018-08-09T11:22:08.942085: step 8146, loss 0.582008.
Train: 2018-08-09T11:22:09.020190: step 8147, loss 0.709535.
Train: 2018-08-09T11:22:09.098297: step 8148, loss 0.497669.
Train: 2018-08-09T11:22:09.192057: step 8149, loss 0.530467.
Train: 2018-08-09T11:22:09.270163: step 8150, loss 0.644223.
Test: 2018-08-09T11:22:09.770022: step 8150, loss 0.549888.
Train: 2018-08-09T11:22:09.848151: step 8151, loss 0.658397.
Train: 2018-08-09T11:22:09.928604: step 8152, loss 0.626324.
Train: 2018-08-09T11:22:10.006740: step 8153, loss 0.547713.
Train: 2018-08-09T11:22:10.084817: step 8154, loss 0.485973.
Train: 2018-08-09T11:22:10.162954: step 8155, loss 0.486165.
Train: 2018-08-09T11:22:10.241059: step 8156, loss 0.594375.
Train: 2018-08-09T11:22:10.334789: step 8157, loss 0.455203.
Train: 2018-08-09T11:22:10.412895: step 8158, loss 0.547878.
Train: 2018-08-09T11:22:10.491002: step 8159, loss 0.594582.
Train: 2018-08-09T11:22:10.569080: step 8160, loss 0.516144.
Test: 2018-08-09T11:22:11.068961: step 8160, loss 0.549655.
Train: 2018-08-09T11:22:11.147101: step 8161, loss 0.484842.
Train: 2018-08-09T11:22:11.225204: step 8162, loss 0.564449.
Train: 2018-08-09T11:22:11.318933: step 8163, loss 0.563133.
Train: 2018-08-09T11:22:11.397042: step 8164, loss 0.579062.
Train: 2018-08-09T11:22:11.475115: step 8165, loss 0.578854.
Train: 2018-08-09T11:22:11.553224: step 8166, loss 0.54724.
Train: 2018-08-09T11:22:11.631358: step 8167, loss 0.627355.
Train: 2018-08-09T11:22:11.709465: step 8168, loss 0.562734.
Train: 2018-08-09T11:22:11.787567: step 8169, loss 0.642714.
Train: 2018-08-09T11:22:11.881271: step 8170, loss 0.626924.
Test: 2018-08-09T11:22:12.366860: step 8170, loss 0.547978.
Train: 2018-08-09T11:22:12.444993: step 8171, loss 0.626791.
Train: 2018-08-09T11:22:12.538725: step 8172, loss 0.51537.
Train: 2018-08-09T11:22:12.616802: step 8173, loss 0.610621.
Train: 2018-08-09T11:22:12.694905: step 8174, loss 0.484029.
Train: 2018-08-09T11:22:12.773043: step 8175, loss 0.515625.
Train: 2018-08-09T11:22:12.851154: step 8176, loss 0.673808.
Train: 2018-08-09T11:22:12.929257: step 8177, loss 0.578872.
Train: 2018-08-09T11:22:13.007333: step 8178, loss 0.610468.
Train: 2018-08-09T11:22:13.101091: step 8179, loss 0.610364.
Train: 2018-08-09T11:22:13.179168: step 8180, loss 0.626018.
Test: 2018-08-09T11:22:13.679080: step 8180, loss 0.549133.
Train: 2018-08-09T11:22:13.757189: step 8181, loss 0.531881.
Train: 2018-08-09T11:22:13.835262: step 8182, loss 0.485086.
Train: 2018-08-09T11:22:13.915795: step 8183, loss 0.610152.
Train: 2018-08-09T11:22:13.993932: step 8184, loss 0.641382.
Train: 2018-08-09T11:22:14.072009: step 8185, loss 0.469812.
Train: 2018-08-09T11:22:14.150116: step 8186, loss 0.563316.
Train: 2018-08-09T11:22:14.236805: step 8187, loss 0.563308.
Train: 2018-08-09T11:22:14.314911: step 8188, loss 0.563304.
Train: 2018-08-09T11:22:14.393019: step 8189, loss 0.516494.
Train: 2018-08-09T11:22:14.471125: step 8190, loss 0.469525.
Test: 2018-08-09T11:22:14.971007: step 8190, loss 0.55148.
Train: 2018-08-09T11:22:15.049143: step 8191, loss 0.641575.
Train: 2018-08-09T11:22:15.127247: step 8192, loss 0.547466.
Train: 2018-08-09T11:22:15.205358: step 8193, loss 0.578876.
Train: 2018-08-09T11:22:15.299054: step 8194, loss 0.578859.
Train: 2018-08-09T11:22:15.377191: step 8195, loss 0.563072.
Train: 2018-08-09T11:22:15.455299: step 8196, loss 0.515667.
Train: 2018-08-09T11:22:15.533404: step 8197, loss 0.515422.
Train: 2018-08-09T11:22:15.611511: step 8198, loss 0.578851.
Train: 2018-08-09T11:22:15.689620: step 8199, loss 0.49907.
Train: 2018-08-09T11:22:15.783345: step 8200, loss 0.450404.
Test: 2018-08-09T11:22:16.275947: step 8200, loss 0.549909.
Train: 2018-08-09T11:22:16.900777: step 8201, loss 0.6434.
Train: 2018-08-09T11:22:16.978913: step 8202, loss 0.579139.
Train: 2018-08-09T11:22:17.057017: step 8203, loss 0.447007.
Train: 2018-08-09T11:22:17.135125: step 8204, loss 0.530211.
Train: 2018-08-09T11:22:17.213232: step 8205, loss 0.614877.
Train: 2018-08-09T11:22:17.306930: step 8206, loss 0.530643.
Train: 2018-08-09T11:22:17.385066: step 8207, loss 0.511391.
Train: 2018-08-09T11:22:17.463172: step 8208, loss 0.684135.
Train: 2018-08-09T11:22:17.541275: step 8209, loss 0.613705.
Train: 2018-08-09T11:22:17.619387: step 8210, loss 0.612951.
Test: 2018-08-09T11:22:18.119238: step 8210, loss 0.551245.
Train: 2018-08-09T11:22:18.197377: step 8211, loss 0.62951.
Train: 2018-08-09T11:22:18.275481: step 8212, loss 0.562718.
Train: 2018-08-09T11:22:18.369180: step 8213, loss 0.562542.
Train: 2018-08-09T11:22:18.447318: step 8214, loss 0.578924.
Train: 2018-08-09T11:22:18.525422: step 8215, loss 0.481242.
Train: 2018-08-09T11:22:18.603531: step 8216, loss 0.448748.
Train: 2018-08-09T11:22:18.681638: step 8217, loss 0.562627.
Train: 2018-08-09T11:22:18.759712: step 8218, loss 0.480859.
Train: 2018-08-09T11:22:18.853442: step 8219, loss 0.595327.
Train: 2018-08-09T11:22:18.918355: step 8220, loss 0.512976.
Test: 2018-08-09T11:22:19.418263: step 8220, loss 0.548837.
Train: 2018-08-09T11:22:19.496344: step 8221, loss 0.528713.
Train: 2018-08-09T11:22:19.590107: step 8222, loss 0.563381.
Train: 2018-08-09T11:22:19.668210: step 8223, loss 0.560899.
Train: 2018-08-09T11:22:19.746316: step 8224, loss 0.579052.
Train: 2018-08-09T11:22:19.824426: step 8225, loss 0.631046.
Train: 2018-08-09T11:22:19.902530: step 8226, loss 0.614769.
Train: 2018-08-09T11:22:19.980635: step 8227, loss 0.61346.
Train: 2018-08-09T11:22:20.074333: step 8228, loss 0.529198.
Train: 2018-08-09T11:22:20.152470: step 8229, loss 0.545913.
Train: 2018-08-09T11:22:20.230574: step 8230, loss 0.646103.
Test: 2018-08-09T11:22:20.730429: step 8230, loss 0.549566.
Train: 2018-08-09T11:22:20.808566: step 8231, loss 0.562614.
Train: 2018-08-09T11:22:20.902294: step 8232, loss 0.578969.
Train: 2018-08-09T11:22:20.982677: step 8233, loss 0.529874.
Train: 2018-08-09T11:22:21.060774: step 8234, loss 0.57899.
Train: 2018-08-09T11:22:21.138879: step 8235, loss 0.54625.
Train: 2018-08-09T11:22:21.217017: step 8236, loss 0.497399.
Train: 2018-08-09T11:22:21.295128: step 8237, loss 0.530006.
Train: 2018-08-09T11:22:21.373232: step 8238, loss 0.595293.
Train: 2018-08-09T11:22:21.451307: step 8239, loss 0.578955.
Train: 2018-08-09T11:22:21.545060: step 8240, loss 0.529871.
Test: 2018-08-09T11:22:22.029295: step 8240, loss 0.547244.
Train: 2018-08-09T11:22:22.123023: step 8241, loss 0.496918.
Train: 2018-08-09T11:22:22.201160: step 8242, loss 0.578421.
Train: 2018-08-09T11:22:22.279266: step 8243, loss 0.529276.
Train: 2018-08-09T11:22:22.357374: step 8244, loss 0.476929.
Train: 2018-08-09T11:22:22.435450: step 8245, loss 0.577241.
Train: 2018-08-09T11:22:22.513559: step 8246, loss 0.573009.
Train: 2018-08-09T11:22:22.607283: step 8247, loss 0.588007.
Train: 2018-08-09T11:22:22.685390: step 8248, loss 0.580282.
Train: 2018-08-09T11:22:22.763530: step 8249, loss 0.451563.
Train: 2018-08-09T11:22:22.841603: step 8250, loss 0.54149.
Test: 2018-08-09T11:22:23.342818: step 8250, loss 0.546407.
Train: 2018-08-09T11:22:23.420953: step 8251, loss 0.631714.
Train: 2018-08-09T11:22:23.499060: step 8252, loss 0.616411.
Train: 2018-08-09T11:22:23.577171: step 8253, loss 0.637531.
Train: 2018-08-09T11:22:23.655273: step 8254, loss 0.582818.
Train: 2018-08-09T11:22:23.748972: step 8255, loss 0.667718.
Train: 2018-08-09T11:22:23.827111: step 8256, loss 0.556955.
Train: 2018-08-09T11:22:23.905208: step 8257, loss 0.542051.
Train: 2018-08-09T11:22:23.983292: step 8258, loss 0.509129.
Train: 2018-08-09T11:22:24.061425: step 8259, loss 0.557593.
Train: 2018-08-09T11:22:24.139504: step 8260, loss 0.644031.
Test: 2018-08-09T11:22:24.639386: step 8260, loss 0.548571.
Train: 2018-08-09T11:22:24.717518: step 8261, loss 0.54286.
Train: 2018-08-09T11:22:24.811250: step 8262, loss 0.543299.
Train: 2018-08-09T11:22:24.889359: step 8263, loss 0.495378.
Train: 2018-08-09T11:22:24.968121: step 8264, loss 0.516046.
Train: 2018-08-09T11:22:25.046226: step 8265, loss 0.542911.
Train: 2018-08-09T11:22:25.124337: step 8266, loss 0.560703.
Train: 2018-08-09T11:22:25.218062: step 8267, loss 0.670353.
Train: 2018-08-09T11:22:25.296137: step 8268, loss 0.56534.
Train: 2018-08-09T11:22:25.374244: step 8269, loss 0.494745.
Train: 2018-08-09T11:22:25.452382: step 8270, loss 0.726306.
Test: 2018-08-09T11:22:25.952234: step 8270, loss 0.549194.
Train: 2018-08-09T11:22:26.030339: step 8271, loss 0.657912.
Train: 2018-08-09T11:22:26.108476: step 8272, loss 0.548622.
Train: 2018-08-09T11:22:26.186553: step 8273, loss 0.579776.
Train: 2018-08-09T11:22:26.280313: step 8274, loss 0.500964.
Train: 2018-08-09T11:22:26.358388: step 8275, loss 0.625917.
Train: 2018-08-09T11:22:26.436503: step 8276, loss 0.640395.
Train: 2018-08-09T11:22:26.530224: step 8277, loss 0.578415.
Train: 2018-08-09T11:22:26.608361: step 8278, loss 0.503313.
Train: 2018-08-09T11:22:26.686466: step 8279, loss 0.579065.
Train: 2018-08-09T11:22:26.764543: step 8280, loss 0.518618.
Test: 2018-08-09T11:22:27.264426: step 8280, loss 0.551449.
Train: 2018-08-09T11:22:27.342561: step 8281, loss 0.593841.
Train: 2018-08-09T11:22:27.420663: step 8282, loss 0.578986.
Train: 2018-08-09T11:22:27.514395: step 8283, loss 0.655403.
Train: 2018-08-09T11:22:27.592503: step 8284, loss 0.533577.
Train: 2018-08-09T11:22:27.670578: step 8285, loss 0.564094.
Train: 2018-08-09T11:22:27.748717: step 8286, loss 0.548879.
Train: 2018-08-09T11:22:27.826822: step 8287, loss 0.563561.
Train: 2018-08-09T11:22:27.906281: step 8288, loss 0.655878.
Train: 2018-08-09T11:22:28.000036: step 8289, loss 0.548538.
Train: 2018-08-09T11:22:28.078146: step 8290, loss 0.532803.
Test: 2018-08-09T11:22:28.562409: step 8290, loss 0.549949.
Train: 2018-08-09T11:22:28.640508: step 8291, loss 0.670908.
Train: 2018-08-09T11:22:28.734242: step 8292, loss 0.518124.
Train: 2018-08-09T11:22:28.812344: step 8293, loss 0.563219.
Train: 2018-08-09T11:22:28.890455: step 8294, loss 0.579016.
Train: 2018-08-09T11:22:28.968556: step 8295, loss 0.548916.
Train: 2018-08-09T11:22:29.046665: step 8296, loss 0.452946.
Train: 2018-08-09T11:22:29.140365: step 8297, loss 0.597044.
Train: 2018-08-09T11:22:29.218503: step 8298, loss 0.517331.
Train: 2018-08-09T11:22:29.296610: step 8299, loss 0.51256.
Train: 2018-08-09T11:22:29.374685: step 8300, loss 0.576985.
Test: 2018-08-09T11:22:29.874568: step 8300, loss 0.548011.
Train: 2018-08-09T11:22:30.422811: step 8301, loss 0.488476.
Train: 2018-08-09T11:22:30.500917: step 8302, loss 0.512747.
Train: 2018-08-09T11:22:30.579024: step 8303, loss 0.650096.
Train: 2018-08-09T11:22:30.657102: step 8304, loss 0.476738.
Train: 2018-08-09T11:22:30.750828: step 8305, loss 0.52827.
Train: 2018-08-09T11:22:30.828965: step 8306, loss 0.414569.
Train: 2018-08-09T11:22:30.907042: step 8307, loss 0.623545.
Train: 2018-08-09T11:22:30.985174: step 8308, loss 0.549109.
Train: 2018-08-09T11:22:31.063285: step 8309, loss 0.529715.
Train: 2018-08-09T11:22:31.141362: step 8310, loss 0.590882.
Test: 2018-08-09T11:22:31.641268: step 8310, loss 0.547755.
Train: 2018-08-09T11:22:31.719351: step 8311, loss 0.540615.
Train: 2018-08-09T11:22:31.797458: step 8312, loss 0.618935.
Train: 2018-08-09T11:22:31.891217: step 8313, loss 0.561467.
Train: 2018-08-09T11:22:31.971675: step 8314, loss 0.656446.
Train: 2018-08-09T11:22:32.056479: step 8315, loss 0.498337.
Train: 2018-08-09T11:22:32.134583: step 8316, loss 0.545819.
Train: 2018-08-09T11:22:32.212688: step 8317, loss 0.64073.
Train: 2018-08-09T11:22:32.290789: step 8318, loss 0.594901.
Train: 2018-08-09T11:22:32.384493: step 8319, loss 0.530315.
Train: 2018-08-09T11:22:32.462598: step 8320, loss 0.51842.
Test: 2018-08-09T11:22:32.962512: step 8320, loss 0.551307.
Train: 2018-08-09T11:22:33.040588: step 8321, loss 0.564919.
Train: 2018-08-09T11:22:33.118725: step 8322, loss 0.436119.
Train: 2018-08-09T11:22:33.196801: step 8323, loss 0.434611.
Train: 2018-08-09T11:22:33.274937: step 8324, loss 0.589549.
Train: 2018-08-09T11:22:33.353044: step 8325, loss 0.462308.
Train: 2018-08-09T11:22:33.431151: step 8326, loss 0.532625.
Train: 2018-08-09T11:22:33.509257: step 8327, loss 0.519378.
Train: 2018-08-09T11:22:33.602956: step 8328, loss 0.622408.
Train: 2018-08-09T11:22:33.681091: step 8329, loss 0.570151.
Train: 2018-08-09T11:22:33.759169: step 8330, loss 0.546225.
Test: 2018-08-09T11:22:34.259051: step 8330, loss 0.549325.
Train: 2018-08-09T11:22:34.337182: step 8331, loss 0.522481.
Train: 2018-08-09T11:22:34.415264: step 8332, loss 0.632002.
Train: 2018-08-09T11:22:34.493396: step 8333, loss 0.509046.
Train: 2018-08-09T11:22:34.587125: step 8334, loss 0.557178.
Train: 2018-08-09T11:22:34.665235: step 8335, loss 0.579128.
Train: 2018-08-09T11:22:34.743342: step 8336, loss 0.544437.
Train: 2018-08-09T11:22:34.821418: step 8337, loss 0.477615.
Train: 2018-08-09T11:22:34.899531: step 8338, loss 0.615975.
Train: 2018-08-09T11:22:34.977662: step 8339, loss 0.600788.
Train: 2018-08-09T11:22:35.071390: step 8340, loss 0.556124.
Test: 2018-08-09T11:22:35.571241: step 8340, loss 0.550809.
Train: 2018-08-09T11:22:35.649378: step 8341, loss 0.545728.
Train: 2018-08-09T11:22:35.727479: step 8342, loss 0.5302.
Train: 2018-08-09T11:22:35.805561: step 8343, loss 0.548315.
Train: 2018-08-09T11:22:35.883668: step 8344, loss 0.519506.
Train: 2018-08-09T11:22:35.964107: step 8345, loss 0.596738.
Train: 2018-08-09T11:22:36.042245: step 8346, loss 0.527259.
Train: 2018-08-09T11:22:36.135971: step 8347, loss 0.529571.
Train: 2018-08-09T11:22:36.214078: step 8348, loss 0.56281.
Train: 2018-08-09T11:22:36.292185: step 8349, loss 0.516083.
Train: 2018-08-09T11:22:36.370291: step 8350, loss 0.582616.
Test: 2018-08-09T11:22:36.867110: step 8350, loss 0.547531.
Train: 2018-08-09T11:22:36.945245: step 8351, loss 0.613828.
Train: 2018-08-09T11:22:37.023351: step 8352, loss 0.55992.
Train: 2018-08-09T11:22:37.101430: step 8353, loss 0.52888.
Train: 2018-08-09T11:22:37.195155: step 8354, loss 0.528134.
Train: 2018-08-09T11:22:37.273269: step 8355, loss 0.595632.
Train: 2018-08-09T11:22:37.351401: step 8356, loss 0.460385.
Train: 2018-08-09T11:22:37.429500: step 8357, loss 0.492161.
Train: 2018-08-09T11:22:37.523229: step 8358, loss 0.560298.
Train: 2018-08-09T11:22:37.601340: step 8359, loss 0.574408.
Train: 2018-08-09T11:22:37.679447: step 8360, loss 0.54184.
Test: 2018-08-09T11:22:38.183563: step 8360, loss 0.547251.
Train: 2018-08-09T11:22:38.261670: step 8361, loss 0.48565.
Train: 2018-08-09T11:22:38.339806: step 8362, loss 0.573185.
Train: 2018-08-09T11:22:38.417885: step 8363, loss 0.581392.
Train: 2018-08-09T11:22:38.496020: step 8364, loss 0.668736.
Train: 2018-08-09T11:22:38.589743: step 8365, loss 0.577782.
Train: 2018-08-09T11:22:38.667824: step 8366, loss 0.579058.
Train: 2018-08-09T11:22:38.745931: step 8367, loss 0.65864.
Train: 2018-08-09T11:22:38.824069: step 8368, loss 0.613749.
Train: 2018-08-09T11:22:38.898133: step 8369, loss 0.531185.
Train: 2018-08-09T11:22:38.976211: step 8370, loss 0.594996.
Test: 2018-08-09T11:22:39.476118: step 8370, loss 0.550536.
Train: 2018-08-09T11:22:39.569850: step 8371, loss 0.594144.
Train: 2018-08-09T11:22:39.647956: step 8372, loss 0.57875.
Train: 2018-08-09T11:22:39.726063: step 8373, loss 0.532138.
Train: 2018-08-09T11:22:39.804140: step 8374, loss 0.515856.
Train: 2018-08-09T11:22:39.882277: step 8375, loss 0.561043.
Train: 2018-08-09T11:22:39.960386: step 8376, loss 0.532425.
Train: 2018-08-09T11:22:40.038490: step 8377, loss 0.687114.
Train: 2018-08-09T11:22:40.132218: step 8378, loss 0.50075.
Train: 2018-08-09T11:22:40.210328: step 8379, loss 0.531991.
Train: 2018-08-09T11:22:40.288400: step 8380, loss 0.530598.
Test: 2018-08-09T11:22:40.788284: step 8380, loss 0.55018.
Train: 2018-08-09T11:22:40.866419: step 8381, loss 0.640379.
Train: 2018-08-09T11:22:40.946882: step 8382, loss 0.548628.
Train: 2018-08-09T11:22:41.024989: step 8383, loss 0.594738.
Train: 2018-08-09T11:22:41.103127: step 8384, loss 0.549491.
Train: 2018-08-09T11:22:41.181233: step 8385, loss 0.51695.
Train: 2018-08-09T11:22:41.259310: step 8386, loss 0.564364.
Train: 2018-08-09T11:22:41.353067: step 8387, loss 0.595403.
Train: 2018-08-09T11:22:41.431175: step 8388, loss 0.547918.
Train: 2018-08-09T11:22:41.509283: step 8389, loss 0.501513.
Train: 2018-08-09T11:22:41.587388: step 8390, loss 0.626347.
Test: 2018-08-09T11:22:42.087241: step 8390, loss 0.551951.
Train: 2018-08-09T11:22:42.165376: step 8391, loss 0.484905.
Train: 2018-08-09T11:22:42.243454: step 8392, loss 0.484309.
Train: 2018-08-09T11:22:42.321594: step 8393, loss 0.593825.
Train: 2018-08-09T11:22:42.415316: step 8394, loss 0.628445.
Train: 2018-08-09T11:22:42.493423: step 8395, loss 0.482345.
Train: 2018-08-09T11:22:42.571531: step 8396, loss 0.642592.
Train: 2018-08-09T11:22:42.649636: step 8397, loss 0.660973.
Train: 2018-08-09T11:22:42.727744: step 8398, loss 0.499587.
Train: 2018-08-09T11:22:42.821473: step 8399, loss 0.642694.
Train: 2018-08-09T11:22:42.899578: step 8400, loss 0.562503.
Test: 2018-08-09T11:22:43.400741: step 8400, loss 0.547365.
Train: 2018-08-09T11:22:43.978757: step 8401, loss 0.580276.
Train: 2018-08-09T11:22:44.056863: step 8402, loss 0.483424.
Train: 2018-08-09T11:22:44.134970: step 8403, loss 0.531681.
Train: 2018-08-09T11:22:44.213076: step 8404, loss 0.595733.
Train: 2018-08-09T11:22:44.306804: step 8405, loss 0.595423.
Train: 2018-08-09T11:22:44.384912: step 8406, loss 0.483983.
Train: 2018-08-09T11:22:44.462988: step 8407, loss 0.546691.
Train: 2018-08-09T11:22:44.541094: step 8408, loss 0.594479.
Train: 2018-08-09T11:22:44.619202: step 8409, loss 0.562326.
Train: 2018-08-09T11:22:44.697306: step 8410, loss 0.546198.
Test: 2018-08-09T11:22:45.198605: step 8410, loss 0.549177.
Train: 2018-08-09T11:22:45.276708: step 8411, loss 0.497446.
Train: 2018-08-09T11:22:45.370438: step 8412, loss 0.527952.
Train: 2018-08-09T11:22:45.448573: step 8413, loss 0.697151.
Train: 2018-08-09T11:22:45.526679: step 8414, loss 0.639552.
Train: 2018-08-09T11:22:45.609398: step 8415, loss 0.558718.
Train: 2018-08-09T11:22:45.687476: step 8416, loss 0.579827.
Train: 2018-08-09T11:22:45.765611: step 8417, loss 0.599872.
Train: 2018-08-09T11:22:45.843713: step 8418, loss 0.55256.
Train: 2018-08-09T11:22:45.937445: step 8419, loss 0.626876.
Train: 2018-08-09T11:22:46.015551: step 8420, loss 0.578909.
Test: 2018-08-09T11:22:46.499811: step 8420, loss 0.548121.
Train: 2018-08-09T11:22:46.577920: step 8421, loss 0.481263.
Train: 2018-08-09T11:22:46.671641: step 8422, loss 0.659925.
Train: 2018-08-09T11:22:46.749724: step 8423, loss 0.563055.
Train: 2018-08-09T11:22:46.827860: step 8424, loss 0.627166.
Train: 2018-08-09T11:22:46.914184: step 8425, loss 0.547447.
Train: 2018-08-09T11:22:46.980716: step 8426, loss 0.625682.
Train: 2018-08-09T11:22:47.074413: step 8427, loss 0.53226.
Train: 2018-08-09T11:22:47.152520: step 8428, loss 0.563142.
Train: 2018-08-09T11:22:47.230657: step 8429, loss 0.501875.
Train: 2018-08-09T11:22:47.308764: step 8430, loss 0.501662.
Test: 2018-08-09T11:22:47.808617: step 8430, loss 0.549027.
Train: 2018-08-09T11:22:47.886724: step 8431, loss 0.609718.
Train: 2018-08-09T11:22:47.964828: step 8432, loss 0.50158.
Train: 2018-08-09T11:22:48.042936: step 8433, loss 0.563583.
Train: 2018-08-09T11:22:48.136688: step 8434, loss 0.501237.
Train: 2018-08-09T11:22:48.214770: step 8435, loss 0.56331.
Train: 2018-08-09T11:22:48.292877: step 8436, loss 0.53206.
Train: 2018-08-09T11:22:48.371017: step 8437, loss 0.578563.
Train: 2018-08-09T11:22:48.449091: step 8438, loss 0.467106.
Train: 2018-08-09T11:22:48.527228: step 8439, loss 0.514533.
Train: 2018-08-09T11:22:48.620926: step 8440, loss 0.681803.
Test: 2018-08-09T11:22:49.120835: step 8440, loss 0.54706.
Train: 2018-08-09T11:22:49.198943: step 8441, loss 0.581196.
Train: 2018-08-09T11:22:49.277020: step 8442, loss 0.4155.
Train: 2018-08-09T11:22:49.355157: step 8443, loss 0.612429.
Train: 2018-08-09T11:22:49.433232: step 8444, loss 0.462483.
Train: 2018-08-09T11:22:49.511373: step 8445, loss 0.564982.
Train: 2018-08-09T11:22:49.605100: step 8446, loss 0.528405.
Train: 2018-08-09T11:22:49.683205: step 8447, loss 0.598054.
Train: 2018-08-09T11:22:49.761280: step 8448, loss 0.564986.
Train: 2018-08-09T11:22:49.839390: step 8449, loss 0.546759.
Train: 2018-08-09T11:22:49.918850: step 8450, loss 0.461537.
Test: 2018-08-09T11:22:50.418734: step 8450, loss 0.55022.
Train: 2018-08-09T11:22:50.496870: step 8451, loss 0.560685.
Train: 2018-08-09T11:22:50.574976: step 8452, loss 0.508336.
Train: 2018-08-09T11:22:50.668706: step 8453, loss 0.617375.
Train: 2018-08-09T11:22:50.746805: step 8454, loss 0.563101.
Train: 2018-08-09T11:22:50.824888: step 8455, loss 0.474423.
Train: 2018-08-09T11:22:50.903026: step 8456, loss 0.684543.
Train: 2018-08-09T11:22:50.981134: step 8457, loss 0.596251.
Train: 2018-08-09T11:22:51.074828: step 8458, loss 0.560977.
Train: 2018-08-09T11:22:51.152934: step 8459, loss 0.548234.
Train: 2018-08-09T11:22:51.231041: step 8460, loss 0.440489.
Test: 2018-08-09T11:22:51.730924: step 8460, loss 0.546811.
Train: 2018-08-09T11:22:51.809060: step 8461, loss 0.601902.
Train: 2018-08-09T11:22:51.887167: step 8462, loss 0.528755.
Train: 2018-08-09T11:22:51.967591: step 8463, loss 0.578825.
Train: 2018-08-09T11:22:52.045727: step 8464, loss 0.581369.
Train: 2018-08-09T11:22:52.123804: step 8465, loss 0.63107.
Train: 2018-08-09T11:22:52.201940: step 8466, loss 0.512764.
Train: 2018-08-09T11:22:52.280046: step 8467, loss 0.6129.
Train: 2018-08-09T11:22:52.373774: step 8468, loss 0.596967.
Train: 2018-08-09T11:22:52.451880: step 8469, loss 0.628965.
Train: 2018-08-09T11:22:52.529958: step 8470, loss 0.545768.
Test: 2018-08-09T11:22:53.029840: step 8470, loss 0.548516.
Train: 2018-08-09T11:22:53.107970: step 8471, loss 0.579209.
Train: 2018-08-09T11:22:53.186082: step 8472, loss 0.660004.
Train: 2018-08-09T11:22:53.264160: step 8473, loss 0.530025.
Train: 2018-08-09T11:22:53.342265: step 8474, loss 0.514067.
Train: 2018-08-09T11:22:53.420403: step 8475, loss 0.627257.
Train: 2018-08-09T11:22:53.514131: step 8476, loss 0.594552.
Train: 2018-08-09T11:22:53.592208: step 8477, loss 0.643077.
Train: 2018-08-09T11:22:53.670344: step 8478, loss 0.451441.
Train: 2018-08-09T11:22:53.748452: step 8479, loss 0.595388.
Train: 2018-08-09T11:22:53.826557: step 8480, loss 0.499559.
Test: 2018-08-09T11:22:54.327165: step 8480, loss 0.548028.
Train: 2018-08-09T11:22:54.405272: step 8481, loss 0.546861.
Train: 2018-08-09T11:22:54.483410: step 8482, loss 0.531174.
Train: 2018-08-09T11:22:54.577138: step 8483, loss 0.514917.
Train: 2018-08-09T11:22:54.655212: step 8484, loss 0.595348.
Train: 2018-08-09T11:22:54.733331: step 8485, loss 0.466483.
Train: 2018-08-09T11:22:54.811455: step 8486, loss 0.417469.
Train: 2018-08-09T11:22:54.889567: step 8487, loss 0.579128.
Train: 2018-08-09T11:22:54.967669: step 8488, loss 0.579567.
Train: 2018-08-09T11:22:55.061398: step 8489, loss 0.564042.
Train: 2018-08-09T11:22:55.139503: step 8490, loss 0.529507.
Test: 2018-08-09T11:22:55.639357: step 8490, loss 0.546012.
Train: 2018-08-09T11:22:55.717499: step 8491, loss 0.579885.
Train: 2018-08-09T11:22:55.795569: step 8492, loss 0.527933.
Train: 2018-08-09T11:22:55.873677: step 8493, loss 0.477799.
Train: 2018-08-09T11:22:55.951783: step 8494, loss 0.613706.
Train: 2018-08-09T11:22:56.045541: step 8495, loss 0.700112.
Train: 2018-08-09T11:22:56.123618: step 8496, loss 0.563407.
Train: 2018-08-09T11:22:56.201754: step 8497, loss 0.561641.
Train: 2018-08-09T11:22:56.279830: step 8498, loss 0.594954.
Train: 2018-08-09T11:22:56.357967: step 8499, loss 0.56372.
Train: 2018-08-09T11:22:56.451665: step 8500, loss 0.628802.
Test: 2018-08-09T11:22:56.937334: step 8500, loss 0.547709.
Train: 2018-08-09T11:22:57.499731: step 8501, loss 0.546125.
Train: 2018-08-09T11:22:57.577806: step 8502, loss 0.529121.
Train: 2018-08-09T11:22:57.671565: step 8503, loss 0.562758.
Train: 2018-08-09T11:22:57.749642: step 8504, loss 0.53.
Train: 2018-08-09T11:22:57.827778: step 8505, loss 0.530031.
Train: 2018-08-09T11:22:57.905856: step 8506, loss 0.513974.
Train: 2018-08-09T11:22:57.983991: step 8507, loss 0.692907.
Train: 2018-08-09T11:22:58.077688: step 8508, loss 0.529843.
Train: 2018-08-09T11:22:58.155827: step 8509, loss 0.579061.
Train: 2018-08-09T11:22:58.233933: step 8510, loss 0.513708.
Test: 2018-08-09T11:22:58.733799: step 8510, loss 0.549857.
Train: 2018-08-09T11:22:58.811890: step 8511, loss 0.595248.
Train: 2018-08-09T11:22:58.889999: step 8512, loss 0.497626.
Train: 2018-08-09T11:22:58.970493: step 8513, loss 0.61147.
Train: 2018-08-09T11:22:59.048601: step 8514, loss 0.546548.
Train: 2018-08-09T11:22:59.126736: step 8515, loss 0.644092.
Train: 2018-08-09T11:22:59.220465: step 8516, loss 0.546313.
Train: 2018-08-09T11:22:59.298541: step 8517, loss 0.611351.
Train: 2018-08-09T11:22:59.376678: step 8518, loss 0.546615.
Train: 2018-08-09T11:22:59.454779: step 8519, loss 0.530435.
Train: 2018-08-09T11:22:59.532860: step 8520, loss 0.514237.
Test: 2018-08-09T11:23:00.032744: step 8520, loss 0.551843.
Train: 2018-08-09T11:23:00.110850: step 8521, loss 0.44956.
Train: 2018-08-09T11:23:00.188986: step 8522, loss 0.562756.
Train: 2018-08-09T11:23:00.282716: step 8523, loss 0.546329.
Train: 2018-08-09T11:23:00.360820: step 8524, loss 0.49735.
Train: 2018-08-09T11:23:00.438928: step 8525, loss 0.562695.
Train: 2018-08-09T11:23:00.517035: step 8526, loss 0.430896.
Train: 2018-08-09T11:23:00.595111: step 8527, loss 0.628375.
Train: 2018-08-09T11:23:00.673247: step 8528, loss 0.661489.
Train: 2018-08-09T11:23:00.751323: step 8529, loss 0.529572.
Train: 2018-08-09T11:23:00.829460: step 8530, loss 0.612685.
Test: 2018-08-09T11:23:01.331666: step 8530, loss 0.550403.
Train: 2018-08-09T11:23:01.409796: step 8531, loss 0.529338.
Train: 2018-08-09T11:23:01.487877: step 8532, loss 0.528933.
Train: 2018-08-09T11:23:01.566015: step 8533, loss 0.56117.
Train: 2018-08-09T11:23:01.659713: step 8534, loss 0.578538.
Train: 2018-08-09T11:23:01.737849: step 8535, loss 0.595696.
Train: 2018-08-09T11:23:01.815926: step 8536, loss 0.563986.
Train: 2018-08-09T11:23:01.894032: step 8537, loss 0.511029.
Train: 2018-08-09T11:23:01.972168: step 8538, loss 0.490612.
Train: 2018-08-09T11:23:02.050276: step 8539, loss 0.578537.
Train: 2018-08-09T11:23:02.128352: step 8540, loss 0.616136.
Test: 2018-08-09T11:23:02.628259: step 8540, loss 0.548623.
Train: 2018-08-09T11:23:02.706342: step 8541, loss 0.669636.
Train: 2018-08-09T11:23:02.800099: step 8542, loss 0.772427.
Train: 2018-08-09T11:23:02.878206: step 8543, loss 0.544047.
Train: 2018-08-09T11:23:02.958602: step 8544, loss 0.395451.
Train: 2018-08-09T11:23:03.036708: step 8545, loss 0.462935.
Train: 2018-08-09T11:23:03.114818: step 8546, loss 0.51309.
Train: 2018-08-09T11:23:03.192922: step 8547, loss 0.54588.
Train: 2018-08-09T11:23:03.270998: step 8548, loss 0.57924.
Train: 2018-08-09T11:23:03.364758: step 8549, loss 0.578414.
Train: 2018-08-09T11:23:03.442863: step 8550, loss 0.530555.
Test: 2018-08-09T11:23:03.942659: step 8550, loss 0.550186.
Train: 2018-08-09T11:23:04.020741: step 8551, loss 0.578953.
Train: 2018-08-09T11:23:04.098873: step 8552, loss 0.496709.
Train: 2018-08-09T11:23:04.176986: step 8553, loss 0.595656.
Train: 2018-08-09T11:23:04.255061: step 8554, loss 0.513039.
Train: 2018-08-09T11:23:04.348788: step 8555, loss 0.529571.
Train: 2018-08-09T11:23:04.426927: step 8556, loss 0.562366.
Train: 2018-08-09T11:23:04.505032: step 8557, loss 0.644902.
Train: 2018-08-09T11:23:04.583139: step 8558, loss 0.529243.
Train: 2018-08-09T11:23:04.661245: step 8559, loss 0.46322.
Train: 2018-08-09T11:23:04.754973: step 8560, loss 0.662936.
Test: 2018-08-09T11:23:05.241597: step 8560, loss 0.549434.
Train: 2018-08-09T11:23:05.319703: step 8561, loss 0.562308.
Train: 2018-08-09T11:23:05.413401: step 8562, loss 0.413356.
Train: 2018-08-09T11:23:05.491508: step 8563, loss 0.462822.
Train: 2018-08-09T11:23:05.569645: step 8564, loss 0.579024.
Train: 2018-08-09T11:23:05.663368: step 8565, loss 0.612739.
Train: 2018-08-09T11:23:05.741450: step 8566, loss 0.444618.
Train: 2018-08-09T11:23:05.819586: step 8567, loss 0.596935.
Train: 2018-08-09T11:23:05.897693: step 8568, loss 0.512239.
Train: 2018-08-09T11:23:05.975800: step 8569, loss 0.494428.
Train: 2018-08-09T11:23:06.069497: step 8570, loss 0.477229.
Test: 2018-08-09T11:23:06.553759: step 8570, loss 0.550169.
Train: 2018-08-09T11:23:06.647510: step 8571, loss 0.630763.
Train: 2018-08-09T11:23:06.725622: step 8572, loss 0.528355.
Train: 2018-08-09T11:23:06.803699: step 8573, loss 0.579045.
Train: 2018-08-09T11:23:06.881806: step 8574, loss 0.527609.
Train: 2018-08-09T11:23:06.962211: step 8575, loss 0.614795.
Train: 2018-08-09T11:23:07.040317: step 8576, loss 0.580109.
Train: 2018-08-09T11:23:07.118455: step 8577, loss 0.596285.
Train: 2018-08-09T11:23:07.212153: step 8578, loss 0.631796.
Train: 2018-08-09T11:23:07.290259: step 8579, loss 0.59648.
Train: 2018-08-09T11:23:07.368395: step 8580, loss 0.528039.
Test: 2018-08-09T11:23:07.868363: step 8580, loss 0.54823.
Train: 2018-08-09T11:23:07.937709: step 8581, loss 0.61302.
Train: 2018-08-09T11:23:08.015847: step 8582, loss 0.511848.
Train: 2018-08-09T11:23:08.093923: step 8583, loss 0.391931.
Train: 2018-08-09T11:23:08.187650: step 8584, loss 0.579047.
Train: 2018-08-09T11:23:08.265759: step 8585, loss 0.545394.
Train: 2018-08-09T11:23:08.343895: step 8586, loss 0.511009.
Train: 2018-08-09T11:23:08.422000: step 8587, loss 0.527559.
Train: 2018-08-09T11:23:08.500106: step 8588, loss 0.528333.
Train: 2018-08-09T11:23:08.578185: step 8589, loss 0.527701.
Train: 2018-08-09T11:23:08.671912: step 8590, loss 0.596182.
Test: 2018-08-09T11:23:09.165434: step 8590, loss 0.548111.
Train: 2018-08-09T11:23:09.243541: step 8591, loss 0.546013.
Train: 2018-08-09T11:23:09.321648: step 8592, loss 0.563224.
Train: 2018-08-09T11:23:09.399754: step 8593, loss 0.614882.
Train: 2018-08-09T11:23:09.493481: step 8594, loss 0.613961.
Train: 2018-08-09T11:23:09.571587: step 8595, loss 0.578752.
Train: 2018-08-09T11:23:09.649696: step 8596, loss 0.579451.
Train: 2018-08-09T11:23:09.727832: step 8597, loss 0.595977.
Train: 2018-08-09T11:23:09.805908: step 8598, loss 0.630969.
Train: 2018-08-09T11:23:09.884044: step 8599, loss 0.597012.
Train: 2018-08-09T11:23:09.962120: step 8600, loss 0.511384.
Test: 2018-08-09T11:23:10.462108: step 8600, loss 0.547863.
Train: 2018-08-09T11:23:11.024480: step 8601, loss 0.511508.
Train: 2018-08-09T11:23:11.118207: step 8602, loss 0.528793.
Train: 2018-08-09T11:23:11.196314: step 8603, loss 0.545481.
Train: 2018-08-09T11:23:11.274420: step 8604, loss 0.495578.
Train: 2018-08-09T11:23:11.352528: step 8605, loss 0.511906.
Train: 2018-08-09T11:23:11.430632: step 8606, loss 0.57944.
Train: 2018-08-09T11:23:11.524333: step 8607, loss 0.646246.
Train: 2018-08-09T11:23:11.602469: step 8608, loss 0.630065.
Train: 2018-08-09T11:23:11.680578: step 8609, loss 0.512375.
Train: 2018-08-09T11:23:11.758652: step 8610, loss 0.695895.
Test: 2018-08-09T11:23:12.260957: step 8610, loss 0.550642.
Train: 2018-08-09T11:23:12.339082: step 8611, loss 0.579.
Train: 2018-08-09T11:23:12.417184: step 8612, loss 0.463053.
Train: 2018-08-09T11:23:12.495268: step 8613, loss 0.595538.
Train: 2018-08-09T11:23:12.573402: step 8614, loss 0.578999.
Train: 2018-08-09T11:23:12.667130: step 8615, loss 0.496619.
Train: 2018-08-09T11:23:12.745238: step 8616, loss 0.595444.
Train: 2018-08-09T11:23:12.823344: step 8617, loss 0.611891.
Train: 2018-08-09T11:23:12.901450: step 8618, loss 0.579007.
Train: 2018-08-09T11:23:12.979557: step 8619, loss 0.448093.
Train: 2018-08-09T11:23:13.057634: step 8620, loss 0.562612.
Test: 2018-08-09T11:23:13.557515: step 8620, loss 0.547262.
Train: 2018-08-09T11:23:13.635623: step 8621, loss 0.546196.
Train: 2018-08-09T11:23:13.713759: step 8622, loss 0.513526.
Train: 2018-08-09T11:23:13.807458: step 8623, loss 0.480667.
Train: 2018-08-09T11:23:13.885593: step 8624, loss 0.529714.
Train: 2018-08-09T11:23:13.966000: step 8625, loss 0.496675.
Train: 2018-08-09T11:23:14.044102: step 8626, loss 0.612098.
Train: 2018-08-09T11:23:14.122211: step 8627, loss 0.545955.
Train: 2018-08-09T11:23:14.200318: step 8628, loss 0.59567.
Train: 2018-08-09T11:23:14.294016: step 8629, loss 0.512622.
Train: 2018-08-09T11:23:14.372121: step 8630, loss 0.57915.
Test: 2018-08-09T11:23:14.872004: step 8630, loss 0.54744.
Train: 2018-08-09T11:23:14.950140: step 8631, loss 0.595696.
Train: 2018-08-09T11:23:15.028248: step 8632, loss 0.529123.
Train: 2018-08-09T11:23:15.106355: step 8633, loss 0.562419.
Train: 2018-08-09T11:23:15.184460: step 8634, loss 0.579191.
Train: 2018-08-09T11:23:15.278159: step 8635, loss 0.5457.
Train: 2018-08-09T11:23:15.356297: step 8636, loss 0.57913.
Train: 2018-08-09T11:23:15.434373: step 8637, loss 0.49548.
Train: 2018-08-09T11:23:15.512508: step 8638, loss 0.595944.
Train: 2018-08-09T11:23:15.590615: step 8639, loss 0.612552.
Train: 2018-08-09T11:23:15.684338: step 8640, loss 0.679608.
Test: 2018-08-09T11:23:16.169885: step 8640, loss 0.551818.
Train: 2018-08-09T11:23:16.263611: step 8641, loss 0.562395.
Train: 2018-08-09T11:23:16.341750: step 8642, loss 0.645933.
Train: 2018-08-09T11:23:16.419826: step 8643, loss 0.66199.
Train: 2018-08-09T11:23:16.497961: step 8644, loss 0.562488.
Train: 2018-08-09T11:23:16.576067: step 8645, loss 0.628091.
Train: 2018-08-09T11:23:16.669766: step 8646, loss 0.578856.
Train: 2018-08-09T11:23:16.747902: step 8647, loss 0.59524.
Train: 2018-08-09T11:23:16.826002: step 8648, loss 0.611053.
Train: 2018-08-09T11:23:16.904115: step 8649, loss 0.515464.
Train: 2018-08-09T11:23:16.996301: step 8650, loss 0.674912.
Test: 2018-08-09T11:23:17.480069: step 8650, loss 0.551153.
Train: 2018-08-09T11:23:17.573827: step 8651, loss 0.531051.
Train: 2018-08-09T11:23:17.651934: step 8652, loss 0.547496.
Train: 2018-08-09T11:23:17.730041: step 8653, loss 0.578852.
Train: 2018-08-09T11:23:17.808150: step 8654, loss 0.56318.
Train: 2018-08-09T11:23:17.886254: step 8655, loss 0.594597.
Train: 2018-08-09T11:23:17.967697: step 8656, loss 0.610256.
Train: 2018-08-09T11:23:18.045803: step 8657, loss 0.672728.
Train: 2018-08-09T11:23:18.123910: step 8658, loss 0.563341.
Train: 2018-08-09T11:23:18.217608: step 8659, loss 0.640927.
Train: 2018-08-09T11:23:18.295714: step 8660, loss 0.455555.
Test: 2018-08-09T11:23:18.779976: step 8660, loss 0.55148.
Train: 2018-08-09T11:23:18.858112: step 8661, loss 0.502018.
Train: 2018-08-09T11:23:18.936223: step 8662, loss 0.548209.
Train: 2018-08-09T11:23:19.029917: step 8663, loss 0.609715.
Train: 2018-08-09T11:23:19.108043: step 8664, loss 0.625061.
Train: 2018-08-09T11:23:19.186161: step 8665, loss 0.548302.
Train: 2018-08-09T11:23:19.264268: step 8666, loss 0.502361.
Train: 2018-08-09T11:23:19.342373: step 8667, loss 0.624984.
Train: 2018-08-09T11:23:19.436070: step 8668, loss 0.548327.
Train: 2018-08-09T11:23:19.514179: step 8669, loss 0.609647.
Train: 2018-08-09T11:23:19.592285: step 8670, loss 0.640283.
Test: 2018-08-09T11:23:20.092840: step 8670, loss 0.551678.
Train: 2018-08-09T11:23:20.170947: step 8671, loss 0.517802.
Train: 2018-08-09T11:23:20.249052: step 8672, loss 0.655467.
Train: 2018-08-09T11:23:20.327160: step 8673, loss 0.563748.
Train: 2018-08-09T11:23:20.420920: step 8674, loss 0.624762.
Train: 2018-08-09T11:23:20.499024: step 8675, loss 0.563811.
Train: 2018-08-09T11:23:20.577102: step 8676, loss 0.5183.
Train: 2018-08-09T11:23:20.655208: step 8677, loss 0.548696.
Train: 2018-08-09T11:23:20.733344: step 8678, loss 0.579023.
Train: 2018-08-09T11:23:20.811452: step 8679, loss 0.639874.
Train: 2018-08-09T11:23:20.889527: step 8680, loss 0.59427.
Test: 2018-08-09T11:23:21.405055: step 8680, loss 0.551412.
Train: 2018-08-09T11:23:21.483136: step 8681, loss 0.54876.
Train: 2018-08-09T11:23:21.561244: step 8682, loss 0.609354.
Train: 2018-08-09T11:23:21.639381: step 8683, loss 0.548795.
Train: 2018-08-09T11:23:21.717481: step 8684, loss 0.624558.
Train: 2018-08-09T11:23:21.811215: step 8685, loss 0.579109.
Train: 2018-08-09T11:23:21.889321: step 8686, loss 0.564016.
Train: 2018-08-09T11:23:21.967398: step 8687, loss 0.548965.
Train: 2018-08-09T11:23:22.045546: step 8688, loss 0.669602.
Train: 2018-08-09T11:23:22.123610: step 8689, loss 0.549049.
Train: 2018-08-09T11:23:22.201747: step 8690, loss 0.549074.
Test: 2018-08-09T11:23:22.701601: step 8690, loss 0.553445.
Train: 2018-08-09T11:23:22.779732: step 8691, loss 0.549096.
Train: 2018-08-09T11:23:22.857812: step 8692, loss 0.549076.
Train: 2018-08-09T11:23:22.938196: step 8693, loss 0.699574.
Train: 2018-08-09T11:23:23.031939: step 8694, loss 0.669358.
Train: 2018-08-09T11:23:23.110031: step 8695, loss 0.594163.
Train: 2018-08-09T11:23:23.188139: step 8696, loss 0.56428.
Train: 2018-08-09T11:23:23.266274: step 8697, loss 0.504742.
Train: 2018-08-09T11:23:23.344351: step 8698, loss 0.54947.
Train: 2018-08-09T11:23:23.422492: step 8699, loss 0.594173.
Train: 2018-08-09T11:23:23.516196: step 8700, loss 0.65377.
Test: 2018-08-09T11:23:24.000471: step 8700, loss 0.553847.
Train: 2018-08-09T11:23:24.562843: step 8701, loss 0.519773.
Train: 2018-08-09T11:23:24.640923: step 8702, loss 0.579242.
Train: 2018-08-09T11:23:24.734679: step 8703, loss 0.534683.
Train: 2018-08-09T11:23:24.812785: step 8704, loss 0.519735.
Train: 2018-08-09T11:23:24.890862: step 8705, loss 0.474901.
Train: 2018-08-09T11:23:24.970378: step 8706, loss 0.594252.
Train: 2018-08-09T11:23:25.048515: step 8707, loss 0.519073.
Train: 2018-08-09T11:23:25.126621: step 8708, loss 0.518821.
Train: 2018-08-09T11:23:25.220349: step 8709, loss 0.548759.
Train: 2018-08-09T11:23:25.298457: step 8710, loss 0.624808.
Test: 2018-08-09T11:23:25.798324: step 8710, loss 0.549917.
Train: 2018-08-09T11:23:25.876415: step 8711, loss 0.624742.
Train: 2018-08-09T11:23:25.954522: step 8712, loss 0.548366.
Train: 2018-08-09T11:23:26.032628: step 8713, loss 0.609571.
Train: 2018-08-09T11:23:26.110764: step 8714, loss 0.517138.
Train: 2018-08-09T11:23:26.188871: step 8715, loss 0.548003.
Train: 2018-08-09T11:23:26.282605: step 8716, loss 0.578812.
Train: 2018-08-09T11:23:26.360705: step 8717, loss 0.594917.
Train: 2018-08-09T11:23:26.438812: step 8718, loss 0.563196.
Train: 2018-08-09T11:23:26.516922: step 8719, loss 0.657883.
Train: 2018-08-09T11:23:26.595021: step 8720, loss 0.499642.
Test: 2018-08-09T11:23:27.097293: step 8720, loss 0.551421.
Train: 2018-08-09T11:23:27.175399: step 8721, loss 0.563541.
Train: 2018-08-09T11:23:27.253506: step 8722, loss 0.562482.
Train: 2018-08-09T11:23:27.331613: step 8723, loss 0.563899.
Train: 2018-08-09T11:23:27.425335: step 8724, loss 0.468282.
Train: 2018-08-09T11:23:27.503447: step 8725, loss 0.515232.
Train: 2018-08-09T11:23:27.581549: step 8726, loss 0.577895.
Train: 2018-08-09T11:23:27.659663: step 8727, loss 0.499013.
Train: 2018-08-09T11:23:27.737737: step 8728, loss 0.627513.
Train: 2018-08-09T11:23:27.831495: step 8729, loss 0.57731.
Train: 2018-08-09T11:23:27.909601: step 8730, loss 0.647243.
Test: 2018-08-09T11:23:28.409454: step 8730, loss 0.548205.
Train: 2018-08-09T11:23:28.487559: step 8731, loss 0.545487.
Train: 2018-08-09T11:23:28.565697: step 8732, loss 0.547206.
Train: 2018-08-09T11:23:28.643774: step 8733, loss 0.464327.
Train: 2018-08-09T11:23:28.737503: step 8734, loss 0.627283.
Train: 2018-08-09T11:23:28.815637: step 8735, loss 0.632086.
Train: 2018-08-09T11:23:28.893715: step 8736, loss 0.480903.
Train: 2018-08-09T11:23:28.973151: step 8737, loss 0.547294.
Train: 2018-08-09T11:23:29.051227: step 8738, loss 0.561367.
Train: 2018-08-09T11:23:29.129366: step 8739, loss 0.514872.
Train: 2018-08-09T11:23:29.207442: step 8740, loss 0.563183.
Test: 2018-08-09T11:23:29.707324: step 8740, loss 0.548479.
Train: 2018-08-09T11:23:29.785429: step 8741, loss 0.546805.
Train: 2018-08-09T11:23:29.863573: step 8742, loss 0.496395.
Train: 2018-08-09T11:23:29.957296: step 8743, loss 0.530165.
Train: 2018-08-09T11:23:30.035401: step 8744, loss 0.546471.
Train: 2018-08-09T11:23:30.113478: step 8745, loss 0.513586.
Train: 2018-08-09T11:23:30.191615: step 8746, loss 0.546774.
Train: 2018-08-09T11:23:30.269721: step 8747, loss 0.545459.
Train: 2018-08-09T11:23:30.363419: step 8748, loss 0.630927.
Train: 2018-08-09T11:23:30.441556: step 8749, loss 0.52941.
Train: 2018-08-09T11:23:30.535283: step 8750, loss 0.580036.
Test: 2018-08-09T11:23:31.021832: step 8750, loss 0.546891.
Train: 2018-08-09T11:23:31.099911: step 8751, loss 0.512768.
Train: 2018-08-09T11:23:31.193668: step 8752, loss 0.579055.
Train: 2018-08-09T11:23:31.271774: step 8753, loss 0.529117.
Train: 2018-08-09T11:23:31.349850: step 8754, loss 0.595607.
Train: 2018-08-09T11:23:31.427990: step 8755, loss 0.495972.
Train: 2018-08-09T11:23:31.506094: step 8756, loss 0.579081.
Train: 2018-08-09T11:23:31.584202: step 8757, loss 0.462329.
Train: 2018-08-09T11:23:31.677898: step 8758, loss 0.495357.
Train: 2018-08-09T11:23:31.756005: step 8759, loss 0.545394.
Train: 2018-08-09T11:23:31.834142: step 8760, loss 0.494472.
Test: 2018-08-09T11:23:32.333995: step 8760, loss 0.548246.
Train: 2018-08-09T11:23:32.412102: step 8761, loss 0.580307.
Train: 2018-08-09T11:23:32.490207: step 8762, loss 0.648605.
Train: 2018-08-09T11:23:32.568344: step 8763, loss 0.613731.
Train: 2018-08-09T11:23:32.646450: step 8764, loss 0.544372.
Train: 2018-08-09T11:23:32.740173: step 8765, loss 0.647656.
Train: 2018-08-09T11:23:32.818255: step 8766, loss 0.596066.
Train: 2018-08-09T11:23:32.896361: step 8767, loss 0.46024.
Train: 2018-08-09T11:23:32.976845: step 8768, loss 0.5463.
Train: 2018-08-09T11:23:33.054955: step 8769, loss 0.494467.
Train: 2018-08-09T11:23:33.133028: step 8770, loss 0.613851.
Test: 2018-08-09T11:23:33.632944: step 8770, loss 0.548376.
Train: 2018-08-09T11:23:33.711048: step 8771, loss 0.545382.
Train: 2018-08-09T11:23:33.804776: step 8772, loss 0.57929.
Train: 2018-08-09T11:23:33.882884: step 8773, loss 0.528391.
Train: 2018-08-09T11:23:33.960989: step 8774, loss 0.664101.
Train: 2018-08-09T11:23:34.039094: step 8775, loss 0.579405.
Train: 2018-08-09T11:23:34.117203: step 8776, loss 0.495165.
Train: 2018-08-09T11:23:34.210929: step 8777, loss 0.612783.
Train: 2018-08-09T11:23:34.289036: step 8778, loss 0.545643.
Train: 2018-08-09T11:23:34.367143: step 8779, loss 0.49549.
Train: 2018-08-09T11:23:34.445221: step 8780, loss 0.47879.
Test: 2018-08-09T11:23:34.947711: step 8780, loss 0.548613.
Train: 2018-08-09T11:23:35.025841: step 8781, loss 0.495424.
Train: 2018-08-09T11:23:35.103946: step 8782, loss 0.612745.
Train: 2018-08-09T11:23:35.182050: step 8783, loss 0.545591.
Train: 2018-08-09T11:23:35.275751: step 8784, loss 0.579186.
Train: 2018-08-09T11:23:35.341841: step 8785, loss 0.545531.
Train: 2018-08-09T11:23:35.435568: step 8786, loss 0.680164.
Train: 2018-08-09T11:23:35.513705: step 8787, loss 0.646399.
Train: 2018-08-09T11:23:35.591781: step 8788, loss 0.562373.
Train: 2018-08-09T11:23:35.669920: step 8789, loss 0.57913.
Train: 2018-08-09T11:23:35.747995: step 8790, loss 0.579061.
Test: 2018-08-09T11:23:36.247909: step 8790, loss 0.549366.
Train: 2018-08-09T11:23:36.325984: step 8791, loss 0.495822.
Train: 2018-08-09T11:23:36.419713: step 8792, loss 0.662432.
Train: 2018-08-09T11:23:36.497849: step 8793, loss 0.612216.
Train: 2018-08-09T11:23:36.575955: step 8794, loss 0.579054.
Train: 2018-08-09T11:23:36.654063: step 8795, loss 0.644721.
Train: 2018-08-09T11:23:36.732169: step 8796, loss 0.611622.
Train: 2018-08-09T11:23:36.810276: step 8797, loss 0.53013.
Train: 2018-08-09T11:23:36.903973: step 8798, loss 0.659854.
Train: 2018-08-09T11:23:36.982107: step 8799, loss 0.57882.
Train: 2018-08-09T11:23:37.060185: step 8800, loss 0.498836.
Test: 2018-08-09T11:23:37.560069: step 8800, loss 0.550371.
Train: 2018-08-09T11:23:38.153708: step 8801, loss 0.578841.
Train: 2018-08-09T11:23:38.231816: step 8802, loss 0.483314.
Train: 2018-08-09T11:23:38.325545: step 8803, loss 0.562977.
Train: 2018-08-09T11:23:38.403650: step 8804, loss 0.5155.
Train: 2018-08-09T11:23:38.481751: step 8805, loss 0.547103.
Train: 2018-08-09T11:23:38.559864: step 8806, loss 0.531183.
Train: 2018-08-09T11:23:38.637970: step 8807, loss 0.642489.
Train: 2018-08-09T11:23:38.716075: step 8808, loss 0.594759.
Train: 2018-08-09T11:23:38.809773: step 8809, loss 0.61063.
Train: 2018-08-09T11:23:38.887911: step 8810, loss 0.531274.
Test: 2018-08-09T11:23:39.385030: step 8810, loss 0.548758.
Train: 2018-08-09T11:23:39.463166: step 8811, loss 0.515459.
Train: 2018-08-09T11:23:39.541272: step 8812, loss 0.705699.
Train: 2018-08-09T11:23:39.619380: step 8813, loss 0.547221.
Train: 2018-08-09T11:23:39.697486: step 8814, loss 0.499879.
Train: 2018-08-09T11:23:39.791213: step 8815, loss 0.436713.
Train: 2018-08-09T11:23:39.869325: step 8816, loss 0.56303.
Train: 2018-08-09T11:23:39.947427: step 8817, loss 0.642317.
Train: 2018-08-09T11:23:40.025503: step 8818, loss 0.642359.
Train: 2018-08-09T11:23:40.119266: step 8819, loss 0.610576.
Train: 2018-08-09T11:23:40.197367: step 8820, loss 0.499675.
Test: 2018-08-09T11:23:40.697219: step 8820, loss 0.54878.
Train: 2018-08-09T11:23:40.775356: step 8821, loss 0.563021.
Train: 2018-08-09T11:23:40.853462: step 8822, loss 0.658069.
Train: 2018-08-09T11:23:40.937035: step 8823, loss 0.642138.
Train: 2018-08-09T11:23:41.015143: step 8824, loss 0.610419.
Train: 2018-08-09T11:23:41.093249: step 8825, loss 0.578857.
Train: 2018-08-09T11:23:41.186979: step 8826, loss 0.484824.
Train: 2018-08-09T11:23:41.265084: step 8827, loss 0.578877.
Train: 2018-08-09T11:23:41.343161: step 8828, loss 0.531957.
Train: 2018-08-09T11:23:41.421269: step 8829, loss 0.578878.
Train: 2018-08-09T11:23:41.499404: step 8830, loss 0.54761.
Test: 2018-08-09T11:23:41.993315: step 8830, loss 0.550362.
Train: 2018-08-09T11:23:42.071422: step 8831, loss 0.469402.
Train: 2018-08-09T11:23:42.165179: step 8832, loss 0.578916.
Train: 2018-08-09T11:23:42.243255: step 8833, loss 0.484674.
Train: 2018-08-09T11:23:42.321362: step 8834, loss 0.563162.
Train: 2018-08-09T11:23:42.399498: step 8835, loss 0.594683.
Train: 2018-08-09T11:23:42.477607: step 8836, loss 0.578898.
Train: 2018-08-09T11:23:42.555681: step 8837, loss 0.547135.
Train: 2018-08-09T11:23:42.649439: step 8838, loss 0.626501.
Train: 2018-08-09T11:23:42.727547: step 8839, loss 0.547035.
Train: 2018-08-09T11:23:42.805653: step 8840, loss 0.499323.
Test: 2018-08-09T11:23:43.305505: step 8840, loss 0.551573.
Train: 2018-08-09T11:23:43.383612: step 8841, loss 0.562884.
Train: 2018-08-09T11:23:43.461718: step 8842, loss 0.658926.
Train: 2018-08-09T11:23:43.539826: step 8843, loss 0.56285.
Train: 2018-08-09T11:23:43.617931: step 8844, loss 0.546797.
Train: 2018-08-09T11:23:43.711689: step 8845, loss 0.642921.
Train: 2018-08-09T11:23:43.789796: step 8846, loss 0.578857.
Train: 2018-08-09T11:23:43.867902: step 8847, loss 0.514968.
Train: 2018-08-09T11:23:43.947438: step 8848, loss 0.57891.
Train: 2018-08-09T11:23:44.025516: step 8849, loss 0.514972.
Train: 2018-08-09T11:23:44.119272: step 8850, loss 0.51499.
Test: 2018-08-09T11:23:44.603503: step 8850, loss 0.552703.
Train: 2018-08-09T11:23:44.697230: step 8851, loss 0.514772.
Train: 2018-08-09T11:23:44.775337: step 8852, loss 0.562946.
Train: 2018-08-09T11:23:44.853476: step 8853, loss 0.627148.
Train: 2018-08-09T11:23:44.931551: step 8854, loss 0.546552.
Train: 2018-08-09T11:23:45.009656: step 8855, loss 0.562817.
Train: 2018-08-09T11:23:45.087796: step 8856, loss 0.498264.
Train: 2018-08-09T11:23:45.181493: step 8857, loss 0.530303.
Train: 2018-08-09T11:23:45.259628: step 8858, loss 0.578928.
Train: 2018-08-09T11:23:45.337735: step 8859, loss 0.530169.
Train: 2018-08-09T11:23:45.415841: step 8860, loss 0.530182.
Test: 2018-08-09T11:23:45.918078: step 8860, loss 0.546635.
Train: 2018-08-09T11:23:45.996185: step 8861, loss 0.56237.
Train: 2018-08-09T11:23:46.074291: step 8862, loss 0.759591.
Train: 2018-08-09T11:23:46.152399: step 8863, loss 0.530146.
Train: 2018-08-09T11:23:46.246126: step 8864, loss 0.693487.
Train: 2018-08-09T11:23:46.324264: step 8865, loss 0.481293.
Train: 2018-08-09T11:23:46.402371: step 8866, loss 0.578907.
Train: 2018-08-09T11:23:46.480445: step 8867, loss 0.611382.
Train: 2018-08-09T11:23:46.558583: step 8868, loss 0.514029.
Train: 2018-08-09T11:23:46.636691: step 8869, loss 0.611275.
Train: 2018-08-09T11:23:46.730418: step 8870, loss 0.659799.
Test: 2018-08-09T11:23:47.214649: step 8870, loss 0.550674.
Train: 2018-08-09T11:23:47.308407: step 8871, loss 0.546582.
Train: 2018-08-09T11:23:47.386512: step 8872, loss 0.643382.
Train: 2018-08-09T11:23:47.464625: step 8873, loss 0.594891.
Train: 2018-08-09T11:23:47.542727: step 8874, loss 0.499069.
Train: 2018-08-09T11:23:47.620835: step 8875, loss 0.578855.
Train: 2018-08-09T11:23:47.698941: step 8876, loss 0.499381.
Train: 2018-08-09T11:23:47.792667: step 8877, loss 0.531208.
Train: 2018-08-09T11:23:47.870774: step 8878, loss 0.56298.
Train: 2018-08-09T11:23:47.950234: step 8879, loss 0.515314.
Train: 2018-08-09T11:23:48.028340: step 8880, loss 0.674288.
Test: 2018-08-09T11:23:48.520129: step 8880, loss 0.549281.
Train: 2018-08-09T11:23:48.598264: step 8881, loss 0.451733.
Train: 2018-08-09T11:23:48.691993: step 8882, loss 0.62659.
Train: 2018-08-09T11:23:48.770101: step 8883, loss 0.531125.
Train: 2018-08-09T11:23:48.848205: step 8884, loss 0.562932.
Train: 2018-08-09T11:23:48.926312: step 8885, loss 0.546983.
Train: 2018-08-09T11:23:49.004422: step 8886, loss 0.515032.
Train: 2018-08-09T11:23:49.082497: step 8887, loss 0.482915.
Train: 2018-08-09T11:23:49.160602: step 8888, loss 0.594904.
Train: 2018-08-09T11:23:49.254331: step 8889, loss 0.57899.
Train: 2018-08-09T11:23:49.332437: step 8890, loss 0.481865.
Test: 2018-08-09T11:23:49.832320: step 8890, loss 0.549934.
Train: 2018-08-09T11:23:49.913642: step 8891, loss 0.562508.
Train: 2018-08-09T11:23:49.982197: step 8892, loss 0.628042.
Train: 2018-08-09T11:23:50.075896: step 8893, loss 0.693669.
Train: 2018-08-09T11:23:50.154001: step 8894, loss 0.530311.
Train: 2018-08-09T11:23:50.232109: step 8895, loss 0.627733.
Train: 2018-08-09T11:23:50.310216: step 8896, loss 0.59507.
Train: 2018-08-09T11:23:50.388354: step 8897, loss 0.514294.
Train: 2018-08-09T11:23:50.482073: step 8898, loss 0.772431.
Train: 2018-08-09T11:23:50.560157: step 8899, loss 0.498565.
Train: 2018-08-09T11:23:50.638263: step 8900, loss 0.642905.
Test: 2018-08-09T11:23:51.138145: step 8900, loss 0.548565.
Train: 2018-08-09T11:23:51.669299: step 8901, loss 0.515141.
Train: 2018-08-09T11:23:51.747406: step 8902, loss 0.59482.
Train: 2018-08-09T11:23:51.825514: step 8903, loss 0.578864.
Train: 2018-08-09T11:23:51.903620: step 8904, loss 0.563015.
Train: 2018-08-09T11:23:51.981695: step 8905, loss 0.594686.
Train: 2018-08-09T11:23:52.059833: step 8906, loss 0.547322.
Train: 2018-08-09T11:23:52.153560: step 8907, loss 0.610351.
Train: 2018-08-09T11:23:52.231636: step 8908, loss 0.563154.
Train: 2018-08-09T11:23:52.309773: step 8909, loss 0.43781.
Train: 2018-08-09T11:23:52.387889: step 8910, loss 0.578856.
Test: 2018-08-09T11:23:52.887732: step 8910, loss 0.550849.
Train: 2018-08-09T11:23:52.968262: step 8911, loss 0.453321.
Train: 2018-08-09T11:23:53.046368: step 8912, loss 0.515938.
Train: 2018-08-09T11:23:53.124475: step 8913, loss 0.547288.
Train: 2018-08-09T11:23:53.202551: step 8914, loss 0.499655.
Train: 2018-08-09T11:23:53.280687: step 8915, loss 0.562945.
Train: 2018-08-09T11:23:53.374416: step 8916, loss 0.498998.
Train: 2018-08-09T11:23:53.452519: step 8917, loss 0.675115.
Train: 2018-08-09T11:23:53.530631: step 8918, loss 0.546785.
Train: 2018-08-09T11:23:53.608735: step 8919, loss 0.611128.
Train: 2018-08-09T11:23:53.686814: step 8920, loss 0.562709.
Test: 2018-08-09T11:23:54.186695: step 8920, loss 0.550018.
Train: 2018-08-09T11:23:54.264831: step 8921, loss 0.498024.
Train: 2018-08-09T11:23:54.358554: step 8922, loss 0.53024.
Train: 2018-08-09T11:23:54.436666: step 8923, loss 0.546357.
Train: 2018-08-09T11:23:54.514741: step 8924, loss 0.562693.
Train: 2018-08-09T11:23:54.592849: step 8925, loss 0.595538.
Train: 2018-08-09T11:23:54.670982: step 8926, loss 0.611704.
Train: 2018-08-09T11:23:54.764684: step 8927, loss 0.464257.
Train: 2018-08-09T11:23:54.842846: step 8928, loss 0.496887.
Train: 2018-08-09T11:23:54.921639: step 8929, loss 0.414107.
Train: 2018-08-09T11:23:54.999771: step 8930, loss 0.595995.
Test: 2018-08-09T11:23:55.499630: step 8930, loss 0.548703.
Train: 2018-08-09T11:23:55.577769: step 8931, loss 0.545784.
Train: 2018-08-09T11:23:55.655872: step 8932, loss 0.545555.
Train: 2018-08-09T11:23:55.733950: step 8933, loss 0.646109.
Train: 2018-08-09T11:23:55.827709: step 8934, loss 0.578813.
Train: 2018-08-09T11:23:55.905812: step 8935, loss 0.478357.
Train: 2018-08-09T11:23:55.983891: step 8936, loss 0.477129.
Train: 2018-08-09T11:23:56.062026: step 8937, loss 0.544392.
Train: 2018-08-09T11:23:56.140103: step 8938, loss 0.664395.
Train: 2018-08-09T11:23:56.233831: step 8939, loss 0.580845.
Train: 2018-08-09T11:23:56.311967: step 8940, loss 0.578743.
Test: 2018-08-09T11:23:56.811825: step 8940, loss 0.546.
Train: 2018-08-09T11:23:56.889956: step 8941, loss 0.562288.
Train: 2018-08-09T11:23:56.968034: step 8942, loss 0.546562.
Train: 2018-08-09T11:23:57.061792: step 8943, loss 0.526786.
Train: 2018-08-09T11:23:57.139897: step 8944, loss 0.561521.
Train: 2018-08-09T11:23:57.217974: step 8945, loss 0.613156.
Train: 2018-08-09T11:23:57.296106: step 8946, loss 0.509572.
Train: 2018-08-09T11:23:57.374188: step 8947, loss 0.579215.
Train: 2018-08-09T11:23:57.452293: step 8948, loss 0.510588.
Train: 2018-08-09T11:23:57.530401: step 8949, loss 0.597401.
Train: 2018-08-09T11:23:57.624128: step 8950, loss 0.66743.
Test: 2018-08-09T11:23:58.110733: step 8950, loss 0.55001.
Train: 2018-08-09T11:23:58.188843: step 8951, loss 0.593645.
Train: 2018-08-09T11:23:58.266950: step 8952, loss 0.529569.
Train: 2018-08-09T11:23:58.360678: step 8953, loss 0.476574.
Train: 2018-08-09T11:23:58.438756: step 8954, loss 0.61405.
Train: 2018-08-09T11:23:58.516893: step 8955, loss 0.528428.
Train: 2018-08-09T11:23:58.594998: step 8956, loss 0.613951.
Train: 2018-08-09T11:23:58.673107: step 8957, loss 0.580001.
Train: 2018-08-09T11:23:58.766802: step 8958, loss 0.545888.
Train: 2018-08-09T11:23:58.844939: step 8959, loss 0.546072.
Train: 2018-08-09T11:23:58.923015: step 8960, loss 0.529743.
Test: 2018-08-09T11:23:59.422898: step 8960, loss 0.550825.
Train: 2018-08-09T11:23:59.501034: step 8961, loss 0.562546.
Train: 2018-08-09T11:23:59.579141: step 8962, loss 0.513191.
Train: 2018-08-09T11:23:59.657217: step 8963, loss 0.693893.
Train: 2018-08-09T11:23:59.735356: step 8964, loss 0.595334.
Train: 2018-08-09T11:23:59.813430: step 8965, loss 0.546285.
Train: 2018-08-09T11:23:59.908594: step 8966, loss 0.546339.
Train: 2018-08-09T11:23:59.986701: step 8967, loss 0.627702.
Train: 2018-08-09T11:24:00.064808: step 8968, loss 0.627566.
Train: 2018-08-09T11:24:00.142947: step 8969, loss 0.498055.
Train: 2018-08-09T11:24:00.221021: step 8970, loss 0.611156.
Test: 2018-08-09T11:24:00.720915: step 8970, loss 0.550131.
Train: 2018-08-09T11:24:00.799010: step 8971, loss 0.61107.
Train: 2018-08-09T11:24:00.892739: step 8972, loss 0.578868.
Train: 2018-08-09T11:24:00.970876: step 8973, loss 0.674878.
Train: 2018-08-09T11:24:01.048982: step 8974, loss 0.499206.
Train: 2018-08-09T11:24:01.127057: step 8975, loss 0.562973.
Train: 2018-08-09T11:24:01.205165: step 8976, loss 0.531322.
Train: 2018-08-09T11:24:01.283302: step 8977, loss 0.483936.
Train: 2018-08-09T11:24:01.361378: step 8978, loss 0.563038.
Train: 2018-08-09T11:24:01.455139: step 8979, loss 0.563035.
Train: 2018-08-09T11:24:01.533242: step 8980, loss 0.5472.
Test: 2018-08-09T11:24:02.034495: step 8980, loss 0.546979.
Train: 2018-08-09T11:24:02.112606: step 8981, loss 0.499657.
Train: 2018-08-09T11:24:02.190714: step 8982, loss 0.562991.
Train: 2018-08-09T11:24:02.268824: step 8983, loss 0.531166.
Train: 2018-08-09T11:24:02.346927: step 8984, loss 0.49919.
Train: 2018-08-09T11:24:02.440655: step 8985, loss 0.403007.
Train: 2018-08-09T11:24:02.518762: step 8986, loss 0.578842.
Train: 2018-08-09T11:24:02.596867: step 8987, loss 0.611163.
Train: 2018-08-09T11:24:02.674978: step 8988, loss 0.497552.
Train: 2018-08-09T11:24:02.753081: step 8989, loss 0.611343.
Train: 2018-08-09T11:24:02.831187: step 8990, loss 0.645194.
Test: 2018-08-09T11:24:03.331040: step 8990, loss 0.548956.
Train: 2018-08-09T11:24:03.424768: step 8991, loss 0.545749.
Train: 2018-08-09T11:24:03.502904: step 8992, loss 0.579729.
Train: 2018-08-09T11:24:03.581011: step 8993, loss 0.562676.
Train: 2018-08-09T11:24:03.659087: step 8994, loss 0.546721.
Train: 2018-08-09T11:24:03.737224: step 8995, loss 0.546143.
Train: 2018-08-09T11:24:03.830952: step 8996, loss 0.562593.
Train: 2018-08-09T11:24:03.911457: step 8997, loss 0.529619.
Train: 2018-08-09T11:24:03.989563: step 8998, loss 0.595476.
Train: 2018-08-09T11:24:04.067702: step 8999, loss 0.546036.
Train: 2018-08-09T11:24:04.145777: step 9000, loss 0.611997.
Test: 2018-08-09T11:24:04.645660: step 9000, loss 0.5483.
Train: 2018-08-09T11:24:05.176813: step 9001, loss 0.677978.
Train: 2018-08-09T11:24:05.254921: step 9002, loss 0.529602.
Train: 2018-08-09T11:24:05.332999: step 9003, loss 0.578984.
Train: 2018-08-09T11:24:05.411135: step 9004, loss 0.578968.
Train: 2018-08-09T11:24:05.489240: step 9005, loss 0.595346.
Train: 2018-08-09T11:24:05.582938: step 9006, loss 0.546234.
Train: 2018-08-09T11:24:05.661075: step 9007, loss 0.627925.
Train: 2018-08-09T11:24:05.739182: step 9008, loss 0.546343.
Train: 2018-08-09T11:24:05.817289: step 9009, loss 0.578912.
Train: 2018-08-09T11:24:05.912387: step 9010, loss 0.546455.
Test: 2018-08-09T11:24:06.397805: step 9010, loss 0.548745.
Train: 2018-08-09T11:24:06.491559: step 9011, loss 0.578896.
Train: 2018-08-09T11:24:06.569639: step 9012, loss 0.546546.
Train: 2018-08-09T11:24:06.647776: step 9013, loss 0.530426.
Train: 2018-08-09T11:24:06.725882: step 9014, loss 0.56274.
Train: 2018-08-09T11:24:06.803959: step 9015, loss 0.643442.
Train: 2018-08-09T11:24:06.882096: step 9016, loss 0.627216.
Train: 2018-08-09T11:24:06.978289: step 9017, loss 0.482447.
Train: 2018-08-09T11:24:07.051861: step 9018, loss 0.498584.
Train: 2018-08-09T11:24:07.129972: step 9019, loss 0.514618.
Train: 2018-08-09T11:24:07.223670: step 9020, loss 0.51455.
Test: 2018-08-09T11:24:07.723580: step 9020, loss 0.549495.
Train: 2018-08-09T11:24:07.801659: step 9021, loss 0.611114.
Train: 2018-08-09T11:24:07.879794: step 9022, loss 0.514359.
Train: 2018-08-09T11:24:07.961194: step 9023, loss 0.465786.
Train: 2018-08-09T11:24:08.054922: step 9024, loss 0.530268.
Train: 2018-08-09T11:24:08.133028: step 9025, loss 0.530112.
Train: 2018-08-09T11:24:08.211105: step 9026, loss 0.464644.
Train: 2018-08-09T11:24:08.289242: step 9027, loss 0.513345.
Train: 2018-08-09T11:24:08.367349: step 9028, loss 0.611995.
Train: 2018-08-09T11:24:08.445459: step 9029, loss 0.578987.
Train: 2018-08-09T11:24:08.523561: step 9030, loss 0.615601.
Test: 2018-08-09T11:24:09.023415: step 9030, loss 0.550622.
Train: 2018-08-09T11:24:09.101544: step 9031, loss 0.59572.
Train: 2018-08-09T11:24:09.179656: step 9032, loss 0.562441.
Train: 2018-08-09T11:24:09.273385: step 9033, loss 0.51247.
Train: 2018-08-09T11:24:09.351494: step 9034, loss 0.612458.
Train: 2018-08-09T11:24:09.429598: step 9035, loss 0.562429.
Train: 2018-08-09T11:24:09.507674: step 9036, loss 0.545686.
Train: 2018-08-09T11:24:09.585811: step 9037, loss 0.545691.
Train: 2018-08-09T11:24:09.679510: step 9038, loss 0.495458.
Train: 2018-08-09T11:24:09.757648: step 9039, loss 0.528852.
Train: 2018-08-09T11:24:09.835752: step 9040, loss 0.528802.
Test: 2018-08-09T11:24:10.337963: step 9040, loss 0.551696.
Train: 2018-08-09T11:24:10.416099: step 9041, loss 0.545622.
Train: 2018-08-09T11:24:10.494206: step 9042, loss 0.629751.
Train: 2018-08-09T11:24:10.572311: step 9043, loss 0.528516.
Train: 2018-08-09T11:24:10.648376: step 9044, loss 0.579228.
Train: 2018-08-09T11:24:10.742128: step 9045, loss 0.477823.
Train: 2018-08-09T11:24:10.820240: step 9046, loss 0.613453.
Train: 2018-08-09T11:24:10.898316: step 9047, loss 0.545465.
Train: 2018-08-09T11:24:10.976453: step 9048, loss 0.579477.
Train: 2018-08-09T11:24:11.054562: step 9049, loss 0.528595.
Train: 2018-08-09T11:24:11.132635: step 9050, loss 0.748879.
Test: 2018-08-09T11:24:11.632519: step 9050, loss 0.547798.
Train: 2018-08-09T11:24:11.726269: step 9051, loss 0.511692.
Train: 2018-08-09T11:24:11.804384: step 9052, loss 0.579229.
Train: 2018-08-09T11:24:11.882491: step 9053, loss 0.478226.
Train: 2018-08-09T11:24:11.965131: step 9054, loss 0.579206.
Train: 2018-08-09T11:24:12.043238: step 9055, loss 0.528773.
Train: 2018-08-09T11:24:12.121344: step 9056, loss 0.612788.
Train: 2018-08-09T11:24:12.199451: step 9057, loss 0.612733.
Train: 2018-08-09T11:24:12.277558: step 9058, loss 0.528908.
Train: 2018-08-09T11:24:12.355660: step 9059, loss 0.612585.
Train: 2018-08-09T11:24:12.449392: step 9060, loss 0.695941.
Test: 2018-08-09T11:24:12.944220: step 9060, loss 0.550018.
Train: 2018-08-09T11:24:13.022360: step 9061, loss 0.545828.
Train: 2018-08-09T11:24:13.100464: step 9062, loss 0.512784.
Train: 2018-08-09T11:24:13.178541: step 9063, loss 0.579018.
Train: 2018-08-09T11:24:13.256677: step 9064, loss 0.611963.
Train: 2018-08-09T11:24:13.334755: step 9065, loss 0.546112.
Train: 2018-08-09T11:24:13.428512: step 9066, loss 0.497009.
Train: 2018-08-09T11:24:13.506588: step 9067, loss 0.497103.
Train: 2018-08-09T11:24:13.584724: step 9068, loss 0.546212.
Train: 2018-08-09T11:24:13.662833: step 9069, loss 0.644439.
Train: 2018-08-09T11:24:13.740907: step 9070, loss 0.480827.
Test: 2018-08-09T11:24:14.247060: step 9070, loss 0.550973.
Train: 2018-08-09T11:24:14.325196: step 9071, loss 0.464444.
Train: 2018-08-09T11:24:14.403273: step 9072, loss 0.562569.
Train: 2018-08-09T11:24:14.481410: step 9073, loss 0.562552.
Train: 2018-08-09T11:24:14.559486: step 9074, loss 0.611856.
Train: 2018-08-09T11:24:14.637592: step 9075, loss 0.529641.
Train: 2018-08-09T11:24:14.731352: step 9076, loss 0.54606.
Train: 2018-08-09T11:24:14.809456: step 9077, loss 0.529553.
Train: 2018-08-09T11:24:14.887563: step 9078, loss 0.51297.
Train: 2018-08-09T11:24:14.965641: step 9079, loss 0.512865.
Train: 2018-08-09T11:24:15.043777: step 9080, loss 0.678659.
Test: 2018-08-09T11:24:15.543629: step 9080, loss 0.550046.
Train: 2018-08-09T11:24:15.621765: step 9081, loss 0.529273.
Train: 2018-08-09T11:24:15.699872: step 9082, loss 0.612264.
Train: 2018-08-09T11:24:15.777986: step 9083, loss 0.612255.
Train: 2018-08-09T11:24:15.871706: step 9084, loss 0.545863.
Train: 2018-08-09T11:24:15.949812: step 9085, loss 0.512733.
Train: 2018-08-09T11:24:16.027904: step 9086, loss 0.562466.
Train: 2018-08-09T11:24:16.106025: step 9087, loss 0.496114.
Train: 2018-08-09T11:24:16.184133: step 9088, loss 0.529243.
Train: 2018-08-09T11:24:16.277863: step 9089, loss 0.479213.
Train: 2018-08-09T11:24:16.355967: step 9090, loss 0.495562.
Test: 2018-08-09T11:24:16.855830: step 9090, loss 0.549803.
Train: 2018-08-09T11:24:16.936372: step 9091, loss 0.528836.
Train: 2018-08-09T11:24:17.014448: step 9092, loss 0.579602.
Train: 2018-08-09T11:24:17.092557: step 9093, loss 0.494561.
Train: 2018-08-09T11:24:17.170663: step 9094, loss 0.54501.
Train: 2018-08-09T11:24:17.264416: step 9095, loss 0.648985.
Train: 2018-08-09T11:24:17.342528: step 9096, loss 0.474674.
Train: 2018-08-09T11:24:17.420633: step 9097, loss 0.579235.
Train: 2018-08-09T11:24:17.498740: step 9098, loss 0.563736.
Train: 2018-08-09T11:24:17.576848: step 9099, loss 0.562594.
Train: 2018-08-09T11:24:17.654953: step 9100, loss 0.630708.
Test: 2018-08-09T11:24:18.154830: step 9100, loss 0.548722.
Train: 2018-08-09T11:24:18.701552: step 9101, loss 0.56144.
Train: 2018-08-09T11:24:18.779689: step 9102, loss 0.459703.
Train: 2018-08-09T11:24:18.857765: step 9103, loss 0.561763.
Train: 2018-08-09T11:24:18.936573: step 9104, loss 0.578922.
Train: 2018-08-09T11:24:19.014685: step 9105, loss 0.665855.
Train: 2018-08-09T11:24:19.092790: step 9106, loss 0.614319.
Train: 2018-08-09T11:24:19.186489: step 9107, loss 0.631127.
Train: 2018-08-09T11:24:19.264596: step 9108, loss 0.563091.
Train: 2018-08-09T11:24:19.342734: step 9109, loss 0.494586.
Train: 2018-08-09T11:24:19.420839: step 9110, loss 0.461223.
Test: 2018-08-09T11:24:19.922183: step 9110, loss 0.547895.
Train: 2018-08-09T11:24:19.995760: step 9111, loss 0.562382.
Train: 2018-08-09T11:24:20.073869: step 9112, loss 0.528822.
Train: 2018-08-09T11:24:20.151970: step 9113, loss 0.663049.
Train: 2018-08-09T11:24:20.245672: step 9114, loss 0.545598.
Train: 2018-08-09T11:24:20.323807: step 9115, loss 0.629287.
Train: 2018-08-09T11:24:20.401916: step 9116, loss 0.479031.
Train: 2018-08-09T11:24:20.480004: step 9117, loss 0.529123.
Train: 2018-08-09T11:24:20.558127: step 9118, loss 0.529134.
Train: 2018-08-09T11:24:20.636230: step 9119, loss 0.545785.
Train: 2018-08-09T11:24:20.729962: step 9120, loss 0.562437.
Test: 2018-08-09T11:24:21.214195: step 9120, loss 0.548084.
Train: 2018-08-09T11:24:21.307921: step 9121, loss 0.562434.
Train: 2018-08-09T11:24:21.386057: step 9122, loss 0.679061.
Train: 2018-08-09T11:24:21.464135: step 9123, loss 0.412777.
Train: 2018-08-09T11:24:21.542240: step 9124, loss 0.495879.
Train: 2018-08-09T11:24:21.620377: step 9125, loss 0.612432.
Train: 2018-08-09T11:24:21.698484: step 9126, loss 0.512402.
Train: 2018-08-09T11:24:21.792182: step 9127, loss 0.495641.
Train: 2018-08-09T11:24:21.870289: step 9128, loss 0.528953.
Train: 2018-08-09T11:24:21.948425: step 9129, loss 0.629466.
Train: 2018-08-09T11:24:22.026501: step 9130, loss 0.512042.
Test: 2018-08-09T11:24:22.526384: step 9130, loss 0.550448.
Train: 2018-08-09T11:24:22.604520: step 9131, loss 0.528764.
Train: 2018-08-09T11:24:22.698218: step 9132, loss 0.494996.
Train: 2018-08-09T11:24:22.776355: step 9133, loss 0.545469.
Train: 2018-08-09T11:24:22.854462: step 9134, loss 0.562404.
Train: 2018-08-09T11:24:22.932568: step 9135, loss 0.528398.
Train: 2018-08-09T11:24:23.010646: step 9136, loss 0.596266.
Train: 2018-08-09T11:24:23.088783: step 9137, loss 0.545488.
Train: 2018-08-09T11:24:23.182509: step 9138, loss 0.545214.
Train: 2018-08-09T11:24:23.260611: step 9139, loss 0.493813.
Train: 2018-08-09T11:24:23.338725: step 9140, loss 0.613476.
Test: 2018-08-09T11:24:23.838575: step 9140, loss 0.551409.
Train: 2018-08-09T11:24:23.918178: step 9141, loss 0.597411.
Train: 2018-08-09T11:24:23.996315: step 9142, loss 0.579344.
Train: 2018-08-09T11:24:24.074393: step 9143, loss 0.579333.
Train: 2018-08-09T11:24:24.152529: step 9144, loss 0.630534.
Train: 2018-08-09T11:24:24.230635: step 9145, loss 0.476903.
Train: 2018-08-09T11:24:24.324366: step 9146, loss 0.597087.
Train: 2018-08-09T11:24:24.402469: step 9147, loss 0.613421.
Train: 2018-08-09T11:24:24.480576: step 9148, loss 0.647809.
Train: 2018-08-09T11:24:24.558654: step 9149, loss 0.664239.
Train: 2018-08-09T11:24:24.636784: step 9150, loss 0.596028.
Test: 2018-08-09T11:24:25.136667: step 9150, loss 0.5467.
Train: 2018-08-09T11:24:25.214749: step 9151, loss 0.528888.
Train: 2018-08-09T11:24:25.292854: step 9152, loss 0.529047.
Train: 2018-08-09T11:24:25.386613: step 9153, loss 0.562444.
Train: 2018-08-09T11:24:25.464720: step 9154, loss 0.562465.
Train: 2018-08-09T11:24:25.542795: step 9155, loss 0.579026.
Train: 2018-08-09T11:24:25.620935: step 9156, loss 0.71095.
Train: 2018-08-09T11:24:25.699009: step 9157, loss 0.562555.
Train: 2018-08-09T11:24:25.792738: step 9158, loss 0.529927.
Train: 2018-08-09T11:24:25.870878: step 9159, loss 0.497524.
Train: 2018-08-09T11:24:25.951266: step 9160, loss 0.578909.
Test: 2018-08-09T11:24:26.435528: step 9160, loss 0.550567.
Train: 2018-08-09T11:24:26.529250: step 9161, loss 0.530271.
Train: 2018-08-09T11:24:26.607365: step 9162, loss 0.562707.
Train: 2018-08-09T11:24:26.685463: step 9163, loss 0.481885.
Train: 2018-08-09T11:24:26.763575: step 9164, loss 0.498029.
Train: 2018-08-09T11:24:26.841652: step 9165, loss 0.659874.
Train: 2018-08-09T11:24:26.919760: step 9166, loss 0.692223.
Train: 2018-08-09T11:24:27.013487: step 9167, loss 0.562738.
Train: 2018-08-09T11:24:27.091593: step 9168, loss 0.627218.
Train: 2018-08-09T11:24:27.169701: step 9169, loss 0.546742.
Train: 2018-08-09T11:24:27.247836: step 9170, loss 0.610917.
Test: 2018-08-09T11:24:27.747719: step 9170, loss 0.549724.
Train: 2018-08-09T11:24:27.825826: step 9171, loss 0.610818.
Train: 2018-08-09T11:24:27.903932: step 9172, loss 0.594783.
Train: 2018-08-09T11:24:27.984392: step 9173, loss 0.594726.
Train: 2018-08-09T11:24:28.062465: step 9174, loss 0.563051.
Train: 2018-08-09T11:24:28.156222: step 9175, loss 0.626141.
Train: 2018-08-09T11:24:28.234332: step 9176, loss 0.610269.
Train: 2018-08-09T11:24:28.312406: step 9177, loss 0.516329.
Train: 2018-08-09T11:24:28.390542: step 9178, loss 0.500874.
Train: 2018-08-09T11:24:28.484272: step 9179, loss 0.532133.
Train: 2018-08-09T11:24:28.562347: step 9180, loss 0.578907.
Test: 2018-08-09T11:24:29.046610: step 9180, loss 0.548627.
Train: 2018-08-09T11:24:29.140366: step 9181, loss 0.515966.
Train: 2018-08-09T11:24:29.218443: step 9182, loss 0.531994.
Train: 2018-08-09T11:24:29.296579: step 9183, loss 0.499871.
Train: 2018-08-09T11:24:29.374655: step 9184, loss 0.53053.
Train: 2018-08-09T11:24:29.452792: step 9185, loss 0.544854.
Train: 2018-08-09T11:24:29.530896: step 9186, loss 0.578484.
Train: 2018-08-09T11:24:29.624596: step 9187, loss 0.681232.
Train: 2018-08-09T11:24:29.702702: step 9188, loss 0.51409.
Train: 2018-08-09T11:24:29.780844: step 9189, loss 0.619852.
Train: 2018-08-09T11:24:29.858917: step 9190, loss 0.558946.
Test: 2018-08-09T11:24:30.359431: step 9190, loss 0.545695.
Train: 2018-08-09T11:24:30.437550: step 9191, loss 0.429981.
Train: 2018-08-09T11:24:30.515625: step 9192, loss 0.533807.
Train: 2018-08-09T11:24:30.593733: step 9193, loss 0.630694.
Train: 2018-08-09T11:24:30.671869: step 9194, loss 0.493885.
Train: 2018-08-09T11:24:30.765597: step 9195, loss 0.619248.
Train: 2018-08-09T11:24:30.843704: step 9196, loss 0.611603.
Train: 2018-08-09T11:24:30.921811: step 9197, loss 0.526272.
Train: 2018-08-09T11:24:30.999917: step 9198, loss 0.722048.
Train: 2018-08-09T11:24:31.078024: step 9199, loss 0.696367.
Train: 2018-08-09T11:24:31.156127: step 9200, loss 0.610253.
Test: 2018-08-09T11:24:31.655983: step 9200, loss 0.548022.
Train: 2018-08-09T11:24:32.218380: step 9201, loss 0.561119.
Train: 2018-08-09T11:24:32.312108: step 9202, loss 0.563662.
Train: 2018-08-09T11:24:32.390215: step 9203, loss 0.610352.
Train: 2018-08-09T11:24:32.468291: step 9204, loss 0.486526.
Train: 2018-08-09T11:24:32.546428: step 9205, loss 0.501887.
Train: 2018-08-09T11:24:32.624536: step 9206, loss 0.625006.
Train: 2018-08-09T11:24:32.702641: step 9207, loss 0.54845.
Train: 2018-08-09T11:24:32.796368: step 9208, loss 0.639903.
Train: 2018-08-09T11:24:32.874447: step 9209, loss 0.563894.
Train: 2018-08-09T11:24:32.954937: step 9210, loss 0.579075.
Test: 2018-08-09T11:24:33.454793: step 9210, loss 0.552065.
Train: 2018-08-09T11:24:33.532897: step 9211, loss 0.579089.
Train: 2018-08-09T11:24:33.611005: step 9212, loss 0.563989.
Train: 2018-08-09T11:24:33.689144: step 9213, loss 0.533815.
Train: 2018-08-09T11:24:33.767249: step 9214, loss 0.564011.
Train: 2018-08-09T11:24:33.845324: step 9215, loss 0.669735.
Train: 2018-08-09T11:24:33.939084: step 9216, loss 0.564038.
Train: 2018-08-09T11:24:34.017160: step 9217, loss 0.548995.
Train: 2018-08-09T11:24:34.095296: step 9218, loss 0.56407.
Train: 2018-08-09T11:24:34.173374: step 9219, loss 0.533942.
Train: 2018-08-09T11:24:34.251480: step 9220, loss 0.503733.
Test: 2018-08-09T11:24:34.751361: step 9220, loss 0.549827.
Train: 2018-08-09T11:24:34.829497: step 9221, loss 0.548878.
Train: 2018-08-09T11:24:34.909891: step 9222, loss 0.579073.
Train: 2018-08-09T11:24:34.987998: step 9223, loss 0.472711.
Train: 2018-08-09T11:24:35.081725: step 9224, loss 0.563704.
Train: 2018-08-09T11:24:35.159863: step 9225, loss 0.486955.
Train: 2018-08-09T11:24:35.237969: step 9226, loss 0.594576.
Train: 2018-08-09T11:24:35.316071: step 9227, loss 0.486047.
Train: 2018-08-09T11:24:35.394182: step 9228, loss 0.563479.
Train: 2018-08-09T11:24:35.487911: step 9229, loss 0.594277.
Train: 2018-08-09T11:24:35.565987: step 9230, loss 0.484047.
Test: 2018-08-09T11:24:36.065869: step 9230, loss 0.548468.
Train: 2018-08-09T11:24:36.144000: step 9231, loss 0.642564.
Train: 2018-08-09T11:24:36.222112: step 9232, loss 0.464953.
Train: 2018-08-09T11:24:36.300221: step 9233, loss 0.678517.
Train: 2018-08-09T11:24:36.378295: step 9234, loss 0.626496.
Train: 2018-08-09T11:24:36.456402: step 9235, loss 0.694208.
Train: 2018-08-09T11:24:36.534535: step 9236, loss 0.532022.
Train: 2018-08-09T11:24:36.628236: step 9237, loss 0.563848.
Train: 2018-08-09T11:24:36.706373: step 9238, loss 0.578729.
Train: 2018-08-09T11:24:36.784481: step 9239, loss 0.579103.
Train: 2018-08-09T11:24:36.862586: step 9240, loss 0.658136.
Test: 2018-08-09T11:24:37.364771: step 9240, loss 0.548761.
Train: 2018-08-09T11:24:37.442880: step 9241, loss 0.562993.
Train: 2018-08-09T11:24:37.520958: step 9242, loss 0.626286.
Train: 2018-08-09T11:24:37.614684: step 9243, loss 0.594626.
Train: 2018-08-09T11:24:37.692792: step 9244, loss 0.453146.
Train: 2018-08-09T11:24:37.770898: step 9245, loss 0.468928.
Train: 2018-08-09T11:24:37.849038: step 9246, loss 0.578872.
Train: 2018-08-09T11:24:37.927111: step 9247, loss 0.468668.
Train: 2018-08-09T11:24:38.020869: step 9248, loss 0.499935.
Train: 2018-08-09T11:24:38.098977: step 9249, loss 0.515481.
Train: 2018-08-09T11:24:38.177082: step 9250, loss 0.594769.
Test: 2018-08-09T11:24:38.681090: step 9250, loss 0.547938.
Train: 2018-08-09T11:24:38.759689: step 9251, loss 0.546935.
Train: 2018-08-09T11:24:38.837766: step 9252, loss 0.610905.
Train: 2018-08-09T11:24:38.915899: step 9253, loss 0.530714.
Train: 2018-08-09T11:24:38.994008: step 9254, loss 0.659363.
Train: 2018-08-09T11:24:39.072115: step 9255, loss 0.450014.
Train: 2018-08-09T11:24:39.150224: step 9256, loss 0.51428.
Train: 2018-08-09T11:24:39.243944: step 9257, loss 0.481664.
Train: 2018-08-09T11:24:39.322027: step 9258, loss 0.530085.
Train: 2018-08-09T11:24:39.400140: step 9259, loss 0.578912.
Train: 2018-08-09T11:24:39.478272: step 9260, loss 0.562575.
Test: 2018-08-09T11:24:39.978123: step 9260, loss 0.549553.
Train: 2018-08-09T11:24:40.056253: step 9261, loss 0.595353.
Train: 2018-08-09T11:24:40.134365: step 9262, loss 0.579021.
Train: 2018-08-09T11:24:40.228064: step 9263, loss 0.52904.
Train: 2018-08-09T11:24:40.306199: step 9264, loss 0.57911.
Train: 2018-08-09T11:24:40.384306: step 9265, loss 0.563089.
Train: 2018-08-09T11:24:40.462412: step 9266, loss 0.529123.
Train: 2018-08-09T11:24:40.540489: step 9267, loss 0.529379.
Train: 2018-08-09T11:24:40.618596: step 9268, loss 0.511828.
Train: 2018-08-09T11:24:40.712355: step 9269, loss 0.562063.
Train: 2018-08-09T11:24:40.790430: step 9270, loss 0.511472.
Test: 2018-08-09T11:24:41.277203: step 9270, loss 0.54958.
Train: 2018-08-09T11:24:41.355339: step 9271, loss 0.425567.
Train: 2018-08-09T11:24:41.449040: step 9272, loss 0.543525.
Train: 2018-08-09T11:24:41.527168: step 9273, loss 0.652683.
Train: 2018-08-09T11:24:41.605281: step 9274, loss 0.528013.
Train: 2018-08-09T11:24:41.683387: step 9275, loss 0.61641.
Train: 2018-08-09T11:24:41.761488: step 9276, loss 0.616077.
Train: 2018-08-09T11:24:41.839601: step 9277, loss 0.544889.
Train: 2018-08-09T11:24:41.933328: step 9278, loss 0.543976.
Train: 2018-08-09T11:24:42.011405: step 9279, loss 0.563055.
Train: 2018-08-09T11:24:42.097187: step 9280, loss 0.614418.
Test: 2018-08-09T11:24:42.597075: step 9280, loss 0.550858.
Train: 2018-08-09T11:24:42.675176: step 9281, loss 0.511599.
Train: 2018-08-09T11:24:42.753312: step 9282, loss 0.579345.
Train: 2018-08-09T11:24:42.831389: step 9283, loss 0.579516.
Train: 2018-08-09T11:24:42.918758: step 9284, loss 0.596134.
Train: 2018-08-09T11:24:42.996895: step 9285, loss 0.478207.
Train: 2018-08-09T11:24:43.074999: step 9286, loss 0.545534.
Train: 2018-08-09T11:24:43.153078: step 9287, loss 0.562288.
Train: 2018-08-09T11:24:43.231215: step 9288, loss 0.663349.
Train: 2018-08-09T11:24:43.324913: step 9289, loss 0.596111.
Train: 2018-08-09T11:24:43.403050: step 9290, loss 0.495579.
Test: 2018-08-09T11:24:43.887305: step 9290, loss 0.550546.
Train: 2018-08-09T11:24:43.981040: step 9291, loss 0.445467.
Train: 2018-08-09T11:24:44.059145: step 9292, loss 0.57908.
Train: 2018-08-09T11:24:44.137255: step 9293, loss 0.612488.
Train: 2018-08-09T11:24:44.215358: step 9294, loss 0.528984.
Train: 2018-08-09T11:24:44.293436: step 9295, loss 0.612648.
Train: 2018-08-09T11:24:44.377293: step 9296, loss 0.495623.
Train: 2018-08-09T11:24:44.455372: step 9297, loss 0.529035.
Train: 2018-08-09T11:24:44.549128: step 9298, loss 0.645932.
Train: 2018-08-09T11:24:44.627206: step 9299, loss 0.57918.
Train: 2018-08-09T11:24:44.705312: step 9300, loss 0.579072.
Test: 2018-08-09T11:24:45.205195: step 9300, loss 0.548118.
Train: 2018-08-09T11:24:45.736319: step 9301, loss 0.579099.
Train: 2018-08-09T11:24:45.814451: step 9302, loss 0.56247.
Train: 2018-08-09T11:24:45.910471: step 9303, loss 0.512755.
Train: 2018-08-09T11:24:45.981983: step 9304, loss 0.545908.
Train: 2018-08-09T11:24:46.060119: step 9305, loss 0.5956.
Train: 2018-08-09T11:24:46.138197: step 9306, loss 0.562484.
Train: 2018-08-09T11:24:46.231955: step 9307, loss 0.595544.
Train: 2018-08-09T11:24:46.310031: step 9308, loss 0.57901.
Train: 2018-08-09T11:24:46.388171: step 9309, loss 0.61196.
Train: 2018-08-09T11:24:46.466274: step 9310, loss 0.69399.
Test: 2018-08-09T11:24:46.966127: step 9310, loss 0.547884.
Train: 2018-08-09T11:24:47.044234: step 9311, loss 0.529916.
Train: 2018-08-09T11:24:47.122339: step 9312, loss 0.562683.
Train: 2018-08-09T11:24:47.216106: step 9313, loss 0.51386.
Train: 2018-08-09T11:24:47.294204: step 9314, loss 0.660099.
Train: 2018-08-09T11:24:47.372311: step 9315, loss 0.562708.
Train: 2018-08-09T11:24:47.450414: step 9316, loss 0.562738.
Train: 2018-08-09T11:24:47.528518: step 9317, loss 0.546669.
Train: 2018-08-09T11:24:47.622252: step 9318, loss 0.49852.
Train: 2018-08-09T11:24:47.700362: step 9319, loss 0.659172.
Train: 2018-08-09T11:24:47.778467: step 9320, loss 0.514731.
Test: 2018-08-09T11:24:48.278326: step 9320, loss 0.548448.
Train: 2018-08-09T11:24:48.356454: step 9321, loss 0.498733.
Train: 2018-08-09T11:24:48.434530: step 9322, loss 0.562985.
Train: 2018-08-09T11:24:48.512666: step 9323, loss 0.562829.
Train: 2018-08-09T11:24:48.590744: step 9324, loss 0.579051.
Train: 2018-08-09T11:24:48.668850: step 9325, loss 0.562898.
Train: 2018-08-09T11:24:48.762612: step 9326, loss 0.418347.
Train: 2018-08-09T11:24:48.840714: step 9327, loss 0.643289.
Train: 2018-08-09T11:24:48.920240: step 9328, loss 0.578885.
Train: 2018-08-09T11:24:48.998380: step 9329, loss 0.611232.
Train: 2018-08-09T11:24:49.076479: step 9330, loss 0.611177.
Test: 2018-08-09T11:24:49.576337: step 9330, loss 0.5471.
Train: 2018-08-09T11:24:49.654444: step 9331, loss 0.545546.
Train: 2018-08-09T11:24:49.748170: step 9332, loss 0.546695.
Train: 2018-08-09T11:24:49.826310: step 9333, loss 0.514571.
Train: 2018-08-09T11:24:49.904417: step 9334, loss 0.643244.
Train: 2018-08-09T11:24:49.982520: step 9335, loss 0.530673.
Train: 2018-08-09T11:24:50.060624: step 9336, loss 0.498519.
Train: 2018-08-09T11:24:50.138736: step 9337, loss 0.57884.
Train: 2018-08-09T11:24:50.232432: step 9338, loss 0.562763.
Train: 2018-08-09T11:24:50.310569: step 9339, loss 0.611195.
Train: 2018-08-09T11:24:50.388674: step 9340, loss 0.546659.
Test: 2018-08-09T11:24:50.888527: step 9340, loss 0.548251.
Train: 2018-08-09T11:24:50.967405: step 9341, loss 0.482126.
Train: 2018-08-09T11:24:51.045512: step 9342, loss 0.611138.
Train: 2018-08-09T11:24:51.123615: step 9343, loss 0.562629.
Train: 2018-08-09T11:24:51.201696: step 9344, loss 0.56267.
Train: 2018-08-09T11:24:51.279832: step 9345, loss 0.595159.
Train: 2018-08-09T11:24:51.373531: step 9346, loss 0.497937.
Train: 2018-08-09T11:24:51.453278: step 9347, loss 0.530049.
Train: 2018-08-09T11:24:51.531385: step 9348, loss 0.562774.
Train: 2018-08-09T11:24:51.609523: step 9349, loss 0.497056.
Train: 2018-08-09T11:24:51.699612: step 9350, loss 0.579359.
Test: 2018-08-09T11:24:52.199496: step 9350, loss 0.548981.
Train: 2018-08-09T11:24:52.277625: step 9351, loss 0.611805.
Train: 2018-08-09T11:24:52.355707: step 9352, loss 0.661109.
Train: 2018-08-09T11:24:52.433845: step 9353, loss 0.529242.
Train: 2018-08-09T11:24:52.511949: step 9354, loss 0.644525.
Train: 2018-08-09T11:24:52.590056: step 9355, loss 0.645506.
Train: 2018-08-09T11:24:52.683786: step 9356, loss 0.46499.
Train: 2018-08-09T11:24:52.761861: step 9357, loss 0.562545.
Train: 2018-08-09T11:24:52.839966: step 9358, loss 0.611339.
Train: 2018-08-09T11:24:52.918075: step 9359, loss 0.481827.
Train: 2018-08-09T11:24:52.996211: step 9360, loss 0.56272.
Test: 2018-08-09T11:24:53.496095: step 9360, loss 0.548172.
Train: 2018-08-09T11:24:53.574199: step 9361, loss 0.481837.
Train: 2018-08-09T11:24:53.652275: step 9362, loss 0.659887.
Train: 2018-08-09T11:24:53.746005: step 9363, loss 0.643622.
Train: 2018-08-09T11:24:53.824140: step 9364, loss 0.57889.
Train: 2018-08-09T11:24:53.902247: step 9365, loss 0.401484.
Train: 2018-08-09T11:24:53.982540: step 9366, loss 0.546591.
Train: 2018-08-09T11:24:54.060647: step 9367, loss 0.530382.
Train: 2018-08-09T11:24:54.138783: step 9368, loss 0.546498.
Train: 2018-08-09T11:24:54.232482: step 9369, loss 0.497753.
Train: 2018-08-09T11:24:54.310618: step 9370, loss 0.497531.
Test: 2018-08-09T11:24:54.794850: step 9370, loss 0.548529.
Train: 2018-08-09T11:24:54.888576: step 9371, loss 0.611618.
Train: 2018-08-09T11:24:54.966713: step 9372, loss 0.611703.
Train: 2018-08-09T11:24:55.044791: step 9373, loss 0.578966.
Train: 2018-08-09T11:24:55.122930: step 9374, loss 0.59538.
Train: 2018-08-09T11:24:55.201027: step 9375, loss 0.628233.
Train: 2018-08-09T11:24:55.294733: step 9376, loss 0.46413.
Train: 2018-08-09T11:24:55.372871: step 9377, loss 0.546131.
Train: 2018-08-09T11:24:55.450975: step 9378, loss 0.694026.
Train: 2018-08-09T11:24:55.529084: step 9379, loss 0.611793.
Train: 2018-08-09T11:24:55.607191: step 9380, loss 0.513406.
Test: 2018-08-09T11:24:56.109433: step 9380, loss 0.548475.
Train: 2018-08-09T11:24:56.203162: step 9381, loss 0.562593.
Train: 2018-08-09T11:24:56.281297: step 9382, loss 0.562522.
Train: 2018-08-09T11:24:56.359375: step 9383, loss 0.513625.
Train: 2018-08-09T11:24:56.437510: step 9384, loss 0.546125.
Train: 2018-08-09T11:24:56.515618: step 9385, loss 0.513528.
Train: 2018-08-09T11:24:56.609341: step 9386, loss 0.676982.
Train: 2018-08-09T11:24:56.687451: step 9387, loss 0.546031.
Train: 2018-08-09T11:24:56.765558: step 9388, loss 0.595718.
Train: 2018-08-09T11:24:56.843638: step 9389, loss 0.513929.
Train: 2018-08-09T11:24:56.921766: step 9390, loss 0.496674.
Test: 2018-08-09T11:24:57.421623: step 9390, loss 0.54773.
Train: 2018-08-09T11:24:57.499760: step 9391, loss 0.644372.
Train: 2018-08-09T11:24:57.577866: step 9392, loss 0.431608.
Train: 2018-08-09T11:24:57.671595: step 9393, loss 0.579122.
Train: 2018-08-09T11:24:57.749670: step 9394, loss 0.5625.
Train: 2018-08-09T11:24:57.827777: step 9395, loss 0.579588.
Train: 2018-08-09T11:24:57.918125: step 9396, loss 0.512534.
Train: 2018-08-09T11:24:57.984716: step 9397, loss 0.529857.
Train: 2018-08-09T11:24:58.078444: step 9398, loss 0.52883.
Train: 2018-08-09T11:24:58.156551: step 9399, loss 0.529153.
Train: 2018-08-09T11:24:58.234655: step 9400, loss 0.578686.
Test: 2018-08-09T11:24:58.718955: step 9400, loss 0.548535.
Train: 2018-08-09T11:24:59.312527: step 9401, loss 0.496304.
Train: 2018-08-09T11:24:59.390634: step 9402, loss 0.546332.
Train: 2018-08-09T11:24:59.468742: step 9403, loss 0.510387.
Train: 2018-08-09T11:24:59.546848: step 9404, loss 0.633456.
Train: 2018-08-09T11:24:59.640574: step 9405, loss 0.594658.
Train: 2018-08-09T11:24:59.718713: step 9406, loss 0.477037.
Train: 2018-08-09T11:24:59.796819: step 9407, loss 0.476847.
Train: 2018-08-09T11:24:59.874937: step 9408, loss 0.561037.
Train: 2018-08-09T11:24:59.953011: step 9409, loss 0.634412.
Train: 2018-08-09T11:25:00.031139: step 9410, loss 0.510132.
Test: 2018-08-09T11:25:00.530992: step 9410, loss 0.548801.
Train: 2018-08-09T11:25:00.624744: step 9411, loss 0.441607.
Train: 2018-08-09T11:25:00.702855: step 9412, loss 0.580445.
Train: 2018-08-09T11:25:00.780957: step 9413, loss 0.561743.
Train: 2018-08-09T11:25:00.859069: step 9414, loss 0.526461.
Train: 2018-08-09T11:25:00.939489: step 9415, loss 0.548051.
Train: 2018-08-09T11:25:01.017628: step 9416, loss 0.598606.
Train: 2018-08-09T11:25:01.111323: step 9417, loss 0.686313.
Train: 2018-08-09T11:25:01.189459: step 9418, loss 0.665618.
Train: 2018-08-09T11:25:01.267536: step 9419, loss 0.664504.
Train: 2018-08-09T11:25:01.345668: step 9420, loss 0.664409.
Test: 2018-08-09T11:25:01.845524: step 9420, loss 0.549211.
Train: 2018-08-09T11:25:01.923655: step 9421, loss 0.528844.
Train: 2018-08-09T11:25:02.017358: step 9422, loss 0.579066.
Train: 2018-08-09T11:25:02.095497: step 9423, loss 0.612185.
Train: 2018-08-09T11:25:02.173603: step 9424, loss 0.578999.
Train: 2018-08-09T11:25:02.251713: step 9425, loss 0.529753.
Train: 2018-08-09T11:25:02.329812: step 9426, loss 0.627956.
Train: 2018-08-09T11:25:02.407924: step 9427, loss 0.611441.
Train: 2018-08-09T11:25:02.501619: step 9428, loss 0.530345.
Train: 2018-08-09T11:25:02.579757: step 9429, loss 0.466048.
Train: 2018-08-09T11:25:02.657863: step 9430, loss 0.546697.
Test: 2018-08-09T11:25:03.144320: step 9430, loss 0.548358.
Train: 2018-08-09T11:25:03.238080: step 9431, loss 0.514606.
Train: 2018-08-09T11:25:03.316185: step 9432, loss 0.643112.
Train: 2018-08-09T11:25:03.394263: step 9433, loss 0.594901.
Train: 2018-08-09T11:25:03.472400: step 9434, loss 0.562861.
Train: 2018-08-09T11:25:03.550505: step 9435, loss 0.530935.
Train: 2018-08-09T11:25:03.628613: step 9436, loss 0.594821.
Train: 2018-08-09T11:25:03.722309: step 9437, loss 0.674502.
Train: 2018-08-09T11:25:03.800446: step 9438, loss 0.578858.
Train: 2018-08-09T11:25:03.878554: step 9439, loss 0.515472.
Train: 2018-08-09T11:25:03.956659: step 9440, loss 0.594683.
Test: 2018-08-09T11:25:04.456532: step 9440, loss 0.550666.
Train: 2018-08-09T11:25:04.534648: step 9441, loss 0.578864.
Train: 2018-08-09T11:25:04.612756: step 9442, loss 0.578864.
Train: 2018-08-09T11:25:04.690834: step 9443, loss 0.531678.
Train: 2018-08-09T11:25:04.784559: step 9444, loss 0.531724.
Train: 2018-08-09T11:25:04.862667: step 9445, loss 0.563156.
Train: 2018-08-09T11:25:04.943224: step 9446, loss 0.594579.
Train: 2018-08-09T11:25:05.021331: step 9447, loss 0.531734.
Train: 2018-08-09T11:25:05.099405: step 9448, loss 0.641672.
Train: 2018-08-09T11:25:05.177513: step 9449, loss 0.500373.
Train: 2018-08-09T11:25:05.271272: step 9450, loss 0.579013.
Test: 2018-08-09T11:25:05.755502: step 9450, loss 0.547794.
Train: 2018-08-09T11:25:05.849230: step 9451, loss 0.484455.
Train: 2018-08-09T11:25:05.927361: step 9452, loss 0.594637.
Train: 2018-08-09T11:25:06.005442: step 9453, loss 0.515674.
Train: 2018-08-09T11:25:06.083579: step 9454, loss 0.62617.
Train: 2018-08-09T11:25:06.177308: step 9455, loss 0.499413.
Train: 2018-08-09T11:25:06.255412: step 9456, loss 0.530858.
Train: 2018-08-09T11:25:06.349111: step 9457, loss 0.563179.
Train: 2018-08-09T11:25:06.427249: step 9458, loss 0.516042.
Train: 2018-08-09T11:25:06.505355: step 9459, loss 0.465304.
Train: 2018-08-09T11:25:06.583462: step 9460, loss 0.661939.
Test: 2018-08-09T11:25:07.085649: step 9460, loss 0.550246.
Train: 2018-08-09T11:25:07.163725: step 9461, loss 0.612203.
Train: 2018-08-09T11:25:07.241831: step 9462, loss 0.644078.
Train: 2018-08-09T11:25:07.319938: step 9463, loss 0.562889.
Train: 2018-08-09T11:25:07.413666: step 9464, loss 0.562268.
Train: 2018-08-09T11:25:07.491804: step 9465, loss 0.530831.
Train: 2018-08-09T11:25:07.569880: step 9466, loss 0.610952.
Train: 2018-08-09T11:25:07.648017: step 9467, loss 0.482599.
Train: 2018-08-09T11:25:07.726123: step 9468, loss 0.514609.
Train: 2018-08-09T11:25:07.804226: step 9469, loss 0.514497.
Train: 2018-08-09T11:25:07.882338: step 9470, loss 0.594991.
Test: 2018-08-09T11:25:08.397810: step 9470, loss 0.550593.
Train: 2018-08-09T11:25:08.475946: step 9471, loss 0.676052.
Train: 2018-08-09T11:25:08.554053: step 9472, loss 0.530411.
Train: 2018-08-09T11:25:08.632130: step 9473, loss 0.595302.
Train: 2018-08-09T11:25:08.710266: step 9474, loss 0.627483.
Train: 2018-08-09T11:25:08.788373: step 9475, loss 0.51446.
Train: 2018-08-09T11:25:08.866476: step 9476, loss 0.627162.
Train: 2018-08-09T11:25:08.946823: step 9477, loss 0.562798.
Train: 2018-08-09T11:25:09.040551: step 9478, loss 0.578867.
Train: 2018-08-09T11:25:09.118664: step 9479, loss 0.514722.
Train: 2018-08-09T11:25:09.196739: step 9480, loss 0.530774.
Test: 2018-08-09T11:25:09.696623: step 9480, loss 0.548407.
Train: 2018-08-09T11:25:09.774758: step 9481, loss 0.514705.
Train: 2018-08-09T11:25:09.852865: step 9482, loss 0.691278.
Train: 2018-08-09T11:25:09.946589: step 9483, loss 0.498646.
Train: 2018-08-09T11:25:10.024699: step 9484, loss 0.578857.
Train: 2018-08-09T11:25:10.102775: step 9485, loss 0.514671.
Train: 2018-08-09T11:25:10.180912: step 9486, loss 0.530668.
Train: 2018-08-09T11:25:10.259014: step 9487, loss 0.611053.
Train: 2018-08-09T11:25:10.337127: step 9488, loss 0.61101.
Train: 2018-08-09T11:25:10.430823: step 9489, loss 0.707736.
Train: 2018-08-09T11:25:10.508930: step 9490, loss 0.594868.
Test: 2018-08-09T11:25:11.008844: step 9490, loss 0.547258.
Train: 2018-08-09T11:25:11.086950: step 9491, loss 0.546829.
Train: 2018-08-09T11:25:11.165028: step 9492, loss 0.562825.
Train: 2018-08-09T11:25:11.243164: step 9493, loss 0.56299.
Train: 2018-08-09T11:25:11.321272: step 9494, loss 0.578927.
Train: 2018-08-09T11:25:11.414997: step 9495, loss 0.515385.
Train: 2018-08-09T11:25:11.493074: step 9496, loss 0.562931.
Train: 2018-08-09T11:25:11.571213: step 9497, loss 0.594718.
Train: 2018-08-09T11:25:11.649317: step 9498, loss 0.53127.
Train: 2018-08-09T11:25:11.727423: step 9499, loss 0.610633.
Train: 2018-08-09T11:25:11.805530: step 9500, loss 0.563076.
Test: 2018-08-09T11:25:12.305408: step 9500, loss 0.552396.
Train: 2018-08-09T11:25:12.883370: step 9501, loss 0.547248.
Train: 2018-08-09T11:25:12.963802: step 9502, loss 0.531422.
Train: 2018-08-09T11:25:13.041935: step 9503, loss 0.531387.
Train: 2018-08-09T11:25:13.120015: step 9504, loss 0.547125.
Train: 2018-08-09T11:25:13.213742: step 9505, loss 0.499375.
Train: 2018-08-09T11:25:13.291880: step 9506, loss 0.610612.
Train: 2018-08-09T11:25:13.369988: step 9507, loss 0.515263.
Train: 2018-08-09T11:25:13.448061: step 9508, loss 0.546435.
Train: 2018-08-09T11:25:13.527850: step 9509, loss 0.546313.
Train: 2018-08-09T11:25:13.605956: step 9510, loss 0.497132.
Test: 2018-08-09T11:25:14.105809: step 9510, loss 0.548274.
Train: 2018-08-09T11:25:14.183915: step 9511, loss 0.528934.
Train: 2018-08-09T11:25:14.262052: step 9512, loss 0.527239.
Train: 2018-08-09T11:25:14.355751: step 9513, loss 0.563714.
Train: 2018-08-09T11:25:14.433888: step 9514, loss 0.722721.
Train: 2018-08-09T11:25:14.511964: step 9515, loss 0.668319.
Train: 2018-08-09T11:25:14.590099: step 9516, loss 0.56338.
Train: 2018-08-09T11:25:14.668208: step 9517, loss 0.581413.
Train: 2018-08-09T11:25:14.746282: step 9518, loss 0.564665.
Train: 2018-08-09T11:25:14.824419: step 9519, loss 0.48085.
Train: 2018-08-09T11:25:14.918147: step 9520, loss 0.610895.
Test: 2018-08-09T11:25:15.402379: step 9520, loss 0.549437.
Train: 2018-08-09T11:25:15.496136: step 9521, loss 0.498412.
Train: 2018-08-09T11:25:15.574243: step 9522, loss 0.530518.
Train: 2018-08-09T11:25:15.652350: step 9523, loss 0.54666.
Train: 2018-08-09T11:25:15.730426: step 9524, loss 0.594998.
Train: 2018-08-09T11:25:15.808368: step 9525, loss 0.449883.
Train: 2018-08-09T11:25:15.886474: step 9526, loss 0.546566.
Train: 2018-08-09T11:25:15.980201: step 9527, loss 0.5303.
Train: 2018-08-09T11:25:16.058339: step 9528, loss 0.530182.
Train: 2018-08-09T11:25:16.136442: step 9529, loss 0.595212.
Train: 2018-08-09T11:25:16.214550: step 9530, loss 0.529961.
Test: 2018-08-09T11:25:16.714405: step 9530, loss 0.548489.
Train: 2018-08-09T11:25:16.792542: step 9531, loss 0.480764.
Train: 2018-08-09T11:25:16.870619: step 9532, loss 0.546129.
Train: 2018-08-09T11:25:16.950133: step 9533, loss 0.595475.
Train: 2018-08-09T11:25:17.043860: step 9534, loss 0.463388.
Train: 2018-08-09T11:25:17.121936: step 9535, loss 0.595632.
Train: 2018-08-09T11:25:17.200075: step 9536, loss 0.645581.
Train: 2018-08-09T11:25:17.278180: step 9537, loss 0.62902.
Train: 2018-08-09T11:25:17.356266: step 9538, loss 0.495869.
Train: 2018-08-09T11:25:17.450022: step 9539, loss 0.662378.
Train: 2018-08-09T11:25:17.528091: step 9540, loss 0.545799.
Test: 2018-08-09T11:25:18.012353: step 9540, loss 0.54749.
Train: 2018-08-09T11:25:18.090489: step 9541, loss 0.545812.
Train: 2018-08-09T11:25:18.168595: step 9542, loss 0.579074.
Train: 2018-08-09T11:25:18.262324: step 9543, loss 0.678763.
Train: 2018-08-09T11:25:18.340399: step 9544, loss 0.512746.
Train: 2018-08-09T11:25:18.418506: step 9545, loss 0.61213.
Train: 2018-08-09T11:25:18.496615: step 9546, loss 0.413881.
Train: 2018-08-09T11:25:18.574749: step 9547, loss 0.628582.
Train: 2018-08-09T11:25:18.668478: step 9548, loss 0.463455.
Train: 2018-08-09T11:25:18.746584: step 9549, loss 0.661622.
Train: 2018-08-09T11:25:18.824691: step 9550, loss 0.529488.
Test: 2018-08-09T11:25:19.325963: step 9550, loss 0.548909.
Train: 2018-08-09T11:25:19.404100: step 9551, loss 0.545992.
Train: 2018-08-09T11:25:19.482206: step 9552, loss 0.66156.
Train: 2018-08-09T11:25:19.560281: step 9553, loss 0.529581.
Train: 2018-08-09T11:25:19.654010: step 9554, loss 0.546044.
Train: 2018-08-09T11:25:19.732115: step 9555, loss 0.595477.
Train: 2018-08-09T11:25:19.810254: step 9556, loss 0.578975.
Train: 2018-08-09T11:25:19.888359: step 9557, loss 0.546158.
Train: 2018-08-09T11:25:19.966436: step 9558, loss 0.611724.
Train: 2018-08-09T11:25:20.044574: step 9559, loss 0.595305.
Train: 2018-08-09T11:25:20.138300: step 9560, loss 0.611581.
Test: 2018-08-09T11:25:20.638153: step 9560, loss 0.549846.
Train: 2018-08-09T11:25:20.716258: step 9561, loss 0.562649.
Train: 2018-08-09T11:25:20.794396: step 9562, loss 0.546427.
Train: 2018-08-09T11:25:20.872497: step 9563, loss 0.497846.
Train: 2018-08-09T11:25:20.951959: step 9564, loss 0.595093.
Train: 2018-08-09T11:25:21.030035: step 9565, loss 0.708349.
Train: 2018-08-09T11:25:21.108172: step 9566, loss 0.643413.
Train: 2018-08-09T11:25:21.201871: step 9567, loss 0.450408.
Train: 2018-08-09T11:25:21.280010: step 9568, loss 0.514767.
Train: 2018-08-09T11:25:21.358110: step 9569, loss 0.578859.
Train: 2018-08-09T11:25:21.436214: step 9570, loss 0.530876.
Test: 2018-08-09T11:25:21.936073: step 9570, loss 0.549703.
Train: 2018-08-09T11:25:22.014212: step 9571, loss 0.62683.
Train: 2018-08-09T11:25:22.092285: step 9572, loss 0.51499.
Train: 2018-08-09T11:25:22.170422: step 9573, loss 0.483052.
Train: 2018-08-09T11:25:22.248528: step 9574, loss 0.610849.
Train: 2018-08-09T11:25:22.342257: step 9575, loss 0.594865.
Train: 2018-08-09T11:25:22.420335: step 9576, loss 0.658843.
Train: 2018-08-09T11:25:22.498440: step 9577, loss 0.499052.
Train: 2018-08-09T11:25:22.576580: step 9578, loss 0.467173.
Train: 2018-08-09T11:25:22.670305: step 9579, loss 0.514941.
Train: 2018-08-09T11:25:22.748410: step 9580, loss 0.49879.
Test: 2018-08-09T11:25:23.236905: step 9580, loss 0.548349.
Train: 2018-08-09T11:25:23.315042: step 9581, loss 0.498509.
Train: 2018-08-09T11:25:23.393147: step 9582, loss 0.514285.
Train: 2018-08-09T11:25:23.471252: step 9583, loss 0.595121.
Train: 2018-08-09T11:25:23.564986: step 9584, loss 0.530168.
Train: 2018-08-09T11:25:23.643059: step 9585, loss 0.644832.
Train: 2018-08-09T11:25:23.721167: step 9586, loss 0.579024.
Train: 2018-08-09T11:25:23.799272: step 9587, loss 0.546194.
Train: 2018-08-09T11:25:23.893001: step 9588, loss 0.578818.
Train: 2018-08-09T11:25:23.971136: step 9589, loss 0.579254.
Train: 2018-08-09T11:25:24.049243: step 9590, loss 0.595496.
Test: 2018-08-09T11:25:24.549097: step 9590, loss 0.547154.
Train: 2018-08-09T11:25:24.627232: step 9591, loss 0.611896.
Train: 2018-08-09T11:25:24.705339: step 9592, loss 0.447783.
Train: 2018-08-09T11:25:24.783449: step 9593, loss 0.546143.
Train: 2018-08-09T11:25:24.861552: step 9594, loss 0.595405.
Train: 2018-08-09T11:25:24.955281: step 9595, loss 0.546097.
Train: 2018-08-09T11:25:25.033388: step 9596, loss 0.52962.
Train: 2018-08-09T11:25:25.111494: step 9597, loss 0.52956.
Train: 2018-08-09T11:25:25.189570: step 9598, loss 0.496493.
Train: 2018-08-09T11:25:25.267705: step 9599, loss 0.595545.
Train: 2018-08-09T11:25:25.345782: step 9600, loss 0.562515.
Test: 2018-08-09T11:25:25.845666: step 9600, loss 0.549407.
Train: 2018-08-09T11:25:26.409625: step 9601, loss 0.512675.
Train: 2018-08-09T11:25:26.503352: step 9602, loss 0.512517.
Train: 2018-08-09T11:25:26.581457: step 9603, loss 0.629366.
Train: 2018-08-09T11:25:26.659561: step 9604, loss 0.595792.
Train: 2018-08-09T11:25:26.737669: step 9605, loss 0.528867.
Train: 2018-08-09T11:25:26.815772: step 9606, loss 0.529105.
Train: 2018-08-09T11:25:26.909474: step 9607, loss 0.562384.
Train: 2018-08-09T11:25:26.987610: step 9608, loss 0.713057.
Train: 2018-08-09T11:25:27.065718: step 9609, loss 0.495482.
Train: 2018-08-09T11:25:27.143792: step 9610, loss 0.595838.
Test: 2018-08-09T11:25:27.643675: step 9610, loss 0.549936.
Train: 2018-08-09T11:25:27.721812: step 9611, loss 0.545817.
Train: 2018-08-09T11:25:27.799918: step 9612, loss 0.54578.
Train: 2018-08-09T11:25:27.878023: step 9613, loss 0.512455.
Train: 2018-08-09T11:25:27.957464: step 9614, loss 0.545748.
Train: 2018-08-09T11:25:28.035570: step 9615, loss 0.545806.
Train: 2018-08-09T11:25:28.129300: step 9616, loss 0.545725.
Train: 2018-08-09T11:25:28.207405: step 9617, loss 0.529024.
Train: 2018-08-09T11:25:28.285511: step 9618, loss 0.629309.
Train: 2018-08-09T11:25:28.363589: step 9619, loss 0.562382.
Train: 2018-08-09T11:25:28.441695: step 9620, loss 0.528956.
Test: 2018-08-09T11:25:28.941578: step 9620, loss 0.547375.
Train: 2018-08-09T11:25:29.035334: step 9621, loss 0.646037.
Train: 2018-08-09T11:25:29.113441: step 9622, loss 0.529107.
Train: 2018-08-09T11:25:29.191550: step 9623, loss 0.579104.
Train: 2018-08-09T11:25:29.269654: step 9624, loss 0.579067.
Train: 2018-08-09T11:25:29.363354: step 9625, loss 0.579043.
Train: 2018-08-09T11:25:29.441492: step 9626, loss 0.51259.
Train: 2018-08-09T11:25:29.519565: step 9627, loss 0.612384.
Train: 2018-08-09T11:25:29.597673: step 9628, loss 0.562454.
Train: 2018-08-09T11:25:29.691425: step 9629, loss 0.44654.
Train: 2018-08-09T11:25:29.769536: step 9630, loss 0.612194.
Test: 2018-08-09T11:25:30.255179: step 9630, loss 0.550078.
Train: 2018-08-09T11:25:30.333314: step 9631, loss 0.562481.
Train: 2018-08-09T11:25:30.411421: step 9632, loss 0.580189.
Train: 2018-08-09T11:25:30.505121: step 9633, loss 0.595597.
Train: 2018-08-09T11:25:30.598876: step 9634, loss 0.479831.
Train: 2018-08-09T11:25:30.676953: step 9635, loss 0.562495.
Train: 2018-08-09T11:25:30.755091: step 9636, loss 0.529417.
Train: 2018-08-09T11:25:30.833166: step 9637, loss 0.645217.
Train: 2018-08-09T11:25:30.911274: step 9638, loss 0.47982.
Train: 2018-08-09T11:25:30.989412: step 9639, loss 0.545939.
Train: 2018-08-09T11:25:31.083139: step 9640, loss 0.529372.
Test: 2018-08-09T11:25:31.582990: step 9640, loss 0.549449.
Train: 2018-08-09T11:25:31.661127: step 9641, loss 0.562478.
Train: 2018-08-09T11:25:31.739233: step 9642, loss 0.5127.
Train: 2018-08-09T11:25:31.817336: step 9643, loss 0.462764.
Train: 2018-08-09T11:25:31.895416: step 9644, loss 0.512442.
Train: 2018-08-09T11:25:31.976013: step 9645, loss 0.545687.
Train: 2018-08-09T11:25:32.069771: step 9646, loss 0.579166.
Train: 2018-08-09T11:25:32.147877: step 9647, loss 0.663273.
Train: 2018-08-09T11:25:32.225954: step 9648, loss 0.696999.
Train: 2018-08-09T11:25:32.304061: step 9649, loss 0.495206.
Train: 2018-08-09T11:25:32.382197: step 9650, loss 0.646329.
Test: 2018-08-09T11:25:32.882050: step 9650, loss 0.549237.
Train: 2018-08-09T11:25:32.960186: step 9651, loss 0.595906.
Train: 2018-08-09T11:25:33.038292: step 9652, loss 0.662694.
Train: 2018-08-09T11:25:33.132027: step 9653, loss 0.529138.
Train: 2018-08-09T11:25:33.210096: step 9654, loss 0.545858.
Train: 2018-08-09T11:25:33.288233: step 9655, loss 0.545918.
Train: 2018-08-09T11:25:33.366336: step 9656, loss 0.595543.
Train: 2018-08-09T11:25:33.444415: step 9657, loss 0.513062.
Train: 2018-08-09T11:25:33.522523: step 9658, loss 0.529604.
Train: 2018-08-09T11:25:33.600661: step 9659, loss 0.480289.
Train: 2018-08-09T11:25:33.694382: step 9660, loss 0.529613.
Test: 2018-08-09T11:25:34.180079: step 9660, loss 0.547704.
Train: 2018-08-09T11:25:34.273812: step 9661, loss 0.513089.
Train: 2018-08-09T11:25:34.351919: step 9662, loss 0.545996.
Train: 2018-08-09T11:25:34.430032: step 9663, loss 0.512884.
Train: 2018-08-09T11:25:34.508101: step 9664, loss 0.595605.
Train: 2018-08-09T11:25:34.586209: step 9665, loss 0.662048.
Train: 2018-08-09T11:25:34.664345: step 9666, loss 0.595659.
Train: 2018-08-09T11:25:34.758043: step 9667, loss 0.579043.
Train: 2018-08-09T11:25:34.836180: step 9668, loss 0.678409.
Train: 2018-08-09T11:25:34.914285: step 9669, loss 0.579011.
Train: 2018-08-09T11:25:34.992363: step 9670, loss 0.496665.
Test: 2018-08-09T11:25:35.492270: step 9670, loss 0.548383.
Train: 2018-08-09T11:25:35.570382: step 9671, loss 0.56254.
Train: 2018-08-09T11:25:35.648488: step 9672, loss 0.496909.
Train: 2018-08-09T11:25:35.742212: step 9673, loss 0.496928.
Train: 2018-08-09T11:25:35.820323: step 9674, loss 0.52972.
Train: 2018-08-09T11:25:35.898430: step 9675, loss 0.595424.
Train: 2018-08-09T11:25:35.977887: step 9676, loss 0.595436.
Train: 2018-08-09T11:25:36.055962: step 9677, loss 0.529636.
Train: 2018-08-09T11:25:36.149719: step 9678, loss 0.595446.
Train: 2018-08-09T11:25:36.227827: step 9679, loss 0.628332.
Train: 2018-08-09T11:25:36.305933: step 9680, loss 0.480397.
Test: 2018-08-09T11:25:36.805810: step 9680, loss 0.550875.
Train: 2018-08-09T11:25:36.883921: step 9681, loss 0.677562.
Train: 2018-08-09T11:25:36.962028: step 9682, loss 0.480533.
Train: 2018-08-09T11:25:37.040136: step 9683, loss 0.496956.
Train: 2018-08-09T11:25:37.118241: step 9684, loss 0.61177.
Train: 2018-08-09T11:25:37.196319: step 9685, loss 0.496852.
Train: 2018-08-09T11:25:37.290077: step 9686, loss 0.546101.
Train: 2018-08-09T11:25:37.368153: step 9687, loss 0.562538.
Train: 2018-08-09T11:25:37.446289: step 9688, loss 0.645147.
Train: 2018-08-09T11:25:37.524397: step 9689, loss 0.496614.
Train: 2018-08-09T11:25:37.602502: step 9690, loss 0.579038.
Test: 2018-08-09T11:25:38.104772: step 9690, loss 0.551422.
Train: 2018-08-09T11:25:38.229756: step 9691, loss 0.562426.
Train: 2018-08-09T11:25:38.307862: step 9692, loss 0.61195.
Train: 2018-08-09T11:25:38.401590: step 9693, loss 0.545929.
Train: 2018-08-09T11:25:38.479696: step 9694, loss 0.694184.
Train: 2018-08-09T11:25:38.557773: step 9695, loss 0.578892.
Train: 2018-08-09T11:25:38.635907: step 9696, loss 0.611671.
Train: 2018-08-09T11:25:38.714017: step 9697, loss 0.54632.
Train: 2018-08-09T11:25:38.807744: step 9698, loss 0.578867.
Train: 2018-08-09T11:25:38.885854: step 9699, loss 0.546447.
Train: 2018-08-09T11:25:38.963928: step 9700, loss 0.61143.
Test: 2018-08-09T11:25:39.463809: step 9700, loss 0.548773.
Train: 2018-08-09T11:25:40.010586: step 9701, loss 0.611237.
Train: 2018-08-09T11:25:40.104284: step 9702, loss 0.546563.
Train: 2018-08-09T11:25:40.182420: step 9703, loss 0.578882.
Train: 2018-08-09T11:25:40.260527: step 9704, loss 0.56285.
Train: 2018-08-09T11:25:40.338628: step 9705, loss 0.498888.
Train: 2018-08-09T11:25:40.416742: step 9706, loss 0.499001.
Train: 2018-08-09T11:25:40.510446: step 9707, loss 0.546951.
Train: 2018-08-09T11:25:40.588575: step 9708, loss 0.514932.
Train: 2018-08-09T11:25:40.666682: step 9709, loss 0.578865.
Train: 2018-08-09T11:25:40.744789: step 9710, loss 0.530713.
Test: 2018-08-09T11:25:41.244642: step 9710, loss 0.54896.
Train: 2018-08-09T11:25:41.322781: step 9711, loss 0.59488.
Train: 2018-08-09T11:25:41.400883: step 9712, loss 0.498443.
Train: 2018-08-09T11:25:41.478995: step 9713, loss 0.530489.
Train: 2018-08-09T11:25:41.557097: step 9714, loss 0.530426.
Train: 2018-08-09T11:25:41.650794: step 9715, loss 0.481598.
Train: 2018-08-09T11:25:41.728901: step 9716, loss 0.530145.
Train: 2018-08-09T11:25:41.807037: step 9717, loss 0.595311.
Train: 2018-08-09T11:25:41.885115: step 9718, loss 0.545447.
Train: 2018-08-09T11:25:41.965516: step 9719, loss 0.611102.
Train: 2018-08-09T11:25:42.059242: step 9720, loss 0.562694.
Test: 2018-08-09T11:25:42.543474: step 9720, loss 0.549223.
Train: 2018-08-09T11:25:42.637200: step 9721, loss 0.562492.
Train: 2018-08-09T11:25:42.715338: step 9722, loss 0.595196.
Train: 2018-08-09T11:25:42.793444: step 9723, loss 0.527229.
Train: 2018-08-09T11:25:42.871520: step 9724, loss 0.479872.
Train: 2018-08-09T11:25:42.949659: step 9725, loss 0.473733.
Train: 2018-08-09T11:25:43.027735: step 9726, loss 0.522927.
Train: 2018-08-09T11:25:43.121494: step 9727, loss 0.589392.
Train: 2018-08-09T11:25:43.199569: step 9728, loss 0.580886.
Train: 2018-08-09T11:25:43.277706: step 9729, loss 0.493025.
Train: 2018-08-09T11:25:43.355812: step 9730, loss 0.530347.
Test: 2018-08-09T11:25:43.855665: step 9730, loss 0.548463.
Train: 2018-08-09T11:25:43.936156: step 9731, loss 0.599406.
Train: 2018-08-09T11:25:44.014288: step 9732, loss 0.579614.
Train: 2018-08-09T11:25:44.107992: step 9733, loss 0.51101.
Train: 2018-08-09T11:25:44.186127: step 9734, loss 0.52293.
Train: 2018-08-09T11:25:44.264234: step 9735, loss 0.635075.
Train: 2018-08-09T11:25:44.342312: step 9736, loss 0.507317.
Train: 2018-08-09T11:25:44.420447: step 9737, loss 0.670086.
Train: 2018-08-09T11:25:44.498553: step 9738, loss 0.562345.
Train: 2018-08-09T11:25:44.592284: step 9739, loss 0.492575.
Train: 2018-08-09T11:25:44.670358: step 9740, loss 0.492496.
Test: 2018-08-09T11:25:45.159356: step 9740, loss 0.546779.
Train: 2018-08-09T11:25:45.237468: step 9741, loss 0.457874.
Train: 2018-08-09T11:25:45.331166: step 9742, loss 0.564382.
Train: 2018-08-09T11:25:45.409307: step 9743, loss 0.6174.
Train: 2018-08-09T11:25:45.487434: step 9744, loss 0.525699.
Train: 2018-08-09T11:25:45.565486: step 9745, loss 0.512642.
Train: 2018-08-09T11:25:45.643594: step 9746, loss 0.614347.
Train: 2018-08-09T11:25:45.737350: step 9747, loss 0.528359.
Train: 2018-08-09T11:25:45.815451: step 9748, loss 0.595975.
Train: 2018-08-09T11:25:45.893566: step 9749, loss 0.597117.
Train: 2018-08-09T11:25:45.968366: step 9750, loss 0.647846.
Test: 2018-08-09T11:25:46.468238: step 9750, loss 0.547768.
Train: 2018-08-09T11:25:46.546354: step 9751, loss 0.646138.
Train: 2018-08-09T11:25:46.640053: step 9752, loss 0.597271.
Train: 2018-08-09T11:25:46.718191: step 9753, loss 0.563127.
Train: 2018-08-09T11:25:46.796295: step 9754, loss 0.545746.
Train: 2018-08-09T11:25:46.874401: step 9755, loss 0.611828.
Train: 2018-08-09T11:25:46.952479: step 9756, loss 0.562447.
Train: 2018-08-09T11:25:47.030585: step 9757, loss 0.578853.
Train: 2018-08-09T11:25:47.124313: step 9758, loss 0.644192.
Train: 2018-08-09T11:25:47.202452: step 9759, loss 0.660304.
Train: 2018-08-09T11:25:47.284284: step 9760, loss 0.530511.
Test: 2018-08-09T11:25:47.768571: step 9760, loss 0.547169.
Train: 2018-08-09T11:25:47.862299: step 9761, loss 0.498664.
Train: 2018-08-09T11:25:47.940381: step 9762, loss 0.530832.
Train: 2018-08-09T11:25:48.018518: step 9763, loss 0.499021.
Train: 2018-08-09T11:25:48.096593: step 9764, loss 0.499066.
Train: 2018-08-09T11:25:48.174728: step 9765, loss 0.626772.
Train: 2018-08-09T11:25:48.268459: step 9766, loss 0.578851.
Train: 2018-08-09T11:25:48.346565: step 9767, loss 0.642689.
Train: 2018-08-09T11:25:48.424671: step 9768, loss 0.531099.
Train: 2018-08-09T11:25:48.502781: step 9769, loss 0.626571.
Train: 2018-08-09T11:25:48.580886: step 9770, loss 0.578891.
Test: 2018-08-09T11:25:49.080738: step 9770, loss 0.549962.
Train: 2018-08-09T11:25:49.158868: step 9771, loss 0.531318.
Train: 2018-08-09T11:25:49.236980: step 9772, loss 0.483894.
Train: 2018-08-09T11:25:49.315057: step 9773, loss 0.594708.
Train: 2018-08-09T11:25:49.408785: step 9774, loss 0.499676.
Train: 2018-08-09T11:25:49.486922: step 9775, loss 0.54715.
Train: 2018-08-09T11:25:49.565029: step 9776, loss 0.59474.
Train: 2018-08-09T11:25:49.643134: step 9777, loss 0.578861.
Train: 2018-08-09T11:25:49.721235: step 9778, loss 0.531144.
Train: 2018-08-09T11:25:49.814965: step 9779, loss 0.594788.
Train: 2018-08-09T11:25:49.889130: step 9780, loss 0.674466.
Test: 2018-08-09T11:25:50.389005: step 9780, loss 0.549842.
Train: 2018-08-09T11:25:50.467118: step 9781, loss 0.467475.
Train: 2018-08-09T11:25:50.545224: step 9782, loss 0.690308.
Train: 2018-08-09T11:25:50.623330: step 9783, loss 0.531171.
Train: 2018-08-09T11:25:50.701406: step 9784, loss 0.420024.
Train: 2018-08-09T11:25:50.795135: step 9785, loss 0.578859.
Train: 2018-08-09T11:25:50.873272: step 9786, loss 0.594798.
Train: 2018-08-09T11:25:50.956660: step 9787, loss 0.594813.
Train: 2018-08-09T11:25:51.034760: step 9788, loss 0.483119.
Train: 2018-08-09T11:25:51.112841: step 9789, loss 0.530909.
Train: 2018-08-09T11:25:51.190976: step 9790, loss 0.642948.
Test: 2018-08-09T11:25:51.690853: step 9790, loss 0.548421.
Train: 2018-08-09T11:25:51.768935: step 9791, loss 0.642991.
Train: 2018-08-09T11:25:51.862692: step 9792, loss 0.578864.
Train: 2018-08-09T11:25:51.940800: step 9793, loss 0.626882.
Train: 2018-08-09T11:25:52.018905: step 9794, loss 0.546905.
Train: 2018-08-09T11:25:52.097012: step 9795, loss 0.594817.
Train: 2018-08-09T11:25:52.175089: step 9796, loss 0.658523.
Train: 2018-08-09T11:25:52.268846: step 9797, loss 0.515326.
Train: 2018-08-09T11:25:52.346922: step 9798, loss 0.594713.
Train: 2018-08-09T11:25:52.425031: step 9799, loss 0.547218.
Train: 2018-08-09T11:25:52.503136: step 9800, loss 0.610458.
Test: 2018-08-09T11:25:53.003019: step 9800, loss 0.548923.
Train: 2018-08-09T11:25:53.565416: step 9801, loss 0.531567.
Train: 2018-08-09T11:25:53.643492: step 9802, loss 0.61036.
Train: 2018-08-09T11:25:53.737246: step 9803, loss 0.610314.
Train: 2018-08-09T11:25:53.815328: step 9804, loss 0.641614.
Train: 2018-08-09T11:25:53.903729: step 9805, loss 0.610154.
Train: 2018-08-09T11:25:53.981868: step 9806, loss 0.532166.
Train: 2018-08-09T11:25:54.059972: step 9807, loss 0.563366.
Train: 2018-08-09T11:25:54.138049: step 9808, loss 0.516909.
Train: 2018-08-09T11:25:54.231776: step 9809, loss 0.563438.
Train: 2018-08-09T11:25:54.309883: step 9810, loss 0.501499.
Test: 2018-08-09T11:25:54.799713: step 9810, loss 0.550664.
Train: 2018-08-09T11:25:54.877842: step 9811, loss 0.625419.
Train: 2018-08-09T11:25:54.955925: step 9812, loss 0.501428.
Train: 2018-08-09T11:25:55.049653: step 9813, loss 0.578889.
Train: 2018-08-09T11:25:55.127789: step 9814, loss 0.50122.
Train: 2018-08-09T11:25:55.205895: step 9815, loss 0.672523.
Train: 2018-08-09T11:25:55.283999: step 9816, loss 0.578893.
Train: 2018-08-09T11:25:55.377699: step 9817, loss 0.532221.
Train: 2018-08-09T11:25:55.455837: step 9818, loss 0.407421.
Train: 2018-08-09T11:25:55.533913: step 9819, loss 0.594355.
Train: 2018-08-09T11:25:55.612019: step 9820, loss 0.610419.
Test: 2018-08-09T11:25:56.107566: step 9820, loss 0.551323.
Train: 2018-08-09T11:25:56.185704: step 9821, loss 0.610435.
Train: 2018-08-09T11:25:56.263813: step 9822, loss 0.547579.
Train: 2018-08-09T11:25:56.357509: step 9823, loss 0.626303.
Train: 2018-08-09T11:25:56.435644: step 9824, loss 0.626479.
Train: 2018-08-09T11:25:56.513751: step 9825, loss 0.452848.
Train: 2018-08-09T11:25:56.591830: step 9826, loss 0.610365.
Train: 2018-08-09T11:25:56.685556: step 9827, loss 0.578923.
Train: 2018-08-09T11:25:56.763662: step 9828, loss 0.484272.
Train: 2018-08-09T11:25:56.841800: step 9829, loss 0.594659.
Train: 2018-08-09T11:25:56.919908: step 9830, loss 0.578843.
Test: 2018-08-09T11:25:57.419757: step 9830, loss 0.550572.
Train: 2018-08-09T11:25:57.497895: step 9831, loss 0.610521.
Train: 2018-08-09T11:25:57.575971: step 9832, loss 0.594746.
Train: 2018-08-09T11:25:57.654077: step 9833, loss 0.547321.
Train: 2018-08-09T11:25:57.747835: step 9834, loss 0.626297.
Train: 2018-08-09T11:25:57.825941: step 9835, loss 0.499911.
Train: 2018-08-09T11:25:57.904044: step 9836, loss 0.642057.
Train: 2018-08-09T11:25:57.984533: step 9837, loss 0.673537.
Train: 2018-08-09T11:25:58.062638: step 9838, loss 0.531661.
Train: 2018-08-09T11:25:58.140742: step 9839, loss 0.610288.
Train: 2018-08-09T11:25:58.218852: step 9840, loss 0.547535.
Test: 2018-08-09T11:25:58.718735: step 9840, loss 0.550353.
Train: 2018-08-09T11:25:58.796841: step 9841, loss 0.516294.
Train: 2018-08-09T11:25:58.874947: step 9842, loss 0.438142.
Train: 2018-08-09T11:25:58.953061: step 9843, loss 0.500528.
Train: 2018-08-09T11:25:59.046751: step 9844, loss 0.547427.
Train: 2018-08-09T11:25:59.124888: step 9845, loss 0.610401.
Train: 2018-08-09T11:25:59.202965: step 9846, loss 0.563064.
Train: 2018-08-09T11:25:59.281104: step 9847, loss 0.610557.
Train: 2018-08-09T11:25:59.359208: step 9848, loss 0.578832.
Train: 2018-08-09T11:25:59.437286: step 9849, loss 0.658234.
Train: 2018-08-09T11:25:59.531012: step 9850, loss 0.562994.
Test: 2018-08-09T11:26:00.031667: step 9850, loss 0.551159.
Train: 2018-08-09T11:26:00.109771: step 9851, loss 0.563041.
Train: 2018-08-09T11:26:00.187877: step 9852, loss 0.610543.
Train: 2018-08-09T11:26:00.265955: step 9853, loss 0.563047.
Train: 2018-08-09T11:26:00.344060: step 9854, loss 0.54726.
Train: 2018-08-09T11:26:00.422191: step 9855, loss 0.594656.
Train: 2018-08-09T11:26:00.515927: step 9856, loss 0.531522.
Train: 2018-08-09T11:26:00.594001: step 9857, loss 0.594643.
Train: 2018-08-09T11:26:00.672139: step 9858, loss 0.484235.
Train: 2018-08-09T11:26:00.750246: step 9859, loss 0.547285.
Train: 2018-08-09T11:26:00.828321: step 9860, loss 0.673722.
Test: 2018-08-09T11:26:01.328225: step 9860, loss 0.550049.
Train: 2018-08-09T11:26:01.406311: step 9861, loss 0.54726.
Train: 2018-08-09T11:26:01.484447: step 9862, loss 0.610458.
Train: 2018-08-09T11:26:01.578176: step 9863, loss 0.578863.
Train: 2018-08-09T11:26:01.656251: step 9864, loss 0.626178.
Train: 2018-08-09T11:26:01.734358: step 9865, loss 0.531641.
Train: 2018-08-09T11:26:01.812496: step 9866, loss 0.578872.
Train: 2018-08-09T11:26:01.906224: step 9867, loss 0.594577.
Train: 2018-08-09T11:26:01.984329: step 9868, loss 0.61024.
Train: 2018-08-09T11:26:02.062436: step 9869, loss 0.484907.
Train: 2018-08-09T11:26:02.140543: step 9870, loss 0.547546.
Test: 2018-08-09T11:26:02.640411: step 9870, loss 0.550894.
Train: 2018-08-09T11:26:02.718531: step 9871, loss 0.500551.
Train: 2018-08-09T11:26:02.812230: step 9872, loss 0.500428.
Train: 2018-08-09T11:26:02.890366: step 9873, loss 0.563187.
Train: 2018-08-09T11:26:02.969846: step 9874, loss 0.578764.
Train: 2018-08-09T11:26:03.047952: step 9875, loss 0.578858.
Train: 2018-08-09T11:26:03.126058: step 9876, loss 0.578741.
Train: 2018-08-09T11:26:03.204165: step 9877, loss 0.563006.
Train: 2018-08-09T11:26:03.282242: step 9878, loss 0.515579.
Train: 2018-08-09T11:26:03.375971: step 9879, loss 0.514843.
Train: 2018-08-09T11:26:03.454106: step 9880, loss 0.578713.
Test: 2018-08-09T11:26:03.953577: step 9880, loss 0.547112.
Train: 2018-08-09T11:26:04.031708: step 9881, loss 0.610538.
Train: 2018-08-09T11:26:04.109820: step 9882, loss 0.53031.
Train: 2018-08-09T11:26:04.187926: step 9883, loss 0.56241.
Train: 2018-08-09T11:26:04.266034: step 9884, loss 0.546767.
Train: 2018-08-09T11:26:04.344139: step 9885, loss 0.562591.
Train: 2018-08-09T11:26:04.437840: step 9886, loss 0.646139.
Train: 2018-08-09T11:26:04.515976: step 9887, loss 0.529847.
Train: 2018-08-09T11:26:04.594050: step 9888, loss 0.546274.
Train: 2018-08-09T11:26:04.672188: step 9889, loss 0.663106.
Train: 2018-08-09T11:26:04.765884: step 9890, loss 0.577915.
Test: 2018-08-09T11:26:05.252500: step 9890, loss 0.546236.
Train: 2018-08-09T11:26:05.330636: step 9891, loss 0.563115.
Train: 2018-08-09T11:26:05.424364: step 9892, loss 0.530608.
Train: 2018-08-09T11:26:05.502442: step 9893, loss 0.514337.
Train: 2018-08-09T11:26:05.580578: step 9894, loss 0.611127.
Train: 2018-08-09T11:26:05.658685: step 9895, loss 0.401652.
Train: 2018-08-09T11:26:05.736788: step 9896, loss 0.65976.
Train: 2018-08-09T11:26:05.830489: step 9897, loss 0.53009.
Train: 2018-08-09T11:26:05.908597: step 9898, loss 0.578503.
Train: 2018-08-09T11:26:05.986733: step 9899, loss 0.530377.
Train: 2018-08-09T11:26:06.080461: step 9900, loss 0.547931.
Test: 2018-08-09T11:26:06.580344: step 9900, loss 0.550915.
Train: 2018-08-09T11:26:07.113914: step 9901, loss 0.512916.
Train: 2018-08-09T11:26:07.192053: step 9902, loss 0.448162.
Train: 2018-08-09T11:26:07.285781: step 9903, loss 0.594682.
Train: 2018-08-09T11:26:07.363857: step 9904, loss 0.512955.
Train: 2018-08-09T11:26:07.441993: step 9905, loss 0.49564.
Train: 2018-08-09T11:26:07.520071: step 9906, loss 0.611004.
Train: 2018-08-09T11:26:07.613829: step 9907, loss 0.494471.
Train: 2018-08-09T11:26:07.691930: step 9908, loss 0.595689.
Train: 2018-08-09T11:26:07.770042: step 9909, loss 0.5431.
Train: 2018-08-09T11:26:07.848118: step 9910, loss 0.560338.
Test: 2018-08-09T11:26:08.348025: step 9910, loss 0.547764.
Train: 2018-08-09T11:26:08.426136: step 9911, loss 0.546689.
Train: 2018-08-09T11:26:08.504242: step 9912, loss 0.525505.
Train: 2018-08-09T11:26:08.597940: step 9913, loss 0.660293.
Train: 2018-08-09T11:26:08.676047: step 9914, loss 0.599765.
Train: 2018-08-09T11:26:08.754183: step 9915, loss 0.629904.
Train: 2018-08-09T11:26:08.832288: step 9916, loss 0.545592.
Train: 2018-08-09T11:26:08.911724: step 9917, loss 0.529378.
Train: 2018-08-09T11:26:09.005486: step 9918, loss 0.495479.
Train: 2018-08-09T11:26:09.083590: step 9919, loss 0.427132.
Train: 2018-08-09T11:26:09.161696: step 9920, loss 0.563258.
Test: 2018-08-09T11:26:09.661549: step 9920, loss 0.549207.
Train: 2018-08-09T11:26:09.739685: step 9921, loss 0.612734.
Train: 2018-08-09T11:26:09.817760: step 9922, loss 0.562274.
Train: 2018-08-09T11:26:09.911489: step 9923, loss 0.578895.
Train: 2018-08-09T11:26:09.989595: step 9924, loss 0.495802.
Train: 2018-08-09T11:26:10.067702: step 9925, loss 0.595747.
Train: 2018-08-09T11:26:10.145809: step 9926, loss 0.512402.
Train: 2018-08-09T11:26:10.223945: step 9927, loss 0.59577.
Train: 2018-08-09T11:26:10.302022: step 9928, loss 0.412941.
Train: 2018-08-09T11:26:10.380129: step 9929, loss 0.478856.
Train: 2018-08-09T11:26:10.473857: step 9930, loss 0.512293.
Test: 2018-08-09T11:26:10.958119: step 9930, loss 0.547917.
Train: 2018-08-09T11:26:11.051845: step 9931, loss 0.495221.
Train: 2018-08-09T11:26:11.129986: step 9932, loss 0.579323.
Train: 2018-08-09T11:26:11.208090: step 9933, loss 0.67084.
Train: 2018-08-09T11:26:11.286194: step 9934, loss 0.596135.
Train: 2018-08-09T11:26:11.364272: step 9935, loss 0.630137.
Train: 2018-08-09T11:26:11.458030: step 9936, loss 0.477703.
Train: 2018-08-09T11:26:11.536136: step 9937, loss 0.613004.
Train: 2018-08-09T11:26:11.614242: step 9938, loss 0.494524.
Train: 2018-08-09T11:26:11.692349: step 9939, loss 0.629984.
Train: 2018-08-09T11:26:11.770456: step 9940, loss 0.647152.
Test: 2018-08-09T11:26:12.270308: step 9940, loss 0.551639.
Train: 2018-08-09T11:26:12.348444: step 9941, loss 0.52878.
Train: 2018-08-09T11:26:12.426551: step 9942, loss 0.494913.
Train: 2018-08-09T11:26:12.504653: step 9943, loss 0.528619.
Train: 2018-08-09T11:26:12.598386: step 9944, loss 0.528809.
Train: 2018-08-09T11:26:12.676489: step 9945, loss 0.562292.
Train: 2018-08-09T11:26:12.754568: step 9946, loss 0.596065.
Train: 2018-08-09T11:26:12.832706: step 9947, loss 0.596133.
Train: 2018-08-09T11:26:12.913256: step 9948, loss 0.629854.
Train: 2018-08-09T11:26:12.991366: step 9949, loss 0.562365.
Train: 2018-08-09T11:26:13.085122: step 9950, loss 0.629519.
Test: 2018-08-09T11:26:13.569377: step 9950, loss 0.549906.
Train: 2018-08-09T11:26:13.647489: step 9951, loss 0.495526.
Train: 2018-08-09T11:26:13.741186: step 9952, loss 0.54577.
Train: 2018-08-09T11:26:13.819323: step 9953, loss 0.512436.
Train: 2018-08-09T11:26:13.897430: step 9954, loss 0.529127.
Train: 2018-08-09T11:26:13.975536: step 9955, loss 0.529132.
Train: 2018-08-09T11:26:14.053645: step 9956, loss 0.612422.
Train: 2018-08-09T11:26:14.147383: step 9957, loss 0.595744.
Train: 2018-08-09T11:26:14.225481: step 9958, loss 0.628996.
Train: 2018-08-09T11:26:14.303585: step 9959, loss 0.595665.
Train: 2018-08-09T11:26:14.381660: step 9960, loss 0.51279.
Test: 2018-08-09T11:26:14.881543: step 9960, loss 0.549493.
Train: 2018-08-09T11:26:14.961987: step 9961, loss 0.628637.
Train: 2018-08-09T11:26:15.040098: step 9962, loss 0.611998.
Train: 2018-08-09T11:26:15.118208: step 9963, loss 0.54609.
Train: 2018-08-09T11:26:15.196311: step 9964, loss 0.562559.
Train: 2018-08-09T11:26:15.290010: step 9965, loss 0.513477.
Train: 2018-08-09T11:26:15.368148: step 9966, loss 0.562593.
Train: 2018-08-09T11:26:15.446252: step 9967, loss 0.497286.
Train: 2018-08-09T11:26:15.524359: step 9968, loss 0.562603.
Train: 2018-08-09T11:26:15.602437: step 9969, loss 0.578932.
Train: 2018-08-09T11:26:15.696187: step 9970, loss 0.676908.
Test: 2018-08-09T11:26:16.180443: step 9970, loss 0.549826.
Train: 2018-08-09T11:26:16.258561: step 9971, loss 0.546314.
Train: 2018-08-09T11:26:16.352288: step 9972, loss 0.578871.
Train: 2018-08-09T11:26:16.430396: step 9973, loss 0.660091.
Train: 2018-08-09T11:26:16.508148: step 9974, loss 0.659852.
Train: 2018-08-09T11:26:16.586252: step 9975, loss 0.62726.
Train: 2018-08-09T11:26:16.664361: step 9976, loss 0.626992.
Train: 2018-08-09T11:26:16.758059: step 9977, loss 0.562913.
Train: 2018-08-09T11:26:16.836195: step 9978, loss 0.594766.
Train: 2018-08-09T11:26:16.916608: step 9979, loss 0.578881.
Train: 2018-08-09T11:26:16.994746: step 9980, loss 0.484675.
Test: 2018-08-09T11:26:17.494598: step 9980, loss 0.549141.
Train: 2018-08-09T11:26:17.572703: step 9981, loss 0.594542.
Train: 2018-08-09T11:26:17.650841: step 9982, loss 0.469584.
Train: 2018-08-09T11:26:17.728948: step 9983, loss 0.516475.
Train: 2018-08-09T11:26:17.807057: step 9984, loss 0.62572.
Train: 2018-08-09T11:26:17.900782: step 9985, loss 0.688095.
Train: 2018-08-09T11:26:17.978858: step 9986, loss 0.532225.
Train: 2018-08-09T11:26:18.056998: step 9987, loss 0.53231.
Train: 2018-08-09T11:26:18.135072: step 9988, loss 0.516826.
Train: 2018-08-09T11:26:18.213206: step 9989, loss 0.609967.
Train: 2018-08-09T11:26:18.291316: step 9990, loss 0.578912.
Test: 2018-08-09T11:26:18.790344: step 9990, loss 0.551206.
Train: 2018-08-09T11:26:18.868452: step 9991, loss 0.594431.
Train: 2018-08-09T11:26:18.948966: step 9992, loss 0.547908.
Train: 2018-08-09T11:26:19.027069: step 9993, loss 0.625425.
Train: 2018-08-09T11:26:19.120770: step 9994, loss 0.578924.
Train: 2018-08-09T11:26:19.198907: step 9995, loss 0.563462.
Train: 2018-08-09T11:26:19.276985: step 9996, loss 0.563478.
Train: 2018-08-09T11:26:19.355090: step 9997, loss 0.501696.
Train: 2018-08-09T11:26:19.433197: step 9998, loss 0.486158.
Train: 2018-08-09T11:26:19.526956: step 9999, loss 0.609918.
Train: 2018-08-09T11:26:19.605062: step 10000, loss 0.501292.
Test: 2018-08-09T11:26:20.092310: step 10000, loss 0.549925.
Train: 2018-08-09T11:26:20.654678: step 10001, loss 0.610031.
Train: 2018-08-09T11:26:20.732815: step 10002, loss 0.469732.
Train: 2018-08-09T11:26:20.826513: step 10003, loss 0.422366.
Train: 2018-08-09T11:26:20.911916: step 10004, loss 0.547363.
Train: 2018-08-09T11:26:20.980526: step 10005, loss 0.48373.
Train: 2018-08-09T11:26:21.058687: step 10006, loss 0.61111.
Train: 2018-08-09T11:26:21.152390: step 10007, loss 0.62681.
Train: 2018-08-09T11:26:21.230465: step 10008, loss 0.643407.
Train: 2018-08-09T11:26:21.308576: step 10009, loss 0.659729.
Train: 2018-08-09T11:26:21.386679: step 10010, loss 0.579062.
Test: 2018-08-09T11:26:21.886573: step 10010, loss 0.551357.
Train: 2018-08-09T11:26:21.964698: step 10011, loss 0.498403.
Train: 2018-08-09T11:26:22.042775: step 10012, loss 0.53057.
Train: 2018-08-09T11:26:22.120881: step 10013, loss 0.595008.
Train: 2018-08-09T11:26:22.214639: step 10014, loss 0.530469.
Train: 2018-08-09T11:26:22.292715: step 10015, loss 0.54657.
Train: 2018-08-09T11:26:22.370853: step 10016, loss 0.546528.
Train: 2018-08-09T11:26:22.448959: step 10017, loss 0.578899.
Train: 2018-08-09T11:26:22.527065: step 10018, loss 0.595132.
Train: 2018-08-09T11:26:22.605142: step 10019, loss 0.513959.
Train: 2018-08-09T11:26:22.698901: step 10020, loss 0.530136.
Test: 2018-08-09T11:26:23.183132: step 10020, loss 0.549221.
Train: 2018-08-09T11:26:23.261238: step 10021, loss 0.497487.
Train: 2018-08-09T11:26:23.354999: step 10022, loss 0.562606.
Train: 2018-08-09T11:26:23.433098: step 10023, loss 0.578952.
Train: 2018-08-09T11:26:23.511209: step 10024, loss 0.578963.
Train: 2018-08-09T11:26:23.589286: step 10025, loss 0.546125.
Train: 2018-08-09T11:26:23.667392: step 10026, loss 0.595428.
Train: 2018-08-09T11:26:23.745528: step 10027, loss 0.595443.
Train: 2018-08-09T11:26:23.839256: step 10028, loss 0.595445.
Train: 2018-08-09T11:26:23.919592: step 10029, loss 0.562538.
Train: 2018-08-09T11:26:23.997730: step 10030, loss 0.513217.
Test: 2018-08-09T11:26:24.497581: step 10030, loss 0.549606.
Train: 2018-08-09T11:26:24.575718: step 10031, loss 0.529627.
Train: 2018-08-09T11:26:24.653794: step 10032, loss 0.529584.
Train: 2018-08-09T11:26:24.731902: step 10033, loss 0.529514.
Train: 2018-08-09T11:26:24.810038: step 10034, loss 0.612019.
Train: 2018-08-09T11:26:24.903736: step 10035, loss 0.529394.
Train: 2018-08-09T11:26:24.981873: step 10036, loss 0.529282.
Train: 2018-08-09T11:26:25.059979: step 10037, loss 0.56216.
Train: 2018-08-09T11:26:25.138085: step 10038, loss 0.612314.
Train: 2018-08-09T11:26:25.216194: step 10039, loss 0.529629.
Train: 2018-08-09T11:26:25.294298: step 10040, loss 0.562745.
Test: 2018-08-09T11:26:25.794182: step 10040, loss 0.547263.
Train: 2018-08-09T11:26:25.872287: step 10041, loss 0.546228.
Train: 2018-08-09T11:26:25.967326: step 10042, loss 0.696634.
Train: 2018-08-09T11:26:26.045438: step 10043, loss 0.596013.
Train: 2018-08-09T11:26:26.127222: step 10044, loss 0.562462.
Train: 2018-08-09T11:26:26.205359: step 10045, loss 0.479933.
Train: 2018-08-09T11:26:26.283468: step 10046, loss 0.628524.
Train: 2018-08-09T11:26:26.361573: step 10047, loss 0.644904.
Train: 2018-08-09T11:26:26.455295: step 10048, loss 0.628265.
Train: 2018-08-09T11:26:26.533407: step 10049, loss 0.480728.
Train: 2018-08-09T11:26:26.611482: step 10050, loss 0.611614.
Test: 2018-08-09T11:26:27.111367: step 10050, loss 0.548592.
Train: 2018-08-09T11:26:27.189506: step 10051, loss 0.595221.
Train: 2018-08-09T11:26:27.267578: step 10052, loss 0.660153.
Train: 2018-08-09T11:26:27.345685: step 10053, loss 0.465633.
Train: 2018-08-09T11:26:27.439444: step 10054, loss 0.595029.
Train: 2018-08-09T11:26:27.517550: step 10055, loss 0.498348.
Train: 2018-08-09T11:26:27.595656: step 10056, loss 0.498431.
Train: 2018-08-09T11:26:27.673764: step 10057, loss 0.530598.
Train: 2018-08-09T11:26:27.751869: step 10058, loss 0.546667.
Train: 2018-08-09T11:26:27.829976: step 10059, loss 0.498269.
Train: 2018-08-09T11:26:27.921385: step 10060, loss 0.627361.
Test: 2018-08-09T11:26:28.421268: step 10060, loss 0.548792.
Train: 2018-08-09T11:26:28.499404: step 10061, loss 0.530367.
Train: 2018-08-09T11:26:28.577505: step 10062, loss 0.54655.
Train: 2018-08-09T11:26:28.655618: step 10063, loss 0.481583.
Train: 2018-08-09T11:26:28.733693: step 10064, loss 0.513848.
Train: 2018-08-09T11:26:28.811831: step 10065, loss 0.513644.
Train: 2018-08-09T11:26:28.905559: step 10066, loss 0.464036.
Train: 2018-08-09T11:26:28.983635: step 10067, loss 0.612148.
Train: 2018-08-09T11:26:29.061741: step 10068, loss 0.429364.
Train: 2018-08-09T11:26:29.139882: step 10069, loss 0.629252.
Train: 2018-08-09T11:26:29.217986: step 10070, loss 0.615567.
Test: 2018-08-09T11:26:29.717837: step 10070, loss 0.548426.
Train: 2018-08-09T11:26:29.795968: step 10071, loss 0.51172.
Train: 2018-08-09T11:26:29.889704: step 10072, loss 0.562787.
Train: 2018-08-09T11:26:29.967810: step 10073, loss 0.57916.
Train: 2018-08-09T11:26:30.045914: step 10074, loss 0.612208.
Train: 2018-08-09T11:26:30.124021: step 10075, loss 0.511787.
Train: 2018-08-09T11:26:30.202124: step 10076, loss 0.511509.
Train: 2018-08-09T11:26:30.295825: step 10077, loss 0.511396.
Train: 2018-08-09T11:26:30.373932: step 10078, loss 0.597185.
Train: 2018-08-09T11:26:30.452041: step 10079, loss 0.545722.
Train: 2018-08-09T11:26:30.530177: step 10080, loss 0.66508.
Test: 2018-08-09T11:26:31.032419: step 10080, loss 0.547027.
Train: 2018-08-09T11:26:31.110556: step 10081, loss 0.57973.
Train: 2018-08-09T11:26:31.188657: step 10082, loss 0.494503.
Train: 2018-08-09T11:26:31.266741: step 10083, loss 0.630149.
Train: 2018-08-09T11:26:31.360468: step 10084, loss 0.57925.
Train: 2018-08-09T11:26:31.438603: step 10085, loss 0.494907.
Train: 2018-08-09T11:26:31.516711: step 10086, loss 0.528668.
Train: 2018-08-09T11:26:31.594820: step 10087, loss 0.51179.
Train: 2018-08-09T11:26:31.672924: step 10088, loss 0.511776.
Train: 2018-08-09T11:26:31.766653: step 10089, loss 0.460999.
Train: 2018-08-09T11:26:31.844727: step 10090, loss 0.596234.
Test: 2018-08-09T11:26:32.328989: step 10090, loss 0.54901.
Train: 2018-08-09T11:26:32.422744: step 10091, loss 0.579346.
Train: 2018-08-09T11:26:32.500823: step 10092, loss 0.426503.
Train: 2018-08-09T11:26:32.578960: step 10093, loss 0.511246.
Train: 2018-08-09T11:26:32.657037: step 10094, loss 0.579483.
Train: 2018-08-09T11:26:32.735173: step 10095, loss 0.528047.
Train: 2018-08-09T11:26:32.813277: step 10096, loss 0.527946.
Train: 2018-08-09T11:26:32.908338: step 10097, loss 0.545095.
Train: 2018-08-09T11:26:32.986478: step 10098, loss 0.527816.
Train: 2018-08-09T11:26:33.064554: step 10099, loss 0.579676.
Train: 2018-08-09T11:26:33.142658: step 10100, loss 0.510319.
Test: 2018-08-09T11:26:33.642541: step 10100, loss 0.546719.
Train: 2018-08-09T11:26:34.189319: step 10101, loss 0.527571.
Train: 2018-08-09T11:26:34.267393: step 10102, loss 0.510039.
Train: 2018-08-09T11:26:34.345500: step 10103, loss 0.632239.
Train: 2018-08-09T11:26:34.423638: step 10104, loss 0.579739.
Train: 2018-08-09T11:26:34.517365: step 10105, loss 0.579843.
Train: 2018-08-09T11:26:34.595442: step 10106, loss 0.492466.
Train: 2018-08-09T11:26:34.673577: step 10107, loss 0.509737.
Train: 2018-08-09T11:26:34.751684: step 10108, loss 0.527328.
Train: 2018-08-09T11:26:34.829788: step 10109, loss 0.615209.
Train: 2018-08-09T11:26:34.910175: step 10110, loss 0.562307.
Test: 2018-08-09T11:26:35.410058: step 10110, loss 0.550581.
Train: 2018-08-09T11:26:35.488195: step 10111, loss 0.685489.
Train: 2018-08-09T11:26:35.566301: step 10112, loss 0.527242.
Train: 2018-08-09T11:26:35.660033: step 10113, loss 0.562277.
Train: 2018-08-09T11:26:35.738137: step 10114, loss 0.562285.
Train: 2018-08-09T11:26:35.816211: step 10115, loss 0.562403.
Train: 2018-08-09T11:26:35.894350: step 10116, loss 0.597187.
Train: 2018-08-09T11:26:35.988073: step 10117, loss 0.666587.
Train: 2018-08-09T11:26:36.066183: step 10118, loss 0.579694.
Train: 2018-08-09T11:26:36.144291: step 10119, loss 0.579501.
Train: 2018-08-09T11:26:36.222400: step 10120, loss 0.699286.
Test: 2018-08-09T11:26:36.722250: step 10120, loss 0.547693.
Train: 2018-08-09T11:26:36.815978: step 10121, loss 0.579356.
Train: 2018-08-09T11:26:36.894083: step 10122, loss 0.460906.
Train: 2018-08-09T11:26:36.974409: step 10123, loss 0.545531.
Train: 2018-08-09T11:26:37.052519: step 10124, loss 0.545601.
Train: 2018-08-09T11:26:37.130620: step 10125, loss 0.562405.
Train: 2018-08-09T11:26:37.224352: step 10126, loss 0.612512.
Train: 2018-08-09T11:26:37.302485: step 10127, loss 0.512512.
Train: 2018-08-09T11:26:37.380591: step 10128, loss 0.512639.
Train: 2018-08-09T11:26:37.458702: step 10129, loss 0.61222.
Train: 2018-08-09T11:26:37.536805: step 10130, loss 0.463173.
Test: 2018-08-09T11:26:38.036683: step 10130, loss 0.546979.
Train: 2018-08-09T11:26:38.114788: step 10131, loss 0.612124.
Train: 2018-08-09T11:26:38.192900: step 10132, loss 0.545962.
Train: 2018-08-09T11:26:38.286629: step 10133, loss 0.595531.
Train: 2018-08-09T11:26:38.364704: step 10134, loss 0.611999.
Train: 2018-08-09T11:26:38.442812: step 10135, loss 0.513137.
Train: 2018-08-09T11:26:38.520950: step 10136, loss 0.628325.
Train: 2018-08-09T11:26:38.599024: step 10137, loss 0.447664.
Train: 2018-08-09T11:26:38.692753: step 10138, loss 0.546141.
Train: 2018-08-09T11:26:38.770860: step 10139, loss 0.464026.
Train: 2018-08-09T11:26:38.849000: step 10140, loss 0.59544.
Test: 2018-08-09T11:26:39.351155: step 10140, loss 0.548327.
Train: 2018-08-09T11:26:39.429291: step 10141, loss 0.529615.
Train: 2018-08-09T11:26:39.507392: step 10142, loss 0.645045.
Train: 2018-08-09T11:26:39.585505: step 10143, loss 0.480042.
Train: 2018-08-09T11:26:39.679203: step 10144, loss 0.562504.
Train: 2018-08-09T11:26:39.757339: step 10145, loss 0.595538.
Train: 2018-08-09T11:26:39.835446: step 10146, loss 0.545946.
Train: 2018-08-09T11:26:39.913523: step 10147, loss 0.463174.
Train: 2018-08-09T11:26:39.991658: step 10148, loss 0.512733.
Train: 2018-08-09T11:26:40.069767: step 10149, loss 0.595661.
Train: 2018-08-09T11:26:40.163494: step 10150, loss 0.495787.
Test: 2018-08-09T11:26:40.663346: step 10150, loss 0.549256.
Train: 2018-08-09T11:26:40.741452: step 10151, loss 0.59571.
Train: 2018-08-09T11:26:40.819588: step 10152, loss 0.528697.
Train: 2018-08-09T11:26:40.897666: step 10153, loss 0.528623.
Train: 2018-08-09T11:26:40.978166: step 10154, loss 0.510705.
Train: 2018-08-09T11:26:41.056301: step 10155, loss 0.511819.
Train: 2018-08-09T11:26:41.150025: step 10156, loss 0.650636.
Train: 2018-08-09T11:26:41.228136: step 10157, loss 0.493664.
Train: 2018-08-09T11:26:41.306215: step 10158, loss 0.54379.
Train: 2018-08-09T11:26:41.384351: step 10159, loss 0.617999.
Train: 2018-08-09T11:26:41.478073: step 10160, loss 0.510756.
Test: 2018-08-09T11:26:41.962343: step 10160, loss 0.548158.
Train: 2018-08-09T11:26:42.056037: step 10161, loss 0.458944.
Train: 2018-08-09T11:26:42.134172: step 10162, loss 0.56256.
Train: 2018-08-09T11:26:42.212284: step 10163, loss 0.528216.
Train: 2018-08-09T11:26:42.290386: step 10164, loss 0.561221.
Train: 2018-08-09T11:26:42.368492: step 10165, loss 0.597021.
Train: 2018-08-09T11:26:42.446570: step 10166, loss 0.596626.
Train: 2018-08-09T11:26:42.540327: step 10167, loss 0.526992.
Train: 2018-08-09T11:26:42.618402: step 10168, loss 0.494575.
Train: 2018-08-09T11:26:42.696540: step 10169, loss 0.632358.
Train: 2018-08-09T11:26:42.774646: step 10170, loss 0.493918.
Test: 2018-08-09T11:26:43.276932: step 10170, loss 0.54618.
Train: 2018-08-09T11:26:43.355016: step 10171, loss 0.717497.
Train: 2018-08-09T11:26:43.433151: step 10172, loss 0.57947.
Train: 2018-08-09T11:26:43.511254: step 10173, loss 0.442624.
Train: 2018-08-09T11:26:43.604985: step 10174, loss 0.528123.
Train: 2018-08-09T11:26:43.683085: step 10175, loss 0.6307.
Train: 2018-08-09T11:26:43.761198: step 10176, loss 0.698795.
Train: 2018-08-09T11:26:43.839305: step 10177, loss 0.579344.
Train: 2018-08-09T11:26:43.917412: step 10178, loss 0.613178.
Train: 2018-08-09T11:26:43.995518: step 10179, loss 0.629861.
Train: 2018-08-09T11:26:44.089243: step 10180, loss 0.512002.
Test: 2018-08-09T11:26:44.573477: step 10180, loss 0.548625.
Train: 2018-08-09T11:26:44.667205: step 10181, loss 0.5624.
Train: 2018-08-09T11:26:44.745310: step 10182, loss 0.562443.
Train: 2018-08-09T11:26:44.823448: step 10183, loss 0.512565.
Train: 2018-08-09T11:26:44.901556: step 10184, loss 0.396532.
Train: 2018-08-09T11:26:44.981964: step 10185, loss 0.529185.
Train: 2018-08-09T11:26:45.060038: step 10186, loss 0.629014.
Train: 2018-08-09T11:26:45.138146: step 10187, loss 0.57916.
Train: 2018-08-09T11:26:45.216284: step 10188, loss 0.495712.
Train: 2018-08-09T11:26:45.309981: step 10189, loss 0.462395.
Train: 2018-08-09T11:26:45.388089: step 10190, loss 0.562498.
Test: 2018-08-09T11:26:45.887971: step 10190, loss 0.547225.
Train: 2018-08-09T11:26:45.966106: step 10191, loss 0.528838.
Train: 2018-08-09T11:26:46.044216: step 10192, loss 0.68076.
Train: 2018-08-09T11:26:46.122321: step 10193, loss 0.545875.
Train: 2018-08-09T11:26:46.200426: step 10194, loss 0.56161.
Train: 2018-08-09T11:26:46.294156: step 10195, loss 0.646752.
Train: 2018-08-09T11:26:46.372260: step 10196, loss 0.612243.
Train: 2018-08-09T11:26:46.450336: step 10197, loss 0.628955.
Train: 2018-08-09T11:26:46.528443: step 10198, loss 0.595654.
Train: 2018-08-09T11:26:46.606549: step 10199, loss 0.595409.
Train: 2018-08-09T11:26:46.684686: step 10200, loss 0.645015.
Test: 2018-08-09T11:26:47.186921: step 10200, loss 0.548409.
Train: 2018-08-09T11:26:47.764934: step 10201, loss 0.579002.
Train: 2018-08-09T11:26:47.843040: step 10202, loss 0.481061.
Train: 2018-08-09T11:26:47.936768: step 10203, loss 0.530047.
Train: 2018-08-09T11:26:48.010899: step 10204, loss 0.611357.
Train: 2018-08-09T11:26:48.089036: step 10205, loss 0.578921.
Train: 2018-08-09T11:26:48.167143: step 10206, loss 0.514299.
Train: 2018-08-09T11:26:48.260880: step 10207, loss 0.530523.
Train: 2018-08-09T11:26:48.338977: step 10208, loss 0.546661.
Train: 2018-08-09T11:26:48.417054: step 10209, loss 0.594977.
Train: 2018-08-09T11:26:48.495191: step 10210, loss 0.530609.
Test: 2018-08-09T11:26:48.985721: step 10210, loss 0.551366.
Train: 2018-08-09T11:26:49.063829: step 10211, loss 0.578875.
Train: 2018-08-09T11:26:49.157555: step 10212, loss 0.57887.
Train: 2018-08-09T11:26:49.235694: step 10213, loss 0.5628.
Train: 2018-08-09T11:26:49.313768: step 10214, loss 0.594933.
Train: 2018-08-09T11:26:49.391906: step 10215, loss 0.562822.
Train: 2018-08-09T11:26:49.469984: step 10216, loss 0.594903.
Train: 2018-08-09T11:26:49.563743: step 10217, loss 0.482747.
Train: 2018-08-09T11:26:49.641846: step 10218, loss 0.53078.
Train: 2018-08-09T11:26:49.719953: step 10219, loss 0.546781.
Train: 2018-08-09T11:26:49.798059: step 10220, loss 0.627071.
Test: 2018-08-09T11:26:50.303941: step 10220, loss 0.550181.
Train: 2018-08-09T11:26:50.370988: step 10221, loss 0.578869.
Train: 2018-08-09T11:26:50.464692: step 10222, loss 0.546731.
Train: 2018-08-09T11:26:50.542799: step 10223, loss 0.578874.
Train: 2018-08-09T11:26:50.620936: step 10224, loss 0.54676.
Train: 2018-08-09T11:26:50.699042: step 10225, loss 0.594933.
Train: 2018-08-09T11:26:50.777149: step 10226, loss 0.643149.
Train: 2018-08-09T11:26:50.855225: step 10227, loss 0.514669.
Train: 2018-08-09T11:26:50.948984: step 10228, loss 0.594848.
Train: 2018-08-09T11:26:51.027059: step 10229, loss 0.675171.
Train: 2018-08-09T11:26:51.105166: step 10230, loss 0.642754.
Test: 2018-08-09T11:26:51.605099: step 10230, loss 0.54984.
Train: 2018-08-09T11:26:51.683184: step 10231, loss 0.610704.
Train: 2018-08-09T11:26:51.761260: step 10232, loss 0.626389.
Train: 2018-08-09T11:26:51.839367: step 10233, loss 0.452726.
Train: 2018-08-09T11:26:51.917476: step 10234, loss 0.613473.
Train: 2018-08-09T11:26:52.011234: step 10235, loss 0.62594.
Train: 2018-08-09T11:26:52.089341: step 10236, loss 0.547614.
Train: 2018-08-09T11:26:52.167446: step 10237, loss 0.547709.
Train: 2018-08-09T11:26:52.245554: step 10238, loss 0.625594.
Train: 2018-08-09T11:26:52.323661: step 10239, loss 0.532354.
Train: 2018-08-09T11:26:52.417356: step 10240, loss 0.54794.
Test: 2018-08-09T11:26:52.901619: step 10240, loss 0.550706.
Train: 2018-08-09T11:26:52.995377: step 10241, loss 0.532501.
Train: 2018-08-09T11:26:53.073477: step 10242, loss 0.517027.
Train: 2018-08-09T11:26:53.151589: step 10243, loss 0.501484.
Train: 2018-08-09T11:26:53.229694: step 10244, loss 0.532359.
Train: 2018-08-09T11:26:53.307802: step 10245, loss 0.516648.
Train: 2018-08-09T11:26:53.401531: step 10246, loss 0.59451.
Train: 2018-08-09T11:26:53.479607: step 10247, loss 0.610213.
Train: 2018-08-09T11:26:53.557743: step 10248, loss 0.578884.
Train: 2018-08-09T11:26:53.635820: step 10249, loss 0.610262.
Train: 2018-08-09T11:26:53.713926: step 10250, loss 0.484538.
Test: 2018-08-09T11:26:54.215291: step 10250, loss 0.548942.
Train: 2018-08-09T11:26:54.293428: step 10251, loss 0.547337.
Train: 2018-08-09T11:26:54.371504: step 10252, loss 0.531427.
Train: 2018-08-09T11:26:54.465257: step 10253, loss 0.578949.
Train: 2018-08-09T11:26:54.543363: step 10254, loss 0.515306.
Train: 2018-08-09T11:26:54.621444: step 10255, loss 0.531051.
Train: 2018-08-09T11:26:54.699582: step 10256, loss 0.531291.
Train: 2018-08-09T11:26:54.777690: step 10257, loss 0.65909.
Train: 2018-08-09T11:26:54.871387: step 10258, loss 0.546622.
Train: 2018-08-09T11:26:54.949493: step 10259, loss 0.562686.
Train: 2018-08-09T11:26:55.027631: step 10260, loss 0.611341.
Test: 2018-08-09T11:26:55.527510: step 10260, loss 0.550031.
Train: 2018-08-09T11:26:55.605619: step 10261, loss 0.546622.
Train: 2018-08-09T11:26:55.683695: step 10262, loss 0.562596.
Train: 2018-08-09T11:26:55.761831: step 10263, loss 0.562674.
Train: 2018-08-09T11:26:55.839938: step 10264, loss 0.546413.
Train: 2018-08-09T11:26:55.919423: step 10265, loss 0.579025.
Train: 2018-08-09T11:26:56.013153: step 10266, loss 0.594936.
Train: 2018-08-09T11:26:56.091289: step 10267, loss 0.481152.
Train: 2018-08-09T11:26:56.169364: step 10268, loss 0.5301.
Train: 2018-08-09T11:26:56.247472: step 10269, loss 0.579856.
Train: 2018-08-09T11:26:56.341199: step 10270, loss 0.661325.
Test: 2018-08-09T11:26:56.825461: step 10270, loss 0.549662.
Train: 2018-08-09T11:26:56.903567: step 10271, loss 0.611103.
Train: 2018-08-09T11:26:56.997320: step 10272, loss 0.57833.
Train: 2018-08-09T11:26:57.075409: step 10273, loss 0.546459.
Train: 2018-08-09T11:26:57.153535: step 10274, loss 0.546654.
Train: 2018-08-09T11:26:57.231645: step 10275, loss 0.6117.
Train: 2018-08-09T11:26:57.309753: step 10276, loss 0.562873.
Train: 2018-08-09T11:26:57.403479: step 10277, loss 0.481483.
Train: 2018-08-09T11:26:57.481586: step 10278, loss 0.562646.
Train: 2018-08-09T11:26:57.559693: step 10279, loss 0.465601.
Train: 2018-08-09T11:26:57.637800: step 10280, loss 0.611222.
Test: 2018-08-09T11:26:58.138292: step 10280, loss 0.548601.
Train: 2018-08-09T11:26:58.216413: step 10281, loss 0.513532.
Train: 2018-08-09T11:26:58.294490: step 10282, loss 0.612043.
Train: 2018-08-09T11:26:58.372598: step 10283, loss 0.610995.
Train: 2018-08-09T11:26:58.466355: step 10284, loss 0.644734.
Train: 2018-08-09T11:26:58.544461: step 10285, loss 0.545949.
Train: 2018-08-09T11:26:58.622568: step 10286, loss 0.61139.
Train: 2018-08-09T11:26:58.700644: step 10287, loss 0.578976.
Train: 2018-08-09T11:26:58.778781: step 10288, loss 0.579026.
Train: 2018-08-09T11:26:58.872508: step 10289, loss 0.5628.
Train: 2018-08-09T11:26:58.950616: step 10290, loss 0.498403.
Test: 2018-08-09T11:26:59.450467: step 10290, loss 0.549513.
Train: 2018-08-09T11:26:59.528599: step 10291, loss 0.562855.
Train: 2018-08-09T11:26:59.606711: step 10292, loss 0.659341.
Train: 2018-08-09T11:26:59.684823: step 10293, loss 0.562719.
Train: 2018-08-09T11:26:59.762924: step 10294, loss 0.482899.
Train: 2018-08-09T11:26:59.841031: step 10295, loss 0.530884.
Train: 2018-08-09T11:26:59.921499: step 10296, loss 0.626937.
Train: 2018-08-09T11:26:59.999637: step 10297, loss 0.578809.
Train: 2018-08-09T11:27:00.093364: step 10298, loss 0.562917.
Train: 2018-08-09T11:27:00.171477: step 10299, loss 0.562886.
Train: 2018-08-09T11:27:00.249547: step 10300, loss 0.610814.
Test: 2018-08-09T11:27:00.749431: step 10300, loss 0.547959.
Train: 2018-08-09T11:27:01.296207: step 10301, loss 0.578808.
Train: 2018-08-09T11:27:01.374313: step 10302, loss 0.531092.
Train: 2018-08-09T11:27:01.468040: step 10303, loss 0.594836.
Train: 2018-08-09T11:27:01.546148: step 10304, loss 0.658461.
Train: 2018-08-09T11:27:01.624223: step 10305, loss 0.563016.
Train: 2018-08-09T11:27:01.702362: step 10306, loss 0.547199.
Train: 2018-08-09T11:27:01.796083: step 10307, loss 0.54726.
Train: 2018-08-09T11:27:01.874196: step 10308, loss 0.48413.
Train: 2018-08-09T11:27:01.953022: step 10309, loss 0.563061.
Train: 2018-08-09T11:27:02.031125: step 10310, loss 0.563048.
Test: 2018-08-09T11:27:02.530978: step 10310, loss 0.548805.
Train: 2018-08-09T11:27:02.609084: step 10311, loss 0.61051.
Train: 2018-08-09T11:27:02.687218: step 10312, loss 0.531381.
Train: 2018-08-09T11:27:02.780920: step 10313, loss 0.563016.
Train: 2018-08-09T11:27:02.859054: step 10314, loss 0.515458.
Train: 2018-08-09T11:27:02.937131: step 10315, loss 0.674152.
Train: 2018-08-09T11:27:03.015238: step 10316, loss 0.436011.
Train: 2018-08-09T11:27:03.093374: step 10317, loss 0.626587.
Train: 2018-08-09T11:27:03.187072: step 10318, loss 0.578879.
Train: 2018-08-09T11:27:03.265209: step 10319, loss 0.626631.
Train: 2018-08-09T11:27:03.343316: step 10320, loss 0.578853.
Test: 2018-08-09T11:27:03.843169: step 10320, loss 0.548066.
Train: 2018-08-09T11:27:03.921274: step 10321, loss 0.547072.
Train: 2018-08-09T11:27:03.999381: step 10322, loss 0.467612.
Train: 2018-08-09T11:27:04.077517: step 10323, loss 0.562928.
Train: 2018-08-09T11:27:04.155593: step 10324, loss 0.546959.
Train: 2018-08-09T11:27:04.233700: step 10325, loss 0.578896.
Train: 2018-08-09T11:27:04.311837: step 10326, loss 0.562881.
Train: 2018-08-09T11:27:04.405535: step 10327, loss 0.546785.
Train: 2018-08-09T11:27:04.483643: step 10328, loss 0.498484.
Train: 2018-08-09T11:27:04.561782: step 10329, loss 0.530301.
Train: 2018-08-09T11:27:04.639885: step 10330, loss 0.579179.
Test: 2018-08-09T11:27:05.141141: step 10330, loss 0.54843.
Train: 2018-08-09T11:27:05.219280: step 10331, loss 0.628285.
Train: 2018-08-09T11:27:05.297384: step 10332, loss 0.563424.
Train: 2018-08-09T11:27:05.375462: step 10333, loss 0.628028.
Train: 2018-08-09T11:27:05.469222: step 10334, loss 0.562187.
Train: 2018-08-09T11:27:05.547325: step 10335, loss 0.562574.
Train: 2018-08-09T11:27:05.625403: step 10336, loss 0.514215.
Train: 2018-08-09T11:27:05.703509: step 10337, loss 0.562631.
Train: 2018-08-09T11:27:05.781644: step 10338, loss 0.514046.
Train: 2018-08-09T11:27:05.875374: step 10339, loss 0.611188.
Train: 2018-08-09T11:27:05.953449: step 10340, loss 0.51368.
Test: 2018-08-09T11:27:06.453366: step 10340, loss 0.549171.
Train: 2018-08-09T11:27:06.531463: step 10341, loss 0.562616.
Train: 2018-08-09T11:27:06.609544: step 10342, loss 0.611387.
Train: 2018-08-09T11:27:06.703303: step 10343, loss 0.595801.
Train: 2018-08-09T11:27:06.781409: step 10344, loss 0.562928.
Train: 2018-08-09T11:27:06.859517: step 10345, loss 0.628118.
Train: 2018-08-09T11:27:06.938874: step 10346, loss 0.644246.
Train: 2018-08-09T11:27:07.017012: step 10347, loss 0.497815.
Train: 2018-08-09T11:27:07.095109: step 10348, loss 0.53046.
Train: 2018-08-09T11:27:07.188843: step 10349, loss 0.627352.
Train: 2018-08-09T11:27:07.266918: step 10350, loss 0.562767.
Test: 2018-08-09T11:27:07.766801: step 10350, loss 0.553798.
Train: 2018-08-09T11:27:07.844908: step 10351, loss 0.578874.
Train: 2018-08-09T11:27:07.923044: step 10352, loss 0.627056.
Train: 2018-08-09T11:27:08.001148: step 10353, loss 0.498751.
Train: 2018-08-09T11:27:08.079258: step 10354, loss 0.610882.
Train: 2018-08-09T11:27:08.172985: step 10355, loss 0.578858.
Train: 2018-08-09T11:27:08.251094: step 10356, loss 0.530965.
Train: 2018-08-09T11:27:08.329169: step 10357, loss 0.530987.
Train: 2018-08-09T11:27:08.407305: step 10358, loss 0.498894.
Train: 2018-08-09T11:27:08.485416: step 10359, loss 0.627758.
Train: 2018-08-09T11:27:08.563488: step 10360, loss 0.594585.
Test: 2018-08-09T11:27:09.065748: step 10360, loss 0.550279.
Train: 2018-08-09T11:27:09.143877: step 10361, loss 0.611132.
Train: 2018-08-09T11:27:09.221990: step 10362, loss 0.483167.
Train: 2018-08-09T11:27:09.315719: step 10363, loss 0.546793.
Train: 2018-08-09T11:27:09.393826: step 10364, loss 0.595087.
Train: 2018-08-09T11:27:09.471931: step 10365, loss 0.450987.
Train: 2018-08-09T11:27:09.550010: step 10366, loss 0.627495.
Train: 2018-08-09T11:27:09.628146: step 10367, loss 0.530887.
Train: 2018-08-09T11:27:09.706246: step 10368, loss 0.562883.
Train: 2018-08-09T11:27:09.799980: step 10369, loss 0.546826.
Train: 2018-08-09T11:27:09.878085: step 10370, loss 0.514525.
Test: 2018-08-09T11:27:10.377965: step 10370, loss 0.54767.
Train: 2018-08-09T11:27:10.456074: step 10371, loss 0.530695.
Train: 2018-08-09T11:27:10.534181: step 10372, loss 0.546637.
Train: 2018-08-09T11:27:10.612284: step 10373, loss 0.546515.
Train: 2018-08-09T11:27:10.690394: step 10374, loss 0.530189.
Train: 2018-08-09T11:27:10.784123: step 10375, loss 0.464799.
Train: 2018-08-09T11:27:10.862200: step 10376, loss 0.578879.
Train: 2018-08-09T11:27:10.942596: step 10377, loss 0.56242.
Train: 2018-08-09T11:27:11.020734: step 10378, loss 0.545346.
Train: 2018-08-09T11:27:11.098811: step 10379, loss 0.496334.
Train: 2018-08-09T11:27:11.192569: step 10380, loss 0.5274.
Test: 2018-08-09T11:27:11.676833: step 10380, loss 0.544855.
Train: 2018-08-09T11:27:11.754938: step 10381, loss 0.567893.
Train: 2018-08-09T11:27:11.848664: step 10382, loss 0.682388.
Train: 2018-08-09T11:27:11.926773: step 10383, loss 0.462016.
Train: 2018-08-09T11:27:12.004849: step 10384, loss 0.529863.
Train: 2018-08-09T11:27:12.082984: step 10385, loss 0.613207.
Train: 2018-08-09T11:27:12.176682: step 10386, loss 0.596418.
Train: 2018-08-09T11:27:12.254818: step 10387, loss 0.511857.
Train: 2018-08-09T11:27:12.332925: step 10388, loss 0.528673.
Train: 2018-08-09T11:27:12.411030: step 10389, loss 0.697234.
Train: 2018-08-09T11:27:12.489107: step 10390, loss 0.545427.
Test: 2018-08-09T11:27:12.990335: step 10390, loss 0.54724.
Train: 2018-08-09T11:27:13.068443: step 10391, loss 0.511821.
Train: 2018-08-09T11:27:13.146575: step 10392, loss 0.562573.
Train: 2018-08-09T11:27:13.240278: step 10393, loss 0.461357.
Train: 2018-08-09T11:27:13.318382: step 10394, loss 0.545578.
Train: 2018-08-09T11:27:13.396520: step 10395, loss 0.5456.
Train: 2018-08-09T11:27:13.474597: step 10396, loss 0.579266.
Train: 2018-08-09T11:27:13.552733: step 10397, loss 0.409705.
Train: 2018-08-09T11:27:13.646431: step 10398, loss 0.528475.
Train: 2018-08-09T11:27:13.724574: step 10399, loss 0.579642.
Train: 2018-08-09T11:27:13.802674: step 10400, loss 0.528347.
Test: 2018-08-09T11:27:14.302538: step 10400, loss 0.54876.
Train: 2018-08-09T11:27:14.864893: step 10401, loss 0.563121.
Train: 2018-08-09T11:27:14.944448: step 10402, loss 0.544758.
Train: 2018-08-09T11:27:15.038146: step 10403, loss 0.579536.
Train: 2018-08-09T11:27:15.116253: step 10404, loss 0.562348.
Train: 2018-08-09T11:27:15.194389: step 10405, loss 0.493414.
Train: 2018-08-09T11:27:15.272468: step 10406, loss 0.597235.
Train: 2018-08-09T11:27:15.350603: step 10407, loss 0.667696.
Train: 2018-08-09T11:27:15.428709: step 10408, loss 0.492963.
Train: 2018-08-09T11:27:15.522433: step 10409, loss 0.63131.
Train: 2018-08-09T11:27:15.600544: step 10410, loss 0.510753.
Test: 2018-08-09T11:27:16.100413: step 10410, loss 0.549461.
Train: 2018-08-09T11:27:16.178532: step 10411, loss 0.6489.
Train: 2018-08-09T11:27:16.256639: step 10412, loss 0.52805.
Train: 2018-08-09T11:27:16.334749: step 10413, loss 0.647637.
Train: 2018-08-09T11:27:16.412852: step 10414, loss 0.613323.
Train: 2018-08-09T11:27:16.506580: step 10415, loss 0.494656.
Train: 2018-08-09T11:27:16.584689: step 10416, loss 0.562215.
Train: 2018-08-09T11:27:16.662764: step 10417, loss 0.545525.
Train: 2018-08-09T11:27:16.740900: step 10418, loss 0.595982.
Train: 2018-08-09T11:27:16.819006: step 10419, loss 0.629403.
Train: 2018-08-09T11:27:16.914041: step 10420, loss 0.595617.
Test: 2018-08-09T11:27:17.398302: step 10420, loss 0.547044.
Train: 2018-08-09T11:27:17.476439: step 10421, loss 0.546438.
Train: 2018-08-09T11:27:17.570161: step 10422, loss 0.579302.
Train: 2018-08-09T11:27:17.648275: step 10423, loss 0.579158.
Train: 2018-08-09T11:27:17.726380: step 10424, loss 0.578871.
Train: 2018-08-09T11:27:17.804486: step 10425, loss 0.562423.
Train: 2018-08-09T11:27:17.882598: step 10426, loss 0.56246.
Train: 2018-08-09T11:27:17.960700: step 10427, loss 0.513416.
Train: 2018-08-09T11:27:18.054428: step 10428, loss 0.57887.
Train: 2018-08-09T11:27:18.132506: step 10429, loss 0.627969.
Train: 2018-08-09T11:27:18.210611: step 10430, loss 0.578951.
Test: 2018-08-09T11:27:18.710495: step 10430, loss 0.549907.
Train: 2018-08-09T11:27:18.788600: step 10431, loss 0.514038.
Train: 2018-08-09T11:27:18.866706: step 10432, loss 0.64381.
Train: 2018-08-09T11:27:18.947095: step 10433, loss 0.578892.
Train: 2018-08-09T11:27:19.025225: step 10434, loss 0.627321.
Train: 2018-08-09T11:27:19.103307: step 10435, loss 0.530614.
Train: 2018-08-09T11:27:19.197066: step 10436, loss 0.61096.
Train: 2018-08-09T11:27:19.275140: step 10437, loss 0.530877.
Train: 2018-08-09T11:27:19.353278: step 10438, loss 0.499052.
Train: 2018-08-09T11:27:19.431354: step 10439, loss 0.594811.
Train: 2018-08-09T11:27:19.509461: step 10440, loss 0.562923.
Test: 2018-08-09T11:27:20.011301: step 10440, loss 0.548014.
Train: 2018-08-09T11:27:20.089408: step 10441, loss 0.562928.
Train: 2018-08-09T11:27:20.184715: step 10442, loss 0.515242.
Train: 2018-08-09T11:27:20.262851: step 10443, loss 0.658485.
Train: 2018-08-09T11:27:20.340927: step 10444, loss 0.61062.
Train: 2018-08-09T11:27:20.419033: step 10445, loss 0.515401.
Train: 2018-08-09T11:27:20.512793: step 10446, loss 0.547218.
Train: 2018-08-09T11:27:20.590899: step 10447, loss 0.594765.
Train: 2018-08-09T11:27:20.669005: step 10448, loss 0.515444.
Train: 2018-08-09T11:27:20.747114: step 10449, loss 0.53129.
Train: 2018-08-09T11:27:20.825218: step 10450, loss 0.515427.
Test: 2018-08-09T11:27:21.325072: step 10450, loss 0.549241.
Train: 2018-08-09T11:27:21.403177: step 10451, loss 0.531138.
Train: 2018-08-09T11:27:21.496935: step 10452, loss 0.531275.
Train: 2018-08-09T11:27:21.575042: step 10453, loss 0.530644.
Train: 2018-08-09T11:27:21.653119: step 10454, loss 0.482521.
Train: 2018-08-09T11:27:21.731225: step 10455, loss 0.59533.
Train: 2018-08-09T11:27:21.809331: step 10456, loss 0.593971.
Train: 2018-08-09T11:27:21.902169: step 10457, loss 0.562972.
Train: 2018-08-09T11:27:21.980275: step 10458, loss 0.547583.
Train: 2018-08-09T11:27:22.058383: step 10459, loss 0.528406.
Train: 2018-08-09T11:27:22.136489: step 10460, loss 0.578633.
Test: 2018-08-09T11:27:22.636343: step 10460, loss 0.550247.
Train: 2018-08-09T11:27:22.714478: step 10461, loss 0.527249.
Train: 2018-08-09T11:27:22.808209: step 10462, loss 0.58021.
Train: 2018-08-09T11:27:22.886284: step 10463, loss 0.438768.
Train: 2018-08-09T11:27:22.967797: step 10464, loss 0.507417.
Train: 2018-08-09T11:27:23.045927: step 10465, loss 0.622644.
Train: 2018-08-09T11:27:23.124041: step 10466, loss 0.660819.
Train: 2018-08-09T11:27:23.202145: step 10467, loss 0.510021.
Train: 2018-08-09T11:27:23.280223: step 10468, loss 0.598851.
Train: 2018-08-09T11:27:23.373982: step 10469, loss 0.510853.
Train: 2018-08-09T11:27:23.452056: step 10470, loss 0.509871.
Test: 2018-08-09T11:27:23.951939: step 10470, loss 0.546801.
Train: 2018-08-09T11:27:24.030075: step 10471, loss 0.512435.
Train: 2018-08-09T11:27:24.108152: step 10472, loss 0.546435.
Train: 2018-08-09T11:27:24.186292: step 10473, loss 0.525394.
Train: 2018-08-09T11:27:24.264398: step 10474, loss 0.596985.
Train: 2018-08-09T11:27:24.342506: step 10475, loss 0.528591.
Train: 2018-08-09T11:27:24.436234: step 10476, loss 0.561073.
Train: 2018-08-09T11:27:24.514336: step 10477, loss 0.630634.
Train: 2018-08-09T11:27:24.592412: step 10478, loss 0.614397.
Train: 2018-08-09T11:27:24.670552: step 10479, loss 0.511767.
Train: 2018-08-09T11:27:24.748625: step 10480, loss 0.596724.
Test: 2018-08-09T11:27:25.249147: step 10480, loss 0.548725.
Train: 2018-08-09T11:27:25.327279: step 10481, loss 0.594796.
Train: 2018-08-09T11:27:25.420981: step 10482, loss 0.529468.
Train: 2018-08-09T11:27:25.499119: step 10483, loss 0.611928.
Train: 2018-08-09T11:27:25.577225: step 10484, loss 0.513128.
Train: 2018-08-09T11:27:25.655332: step 10485, loss 0.447275.
Train: 2018-08-09T11:27:25.733411: step 10486, loss 0.546397.
Train: 2018-08-09T11:27:25.827138: step 10487, loss 0.529346.
Train: 2018-08-09T11:27:25.905272: step 10488, loss 0.528972.
Train: 2018-08-09T11:27:25.983380: step 10489, loss 0.595737.
Train: 2018-08-09T11:27:26.061456: step 10490, loss 0.480091.
Test: 2018-08-09T11:27:26.561337: step 10490, loss 0.548149.
Train: 2018-08-09T11:27:26.639474: step 10491, loss 0.595633.
Train: 2018-08-09T11:27:26.733196: step 10492, loss 0.595749.
Train: 2018-08-09T11:27:26.811309: step 10493, loss 0.628817.
Train: 2018-08-09T11:27:26.889415: step 10494, loss 0.745212.
Train: 2018-08-09T11:27:26.967523: step 10495, loss 0.496253.
Train: 2018-08-09T11:27:27.045630: step 10496, loss 0.562549.
Train: 2018-08-09T11:27:27.123735: step 10497, loss 0.496888.
Train: 2018-08-09T11:27:27.217434: step 10498, loss 0.529742.
Train: 2018-08-09T11:27:27.295572: step 10499, loss 0.562272.
Train: 2018-08-09T11:27:27.373648: step 10500, loss 0.546247.
Test: 2018-08-09T11:27:27.873530: step 10500, loss 0.546531.
Train: 2018-08-09T11:27:28.499838: step 10501, loss 0.71057.
Train: 2018-08-09T11:27:28.577945: step 10502, loss 0.579052.
Train: 2018-08-09T11:27:28.656023: step 10503, loss 0.530026.
Train: 2018-08-09T11:27:28.734159: step 10504, loss 0.497385.
Train: 2018-08-09T11:27:28.812235: step 10505, loss 0.627779.
Train: 2018-08-09T11:27:28.905993: step 10506, loss 0.611391.
Train: 2018-08-09T11:27:28.984099: step 10507, loss 0.627576.
Train: 2018-08-09T11:27:29.062175: step 10508, loss 0.562694.
Train: 2018-08-09T11:27:29.140317: step 10509, loss 0.449931.
Train: 2018-08-09T11:27:29.218419: step 10510, loss 0.449914.
Test: 2018-08-09T11:27:29.724950: step 10510, loss 0.546376.
Train: 2018-08-09T11:27:29.803055: step 10511, loss 0.481991.
Train: 2018-08-09T11:27:29.881131: step 10512, loss 0.64389.
Train: 2018-08-09T11:27:29.959270: step 10513, loss 0.481482.
Train: 2018-08-09T11:27:30.037370: step 10514, loss 0.595465.
Train: 2018-08-09T11:27:30.115482: step 10515, loss 0.627892.
Train: 2018-08-09T11:27:30.209180: step 10516, loss 0.546347.
Train: 2018-08-09T11:27:30.287317: step 10517, loss 0.611559.
Train: 2018-08-09T11:27:30.365423: step 10518, loss 0.497377.
Train: 2018-08-09T11:27:30.443531: step 10519, loss 0.644235.
Train: 2018-08-09T11:27:30.521607: step 10520, loss 0.546238.
Test: 2018-08-09T11:27:31.068377: step 10520, loss 0.54978.
Train: 2018-08-09T11:27:31.146489: step 10521, loss 0.611583.
Train: 2018-08-09T11:27:31.224597: step 10522, loss 0.529707.
Train: 2018-08-09T11:27:31.318295: step 10523, loss 0.595331.
Train: 2018-08-09T11:27:31.396430: step 10524, loss 0.56237.
Train: 2018-08-09T11:27:31.474533: step 10525, loss 0.514467.
Train: 2018-08-09T11:27:31.552615: step 10526, loss 0.595068.
Train: 2018-08-09T11:27:31.630750: step 10527, loss 0.512928.
Train: 2018-08-09T11:27:31.724447: step 10528, loss 0.529479.
Train: 2018-08-09T11:27:31.802554: step 10529, loss 0.58264.
Train: 2018-08-09T11:27:31.880691: step 10530, loss 0.629217.
Test: 2018-08-09T11:27:32.381997: step 10530, loss 0.547796.
Train: 2018-08-09T11:27:32.460072: step 10531, loss 0.595039.
Train: 2018-08-09T11:27:32.538210: step 10532, loss 0.627727.
Train: 2018-08-09T11:27:32.616284: step 10533, loss 0.530045.
Train: 2018-08-09T11:27:32.710041: step 10534, loss 0.578735.
Train: 2018-08-09T11:27:32.788151: step 10535, loss 0.631621.
Train: 2018-08-09T11:27:32.866257: step 10536, loss 0.562851.
Train: 2018-08-09T11:27:32.944362: step 10537, loss 0.658973.
Train: 2018-08-09T11:27:33.022468: step 10538, loss 0.531001.
Train: 2018-08-09T11:27:33.116198: step 10539, loss 0.531119.
Train: 2018-08-09T11:27:33.194273: step 10540, loss 0.610588.
Test: 2018-08-09T11:27:33.694155: step 10540, loss 0.551172.
Train: 2018-08-09T11:27:33.772291: step 10541, loss 0.562964.
Train: 2018-08-09T11:27:33.850369: step 10542, loss 0.515595.
Train: 2018-08-09T11:27:33.930885: step 10543, loss 0.610423.
Train: 2018-08-09T11:27:34.008991: step 10544, loss 0.563056.
Train: 2018-08-09T11:27:34.087098: step 10545, loss 0.626221.
Train: 2018-08-09T11:27:34.180826: step 10546, loss 0.594651.
Train: 2018-08-09T11:27:34.258932: step 10547, loss 0.563302.
Train: 2018-08-09T11:27:34.337070: step 10548, loss 0.594629.
Train: 2018-08-09T11:27:34.415178: step 10549, loss 0.54766.
Train: 2018-08-09T11:27:34.508873: step 10550, loss 0.750762.
Test: 2018-08-09T11:27:34.993135: step 10550, loss 0.550544.
Train: 2018-08-09T11:27:35.071265: step 10551, loss 0.610008.
Train: 2018-08-09T11:27:35.165002: step 10552, loss 0.594404.
Train: 2018-08-09T11:27:35.243107: step 10553, loss 0.563552.
Train: 2018-08-09T11:27:35.321181: step 10554, loss 0.502299.
Train: 2018-08-09T11:27:35.399320: step 10555, loss 0.624901.
Train: 2018-08-09T11:27:35.477431: step 10556, loss 0.502743.
Train: 2018-08-09T11:27:35.571154: step 10557, loss 0.62474.
Train: 2018-08-09T11:27:35.649259: step 10558, loss 0.533439.
Train: 2018-08-09T11:27:35.727371: step 10559, loss 0.579046.
Train: 2018-08-09T11:27:35.805473: step 10560, loss 0.609399.
Test: 2018-08-09T11:27:36.307604: step 10560, loss 0.549163.
Train: 2018-08-09T11:27:36.385740: step 10561, loss 0.548782.
Train: 2018-08-09T11:27:36.463847: step 10562, loss 0.488291.
Train: 2018-08-09T11:27:36.541923: step 10563, loss 0.457792.
Train: 2018-08-09T11:27:36.635681: step 10564, loss 0.518177.
Train: 2018-08-09T11:27:36.713788: step 10565, loss 0.563749.
Train: 2018-08-09T11:27:36.791895: step 10566, loss 0.517608.
Train: 2018-08-09T11:27:36.869969: step 10567, loss 0.609765.
Train: 2018-08-09T11:27:36.948107: step 10568, loss 0.625405.
Train: 2018-08-09T11:27:37.041806: step 10569, loss 0.516887.
Train: 2018-08-09T11:27:37.119942: step 10570, loss 0.532284.
Test: 2018-08-09T11:27:37.604197: step 10570, loss 0.549813.
Train: 2018-08-09T11:27:37.697925: step 10571, loss 0.625714.
Train: 2018-08-09T11:27:37.776008: step 10572, loss 0.484969.
Train: 2018-08-09T11:27:37.854145: step 10573, loss 0.531722.
Train: 2018-08-09T11:27:37.933625: step 10574, loss 0.578932.
Train: 2018-08-09T11:27:38.011731: step 10575, loss 0.594591.
Train: 2018-08-09T11:27:38.089838: step 10576, loss 0.62672.
Train: 2018-08-09T11:27:38.183535: step 10577, loss 0.579075.
Train: 2018-08-09T11:27:38.261643: step 10578, loss 0.563045.
Train: 2018-08-09T11:27:38.339779: step 10579, loss 0.499281.
Train: 2018-08-09T11:27:38.417886: step 10580, loss 0.594794.
Test: 2018-08-09T11:27:38.917738: step 10580, loss 0.550334.
Train: 2018-08-09T11:27:39.058329: step 10581, loss 0.514945.
Train: 2018-08-09T11:27:39.136465: step 10582, loss 0.482812.
Train: 2018-08-09T11:27:39.214572: step 10583, loss 0.546761.
Train: 2018-08-09T11:27:39.292679: step 10584, loss 0.594928.
Train: 2018-08-09T11:27:39.370755: step 10585, loss 0.497897.
Train: 2018-08-09T11:27:39.464513: step 10586, loss 0.529986.
Train: 2018-08-09T11:27:39.542623: step 10587, loss 0.577597.
Train: 2018-08-09T11:27:39.620698: step 10588, loss 0.397643.
Train: 2018-08-09T11:27:39.698833: step 10589, loss 0.458698.
Train: 2018-08-09T11:27:39.776941: step 10590, loss 0.560703.
Test: 2018-08-09T11:27:40.279095: step 10590, loss 0.545693.
Train: 2018-08-09T11:27:40.357230: step 10591, loss 0.602656.
Train: 2018-08-09T11:27:40.435337: step 10592, loss 0.583419.
Train: 2018-08-09T11:27:40.513447: step 10593, loss 0.596705.
Train: 2018-08-09T11:27:40.607171: step 10594, loss 0.452588.
Train: 2018-08-09T11:27:40.685247: step 10595, loss 0.584661.
Train: 2018-08-09T11:27:40.763355: step 10596, loss 0.544333.
Train: 2018-08-09T11:27:40.841461: step 10597, loss 0.433016.
Train: 2018-08-09T11:27:40.919567: step 10598, loss 0.555815.
Train: 2018-08-09T11:27:41.013328: step 10599, loss 0.527878.
Train: 2018-08-09T11:27:41.091403: step 10600, loss 0.539637.
Test: 2018-08-09T11:27:41.591310: step 10600, loss 0.546529.
Train: 2018-08-09T11:27:42.186634: step 10601, loss 0.757882.
Train: 2018-08-09T11:27:42.264710: step 10602, loss 0.671934.
Train: 2018-08-09T11:27:42.342817: step 10603, loss 0.600335.
Train: 2018-08-09T11:27:42.436574: step 10604, loss 0.615395.
Train: 2018-08-09T11:27:42.514650: step 10605, loss 0.477734.
Train: 2018-08-09T11:27:42.592756: step 10606, loss 0.512234.
Train: 2018-08-09T11:27:42.670863: step 10607, loss 0.462597.
Train: 2018-08-09T11:27:42.749000: step 10608, loss 0.595884.
Train: 2018-08-09T11:27:42.842699: step 10609, loss 0.579254.
Train: 2018-08-09T11:27:42.920810: step 10610, loss 0.580068.
Test: 2018-08-09T11:27:43.420721: step 10610, loss 0.548276.
Train: 2018-08-09T11:27:43.498818: step 10611, loss 0.579229.
Train: 2018-08-09T11:27:43.576930: step 10612, loss 0.662337.
Train: 2018-08-09T11:27:43.655008: step 10613, loss 0.496957.
Train: 2018-08-09T11:27:43.733114: step 10614, loss 0.579008.
Train: 2018-08-09T11:27:43.826840: step 10615, loss 0.529839.
Train: 2018-08-09T11:27:43.909334: step 10616, loss 0.611717.
Train: 2018-08-09T11:27:43.983870: step 10617, loss 0.513551.
Train: 2018-08-09T11:27:44.061977: step 10618, loss 0.497155.
Train: 2018-08-09T11:27:44.140083: step 10619, loss 0.710117.
Train: 2018-08-09T11:27:44.218160: step 10620, loss 0.563277.
Test: 2018-08-09T11:27:44.718042: step 10620, loss 0.550474.
Train: 2018-08-09T11:27:44.811769: step 10621, loss 0.464969.
Train: 2018-08-09T11:27:44.889906: step 10622, loss 0.562544.
Train: 2018-08-09T11:27:44.968015: step 10623, loss 0.497602.
Train: 2018-08-09T11:27:45.046120: step 10624, loss 0.578901.
Train: 2018-08-09T11:27:45.124226: step 10625, loss 0.611906.
Train: 2018-08-09T11:27:45.217928: step 10626, loss 0.562452.
Train: 2018-08-09T11:27:45.296030: step 10627, loss 0.529794.
Train: 2018-08-09T11:27:45.374167: step 10628, loss 0.644044.
Train: 2018-08-09T11:27:45.452271: step 10629, loss 0.660623.
Train: 2018-08-09T11:27:45.530381: step 10630, loss 0.530505.
Test: 2018-08-09T11:27:46.030265: step 10630, loss 0.549324.
Train: 2018-08-09T11:27:46.108370: step 10631, loss 0.578981.
Train: 2018-08-09T11:27:46.186476: step 10632, loss 0.611126.
Train: 2018-08-09T11:27:46.280175: step 10633, loss 0.53058.
Train: 2018-08-09T11:27:46.358310: step 10634, loss 0.546688.
Train: 2018-08-09T11:27:46.436417: step 10635, loss 0.675596.
Train: 2018-08-09T11:27:46.514526: step 10636, loss 0.594905.
Train: 2018-08-09T11:27:46.592633: step 10637, loss 0.546977.
Train: 2018-08-09T11:27:46.686327: step 10638, loss 0.57882.
Train: 2018-08-09T11:27:46.764464: step 10639, loss 0.515511.
Train: 2018-08-09T11:27:46.842542: step 10640, loss 0.642216.
Test: 2018-08-09T11:27:47.344851: step 10640, loss 0.548872.
Train: 2018-08-09T11:27:47.422957: step 10641, loss 0.531581.
Train: 2018-08-09T11:27:47.501099: step 10642, loss 0.563096.
Train: 2018-08-09T11:27:47.579203: step 10643, loss 0.563088.
Train: 2018-08-09T11:27:47.657278: step 10644, loss 0.610409.
Train: 2018-08-09T11:27:47.751005: step 10645, loss 0.594679.
Train: 2018-08-09T11:27:47.829113: step 10646, loss 0.547441.
Train: 2018-08-09T11:27:47.907219: step 10647, loss 0.578863.
Train: 2018-08-09T11:27:47.985355: step 10648, loss 0.704314.
Train: 2018-08-09T11:27:48.079054: step 10649, loss 0.594517.
Train: 2018-08-09T11:27:48.157190: step 10650, loss 0.53226.
Test: 2018-08-09T11:27:48.657042: step 10650, loss 0.550039.
Train: 2018-08-09T11:27:48.735179: step 10651, loss 0.501335.
Train: 2018-08-09T11:27:48.813285: step 10652, loss 0.516914.
Train: 2018-08-09T11:27:48.891363: step 10653, loss 0.672033.
Train: 2018-08-09T11:27:48.971795: step 10654, loss 0.609922.
Train: 2018-08-09T11:27:49.049872: step 10655, loss 0.609888.
Train: 2018-08-09T11:27:49.143631: step 10656, loss 0.563455.
Train: 2018-08-09T11:27:49.221737: step 10657, loss 0.625148.
Train: 2018-08-09T11:27:49.299813: step 10658, loss 0.517652.
Train: 2018-08-09T11:27:49.377919: step 10659, loss 0.563703.
Train: 2018-08-09T11:27:49.456055: step 10660, loss 0.548394.
Test: 2018-08-09T11:27:49.955909: step 10660, loss 0.54934.
Train: 2018-08-09T11:27:50.034014: step 10661, loss 0.548447.
Train: 2018-08-09T11:27:50.112163: step 10662, loss 0.640226.
Train: 2018-08-09T11:27:50.190259: step 10663, loss 0.51772.
Train: 2018-08-09T11:27:50.287090: step 10664, loss 0.655691.
Train: 2018-08-09T11:27:50.355778: step 10665, loss 0.548408.
Train: 2018-08-09T11:27:50.449474: step 10666, loss 0.640098.
Train: 2018-08-09T11:27:50.527580: step 10667, loss 0.609503.
Train: 2018-08-09T11:27:50.605717: step 10668, loss 0.533558.
Train: 2018-08-09T11:27:50.683823: step 10669, loss 0.442586.
Train: 2018-08-09T11:27:50.761903: step 10670, loss 0.53342.
Test: 2018-08-09T11:27:51.264030: step 10670, loss 0.550053.
Train: 2018-08-09T11:27:51.342161: step 10671, loss 0.640148.
Train: 2018-08-09T11:27:51.420244: step 10672, loss 0.548502.
Train: 2018-08-09T11:27:51.498352: step 10673, loss 0.471857.
Train: 2018-08-09T11:27:51.592108: step 10674, loss 0.548337.
Train: 2018-08-09T11:27:51.670215: step 10675, loss 0.53235.
Train: 2018-08-09T11:27:51.748321: step 10676, loss 0.563089.
Train: 2018-08-09T11:27:51.826429: step 10677, loss 0.563567.
Train: 2018-08-09T11:27:51.904506: step 10678, loss 0.531142.
Train: 2018-08-09T11:27:51.982641: step 10679, loss 0.594545.
Train: 2018-08-09T11:27:52.076374: step 10680, loss 0.645908.
Test: 2018-08-09T11:27:52.572144: step 10680, loss 0.549136.
Train: 2018-08-09T11:27:52.650247: step 10681, loss 0.547197.
Train: 2018-08-09T11:27:52.728353: step 10682, loss 0.447979.
Train: 2018-08-09T11:27:52.806491: step 10683, loss 0.561995.
Train: 2018-08-09T11:27:52.900219: step 10684, loss 0.4087.
Train: 2018-08-09T11:27:52.981698: step 10685, loss 0.618176.
Train: 2018-08-09T11:27:53.059805: step 10686, loss 0.560014.
Train: 2018-08-09T11:27:53.137916: step 10687, loss 0.581207.
Train: 2018-08-09T11:27:53.216020: step 10688, loss 0.565059.
Train: 2018-08-09T11:27:53.309744: step 10689, loss 0.50816.
Train: 2018-08-09T11:27:53.387855: step 10690, loss 0.601908.
Test: 2018-08-09T11:27:53.884110: step 10690, loss 0.545126.
Train: 2018-08-09T11:27:53.962213: step 10691, loss 0.528515.
Train: 2018-08-09T11:27:54.040320: step 10692, loss 0.557682.
Train: 2018-08-09T11:27:54.134080: step 10693, loss 0.598907.
Train: 2018-08-09T11:27:54.212185: step 10694, loss 0.620816.
Train: 2018-08-09T11:27:54.290292: step 10695, loss 0.592855.
Train: 2018-08-09T11:27:54.368399: step 10696, loss 0.530754.
Train: 2018-08-09T11:27:54.446474: step 10697, loss 0.57851.
Train: 2018-08-09T11:27:54.540202: step 10698, loss 0.512387.
Train: 2018-08-09T11:27:54.618310: step 10699, loss 0.627032.
Train: 2018-08-09T11:27:54.696416: step 10700, loss 0.481972.
Test: 2018-08-09T11:27:55.184937: step 10700, loss 0.548694.
Train: 2018-08-09T11:27:55.841062: step 10701, loss 0.464305.
Train: 2018-08-09T11:27:55.919168: step 10702, loss 0.547268.
Train: 2018-08-09T11:27:56.012866: step 10703, loss 0.463991.
Train: 2018-08-09T11:27:56.090974: step 10704, loss 0.530184.
Train: 2018-08-09T11:27:56.169110: step 10705, loss 0.562874.
Train: 2018-08-09T11:27:56.247218: step 10706, loss 0.479708.
Train: 2018-08-09T11:27:56.325294: step 10707, loss 0.57076.
Train: 2018-08-09T11:27:56.403429: step 10708, loss 0.515085.
Train: 2018-08-09T11:27:56.497129: step 10709, loss 0.621184.
Train: 2018-08-09T11:27:56.575235: step 10710, loss 0.61367.
Test: 2018-08-09T11:27:57.075117: step 10710, loss 0.546884.
Train: 2018-08-09T11:27:57.153223: step 10711, loss 0.65108.
Train: 2018-08-09T11:27:57.231359: step 10712, loss 0.646385.
Train: 2018-08-09T11:27:57.309436: step 10713, loss 0.562739.
Train: 2018-08-09T11:27:57.403195: step 10714, loss 0.531245.
Train: 2018-08-09T11:27:57.481301: step 10715, loss 0.659975.
Train: 2018-08-09T11:27:57.559408: step 10716, loss 0.482718.
Train: 2018-08-09T11:27:57.637483: step 10717, loss 0.482449.
Train: 2018-08-09T11:27:57.715621: step 10718, loss 0.562698.
Train: 2018-08-09T11:27:57.809321: step 10719, loss 0.530532.
Train: 2018-08-09T11:27:57.887455: step 10720, loss 0.513929.
Test: 2018-08-09T11:27:58.389660: step 10720, loss 0.547222.
Train: 2018-08-09T11:27:58.467765: step 10721, loss 0.643241.
Train: 2018-08-09T11:27:58.545902: step 10722, loss 0.579324.
Train: 2018-08-09T11:27:58.624010: step 10723, loss 0.529593.
Train: 2018-08-09T11:27:58.702115: step 10724, loss 0.546226.
Train: 2018-08-09T11:27:58.795843: step 10725, loss 0.52929.
Train: 2018-08-09T11:27:58.873946: step 10726, loss 0.513388.
Train: 2018-08-09T11:27:58.952056: step 10727, loss 0.595626.
Train: 2018-08-09T11:27:59.030163: step 10728, loss 0.664472.
Train: 2018-08-09T11:27:59.123861: step 10729, loss 0.56299.
Train: 2018-08-09T11:27:59.201997: step 10730, loss 0.628696.
Test: 2018-08-09T11:27:59.701851: step 10730, loss 0.5467.
Train: 2018-08-09T11:27:59.779980: step 10731, loss 0.579655.
Train: 2018-08-09T11:27:59.858062: step 10732, loss 0.595281.
Train: 2018-08-09T11:27:59.936874: step 10733, loss 0.482488.
Train: 2018-08-09T11:28:00.030573: step 10734, loss 0.562961.
Train: 2018-08-09T11:28:00.108710: step 10735, loss 0.610994.
Train: 2018-08-09T11:28:00.186816: step 10736, loss 0.434408.
Train: 2018-08-09T11:28:00.264923: step 10737, loss 0.514495.
Train: 2018-08-09T11:28:00.343029: step 10738, loss 0.530401.
Train: 2018-08-09T11:28:00.436758: step 10739, loss 0.595.
Train: 2018-08-09T11:28:00.514864: step 10740, loss 0.464781.
Test: 2018-08-09T11:28:00.999120: step 10740, loss 0.551411.
Train: 2018-08-09T11:28:01.090751: step 10741, loss 0.546204.
Train: 2018-08-09T11:28:01.168886: step 10742, loss 0.545491.
Train: 2018-08-09T11:28:01.246963: step 10743, loss 0.563891.
Train: 2018-08-09T11:28:01.325100: step 10744, loss 0.512153.
Train: 2018-08-09T11:28:01.403208: step 10745, loss 0.527125.
Train: 2018-08-09T11:28:01.496938: step 10746, loss 0.599802.
Train: 2018-08-09T11:28:01.575041: step 10747, loss 0.580076.
Train: 2018-08-09T11:28:01.653118: step 10748, loss 0.52685.
Train: 2018-08-09T11:28:01.731225: step 10749, loss 0.487241.
Train: 2018-08-09T11:28:01.809331: step 10750, loss 0.710278.
Test: 2018-08-09T11:28:02.309227: step 10750, loss 0.549198.
Train: 2018-08-09T11:28:02.387352: step 10751, loss 0.596543.
Train: 2018-08-09T11:28:02.465450: step 10752, loss 0.545298.
Train: 2018-08-09T11:28:02.543564: step 10753, loss 0.611335.
Train: 2018-08-09T11:28:02.621669: step 10754, loss 0.6156.
Train: 2018-08-09T11:28:02.715397: step 10755, loss 0.427451.
Train: 2018-08-09T11:28:02.793505: step 10756, loss 0.492759.
Train: 2018-08-09T11:28:02.871581: step 10757, loss 0.580487.
Train: 2018-08-09T11:28:02.949687: step 10758, loss 0.665476.
Train: 2018-08-09T11:28:03.027822: step 10759, loss 0.495764.
Train: 2018-08-09T11:28:03.121521: step 10760, loss 0.576751.
Test: 2018-08-09T11:28:03.605807: step 10760, loss 0.54929.
Train: 2018-08-09T11:28:03.683914: step 10761, loss 0.543641.
Train: 2018-08-09T11:28:03.777642: step 10762, loss 0.495635.
Train: 2018-08-09T11:28:03.855749: step 10763, loss 0.597806.
Train: 2018-08-09T11:28:03.933854: step 10764, loss 0.646944.
Train: 2018-08-09T11:28:04.011937: step 10765, loss 0.610853.
Train: 2018-08-09T11:28:04.105696: step 10766, loss 0.528894.
Train: 2018-08-09T11:28:04.183801: step 10767, loss 0.528773.
Train: 2018-08-09T11:28:04.261910: step 10768, loss 0.662593.
Train: 2018-08-09T11:28:04.340015: step 10769, loss 0.645917.
Train: 2018-08-09T11:28:04.418121: step 10770, loss 0.529272.
Test: 2018-08-09T11:28:04.920267: step 10770, loss 0.54989.
Train: 2018-08-09T11:28:04.998403: step 10771, loss 0.56105.
Train: 2018-08-09T11:28:05.076510: step 10772, loss 0.577673.
Train: 2018-08-09T11:28:05.170241: step 10773, loss 0.529985.
Train: 2018-08-09T11:28:05.248316: step 10774, loss 0.580051.
Train: 2018-08-09T11:28:05.326452: step 10775, loss 0.62755.
Train: 2018-08-09T11:28:05.404554: step 10776, loss 0.530687.
Train: 2018-08-09T11:28:05.498280: step 10777, loss 0.481692.
Train: 2018-08-09T11:28:05.576396: step 10778, loss 0.53141.
Train: 2018-08-09T11:28:05.654500: step 10779, loss 0.514835.
Train: 2018-08-09T11:28:05.732606: step 10780, loss 0.547298.
Test: 2018-08-09T11:28:06.232459: step 10780, loss 0.549267.
Train: 2018-08-09T11:28:06.310565: step 10781, loss 0.546168.
Train: 2018-08-09T11:28:06.388682: step 10782, loss 0.547886.
Train: 2018-08-09T11:28:06.482399: step 10783, loss 0.479144.
Train: 2018-08-09T11:28:06.560507: step 10784, loss 0.578288.
Train: 2018-08-09T11:28:06.638645: step 10785, loss 0.546906.
Train: 2018-08-09T11:28:06.716719: step 10786, loss 0.527948.
Train: 2018-08-09T11:28:06.794857: step 10787, loss 0.594495.
Train: 2018-08-09T11:28:06.872933: step 10788, loss 0.511876.
Train: 2018-08-09T11:28:06.953525: step 10789, loss 0.440745.
Train: 2018-08-09T11:28:07.047255: step 10790, loss 0.538277.
Test: 2018-08-09T11:28:07.531490: step 10790, loss 0.550695.
Train: 2018-08-09T11:28:07.609625: step 10791, loss 0.4727.
Train: 2018-08-09T11:28:07.703324: step 10792, loss 0.555371.
Train: 2018-08-09T11:28:07.781460: step 10793, loss 0.444929.
Train: 2018-08-09T11:28:07.859562: step 10794, loss 0.67391.
Train: 2018-08-09T11:28:07.937644: step 10795, loss 0.627314.
Train: 2018-08-09T11:28:08.031403: step 10796, loss 0.505272.
Train: 2018-08-09T11:28:08.109507: step 10797, loss 0.637714.
Train: 2018-08-09T11:28:08.187614: step 10798, loss 0.666186.
Train: 2018-08-09T11:28:08.265724: step 10799, loss 0.546678.
Train: 2018-08-09T11:28:08.343831: step 10800, loss 0.579318.
Test: 2018-08-09T11:28:08.843681: step 10800, loss 0.549413.
Train: 2018-08-09T11:28:09.422700: step 10801, loss 0.578937.
Train: 2018-08-09T11:28:09.500805: step 10802, loss 0.627591.
Train: 2018-08-09T11:28:09.578908: step 10803, loss 0.674265.
Train: 2018-08-09T11:28:09.657015: step 10804, loss 0.611657.
Train: 2018-08-09T11:28:09.750746: step 10805, loss 0.550768.
Train: 2018-08-09T11:28:09.828857: step 10806, loss 0.593879.
Train: 2018-08-09T11:28:09.906929: step 10807, loss 0.6233.
Train: 2018-08-09T11:28:09.985035: step 10808, loss 0.580502.
Train: 2018-08-09T11:28:10.063145: step 10809, loss 0.565371.
Train: 2018-08-09T11:28:10.141281: step 10810, loss 0.54772.
Test: 2018-08-09T11:28:10.641133: step 10810, loss 0.549132.
Train: 2018-08-09T11:28:10.719268: step 10811, loss 0.519424.
Train: 2018-08-09T11:28:10.797375: step 10812, loss 0.516118.
Train: 2018-08-09T11:28:10.891103: step 10813, loss 0.469628.
Train: 2018-08-09T11:28:10.969179: step 10814, loss 0.530551.
Train: 2018-08-09T11:28:11.047286: step 10815, loss 0.546403.
Train: 2018-08-09T11:28:11.125424: step 10816, loss 0.595568.
Train: 2018-08-09T11:28:11.219120: step 10817, loss 0.532353.
Train: 2018-08-09T11:28:11.297226: step 10818, loss 0.498719.
Train: 2018-08-09T11:28:11.375334: step 10819, loss 0.627997.
Train: 2018-08-09T11:28:11.453470: step 10820, loss 0.59419.
Test: 2018-08-09T11:28:11.954844: step 10820, loss 0.549605.
Train: 2018-08-09T11:28:12.032975: step 10821, loss 0.563332.
Train: 2018-08-09T11:28:12.111056: step 10822, loss 0.495942.
Train: 2018-08-09T11:28:12.204787: step 10823, loss 0.497519.
Train: 2018-08-09T11:28:12.282922: step 10824, loss 0.511896.
Train: 2018-08-09T11:28:12.361028: step 10825, loss 0.601544.
Train: 2018-08-09T11:28:12.439105: step 10826, loss 0.642538.
Train: 2018-08-09T11:28:12.517212: step 10827, loss 0.456493.
Train: 2018-08-09T11:28:12.595348: step 10828, loss 0.637064.
Train: 2018-08-09T11:28:12.689048: step 10829, loss 0.538054.
Train: 2018-08-09T11:28:12.767182: step 10830, loss 0.602919.
Test: 2018-08-09T11:28:13.251414: step 10830, loss 0.545321.
Train: 2018-08-09T11:28:13.345171: step 10831, loss 0.647631.
Train: 2018-08-09T11:28:13.423273: step 10832, loss 0.581337.
Train: 2018-08-09T11:28:13.501355: step 10833, loss 0.560224.
Train: 2018-08-09T11:28:13.579462: step 10834, loss 0.63349.
Train: 2018-08-09T11:28:13.657568: step 10835, loss 0.543486.
Train: 2018-08-09T11:28:13.735705: step 10836, loss 0.476244.
Train: 2018-08-09T11:28:13.829402: step 10837, loss 0.564072.
Train: 2018-08-09T11:28:13.909052: step 10838, loss 0.531444.
Train: 2018-08-09T11:28:13.987161: step 10839, loss 0.514634.
Train: 2018-08-09T11:28:14.065298: step 10840, loss 0.514058.
Test: 2018-08-09T11:28:14.565149: step 10840, loss 0.550171.
Train: 2018-08-09T11:28:14.643285: step 10841, loss 0.562269.
Train: 2018-08-09T11:28:14.721362: step 10842, loss 0.510138.
Train: 2018-08-09T11:28:14.815115: step 10843, loss 0.579239.
Train: 2018-08-09T11:28:14.893226: step 10844, loss 0.47801.
Train: 2018-08-09T11:28:14.971303: step 10845, loss 0.560502.
Train: 2018-08-09T11:28:15.049440: step 10846, loss 0.656196.
Train: 2018-08-09T11:28:15.127517: step 10847, loss 0.627787.
Train: 2018-08-09T11:28:15.205622: step 10848, loss 0.528348.
Train: 2018-08-09T11:28:15.299380: step 10849, loss 0.542416.
Train: 2018-08-09T11:28:15.377487: step 10850, loss 0.544501.
Test: 2018-08-09T11:28:15.877340: step 10850, loss 0.547392.
Train: 2018-08-09T11:28:15.957762: step 10851, loss 0.572926.
Train: 2018-08-09T11:28:16.035899: step 10852, loss 0.601911.
Train: 2018-08-09T11:28:16.114010: step 10853, loss 0.654712.
Train: 2018-08-09T11:28:16.192112: step 10854, loss 0.616673.
Train: 2018-08-09T11:28:16.270189: step 10855, loss 0.580332.
Train: 2018-08-09T11:28:16.363947: step 10856, loss 0.500871.
Train: 2018-08-09T11:28:16.442053: step 10857, loss 0.594284.
Train: 2018-08-09T11:28:16.520160: step 10858, loss 0.501109.
Train: 2018-08-09T11:28:16.598236: step 10859, loss 0.531653.
Train: 2018-08-09T11:28:16.676344: step 10860, loss 0.484244.
Test: 2018-08-09T11:28:17.176225: step 10860, loss 0.550061.
Train: 2018-08-09T11:28:17.254364: step 10861, loss 0.531378.
Train: 2018-08-09T11:28:17.332437: step 10862, loss 0.531332.
Train: 2018-08-09T11:28:17.426196: step 10863, loss 0.498762.
Train: 2018-08-09T11:28:17.504273: step 10864, loss 0.593687.
Train: 2018-08-09T11:28:17.582409: step 10865, loss 0.578174.
Train: 2018-08-09T11:28:17.660487: step 10866, loss 0.561173.
Train: 2018-08-09T11:28:17.754243: step 10867, loss 0.496665.
Train: 2018-08-09T11:28:17.832350: step 10868, loss 0.545956.
Train: 2018-08-09T11:28:17.911853: step 10869, loss 0.596992.
Train: 2018-08-09T11:28:17.989992: step 10870, loss 0.633722.
Test: 2018-08-09T11:28:18.489843: step 10870, loss 0.547732.
Train: 2018-08-09T11:28:18.567974: step 10871, loss 0.664189.
Train: 2018-08-09T11:28:18.646080: step 10872, loss 0.562567.
Train: 2018-08-09T11:28:18.724162: step 10873, loss 0.627055.
Train: 2018-08-09T11:28:18.817920: step 10874, loss 0.515635.
Train: 2018-08-09T11:28:18.896026: step 10875, loss 0.43525.
Train: 2018-08-09T11:28:18.974103: step 10876, loss 0.626704.
Train: 2018-08-09T11:28:19.052240: step 10877, loss 0.595044.
Train: 2018-08-09T11:28:19.130346: step 10878, loss 0.547233.
Train: 2018-08-09T11:28:19.224073: step 10879, loss 0.546232.
Train: 2018-08-09T11:28:19.302151: step 10880, loss 0.544654.
Test: 2018-08-09T11:28:19.786411: step 10880, loss 0.548475.
Train: 2018-08-09T11:28:19.880164: step 10881, loss 0.497613.
Train: 2018-08-09T11:28:19.960568: step 10882, loss 0.560404.
Train: 2018-08-09T11:28:20.038707: step 10883, loss 0.455786.
Train: 2018-08-09T11:28:20.116811: step 10884, loss 0.531297.
Train: 2018-08-09T11:28:20.194888: step 10885, loss 0.548036.
Train: 2018-08-09T11:28:20.273028: step 10886, loss 0.680425.
Train: 2018-08-09T11:28:20.351132: step 10887, loss 0.532089.
Train: 2018-08-09T11:28:20.444853: step 10888, loss 0.497082.
Train: 2018-08-09T11:28:20.526504: step 10889, loss 0.59427.
Train: 2018-08-09T11:28:20.604578: step 10890, loss 0.562782.
Test: 2018-08-09T11:28:21.104488: step 10890, loss 0.550199.
Train: 2018-08-09T11:28:21.182598: step 10891, loss 0.594488.
Train: 2018-08-09T11:28:21.260674: step 10892, loss 0.598277.
Train: 2018-08-09T11:28:21.338807: step 10893, loss 0.578672.
Train: 2018-08-09T11:28:21.432509: step 10894, loss 0.4958.
Train: 2018-08-09T11:28:21.510615: step 10895, loss 0.601434.
Train: 2018-08-09T11:28:21.588753: step 10896, loss 0.517717.
Train: 2018-08-09T11:28:21.666829: step 10897, loss 0.629356.
Train: 2018-08-09T11:28:21.744965: step 10898, loss 0.691363.
Train: 2018-08-09T11:28:21.838694: step 10899, loss 0.546917.
Train: 2018-08-09T11:28:21.916769: step 10900, loss 0.515933.
Test: 2018-08-09T11:28:22.416652: step 10900, loss 0.548856.
Train: 2018-08-09T11:28:22.973108: step 10901, loss 0.499405.
Train: 2018-08-09T11:28:23.051183: step 10902, loss 0.532054.
Train: 2018-08-09T11:28:23.129290: step 10903, loss 0.482853.
Train: 2018-08-09T11:28:23.207398: step 10904, loss 0.526784.
Train: 2018-08-09T11:28:23.285503: step 10905, loss 0.56229.
Train: 2018-08-09T11:28:23.379262: step 10906, loss 0.559442.
Train: 2018-08-09T11:28:23.457339: step 10907, loss 0.475086.
Train: 2018-08-09T11:28:23.535444: step 10908, loss 0.625711.
Train: 2018-08-09T11:28:23.613578: step 10909, loss 0.585453.
Train: 2018-08-09T11:28:23.691688: step 10910, loss 0.509942.
Test: 2018-08-09T11:28:24.199145: step 10910, loss 0.548506.
Train: 2018-08-09T11:28:24.277281: step 10911, loss 0.596349.
Train: 2018-08-09T11:28:24.355358: step 10912, loss 0.563371.
Train: 2018-08-09T11:28:24.433492: step 10913, loss 0.428387.
Train: 2018-08-09T11:28:24.511600: step 10914, loss 0.576237.
Train: 2018-08-09T11:28:24.605328: step 10915, loss 0.543168.
Train: 2018-08-09T11:28:24.683434: step 10916, loss 0.470996.
Train: 2018-08-09T11:28:24.761541: step 10917, loss 0.516494.
Train: 2018-08-09T11:28:24.839620: step 10918, loss 0.660471.
Train: 2018-08-09T11:28:24.933377: step 10919, loss 0.534624.
Train: 2018-08-09T11:28:25.011483: step 10920, loss 0.617249.
Test: 2018-08-09T11:28:25.504332: step 10920, loss 0.544975.
Train: 2018-08-09T11:28:25.582466: step 10921, loss 0.458855.
Train: 2018-08-09T11:28:25.660543: step 10922, loss 0.575828.
Train: 2018-08-09T11:28:25.754302: step 10923, loss 0.631511.
Train: 2018-08-09T11:28:25.832412: step 10924, loss 0.596857.
Train: 2018-08-09T11:28:25.910514: step 10925, loss 0.494414.
Train: 2018-08-09T11:28:25.988593: step 10926, loss 0.56274.
Train: 2018-08-09T11:28:26.066731: step 10927, loss 0.56212.
Train: 2018-08-09T11:28:26.160458: step 10928, loss 0.5596.
Train: 2018-08-09T11:28:26.238532: step 10929, loss 0.578328.
Train: 2018-08-09T11:28:26.316669: step 10930, loss 0.448946.
Test: 2018-08-09T11:28:26.816522: step 10930, loss 0.549348.
Train: 2018-08-09T11:28:26.894658: step 10931, loss 0.711336.
Train: 2018-08-09T11:28:26.980980: step 10932, loss 0.499786.
Train: 2018-08-09T11:28:27.059085: step 10933, loss 0.592224.
Train: 2018-08-09T11:28:27.137162: step 10934, loss 0.659964.
Train: 2018-08-09T11:28:27.215298: step 10935, loss 0.531767.
Train: 2018-08-09T11:28:27.293405: step 10936, loss 0.57586.
Train: 2018-08-09T11:28:27.387132: step 10937, loss 0.530412.
Train: 2018-08-09T11:28:27.465239: step 10938, loss 0.546234.
Train: 2018-08-09T11:28:27.543348: step 10939, loss 0.532925.
Train: 2018-08-09T11:28:27.621452: step 10940, loss 0.661837.
Test: 2018-08-09T11:28:28.121310: step 10940, loss 0.552569.
Train: 2018-08-09T11:28:28.199412: step 10941, loss 0.513419.
Train: 2018-08-09T11:28:28.277551: step 10942, loss 0.579406.
Train: 2018-08-09T11:28:28.355655: step 10943, loss 0.57895.
Train: 2018-08-09T11:28:28.433762: step 10944, loss 0.467498.
Train: 2018-08-09T11:28:28.527458: step 10945, loss 0.546958.
Train: 2018-08-09T11:28:28.605567: step 10946, loss 0.531627.
Train: 2018-08-09T11:28:28.683703: step 10947, loss 0.612153.
Train: 2018-08-09T11:28:28.761809: step 10948, loss 0.578733.
Train: 2018-08-09T11:28:28.839887: step 10949, loss 0.483585.
Train: 2018-08-09T11:28:28.933643: step 10950, loss 0.577559.
Test: 2018-08-09T11:28:29.433496: step 10950, loss 0.551439.
Train: 2018-08-09T11:28:29.511632: step 10951, loss 0.643471.
Train: 2018-08-09T11:28:29.589738: step 10952, loss 0.5467.
Train: 2018-08-09T11:28:29.667846: step 10953, loss 0.530554.
Train: 2018-08-09T11:28:29.745952: step 10954, loss 0.594285.
Train: 2018-08-09T11:28:29.839680: step 10955, loss 0.561154.
Train: 2018-08-09T11:28:29.920057: step 10956, loss 0.627777.
Train: 2018-08-09T11:28:29.998195: step 10957, loss 0.56357.
Train: 2018-08-09T11:28:30.076301: step 10958, loss 0.579534.
Train: 2018-08-09T11:28:30.154380: step 10959, loss 0.610291.
Train: 2018-08-09T11:28:30.232485: step 10960, loss 0.627172.
Test: 2018-08-09T11:28:30.732383: step 10960, loss 0.551044.
Train: 2018-08-09T11:28:30.810498: step 10961, loss 0.578171.
Train: 2018-08-09T11:28:30.904232: step 10962, loss 0.531527.
Train: 2018-08-09T11:28:30.982338: step 10963, loss 0.723033.
Train: 2018-08-09T11:28:31.060445: step 10964, loss 0.563316.
Train: 2018-08-09T11:28:31.138554: step 10965, loss 0.48457.
Train: 2018-08-09T11:28:31.216660: step 10966, loss 0.594741.
Train: 2018-08-09T11:28:31.294764: step 10967, loss 0.499859.
Train: 2018-08-09T11:28:31.388463: step 10968, loss 0.499608.
Train: 2018-08-09T11:28:31.466569: step 10969, loss 0.499569.
Train: 2018-08-09T11:28:31.544706: step 10970, loss 0.598068.
Test: 2018-08-09T11:28:32.046820: step 10970, loss 0.547856.
Train: 2018-08-09T11:28:32.124955: step 10971, loss 0.561121.
Train: 2018-08-09T11:28:32.203063: step 10972, loss 0.568097.
Train: 2018-08-09T11:28:32.281166: step 10973, loss 0.495312.
Train: 2018-08-09T11:28:32.374868: step 10974, loss 0.509662.
Train: 2018-08-09T11:28:32.453003: step 10975, loss 0.542524.
Train: 2018-08-09T11:28:32.531082: step 10976, loss 0.532997.
Train: 2018-08-09T11:28:32.609211: step 10977, loss 0.55493.
Train: 2018-08-09T11:28:32.687293: step 10978, loss 0.62963.
Train: 2018-08-09T11:28:32.782723: step 10979, loss 0.644706.
Train: 2018-08-09T11:28:32.860861: step 10980, loss 0.629965.
Test: 2018-08-09T11:28:33.345092: step 10980, loss 0.550087.
Train: 2018-08-09T11:28:33.438849: step 10981, loss 0.495215.
Train: 2018-08-09T11:28:33.516926: step 10982, loss 0.626484.
Train: 2018-08-09T11:28:33.595065: step 10983, loss 0.612453.
Train: 2018-08-09T11:28:33.673164: step 10984, loss 0.531051.
Train: 2018-08-09T11:28:33.751276: step 10985, loss 0.497362.
Train: 2018-08-09T11:28:33.844973: step 10986, loss 0.582213.
Train: 2018-08-09T11:28:33.922716: step 10987, loss 0.575335.
Train: 2018-08-09T11:28:34.000854: step 10988, loss 0.530912.
Train: 2018-08-09T11:28:34.078960: step 10989, loss 0.531387.
Train: 2018-08-09T11:28:34.157036: step 10990, loss 0.53061.
Test: 2018-08-09T11:28:34.656950: step 10990, loss 0.549662.
Train: 2018-08-09T11:28:34.735060: step 10991, loss 0.499908.
Train: 2018-08-09T11:28:34.813162: step 10992, loss 0.563569.
Train: 2018-08-09T11:28:34.906890: step 10993, loss 0.578272.
Train: 2018-08-09T11:28:34.984996: step 10994, loss 0.563823.
Train: 2018-08-09T11:28:35.063106: step 10995, loss 0.481673.
Train: 2018-08-09T11:28:35.141180: step 10996, loss 0.611426.
Train: 2018-08-09T11:28:35.219317: step 10997, loss 0.528641.
Train: 2018-08-09T11:28:35.297423: step 10998, loss 0.579068.
Train: 2018-08-09T11:28:35.391153: step 10999, loss 0.628429.
Train: 2018-08-09T11:28:35.469229: step 11000, loss 0.496033.
Test: 2018-08-09T11:28:35.953489: step 11000, loss 0.547161.
Train: 2018-08-09T11:28:36.562746: step 11001, loss 0.496883.
Train: 2018-08-09T11:28:36.640827: step 11002, loss 0.56482.
Train: 2018-08-09T11:28:36.718961: step 11003, loss 0.512528.
Train: 2018-08-09T11:28:36.812692: step 11004, loss 0.577118.
Train: 2018-08-09T11:28:36.890797: step 11005, loss 0.528021.
Train: 2018-08-09T11:28:36.971279: step 11006, loss 0.545278.
Train: 2018-08-09T11:28:37.049355: step 11007, loss 0.463526.
Train: 2018-08-09T11:28:37.127492: step 11008, loss 0.562728.
Train: 2018-08-09T11:28:37.205592: step 11009, loss 0.703672.
Train: 2018-08-09T11:28:37.299296: step 11010, loss 0.633121.
Test: 2018-08-09T11:28:37.783559: step 11010, loss 0.549606.
Train: 2018-08-09T11:28:37.861663: step 11011, loss 0.631104.
Train: 2018-08-09T11:28:37.955422: step 11012, loss 0.61507.
Train: 2018-08-09T11:28:38.033499: step 11013, loss 0.59508.
Train: 2018-08-09T11:28:38.111635: step 11014, loss 0.563217.
Train: 2018-08-09T11:28:38.189711: step 11015, loss 0.660352.
Train: 2018-08-09T11:28:38.283454: step 11016, loss 0.499126.
Train: 2018-08-09T11:28:38.361576: step 11017, loss 0.626713.
Train: 2018-08-09T11:28:38.439683: step 11018, loss 0.578546.
Train: 2018-08-09T11:28:38.517784: step 11019, loss 0.547113.
Train: 2018-08-09T11:28:38.595867: step 11020, loss 0.752522.
Test: 2018-08-09T11:28:39.097155: step 11020, loss 0.549739.
Train: 2018-08-09T11:28:39.175291: step 11021, loss 0.594501.
Train: 2018-08-09T11:28:39.253397: step 11022, loss 0.578682.
Train: 2018-08-09T11:28:39.331475: step 11023, loss 0.579009.
Train: 2018-08-09T11:28:39.425232: step 11024, loss 0.610047.
Train: 2018-08-09T11:28:39.503338: step 11025, loss 0.410543.
Train: 2018-08-09T11:28:39.581440: step 11026, loss 0.533058.
Train: 2018-08-09T11:28:39.659557: step 11027, loss 0.624903.
Train: 2018-08-09T11:28:39.737658: step 11028, loss 0.579004.
Train: 2018-08-09T11:28:39.815767: step 11029, loss 0.594348.
Train: 2018-08-09T11:28:39.909462: step 11030, loss 0.548456.
Test: 2018-08-09T11:28:40.393724: step 11030, loss 0.551795.
Train: 2018-08-09T11:28:40.471830: step 11031, loss 0.655281.
Train: 2018-08-09T11:28:40.565588: step 11032, loss 0.487703.
Train: 2018-08-09T11:28:40.643695: step 11033, loss 0.502957.
Train: 2018-08-09T11:28:40.721801: step 11034, loss 0.533348.
Train: 2018-08-09T11:28:40.799877: step 11035, loss 0.579021.
Train: 2018-08-09T11:28:40.877985: step 11036, loss 0.578965.
Train: 2018-08-09T11:28:40.958481: step 11037, loss 0.609564.
Train: 2018-08-09T11:28:41.052183: step 11038, loss 0.51763.
Train: 2018-08-09T11:28:41.130314: step 11039, loss 0.594281.
Train: 2018-08-09T11:28:41.208421: step 11040, loss 0.594109.
Test: 2018-08-09T11:28:41.708272: step 11040, loss 0.548915.
Train: 2018-08-09T11:28:41.786379: step 11041, loss 0.548348.
Train: 2018-08-09T11:28:41.864515: step 11042, loss 0.594484.
Train: 2018-08-09T11:28:41.942623: step 11043, loss 0.485494.
Train: 2018-08-09T11:28:42.020728: step 11044, loss 0.563378.
Train: 2018-08-09T11:28:42.098835: step 11045, loss 0.45319.
Train: 2018-08-09T11:28:42.192563: step 11046, loss 0.562491.
Train: 2018-08-09T11:28:42.270671: step 11047, loss 0.629942.
Train: 2018-08-09T11:28:42.348746: step 11048, loss 0.6127.
Train: 2018-08-09T11:28:42.426855: step 11049, loss 0.609142.
Train: 2018-08-09T11:28:42.504990: step 11050, loss 0.530602.
Test: 2018-08-09T11:28:43.007211: step 11050, loss 0.549307.
Train: 2018-08-09T11:28:43.085342: step 11051, loss 0.547596.
Train: 2018-08-09T11:28:43.179078: step 11052, loss 0.482357.
Train: 2018-08-09T11:28:43.257182: step 11053, loss 0.496691.
Train: 2018-08-09T11:28:43.335259: step 11054, loss 0.543417.
Train: 2018-08-09T11:28:43.413395: step 11055, loss 0.542099.
Train: 2018-08-09T11:28:43.507118: step 11056, loss 0.585813.
Train: 2018-08-09T11:28:43.585199: step 11057, loss 0.51101.
Train: 2018-08-09T11:28:43.663305: step 11058, loss 0.532176.
Train: 2018-08-09T11:28:43.741413: step 11059, loss 0.539395.
Train: 2018-08-09T11:28:43.819553: step 11060, loss 0.531007.
Test: 2018-08-09T11:28:44.319403: step 11060, loss 0.547139.
Train: 2018-08-09T11:28:44.397507: step 11061, loss 0.57744.
Train: 2018-08-09T11:28:44.475645: step 11062, loss 0.599965.
Train: 2018-08-09T11:28:44.553750: step 11063, loss 0.494548.
Train: 2018-08-09T11:28:44.647449: step 11064, loss 0.497628.
Train: 2018-08-09T11:28:44.725586: step 11065, loss 0.541217.
Train: 2018-08-09T11:28:44.803695: step 11066, loss 0.513539.
Train: 2018-08-09T11:28:44.881799: step 11067, loss 0.618346.
Train: 2018-08-09T11:28:44.962251: step 11068, loss 0.58191.
Train: 2018-08-09T11:28:45.040353: step 11069, loss 0.544887.
Train: 2018-08-09T11:28:45.118460: step 11070, loss 0.647429.
Test: 2018-08-09T11:28:45.618312: step 11070, loss 0.54864.
Train: 2018-08-09T11:28:45.696451: step 11071, loss 0.494554.
Train: 2018-08-09T11:28:45.790176: step 11072, loss 0.663246.
Train: 2018-08-09T11:28:45.868283: step 11073, loss 0.644308.
Train: 2018-08-09T11:28:45.946389: step 11074, loss 0.57907.
Train: 2018-08-09T11:28:46.024496: step 11075, loss 0.546536.
Train: 2018-08-09T11:28:46.102604: step 11076, loss 0.562525.
Train: 2018-08-09T11:28:46.180709: step 11077, loss 0.499233.
Train: 2018-08-09T11:28:46.274437: step 11078, loss 0.498483.
Train: 2018-08-09T11:28:46.352544: step 11079, loss 0.579093.
Train: 2018-08-09T11:28:46.430651: step 11080, loss 0.562541.
Test: 2018-08-09T11:28:46.932901: step 11080, loss 0.550253.
Train: 2018-08-09T11:28:47.011000: step 11081, loss 0.579092.
Train: 2018-08-09T11:28:47.089106: step 11082, loss 0.546823.
Train: 2018-08-09T11:28:47.167270: step 11083, loss 0.546696.
Train: 2018-08-09T11:28:47.260971: step 11084, loss 0.562785.
Train: 2018-08-09T11:28:47.339050: step 11085, loss 0.546439.
Train: 2018-08-09T11:28:47.417188: step 11086, loss 0.561976.
Train: 2018-08-09T11:28:47.495261: step 11087, loss 0.546784.
Train: 2018-08-09T11:28:47.573399: step 11088, loss 0.514033.
Train: 2018-08-09T11:28:47.667098: step 11089, loss 0.563023.
Train: 2018-08-09T11:28:47.745203: step 11090, loss 0.530643.
Test: 2018-08-09T11:28:48.229490: step 11090, loss 0.549639.
Train: 2018-08-09T11:28:48.323192: step 11091, loss 0.693798.
Train: 2018-08-09T11:28:48.401328: step 11092, loss 0.544908.
Train: 2018-08-09T11:28:48.479436: step 11093, loss 0.5957.
Train: 2018-08-09T11:28:48.557541: step 11094, loss 0.480619.
Train: 2018-08-09T11:28:48.635618: step 11095, loss 0.561255.
Train: 2018-08-09T11:28:48.729346: step 11096, loss 0.543844.
Train: 2018-08-09T11:28:48.807481: step 11097, loss 0.611694.
Train: 2018-08-09T11:28:48.885558: step 11098, loss 0.613262.
Train: 2018-08-09T11:28:48.966117: step 11099, loss 0.532243.
Train: 2018-08-09T11:28:49.044228: step 11100, loss 0.644773.
Test: 2018-08-09T11:28:49.544104: step 11100, loss 0.546956.
Train: 2018-08-09T11:28:50.059612: step 11101, loss 0.610952.
Train: 2018-08-09T11:28:50.137688: step 11102, loss 0.545583.
Train: 2018-08-09T11:28:50.231441: step 11103, loss 0.495067.
Train: 2018-08-09T11:28:50.309524: step 11104, loss 0.532395.
Train: 2018-08-09T11:28:50.387660: step 11105, loss 0.629248.
Train: 2018-08-09T11:28:50.465762: step 11106, loss 0.611822.
Train: 2018-08-09T11:28:50.543873: step 11107, loss 0.547897.
Train: 2018-08-09T11:28:50.621979: step 11108, loss 0.513948.
Train: 2018-08-09T11:28:50.712662: step 11109, loss 0.563377.
Train: 2018-08-09T11:28:50.790777: step 11110, loss 0.660839.
Test: 2018-08-09T11:28:51.295909: step 11110, loss 0.547817.
Train: 2018-08-09T11:28:51.374015: step 11111, loss 0.691199.
Train: 2018-08-09T11:28:51.452151: step 11112, loss 0.690231.
Train: 2018-08-09T11:28:51.530257: step 11113, loss 0.547402.
Train: 2018-08-09T11:28:51.608365: step 11114, loss 0.532014.
Train: 2018-08-09T11:28:51.686467: step 11115, loss 0.532202.
Train: 2018-08-09T11:28:51.780170: step 11116, loss 0.454697.
Train: 2018-08-09T11:28:51.858305: step 11117, loss 0.656537.
Train: 2018-08-09T11:28:51.936382: step 11118, loss 0.578923.
Train: 2018-08-09T11:28:52.014523: step 11119, loss 0.625351.
Train: 2018-08-09T11:28:52.092627: step 11120, loss 0.687017.
Test: 2018-08-09T11:28:52.608101: step 11120, loss 0.548604.
Train: 2018-08-09T11:28:52.686206: step 11121, loss 0.609716.
Train: 2018-08-09T11:28:52.764342: step 11122, loss 0.533077.
Train: 2018-08-09T11:28:52.842445: step 11123, loss 0.62479.
Train: 2018-08-09T11:28:52.920559: step 11124, loss 0.563855.
Train: 2018-08-09T11:28:52.998662: step 11125, loss 0.639688.
Train: 2018-08-09T11:28:53.092361: step 11126, loss 0.548936.
Train: 2018-08-09T11:28:53.170499: step 11127, loss 0.564104.
Train: 2018-08-09T11:28:53.248603: step 11128, loss 0.639201.
Train: 2018-08-09T11:28:53.326706: step 11129, loss 0.504432.
Train: 2018-08-09T11:28:53.404814: step 11130, loss 0.459749.
Test: 2018-08-09T11:28:53.906029: step 11130, loss 0.551949.
Train: 2018-08-09T11:28:53.984167: step 11131, loss 0.489507.
Train: 2018-08-09T11:28:54.077894: step 11132, loss 0.489223.
Train: 2018-08-09T11:28:54.155973: step 11133, loss 0.503845.
Train: 2018-08-09T11:28:54.234108: step 11134, loss 0.594225.
Train: 2018-08-09T11:28:54.312185: step 11135, loss 0.563838.
Train: 2018-08-09T11:28:54.406016: step 11136, loss 0.579006.
Train: 2018-08-09T11:28:54.482009: step 11137, loss 0.547257.
Train: 2018-08-09T11:28:54.560087: step 11138, loss 0.578925.
Train: 2018-08-09T11:28:54.638192: step 11139, loss 0.609894.
Train: 2018-08-09T11:28:54.731948: step 11140, loss 0.516889.
Test: 2018-08-09T11:28:55.216181: step 11140, loss 0.548723.
Train: 2018-08-09T11:28:55.294317: step 11141, loss 0.532235.
Train: 2018-08-09T11:28:55.388044: step 11142, loss 0.594531.
Train: 2018-08-09T11:28:55.466151: step 11143, loss 0.610631.
Train: 2018-08-09T11:28:55.544257: step 11144, loss 0.688828.
Train: 2018-08-09T11:28:55.622367: step 11145, loss 0.531782.
Train: 2018-08-09T11:28:55.700472: step 11146, loss 0.531848.
Train: 2018-08-09T11:28:55.794198: step 11147, loss 0.610304.
Train: 2018-08-09T11:28:55.872304: step 11148, loss 0.625972.
Train: 2018-08-09T11:28:55.950411: step 11149, loss 0.516181.
Train: 2018-08-09T11:28:56.028519: step 11150, loss 0.516144.
Test: 2018-08-09T11:28:56.528385: step 11150, loss 0.550835.
Train: 2018-08-09T11:28:56.606476: step 11151, loss 0.594571.
Train: 2018-08-09T11:28:56.684583: step 11152, loss 0.531722.
Train: 2018-08-09T11:28:56.778936: step 11153, loss 0.594615.
Train: 2018-08-09T11:28:56.857043: step 11154, loss 0.484349.
Train: 2018-08-09T11:28:56.935148: step 11155, loss 0.53144.
Train: 2018-08-09T11:28:57.013259: step 11156, loss 0.594785.
Train: 2018-08-09T11:28:57.091362: step 11157, loss 0.451709.
Train: 2018-08-09T11:28:57.169438: step 11158, loss 0.642878.
Train: 2018-08-09T11:28:57.263196: step 11159, loss 0.563175.
Train: 2018-08-09T11:28:57.341302: step 11160, loss 0.595026.
Test: 2018-08-09T11:28:57.841159: step 11160, loss 0.548991.
Train: 2018-08-09T11:28:57.919295: step 11161, loss 0.482544.
Train: 2018-08-09T11:28:57.997399: step 11162, loss 0.562969.
Train: 2018-08-09T11:28:58.075507: step 11163, loss 0.611232.
Train: 2018-08-09T11:28:58.169237: step 11164, loss 0.482079.
Train: 2018-08-09T11:28:58.247308: step 11165, loss 0.514031.
Train: 2018-08-09T11:28:58.325447: step 11166, loss 0.530206.
Train: 2018-08-09T11:28:58.403523: step 11167, loss 0.529676.
Train: 2018-08-09T11:28:58.481659: step 11168, loss 0.579005.
Train: 2018-08-09T11:28:58.575387: step 11169, loss 0.562964.
Train: 2018-08-09T11:28:58.653464: step 11170, loss 0.679335.
Test: 2018-08-09T11:28:59.153388: step 11170, loss 0.548184.
Train: 2018-08-09T11:28:59.231482: step 11171, loss 0.462679.
Train: 2018-08-09T11:28:59.309589: step 11172, loss 0.529258.
Train: 2018-08-09T11:28:59.387667: step 11173, loss 0.562397.
Train: 2018-08-09T11:28:59.481423: step 11174, loss 0.562489.
Train: 2018-08-09T11:28:59.559533: step 11175, loss 0.579885.
Train: 2018-08-09T11:28:59.637637: step 11176, loss 0.562054.
Train: 2018-08-09T11:28:59.715743: step 11177, loss 0.596921.
Train: 2018-08-09T11:28:59.793853: step 11178, loss 0.562706.
Train: 2018-08-09T11:28:59.887578: step 11179, loss 0.444543.
Train: 2018-08-09T11:28:59.966984: step 11180, loss 0.580063.
Test: 2018-08-09T11:29:00.451254: step 11180, loss 0.546492.
Train: 2018-08-09T11:29:00.544977: step 11181, loss 0.409624.
Train: 2018-08-09T11:29:00.623080: step 11182, loss 0.681243.
Train: 2018-08-09T11:29:00.701181: step 11183, loss 0.612879.
Train: 2018-08-09T11:29:00.779262: step 11184, loss 0.424792.
Train: 2018-08-09T11:29:00.857404: step 11185, loss 0.386995.
Train: 2018-08-09T11:29:00.951099: step 11186, loss 0.490631.
Train: 2018-08-09T11:29:01.029204: step 11187, loss 0.5439.
Train: 2018-08-09T11:29:01.107340: step 11188, loss 0.619412.
Train: 2018-08-09T11:29:01.185419: step 11189, loss 0.617506.
Train: 2018-08-09T11:29:01.263524: step 11190, loss 0.568304.
Test: 2018-08-09T11:29:01.763407: step 11190, loss 0.546714.
Train: 2018-08-09T11:29:01.841543: step 11191, loss 0.466403.
Train: 2018-08-09T11:29:01.921116: step 11192, loss 0.715149.
Train: 2018-08-09T11:29:01.999254: step 11193, loss 0.637366.
Train: 2018-08-09T11:29:02.092982: step 11194, loss 0.523316.
Train: 2018-08-09T11:29:02.171091: step 11195, loss 0.472883.
Train: 2018-08-09T11:29:02.249196: step 11196, loss 0.554987.
Train: 2018-08-09T11:29:02.327270: step 11197, loss 0.601346.
Train: 2018-08-09T11:29:02.405378: step 11198, loss 0.568734.
Train: 2018-08-09T11:29:02.499136: step 11199, loss 0.458525.
Train: 2018-08-09T11:29:02.577242: step 11200, loss 0.527396.
Test: 2018-08-09T11:29:03.077124: step 11200, loss 0.546652.
Train: 2018-08-09T11:29:03.623841: step 11201, loss 0.636424.
Train: 2018-08-09T11:29:03.701977: step 11202, loss 0.582162.
Train: 2018-08-09T11:29:03.780079: step 11203, loss 0.547586.
Train: 2018-08-09T11:29:03.858160: step 11204, loss 0.581284.
Train: 2018-08-09T11:29:03.952580: step 11205, loss 0.646152.
Train: 2018-08-09T11:29:04.030688: step 11206, loss 0.562478.
Train: 2018-08-09T11:29:04.108763: step 11207, loss 0.578892.
Train: 2018-08-09T11:29:04.186900: step 11208, loss 0.546516.
Train: 2018-08-09T11:29:04.280600: step 11209, loss 0.595322.
Train: 2018-08-09T11:29:04.358735: step 11210, loss 0.530267.
Test: 2018-08-09T11:29:04.847601: step 11210, loss 0.547568.
Train: 2018-08-09T11:29:04.925682: step 11211, loss 0.627374.
Train: 2018-08-09T11:29:05.019434: step 11212, loss 0.595016.
Train: 2018-08-09T11:29:05.097540: step 11213, loss 0.611025.
Train: 2018-08-09T11:29:05.175653: step 11214, loss 0.514821.
Train: 2018-08-09T11:29:05.253759: step 11215, loss 0.562889.
Train: 2018-08-09T11:29:05.347459: step 11216, loss 0.467286.
Train: 2018-08-09T11:29:05.425594: step 11217, loss 0.610748.
Train: 2018-08-09T11:29:05.503703: step 11218, loss 0.594799.
Train: 2018-08-09T11:29:05.581778: step 11219, loss 0.578854.
Train: 2018-08-09T11:29:05.659885: step 11220, loss 0.547083.
Test: 2018-08-09T11:29:06.159766: step 11220, loss 0.551703.
Train: 2018-08-09T11:29:06.237902: step 11221, loss 0.626525.
Train: 2018-08-09T11:29:06.316009: step 11222, loss 0.563009.
Train: 2018-08-09T11:29:06.409742: step 11223, loss 0.547218.
Train: 2018-08-09T11:29:06.487838: step 11224, loss 0.578896.
Train: 2018-08-09T11:29:06.565950: step 11225, loss 0.452462.
Train: 2018-08-09T11:29:06.659650: step 11226, loss 0.54721.
Train: 2018-08-09T11:29:06.737754: step 11227, loss 0.499499.
Train: 2018-08-09T11:29:06.815891: step 11228, loss 0.562949.
Train: 2018-08-09T11:29:06.893997: step 11229, loss 0.610726.
Train: 2018-08-09T11:29:06.974797: step 11230, loss 0.530784.
Test: 2018-08-09T11:29:07.474650: step 11230, loss 0.550039.
Train: 2018-08-09T11:29:07.552786: step 11231, loss 0.562835.
Train: 2018-08-09T11:29:07.646485: step 11232, loss 0.578765.
Train: 2018-08-09T11:29:07.724590: step 11233, loss 0.464523.
Train: 2018-08-09T11:29:07.802727: step 11234, loss 0.59727.
Train: 2018-08-09T11:29:07.880836: step 11235, loss 0.513337.
Train: 2018-08-09T11:29:07.974532: step 11236, loss 0.59574.
Train: 2018-08-09T11:29:08.052638: step 11237, loss 0.57936.
Train: 2018-08-09T11:29:08.130744: step 11238, loss 0.611967.
Train: 2018-08-09T11:29:08.208851: step 11239, loss 0.577199.
Train: 2018-08-09T11:29:08.286988: step 11240, loss 0.597165.
Test: 2018-08-09T11:29:08.786865: step 11240, loss 0.546566.
Train: 2018-08-09T11:29:08.864947: step 11241, loss 0.545743.
Train: 2018-08-09T11:29:08.960966: step 11242, loss 0.58284.
Train: 2018-08-09T11:29:09.039105: step 11243, loss 0.596339.
Train: 2018-08-09T11:29:09.117209: step 11244, loss 0.562552.
Train: 2018-08-09T11:29:09.195317: step 11245, loss 0.495415.
Train: 2018-08-09T11:29:09.273424: step 11246, loss 0.547949.
Train: 2018-08-09T11:29:09.351499: step 11247, loss 0.545122.
Train: 2018-08-09T11:29:09.445258: step 11248, loss 0.579834.
Train: 2018-08-09T11:29:09.523365: step 11249, loss 0.725199.
Train: 2018-08-09T11:29:09.601470: step 11250, loss 0.644804.
Test: 2018-08-09T11:29:10.101323: step 11250, loss 0.548197.
Train: 2018-08-09T11:29:10.179460: step 11251, loss 0.514844.
Train: 2018-08-09T11:29:10.257560: step 11252, loss 0.531515.
Train: 2018-08-09T11:29:10.351295: step 11253, loss 0.546862.
Train: 2018-08-09T11:29:10.429370: step 11254, loss 0.610716.
Train: 2018-08-09T11:29:10.507477: step 11255, loss 0.563167.
Train: 2018-08-09T11:29:10.585610: step 11256, loss 0.48408.
Train: 2018-08-09T11:29:10.679343: step 11257, loss 0.673624.
Train: 2018-08-09T11:29:10.757419: step 11258, loss 0.563099.
Train: 2018-08-09T11:29:10.835558: step 11259, loss 0.500167.
Train: 2018-08-09T11:29:10.914955: step 11260, loss 0.484424.
Test: 2018-08-09T11:29:11.414839: step 11260, loss 0.548329.
Train: 2018-08-09T11:29:11.492976: step 11261, loss 0.626181.
Train: 2018-08-09T11:29:11.571052: step 11262, loss 0.468478.
Train: 2018-08-09T11:29:11.649189: step 11263, loss 0.547256.
Train: 2018-08-09T11:29:11.742885: step 11264, loss 0.61051.
Train: 2018-08-09T11:29:11.820994: step 11265, loss 0.515371.
Train: 2018-08-09T11:29:11.899099: step 11266, loss 0.578836.
Train: 2018-08-09T11:29:11.977206: step 11267, loss 0.610844.
Train: 2018-08-09T11:29:12.070944: step 11268, loss 0.514986.
Train: 2018-08-09T11:29:12.149071: step 11269, loss 0.674872.
Train: 2018-08-09T11:29:12.227177: step 11270, loss 0.642738.
Test: 2018-08-09T11:29:12.727030: step 11270, loss 0.549786.
Train: 2018-08-09T11:29:12.805135: step 11271, loss 0.610817.
Train: 2018-08-09T11:29:12.883272: step 11272, loss 0.562937.
Train: 2018-08-09T11:29:12.963668: step 11273, loss 0.626488.
Train: 2018-08-09T11:29:13.047075: step 11274, loss 0.531393.
Train: 2018-08-09T11:29:13.125957: step 11275, loss 0.563065.
Train: 2018-08-09T11:29:13.219685: step 11276, loss 0.515787.
Train: 2018-08-09T11:29:13.297790: step 11277, loss 0.578877.
Train: 2018-08-09T11:29:13.375867: step 11278, loss 0.578908.
Train: 2018-08-09T11:29:13.454005: step 11279, loss 0.578803.
Train: 2018-08-09T11:29:13.532110: step 11280, loss 0.515888.
Test: 2018-08-09T11:29:14.031963: step 11280, loss 0.550116.
Train: 2018-08-09T11:29:14.110069: step 11281, loss 0.500101.
Train: 2018-08-09T11:29:14.188208: step 11282, loss 0.657841.
Train: 2018-08-09T11:29:14.281904: step 11283, loss 0.547226.
Train: 2018-08-09T11:29:14.360039: step 11284, loss 0.562865.
Train: 2018-08-09T11:29:14.438116: step 11285, loss 0.594688.
Train: 2018-08-09T11:29:14.516223: step 11286, loss 0.626279.
Train: 2018-08-09T11:29:14.594330: step 11287, loss 0.626331.
Train: 2018-08-09T11:29:14.672437: step 11288, loss 0.594679.
Train: 2018-08-09T11:29:14.766164: step 11289, loss 0.516055.
Train: 2018-08-09T11:29:14.844301: step 11290, loss 0.626091.
Test: 2018-08-09T11:29:15.338800: step 11290, loss 0.548432.
Train: 2018-08-09T11:29:15.416906: step 11291, loss 0.642077.
Train: 2018-08-09T11:29:15.495044: step 11292, loss 0.547593.
Train: 2018-08-09T11:29:15.573150: step 11293, loss 0.485447.
Train: 2018-08-09T11:29:15.666876: step 11294, loss 0.578891.
Train: 2018-08-09T11:29:15.744985: step 11295, loss 0.610035.
Train: 2018-08-09T11:29:15.823089: step 11296, loss 0.563298.
Train: 2018-08-09T11:29:15.901165: step 11297, loss 0.547768.
Train: 2018-08-09T11:29:15.979272: step 11298, loss 0.485546.
Train: 2018-08-09T11:29:16.057409: step 11299, loss 0.531705.
Train: 2018-08-09T11:29:16.135485: step 11300, loss 0.563082.
Test: 2018-08-09T11:29:16.635368: step 11300, loss 0.549983.
Train: 2018-08-09T11:29:17.166492: step 11301, loss 0.579318.
Train: 2018-08-09T11:29:17.244624: step 11302, loss 0.515565.
Train: 2018-08-09T11:29:17.322732: step 11303, loss 0.563676.
Train: 2018-08-09T11:29:17.416465: step 11304, loss 0.481794.
Train: 2018-08-09T11:29:17.494573: step 11305, loss 0.610902.
Train: 2018-08-09T11:29:17.572678: step 11306, loss 0.544521.
Train: 2018-08-09T11:29:17.650783: step 11307, loss 0.493572.
Train: 2018-08-09T11:29:17.728890: step 11308, loss 0.60116.
Train: 2018-08-09T11:29:17.807000: step 11309, loss 0.479205.
Train: 2018-08-09T11:29:17.900725: step 11310, loss 0.476786.
Test: 2018-08-09T11:29:18.387336: step 11310, loss 0.550498.
Train: 2018-08-09T11:29:18.465441: step 11311, loss 0.600366.
Train: 2018-08-09T11:29:18.543579: step 11312, loss 0.715355.
Train: 2018-08-09T11:29:18.637277: step 11313, loss 0.545141.
Train: 2018-08-09T11:29:18.715413: step 11314, loss 0.635816.
Train: 2018-08-09T11:29:18.793520: step 11315, loss 0.476276.
Train: 2018-08-09T11:29:18.871629: step 11316, loss 0.696042.
Train: 2018-08-09T11:29:18.949732: step 11317, loss 0.563295.
Train: 2018-08-09T11:29:19.043460: step 11318, loss 0.610848.
Train: 2018-08-09T11:29:19.121568: step 11319, loss 0.579695.
Train: 2018-08-09T11:29:19.199643: step 11320, loss 0.561998.
Test: 2018-08-09T11:29:19.699556: step 11320, loss 0.551508.
Train: 2018-08-09T11:29:19.777664: step 11321, loss 0.563525.
Train: 2018-08-09T11:29:19.855769: step 11322, loss 0.529926.
Train: 2018-08-09T11:29:19.936401: step 11323, loss 0.594855.
Train: 2018-08-09T11:29:20.014505: step 11324, loss 0.483564.
Train: 2018-08-09T11:29:20.092614: step 11325, loss 0.627101.
Train: 2018-08-09T11:29:20.186342: step 11326, loss 0.594303.
Train: 2018-08-09T11:29:20.264447: step 11327, loss 0.499975.
Train: 2018-08-09T11:29:20.342554: step 11328, loss 0.532025.
Train: 2018-08-09T11:29:20.420660: step 11329, loss 0.610376.
Train: 2018-08-09T11:29:20.498797: step 11330, loss 0.562641.
Test: 2018-08-09T11:29:21.008343: step 11330, loss 0.551092.
Train: 2018-08-09T11:29:21.086480: step 11331, loss 0.48322.
Train: 2018-08-09T11:29:21.164586: step 11332, loss 0.484606.
Train: 2018-08-09T11:29:21.242693: step 11333, loss 0.40141.
Train: 2018-08-09T11:29:21.320769: step 11334, loss 0.593377.
Train: 2018-08-09T11:29:21.414527: step 11335, loss 0.513589.
Train: 2018-08-09T11:29:21.492604: step 11336, loss 0.597715.
Train: 2018-08-09T11:29:21.570740: step 11337, loss 0.560077.
Train: 2018-08-09T11:29:21.648847: step 11338, loss 0.508065.
Train: 2018-08-09T11:29:21.726925: step 11339, loss 0.582287.
Train: 2018-08-09T11:29:21.805030: step 11340, loss 0.58453.
Test: 2018-08-09T11:29:22.312143: step 11340, loss 0.546503.
Train: 2018-08-09T11:29:22.390278: step 11341, loss 0.457209.
Train: 2018-08-09T11:29:22.468384: step 11342, loss 0.563893.
Train: 2018-08-09T11:29:22.562113: step 11343, loss 0.493492.
Train: 2018-08-09T11:29:22.640218: step 11344, loss 0.594178.
Train: 2018-08-09T11:29:22.718325: step 11345, loss 0.63085.
Train: 2018-08-09T11:29:22.796403: step 11346, loss 0.643493.
Train: 2018-08-09T11:29:22.874539: step 11347, loss 0.497953.
Train: 2018-08-09T11:29:22.968261: step 11348, loss 0.449785.
Train: 2018-08-09T11:29:23.046373: step 11349, loss 0.634997.
Train: 2018-08-09T11:29:23.124450: step 11350, loss 0.523041.
Test: 2018-08-09T11:29:23.624332: step 11350, loss 0.54982.
Train: 2018-08-09T11:29:23.702468: step 11351, loss 0.598639.
Train: 2018-08-09T11:29:23.780575: step 11352, loss 0.689809.
Train: 2018-08-09T11:29:23.858687: step 11353, loss 0.511843.
Train: 2018-08-09T11:29:23.952409: step 11354, loss 0.579716.
Train: 2018-08-09T11:29:24.030516: step 11355, loss 0.511201.
Train: 2018-08-09T11:29:24.108593: step 11356, loss 0.543572.
Train: 2018-08-09T11:29:24.186698: step 11357, loss 0.544357.
Train: 2018-08-09T11:29:24.264836: step 11358, loss 0.510401.
Train: 2018-08-09T11:29:24.342943: step 11359, loss 0.530582.
Train: 2018-08-09T11:29:24.436670: step 11360, loss 0.595866.
Test: 2018-08-09T11:29:24.922300: step 11360, loss 0.548658.
Train: 2018-08-09T11:29:25.016050: step 11361, loss 0.615689.
Train: 2018-08-09T11:29:25.094126: step 11362, loss 0.561808.
Train: 2018-08-09T11:29:25.172233: step 11363, loss 0.47912.
Train: 2018-08-09T11:29:25.250365: step 11364, loss 0.642717.
Train: 2018-08-09T11:29:25.328477: step 11365, loss 0.610147.
Train: 2018-08-09T11:29:25.422204: step 11366, loss 0.60912.
Train: 2018-08-09T11:29:25.500311: step 11367, loss 0.595245.
Train: 2018-08-09T11:29:25.578417: step 11368, loss 0.511922.
Train: 2018-08-09T11:29:25.656524: step 11369, loss 0.541915.
Train: 2018-08-09T11:29:25.734601: step 11370, loss 0.479365.
Test: 2018-08-09T11:29:26.240116: step 11370, loss 0.548263.
Train: 2018-08-09T11:29:26.318223: step 11371, loss 0.56232.
Train: 2018-08-09T11:29:26.396329: step 11372, loss 0.626565.
Train: 2018-08-09T11:29:26.474466: step 11373, loss 0.576566.
Train: 2018-08-09T11:29:26.552572: step 11374, loss 0.527876.
Train: 2018-08-09T11:29:26.646299: step 11375, loss 0.595472.
Train: 2018-08-09T11:29:26.724407: step 11376, loss 0.565471.
Train: 2018-08-09T11:29:26.802516: step 11377, loss 0.511023.
Train: 2018-08-09T11:29:26.880620: step 11378, loss 0.516635.
Train: 2018-08-09T11:29:26.958728: step 11379, loss 0.514701.
Train: 2018-08-09T11:29:27.052454: step 11380, loss 0.514607.
Test: 2018-08-09T11:29:27.536686: step 11380, loss 0.547678.
Train: 2018-08-09T11:29:27.630443: step 11381, loss 0.574856.
Train: 2018-08-09T11:29:27.708550: step 11382, loss 0.685418.
Train: 2018-08-09T11:29:27.786627: step 11383, loss 0.52984.
Train: 2018-08-09T11:29:27.864763: step 11384, loss 0.612355.
Train: 2018-08-09T11:29:27.942864: step 11385, loss 0.531419.
Train: 2018-08-09T11:29:28.036569: step 11386, loss 0.596955.
Train: 2018-08-09T11:29:28.114704: step 11387, loss 0.449089.
Train: 2018-08-09T11:29:28.192810: step 11388, loss 0.642347.
Train: 2018-08-09T11:29:28.270918: step 11389, loss 0.64236.
Train: 2018-08-09T11:29:28.349023: step 11390, loss 0.590544.
Test: 2018-08-09T11:29:28.849280: step 11390, loss 0.546411.
Train: 2018-08-09T11:29:28.927414: step 11391, loss 0.578385.
Train: 2018-08-09T11:29:29.005492: step 11392, loss 0.53398.
Train: 2018-08-09T11:29:29.083599: step 11393, loss 0.644511.
Train: 2018-08-09T11:29:29.177326: step 11394, loss 0.530845.
Train: 2018-08-09T11:29:29.255433: step 11395, loss 0.46904.
Train: 2018-08-09T11:29:29.333564: step 11396, loss 0.627008.
Train: 2018-08-09T11:29:29.411676: step 11397, loss 0.529862.
Train: 2018-08-09T11:29:29.489784: step 11398, loss 0.624837.
Train: 2018-08-09T11:29:29.583481: step 11399, loss 0.659509.
Train: 2018-08-09T11:29:29.661617: step 11400, loss 0.561549.
Test: 2018-08-09T11:29:30.161469: step 11400, loss 0.548627.
Train: 2018-08-09T11:29:30.786352: step 11401, loss 0.61178.
Train: 2018-08-09T11:29:30.895701: step 11402, loss 0.514891.
Train: 2018-08-09T11:29:30.976164: step 11403, loss 0.563284.
Train: 2018-08-09T11:29:31.054271: step 11404, loss 0.610263.
Train: 2018-08-09T11:29:31.132373: step 11405, loss 0.532113.
Train: 2018-08-09T11:29:31.210455: step 11406, loss 0.578246.
Train: 2018-08-09T11:29:31.304212: step 11407, loss 0.485857.
Train: 2018-08-09T11:29:31.382287: step 11408, loss 0.578942.
Train: 2018-08-09T11:29:31.460394: step 11409, loss 0.409636.
Train: 2018-08-09T11:29:31.538531: step 11410, loss 0.499355.
Test: 2018-08-09T11:29:32.038384: step 11410, loss 0.551618.
Train: 2018-08-09T11:29:32.116514: step 11411, loss 0.578493.
Train: 2018-08-09T11:29:32.194627: step 11412, loss 0.498119.
Train: 2018-08-09T11:29:32.272735: step 11413, loss 0.561179.
Train: 2018-08-09T11:29:32.366455: step 11414, loss 0.547323.
Train: 2018-08-09T11:29:32.444568: step 11415, loss 0.500606.
Train: 2018-08-09T11:29:32.522644: step 11416, loss 0.548395.
Train: 2018-08-09T11:29:32.600781: step 11417, loss 0.560726.
Train: 2018-08-09T11:29:32.678887: step 11418, loss 0.58745.
Train: 2018-08-09T11:29:32.756965: step 11419, loss 0.426835.
Train: 2018-08-09T11:29:32.850723: step 11420, loss 0.611284.
Test: 2018-08-09T11:29:33.337748: step 11420, loss 0.545829.
Train: 2018-08-09T11:29:33.415885: step 11421, loss 0.602456.
Train: 2018-08-09T11:29:33.509612: step 11422, loss 0.528888.
Train: 2018-08-09T11:29:33.587719: step 11423, loss 0.439017.
Train: 2018-08-09T11:29:33.665825: step 11424, loss 0.597394.
Train: 2018-08-09T11:29:33.743933: step 11425, loss 0.671599.
Train: 2018-08-09T11:29:33.822039: step 11426, loss 0.700348.
Train: 2018-08-09T11:29:33.900148: step 11427, loss 0.546558.
Train: 2018-08-09T11:29:33.993845: step 11428, loss 0.479471.
Train: 2018-08-09T11:29:34.071981: step 11429, loss 0.597357.
Train: 2018-08-09T11:29:34.150089: step 11430, loss 0.6129.
Test: 2018-08-09T11:29:34.649940: step 11430, loss 0.547812.
Train: 2018-08-09T11:29:34.728076: step 11431, loss 0.627734.
Train: 2018-08-09T11:29:34.806182: step 11432, loss 0.675491.
Train: 2018-08-09T11:29:34.884296: step 11433, loss 0.610882.
Train: 2018-08-09T11:29:34.963708: step 11434, loss 0.594861.
Train: 2018-08-09T11:29:35.057436: step 11435, loss 0.499974.
Train: 2018-08-09T11:29:35.135574: step 11436, loss 0.531982.
Train: 2018-08-09T11:29:35.213682: step 11437, loss 0.515788.
Train: 2018-08-09T11:29:35.291786: step 11438, loss 0.630218.
Train: 2018-08-09T11:29:35.369889: step 11439, loss 0.547873.
Train: 2018-08-09T11:29:35.463623: step 11440, loss 0.516479.
Test: 2018-08-09T11:29:35.947852: step 11440, loss 0.551144.
Train: 2018-08-09T11:29:36.041610: step 11441, loss 0.625553.
Train: 2018-08-09T11:29:36.119715: step 11442, loss 0.594478.
Train: 2018-08-09T11:29:36.197823: step 11443, loss 0.532402.
Train: 2018-08-09T11:29:36.285525: step 11444, loss 0.470637.
Train: 2018-08-09T11:29:36.363662: step 11445, loss 0.485876.
Train: 2018-08-09T11:29:36.441740: step 11446, loss 0.516749.
Train: 2018-08-09T11:29:36.519879: step 11447, loss 0.594534.
Train: 2018-08-09T11:29:36.597952: step 11448, loss 0.531926.
Train: 2018-08-09T11:29:36.676089: step 11449, loss 0.563262.
Train: 2018-08-09T11:29:36.769785: step 11450, loss 0.610386.
Test: 2018-08-09T11:29:37.261305: step 11450, loss 0.548298.
Train: 2018-08-09T11:29:37.339413: step 11451, loss 0.563048.
Train: 2018-08-09T11:29:37.417549: step 11452, loss 0.610542.
Train: 2018-08-09T11:29:37.495626: step 11453, loss 0.483959.
Train: 2018-08-09T11:29:37.589383: step 11454, loss 0.57883.
Train: 2018-08-09T11:29:37.667489: step 11455, loss 0.515347.
Train: 2018-08-09T11:29:37.745598: step 11456, loss 0.546938.
Train: 2018-08-09T11:29:37.823702: step 11457, loss 0.49896.
Train: 2018-08-09T11:29:37.901809: step 11458, loss 0.594912.
Train: 2018-08-09T11:29:37.979886: step 11459, loss 0.450154.
Train: 2018-08-09T11:29:38.057992: step 11460, loss 0.595115.
Test: 2018-08-09T11:29:38.557875: step 11460, loss 0.549283.
Train: 2018-08-09T11:29:38.635982: step 11461, loss 0.578959.
Train: 2018-08-09T11:29:38.729739: step 11462, loss 0.595263.
Train: 2018-08-09T11:29:38.807818: step 11463, loss 0.513678.
Train: 2018-08-09T11:29:38.885956: step 11464, loss 0.529817.
Train: 2018-08-09T11:29:38.964030: step 11465, loss 0.513196.
Train: 2018-08-09T11:29:39.057788: step 11466, loss 0.628836.
Train: 2018-08-09T11:29:39.135897: step 11467, loss 0.661588.
Train: 2018-08-09T11:29:39.214000: step 11468, loss 0.513027.
Train: 2018-08-09T11:29:39.292110: step 11469, loss 0.578952.
Train: 2018-08-09T11:29:39.370214: step 11470, loss 0.645103.
Test: 2018-08-09T11:29:39.870093: step 11470, loss 0.548928.
Train: 2018-08-09T11:29:39.980902: step 11471, loss 0.562496.
Train: 2018-08-09T11:29:40.059008: step 11472, loss 0.562615.
Train: 2018-08-09T11:29:40.137115: step 11473, loss 0.529591.
Train: 2018-08-09T11:29:40.230812: step 11474, loss 0.595408.
Train: 2018-08-09T11:29:40.308919: step 11475, loss 0.644727.
Train: 2018-08-09T11:29:40.387057: step 11476, loss 0.644578.
Train: 2018-08-09T11:29:40.465163: step 11477, loss 0.595305.
Train: 2018-08-09T11:29:40.543239: step 11478, loss 0.513767.
Train: 2018-08-09T11:29:40.636968: step 11479, loss 0.51391.
Train: 2018-08-09T11:29:40.715074: step 11480, loss 0.578934.
Test: 2018-08-09T11:29:41.199360: step 11480, loss 0.549962.
Train: 2018-08-09T11:29:41.293062: step 11481, loss 0.62751.
Train: 2018-08-09T11:29:41.371202: step 11482, loss 0.54655.
Train: 2018-08-09T11:29:41.449278: step 11483, loss 0.498195.
Train: 2018-08-09T11:29:41.527412: step 11484, loss 0.57887.
Train: 2018-08-09T11:29:41.605517: step 11485, loss 0.546646.
Train: 2018-08-09T11:29:41.699218: step 11486, loss 0.595018.
Train: 2018-08-09T11:29:41.777323: step 11487, loss 0.466143.
Train: 2018-08-09T11:29:41.855429: step 11488, loss 0.466052.
Train: 2018-08-09T11:29:41.935103: step 11489, loss 0.514254.
Train: 2018-08-09T11:29:42.013178: step 11490, loss 0.49781.
Test: 2018-08-09T11:29:42.513063: step 11490, loss 0.549841.
Train: 2018-08-09T11:29:42.591197: step 11491, loss 0.546325.
Train: 2018-08-09T11:29:42.669273: step 11492, loss 0.513503.
Train: 2018-08-09T11:29:42.763004: step 11493, loss 0.611779.
Train: 2018-08-09T11:29:42.841108: step 11494, loss 0.612093.
Train: 2018-08-09T11:29:42.919245: step 11495, loss 0.562424.
Train: 2018-08-09T11:29:42.997356: step 11496, loss 0.529447.
Train: 2018-08-09T11:29:43.075429: step 11497, loss 0.579162.
Train: 2018-08-09T11:29:43.153565: step 11498, loss 0.579079.
Train: 2018-08-09T11:29:43.247297: step 11499, loss 0.562562.
Train: 2018-08-09T11:29:43.325370: step 11500, loss 0.512638.
Test: 2018-08-09T11:29:43.825251: step 11500, loss 0.548723.
Train: 2018-08-09T11:29:44.373440: step 11501, loss 0.562399.
Train: 2018-08-09T11:29:44.467166: step 11502, loss 0.495735.
Train: 2018-08-09T11:29:44.545244: step 11503, loss 0.579089.
Train: 2018-08-09T11:29:44.623376: step 11504, loss 0.629798.
Train: 2018-08-09T11:29:44.701481: step 11505, loss 0.646135.
Train: 2018-08-09T11:29:44.795215: step 11506, loss 0.61264.
Train: 2018-08-09T11:29:44.873321: step 11507, loss 0.579127.
Train: 2018-08-09T11:29:44.951428: step 11508, loss 0.612186.
Train: 2018-08-09T11:29:45.029536: step 11509, loss 0.579081.
Train: 2018-08-09T11:29:45.107642: step 11510, loss 0.546008.
Test: 2018-08-09T11:29:45.607493: step 11510, loss 0.548334.
Train: 2018-08-09T11:29:45.685600: step 11511, loss 0.595453.
Train: 2018-08-09T11:29:45.763736: step 11512, loss 0.529681.
Train: 2018-08-09T11:29:45.841846: step 11513, loss 0.61175.
Train: 2018-08-09T11:29:45.936225: step 11514, loss 0.595276.
Train: 2018-08-09T11:29:46.014332: step 11515, loss 0.497379.
Train: 2018-08-09T11:29:46.092441: step 11516, loss 0.627757.
Train: 2018-08-09T11:29:46.170546: step 11517, loss 0.530153.
Train: 2018-08-09T11:29:46.248623: step 11518, loss 0.514067.
Train: 2018-08-09T11:29:46.342380: step 11519, loss 0.5789.
Train: 2018-08-09T11:29:46.420457: step 11520, loss 0.643673.
Test: 2018-08-09T11:29:46.904743: step 11520, loss 0.550031.
Train: 2018-08-09T11:29:46.982855: step 11521, loss 0.514234.
Train: 2018-08-09T11:29:47.076584: step 11522, loss 0.562729.
Train: 2018-08-09T11:29:47.154688: step 11523, loss 0.594986.
Train: 2018-08-09T11:29:47.232764: step 11524, loss 0.59497.
Train: 2018-08-09T11:29:47.310873: step 11525, loss 0.530634.
Train: 2018-08-09T11:29:47.404631: step 11526, loss 0.61104.
Train: 2018-08-09T11:29:47.482736: step 11527, loss 0.594944.
Train: 2018-08-09T11:29:47.560812: step 11528, loss 0.562807.
Train: 2018-08-09T11:29:47.638946: step 11529, loss 0.610849.
Train: 2018-08-09T11:29:47.717026: step 11530, loss 0.610781.
Test: 2018-08-09T11:29:48.216910: step 11530, loss 0.548651.
Train: 2018-08-09T11:29:48.295045: step 11531, loss 0.51525.
Train: 2018-08-09T11:29:48.373151: step 11532, loss 0.610696.
Train: 2018-08-09T11:29:48.466864: step 11533, loss 0.531301.
Train: 2018-08-09T11:29:48.544955: step 11534, loss 0.499729.
Train: 2018-08-09T11:29:48.623093: step 11535, loss 0.515479.
Train: 2018-08-09T11:29:48.701169: step 11536, loss 0.594687.
Train: 2018-08-09T11:29:48.779275: step 11537, loss 0.515008.
Train: 2018-08-09T11:29:48.857383: step 11538, loss 0.610559.
Train: 2018-08-09T11:29:48.953404: step 11539, loss 0.514378.
Train: 2018-08-09T11:29:49.031506: step 11540, loss 0.481293.
Test: 2018-08-09T11:29:49.531359: step 11540, loss 0.551508.
Train: 2018-08-09T11:29:49.609495: step 11541, loss 0.644447.
Train: 2018-08-09T11:29:49.687602: step 11542, loss 0.514361.
Train: 2018-08-09T11:29:49.765680: step 11543, loss 0.599249.
Train: 2018-08-09T11:29:49.843816: step 11544, loss 0.546558.
Train: 2018-08-09T11:29:49.937514: step 11545, loss 0.493681.
Train: 2018-08-09T11:29:50.015652: step 11546, loss 0.59635.
Train: 2018-08-09T11:29:50.093757: step 11547, loss 0.577176.
Train: 2018-08-09T11:29:50.171863: step 11548, loss 0.576554.
Train: 2018-08-09T11:29:50.265562: step 11549, loss 0.628783.
Train: 2018-08-09T11:29:50.343697: step 11550, loss 0.629979.
Test: 2018-08-09T11:29:50.827953: step 11550, loss 0.549365.
Train: 2018-08-09T11:29:50.923115: step 11551, loss 0.598722.
Train: 2018-08-09T11:29:51.001252: step 11552, loss 0.5938.
Train: 2018-08-09T11:29:51.079329: step 11553, loss 0.612055.
Train: 2018-08-09T11:29:51.157434: step 11554, loss 0.482004.
Train: 2018-08-09T11:29:51.242592: step 11555, loss 0.498566.
Train: 2018-08-09T11:29:51.320699: step 11556, loss 0.482167.
Train: 2018-08-09T11:29:51.398836: step 11557, loss 0.562556.
Train: 2018-08-09T11:29:51.476943: step 11558, loss 0.51463.
Train: 2018-08-09T11:29:51.555043: step 11559, loss 0.513229.
Train: 2018-08-09T11:29:51.633156: step 11560, loss 0.546616.
Test: 2018-08-09T11:29:52.133009: step 11560, loss 0.549792.
Train: 2018-08-09T11:29:52.211114: step 11561, loss 0.56128.
Train: 2018-08-09T11:29:52.304842: step 11562, loss 0.579203.
Train: 2018-08-09T11:29:52.382979: step 11563, loss 0.580105.
Train: 2018-08-09T11:29:52.461085: step 11564, loss 0.62842.
Train: 2018-08-09T11:29:52.539161: step 11565, loss 0.464111.
Train: 2018-08-09T11:29:52.617269: step 11566, loss 0.627163.
Train: 2018-08-09T11:29:52.695375: step 11567, loss 0.629184.
Train: 2018-08-09T11:29:52.789133: step 11568, loss 0.528996.
Train: 2018-08-09T11:29:52.867240: step 11569, loss 0.562599.
Train: 2018-08-09T11:29:52.945346: step 11570, loss 0.594951.
Test: 2018-08-09T11:29:53.445200: step 11570, loss 0.548182.
Train: 2018-08-09T11:29:53.523336: step 11571, loss 0.595883.
Train: 2018-08-09T11:29:53.601442: step 11572, loss 0.478631.
Train: 2018-08-09T11:29:53.679549: step 11573, loss 0.528863.
Train: 2018-08-09T11:29:53.773277: step 11574, loss 0.56484.
Train: 2018-08-09T11:29:53.851353: step 11575, loss 0.628457.
Train: 2018-08-09T11:29:53.929459: step 11576, loss 0.626223.
Train: 2018-08-09T11:29:54.007596: step 11577, loss 0.514154.
Train: 2018-08-09T11:29:54.101293: step 11578, loss 0.529439.
Train: 2018-08-09T11:29:54.179431: step 11579, loss 0.596889.
Train: 2018-08-09T11:29:54.257537: step 11580, loss 0.563746.
Test: 2018-08-09T11:29:54.741769: step 11580, loss 0.549076.
Train: 2018-08-09T11:29:54.835495: step 11581, loss 0.481845.
Train: 2018-08-09T11:29:54.915887: step 11582, loss 0.514738.
Train: 2018-08-09T11:29:54.994027: step 11583, loss 0.529573.
Train: 2018-08-09T11:29:55.072132: step 11584, loss 0.595593.
Train: 2018-08-09T11:29:55.150209: step 11585, loss 0.696378.
Train: 2018-08-09T11:29:55.243967: step 11586, loss 0.595141.
Train: 2018-08-09T11:29:55.322072: step 11587, loss 0.563434.
Train: 2018-08-09T11:29:55.400148: step 11588, loss 0.498024.
Train: 2018-08-09T11:29:55.478286: step 11589, loss 0.530712.
Train: 2018-08-09T11:29:55.556396: step 11590, loss 0.610593.
Test: 2018-08-09T11:29:56.056245: step 11590, loss 0.550124.
Train: 2018-08-09T11:29:56.134382: step 11591, loss 0.514662.
Train: 2018-08-09T11:29:56.212482: step 11592, loss 0.467386.
Train: 2018-08-09T11:29:56.306185: step 11593, loss 0.595374.
Train: 2018-08-09T11:29:56.384292: step 11594, loss 0.448975.
Train: 2018-08-09T11:29:56.462431: step 11595, loss 0.497247.
Train: 2018-08-09T11:29:56.540536: step 11596, loss 0.447381.
Train: 2018-08-09T11:29:56.618642: step 11597, loss 0.560574.
Train: 2018-08-09T11:29:56.696718: step 11598, loss 0.524497.
Train: 2018-08-09T11:29:56.790472: step 11599, loss 0.527916.
Train: 2018-08-09T11:29:56.868555: step 11600, loss 0.423621.
Test: 2018-08-09T11:29:57.355247: step 11600, loss 0.545103.
Train: 2018-08-09T11:29:57.917630: step 11601, loss 0.592113.
Train: 2018-08-09T11:29:57.995737: step 11602, loss 0.544635.
Train: 2018-08-09T11:29:58.089438: step 11603, loss 0.526149.
Train: 2018-08-09T11:29:58.167571: step 11604, loss 0.503414.
Train: 2018-08-09T11:29:58.245678: step 11605, loss 0.577423.
Train: 2018-08-09T11:29:58.323755: step 11606, loss 0.532741.
Train: 2018-08-09T11:29:58.417512: step 11607, loss 0.561198.
Train: 2018-08-09T11:29:58.495619: step 11608, loss 0.525258.
Train: 2018-08-09T11:29:58.573696: step 11609, loss 0.551945.
Train: 2018-08-09T11:29:58.651831: step 11610, loss 0.580576.
Test: 2018-08-09T11:29:59.151685: step 11610, loss 0.549814.
Train: 2018-08-09T11:29:59.229790: step 11611, loss 0.624282.
Train: 2018-08-09T11:29:59.307927: step 11612, loss 0.546147.
Train: 2018-08-09T11:29:59.401655: step 11613, loss 0.511779.
Train: 2018-08-09T11:29:59.479762: step 11614, loss 0.614119.
Train: 2018-08-09T11:29:59.557869: step 11615, loss 0.545712.
Train: 2018-08-09T11:29:59.635946: step 11616, loss 0.646188.
Train: 2018-08-09T11:29:59.714050: step 11617, loss 0.579205.
Train: 2018-08-09T11:29:59.807803: step 11618, loss 0.545852.
Train: 2018-08-09T11:29:59.885913: step 11619, loss 0.462887.
Train: 2018-08-09T11:29:59.963995: step 11620, loss 0.67856.
Test: 2018-08-09T11:30:00.462815: step 11620, loss 0.54886.
Train: 2018-08-09T11:30:00.540951: step 11621, loss 0.595576.
Train: 2018-08-09T11:30:00.619073: step 11622, loss 0.628503.
Train: 2018-08-09T11:30:00.697134: step 11623, loss 0.546097.
Train: 2018-08-09T11:30:00.775271: step 11624, loss 0.595348.
Train: 2018-08-09T11:30:00.868999: step 11625, loss 0.546264.
Train: 2018-08-09T11:30:00.935048: step 11626, loss 0.578923.
Train: 2018-08-09T11:30:01.028769: step 11627, loss 0.432671.
Train: 2018-08-09T11:30:01.106876: step 11628, loss 0.448947.
Train: 2018-08-09T11:30:01.184987: step 11629, loss 0.513822.
Train: 2018-08-09T11:30:01.263092: step 11630, loss 0.660484.
Test: 2018-08-09T11:30:01.762942: step 11630, loss 0.5467.
Train: 2018-08-09T11:30:01.841078: step 11631, loss 0.660529.
Train: 2018-08-09T11:30:01.919187: step 11632, loss 0.562605.
Train: 2018-08-09T11:30:01.997262: step 11633, loss 0.562662.
Train: 2018-08-09T11:30:02.091019: step 11634, loss 0.497511.
Train: 2018-08-09T11:30:02.169094: step 11635, loss 0.595198.
Train: 2018-08-09T11:30:02.247233: step 11636, loss 0.643932.
Train: 2018-08-09T11:30:02.325343: step 11637, loss 0.644159.
Train: 2018-08-09T11:30:02.403447: step 11638, loss 0.595154.
Train: 2018-08-09T11:30:02.497174: step 11639, loss 0.48202.
Train: 2018-08-09T11:30:02.575280: step 11640, loss 0.578874.
Test: 2018-08-09T11:30:03.060892: step 11640, loss 0.548913.
Train: 2018-08-09T11:30:03.154651: step 11641, loss 0.562772.
Train: 2018-08-09T11:30:03.232758: step 11642, loss 0.530642.
Train: 2018-08-09T11:30:03.310864: step 11643, loss 0.530668.
Train: 2018-08-09T11:30:03.388974: step 11644, loss 0.59494.
Train: 2018-08-09T11:30:03.467077: step 11645, loss 0.594936.
Train: 2018-08-09T11:30:03.560805: step 11646, loss 0.594922.
Train: 2018-08-09T11:30:03.638915: step 11647, loss 0.514753.
Train: 2018-08-09T11:30:03.717019: step 11648, loss 0.514755.
Train: 2018-08-09T11:30:03.795125: step 11649, loss 0.498686.
Train: 2018-08-09T11:30:03.873231: step 11650, loss 0.578876.
Test: 2018-08-09T11:30:04.373112: step 11650, loss 0.549533.
Train: 2018-08-09T11:30:04.451191: step 11651, loss 0.514528.
Train: 2018-08-09T11:30:04.544949: step 11652, loss 0.562754.
Train: 2018-08-09T11:30:04.623056: step 11653, loss 0.578872.
Train: 2018-08-09T11:30:04.701161: step 11654, loss 0.578893.
Train: 2018-08-09T11:30:04.779237: step 11655, loss 0.514093.
Train: 2018-08-09T11:30:04.857370: step 11656, loss 0.578928.
Train: 2018-08-09T11:30:04.937882: step 11657, loss 0.676443.
Train: 2018-08-09T11:30:05.016024: step 11658, loss 0.497664.
Train: 2018-08-09T11:30:05.109746: step 11659, loss 0.611453.
Train: 2018-08-09T11:30:05.187853: step 11660, loss 0.432575.
Test: 2018-08-09T11:30:05.687731: step 11660, loss 0.549818.
Train: 2018-08-09T11:30:05.765846: step 11661, loss 0.432255.
Train: 2018-08-09T11:30:05.843949: step 11662, loss 0.546179.
Train: 2018-08-09T11:30:05.922025: step 11663, loss 0.595833.
Train: 2018-08-09T11:30:06.015783: step 11664, loss 0.545777.
Train: 2018-08-09T11:30:06.093889: step 11665, loss 0.662527.
Train: 2018-08-09T11:30:06.171967: step 11666, loss 0.579054.
Train: 2018-08-09T11:30:06.250102: step 11667, loss 0.61228.
Train: 2018-08-09T11:30:06.343831: step 11668, loss 0.496368.
Train: 2018-08-09T11:30:06.421937: step 11669, loss 0.54593.
Train: 2018-08-09T11:30:06.500015: step 11670, loss 0.529346.
Test: 2018-08-09T11:30:07.000512: step 11670, loss 0.54696.
Train: 2018-08-09T11:30:07.078642: step 11671, loss 0.595547.
Train: 2018-08-09T11:30:07.156724: step 11672, loss 0.694928.
Train: 2018-08-09T11:30:07.250481: step 11673, loss 0.529541.
Train: 2018-08-09T11:30:07.328558: step 11674, loss 0.546002.
Train: 2018-08-09T11:30:07.406695: step 11675, loss 0.612136.
Train: 2018-08-09T11:30:07.484802: step 11676, loss 0.496736.
Train: 2018-08-09T11:30:07.562909: step 11677, loss 0.579023.
Train: 2018-08-09T11:30:07.656609: step 11678, loss 0.513264.
Train: 2018-08-09T11:30:07.734747: step 11679, loss 0.546087.
Train: 2018-08-09T11:30:07.812849: step 11680, loss 0.59542.
Test: 2018-08-09T11:30:08.314306: step 11680, loss 0.549627.
Train: 2018-08-09T11:30:08.392413: step 11681, loss 0.562542.
Train: 2018-08-09T11:30:08.470519: step 11682, loss 0.546111.
Train: 2018-08-09T11:30:08.548656: step 11683, loss 0.710438.
Train: 2018-08-09T11:30:08.626760: step 11684, loss 0.513386.
Train: 2018-08-09T11:30:08.720490: step 11685, loss 0.464354.
Train: 2018-08-09T11:30:08.798567: step 11686, loss 0.562577.
Train: 2018-08-09T11:30:08.876703: step 11687, loss 0.54619.
Train: 2018-08-09T11:30:08.954810: step 11688, loss 0.513395.
Train: 2018-08-09T11:30:09.032917: step 11689, loss 0.628202.
Train: 2018-08-09T11:30:09.126665: step 11690, loss 0.59538.
Test: 2018-08-09T11:30:09.610874: step 11690, loss 0.547189.
Train: 2018-08-09T11:30:09.689006: step 11691, loss 0.562561.
Train: 2018-08-09T11:30:09.782733: step 11692, loss 0.693738.
Train: 2018-08-09T11:30:09.860817: step 11693, loss 0.611654.
Train: 2018-08-09T11:30:09.938923: step 11694, loss 0.530024.
Train: 2018-08-09T11:30:10.017059: step 11695, loss 0.676492.
Train: 2018-08-09T11:30:10.095167: step 11696, loss 0.562697.
Train: 2018-08-09T11:30:10.188894: step 11697, loss 0.514316.
Train: 2018-08-09T11:30:10.266972: step 11698, loss 0.6755.
Train: 2018-08-09T11:30:10.345076: step 11699, loss 0.562813.
Train: 2018-08-09T11:30:10.423214: step 11700, loss 0.594854.
Test: 2018-08-09T11:30:10.923066: step 11700, loss 0.551622.
Train: 2018-08-09T11:30:11.485459: step 11701, loss 0.531084.
Train: 2018-08-09T11:30:11.579191: step 11702, loss 0.594758.
Train: 2018-08-09T11:30:11.657293: step 11703, loss 0.531395.
Train: 2018-08-09T11:30:11.735374: step 11704, loss 0.594662.
Train: 2018-08-09T11:30:11.813481: step 11705, loss 0.594639.
Train: 2018-08-09T11:30:11.908715: step 11706, loss 0.547406.
Train: 2018-08-09T11:30:11.986854: step 11707, loss 0.735898.
Train: 2018-08-09T11:30:12.064958: step 11708, loss 0.62578.
Train: 2018-08-09T11:30:12.143068: step 11709, loss 0.563352.
Train: 2018-08-09T11:30:12.221142: step 11710, loss 0.563441.
Test: 2018-08-09T11:30:12.721048: step 11710, loss 0.550233.
Train: 2018-08-09T11:30:12.799161: step 11711, loss 0.53267.
Train: 2018-08-09T11:30:12.892888: step 11712, loss 0.517422.
Train: 2018-08-09T11:30:12.970967: step 11713, loss 0.532875.
Train: 2018-08-09T11:30:13.049101: step 11714, loss 0.486817.
Train: 2018-08-09T11:30:13.127208: step 11715, loss 0.594342.
Train: 2018-08-09T11:30:13.205319: step 11716, loss 0.532779.
Train: 2018-08-09T11:30:13.299042: step 11717, loss 0.594364.
Train: 2018-08-09T11:30:13.377149: step 11718, loss 0.517207.
Train: 2018-08-09T11:30:13.455256: step 11719, loss 0.501611.
Train: 2018-08-09T11:30:13.533356: step 11720, loss 0.501332.
Test: 2018-08-09T11:30:14.033767: step 11720, loss 0.549888.
Train: 2018-08-09T11:30:14.111903: step 11721, loss 0.578924.
Train: 2018-08-09T11:30:14.189979: step 11722, loss 0.53191.
Train: 2018-08-09T11:30:14.268118: step 11723, loss 0.515984.
Train: 2018-08-09T11:30:14.361815: step 11724, loss 0.531546.
Train: 2018-08-09T11:30:14.439952: step 11725, loss 0.611091.
Train: 2018-08-09T11:30:14.518058: step 11726, loss 0.594224.
Train: 2018-08-09T11:30:14.596134: step 11727, loss 0.594897.
Train: 2018-08-09T11:30:14.674272: step 11728, loss 0.627369.
Train: 2018-08-09T11:30:14.752347: step 11729, loss 0.579073.
Train: 2018-08-09T11:30:14.846100: step 11730, loss 0.610935.
Test: 2018-08-09T11:30:15.330336: step 11730, loss 0.548483.
Train: 2018-08-09T11:30:15.408468: step 11731, loss 0.482878.
Train: 2018-08-09T11:30:15.502195: step 11732, loss 0.578888.
Train: 2018-08-09T11:30:15.580309: step 11733, loss 0.658925.
Train: 2018-08-09T11:30:15.658408: step 11734, loss 0.562857.
Train: 2018-08-09T11:30:15.736490: step 11735, loss 0.626833.
Train: 2018-08-09T11:30:15.814598: step 11736, loss 0.515045.
Train: 2018-08-09T11:30:15.909750: step 11737, loss 0.594802.
Train: 2018-08-09T11:30:15.987888: step 11738, loss 0.610711.
Train: 2018-08-09T11:30:16.065999: step 11739, loss 0.52904.
Train: 2018-08-09T11:30:16.144095: step 11740, loss 0.626517.
Test: 2018-08-09T11:30:16.643953: step 11740, loss 0.54934.
Train: 2018-08-09T11:30:16.722060: step 11741, loss 0.547141.
Train: 2018-08-09T11:30:16.800196: step 11742, loss 0.5155.
Train: 2018-08-09T11:30:16.878298: step 11743, loss 0.610534.
Train: 2018-08-09T11:30:16.956378: step 11744, loss 0.499737.
Train: 2018-08-09T11:30:17.050106: step 11745, loss 0.610524.
Train: 2018-08-09T11:30:17.128244: step 11746, loss 0.642167.
Train: 2018-08-09T11:30:17.206353: step 11747, loss 0.642078.
Train: 2018-08-09T11:30:17.284427: step 11748, loss 0.610389.
Train: 2018-08-09T11:30:17.362567: step 11749, loss 0.516032.
Train: 2018-08-09T11:30:17.456292: step 11750, loss 0.547512.
Test: 2018-08-09T11:30:17.941888: step 11750, loss 0.549134.
Train: 2018-08-09T11:30:18.019996: step 11751, loss 0.547556.
Train: 2018-08-09T11:30:18.113717: step 11752, loss 0.484974.
Train: 2018-08-09T11:30:18.191823: step 11753, loss 0.578876.
Train: 2018-08-09T11:30:18.269934: step 11754, loss 0.688626.
Train: 2018-08-09T11:30:18.348035: step 11755, loss 0.531911.
Train: 2018-08-09T11:30:18.426149: step 11756, loss 0.531932.
Train: 2018-08-09T11:30:18.519869: step 11757, loss 0.578876.
Train: 2018-08-09T11:30:18.597982: step 11758, loss 0.500587.
Train: 2018-08-09T11:30:18.676089: step 11759, loss 0.563141.
Train: 2018-08-09T11:30:18.754196: step 11760, loss 0.562774.
Test: 2018-08-09T11:30:19.254049: step 11760, loss 0.550609.
Train: 2018-08-09T11:30:19.332185: step 11761, loss 0.611296.
Train: 2018-08-09T11:30:19.410293: step 11762, loss 0.499985.
Train: 2018-08-09T11:30:19.504021: step 11763, loss 0.515393.
Train: 2018-08-09T11:30:19.582126: step 11764, loss 0.563096.
Train: 2018-08-09T11:30:19.660201: step 11765, loss 0.691151.
Train: 2018-08-09T11:30:19.738310: step 11766, loss 0.546847.
Train: 2018-08-09T11:30:19.816444: step 11767, loss 0.595255.
Train: 2018-08-09T11:30:19.894521: step 11768, loss 0.563283.
Train: 2018-08-09T11:30:19.973975: step 11769, loss 0.610645.
Train: 2018-08-09T11:30:20.067676: step 11770, loss 0.546981.
Test: 2018-08-09T11:30:20.567559: step 11770, loss 0.550001.
Train: 2018-08-09T11:30:20.645693: step 11771, loss 0.61042.
Train: 2018-08-09T11:30:20.723772: step 11772, loss 0.563033.
Train: 2018-08-09T11:30:20.817501: step 11773, loss 0.515856.
Train: 2018-08-09T11:30:20.895637: step 11774, loss 0.59462.
Train: 2018-08-09T11:30:20.973743: step 11775, loss 0.547301.
Train: 2018-08-09T11:30:21.051852: step 11776, loss 0.594624.
Train: 2018-08-09T11:30:21.129957: step 11777, loss 0.563089.
Train: 2018-08-09T11:30:21.223684: step 11778, loss 0.657738.
Train: 2018-08-09T11:30:21.301786: step 11779, loss 0.610345.
Train: 2018-08-09T11:30:21.379897: step 11780, loss 0.531762.
Test: 2018-08-09T11:30:21.864759: step 11780, loss 0.549088.
Train: 2018-08-09T11:30:21.958485: step 11781, loss 0.594559.
Train: 2018-08-09T11:30:22.036592: step 11782, loss 0.704153.
Train: 2018-08-09T11:30:22.114730: step 11783, loss 0.610091.
Train: 2018-08-09T11:30:22.192806: step 11784, loss 0.563374.
Train: 2018-08-09T11:30:22.270942: step 11785, loss 0.517005.
Train: 2018-08-09T11:30:22.364672: step 11786, loss 0.501702.
Train: 2018-08-09T11:30:22.442780: step 11787, loss 0.563502.
Train: 2018-08-09T11:30:22.520884: step 11788, loss 0.440043.
Train: 2018-08-09T11:30:22.598962: step 11789, loss 0.517061.
Train: 2018-08-09T11:30:22.677094: step 11790, loss 0.516857.
Test: 2018-08-09T11:30:23.176950: step 11790, loss 0.549323.
Train: 2018-08-09T11:30:23.270708: step 11791, loss 0.547753.
Train: 2018-08-09T11:30:23.348782: step 11792, loss 0.50072.
Train: 2018-08-09T11:30:23.426890: step 11793, loss 0.531752.
Train: 2018-08-09T11:30:23.505027: step 11794, loss 0.626218.
Train: 2018-08-09T11:30:23.583133: step 11795, loss 0.563019.
Train: 2018-08-09T11:30:23.676866: step 11796, loss 0.61064.
Train: 2018-08-09T11:30:23.754937: step 11797, loss 0.547013.
Train: 2018-08-09T11:30:23.833044: step 11798, loss 0.578856.
Train: 2018-08-09T11:30:23.912672: step 11799, loss 0.578859.
Train: 2018-08-09T11:30:23.990811: step 11800, loss 0.498799.
Test: 2018-08-09T11:30:24.490661: step 11800, loss 0.550806.
Train: 2018-08-09T11:30:25.068679: step 11801, loss 0.578883.
Train: 2018-08-09T11:30:25.146786: step 11802, loss 0.594976.
Train: 2018-08-09T11:30:25.224891: step 11803, loss 0.498341.
Train: 2018-08-09T11:30:25.318621: step 11804, loss 0.465863.
Train: 2018-08-09T11:30:25.396727: step 11805, loss 0.49787.
Train: 2018-08-09T11:30:25.474829: step 11806, loss 0.57894.
Train: 2018-08-09T11:30:25.552941: step 11807, loss 0.513532.
Train: 2018-08-09T11:30:25.631016: step 11808, loss 0.562518.
Train: 2018-08-09T11:30:25.724747: step 11809, loss 0.644958.
Train: 2018-08-09T11:30:25.802882: step 11810, loss 0.645089.
Test: 2018-08-09T11:30:26.289502: step 11810, loss 0.548263.
Train: 2018-08-09T11:30:26.367607: step 11811, loss 0.44687.
Train: 2018-08-09T11:30:26.461365: step 11812, loss 0.579045.
Train: 2018-08-09T11:30:26.539442: step 11813, loss 0.562497.
Train: 2018-08-09T11:30:26.617548: step 11814, loss 0.49599.
Train: 2018-08-09T11:30:26.695685: step 11815, loss 0.495768.
Train: 2018-08-09T11:30:26.773762: step 11816, loss 0.645945.
Train: 2018-08-09T11:30:26.867523: step 11817, loss 0.528864.
Train: 2018-08-09T11:30:26.945627: step 11818, loss 0.612938.
Train: 2018-08-09T11:30:27.023733: step 11819, loss 0.595971.
Train: 2018-08-09T11:30:27.101809: step 11820, loss 0.596146.
Test: 2018-08-09T11:30:27.601692: step 11820, loss 0.548615.
Train: 2018-08-09T11:30:27.679797: step 11821, loss 0.528962.
Train: 2018-08-09T11:30:27.757935: step 11822, loss 0.59585.
Train: 2018-08-09T11:30:27.851663: step 11823, loss 0.495616.
Train: 2018-08-09T11:30:27.931202: step 11824, loss 0.54572.
Train: 2018-08-09T11:30:28.009306: step 11825, loss 0.445455.
Train: 2018-08-09T11:30:28.087383: step 11826, loss 0.579152.
Train: 2018-08-09T11:30:28.165513: step 11827, loss 0.562403.
Train: 2018-08-09T11:30:28.243625: step 11828, loss 0.629547.
Train: 2018-08-09T11:30:28.337353: step 11829, loss 0.562398.
Train: 2018-08-09T11:30:28.415459: step 11830, loss 0.629527.
Test: 2018-08-09T11:30:28.915312: step 11830, loss 0.547321.
Train: 2018-08-09T11:30:28.993449: step 11831, loss 0.461793.
Train: 2018-08-09T11:30:29.071557: step 11832, loss 0.545611.
Train: 2018-08-09T11:30:29.149665: step 11833, loss 0.512038.
Train: 2018-08-09T11:30:29.227768: step 11834, loss 0.629557.
Train: 2018-08-09T11:30:29.321496: step 11835, loss 0.562306.
Train: 2018-08-09T11:30:29.399604: step 11836, loss 0.562352.
Train: 2018-08-09T11:30:29.477709: step 11837, loss 0.495171.
Train: 2018-08-09T11:30:29.560538: step 11838, loss 0.579377.
Train: 2018-08-09T11:30:29.638680: step 11839, loss 0.495021.
Train: 2018-08-09T11:30:29.732373: step 11840, loss 0.528443.
Test: 2018-08-09T11:30:30.216659: step 11840, loss 0.54972.
Train: 2018-08-09T11:30:30.310393: step 11841, loss 0.579236.
Train: 2018-08-09T11:30:30.388501: step 11842, loss 0.596465.
Train: 2018-08-09T11:30:30.466606: step 11843, loss 0.646524.
Train: 2018-08-09T11:30:30.544712: step 11844, loss 0.646833.
Train: 2018-08-09T11:30:30.622819: step 11845, loss 0.511845.
Train: 2018-08-09T11:30:30.716549: step 11846, loss 0.495029.
Train: 2018-08-09T11:30:30.794622: step 11847, loss 0.545918.
Train: 2018-08-09T11:30:30.872759: step 11848, loss 0.545871.
Train: 2018-08-09T11:30:30.950868: step 11849, loss 0.646035.
Train: 2018-08-09T11:30:31.028974: step 11850, loss 0.529017.
Test: 2018-08-09T11:30:31.528825: step 11850, loss 0.549875.
Train: 2018-08-09T11:30:31.606965: step 11851, loss 0.562544.
Train: 2018-08-09T11:30:31.700660: step 11852, loss 0.528871.
Train: 2018-08-09T11:30:31.778797: step 11853, loss 0.629351.
Train: 2018-08-09T11:30:31.864343: step 11854, loss 0.54578.
Train: 2018-08-09T11:30:31.936191: step 11855, loss 0.562469.
Train: 2018-08-09T11:30:32.014268: step 11856, loss 0.56243.
Train: 2018-08-09T11:30:32.092375: step 11857, loss 0.59565.
Train: 2018-08-09T11:30:32.186132: step 11858, loss 0.54591.
Train: 2018-08-09T11:30:32.264239: step 11859, loss 0.595583.
Train: 2018-08-09T11:30:32.342345: step 11860, loss 0.463421.
Test: 2018-08-09T11:30:32.842198: step 11860, loss 0.548277.
Train: 2018-08-09T11:30:32.920329: step 11861, loss 0.512962.
Train: 2018-08-09T11:30:32.998441: step 11862, loss 0.678166.
Train: 2018-08-09T11:30:33.076548: step 11863, loss 0.562504.
Train: 2018-08-09T11:30:33.154654: step 11864, loss 0.562516.
Train: 2018-08-09T11:30:33.248382: step 11865, loss 0.447256.
Train: 2018-08-09T11:30:33.326457: step 11866, loss 0.578998.
Train: 2018-08-09T11:30:33.404595: step 11867, loss 0.463567.
Train: 2018-08-09T11:30:33.482703: step 11868, loss 0.57901.
Train: 2018-08-09T11:30:33.560807: step 11869, loss 0.529374.
Train: 2018-08-09T11:30:33.654536: step 11870, loss 0.61224.
Test: 2018-08-09T11:30:34.154388: step 11870, loss 0.548158.
Train: 2018-08-09T11:30:34.232525: step 11871, loss 0.595626.
Train: 2018-08-09T11:30:34.310601: step 11872, loss 0.678654.
Train: 2018-08-09T11:30:34.388738: step 11873, loss 0.62881.
Train: 2018-08-09T11:30:34.466815: step 11874, loss 0.512951.
Train: 2018-08-09T11:30:34.544947: step 11875, loss 0.480096.
Train: 2018-08-09T11:30:34.638680: step 11876, loss 0.52956.
Train: 2018-08-09T11:30:34.716785: step 11877, loss 0.611964.
Train: 2018-08-09T11:30:34.794892: step 11878, loss 0.62841.
Train: 2018-08-09T11:30:34.872969: step 11879, loss 0.595426.
Train: 2018-08-09T11:30:34.952650: step 11880, loss 0.513325.
Test: 2018-08-09T11:30:35.452502: step 11880, loss 0.547207.
Train: 2018-08-09T11:30:35.530632: step 11881, loss 0.562567.
Train: 2018-08-09T11:30:35.624368: step 11882, loss 0.578952.
Train: 2018-08-09T11:30:35.702474: step 11883, loss 0.628015.
Train: 2018-08-09T11:30:35.780550: step 11884, loss 0.54629.
Train: 2018-08-09T11:30:35.858657: step 11885, loss 0.497455.
Train: 2018-08-09T11:30:35.936795: step 11886, loss 0.513775.
Train: 2018-08-09T11:30:36.014869: step 11887, loss 0.513749.
Train: 2018-08-09T11:30:36.108627: step 11888, loss 0.562616.
Train: 2018-08-09T11:30:36.186703: step 11889, loss 0.546273.
Train: 2018-08-09T11:30:36.264810: step 11890, loss 0.529889.
Test: 2018-08-09T11:30:36.764692: step 11890, loss 0.548467.
Train: 2018-08-09T11:30:36.842830: step 11891, loss 0.644468.
Train: 2018-08-09T11:30:36.922359: step 11892, loss 0.677237.
Train: 2018-08-09T11:30:37.000498: step 11893, loss 0.51354.
Train: 2018-08-09T11:30:37.094223: step 11894, loss 0.497252.
Train: 2018-08-09T11:30:37.172330: step 11895, loss 0.513569.
Train: 2018-08-09T11:30:37.250435: step 11896, loss 0.578947.
Train: 2018-08-09T11:30:37.328557: step 11897, loss 0.56258.
Train: 2018-08-09T11:30:37.406650: step 11898, loss 0.562574.
Train: 2018-08-09T11:30:37.484757: step 11899, loss 0.562567.
Train: 2018-08-09T11:30:37.578493: step 11900, loss 0.595365.
Test: 2018-08-09T11:30:38.062716: step 11900, loss 0.549679.
Train: 2018-08-09T11:30:38.625112: step 11901, loss 0.628152.
Train: 2018-08-09T11:30:38.703218: step 11902, loss 0.513439.
Train: 2018-08-09T11:30:38.796916: step 11903, loss 0.611695.
Train: 2018-08-09T11:30:38.875053: step 11904, loss 0.448101.
Train: 2018-08-09T11:30:38.954482: step 11905, loss 0.628049.
Train: 2018-08-09T11:30:39.032622: step 11906, loss 0.628056.
Train: 2018-08-09T11:30:39.110724: step 11907, loss 0.57895.
Train: 2018-08-09T11:30:39.204452: step 11908, loss 0.546245.
Train: 2018-08-09T11:30:39.282559: step 11909, loss 0.432196.
Train: 2018-08-09T11:30:39.360665: step 11910, loss 0.660689.
Test: 2018-08-09T11:30:39.860518: step 11910, loss 0.549169.
Train: 2018-08-09T11:30:39.934569: step 11911, loss 0.595274.
Train: 2018-08-09T11:30:40.012707: step 11912, loss 0.497459.
Train: 2018-08-09T11:30:40.106434: step 11913, loss 0.644109.
Train: 2018-08-09T11:30:40.184509: step 11914, loss 0.481267.
Train: 2018-08-09T11:30:40.262647: step 11915, loss 0.595196.
Train: 2018-08-09T11:30:40.340753: step 11916, loss 0.481276.
Train: 2018-08-09T11:30:40.418830: step 11917, loss 0.546339.
Train: 2018-08-09T11:30:40.496937: step 11918, loss 0.529993.
Train: 2018-08-09T11:30:40.590663: step 11919, loss 0.54626.
Train: 2018-08-09T11:30:40.668800: step 11920, loss 0.513472.
Test: 2018-08-09T11:30:41.168654: step 11920, loss 0.548424.
Train: 2018-08-09T11:30:41.246789: step 11921, loss 0.480516.
Train: 2018-08-09T11:30:41.324896: step 11922, loss 0.529588.
Train: 2018-08-09T11:30:41.402974: step 11923, loss 0.512913.
Train: 2018-08-09T11:30:41.481109: step 11924, loss 0.678612.
Train: 2018-08-09T11:30:41.559217: step 11925, loss 0.595746.
Train: 2018-08-09T11:30:41.652944: step 11926, loss 0.662234.
Train: 2018-08-09T11:30:41.731021: step 11927, loss 0.579087.
Train: 2018-08-09T11:30:41.809127: step 11928, loss 0.52926.
Train: 2018-08-09T11:30:41.887234: step 11929, loss 0.562466.
Train: 2018-08-09T11:30:41.965371: step 11930, loss 0.529308.
Test: 2018-08-09T11:30:42.465224: step 11930, loss 0.547561.
Train: 2018-08-09T11:30:42.543359: step 11931, loss 0.579047.
Train: 2018-08-09T11:30:42.637087: step 11932, loss 0.51275.
Train: 2018-08-09T11:30:42.715193: step 11933, loss 0.595628.
Train: 2018-08-09T11:30:42.793271: step 11934, loss 0.661931.
Train: 2018-08-09T11:30:42.871377: step 11935, loss 0.52939.
Train: 2018-08-09T11:30:42.951754: step 11936, loss 0.645127.
Train: 2018-08-09T11:30:43.045449: step 11937, loss 0.480082.
Train: 2018-08-09T11:30:43.123585: step 11938, loss 0.677821.
Train: 2018-08-09T11:30:43.201692: step 11939, loss 0.529693.
Train: 2018-08-09T11:30:43.279793: step 11940, loss 0.529778.
Test: 2018-08-09T11:30:43.779650: step 11940, loss 0.545997.
Train: 2018-08-09T11:30:43.857756: step 11941, loss 0.578952.
Train: 2018-08-09T11:30:43.951516: step 11942, loss 0.497189.
Train: 2018-08-09T11:30:44.029592: step 11943, loss 0.69338.
Train: 2018-08-09T11:30:44.107699: step 11944, loss 0.595238.
Train: 2018-08-09T11:30:44.185805: step 11945, loss 0.530117.
Train: 2018-08-09T11:30:44.279564: step 11946, loss 0.48149.
Train: 2018-08-09T11:30:44.357670: step 11947, loss 0.692518.
Train: 2018-08-09T11:30:44.435776: step 11948, loss 0.562712.
Train: 2018-08-09T11:30:44.513881: step 11949, loss 0.578881.
Train: 2018-08-09T11:30:44.591989: step 11950, loss 0.482165.
Test: 2018-08-09T11:30:45.092557: step 11950, loss 0.551934.
Train: 2018-08-09T11:30:45.170691: step 11951, loss 0.611099.
Train: 2018-08-09T11:30:45.248767: step 11952, loss 0.498412.
Train: 2018-08-09T11:30:45.342526: step 11953, loss 0.562779.
Train: 2018-08-09T11:30:45.420600: step 11954, loss 0.5306.
Train: 2018-08-09T11:30:45.498737: step 11955, loss 0.562773.
Train: 2018-08-09T11:30:45.576845: step 11956, loss 0.562765.
Train: 2018-08-09T11:30:45.654951: step 11957, loss 0.546632.
Train: 2018-08-09T11:30:45.733058: step 11958, loss 0.562752.
Train: 2018-08-09T11:30:45.826780: step 11959, loss 0.498134.
Train: 2018-08-09T11:30:45.904862: step 11960, loss 0.578893.
Test: 2018-08-09T11:30:46.404745: step 11960, loss 0.550576.
Train: 2018-08-09T11:30:46.482851: step 11961, loss 0.611304.
Train: 2018-08-09T11:30:46.560958: step 11962, loss 0.5789.
Train: 2018-08-09T11:30:46.639096: step 11963, loss 0.546472.
Train: 2018-08-09T11:30:46.717201: step 11964, loss 0.546459.
Train: 2018-08-09T11:30:46.810897: step 11965, loss 0.49774.
Train: 2018-08-09T11:30:46.889004: step 11966, loss 0.546391.
Train: 2018-08-09T11:30:46.967111: step 11967, loss 0.530048.
Train: 2018-08-09T11:30:47.045217: step 11968, loss 0.627917.
Train: 2018-08-09T11:30:47.123351: step 11969, loss 0.57894.
Train: 2018-08-09T11:30:47.217082: step 11970, loss 0.578943.
Test: 2018-08-09T11:30:47.701337: step 11970, loss 0.549123.
Train: 2018-08-09T11:30:47.795040: step 11971, loss 0.480821.
Train: 2018-08-09T11:30:47.873147: step 11972, loss 0.578954.
Train: 2018-08-09T11:30:47.953540: step 11973, loss 0.513375.
Train: 2018-08-09T11:30:48.031647: step 11974, loss 0.562547.
Train: 2018-08-09T11:30:48.109753: step 11975, loss 0.595434.
Train: 2018-08-09T11:30:48.187860: step 11976, loss 0.513132.
Train: 2018-08-09T11:30:48.281587: step 11977, loss 0.579002.
Train: 2018-08-09T11:30:48.359727: step 11978, loss 0.562502.
Train: 2018-08-09T11:30:48.437800: step 11979, loss 0.496409.
Train: 2018-08-09T11:30:48.515938: step 11980, loss 0.56248.
Test: 2018-08-09T11:30:49.015790: step 11980, loss 0.54568.
Train: 2018-08-09T11:30:49.093926: step 11981, loss 0.52931.
Train: 2018-08-09T11:30:49.172032: step 11982, loss 0.562454.
Train: 2018-08-09T11:30:49.265762: step 11983, loss 0.628996.
Train: 2018-08-09T11:30:49.343868: step 11984, loss 0.579079.
Train: 2018-08-09T11:30:49.421943: step 11985, loss 0.612365.
Train: 2018-08-09T11:30:49.500051: step 11986, loss 0.529195.
Train: 2018-08-09T11:30:49.578188: step 11987, loss 0.562454.
Train: 2018-08-09T11:30:49.671915: step 11988, loss 0.562455.
Train: 2018-08-09T11:30:49.750025: step 11989, loss 0.545856.
Train: 2018-08-09T11:30:49.828097: step 11990, loss 0.512629.
Test: 2018-08-09T11:30:50.329345: step 11990, loss 0.548138.
Train: 2018-08-09T11:30:50.407481: step 11991, loss 0.562457.
Train: 2018-08-09T11:30:50.485588: step 11992, loss 0.529193.
Train: 2018-08-09T11:30:50.563666: step 11993, loss 0.5957.
Train: 2018-08-09T11:30:50.657421: step 11994, loss 0.629036.
Train: 2018-08-09T11:30:50.735528: step 11995, loss 0.562456.
Train: 2018-08-09T11:30:50.813635: step 11996, loss 0.545836.
Train: 2018-08-09T11:30:50.891742: step 11997, loss 0.612268.
Train: 2018-08-09T11:30:50.969819: step 11998, loss 0.628765.
Train: 2018-08-09T11:30:51.063547: step 11999, loss 0.545958.
Train: 2018-08-09T11:30:51.141653: step 12000, loss 0.513011.
Test: 2018-08-09T11:30:51.641535: step 12000, loss 0.547697.
Train: 2018-08-09T11:30:52.205568: step 12001, loss 0.546034.
Train: 2018-08-09T11:30:52.283676: step 12002, loss 0.529579.
Train: 2018-08-09T11:30:52.361815: step 12003, loss 0.61193.
Train: 2018-08-09T11:30:52.439919: step 12004, loss 0.56253.
Train: 2018-08-09T11:30:52.518026: step 12005, loss 0.661188.
Train: 2018-08-09T11:30:52.609672: step 12006, loss 0.546154.
Train: 2018-08-09T11:30:52.687813: step 12007, loss 0.497073.
Train: 2018-08-09T11:30:52.765919: step 12008, loss 0.595314.
Train: 2018-08-09T11:30:52.844029: step 12009, loss 0.480855.
Train: 2018-08-09T11:30:52.922146: step 12010, loss 0.415425.
Test: 2018-08-09T11:30:53.422017: step 12010, loss 0.550308.
Train: 2018-08-09T11:30:53.500124: step 12011, loss 0.562567.
Train: 2018-08-09T11:30:53.578229: step 12012, loss 0.710389.
Train: 2018-08-09T11:30:53.671960: step 12013, loss 0.546127.
Train: 2018-08-09T11:30:53.750033: step 12014, loss 0.546128.
Train: 2018-08-09T11:30:53.843761: step 12015, loss 0.529698.
Train: 2018-08-09T11:30:53.926314: step 12016, loss 0.546106.
Train: 2018-08-09T11:30:54.004417: step 12017, loss 0.644775.
Train: 2018-08-09T11:30:54.082524: step 12018, loss 0.513221.
Train: 2018-08-09T11:30:54.160625: step 12019, loss 0.562537.
Train: 2018-08-09T11:30:54.254352: step 12020, loss 0.578981.
Test: 2018-08-09T11:30:54.738590: step 12020, loss 0.549616.
Train: 2018-08-09T11:30:54.816726: step 12021, loss 0.661202.
Train: 2018-08-09T11:30:54.894833: step 12022, loss 0.57897.
Train: 2018-08-09T11:30:54.988555: step 12023, loss 0.562568.
Train: 2018-08-09T11:30:55.066667: step 12024, loss 0.529855.
Train: 2018-08-09T11:30:55.144774: step 12025, loss 0.546243.
Train: 2018-08-09T11:30:55.222850: step 12026, loss 0.49724.
Train: 2018-08-09T11:30:55.300991: step 12027, loss 0.562595.
Train: 2018-08-09T11:30:55.394711: step 12028, loss 0.513529.
Train: 2018-08-09T11:30:55.472791: step 12029, loss 0.578947.
Train: 2018-08-09T11:30:55.550932: step 12030, loss 0.52981.
Test: 2018-08-09T11:30:56.052202: step 12030, loss 0.552154.
Train: 2018-08-09T11:30:56.130340: step 12031, loss 0.529761.
Train: 2018-08-09T11:30:56.208445: step 12032, loss 0.578985.
Train: 2018-08-09T11:30:56.286546: step 12033, loss 0.48032.
Train: 2018-08-09T11:30:56.364659: step 12034, loss 0.611955.
Train: 2018-08-09T11:30:56.458358: step 12035, loss 0.529513.
Train: 2018-08-09T11:30:56.536464: step 12036, loss 0.612058.
Train: 2018-08-09T11:30:56.614595: step 12037, loss 0.562491.
Train: 2018-08-09T11:30:56.692706: step 12038, loss 0.562487.
Train: 2018-08-09T11:30:56.786436: step 12039, loss 0.579029.
Train: 2018-08-09T11:30:56.864511: step 12040, loss 0.633076.
Test: 2018-08-09T11:30:57.348796: step 12040, loss 0.548881.
Train: 2018-08-09T11:30:57.442524: step 12041, loss 0.595548.
Train: 2018-08-09T11:30:57.520636: step 12042, loss 0.595511.
Train: 2018-08-09T11:30:57.598712: step 12043, loss 0.529576.
Train: 2018-08-09T11:30:57.676850: step 12044, loss 0.595437.
Train: 2018-08-09T11:30:57.754926: step 12045, loss 0.562547.
Train: 2018-08-09T11:30:57.848655: step 12046, loss 0.595365.
Train: 2018-08-09T11:30:57.928259: step 12047, loss 0.529837.
Train: 2018-08-09T11:30:58.006331: step 12048, loss 0.595295.
Train: 2018-08-09T11:30:58.084439: step 12049, loss 0.480977.
Train: 2018-08-09T11:30:58.162575: step 12050, loss 0.513641.
Test: 2018-08-09T11:30:58.662428: step 12050, loss 0.548536.
Train: 2018-08-09T11:30:58.740564: step 12051, loss 0.529939.
Train: 2018-08-09T11:30:58.818640: step 12052, loss 0.546244.
Train: 2018-08-09T11:30:58.896748: step 12053, loss 0.480744.
Train: 2018-08-09T11:30:58.974886: step 12054, loss 0.480538.
Train: 2018-08-09T11:30:59.052990: step 12055, loss 0.513158.
Train: 2018-08-09T11:30:59.146688: step 12056, loss 0.529466.
Train: 2018-08-09T11:30:59.224819: step 12057, loss 0.595625.
Train: 2018-08-09T11:30:59.302925: step 12058, loss 0.579072.
Train: 2018-08-09T11:30:59.396630: step 12059, loss 0.645728.
Train: 2018-08-09T11:30:59.474770: step 12060, loss 0.495751.
Test: 2018-08-09T11:30:59.975973: step 12060, loss 0.548041.
Train: 2018-08-09T11:31:00.054110: step 12061, loss 0.645894.
Train: 2018-08-09T11:31:00.132218: step 12062, loss 0.56242.
Train: 2018-08-09T11:31:00.210320: step 12063, loss 0.595811.
Train: 2018-08-09T11:31:00.288430: step 12064, loss 0.629166.
Train: 2018-08-09T11:31:00.382126: step 12065, loss 0.545775.
Train: 2018-08-09T11:31:00.460264: step 12066, loss 0.579081.
Train: 2018-08-09T11:31:00.538371: step 12067, loss 0.562452.
Train: 2018-08-09T11:31:00.616446: step 12068, loss 0.628831.
Train: 2018-08-09T11:31:00.694581: step 12069, loss 0.612144.
Train: 2018-08-09T11:31:00.772660: step 12070, loss 0.612027.
Test: 2018-08-09T11:31:01.272543: step 12070, loss 0.549603.
Train: 2018-08-09T11:31:01.357328: step 12071, loss 0.546077.
Train: 2018-08-09T11:31:01.435434: step 12072, loss 0.562557.
Train: 2018-08-09T11:31:01.513512: step 12073, loss 0.595315.
Train: 2018-08-09T11:31:01.591648: step 12074, loss 0.644217.
Train: 2018-08-09T11:31:01.669754: step 12075, loss 0.627692.
Train: 2018-08-09T11:31:01.763483: step 12076, loss 0.562706.
Train: 2018-08-09T11:31:01.841589: step 12077, loss 0.54663.
Train: 2018-08-09T11:31:01.919695: step 12078, loss 0.594943.
Train: 2018-08-09T11:31:01.997805: step 12079, loss 0.578863.
Train: 2018-08-09T11:31:02.075881: step 12080, loss 0.435187.
Test: 2018-08-09T11:31:02.575762: step 12080, loss 0.547962.
Train: 2018-08-09T11:31:02.653868: step 12081, loss 0.515054.
Train: 2018-08-09T11:31:02.731974: step 12082, loss 0.626719.
Train: 2018-08-09T11:31:02.825733: step 12083, loss 0.499164.
Train: 2018-08-09T11:31:02.903842: step 12084, loss 0.562916.
Train: 2018-08-09T11:31:02.981946: step 12085, loss 0.499112.
Train: 2018-08-09T11:31:03.060054: step 12086, loss 0.626777.
Train: 2018-08-09T11:31:03.138158: step 12087, loss 0.610816.
Train: 2018-08-09T11:31:03.216265: step 12088, loss 0.56289.
Train: 2018-08-09T11:31:03.294342: step 12089, loss 0.483063.
Train: 2018-08-09T11:31:03.388101: step 12090, loss 0.546895.
Test: 2018-08-09T11:31:03.872545: step 12090, loss 0.547258.
Train: 2018-08-09T11:31:03.951942: step 12091, loss 0.562861.
Train: 2018-08-09T11:31:04.030044: step 12092, loss 0.546818.
Train: 2018-08-09T11:31:04.123767: step 12093, loss 0.610961.
Train: 2018-08-09T11:31:04.201879: step 12094, loss 0.546757.
Train: 2018-08-09T11:31:04.279984: step 12095, loss 0.59494.
Train: 2018-08-09T11:31:04.358093: step 12096, loss 0.627091.
Train: 2018-08-09T11:31:04.436169: step 12097, loss 0.562811.
Train: 2018-08-09T11:31:04.514304: step 12098, loss 0.546764.
Train: 2018-08-09T11:31:04.592413: step 12099, loss 0.514673.
Train: 2018-08-09T11:31:04.686109: step 12100, loss 0.546753.
Test: 2018-08-09T11:31:05.185992: step 12100, loss 0.548348.
Train: 2018-08-09T11:31:05.717147: step 12101, loss 0.49851.
Train: 2018-08-09T11:31:05.795253: step 12102, loss 0.466165.
Train: 2018-08-09T11:31:05.873355: step 12103, loss 0.530426.
Train: 2018-08-09T11:31:05.952155: step 12104, loss 0.611316.
Train: 2018-08-09T11:31:06.030231: step 12105, loss 0.611401.
Train: 2018-08-09T11:31:06.108367: step 12106, loss 0.546379.
Train: 2018-08-09T11:31:06.202067: step 12107, loss 0.676676.
Train: 2018-08-09T11:31:06.280197: step 12108, loss 0.627775.
Train: 2018-08-09T11:31:06.358309: step 12109, loss 0.578913.
Train: 2018-08-09T11:31:06.436415: step 12110, loss 0.627622.
Test: 2018-08-09T11:31:06.936267: step 12110, loss 0.546291.
Train: 2018-08-09T11:31:07.014374: step 12111, loss 0.481692.
Train: 2018-08-09T11:31:07.092510: step 12112, loss 0.562706.
Train: 2018-08-09T11:31:07.170618: step 12113, loss 0.675951.
Train: 2018-08-09T11:31:07.248718: step 12114, loss 0.61116.
Train: 2018-08-09T11:31:07.326834: step 12115, loss 0.659338.
Train: 2018-08-09T11:31:07.420559: step 12116, loss 0.594894.
Train: 2018-08-09T11:31:07.498634: step 12117, loss 0.515011.
Train: 2018-08-09T11:31:07.576741: step 12118, loss 0.547025.
Train: 2018-08-09T11:31:07.654880: step 12119, loss 0.610621.
Train: 2018-08-09T11:31:07.732985: step 12120, loss 0.594698.
Test: 2018-08-09T11:31:08.232837: step 12120, loss 0.549465.
Train: 2018-08-09T11:31:08.310973: step 12121, loss 0.547276.
Train: 2018-08-09T11:31:08.389080: step 12122, loss 0.578865.
Train: 2018-08-09T11:31:08.467188: step 12123, loss 0.500244.
Train: 2018-08-09T11:31:08.545262: step 12124, loss 0.610297.
Train: 2018-08-09T11:31:08.623400: step 12125, loss 0.54749.
Train: 2018-08-09T11:31:08.701508: step 12126, loss 0.531838.
Train: 2018-08-09T11:31:08.795234: step 12127, loss 0.500485.
Train: 2018-08-09T11:31:08.873341: step 12128, loss 0.516092.
Train: 2018-08-09T11:31:08.938121: step 12129, loss 0.531694.
Train: 2018-08-09T11:31:09.031841: step 12130, loss 0.547341.
Test: 2018-08-09T11:31:09.516110: step 12130, loss 0.54945.
Train: 2018-08-09T11:31:09.594215: step 12131, loss 0.547259.
Train: 2018-08-09T11:31:09.687913: step 12132, loss 0.578859.
Train: 2018-08-09T11:31:09.766050: step 12133, loss 0.531227.
Train: 2018-08-09T11:31:09.844157: step 12134, loss 0.594778.
Train: 2018-08-09T11:31:09.922263: step 12135, loss 0.626707.
Train: 2018-08-09T11:31:10.000370: step 12136, loss 0.562895.
Train: 2018-08-09T11:31:10.078475: step 12137, loss 0.610796.
Train: 2018-08-09T11:31:10.156581: step 12138, loss 0.514966.
Train: 2018-08-09T11:31:10.234690: step 12139, loss 0.49895.
Train: 2018-08-09T11:31:10.312795: step 12140, loss 0.530756.
Test: 2018-08-09T11:31:10.812648: step 12140, loss 0.549575.
Train: 2018-08-09T11:31:10.890784: step 12141, loss 0.562845.
Train: 2018-08-09T11:31:10.971201: step 12142, loss 0.514374.
Train: 2018-08-09T11:31:11.049278: step 12143, loss 0.627395.
Train: 2018-08-09T11:31:11.127414: step 12144, loss 0.514302.
Train: 2018-08-09T11:31:11.205492: step 12145, loss 0.562165.
Train: 2018-08-09T11:31:11.283626: step 12146, loss 0.578489.
Train: 2018-08-09T11:31:11.361733: step 12147, loss 0.530123.
Train: 2018-08-09T11:31:11.439840: step 12148, loss 0.547521.
Train: 2018-08-09T11:31:11.533539: step 12149, loss 0.56118.
Train: 2018-08-09T11:31:11.611676: step 12150, loss 0.56247.
Test: 2018-08-09T11:31:12.099530: step 12150, loss 0.54742.
Train: 2018-08-09T11:31:12.177619: step 12151, loss 0.427904.
Train: 2018-08-09T11:31:12.255726: step 12152, loss 0.561735.
Train: 2018-08-09T11:31:12.349484: step 12153, loss 0.462315.
Train: 2018-08-09T11:31:12.427590: step 12154, loss 0.703808.
Train: 2018-08-09T11:31:12.505697: step 12155, loss 0.491479.
Train: 2018-08-09T11:31:12.583805: step 12156, loss 0.515339.
Train: 2018-08-09T11:31:12.661904: step 12157, loss 0.546367.
Train: 2018-08-09T11:31:12.740016: step 12158, loss 0.578777.
Train: 2018-08-09T11:31:12.818094: step 12159, loss 0.559638.
Train: 2018-08-09T11:31:12.896223: step 12160, loss 0.577889.
Test: 2018-08-09T11:31:13.396083: step 12160, loss 0.546163.
Train: 2018-08-09T11:31:13.474218: step 12161, loss 0.475854.
Train: 2018-08-09T11:31:13.552325: step 12162, loss 0.577577.
Train: 2018-08-09T11:31:13.630432: step 12163, loss 0.510074.
Train: 2018-08-09T11:31:13.724160: step 12164, loss 0.527279.
Train: 2018-08-09T11:31:13.802235: step 12165, loss 0.634864.
Train: 2018-08-09T11:31:13.880374: step 12166, loss 0.561494.
Train: 2018-08-09T11:31:13.958448: step 12167, loss 0.458311.
Train: 2018-08-09T11:31:14.036590: step 12168, loss 0.632304.
Train: 2018-08-09T11:31:14.114694: step 12169, loss 0.578556.
Train: 2018-08-09T11:31:14.192769: step 12170, loss 0.613885.
Test: 2018-08-09T11:31:14.692651: step 12170, loss 0.548699.
Train: 2018-08-09T11:31:14.770782: step 12171, loss 0.597943.
Train: 2018-08-09T11:31:14.848865: step 12172, loss 0.494267.
Train: 2018-08-09T11:31:14.928426: step 12173, loss 0.48882.
Train: 2018-08-09T11:31:15.006532: step 12174, loss 0.5455.
Train: 2018-08-09T11:31:15.084640: step 12175, loss 0.494936.
Train: 2018-08-09T11:31:15.162742: step 12176, loss 0.594972.
Train: 2018-08-09T11:31:15.240855: step 12177, loss 0.580933.
Train: 2018-08-09T11:31:15.318960: step 12178, loss 0.530569.
Train: 2018-08-09T11:31:15.397067: step 12179, loss 0.682236.
Train: 2018-08-09T11:31:15.475143: step 12180, loss 0.528155.
Test: 2018-08-09T11:31:15.975053: step 12180, loss 0.548286.
Train: 2018-08-09T11:31:16.053161: step 12181, loss 0.611821.
Train: 2018-08-09T11:31:16.131237: step 12182, loss 0.582433.
Train: 2018-08-09T11:31:16.224997: step 12183, loss 0.581137.
Train: 2018-08-09T11:31:16.303071: step 12184, loss 0.445356.
Train: 2018-08-09T11:31:16.381178: step 12185, loss 0.59585.
Train: 2018-08-09T11:31:16.459285: step 12186, loss 0.694664.
Train: 2018-08-09T11:31:16.537391: step 12187, loss 0.579548.
Train: 2018-08-09T11:31:16.615531: step 12188, loss 0.529637.
Train: 2018-08-09T11:31:16.693636: step 12189, loss 0.562926.
Train: 2018-08-09T11:31:16.771712: step 12190, loss 0.513639.
Test: 2018-08-09T11:31:17.273068: step 12190, loss 0.549185.
Train: 2018-08-09T11:31:17.351150: step 12191, loss 0.644187.
Train: 2018-08-09T11:31:17.429256: step 12192, loss 0.562744.
Train: 2018-08-09T11:31:17.507394: step 12193, loss 0.627587.
Train: 2018-08-09T11:31:17.585500: step 12194, loss 0.546544.
Train: 2018-08-09T11:31:17.663576: step 12195, loss 0.530485.
Train: 2018-08-09T11:31:17.741710: step 12196, loss 0.562769.
Train: 2018-08-09T11:31:17.819819: step 12197, loss 0.675397.
Train: 2018-08-09T11:31:17.897927: step 12198, loss 0.659071.
Train: 2018-08-09T11:31:17.976003: step 12199, loss 0.594836.
Train: 2018-08-09T11:31:18.054110: step 12200, loss 0.499312.
Test: 2018-08-09T11:31:18.553992: step 12200, loss 0.549924.
Train: 2018-08-09T11:31:19.164596: step 12201, loss 0.594726.
Train: 2018-08-09T11:31:19.242671: step 12202, loss 0.499743.
Train: 2018-08-09T11:31:19.320810: step 12203, loss 0.547259.
Train: 2018-08-09T11:31:19.398915: step 12204, loss 0.468347.
Train: 2018-08-09T11:31:19.477021: step 12205, loss 0.531454.
Train: 2018-08-09T11:31:19.555129: step 12206, loss 0.626342.
Train: 2018-08-09T11:31:19.633235: step 12207, loss 0.547201.
Train: 2018-08-09T11:31:19.711341: step 12208, loss 0.499663.
Train: 2018-08-09T11:31:19.789417: step 12209, loss 0.64234.
Train: 2018-08-09T11:31:19.867525: step 12210, loss 0.531244.
Test: 2018-08-09T11:31:20.367437: step 12210, loss 0.55049.
Train: 2018-08-09T11:31:20.445513: step 12211, loss 0.531197.
Train: 2018-08-09T11:31:20.523650: step 12212, loss 0.56295.
Train: 2018-08-09T11:31:20.601726: step 12213, loss 0.57886.
Train: 2018-08-09T11:31:20.679832: step 12214, loss 0.483173.
Train: 2018-08-09T11:31:20.757971: step 12215, loss 0.642803.
Train: 2018-08-09T11:31:20.836077: step 12216, loss 0.642857.
Train: 2018-08-09T11:31:20.915585: step 12217, loss 0.530889.
Train: 2018-08-09T11:31:20.993720: step 12218, loss 0.610852.
Train: 2018-08-09T11:31:21.087419: step 12219, loss 0.530904.
Train: 2018-08-09T11:31:21.165557: step 12220, loss 0.578862.
Test: 2018-08-09T11:31:21.649796: step 12220, loss 0.546688.
Train: 2018-08-09T11:31:21.743540: step 12221, loss 0.530904.
Train: 2018-08-09T11:31:21.821621: step 12222, loss 0.658833.
Train: 2018-08-09T11:31:21.899730: step 12223, loss 0.642773.
Train: 2018-08-09T11:31:21.977842: step 12224, loss 0.626692.
Train: 2018-08-09T11:31:22.055976: step 12225, loss 0.578858.
Train: 2018-08-09T11:31:22.134080: step 12226, loss 0.483726.
Train: 2018-08-09T11:31:22.212184: step 12227, loss 0.547183.
Train: 2018-08-09T11:31:22.292295: step 12228, loss 0.48389.
Train: 2018-08-09T11:31:22.370403: step 12229, loss 0.547177.
Train: 2018-08-09T11:31:22.448503: step 12230, loss 0.594717.
Test: 2018-08-09T11:31:22.948360: step 12230, loss 0.549324.
Train: 2018-08-09T11:31:23.026494: step 12231, loss 0.562991.
Train: 2018-08-09T11:31:23.104575: step 12232, loss 0.420087.
Train: 2018-08-09T11:31:23.182708: step 12233, loss 0.562934.
Train: 2018-08-09T11:31:23.260817: step 12234, loss 0.658704.
Train: 2018-08-09T11:31:23.338924: step 12235, loss 0.514932.
Train: 2018-08-09T11:31:23.432646: step 12236, loss 0.498817.
Train: 2018-08-09T11:31:23.510729: step 12237, loss 0.530715.
Train: 2018-08-09T11:31:23.588867: step 12238, loss 0.546679.
Train: 2018-08-09T11:31:23.666942: step 12239, loss 0.627317.
Train: 2018-08-09T11:31:23.745047: step 12240, loss 0.546548.
Test: 2018-08-09T11:31:24.229338: step 12240, loss 0.548744.
Train: 2018-08-09T11:31:24.307415: step 12241, loss 0.611298.
Train: 2018-08-09T11:31:24.385552: step 12242, loss 0.578902.
Train: 2018-08-09T11:31:24.463656: step 12243, loss 0.481576.
Train: 2018-08-09T11:31:24.541760: step 12244, loss 0.595159.
Train: 2018-08-09T11:31:24.619876: step 12245, loss 0.562648.
Train: 2018-08-09T11:31:24.697948: step 12246, loss 0.611485.
Train: 2018-08-09T11:31:24.776089: step 12247, loss 0.432356.
Train: 2018-08-09T11:31:24.854192: step 12248, loss 0.513655.
Train: 2018-08-09T11:31:24.933679: step 12249, loss 0.52986.
Train: 2018-08-09T11:31:25.027403: step 12250, loss 0.578966.
Test: 2018-08-09T11:31:25.511669: step 12250, loss 0.5521.
Train: 2018-08-09T11:31:25.589769: step 12251, loss 0.595428.
Train: 2018-08-09T11:31:25.667851: step 12252, loss 0.546049.
Train: 2018-08-09T11:31:25.761604: step 12253, loss 0.513012.
Train: 2018-08-09T11:31:25.839685: step 12254, loss 0.56249.
Train: 2018-08-09T11:31:25.917791: step 12255, loss 0.479652.
Train: 2018-08-09T11:31:25.995899: step 12256, loss 0.645512.
Train: 2018-08-09T11:31:26.074006: step 12257, loss 0.529179.
Train: 2018-08-09T11:31:26.152145: step 12258, loss 0.562434.
Train: 2018-08-09T11:31:26.230219: step 12259, loss 0.545744.
Train: 2018-08-09T11:31:26.308327: step 12260, loss 0.529008.
Test: 2018-08-09T11:31:26.808207: step 12260, loss 0.548628.
Train: 2018-08-09T11:31:26.886314: step 12261, loss 0.528945.
Train: 2018-08-09T11:31:26.966903: step 12262, loss 0.512107.
Train: 2018-08-09T11:31:27.045009: step 12263, loss 0.612795.
Train: 2018-08-09T11:31:27.123112: step 12264, loss 0.495085.
Train: 2018-08-09T11:31:27.201222: step 12265, loss 0.444363.
Train: 2018-08-09T11:31:27.279299: step 12266, loss 0.596187.
Train: 2018-08-09T11:31:27.357435: step 12267, loss 0.579336.
Train: 2018-08-09T11:31:27.435511: step 12268, loss 0.596322.
Train: 2018-08-09T11:31:27.513648: step 12269, loss 0.494332.
Train: 2018-08-09T11:31:27.591755: step 12270, loss 0.766743.
Test: 2018-08-09T11:31:28.091608: step 12270, loss 0.547696.
Train: 2018-08-09T11:31:28.169744: step 12271, loss 0.579353.
Train: 2018-08-09T11:31:28.247820: step 12272, loss 0.630261.
Train: 2018-08-09T11:31:28.325929: step 12273, loss 0.545421.
Train: 2018-08-09T11:31:28.419654: step 12274, loss 0.646847.
Train: 2018-08-09T11:31:28.497760: step 12275, loss 0.629739.
Train: 2018-08-09T11:31:28.575898: step 12276, loss 0.562395.
Train: 2018-08-09T11:31:28.654006: step 12277, loss 0.579125.
Train: 2018-08-09T11:31:28.732111: step 12278, loss 0.529143.
Train: 2018-08-09T11:31:28.810218: step 12279, loss 0.628855.
Train: 2018-08-09T11:31:28.888323: step 12280, loss 0.579025.
Test: 2018-08-09T11:31:29.390456: step 12280, loss 0.55144.
Train: 2018-08-09T11:31:29.468563: step 12281, loss 0.54604.
Train: 2018-08-09T11:31:29.546669: step 12282, loss 0.578974.
Train: 2018-08-09T11:31:29.624802: step 12283, loss 0.44793.
Train: 2018-08-09T11:31:29.702913: step 12284, loss 0.578947.
Train: 2018-08-09T11:31:29.781019: step 12285, loss 0.448205.
Train: 2018-08-09T11:31:29.859097: step 12286, loss 0.546239.
Train: 2018-08-09T11:31:29.937202: step 12287, loss 0.595315.
Train: 2018-08-09T11:31:30.015339: step 12288, loss 0.43162.
Train: 2018-08-09T11:31:30.093449: step 12289, loss 0.529748.
Train: 2018-08-09T11:31:30.171522: step 12290, loss 0.546091.
Test: 2018-08-09T11:31:30.671436: step 12290, loss 0.546446.
Train: 2018-08-09T11:31:30.749511: step 12291, loss 0.595483.
Train: 2018-08-09T11:31:30.827617: step 12292, loss 0.54599.
Train: 2018-08-09T11:31:30.953281: step 12293, loss 0.512871.
Train: 2018-08-09T11:31:31.031390: step 12294, loss 0.595619.
Train: 2018-08-09T11:31:31.109465: step 12295, loss 0.529263.
Train: 2018-08-09T11:31:31.203223: step 12296, loss 0.562447.
Train: 2018-08-09T11:31:31.281331: step 12297, loss 0.595743.
Train: 2018-08-09T11:31:31.359443: step 12298, loss 0.579094.
Train: 2018-08-09T11:31:31.437544: step 12299, loss 0.529091.
Train: 2018-08-09T11:31:31.515651: step 12300, loss 0.462329.
Test: 2018-08-09T11:31:32.015501: step 12300, loss 0.548645.
Train: 2018-08-09T11:31:32.577898: step 12301, loss 0.579131.
Train: 2018-08-09T11:31:32.655975: step 12302, loss 0.629379.
Train: 2018-08-09T11:31:32.734112: step 12303, loss 0.5624.
Train: 2018-08-09T11:31:32.812219: step 12304, loss 0.528895.
Train: 2018-08-09T11:31:32.890322: step 12305, loss 0.595925.
Train: 2018-08-09T11:31:32.968436: step 12306, loss 0.545642.
Train: 2018-08-09T11:31:33.046507: step 12307, loss 0.62944.
Train: 2018-08-09T11:31:33.136241: step 12308, loss 0.528914.
Train: 2018-08-09T11:31:33.214379: step 12309, loss 0.512198.
Train: 2018-08-09T11:31:33.292484: step 12310, loss 0.662843.
Test: 2018-08-09T11:31:33.792337: step 12310, loss 0.547381.
Train: 2018-08-09T11:31:33.870474: step 12311, loss 0.562413.
Train: 2018-08-09T11:31:33.948548: step 12312, loss 0.629202.
Train: 2018-08-09T11:31:34.026688: step 12313, loss 0.545776.
Train: 2018-08-09T11:31:34.104762: step 12314, loss 0.595703.
Train: 2018-08-09T11:31:34.182899: step 12315, loss 0.529279.
Train: 2018-08-09T11:31:34.260977: step 12316, loss 0.545908.
Train: 2018-08-09T11:31:34.339112: step 12317, loss 0.4963.
Train: 2018-08-09T11:31:34.417219: step 12318, loss 0.545941.
Train: 2018-08-09T11:31:34.510948: step 12319, loss 0.512852.
Train: 2018-08-09T11:31:34.589053: step 12320, loss 0.562478.
Test: 2018-08-09T11:31:35.088907: step 12320, loss 0.548831.
Train: 2018-08-09T11:31:35.167064: step 12321, loss 0.645307.
Train: 2018-08-09T11:31:35.245149: step 12322, loss 0.446597.
Train: 2018-08-09T11:31:35.323259: step 12323, loss 0.562472.
Train: 2018-08-09T11:31:35.409230: step 12324, loss 0.54588.
Train: 2018-08-09T11:31:35.481204: step 12325, loss 0.562458.
Train: 2018-08-09T11:31:35.559312: step 12326, loss 0.579067.
Train: 2018-08-09T11:31:35.637419: step 12327, loss 0.595693.
Train: 2018-08-09T11:31:35.715525: step 12328, loss 0.462739.
Train: 2018-08-09T11:31:35.793633: step 12329, loss 0.645638.
Train: 2018-08-09T11:31:35.871706: step 12330, loss 0.57908.
Test: 2018-08-09T11:31:36.371614: step 12330, loss 0.547494.
Train: 2018-08-09T11:31:36.449722: step 12331, loss 0.595704.
Train: 2018-08-09T11:31:36.527833: step 12332, loss 0.529228.
Train: 2018-08-09T11:31:36.605936: step 12333, loss 0.562456.
Train: 2018-08-09T11:31:36.684045: step 12334, loss 0.595655.
Train: 2018-08-09T11:31:36.762153: step 12335, loss 0.562467.
Train: 2018-08-09T11:31:36.840230: step 12336, loss 0.463075.
Train: 2018-08-09T11:31:36.918365: step 12337, loss 0.661926.
Train: 2018-08-09T11:31:37.012062: step 12338, loss 0.628716.
Train: 2018-08-09T11:31:37.090201: step 12339, loss 0.612076.
Train: 2018-08-09T11:31:37.168277: step 12340, loss 0.529541.
Test: 2018-08-09T11:31:37.668159: step 12340, loss 0.548355.
Train: 2018-08-09T11:31:37.746295: step 12341, loss 0.580083.
Train: 2018-08-09T11:31:37.824402: step 12342, loss 0.431149.
Train: 2018-08-09T11:31:37.902511: step 12343, loss 0.546117.
Train: 2018-08-09T11:31:37.982035: step 12344, loss 0.595412.
Train: 2018-08-09T11:31:38.060142: step 12345, loss 0.611842.
Train: 2018-08-09T11:31:38.138281: step 12346, loss 0.578969.
Train: 2018-08-09T11:31:38.216386: step 12347, loss 0.562562.
Train: 2018-08-09T11:31:38.294463: step 12348, loss 0.513421.
Train: 2018-08-09T11:31:38.372603: step 12349, loss 0.546192.
Train: 2018-08-09T11:31:38.450707: step 12350, loss 0.431514.
Test: 2018-08-09T11:31:38.950559: step 12350, loss 0.548413.
Train: 2018-08-09T11:31:39.028695: step 12351, loss 0.677462.
Train: 2018-08-09T11:31:39.106771: step 12352, loss 0.546135.
Train: 2018-08-09T11:31:39.184909: step 12353, loss 0.546128.
Train: 2018-08-09T11:31:39.263015: step 12354, loss 0.578974.
Train: 2018-08-09T11:31:39.356713: step 12355, loss 0.595408.
Train: 2018-08-09T11:31:39.434852: step 12356, loss 0.480414.
Train: 2018-08-09T11:31:39.512926: step 12357, loss 0.546097.
Train: 2018-08-09T11:31:39.591063: step 12358, loss 0.628359.
Train: 2018-08-09T11:31:39.669169: step 12359, loss 0.463796.
Train: 2018-08-09T11:31:39.747276: step 12360, loss 0.562518.
Test: 2018-08-09T11:31:40.233875: step 12360, loss 0.547674.
Train: 2018-08-09T11:31:40.312014: step 12361, loss 0.496519.
Train: 2018-08-09T11:31:40.405710: step 12362, loss 0.579021.
Train: 2018-08-09T11:31:40.483849: step 12363, loss 0.545924.
Train: 2018-08-09T11:31:40.561923: step 12364, loss 0.595628.
Train: 2018-08-09T11:31:40.640059: step 12365, loss 0.579056.
Train: 2018-08-09T11:31:40.718167: step 12366, loss 0.612263.
Train: 2018-08-09T11:31:40.796243: step 12367, loss 0.545866.
Train: 2018-08-09T11:31:40.874379: step 12368, loss 0.545862.
Train: 2018-08-09T11:31:40.952487: step 12369, loss 0.529271.
Train: 2018-08-09T11:31:41.030592: step 12370, loss 0.52925.
Test: 2018-08-09T11:31:41.530470: step 12370, loss 0.547508.
Train: 2018-08-09T11:31:41.608551: step 12371, loss 0.61231.
Train: 2018-08-09T11:31:41.702303: step 12372, loss 0.49598.
Train: 2018-08-09T11:31:41.780387: step 12373, loss 0.47928.
Train: 2018-08-09T11:31:41.858521: step 12374, loss 0.56244.
Train: 2018-08-09T11:31:41.937262: step 12375, loss 0.545709.
Train: 2018-08-09T11:31:42.015365: step 12376, loss 0.562419.
Train: 2018-08-09T11:31:42.093477: step 12377, loss 0.545655.
Train: 2018-08-09T11:31:42.171581: step 12378, loss 0.4785.
Train: 2018-08-09T11:31:42.249658: step 12379, loss 0.562445.
Train: 2018-08-09T11:31:42.327765: step 12380, loss 0.545569.
Test: 2018-08-09T11:31:42.827680: step 12380, loss 0.549733.
Train: 2018-08-09T11:31:42.905754: step 12381, loss 0.562424.
Train: 2018-08-09T11:31:42.983859: step 12382, loss 0.562356.
Train: 2018-08-09T11:31:43.061999: step 12383, loss 0.545419.
Train: 2018-08-09T11:31:43.140103: step 12384, loss 0.56236.
Train: 2018-08-09T11:31:43.218209: step 12385, loss 0.579316.
Train: 2018-08-09T11:31:43.311938: step 12386, loss 0.562343.
Train: 2018-08-09T11:31:43.390044: step 12387, loss 0.613318.
Train: 2018-08-09T11:31:43.468150: step 12388, loss 0.528384.
Train: 2018-08-09T11:31:43.546259: step 12389, loss 0.630267.
Train: 2018-08-09T11:31:43.631912: step 12390, loss 0.57931.
Test: 2018-08-09T11:31:44.121247: step 12390, loss 0.550332.
Train: 2018-08-09T11:31:44.199359: step 12391, loss 0.663952.
Train: 2018-08-09T11:31:44.277460: step 12392, loss 0.562365.
Train: 2018-08-09T11:31:44.355577: step 12393, loss 0.545537.
Train: 2018-08-09T11:31:44.449303: step 12394, loss 0.612788.
Train: 2018-08-09T11:31:44.527402: step 12395, loss 0.478641.
Train: 2018-08-09T11:31:44.605515: step 12396, loss 0.528955.
Train: 2018-08-09T11:31:44.683620: step 12397, loss 0.562414.
Train: 2018-08-09T11:31:44.761696: step 12398, loss 0.629205.
Train: 2018-08-09T11:31:44.839830: step 12399, loss 0.695751.
Train: 2018-08-09T11:31:44.917940: step 12400, loss 0.595663.
Test: 2018-08-09T11:31:45.417792: step 12400, loss 0.548868.
Train: 2018-08-09T11:31:45.948948: step 12401, loss 0.562487.
Train: 2018-08-09T11:31:46.027049: step 12402, loss 0.546038.
Train: 2018-08-09T11:31:46.105161: step 12403, loss 0.611832.
Train: 2018-08-09T11:31:46.198888: step 12404, loss 0.546207.
Train: 2018-08-09T11:31:46.276964: step 12405, loss 0.48099.
Train: 2018-08-09T11:31:46.355072: step 12406, loss 0.562625.
Train: 2018-08-09T11:31:46.433208: step 12407, loss 0.562639.
Train: 2018-08-09T11:31:46.511314: step 12408, loss 0.562651.
Train: 2018-08-09T11:31:46.589418: step 12409, loss 0.595155.
Train: 2018-08-09T11:31:46.667498: step 12410, loss 0.465338.
Test: 2018-08-09T11:31:47.169743: step 12410, loss 0.546857.
Train: 2018-08-09T11:31:47.247879: step 12411, loss 0.546443.
Train: 2018-08-09T11:31:47.325989: step 12412, loss 0.416536.
Train: 2018-08-09T11:31:47.404065: step 12413, loss 0.497499.
Train: 2018-08-09T11:31:47.497821: step 12414, loss 0.578966.
Train: 2018-08-09T11:31:47.575929: step 12415, loss 0.578966.
Train: 2018-08-09T11:31:47.654004: step 12416, loss 0.496843.
Train: 2018-08-09T11:31:47.732141: step 12417, loss 0.480116.
Train: 2018-08-09T11:31:47.810216: step 12418, loss 0.595546.
Train: 2018-08-09T11:31:47.888356: step 12419, loss 0.512419.
Train: 2018-08-09T11:31:47.982081: step 12420, loss 0.545903.
Test: 2018-08-09T11:31:48.466356: step 12420, loss 0.549854.
Train: 2018-08-09T11:31:48.544444: step 12421, loss 0.562272.
Train: 2018-08-09T11:31:48.638177: step 12422, loss 0.461211.
Train: 2018-08-09T11:31:48.716284: step 12423, loss 0.494177.
Train: 2018-08-09T11:31:48.794390: step 12424, loss 0.579801.
Train: 2018-08-09T11:31:48.872466: step 12425, loss 0.615181.
Train: 2018-08-09T11:31:48.952058: step 12426, loss 0.665743.
Train: 2018-08-09T11:31:49.030166: step 12427, loss 0.649237.
Train: 2018-08-09T11:31:49.108244: step 12428, loss 0.579479.
Train: 2018-08-09T11:31:49.201996: step 12429, loss 0.664155.
Train: 2018-08-09T11:31:49.280078: step 12430, loss 0.59586.
Test: 2018-08-09T11:31:49.779962: step 12430, loss 0.547871.
Train: 2018-08-09T11:31:49.858090: step 12431, loss 0.444407.
Train: 2018-08-09T11:31:49.936205: step 12432, loss 0.562379.
Train: 2018-08-09T11:31:50.014281: step 12433, loss 0.646294.
Train: 2018-08-09T11:31:50.092416: step 12434, loss 0.545666.
Train: 2018-08-09T11:31:50.170523: step 12435, loss 0.545712.
Train: 2018-08-09T11:31:50.264253: step 12436, loss 0.54575.
Train: 2018-08-09T11:31:50.342357: step 12437, loss 0.545781.
Train: 2018-08-09T11:31:50.420434: step 12438, loss 0.545806.
Train: 2018-08-09T11:31:50.498567: step 12439, loss 0.545823.
Train: 2018-08-09T11:31:50.576647: step 12440, loss 0.595684.
Test: 2018-08-09T11:31:51.077190: step 12440, loss 0.551299.
Train: 2018-08-09T11:31:51.155326: step 12441, loss 0.562459.
Train: 2018-08-09T11:31:51.233402: step 12442, loss 0.661968.
Train: 2018-08-09T11:31:51.311542: step 12443, loss 0.628661.
Train: 2018-08-09T11:31:51.389648: step 12444, loss 0.529528.
Train: 2018-08-09T11:31:51.483375: step 12445, loss 0.611887.
Train: 2018-08-09T11:31:51.561481: step 12446, loss 0.513352.
Train: 2018-08-09T11:31:51.639587: step 12447, loss 0.562579.
Train: 2018-08-09T11:31:51.717663: step 12448, loss 0.595281.
Train: 2018-08-09T11:31:51.795770: step 12449, loss 0.562616.
Train: 2018-08-09T11:31:51.873878: step 12450, loss 0.627791.
Test: 2018-08-09T11:31:52.373784: step 12450, loss 0.548677.
Train: 2018-08-09T11:31:52.451896: step 12451, loss 0.578918.
Train: 2018-08-09T11:31:52.545625: step 12452, loss 0.497947.
Train: 2018-08-09T11:31:52.627952: step 12453, loss 0.562707.
Train: 2018-08-09T11:31:52.707521: step 12454, loss 0.546588.
Train: 2018-08-09T11:31:52.785658: step 12455, loss 0.595117.
Train: 2018-08-09T11:31:52.863765: step 12456, loss 0.546637.
Train: 2018-08-09T11:31:52.941841: step 12457, loss 0.482253.
Train: 2018-08-09T11:31:53.019948: step 12458, loss 0.562765.
Train: 2018-08-09T11:31:53.098081: step 12459, loss 0.595002.
Train: 2018-08-09T11:31:53.176191: step 12460, loss 0.643371.
Test: 2018-08-09T11:31:53.676043: step 12460, loss 0.549513.
Train: 2018-08-09T11:31:53.754181: step 12461, loss 0.594982.
Train: 2018-08-09T11:31:53.832286: step 12462, loss 0.514562.
Train: 2018-08-09T11:31:53.926014: step 12463, loss 0.627076.
Train: 2018-08-09T11:31:54.004121: step 12464, loss 0.562823.
Train: 2018-08-09T11:31:54.082227: step 12465, loss 0.59489.
Train: 2018-08-09T11:31:54.160330: step 12466, loss 0.594862.
Train: 2018-08-09T11:31:54.238410: step 12467, loss 0.610798.
Train: 2018-08-09T11:31:54.316543: step 12468, loss 0.578859.
Train: 2018-08-09T11:31:54.410276: step 12469, loss 0.562965.
Train: 2018-08-09T11:31:54.488351: step 12470, loss 0.547137.
Test: 2018-08-09T11:31:54.972612: step 12470, loss 0.548781.
Train: 2018-08-09T11:31:55.066371: step 12471, loss 0.626373.
Train: 2018-08-09T11:31:55.144446: step 12472, loss 0.610465.
Train: 2018-08-09T11:31:55.222584: step 12473, loss 0.563106.
Train: 2018-08-09T11:31:55.300690: step 12474, loss 0.46881.
Train: 2018-08-09T11:31:55.378797: step 12475, loss 0.468855.
Train: 2018-08-09T11:31:55.456873: step 12476, loss 0.641817.
Train: 2018-08-09T11:31:55.534980: step 12477, loss 0.563129.
Train: 2018-08-09T11:31:55.628707: step 12478, loss 0.484428.
Train: 2018-08-09T11:31:55.706816: step 12479, loss 0.500039.
Train: 2018-08-09T11:31:55.784951: step 12480, loss 0.594666.
Test: 2018-08-09T11:31:56.287218: step 12480, loss 0.549383.
Train: 2018-08-09T11:31:56.365354: step 12481, loss 0.5947.
Train: 2018-08-09T11:31:56.443463: step 12482, loss 0.658139.
Train: 2018-08-09T11:31:56.521563: step 12483, loss 0.483755.
Train: 2018-08-09T11:31:56.599673: step 12484, loss 0.578866.
Train: 2018-08-09T11:31:56.677780: step 12485, loss 0.610629.
Train: 2018-08-09T11:31:56.755856: step 12486, loss 0.626507.
Train: 2018-08-09T11:31:56.833963: step 12487, loss 0.515386.
Train: 2018-08-09T11:31:56.927721: step 12488, loss 0.531249.
Train: 2018-08-09T11:31:57.005826: step 12489, loss 0.610619.
Train: 2018-08-09T11:31:57.083904: step 12490, loss 0.547098.
Test: 2018-08-09T11:31:57.583790: step 12490, loss 0.551095.
Train: 2018-08-09T11:31:57.661893: step 12491, loss 0.531208.
Train: 2018-08-09T11:31:57.740024: step 12492, loss 0.562966.
Train: 2018-08-09T11:31:57.818140: step 12493, loss 0.70615.
Train: 2018-08-09T11:31:57.896213: step 12494, loss 0.515309.
Train: 2018-08-09T11:31:57.991383: step 12495, loss 0.578861.
Train: 2018-08-09T11:31:58.069491: step 12496, loss 0.531239.
Train: 2018-08-09T11:31:58.147597: step 12497, loss 0.531242.
Train: 2018-08-09T11:31:58.225703: step 12498, loss 0.594743.
Train: 2018-08-09T11:31:58.303842: step 12499, loss 0.610622.
Train: 2018-08-09T11:31:58.381948: step 12500, loss 0.51535.
Test: 2018-08-09T11:31:58.881799: step 12500, loss 0.550489.
Train: 2018-08-09T11:31:59.412924: step 12501, loss 0.626518.
Train: 2018-08-09T11:31:59.491061: step 12502, loss 0.562982.
Train: 2018-08-09T11:31:59.569157: step 12503, loss 0.562985.
Train: 2018-08-09T11:31:59.647275: step 12504, loss 0.578856.
Train: 2018-08-09T11:31:59.725351: step 12505, loss 0.563007.
Train: 2018-08-09T11:31:59.819108: step 12506, loss 0.54715.
Train: 2018-08-09T11:31:59.897204: step 12507, loss 0.563004.
Train: 2018-08-09T11:31:59.977696: step 12508, loss 0.531279.
Train: 2018-08-09T11:32:00.055798: step 12509, loss 0.626475.
Train: 2018-08-09T11:32:00.133913: step 12510, loss 0.531251.
Test: 2018-08-09T11:32:00.633762: step 12510, loss 0.552311.
Train: 2018-08-09T11:32:00.711867: step 12511, loss 0.61061.
Train: 2018-08-09T11:32:00.790004: step 12512, loss 0.6106.
Train: 2018-08-09T11:32:00.883737: step 12513, loss 0.658132.
Train: 2018-08-09T11:32:00.961839: step 12514, loss 0.483955.
Train: 2018-08-09T11:32:01.039945: step 12515, loss 0.594667.
Train: 2018-08-09T11:32:01.118025: step 12516, loss 0.547282.
Train: 2018-08-09T11:32:01.196160: step 12517, loss 0.641989.
Train: 2018-08-09T11:32:01.274234: step 12518, loss 0.547356.
Train: 2018-08-09T11:32:01.367988: step 12519, loss 0.578868.
Train: 2018-08-09T11:32:01.446100: step 12520, loss 0.531715.
Test: 2018-08-09T11:32:01.931818: step 12520, loss 0.552002.
Train: 2018-08-09T11:32:02.025575: step 12521, loss 0.531733.
Train: 2018-08-09T11:32:02.103681: step 12522, loss 0.626022.
Train: 2018-08-09T11:32:02.181790: step 12523, loss 0.578865.
Train: 2018-08-09T11:32:02.259894: step 12524, loss 0.610263.
Train: 2018-08-09T11:32:02.338002: step 12525, loss 0.610212.
Train: 2018-08-09T11:32:02.416079: step 12526, loss 0.578861.
Train: 2018-08-09T11:32:02.509835: step 12527, loss 0.531989.
Train: 2018-08-09T11:32:02.587942: step 12528, loss 0.563335.
Train: 2018-08-09T11:32:02.666050: step 12529, loss 0.422889.
Train: 2018-08-09T11:32:02.744155: step 12530, loss 0.563091.
Test: 2018-08-09T11:32:03.244008: step 12530, loss 0.551413.
Train: 2018-08-09T11:32:03.322144: step 12531, loss 0.610347.
Train: 2018-08-09T11:32:03.400251: step 12532, loss 0.500204.
Train: 2018-08-09T11:32:03.478358: step 12533, loss 0.563555.
Train: 2018-08-09T11:32:03.572086: step 12534, loss 0.531427.
Train: 2018-08-09T11:32:03.650193: step 12535, loss 0.579017.
Train: 2018-08-09T11:32:03.728299: step 12536, loss 0.531347.
Train: 2018-08-09T11:32:03.806405: step 12537, loss 0.531286.
Train: 2018-08-09T11:32:03.884482: step 12538, loss 0.578536.
Train: 2018-08-09T11:32:03.980542: step 12539, loss 0.595046.
Train: 2018-08-09T11:32:04.058648: step 12540, loss 0.562931.
Test: 2018-08-09T11:32:04.558533: step 12540, loss 0.548379.
Train: 2018-08-09T11:32:04.636667: step 12541, loss 0.659242.
Train: 2018-08-09T11:32:04.714775: step 12542, loss 0.530584.
Train: 2018-08-09T11:32:04.792875: step 12543, loss 0.530803.
Train: 2018-08-09T11:32:04.870987: step 12544, loss 0.562962.
Train: 2018-08-09T11:32:04.960039: step 12545, loss 0.546797.
Train: 2018-08-09T11:32:05.038146: step 12546, loss 0.562847.
Train: 2018-08-09T11:32:05.116253: step 12547, loss 0.562767.
Train: 2018-08-09T11:32:05.194359: step 12548, loss 0.482514.
Train: 2018-08-09T11:32:05.272464: step 12549, loss 0.546654.
Train: 2018-08-09T11:32:05.366193: step 12550, loss 0.594967.
Test: 2018-08-09T11:32:05.866046: step 12550, loss 0.548775.
Train: 2018-08-09T11:32:05.934880: step 12551, loss 0.562765.
Train: 2018-08-09T11:32:06.028579: step 12552, loss 0.562791.
Train: 2018-08-09T11:32:06.106684: step 12553, loss 0.627546.
Train: 2018-08-09T11:32:06.184791: step 12554, loss 0.56267.
Train: 2018-08-09T11:32:06.262898: step 12555, loss 0.546457.
Train: 2018-08-09T11:32:06.356652: step 12556, loss 0.660021.
Train: 2018-08-09T11:32:06.434763: step 12557, loss 0.595101.
Train: 2018-08-09T11:32:06.512872: step 12558, loss 0.514174.
Train: 2018-08-09T11:32:06.590947: step 12559, loss 0.627457.
Train: 2018-08-09T11:32:06.684704: step 12560, loss 0.578894.
Test: 2018-08-09T11:32:07.184557: step 12560, loss 0.550702.
Train: 2018-08-09T11:32:07.257664: step 12561, loss 0.57888.
Train: 2018-08-09T11:32:07.351425: step 12562, loss 0.643255.
Train: 2018-08-09T11:32:07.429525: step 12563, loss 0.466507.
Train: 2018-08-09T11:32:07.507631: step 12564, loss 0.578867.
Train: 2018-08-09T11:32:07.585743: step 12565, loss 0.562842.
Train: 2018-08-09T11:32:07.679472: step 12566, loss 0.562854.
Train: 2018-08-09T11:32:07.757548: step 12567, loss 0.562863.
Train: 2018-08-09T11:32:07.835655: step 12568, loss 0.578863.
Train: 2018-08-09T11:32:07.913791: step 12569, loss 0.498954.
Train: 2018-08-09T11:32:07.991897: step 12570, loss 0.466934.
Test: 2018-08-09T11:32:08.491783: step 12570, loss 0.549041.
Train: 2018-08-09T11:32:08.585507: step 12571, loss 0.530797.
Train: 2018-08-09T11:32:08.663590: step 12572, loss 0.546747.
Train: 2018-08-09T11:32:08.741722: step 12573, loss 0.514469.
Train: 2018-08-09T11:32:08.819796: step 12574, loss 0.595037.
Train: 2018-08-09T11:32:08.913563: step 12575, loss 0.562706.
Train: 2018-08-09T11:32:08.983314: step 12576, loss 0.595123.
Train: 2018-08-09T11:32:09.061391: step 12577, loss 0.513935.
Train: 2018-08-09T11:32:09.139496: step 12578, loss 0.497535.
Train: 2018-08-09T11:32:09.233254: step 12579, loss 0.513638.
Train: 2018-08-09T11:32:09.311361: step 12580, loss 0.529827.
Test: 2018-08-09T11:32:09.811214: step 12580, loss 0.549003.
Train: 2018-08-09T11:32:09.889350: step 12581, loss 0.546096.
Train: 2018-08-09T11:32:09.967457: step 12582, loss 0.562513.
Train: 2018-08-09T11:32:10.045534: step 12583, loss 0.512848.
Train: 2018-08-09T11:32:10.123670: step 12584, loss 0.579085.
Train: 2018-08-09T11:32:10.217368: step 12585, loss 0.51242.
Train: 2018-08-09T11:32:10.295504: step 12586, loss 0.528753.
Train: 2018-08-09T11:32:10.373610: step 12587, loss 0.528726.
Train: 2018-08-09T11:32:10.451718: step 12588, loss 0.527823.
Train: 2018-08-09T11:32:10.529794: step 12589, loss 0.494085.
Train: 2018-08-09T11:32:10.607931: step 12590, loss 0.650387.
Test: 2018-08-09T11:32:11.107783: step 12590, loss 0.545396.
Train: 2018-08-09T11:32:11.185915: step 12591, loss 0.5776.
Train: 2018-08-09T11:32:11.279648: step 12592, loss 0.529059.
Train: 2018-08-09T11:32:11.357753: step 12593, loss 0.493393.
Train: 2018-08-09T11:32:11.435860: step 12594, loss 0.617886.
Train: 2018-08-09T11:32:11.513964: step 12595, loss 0.562209.
Train: 2018-08-09T11:32:11.607696: step 12596, loss 0.614039.
Train: 2018-08-09T11:32:11.685801: step 12597, loss 0.614016.
Train: 2018-08-09T11:32:11.763908: step 12598, loss 0.579359.
Train: 2018-08-09T11:32:11.842016: step 12599, loss 0.561978.
Train: 2018-08-09T11:32:11.921553: step 12600, loss 0.460176.
Test: 2018-08-09T11:32:12.421436: step 12600, loss 0.54771.
Train: 2018-08-09T11:32:12.952530: step 12601, loss 0.528496.
Train: 2018-08-09T11:32:13.030637: step 12602, loss 0.528363.
Train: 2018-08-09T11:32:13.108774: step 12603, loss 0.477415.
Train: 2018-08-09T11:32:13.186880: step 12604, loss 0.545399.
Train: 2018-08-09T11:32:13.280610: step 12605, loss 0.545305.
Train: 2018-08-09T11:32:13.358715: step 12606, loss 0.5282.
Train: 2018-08-09T11:32:13.436791: step 12607, loss 0.596535.
Train: 2018-08-09T11:32:13.514928: step 12608, loss 0.57944.
Train: 2018-08-09T11:32:13.593006: step 12609, loss 0.54521.
Train: 2018-08-09T11:32:13.686762: step 12610, loss 0.45959.
Test: 2018-08-09T11:32:14.172483: step 12610, loss 0.547551.
Train: 2018-08-09T11:32:14.266240: step 12611, loss 0.52808.
Train: 2018-08-09T11:32:14.344347: step 12612, loss 0.631034.
Train: 2018-08-09T11:32:14.422455: step 12613, loss 0.544937.
Train: 2018-08-09T11:32:14.500560: step 12614, loss 0.57965.
Train: 2018-08-09T11:32:14.578667: step 12615, loss 0.631705.
Train: 2018-08-09T11:32:14.672396: step 12616, loss 0.527878.
Train: 2018-08-09T11:32:14.750502: step 12617, loss 0.613724.
Train: 2018-08-09T11:32:14.828608: step 12618, loss 0.545227.
Train: 2018-08-09T11:32:14.906712: step 12619, loss 0.596596.
Train: 2018-08-09T11:32:14.984791: step 12620, loss 0.511107.
Test: 2018-08-09T11:32:15.484673: step 12620, loss 0.548935.
Train: 2018-08-09T11:32:15.563380: step 12621, loss 0.579401.
Train: 2018-08-09T11:32:15.657106: step 12622, loss 0.477148.
Train: 2018-08-09T11:32:15.735213: step 12623, loss 0.42606.
Train: 2018-08-09T11:32:15.813350: step 12624, loss 0.545289.
Train: 2018-08-09T11:32:15.891461: step 12625, loss 0.579444.
Train: 2018-08-09T11:32:15.969563: step 12626, loss 0.665114.
Train: 2018-08-09T11:32:16.047640: step 12627, loss 0.528107.
Train: 2018-08-09T11:32:16.141367: step 12628, loss 0.562357.
Train: 2018-08-09T11:32:16.219477: step 12629, loss 0.579397.
Train: 2018-08-09T11:32:16.297612: step 12630, loss 0.579463.
Test: 2018-08-09T11:32:16.797463: step 12630, loss 0.547645.
Train: 2018-08-09T11:32:16.875595: step 12631, loss 0.56235.
Train: 2018-08-09T11:32:16.953708: step 12632, loss 0.562397.
Train: 2018-08-09T11:32:17.031783: step 12633, loss 0.596415.
Train: 2018-08-09T11:32:17.125512: step 12634, loss 0.715194.
Train: 2018-08-09T11:32:17.203648: step 12635, loss 0.376288.
Train: 2018-08-09T11:32:17.281755: step 12636, loss 0.663764.
Train: 2018-08-09T11:32:17.359861: step 12637, loss 0.64667.
Train: 2018-08-09T11:32:17.437967: step 12638, loss 0.62959.
Train: 2018-08-09T11:32:17.531696: step 12639, loss 0.579138.
Train: 2018-08-09T11:32:17.609802: step 12640, loss 0.595755.
Test: 2018-08-09T11:32:18.096388: step 12640, loss 0.548802.
Train: 2018-08-09T11:32:18.174494: step 12641, loss 0.562464.
Train: 2018-08-09T11:32:18.268254: step 12642, loss 0.597744.
Train: 2018-08-09T11:32:18.346355: step 12643, loss 0.496714.
Train: 2018-08-09T11:32:18.424461: step 12644, loss 0.513332.
Train: 2018-08-09T11:32:18.518163: step 12645, loss 0.546195.
Train: 2018-08-09T11:32:18.596270: step 12646, loss 0.546231.
Train: 2018-08-09T11:32:18.674376: step 12647, loss 0.562598.
Train: 2018-08-09T11:32:18.752515: step 12648, loss 0.497293.
Train: 2018-08-09T11:32:18.830591: step 12649, loss 0.578927.
Train: 2018-08-09T11:32:18.908696: step 12650, loss 0.480947.
Test: 2018-08-09T11:32:19.408580: step 12650, loss 0.549116.
Train: 2018-08-09T11:32:19.502338: step 12651, loss 0.644384.
Train: 2018-08-09T11:32:19.580414: step 12652, loss 0.513481.
Train: 2018-08-09T11:32:19.658551: step 12653, loss 0.579015.
Train: 2018-08-09T11:32:19.736628: step 12654, loss 0.56252.
Train: 2018-08-09T11:32:19.830385: step 12655, loss 0.611683.
Train: 2018-08-09T11:32:19.910831: step 12656, loss 0.677176.
Train: 2018-08-09T11:32:19.988939: step 12657, loss 0.57894.
Train: 2018-08-09T11:32:20.067044: step 12658, loss 0.627902.
Train: 2018-08-09T11:32:20.145182: step 12659, loss 0.643812.
Train: 2018-08-09T11:32:20.223289: step 12660, loss 0.595044.
Test: 2018-08-09T11:32:20.723172: step 12660, loss 0.548318.
Train: 2018-08-09T11:32:20.801277: step 12661, loss 0.578874.
Train: 2018-08-09T11:32:20.894976: step 12662, loss 0.450659.
Train: 2018-08-09T11:32:20.973112: step 12663, loss 0.594862.
Train: 2018-08-09T11:32:21.051218: step 12664, loss 0.499026.
Train: 2018-08-09T11:32:21.129325: step 12665, loss 0.610762.
Train: 2018-08-09T11:32:21.207401: step 12666, loss 0.546989.
Train: 2018-08-09T11:32:21.301160: step 12667, loss 0.61069.
Train: 2018-08-09T11:32:21.379237: step 12668, loss 0.531149.
Train: 2018-08-09T11:32:21.457373: step 12669, loss 0.594754.
Train: 2018-08-09T11:32:21.535479: step 12670, loss 0.690008.
Test: 2018-08-09T11:32:22.037711: step 12670, loss 0.549389.
Train: 2018-08-09T11:32:22.115849: step 12671, loss 0.578865.
Train: 2018-08-09T11:32:22.193956: step 12672, loss 0.547254.
Train: 2018-08-09T11:32:22.272062: step 12673, loss 0.547306.
Train: 2018-08-09T11:32:22.365783: step 12674, loss 0.515951.
Train: 2018-08-09T11:32:22.443899: step 12675, loss 0.531598.
Train: 2018-08-09T11:32:22.522003: step 12676, loss 0.531586.
Train: 2018-08-09T11:32:22.600111: step 12677, loss 0.499905.
Train: 2018-08-09T11:32:22.678215: step 12678, loss 0.467795.
Train: 2018-08-09T11:32:22.771943: step 12679, loss 0.500514.
Train: 2018-08-09T11:32:22.850052: step 12680, loss 0.547041.
Test: 2018-08-09T11:32:23.339912: step 12680, loss 0.547497.
Train: 2018-08-09T11:32:23.418049: step 12681, loss 0.595054.
Train: 2018-08-09T11:32:23.511745: step 12682, loss 0.52982.
Train: 2018-08-09T11:32:23.589854: step 12683, loss 0.562285.
Train: 2018-08-09T11:32:23.667990: step 12684, loss 0.512067.
Train: 2018-08-09T11:32:23.746065: step 12685, loss 0.512489.
Train: 2018-08-09T11:32:23.839795: step 12686, loss 0.547723.
Train: 2018-08-09T11:32:23.917930: step 12687, loss 0.547481.
Train: 2018-08-09T11:32:23.996037: step 12688, loss 0.647552.
Train: 2018-08-09T11:32:24.074113: step 12689, loss 0.734173.
Train: 2018-08-09T11:32:24.152250: step 12690, loss 0.563598.
Test: 2018-08-09T11:32:24.652102: step 12690, loss 0.549319.
Train: 2018-08-09T11:32:24.730209: step 12691, loss 0.595823.
Train: 2018-08-09T11:32:24.823936: step 12692, loss 0.561624.
Train: 2018-08-09T11:32:24.902075: step 12693, loss 0.54649.
Train: 2018-08-09T11:32:24.980149: step 12694, loss 0.431855.
Train: 2018-08-09T11:32:25.058257: step 12695, loss 0.530376.
Train: 2018-08-09T11:32:25.136364: step 12696, loss 0.660377.
Train: 2018-08-09T11:32:25.230124: step 12697, loss 0.578804.
Train: 2018-08-09T11:32:25.308229: step 12698, loss 0.562555.
Train: 2018-08-09T11:32:25.386305: step 12699, loss 0.595146.
Train: 2018-08-09T11:32:25.464412: step 12700, loss 0.578908.
Test: 2018-08-09T11:32:25.966569: step 12700, loss 0.550577.
Train: 2018-08-09T11:32:26.607074: step 12701, loss 0.578898.
Train: 2018-08-09T11:32:26.700772: step 12702, loss 0.546526.
Train: 2018-08-09T11:32:26.778908: step 12703, loss 0.530378.
Train: 2018-08-09T11:32:26.857015: step 12704, loss 0.546554.
Train: 2018-08-09T11:32:26.935121: step 12705, loss 0.481872.
Train: 2018-08-09T11:32:27.028820: step 12706, loss 0.578895.
Train: 2018-08-09T11:32:27.106926: step 12707, loss 0.578899.
Train: 2018-08-09T11:32:27.185062: step 12708, loss 0.53024.
Train: 2018-08-09T11:32:27.263171: step 12709, loss 0.546427.
Train: 2018-08-09T11:32:27.341275: step 12710, loss 0.611437.
Test: 2018-08-09T11:32:27.841128: step 12710, loss 0.548634.
Train: 2018-08-09T11:32:27.920605: step 12711, loss 0.611452.
Train: 2018-08-09T11:32:27.998712: step 12712, loss 0.562649.
Train: 2018-08-09T11:32:28.076849: step 12713, loss 0.448843.
Train: 2018-08-09T11:32:28.170546: step 12714, loss 0.546347.
Train: 2018-08-09T11:32:28.248652: step 12715, loss 0.546292.
Train: 2018-08-09T11:32:28.326761: step 12716, loss 0.644362.
Train: 2018-08-09T11:32:28.404896: step 12717, loss 0.546192.
Train: 2018-08-09T11:32:28.482972: step 12718, loss 0.660729.
Train: 2018-08-09T11:32:28.576701: step 12719, loss 0.595302.
Train: 2018-08-09T11:32:28.654808: step 12720, loss 0.627882.
Test: 2018-08-09T11:32:29.154691: step 12720, loss 0.550465.
Train: 2018-08-09T11:32:29.232795: step 12721, loss 0.530091.
Train: 2018-08-09T11:32:29.310903: step 12722, loss 0.595178.
Train: 2018-08-09T11:32:29.389036: step 12723, loss 0.611325.
Train: 2018-08-09T11:32:29.467145: step 12724, loss 0.562725.
Train: 2018-08-09T11:32:29.545252: step 12725, loss 0.498214.
Train: 2018-08-09T11:32:29.638950: step 12726, loss 0.514412.
Train: 2018-08-09T11:32:29.717057: step 12727, loss 0.530497.
Train: 2018-08-09T11:32:29.795164: step 12728, loss 0.546605.
Train: 2018-08-09T11:32:29.873308: step 12729, loss 0.562736.
Train: 2018-08-09T11:32:29.951966: step 12730, loss 0.546525.
Test: 2018-08-09T11:32:30.451848: step 12730, loss 0.549324.
Train: 2018-08-09T11:32:30.529984: step 12731, loss 0.562738.
Train: 2018-08-09T11:32:30.623683: step 12732, loss 0.546439.
Train: 2018-08-09T11:32:30.701822: step 12733, loss 0.562869.
Train: 2018-08-09T11:32:30.779926: step 12734, loss 0.627884.
Train: 2018-08-09T11:32:30.858033: step 12735, loss 0.61134.
Train: 2018-08-09T11:32:30.951763: step 12736, loss 0.578795.
Train: 2018-08-09T11:32:31.029866: step 12737, loss 0.481732.
Train: 2018-08-09T11:32:31.107973: step 12738, loss 0.530329.
Train: 2018-08-09T11:32:31.186078: step 12739, loss 0.595172.
Train: 2018-08-09T11:32:31.264187: step 12740, loss 0.562626.
Test: 2018-08-09T11:32:31.764070: step 12740, loss 0.546851.
Train: 2018-08-09T11:32:31.842175: step 12741, loss 0.530303.
Train: 2018-08-09T11:32:31.920281: step 12742, loss 0.595095.
Train: 2018-08-09T11:32:32.014011: step 12743, loss 0.578972.
Train: 2018-08-09T11:32:32.092116: step 12744, loss 0.562605.
Train: 2018-08-09T11:32:32.170223: step 12745, loss 0.595134.
Train: 2018-08-09T11:32:32.248300: step 12746, loss 0.578911.
Train: 2018-08-09T11:32:32.326405: step 12747, loss 0.546476.
Train: 2018-08-09T11:32:32.404512: step 12748, loss 0.595225.
Train: 2018-08-09T11:32:32.498241: step 12749, loss 0.546361.
Train: 2018-08-09T11:32:32.576379: step 12750, loss 0.514175.
Test: 2018-08-09T11:32:33.062056: step 12750, loss 0.548729.
Train: 2018-08-09T11:32:33.155792: step 12751, loss 0.546585.
Train: 2018-08-09T11:32:33.233896: step 12752, loss 0.514023.
Train: 2018-08-09T11:32:33.312003: step 12753, loss 0.578925.
Train: 2018-08-09T11:32:33.390109: step 12754, loss 0.513049.
Train: 2018-08-09T11:32:33.468215: step 12755, loss 0.594327.
Train: 2018-08-09T11:32:33.561945: step 12756, loss 0.545587.
Train: 2018-08-09T11:32:33.640019: step 12757, loss 0.462375.
Train: 2018-08-09T11:32:33.718156: step 12758, loss 0.476334.
Train: 2018-08-09T11:32:33.796259: step 12759, loss 0.60168.
Train: 2018-08-09T11:32:33.874339: step 12760, loss 0.5315.
Test: 2018-08-09T11:32:34.374223: step 12760, loss 0.547135.
Train: 2018-08-09T11:32:34.452358: step 12761, loss 0.525519.
Train: 2018-08-09T11:32:34.530468: step 12762, loss 0.527037.
Train: 2018-08-09T11:32:34.624194: step 12763, loss 0.506877.
Train: 2018-08-09T11:32:34.702271: step 12764, loss 0.630856.
Train: 2018-08-09T11:32:34.780409: step 12765, loss 0.547154.
Train: 2018-08-09T11:32:34.858485: step 12766, loss 0.556107.
Train: 2018-08-09T11:32:34.938943: step 12767, loss 0.51384.
Train: 2018-08-09T11:32:35.017022: step 12768, loss 0.559105.
Train: 2018-08-09T11:32:35.110752: step 12769, loss 0.636332.
Train: 2018-08-09T11:32:35.188851: step 12770, loss 0.507748.
Test: 2018-08-09T11:32:35.688710: step 12770, loss 0.545858.
Train: 2018-08-09T11:32:35.766846: step 12771, loss 0.576475.
Train: 2018-08-09T11:32:35.844953: step 12772, loss 0.654885.
Train: 2018-08-09T11:32:35.923029: step 12773, loss 0.527388.
Train: 2018-08-09T11:32:36.001166: step 12774, loss 0.476377.
Train: 2018-08-09T11:32:36.094893: step 12775, loss 0.477464.
Train: 2018-08-09T11:32:36.173005: step 12776, loss 0.562887.
Train: 2018-08-09T11:32:36.251107: step 12777, loss 0.600471.
Train: 2018-08-09T11:32:36.329214: step 12778, loss 0.54777.
Train: 2018-08-09T11:32:36.407291: step 12779, loss 0.563403.
Train: 2018-08-09T11:32:36.485426: step 12780, loss 0.529103.
Test: 2018-08-09T11:32:36.986372: step 12780, loss 0.546549.
Train: 2018-08-09T11:32:37.064509: step 12781, loss 0.626775.
Train: 2018-08-09T11:32:37.142610: step 12782, loss 0.478611.
Train: 2018-08-09T11:32:37.220692: step 12783, loss 0.680471.
Train: 2018-08-09T11:32:37.314450: step 12784, loss 0.529103.
Train: 2018-08-09T11:32:37.392557: step 12785, loss 0.562643.
Train: 2018-08-09T11:32:37.470634: step 12786, loss 0.462604.
Train: 2018-08-09T11:32:37.548769: step 12787, loss 0.479843.
Train: 2018-08-09T11:32:37.626876: step 12788, loss 0.56086.
Train: 2018-08-09T11:32:37.704952: step 12789, loss 0.576945.
Train: 2018-08-09T11:32:37.798705: step 12790, loss 0.579401.
Test: 2018-08-09T11:32:38.298562: step 12790, loss 0.552978.
Train: 2018-08-09T11:32:38.376699: step 12791, loss 0.561638.
Train: 2018-08-09T11:32:38.454777: step 12792, loss 0.615004.
Train: 2018-08-09T11:32:38.532913: step 12793, loss 0.479959.
Train: 2018-08-09T11:32:38.611019: step 12794, loss 0.613815.
Train: 2018-08-09T11:32:38.689126: step 12795, loss 0.44691.
Train: 2018-08-09T11:32:38.782848: step 12796, loss 0.577852.
Train: 2018-08-09T11:32:38.860961: step 12797, loss 0.612214.
Train: 2018-08-09T11:32:38.941446: step 12798, loss 0.513452.
Train: 2018-08-09T11:32:39.019554: step 12799, loss 0.545276.
Train: 2018-08-09T11:32:39.110529: step 12800, loss 0.628321.
Test: 2018-08-09T11:32:39.594791: step 12800, loss 0.548569.
Train: 2018-08-09T11:32:40.172810: step 12801, loss 0.595523.
Train: 2018-08-09T11:32:40.250916: step 12802, loss 0.529848.
Train: 2018-08-09T11:32:40.344645: step 12803, loss 0.545197.
Train: 2018-08-09T11:32:40.422751: step 12804, loss 0.578878.
Train: 2018-08-09T11:32:40.500858: step 12805, loss 0.596791.
Train: 2018-08-09T11:32:40.578966: step 12806, loss 0.62947.
Train: 2018-08-09T11:32:40.657040: step 12807, loss 0.563247.
Train: 2018-08-09T11:32:40.735177: step 12808, loss 0.578736.
Train: 2018-08-09T11:32:40.828882: step 12809, loss 0.611774.
Train: 2018-08-09T11:32:40.909307: step 12810, loss 0.54654.
Test: 2018-08-09T11:32:41.395734: step 12810, loss 0.549863.
Train: 2018-08-09T11:32:41.473829: step 12811, loss 0.595366.
Train: 2018-08-09T11:32:41.551944: step 12812, loss 0.416817.
Train: 2018-08-09T11:32:41.645671: step 12813, loss 0.643731.
Train: 2018-08-09T11:32:41.723773: step 12814, loss 0.562706.
Train: 2018-08-09T11:32:41.801885: step 12815, loss 0.56273.
Train: 2018-08-09T11:32:41.879998: step 12816, loss 0.643475.
Train: 2018-08-09T11:32:41.958068: step 12817, loss 0.594984.
Train: 2018-08-09T11:32:42.036203: step 12818, loss 0.61102.
Train: 2018-08-09T11:32:42.129931: step 12819, loss 0.626976.
Train: 2018-08-09T11:32:42.208007: step 12820, loss 0.578863.
Test: 2018-08-09T11:32:42.707916: step 12820, loss 0.548622.
Train: 2018-08-09T11:32:42.786029: step 12821, loss 0.499251.
Train: 2018-08-09T11:32:42.864132: step 12822, loss 0.467621.
Train: 2018-08-09T11:32:42.944543: step 12823, loss 0.578858.
Train: 2018-08-09T11:32:43.022647: step 12824, loss 0.467651.
Train: 2018-08-09T11:32:43.100724: step 12825, loss 0.531129.
Train: 2018-08-09T11:32:43.178863: step 12826, loss 0.658563.
Train: 2018-08-09T11:32:43.256967: step 12827, loss 0.578859.
Train: 2018-08-09T11:32:43.350695: step 12828, loss 0.562913.
Train: 2018-08-09T11:32:43.428802: step 12829, loss 0.531018.
Train: 2018-08-09T11:32:43.506907: step 12830, loss 0.530984.
Test: 2018-08-09T11:32:44.006785: step 12830, loss 0.546703.
Train: 2018-08-09T11:32:44.084896: step 12831, loss 0.642777.
Train: 2018-08-09T11:32:44.163003: step 12832, loss 0.610814.
Train: 2018-08-09T11:32:44.241081: step 12833, loss 0.49904.
Train: 2018-08-09T11:32:44.319217: step 12834, loss 0.514978.
Train: 2018-08-09T11:32:44.397323: step 12835, loss 0.626837.
Train: 2018-08-09T11:32:44.491052: step 12836, loss 0.514896.
Train: 2018-08-09T11:32:44.569158: step 12837, loss 0.626895.
Train: 2018-08-09T11:32:44.647264: step 12838, loss 0.530849.
Train: 2018-08-09T11:32:44.725367: step 12839, loss 0.594893.
Train: 2018-08-09T11:32:44.803478: step 12840, loss 0.56286.
Test: 2018-08-09T11:32:45.304743: step 12840, loss 0.549666.
Train: 2018-08-09T11:32:45.382849: step 12841, loss 0.626915.
Train: 2018-08-09T11:32:45.460955: step 12842, loss 0.610861.
Train: 2018-08-09T11:32:45.539095: step 12843, loss 0.57886.
Train: 2018-08-09T11:32:45.617199: step 12844, loss 0.610748.
Train: 2018-08-09T11:32:45.695307: step 12845, loss 0.610679.
Train: 2018-08-09T11:32:45.789041: step 12846, loss 0.547122.
Train: 2018-08-09T11:32:45.867111: step 12847, loss 0.689715.
Train: 2018-08-09T11:32:45.945217: step 12848, loss 0.578863.
Train: 2018-08-09T11:32:46.023349: step 12849, loss 0.500273.
Train: 2018-08-09T11:32:46.101456: step 12850, loss 0.563187.
Test: 2018-08-09T11:32:46.601312: step 12850, loss 0.550322.
Train: 2018-08-09T11:32:46.679418: step 12851, loss 0.59454.
Train: 2018-08-09T11:32:46.757555: step 12852, loss 0.594515.
Train: 2018-08-09T11:32:46.835633: step 12853, loss 0.641282.
Train: 2018-08-09T11:32:46.916026: step 12854, loss 0.51671.
Train: 2018-08-09T11:32:46.994165: step 12855, loss 0.563393.
Train: 2018-08-09T11:32:47.087889: step 12856, loss 0.516929.
Train: 2018-08-09T11:32:47.165999: step 12857, loss 0.563429.
Train: 2018-08-09T11:32:47.244074: step 12858, loss 0.563431.
Train: 2018-08-09T11:32:47.322211: step 12859, loss 0.625398.
Train: 2018-08-09T11:32:47.400319: step 12860, loss 0.594404.
Test: 2018-08-09T11:32:47.893383: step 12860, loss 0.551318.
Train: 2018-08-09T11:32:47.971490: step 12861, loss 0.563471.
Train: 2018-08-09T11:32:48.065247: step 12862, loss 0.470777.
Train: 2018-08-09T11:32:48.143325: step 12863, loss 0.578935.
Train: 2018-08-09T11:32:48.221464: step 12864, loss 0.50151.
Train: 2018-08-09T11:32:48.299567: step 12865, loss 0.563385.
Train: 2018-08-09T11:32:48.377675: step 12866, loss 0.563354.
Train: 2018-08-09T11:32:48.455780: step 12867, loss 0.578917.
Train: 2018-08-09T11:32:48.533889: step 12868, loss 0.56329.
Train: 2018-08-09T11:32:48.611990: step 12869, loss 0.500663.
Train: 2018-08-09T11:32:48.690100: step 12870, loss 0.57886.
Test: 2018-08-09T11:32:49.197334: step 12870, loss 0.551947.
Train: 2018-08-09T11:32:49.275460: step 12871, loss 0.594581.
Train: 2018-08-09T11:32:49.353572: step 12872, loss 0.563164.
Train: 2018-08-09T11:32:49.431675: step 12873, loss 0.626448.
Train: 2018-08-09T11:32:49.509756: step 12874, loss 0.578828.
Train: 2018-08-09T11:32:49.587892: step 12875, loss 0.594601.
Train: 2018-08-09T11:32:49.681621: step 12876, loss 0.594645.
Train: 2018-08-09T11:32:49.759697: step 12877, loss 0.531569.
Train: 2018-08-09T11:32:49.837833: step 12878, loss 0.563095.
Train: 2018-08-09T11:32:49.915942: step 12879, loss 0.563094.
Train: 2018-08-09T11:32:49.994017: step 12880, loss 0.484205.
Test: 2018-08-09T11:32:50.493903: step 12880, loss 0.550638.
Train: 2018-08-09T11:32:50.572036: step 12881, loss 0.547257.
Train: 2018-08-09T11:32:50.650112: step 12882, loss 0.483834.
Train: 2018-08-09T11:32:50.728248: step 12883, loss 0.642432.
Train: 2018-08-09T11:32:50.806350: step 12884, loss 0.626658.
Train: 2018-08-09T11:32:50.884458: step 12885, loss 0.562937.
Train: 2018-08-09T11:32:50.978160: step 12886, loss 0.610714.
Train: 2018-08-09T11:32:51.056296: step 12887, loss 0.499237.
Train: 2018-08-09T11:32:51.134402: step 12888, loss 0.483224.
Train: 2018-08-09T11:32:51.212513: step 12889, loss 0.578861.
Train: 2018-08-09T11:32:51.290616: step 12890, loss 0.53085.
Test: 2018-08-09T11:32:51.790468: step 12890, loss 0.550831.
Train: 2018-08-09T11:32:51.868607: step 12891, loss 0.594909.
Train: 2018-08-09T11:32:51.948986: step 12892, loss 0.530678.
Train: 2018-08-09T11:32:52.027063: step 12893, loss 0.562782.
Train: 2018-08-09T11:32:52.105170: step 12894, loss 0.530506.
Train: 2018-08-09T11:32:52.183307: step 12895, loss 0.514236.
Train: 2018-08-09T11:32:52.277028: step 12896, loss 0.659939.
Train: 2018-08-09T11:32:52.355139: step 12897, loss 0.546465.
Train: 2018-08-09T11:32:52.433216: step 12898, loss 0.497706.
Train: 2018-08-09T11:32:52.511354: step 12899, loss 0.64401.
Train: 2018-08-09T11:32:52.589459: step 12900, loss 0.595193.
Test: 2018-08-09T11:32:53.089353: step 12900, loss 0.547994.
Train: 2018-08-09T11:32:53.643586: step 12901, loss 0.562616.
Train: 2018-08-09T11:32:53.737317: step 12902, loss 0.627781.
Train: 2018-08-09T11:32:53.815450: step 12903, loss 0.465011.
Train: 2018-08-09T11:32:53.893527: step 12904, loss 0.546355.
Train: 2018-08-09T11:32:53.965284: step 12905, loss 0.627864.
Train: 2018-08-09T11:32:54.043420: step 12906, loss 0.546306.
Train: 2018-08-09T11:32:54.121496: step 12907, loss 0.481117.
Train: 2018-08-09T11:32:54.199634: step 12908, loss 0.432018.
Train: 2018-08-09T11:32:54.293361: step 12909, loss 0.496795.
Train: 2018-08-09T11:32:54.371438: step 12910, loss 0.529803.
Test: 2018-08-09T11:32:54.871320: step 12910, loss 0.550647.
Train: 2018-08-09T11:32:54.949428: step 12911, loss 0.545913.
Train: 2018-08-09T11:32:55.027534: step 12912, loss 0.529273.
Train: 2018-08-09T11:32:55.105671: step 12913, loss 0.596804.
Train: 2018-08-09T11:32:55.183748: step 12914, loss 0.579095.
Train: 2018-08-09T11:32:55.261884: step 12915, loss 0.5113.
Train: 2018-08-09T11:32:55.339960: step 12916, loss 0.612872.
Train: 2018-08-09T11:32:55.418097: step 12917, loss 0.579452.
Train: 2018-08-09T11:32:55.511825: step 12918, loss 0.528383.
Train: 2018-08-09T11:32:55.589931: step 12919, loss 0.57959.
Train: 2018-08-09T11:32:55.668038: step 12920, loss 0.630139.
Test: 2018-08-09T11:32:56.152269: step 12920, loss 0.550294.
Train: 2018-08-09T11:32:56.245997: step 12921, loss 0.578824.
Train: 2018-08-09T11:32:56.324102: step 12922, loss 0.476706.
Train: 2018-08-09T11:32:56.402241: step 12923, loss 0.476995.
Train: 2018-08-09T11:32:56.480347: step 12924, loss 0.57991.
Train: 2018-08-09T11:32:56.558423: step 12925, loss 0.597707.
Train: 2018-08-09T11:32:56.636560: step 12926, loss 0.544658.
Train: 2018-08-09T11:32:56.714666: step 12927, loss 0.561687.
Train: 2018-08-09T11:32:56.792742: step 12928, loss 0.527768.
Train: 2018-08-09T11:32:56.870885: step 12929, loss 0.528829.
Train: 2018-08-09T11:32:56.951330: step 12930, loss 0.615599.
Test: 2018-08-09T11:32:57.451210: step 12930, loss 0.546113.
Train: 2018-08-09T11:32:57.529317: step 12931, loss 0.562941.
Train: 2018-08-09T11:32:57.607423: step 12932, loss 0.544765.
Train: 2018-08-09T11:32:57.701181: step 12933, loss 0.475602.
Train: 2018-08-09T11:32:57.779257: step 12934, loss 0.611883.
Train: 2018-08-09T11:32:57.857394: step 12935, loss 0.542069.
Train: 2018-08-09T11:32:57.935471: step 12936, loss 0.616674.
Train: 2018-08-09T11:32:58.013608: step 12937, loss 0.617284.
Train: 2018-08-09T11:32:58.091714: step 12938, loss 0.631143.
Train: 2018-08-09T11:32:58.169791: step 12939, loss 0.562514.
Train: 2018-08-09T11:32:58.263545: step 12940, loss 0.546526.
Test: 2018-08-09T11:32:58.747780: step 12940, loss 0.548601.
Train: 2018-08-09T11:32:58.825886: step 12941, loss 0.6455.
Train: 2018-08-09T11:32:58.904024: step 12942, loss 0.545702.
Train: 2018-08-09T11:32:58.984435: step 12943, loss 0.580309.
Train: 2018-08-09T11:32:59.078161: step 12944, loss 0.546158.
Train: 2018-08-09T11:32:59.156267: step 12945, loss 0.496846.
Train: 2018-08-09T11:32:59.234374: step 12946, loss 0.611717.
Train: 2018-08-09T11:32:59.312450: step 12947, loss 0.562594.
Train: 2018-08-09T11:32:59.390587: step 12948, loss 0.578937.
Train: 2018-08-09T11:32:59.468694: step 12949, loss 0.562636.
Train: 2018-08-09T11:32:59.546803: step 12950, loss 0.51385.
Test: 2018-08-09T11:33:00.046653: step 12950, loss 0.550499.
Train: 2018-08-09T11:33:00.124790: step 12951, loss 0.578923.
Train: 2018-08-09T11:33:00.202896: step 12952, loss 0.676353.
Train: 2018-08-09T11:33:00.296595: step 12953, loss 0.562697.
Train: 2018-08-09T11:33:00.374731: step 12954, loss 0.54658.
Train: 2018-08-09T11:33:00.452837: step 12955, loss 0.498239.
Train: 2018-08-09T11:33:00.530914: step 12956, loss 0.675597.
Train: 2018-08-09T11:33:00.609032: step 12957, loss 0.56279.
Train: 2018-08-09T11:33:00.687128: step 12958, loss 0.562818.
Train: 2018-08-09T11:33:00.765264: step 12959, loss 0.530798.
Train: 2018-08-09T11:33:00.858962: step 12960, loss 0.594871.
Test: 2018-08-09T11:33:01.343927: step 12960, loss 0.549109.
Train: 2018-08-09T11:33:01.422063: step 12961, loss 0.546891.
Train: 2018-08-09T11:33:01.515791: step 12962, loss 0.57886.
Train: 2018-08-09T11:33:01.593898: step 12963, loss 0.578861.
Train: 2018-08-09T11:33:01.671976: step 12964, loss 0.610738.
Train: 2018-08-09T11:33:01.750112: step 12965, loss 0.59477.
Train: 2018-08-09T11:33:01.828220: step 12966, loss 0.578858.
Train: 2018-08-09T11:33:01.906324: step 12967, loss 0.531308.
Train: 2018-08-09T11:33:01.984431: step 12968, loss 0.594693.
Train: 2018-08-09T11:33:02.062544: step 12969, loss 0.531429.
Train: 2018-08-09T11:33:02.140644: step 12970, loss 0.547262.
Test: 2018-08-09T11:33:02.640497: step 12970, loss 0.549457.
Train: 2018-08-09T11:33:02.718633: step 12971, loss 0.547263.
Train: 2018-08-09T11:33:02.796740: step 12972, loss 0.51565.
Train: 2018-08-09T11:33:02.890468: step 12973, loss 0.59469.
Train: 2018-08-09T11:33:02.968574: step 12974, loss 0.563045.
Train: 2018-08-09T11:33:03.046651: step 12975, loss 0.658097.
Train: 2018-08-09T11:33:03.124758: step 12976, loss 0.642146.
Train: 2018-08-09T11:33:03.202893: step 12977, loss 0.610443.
Train: 2018-08-09T11:33:03.281000: step 12978, loss 0.610359.
Train: 2018-08-09T11:33:03.359078: step 12979, loss 0.578874.
Train: 2018-08-09T11:33:03.437215: step 12980, loss 0.57888.
Test: 2018-08-09T11:33:03.939337: step 12980, loss 0.550418.
Train: 2018-08-09T11:33:04.017474: step 12981, loss 0.547663.
Train: 2018-08-09T11:33:04.111173: step 12982, loss 0.532148.
Train: 2018-08-09T11:33:04.189308: step 12983, loss 0.594466.
Train: 2018-08-09T11:33:04.267385: step 12984, loss 0.532276.
Train: 2018-08-09T11:33:04.345523: step 12985, loss 0.563366.
Train: 2018-08-09T11:33:04.423628: step 12986, loss 0.687674.
Train: 2018-08-09T11:33:04.501734: step 12987, loss 0.578925.
Train: 2018-08-09T11:33:04.579841: step 12988, loss 0.517074.
Train: 2018-08-09T11:33:04.657948: step 12989, loss 0.501684.
Train: 2018-08-09T11:33:04.736054: step 12990, loss 0.594391.
Test: 2018-08-09T11:33:05.235906: step 12990, loss 0.550158.
Train: 2018-08-09T11:33:05.329634: step 12991, loss 0.563474.
Train: 2018-08-09T11:33:05.407762: step 12992, loss 0.548004.
Train: 2018-08-09T11:33:05.485878: step 12993, loss 0.578928.
Train: 2018-08-09T11:33:05.563984: step 12994, loss 0.625373.
Train: 2018-08-09T11:33:05.642091: step 12995, loss 0.563452.
Train: 2018-08-09T11:33:05.720199: step 12996, loss 0.578922.
Train: 2018-08-09T11:33:05.798303: step 12997, loss 0.517041.
Train: 2018-08-09T11:33:05.876411: step 12998, loss 0.656356.
Train: 2018-08-09T11:33:05.956032: step 12999, loss 0.517016.
Train: 2018-08-09T11:33:06.034141: step 13000, loss 0.563427.
Test: 2018-08-09T11:33:06.533991: step 13000, loss 0.55124.
Train: 2018-08-09T11:33:07.096387: step 13001, loss 0.54793.
Train: 2018-08-09T11:33:07.174494: step 13002, loss 0.454718.
Train: 2018-08-09T11:33:07.268191: step 13003, loss 0.656821.
Train: 2018-08-09T11:33:07.346329: step 13004, loss 0.547681.
Train: 2018-08-09T11:33:07.424436: step 13005, loss 0.594227.
Train: 2018-08-09T11:33:07.502542: step 13006, loss 0.515554.
Train: 2018-08-09T11:33:07.580648: step 13007, loss 0.515891.
Train: 2018-08-09T11:33:07.658758: step 13008, loss 0.562347.
Train: 2018-08-09T11:33:07.736864: step 13009, loss 0.611821.
Train: 2018-08-09T11:33:07.830589: step 13010, loss 0.613066.
Test: 2018-08-09T11:33:08.316239: step 13010, loss 0.549324.
Train: 2018-08-09T11:33:08.394354: step 13011, loss 0.628123.
Train: 2018-08-09T11:33:08.472430: step 13012, loss 0.611673.
Train: 2018-08-09T11:33:08.550538: step 13013, loss 0.658393.
Train: 2018-08-09T11:33:08.644266: step 13014, loss 0.610687.
Train: 2018-08-09T11:33:08.709411: step 13015, loss 0.625734.
Train: 2018-08-09T11:33:08.803170: step 13016, loss 0.578914.
Train: 2018-08-09T11:33:08.881277: step 13017, loss 0.609891.
Train: 2018-08-09T11:33:08.959382: step 13018, loss 0.532682.
Train: 2018-08-09T11:33:09.037489: step 13019, loss 0.548205.
Train: 2018-08-09T11:33:09.115566: step 13020, loss 0.517579.
Test: 2018-08-09T11:33:09.615466: step 13020, loss 0.551011.
Train: 2018-08-09T11:33:09.693554: step 13021, loss 0.548304.
Train: 2018-08-09T11:33:09.771694: step 13022, loss 0.517637.
Train: 2018-08-09T11:33:09.849800: step 13023, loss 0.532925.
Train: 2018-08-09T11:33:09.927906: step 13024, loss 0.578966.
Train: 2018-08-09T11:33:10.006011: step 13025, loss 0.640535.
Train: 2018-08-09T11:33:10.084089: step 13026, loss 0.594346.
Train: 2018-08-09T11:33:10.162224: step 13027, loss 0.640558.
Train: 2018-08-09T11:33:10.240331: step 13028, loss 0.48673.
Train: 2018-08-09T11:33:10.334029: step 13029, loss 0.594337.
Train: 2018-08-09T11:33:10.412165: step 13030, loss 0.502075.
Test: 2018-08-09T11:33:10.912042: step 13030, loss 0.550873.
Train: 2018-08-09T11:33:10.992091: step 13031, loss 0.60974.
Train: 2018-08-09T11:33:11.070197: step 13032, loss 0.471098.
Train: 2018-08-09T11:33:11.148335: step 13033, loss 0.532578.
Train: 2018-08-09T11:33:11.226442: step 13034, loss 0.516967.
Train: 2018-08-09T11:33:11.304548: step 13035, loss 0.578772.
Train: 2018-08-09T11:33:11.382623: step 13036, loss 0.516165.
Train: 2018-08-09T11:33:11.460760: step 13037, loss 0.484139.
Train: 2018-08-09T11:33:11.538852: step 13038, loss 0.562273.
Train: 2018-08-09T11:33:11.632595: step 13039, loss 0.53022.
Train: 2018-08-09T11:33:11.710671: step 13040, loss 0.649429.
Test: 2018-08-09T11:33:12.210555: step 13040, loss 0.547568.
Train: 2018-08-09T11:33:12.288691: step 13041, loss 0.576995.
Train: 2018-08-09T11:33:12.366797: step 13042, loss 0.646116.
Train: 2018-08-09T11:33:12.444904: step 13043, loss 0.610678.
Train: 2018-08-09T11:33:12.523010: step 13044, loss 0.4984.
Train: 2018-08-09T11:33:12.601087: step 13045, loss 0.480789.
Train: 2018-08-09T11:33:12.679195: step 13046, loss 0.531953.
Train: 2018-08-09T11:33:12.757330: step 13047, loss 0.545731.
Train: 2018-08-09T11:33:12.835440: step 13048, loss 0.497152.
Train: 2018-08-09T11:33:12.929134: step 13049, loss 0.528839.
Train: 2018-08-09T11:33:13.007271: step 13050, loss 0.511947.
Test: 2018-08-09T11:33:13.507123: step 13050, loss 0.549123.
Train: 2018-08-09T11:33:13.585263: step 13051, loss 0.528584.
Train: 2018-08-09T11:33:13.663337: step 13052, loss 0.561874.
Train: 2018-08-09T11:33:13.741473: step 13053, loss 0.685003.
Train: 2018-08-09T11:33:13.819579: step 13054, loss 0.648566.
Train: 2018-08-09T11:33:13.897686: step 13055, loss 0.424461.
Train: 2018-08-09T11:33:13.978031: step 13056, loss 0.441622.
Train: 2018-08-09T11:33:14.056167: step 13057, loss 0.545421.
Train: 2018-08-09T11:33:14.134272: step 13058, loss 0.596682.
Train: 2018-08-09T11:33:14.227972: step 13059, loss 0.577377.
Train: 2018-08-09T11:33:14.306077: step 13060, loss 0.602697.
Test: 2018-08-09T11:33:14.805959: step 13060, loss 0.545983.
Train: 2018-08-09T11:33:14.884066: step 13061, loss 0.597291.
Train: 2018-08-09T11:33:14.962204: step 13062, loss 0.577238.
Train: 2018-08-09T11:33:15.040315: step 13063, loss 0.630218.
Train: 2018-08-09T11:33:15.118415: step 13064, loss 0.543674.
Train: 2018-08-09T11:33:15.196522: step 13065, loss 0.579487.
Train: 2018-08-09T11:33:15.274630: step 13066, loss 0.649319.
Train: 2018-08-09T11:33:15.352737: step 13067, loss 0.560676.
Train: 2018-08-09T11:33:15.430842: step 13068, loss 0.545648.
Train: 2018-08-09T11:33:15.524571: step 13069, loss 0.479072.
Train: 2018-08-09T11:33:15.602646: step 13070, loss 0.529567.
Test: 2018-08-09T11:33:16.089219: step 13070, loss 0.547052.
Train: 2018-08-09T11:33:16.167324: step 13071, loss 0.561204.
Train: 2018-08-09T11:33:16.261055: step 13072, loss 0.511784.
Train: 2018-08-09T11:33:16.339158: step 13073, loss 0.563139.
Train: 2018-08-09T11:33:16.417265: step 13074, loss 0.547495.
Train: 2018-08-09T11:33:16.495341: step 13075, loss 0.564757.
Train: 2018-08-09T11:33:16.573481: step 13076, loss 0.578971.
Train: 2018-08-09T11:33:16.651584: step 13077, loss 0.462974.
Train: 2018-08-09T11:33:16.729662: step 13078, loss 0.58012.
Train: 2018-08-09T11:33:16.807769: step 13079, loss 0.530641.
Train: 2018-08-09T11:33:16.901526: step 13080, loss 0.462685.
Test: 2018-08-09T11:33:17.385758: step 13080, loss 0.547384.
Train: 2018-08-09T11:33:17.463894: step 13081, loss 0.54505.
Train: 2018-08-09T11:33:17.542000: step 13082, loss 0.564815.
Train: 2018-08-09T11:33:17.635740: step 13083, loss 0.580877.
Train: 2018-08-09T11:33:17.713805: step 13084, loss 0.528884.
Train: 2018-08-09T11:33:17.791910: step 13085, loss 0.613195.
Train: 2018-08-09T11:33:17.870049: step 13086, loss 0.495115.
Train: 2018-08-09T11:33:17.950445: step 13087, loss 0.597033.
Train: 2018-08-09T11:33:18.028552: step 13088, loss 0.578647.
Train: 2018-08-09T11:33:18.106630: step 13089, loss 0.52999.
Train: 2018-08-09T11:33:18.184735: step 13090, loss 0.495877.
Test: 2018-08-09T11:33:18.684618: step 13090, loss 0.547822.
Train: 2018-08-09T11:33:18.762723: step 13091, loss 0.494175.
Train: 2018-08-09T11:33:18.840860: step 13092, loss 0.527769.
Train: 2018-08-09T11:33:18.918970: step 13093, loss 0.527175.
Train: 2018-08-09T11:33:18.997073: step 13094, loss 0.613186.
Train: 2018-08-09T11:33:19.075149: step 13095, loss 0.568067.
Train: 2018-08-09T11:33:19.153289: step 13096, loss 0.560537.
Train: 2018-08-09T11:33:19.247014: step 13097, loss 0.547633.
Train: 2018-08-09T11:33:19.325091: step 13098, loss 0.5807.
Train: 2018-08-09T11:33:19.403199: step 13099, loss 0.456243.
Train: 2018-08-09T11:33:19.486931: step 13100, loss 0.602787.
Test: 2018-08-09T11:33:19.982549: step 13100, loss 0.549251.
Train: 2018-08-09T11:33:20.529324: step 13101, loss 0.507593.
Train: 2018-08-09T11:33:20.623050: step 13102, loss 0.60311.
Train: 2018-08-09T11:33:20.701158: step 13103, loss 0.597711.
Train: 2018-08-09T11:33:20.779260: step 13104, loss 0.561329.
Train: 2018-08-09T11:33:20.857375: step 13105, loss 0.441739.
Train: 2018-08-09T11:33:20.935479: step 13106, loss 0.600414.
Train: 2018-08-09T11:33:21.029205: step 13107, loss 0.544727.
Train: 2018-08-09T11:33:21.107312: step 13108, loss 0.527581.
Train: 2018-08-09T11:33:21.185420: step 13109, loss 0.632684.
Train: 2018-08-09T11:33:21.263525: step 13110, loss 0.598336.
Test: 2018-08-09T11:33:21.763379: step 13110, loss 0.549581.
Train: 2018-08-09T11:33:21.841485: step 13111, loss 0.493432.
Train: 2018-08-09T11:33:21.919620: step 13112, loss 0.443367.
Train: 2018-08-09T11:33:21.997697: step 13113, loss 0.494379.
Train: 2018-08-09T11:33:22.075834: step 13114, loss 0.579039.
Train: 2018-08-09T11:33:22.153940: step 13115, loss 0.546017.
Train: 2018-08-09T11:33:22.247673: step 13116, loss 0.613223.
Train: 2018-08-09T11:33:22.325745: step 13117, loss 0.561676.
Train: 2018-08-09T11:33:22.403881: step 13118, loss 0.578335.
Train: 2018-08-09T11:33:22.481958: step 13119, loss 0.786078.
Train: 2018-08-09T11:33:22.560095: step 13120, loss 0.612364.
Test: 2018-08-09T11:33:23.059948: step 13120, loss 0.548523.
Train: 2018-08-09T11:33:23.138087: step 13121, loss 0.545124.
Train: 2018-08-09T11:33:23.216161: step 13122, loss 0.495107.
Train: 2018-08-09T11:33:23.309919: step 13123, loss 0.645239.
Train: 2018-08-09T11:33:23.388024: step 13124, loss 0.661108.
Train: 2018-08-09T11:33:23.466132: step 13125, loss 0.596788.
Train: 2018-08-09T11:33:23.544240: step 13126, loss 0.59561.
Train: 2018-08-09T11:33:23.622315: step 13127, loss 0.51343.
Train: 2018-08-09T11:33:23.700421: step 13128, loss 0.513225.
Train: 2018-08-09T11:33:23.794179: step 13129, loss 0.595454.
Train: 2018-08-09T11:33:23.872290: step 13130, loss 0.514393.
Test: 2018-08-09T11:33:24.367070: step 13130, loss 0.553082.
Train: 2018-08-09T11:33:24.445195: step 13131, loss 0.530295.
Train: 2018-08-09T11:33:24.523272: step 13132, loss 0.562413.
Train: 2018-08-09T11:33:24.601408: step 13133, loss 0.514423.
Train: 2018-08-09T11:33:24.679514: step 13134, loss 0.465486.
Train: 2018-08-09T11:33:24.757622: step 13135, loss 0.675848.
Train: 2018-08-09T11:33:24.851349: step 13136, loss 0.562098.
Train: 2018-08-09T11:33:24.921185: step 13137, loss 0.546598.
Train: 2018-08-09T11:33:24.999292: step 13138, loss 0.529663.
Train: 2018-08-09T11:33:25.093050: step 13139, loss 0.595583.
Train: 2018-08-09T11:33:25.171157: step 13140, loss 0.514174.
Test: 2018-08-09T11:33:25.671011: step 13140, loss 0.544943.
Train: 2018-08-09T11:33:25.749147: step 13141, loss 0.416227.
Train: 2018-08-09T11:33:25.827252: step 13142, loss 0.546348.
Train: 2018-08-09T11:33:25.905330: step 13143, loss 0.562238.
Train: 2018-08-09T11:33:25.999056: step 13144, loss 0.628439.
Train: 2018-08-09T11:33:26.077194: step 13145, loss 0.612774.
Train: 2018-08-09T11:33:26.155290: step 13146, loss 0.545952.
Train: 2018-08-09T11:33:26.233377: step 13147, loss 0.627445.
Train: 2018-08-09T11:33:26.327136: step 13148, loss 0.647384.
Train: 2018-08-09T11:33:26.405242: step 13149, loss 0.512542.
Train: 2018-08-09T11:33:26.483347: step 13150, loss 0.62817.
Test: 2018-08-09T11:33:26.983225: step 13150, loss 0.550174.
Train: 2018-08-09T11:33:27.061337: step 13151, loss 0.727743.
Train: 2018-08-09T11:33:27.139444: step 13152, loss 0.546174.
Train: 2018-08-09T11:33:27.217545: step 13153, loss 0.530709.
Train: 2018-08-09T11:33:27.295657: step 13154, loss 0.578442.
Train: 2018-08-09T11:33:27.373764: step 13155, loss 0.514263.
Train: 2018-08-09T11:33:27.467492: step 13156, loss 0.740989.
Train: 2018-08-09T11:33:27.545597: step 13157, loss 0.577664.
Train: 2018-08-09T11:33:27.623705: step 13158, loss 0.467215.
Train: 2018-08-09T11:33:27.701812: step 13159, loss 0.626488.
Train: 2018-08-09T11:33:27.779917: step 13160, loss 0.497526.
Test: 2018-08-09T11:33:28.282038: step 13160, loss 0.547744.
Train: 2018-08-09T11:33:28.360146: step 13161, loss 0.546655.
Train: 2018-08-09T11:33:28.438283: step 13162, loss 0.595675.
Train: 2018-08-09T11:33:28.532004: step 13163, loss 0.56395.
Train: 2018-08-09T11:33:28.610120: step 13164, loss 0.483044.
Train: 2018-08-09T11:33:28.688193: step 13165, loss 0.565179.
Train: 2018-08-09T11:33:28.766301: step 13166, loss 0.530905.
Train: 2018-08-09T11:33:28.844407: step 13167, loss 0.562875.
Train: 2018-08-09T11:33:28.922546: step 13168, loss 0.610585.
Train: 2018-08-09T11:33:29.000652: step 13169, loss 0.479453.
Train: 2018-08-09T11:33:29.094378: step 13170, loss 0.461058.
Test: 2018-08-09T11:33:29.578608: step 13170, loss 0.54621.
Train: 2018-08-09T11:33:29.656715: step 13171, loss 0.514192.
Train: 2018-08-09T11:33:29.750443: step 13172, loss 0.59492.
Train: 2018-08-09T11:33:29.828580: step 13173, loss 0.50999.
Train: 2018-08-09T11:33:29.908109: step 13174, loss 0.487609.
Train: 2018-08-09T11:33:29.986247: step 13175, loss 0.56873.
Train: 2018-08-09T11:33:30.064354: step 13176, loss 0.573098.
Train: 2018-08-09T11:33:30.158080: step 13177, loss 0.633132.
Train: 2018-08-09T11:33:30.236156: step 13178, loss 0.577378.
Train: 2018-08-09T11:33:30.314295: step 13179, loss 0.60364.
Train: 2018-08-09T11:33:30.392371: step 13180, loss 0.547807.
Test: 2018-08-09T11:33:30.892253: step 13180, loss 0.549776.
Train: 2018-08-09T11:33:30.970359: step 13181, loss 0.530532.
Train: 2018-08-09T11:33:31.095330: step 13182, loss 0.680086.
Train: 2018-08-09T11:33:31.189058: step 13183, loss 0.645638.
Train: 2018-08-09T11:33:31.267194: step 13184, loss 0.578585.
Train: 2018-08-09T11:33:31.345273: step 13185, loss 0.515677.
Train: 2018-08-09T11:33:31.423408: step 13186, loss 0.610098.
Train: 2018-08-09T11:33:31.501514: step 13187, loss 0.640878.
Train: 2018-08-09T11:33:31.595243: step 13188, loss 0.501378.
Train: 2018-08-09T11:33:31.673349: step 13189, loss 0.548929.
Train: 2018-08-09T11:33:31.751455: step 13190, loss 0.609992.
Test: 2018-08-09T11:33:32.252703: step 13190, loss 0.549466.
Train: 2018-08-09T11:33:32.330838: step 13191, loss 0.454469.
Train: 2018-08-09T11:33:32.408945: step 13192, loss 0.485303.
Train: 2018-08-09T11:33:32.487057: step 13193, loss 0.531761.
Train: 2018-08-09T11:33:32.580780: step 13194, loss 0.578945.
Train: 2018-08-09T11:33:32.658886: step 13195, loss 0.578384.
Train: 2018-08-09T11:33:32.737013: step 13196, loss 0.610915.
Train: 2018-08-09T11:33:32.815099: step 13197, loss 0.547124.
Train: 2018-08-09T11:33:32.893209: step 13198, loss 0.499219.
Train: 2018-08-09T11:33:32.986903: step 13199, loss 0.513913.
Train: 2018-08-09T11:33:33.065029: step 13200, loss 0.530684.
Test: 2018-08-09T11:33:33.564894: step 13200, loss 0.549926.
Train: 2018-08-09T11:33:34.081892: step 13201, loss 0.510685.
Train: 2018-08-09T11:33:34.175649: step 13202, loss 0.491245.
Train: 2018-08-09T11:33:34.253755: step 13203, loss 0.585664.
Train: 2018-08-09T11:33:34.331862: step 13204, loss 0.620743.
Train: 2018-08-09T11:33:34.409968: step 13205, loss 0.543116.
Train: 2018-08-09T11:33:34.488078: step 13206, loss 0.470695.
Train: 2018-08-09T11:33:34.566182: step 13207, loss 0.595714.
Train: 2018-08-09T11:33:34.659910: step 13208, loss 0.514385.
Train: 2018-08-09T11:33:34.738016: step 13209, loss 0.512046.
Train: 2018-08-09T11:33:34.816123: step 13210, loss 0.512653.
Test: 2018-08-09T11:33:35.315976: step 13210, loss 0.547174.
Train: 2018-08-09T11:33:35.394082: step 13211, loss 0.575249.
Train: 2018-08-09T11:33:35.472188: step 13212, loss 0.487924.
Train: 2018-08-09T11:33:35.550294: step 13213, loss 0.565797.
Train: 2018-08-09T11:33:35.628432: step 13214, loss 0.524456.
Train: 2018-08-09T11:33:35.706538: step 13215, loss 0.621407.
Train: 2018-08-09T11:33:35.800265: step 13216, loss 0.549566.
Train: 2018-08-09T11:33:35.878343: step 13217, loss 0.543912.
Train: 2018-08-09T11:33:35.957885: step 13218, loss 0.550571.
Train: 2018-08-09T11:33:36.035992: step 13219, loss 0.545167.
Train: 2018-08-09T11:33:36.129709: step 13220, loss 0.633267.
Test: 2018-08-09T11:33:36.613951: step 13220, loss 0.549674.
Train: 2018-08-09T11:33:36.692086: step 13221, loss 0.563891.
Train: 2018-08-09T11:33:36.785815: step 13222, loss 0.547869.
Train: 2018-08-09T11:33:36.863921: step 13223, loss 0.628749.
Train: 2018-08-09T11:33:36.942028: step 13224, loss 0.579217.
Train: 2018-08-09T11:33:37.020105: step 13225, loss 0.579083.
Train: 2018-08-09T11:33:37.098212: step 13226, loss 0.610611.
Train: 2018-08-09T11:33:37.191981: step 13227, loss 0.578593.
Train: 2018-08-09T11:33:37.270075: step 13228, loss 0.547378.
Train: 2018-08-09T11:33:37.348183: step 13229, loss 0.499788.
Train: 2018-08-09T11:33:37.426293: step 13230, loss 0.642415.
Test: 2018-08-09T11:33:37.928611: step 13230, loss 0.548878.
Train: 2018-08-09T11:33:38.006722: step 13231, loss 0.594506.
Train: 2018-08-09T11:33:38.084833: step 13232, loss 0.484535.
Train: 2018-08-09T11:33:38.178558: step 13233, loss 0.531514.
Train: 2018-08-09T11:33:38.256664: step 13234, loss 0.483916.
Train: 2018-08-09T11:33:38.334770: step 13235, loss 0.626872.
Train: 2018-08-09T11:33:38.412879: step 13236, loss 0.515724.
Train: 2018-08-09T11:33:38.490986: step 13237, loss 0.643076.
Train: 2018-08-09T11:33:38.584681: step 13238, loss 0.546644.
Train: 2018-08-09T11:33:38.662788: step 13239, loss 0.530685.
Train: 2018-08-09T11:33:38.740895: step 13240, loss 0.514076.
Test: 2018-08-09T11:33:39.225156: step 13240, loss 0.547431.
Train: 2018-08-09T11:33:39.318913: step 13241, loss 0.626433.
Train: 2018-08-09T11:33:39.397020: step 13242, loss 0.562755.
Train: 2018-08-09T11:33:39.475127: step 13243, loss 0.54549.
Train: 2018-08-09T11:33:39.553204: step 13244, loss 0.636284.
Train: 2018-08-09T11:33:39.631334: step 13245, loss 0.563947.
Train: 2018-08-09T11:33:39.709446: step 13246, loss 0.529191.
Train: 2018-08-09T11:33:39.803147: step 13247, loss 0.531183.
Train: 2018-08-09T11:33:39.881281: step 13248, loss 0.609845.
Train: 2018-08-09T11:33:39.961781: step 13249, loss 0.530993.
Train: 2018-08-09T11:33:40.039888: step 13250, loss 0.550009.
Test: 2018-08-09T11:33:40.539766: step 13250, loss 0.548672.
Train: 2018-08-09T11:33:40.667894: step 13251, loss 0.497714.
Train: 2018-08-09T11:33:40.745986: step 13252, loss 0.545599.
Train: 2018-08-09T11:33:40.824092: step 13253, loss 0.578188.
Train: 2018-08-09T11:33:40.902198: step 13254, loss 0.576832.
Train: 2018-08-09T11:33:40.980306: step 13255, loss 0.528175.
Train: 2018-08-09T11:33:41.074032: step 13256, loss 0.580176.
Train: 2018-08-09T11:33:41.152171: step 13257, loss 0.493266.
Train: 2018-08-09T11:33:41.230247: step 13258, loss 0.682352.
Train: 2018-08-09T11:33:41.324008: step 13259, loss 0.543559.
Train: 2018-08-09T11:33:41.402110: step 13260, loss 0.543555.
Test: 2018-08-09T11:33:41.901963: step 13260, loss 0.549412.
Train: 2018-08-09T11:33:41.980099: step 13261, loss 0.648731.
Train: 2018-08-09T11:33:42.058177: step 13262, loss 0.616773.
Train: 2018-08-09T11:33:42.136315: step 13263, loss 0.543096.
Train: 2018-08-09T11:33:42.214419: step 13264, loss 0.560928.
Train: 2018-08-09T11:33:42.292526: step 13265, loss 0.509579.
Train: 2018-08-09T11:33:42.386257: step 13266, loss 0.609517.
Train: 2018-08-09T11:33:42.464360: step 13267, loss 0.682959.
Train: 2018-08-09T11:33:42.542467: step 13268, loss 0.527191.
Train: 2018-08-09T11:33:42.620575: step 13269, loss 0.674812.
Train: 2018-08-09T11:33:42.698683: step 13270, loss 0.460808.
Test: 2018-08-09T11:33:43.199535: step 13270, loss 0.549178.
Train: 2018-08-09T11:33:43.293251: step 13271, loss 0.657964.
Train: 2018-08-09T11:33:43.371387: step 13272, loss 0.604086.
Train: 2018-08-09T11:33:43.449495: step 13273, loss 0.530798.
Train: 2018-08-09T11:33:43.527571: step 13274, loss 0.454744.
Train: 2018-08-09T11:33:43.605707: step 13275, loss 0.561318.
Train: 2018-08-09T11:33:43.699435: step 13276, loss 0.566466.
Train: 2018-08-09T11:33:43.777541: step 13277, loss 0.448695.
Train: 2018-08-09T11:33:43.855648: step 13278, loss 0.580775.
Train: 2018-08-09T11:33:43.933756: step 13279, loss 0.511183.
Train: 2018-08-09T11:33:44.011861: step 13280, loss 0.579683.
Test: 2018-08-09T11:33:44.511747: step 13280, loss 0.549663.
Train: 2018-08-09T11:33:44.589820: step 13281, loss 0.59684.
Train: 2018-08-09T11:33:44.683578: step 13282, loss 0.66823.
Train: 2018-08-09T11:33:44.761656: step 13283, loss 0.596155.
Train: 2018-08-09T11:33:44.839792: step 13284, loss 0.528963.
Train: 2018-08-09T11:33:44.917898: step 13285, loss 0.511008.
Train: 2018-08-09T11:33:44.995999: step 13286, loss 0.562666.
Train: 2018-08-09T11:33:45.089732: step 13287, loss 0.61145.
Train: 2018-08-09T11:33:45.167844: step 13288, loss 0.597238.
Train: 2018-08-09T11:33:45.245915: step 13289, loss 0.531482.
Train: 2018-08-09T11:33:45.324052: step 13290, loss 0.595199.
Test: 2018-08-09T11:33:45.823929: step 13290, loss 0.548232.
Train: 2018-08-09T11:33:45.902010: step 13291, loss 0.498709.
Train: 2018-08-09T11:33:45.982462: step 13292, loss 0.581035.
Train: 2018-08-09T11:33:46.076185: step 13293, loss 0.592159.
Train: 2018-08-09T11:33:46.154266: step 13294, loss 0.534274.
Train: 2018-08-09T11:33:46.232404: step 13295, loss 0.590537.
Train: 2018-08-09T11:33:46.310480: step 13296, loss 0.597668.
Train: 2018-08-09T11:33:46.388621: step 13297, loss 0.659974.
Train: 2018-08-09T11:33:46.466724: step 13298, loss 0.517625.
Train: 2018-08-09T11:33:46.560452: step 13299, loss 0.593077.
Train: 2018-08-09T11:33:46.638527: step 13300, loss 0.548119.
Test: 2018-08-09T11:33:47.138409: step 13300, loss 0.551714.
Train: 2018-08-09T11:33:47.716399: step 13301, loss 0.53355.
Train: 2018-08-09T11:33:47.794535: step 13302, loss 0.547879.
Train: 2018-08-09T11:33:47.888260: step 13303, loss 0.547796.
Train: 2018-08-09T11:33:47.967759: step 13304, loss 0.640619.
Train: 2018-08-09T11:33:48.045866: step 13305, loss 0.486104.
Train: 2018-08-09T11:33:48.123948: step 13306, loss 0.624926.
Train: 2018-08-09T11:33:48.217704: step 13307, loss 0.594425.
Train: 2018-08-09T11:33:48.295806: step 13308, loss 0.532959.
Train: 2018-08-09T11:33:48.373888: step 13309, loss 0.53294.
Train: 2018-08-09T11:33:48.467641: step 13310, loss 0.532374.
Test: 2018-08-09T11:33:48.951878: step 13310, loss 0.5499.
Train: 2018-08-09T11:33:49.030014: step 13311, loss 0.578144.
Train: 2018-08-09T11:33:49.123741: step 13312, loss 0.563056.
Train: 2018-08-09T11:33:49.201848: step 13313, loss 0.563641.
Train: 2018-08-09T11:33:49.279924: step 13314, loss 0.563182.
Train: 2018-08-09T11:33:49.358032: step 13315, loss 0.515571.
Train: 2018-08-09T11:33:49.436138: step 13316, loss 0.564722.
Train: 2018-08-09T11:33:49.529899: step 13317, loss 0.516129.
Train: 2018-08-09T11:33:49.607973: step 13318, loss 0.578903.
Train: 2018-08-09T11:33:49.686080: step 13319, loss 0.516012.
Train: 2018-08-09T11:33:49.764215: step 13320, loss 0.54541.
Test: 2018-08-09T11:33:50.264720: step 13320, loss 0.548567.
Train: 2018-08-09T11:33:50.342854: step 13321, loss 0.709037.
Train: 2018-08-09T11:33:50.420932: step 13322, loss 0.561942.
Train: 2018-08-09T11:33:50.499038: step 13323, loss 0.497606.
Train: 2018-08-09T11:33:50.592766: step 13324, loss 0.497853.
Train: 2018-08-09T11:33:50.670903: step 13325, loss 0.513824.
Train: 2018-08-09T11:33:50.749010: step 13326, loss 0.478772.
Train: 2018-08-09T11:33:50.827086: step 13327, loss 0.526493.
Train: 2018-08-09T11:33:50.905225: step 13328, loss 0.512211.
Train: 2018-08-09T11:33:50.998951: step 13329, loss 0.616456.
Train: 2018-08-09T11:33:51.077058: step 13330, loss 0.685291.
Test: 2018-08-09T11:33:51.562951: step 13330, loss 0.54785.
Train: 2018-08-09T11:33:51.656682: step 13331, loss 0.633517.
Train: 2018-08-09T11:33:51.734784: step 13332, loss 0.579182.
Train: 2018-08-09T11:33:51.812891: step 13333, loss 0.544295.
Train: 2018-08-09T11:33:51.890968: step 13334, loss 0.679961.
Train: 2018-08-09T11:33:51.969104: step 13335, loss 0.692857.
Train: 2018-08-09T11:33:52.062833: step 13336, loss 0.4842.
Train: 2018-08-09T11:33:52.140940: step 13337, loss 0.500257.
Train: 2018-08-09T11:33:52.219015: step 13338, loss 0.500361.
Train: 2018-08-09T11:33:52.297152: step 13339, loss 0.516032.
Train: 2018-08-09T11:33:52.375261: step 13340, loss 0.531878.
Test: 2018-08-09T11:33:52.875111: step 13340, loss 0.549572.
Train: 2018-08-09T11:33:52.953216: step 13341, loss 0.531637.
Train: 2018-08-09T11:33:53.031354: step 13342, loss 0.610388.
Train: 2018-08-09T11:33:53.120477: step 13343, loss 0.626432.
Train: 2018-08-09T11:33:53.198587: step 13344, loss 0.562906.
Train: 2018-08-09T11:33:53.276664: step 13345, loss 0.483932.
Train: 2018-08-09T11:33:53.370423: step 13346, loss 0.626375.
Train: 2018-08-09T11:33:53.448498: step 13347, loss 0.626332.
Train: 2018-08-09T11:33:53.526605: step 13348, loss 0.610546.
Train: 2018-08-09T11:33:53.604745: step 13349, loss 0.515599.
Train: 2018-08-09T11:33:53.698439: step 13350, loss 0.610537.
Test: 2018-08-09T11:33:54.198322: step 13350, loss 0.548297.
Train: 2018-08-09T11:33:54.276452: step 13351, loss 0.563195.
Train: 2018-08-09T11:33:54.354566: step 13352, loss 0.610394.
Train: 2018-08-09T11:33:54.432642: step 13353, loss 0.547364.
Train: 2018-08-09T11:33:54.510778: step 13354, loss 0.515966.
Train: 2018-08-09T11:33:54.605057: step 13355, loss 0.594545.
Train: 2018-08-09T11:33:54.683164: step 13356, loss 0.563254.
Train: 2018-08-09T11:33:54.761269: step 13357, loss 0.500296.
Train: 2018-08-09T11:33:54.839375: step 13358, loss 0.578972.
Train: 2018-08-09T11:33:54.917453: step 13359, loss 0.468501.
Train: 2018-08-09T11:33:55.011211: step 13360, loss 0.626184.
Test: 2018-08-09T11:33:55.495473: step 13360, loss 0.54814.
Train: 2018-08-09T11:33:55.589193: step 13361, loss 0.610533.
Train: 2018-08-09T11:33:55.667305: step 13362, loss 0.595045.
Train: 2018-08-09T11:33:55.745407: step 13363, loss 0.547077.
Train: 2018-08-09T11:33:55.823512: step 13364, loss 0.562869.
Train: 2018-08-09T11:33:55.917224: step 13365, loss 0.53102.
Train: 2018-08-09T11:33:55.995324: step 13366, loss 0.53043.
Train: 2018-08-09T11:33:56.073459: step 13367, loss 0.515403.
Train: 2018-08-09T11:33:56.151566: step 13368, loss 0.528777.
Train: 2018-08-09T11:33:56.229644: step 13369, loss 0.561495.
Train: 2018-08-09T11:33:56.307750: step 13370, loss 0.528682.
Test: 2018-08-09T11:33:56.807633: step 13370, loss 0.547327.
Train: 2018-08-09T11:33:56.885738: step 13371, loss 0.510598.
Train: 2018-08-09T11:33:56.966233: step 13372, loss 0.509295.
Train: 2018-08-09T11:33:57.059961: step 13373, loss 0.57162.
Train: 2018-08-09T11:33:57.138038: step 13374, loss 0.595974.
Train: 2018-08-09T11:33:57.216145: step 13375, loss 0.668319.
Train: 2018-08-09T11:33:57.294277: step 13376, loss 0.562581.
Train: 2018-08-09T11:33:57.372358: step 13377, loss 0.65156.
Train: 2018-08-09T11:33:57.450494: step 13378, loss 0.561021.
Train: 2018-08-09T11:33:57.544223: step 13379, loss 0.562122.
Train: 2018-08-09T11:33:57.622298: step 13380, loss 0.611776.
Test: 2018-08-09T11:33:58.122181: step 13380, loss 0.548932.
Train: 2018-08-09T11:33:58.200287: step 13381, loss 0.595034.
Train: 2018-08-09T11:33:58.278393: step 13382, loss 0.626739.
Train: 2018-08-09T11:33:58.356532: step 13383, loss 0.499541.
Train: 2018-08-09T11:33:58.434607: step 13384, loss 0.579043.
Train: 2018-08-09T11:33:58.512744: step 13385, loss 0.547363.
Train: 2018-08-09T11:33:58.606472: step 13386, loss 0.51591.
Train: 2018-08-09T11:33:58.684578: step 13387, loss 0.531712.
Train: 2018-08-09T11:33:58.762684: step 13388, loss 0.515835.
Train: 2018-08-09T11:33:58.840763: step 13389, loss 0.594575.
Train: 2018-08-09T11:33:58.920339: step 13390, loss 0.562993.
Test: 2018-08-09T11:33:59.420222: step 13390, loss 0.546459.
Train: 2018-08-09T11:33:59.498329: step 13391, loss 0.642146.
Train: 2018-08-09T11:33:59.592087: step 13392, loss 0.468265.
Train: 2018-08-09T11:33:59.670193: step 13393, loss 0.531459.
Train: 2018-08-09T11:33:59.748269: step 13394, loss 0.610863.
Train: 2018-08-09T11:33:59.842027: step 13395, loss 0.578943.
Train: 2018-08-09T11:33:59.920135: step 13396, loss 0.546989.
Train: 2018-08-09T11:33:59.998240: step 13397, loss 0.579058.
Train: 2018-08-09T11:34:00.076317: step 13398, loss 0.563034.
Train: 2018-08-09T11:34:00.154457: step 13399, loss 0.562893.
Train: 2018-08-09T11:34:00.232563: step 13400, loss 0.467391.
Test: 2018-08-09T11:34:00.732413: step 13400, loss 0.547838.
Train: 2018-08-09T11:34:01.295477: step 13401, loss 0.54667.
Train: 2018-08-09T11:34:01.389176: step 13402, loss 0.49807.
Train: 2018-08-09T11:34:01.467315: step 13403, loss 0.547249.
Train: 2018-08-09T11:34:01.545418: step 13404, loss 0.547576.
Train: 2018-08-09T11:34:01.623524: step 13405, loss 0.594437.
Train: 2018-08-09T11:34:01.717254: step 13406, loss 0.495779.
Train: 2018-08-09T11:34:01.795359: step 13407, loss 0.579529.
Train: 2018-08-09T11:34:01.873436: step 13408, loss 0.663718.
Train: 2018-08-09T11:34:01.951543: step 13409, loss 0.614653.
Train: 2018-08-09T11:34:02.029679: step 13410, loss 0.596317.
Test: 2018-08-09T11:34:02.529531: step 13410, loss 0.547649.
Train: 2018-08-09T11:34:02.607668: step 13411, loss 0.528677.
Train: 2018-08-09T11:34:02.685744: step 13412, loss 0.57817.
Train: 2018-08-09T11:34:02.779503: step 13413, loss 0.545775.
Train: 2018-08-09T11:34:02.857609: step 13414, loss 0.512747.
Train: 2018-08-09T11:34:02.935721: step 13415, loss 0.512301.
Train: 2018-08-09T11:34:03.013823: step 13416, loss 0.495322.
Train: 2018-08-09T11:34:03.107544: step 13417, loss 0.647446.
Train: 2018-08-09T11:34:03.185657: step 13418, loss 0.580527.
Train: 2018-08-09T11:34:03.263764: step 13419, loss 0.599693.
Train: 2018-08-09T11:34:03.341870: step 13420, loss 0.665336.
Test: 2018-08-09T11:34:03.841748: step 13420, loss 0.550047.
Train: 2018-08-09T11:34:03.921300: step 13421, loss 0.645275.
Train: 2018-08-09T11:34:03.999401: step 13422, loss 0.57713.
Train: 2018-08-09T11:34:04.077512: step 13423, loss 0.497907.
Train: 2018-08-09T11:34:04.155589: step 13424, loss 0.610983.
Train: 2018-08-09T11:34:04.233697: step 13425, loss 0.610556.
Train: 2018-08-09T11:34:04.327425: step 13426, loss 0.578993.
Train: 2018-08-09T11:34:04.405560: step 13427, loss 0.642584.
Train: 2018-08-09T11:34:04.483667: step 13428, loss 0.530349.
Train: 2018-08-09T11:34:04.561744: step 13429, loss 0.59454.
Train: 2018-08-09T11:34:04.639881: step 13430, loss 0.530837.
Test: 2018-08-09T11:34:05.139758: step 13430, loss 0.549349.
Train: 2018-08-09T11:34:05.217839: step 13431, loss 0.499519.
Train: 2018-08-09T11:34:05.295976: step 13432, loss 0.626866.
Train: 2018-08-09T11:34:05.374055: step 13433, loss 0.48437.
Train: 2018-08-09T11:34:05.467815: step 13434, loss 0.515674.
Train: 2018-08-09T11:34:05.545917: step 13435, loss 0.610924.
Train: 2018-08-09T11:34:05.624025: step 13436, loss 0.565173.
Train: 2018-08-09T11:34:05.702124: step 13437, loss 0.499356.
Train: 2018-08-09T11:34:05.780207: step 13438, loss 0.56388.
Train: 2018-08-09T11:34:05.858345: step 13439, loss 0.57854.
Train: 2018-08-09T11:34:05.937769: step 13440, loss 0.610315.
Test: 2018-08-09T11:34:06.437692: step 13440, loss 0.552098.
Train: 2018-08-09T11:34:06.515783: step 13441, loss 0.609721.
Train: 2018-08-09T11:34:06.609486: step 13442, loss 0.546669.
Train: 2018-08-09T11:34:06.687623: step 13443, loss 0.483138.
Train: 2018-08-09T11:34:06.765729: step 13444, loss 0.531551.
Train: 2018-08-09T11:34:06.859457: step 13445, loss 0.446918.
Train: 2018-08-09T11:34:06.937564: step 13446, loss 0.69753.
Train: 2018-08-09T11:34:07.015675: step 13447, loss 0.615898.
Train: 2018-08-09T11:34:07.093747: step 13448, loss 0.542262.
Train: 2018-08-09T11:34:07.171855: step 13449, loss 0.531409.
Train: 2018-08-09T11:34:07.249990: step 13450, loss 0.583333.
Test: 2018-08-09T11:34:07.749843: step 13450, loss 0.550293.
Train: 2018-08-09T11:34:07.827979: step 13451, loss 0.51388.
Train: 2018-08-09T11:34:07.907573: step 13452, loss 0.513546.
Train: 2018-08-09T11:34:08.001302: step 13453, loss 0.447872.
Train: 2018-08-09T11:34:08.079439: step 13454, loss 0.545951.
Train: 2018-08-09T11:34:08.157514: step 13455, loss 0.562558.
Train: 2018-08-09T11:34:08.235653: step 13456, loss 0.59716.
Train: 2018-08-09T11:34:08.313728: step 13457, loss 0.583977.
Train: 2018-08-09T11:34:08.391865: step 13458, loss 0.517453.
Train: 2018-08-09T11:34:08.469942: step 13459, loss 0.562636.
Train: 2018-08-09T11:34:08.548079: step 13460, loss 0.512508.
Test: 2018-08-09T11:34:09.047955: step 13460, loss 0.549391.
Train: 2018-08-09T11:34:09.126037: step 13461, loss 0.578584.
Train: 2018-08-09T11:34:09.219795: step 13462, loss 0.596486.
Train: 2018-08-09T11:34:09.297902: step 13463, loss 0.562703.
Train: 2018-08-09T11:34:09.376007: step 13464, loss 0.661325.
Train: 2018-08-09T11:34:09.454085: step 13465, loss 0.497055.
Train: 2018-08-09T11:34:09.532193: step 13466, loss 0.661774.
Train: 2018-08-09T11:34:09.610297: step 13467, loss 0.611534.
Train: 2018-08-09T11:34:09.704025: step 13468, loss 0.579095.
Train: 2018-08-09T11:34:09.782163: step 13469, loss 0.546428.
Train: 2018-08-09T11:34:09.860269: step 13470, loss 0.546609.
Test: 2018-08-09T11:34:10.345764: step 13470, loss 0.54892.
Train: 2018-08-09T11:34:10.439491: step 13471, loss 0.498512.
Train: 2018-08-09T11:34:10.517631: step 13472, loss 0.56311.
Train: 2018-08-09T11:34:10.595736: step 13473, loss 0.514527.
Train: 2018-08-09T11:34:10.673812: step 13474, loss 0.595174.
Train: 2018-08-09T11:34:10.751948: step 13475, loss 0.595039.
Train: 2018-08-09T11:34:10.845677: step 13476, loss 0.59502.
Train: 2018-08-09T11:34:10.923783: step 13477, loss 0.643175.
Train: 2018-08-09T11:34:11.001859: step 13478, loss 0.546806.
Train: 2018-08-09T11:34:11.079993: step 13479, loss 0.57882.
Train: 2018-08-09T11:34:11.158103: step 13480, loss 0.59485.
Test: 2018-08-09T11:34:11.657986: step 13480, loss 0.549185.
Train: 2018-08-09T11:34:11.736087: step 13481, loss 0.515091.
Train: 2018-08-09T11:34:11.814202: step 13482, loss 0.626748.
Train: 2018-08-09T11:34:11.910412: step 13483, loss 0.547019.
Train: 2018-08-09T11:34:11.988551: step 13484, loss 0.59477.
Train: 2018-08-09T11:34:12.066656: step 13485, loss 0.578824.
Train: 2018-08-09T11:34:12.144733: step 13486, loss 0.578877.
Train: 2018-08-09T11:34:12.222870: step 13487, loss 0.547205.
Train: 2018-08-09T11:34:12.300945: step 13488, loss 0.59469.
Train: 2018-08-09T11:34:12.379085: step 13489, loss 0.421018.
Train: 2018-08-09T11:34:12.457189: step 13490, loss 0.594703.
Test: 2018-08-09T11:34:12.966798: step 13490, loss 0.548799.
Train: 2018-08-09T11:34:13.044938: step 13491, loss 0.563046.
Train: 2018-08-09T11:34:13.123041: step 13492, loss 0.563032.
Train: 2018-08-09T11:34:13.201145: step 13493, loss 0.499582.
Train: 2018-08-09T11:34:13.279254: step 13494, loss 0.594722.
Train: 2018-08-09T11:34:13.357361: step 13495, loss 0.499098.
Train: 2018-08-09T11:34:13.451090: step 13496, loss 0.642851.
Train: 2018-08-09T11:34:13.529196: step 13497, loss 0.578835.
Train: 2018-08-09T11:34:13.607271: step 13498, loss 0.562641.
Train: 2018-08-09T11:34:13.685409: step 13499, loss 0.546973.
Train: 2018-08-09T11:34:13.763515: step 13500, loss 0.49792.
Test: 2018-08-09T11:34:14.263393: step 13500, loss 0.549906.
Train: 2018-08-09T11:34:14.842419: step 13501, loss 0.610223.
Train: 2018-08-09T11:34:14.920556: step 13502, loss 0.512563.
Train: 2018-08-09T11:34:14.998659: step 13503, loss 0.494693.
Train: 2018-08-09T11:34:15.092391: step 13504, loss 0.546948.
Train: 2018-08-09T11:34:15.170499: step 13505, loss 0.672036.
Train: 2018-08-09T11:34:15.248578: step 13506, loss 0.507823.
Train: 2018-08-09T11:34:15.326712: step 13507, loss 0.632992.
Train: 2018-08-09T11:34:15.404818: step 13508, loss 0.491273.
Train: 2018-08-09T11:34:15.482926: step 13509, loss 0.629915.
Train: 2018-08-09T11:34:15.561032: step 13510, loss 0.507986.
Test: 2018-08-09T11:34:16.061514: step 13510, loss 0.547367.
Train: 2018-08-09T11:34:16.139650: step 13511, loss 0.614145.
Train: 2018-08-09T11:34:16.217758: step 13512, loss 0.57838.
Train: 2018-08-09T11:34:16.311486: step 13513, loss 0.49152.
Train: 2018-08-09T11:34:16.389591: step 13514, loss 0.546246.
Train: 2018-08-09T11:34:16.467698: step 13515, loss 0.550037.
Train: 2018-08-09T11:34:16.545801: step 13516, loss 0.685822.
Train: 2018-08-09T11:34:16.623911: step 13517, loss 0.665632.
Train: 2018-08-09T11:34:16.701986: step 13518, loss 0.529602.
Train: 2018-08-09T11:34:16.780126: step 13519, loss 0.481575.
Train: 2018-08-09T11:34:16.873851: step 13520, loss 0.59516.
Test: 2018-08-09T11:34:17.358084: step 13520, loss 0.548793.
Train: 2018-08-09T11:34:17.436219: step 13521, loss 0.514786.
Train: 2018-08-09T11:34:17.514324: step 13522, loss 0.499335.
Train: 2018-08-09T11:34:17.592435: step 13523, loss 0.563095.
Train: 2018-08-09T11:34:17.686161: step 13524, loss 0.513925.
Train: 2018-08-09T11:34:17.764269: step 13525, loss 0.562437.
Train: 2018-08-09T11:34:17.842374: step 13526, loss 0.515716.
Train: 2018-08-09T11:34:17.920480: step 13527, loss 0.627568.
Train: 2018-08-09T11:34:17.998590: step 13528, loss 0.530352.
Train: 2018-08-09T11:34:18.076665: step 13529, loss 0.610741.
Train: 2018-08-09T11:34:18.154801: step 13530, loss 0.530725.
Test: 2018-08-09T11:34:18.654678: step 13530, loss 0.550121.
Train: 2018-08-09T11:34:18.732789: step 13531, loss 0.611393.
Train: 2018-08-09T11:34:18.810896: step 13532, loss 0.54705.
Train: 2018-08-09T11:34:18.889003: step 13533, loss 0.498495.
Train: 2018-08-09T11:34:18.984194: step 13534, loss 0.499136.
Train: 2018-08-09T11:34:19.062296: step 13535, loss 0.53045.
Train: 2018-08-09T11:34:19.140433: step 13536, loss 0.482176.
Train: 2018-08-09T11:34:19.218510: step 13537, loss 0.611718.
Train: 2018-08-09T11:34:19.296615: step 13538, loss 0.578161.
Train: 2018-08-09T11:34:19.374754: step 13539, loss 0.513548.
Train: 2018-08-09T11:34:19.452859: step 13540, loss 0.529696.
Test: 2018-08-09T11:34:19.952711: step 13540, loss 0.548186.
Train: 2018-08-09T11:34:20.030817: step 13541, loss 0.529679.
Train: 2018-08-09T11:34:20.124575: step 13542, loss 0.512346.
Train: 2018-08-09T11:34:20.202683: step 13543, loss 0.613423.
Train: 2018-08-09T11:34:20.280759: step 13544, loss 0.545333.
Train: 2018-08-09T11:34:20.358896: step 13545, loss 0.561666.
Train: 2018-08-09T11:34:20.437006: step 13546, loss 0.579936.
Train: 2018-08-09T11:34:20.515078: step 13547, loss 0.545451.
Train: 2018-08-09T11:34:20.593220: step 13548, loss 0.442116.
Train: 2018-08-09T11:34:20.686914: step 13549, loss 0.599528.
Train: 2018-08-09T11:34:20.765050: step 13550, loss 0.545411.
Test: 2018-08-09T11:34:21.266342: step 13550, loss 0.547966.
Train: 2018-08-09T11:34:21.344479: step 13551, loss 0.632538.
Train: 2018-08-09T11:34:21.422585: step 13552, loss 0.545464.
Train: 2018-08-09T11:34:21.500664: step 13553, loss 0.492021.
Train: 2018-08-09T11:34:21.578793: step 13554, loss 0.527743.
Train: 2018-08-09T11:34:21.656904: step 13555, loss 0.546964.
Train: 2018-08-09T11:34:21.735015: step 13556, loss 0.59913.
Train: 2018-08-09T11:34:21.828749: step 13557, loss 0.527108.
Train: 2018-08-09T11:34:21.906816: step 13558, loss 0.527185.
Train: 2018-08-09T11:34:21.984924: step 13559, loss 0.57985.
Train: 2018-08-09T11:34:22.063053: step 13560, loss 0.51201.
Test: 2018-08-09T11:34:22.562912: step 13560, loss 0.547242.
Train: 2018-08-09T11:34:22.641048: step 13561, loss 0.564647.
Train: 2018-08-09T11:34:22.719155: step 13562, loss 0.525275.
Train: 2018-08-09T11:34:22.797263: step 13563, loss 0.509342.
Train: 2018-08-09T11:34:22.875372: step 13564, loss 0.50727.
Train: 2018-08-09T11:34:22.954916: step 13565, loss 0.569355.
Train: 2018-08-09T11:34:23.033022: step 13566, loss 0.618748.
Train: 2018-08-09T11:34:23.126751: step 13567, loss 0.578308.
Train: 2018-08-09T11:34:23.204857: step 13568, loss 0.451943.
Train: 2018-08-09T11:34:23.282968: step 13569, loss 0.469935.
Train: 2018-08-09T11:34:23.361041: step 13570, loss 0.584213.
Test: 2018-08-09T11:34:23.854934: step 13570, loss 0.547088.
Train: 2018-08-09T11:34:23.933040: step 13571, loss 0.546211.
Train: 2018-08-09T11:34:24.011177: step 13572, loss 0.5266.
Train: 2018-08-09T11:34:24.104907: step 13573, loss 0.470792.
Train: 2018-08-09T11:34:24.183011: step 13574, loss 0.484279.
Train: 2018-08-09T11:34:24.261119: step 13575, loss 0.640172.
Train: 2018-08-09T11:34:24.339227: step 13576, loss 0.57474.
Train: 2018-08-09T11:34:24.417302: step 13577, loss 0.447853.
Train: 2018-08-09T11:34:24.495434: step 13578, loss 0.484164.
Train: 2018-08-09T11:34:24.573547: step 13579, loss 0.585892.
Train: 2018-08-09T11:34:24.651623: step 13580, loss 0.623169.
Test: 2018-08-09T11:34:25.154137: step 13580, loss 0.545675.
Train: 2018-08-09T11:34:25.232243: step 13581, loss 0.733153.
Train: 2018-08-09T11:34:25.310357: step 13582, loss 0.509709.
Train: 2018-08-09T11:34:25.404079: step 13583, loss 0.54198.
Train: 2018-08-09T11:34:25.482184: step 13584, loss 0.596381.
Train: 2018-08-09T11:34:25.560291: step 13585, loss 0.561203.
Train: 2018-08-09T11:34:25.638398: step 13586, loss 0.549735.
Train: 2018-08-09T11:34:25.716504: step 13587, loss 0.628246.
Train: 2018-08-09T11:34:25.794580: step 13588, loss 0.614837.
Train: 2018-08-09T11:34:25.872718: step 13589, loss 0.579758.
Train: 2018-08-09T11:34:25.950793: step 13590, loss 0.480257.
Test: 2018-08-09T11:34:26.450678: step 13590, loss 0.549719.
Train: 2018-08-09T11:34:26.528812: step 13591, loss 0.595645.
Train: 2018-08-09T11:34:26.606890: step 13592, loss 0.644269.
Train: 2018-08-09T11:34:26.685029: step 13593, loss 0.529893.
Train: 2018-08-09T11:34:26.763132: step 13594, loss 0.498195.
Train: 2018-08-09T11:34:26.856831: step 13595, loss 0.578792.
Train: 2018-08-09T11:34:26.934966: step 13596, loss 0.498506.
Train: 2018-08-09T11:34:27.013074: step 13597, loss 0.643157.
Train: 2018-08-09T11:34:27.091149: step 13598, loss 0.514706.
Train: 2018-08-09T11:34:27.169290: step 13599, loss 0.578873.
Train: 2018-08-09T11:34:27.247394: step 13600, loss 0.594886.
Test: 2018-08-09T11:34:27.747247: step 13600, loss 0.549085.
Train: 2018-08-09T11:34:28.309642: step 13601, loss 0.546873.
Train: 2018-08-09T11:34:28.387751: step 13602, loss 0.642821.
Train: 2018-08-09T11:34:28.465857: step 13603, loss 0.658649.
Train: 2018-08-09T11:34:28.543964: step 13604, loss 0.578863.
Train: 2018-08-09T11:34:28.622069: step 13605, loss 0.578843.
Train: 2018-08-09T11:34:28.700178: step 13606, loss 0.578871.
Train: 2018-08-09T11:34:28.778253: step 13607, loss 0.578851.
Train: 2018-08-09T11:34:28.856360: step 13608, loss 0.437233.
Train: 2018-08-09T11:34:28.936814: step 13609, loss 0.610351.
Train: 2018-08-09T11:34:29.030512: step 13610, loss 0.594629.
Test: 2018-08-09T11:34:29.514798: step 13610, loss 0.550202.
Train: 2018-08-09T11:34:29.592903: step 13611, loss 0.610198.
Train: 2018-08-09T11:34:29.671016: step 13612, loss 0.469033.
Train: 2018-08-09T11:34:29.764714: step 13613, loss 0.468929.
Train: 2018-08-09T11:34:29.842851: step 13614, loss 0.452473.
Train: 2018-08-09T11:34:29.920956: step 13615, loss 0.531326.
Train: 2018-08-09T11:34:29.999063: step 13616, loss 0.514286.
Train: 2018-08-09T11:34:30.077173: step 13617, loss 0.56317.
Train: 2018-08-09T11:34:30.155246: step 13618, loss 0.613272.
Train: 2018-08-09T11:34:30.233354: step 13619, loss 0.565268.
Train: 2018-08-09T11:34:30.311462: step 13620, loss 0.561424.
Test: 2018-08-09T11:34:30.811342: step 13620, loss 0.548904.
Train: 2018-08-09T11:34:30.889479: step 13621, loss 0.677576.
Train: 2018-08-09T11:34:30.968268: step 13622, loss 0.645101.
Train: 2018-08-09T11:34:31.046370: step 13623, loss 0.628089.
Train: 2018-08-09T11:34:31.124484: step 13624, loss 0.562683.
Train: 2018-08-09T11:34:31.202558: step 13625, loss 0.578923.
Train: 2018-08-09T11:34:31.280698: step 13626, loss 0.451039.
Train: 2018-08-09T11:34:31.374393: step 13627, loss 0.562936.
Train: 2018-08-09T11:34:31.452529: step 13628, loss 0.562794.
Train: 2018-08-09T11:34:31.530639: step 13629, loss 0.578905.
Train: 2018-08-09T11:34:31.608744: step 13630, loss 0.658859.
Test: 2018-08-09T11:34:32.108625: step 13630, loss 0.548559.
Train: 2018-08-09T11:34:32.186731: step 13631, loss 0.515054.
Train: 2018-08-09T11:34:32.264837: step 13632, loss 0.562918.
Train: 2018-08-09T11:34:32.342944: step 13633, loss 0.578856.
Train: 2018-08-09T11:34:32.436641: step 13634, loss 0.674418.
Train: 2018-08-09T11:34:32.514778: step 13635, loss 0.499399.
Train: 2018-08-09T11:34:32.592886: step 13636, loss 0.674137.
Train: 2018-08-09T11:34:32.670992: step 13637, loss 0.594702.
Train: 2018-08-09T11:34:32.749099: step 13638, loss 0.610456.
Train: 2018-08-09T11:34:32.827202: step 13639, loss 0.673358.
Train: 2018-08-09T11:34:32.920903: step 13640, loss 0.594559.
Test: 2018-08-09T11:34:33.405164: step 13640, loss 0.553381.
Train: 2018-08-09T11:34:33.483270: step 13641, loss 0.547675.
Train: 2018-08-09T11:34:33.577028: step 13642, loss 0.547804.
Train: 2018-08-09T11:34:33.655139: step 13643, loss 0.547901.
Train: 2018-08-09T11:34:33.733211: step 13644, loss 0.578927.
Train: 2018-08-09T11:34:33.811319: step 13645, loss 0.625276.
Train: 2018-08-09T11:34:33.889426: step 13646, loss 0.578952.
Train: 2018-08-09T11:34:33.970035: step 13647, loss 0.532867.
Train: 2018-08-09T11:34:34.048142: step 13648, loss 0.594334.
Train: 2018-08-09T11:34:34.126251: step 13649, loss 0.563669.
Train: 2018-08-09T11:34:34.204355: step 13650, loss 0.471895.
Test: 2018-08-09T11:34:34.704219: step 13650, loss 0.55165.
Train: 2018-08-09T11:34:34.782348: step 13651, loss 0.609616.
Train: 2018-08-09T11:34:34.860450: step 13652, loss 0.563682.
Train: 2018-08-09T11:34:34.938559: step 13653, loss 0.517716.
Train: 2018-08-09T11:34:35.016664: step 13654, loss 0.532969.
Train: 2018-08-09T11:34:35.094741: step 13655, loss 0.486745.
Train: 2018-08-09T11:34:35.188498: step 13656, loss 0.578942.
Train: 2018-08-09T11:34:35.266605: step 13657, loss 0.64083.
Train: 2018-08-09T11:34:35.344712: step 13658, loss 0.501351.
Train: 2018-08-09T11:34:35.422787: step 13659, loss 0.578969.
Train: 2018-08-09T11:34:35.500895: step 13660, loss 0.500707.
Test: 2018-08-09T11:34:36.002266: step 13660, loss 0.550236.
Train: 2018-08-09T11:34:36.080405: step 13661, loss 0.531882.
Train: 2018-08-09T11:34:36.158513: step 13662, loss 0.546988.
Train: 2018-08-09T11:34:36.236616: step 13663, loss 0.51442.
Train: 2018-08-09T11:34:36.314692: step 13664, loss 0.498593.
Train: 2018-08-09T11:34:36.392824: step 13665, loss 0.61141.
Train: 2018-08-09T11:34:36.470930: step 13666, loss 0.565172.
Train: 2018-08-09T11:34:36.564658: step 13667, loss 0.420703.
Train: 2018-08-09T11:34:36.642771: step 13668, loss 0.583998.
Train: 2018-08-09T11:34:36.720876: step 13669, loss 0.602115.
Train: 2018-08-09T11:34:36.798954: step 13670, loss 0.618381.
Test: 2018-08-09T11:34:37.298835: step 13670, loss 0.547879.
Train: 2018-08-09T11:34:37.376941: step 13671, loss 0.470238.
Train: 2018-08-09T11:34:37.455048: step 13672, loss 0.634372.
Train: 2018-08-09T11:34:37.533185: step 13673, loss 0.653595.
Train: 2018-08-09T11:34:37.626913: step 13674, loss 0.475154.
Train: 2018-08-09T11:34:37.704988: step 13675, loss 0.548233.
Train: 2018-08-09T11:34:37.783127: step 13676, loss 0.631908.
Train: 2018-08-09T11:34:37.861232: step 13677, loss 0.511071.
Train: 2018-08-09T11:34:37.940646: step 13678, loss 0.580231.
Train: 2018-08-09T11:34:38.018754: step 13679, loss 0.579717.
Train: 2018-08-09T11:34:38.096890: step 13680, loss 0.596214.
Test: 2018-08-09T11:34:38.596752: step 13680, loss 0.549103.
Train: 2018-08-09T11:34:38.690501: step 13681, loss 0.579775.
Train: 2018-08-09T11:34:38.768607: step 13682, loss 0.497525.
Train: 2018-08-09T11:34:38.846685: step 13683, loss 0.546419.
Train: 2018-08-09T11:34:38.924820: step 13684, loss 0.546441.
Train: 2018-08-09T11:34:39.002930: step 13685, loss 0.530877.
Train: 2018-08-09T11:34:39.081006: step 13686, loss 0.514833.
Train: 2018-08-09T11:34:39.159141: step 13687, loss 0.498449.
Train: 2018-08-09T11:34:39.237247: step 13688, loss 0.578736.
Train: 2018-08-09T11:34:39.330974: step 13689, loss 0.595108.
Train: 2018-08-09T11:34:39.409050: step 13690, loss 0.61135.
Test: 2018-08-09T11:34:39.908933: step 13690, loss 0.549917.
Train: 2018-08-09T11:34:39.974270: step 13691, loss 0.546633.
Train: 2018-08-09T11:34:40.068028: step 13692, loss 0.562849.
Train: 2018-08-09T11:34:40.146137: step 13693, loss 0.497677.
Train: 2018-08-09T11:34:40.224241: step 13694, loss 0.514036.
Train: 2018-08-09T11:34:40.302347: step 13695, loss 0.578744.
Train: 2018-08-09T11:34:40.380455: step 13696, loss 0.579312.
Train: 2018-08-09T11:34:40.458561: step 13697, loss 0.562668.
Train: 2018-08-09T11:34:40.536663: step 13698, loss 0.496979.
Train: 2018-08-09T11:34:40.630365: step 13699, loss 0.529644.
Train: 2018-08-09T11:34:40.708502: step 13700, loss 0.694754.
Test: 2018-08-09T11:34:41.208354: step 13700, loss 0.547113.
Train: 2018-08-09T11:34:41.770752: step 13701, loss 0.595353.
Train: 2018-08-09T11:34:41.848858: step 13702, loss 0.644676.
Train: 2018-08-09T11:34:41.928292: step 13703, loss 0.562649.
Train: 2018-08-09T11:34:42.022021: step 13704, loss 0.578974.
Train: 2018-08-09T11:34:42.100126: step 13705, loss 0.644185.
Train: 2018-08-09T11:34:42.178234: step 13706, loss 0.513826.
Train: 2018-08-09T11:34:42.256319: step 13707, loss 0.611386.
Train: 2018-08-09T11:34:42.334416: step 13708, loss 0.514077.
Train: 2018-08-09T11:34:42.412523: step 13709, loss 0.530328.
Train: 2018-08-09T11:34:42.490658: step 13710, loss 0.611171.
Test: 2018-08-09T11:34:42.990511: step 13710, loss 0.549431.
Train: 2018-08-09T11:34:43.068648: step 13711, loss 0.546632.
Train: 2018-08-09T11:34:43.146757: step 13712, loss 0.530456.
Train: 2018-08-09T11:34:43.240491: step 13713, loss 0.594931.
Train: 2018-08-09T11:34:43.318589: step 13714, loss 0.578856.
Train: 2018-08-09T11:34:43.396698: step 13715, loss 0.562696.
Train: 2018-08-09T11:34:43.474802: step 13716, loss 0.546444.
Train: 2018-08-09T11:34:43.552908: step 13717, loss 0.546635.
Train: 2018-08-09T11:34:43.630985: step 13718, loss 0.579013.
Train: 2018-08-09T11:34:43.724738: step 13719, loss 0.643048.
Train: 2018-08-09T11:34:43.802820: step 13720, loss 0.643939.
Test: 2018-08-09T11:34:44.305060: step 13720, loss 0.549545.
Train: 2018-08-09T11:34:44.383197: step 13721, loss 0.610707.
Train: 2018-08-09T11:34:44.461304: step 13722, loss 0.611556.
Train: 2018-08-09T11:34:44.545065: step 13723, loss 0.65848.
Train: 2018-08-09T11:34:44.623172: step 13724, loss 0.578899.
Train: 2018-08-09T11:34:44.701309: step 13725, loss 0.547257.
Train: 2018-08-09T11:34:44.779416: step 13726, loss 0.531763.
Train: 2018-08-09T11:34:44.873114: step 13727, loss 0.563215.
Train: 2018-08-09T11:34:44.951251: step 13728, loss 0.453864.
Train: 2018-08-09T11:34:45.029325: step 13729, loss 0.453834.
Train: 2018-08-09T11:34:45.107460: step 13730, loss 0.735558.
Test: 2018-08-09T11:34:45.607316: step 13730, loss 0.550923.
Train: 2018-08-09T11:34:45.685451: step 13731, loss 0.594476.
Train: 2018-08-09T11:34:45.763528: step 13732, loss 0.5007.
Train: 2018-08-09T11:34:45.841667: step 13733, loss 0.610211.
Train: 2018-08-09T11:34:45.935397: step 13734, loss 0.578934.
Train: 2018-08-09T11:34:46.013470: step 13735, loss 0.563177.
Train: 2018-08-09T11:34:46.091607: step 13736, loss 0.51621.
Train: 2018-08-09T11:34:46.169713: step 13737, loss 0.563093.
Train: 2018-08-09T11:34:46.247789: step 13738, loss 0.578859.
Train: 2018-08-09T11:34:46.341518: step 13739, loss 0.546942.
Train: 2018-08-09T11:34:46.419625: step 13740, loss 0.625958.
Test: 2018-08-09T11:34:46.904972: step 13740, loss 0.546597.
Train: 2018-08-09T11:34:46.983078: step 13741, loss 0.54441.
Train: 2018-08-09T11:34:47.076837: step 13742, loss 0.625787.
Train: 2018-08-09T11:34:47.154945: step 13743, loss 0.496809.
Train: 2018-08-09T11:34:47.233019: step 13744, loss 0.478118.
Train: 2018-08-09T11:34:47.311156: step 13745, loss 0.52872.
Train: 2018-08-09T11:34:47.389266: step 13746, loss 0.543268.
Train: 2018-08-09T11:34:47.482985: step 13747, loss 0.641019.
Train: 2018-08-09T11:34:47.561098: step 13748, loss 0.532811.
Train: 2018-08-09T11:34:47.639205: step 13749, loss 0.550511.
Train: 2018-08-09T11:34:47.717305: step 13750, loss 0.471352.
Test: 2018-08-09T11:34:48.217773: step 13750, loss 0.545123.
Train: 2018-08-09T11:34:48.295904: step 13751, loss 0.602457.
Train: 2018-08-09T11:34:48.374016: step 13752, loss 0.510613.
Train: 2018-08-09T11:34:48.467748: step 13753, loss 0.589366.
Train: 2018-08-09T11:34:48.545816: step 13754, loss 0.550406.
Train: 2018-08-09T11:34:48.623951: step 13755, loss 0.593326.
Train: 2018-08-09T11:34:48.702055: step 13756, loss 0.54439.
Train: 2018-08-09T11:34:48.780165: step 13757, loss 0.527982.
Train: 2018-08-09T11:34:48.858272: step 13758, loss 0.615763.
Train: 2018-08-09T11:34:48.952000: step 13759, loss 0.582048.
Train: 2018-08-09T11:34:49.030076: step 13760, loss 0.579753.
Test: 2018-08-09T11:34:49.529958: step 13760, loss 0.548837.
Train: 2018-08-09T11:34:49.608098: step 13761, loss 0.659542.
Train: 2018-08-09T11:34:49.686201: step 13762, loss 0.642343.
Train: 2018-08-09T11:34:49.764280: step 13763, loss 0.547403.
Train: 2018-08-09T11:34:49.858022: step 13764, loss 0.563744.
Train: 2018-08-09T11:34:49.936143: step 13765, loss 0.625686.
Train: 2018-08-09T11:34:50.014251: step 13766, loss 0.532373.
Train: 2018-08-09T11:34:50.092356: step 13767, loss 0.594398.
Train: 2018-08-09T11:34:50.170463: step 13768, loss 0.548265.
Train: 2018-08-09T11:34:50.248570: step 13769, loss 0.548238.
Train: 2018-08-09T11:34:50.342297: step 13770, loss 0.594309.
Test: 2018-08-09T11:34:50.826528: step 13770, loss 0.551037.
Train: 2018-08-09T11:34:50.906973: step 13771, loss 0.624933.
Train: 2018-08-09T11:34:50.985111: step 13772, loss 0.441378.
Train: 2018-08-09T11:34:51.078838: step 13773, loss 0.517793.
Train: 2018-08-09T11:34:51.156946: step 13774, loss 0.517661.
Train: 2018-08-09T11:34:51.235022: step 13775, loss 0.609709.
Train: 2018-08-09T11:34:51.313129: step 13776, loss 0.625104.
Train: 2018-08-09T11:34:51.391264: step 13777, loss 0.501793.
Train: 2018-08-09T11:34:51.484962: step 13778, loss 0.656191.
Train: 2018-08-09T11:34:51.563100: step 13779, loss 0.563247.
Train: 2018-08-09T11:34:51.641207: step 13780, loss 0.610377.
Test: 2018-08-09T11:34:52.141058: step 13780, loss 0.547669.
Train: 2018-08-09T11:34:52.219195: step 13781, loss 0.50115.
Train: 2018-08-09T11:34:52.297302: step 13782, loss 0.516935.
Train: 2018-08-09T11:34:52.375378: step 13783, loss 0.546787.
Train: 2018-08-09T11:34:52.469105: step 13784, loss 0.720856.
Train: 2018-08-09T11:34:52.547213: step 13785, loss 0.531745.
Train: 2018-08-09T11:34:52.625349: step 13786, loss 0.595158.
Train: 2018-08-09T11:34:52.703456: step 13787, loss 0.595278.
Train: 2018-08-09T11:34:52.797154: step 13788, loss 0.57925.
Train: 2018-08-09T11:34:52.875292: step 13789, loss 0.625134.
Train: 2018-08-09T11:34:52.954790: step 13790, loss 0.594081.
Test: 2018-08-09T11:34:53.454639: step 13790, loss 0.553031.
Train: 2018-08-09T11:34:53.532769: step 13791, loss 0.470674.
Train: 2018-08-09T11:34:53.626503: step 13792, loss 0.578635.
Train: 2018-08-09T11:34:53.704581: step 13793, loss 0.547831.
Train: 2018-08-09T11:34:53.798337: step 13794, loss 0.625251.
Train: 2018-08-09T11:34:53.892066: step 13795, loss 0.641785.
Train: 2018-08-09T11:34:53.970166: step 13796, loss 0.563491.
Train: 2018-08-09T11:34:54.063869: step 13797, loss 0.532645.
Train: 2018-08-09T11:34:54.157628: step 13798, loss 0.470714.
Train: 2018-08-09T11:34:54.235734: step 13799, loss 0.548124.
Train: 2018-08-09T11:34:54.329432: step 13800, loss 0.516965.
Test: 2018-08-09T11:34:54.829324: step 13800, loss 0.548169.
Train: 2018-08-09T11:34:55.362117: step 13801, loss 0.532291.
Train: 2018-08-09T11:34:55.455845: step 13802, loss 0.563193.
Train: 2018-08-09T11:34:55.551329: step 13803, loss 0.531866.
Train: 2018-08-09T11:34:55.629436: step 13804, loss 0.531644.
Train: 2018-08-09T11:34:55.733445: step 13805, loss 0.658845.
Train: 2018-08-09T11:34:55.811552: step 13806, loss 0.609812.
Train: 2018-08-09T11:34:55.905250: step 13807, loss 0.611698.
Train: 2018-08-09T11:34:55.998979: step 13808, loss 0.562809.
Train: 2018-08-09T11:34:56.092735: step 13809, loss 0.578895.
Train: 2018-08-09T11:34:56.186464: step 13810, loss 0.499871.
Test: 2018-08-09T11:34:56.686316: step 13810, loss 0.548205.
Train: 2018-08-09T11:34:56.764422: step 13811, loss 0.499992.
Train: 2018-08-09T11:34:56.858149: step 13812, loss 0.546843.
Train: 2018-08-09T11:34:56.956269: step 13813, loss 0.530899.
Train: 2018-08-09T11:34:57.049966: step 13814, loss 0.530759.
Train: 2018-08-09T11:34:57.143695: step 13815, loss 0.578177.
Train: 2018-08-09T11:34:57.237452: step 13816, loss 0.562531.
Train: 2018-08-09T11:34:57.331180: step 13817, loss 0.498782.
Train: 2018-08-09T11:34:57.424909: step 13818, loss 0.610749.
Train: 2018-08-09T11:34:57.518607: step 13819, loss 0.528732.
Train: 2018-08-09T11:34:57.612335: step 13820, loss 0.495469.
Test: 2018-08-09T11:34:58.096596: step 13820, loss 0.547358.
Train: 2018-08-09T11:34:58.190322: step 13821, loss 0.564611.
Train: 2018-08-09T11:34:58.284050: step 13822, loss 0.544699.
Train: 2018-08-09T11:34:58.377779: step 13823, loss 0.545555.
Train: 2018-08-09T11:34:58.471506: step 13824, loss 0.52577.
Train: 2018-08-09T11:34:58.565264: step 13825, loss 0.513754.
Train: 2018-08-09T11:34:58.658993: step 13826, loss 0.526479.
Train: 2018-08-09T11:34:58.752721: step 13827, loss 0.403771.
Train: 2018-08-09T11:34:58.846449: step 13828, loss 0.580845.
Train: 2018-08-09T11:34:58.940177: step 13829, loss 0.546755.
Train: 2018-08-09T11:34:59.033874: step 13830, loss 0.527037.
Test: 2018-08-09T11:34:59.533758: step 13830, loss 0.544979.
Train: 2018-08-09T11:34:59.627508: step 13831, loss 0.571719.
Train: 2018-08-09T11:34:59.721212: step 13832, loss 0.669639.
Train: 2018-08-09T11:34:59.814971: step 13833, loss 0.483536.
Train: 2018-08-09T11:34:59.910025: step 13834, loss 0.600227.
Train: 2018-08-09T11:35:00.003753: step 13835, loss 0.489846.
Train: 2018-08-09T11:35:00.113103: step 13836, loss 0.558718.
Train: 2018-08-09T11:35:00.206861: step 13837, loss 0.663223.
Train: 2018-08-09T11:35:00.300583: step 13838, loss 0.662315.
Train: 2018-08-09T11:35:00.394287: step 13839, loss 0.492626.
Train: 2018-08-09T11:35:00.488044: step 13840, loss 0.51202.
Test: 2018-08-09T11:35:00.987914: step 13840, loss 0.550042.
Train: 2018-08-09T11:35:01.081654: step 13841, loss 0.599595.
Train: 2018-08-09T11:35:01.175376: step 13842, loss 0.531065.
Train: 2018-08-09T11:35:01.269110: step 13843, loss 0.581741.
Train: 2018-08-09T11:35:01.378430: step 13844, loss 0.629086.
Train: 2018-08-09T11:35:01.472189: step 13845, loss 0.479057.
Train: 2018-08-09T11:35:01.565885: step 13846, loss 0.528097.
Train: 2018-08-09T11:35:01.659643: step 13847, loss 0.595998.
Train: 2018-08-09T11:35:01.753340: step 13848, loss 0.54551.
Train: 2018-08-09T11:35:01.847070: step 13849, loss 0.54639.
Train: 2018-08-09T11:35:01.958915: step 13850, loss 0.529578.
Test: 2018-08-09T11:35:02.458798: step 13850, loss 0.549564.
Train: 2018-08-09T11:35:02.552559: step 13851, loss 0.497361.
Train: 2018-08-09T11:35:02.646282: step 13852, loss 0.430509.
Train: 2018-08-09T11:35:02.739981: step 13853, loss 0.529478.
Train: 2018-08-09T11:35:02.833709: step 13854, loss 0.479183.
Train: 2018-08-09T11:35:02.927466: step 13855, loss 0.629195.
Train: 2018-08-09T11:35:03.021194: step 13856, loss 0.629014.
Train: 2018-08-09T11:35:03.130543: step 13857, loss 0.563155.
Train: 2018-08-09T11:35:03.224271: step 13858, loss 0.629559.
Train: 2018-08-09T11:35:03.318002: step 13859, loss 0.529511.
Train: 2018-08-09T11:35:03.411727: step 13860, loss 0.495692.
Test: 2018-08-09T11:35:03.913905: step 13860, loss 0.551223.
Train: 2018-08-09T11:35:04.007658: step 13861, loss 0.545552.
Train: 2018-08-09T11:35:04.117012: step 13862, loss 0.49567.
Train: 2018-08-09T11:35:04.210743: step 13863, loss 0.579207.
Train: 2018-08-09T11:35:04.304469: step 13864, loss 0.545605.
Train: 2018-08-09T11:35:04.398166: step 13865, loss 0.545494.
Train: 2018-08-09T11:35:04.491924: step 13866, loss 0.579066.
Train: 2018-08-09T11:35:04.601274: step 13867, loss 0.512107.
Train: 2018-08-09T11:35:04.695002: step 13868, loss 0.562914.
Train: 2018-08-09T11:35:04.788729: step 13869, loss 0.595929.
Train: 2018-08-09T11:35:04.882459: step 13870, loss 0.54561.
Test: 2018-08-09T11:35:05.397931: step 13870, loss 0.548413.
Train: 2018-08-09T11:35:05.491683: step 13871, loss 0.545663.
Train: 2018-08-09T11:35:05.585387: step 13872, loss 0.562544.
Train: 2018-08-09T11:35:05.694768: step 13873, loss 0.545444.
Train: 2018-08-09T11:35:05.788463: step 13874, loss 0.443923.
Train: 2018-08-09T11:35:05.882224: step 13875, loss 0.511703.
Train: 2018-08-09T11:35:05.977370: step 13876, loss 0.511567.
Train: 2018-08-09T11:35:06.086717: step 13877, loss 0.579413.
Train: 2018-08-09T11:35:06.180445: step 13878, loss 0.630659.
Train: 2018-08-09T11:35:06.274144: step 13879, loss 0.562302.
Train: 2018-08-09T11:35:06.367872: step 13880, loss 0.511063.
Test: 2018-08-09T11:35:06.867758: step 13880, loss 0.549557.
Train: 2018-08-09T11:35:06.961506: step 13881, loss 0.682059.
Train: 2018-08-09T11:35:07.070861: step 13882, loss 0.408735.
Train: 2018-08-09T11:35:07.164559: step 13883, loss 0.511098.
Train: 2018-08-09T11:35:07.258296: step 13884, loss 0.56225.
Train: 2018-08-09T11:35:07.367666: step 13885, loss 0.56234.
Train: 2018-08-09T11:35:07.461363: step 13886, loss 0.648156.
Train: 2018-08-09T11:35:07.555122: step 13887, loss 0.562622.
Train: 2018-08-09T11:35:07.648820: step 13888, loss 0.493703.
Train: 2018-08-09T11:35:07.758199: step 13889, loss 0.528094.
Train: 2018-08-09T11:35:07.851928: step 13890, loss 0.699963.
Test: 2018-08-09T11:35:08.354199: step 13890, loss 0.547575.
Train: 2018-08-09T11:35:08.447927: step 13891, loss 0.613714.
Train: 2018-08-09T11:35:08.541686: step 13892, loss 0.579467.
Train: 2018-08-09T11:35:08.651004: step 13893, loss 0.47714.
Train: 2018-08-09T11:35:08.744734: step 13894, loss 0.528363.
Train: 2018-08-09T11:35:08.838461: step 13895, loss 0.511343.
Train: 2018-08-09T11:35:08.932189: step 13896, loss 0.715247.
Train: 2018-08-09T11:35:09.041567: step 13897, loss 0.562337.
Train: 2018-08-09T11:35:09.135298: step 13898, loss 0.528533.
Train: 2018-08-09T11:35:09.229023: step 13899, loss 0.579278.
Train: 2018-08-09T11:35:09.338376: step 13900, loss 0.59609.
Test: 2018-08-09T11:35:09.838225: step 13900, loss 0.549175.
Train: 2018-08-09T11:35:10.386508: step 13901, loss 0.478342.
Train: 2018-08-09T11:35:10.495881: step 13902, loss 0.579184.
Train: 2018-08-09T11:35:10.589584: step 13903, loss 0.595941.
Train: 2018-08-09T11:35:10.683312: step 13904, loss 0.545659.
Train: 2018-08-09T11:35:10.777070: step 13905, loss 0.562413.
Train: 2018-08-09T11:35:10.886422: step 13906, loss 0.462206.
Train: 2018-08-09T11:35:10.980118: step 13907, loss 0.595826.
Train: 2018-08-09T11:35:11.073876: step 13908, loss 0.529019.
Train: 2018-08-09T11:35:11.183225: step 13909, loss 0.579127.
Train: 2018-08-09T11:35:11.276923: step 13910, loss 0.562415.
Test: 2018-08-09T11:35:11.776830: step 13910, loss 0.547414.
Train: 2018-08-09T11:35:11.886156: step 13911, loss 0.629188.
Train: 2018-08-09T11:35:11.981310: step 13912, loss 0.562429.
Train: 2018-08-09T11:35:12.075038: step 13913, loss 0.529145.
Train: 2018-08-09T11:35:12.168767: step 13914, loss 0.495904.
Train: 2018-08-09T11:35:12.278117: step 13915, loss 0.628997.
Train: 2018-08-09T11:35:12.371813: step 13916, loss 0.56245.
Train: 2018-08-09T11:35:12.481194: step 13917, loss 0.545843.
Train: 2018-08-09T11:35:12.574891: step 13918, loss 0.562457.
Train: 2018-08-09T11:35:12.668643: step 13919, loss 0.562454.
Train: 2018-08-09T11:35:12.777999: step 13920, loss 0.545876.
Test: 2018-08-09T11:35:13.277850: step 13920, loss 0.550065.
Train: 2018-08-09T11:35:13.371577: step 13921, loss 0.562463.
Train: 2018-08-09T11:35:13.465335: step 13922, loss 0.562475.
Train: 2018-08-09T11:35:13.559063: step 13923, loss 0.479608.
Train: 2018-08-09T11:35:13.668413: step 13924, loss 0.595639.
Train: 2018-08-09T11:35:13.762140: step 13925, loss 0.645454.
Train: 2018-08-09T11:35:13.855869: step 13926, loss 0.612213.
Train: 2018-08-09T11:35:13.967557: step 13927, loss 0.496317.
Train: 2018-08-09T11:35:14.061313: step 13928, loss 0.529433.
Train: 2018-08-09T11:35:14.170662: step 13929, loss 0.479856.
Train: 2018-08-09T11:35:14.264361: step 13930, loss 0.51285.
Test: 2018-08-09T11:35:14.764244: step 13930, loss 0.550701.
Train: 2018-08-09T11:35:14.858002: step 13931, loss 0.59562.
Train: 2018-08-09T11:35:14.967319: step 13932, loss 0.529286.
Train: 2018-08-09T11:35:15.061079: step 13933, loss 0.529247.
Train: 2018-08-09T11:35:15.170426: step 13934, loss 0.678921.
Train: 2018-08-09T11:35:15.264125: step 13935, loss 0.495918.
Train: 2018-08-09T11:35:15.357853: step 13936, loss 0.595722.
Train: 2018-08-09T11:35:15.467203: step 13937, loss 0.562443.
Train: 2018-08-09T11:35:15.560929: step 13938, loss 0.462615.
Train: 2018-08-09T11:35:15.670278: step 13939, loss 0.495787.
Train: 2018-08-09T11:35:15.764037: step 13940, loss 0.612532.
Test: 2018-08-09T11:35:16.264478: step 13940, loss 0.546118.
Train: 2018-08-09T11:35:16.373828: step 13941, loss 0.629264.
Train: 2018-08-09T11:35:16.479677: step 13942, loss 0.595848.
Train: 2018-08-09T11:35:16.573981: step 13943, loss 0.562403.
Train: 2018-08-09T11:35:16.667679: step 13944, loss 0.595813.
Train: 2018-08-09T11:35:16.777028: step 13945, loss 0.595759.
Train: 2018-08-09T11:35:16.870755: step 13946, loss 0.495868.
Train: 2018-08-09T11:35:16.980135: step 13947, loss 0.529178.
Train: 2018-08-09T11:35:17.073863: step 13948, loss 0.612352.
Train: 2018-08-09T11:35:17.183182: step 13949, loss 0.628918.
Train: 2018-08-09T11:35:17.276910: step 13950, loss 0.579049.
Test: 2018-08-09T11:35:17.776817: step 13950, loss 0.550099.
Train: 2018-08-09T11:35:17.870551: step 13951, loss 0.595575.
Train: 2018-08-09T11:35:17.979870: step 13952, loss 0.645069.
Train: 2018-08-09T11:35:18.073597: step 13953, loss 0.611903.
Train: 2018-08-09T11:35:18.167324: step 13954, loss 0.513377.
Train: 2018-08-09T11:35:18.276708: step 13955, loss 0.529885.
Train: 2018-08-09T11:35:18.370402: step 13956, loss 0.59524.
Train: 2018-08-09T11:35:18.464130: step 13957, loss 0.49753.
Train: 2018-08-09T11:35:18.573479: step 13958, loss 0.530087.
Train: 2018-08-09T11:35:18.667238: step 13959, loss 0.513884.
Train: 2018-08-09T11:35:18.779702: step 13960, loss 0.578912.
Test: 2018-08-09T11:35:19.261854: step 13960, loss 0.548617.
Train: 2018-08-09T11:35:19.371177: step 13961, loss 0.546346.
Train: 2018-08-09T11:35:19.464935: step 13962, loss 0.56266.
Train: 2018-08-09T11:35:19.574286: step 13963, loss 0.49739.
Train: 2018-08-09T11:35:19.667983: step 13964, loss 0.497183.
Train: 2018-08-09T11:35:19.761741: step 13965, loss 0.628458.
Train: 2018-08-09T11:35:19.871090: step 13966, loss 0.513363.
Train: 2018-08-09T11:35:19.964788: step 13967, loss 0.595362.
Train: 2018-08-09T11:35:20.074168: step 13968, loss 0.645165.
Train: 2018-08-09T11:35:20.167865: step 13969, loss 0.513291.
Train: 2018-08-09T11:35:20.277244: step 13970, loss 0.447639.
Test: 2018-08-09T11:35:20.777098: step 13970, loss 0.547115.
Train: 2018-08-09T11:35:20.870823: step 13971, loss 0.644782.
Train: 2018-08-09T11:35:20.966845: step 13972, loss 0.513284.
Train: 2018-08-09T11:35:21.076194: step 13973, loss 0.595591.
Train: 2018-08-09T11:35:21.169923: step 13974, loss 0.645015.
Train: 2018-08-09T11:35:21.279273: step 13975, loss 0.595456.
Train: 2018-08-09T11:35:21.373030: step 13976, loss 0.464017.
Train: 2018-08-09T11:35:21.482350: step 13977, loss 0.644693.
Train: 2018-08-09T11:35:21.576077: step 13978, loss 0.628211.
Train: 2018-08-09T11:35:21.685426: step 13979, loss 0.578956.
Train: 2018-08-09T11:35:21.779186: step 13980, loss 0.578944.
Test: 2018-08-09T11:35:22.279037: step 13980, loss 0.549806.
Train: 2018-08-09T11:35:22.372796: step 13981, loss 0.464771.
Train: 2018-08-09T11:35:22.482114: step 13982, loss 0.595226.
Train: 2018-08-09T11:35:22.591463: step 13983, loss 0.595211.
Train: 2018-08-09T11:35:22.685191: step 13984, loss 0.562646.
Train: 2018-08-09T11:35:22.794541: step 13985, loss 0.56266.
Train: 2018-08-09T11:35:22.888268: step 13986, loss 0.562671.
Train: 2018-08-09T11:35:22.983302: step 13987, loss 0.530241.
Train: 2018-08-09T11:35:23.092650: step 13988, loss 0.643769.
Train: 2018-08-09T11:35:23.186377: step 13989, loss 0.530313.
Train: 2018-08-09T11:35:23.280105: step 13990, loss 0.611263.
Test: 2018-08-09T11:35:23.779959: step 13990, loss 0.547592.
Train: 2018-08-09T11:35:23.889337: step 13991, loss 0.449635.
Train: 2018-08-09T11:35:23.983035: step 13992, loss 0.675876.
Train: 2018-08-09T11:35:24.092414: step 13993, loss 0.51431.
Train: 2018-08-09T11:35:24.186142: step 13994, loss 0.546607.
Train: 2018-08-09T11:35:24.279840: step 13995, loss 0.530473.
Train: 2018-08-09T11:35:24.389219: step 13996, loss 0.643469.
Train: 2018-08-09T11:35:24.482948: step 13997, loss 0.482086.
Train: 2018-08-09T11:35:24.576675: step 13998, loss 0.61117.
Train: 2018-08-09T11:35:24.685993: step 13999, loss 0.48205.
Train: 2018-08-09T11:35:24.779753: step 14000, loss 0.65967.
Test: 2018-08-09T11:35:25.280252: step 14000, loss 0.549435.
Train: 2018-08-09T11:35:25.873862: step 14001, loss 0.675792.
Train: 2018-08-09T11:35:25.967590: step 14002, loss 0.562761.
Train: 2018-08-09T11:35:26.061348: step 14003, loss 0.627124.
Train: 2018-08-09T11:35:26.170697: step 14004, loss 0.626992.
Train: 2018-08-09T11:35:26.277558: step 14005, loss 0.578866.
Train: 2018-08-09T11:35:26.374497: step 14006, loss 0.578859.
Train: 2018-08-09T11:35:26.468197: step 14007, loss 0.562976.
Train: 2018-08-09T11:35:26.577578: step 14008, loss 0.594699.
Train: 2018-08-09T11:35:26.671273: step 14009, loss 0.642036.
Train: 2018-08-09T11:35:26.780622: step 14010, loss 0.500202.
Test: 2018-08-09T11:35:27.280504: step 14010, loss 0.550248.
Train: 2018-08-09T11:35:27.374231: step 14011, loss 0.547477.
Train: 2018-08-09T11:35:27.467960: step 14012, loss 0.594548.
Train: 2018-08-09T11:35:27.577309: step 14013, loss 0.516311.
Train: 2018-08-09T11:35:27.682592: step 14014, loss 0.625788.
Train: 2018-08-09T11:35:27.776320: step 14015, loss 0.500851.
Train: 2018-08-09T11:35:27.885669: step 14016, loss 0.719331.
Train: 2018-08-09T11:35:27.979398: step 14017, loss 0.64115.
Train: 2018-08-09T11:35:28.073124: step 14018, loss 0.485874.
Train: 2018-08-09T11:35:28.182443: step 14019, loss 0.625368.
Train: 2018-08-09T11:35:28.276171: step 14020, loss 0.501728.
Test: 2018-08-09T11:35:28.776079: step 14020, loss 0.547308.
Train: 2018-08-09T11:35:28.869812: step 14021, loss 0.501801.
Train: 2018-08-09T11:35:28.979156: step 14022, loss 0.640685.
Train: 2018-08-09T11:35:29.072889: step 14023, loss 0.578948.
Train: 2018-08-09T11:35:29.182238: step 14024, loss 0.717659.
Train: 2018-08-09T11:35:29.275966: step 14025, loss 0.517515.
Train: 2018-08-09T11:35:29.385317: step 14026, loss 0.548311.
Train: 2018-08-09T11:35:29.479045: step 14027, loss 0.548354.
Train: 2018-08-09T11:35:29.572771: step 14028, loss 0.533066.
Train: 2018-08-09T11:35:29.682121: step 14029, loss 0.624934.
Train: 2018-08-09T11:35:29.775852: step 14030, loss 0.5943.
Test: 2018-08-09T11:35:30.277058: step 14030, loss 0.55054.
Train: 2018-08-09T11:35:30.386436: step 14031, loss 0.471963.
Train: 2018-08-09T11:35:30.480164: step 14032, loss 0.471841.
Train: 2018-08-09T11:35:30.589513: step 14033, loss 0.486871.
Train: 2018-08-09T11:35:30.683211: step 14034, loss 0.594362.
Train: 2018-08-09T11:35:30.792592: step 14035, loss 0.594402.
Train: 2018-08-09T11:35:30.886318: step 14036, loss 0.532412.
Train: 2018-08-09T11:35:30.980046: step 14037, loss 0.610016.
Train: 2018-08-09T11:35:31.136260: step 14038, loss 0.500975.
Train: 2018-08-09T11:35:31.229958: step 14039, loss 0.547618.
Train: 2018-08-09T11:35:31.339306: step 14040, loss 0.516142.
Test: 2018-08-09T11:35:31.839214: step 14040, loss 0.551349.
Train: 2018-08-09T11:35:31.934381: step 14041, loss 0.515895.
Train: 2018-08-09T11:35:32.028106: step 14042, loss 0.531426.
Train: 2018-08-09T11:35:32.137459: step 14043, loss 0.594741.
Train: 2018-08-09T11:35:32.231187: step 14044, loss 0.515066.
Train: 2018-08-09T11:35:32.324914: step 14045, loss 0.642903.
Train: 2018-08-09T11:35:32.434233: step 14046, loss 0.626944.
Train: 2018-08-09T11:35:32.527962: step 14047, loss 0.498395.
Train: 2018-08-09T11:35:32.637311: step 14048, loss 0.595143.
Train: 2018-08-09T11:35:32.731069: step 14049, loss 0.627189.
Train: 2018-08-09T11:35:32.840418: step 14050, loss 0.611169.
Test: 2018-08-09T11:35:33.324650: step 14050, loss 0.551811.
Train: 2018-08-09T11:35:33.434028: step 14051, loss 0.562631.
Train: 2018-08-09T11:35:33.527755: step 14052, loss 0.530299.
Train: 2018-08-09T11:35:33.637075: step 14053, loss 0.611718.
Train: 2018-08-09T11:35:33.730834: step 14054, loss 0.595443.
Train: 2018-08-09T11:35:33.840153: step 14055, loss 0.498327.
Train: 2018-08-09T11:35:33.936148: step 14056, loss 0.562836.
Train: 2018-08-09T11:35:34.029905: step 14057, loss 0.578893.
Train: 2018-08-09T11:35:34.139225: step 14058, loss 0.498412.
Train: 2018-08-09T11:35:34.232983: step 14059, loss 0.562758.
Train: 2018-08-09T11:35:34.342332: step 14060, loss 0.578894.
Test: 2018-08-09T11:35:34.826564: step 14060, loss 0.547626.
Train: 2018-08-09T11:35:34.935942: step 14061, loss 0.611151.
Train: 2018-08-09T11:35:35.029640: step 14062, loss 0.64342.
Train: 2018-08-09T11:35:35.138989: step 14063, loss 0.740001.
Train: 2018-08-09T11:35:35.232743: step 14064, loss 0.562821.
Train: 2018-08-09T11:35:35.342097: step 14065, loss 0.546882.
Train: 2018-08-09T11:35:35.435825: step 14066, loss 0.546972.
Train: 2018-08-09T11:35:35.545167: step 14067, loss 0.547043.
Train: 2018-08-09T11:35:35.638870: step 14068, loss 0.435928.
Train: 2018-08-09T11:35:35.732629: step 14069, loss 0.562968.
Train: 2018-08-09T11:35:35.841949: step 14070, loss 0.547055.
Test: 2018-08-09T11:35:36.342486: step 14070, loss 0.548629.
Train: 2018-08-09T11:35:36.436205: step 14071, loss 0.594778.
Train: 2018-08-09T11:35:36.529962: step 14072, loss 0.562936.
Train: 2018-08-09T11:35:36.639282: step 14073, loss 0.642581.
Train: 2018-08-09T11:35:36.733039: step 14074, loss 0.483352.
Train: 2018-08-09T11:35:36.842401: step 14075, loss 0.531072.
Train: 2018-08-09T11:35:36.936117: step 14076, loss 0.515069.
Train: 2018-08-09T11:35:37.029845: step 14077, loss 0.435028.
Train: 2018-08-09T11:35:37.139198: step 14078, loss 0.546778.
Train: 2018-08-09T11:35:37.232891: step 14079, loss 0.57883.
Train: 2018-08-09T11:35:37.342240: step 14080, loss 0.562774.
Test: 2018-08-09T11:35:37.842124: step 14080, loss 0.547435.
Train: 2018-08-09T11:35:37.935851: step 14081, loss 0.530155.
Train: 2018-08-09T11:35:38.029608: step 14082, loss 0.448396.
Train: 2018-08-09T11:35:38.138962: step 14083, loss 0.57916.
Train: 2018-08-09T11:35:38.232686: step 14084, loss 0.612272.
Train: 2018-08-09T11:35:38.342035: step 14085, loss 0.579458.
Train: 2018-08-09T11:35:38.435763: step 14086, loss 0.695995.
Train: 2018-08-09T11:35:38.529491: step 14087, loss 0.46318.
Train: 2018-08-09T11:35:38.638810: step 14088, loss 0.545804.
Train: 2018-08-09T11:35:38.732539: step 14089, loss 0.496298.
Train: 2018-08-09T11:35:38.841887: step 14090, loss 0.579141.
Test: 2018-08-09T11:35:39.328507: step 14090, loss 0.548138.
Train: 2018-08-09T11:35:39.437862: step 14091, loss 0.512613.
Train: 2018-08-09T11:35:39.531560: step 14092, loss 0.612393.
Train: 2018-08-09T11:35:39.640909: step 14093, loss 0.595681.
Train: 2018-08-09T11:35:39.734666: step 14094, loss 0.54558.
Train: 2018-08-09T11:35:39.828394: step 14095, loss 0.545725.
Train: 2018-08-09T11:35:39.937745: step 14096, loss 0.461896.
Train: 2018-08-09T11:35:40.031471: step 14097, loss 0.511811.
Train: 2018-08-09T11:35:40.125200: step 14098, loss 0.613776.
Train: 2018-08-09T11:35:40.234548: step 14099, loss 0.561181.
Train: 2018-08-09T11:35:40.328277: step 14100, loss 0.563228.
Test: 2018-08-09T11:35:40.828128: step 14100, loss 0.548901.
Train: 2018-08-09T11:35:41.376498: step 14101, loss 0.460099.
Train: 2018-08-09T11:35:41.470195: step 14102, loss 0.528622.
Train: 2018-08-09T11:35:41.579545: step 14103, loss 0.615057.
Train: 2018-08-09T11:35:41.673273: step 14104, loss 0.579259.
Train: 2018-08-09T11:35:41.782623: step 14105, loss 0.545977.
Train: 2018-08-09T11:35:41.876380: step 14106, loss 0.371727.
Train: 2018-08-09T11:35:41.985724: step 14107, loss 0.58084.
Train: 2018-08-09T11:35:42.079428: step 14108, loss 0.578917.
Train: 2018-08-09T11:35:42.173155: step 14109, loss 0.7025.
Train: 2018-08-09T11:35:42.282535: step 14110, loss 0.646514.
Test: 2018-08-09T11:35:42.782387: step 14110, loss 0.548762.
Train: 2018-08-09T11:35:42.876144: step 14111, loss 0.614615.
Train: 2018-08-09T11:35:42.972195: step 14112, loss 0.666854.
Train: 2018-08-09T11:35:43.081537: step 14113, loss 0.527736.
Train: 2018-08-09T11:35:43.175239: step 14114, loss 0.613676.
Train: 2018-08-09T11:35:43.268997: step 14115, loss 0.445614.
Train: 2018-08-09T11:35:43.378348: step 14116, loss 0.562596.
Train: 2018-08-09T11:35:43.472077: step 14117, loss 0.595666.
Train: 2018-08-09T11:35:43.565772: step 14118, loss 0.562584.
Train: 2018-08-09T11:35:43.675152: step 14119, loss 0.545752.
Train: 2018-08-09T11:35:43.768849: step 14120, loss 0.545894.
Test: 2018-08-09T11:35:44.268733: step 14120, loss 0.548201.
Train: 2018-08-09T11:35:44.362490: step 14121, loss 0.529141.
Train: 2018-08-09T11:35:44.471842: step 14122, loss 0.528766.
Train: 2018-08-09T11:35:44.565538: step 14123, loss 0.596859.
Train: 2018-08-09T11:35:44.659298: step 14124, loss 0.612372.
Train: 2018-08-09T11:35:44.768629: step 14125, loss 0.54622.
Train: 2018-08-09T11:35:44.862342: step 14126, loss 0.610911.
Train: 2018-08-09T11:35:44.958383: step 14127, loss 0.445902.
Train: 2018-08-09T11:35:45.067732: step 14128, loss 0.595447.
Train: 2018-08-09T11:35:45.161462: step 14129, loss 0.64612.
Train: 2018-08-09T11:35:45.255158: step 14130, loss 0.511914.
Test: 2018-08-09T11:35:45.755041: step 14130, loss 0.548663.
Train: 2018-08-09T11:35:45.848769: step 14131, loss 0.595063.
Train: 2018-08-09T11:35:45.958147: step 14132, loss 0.5812.
Train: 2018-08-09T11:35:46.051846: step 14133, loss 0.610619.
Train: 2018-08-09T11:35:46.145604: step 14134, loss 0.528566.
Train: 2018-08-09T11:35:46.254953: step 14135, loss 0.545485.
Train: 2018-08-09T11:35:46.348680: step 14136, loss 0.576266.
Train: 2018-08-09T11:35:46.442408: step 14137, loss 0.579451.
Train: 2018-08-09T11:35:46.551752: step 14138, loss 0.479902.
Train: 2018-08-09T11:35:46.645485: step 14139, loss 0.611685.
Train: 2018-08-09T11:35:46.739214: step 14140, loss 0.662939.
Test: 2018-08-09T11:35:47.240579: step 14140, loss 0.546767.
Train: 2018-08-09T11:35:47.334305: step 14141, loss 0.560747.
Train: 2018-08-09T11:35:47.428064: step 14142, loss 0.614088.
Train: 2018-08-09T11:35:47.537413: step 14143, loss 0.465265.
Train: 2018-08-09T11:35:47.631110: step 14144, loss 0.578639.
Train: 2018-08-09T11:35:47.724873: step 14145, loss 0.577214.
Train: 2018-08-09T11:35:47.818597: step 14146, loss 0.496439.
Train: 2018-08-09T11:35:47.912328: step 14147, loss 0.581402.
Train: 2018-08-09T11:35:48.021668: step 14148, loss 0.530657.
Train: 2018-08-09T11:35:48.115372: step 14149, loss 0.596069.
Train: 2018-08-09T11:35:48.209129: step 14150, loss 0.612571.
Test: 2018-08-09T11:35:48.694048: step 14150, loss 0.548654.
Train: 2018-08-09T11:35:48.803422: step 14151, loss 0.579652.
Train: 2018-08-09T11:35:48.897155: step 14152, loss 0.498846.
Train: 2018-08-09T11:35:48.990883: step 14153, loss 0.561897.
Train: 2018-08-09T11:35:49.084611: step 14154, loss 0.513608.
Train: 2018-08-09T11:35:49.178309: step 14155, loss 0.514179.
Train: 2018-08-09T11:35:49.272067: step 14156, loss 0.659669.
Train: 2018-08-09T11:35:49.365795: step 14157, loss 0.644423.
Train: 2018-08-09T11:35:49.459494: step 14158, loss 0.515619.
Train: 2018-08-09T11:35:49.553221: step 14159, loss 0.530891.
Train: 2018-08-09T11:35:49.646979: step 14160, loss 0.595746.
Test: 2018-08-09T11:35:50.146839: step 14160, loss 0.54913.
Train: 2018-08-09T11:35:50.224968: step 14161, loss 0.562883.
Train: 2018-08-09T11:35:50.318696: step 14162, loss 0.579263.
Train: 2018-08-09T11:35:50.412424: step 14163, loss 0.595153.
Train: 2018-08-09T11:35:50.506121: step 14164, loss 0.562903.
Train: 2018-08-09T11:35:50.599849: step 14165, loss 0.467946.
Train: 2018-08-09T11:35:50.693607: step 14166, loss 0.515426.
Train: 2018-08-09T11:35:50.787335: step 14167, loss 0.515271.
Train: 2018-08-09T11:35:50.866406: step 14168, loss 0.546821.
Train: 2018-08-09T11:35:50.960133: step 14169, loss 0.482589.
Train: 2018-08-09T11:35:51.053832: step 14170, loss 0.578732.
Test: 2018-08-09T11:35:51.553739: step 14170, loss 0.550424.
Train: 2018-08-09T11:35:51.631851: step 14171, loss 0.497325.
Train: 2018-08-09T11:35:51.725578: step 14172, loss 0.612509.
Train: 2018-08-09T11:35:51.819277: step 14173, loss 0.478876.
Train: 2018-08-09T11:35:51.897413: step 14174, loss 0.545643.
Train: 2018-08-09T11:35:51.991141: step 14175, loss 0.510952.
Train: 2018-08-09T11:35:52.084869: step 14176, loss 0.578377.
Train: 2018-08-09T11:35:52.162975: step 14177, loss 0.49155.
Train: 2018-08-09T11:35:52.256705: step 14178, loss 0.534267.
Train: 2018-08-09T11:35:52.350431: step 14179, loss 0.548421.
Train: 2018-08-09T11:35:52.428508: step 14180, loss 0.661986.
Test: 2018-08-09T11:35:52.929950: step 14180, loss 0.547065.
Train: 2018-08-09T11:35:53.008086: step 14181, loss 0.545513.
Train: 2018-08-09T11:35:53.101783: step 14182, loss 0.512284.
Train: 2018-08-09T11:35:53.195511: step 14183, loss 0.459473.
Train: 2018-08-09T11:35:53.273617: step 14184, loss 0.56276.
Train: 2018-08-09T11:35:53.367376: step 14185, loss 0.525794.
Train: 2018-08-09T11:35:53.445477: step 14186, loss 0.633521.
Train: 2018-08-09T11:35:53.539210: step 14187, loss 0.528336.
Train: 2018-08-09T11:35:53.617286: step 14188, loss 0.564715.
Train: 2018-08-09T11:35:53.711045: step 14189, loss 0.545949.
Train: 2018-08-09T11:35:53.804772: step 14190, loss 0.4598.
Test: 2018-08-09T11:35:54.304625: step 14190, loss 0.549682.
Train: 2018-08-09T11:35:54.382730: step 14191, loss 0.547783.
Train: 2018-08-09T11:35:54.476489: step 14192, loss 0.49397.
Train: 2018-08-09T11:35:54.554597: step 14193, loss 0.477753.
Train: 2018-08-09T11:35:54.648324: step 14194, loss 0.528458.
Train: 2018-08-09T11:35:54.726430: step 14195, loss 0.546047.
Train: 2018-08-09T11:35:54.820158: step 14196, loss 0.545613.
Train: 2018-08-09T11:35:54.898266: step 14197, loss 0.442613.
Train: 2018-08-09T11:35:54.977730: step 14198, loss 0.511635.
Train: 2018-08-09T11:35:55.071425: step 14199, loss 0.562648.
Train: 2018-08-09T11:35:55.149530: step 14200, loss 0.597162.
Test: 2018-08-09T11:35:55.649444: step 14200, loss 0.547406.
Train: 2018-08-09T11:35:56.230195: step 14201, loss 0.682727.
Train: 2018-08-09T11:35:56.308302: step 14202, loss 0.388924.
Train: 2018-08-09T11:35:56.402030: step 14203, loss 0.562522.
Train: 2018-08-09T11:35:56.480167: step 14204, loss 0.510323.
Train: 2018-08-09T11:35:56.573895: step 14205, loss 0.579818.
Train: 2018-08-09T11:35:56.652003: step 14206, loss 0.45694.
Train: 2018-08-09T11:35:56.745730: step 14207, loss 0.562491.
Train: 2018-08-09T11:35:56.823836: step 14208, loss 0.562363.
Train: 2018-08-09T11:35:56.915264: step 14209, loss 0.4735.
Train: 2018-08-09T11:35:56.998512: step 14210, loss 0.454373.
Test: 2018-08-09T11:35:57.482773: step 14210, loss 0.546378.
Train: 2018-08-09T11:35:57.576511: step 14211, loss 0.543939.
Train: 2018-08-09T11:35:57.654606: step 14212, loss 0.580879.
Train: 2018-08-09T11:35:57.732745: step 14213, loss 0.601467.
Train: 2018-08-09T11:35:57.826471: step 14214, loss 0.58397.
Train: 2018-08-09T11:35:57.904578: step 14215, loss 0.526283.
Train: 2018-08-09T11:35:57.982686: step 14216, loss 0.618833.
Train: 2018-08-09T11:35:58.076412: step 14217, loss 0.637176.
Train: 2018-08-09T11:35:58.154519: step 14218, loss 0.525256.
Train: 2018-08-09T11:35:58.232627: step 14219, loss 0.508954.
Train: 2018-08-09T11:35:58.326353: step 14220, loss 0.544497.
Test: 2018-08-09T11:35:58.826206: step 14220, loss 0.54719.
Train: 2018-08-09T11:35:58.904342: step 14221, loss 0.598785.
Train: 2018-08-09T11:35:58.982449: step 14222, loss 0.492131.
Train: 2018-08-09T11:35:59.076178: step 14223, loss 0.491977.
Train: 2018-08-09T11:35:59.154284: step 14224, loss 0.562225.
Train: 2018-08-09T11:35:59.232390: step 14225, loss 0.562091.
Train: 2018-08-09T11:35:59.326089: step 14226, loss 0.474393.
Train: 2018-08-09T11:35:59.404225: step 14227, loss 0.527237.
Train: 2018-08-09T11:35:59.482331: step 14228, loss 0.544099.
Train: 2018-08-09T11:35:59.576060: step 14229, loss 0.668404.
Train: 2018-08-09T11:35:59.654166: step 14230, loss 0.579221.
Test: 2018-08-09T11:36:00.155584: step 14230, loss 0.546432.
Train: 2018-08-09T11:36:00.233689: step 14231, loss 0.544533.
Train: 2018-08-09T11:36:00.327449: step 14232, loss 0.632352.
Train: 2018-08-09T11:36:00.405556: step 14233, loss 0.581829.
Train: 2018-08-09T11:36:00.483661: step 14234, loss 0.474864.
Train: 2018-08-09T11:36:00.577383: step 14235, loss 0.440393.
Train: 2018-08-09T11:36:00.655500: step 14236, loss 0.439683.
Train: 2018-08-09T11:36:00.733572: step 14237, loss 0.545717.
Train: 2018-08-09T11:36:00.811709: step 14238, loss 0.578422.
Train: 2018-08-09T11:36:00.905438: step 14239, loss 0.544234.
Train: 2018-08-09T11:36:00.983543: step 14240, loss 0.599075.
Test: 2018-08-09T11:36:01.483395: step 14240, loss 0.545189.
Train: 2018-08-09T11:36:01.561503: step 14241, loss 0.546467.
Train: 2018-08-09T11:36:01.655229: step 14242, loss 0.562885.
Train: 2018-08-09T11:36:01.733368: step 14243, loss 0.668496.
Train: 2018-08-09T11:36:01.811474: step 14244, loss 0.68366.
Train: 2018-08-09T11:36:01.889580: step 14245, loss 0.579186.
Train: 2018-08-09T11:36:01.985622: step 14246, loss 0.579439.
Train: 2018-08-09T11:36:02.063727: step 14247, loss 0.579591.
Train: 2018-08-09T11:36:02.141840: step 14248, loss 0.442478.
Train: 2018-08-09T11:36:02.235564: step 14249, loss 0.767495.
Train: 2018-08-09T11:36:02.313638: step 14250, loss 0.545355.
Test: 2018-08-09T11:36:02.813522: step 14250, loss 0.547841.
Train: 2018-08-09T11:36:02.891627: step 14251, loss 0.528686.
Train: 2018-08-09T11:36:02.969733: step 14252, loss 0.646442.
Train: 2018-08-09T11:36:03.063487: step 14253, loss 0.612535.
Train: 2018-08-09T11:36:03.141567: step 14254, loss 0.628996.
Train: 2018-08-09T11:36:03.219704: step 14255, loss 0.578985.
Train: 2018-08-09T11:36:03.313434: step 14256, loss 0.546034.
Train: 2018-08-09T11:36:03.391539: step 14257, loss 0.513555.
Train: 2018-08-09T11:36:03.469616: step 14258, loss 0.578859.
Train: 2018-08-09T11:36:03.563375: step 14259, loss 0.627582.
Train: 2018-08-09T11:36:03.641480: step 14260, loss 0.530452.
Test: 2018-08-09T11:36:04.140148: step 14260, loss 0.547678.
Train: 2018-08-09T11:36:04.218284: step 14261, loss 0.450019.
Train: 2018-08-09T11:36:04.296392: step 14262, loss 0.659377.
Train: 2018-08-09T11:36:04.390120: step 14263, loss 0.498499.
Train: 2018-08-09T11:36:04.468226: step 14264, loss 0.53085.
Train: 2018-08-09T11:36:04.546302: step 14265, loss 0.434352.
Train: 2018-08-09T11:36:04.640065: step 14266, loss 0.594959.
Train: 2018-08-09T11:36:04.718168: step 14267, loss 0.643381.
Train: 2018-08-09T11:36:04.796244: step 14268, loss 0.594982.
Train: 2018-08-09T11:36:04.874351: step 14269, loss 0.65935.
Train: 2018-08-09T11:36:04.968108: step 14270, loss 0.578887.
Test: 2018-08-09T11:36:05.467961: step 14270, loss 0.550227.
Train: 2018-08-09T11:36:05.546067: step 14271, loss 0.659076.
Train: 2018-08-09T11:36:05.624206: step 14272, loss 0.498974.
Train: 2018-08-09T11:36:05.702311: step 14273, loss 0.531021.
Train: 2018-08-09T11:36:05.780388: step 14274, loss 0.578859.
Train: 2018-08-09T11:36:05.874147: step 14275, loss 0.610676.
Train: 2018-08-09T11:36:05.954555: step 14276, loss 0.435923.
Train: 2018-08-09T11:36:06.032661: step 14277, loss 0.499403.
Train: 2018-08-09T11:36:06.110767: step 14278, loss 0.499274.
Train: 2018-08-09T11:36:06.188845: step 14279, loss 0.467167.
Train: 2018-08-09T11:36:06.282602: step 14280, loss 0.642938.
Test: 2018-08-09T11:36:06.782455: step 14280, loss 0.548375.
Train: 2018-08-09T11:36:06.860586: step 14281, loss 0.466476.
Train: 2018-08-09T11:36:06.938698: step 14282, loss 0.546643.
Train: 2018-08-09T11:36:07.032428: step 14283, loss 0.530374.
Train: 2018-08-09T11:36:07.110504: step 14284, loss 0.595091.
Train: 2018-08-09T11:36:07.188608: step 14285, loss 0.546404.
Train: 2018-08-09T11:36:07.282337: step 14286, loss 0.546221.
Train: 2018-08-09T11:36:07.360443: step 14287, loss 0.693866.
Train: 2018-08-09T11:36:07.438580: step 14288, loss 0.562431.
Train: 2018-08-09T11:36:07.516688: step 14289, loss 0.578911.
Train: 2018-08-09T11:36:07.610414: step 14290, loss 0.56266.
Test: 2018-08-09T11:36:08.096029: step 14290, loss 0.550913.
Train: 2018-08-09T11:36:08.174166: step 14291, loss 0.628146.
Train: 2018-08-09T11:36:08.267895: step 14292, loss 0.529826.
Train: 2018-08-09T11:36:08.346001: step 14293, loss 0.660802.
Train: 2018-08-09T11:36:08.424078: step 14294, loss 0.693252.
Train: 2018-08-09T11:36:08.502213: step 14295, loss 0.578924.
Train: 2018-08-09T11:36:08.595948: step 14296, loss 0.611323.
Train: 2018-08-09T11:36:08.674048: step 14297, loss 0.611181.
Train: 2018-08-09T11:36:08.752124: step 14298, loss 0.546718.
Train: 2018-08-09T11:36:08.830233: step 14299, loss 0.578873.
Train: 2018-08-09T11:36:08.908367: step 14300, loss 0.562887.
Test: 2018-08-09T11:36:09.408220: step 14300, loss 0.549221.
Train: 2018-08-09T11:36:10.001860: step 14301, loss 0.610707.
Train: 2018-08-09T11:36:10.079936: step 14302, loss 0.689937.
Train: 2018-08-09T11:36:10.158074: step 14303, loss 0.515712.
Train: 2018-08-09T11:36:10.251802: step 14304, loss 0.56313.
Train: 2018-08-09T11:36:10.329877: step 14305, loss 0.672962.
Train: 2018-08-09T11:36:10.408016: step 14306, loss 0.578898.
Train: 2018-08-09T11:36:10.486120: step 14307, loss 0.594463.
Train: 2018-08-09T11:36:10.564229: step 14308, loss 0.516999.
Train: 2018-08-09T11:36:10.657956: step 14309, loss 0.625239.
Train: 2018-08-09T11:36:10.736062: step 14310, loss 0.532799.
Test: 2018-08-09T11:36:11.235914: step 14310, loss 0.549824.
Train: 2018-08-09T11:36:11.314020: step 14311, loss 0.57895.
Train: 2018-08-09T11:36:11.392157: step 14312, loss 0.548363.
Train: 2018-08-09T11:36:11.470235: step 14313, loss 0.533136.
Train: 2018-08-09T11:36:11.548340: step 14314, loss 0.533111.
Train: 2018-08-09T11:36:11.626477: step 14315, loss 0.563718.
Train: 2018-08-09T11:36:11.720206: step 14316, loss 0.578998.
Train: 2018-08-09T11:36:11.798312: step 14317, loss 0.640231.
Train: 2018-08-09T11:36:11.876419: step 14318, loss 0.594296.
Train: 2018-08-09T11:36:11.956757: step 14319, loss 0.579005.
Train: 2018-08-09T11:36:12.034893: step 14320, loss 0.456854.
Test: 2018-08-09T11:36:12.534745: step 14320, loss 0.550544.
Train: 2018-08-09T11:36:12.612882: step 14321, loss 0.655447.
Train: 2018-08-09T11:36:12.706611: step 14322, loss 0.579001.
Train: 2018-08-09T11:36:12.784687: step 14323, loss 0.548439.
Train: 2018-08-09T11:36:12.862823: step 14324, loss 0.517848.
Train: 2018-08-09T11:36:12.940931: step 14325, loss 0.563662.
Train: 2018-08-09T11:36:13.019007: step 14326, loss 0.578999.
Train: 2018-08-09T11:36:13.097143: step 14327, loss 0.625053.
Train: 2018-08-09T11:36:13.190872: step 14328, loss 0.578937.
Train: 2018-08-09T11:36:13.268979: step 14329, loss 0.502033.
Train: 2018-08-09T11:36:13.347055: step 14330, loss 0.501826.
Test: 2018-08-09T11:36:13.846937: step 14330, loss 0.551757.
Train: 2018-08-09T11:36:13.927347: step 14331, loss 0.516989.
Train: 2018-08-09T11:36:14.005454: step 14332, loss 0.532178.
Train: 2018-08-09T11:36:14.083562: step 14333, loss 0.484762.
Train: 2018-08-09T11:36:14.161698: step 14334, loss 0.546802.
Train: 2018-08-09T11:36:14.239804: step 14335, loss 0.561201.
Train: 2018-08-09T11:36:14.333533: step 14336, loss 0.578756.
Train: 2018-08-09T11:36:14.411609: step 14337, loss 0.575032.
Train: 2018-08-09T11:36:14.489745: step 14338, loss 0.594718.
Train: 2018-08-09T11:36:14.567853: step 14339, loss 0.609899.
Train: 2018-08-09T11:36:14.645958: step 14340, loss 0.487585.
Test: 2018-08-09T11:36:15.145811: step 14340, loss 0.549378.
Train: 2018-08-09T11:36:15.223947: step 14341, loss 0.514904.
Train: 2018-08-09T11:36:15.302054: step 14342, loss 0.635588.
Train: 2018-08-09T11:36:15.395783: step 14343, loss 0.562984.
Train: 2018-08-09T11:36:15.473888: step 14344, loss 0.563652.
Train: 2018-08-09T11:36:15.551966: step 14345, loss 0.611838.
Train: 2018-08-09T11:36:15.630073: step 14346, loss 0.594918.
Train: 2018-08-09T11:36:15.708208: step 14347, loss 0.562784.
Train: 2018-08-09T11:36:15.786316: step 14348, loss 0.578819.
Train: 2018-08-09T11:36:15.880046: step 14349, loss 0.547188.
Train: 2018-08-09T11:36:15.960533: step 14350, loss 0.578809.
Test: 2018-08-09T11:36:16.460395: step 14350, loss 0.548216.
Train: 2018-08-09T11:36:16.538491: step 14351, loss 0.547141.
Train: 2018-08-09T11:36:16.616628: step 14352, loss 0.610596.
Train: 2018-08-09T11:36:16.694738: step 14353, loss 0.57891.
Train: 2018-08-09T11:36:16.788465: step 14354, loss 0.547177.
Train: 2018-08-09T11:36:16.866569: step 14355, loss 0.563027.
Train: 2018-08-09T11:36:16.944677: step 14356, loss 0.468248.
Train: 2018-08-09T11:36:17.022782: step 14357, loss 0.610426.
Train: 2018-08-09T11:36:17.100891: step 14358, loss 0.562914.
Train: 2018-08-09T11:36:17.194617: step 14359, loss 0.642666.
Train: 2018-08-09T11:36:17.272723: step 14360, loss 0.610508.
Test: 2018-08-09T11:36:17.772577: step 14360, loss 0.551157.
Train: 2018-08-09T11:36:17.850681: step 14361, loss 0.547085.
Train: 2018-08-09T11:36:17.930313: step 14362, loss 0.547304.
Train: 2018-08-09T11:36:18.008422: step 14363, loss 0.59501.
Train: 2018-08-09T11:36:18.086526: step 14364, loss 0.562836.
Train: 2018-08-09T11:36:18.180255: step 14365, loss 0.483807.
Train: 2018-08-09T11:36:18.258362: step 14366, loss 0.499575.
Train: 2018-08-09T11:36:18.336472: step 14367, loss 0.57944.
Train: 2018-08-09T11:36:18.414574: step 14368, loss 0.610811.
Train: 2018-08-09T11:36:18.492683: step 14369, loss 0.451382.
Train: 2018-08-09T11:36:18.586413: step 14370, loss 0.515539.
Test: 2018-08-09T11:36:19.070669: step 14370, loss 0.551351.
Train: 2018-08-09T11:36:19.164369: step 14371, loss 0.41771.
Train: 2018-08-09T11:36:19.242504: step 14372, loss 0.661323.
Train: 2018-08-09T11:36:19.320607: step 14373, loss 0.611452.
Train: 2018-08-09T11:36:19.398718: step 14374, loss 0.513752.
Train: 2018-08-09T11:36:19.476824: step 14375, loss 0.54502.
Train: 2018-08-09T11:36:19.570553: step 14376, loss 0.562719.
Train: 2018-08-09T11:36:19.648629: step 14377, loss 0.493479.
Train: 2018-08-09T11:36:19.726765: step 14378, loss 0.579369.
Train: 2018-08-09T11:36:19.804876: step 14379, loss 0.564085.
Train: 2018-08-09T11:36:19.882978: step 14380, loss 0.600092.
Test: 2018-08-09T11:36:20.385214: step 14380, loss 0.549398.
Train: 2018-08-09T11:36:20.463320: step 14381, loss 0.578939.
Train: 2018-08-09T11:36:20.541456: step 14382, loss 0.543666.
Train: 2018-08-09T11:36:20.624269: step 14383, loss 0.597326.
Train: 2018-08-09T11:36:20.702374: step 14384, loss 0.63018.
Train: 2018-08-09T11:36:20.796102: step 14385, loss 0.527833.
Train: 2018-08-09T11:36:20.874181: step 14386, loss 0.594557.
Train: 2018-08-09T11:36:20.952315: step 14387, loss 0.480004.
Train: 2018-08-09T11:36:21.030421: step 14388, loss 0.545937.
Train: 2018-08-09T11:36:21.108499: step 14389, loss 0.579849.
Train: 2018-08-09T11:36:21.202256: step 14390, loss 0.496501.
Test: 2018-08-09T11:36:21.702108: step 14390, loss 0.550137.
Train: 2018-08-09T11:36:21.780214: step 14391, loss 0.662206.
Train: 2018-08-09T11:36:21.858321: step 14392, loss 0.578443.
Train: 2018-08-09T11:36:21.933087: step 14393, loss 0.64459.
Train: 2018-08-09T11:36:22.026813: step 14394, loss 0.628223.
Train: 2018-08-09T11:36:22.104920: step 14395, loss 0.529949.
Train: 2018-08-09T11:36:22.183058: step 14396, loss 0.562831.
Train: 2018-08-09T11:36:22.261163: step 14397, loss 0.578894.
Train: 2018-08-09T11:36:22.339241: step 14398, loss 0.514665.
Train: 2018-08-09T11:36:22.432999: step 14399, loss 0.546783.
Train: 2018-08-09T11:36:22.511105: step 14400, loss 0.56282.
Test: 2018-08-09T11:36:23.000586: step 14400, loss 0.549043.
Train: 2018-08-09T11:36:23.578574: step 14401, loss 0.64288.
Train: 2018-08-09T11:36:23.656711: step 14402, loss 0.562843.
Train: 2018-08-09T11:36:23.734816: step 14403, loss 0.530949.
Train: 2018-08-09T11:36:23.828517: step 14404, loss 0.56291.
Train: 2018-08-09T11:36:23.906623: step 14405, loss 0.658614.
Train: 2018-08-09T11:36:23.984762: step 14406, loss 0.594779.
Train: 2018-08-09T11:36:24.062865: step 14407, loss 0.610641.
Train: 2018-08-09T11:36:24.140942: step 14408, loss 0.53131.
Train: 2018-08-09T11:36:24.234695: step 14409, loss 0.562989.
Train: 2018-08-09T11:36:24.312806: step 14410, loss 0.499853.
Test: 2018-08-09T11:36:24.812659: step 14410, loss 0.550647.
Train: 2018-08-09T11:36:24.890795: step 14411, loss 0.531397.
Train: 2018-08-09T11:36:24.971267: step 14412, loss 0.610375.
Train: 2018-08-09T11:36:25.049374: step 14413, loss 0.578855.
Train: 2018-08-09T11:36:25.127449: step 14414, loss 0.610522.
Train: 2018-08-09T11:36:25.221177: step 14415, loss 0.61051.
Train: 2018-08-09T11:36:25.299316: step 14416, loss 0.594612.
Train: 2018-08-09T11:36:25.377423: step 14417, loss 0.5158.
Train: 2018-08-09T11:36:25.455497: step 14418, loss 0.563212.
Train: 2018-08-09T11:36:25.533638: step 14419, loss 0.673555.
Train: 2018-08-09T11:36:25.611740: step 14420, loss 0.641707.
Test: 2018-08-09T11:36:26.127215: step 14420, loss 0.549785.
Train: 2018-08-09T11:36:26.205351: step 14421, loss 0.625713.
Train: 2018-08-09T11:36:26.283457: step 14422, loss 0.594483.
Train: 2018-08-09T11:36:26.361566: step 14423, loss 0.454818.
Train: 2018-08-09T11:36:26.439640: step 14424, loss 0.578929.
Train: 2018-08-09T11:36:26.533369: step 14425, loss 0.563487.
Train: 2018-08-09T11:36:26.611476: step 14426, loss 0.501697.
Train: 2018-08-09T11:36:26.689581: step 14427, loss 0.501679.
Train: 2018-08-09T11:36:26.767718: step 14428, loss 0.547966.
Train: 2018-08-09T11:36:26.845826: step 14429, loss 0.578904.
Train: 2018-08-09T11:36:26.926217: step 14430, loss 0.625494.
Test: 2018-08-09T11:36:27.426130: step 14430, loss 0.549985.
Train: 2018-08-09T11:36:27.519856: step 14431, loss 0.578907.
Train: 2018-08-09T11:36:27.589517: step 14432, loss 0.454533.
Train: 2018-08-09T11:36:27.683280: step 14433, loss 0.516529.
Train: 2018-08-09T11:36:27.761383: step 14434, loss 0.516311.
Train: 2018-08-09T11:36:27.839461: step 14435, loss 0.453124.
Train: 2018-08-09T11:36:27.917595: step 14436, loss 0.657944.
Train: 2018-08-09T11:36:27.995703: step 14437, loss 0.594527.
Train: 2018-08-09T11:36:28.089431: step 14438, loss 0.530693.
Train: 2018-08-09T11:36:28.167509: step 14439, loss 0.625955.
Train: 2018-08-09T11:36:28.245644: step 14440, loss 0.626163.
Test: 2018-08-09T11:36:28.745497: step 14440, loss 0.547418.
Train: 2018-08-09T11:36:28.823602: step 14441, loss 0.610829.
Train: 2018-08-09T11:36:28.901709: step 14442, loss 0.52866.
Train: 2018-08-09T11:36:28.979848: step 14443, loss 0.579084.
Train: 2018-08-09T11:36:29.073574: step 14444, loss 0.627382.
Train: 2018-08-09T11:36:29.151680: step 14445, loss 0.644082.
Train: 2018-08-09T11:36:29.229787: step 14446, loss 0.512536.
Train: 2018-08-09T11:36:29.307864: step 14447, loss 0.630545.
Train: 2018-08-09T11:36:29.386000: step 14448, loss 0.654694.
Train: 2018-08-09T11:36:29.479730: step 14449, loss 0.545129.
Train: 2018-08-09T11:36:29.557838: step 14450, loss 0.560809.
Test: 2018-08-09T11:36:30.057688: step 14450, loss 0.547893.
Train: 2018-08-09T11:36:30.135819: step 14451, loss 0.546132.
Train: 2018-08-09T11:36:30.213930: step 14452, loss 0.531724.
Train: 2018-08-09T11:36:30.292009: step 14453, loss 0.675931.
Train: 2018-08-09T11:36:30.385735: step 14454, loss 0.562648.
Train: 2018-08-09T11:36:30.463842: step 14455, loss 0.625871.
Train: 2018-08-09T11:36:30.541979: step 14456, loss 0.531894.
Train: 2018-08-09T11:36:30.620085: step 14457, loss 0.485493.
Train: 2018-08-09T11:36:30.698191: step 14458, loss 0.5781.
Train: 2018-08-09T11:36:30.791920: step 14459, loss 0.57796.
Train: 2018-08-09T11:36:30.870025: step 14460, loss 0.625622.
Test: 2018-08-09T11:36:31.372249: step 14460, loss 0.54829.
Train: 2018-08-09T11:36:31.450361: step 14461, loss 0.563787.
Train: 2018-08-09T11:36:31.528467: step 14462, loss 0.501085.
Train: 2018-08-09T11:36:31.606544: step 14463, loss 0.54791.
Train: 2018-08-09T11:36:31.684651: step 14464, loss 0.640915.
Train: 2018-08-09T11:36:31.778378: step 14465, loss 0.533369.
Train: 2018-08-09T11:36:31.861873: step 14466, loss 0.562877.
Train: 2018-08-09T11:36:31.938455: step 14467, loss 0.547536.
Train: 2018-08-09T11:36:32.016563: step 14468, loss 0.563646.
Train: 2018-08-09T11:36:32.094668: step 14469, loss 0.719775.
Train: 2018-08-09T11:36:32.188397: step 14470, loss 0.547975.
Test: 2018-08-09T11:36:32.688249: step 14470, loss 0.54944.
Train: 2018-08-09T11:36:32.766356: step 14471, loss 0.609914.
Train: 2018-08-09T11:36:32.844487: step 14472, loss 0.62537.
Train: 2018-08-09T11:36:32.920266: step 14473, loss 0.548125.
Train: 2018-08-09T11:36:32.998403: step 14474, loss 0.471447.
Train: 2018-08-09T11:36:33.092131: step 14475, loss 0.547958.
Train: 2018-08-09T11:36:33.170237: step 14476, loss 0.578828.
Train: 2018-08-09T11:36:33.248344: step 14477, loss 0.517214.
Train: 2018-08-09T11:36:33.326455: step 14478, loss 0.57892.
Train: 2018-08-09T11:36:33.420178: step 14479, loss 0.424281.
Train: 2018-08-09T11:36:33.498285: step 14480, loss 0.594483.
Test: 2018-08-09T11:36:33.982520: step 14480, loss 0.550464.
Train: 2018-08-09T11:36:34.076245: step 14481, loss 0.687846.
Train: 2018-08-09T11:36:34.154380: step 14482, loss 0.625961.
Train: 2018-08-09T11:36:34.232482: step 14483, loss 0.469231.
Train: 2018-08-09T11:36:34.310594: step 14484, loss 0.563994.
Train: 2018-08-09T11:36:34.388700: step 14485, loss 0.563352.
Train: 2018-08-09T11:36:34.482428: step 14486, loss 0.578705.
Train: 2018-08-09T11:36:34.560514: step 14487, loss 0.563568.
Train: 2018-08-09T11:36:34.638642: step 14488, loss 0.626703.
Train: 2018-08-09T11:36:34.716719: step 14489, loss 0.625899.
Train: 2018-08-09T11:36:34.810445: step 14490, loss 0.531875.
Test: 2018-08-09T11:36:35.294706: step 14490, loss 0.550297.
Train: 2018-08-09T11:36:35.388465: step 14491, loss 0.609827.
Train: 2018-08-09T11:36:35.466572: step 14492, loss 0.594702.
Train: 2018-08-09T11:36:35.544677: step 14493, loss 0.625832.
Train: 2018-08-09T11:36:35.622784: step 14494, loss 0.594559.
Train: 2018-08-09T11:36:35.700861: step 14495, loss 0.547785.
Train: 2018-08-09T11:36:35.794589: step 14496, loss 0.641089.
Train: 2018-08-09T11:36:35.872726: step 14497, loss 0.485891.
Train: 2018-08-09T11:36:35.950801: step 14498, loss 0.702663.
Train: 2018-08-09T11:36:36.028909: step 14499, loss 0.579025.
Train: 2018-08-09T11:36:36.107045: step 14500, loss 0.609811.
Test: 2018-08-09T11:36:36.606898: step 14500, loss 0.549281.
Train: 2018-08-09T11:36:37.216153: step 14501, loss 0.563661.
Train: 2018-08-09T11:36:37.294265: step 14502, loss 0.594321.
Train: 2018-08-09T11:36:37.372372: step 14503, loss 0.502737.
Train: 2018-08-09T11:36:37.450481: step 14504, loss 0.54854.
Train: 2018-08-09T11:36:37.528554: step 14505, loss 0.579027.
Train: 2018-08-09T11:36:37.606692: step 14506, loss 0.563823.
Train: 2018-08-09T11:36:37.700389: step 14507, loss 0.609496.
Train: 2018-08-09T11:36:37.778497: step 14508, loss 0.624721.
Train: 2018-08-09T11:36:37.856633: step 14509, loss 0.655068.
Train: 2018-08-09T11:36:37.936219: step 14510, loss 0.579072.
Test: 2018-08-09T11:36:38.436076: step 14510, loss 0.554367.
Train: 2018-08-09T11:36:38.514177: step 14511, loss 0.518576.
Train: 2018-08-09T11:36:38.592315: step 14512, loss 0.548883.
Train: 2018-08-09T11:36:38.686011: step 14513, loss 0.488472.
Train: 2018-08-09T11:36:38.764148: step 14514, loss 0.548846.
Train: 2018-08-09T11:36:38.842254: step 14515, loss 0.503327.
Train: 2018-08-09T11:36:38.920362: step 14516, loss 0.639835.
Train: 2018-08-09T11:36:38.998469: step 14517, loss 0.57904.
Train: 2018-08-09T11:36:39.076575: step 14518, loss 0.548563.
Train: 2018-08-09T11:36:39.170304: step 14519, loss 0.670573.
Train: 2018-08-09T11:36:39.248379: step 14520, loss 0.594279.
Test: 2018-08-09T11:36:39.732641: step 14520, loss 0.552366.
Train: 2018-08-09T11:36:39.810746: step 14521, loss 0.640031.
Train: 2018-08-09T11:36:39.905789: step 14522, loss 0.624718.
Train: 2018-08-09T11:36:39.983316: step 14523, loss 0.624639.
Train: 2018-08-09T11:36:40.061453: step 14524, loss 0.48815.
Train: 2018-08-09T11:36:40.139530: step 14525, loss 0.594224.
Train: 2018-08-09T11:36:40.217638: step 14526, loss 0.579094.
Train: 2018-08-09T11:36:40.311393: step 14527, loss 0.594214.
Train: 2018-08-09T11:36:40.389472: step 14528, loss 0.579114.
Train: 2018-08-09T11:36:40.467578: step 14529, loss 0.609281.
Train: 2018-08-09T11:36:40.545684: step 14530, loss 0.549023.
Test: 2018-08-09T11:36:41.045567: step 14530, loss 0.55057.
Train: 2018-08-09T11:36:41.123671: step 14531, loss 0.549052.
Train: 2018-08-09T11:36:41.201809: step 14532, loss 0.594191.
Train: 2018-08-09T11:36:41.295538: step 14533, loss 0.609228.
Train: 2018-08-09T11:36:41.373613: step 14534, loss 0.47397.
Train: 2018-08-09T11:36:41.451750: step 14535, loss 0.564102.
Train: 2018-08-09T11:36:41.529857: step 14536, loss 0.518878.
Train: 2018-08-09T11:36:41.607963: step 14537, loss 0.609306.
Train: 2018-08-09T11:36:41.701691: step 14538, loss 0.488373.
Train: 2018-08-09T11:36:41.779798: step 14539, loss 0.54874.
Train: 2018-08-09T11:36:41.857904: step 14540, loss 0.51819.
Test: 2018-08-09T11:36:42.357758: step 14540, loss 0.552317.
Train: 2018-08-09T11:36:42.435865: step 14541, loss 0.456834.
Train: 2018-08-09T11:36:42.514000: step 14542, loss 0.532902.
Train: 2018-08-09T11:36:42.592109: step 14543, loss 0.578935.
Train: 2018-08-09T11:36:42.670216: step 14544, loss 0.594432.
Train: 2018-08-09T11:36:42.763910: step 14545, loss 0.563315.
Train: 2018-08-09T11:36:42.842049: step 14546, loss 0.516282.
Train: 2018-08-09T11:36:42.921724: step 14547, loss 0.515967.
Train: 2018-08-09T11:36:42.999834: step 14548, loss 0.563023.
Train: 2018-08-09T11:36:43.077939: step 14549, loss 0.467502.
Train: 2018-08-09T11:36:43.156045: step 14550, loss 0.498672.
Test: 2018-08-09T11:36:43.655897: step 14550, loss 0.546815.
Train: 2018-08-09T11:36:43.734003: step 14551, loss 0.546709.
Train: 2018-08-09T11:36:43.827731: step 14552, loss 0.529365.
Train: 2018-08-09T11:36:43.905868: step 14553, loss 0.513142.
Train: 2018-08-09T11:36:43.983974: step 14554, loss 0.460069.
Train: 2018-08-09T11:36:44.062081: step 14555, loss 0.636233.
Train: 2018-08-09T11:36:44.140190: step 14556, loss 0.599625.
Train: 2018-08-09T11:36:44.218295: step 14557, loss 0.616346.
Train: 2018-08-09T11:36:44.296401: step 14558, loss 0.527771.
Train: 2018-08-09T11:36:44.390130: step 14559, loss 0.459596.
Train: 2018-08-09T11:36:44.468206: step 14560, loss 0.562435.
Test: 2018-08-09T11:36:44.954813: step 14560, loss 0.550147.
Train: 2018-08-09T11:36:45.048541: step 14561, loss 0.544892.
Train: 2018-08-09T11:36:45.126679: step 14562, loss 0.51139.
Train: 2018-08-09T11:36:45.204754: step 14563, loss 0.682923.
Train: 2018-08-09T11:36:45.282862: step 14564, loss 0.664546.
Train: 2018-08-09T11:36:45.360997: step 14565, loss 0.426194.
Train: 2018-08-09T11:36:45.454726: step 14566, loss 0.613117.
Train: 2018-08-09T11:36:45.532832: step 14567, loss 0.647887.
Train: 2018-08-09T11:36:45.610938: step 14568, loss 0.495014.
Train: 2018-08-09T11:36:45.689014: step 14569, loss 0.545466.
Train: 2018-08-09T11:36:45.767151: step 14570, loss 0.680394.
Test: 2018-08-09T11:36:46.267004: step 14570, loss 0.550471.
Train: 2018-08-09T11:36:46.345111: step 14571, loss 0.528869.
Train: 2018-08-09T11:36:46.423247: step 14572, loss 0.629347.
Train: 2018-08-09T11:36:46.516947: step 14573, loss 0.529134.
Train: 2018-08-09T11:36:46.595081: step 14574, loss 0.612369.
Train: 2018-08-09T11:36:46.673159: step 14575, loss 0.529264.
Train: 2018-08-09T11:36:46.751295: step 14576, loss 0.579042.
Train: 2018-08-09T11:36:46.829401: step 14577, loss 0.529406.
Train: 2018-08-09T11:36:46.908783: step 14578, loss 0.67814.
Train: 2018-08-09T11:36:47.002542: step 14579, loss 0.644892.
Train: 2018-08-09T11:36:47.080649: step 14580, loss 0.546142.
Test: 2018-08-09T11:36:47.580501: step 14580, loss 0.548495.
Train: 2018-08-09T11:36:47.658607: step 14581, loss 0.546226.
Train: 2018-08-09T11:36:47.736715: step 14582, loss 0.578937.
Train: 2018-08-09T11:36:47.814851: step 14583, loss 0.627759.
Train: 2018-08-09T11:36:47.892941: step 14584, loss 0.66002.
Train: 2018-08-09T11:36:47.971064: step 14585, loss 0.675773.
Train: 2018-08-09T11:36:48.049140: step 14586, loss 0.514656.
Train: 2018-08-09T11:36:48.127247: step 14587, loss 0.594845.
Train: 2018-08-09T11:36:48.220975: step 14588, loss 0.531115.
Train: 2018-08-09T11:36:48.299082: step 14589, loss 0.578849.
Train: 2018-08-09T11:36:48.377218: step 14590, loss 0.531457.
Test: 2018-08-09T11:36:48.877070: step 14590, loss 0.54951.
Train: 2018-08-09T11:36:48.957598: step 14591, loss 0.578862.
Train: 2018-08-09T11:36:49.035704: step 14592, loss 0.563136.
Train: 2018-08-09T11:36:49.113842: step 14593, loss 0.531748.
Train: 2018-08-09T11:36:49.191948: step 14594, loss 0.531801.
Train: 2018-08-09T11:36:49.270024: step 14595, loss 0.578878.
Train: 2018-08-09T11:36:49.363785: step 14596, loss 0.610234.
Train: 2018-08-09T11:36:49.441889: step 14597, loss 0.51622.
Train: 2018-08-09T11:36:49.519966: step 14598, loss 0.625871.
Train: 2018-08-09T11:36:49.598074: step 14599, loss 0.672792.
Train: 2018-08-09T11:36:49.676179: step 14600, loss 0.672563.
Test: 2018-08-09T11:36:50.176061: step 14600, loss 0.549955.
Train: 2018-08-09T11:36:50.738459: step 14601, loss 0.563351.
Train: 2018-08-09T11:36:50.816566: step 14602, loss 0.501411.
Train: 2018-08-09T11:36:50.894667: step 14603, loss 0.594404.
Train: 2018-08-09T11:36:50.974440: step 14604, loss 0.532603.
Train: 2018-08-09T11:36:51.068168: step 14605, loss 0.594378.
Train: 2018-08-09T11:36:51.146275: step 14606, loss 0.62518.
Train: 2018-08-09T11:36:51.224382: step 14607, loss 0.563579.
Train: 2018-08-09T11:36:51.302459: step 14608, loss 0.548261.
Train: 2018-08-09T11:36:51.380595: step 14609, loss 0.578983.
Train: 2018-08-09T11:36:51.474307: step 14610, loss 0.594301.
Test: 2018-08-09T11:36:51.974175: step 14610, loss 0.550489.
Train: 2018-08-09T11:36:52.052281: step 14611, loss 0.517735.
Train: 2018-08-09T11:36:52.130388: step 14612, loss 0.441148.
Train: 2018-08-09T11:36:52.208496: step 14613, loss 0.578972.
Train: 2018-08-09T11:36:52.286600: step 14614, loss 0.579027.
Train: 2018-08-09T11:36:52.380329: step 14615, loss 0.56338.
Train: 2018-08-09T11:36:52.458437: step 14616, loss 0.547786.
Train: 2018-08-09T11:36:52.536543: step 14617, loss 0.438903.
Train: 2018-08-09T11:36:52.614679: step 14618, loss 0.594792.
Train: 2018-08-09T11:36:52.693447: step 14619, loss 0.531646.
Train: 2018-08-09T11:36:52.771555: step 14620, loss 0.530367.
Test: 2018-08-09T11:36:53.271406: step 14620, loss 0.547998.
Train: 2018-08-09T11:36:53.349540: step 14621, loss 0.576401.
Train: 2018-08-09T11:36:53.427647: step 14622, loss 0.496986.
Train: 2018-08-09T11:36:53.505753: step 14623, loss 0.508715.
Train: 2018-08-09T11:36:53.595370: step 14624, loss 0.580437.
Train: 2018-08-09T11:36:53.673447: step 14625, loss 0.624899.
Train: 2018-08-09T11:36:53.751584: step 14626, loss 0.490691.
Train: 2018-08-09T11:36:53.845280: step 14627, loss 0.590544.
Train: 2018-08-09T11:36:53.923417: step 14628, loss 0.494573.
Train: 2018-08-09T11:36:54.001524: step 14629, loss 0.542223.
Train: 2018-08-09T11:36:54.079600: step 14630, loss 0.674264.
Test: 2018-08-09T11:36:54.579483: step 14630, loss 0.548472.
Train: 2018-08-09T11:36:54.657589: step 14631, loss 0.452133.
Train: 2018-08-09T11:36:54.735726: step 14632, loss 0.538915.
Train: 2018-08-09T11:36:54.813838: step 14633, loss 0.634049.
Train: 2018-08-09T11:36:54.891939: step 14634, loss 0.455612.
Train: 2018-08-09T11:36:54.975776: step 14635, loss 0.522074.
Train: 2018-08-09T11:36:55.053912: step 14636, loss 0.464311.
Train: 2018-08-09T11:36:55.147636: step 14637, loss 0.721997.
Train: 2018-08-09T11:36:55.225748: step 14638, loss 0.44885.
Train: 2018-08-09T11:36:55.303854: step 14639, loss 0.596467.
Train: 2018-08-09T11:36:55.381960: step 14640, loss 0.601273.
Test: 2018-08-09T11:36:55.881838: step 14640, loss 0.546367.
Train: 2018-08-09T11:36:55.959919: step 14641, loss 0.504255.
Train: 2018-08-09T11:36:56.038056: step 14642, loss 0.538142.
Train: 2018-08-09T11:36:56.116163: step 14643, loss 0.543125.
Train: 2018-08-09T11:36:56.209893: step 14644, loss 0.522535.
Train: 2018-08-09T11:36:56.287997: step 14645, loss 0.601198.
Train: 2018-08-09T11:36:56.366073: step 14646, loss 0.462148.
Train: 2018-08-09T11:36:56.444213: step 14647, loss 0.600793.
Train: 2018-08-09T11:36:56.522322: step 14648, loss 0.572259.
Train: 2018-08-09T11:36:56.600423: step 14649, loss 0.56071.
Train: 2018-08-09T11:36:56.694154: step 14650, loss 0.541893.
Test: 2018-08-09T11:36:57.194034: step 14650, loss 0.550219.
Train: 2018-08-09T11:36:57.272140: step 14651, loss 0.490404.
Train: 2018-08-09T11:36:57.350247: step 14652, loss 0.647578.
Train: 2018-08-09T11:36:57.428353: step 14653, loss 0.612286.
Train: 2018-08-09T11:36:57.506454: step 14654, loss 0.585278.
Train: 2018-08-09T11:36:57.584567: step 14655, loss 0.53192.
Train: 2018-08-09T11:36:57.678298: step 14656, loss 0.574796.
Train: 2018-08-09T11:36:57.756372: step 14657, loss 0.530925.
Train: 2018-08-09T11:36:57.834508: step 14658, loss 0.509982.
Train: 2018-08-09T11:36:57.914084: step 14659, loss 0.529823.
Train: 2018-08-09T11:36:57.992191: step 14660, loss 0.512648.
Test: 2018-08-09T11:36:58.491797: step 14660, loss 0.547619.
Train: 2018-08-09T11:36:58.569903: step 14661, loss 0.618339.
Train: 2018-08-09T11:36:58.648010: step 14662, loss 0.495811.
Train: 2018-08-09T11:36:58.726118: step 14663, loss 0.498315.
Train: 2018-08-09T11:36:58.819875: step 14664, loss 0.511571.
Train: 2018-08-09T11:36:58.897981: step 14665, loss 0.575751.
Train: 2018-08-09T11:36:58.976089: step 14666, loss 0.530976.
Train: 2018-08-09T11:36:59.054195: step 14667, loss 0.502676.
Train: 2018-08-09T11:36:59.132302: step 14668, loss 0.530217.
Train: 2018-08-09T11:36:59.210409: step 14669, loss 0.646866.
Train: 2018-08-09T11:36:59.304129: step 14670, loss 0.495005.
Test: 2018-08-09T11:36:59.788367: step 14670, loss 0.547215.
Train: 2018-08-09T11:36:59.866503: step 14671, loss 0.53226.
Train: 2018-08-09T11:36:59.961655: step 14672, loss 0.62056.
Train: 2018-08-09T11:37:00.039793: step 14673, loss 0.527992.
Train: 2018-08-09T11:37:00.117897: step 14674, loss 0.495505.
Train: 2018-08-09T11:37:00.196007: step 14675, loss 0.531131.
Train: 2018-08-09T11:37:00.274113: step 14676, loss 0.464919.
Train: 2018-08-09T11:37:00.367810: step 14677, loss 0.512979.
Train: 2018-08-09T11:37:00.445945: step 14678, loss 0.561999.
Train: 2018-08-09T11:37:00.524053: step 14679, loss 0.613092.
Train: 2018-08-09T11:37:00.602162: step 14680, loss 0.512882.
Test: 2018-08-09T11:37:01.102012: step 14680, loss 0.549718.
Train: 2018-08-09T11:37:01.180148: step 14681, loss 0.527573.
Train: 2018-08-09T11:37:01.258255: step 14682, loss 0.593499.
Train: 2018-08-09T11:37:01.351986: step 14683, loss 0.542863.
Train: 2018-08-09T11:37:01.430089: step 14684, loss 0.597004.
Train: 2018-08-09T11:37:01.508195: step 14685, loss 0.563473.
Train: 2018-08-09T11:37:01.586300: step 14686, loss 0.561402.
Train: 2018-08-09T11:37:01.664408: step 14687, loss 0.475614.
Train: 2018-08-09T11:37:01.742485: step 14688, loss 0.525434.
Train: 2018-08-09T11:37:01.820592: step 14689, loss 0.596242.
Train: 2018-08-09T11:37:01.914340: step 14690, loss 0.578623.
Test: 2018-08-09T11:37:02.400953: step 14690, loss 0.544656.
Train: 2018-08-09T11:37:02.479059: step 14691, loss 0.578451.
Train: 2018-08-09T11:37:02.557166: step 14692, loss 0.490217.
Train: 2018-08-09T11:37:02.650895: step 14693, loss 0.581224.
Train: 2018-08-09T11:37:02.729030: step 14694, loss 0.472833.
Train: 2018-08-09T11:37:02.807137: step 14695, loss 0.569228.
Train: 2018-08-09T11:37:02.885242: step 14696, loss 0.50875.
Train: 2018-08-09T11:37:02.963353: step 14697, loss 0.54597.
Train: 2018-08-09T11:37:03.041457: step 14698, loss 0.668454.
Train: 2018-08-09T11:37:03.135186: step 14699, loss 0.562264.
Train: 2018-08-09T11:37:03.213291: step 14700, loss 0.495236.
Test: 2018-08-09T11:37:03.697522: step 14700, loss 0.547326.
Train: 2018-08-09T11:37:04.329680: step 14701, loss 0.612926.
Train: 2018-08-09T11:37:04.407787: step 14702, loss 0.56127.
Train: 2018-08-09T11:37:04.485889: step 14703, loss 0.559518.
Train: 2018-08-09T11:37:04.563997: step 14704, loss 0.529312.
Train: 2018-08-09T11:37:04.642107: step 14705, loss 0.421616.
Train: 2018-08-09T11:37:04.735806: step 14706, loss 0.617415.
Train: 2018-08-09T11:37:04.813911: step 14707, loss 0.664677.
Train: 2018-08-09T11:37:04.892048: step 14708, loss 0.562184.
Train: 2018-08-09T11:37:04.970126: step 14709, loss 0.49817.
Train: 2018-08-09T11:37:05.048232: step 14710, loss 0.564022.
Test: 2018-08-09T11:37:05.548114: step 14710, loss 0.548324.
Train: 2018-08-09T11:37:05.626250: step 14711, loss 0.511375.
Train: 2018-08-09T11:37:05.704359: step 14712, loss 0.562563.
Train: 2018-08-09T11:37:05.798087: step 14713, loss 0.598122.
Train: 2018-08-09T11:37:05.876191: step 14714, loss 0.543473.
Train: 2018-08-09T11:37:05.950888: step 14715, loss 0.545622.
Train: 2018-08-09T11:37:06.044616: step 14716, loss 0.595117.
Train: 2018-08-09T11:37:06.122721: step 14717, loss 0.694051.
Train: 2018-08-09T11:37:06.200834: step 14718, loss 0.595623.
Train: 2018-08-09T11:37:06.278936: step 14719, loss 0.51467.
Train: 2018-08-09T11:37:06.357042: step 14720, loss 0.494285.
Test: 2018-08-09T11:37:06.856895: step 14720, loss 0.548924.
Train: 2018-08-09T11:37:06.950624: step 14721, loss 0.481479.
Train: 2018-08-09T11:37:07.028759: step 14722, loss 0.562017.
Train: 2018-08-09T11:37:07.106866: step 14723, loss 0.577178.
Train: 2018-08-09T11:37:07.184974: step 14724, loss 0.647081.
Train: 2018-08-09T11:37:07.263078: step 14725, loss 0.562965.
Train: 2018-08-09T11:37:07.356807: step 14726, loss 0.662244.
Train: 2018-08-09T11:37:07.434912: step 14727, loss 0.597264.
Train: 2018-08-09T11:37:07.513019: step 14728, loss 0.579396.
Train: 2018-08-09T11:37:07.591127: step 14729, loss 0.578395.
Train: 2018-08-09T11:37:07.669235: step 14730, loss 0.499744.
Test: 2018-08-09T11:37:08.169085: step 14730, loss 0.551454.
Train: 2018-08-09T11:37:08.247221: step 14731, loss 0.498569.
Train: 2018-08-09T11:37:08.325328: step 14732, loss 0.546214.
Train: 2018-08-09T11:37:08.419028: step 14733, loss 0.594922.
Train: 2018-08-09T11:37:08.497162: step 14734, loss 0.563092.
Train: 2018-08-09T11:37:08.575269: step 14735, loss 0.595918.
Train: 2018-08-09T11:37:08.653345: step 14736, loss 0.53072.
Train: 2018-08-09T11:37:08.731485: step 14737, loss 0.530589.
Train: 2018-08-09T11:37:08.809559: step 14738, loss 0.579011.
Train: 2018-08-09T11:37:08.903318: step 14739, loss 0.611075.
Train: 2018-08-09T11:37:08.983802: step 14740, loss 0.531309.
Test: 2018-08-09T11:37:09.483657: step 14740, loss 0.552212.
Train: 2018-08-09T11:37:09.561789: step 14741, loss 0.62625.
Train: 2018-08-09T11:37:09.639867: step 14742, loss 0.499325.
Train: 2018-08-09T11:37:09.718003: step 14743, loss 0.56309.
Train: 2018-08-09T11:37:09.796110: step 14744, loss 0.530536.
Train: 2018-08-09T11:37:09.874185: step 14745, loss 0.6104.
Train: 2018-08-09T11:37:09.967915: step 14746, loss 0.562988.
Train: 2018-08-09T11:37:10.046021: step 14747, loss 0.467425.
Train: 2018-08-09T11:37:10.124158: step 14748, loss 0.562674.
Train: 2018-08-09T11:37:10.202234: step 14749, loss 0.477706.
Train: 2018-08-09T11:37:10.280370: step 14750, loss 0.627308.
Test: 2018-08-09T11:37:10.780254: step 14750, loss 0.548296.
Train: 2018-08-09T11:37:10.858328: step 14751, loss 0.546802.
Train: 2018-08-09T11:37:10.937153: step 14752, loss 0.546464.
Train: 2018-08-09T11:37:11.015256: step 14753, loss 0.596044.
Train: 2018-08-09T11:37:11.108989: step 14754, loss 0.562461.
Train: 2018-08-09T11:37:11.187094: step 14755, loss 0.513645.
Train: 2018-08-09T11:37:11.265197: step 14756, loss 0.514338.
Train: 2018-08-09T11:37:11.343307: step 14757, loss 0.595554.
Train: 2018-08-09T11:37:11.421385: step 14758, loss 0.627638.
Train: 2018-08-09T11:37:11.515159: step 14759, loss 0.546304.
Train: 2018-08-09T11:37:11.593248: step 14760, loss 0.578846.
Test: 2018-08-09T11:37:12.077480: step 14760, loss 0.54785.
Train: 2018-08-09T11:37:12.155585: step 14761, loss 0.562797.
Train: 2018-08-09T11:37:12.249343: step 14762, loss 0.644472.
Train: 2018-08-09T11:37:12.327422: step 14763, loss 0.562605.
Train: 2018-08-09T11:37:12.405556: step 14764, loss 0.693231.
Train: 2018-08-09T11:37:12.483661: step 14765, loss 0.692183.
Train: 2018-08-09T11:37:12.561772: step 14766, loss 0.530607.
Train: 2018-08-09T11:37:12.639876: step 14767, loss 0.626693.
Train: 2018-08-09T11:37:12.733607: step 14768, loss 0.546907.
Train: 2018-08-09T11:37:12.811712: step 14769, loss 0.563037.
Train: 2018-08-09T11:37:12.889787: step 14770, loss 0.499477.
Test: 2018-08-09T11:37:13.389670: step 14770, loss 0.548139.
Train: 2018-08-09T11:37:13.467775: step 14771, loss 0.578798.
Train: 2018-08-09T11:37:13.545883: step 14772, loss 0.610394.
Train: 2018-08-09T11:37:13.623989: step 14773, loss 0.578812.
Train: 2018-08-09T11:37:13.702126: step 14774, loss 0.578812.
Train: 2018-08-09T11:37:13.795854: step 14775, loss 0.594587.
Train: 2018-08-09T11:37:13.873962: step 14776, loss 0.578844.
Train: 2018-08-09T11:37:13.954334: step 14777, loss 0.594567.
Train: 2018-08-09T11:37:14.032471: step 14778, loss 0.563312.
Train: 2018-08-09T11:37:14.110548: step 14779, loss 0.563386.
Train: 2018-08-09T11:37:14.188686: step 14780, loss 0.656715.
Test: 2018-08-09T11:37:14.688537: step 14780, loss 0.551766.
Train: 2018-08-09T11:37:14.766675: step 14781, loss 0.516723.
Train: 2018-08-09T11:37:14.844781: step 14782, loss 0.516944.
Train: 2018-08-09T11:37:14.938509: step 14783, loss 0.563411.
Train: 2018-08-09T11:37:15.016615: step 14784, loss 0.501253.
Train: 2018-08-09T11:37:15.094692: step 14785, loss 0.516792.
Train: 2018-08-09T11:37:15.172798: step 14786, loss 0.56349.
Train: 2018-08-09T11:37:15.266526: step 14787, loss 0.61016.
Train: 2018-08-09T11:37:15.344657: step 14788, loss 0.54769.
Train: 2018-08-09T11:37:15.422769: step 14789, loss 0.578853.
Train: 2018-08-09T11:37:15.500870: step 14790, loss 0.563264.
Test: 2018-08-09T11:37:16.003000: step 14790, loss 0.550293.
Train: 2018-08-09T11:37:16.081136: step 14791, loss 0.51608.
Train: 2018-08-09T11:37:16.159243: step 14792, loss 0.579141.
Train: 2018-08-09T11:37:16.237351: step 14793, loss 0.516048.
Train: 2018-08-09T11:37:16.315452: step 14794, loss 0.562803.
Train: 2018-08-09T11:37:16.409166: step 14795, loss 0.610829.
Train: 2018-08-09T11:37:16.487291: step 14796, loss 0.483356.
Train: 2018-08-09T11:37:16.565392: step 14797, loss 0.515046.
Train: 2018-08-09T11:37:16.643473: step 14798, loss 0.628886.
Train: 2018-08-09T11:37:16.721580: step 14799, loss 0.579586.
Train: 2018-08-09T11:37:16.815308: step 14800, loss 0.498361.
Test: 2018-08-09T11:37:17.299570: step 14800, loss 0.548771.
Train: 2018-08-09T11:37:17.861969: step 14801, loss 0.594698.
Train: 2018-08-09T11:37:17.957993: step 14802, loss 0.448382.
Train: 2018-08-09T11:37:18.036100: step 14803, loss 0.597624.
Train: 2018-08-09T11:37:18.114210: step 14804, loss 0.510984.
Train: 2018-08-09T11:37:18.192283: step 14805, loss 0.577815.
Train: 2018-08-09T11:37:18.286042: step 14806, loss 0.59483.
Train: 2018-08-09T11:37:18.364117: step 14807, loss 0.564916.
Train: 2018-08-09T11:37:18.442223: step 14808, loss 0.717929.
Train: 2018-08-09T11:37:18.520358: step 14809, loss 0.495488.
Train: 2018-08-09T11:37:18.598438: step 14810, loss 0.594614.
Test: 2018-08-09T11:37:19.098344: step 14810, loss 0.54684.
Train: 2018-08-09T11:37:19.176427: step 14811, loss 0.545807.
Train: 2018-08-09T11:37:19.254563: step 14812, loss 0.611747.
Train: 2018-08-09T11:37:19.348261: step 14813, loss 0.528853.
Train: 2018-08-09T11:37:19.426366: step 14814, loss 0.629822.
Train: 2018-08-09T11:37:19.504473: step 14815, loss 0.527357.
Train: 2018-08-09T11:37:19.582582: step 14816, loss 0.573917.
Train: 2018-08-09T11:37:19.660717: step 14817, loss 0.717042.
Train: 2018-08-09T11:37:19.738827: step 14818, loss 0.562958.
Train: 2018-08-09T11:37:19.832553: step 14819, loss 0.526952.
Train: 2018-08-09T11:37:19.911999: step 14820, loss 0.596267.
Test: 2018-08-09T11:37:20.411882: step 14820, loss 0.547662.
Train: 2018-08-09T11:37:20.490018: step 14821, loss 0.544127.
Train: 2018-08-09T11:37:20.568094: step 14822, loss 0.528413.
Train: 2018-08-09T11:37:20.646201: step 14823, loss 0.592857.
Train: 2018-08-09T11:37:20.739929: step 14824, loss 0.595516.
Train: 2018-08-09T11:37:20.818066: step 14825, loss 0.529397.
Train: 2018-08-09T11:37:20.896173: step 14826, loss 0.52974.
Train: 2018-08-09T11:37:20.974282: step 14827, loss 0.480298.
Train: 2018-08-09T11:37:21.052387: step 14828, loss 0.516172.
Train: 2018-08-09T11:37:21.146118: step 14829, loss 0.592362.
Train: 2018-08-09T11:37:21.224222: step 14830, loss 0.566138.
Test: 2018-08-09T11:37:21.724073: step 14830, loss 0.549128.
Train: 2018-08-09T11:37:21.802180: step 14831, loss 0.516115.
Train: 2018-08-09T11:37:21.880316: step 14832, loss 0.57762.
Train: 2018-08-09T11:37:21.959797: step 14833, loss 0.513082.
Train: 2018-08-09T11:37:22.037901: step 14834, loss 0.631319.
Train: 2018-08-09T11:37:22.131599: step 14835, loss 0.563055.
Train: 2018-08-09T11:37:22.209738: step 14836, loss 0.550184.
Train: 2018-08-09T11:37:22.287841: step 14837, loss 0.628916.
Train: 2018-08-09T11:37:22.365918: step 14838, loss 0.482322.
Train: 2018-08-09T11:37:22.459679: step 14839, loss 0.58258.
Train: 2018-08-09T11:37:22.537753: step 14840, loss 0.614049.
Test: 2018-08-09T11:37:23.037634: step 14840, loss 0.550168.
Train: 2018-08-09T11:37:23.115741: step 14841, loss 0.578397.
Train: 2018-08-09T11:37:23.193848: step 14842, loss 0.5318.
Train: 2018-08-09T11:37:23.271956: step 14843, loss 0.48342.
Train: 2018-08-09T11:37:23.365687: step 14844, loss 0.562458.
Train: 2018-08-09T11:37:23.459414: step 14845, loss 0.513936.
Train: 2018-08-09T11:37:23.537518: step 14846, loss 0.609316.
Train: 2018-08-09T11:37:23.631245: step 14847, loss 0.593832.
Train: 2018-08-09T11:37:23.709356: step 14848, loss 0.497374.
Train: 2018-08-09T11:37:23.803080: step 14849, loss 0.49891.
Train: 2018-08-09T11:37:23.881214: step 14850, loss 0.561716.
Test: 2018-08-09T11:37:24.383365: step 14850, loss 0.547224.
Train: 2018-08-09T11:37:24.477094: step 14851, loss 0.597142.
Train: 2018-08-09T11:37:24.571333: step 14852, loss 0.511451.
Train: 2018-08-09T11:37:24.665061: step 14853, loss 0.550591.
Train: 2018-08-09T11:37:24.769161: step 14854, loss 0.582589.
Train: 2018-08-09T11:37:24.855715: step 14855, loss 0.61584.
Train: 2018-08-09T11:37:24.949442: step 14856, loss 0.475309.
Train: 2018-08-09T11:37:25.027548: step 14857, loss 0.542908.
Train: 2018-08-09T11:37:25.105655: step 14858, loss 0.598738.
Train: 2018-08-09T11:37:25.199385: step 14859, loss 0.529194.
Train: 2018-08-09T11:37:25.277508: step 14860, loss 0.629469.
Test: 2018-08-09T11:37:25.808616: step 14860, loss 0.549125.
Train: 2018-08-09T11:37:25.886746: step 14861, loss 0.561674.
Train: 2018-08-09T11:37:25.980476: step 14862, loss 0.460816.
Train: 2018-08-09T11:37:26.058555: step 14863, loss 0.497355.
Train: 2018-08-09T11:37:26.136694: step 14864, loss 0.612686.
Train: 2018-08-09T11:37:26.230391: step 14865, loss 0.64886.
Train: 2018-08-09T11:37:26.308527: step 14866, loss 0.582352.
Train: 2018-08-09T11:37:26.386604: step 14867, loss 0.528112.
Train: 2018-08-09T11:37:26.480332: step 14868, loss 0.578961.
Train: 2018-08-09T11:37:26.558464: step 14869, loss 0.514897.
Train: 2018-08-09T11:37:26.636575: step 14870, loss 0.465363.
Test: 2018-08-09T11:37:27.151504: step 14870, loss 0.548809.
Train: 2018-08-09T11:37:27.229611: step 14871, loss 0.514945.
Train: 2018-08-09T11:37:27.307689: step 14872, loss 0.627019.
Train: 2018-08-09T11:37:27.385821: step 14873, loss 0.513575.
Train: 2018-08-09T11:37:27.479523: step 14874, loss 0.545442.
Train: 2018-08-09T11:37:27.557630: step 14875, loss 0.581417.
Train: 2018-08-09T11:37:27.635762: step 14876, loss 0.413785.
Train: 2018-08-09T11:37:27.713844: step 14877, loss 0.5462.
Train: 2018-08-09T11:37:27.807600: step 14878, loss 0.579762.
Train: 2018-08-09T11:37:27.885711: step 14879, loss 0.663204.
Train: 2018-08-09T11:37:27.966059: step 14880, loss 0.513307.
Test: 2018-08-09T11:37:28.465946: step 14880, loss 0.550582.
Train: 2018-08-09T11:37:28.544050: step 14881, loss 0.513478.
Train: 2018-08-09T11:37:28.622189: step 14882, loss 0.545789.
Train: 2018-08-09T11:37:28.715884: step 14883, loss 0.56169.
Train: 2018-08-09T11:37:28.794020: step 14884, loss 0.54613.
Train: 2018-08-09T11:37:28.872097: step 14885, loss 0.510361.
Train: 2018-08-09T11:37:28.950235: step 14886, loss 0.511487.
Train: 2018-08-09T11:37:29.034032: step 14887, loss 0.494906.
Train: 2018-08-09T11:37:29.112138: step 14888, loss 0.597781.
Train: 2018-08-09T11:37:29.205838: step 14889, loss 0.545633.
Train: 2018-08-09T11:37:29.283973: step 14890, loss 0.619097.
Test: 2018-08-09T11:37:29.783827: step 14890, loss 0.54934.
Train: 2018-08-09T11:37:29.861962: step 14891, loss 0.513171.
Train: 2018-08-09T11:37:29.935829: step 14892, loss 0.578857.
Train: 2018-08-09T11:37:30.029527: step 14893, loss 0.494503.
Train: 2018-08-09T11:37:30.107663: step 14894, loss 0.510273.
Train: 2018-08-09T11:37:30.185770: step 14895, loss 0.528775.
Train: 2018-08-09T11:37:30.263881: step 14896, loss 0.544135.
Train: 2018-08-09T11:37:30.357605: step 14897, loss 0.683212.
Train: 2018-08-09T11:37:30.435680: step 14898, loss 0.68352.
Train: 2018-08-09T11:37:30.513787: step 14899, loss 0.425971.
Train: 2018-08-09T11:37:30.607515: step 14900, loss 0.579241.
Test: 2018-08-09T11:37:31.107426: step 14900, loss 0.549677.
Train: 2018-08-09T11:37:31.701038: step 14901, loss 0.596903.
Train: 2018-08-09T11:37:31.810356: step 14902, loss 0.613204.
Train: 2018-08-09T11:37:31.888464: step 14903, loss 0.495295.
Train: 2018-08-09T11:37:31.966596: step 14904, loss 0.612865.
Train: 2018-08-09T11:37:32.060298: step 14905, loss 0.628551.
Train: 2018-08-09T11:37:32.138434: step 14906, loss 0.579832.
Train: 2018-08-09T11:37:32.216542: step 14907, loss 0.562923.
Train: 2018-08-09T11:37:32.294647: step 14908, loss 0.545481.
Train: 2018-08-09T11:37:32.388376: step 14909, loss 0.612055.
Train: 2018-08-09T11:37:32.466483: step 14910, loss 0.596094.
Test: 2018-08-09T11:37:32.966336: step 14910, loss 0.548392.
Train: 2018-08-09T11:37:33.044471: step 14911, loss 0.612368.
Train: 2018-08-09T11:37:33.122548: step 14912, loss 0.464906.
Train: 2018-08-09T11:37:33.216301: step 14913, loss 0.562734.
Train: 2018-08-09T11:37:33.294412: step 14914, loss 0.595258.
Train: 2018-08-09T11:37:33.372488: step 14915, loss 0.48149.
Train: 2018-08-09T11:37:33.450632: step 14916, loss 0.595154.
Train: 2018-08-09T11:37:33.544350: step 14917, loss 0.578827.
Train: 2018-08-09T11:37:33.622429: step 14918, loss 0.514137.
Train: 2018-08-09T11:37:33.700537: step 14919, loss 0.481785.
Train: 2018-08-09T11:37:33.778673: step 14920, loss 0.546573.
Test: 2018-08-09T11:37:34.278551: step 14920, loss 0.551762.
Train: 2018-08-09T11:37:34.356662: step 14921, loss 0.578926.
Train: 2018-08-09T11:37:34.434768: step 14922, loss 0.595127.
Train: 2018-08-09T11:37:34.528485: step 14923, loss 0.546358.
Train: 2018-08-09T11:37:34.606603: step 14924, loss 0.578875.
Train: 2018-08-09T11:37:34.684679: step 14925, loss 0.546414.
Train: 2018-08-09T11:37:34.778463: step 14926, loss 0.595072.
Train: 2018-08-09T11:37:34.856515: step 14927, loss 0.627824.
Train: 2018-08-09T11:37:34.936922: step 14928, loss 0.595225.
Train: 2018-08-09T11:37:35.015036: step 14929, loss 0.70904.
Train: 2018-08-09T11:37:35.093164: step 14930, loss 0.611242.
Test: 2018-08-09T11:37:35.593048: step 14930, loss 0.546424.
Train: 2018-08-09T11:37:35.686776: step 14931, loss 0.514369.
Train: 2018-08-09T11:37:35.764882: step 14932, loss 0.546712.
Train: 2018-08-09T11:37:35.842984: step 14933, loss 0.594898.
Train: 2018-08-09T11:37:35.921095: step 14934, loss 0.610843.
Train: 2018-08-09T11:37:36.014793: step 14935, loss 0.674555.
Train: 2018-08-09T11:37:36.092925: step 14936, loss 0.578854.
Train: 2018-08-09T11:37:36.171032: step 14937, loss 0.563048.
Train: 2018-08-09T11:37:36.264337: step 14938, loss 0.531568.
Train: 2018-08-09T11:37:36.342413: step 14939, loss 0.594583.
Train: 2018-08-09T11:37:36.420517: step 14940, loss 0.610277.
Test: 2018-08-09T11:37:36.922047: step 14940, loss 0.549196.
Train: 2018-08-09T11:37:37.000179: step 14941, loss 0.531995.
Train: 2018-08-09T11:37:37.093907: step 14942, loss 0.578864.
Train: 2018-08-09T11:37:37.172017: step 14943, loss 0.563317.
Train: 2018-08-09T11:37:37.265720: step 14944, loss 0.57888.
Train: 2018-08-09T11:37:37.359447: step 14945, loss 0.532306.
Train: 2018-08-09T11:37:37.453174: step 14946, loss 0.578964.
Train: 2018-08-09T11:37:37.546901: step 14947, loss 0.594424.
Train: 2018-08-09T11:37:37.640630: step 14948, loss 0.640829.
Train: 2018-08-09T11:37:37.718736: step 14949, loss 0.60982.
Train: 2018-08-09T11:37:37.812464: step 14950, loss 0.656067.
Test: 2018-08-09T11:37:38.312346: step 14950, loss 0.548645.
Train: 2018-08-09T11:37:38.406113: step 14951, loss 0.548215.
Train: 2018-08-09T11:37:38.484180: step 14952, loss 0.640237.
Train: 2018-08-09T11:37:38.577924: step 14953, loss 0.579.
Train: 2018-08-09T11:37:38.656041: step 14954, loss 0.594251.
Train: 2018-08-09T11:37:38.749771: step 14955, loss 0.382039.
Train: 2018-08-09T11:37:38.827850: step 14956, loss 0.548725.
Train: 2018-08-09T11:37:38.937200: step 14957, loss 0.533494.
Train: 2018-08-09T11:37:39.030930: step 14958, loss 0.518223.
Train: 2018-08-09T11:37:39.109034: step 14959, loss 0.579043.
Train: 2018-08-09T11:37:39.187139: step 14960, loss 0.517863.
Test: 2018-08-09T11:37:39.910159: step 14960, loss 0.552175.
Train: 2018-08-09T11:37:40.003893: step 14961, loss 0.532998.
Train: 2018-08-09T11:37:40.113243: step 14962, loss 0.548162.
Train: 2018-08-09T11:37:40.222590: step 14963, loss 0.486171.
Train: 2018-08-09T11:37:40.331935: step 14964, loss 0.532284.
Train: 2018-08-09T11:37:40.456905: step 14965, loss 0.547646.
Train: 2018-08-09T11:37:40.566255: step 14966, loss 0.50014.
Train: 2018-08-09T11:37:40.659982: step 14967, loss 0.515404.
Train: 2018-08-09T11:37:40.753710: step 14968, loss 0.499665.
Train: 2018-08-09T11:37:40.831816: step 14969, loss 0.547477.
Train: 2018-08-09T11:37:40.925544: step 14970, loss 0.563361.
Test: 2018-08-09T11:37:41.409806: step 14970, loss 0.548897.
Train: 2018-08-09T11:37:41.487943: step 14971, loss 0.544916.
Train: 2018-08-09T11:37:41.581670: step 14972, loss 0.546316.
Train: 2018-08-09T11:37:41.659777: step 14973, loss 0.477636.
Train: 2018-08-09T11:37:41.737883: step 14974, loss 0.544645.
Train: 2018-08-09T11:37:41.831611: step 14975, loss 0.557809.
Train: 2018-08-09T11:37:41.920065: step 14976, loss 0.532482.
Train: 2018-08-09T11:37:41.998197: step 14977, loss 0.635542.
Train: 2018-08-09T11:37:42.076280: step 14978, loss 0.488361.
Train: 2018-08-09T11:37:42.154419: step 14979, loss 0.526515.
Train: 2018-08-09T11:37:42.248144: step 14980, loss 0.606398.
Test: 2018-08-09T11:37:42.747996: step 14980, loss 0.549816.
Train: 2018-08-09T11:37:42.826133: step 14981, loss 0.452698.
Train: 2018-08-09T11:37:42.904233: step 14982, loss 0.465982.
Train: 2018-08-09T11:37:42.997938: step 14983, loss 0.560743.
Train: 2018-08-09T11:37:43.076077: step 14984, loss 0.599354.
Train: 2018-08-09T11:37:43.154182: step 14985, loss 0.606926.
Train: 2018-08-09T11:37:43.232258: step 14986, loss 0.645473.
Train: 2018-08-09T11:37:43.325986: step 14987, loss 0.602008.
Train: 2018-08-09T11:37:43.404128: step 14988, loss 0.487406.
Train: 2018-08-09T11:37:43.482198: step 14989, loss 0.454243.
Train: 2018-08-09T11:37:43.560334: step 14990, loss 0.561858.
Test: 2018-08-09T11:37:44.060218: step 14990, loss 0.546501.
Train: 2018-08-09T11:37:44.153944: step 14991, loss 0.685448.
Train: 2018-08-09T11:37:44.232051: step 14992, loss 0.510395.
Train: 2018-08-09T11:37:44.325766: step 14993, loss 0.583761.
Train: 2018-08-09T11:37:44.403855: step 14994, loss 0.563594.
Train: 2018-08-09T11:37:44.481992: step 14995, loss 0.630696.
Train: 2018-08-09T11:37:44.560069: step 14996, loss 0.478791.
Train: 2018-08-09T11:37:44.638208: step 14997, loss 0.547464.
Train: 2018-08-09T11:37:44.731928: step 14998, loss 0.611001.
Train: 2018-08-09T11:37:44.810040: step 14999, loss 0.595267.
Train: 2018-08-09T11:37:44.888147: step 15000, loss 0.679413.
Test: 2018-08-09T11:37:45.390328: step 15000, loss 0.549458.
Train: 2018-08-09T11:37:45.937104: step 15001, loss 0.496412.
Train: 2018-08-09T11:37:46.030815: step 15002, loss 0.595139.
Train: 2018-08-09T11:37:46.108907: step 15003, loss 0.562767.
Train: 2018-08-09T11:37:46.187045: step 15004, loss 0.56234.
Train: 2018-08-09T11:37:46.265151: step 15005, loss 0.529983.
Train: 2018-08-09T11:37:46.358879: step 15006, loss 0.514012.
Train: 2018-08-09T11:37:46.436981: step 15007, loss 0.578658.
Train: 2018-08-09T11:37:46.515094: step 15008, loss 0.611211.
Train: 2018-08-09T11:37:46.593204: step 15009, loss 0.546413.
Train: 2018-08-09T11:37:46.671306: step 15010, loss 0.627329.
Test: 2018-08-09T11:37:47.171809: step 15010, loss 0.548262.
Train: 2018-08-09T11:37:47.265566: step 15011, loss 0.578846.
Train: 2018-08-09T11:37:47.343673: step 15012, loss 0.546654.
Train: 2018-08-09T11:37:47.421751: step 15013, loss 0.65918.
Train: 2018-08-09T11:37:47.499855: step 15014, loss 0.626873.
Train: 2018-08-09T11:37:47.577993: step 15015, loss 0.515049.
Train: 2018-08-09T11:37:47.671720: step 15016, loss 0.483289.
Train: 2018-08-09T11:37:47.749828: step 15017, loss 0.61076.
Train: 2018-08-09T11:37:47.827934: step 15018, loss 0.562986.
Train: 2018-08-09T11:37:47.921631: step 15019, loss 0.610603.
Train: 2018-08-09T11:37:47.999739: step 15020, loss 0.563058.
Test: 2018-08-09T11:37:48.499621: step 15020, loss 0.549409.
Train: 2018-08-09T11:37:48.577727: step 15021, loss 0.594677.
Train: 2018-08-09T11:37:48.655833: step 15022, loss 0.547231.
Train: 2018-08-09T11:37:48.749592: step 15023, loss 0.389485.
Train: 2018-08-09T11:37:48.827698: step 15024, loss 0.57878.
Train: 2018-08-09T11:37:48.905806: step 15025, loss 0.499697.
Train: 2018-08-09T11:37:48.983907: step 15026, loss 0.594728.
Train: 2018-08-09T11:37:49.061989: step 15027, loss 0.594727.
Train: 2018-08-09T11:37:49.155716: step 15028, loss 0.562812.
Train: 2018-08-09T11:37:49.233825: step 15029, loss 0.594965.
Train: 2018-08-09T11:37:49.311963: step 15030, loss 0.611057.
Test: 2018-08-09T11:37:49.811812: step 15030, loss 0.549015.
Train: 2018-08-09T11:37:49.889948: step 15031, loss 0.595062.
Train: 2018-08-09T11:37:49.970345: step 15032, loss 0.56218.
Train: 2018-08-09T11:37:50.048483: step 15033, loss 0.562928.
Train: 2018-08-09T11:37:50.142212: step 15034, loss 0.62697.
Train: 2018-08-09T11:37:50.220315: step 15035, loss 0.611562.
Train: 2018-08-09T11:37:50.298416: step 15036, loss 0.530885.
Train: 2018-08-09T11:37:50.376526: step 15037, loss 0.579216.
Train: 2018-08-09T11:37:50.470256: step 15038, loss 0.563103.
Train: 2018-08-09T11:37:50.548363: step 15039, loss 0.626508.
Train: 2018-08-09T11:37:50.626440: step 15040, loss 0.468283.
Test: 2018-08-09T11:37:51.126323: step 15040, loss 0.551239.
Train: 2018-08-09T11:37:51.204453: step 15041, loss 0.452508.
Train: 2018-08-09T11:37:51.282565: step 15042, loss 0.483959.
Train: 2018-08-09T11:37:51.376263: step 15043, loss 0.578969.
Train: 2018-08-09T11:37:51.454370: step 15044, loss 0.51523.
Train: 2018-08-09T11:37:51.532507: step 15045, loss 0.514936.
Train: 2018-08-09T11:37:51.610614: step 15046, loss 0.562805.
Train: 2018-08-09T11:37:51.688719: step 15047, loss 0.498283.
Train: 2018-08-09T11:37:51.782447: step 15048, loss 0.57893.
Train: 2018-08-09T11:37:51.860554: step 15049, loss 0.643905.
Train: 2018-08-09T11:37:51.940952: step 15050, loss 0.493404.
Test: 2018-08-09T11:37:52.440803: step 15050, loss 0.548525.
Train: 2018-08-09T11:37:52.518941: step 15051, loss 0.497077.
Train: 2018-08-09T11:37:52.597046: step 15052, loss 0.497278.
Train: 2018-08-09T11:37:52.690747: step 15053, loss 0.562491.
Train: 2018-08-09T11:37:52.768851: step 15054, loss 0.562264.
Train: 2018-08-09T11:37:52.846987: step 15055, loss 0.512933.
Train: 2018-08-09T11:37:52.925097: step 15056, loss 0.495202.
Train: 2018-08-09T11:37:53.003171: step 15057, loss 0.579273.
Train: 2018-08-09T11:37:53.081307: step 15058, loss 0.630646.
Train: 2018-08-09T11:37:53.175034: step 15059, loss 0.510003.
Train: 2018-08-09T11:37:53.253141: step 15060, loss 0.564646.
Test: 2018-08-09T11:37:53.752994: step 15060, loss 0.548182.
Train: 2018-08-09T11:37:53.831130: step 15061, loss 0.476944.
Train: 2018-08-09T11:37:53.910570: step 15062, loss 0.492774.
Train: 2018-08-09T11:37:54.004299: step 15063, loss 0.632349.
Train: 2018-08-09T11:37:54.082435: step 15064, loss 0.5981.
Train: 2018-08-09T11:37:54.160542: step 15065, loss 0.667193.
Train: 2018-08-09T11:37:54.238649: step 15066, loss 0.561336.
Train: 2018-08-09T11:37:54.332376: step 15067, loss 0.477551.
Train: 2018-08-09T11:37:54.410483: step 15068, loss 0.543883.
Train: 2018-08-09T11:37:54.488560: step 15069, loss 0.561308.
Train: 2018-08-09T11:37:54.566699: step 15070, loss 0.562702.
Test: 2018-08-09T11:37:55.077583: step 15070, loss 0.54891.
Train: 2018-08-09T11:37:55.155689: step 15071, loss 0.664424.
Train: 2018-08-09T11:37:55.233826: step 15072, loss 0.595859.
Train: 2018-08-09T11:37:55.311906: step 15073, loss 0.544933.
Train: 2018-08-09T11:37:55.405661: step 15074, loss 0.528024.
Train: 2018-08-09T11:37:55.483771: step 15075, loss 0.59678.
Train: 2018-08-09T11:37:55.561874: step 15076, loss 0.546124.
Train: 2018-08-09T11:37:55.639981: step 15077, loss 0.527684.
Train: 2018-08-09T11:37:55.733686: step 15078, loss 0.528798.
Train: 2018-08-09T11:37:55.811785: step 15079, loss 0.561858.
Train: 2018-08-09T11:37:55.889922: step 15080, loss 0.42754.
Test: 2018-08-09T11:37:56.395104: step 15080, loss 0.547157.
Train: 2018-08-09T11:37:56.473240: step 15081, loss 0.561623.
Train: 2018-08-09T11:37:56.551347: step 15082, loss 0.646687.
Train: 2018-08-09T11:37:56.629423: step 15083, loss 0.579358.
Train: 2018-08-09T11:37:56.723181: step 15084, loss 0.579402.
Train: 2018-08-09T11:37:56.801288: step 15085, loss 0.680087.
Train: 2018-08-09T11:37:56.879366: step 15086, loss 0.696605.
Train: 2018-08-09T11:37:56.967121: step 15087, loss 0.578772.
Train: 2018-08-09T11:37:57.045257: step 15088, loss 0.547827.
Train: 2018-08-09T11:37:57.123360: step 15089, loss 0.661888.
Train: 2018-08-09T11:37:57.201464: step 15090, loss 0.626704.
Test: 2018-08-09T11:37:57.716950: step 15090, loss 0.553914.
Train: 2018-08-09T11:37:57.795080: step 15091, loss 0.547067.
Train: 2018-08-09T11:37:57.873181: step 15092, loss 0.547733.
Train: 2018-08-09T11:37:57.951295: step 15093, loss 0.596556.
Train: 2018-08-09T11:37:58.029400: step 15094, loss 0.547897.
Train: 2018-08-09T11:37:58.123098: step 15095, loss 0.548007.
Train: 2018-08-09T11:37:58.201205: step 15096, loss 0.499033.
Train: 2018-08-09T11:37:58.279341: step 15097, loss 0.595308.
Train: 2018-08-09T11:37:58.357417: step 15098, loss 0.626403.
Train: 2018-08-09T11:37:58.451171: step 15099, loss 0.690134.
Train: 2018-08-09T11:37:58.529251: step 15100, loss 0.547168.
Test: 2018-08-09T11:37:59.029165: step 15100, loss 0.55006.
Train: 2018-08-09T11:37:59.592378: step 15101, loss 0.610413.
Train: 2018-08-09T11:37:59.670484: step 15102, loss 0.610342.
Train: 2018-08-09T11:37:59.757777: step 15103, loss 0.484804.
Train: 2018-08-09T11:37:59.824685: step 15104, loss 0.610182.
Train: 2018-08-09T11:37:59.918443: step 15105, loss 0.500864.
Train: 2018-08-09T11:37:59.996519: step 15106, loss 0.438532.
Train: 2018-08-09T11:38:00.074658: step 15107, loss 0.563218.
Train: 2018-08-09T11:38:00.152763: step 15108, loss 0.531869.
Train: 2018-08-09T11:38:00.230872: step 15109, loss 0.500358.
Train: 2018-08-09T11:38:00.324568: step 15110, loss 0.546721.
Test: 2018-08-09T11:38:00.824451: step 15110, loss 0.550954.
Train: 2018-08-09T11:38:00.902586: step 15111, loss 0.562896.
Train: 2018-08-09T11:38:00.983073: step 15112, loss 0.611146.
Train: 2018-08-09T11:38:01.061180: step 15113, loss 0.579249.
Train: 2018-08-09T11:38:01.139298: step 15114, loss 0.515447.
Train: 2018-08-09T11:38:01.217393: step 15115, loss 0.514812.
Train: 2018-08-09T11:38:01.311125: step 15116, loss 0.529692.
Train: 2018-08-09T11:38:01.389199: step 15117, loss 0.513769.
Train: 2018-08-09T11:38:01.467334: step 15118, loss 0.564581.
Train: 2018-08-09T11:38:01.545411: step 15119, loss 0.596101.
Train: 2018-08-09T11:38:01.639169: step 15120, loss 0.562951.
Test: 2018-08-09T11:38:02.139021: step 15120, loss 0.547531.
Train: 2018-08-09T11:38:02.217157: step 15121, loss 0.580827.
Train: 2018-08-09T11:38:02.295264: step 15122, loss 0.580337.
Train: 2018-08-09T11:38:02.373342: step 15123, loss 0.579744.
Train: 2018-08-09T11:38:02.451448: step 15124, loss 0.465397.
Train: 2018-08-09T11:38:02.545174: step 15125, loss 0.578817.
Train: 2018-08-09T11:38:02.623308: step 15126, loss 0.546487.
Train: 2018-08-09T11:38:02.701389: step 15127, loss 0.497767.
Train: 2018-08-09T11:38:02.779494: step 15128, loss 0.611554.
Train: 2018-08-09T11:38:02.857632: step 15129, loss 0.595291.
Train: 2018-08-09T11:38:02.952018: step 15130, loss 0.562523.
Test: 2018-08-09T11:38:03.436264: step 15130, loss 0.549777.
Train: 2018-08-09T11:38:03.529976: step 15131, loss 0.627908.
Train: 2018-08-09T11:38:03.608112: step 15132, loss 0.562674.
Train: 2018-08-09T11:38:03.686222: step 15133, loss 0.513689.
Train: 2018-08-09T11:38:03.779942: step 15134, loss 0.644303.
Train: 2018-08-09T11:38:03.858053: step 15135, loss 0.595195.
Train: 2018-08-09T11:38:03.936163: step 15136, loss 0.660261.
Train: 2018-08-09T11:38:04.014262: step 15137, loss 0.627586.
Train: 2018-08-09T11:38:04.092373: step 15138, loss 0.675895.
Train: 2018-08-09T11:38:04.186104: step 15139, loss 0.530598.
Train: 2018-08-09T11:38:04.264207: step 15140, loss 0.642991.
Test: 2018-08-09T11:38:04.764090: step 15140, loss 0.549156.
Train: 2018-08-09T11:38:04.842196: step 15141, loss 0.594818.
Train: 2018-08-09T11:38:04.920273: step 15142, loss 0.46766.
Train: 2018-08-09T11:38:04.998411: step 15143, loss 0.610552.
Train: 2018-08-09T11:38:05.076516: step 15144, loss 0.594662.
Train: 2018-08-09T11:38:05.154623: step 15145, loss 0.626123.
Train: 2018-08-09T11:38:05.248350: step 15146, loss 0.484713.
Train: 2018-08-09T11:38:05.326457: step 15147, loss 0.453562.
Train: 2018-08-09T11:38:05.404533: step 15148, loss 0.531869.
Train: 2018-08-09T11:38:05.482642: step 15149, loss 0.578894.
Train: 2018-08-09T11:38:05.560780: step 15150, loss 0.673074.
Test: 2018-08-09T11:38:06.060663: step 15150, loss 0.548503.
Train: 2018-08-09T11:38:06.154356: step 15151, loss 0.610251.
Train: 2018-08-09T11:38:06.232494: step 15152, loss 0.578876.
Train: 2018-08-09T11:38:06.310571: step 15153, loss 0.56326.
Train: 2018-08-09T11:38:06.388707: step 15154, loss 0.51647.
Train: 2018-08-09T11:38:06.466813: step 15155, loss 0.454071.
Train: 2018-08-09T11:38:06.560545: step 15156, loss 0.625777.
Train: 2018-08-09T11:38:06.638648: step 15157, loss 0.594524.
Train: 2018-08-09T11:38:06.716725: step 15158, loss 0.578882.
Train: 2018-08-09T11:38:06.794860: step 15159, loss 0.531936.
Train: 2018-08-09T11:38:06.888589: step 15160, loss 0.500573.
Test: 2018-08-09T11:38:07.388447: step 15160, loss 0.552634.
Train: 2018-08-09T11:38:07.466572: step 15161, loss 0.500408.
Train: 2018-08-09T11:38:07.544654: step 15162, loss 0.56311.
Train: 2018-08-09T11:38:07.638385: step 15163, loss 0.610484.
Train: 2018-08-09T11:38:07.716520: step 15164, loss 0.562963.
Train: 2018-08-09T11:38:07.794624: step 15165, loss 0.610616.
Train: 2018-08-09T11:38:07.872729: step 15166, loss 0.53123.
Train: 2018-08-09T11:38:07.966460: step 15167, loss 0.610873.
Train: 2018-08-09T11:38:08.044538: step 15168, loss 0.531161.
Train: 2018-08-09T11:38:08.122674: step 15169, loss 0.547046.
Train: 2018-08-09T11:38:08.200750: step 15170, loss 0.562897.
Test: 2018-08-09T11:38:08.696756: step 15170, loss 0.549169.
Train: 2018-08-09T11:38:08.790478: step 15171, loss 0.546941.
Train: 2018-08-09T11:38:08.868590: step 15172, loss 0.626742.
Train: 2018-08-09T11:38:08.946697: step 15173, loss 0.610843.
Train: 2018-08-09T11:38:09.024803: step 15174, loss 0.514874.
Train: 2018-08-09T11:38:09.102910: step 15175, loss 0.514785.
Train: 2018-08-09T11:38:09.181018: step 15176, loss 0.707237.
Train: 2018-08-09T11:38:09.274745: step 15177, loss 0.54696.
Train: 2018-08-09T11:38:09.352822: step 15178, loss 0.547086.
Train: 2018-08-09T11:38:09.430957: step 15179, loss 0.610936.
Train: 2018-08-09T11:38:09.509033: step 15180, loss 0.546971.
Test: 2018-08-09T11:38:10.014167: step 15180, loss 0.548587.
Train: 2018-08-09T11:38:10.092264: step 15181, loss 0.562921.
Train: 2018-08-09T11:38:10.170404: step 15182, loss 0.626639.
Train: 2018-08-09T11:38:10.248513: step 15183, loss 0.578875.
Train: 2018-08-09T11:38:10.342236: step 15184, loss 0.5471.
Train: 2018-08-09T11:38:10.420314: step 15185, loss 0.689932.
Train: 2018-08-09T11:38:10.498419: step 15186, loss 0.594682.
Train: 2018-08-09T11:38:10.576556: step 15187, loss 0.531542.
Train: 2018-08-09T11:38:10.654672: step 15188, loss 0.547377.
Train: 2018-08-09T11:38:10.748391: step 15189, loss 0.51597.
Train: 2018-08-09T11:38:10.826467: step 15190, loss 0.547431.
Test: 2018-08-09T11:38:11.326393: step 15190, loss 0.550203.
Train: 2018-08-09T11:38:11.404486: step 15191, loss 0.500267.
Train: 2018-08-09T11:38:11.482564: step 15192, loss 0.547388.
Train: 2018-08-09T11:38:11.560671: step 15193, loss 0.563101.
Train: 2018-08-09T11:38:11.654421: step 15194, loss 0.578862.
Train: 2018-08-09T11:38:11.732536: step 15195, loss 0.547254.
Train: 2018-08-09T11:38:11.810645: step 15196, loss 0.515557.
Train: 2018-08-09T11:38:11.888748: step 15197, loss 0.610579.
Train: 2018-08-09T11:38:11.966855: step 15198, loss 0.57886.
Train: 2018-08-09T11:38:12.060583: step 15199, loss 0.642438.
Train: 2018-08-09T11:38:12.138658: step 15200, loss 0.578858.
Test: 2018-08-09T11:38:12.638577: step 15200, loss 0.552898.
Train: 2018-08-09T11:38:13.218319: step 15201, loss 0.578863.
Train: 2018-08-09T11:38:13.296426: step 15202, loss 0.626474.
Train: 2018-08-09T11:38:13.374502: step 15203, loss 0.610557.
Train: 2018-08-09T11:38:13.468229: step 15204, loss 0.515583.
Train: 2018-08-09T11:38:13.546338: step 15205, loss 0.594664.
Train: 2018-08-09T11:38:13.624443: step 15206, loss 0.610433.
Train: 2018-08-09T11:38:13.702580: step 15207, loss 0.563086.
Train: 2018-08-09T11:38:13.780686: step 15208, loss 0.563135.
Train: 2018-08-09T11:38:13.858791: step 15209, loss 0.515948.
Train: 2018-08-09T11:38:13.952492: step 15210, loss 0.515958.
Test: 2018-08-09T11:38:14.452399: step 15210, loss 0.548351.
Train: 2018-08-09T11:38:14.530511: step 15211, loss 0.547308.
Train: 2018-08-09T11:38:14.608617: step 15212, loss 0.658082.
Train: 2018-08-09T11:38:14.686748: step 15213, loss 0.531425.
Train: 2018-08-09T11:38:14.764831: step 15214, loss 0.563119.
Train: 2018-08-09T11:38:14.858558: step 15215, loss 0.579056.
Train: 2018-08-09T11:38:14.939028: step 15216, loss 0.563118.
Train: 2018-08-09T11:38:15.017131: step 15217, loss 0.515674.
Train: 2018-08-09T11:38:15.095236: step 15218, loss 0.59463.
Train: 2018-08-09T11:38:15.188970: step 15219, loss 0.578728.
Train: 2018-08-09T11:38:15.267074: step 15220, loss 0.499898.
Test: 2018-08-09T11:38:15.766928: step 15220, loss 0.549337.
Train: 2018-08-09T11:38:15.845064: step 15221, loss 0.610632.
Train: 2018-08-09T11:38:15.923141: step 15222, loss 0.547031.
Train: 2018-08-09T11:38:16.001273: step 15223, loss 0.59489.
Train: 2018-08-09T11:38:16.094974: step 15224, loss 0.546806.
Train: 2018-08-09T11:38:16.173111: step 15225, loss 0.674678.
Train: 2018-08-09T11:38:16.251218: step 15226, loss 0.594828.
Train: 2018-08-09T11:38:16.329298: step 15227, loss 0.562932.
Train: 2018-08-09T11:38:16.407431: step 15228, loss 0.689818.
Train: 2018-08-09T11:38:16.485540: step 15229, loss 0.51584.
Train: 2018-08-09T11:38:16.579267: step 15230, loss 0.437467.
Test: 2018-08-09T11:38:17.065815: step 15230, loss 0.551398.
Train: 2018-08-09T11:38:17.159543: step 15231, loss 0.531719.
Train: 2018-08-09T11:38:17.237650: step 15232, loss 0.578862.
Train: 2018-08-09T11:38:17.315782: step 15233, loss 0.46862.
Train: 2018-08-09T11:38:17.393862: step 15234, loss 0.499907.
Train: 2018-08-09T11:38:17.471999: step 15235, loss 0.547146.
Train: 2018-08-09T11:38:17.565698: step 15236, loss 0.531177.
Train: 2018-08-09T11:38:17.643836: step 15237, loss 0.499037.
Train: 2018-08-09T11:38:17.721909: step 15238, loss 0.675182.
Train: 2018-08-09T11:38:17.800048: step 15239, loss 0.562958.
Train: 2018-08-09T11:38:17.878154: step 15240, loss 0.465986.
Test: 2018-08-09T11:38:18.378007: step 15240, loss 0.548769.
Train: 2018-08-09T11:38:18.471734: step 15241, loss 0.530103.
Train: 2018-08-09T11:38:18.549870: step 15242, loss 0.497732.
Train: 2018-08-09T11:38:18.627948: step 15243, loss 0.562614.
Train: 2018-08-09T11:38:18.706084: step 15244, loss 0.462791.
Train: 2018-08-09T11:38:18.784159: step 15245, loss 0.562431.
Train: 2018-08-09T11:38:18.877888: step 15246, loss 0.580774.
Train: 2018-08-09T11:38:18.958332: step 15247, loss 0.578724.
Train: 2018-08-09T11:38:19.036440: step 15248, loss 0.526522.
Train: 2018-08-09T11:38:19.114550: step 15249, loss 0.510234.
Train: 2018-08-09T11:38:19.192652: step 15250, loss 0.51133.
Test: 2018-08-09T11:38:19.692506: step 15250, loss 0.551895.
Train: 2018-08-09T11:38:19.770635: step 15251, loss 0.543827.
Train: 2018-08-09T11:38:19.848742: step 15252, loss 0.546968.
Train: 2018-08-09T11:38:19.926826: step 15253, loss 0.645079.
Train: 2018-08-09T11:38:20.020585: step 15254, loss 0.507412.
Train: 2018-08-09T11:38:20.098689: step 15255, loss 0.565685.
Train: 2018-08-09T11:38:20.176795: step 15256, loss 0.688449.
Train: 2018-08-09T11:38:20.254902: step 15257, loss 0.687087.
Train: 2018-08-09T11:38:20.333009: step 15258, loss 0.458925.
Train: 2018-08-09T11:38:20.411116: step 15259, loss 0.594489.
Train: 2018-08-09T11:38:20.504841: step 15260, loss 0.58028.
Test: 2018-08-09T11:38:21.007071: step 15260, loss 0.546351.
Train: 2018-08-09T11:38:21.085204: step 15261, loss 0.426901.
Train: 2018-08-09T11:38:21.163310: step 15262, loss 0.529333.
Train: 2018-08-09T11:38:21.241413: step 15263, loss 0.663944.
Train: 2018-08-09T11:38:21.319528: step 15264, loss 0.629938.
Train: 2018-08-09T11:38:21.413251: step 15265, loss 0.546631.
Train: 2018-08-09T11:38:21.491359: step 15266, loss 0.512427.
Train: 2018-08-09T11:38:21.569464: step 15267, loss 0.545572.
Train: 2018-08-09T11:38:21.647572: step 15268, loss 0.529448.
Train: 2018-08-09T11:38:21.725685: step 15269, loss 0.578993.
Train: 2018-08-09T11:38:21.819406: step 15270, loss 0.513081.
Test: 2018-08-09T11:38:22.303666: step 15270, loss 0.549554.
Train: 2018-08-09T11:38:22.381774: step 15271, loss 0.611985.
Train: 2018-08-09T11:38:22.475501: step 15272, loss 0.513091.
Train: 2018-08-09T11:38:22.553608: step 15273, loss 0.546038.
Train: 2018-08-09T11:38:22.631685: step 15274, loss 0.56251.
Train: 2018-08-09T11:38:22.709821: step 15275, loss 0.513058.
Train: 2018-08-09T11:38:22.787925: step 15276, loss 0.59552.
Train: 2018-08-09T11:38:22.866034: step 15277, loss 0.579007.
Train: 2018-08-09T11:38:22.945562: step 15278, loss 0.545999.
Train: 2018-08-09T11:38:23.039292: step 15279, loss 0.579003.
Train: 2018-08-09T11:38:23.117395: step 15280, loss 0.512922.
Test: 2018-08-09T11:38:23.617248: step 15280, loss 0.547626.
Train: 2018-08-09T11:38:23.695384: step 15281, loss 0.562499.
Train: 2018-08-09T11:38:23.773494: step 15282, loss 0.578983.
Train: 2018-08-09T11:38:23.851599: step 15283, loss 0.496091.
Train: 2018-08-09T11:38:23.929705: step 15284, loss 0.612084.
Train: 2018-08-09T11:38:24.023432: step 15285, loss 0.479516.
Train: 2018-08-09T11:38:24.101508: step 15286, loss 0.511839.
Train: 2018-08-09T11:38:24.179646: step 15287, loss 0.614129.
Train: 2018-08-09T11:38:24.257752: step 15288, loss 0.563281.
Train: 2018-08-09T11:38:24.335829: step 15289, loss 0.647106.
Train: 2018-08-09T11:38:24.413968: step 15290, loss 0.546088.
Test: 2018-08-09T11:38:24.916121: step 15290, loss 0.546785.
Train: 2018-08-09T11:38:24.994255: step 15291, loss 0.495868.
Train: 2018-08-09T11:38:25.072367: step 15292, loss 0.462459.
Train: 2018-08-09T11:38:25.166095: step 15293, loss 0.62909.
Train: 2018-08-09T11:38:25.244199: step 15294, loss 0.562476.
Train: 2018-08-09T11:38:25.322305: step 15295, loss 0.579081.
Train: 2018-08-09T11:38:25.400382: step 15296, loss 0.579342.
Train: 2018-08-09T11:38:25.478519: step 15297, loss 0.47903.
Train: 2018-08-09T11:38:25.556621: step 15298, loss 0.529077.
Train: 2018-08-09T11:38:25.650324: step 15299, loss 0.69611.
Train: 2018-08-09T11:38:25.728459: step 15300, loss 0.529091.
Test: 2018-08-09T11:38:26.230901: step 15300, loss 0.546821.
Train: 2018-08-09T11:38:26.777647: step 15301, loss 0.512422.
Train: 2018-08-09T11:38:26.871343: step 15302, loss 0.679072.
Train: 2018-08-09T11:38:26.949481: step 15303, loss 0.612409.
Train: 2018-08-09T11:38:27.027558: step 15304, loss 0.512672.
Train: 2018-08-09T11:38:27.105696: step 15305, loss 0.579085.
Train: 2018-08-09T11:38:27.199416: step 15306, loss 0.579033.
Train: 2018-08-09T11:38:27.277528: step 15307, loss 0.628657.
Train: 2018-08-09T11:38:27.355635: step 15308, loss 0.562515.
Train: 2018-08-09T11:38:27.433740: step 15309, loss 0.546094.
Train: 2018-08-09T11:38:27.511848: step 15310, loss 0.595385.
Test: 2018-08-09T11:38:28.011701: step 15310, loss 0.546617.
Train: 2018-08-09T11:38:28.089840: step 15311, loss 0.480707.
Train: 2018-08-09T11:38:28.183537: step 15312, loss 0.529857.
Train: 2018-08-09T11:38:28.261667: step 15313, loss 0.644374.
Train: 2018-08-09T11:38:28.339776: step 15314, loss 0.48093.
Train: 2018-08-09T11:38:28.417890: step 15315, loss 0.51359.
Train: 2018-08-09T11:38:28.511612: step 15316, loss 0.578937.
Train: 2018-08-09T11:38:28.589720: step 15317, loss 0.529867.
Train: 2018-08-09T11:38:28.667795: step 15318, loss 0.644442.
Train: 2018-08-09T11:38:28.745903: step 15319, loss 0.529852.
Train: 2018-08-09T11:38:28.824039: step 15320, loss 0.628059.
Test: 2018-08-09T11:38:29.321521: step 15320, loss 0.549749.
Train: 2018-08-09T11:38:29.399630: step 15321, loss 0.497189.
Train: 2018-08-09T11:38:29.493331: step 15322, loss 0.513542.
Train: 2018-08-09T11:38:29.571437: step 15323, loss 0.513466.
Train: 2018-08-09T11:38:29.649543: step 15324, loss 0.44783.
Train: 2018-08-09T11:38:29.727680: step 15325, loss 0.578932.
Train: 2018-08-09T11:38:29.805789: step 15326, loss 0.678145.
Train: 2018-08-09T11:38:29.899514: step 15327, loss 0.545939.
Train: 2018-08-09T11:38:29.977621: step 15328, loss 0.512844.
Train: 2018-08-09T11:38:30.055729: step 15329, loss 0.529464.
Train: 2018-08-09T11:38:30.133804: step 15330, loss 0.496019.
Test: 2018-08-09T11:38:30.629627: step 15330, loss 0.548097.
Train: 2018-08-09T11:38:30.707765: step 15331, loss 0.579078.
Train: 2018-08-09T11:38:30.801487: step 15332, loss 0.579245.
Train: 2018-08-09T11:38:30.879566: step 15333, loss 0.611436.
Train: 2018-08-09T11:38:30.951376: step 15334, loss 0.461097.
Train: 2018-08-09T11:38:31.029514: step 15335, loss 0.59711.
Train: 2018-08-09T11:38:31.123221: step 15336, loss 0.69914.
Train: 2018-08-09T11:38:31.201343: step 15337, loss 0.62991.
Train: 2018-08-09T11:38:31.279455: step 15338, loss 0.562422.
Train: 2018-08-09T11:38:31.357531: step 15339, loss 0.529178.
Train: 2018-08-09T11:38:31.436005: step 15340, loss 0.512153.
Test: 2018-08-09T11:38:31.935893: step 15340, loss 0.548761.
Train: 2018-08-09T11:38:32.029620: step 15341, loss 0.546073.
Train: 2018-08-09T11:38:32.107727: step 15342, loss 0.512501.
Train: 2018-08-09T11:38:32.185804: step 15343, loss 0.579078.
Train: 2018-08-09T11:38:32.263909: step 15344, loss 0.562666.
Train: 2018-08-09T11:38:32.342017: step 15345, loss 0.612404.
Train: 2018-08-09T11:38:32.435774: step 15346, loss 0.496304.
Train: 2018-08-09T11:38:32.513875: step 15347, loss 0.463226.
Train: 2018-08-09T11:38:32.591987: step 15348, loss 0.463034.
Train: 2018-08-09T11:38:32.670096: step 15349, loss 0.529284.
Train: 2018-08-09T11:38:32.748201: step 15350, loss 0.529088.
Test: 2018-08-09T11:38:33.248086: step 15350, loss 0.547995.
Train: 2018-08-09T11:38:33.326184: step 15351, loss 0.562347.
Train: 2018-08-09T11:38:33.404296: step 15352, loss 0.444791.
Train: 2018-08-09T11:38:33.497994: step 15353, loss 0.410327.
Train: 2018-08-09T11:38:33.576102: step 15354, loss 0.664969.
Train: 2018-08-09T11:38:33.654206: step 15355, loss 0.699091.
Train: 2018-08-09T11:38:33.732345: step 15356, loss 0.579092.
Train: 2018-08-09T11:38:33.810450: step 15357, loss 0.613868.
Train: 2018-08-09T11:38:33.904178: step 15358, loss 0.477017.
Train: 2018-08-09T11:38:33.982285: step 15359, loss 0.63057.
Train: 2018-08-09T11:38:34.060392: step 15360, loss 0.544934.
Test: 2018-08-09T11:38:34.560246: step 15360, loss 0.550866.
Train: 2018-08-09T11:38:34.638375: step 15361, loss 0.613686.
Train: 2018-08-09T11:38:34.716488: step 15362, loss 0.630876.
Train: 2018-08-09T11:38:34.794564: step 15363, loss 0.579608.
Train: 2018-08-09T11:38:34.872700: step 15364, loss 0.595847.
Train: 2018-08-09T11:38:34.966428: step 15365, loss 0.562222.
Train: 2018-08-09T11:38:35.044534: step 15366, loss 0.612658.
Train: 2018-08-09T11:38:35.122612: step 15367, loss 0.562421.
Train: 2018-08-09T11:38:35.200748: step 15368, loss 0.495575.
Train: 2018-08-09T11:38:35.294475: step 15369, loss 0.562353.
Train: 2018-08-09T11:38:35.372585: step 15370, loss 0.51245.
Test: 2018-08-09T11:38:35.872454: step 15370, loss 0.548725.
Train: 2018-08-09T11:38:35.951941: step 15371, loss 0.529182.
Train: 2018-08-09T11:38:36.030021: step 15372, loss 0.629033.
Train: 2018-08-09T11:38:36.108156: step 15373, loss 0.529293.
Train: 2018-08-09T11:38:36.201882: step 15374, loss 0.462707.
Train: 2018-08-09T11:38:36.279988: step 15375, loss 0.512718.
Train: 2018-08-09T11:38:36.358097: step 15376, loss 0.579205.
Train: 2018-08-09T11:38:36.436197: step 15377, loss 0.4958.
Train: 2018-08-09T11:38:36.514278: step 15378, loss 0.545985.
Train: 2018-08-09T11:38:36.608038: step 15379, loss 0.59598.
Train: 2018-08-09T11:38:36.682059: step 15380, loss 0.629754.
Test: 2018-08-09T11:38:37.181913: step 15380, loss 0.54801.
Train: 2018-08-09T11:38:37.260018: step 15381, loss 0.612672.
Train: 2018-08-09T11:38:37.338124: step 15382, loss 0.579087.
Train: 2018-08-09T11:38:37.416232: step 15383, loss 0.595748.
Train: 2018-08-09T11:38:37.509960: step 15384, loss 0.579073.
Train: 2018-08-09T11:38:37.588100: step 15385, loss 0.47946.
Train: 2018-08-09T11:38:37.666200: step 15386, loss 0.579053.
Train: 2018-08-09T11:38:37.744311: step 15387, loss 0.512724.
Train: 2018-08-09T11:38:37.838046: step 15388, loss 0.562478.
Train: 2018-08-09T11:38:37.920393: step 15389, loss 0.512719.
Train: 2018-08-09T11:38:37.985181: step 15390, loss 0.612234.
Test: 2018-08-09T11:38:38.485059: step 15390, loss 0.54629.
Train: 2018-08-09T11:38:38.563171: step 15391, loss 0.595652.
Train: 2018-08-09T11:38:38.656869: step 15392, loss 0.562481.
Train: 2018-08-09T11:38:38.735006: step 15393, loss 0.711553.
Train: 2018-08-09T11:38:38.813112: step 15394, loss 0.661588.
Train: 2018-08-09T11:38:38.891188: step 15395, loss 0.562535.
Train: 2018-08-09T11:38:38.969323: step 15396, loss 0.513423.
Train: 2018-08-09T11:38:39.063053: step 15397, loss 0.611626.
Train: 2018-08-09T11:38:39.141160: step 15398, loss 0.562629.
Train: 2018-08-09T11:38:39.219267: step 15399, loss 0.64389.
Train: 2018-08-09T11:38:39.297373: step 15400, loss 0.643629.
Test: 2018-08-09T11:38:39.797226: step 15400, loss 0.549502.
Train: 2018-08-09T11:38:40.328379: step 15401, loss 0.578876.
Train: 2018-08-09T11:38:40.406482: step 15402, loss 0.562829.
Train: 2018-08-09T11:38:40.484589: step 15403, loss 0.514961.
Train: 2018-08-09T11:38:40.562699: step 15404, loss 0.610726.
Train: 2018-08-09T11:38:40.656428: step 15405, loss 0.547092.
Train: 2018-08-09T11:38:40.736298: step 15406, loss 0.578857.
Train: 2018-08-09T11:38:40.814436: step 15407, loss 0.689512.
Train: 2018-08-09T11:38:40.892545: step 15408, loss 0.594612.
Train: 2018-08-09T11:38:40.970645: step 15409, loss 0.672942.
Train: 2018-08-09T11:38:41.064376: step 15410, loss 0.688049.
Test: 2018-08-09T11:38:41.552173: step 15410, loss 0.551264.
Train: 2018-08-09T11:38:41.630273: step 15411, loss 0.594412.
Train: 2018-08-09T11:38:41.708356: step 15412, loss 0.609738.
Train: 2018-08-09T11:38:41.802114: step 15413, loss 0.502549.
Train: 2018-08-09T11:38:41.880221: step 15414, loss 0.670368.
Train: 2018-08-09T11:38:41.958329: step 15415, loss 0.518543.
Train: 2018-08-09T11:38:42.036439: step 15416, loss 0.548975.
Train: 2018-08-09T11:38:42.114509: step 15417, loss 0.504018.
Train: 2018-08-09T11:38:42.208237: step 15418, loss 0.54914.
Train: 2018-08-09T11:38:42.286344: step 15419, loss 0.489136.
Train: 2018-08-09T11:38:42.364482: step 15420, loss 0.563937.
Test: 2018-08-09T11:38:42.864358: step 15420, loss 0.549823.
Train: 2018-08-09T11:38:42.942471: step 15421, loss 0.579179.
Train: 2018-08-09T11:38:43.020580: step 15422, loss 0.48838.
Train: 2018-08-09T11:38:43.114308: step 15423, loss 0.502354.
Train: 2018-08-09T11:38:43.192411: step 15424, loss 0.5012.
Train: 2018-08-09T11:38:43.270518: step 15425, loss 0.625575.
Train: 2018-08-09T11:38:43.348625: step 15426, loss 0.464274.
Train: 2018-08-09T11:38:43.442323: step 15427, loss 0.53042.
Train: 2018-08-09T11:38:43.520461: step 15428, loss 0.569057.
Train: 2018-08-09T11:38:43.598566: step 15429, loss 0.543496.
Train: 2018-08-09T11:38:43.676671: step 15430, loss 0.667809.
Test: 2018-08-09T11:38:44.176568: step 15430, loss 0.544982.
Train: 2018-08-09T11:38:44.254660: step 15431, loss 0.588709.
Train: 2018-08-09T11:38:44.332767: step 15432, loss 0.602018.
Train: 2018-08-09T11:38:44.410879: step 15433, loss 0.54961.
Train: 2018-08-09T11:38:44.504606: step 15434, loss 0.611021.
Train: 2018-08-09T11:38:44.582709: step 15435, loss 0.592471.
Train: 2018-08-09T11:38:44.660818: step 15436, loss 0.598239.
Train: 2018-08-09T11:38:44.738891: step 15437, loss 0.594613.
Train: 2018-08-09T11:38:44.817028: step 15438, loss 0.482452.
Train: 2018-08-09T11:38:44.895134: step 15439, loss 0.562629.
Train: 2018-08-09T11:38:44.975727: step 15440, loss 0.547042.
Test: 2018-08-09T11:38:45.506856: step 15440, loss 0.548897.
Train: 2018-08-09T11:38:45.584928: step 15441, loss 0.563106.
Train: 2018-08-09T11:38:45.663034: step 15442, loss 0.470034.
Train: 2018-08-09T11:38:45.741142: step 15443, loss 0.562813.
Train: 2018-08-09T11:38:45.819278: step 15444, loss 0.579506.
Train: 2018-08-09T11:38:45.913006: step 15445, loss 0.577796.
Train: 2018-08-09T11:38:45.991084: step 15446, loss 0.546937.
Train: 2018-08-09T11:38:46.069219: step 15447, loss 0.628242.
Train: 2018-08-09T11:38:46.147326: step 15448, loss 0.689168.
Train: 2018-08-09T11:38:46.225435: step 15449, loss 0.563896.
Train: 2018-08-09T11:38:46.303539: step 15450, loss 0.641233.
Test: 2018-08-09T11:38:46.803392: step 15450, loss 0.548775.
Train: 2018-08-09T11:38:46.881497: step 15451, loss 0.563348.
Train: 2018-08-09T11:38:46.976659: step 15452, loss 0.516861.
Train: 2018-08-09T11:38:47.054767: step 15453, loss 0.640532.
Train: 2018-08-09T11:38:47.132872: step 15454, loss 0.532634.
Train: 2018-08-09T11:38:47.210978: step 15455, loss 0.56359.
Train: 2018-08-09T11:38:47.289086: step 15456, loss 0.625083.
Train: 2018-08-09T11:38:47.382842: step 15457, loss 0.548192.
Train: 2018-08-09T11:38:47.460949: step 15458, loss 0.563598.
Train: 2018-08-09T11:38:47.539026: step 15459, loss 0.594346.
Train: 2018-08-09T11:38:47.617163: step 15460, loss 0.563797.
Test: 2018-08-09T11:38:48.117040: step 15460, loss 0.548692.
Train: 2018-08-09T11:38:48.195152: step 15461, loss 0.578903.
Train: 2018-08-09T11:38:48.288850: step 15462, loss 0.548251.
Train: 2018-08-09T11:38:48.366957: step 15463, loss 0.471576.
Train: 2018-08-09T11:38:48.445062: step 15464, loss 0.501593.
Train: 2018-08-09T11:38:48.538820: step 15465, loss 0.579361.
Train: 2018-08-09T11:38:48.616929: step 15466, loss 0.625962.
Train: 2018-08-09T11:38:48.695003: step 15467, loss 0.596678.
Train: 2018-08-09T11:38:48.773143: step 15468, loss 0.548135.
Train: 2018-08-09T11:38:48.851217: step 15469, loss 0.563525.
Train: 2018-08-09T11:38:48.931752: step 15470, loss 0.579014.
Test: 2018-08-09T11:38:49.431605: step 15470, loss 0.549352.
Train: 2018-08-09T11:38:49.509742: step 15471, loss 0.594388.
Train: 2018-08-09T11:38:49.603440: step 15472, loss 0.609992.
Train: 2018-08-09T11:38:49.681576: step 15473, loss 0.485628.
Train: 2018-08-09T11:38:49.759682: step 15474, loss 0.672342.
Train: 2018-08-09T11:38:49.837789: step 15475, loss 0.625396.
Train: 2018-08-09T11:38:49.915896: step 15476, loss 0.485949.
Train: 2018-08-09T11:38:50.009593: step 15477, loss 0.579152.
Train: 2018-08-09T11:38:50.087734: step 15478, loss 0.60989.
Train: 2018-08-09T11:38:50.165807: step 15479, loss 0.609989.
Train: 2018-08-09T11:38:50.243942: step 15480, loss 0.470378.
Test: 2018-08-09T11:38:50.759422: step 15480, loss 0.548876.
Train: 2018-08-09T11:38:50.837522: step 15481, loss 0.594426.
Train: 2018-08-09T11:38:50.917101: step 15482, loss 0.594468.
Train: 2018-08-09T11:38:50.995209: step 15483, loss 0.516795.
Train: 2018-08-09T11:38:51.073345: step 15484, loss 0.594515.
Train: 2018-08-09T11:38:51.151451: step 15485, loss 0.594372.
Train: 2018-08-09T11:38:51.245181: step 15486, loss 0.53237.
Train: 2018-08-09T11:38:51.323256: step 15487, loss 0.625581.
Train: 2018-08-09T11:38:51.401394: step 15488, loss 0.609957.
Train: 2018-08-09T11:38:51.479502: step 15489, loss 0.563359.
Train: 2018-08-09T11:38:51.557576: step 15490, loss 0.547943.
Test: 2018-08-09T11:38:52.057460: step 15490, loss 0.549426.
Train: 2018-08-09T11:38:52.135565: step 15491, loss 0.547855.
Train: 2018-08-09T11:38:52.213672: step 15492, loss 0.516761.
Train: 2018-08-09T11:38:52.307433: step 15493, loss 0.547814.
Train: 2018-08-09T11:38:52.385506: step 15494, loss 0.547735.
Train: 2018-08-09T11:38:52.463642: step 15495, loss 0.563184.
Train: 2018-08-09T11:38:52.541749: step 15496, loss 0.563175.
Train: 2018-08-09T11:38:52.635477: step 15497, loss 0.547543.
Train: 2018-08-09T11:38:52.713581: step 15498, loss 0.704595.
Train: 2018-08-09T11:38:52.791690: step 15499, loss 0.562816.
Train: 2018-08-09T11:38:52.869797: step 15500, loss 0.500414.
Test: 2018-08-09T11:38:53.371927: step 15500, loss 0.551312.
Train: 2018-08-09T11:38:53.918679: step 15501, loss 0.610079.
Train: 2018-08-09T11:38:53.996789: step 15502, loss 0.579174.
Train: 2018-08-09T11:38:54.074894: step 15503, loss 0.547218.
Train: 2018-08-09T11:38:54.152970: step 15504, loss 0.499524.
Train: 2018-08-09T11:38:54.246696: step 15505, loss 0.499408.
Train: 2018-08-09T11:38:54.324834: step 15506, loss 0.546328.
Train: 2018-08-09T11:38:54.402940: step 15507, loss 0.578325.
Train: 2018-08-09T11:38:54.481017: step 15508, loss 0.530404.
Train: 2018-08-09T11:38:54.559155: step 15509, loss 0.465711.
Train: 2018-08-09T11:38:54.652882: step 15510, loss 0.531246.
Test: 2018-08-09T11:38:55.139532: step 15510, loss 0.547194.
Train: 2018-08-09T11:38:55.217613: step 15511, loss 0.491062.
Train: 2018-08-09T11:38:55.311371: step 15512, loss 0.617786.
Train: 2018-08-09T11:38:55.389479: step 15513, loss 0.476088.
Train: 2018-08-09T11:38:55.467554: step 15514, loss 0.664269.
Train: 2018-08-09T11:38:55.545660: step 15515, loss 0.630124.
Train: 2018-08-09T11:38:55.623799: step 15516, loss 0.54323.
Train: 2018-08-09T11:38:55.717526: step 15517, loss 0.488894.
Train: 2018-08-09T11:38:55.795633: step 15518, loss 0.456446.
Train: 2018-08-09T11:38:55.873708: step 15519, loss 0.546214.
Train: 2018-08-09T11:38:55.951846: step 15520, loss 0.572779.
Test: 2018-08-09T11:38:56.451698: step 15520, loss 0.544277.
Train: 2018-08-09T11:38:56.529836: step 15521, loss 0.538699.
Train: 2018-08-09T11:38:56.607944: step 15522, loss 0.565316.
Train: 2018-08-09T11:38:56.701670: step 15523, loss 0.56126.
Train: 2018-08-09T11:38:56.779771: step 15524, loss 0.666549.
Train: 2018-08-09T11:38:56.857885: step 15525, loss 0.526226.
Train: 2018-08-09T11:38:56.936618: step 15526, loss 0.523914.
Train: 2018-08-09T11:38:57.014693: step 15527, loss 0.634789.
Train: 2018-08-09T11:38:57.108451: step 15528, loss 0.541244.
Train: 2018-08-09T11:38:57.186553: step 15529, loss 0.673377.
Train: 2018-08-09T11:38:57.264632: step 15530, loss 0.495016.
Test: 2018-08-09T11:38:57.764515: step 15530, loss 0.547851.
Train: 2018-08-09T11:38:57.842652: step 15531, loss 0.491935.
Train: 2018-08-09T11:38:57.920758: step 15532, loss 0.61643.
Train: 2018-08-09T11:38:57.998866: step 15533, loss 0.678028.
Train: 2018-08-09T11:38:58.092562: step 15534, loss 0.514374.
Train: 2018-08-09T11:38:58.170669: step 15535, loss 0.546235.
Train: 2018-08-09T11:38:58.248806: step 15536, loss 0.40125.
Train: 2018-08-09T11:38:58.326883: step 15537, loss 0.530015.
Train: 2018-08-09T11:38:58.405020: step 15538, loss 0.483152.
Train: 2018-08-09T11:38:58.498720: step 15539, loss 0.481412.
Train: 2018-08-09T11:38:58.576854: step 15540, loss 0.644203.
Test: 2018-08-09T11:38:59.076706: step 15540, loss 0.551636.
Train: 2018-08-09T11:38:59.154843: step 15541, loss 0.577818.
Train: 2018-08-09T11:38:59.232918: step 15542, loss 0.528817.
Train: 2018-08-09T11:38:59.311058: step 15543, loss 0.512271.
Train: 2018-08-09T11:38:59.404778: step 15544, loss 0.560617.
Train: 2018-08-09T11:38:59.482861: step 15545, loss 0.479538.
Train: 2018-08-09T11:38:59.560998: step 15546, loss 0.535303.
Train: 2018-08-09T11:38:59.639104: step 15547, loss 0.662765.
Train: 2018-08-09T11:38:59.717181: step 15548, loss 0.493248.
Train: 2018-08-09T11:38:59.810910: step 15549, loss 0.630779.
Train: 2018-08-09T11:38:59.889014: step 15550, loss 0.615341.
Test: 2018-08-09T11:39:00.375626: step 15550, loss 0.547969.
Train: 2018-08-09T11:39:00.453738: step 15551, loss 0.561773.
Train: 2018-08-09T11:39:00.547466: step 15552, loss 0.51287.
Train: 2018-08-09T11:39:00.625542: step 15553, loss 0.478845.
Train: 2018-08-09T11:39:00.703679: step 15554, loss 0.509995.
Train: 2018-08-09T11:39:00.781784: step 15555, loss 0.631251.
Train: 2018-08-09T11:39:00.875509: step 15556, loss 0.528098.
Train: 2018-08-09T11:39:00.953621: step 15557, loss 0.496071.
Train: 2018-08-09T11:39:01.031721: step 15558, loss 0.514198.
Train: 2018-08-09T11:39:01.109803: step 15559, loss 0.496768.
Train: 2018-08-09T11:39:01.203562: step 15560, loss 0.580062.
Test: 2018-08-09T11:39:01.699269: step 15560, loss 0.547447.
Train: 2018-08-09T11:39:01.777399: step 15561, loss 0.581298.
Train: 2018-08-09T11:39:01.855512: step 15562, loss 0.526989.
Train: 2018-08-09T11:39:01.949241: step 15563, loss 0.684395.
Train: 2018-08-09T11:39:02.027346: step 15564, loss 0.52889.
Train: 2018-08-09T11:39:02.105454: step 15565, loss 0.494077.
Train: 2018-08-09T11:39:02.183560: step 15566, loss 0.61157.
Train: 2018-08-09T11:39:02.261667: step 15567, loss 0.547938.
Train: 2018-08-09T11:39:02.355364: step 15568, loss 0.612782.
Train: 2018-08-09T11:39:02.433469: step 15569, loss 0.512393.
Train: 2018-08-09T11:39:02.511576: step 15570, loss 0.562635.
Test: 2018-08-09T11:39:03.011459: step 15570, loss 0.549817.
Train: 2018-08-09T11:39:03.089566: step 15571, loss 0.562973.
Train: 2018-08-09T11:39:03.167672: step 15572, loss 0.562832.
Train: 2018-08-09T11:39:03.261430: step 15573, loss 0.663687.
Train: 2018-08-09T11:39:03.339535: step 15574, loss 0.578491.
Train: 2018-08-09T11:39:03.417612: step 15575, loss 0.529966.
Train: 2018-08-09T11:39:03.495751: step 15576, loss 0.545951.
Train: 2018-08-09T11:39:03.589974: step 15577, loss 0.431167.
Train: 2018-08-09T11:39:03.667564: step 15578, loss 0.562792.
Train: 2018-08-09T11:39:03.745671: step 15579, loss 0.529419.
Train: 2018-08-09T11:39:03.823776: step 15580, loss 0.628268.
Test: 2018-08-09T11:39:04.325987: step 15580, loss 0.54643.
Train: 2018-08-09T11:39:04.404124: step 15581, loss 0.463608.
Train: 2018-08-09T11:39:04.482199: step 15582, loss 0.595641.
Train: 2018-08-09T11:39:04.560334: step 15583, loss 0.59521.
Train: 2018-08-09T11:39:04.654064: step 15584, loss 0.545946.
Train: 2018-08-09T11:39:04.732170: step 15585, loss 0.561519.
Train: 2018-08-09T11:39:04.810247: step 15586, loss 0.595615.
Train: 2018-08-09T11:39:04.888384: step 15587, loss 0.444714.
Train: 2018-08-09T11:39:04.982112: step 15588, loss 0.558616.
Train: 2018-08-09T11:39:05.060188: step 15589, loss 0.593692.
Train: 2018-08-09T11:39:05.138294: step 15590, loss 0.6576.
Test: 2018-08-09T11:39:05.638178: step 15590, loss 0.550005.
Train: 2018-08-09T11:39:05.716313: step 15591, loss 0.460477.
Train: 2018-08-09T11:39:05.794421: step 15592, loss 0.527531.
Train: 2018-08-09T11:39:05.872529: step 15593, loss 0.596994.
Train: 2018-08-09T11:39:05.951990: step 15594, loss 0.457794.
Train: 2018-08-09T11:39:06.045719: step 15595, loss 0.632164.
Train: 2018-08-09T11:39:06.123796: step 15596, loss 0.584355.
Train: 2018-08-09T11:39:06.201901: step 15597, loss 0.613536.
Train: 2018-08-09T11:39:06.280041: step 15598, loss 0.563505.
Train: 2018-08-09T11:39:06.358117: step 15599, loss 0.511868.
Train: 2018-08-09T11:39:06.451873: step 15600, loss 0.494641.
Test: 2018-08-09T11:39:06.951757: step 15600, loss 0.54844.
Train: 2018-08-09T11:39:07.482880: step 15601, loss 0.560025.
Train: 2018-08-09T11:39:07.560987: step 15602, loss 0.546001.
Train: 2018-08-09T11:39:07.639095: step 15603, loss 0.63276.
Train: 2018-08-09T11:39:07.717200: step 15604, loss 0.628238.
Train: 2018-08-09T11:39:07.795306: step 15605, loss 0.445909.
Train: 2018-08-09T11:39:07.889006: step 15606, loss 0.595959.
Train: 2018-08-09T11:39:07.967871: step 15607, loss 0.546045.
Train: 2018-08-09T11:39:08.045947: step 15608, loss 0.528904.
Train: 2018-08-09T11:39:08.124085: step 15609, loss 0.646681.
Train: 2018-08-09T11:39:08.202191: step 15610, loss 0.628424.
Test: 2018-08-09T11:39:08.702068: step 15610, loss 0.547769.
Train: 2018-08-09T11:39:08.780180: step 15611, loss 0.644578.
Train: 2018-08-09T11:39:08.858289: step 15612, loss 0.529983.
Train: 2018-08-09T11:39:08.952016: step 15613, loss 0.546476.
Train: 2018-08-09T11:39:09.030120: step 15614, loss 0.579206.
Train: 2018-08-09T11:39:09.108197: step 15615, loss 0.546497.
Train: 2018-08-09T11:39:09.186336: step 15616, loss 0.498054.
Train: 2018-08-09T11:39:09.264445: step 15617, loss 0.417514.
Train: 2018-08-09T11:39:09.358156: step 15618, loss 0.611171.
Train: 2018-08-09T11:39:09.436275: step 15619, loss 0.611242.
Train: 2018-08-09T11:39:09.514385: step 15620, loss 0.562663.
Test: 2018-08-09T11:39:10.014237: step 15620, loss 0.547525.
Train: 2018-08-09T11:39:10.092341: step 15621, loss 0.530306.
Train: 2018-08-09T11:39:10.170479: step 15622, loss 0.449191.
Train: 2018-08-09T11:39:10.264210: step 15623, loss 0.497699.
Train: 2018-08-09T11:39:10.342306: step 15624, loss 0.464266.
Train: 2018-08-09T11:39:10.420389: step 15625, loss 0.496008.
Train: 2018-08-09T11:39:10.498523: step 15626, loss 0.629475.
Train: 2018-08-09T11:39:10.592260: step 15627, loss 0.477916.
Train: 2018-08-09T11:39:10.670359: step 15628, loss 0.461217.
Train: 2018-08-09T11:39:10.748466: step 15629, loss 0.54613.
Train: 2018-08-09T11:39:10.826542: step 15630, loss 0.526281.
Test: 2018-08-09T11:39:11.328921: step 15630, loss 0.547812.
Train: 2018-08-09T11:39:11.407057: step 15631, loss 0.474438.
Train: 2018-08-09T11:39:11.485134: step 15632, loss 0.634098.
Train: 2018-08-09T11:39:11.578896: step 15633, loss 0.691461.
Train: 2018-08-09T11:39:11.656998: step 15634, loss 0.522368.
Train: 2018-08-09T11:39:11.735074: step 15635, loss 0.563525.
Train: 2018-08-09T11:39:11.813182: step 15636, loss 0.619633.
Train: 2018-08-09T11:39:11.891319: step 15637, loss 0.596692.
Train: 2018-08-09T11:39:11.985047: step 15638, loss 0.542513.
Train: 2018-08-09T11:39:12.063155: step 15639, loss 0.61432.
Train: 2018-08-09T11:39:12.141229: step 15640, loss 0.545921.
Test: 2018-08-09T11:39:12.641136: step 15640, loss 0.546878.
Train: 2018-08-09T11:39:12.719248: step 15641, loss 0.527974.
Train: 2018-08-09T11:39:12.797355: step 15642, loss 0.563256.
Train: 2018-08-09T11:39:12.891052: step 15643, loss 0.545777.
Train: 2018-08-09T11:39:12.971476: step 15644, loss 0.477605.
Train: 2018-08-09T11:39:13.055277: step 15645, loss 0.562061.
Train: 2018-08-09T11:39:13.133355: step 15646, loss 0.562956.
Train: 2018-08-09T11:39:13.211459: step 15647, loss 0.629647.
Train: 2018-08-09T11:39:13.289597: step 15648, loss 0.461693.
Train: 2018-08-09T11:39:13.383297: step 15649, loss 0.596132.
Train: 2018-08-09T11:39:13.461400: step 15650, loss 0.630142.
Test: 2018-08-09T11:39:13.961284: step 15650, loss 0.547308.
Train: 2018-08-09T11:39:14.039415: step 15651, loss 0.562546.
Train: 2018-08-09T11:39:14.117496: step 15652, loss 0.580413.
Train: 2018-08-09T11:39:14.195604: step 15653, loss 0.462552.
Train: 2018-08-09T11:39:14.273709: step 15654, loss 0.645842.
Train: 2018-08-09T11:39:14.367438: step 15655, loss 0.59565.
Train: 2018-08-09T11:39:14.445577: step 15656, loss 0.529252.
Train: 2018-08-09T11:39:14.523681: step 15657, loss 0.529344.
Train: 2018-08-09T11:39:14.601788: step 15658, loss 0.562446.
Train: 2018-08-09T11:39:14.679895: step 15659, loss 0.612076.
Train: 2018-08-09T11:39:14.773591: step 15660, loss 0.562449.
Test: 2018-08-09T11:39:15.273506: step 15660, loss 0.546421.
Train: 2018-08-09T11:39:15.351580: step 15661, loss 0.661566.
Train: 2018-08-09T11:39:15.429713: step 15662, loss 0.529718.
Train: 2018-08-09T11:39:15.507820: step 15663, loss 0.56259.
Train: 2018-08-09T11:39:15.601521: step 15664, loss 0.611752.
Train: 2018-08-09T11:39:15.679629: step 15665, loss 0.611586.
Train: 2018-08-09T11:39:15.773356: step 15666, loss 0.530069.
Train: 2018-08-09T11:39:15.851493: step 15667, loss 0.57897.
Train: 2018-08-09T11:39:15.929569: step 15668, loss 0.530265.
Train: 2018-08-09T11:39:16.023329: step 15669, loss 0.660011.
Train: 2018-08-09T11:39:16.101403: step 15670, loss 0.481866.
Test: 2018-08-09T11:39:16.601317: step 15670, loss 0.549427.
Train: 2018-08-09T11:39:16.679423: step 15671, loss 0.627314.
Train: 2018-08-09T11:39:16.757529: step 15672, loss 0.546662.
Train: 2018-08-09T11:39:16.835639: step 15673, loss 0.514495.
Train: 2018-08-09T11:39:16.916031: step 15674, loss 0.498438.
Train: 2018-08-09T11:39:17.009760: step 15675, loss 0.562804.
Train: 2018-08-09T11:39:17.087897: step 15676, loss 0.466135.
Train: 2018-08-09T11:39:17.166007: step 15677, loss 0.578881.
Train: 2018-08-09T11:39:17.244110: step 15678, loss 0.562649.
Train: 2018-08-09T11:39:17.337839: step 15679, loss 0.627537.
Train: 2018-08-09T11:39:17.415946: step 15680, loss 0.497621.
Test: 2018-08-09T11:39:17.915797: step 15680, loss 0.546688.
Train: 2018-08-09T11:39:17.993935: step 15681, loss 0.710381.
Train: 2018-08-09T11:39:18.072039: step 15682, loss 0.562701.
Train: 2018-08-09T11:39:18.150118: step 15683, loss 0.498083.
Train: 2018-08-09T11:39:18.243874: step 15684, loss 0.546521.
Train: 2018-08-09T11:39:18.321980: step 15685, loss 0.530257.
Train: 2018-08-09T11:39:18.400056: step 15686, loss 0.740855.
Train: 2018-08-09T11:39:18.478163: step 15687, loss 0.659714.
Train: 2018-08-09T11:39:18.556300: step 15688, loss 0.594971.
Train: 2018-08-09T11:39:18.650022: step 15689, loss 0.562804.
Train: 2018-08-09T11:39:18.728135: step 15690, loss 0.578884.
Test: 2018-08-09T11:39:19.229422: step 15690, loss 0.550349.
Train: 2018-08-09T11:39:19.307559: step 15691, loss 0.594842.
Train: 2018-08-09T11:39:19.385669: step 15692, loss 0.562943.
Train: 2018-08-09T11:39:19.463776: step 15693, loss 0.451839.
Train: 2018-08-09T11:39:19.557494: step 15694, loss 0.547111.
Train: 2018-08-09T11:39:19.635606: step 15695, loss 0.610576.
Train: 2018-08-09T11:39:19.713682: step 15696, loss 0.610589.
Train: 2018-08-09T11:39:19.791821: step 15697, loss 0.626367.
Train: 2018-08-09T11:39:19.869926: step 15698, loss 0.468244.
Train: 2018-08-09T11:39:19.948034: step 15699, loss 0.642053.
Train: 2018-08-09T11:39:20.041761: step 15700, loss 0.594624.
Test: 2018-08-09T11:39:20.541619: step 15700, loss 0.550148.
Train: 2018-08-09T11:39:21.075195: step 15701, loss 0.515874.
Train: 2018-08-09T11:39:21.153301: step 15702, loss 0.657571.
Train: 2018-08-09T11:39:21.231411: step 15703, loss 0.547436.
Train: 2018-08-09T11:39:21.309515: step 15704, loss 0.53182.
Train: 2018-08-09T11:39:21.387622: step 15705, loss 0.610235.
Train: 2018-08-09T11:39:21.481345: step 15706, loss 0.547562.
Train: 2018-08-09T11:39:21.559452: step 15707, loss 0.704098.
Train: 2018-08-09T11:39:21.637562: step 15708, loss 0.469655.
Train: 2018-08-09T11:39:21.715641: step 15709, loss 0.563287.
Train: 2018-08-09T11:39:21.809368: step 15710, loss 0.485373.
Test: 2018-08-09T11:39:22.309249: step 15710, loss 0.550434.
Train: 2018-08-09T11:39:22.387355: step 15711, loss 0.578876.
Train: 2018-08-09T11:39:22.465461: step 15712, loss 0.610096.
Train: 2018-08-09T11:39:22.543596: step 15713, loss 0.6101.
Train: 2018-08-09T11:39:22.621706: step 15714, loss 0.57877.
Train: 2018-08-09T11:39:22.715403: step 15715, loss 0.516463.
Train: 2018-08-09T11:39:22.793542: step 15716, loss 0.500738.
Train: 2018-08-09T11:39:22.871650: step 15717, loss 0.595054.
Train: 2018-08-09T11:39:22.952057: step 15718, loss 0.610553.
Train: 2018-08-09T11:39:23.030167: step 15719, loss 0.531314.
Train: 2018-08-09T11:39:23.108272: step 15720, loss 0.594304.
Test: 2018-08-09T11:39:23.608124: step 15720, loss 0.548921.
Train: 2018-08-09T11:39:23.686254: step 15721, loss 0.515488.
Train: 2018-08-09T11:39:23.764366: step 15722, loss 0.562904.
Train: 2018-08-09T11:39:23.858063: step 15723, loss 0.610927.
Train: 2018-08-09T11:39:23.936200: step 15724, loss 0.595261.
Train: 2018-08-09T11:39:24.014306: step 15725, loss 0.514632.
Train: 2018-08-09T11:39:24.092383: step 15726, loss 0.514439.
Train: 2018-08-09T11:39:24.170490: step 15727, loss 0.609875.
Train: 2018-08-09T11:39:24.264247: step 15728, loss 0.596111.
Train: 2018-08-09T11:39:24.342354: step 15729, loss 0.513704.
Train: 2018-08-09T11:39:24.420461: step 15730, loss 0.545666.
Test: 2018-08-09T11:39:24.921020: step 15730, loss 0.548936.
Train: 2018-08-09T11:39:24.999157: step 15731, loss 0.615821.
Train: 2018-08-09T11:39:25.077235: step 15732, loss 0.610565.
Train: 2018-08-09T11:39:25.155343: step 15733, loss 0.531711.
Train: 2018-08-09T11:39:25.233477: step 15734, loss 0.529619.
Train: 2018-08-09T11:39:25.327205: step 15735, loss 0.545959.
Train: 2018-08-09T11:39:25.405283: step 15736, loss 0.464499.
Train: 2018-08-09T11:39:25.483414: step 15737, loss 0.593818.
Train: 2018-08-09T11:39:25.561525: step 15738, loss 0.579961.
Train: 2018-08-09T11:39:25.639627: step 15739, loss 0.53002.
Train: 2018-08-09T11:39:25.733359: step 15740, loss 0.5286.
Test: 2018-08-09T11:39:26.233236: step 15740, loss 0.548946.
Train: 2018-08-09T11:39:26.311348: step 15741, loss 0.529888.
Train: 2018-08-09T11:39:26.389455: step 15742, loss 0.545635.
Train: 2018-08-09T11:39:26.467566: step 15743, loss 0.510672.
Train: 2018-08-09T11:39:26.545639: step 15744, loss 0.707066.
Train: 2018-08-09T11:39:26.623745: step 15745, loss 0.561354.
Train: 2018-08-09T11:39:26.717504: step 15746, loss 0.595631.
Train: 2018-08-09T11:39:26.795580: step 15747, loss 0.60061.
Train: 2018-08-09T11:39:26.873686: step 15748, loss 0.529125.
Train: 2018-08-09T11:39:26.951794: step 15749, loss 0.512546.
Train: 2018-08-09T11:39:27.029933: step 15750, loss 0.612976.
Test: 2018-08-09T11:39:27.529782: step 15750, loss 0.5484.
Train: 2018-08-09T11:39:27.607887: step 15751, loss 0.496724.
Train: 2018-08-09T11:39:27.685995: step 15752, loss 0.580414.
Train: 2018-08-09T11:39:27.779723: step 15753, loss 0.529687.
Train: 2018-08-09T11:39:27.857828: step 15754, loss 0.544693.
Train: 2018-08-09T11:39:27.937309: step 15755, loss 0.577114.
Train: 2018-08-09T11:39:28.015418: step 15756, loss 0.545584.
Train: 2018-08-09T11:39:28.093494: step 15757, loss 0.71217.
Train: 2018-08-09T11:39:28.171600: step 15758, loss 0.645206.
Train: 2018-08-09T11:39:28.265327: step 15759, loss 0.545733.
Train: 2018-08-09T11:39:28.343467: step 15760, loss 0.546456.
Test: 2018-08-09T11:39:28.827717: step 15760, loss 0.54874.
Train: 2018-08-09T11:39:28.921472: step 15761, loss 0.612049.
Train: 2018-08-09T11:39:28.999559: step 15762, loss 0.561462.
Train: 2018-08-09T11:39:29.077667: step 15763, loss 0.59496.
Train: 2018-08-09T11:39:29.155773: step 15764, loss 0.513556.
Train: 2018-08-09T11:39:29.233879: step 15765, loss 0.562681.
Train: 2018-08-09T11:39:29.327608: step 15766, loss 0.561776.
Train: 2018-08-09T11:39:29.405715: step 15767, loss 0.516098.
Train: 2018-08-09T11:39:29.483821: step 15768, loss 0.514728.
Train: 2018-08-09T11:39:29.561897: step 15769, loss 0.529178.
Train: 2018-08-09T11:39:29.640004: step 15770, loss 0.691185.
Test: 2018-08-09T11:39:30.142250: step 15770, loss 0.548705.
Train: 2018-08-09T11:39:30.220356: step 15771, loss 0.497731.
Train: 2018-08-09T11:39:30.298463: step 15772, loss 0.483291.
Train: 2018-08-09T11:39:30.392191: step 15773, loss 0.5418.
Train: 2018-08-09T11:39:30.470327: step 15774, loss 0.595978.
Train: 2018-08-09T11:39:30.548403: step 15775, loss 0.525418.
Train: 2018-08-09T11:39:30.626542: step 15776, loss 0.704966.
Train: 2018-08-09T11:39:30.704647: step 15777, loss 0.532415.
Train: 2018-08-09T11:39:30.782756: step 15778, loss 0.559917.
Train: 2018-08-09T11:39:30.876483: step 15779, loss 0.578762.
Train: 2018-08-09T11:39:30.954588: step 15780, loss 0.664085.
Test: 2018-08-09T11:39:31.454440: step 15780, loss 0.549689.
Train: 2018-08-09T11:39:31.532576: step 15781, loss 0.482035.
Train: 2018-08-09T11:39:31.610683: step 15782, loss 0.64243.
Train: 2018-08-09T11:39:31.688794: step 15783, loss 0.497776.
Train: 2018-08-09T11:39:31.766868: step 15784, loss 0.565175.
Train: 2018-08-09T11:39:31.941137: step 15785, loss 0.531313.
Train: 2018-08-09T11:39:32.019273: step 15786, loss 0.453035.
Train: 2018-08-09T11:39:32.097380: step 15787, loss 0.499242.
Train: 2018-08-09T11:39:32.175490: step 15788, loss 0.577739.
Train: 2018-08-09T11:39:32.269140: step 15789, loss 0.499258.
Train: 2018-08-09T11:39:32.347247: step 15790, loss 0.611542.
Test: 2018-08-09T11:39:32.847099: step 15790, loss 0.546126.
Train: 2018-08-09T11:39:32.925207: step 15791, loss 0.564037.
Train: 2018-08-09T11:39:33.003345: step 15792, loss 0.513198.
Train: 2018-08-09T11:39:33.081451: step 15793, loss 0.465405.
Train: 2018-08-09T11:39:33.175179: step 15794, loss 0.497509.
Train: 2018-08-09T11:39:33.253286: step 15795, loss 0.594944.
Train: 2018-08-09T11:39:33.331362: step 15796, loss 0.562892.
Train: 2018-08-09T11:39:33.409466: step 15797, loss 0.545582.
Train: 2018-08-09T11:39:33.487574: step 15798, loss 0.565369.
Train: 2018-08-09T11:39:33.569442: step 15799, loss 0.686576.
Train: 2018-08-09T11:39:33.663170: step 15800, loss 0.544424.
Test: 2018-08-09T11:39:34.163076: step 15800, loss 0.549975.
Train: 2018-08-09T11:39:34.694176: step 15801, loss 0.579168.
Train: 2018-08-09T11:39:34.772283: step 15802, loss 0.526496.
Train: 2018-08-09T11:39:34.866044: step 15803, loss 0.596222.
Train: 2018-08-09T11:39:34.944148: step 15804, loss 0.498113.
Train: 2018-08-09T11:39:35.022224: step 15805, loss 0.514532.
Train: 2018-08-09T11:39:35.100364: step 15806, loss 0.562876.
Train: 2018-08-09T11:39:35.194084: step 15807, loss 0.54599.
Train: 2018-08-09T11:39:35.272195: step 15808, loss 0.545998.
Train: 2018-08-09T11:39:35.350271: step 15809, loss 0.594322.
Train: 2018-08-09T11:39:35.428379: step 15810, loss 0.561826.
Test: 2018-08-09T11:39:35.930890: step 15810, loss 0.548953.
Train: 2018-08-09T11:39:36.008997: step 15811, loss 0.513966.
Train: 2018-08-09T11:39:36.087134: step 15812, loss 0.529529.
Train: 2018-08-09T11:39:36.165238: step 15813, loss 0.513392.
Train: 2018-08-09T11:39:36.243347: step 15814, loss 0.563347.
Train: 2018-08-09T11:39:36.321452: step 15815, loss 0.595065.
Train: 2018-08-09T11:39:36.415150: step 15816, loss 0.613321.
Train: 2018-08-09T11:39:36.493288: step 15817, loss 0.545945.
Train: 2018-08-09T11:39:36.571394: step 15818, loss 0.561328.
Train: 2018-08-09T11:39:36.649501: step 15819, loss 0.563578.
Train: 2018-08-09T11:39:36.743199: step 15820, loss 0.496779.
Test: 2018-08-09T11:39:37.227484: step 15820, loss 0.549448.
Train: 2018-08-09T11:39:37.305596: step 15821, loss 0.480308.
Train: 2018-08-09T11:39:37.399294: step 15822, loss 0.545944.
Train: 2018-08-09T11:39:37.477432: step 15823, loss 0.61241.
Train: 2018-08-09T11:39:37.555537: step 15824, loss 0.62842.
Train: 2018-08-09T11:39:37.633647: step 15825, loss 0.495201.
Train: 2018-08-09T11:39:37.711746: step 15826, loss 0.595124.
Train: 2018-08-09T11:39:37.805479: step 15827, loss 0.511951.
Train: 2018-08-09T11:39:37.883585: step 15828, loss 0.562569.
Train: 2018-08-09T11:39:37.964055: step 15829, loss 0.512699.
Train: 2018-08-09T11:39:38.042160: step 15830, loss 0.529105.
Test: 2018-08-09T11:39:38.542021: step 15830, loss 0.549212.
Train: 2018-08-09T11:39:38.620151: step 15831, loss 0.6453.
Train: 2018-08-09T11:39:38.698224: step 15832, loss 0.596414.
Train: 2018-08-09T11:39:38.791953: step 15833, loss 0.545523.
Train: 2018-08-09T11:39:38.870092: step 15834, loss 0.596014.
Train: 2018-08-09T11:39:38.948197: step 15835, loss 0.495213.
Train: 2018-08-09T11:39:39.026274: step 15836, loss 0.461458.
Train: 2018-08-09T11:39:39.104409: step 15837, loss 0.561408.
Train: 2018-08-09T11:39:39.198138: step 15838, loss 0.47822.
Train: 2018-08-09T11:39:39.276244: step 15839, loss 0.561794.
Train: 2018-08-09T11:39:39.354350: step 15840, loss 0.459612.
Test: 2018-08-09T11:39:39.854204: step 15840, loss 0.550191.
Train: 2018-08-09T11:39:39.934826: step 15841, loss 0.562594.
Train: 2018-08-09T11:39:40.012932: step 15842, loss 0.647218.
Train: 2018-08-09T11:39:40.091042: step 15843, loss 0.544538.
Train: 2018-08-09T11:39:40.184767: step 15844, loss 0.545889.
Train: 2018-08-09T11:39:40.262874: step 15845, loss 0.559158.
Train: 2018-08-09T11:39:40.340981: step 15846, loss 0.595757.
Train: 2018-08-09T11:39:40.419087: step 15847, loss 0.635395.
Train: 2018-08-09T11:39:40.497193: step 15848, loss 0.681102.
Train: 2018-08-09T11:39:40.575303: step 15849, loss 0.563092.
Train: 2018-08-09T11:39:40.669028: step 15850, loss 0.578128.
Test: 2018-08-09T11:39:41.153283: step 15850, loss 0.55148.
Train: 2018-08-09T11:39:41.231396: step 15851, loss 0.510362.
Train: 2018-08-09T11:39:41.325094: step 15852, loss 0.614896.
Train: 2018-08-09T11:39:41.403253: step 15853, loss 0.685059.
Train: 2018-08-09T11:39:41.481337: step 15854, loss 0.494701.
Train: 2018-08-09T11:39:41.559443: step 15855, loss 0.49662.
Train: 2018-08-09T11:39:41.637551: step 15856, loss 0.646091.
Train: 2018-08-09T11:39:41.731279: step 15857, loss 0.494237.
Train: 2018-08-09T11:39:41.809353: step 15858, loss 0.56277.
Train: 2018-08-09T11:39:41.887491: step 15859, loss 0.463889.
Train: 2018-08-09T11:39:41.967973: step 15860, loss 0.530143.
Test: 2018-08-09T11:39:42.467827: step 15860, loss 0.5458.
Train: 2018-08-09T11:39:42.577174: step 15861, loss 0.529919.
Train: 2018-08-09T11:39:42.655312: step 15862, loss 0.562859.
Train: 2018-08-09T11:39:42.733420: step 15863, loss 0.495978.
Train: 2018-08-09T11:39:42.811494: step 15864, loss 0.678343.
Train: 2018-08-09T11:39:42.889602: step 15865, loss 0.478714.
Train: 2018-08-09T11:39:42.983329: step 15866, loss 0.530265.
Train: 2018-08-09T11:39:43.061466: step 15867, loss 0.463752.
Train: 2018-08-09T11:39:43.139567: step 15868, loss 0.527758.
Train: 2018-08-09T11:39:43.217649: step 15869, loss 0.562473.
Train: 2018-08-09T11:39:43.295787: step 15870, loss 0.529016.
Test: 2018-08-09T11:39:43.795638: step 15870, loss 0.549074.
Train: 2018-08-09T11:39:43.873745: step 15871, loss 0.427708.
Train: 2018-08-09T11:39:43.952523: step 15872, loss 0.647668.
Train: 2018-08-09T11:39:44.046222: step 15873, loss 0.61295.
Train: 2018-08-09T11:39:44.124327: step 15874, loss 0.492886.
Train: 2018-08-09T11:39:44.202464: step 15875, loss 0.528731.
Train: 2018-08-09T11:39:44.280572: step 15876, loss 0.629445.
Train: 2018-08-09T11:39:44.358677: step 15877, loss 0.543974.
Train: 2018-08-09T11:39:44.452405: step 15878, loss 0.628022.
Train: 2018-08-09T11:39:44.530512: step 15879, loss 0.649807.
Train: 2018-08-09T11:39:44.608621: step 15880, loss 0.594356.
Test: 2018-08-09T11:39:45.108472: step 15880, loss 0.546313.
Train: 2018-08-09T11:39:45.186607: step 15881, loss 0.663697.
Train: 2018-08-09T11:39:45.264714: step 15882, loss 0.492624.
Train: 2018-08-09T11:39:45.356394: step 15883, loss 0.614312.
Train: 2018-08-09T11:39:45.434501: step 15884, loss 0.595132.
Train: 2018-08-09T11:39:45.512638: step 15885, loss 0.632865.
Train: 2018-08-09T11:39:45.590740: step 15886, loss 0.579914.
Train: 2018-08-09T11:39:45.668851: step 15887, loss 0.528532.
Train: 2018-08-09T11:39:45.762549: step 15888, loss 0.543366.
Train: 2018-08-09T11:39:45.840686: step 15889, loss 0.547758.
Train: 2018-08-09T11:39:45.918792: step 15890, loss 0.480863.
Test: 2018-08-09T11:39:46.418644: step 15890, loss 0.548904.
Train: 2018-08-09T11:39:46.496781: step 15891, loss 0.528064.
Train: 2018-08-09T11:39:46.574887: step 15892, loss 0.628403.
Train: 2018-08-09T11:39:46.652994: step 15893, loss 0.594996.
Train: 2018-08-09T11:39:46.746692: step 15894, loss 0.579192.
Train: 2018-08-09T11:39:46.824828: step 15895, loss 0.593062.
Train: 2018-08-09T11:39:46.902906: step 15896, loss 0.563522.
Train: 2018-08-09T11:39:46.981043: step 15897, loss 0.52961.
Train: 2018-08-09T11:39:47.059149: step 15898, loss 0.54598.
Train: 2018-08-09T11:39:47.152880: step 15899, loss 0.56326.
Train: 2018-08-09T11:39:47.230983: step 15900, loss 0.530078.
Test: 2018-08-09T11:39:47.730835: step 15900, loss 0.549795.
Train: 2018-08-09T11:39:48.277614: step 15901, loss 0.59245.
Train: 2018-08-09T11:39:48.355712: step 15902, loss 0.499321.
Train: 2018-08-09T11:39:48.433797: step 15903, loss 0.64454.
Train: 2018-08-09T11:39:48.511934: step 15904, loss 0.676171.
Train: 2018-08-09T11:39:48.590038: step 15905, loss 0.482481.
Train: 2018-08-09T11:39:48.683766: step 15906, loss 0.593131.
Train: 2018-08-09T11:39:48.761870: step 15907, loss 0.595718.
Train: 2018-08-09T11:39:48.839979: step 15908, loss 0.645752.
Train: 2018-08-09T11:39:48.919562: step 15909, loss 0.498413.
Train: 2018-08-09T11:39:48.997699: step 15910, loss 0.577839.
Test: 2018-08-09T11:39:49.497577: step 15910, loss 0.551469.
Train: 2018-08-09T11:39:49.575658: step 15911, loss 0.579173.
Train: 2018-08-09T11:39:49.669416: step 15912, loss 0.548367.
Train: 2018-08-09T11:39:49.747522: step 15913, loss 0.578793.
Train: 2018-08-09T11:39:49.825631: step 15914, loss 0.563356.
Train: 2018-08-09T11:39:49.903736: step 15915, loss 0.498428.
Train: 2018-08-09T11:39:49.981842: step 15916, loss 0.531142.
Train: 2018-08-09T11:39:50.075541: step 15917, loss 0.609941.
Train: 2018-08-09T11:39:50.153677: step 15918, loss 0.562744.
Train: 2018-08-09T11:39:50.231783: step 15919, loss 0.545964.
Train: 2018-08-09T11:39:50.309890: step 15920, loss 0.660297.
Test: 2018-08-09T11:39:50.809747: step 15920, loss 0.548036.
Train: 2018-08-09T11:39:50.887879: step 15921, loss 0.499658.
Train: 2018-08-09T11:39:50.968334: step 15922, loss 0.516068.
Train: 2018-08-09T11:39:51.046443: step 15923, loss 0.578713.
Train: 2018-08-09T11:39:51.140173: step 15924, loss 0.611496.
Train: 2018-08-09T11:39:51.218283: step 15925, loss 0.51412.
Train: 2018-08-09T11:39:51.296359: step 15926, loss 0.564486.
Train: 2018-08-09T11:39:51.374464: step 15927, loss 0.581426.
Train: 2018-08-09T11:39:51.452570: step 15928, loss 0.562526.
Train: 2018-08-09T11:39:51.546298: step 15929, loss 0.626377.
Train: 2018-08-09T11:39:51.624434: step 15930, loss 0.595062.
Test: 2018-08-09T11:39:52.124287: step 15930, loss 0.551591.
Train: 2018-08-09T11:39:52.202429: step 15931, loss 0.562177.
Train: 2018-08-09T11:39:52.280500: step 15932, loss 0.610686.
Train: 2018-08-09T11:39:52.358607: step 15933, loss 0.64205.
Train: 2018-08-09T11:39:52.452358: step 15934, loss 0.531671.
Train: 2018-08-09T11:39:52.530440: step 15935, loss 0.562643.
Train: 2018-08-09T11:39:52.608550: step 15936, loss 0.548237.
Train: 2018-08-09T11:39:52.686685: step 15937, loss 0.594455.
Train: 2018-08-09T11:39:52.764790: step 15938, loss 0.516156.
Train: 2018-08-09T11:39:52.842869: step 15939, loss 0.547396.
Train: 2018-08-09T11:39:52.937240: step 15940, loss 0.578431.
Test: 2018-08-09T11:39:53.421471: step 15940, loss 0.551388.
Train: 2018-08-09T11:39:53.515229: step 15941, loss 0.546518.
Train: 2018-08-09T11:39:53.593306: step 15942, loss 0.516223.
Train: 2018-08-09T11:39:53.671413: step 15943, loss 0.49999.
Train: 2018-08-09T11:39:53.749549: step 15944, loss 0.562645.
Train: 2018-08-09T11:39:53.827655: step 15945, loss 0.450152.
Train: 2018-08-09T11:39:53.905761: step 15946, loss 0.465774.
Train: 2018-08-09T11:39:53.999484: step 15947, loss 0.594382.
Train: 2018-08-09T11:39:54.077600: step 15948, loss 0.530409.
Train: 2018-08-09T11:39:54.155673: step 15949, loss 0.637011.
Train: 2018-08-09T11:39:54.233810: step 15950, loss 0.564354.
Test: 2018-08-09T11:39:54.733662: step 15950, loss 0.547789.
Train: 2018-08-09T11:39:54.811770: step 15951, loss 0.508136.
Train: 2018-08-09T11:39:54.889904: step 15952, loss 0.541708.
Train: 2018-08-09T11:39:54.968012: step 15953, loss 0.51744.
Train: 2018-08-09T11:39:55.061740: step 15954, loss 0.577666.
Train: 2018-08-09T11:39:55.139846: step 15955, loss 0.759686.
Train: 2018-08-09T11:39:55.217959: step 15956, loss 0.528901.
Train: 2018-08-09T11:39:55.296029: step 15957, loss 0.638192.
Train: 2018-08-09T11:39:55.389787: step 15958, loss 0.543987.
Train: 2018-08-09T11:39:55.467865: step 15959, loss 0.679311.
Train: 2018-08-09T11:39:55.545971: step 15960, loss 0.544117.
Test: 2018-08-09T11:39:56.047295: step 15960, loss 0.546394.
Train: 2018-08-09T11:39:56.125377: step 15961, loss 0.61306.
Train: 2018-08-09T11:39:56.203508: step 15962, loss 0.529362.
Train: 2018-08-09T11:39:56.281621: step 15963, loss 0.398397.
Train: 2018-08-09T11:39:56.375349: step 15964, loss 0.459384.
Train: 2018-08-09T11:39:56.453454: step 15965, loss 0.562404.
Train: 2018-08-09T11:39:56.531531: step 15966, loss 0.545476.
Train: 2018-08-09T11:39:56.609667: step 15967, loss 0.612554.
Train: 2018-08-09T11:39:56.687774: step 15968, loss 0.566072.
Train: 2018-08-09T11:39:56.765885: step 15969, loss 0.625249.
Train: 2018-08-09T11:39:56.859609: step 15970, loss 0.576898.
Test: 2018-08-09T11:39:57.343877: step 15970, loss 0.549976.
Train: 2018-08-09T11:39:57.421977: step 15971, loss 0.547809.
Train: 2018-08-09T11:39:57.515707: step 15972, loss 0.612745.
Train: 2018-08-09T11:39:57.593812: step 15973, loss 0.513078.
Train: 2018-08-09T11:39:57.671918: step 15974, loss 0.529688.
Train: 2018-08-09T11:39:57.750020: step 15975, loss 0.514518.
Train: 2018-08-09T11:39:57.828132: step 15976, loss 0.580561.
Train: 2018-08-09T11:39:57.922395: step 15977, loss 0.49577.
Train: 2018-08-09T11:39:58.000500: step 15978, loss 0.545833.
Train: 2018-08-09T11:39:58.078608: step 15979, loss 0.546516.
Train: 2018-08-09T11:39:58.156714: step 15980, loss 0.543014.
Test: 2018-08-09T11:39:58.656567: step 15980, loss 0.54613.
Train: 2018-08-09T11:39:58.734703: step 15981, loss 0.545897.
Train: 2018-08-09T11:39:58.812810: step 15982, loss 0.47578.
Train: 2018-08-09T11:39:58.890917: step 15983, loss 0.579855.
Train: 2018-08-09T11:39:58.984644: step 15984, loss 0.583344.
Train: 2018-08-09T11:39:59.062719: step 15985, loss 0.559949.
Train: 2018-08-09T11:39:59.140827: step 15986, loss 0.510866.
Train: 2018-08-09T11:39:59.218934: step 15987, loss 0.436924.
Train: 2018-08-09T11:39:59.312662: step 15988, loss 0.580348.
Train: 2018-08-09T11:39:59.390793: step 15989, loss 0.489179.
Train: 2018-08-09T11:39:59.468904: step 15990, loss 0.441894.
Test: 2018-08-09T11:39:59.970139: step 15990, loss 0.548458.
Train: 2018-08-09T11:40:00.079486: step 15991, loss 0.626952.
Train: 2018-08-09T11:40:00.157562: step 15992, loss 0.576375.
Train: 2018-08-09T11:40:00.235668: step 15993, loss 0.586571.
Train: 2018-08-09T11:40:00.313805: step 15994, loss 0.569064.
Train: 2018-08-09T11:40:00.391881: step 15995, loss 0.456632.
Train: 2018-08-09T11:40:00.485641: step 15996, loss 0.475485.
Train: 2018-08-09T11:40:00.563716: step 15997, loss 0.581509.
Train: 2018-08-09T11:40:00.641853: step 15998, loss 0.57366.
Train: 2018-08-09T11:40:00.719961: step 15999, loss 0.54878.
Train: 2018-08-09T11:40:00.798066: step 16000, loss 0.472097.
Test: 2018-08-09T11:40:01.297919: step 16000, loss 0.54916.
Train: 2018-08-09T11:40:01.875938: step 16001, loss 0.682166.
Train: 2018-08-09T11:40:01.956321: step 16002, loss 0.490124.
Train: 2018-08-09T11:40:02.034454: step 16003, loss 0.577887.
Train: 2018-08-09T11:40:02.128154: step 16004, loss 0.5776.
Train: 2018-08-09T11:40:02.206261: step 16005, loss 0.458222.
Train: 2018-08-09T11:40:02.284398: step 16006, loss 0.532064.
Train: 2018-08-09T11:40:02.362474: step 16007, loss 0.420463.
Train: 2018-08-09T11:40:02.440614: step 16008, loss 0.56742.
Train: 2018-08-09T11:40:02.534310: step 16009, loss 0.475545.
Train: 2018-08-09T11:40:02.612450: step 16010, loss 0.456707.
Test: 2018-08-09T11:40:03.111766: step 16010, loss 0.545751.
Train: 2018-08-09T11:40:03.180397: step 16011, loss 0.677836.
Train: 2018-08-09T11:40:03.274102: step 16012, loss 0.490924.
Train: 2018-08-09T11:40:03.352238: step 16013, loss 0.531593.
Train: 2018-08-09T11:40:03.430344: step 16014, loss 0.547944.
Train: 2018-08-09T11:40:03.508420: step 16015, loss 0.647689.
Train: 2018-08-09T11:40:03.586561: step 16016, loss 0.650013.
Train: 2018-08-09T11:40:03.680285: step 16017, loss 0.509131.
Train: 2018-08-09T11:40:03.758392: step 16018, loss 0.545563.
Train: 2018-08-09T11:40:03.836498: step 16019, loss 0.524706.
Train: 2018-08-09T11:40:03.914605: step 16020, loss 0.574486.
Test: 2018-08-09T11:40:04.414490: step 16020, loss 0.545774.
Train: 2018-08-09T11:40:04.492597: step 16021, loss 0.564341.
Train: 2018-08-09T11:40:04.570726: step 16022, loss 0.53167.
Train: 2018-08-09T11:40:04.648809: step 16023, loss 0.49826.
Train: 2018-08-09T11:40:04.742537: step 16024, loss 0.52861.
Train: 2018-08-09T11:40:04.820641: step 16025, loss 0.594922.
Train: 2018-08-09T11:40:04.898743: step 16026, loss 0.513443.
Train: 2018-08-09T11:40:04.976855: step 16027, loss 0.545172.
Train: 2018-08-09T11:40:05.054961: step 16028, loss 0.628503.
Train: 2018-08-09T11:40:05.133038: step 16029, loss 0.561198.
Train: 2018-08-09T11:40:05.226797: step 16030, loss 0.578964.
Test: 2018-08-09T11:40:05.726649: step 16030, loss 0.548326.
Train: 2018-08-09T11:40:05.796830: step 16031, loss 0.611609.
Train: 2018-08-09T11:40:05.890528: step 16032, loss 0.548925.
Train: 2018-08-09T11:40:05.962220: step 16033, loss 0.549044.
Train: 2018-08-09T11:40:06.040328: step 16034, loss 0.562093.
Train: 2018-08-09T11:40:06.118432: step 16035, loss 0.511281.
Train: 2018-08-09T11:40:06.212161: step 16036, loss 0.71198.
Train: 2018-08-09T11:40:06.290270: step 16037, loss 0.513059.
Train: 2018-08-09T11:40:06.368373: step 16038, loss 0.613017.
Train: 2018-08-09T11:40:06.446481: step 16039, loss 0.627776.
Train: 2018-08-09T11:40:06.540202: step 16040, loss 0.546787.
Test: 2018-08-09T11:40:07.024451: step 16040, loss 0.550619.
Train: 2018-08-09T11:40:07.118167: step 16041, loss 0.578953.
Train: 2018-08-09T11:40:07.196303: step 16042, loss 0.562077.
Train: 2018-08-09T11:40:07.274381: step 16043, loss 0.594775.
Train: 2018-08-09T11:40:07.368108: step 16044, loss 0.643282.
Train: 2018-08-09T11:40:07.446244: step 16045, loss 0.499307.
Train: 2018-08-09T11:40:07.524348: step 16046, loss 0.499006.
Train: 2018-08-09T11:40:07.602428: step 16047, loss 0.642339.
Train: 2018-08-09T11:40:07.696179: step 16048, loss 0.626376.
Train: 2018-08-09T11:40:07.774292: step 16049, loss 0.61017.
Train: 2018-08-09T11:40:07.852400: step 16050, loss 0.531448.
Test: 2018-08-09T11:40:08.343250: step 16050, loss 0.551943.
Train: 2018-08-09T11:40:08.421357: step 16051, loss 0.53138.
Train: 2018-08-09T11:40:08.515055: step 16052, loss 0.531712.
Train: 2018-08-09T11:40:08.593192: step 16053, loss 0.594242.
Train: 2018-08-09T11:40:08.671299: step 16054, loss 0.610197.
Train: 2018-08-09T11:40:08.749406: step 16055, loss 0.610232.
Train: 2018-08-09T11:40:08.843133: step 16056, loss 0.641696.
Train: 2018-08-09T11:40:08.916941: step 16057, loss 0.641691.
Train: 2018-08-09T11:40:08.995079: step 16058, loss 0.656376.
Train: 2018-08-09T11:40:09.073188: step 16059, loss 0.593784.
Train: 2018-08-09T11:40:09.166914: step 16060, loss 0.610039.
Test: 2018-08-09T11:40:09.651144: step 16060, loss 0.551044.
Train: 2018-08-09T11:40:09.744902: step 16061, loss 0.456703.
Train: 2018-08-09T11:40:09.822979: step 16062, loss 0.655848.
Train: 2018-08-09T11:40:09.901118: step 16063, loss 0.594484.
Train: 2018-08-09T11:40:09.979222: step 16064, loss 0.579354.
Train: 2018-08-09T11:40:10.057330: step 16065, loss 0.488228.
Train: 2018-08-09T11:40:10.151027: step 16066, loss 0.548929.
Train: 2018-08-09T11:40:10.229133: step 16067, loss 0.563632.
Train: 2018-08-09T11:40:10.307270: step 16068, loss 0.473482.
Train: 2018-08-09T11:40:10.385378: step 16069, loss 0.579143.
Train: 2018-08-09T11:40:10.463483: step 16070, loss 0.624789.
Test: 2018-08-09T11:40:10.963336: step 16070, loss 0.552025.
Train: 2018-08-09T11:40:11.041471: step 16071, loss 0.503447.
Train: 2018-08-09T11:40:11.135194: step 16072, loss 0.457573.
Train: 2018-08-09T11:40:11.213307: step 16073, loss 0.578759.
Train: 2018-08-09T11:40:11.291412: step 16074, loss 0.51801.
Train: 2018-08-09T11:40:11.369519: step 16075, loss 0.548203.
Train: 2018-08-09T11:40:11.447624: step 16076, loss 0.547748.
Train: 2018-08-09T11:40:11.525702: step 16077, loss 0.547773.
Train: 2018-08-09T11:40:11.619430: step 16078, loss 0.593972.
Train: 2018-08-09T11:40:11.697566: step 16079, loss 0.594265.
Train: 2018-08-09T11:40:11.775644: step 16080, loss 0.54751.
Test: 2018-08-09T11:40:12.277916: step 16080, loss 0.549514.
Train: 2018-08-09T11:40:12.356047: step 16081, loss 0.500416.
Train: 2018-08-09T11:40:12.434123: step 16082, loss 0.642529.
Train: 2018-08-09T11:40:12.527852: step 16083, loss 0.4678.
Train: 2018-08-09T11:40:12.605958: step 16084, loss 0.515276.
Train: 2018-08-09T11:40:12.684063: step 16085, loss 0.530817.
Train: 2018-08-09T11:40:12.762202: step 16086, loss 0.497356.
Train: 2018-08-09T11:40:12.840307: step 16087, loss 0.593894.
Train: 2018-08-09T11:40:12.918414: step 16088, loss 0.628625.
Train: 2018-08-09T11:40:12.996522: step 16089, loss 0.595729.
Train: 2018-08-09T11:40:13.090248: step 16090, loss 0.594649.
Test: 2018-08-09T11:40:13.574505: step 16090, loss 0.548976.
Train: 2018-08-09T11:40:13.668208: step 16091, loss 0.611045.
Train: 2018-08-09T11:40:13.746348: step 16092, loss 0.629213.
Train: 2018-08-09T11:40:13.824453: step 16093, loss 0.594989.
Train: 2018-08-09T11:40:13.902528: step 16094, loss 0.613666.
Train: 2018-08-09T11:40:13.983124: step 16095, loss 0.545963.
Train: 2018-08-09T11:40:14.076854: step 16096, loss 0.594319.
Train: 2018-08-09T11:40:14.154959: step 16097, loss 0.52956.
Train: 2018-08-09T11:40:14.233066: step 16098, loss 0.562532.
Train: 2018-08-09T11:40:14.311174: step 16099, loss 0.593797.
Train: 2018-08-09T11:40:14.389279: step 16100, loss 0.513891.
Test: 2018-08-09T11:40:14.889156: step 16100, loss 0.550669.
Train: 2018-08-09T11:40:15.467150: step 16101, loss 0.467148.
Train: 2018-08-09T11:40:15.545259: step 16102, loss 0.611361.
Train: 2018-08-09T11:40:15.638986: step 16103, loss 0.594409.
Train: 2018-08-09T11:40:15.717062: step 16104, loss 0.577547.
Train: 2018-08-09T11:40:15.795169: step 16105, loss 0.514987.
Train: 2018-08-09T11:40:15.873276: step 16106, loss 0.61082.
Train: 2018-08-09T11:40:15.967663: step 16107, loss 0.465492.
Train: 2018-08-09T11:40:16.045769: step 16108, loss 0.547883.
Train: 2018-08-09T11:40:16.123908: step 16109, loss 0.480025.
Train: 2018-08-09T11:40:16.201982: step 16110, loss 0.593172.
Test: 2018-08-09T11:40:16.701882: step 16110, loss 0.5484.
Train: 2018-08-09T11:40:16.780002: step 16111, loss 0.609869.
Train: 2018-08-09T11:40:16.858078: step 16112, loss 0.609389.
Train: 2018-08-09T11:40:16.936212: step 16113, loss 0.514328.
Train: 2018-08-09T11:40:17.029937: step 16114, loss 0.547251.
Train: 2018-08-09T11:40:17.108051: step 16115, loss 0.646328.
Train: 2018-08-09T11:40:17.186158: step 16116, loss 0.742585.
Train: 2018-08-09T11:40:17.264257: step 16117, loss 0.594135.
Train: 2018-08-09T11:40:17.342339: step 16118, loss 0.562546.
Train: 2018-08-09T11:40:17.420473: step 16119, loss 0.579173.
Train: 2018-08-09T11:40:17.514203: step 16120, loss 0.693933.
Test: 2018-08-09T11:40:18.014678: step 16120, loss 0.546982.
Train: 2018-08-09T11:40:18.092816: step 16121, loss 0.481875.
Train: 2018-08-09T11:40:18.170920: step 16122, loss 0.547466.
Train: 2018-08-09T11:40:18.248999: step 16123, loss 0.62541.
Train: 2018-08-09T11:40:18.327134: step 16124, loss 0.435154.
Train: 2018-08-09T11:40:18.405236: step 16125, loss 0.564236.
Train: 2018-08-09T11:40:18.498970: step 16126, loss 0.613241.
Train: 2018-08-09T11:40:18.577074: step 16127, loss 0.593836.
Train: 2018-08-09T11:40:18.655150: step 16128, loss 0.563127.
Train: 2018-08-09T11:40:18.733289: step 16129, loss 0.65778.
Train: 2018-08-09T11:40:18.811364: step 16130, loss 0.613269.
Test: 2018-08-09T11:40:19.311247: step 16130, loss 0.550847.
Train: 2018-08-09T11:40:19.389354: step 16131, loss 0.516944.
Train: 2018-08-09T11:40:19.467459: step 16132, loss 0.517246.
Train: 2018-08-09T11:40:19.561220: step 16133, loss 0.594722.
Train: 2018-08-09T11:40:19.639325: step 16134, loss 0.610745.
Train: 2018-08-09T11:40:19.717402: step 16135, loss 0.563165.
Train: 2018-08-09T11:40:19.795539: step 16136, loss 0.501657.
Train: 2018-08-09T11:40:19.873644: step 16137, loss 0.547245.
Train: 2018-08-09T11:40:19.951721: step 16138, loss 0.563116.
Train: 2018-08-09T11:40:20.045479: step 16139, loss 0.609902.
Train: 2018-08-09T11:40:20.123585: step 16140, loss 0.470699.
Test: 2018-08-09T11:40:20.623438: step 16140, loss 0.550029.
Train: 2018-08-09T11:40:20.701575: step 16141, loss 0.485995.
Train: 2018-08-09T11:40:20.779686: step 16142, loss 0.45387.
Train: 2018-08-09T11:40:20.857788: step 16143, loss 0.563265.
Train: 2018-08-09T11:40:20.938170: step 16144, loss 0.563452.
Train: 2018-08-09T11:40:21.016276: step 16145, loss 0.528674.
Train: 2018-08-09T11:40:21.110006: step 16146, loss 0.611507.
Train: 2018-08-09T11:40:21.188082: step 16147, loss 0.593695.
Train: 2018-08-09T11:40:21.266189: step 16148, loss 0.478257.
Train: 2018-08-09T11:40:21.344296: step 16149, loss 0.613442.
Train: 2018-08-09T11:40:21.422426: step 16150, loss 0.530829.
Test: 2018-08-09T11:40:21.922314: step 16150, loss 0.54739.
Train: 2018-08-09T11:40:22.016036: step 16151, loss 0.508785.
Train: 2018-08-09T11:40:22.094118: step 16152, loss 0.545116.
Train: 2018-08-09T11:40:22.172224: step 16153, loss 0.469241.
Train: 2018-08-09T11:40:22.250331: step 16154, loss 0.525666.
Train: 2018-08-09T11:40:22.328467: step 16155, loss 0.495475.
Train: 2018-08-09T11:40:22.406546: step 16156, loss 0.568225.
Train: 2018-08-09T11:40:22.500273: step 16157, loss 0.637111.
Train: 2018-08-09T11:40:22.578409: step 16158, loss 0.507885.
Train: 2018-08-09T11:40:22.656517: step 16159, loss 0.530788.
Train: 2018-08-09T11:40:22.734591: step 16160, loss 0.636985.
Test: 2018-08-09T11:40:23.235033: step 16160, loss 0.545732.
Train: 2018-08-09T11:40:23.313137: step 16161, loss 0.560118.
Train: 2018-08-09T11:40:23.391214: step 16162, loss 0.495732.
Train: 2018-08-09T11:40:23.469347: step 16163, loss 0.598576.
Train: 2018-08-09T11:40:23.547458: step 16164, loss 0.563823.
Train: 2018-08-09T11:40:23.641180: step 16165, loss 0.562103.
Train: 2018-08-09T11:40:23.719293: step 16166, loss 0.611405.
Train: 2018-08-09T11:40:23.797394: step 16167, loss 0.530305.
Train: 2018-08-09T11:40:23.875475: step 16168, loss 0.530224.
Train: 2018-08-09T11:40:23.953583: step 16169, loss 0.512929.
Train: 2018-08-09T11:40:24.031720: step 16170, loss 0.547107.
Test: 2018-08-09T11:40:24.531572: step 16170, loss 0.547697.
Train: 2018-08-09T11:40:24.609702: step 16171, loss 0.462572.
Train: 2018-08-09T11:40:24.703438: step 16172, loss 0.577573.
Train: 2018-08-09T11:40:24.781513: step 16173, loss 0.460311.
Train: 2018-08-09T11:40:24.859618: step 16174, loss 0.47844.
Train: 2018-08-09T11:40:24.940043: step 16175, loss 0.510121.
Train: 2018-08-09T11:40:25.018149: step 16176, loss 0.564985.
Train: 2018-08-09T11:40:25.096258: step 16177, loss 0.709222.
Train: 2018-08-09T11:40:25.174362: step 16178, loss 0.68808.
Train: 2018-08-09T11:40:25.268059: step 16179, loss 0.51087.
Train: 2018-08-09T11:40:25.346196: step 16180, loss 0.460215.
Test: 2018-08-09T11:40:25.846049: step 16180, loss 0.548298.
Train: 2018-08-09T11:40:25.924179: step 16181, loss 0.630586.
Train: 2018-08-09T11:40:26.002295: step 16182, loss 0.561384.
Train: 2018-08-09T11:40:26.080399: step 16183, loss 0.494372.
Train: 2018-08-09T11:40:26.174120: step 16184, loss 0.51535.
Train: 2018-08-09T11:40:26.252203: step 16185, loss 0.510905.
Train: 2018-08-09T11:40:26.330339: step 16186, loss 0.634577.
Train: 2018-08-09T11:40:26.408446: step 16187, loss 0.47978.
Train: 2018-08-09T11:40:26.486552: step 16188, loss 0.49416.
Train: 2018-08-09T11:40:26.564630: step 16189, loss 0.594453.
Train: 2018-08-09T11:40:26.658358: step 16190, loss 0.578679.
Test: 2018-08-09T11:40:27.143988: step 16190, loss 0.547234.
Train: 2018-08-09T11:40:27.237691: step 16191, loss 0.5626.
Train: 2018-08-09T11:40:27.315829: step 16192, loss 0.613075.
Train: 2018-08-09T11:40:27.393904: step 16193, loss 0.511478.
Train: 2018-08-09T11:40:27.472010: step 16194, loss 0.530172.
Train: 2018-08-09T11:40:27.550146: step 16195, loss 0.428099.
Train: 2018-08-09T11:40:27.643875: step 16196, loss 0.648975.
Train: 2018-08-09T11:40:27.721952: step 16197, loss 0.561487.
Train: 2018-08-09T11:40:27.800058: step 16198, loss 0.579115.
Train: 2018-08-09T11:40:27.878165: step 16199, loss 0.52641.
Train: 2018-08-09T11:40:27.971892: step 16200, loss 0.52995.
Test: 2018-08-09T11:40:28.471807: step 16200, loss 0.548941.
Train: 2018-08-09T11:40:29.035739: step 16201, loss 0.580399.
Train: 2018-08-09T11:40:29.113845: step 16202, loss 0.631676.
Train: 2018-08-09T11:40:29.191954: step 16203, loss 0.61442.
Train: 2018-08-09T11:40:29.270059: step 16204, loss 0.695573.
Train: 2018-08-09T11:40:29.348168: step 16205, loss 0.578555.
Train: 2018-08-09T11:40:29.441893: step 16206, loss 0.713391.
Train: 2018-08-09T11:40:29.520000: step 16207, loss 0.579275.
Train: 2018-08-09T11:40:29.598110: step 16208, loss 0.545858.
Train: 2018-08-09T11:40:29.676184: step 16209, loss 0.59492.
Train: 2018-08-09T11:40:29.754289: step 16210, loss 0.659347.
Test: 2018-08-09T11:40:30.254173: step 16210, loss 0.550312.
Train: 2018-08-09T11:40:30.332310: step 16211, loss 0.595103.
Train: 2018-08-09T11:40:30.410384: step 16212, loss 0.642445.
Train: 2018-08-09T11:40:30.504143: step 16213, loss 0.5471.
Train: 2018-08-09T11:40:30.582250: step 16214, loss 0.578767.
Train: 2018-08-09T11:40:30.660356: step 16215, loss 0.532291.
Train: 2018-08-09T11:40:30.738464: step 16216, loss 0.719071.
Train: 2018-08-09T11:40:30.816573: step 16217, loss 0.517072.
Train: 2018-08-09T11:40:30.911612: step 16218, loss 0.686604.
Train: 2018-08-09T11:40:30.989719: step 16219, loss 0.548381.
Train: 2018-08-09T11:40:31.067857: step 16220, loss 0.563675.
Test: 2018-08-09T11:40:31.567710: step 16220, loss 0.551435.
Train: 2018-08-09T11:40:31.645844: step 16221, loss 0.579112.
Train: 2018-08-09T11:40:31.723922: step 16222, loss 0.639422.
Train: 2018-08-09T11:40:31.802029: step 16223, loss 0.534096.
Train: 2018-08-09T11:40:31.880134: step 16224, loss 0.624154.
Train: 2018-08-09T11:40:31.973863: step 16225, loss 0.5046.
Train: 2018-08-09T11:40:32.052000: step 16226, loss 0.549442.
Train: 2018-08-09T11:40:32.130105: step 16227, loss 0.56432.
Train: 2018-08-09T11:40:32.208212: step 16228, loss 0.519721.
Train: 2018-08-09T11:40:32.286322: step 16229, loss 0.549419.
Train: 2018-08-09T11:40:32.380045: step 16230, loss 0.579179.
Test: 2018-08-09T11:40:32.864304: step 16230, loss 0.550712.
Train: 2018-08-09T11:40:32.943771: step 16231, loss 0.54924.
Train: 2018-08-09T11:40:33.021873: step 16232, loss 0.518973.
Train: 2018-08-09T11:40:33.115597: step 16233, loss 0.609864.
Train: 2018-08-09T11:40:33.193676: step 16234, loss 0.518366.
Train: 2018-08-09T11:40:33.271815: step 16235, loss 0.548084.
Train: 2018-08-09T11:40:33.349921: step 16236, loss 0.532496.
Train: 2018-08-09T11:40:33.428029: step 16237, loss 0.563591.
Train: 2018-08-09T11:40:33.521725: step 16238, loss 0.484199.
Train: 2018-08-09T11:40:33.599831: step 16239, loss 0.581248.
Train: 2018-08-09T11:40:33.677970: step 16240, loss 0.546394.
Test: 2018-08-09T11:40:34.167846: step 16240, loss 0.546561.
Train: 2018-08-09T11:40:34.245976: step 16241, loss 0.611989.
Train: 2018-08-09T11:40:34.339678: step 16242, loss 0.592507.
Train: 2018-08-09T11:40:34.417786: step 16243, loss 0.531895.
Train: 2018-08-09T11:40:34.495922: step 16244, loss 0.669856.
Train: 2018-08-09T11:40:34.574029: step 16245, loss 0.48899.
Train: 2018-08-09T11:40:34.652134: step 16246, loss 0.607769.
Train: 2018-08-09T11:40:34.730212: step 16247, loss 0.528232.
Train: 2018-08-09T11:40:34.823970: step 16248, loss 0.596024.
Train: 2018-08-09T11:40:34.902076: step 16249, loss 0.543859.
Train: 2018-08-09T11:40:34.980184: step 16250, loss 0.647858.
Test: 2018-08-09T11:40:35.480067: step 16250, loss 0.550531.
Train: 2018-08-09T11:40:35.558142: step 16251, loss 0.563977.
Train: 2018-08-09T11:40:35.636279: step 16252, loss 0.563036.
Train: 2018-08-09T11:40:35.714381: step 16253, loss 0.636764.
Train: 2018-08-09T11:40:35.792493: step 16254, loss 0.512744.
Train: 2018-08-09T11:40:35.870568: step 16255, loss 0.518369.
Train: 2018-08-09T11:40:35.948707: step 16256, loss 0.592674.
Train: 2018-08-09T11:40:36.042434: step 16257, loss 0.562062.
Train: 2018-08-09T11:40:36.120543: step 16258, loss 0.594537.
Train: 2018-08-09T11:40:36.198647: step 16259, loss 0.580731.
Train: 2018-08-09T11:40:36.276755: step 16260, loss 0.562556.
Test: 2018-08-09T11:40:36.776619: step 16260, loss 0.551536.
Train: 2018-08-09T11:40:36.854711: step 16261, loss 0.593825.
Train: 2018-08-09T11:40:36.935159: step 16262, loss 0.563286.
Train: 2018-08-09T11:40:37.013267: step 16263, loss 0.516669.
Train: 2018-08-09T11:40:37.107023: step 16264, loss 0.658897.
Train: 2018-08-09T11:40:37.185132: step 16265, loss 0.595039.
Train: 2018-08-09T11:40:37.263237: step 16266, loss 0.547499.
Train: 2018-08-09T11:40:37.341312: step 16267, loss 0.516748.
Train: 2018-08-09T11:40:37.419449: step 16268, loss 0.548925.
Train: 2018-08-09T11:40:37.513179: step 16269, loss 0.578819.
Train: 2018-08-09T11:40:37.591284: step 16270, loss 0.53325.
Test: 2018-08-09T11:40:38.084124: step 16270, loss 0.549544.
Train: 2018-08-09T11:40:38.162230: step 16271, loss 0.577643.
Train: 2018-08-09T11:40:38.240367: step 16272, loss 0.530798.
Train: 2018-08-09T11:40:38.334064: step 16273, loss 0.547339.
Train: 2018-08-09T11:40:38.412170: step 16274, loss 0.51548.
Train: 2018-08-09T11:40:38.490276: step 16275, loss 0.593865.
Train: 2018-08-09T11:40:38.568416: step 16276, loss 0.527398.
Train: 2018-08-09T11:40:38.646517: step 16277, loss 0.49747.
Train: 2018-08-09T11:40:38.724598: step 16278, loss 0.574467.
Train: 2018-08-09T11:40:38.818324: step 16279, loss 0.605501.
Train: 2018-08-09T11:40:38.896431: step 16280, loss 0.556556.
Test: 2018-08-09T11:40:39.388989: step 16280, loss 0.547871.
Train: 2018-08-09T11:40:39.467125: step 16281, loss 0.569059.
Train: 2018-08-09T11:40:39.545201: step 16282, loss 0.579821.
Train: 2018-08-09T11:40:39.623334: step 16283, loss 0.596049.
Train: 2018-08-09T11:40:39.717069: step 16284, loss 0.675305.
Train: 2018-08-09T11:40:39.795173: step 16285, loss 0.516758.
Train: 2018-08-09T11:40:39.873280: step 16286, loss 0.577767.
Train: 2018-08-09T11:40:39.951355: step 16287, loss 0.527727.
Train: 2018-08-09T11:40:40.029495: step 16288, loss 0.645126.
Train: 2018-08-09T11:40:40.107601: step 16289, loss 0.597172.
Train: 2018-08-09T11:40:40.201329: step 16290, loss 0.479579.
Test: 2018-08-09T11:40:40.693323: step 16290, loss 0.551343.
Train: 2018-08-09T11:40:40.771457: step 16291, loss 0.582746.
Train: 2018-08-09T11:40:40.849565: step 16292, loss 0.588898.
Train: 2018-08-09T11:40:40.943294: step 16293, loss 0.54544.
Train: 2018-08-09T11:40:41.021398: step 16294, loss 0.58381.
Train: 2018-08-09T11:40:41.099506: step 16295, loss 0.564736.
Train: 2018-08-09T11:40:41.193228: step 16296, loss 0.479135.
Train: 2018-08-09T11:40:41.271340: step 16297, loss 0.542565.
Train: 2018-08-09T11:40:41.349417: step 16298, loss 0.497296.
Train: 2018-08-09T11:40:41.427523: step 16299, loss 0.612563.
Train: 2018-08-09T11:40:41.505661: step 16300, loss 0.699338.
Test: 2018-08-09T11:40:42.005512: step 16300, loss 0.549033.
Train: 2018-08-09T11:40:42.599152: step 16301, loss 0.546583.
Train: 2018-08-09T11:40:42.677259: step 16302, loss 0.579513.
Train: 2018-08-09T11:40:42.755363: step 16303, loss 0.576229.
Train: 2018-08-09T11:40:42.833442: step 16304, loss 0.593377.
Train: 2018-08-09T11:40:42.927170: step 16305, loss 0.46676.
Train: 2018-08-09T11:40:43.005307: step 16306, loss 0.5347.
Train: 2018-08-09T11:40:43.083413: step 16307, loss 0.536133.
Train: 2018-08-09T11:40:43.161520: step 16308, loss 0.500174.
Train: 2018-08-09T11:40:43.239627: step 16309, loss 0.579404.
Train: 2018-08-09T11:40:43.333350: step 16310, loss 0.574757.
Test: 2018-08-09T11:40:43.817618: step 16310, loss 0.551432.
Train: 2018-08-09T11:40:43.895691: step 16311, loss 0.504926.
Train: 2018-08-09T11:40:43.976155: step 16312, loss 0.546586.
Train: 2018-08-09T11:40:44.069914: step 16313, loss 0.529858.
Train: 2018-08-09T11:40:44.148020: step 16314, loss 0.580143.
Train: 2018-08-09T11:40:44.226126: step 16315, loss 0.614277.
Train: 2018-08-09T11:40:44.304236: step 16316, loss 0.596682.
Train: 2018-08-09T11:40:44.382311: step 16317, loss 0.613142.
Train: 2018-08-09T11:40:44.476068: step 16318, loss 0.59571.
Train: 2018-08-09T11:40:44.554175: step 16319, loss 0.467356.
Train: 2018-08-09T11:40:44.632282: step 16320, loss 0.594187.
Test: 2018-08-09T11:40:45.132142: step 16320, loss 0.547437.
Train: 2018-08-09T11:40:45.210270: step 16321, loss 0.56833.
Train: 2018-08-09T11:40:45.288378: step 16322, loss 0.561046.
Train: 2018-08-09T11:40:45.366485: step 16323, loss 0.547688.
Train: 2018-08-09T11:40:45.460211: step 16324, loss 0.578482.
Train: 2018-08-09T11:40:45.538288: step 16325, loss 0.594846.
Train: 2018-08-09T11:40:45.616425: step 16326, loss 0.579022.
Train: 2018-08-09T11:40:45.694531: step 16327, loss 0.546756.
Train: 2018-08-09T11:40:45.772637: step 16328, loss 0.594265.
Train: 2018-08-09T11:40:45.866366: step 16329, loss 0.625081.
Train: 2018-08-09T11:40:45.946719: step 16330, loss 0.547489.
Test: 2018-08-09T11:40:46.430947: step 16330, loss 0.550659.
Train: 2018-08-09T11:40:46.509082: step 16331, loss 0.531068.
Train: 2018-08-09T11:40:46.602811: step 16332, loss 0.564923.
Train: 2018-08-09T11:40:46.680888: step 16333, loss 0.529965.
Train: 2018-08-09T11:40:46.759024: step 16334, loss 0.547437.
Train: 2018-08-09T11:40:46.837134: step 16335, loss 0.466644.
Train: 2018-08-09T11:40:46.915233: step 16336, loss 0.527551.
Train: 2018-08-09T11:40:47.008934: step 16337, loss 0.498438.
Train: 2018-08-09T11:40:47.087040: step 16338, loss 0.597652.
Train: 2018-08-09T11:40:47.165147: step 16339, loss 0.57577.
Train: 2018-08-09T11:40:47.243253: step 16340, loss 0.582374.
Test: 2018-08-09T11:40:47.743167: step 16340, loss 0.549465.
Train: 2018-08-09T11:40:47.821268: step 16341, loss 0.493557.
Train: 2018-08-09T11:40:47.899380: step 16342, loss 0.581702.
Train: 2018-08-09T11:40:47.995490: step 16343, loss 0.528457.
Train: 2018-08-09T11:40:48.073601: step 16344, loss 0.534615.
Train: 2018-08-09T11:40:48.151708: step 16345, loss 0.562315.
Train: 2018-08-09T11:40:48.229815: step 16346, loss 0.474727.
Train: 2018-08-09T11:40:48.307921: step 16347, loss 0.581998.
Train: 2018-08-09T11:40:48.401643: step 16348, loss 0.637094.
Train: 2018-08-09T11:40:48.479757: step 16349, loss 0.598922.
Train: 2018-08-09T11:40:48.557864: step 16350, loss 0.419243.
Test: 2018-08-09T11:40:49.057715: step 16350, loss 0.548038.
Train: 2018-08-09T11:40:49.135850: step 16351, loss 0.521134.
Train: 2018-08-09T11:40:49.213956: step 16352, loss 0.558721.
Train: 2018-08-09T11:40:49.292060: step 16353, loss 0.6498.
Train: 2018-08-09T11:40:49.370172: step 16354, loss 0.547335.
Train: 2018-08-09T11:40:49.463869: step 16355, loss 0.587834.
Train: 2018-08-09T11:40:49.541975: step 16356, loss 0.559177.
Train: 2018-08-09T11:40:49.620083: step 16357, loss 0.545088.
Train: 2018-08-09T11:40:49.698218: step 16358, loss 0.430561.
Train: 2018-08-09T11:40:49.776326: step 16359, loss 0.477342.
Train: 2018-08-09T11:40:49.870052: step 16360, loss 0.473028.
Test: 2018-08-09T11:40:50.358290: step 16360, loss 0.546387.
Train: 2018-08-09T11:40:50.436396: step 16361, loss 0.633644.
Train: 2018-08-09T11:40:50.530133: step 16362, loss 0.519012.
Train: 2018-08-09T11:40:50.608237: step 16363, loss 0.502554.
Train: 2018-08-09T11:40:50.686312: step 16364, loss 0.586484.
Train: 2018-08-09T11:40:50.764450: step 16365, loss 0.533709.
Train: 2018-08-09T11:40:50.842559: step 16366, loss 0.528098.
Train: 2018-08-09T11:40:50.936288: step 16367, loss 0.434279.
Train: 2018-08-09T11:40:51.014390: step 16368, loss 0.659921.
Train: 2018-08-09T11:40:51.092498: step 16369, loss 0.663841.
Train: 2018-08-09T11:40:51.170604: step 16370, loss 0.451423.
Test: 2018-08-09T11:40:51.670457: step 16370, loss 0.549138.
Train: 2018-08-09T11:40:51.748587: step 16371, loss 0.631578.
Train: 2018-08-09T11:40:51.826699: step 16372, loss 0.564002.
Train: 2018-08-09T11:40:51.920081: step 16373, loss 0.550833.
Train: 2018-08-09T11:40:51.998174: step 16374, loss 0.514574.
Train: 2018-08-09T11:40:52.076311: step 16375, loss 0.546931.
Train: 2018-08-09T11:40:52.154387: step 16376, loss 0.545046.
Train: 2018-08-09T11:40:52.232524: step 16377, loss 0.563582.
Train: 2018-08-09T11:40:52.326248: step 16378, loss 0.430822.
Train: 2018-08-09T11:40:52.404361: step 16379, loss 0.565002.
Train: 2018-08-09T11:40:52.482436: step 16380, loss 0.610538.
Test: 2018-08-09T11:40:52.982319: step 16380, loss 0.550765.
Train: 2018-08-09T11:40:53.060455: step 16381, loss 0.631297.
Train: 2018-08-09T11:40:53.138555: step 16382, loss 0.530683.
Train: 2018-08-09T11:40:53.232260: step 16383, loss 0.59553.
Train: 2018-08-09T11:40:53.310396: step 16384, loss 0.659736.
Train: 2018-08-09T11:40:53.388502: step 16385, loss 0.627016.
Train: 2018-08-09T11:40:53.466578: step 16386, loss 0.6273.
Train: 2018-08-09T11:40:53.544717: step 16387, loss 0.546885.
Train: 2018-08-09T11:40:53.638443: step 16388, loss 0.483682.
Train: 2018-08-09T11:40:53.716549: step 16389, loss 0.578719.
Train: 2018-08-09T11:40:53.794653: step 16390, loss 0.579123.
Test: 2018-08-09T11:40:54.294511: step 16390, loss 0.549445.
Train: 2018-08-09T11:40:54.372614: step 16391, loss 0.674119.
Train: 2018-08-09T11:40:54.466373: step 16392, loss 0.578909.
Train: 2018-08-09T11:40:54.544480: step 16393, loss 0.563096.
Train: 2018-08-09T11:40:54.622555: step 16394, loss 0.531745.
Train: 2018-08-09T11:40:54.700692: step 16395, loss 0.563082.
Train: 2018-08-09T11:40:54.778770: step 16396, loss 0.469616.
Train: 2018-08-09T11:40:54.872528: step 16397, loss 0.735402.
Train: 2018-08-09T11:40:54.950604: step 16398, loss 0.532173.
Train: 2018-08-09T11:40:55.028743: step 16399, loss 0.547752.
Train: 2018-08-09T11:40:55.106816: step 16400, loss 0.563495.
Test: 2018-08-09T11:40:55.606725: step 16400, loss 0.551139.
Train: 2018-08-09T11:40:56.189195: step 16401, loss 0.563238.
Train: 2018-08-09T11:40:56.267332: step 16402, loss 0.501171.
Train: 2018-08-09T11:40:56.345435: step 16403, loss 0.563232.
Train: 2018-08-09T11:40:56.423546: step 16404, loss 0.453769.
Train: 2018-08-09T11:40:56.501653: step 16405, loss 0.515807.
Train: 2018-08-09T11:40:56.595375: step 16406, loss 0.467206.
Train: 2018-08-09T11:40:56.673458: step 16407, loss 0.564561.
Train: 2018-08-09T11:40:56.751593: step 16408, loss 0.530578.
Train: 2018-08-09T11:40:56.829671: step 16409, loss 0.551727.
Train: 2018-08-09T11:40:56.919981: step 16410, loss 0.545914.
Test: 2018-08-09T11:40:57.419864: step 16410, loss 0.547843.
Train: 2018-08-09T11:40:57.498000: step 16411, loss 0.611293.
Train: 2018-08-09T11:40:57.576106: step 16412, loss 0.594975.
Train: 2018-08-09T11:40:57.654209: step 16413, loss 0.613612.
Train: 2018-08-09T11:40:57.747941: step 16414, loss 0.611434.
Train: 2018-08-09T11:40:57.826052: step 16415, loss 0.578882.
Train: 2018-08-09T11:40:57.904155: step 16416, loss 0.514564.
Train: 2018-08-09T11:40:57.982257: step 16417, loss 0.528721.
Train: 2018-08-09T11:40:58.075988: step 16418, loss 0.480441.
Train: 2018-08-09T11:40:58.154096: step 16419, loss 0.663911.
Train: 2018-08-09T11:40:58.232203: step 16420, loss 0.544525.
Test: 2018-08-09T11:40:58.732056: step 16420, loss 0.545831.
Train: 2018-08-09T11:40:58.810191: step 16421, loss 0.545113.
Train: 2018-08-09T11:40:58.888267: step 16422, loss 0.679492.
Train: 2018-08-09T11:40:58.966400: step 16423, loss 0.497168.
Train: 2018-08-09T11:40:59.060132: step 16424, loss 0.562926.
Train: 2018-08-09T11:40:59.138239: step 16425, loss 0.560974.
Train: 2018-08-09T11:40:59.216347: step 16426, loss 0.513954.
Train: 2018-08-09T11:40:59.294421: step 16427, loss 0.646557.
Train: 2018-08-09T11:40:59.388179: step 16428, loss 0.627445.
Train: 2018-08-09T11:40:59.466257: step 16429, loss 0.594027.
Train: 2018-08-09T11:40:59.544363: step 16430, loss 0.547808.
Test: 2018-08-09T11:41:00.046567: step 16430, loss 0.548707.
Train: 2018-08-09T11:41:00.124674: step 16431, loss 0.57804.
Train: 2018-08-09T11:41:00.202748: step 16432, loss 0.54702.
Train: 2018-08-09T11:41:00.280858: step 16433, loss 0.546209.
Train: 2018-08-09T11:41:00.374584: step 16434, loss 0.497414.
Train: 2018-08-09T11:41:00.452691: step 16435, loss 0.626938.
Train: 2018-08-09T11:41:00.530827: step 16436, loss 0.481951.
Train: 2018-08-09T11:41:00.608903: step 16437, loss 0.46552.
Train: 2018-08-09T11:41:00.687040: step 16438, loss 0.529717.
Train: 2018-08-09T11:41:00.780738: step 16439, loss 0.611898.
Train: 2018-08-09T11:41:00.858844: step 16440, loss 0.696945.
Test: 2018-08-09T11:41:01.358728: step 16440, loss 0.550274.
Train: 2018-08-09T11:41:01.436863: step 16441, loss 0.546189.
Train: 2018-08-09T11:41:01.514940: step 16442, loss 0.480133.
Train: 2018-08-09T11:41:01.593073: step 16443, loss 0.563011.
Train: 2018-08-09T11:41:01.686799: step 16444, loss 0.546539.
Train: 2018-08-09T11:41:01.764880: step 16445, loss 0.497706.
Train: 2018-08-09T11:41:01.842989: step 16446, loss 0.547189.
Train: 2018-08-09T11:41:01.921849: step 16447, loss 0.51202.
Train: 2018-08-09T11:41:02.015611: step 16448, loss 0.528.
Train: 2018-08-09T11:41:02.093685: step 16449, loss 0.579299.
Train: 2018-08-09T11:41:02.171823: step 16450, loss 0.578965.
Test: 2018-08-09T11:41:02.671674: step 16450, loss 0.548517.
Train: 2018-08-09T11:41:02.749779: step 16451, loss 0.590978.
Train: 2018-08-09T11:41:02.827887: step 16452, loss 0.611194.
Train: 2018-08-09T11:41:02.921646: step 16453, loss 0.55786.
Train: 2018-08-09T11:41:02.999750: step 16454, loss 0.576236.
Train: 2018-08-09T11:41:03.077828: step 16455, loss 0.508099.
Train: 2018-08-09T11:41:03.155934: step 16456, loss 0.596455.
Train: 2018-08-09T11:41:03.249696: step 16457, loss 0.603946.
Train: 2018-08-09T11:41:03.327799: step 16458, loss 0.506749.
Train: 2018-08-09T11:41:03.405910: step 16459, loss 0.441258.
Train: 2018-08-09T11:41:03.483981: step 16460, loss 0.54276.
Test: 2018-08-09T11:41:03.982429: step 16460, loss 0.547063.
Train: 2018-08-09T11:41:04.076180: step 16461, loss 0.451904.
Train: 2018-08-09T11:41:04.154264: step 16462, loss 0.47304.
Train: 2018-08-09T11:41:04.232371: step 16463, loss 0.564029.
Train: 2018-08-09T11:41:04.310506: step 16464, loss 0.453413.
Train: 2018-08-09T11:41:04.404205: step 16465, loss 0.49286.
Train: 2018-08-09T11:41:04.482345: step 16466, loss 0.589719.
Train: 2018-08-09T11:41:04.560447: step 16467, loss 0.516279.
Train: 2018-08-09T11:41:04.638555: step 16468, loss 0.587209.
Train: 2018-08-09T11:41:04.716661: step 16469, loss 0.474304.
Train: 2018-08-09T11:41:04.810389: step 16470, loss 0.582494.
Test: 2018-08-09T11:41:05.310757: step 16470, loss 0.54751.
Train: 2018-08-09T11:41:05.388894: step 16471, loss 0.594656.
Train: 2018-08-09T11:41:05.467000: step 16472, loss 0.567577.
Train: 2018-08-09T11:41:05.545077: step 16473, loss 0.643683.
Train: 2018-08-09T11:41:05.623214: step 16474, loss 0.690612.
Train: 2018-08-09T11:41:05.716941: step 16475, loss 0.600471.
Train: 2018-08-09T11:41:05.795048: step 16476, loss 0.595833.
Train: 2018-08-09T11:41:05.873155: step 16477, loss 0.63008.
Train: 2018-08-09T11:41:05.951261: step 16478, loss 0.498424.
Train: 2018-08-09T11:41:06.044992: step 16479, loss 0.610233.
Train: 2018-08-09T11:41:06.123066: step 16480, loss 0.561803.
Test: 2018-08-09T11:41:06.622948: step 16480, loss 0.549541.
Train: 2018-08-09T11:41:06.701084: step 16481, loss 0.609363.
Train: 2018-08-09T11:41:06.779195: step 16482, loss 0.62733.
Train: 2018-08-09T11:41:06.872890: step 16483, loss 0.485172.
Train: 2018-08-09T11:41:06.951026: step 16484, loss 0.624505.
Train: 2018-08-09T11:41:07.029104: step 16485, loss 0.501862.
Train: 2018-08-09T11:41:07.122862: step 16486, loss 0.59452.
Train: 2018-08-09T11:41:07.200966: step 16487, loss 0.486191.
Train: 2018-08-09T11:41:07.279043: step 16488, loss 0.47045.
Train: 2018-08-09T11:41:07.357151: step 16489, loss 0.625444.
Train: 2018-08-09T11:41:07.450878: step 16490, loss 0.672202.
Test: 2018-08-09T11:41:07.950761: step 16490, loss 0.551117.
Train: 2018-08-09T11:41:08.028891: step 16491, loss 0.532151.
Train: 2018-08-09T11:41:08.122597: step 16492, loss 0.610397.
Train: 2018-08-09T11:41:08.200732: step 16493, loss 0.562939.
Train: 2018-08-09T11:41:08.278807: step 16494, loss 0.578534.
Train: 2018-08-09T11:41:08.356914: step 16495, loss 0.641178.
Train: 2018-08-09T11:41:08.450674: step 16496, loss 0.563776.
Train: 2018-08-09T11:41:08.528779: step 16497, loss 0.517063.
Train: 2018-08-09T11:41:08.606886: step 16498, loss 0.610343.
Train: 2018-08-09T11:41:08.684963: step 16499, loss 0.548444.
Train: 2018-08-09T11:41:08.778721: step 16500, loss 0.655772.
Test: 2018-08-09T11:41:09.265395: step 16500, loss 0.552701.
Train: 2018-08-09T11:41:09.827762: step 16501, loss 0.640434.
Train: 2018-08-09T11:41:09.905868: step 16502, loss 0.487237.
Train: 2018-08-09T11:41:09.984006: step 16503, loss 0.594249.
Train: 2018-08-09T11:41:10.077727: step 16504, loss 0.59429.
Train: 2018-08-09T11:41:10.155840: step 16505, loss 0.487674.
Train: 2018-08-09T11:41:10.233946: step 16506, loss 0.563517.
Train: 2018-08-09T11:41:10.312057: step 16507, loss 0.579005.
Train: 2018-08-09T11:41:10.400811: step 16508, loss 0.578714.
Train: 2018-08-09T11:41:10.478918: step 16509, loss 0.610162.
Train: 2018-08-09T11:41:10.557024: step 16510, loss 0.532847.
Test: 2018-08-09T11:41:11.063224: step 16510, loss 0.551288.
Train: 2018-08-09T11:41:11.141355: step 16511, loss 0.625193.
Train: 2018-08-09T11:41:11.219437: step 16512, loss 0.547292.
Train: 2018-08-09T11:41:11.313196: step 16513, loss 0.580143.
Train: 2018-08-09T11:41:11.391304: step 16514, loss 0.563223.
Train: 2018-08-09T11:41:11.469408: step 16515, loss 0.579735.
Train: 2018-08-09T11:41:11.547516: step 16516, loss 0.64021.
Train: 2018-08-09T11:41:11.641245: step 16517, loss 0.593362.
Train: 2018-08-09T11:41:11.719349: step 16518, loss 0.581861.
Train: 2018-08-09T11:41:11.797457: step 16519, loss 0.547721.
Train: 2018-08-09T11:41:11.875564: step 16520, loss 0.547684.
Test: 2018-08-09T11:41:12.375417: step 16520, loss 0.550153.
Train: 2018-08-09T11:41:12.469173: step 16521, loss 0.593153.
Train: 2018-08-09T11:41:12.547279: step 16522, loss 0.611122.
Train: 2018-08-09T11:41:12.625388: step 16523, loss 0.515898.
Train: 2018-08-09T11:41:12.713214: step 16524, loss 0.613764.
Train: 2018-08-09T11:41:12.791328: step 16525, loss 0.516639.
Train: 2018-08-09T11:41:12.869430: step 16526, loss 0.64183.
Train: 2018-08-09T11:41:12.963160: step 16527, loss 0.595273.
Train: 2018-08-09T11:41:13.041270: step 16528, loss 0.518653.
Train: 2018-08-09T11:41:13.119375: step 16529, loss 0.609257.
Train: 2018-08-09T11:41:13.197480: step 16530, loss 0.579373.
Test: 2018-08-09T11:41:13.697333: step 16530, loss 0.549724.
Train: 2018-08-09T11:41:13.775464: step 16531, loss 0.579367.
Train: 2018-08-09T11:41:13.853572: step 16532, loss 0.548971.
Train: 2018-08-09T11:41:13.947305: step 16533, loss 0.624454.
Train: 2018-08-09T11:41:14.025381: step 16534, loss 0.533855.
Train: 2018-08-09T11:41:14.103487: step 16535, loss 0.564044.
Train: 2018-08-09T11:41:14.197247: step 16536, loss 0.579118.
Train: 2018-08-09T11:41:14.275321: step 16537, loss 0.594221.
Train: 2018-08-09T11:41:14.353458: step 16538, loss 0.533807.
Train: 2018-08-09T11:41:14.431560: step 16539, loss 0.563948.
Train: 2018-08-09T11:41:14.509671: step 16540, loss 0.62446.
Test: 2018-08-09T11:41:15.009524: step 16540, loss 0.550322.
Train: 2018-08-09T11:41:15.103252: step 16541, loss 0.609415.
Train: 2018-08-09T11:41:15.181358: step 16542, loss 0.53367.
Train: 2018-08-09T11:41:15.259464: step 16543, loss 0.548777.
Train: 2018-08-09T11:41:15.337602: step 16544, loss 0.5639.
Train: 2018-08-09T11:41:15.415679: step 16545, loss 0.548729.
Train: 2018-08-09T11:41:15.509430: step 16546, loss 0.563773.
Train: 2018-08-09T11:41:15.587542: step 16547, loss 0.563854.
Train: 2018-08-09T11:41:15.665619: step 16548, loss 0.533285.
Train: 2018-08-09T11:41:15.743725: step 16549, loss 0.594375.
Train: 2018-08-09T11:41:15.837454: step 16550, loss 0.533161.
Test: 2018-08-09T11:41:16.338727: step 16550, loss 0.551473.
Train: 2018-08-09T11:41:16.416854: step 16551, loss 0.578943.
Train: 2018-08-09T11:41:16.494930: step 16552, loss 0.640542.
Train: 2018-08-09T11:41:16.573063: step 16553, loss 0.609699.
Train: 2018-08-09T11:41:16.666790: step 16554, loss 0.470966.
Train: 2018-08-09T11:41:16.744871: step 16555, loss 0.513876.
Train: 2018-08-09T11:41:16.823008: step 16556, loss 0.485904.
Train: 2018-08-09T11:41:16.901086: step 16557, loss 0.625744.
Train: 2018-08-09T11:41:16.979224: step 16558, loss 0.578731.
Train: 2018-08-09T11:41:17.072950: step 16559, loss 0.579219.
Train: 2018-08-09T11:41:17.151056: step 16560, loss 0.579169.
Test: 2018-08-09T11:41:17.650909: step 16560, loss 0.550544.
Train: 2018-08-09T11:41:17.729016: step 16561, loss 0.57797.
Train: 2018-08-09T11:41:17.807120: step 16562, loss 0.642569.
Train: 2018-08-09T11:41:17.885231: step 16563, loss 0.547332.
Train: 2018-08-09T11:41:17.980366: step 16564, loss 0.546768.
Train: 2018-08-09T11:41:18.058470: step 16565, loss 0.516339.
Train: 2018-08-09T11:41:18.136607: step 16566, loss 0.594056.
Train: 2018-08-09T11:41:18.214714: step 16567, loss 0.531491.
Train: 2018-08-09T11:41:18.308412: step 16568, loss 0.56417.
Train: 2018-08-09T11:41:18.386549: step 16569, loss 0.613107.
Train: 2018-08-09T11:41:18.464656: step 16570, loss 0.530995.
Test: 2018-08-09T11:41:18.964541: step 16570, loss 0.550944.
Train: 2018-08-09T11:41:19.042644: step 16571, loss 0.499022.
Train: 2018-08-09T11:41:19.136372: step 16572, loss 0.595059.
Train: 2018-08-09T11:41:19.214478: step 16573, loss 0.532027.
Train: 2018-08-09T11:41:19.292585: step 16574, loss 0.482269.
Train: 2018-08-09T11:41:19.370661: step 16575, loss 0.546703.
Train: 2018-08-09T11:41:19.464420: step 16576, loss 0.546497.
Train: 2018-08-09T11:41:19.542526: step 16577, loss 0.612548.
Train: 2018-08-09T11:41:19.620634: step 16578, loss 0.561085.
Train: 2018-08-09T11:41:19.698741: step 16579, loss 0.512694.
Train: 2018-08-09T11:41:19.792462: step 16580, loss 0.579008.
Test: 2018-08-09T11:41:20.276717: step 16580, loss 0.547733.
Train: 2018-08-09T11:41:20.370426: step 16581, loss 0.611982.
Train: 2018-08-09T11:41:20.448533: step 16582, loss 0.560314.
Train: 2018-08-09T11:41:20.526670: step 16583, loss 0.545038.
Train: 2018-08-09T11:41:20.604774: step 16584, loss 0.56555.
Train: 2018-08-09T11:41:20.698474: step 16585, loss 0.513017.
Train: 2018-08-09T11:41:20.776612: step 16586, loss 0.704102.
Train: 2018-08-09T11:41:20.854686: step 16587, loss 0.629223.
Train: 2018-08-09T11:41:20.932823: step 16588, loss 0.596408.
Train: 2018-08-09T11:41:21.026521: step 16589, loss 0.645719.
Train: 2018-08-09T11:41:21.104661: step 16590, loss 0.727046.
Test: 2018-08-09T11:41:21.604511: step 16590, loss 0.550805.
Train: 2018-08-09T11:41:21.682617: step 16591, loss 0.563313.
Train: 2018-08-09T11:41:21.760724: step 16592, loss 0.579035.
Train: 2018-08-09T11:41:21.854452: step 16593, loss 0.51586.
Train: 2018-08-09T11:41:21.933921: step 16594, loss 0.531943.
Train: 2018-08-09T11:41:22.012027: step 16595, loss 0.532166.
Train: 2018-08-09T11:41:22.090138: step 16596, loss 0.59438.
Train: 2018-08-09T11:41:22.183862: step 16597, loss 0.547857.
Train: 2018-08-09T11:41:22.261939: step 16598, loss 0.470378.
Train: 2018-08-09T11:41:22.340046: step 16599, loss 0.60998.
Train: 2018-08-09T11:41:22.418182: step 16600, loss 0.609861.
Test: 2018-08-09T11:41:22.917786: step 16600, loss 0.552951.
Train: 2018-08-09T11:41:23.495803: step 16601, loss 0.687459.
Train: 2018-08-09T11:41:23.573911: step 16602, loss 0.547954.
Train: 2018-08-09T11:41:23.652017: step 16603, loss 0.516952.
Train: 2018-08-09T11:41:23.730124: step 16604, loss 0.594515.
Train: 2018-08-09T11:41:23.823852: step 16605, loss 0.517335.
Train: 2018-08-09T11:41:23.901927: step 16606, loss 0.59443.
Train: 2018-08-09T11:41:23.982473: step 16607, loss 0.516859.
Train: 2018-08-09T11:41:24.060574: step 16608, loss 0.563608.
Train: 2018-08-09T11:41:24.138690: step 16609, loss 0.56339.
Train: 2018-08-09T11:41:24.232384: step 16610, loss 0.610349.
Test: 2018-08-09T11:41:24.716671: step 16610, loss 0.552247.
Train: 2018-08-09T11:41:24.810403: step 16611, loss 0.516623.
Train: 2018-08-09T11:41:24.888512: step 16612, loss 0.578842.
Train: 2018-08-09T11:41:24.966618: step 16613, loss 0.610615.
Train: 2018-08-09T11:41:25.044692: step 16614, loss 0.67238.
Train: 2018-08-09T11:41:25.122829: step 16615, loss 0.531944.
Train: 2018-08-09T11:41:25.216559: step 16616, loss 0.625409.
Train: 2018-08-09T11:41:25.294664: step 16617, loss 0.656466.
Train: 2018-08-09T11:41:25.372781: step 16618, loss 0.532645.
Train: 2018-08-09T11:41:25.466499: step 16619, loss 0.563486.
Train: 2018-08-09T11:41:25.544604: step 16620, loss 0.563487.
Test: 2018-08-09T11:41:26.031347: step 16620, loss 0.553861.
Train: 2018-08-09T11:41:26.125106: step 16621, loss 0.532838.
Train: 2018-08-09T11:41:26.203211: step 16622, loss 0.53271.
Train: 2018-08-09T11:41:26.281323: step 16623, loss 0.548015.
Train: 2018-08-09T11:41:26.359423: step 16624, loss 0.578483.
Train: 2018-08-09T11:41:26.453121: step 16625, loss 0.594938.
Train: 2018-08-09T11:41:26.531258: step 16626, loss 0.594669.
Train: 2018-08-09T11:41:26.609334: step 16627, loss 0.485724.
Train: 2018-08-09T11:41:26.687470: step 16628, loss 0.547781.
Train: 2018-08-09T11:41:26.765574: step 16629, loss 0.578622.
Train: 2018-08-09T11:41:26.859307: step 16630, loss 0.514534.
Test: 2018-08-09T11:41:27.359162: step 16630, loss 0.548577.
Train: 2018-08-09T11:41:27.437266: step 16631, loss 0.514116.
Train: 2018-08-09T11:41:27.515401: step 16632, loss 0.592917.
Train: 2018-08-09T11:41:27.593481: step 16633, loss 0.563336.
Train: 2018-08-09T11:41:27.687236: step 16634, loss 0.607541.
Train: 2018-08-09T11:41:27.765342: step 16635, loss 0.577296.
Train: 2018-08-09T11:41:27.843424: step 16636, loss 0.562613.
Train: 2018-08-09T11:41:27.922247: step 16637, loss 0.496732.
Train: 2018-08-09T11:41:28.000385: step 16638, loss 0.533067.
Train: 2018-08-09T11:41:28.094114: step 16639, loss 0.688389.
Train: 2018-08-09T11:41:28.172190: step 16640, loss 0.50729.
Test: 2018-08-09T11:41:28.672073: step 16640, loss 0.546748.
Train: 2018-08-09T11:41:28.750179: step 16641, loss 0.593213.
Train: 2018-08-09T11:41:28.828285: step 16642, loss 0.484133.
Train: 2018-08-09T11:41:28.906391: step 16643, loss 0.412421.
Train: 2018-08-09T11:41:29.000149: step 16644, loss 0.470357.
Train: 2018-08-09T11:41:29.078227: step 16645, loss 0.528717.
Train: 2018-08-09T11:41:29.156363: step 16646, loss 0.682875.
Train: 2018-08-09T11:41:29.234464: step 16647, loss 0.539561.
Train: 2018-08-09T11:41:29.328197: step 16648, loss 0.567717.
Train: 2018-08-09T11:41:29.406274: step 16649, loss 0.52707.
Train: 2018-08-09T11:41:29.484381: step 16650, loss 0.566395.
Test: 2018-08-09T11:41:29.984264: step 16650, loss 0.545755.
Train: 2018-08-09T11:41:30.062399: step 16651, loss 0.602323.
Train: 2018-08-09T11:41:30.140506: step 16652, loss 0.561076.
Train: 2018-08-09T11:41:30.234204: step 16653, loss 0.565824.
Train: 2018-08-09T11:41:30.312339: step 16654, loss 0.575709.
Train: 2018-08-09T11:41:30.390446: step 16655, loss 0.578666.
Train: 2018-08-09T11:41:30.468524: step 16656, loss 0.54396.
Train: 2018-08-09T11:41:30.562281: step 16657, loss 0.493977.
Train: 2018-08-09T11:41:30.640388: step 16658, loss 0.595806.
Train: 2018-08-09T11:41:30.718494: step 16659, loss 0.577659.
Train: 2018-08-09T11:41:30.796570: step 16660, loss 0.595488.
Test: 2018-08-09T11:41:31.298740: step 16660, loss 0.548343.
Train: 2018-08-09T11:41:31.392465: step 16661, loss 0.528685.
Train: 2018-08-09T11:41:31.470573: step 16662, loss 0.564521.
Train: 2018-08-09T11:41:31.548710: step 16663, loss 0.605793.
Train: 2018-08-09T11:41:31.626816: step 16664, loss 0.598149.
Train: 2018-08-09T11:41:31.704922: step 16665, loss 0.466294.
Train: 2018-08-09T11:41:31.798651: step 16666, loss 0.496713.
Train: 2018-08-09T11:41:31.939242: step 16667, loss 0.478022.
Train: 2018-08-09T11:41:32.017318: step 16668, loss 0.528482.
Train: 2018-08-09T11:41:32.095425: step 16669, loss 0.529821.
Train: 2018-08-09T11:41:32.173531: step 16670, loss 0.577243.
Test: 2018-08-09T11:41:32.673414: step 16670, loss 0.546677.
Train: 2018-08-09T11:41:32.751550: step 16671, loss 0.740724.
Train: 2018-08-09T11:41:32.845248: step 16672, loss 0.641213.
Train: 2018-08-09T11:41:32.924823: step 16673, loss 0.60087.
Train: 2018-08-09T11:41:33.002930: step 16674, loss 0.464997.
Train: 2018-08-09T11:41:33.081037: step 16675, loss 0.578343.
Train: 2018-08-09T11:41:33.159139: step 16676, loss 0.582009.
Train: 2018-08-09T11:41:33.252870: step 16677, loss 0.627794.
Train: 2018-08-09T11:41:33.330977: step 16678, loss 0.563338.
Train: 2018-08-09T11:41:33.409083: step 16679, loss 0.624658.
Train: 2018-08-09T11:41:33.487161: step 16680, loss 0.643524.
Test: 2018-08-09T11:41:33.987043: step 16680, loss 0.548577.
Train: 2018-08-09T11:41:34.065174: step 16681, loss 0.532459.
Train: 2018-08-09T11:41:34.158878: step 16682, loss 0.672957.
Train: 2018-08-09T11:41:34.237017: step 16683, loss 0.533418.
Train: 2018-08-09T11:41:34.315090: step 16684, loss 0.50267.
Train: 2018-08-09T11:41:34.393229: step 16685, loss 0.518469.
Train: 2018-08-09T11:41:34.471335: step 16686, loss 0.56298.
Train: 2018-08-09T11:41:34.565061: step 16687, loss 0.532851.
Train: 2018-08-09T11:41:34.643171: step 16688, loss 0.577748.
Train: 2018-08-09T11:41:34.721276: step 16689, loss 0.48655.
Train: 2018-08-09T11:41:34.799381: step 16690, loss 0.564097.
Test: 2018-08-09T11:41:35.299234: step 16690, loss 0.550715.
Train: 2018-08-09T11:41:35.377370: step 16691, loss 0.577877.
Train: 2018-08-09T11:41:35.455447: step 16692, loss 0.609651.
Train: 2018-08-09T11:41:35.549175: step 16693, loss 0.532438.
Train: 2018-08-09T11:41:35.627284: step 16694, loss 0.595169.
Train: 2018-08-09T11:41:35.705417: step 16695, loss 0.643448.
Train: 2018-08-09T11:41:35.783494: step 16696, loss 0.577903.
Train: 2018-08-09T11:41:35.877223: step 16697, loss 0.532411.
Train: 2018-08-09T11:41:35.952603: step 16698, loss 0.48601.
Train: 2018-08-09T11:41:36.030710: step 16699, loss 0.563375.
Train: 2018-08-09T11:41:36.108817: step 16700, loss 0.484956.
Test: 2018-08-09T11:41:36.608699: step 16700, loss 0.548934.
Train: 2018-08-09T11:41:37.206118: step 16701, loss 0.546559.
Train: 2018-08-09T11:41:37.284195: step 16702, loss 0.483219.
Train: 2018-08-09T11:41:37.362333: step 16703, loss 0.546803.
Train: 2018-08-09T11:41:37.456060: step 16704, loss 0.547388.
Train: 2018-08-09T11:41:37.534166: step 16705, loss 0.596591.
Train: 2018-08-09T11:41:37.612272: step 16706, loss 0.448098.
Train: 2018-08-09T11:41:37.690379: step 16707, loss 0.513969.
Train: 2018-08-09T11:41:37.768486: step 16708, loss 0.472519.
Train: 2018-08-09T11:41:37.862214: step 16709, loss 0.497221.
Train: 2018-08-09T11:41:37.940290: step 16710, loss 0.471822.
Test: 2018-08-09T11:41:38.440173: step 16710, loss 0.545827.
Train: 2018-08-09T11:41:38.518310: step 16711, loss 0.668386.
Train: 2018-08-09T11:41:38.596417: step 16712, loss 0.606181.
Train: 2018-08-09T11:41:38.690144: step 16713, loss 0.625558.
Train: 2018-08-09T11:41:38.768221: step 16714, loss 0.711216.
Train: 2018-08-09T11:41:38.846326: step 16715, loss 0.626755.
Train: 2018-08-09T11:41:38.924460: step 16716, loss 0.659988.
Train: 2018-08-09T11:41:39.018192: step 16717, loss 0.562868.
Train: 2018-08-09T11:41:39.096298: step 16718, loss 0.530961.
Train: 2018-08-09T11:41:39.174405: step 16719, loss 0.594822.
Train: 2018-08-09T11:41:39.252512: step 16720, loss 0.578858.
Test: 2018-08-09T11:41:39.752363: step 16720, loss 0.548831.
Train: 2018-08-09T11:41:39.830470: step 16721, loss 0.546985.
Train: 2018-08-09T11:41:39.910130: step 16722, loss 0.437109.
Train: 2018-08-09T11:41:40.003893: step 16723, loss 0.578927.
Train: 2018-08-09T11:41:40.081995: step 16724, loss 0.578942.
Train: 2018-08-09T11:41:40.160102: step 16725, loss 0.51481.
Train: 2018-08-09T11:41:40.238210: step 16726, loss 0.562039.
Train: 2018-08-09T11:41:40.331907: step 16727, loss 0.67537.
Train: 2018-08-09T11:41:40.410042: step 16728, loss 0.530941.
Train: 2018-08-09T11:41:40.488151: step 16729, loss 0.563282.
Train: 2018-08-09T11:41:40.566227: step 16730, loss 0.531197.
Test: 2018-08-09T11:41:41.066109: step 16730, loss 0.550511.
Train: 2018-08-09T11:41:41.144244: step 16731, loss 0.610422.
Train: 2018-08-09T11:41:41.237972: step 16732, loss 0.547118.
Train: 2018-08-09T11:41:41.316080: step 16733, loss 0.547131.
Train: 2018-08-09T11:41:41.394155: step 16734, loss 0.578852.
Train: 2018-08-09T11:41:41.472263: step 16735, loss 0.563012.
Train: 2018-08-09T11:41:41.550400: step 16736, loss 0.467798.
Train: 2018-08-09T11:41:41.644127: step 16737, loss 0.547034.
Train: 2018-08-09T11:41:41.722234: step 16738, loss 0.483048.
Train: 2018-08-09T11:41:41.798284: step 16739, loss 0.562722.
Train: 2018-08-09T11:41:41.876391: step 16740, loss 0.546934.
Test: 2018-08-09T11:41:42.379666: step 16740, loss 0.549365.
Train: 2018-08-09T11:41:42.457802: step 16741, loss 0.562587.
Train: 2018-08-09T11:41:42.535880: step 16742, loss 0.595891.
Train: 2018-08-09T11:41:42.629607: step 16743, loss 0.546339.
Train: 2018-08-09T11:41:42.709886: step 16744, loss 0.579384.
Train: 2018-08-09T11:41:42.777831: step 16745, loss 0.562267.
Train: 2018-08-09T11:41:42.871589: step 16746, loss 0.513166.
Train: 2018-08-09T11:41:42.949696: step 16747, loss 0.530274.
Train: 2018-08-09T11:41:43.027804: step 16748, loss 0.463807.
Train: 2018-08-09T11:41:43.105904: step 16749, loss 0.495734.
Train: 2018-08-09T11:41:43.199606: step 16750, loss 0.579392.
Test: 2018-08-09T11:41:43.699513: step 16750, loss 0.549004.
Train: 2018-08-09T11:41:43.793247: step 16751, loss 0.596839.
Train: 2018-08-09T11:41:43.871322: step 16752, loss 0.562811.
Train: 2018-08-09T11:41:43.949460: step 16753, loss 0.545405.
Train: 2018-08-09T11:41:44.027560: step 16754, loss 0.578996.
Train: 2018-08-09T11:41:44.121295: step 16755, loss 0.528725.
Train: 2018-08-09T11:41:44.199401: step 16756, loss 0.546233.
Train: 2018-08-09T11:41:44.277509: step 16757, loss 0.510153.
Train: 2018-08-09T11:41:44.355613: step 16758, loss 0.492703.
Train: 2018-08-09T11:41:44.433692: step 16759, loss 0.489395.
Train: 2018-08-09T11:41:44.527418: step 16760, loss 0.504078.
Test: 2018-08-09T11:41:45.026822: step 16760, loss 0.547185.
Train: 2018-08-09T11:41:45.105400: step 16761, loss 0.598402.
Train: 2018-08-09T11:41:45.183506: step 16762, loss 0.677362.
Train: 2018-08-09T11:41:45.261615: step 16763, loss 0.593096.
Train: 2018-08-09T11:41:45.339689: step 16764, loss 0.5869.
Train: 2018-08-09T11:41:45.417827: step 16765, loss 0.68826.
Train: 2018-08-09T11:41:45.511556: step 16766, loss 0.523327.
Train: 2018-08-09T11:41:45.589630: step 16767, loss 0.52192.
Train: 2018-08-09T11:41:45.667738: step 16768, loss 0.598258.
Train: 2018-08-09T11:41:45.745872: step 16769, loss 0.577556.
Train: 2018-08-09T11:41:45.839602: step 16770, loss 0.689785.
Test: 2018-08-09T11:41:46.326277: step 16770, loss 0.5461.
Train: 2018-08-09T11:41:46.420005: step 16771, loss 0.594619.
Train: 2018-08-09T11:41:46.498111: step 16772, loss 0.698785.
Train: 2018-08-09T11:41:46.576219: step 16773, loss 0.561025.
Train: 2018-08-09T11:41:46.654325: step 16774, loss 0.660838.
Train: 2018-08-09T11:41:46.732402: step 16775, loss 0.530132.
Train: 2018-08-09T11:41:46.826159: step 16776, loss 0.594389.
Train: 2018-08-09T11:41:46.904236: step 16777, loss 0.503087.
Train: 2018-08-09T11:41:46.982373: step 16778, loss 0.609492.
Train: 2018-08-09T11:41:47.076070: step 16779, loss 0.548745.
Train: 2018-08-09T11:41:47.154207: step 16780, loss 0.48656.
Test: 2018-08-09T11:41:47.654090: step 16780, loss 0.551157.
Train: 2018-08-09T11:41:47.732196: step 16781, loss 0.485207.
Train: 2018-08-09T11:41:47.810305: step 16782, loss 0.609761.
Train: 2018-08-09T11:41:47.888410: step 16783, loss 0.563066.
Train: 2018-08-09T11:41:47.967882: step 16784, loss 0.59419.
Train: 2018-08-09T11:41:48.045989: step 16785, loss 0.532615.
Train: 2018-08-09T11:41:48.139718: step 16786, loss 0.563165.
Train: 2018-08-09T11:41:48.217857: step 16787, loss 0.531771.
Train: 2018-08-09T11:41:48.295960: step 16788, loss 0.688542.
Train: 2018-08-09T11:41:48.374038: step 16789, loss 0.609669.
Train: 2018-08-09T11:41:48.467798: step 16790, loss 0.516697.
Test: 2018-08-09T11:41:48.967647: step 16790, loss 0.549962.
Train: 2018-08-09T11:41:49.045779: step 16791, loss 0.532268.
Train: 2018-08-09T11:41:49.123859: step 16792, loss 0.48561.
Train: 2018-08-09T11:41:49.201995: step 16793, loss 0.470311.
Train: 2018-08-09T11:41:49.295726: step 16794, loss 0.594383.
Train: 2018-08-09T11:41:49.373831: step 16795, loss 0.547836.
Train: 2018-08-09T11:41:49.451907: step 16796, loss 0.499994.
Train: 2018-08-09T11:41:49.530045: step 16797, loss 0.642432.
Train: 2018-08-09T11:41:49.608151: step 16798, loss 0.610883.
Train: 2018-08-09T11:41:49.701882: step 16799, loss 0.609759.
Train: 2018-08-09T11:41:49.779985: step 16800, loss 0.499783.
Test: 2018-08-09T11:41:50.280553: step 16800, loss 0.549242.
Train: 2018-08-09T11:41:50.827305: step 16801, loss 0.499042.
Train: 2018-08-09T11:41:50.905413: step 16802, loss 0.61081.
Train: 2018-08-09T11:41:50.983490: step 16803, loss 0.595084.
Train: 2018-08-09T11:41:51.061595: step 16804, loss 0.53083.
Train: 2018-08-09T11:41:51.155323: step 16805, loss 0.579629.
Train: 2018-08-09T11:41:51.233455: step 16806, loss 0.546385.
Train: 2018-08-09T11:41:51.311566: step 16807, loss 0.563181.
Train: 2018-08-09T11:41:51.389642: step 16808, loss 0.546069.
Train: 2018-08-09T11:41:51.483401: step 16809, loss 0.563219.
Train: 2018-08-09T11:41:51.561476: step 16810, loss 0.593792.
Test: 2018-08-09T11:41:52.061391: step 16810, loss 0.546909.
Train: 2018-08-09T11:41:52.139496: step 16811, loss 0.531713.
Train: 2018-08-09T11:41:52.217602: step 16812, loss 0.611084.
Train: 2018-08-09T11:41:52.295708: step 16813, loss 0.530891.
Train: 2018-08-09T11:41:52.389432: step 16814, loss 0.513563.
Train: 2018-08-09T11:41:52.467551: step 16815, loss 0.514551.
Train: 2018-08-09T11:41:52.545620: step 16816, loss 0.561977.
Train: 2018-08-09T11:41:52.623727: step 16817, loss 0.496119.
Train: 2018-08-09T11:41:52.701863: step 16818, loss 0.579957.
Train: 2018-08-09T11:41:52.795563: step 16819, loss 0.629239.
Train: 2018-08-09T11:41:52.873698: step 16820, loss 0.480635.
Test: 2018-08-09T11:41:53.375917: step 16820, loss 0.550643.
Train: 2018-08-09T11:41:53.454022: step 16821, loss 0.444867.
Train: 2018-08-09T11:41:53.532128: step 16822, loss 0.6129.
Train: 2018-08-09T11:41:53.610237: step 16823, loss 0.477882.
Train: 2018-08-09T11:41:53.688341: step 16824, loss 0.526489.
Train: 2018-08-09T11:41:53.782100: step 16825, loss 0.560715.
Train: 2018-08-09T11:41:53.860178: step 16826, loss 0.637646.
Train: 2018-08-09T11:41:53.938313: step 16827, loss 0.492683.
Train: 2018-08-09T11:41:54.016423: step 16828, loss 0.581597.
Train: 2018-08-09T11:41:54.110117: step 16829, loss 0.562987.
Train: 2018-08-09T11:41:54.188253: step 16830, loss 0.65447.
Test: 2018-08-09T11:41:54.688137: step 16830, loss 0.546796.
Train: 2018-08-09T11:41:54.766212: step 16831, loss 0.594109.
Train: 2018-08-09T11:41:54.844319: step 16832, loss 0.54459.
Train: 2018-08-09T11:41:54.928998: step 16833, loss 0.477383.
Train: 2018-08-09T11:41:55.007104: step 16834, loss 0.477697.
Train: 2018-08-09T11:41:55.085210: step 16835, loss 0.578519.
Train: 2018-08-09T11:41:55.178938: step 16836, loss 0.544974.
Train: 2018-08-09T11:41:55.257075: step 16837, loss 0.529236.
Train: 2018-08-09T11:41:55.335181: step 16838, loss 0.627838.
Train: 2018-08-09T11:41:55.413289: step 16839, loss 0.579534.
Train: 2018-08-09T11:41:55.491395: step 16840, loss 0.513259.
Test: 2018-08-09T11:41:55.991247: step 16840, loss 0.549458.
Train: 2018-08-09T11:41:56.069354: step 16841, loss 0.56365.
Train: 2018-08-09T11:41:56.163111: step 16842, loss 0.617135.
Train: 2018-08-09T11:41:56.241218: step 16843, loss 0.49402.
Train: 2018-08-09T11:41:56.319295: step 16844, loss 0.494207.
Train: 2018-08-09T11:41:56.397433: step 16845, loss 0.528679.
Train: 2018-08-09T11:41:56.491159: step 16846, loss 0.544678.
Train: 2018-08-09T11:41:56.569266: step 16847, loss 0.631614.
Train: 2018-08-09T11:41:56.647367: step 16848, loss 0.562653.
Train: 2018-08-09T11:41:56.725449: step 16849, loss 0.580589.
Train: 2018-08-09T11:41:56.803586: step 16850, loss 0.528472.
Test: 2018-08-09T11:41:57.304821: step 16850, loss 0.551045.
Train: 2018-08-09T11:41:57.382954: step 16851, loss 0.528585.
Train: 2018-08-09T11:41:57.476687: step 16852, loss 0.612903.
Train: 2018-08-09T11:41:57.554793: step 16853, loss 0.630721.
Train: 2018-08-09T11:41:57.632895: step 16854, loss 0.663125.
Train: 2018-08-09T11:41:57.711011: step 16855, loss 0.596564.
Train: 2018-08-09T11:41:57.789109: step 16856, loss 0.544548.
Train: 2018-08-09T11:41:57.882841: step 16857, loss 0.530197.
Train: 2018-08-09T11:41:57.960917: step 16858, loss 0.54629.
Train: 2018-08-09T11:41:58.039023: step 16859, loss 0.628353.
Train: 2018-08-09T11:41:58.117161: step 16860, loss 0.530463.
Test: 2018-08-09T11:41:58.617014: step 16860, loss 0.548051.
Train: 2018-08-09T11:41:58.695118: step 16861, loss 0.562219.
Train: 2018-08-09T11:41:58.788846: step 16862, loss 0.611212.
Train: 2018-08-09T11:41:58.866984: step 16863, loss 0.595074.
Train: 2018-08-09T11:41:58.946350: step 16864, loss 0.56265.
Train: 2018-08-09T11:41:59.024433: step 16865, loss 0.595071.
Train: 2018-08-09T11:41:59.118216: step 16866, loss 0.498531.
Train: 2018-08-09T11:41:59.196297: step 16867, loss 0.578945.
Train: 2018-08-09T11:41:59.274404: step 16868, loss 0.514904.
Train: 2018-08-09T11:41:59.352511: step 16869, loss 0.562832.
Train: 2018-08-09T11:41:59.430587: step 16870, loss 0.498886.
Test: 2018-08-09T11:41:59.930500: step 16870, loss 0.550259.
Train: 2018-08-09T11:42:00.024224: step 16871, loss 0.674958.
Train: 2018-08-09T11:42:00.102333: step 16872, loss 0.64295.
Train: 2018-08-09T11:42:00.180410: step 16873, loss 0.5629.
Train: 2018-08-09T11:42:00.258550: step 16874, loss 0.642606.
Train: 2018-08-09T11:42:00.336622: step 16875, loss 0.578865.
Train: 2018-08-09T11:42:00.430353: step 16876, loss 0.578859.
Train: 2018-08-09T11:42:00.508489: step 16877, loss 0.642071.
Train: 2018-08-09T11:42:00.586594: step 16878, loss 0.641848.
Train: 2018-08-09T11:42:00.664701: step 16879, loss 0.610235.
Train: 2018-08-09T11:42:00.742808: step 16880, loss 0.563281.
Test: 2018-08-09T11:42:01.244926: step 16880, loss 0.548792.
Train: 2018-08-09T11:42:01.323062: step 16881, loss 0.547814.
Train: 2018-08-09T11:42:01.401169: step 16882, loss 0.485929.
Train: 2018-08-09T11:42:01.494868: step 16883, loss 0.54797.
Train: 2018-08-09T11:42:01.572974: step 16884, loss 0.486132.
Train: 2018-08-09T11:42:01.651110: step 16885, loss 0.563446.
Train: 2018-08-09T11:42:01.729215: step 16886, loss 0.625404.
Train: 2018-08-09T11:42:01.822947: step 16887, loss 0.516952.
Train: 2018-08-09T11:42:01.901021: step 16888, loss 0.563405.
Train: 2018-08-09T11:42:01.979127: step 16889, loss 0.547859.
Train: 2018-08-09T11:42:02.057234: step 16890, loss 0.578915.
Test: 2018-08-09T11:42:02.557149: step 16890, loss 0.548163.
Train: 2018-08-09T11:42:02.635253: step 16891, loss 0.641141.
Train: 2018-08-09T11:42:02.713330: step 16892, loss 0.656732.
Train: 2018-08-09T11:42:02.807057: step 16893, loss 0.547831.
Train: 2018-08-09T11:42:02.885195: step 16894, loss 0.532337.
Train: 2018-08-09T11:42:02.965604: step 16895, loss 0.501296.
Train: 2018-08-09T11:42:03.043707: step 16896, loss 0.501206.
Train: 2018-08-09T11:42:03.121786: step 16897, loss 0.563352.
Train: 2018-08-09T11:42:03.215515: step 16898, loss 0.516447.
Train: 2018-08-09T11:42:03.293652: step 16899, loss 0.563155.
Train: 2018-08-09T11:42:03.371728: step 16900, loss 0.453182.
Test: 2018-08-09T11:42:03.871621: step 16900, loss 0.550626.
Train: 2018-08-09T11:42:04.434008: step 16901, loss 0.531319.
Train: 2018-08-09T11:42:04.527736: step 16902, loss 0.579173.
Train: 2018-08-09T11:42:04.605844: step 16903, loss 0.626876.
Train: 2018-08-09T11:42:04.683950: step 16904, loss 0.563419.
Train: 2018-08-09T11:42:04.762055: step 16905, loss 0.611145.
Train: 2018-08-09T11:42:04.840133: step 16906, loss 0.627215.
Train: 2018-08-09T11:42:04.935325: step 16907, loss 0.546727.
Train: 2018-08-09T11:42:05.013403: step 16908, loss 0.562705.
Train: 2018-08-09T11:42:05.091509: step 16909, loss 0.610884.
Train: 2018-08-09T11:42:05.169649: step 16910, loss 0.530818.
Test: 2018-08-09T11:42:05.669510: step 16910, loss 0.552018.
Train: 2018-08-09T11:42:05.747634: step 16911, loss 0.547071.
Train: 2018-08-09T11:42:05.825737: step 16912, loss 0.675095.
Train: 2018-08-09T11:42:05.919439: step 16913, loss 0.562751.
Train: 2018-08-09T11:42:05.997576: step 16914, loss 0.514851.
Train: 2018-08-09T11:42:06.075682: step 16915, loss 0.642894.
Train: 2018-08-09T11:42:06.153788: step 16916, loss 0.578856.
Train: 2018-08-09T11:42:06.231895: step 16917, loss 0.546972.
Train: 2018-08-09T11:42:06.325594: step 16918, loss 0.483281.
Train: 2018-08-09T11:42:06.403734: step 16919, loss 0.546982.
Train: 2018-08-09T11:42:06.481806: step 16920, loss 0.515048.
Test: 2018-08-09T11:42:06.984266: step 16920, loss 0.54912.
Train: 2018-08-09T11:42:07.062388: step 16921, loss 0.498967.
Train: 2018-08-09T11:42:07.156085: step 16922, loss 0.658976.
Train: 2018-08-09T11:42:07.234191: step 16923, loss 0.594907.
Train: 2018-08-09T11:42:07.312328: step 16924, loss 0.498677.
Train: 2018-08-09T11:42:07.390406: step 16925, loss 0.562829.
Train: 2018-08-09T11:42:07.484135: step 16926, loss 0.514539.
Train: 2018-08-09T11:42:07.562270: step 16927, loss 0.546678.
Train: 2018-08-09T11:42:07.640345: step 16928, loss 0.595053.
Train: 2018-08-09T11:42:07.734105: step 16929, loss 0.595044.
Train: 2018-08-09T11:42:07.812210: step 16930, loss 0.595091.
Test: 2018-08-09T11:42:08.312088: step 16930, loss 0.550001.
Train: 2018-08-09T11:42:08.390200: step 16931, loss 0.498.
Train: 2018-08-09T11:42:08.468276: step 16932, loss 0.611293.
Train: 2018-08-09T11:42:08.562003: step 16933, loss 0.578892.
Train: 2018-08-09T11:42:08.640142: step 16934, loss 0.546493.
Train: 2018-08-09T11:42:08.718247: step 16935, loss 0.676172.
Train: 2018-08-09T11:42:08.796356: step 16936, loss 0.562715.
Train: 2018-08-09T11:42:08.874429: step 16937, loss 0.562721.
Train: 2018-08-09T11:42:08.968850: step 16938, loss 0.56275.
Train: 2018-08-09T11:42:09.046965: step 16939, loss 0.595014.
Train: 2018-08-09T11:42:09.125035: step 16940, loss 0.595.
Test: 2018-08-09T11:42:09.624918: step 16940, loss 0.54954.
Train: 2018-08-09T11:42:09.703054: step 16941, loss 0.562791.
Train: 2018-08-09T11:42:09.781160: step 16942, loss 0.546746.
Train: 2018-08-09T11:42:09.874889: step 16943, loss 0.498639.
Train: 2018-08-09T11:42:09.952994: step 16944, loss 0.482578.
Train: 2018-08-09T11:42:10.031102: step 16945, loss 0.578873.
Train: 2018-08-09T11:42:10.109208: step 16946, loss 0.627163.
Train: 2018-08-09T11:42:10.187314: step 16947, loss 0.514516.
Train: 2018-08-09T11:42:10.281011: step 16948, loss 0.546608.
Train: 2018-08-09T11:42:10.359118: step 16949, loss 0.595072.
Train: 2018-08-09T11:42:10.437255: step 16950, loss 0.546658.
Test: 2018-08-09T11:42:10.937109: step 16950, loss 0.548831.
Train: 2018-08-09T11:42:11.015244: step 16951, loss 0.546622.
Train: 2018-08-09T11:42:11.093351: step 16952, loss 0.465775.
Train: 2018-08-09T11:42:11.187049: step 16953, loss 0.595117.
Train: 2018-08-09T11:42:11.265185: step 16954, loss 0.546468.
Train: 2018-08-09T11:42:11.343292: step 16955, loss 0.530161.
Train: 2018-08-09T11:42:11.421394: step 16956, loss 0.595208.
Train: 2018-08-09T11:42:11.499505: step 16957, loss 0.56261.
Train: 2018-08-09T11:42:11.593233: step 16958, loss 0.627902.
Train: 2018-08-09T11:42:11.671309: step 16959, loss 0.48096.
Train: 2018-08-09T11:42:11.749416: step 16960, loss 0.546238.
Test: 2018-08-09T11:42:12.251663: step 16960, loss 0.549705.
Train: 2018-08-09T11:42:12.329798: step 16961, loss 0.513449.
Train: 2018-08-09T11:42:12.407875: step 16962, loss 0.480455.
Train: 2018-08-09T11:42:12.501603: step 16963, loss 0.562493.
Train: 2018-08-09T11:42:12.579708: step 16964, loss 0.611885.
Train: 2018-08-09T11:42:12.657841: step 16965, loss 0.628959.
Train: 2018-08-09T11:42:12.735953: step 16966, loss 0.512801.
Train: 2018-08-09T11:42:12.829680: step 16967, loss 0.545731.
Train: 2018-08-09T11:42:12.907786: step 16968, loss 0.562335.
Train: 2018-08-09T11:42:12.985894: step 16969, loss 0.612859.
Train: 2018-08-09T11:42:13.063999: step 16970, loss 0.512705.
Test: 2018-08-09T11:42:13.563854: step 16970, loss 0.548716.
Train: 2018-08-09T11:42:13.641983: step 16971, loss 0.612756.
Train: 2018-08-09T11:42:13.720095: step 16972, loss 0.512634.
Train: 2018-08-09T11:42:13.813794: step 16973, loss 0.562433.
Train: 2018-08-09T11:42:13.891900: step 16974, loss 0.545799.
Train: 2018-08-09T11:42:13.971447: step 16975, loss 0.612382.
Train: 2018-08-09T11:42:14.049526: step 16976, loss 0.562421.
Train: 2018-08-09T11:42:14.127661: step 16977, loss 0.645608.
Train: 2018-08-09T11:42:14.221388: step 16978, loss 0.529185.
Train: 2018-08-09T11:42:14.299497: step 16979, loss 0.628922.
Train: 2018-08-09T11:42:14.377603: step 16980, loss 0.46315.
Test: 2018-08-09T11:42:14.877456: step 16980, loss 0.548218.
Train: 2018-08-09T11:42:14.955585: step 16981, loss 0.496454.
Train: 2018-08-09T11:42:15.033697: step 16982, loss 0.545662.
Train: 2018-08-09T11:42:15.125240: step 16983, loss 0.728373.
Train: 2018-08-09T11:42:15.203377: step 16984, loss 0.545922.
Train: 2018-08-09T11:42:15.281484: step 16985, loss 0.612042.
Train: 2018-08-09T11:42:15.359593: step 16986, loss 0.51308.
Train: 2018-08-09T11:42:15.437667: step 16987, loss 0.578881.
Train: 2018-08-09T11:42:15.531396: step 16988, loss 0.628429.
Train: 2018-08-09T11:42:15.609535: step 16989, loss 0.529798.
Train: 2018-08-09T11:42:15.687639: step 16990, loss 0.578973.
Test: 2018-08-09T11:42:16.191031: step 16990, loss 0.549149.
Train: 2018-08-09T11:42:16.269135: step 16991, loss 0.546303.
Train: 2018-08-09T11:42:16.347242: step 16992, loss 0.546294.
Train: 2018-08-09T11:42:16.425343: step 16993, loss 0.709298.
Train: 2018-08-09T11:42:16.519045: step 16994, loss 0.562659.
Train: 2018-08-09T11:42:16.597153: step 16995, loss 0.627503.
Train: 2018-08-09T11:42:16.675289: step 16996, loss 0.514283.
Train: 2018-08-09T11:42:16.769012: step 16997, loss 0.562787.
Train: 2018-08-09T11:42:16.847124: step 16998, loss 0.627123.
Train: 2018-08-09T11:42:16.925230: step 16999, loss 0.56285.
Train: 2018-08-09T11:42:17.003307: step 17000, loss 0.514893.
Test: 2018-08-09T11:42:17.495742: step 17000, loss 0.551552.
Train: 2018-08-09T11:42:18.073760: step 17001, loss 0.658706.
Train: 2018-08-09T11:42:18.151834: step 17002, loss 0.626637.
Train: 2018-08-09T11:42:18.229972: step 17003, loss 0.483629.
Train: 2018-08-09T11:42:18.323700: step 17004, loss 0.531325.
Train: 2018-08-09T11:42:18.401825: step 17005, loss 0.594691.
Train: 2018-08-09T11:42:18.479914: step 17006, loss 0.578861.
Train: 2018-08-09T11:42:18.573663: step 17007, loss 0.578862.
Train: 2018-08-09T11:42:18.651749: step 17008, loss 0.547319.
Train: 2018-08-09T11:42:18.729835: step 17009, loss 0.594624.
Train: 2018-08-09T11:42:18.807961: step 17010, loss 0.500153.
Test: 2018-08-09T11:42:19.307813: step 17010, loss 0.548367.
Train: 2018-08-09T11:42:19.385952: step 17011, loss 0.547396.
Train: 2018-08-09T11:42:19.464025: step 17012, loss 0.578854.
Train: 2018-08-09T11:42:19.557779: step 17013, loss 0.626159.
Train: 2018-08-09T11:42:19.635890: step 17014, loss 0.578864.
Train: 2018-08-09T11:42:19.713997: step 17015, loss 0.594616.
Train: 2018-08-09T11:42:19.792104: step 17016, loss 0.453096.
Train: 2018-08-09T11:42:19.870181: step 17017, loss 0.594618.
Train: 2018-08-09T11:42:19.949681: step 17018, loss 0.578856.
Train: 2018-08-09T11:42:20.043379: step 17019, loss 0.594613.
Train: 2018-08-09T11:42:20.121516: step 17020, loss 0.547363.
Test: 2018-08-09T11:42:20.605770: step 17020, loss 0.550135.
Train: 2018-08-09T11:42:20.699474: step 17021, loss 0.594628.
Train: 2018-08-09T11:42:20.777614: step 17022, loss 0.531594.
Train: 2018-08-09T11:42:20.855687: step 17023, loss 0.578872.
Train: 2018-08-09T11:42:20.933824: step 17024, loss 0.452737.
Train: 2018-08-09T11:42:21.011930: step 17025, loss 0.499852.
Train: 2018-08-09T11:42:21.105658: step 17026, loss 0.547141.
Train: 2018-08-09T11:42:21.183735: step 17027, loss 0.4993.
Train: 2018-08-09T11:42:21.261871: step 17028, loss 0.626859.
Train: 2018-08-09T11:42:21.339978: step 17029, loss 0.498767.
Train: 2018-08-09T11:42:21.418055: step 17030, loss 0.482366.
Test: 2018-08-09T11:42:21.919260: step 17030, loss 0.548816.
Train: 2018-08-09T11:42:22.013018: step 17031, loss 0.514265.
Train: 2018-08-09T11:42:22.091125: step 17032, loss 0.578923.
Train: 2018-08-09T11:42:22.169202: step 17033, loss 0.578744.
Train: 2018-08-09T11:42:22.247307: step 17034, loss 0.611941.
Train: 2018-08-09T11:42:22.341066: step 17035, loss 0.578597.
Train: 2018-08-09T11:42:22.419173: step 17036, loss 0.563001.
Train: 2018-08-09T11:42:22.497279: step 17037, loss 0.545949.
Train: 2018-08-09T11:42:22.575356: step 17038, loss 0.529328.
Train: 2018-08-09T11:42:22.653464: step 17039, loss 0.562467.
Train: 2018-08-09T11:42:22.747191: step 17040, loss 0.695457.
Test: 2018-08-09T11:42:23.231482: step 17040, loss 0.550071.
Train: 2018-08-09T11:42:23.325209: step 17041, loss 0.611959.
Train: 2018-08-09T11:42:23.403318: step 17042, loss 0.578967.
Train: 2018-08-09T11:42:23.481392: step 17043, loss 0.480133.
Train: 2018-08-09T11:42:23.559529: step 17044, loss 0.513196.
Train: 2018-08-09T11:42:23.653259: step 17045, loss 0.579234.
Train: 2018-08-09T11:42:23.731335: step 17046, loss 0.513017.
Train: 2018-08-09T11:42:23.809470: step 17047, loss 0.595489.
Train: 2018-08-09T11:42:23.887577: step 17048, loss 0.496427.
Train: 2018-08-09T11:42:23.967992: step 17049, loss 0.513034.
Train: 2018-08-09T11:42:24.061719: step 17050, loss 0.578992.
Test: 2018-08-09T11:42:24.545982: step 17050, loss 0.548836.
Train: 2018-08-09T11:42:24.639679: step 17051, loss 0.56255.
Train: 2018-08-09T11:42:24.717785: step 17052, loss 0.512811.
Train: 2018-08-09T11:42:24.795892: step 17053, loss 0.562492.
Train: 2018-08-09T11:42:24.873997: step 17054, loss 0.595629.
Train: 2018-08-09T11:42:24.952135: step 17055, loss 0.595615.
Train: 2018-08-09T11:42:25.045864: step 17056, loss 0.579072.
Train: 2018-08-09T11:42:25.123969: step 17057, loss 0.529129.
Train: 2018-08-09T11:42:25.202075: step 17058, loss 0.595648.
Train: 2018-08-09T11:42:25.280183: step 17059, loss 0.462617.
Train: 2018-08-09T11:42:25.373880: step 17060, loss 0.562393.
Test: 2018-08-09T11:42:25.858166: step 17060, loss 0.550565.
Train: 2018-08-09T11:42:25.952534: step 17061, loss 0.612835.
Train: 2018-08-09T11:42:26.030645: step 17062, loss 0.612415.
Train: 2018-08-09T11:42:26.108723: step 17063, loss 0.579221.
Train: 2018-08-09T11:42:26.186859: step 17064, loss 0.628935.
Train: 2018-08-09T11:42:26.264969: step 17065, loss 0.545896.
Train: 2018-08-09T11:42:26.358664: step 17066, loss 0.56244.
Train: 2018-08-09T11:42:26.436801: step 17067, loss 0.628646.
Train: 2018-08-09T11:42:26.514910: step 17068, loss 0.496498.
Train: 2018-08-09T11:42:26.593015: step 17069, loss 0.529564.
Train: 2018-08-09T11:42:26.686711: step 17070, loss 0.513115.
Test: 2018-08-09T11:42:27.170973: step 17070, loss 0.552695.
Train: 2018-08-09T11:42:27.264724: step 17071, loss 0.562524.
Train: 2018-08-09T11:42:27.345978: step 17072, loss 0.529581.
Train: 2018-08-09T11:42:27.421572: step 17073, loss 0.529554.
Train: 2018-08-09T11:42:27.499676: step 17074, loss 0.54601.
Train: 2018-08-09T11:42:27.577777: step 17075, loss 0.545982.
Train: 2018-08-09T11:42:27.671512: step 17076, loss 0.579027.
Train: 2018-08-09T11:42:27.749618: step 17077, loss 0.529393.
Train: 2018-08-09T11:42:27.827696: step 17078, loss 0.595601.
Train: 2018-08-09T11:42:27.905832: step 17079, loss 0.612187.
Train: 2018-08-09T11:42:27.999528: step 17080, loss 0.512769.
Test: 2018-08-09T11:42:28.483790: step 17080, loss 0.546941.
Train: 2018-08-09T11:42:28.561897: step 17081, loss 0.595631.
Train: 2018-08-09T11:42:28.655625: step 17082, loss 0.545898.
Train: 2018-08-09T11:42:28.733762: step 17083, loss 0.579037.
Train: 2018-08-09T11:42:28.811868: step 17084, loss 0.595608.
Train: 2018-08-09T11:42:28.889977: step 17085, loss 0.62869.
Train: 2018-08-09T11:42:28.968082: step 17086, loss 0.595532.
Train: 2018-08-09T11:42:29.061778: step 17087, loss 0.496583.
Train: 2018-08-09T11:42:29.139915: step 17088, loss 0.562525.
Train: 2018-08-09T11:42:29.218022: step 17089, loss 0.546081.
Train: 2018-08-09T11:42:29.296098: step 17090, loss 0.546103.
Test: 2018-08-09T11:42:29.795981: step 17090, loss 0.551491.
Train: 2018-08-09T11:42:29.874111: step 17091, loss 0.463923.
Train: 2018-08-09T11:42:29.952224: step 17092, loss 0.562529.
Train: 2018-08-09T11:42:30.030327: step 17093, loss 0.562529.
Train: 2018-08-09T11:42:30.124029: step 17094, loss 0.57901.
Train: 2018-08-09T11:42:30.202136: step 17095, loss 0.496527.
Train: 2018-08-09T11:42:30.280243: step 17096, loss 0.595534.
Train: 2018-08-09T11:42:30.373994: step 17097, loss 0.479836.
Train: 2018-08-09T11:42:30.452106: step 17098, loss 0.628725.
Train: 2018-08-09T11:42:30.530207: step 17099, loss 0.579043.
Train: 2018-08-09T11:42:30.608320: step 17100, loss 0.51274.
Test: 2018-08-09T11:42:31.110545: step 17100, loss 0.545661.
Train: 2018-08-09T11:42:31.672911: step 17101, loss 0.579054.
Train: 2018-08-09T11:42:31.751048: step 17102, loss 0.628863.
Train: 2018-08-09T11:42:31.829126: step 17103, loss 0.579054.
Train: 2018-08-09T11:42:31.907231: step 17104, loss 0.429852.
Train: 2018-08-09T11:42:31.985369: step 17105, loss 0.496072.
Train: 2018-08-09T11:42:32.079097: step 17106, loss 0.545816.
Train: 2018-08-09T11:42:32.157203: step 17107, loss 0.579097.
Train: 2018-08-09T11:42:32.235310: step 17108, loss 0.695926.
Train: 2018-08-09T11:42:32.313413: step 17109, loss 0.529076.
Train: 2018-08-09T11:42:32.407144: step 17110, loss 0.512416.
Test: 2018-08-09T11:42:32.891376: step 17110, loss 0.54743.
Train: 2018-08-09T11:42:32.985127: step 17111, loss 0.662498.
Train: 2018-08-09T11:42:33.063239: step 17112, loss 0.47914.
Train: 2018-08-09T11:42:33.141347: step 17113, loss 0.445803.
Train: 2018-08-09T11:42:33.219453: step 17114, loss 0.529047.
Train: 2018-08-09T11:42:33.297554: step 17115, loss 0.612593.
Train: 2018-08-09T11:42:33.391256: step 17116, loss 0.679588.
Train: 2018-08-09T11:42:33.469395: step 17117, loss 0.462065.
Train: 2018-08-09T11:42:33.547471: step 17118, loss 0.57914.
Train: 2018-08-09T11:42:33.625607: step 17119, loss 0.512197.
Train: 2018-08-09T11:42:33.703714: step 17120, loss 0.595894.
Test: 2018-08-09T11:42:34.203566: step 17120, loss 0.547969.
Train: 2018-08-09T11:42:34.281696: step 17121, loss 0.54565.
Train: 2018-08-09T11:42:34.375424: step 17122, loss 0.579148.
Train: 2018-08-09T11:42:34.453536: step 17123, loss 0.595874.
Train: 2018-08-09T11:42:34.531614: step 17124, loss 0.528853.
Train: 2018-08-09T11:42:34.609750: step 17125, loss 0.596085.
Train: 2018-08-09T11:42:34.687827: step 17126, loss 0.646016.
Train: 2018-08-09T11:42:34.781592: step 17127, loss 0.662871.
Train: 2018-08-09T11:42:34.859692: step 17128, loss 0.579093.
Train: 2018-08-09T11:42:34.940055: step 17129, loss 0.545883.
Train: 2018-08-09T11:42:35.018131: step 17130, loss 0.545952.
Test: 2018-08-09T11:42:35.518012: step 17130, loss 0.548297.
Train: 2018-08-09T11:42:35.596143: step 17131, loss 0.579002.
Train: 2018-08-09T11:42:35.674252: step 17132, loss 0.562519.
Train: 2018-08-09T11:42:35.767983: step 17133, loss 0.578972.
Train: 2018-08-09T11:42:35.846060: step 17134, loss 0.46426.
Train: 2018-08-09T11:42:35.924167: step 17135, loss 0.56262.
Train: 2018-08-09T11:42:36.002305: step 17136, loss 0.546216.
Train: 2018-08-09T11:42:36.096042: step 17137, loss 0.546222.
Train: 2018-08-09T11:42:36.174138: step 17138, loss 0.480746.
Train: 2018-08-09T11:42:36.252215: step 17139, loss 0.595354.
Train: 2018-08-09T11:42:36.330322: step 17140, loss 0.611757.
Test: 2018-08-09T11:42:36.830203: step 17140, loss 0.548447.
Train: 2018-08-09T11:42:36.910601: step 17141, loss 0.529786.
Train: 2018-08-09T11:42:36.988739: step 17142, loss 0.562568.
Train: 2018-08-09T11:42:37.066846: step 17143, loss 0.496961.
Train: 2018-08-09T11:42:37.144922: step 17144, loss 0.51329.
Train: 2018-08-09T11:42:37.238680: step 17145, loss 0.628336.
Train: 2018-08-09T11:42:37.316787: step 17146, loss 0.529609.
Train: 2018-08-09T11:42:37.394893: step 17147, loss 0.496619.
Train: 2018-08-09T11:42:37.473000: step 17148, loss 0.612038.
Train: 2018-08-09T11:42:37.566728: step 17149, loss 0.529448.
Train: 2018-08-09T11:42:37.644834: step 17150, loss 0.579024.
Test: 2018-08-09T11:42:38.138709: step 17150, loss 0.548844.
Train: 2018-08-09T11:42:38.216845: step 17151, loss 0.595588.
Train: 2018-08-09T11:42:38.294952: step 17152, loss 0.496241.
Train: 2018-08-09T11:42:38.388652: step 17153, loss 0.545886.
Train: 2018-08-09T11:42:38.466786: step 17154, loss 0.479468.
Train: 2018-08-09T11:42:38.544894: step 17155, loss 0.495882.
Train: 2018-08-09T11:42:38.623001: step 17156, loss 0.562379.
Train: 2018-08-09T11:42:38.701109: step 17157, loss 0.562364.
Train: 2018-08-09T11:42:38.779183: step 17158, loss 0.713555.
Train: 2018-08-09T11:42:38.872942: step 17159, loss 0.579156.
Train: 2018-08-09T11:42:38.951048: step 17160, loss 0.512075.
Test: 2018-08-09T11:42:39.450899: step 17160, loss 0.549851.
Train: 2018-08-09T11:42:39.529007: step 17161, loss 0.662955.
Train: 2018-08-09T11:42:39.607112: step 17162, loss 0.595842.
Train: 2018-08-09T11:42:39.685251: step 17163, loss 0.579108.
Train: 2018-08-09T11:42:39.763325: step 17164, loss 0.529018.
Train: 2018-08-09T11:42:39.857055: step 17165, loss 0.545869.
Train: 2018-08-09T11:42:39.935190: step 17166, loss 0.595762.
Train: 2018-08-09T11:42:40.013299: step 17167, loss 0.529295.
Train: 2018-08-09T11:42:40.091403: step 17168, loss 0.545918.
Train: 2018-08-09T11:42:40.169482: step 17169, loss 0.51278.
Train: 2018-08-09T11:42:40.263232: step 17170, loss 0.645324.
Test: 2018-08-09T11:42:40.747496: step 17170, loss 0.549486.
Train: 2018-08-09T11:42:40.825576: step 17171, loss 0.479762.
Train: 2018-08-09T11:42:40.903713: step 17172, loss 0.496315.
Train: 2018-08-09T11:42:40.999722: step 17173, loss 0.628715.
Train: 2018-08-09T11:42:41.077859: step 17174, loss 0.694944.
Train: 2018-08-09T11:42:41.155935: step 17175, loss 0.612063.
Train: 2018-08-09T11:42:41.249689: step 17176, loss 0.59547.
Train: 2018-08-09T11:42:41.327800: step 17177, loss 0.611828.
Train: 2018-08-09T11:42:41.405906: step 17178, loss 0.57895.
Train: 2018-08-09T11:42:41.484013: step 17179, loss 0.481043.
Train: 2018-08-09T11:42:41.562119: step 17180, loss 0.530065.
Test: 2018-08-09T11:42:42.061997: step 17180, loss 0.552327.
Train: 2018-08-09T11:42:42.140112: step 17181, loss 0.578917.
Train: 2018-08-09T11:42:42.233830: step 17182, loss 0.57891.
Train: 2018-08-09T11:42:42.311937: step 17183, loss 0.578902.
Train: 2018-08-09T11:42:42.390053: step 17184, loss 0.514095.
Train: 2018-08-09T11:42:42.468126: step 17185, loss 0.578898.
Train: 2018-08-09T11:42:42.546264: step 17186, loss 0.465635.
Train: 2018-08-09T11:42:42.639960: step 17187, loss 0.546507.
Train: 2018-08-09T11:42:42.718096: step 17188, loss 0.497842.
Train: 2018-08-09T11:42:42.796203: step 17189, loss 0.465191.
Train: 2018-08-09T11:42:42.874304: step 17190, loss 0.578928.
Test: 2018-08-09T11:42:43.374767: step 17190, loss 0.548514.
Train: 2018-08-09T11:42:43.452872: step 17191, loss 0.546233.
Train: 2018-08-09T11:42:43.531009: step 17192, loss 0.496999.
Train: 2018-08-09T11:42:43.624708: step 17193, loss 0.611878.
Train: 2018-08-09T11:42:43.702844: step 17194, loss 0.628486.
Train: 2018-08-09T11:42:43.780921: step 17195, loss 0.595505.
Train: 2018-08-09T11:42:43.859055: step 17196, loss 0.479943.
Train: 2018-08-09T11:42:43.937164: step 17197, loss 0.612247.
Train: 2018-08-09T11:42:44.030861: step 17198, loss 0.595651.
Train: 2018-08-09T11:42:44.108968: step 17199, loss 0.6948.
Train: 2018-08-09T11:42:44.187106: step 17200, loss 0.595511.
Test: 2018-08-09T11:42:44.686957: step 17200, loss 0.550835.
Train: 2018-08-09T11:42:45.249324: step 17201, loss 0.414355.
Train: 2018-08-09T11:42:45.327461: step 17202, loss 0.57899.
Train: 2018-08-09T11:42:45.421190: step 17203, loss 0.628372.
Train: 2018-08-09T11:42:45.499298: step 17204, loss 0.611854.
Train: 2018-08-09T11:42:45.577402: step 17205, loss 0.562559.
Train: 2018-08-09T11:42:45.655511: step 17206, loss 0.578949.
Train: 2018-08-09T11:42:45.749239: step 17207, loss 0.595302.
Train: 2018-08-09T11:42:45.827343: step 17208, loss 0.595258.
Train: 2018-08-09T11:42:45.907042: step 17209, loss 0.51381.
Train: 2018-08-09T11:42:45.985180: step 17210, loss 0.595165.
Test: 2018-08-09T11:42:46.485057: step 17210, loss 0.547478.
Train: 2018-08-09T11:42:46.563168: step 17211, loss 0.611349.
Train: 2018-08-09T11:42:46.656896: step 17212, loss 0.595078.
Train: 2018-08-09T11:42:46.735004: step 17213, loss 0.562741.
Train: 2018-08-09T11:42:46.813109: step 17214, loss 0.611088.
Train: 2018-08-09T11:42:46.891215: step 17215, loss 0.627051.
Train: 2018-08-09T11:42:46.984945: step 17216, loss 0.482839.
Train: 2018-08-09T11:42:47.063050: step 17217, loss 0.530936.
Train: 2018-08-09T11:42:47.141157: step 17218, loss 0.546944.
Train: 2018-08-09T11:42:47.234886: step 17219, loss 0.610756.
Train: 2018-08-09T11:42:47.312961: step 17220, loss 0.419577.
Test: 2018-08-09T11:42:47.806074: step 17220, loss 0.550379.
Train: 2018-08-09T11:42:47.884154: step 17221, loss 0.499115.
Train: 2018-08-09T11:42:47.977913: step 17222, loss 0.562872.
Train: 2018-08-09T11:42:48.056015: step 17223, loss 0.626937.
Train: 2018-08-09T11:42:48.134126: step 17224, loss 0.530747.
Train: 2018-08-09T11:42:48.212232: step 17225, loss 0.578912.
Train: 2018-08-09T11:42:48.305960: step 17226, loss 0.546742.
Train: 2018-08-09T11:42:48.384069: step 17227, loss 0.627142.
Train: 2018-08-09T11:42:48.462173: step 17228, loss 0.433941.
Train: 2018-08-09T11:42:48.555872: step 17229, loss 0.514296.
Train: 2018-08-09T11:42:48.634010: step 17230, loss 0.579005.
Test: 2018-08-09T11:42:49.133861: step 17230, loss 0.54928.
Train: 2018-08-09T11:42:49.211996: step 17231, loss 0.497651.
Train: 2018-08-09T11:42:49.290103: step 17232, loss 0.530132.
Train: 2018-08-09T11:42:49.368181: step 17233, loss 0.562373.
Train: 2018-08-09T11:42:49.461938: step 17234, loss 0.56256.
Train: 2018-08-09T11:42:49.540045: step 17235, loss 0.545925.
Train: 2018-08-09T11:42:49.618121: step 17236, loss 0.512845.
Train: 2018-08-09T11:42:49.711850: step 17237, loss 0.56177.
Train: 2018-08-09T11:42:49.797119: step 17238, loss 0.61349.
Train: 2018-08-09T11:42:49.868107: step 17239, loss 0.58085.
Train: 2018-08-09T11:42:49.947578: step 17240, loss 0.543362.
Test: 2018-08-09T11:42:50.447455: step 17240, loss 0.547783.
Train: 2018-08-09T11:42:50.525560: step 17241, loss 0.529493.
Train: 2018-08-09T11:42:50.603644: step 17242, loss 0.495068.
Train: 2018-08-09T11:42:50.697402: step 17243, loss 0.544438.
Train: 2018-08-09T11:42:50.775476: step 17244, loss 0.546224.
Train: 2018-08-09T11:42:50.853585: step 17245, loss 0.579904.
Train: 2018-08-09T11:42:50.947343: step 17246, loss 0.594571.
Train: 2018-08-09T11:42:51.025419: step 17247, loss 0.527371.
Train: 2018-08-09T11:42:51.103559: step 17248, loss 0.528125.
Train: 2018-08-09T11:42:51.181663: step 17249, loss 0.596642.
Train: 2018-08-09T11:42:51.275389: step 17250, loss 0.529671.
Test: 2018-08-09T11:42:51.775242: step 17250, loss 0.546723.
Train: 2018-08-09T11:42:51.853378: step 17251, loss 0.542918.
Train: 2018-08-09T11:42:51.932922: step 17252, loss 0.507689.
Train: 2018-08-09T11:42:52.011026: step 17253, loss 0.665212.
Train: 2018-08-09T11:42:52.104762: step 17254, loss 0.618518.
Train: 2018-08-09T11:42:52.182866: step 17255, loss 0.527684.
Train: 2018-08-09T11:42:52.260970: step 17256, loss 0.57889.
Train: 2018-08-09T11:42:52.354698: step 17257, loss 0.544402.
Train: 2018-08-09T11:42:52.432774: step 17258, loss 0.563188.
Train: 2018-08-09T11:42:52.510911: step 17259, loss 0.596559.
Train: 2018-08-09T11:42:52.604608: step 17260, loss 0.578806.
Test: 2018-08-09T11:42:53.088894: step 17260, loss 0.546562.
Train: 2018-08-09T11:42:53.182628: step 17261, loss 0.562258.
Train: 2018-08-09T11:42:53.260734: step 17262, loss 0.511283.
Train: 2018-08-09T11:42:53.338841: step 17263, loss 0.56338.
Train: 2018-08-09T11:42:53.432537: step 17264, loss 0.595009.
Train: 2018-08-09T11:42:53.510675: step 17265, loss 0.57865.
Train: 2018-08-09T11:42:53.588783: step 17266, loss 0.562976.
Train: 2018-08-09T11:42:53.682509: step 17267, loss 0.629966.
Train: 2018-08-09T11:42:53.760616: step 17268, loss 0.628269.
Train: 2018-08-09T11:42:53.838724: step 17269, loss 0.595253.
Train: 2018-08-09T11:42:53.918236: step 17270, loss 0.448582.
Test: 2018-08-09T11:42:54.418152: step 17270, loss 0.548054.
Train: 2018-08-09T11:42:54.496226: step 17271, loss 0.530161.
Train: 2018-08-09T11:42:54.589983: step 17272, loss 0.546481.
Train: 2018-08-09T11:42:54.668059: step 17273, loss 0.481569.
Train: 2018-08-09T11:42:54.746199: step 17274, loss 0.56268.
Train: 2018-08-09T11:42:54.839924: step 17275, loss 0.595142.
Train: 2018-08-09T11:42:54.918002: step 17276, loss 0.611373.
Train: 2018-08-09T11:42:54.996138: step 17277, loss 0.449021.
Train: 2018-08-09T11:42:55.089836: step 17278, loss 0.59526.
Train: 2018-08-09T11:42:55.167942: step 17279, loss 0.644027.
Train: 2018-08-09T11:42:55.246079: step 17280, loss 0.546409.
Test: 2018-08-09T11:42:55.745931: step 17280, loss 0.549272.
Train: 2018-08-09T11:42:55.839658: step 17281, loss 0.595162.
Train: 2018-08-09T11:42:55.919245: step 17282, loss 0.595144.
Train: 2018-08-09T11:42:55.997377: step 17283, loss 0.595114.
Train: 2018-08-09T11:42:56.091080: step 17284, loss 0.659853.
Train: 2018-08-09T11:42:56.169188: step 17285, loss 0.562742.
Train: 2018-08-09T11:42:56.247293: step 17286, loss 0.578876.
Train: 2018-08-09T11:42:56.341051: step 17287, loss 0.56281.
Train: 2018-08-09T11:42:56.419158: step 17288, loss 0.482722.
Train: 2018-08-09T11:42:56.512886: step 17289, loss 0.706969.
Train: 2018-08-09T11:42:56.590963: step 17290, loss 0.562894.
Test: 2018-08-09T11:42:57.090873: step 17290, loss 0.550417.
Train: 2018-08-09T11:42:57.168981: step 17291, loss 0.562931.
Train: 2018-08-09T11:42:57.262679: step 17292, loss 0.578858.
Train: 2018-08-09T11:42:57.340817: step 17293, loss 0.563.
Train: 2018-08-09T11:42:57.418923: step 17294, loss 0.531369.
Train: 2018-08-09T11:42:57.512652: step 17295, loss 0.515603.
Train: 2018-08-09T11:42:57.590727: step 17296, loss 0.563042.
Train: 2018-08-09T11:42:57.668864: step 17297, loss 0.547222.
Train: 2018-08-09T11:42:57.762561: step 17298, loss 0.578851.
Train: 2018-08-09T11:42:57.840668: step 17299, loss 0.56306.
Train: 2018-08-09T11:42:57.920167: step 17300, loss 0.531375.
Test: 2018-08-09T11:42:58.420081: step 17300, loss 0.550546.
Train: 2018-08-09T11:42:58.998068: step 17301, loss 0.658086.
Train: 2018-08-09T11:42:59.076175: step 17302, loss 0.547097.
Train: 2018-08-09T11:42:59.169903: step 17303, loss 0.563086.
Train: 2018-08-09T11:42:59.248010: step 17304, loss 0.499647.
Train: 2018-08-09T11:42:59.326116: step 17305, loss 0.515323.
Train: 2018-08-09T11:42:59.419815: step 17306, loss 0.642623.
Train: 2018-08-09T11:42:59.497951: step 17307, loss 0.54695.
Train: 2018-08-09T11:42:59.576057: step 17308, loss 0.594541.
Train: 2018-08-09T11:42:59.669757: step 17309, loss 0.450834.
Train: 2018-08-09T11:42:59.747892: step 17310, loss 0.611307.
Test: 2018-08-09T11:43:00.249077: step 17310, loss 0.551947.
Train: 2018-08-09T11:43:00.327212: step 17311, loss 0.610329.
Train: 2018-08-09T11:43:00.420941: step 17312, loss 0.497817.
Train: 2018-08-09T11:43:00.499051: step 17313, loss 0.692935.
Train: 2018-08-09T11:43:00.577124: step 17314, loss 0.516102.
Train: 2018-08-09T11:43:00.670882: step 17315, loss 0.609754.
Train: 2018-08-09T11:43:00.748992: step 17316, loss 0.497204.
Train: 2018-08-09T11:43:00.842687: step 17317, loss 0.594368.
Train: 2018-08-09T11:43:00.920793: step 17318, loss 0.547666.
Train: 2018-08-09T11:43:00.998932: step 17319, loss 0.562761.
Train: 2018-08-09T11:43:01.092657: step 17320, loss 0.545893.
Test: 2018-08-09T11:43:01.576888: step 17320, loss 0.549173.
Train: 2018-08-09T11:43:01.670648: step 17321, loss 0.481261.
Train: 2018-08-09T11:43:01.748755: step 17322, loss 0.580491.
Train: 2018-08-09T11:43:01.842481: step 17323, loss 0.631335.
Train: 2018-08-09T11:43:01.920587: step 17324, loss 0.512842.
Train: 2018-08-09T11:43:01.998697: step 17325, loss 0.513664.
Train: 2018-08-09T11:43:02.092392: step 17326, loss 0.545829.
Train: 2018-08-09T11:43:02.170527: step 17327, loss 0.612443.
Train: 2018-08-09T11:43:02.264256: step 17328, loss 0.563846.
Train: 2018-08-09T11:43:02.342364: step 17329, loss 0.578075.
Train: 2018-08-09T11:43:02.436087: step 17330, loss 0.464688.
Test: 2018-08-09T11:43:02.938342: step 17330, loss 0.549757.
Train: 2018-08-09T11:43:03.016479: step 17331, loss 0.579526.
Train: 2018-08-09T11:43:03.094589: step 17332, loss 0.480414.
Train: 2018-08-09T11:43:03.188283: step 17333, loss 0.497401.
Train: 2018-08-09T11:43:03.266419: step 17334, loss 0.496035.
Train: 2018-08-09T11:43:03.360142: step 17335, loss 0.562222.
Train: 2018-08-09T11:43:03.438224: step 17336, loss 0.66596.
Train: 2018-08-09T11:43:03.531982: step 17337, loss 0.578821.
Train: 2018-08-09T11:43:03.610088: step 17338, loss 0.529636.
Train: 2018-08-09T11:43:03.688196: step 17339, loss 0.613947.
Train: 2018-08-09T11:43:03.781922: step 17340, loss 0.511948.
Test: 2018-08-09T11:43:04.281776: step 17340, loss 0.546593.
Train: 2018-08-09T11:43:04.359912: step 17341, loss 0.529253.
Train: 2018-08-09T11:43:04.438018: step 17342, loss 0.579522.
Train: 2018-08-09T11:43:04.531747: step 17343, loss 0.595559.
Train: 2018-08-09T11:43:04.609822: step 17344, loss 0.646133.
Train: 2018-08-09T11:43:04.703581: step 17345, loss 0.595267.
Train: 2018-08-09T11:43:04.781690: step 17346, loss 0.513259.
Train: 2018-08-09T11:43:04.875415: step 17347, loss 0.612308.
Train: 2018-08-09T11:43:04.955762: step 17348, loss 0.546071.
Train: 2018-08-09T11:43:05.033898: step 17349, loss 0.595338.
Train: 2018-08-09T11:43:05.127627: step 17350, loss 0.529919.
Test: 2018-08-09T11:43:05.627479: step 17350, loss 0.54733.
Train: 2018-08-09T11:43:05.705585: step 17351, loss 0.529986.
Train: 2018-08-09T11:43:05.799312: step 17352, loss 0.497423.
Train: 2018-08-09T11:43:05.877443: step 17353, loss 0.562592.
Train: 2018-08-09T11:43:05.971177: step 17354, loss 0.562633.
Train: 2018-08-09T11:43:06.049285: step 17355, loss 0.546194.
Train: 2018-08-09T11:43:06.143013: step 17356, loss 0.595235.
Train: 2018-08-09T11:43:06.221118: step 17357, loss 0.56265.
Train: 2018-08-09T11:43:06.299226: step 17358, loss 0.54629.
Train: 2018-08-09T11:43:06.392954: step 17359, loss 0.628033.
Train: 2018-08-09T11:43:06.471028: step 17360, loss 0.513465.
Test: 2018-08-09T11:43:06.972280: step 17360, loss 0.548472.
Train: 2018-08-09T11:43:07.050386: step 17361, loss 0.628113.
Train: 2018-08-09T11:43:07.144083: step 17362, loss 0.595329.
Train: 2018-08-09T11:43:07.237843: step 17363, loss 0.513504.
Train: 2018-08-09T11:43:07.315917: step 17364, loss 0.546093.
Train: 2018-08-09T11:43:07.409645: step 17365, loss 0.611599.
Train: 2018-08-09T11:43:07.487783: step 17366, loss 0.56308.
Train: 2018-08-09T11:43:07.581510: step 17367, loss 0.513118.
Train: 2018-08-09T11:43:07.659586: step 17368, loss 0.660581.
Train: 2018-08-09T11:43:07.753345: step 17369, loss 0.611464.
Train: 2018-08-09T11:43:07.831436: step 17370, loss 0.709108.
Test: 2018-08-09T11:43:08.331303: step 17370, loss 0.548734.
Train: 2018-08-09T11:43:08.425030: step 17371, loss 0.56278.
Train: 2018-08-09T11:43:08.503170: step 17372, loss 0.546526.
Train: 2018-08-09T11:43:08.596897: step 17373, loss 0.578908.
Train: 2018-08-09T11:43:08.675002: step 17374, loss 0.530868.
Train: 2018-08-09T11:43:08.768730: step 17375, loss 0.690618.
Train: 2018-08-09T11:43:08.862454: step 17376, loss 0.467737.
Train: 2018-08-09T11:43:08.941914: step 17377, loss 0.483826.
Train: 2018-08-09T11:43:09.026664: step 17378, loss 0.483868.
Train: 2018-08-09T11:43:09.120422: step 17379, loss 0.515461.
Train: 2018-08-09T11:43:09.198500: step 17380, loss 0.658255.
Test: 2018-08-09T11:43:09.698381: step 17380, loss 0.552898.
Train: 2018-08-09T11:43:09.792109: step 17381, loss 0.578858.
Train: 2018-08-09T11:43:09.870246: step 17382, loss 0.578858.
Train: 2018-08-09T11:43:09.963974: step 17383, loss 0.578858.
Train: 2018-08-09T11:43:10.042074: step 17384, loss 0.499473.
Train: 2018-08-09T11:43:10.135808: step 17385, loss 0.626532.
Train: 2018-08-09T11:43:10.213885: step 17386, loss 0.610635.
Train: 2018-08-09T11:43:10.307613: step 17387, loss 0.531232.
Train: 2018-08-09T11:43:10.385749: step 17388, loss 0.547111.
Train: 2018-08-09T11:43:10.479471: step 17389, loss 0.594735.
Train: 2018-08-09T11:43:10.557587: step 17390, loss 0.547112.
Test: 2018-08-09T11:43:11.057436: step 17390, loss 0.549907.
Train: 2018-08-09T11:43:11.151195: step 17391, loss 0.467722.
Train: 2018-08-09T11:43:11.229300: step 17392, loss 0.658387.
Train: 2018-08-09T11:43:11.322999: step 17393, loss 0.531139.
Train: 2018-08-09T11:43:11.401136: step 17394, loss 0.467416.
Train: 2018-08-09T11:43:11.494863: step 17395, loss 0.483112.
Train: 2018-08-09T11:43:11.572940: step 17396, loss 0.562851.
Train: 2018-08-09T11:43:11.666668: step 17397, loss 0.643112.
Train: 2018-08-09T11:43:11.744804: step 17398, loss 0.546698.
Train: 2018-08-09T11:43:11.838533: step 17399, loss 0.611127.
Train: 2018-08-09T11:43:11.932261: step 17400, loss 0.482069.
Test: 2018-08-09T11:43:12.432112: step 17400, loss 0.550012.
Train: 2018-08-09T11:43:12.948988: step 17401, loss 0.546567.
Train: 2018-08-09T11:43:13.042746: step 17402, loss 0.514097.
Train: 2018-08-09T11:43:13.120853: step 17403, loss 0.546433.
Train: 2018-08-09T11:43:13.214580: step 17404, loss 0.529991.
Train: 2018-08-09T11:43:13.292656: step 17405, loss 0.644336.
Train: 2018-08-09T11:43:13.386414: step 17406, loss 0.497161.
Train: 2018-08-09T11:43:13.464522: step 17407, loss 0.480354.
Train: 2018-08-09T11:43:13.558249: step 17408, loss 0.562564.
Train: 2018-08-09T11:43:13.636361: step 17409, loss 0.562741.
Train: 2018-08-09T11:43:13.730084: step 17410, loss 0.54568.
Test: 2018-08-09T11:43:14.229935: step 17410, loss 0.546161.
Train: 2018-08-09T11:43:14.308072: step 17411, loss 0.512466.
Train: 2018-08-09T11:43:14.401801: step 17412, loss 0.562414.
Train: 2018-08-09T11:43:14.479878: step 17413, loss 0.527892.
Train: 2018-08-09T11:43:14.573604: step 17414, loss 0.511177.
Train: 2018-08-09T11:43:14.651741: step 17415, loss 0.477052.
Train: 2018-08-09T11:43:14.745469: step 17416, loss 0.476333.
Train: 2018-08-09T11:43:14.823577: step 17417, loss 0.638746.
Train: 2018-08-09T11:43:14.919611: step 17418, loss 0.580566.
Train: 2018-08-09T11:43:14.997748: step 17419, loss 0.495168.
Train: 2018-08-09T11:43:15.091476: step 17420, loss 0.529137.
Test: 2018-08-09T11:43:15.591328: step 17420, loss 0.546572.
Train: 2018-08-09T11:43:15.669434: step 17421, loss 0.581211.
Train: 2018-08-09T11:43:15.763193: step 17422, loss 0.633776.
Train: 2018-08-09T11:43:15.841270: step 17423, loss 0.614322.
Train: 2018-08-09T11:43:15.935027: step 17424, loss 0.493264.
Train: 2018-08-09T11:43:16.028726: step 17425, loss 0.476236.
Train: 2018-08-09T11:43:16.106862: step 17426, loss 0.511218.
Train: 2018-08-09T11:43:16.200559: step 17427, loss 0.562232.
Train: 2018-08-09T11:43:16.278666: step 17428, loss 0.544995.
Train: 2018-08-09T11:43:16.372395: step 17429, loss 0.545101.
Train: 2018-08-09T11:43:16.450531: step 17430, loss 0.648813.
Test: 2018-08-09T11:43:16.951722: step 17430, loss 0.550112.
Train: 2018-08-09T11:43:17.045480: step 17431, loss 0.579314.
Train: 2018-08-09T11:43:17.123557: step 17432, loss 0.544972.
Train: 2018-08-09T11:43:17.217314: step 17433, loss 0.562365.
Train: 2018-08-09T11:43:17.295421: step 17434, loss 0.579567.
Train: 2018-08-09T11:43:17.389149: step 17435, loss 0.545281.
Train: 2018-08-09T11:43:17.467256: step 17436, loss 0.493772.
Train: 2018-08-09T11:43:17.560953: step 17437, loss 0.579476.
Train: 2018-08-09T11:43:17.654711: step 17438, loss 0.528074.
Train: 2018-08-09T11:43:17.732788: step 17439, loss 0.613983.
Train: 2018-08-09T11:43:17.826545: step 17440, loss 0.61366.
Test: 2018-08-09T11:43:18.326429: step 17440, loss 0.547651.
Train: 2018-08-09T11:43:18.404537: step 17441, loss 0.511196.
Train: 2018-08-09T11:43:18.498265: step 17442, loss 0.647547.
Train: 2018-08-09T11:43:18.576371: step 17443, loss 0.443441.
Train: 2018-08-09T11:43:18.670097: step 17444, loss 0.630267.
Train: 2018-08-09T11:43:18.748206: step 17445, loss 0.528447.
Train: 2018-08-09T11:43:18.841932: step 17446, loss 0.6809.
Train: 2018-08-09T11:43:18.936297: step 17447, loss 0.579255.
Train: 2018-08-09T11:43:19.014404: step 17448, loss 0.427652.
Train: 2018-08-09T11:43:19.108132: step 17449, loss 0.562379.
Train: 2018-08-09T11:43:19.186238: step 17450, loss 0.646466.
Test: 2018-08-09T11:43:19.686121: step 17450, loss 0.550474.
Train: 2018-08-09T11:43:19.779878: step 17451, loss 0.579175.
Train: 2018-08-09T11:43:19.857985: step 17452, loss 0.629394.
Train: 2018-08-09T11:43:19.958785: step 17453, loss 0.545721.
Train: 2018-08-09T11:43:20.036921: step 17454, loss 0.629059.
Train: 2018-08-09T11:43:20.130644: step 17455, loss 0.579059.
Train: 2018-08-09T11:43:20.208757: step 17456, loss 0.512848.
Train: 2018-08-09T11:43:20.302479: step 17457, loss 0.545995.
Train: 2018-08-09T11:43:20.380567: step 17458, loss 0.544944.
Train: 2018-08-09T11:43:20.474319: step 17459, loss 0.562532.
Train: 2018-08-09T11:43:20.568046: step 17460, loss 0.628265.
Test: 2018-08-09T11:43:21.067924: step 17460, loss 0.548445.
Train: 2018-08-09T11:43:21.146030: step 17461, loss 0.496991.
Train: 2018-08-09T11:43:21.239757: step 17462, loss 0.497068.
Train: 2018-08-09T11:43:21.333461: step 17463, loss 0.611714.
Train: 2018-08-09T11:43:21.411598: step 17464, loss 0.52985.
Train: 2018-08-09T11:43:21.505328: step 17465, loss 0.59531.
Train: 2018-08-09T11:43:21.583432: step 17466, loss 0.628002.
Train: 2018-08-09T11:43:21.677129: step 17467, loss 0.529952.
Train: 2018-08-09T11:43:21.770857: step 17468, loss 0.578933.
Train: 2018-08-09T11:43:21.848995: step 17469, loss 0.562631.
Train: 2018-08-09T11:43:21.942723: step 17470, loss 0.56264.
Test: 2018-08-09T11:43:22.441533: step 17470, loss 0.550488.
Train: 2018-08-09T11:43:22.519668: step 17471, loss 0.481336.
Train: 2018-08-09T11:43:22.613367: step 17472, loss 0.562647.
Train: 2018-08-09T11:43:22.707119: step 17473, loss 0.546362.
Train: 2018-08-09T11:43:22.785232: step 17474, loss 0.562633.
Train: 2018-08-09T11:43:22.878959: step 17475, loss 0.627822.
Train: 2018-08-09T11:43:22.957036: step 17476, loss 0.611506.
Train: 2018-08-09T11:43:23.050794: step 17477, loss 0.530103.
Train: 2018-08-09T11:43:23.128870: step 17478, loss 0.595175.
Train: 2018-08-09T11:43:23.222627: step 17479, loss 0.595156.
Train: 2018-08-09T11:43:23.316353: step 17480, loss 0.578903.
Test: 2018-08-09T11:43:23.800587: step 17480, loss 0.550583.
Train: 2018-08-09T11:43:23.894344: step 17481, loss 0.643689.
Train: 2018-08-09T11:43:23.974808: step 17482, loss 0.546574.
Train: 2018-08-09T11:43:24.068535: step 17483, loss 0.546634.
Train: 2018-08-09T11:43:24.146644: step 17484, loss 0.562779.
Train: 2018-08-09T11:43:24.240340: step 17485, loss 0.675329.
Train: 2018-08-09T11:43:24.334093: step 17486, loss 0.530781.
Train: 2018-08-09T11:43:24.412173: step 17487, loss 0.546875.
Train: 2018-08-09T11:43:24.505932: step 17488, loss 0.578862.
Train: 2018-08-09T11:43:24.599660: step 17489, loss 0.54697.
Train: 2018-08-09T11:43:24.677737: step 17490, loss 0.594783.
Test: 2018-08-09T11:43:25.177619: step 17490, loss 0.55046.
Train: 2018-08-09T11:43:25.271346: step 17491, loss 0.721987.
Train: 2018-08-09T11:43:25.349453: step 17492, loss 0.483803.
Train: 2018-08-09T11:43:25.443181: step 17493, loss 0.578861.
Train: 2018-08-09T11:43:25.536939: step 17494, loss 0.563088.
Train: 2018-08-09T11:43:25.615047: step 17495, loss 0.626125.
Train: 2018-08-09T11:43:25.708744: step 17496, loss 0.594586.
Train: 2018-08-09T11:43:25.802496: step 17497, loss 0.578877.
Train: 2018-08-09T11:43:25.880579: step 17498, loss 0.547615.
Train: 2018-08-09T11:43:25.976692: step 17499, loss 0.703743.
Train: 2018-08-09T11:43:26.054800: step 17500, loss 0.609999.
Test: 2018-08-09T11:43:26.554652: step 17500, loss 0.550692.
Train: 2018-08-09T11:43:27.195155: step 17501, loss 0.547959.
Train: 2018-08-09T11:43:27.273265: step 17502, loss 0.532646.
Train: 2018-08-09T11:43:27.366993: step 17503, loss 0.471167.
Train: 2018-08-09T11:43:27.460713: step 17504, loss 0.548165.
Train: 2018-08-09T11:43:27.538793: step 17505, loss 0.578953.
Train: 2018-08-09T11:43:27.632522: step 17506, loss 0.486531.
Train: 2018-08-09T11:43:27.726280: step 17507, loss 0.548075.
Train: 2018-08-09T11:43:27.804387: step 17508, loss 0.594381.
Train: 2018-08-09T11:43:27.898086: step 17509, loss 0.563417.
Train: 2018-08-09T11:43:27.977567: step 17510, loss 0.578881.
Test: 2018-08-09T11:43:28.477420: step 17510, loss 0.549955.
Train: 2018-08-09T11:43:28.571147: step 17511, loss 0.501117.
Train: 2018-08-09T11:43:28.664907: step 17512, loss 0.547548.
Train: 2018-08-09T11:43:28.743015: step 17513, loss 0.641776.
Train: 2018-08-09T11:43:28.836740: step 17514, loss 0.610734.
Train: 2018-08-09T11:43:28.914848: step 17515, loss 0.610464.
Train: 2018-08-09T11:43:29.008546: step 17516, loss 0.516311.
Train: 2018-08-09T11:43:29.102273: step 17517, loss 0.516193.
Train: 2018-08-09T11:43:29.180410: step 17518, loss 0.578861.
Train: 2018-08-09T11:43:29.274106: step 17519, loss 0.547485.
Train: 2018-08-09T11:43:29.352244: step 17520, loss 0.578818.
Test: 2018-08-09T11:43:29.852112: step 17520, loss 0.550789.
Train: 2018-08-09T11:43:29.932519: step 17521, loss 0.531713.
Train: 2018-08-09T11:43:30.026276: step 17522, loss 0.626098.
Train: 2018-08-09T11:43:30.120004: step 17523, loss 0.56308.
Train: 2018-08-09T11:43:30.198112: step 17524, loss 0.531486.
Train: 2018-08-09T11:43:30.291839: step 17525, loss 0.578807.
Train: 2018-08-09T11:43:30.385537: step 17526, loss 0.468005.
Train: 2018-08-09T11:43:30.463673: step 17527, loss 0.467562.
Train: 2018-08-09T11:43:30.557402: step 17528, loss 0.466375.
Train: 2018-08-09T11:43:30.635509: step 17529, loss 0.562147.
Train: 2018-08-09T11:43:30.729235: step 17530, loss 0.597476.
Test: 2018-08-09T11:43:31.229089: step 17530, loss 0.551215.
Train: 2018-08-09T11:43:31.322840: step 17531, loss 0.513655.
Train: 2018-08-09T11:43:31.400921: step 17532, loss 0.615829.
Train: 2018-08-09T11:43:31.494680: step 17533, loss 0.528751.
Train: 2018-08-09T11:43:31.572787: step 17534, loss 0.629291.
Train: 2018-08-09T11:43:31.666485: step 17535, loss 0.543267.
Train: 2018-08-09T11:43:31.760243: step 17536, loss 0.648178.
Train: 2018-08-09T11:43:31.838319: step 17537, loss 0.462932.
Train: 2018-08-09T11:43:31.965625: step 17538, loss 0.595376.
Train: 2018-08-09T11:43:32.043731: step 17539, loss 0.54568.
Train: 2018-08-09T11:43:32.137463: step 17540, loss 0.546023.
Test: 2018-08-09T11:43:32.633336: step 17540, loss 0.549414.
Train: 2018-08-09T11:43:32.727060: step 17541, loss 0.579459.
Train: 2018-08-09T11:43:32.805165: step 17542, loss 0.479311.
Train: 2018-08-09T11:43:32.898863: step 17543, loss 0.513477.
Train: 2018-08-09T11:43:32.992623: step 17544, loss 0.528859.
Train: 2018-08-09T11:43:33.070727: step 17545, loss 0.563547.
Train: 2018-08-09T11:43:33.164456: step 17546, loss 0.511928.
Train: 2018-08-09T11:43:33.242562: step 17547, loss 0.545398.
Train: 2018-08-09T11:43:33.336290: step 17548, loss 0.579774.
Train: 2018-08-09T11:43:33.430012: step 17549, loss 0.544792.
Train: 2018-08-09T11:43:33.508121: step 17550, loss 0.596102.
Test: 2018-08-09T11:43:34.014268: step 17550, loss 0.547769.
Train: 2018-08-09T11:43:34.092406: step 17551, loss 0.563874.
Train: 2018-08-09T11:43:34.186132: step 17552, loss 0.561051.
Train: 2018-08-09T11:43:34.264240: step 17553, loss 0.681242.
Train: 2018-08-09T11:43:34.357968: step 17554, loss 0.630764.
Train: 2018-08-09T11:43:34.451696: step 17555, loss 0.495657.
Train: 2018-08-09T11:43:34.529802: step 17556, loss 0.495517.
Train: 2018-08-09T11:43:34.623530: step 17557, loss 0.678899.
Train: 2018-08-09T11:43:34.701606: step 17558, loss 0.61237.
Train: 2018-08-09T11:43:34.795365: step 17559, loss 0.512852.
Train: 2018-08-09T11:43:34.889086: step 17560, loss 0.479875.
Test: 2018-08-09T11:43:35.373324: step 17560, loss 0.547648.
Train: 2018-08-09T11:43:35.467051: step 17561, loss 0.479892.
Train: 2018-08-09T11:43:35.560809: step 17562, loss 0.579024.
Train: 2018-08-09T11:43:35.638915: step 17563, loss 0.529422.
Train: 2018-08-09T11:43:35.732644: step 17564, loss 0.496286.
Train: 2018-08-09T11:43:35.810720: step 17565, loss 0.529318.
Train: 2018-08-09T11:43:35.904478: step 17566, loss 0.512602.
Train: 2018-08-09T11:43:35.998176: step 17567, loss 0.612406.
Train: 2018-08-09T11:43:36.076309: step 17568, loss 0.629131.
Train: 2018-08-09T11:43:36.170040: step 17569, loss 0.51238.
Train: 2018-08-09T11:43:36.263768: step 17570, loss 0.529014.
Test: 2018-08-09T11:43:36.748024: step 17570, loss 0.548008.
Train: 2018-08-09T11:43:36.841727: step 17571, loss 0.579151.
Train: 2018-08-09T11:43:36.937781: step 17572, loss 0.595856.
Train: 2018-08-09T11:43:37.015888: step 17573, loss 0.545659.
Train: 2018-08-09T11:43:37.109586: step 17574, loss 0.545657.
Train: 2018-08-09T11:43:37.187723: step 17575, loss 0.64622.
Train: 2018-08-09T11:43:37.281450: step 17576, loss 0.461964.
Train: 2018-08-09T11:43:37.375180: step 17577, loss 0.545653.
Train: 2018-08-09T11:43:37.453284: step 17578, loss 0.512244.
Train: 2018-08-09T11:43:37.547012: step 17579, loss 0.528844.
Train: 2018-08-09T11:43:37.625120: step 17580, loss 0.680017.
Test: 2018-08-09T11:43:38.125003: step 17580, loss 0.546658.
Train: 2018-08-09T11:43:38.218729: step 17581, loss 0.545588.
Train: 2018-08-09T11:43:38.296836: step 17582, loss 0.629518.
Train: 2018-08-09T11:43:38.390534: step 17583, loss 0.612701.
Train: 2018-08-09T11:43:38.484261: step 17584, loss 0.562437.
Train: 2018-08-09T11:43:38.562398: step 17585, loss 0.512344.
Train: 2018-08-09T11:43:38.656097: step 17586, loss 0.545749.
Train: 2018-08-09T11:43:38.734236: step 17587, loss 0.595759.
Train: 2018-08-09T11:43:38.827930: step 17588, loss 0.579084.
Train: 2018-08-09T11:43:38.907391: step 17589, loss 0.628895.
Train: 2018-08-09T11:43:39.001150: step 17590, loss 0.612188.
Test: 2018-08-09T11:43:39.501003: step 17590, loss 0.548888.
Train: 2018-08-09T11:43:39.579109: step 17591, loss 0.545969.
Train: 2018-08-09T11:43:39.672866: step 17592, loss 0.496589.
Train: 2018-08-09T11:43:39.766598: step 17593, loss 0.57899.
Train: 2018-08-09T11:43:39.844701: step 17594, loss 0.513229.
Train: 2018-08-09T11:43:39.938429: step 17595, loss 0.562545.
Train: 2018-08-09T11:43:40.016540: step 17596, loss 0.64465.
Train: 2018-08-09T11:43:40.115843: step 17597, loss 0.611741.
Train: 2018-08-09T11:43:40.193981: step 17598, loss 0.644354.
Train: 2018-08-09T11:43:40.287679: step 17599, loss 0.562628.
Train: 2018-08-09T11:43:40.381438: step 17600, loss 0.51392.
Test: 2018-08-09T11:43:40.865700: step 17600, loss 0.55301.
Train: 2018-08-09T11:43:41.412444: step 17601, loss 0.562687.
Train: 2018-08-09T11:43:41.506142: step 17602, loss 0.562715.
Train: 2018-08-09T11:43:41.599871: step 17603, loss 0.611207.
Train: 2018-08-09T11:43:41.678006: step 17604, loss 0.530501.
Train: 2018-08-09T11:43:41.771734: step 17605, loss 0.562762.
Train: 2018-08-09T11:43:41.849841: step 17606, loss 0.482363.
Train: 2018-08-09T11:43:41.943571: step 17607, loss 0.627132.
Train: 2018-08-09T11:43:42.021676: step 17608, loss 0.514596.
Train: 2018-08-09T11:43:42.115404: step 17609, loss 0.643156.
Train: 2018-08-09T11:43:42.209134: step 17610, loss 0.627064.
Test: 2018-08-09T11:43:42.708983: step 17610, loss 0.547804.
Train: 2018-08-09T11:43:42.787089: step 17611, loss 0.594911.
Train: 2018-08-09T11:43:42.880849: step 17612, loss 0.466896.
Train: 2018-08-09T11:43:42.960279: step 17613, loss 0.626866.
Train: 2018-08-09T11:43:43.054005: step 17614, loss 0.562883.
Train: 2018-08-09T11:43:43.132142: step 17615, loss 0.531019.
Train: 2018-08-09T11:43:43.225870: step 17616, loss 0.626675.
Train: 2018-08-09T11:43:43.303946: step 17617, loss 0.531108.
Train: 2018-08-09T11:43:43.397674: step 17618, loss 0.515229.
Train: 2018-08-09T11:43:43.475782: step 17619, loss 0.674334.
Train: 2018-08-09T11:43:43.569540: step 17620, loss 0.642417.
Test: 2018-08-09T11:43:44.069391: step 17620, loss 0.549957.
Train: 2018-08-09T11:43:44.241260: step 17621, loss 0.53131.
Train: 2018-08-09T11:43:44.319334: step 17622, loss 0.626332.
Train: 2018-08-09T11:43:44.413091: step 17623, loss 0.673576.
Train: 2018-08-09T11:43:44.491166: step 17624, loss 0.484525.
Train: 2018-08-09T11:43:44.584925: step 17625, loss 0.547491.
Train: 2018-08-09T11:43:44.678624: step 17626, loss 0.516205.
Train: 2018-08-09T11:43:44.756760: step 17627, loss 0.610203.
Train: 2018-08-09T11:43:44.850487: step 17628, loss 0.516297.
Train: 2018-08-09T11:43:44.931132: step 17629, loss 0.500648.
Train: 2018-08-09T11:43:45.024859: step 17630, loss 0.563212.
Test: 2018-08-09T11:43:45.509120: step 17630, loss 0.549086.
Train: 2018-08-09T11:43:45.602848: step 17631, loss 0.547504.
Train: 2018-08-09T11:43:45.696546: step 17632, loss 0.563162.
Train: 2018-08-09T11:43:45.774682: step 17633, loss 0.563136.
Train: 2018-08-09T11:43:45.868410: step 17634, loss 0.531598.
Train: 2018-08-09T11:43:45.946517: step 17635, loss 0.531502.
Train: 2018-08-09T11:43:46.040244: step 17636, loss 0.54721.
Train: 2018-08-09T11:43:46.118322: step 17637, loss 0.499533.
Train: 2018-08-09T11:43:46.212050: step 17638, loss 0.499262.
Train: 2018-08-09T11:43:46.305808: step 17639, loss 0.594848.
Train: 2018-08-09T11:43:46.383916: step 17640, loss 0.530751.
Test: 2018-08-09T11:43:46.883766: step 17640, loss 0.547698.
Train: 2018-08-09T11:43:46.964265: step 17641, loss 0.54668.
Train: 2018-08-09T11:43:47.057993: step 17642, loss 0.546579.
Train: 2018-08-09T11:43:47.136094: step 17643, loss 0.465447.
Train: 2018-08-09T11:43:47.229798: step 17644, loss 0.497476.
Train: 2018-08-09T11:43:47.323555: step 17645, loss 0.628087.
Train: 2018-08-09T11:43:47.401663: step 17646, loss 0.628287.
Train: 2018-08-09T11:43:47.495390: step 17647, loss 0.611888.
Train: 2018-08-09T11:43:47.573499: step 17648, loss 0.513062.
Train: 2018-08-09T11:43:47.667225: step 17649, loss 0.579023.
Train: 2018-08-09T11:43:47.760923: step 17650, loss 0.545994.
Test: 2018-08-09T11:43:48.245183: step 17650, loss 0.548209.
Train: 2018-08-09T11:43:48.338941: step 17651, loss 0.529316.
Train: 2018-08-09T11:43:48.417017: step 17652, loss 0.52928.
Train: 2018-08-09T11:43:48.510777: step 17653, loss 0.579108.
Train: 2018-08-09T11:43:48.604504: step 17654, loss 0.545698.
Train: 2018-08-09T11:43:48.682581: step 17655, loss 0.579212.
Train: 2018-08-09T11:43:48.776309: step 17656, loss 0.595689.
Train: 2018-08-09T11:43:48.854415: step 17657, loss 0.629196.
Train: 2018-08-09T11:43:48.950433: step 17658, loss 0.512165.
Train: 2018-08-09T11:43:49.028539: step 17659, loss 0.59589.
Train: 2018-08-09T11:43:49.122297: step 17660, loss 0.512352.
Test: 2018-08-09T11:43:49.606559: step 17660, loss 0.548617.
Train: 2018-08-09T11:43:49.700285: step 17661, loss 0.528987.
Train: 2018-08-09T11:43:49.793985: step 17662, loss 0.629523.
Train: 2018-08-09T11:43:49.872120: step 17663, loss 0.595662.
Train: 2018-08-09T11:43:49.965848: step 17664, loss 0.595388.
Train: 2018-08-09T11:43:50.043954: step 17665, loss 0.462193.
Train: 2018-08-09T11:43:50.137683: step 17666, loss 0.662891.
Train: 2018-08-09T11:43:50.215759: step 17667, loss 0.545817.
Train: 2018-08-09T11:43:50.309517: step 17668, loss 0.41248.
Train: 2018-08-09T11:43:50.387623: step 17669, loss 0.562721.
Train: 2018-08-09T11:43:50.481322: step 17670, loss 0.546609.
Test: 2018-08-09T11:43:50.966888: step 17670, loss 0.549876.
Train: 2018-08-09T11:43:51.060613: step 17671, loss 0.562169.
Train: 2018-08-09T11:43:51.138750: step 17672, loss 0.596419.
Train: 2018-08-09T11:43:51.232481: step 17673, loss 0.512679.
Train: 2018-08-09T11:43:51.310586: step 17674, loss 0.562581.
Train: 2018-08-09T11:43:51.404313: step 17675, loss 0.478814.
Train: 2018-08-09T11:43:51.482389: step 17676, loss 0.595751.
Train: 2018-08-09T11:43:51.576147: step 17677, loss 0.462192.
Train: 2018-08-09T11:43:51.654224: step 17678, loss 0.612751.
Train: 2018-08-09T11:43:51.747984: step 17679, loss 0.596082.
Train: 2018-08-09T11:43:51.841682: step 17680, loss 0.495326.
Test: 2018-08-09T11:43:52.341563: step 17680, loss 0.547927.
Train: 2018-08-09T11:43:52.420368: step 17681, loss 0.595977.
Train: 2018-08-09T11:43:52.514096: step 17682, loss 0.562296.
Train: 2018-08-09T11:43:52.592203: step 17683, loss 0.579258.
Train: 2018-08-09T11:43:52.685930: step 17684, loss 0.562315.
Train: 2018-08-09T11:43:52.764036: step 17685, loss 0.596018.
Train: 2018-08-09T11:43:52.857736: step 17686, loss 0.511978.
Train: 2018-08-09T11:43:52.952145: step 17687, loss 0.444934.
Train: 2018-08-09T11:43:53.030223: step 17688, loss 0.545596.
Train: 2018-08-09T11:43:53.123983: step 17689, loss 0.612811.
Train: 2018-08-09T11:43:53.202087: step 17690, loss 0.545416.
Test: 2018-08-09T11:43:53.701972: step 17690, loss 0.54781.
Train: 2018-08-09T11:43:53.795697: step 17691, loss 0.596157.
Train: 2018-08-09T11:43:53.873802: step 17692, loss 0.56227.
Train: 2018-08-09T11:43:53.967502: step 17693, loss 0.52839.
Train: 2018-08-09T11:43:54.045607: step 17694, loss 0.511512.
Train: 2018-08-09T11:43:54.139368: step 17695, loss 0.563119.
Train: 2018-08-09T11:43:54.217474: step 17696, loss 0.510873.
Train: 2018-08-09T11:43:54.311202: step 17697, loss 0.613223.
Train: 2018-08-09T11:43:54.389307: step 17698, loss 0.545039.
Train: 2018-08-09T11:43:54.483035: step 17699, loss 0.561899.
Train: 2018-08-09T11:43:54.576757: step 17700, loss 0.667838.
Test: 2018-08-09T11:43:55.074652: step 17700, loss 0.548818.
Train: 2018-08-09T11:43:55.652634: step 17701, loss 0.562572.
Train: 2018-08-09T11:43:55.730740: step 17702, loss 0.630983.
Train: 2018-08-09T11:43:55.824497: step 17703, loss 0.511083.
Train: 2018-08-09T11:43:55.918197: step 17704, loss 0.443877.
Train: 2018-08-09T11:43:55.996302: step 17705, loss 0.613159.
Train: 2018-08-09T11:43:56.090061: step 17706, loss 0.545402.
Train: 2018-08-09T11:43:56.168137: step 17707, loss 0.679939.
Train: 2018-08-09T11:43:56.261895: step 17708, loss 0.545627.
Train: 2018-08-09T11:43:56.340003: step 17709, loss 0.56236.
Train: 2018-08-09T11:43:56.433730: step 17710, loss 0.562503.
Test: 2018-08-09T11:43:56.933582: step 17710, loss 0.546854.
Train: 2018-08-09T11:43:57.011718: step 17711, loss 0.529053.
Train: 2018-08-09T11:43:57.105417: step 17712, loss 0.695381.
Train: 2018-08-09T11:43:57.183554: step 17713, loss 0.512814.
Train: 2018-08-09T11:43:57.261659: step 17714, loss 0.579004.
Train: 2018-08-09T11:43:57.355388: step 17715, loss 0.496538.
Train: 2018-08-09T11:43:57.449085: step 17716, loss 0.595426.
Train: 2018-08-09T11:43:57.527221: step 17717, loss 0.414533.
Train: 2018-08-09T11:43:57.620949: step 17718, loss 0.546042.
Train: 2018-08-09T11:43:57.699056: step 17719, loss 0.529563.
Train: 2018-08-09T11:43:57.792753: step 17720, loss 0.562424.
Test: 2018-08-09T11:43:58.277015: step 17720, loss 0.548869.
Train: 2018-08-09T11:43:58.370743: step 17721, loss 0.512922.
Train: 2018-08-09T11:43:58.448880: step 17722, loss 0.612028.
Train: 2018-08-09T11:43:58.542578: step 17723, loss 0.579128.
Train: 2018-08-09T11:43:58.636306: step 17724, loss 0.495891.
Train: 2018-08-09T11:43:58.714442: step 17725, loss 0.596007.
Train: 2018-08-09T11:43:58.808141: step 17726, loss 0.5625.
Train: 2018-08-09T11:43:58.886277: step 17727, loss 0.545615.
Train: 2018-08-09T11:43:58.980006: step 17728, loss 0.545815.
Train: 2018-08-09T11:43:59.058112: step 17729, loss 0.629638.
Train: 2018-08-09T11:43:59.151839: step 17730, loss 0.54587.
Test: 2018-08-09T11:43:59.636069: step 17730, loss 0.548706.
Train: 2018-08-09T11:43:59.729827: step 17731, loss 0.679241.
Train: 2018-08-09T11:43:59.807934: step 17732, loss 0.562451.
Train: 2018-08-09T11:43:59.901662: step 17733, loss 0.628671.
Train: 2018-08-09T11:43:59.982069: step 17734, loss 0.562458.
Train: 2018-08-09T11:44:00.075796: step 17735, loss 0.562527.
Train: 2018-08-09T11:44:00.153934: step 17736, loss 0.562555.
Train: 2018-08-09T11:44:00.247661: step 17737, loss 0.578957.
Train: 2018-08-09T11:44:00.325768: step 17738, loss 0.562594.
Train: 2018-08-09T11:44:00.419466: step 17739, loss 0.562614.
Train: 2018-08-09T11:44:00.497572: step 17740, loss 0.546345.
Test: 2018-08-09T11:44:00.997456: step 17740, loss 0.545562.
Train: 2018-08-09T11:44:01.091212: step 17741, loss 0.546382.
Train: 2018-08-09T11:44:01.169319: step 17742, loss 0.562659.
Train: 2018-08-09T11:44:01.263047: step 17743, loss 0.530194.
Train: 2018-08-09T11:44:01.341154: step 17744, loss 0.595139.
Train: 2018-08-09T11:44:01.434883: step 17745, loss 0.578902.
Train: 2018-08-09T11:44:01.512989: step 17746, loss 0.465443.
Train: 2018-08-09T11:44:01.606716: step 17747, loss 0.595122.
Train: 2018-08-09T11:44:01.684823: step 17748, loss 0.562679.
Train: 2018-08-09T11:44:01.778520: step 17749, loss 0.627586.
Train: 2018-08-09T11:44:01.856657: step 17750, loss 0.546473.
Test: 2018-08-09T11:44:02.358930: step 17750, loss 0.551184.
Train: 2018-08-09T11:44:02.437037: step 17751, loss 0.514072.
Train: 2018-08-09T11:44:02.530795: step 17752, loss 0.56269.
Train: 2018-08-09T11:44:02.624491: step 17753, loss 0.514026.
Train: 2018-08-09T11:44:02.702629: step 17754, loss 0.562675.
Train: 2018-08-09T11:44:02.796357: step 17755, loss 0.513889.
Train: 2018-08-09T11:44:02.874464: step 17756, loss 0.611495.
Train: 2018-08-09T11:44:02.968160: step 17757, loss 0.562597.
Train: 2018-08-09T11:44:03.046299: step 17758, loss 0.513719.
Train: 2018-08-09T11:44:03.139996: step 17759, loss 0.597573.
Train: 2018-08-09T11:44:03.218103: step 17760, loss 0.513595.
Test: 2018-08-09T11:44:03.717984: step 17760, loss 0.550969.
Train: 2018-08-09T11:44:03.811713: step 17761, loss 0.497154.
Train: 2018-08-09T11:44:03.889848: step 17762, loss 0.529773.
Train: 2018-08-09T11:44:03.984214: step 17763, loss 0.595413.
Train: 2018-08-09T11:44:04.062324: step 17764, loss 0.644851.
Train: 2018-08-09T11:44:04.156021: step 17765, loss 0.710746.
Train: 2018-08-09T11:44:04.234127: step 17766, loss 0.661149.
Train: 2018-08-09T11:44:04.327856: step 17767, loss 0.578954.
Train: 2018-08-09T11:44:04.421614: step 17768, loss 0.529964.
Train: 2018-08-09T11:44:04.499721: step 17769, loss 0.562638.
Train: 2018-08-09T11:44:04.593419: step 17770, loss 0.530174.
Test: 2018-08-09T11:44:05.086258: step 17770, loss 0.549324.
Train: 2018-08-09T11:44:05.179972: step 17771, loss 0.530242.
Train: 2018-08-09T11:44:05.258111: step 17772, loss 0.595105.
Train: 2018-08-09T11:44:05.351808: step 17773, loss 0.546522.
Train: 2018-08-09T11:44:05.429915: step 17774, loss 0.530374.
Train: 2018-08-09T11:44:05.523642: step 17775, loss 0.514215.
Train: 2018-08-09T11:44:05.601779: step 17776, loss 0.546535.
Train: 2018-08-09T11:44:05.695476: step 17777, loss 0.449361.
Train: 2018-08-09T11:44:05.773584: step 17778, loss 0.49774.
Train: 2018-08-09T11:44:05.867341: step 17779, loss 0.546346.
Train: 2018-08-09T11:44:05.945448: step 17780, loss 0.59528.
Test: 2018-08-09T11:44:06.445300: step 17780, loss 0.547224.
Train: 2018-08-09T11:44:06.539065: step 17781, loss 0.562575.
Train: 2018-08-09T11:44:06.617165: step 17782, loss 0.595399.
Train: 2018-08-09T11:44:06.710892: step 17783, loss 0.611855.
Train: 2018-08-09T11:44:06.788999: step 17784, loss 0.611874.
Train: 2018-08-09T11:44:06.882733: step 17785, loss 0.628301.
Train: 2018-08-09T11:44:06.960805: step 17786, loss 0.611803.
Train: 2018-08-09T11:44:07.054562: step 17787, loss 0.546187.
Train: 2018-08-09T11:44:07.132638: step 17788, loss 0.480796.
Train: 2018-08-09T11:44:07.226366: step 17789, loss 0.529878.
Train: 2018-08-09T11:44:07.320124: step 17790, loss 0.513502.
Test: 2018-08-09T11:44:07.819976: step 17790, loss 0.549705.
Train: 2018-08-09T11:44:07.898108: step 17791, loss 0.546196.
Train: 2018-08-09T11:44:07.991840: step 17792, loss 0.546163.
Train: 2018-08-09T11:44:08.069948: step 17793, loss 0.480448.
Train: 2018-08-09T11:44:08.163675: step 17794, loss 0.463759.
Train: 2018-08-09T11:44:08.241785: step 17795, loss 0.562517.
Train: 2018-08-09T11:44:08.335510: step 17796, loss 0.545893.
Train: 2018-08-09T11:44:08.429237: step 17797, loss 0.628976.
Train: 2018-08-09T11:44:08.507314: step 17798, loss 0.579089.
Train: 2018-08-09T11:44:08.601043: step 17799, loss 0.529081.
Train: 2018-08-09T11:44:08.679179: step 17800, loss 0.545721.
Test: 2018-08-09T11:44:09.181390: step 17800, loss 0.548004.
Train: 2018-08-09T11:44:09.743733: step 17801, loss 0.579132.
Train: 2018-08-09T11:44:09.821869: step 17802, loss 0.512177.
Train: 2018-08-09T11:44:09.915597: step 17803, loss 0.612702.
Train: 2018-08-09T11:44:09.993698: step 17804, loss 0.612738.
Train: 2018-08-09T11:44:10.087433: step 17805, loss 0.528856.
Train: 2018-08-09T11:44:10.165538: step 17806, loss 0.56239.
Train: 2018-08-09T11:44:10.259235: step 17807, loss 0.579175.
Train: 2018-08-09T11:44:10.337373: step 17808, loss 0.495317.
Train: 2018-08-09T11:44:10.431103: step 17809, loss 0.646303.
Train: 2018-08-09T11:44:10.509211: step 17810, loss 0.679768.
Test: 2018-08-09T11:44:11.010406: step 17810, loss 0.549269.
Train: 2018-08-09T11:44:11.088542: step 17811, loss 0.579135.
Train: 2018-08-09T11:44:11.182270: step 17812, loss 0.645828.
Train: 2018-08-09T11:44:11.266918: step 17813, loss 0.529213.
Train: 2018-08-09T11:44:11.360644: step 17814, loss 0.49619.
Train: 2018-08-09T11:44:11.438720: step 17815, loss 0.661741.
Train: 2018-08-09T11:44:11.532478: step 17816, loss 0.430575.
Train: 2018-08-09T11:44:11.610585: step 17817, loss 0.463643.
Train: 2018-08-09T11:44:11.688662: step 17818, loss 0.480053.
Train: 2018-08-09T11:44:11.782420: step 17819, loss 0.512925.
Train: 2018-08-09T11:44:11.876148: step 17820, loss 0.545912.
Test: 2018-08-09T11:44:12.376032: step 17820, loss 0.548157.
Train: 2018-08-09T11:44:12.454137: step 17821, loss 0.562457.
Train: 2018-08-09T11:44:12.547866: step 17822, loss 0.595717.
Train: 2018-08-09T11:44:12.625940: step 17823, loss 0.62907.
Train: 2018-08-09T11:44:12.719699: step 17824, loss 0.562434.
Train: 2018-08-09T11:44:12.797805: step 17825, loss 0.545774.
Train: 2018-08-09T11:44:12.891533: step 17826, loss 0.562433.
Train: 2018-08-09T11:44:12.979979: step 17827, loss 0.562431.
Train: 2018-08-09T11:44:13.058056: step 17828, loss 0.562426.
Train: 2018-08-09T11:44:13.151815: step 17829, loss 0.545766.
Train: 2018-08-09T11:44:13.229921: step 17830, loss 0.612417.
Test: 2018-08-09T11:44:13.729772: step 17830, loss 0.546214.
Train: 2018-08-09T11:44:13.807903: step 17831, loss 0.495836.
Train: 2018-08-09T11:44:13.901606: step 17832, loss 0.545808.
Train: 2018-08-09T11:44:13.995359: step 17833, loss 0.579096.
Train: 2018-08-09T11:44:14.073472: step 17834, loss 0.54574.
Train: 2018-08-09T11:44:14.167199: step 17835, loss 0.479117.
Train: 2018-08-09T11:44:14.245309: step 17836, loss 0.512353.
Train: 2018-08-09T11:44:14.339004: step 17837, loss 0.462038.
Train: 2018-08-09T11:44:14.417142: step 17838, loss 0.528815.
Train: 2018-08-09T11:44:14.510838: step 17839, loss 0.478168.
Train: 2018-08-09T11:44:14.588975: step 17840, loss 0.596171.
Test: 2018-08-09T11:44:15.088852: step 17840, loss 0.548997.
Train: 2018-08-09T11:44:15.166966: step 17841, loss 0.494367.
Train: 2018-08-09T11:44:15.260692: step 17842, loss 0.562296.
Train: 2018-08-09T11:44:15.338769: step 17843, loss 0.527982.
Train: 2018-08-09T11:44:15.432528: step 17844, loss 0.648948.
Train: 2018-08-09T11:44:15.526223: step 17845, loss 0.545356.
Train: 2018-08-09T11:44:15.604361: step 17846, loss 0.545181.
Train: 2018-08-09T11:44:15.698089: step 17847, loss 0.596849.
Train: 2018-08-09T11:44:15.776166: step 17848, loss 0.613913.
Train: 2018-08-09T11:44:15.869924: step 17849, loss 0.613852.
Train: 2018-08-09T11:44:15.950278: step 17850, loss 0.562335.
Test: 2018-08-09T11:44:16.450130: step 17850, loss 0.550177.
Train: 2018-08-09T11:44:16.543888: step 17851, loss 0.493834.
Train: 2018-08-09T11:44:16.621994: step 17852, loss 0.493869.
Train: 2018-08-09T11:44:16.715722: step 17853, loss 0.682266.
Train: 2018-08-09T11:44:16.793828: step 17854, loss 0.579438.
Train: 2018-08-09T11:44:16.887557: step 17855, loss 0.613533.
Train: 2018-08-09T11:44:16.965663: step 17856, loss 0.528308.
Train: 2018-08-09T11:44:17.059361: step 17857, loss 0.596319.
Train: 2018-08-09T11:44:17.137467: step 17858, loss 0.630131.
Train: 2018-08-09T11:44:17.231195: step 17859, loss 0.494809.
Train: 2018-08-09T11:44:17.309333: step 17860, loss 0.562372.
Test: 2018-08-09T11:44:17.809185: step 17860, loss 0.548527.
Train: 2018-08-09T11:44:17.902925: step 17861, loss 0.5792.
Train: 2018-08-09T11:44:17.983427: step 17862, loss 0.730268.
Train: 2018-08-09T11:44:18.077154: step 17863, loss 0.545708.
Train: 2018-08-09T11:44:18.155262: step 17864, loss 0.579087.
Train: 2018-08-09T11:44:18.248958: step 17865, loss 0.595629.
Train: 2018-08-09T11:44:18.327100: step 17866, loss 0.545983.
Train: 2018-08-09T11:44:18.420824: step 17867, loss 0.628375.
Train: 2018-08-09T11:44:18.498931: step 17868, loss 0.578959.
Train: 2018-08-09T11:44:18.592629: step 17869, loss 0.595263.
Train: 2018-08-09T11:44:18.686380: step 17870, loss 0.595173.
Test: 2018-08-09T11:44:19.170619: step 17870, loss 0.549369.
Train: 2018-08-09T11:44:19.264382: step 17871, loss 0.708425.
Train: 2018-08-09T11:44:19.342482: step 17872, loss 0.498412.
Train: 2018-08-09T11:44:19.436210: step 17873, loss 0.514781.
Train: 2018-08-09T11:44:19.529937: step 17874, loss 0.49901.
Train: 2018-08-09T11:44:19.608014: step 17875, loss 0.626688.
Train: 2018-08-09T11:44:19.701775: step 17876, loss 0.483425.
Train: 2018-08-09T11:44:19.779849: step 17877, loss 0.562935.
Train: 2018-08-09T11:44:19.873606: step 17878, loss 0.578795.
Train: 2018-08-09T11:44:19.952351: step 17879, loss 0.626385.
Train: 2018-08-09T11:44:20.046079: step 17880, loss 0.547062.
Test: 2018-08-09T11:44:20.545932: step 17880, loss 0.548671.
Train: 2018-08-09T11:44:20.624067: step 17881, loss 0.625933.
Train: 2018-08-09T11:44:20.717765: step 17882, loss 0.54677.
Train: 2018-08-09T11:44:20.795875: step 17883, loss 0.499263.
Train: 2018-08-09T11:44:20.889633: step 17884, loss 0.515388.
Train: 2018-08-09T11:44:20.983354: step 17885, loss 0.581491.
Train: 2018-08-09T11:44:21.077083: step 17886, loss 0.644393.
Train: 2018-08-09T11:44:21.170785: step 17887, loss 0.546429.
Train: 2018-08-09T11:44:21.248891: step 17888, loss 0.612814.
Train: 2018-08-09T11:44:21.358241: step 17889, loss 0.546247.
Train: 2018-08-09T11:44:21.436378: step 17890, loss 0.562697.
Test: 2018-08-09T11:44:21.936229: step 17890, loss 0.550549.
Train: 2018-08-09T11:44:22.029957: step 17891, loss 0.57844.
Train: 2018-08-09T11:44:22.108063: step 17892, loss 0.547704.
Train: 2018-08-09T11:44:22.201792: step 17893, loss 0.531191.
Train: 2018-08-09T11:44:22.279897: step 17894, loss 0.547439.
Train: 2018-08-09T11:44:22.373625: step 17895, loss 0.579184.
Train: 2018-08-09T11:44:22.451763: step 17896, loss 0.626057.
Train: 2018-08-09T11:44:22.545491: step 17897, loss 0.578734.
Train: 2018-08-09T11:44:22.623597: step 17898, loss 0.468613.
Train: 2018-08-09T11:44:22.717325: step 17899, loss 0.641962.
Train: 2018-08-09T11:44:22.811024: step 17900, loss 0.53148.
Test: 2018-08-09T11:44:23.297548: step 17900, loss 0.549521.
Train: 2018-08-09T11:44:23.859914: step 17901, loss 0.610564.
Train: 2018-08-09T11:44:23.938052: step 17902, loss 0.61045.
Train: 2018-08-09T11:44:24.031781: step 17903, loss 0.547518.
Train: 2018-08-09T11:44:24.125512: step 17904, loss 0.516032.
Train: 2018-08-09T11:44:24.203615: step 17905, loss 0.657529.
Train: 2018-08-09T11:44:24.297344: step 17906, loss 0.610324.
Train: 2018-08-09T11:44:24.375454: step 17907, loss 0.531856.
Train: 2018-08-09T11:44:24.469146: step 17908, loss 0.610204.
Train: 2018-08-09T11:44:24.547284: step 17909, loss 0.547602.
Train: 2018-08-09T11:44:24.640982: step 17910, loss 0.547627.
Test: 2018-08-09T11:44:25.141877: step 17910, loss 0.551574.
Train: 2018-08-09T11:44:25.235628: step 17911, loss 0.578886.
Train: 2018-08-09T11:44:25.313710: step 17912, loss 0.485164.
Train: 2018-08-09T11:44:25.407469: step 17913, loss 0.516327.
Train: 2018-08-09T11:44:25.485575: step 17914, loss 0.641561.
Train: 2018-08-09T11:44:25.579303: step 17915, loss 0.547515.
Train: 2018-08-09T11:44:25.673000: step 17916, loss 0.610265.
Train: 2018-08-09T11:44:25.751137: step 17917, loss 0.531775.
Train: 2018-08-09T11:44:25.844866: step 17918, loss 0.594583.
Train: 2018-08-09T11:44:25.922976: step 17919, loss 0.531714.
Train: 2018-08-09T11:44:26.016700: step 17920, loss 0.547399.
Test: 2018-08-09T11:44:26.516553: step 17920, loss 0.54954.
Train: 2018-08-09T11:44:26.594691: step 17921, loss 0.594621.
Train: 2018-08-09T11:44:26.688388: step 17922, loss 0.563098.
Train: 2018-08-09T11:44:26.782115: step 17923, loss 0.578864.
Train: 2018-08-09T11:44:26.860251: step 17924, loss 0.531495.
Train: 2018-08-09T11:44:26.956262: step 17925, loss 0.547243.
Train: 2018-08-09T11:44:27.034337: step 17926, loss 0.547199.
Train: 2018-08-09T11:44:27.128095: step 17927, loss 0.563019.
Train: 2018-08-09T11:44:27.221784: step 17928, loss 0.626497.
Train: 2018-08-09T11:44:27.299920: step 17929, loss 0.547057.
Train: 2018-08-09T11:44:27.393648: step 17930, loss 0.594806.
Test: 2018-08-09T11:44:27.877878: step 17930, loss 0.550441.
Train: 2018-08-09T11:44:27.971606: step 17931, loss 0.578897.
Train: 2018-08-09T11:44:28.065345: step 17932, loss 0.626603.
Train: 2018-08-09T11:44:28.143471: step 17933, loss 0.594752.
Train: 2018-08-09T11:44:28.237199: step 17934, loss 0.483647.
Train: 2018-08-09T11:44:28.315307: step 17935, loss 0.62648.
Train: 2018-08-09T11:44:28.409034: step 17936, loss 0.562997.
Train: 2018-08-09T11:44:28.487109: step 17937, loss 0.610565.
Train: 2018-08-09T11:44:28.580868: step 17938, loss 0.547192.
Train: 2018-08-09T11:44:28.658975: step 17939, loss 0.578861.
Train: 2018-08-09T11:44:28.752673: step 17940, loss 0.54724.
Test: 2018-08-09T11:44:29.254886: step 17940, loss 0.548247.
Train: 2018-08-09T11:44:29.333021: step 17941, loss 0.484029.
Train: 2018-08-09T11:44:29.426719: step 17942, loss 0.468096.
Train: 2018-08-09T11:44:29.520481: step 17943, loss 0.547126.
Train: 2018-08-09T11:44:29.598583: step 17944, loss 0.594769.
Train: 2018-08-09T11:44:29.692312: step 17945, loss 0.610747.
Train: 2018-08-09T11:44:29.770413: step 17946, loss 0.51501.
Train: 2018-08-09T11:44:29.864146: step 17947, loss 0.498897.
Train: 2018-08-09T11:44:29.957874: step 17948, loss 0.51471.
Train: 2018-08-09T11:44:30.035981: step 17949, loss 0.578885.
Train: 2018-08-09T11:44:30.129679: step 17950, loss 0.562743.
Test: 2018-08-09T11:44:30.629561: step 17950, loss 0.546323.
Train: 2018-08-09T11:44:30.707698: step 17951, loss 0.546536.
Train: 2018-08-09T11:44:30.801426: step 17952, loss 0.676236.
Train: 2018-08-09T11:44:30.895153: step 17953, loss 0.54644.
Train: 2018-08-09T11:44:30.975688: step 17954, loss 0.660113.
Train: 2018-08-09T11:44:31.069386: step 17955, loss 0.578928.
Train: 2018-08-09T11:44:31.147524: step 17956, loss 0.578904.
Train: 2018-08-09T11:44:31.241250: step 17957, loss 0.595085.
Train: 2018-08-09T11:44:31.334978: step 17958, loss 0.546546.
Train: 2018-08-09T11:44:31.413055: step 17959, loss 0.546587.
Train: 2018-08-09T11:44:31.506784: step 17960, loss 0.578882.
Test: 2018-08-09T11:44:32.006665: step 17960, loss 0.550691.
Train: 2018-08-09T11:44:32.084802: step 17961, loss 0.627258.
Train: 2018-08-09T11:44:32.178499: step 17962, loss 0.627181.
Train: 2018-08-09T11:44:32.272258: step 17963, loss 0.466482.
Train: 2018-08-09T11:44:32.350335: step 17964, loss 0.643042.
Train: 2018-08-09T11:44:32.444096: step 17965, loss 0.626904.
Train: 2018-08-09T11:44:32.522201: step 17966, loss 0.467071.
Train: 2018-08-09T11:44:32.615926: step 17967, loss 0.626734.
Train: 2018-08-09T11:44:32.709655: step 17968, loss 0.626653.
Train: 2018-08-09T11:44:32.787732: step 17969, loss 0.626534.
Train: 2018-08-09T11:44:32.881489: step 17970, loss 0.531336.
Test: 2018-08-09T11:44:33.382770: step 17970, loss 0.549442.
Train: 2018-08-09T11:44:33.460862: step 17971, loss 0.610473.
Train: 2018-08-09T11:44:33.554591: step 17972, loss 0.500051.
Train: 2018-08-09T11:44:33.648320: step 17973, loss 0.610352.
Train: 2018-08-09T11:44:33.726425: step 17974, loss 0.610301.
Train: 2018-08-09T11:44:33.820153: step 17975, loss 0.516161.
Train: 2018-08-09T11:44:33.898261: step 17976, loss 0.594541.
Train: 2018-08-09T11:44:33.991987: step 17977, loss 0.578887.
Train: 2018-08-09T11:44:34.085715: step 17978, loss 0.578892.
Train: 2018-08-09T11:44:34.163823: step 17979, loss 0.59449.
Train: 2018-08-09T11:44:34.257549: step 17980, loss 0.547758.
Test: 2018-08-09T11:44:34.757403: step 17980, loss 0.548775.
Train: 2018-08-09T11:44:34.835539: step 17981, loss 0.485577.
Train: 2018-08-09T11:44:34.930692: step 17982, loss 0.641157.
Train: 2018-08-09T11:44:35.008800: step 17983, loss 0.501134.
Train: 2018-08-09T11:44:35.102527: step 17984, loss 0.672286.
Train: 2018-08-09T11:44:35.196255: step 17985, loss 0.609996.
Train: 2018-08-09T11:44:35.274365: step 17986, loss 0.640995.
Train: 2018-08-09T11:44:35.368089: step 17987, loss 0.547968.
Train: 2018-08-09T11:44:35.461821: step 17988, loss 0.578936.
Train: 2018-08-09T11:44:35.539918: step 17989, loss 0.532714.
Train: 2018-08-09T11:44:35.633622: step 17990, loss 0.578953.
Test: 2018-08-09T11:44:36.133536: step 17990, loss 0.549716.
Train: 2018-08-09T11:44:36.211645: step 17991, loss 0.56356.
Train: 2018-08-09T11:44:36.305338: step 17992, loss 0.532795.
Train: 2018-08-09T11:44:36.399097: step 17993, loss 0.532773.
Train: 2018-08-09T11:44:36.477204: step 17994, loss 0.48648.
Train: 2018-08-09T11:44:36.570932: step 17995, loss 0.517108.
Train: 2018-08-09T11:44:36.664629: step 17996, loss 0.578784.
Train: 2018-08-09T11:44:36.742766: step 17997, loss 0.641381.
Train: 2018-08-09T11:44:36.836465: step 17998, loss 0.594534.
Train: 2018-08-09T11:44:36.932617: step 17999, loss 0.516427.
Train: 2018-08-09T11:44:37.010723: step 18000, loss 0.594694.
Test: 2018-08-09T11:44:37.504617: step 18000, loss 0.549109.
Train: 2018-08-09T11:44:38.067015: step 18001, loss 0.53164.
Train: 2018-08-09T11:44:38.145121: step 18002, loss 0.562821.
Train: 2018-08-09T11:44:38.238849: step 18003, loss 0.548182.
Train: 2018-08-09T11:44:38.332578: step 18004, loss 0.515523.
Train: 2018-08-09T11:44:38.410683: step 18005, loss 0.563244.
Train: 2018-08-09T11:44:38.504381: step 18006, loss 0.419786.
Train: 2018-08-09T11:44:38.598108: step 18007, loss 0.578981.
Train: 2018-08-09T11:44:38.676246: step 18008, loss 0.578619.
Train: 2018-08-09T11:44:38.769944: step 18009, loss 0.515275.
Train: 2018-08-09T11:44:38.863702: step 18010, loss 0.531312.
Test: 2018-08-09T11:44:39.355385: step 18010, loss 0.547746.
Train: 2018-08-09T11:44:39.449111: step 18011, loss 0.562546.
Train: 2018-08-09T11:44:39.527248: step 18012, loss 0.54418.
Train: 2018-08-09T11:44:39.620976: step 18013, loss 0.54658.
Train: 2018-08-09T11:44:39.714704: step 18014, loss 0.56356.
Train: 2018-08-09T11:44:39.792781: step 18015, loss 0.660127.
Train: 2018-08-09T11:44:39.886539: step 18016, loss 0.582342.
Train: 2018-08-09T11:44:39.980268: step 18017, loss 0.478072.
Train: 2018-08-09T11:44:40.058373: step 18018, loss 0.514037.
Train: 2018-08-09T11:44:40.152101: step 18019, loss 0.494306.
Train: 2018-08-09T11:44:40.230203: step 18020, loss 0.61473.
Test: 2018-08-09T11:44:40.730091: step 18020, loss 0.549142.
Train: 2018-08-09T11:44:40.823822: step 18021, loss 0.526519.
Train: 2018-08-09T11:44:40.917545: step 18022, loss 0.475183.
Train: 2018-08-09T11:44:40.995653: step 18023, loss 0.580296.
Train: 2018-08-09T11:44:41.089350: step 18024, loss 0.57933.
Train: 2018-08-09T11:44:41.183108: step 18025, loss 0.493251.
Train: 2018-08-09T11:44:41.276837: step 18026, loss 0.631533.
Train: 2018-08-09T11:44:41.354912: step 18027, loss 0.631243.
Train: 2018-08-09T11:44:41.448640: step 18028, loss 0.600061.
Train: 2018-08-09T11:44:41.542369: step 18029, loss 0.491718.
Train: 2018-08-09T11:44:41.636097: step 18030, loss 0.542912.
Test: 2018-08-09T11:44:42.122997: step 18030, loss 0.548158.
Train: 2018-08-09T11:44:42.215668: step 18031, loss 0.477859.
Train: 2018-08-09T11:44:42.309399: step 18032, loss 0.493624.
Train: 2018-08-09T11:44:42.401200: step 18033, loss 0.559904.
Train: 2018-08-09T11:44:42.479337: step 18034, loss 0.581672.
Train: 2018-08-09T11:44:42.573068: step 18035, loss 0.525831.
Train: 2018-08-09T11:44:42.666793: step 18036, loss 0.544853.
Train: 2018-08-09T11:44:42.744899: step 18037, loss 0.57699.
Train: 2018-08-09T11:44:42.838622: step 18038, loss 0.562561.
Train: 2018-08-09T11:44:42.932355: step 18039, loss 0.508273.
Train: 2018-08-09T11:44:43.026082: step 18040, loss 0.60091.
Test: 2018-08-09T11:44:43.510314: step 18040, loss 0.549847.
Train: 2018-08-09T11:44:43.604066: step 18041, loss 0.490748.
Train: 2018-08-09T11:44:43.697803: step 18042, loss 0.536333.
Train: 2018-08-09T11:44:43.775907: step 18043, loss 0.529045.
Train: 2018-08-09T11:44:43.869634: step 18044, loss 0.602821.
Train: 2018-08-09T11:44:43.966646: step 18045, loss 0.525727.
Train: 2018-08-09T11:44:44.044720: step 18046, loss 0.615509.
Train: 2018-08-09T11:44:44.138478: step 18047, loss 0.527726.
Train: 2018-08-09T11:44:44.232206: step 18048, loss 0.54389.
Train: 2018-08-09T11:44:44.325927: step 18049, loss 0.58052.
Train: 2018-08-09T11:44:44.404010: step 18050, loss 0.592797.
Test: 2018-08-09T11:44:44.903893: step 18050, loss 0.547924.
Train: 2018-08-09T11:44:44.997620: step 18051, loss 0.494266.
Train: 2018-08-09T11:44:45.075757: step 18052, loss 0.598793.
Train: 2018-08-09T11:44:45.169486: step 18053, loss 0.559122.
Train: 2018-08-09T11:44:45.263212: step 18054, loss 0.488967.
Train: 2018-08-09T11:44:45.341322: step 18055, loss 0.528238.
Train: 2018-08-09T11:44:45.435047: step 18056, loss 0.585602.
Train: 2018-08-09T11:44:45.528775: step 18057, loss 0.474986.
Train: 2018-08-09T11:44:45.622503: step 18058, loss 0.548046.
Train: 2018-08-09T11:44:45.700610: step 18059, loss 0.55067.
Train: 2018-08-09T11:44:45.794337: step 18060, loss 0.508426.
Test: 2018-08-09T11:44:46.294198: step 18060, loss 0.54731.
Train: 2018-08-09T11:44:46.372326: step 18061, loss 0.54025.
Train: 2018-08-09T11:44:46.466057: step 18062, loss 0.452633.
Train: 2018-08-09T11:44:46.559782: step 18063, loss 0.530532.
Train: 2018-08-09T11:44:46.653510: step 18064, loss 0.48889.
Train: 2018-08-09T11:44:46.731620: step 18065, loss 0.517701.
Train: 2018-08-09T11:44:46.825345: step 18066, loss 0.54295.
Train: 2018-08-09T11:44:46.921323: step 18067, loss 0.602191.
Train: 2018-08-09T11:44:47.015082: step 18068, loss 0.684165.
Train: 2018-08-09T11:44:47.093189: step 18069, loss 0.531212.
Train: 2018-08-09T11:44:47.186917: step 18070, loss 0.527471.
Test: 2018-08-09T11:44:47.686800: step 18070, loss 0.549836.
Train: 2018-08-09T11:44:47.780530: step 18071, loss 0.670549.
Train: 2018-08-09T11:44:47.874255: step 18072, loss 0.525088.
Train: 2018-08-09T11:44:47.967985: step 18073, loss 0.614892.
Train: 2018-08-09T11:44:48.046090: step 18074, loss 0.600321.
Train: 2018-08-09T11:44:48.139820: step 18075, loss 0.545212.
Train: 2018-08-09T11:44:48.233545: step 18076, loss 0.645708.
Train: 2018-08-09T11:44:48.327273: step 18077, loss 0.612285.
Train: 2018-08-09T11:44:48.420997: step 18078, loss 0.546547.
Train: 2018-08-09T11:44:48.499108: step 18079, loss 0.595013.
Train: 2018-08-09T11:44:48.592806: step 18080, loss 0.578419.
Test: 2018-08-09T11:44:49.093559: step 18080, loss 0.550108.
Train: 2018-08-09T11:44:49.187317: step 18081, loss 0.579552.
Train: 2018-08-09T11:44:49.281044: step 18082, loss 0.530357.
Train: 2018-08-09T11:44:49.374773: step 18083, loss 0.562879.
Train: 2018-08-09T11:44:49.468494: step 18084, loss 0.498853.
Train: 2018-08-09T11:44:49.546607: step 18085, loss 0.499075.
Train: 2018-08-09T11:44:49.640305: step 18086, loss 0.546751.
Train: 2018-08-09T11:44:49.734064: step 18087, loss 0.530964.
Train: 2018-08-09T11:44:49.827761: step 18088, loss 0.579168.
Train: 2018-08-09T11:44:49.905868: step 18089, loss 0.546829.
Train: 2018-08-09T11:44:49.999595: step 18090, loss 0.610724.
Test: 2018-08-09T11:44:50.499478: step 18090, loss 0.550273.
Train: 2018-08-09T11:44:50.593205: step 18091, loss 0.674984.
Train: 2018-08-09T11:44:50.686964: step 18092, loss 0.578882.
Train: 2018-08-09T11:44:50.765070: step 18093, loss 0.658696.
Train: 2018-08-09T11:44:50.858800: step 18094, loss 0.674098.
Train: 2018-08-09T11:44:50.952527: step 18095, loss 0.563015.
Train: 2018-08-09T11:44:51.046253: step 18096, loss 0.578862.
Train: 2018-08-09T11:44:51.139981: step 18097, loss 0.437623.
Train: 2018-08-09T11:44:51.218058: step 18098, loss 0.578883.
Train: 2018-08-09T11:44:51.311817: step 18099, loss 0.578886.
Train: 2018-08-09T11:44:51.405544: step 18100, loss 0.59453.
Test: 2018-08-09T11:44:51.906741: step 18100, loss 0.549213.
Train: 2018-08-09T11:44:52.469139: step 18101, loss 0.578889.
Train: 2018-08-09T11:44:52.562862: step 18102, loss 0.578902.
Train: 2018-08-09T11:44:52.640943: step 18103, loss 0.547734.
Train: 2018-08-09T11:44:52.734702: step 18104, loss 0.610038.
Train: 2018-08-09T11:44:52.828430: step 18105, loss 0.532252.
Train: 2018-08-09T11:44:52.922159: step 18106, loss 0.594435.
Train: 2018-08-09T11:44:53.015885: step 18107, loss 0.470177.
Train: 2018-08-09T11:44:53.109614: step 18108, loss 0.625557.
Train: 2018-08-09T11:44:53.203341: step 18109, loss 0.578985.
Train: 2018-08-09T11:44:53.281448: step 18110, loss 0.563354.
Test: 2018-08-09T11:44:53.781301: step 18110, loss 0.54935.
Train: 2018-08-09T11:44:53.875058: step 18111, loss 0.703389.
Train: 2018-08-09T11:44:53.971086: step 18112, loss 0.547836.
Train: 2018-08-09T11:44:54.064813: step 18113, loss 0.578911.
Train: 2018-08-09T11:44:54.142890: step 18114, loss 0.501531.
Train: 2018-08-09T11:44:54.252241: step 18115, loss 0.56348.
Train: 2018-08-09T11:44:54.345968: step 18116, loss 0.51702.
Train: 2018-08-09T11:44:54.439726: step 18117, loss 0.532401.
Train: 2018-08-09T11:44:54.533422: step 18118, loss 0.656539.
Train: 2018-08-09T11:44:54.611529: step 18119, loss 0.547865.
Train: 2018-08-09T11:44:54.705257: step 18120, loss 0.43909.
Test: 2018-08-09T11:44:55.205171: step 18120, loss 0.554004.
Train: 2018-08-09T11:44:55.298897: step 18121, loss 0.563278.
Train: 2018-08-09T11:44:55.392629: step 18122, loss 0.641402.
Train: 2018-08-09T11:44:55.486322: step 18123, loss 0.53183.
Train: 2018-08-09T11:44:55.580081: step 18124, loss 0.610275.
Train: 2018-08-09T11:44:55.673809: step 18125, loss 0.563143.
Train: 2018-08-09T11:44:55.767506: step 18126, loss 0.673097.
Train: 2018-08-09T11:44:55.861265: step 18127, loss 0.515938.
Train: 2018-08-09T11:44:55.956395: step 18128, loss 0.578778.
Train: 2018-08-09T11:44:56.050123: step 18129, loss 0.547491.
Train: 2018-08-09T11:44:56.128230: step 18130, loss 0.49959.
Test: 2018-08-09T11:44:56.628082: step 18130, loss 0.551184.
Train: 2018-08-09T11:44:56.721840: step 18131, loss 0.547702.
Train: 2018-08-09T11:44:56.815572: step 18132, loss 0.562546.
Train: 2018-08-09T11:44:56.909296: step 18133, loss 0.659563.
Train: 2018-08-09T11:44:57.003023: step 18134, loss 0.562098.
Train: 2018-08-09T11:44:57.096751: step 18135, loss 0.562701.
Train: 2018-08-09T11:44:57.190484: step 18136, loss 0.530077.
Train: 2018-08-09T11:44:57.284203: step 18137, loss 0.644301.
Train: 2018-08-09T11:44:57.377937: step 18138, loss 0.562878.
Train: 2018-08-09T11:44:57.476316: step 18139, loss 0.706978.
Train: 2018-08-09T11:44:57.570047: step 18140, loss 0.562017.
Test: 2018-08-09T11:44:58.069896: step 18140, loss 0.550564.
Train: 2018-08-09T11:44:58.148001: step 18141, loss 0.499708.
Train: 2018-08-09T11:44:58.241759: step 18142, loss 0.531955.
Train: 2018-08-09T11:44:58.335457: step 18143, loss 0.547884.
Train: 2018-08-09T11:44:58.444831: step 18144, loss 0.532267.
Train: 2018-08-09T11:44:58.538533: step 18145, loss 0.594745.
Train: 2018-08-09T11:44:58.616671: step 18146, loss 0.468433.
Train: 2018-08-09T11:44:58.710399: step 18147, loss 0.642399.
Train: 2018-08-09T11:44:58.804096: step 18148, loss 0.546941.
Train: 2018-08-09T11:44:58.897854: step 18149, loss 0.57896.
Train: 2018-08-09T11:44:58.991577: step 18150, loss 0.610714.
Test: 2018-08-09T11:44:59.491434: step 18150, loss 0.551826.
Train: 2018-08-09T11:44:59.585192: step 18151, loss 0.468256.
Train: 2018-08-09T11:44:59.678920: step 18152, loss 0.657743.
Train: 2018-08-09T11:44:59.772146: step 18153, loss 0.68969.
Train: 2018-08-09T11:44:59.865873: step 18154, loss 0.610538.
Train: 2018-08-09T11:44:59.963010: step 18155, loss 0.594605.
Train: 2018-08-09T11:45:00.056738: step 18156, loss 0.53189.
Train: 2018-08-09T11:45:00.150466: step 18157, loss 0.516358.
Train: 2018-08-09T11:45:00.244194: step 18158, loss 0.610138.
Train: 2018-08-09T11:45:00.337922: step 18159, loss 0.688109.
Train: 2018-08-09T11:45:00.431644: step 18160, loss 0.532243.
Test: 2018-08-09T11:45:00.931504: step 18160, loss 0.549434.
Train: 2018-08-09T11:45:01.025261: step 18161, loss 0.563388.
Train: 2018-08-09T11:45:01.118988: step 18162, loss 0.547931.
Train: 2018-08-09T11:45:01.212717: step 18163, loss 0.547968.
Train: 2018-08-09T11:45:01.306414: step 18164, loss 0.501569.
Train: 2018-08-09T11:45:01.400173: step 18165, loss 0.532471.
Train: 2018-08-09T11:45:01.509492: step 18166, loss 0.5479.
Train: 2018-08-09T11:45:01.603249: step 18167, loss 0.470151.
Train: 2018-08-09T11:45:01.696977: step 18168, loss 0.438571.
Train: 2018-08-09T11:45:01.790705: step 18169, loss 0.547491.
Train: 2018-08-09T11:45:01.884433: step 18170, loss 0.468453.
Test: 2018-08-09T11:45:02.385636: step 18170, loss 0.549828.
Train: 2018-08-09T11:45:02.479397: step 18171, loss 0.562671.
Train: 2018-08-09T11:45:02.573122: step 18172, loss 0.466374.
Train: 2018-08-09T11:45:02.666850: step 18173, loss 0.562049.
Train: 2018-08-09T11:45:02.760581: step 18174, loss 0.495851.
Train: 2018-08-09T11:45:02.854308: step 18175, loss 0.612063.
Train: 2018-08-09T11:45:02.948034: step 18176, loss 0.564999.
Train: 2018-08-09T11:45:03.041761: step 18177, loss 0.579174.
Train: 2018-08-09T11:45:03.135490: step 18178, loss 0.638881.
Train: 2018-08-09T11:45:03.229220: step 18179, loss 0.613249.
Train: 2018-08-09T11:45:03.322945: step 18180, loss 0.544858.
Test: 2018-08-09T11:45:03.822797: step 18180, loss 0.548322.
Train: 2018-08-09T11:45:03.917942: step 18181, loss 0.56183.
Train: 2018-08-09T11:45:04.011671: step 18182, loss 0.645636.
Train: 2018-08-09T11:45:04.105428: step 18183, loss 0.646177.
Train: 2018-08-09T11:45:04.199125: step 18184, loss 0.562827.
Train: 2018-08-09T11:45:04.292887: step 18185, loss 0.595394.
Train: 2018-08-09T11:45:04.402204: step 18186, loss 0.480439.
Train: 2018-08-09T11:45:04.495961: step 18187, loss 0.497375.
Train: 2018-08-09T11:45:04.589689: step 18188, loss 0.64438.
Train: 2018-08-09T11:45:04.683417: step 18189, loss 0.530038.
Train: 2018-08-09T11:45:04.777115: step 18190, loss 0.611433.
Test: 2018-08-09T11:45:05.276997: step 18190, loss 0.549288.
Train: 2018-08-09T11:45:05.370755: step 18191, loss 0.497686.
Train: 2018-08-09T11:45:05.480106: step 18192, loss 0.562671.
Train: 2018-08-09T11:45:05.573832: step 18193, loss 0.546423.
Train: 2018-08-09T11:45:05.667560: step 18194, loss 0.578922.
Train: 2018-08-09T11:45:05.761288: step 18195, loss 0.611386.
Train: 2018-08-09T11:45:05.854986: step 18196, loss 0.676244.
Train: 2018-08-09T11:45:05.950157: step 18197, loss 0.530358.
Train: 2018-08-09T11:45:06.043856: step 18198, loss 0.514283.
Train: 2018-08-09T11:45:06.137613: step 18199, loss 0.562744.
Train: 2018-08-09T11:45:06.246962: step 18200, loss 0.578882.
Test: 2018-08-09T11:45:06.731193: step 18200, loss 0.551933.
Train: 2018-08-09T11:45:07.324803: step 18201, loss 0.514427.
Train: 2018-08-09T11:45:07.418563: step 18202, loss 0.546648.
Train: 2018-08-09T11:45:07.527910: step 18203, loss 0.530508.
Train: 2018-08-09T11:45:07.621609: step 18204, loss 0.48204.
Train: 2018-08-09T11:45:07.715367: step 18205, loss 0.627407.
Train: 2018-08-09T11:45:07.824715: step 18206, loss 0.611373.
Train: 2018-08-09T11:45:07.919906: step 18207, loss 0.562632.
Train: 2018-08-09T11:45:08.013664: step 18208, loss 0.660096.
Train: 2018-08-09T11:45:08.107393: step 18209, loss 0.611148.
Train: 2018-08-09T11:45:08.201123: step 18210, loss 0.530446.
Test: 2018-08-09T11:45:08.700974: step 18210, loss 0.550708.
Train: 2018-08-09T11:45:08.794730: step 18211, loss 0.514405.
Train: 2018-08-09T11:45:08.904074: step 18212, loss 0.659536.
Train: 2018-08-09T11:45:08.997792: step 18213, loss 0.514549.
Train: 2018-08-09T11:45:09.091535: step 18214, loss 0.562731.
Train: 2018-08-09T11:45:09.200855: step 18215, loss 0.578818.
Train: 2018-08-09T11:45:09.294613: step 18216, loss 0.562773.
Train: 2018-08-09T11:45:09.388341: step 18217, loss 0.611004.
Train: 2018-08-09T11:45:09.482038: step 18218, loss 0.49868.
Train: 2018-08-09T11:45:09.575767: step 18219, loss 0.482302.
Train: 2018-08-09T11:45:09.685146: step 18220, loss 0.481912.
Test: 2018-08-09T11:45:10.184962: step 18220, loss 0.551114.
Train: 2018-08-09T11:45:10.278721: step 18221, loss 0.530401.
Train: 2018-08-09T11:45:10.372448: step 18222, loss 0.677546.
Train: 2018-08-09T11:45:10.481806: step 18223, loss 0.578499.
Train: 2018-08-09T11:45:10.575524: step 18224, loss 0.595944.
Train: 2018-08-09T11:45:10.669256: step 18225, loss 0.67979.
Train: 2018-08-09T11:45:10.778602: step 18226, loss 0.546267.
Train: 2018-08-09T11:45:10.872329: step 18227, loss 0.498098.
Train: 2018-08-09T11:45:10.966061: step 18228, loss 0.562876.
Train: 2018-08-09T11:45:11.075406: step 18229, loss 0.530562.
Train: 2018-08-09T11:45:11.169138: step 18230, loss 0.546645.
Test: 2018-08-09T11:45:11.668986: step 18230, loss 0.548317.
Train: 2018-08-09T11:45:11.762715: step 18231, loss 0.611114.
Train: 2018-08-09T11:45:11.856472: step 18232, loss 0.594951.
Train: 2018-08-09T11:45:11.969130: step 18233, loss 0.594944.
Train: 2018-08-09T11:45:12.062832: step 18234, loss 0.514697.
Train: 2018-08-09T11:45:12.156591: step 18235, loss 0.450493.
Train: 2018-08-09T11:45:12.250289: step 18236, loss 0.627217.
Train: 2018-08-09T11:45:12.359639: step 18237, loss 0.595131.
Train: 2018-08-09T11:45:12.453396: step 18238, loss 0.562697.
Train: 2018-08-09T11:45:12.547124: step 18239, loss 0.530612.
Train: 2018-08-09T11:45:12.656467: step 18240, loss 0.433959.
Test: 2018-08-09T11:45:13.156325: step 18240, loss 0.54819.
Train: 2018-08-09T11:45:13.250084: step 18241, loss 0.498266.
Train: 2018-08-09T11:45:13.343781: step 18242, loss 0.578868.
Train: 2018-08-09T11:45:13.453161: step 18243, loss 0.562921.
Train: 2018-08-09T11:45:13.552773: step 18244, loss 0.529957.
Train: 2018-08-09T11:45:13.643005: step 18245, loss 0.5138.
Train: 2018-08-09T11:45:13.736733: step 18246, loss 0.562243.
Train: 2018-08-09T11:45:13.846086: step 18247, loss 0.678471.
Train: 2018-08-09T11:45:13.938569: step 18248, loss 0.595321.
Train: 2018-08-09T11:45:14.047890: step 18249, loss 0.546016.
Train: 2018-08-09T11:45:14.141616: step 18250, loss 0.595748.
Test: 2018-08-09T11:45:14.641499: step 18250, loss 0.548959.
Train: 2018-08-09T11:45:14.735258: step 18251, loss 0.546241.
Train: 2018-08-09T11:45:14.828954: step 18252, loss 0.5955.
Train: 2018-08-09T11:45:14.938333: step 18253, loss 0.595339.
Train: 2018-08-09T11:45:15.032060: step 18254, loss 0.578964.
Train: 2018-08-09T11:45:15.141410: step 18255, loss 0.546187.
Train: 2018-08-09T11:45:15.235107: step 18256, loss 0.595321.
Train: 2018-08-09T11:45:15.328865: step 18257, loss 0.529892.
Train: 2018-08-09T11:45:15.438186: step 18258, loss 0.513587.
Train: 2018-08-09T11:45:15.531943: step 18259, loss 0.660657.
Train: 2018-08-09T11:45:15.625671: step 18260, loss 0.627908.
Test: 2018-08-09T11:45:16.125524: step 18260, loss 0.547384.
Train: 2018-08-09T11:45:16.234904: step 18261, loss 0.578932.
Train: 2018-08-09T11:45:16.328630: step 18262, loss 0.627634.
Train: 2018-08-09T11:45:16.437951: step 18263, loss 0.48176.
Train: 2018-08-09T11:45:16.531709: step 18264, loss 0.562724.
Train: 2018-08-09T11:45:16.625435: step 18265, loss 0.530456.
Train: 2018-08-09T11:45:16.734786: step 18266, loss 0.578884.
Train: 2018-08-09T11:45:16.828512: step 18267, loss 0.627249.
Train: 2018-08-09T11:45:16.923842: step 18268, loss 0.514495.
Train: 2018-08-09T11:45:17.033223: step 18269, loss 0.578873.
Train: 2018-08-09T11:45:17.126951: step 18270, loss 0.659227.
Test: 2018-08-09T11:45:17.611191: step 18270, loss 0.549022.
Train: 2018-08-09T11:45:17.720555: step 18271, loss 0.578866.
Train: 2018-08-09T11:45:17.814290: step 18272, loss 0.658854.
Train: 2018-08-09T11:45:17.923633: step 18273, loss 0.642634.
Train: 2018-08-09T11:45:18.017366: step 18274, loss 0.642351.
Train: 2018-08-09T11:45:18.126715: step 18275, loss 0.531487.
Train: 2018-08-09T11:45:18.220442: step 18276, loss 0.531694.
Train: 2018-08-09T11:45:18.329763: step 18277, loss 0.563195.
Train: 2018-08-09T11:45:18.423520: step 18278, loss 0.594527.
Train: 2018-08-09T11:45:18.517248: step 18279, loss 0.578892.
Train: 2018-08-09T11:45:18.626598: step 18280, loss 0.532216.
Test: 2018-08-09T11:45:19.112174: step 18280, loss 0.552928.
Train: 2018-08-09T11:45:19.221532: step 18281, loss 0.547833.
Train: 2018-08-09T11:45:19.315257: step 18282, loss 0.485758.
Train: 2018-08-09T11:45:19.408954: step 18283, loss 0.532257.
Train: 2018-08-09T11:45:19.518304: step 18284, loss 0.610093.
Train: 2018-08-09T11:45:19.612061: step 18285, loss 0.501045.
Train: 2018-08-09T11:45:19.721411: step 18286, loss 0.625701.
Train: 2018-08-09T11:45:19.815139: step 18287, loss 0.578554.
Train: 2018-08-09T11:45:19.908867: step 18288, loss 0.516349.
Train: 2018-08-09T11:45:20.018216: step 18289, loss 0.547812.
Train: 2018-08-09T11:45:20.111944: step 18290, loss 0.57919.
Test: 2018-08-09T11:45:20.611797: step 18290, loss 0.551786.
Train: 2018-08-09T11:45:20.705523: step 18291, loss 0.594748.
Train: 2018-08-09T11:45:20.814874: step 18292, loss 0.515418.
Train: 2018-08-09T11:45:20.910928: step 18293, loss 0.562538.
Train: 2018-08-09T11:45:21.020308: step 18294, loss 0.515181.
Train: 2018-08-09T11:45:21.114006: step 18295, loss 0.546237.
Train: 2018-08-09T11:45:21.207764: step 18296, loss 0.611738.
Train: 2018-08-09T11:45:21.317114: step 18297, loss 0.511096.
Train: 2018-08-09T11:45:21.410841: step 18298, loss 0.577629.
Train: 2018-08-09T11:45:21.520166: step 18299, loss 0.574818.
Train: 2018-08-09T11:45:21.613918: step 18300, loss 0.580568.
Test: 2018-08-09T11:45:22.113796: step 18300, loss 0.546721.
Train: 2018-08-09T11:45:22.676169: step 18301, loss 0.530636.
Train: 2018-08-09T11:45:22.769867: step 18302, loss 0.588295.
Train: 2018-08-09T11:45:22.879214: step 18303, loss 0.543039.
Train: 2018-08-09T11:45:22.975207: step 18304, loss 0.497198.
Train: 2018-08-09T11:45:23.068965: step 18305, loss 0.582508.
Train: 2018-08-09T11:45:23.178285: step 18306, loss 0.564864.
Train: 2018-08-09T11:45:23.272043: step 18307, loss 0.507938.
Train: 2018-08-09T11:45:23.381394: step 18308, loss 0.582213.
Train: 2018-08-09T11:45:23.475120: step 18309, loss 0.475194.
Train: 2018-08-09T11:45:23.584468: step 18310, loss 0.511555.
Test: 2018-08-09T11:45:24.068699: step 18310, loss 0.547857.
Train: 2018-08-09T11:45:24.178079: step 18311, loss 0.595397.
Train: 2018-08-09T11:45:24.271807: step 18312, loss 0.58925.
Train: 2018-08-09T11:45:24.365505: step 18313, loss 0.617776.
Train: 2018-08-09T11:45:24.474884: step 18314, loss 0.475434.
Train: 2018-08-09T11:45:24.568581: step 18315, loss 0.481622.
Train: 2018-08-09T11:45:24.677961: step 18316, loss 0.510223.
Train: 2018-08-09T11:45:24.771689: step 18317, loss 0.652962.
Train: 2018-08-09T11:45:24.865417: step 18318, loss 0.532852.
Train: 2018-08-09T11:45:24.976152: step 18319, loss 0.510799.
Train: 2018-08-09T11:45:25.069851: step 18320, loss 0.576163.
Test: 2018-08-09T11:45:25.569757: step 18320, loss 0.546387.
Train: 2018-08-09T11:45:25.663490: step 18321, loss 0.599003.
Train: 2018-08-09T11:45:25.772810: step 18322, loss 0.563843.
Train: 2018-08-09T11:45:25.866536: step 18323, loss 0.444004.
Train: 2018-08-09T11:45:25.975886: step 18324, loss 0.57814.
Train: 2018-08-09T11:45:26.069647: step 18325, loss 0.592856.
Train: 2018-08-09T11:45:26.163372: step 18326, loss 0.493663.
Train: 2018-08-09T11:45:26.272691: step 18327, loss 0.681858.
Train: 2018-08-09T11:45:26.366420: step 18328, loss 0.577884.
Train: 2018-08-09T11:45:26.475767: step 18329, loss 0.594756.
Train: 2018-08-09T11:45:26.569529: step 18330, loss 0.529734.
Test: 2018-08-09T11:45:27.071770: step 18330, loss 0.548887.
Train: 2018-08-09T11:45:27.165522: step 18331, loss 0.432736.
Train: 2018-08-09T11:45:27.274842: step 18332, loss 0.513904.
Train: 2018-08-09T11:45:27.368570: step 18333, loss 0.478575.
Train: 2018-08-09T11:45:27.477950: step 18334, loss 0.561101.
Train: 2018-08-09T11:45:27.571679: step 18335, loss 0.497031.
Train: 2018-08-09T11:45:27.681026: step 18336, loss 0.510406.
Train: 2018-08-09T11:45:27.774754: step 18337, loss 0.579827.
Train: 2018-08-09T11:45:27.884103: step 18338, loss 0.580928.
Train: 2018-08-09T11:45:27.977834: step 18339, loss 0.459489.
Train: 2018-08-09T11:45:28.071528: step 18340, loss 0.596713.
Test: 2018-08-09T11:45:28.571435: step 18340, loss 0.548599.
Train: 2018-08-09T11:45:28.680790: step 18341, loss 0.581117.
Train: 2018-08-09T11:45:28.774522: step 18342, loss 0.494935.
Train: 2018-08-09T11:45:28.883875: step 18343, loss 0.565335.
Train: 2018-08-09T11:45:28.978981: step 18344, loss 0.458666.
Train: 2018-08-09T11:45:29.072709: step 18345, loss 0.476822.
Train: 2018-08-09T11:45:29.182059: step 18346, loss 0.510692.
Train: 2018-08-09T11:45:29.275755: step 18347, loss 0.601793.
Train: 2018-08-09T11:45:29.369514: step 18348, loss 0.58028.
Train: 2018-08-09T11:45:29.478863: step 18349, loss 0.561476.
Train: 2018-08-09T11:45:29.572591: step 18350, loss 0.56051.
Test: 2018-08-09T11:45:30.069877: step 18350, loss 0.546469.
Train: 2018-08-09T11:45:30.179223: step 18351, loss 0.508831.
Train: 2018-08-09T11:45:30.272921: step 18352, loss 0.509768.
Train: 2018-08-09T11:45:30.366678: step 18353, loss 0.526188.
Train: 2018-08-09T11:45:30.476028: step 18354, loss 0.63691.
Train: 2018-08-09T11:45:30.569726: step 18355, loss 0.504296.
Train: 2018-08-09T11:45:30.663483: step 18356, loss 0.544384.
Train: 2018-08-09T11:45:30.772833: step 18357, loss 0.545402.
Train: 2018-08-09T11:45:30.866560: step 18358, loss 0.546123.
Train: 2018-08-09T11:45:30.964797: step 18359, loss 0.614783.
Train: 2018-08-09T11:45:31.074148: step 18360, loss 0.597041.
Test: 2018-08-09T11:45:31.573998: step 18360, loss 0.545333.
Train: 2018-08-09T11:45:31.667727: step 18361, loss 0.467113.
Train: 2018-08-09T11:45:31.761484: step 18362, loss 0.543773.
Train: 2018-08-09T11:45:31.870833: step 18363, loss 0.526628.
Train: 2018-08-09T11:45:32.011396: step 18364, loss 0.560615.
Train: 2018-08-09T11:45:32.105124: step 18365, loss 0.558375.
Train: 2018-08-09T11:45:32.198852: step 18366, loss 0.455497.
Train: 2018-08-09T11:45:32.303165: step 18367, loss 0.527202.
Train: 2018-08-09T11:45:32.412545: step 18368, loss 0.550752.
Train: 2018-08-09T11:45:32.506272: step 18369, loss 0.530091.
Train: 2018-08-09T11:45:32.600001: step 18370, loss 0.633049.
Test: 2018-08-09T11:45:33.105381: step 18370, loss 0.548468.
Train: 2018-08-09T11:45:33.199139: step 18371, loss 0.646531.
Train: 2018-08-09T11:45:33.308488: step 18372, loss 0.582936.
Train: 2018-08-09T11:45:33.402216: step 18373, loss 0.721233.
Train: 2018-08-09T11:45:33.495944: step 18374, loss 0.679585.
Train: 2018-08-09T11:45:33.605289: step 18375, loss 0.57885.
Train: 2018-08-09T11:45:33.699023: step 18376, loss 0.562448.
Train: 2018-08-09T11:45:33.792718: step 18377, loss 0.579001.
Train: 2018-08-09T11:45:33.902099: step 18378, loss 0.531654.
Train: 2018-08-09T11:45:33.995827: step 18379, loss 0.466138.
Train: 2018-08-09T11:45:34.089555: step 18380, loss 0.44984.
Test: 2018-08-09T11:45:34.589407: step 18380, loss 0.55047.
Train: 2018-08-09T11:45:34.683134: step 18381, loss 0.611802.
Train: 2018-08-09T11:45:34.792517: step 18382, loss 0.513603.
Train: 2018-08-09T11:45:34.886212: step 18383, loss 0.530338.
Train: 2018-08-09T11:45:34.979971: step 18384, loss 0.529948.
Train: 2018-08-09T11:45:35.089326: step 18385, loss 0.595452.
Train: 2018-08-09T11:45:35.183048: step 18386, loss 0.611466.
Train: 2018-08-09T11:45:35.292395: step 18387, loss 0.611381.
Train: 2018-08-09T11:45:35.386124: step 18388, loss 0.497893.
Train: 2018-08-09T11:45:35.479852: step 18389, loss 0.740971.
Train: 2018-08-09T11:45:35.589201: step 18390, loss 0.643194.
Test: 2018-08-09T11:45:36.074801: step 18390, loss 0.55034.
Train: 2018-08-09T11:45:36.184180: step 18391, loss 0.562862.
Train: 2018-08-09T11:45:36.277878: step 18392, loss 0.721879.
Train: 2018-08-09T11:45:36.371637: step 18393, loss 0.578352.
Train: 2018-08-09T11:45:36.480987: step 18394, loss 0.594364.
Train: 2018-08-09T11:45:36.574713: step 18395, loss 0.593823.
Train: 2018-08-09T11:45:36.684032: step 18396, loss 0.521798.
Train: 2018-08-09T11:45:36.777760: step 18397, loss 0.593319.
Train: 2018-08-09T11:45:36.871487: step 18398, loss 0.58069.
Train: 2018-08-09T11:45:36.965215: step 18399, loss 0.56394.
Train: 2018-08-09T11:45:37.074564: step 18400, loss 0.563171.
Test: 2018-08-09T11:45:37.574447: step 18400, loss 0.54948.
Train: 2018-08-09T11:45:38.107086: step 18401, loss 0.609015.
Train: 2018-08-09T11:45:38.216403: step 18402, loss 0.518679.
Train: 2018-08-09T11:45:38.310132: step 18403, loss 0.562915.
Train: 2018-08-09T11:45:38.419481: step 18404, loss 0.593858.
Train: 2018-08-09T11:45:38.513241: step 18405, loss 0.640534.
Train: 2018-08-09T11:45:38.622558: step 18406, loss 0.56401.
Train: 2018-08-09T11:45:38.716316: step 18407, loss 0.624548.
Train: 2018-08-09T11:45:38.810043: step 18408, loss 0.518246.
Train: 2018-08-09T11:45:38.919393: step 18409, loss 0.563871.
Train: 2018-08-09T11:45:39.013124: step 18410, loss 0.518056.
Test: 2018-08-09T11:45:39.497351: step 18410, loss 0.550534.
Train: 2018-08-09T11:45:39.606731: step 18411, loss 0.548083.
Train: 2018-08-09T11:45:39.700460: step 18412, loss 0.594723.
Train: 2018-08-09T11:45:39.794187: step 18413, loss 0.577387.
Train: 2018-08-09T11:45:39.903536: step 18414, loss 0.533105.
Train: 2018-08-09T11:45:39.999533: step 18415, loss 0.531496.
Train: 2018-08-09T11:45:40.093261: step 18416, loss 0.609362.
Train: 2018-08-09T11:45:40.186987: step 18417, loss 0.483603.
Train: 2018-08-09T11:45:40.296337: step 18418, loss 0.479791.
Train: 2018-08-09T11:45:40.390034: step 18419, loss 0.599008.
Train: 2018-08-09T11:45:40.483762: step 18420, loss 0.514353.
Test: 2018-08-09T11:45:40.983678: step 18420, loss 0.546623.
Train: 2018-08-09T11:45:41.077373: step 18421, loss 0.524852.
Train: 2018-08-09T11:45:41.186720: step 18422, loss 0.57717.
Train: 2018-08-09T11:45:41.280448: step 18423, loss 0.587848.
Train: 2018-08-09T11:45:41.374210: step 18424, loss 0.59637.
Train: 2018-08-09T11:45:41.467935: step 18425, loss 0.55027.
Train: 2018-08-09T11:45:41.577284: step 18426, loss 0.665863.
Train: 2018-08-09T11:45:41.671012: step 18427, loss 0.531452.
Train: 2018-08-09T11:45:41.764740: step 18428, loss 0.561505.
Train: 2018-08-09T11:45:41.858468: step 18429, loss 0.595816.
Train: 2018-08-09T11:45:41.968487: step 18430, loss 0.627632.
Test: 2018-08-09T11:45:42.452780: step 18430, loss 0.55038.
Train: 2018-08-09T11:45:42.546476: step 18431, loss 0.578326.
Train: 2018-08-09T11:45:42.655855: step 18432, loss 0.57847.
Train: 2018-08-09T11:45:42.753214: step 18433, loss 0.517005.
Train: 2018-08-09T11:45:42.846942: step 18434, loss 0.548388.
Train: 2018-08-09T11:45:42.940672: step 18435, loss 0.609894.
Train: 2018-08-09T11:45:43.034397: step 18436, loss 0.578898.
Train: 2018-08-09T11:45:43.143748: step 18437, loss 0.563734.
Train: 2018-08-09T11:45:43.237478: step 18438, loss 0.563674.
Train: 2018-08-09T11:45:43.331203: step 18439, loss 0.50252.
Train: 2018-08-09T11:45:43.424931: step 18440, loss 0.57899.
Test: 2018-08-09T11:45:43.924783: step 18440, loss 0.551032.
Train: 2018-08-09T11:45:44.018541: step 18441, loss 0.502327.
Train: 2018-08-09T11:45:44.127884: step 18442, loss 0.517437.
Train: 2018-08-09T11:45:44.221619: step 18443, loss 0.501743.
Train: 2018-08-09T11:45:44.315348: step 18444, loss 0.547942.
Train: 2018-08-09T11:45:44.409075: step 18445, loss 0.578942.
Train: 2018-08-09T11:45:44.502802: step 18446, loss 0.54719.
Train: 2018-08-09T11:45:44.612122: step 18447, loss 0.467471.
Train: 2018-08-09T11:45:44.705879: step 18448, loss 0.658021.
Train: 2018-08-09T11:45:44.805212: step 18449, loss 0.578394.
Train: 2018-08-09T11:45:44.898908: step 18450, loss 0.463181.
Test: 2018-08-09T11:45:45.398815: step 18450, loss 0.547072.
Train: 2018-08-09T11:45:45.539382: step 18451, loss 0.529958.
Train: 2018-08-09T11:45:45.648731: step 18452, loss 0.537284.
Train: 2018-08-09T11:45:45.742489: step 18453, loss 0.568841.
Train: 2018-08-09T11:45:45.836186: step 18454, loss 0.531383.
Train: 2018-08-09T11:45:45.929946: step 18455, loss 0.550785.
Train: 2018-08-09T11:45:46.023643: step 18456, loss 0.595784.
Train: 2018-08-09T11:45:46.117400: step 18457, loss 0.555362.
Train: 2018-08-09T11:45:46.226719: step 18458, loss 0.568417.
Train: 2018-08-09T11:45:46.320448: step 18459, loss 0.595595.
Train: 2018-08-09T11:45:46.414176: step 18460, loss 0.560592.
Test: 2018-08-09T11:45:46.915477: step 18460, loss 0.545309.
Train: 2018-08-09T11:45:47.009230: step 18461, loss 0.633636.
Train: 2018-08-09T11:45:47.102964: step 18462, loss 0.635961.
Train: 2018-08-09T11:45:47.196692: step 18463, loss 0.613387.
Train: 2018-08-09T11:45:47.290389: step 18464, loss 0.579389.
Train: 2018-08-09T11:45:47.384147: step 18465, loss 0.512638.
Train: 2018-08-09T11:45:47.477845: step 18466, loss 0.529783.
Train: 2018-08-09T11:45:47.587224: step 18467, loss 0.434059.
Train: 2018-08-09T11:45:47.680923: step 18468, loss 0.627782.
Train: 2018-08-09T11:45:47.774681: step 18469, loss 0.514232.
Train: 2018-08-09T11:45:47.868378: step 18470, loss 0.577115.
Test: 2018-08-09T11:45:48.368295: step 18470, loss 0.547484.
Train: 2018-08-09T11:45:48.461989: step 18471, loss 0.481771.
Train: 2018-08-09T11:45:48.555746: step 18472, loss 0.513187.
Train: 2018-08-09T11:45:48.649445: step 18473, loss 0.578886.
Train: 2018-08-09T11:45:48.743205: step 18474, loss 0.582043.
Train: 2018-08-09T11:45:48.836930: step 18475, loss 0.630193.
Train: 2018-08-09T11:45:48.946792: step 18476, loss 0.562843.
Train: 2018-08-09T11:45:49.040496: step 18477, loss 0.645699.
Train: 2018-08-09T11:45:49.134254: step 18478, loss 0.496024.
Train: 2018-08-09T11:45:49.227984: step 18479, loss 0.53045.
Train: 2018-08-09T11:45:49.321680: step 18480, loss 0.52971.
Test: 2018-08-09T11:45:49.821563: step 18480, loss 0.549491.
Train: 2018-08-09T11:45:49.915288: step 18481, loss 0.577862.
Train: 2018-08-09T11:45:50.009047: step 18482, loss 0.647051.
Train: 2018-08-09T11:45:50.102778: step 18483, loss 0.578741.
Train: 2018-08-09T11:45:50.196499: step 18484, loss 0.594241.
Train: 2018-08-09T11:45:50.290231: step 18485, loss 0.530309.
Train: 2018-08-09T11:45:50.383959: step 18486, loss 0.546861.
Train: 2018-08-09T11:45:50.477658: step 18487, loss 0.465739.
Train: 2018-08-09T11:45:50.571415: step 18488, loss 0.547154.
Train: 2018-08-09T11:45:50.665143: step 18489, loss 0.498181.
Train: 2018-08-09T11:45:50.758841: step 18490, loss 0.54624.
Test: 2018-08-09T11:45:51.260236: step 18490, loss 0.548351.
Train: 2018-08-09T11:45:51.354003: step 18491, loss 0.547094.
Train: 2018-08-09T11:45:51.447722: step 18492, loss 0.480425.
Train: 2018-08-09T11:45:51.541419: step 18493, loss 0.513411.
Train: 2018-08-09T11:45:51.635148: step 18494, loss 0.597102.
Train: 2018-08-09T11:45:51.728909: step 18495, loss 0.581113.
Train: 2018-08-09T11:45:51.822633: step 18496, loss 0.578913.
Train: 2018-08-09T11:45:51.916365: step 18497, loss 0.49274.
Train: 2018-08-09T11:45:52.010060: step 18498, loss 0.671086.
Train: 2018-08-09T11:45:52.103788: step 18499, loss 0.681416.
Train: 2018-08-09T11:45:52.197545: step 18500, loss 0.596129.
Test: 2018-08-09T11:45:52.697400: step 18500, loss 0.54822.
Train: 2018-08-09T11:45:53.322281: step 18501, loss 0.54585.
Train: 2018-08-09T11:45:53.416012: step 18502, loss 0.64378.
Train: 2018-08-09T11:45:53.509736: step 18503, loss 0.579652.
Train: 2018-08-09T11:45:53.603464: step 18504, loss 0.562659.
Train: 2018-08-09T11:45:53.697192: step 18505, loss 0.546676.
Train: 2018-08-09T11:45:53.806511: step 18506, loss 0.610842.
Train: 2018-08-09T11:45:53.900244: step 18507, loss 0.515027.
Train: 2018-08-09T11:45:53.993997: step 18508, loss 0.530997.
Train: 2018-08-09T11:45:54.087725: step 18509, loss 0.562657.
Train: 2018-08-09T11:45:54.181453: step 18510, loss 0.531084.
Test: 2018-08-09T11:45:54.681305: step 18510, loss 0.550296.
Train: 2018-08-09T11:45:54.759443: step 18511, loss 0.578437.
Train: 2018-08-09T11:45:54.853171: step 18512, loss 0.482275.
Train: 2018-08-09T11:45:54.949176: step 18513, loss 0.531076.
Train: 2018-08-09T11:45:55.042935: step 18514, loss 0.514172.
Train: 2018-08-09T11:45:55.136661: step 18515, loss 0.480298.
Train: 2018-08-09T11:45:55.230389: step 18516, loss 0.581172.
Train: 2018-08-09T11:45:55.324117: step 18517, loss 0.513784.
Train: 2018-08-09T11:45:55.417845: step 18518, loss 0.543447.
Train: 2018-08-09T11:45:55.511573: step 18519, loss 0.629239.
Train: 2018-08-09T11:45:55.605301: step 18520, loss 0.651487.
Test: 2018-08-09T11:45:56.109451: step 18520, loss 0.546815.
Train: 2018-08-09T11:45:56.187588: step 18521, loss 0.625193.
Train: 2018-08-09T11:45:56.281315: step 18522, loss 0.528313.
Train: 2018-08-09T11:45:56.375043: step 18523, loss 0.511273.
Train: 2018-08-09T11:45:56.468771: step 18524, loss 0.488846.
Train: 2018-08-09T11:45:56.562500: step 18525, loss 0.615948.
Train: 2018-08-09T11:45:56.656197: step 18526, loss 0.477728.
Train: 2018-08-09T11:45:56.749955: step 18527, loss 0.507753.
Train: 2018-08-09T11:45:56.843653: step 18528, loss 0.669796.
Train: 2018-08-09T11:45:56.934145: step 18529, loss 0.549022.
Train: 2018-08-09T11:45:57.027870: step 18530, loss 0.455629.
Test: 2018-08-09T11:45:57.527722: step 18530, loss 0.547157.
Train: 2018-08-09T11:45:57.621474: step 18531, loss 0.51295.
Train: 2018-08-09T11:45:57.699587: step 18532, loss 0.562999.
Train: 2018-08-09T11:45:57.793285: step 18533, loss 0.677871.
Train: 2018-08-09T11:45:57.887038: step 18534, loss 0.542963.
Train: 2018-08-09T11:45:57.980770: step 18535, loss 0.551567.
Train: 2018-08-09T11:45:58.074498: step 18536, loss 0.573777.
Train: 2018-08-09T11:45:58.168226: step 18537, loss 0.668077.
Train: 2018-08-09T11:45:58.261955: step 18538, loss 0.456434.
Train: 2018-08-09T11:45:58.355682: step 18539, loss 0.477328.
Train: 2018-08-09T11:45:58.449410: step 18540, loss 0.561451.
Test: 2018-08-09T11:45:58.933665: step 18540, loss 0.548537.
Train: 2018-08-09T11:45:59.027399: step 18541, loss 0.542695.
Train: 2018-08-09T11:45:59.121126: step 18542, loss 0.63277.
Train: 2018-08-09T11:45:59.214854: step 18543, loss 0.519073.
Train: 2018-08-09T11:45:59.308553: step 18544, loss 0.553215.
Train: 2018-08-09T11:45:59.402281: step 18545, loss 0.617748.
Train: 2018-08-09T11:45:59.480417: step 18546, loss 0.558078.
Train: 2018-08-09T11:45:59.574145: step 18547, loss 0.505744.
Train: 2018-08-09T11:45:59.667873: step 18548, loss 0.553502.
Train: 2018-08-09T11:45:59.761602: step 18549, loss 0.564986.
Train: 2018-08-09T11:45:59.855328: step 18550, loss 0.463336.
Test: 2018-08-09T11:46:00.339590: step 18550, loss 0.547289.
Train: 2018-08-09T11:46:00.433286: step 18551, loss 0.68128.
Train: 2018-08-09T11:46:00.527014: step 18552, loss 0.446694.
Train: 2018-08-09T11:46:00.620776: step 18553, loss 0.511643.
Train: 2018-08-09T11:46:00.698880: step 18554, loss 0.58176.
Train: 2018-08-09T11:46:00.792608: step 18555, loss 0.445977.
Train: 2018-08-09T11:46:00.886305: step 18556, loss 0.531709.
Train: 2018-08-09T11:46:00.980064: step 18557, loss 0.549405.
Train: 2018-08-09T11:46:01.073786: step 18558, loss 0.527839.
Train: 2018-08-09T11:46:01.151898: step 18559, loss 0.562243.
Train: 2018-08-09T11:46:01.245628: step 18560, loss 0.513114.
Test: 2018-08-09T11:46:01.745518: step 18560, loss 0.547636.
Train: 2018-08-09T11:46:01.839236: step 18561, loss 0.594095.
Train: 2018-08-09T11:46:01.934307: step 18562, loss 0.582309.
Train: 2018-08-09T11:46:02.028008: step 18563, loss 0.44092.
Train: 2018-08-09T11:46:02.106145: step 18564, loss 0.557879.
Train: 2018-08-09T11:46:02.199876: step 18565, loss 0.601055.
Train: 2018-08-09T11:46:02.293600: step 18566, loss 0.527692.
Train: 2018-08-09T11:46:02.387298: step 18567, loss 0.440552.
Train: 2018-08-09T11:46:02.465405: step 18568, loss 0.529369.
Train: 2018-08-09T11:46:02.566867: step 18569, loss 0.561629.
Train: 2018-08-09T11:46:02.660600: step 18570, loss 0.58002.
Test: 2018-08-09T11:46:03.144856: step 18570, loss 0.547068.
Train: 2018-08-09T11:46:03.238590: step 18571, loss 0.604379.
Train: 2018-08-09T11:46:03.332316: step 18572, loss 0.473295.
Train: 2018-08-09T11:46:03.410423: step 18573, loss 0.602022.
Train: 2018-08-09T11:46:03.504152: step 18574, loss 0.469411.
Train: 2018-08-09T11:46:03.597879: step 18575, loss 0.489268.
Train: 2018-08-09T11:46:03.675986: step 18576, loss 0.567508.
Train: 2018-08-09T11:46:03.769684: step 18577, loss 0.508917.
Train: 2018-08-09T11:46:03.873075: step 18578, loss 0.547118.
Train: 2018-08-09T11:46:03.949951: step 18579, loss 0.659175.
Train: 2018-08-09T11:46:04.043685: step 18580, loss 0.729999.
Test: 2018-08-09T11:46:04.543568: step 18580, loss 0.54743.
Train: 2018-08-09T11:46:04.637295: step 18581, loss 0.474026.
Train: 2018-08-09T11:46:04.731022: step 18582, loss 0.510983.
Train: 2018-08-09T11:46:04.809100: step 18583, loss 0.584259.
Train: 2018-08-09T11:46:04.908673: step 18584, loss 0.578434.
Train: 2018-08-09T11:46:04.986781: step 18585, loss 0.545755.
Train: 2018-08-09T11:46:05.080538: step 18586, loss 0.628702.
Train: 2018-08-09T11:46:05.174265: step 18587, loss 0.496086.
Train: 2018-08-09T11:46:05.252374: step 18588, loss 0.645854.
Train: 2018-08-09T11:46:05.346070: step 18589, loss 0.611462.
Train: 2018-08-09T11:46:05.439797: step 18590, loss 0.513035.
Test: 2018-08-09T11:46:05.935459: step 18590, loss 0.547894.
Train: 2018-08-09T11:46:06.013596: step 18591, loss 0.678684.
Train: 2018-08-09T11:46:06.107329: step 18592, loss 0.643637.
Train: 2018-08-09T11:46:06.201052: step 18593, loss 0.514545.
Train: 2018-08-09T11:46:06.279128: step 18594, loss 0.530773.
Train: 2018-08-09T11:46:06.372883: step 18595, loss 0.531078.
Train: 2018-08-09T11:46:06.466584: step 18596, loss 0.59479.
Train: 2018-08-09T11:46:06.560343: step 18597, loss 0.610571.
Train: 2018-08-09T11:46:06.638449: step 18598, loss 0.483918.
Train: 2018-08-09T11:46:06.732176: step 18599, loss 0.65792.
Train: 2018-08-09T11:46:06.825876: step 18600, loss 0.515764.
Test: 2018-08-09T11:46:07.310137: step 18600, loss 0.548928.
Train: 2018-08-09T11:46:07.856882: step 18601, loss 0.657661.
Train: 2018-08-09T11:46:07.935018: step 18602, loss 0.531697.
Train: 2018-08-09T11:46:08.028747: step 18603, loss 0.563173.
Train: 2018-08-09T11:46:08.122474: step 18604, loss 0.516125.
Train: 2018-08-09T11:46:08.200580: step 18605, loss 0.563195.
Train: 2018-08-09T11:46:08.294308: step 18606, loss 0.625959.
Train: 2018-08-09T11:46:08.388007: step 18607, loss 0.594553.
Train: 2018-08-09T11:46:08.466144: step 18608, loss 0.531873.
Train: 2018-08-09T11:46:08.559872: step 18609, loss 0.672772.
Train: 2018-08-09T11:46:08.653599: step 18610, loss 0.67265.
Test: 2018-08-09T11:46:09.140121: step 18610, loss 0.552288.
Train: 2018-08-09T11:46:09.233849: step 18611, loss 0.563343.
Train: 2018-08-09T11:46:09.311955: step 18612, loss 0.578914.
Train: 2018-08-09T11:46:09.405715: step 18613, loss 0.470654.
Train: 2018-08-09T11:46:09.499439: step 18614, loss 0.609859.
Train: 2018-08-09T11:46:09.577547: step 18615, loss 0.48633.
Train: 2018-08-09T11:46:09.671270: step 18616, loss 0.671582.
Train: 2018-08-09T11:46:09.765003: step 18617, loss 0.62525.
Train: 2018-08-09T11:46:09.843104: step 18618, loss 0.486622.
Train: 2018-08-09T11:46:09.936807: step 18619, loss 0.594331.
Train: 2018-08-09T11:46:10.014947: step 18620, loss 0.609697.
Test: 2018-08-09T11:46:10.514825: step 18620, loss 0.552121.
Train: 2018-08-09T11:46:10.608554: step 18621, loss 0.548231.
Train: 2018-08-09T11:46:10.686663: step 18622, loss 0.563613.
Train: 2018-08-09T11:46:10.780360: step 18623, loss 0.502193.
Train: 2018-08-09T11:46:10.874117: step 18624, loss 0.671256.
Train: 2018-08-09T11:46:10.952904: step 18625, loss 0.609728.
Train: 2018-08-09T11:46:11.046628: step 18626, loss 0.517536.
Train: 2018-08-09T11:46:11.124709: step 18627, loss 0.533004.
Train: 2018-08-09T11:46:11.218436: step 18628, loss 0.594247.
Train: 2018-08-09T11:46:11.312194: step 18629, loss 0.486729.
Train: 2018-08-09T11:46:11.390299: step 18630, loss 0.517295.
Test: 2018-08-09T11:46:11.890153: step 18630, loss 0.552414.
Train: 2018-08-09T11:46:11.983911: step 18631, loss 0.548043.
Train: 2018-08-09T11:46:12.062018: step 18632, loss 0.671965.
Train: 2018-08-09T11:46:12.155745: step 18633, loss 0.609651.
Train: 2018-08-09T11:46:12.249473: step 18634, loss 0.53244.
Train: 2018-08-09T11:46:12.327583: step 18635, loss 0.594625.
Train: 2018-08-09T11:46:12.421308: step 18636, loss 0.59353.
Train: 2018-08-09T11:46:12.499415: step 18637, loss 0.578694.
Train: 2018-08-09T11:46:12.593112: step 18638, loss 0.437347.
Train: 2018-08-09T11:46:12.671244: step 18639, loss 0.594935.
Train: 2018-08-09T11:46:12.764948: step 18640, loss 0.576909.
Test: 2018-08-09T11:46:13.264836: step 18640, loss 0.548001.
Train: 2018-08-09T11:46:13.342936: step 18641, loss 0.564095.
Train: 2018-08-09T11:46:13.436693: step 18642, loss 0.578342.
Train: 2018-08-09T11:46:13.514799: step 18643, loss 0.531237.
Train: 2018-08-09T11:46:13.608528: step 18644, loss 0.648421.
Train: 2018-08-09T11:46:13.686635: step 18645, loss 0.533061.
Train: 2018-08-09T11:46:13.780362: step 18646, loss 0.481438.
Train: 2018-08-09T11:46:13.858469: step 18647, loss 0.549099.
Train: 2018-08-09T11:46:13.953596: step 18648, loss 0.547471.
Train: 2018-08-09T11:46:14.031735: step 18649, loss 0.548146.
Train: 2018-08-09T11:46:14.125460: step 18650, loss 0.611686.
Test: 2018-08-09T11:46:14.625338: step 18650, loss 0.545697.
Train: 2018-08-09T11:46:14.703449: step 18651, loss 0.642627.
Train: 2018-08-09T11:46:14.797146: step 18652, loss 0.482166.
Train: 2018-08-09T11:46:14.875284: step 18653, loss 0.402349.
Train: 2018-08-09T11:46:14.969006: step 18654, loss 0.62716.
Train: 2018-08-09T11:46:15.047118: step 18655, loss 0.575554.
Train: 2018-08-09T11:46:15.140846: step 18656, loss 0.565544.
Train: 2018-08-09T11:46:15.218957: step 18657, loss 0.505784.
Train: 2018-08-09T11:46:15.312652: step 18658, loss 0.544912.
Train: 2018-08-09T11:46:15.396405: step 18659, loss 0.54269.
Train: 2018-08-09T11:46:15.490134: step 18660, loss 0.706001.
Test: 2018-08-09T11:46:15.992392: step 18660, loss 0.547426.
Train: 2018-08-09T11:46:16.075971: step 18661, loss 0.601977.
Train: 2018-08-09T11:46:16.154082: step 18662, loss 0.543885.
Train: 2018-08-09T11:46:16.247806: step 18663, loss 0.478898.
Train: 2018-08-09T11:46:16.325881: step 18664, loss 0.681136.
Train: 2018-08-09T11:46:16.419640: step 18665, loss 0.515779.
Train: 2018-08-09T11:46:16.497747: step 18666, loss 0.579156.
Train: 2018-08-09T11:46:16.591475: step 18667, loss 0.513169.
Train: 2018-08-09T11:46:16.669550: step 18668, loss 0.630853.
Train: 2018-08-09T11:46:16.763311: step 18669, loss 0.510859.
Train: 2018-08-09T11:46:16.841416: step 18670, loss 0.579108.
Test: 2018-08-09T11:46:17.341294: step 18670, loss 0.549386.
Train: 2018-08-09T11:46:17.434997: step 18671, loss 0.564457.
Train: 2018-08-09T11:46:17.513103: step 18672, loss 0.562679.
Train: 2018-08-09T11:46:17.606860: step 18673, loss 0.563708.
Train: 2018-08-09T11:46:17.700583: step 18674, loss 0.564246.
Train: 2018-08-09T11:46:17.778699: step 18675, loss 0.67374.
Train: 2018-08-09T11:46:17.872417: step 18676, loss 0.483341.
Train: 2018-08-09T11:46:17.950530: step 18677, loss 0.546772.
Train: 2018-08-09T11:46:18.044258: step 18678, loss 0.659425.
Train: 2018-08-09T11:46:18.122335: step 18679, loss 0.579821.
Train: 2018-08-09T11:46:18.216092: step 18680, loss 0.626374.
Test: 2018-08-09T11:46:18.715978: step 18680, loss 0.55202.
Train: 2018-08-09T11:46:18.794082: step 18681, loss 0.531796.
Train: 2018-08-09T11:46:18.887809: step 18682, loss 0.610265.
Train: 2018-08-09T11:46:18.965915: step 18683, loss 0.610249.
Train: 2018-08-09T11:46:19.059643: step 18684, loss 0.547817.
Train: 2018-08-09T11:46:19.137750: step 18685, loss 0.470096.
Train: 2018-08-09T11:46:19.231447: step 18686, loss 0.501253.
Train: 2018-08-09T11:46:19.309555: step 18687, loss 0.51636.
Train: 2018-08-09T11:46:19.403316: step 18688, loss 0.500445.
Train: 2018-08-09T11:46:19.481420: step 18689, loss 0.546569.
Train: 2018-08-09T11:46:19.575142: step 18690, loss 0.546978.
Test: 2018-08-09T11:46:20.061770: step 18690, loss 0.548255.
Train: 2018-08-09T11:46:20.155529: step 18691, loss 0.56217.
Train: 2018-08-09T11:46:20.233605: step 18692, loss 0.58141.
Train: 2018-08-09T11:46:20.327363: step 18693, loss 0.595628.
Train: 2018-08-09T11:46:20.405470: step 18694, loss 0.560933.
Train: 2018-08-09T11:46:20.499197: step 18695, loss 0.546372.
Train: 2018-08-09T11:46:20.577274: step 18696, loss 0.494585.
Train: 2018-08-09T11:46:20.671026: step 18697, loss 0.541646.
Train: 2018-08-09T11:46:20.749139: step 18698, loss 0.585842.
Train: 2018-08-09T11:46:20.842866: step 18699, loss 0.570353.
Train: 2018-08-09T11:46:20.920973: step 18700, loss 0.527117.
Test: 2018-08-09T11:46:21.420830: step 18700, loss 0.548018.
Train: 2018-08-09T11:46:21.968215: step 18701, loss 0.563007.
Train: 2018-08-09T11:46:22.046321: step 18702, loss 0.545472.
Train: 2018-08-09T11:46:22.140019: step 18703, loss 0.581958.
Train: 2018-08-09T11:46:22.233778: step 18704, loss 0.558137.
Train: 2018-08-09T11:46:22.311854: step 18705, loss 0.676019.
Train: 2018-08-09T11:46:22.405582: step 18706, loss 0.564233.
Train: 2018-08-09T11:46:22.483715: step 18707, loss 0.566266.
Train: 2018-08-09T11:46:22.577446: step 18708, loss 0.670646.
Train: 2018-08-09T11:46:22.655553: step 18709, loss 0.480412.
Train: 2018-08-09T11:46:22.749280: step 18710, loss 0.546085.
Test: 2018-08-09T11:46:23.233511: step 18710, loss 0.550636.
Train: 2018-08-09T11:46:23.327263: step 18711, loss 0.546816.
Train: 2018-08-09T11:46:23.405376: step 18712, loss 0.546555.
Train: 2018-08-09T11:46:23.499105: step 18713, loss 0.532653.
Train: 2018-08-09T11:46:23.577210: step 18714, loss 0.610968.
Train: 2018-08-09T11:46:23.670908: step 18715, loss 0.56365.
Train: 2018-08-09T11:46:23.749046: step 18716, loss 0.674281.
Train: 2018-08-09T11:46:23.842742: step 18717, loss 0.610644.
Train: 2018-08-09T11:46:23.920880: step 18718, loss 0.689405.
Train: 2018-08-09T11:46:24.014608: step 18719, loss 0.688859.
Train: 2018-08-09T11:46:24.092715: step 18720, loss 0.547646.
Test: 2018-08-09T11:46:24.592567: step 18720, loss 0.549382.
Train: 2018-08-09T11:46:24.686323: step 18721, loss 0.578906.
Train: 2018-08-09T11:46:24.764431: step 18722, loss 0.594403.
Train: 2018-08-09T11:46:24.842537: step 18723, loss 0.563543.
Train: 2018-08-09T11:46:24.937621: step 18724, loss 0.532923.
Train: 2018-08-09T11:46:25.015728: step 18725, loss 0.609613.
Train: 2018-08-09T11:46:25.109459: step 18726, loss 0.640075.
Train: 2018-08-09T11:46:25.187534: step 18727, loss 0.655081.
Train: 2018-08-09T11:46:25.281261: step 18728, loss 0.563954.
Train: 2018-08-09T11:46:25.359400: step 18729, loss 0.548982.
Train: 2018-08-09T11:46:25.453126: step 18730, loss 0.42889.
Test: 2018-08-09T11:46:25.952977: step 18730, loss 0.55346.
Train: 2018-08-09T11:46:26.031114: step 18731, loss 0.504027.
Train: 2018-08-09T11:46:26.109221: step 18732, loss 0.533989.
Train: 2018-08-09T11:46:26.202948: step 18733, loss 0.609327.
Train: 2018-08-09T11:46:26.281056: step 18734, loss 0.518677.
Train: 2018-08-09T11:46:26.374787: step 18735, loss 0.609415.
Train: 2018-08-09T11:46:26.452892: step 18736, loss 0.609358.
Train: 2018-08-09T11:46:26.546618: step 18737, loss 0.563891.
Train: 2018-08-09T11:46:26.624728: step 18738, loss 0.579112.
Train: 2018-08-09T11:46:26.718423: step 18739, loss 0.51821.
Train: 2018-08-09T11:46:26.796559: step 18740, loss 0.609426.
Test: 2018-08-09T11:46:27.297838: step 18740, loss 0.551141.
Train: 2018-08-09T11:46:27.375938: step 18741, loss 0.563864.
Train: 2018-08-09T11:46:27.469664: step 18742, loss 0.548423.
Train: 2018-08-09T11:46:27.547775: step 18743, loss 0.579255.
Train: 2018-08-09T11:46:27.641470: step 18744, loss 0.502125.
Train: 2018-08-09T11:46:27.719606: step 18745, loss 0.502063.
Train: 2018-08-09T11:46:27.813303: step 18746, loss 0.578757.
Train: 2018-08-09T11:46:27.907032: step 18747, loss 0.532148.
Train: 2018-08-09T11:46:27.985168: step 18748, loss 0.594572.
Train: 2018-08-09T11:46:28.063244: step 18749, loss 0.388508.
Train: 2018-08-09T11:46:28.157002: step 18750, loss 0.579277.
Test: 2018-08-09T11:46:28.656870: step 18750, loss 0.553165.
Train: 2018-08-09T11:46:28.734993: step 18751, loss 0.597072.
Train: 2018-08-09T11:46:28.828719: step 18752, loss 0.612661.
Train: 2018-08-09T11:46:28.909153: step 18753, loss 0.477701.
Train: 2018-08-09T11:46:29.002912: step 18754, loss 0.595378.
Train: 2018-08-09T11:46:29.080989: step 18755, loss 0.594148.
Train: 2018-08-09T11:46:29.174716: step 18756, loss 0.614776.
Train: 2018-08-09T11:46:29.252853: step 18757, loss 0.630626.
Train: 2018-08-09T11:46:29.346584: step 18758, loss 0.548158.
Train: 2018-08-09T11:46:29.424658: step 18759, loss 0.546868.
Train: 2018-08-09T11:46:29.518387: step 18760, loss 0.59492.
Test: 2018-08-09T11:46:30.018269: step 18760, loss 0.551213.
Train: 2018-08-09T11:46:30.096405: step 18761, loss 0.43338.
Train: 2018-08-09T11:46:30.190133: step 18762, loss 0.447853.
Train: 2018-08-09T11:46:30.268240: step 18763, loss 0.659832.
Train: 2018-08-09T11:46:30.361937: step 18764, loss 0.579417.
Train: 2018-08-09T11:46:30.440068: step 18765, loss 0.52962.
Train: 2018-08-09T11:46:30.518181: step 18766, loss 0.49824.
Train: 2018-08-09T11:46:30.611907: step 18767, loss 0.5799.
Train: 2018-08-09T11:46:30.705607: step 18768, loss 0.593774.
Train: 2018-08-09T11:46:30.783743: step 18769, loss 0.513139.
Train: 2018-08-09T11:46:30.877465: step 18770, loss 0.512176.
Test: 2018-08-09T11:46:31.364075: step 18770, loss 0.547367.
Train: 2018-08-09T11:46:31.442210: step 18771, loss 0.543702.
Train: 2018-08-09T11:46:31.535937: step 18772, loss 0.446959.
Train: 2018-08-09T11:46:31.629666: step 18773, loss 0.440469.
Train: 2018-08-09T11:46:31.707774: step 18774, loss 0.493914.
Train: 2018-08-09T11:46:31.801471: step 18775, loss 0.542572.
Train: 2018-08-09T11:46:31.879577: step 18776, loss 0.626743.
Train: 2018-08-09T11:46:31.973328: step 18777, loss 0.506464.
Train: 2018-08-09T11:46:32.051441: step 18778, loss 0.587788.
Train: 2018-08-09T11:46:32.129518: step 18779, loss 0.581162.
Train: 2018-08-09T11:46:32.223275: step 18780, loss 0.471084.
Test: 2018-08-09T11:46:32.723153: step 18780, loss 0.547752.
Train: 2018-08-09T11:46:32.801265: step 18781, loss 0.541596.
Train: 2018-08-09T11:46:32.894963: step 18782, loss 0.60477.
Train: 2018-08-09T11:46:32.975362: step 18783, loss 0.497153.
Train: 2018-08-09T11:46:33.069089: step 18784, loss 0.658669.
Train: 2018-08-09T11:46:33.147226: step 18785, loss 0.492366.
Train: 2018-08-09T11:46:33.240957: step 18786, loss 0.583971.
Train: 2018-08-09T11:46:33.319060: step 18787, loss 0.559807.
Train: 2018-08-09T11:46:33.412789: step 18788, loss 0.569065.
Train: 2018-08-09T11:46:33.490864: step 18789, loss 0.564518.
Train: 2018-08-09T11:46:33.584592: step 18790, loss 0.61343.
Test: 2018-08-09T11:46:34.084482: step 18790, loss 0.548821.
Train: 2018-08-09T11:46:34.162615: step 18791, loss 0.443013.
Train: 2018-08-09T11:46:34.256309: step 18792, loss 0.577264.
Train: 2018-08-09T11:46:34.334417: step 18793, loss 0.734191.
Train: 2018-08-09T11:46:34.412552: step 18794, loss 0.594824.
Train: 2018-08-09T11:46:34.506249: step 18795, loss 0.646442.
Train: 2018-08-09T11:46:34.584358: step 18796, loss 0.481081.
Train: 2018-08-09T11:46:34.678084: step 18797, loss 0.546365.
Train: 2018-08-09T11:46:34.756222: step 18798, loss 0.480758.
Train: 2018-08-09T11:46:34.849918: step 18799, loss 0.66016.
Train: 2018-08-09T11:46:34.929434: step 18800, loss 0.562622.
Test: 2018-08-09T11:46:35.428543: step 18800, loss 0.549993.
Train: 2018-08-09T11:46:35.975319: step 18801, loss 0.627079.
Train: 2018-08-09T11:46:36.053420: step 18802, loss 0.643264.
Train: 2018-08-09T11:46:36.147154: step 18803, loss 0.54671.
Train: 2018-08-09T11:46:36.225230: step 18804, loss 0.514932.
Train: 2018-08-09T11:46:36.303339: step 18805, loss 0.562897.
Train: 2018-08-09T11:46:36.397097: step 18806, loss 0.626609.
Train: 2018-08-09T11:46:36.475202: step 18807, loss 0.499417.
Train: 2018-08-09T11:46:36.568900: step 18808, loss 0.515495.
Train: 2018-08-09T11:46:36.662658: step 18809, loss 0.610601.
Train: 2018-08-09T11:46:36.740735: step 18810, loss 0.531317.
Test: 2018-08-09T11:46:37.242064: step 18810, loss 0.549963.
Train: 2018-08-09T11:46:37.320201: step 18811, loss 0.594695.
Train: 2018-08-09T11:46:37.413923: step 18812, loss 0.578856.
Train: 2018-08-09T11:46:37.495234: step 18813, loss 0.578866.
Train: 2018-08-09T11:46:37.588964: step 18814, loss 0.705335.
Train: 2018-08-09T11:46:37.667069: step 18815, loss 0.500005.
Train: 2018-08-09T11:46:37.760797: step 18816, loss 0.61036.
Train: 2018-08-09T11:46:37.838902: step 18817, loss 0.547403.
Train: 2018-08-09T11:46:37.932630: step 18818, loss 0.453224.
Train: 2018-08-09T11:46:38.010735: step 18819, loss 0.641787.
Train: 2018-08-09T11:46:38.104435: step 18820, loss 0.641765.
Test: 2018-08-09T11:46:38.604318: step 18820, loss 0.550255.
Train: 2018-08-09T11:46:38.682453: step 18821, loss 0.469061.
Train: 2018-08-09T11:46:38.776184: step 18822, loss 0.500345.
Train: 2018-08-09T11:46:38.854288: step 18823, loss 0.563289.
Train: 2018-08-09T11:46:38.948016: step 18824, loss 0.468489.
Train: 2018-08-09T11:46:39.026125: step 18825, loss 0.721193.
Train: 2018-08-09T11:46:39.119850: step 18826, loss 0.626486.
Train: 2018-08-09T11:46:39.197957: step 18827, loss 0.547441.
Train: 2018-08-09T11:46:39.291686: step 18828, loss 0.594892.
Train: 2018-08-09T11:46:39.369792: step 18829, loss 0.594568.
Train: 2018-08-09T11:46:39.463519: step 18830, loss 0.626026.
Test: 2018-08-09T11:46:39.963372: step 18830, loss 0.549672.
Train: 2018-08-09T11:46:40.041508: step 18831, loss 0.625925.
Train: 2018-08-09T11:46:40.135236: step 18832, loss 0.485015.
Train: 2018-08-09T11:46:40.213343: step 18833, loss 0.594515.
Train: 2018-08-09T11:46:40.307072: step 18834, loss 0.5945.
Train: 2018-08-09T11:46:40.385148: step 18835, loss 0.563304.
Train: 2018-08-09T11:46:40.478905: step 18836, loss 0.501036.
Train: 2018-08-09T11:46:40.556982: step 18837, loss 0.46987.
Train: 2018-08-09T11:46:40.650742: step 18838, loss 0.625707.
Train: 2018-08-09T11:46:40.744437: step 18839, loss 0.625738.
Train: 2018-08-09T11:46:40.822544: step 18840, loss 0.438349.
Test: 2018-08-09T11:46:41.323868: step 18840, loss 0.549754.
Train: 2018-08-09T11:46:41.401990: step 18841, loss 0.610163.
Train: 2018-08-09T11:46:41.495690: step 18842, loss 0.437843.
Train: 2018-08-09T11:46:41.573796: step 18843, loss 0.531662.
Train: 2018-08-09T11:46:41.667552: step 18844, loss 0.531429.
Train: 2018-08-09T11:46:41.761275: step 18845, loss 0.562827.
Train: 2018-08-09T11:46:41.839392: step 18846, loss 0.498797.
Train: 2018-08-09T11:46:41.933115: step 18847, loss 0.546062.
Train: 2018-08-09T11:46:42.011222: step 18848, loss 0.546618.
Train: 2018-08-09T11:46:42.104919: step 18849, loss 0.565291.
Train: 2018-08-09T11:46:42.183057: step 18850, loss 0.564092.
Test: 2018-08-09T11:46:42.682943: step 18850, loss 0.547466.
Train: 2018-08-09T11:46:42.776650: step 18851, loss 0.512248.
Train: 2018-08-09T11:46:42.854776: step 18852, loss 0.57774.
Train: 2018-08-09T11:46:42.950015: step 18853, loss 0.579658.
Train: 2018-08-09T11:46:43.028127: step 18854, loss 0.547233.
Train: 2018-08-09T11:46:43.121844: step 18855, loss 0.580561.
Train: 2018-08-09T11:46:43.199928: step 18856, loss 0.594606.
Train: 2018-08-09T11:46:43.293655: step 18857, loss 0.629122.
Train: 2018-08-09T11:46:43.371791: step 18858, loss 0.496556.
Train: 2018-08-09T11:46:43.465513: step 18859, loss 0.562765.
Train: 2018-08-09T11:46:43.559247: step 18860, loss 0.595997.
Test: 2018-08-09T11:46:44.043510: step 18860, loss 0.54894.
Train: 2018-08-09T11:46:44.137237: step 18861, loss 0.546448.
Train: 2018-08-09T11:46:44.215342: step 18862, loss 0.513119.
Train: 2018-08-09T11:46:44.309041: step 18863, loss 0.628275.
Train: 2018-08-09T11:46:44.387177: step 18864, loss 0.496835.
Train: 2018-08-09T11:46:44.480876: step 18865, loss 0.546068.
Train: 2018-08-09T11:46:44.574633: step 18866, loss 0.611865.
Train: 2018-08-09T11:46:44.652710: step 18867, loss 0.51321.
Train: 2018-08-09T11:46:44.746438: step 18868, loss 0.612066.
Train: 2018-08-09T11:46:44.824545: step 18869, loss 0.546054.
Train: 2018-08-09T11:46:44.920567: step 18870, loss 0.529586.
Test: 2018-08-09T11:46:45.420450: step 18870, loss 0.550201.
Train: 2018-08-09T11:46:45.498587: step 18871, loss 0.661394.
Train: 2018-08-09T11:46:45.592285: step 18872, loss 0.529651.
Train: 2018-08-09T11:46:45.670421: step 18873, loss 0.595402.
Train: 2018-08-09T11:46:45.764119: step 18874, loss 0.464044.
Train: 2018-08-09T11:46:45.857877: step 18875, loss 0.578978.
Train: 2018-08-09T11:46:45.935954: step 18876, loss 0.644682.
Train: 2018-08-09T11:46:46.029682: step 18877, loss 0.611779.
Train: 2018-08-09T11:46:46.107818: step 18878, loss 0.546191.
Train: 2018-08-09T11:46:46.201546: step 18879, loss 0.497111.
Train: 2018-08-09T11:46:46.295244: step 18880, loss 0.497109.
Test: 2018-08-09T11:46:46.795126: step 18880, loss 0.548458.
Train: 2018-08-09T11:46:46.873263: step 18881, loss 0.529804.
Train: 2018-08-09T11:46:46.967565: step 18882, loss 0.513328.
Train: 2018-08-09T11:46:47.045703: step 18883, loss 0.595387.
Train: 2018-08-09T11:46:47.139430: step 18884, loss 0.513082.
Train: 2018-08-09T11:46:47.233157: step 18885, loss 0.480007.
Train: 2018-08-09T11:46:47.313952: step 18886, loss 0.579231.
Train: 2018-08-09T11:46:47.407651: step 18887, loss 0.662463.
Train: 2018-08-09T11:46:47.485787: step 18888, loss 0.528936.
Train: 2018-08-09T11:46:47.579485: step 18889, loss 0.645322.
Train: 2018-08-09T11:46:47.657621: step 18890, loss 0.612098.
Test: 2018-08-09T11:46:48.161011: step 18890, loss 0.546275.
Train: 2018-08-09T11:46:48.254739: step 18891, loss 0.546247.
Train: 2018-08-09T11:46:48.332846: step 18892, loss 0.545832.
Train: 2018-08-09T11:46:48.426573: step 18893, loss 0.595661.
Train: 2018-08-09T11:46:48.520301: step 18894, loss 0.578977.
Train: 2018-08-09T11:46:48.598436: step 18895, loss 0.52948.
Train: 2018-08-09T11:46:48.692135: step 18896, loss 0.562531.
Train: 2018-08-09T11:46:48.770242: step 18897, loss 0.661385.
Train: 2018-08-09T11:46:48.863999: step 18898, loss 0.578964.
Train: 2018-08-09T11:46:48.957728: step 18899, loss 0.562597.
Train: 2018-08-09T11:46:49.035834: step 18900, loss 0.595291.
Test: 2018-08-09T11:46:49.535689: step 18900, loss 0.55104.
Train: 2018-08-09T11:46:50.082462: step 18901, loss 0.464828.
Train: 2018-08-09T11:46:50.176161: step 18902, loss 0.530064.
Train: 2018-08-09T11:46:50.269887: step 18903, loss 0.53004.
Train: 2018-08-09T11:46:50.363645: step 18904, loss 0.497448.
Train: 2018-08-09T11:46:50.441722: step 18905, loss 0.611563.
Train: 2018-08-09T11:46:50.535483: step 18906, loss 0.611595.
Train: 2018-08-09T11:46:50.629208: step 18907, loss 0.595249.
Train: 2018-08-09T11:46:50.707314: step 18908, loss 0.562632.
Train: 2018-08-09T11:46:50.801048: step 18909, loss 0.562621.
Train: 2018-08-09T11:46:50.894770: step 18910, loss 0.660373.
Test: 2018-08-09T11:46:51.379033: step 18910, loss 0.54988.
Train: 2018-08-09T11:46:51.472730: step 18911, loss 0.546397.
Train: 2018-08-09T11:46:51.566488: step 18912, loss 0.432818.
Train: 2018-08-09T11:46:51.644565: step 18913, loss 0.546416.
Train: 2018-08-09T11:46:51.738291: step 18914, loss 0.513886.
Train: 2018-08-09T11:46:51.832019: step 18915, loss 0.546342.
Train: 2018-08-09T11:46:51.912466: step 18916, loss 0.578975.
Train: 2018-08-09T11:46:52.006224: step 18917, loss 0.513523.
Train: 2018-08-09T11:46:52.099955: step 18918, loss 0.611637.
Train: 2018-08-09T11:46:52.178029: step 18919, loss 0.546227.
Train: 2018-08-09T11:46:52.271756: step 18920, loss 0.611759.
Test: 2018-08-09T11:46:52.771640: step 18920, loss 0.549616.
Train: 2018-08-09T11:46:52.849777: step 18921, loss 0.562642.
Train: 2018-08-09T11:46:52.943505: step 18922, loss 0.513454.
Train: 2018-08-09T11:46:53.037233: step 18923, loss 0.463811.
Train: 2018-08-09T11:46:53.130930: step 18924, loss 0.562557.
Train: 2018-08-09T11:46:53.209037: step 18925, loss 0.512417.
Train: 2018-08-09T11:46:53.302765: step 18926, loss 0.595499.
Train: 2018-08-09T11:46:53.396491: step 18927, loss 0.578872.
Train: 2018-08-09T11:46:53.474598: step 18928, loss 0.528163.
Train: 2018-08-09T11:46:53.568326: step 18929, loss 0.494699.
Train: 2018-08-09T11:46:53.662085: step 18930, loss 0.599757.
Test: 2018-08-09T11:46:54.163280: step 18930, loss 0.548786.
Train: 2018-08-09T11:46:54.241416: step 18931, loss 0.580884.
Train: 2018-08-09T11:46:54.335144: step 18932, loss 0.631634.
Train: 2018-08-09T11:46:54.428871: step 18933, loss 0.613351.
Train: 2018-08-09T11:46:54.522601: step 18934, loss 0.629278.
Train: 2018-08-09T11:46:54.600707: step 18935, loss 0.579589.
Train: 2018-08-09T11:46:54.694435: step 18936, loss 0.629128.
Train: 2018-08-09T11:46:54.788163: step 18937, loss 0.595366.
Train: 2018-08-09T11:46:54.881890: step 18938, loss 0.546044.
Train: 2018-08-09T11:46:54.959997: step 18939, loss 0.49709.
Train: 2018-08-09T11:46:55.053724: step 18940, loss 0.595331.
Test: 2018-08-09T11:46:55.553577: step 18940, loss 0.547943.
Train: 2018-08-09T11:46:55.647305: step 18941, loss 0.481118.
Train: 2018-08-09T11:46:55.741044: step 18942, loss 0.660414.
Train: 2018-08-09T11:46:55.819169: step 18943, loss 0.465069.
Train: 2018-08-09T11:46:55.914291: step 18944, loss 0.530161.
Train: 2018-08-09T11:46:56.008050: step 18945, loss 0.562655.
Train: 2018-08-09T11:46:56.101779: step 18946, loss 0.643981.
Train: 2018-08-09T11:46:56.179884: step 18947, loss 0.465183.
Train: 2018-08-09T11:46:56.273601: step 18948, loss 0.562655.
Train: 2018-08-09T11:46:56.367339: step 18949, loss 0.595177.
Train: 2018-08-09T11:46:56.461067: step 18950, loss 0.611441.
Test: 2018-08-09T11:46:56.960945: step 18950, loss 0.551119.
Train: 2018-08-09T11:46:57.054678: step 18951, loss 0.595163.
Train: 2018-08-09T11:46:57.132784: step 18952, loss 0.481508.
Train: 2018-08-09T11:46:57.226514: step 18953, loss 0.595141.
Train: 2018-08-09T11:46:57.320240: step 18954, loss 0.61137.
Train: 2018-08-09T11:46:57.413939: step 18955, loss 0.578902.
Train: 2018-08-09T11:46:57.507666: step 18956, loss 0.562696.
Train: 2018-08-09T11:46:57.601394: step 18957, loss 0.49798.
Train: 2018-08-09T11:46:57.695131: step 18958, loss 0.497972.
Train: 2018-08-09T11:46:57.773259: step 18959, loss 0.530284.
Train: 2018-08-09T11:46:57.866957: step 18960, loss 0.530213.
Test: 2018-08-09T11:46:58.368306: step 18960, loss 0.550487.
Train: 2018-08-09T11:46:58.462059: step 18961, loss 0.51386.
Train: 2018-08-09T11:46:58.540150: step 18962, loss 0.595245.
Train: 2018-08-09T11:46:58.633893: step 18963, loss 0.614878.
Train: 2018-08-09T11:46:58.727622: step 18964, loss 0.529874.
Train: 2018-08-09T11:46:58.821319: step 18965, loss 0.562519.
Train: 2018-08-09T11:46:58.915077: step 18966, loss 0.628222.
Train: 2018-08-09T11:46:59.008805: step 18967, loss 0.578982.
Train: 2018-08-09T11:46:59.102503: step 18968, loss 0.546208.
Train: 2018-08-09T11:46:59.196230: step 18969, loss 0.546256.
Train: 2018-08-09T11:46:59.289958: step 18970, loss 0.595394.
Test: 2018-08-09T11:46:59.774219: step 18970, loss 0.548464.
Train: 2018-08-09T11:46:59.867977: step 18971, loss 0.562589.
Train: 2018-08-09T11:46:59.963994: step 18972, loss 0.529811.
Train: 2018-08-09T11:47:00.057723: step 18973, loss 0.546169.
Train: 2018-08-09T11:47:00.151451: step 18974, loss 0.595311.
Train: 2018-08-09T11:47:00.245178: step 18975, loss 0.611657.
Train: 2018-08-09T11:47:00.338875: step 18976, loss 0.51358.
Train: 2018-08-09T11:47:00.432634: step 18977, loss 0.628086.
Train: 2018-08-09T11:47:00.526365: step 18978, loss 0.70971.
Train: 2018-08-09T11:47:00.620090: step 18979, loss 0.562622.
Train: 2018-08-09T11:47:00.713817: step 18980, loss 0.578921.
Test: 2018-08-09T11:47:01.198050: step 18980, loss 0.548763.
Train: 2018-08-09T11:47:01.291807: step 18981, loss 0.627447.
Train: 2018-08-09T11:47:01.385535: step 18982, loss 0.498236.
Train: 2018-08-09T11:47:01.479233: step 18983, loss 0.643242.
Train: 2018-08-09T11:47:01.572961: step 18984, loss 0.562825.
Train: 2018-08-09T11:47:01.666718: step 18985, loss 0.466879.
Train: 2018-08-09T11:47:01.760416: step 18986, loss 0.530917.
Train: 2018-08-09T11:47:01.869766: step 18987, loss 0.594842.
Train: 2018-08-09T11:47:01.950166: step 18988, loss 0.51497.
Train: 2018-08-09T11:47:02.043897: step 18989, loss 0.514942.
Train: 2018-08-09T11:47:02.137622: step 18990, loss 0.498913.
Test: 2018-08-09T11:47:02.637475: step 18990, loss 0.549639.
Train: 2018-08-09T11:47:02.731226: step 18991, loss 0.514747.
Train: 2018-08-09T11:47:02.824954: step 18992, loss 0.546698.
Train: 2018-08-09T11:47:02.918675: step 18993, loss 0.578789.
Train: 2018-08-09T11:47:03.012416: step 18994, loss 0.59499.
Train: 2018-08-09T11:47:03.106146: step 18995, loss 0.449453.
Train: 2018-08-09T11:47:03.199841: step 18996, loss 0.644082.
Train: 2018-08-09T11:47:03.293599: step 18997, loss 0.497268.
Train: 2018-08-09T11:47:03.387328: step 18998, loss 0.513814.
Train: 2018-08-09T11:47:03.496648: step 18999, loss 0.579281.
Train: 2018-08-09T11:47:03.590404: step 19000, loss 0.530224.
Test: 2018-08-09T11:47:04.077057: step 19000, loss 0.549572.
Train: 2018-08-09T11:47:04.592526: step 19001, loss 0.479842.
Train: 2018-08-09T11:47:04.686285: step 19002, loss 0.545801.
Train: 2018-08-09T11:47:04.795604: step 19003, loss 0.561915.
Train: 2018-08-09T11:47:04.889362: step 19004, loss 0.545294.
Train: 2018-08-09T11:47:04.983093: step 19005, loss 0.51202.
Train: 2018-08-09T11:47:05.076788: step 19006, loss 0.562735.
Train: 2018-08-09T11:47:05.170515: step 19007, loss 0.61275.
Train: 2018-08-09T11:47:05.264243: step 19008, loss 0.630394.
Train: 2018-08-09T11:47:05.358005: step 19009, loss 0.528539.
Train: 2018-08-09T11:47:05.451729: step 19010, loss 0.647991.
Test: 2018-08-09T11:47:05.952183: step 19010, loss 0.543859.
Train: 2018-08-09T11:47:06.045936: step 19011, loss 0.697349.
Train: 2018-08-09T11:47:06.139638: step 19012, loss 0.477648.
Train: 2018-08-09T11:47:06.233398: step 19013, loss 0.630223.
Train: 2018-08-09T11:47:06.327126: step 19014, loss 0.562467.
Train: 2018-08-09T11:47:06.420857: step 19015, loss 0.478088.
Train: 2018-08-09T11:47:06.530202: step 19016, loss 0.628313.
Train: 2018-08-09T11:47:06.623930: step 19017, loss 0.545472.
Train: 2018-08-09T11:47:06.717663: step 19018, loss 0.561968.
Train: 2018-08-09T11:47:06.811389: step 19019, loss 0.645379.
Train: 2018-08-09T11:47:06.905113: step 19020, loss 0.4969.
Test: 2018-08-09T11:47:07.404977: step 19020, loss 0.549422.
Train: 2018-08-09T11:47:07.498724: step 19021, loss 0.529701.
Train: 2018-08-09T11:47:07.608044: step 19022, loss 0.51215.
Train: 2018-08-09T11:47:07.701772: step 19023, loss 0.479484.
Train: 2018-08-09T11:47:07.816846: step 19024, loss 0.49544.
Train: 2018-08-09T11:47:07.910574: step 19025, loss 0.561253.
Train: 2018-08-09T11:47:08.004301: step 19026, loss 0.59491.
Train: 2018-08-09T11:47:08.098029: step 19027, loss 0.493156.
Train: 2018-08-09T11:47:08.191756: step 19028, loss 0.599948.
Train: 2018-08-09T11:47:08.301106: step 19029, loss 0.628943.
Train: 2018-08-09T11:47:08.394834: step 19030, loss 0.5944.
Test: 2018-08-09T11:47:08.894686: step 19030, loss 0.546283.
Train: 2018-08-09T11:47:08.988444: step 19031, loss 0.56518.
Train: 2018-08-09T11:47:09.082172: step 19032, loss 0.667335.
Train: 2018-08-09T11:47:09.175900: step 19033, loss 0.580138.
Train: 2018-08-09T11:47:09.285250: step 19034, loss 0.545814.
Train: 2018-08-09T11:47:09.378977: step 19035, loss 0.512947.
Train: 2018-08-09T11:47:09.472674: step 19036, loss 0.445555.
Train: 2018-08-09T11:47:09.582054: step 19037, loss 0.47966.
Train: 2018-08-09T11:47:09.675782: step 19038, loss 0.511561.
Train: 2018-08-09T11:47:09.769480: step 19039, loss 0.613058.
Train: 2018-08-09T11:47:09.878829: step 19040, loss 0.511124.
Test: 2018-08-09T11:47:10.376146: step 19040, loss 0.548634.
Train: 2018-08-09T11:47:10.469902: step 19041, loss 0.595868.
Train: 2018-08-09T11:47:10.563632: step 19042, loss 0.511307.
Train: 2018-08-09T11:47:10.672950: step 19043, loss 0.563434.
Train: 2018-08-09T11:47:10.766678: step 19044, loss 0.509415.
Train: 2018-08-09T11:47:10.860436: step 19045, loss 0.544076.
Train: 2018-08-09T11:47:10.957510: step 19046, loss 0.442584.
Train: 2018-08-09T11:47:11.051269: step 19047, loss 0.474414.
Train: 2018-08-09T11:47:11.160624: step 19048, loss 0.524412.
Train: 2018-08-09T11:47:11.254345: step 19049, loss 0.531727.
Train: 2018-08-09T11:47:11.348042: step 19050, loss 0.636439.
Test: 2018-08-09T11:47:11.847951: step 19050, loss 0.548481.
Train: 2018-08-09T11:47:11.957274: step 19051, loss 0.526504.
Train: 2018-08-09T11:47:12.051034: step 19052, loss 0.529155.
Train: 2018-08-09T11:47:12.144761: step 19053, loss 0.513109.
Train: 2018-08-09T11:47:12.254080: step 19054, loss 0.691697.
Train: 2018-08-09T11:47:12.347840: step 19055, loss 0.42051.
Train: 2018-08-09T11:47:12.441566: step 19056, loss 0.548114.
Train: 2018-08-09T11:47:12.550915: step 19057, loss 0.613184.
Train: 2018-08-09T11:47:12.644643: step 19058, loss 0.540246.
Train: 2018-08-09T11:47:12.753961: step 19059, loss 0.524871.
Train: 2018-08-09T11:47:12.847720: step 19060, loss 0.527585.
Test: 2018-08-09T11:47:13.347596: step 19060, loss 0.549841.
Train: 2018-08-09T11:47:13.441330: step 19061, loss 0.584261.
Train: 2018-08-09T11:47:13.535059: step 19062, loss 0.525418.
Train: 2018-08-09T11:47:13.644378: step 19063, loss 0.543707.
Train: 2018-08-09T11:47:13.738104: step 19064, loss 0.507484.
Train: 2018-08-09T11:47:13.831832: step 19065, loss 0.598032.
Train: 2018-08-09T11:47:13.942677: step 19066, loss 0.474821.
Train: 2018-08-09T11:47:14.036375: step 19067, loss 0.508729.
Train: 2018-08-09T11:47:14.130135: step 19068, loss 0.543724.
Train: 2018-08-09T11:47:14.239496: step 19069, loss 0.559762.
Train: 2018-08-09T11:47:14.333180: step 19070, loss 0.506963.
Test: 2018-08-09T11:47:14.833062: step 19070, loss 0.548441.
Train: 2018-08-09T11:47:14.926820: step 19071, loss 0.580907.
Train: 2018-08-09T11:47:15.036163: step 19072, loss 0.48881.
Train: 2018-08-09T11:47:15.129900: step 19073, loss 0.525444.
Train: 2018-08-09T11:47:15.223594: step 19074, loss 0.545038.
Train: 2018-08-09T11:47:15.332974: step 19075, loss 0.618317.
Train: 2018-08-09T11:47:15.426673: step 19076, loss 0.525246.
Train: 2018-08-09T11:47:15.520434: step 19077, loss 0.597892.
Train: 2018-08-09T11:47:15.629782: step 19078, loss 0.583315.
Train: 2018-08-09T11:47:15.723507: step 19079, loss 0.541428.
Train: 2018-08-09T11:47:15.832857: step 19080, loss 0.634682.
Test: 2018-08-09T11:47:16.319441: step 19080, loss 0.547086.
Train: 2018-08-09T11:47:16.428788: step 19081, loss 0.599452.
Train: 2018-08-09T11:47:16.522547: step 19082, loss 0.544751.
Train: 2018-08-09T11:47:16.616277: step 19083, loss 0.561129.
Train: 2018-08-09T11:47:16.725624: step 19084, loss 0.61678.
Train: 2018-08-09T11:47:16.819323: step 19085, loss 0.648195.
Train: 2018-08-09T11:47:16.928670: step 19086, loss 0.597265.
Train: 2018-08-09T11:47:17.022429: step 19087, loss 0.594482.
Train: 2018-08-09T11:47:17.131749: step 19088, loss 0.527402.
Train: 2018-08-09T11:47:17.225506: step 19089, loss 0.597561.
Train: 2018-08-09T11:47:17.334849: step 19090, loss 0.578416.
Test: 2018-08-09T11:47:17.819087: step 19090, loss 0.550089.
Train: 2018-08-09T11:47:17.930006: step 19091, loss 0.546937.
Train: 2018-08-09T11:47:18.023734: step 19092, loss 0.547195.
Train: 2018-08-09T11:47:18.133053: step 19093, loss 0.544695.
Train: 2018-08-09T11:47:18.226815: step 19094, loss 0.497919.
Train: 2018-08-09T11:47:18.336131: step 19095, loss 0.578186.
Train: 2018-08-09T11:47:18.429888: step 19096, loss 0.531041.
Train: 2018-08-09T11:47:18.539238: step 19097, loss 0.562982.
Train: 2018-08-09T11:47:18.628872: step 19098, loss 0.562517.
Train: 2018-08-09T11:47:18.738221: step 19099, loss 0.579912.
Train: 2018-08-09T11:47:18.831950: step 19100, loss 0.546526.
Test: 2018-08-09T11:47:19.331801: step 19100, loss 0.549457.
Train: 2018-08-09T11:47:19.961920: step 19101, loss 0.546622.
Train: 2018-08-09T11:47:20.071269: step 19102, loss 0.563384.
Train: 2018-08-09T11:47:20.164968: step 19103, loss 0.611798.
Train: 2018-08-09T11:47:20.274317: step 19104, loss 0.546903.
Train: 2018-08-09T11:47:20.368075: step 19105, loss 0.579083.
Train: 2018-08-09T11:47:20.477423: step 19106, loss 0.530795.
Train: 2018-08-09T11:47:20.571151: step 19107, loss 0.530501.
Train: 2018-08-09T11:47:20.680501: step 19108, loss 0.579055.
Train: 2018-08-09T11:47:20.781896: step 19109, loss 0.498667.
Train: 2018-08-09T11:47:20.875655: step 19110, loss 0.626817.
Test: 2018-08-09T11:47:21.391129: step 19110, loss 0.550188.
Train: 2018-08-09T11:47:21.484886: step 19111, loss 0.466528.
Train: 2018-08-09T11:47:21.594206: step 19112, loss 0.498331.
Train: 2018-08-09T11:47:21.687933: step 19113, loss 0.546466.
Train: 2018-08-09T11:47:21.797283: step 19114, loss 0.546829.
Train: 2018-08-09T11:47:21.891040: step 19115, loss 0.432244.
Train: 2018-08-09T11:47:22.000389: step 19116, loss 0.463866.
Train: 2018-08-09T11:47:22.094121: step 19117, loss 0.562867.
Train: 2018-08-09T11:47:22.203468: step 19118, loss 0.597251.
Train: 2018-08-09T11:47:22.297194: step 19119, loss 0.597638.
Train: 2018-08-09T11:47:22.406514: step 19120, loss 0.528527.
Test: 2018-08-09T11:47:22.890800: step 19120, loss 0.546626.
Train: 2018-08-09T11:47:23.001615: step 19121, loss 0.460433.
Train: 2018-08-09T11:47:23.095342: step 19122, loss 0.596294.
Train: 2018-08-09T11:47:23.204692: step 19123, loss 0.510227.
Train: 2018-08-09T11:47:23.298418: step 19124, loss 0.511429.
Train: 2018-08-09T11:47:23.407768: step 19125, loss 0.649295.
Train: 2018-08-09T11:47:23.501497: step 19126, loss 0.613333.
Train: 2018-08-09T11:47:23.610815: step 19127, loss 0.458208.
Train: 2018-08-09T11:47:23.720166: step 19128, loss 0.579037.
Train: 2018-08-09T11:47:23.813894: step 19129, loss 0.494045.
Train: 2018-08-09T11:47:23.923271: step 19130, loss 0.52757.
Test: 2018-08-09T11:47:24.407529: step 19130, loss 0.545338.
Train: 2018-08-09T11:47:24.516853: step 19131, loss 0.45798.
Train: 2018-08-09T11:47:24.610610: step 19132, loss 0.525163.
Train: 2018-08-09T11:47:24.719959: step 19133, loss 0.673382.
Train: 2018-08-09T11:47:24.813682: step 19134, loss 0.668138.
Train: 2018-08-09T11:47:24.924446: step 19135, loss 0.649735.
Train: 2018-08-09T11:47:25.018174: step 19136, loss 0.632823.
Train: 2018-08-09T11:47:25.127523: step 19137, loss 0.546717.
Train: 2018-08-09T11:47:25.236843: step 19138, loss 0.494904.
Train: 2018-08-09T11:47:25.330601: step 19139, loss 0.562959.
Train: 2018-08-09T11:47:25.439944: step 19140, loss 0.596182.
Test: 2018-08-09T11:47:25.939834: step 19140, loss 0.549698.
Train: 2018-08-09T11:47:26.033562: step 19141, loss 0.579063.
Train: 2018-08-09T11:47:26.142909: step 19142, loss 0.629729.
Train: 2018-08-09T11:47:26.236607: step 19143, loss 0.545139.
Train: 2018-08-09T11:47:26.345987: step 19144, loss 0.562362.
Train: 2018-08-09T11:47:26.455305: step 19145, loss 0.545752.
Train: 2018-08-09T11:47:26.549034: step 19146, loss 0.545491.
Train: 2018-08-09T11:47:26.658416: step 19147, loss 0.612588.
Train: 2018-08-09T11:47:26.752140: step 19148, loss 0.595122.
Train: 2018-08-09T11:47:26.861490: step 19149, loss 0.529239.
Train: 2018-08-09T11:47:26.957493: step 19150, loss 0.562241.
Test: 2018-08-09T11:47:27.457376: step 19150, loss 0.546445.
Train: 2018-08-09T11:47:27.566695: step 19151, loss 0.612171.
Train: 2018-08-09T11:47:27.660422: step 19152, loss 0.677776.
Train: 2018-08-09T11:47:27.769803: step 19153, loss 0.578887.
Train: 2018-08-09T11:47:27.863531: step 19154, loss 0.579215.
Train: 2018-08-09T11:47:27.972849: step 19155, loss 0.56291.
Train: 2018-08-09T11:47:28.066607: step 19156, loss 0.53068.
Train: 2018-08-09T11:47:28.175956: step 19157, loss 0.417508.
Train: 2018-08-09T11:47:28.269655: step 19158, loss 0.578911.
Train: 2018-08-09T11:47:28.379035: step 19159, loss 0.530637.
Train: 2018-08-09T11:47:28.488383: step 19160, loss 0.578897.
Test: 2018-08-09T11:47:28.989633: step 19160, loss 0.551288.
Train: 2018-08-09T11:47:29.083392: step 19161, loss 0.562805.
Train: 2018-08-09T11:47:29.192710: step 19162, loss 0.562849.
Train: 2018-08-09T11:47:29.286437: step 19163, loss 0.449735.
Train: 2018-08-09T11:47:29.395818: step 19164, loss 0.6274.
Train: 2018-08-09T11:47:29.489514: step 19165, loss 0.497987.
Train: 2018-08-09T11:47:29.598896: step 19166, loss 0.530256.
Train: 2018-08-09T11:47:29.708213: step 19167, loss 0.530098.
Train: 2018-08-09T11:47:29.801941: step 19168, loss 0.676844.
Train: 2018-08-09T11:47:29.911322: step 19169, loss 0.627839.
Train: 2018-08-09T11:47:30.005049: step 19170, loss 0.644302.
Test: 2018-08-09T11:47:30.504902: step 19170, loss 0.550508.
Train: 2018-08-09T11:47:30.614280: step 19171, loss 0.578836.
Train: 2018-08-09T11:47:30.708009: step 19172, loss 0.595135.
Train: 2018-08-09T11:47:30.817358: step 19173, loss 0.562732.
Train: 2018-08-09T11:47:30.912488: step 19174, loss 0.562802.
Train: 2018-08-09T11:47:31.021869: step 19175, loss 0.611141.
Train: 2018-08-09T11:47:31.115596: step 19176, loss 0.530652.
Train: 2018-08-09T11:47:31.224916: step 19177, loss 0.755402.
Train: 2018-08-09T11:47:31.334265: step 19178, loss 0.658685.
Train: 2018-08-09T11:47:31.427992: step 19179, loss 0.547098.
Train: 2018-08-09T11:47:31.537371: step 19180, loss 0.626277.
Test: 2018-08-09T11:47:32.021602: step 19180, loss 0.551375.
Train: 2018-08-09T11:47:32.130982: step 19181, loss 0.657515.
Train: 2018-08-09T11:47:32.287195: step 19182, loss 0.531972.
Train: 2018-08-09T11:47:32.380923: step 19183, loss 0.547771.
Train: 2018-08-09T11:47:32.490272: step 19184, loss 0.609928.
Train: 2018-08-09T11:47:32.584001: step 19185, loss 0.578938.
Train: 2018-08-09T11:47:32.693349: step 19186, loss 0.548195.
Train: 2018-08-09T11:47:32.802700: step 19187, loss 0.717023.
Train: 2018-08-09T11:47:32.896395: step 19188, loss 0.670583.
Train: 2018-08-09T11:47:33.008173: step 19189, loss 0.548735.
Train: 2018-08-09T11:47:33.101902: step 19190, loss 0.564025.
Test: 2018-08-09T11:47:33.601815: step 19190, loss 0.550056.
Train: 2018-08-09T11:47:33.711134: step 19191, loss 0.624234.
Train: 2018-08-09T11:47:33.804892: step 19192, loss 0.624075.
Train: 2018-08-09T11:47:33.914240: step 19193, loss 0.534633.
Train: 2018-08-09T11:47:34.007969: step 19194, loss 0.520012.
Train: 2018-08-09T11:47:34.117319: step 19195, loss 0.520164.
Train: 2018-08-09T11:47:34.211045: step 19196, loss 0.608916.
Train: 2018-08-09T11:47:34.320389: step 19197, loss 0.579369.
Train: 2018-08-09T11:47:34.429745: step 19198, loss 0.535111.
Train: 2018-08-09T11:47:34.523472: step 19199, loss 0.638417.
Train: 2018-08-09T11:47:34.632821: step 19200, loss 0.638374.
Test: 2018-08-09T11:47:35.135025: step 19200, loss 0.550906.
Train: 2018-08-09T11:47:35.697423: step 19201, loss 0.505827.
Train: 2018-08-09T11:47:35.806771: step 19202, loss 0.579387.
Train: 2018-08-09T11:47:35.916091: step 19203, loss 0.564669.
Train: 2018-08-09T11:47:36.009818: step 19204, loss 0.57937.
Train: 2018-08-09T11:47:36.119168: step 19205, loss 0.594161.
Train: 2018-08-09T11:47:36.212927: step 19206, loss 0.475901.
Train: 2018-08-09T11:47:36.322274: step 19207, loss 0.520245.
Train: 2018-08-09T11:47:36.431593: step 19208, loss 0.564368.
Train: 2018-08-09T11:47:36.525352: step 19209, loss 0.58039.
Train: 2018-08-09T11:47:36.634701: step 19210, loss 0.579812.
Test: 2018-08-09T11:47:37.135983: step 19210, loss 0.550444.
Train: 2018-08-09T11:47:37.229739: step 19211, loss 0.518942.
Train: 2018-08-09T11:47:37.339090: step 19212, loss 0.56332.
Train: 2018-08-09T11:47:37.432817: step 19213, loss 0.579178.
Train: 2018-08-09T11:47:37.542136: step 19214, loss 0.669792.
Train: 2018-08-09T11:47:37.635893: step 19215, loss 0.487472.
Train: 2018-08-09T11:47:37.745243: step 19216, loss 0.517647.
Train: 2018-08-09T11:47:37.854592: step 19217, loss 0.53218.
Train: 2018-08-09T11:47:37.948320: step 19218, loss 0.517329.
Train: 2018-08-09T11:47:38.057669: step 19219, loss 0.469.
Train: 2018-08-09T11:47:38.151366: step 19220, loss 0.644015.
Test: 2018-08-09T11:47:38.666899: step 19220, loss 0.545945.
Train: 2018-08-09T11:47:38.760597: step 19221, loss 0.5788.
Train: 2018-08-09T11:47:38.869978: step 19222, loss 0.567929.
Train: 2018-08-09T11:47:38.966031: step 19223, loss 0.496276.
Train: 2018-08-09T11:47:39.075381: step 19224, loss 0.61091.
Train: 2018-08-09T11:47:39.169109: step 19225, loss 0.532639.
Train: 2018-08-09T11:47:39.278459: step 19226, loss 0.529182.
Train: 2018-08-09T11:47:39.387802: step 19227, loss 0.594944.
Train: 2018-08-09T11:47:39.481537: step 19228, loss 0.515571.
Train: 2018-08-09T11:47:39.590886: step 19229, loss 0.5776.
Train: 2018-08-09T11:47:39.684613: step 19230, loss 0.513605.
Test: 2018-08-09T11:47:40.184497: step 19230, loss 0.54829.
Train: 2018-08-09T11:47:40.293844: step 19231, loss 0.678037.
Train: 2018-08-09T11:47:40.387571: step 19232, loss 0.577868.
Train: 2018-08-09T11:47:40.491200: step 19233, loss 0.481877.
Train: 2018-08-09T11:47:40.600547: step 19234, loss 0.530719.
Train: 2018-08-09T11:47:40.694275: step 19235, loss 0.545167.
Train: 2018-08-09T11:47:40.803624: step 19236, loss 0.563014.
Train: 2018-08-09T11:47:40.897352: step 19237, loss 0.580409.
Train: 2018-08-09T11:47:40.997373: step 19238, loss 0.642195.
Train: 2018-08-09T11:47:41.106753: step 19239, loss 0.495358.
Train: 2018-08-09T11:47:41.200481: step 19240, loss 0.543974.
Test: 2018-08-09T11:47:41.715967: step 19240, loss 0.55011.
Train: 2018-08-09T11:47:41.809712: step 19241, loss 0.525219.
Train: 2018-08-09T11:47:41.919062: step 19242, loss 0.643392.
Train: 2018-08-09T11:47:42.012759: step 19243, loss 0.602021.
Train: 2018-08-09T11:47:42.122140: step 19244, loss 0.611635.
Train: 2018-08-09T11:47:42.215870: step 19245, loss 0.617567.
Train: 2018-08-09T11:47:42.325215: step 19246, loss 0.57772.
Train: 2018-08-09T11:47:42.418946: step 19247, loss 0.680367.
Train: 2018-08-09T11:47:42.528293: step 19248, loss 0.515382.
Train: 2018-08-09T11:47:42.622022: step 19249, loss 0.642643.
Train: 2018-08-09T11:47:42.738218: step 19250, loss 0.579104.
Test: 2018-08-09T11:47:43.223715: step 19250, loss 0.550113.
Train: 2018-08-09T11:47:43.333096: step 19251, loss 0.452367.
Train: 2018-08-09T11:47:43.426823: step 19252, loss 0.531424.
Train: 2018-08-09T11:47:43.536172: step 19253, loss 0.595503.
Train: 2018-08-09T11:47:43.629900: step 19254, loss 0.578976.
Train: 2018-08-09T11:47:43.739249: step 19255, loss 0.578725.
Train: 2018-08-09T11:47:43.832978: step 19256, loss 0.563015.
Train: 2018-08-09T11:47:43.936017: step 19257, loss 0.563242.
Train: 2018-08-09T11:47:44.045367: step 19258, loss 0.65704.
Train: 2018-08-09T11:47:44.139097: step 19259, loss 0.594292.
Train: 2018-08-09T11:47:44.248443: step 19260, loss 0.516824.
Test: 2018-08-09T11:47:44.748296: step 19260, loss 0.549428.
Train: 2018-08-09T11:47:44.842053: step 19261, loss 0.516871.
Train: 2018-08-09T11:47:44.935750: step 19262, loss 0.547932.
Train: 2018-08-09T11:47:45.045132: step 19263, loss 0.516835.
Train: 2018-08-09T11:47:45.138859: step 19264, loss 0.596674.
Train: 2018-08-09T11:47:45.248207: step 19265, loss 0.485566.
Train: 2018-08-09T11:47:45.341935: step 19266, loss 0.547359.
Train: 2018-08-09T11:47:45.451284: step 19267, loss 0.531745.
Train: 2018-08-09T11:47:45.544983: step 19268, loss 0.562858.
Train: 2018-08-09T11:47:45.654332: step 19269, loss 0.579163.
Train: 2018-08-09T11:47:45.748091: step 19270, loss 0.45162.
Test: 2018-08-09T11:47:46.247942: step 19270, loss 0.54843.
Train: 2018-08-09T11:47:46.372942: step 19271, loss 0.563368.
Train: 2018-08-09T11:47:46.482293: step 19272, loss 0.48104.
Train: 2018-08-09T11:47:46.576002: step 19273, loss 0.448295.
Train: 2018-08-09T11:47:46.685370: step 19274, loss 0.611571.
Train: 2018-08-09T11:47:46.779067: step 19275, loss 0.543901.
Train: 2018-08-09T11:47:46.888416: step 19276, loss 0.612192.
Train: 2018-08-09T11:47:46.982143: step 19277, loss 0.566881.
Train: 2018-08-09T11:47:47.091523: step 19278, loss 0.495099.
Train: 2018-08-09T11:47:47.185222: step 19279, loss 0.603581.
Train: 2018-08-09T11:47:47.294601: step 19280, loss 0.599479.
Test: 2018-08-09T11:47:47.778857: step 19280, loss 0.548107.
Train: 2018-08-09T11:47:47.888181: step 19281, loss 0.458656.
Train: 2018-08-09T11:47:47.981938: step 19282, loss 0.616015.
Train: 2018-08-09T11:47:48.091287: step 19283, loss 0.596416.
Train: 2018-08-09T11:47:48.184986: step 19284, loss 0.596395.
Train: 2018-08-09T11:47:48.278743: step 19285, loss 0.529426.
Train: 2018-08-09T11:47:48.388093: step 19286, loss 0.647594.
Train: 2018-08-09T11:47:48.481824: step 19287, loss 0.544734.
Train: 2018-08-09T11:47:48.591172: step 19288, loss 0.579177.
Train: 2018-08-09T11:47:48.684898: step 19289, loss 0.612497.
Train: 2018-08-09T11:47:48.778620: step 19290, loss 0.595164.
Test: 2018-08-09T11:47:49.279914: step 19290, loss 0.548562.
Train: 2018-08-09T11:47:49.389287: step 19291, loss 0.481405.
Train: 2018-08-09T11:47:49.483021: step 19292, loss 0.595526.
Train: 2018-08-09T11:47:49.592364: step 19293, loss 0.513914.
Train: 2018-08-09T11:47:49.686097: step 19294, loss 0.595128.
Train: 2018-08-09T11:47:49.779796: step 19295, loss 0.562737.
Train: 2018-08-09T11:47:49.892325: step 19296, loss 0.595038.
Train: 2018-08-09T11:47:49.989474: step 19297, loss 0.562737.
Train: 2018-08-09T11:47:50.083203: step 19298, loss 0.562789.
Train: 2018-08-09T11:47:50.176898: step 19299, loss 0.530613.
Train: 2018-08-09T11:47:50.286278: step 19300, loss 0.643224.
Test: 2018-08-09T11:47:50.786131: step 19300, loss 0.549578.
Train: 2018-08-09T11:47:51.358107: step 19301, loss 0.498572.
Train: 2018-08-09T11:47:51.451804: step 19302, loss 0.594929.
Train: 2018-08-09T11:47:51.561184: step 19303, loss 0.594932.
Train: 2018-08-09T11:47:51.654882: step 19304, loss 0.530771.
Train: 2018-08-09T11:47:51.764261: step 19305, loss 0.530789.
Train: 2018-08-09T11:47:51.857989: step 19306, loss 0.578863.
Train: 2018-08-09T11:47:51.951717: step 19307, loss 0.610922.
Train: 2018-08-09T11:47:52.045450: step 19308, loss 0.57886.
Train: 2018-08-09T11:47:52.154794: step 19309, loss 0.562849.
Train: 2018-08-09T11:47:52.248522: step 19310, loss 0.450829.
Test: 2018-08-09T11:47:52.748405: step 19310, loss 0.548423.
Train: 2018-08-09T11:47:52.842133: step 19311, loss 0.466672.
Train: 2018-08-09T11:47:52.951450: step 19312, loss 0.594887.
Train: 2018-08-09T11:47:53.045180: step 19313, loss 0.627295.
Train: 2018-08-09T11:47:53.138937: step 19314, loss 0.74065.
Train: 2018-08-09T11:47:53.248286: step 19315, loss 0.530601.
Train: 2018-08-09T11:47:53.341985: step 19316, loss 0.5145.
Train: 2018-08-09T11:47:53.435742: step 19317, loss 0.562731.
Train: 2018-08-09T11:47:53.540068: step 19318, loss 0.498491.
Train: 2018-08-09T11:47:53.633796: step 19319, loss 0.611115.
Train: 2018-08-09T11:47:53.743145: step 19320, loss 0.594984.
Test: 2018-08-09T11:47:54.227377: step 19320, loss 0.55017.
Train: 2018-08-09T11:47:54.336760: step 19321, loss 0.643061.
Train: 2018-08-09T11:47:54.430482: step 19322, loss 0.707058.
Train: 2018-08-09T11:47:54.524181: step 19323, loss 0.546771.
Train: 2018-08-09T11:47:54.633531: step 19324, loss 0.515202.
Train: 2018-08-09T11:47:54.727257: step 19325, loss 0.515186.
Train: 2018-08-09T11:47:54.821017: step 19326, loss 0.626395.
Train: 2018-08-09T11:47:54.914744: step 19327, loss 0.563074.
Train: 2018-08-09T11:47:55.024093: step 19328, loss 0.578907.
Train: 2018-08-09T11:47:55.117821: step 19329, loss 0.658422.
Train: 2018-08-09T11:47:55.211549: step 19330, loss 0.484022.
Test: 2018-08-09T11:47:55.711401: step 19330, loss 0.55069.
Train: 2018-08-09T11:47:55.805160: step 19331, loss 0.626433.
Train: 2018-08-09T11:47:55.898857: step 19332, loss 0.515973.
Train: 2018-08-09T11:47:56.010658: step 19333, loss 0.515981.
Train: 2018-08-09T11:47:56.104386: step 19334, loss 0.578876.
Train: 2018-08-09T11:47:56.198113: step 19335, loss 0.578872.
Train: 2018-08-09T11:47:56.291842: step 19336, loss 0.578879.
Train: 2018-08-09T11:47:56.385599: step 19337, loss 0.563197.
Train: 2018-08-09T11:47:56.494949: step 19338, loss 0.531764.
Train: 2018-08-09T11:47:56.588678: step 19339, loss 0.594572.
Train: 2018-08-09T11:47:56.682376: step 19340, loss 0.578877.
Test: 2018-08-09T11:47:57.182259: step 19340, loss 0.551404.
Train: 2018-08-09T11:47:57.275985: step 19341, loss 0.641737.
Train: 2018-08-09T11:47:57.369713: step 19342, loss 0.469007.
Train: 2018-08-09T11:47:57.479092: step 19343, loss 0.59459.
Train: 2018-08-09T11:47:57.572790: step 19344, loss 0.547446.
Train: 2018-08-09T11:47:57.666518: step 19345, loss 0.500285.
Train: 2018-08-09T11:47:57.760275: step 19346, loss 0.515893.
Train: 2018-08-09T11:47:57.869595: step 19347, loss 0.563077.
Train: 2018-08-09T11:47:57.964701: step 19348, loss 0.594702.
Train: 2018-08-09T11:47:58.058428: step 19349, loss 0.499566.
Train: 2018-08-09T11:47:58.152126: step 19350, loss 0.578796.
Test: 2018-08-09T11:47:58.652008: step 19350, loss 0.550966.
Train: 2018-08-09T11:47:58.745766: step 19351, loss 0.514962.
Train: 2018-08-09T11:47:58.839464: step 19352, loss 0.56265.
Train: 2018-08-09T11:47:58.933222: step 19353, loss 0.707696.
Train: 2018-08-09T11:47:59.026951: step 19354, loss 0.450215.
Train: 2018-08-09T11:47:59.120649: step 19355, loss 0.658744.
Train: 2018-08-09T11:47:59.214406: step 19356, loss 0.580007.
Train: 2018-08-09T11:47:59.308133: step 19357, loss 0.449573.
Train: 2018-08-09T11:47:59.401831: step 19358, loss 0.579715.
Train: 2018-08-09T11:47:59.495590: step 19359, loss 0.497863.
Train: 2018-08-09T11:47:59.604909: step 19360, loss 0.57924.
Test: 2018-08-09T11:48:00.091472: step 19360, loss 0.549798.
Train: 2018-08-09T11:48:00.200847: step 19361, loss 0.513528.
Train: 2018-08-09T11:48:00.294549: step 19362, loss 0.51387.
Train: 2018-08-09T11:48:00.403930: step 19363, loss 0.66191.
Train: 2018-08-09T11:48:00.497627: step 19364, loss 0.529449.
Train: 2018-08-09T11:48:00.591384: step 19365, loss 0.643981.
Train: 2018-08-09T11:48:00.700705: step 19366, loss 0.562736.
Train: 2018-08-09T11:48:00.794462: step 19367, loss 0.562782.
Train: 2018-08-09T11:48:00.888159: step 19368, loss 0.562656.
Train: 2018-08-09T11:48:00.997535: step 19369, loss 0.513647.
Train: 2018-08-09T11:48:01.091266: step 19370, loss 0.578997.
Test: 2018-08-09T11:48:01.591119: step 19370, loss 0.551048.
Train: 2018-08-09T11:48:01.684866: step 19371, loss 0.595293.
Train: 2018-08-09T11:48:01.778575: step 19372, loss 0.611531.
Train: 2018-08-09T11:48:01.887955: step 19373, loss 0.546429.
Train: 2018-08-09T11:48:01.982981: step 19374, loss 0.627681.
Train: 2018-08-09T11:48:02.076739: step 19375, loss 0.54647.
Train: 2018-08-09T11:48:02.170467: step 19376, loss 0.659911.
Train: 2018-08-09T11:48:02.264166: step 19377, loss 0.546564.
Train: 2018-08-09T11:48:02.357893: step 19378, loss 0.578881.
Train: 2018-08-09T11:48:02.451651: step 19379, loss 0.514471.
Train: 2018-08-09T11:48:02.545349: step 19380, loss 0.578875.
Test: 2018-08-09T11:48:03.045231: step 19380, loss 0.551391.
Train: 2018-08-09T11:48:03.138989: step 19381, loss 0.530663.
Train: 2018-08-09T11:48:03.232717: step 19382, loss 0.594933.
Train: 2018-08-09T11:48:03.326444: step 19383, loss 0.546769.
Train: 2018-08-09T11:48:03.420172: step 19384, loss 0.675128.
Train: 2018-08-09T11:48:03.513871: step 19385, loss 0.594874.
Train: 2018-08-09T11:48:03.607628: step 19386, loss 0.483024.
Train: 2018-08-09T11:48:03.701356: step 19387, loss 0.5629.
Train: 2018-08-09T11:48:03.795084: step 19388, loss 0.562909.
Train: 2018-08-09T11:48:03.888781: step 19389, loss 0.578859.
Train: 2018-08-09T11:48:03.967514: step 19390, loss 0.594792.
Test: 2018-08-09T11:48:04.482989: step 19390, loss 0.550432.
Train: 2018-08-09T11:48:04.561095: step 19391, loss 0.547018.
Train: 2018-08-09T11:48:04.654854: step 19392, loss 0.483395.
Train: 2018-08-09T11:48:04.748551: step 19393, loss 0.57886.
Train: 2018-08-09T11:48:04.842277: step 19394, loss 0.594793.
Train: 2018-08-09T11:48:04.936036: step 19395, loss 0.515082.
Train: 2018-08-09T11:48:05.029764: step 19396, loss 0.594801.
Train: 2018-08-09T11:48:05.123492: step 19397, loss 0.658815.
Train: 2018-08-09T11:48:05.217189: step 19398, loss 0.594791.
Train: 2018-08-09T11:48:05.310948: step 19399, loss 0.547003.
Train: 2018-08-09T11:48:05.404646: step 19400, loss 0.57887.
Test: 2018-08-09T11:48:05.904540: step 19400, loss 0.552284.
Train: 2018-08-09T11:48:06.466925: step 19401, loss 0.626534.
Train: 2018-08-09T11:48:06.560654: step 19402, loss 0.531275.
Train: 2018-08-09T11:48:06.638760: step 19403, loss 0.515485.
Train: 2018-08-09T11:48:06.732491: step 19404, loss 0.547149.
Train: 2018-08-09T11:48:06.826215: step 19405, loss 0.51544.
Train: 2018-08-09T11:48:06.922262: step 19406, loss 0.690015.
Train: 2018-08-09T11:48:07.015989: step 19407, loss 0.515393.
Train: 2018-08-09T11:48:07.109718: step 19408, loss 0.562932.
Train: 2018-08-09T11:48:07.203415: step 19409, loss 0.531274.
Train: 2018-08-09T11:48:07.297173: step 19410, loss 0.6109.
Test: 2018-08-09T11:48:07.797025: step 19410, loss 0.549289.
Train: 2018-08-09T11:48:07.890778: step 19411, loss 0.531243.
Train: 2018-08-09T11:48:07.984480: step 19412, loss 0.562863.
Train: 2018-08-09T11:48:08.078209: step 19413, loss 0.467685.
Train: 2018-08-09T11:48:08.171966: step 19414, loss 0.515151.
Train: 2018-08-09T11:48:08.250073: step 19415, loss 0.626912.
Train: 2018-08-09T11:48:08.343803: step 19416, loss 0.674891.
Train: 2018-08-09T11:48:08.437500: step 19417, loss 0.530899.
Train: 2018-08-09T11:48:08.531228: step 19418, loss 0.594813.
Train: 2018-08-09T11:48:08.624955: step 19419, loss 0.562895.
Train: 2018-08-09T11:48:08.718683: step 19420, loss 0.546921.
Test: 2018-08-09T11:48:09.204407: step 19420, loss 0.552737.
Train: 2018-08-09T11:48:09.298133: step 19421, loss 0.594853.
Train: 2018-08-09T11:48:09.391831: step 19422, loss 0.562899.
Train: 2018-08-09T11:48:09.485559: step 19423, loss 0.57886.
Train: 2018-08-09T11:48:09.579310: step 19424, loss 0.62677.
Train: 2018-08-09T11:48:09.657424: step 19425, loss 0.626708.
Train: 2018-08-09T11:48:09.751122: step 19426, loss 0.499276.
Train: 2018-08-09T11:48:09.844879: step 19427, loss 0.562954.
Train: 2018-08-09T11:48:09.938606: step 19428, loss 0.610651.
Train: 2018-08-09T11:48:10.032335: step 19429, loss 0.483599.
Train: 2018-08-09T11:48:10.110410: step 19430, loss 0.531214.
Test: 2018-08-09T11:48:10.610318: step 19430, loss 0.548069.
Train: 2018-08-09T11:48:10.704020: step 19431, loss 0.594756.
Train: 2018-08-09T11:48:10.797783: step 19432, loss 0.690163.
Train: 2018-08-09T11:48:10.891477: step 19433, loss 0.594733.
Train: 2018-08-09T11:48:10.971111: step 19434, loss 0.626392.
Train: 2018-08-09T11:48:11.064839: step 19435, loss 0.57886.
Train: 2018-08-09T11:48:11.158567: step 19436, loss 0.578864.
Train: 2018-08-09T11:48:11.252294: step 19437, loss 0.515972.
Train: 2018-08-09T11:48:11.330372: step 19438, loss 0.531757.
Train: 2018-08-09T11:48:11.424098: step 19439, loss 0.594571.
Train: 2018-08-09T11:48:11.517859: step 19440, loss 0.516139.
Test: 2018-08-09T11:48:12.017709: step 19440, loss 0.550862.
Train: 2018-08-09T11:48:12.095846: step 19441, loss 0.516128.
Train: 2018-08-09T11:48:12.189574: step 19442, loss 0.484655.
Train: 2018-08-09T11:48:12.283273: step 19443, loss 0.547395.
Train: 2018-08-09T11:48:12.377000: step 19444, loss 0.547296.
Train: 2018-08-09T11:48:12.455137: step 19445, loss 0.578862.
Train: 2018-08-09T11:48:12.548835: step 19446, loss 0.578854.
Train: 2018-08-09T11:48:12.642594: step 19447, loss 0.547093.
Train: 2018-08-09T11:48:12.736289: step 19448, loss 0.5311.
Train: 2018-08-09T11:48:12.814396: step 19449, loss 0.610764.
Train: 2018-08-09T11:48:12.910421: step 19450, loss 0.48298.
Test: 2018-08-09T11:48:13.411995: step 19450, loss 0.549025.
Train: 2018-08-09T11:48:13.490132: step 19451, loss 0.546756.
Train: 2018-08-09T11:48:13.583863: step 19452, loss 0.578869.
Train: 2018-08-09T11:48:13.677587: step 19453, loss 0.433804.
Train: 2018-08-09T11:48:13.771316: step 19454, loss 0.595394.
Train: 2018-08-09T11:48:13.849391: step 19455, loss 0.497255.
Train: 2018-08-09T11:48:13.943120: step 19456, loss 0.529582.
Train: 2018-08-09T11:48:14.036877: step 19457, loss 0.596489.
Train: 2018-08-09T11:48:14.130606: step 19458, loss 0.512538.
Train: 2018-08-09T11:48:14.224304: step 19459, loss 0.579217.
Train: 2018-08-09T11:48:14.302410: step 19460, loss 0.527917.
Test: 2018-08-09T11:48:14.802294: step 19460, loss 0.548509.
Train: 2018-08-09T11:48:14.896019: step 19461, loss 0.595738.
Train: 2018-08-09T11:48:14.989778: step 19462, loss 0.544174.
Train: 2018-08-09T11:48:15.067885: step 19463, loss 0.492982.
Train: 2018-08-09T11:48:15.161613: step 19464, loss 0.564634.
Train: 2018-08-09T11:48:15.255340: step 19465, loss 0.509316.
Train: 2018-08-09T11:48:15.333451: step 19466, loss 0.666769.
Train: 2018-08-09T11:48:15.426094: step 19467, loss 0.592592.
Train: 2018-08-09T11:48:15.519853: step 19468, loss 0.579052.
Train: 2018-08-09T11:48:15.613581: step 19469, loss 0.599823.
Train: 2018-08-09T11:48:15.691688: step 19470, loss 0.603758.
Test: 2018-08-09T11:48:16.191540: step 19470, loss 0.550041.
Train: 2018-08-09T11:48:16.285269: step 19471, loss 0.669585.
Train: 2018-08-09T11:48:16.378996: step 19472, loss 0.54318.
Train: 2018-08-09T11:48:16.457103: step 19473, loss 0.479602.
Train: 2018-08-09T11:48:16.550860: step 19474, loss 0.613446.
Train: 2018-08-09T11:48:16.644588: step 19475, loss 0.528955.
Train: 2018-08-09T11:48:16.722695: step 19476, loss 0.512944.
Train: 2018-08-09T11:48:16.816423: step 19477, loss 0.463584.
Train: 2018-08-09T11:48:16.912354: step 19478, loss 0.496556.
Train: 2018-08-09T11:48:16.991647: step 19479, loss 0.578999.
Train: 2018-08-09T11:48:17.085374: step 19480, loss 0.677839.
Test: 2018-08-09T11:48:17.585226: step 19480, loss 0.54958.
Train: 2018-08-09T11:48:17.663362: step 19481, loss 0.59546.
Train: 2018-08-09T11:48:17.757059: step 19482, loss 0.56255.
Train: 2018-08-09T11:48:17.850788: step 19483, loss 0.398286.
Train: 2018-08-09T11:48:17.928925: step 19484, loss 0.546085.
Train: 2018-08-09T11:48:18.022654: step 19485, loss 0.480149.
Train: 2018-08-09T11:48:18.116382: step 19486, loss 0.678112.
Train: 2018-08-09T11:48:18.194487: step 19487, loss 0.529443.
Train: 2018-08-09T11:48:18.288184: step 19488, loss 0.47978.
Train: 2018-08-09T11:48:18.381938: step 19489, loss 0.496169.
Train: 2018-08-09T11:48:18.460050: step 19490, loss 0.595693.
Test: 2018-08-09T11:48:18.962255: step 19490, loss 0.549351.
Train: 2018-08-09T11:48:19.055957: step 19491, loss 0.495824.
Train: 2018-08-09T11:48:19.149719: step 19492, loss 0.495626.
Train: 2018-08-09T11:48:19.227825: step 19493, loss 0.612662.
Train: 2018-08-09T11:48:19.321551: step 19494, loss 0.612802.
Train: 2018-08-09T11:48:19.415249: step 19495, loss 0.562366.
Train: 2018-08-09T11:48:19.493357: step 19496, loss 0.579272.
Train: 2018-08-09T11:48:19.587113: step 19497, loss 0.562399.
Train: 2018-08-09T11:48:19.680810: step 19498, loss 0.444501.
Train: 2018-08-09T11:48:19.774539: step 19499, loss 0.629764.
Train: 2018-08-09T11:48:19.852646: step 19500, loss 0.596149.
Test: 2018-08-09T11:48:20.352552: step 19500, loss 0.550367.
Train: 2018-08-09T11:48:20.868061: step 19501, loss 0.545462.
Train: 2018-08-09T11:48:20.964022: step 19502, loss 0.630113.
Train: 2018-08-09T11:48:21.057751: step 19503, loss 0.562298.
Train: 2018-08-09T11:48:21.135887: step 19504, loss 0.61293.
Train: 2018-08-09T11:48:21.229615: step 19505, loss 0.461474.
Train: 2018-08-09T11:48:21.318177: step 19506, loss 0.528796.
Train: 2018-08-09T11:48:21.396314: step 19507, loss 0.444685.
Train: 2018-08-09T11:48:21.490045: step 19508, loss 0.579186.
Train: 2018-08-09T11:48:21.583770: step 19509, loss 0.477849.
Train: 2018-08-09T11:48:21.677497: step 19510, loss 0.579488.
Test: 2018-08-09T11:48:22.161759: step 19510, loss 0.547085.
Train: 2018-08-09T11:48:22.255486: step 19511, loss 0.579291.
Train: 2018-08-09T11:48:22.349213: step 19512, loss 0.562438.
Train: 2018-08-09T11:48:22.427291: step 19513, loss 0.562389.
Train: 2018-08-09T11:48:22.521018: step 19514, loss 0.494053.
Train: 2018-08-09T11:48:22.614770: step 19515, loss 0.596218.
Train: 2018-08-09T11:48:22.692884: step 19516, loss 0.596506.
Train: 2018-08-09T11:48:22.786611: step 19517, loss 0.6479.
Train: 2018-08-09T11:48:22.880338: step 19518, loss 0.545696.
Train: 2018-08-09T11:48:22.965771: step 19519, loss 0.630345.
Train: 2018-08-09T11:48:23.059500: step 19520, loss 0.596372.
Test: 2018-08-09T11:48:23.559351: step 19520, loss 0.549101.
Train: 2018-08-09T11:48:23.637458: step 19521, loss 0.562163.
Train: 2018-08-09T11:48:23.731217: step 19522, loss 0.579188.
Train: 2018-08-09T11:48:23.809317: step 19523, loss 0.679896.
Train: 2018-08-09T11:48:23.903021: step 19524, loss 0.528985.
Train: 2018-08-09T11:48:23.996749: step 19525, loss 0.512472.
Train: 2018-08-09T11:48:24.074885: step 19526, loss 0.628914.
Train: 2018-08-09T11:48:24.168613: step 19527, loss 0.529335.
Train: 2018-08-09T11:48:24.246722: step 19528, loss 0.612085.
Train: 2018-08-09T11:48:24.340451: step 19529, loss 0.611959.
Train: 2018-08-09T11:48:24.434175: step 19530, loss 0.480442.
Test: 2018-08-09T11:48:24.934031: step 19530, loss 0.547829.
Train: 2018-08-09T11:48:25.012134: step 19531, loss 0.529792.
Train: 2018-08-09T11:48:25.105892: step 19532, loss 0.497088.
Train: 2018-08-09T11:48:25.199591: step 19533, loss 0.578944.
Train: 2018-08-09T11:48:25.277727: step 19534, loss 0.546166.
Train: 2018-08-09T11:48:25.371457: step 19535, loss 0.513446.
Train: 2018-08-09T11:48:25.465153: step 19536, loss 0.660969.
Train: 2018-08-09T11:48:25.543259: step 19537, loss 0.546186.
Train: 2018-08-09T11:48:25.637018: step 19538, loss 0.578879.
Train: 2018-08-09T11:48:25.715094: step 19539, loss 0.56239.
Train: 2018-08-09T11:48:25.808821: step 19540, loss 0.497005.
Test: 2018-08-09T11:48:26.310550: step 19540, loss 0.548391.
Train: 2018-08-09T11:48:26.388688: step 19541, loss 0.612549.
Train: 2018-08-09T11:48:26.482415: step 19542, loss 0.562509.
Train: 2018-08-09T11:48:26.576139: step 19543, loss 0.546147.
Train: 2018-08-09T11:48:26.654249: step 19544, loss 0.497125.
Train: 2018-08-09T11:48:26.747978: step 19545, loss 0.51305.
Train: 2018-08-09T11:48:26.841676: step 19546, loss 0.595336.
Train: 2018-08-09T11:48:26.919812: step 19547, loss 0.595915.
Train: 2018-08-09T11:48:27.013509: step 19548, loss 0.495779.
Train: 2018-08-09T11:48:27.091648: step 19549, loss 0.72716.
Train: 2018-08-09T11:48:27.185375: step 19550, loss 0.530051.
Test: 2018-08-09T11:48:27.685245: step 19550, loss 0.550761.
Train: 2018-08-09T11:48:27.778955: step 19551, loss 0.628423.
Train: 2018-08-09T11:48:27.857091: step 19552, loss 0.56286.
Train: 2018-08-09T11:48:27.952250: step 19553, loss 0.529196.
Train: 2018-08-09T11:48:28.030357: step 19554, loss 0.530562.
Train: 2018-08-09T11:48:28.124085: step 19555, loss 0.513679.
Train: 2018-08-09T11:48:28.217813: step 19556, loss 0.546484.
Train: 2018-08-09T11:48:28.295919: step 19557, loss 0.546015.
Train: 2018-08-09T11:48:28.389647: step 19558, loss 0.562533.
Train: 2018-08-09T11:48:28.483375: step 19559, loss 0.513177.
Train: 2018-08-09T11:48:28.561482: step 19560, loss 0.628141.
Test: 2018-08-09T11:48:29.061334: step 19560, loss 0.548397.
Train: 2018-08-09T11:48:29.155086: step 19561, loss 0.546509.
Train: 2018-08-09T11:48:29.248790: step 19562, loss 0.529651.
Train: 2018-08-09T11:48:29.326926: step 19563, loss 0.596017.
Train: 2018-08-09T11:48:29.420654: step 19564, loss 0.513297.
Train: 2018-08-09T11:48:29.498761: step 19565, loss 0.54465.
Train: 2018-08-09T11:48:29.592489: step 19566, loss 0.52976.
Train: 2018-08-09T11:48:29.686217: step 19567, loss 0.628791.
Train: 2018-08-09T11:48:29.764328: step 19568, loss 0.562431.
Train: 2018-08-09T11:48:29.858023: step 19569, loss 0.496929.
Train: 2018-08-09T11:48:29.952514: step 19570, loss 0.480049.
Test: 2018-08-09T11:48:30.452415: step 19570, loss 0.549528.
Train: 2018-08-09T11:48:30.530541: step 19571, loss 0.595524.
Train: 2018-08-09T11:48:30.624231: step 19572, loss 0.562354.
Train: 2018-08-09T11:48:30.717989: step 19573, loss 0.446667.
Train: 2018-08-09T11:48:30.796064: step 19574, loss 0.51291.
Train: 2018-08-09T11:48:30.889793: step 19575, loss 0.512406.
Train: 2018-08-09T11:48:30.967930: step 19576, loss 0.528492.
Train: 2018-08-09T11:48:31.061657: step 19577, loss 0.596034.
Train: 2018-08-09T11:48:31.155384: step 19578, loss 0.647447.
Train: 2018-08-09T11:48:31.249113: step 19579, loss 0.647209.
Train: 2018-08-09T11:48:31.327219: step 19580, loss 0.562546.
Test: 2018-08-09T11:48:31.827083: step 19580, loss 0.548459.
Train: 2018-08-09T11:48:31.920799: step 19581, loss 0.562794.
Train: 2018-08-09T11:48:31.998907: step 19582, loss 0.579277.
Train: 2018-08-09T11:48:32.092634: step 19583, loss 0.612634.
Train: 2018-08-09T11:48:32.186392: step 19584, loss 0.545677.
Train: 2018-08-09T11:48:32.264499: step 19585, loss 0.49563.
Train: 2018-08-09T11:48:32.358226: step 19586, loss 0.529072.
Train: 2018-08-09T11:48:32.451954: step 19587, loss 0.545766.
Train: 2018-08-09T11:48:32.530064: step 19588, loss 0.562381.
Train: 2018-08-09T11:48:32.623789: step 19589, loss 0.529039.
Train: 2018-08-09T11:48:32.717517: step 19590, loss 0.51225.
Test: 2018-08-09T11:48:33.218952: step 19590, loss 0.54989.
Train: 2018-08-09T11:48:33.312706: step 19591, loss 0.71293.
Train: 2018-08-09T11:48:33.390813: step 19592, loss 0.529022.
Train: 2018-08-09T11:48:33.484541: step 19593, loss 0.545697.
Train: 2018-08-09T11:48:33.578268: step 19594, loss 0.49576.
Train: 2018-08-09T11:48:33.656378: step 19595, loss 0.54547.
Train: 2018-08-09T11:48:33.750103: step 19596, loss 0.478815.
Train: 2018-08-09T11:48:33.843831: step 19597, loss 0.478852.
Train: 2018-08-09T11:48:33.937559: step 19598, loss 0.61292.
Train: 2018-08-09T11:48:34.015666: step 19599, loss 0.46159.
Train: 2018-08-09T11:48:34.109362: step 19600, loss 0.579011.
Test: 2018-08-09T11:48:34.609246: step 19600, loss 0.547761.
Train: 2018-08-09T11:48:35.126250: step 19601, loss 0.562283.
Train: 2018-08-09T11:48:35.219973: step 19602, loss 0.443175.
Train: 2018-08-09T11:48:35.298055: step 19603, loss 0.562393.
Train: 2018-08-09T11:48:35.391783: step 19604, loss 0.510742.
Train: 2018-08-09T11:48:35.485512: step 19605, loss 0.492716.
Train: 2018-08-09T11:48:35.579270: step 19606, loss 0.56247.
Train: 2018-08-09T11:48:35.657378: step 19607, loss 0.631795.
Train: 2018-08-09T11:48:35.751073: step 19608, loss 0.667907.
Train: 2018-08-09T11:48:35.844802: step 19609, loss 0.633552.
Train: 2018-08-09T11:48:35.922907: step 19610, loss 0.614373.
Test: 2018-08-09T11:48:36.422791: step 19610, loss 0.548727.
Train: 2018-08-09T11:48:36.516517: step 19611, loss 0.543844.
Train: 2018-08-09T11:48:36.610246: step 19612, loss 0.545076.
Train: 2018-08-09T11:48:36.688382: step 19613, loss 0.700893.
Train: 2018-08-09T11:48:36.782111: step 19614, loss 0.562615.
Train: 2018-08-09T11:48:36.875839: step 19615, loss 0.646971.
Train: 2018-08-09T11:48:36.955229: step 19616, loss 0.545461.
Train: 2018-08-09T11:48:37.048957: step 19617, loss 0.663066.
Train: 2018-08-09T11:48:37.142714: step 19618, loss 0.57911.
Train: 2018-08-09T11:48:37.236436: step 19619, loss 0.562456.
Train: 2018-08-09T11:48:37.314551: step 19620, loss 0.545959.
Test: 2018-08-09T11:48:37.814401: step 19620, loss 0.54834.
Train: 2018-08-09T11:48:37.908128: step 19621, loss 0.562524.
Train: 2018-08-09T11:48:38.001889: step 19622, loss 0.611784.
Train: 2018-08-09T11:48:38.079994: step 19623, loss 0.611631.
Train: 2018-08-09T11:48:38.173721: step 19624, loss 0.660289.
Train: 2018-08-09T11:48:38.267450: step 19625, loss 0.611268.
Train: 2018-08-09T11:48:38.361177: step 19626, loss 0.514488.
Train: 2018-08-09T11:48:38.439278: step 19627, loss 0.626952.
Train: 2018-08-09T11:48:38.533013: step 19628, loss 0.451245.
Train: 2018-08-09T11:48:38.626734: step 19629, loss 0.578859.
Train: 2018-08-09T11:48:38.720467: step 19630, loss 0.674102.
Test: 2018-08-09T11:48:39.206075: step 19630, loss 0.550624.
Train: 2018-08-09T11:48:39.299833: step 19631, loss 0.547235.
Train: 2018-08-09T11:48:39.393560: step 19632, loss 0.531574.
Train: 2018-08-09T11:48:39.487258: step 19633, loss 0.610327.
Train: 2018-08-09T11:48:39.565395: step 19634, loss 0.516123.
Train: 2018-08-09T11:48:39.659123: step 19635, loss 0.563214.
Train: 2018-08-09T11:48:39.752835: step 19636, loss 0.57888.
Train: 2018-08-09T11:48:39.846588: step 19637, loss 0.563263.
Train: 2018-08-09T11:48:39.924681: step 19638, loss 0.532048.
Train: 2018-08-09T11:48:40.018414: step 19639, loss 0.641342.
Train: 2018-08-09T11:48:40.112110: step 19640, loss 0.594487.
Test: 2018-08-09T11:48:40.611993: step 19640, loss 0.554043.
Train: 2018-08-09T11:48:40.705751: step 19641, loss 0.501062.
Train: 2018-08-09T11:48:40.783858: step 19642, loss 0.501074.
Train: 2018-08-09T11:48:40.877585: step 19643, loss 0.563316.
Train: 2018-08-09T11:48:40.972669: step 19644, loss 0.578883.
Train: 2018-08-09T11:48:41.066397: step 19645, loss 0.563273.
Train: 2018-08-09T11:48:41.160126: step 19646, loss 0.500718.
Train: 2018-08-09T11:48:41.238263: step 19647, loss 0.57889.
Train: 2018-08-09T11:48:41.331989: step 19648, loss 0.516115.
Train: 2018-08-09T11:48:41.425717: step 19649, loss 0.547382.
Train: 2018-08-09T11:48:41.519415: step 19650, loss 0.563136.
Test: 2018-08-09T11:48:42.019301: step 19650, loss 0.549992.
Train: 2018-08-09T11:48:42.097403: step 19651, loss 0.579027.
Train: 2018-08-09T11:48:42.191161: step 19652, loss 0.594733.
Train: 2018-08-09T11:48:42.284893: step 19653, loss 0.547072.
Train: 2018-08-09T11:48:42.378586: step 19654, loss 0.57888.
Train: 2018-08-09T11:48:42.472314: step 19655, loss 0.467491.
Train: 2018-08-09T11:48:42.566042: step 19656, loss 0.53101.
Train: 2018-08-09T11:48:42.644150: step 19657, loss 0.546886.
Train: 2018-08-09T11:48:42.737908: step 19658, loss 0.594912.
Train: 2018-08-09T11:48:42.831606: step 19659, loss 0.594945.
Train: 2018-08-09T11:48:42.926807: step 19660, loss 0.482165.
Test: 2018-08-09T11:48:43.426659: step 19660, loss 0.552422.
Train: 2018-08-09T11:48:43.504795: step 19661, loss 0.578653.
Train: 2018-08-09T11:48:43.598526: step 19662, loss 0.595139.
Train: 2018-08-09T11:48:43.692252: step 19663, loss 0.514152.
Train: 2018-08-09T11:48:43.785978: step 19664, loss 0.546556.
Train: 2018-08-09T11:48:43.879707: step 19665, loss 0.513468.
Train: 2018-08-09T11:48:43.973434: step 19666, loss 0.545905.
Train: 2018-08-09T11:48:44.067133: step 19667, loss 0.578849.
Train: 2018-08-09T11:48:44.160861: step 19668, loss 0.612449.
Train: 2018-08-09T11:48:44.238996: step 19669, loss 0.562593.
Train: 2018-08-09T11:48:44.332725: step 19670, loss 0.646106.
Test: 2018-08-09T11:48:44.832577: step 19670, loss 0.548179.
Train: 2018-08-09T11:48:44.927701: step 19671, loss 0.612368.
Train: 2018-08-09T11:48:45.021400: step 19672, loss 0.529499.
Train: 2018-08-09T11:48:45.115157: step 19673, loss 0.480218.
Train: 2018-08-09T11:48:45.208885: step 19674, loss 0.677383.
Train: 2018-08-09T11:48:45.302608: step 19675, loss 0.579131.
Train: 2018-08-09T11:48:45.380719: step 19676, loss 0.57868.
Train: 2018-08-09T11:48:45.474447: step 19677, loss 0.529658.
Train: 2018-08-09T11:48:45.568145: step 19678, loss 0.579124.
Train: 2018-08-09T11:48:45.661904: step 19679, loss 0.530111.
Train: 2018-08-09T11:48:45.755631: step 19680, loss 0.546129.
Test: 2018-08-09T11:48:46.251581: step 19680, loss 0.548476.
Train: 2018-08-09T11:48:46.345341: step 19681, loss 0.480682.
Train: 2018-08-09T11:48:46.439060: step 19682, loss 0.562695.
Train: 2018-08-09T11:48:46.532794: step 19683, loss 0.710744.
Train: 2018-08-09T11:48:46.626521: step 19684, loss 0.480283.
Train: 2018-08-09T11:48:46.720249: step 19685, loss 0.579931.
Train: 2018-08-09T11:48:46.813948: step 19686, loss 0.561762.
Train: 2018-08-09T11:48:46.912955: step 19687, loss 0.595703.
Train: 2018-08-09T11:48:47.006713: step 19688, loss 0.611319.
Train: 2018-08-09T11:48:47.100412: step 19689, loss 0.578741.
Train: 2018-08-09T11:48:47.194140: step 19690, loss 0.51388.
Test: 2018-08-09T11:48:47.694021: step 19690, loss 0.549255.
Train: 2018-08-09T11:48:47.772157: step 19691, loss 0.627394.
Train: 2018-08-09T11:48:47.865887: step 19692, loss 0.611281.
Train: 2018-08-09T11:48:47.959584: step 19693, loss 0.530328.
Train: 2018-08-09T11:48:48.053312: step 19694, loss 0.449771.
Train: 2018-08-09T11:48:48.156733: step 19695, loss 0.594715.
Train: 2018-08-09T11:48:48.250460: step 19696, loss 0.579589.
Train: 2018-08-09T11:48:48.344159: step 19697, loss 0.643854.
Train: 2018-08-09T11:48:48.437916: step 19698, loss 0.562638.
Train: 2018-08-09T11:48:48.531644: step 19699, loss 0.643687.
Train: 2018-08-09T11:48:48.625343: step 19700, loss 0.546723.
Test: 2018-08-09T11:48:49.125227: step 19700, loss 0.549043.
Train: 2018-08-09T11:48:49.687623: step 19701, loss 0.434718.
Train: 2018-08-09T11:48:49.781350: step 19702, loss 0.594861.
Train: 2018-08-09T11:48:49.875078: step 19703, loss 0.498609.
Train: 2018-08-09T11:48:49.977092: step 19704, loss 0.659397.
Train: 2018-08-09T11:48:50.070790: step 19705, loss 0.579025.
Train: 2018-08-09T11:48:50.164544: step 19706, loss 0.546847.
Train: 2018-08-09T11:48:50.258276: step 19707, loss 0.578786.
Train: 2018-08-09T11:48:50.352004: step 19708, loss 0.562882.
Train: 2018-08-09T11:48:50.445731: step 19709, loss 0.674961.
Train: 2018-08-09T11:48:50.539461: step 19710, loss 0.467156.
Test: 2018-08-09T11:48:51.039313: step 19710, loss 0.55158.
Train: 2018-08-09T11:48:51.133039: step 19711, loss 0.562922.
Train: 2018-08-09T11:48:51.242389: step 19712, loss 0.499115.
Train: 2018-08-09T11:48:51.336117: step 19713, loss 0.690614.
Train: 2018-08-09T11:48:51.429845: step 19714, loss 0.546992.
Train: 2018-08-09T11:48:51.523603: step 19715, loss 0.626676.
Train: 2018-08-09T11:48:51.617301: step 19716, loss 0.578882.
Train: 2018-08-09T11:48:51.711060: step 19717, loss 0.594754.
Train: 2018-08-09T11:48:51.804788: step 19718, loss 0.499573.
Train: 2018-08-09T11:48:51.898515: step 19719, loss 0.57886.
Train: 2018-08-09T11:48:52.007864: step 19720, loss 0.56302.
Test: 2018-08-09T11:48:52.492096: step 19720, loss 0.549993.
Train: 2018-08-09T11:48:52.601474: step 19721, loss 0.658005.
Train: 2018-08-09T11:48:52.691162: step 19722, loss 0.531454.
Train: 2018-08-09T11:48:52.784860: step 19723, loss 0.563065.
Train: 2018-08-09T11:48:52.894207: step 19724, loss 0.578862.
Train: 2018-08-09T11:48:52.994368: step 19725, loss 0.547313.
Train: 2018-08-09T11:48:53.088126: step 19726, loss 0.484303.
Train: 2018-08-09T11:48:53.181854: step 19727, loss 0.578918.
Train: 2018-08-09T11:48:53.275582: step 19728, loss 0.48394.
Train: 2018-08-09T11:48:53.369304: step 19729, loss 0.562817.
Train: 2018-08-09T11:48:53.463007: step 19730, loss 0.514922.
Test: 2018-08-09T11:48:53.962890: step 19730, loss 0.548369.
Train: 2018-08-09T11:48:54.056648: step 19731, loss 0.547104.
Train: 2018-08-09T11:48:54.165977: step 19732, loss 0.54696.
Train: 2018-08-09T11:48:54.259722: step 19733, loss 0.692491.
Train: 2018-08-09T11:48:54.353423: step 19734, loss 0.497607.
Train: 2018-08-09T11:48:54.447180: step 19735, loss 0.595695.
Train: 2018-08-09T11:48:54.540878: step 19736, loss 0.579502.
Train: 2018-08-09T11:48:54.650228: step 19737, loss 0.595505.
Train: 2018-08-09T11:48:54.743987: step 19738, loss 0.530484.
Train: 2018-08-09T11:48:54.837714: step 19739, loss 0.546532.
Train: 2018-08-09T11:48:54.931441: step 19740, loss 0.449323.
Test: 2018-08-09T11:48:55.431293: step 19740, loss 0.549951.
Train: 2018-08-09T11:48:55.525022: step 19741, loss 0.643516.
Train: 2018-08-09T11:48:55.634370: step 19742, loss 0.514414.
Train: 2018-08-09T11:48:55.728098: step 19743, loss 0.563149.
Train: 2018-08-09T11:48:55.821856: step 19744, loss 0.497492.
Train: 2018-08-09T11:48:55.916959: step 19745, loss 0.546448.
Train: 2018-08-09T11:48:56.010718: step 19746, loss 0.464838.
Train: 2018-08-09T11:48:56.120067: step 19747, loss 0.496482.
Train: 2018-08-09T11:48:56.213763: step 19748, loss 0.59498.
Train: 2018-08-09T11:48:56.307492: step 19749, loss 0.594625.
Train: 2018-08-09T11:48:56.416840: step 19750, loss 0.511915.
Test: 2018-08-09T11:48:56.916772: step 19750, loss 0.548807.
Train: 2018-08-09T11:48:57.010481: step 19751, loss 0.613752.
Train: 2018-08-09T11:48:57.104209: step 19752, loss 0.577392.
Train: 2018-08-09T11:48:57.197938: step 19753, loss 0.635054.
Train: 2018-08-09T11:48:57.307286: step 19754, loss 0.510746.
Train: 2018-08-09T11:48:57.401014: step 19755, loss 0.581397.
Train: 2018-08-09T11:48:57.494714: step 19756, loss 0.580218.
Train: 2018-08-09T11:48:57.604062: step 19757, loss 0.580178.
Train: 2018-08-09T11:48:57.697789: step 19758, loss 0.479042.
Train: 2018-08-09T11:48:57.791548: step 19759, loss 0.611866.
Train: 2018-08-09T11:48:57.900897: step 19760, loss 0.51222.
Test: 2018-08-09T11:48:58.387458: step 19760, loss 0.546216.
Train: 2018-08-09T11:48:58.481212: step 19761, loss 0.629266.
Train: 2018-08-09T11:48:58.590565: step 19762, loss 0.562823.
Train: 2018-08-09T11:48:58.684289: step 19763, loss 0.529372.
Train: 2018-08-09T11:48:58.778019: step 19764, loss 0.529319.
Train: 2018-08-09T11:48:58.887366: step 19765, loss 0.496135.
Train: 2018-08-09T11:48:58.981064: step 19766, loss 0.579142.
Train: 2018-08-09T11:48:59.088914: step 19767, loss 0.430329.
Train: 2018-08-09T11:48:59.186829: step 19768, loss 0.578912.
Train: 2018-08-09T11:48:59.280555: step 19769, loss 0.563158.
Train: 2018-08-09T11:48:59.389874: step 19770, loss 0.614127.
Test: 2018-08-09T11:48:59.874135: step 19770, loss 0.547418.
Train: 2018-08-09T11:48:59.982119: step 19771, loss 0.49555.
Train: 2018-08-09T11:49:00.075878: step 19772, loss 0.462212.
Train: 2018-08-09T11:49:00.185227: step 19773, loss 0.646274.
Train: 2018-08-09T11:49:00.278955: step 19774, loss 0.612741.
Train: 2018-08-09T11:49:00.372684: step 19775, loss 0.596043.
Train: 2018-08-09T11:49:00.482034: step 19776, loss 0.612659.
Train: 2018-08-09T11:49:00.575760: step 19777, loss 0.495753.
Train: 2018-08-09T11:49:00.669488: step 19778, loss 0.66232.
Train: 2018-08-09T11:49:00.778837: step 19779, loss 0.595679.
Train: 2018-08-09T11:49:00.872567: step 19780, loss 0.496152.
Test: 2018-08-09T11:49:01.372418: step 19780, loss 0.548208.
Train: 2018-08-09T11:49:01.466175: step 19781, loss 0.529345.
Train: 2018-08-09T11:49:01.575495: step 19782, loss 0.463135.
Train: 2018-08-09T11:49:01.669222: step 19783, loss 0.645348.
Train: 2018-08-09T11:49:01.762981: step 19784, loss 0.545897.
Train: 2018-08-09T11:49:01.872300: step 19785, loss 0.612176.
Train: 2018-08-09T11:49:01.966057: step 19786, loss 0.579084.
Train: 2018-08-09T11:49:02.075377: step 19787, loss 0.529404.
Train: 2018-08-09T11:49:02.169134: step 19788, loss 0.479852.
Train: 2018-08-09T11:49:02.262862: step 19789, loss 0.661725.
Train: 2018-08-09T11:49:02.372182: step 19790, loss 0.512921.
Test: 2018-08-09T11:49:02.872064: step 19790, loss 0.548888.
Train: 2018-08-09T11:49:02.967225: step 19791, loss 0.463358.
Train: 2018-08-09T11:49:03.060982: step 19792, loss 0.645209.
Train: 2018-08-09T11:49:03.170301: step 19793, loss 0.479767.
Train: 2018-08-09T11:49:03.264059: step 19794, loss 0.628723.
Train: 2018-08-09T11:49:03.373378: step 19795, loss 0.545915.
Train: 2018-08-09T11:49:03.467136: step 19796, loss 0.612167.
Train: 2018-08-09T11:49:03.576485: step 19797, loss 0.512819.
Train: 2018-08-09T11:49:03.670182: step 19798, loss 0.59559.
Train: 2018-08-09T11:49:03.779563: step 19799, loss 0.49629.
Train: 2018-08-09T11:49:03.873294: step 19800, loss 0.47969.
Test: 2018-08-09T11:49:04.373154: step 19800, loss 0.550688.
Train: 2018-08-09T11:49:04.936194: step 19801, loss 0.562465.
Train: 2018-08-09T11:49:05.045542: step 19802, loss 0.628885.
Train: 2018-08-09T11:49:05.139272: step 19803, loss 0.645524.
Train: 2018-08-09T11:49:05.248619: step 19804, loss 0.628841.
Train: 2018-08-09T11:49:05.342347: step 19805, loss 0.545908.
Train: 2018-08-09T11:49:05.451696: step 19806, loss 0.595577.
Train: 2018-08-09T11:49:05.545395: step 19807, loss 0.546008.
Train: 2018-08-09T11:49:05.654773: step 19808, loss 0.562519.
Train: 2018-08-09T11:49:05.748495: step 19809, loss 0.628353.
Train: 2018-08-09T11:49:05.857845: step 19810, loss 0.578968.
Test: 2018-08-09T11:49:06.357734: step 19810, loss 0.548469.
Train: 2018-08-09T11:49:06.451431: step 19811, loss 0.546199.
Train: 2018-08-09T11:49:06.560805: step 19812, loss 0.709714.
Train: 2018-08-09T11:49:06.654507: step 19813, loss 0.546356.
Train: 2018-08-09T11:49:06.763857: step 19814, loss 0.595132.
Train: 2018-08-09T11:49:06.857615: step 19815, loss 0.6921.
Train: 2018-08-09T11:49:06.966964: step 19816, loss 0.627151.
Train: 2018-08-09T11:49:07.060662: step 19817, loss 0.546859.
Train: 2018-08-09T11:49:07.170011: step 19818, loss 0.578858.
Train: 2018-08-09T11:49:07.263738: step 19819, loss 0.673999.
Train: 2018-08-09T11:49:07.373089: step 19820, loss 0.452727.
Test: 2018-08-09T11:49:07.872971: step 19820, loss 0.550211.
Train: 2018-08-09T11:49:07.985066: step 19821, loss 0.547439.
Train: 2018-08-09T11:49:08.078823: step 19822, loss 0.594554.
Train: 2018-08-09T11:49:08.188172: step 19823, loss 0.625791.
Train: 2018-08-09T11:49:08.281900: step 19824, loss 0.625645.
Train: 2018-08-09T11:49:08.391220: step 19825, loss 0.594436.
Train: 2018-08-09T11:49:08.484977: step 19826, loss 0.517071.
Train: 2018-08-09T11:49:08.594330: step 19827, loss 0.50181.
Train: 2018-08-09T11:49:08.688025: step 19828, loss 0.486485.
Train: 2018-08-09T11:49:08.797374: step 19829, loss 0.594367.
Train: 2018-08-09T11:49:08.906753: step 19830, loss 0.532673.
Test: 2018-08-09T11:49:09.406610: step 19830, loss 0.547871.
Train: 2018-08-09T11:49:09.500363: step 19831, loss 0.548067.
Train: 2018-08-09T11:49:09.594094: step 19832, loss 0.548018.
Train: 2018-08-09T11:49:09.703411: step 19833, loss 0.563441.
Train: 2018-08-09T11:49:09.797168: step 19834, loss 0.57892.
Train: 2018-08-09T11:49:09.906488: step 19835, loss 0.594429.
Train: 2018-08-09T11:49:10.015867: step 19836, loss 0.53227.
Train: 2018-08-09T11:49:10.109595: step 19837, loss 0.532179.
Train: 2018-08-09T11:49:10.218948: step 19838, loss 0.563281.
Train: 2018-08-09T11:49:10.312671: step 19839, loss 0.485091.
Train: 2018-08-09T11:49:10.422021: step 19840, loss 0.531691.
Test: 2018-08-09T11:49:10.921873: step 19840, loss 0.548274.
Train: 2018-08-09T11:49:11.015631: step 19841, loss 0.578693.
Train: 2018-08-09T11:49:11.124980: step 19842, loss 0.562882.
Train: 2018-08-09T11:49:11.218712: step 19843, loss 0.515189.
Train: 2018-08-09T11:49:11.328057: step 19844, loss 0.514855.
Train: 2018-08-09T11:49:11.421755: step 19845, loss 0.529618.
Train: 2018-08-09T11:49:11.531134: step 19846, loss 0.580218.
Train: 2018-08-09T11:49:11.640454: step 19847, loss 0.47833.
Train: 2018-08-09T11:49:11.734182: step 19848, loss 0.575271.
Train: 2018-08-09T11:49:11.843576: step 19849, loss 0.578314.
Train: 2018-08-09T11:49:11.938697: step 19850, loss 0.542717.
Test: 2018-08-09T11:49:12.438551: step 19850, loss 0.544975.
Train: 2018-08-09T11:49:12.547899: step 19851, loss 0.488606.
Train: 2018-08-09T11:49:12.641627: step 19852, loss 0.540741.
Train: 2018-08-09T11:49:12.751005: step 19853, loss 0.708292.
Train: 2018-08-09T11:49:12.860357: step 19854, loss 0.631983.
Train: 2018-08-09T11:49:12.954084: step 19855, loss 0.596946.
Train: 2018-08-09T11:49:13.063432: step 19856, loss 0.435957.
Train: 2018-08-09T11:49:13.157159: step 19857, loss 0.490998.
Train: 2018-08-09T11:49:13.266509: step 19858, loss 0.580271.
Train: 2018-08-09T11:49:13.375858: step 19859, loss 0.577662.
Train: 2018-08-09T11:49:13.469555: step 19860, loss 0.597005.
Test: 2018-08-09T11:49:13.970826: step 19860, loss 0.548099.
Train: 2018-08-09T11:49:14.080178: step 19861, loss 0.529242.
Train: 2018-08-09T11:49:14.173906: step 19862, loss 0.460119.
Train: 2018-08-09T11:49:14.283225: step 19863, loss 0.629987.
Train: 2018-08-09T11:49:14.392604: step 19864, loss 0.546508.
Train: 2018-08-09T11:49:14.486301: step 19865, loss 0.441729.
Train: 2018-08-09T11:49:14.580060: step 19866, loss 0.542641.
Train: 2018-08-09T11:49:14.689409: step 19867, loss 0.566553.
Train: 2018-08-09T11:49:14.798758: step 19868, loss 0.525078.
Train: 2018-08-09T11:49:14.908076: step 19869, loss 0.611439.
Train: 2018-08-09T11:49:15.001835: step 19870, loss 0.531035.
Test: 2018-08-09T11:49:15.501713: step 19870, loss 0.549348.
Train: 2018-08-09T11:49:15.611036: step 19871, loss 0.582504.
Train: 2018-08-09T11:49:15.720416: step 19872, loss 0.599764.
Train: 2018-08-09T11:49:15.814144: step 19873, loss 0.581212.
Train: 2018-08-09T11:49:15.924927: step 19874, loss 0.562528.
Train: 2018-08-09T11:49:16.018657: step 19875, loss 0.461211.
Train: 2018-08-09T11:49:16.128004: step 19876, loss 0.612795.
Train: 2018-08-09T11:49:16.237353: step 19877, loss 0.609182.
Train: 2018-08-09T11:49:16.331052: step 19878, loss 0.56297.
Train: 2018-08-09T11:49:16.440431: step 19879, loss 0.51608.
Train: 2018-08-09T11:49:16.534158: step 19880, loss 0.594745.
Test: 2018-08-09T11:49:17.034011: step 19880, loss 0.55022.
Train: 2018-08-09T11:49:17.143359: step 19881, loss 0.644009.
Train: 2018-08-09T11:49:17.252708: step 19882, loss 0.515126.
Train: 2018-08-09T11:49:17.346467: step 19883, loss 0.563389.
Train: 2018-08-09T11:49:17.455816: step 19884, loss 0.497116.
Train: 2018-08-09T11:49:17.565167: step 19885, loss 0.593557.
Train: 2018-08-09T11:49:17.658897: step 19886, loss 0.611498.
Train: 2018-08-09T11:49:17.768242: step 19887, loss 0.547596.
Train: 2018-08-09T11:49:17.877592: step 19888, loss 0.595052.
Train: 2018-08-09T11:49:17.972583: step 19889, loss 0.578334.
Train: 2018-08-09T11:49:18.081932: step 19890, loss 0.626635.
Test: 2018-08-09T11:49:18.572825: step 19890, loss 0.548617.
Train: 2018-08-09T11:49:18.682174: step 19891, loss 0.56331.
Train: 2018-08-09T11:49:18.775934: step 19892, loss 0.562373.
Train: 2018-08-09T11:49:18.885252: step 19893, loss 0.62605.
Train: 2018-08-09T11:49:18.994631: step 19894, loss 0.610052.
Train: 2018-08-09T11:49:19.103981: step 19895, loss 0.625829.
Train: 2018-08-09T11:49:19.197678: step 19896, loss 0.57887.
Train: 2018-08-09T11:49:19.307058: step 19897, loss 0.563394.
Train: 2018-08-09T11:49:19.400786: step 19898, loss 0.454514.
Train: 2018-08-09T11:49:19.510135: step 19899, loss 0.656954.
Train: 2018-08-09T11:49:19.603862: step 19900, loss 0.532484.
Test: 2018-08-09T11:49:20.103740: step 19900, loss 0.551836.
Train: 2018-08-09T11:49:20.697350: step 19901, loss 0.470301.
Train: 2018-08-09T11:49:20.791083: step 19902, loss 0.485931.
Train: 2018-08-09T11:49:20.899433: step 19903, loss 0.641084.
Train: 2018-08-09T11:49:20.993162: step 19904, loss 0.625633.
Train: 2018-08-09T11:49:21.102511: step 19905, loss 0.562917.
Train: 2018-08-09T11:49:21.211830: step 19906, loss 0.516634.
Train: 2018-08-09T11:49:21.305558: step 19907, loss 0.594879.
Train: 2018-08-09T11:49:21.414907: step 19908, loss 0.516311.
Train: 2018-08-09T11:49:21.508665: step 19909, loss 0.532263.
Train: 2018-08-09T11:49:21.618014: step 19910, loss 0.531904.
Test: 2018-08-09T11:49:22.120234: step 19910, loss 0.550146.
Train: 2018-08-09T11:49:22.213956: step 19911, loss 0.468039.
Train: 2018-08-09T11:49:22.323335: step 19912, loss 0.547506.
Train: 2018-08-09T11:49:22.432683: step 19913, loss 0.580042.
Train: 2018-08-09T11:49:22.526412: step 19914, loss 0.659225.
Train: 2018-08-09T11:49:22.635731: step 19915, loss 0.450678.
Train: 2018-08-09T11:49:22.729459: step 19916, loss 0.610078.
Train: 2018-08-09T11:49:22.838840: step 19917, loss 0.481308.
Train: 2018-08-09T11:49:22.948156: step 19918, loss 0.627674.
Train: 2018-08-09T11:49:23.041915: step 19919, loss 0.545279.
Train: 2018-08-09T11:49:23.151268: step 19920, loss 0.513476.
Test: 2018-08-09T11:49:23.635495: step 19920, loss 0.548289.
Train: 2018-08-09T11:49:23.744874: step 19921, loss 0.611889.
Train: 2018-08-09T11:49:23.854223: step 19922, loss 0.529231.
Train: 2018-08-09T11:49:23.950255: step 19923, loss 0.478985.
Train: 2018-08-09T11:49:24.064797: step 19924, loss 0.613857.
Train: 2018-08-09T11:49:24.155943: step 19925, loss 0.5654.
Train: 2018-08-09T11:49:24.265321: step 19926, loss 0.51277.
Train: 2018-08-09T11:49:24.359052: step 19927, loss 0.526079.
Train: 2018-08-09T11:49:24.468368: step 19928, loss 0.646631.
Train: 2018-08-09T11:49:24.577748: step 19929, loss 0.665691.
Train: 2018-08-09T11:49:24.671476: step 19930, loss 0.545343.
Test: 2018-08-09T11:49:25.171329: step 19930, loss 0.547887.
Train: 2018-08-09T11:49:25.280678: step 19931, loss 0.579425.
Train: 2018-08-09T11:49:25.374435: step 19932, loss 0.545602.
Train: 2018-08-09T11:49:25.483754: step 19933, loss 0.560099.
Train: 2018-08-09T11:49:25.577512: step 19934, loss 0.514787.
Train: 2018-08-09T11:49:25.686861: step 19935, loss 0.531393.
Train: 2018-08-09T11:49:25.796181: step 19936, loss 0.445328.
Train: 2018-08-09T11:49:25.889939: step 19937, loss 0.630634.
Train: 2018-08-09T11:49:25.997944: step 19938, loss 0.629281.
Train: 2018-08-09T11:49:26.107323: step 19939, loss 0.530676.
Train: 2018-08-09T11:49:26.201022: step 19940, loss 0.528721.
Test: 2018-08-09T11:49:26.700912: step 19940, loss 0.547553.
Train: 2018-08-09T11:49:26.794630: step 19941, loss 0.627854.
Train: 2018-08-09T11:49:26.904010: step 19942, loss 0.562538.
Train: 2018-08-09T11:49:26.997739: step 19943, loss 0.578517.
Train: 2018-08-09T11:49:27.107088: step 19944, loss 0.530114.
Train: 2018-08-09T11:49:27.216407: step 19945, loss 0.545421.
Train: 2018-08-09T11:49:27.310165: step 19946, loss 0.464001.
Train: 2018-08-09T11:49:27.419515: step 19947, loss 0.597173.
Train: 2018-08-09T11:49:27.528833: step 19948, loss 0.562388.
Train: 2018-08-09T11:49:27.622561: step 19949, loss 0.545406.
Train: 2018-08-09T11:49:27.731941: step 19950, loss 0.513971.
Test: 2018-08-09T11:49:28.231818: step 19950, loss 0.548189.
Train: 2018-08-09T11:49:28.325555: step 19951, loss 0.578307.
Train: 2018-08-09T11:49:28.434869: step 19952, loss 0.612132.
Train: 2018-08-09T11:49:28.544244: step 19953, loss 0.462612.
Train: 2018-08-09T11:49:28.637946: step 19954, loss 0.496816.
Train: 2018-08-09T11:49:28.747295: step 19955, loss 0.630291.
Train: 2018-08-09T11:49:28.841054: step 19956, loss 0.528193.
Train: 2018-08-09T11:49:28.951836: step 19957, loss 0.527709.
Train: 2018-08-09T11:49:29.045532: step 19958, loss 0.630372.
Train: 2018-08-09T11:49:29.154881: step 19959, loss 0.528843.
Train: 2018-08-09T11:49:29.264231: step 19960, loss 0.530334.
Test: 2018-08-09T11:49:29.748525: step 19960, loss 0.55093.
Train: 2018-08-09T11:49:29.857871: step 19961, loss 0.613541.
Train: 2018-08-09T11:49:29.967190: step 19962, loss 0.563259.
Train: 2018-08-09T11:49:30.060947: step 19963, loss 0.529983.
Train: 2018-08-09T11:49:30.170297: step 19964, loss 0.612014.
Train: 2018-08-09T11:49:30.264027: step 19965, loss 0.54458.
Train: 2018-08-09T11:49:30.373373: step 19966, loss 0.446037.
Train: 2018-08-09T11:49:30.467071: step 19967, loss 0.562177.
Train: 2018-08-09T11:49:30.576451: step 19968, loss 0.629458.
Train: 2018-08-09T11:49:30.670148: step 19969, loss 0.579623.
Train: 2018-08-09T11:49:30.779528: step 19970, loss 0.578296.
Test: 2018-08-09T11:49:31.280441: step 19970, loss 0.551179.
Train: 2018-08-09T11:49:31.374132: step 19971, loss 0.611503.
Train: 2018-08-09T11:49:31.483512: step 19972, loss 0.561658.
Train: 2018-08-09T11:49:31.577212: step 19973, loss 0.613168.
Train: 2018-08-09T11:49:31.686589: step 19974, loss 0.513327.
Train: 2018-08-09T11:49:31.780316: step 19975, loss 0.480076.
Train: 2018-08-09T11:49:31.889669: step 19976, loss 0.578827.
Train: 2018-08-09T11:49:31.981956: step 19977, loss 0.578861.
Train: 2018-08-09T11:49:32.091305: step 19978, loss 0.545981.
Train: 2018-08-09T11:49:32.200623: step 19979, loss 0.480331.
Train: 2018-08-09T11:49:32.356834: step 19980, loss 0.594881.
Test: 2018-08-09T11:49:32.856719: step 19980, loss 0.549491.
Train: 2018-08-09T11:49:32.950476: step 19981, loss 0.611434.
Train: 2018-08-09T11:49:33.059794: step 19982, loss 0.530067.
Train: 2018-08-09T11:49:33.153523: step 19983, loss 0.529149.
Train: 2018-08-09T11:49:33.262903: step 19984, loss 0.529364.
Train: 2018-08-09T11:49:33.356599: step 19985, loss 0.546715.
Train: 2018-08-09T11:49:33.465979: step 19986, loss 0.612228.
Train: 2018-08-09T11:49:33.575298: step 19987, loss 0.595568.
Train: 2018-08-09T11:49:33.669026: step 19988, loss 0.446179.
Train: 2018-08-09T11:49:33.762784: step 19989, loss 0.496563.
Train: 2018-08-09T11:49:33.872134: step 19990, loss 0.595819.
Test: 2018-08-09T11:49:34.371986: step 19990, loss 0.546752.
Train: 2018-08-09T11:49:34.465743: step 19991, loss 0.579338.
Train: 2018-08-09T11:49:34.575062: step 19992, loss 0.561737.
Train: 2018-08-09T11:49:34.668821: step 19993, loss 0.578062.
Train: 2018-08-09T11:49:34.778170: step 19994, loss 0.612452.
Train: 2018-08-09T11:49:34.871898: step 19995, loss 0.478967.
Train: 2018-08-09T11:49:34.981250: step 19996, loss 0.49417.
Train: 2018-08-09T11:49:35.074945: step 19997, loss 0.479103.
Train: 2018-08-09T11:49:35.184332: step 19998, loss 0.613494.
Train: 2018-08-09T11:49:35.278052: step 19999, loss 0.544527.
Train: 2018-08-09T11:49:35.387372: step 20000, loss 0.682395.
Test: 2018-08-09T11:49:35.887258: step 20000, loss 0.54833.
Train: 2018-08-09T11:49:36.466820: step 20001, loss 0.495586.
Train: 2018-08-09T11:49:36.560547: step 20002, loss 0.459267.
Train: 2018-08-09T11:49:36.669897: step 20003, loss 0.611824.
Train: 2018-08-09T11:49:36.763625: step 20004, loss 0.477757.
Train: 2018-08-09T11:49:36.872943: step 20005, loss 0.578461.
Train: 2018-08-09T11:49:36.966702: step 20006, loss 0.57767.
Train: 2018-08-09T11:49:37.060432: step 20007, loss 0.59942.
Train: 2018-08-09T11:49:37.169749: step 20008, loss 0.614166.
Train: 2018-08-09T11:49:37.279129: step 20009, loss 0.581512.
Train: 2018-08-09T11:49:37.372856: step 20010, loss 0.494648.
Test: 2018-08-09T11:49:37.872710: step 20010, loss 0.548237.
Train: 2018-08-09T11:49:37.967757: step 20011, loss 0.526231.
Train: 2018-08-09T11:49:38.077106: step 20012, loss 0.561662.
Train: 2018-08-09T11:49:38.170835: step 20013, loss 0.614645.
Train: 2018-08-09T11:49:38.280198: step 20014, loss 0.562559.
Train: 2018-08-09T11:49:38.373914: step 20015, loss 0.544701.
Train: 2018-08-09T11:49:38.483262: step 20016, loss 0.459644.
Train: 2018-08-09T11:49:38.576989: step 20017, loss 0.526861.
Train: 2018-08-09T11:49:38.670718: step 20018, loss 0.596783.
Train: 2018-08-09T11:49:38.780097: step 20019, loss 0.649394.
Train: 2018-08-09T11:49:38.873795: step 20020, loss 0.561719.
Test: 2018-08-09T11:49:39.373678: step 20020, loss 0.548974.
Train: 2018-08-09T11:49:39.467404: step 20021, loss 0.512183.
Train: 2018-08-09T11:49:39.576778: step 20022, loss 0.494709.
Train: 2018-08-09T11:49:39.670481: step 20023, loss 0.564232.
Train: 2018-08-09T11:49:39.779831: step 20024, loss 0.529357.
Train: 2018-08-09T11:49:39.873559: step 20025, loss 0.579423.
Train: 2018-08-09T11:49:39.967937: step 20026, loss 0.579025.
Train: 2018-08-09T11:49:40.077286: step 20027, loss 0.612559.
Train: 2018-08-09T11:49:40.171014: step 20028, loss 0.595769.
Train: 2018-08-09T11:49:40.264772: step 20029, loss 0.611653.
Train: 2018-08-09T11:49:40.374121: step 20030, loss 0.563418.
Test: 2018-08-09T11:49:40.873974: step 20030, loss 0.547441.
Train: 2018-08-09T11:49:40.967702: step 20031, loss 0.512738.
Train: 2018-08-09T11:49:41.061430: step 20032, loss 0.594912.
Train: 2018-08-09T11:49:41.170812: step 20033, loss 0.612006.
Train: 2018-08-09T11:49:41.264536: step 20034, loss 0.496234.
Train: 2018-08-09T11:49:41.358264: step 20035, loss 0.528987.
Train: 2018-08-09T11:49:41.467614: step 20036, loss 0.645526.
Train: 2018-08-09T11:49:41.561311: step 20037, loss 0.545964.
Train: 2018-08-09T11:49:41.655069: step 20038, loss 0.562987.
Train: 2018-08-09T11:49:41.764418: step 20039, loss 0.662136.
Train: 2018-08-09T11:49:41.858148: step 20040, loss 0.611473.
Test: 2018-08-09T11:49:42.358000: step 20040, loss 0.548641.
Train: 2018-08-09T11:49:42.451726: step 20041, loss 0.497275.
Train: 2018-08-09T11:49:42.561076: step 20042, loss 0.530317.
Train: 2018-08-09T11:49:42.654834: step 20043, loss 0.481855.
Train: 2018-08-09T11:49:42.748532: step 20044, loss 0.514021.
Train: 2018-08-09T11:49:42.857882: step 20045, loss 0.531053.
Train: 2018-08-09T11:49:42.953828: step 20046, loss 0.530321.
Train: 2018-08-09T11:49:43.047589: step 20047, loss 0.497469.
Train: 2018-08-09T11:49:43.141314: step 20048, loss 0.481025.
Train: 2018-08-09T11:49:43.235012: step 20049, loss 0.595001.
Train: 2018-08-09T11:49:43.344391: step 20050, loss 0.595792.
Test: 2018-08-09T11:49:43.844251: step 20050, loss 0.549495.
Train: 2018-08-09T11:49:43.938002: step 20051, loss 0.52965.
Train: 2018-08-09T11:49:44.031733: step 20052, loss 0.479537.
Train: 2018-08-09T11:49:44.125427: step 20053, loss 0.563386.
Train: 2018-08-09T11:49:44.234776: step 20054, loss 0.680785.
Train: 2018-08-09T11:49:44.328534: step 20055, loss 0.546755.
Train: 2018-08-09T11:49:44.422232: step 20056, loss 0.595815.
Train: 2018-08-09T11:49:44.531581: step 20057, loss 0.529026.
Train: 2018-08-09T11:49:44.625339: step 20058, loss 0.595785.
Train: 2018-08-09T11:49:44.719062: step 20059, loss 0.545552.
Train: 2018-08-09T11:49:44.828416: step 20060, loss 0.562748.
Test: 2018-08-09T11:49:45.314154: step 20060, loss 0.548032.
Train: 2018-08-09T11:49:45.423505: step 20061, loss 0.645943.
Train: 2018-08-09T11:49:45.517204: step 20062, loss 0.61334.
Train: 2018-08-09T11:49:45.610960: step 20063, loss 0.628251.
Train: 2018-08-09T11:49:45.704657: step 20064, loss 0.578809.
Train: 2018-08-09T11:49:45.814038: step 20065, loss 0.496966.
Train: 2018-08-09T11:49:45.907766: step 20066, loss 0.594983.
Train: 2018-08-09T11:49:46.001493: step 20067, loss 0.595168.
Train: 2018-08-09T11:49:46.095224: step 20068, loss 0.578945.
Train: 2018-08-09T11:49:46.188949: step 20069, loss 0.595.
Train: 2018-08-09T11:49:46.298298: step 20070, loss 0.611153.
Test: 2018-08-09T11:49:46.782532: step 20070, loss 0.546407.
Train: 2018-08-09T11:49:46.940169: step 20071, loss 0.482159.
Train: 2018-08-09T11:49:47.033898: step 20072, loss 0.691673.
Train: 2018-08-09T11:49:47.143246: step 20073, loss 0.482574.
Train: 2018-08-09T11:49:47.236973: step 20074, loss 0.594796.
Train: 2018-08-09T11:49:47.330701: step 20075, loss 0.483089.
Train: 2018-08-09T11:49:47.424429: step 20076, loss 0.546925.
Train: 2018-08-09T11:49:47.518128: step 20077, loss 0.594748.
Train: 2018-08-09T11:49:47.611885: step 20078, loss 0.546742.
Train: 2018-08-09T11:49:47.721205: step 20079, loss 0.466844.
Train: 2018-08-09T11:49:47.814962: step 20080, loss 0.530906.
Test: 2018-08-09T11:49:48.314815: step 20080, loss 0.550815.
Train: 2018-08-09T11:49:48.408573: step 20081, loss 0.659274.
Train: 2018-08-09T11:49:48.502304: step 20082, loss 0.643082.
Train: 2018-08-09T11:49:48.596001: step 20083, loss 0.530761.
Train: 2018-08-09T11:49:48.689756: step 20084, loss 0.562994.
Train: 2018-08-09T11:49:48.799108: step 20085, loss 0.626922.
Train: 2018-08-09T11:49:48.892833: step 20086, loss 0.498758.
Train: 2018-08-09T11:49:48.988837: step 20087, loss 0.579058.
Train: 2018-08-09T11:49:49.082563: step 20088, loss 0.61081.
Train: 2018-08-09T11:49:49.176292: step 20089, loss 0.610993.
Train: 2018-08-09T11:49:49.270050: step 20090, loss 0.69067.
Test: 2018-08-09T11:49:49.769902: step 20090, loss 0.54746.
Train: 2018-08-09T11:49:49.863654: step 20091, loss 0.483517.
Train: 2018-08-09T11:49:49.957388: step 20092, loss 0.610575.
Train: 2018-08-09T11:49:50.066738: step 20093, loss 0.6422.
Train: 2018-08-09T11:49:50.160465: step 20094, loss 0.531515.
Train: 2018-08-09T11:49:50.254197: step 20095, loss 0.65761.
Train: 2018-08-09T11:49:50.347920: step 20096, loss 0.563121.
Train: 2018-08-09T11:49:50.441619: step 20097, loss 0.578928.
Train: 2018-08-09T11:49:50.535376: step 20098, loss 0.532095.
Train: 2018-08-09T11:49:50.644697: step 20099, loss 0.578889.
Train: 2018-08-09T11:49:50.738453: step 20100, loss 0.532302.
Test: 2018-08-09T11:49:51.224195: step 20100, loss 0.551771.
Train: 2018-08-09T11:49:51.780490: step 20101, loss 0.501262.
Train: 2018-08-09T11:49:51.874218: step 20102, loss 0.532297.
Train: 2018-08-09T11:49:51.967946: step 20103, loss 0.563366.
Train: 2018-08-09T11:49:52.061674: step 20104, loss 0.641213.
Train: 2018-08-09T11:49:52.155402: step 20105, loss 0.423082.
Train: 2018-08-09T11:49:52.249130: step 20106, loss 0.641412.
Train: 2018-08-09T11:49:52.342858: step 20107, loss 0.516247.
Train: 2018-08-09T11:49:52.436555: step 20108, loss 0.484732.
Train: 2018-08-09T11:49:52.530313: step 20109, loss 0.610336.
Train: 2018-08-09T11:49:52.624042: step 20110, loss 0.515563.
Test: 2018-08-09T11:49:53.115526: step 20110, loss 0.550438.
Train: 2018-08-09T11:49:53.209287: step 20111, loss 0.563061.
Train: 2018-08-09T11:49:53.318632: step 20112, loss 0.531016.
Train: 2018-08-09T11:49:53.412360: step 20113, loss 0.578725.
Train: 2018-08-09T11:49:53.506088: step 20114, loss 0.611248.
Train: 2018-08-09T11:49:53.599816: step 20115, loss 0.530024.
Train: 2018-08-09T11:49:53.685537: step 20116, loss 0.528525.
Train: 2018-08-09T11:49:53.794886: step 20117, loss 0.580156.
Train: 2018-08-09T11:49:53.888614: step 20118, loss 0.579667.
Train: 2018-08-09T11:49:53.982342: step 20119, loss 0.57872.
Train: 2018-08-09T11:49:54.076069: step 20120, loss 0.529385.
Test: 2018-08-09T11:49:54.575923: step 20120, loss 0.549676.
Train: 2018-08-09T11:49:54.669680: step 20121, loss 0.598397.
Train: 2018-08-09T11:49:54.763378: step 20122, loss 0.576249.
Train: 2018-08-09T11:49:54.857136: step 20123, loss 0.678856.
Train: 2018-08-09T11:49:54.950863: step 20124, loss 0.664874.
Train: 2018-08-09T11:49:55.044592: step 20125, loss 0.610319.
Train: 2018-08-09T11:49:55.138290: step 20126, loss 0.547122.
Train: 2018-08-09T11:49:55.232047: step 20127, loss 0.609879.
Train: 2018-08-09T11:49:55.325747: step 20128, loss 0.610194.
Train: 2018-08-09T11:49:55.419503: step 20129, loss 0.515611.
Train: 2018-08-09T11:49:55.523690: step 20130, loss 0.563232.
Test: 2018-08-09T11:49:56.015309: step 20130, loss 0.55028.
Train: 2018-08-09T11:49:56.109067: step 20131, loss 0.594724.
Train: 2018-08-09T11:49:56.202764: step 20132, loss 0.547637.
Train: 2018-08-09T11:49:56.296523: step 20133, loss 0.578909.
Train: 2018-08-09T11:49:56.390254: step 20134, loss 0.516666.
Train: 2018-08-09T11:49:56.483948: step 20135, loss 0.563344.
Train: 2018-08-09T11:49:56.577707: step 20136, loss 0.578874.
Train: 2018-08-09T11:49:56.671437: step 20137, loss 0.625533.
Train: 2018-08-09T11:49:56.765132: step 20138, loss 0.563379.
Train: 2018-08-09T11:49:56.843269: step 20139, loss 0.470403.
Train: 2018-08-09T11:49:56.936997: step 20140, loss 0.547907.
Test: 2018-08-09T11:49:57.436850: step 20140, loss 0.549394.
Train: 2018-08-09T11:49:57.530578: step 20141, loss 0.532286.
Train: 2018-08-09T11:49:57.624305: step 20142, loss 0.594446.
Train: 2018-08-09T11:49:57.718063: step 20143, loss 0.578854.
Train: 2018-08-09T11:49:57.811791: step 20144, loss 0.547621.
Train: 2018-08-09T11:49:57.905519: step 20145, loss 0.594726.
Train: 2018-08-09T11:49:57.999246: step 20146, loss 0.578972.
Train: 2018-08-09T11:49:58.092974: step 20147, loss 0.531805.
Train: 2018-08-09T11:49:58.186702: step 20148, loss 0.578851.
Train: 2018-08-09T11:49:58.280431: step 20149, loss 0.6417.
Train: 2018-08-09T11:49:58.374159: step 20150, loss 0.594766.
Test: 2018-08-09T11:49:58.858391: step 20150, loss 0.550222.
Train: 2018-08-09T11:49:58.954394: step 20151, loss 0.59447.
Train: 2018-08-09T11:49:59.048152: step 20152, loss 0.610064.
Train: 2018-08-09T11:49:59.141880: step 20153, loss 0.500568.
Train: 2018-08-09T11:49:59.235608: step 20154, loss 0.500488.
Train: 2018-08-09T11:49:59.329336: step 20155, loss 0.594329.
Train: 2018-08-09T11:49:59.423064: step 20156, loss 0.610672.
Train: 2018-08-09T11:49:59.516761: step 20157, loss 0.531787.
Train: 2018-08-09T11:49:59.610524: step 20158, loss 0.467864.
Train: 2018-08-09T11:49:59.704248: step 20159, loss 0.530521.
Train: 2018-08-09T11:49:59.797976: step 20160, loss 0.561317.
Test: 2018-08-09T11:50:00.282208: step 20160, loss 0.549053.
Train: 2018-08-09T11:50:00.375964: step 20161, loss 0.516209.
Train: 2018-08-09T11:50:00.469696: step 20162, loss 0.563565.
Train: 2018-08-09T11:50:00.563420: step 20163, loss 0.508779.
Train: 2018-08-09T11:50:00.657118: step 20164, loss 0.46768.
Train: 2018-08-09T11:50:00.750876: step 20165, loss 0.524471.
Train: 2018-08-09T11:50:00.844600: step 20166, loss 0.58115.
Train: 2018-08-09T11:50:00.940755: step 20167, loss 0.583681.
Train: 2018-08-09T11:50:01.034516: step 20168, loss 0.575542.
Train: 2018-08-09T11:50:01.128212: step 20169, loss 0.512719.
Train: 2018-08-09T11:50:01.206347: step 20170, loss 0.611683.
Test: 2018-08-09T11:50:01.706226: step 20170, loss 0.54727.
Train: 2018-08-09T11:50:01.799928: step 20171, loss 0.509869.
Train: 2018-08-09T11:50:01.893686: step 20172, loss 0.417559.
Train: 2018-08-09T11:50:01.987413: step 20173, loss 0.491536.
Train: 2018-08-09T11:50:02.081112: step 20174, loss 0.534086.
Train: 2018-08-09T11:50:02.174839: step 20175, loss 0.478325.
Train: 2018-08-09T11:50:02.268597: step 20176, loss 0.578577.
Train: 2018-08-09T11:50:02.362325: step 20177, loss 0.554122.
Train: 2018-08-09T11:50:02.456053: step 20178, loss 0.480849.
Train: 2018-08-09T11:50:02.549782: step 20179, loss 0.5123.
Train: 2018-08-09T11:50:02.627888: step 20180, loss 0.72965.
Test: 2018-08-09T11:50:03.130054: step 20180, loss 0.548441.
Train: 2018-08-09T11:50:03.223812: step 20181, loss 0.513195.
Train: 2018-08-09T11:50:03.317540: step 20182, loss 0.517875.
Train: 2018-08-09T11:50:03.395646: step 20183, loss 0.532071.
Train: 2018-08-09T11:50:03.489374: step 20184, loss 0.510821.
Train: 2018-08-09T11:50:03.583102: step 20185, loss 0.581657.
Train: 2018-08-09T11:50:03.676829: step 20186, loss 0.493624.
Train: 2018-08-09T11:50:03.770558: step 20187, loss 0.575155.
Train: 2018-08-09T11:50:03.864255: step 20188, loss 0.530895.
Train: 2018-08-09T11:50:03.957984: step 20189, loss 0.715277.
Train: 2018-08-09T11:50:04.051742: step 20190, loss 0.564627.
Test: 2018-08-09T11:50:04.551626: step 20190, loss 0.548375.
Train: 2018-08-09T11:50:04.629701: step 20191, loss 0.547146.
Train: 2018-08-09T11:50:04.723428: step 20192, loss 0.46114.
Train: 2018-08-09T11:50:04.820684: step 20193, loss 0.56303.
Train: 2018-08-09T11:50:04.914441: step 20194, loss 0.645992.
Train: 2018-08-09T11:50:05.008140: step 20195, loss 0.478714.
Train: 2018-08-09T11:50:05.101898: step 20196, loss 0.596897.
Train: 2018-08-09T11:50:05.195628: step 20197, loss 0.562542.
Train: 2018-08-09T11:50:05.289322: step 20198, loss 0.545879.
Train: 2018-08-09T11:50:05.383081: step 20199, loss 0.595639.
Train: 2018-08-09T11:50:05.476809: step 20200, loss 0.462998.
Test: 2018-08-09T11:50:05.976687: step 20200, loss 0.550067.
Train: 2018-08-09T11:50:06.507815: step 20201, loss 0.628915.
Train: 2018-08-09T11:50:06.601544: step 20202, loss 0.628698.
Train: 2018-08-09T11:50:06.695272: step 20203, loss 0.578985.
Train: 2018-08-09T11:50:06.788969: step 20204, loss 0.562569.
Train: 2018-08-09T11:50:06.882728: step 20205, loss 0.529661.
Train: 2018-08-09T11:50:06.978725: step 20206, loss 0.661106.
Train: 2018-08-09T11:50:07.072452: step 20207, loss 0.480659.
Train: 2018-08-09T11:50:07.166160: step 20208, loss 0.546207.
Train: 2018-08-09T11:50:07.259878: step 20209, loss 0.562592.
Train: 2018-08-09T11:50:07.337984: step 20210, loss 0.480798.
Test: 2018-08-09T11:50:07.837898: step 20210, loss 0.547236.
Train: 2018-08-09T11:50:07.947247: step 20211, loss 0.628035.
Train: 2018-08-09T11:50:08.040945: step 20212, loss 0.513457.
Train: 2018-08-09T11:50:08.119081: step 20213, loss 0.578916.
Train: 2018-08-09T11:50:08.212809: step 20214, loss 0.562612.
Train: 2018-08-09T11:50:08.306536: step 20215, loss 0.546216.
Train: 2018-08-09T11:50:08.400265: step 20216, loss 0.529912.
Train: 2018-08-09T11:50:08.493992: step 20217, loss 0.578811.
Train: 2018-08-09T11:50:08.587690: step 20218, loss 0.661324.
Train: 2018-08-09T11:50:08.681449: step 20219, loss 0.562635.
Train: 2018-08-09T11:50:08.775177: step 20220, loss 0.578935.
Test: 2018-08-09T11:50:09.261780: step 20220, loss 0.548509.
Train: 2018-08-09T11:50:09.355539: step 20221, loss 0.546232.
Train: 2018-08-09T11:50:09.449267: step 20222, loss 0.595271.
Train: 2018-08-09T11:50:09.542989: step 20223, loss 0.57894.
Train: 2018-08-09T11:50:09.636722: step 20224, loss 0.513763.
Train: 2018-08-09T11:50:09.730450: step 20225, loss 0.692911.
Train: 2018-08-09T11:50:09.824178: step 20226, loss 0.513948.
Train: 2018-08-09T11:50:09.917894: step 20227, loss 0.627554.
Train: 2018-08-09T11:50:09.996012: step 20228, loss 0.562704.
Train: 2018-08-09T11:50:10.089741: step 20229, loss 0.578877.
Train: 2018-08-09T11:50:10.183468: step 20230, loss 0.627231.
Test: 2018-08-09T11:50:10.683322: step 20230, loss 0.549575.
Train: 2018-08-09T11:50:10.777078: step 20231, loss 0.578874.
Train: 2018-08-09T11:50:10.870806: step 20232, loss 0.56282.
Train: 2018-08-09T11:50:10.965894: step 20233, loss 0.451023.
Train: 2018-08-09T11:50:11.044001: step 20234, loss 0.562872.
Train: 2018-08-09T11:50:11.153321: step 20235, loss 0.562872.
Train: 2018-08-09T11:50:11.231460: step 20236, loss 0.610817.
Train: 2018-08-09T11:50:11.325185: step 20237, loss 0.642724.
Train: 2018-08-09T11:50:11.418913: step 20238, loss 0.610729.
Train: 2018-08-09T11:50:11.512641: step 20239, loss 0.547069.
Train: 2018-08-09T11:50:11.606339: step 20240, loss 0.45192.
Test: 2018-08-09T11:50:12.106223: step 20240, loss 0.549914.
Train: 2018-08-09T11:50:12.199979: step 20241, loss 0.547128.
Train: 2018-08-09T11:50:12.293707: step 20242, loss 0.547098.
Train: 2018-08-09T11:50:12.387435: step 20243, loss 0.531176.
Train: 2018-08-09T11:50:12.481164: step 20244, loss 0.499275.
Train: 2018-08-09T11:50:12.574893: step 20245, loss 0.626716.
Train: 2018-08-09T11:50:12.652997: step 20246, loss 0.642782.
Train: 2018-08-09T11:50:12.746726: step 20247, loss 0.530963.
Train: 2018-08-09T11:50:12.840424: step 20248, loss 0.562887.
Train: 2018-08-09T11:50:12.936532: step 20249, loss 0.594866.
Train: 2018-08-09T11:50:13.030290: step 20250, loss 0.498953.
Test: 2018-08-09T11:50:13.530142: step 20250, loss 0.549082.
Train: 2018-08-09T11:50:13.623870: step 20251, loss 0.514868.
Train: 2018-08-09T11:50:13.717627: step 20252, loss 0.594893.
Train: 2018-08-09T11:50:13.811357: step 20253, loss 0.514692.
Train: 2018-08-09T11:50:13.905083: step 20254, loss 0.562817.
Train: 2018-08-09T11:50:13.998809: step 20255, loss 0.530547.
Train: 2018-08-09T11:50:14.092541: step 20256, loss 0.530464.
Train: 2018-08-09T11:50:14.170648: step 20257, loss 0.578915.
Train: 2018-08-09T11:50:14.264344: step 20258, loss 0.562671.
Train: 2018-08-09T11:50:14.358105: step 20259, loss 0.725102.
Train: 2018-08-09T11:50:14.451830: step 20260, loss 0.562663.
Test: 2018-08-09T11:50:14.952218: step 20260, loss 0.549336.
Train: 2018-08-09T11:50:15.045976: step 20261, loss 0.643793.
Train: 2018-08-09T11:50:15.139702: step 20262, loss 0.611248.
Train: 2018-08-09T11:50:15.233430: step 20263, loss 0.546614.
Train: 2018-08-09T11:50:15.327158: step 20264, loss 0.627171.
Train: 2018-08-09T11:50:15.420881: step 20265, loss 0.562801.
Train: 2018-08-09T11:50:15.514585: step 20266, loss 0.57884.
Train: 2018-08-09T11:50:15.608312: step 20267, loss 0.610734.
Train: 2018-08-09T11:50:15.702069: step 20268, loss 0.531155.
Train: 2018-08-09T11:50:15.795799: step 20269, loss 0.594757.
Train: 2018-08-09T11:50:15.889496: step 20270, loss 0.626138.
Test: 2018-08-09T11:50:16.389380: step 20270, loss 0.549582.
Train: 2018-08-09T11:50:16.483106: step 20271, loss 0.531678.
Train: 2018-08-09T11:50:16.576834: step 20272, loss 0.532549.
Train: 2018-08-09T11:50:16.670586: step 20273, loss 0.547494.
Train: 2018-08-09T11:50:16.764289: step 20274, loss 0.531562.
Train: 2018-08-09T11:50:16.842427: step 20275, loss 0.563111.
Train: 2018-08-09T11:50:16.936155: step 20276, loss 0.500021.
Train: 2018-08-09T11:50:17.029884: step 20277, loss 0.563074.
Train: 2018-08-09T11:50:17.123611: step 20278, loss 0.578861.
Train: 2018-08-09T11:50:17.217337: step 20279, loss 0.531381.
Train: 2018-08-09T11:50:17.311036: step 20280, loss 0.483756.
Test: 2018-08-09T11:50:17.810918: step 20280, loss 0.553481.
Train: 2018-08-09T11:50:17.907323: step 20281, loss 0.547069.
Train: 2018-08-09T11:50:18.001052: step 20282, loss 0.562915.
Train: 2018-08-09T11:50:18.094810: step 20283, loss 0.578853.
Train: 2018-08-09T11:50:18.188539: step 20284, loss 0.514784.
Train: 2018-08-09T11:50:18.282265: step 20285, loss 0.643123.
Train: 2018-08-09T11:50:18.375994: step 20286, loss 0.466279.
Train: 2018-08-09T11:50:18.469722: step 20287, loss 0.546633.
Train: 2018-08-09T11:50:18.563449: step 20288, loss 0.659783.
Train: 2018-08-09T11:50:18.657177: step 20289, loss 0.497921.
Train: 2018-08-09T11:50:18.750904: step 20290, loss 0.643877.
Test: 2018-08-09T11:50:19.250759: step 20290, loss 0.552985.
Train: 2018-08-09T11:50:19.344516: step 20291, loss 0.562643.
Train: 2018-08-09T11:50:19.438213: step 20292, loss 0.513967.
Train: 2018-08-09T11:50:19.531972: step 20293, loss 0.497634.
Train: 2018-08-09T11:50:19.625700: step 20294, loss 0.529935.
Train: 2018-08-09T11:50:19.719427: step 20295, loss 0.595234.
Train: 2018-08-09T11:50:19.813124: step 20296, loss 0.595172.
Train: 2018-08-09T11:50:19.908276: step 20297, loss 0.562417.
Train: 2018-08-09T11:50:20.002035: step 20298, loss 0.545653.
Train: 2018-08-09T11:50:20.095762: step 20299, loss 0.579215.
Train: 2018-08-09T11:50:20.189490: step 20300, loss 0.578807.
Test: 2018-08-09T11:50:20.689345: step 20300, loss 0.549504.
Train: 2018-08-09T11:50:21.267361: step 20301, loss 0.611868.
Train: 2018-08-09T11:50:21.361091: step 20302, loss 0.528391.
Train: 2018-08-09T11:50:21.454817: step 20303, loss 0.561898.
Train: 2018-08-09T11:50:21.564137: step 20304, loss 0.56211.
Train: 2018-08-09T11:50:21.657865: step 20305, loss 0.596333.
Train: 2018-08-09T11:50:21.751617: step 20306, loss 0.512983.
Train: 2018-08-09T11:50:21.845350: step 20307, loss 0.647109.
Train: 2018-08-09T11:50:21.940625: step 20308, loss 0.56133.
Train: 2018-08-09T11:50:22.034353: step 20309, loss 0.560825.
Train: 2018-08-09T11:50:22.128082: step 20310, loss 0.628595.
Test: 2018-08-09T11:50:22.627934: step 20310, loss 0.548704.
Train: 2018-08-09T11:50:22.721662: step 20311, loss 0.529595.
Train: 2018-08-09T11:50:22.815420: step 20312, loss 0.597859.
Train: 2018-08-09T11:50:22.909147: step 20313, loss 0.512805.
Train: 2018-08-09T11:50:23.002876: step 20314, loss 0.461869.
Train: 2018-08-09T11:50:23.096603: step 20315, loss 0.495979.
Train: 2018-08-09T11:50:23.205925: step 20316, loss 0.545083.
Train: 2018-08-09T11:50:23.299649: step 20317, loss 0.511966.
Train: 2018-08-09T11:50:23.393379: step 20318, loss 0.478567.
Train: 2018-08-09T11:50:23.487136: step 20319, loss 0.595477.
Train: 2018-08-09T11:50:23.580864: step 20320, loss 0.646493.
Test: 2018-08-09T11:50:24.067445: step 20320, loss 0.546786.
Train: 2018-08-09T11:50:24.175080: step 20321, loss 0.509827.
Train: 2018-08-09T11:50:24.268808: step 20322, loss 0.561401.
Train: 2018-08-09T11:50:24.362507: step 20323, loss 0.567005.
Train: 2018-08-09T11:50:24.456234: step 20324, loss 0.445061.
Train: 2018-08-09T11:50:24.549992: step 20325, loss 0.610446.
Train: 2018-08-09T11:50:24.659341: step 20326, loss 0.544831.
Train: 2018-08-09T11:50:24.753069: step 20327, loss 0.574089.
Train: 2018-08-09T11:50:24.846797: step 20328, loss 0.512699.
Train: 2018-08-09T11:50:24.940524: step 20329, loss 0.665805.
Train: 2018-08-09T11:50:25.034253: step 20330, loss 0.494808.
Test: 2018-08-09T11:50:25.534107: step 20330, loss 0.547307.
Train: 2018-08-09T11:50:25.627864: step 20331, loss 0.527244.
Train: 2018-08-09T11:50:25.721593: step 20332, loss 0.72317.
Train: 2018-08-09T11:50:25.815319: step 20333, loss 0.548907.
Train: 2018-08-09T11:50:25.912358: step 20334, loss 0.546514.
Train: 2018-08-09T11:50:26.006117: step 20335, loss 0.59601.
Train: 2018-08-09T11:50:26.099845: step 20336, loss 0.543567.
Train: 2018-08-09T11:50:26.209193: step 20337, loss 0.545343.
Train: 2018-08-09T11:50:26.302921: step 20338, loss 0.662181.
Train: 2018-08-09T11:50:26.396649: step 20339, loss 0.479723.
Train: 2018-08-09T11:50:26.484398: step 20340, loss 0.529861.
Test: 2018-08-09T11:50:26.997430: step 20340, loss 0.551013.
Train: 2018-08-09T11:50:27.078659: step 20341, loss 0.513288.
Train: 2018-08-09T11:50:27.188008: step 20342, loss 0.512719.
Train: 2018-08-09T11:50:27.281736: step 20343, loss 0.4807.
Train: 2018-08-09T11:50:27.375464: step 20344, loss 0.513393.
Train: 2018-08-09T11:50:27.469192: step 20345, loss 0.512662.
Train: 2018-08-09T11:50:27.578541: step 20346, loss 0.496658.
Train: 2018-08-09T11:50:27.672269: step 20347, loss 0.577796.
Train: 2018-08-09T11:50:27.765997: step 20348, loss 0.560945.
Train: 2018-08-09T11:50:27.859694: step 20349, loss 0.581375.
Train: 2018-08-09T11:50:27.959746: step 20350, loss 0.596405.
Test: 2018-08-09T11:50:28.459599: step 20350, loss 0.547677.
Train: 2018-08-09T11:50:28.553355: step 20351, loss 0.562302.
Train: 2018-08-09T11:50:28.647083: step 20352, loss 0.696934.
Train: 2018-08-09T11:50:28.740782: step 20353, loss 0.563251.
Train: 2018-08-09T11:50:28.834539: step 20354, loss 0.493903.
Train: 2018-08-09T11:50:28.943889: step 20355, loss 0.596908.
Train: 2018-08-09T11:50:29.037616: step 20356, loss 0.648408.
Train: 2018-08-09T11:50:29.131345: step 20357, loss 0.613065.
Train: 2018-08-09T11:50:29.225074: step 20358, loss 0.528171.
Train: 2018-08-09T11:50:29.334422: step 20359, loss 0.479918.
Train: 2018-08-09T11:50:29.428119: step 20360, loss 0.58016.
Test: 2018-08-09T11:50:29.928003: step 20360, loss 0.548872.
Train: 2018-08-09T11:50:30.021730: step 20361, loss 0.628533.
Train: 2018-08-09T11:50:30.115458: step 20362, loss 0.644551.
Train: 2018-08-09T11:50:30.209215: step 20363, loss 0.513244.
Train: 2018-08-09T11:50:30.318565: step 20364, loss 0.52984.
Train: 2018-08-09T11:50:30.412292: step 20365, loss 0.545958.
Train: 2018-08-09T11:50:30.506020: step 20366, loss 0.465043.
Train: 2018-08-09T11:50:30.599748: step 20367, loss 0.660263.
Train: 2018-08-09T11:50:30.709097: step 20368, loss 0.56338.
Train: 2018-08-09T11:50:30.802826: step 20369, loss 0.659624.
Train: 2018-08-09T11:50:30.896553: step 20370, loss 0.562681.
Test: 2018-08-09T11:50:31.398790: step 20370, loss 0.548875.
Train: 2018-08-09T11:50:31.492518: step 20371, loss 0.643441.
Train: 2018-08-09T11:50:31.601897: step 20372, loss 0.610954.
Train: 2018-08-09T11:50:31.695624: step 20373, loss 0.499066.
Train: 2018-08-09T11:50:31.789352: step 20374, loss 0.626863.
Train: 2018-08-09T11:50:31.898671: step 20375, loss 0.467598.
Train: 2018-08-09T11:50:31.992429: step 20376, loss 0.531179.
Train: 2018-08-09T11:50:32.086157: step 20377, loss 0.562989.
Train: 2018-08-09T11:50:32.195507: step 20378, loss 0.642366.
Train: 2018-08-09T11:50:32.289235: step 20379, loss 0.547199.
Train: 2018-08-09T11:50:32.398578: step 20380, loss 0.563007.
Test: 2018-08-09T11:50:32.882816: step 20380, loss 0.551152.
Train: 2018-08-09T11:50:32.994466: step 20381, loss 0.563023.
Train: 2018-08-09T11:50:33.088193: step 20382, loss 0.594687.
Train: 2018-08-09T11:50:33.181921: step 20383, loss 0.515583.
Train: 2018-08-09T11:50:33.291271: step 20384, loss 0.483808.
Train: 2018-08-09T11:50:33.384998: step 20385, loss 0.547147.
Train: 2018-08-09T11:50:33.478727: step 20386, loss 0.578799.
Train: 2018-08-09T11:50:33.588075: step 20387, loss 0.626375.
Train: 2018-08-09T11:50:33.681805: step 20388, loss 0.51426.
Train: 2018-08-09T11:50:33.775531: step 20389, loss 0.546358.
Train: 2018-08-09T11:50:33.884851: step 20390, loss 0.57779.
Test: 2018-08-09T11:50:34.384733: step 20390, loss 0.550794.
Train: 2018-08-09T11:50:34.478461: step 20391, loss 0.566736.
Train: 2018-08-09T11:50:34.587841: step 20392, loss 0.597632.
Train: 2018-08-09T11:50:34.681567: step 20393, loss 0.562583.
Train: 2018-08-09T11:50:34.775265: step 20394, loss 0.547026.
Train: 2018-08-09T11:50:34.884622: step 20395, loss 0.580375.
Train: 2018-08-09T11:50:34.980766: step 20396, loss 0.546338.
Train: 2018-08-09T11:50:35.074523: step 20397, loss 0.579264.
Train: 2018-08-09T11:50:35.168252: step 20398, loss 0.563002.
Train: 2018-08-09T11:50:35.277601: step 20399, loss 0.514884.
Train: 2018-08-09T11:50:35.371329: step 20400, loss 0.578803.
Test: 2018-08-09T11:50:35.871182: step 20400, loss 0.548986.
Train: 2018-08-09T11:50:36.449200: step 20401, loss 0.562758.
Train: 2018-08-09T11:50:36.542928: step 20402, loss 0.611196.
Train: 2018-08-09T11:50:36.652248: step 20403, loss 0.594686.
Train: 2018-08-09T11:50:36.745975: step 20404, loss 0.594787.
Train: 2018-08-09T11:50:36.855324: step 20405, loss 0.562926.
Train: 2018-08-09T11:50:36.950461: step 20406, loss 0.530874.
Train: 2018-08-09T11:50:37.044188: step 20407, loss 0.547176.
Train: 2018-08-09T11:50:37.153507: step 20408, loss 0.594804.
Train: 2018-08-09T11:50:37.247265: step 20409, loss 0.546956.
Train: 2018-08-09T11:50:37.356617: step 20410, loss 0.546721.
Test: 2018-08-09T11:50:37.843412: step 20410, loss 0.54659.
Train: 2018-08-09T11:50:37.952790: step 20411, loss 0.659485.
Train: 2018-08-09T11:50:38.046488: step 20412, loss 0.546749.
Train: 2018-08-09T11:50:38.155838: step 20413, loss 0.562938.
Train: 2018-08-09T11:50:38.249596: step 20414, loss 0.515093.
Train: 2018-08-09T11:50:38.358914: step 20415, loss 0.626677.
Train: 2018-08-09T11:50:38.452677: step 20416, loss 0.467405.
Train: 2018-08-09T11:50:38.562026: step 20417, loss 0.642588.
Train: 2018-08-09T11:50:38.655753: step 20418, loss 0.547001.
Train: 2018-08-09T11:50:38.765099: step 20419, loss 0.594797.
Train: 2018-08-09T11:50:38.858798: step 20420, loss 0.562941.
Test: 2018-08-09T11:50:39.357269: step 20420, loss 0.551627.
Train: 2018-08-09T11:50:39.451027: step 20421, loss 0.594797.
Train: 2018-08-09T11:50:39.560376: step 20422, loss 0.594776.
Train: 2018-08-09T11:50:39.654073: step 20423, loss 0.483469.
Train: 2018-08-09T11:50:39.763453: step 20424, loss 0.531147.
Train: 2018-08-09T11:50:39.872803: step 20425, loss 0.57885.
Train: 2018-08-09T11:50:39.966534: step 20426, loss 0.674467.
Train: 2018-08-09T11:50:40.075881: step 20427, loss 0.56297.
Train: 2018-08-09T11:50:40.169578: step 20428, loss 0.48344.
Train: 2018-08-09T11:50:40.263336: step 20429, loss 0.483377.
Train: 2018-08-09T11:50:40.372685: step 20430, loss 0.531027.
Test: 2018-08-09T11:50:40.872538: step 20430, loss 0.549711.
Train: 2018-08-09T11:50:40.966266: step 20431, loss 0.546882.
Train: 2018-08-09T11:50:41.075614: step 20432, loss 0.482749.
Train: 2018-08-09T11:50:41.169373: step 20433, loss 0.562823.
Train: 2018-08-09T11:50:41.278721: step 20434, loss 0.562707.
Train: 2018-08-09T11:50:41.372450: step 20435, loss 0.578871.
Train: 2018-08-09T11:50:41.481798: step 20436, loss 0.497456.
Train: 2018-08-09T11:50:41.575526: step 20437, loss 0.627942.
Train: 2018-08-09T11:50:41.684876: step 20438, loss 0.447493.
Train: 2018-08-09T11:50:41.778603: step 20439, loss 0.577875.
Train: 2018-08-09T11:50:41.887953: step 20440, loss 0.577122.
Test: 2018-08-09T11:50:42.374415: step 20440, loss 0.549502.
Train: 2018-08-09T11:50:42.483793: step 20441, loss 0.510799.
Train: 2018-08-09T11:50:42.577521: step 20442, loss 0.545552.
Train: 2018-08-09T11:50:42.686873: step 20443, loss 0.5439.
Train: 2018-08-09T11:50:42.780599: step 20444, loss 0.680642.
Train: 2018-08-09T11:50:42.889948: step 20445, loss 0.571176.
Train: 2018-08-09T11:50:42.983676: step 20446, loss 0.578491.
Train: 2018-08-09T11:50:43.093025: step 20447, loss 0.544105.
Train: 2018-08-09T11:50:43.198326: step 20448, loss 0.524844.
Train: 2018-08-09T11:50:43.292057: step 20449, loss 0.563775.
Train: 2018-08-09T11:50:43.401402: step 20450, loss 0.475779.
Test: 2018-08-09T11:50:43.901256: step 20450, loss 0.545724.
Train: 2018-08-09T11:50:44.000256: step 20451, loss 0.580668.
Train: 2018-08-09T11:50:44.109635: step 20452, loss 0.56108.
Train: 2018-08-09T11:50:44.203365: step 20453, loss 0.526915.
Train: 2018-08-09T11:50:44.312712: step 20454, loss 0.647656.
Train: 2018-08-09T11:50:44.406410: step 20455, loss 0.529847.
Train: 2018-08-09T11:50:44.515789: step 20456, loss 0.511926.
Train: 2018-08-09T11:50:44.609519: step 20457, loss 0.630352.
Train: 2018-08-09T11:50:44.718867: step 20458, loss 0.597666.
Train: 2018-08-09T11:50:44.812596: step 20459, loss 0.595553.
Train: 2018-08-09T11:50:44.921944: step 20460, loss 0.628616.
Test: 2018-08-09T11:50:45.421798: step 20460, loss 0.547806.
Train: 2018-08-09T11:50:45.515554: step 20461, loss 0.644798.
Train: 2018-08-09T11:50:45.624903: step 20462, loss 0.611474.
Train: 2018-08-09T11:50:45.734253: step 20463, loss 0.660024.
Train: 2018-08-09T11:50:45.827980: step 20464, loss 0.562748.
Train: 2018-08-09T11:50:45.937300: step 20465, loss 0.562822.
Train: 2018-08-09T11:50:46.031028: step 20466, loss 0.530978.
Train: 2018-08-09T11:50:46.140407: step 20467, loss 0.578859.
Train: 2018-08-09T11:50:46.234135: step 20468, loss 0.512236.
Train: 2018-08-09T11:50:46.343483: step 20469, loss 0.578873.
Train: 2018-08-09T11:50:46.452834: step 20470, loss 0.547236.
Test: 2018-08-09T11:50:46.939448: step 20470, loss 0.551259.
Train: 2018-08-09T11:50:47.048766: step 20471, loss 0.689441.
Train: 2018-08-09T11:50:47.158114: step 20472, loss 0.50017.
Train: 2018-08-09T11:50:47.251871: step 20473, loss 0.531728.
Train: 2018-08-09T11:50:47.361221: step 20474, loss 0.500338.
Train: 2018-08-09T11:50:47.470550: step 20475, loss 0.563137.
Train: 2018-08-09T11:50:47.564298: step 20476, loss 0.626056.
Train: 2018-08-09T11:50:47.673647: step 20477, loss 0.531464.
Train: 2018-08-09T11:50:47.767375: step 20478, loss 0.515918.
Train: 2018-08-09T11:50:47.876724: step 20479, loss 0.563026.
Train: 2018-08-09T11:50:47.970423: step 20480, loss 0.57841.
Test: 2018-08-09T11:50:48.470305: step 20480, loss 0.549519.
Train: 2018-08-09T11:50:48.579689: step 20481, loss 0.530687.
Train: 2018-08-09T11:50:48.673412: step 20482, loss 0.529967.
Train: 2018-08-09T11:50:48.782732: step 20483, loss 0.62731.
Train: 2018-08-09T11:50:48.892080: step 20484, loss 0.477609.
Train: 2018-08-09T11:50:48.988199: step 20485, loss 0.565738.
Train: 2018-08-09T11:50:49.097543: step 20486, loss 0.568282.
Train: 2018-08-09T11:50:49.206869: step 20487, loss 0.54239.
Train: 2018-08-09T11:50:49.300629: step 20488, loss 0.592727.
Train: 2018-08-09T11:50:49.409977: step 20489, loss 0.494549.
Train: 2018-08-09T11:50:49.503703: step 20490, loss 0.629144.
Test: 2018-08-09T11:50:50.003588: step 20490, loss 0.547354.
Train: 2018-08-09T11:50:50.112906: step 20491, loss 0.73616.
Train: 2018-08-09T11:50:50.222284: step 20492, loss 0.546175.
Train: 2018-08-09T11:50:50.316012: step 20493, loss 0.660298.
Train: 2018-08-09T11:50:50.425331: step 20494, loss 0.614014.
Train: 2018-08-09T11:50:50.519058: step 20495, loss 0.706297.
Train: 2018-08-09T11:50:50.628440: step 20496, loss 0.469339.
Train: 2018-08-09T11:50:50.722137: step 20497, loss 0.547585.
Train: 2018-08-09T11:50:50.831485: step 20498, loss 0.563503.
Train: 2018-08-09T11:50:50.927530: step 20499, loss 0.563649.
Train: 2018-08-09T11:50:51.036911: step 20500, loss 0.609752.
Test: 2018-08-09T11:50:51.536762: step 20500, loss 0.551563.
Train: 2018-08-09T11:50:52.192890: step 20501, loss 0.486852.
Train: 2018-08-09T11:50:52.286616: step 20502, loss 0.502335.
Train: 2018-08-09T11:50:52.395964: step 20503, loss 0.609652.
Train: 2018-08-09T11:50:52.489663: step 20504, loss 0.624976.
Train: 2018-08-09T11:50:52.599042: step 20505, loss 0.548358.
Train: 2018-08-09T11:50:52.708392: step 20506, loss 0.517751.
Train: 2018-08-09T11:50:52.802120: step 20507, loss 0.502383.
Train: 2018-08-09T11:50:52.913812: step 20508, loss 0.563626.
Train: 2018-08-09T11:50:53.007570: step 20509, loss 0.532823.
Train: 2018-08-09T11:50:53.116890: step 20510, loss 0.471021.
Test: 2018-08-09T11:50:53.616772: step 20510, loss 0.551868.
Train: 2018-08-09T11:50:53.726151: step 20511, loss 0.594405.
Train: 2018-08-09T11:50:53.835470: step 20512, loss 0.547852.
Train: 2018-08-09T11:50:53.936919: step 20513, loss 0.641229.
Train: 2018-08-09T11:50:54.046238: step 20514, loss 0.688167.
Train: 2018-08-09T11:50:54.139999: step 20515, loss 0.610104.
Train: 2018-08-09T11:50:54.249347: step 20516, loss 0.547703.
Train: 2018-08-09T11:50:54.360861: step 20517, loss 0.563305.
Train: 2018-08-09T11:50:54.464012: step 20518, loss 0.500973.
Train: 2018-08-09T11:50:54.557770: step 20519, loss 0.610103.
Train: 2018-08-09T11:50:54.667120: step 20520, loss 0.625748.
Test: 2018-08-09T11:50:55.167004: step 20520, loss 0.549831.
Train: 2018-08-09T11:50:55.260730: step 20521, loss 0.516474.
Train: 2018-08-09T11:50:55.370083: step 20522, loss 0.516413.
Train: 2018-08-09T11:50:55.479428: step 20523, loss 0.531812.
Train: 2018-08-09T11:50:55.573156: step 20524, loss 0.594447.
Train: 2018-08-09T11:50:55.682507: step 20525, loss 0.547109.
Train: 2018-08-09T11:50:55.791855: step 20526, loss 0.594572.
Train: 2018-08-09T11:50:55.885852: step 20527, loss 0.594499.
Train: 2018-08-09T11:50:55.995234: step 20528, loss 0.577525.
Train: 2018-08-09T11:50:56.104580: step 20529, loss 0.482554.
Train: 2018-08-09T11:50:56.198310: step 20530, loss 0.480934.
Test: 2018-08-09T11:50:56.698219: step 20530, loss 0.547932.
Train: 2018-08-09T11:50:56.791919: step 20531, loss 0.629189.
Train: 2018-08-09T11:50:56.901238: step 20532, loss 0.532762.
Train: 2018-08-09T11:50:56.999403: step 20533, loss 0.62386.
Train: 2018-08-09T11:50:57.108723: step 20534, loss 0.662612.
Train: 2018-08-09T11:50:57.218103: step 20535, loss 0.579224.
Train: 2018-08-09T11:50:57.311830: step 20536, loss 0.629477.
Train: 2018-08-09T11:50:57.421149: step 20537, loss 0.598967.
Train: 2018-08-09T11:50:57.530523: step 20538, loss 0.658851.
Train: 2018-08-09T11:50:57.624226: step 20539, loss 0.656904.
Train: 2018-08-09T11:50:57.733576: step 20540, loss 0.625031.
Test: 2018-08-09T11:50:58.233459: step 20540, loss 0.552867.
Train: 2018-08-09T11:50:58.327217: step 20541, loss 0.51657.
Train: 2018-08-09T11:50:58.436565: step 20542, loss 0.547593.
Train: 2018-08-09T11:50:58.551702: step 20543, loss 0.563522.
Train: 2018-08-09T11:50:58.645460: step 20544, loss 0.487064.
Train: 2018-08-09T11:50:58.754810: step 20545, loss 0.624962.
Train: 2018-08-09T11:50:58.848537: step 20546, loss 0.594327.
Train: 2018-08-09T11:50:58.968180: step 20547, loss 0.533321.
Train: 2018-08-09T11:50:59.061909: step 20548, loss 0.548627.
Train: 2018-08-09T11:50:59.171258: step 20549, loss 0.579045.
Train: 2018-08-09T11:50:59.280867: step 20550, loss 0.68546.
Test: 2018-08-09T11:50:59.765144: step 20550, loss 0.551984.
Train: 2018-08-09T11:50:59.874478: step 20551, loss 0.548746.
Train: 2018-08-09T11:50:59.968236: step 20552, loss 0.533653.
Train: 2018-08-09T11:51:00.077585: step 20553, loss 0.518538.
Train: 2018-08-09T11:51:00.171316: step 20554, loss 0.63968.
Train: 2018-08-09T11:51:00.280633: step 20555, loss 0.518525.
Train: 2018-08-09T11:51:00.390012: step 20556, loss 0.563932.
Train: 2018-08-09T11:51:00.499331: step 20557, loss 0.503252.
Train: 2018-08-09T11:51:00.593089: step 20558, loss 0.533447.
Train: 2018-08-09T11:51:00.702409: step 20559, loss 0.594277.
Train: 2018-08-09T11:51:00.796169: step 20560, loss 0.594324.
Test: 2018-08-09T11:51:01.296019: step 20560, loss 0.552807.
Train: 2018-08-09T11:51:01.405397: step 20561, loss 0.563761.
Train: 2018-08-09T11:51:01.499125: step 20562, loss 0.53289.
Train: 2018-08-09T11:51:01.608474: step 20563, loss 0.471085.
Train: 2018-08-09T11:51:01.717793: step 20564, loss 0.516566.
Train: 2018-08-09T11:51:01.811552: step 20565, loss 0.562529.
Train: 2018-08-09T11:51:01.922271: step 20566, loss 0.594745.
Train: 2018-08-09T11:51:02.016003: step 20567, loss 0.596772.
Train: 2018-08-09T11:51:02.125348: step 20568, loss 0.529193.
Train: 2018-08-09T11:51:02.219077: step 20569, loss 0.563151.
Train: 2018-08-09T11:51:02.328425: step 20570, loss 0.565058.
Test: 2018-08-09T11:51:02.828279: step 20570, loss 0.552685.
Train: 2018-08-09T11:51:02.937627: step 20571, loss 0.542857.
Train: 2018-08-09T11:51:03.031388: step 20572, loss 0.480324.
Train: 2018-08-09T11:51:03.140734: step 20573, loss 0.412141.
Train: 2018-08-09T11:51:03.234431: step 20574, loss 0.54253.
Train: 2018-08-09T11:51:03.343781: step 20575, loss 0.537551.
Train: 2018-08-09T11:51:03.453160: step 20576, loss 0.528957.
Train: 2018-08-09T11:51:03.546888: step 20577, loss 0.547694.
Train: 2018-08-09T11:51:03.656208: step 20578, loss 0.61078.
Train: 2018-08-09T11:51:03.749934: step 20579, loss 0.440482.
Train: 2018-08-09T11:51:03.869621: step 20580, loss 0.604711.
Test: 2018-08-09T11:51:04.364559: step 20580, loss 0.548966.
Train: 2018-08-09T11:51:04.458288: step 20581, loss 0.487915.
Train: 2018-08-09T11:51:04.567638: step 20582, loss 0.625556.
Train: 2018-08-09T11:51:04.676984: step 20583, loss 0.508142.
Train: 2018-08-09T11:51:04.770712: step 20584, loss 0.507393.
Train: 2018-08-09T11:51:04.880062: step 20585, loss 0.559753.
Train: 2018-08-09T11:51:04.973791: step 20586, loss 0.616249.
Train: 2018-08-09T11:51:05.083142: step 20587, loss 0.523343.
Train: 2018-08-09T11:51:05.176867: step 20588, loss 0.712161.
Train: 2018-08-09T11:51:05.286216: step 20589, loss 0.563175.
Train: 2018-08-09T11:51:05.379944: step 20590, loss 0.544133.
Test: 2018-08-09T11:51:05.879816: step 20590, loss 0.54982.
Train: 2018-08-09T11:51:05.989145: step 20591, loss 0.628247.
Train: 2018-08-09T11:51:06.082905: step 20592, loss 0.444526.
Train: 2018-08-09T11:51:06.192255: step 20593, loss 0.496402.
Train: 2018-08-09T11:51:06.301601: step 20594, loss 0.614185.
Train: 2018-08-09T11:51:06.395330: step 20595, loss 0.612907.
Train: 2018-08-09T11:51:06.504679: step 20596, loss 0.560722.
Train: 2018-08-09T11:51:06.614031: step 20597, loss 0.611894.
Train: 2018-08-09T11:51:06.707758: step 20598, loss 0.545979.
Train: 2018-08-09T11:51:06.817107: step 20599, loss 0.578945.
Train: 2018-08-09T11:51:06.910833: step 20600, loss 0.546906.
Test: 2018-08-09T11:51:07.410716: step 20600, loss 0.550762.
Train: 2018-08-09T11:51:07.975554: step 20601, loss 0.67508.
Train: 2018-08-09T11:51:08.069279: step 20602, loss 0.562538.
Train: 2018-08-09T11:51:08.178598: step 20603, loss 0.562874.
Train: 2018-08-09T11:51:08.287977: step 20604, loss 0.563476.
Train: 2018-08-09T11:51:08.381705: step 20605, loss 0.611154.
Train: 2018-08-09T11:51:08.491049: step 20606, loss 0.563934.
Train: 2018-08-09T11:51:08.584783: step 20607, loss 0.531437.
Train: 2018-08-09T11:51:08.694134: step 20608, loss 0.59431.
Train: 2018-08-09T11:51:08.787830: step 20609, loss 0.532428.
Train: 2018-08-09T11:51:08.897179: step 20610, loss 0.579225.
Test: 2018-08-09T11:51:09.381441: step 20610, loss 0.550689.
Train: 2018-08-09T11:51:09.490819: step 20611, loss 0.54765.
Train: 2018-08-09T11:51:09.600172: step 20612, loss 0.547779.
Train: 2018-08-09T11:51:09.693896: step 20613, loss 0.656437.
Train: 2018-08-09T11:51:09.803246: step 20614, loss 0.64075.
Train: 2018-08-09T11:51:09.896974: step 20615, loss 0.532981.
Train: 2018-08-09T11:51:10.008756: step 20616, loss 0.578907.
Train: 2018-08-09T11:51:10.102485: step 20617, loss 0.594335.
Train: 2018-08-09T11:51:10.211835: step 20618, loss 0.532996.
Train: 2018-08-09T11:51:10.305532: step 20619, loss 0.502197.
Train: 2018-08-09T11:51:10.414911: step 20620, loss 0.60947.
Test: 2018-08-09T11:51:10.911684: step 20620, loss 0.549795.
Train: 2018-08-09T11:51:11.005413: step 20621, loss 0.578973.
Train: 2018-08-09T11:51:11.099170: step 20622, loss 0.532801.
Train: 2018-08-09T11:51:11.208519: step 20623, loss 0.532866.
Train: 2018-08-09T11:51:11.302247: step 20624, loss 0.594396.
Train: 2018-08-09T11:51:11.411596: step 20625, loss 0.516795.
Train: 2018-08-09T11:51:11.520945: step 20626, loss 0.54814.
Train: 2018-08-09T11:51:11.614676: step 20627, loss 0.625674.
Train: 2018-08-09T11:51:11.724022: step 20628, loss 0.547439.
Train: 2018-08-09T11:51:11.817750: step 20629, loss 0.609747.
Train: 2018-08-09T11:51:11.916984: step 20630, loss 0.626924.
Test: 2018-08-09T11:51:12.416867: step 20630, loss 0.551954.
Train: 2018-08-09T11:51:12.526249: step 20631, loss 0.516258.
Train: 2018-08-09T11:51:12.619974: step 20632, loss 0.626638.
Train: 2018-08-09T11:51:12.729324: step 20633, loss 0.688368.
Train: 2018-08-09T11:51:12.823051: step 20634, loss 0.59502.
Train: 2018-08-09T11:51:12.932401: step 20635, loss 0.563403.
Train: 2018-08-09T11:51:13.026131: step 20636, loss 0.563824.
Train: 2018-08-09T11:51:13.135478: step 20637, loss 0.624994.
Train: 2018-08-09T11:51:13.229206: step 20638, loss 0.578993.
Train: 2018-08-09T11:51:13.338556: step 20639, loss 0.594493.
Train: 2018-08-09T11:51:13.432252: step 20640, loss 0.594174.
Test: 2018-08-09T11:51:13.932137: step 20640, loss 0.554208.
Train: 2018-08-09T11:51:14.025895: step 20641, loss 0.533448.
Train: 2018-08-09T11:51:14.135242: step 20642, loss 0.579083.
Train: 2018-08-09T11:51:14.228970: step 20643, loss 0.518484.
Train: 2018-08-09T11:51:14.338320: step 20644, loss 0.54877.
Train: 2018-08-09T11:51:14.432047: step 20645, loss 0.488117.
Train: 2018-08-09T11:51:14.541396: step 20646, loss 0.594352.
Train: 2018-08-09T11:51:14.635094: step 20647, loss 0.578987.
Train: 2018-08-09T11:51:14.744474: step 20648, loss 0.502704.
Train: 2018-08-09T11:51:14.838201: step 20649, loss 0.563653.
Train: 2018-08-09T11:51:14.933318: step 20650, loss 0.60971.
Test: 2018-08-09T11:51:15.433171: step 20650, loss 0.551372.
Train: 2018-08-09T11:51:15.542543: step 20651, loss 0.547896.
Train: 2018-08-09T11:51:15.636246: step 20652, loss 0.53196.
Train: 2018-08-09T11:51:15.745621: step 20653, loss 0.547584.
Train: 2018-08-09T11:51:15.839355: step 20654, loss 0.467721.
Train: 2018-08-09T11:51:15.948705: step 20655, loss 0.592846.
Train: 2018-08-09T11:51:16.042432: step 20656, loss 0.497943.
Train: 2018-08-09T11:51:16.136159: step 20657, loss 0.633102.
Train: 2018-08-09T11:51:16.245509: step 20658, loss 0.520971.
Train: 2018-08-09T11:51:16.339237: step 20659, loss 0.527579.
Train: 2018-08-09T11:51:16.448587: step 20660, loss 0.547362.
Test: 2018-08-09T11:51:16.949798: step 20660, loss 0.548095.
Train: 2018-08-09T11:51:17.043525: step 20661, loss 0.609639.
Train: 2018-08-09T11:51:17.137282: step 20662, loss 0.561735.
Train: 2018-08-09T11:51:17.246601: step 20663, loss 0.50748.
Train: 2018-08-09T11:51:17.340360: step 20664, loss 0.547783.
Train: 2018-08-09T11:51:17.434088: step 20665, loss 0.457609.
Train: 2018-08-09T11:51:17.543437: step 20666, loss 0.595667.
Train: 2018-08-09T11:51:17.637168: step 20667, loss 0.579411.
Train: 2018-08-09T11:51:17.730895: step 20668, loss 0.560396.
Train: 2018-08-09T11:51:17.840213: step 20669, loss 0.477071.
Train: 2018-08-09T11:51:17.933941: step 20670, loss 0.580653.
Test: 2018-08-09T11:51:18.433822: step 20670, loss 0.548442.
Train: 2018-08-09T11:51:18.527580: step 20671, loss 0.587993.
Train: 2018-08-09T11:51:18.636900: step 20672, loss 0.595367.
Train: 2018-08-09T11:51:18.730660: step 20673, loss 0.494151.
Train: 2018-08-09T11:51:18.839977: step 20674, loss 0.564846.
Train: 2018-08-09T11:51:18.935062: step 20675, loss 0.495575.
Train: 2018-08-09T11:51:19.028789: step 20676, loss 0.565039.
Train: 2018-08-09T11:51:19.138137: step 20677, loss 0.543953.
Train: 2018-08-09T11:51:19.231866: step 20678, loss 0.512874.
Train: 2018-08-09T11:51:19.325595: step 20679, loss 0.510748.
Train: 2018-08-09T11:51:19.434943: step 20680, loss 0.578721.
Test: 2018-08-09T11:51:19.919177: step 20680, loss 0.54618.
Train: 2018-08-09T11:51:20.028523: step 20681, loss 0.474641.
Train: 2018-08-09T11:51:20.122251: step 20682, loss 0.599572.
Train: 2018-08-09T11:51:20.216012: step 20683, loss 0.511832.
Train: 2018-08-09T11:51:20.325360: step 20684, loss 0.577633.
Train: 2018-08-09T11:51:20.419086: step 20685, loss 0.619289.
Train: 2018-08-09T11:51:20.512813: step 20686, loss 0.493468.
Train: 2018-08-09T11:51:20.622133: step 20687, loss 0.565654.
Train: 2018-08-09T11:51:20.715894: step 20688, loss 0.545303.
Train: 2018-08-09T11:51:20.809623: step 20689, loss 0.615498.
Train: 2018-08-09T11:51:20.920303: step 20690, loss 0.529723.
Test: 2018-08-09T11:51:21.404553: step 20690, loss 0.548766.
Train: 2018-08-09T11:51:21.513931: step 20691, loss 0.580204.
Train: 2018-08-09T11:51:21.607658: step 20692, loss 0.595717.
Train: 2018-08-09T11:51:21.701386: step 20693, loss 0.48036.
Train: 2018-08-09T11:51:21.810735: step 20694, loss 0.513036.
Train: 2018-08-09T11:51:21.904463: step 20695, loss 0.61302.
Train: 2018-08-09T11:51:21.998193: step 20696, loss 0.596897.
Train: 2018-08-09T11:51:22.107510: step 20697, loss 0.512813.
Train: 2018-08-09T11:51:22.201268: step 20698, loss 0.545602.
Train: 2018-08-09T11:51:22.294997: step 20699, loss 0.546556.
Train: 2018-08-09T11:51:22.404316: step 20700, loss 0.594349.
Test: 2018-08-09T11:51:22.904198: step 20700, loss 0.552164.
Train: 2018-08-09T11:51:23.499397: step 20701, loss 0.513975.
Train: 2018-08-09T11:51:23.593095: step 20702, loss 0.463885.
Train: 2018-08-09T11:51:23.686852: step 20703, loss 0.480593.
Train: 2018-08-09T11:51:23.796202: step 20704, loss 0.545412.
Train: 2018-08-09T11:51:23.889899: step 20705, loss 0.579472.
Train: 2018-08-09T11:51:23.983627: step 20706, loss 0.579187.
Train: 2018-08-09T11:51:24.077385: step 20707, loss 0.57792.
Train: 2018-08-09T11:51:24.186735: step 20708, loss 0.57753.
Train: 2018-08-09T11:51:24.280463: step 20709, loss 0.592666.
Train: 2018-08-09T11:51:24.374191: step 20710, loss 0.421739.
Test: 2018-08-09T11:51:24.874043: step 20710, loss 0.547755.
Train: 2018-08-09T11:51:24.968441: step 20711, loss 0.658854.
Train: 2018-08-09T11:51:25.077791: step 20712, loss 0.51202.
Train: 2018-08-09T11:51:25.171518: step 20713, loss 0.599474.
Train: 2018-08-09T11:51:25.265246: step 20714, loss 0.559078.
Train: 2018-08-09T11:51:25.358973: step 20715, loss 0.583782.
Train: 2018-08-09T11:51:25.452702: step 20716, loss 0.547329.
Train: 2018-08-09T11:51:25.562050: step 20717, loss 0.563063.
Train: 2018-08-09T11:51:25.655780: step 20718, loss 0.630071.
Train: 2018-08-09T11:51:25.749506: step 20719, loss 0.545379.
Train: 2018-08-09T11:51:25.843205: step 20720, loss 0.613658.
Test: 2018-08-09T11:51:26.343088: step 20720, loss 0.548172.
Train: 2018-08-09T11:51:26.436815: step 20721, loss 0.512851.
Train: 2018-08-09T11:51:26.546194: step 20722, loss 0.447581.
Train: 2018-08-09T11:51:26.639922: step 20723, loss 0.546745.
Train: 2018-08-09T11:51:26.733650: step 20724, loss 0.561361.
Train: 2018-08-09T11:51:26.827378: step 20725, loss 0.612505.
Train: 2018-08-09T11:51:26.921106: step 20726, loss 0.545988.
Train: 2018-08-09T11:51:27.030455: step 20727, loss 0.545913.
Train: 2018-08-09T11:51:27.124153: step 20728, loss 0.547028.
Train: 2018-08-09T11:51:27.217911: step 20729, loss 0.628932.
Train: 2018-08-09T11:51:27.311643: step 20730, loss 0.578676.
Test: 2018-08-09T11:51:27.811492: step 20730, loss 0.549809.
Train: 2018-08-09T11:51:27.907578: step 20731, loss 0.513766.
Train: 2018-08-09T11:51:28.016929: step 20732, loss 0.579024.
Train: 2018-08-09T11:51:28.110686: step 20733, loss 0.578885.
Train: 2018-08-09T11:51:28.204414: step 20734, loss 0.545967.
Train: 2018-08-09T11:51:28.298111: step 20735, loss 0.384148.
Train: 2018-08-09T11:51:28.391840: step 20736, loss 0.660685.
Train: 2018-08-09T11:51:28.501219: step 20737, loss 0.595257.
Train: 2018-08-09T11:51:28.594947: step 20738, loss 0.497886.
Train: 2018-08-09T11:51:28.688675: step 20739, loss 0.627549.
Train: 2018-08-09T11:51:28.782404: step 20740, loss 0.546145.
Test: 2018-08-09T11:51:29.282257: step 20740, loss 0.548521.
Train: 2018-08-09T11:51:29.376013: step 20741, loss 0.480674.
Train: 2018-08-09T11:51:29.469711: step 20742, loss 0.54599.
Train: 2018-08-09T11:51:29.563469: step 20743, loss 0.546085.
Train: 2018-08-09T11:51:29.672820: step 20744, loss 0.594455.
Train: 2018-08-09T11:51:29.766548: step 20745, loss 0.56216.
Train: 2018-08-09T11:51:29.854276: step 20746, loss 0.562735.
Train: 2018-08-09T11:51:29.963625: step 20747, loss 0.579553.
Train: 2018-08-09T11:51:30.053758: step 20748, loss 0.596346.
Train: 2018-08-09T11:51:30.147484: step 20749, loss 0.561975.
Train: 2018-08-09T11:51:30.241212: step 20750, loss 0.62837.
Test: 2018-08-09T11:51:30.756686: step 20750, loss 0.548874.
Train: 2018-08-09T11:51:30.850443: step 20751, loss 0.412829.
Train: 2018-08-09T11:51:30.944172: step 20752, loss 0.578367.
Train: 2018-08-09T11:51:31.037899: step 20753, loss 0.512128.
Train: 2018-08-09T11:51:31.131629: step 20754, loss 0.444951.
Train: 2018-08-09T11:51:31.225325: step 20755, loss 0.580329.
Train: 2018-08-09T11:51:31.334698: step 20756, loss 0.513361.
Train: 2018-08-09T11:51:31.428432: step 20757, loss 0.646324.
Train: 2018-08-09T11:51:31.522161: step 20758, loss 0.514827.
Train: 2018-08-09T11:51:31.615888: step 20759, loss 0.579607.
Train: 2018-08-09T11:51:31.709616: step 20760, loss 0.614346.
Test: 2018-08-09T11:51:32.214397: step 20760, loss 0.5458.
Train: 2018-08-09T11:51:32.308123: step 20761, loss 0.526896.
Train: 2018-08-09T11:51:32.448716: step 20762, loss 0.63126.
Train: 2018-08-09T11:51:32.558064: step 20763, loss 0.581141.
Train: 2018-08-09T11:51:32.651822: step 20764, loss 0.54604.
Train: 2018-08-09T11:51:32.745550: step 20765, loss 0.529218.
Train: 2018-08-09T11:51:32.839247: step 20766, loss 0.52887.
Train: 2018-08-09T11:51:32.933006: step 20767, loss 0.661655.
Train: 2018-08-09T11:51:33.026734: step 20768, loss 0.629546.
Train: 2018-08-09T11:51:33.120462: step 20769, loss 0.633192.
Train: 2018-08-09T11:51:33.229781: step 20770, loss 0.579231.
Test: 2018-08-09T11:51:33.714042: step 20770, loss 0.550894.
Train: 2018-08-09T11:51:33.823424: step 20771, loss 0.627991.
Train: 2018-08-09T11:51:33.917149: step 20772, loss 0.546233.
Train: 2018-08-09T11:51:34.010895: step 20773, loss 0.448709.
Train: 2018-08-09T11:51:34.104604: step 20774, loss 0.594951.
Train: 2018-08-09T11:51:34.198336: step 20775, loss 0.627496.
Train: 2018-08-09T11:51:34.292061: step 20776, loss 0.595023.
Train: 2018-08-09T11:51:34.385758: step 20777, loss 0.546668.
Train: 2018-08-09T11:51:34.479516: step 20778, loss 0.627015.
Train: 2018-08-09T11:51:34.588835: step 20779, loss 0.514762.
Train: 2018-08-09T11:51:34.682593: step 20780, loss 0.626863.
Test: 2018-08-09T11:51:35.168180: step 20780, loss 0.55098.
Train: 2018-08-09T11:51:35.277557: step 20781, loss 0.530998.
Train: 2018-08-09T11:51:35.371287: step 20782, loss 0.547036.
Train: 2018-08-09T11:51:35.464984: step 20783, loss 0.562972.
Train: 2018-08-09T11:51:35.558741: step 20784, loss 0.515326.
Train: 2018-08-09T11:51:35.652469: step 20785, loss 0.594777.
Train: 2018-08-09T11:51:35.746197: step 20786, loss 0.547089.
Train: 2018-08-09T11:51:35.839896: step 20787, loss 0.610588.
Train: 2018-08-09T11:51:35.933624: step 20788, loss 0.547073.
Train: 2018-08-09T11:51:36.043002: step 20789, loss 0.594723.
Train: 2018-08-09T11:51:36.136716: step 20790, loss 0.626475.
Test: 2018-08-09T11:51:36.636583: step 20790, loss 0.548768.
Train: 2018-08-09T11:51:36.730311: step 20791, loss 0.547128.
Train: 2018-08-09T11:51:36.824068: step 20792, loss 0.531435.
Train: 2018-08-09T11:51:36.919206: step 20793, loss 0.467994.
Train: 2018-08-09T11:51:37.012968: step 20794, loss 0.515459.
Train: 2018-08-09T11:51:37.106692: step 20795, loss 0.515429.
Train: 2018-08-09T11:51:37.200391: step 20796, loss 0.546279.
Train: 2018-08-09T11:51:37.294118: step 20797, loss 0.596196.
Train: 2018-08-09T11:51:37.387879: step 20798, loss 0.545944.
Train: 2018-08-09T11:51:37.481605: step 20799, loss 0.545682.
Train: 2018-08-09T11:51:37.590953: step 20800, loss 0.513574.
Test: 2018-08-09T11:51:38.090836: step 20800, loss 0.546899.
Train: 2018-08-09T11:51:38.621960: step 20801, loss 0.54586.
Train: 2018-08-09T11:51:38.715688: step 20802, loss 0.576097.
Train: 2018-08-09T11:51:38.809416: step 20803, loss 0.697656.
Train: 2018-08-09T11:51:38.903144: step 20804, loss 0.562968.
Train: 2018-08-09T11:51:38.999188: step 20805, loss 0.579079.
Train: 2018-08-09T11:51:39.108535: step 20806, loss 0.564672.
Train: 2018-08-09T11:51:39.202235: step 20807, loss 0.561416.
Train: 2018-08-09T11:51:39.295963: step 20808, loss 0.624139.
Train: 2018-08-09T11:51:39.389721: step 20809, loss 0.711942.
Train: 2018-08-09T11:51:39.483451: step 20810, loss 0.513596.
Test: 2018-08-09T11:51:39.983333: step 20810, loss 0.546745.
Train: 2018-08-09T11:51:40.077059: step 20811, loss 0.562858.
Train: 2018-08-09T11:51:40.170757: step 20812, loss 0.529512.
Train: 2018-08-09T11:51:40.264515: step 20813, loss 0.628425.
Train: 2018-08-09T11:51:40.358245: step 20814, loss 0.547373.
Train: 2018-08-09T11:51:40.467592: step 20815, loss 0.547447.
Train: 2018-08-09T11:51:40.561320: step 20816, loss 0.61075.
Train: 2018-08-09T11:51:40.655048: step 20817, loss 0.626754.
Train: 2018-08-09T11:51:40.748777: step 20818, loss 0.562691.
Train: 2018-08-09T11:51:40.842474: step 20819, loss 0.5631.
Train: 2018-08-09T11:51:40.936960: step 20820, loss 0.500561.
Test: 2018-08-09T11:51:41.436814: step 20820, loss 0.549159.
Train: 2018-08-09T11:51:41.530541: step 20821, loss 0.469321.
Train: 2018-08-09T11:51:41.624299: step 20822, loss 0.516211.
Train: 2018-08-09T11:51:41.733618: step 20823, loss 0.594592.
Train: 2018-08-09T11:51:41.827376: step 20824, loss 0.594578.
Train: 2018-08-09T11:51:41.921103: step 20825, loss 0.594577.
Train: 2018-08-09T11:51:42.014831: step 20826, loss 0.594617.
Train: 2018-08-09T11:51:42.108529: step 20827, loss 0.657516.
Train: 2018-08-09T11:51:42.202287: step 20828, loss 0.610302.
Train: 2018-08-09T11:51:42.296015: step 20829, loss 0.610235.
Train: 2018-08-09T11:51:42.405364: step 20830, loss 0.672735.
Test: 2018-08-09T11:51:42.905218: step 20830, loss 0.551091.
Train: 2018-08-09T11:51:42.998974: step 20831, loss 0.547755.
Train: 2018-08-09T11:51:43.092706: step 20832, loss 0.439203.
Train: 2018-08-09T11:51:43.186430: step 20833, loss 0.640978.
Train: 2018-08-09T11:51:43.280158: step 20834, loss 0.51696.
Train: 2018-08-09T11:51:43.373886: step 20835, loss 0.594409.
Train: 2018-08-09T11:51:43.484423: step 20836, loss 0.47063.
Train: 2018-08-09T11:51:43.568963: step 20837, loss 0.501477.
Train: 2018-08-09T11:51:43.662690: step 20838, loss 0.532344.
Train: 2018-08-09T11:51:43.756421: step 20839, loss 0.578903.
Train: 2018-08-09T11:51:43.865739: step 20840, loss 0.563278.
Test: 2018-08-09T11:51:44.365621: step 20840, loss 0.551543.
Train: 2018-08-09T11:51:44.459348: step 20841, loss 0.641448.
Train: 2018-08-09T11:51:44.553106: step 20842, loss 0.594532.
Train: 2018-08-09T11:51:44.646805: step 20843, loss 0.563224.
Train: 2018-08-09T11:51:44.740562: step 20844, loss 0.484851.
Train: 2018-08-09T11:51:44.849882: step 20845, loss 0.641693.
Train: 2018-08-09T11:51:44.943639: step 20846, loss 0.547454.
Train: 2018-08-09T11:51:45.037367: step 20847, loss 0.48452.
Train: 2018-08-09T11:51:45.131094: step 20848, loss 0.499975.
Train: 2018-08-09T11:51:45.224826: step 20849, loss 0.64228.
Train: 2018-08-09T11:51:45.318551: step 20850, loss 0.626393.
Test: 2018-08-09T11:51:45.818434: step 20850, loss 0.549873.
Train: 2018-08-09T11:51:45.913419: step 20851, loss 0.562951.
Train: 2018-08-09T11:51:46.022798: step 20852, loss 0.515385.
Train: 2018-08-09T11:51:46.116528: step 20853, loss 0.451223.
Train: 2018-08-09T11:51:46.210254: step 20854, loss 0.562483.
Train: 2018-08-09T11:51:46.303982: step 20855, loss 0.61117.
Train: 2018-08-09T11:51:46.397681: step 20856, loss 0.595158.
Train: 2018-08-09T11:51:46.507060: step 20857, loss 0.546637.
Train: 2018-08-09T11:51:46.600788: step 20858, loss 0.562813.
Train: 2018-08-09T11:51:46.694485: step 20859, loss 0.511516.
Train: 2018-08-09T11:51:46.788243: step 20860, loss 0.547566.
Test: 2018-08-09T11:51:47.288096: step 20860, loss 0.549135.
Train: 2018-08-09T11:51:47.428721: step 20861, loss 0.562344.
Train: 2018-08-09T11:51:47.538073: step 20862, loss 0.596194.
Train: 2018-08-09T11:51:47.631795: step 20863, loss 0.563297.
Train: 2018-08-09T11:51:47.725523: step 20864, loss 0.508911.
Train: 2018-08-09T11:51:47.819221: step 20865, loss 0.544349.
Train: 2018-08-09T11:51:47.915233: step 20866, loss 0.530443.
Train: 2018-08-09T11:51:48.008962: step 20867, loss 0.578596.
Train: 2018-08-09T11:51:48.102720: step 20868, loss 0.647554.
Train: 2018-08-09T11:51:48.212038: step 20869, loss 0.502713.
Train: 2018-08-09T11:51:48.305800: step 20870, loss 0.615458.
Test: 2018-08-09T11:51:48.805649: step 20870, loss 0.547103.
Train: 2018-08-09T11:51:48.899377: step 20871, loss 0.600154.
Train: 2018-08-09T11:51:48.993105: step 20872, loss 0.547756.
Train: 2018-08-09T11:51:49.086862: step 20873, loss 0.563212.
Train: 2018-08-09T11:51:49.196213: step 20874, loss 0.568674.
Train: 2018-08-09T11:51:49.289940: step 20875, loss 0.616596.
Train: 2018-08-09T11:51:49.383638: step 20876, loss 0.525742.
Train: 2018-08-09T11:51:49.477395: step 20877, loss 0.52897.
Train: 2018-08-09T11:51:49.586746: step 20878, loss 0.616167.
Train: 2018-08-09T11:51:49.680477: step 20879, loss 0.54683.
Train: 2018-08-09T11:51:49.774201: step 20880, loss 0.562547.
Test: 2018-08-09T11:51:50.275429: step 20880, loss 0.547493.
Train: 2018-08-09T11:51:50.369186: step 20881, loss 0.642805.
Train: 2018-08-09T11:51:50.462913: step 20882, loss 0.578713.
Train: 2018-08-09T11:51:50.572234: step 20883, loss 0.50065.
Train: 2018-08-09T11:51:50.665960: step 20884, loss 0.657924.
Train: 2018-08-09T11:51:50.759719: step 20885, loss 0.515569.
Train: 2018-08-09T11:51:50.853447: step 20886, loss 0.56341.
Train: 2018-08-09T11:51:50.962796: step 20887, loss 0.54751.
Train: 2018-08-09T11:51:51.056524: step 20888, loss 0.578923.
Train: 2018-08-09T11:51:51.150222: step 20889, loss 0.594465.
Train: 2018-08-09T11:51:51.243980: step 20890, loss 0.469763.
Test: 2018-08-09T11:51:51.743866: step 20890, loss 0.549267.
Train: 2018-08-09T11:51:51.853212: step 20891, loss 0.563264.
Train: 2018-08-09T11:51:51.948244: step 20892, loss 0.563312.
Train: 2018-08-09T11:51:52.041969: step 20893, loss 0.594511.
Train: 2018-08-09T11:51:52.135667: step 20894, loss 0.62578.
Train: 2018-08-09T11:51:52.245016: step 20895, loss 0.56326.
Train: 2018-08-09T11:51:52.338778: step 20896, loss 0.547638.
Train: 2018-08-09T11:51:52.432503: step 20897, loss 0.516377.
Train: 2018-08-09T11:51:52.526230: step 20898, loss 0.531886.
Train: 2018-08-09T11:51:52.635574: step 20899, loss 0.610276.
Train: 2018-08-09T11:51:52.729276: step 20900, loss 0.626048.
Test: 2018-08-09T11:51:53.229160: step 20900, loss 0.551424.
Train: 2018-08-09T11:51:53.775935: step 20901, loss 0.563176.
Train: 2018-08-09T11:51:53.885285: step 20902, loss 0.56319.
Train: 2018-08-09T11:51:53.981320: step 20903, loss 0.563242.
Train: 2018-08-09T11:51:54.075048: step 20904, loss 0.610291.
Train: 2018-08-09T11:51:54.168805: step 20905, loss 0.516105.
Train: 2018-08-09T11:51:54.278155: step 20906, loss 0.51583.
Train: 2018-08-09T11:51:54.371885: step 20907, loss 0.610696.
Train: 2018-08-09T11:51:54.465611: step 20908, loss 0.563357.
Train: 2018-08-09T11:51:54.574960: step 20909, loss 0.531349.
Train: 2018-08-09T11:51:54.668663: step 20910, loss 0.531628.
Test: 2018-08-09T11:51:55.168541: step 20910, loss 0.548149.
Train: 2018-08-09T11:51:55.262268: step 20911, loss 0.547083.
Train: 2018-08-09T11:51:55.371616: step 20912, loss 0.51506.
Train: 2018-08-09T11:51:55.465345: step 20913, loss 0.515301.
Train: 2018-08-09T11:51:55.559103: step 20914, loss 0.627739.
Train: 2018-08-09T11:51:55.668452: step 20915, loss 0.546405.
Train: 2018-08-09T11:51:55.762179: step 20916, loss 0.529856.
Train: 2018-08-09T11:51:55.871532: step 20917, loss 0.594239.
Train: 2018-08-09T11:51:55.967494: step 20918, loss 0.544348.
Train: 2018-08-09T11:51:56.061221: step 20919, loss 0.594846.
Train: 2018-08-09T11:51:56.170570: step 20920, loss 0.575887.
Test: 2018-08-09T11:51:56.654834: step 20920, loss 0.547566.
Train: 2018-08-09T11:51:56.764210: step 20921, loss 0.616471.
Train: 2018-08-09T11:51:56.857909: step 20922, loss 0.478578.
Train: 2018-08-09T11:51:56.951666: step 20923, loss 0.440823.
Train: 2018-08-09T11:51:57.060992: step 20924, loss 0.417084.
Train: 2018-08-09T11:51:57.154744: step 20925, loss 0.546349.
Train: 2018-08-09T11:51:57.248471: step 20926, loss 0.571814.
Train: 2018-08-09T11:51:57.357824: step 20927, loss 0.587844.
Train: 2018-08-09T11:51:57.451549: step 20928, loss 0.599666.
Train: 2018-08-09T11:51:57.545276: step 20929, loss 0.509143.
Train: 2018-08-09T11:51:57.654626: step 20930, loss 0.634558.
Test: 2018-08-09T11:51:58.155292: step 20930, loss 0.547765.
Train: 2018-08-09T11:51:58.249049: step 20931, loss 0.469231.
Train: 2018-08-09T11:51:58.358399: step 20932, loss 0.515085.
Train: 2018-08-09T11:51:58.452126: step 20933, loss 0.57789.
Train: 2018-08-09T11:51:58.545854: step 20934, loss 0.595473.
Train: 2018-08-09T11:51:58.655204: step 20935, loss 0.556962.
Train: 2018-08-09T11:51:58.748902: step 20936, loss 0.526067.
Train: 2018-08-09T11:51:58.858251: step 20937, loss 0.532832.
Train: 2018-08-09T11:51:58.952008: step 20938, loss 0.612927.
Train: 2018-08-09T11:51:59.061357: step 20939, loss 0.563994.
Train: 2018-08-09T11:51:59.155085: step 20940, loss 0.601457.
Test: 2018-08-09T11:51:59.654947: step 20940, loss 0.550331.
Train: 2018-08-09T11:51:59.748695: step 20941, loss 0.770102.
Train: 2018-08-09T11:51:59.858045: step 20942, loss 0.563211.
Train: 2018-08-09T11:51:59.951773: step 20943, loss 0.562475.
Train: 2018-08-09T11:52:00.061122: step 20944, loss 0.482996.
Train: 2018-08-09T11:52:00.154849: step 20945, loss 0.626502.
Train: 2018-08-09T11:52:00.248573: step 20946, loss 0.435432.
Train: 2018-08-09T11:52:00.357928: step 20947, loss 0.689859.
Train: 2018-08-09T11:52:00.451655: step 20948, loss 0.578371.
Train: 2018-08-09T11:52:00.561004: step 20949, loss 0.626131.
Train: 2018-08-09T11:52:00.654732: step 20950, loss 0.53153.
Test: 2018-08-09T11:52:01.157156: step 20950, loss 0.550314.
Train: 2018-08-09T11:52:01.250882: step 20951, loss 0.6418.
Train: 2018-08-09T11:52:01.360262: step 20952, loss 0.594241.
Train: 2018-08-09T11:52:01.453991: step 20953, loss 0.56367.
Train: 2018-08-09T11:52:01.563308: step 20954, loss 0.470959.
Train: 2018-08-09T11:52:01.663602: step 20955, loss 0.532175.
Train: 2018-08-09T11:52:01.757329: step 20956, loss 0.532331.
Train: 2018-08-09T11:52:01.866678: step 20957, loss 0.578872.
Train: 2018-08-09T11:52:01.960407: step 20958, loss 0.594414.
Train: 2018-08-09T11:52:02.069756: step 20959, loss 0.62572.
Train: 2018-08-09T11:52:02.163483: step 20960, loss 0.656686.
Test: 2018-08-09T11:52:02.663337: step 20960, loss 0.549913.
Train: 2018-08-09T11:52:02.767764: step 20961, loss 0.594458.
Train: 2018-08-09T11:52:02.861462: step 20962, loss 0.532246.
Train: 2018-08-09T11:52:02.970841: step 20963, loss 0.501356.
Train: 2018-08-09T11:52:03.064571: step 20964, loss 0.547987.
Train: 2018-08-09T11:52:03.173918: step 20965, loss 0.594438.
Train: 2018-08-09T11:52:03.267646: step 20966, loss 0.57877.
Train: 2018-08-09T11:52:03.376966: step 20967, loss 0.501106.
Train: 2018-08-09T11:52:03.470723: step 20968, loss 0.469502.
Train: 2018-08-09T11:52:03.580072: step 20969, loss 0.609002.
Train: 2018-08-09T11:52:03.673770: step 20970, loss 0.611497.
Test: 2018-08-09T11:52:04.173655: step 20970, loss 0.552613.
Train: 2018-08-09T11:52:04.267410: step 20971, loss 0.578612.
Train: 2018-08-09T11:52:04.376761: step 20972, loss 0.659819.
Train: 2018-08-09T11:52:04.470459: step 20973, loss 0.563541.
Train: 2018-08-09T11:52:04.579840: step 20974, loss 0.516553.
Train: 2018-08-09T11:52:04.673567: step 20975, loss 0.594588.
Train: 2018-08-09T11:52:04.782915: step 20976, loss 0.546595.
Train: 2018-08-09T11:52:04.876642: step 20977, loss 0.593436.
Train: 2018-08-09T11:52:04.989646: step 20978, loss 0.609787.
Train: 2018-08-09T11:52:05.082235: step 20979, loss 0.4669.
Train: 2018-08-09T11:52:05.191587: step 20980, loss 0.562924.
Test: 2018-08-09T11:52:05.691436: step 20980, loss 0.549522.
Train: 2018-08-09T11:52:05.785164: step 20981, loss 0.642908.
Train: 2018-08-09T11:52:05.878921: step 20982, loss 0.645816.
Train: 2018-08-09T11:52:05.988270: step 20983, loss 0.501451.
Train: 2018-08-09T11:52:06.097619: step 20984, loss 0.513072.
Train: 2018-08-09T11:52:06.191347: step 20985, loss 0.544411.
Train: 2018-08-09T11:52:06.300697: step 20986, loss 0.559405.
Train: 2018-08-09T11:52:06.394394: step 20987, loss 0.463286.
Train: 2018-08-09T11:52:06.503775: step 20988, loss 0.544177.
Train: 2018-08-09T11:52:06.597503: step 20989, loss 0.553164.
Train: 2018-08-09T11:52:06.706851: step 20990, loss 0.54969.
Test: 2018-08-09T11:52:07.193370: step 20990, loss 0.549149.
Train: 2018-08-09T11:52:07.302719: step 20991, loss 0.59517.
Train: 2018-08-09T11:52:07.396476: step 20992, loss 0.589782.
Train: 2018-08-09T11:52:07.505826: step 20993, loss 0.528917.
Train: 2018-08-09T11:52:07.615169: step 20994, loss 0.57379.
Train: 2018-08-09T11:52:07.708873: step 20995, loss 0.618028.
Train: 2018-08-09T11:52:07.818252: step 20996, loss 0.566609.
Train: 2018-08-09T11:52:07.927601: step 20997, loss 0.616043.
Train: 2018-08-09T11:52:08.021325: step 20998, loss 0.513574.
Train: 2018-08-09T11:52:08.146300: step 20999, loss 0.481947.
Train: 2018-08-09T11:52:08.240028: step 21000, loss 0.551188.
Test: 2018-08-09T11:52:08.739880: step 21000, loss 0.550452.
Train: 2018-08-09T11:52:09.318402: step 21001, loss 0.54354.
Train: 2018-08-09T11:52:09.427753: step 21002, loss 0.493089.
Train: 2018-08-09T11:52:09.537101: step 21003, loss 0.496003.
Train: 2018-08-09T11:52:09.630831: step 21004, loss 0.559707.
Train: 2018-08-09T11:52:09.740179: step 21005, loss 0.597928.
Train: 2018-08-09T11:52:09.833904: step 21006, loss 0.494167.
Train: 2018-08-09T11:52:09.943255: step 21007, loss 0.637914.
Train: 2018-08-09T11:52:10.052576: step 21008, loss 0.506004.
Train: 2018-08-09T11:52:10.146335: step 21009, loss 0.578937.
Train: 2018-08-09T11:52:10.255679: step 21010, loss 0.559862.
Test: 2018-08-09T11:52:10.755536: step 21010, loss 0.546281.
Train: 2018-08-09T11:52:10.849291: step 21011, loss 0.443019.
Train: 2018-08-09T11:52:10.960194: step 21012, loss 0.516003.
Train: 2018-08-09T11:52:11.069511: step 21013, loss 0.489542.
Train: 2018-08-09T11:52:11.163238: step 21014, loss 0.545484.
Train: 2018-08-09T11:52:11.272620: step 21015, loss 0.452373.
Train: 2018-08-09T11:52:11.381942: step 21016, loss 0.554007.
Train: 2018-08-09T11:52:11.475694: step 21017, loss 0.565564.
Train: 2018-08-09T11:52:11.585044: step 21018, loss 0.668748.
Train: 2018-08-09T11:52:11.694393: step 21019, loss 0.417807.
Train: 2018-08-09T11:52:11.788121: step 21020, loss 0.722843.
Test: 2018-08-09T11:52:12.287974: step 21020, loss 0.547133.
Train: 2018-08-09T11:52:12.397353: step 21021, loss 0.487972.
Train: 2018-08-09T11:52:12.506701: step 21022, loss 0.513373.
Train: 2018-08-09T11:52:12.600434: step 21023, loss 0.592617.
Train: 2018-08-09T11:52:12.709749: step 21024, loss 0.4937.
Train: 2018-08-09T11:52:12.819128: step 21025, loss 0.643207.
Train: 2018-08-09T11:52:12.914377: step 21026, loss 0.611036.
Train: 2018-08-09T11:52:13.023758: step 21027, loss 0.539813.
Train: 2018-08-09T11:52:13.133102: step 21028, loss 0.54662.
Train: 2018-08-09T11:52:13.226803: step 21029, loss 0.515393.
Train: 2018-08-09T11:52:13.336152: step 21030, loss 0.511789.
Test: 2018-08-09T11:52:13.836037: step 21030, loss 0.550339.
Train: 2018-08-09T11:52:13.929764: step 21031, loss 0.562933.
Train: 2018-08-09T11:52:14.039113: step 21032, loss 0.426018.
Train: 2018-08-09T11:52:14.148462: step 21033, loss 0.545586.
Train: 2018-08-09T11:52:14.242219: step 21034, loss 0.560623.
Train: 2018-08-09T11:52:14.351569: step 21035, loss 0.565658.
Train: 2018-08-09T11:52:14.460918: step 21036, loss 0.543039.
Train: 2018-08-09T11:52:14.554616: step 21037, loss 0.600626.
Train: 2018-08-09T11:52:14.663966: step 21038, loss 0.480457.
Train: 2018-08-09T11:52:14.773315: step 21039, loss 0.546471.
Train: 2018-08-09T11:52:14.867043: step 21040, loss 0.477534.
Test: 2018-08-09T11:52:15.369169: step 21040, loss 0.548254.
Train: 2018-08-09T11:52:15.462897: step 21041, loss 0.595823.
Train: 2018-08-09T11:52:15.572245: step 21042, loss 0.683138.
Train: 2018-08-09T11:52:15.681625: step 21043, loss 0.526746.
Train: 2018-08-09T11:52:15.775353: step 21044, loss 0.408857.
Train: 2018-08-09T11:52:15.884702: step 21045, loss 0.509863.
Train: 2018-08-09T11:52:15.994021: step 21046, loss 0.594118.
Train: 2018-08-09T11:52:16.087779: step 21047, loss 0.577885.
Train: 2018-08-09T11:52:16.197100: step 21048, loss 0.597252.
Train: 2018-08-09T11:52:16.306478: step 21049, loss 0.580672.
Train: 2018-08-09T11:52:16.400205: step 21050, loss 0.632321.
Test: 2018-08-09T11:52:16.910651: step 21050, loss 0.547643.
Train: 2018-08-09T11:52:17.010702: step 21051, loss 0.545371.
Train: 2018-08-09T11:52:17.104460: step 21052, loss 0.59782.
Train: 2018-08-09T11:52:17.213779: step 21053, loss 0.475881.
Train: 2018-08-09T11:52:17.323175: step 21054, loss 0.564168.
Train: 2018-08-09T11:52:17.432507: step 21055, loss 0.648429.
Train: 2018-08-09T11:52:17.526206: step 21056, loss 0.59668.
Train: 2018-08-09T11:52:17.635555: step 21057, loss 0.611926.
Train: 2018-08-09T11:52:17.744904: step 21058, loss 0.579986.
Train: 2018-08-09T11:52:17.838662: step 21059, loss 0.498488.
Train: 2018-08-09T11:52:17.948013: step 21060, loss 0.547218.
Test: 2018-08-09T11:52:18.447863: step 21060, loss 0.552072.
Train: 2018-08-09T11:52:18.557242: step 21061, loss 0.644295.
Train: 2018-08-09T11:52:18.650970: step 21062, loss 0.561861.
Train: 2018-08-09T11:52:18.760320: step 21063, loss 0.497139.
Train: 2018-08-09T11:52:18.869639: step 21064, loss 0.546524.
Train: 2018-08-09T11:52:18.978994: step 21065, loss 0.594865.
Train: 2018-08-09T11:52:19.072748: step 21066, loss 0.545589.
Train: 2018-08-09T11:52:19.182095: step 21067, loss 0.577994.
Train: 2018-08-09T11:52:19.291445: step 21068, loss 0.642567.
Train: 2018-08-09T11:52:19.400768: step 21069, loss 0.611395.
Train: 2018-08-09T11:52:19.494522: step 21070, loss 0.581029.
Test: 2018-08-09T11:52:19.996907: step 21070, loss 0.549521.
Train: 2018-08-09T11:52:20.090641: step 21071, loss 0.561997.
Train: 2018-08-09T11:52:20.199989: step 21072, loss 0.547037.
Train: 2018-08-09T11:52:20.309339: step 21073, loss 0.643453.
Train: 2018-08-09T11:52:20.418689: step 21074, loss 0.48339.
Train: 2018-08-09T11:52:20.512417: step 21075, loss 0.531661.
Train: 2018-08-09T11:52:20.621765: step 21076, loss 0.579221.
Train: 2018-08-09T11:52:20.715494: step 21077, loss 0.530973.
Train: 2018-08-09T11:52:20.824842: step 21078, loss 0.563081.
Train: 2018-08-09T11:52:20.934192: step 21079, loss 0.578796.
Train: 2018-08-09T11:52:21.043541: step 21080, loss 0.706383.
Test: 2018-08-09T11:52:21.527773: step 21080, loss 0.549342.
Train: 2018-08-09T11:52:21.643299: step 21081, loss 0.563144.
Train: 2018-08-09T11:52:21.737028: step 21082, loss 0.546814.
Train: 2018-08-09T11:52:21.846377: step 21083, loss 0.579172.
Train: 2018-08-09T11:52:21.950760: step 21084, loss 0.594787.
Train: 2018-08-09T11:52:22.060109: step 21085, loss 0.563206.
Train: 2018-08-09T11:52:22.169461: step 21086, loss 0.547514.
Train: 2018-08-09T11:52:22.278808: step 21087, loss 0.641678.
Train: 2018-08-09T11:52:22.388158: step 21088, loss 0.594903.
Train: 2018-08-09T11:52:22.497509: step 21089, loss 0.469607.
Train: 2018-08-09T11:52:22.591234: step 21090, loss 0.625559.
Test: 2018-08-09T11:52:23.091088: step 21090, loss 0.551663.
Train: 2018-08-09T11:52:23.200436: step 21091, loss 0.563266.
Train: 2018-08-09T11:52:23.294164: step 21092, loss 0.641101.
Train: 2018-08-09T11:52:23.403513: step 21093, loss 0.625458.
Train: 2018-08-09T11:52:23.512862: step 21094, loss 0.563226.
Train: 2018-08-09T11:52:23.622243: step 21095, loss 0.563581.
Train: 2018-08-09T11:52:23.715969: step 21096, loss 0.517168.
Train: 2018-08-09T11:52:23.825319: step 21097, loss 0.59419.
Train: 2018-08-09T11:52:23.934668: step 21098, loss 0.625213.
Train: 2018-08-09T11:52:24.028398: step 21099, loss 0.532763.
Train: 2018-08-09T11:52:24.137746: step 21100, loss 0.578857.
Test: 2018-08-09T11:52:24.637598: step 21100, loss 0.551521.
Train: 2018-08-09T11:52:25.264924: step 21101, loss 0.579109.
Train: 2018-08-09T11:52:25.358652: step 21102, loss 0.609734.
Train: 2018-08-09T11:52:25.467970: step 21103, loss 0.609686.
Train: 2018-08-09T11:52:25.577349: step 21104, loss 0.533016.
Train: 2018-08-09T11:52:25.671047: step 21105, loss 0.578968.
Train: 2018-08-09T11:52:25.780428: step 21106, loss 0.594447.
Train: 2018-08-09T11:52:25.874125: step 21107, loss 0.578969.
Train: 2018-08-09T11:52:25.983504: step 21108, loss 0.533226.
Train: 2018-08-09T11:52:26.092854: step 21109, loss 0.533359.
Train: 2018-08-09T11:52:26.202172: step 21110, loss 0.426192.
Test: 2018-08-09T11:52:26.686435: step 21110, loss 0.54989.
Train: 2018-08-09T11:52:26.795783: step 21111, loss 0.563732.
Train: 2018-08-09T11:52:26.907382: step 21112, loss 0.609906.
Train: 2018-08-09T11:52:26.999578: step 21113, loss 0.609777.
Train: 2018-08-09T11:52:27.108928: step 21114, loss 0.424573.
Train: 2018-08-09T11:52:27.218277: step 21115, loss 0.609741.
Train: 2018-08-09T11:52:27.327627: step 21116, loss 0.609789.
Train: 2018-08-09T11:52:27.421354: step 21117, loss 0.610202.
Train: 2018-08-09T11:52:27.530703: step 21118, loss 0.578789.
Train: 2018-08-09T11:52:27.640052: step 21119, loss 0.547808.
Train: 2018-08-09T11:52:27.733780: step 21120, loss 0.594148.
Test: 2018-08-09T11:52:28.233634: step 21120, loss 0.550848.
Train: 2018-08-09T11:52:28.342982: step 21121, loss 0.626097.
Train: 2018-08-09T11:52:28.452361: step 21122, loss 0.578618.
Train: 2018-08-09T11:52:28.546089: step 21123, loss 0.579374.
Train: 2018-08-09T11:52:28.655438: step 21124, loss 0.562951.
Train: 2018-08-09T11:52:28.764789: step 21125, loss 0.672969.
Train: 2018-08-09T11:52:28.874136: step 21126, loss 0.50044.
Train: 2018-08-09T11:52:28.967864: step 21127, loss 0.594467.
Train: 2018-08-09T11:52:29.077214: step 21128, loss 0.562951.
Train: 2018-08-09T11:52:29.186563: step 21129, loss 0.610376.
Train: 2018-08-09T11:52:29.280291: step 21130, loss 0.563162.
Test: 2018-08-09T11:52:29.780144: step 21130, loss 0.552827.
Train: 2018-08-09T11:52:29.889523: step 21131, loss 0.516556.
Train: 2018-08-09T11:52:30.000256: step 21132, loss 0.563279.
Train: 2018-08-09T11:52:30.093984: step 21133, loss 0.547307.
Train: 2018-08-09T11:52:30.203303: step 21134, loss 0.578168.
Train: 2018-08-09T11:52:30.297031: step 21135, loss 0.594792.
Train: 2018-08-09T11:52:30.406412: step 21136, loss 0.515444.
Train: 2018-08-09T11:52:30.515760: step 21137, loss 0.514828.
Train: 2018-08-09T11:52:30.625078: step 21138, loss 0.627887.
Train: 2018-08-09T11:52:30.718837: step 21139, loss 0.577717.
Train: 2018-08-09T11:52:30.828186: step 21140, loss 0.614387.
Test: 2018-08-09T11:52:31.328040: step 21140, loss 0.549511.
Train: 2018-08-09T11:52:31.421796: step 21141, loss 0.566357.
Train: 2018-08-09T11:52:31.531147: step 21142, loss 0.514572.
Train: 2018-08-09T11:52:31.640495: step 21143, loss 0.581167.
Train: 2018-08-09T11:52:31.734223: step 21144, loss 0.499249.
Train: 2018-08-09T11:52:31.843571: step 21145, loss 0.563773.
Train: 2018-08-09T11:52:31.938814: step 21146, loss 0.563096.
Train: 2018-08-09T11:52:32.048163: step 21147, loss 0.562197.
Train: 2018-08-09T11:52:32.157512: step 21148, loss 0.595235.
Train: 2018-08-09T11:52:32.251240: step 21149, loss 0.531763.
Train: 2018-08-09T11:52:32.360590: step 21150, loss 0.547491.
Test: 2018-08-09T11:52:32.860443: step 21150, loss 0.550001.
Train: 2018-08-09T11:52:32.954169: step 21151, loss 0.515642.
Train: 2018-08-09T11:52:33.063549: step 21152, loss 0.531032.
Train: 2018-08-09T11:52:33.175991: step 21153, loss 0.61074.
Train: 2018-08-09T11:52:33.271173: step 21154, loss 0.627745.
Train: 2018-08-09T11:52:33.380521: step 21155, loss 0.546446.
Train: 2018-08-09T11:52:33.474250: step 21156, loss 0.53103.
Train: 2018-08-09T11:52:33.583599: step 21157, loss 0.531069.
Train: 2018-08-09T11:52:33.692918: step 21158, loss 0.531166.
Train: 2018-08-09T11:52:33.786676: step 21159, loss 0.545781.
Train: 2018-08-09T11:52:33.895995: step 21160, loss 0.480129.
Test: 2018-08-09T11:52:34.395877: step 21160, loss 0.547473.
Train: 2018-08-09T11:52:34.489605: step 21161, loss 0.545619.
Train: 2018-08-09T11:52:34.598985: step 21162, loss 0.491806.
Train: 2018-08-09T11:52:34.708304: step 21163, loss 0.568944.
Train: 2018-08-09T11:52:34.802032: step 21164, loss 0.584594.
Train: 2018-08-09T11:52:34.911413: step 21165, loss 0.539333.
Train: 2018-08-09T11:52:35.020765: step 21166, loss 0.524669.
Train: 2018-08-09T11:52:35.130079: step 21167, loss 0.560058.
Train: 2018-08-09T11:52:35.223837: step 21168, loss 0.524584.
Train: 2018-08-09T11:52:35.333190: step 21169, loss 0.511792.
Train: 2018-08-09T11:52:35.426914: step 21170, loss 0.584097.
Test: 2018-08-09T11:52:35.928434: step 21170, loss 0.54706.
Train: 2018-08-09T11:52:36.037810: step 21171, loss 0.565702.
Train: 2018-08-09T11:52:36.131536: step 21172, loss 0.559955.
Train: 2018-08-09T11:52:36.240887: step 21173, loss 0.43789.
Train: 2018-08-09T11:52:36.334617: step 21174, loss 0.487643.
Train: 2018-08-09T11:52:36.443963: step 21175, loss 0.489374.
Train: 2018-08-09T11:52:36.553312: step 21176, loss 0.591327.
Train: 2018-08-09T11:52:36.647039: step 21177, loss 0.563566.
Train: 2018-08-09T11:52:36.756360: step 21178, loss 0.627482.
Train: 2018-08-09T11:52:36.865709: step 21179, loss 0.632163.
Train: 2018-08-09T11:52:36.959436: step 21180, loss 0.560597.
Test: 2018-08-09T11:52:37.459320: step 21180, loss 0.547386.
Train: 2018-08-09T11:52:37.568668: step 21181, loss 0.490347.
Train: 2018-08-09T11:52:37.662396: step 21182, loss 0.531826.
Train: 2018-08-09T11:52:37.771775: step 21183, loss 0.526492.
Train: 2018-08-09T11:52:37.881094: step 21184, loss 0.596409.
Train: 2018-08-09T11:52:37.976945: step 21185, loss 0.52901.
Train: 2018-08-09T11:52:38.086294: step 21186, loss 0.631305.
Train: 2018-08-09T11:52:38.195643: step 21187, loss 0.528852.
Train: 2018-08-09T11:52:38.289371: step 21188, loss 0.614087.
Train: 2018-08-09T11:52:38.398725: step 21189, loss 0.495149.
Train: 2018-08-09T11:52:38.492449: step 21190, loss 0.528582.
Test: 2018-08-09T11:52:38.992302: step 21190, loss 0.547542.
Train: 2018-08-09T11:52:39.101651: step 21191, loss 0.496259.
Train: 2018-08-09T11:52:39.195378: step 21192, loss 0.612326.
Train: 2018-08-09T11:52:39.304727: step 21193, loss 0.645575.
Train: 2018-08-09T11:52:39.398455: step 21194, loss 0.52935.
Train: 2018-08-09T11:52:39.507805: step 21195, loss 0.430961.
Train: 2018-08-09T11:52:39.617184: step 21196, loss 0.561659.
Train: 2018-08-09T11:52:39.710912: step 21197, loss 0.578311.
Train: 2018-08-09T11:52:39.820261: step 21198, loss 0.59465.
Train: 2018-08-09T11:52:39.913988: step 21199, loss 0.512935.
Train: 2018-08-09T11:52:40.023337: step 21200, loss 0.613896.
Test: 2018-08-09T11:52:40.523200: step 21200, loss 0.550001.
Train: 2018-08-09T11:52:41.087053: step 21201, loss 0.529698.
Train: 2018-08-09T11:52:41.196401: step 21202, loss 0.611931.
Train: 2018-08-09T11:52:41.305750: step 21203, loss 0.463064.
Train: 2018-08-09T11:52:41.399478: step 21204, loss 0.529104.
Train: 2018-08-09T11:52:41.508828: step 21205, loss 0.512705.
Train: 2018-08-09T11:52:41.602564: step 21206, loss 0.595379.
Train: 2018-08-09T11:52:41.711875: step 21207, loss 0.544727.
Train: 2018-08-09T11:52:41.821226: step 21208, loss 0.444421.
Train: 2018-08-09T11:52:41.914981: step 21209, loss 0.528263.
Train: 2018-08-09T11:52:42.024300: step 21210, loss 0.545972.
Test: 2018-08-09T11:52:42.508562: step 21210, loss 0.548932.
Train: 2018-08-09T11:52:42.617942: step 21211, loss 0.479078.
Train: 2018-08-09T11:52:42.727293: step 21212, loss 0.736512.
Train: 2018-08-09T11:52:42.821018: step 21213, loss 0.664707.
Train: 2018-08-09T11:52:42.931915: step 21214, loss 0.547756.
Train: 2018-08-09T11:52:43.025644: step 21215, loss 0.596817.
Train: 2018-08-09T11:52:43.134993: step 21216, loss 0.480015.
Train: 2018-08-09T11:52:43.228690: step 21217, loss 0.546984.
Train: 2018-08-09T11:52:43.338070: step 21218, loss 0.529086.
Train: 2018-08-09T11:52:43.431798: step 21219, loss 0.596184.
Train: 2018-08-09T11:52:43.541116: step 21220, loss 0.61305.
Test: 2018-08-09T11:52:44.041000: step 21220, loss 0.546763.
Train: 2018-08-09T11:52:44.134727: step 21221, loss 0.612223.
Train: 2018-08-09T11:52:44.244106: step 21222, loss 0.578694.
Train: 2018-08-09T11:52:44.337834: step 21223, loss 0.662412.
Train: 2018-08-09T11:52:44.447184: step 21224, loss 0.529712.
Train: 2018-08-09T11:52:44.540911: step 21225, loss 0.595248.
Train: 2018-08-09T11:52:44.650261: step 21226, loss 0.611343.
Train: 2018-08-09T11:52:44.743959: step 21227, loss 0.513704.
Train: 2018-08-09T11:52:44.853308: step 21228, loss 0.595128.
Train: 2018-08-09T11:52:44.947035: step 21229, loss 0.594809.
Train: 2018-08-09T11:52:45.056384: step 21230, loss 0.611586.
Test: 2018-08-09T11:52:45.556268: step 21230, loss 0.549456.
Train: 2018-08-09T11:52:45.665618: step 21231, loss 0.4819.
Train: 2018-08-09T11:52:45.759373: step 21232, loss 0.530463.
Train: 2018-08-09T11:52:45.868723: step 21233, loss 0.578748.
Train: 2018-08-09T11:52:45.962451: step 21234, loss 0.691226.
Train: 2018-08-09T11:52:46.071801: step 21235, loss 0.578992.
Train: 2018-08-09T11:52:46.165498: step 21236, loss 0.466982.
Train: 2018-08-09T11:52:46.274878: step 21237, loss 0.578979.
Train: 2018-08-09T11:52:46.368605: step 21238, loss 0.499057.
Train: 2018-08-09T11:52:46.477954: step 21239, loss 0.578901.
Train: 2018-08-09T11:52:46.571683: step 21240, loss 0.514843.
Test: 2018-08-09T11:52:47.074087: step 21240, loss 0.551413.
Train: 2018-08-09T11:52:47.183442: step 21241, loss 0.561906.
Train: 2018-08-09T11:52:47.277168: step 21242, loss 0.530522.
Train: 2018-08-09T11:52:47.386518: step 21243, loss 0.593607.
Train: 2018-08-09T11:52:47.480246: step 21244, loss 0.512905.
Train: 2018-08-09T11:52:47.589595: step 21245, loss 0.560319.
Train: 2018-08-09T11:52:47.683325: step 21246, loss 0.528358.
Train: 2018-08-09T11:52:47.792649: step 21247, loss 0.453589.
Train: 2018-08-09T11:52:47.886400: step 21248, loss 0.60715.
Train: 2018-08-09T11:52:47.995745: step 21249, loss 0.479993.
Train: 2018-08-09T11:52:48.089447: step 21250, loss 0.57074.
Test: 2018-08-09T11:52:48.589330: step 21250, loss 0.548556.
Train: 2018-08-09T11:52:48.683089: step 21251, loss 0.548659.
Train: 2018-08-09T11:52:48.792436: step 21252, loss 0.625087.
Train: 2018-08-09T11:52:48.886135: step 21253, loss 0.509545.
Train: 2018-08-09T11:52:48.982185: step 21254, loss 0.552602.
Train: 2018-08-09T11:52:49.091537: step 21255, loss 0.561561.
Train: 2018-08-09T11:52:49.185262: step 21256, loss 0.628531.
Train: 2018-08-09T11:52:49.294612: step 21257, loss 0.619135.
Train: 2018-08-09T11:52:49.388339: step 21258, loss 0.48228.
Train: 2018-08-09T11:52:49.499333: step 21259, loss 0.486862.
Train: 2018-08-09T11:52:49.593059: step 21260, loss 0.481522.
Test: 2018-08-09T11:52:50.092911: step 21260, loss 0.546101.
Train: 2018-08-09T11:52:50.186662: step 21261, loss 0.582803.
Train: 2018-08-09T11:52:50.296018: step 21262, loss 0.58384.
Train: 2018-08-09T11:52:50.389717: step 21263, loss 0.594233.
Train: 2018-08-09T11:52:50.499095: step 21264, loss 0.654668.
Train: 2018-08-09T11:52:50.592822: step 21265, loss 0.649333.
Train: 2018-08-09T11:52:50.702173: step 21266, loss 0.578041.
Train: 2018-08-09T11:52:50.795901: step 21267, loss 0.546175.
Train: 2018-08-09T11:52:50.907475: step 21268, loss 0.499988.
Train: 2018-08-09T11:52:50.998625: step 21269, loss 0.548756.
Train: 2018-08-09T11:52:51.107944: step 21270, loss 0.579496.
Test: 2018-08-09T11:52:51.607858: step 21270, loss 0.549921.
Train: 2018-08-09T11:52:51.701554: step 21271, loss 0.54683.
Train: 2018-08-09T11:52:51.795309: step 21272, loss 0.594743.
Train: 2018-08-09T11:52:51.904632: step 21273, loss 0.547266.
Train: 2018-08-09T11:52:51.998389: step 21274, loss 0.515354.
Train: 2018-08-09T11:52:52.107738: step 21275, loss 0.641575.
Train: 2018-08-09T11:52:52.201470: step 21276, loss 0.595426.
Train: 2018-08-09T11:52:52.310785: step 21277, loss 0.563086.
Train: 2018-08-09T11:52:52.404543: step 21278, loss 0.641512.
Train: 2018-08-09T11:52:52.513893: step 21279, loss 0.484964.
Train: 2018-08-09T11:52:52.607620: step 21280, loss 0.578965.
Test: 2018-08-09T11:52:53.107479: step 21280, loss 0.547465.
Train: 2018-08-09T11:52:53.201232: step 21281, loss 0.594495.
Train: 2018-08-09T11:52:53.294959: step 21282, loss 0.532109.
Train: 2018-08-09T11:52:53.404277: step 21283, loss 0.5321.
Train: 2018-08-09T11:52:53.498035: step 21284, loss 0.594471.
Train: 2018-08-09T11:52:53.607354: step 21285, loss 0.657051.
Train: 2018-08-09T11:52:53.701116: step 21286, loss 0.625726.
Train: 2018-08-09T11:52:53.810466: step 21287, loss 0.609993.
Train: 2018-08-09T11:52:53.904190: step 21288, loss 0.516864.
Train: 2018-08-09T11:52:53.997918: step 21289, loss 0.547941.
Train: 2018-08-09T11:52:54.107268: step 21290, loss 0.547959.
Test: 2018-08-09T11:52:54.591499: step 21290, loss 0.549525.
Train: 2018-08-09T11:52:54.700847: step 21291, loss 0.470574.
Train: 2018-08-09T11:52:54.794576: step 21292, loss 0.56341.
Train: 2018-08-09T11:52:54.903924: step 21293, loss 0.563337.
Train: 2018-08-09T11:52:54.997682: step 21294, loss 0.625657.
Train: 2018-08-09T11:52:55.091410: step 21295, loss 0.532207.
Train: 2018-08-09T11:52:55.200759: step 21296, loss 0.57888.
Train: 2018-08-09T11:52:55.294491: step 21297, loss 0.641364.
Train: 2018-08-09T11:52:55.403837: step 21298, loss 0.532072.
Train: 2018-08-09T11:52:55.497565: step 21299, loss 0.500789.
Train: 2018-08-09T11:52:55.591292: step 21300, loss 0.610189.
Test: 2018-08-09T11:52:56.093451: step 21300, loss 0.550907.
Train: 2018-08-09T11:52:56.655795: step 21301, loss 0.563176.
Train: 2018-08-09T11:52:56.765145: step 21302, loss 0.469128.
Train: 2018-08-09T11:52:56.858871: step 21303, loss 0.531673.
Train: 2018-08-09T11:52:56.968220: step 21304, loss 0.515558.
Train: 2018-08-09T11:52:57.061978: step 21305, loss 0.530334.
Train: 2018-08-09T11:52:57.155705: step 21306, loss 0.610718.
Train: 2018-08-09T11:52:57.265055: step 21307, loss 0.578911.
Train: 2018-08-09T11:52:57.358754: step 21308, loss 0.530044.
Train: 2018-08-09T11:52:57.452511: step 21309, loss 0.49725.
Train: 2018-08-09T11:52:57.561830: step 21310, loss 0.647731.
Test: 2018-08-09T11:52:58.046892: step 21310, loss 0.54988.
Train: 2018-08-09T11:52:58.156222: step 21311, loss 0.495458.
Train: 2018-08-09T11:52:58.249983: step 21312, loss 0.599067.
Train: 2018-08-09T11:52:58.359332: step 21313, loss 0.629644.
Train: 2018-08-09T11:52:58.453028: step 21314, loss 0.546844.
Train: 2018-08-09T11:52:58.546785: step 21315, loss 0.595265.
Train: 2018-08-09T11:52:58.640513: step 21316, loss 0.561363.
Train: 2018-08-09T11:52:58.749862: step 21317, loss 0.610422.
Train: 2018-08-09T11:52:58.843592: step 21318, loss 0.546783.
Train: 2018-08-09T11:52:58.937319: step 21319, loss 0.513945.
Train: 2018-08-09T11:52:59.046667: step 21320, loss 0.465131.
Test: 2018-08-09T11:52:59.530929: step 21320, loss 0.549859.
Train: 2018-08-09T11:52:59.624657: step 21321, loss 0.465297.
Train: 2018-08-09T11:52:59.734005: step 21322, loss 0.465114.
Train: 2018-08-09T11:52:59.827732: step 21323, loss 0.611362.
Train: 2018-08-09T11:52:59.921461: step 21324, loss 0.51217.
Train: 2018-08-09T11:53:00.030811: step 21325, loss 0.56242.
Train: 2018-08-09T11:53:00.124538: step 21326, loss 0.5625.
Train: 2018-08-09T11:53:00.218236: step 21327, loss 0.579757.
Train: 2018-08-09T11:53:00.327615: step 21328, loss 0.492609.
Train: 2018-08-09T11:53:00.421343: step 21329, loss 0.528025.
Train: 2018-08-09T11:53:00.515072: step 21330, loss 0.594538.
Test: 2018-08-09T11:53:01.017250: step 21330, loss 0.549959.
Train: 2018-08-09T11:53:01.110977: step 21331, loss 0.680799.
Train: 2018-08-09T11:53:01.204735: step 21332, loss 0.682829.
Train: 2018-08-09T11:53:01.314056: step 21333, loss 0.475657.
Train: 2018-08-09T11:53:01.407814: step 21334, loss 0.66444.
Train: 2018-08-09T11:53:01.501540: step 21335, loss 0.627451.
Train: 2018-08-09T11:53:01.610884: step 21336, loss 0.627861.
Train: 2018-08-09T11:53:01.704617: step 21337, loss 0.493953.
Train: 2018-08-09T11:53:01.798346: step 21338, loss 0.509977.
Train: 2018-08-09T11:53:01.892073: step 21339, loss 0.561408.
Train: 2018-08-09T11:53:02.001423: step 21340, loss 0.631504.
Test: 2018-08-09T11:53:02.501276: step 21340, loss 0.54831.
Train: 2018-08-09T11:53:02.595032: step 21341, loss 0.489749.
Train: 2018-08-09T11:53:02.688760: step 21342, loss 0.444395.
Train: 2018-08-09T11:53:02.782489: step 21343, loss 0.666662.
Train: 2018-08-09T11:53:02.891836: step 21344, loss 0.54751.
Train: 2018-08-09T11:53:02.987978: step 21345, loss 0.550364.
Train: 2018-08-09T11:53:03.081677: step 21346, loss 0.549587.
Train: 2018-08-09T11:53:03.175435: step 21347, loss 0.462368.
Train: 2018-08-09T11:53:03.269133: step 21348, loss 0.627249.
Train: 2018-08-09T11:53:03.378514: step 21349, loss 0.479243.
Train: 2018-08-09T11:53:03.472209: step 21350, loss 0.594336.
Test: 2018-08-09T11:53:03.972092: step 21350, loss 0.547932.
Train: 2018-08-09T11:53:04.065819: step 21351, loss 0.565109.
Train: 2018-08-09T11:53:04.159578: step 21352, loss 0.513994.
Train: 2018-08-09T11:53:04.268929: step 21353, loss 0.561681.
Train: 2018-08-09T11:53:04.362655: step 21354, loss 0.579474.
Train: 2018-08-09T11:53:04.456387: step 21355, loss 0.595574.
Train: 2018-08-09T11:53:04.565732: step 21356, loss 0.599052.
Train: 2018-08-09T11:53:04.659460: step 21357, loss 0.57942.
Train: 2018-08-09T11:53:04.753189: step 21358, loss 0.465159.
Train: 2018-08-09T11:53:04.860159: step 21359, loss 0.578997.
Train: 2018-08-09T11:53:04.958219: step 21360, loss 0.59483.
Test: 2018-08-09T11:53:05.442457: step 21360, loss 0.5487.
Train: 2018-08-09T11:53:05.536214: step 21361, loss 0.56235.
Train: 2018-08-09T11:53:05.645563: step 21362, loss 0.562534.
Train: 2018-08-09T11:53:05.739290: step 21363, loss 0.563295.
Train: 2018-08-09T11:53:05.833018: step 21364, loss 0.627599.
Train: 2018-08-09T11:53:05.942368: step 21365, loss 0.627221.
Train: 2018-08-09T11:53:06.036096: step 21366, loss 0.594755.
Train: 2018-08-09T11:53:06.129823: step 21367, loss 0.563069.
Train: 2018-08-09T11:53:06.223553: step 21368, loss 0.514824.
Train: 2018-08-09T11:53:06.332903: step 21369, loss 0.547158.
Train: 2018-08-09T11:53:06.426629: step 21370, loss 0.610435.
Test: 2018-08-09T11:53:06.928061: step 21370, loss 0.549098.
Train: 2018-08-09T11:53:07.021817: step 21371, loss 0.546148.
Train: 2018-08-09T11:53:07.131167: step 21372, loss 0.627235.
Train: 2018-08-09T11:53:07.224896: step 21373, loss 0.483109.
Train: 2018-08-09T11:53:07.318622: step 21374, loss 0.594974.
Train: 2018-08-09T11:53:07.412350: step 21375, loss 0.642294.
Train: 2018-08-09T11:53:07.521671: step 21376, loss 0.466804.
Train: 2018-08-09T11:53:07.615427: step 21377, loss 0.466348.
Train: 2018-08-09T11:53:07.709155: step 21378, loss 0.513632.
Train: 2018-08-09T11:53:07.818474: step 21379, loss 0.61161.
Train: 2018-08-09T11:53:07.912233: step 21380, loss 0.561262.
Test: 2018-08-09T11:53:08.412087: step 21380, loss 0.547278.
Train: 2018-08-09T11:53:08.505839: step 21381, loss 0.529798.
Train: 2018-08-09T11:53:08.601338: step 21382, loss 0.540463.
Train: 2018-08-09T11:53:08.710688: step 21383, loss 0.707334.
Train: 2018-08-09T11:53:08.804416: step 21384, loss 0.588692.
Train: 2018-08-09T11:53:08.898144: step 21385, loss 0.633546.
Train: 2018-08-09T11:53:09.007493: step 21386, loss 0.626046.
Train: 2018-08-09T11:53:09.101222: step 21387, loss 0.546158.
Train: 2018-08-09T11:53:09.194948: step 21388, loss 0.627233.
Train: 2018-08-09T11:53:09.288681: step 21389, loss 0.530771.
Train: 2018-08-09T11:53:09.398029: step 21390, loss 0.546771.
Test: 2018-08-09T11:53:09.897903: step 21390, loss 0.547538.
Train: 2018-08-09T11:53:09.991637: step 21391, loss 0.53158.
Train: 2018-08-09T11:53:10.085364: step 21392, loss 0.56383.
Train: 2018-08-09T11:53:10.179092: step 21393, loss 0.500213.
Train: 2018-08-09T11:53:10.288445: step 21394, loss 0.499806.
Train: 2018-08-09T11:53:10.382170: step 21395, loss 0.483874.
Train: 2018-08-09T11:53:10.475898: step 21396, loss 0.610462.
Train: 2018-08-09T11:53:10.585242: step 21397, loss 0.626289.
Train: 2018-08-09T11:53:10.678945: step 21398, loss 0.626251.
Train: 2018-08-09T11:53:10.772703: step 21399, loss 0.562853.
Train: 2018-08-09T11:53:10.868434: step 21400, loss 0.531526.
Test: 2018-08-09T11:53:11.368318: step 21400, loss 0.550005.
Train: 2018-08-09T11:53:11.930714: step 21401, loss 0.578704.
Train: 2018-08-09T11:53:12.024442: step 21402, loss 0.499853.
Train: 2018-08-09T11:53:12.133792: step 21403, loss 0.515873.
Train: 2018-08-09T11:53:12.227490: step 21404, loss 0.626788.
Train: 2018-08-09T11:53:12.321249: step 21405, loss 0.531101.
Train: 2018-08-09T11:53:12.430596: step 21406, loss 0.658713.
Train: 2018-08-09T11:53:12.524324: step 21407, loss 0.594904.
Train: 2018-08-09T11:53:12.618056: step 21408, loss 0.610603.
Train: 2018-08-09T11:53:12.711782: step 21409, loss 0.578769.
Train: 2018-08-09T11:53:12.821132: step 21410, loss 0.547233.
Test: 2018-08-09T11:53:13.307859: step 21410, loss 0.550602.
Train: 2018-08-09T11:53:13.417209: step 21411, loss 0.515483.
Train: 2018-08-09T11:53:13.510966: step 21412, loss 0.49983.
Train: 2018-08-09T11:53:13.620286: step 21413, loss 0.547225.
Train: 2018-08-09T11:53:13.714043: step 21414, loss 0.483484.
Train: 2018-08-09T11:53:13.807741: step 21415, loss 0.483399.
Train: 2018-08-09T11:53:13.901468: step 21416, loss 0.562646.
Train: 2018-08-09T11:53:14.010848: step 21417, loss 0.498346.
Train: 2018-08-09T11:53:14.104576: step 21418, loss 0.513945.
Train: 2018-08-09T11:53:14.213902: step 21419, loss 0.594192.
Train: 2018-08-09T11:53:14.307654: step 21420, loss 0.546336.
Test: 2018-08-09T11:53:14.791884: step 21420, loss 0.551957.
Train: 2018-08-09T11:53:14.901264: step 21421, loss 0.609446.
Train: 2018-08-09T11:53:14.996397: step 21422, loss 0.476497.
Train: 2018-08-09T11:53:15.090125: step 21423, loss 0.582693.
Train: 2018-08-09T11:53:15.199474: step 21424, loss 0.444979.
Train: 2018-08-09T11:53:15.293172: step 21425, loss 0.439952.
Train: 2018-08-09T11:53:15.386901: step 21426, loss 0.538577.
Train: 2018-08-09T11:53:15.496250: step 21427, loss 0.615212.
Train: 2018-08-09T11:53:15.589977: step 21428, loss 0.629198.
Train: 2018-08-09T11:53:15.683736: step 21429, loss 0.66835.
Train: 2018-08-09T11:53:15.777464: step 21430, loss 0.527244.
Test: 2018-08-09T11:53:16.277317: step 21430, loss 0.547758.
Train: 2018-08-09T11:53:16.386665: step 21431, loss 0.553331.
Train: 2018-08-09T11:53:16.480393: step 21432, loss 0.615976.
Train: 2018-08-09T11:53:16.574150: step 21433, loss 0.510056.
Train: 2018-08-09T11:53:16.683495: step 21434, loss 0.506172.
Train: 2018-08-09T11:53:16.777229: step 21435, loss 0.619266.
Train: 2018-08-09T11:53:16.886578: step 21436, loss 0.491521.
Train: 2018-08-09T11:53:16.981617: step 21437, loss 0.561919.
Train: 2018-08-09T11:53:17.075341: step 21438, loss 0.472002.
Train: 2018-08-09T11:53:17.184686: step 21439, loss 0.528567.
Train: 2018-08-09T11:53:17.278417: step 21440, loss 0.488997.
Test: 2018-08-09T11:53:17.778271: step 21440, loss 0.547198.
Train: 2018-08-09T11:53:17.871999: step 21441, loss 0.618801.
Train: 2018-08-09T11:53:17.965727: step 21442, loss 0.491404.
Train: 2018-08-09T11:53:18.075105: step 21443, loss 0.583034.
Train: 2018-08-09T11:53:18.168837: step 21444, loss 0.614433.
Train: 2018-08-09T11:53:18.278183: step 21445, loss 0.548906.
Train: 2018-08-09T11:53:18.371914: step 21446, loss 0.508795.
Train: 2018-08-09T11:53:18.465609: step 21447, loss 0.61581.
Train: 2018-08-09T11:53:18.574988: step 21448, loss 0.615325.
Train: 2018-08-09T11:53:18.668716: step 21449, loss 0.477304.
Train: 2018-08-09T11:53:18.762443: step 21450, loss 0.561021.
Test: 2018-08-09T11:53:19.263675: step 21450, loss 0.5476.
Train: 2018-08-09T11:53:19.373024: step 21451, loss 0.544126.
Train: 2018-08-09T11:53:19.466783: step 21452, loss 0.613751.
Train: 2018-08-09T11:53:19.560510: step 21453, loss 0.511891.
Train: 2018-08-09T11:53:19.669859: step 21454, loss 0.527802.
Train: 2018-08-09T11:53:19.763587: step 21455, loss 0.524576.
Train: 2018-08-09T11:53:19.872907: step 21456, loss 0.58207.
Train: 2018-08-09T11:53:19.966664: step 21457, loss 0.514142.
Train: 2018-08-09T11:53:20.060392: step 21458, loss 0.661816.
Train: 2018-08-09T11:53:20.169741: step 21459, loss 0.52927.
Train: 2018-08-09T11:53:20.263471: step 21460, loss 0.577025.
Test: 2018-08-09T11:53:20.763323: step 21460, loss 0.549464.
Train: 2018-08-09T11:53:20.857050: step 21461, loss 0.549183.
Train: 2018-08-09T11:53:20.968886: step 21462, loss 0.564307.
Train: 2018-08-09T11:53:21.062584: step 21463, loss 0.497014.
Train: 2018-08-09T11:53:21.156342: step 21464, loss 0.632566.
Train: 2018-08-09T11:53:21.265661: step 21465, loss 0.577537.
Train: 2018-08-09T11:53:21.359418: step 21466, loss 0.545568.
Train: 2018-08-09T11:53:21.468738: step 21467, loss 0.497171.
Train: 2018-08-09T11:53:21.562466: step 21468, loss 0.431217.
Train: 2018-08-09T11:53:21.656224: step 21469, loss 0.54646.
Train: 2018-08-09T11:53:21.765573: step 21470, loss 0.562066.
Test: 2018-08-09T11:53:22.249805: step 21470, loss 0.551436.
Train: 2018-08-09T11:53:22.359183: step 21471, loss 0.661698.
Train: 2018-08-09T11:53:22.452915: step 21472, loss 0.562121.
Train: 2018-08-09T11:53:22.553205: step 21473, loss 0.660912.
Train: 2018-08-09T11:53:22.662555: step 21474, loss 0.693649.
Train: 2018-08-09T11:53:22.756283: step 21475, loss 0.562543.
Train: 2018-08-09T11:53:22.865632: step 21476, loss 0.627624.
Train: 2018-08-09T11:53:22.955086: step 21477, loss 0.530396.
Train: 2018-08-09T11:53:23.064434: step 21478, loss 0.643429.
Train: 2018-08-09T11:53:23.158161: step 21479, loss 0.482455.
Train: 2018-08-09T11:53:23.267510: step 21480, loss 0.578906.
Test: 2018-08-09T11:53:23.751743: step 21480, loss 0.552726.
Train: 2018-08-09T11:53:23.861091: step 21481, loss 0.530891.
Train: 2018-08-09T11:53:23.954819: step 21482, loss 0.578879.
Train: 2018-08-09T11:53:24.064198: step 21483, loss 0.562909.
Train: 2018-08-09T11:53:24.157925: step 21484, loss 0.499249.
Train: 2018-08-09T11:53:24.267246: step 21485, loss 0.499225.
Train: 2018-08-09T11:53:24.360973: step 21486, loss 0.57886.
Train: 2018-08-09T11:53:24.470352: step 21487, loss 0.530972.
Train: 2018-08-09T11:53:24.564080: step 21488, loss 0.642799.
Train: 2018-08-09T11:53:24.673430: step 21489, loss 0.610766.
Train: 2018-08-09T11:53:24.767158: step 21490, loss 0.610817.
Test: 2018-08-09T11:53:25.267011: step 21490, loss 0.552134.
Train: 2018-08-09T11:53:25.376359: step 21491, loss 0.483065.
Train: 2018-08-09T11:53:25.470117: step 21492, loss 0.594899.
Train: 2018-08-09T11:53:25.579460: step 21493, loss 0.514889.
Train: 2018-08-09T11:53:25.673194: step 21494, loss 0.530581.
Train: 2018-08-09T11:53:25.782514: step 21495, loss 0.610971.
Train: 2018-08-09T11:53:25.876241: step 21496, loss 0.578942.
Train: 2018-08-09T11:53:25.972374: step 21497, loss 0.578302.
Train: 2018-08-09T11:53:26.081754: step 21498, loss 0.530988.
Train: 2018-08-09T11:53:26.175481: step 21499, loss 0.498298.
Train: 2018-08-09T11:53:26.284830: step 21500, loss 0.481019.
Test: 2018-08-09T11:53:26.769061: step 21500, loss 0.549462.
Train: 2018-08-09T11:53:27.378323: step 21501, loss 0.54546.
Train: 2018-08-09T11:53:27.487673: step 21502, loss 0.662476.
Train: 2018-08-09T11:53:27.581400: step 21503, loss 0.59618.
Train: 2018-08-09T11:53:27.690750: step 21504, loss 0.563125.
Train: 2018-08-09T11:53:27.784477: step 21505, loss 0.631305.
Train: 2018-08-09T11:53:27.893826: step 21506, loss 0.54553.
Train: 2018-08-09T11:53:27.988919: step 21507, loss 0.596994.
Train: 2018-08-09T11:53:28.098268: step 21508, loss 0.480568.
Train: 2018-08-09T11:53:28.191998: step 21509, loss 0.497159.
Train: 2018-08-09T11:53:28.301346: step 21510, loss 0.481237.
Test: 2018-08-09T11:53:28.785595: step 21510, loss 0.548462.
Train: 2018-08-09T11:53:28.894956: step 21511, loss 0.512754.
Train: 2018-08-09T11:53:28.988684: step 21512, loss 0.563349.
Train: 2018-08-09T11:53:29.098032: step 21513, loss 0.595372.
Train: 2018-08-09T11:53:29.191732: step 21514, loss 0.412387.
Train: 2018-08-09T11:53:29.301114: step 21515, loss 0.597117.
Train: 2018-08-09T11:53:29.394808: step 21516, loss 0.546963.
Train: 2018-08-09T11:53:29.504187: step 21517, loss 0.560255.
Train: 2018-08-09T11:53:29.597918: step 21518, loss 0.478373.
Train: 2018-08-09T11:53:29.707265: step 21519, loss 0.544176.
Train: 2018-08-09T11:53:29.800963: step 21520, loss 0.525899.
Test: 2018-08-09T11:53:30.302197: step 21520, loss 0.546508.
Train: 2018-08-09T11:53:30.411575: step 21521, loss 0.594261.
Train: 2018-08-09T11:53:30.505303: step 21522, loss 0.506588.
Train: 2018-08-09T11:53:30.614656: step 21523, loss 0.615666.
Train: 2018-08-09T11:53:30.708350: step 21524, loss 0.471089.
Train: 2018-08-09T11:53:30.817730: step 21525, loss 0.53283.
Train: 2018-08-09T11:53:30.911459: step 21526, loss 0.579572.
Train: 2018-08-09T11:53:31.020808: step 21527, loss 0.594372.
Train: 2018-08-09T11:53:31.114504: step 21528, loss 0.590441.
Train: 2018-08-09T11:53:31.223884: step 21529, loss 0.640137.
Train: 2018-08-09T11:53:31.333234: step 21530, loss 0.66577.
Test: 2018-08-09T11:53:31.817465: step 21530, loss 0.545816.
Train: 2018-08-09T11:53:31.928267: step 21531, loss 0.477393.
Train: 2018-08-09T11:53:32.021965: step 21532, loss 0.491555.
Train: 2018-08-09T11:53:32.131344: step 21533, loss 0.527949.
Train: 2018-08-09T11:53:32.225074: step 21534, loss 0.613335.
Train: 2018-08-09T11:53:32.334421: step 21535, loss 0.521674.
Train: 2018-08-09T11:53:32.490635: step 21536, loss 0.56181.
Train: 2018-08-09T11:53:32.584363: step 21537, loss 0.491215.
Train: 2018-08-09T11:53:32.693715: step 21538, loss 0.600919.
Train: 2018-08-09T11:53:32.803061: step 21539, loss 0.544301.
Train: 2018-08-09T11:53:32.912406: step 21540, loss 0.602579.
Test: 2018-08-09T11:53:33.396642: step 21540, loss 0.549077.
Train: 2018-08-09T11:53:33.505990: step 21541, loss 0.593888.
Train: 2018-08-09T11:53:33.599717: step 21542, loss 0.563165.
Train: 2018-08-09T11:53:33.709097: step 21543, loss 0.595737.
Train: 2018-08-09T11:53:33.818446: step 21544, loss 0.480782.
Train: 2018-08-09T11:53:33.913573: step 21545, loss 0.510812.
Train: 2018-08-09T11:53:34.022955: step 21546, loss 0.594907.
Train: 2018-08-09T11:53:34.116650: step 21547, loss 0.513798.
Train: 2018-08-09T11:53:34.226031: step 21548, loss 0.563466.
Train: 2018-08-09T11:53:34.319757: step 21549, loss 0.54725.
Train: 2018-08-09T11:53:34.429077: step 21550, loss 0.544336.
Test: 2018-08-09T11:53:34.928973: step 21550, loss 0.548319.
Train: 2018-08-09T11:53:35.022688: step 21551, loss 0.609595.
Train: 2018-08-09T11:53:35.132036: step 21552, loss 0.580388.
Train: 2018-08-09T11:53:35.225794: step 21553, loss 0.615219.
Train: 2018-08-09T11:53:35.335143: step 21554, loss 0.53143.
Train: 2018-08-09T11:53:35.444494: step 21555, loss 0.611559.
Train: 2018-08-09T11:53:35.538221: step 21556, loss 0.513867.
Train: 2018-08-09T11:53:35.647570: step 21557, loss 0.578147.
Train: 2018-08-09T11:53:35.756921: step 21558, loss 0.644077.
Train: 2018-08-09T11:53:35.850617: step 21559, loss 0.481839.
Train: 2018-08-09T11:53:35.961381: step 21560, loss 0.530634.
Test: 2018-08-09T11:53:36.460708: step 21560, loss 0.547669.
Train: 2018-08-09T11:53:36.556600: step 21561, loss 0.530694.
Train: 2018-08-09T11:53:36.650298: step 21562, loss 0.723798.
Train: 2018-08-09T11:53:36.759647: step 21563, loss 0.45038.
Train: 2018-08-09T11:53:36.868997: step 21564, loss 0.546477.
Train: 2018-08-09T11:53:36.962754: step 21565, loss 0.530126.
Train: 2018-08-09T11:53:37.072104: step 21566, loss 0.627053.
Train: 2018-08-09T11:53:37.181423: step 21567, loss 0.611001.
Train: 2018-08-09T11:53:37.275183: step 21568, loss 0.674974.
Train: 2018-08-09T11:53:37.384530: step 21569, loss 0.530697.
Train: 2018-08-09T11:53:37.478257: step 21570, loss 0.515081.
Test: 2018-08-09T11:53:37.978111: step 21570, loss 0.547879.
Train: 2018-08-09T11:53:38.087489: step 21571, loss 0.531013.
Train: 2018-08-09T11:53:38.196809: step 21572, loss 0.546931.
Train: 2018-08-09T11:53:38.290566: step 21573, loss 0.530469.
Train: 2018-08-09T11:53:38.399916: step 21574, loss 0.546662.
Train: 2018-08-09T11:53:38.493646: step 21575, loss 0.707013.
Train: 2018-08-09T11:53:38.602992: step 21576, loss 0.546868.
Train: 2018-08-09T11:53:38.712344: step 21577, loss 0.563272.
Train: 2018-08-09T11:53:38.806070: step 21578, loss 0.49855.
Train: 2018-08-09T11:53:38.915419: step 21579, loss 0.562807.
Train: 2018-08-09T11:53:39.024757: step 21580, loss 0.642449.
Test: 2018-08-09T11:53:39.509000: step 21580, loss 0.549643.
Train: 2018-08-09T11:53:39.618381: step 21581, loss 0.547525.
Train: 2018-08-09T11:53:39.727726: step 21582, loss 0.594878.
Train: 2018-08-09T11:53:39.821456: step 21583, loss 0.627217.
Train: 2018-08-09T11:53:39.932253: step 21584, loss 0.593776.
Train: 2018-08-09T11:53:40.025981: step 21585, loss 0.610765.
Train: 2018-08-09T11:53:40.135330: step 21586, loss 0.499715.
Train: 2018-08-09T11:53:40.244650: step 21587, loss 0.610562.
Train: 2018-08-09T11:53:40.354029: step 21588, loss 0.51576.
Train: 2018-08-09T11:53:40.463360: step 21589, loss 0.579035.
Train: 2018-08-09T11:53:40.572729: step 21590, loss 0.531463.
Test: 2018-08-09T11:53:41.072581: step 21590, loss 0.548819.
Train: 2018-08-09T11:53:41.166306: step 21591, loss 0.594743.
Train: 2018-08-09T11:53:41.275687: step 21592, loss 0.56306.
Train: 2018-08-09T11:53:41.385036: step 21593, loss 0.579044.
Train: 2018-08-09T11:53:41.478764: step 21594, loss 0.610158.
Train: 2018-08-09T11:53:41.583071: step 21595, loss 0.547604.
Train: 2018-08-09T11:53:41.692421: step 21596, loss 0.563066.
Train: 2018-08-09T11:53:41.801770: step 21597, loss 0.484096.
Train: 2018-08-09T11:53:41.895499: step 21598, loss 0.657299.
Train: 2018-08-09T11:53:42.011254: step 21599, loss 0.500606.
Train: 2018-08-09T11:53:42.104979: step 21600, loss 0.610553.
Test: 2018-08-09T11:53:42.604833: step 21600, loss 0.549498.
Train: 2018-08-09T11:53:43.167230: step 21601, loss 0.562921.
Train: 2018-08-09T11:53:43.276579: step 21602, loss 0.563045.
Train: 2018-08-09T11:53:43.385930: step 21603, loss 0.563092.
Train: 2018-08-09T11:53:43.479655: step 21604, loss 0.626415.
Train: 2018-08-09T11:53:43.588975: step 21605, loss 0.59473.
Train: 2018-08-09T11:53:43.694277: step 21606, loss 0.594621.
Train: 2018-08-09T11:53:43.807798: step 21607, loss 0.500521.
Train: 2018-08-09T11:53:43.897405: step 21608, loss 0.641582.
Train: 2018-08-09T11:53:44.006724: step 21609, loss 0.563107.
Train: 2018-08-09T11:53:44.116104: step 21610, loss 0.625913.
Test: 2018-08-09T11:53:44.600359: step 21610, loss 0.549163.
Train: 2018-08-09T11:53:44.709713: step 21611, loss 0.500512.
Train: 2018-08-09T11:53:44.819062: step 21612, loss 0.563513.
Train: 2018-08-09T11:53:44.918101: step 21613, loss 0.625459.
Train: 2018-08-09T11:53:45.027480: step 21614, loss 0.609891.
Train: 2018-08-09T11:53:45.136833: step 21615, loss 0.563302.
Train: 2018-08-09T11:53:45.230560: step 21616, loss 0.547911.
Train: 2018-08-09T11:53:45.339878: step 21617, loss 0.610088.
Train: 2018-08-09T11:53:45.449256: step 21618, loss 0.547889.
Train: 2018-08-09T11:53:45.542984: step 21619, loss 0.485779.
Train: 2018-08-09T11:53:45.652338: step 21620, loss 0.625288.
Test: 2018-08-09T11:53:46.152187: step 21620, loss 0.548221.
Train: 2018-08-09T11:53:46.261535: step 21621, loss 0.501076.
Train: 2018-08-09T11:53:46.355293: step 21622, loss 0.516462.
Train: 2018-08-09T11:53:46.464643: step 21623, loss 0.563164.
Train: 2018-08-09T11:53:46.573961: step 21624, loss 0.531918.
Train: 2018-08-09T11:53:46.683336: step 21625, loss 0.562941.
Train: 2018-08-09T11:53:46.777039: step 21626, loss 0.546871.
Train: 2018-08-09T11:53:46.886388: step 21627, loss 0.515826.
Train: 2018-08-09T11:53:46.995768: step 21628, loss 0.49984.
Train: 2018-08-09T11:53:47.105087: step 21629, loss 0.643085.
Train: 2018-08-09T11:53:47.198844: step 21630, loss 0.515451.
Test: 2018-08-09T11:53:47.698697: step 21630, loss 0.549007.
Train: 2018-08-09T11:53:47.854940: step 21631, loss 0.546786.
Train: 2018-08-09T11:53:47.951049: step 21632, loss 0.578814.
Train: 2018-08-09T11:53:48.060401: step 21633, loss 0.594301.
Train: 2018-08-09T11:53:48.169747: step 21634, loss 0.482408.
Train: 2018-08-09T11:53:48.263476: step 21635, loss 0.579879.
Train: 2018-08-09T11:53:48.372825: step 21636, loss 0.595219.
Train: 2018-08-09T11:53:48.482144: step 21637, loss 0.562655.
Train: 2018-08-09T11:53:48.575902: step 21638, loss 0.676364.
Train: 2018-08-09T11:53:48.685254: step 21639, loss 0.579352.
Train: 2018-08-09T11:53:48.794603: step 21640, loss 0.627355.
Test: 2018-08-09T11:53:49.294452: step 21640, loss 0.550507.
Train: 2018-08-09T11:53:49.388214: step 21641, loss 0.660158.
Train: 2018-08-09T11:53:49.497559: step 21642, loss 0.546301.
Train: 2018-08-09T11:53:49.606909: step 21643, loss 0.513511.
Train: 2018-08-09T11:53:49.700637: step 21644, loss 0.530139.
Train: 2018-08-09T11:53:49.809957: step 21645, loss 0.627762.
Train: 2018-08-09T11:53:49.920662: step 21646, loss 0.563152.
Train: 2018-08-09T11:53:50.014422: step 21647, loss 0.707439.
Train: 2018-08-09T11:53:50.123772: step 21648, loss 0.563135.
Train: 2018-08-09T11:53:50.233119: step 21649, loss 0.610888.
Train: 2018-08-09T11:53:50.342469: step 21650, loss 0.673664.
Test: 2018-08-09T11:53:50.826700: step 21650, loss 0.546607.
Train: 2018-08-09T11:53:50.936048: step 21651, loss 0.531578.
Train: 2018-08-09T11:53:51.045427: step 21652, loss 0.547362.
Train: 2018-08-09T11:53:51.154748: step 21653, loss 0.532118.
Train: 2018-08-09T11:53:51.248505: step 21654, loss 0.594568.
Train: 2018-08-09T11:53:51.357854: step 21655, loss 0.578919.
Train: 2018-08-09T11:53:51.467203: step 21656, loss 0.578878.
Train: 2018-08-09T11:53:51.560931: step 21657, loss 0.563413.
Train: 2018-08-09T11:53:51.670280: step 21658, loss 0.563337.
Train: 2018-08-09T11:53:51.779630: step 21659, loss 0.517236.
Train: 2018-08-09T11:53:51.888949: step 21660, loss 0.501747.
Test: 2018-08-09T11:53:52.389577: step 21660, loss 0.549634.
Train: 2018-08-09T11:53:52.483331: step 21661, loss 0.579027.
Train: 2018-08-09T11:53:52.592680: step 21662, loss 0.455463.
Train: 2018-08-09T11:53:52.686408: step 21663, loss 0.656304.
Train: 2018-08-09T11:53:52.795758: step 21664, loss 0.532571.
Train: 2018-08-09T11:53:52.905110: step 21665, loss 0.563273.
Train: 2018-08-09T11:53:53.014460: step 21666, loss 0.579015.
Train: 2018-08-09T11:53:53.108184: step 21667, loss 0.531895.
Train: 2018-08-09T11:53:53.217533: step 21668, loss 0.563151.
Train: 2018-08-09T11:53:53.326882: step 21669, loss 0.578791.
Train: 2018-08-09T11:53:53.420610: step 21670, loss 0.625981.
Test: 2018-08-09T11:53:53.920463: step 21670, loss 0.549627.
Train: 2018-08-09T11:53:54.029845: step 21671, loss 0.516201.
Train: 2018-08-09T11:53:54.139161: step 21672, loss 0.546263.
Train: 2018-08-09T11:53:54.232918: step 21673, loss 0.515788.
Train: 2018-08-09T11:53:54.342268: step 21674, loss 0.547443.
Train: 2018-08-09T11:53:54.451617: step 21675, loss 0.531226.
Train: 2018-08-09T11:53:54.560938: step 21676, loss 0.499257.
Train: 2018-08-09T11:53:54.654698: step 21677, loss 0.578523.
Train: 2018-08-09T11:53:54.764044: step 21678, loss 0.514685.
Train: 2018-08-09T11:53:54.873393: step 21679, loss 0.529636.
Train: 2018-08-09T11:53:54.968483: step 21680, loss 0.546385.
Test: 2018-08-09T11:53:55.468367: step 21680, loss 0.548233.
Train: 2018-08-09T11:53:55.572770: step 21681, loss 0.628616.
Train: 2018-08-09T11:53:55.682119: step 21682, loss 0.596447.
Train: 2018-08-09T11:53:55.791469: step 21683, loss 0.662043.
Train: 2018-08-09T11:53:55.885196: step 21684, loss 0.579361.
Train: 2018-08-09T11:53:55.994546: step 21685, loss 0.54612.
Train: 2018-08-09T11:53:56.103895: step 21686, loss 0.596246.
Train: 2018-08-09T11:53:56.213244: step 21687, loss 0.464384.
Train: 2018-08-09T11:53:56.306972: step 21688, loss 0.59639.
Train: 2018-08-09T11:53:56.416322: step 21689, loss 0.562707.
Train: 2018-08-09T11:53:56.525673: step 21690, loss 0.611722.
Test: 2018-08-09T11:53:57.025523: step 21690, loss 0.549115.
Train: 2018-08-09T11:53:57.119251: step 21691, loss 0.595424.
Train: 2018-08-09T11:53:57.228630: step 21692, loss 0.610368.
Train: 2018-08-09T11:53:57.337979: step 21693, loss 0.530291.
Train: 2018-08-09T11:53:57.431707: step 21694, loss 0.627985.
Train: 2018-08-09T11:53:57.541027: step 21695, loss 0.53015.
Train: 2018-08-09T11:53:57.650407: step 21696, loss 0.546567.
Train: 2018-08-09T11:53:57.759727: step 21697, loss 0.627492.
Train: 2018-08-09T11:53:57.869104: step 21698, loss 0.547001.
Train: 2018-08-09T11:53:57.962833: step 21699, loss 0.594696.
Train: 2018-08-09T11:53:58.072182: step 21700, loss 0.595094.
Test: 2018-08-09T11:53:58.572035: step 21700, loss 0.549003.
Train: 2018-08-09T11:53:59.136835: step 21701, loss 0.514484.
Train: 2018-08-09T11:53:59.230561: step 21702, loss 0.482583.
Train: 2018-08-09T11:53:59.339941: step 21703, loss 0.466374.
Train: 2018-08-09T11:53:59.449290: step 21704, loss 0.514909.
Train: 2018-08-09T11:53:59.542987: step 21705, loss 0.563215.
Train: 2018-08-09T11:53:59.652369: step 21706, loss 0.595975.
Train: 2018-08-09T11:53:59.761718: step 21707, loss 0.498046.
Train: 2018-08-09T11:53:59.871065: step 21708, loss 0.546407.
Train: 2018-08-09T11:53:59.964819: step 21709, loss 0.529806.
Train: 2018-08-09T11:54:00.074112: step 21710, loss 0.628327.
Test: 2018-08-09T11:54:00.573995: step 21710, loss 0.547053.
Train: 2018-08-09T11:54:00.667723: step 21711, loss 0.546093.
Train: 2018-08-09T11:54:00.777104: step 21712, loss 0.51229.
Train: 2018-08-09T11:54:00.886453: step 21713, loss 0.580307.
Train: 2018-08-09T11:54:00.982459: step 21714, loss 0.613302.
Train: 2018-08-09T11:54:01.091778: step 21715, loss 0.546219.
Train: 2018-08-09T11:54:01.201157: step 21716, loss 0.644615.
Train: 2018-08-09T11:54:01.310506: step 21717, loss 0.627532.
Train: 2018-08-09T11:54:01.404235: step 21718, loss 0.529702.
Train: 2018-08-09T11:54:01.513584: step 21719, loss 0.611851.
Train: 2018-08-09T11:54:01.622933: step 21720, loss 0.643837.
Test: 2018-08-09T11:54:02.107189: step 21720, loss 0.5498.
Train: 2018-08-09T11:54:02.216547: step 21721, loss 0.562559.
Train: 2018-08-09T11:54:02.325894: step 21722, loss 0.514504.
Train: 2018-08-09T11:54:02.419621: step 21723, loss 0.546374.
Train: 2018-08-09T11:54:02.528970: step 21724, loss 0.660408.
Train: 2018-08-09T11:54:02.638319: step 21725, loss 0.627296.
Train: 2018-08-09T11:54:02.732048: step 21726, loss 0.562629.
Train: 2018-08-09T11:54:02.841400: step 21727, loss 0.43431.
Train: 2018-08-09T11:54:02.952254: step 21728, loss 0.546815.
Train: 2018-08-09T11:54:03.061603: step 21729, loss 0.466532.
Train: 2018-08-09T11:54:03.155332: step 21730, loss 0.514436.
Test: 2018-08-09T11:54:03.655190: step 21730, loss 0.548275.
Train: 2018-08-09T11:54:03.764563: step 21731, loss 0.691687.
Train: 2018-08-09T11:54:03.858291: step 21732, loss 0.64391.
Train: 2018-08-09T11:54:03.967640: step 21733, loss 0.482399.
Train: 2018-08-09T11:54:04.076960: step 21734, loss 0.546759.
Train: 2018-08-09T11:54:04.170686: step 21735, loss 0.594745.
Train: 2018-08-09T11:54:04.280067: step 21736, loss 0.594726.
Train: 2018-08-09T11:54:04.389419: step 21737, loss 0.643412.
Train: 2018-08-09T11:54:04.483113: step 21738, loss 0.611095.
Train: 2018-08-09T11:54:04.592492: step 21739, loss 0.658788.
Train: 2018-08-09T11:54:04.701842: step 21740, loss 0.56309.
Test: 2018-08-09T11:54:05.186677: step 21740, loss 0.549888.
Train: 2018-08-09T11:54:05.296013: step 21741, loss 0.547089.
Train: 2018-08-09T11:54:05.405362: step 21742, loss 0.499802.
Train: 2018-08-09T11:54:05.514741: step 21743, loss 0.626217.
Train: 2018-08-09T11:54:05.608468: step 21744, loss 0.547327.
Train: 2018-08-09T11:54:05.717818: step 21745, loss 0.626271.
Train: 2018-08-09T11:54:05.811547: step 21746, loss 0.531628.
Train: 2018-08-09T11:54:05.920895: step 21747, loss 0.625942.
Train: 2018-08-09T11:54:06.030245: step 21748, loss 0.48485.
Train: 2018-08-09T11:54:06.123973: step 21749, loss 0.59454.
Train: 2018-08-09T11:54:06.233321: step 21750, loss 0.578823.
Test: 2018-08-09T11:54:06.733185: step 21750, loss 0.550347.
Train: 2018-08-09T11:54:06.826902: step 21751, loss 0.594574.
Train: 2018-08-09T11:54:06.936251: step 21752, loss 0.46951.
Train: 2018-08-09T11:54:07.045630: step 21753, loss 0.594533.
Train: 2018-08-09T11:54:07.139358: step 21754, loss 0.61023.
Train: 2018-08-09T11:54:07.248701: step 21755, loss 0.500787.
Train: 2018-08-09T11:54:07.358057: step 21756, loss 0.53193.
Train: 2018-08-09T11:54:07.451785: step 21757, loss 0.547527.
Train: 2018-08-09T11:54:07.561135: step 21758, loss 0.531655.
Train: 2018-08-09T11:54:07.654832: step 21759, loss 0.610307.
Train: 2018-08-09T11:54:07.764210: step 21760, loss 0.610384.
Test: 2018-08-09T11:54:08.266140: step 21760, loss 0.5494.
Train: 2018-08-09T11:54:08.375487: step 21761, loss 0.483798.
Train: 2018-08-09T11:54:08.469215: step 21762, loss 0.562369.
Train: 2018-08-09T11:54:08.578535: step 21763, loss 0.530848.
Train: 2018-08-09T11:54:08.672292: step 21764, loss 0.594066.
Train: 2018-08-09T11:54:08.781611: step 21765, loss 0.500415.
Train: 2018-08-09T11:54:08.890961: step 21766, loss 0.547694.
Train: 2018-08-09T11:54:08.984719: step 21767, loss 0.613274.
Train: 2018-08-09T11:54:09.094068: step 21768, loss 0.495563.
Train: 2018-08-09T11:54:09.203422: step 21769, loss 0.613664.
Train: 2018-08-09T11:54:09.297145: step 21770, loss 0.48043.
Test: 2018-08-09T11:54:09.796997: step 21770, loss 0.548225.
Train: 2018-08-09T11:54:09.908611: step 21771, loss 0.479997.
Train: 2018-08-09T11:54:10.002369: step 21772, loss 0.596308.
Train: 2018-08-09T11:54:10.111688: step 21773, loss 0.578556.
Train: 2018-08-09T11:54:10.205416: step 21774, loss 0.477402.
Train: 2018-08-09T11:54:10.314796: step 21775, loss 0.514238.
Train: 2018-08-09T11:54:10.424147: step 21776, loss 0.598048.
Train: 2018-08-09T11:54:10.517872: step 21777, loss 0.558534.
Train: 2018-08-09T11:54:10.627222: step 21778, loss 0.580221.
Train: 2018-08-09T11:54:10.720950: step 21779, loss 0.577536.
Train: 2018-08-09T11:54:10.830302: step 21780, loss 0.540711.
Test: 2018-08-09T11:54:11.330159: step 21780, loss 0.54598.
Train: 2018-08-09T11:54:11.423909: step 21781, loss 0.613536.
Train: 2018-08-09T11:54:11.533258: step 21782, loss 0.687207.
Train: 2018-08-09T11:54:11.626986: step 21783, loss 0.492086.
Train: 2018-08-09T11:54:11.736335: step 21784, loss 0.557874.
Train: 2018-08-09T11:54:11.845685: step 21785, loss 0.577039.
Train: 2018-08-09T11:54:11.940893: step 21786, loss 0.497704.
Train: 2018-08-09T11:54:12.050243: step 21787, loss 0.600136.
Train: 2018-08-09T11:54:12.143971: step 21788, loss 0.510534.
Train: 2018-08-09T11:54:12.253320: step 21789, loss 0.476576.
Train: 2018-08-09T11:54:12.362672: step 21790, loss 0.560309.
Test: 2018-08-09T11:54:12.846901: step 21790, loss 0.546147.
Train: 2018-08-09T11:54:12.956280: step 21791, loss 0.532033.
Train: 2018-08-09T11:54:13.065629: step 21792, loss 0.527975.
Train: 2018-08-09T11:54:13.159356: step 21793, loss 0.679768.
Train: 2018-08-09T11:54:13.268706: step 21794, loss 0.427198.
Train: 2018-08-09T11:54:13.378056: step 21795, loss 0.455588.
Train: 2018-08-09T11:54:13.471783: step 21796, loss 0.561563.
Train: 2018-08-09T11:54:13.581132: step 21797, loss 0.577547.
Train: 2018-08-09T11:54:13.674860: step 21798, loss 0.547167.
Train: 2018-08-09T11:54:13.784210: step 21799, loss 0.472948.
Train: 2018-08-09T11:54:13.893559: step 21800, loss 0.519488.
Test: 2018-08-09T11:54:14.395707: step 21800, loss 0.548446.
Train: 2018-08-09T11:54:14.983747: step 21801, loss 0.589447.
Train: 2018-08-09T11:54:15.077443: step 21802, loss 0.488925.
Train: 2018-08-09T11:54:15.186823: step 21803, loss 0.527955.
Train: 2018-08-09T11:54:15.296143: step 21804, loss 0.626176.
Train: 2018-08-09T11:54:15.389870: step 21805, loss 0.486001.
Train: 2018-08-09T11:54:15.499249: step 21806, loss 0.515856.
Train: 2018-08-09T11:54:15.592977: step 21807, loss 0.571016.
Train: 2018-08-09T11:54:15.702329: step 21808, loss 0.568866.
Train: 2018-08-09T11:54:15.796054: step 21809, loss 0.577982.
Train: 2018-08-09T11:54:15.907611: step 21810, loss 0.542343.
Test: 2018-08-09T11:54:16.395865: step 21810, loss 0.546765.
Train: 2018-08-09T11:54:16.505214: step 21811, loss 0.510189.
Train: 2018-08-09T11:54:16.598972: step 21812, loss 0.560832.
Train: 2018-08-09T11:54:16.708322: step 21813, loss 0.477878.
Train: 2018-08-09T11:54:16.817153: step 21814, loss 0.665687.
Train: 2018-08-09T11:54:16.907398: step 21815, loss 0.665778.
Train: 2018-08-09T11:54:17.016748: step 21816, loss 0.595599.
Train: 2018-08-09T11:54:17.110446: step 21817, loss 0.460988.
Train: 2018-08-09T11:54:17.219795: step 21818, loss 0.530313.
Train: 2018-08-09T11:54:17.329144: step 21819, loss 0.512698.
Train: 2018-08-09T11:54:17.422902: step 21820, loss 0.494133.
Test: 2018-08-09T11:54:17.922756: step 21820, loss 0.550399.
Train: 2018-08-09T11:54:18.032133: step 21821, loss 0.546155.
Train: 2018-08-09T11:54:18.125830: step 21822, loss 0.528917.
Train: 2018-08-09T11:54:18.235211: step 21823, loss 0.424888.
Train: 2018-08-09T11:54:18.328938: step 21824, loss 0.565702.
Train: 2018-08-09T11:54:18.438287: step 21825, loss 0.558945.
Train: 2018-08-09T11:54:18.532015: step 21826, loss 0.614165.
Train: 2018-08-09T11:54:18.641365: step 21827, loss 0.546254.
Train: 2018-08-09T11:54:18.735092: step 21828, loss 0.511675.
Train: 2018-08-09T11:54:18.844411: step 21829, loss 0.57985.
Train: 2018-08-09T11:54:18.939626: step 21830, loss 0.560779.
Test: 2018-08-09T11:54:19.439512: step 21830, loss 0.548829.
Train: 2018-08-09T11:54:19.533236: step 21831, loss 0.63105.
Train: 2018-08-09T11:54:19.642585: step 21832, loss 0.559279.
Train: 2018-08-09T11:54:19.736284: step 21833, loss 0.597819.
Train: 2018-08-09T11:54:19.845662: step 21834, loss 0.650184.
Train: 2018-08-09T11:54:19.939391: step 21835, loss 0.613824.
Train: 2018-08-09T11:54:20.048739: step 21836, loss 0.477357.
Train: 2018-08-09T11:54:20.158089: step 21837, loss 0.512622.
Train: 2018-08-09T11:54:20.251829: step 21838, loss 0.528981.
Train: 2018-08-09T11:54:20.361166: step 21839, loss 0.54503.
Train: 2018-08-09T11:54:20.454888: step 21840, loss 0.513119.
Test: 2018-08-09T11:54:20.957036: step 21840, loss 0.54858.
Train: 2018-08-09T11:54:21.066384: step 21841, loss 0.611023.
Train: 2018-08-09T11:54:21.160146: step 21842, loss 0.545868.
Train: 2018-08-09T11:54:21.269491: step 21843, loss 0.560477.
Train: 2018-08-09T11:54:21.363219: step 21844, loss 0.563486.
Train: 2018-08-09T11:54:21.472569: step 21845, loss 0.579488.
Train: 2018-08-09T11:54:21.566296: step 21846, loss 0.544599.
Train: 2018-08-09T11:54:21.675647: step 21847, loss 0.54544.
Train: 2018-08-09T11:54:21.769374: step 21848, loss 0.56043.
Train: 2018-08-09T11:54:21.878693: step 21849, loss 0.527712.
Train: 2018-08-09T11:54:21.972451: step 21850, loss 0.62971.
Test: 2018-08-09T11:54:22.472304: step 21850, loss 0.548582.
Train: 2018-08-09T11:54:22.566031: step 21851, loss 0.577461.
Train: 2018-08-09T11:54:22.675410: step 21852, loss 0.612072.
Train: 2018-08-09T11:54:22.769138: step 21853, loss 0.4781.
Train: 2018-08-09T11:54:22.878458: step 21854, loss 0.628573.
Train: 2018-08-09T11:54:22.974513: step 21855, loss 0.562273.
Train: 2018-08-09T11:54:23.083862: step 21856, loss 0.56345.
Train: 2018-08-09T11:54:23.177590: step 21857, loss 0.512669.
Train: 2018-08-09T11:54:23.286909: step 21858, loss 0.628609.
Train: 2018-08-09T11:54:23.380667: step 21859, loss 0.546019.
Train: 2018-08-09T11:54:23.490016: step 21860, loss 0.612626.
Test: 2018-08-09T11:54:23.989870: step 21860, loss 0.547431.
Train: 2018-08-09T11:54:24.083597: step 21861, loss 0.46269.
Train: 2018-08-09T11:54:24.192976: step 21862, loss 0.611134.
Train: 2018-08-09T11:54:24.286674: step 21863, loss 0.54558.
Train: 2018-08-09T11:54:24.396053: step 21864, loss 0.593297.
Train: 2018-08-09T11:54:24.489751: step 21865, loss 0.610109.
Train: 2018-08-09T11:54:24.599100: step 21866, loss 0.613499.
Train: 2018-08-09T11:54:24.692858: step 21867, loss 0.642588.
Train: 2018-08-09T11:54:24.786586: step 21868, loss 0.497639.
Train: 2018-08-09T11:54:24.895935: step 21869, loss 0.614641.
Train: 2018-08-09T11:54:24.991875: step 21870, loss 0.481034.
Test: 2018-08-09T11:54:25.491759: step 21870, loss 0.549137.
Train: 2018-08-09T11:54:25.585486: step 21871, loss 0.512438.
Train: 2018-08-09T11:54:25.694834: step 21872, loss 0.513369.
Train: 2018-08-09T11:54:25.788592: step 21873, loss 0.61236.
Train: 2018-08-09T11:54:25.897941: step 21874, loss 0.561663.
Train: 2018-08-09T11:54:25.991672: step 21875, loss 0.59597.
Train: 2018-08-09T11:54:26.101021: step 21876, loss 0.480682.
Train: 2018-08-09T11:54:26.194746: step 21877, loss 0.482185.
Train: 2018-08-09T11:54:26.304095: step 21878, loss 0.548886.
Train: 2018-08-09T11:54:26.397824: step 21879, loss 0.577544.
Train: 2018-08-09T11:54:26.507177: step 21880, loss 0.513746.
Test: 2018-08-09T11:54:26.993702: step 21880, loss 0.548916.
Train: 2018-08-09T11:54:27.103080: step 21881, loss 0.681609.
Train: 2018-08-09T11:54:27.196809: step 21882, loss 0.711046.
Train: 2018-08-09T11:54:27.306156: step 21883, loss 0.562614.
Train: 2018-08-09T11:54:27.399879: step 21884, loss 0.612207.
Train: 2018-08-09T11:54:27.509234: step 21885, loss 0.577519.
Train: 2018-08-09T11:54:27.602961: step 21886, loss 0.515242.
Train: 2018-08-09T11:54:27.712311: step 21887, loss 0.561734.
Train: 2018-08-09T11:54:27.806039: step 21888, loss 0.547925.
Train: 2018-08-09T11:54:27.899767: step 21889, loss 0.562704.
Train: 2018-08-09T11:54:28.009116: step 21890, loss 0.562385.
Test: 2018-08-09T11:54:28.508969: step 21890, loss 0.54866.
Train: 2018-08-09T11:54:28.614902: step 21891, loss 0.545767.
Train: 2018-08-09T11:54:28.713753: step 21892, loss 0.626245.
Train: 2018-08-09T11:54:28.807481: step 21893, loss 0.516381.
Train: 2018-08-09T11:54:28.901209: step 21894, loss 0.515464.
Train: 2018-08-09T11:54:29.010528: step 21895, loss 0.563076.
Train: 2018-08-09T11:54:29.104288: step 21896, loss 0.420147.
Train: 2018-08-09T11:54:29.213605: step 21897, loss 0.579843.
Train: 2018-08-09T11:54:29.307364: step 21898, loss 0.481548.
Train: 2018-08-09T11:54:29.401092: step 21899, loss 0.546287.
Train: 2018-08-09T11:54:29.510412: step 21900, loss 0.530425.
Test: 2018-08-09T11:54:30.010293: step 21900, loss 0.549201.
Train: 2018-08-09T11:54:30.572690: step 21901, loss 0.578518.
Train: 2018-08-09T11:54:30.682040: step 21902, loss 0.513759.
Train: 2018-08-09T11:54:30.775770: step 21903, loss 0.595021.
Train: 2018-08-09T11:54:30.885088: step 21904, loss 0.561246.
Train: 2018-08-09T11:54:30.981061: step 21905, loss 0.645346.
Train: 2018-08-09T11:54:31.074789: step 21906, loss 0.662681.
Train: 2018-08-09T11:54:31.184138: step 21907, loss 0.713495.
Train: 2018-08-09T11:54:31.277866: step 21908, loss 0.629219.
Train: 2018-08-09T11:54:31.387215: step 21909, loss 0.593848.
Train: 2018-08-09T11:54:31.480942: step 21910, loss 0.481781.
Test: 2018-08-09T11:54:31.980825: step 21910, loss 0.548163.
Train: 2018-08-09T11:54:32.074553: step 21911, loss 0.59519.
Train: 2018-08-09T11:54:32.183932: step 21912, loss 0.466496.
Train: 2018-08-09T11:54:32.277662: step 21913, loss 0.546516.
Train: 2018-08-09T11:54:32.387011: step 21914, loss 0.545712.
Train: 2018-08-09T11:54:32.480737: step 21915, loss 0.70792.
Train: 2018-08-09T11:54:32.590086: step 21916, loss 0.660482.
Train: 2018-08-09T11:54:32.683814: step 21917, loss 0.612422.
Train: 2018-08-09T11:54:32.793164: step 21918, loss 0.514822.
Train: 2018-08-09T11:54:32.886893: step 21919, loss 0.483658.
Train: 2018-08-09T11:54:32.983106: step 21920, loss 0.626015.
Test: 2018-08-09T11:54:33.482984: step 21920, loss 0.5519.
Train: 2018-08-09T11:54:33.592340: step 21921, loss 0.578776.
Train: 2018-08-09T11:54:33.686065: step 21922, loss 0.657699.
Train: 2018-08-09T11:54:33.779793: step 21923, loss 0.563376.
Train: 2018-08-09T11:54:33.889142: step 21924, loss 0.500878.
Train: 2018-08-09T11:54:33.982871: step 21925, loss 0.59462.
Train: 2018-08-09T11:54:34.092219: step 21926, loss 0.532409.
Train: 2018-08-09T11:54:34.185947: step 21927, loss 0.547764.
Train: 2018-08-09T11:54:34.295267: step 21928, loss 0.516764.
Train: 2018-08-09T11:54:34.389027: step 21929, loss 0.703424.
Train: 2018-08-09T11:54:34.482753: step 21930, loss 0.563504.
Test: 2018-08-09T11:54:34.983228: step 21930, loss 0.548882.
Train: 2018-08-09T11:54:35.092576: step 21931, loss 0.532575.
Train: 2018-08-09T11:54:35.186334: step 21932, loss 0.548089.
Train: 2018-08-09T11:54:35.280062: step 21933, loss 0.594459.
Train: 2018-08-09T11:54:35.389411: step 21934, loss 0.579.
Train: 2018-08-09T11:54:35.483140: step 21935, loss 0.548063.
Train: 2018-08-09T11:54:35.592489: step 21936, loss 0.563449.
Train: 2018-08-09T11:54:35.686216: step 21937, loss 0.656352.
Train: 2018-08-09T11:54:35.795567: step 21938, loss 0.578926.
Train: 2018-08-09T11:54:35.889293: step 21939, loss 0.548098.
Train: 2018-08-09T11:54:35.998613: step 21940, loss 0.625232.
Test: 2018-08-09T11:54:36.482904: step 21940, loss 0.552012.
Train: 2018-08-09T11:54:36.592241: step 21941, loss 0.594418.
Train: 2018-08-09T11:54:36.701603: step 21942, loss 0.532804.
Train: 2018-08-09T11:54:36.795332: step 21943, loss 0.60969.
Train: 2018-08-09T11:54:36.904648: step 21944, loss 0.548258.
Train: 2018-08-09T11:54:36.998407: step 21945, loss 0.548285.
Train: 2018-08-09T11:54:37.092105: step 21946, loss 0.594316.
Train: 2018-08-09T11:54:37.201455: step 21947, loss 0.563609.
Train: 2018-08-09T11:54:37.295214: step 21948, loss 0.517575.
Train: 2018-08-09T11:54:37.404532: step 21949, loss 0.517537.
Train: 2018-08-09T11:54:37.498259: step 21950, loss 0.501939.
Test: 2018-08-09T11:54:38.000433: step 21950, loss 0.548935.
Train: 2018-08-09T11:54:38.094191: step 21951, loss 0.625243.
Train: 2018-08-09T11:54:38.203534: step 21952, loss 0.548248.
Train: 2018-08-09T11:54:38.297268: step 21953, loss 0.547992.
Train: 2018-08-09T11:54:38.390996: step 21954, loss 0.500845.
Train: 2018-08-09T11:54:38.500345: step 21955, loss 0.469134.
Train: 2018-08-09T11:54:38.594044: step 21956, loss 0.626658.
Train: 2018-08-09T11:54:38.703426: step 21957, loss 0.563367.
Train: 2018-08-09T11:54:38.797153: step 21958, loss 0.578014.
Train: 2018-08-09T11:54:38.906499: step 21959, loss 0.578069.
Train: 2018-08-09T11:54:39.000228: step 21960, loss 0.594275.
Test: 2018-08-09T11:54:39.500082: step 21960, loss 0.54954.
Train: 2018-08-09T11:54:39.593837: step 21961, loss 0.549547.
Train: 2018-08-09T11:54:39.703190: step 21962, loss 0.531111.
Train: 2018-08-09T11:54:39.796915: step 21963, loss 0.481985.
Train: 2018-08-09T11:54:39.910899: step 21964, loss 0.546275.
Train: 2018-08-09T11:54:40.004630: step 21965, loss 0.528262.
Train: 2018-08-09T11:54:40.113976: step 21966, loss 0.664824.
Train: 2018-08-09T11:54:40.207704: step 21967, loss 0.615077.
Train: 2018-08-09T11:54:40.301432: step 21968, loss 0.56447.
Train: 2018-08-09T11:54:40.410782: step 21969, loss 0.660332.
Train: 2018-08-09T11:54:40.504480: step 21970, loss 0.44872.
Test: 2018-08-09T11:54:41.004363: step 21970, loss 0.549357.
Train: 2018-08-09T11:54:41.113711: step 21971, loss 0.61146.
Train: 2018-08-09T11:54:41.207468: step 21972, loss 0.57936.
Train: 2018-08-09T11:54:41.316788: step 21973, loss 0.528593.
Train: 2018-08-09T11:54:41.410545: step 21974, loss 0.594755.
Train: 2018-08-09T11:54:41.519895: step 21975, loss 0.530666.
Train: 2018-08-09T11:54:41.613622: step 21976, loss 0.59498.
Train: 2018-08-09T11:54:41.722944: step 21977, loss 0.530854.
Train: 2018-08-09T11:54:41.816701: step 21978, loss 0.611067.
Train: 2018-08-09T11:54:41.912760: step 21979, loss 0.579057.
Train: 2018-08-09T11:54:42.022109: step 21980, loss 0.626741.
Test: 2018-08-09T11:54:42.521994: step 21980, loss 0.547925.
Train: 2018-08-09T11:54:42.615720: step 21981, loss 0.48301.
Train: 2018-08-09T11:54:42.725099: step 21982, loss 0.578803.
Train: 2018-08-09T11:54:42.818828: step 21983, loss 0.578827.
Train: 2018-08-09T11:54:42.928176: step 21984, loss 0.594803.
Train: 2018-08-09T11:54:43.021905: step 21985, loss 0.499128.
Train: 2018-08-09T11:54:43.131254: step 21986, loss 0.579026.
Train: 2018-08-09T11:54:43.224982: step 21987, loss 0.546891.
Train: 2018-08-09T11:54:43.334334: step 21988, loss 0.562948.
Train: 2018-08-09T11:54:43.428058: step 21989, loss 0.546828.
Train: 2018-08-09T11:54:43.537408: step 21990, loss 0.46683.
Test: 2018-08-09T11:54:44.023923: step 21990, loss 0.548974.
Train: 2018-08-09T11:54:44.133302: step 21991, loss 0.723155.
Train: 2018-08-09T11:54:44.227030: step 21992, loss 0.482209.
Train: 2018-08-09T11:54:44.336379: step 21993, loss 0.482598.
Train: 2018-08-09T11:54:44.430076: step 21994, loss 0.578912.
Train: 2018-08-09T11:54:44.539457: step 21995, loss 0.643757.
Train: 2018-08-09T11:54:44.633186: step 21996, loss 0.54714.
Train: 2018-08-09T11:54:44.742502: step 21997, loss 0.53016.
Train: 2018-08-09T11:54:44.836261: step 21998, loss 0.644125.
Train: 2018-08-09T11:54:44.945610: step 21999, loss 0.530936.
Train: 2018-08-09T11:54:45.054959: step 22000, loss 0.465285.
Test: 2018-08-09T11:54:45.539192: step 22000, loss 0.548678.
Train: 2018-08-09T11:54:46.072822: step 22001, loss 0.611785.
Train: 2018-08-09T11:54:46.182165: step 22002, loss 0.57929.
Train: 2018-08-09T11:54:46.275899: step 22003, loss 0.514006.
Train: 2018-08-09T11:54:46.369627: step 22004, loss 0.595159.
Train: 2018-08-09T11:54:46.478945: step 22005, loss 0.562853.
Train: 2018-08-09T11:54:46.588320: step 22006, loss 0.595058.
Train: 2018-08-09T11:54:46.682053: step 22007, loss 0.611257.
Train: 2018-08-09T11:54:46.791405: step 22008, loss 0.546532.
Train: 2018-08-09T11:54:46.885100: step 22009, loss 0.546466.
Train: 2018-08-09T11:54:46.994479: step 22010, loss 0.497767.
Test: 2018-08-09T11:54:47.478712: step 22010, loss 0.548067.
Train: 2018-08-09T11:54:47.584045: step 22011, loss 0.497616.
Train: 2018-08-09T11:54:47.693365: step 22012, loss 0.513994.
Train: 2018-08-09T11:54:47.787118: step 22013, loss 0.661225.
Train: 2018-08-09T11:54:47.896472: step 22014, loss 0.578565.
Train: 2018-08-09T11:54:47.995596: step 22015, loss 0.595582.
Train: 2018-08-09T11:54:48.104944: step 22016, loss 0.579504.
Train: 2018-08-09T11:54:48.198672: step 22017, loss 0.562137.
Train: 2018-08-09T11:54:48.307993: step 22018, loss 0.563197.
Train: 2018-08-09T11:54:48.401750: step 22019, loss 0.497683.
Train: 2018-08-09T11:54:48.511100: step 22020, loss 0.627951.
Test: 2018-08-09T11:54:49.010951: step 22020, loss 0.552323.
Train: 2018-08-09T11:54:49.104710: step 22021, loss 0.676604.
Train: 2018-08-09T11:54:49.214059: step 22022, loss 0.56266.
Train: 2018-08-09T11:54:49.307790: step 22023, loss 0.530335.
Train: 2018-08-09T11:54:49.417138: step 22024, loss 0.562728.
Train: 2018-08-09T11:54:49.510863: step 22025, loss 0.627367.
Train: 2018-08-09T11:54:49.620213: step 22026, loss 0.562748.
Train: 2018-08-09T11:54:49.729562: step 22027, loss 0.530591.
Train: 2018-08-09T11:54:49.835571: step 22028, loss 0.498452.
Train: 2018-08-09T11:54:49.929842: step 22029, loss 0.546693.
Train: 2018-08-09T11:54:50.039192: step 22030, loss 0.466195.
Test: 2018-08-09T11:54:50.539043: step 22030, loss 0.550066.
Train: 2018-08-09T11:54:50.632770: step 22031, loss 0.498212.
Train: 2018-08-09T11:54:50.742152: step 22032, loss 0.497976.
Train: 2018-08-09T11:54:50.835849: step 22033, loss 0.579017.
Train: 2018-08-09T11:54:50.950673: step 22034, loss 0.6281.
Train: 2018-08-09T11:54:51.044402: step 22035, loss 0.578859.
Train: 2018-08-09T11:54:51.153750: step 22036, loss 0.595375.
Train: 2018-08-09T11:54:51.247448: step 22037, loss 0.529925.
Train: 2018-08-09T11:54:51.356827: step 22038, loss 0.52966.
Train: 2018-08-09T11:54:51.450524: step 22039, loss 0.611835.
Train: 2018-08-09T11:54:51.559905: step 22040, loss 0.496732.
Test: 2018-08-09T11:54:52.059758: step 22040, loss 0.550797.
Train: 2018-08-09T11:54:52.153515: step 22041, loss 0.579125.
Train: 2018-08-09T11:54:52.262864: step 22042, loss 0.529019.
Train: 2018-08-09T11:54:52.356592: step 22043, loss 0.595573.
Train: 2018-08-09T11:54:52.465911: step 22044, loss 0.662644.
Train: 2018-08-09T11:54:52.575284: step 22045, loss 0.578306.
Train: 2018-08-09T11:54:52.669019: step 22046, loss 0.612322.
Train: 2018-08-09T11:54:52.778370: step 22047, loss 0.561947.
Train: 2018-08-09T11:54:52.872095: step 22048, loss 0.545534.
Train: 2018-08-09T11:54:52.981415: step 22049, loss 0.480596.
Train: 2018-08-09T11:54:53.090794: step 22050, loss 0.479877.
Test: 2018-08-09T11:54:53.590646: step 22050, loss 0.550658.
Train: 2018-08-09T11:54:53.684400: step 22051, loss 0.56149.
Train: 2018-08-09T11:54:53.793723: step 22052, loss 0.561936.
Train: 2018-08-09T11:54:53.903081: step 22053, loss 0.564364.
Train: 2018-08-09T11:54:53.998148: step 22054, loss 0.546318.
Train: 2018-08-09T11:54:54.107529: step 22055, loss 0.561143.
Train: 2018-08-09T11:54:54.216845: step 22056, loss 0.52266.
Train: 2018-08-09T11:54:54.310604: step 22057, loss 0.509912.
Train: 2018-08-09T11:54:54.419952: step 22058, loss 0.509002.
Train: 2018-08-09T11:54:54.513681: step 22059, loss 0.526861.
Train: 2018-08-09T11:54:54.623030: step 22060, loss 0.623452.
Test: 2018-08-09T11:54:55.107261: step 22060, loss 0.547086.
Train: 2018-08-09T11:54:55.216640: step 22061, loss 0.602584.
Train: 2018-08-09T11:54:55.325965: step 22062, loss 0.602929.
Train: 2018-08-09T11:54:55.419716: step 22063, loss 0.523043.
Train: 2018-08-09T11:54:55.529066: step 22064, loss 0.59775.
Train: 2018-08-09T11:54:55.622795: step 22065, loss 0.470798.
Train: 2018-08-09T11:54:55.732146: step 22066, loss 0.615139.
Train: 2018-08-09T11:54:55.825871: step 22067, loss 0.512811.
Train: 2018-08-09T11:54:55.936618: step 22068, loss 0.650927.
Train: 2018-08-09T11:54:56.045938: step 22069, loss 0.577187.
Train: 2018-08-09T11:54:56.139690: step 22070, loss 0.561583.
Test: 2018-08-09T11:54:56.639548: step 22070, loss 0.550376.
Train: 2018-08-09T11:54:56.748927: step 22071, loss 0.595289.
Train: 2018-08-09T11:54:56.858245: step 22072, loss 0.578517.
Train: 2018-08-09T11:54:56.952004: step 22073, loss 0.497378.
Train: 2018-08-09T11:54:57.061323: step 22074, loss 0.478022.
Train: 2018-08-09T11:54:57.155081: step 22075, loss 0.56141.
Train: 2018-08-09T11:54:57.264433: step 22076, loss 0.595337.
Train: 2018-08-09T11:54:57.373749: step 22077, loss 0.595705.
Train: 2018-08-09T11:54:57.467511: step 22078, loss 0.496635.
Train: 2018-08-09T11:54:57.576827: step 22079, loss 0.595802.
Train: 2018-08-09T11:54:57.686176: step 22080, loss 0.563323.
Test: 2018-08-09T11:54:58.186731: step 22080, loss 0.548284.
Train: 2018-08-09T11:54:58.280488: step 22081, loss 0.57667.
Train: 2018-08-09T11:54:58.389837: step 22082, loss 0.51171.
Train: 2018-08-09T11:54:58.499187: step 22083, loss 0.545063.
Train: 2018-08-09T11:54:58.592884: step 22084, loss 0.563652.
Train: 2018-08-09T11:54:58.702264: step 22085, loss 0.648373.
Train: 2018-08-09T11:54:58.811614: step 22086, loss 0.513919.
Train: 2018-08-09T11:54:58.905312: step 22087, loss 0.544858.
Train: 2018-08-09T11:54:59.014660: step 22088, loss 0.5298.
Train: 2018-08-09T11:54:59.124010: step 22089, loss 0.563196.
Train: 2018-08-09T11:54:59.233389: step 22090, loss 0.61389.
Test: 2018-08-09T11:54:59.717621: step 22090, loss 0.546381.
Train: 2018-08-09T11:54:59.826969: step 22091, loss 0.646273.
Train: 2018-08-09T11:54:59.920729: step 22092, loss 0.612452.
Train: 2018-08-09T11:55:00.030077: step 22093, loss 0.562645.
Train: 2018-08-09T11:55:00.139425: step 22094, loss 0.595103.
Train: 2018-08-09T11:55:00.248774: step 22095, loss 0.626822.
Train: 2018-08-09T11:55:00.342502: step 22096, loss 0.530968.
Train: 2018-08-09T11:55:00.451851: step 22097, loss 0.499141.
Train: 2018-08-09T11:55:00.561202: step 22098, loss 0.498873.
Train: 2018-08-09T11:55:00.654929: step 22099, loss 0.51533.
Train: 2018-08-09T11:55:00.764278: step 22100, loss 0.57892.
Test: 2018-08-09T11:55:01.266400: step 22100, loss 0.548551.
Train: 2018-08-09T11:55:01.837367: step 22101, loss 0.61087.
Train: 2018-08-09T11:55:01.931096: step 22102, loss 0.626526.
Train: 2018-08-09T11:55:02.040445: step 22103, loss 0.499147.
Train: 2018-08-09T11:55:02.149795: step 22104, loss 0.547155.
Train: 2018-08-09T11:55:02.243522: step 22105, loss 0.403534.
Train: 2018-08-09T11:55:02.352871: step 22106, loss 0.546925.
Train: 2018-08-09T11:55:02.462221: step 22107, loss 0.626879.
Train: 2018-08-09T11:55:02.555919: step 22108, loss 0.530591.
Train: 2018-08-09T11:55:02.665268: step 22109, loss 0.594907.
Train: 2018-08-09T11:55:02.774616: step 22110, loss 0.57887.
Test: 2018-08-09T11:55:03.274500: step 22110, loss 0.548218.
Train: 2018-08-09T11:55:03.368227: step 22111, loss 0.4496.
Train: 2018-08-09T11:55:03.477577: step 22112, loss 0.659883.
Train: 2018-08-09T11:55:03.586926: step 22113, loss 0.562653.
Train: 2018-08-09T11:55:03.696305: step 22114, loss 0.562711.
Train: 2018-08-09T11:55:03.790003: step 22115, loss 0.513926.
Train: 2018-08-09T11:55:03.899382: step 22116, loss 0.57881.
Train: 2018-08-09T11:55:04.008731: step 22117, loss 0.595189.
Train: 2018-08-09T11:55:04.102459: step 22118, loss 0.627923.
Train: 2018-08-09T11:55:04.211808: step 22119, loss 0.513844.
Train: 2018-08-09T11:55:04.321157: step 22120, loss 0.513533.
Test: 2018-08-09T11:55:04.821010: step 22120, loss 0.550392.
Train: 2018-08-09T11:55:04.916158: step 22121, loss 0.529898.
Train: 2018-08-09T11:55:05.025537: step 22122, loss 0.562887.
Train: 2018-08-09T11:55:05.134890: step 22123, loss 0.644526.
Train: 2018-08-09T11:55:05.228615: step 22124, loss 0.628274.
Train: 2018-08-09T11:55:05.337933: step 22125, loss 0.693827.
Train: 2018-08-09T11:55:05.447313: step 22126, loss 0.546375.
Train: 2018-08-09T11:55:05.541011: step 22127, loss 0.595253.
Train: 2018-08-09T11:55:05.650361: step 22128, loss 0.611215.
Train: 2018-08-09T11:55:05.759743: step 22129, loss 0.530503.
Train: 2018-08-09T11:55:05.853467: step 22130, loss 0.530626.
Test: 2018-08-09T11:55:06.353321: step 22130, loss 0.548973.
Train: 2018-08-09T11:55:06.462669: step 22131, loss 0.450391.
Train: 2018-08-09T11:55:06.572018: step 22132, loss 0.466382.
Train: 2018-08-09T11:55:06.681367: step 22133, loss 0.578892.
Train: 2018-08-09T11:55:06.790747: step 22134, loss 0.578918.
Train: 2018-08-09T11:55:06.884478: step 22135, loss 0.627342.
Train: 2018-08-09T11:55:06.996113: step 22136, loss 0.611171.
Train: 2018-08-09T11:55:07.105457: step 22137, loss 0.530488.
Train: 2018-08-09T11:55:07.214800: step 22138, loss 0.546626.
Train: 2018-08-09T11:55:07.308541: step 22139, loss 0.59502.
Train: 2018-08-09T11:55:07.417888: step 22140, loss 0.562749.
Test: 2018-08-09T11:55:07.917742: step 22140, loss 0.548861.
Train: 2018-08-09T11:55:08.027121: step 22141, loss 0.595003.
Train: 2018-08-09T11:55:08.136471: step 22142, loss 0.643352.
Train: 2018-08-09T11:55:08.230198: step 22143, loss 0.41799.
Train: 2018-08-09T11:55:08.355140: step 22144, loss 0.562776.
Train: 2018-08-09T11:55:08.448896: step 22145, loss 0.627216.
Train: 2018-08-09T11:55:08.558247: step 22146, loss 0.514447.
Train: 2018-08-09T11:55:08.667565: step 22147, loss 0.433837.
Train: 2018-08-09T11:55:08.776914: step 22148, loss 0.498089.
Train: 2018-08-09T11:55:08.886293: step 22149, loss 0.546485.
Train: 2018-08-09T11:55:08.982426: step 22150, loss 0.595165.
Test: 2018-08-09T11:55:09.482278: step 22150, loss 0.547333.
Train: 2018-08-09T11:55:09.591658: step 22151, loss 0.595252.
Train: 2018-08-09T11:55:09.685385: step 22152, loss 0.513582.
Train: 2018-08-09T11:55:09.794734: step 22153, loss 0.579.
Train: 2018-08-09T11:55:09.904084: step 22154, loss 0.546131.
Train: 2018-08-09T11:55:10.013403: step 22155, loss 0.578889.
Train: 2018-08-09T11:55:10.107131: step 22156, loss 0.595507.
Train: 2018-08-09T11:55:10.216510: step 22157, loss 0.595402.
Train: 2018-08-09T11:55:10.325859: step 22158, loss 0.57906.
Train: 2018-08-09T11:55:10.435209: step 22159, loss 0.579129.
Train: 2018-08-09T11:55:10.528940: step 22160, loss 0.463645.
Test: 2018-08-09T11:55:11.030124: step 22160, loss 0.545821.
Train: 2018-08-09T11:55:11.139471: step 22161, loss 0.628404.
Train: 2018-08-09T11:55:11.248821: step 22162, loss 0.49656.
Train: 2018-08-09T11:55:11.358202: step 22163, loss 0.562391.
Train: 2018-08-09T11:55:11.467548: step 22164, loss 0.512973.
Train: 2018-08-09T11:55:11.576900: step 22165, loss 0.595564.
Train: 2018-08-09T11:55:11.670627: step 22166, loss 0.645209.
Train: 2018-08-09T11:55:11.779947: step 22167, loss 0.52917.
Train: 2018-08-09T11:55:11.889296: step 22168, loss 0.479809.
Train: 2018-08-09T11:55:11.983055: step 22169, loss 0.56231.
Train: 2018-08-09T11:55:12.092404: step 22170, loss 0.578609.
Test: 2018-08-09T11:55:12.592255: step 22170, loss 0.548084.
Train: 2018-08-09T11:55:12.701634: step 22171, loss 0.428569.
Train: 2018-08-09T11:55:12.795362: step 22172, loss 0.579077.
Train: 2018-08-09T11:55:12.912928: step 22173, loss 0.562147.
Train: 2018-08-09T11:55:13.014697: step 22174, loss 0.562393.
Train: 2018-08-09T11:55:13.108453: step 22175, loss 0.475721.
Train: 2018-08-09T11:55:13.217803: step 22176, loss 0.718198.
Train: 2018-08-09T11:55:13.327152: step 22177, loss 0.459572.
Train: 2018-08-09T11:55:13.436501: step 22178, loss 0.614948.
Train: 2018-08-09T11:55:13.530232: step 22179, loss 0.530084.
Train: 2018-08-09T11:55:13.639581: step 22180, loss 0.666017.
Test: 2018-08-09T11:55:14.139432: step 22180, loss 0.548274.
Train: 2018-08-09T11:55:14.233158: step 22181, loss 0.54653.
Train: 2018-08-09T11:55:14.342508: step 22182, loss 0.630926.
Train: 2018-08-09T11:55:14.451887: step 22183, loss 0.646188.
Train: 2018-08-09T11:55:14.561236: step 22184, loss 0.494997.
Train: 2018-08-09T11:55:14.654967: step 22185, loss 0.579097.
Train: 2018-08-09T11:55:14.764313: step 22186, loss 0.513042.
Train: 2018-08-09T11:55:14.873662: step 22187, loss 0.397277.
Train: 2018-08-09T11:55:14.983012: step 22188, loss 0.562617.
Train: 2018-08-09T11:55:15.076709: step 22189, loss 0.512869.
Train: 2018-08-09T11:55:15.186088: step 22190, loss 0.479788.
Test: 2018-08-09T11:55:15.685941: step 22190, loss 0.54752.
Train: 2018-08-09T11:55:15.779702: step 22191, loss 0.529315.
Train: 2018-08-09T11:55:15.889019: step 22192, loss 0.562488.
Train: 2018-08-09T11:55:16.000866: step 22193, loss 0.662527.
Train: 2018-08-09T11:55:16.110186: step 22194, loss 0.645965.
Train: 2018-08-09T11:55:16.203944: step 22195, loss 0.5791.
Train: 2018-08-09T11:55:16.313264: step 22196, loss 0.595748.
Train: 2018-08-09T11:55:16.422645: step 22197, loss 0.562445.
Train: 2018-08-09T11:55:16.531994: step 22198, loss 0.662118.
Train: 2018-08-09T11:55:16.625719: step 22199, loss 0.545909.
Train: 2018-08-09T11:55:16.735069: step 22200, loss 0.727768.
Test: 2018-08-09T11:55:17.234922: step 22200, loss 0.548361.
Train: 2018-08-09T11:55:17.844182: step 22201, loss 0.529628.
Train: 2018-08-09T11:55:17.954987: step 22202, loss 0.51339.
Train: 2018-08-09T11:55:18.064305: step 22203, loss 0.578943.
Train: 2018-08-09T11:55:18.173656: step 22204, loss 0.611544.
Train: 2018-08-09T11:55:18.267413: step 22205, loss 0.562655.
Train: 2018-08-09T11:55:18.376763: step 22206, loss 0.481624.
Train: 2018-08-09T11:55:18.486111: step 22207, loss 0.546509.
Train: 2018-08-09T11:55:18.595432: step 22208, loss 0.562712.
Train: 2018-08-09T11:55:18.689189: step 22209, loss 0.611225.
Train: 2018-08-09T11:55:18.798509: step 22210, loss 0.514302.
Test: 2018-08-09T11:55:19.298390: step 22210, loss 0.551285.
Train: 2018-08-09T11:55:19.407773: step 22211, loss 0.530467.
Train: 2018-08-09T11:55:19.501467: step 22212, loss 0.482038.
Train: 2018-08-09T11:55:19.610849: step 22213, loss 0.530395.
Train: 2018-08-09T11:55:19.720196: step 22214, loss 0.5789.
Train: 2018-08-09T11:55:19.829545: step 22215, loss 0.611332.
Train: 2018-08-09T11:55:19.924658: step 22216, loss 0.530224.
Train: 2018-08-09T11:55:20.034008: step 22217, loss 0.611398.
Train: 2018-08-09T11:55:20.143358: step 22218, loss 0.481406.
Train: 2018-08-09T11:55:20.237084: step 22219, loss 0.627751.
Train: 2018-08-09T11:55:20.346405: step 22220, loss 0.595208.
Test: 2018-08-09T11:55:20.837547: step 22220, loss 0.548619.
Train: 2018-08-09T11:55:20.946924: step 22221, loss 0.530099.
Train: 2018-08-09T11:55:21.056273: step 22222, loss 0.546355.
Train: 2018-08-09T11:55:21.165623: step 22223, loss 0.595216.
Train: 2018-08-09T11:55:21.259350: step 22224, loss 0.513781.
Train: 2018-08-09T11:55:21.368700: step 22225, loss 0.644141.
Train: 2018-08-09T11:55:21.478049: step 22226, loss 0.562642.
Train: 2018-08-09T11:55:21.587367: step 22227, loss 0.611504.
Train: 2018-08-09T11:55:21.696748: step 22228, loss 0.481379.
Train: 2018-08-09T11:55:21.790475: step 22229, loss 0.660204.
Train: 2018-08-09T11:55:21.899795: step 22230, loss 0.497732.
Test: 2018-08-09T11:55:22.399678: step 22230, loss 0.548696.
Train: 2018-08-09T11:55:22.493404: step 22231, loss 0.513993.
Train: 2018-08-09T11:55:22.602784: step 22232, loss 0.611387.
Train: 2018-08-09T11:55:22.712115: step 22233, loss 0.481501.
Train: 2018-08-09T11:55:22.821453: step 22234, loss 0.595162.
Train: 2018-08-09T11:55:22.918138: step 22235, loss 0.595174.
Train: 2018-08-09T11:55:23.027487: step 22236, loss 0.513878.
Train: 2018-08-09T11:55:23.136836: step 22237, loss 0.578916.
Train: 2018-08-09T11:55:23.246215: step 22238, loss 0.530087.
Train: 2018-08-09T11:55:23.355566: step 22239, loss 0.595212.
Train: 2018-08-09T11:55:23.449262: step 22240, loss 0.513742.
Test: 2018-08-09T11:55:23.949145: step 22240, loss 0.547949.
Train: 2018-08-09T11:55:24.058524: step 22241, loss 0.513681.
Train: 2018-08-09T11:55:24.167876: step 22242, loss 0.595279.
Train: 2018-08-09T11:55:24.261602: step 22243, loss 0.611665.
Train: 2018-08-09T11:55:24.370950: step 22244, loss 0.628026.
Train: 2018-08-09T11:55:24.480301: step 22245, loss 0.52991.
Train: 2018-08-09T11:55:24.589619: step 22246, loss 0.611617.
Train: 2018-08-09T11:55:24.683377: step 22247, loss 0.497331.
Train: 2018-08-09T11:55:24.792728: step 22248, loss 0.513655.
Train: 2018-08-09T11:55:24.902075: step 22249, loss 0.59528.
Train: 2018-08-09T11:55:25.011425: step 22250, loss 0.57894.
Test: 2018-08-09T11:55:25.511277: step 22250, loss 0.549148.
Train: 2018-08-09T11:55:25.605005: step 22251, loss 0.595278.
Train: 2018-08-09T11:55:25.714385: step 22252, loss 0.595262.
Train: 2018-08-09T11:55:25.823704: step 22253, loss 0.546308.
Train: 2018-08-09T11:55:25.919785: step 22254, loss 0.546326.
Train: 2018-08-09T11:55:26.029135: step 22255, loss 0.595218.
Train: 2018-08-09T11:55:26.138514: step 22256, loss 0.464955.
Train: 2018-08-09T11:55:26.232242: step 22257, loss 0.56263.
Train: 2018-08-09T11:55:26.341591: step 22258, loss 0.546317.
Train: 2018-08-09T11:55:26.450912: step 22259, loss 0.67685.
Train: 2018-08-09T11:55:26.544669: step 22260, loss 0.595231.
Test: 2018-08-09T11:55:27.044522: step 22260, loss 0.549845.
Train: 2018-08-09T11:55:27.153900: step 22261, loss 0.546356.
Train: 2018-08-09T11:55:27.263249: step 22262, loss 0.562649.
Train: 2018-08-09T11:55:27.356977: step 22263, loss 0.562659.
Train: 2018-08-09T11:55:27.466326: step 22264, loss 0.562668.
Train: 2018-08-09T11:55:27.575676: step 22265, loss 0.513991.
Train: 2018-08-09T11:55:27.669374: step 22266, loss 0.578906.
Train: 2018-08-09T11:55:27.778723: step 22267, loss 0.595133.
Train: 2018-08-09T11:55:27.888073: step 22268, loss 0.530241.
Train: 2018-08-09T11:55:27.984144: step 22269, loss 0.59512.
Train: 2018-08-09T11:55:28.093493: step 22270, loss 0.530257.
Test: 2018-08-09T11:55:28.593393: step 22270, loss 0.549331.
Train: 2018-08-09T11:55:28.702754: step 22271, loss 0.578899.
Train: 2018-08-09T11:55:28.796452: step 22272, loss 0.643767.
Train: 2018-08-09T11:55:28.905801: step 22273, loss 0.54651.
Train: 2018-08-09T11:55:29.015180: step 22274, loss 0.579969.
Train: 2018-08-09T11:55:29.108908: step 22275, loss 0.675838.
Train: 2018-08-09T11:55:29.218257: step 22276, loss 0.691668.
Train: 2018-08-09T11:55:29.327607: step 22277, loss 0.498672.
Train: 2018-08-09T11:55:29.421334: step 22278, loss 0.54688.
Train: 2018-08-09T11:55:29.530686: step 22279, loss 0.546952.
Train: 2018-08-09T11:55:29.640034: step 22280, loss 0.547009.
Test: 2018-08-09T11:55:30.124922: step 22280, loss 0.552261.
Train: 2018-08-09T11:55:30.234273: step 22281, loss 0.547048.
Train: 2018-08-09T11:55:30.328030: step 22282, loss 0.578858.
Train: 2018-08-09T11:55:30.437379: step 22283, loss 0.626493.
Train: 2018-08-09T11:55:30.546699: step 22284, loss 0.499605.
Train: 2018-08-09T11:55:30.656047: step 22285, loss 0.594706.
Train: 2018-08-09T11:55:30.749806: step 22286, loss 0.610521.
Train: 2018-08-09T11:55:30.859154: step 22287, loss 0.594668.
Train: 2018-08-09T11:55:30.952882: step 22288, loss 0.499954.
Train: 2018-08-09T11:55:31.062232: step 22289, loss 0.515757.
Train: 2018-08-09T11:55:31.171550: step 22290, loss 0.531505.
Test: 2018-08-09T11:55:31.655837: step 22290, loss 0.550041.
Train: 2018-08-09T11:55:31.765191: step 22291, loss 0.626278.
Train: 2018-08-09T11:55:31.874511: step 22292, loss 0.610474.
Train: 2018-08-09T11:55:31.983879: step 22293, loss 0.531472.
Train: 2018-08-09T11:55:32.077618: step 22294, loss 0.499871.
Train: 2018-08-09T11:55:32.186970: step 22295, loss 0.531408.
Train: 2018-08-09T11:55:32.296286: step 22296, loss 0.578851.
Train: 2018-08-09T11:55:32.405665: step 22297, loss 0.547115.
Train: 2018-08-09T11:55:32.561878: step 22298, loss 0.547061.
Train: 2018-08-09T11:55:32.655576: step 22299, loss 0.499178.
Train: 2018-08-09T11:55:32.764957: step 22300, loss 0.546847.
Test: 2018-08-09T11:55:33.267196: step 22300, loss 0.549594.
Train: 2018-08-09T11:55:33.860816: step 22301, loss 0.514561.
Train: 2018-08-09T11:55:33.970135: step 22302, loss 0.594815.
Train: 2018-08-09T11:55:34.079514: step 22303, loss 0.644062.
Train: 2018-08-09T11:55:34.173212: step 22304, loss 0.643857.
Train: 2018-08-09T11:55:34.282561: step 22305, loss 0.643589.
Train: 2018-08-09T11:55:34.391910: step 22306, loss 0.546328.
Train: 2018-08-09T11:55:34.485638: step 22307, loss 0.578781.
Train: 2018-08-09T11:55:34.595018: step 22308, loss 0.530736.
Train: 2018-08-09T11:55:34.704368: step 22309, loss 0.497971.
Train: 2018-08-09T11:55:34.798096: step 22310, loss 0.611465.
Test: 2018-08-09T11:55:35.299577: step 22310, loss 0.551304.
Train: 2018-08-09T11:55:35.408925: step 22311, loss 0.450117.
Train: 2018-08-09T11:55:35.502653: step 22312, loss 0.594728.
Train: 2018-08-09T11:55:35.612004: step 22313, loss 0.546477.
Train: 2018-08-09T11:55:35.705730: step 22314, loss 0.530293.
Train: 2018-08-09T11:55:35.815080: step 22315, loss 0.662253.
Train: 2018-08-09T11:55:35.924429: step 22316, loss 0.643762.
Train: 2018-08-09T11:55:36.018157: step 22317, loss 0.546931.
Train: 2018-08-09T11:55:36.127506: step 22318, loss 0.514314.
Train: 2018-08-09T11:55:36.236856: step 22319, loss 0.610998.
Train: 2018-08-09T11:55:36.330584: step 22320, loss 0.59499.
Test: 2018-08-09T11:55:36.830435: step 22320, loss 0.547763.
Train: 2018-08-09T11:55:36.926474: step 22321, loss 0.546762.
Train: 2018-08-09T11:55:37.035825: step 22322, loss 0.594914.
Train: 2018-08-09T11:55:37.145204: step 22323, loss 0.54681.
Train: 2018-08-09T11:55:37.238902: step 22324, loss 0.59488.
Train: 2018-08-09T11:55:37.348281: step 22325, loss 0.514864.
Train: 2018-08-09T11:55:37.442008: step 22326, loss 0.546869.
Train: 2018-08-09T11:55:37.551328: step 22327, loss 0.594852.
Train: 2018-08-09T11:55:37.660677: step 22328, loss 0.546809.
Train: 2018-08-09T11:55:37.770056: step 22329, loss 0.611044.
Train: 2018-08-09T11:55:37.863784: step 22330, loss 0.658922.
Test: 2018-08-09T11:55:38.363636: step 22330, loss 0.551566.
Train: 2018-08-09T11:55:38.472985: step 22331, loss 0.499065.
Train: 2018-08-09T11:55:38.566746: step 22332, loss 0.51508.
Train: 2018-08-09T11:55:38.676093: step 22333, loss 0.451226.
Train: 2018-08-09T11:55:38.769820: step 22334, loss 0.54691.
Train: 2018-08-09T11:55:38.879140: step 22335, loss 0.62713.
Train: 2018-08-09T11:55:38.975338: step 22336, loss 0.498677.
Train: 2018-08-09T11:55:39.084691: step 22337, loss 0.546705.
Train: 2018-08-09T11:55:39.178421: step 22338, loss 0.562738.
Train: 2018-08-09T11:55:39.287765: step 22339, loss 0.530501.
Train: 2018-08-09T11:55:39.397117: step 22340, loss 0.49787.
Test: 2018-08-09T11:55:39.881346: step 22340, loss 0.549235.
Train: 2018-08-09T11:55:39.990724: step 22341, loss 0.497607.
Train: 2018-08-09T11:55:40.100074: step 22342, loss 0.562769.
Train: 2018-08-09T11:55:40.193777: step 22343, loss 0.595124.
Train: 2018-08-09T11:55:40.303152: step 22344, loss 0.512175.
Train: 2018-08-09T11:55:40.396879: step 22345, loss 0.56296.
Train: 2018-08-09T11:55:40.506228: step 22346, loss 0.512781.
Train: 2018-08-09T11:55:40.599926: step 22347, loss 0.494442.
Train: 2018-08-09T11:55:40.709276: step 22348, loss 0.682555.
Train: 2018-08-09T11:55:40.803003: step 22349, loss 0.560687.
Train: 2018-08-09T11:55:40.913818: step 22350, loss 0.58265.
Test: 2018-08-09T11:55:41.413702: step 22350, loss 0.547636.
Train: 2018-08-09T11:55:41.507428: step 22351, loss 0.511682.
Train: 2018-08-09T11:55:41.616807: step 22352, loss 0.511438.
Train: 2018-08-09T11:55:41.710535: step 22353, loss 0.461232.
Train: 2018-08-09T11:55:41.819885: step 22354, loss 0.561845.
Train: 2018-08-09T11:55:41.913612: step 22355, loss 0.562551.
Train: 2018-08-09T11:55:42.022932: step 22356, loss 0.509682.
Train: 2018-08-09T11:55:42.132327: step 22357, loss 0.526503.
Train: 2018-08-09T11:55:42.226038: step 22358, loss 0.58095.
Train: 2018-08-09T11:55:42.335390: step 22359, loss 0.599296.
Train: 2018-08-09T11:55:42.429118: step 22360, loss 0.524539.
Test: 2018-08-09T11:55:42.931296: step 22360, loss 0.548602.
Train: 2018-08-09T11:55:43.040645: step 22361, loss 0.56075.
Train: 2018-08-09T11:55:43.134372: step 22362, loss 0.524869.
Train: 2018-08-09T11:55:43.243747: step 22363, loss 0.424238.
Train: 2018-08-09T11:55:43.337480: step 22364, loss 0.508723.
Train: 2018-08-09T11:55:43.446834: step 22365, loss 0.614896.
Train: 2018-08-09T11:55:43.556151: step 22366, loss 0.548543.
Train: 2018-08-09T11:55:43.649908: step 22367, loss 0.513487.
Train: 2018-08-09T11:55:43.759255: step 22368, loss 0.571256.
Train: 2018-08-09T11:55:43.852984: step 22369, loss 0.532048.
Train: 2018-08-09T11:55:43.962304: step 22370, loss 0.671771.
Test: 2018-08-09T11:55:44.446565: step 22370, loss 0.547065.
Train: 2018-08-09T11:55:44.555945: step 22371, loss 0.564095.
Train: 2018-08-09T11:55:44.649642: step 22372, loss 0.654235.
Train: 2018-08-09T11:55:44.759020: step 22373, loss 0.563534.
Train: 2018-08-09T11:55:44.852748: step 22374, loss 0.476649.
Train: 2018-08-09T11:55:44.963646: step 22375, loss 0.580488.
Train: 2018-08-09T11:55:45.057344: step 22376, loss 0.494898.
Train: 2018-08-09T11:55:45.166726: step 22377, loss 0.562234.
Train: 2018-08-09T11:55:45.260450: step 22378, loss 0.647111.
Train: 2018-08-09T11:55:45.369794: step 22379, loss 0.511935.
Train: 2018-08-09T11:55:45.463528: step 22380, loss 0.562382.
Test: 2018-08-09T11:55:45.963382: step 22380, loss 0.551146.
Train: 2018-08-09T11:55:46.072760: step 22381, loss 0.562402.
Train: 2018-08-09T11:55:46.166490: step 22382, loss 0.696199.
Train: 2018-08-09T11:55:46.275808: step 22383, loss 0.562417.
Train: 2018-08-09T11:55:46.385156: step 22384, loss 0.462752.
Train: 2018-08-09T11:55:46.478883: step 22385, loss 0.612297.
Train: 2018-08-09T11:55:46.588234: step 22386, loss 0.661866.
Train: 2018-08-09T11:55:46.681961: step 22387, loss 0.446948.
Train: 2018-08-09T11:55:46.791343: step 22388, loss 0.496568.
Train: 2018-08-09T11:55:46.885068: step 22389, loss 0.562521.
Train: 2018-08-09T11:55:46.996690: step 22390, loss 0.513066.
Test: 2018-08-09T11:55:47.496552: step 22390, loss 0.546436.
Train: 2018-08-09T11:55:47.590298: step 22391, loss 0.66146.
Train: 2018-08-09T11:55:47.699647: step 22392, loss 0.578988.
Train: 2018-08-09T11:55:47.793344: step 22393, loss 0.546073.
Train: 2018-08-09T11:55:47.902724: step 22394, loss 0.56253.
Train: 2018-08-09T11:55:47.996452: step 22395, loss 0.595413.
Train: 2018-08-09T11:55:48.105801: step 22396, loss 0.677413.
Train: 2018-08-09T11:55:48.199516: step 22397, loss 0.644362.
Train: 2018-08-09T11:55:48.308879: step 22398, loss 0.497454.
Train: 2018-08-09T11:55:48.402607: step 22399, loss 0.497641.
Train: 2018-08-09T11:55:48.511960: step 22400, loss 0.530198.
Test: 2018-08-09T11:55:49.014160: step 22400, loss 0.550536.
Train: 2018-08-09T11:55:49.545289: step 22401, loss 0.562675.
Train: 2018-08-09T11:55:49.654638: step 22402, loss 0.56264.
Train: 2018-08-09T11:55:49.748370: step 22403, loss 0.481637.
Train: 2018-08-09T11:55:49.846126: step 22404, loss 0.562395.
Train: 2018-08-09T11:55:49.955475: step 22405, loss 0.611266.
Train: 2018-08-09T11:55:50.049203: step 22406, loss 0.514065.
Train: 2018-08-09T11:55:50.158552: step 22407, loss 0.610815.
Train: 2018-08-09T11:55:50.252280: step 22408, loss 0.627714.
Train: 2018-08-09T11:55:50.361600: step 22409, loss 0.512686.
Train: 2018-08-09T11:55:50.455357: step 22410, loss 0.514351.
Test: 2018-08-09T11:55:50.967500: step 22410, loss 0.54813.
Train: 2018-08-09T11:55:51.061257: step 22411, loss 0.546845.
Train: 2018-08-09T11:55:51.154954: step 22412, loss 0.545029.
Train: 2018-08-09T11:55:51.264334: step 22413, loss 0.548194.
Train: 2018-08-09T11:55:51.358033: step 22414, loss 0.626869.
Train: 2018-08-09T11:55:51.467412: step 22415, loss 0.697173.
Train: 2018-08-09T11:55:51.561139: step 22416, loss 0.579262.
Train: 2018-08-09T11:55:51.670489: step 22417, loss 0.59453.
Train: 2018-08-09T11:55:51.764218: step 22418, loss 0.579031.
Train: 2018-08-09T11:55:51.873566: step 22419, loss 0.626579.
Train: 2018-08-09T11:55:51.967293: step 22420, loss 0.562932.
Test: 2018-08-09T11:55:52.467146: step 22420, loss 0.549641.
Train: 2018-08-09T11:55:52.576533: step 22421, loss 0.579124.
Train: 2018-08-09T11:55:52.670252: step 22422, loss 0.578846.
Train: 2018-08-09T11:55:52.779604: step 22423, loss 0.563134.
Train: 2018-08-09T11:55:52.873330: step 22424, loss 0.49964.
Train: 2018-08-09T11:55:52.982679: step 22425, loss 0.689592.
Train: 2018-08-09T11:55:53.076408: step 22426, loss 0.547319.
Train: 2018-08-09T11:55:53.185757: step 22427, loss 0.610341.
Train: 2018-08-09T11:55:53.279486: step 22428, loss 0.547487.
Train: 2018-08-09T11:55:53.388835: step 22429, loss 0.50058.
Train: 2018-08-09T11:55:53.482562: step 22430, loss 0.547584.
Test: 2018-08-09T11:55:53.978495: step 22430, loss 0.550352.
Train: 2018-08-09T11:55:54.087873: step 22431, loss 0.500655.
Train: 2018-08-09T11:55:54.197222: step 22432, loss 0.453594.
Train: 2018-08-09T11:55:54.290951: step 22433, loss 0.516043.
Train: 2018-08-09T11:55:54.396425: step 22434, loss 0.610389.
Train: 2018-08-09T11:55:54.505774: step 22435, loss 0.657862.
Train: 2018-08-09T11:55:54.599506: step 22436, loss 0.610499.
Train: 2018-08-09T11:55:54.708851: step 22437, loss 0.657921.
Train: 2018-08-09T11:55:54.802579: step 22438, loss 0.547279.
Train: 2018-08-09T11:55:54.911928: step 22439, loss 0.547314.
Train: 2018-08-09T11:55:55.005657: step 22440, loss 0.563098.
Test: 2018-08-09T11:55:55.505510: step 22440, loss 0.548927.
Train: 2018-08-09T11:55:55.614858: step 22441, loss 0.578864.
Train: 2018-08-09T11:55:55.708619: step 22442, loss 0.547348.
Train: 2018-08-09T11:55:55.817965: step 22443, loss 0.641897.
Train: 2018-08-09T11:55:55.917967: step 22444, loss 0.641831.
Train: 2018-08-09T11:55:56.013820: step 22445, loss 0.56317.
Train: 2018-08-09T11:55:56.123167: step 22446, loss 0.516179.
Train: 2018-08-09T11:55:56.216895: step 22447, loss 0.594542.
Train: 2018-08-09T11:55:56.326244: step 22448, loss 0.531945.
Train: 2018-08-09T11:55:56.419973: step 22449, loss 0.594526.
Train: 2018-08-09T11:55:56.529305: step 22450, loss 0.594516.
Test: 2018-08-09T11:55:57.029174: step 22450, loss 0.550413.
Train: 2018-08-09T11:55:57.138523: step 22451, loss 0.453964.
Train: 2018-08-09T11:55:57.232281: step 22452, loss 0.469452.
Train: 2018-08-09T11:55:57.326009: step 22453, loss 0.625907.
Train: 2018-08-09T11:55:57.435328: step 22454, loss 0.516071.
Train: 2018-08-09T11:55:57.544678: step 22455, loss 0.563132.
Train: 2018-08-09T11:55:57.638437: step 22456, loss 0.657722.
Train: 2018-08-09T11:55:57.732163: step 22457, loss 0.563084.
Train: 2018-08-09T11:55:57.841514: step 22458, loss 0.468336.
Train: 2018-08-09T11:55:57.950832: step 22459, loss 0.594679.
Train: 2018-08-09T11:55:58.044591: step 22460, loss 0.59472.
Test: 2018-08-09T11:55:58.544442: step 22460, loss 0.550527.
Train: 2018-08-09T11:55:58.638201: step 22461, loss 0.547121.
Train: 2018-08-09T11:55:58.747551: step 22462, loss 0.515314.
Train: 2018-08-09T11:55:58.856893: step 22463, loss 0.642541.
Train: 2018-08-09T11:55:58.950626: step 22464, loss 0.562931.
Train: 2018-08-09T11:55:59.044354: step 22465, loss 0.499146.
Train: 2018-08-09T11:55:59.153673: step 22466, loss 0.562912.
Train: 2018-08-09T11:55:59.247433: step 22467, loss 0.48286.
Train: 2018-08-09T11:55:59.356783: step 22468, loss 0.562817.
Train: 2018-08-09T11:55:59.466130: step 22469, loss 0.562662.
Train: 2018-08-09T11:55:59.559858: step 22470, loss 0.610829.
Test: 2018-08-09T11:56:00.059711: step 22470, loss 0.551152.
Train: 2018-08-09T11:56:00.169059: step 22471, loss 0.675465.
Train: 2018-08-09T11:56:00.262787: step 22472, loss 0.512741.
Train: 2018-08-09T11:56:00.372166: step 22473, loss 0.513118.
Train: 2018-08-09T11:56:00.465894: step 22474, loss 0.543447.
Train: 2018-08-09T11:56:00.575246: step 22475, loss 0.710544.
Train: 2018-08-09T11:56:00.684592: step 22476, loss 0.51248.
Train: 2018-08-09T11:56:00.778320: step 22477, loss 0.489815.
Train: 2018-08-09T11:56:00.887640: step 22478, loss 0.621575.
Train: 2018-08-09T11:56:00.983834: step 22479, loss 0.56169.
Train: 2018-08-09T11:56:01.093183: step 22480, loss 0.644955.
Test: 2018-08-09T11:56:01.593055: step 22480, loss 0.547412.
Train: 2018-08-09T11:56:01.686764: step 22481, loss 0.629175.
Train: 2018-08-09T11:56:01.796143: step 22482, loss 0.566266.
Train: 2018-08-09T11:56:01.889871: step 22483, loss 0.539304.
Train: 2018-08-09T11:56:01.999222: step 22484, loss 0.596488.
Train: 2018-08-09T11:56:02.092948: step 22485, loss 0.552687.
Train: 2018-08-09T11:56:02.202300: step 22486, loss 0.596364.
Train: 2018-08-09T11:56:02.296025: step 22487, loss 0.516463.
Train: 2018-08-09T11:56:02.405374: step 22488, loss 0.593335.
Train: 2018-08-09T11:56:02.514724: step 22489, loss 0.531448.
Train: 2018-08-09T11:56:02.608452: step 22490, loss 0.563282.
Test: 2018-08-09T11:56:03.109107: step 22490, loss 0.549854.
Train: 2018-08-09T11:56:03.202833: step 22491, loss 0.595295.
Train: 2018-08-09T11:56:03.312213: step 22492, loss 0.547894.
Train: 2018-08-09T11:56:03.421563: step 22493, loss 0.579245.
Train: 2018-08-09T11:56:03.515284: step 22494, loss 0.594773.
Train: 2018-08-09T11:56:03.624639: step 22495, loss 0.532034.
Train: 2018-08-09T11:56:03.733958: step 22496, loss 0.500495.
Train: 2018-08-09T11:56:03.827717: step 22497, loss 0.531983.
Train: 2018-08-09T11:56:03.935427: step 22498, loss 0.563315.
Train: 2018-08-09T11:56:04.029155: step 22499, loss 0.500198.
Train: 2018-08-09T11:56:04.138505: step 22500, loss 0.563233.
Test: 2018-08-09T11:56:04.638356: step 22500, loss 0.551206.
Train: 2018-08-09T11:56:05.200755: step 22501, loss 0.610883.
Train: 2018-08-09T11:56:05.294484: step 22502, loss 0.579396.
Train: 2018-08-09T11:56:05.403831: step 22503, loss 0.562935.
Train: 2018-08-09T11:56:05.513180: step 22504, loss 0.499754.
Train: 2018-08-09T11:56:05.606907: step 22505, loss 0.547153.
Train: 2018-08-09T11:56:05.716257: step 22506, loss 0.531386.
Train: 2018-08-09T11:56:05.809985: step 22507, loss 0.578864.
Train: 2018-08-09T11:56:05.920620: step 22508, loss 0.546727.
Train: 2018-08-09T11:56:06.014379: step 22509, loss 0.578876.
Train: 2018-08-09T11:56:06.123728: step 22510, loss 0.59437.
Test: 2018-08-09T11:56:06.623611: step 22510, loss 0.549383.
Train: 2018-08-09T11:56:06.732959: step 22511, loss 0.531243.
Train: 2018-08-09T11:56:06.826681: step 22512, loss 0.530477.
Train: 2018-08-09T11:56:06.936036: step 22513, loss 0.546812.
Train: 2018-08-09T11:56:07.029765: step 22514, loss 0.594866.
Train: 2018-08-09T11:56:07.139114: step 22515, loss 0.546391.
Train: 2018-08-09T11:56:07.248433: step 22516, loss 0.546613.
Train: 2018-08-09T11:56:07.342194: step 22517, loss 0.563745.
Train: 2018-08-09T11:56:07.451540: step 22518, loss 0.545987.
Train: 2018-08-09T11:56:07.545268: step 22519, loss 0.660744.
Train: 2018-08-09T11:56:07.654617: step 22520, loss 0.595097.
Test: 2018-08-09T11:56:08.150090: step 22520, loss 0.549204.
Train: 2018-08-09T11:56:08.259469: step 22521, loss 0.61128.
Train: 2018-08-09T11:56:08.368788: step 22522, loss 0.562934.
Train: 2018-08-09T11:56:08.462545: step 22523, loss 0.465666.
Train: 2018-08-09T11:56:08.571894: step 22524, loss 0.400921.
Train: 2018-08-09T11:56:08.681244: step 22525, loss 0.676493.
Train: 2018-08-09T11:56:08.774942: step 22526, loss 0.54638.
Train: 2018-08-09T11:56:08.884291: step 22527, loss 0.49775.
Train: 2018-08-09T11:56:08.978049: step 22528, loss 0.51384.
Train: 2018-08-09T11:56:09.087398: step 22529, loss 0.546154.
Train: 2018-08-09T11:56:09.196748: step 22530, loss 0.546154.
Test: 2018-08-09T11:56:09.680979: step 22530, loss 0.548368.
Train: 2018-08-09T11:56:09.790328: step 22531, loss 0.579079.
Train: 2018-08-09T11:56:09.899677: step 22532, loss 0.612058.
Train: 2018-08-09T11:56:10.009056: step 22533, loss 0.611917.
Train: 2018-08-09T11:56:10.102784: step 22534, loss 0.628532.
Train: 2018-08-09T11:56:10.212104: step 22535, loss 0.595465.
Train: 2018-08-09T11:56:10.305861: step 22536, loss 0.513281.
Train: 2018-08-09T11:56:10.415210: step 22537, loss 0.447772.
Train: 2018-08-09T11:56:10.524560: step 22538, loss 0.513267.
Train: 2018-08-09T11:56:10.618287: step 22539, loss 0.463712.
Train: 2018-08-09T11:56:10.727638: step 22540, loss 0.579178.
Test: 2018-08-09T11:56:11.230097: step 22540, loss 0.548139.
Train: 2018-08-09T11:56:11.323855: step 22541, loss 0.529192.
Train: 2018-08-09T11:56:11.433175: step 22542, loss 0.479393.
Train: 2018-08-09T11:56:11.542523: step 22543, loss 0.528653.
Train: 2018-08-09T11:56:11.636286: step 22544, loss 0.511293.
Train: 2018-08-09T11:56:11.745631: step 22545, loss 0.456884.
Train: 2018-08-09T11:56:11.839359: step 22546, loss 0.634403.
Train: 2018-08-09T11:56:11.948708: step 22547, loss 0.650481.
Train: 2018-08-09T11:56:12.058028: step 22548, loss 0.61662.
Train: 2018-08-09T11:56:12.151790: step 22549, loss 0.509459.
Train: 2018-08-09T11:56:12.261136: step 22550, loss 0.510402.
Test: 2018-08-09T11:56:12.760988: step 22550, loss 0.54647.
Train: 2018-08-09T11:56:12.854714: step 22551, loss 0.614994.
Train: 2018-08-09T11:56:12.965679: step 22552, loss 0.563615.
Train: 2018-08-09T11:56:13.075028: step 22553, loss 0.489794.
Train: 2018-08-09T11:56:13.168756: step 22554, loss 0.596957.
Train: 2018-08-09T11:56:13.278105: step 22555, loss 0.619387.
Train: 2018-08-09T11:56:13.387457: step 22556, loss 0.490027.
Train: 2018-08-09T11:56:13.481182: step 22557, loss 0.564436.
Train: 2018-08-09T11:56:13.590531: step 22558, loss 0.506348.
Train: 2018-08-09T11:56:13.699880: step 22559, loss 0.54155.
Train: 2018-08-09T11:56:13.793608: step 22560, loss 0.667276.
Test: 2018-08-09T11:56:14.293461: step 22560, loss 0.545926.
Train: 2018-08-09T11:56:14.402810: step 22561, loss 0.474766.
Train: 2018-08-09T11:56:14.496567: step 22562, loss 0.601426.
Train: 2018-08-09T11:56:14.605886: step 22563, loss 0.646525.
Train: 2018-08-09T11:56:14.715235: step 22564, loss 0.524458.
Train: 2018-08-09T11:56:14.808993: step 22565, loss 0.633947.
Train: 2018-08-09T11:56:14.918807: step 22566, loss 0.596342.
Train: 2018-08-09T11:56:15.028190: step 22567, loss 0.493789.
Train: 2018-08-09T11:56:15.121915: step 22568, loss 0.496007.
Train: 2018-08-09T11:56:15.231265: step 22569, loss 0.545454.
Train: 2018-08-09T11:56:15.340613: step 22570, loss 0.663277.
Test: 2018-08-09T11:56:15.824876: step 22570, loss 0.54854.
Train: 2018-08-09T11:56:15.934223: step 22571, loss 0.562289.
Train: 2018-08-09T11:56:16.043573: step 22572, loss 0.59444.
Train: 2018-08-09T11:56:16.152923: step 22573, loss 0.596479.
Train: 2018-08-09T11:56:16.246650: step 22574, loss 0.646659.
Train: 2018-08-09T11:56:16.355995: step 22575, loss 0.580731.
Train: 2018-08-09T11:56:16.465349: step 22576, loss 0.643849.
Train: 2018-08-09T11:56:16.574698: step 22577, loss 0.595194.
Train: 2018-08-09T11:56:16.668426: step 22578, loss 0.578879.
Train: 2018-08-09T11:56:16.777775: step 22579, loss 0.674881.
Train: 2018-08-09T11:56:16.887125: step 22580, loss 0.547045.
Test: 2018-08-09T11:56:17.373593: step 22580, loss 0.548248.
Train: 2018-08-09T11:56:17.482971: step 22581, loss 0.642106.
Train: 2018-08-09T11:56:17.592321: step 22582, loss 0.61029.
Train: 2018-08-09T11:56:17.686048: step 22583, loss 0.641362.
Train: 2018-08-09T11:56:17.795398: step 22584, loss 0.501321.
Train: 2018-08-09T11:56:17.904717: step 22585, loss 0.594388.
Train: 2018-08-09T11:56:18.014067: step 22586, loss 0.548207.
Train: 2018-08-09T11:56:18.107824: step 22587, loss 0.533011.
Train: 2018-08-09T11:56:18.217174: step 22588, loss 0.517857.
Train: 2018-08-09T11:56:18.326517: step 22589, loss 0.579014.
Train: 2018-08-09T11:56:18.420251: step 22590, loss 0.456999.
Test: 2018-08-09T11:56:18.920822: step 22590, loss 0.549434.
Train: 2018-08-09T11:56:19.014581: step 22591, loss 0.56374.
Train: 2018-08-09T11:56:19.123930: step 22592, loss 0.578997.
Train: 2018-08-09T11:56:19.233279: step 22593, loss 0.594306.
Train: 2018-08-09T11:56:19.342628: step 22594, loss 0.51771.
Train: 2018-08-09T11:56:19.436356: step 22595, loss 0.563647.
Train: 2018-08-09T11:56:19.545705: step 22596, loss 0.563626.
Train: 2018-08-09T11:56:19.655057: step 22597, loss 0.548151.
Train: 2018-08-09T11:56:19.748753: step 22598, loss 0.501779.
Train: 2018-08-09T11:56:19.858102: step 22599, loss 0.485931.
Train: 2018-08-09T11:56:19.971256: step 22600, loss 0.641397.
Test: 2018-08-09T11:56:20.471139: step 22600, loss 0.549213.
Train: 2018-08-09T11:56:21.017914: step 22601, loss 0.547647.
Train: 2018-08-09T11:56:21.127263: step 22602, loss 0.610035.
Train: 2018-08-09T11:56:21.236613: step 22603, loss 0.625992.
Train: 2018-08-09T11:56:21.345962: step 22604, loss 0.594781.
Train: 2018-08-09T11:56:21.439660: step 22605, loss 0.578842.
Train: 2018-08-09T11:56:21.549039: step 22606, loss 0.70451.
Train: 2018-08-09T11:56:21.658389: step 22607, loss 0.578867.
Train: 2018-08-09T11:56:21.752116: step 22608, loss 0.656898.
Train: 2018-08-09T11:56:21.861466: step 22609, loss 0.501149.
Train: 2018-08-09T11:56:21.973103: step 22610, loss 0.578911.
Test: 2018-08-09T11:56:22.457364: step 22610, loss 0.550656.
Train: 2018-08-09T11:56:22.566713: step 22611, loss 0.563418.
Train: 2018-08-09T11:56:22.676092: step 22612, loss 0.594406.
Train: 2018-08-09T11:56:22.785441: step 22613, loss 0.548021.
Train: 2018-08-09T11:56:22.879139: step 22614, loss 0.548054.
Train: 2018-08-09T11:56:22.988519: step 22615, loss 0.594373.
Train: 2018-08-09T11:56:23.097869: step 22616, loss 0.578944.
Train: 2018-08-09T11:56:23.191595: step 22617, loss 0.532695.
Train: 2018-08-09T11:56:23.300945: step 22618, loss 0.532678.
Train: 2018-08-09T11:56:23.410294: step 22619, loss 0.671562.
Train: 2018-08-09T11:56:23.519643: step 22620, loss 0.501844.
Test: 2018-08-09T11:56:24.006240: step 22620, loss 0.551978.
Train: 2018-08-09T11:56:24.115618: step 22621, loss 0.517224.
Train: 2018-08-09T11:56:24.224938: step 22622, loss 0.53257.
Train: 2018-08-09T11:56:24.334316: step 22623, loss 0.609888.
Train: 2018-08-09T11:56:24.428044: step 22624, loss 0.578907.
Train: 2018-08-09T11:56:24.537394: step 22625, loss 0.578894.
Train: 2018-08-09T11:56:24.646742: step 22626, loss 0.594464.
Train: 2018-08-09T11:56:24.756063: step 22627, loss 0.625505.
Train: 2018-08-09T11:56:24.849820: step 22628, loss 0.609914.
Train: 2018-08-09T11:56:24.959164: step 22629, loss 0.609995.
Train: 2018-08-09T11:56:25.068518: step 22630, loss 0.625359.
Test: 2018-08-09T11:56:25.568370: step 22630, loss 0.549595.
Train: 2018-08-09T11:56:25.662098: step 22631, loss 0.548001.
Train: 2018-08-09T11:56:25.771448: step 22632, loss 0.656054.
Train: 2018-08-09T11:56:25.880827: step 22633, loss 0.59434.
Train: 2018-08-09T11:56:25.992530: step 22634, loss 0.594251.
Train: 2018-08-09T11:56:26.086258: step 22635, loss 0.609548.
Train: 2018-08-09T11:56:26.195577: step 22636, loss 0.594267.
Train: 2018-08-09T11:56:26.304926: step 22637, loss 0.472671.
Train: 2018-08-09T11:56:26.414305: step 22638, loss 0.487944.
Train: 2018-08-09T11:56:26.523648: step 22639, loss 0.548525.
Train: 2018-08-09T11:56:26.617382: step 22640, loss 0.533141.
Test: 2018-08-09T11:56:27.115913: step 22640, loss 0.550543.
Train: 2018-08-09T11:56:27.225261: step 22641, loss 0.533203.
Train: 2018-08-09T11:56:27.334611: step 22642, loss 0.532463.
Train: 2018-08-09T11:56:27.443961: step 22643, loss 0.609705.
Train: 2018-08-09T11:56:27.553309: step 22644, loss 0.516478.
Train: 2018-08-09T11:56:27.647037: step 22645, loss 0.60927.
Train: 2018-08-09T11:56:27.756386: step 22646, loss 0.546969.
Train: 2018-08-09T11:56:27.865736: step 22647, loss 0.546392.
Train: 2018-08-09T11:56:27.978396: step 22648, loss 0.498075.
Train: 2018-08-09T11:56:28.087745: step 22649, loss 0.547588.
Train: 2018-08-09T11:56:28.181503: step 22650, loss 0.545218.
Test: 2018-08-09T11:56:28.681356: step 22650, loss 0.550678.
Train: 2018-08-09T11:56:28.790734: step 22651, loss 0.631857.
Train: 2018-08-09T11:56:28.900086: step 22652, loss 0.577596.
Train: 2018-08-09T11:56:28.993812: step 22653, loss 0.581526.
Train: 2018-08-09T11:56:29.105978: step 22654, loss 0.586452.
Train: 2018-08-09T11:56:29.215326: step 22655, loss 0.449806.
Train: 2018-08-09T11:56:29.324680: step 22656, loss 0.636706.
Train: 2018-08-09T11:56:29.434025: step 22657, loss 0.491097.
Train: 2018-08-09T11:56:29.527752: step 22658, loss 0.524336.
Train: 2018-08-09T11:56:29.637102: step 22659, loss 0.597415.
Train: 2018-08-09T11:56:29.746451: step 22660, loss 0.546876.
Test: 2018-08-09T11:56:30.246304: step 22660, loss 0.547317.
Train: 2018-08-09T11:56:30.355653: step 22661, loss 0.477499.
Train: 2018-08-09T11:56:30.449381: step 22662, loss 0.581804.
Train: 2018-08-09T11:56:30.558759: step 22663, loss 0.646096.
Train: 2018-08-09T11:56:30.668109: step 22664, loss 0.583736.
Train: 2018-08-09T11:56:30.777458: step 22665, loss 0.599036.
Train: 2018-08-09T11:56:30.886807: step 22666, loss 0.547983.
Train: 2018-08-09T11:56:30.980535: step 22667, loss 0.65991.
Train: 2018-08-09T11:56:31.089855: step 22668, loss 0.530536.
Train: 2018-08-09T11:56:31.199233: step 22669, loss 0.529867.
Train: 2018-08-09T11:56:31.308583: step 22670, loss 0.545934.
Test: 2018-08-09T11:56:31.792815: step 22670, loss 0.548405.
Train: 2018-08-09T11:56:31.902194: step 22671, loss 0.499575.
Train: 2018-08-09T11:56:32.012947: step 22672, loss 0.610454.
Train: 2018-08-09T11:56:32.122296: step 22673, loss 0.675723.
Train: 2018-08-09T11:56:32.216026: step 22674, loss 0.452539.
Train: 2018-08-09T11:56:32.325373: step 22675, loss 0.467889.
Train: 2018-08-09T11:56:32.434723: step 22676, loss 0.673921.
Train: 2018-08-09T11:56:32.544072: step 22677, loss 0.484184.
Train: 2018-08-09T11:56:32.653418: step 22678, loss 0.578842.
Train: 2018-08-09T11:56:32.747149: step 22679, loss 0.563481.
Train: 2018-08-09T11:56:32.856498: step 22680, loss 0.515593.
Test: 2018-08-09T11:56:33.356350: step 22680, loss 0.551149.
Train: 2018-08-09T11:56:33.465700: step 22681, loss 0.435469.
Train: 2018-08-09T11:56:33.559457: step 22682, loss 0.547516.
Train: 2018-08-09T11:56:33.668807: step 22683, loss 0.530371.
Train: 2018-08-09T11:56:33.778160: step 22684, loss 0.660284.
Train: 2018-08-09T11:56:33.887505: step 22685, loss 0.59528.
Train: 2018-08-09T11:56:33.982528: step 22686, loss 0.562478.
Train: 2018-08-09T11:56:34.091907: step 22687, loss 0.498424.
Train: 2018-08-09T11:56:34.201227: step 22688, loss 0.514524.
Train: 2018-08-09T11:56:34.310575: step 22689, loss 0.56296.
Train: 2018-08-09T11:56:34.419924: step 22690, loss 0.529511.
Test: 2018-08-09T11:56:34.904187: step 22690, loss 0.546459.
Train: 2018-08-09T11:56:35.013565: step 22691, loss 0.661002.
Train: 2018-08-09T11:56:35.122914: step 22692, loss 0.580077.
Train: 2018-08-09T11:56:35.232264: step 22693, loss 0.513356.
Train: 2018-08-09T11:56:35.325961: step 22694, loss 0.544644.
Train: 2018-08-09T11:56:35.435340: step 22695, loss 0.545916.
Train: 2018-08-09T11:56:35.544689: step 22696, loss 0.529189.
Train: 2018-08-09T11:56:35.654040: step 22697, loss 0.628344.
Train: 2018-08-09T11:56:35.763388: step 22698, loss 0.429259.
Train: 2018-08-09T11:56:35.872738: step 22699, loss 0.5285.
Train: 2018-08-09T11:56:35.967836: step 22700, loss 0.595544.
Test: 2018-08-09T11:56:36.467690: step 22700, loss 0.547553.
Train: 2018-08-09T11:56:37.030085: step 22701, loss 0.512995.
Train: 2018-08-09T11:56:37.139435: step 22702, loss 0.631289.
Train: 2018-08-09T11:56:37.248755: step 22703, loss 0.509646.
Train: 2018-08-09T11:56:37.358134: step 22704, loss 0.493369.
Train: 2018-08-09T11:56:37.467484: step 22705, loss 0.492246.
Train: 2018-08-09T11:56:37.561181: step 22706, loss 0.668289.
Train: 2018-08-09T11:56:37.670561: step 22707, loss 0.435404.
Train: 2018-08-09T11:56:37.779909: step 22708, loss 0.672658.
Train: 2018-08-09T11:56:37.889258: step 22709, loss 0.609308.
Train: 2018-08-09T11:56:37.983522: step 22710, loss 0.485395.
Test: 2018-08-09T11:56:38.483405: step 22710, loss 0.548442.
Train: 2018-08-09T11:56:38.592753: step 22711, loss 0.537944.
Train: 2018-08-09T11:56:38.702133: step 22712, loss 0.530472.
Train: 2018-08-09T11:56:38.811481: step 22713, loss 0.472783.
Train: 2018-08-09T11:56:38.920831: step 22714, loss 0.486497.
Train: 2018-08-09T11:56:39.014530: step 22715, loss 0.508514.
Train: 2018-08-09T11:56:39.123909: step 22716, loss 0.639628.
Train: 2018-08-09T11:56:39.233251: step 22717, loss 0.572077.
Train: 2018-08-09T11:56:39.342607: step 22718, loss 0.48437.
Train: 2018-08-09T11:56:39.451955: step 22719, loss 0.679364.
Train: 2018-08-09T11:56:39.561305: step 22720, loss 0.489911.
Test: 2018-08-09T11:56:40.045537: step 22720, loss 0.548449.
Train: 2018-08-09T11:56:40.154916: step 22721, loss 0.640106.
Train: 2018-08-09T11:56:40.264235: step 22722, loss 0.511707.
Train: 2018-08-09T11:56:40.373613: step 22723, loss 0.565775.
Train: 2018-08-09T11:56:40.467341: step 22724, loss 0.628184.
Train: 2018-08-09T11:56:40.576666: step 22725, loss 0.64427.
Train: 2018-08-09T11:56:40.686040: step 22726, loss 0.56352.
Train: 2018-08-09T11:56:40.795390: step 22727, loss 0.579056.
Train: 2018-08-09T11:56:40.906150: step 22728, loss 0.530302.
Train: 2018-08-09T11:56:41.015529: step 22729, loss 0.595046.
Train: 2018-08-09T11:56:41.109257: step 22730, loss 0.498635.
Test: 2018-08-09T11:56:41.606061: step 22730, loss 0.551061.
Train: 2018-08-09T11:56:41.715405: step 22731, loss 0.498882.
Train: 2018-08-09T11:56:41.824784: step 22732, loss 0.530582.
Train: 2018-08-09T11:56:41.918512: step 22733, loss 0.530707.
Train: 2018-08-09T11:56:42.027862: step 22734, loss 0.514193.
Train: 2018-08-09T11:56:42.137212: step 22735, loss 0.611445.
Train: 2018-08-09T11:56:42.246562: step 22736, loss 0.546844.
Train: 2018-08-09T11:56:42.355909: step 22737, loss 0.562426.
Train: 2018-08-09T11:56:42.449637: step 22738, loss 0.561669.
Train: 2018-08-09T11:56:42.558986: step 22739, loss 0.563829.
Train: 2018-08-09T11:56:42.668336: step 22740, loss 0.51183.
Test: 2018-08-09T11:56:43.168192: step 22740, loss 0.548003.
Train: 2018-08-09T11:56:43.277567: step 22741, loss 0.613351.
Train: 2018-08-09T11:56:43.371295: step 22742, loss 0.661037.
Train: 2018-08-09T11:56:43.480647: step 22743, loss 0.545594.
Train: 2018-08-09T11:56:43.589995: step 22744, loss 0.546071.
Train: 2018-08-09T11:56:43.699342: step 22745, loss 0.529126.
Train: 2018-08-09T11:56:43.808695: step 22746, loss 0.579807.
Train: 2018-08-09T11:56:43.902419: step 22747, loss 0.562847.
Train: 2018-08-09T11:56:44.011739: step 22748, loss 0.559684.
Train: 2018-08-09T11:56:44.116479: step 22749, loss 0.5781.
Train: 2018-08-09T11:56:44.225827: step 22750, loss 0.563547.
Test: 2018-08-09T11:56:44.725709: step 22750, loss 0.550533.
Train: 2018-08-09T11:56:44.835089: step 22751, loss 0.510627.
Train: 2018-08-09T11:56:44.935146: step 22752, loss 0.529547.
Train: 2018-08-09T11:56:45.044495: step 22753, loss 0.668768.
Train: 2018-08-09T11:56:45.153845: step 22754, loss 0.529517.
Train: 2018-08-09T11:56:45.263194: step 22755, loss 0.592769.
Train: 2018-08-09T11:56:45.372537: step 22756, loss 0.695201.
Train: 2018-08-09T11:56:45.466271: step 22757, loss 0.534085.
Train: 2018-08-09T11:56:45.575623: step 22758, loss 0.611163.
Train: 2018-08-09T11:56:45.684969: step 22759, loss 0.643083.
Train: 2018-08-09T11:56:45.794319: step 22760, loss 0.531502.
Test: 2018-08-09T11:56:46.278559: step 22760, loss 0.550647.
Train: 2018-08-09T11:56:46.387929: step 22761, loss 0.610488.
Train: 2018-08-09T11:56:46.497280: step 22762, loss 0.594734.
Train: 2018-08-09T11:56:46.591006: step 22763, loss 0.594498.
Train: 2018-08-09T11:56:46.700326: step 22764, loss 0.501252.
Train: 2018-08-09T11:56:46.809675: step 22765, loss 0.485797.
Train: 2018-08-09T11:56:46.919054: step 22766, loss 0.454716.
Train: 2018-08-09T11:56:47.012781: step 22767, loss 0.656671.
Train: 2018-08-09T11:56:47.122125: step 22768, loss 0.54779.
Train: 2018-08-09T11:56:47.231480: step 22769, loss 0.547795.
Train: 2018-08-09T11:56:47.340829: step 22770, loss 0.641248.
Test: 2018-08-09T11:56:47.840683: step 22770, loss 0.549302.
Train: 2018-08-09T11:56:47.936754: step 22771, loss 0.500959.
Train: 2018-08-09T11:56:48.046102: step 22772, loss 0.594473.
Train: 2018-08-09T11:56:48.155483: step 22773, loss 0.516415.
Train: 2018-08-09T11:56:48.249210: step 22774, loss 0.563046.
Train: 2018-08-09T11:56:48.358561: step 22775, loss 0.484834.
Train: 2018-08-09T11:56:48.467908: step 22776, loss 0.452753.
Train: 2018-08-09T11:56:48.577228: step 22777, loss 0.53076.
Train: 2018-08-09T11:56:48.670985: step 22778, loss 0.579155.
Train: 2018-08-09T11:56:48.780335: step 22779, loss 0.562482.
Train: 2018-08-09T11:56:48.889684: step 22780, loss 0.544424.
Test: 2018-08-09T11:56:49.389567: step 22780, loss 0.548611.
Train: 2018-08-09T11:56:49.483264: step 22781, loss 0.478241.
Train: 2018-08-09T11:56:49.592643: step 22782, loss 0.525121.
Train: 2018-08-09T11:56:49.701962: step 22783, loss 0.685126.
Train: 2018-08-09T11:56:49.811342: step 22784, loss 0.43719.
Train: 2018-08-09T11:56:49.919275: step 22785, loss 0.567937.
Train: 2018-08-09T11:56:50.017754: step 22786, loss 0.546347.
Train: 2018-08-09T11:56:50.127103: step 22787, loss 0.531542.
Train: 2018-08-09T11:56:50.220831: step 22788, loss 0.58692.
Train: 2018-08-09T11:56:50.330181: step 22789, loss 0.527295.
Train: 2018-08-09T11:56:50.439500: step 22790, loss 0.619972.
Test: 2018-08-09T11:56:50.939382: step 22790, loss 0.546513.
Train: 2018-08-09T11:56:51.033109: step 22791, loss 0.600335.
Train: 2018-08-09T11:56:51.142489: step 22792, loss 0.577979.
Train: 2018-08-09T11:56:51.251808: step 22793, loss 0.560965.
Train: 2018-08-09T11:56:51.361192: step 22794, loss 0.631492.
Train: 2018-08-09T11:56:51.454916: step 22795, loss 0.579039.
Train: 2018-08-09T11:56:51.564264: step 22796, loss 0.595723.
Train: 2018-08-09T11:56:51.673614: step 22797, loss 0.611362.
Train: 2018-08-09T11:56:51.782963: step 22798, loss 0.498353.
Train: 2018-08-09T11:56:51.876671: step 22799, loss 0.578946.
Train: 2018-08-09T11:56:51.986040: step 22800, loss 0.482138.
Test: 2018-08-09T11:56:52.485917: step 22800, loss 0.548791.
Train: 2018-08-09T11:56:53.049585: step 22801, loss 0.546429.
Train: 2018-08-09T11:56:53.143314: step 22802, loss 0.530626.
Train: 2018-08-09T11:56:53.252662: step 22803, loss 0.594647.
Train: 2018-08-09T11:56:53.362042: step 22804, loss 0.627395.
Train: 2018-08-09T11:56:53.471390: step 22805, loss 0.596036.
Train: 2018-08-09T11:56:53.565119: step 22806, loss 0.547146.
Train: 2018-08-09T11:56:53.674468: step 22807, loss 0.547044.
Train: 2018-08-09T11:56:53.783818: step 22808, loss 0.515375.
Train: 2018-08-09T11:56:53.893166: step 22809, loss 0.594941.
Train: 2018-08-09T11:56:54.002529: step 22810, loss 0.578677.
Test: 2018-08-09T11:56:54.502369: step 22810, loss 0.547256.
Train: 2018-08-09T11:56:54.596126: step 22811, loss 0.482806.
Train: 2018-08-09T11:56:54.705478: step 22812, loss 0.482489.
Train: 2018-08-09T11:56:54.814836: step 22813, loss 0.465678.
Train: 2018-08-09T11:56:54.910958: step 22814, loss 0.529704.
Train: 2018-08-09T11:56:55.020307: step 22815, loss 0.512919.
Train: 2018-08-09T11:56:55.114035: step 22816, loss 0.614249.
Train: 2018-08-09T11:56:55.223384: step 22817, loss 0.580909.
Train: 2018-08-09T11:56:55.332764: step 22818, loss 0.714556.
Train: 2018-08-09T11:56:55.442083: step 22819, loss 0.578581.
Train: 2018-08-09T11:56:55.535841: step 22820, loss 0.496802.
Test: 2018-08-09T11:56:56.035695: step 22820, loss 0.549537.
Train: 2018-08-09T11:56:56.145042: step 22821, loss 0.545334.
Train: 2018-08-09T11:56:56.254391: step 22822, loss 0.612974.
Train: 2018-08-09T11:56:56.348149: step 22823, loss 0.594622.
Train: 2018-08-09T11:56:56.457499: step 22824, loss 0.596127.
Train: 2018-08-09T11:56:56.566817: step 22825, loss 0.56185.
Train: 2018-08-09T11:56:56.676166: step 22826, loss 0.496679.
Train: 2018-08-09T11:56:56.769925: step 22827, loss 0.530538.
Train: 2018-08-09T11:56:56.879244: step 22828, loss 0.64368.
Train: 2018-08-09T11:56:56.991030: step 22829, loss 0.643301.
Train: 2018-08-09T11:56:57.084757: step 22830, loss 0.496476.
Test: 2018-08-09T11:56:57.584639: step 22830, loss 0.547316.
Train: 2018-08-09T11:56:57.694019: step 22831, loss 0.626843.
Train: 2018-08-09T11:56:57.803369: step 22832, loss 0.495732.
Train: 2018-08-09T11:56:57.912716: step 22833, loss 0.597495.
Train: 2018-08-09T11:56:58.006444: step 22834, loss 0.677867.
Train: 2018-08-09T11:56:58.115763: step 22835, loss 0.497547.
Train: 2018-08-09T11:56:58.225144: step 22836, loss 0.494645.
Train: 2018-08-09T11:56:58.318872: step 22837, loss 0.562463.
Train: 2018-08-09T11:56:58.428222: step 22838, loss 0.5617.
Train: 2018-08-09T11:56:58.537538: step 22839, loss 0.53223.
Train: 2018-08-09T11:56:58.631297: step 22840, loss 0.494826.
Test: 2018-08-09T11:56:59.132605: step 22840, loss 0.547739.
Train: 2018-08-09T11:56:59.241979: step 22841, loss 0.529353.
Train: 2018-08-09T11:56:59.351304: step 22842, loss 0.630292.
Train: 2018-08-09T11:56:59.445062: step 22843, loss 0.445986.
Train: 2018-08-09T11:56:59.554413: step 22844, loss 0.665223.
Train: 2018-08-09T11:56:59.663760: step 22845, loss 0.644685.
Train: 2018-08-09T11:56:59.757493: step 22846, loss 0.51618.
Train: 2018-08-09T11:56:59.866838: step 22847, loss 0.492696.
Train: 2018-08-09T11:56:59.969222: step 22848, loss 0.598122.
Train: 2018-08-09T11:57:00.078572: step 22849, loss 0.491463.
Train: 2018-08-09T11:57:00.187891: step 22850, loss 0.630557.
Test: 2018-08-09T11:57:00.687806: step 22850, loss 0.548002.
Train: 2018-08-09T11:57:00.781500: step 22851, loss 0.531358.
Train: 2018-08-09T11:57:00.890882: step 22852, loss 0.547457.
Train: 2018-08-09T11:57:00.993961: step 22853, loss 0.580777.
Train: 2018-08-09T11:57:01.103281: step 22854, loss 0.382302.
Train: 2018-08-09T11:57:01.197007: step 22855, loss 0.64764.
Train: 2018-08-09T11:57:01.306356: step 22856, loss 0.562804.
Train: 2018-08-09T11:57:01.415737: step 22857, loss 0.544108.
Train: 2018-08-09T11:57:01.509464: step 22858, loss 0.613297.
Train: 2018-08-09T11:57:01.618814: step 22859, loss 0.464017.
Train: 2018-08-09T11:57:01.728134: step 22860, loss 0.677262.
Test: 2018-08-09T11:57:02.224966: step 22860, loss 0.545938.
Train: 2018-08-09T11:57:02.318724: step 22861, loss 0.530106.
Train: 2018-08-09T11:57:02.428074: step 22862, loss 0.578975.
Train: 2018-08-09T11:57:02.537422: step 22863, loss 0.57834.
Train: 2018-08-09T11:57:02.631150: step 22864, loss 0.496907.
Train: 2018-08-09T11:57:02.740470: step 22865, loss 0.610249.
Train: 2018-08-09T11:57:02.849850: step 22866, loss 0.561789.
Train: 2018-08-09T11:57:02.943576: step 22867, loss 0.677461.
Train: 2018-08-09T11:57:03.052926: step 22868, loss 0.578967.
Train: 2018-08-09T11:57:03.162275: step 22869, loss 0.562595.
Train: 2018-08-09T11:57:03.255974: step 22870, loss 0.676758.
Test: 2018-08-09T11:57:03.755886: step 22870, loss 0.546973.
Train: 2018-08-09T11:57:03.865235: step 22871, loss 0.562525.
Train: 2018-08-09T11:57:03.963299: step 22872, loss 0.530819.
Train: 2018-08-09T11:57:04.072648: step 22873, loss 0.514589.
Train: 2018-08-09T11:57:04.181967: step 22874, loss 0.450388.
Train: 2018-08-09T11:57:04.275726: step 22875, loss 0.58015.
Train: 2018-08-09T11:57:04.385044: step 22876, loss 0.494921.
Train: 2018-08-09T11:57:04.494394: step 22877, loss 0.578441.
Train: 2018-08-09T11:57:04.588152: step 22878, loss 0.596416.
Train: 2018-08-09T11:57:04.697502: step 22879, loss 0.59461.
Train: 2018-08-09T11:57:04.806850: step 22880, loss 0.562174.
Test: 2018-08-09T11:57:05.306702: step 22880, loss 0.547024.
Train: 2018-08-09T11:57:05.400461: step 22881, loss 0.562735.
Train: 2018-08-09T11:57:05.509808: step 22882, loss 0.627968.
Train: 2018-08-09T11:57:05.603537: step 22883, loss 0.531375.
Train: 2018-08-09T11:57:05.712888: step 22884, loss 0.498663.
Train: 2018-08-09T11:57:05.822205: step 22885, loss 0.563503.
Train: 2018-08-09T11:57:05.915964: step 22886, loss 0.546164.
Train: 2018-08-09T11:57:06.025314: step 22887, loss 0.594966.
Train: 2018-08-09T11:57:06.119035: step 22888, loss 0.579514.
Train: 2018-08-09T11:57:06.228360: step 22889, loss 0.579197.
Train: 2018-08-09T11:57:06.337739: step 22890, loss 0.643183.
Test: 2018-08-09T11:57:06.837616: step 22890, loss 0.551385.
Train: 2018-08-09T11:57:06.932762: step 22891, loss 0.578176.
Train: 2018-08-09T11:57:07.042111: step 22892, loss 0.594368.
Train: 2018-08-09T11:57:07.151466: step 22893, loss 0.547125.
Train: 2018-08-09T11:57:07.245193: step 22894, loss 0.515092.
Train: 2018-08-09T11:57:07.354543: step 22895, loss 0.531007.
Train: 2018-08-09T11:57:07.463892: step 22896, loss 0.706704.
Train: 2018-08-09T11:57:07.557621: step 22897, loss 0.690609.
Train: 2018-08-09T11:57:07.666969: step 22898, loss 0.56278.
Train: 2018-08-09T11:57:07.760698: step 22899, loss 0.547372.
Train: 2018-08-09T11:57:07.870048: step 22900, loss 0.578966.
Test: 2018-08-09T11:57:08.369907: step 22900, loss 0.550834.
Train: 2018-08-09T11:57:08.965056: step 22901, loss 0.688602.
Train: 2018-08-09T11:57:09.058783: step 22902, loss 0.57897.
Train: 2018-08-09T11:57:09.168103: step 22903, loss 0.45454.
Train: 2018-08-09T11:57:09.277451: step 22904, loss 0.501105.
Train: 2018-08-09T11:57:09.371179: step 22905, loss 0.594296.
Train: 2018-08-09T11:57:09.480528: step 22906, loss 0.501218.
Train: 2018-08-09T11:57:09.589908: step 22907, loss 0.516002.
Train: 2018-08-09T11:57:09.683630: step 22908, loss 0.547101.
Train: 2018-08-09T11:57:09.792986: step 22909, loss 0.531006.
Train: 2018-08-09T11:57:09.886714: step 22910, loss 0.514873.
Test: 2018-08-09T11:57:10.386590: step 22910, loss 0.551031.
Train: 2018-08-09T11:57:10.495915: step 22911, loss 0.529463.
Train: 2018-08-09T11:57:10.589672: step 22912, loss 0.478075.
Train: 2018-08-09T11:57:10.699022: step 22913, loss 0.630284.
Train: 2018-08-09T11:57:10.808371: step 22914, loss 0.607053.
Train: 2018-08-09T11:57:10.902098: step 22915, loss 0.523416.
Train: 2018-08-09T11:57:11.012851: step 22916, loss 0.645579.
Train: 2018-08-09T11:57:11.106579: step 22917, loss 0.554236.
Train: 2018-08-09T11:57:11.215922: step 22918, loss 0.4724.
Train: 2018-08-09T11:57:11.325248: step 22919, loss 0.508616.
Train: 2018-08-09T11:57:11.434627: step 22920, loss 0.572546.
Test: 2018-08-09T11:57:11.918884: step 22920, loss 0.54667.
Train: 2018-08-09T11:57:12.028236: step 22921, loss 0.502543.
Train: 2018-08-09T11:57:12.137586: step 22922, loss 0.587216.
Train: 2018-08-09T11:57:12.231314: step 22923, loss 0.565464.
Train: 2018-08-09T11:57:12.340664: step 22924, loss 0.541511.
Train: 2018-08-09T11:57:12.434391: step 22925, loss 0.595897.
Train: 2018-08-09T11:57:12.543709: step 22926, loss 0.583311.
Train: 2018-08-09T11:57:12.653084: step 22927, loss 0.534767.
Train: 2018-08-09T11:57:12.746818: step 22928, loss 0.477653.
Train: 2018-08-09T11:57:12.856167: step 22929, loss 0.554662.
Train: 2018-08-09T11:57:12.951380: step 22930, loss 0.596873.
Test: 2018-08-09T11:57:13.451257: step 22930, loss 0.552015.
Train: 2018-08-09T11:57:13.560580: step 22931, loss 0.510074.
Train: 2018-08-09T11:57:13.654339: step 22932, loss 0.48979.
Train: 2018-08-09T11:57:13.763688: step 22933, loss 0.488545.
Train: 2018-08-09T11:57:13.873037: step 22934, loss 0.514186.
Train: 2018-08-09T11:57:13.966768: step 22935, loss 0.624872.
Train: 2018-08-09T11:57:14.076115: step 22936, loss 0.673539.
Train: 2018-08-09T11:57:14.185463: step 22937, loss 0.618924.
Train: 2018-08-09T11:57:14.279191: step 22938, loss 0.659258.
Train: 2018-08-09T11:57:14.388542: step 22939, loss 0.595562.
Train: 2018-08-09T11:57:14.496473: step 22940, loss 0.564293.
Test: 2018-08-09T11:57:14.998713: step 22940, loss 0.549406.
Train: 2018-08-09T11:57:15.092468: step 22941, loss 0.562966.
Train: 2018-08-09T11:57:15.201817: step 22942, loss 0.626092.
Train: 2018-08-09T11:57:15.311167: step 22943, loss 0.626085.
Train: 2018-08-09T11:57:15.404894: step 22944, loss 0.578934.
Train: 2018-08-09T11:57:15.514244: step 22945, loss 0.562702.
Train: 2018-08-09T11:57:15.607971: step 22946, loss 0.594854.
Train: 2018-08-09T11:57:15.717322: step 22947, loss 0.596709.
Train: 2018-08-09T11:57:15.826669: step 22948, loss 0.532685.
Train: 2018-08-09T11:57:15.920368: step 22949, loss 0.562059.
Train: 2018-08-09T11:57:16.029747: step 22950, loss 0.610951.
Test: 2018-08-09T11:57:16.529599: step 22950, loss 0.551104.
Train: 2018-08-09T11:57:16.623356: step 22951, loss 0.578975.
Train: 2018-08-09T11:57:16.732706: step 22952, loss 0.469982.
Train: 2018-08-09T11:57:16.842050: step 22953, loss 0.548274.
Train: 2018-08-09T11:57:16.936351: step 22954, loss 0.517358.
Train: 2018-08-09T11:57:17.045730: step 22955, loss 0.563349.
Train: 2018-08-09T11:57:17.139458: step 22956, loss 0.57936.
Train: 2018-08-09T11:57:17.248807: step 22957, loss 0.501285.
Train: 2018-08-09T11:57:17.358128: step 22958, loss 0.578124.
Train: 2018-08-09T11:57:17.467476: step 22959, loss 0.547628.
Train: 2018-08-09T11:57:17.561204: step 22960, loss 0.592667.
Test: 2018-08-09T11:57:18.061086: step 22960, loss 0.54919.
Train: 2018-08-09T11:57:18.170465: step 22961, loss 0.545762.
Train: 2018-08-09T11:57:18.264164: step 22962, loss 0.599111.
Train: 2018-08-09T11:57:18.373543: step 22963, loss 0.596594.
Train: 2018-08-09T11:57:18.482892: step 22964, loss 0.528702.
Train: 2018-08-09T11:57:18.576589: step 22965, loss 0.69746.
Train: 2018-08-09T11:57:18.685968: step 22966, loss 0.709446.
Train: 2018-08-09T11:57:18.779696: step 22967, loss 0.548429.
Train: 2018-08-09T11:57:18.889046: step 22968, loss 0.563341.
Train: 2018-08-09T11:57:18.982773: step 22969, loss 0.609353.
Train: 2018-08-09T11:57:19.092092: step 22970, loss 0.609385.
Test: 2018-08-09T11:57:19.591976: step 22970, loss 0.552571.
Train: 2018-08-09T11:57:19.685733: step 22971, loss 0.489984.
Train: 2018-08-09T11:57:19.795083: step 22972, loss 0.517594.
Train: 2018-08-09T11:57:19.906839: step 22973, loss 0.564116.
Train: 2018-08-09T11:57:20.000602: step 22974, loss 0.486871.
Train: 2018-08-09T11:57:20.110582: step 22975, loss 0.563435.
Train: 2018-08-09T11:57:20.204340: step 22976, loss 0.625277.
Train: 2018-08-09T11:57:20.313690: step 22977, loss 0.610291.
Train: 2018-08-09T11:57:20.423041: step 22978, loss 0.531819.
Train: 2018-08-09T11:57:20.516767: step 22979, loss 0.609329.
Train: 2018-08-09T11:57:20.626117: step 22980, loss 0.593984.
Test: 2018-08-09T11:57:21.125969: step 22980, loss 0.551029.
Train: 2018-08-09T11:57:21.235350: step 22981, loss 0.578746.
Train: 2018-08-09T11:57:21.329076: step 22982, loss 0.562549.
Train: 2018-08-09T11:57:21.438425: step 22983, loss 0.547686.
Train: 2018-08-09T11:57:21.532122: step 22984, loss 0.547583.
Train: 2018-08-09T11:57:21.641502: step 22985, loss 0.530479.
Train: 2018-08-09T11:57:21.735230: step 22986, loss 0.468068.
Train: 2018-08-09T11:57:21.844549: step 22987, loss 0.645162.
Train: 2018-08-09T11:57:21.953899: step 22988, loss 0.61524.
Train: 2018-08-09T11:57:22.047657: step 22989, loss 0.513352.
Train: 2018-08-09T11:57:22.156976: step 22990, loss 0.611439.
Test: 2018-08-09T11:57:22.656891: step 22990, loss 0.548792.
Train: 2018-08-09T11:57:22.750616: step 22991, loss 0.642736.
Train: 2018-08-09T11:57:22.859934: step 22992, loss 0.565082.
Train: 2018-08-09T11:57:22.953693: step 22993, loss 0.609796.
Train: 2018-08-09T11:57:23.063038: step 22994, loss 0.547632.
Train: 2018-08-09T11:57:23.172392: step 22995, loss 0.452074.
Train: 2018-08-09T11:57:23.266090: step 22996, loss 0.546601.
Train: 2018-08-09T11:57:23.375468: step 22997, loss 0.64242.
Train: 2018-08-09T11:57:23.484819: step 22998, loss 0.515533.
Train: 2018-08-09T11:57:23.578516: step 22999, loss 0.531501.
Train: 2018-08-09T11:57:23.687865: step 23000, loss 0.577565.
Test: 2018-08-09T11:57:24.189135: step 23000, loss 0.548501.
Train: 2018-08-09T11:57:24.720260: step 23001, loss 0.642131.
Train: 2018-08-09T11:57:24.829647: step 23002, loss 0.546104.
Train: 2018-08-09T11:57:24.923367: step 23003, loss 0.528522.
Train: 2018-08-09T11:57:25.032716: step 23004, loss 0.547689.
Train: 2018-08-09T11:57:25.126445: step 23005, loss 0.608576.
Train: 2018-08-09T11:57:25.235763: step 23006, loss 0.611141.
Train: 2018-08-09T11:57:25.329521: step 23007, loss 0.548078.
Train: 2018-08-09T11:57:25.438870: step 23008, loss 0.511405.
Train: 2018-08-09T11:57:25.548221: step 23009, loss 0.528557.
Train: 2018-08-09T11:57:25.641948: step 23010, loss 0.49008.
Test: 2018-08-09T11:57:26.143181: step 23010, loss 0.548367.
Train: 2018-08-09T11:57:26.252560: step 23011, loss 0.497459.
Train: 2018-08-09T11:57:26.346288: step 23012, loss 0.627504.
Train: 2018-08-09T11:57:26.455638: step 23013, loss 0.529127.
Train: 2018-08-09T11:57:26.549367: step 23014, loss 0.479218.
Train: 2018-08-09T11:57:26.658685: step 23015, loss 0.551815.
Train: 2018-08-09T11:57:26.768034: step 23016, loss 0.62151.
Train: 2018-08-09T11:57:26.861791: step 23017, loss 0.603351.
Train: 2018-08-09T11:57:26.971141: step 23018, loss 0.475748.
Train: 2018-08-09T11:57:27.080490: step 23019, loss 0.610095.
Train: 2018-08-09T11:57:27.174220: step 23020, loss 0.545173.
Test: 2018-08-09T11:57:27.674071: step 23020, loss 0.550816.
Train: 2018-08-09T11:57:27.783420: step 23021, loss 0.563968.
Train: 2018-08-09T11:57:27.877178: step 23022, loss 0.512745.
Train: 2018-08-09T11:57:27.987873: step 23023, loss 0.507679.
Train: 2018-08-09T11:57:28.081631: step 23024, loss 0.457073.
Train: 2018-08-09T11:57:28.190950: step 23025, loss 0.515516.
Train: 2018-08-09T11:57:28.300330: step 23026, loss 0.602221.
Train: 2018-08-09T11:57:28.394060: step 23027, loss 0.546873.
Train: 2018-08-09T11:57:28.503425: step 23028, loss 0.52398.
Train: 2018-08-09T11:57:28.612756: step 23029, loss 0.511884.
Train: 2018-08-09T11:57:28.706485: step 23030, loss 0.595138.
Test: 2018-08-09T11:57:29.206368: step 23030, loss 0.54579.
Train: 2018-08-09T11:57:29.315716: step 23031, loss 0.543373.
Train: 2018-08-09T11:57:29.409447: step 23032, loss 0.538986.
Train: 2018-08-09T11:57:29.518797: step 23033, loss 0.557852.
Train: 2018-08-09T11:57:29.628142: step 23034, loss 0.603744.
Train: 2018-08-09T11:57:29.721870: step 23035, loss 0.658099.
Train: 2018-08-09T11:57:29.831220: step 23036, loss 0.567035.
Train: 2018-08-09T11:57:29.942878: step 23037, loss 0.7058.
Train: 2018-08-09T11:57:30.036604: step 23038, loss 0.5274.
Train: 2018-08-09T11:57:30.145985: step 23039, loss 0.530515.
Train: 2018-08-09T11:57:30.239713: step 23040, loss 0.562561.
Test: 2018-08-09T11:57:30.739565: step 23040, loss 0.549616.
Train: 2018-08-09T11:57:30.848944: step 23041, loss 0.577877.
Train: 2018-08-09T11:57:30.958293: step 23042, loss 0.513584.
Train: 2018-08-09T11:57:31.051992: step 23043, loss 0.562789.
Train: 2018-08-09T11:57:31.161341: step 23044, loss 0.480457.
Train: 2018-08-09T11:57:31.270720: step 23045, loss 0.578867.
Train: 2018-08-09T11:57:31.364448: step 23046, loss 0.630077.
Train: 2018-08-09T11:57:31.473796: step 23047, loss 0.544998.
Train: 2018-08-09T11:57:31.583146: step 23048, loss 0.530235.
Train: 2018-08-09T11:57:31.676843: step 23049, loss 0.562415.
Train: 2018-08-09T11:57:31.786192: step 23050, loss 0.451001.
Test: 2018-08-09T11:57:32.287510: step 23050, loss 0.549845.
Train: 2018-08-09T11:57:32.381239: step 23051, loss 0.663486.
Train: 2018-08-09T11:57:32.490588: step 23052, loss 0.576012.
Train: 2018-08-09T11:57:32.615588: step 23053, loss 0.577821.
Train: 2018-08-09T11:57:32.724937: step 23054, loss 0.59564.
Train: 2018-08-09T11:57:32.834289: step 23055, loss 0.549752.
Train: 2018-08-09T11:57:32.928015: step 23056, loss 0.56441.
Train: 2018-08-09T11:57:33.037367: step 23057, loss 0.58051.
Train: 2018-08-09T11:57:33.147401: step 23058, loss 0.532009.
Train: 2018-08-09T11:57:33.241100: step 23059, loss 0.54572.
Train: 2018-08-09T11:57:33.350449: step 23060, loss 0.562343.
Test: 2018-08-09T11:57:33.850330: step 23060, loss 0.545554.
Train: 2018-08-09T11:57:33.959704: step 23061, loss 0.609269.
Train: 2018-08-09T11:57:34.053438: step 23062, loss 0.545571.
Train: 2018-08-09T11:57:34.162782: step 23063, loss 0.659057.
Train: 2018-08-09T11:57:34.272106: step 23064, loss 0.513049.
Train: 2018-08-09T11:57:34.365864: step 23065, loss 0.562287.
Train: 2018-08-09T11:57:34.475184: step 23066, loss 0.56227.
Train: 2018-08-09T11:57:34.584556: step 23067, loss 0.655507.
Train: 2018-08-09T11:57:34.678261: step 23068, loss 0.548142.
Train: 2018-08-09T11:57:34.787610: step 23069, loss 0.603082.
Train: 2018-08-09T11:57:34.896960: step 23070, loss 0.574719.
Test: 2018-08-09T11:57:35.387417: step 23070, loss 0.548779.
Train: 2018-08-09T11:57:35.496797: step 23071, loss 0.631687.
Train: 2018-08-09T11:57:35.606140: step 23072, loss 0.629132.
Train: 2018-08-09T11:57:35.699874: step 23073, loss 0.579112.
Train: 2018-08-09T11:57:35.809193: step 23074, loss 0.578704.
Train: 2018-08-09T11:57:35.918553: step 23075, loss 0.533506.
Train: 2018-08-09T11:57:36.012271: step 23076, loss 0.548571.
Train: 2018-08-09T11:57:36.121649: step 23077, loss 0.594382.
Train: 2018-08-09T11:57:36.231000: step 23078, loss 0.60925.
Train: 2018-08-09T11:57:36.324727: step 23079, loss 0.518229.
Train: 2018-08-09T11:57:36.434076: step 23080, loss 0.51869.
Test: 2018-08-09T11:57:36.933928: step 23080, loss 0.550306.
Train: 2018-08-09T11:57:37.043277: step 23081, loss 0.564391.
Train: 2018-08-09T11:57:37.137036: step 23082, loss 0.563631.
Train: 2018-08-09T11:57:37.246355: step 23083, loss 0.518523.
Train: 2018-08-09T11:57:37.355736: step 23084, loss 0.533923.
Train: 2018-08-09T11:57:37.465083: step 23085, loss 0.5485.
Train: 2018-08-09T11:57:37.558811: step 23086, loss 0.548094.
Train: 2018-08-09T11:57:37.668160: step 23087, loss 0.48505.
Train: 2018-08-09T11:57:37.777510: step 23088, loss 0.624807.
Train: 2018-08-09T11:57:37.886854: step 23089, loss 0.61173.
Train: 2018-08-09T11:57:37.981947: step 23090, loss 0.675982.
Test: 2018-08-09T11:57:38.481801: step 23090, loss 0.549974.
Train: 2018-08-09T11:57:38.591150: step 23091, loss 0.626993.
Train: 2018-08-09T11:57:38.700532: step 23092, loss 0.593754.
Train: 2018-08-09T11:57:38.794256: step 23093, loss 0.548568.
Train: 2018-08-09T11:57:38.903609: step 23094, loss 0.578563.
Train: 2018-08-09T11:57:39.012955: step 23095, loss 0.593785.
Train: 2018-08-09T11:57:39.122274: step 23096, loss 0.517273.
Train: 2018-08-09T11:57:39.216032: step 23097, loss 0.470361.
Train: 2018-08-09T11:57:39.325384: step 23098, loss 0.625981.
Train: 2018-08-09T11:57:39.434700: step 23099, loss 0.501206.
Train: 2018-08-09T11:57:39.544052: step 23100, loss 0.563527.
Test: 2018-08-09T11:57:40.046228: step 23100, loss 0.549197.
Train: 2018-08-09T11:57:40.624213: step 23101, loss 0.547268.
Train: 2018-08-09T11:57:40.717941: step 23102, loss 0.453515.
Train: 2018-08-09T11:57:40.827292: step 23103, loss 0.497769.
Train: 2018-08-09T11:57:40.936639: step 23104, loss 0.596393.
Train: 2018-08-09T11:57:41.045983: step 23105, loss 0.595577.
Train: 2018-08-09T11:57:41.155341: step 23106, loss 0.53104.
Train: 2018-08-09T11:57:41.249069: step 23107, loss 0.513316.
Train: 2018-08-09T11:57:41.358415: step 23108, loss 0.545193.
Train: 2018-08-09T11:57:41.467764: step 23109, loss 0.683341.
Train: 2018-08-09T11:57:41.577116: step 23110, loss 0.56553.
Test: 2018-08-09T11:57:42.062005: step 23110, loss 0.54984.
Train: 2018-08-09T11:57:42.171384: step 23111, loss 0.52589.
Train: 2018-08-09T11:57:42.280733: step 23112, loss 0.595933.
Train: 2018-08-09T11:57:42.390086: step 23113, loss 0.529658.
Train: 2018-08-09T11:57:42.499402: step 23114, loss 0.579278.
Train: 2018-08-09T11:57:42.593160: step 23115, loss 0.56442.
Train: 2018-08-09T11:57:42.702478: step 23116, loss 0.628117.
Train: 2018-08-09T11:57:42.811858: step 23117, loss 0.545739.
Train: 2018-08-09T11:57:42.905586: step 23118, loss 0.579949.
Train: 2018-08-09T11:57:43.014938: step 23119, loss 0.593729.
Train: 2018-08-09T11:57:43.124285: step 23120, loss 0.658484.
Test: 2018-08-09T11:57:43.624138: step 23120, loss 0.54755.
Train: 2018-08-09T11:57:43.733516: step 23121, loss 0.564943.
Train: 2018-08-09T11:57:43.827244: step 23122, loss 0.546084.
Train: 2018-08-09T11:57:43.936562: step 23123, loss 0.466507.
Train: 2018-08-09T11:57:44.045913: step 23124, loss 0.498197.
Train: 2018-08-09T11:57:44.139639: step 23125, loss 0.594042.
Train: 2018-08-09T11:57:44.249019: step 23126, loss 0.513333.
Train: 2018-08-09T11:57:44.358368: step 23127, loss 0.511545.
Train: 2018-08-09T11:57:44.467687: step 23128, loss 0.54626.
Train: 2018-08-09T11:57:44.577067: step 23129, loss 0.56055.
Train: 2018-08-09T11:57:44.670795: step 23130, loss 0.515722.
Test: 2018-08-09T11:57:45.172973: step 23130, loss 0.55104.
Train: 2018-08-09T11:57:45.282352: step 23131, loss 0.543422.
Train: 2018-08-09T11:57:45.391701: step 23132, loss 0.578967.
Train: 2018-08-09T11:57:45.501050: step 23133, loss 0.620517.
Train: 2018-08-09T11:57:45.594778: step 23134, loss 0.552488.
Train: 2018-08-09T11:57:45.704131: step 23135, loss 0.579876.
Train: 2018-08-09T11:57:45.813446: step 23136, loss 0.527512.
Train: 2018-08-09T11:57:45.907204: step 23137, loss 0.580602.
Train: 2018-08-09T11:57:46.016553: step 23138, loss 0.580258.
Train: 2018-08-09T11:57:46.125873: step 23139, loss 0.682467.
Train: 2018-08-09T11:57:46.235252: step 23140, loss 0.498499.
Test: 2018-08-09T11:57:46.735106: step 23140, loss 0.546979.
Train: 2018-08-09T11:57:46.844478: step 23141, loss 0.514051.
Train: 2018-08-09T11:57:46.938774: step 23142, loss 0.578449.
Train: 2018-08-09T11:57:47.048120: step 23143, loss 0.563535.
Train: 2018-08-09T11:57:47.157442: step 23144, loss 0.594169.
Train: 2018-08-09T11:57:47.266789: step 23145, loss 0.529776.
Train: 2018-08-09T11:57:47.360546: step 23146, loss 0.514012.
Train: 2018-08-09T11:57:47.469896: step 23147, loss 0.496924.
Train: 2018-08-09T11:57:47.579214: step 23148, loss 0.530542.
Train: 2018-08-09T11:57:47.680710: step 23149, loss 0.628243.
Train: 2018-08-09T11:57:47.790060: step 23150, loss 0.479593.
Test: 2018-08-09T11:57:48.289943: step 23150, loss 0.548319.
Train: 2018-08-09T11:57:48.383700: step 23151, loss 0.477589.
Train: 2018-08-09T11:57:48.493018: step 23152, loss 0.47841.
Train: 2018-08-09T11:57:48.602398: step 23153, loss 0.442825.
Train: 2018-08-09T11:57:48.711747: step 23154, loss 0.582731.
Train: 2018-08-09T11:57:48.821066: step 23155, loss 0.510299.
Train: 2018-08-09T11:57:48.930447: step 23156, loss 0.604693.
Train: 2018-08-09T11:57:49.024174: step 23157, loss 0.474657.
Train: 2018-08-09T11:57:49.133523: step 23158, loss 0.54842.
Train: 2018-08-09T11:57:49.242843: step 23159, loss 0.508341.
Train: 2018-08-09T11:57:49.352222: step 23160, loss 0.51219.
Test: 2018-08-09T11:57:49.852074: step 23160, loss 0.546453.
Train: 2018-08-09T11:57:49.992695: step 23161, loss 0.522034.
Train: 2018-08-09T11:57:50.102042: step 23162, loss 0.718578.
Train: 2018-08-09T11:57:50.211394: step 23163, loss 0.548861.
Train: 2018-08-09T11:57:50.312595: step 23164, loss 0.562332.
Train: 2018-08-09T11:57:50.421945: step 23165, loss 0.546308.
Train: 2018-08-09T11:57:50.531264: step 23166, loss 0.663545.
Train: 2018-08-09T11:57:50.640612: step 23167, loss 0.644773.
Train: 2018-08-09T11:57:50.749962: step 23168, loss 0.595403.
Train: 2018-08-09T11:57:50.859310: step 23169, loss 0.578791.
Train: 2018-08-09T11:57:50.963421: step 23170, loss 0.529794.
Test: 2018-08-09T11:57:51.463272: step 23170, loss 0.549869.
Train: 2018-08-09T11:57:51.572622: step 23171, loss 0.464375.
Train: 2018-08-09T11:57:51.666349: step 23172, loss 0.596238.
Train: 2018-08-09T11:57:51.775729: step 23173, loss 0.479502.
Train: 2018-08-09T11:57:51.885078: step 23174, loss 0.579738.
Train: 2018-08-09T11:57:51.994429: step 23175, loss 0.446539.
Train: 2018-08-09T11:57:52.103778: step 23176, loss 0.512135.
Train: 2018-08-09T11:57:52.213126: step 23177, loss 0.652021.
Train: 2018-08-09T11:57:52.322444: step 23178, loss 0.546492.
Train: 2018-08-09T11:57:52.431825: step 23179, loss 0.614228.
Train: 2018-08-09T11:57:52.525552: step 23180, loss 0.597108.
Test: 2018-08-09T11:57:53.025404: step 23180, loss 0.551988.
Train: 2018-08-09T11:57:53.134783: step 23181, loss 0.612569.
Train: 2018-08-09T11:57:53.244128: step 23182, loss 0.545821.
Train: 2018-08-09T11:57:53.353482: step 23183, loss 0.54665.
Train: 2018-08-09T11:57:53.447210: step 23184, loss 0.513521.
Train: 2018-08-09T11:57:53.556559: step 23185, loss 0.546167.
Train: 2018-08-09T11:57:53.665878: step 23186, loss 0.529989.
Train: 2018-08-09T11:57:53.775227: step 23187, loss 0.595274.
Train: 2018-08-09T11:57:53.868987: step 23188, loss 0.595479.
Train: 2018-08-09T11:57:53.979913: step 23189, loss 0.513431.
Train: 2018-08-09T11:57:54.089230: step 23190, loss 0.513794.
Test: 2018-08-09T11:57:54.589113: step 23190, loss 0.550918.
Train: 2018-08-09T11:57:54.698491: step 23191, loss 0.660826.
Train: 2018-08-09T11:57:54.792189: step 23192, loss 0.595151.
Train: 2018-08-09T11:57:54.901569: step 23193, loss 0.57907.
Train: 2018-08-09T11:57:55.010918: step 23194, loss 0.530046.
Train: 2018-08-09T11:57:55.120238: step 23195, loss 0.595072.
Train: 2018-08-09T11:57:55.229619: step 23196, loss 0.59507.
Train: 2018-08-09T11:57:55.323345: step 23197, loss 0.513713.
Train: 2018-08-09T11:57:55.432694: step 23198, loss 0.627756.
Train: 2018-08-09T11:57:55.542014: step 23199, loss 0.578945.
Train: 2018-08-09T11:57:55.651392: step 23200, loss 0.643609.
Test: 2018-08-09T11:57:56.153512: step 23200, loss 0.547595.
Train: 2018-08-09T11:57:56.700257: step 23201, loss 0.530415.
Train: 2018-08-09T11:57:56.809637: step 23202, loss 0.514418.
Train: 2018-08-09T11:57:56.903364: step 23203, loss 0.466257.
Train: 2018-08-09T11:57:57.012713: step 23204, loss 0.562625.
Train: 2018-08-09T11:57:57.122063: step 23205, loss 0.449547.
Train: 2018-08-09T11:57:57.231416: step 23206, loss 0.43273.
Train: 2018-08-09T11:57:57.340761: step 23207, loss 0.496995.
Train: 2018-08-09T11:57:57.450111: step 23208, loss 0.512684.
Train: 2018-08-09T11:57:57.543841: step 23209, loss 0.511969.
Train: 2018-08-09T11:57:57.653188: step 23210, loss 0.568593.
Test: 2018-08-09T11:57:58.154519: step 23210, loss 0.546746.
Train: 2018-08-09T11:57:58.263844: step 23211, loss 0.616208.
Train: 2018-08-09T11:57:58.357602: step 23212, loss 0.546531.
Train: 2018-08-09T11:57:58.466951: step 23213, loss 0.546189.
Train: 2018-08-09T11:57:58.576301: step 23214, loss 0.597297.
Train: 2018-08-09T11:57:58.685649: step 23215, loss 0.733088.
Train: 2018-08-09T11:57:58.794968: step 23216, loss 0.612308.
Train: 2018-08-09T11:57:58.904347: step 23217, loss 0.445494.
Train: 2018-08-09T11:57:58.998075: step 23218, loss 0.495714.
Train: 2018-08-09T11:57:59.107425: step 23219, loss 0.662531.
Train: 2018-08-09T11:57:59.216774: step 23220, loss 0.512385.
Test: 2018-08-09T11:57:59.716627: step 23220, loss 0.548653.
Train: 2018-08-09T11:57:59.810384: step 23221, loss 0.629111.
Train: 2018-08-09T11:57:59.921051: step 23222, loss 0.545899.
Train: 2018-08-09T11:58:00.030433: step 23223, loss 0.595786.
Train: 2018-08-09T11:58:00.139787: step 23224, loss 0.646033.
Train: 2018-08-09T11:58:00.249124: step 23225, loss 0.579089.
Train: 2018-08-09T11:58:00.358451: step 23226, loss 0.61231.
Train: 2018-08-09T11:58:00.452207: step 23227, loss 0.496524.
Train: 2018-08-09T11:58:00.561556: step 23228, loss 0.529597.
Train: 2018-08-09T11:58:00.670900: step 23229, loss 0.513337.
Train: 2018-08-09T11:58:00.780255: step 23230, loss 0.529763.
Test: 2018-08-09T11:58:01.280106: step 23230, loss 0.548412.
Train: 2018-08-09T11:58:01.373867: step 23231, loss 0.513327.
Train: 2018-08-09T11:58:01.483213: step 23232, loss 0.64466.
Train: 2018-08-09T11:58:01.592532: step 23233, loss 0.595366.
Train: 2018-08-09T11:58:01.701882: step 23234, loss 0.431345.
Train: 2018-08-09T11:58:01.795610: step 23235, loss 0.66112.
Train: 2018-08-09T11:58:01.918288: step 23236, loss 0.579011.
Train: 2018-08-09T11:58:02.015054: step 23237, loss 0.628239.
Train: 2018-08-09T11:58:02.124436: step 23238, loss 0.529847.
Train: 2018-08-09T11:58:02.233783: step 23239, loss 0.529853.
Train: 2018-08-09T11:58:02.327513: step 23240, loss 0.497179.
Test: 2018-08-09T11:58:02.827388: step 23240, loss 0.546615.
Train: 2018-08-09T11:58:02.936743: step 23241, loss 0.464243.
Train: 2018-08-09T11:58:03.046086: step 23242, loss 0.529838.
Train: 2018-08-09T11:58:03.155412: step 23243, loss 0.562576.
Train: 2018-08-09T11:58:03.264764: step 23244, loss 0.46324.
Train: 2018-08-09T11:58:03.358522: step 23245, loss 0.545303.
Train: 2018-08-09T11:58:03.467869: step 23246, loss 0.596255.
Train: 2018-08-09T11:58:03.577218: step 23247, loss 0.645926.
Train: 2018-08-09T11:58:03.686566: step 23248, loss 0.598434.
Train: 2018-08-09T11:58:03.795910: step 23249, loss 0.648507.
Train: 2018-08-09T11:58:03.889643: step 23250, loss 0.545555.
Test: 2018-08-09T11:58:04.389496: step 23250, loss 0.548665.
Train: 2018-08-09T11:58:04.498875: step 23251, loss 0.579329.
Train: 2018-08-09T11:58:04.608195: step 23252, loss 0.578985.
Train: 2018-08-09T11:58:04.717575: step 23253, loss 0.545859.
Train: 2018-08-09T11:58:04.811301: step 23254, loss 0.496545.
Train: 2018-08-09T11:58:04.922204: step 23255, loss 0.546138.
Train: 2018-08-09T11:58:05.031551: step 23256, loss 0.446634.
Train: 2018-08-09T11:58:05.140900: step 23257, loss 0.529007.
Train: 2018-08-09T11:58:05.234629: step 23258, loss 0.561998.
Train: 2018-08-09T11:58:05.343978: step 23259, loss 0.478162.
Train: 2018-08-09T11:58:05.453297: step 23260, loss 0.628222.
Test: 2018-08-09T11:58:05.953205: step 23260, loss 0.547471.
Train: 2018-08-09T11:58:06.062559: step 23261, loss 0.578965.
Train: 2018-08-09T11:58:06.171909: step 23262, loss 0.492328.
Train: 2018-08-09T11:58:06.275306: step 23263, loss 0.654794.
Train: 2018-08-09T11:58:06.384655: step 23264, loss 0.471044.
Train: 2018-08-09T11:58:06.493998: step 23265, loss 0.650247.
Train: 2018-08-09T11:58:06.587702: step 23266, loss 0.473747.
Train: 2018-08-09T11:58:06.697052: step 23267, loss 0.560283.
Train: 2018-08-09T11:58:06.806430: step 23268, loss 0.62287.
Train: 2018-08-09T11:58:06.915780: step 23269, loss 0.579871.
Train: 2018-08-09T11:58:07.009508: step 23270, loss 0.579934.
Test: 2018-08-09T11:58:07.509391: step 23270, loss 0.548073.
Train: 2018-08-09T11:58:07.618739: step 23271, loss 0.476909.
Train: 2018-08-09T11:58:07.728089: step 23272, loss 0.61501.
Train: 2018-08-09T11:58:07.837406: step 23273, loss 0.683101.
Train: 2018-08-09T11:58:07.931166: step 23274, loss 0.563679.
Train: 2018-08-09T11:58:08.040518: step 23275, loss 0.494633.
Train: 2018-08-09T11:58:08.149835: step 23276, loss 0.479594.
Train: 2018-08-09T11:58:08.259212: step 23277, loss 0.546282.
Train: 2018-08-09T11:58:08.368533: step 23278, loss 0.611827.
Train: 2018-08-09T11:58:08.477887: step 23279, loss 0.529705.
Train: 2018-08-09T11:58:08.593159: step 23280, loss 0.694696.
Test: 2018-08-09T11:58:09.089788: step 23280, loss 0.550282.
Train: 2018-08-09T11:58:09.199166: step 23281, loss 0.57908.
Train: 2018-08-09T11:58:09.308512: step 23282, loss 0.546113.
Train: 2018-08-09T11:58:09.402244: step 23283, loss 0.52992.
Train: 2018-08-09T11:58:09.511597: step 23284, loss 0.497468.
Train: 2018-08-09T11:58:09.620912: step 23285, loss 0.64405.
Train: 2018-08-09T11:58:09.730291: step 23286, loss 0.465133.
Train: 2018-08-09T11:58:09.839640: step 23287, loss 0.497671.
Train: 2018-08-09T11:58:09.933370: step 23288, loss 0.562553.
Train: 2018-08-09T11:58:10.042717: step 23289, loss 0.57883.
Train: 2018-08-09T11:58:10.152067: step 23290, loss 0.595072.
Test: 2018-08-09T11:58:10.651943: step 23290, loss 0.547867.
Train: 2018-08-09T11:58:10.761291: step 23291, loss 0.497166.
Train: 2018-08-09T11:58:10.855026: step 23292, loss 0.578958.
Train: 2018-08-09T11:58:10.964375: step 23293, loss 0.529306.
Train: 2018-08-09T11:58:11.073724: step 23294, loss 0.463547.
Train: 2018-08-09T11:58:11.167453: step 23295, loss 0.530034.
Train: 2018-08-09T11:58:11.276801: step 23296, loss 0.546489.
Train: 2018-08-09T11:58:11.386150: step 23297, loss 0.597721.
Train: 2018-08-09T11:58:11.495503: step 23298, loss 0.64723.
Train: 2018-08-09T11:58:11.589197: step 23299, loss 0.495728.
Train: 2018-08-09T11:58:11.698581: step 23300, loss 0.495354.
Test: 2018-08-09T11:58:12.199997: step 23300, loss 0.547932.
Train: 2018-08-09T11:58:12.777986: step 23301, loss 0.579105.
Train: 2018-08-09T11:58:12.887365: step 23302, loss 0.511847.
Train: 2018-08-09T11:58:12.981094: step 23303, loss 0.477773.
Train: 2018-08-09T11:58:13.090413: step 23304, loss 0.561573.
Train: 2018-08-09T11:58:13.199791: step 23305, loss 0.543944.
Train: 2018-08-09T11:58:13.309112: step 23306, loss 0.596682.
Train: 2018-08-09T11:58:13.402869: step 23307, loss 0.492583.
Train: 2018-08-09T11:58:13.512218: step 23308, loss 0.545784.
Train: 2018-08-09T11:58:13.621567: step 23309, loss 0.600957.
Train: 2018-08-09T11:58:13.730887: step 23310, loss 0.528844.
Test: 2018-08-09T11:58:14.216509: step 23310, loss 0.544496.
Train: 2018-08-09T11:58:14.325864: step 23311, loss 0.638595.
Train: 2018-08-09T11:58:14.435212: step 23312, loss 0.509513.
Train: 2018-08-09T11:58:14.528911: step 23313, loss 0.492414.
Train: 2018-08-09T11:58:14.638261: step 23314, loss 0.492666.
Train: 2018-08-09T11:58:14.747640: step 23315, loss 0.529339.
Train: 2018-08-09T11:58:14.856988: step 23316, loss 0.527457.
Train: 2018-08-09T11:58:14.950716: step 23317, loss 0.438838.
Train: 2018-08-09T11:58:15.060065: step 23318, loss 0.632838.
Train: 2018-08-09T11:58:15.169385: step 23319, loss 0.635012.
Train: 2018-08-09T11:58:15.263144: step 23320, loss 0.545354.
Test: 2018-08-09T11:58:15.762996: step 23320, loss 0.547186.
Train: 2018-08-09T11:58:15.872375: step 23321, loss 0.598104.
Train: 2018-08-09T11:58:15.983116: step 23322, loss 0.562778.
Train: 2018-08-09T11:58:16.076845: step 23323, loss 0.510508.
Train: 2018-08-09T11:58:16.186194: step 23324, loss 0.632888.
Train: 2018-08-09T11:58:16.295513: step 23325, loss 0.545057.
Train: 2018-08-09T11:58:16.389270: step 23326, loss 0.527725.
Train: 2018-08-09T11:58:16.498623: step 23327, loss 0.579304.
Train: 2018-08-09T11:58:16.607939: step 23328, loss 0.51152.
Train: 2018-08-09T11:58:16.717288: step 23329, loss 0.546429.
Train: 2018-08-09T11:58:16.811047: step 23330, loss 0.613091.
Test: 2018-08-09T11:58:17.310900: step 23330, loss 0.550965.
Train: 2018-08-09T11:58:17.420278: step 23331, loss 0.528625.
Train: 2018-08-09T11:58:17.514006: step 23332, loss 0.579479.
Train: 2018-08-09T11:58:17.623349: step 23333, loss 0.579171.
Train: 2018-08-09T11:58:17.732704: step 23334, loss 0.545589.
Train: 2018-08-09T11:58:17.842054: step 23335, loss 0.512053.
Train: 2018-08-09T11:58:17.951990: step 23336, loss 0.562363.
Train: 2018-08-09T11:58:18.045747: step 23337, loss 0.528811.
Train: 2018-08-09T11:58:18.155065: step 23338, loss 0.646362.
Train: 2018-08-09T11:58:18.264416: step 23339, loss 0.646153.
Train: 2018-08-09T11:58:18.358173: step 23340, loss 0.595828.
Test: 2018-08-09T11:58:18.858029: step 23340, loss 0.547456.
Train: 2018-08-09T11:58:18.967406: step 23341, loss 0.562438.
Train: 2018-08-09T11:58:19.076727: step 23342, loss 0.57906.
Train: 2018-08-09T11:58:19.170482: step 23343, loss 0.595569.
Train: 2018-08-09T11:58:19.279825: step 23344, loss 0.463395.
Train: 2018-08-09T11:58:19.389175: step 23345, loss 0.545904.
Train: 2018-08-09T11:58:19.498532: step 23346, loss 0.529655.
Train: 2018-08-09T11:58:19.592257: step 23347, loss 0.579044.
Train: 2018-08-09T11:58:19.701607: step 23348, loss 0.611994.
Train: 2018-08-09T11:58:19.810956: step 23349, loss 0.776549.
Train: 2018-08-09T11:58:19.920275: step 23350, loss 0.529772.
Test: 2018-08-09T11:58:20.404547: step 23350, loss 0.549789.
Train: 2018-08-09T11:58:20.514503: step 23351, loss 0.595258.
Train: 2018-08-09T11:58:20.623882: step 23352, loss 0.578912.
Train: 2018-08-09T11:58:20.717611: step 23353, loss 0.465499.
Train: 2018-08-09T11:58:20.826931: step 23354, loss 0.627412.
Train: 2018-08-09T11:58:20.939449: step 23355, loss 0.498208.
Train: 2018-08-09T11:58:21.033666: step 23356, loss 0.562769.
Train: 2018-08-09T11:58:21.143014: step 23357, loss 0.611092.
Train: 2018-08-09T11:58:21.252333: step 23358, loss 0.546716.
Train: 2018-08-09T11:58:21.346092: step 23359, loss 0.578859.
Train: 2018-08-09T11:58:21.455411: step 23360, loss 0.562829.
Test: 2018-08-09T11:58:21.955294: step 23360, loss 0.548437.
Train: 2018-08-09T11:58:22.049053: step 23361, loss 0.562857.
Train: 2018-08-09T11:58:22.158401: step 23362, loss 0.562854.
Train: 2018-08-09T11:58:22.267751: step 23363, loss 0.626849.
Train: 2018-08-09T11:58:22.377067: step 23364, loss 0.562895.
Train: 2018-08-09T11:58:22.470827: step 23365, loss 0.610763.
Train: 2018-08-09T11:58:22.580145: step 23366, loss 0.483386.
Train: 2018-08-09T11:58:22.689526: step 23367, loss 0.578855.
Train: 2018-08-09T11:58:22.783252: step 23368, loss 0.483446.
Train: 2018-08-09T11:58:22.892602: step 23369, loss 0.531081.
Train: 2018-08-09T11:58:23.001951: step 23370, loss 0.626695.
Test: 2018-08-09T11:58:23.486182: step 23370, loss 0.550371.
Train: 2018-08-09T11:58:23.595561: step 23371, loss 0.51505.
Train: 2018-08-09T11:58:23.704910: step 23372, loss 0.578916.
Train: 2018-08-09T11:58:23.798640: step 23373, loss 0.498902.
Train: 2018-08-09T11:58:23.907987: step 23374, loss 0.595014.
Train: 2018-08-09T11:58:24.017306: step 23375, loss 0.578799.
Train: 2018-08-09T11:58:24.111036: step 23376, loss 0.546766.
Train: 2018-08-09T11:58:24.220415: step 23377, loss 0.595021.
Train: 2018-08-09T11:58:24.329763: step 23378, loss 0.65948.
Train: 2018-08-09T11:58:24.423461: step 23379, loss 0.610988.
Train: 2018-08-09T11:58:24.532843: step 23380, loss 0.594881.
Test: 2018-08-09T11:58:25.019399: step 23380, loss 0.550883.
Train: 2018-08-09T11:58:25.128754: step 23381, loss 0.610855.
Train: 2018-08-09T11:58:25.238103: step 23382, loss 0.610766.
Train: 2018-08-09T11:58:25.331831: step 23383, loss 0.562921.
Train: 2018-08-09T11:58:25.441182: step 23384, loss 0.499504.
Train: 2018-08-09T11:58:25.550527: step 23385, loss 0.562991.
Train: 2018-08-09T11:58:25.644226: step 23386, loss 0.610502.
Train: 2018-08-09T11:58:25.753607: step 23387, loss 0.594619.
Train: 2018-08-09T11:58:25.862956: step 23388, loss 0.547297.
Train: 2018-08-09T11:58:25.956679: step 23389, loss 0.531563.
Train: 2018-08-09T11:58:26.066035: step 23390, loss 0.594611.
Test: 2018-08-09T11:58:26.565904: step 23390, loss 0.549526.
Train: 2018-08-09T11:58:26.659645: step 23391, loss 0.657725.
Train: 2018-08-09T11:58:26.768993: step 23392, loss 0.56307.
Train: 2018-08-09T11:58:26.878342: step 23393, loss 0.578919.
Train: 2018-08-09T11:58:26.974346: step 23394, loss 0.594448.
Train: 2018-08-09T11:58:27.083665: step 23395, loss 0.594573.
Train: 2018-08-09T11:58:27.193013: step 23396, loss 0.547737.
Train: 2018-08-09T11:58:27.286772: step 23397, loss 0.578805.
Train: 2018-08-09T11:58:27.396125: step 23398, loss 0.470063.
Train: 2018-08-09T11:58:27.505464: step 23399, loss 0.547767.
Train: 2018-08-09T11:58:27.599169: step 23400, loss 0.500927.
Test: 2018-08-09T11:58:28.099081: step 23400, loss 0.54914.
Train: 2018-08-09T11:58:28.692661: step 23401, loss 0.516418.
Train: 2018-08-09T11:58:28.786419: step 23402, loss 0.56336.
Train: 2018-08-09T11:58:28.895771: step 23403, loss 0.562993.
Train: 2018-08-09T11:58:29.007514: step 23404, loss 0.562272.
Train: 2018-08-09T11:58:29.101242: step 23405, loss 0.514108.
Train: 2018-08-09T11:58:29.210592: step 23406, loss 0.576966.
Train: 2018-08-09T11:58:29.304349: step 23407, loss 0.647159.
Train: 2018-08-09T11:58:29.413697: step 23408, loss 0.545333.
Train: 2018-08-09T11:58:29.523047: step 23409, loss 0.482144.
Train: 2018-08-09T11:58:29.632397: step 23410, loss 0.648921.
Test: 2018-08-09T11:58:30.132249: step 23410, loss 0.549484.
Train: 2018-08-09T11:58:30.225976: step 23411, loss 0.628567.
Train: 2018-08-09T11:58:30.335356: step 23412, loss 0.545551.
Train: 2018-08-09T11:58:30.444705: step 23413, loss 0.562821.
Train: 2018-08-09T11:58:30.538436: step 23414, loss 0.595616.
Train: 2018-08-09T11:58:30.647782: step 23415, loss 0.563304.
Train: 2018-08-09T11:58:30.741480: step 23416, loss 0.659192.
Train: 2018-08-09T11:58:30.850860: step 23417, loss 0.611709.
Train: 2018-08-09T11:58:30.962688: step 23418, loss 0.515573.
Train: 2018-08-09T11:58:31.056385: step 23419, loss 0.531245.
Train: 2018-08-09T11:58:31.165764: step 23420, loss 0.468318.
Test: 2018-08-09T11:58:31.665617: step 23420, loss 0.550069.
Train: 2018-08-09T11:58:31.774966: step 23421, loss 0.626396.
Train: 2018-08-09T11:58:31.868723: step 23422, loss 0.578875.
Train: 2018-08-09T11:58:31.978074: step 23423, loss 0.531526.
Train: 2018-08-09T11:58:32.087423: step 23424, loss 0.515796.
Train: 2018-08-09T11:58:32.181151: step 23425, loss 0.578881.
Train: 2018-08-09T11:58:32.290469: step 23426, loss 0.547326.
Train: 2018-08-09T11:58:32.399819: step 23427, loss 0.642024.
Train: 2018-08-09T11:58:32.493577: step 23428, loss 0.594661.
Train: 2018-08-09T11:58:32.602926: step 23429, loss 0.51565.
Train: 2018-08-09T11:58:32.712276: step 23430, loss 0.594688.
Test: 2018-08-09T11:58:33.197869: step 23430, loss 0.548234.
Train: 2018-08-09T11:58:33.307248: step 23431, loss 0.515615.
Train: 2018-08-09T11:58:33.416568: step 23432, loss 0.578879.
Train: 2018-08-09T11:58:33.510326: step 23433, loss 0.563015.
Train: 2018-08-09T11:58:33.619674: step 23434, loss 0.705657.
Train: 2018-08-09T11:58:33.713403: step 23435, loss 0.594652.
Train: 2018-08-09T11:58:33.822753: step 23436, loss 0.531498.
Train: 2018-08-09T11:58:33.932101: step 23437, loss 0.610415.
Train: 2018-08-09T11:58:34.025829: step 23438, loss 0.484342.
Train: 2018-08-09T11:58:34.135147: step 23439, loss 0.64186.
Train: 2018-08-09T11:58:34.244528: step 23440, loss 0.594629.
Test: 2018-08-09T11:58:34.728784: step 23440, loss 0.549603.
Train: 2018-08-09T11:58:34.838137: step 23441, loss 0.547481.
Train: 2018-08-09T11:58:34.948893: step 23442, loss 0.657357.
Train: 2018-08-09T11:58:35.058253: step 23443, loss 0.531873.
Train: 2018-08-09T11:58:35.151951: step 23444, loss 0.516288.
Train: 2018-08-09T11:58:35.261299: step 23445, loss 0.516259.
Train: 2018-08-09T11:58:35.370678: step 23446, loss 0.563379.
Train: 2018-08-09T11:58:35.464407: step 23447, loss 0.610213.
Train: 2018-08-09T11:58:35.573757: step 23448, loss 0.578876.
Train: 2018-08-09T11:58:35.667453: step 23449, loss 0.516316.
Train: 2018-08-09T11:58:35.776833: step 23450, loss 0.594561.
Test: 2018-08-09T11:58:36.276686: step 23450, loss 0.552641.
Train: 2018-08-09T11:58:36.370446: step 23451, loss 0.54753.
Train: 2018-08-09T11:58:36.479793: step 23452, loss 0.484675.
Train: 2018-08-09T11:58:36.589141: step 23453, loss 0.547462.
Train: 2018-08-09T11:58:36.682870: step 23454, loss 0.563162.
Train: 2018-08-09T11:58:36.792219: step 23455, loss 0.484001.
Train: 2018-08-09T11:58:36.901568: step 23456, loss 0.658329.
Train: 2018-08-09T11:58:36.996696: step 23457, loss 0.578891.
Train: 2018-08-09T11:58:37.106022: step 23458, loss 0.562906.
Train: 2018-08-09T11:58:37.215371: step 23459, loss 0.483349.
Train: 2018-08-09T11:58:37.309098: step 23460, loss 0.563024.
Test: 2018-08-09T11:58:37.809028: step 23460, loss 0.549666.
Train: 2018-08-09T11:58:37.918361: step 23461, loss 0.530749.
Train: 2018-08-09T11:58:38.027710: step 23462, loss 0.498563.
Train: 2018-08-09T11:58:38.121437: step 23463, loss 0.546572.
Train: 2018-08-09T11:58:38.230786: step 23464, loss 0.513748.
Train: 2018-08-09T11:58:38.340105: step 23465, loss 0.595653.
Train: 2018-08-09T11:58:38.433864: step 23466, loss 0.512389.
Train: 2018-08-09T11:58:38.543183: step 23467, loss 0.546147.
Train: 2018-08-09T11:58:38.652562: step 23468, loss 0.595121.
Train: 2018-08-09T11:58:38.746260: step 23469, loss 0.59677.
Train: 2018-08-09T11:58:38.855610: step 23470, loss 0.63041.
Test: 2018-08-09T11:58:39.357918: step 23470, loss 0.54912.
Train: 2018-08-09T11:58:39.465784: step 23471, loss 0.528628.
Train: 2018-08-09T11:58:39.556108: step 23472, loss 0.581147.
Train: 2018-08-09T11:58:39.665457: step 23473, loss 0.646531.
Train: 2018-08-09T11:58:39.774805: step 23474, loss 0.579387.
Train: 2018-08-09T11:58:39.868533: step 23475, loss 0.660995.
Train: 2018-08-09T11:58:39.977884: step 23476, loss 0.546002.
Train: 2018-08-09T11:58:40.087232: step 23477, loss 0.578935.
Train: 2018-08-09T11:58:40.180961: step 23478, loss 0.562679.
Train: 2018-08-09T11:58:40.290309: step 23479, loss 0.643721.
Train: 2018-08-09T11:58:40.399629: step 23480, loss 0.530419.
Test: 2018-08-09T11:58:40.899512: step 23480, loss 0.547104.
Train: 2018-08-09T11:58:41.000507: step 23481, loss 0.498434.
Train: 2018-08-09T11:58:41.109857: step 23482, loss 0.643161.
Train: 2018-08-09T11:58:41.203582: step 23483, loss 0.578882.
Train: 2018-08-09T11:58:41.312902: step 23484, loss 0.562929.
Train: 2018-08-09T11:58:41.422280: step 23485, loss 0.514954.
Train: 2018-08-09T11:58:41.516008: step 23486, loss 0.610806.
Train: 2018-08-09T11:58:41.625327: step 23487, loss 0.530988.
Train: 2018-08-09T11:58:41.734707: step 23488, loss 0.626702.
Train: 2018-08-09T11:58:41.827389: step 23489, loss 0.499247.
Train: 2018-08-09T11:58:41.936707: step 23490, loss 0.578859.
Test: 2018-08-09T11:58:42.436591: step 23490, loss 0.54804.
Train: 2018-08-09T11:58:42.545969: step 23491, loss 0.419753.
Train: 2018-08-09T11:58:42.639697: step 23492, loss 0.594797.
Train: 2018-08-09T11:58:42.749046: step 23493, loss 0.499029.
Train: 2018-08-09T11:58:42.858396: step 23494, loss 0.514847.
Train: 2018-08-09T11:58:42.952094: step 23495, loss 0.59491.
Train: 2018-08-09T11:58:43.061475: step 23496, loss 0.578914.
Train: 2018-08-09T11:58:43.170793: step 23497, loss 0.514386.
Train: 2018-08-09T11:58:43.264550: step 23498, loss 0.578973.
Train: 2018-08-09T11:58:43.373870: step 23499, loss 0.530344.
Train: 2018-08-09T11:58:43.483244: step 23500, loss 0.530263.
Test: 2018-08-09T11:58:43.985664: step 23500, loss 0.548009.
Train: 2018-08-09T11:58:44.563622: step 23501, loss 0.546386.
Train: 2018-08-09T11:58:44.657381: step 23502, loss 0.448454.
Train: 2018-08-09T11:58:44.766733: step 23503, loss 0.529694.
Train: 2018-08-09T11:58:44.876059: step 23504, loss 0.529452.
Train: 2018-08-09T11:58:44.969806: step 23505, loss 0.529448.
Train: 2018-08-09T11:58:45.079156: step 23506, loss 0.546055.
Train: 2018-08-09T11:58:45.172883: step 23507, loss 0.528389.
Train: 2018-08-09T11:58:45.282203: step 23508, loss 0.458864.
Train: 2018-08-09T11:58:45.391585: step 23509, loss 0.595902.
Train: 2018-08-09T11:58:45.485309: step 23510, loss 0.598234.
Test: 2018-08-09T11:58:45.987483: step 23510, loss 0.547179.
Train: 2018-08-09T11:58:46.096864: step 23511, loss 0.507176.
Train: 2018-08-09T11:58:46.190590: step 23512, loss 0.450289.
Train: 2018-08-09T11:58:46.299939: step 23513, loss 0.395222.
Train: 2018-08-09T11:58:46.409257: step 23514, loss 0.657132.
Train: 2018-08-09T11:58:46.518607: step 23515, loss 0.464509.
Train: 2018-08-09T11:58:46.612336: step 23516, loss 0.706137.
Train: 2018-08-09T11:58:46.721718: step 23517, loss 0.531728.
Train: 2018-08-09T11:58:46.831035: step 23518, loss 0.661257.
Train: 2018-08-09T11:58:46.940413: step 23519, loss 0.581608.
Train: 2018-08-09T11:58:47.034111: step 23520, loss 0.580363.
Test: 2018-08-09T11:58:47.533993: step 23520, loss 0.545682.
Train: 2018-08-09T11:58:47.643367: step 23521, loss 0.511862.
Train: 2018-08-09T11:58:47.752722: step 23522, loss 0.546749.
Train: 2018-08-09T11:58:47.846420: step 23523, loss 0.565286.
Train: 2018-08-09T11:58:47.957190: step 23524, loss 0.457102.
Train: 2018-08-09T11:58:48.066510: step 23525, loss 0.598978.
Train: 2018-08-09T11:58:48.160268: step 23526, loss 0.529232.
Train: 2018-08-09T11:58:48.269624: step 23527, loss 0.681216.
Train: 2018-08-09T11:58:48.378936: step 23528, loss 0.527503.
Train: 2018-08-09T11:58:48.472694: step 23529, loss 0.54539.
Train: 2018-08-09T11:58:48.582043: step 23530, loss 0.579025.
Test: 2018-08-09T11:58:49.081895: step 23530, loss 0.546687.
Train: 2018-08-09T11:58:49.175647: step 23531, loss 0.628849.
Train: 2018-08-09T11:58:49.284972: step 23532, loss 0.578816.
Train: 2018-08-09T11:58:49.394355: step 23533, loss 0.496079.
Train: 2018-08-09T11:58:49.488079: step 23534, loss 0.645518.
Train: 2018-08-09T11:58:49.597399: step 23535, loss 0.612089.
Train: 2018-08-09T11:58:49.706778: step 23536, loss 0.562491.
Train: 2018-08-09T11:58:49.816127: step 23537, loss 0.611892.
Train: 2018-08-09T11:58:49.911277: step 23538, loss 0.578961.
Train: 2018-08-09T11:58:50.020658: step 23539, loss 0.578932.
Train: 2018-08-09T11:58:50.130006: step 23540, loss 0.595208.
Test: 2018-08-09T11:58:50.629858: step 23540, loss 0.548099.
Train: 2018-08-09T11:58:50.732212: step 23541, loss 0.514037.
Train: 2018-08-09T11:58:50.841530: step 23542, loss 0.62744.
Train: 2018-08-09T11:58:50.935258: step 23543, loss 0.611148.
Train: 2018-08-09T11:58:51.044638: step 23544, loss 0.546718.
Train: 2018-08-09T11:58:51.153987: step 23545, loss 0.594901.
Train: 2018-08-09T11:58:51.247715: step 23546, loss 0.498955.
Train: 2018-08-09T11:58:51.357064: step 23547, loss 0.515053.
Train: 2018-08-09T11:58:51.466417: step 23548, loss 0.467228.
Train: 2018-08-09T11:58:51.575732: step 23549, loss 0.467084.
Train: 2018-08-09T11:58:51.685083: step 23550, loss 0.562846.
Test: 2018-08-09T11:58:52.184997: step 23550, loss 0.550759.
Train: 2018-08-09T11:58:52.278722: step 23551, loss 0.562657.
Train: 2018-08-09T11:58:52.388071: step 23552, loss 0.62724.
Train: 2018-08-09T11:58:52.497421: step 23553, loss 0.611211.
Train: 2018-08-09T11:58:52.591118: step 23554, loss 0.724556.
Train: 2018-08-09T11:58:52.700499: step 23555, loss 0.434009.
Train: 2018-08-09T11:58:52.809846: step 23556, loss 0.434083.
Train: 2018-08-09T11:58:52.919197: step 23557, loss 0.610996.
Train: 2018-08-09T11:58:53.012924: step 23558, loss 0.513837.
Train: 2018-08-09T11:58:53.122274: step 23559, loss 0.529956.
Train: 2018-08-09T11:58:53.231623: step 23560, loss 0.611789.
Test: 2018-08-09T11:58:53.715854: step 23560, loss 0.550189.
Train: 2018-08-09T11:58:53.825203: step 23561, loss 0.57948.
Train: 2018-08-09T11:58:53.935997: step 23562, loss 0.595758.
Train: 2018-08-09T11:58:54.045346: step 23563, loss 0.512765.
Train: 2018-08-09T11:58:54.139072: step 23564, loss 0.628672.
Train: 2018-08-09T11:58:54.258092: step 23565, loss 0.414481.
Train: 2018-08-09T11:58:54.367466: step 23566, loss 0.598217.
Train: 2018-08-09T11:58:54.461200: step 23567, loss 0.544692.
Train: 2018-08-09T11:58:54.570550: step 23568, loss 0.578686.
Train: 2018-08-09T11:58:54.679899: step 23569, loss 0.645371.
Train: 2018-08-09T11:58:54.789252: step 23570, loss 0.56251.
Test: 2018-08-09T11:58:55.289101: step 23570, loss 0.549562.
Train: 2018-08-09T11:58:55.382858: step 23571, loss 0.626649.
Train: 2018-08-09T11:58:55.492178: step 23572, loss 0.546738.
Train: 2018-08-09T11:58:55.601556: step 23573, loss 0.546096.
Train: 2018-08-09T11:58:55.710906: step 23574, loss 0.448256.
Train: 2018-08-09T11:58:55.820255: step 23575, loss 0.546652.
Train: 2018-08-09T11:58:55.921339: step 23576, loss 0.512981.
Train: 2018-08-09T11:58:56.030722: step 23577, loss 0.529122.
Train: 2018-08-09T11:58:56.140068: step 23578, loss 0.546502.
Train: 2018-08-09T11:58:56.233766: step 23579, loss 0.530661.
Train: 2018-08-09T11:58:56.343145: step 23580, loss 0.562097.
Test: 2018-08-09T11:58:56.842997: step 23580, loss 0.5473.
Train: 2018-08-09T11:58:56.952377: step 23581, loss 0.546797.
Train: 2018-08-09T11:58:57.061720: step 23582, loss 0.428807.
Train: 2018-08-09T11:58:57.171076: step 23583, loss 0.546697.
Train: 2018-08-09T11:58:57.264805: step 23584, loss 0.510446.
Train: 2018-08-09T11:58:57.374152: step 23585, loss 0.615213.
Train: 2018-08-09T11:58:57.483501: step 23586, loss 0.563748.
Train: 2018-08-09T11:58:57.592822: step 23587, loss 0.525585.
Train: 2018-08-09T11:58:57.686549: step 23588, loss 0.595577.
Train: 2018-08-09T11:58:57.795928: step 23589, loss 0.61306.
Train: 2018-08-09T11:58:57.905281: step 23590, loss 0.497019.
Test: 2018-08-09T11:58:58.405129: step 23590, loss 0.548785.
Train: 2018-08-09T11:58:58.514503: step 23591, loss 0.511665.
Train: 2018-08-09T11:58:58.608237: step 23592, loss 0.560593.
Train: 2018-08-09T11:58:58.717554: step 23593, loss 0.633689.
Train: 2018-08-09T11:58:58.826935: step 23594, loss 0.56257.
Train: 2018-08-09T11:58:58.937698: step 23595, loss 0.546149.
Train: 2018-08-09T11:58:59.031397: step 23596, loss 0.562346.
Train: 2018-08-09T11:58:59.140778: step 23597, loss 0.545521.
Train: 2018-08-09T11:58:59.250126: step 23598, loss 0.61254.
Train: 2018-08-09T11:58:59.359443: step 23599, loss 0.545133.
Train: 2018-08-09T11:58:59.453173: step 23600, loss 0.495044.
Test: 2018-08-09T11:58:59.953055: step 23600, loss 0.546573.
Train: 2018-08-09T11:59:00.531044: step 23601, loss 0.5622.
Train: 2018-08-09T11:59:00.640391: step 23602, loss 0.57948.
Train: 2018-08-09T11:59:00.734151: step 23603, loss 0.495215.
Train: 2018-08-09T11:59:00.843470: step 23604, loss 0.512079.
Train: 2018-08-09T11:59:00.954278: step 23605, loss 0.545619.
Train: 2018-08-09T11:59:01.063628: step 23606, loss 0.562727.
Train: 2018-08-09T11:59:01.173007: step 23607, loss 0.663739.
Train: 2018-08-09T11:59:01.282327: step 23608, loss 0.529162.
Train: 2018-08-09T11:59:01.391705: step 23609, loss 0.461783.
Train: 2018-08-09T11:59:01.485402: step 23610, loss 0.68005.
Test: 2018-08-09T11:59:01.985286: step 23610, loss 0.549851.
Train: 2018-08-09T11:59:02.094659: step 23611, loss 0.54561.
Train: 2018-08-09T11:59:02.203983: step 23612, loss 0.646149.
Train: 2018-08-09T11:59:02.313333: step 23613, loss 0.595747.
Train: 2018-08-09T11:59:02.407091: step 23614, loss 0.412444.
Train: 2018-08-09T11:59:02.516440: step 23615, loss 0.645795.
Train: 2018-08-09T11:59:02.625789: step 23616, loss 0.61239.
Train: 2018-08-09T11:59:02.735109: step 23617, loss 0.529217.
Train: 2018-08-09T11:59:02.844459: step 23618, loss 0.579056.
Train: 2018-08-09T11:59:02.939596: step 23619, loss 0.446476.
Train: 2018-08-09T11:59:03.048946: step 23620, loss 0.661981.
Test: 2018-08-09T11:59:03.548823: step 23620, loss 0.548835.
Train: 2018-08-09T11:59:03.658178: step 23621, loss 0.512796.
Train: 2018-08-09T11:59:03.767527: step 23622, loss 0.512812.
Train: 2018-08-09T11:59:03.876846: step 23623, loss 0.529432.
Train: 2018-08-09T11:59:03.970604: step 23624, loss 0.579043.
Train: 2018-08-09T11:59:04.079956: step 23625, loss 0.529301.
Train: 2018-08-09T11:59:04.189307: step 23626, loss 0.479502.
Train: 2018-08-09T11:59:04.298652: step 23627, loss 0.529192.
Train: 2018-08-09T11:59:04.408001: step 23628, loss 0.462377.
Train: 2018-08-09T11:59:04.501730: step 23629, loss 0.595952.
Train: 2018-08-09T11:59:04.611079: step 23630, loss 0.495307.
Test: 2018-08-09T11:59:05.112341: step 23630, loss 0.549738.
Train: 2018-08-09T11:59:05.221720: step 23631, loss 0.579074.
Train: 2018-08-09T11:59:05.315450: step 23632, loss 0.477769.
Train: 2018-08-09T11:59:05.424799: step 23633, loss 0.510904.
Train: 2018-08-09T11:59:05.534146: step 23634, loss 0.683447.
Train: 2018-08-09T11:59:05.643465: step 23635, loss 0.684507.
Train: 2018-08-09T11:59:05.752845: step 23636, loss 0.630537.
Train: 2018-08-09T11:59:05.846572: step 23637, loss 0.579204.
Train: 2018-08-09T11:59:05.955924: step 23638, loss 0.562414.
Train: 2018-08-09T11:59:06.065269: step 23639, loss 0.545538.
Train: 2018-08-09T11:59:06.174622: step 23640, loss 0.54556.
Test: 2018-08-09T11:59:06.674505: step 23640, loss 0.547281.
Train: 2018-08-09T11:59:06.768230: step 23641, loss 0.511987.
Train: 2018-08-09T11:59:06.877549: step 23642, loss 0.512018.
Train: 2018-08-09T11:59:06.989438: step 23643, loss 0.528803.
Train: 2018-08-09T11:59:07.098787: step 23644, loss 0.46158.
Train: 2018-08-09T11:59:07.192484: step 23645, loss 0.511883.
Train: 2018-08-09T11:59:07.301864: step 23646, loss 0.494891.
Train: 2018-08-09T11:59:07.411214: step 23647, loss 0.54545.
Train: 2018-08-09T11:59:07.520563: step 23648, loss 0.528429.
Train: 2018-08-09T11:59:07.629912: step 23649, loss 0.52833.
Train: 2018-08-09T11:59:07.723642: step 23650, loss 0.749981.
Test: 2018-08-09T11:59:08.223492: step 23650, loss 0.547652.
Train: 2018-08-09T11:59:08.332871: step 23651, loss 0.562335.
Train: 2018-08-09T11:59:08.442221: step 23652, loss 0.511206.
Train: 2018-08-09T11:59:08.551541: step 23653, loss 0.494145.
Train: 2018-08-09T11:59:08.660890: step 23654, loss 0.545272.
Train: 2018-08-09T11:59:08.770269: step 23655, loss 0.511088.
Train: 2018-08-09T11:59:08.879618: step 23656, loss 0.579445.
Train: 2018-08-09T11:59:08.991240: step 23657, loss 0.596583.
Train: 2018-08-09T11:59:09.084999: step 23658, loss 0.562336.
Train: 2018-08-09T11:59:09.194347: step 23659, loss 0.665103.
Train: 2018-08-09T11:59:09.303697: step 23660, loss 0.682047.
Test: 2018-08-09T11:59:09.803553: step 23660, loss 0.545723.
Train: 2018-08-09T11:59:09.912927: step 23661, loss 0.579388.
Train: 2018-08-09T11:59:10.022271: step 23662, loss 0.579338.
Train: 2018-08-09T11:59:10.116006: step 23663, loss 0.49461.
Train: 2018-08-09T11:59:10.225355: step 23664, loss 0.629962.
Train: 2018-08-09T11:59:10.334704: step 23665, loss 0.629771.
Train: 2018-08-09T11:59:10.444052: step 23666, loss 0.528818.
Train: 2018-08-09T11:59:10.553403: step 23667, loss 0.57915.
Train: 2018-08-09T11:59:10.678342: step 23668, loss 0.595795.
Train: 2018-08-09T11:59:10.787691: step 23669, loss 0.61234.
Train: 2018-08-09T11:59:10.897070: step 23670, loss 0.496196.
Test: 2018-08-09T11:59:11.383616: step 23670, loss 0.549505.
Train: 2018-08-09T11:59:11.492965: step 23671, loss 0.545967.
Train: 2018-08-09T11:59:11.602344: step 23672, loss 0.677969.
Train: 2018-08-09T11:59:11.711693: step 23673, loss 0.595429.
Train: 2018-08-09T11:59:11.805420: step 23674, loss 0.529842.
Train: 2018-08-09T11:59:11.914765: step 23675, loss 0.693253.
Train: 2018-08-09T11:59:12.024089: step 23676, loss 0.48138.
Train: 2018-08-09T11:59:12.133438: step 23677, loss 0.578933.
Train: 2018-08-09T11:59:12.242788: step 23678, loss 0.54658.
Train: 2018-08-09T11:59:12.352168: step 23679, loss 0.498257.
Train: 2018-08-09T11:59:12.461517: step 23680, loss 0.611071.
Test: 2018-08-09T11:59:12.947072: step 23680, loss 0.548938.
Train: 2018-08-09T11:59:13.056451: step 23681, loss 0.530604.
Train: 2018-08-09T11:59:13.165801: step 23682, loss 0.562767.
Train: 2018-08-09T11:59:13.275149: step 23683, loss 0.611061.
Train: 2018-08-09T11:59:13.368876: step 23684, loss 0.498622.
Train: 2018-08-09T11:59:13.478229: step 23685, loss 0.594973.
Train: 2018-08-09T11:59:13.587574: step 23686, loss 0.659062.
Train: 2018-08-09T11:59:13.696925: step 23687, loss 0.514838.
Train: 2018-08-09T11:59:13.806275: step 23688, loss 0.562841.
Train: 2018-08-09T11:59:13.915622: step 23689, loss 0.546906.
Train: 2018-08-09T11:59:14.024945: step 23690, loss 0.530973.
Test: 2018-08-09T11:59:14.509229: step 23690, loss 0.549128.
Train: 2018-08-09T11:59:14.618551: step 23691, loss 0.547056.
Train: 2018-08-09T11:59:14.727931: step 23692, loss 0.578928.
Train: 2018-08-09T11:59:14.837281: step 23693, loss 0.482959.
Train: 2018-08-09T11:59:14.932413: step 23694, loss 0.626818.
Train: 2018-08-09T11:59:15.042667: step 23695, loss 0.578875.
Train: 2018-08-09T11:59:15.152017: step 23696, loss 0.530759.
Train: 2018-08-09T11:59:15.261394: step 23697, loss 0.579013.
Train: 2018-08-09T11:59:15.370749: step 23698, loss 0.450533.
Train: 2018-08-09T11:59:15.480101: step 23699, loss 0.594853.
Train: 2018-08-09T11:59:15.589415: step 23700, loss 0.546854.
Test: 2018-08-09T11:59:16.089322: step 23700, loss 0.54882.
Train: 2018-08-09T11:59:16.651694: step 23701, loss 0.514146.
Train: 2018-08-09T11:59:16.761043: step 23702, loss 0.660286.
Train: 2018-08-09T11:59:16.870392: step 23703, loss 0.595105.
Train: 2018-08-09T11:59:16.979745: step 23704, loss 0.611357.
Train: 2018-08-09T11:59:17.089093: step 23705, loss 0.562726.
Train: 2018-08-09T11:59:17.182819: step 23706, loss 0.514157.
Train: 2018-08-09T11:59:17.307760: step 23707, loss 0.643525.
Train: 2018-08-09T11:59:17.401487: step 23708, loss 0.530357.
Train: 2018-08-09T11:59:17.510866: step 23709, loss 0.498331.
Train: 2018-08-09T11:59:17.620215: step 23710, loss 0.64385.
Test: 2018-08-09T11:59:18.120067: step 23710, loss 0.549426.
Train: 2018-08-09T11:59:18.229441: step 23711, loss 0.562805.
Train: 2018-08-09T11:59:18.323175: step 23712, loss 0.465992.
Train: 2018-08-09T11:59:18.432526: step 23713, loss 0.659518.
Train: 2018-08-09T11:59:18.541874: step 23714, loss 0.578799.
Train: 2018-08-09T11:59:18.651222: step 23715, loss 0.643416.
Train: 2018-08-09T11:59:18.760542: step 23716, loss 0.62714.
Train: 2018-08-09T11:59:18.854269: step 23717, loss 0.643064.
Train: 2018-08-09T11:59:18.965979: step 23718, loss 0.498917.
Train: 2018-08-09T11:59:19.075329: step 23719, loss 0.658659.
Train: 2018-08-09T11:59:19.184648: step 23720, loss 0.531166.
Test: 2018-08-09T11:59:19.684530: step 23720, loss 0.552342.
Train: 2018-08-09T11:59:19.778287: step 23721, loss 0.531282.
Train: 2018-08-09T11:59:19.887639: step 23722, loss 0.610521.
Train: 2018-08-09T11:59:19.996989: step 23723, loss 0.594661.
Train: 2018-08-09T11:59:20.121952: step 23724, loss 0.547341.
Train: 2018-08-09T11:59:20.231306: step 23725, loss 0.531661.
Train: 2018-08-09T11:59:20.325003: step 23726, loss 0.547426.
Train: 2018-08-09T11:59:20.434352: step 23727, loss 0.673162.
Train: 2018-08-09T11:59:20.543732: step 23728, loss 0.516142.
Train: 2018-08-09T11:59:20.653052: step 23729, loss 0.578878.
Train: 2018-08-09T11:59:20.762401: step 23730, loss 0.563227.
Test: 2018-08-09T11:59:21.247567: step 23730, loss 0.552133.
Train: 2018-08-09T11:59:21.356950: step 23731, loss 0.516312.
Train: 2018-08-09T11:59:21.466294: step 23732, loss 0.578882.
Train: 2018-08-09T11:59:21.575613: step 23733, loss 0.625827.
Train: 2018-08-09T11:59:21.684993: step 23734, loss 0.641431.
Train: 2018-08-09T11:59:21.778721: step 23735, loss 0.563281.
Train: 2018-08-09T11:59:21.888070: step 23736, loss 0.656818.
Train: 2018-08-09T11:59:21.997419: step 23737, loss 0.547824.
Train: 2018-08-09T11:59:22.106768: step 23738, loss 0.454831.
Train: 2018-08-09T11:59:22.216087: step 23739, loss 0.563396.
Train: 2018-08-09T11:59:22.309846: step 23740, loss 0.563392.
Test: 2018-08-09T11:59:22.809700: step 23740, loss 0.550009.
Train: 2018-08-09T11:59:22.919077: step 23741, loss 0.547861.
Train: 2018-08-09T11:59:23.028426: step 23742, loss 0.532322.
Train: 2018-08-09T11:59:23.122154: step 23743, loss 0.563386.
Train: 2018-08-09T11:59:23.231504: step 23744, loss 0.594465.
Train: 2018-08-09T11:59:23.340853: step 23745, loss 0.547704.
Train: 2018-08-09T11:59:23.450173: step 23746, loss 0.672583.
Train: 2018-08-09T11:59:23.559552: step 23747, loss 0.547672.
Train: 2018-08-09T11:59:23.668901: step 23748, loss 0.65691.
Train: 2018-08-09T11:59:23.778219: step 23749, loss 0.547723.
Train: 2018-08-09T11:59:23.887623: step 23750, loss 0.656749.
Test: 2018-08-09T11:59:24.374416: step 23750, loss 0.549988.
Train: 2018-08-09T11:59:24.483766: step 23751, loss 0.563372.
Train: 2018-08-09T11:59:24.593114: step 23752, loss 0.501363.
Train: 2018-08-09T11:59:24.702493: step 23753, loss 0.454846.
Train: 2018-08-09T11:59:24.796190: step 23754, loss 0.516783.
Train: 2018-08-09T11:59:24.905571: step 23755, loss 0.54764.
Train: 2018-08-09T11:59:25.014919: step 23756, loss 0.641472.
Train: 2018-08-09T11:59:25.124270: step 23757, loss 0.578924.
Train: 2018-08-09T11:59:25.233617: step 23758, loss 0.563313.
Train: 2018-08-09T11:59:25.342961: step 23759, loss 0.657184.
Train: 2018-08-09T11:59:25.452316: step 23760, loss 0.547507.
Test: 2018-08-09T11:59:25.952852: step 23760, loss 0.55029.
Train: 2018-08-09T11:59:26.046602: step 23761, loss 0.531483.
Train: 2018-08-09T11:59:26.155949: step 23762, loss 0.484557.
Train: 2018-08-09T11:59:26.265268: step 23763, loss 0.515659.
Train: 2018-08-09T11:59:26.374644: step 23764, loss 0.594744.
Train: 2018-08-09T11:59:26.483997: step 23765, loss 0.657815.
Train: 2018-08-09T11:59:26.577694: step 23766, loss 0.593968.
Train: 2018-08-09T11:59:26.687045: step 23767, loss 0.563222.
Train: 2018-08-09T11:59:26.796424: step 23768, loss 0.658784.
Train: 2018-08-09T11:59:26.905772: step 23769, loss 0.595001.
Train: 2018-08-09T11:59:27.015123: step 23770, loss 0.56271.
Test: 2018-08-09T11:59:27.499384: step 23770, loss 0.548023.
Train: 2018-08-09T11:59:27.604702: step 23771, loss 0.659858.
Train: 2018-08-09T11:59:27.714022: step 23772, loss 0.56213.
Train: 2018-08-09T11:59:27.823401: step 23773, loss 0.547718.
Train: 2018-08-09T11:59:27.932750: step 23774, loss 0.625716.
Train: 2018-08-09T11:59:28.026478: step 23775, loss 0.641052.
Train: 2018-08-09T11:59:28.135827: step 23776, loss 0.62548.
Train: 2018-08-09T11:59:28.245178: step 23777, loss 0.486454.
Train: 2018-08-09T11:59:28.354525: step 23778, loss 0.594505.
Train: 2018-08-09T11:59:28.448223: step 23779, loss 0.498173.
Train: 2018-08-09T11:59:28.557573: step 23780, loss 0.548353.
Test: 2018-08-09T11:59:29.057455: step 23780, loss 0.549868.
Train: 2018-08-09T11:59:29.166834: step 23781, loss 0.578995.
Train: 2018-08-09T11:59:29.260532: step 23782, loss 0.50236.
Train: 2018-08-09T11:59:29.369880: step 23783, loss 0.57898.
Train: 2018-08-09T11:59:29.479260: step 23784, loss 0.578974.
Train: 2018-08-09T11:59:29.588610: step 23785, loss 0.548239.
Train: 2018-08-09T11:59:29.697959: step 23786, loss 0.563579.
Train: 2018-08-09T11:59:29.791656: step 23787, loss 0.517345.
Train: 2018-08-09T11:59:29.901036: step 23788, loss 0.532637.
Train: 2018-08-09T11:59:30.010354: step 23789, loss 0.625352.
Train: 2018-08-09T11:59:30.119737: step 23790, loss 0.547924.
Test: 2018-08-09T11:59:30.603967: step 23790, loss 0.550014.
Train: 2018-08-09T11:59:30.713314: step 23791, loss 0.594437.
Train: 2018-08-09T11:59:30.822693: step 23792, loss 0.578906.
Train: 2018-08-09T11:59:30.933498: step 23793, loss 0.594462.
Train: 2018-08-09T11:59:31.042848: step 23794, loss 0.532192.
Train: 2018-08-09T11:59:31.136576: step 23795, loss 0.532132.
Train: 2018-08-09T11:59:31.245924: step 23796, loss 0.532045.
Train: 2018-08-09T11:59:31.355244: step 23797, loss 0.641479.
Train: 2018-08-09T11:59:31.464623: step 23798, loss 0.531891.
Train: 2018-08-09T11:59:31.558351: step 23799, loss 0.500451.
Train: 2018-08-09T11:59:31.667690: step 23800, loss 0.57887.
Test: 2018-08-09T11:59:32.167554: step 23800, loss 0.54954.
Train: 2018-08-09T11:59:32.714328: step 23801, loss 0.578866.
Train: 2018-08-09T11:59:32.839301: step 23802, loss 0.547299.
Train: 2018-08-09T11:59:32.950090: step 23803, loss 0.515612.
Train: 2018-08-09T11:59:33.059438: step 23804, loss 0.420331.
Train: 2018-08-09T11:59:33.168758: step 23805, loss 0.674421.
Train: 2018-08-09T11:59:33.262517: step 23806, loss 0.562894.
Train: 2018-08-09T11:59:33.371865: step 23807, loss 0.626875.
Train: 2018-08-09T11:59:33.481215: step 23808, loss 0.466709.
Train: 2018-08-09T11:59:33.574942: step 23809, loss 0.514615.
Train: 2018-08-09T11:59:33.684292: step 23810, loss 0.514423.
Test: 2018-08-09T11:59:34.184143: step 23810, loss 0.548788.
Train: 2018-08-09T11:59:34.293492: step 23811, loss 0.514193.
Train: 2018-08-09T11:59:34.402872: step 23812, loss 0.611384.
Train: 2018-08-09T11:59:34.496600: step 23813, loss 0.546345.
Train: 2018-08-09T11:59:34.605949: step 23814, loss 0.595292.
Train: 2018-08-09T11:59:34.715298: step 23815, loss 0.611706.
Train: 2018-08-09T11:59:34.824648: step 23816, loss 0.52978.
Train: 2018-08-09T11:59:34.920630: step 23817, loss 0.496956.
Train: 2018-08-09T11:59:35.030009: step 23818, loss 0.513216.
Train: 2018-08-09T11:59:35.139359: step 23819, loss 0.578978.
Train: 2018-08-09T11:59:35.248677: step 23820, loss 0.661705.
Test: 2018-08-09T11:59:35.732970: step 23820, loss 0.547625.
Train: 2018-08-09T11:59:35.842318: step 23821, loss 0.545957.
Train: 2018-08-09T11:59:35.951664: step 23822, loss 0.479798.
Train: 2018-08-09T11:59:36.061011: step 23823, loss 0.512784.
Train: 2018-08-09T11:59:36.170361: step 23824, loss 0.462849.
Train: 2018-08-09T11:59:36.264094: step 23825, loss 0.512455.
Train: 2018-08-09T11:59:36.373413: step 23826, loss 0.495504.
Train: 2018-08-09T11:59:36.482792: step 23827, loss 0.495182.
Train: 2018-08-09T11:59:36.576491: step 23828, loss 0.545538.
Train: 2018-08-09T11:59:36.685870: step 23829, loss 0.630149.
Train: 2018-08-09T11:59:36.795213: step 23830, loss 0.528401.
Test: 2018-08-09T11:59:37.295713: step 23830, loss 0.54893.
Train: 2018-08-09T11:59:37.405095: step 23831, loss 0.630499.
Train: 2018-08-09T11:59:37.498789: step 23832, loss 0.66482.
Train: 2018-08-09T11:59:37.608168: step 23833, loss 0.562344.
Train: 2018-08-09T11:59:37.717486: step 23834, loss 0.528246.
Train: 2018-08-09T11:59:37.826837: step 23835, loss 0.579382.
Train: 2018-08-09T11:59:37.920595: step 23836, loss 0.613407.
Train: 2018-08-09T11:59:38.029945: step 23837, loss 0.511323.
Train: 2018-08-09T11:59:38.139262: step 23838, loss 0.613376.
Train: 2018-08-09T11:59:38.248642: step 23839, loss 0.545343.
Train: 2018-08-09T11:59:38.342340: step 23840, loss 0.494373.
Test: 2018-08-09T11:59:38.842247: step 23840, loss 0.549008.
Train: 2018-08-09T11:59:38.951602: step 23841, loss 0.596251.
Train: 2018-08-09T11:59:39.060950: step 23842, loss 0.545391.
Train: 2018-08-09T11:59:39.154681: step 23843, loss 0.511601.
Train: 2018-08-09T11:59:39.264028: step 23844, loss 0.545494.
Train: 2018-08-09T11:59:39.373378: step 23845, loss 0.579267.
Train: 2018-08-09T11:59:39.482695: step 23846, loss 0.562481.
Train: 2018-08-09T11:59:39.576454: step 23847, loss 0.528449.
Train: 2018-08-09T11:59:39.685802: step 23848, loss 0.613204.
Train: 2018-08-09T11:59:39.795153: step 23849, loss 0.54535.
Train: 2018-08-09T11:59:39.905990: step 23850, loss 0.545442.
Test: 2018-08-09T11:59:40.405873: step 23850, loss 0.54907.
Train: 2018-08-09T11:59:40.499631: step 23851, loss 0.663851.
Train: 2018-08-09T11:59:40.608974: step 23852, loss 0.494884.
Train: 2018-08-09T11:59:40.718329: step 23853, loss 0.680381.
Train: 2018-08-09T11:59:40.827647: step 23854, loss 0.696867.
Train: 2018-08-09T11:59:40.921408: step 23855, loss 0.629353.
Train: 2018-08-09T11:59:41.030755: step 23856, loss 0.545782.
Train: 2018-08-09T11:59:41.140104: step 23857, loss 0.529301.
Train: 2018-08-09T11:59:41.249448: step 23858, loss 0.579019.
Train: 2018-08-09T11:59:41.343151: step 23859, loss 0.578992.
Train: 2018-08-09T11:59:41.452500: step 23860, loss 0.661038.
Test: 2018-08-09T11:59:41.955106: step 23860, loss 0.548524.
Train: 2018-08-09T11:59:42.048858: step 23861, loss 0.562599.
Train: 2018-08-09T11:59:42.158207: step 23862, loss 0.676558.
Train: 2018-08-09T11:59:42.267565: step 23863, loss 0.595079.
Train: 2018-08-09T11:59:42.376911: step 23864, loss 0.627174.
Train: 2018-08-09T11:59:42.486256: step 23865, loss 0.546851.
Train: 2018-08-09T11:59:42.579989: step 23866, loss 0.531076.
Train: 2018-08-09T11:59:42.689339: step 23867, loss 0.562992.
Train: 2018-08-09T11:59:42.798658: step 23868, loss 0.563045.
Train: 2018-08-09T11:59:42.892419: step 23869, loss 0.657699.
Train: 2018-08-09T11:59:43.001759: step 23870, loss 0.531769.
Test: 2018-08-09T11:59:43.501616: step 23870, loss 0.549152.
Train: 2018-08-09T11:59:43.610990: step 23871, loss 0.657145.
Train: 2018-08-09T11:59:43.704719: step 23872, loss 0.563315.
Train: 2018-08-09T11:59:43.814073: step 23873, loss 0.64104.
Train: 2018-08-09T11:59:43.924007: step 23874, loss 0.62532.
Train: 2018-08-09T11:59:44.033325: step 23875, loss 0.548181.
Train: 2018-08-09T11:59:44.127055: step 23876, loss 0.487009.
Train: 2018-08-09T11:59:44.236433: step 23877, loss 0.548401.
Train: 2018-08-09T11:59:44.345782: step 23878, loss 0.533172.
Train: 2018-08-09T11:59:44.439510: step 23879, loss 0.56374.
Train: 2018-08-09T11:59:44.548859: step 23880, loss 0.472125.
Test: 2018-08-09T11:59:45.044630: step 23880, loss 0.549953.
Train: 2018-08-09T11:59:45.154010: step 23881, loss 0.594296.
Train: 2018-08-09T11:59:45.247737: step 23882, loss 0.609618.
Train: 2018-08-09T11:59:45.357086: step 23883, loss 0.578988.
Train: 2018-08-09T11:59:45.466405: step 23884, loss 0.624966.
Train: 2018-08-09T11:59:45.560163: step 23885, loss 0.502389.
Train: 2018-08-09T11:59:45.669514: step 23886, loss 0.60965.
Train: 2018-08-09T11:59:45.778863: step 23887, loss 0.578983.
Train: 2018-08-09T11:59:45.888213: step 23888, loss 0.563643.
Train: 2018-08-09T11:59:45.996039: step 23889, loss 0.548294.
Train: 2018-08-09T11:59:46.093959: step 23890, loss 0.563615.
Test: 2018-08-09T11:59:46.593820: step 23890, loss 0.549779.
Train: 2018-08-09T11:59:46.687540: step 23891, loss 0.594336.
Train: 2018-08-09T11:59:46.796889: step 23892, loss 0.548208.
Train: 2018-08-09T11:59:46.906268: step 23893, loss 0.671305.
Train: 2018-08-09T11:59:47.015617: step 23894, loss 0.563584.
Train: 2018-08-09T11:59:47.109314: step 23895, loss 0.532845.
Train: 2018-08-09T11:59:47.218694: step 23896, loss 0.486684.
Train: 2018-08-09T11:59:47.328044: step 23897, loss 0.578982.
Train: 2018-08-09T11:59:47.437395: step 23898, loss 0.455399.
Train: 2018-08-09T11:59:47.531091: step 23899, loss 0.640853.
Train: 2018-08-09T11:59:47.640470: step 23900, loss 0.547695.
Test: 2018-08-09T11:59:48.139014: step 23900, loss 0.550972.
Train: 2018-08-09T11:59:48.754583: step 23901, loss 0.531741.
Train: 2018-08-09T11:59:48.848311: step 23902, loss 0.609522.
Train: 2018-08-09T11:59:48.957660: step 23903, loss 0.562551.
Train: 2018-08-09T11:59:49.067009: step 23904, loss 0.516819.
Train: 2018-08-09T11:59:49.160738: step 23905, loss 0.515128.
Train: 2018-08-09T11:59:49.270087: step 23906, loss 0.63014.
Train: 2018-08-09T11:59:49.379435: step 23907, loss 0.56397.
Train: 2018-08-09T11:59:49.488785: step 23908, loss 0.560622.
Train: 2018-08-09T11:59:49.582512: step 23909, loss 0.594841.
Train: 2018-08-09T11:59:49.691862: step 23910, loss 0.515448.
Test: 2018-08-09T11:59:50.191716: step 23910, loss 0.549106.
Train: 2018-08-09T11:59:50.316685: step 23911, loss 0.579792.
Train: 2018-08-09T11:59:50.426066: step 23912, loss 0.56504.
Train: 2018-08-09T11:59:50.519792: step 23913, loss 0.463681.
Train: 2018-08-09T11:59:50.629141: step 23914, loss 0.513559.
Train: 2018-08-09T11:59:50.738491: step 23915, loss 0.429601.
Train: 2018-08-09T11:59:50.847840: step 23916, loss 0.563771.
Train: 2018-08-09T11:59:50.942978: step 23917, loss 0.507751.
Train: 2018-08-09T11:59:51.052329: step 23918, loss 0.64483.
Train: 2018-08-09T11:59:51.161647: step 23919, loss 0.525314.
Train: 2018-08-09T11:59:51.260983: step 23920, loss 0.597609.
Test: 2018-08-09T11:59:51.760836: step 23920, loss 0.547769.
Train: 2018-08-09T11:59:51.870214: step 23921, loss 0.530897.
Train: 2018-08-09T11:59:51.963942: step 23922, loss 0.561947.
Train: 2018-08-09T11:59:52.073296: step 23923, loss 0.545616.
Train: 2018-08-09T11:59:52.182641: step 23924, loss 0.542507.
Train: 2018-08-09T11:59:52.291990: step 23925, loss 0.493087.
Train: 2018-08-09T11:59:52.385688: step 23926, loss 0.498908.
Train: 2018-08-09T11:59:52.495068: step 23927, loss 0.546499.
Train: 2018-08-09T11:59:52.604414: step 23928, loss 0.651712.
Train: 2018-08-09T11:59:52.713735: step 23929, loss 0.543982.
Train: 2018-08-09T11:59:52.807494: step 23930, loss 0.54227.
Test: 2018-08-09T11:59:53.307347: step 23930, loss 0.549203.
Train: 2018-08-09T11:59:53.416725: step 23931, loss 0.595609.
Train: 2018-08-09T11:59:53.526074: step 23932, loss 0.457492.
Train: 2018-08-09T11:59:53.619802: step 23933, loss 0.490447.
Train: 2018-08-09T11:59:53.729122: step 23934, loss 0.473893.
Train: 2018-08-09T11:59:53.838471: step 23935, loss 0.476601.
Train: 2018-08-09T11:59:53.932198: step 23936, loss 0.577424.
Train: 2018-08-09T11:59:54.041548: step 23937, loss 0.605019.
Train: 2018-08-09T11:59:54.150927: step 23938, loss 0.598858.
Train: 2018-08-09T11:59:54.260277: step 23939, loss 0.526092.
Train: 2018-08-09T11:59:54.354005: step 23940, loss 0.567185.
Test: 2018-08-09T11:59:54.853858: step 23940, loss 0.550498.
Train: 2018-08-09T11:59:54.965473: step 23941, loss 0.476559.
Train: 2018-08-09T11:59:55.074852: step 23942, loss 0.499741.
Train: 2018-08-09T11:59:55.168582: step 23943, loss 0.543747.
Train: 2018-08-09T11:59:55.277929: step 23944, loss 0.472634.
Train: 2018-08-09T11:59:55.387248: step 23945, loss 0.565946.
Train: 2018-08-09T11:59:55.481006: step 23946, loss 0.531014.
Train: 2018-08-09T11:59:55.590355: step 23947, loss 0.592726.
Train: 2018-08-09T11:59:55.699704: step 23948, loss 0.576418.
Train: 2018-08-09T11:59:55.809057: step 23949, loss 0.54456.
Train: 2018-08-09T11:59:55.902782: step 23950, loss 0.557076.
Test: 2018-08-09T11:59:56.402636: step 23950, loss 0.54916.
Train: 2018-08-09T11:59:56.512013: step 23951, loss 0.57624.
Train: 2018-08-09T11:59:56.605741: step 23952, loss 0.657446.
Train: 2018-08-09T11:59:56.715091: step 23953, loss 0.63217.
Train: 2018-08-09T11:59:56.824410: step 23954, loss 0.682692.
Train: 2018-08-09T11:59:56.935275: step 23955, loss 0.476014.
Train: 2018-08-09T11:59:57.029003: step 23956, loss 0.524699.
Train: 2018-08-09T11:59:57.138352: step 23957, loss 0.594066.
Train: 2018-08-09T11:59:57.247702: step 23958, loss 0.66426.
Train: 2018-08-09T11:59:57.341430: step 23959, loss 0.563448.
Train: 2018-08-09T11:59:57.450779: step 23960, loss 0.70887.
Test: 2018-08-09T11:59:57.950632: step 23960, loss 0.547929.
Train: 2018-08-09T11:59:58.044389: step 23961, loss 0.545756.
Train: 2018-08-09T11:59:58.153710: step 23962, loss 0.514057.
Train: 2018-08-09T11:59:58.263058: step 23963, loss 0.577732.
Train: 2018-08-09T11:59:58.372407: step 23964, loss 0.497255.
Train: 2018-08-09T11:59:58.481756: step 23965, loss 0.609651.
Train: 2018-08-09T11:59:58.575514: step 23966, loss 0.625046.
Train: 2018-08-09T11:59:58.684866: step 23967, loss 0.547854.
Train: 2018-08-09T11:59:58.794212: step 23968, loss 0.690054.
Train: 2018-08-09T11:59:58.887941: step 23969, loss 0.579254.
Train: 2018-08-09T11:59:58.999617: step 23970, loss 0.562485.
Test: 2018-08-09T11:59:59.499470: step 23970, loss 0.552176.
Train: 2018-08-09T11:59:59.593226: step 23971, loss 0.562664.
Train: 2018-08-09T11:59:59.702576: step 23972, loss 0.657819.
Train: 2018-08-09T11:59:59.811925: step 23973, loss 0.532187.
Train: 2018-08-09T11:59:59.905625: step 23974, loss 0.547947.
Train: 2018-08-09T12:00:00.015005: step 23975, loss 0.533162.
Train: 2018-08-09T12:00:00.124354: step 23976, loss 0.533197.
Train: 2018-08-09T12:00:00.218076: step 23977, loss 0.549279.
Train: 2018-08-09T12:00:00.327428: step 23978, loss 0.518399.
Train: 2018-08-09T12:00:00.436777: step 23979, loss 0.532641.
Train: 2018-08-09T12:00:00.546127: step 23980, loss 0.517308.
Test: 2018-08-09T12:00:01.025746: step 23980, loss 0.551307.
Train: 2018-08-09T12:00:01.135124: step 23981, loss 0.608038.
Train: 2018-08-09T12:00:01.244473: step 23982, loss 0.640663.
Train: 2018-08-09T12:00:01.353793: step 23983, loss 0.516916.
Train: 2018-08-09T12:00:01.447551: step 23984, loss 0.563053.
Train: 2018-08-09T12:00:01.556900: step 23985, loss 0.564743.
Train: 2018-08-09T12:00:01.666249: step 23986, loss 0.469888.
Train: 2018-08-09T12:00:01.759980: step 23987, loss 0.62471.
Train: 2018-08-09T12:00:01.869297: step 23988, loss 0.561199.
Train: 2018-08-09T12:00:01.978647: step 23989, loss 0.514756.
Train: 2018-08-09T12:00:02.072373: step 23990, loss 0.576561.
Test: 2018-08-09T12:00:02.572257: step 23990, loss 0.546316.
Train: 2018-08-09T12:00:02.681605: step 23991, loss 0.576535.
Train: 2018-08-09T12:00:02.790955: step 23992, loss 0.561657.
Train: 2018-08-09T12:00:02.900303: step 23993, loss 0.478983.
Train: 2018-08-09T12:00:02.994061: step 23994, loss 0.577752.
Train: 2018-08-09T12:00:03.103410: step 23995, loss 0.598439.
Train: 2018-08-09T12:00:03.212731: step 23996, loss 0.54233.
Train: 2018-08-09T12:00:03.322079: step 23997, loss 0.571463.
Train: 2018-08-09T12:00:03.415837: step 23998, loss 0.557906.
Train: 2018-08-09T12:00:03.525189: step 23999, loss 0.61086.
Train: 2018-08-09T12:00:03.634506: step 24000, loss 0.624823.
Test: 2018-08-09T12:00:04.136689: step 24000, loss 0.548074.
Train: 2018-08-09T12:00:04.699086: step 24001, loss 0.596883.
Train: 2018-08-09T12:00:04.808406: step 24002, loss 0.547071.
Train: 2018-08-09T12:00:04.917754: step 24003, loss 0.478004.
Train: 2018-08-09T12:00:05.027121: step 24004, loss 0.582183.
Train: 2018-08-09T12:00:05.120861: step 24005, loss 0.557428.
Train: 2018-08-09T12:00:05.230211: step 24006, loss 0.613589.
Train: 2018-08-09T12:00:05.323938: step 24007, loss 0.480649.
Train: 2018-08-09T12:00:05.433288: step 24008, loss 0.542664.
Train: 2018-08-09T12:00:05.542634: step 24009, loss 0.627582.
Train: 2018-08-09T12:00:05.636365: step 24010, loss 0.595148.
Test: 2018-08-09T12:00:06.138618: step 24010, loss 0.5502.
Train: 2018-08-09T12:00:06.248002: step 24011, loss 0.528795.
Train: 2018-08-09T12:00:06.341695: step 24012, loss 0.547545.
Train: 2018-08-09T12:00:06.451075: step 24013, loss 0.528164.
Train: 2018-08-09T12:00:06.560394: step 24014, loss 0.578042.
Train: 2018-08-09T12:00:06.654152: step 24015, loss 0.634736.
Train: 2018-08-09T12:00:06.763501: step 24016, loss 0.447597.
Train: 2018-08-09T12:00:06.857229: step 24017, loss 0.5135.
Train: 2018-08-09T12:00:06.966548: step 24018, loss 0.625649.
Train: 2018-08-09T12:00:07.060306: step 24019, loss 0.53231.
Train: 2018-08-09T12:00:07.169655: step 24020, loss 0.53822.
Test: 2018-08-09T12:00:07.653896: step 24020, loss 0.550426.
Train: 2018-08-09T12:00:07.763235: step 24021, loss 0.5911.
Train: 2018-08-09T12:00:07.872614: step 24022, loss 0.613652.
Train: 2018-08-09T12:00:07.967741: step 24023, loss 0.606898.
Train: 2018-08-09T12:00:08.077061: step 24024, loss 0.626823.
Train: 2018-08-09T12:00:08.170818: step 24025, loss 0.577125.
Train: 2018-08-09T12:00:08.280163: step 24026, loss 0.614746.
Train: 2018-08-09T12:00:08.373896: step 24027, loss 0.594334.
Train: 2018-08-09T12:00:08.483245: step 24028, loss 0.660081.
Train: 2018-08-09T12:00:08.592594: step 24029, loss 0.533993.
Train: 2018-08-09T12:00:08.686318: step 24030, loss 0.565403.
Test: 2018-08-09T12:00:09.186174: step 24030, loss 0.550655.
Train: 2018-08-09T12:00:09.279902: step 24031, loss 0.517644.
Train: 2018-08-09T12:00:09.389284: step 24032, loss 0.595866.
Train: 2018-08-09T12:00:09.482980: step 24033, loss 0.532991.
Train: 2018-08-09T12:00:09.592329: step 24034, loss 0.577965.
Train: 2018-08-09T12:00:09.686087: step 24035, loss 0.624795.
Train: 2018-08-09T12:00:09.795437: step 24036, loss 0.563287.
Train: 2018-08-09T12:00:09.889164: step 24037, loss 0.563494.
Train: 2018-08-09T12:00:09.999167: step 24038, loss 0.487967.
Train: 2018-08-09T12:00:10.092896: step 24039, loss 0.503067.
Train: 2018-08-09T12:00:10.186593: step 24040, loss 0.53299.
Test: 2018-08-09T12:00:10.686475: step 24040, loss 0.550518.
Train: 2018-08-09T12:00:10.780203: step 24041, loss 0.655819.
Train: 2018-08-09T12:00:10.889582: step 24042, loss 0.533094.
Train: 2018-08-09T12:00:10.983310: step 24043, loss 0.640302.
Train: 2018-08-09T12:00:11.092659: step 24044, loss 0.471743.
Train: 2018-08-09T12:00:11.186388: step 24045, loss 0.625094.
Train: 2018-08-09T12:00:11.280115: step 24046, loss 0.54825.
Train: 2018-08-09T12:00:11.389464: step 24047, loss 0.517456.
Train: 2018-08-09T12:00:11.483193: step 24048, loss 0.594291.
Train: 2018-08-09T12:00:11.576920: step 24049, loss 0.56349.
Train: 2018-08-09T12:00:11.686269: step 24050, loss 0.548186.
Test: 2018-08-09T12:00:12.170502: step 24050, loss 0.549498.
Train: 2018-08-09T12:00:12.279879: step 24051, loss 0.53225.
Train: 2018-08-09T12:00:12.373607: step 24052, loss 0.485786.
Train: 2018-08-09T12:00:12.467335: step 24053, loss 0.578964.
Train: 2018-08-09T12:00:12.576685: step 24054, loss 0.59448.
Train: 2018-08-09T12:00:12.670383: step 24055, loss 0.547605.
Train: 2018-08-09T12:00:12.764140: step 24056, loss 0.578975.
Train: 2018-08-09T12:00:12.873492: step 24057, loss 0.500176.
Train: 2018-08-09T12:00:12.968696: step 24058, loss 0.610299.
Train: 2018-08-09T12:00:13.062455: step 24059, loss 0.515687.
Train: 2018-08-09T12:00:13.171803: step 24060, loss 0.61026.
Test: 2018-08-09T12:00:13.656035: step 24060, loss 0.548558.
Train: 2018-08-09T12:00:13.765384: step 24061, loss 0.546771.
Train: 2018-08-09T12:00:13.859141: step 24062, loss 0.418907.
Train: 2018-08-09T12:00:13.952870: step 24063, loss 0.563895.
Train: 2018-08-09T12:00:14.046597: step 24064, loss 0.724915.
Train: 2018-08-09T12:00:14.155950: step 24065, loss 0.579067.
Train: 2018-08-09T12:00:14.249675: step 24066, loss 0.546602.
Train: 2018-08-09T12:00:14.343402: step 24067, loss 0.62735.
Train: 2018-08-09T12:00:14.452746: step 24068, loss 0.546695.
Train: 2018-08-09T12:00:14.546479: step 24069, loss 0.546893.
Train: 2018-08-09T12:00:14.640208: step 24070, loss 0.498.
Test: 2018-08-09T12:00:15.142397: step 24070, loss 0.549458.
Train: 2018-08-09T12:00:15.236094: step 24071, loss 0.498048.
Train: 2018-08-09T12:00:15.329822: step 24072, loss 0.546482.
Train: 2018-08-09T12:00:15.439204: step 24073, loss 0.562604.
Train: 2018-08-09T12:00:15.532929: step 24074, loss 0.562387.
Train: 2018-08-09T12:00:15.626657: step 24075, loss 0.529643.
Train: 2018-08-09T12:00:15.720385: step 24076, loss 0.563938.
Train: 2018-08-09T12:00:15.814116: step 24077, loss 0.610851.
Train: 2018-08-09T12:00:15.923456: step 24078, loss 0.579537.
Train: 2018-08-09T12:00:16.017161: step 24079, loss 0.613023.
Train: 2018-08-09T12:00:16.110888: step 24080, loss 0.510074.
Test: 2018-08-09T12:00:16.595150: step 24080, loss 0.550151.
Train: 2018-08-09T12:00:16.704498: step 24081, loss 0.627679.
Train: 2018-08-09T12:00:16.798256: step 24082, loss 0.513085.
Train: 2018-08-09T12:00:16.891984: step 24083, loss 0.644711.
Train: 2018-08-09T12:00:16.988132: step 24084, loss 0.513789.
Train: 2018-08-09T12:00:17.081860: step 24085, loss 0.611398.
Train: 2018-08-09T12:00:17.175588: step 24086, loss 0.497218.
Train: 2018-08-09T12:00:17.269286: step 24087, loss 0.709172.
Train: 2018-08-09T12:00:17.363043: step 24088, loss 0.628562.
Train: 2018-08-09T12:00:17.456741: step 24089, loss 0.643328.
Train: 2018-08-09T12:00:17.550499: step 24090, loss 0.610619.
Test: 2018-08-09T12:00:18.050353: step 24090, loss 0.550793.
Train: 2018-08-09T12:00:18.144109: step 24091, loss 0.578843.
Train: 2018-08-09T12:00:18.237808: step 24092, loss 0.626958.
Train: 2018-08-09T12:00:18.331536: step 24093, loss 0.562928.
Train: 2018-08-09T12:00:18.440888: step 24094, loss 0.515633.
Train: 2018-08-09T12:00:18.534642: step 24095, loss 0.499914.
Train: 2018-08-09T12:00:18.628371: step 24096, loss 0.562891.
Train: 2018-08-09T12:00:18.722068: step 24097, loss 0.547354.
Train: 2018-08-09T12:00:18.815798: step 24098, loss 0.531723.
Train: 2018-08-09T12:00:18.910851: step 24099, loss 0.578795.
Train: 2018-08-09T12:00:19.004609: step 24100, loss 0.657498.
Test: 2018-08-09T12:00:19.490539: step 24100, loss 0.551995.
Train: 2018-08-09T12:00:20.052935: step 24101, loss 0.610276.
Train: 2018-08-09T12:00:20.146663: step 24102, loss 0.531851.
Train: 2018-08-09T12:00:20.240395: step 24103, loss 0.500605.
Train: 2018-08-09T12:00:20.334090: step 24104, loss 0.625942.
Train: 2018-08-09T12:00:20.427817: step 24105, loss 0.547494.
Train: 2018-08-09T12:00:20.521545: step 24106, loss 0.578918.
Train: 2018-08-09T12:00:20.615306: step 24107, loss 0.563138.
Train: 2018-08-09T12:00:20.709031: step 24108, loss 0.500581.
Train: 2018-08-09T12:00:20.802759: step 24109, loss 0.547599.
Train: 2018-08-09T12:00:20.896486: step 24110, loss 0.500473.
Test: 2018-08-09T12:00:21.396339: step 24110, loss 0.54903.
Train: 2018-08-09T12:00:21.474476: step 24111, loss 0.500308.
Train: 2018-08-09T12:00:21.570953: step 24112, loss 0.531478.
Train: 2018-08-09T12:00:21.666626: step 24113, loss 0.626451.
Train: 2018-08-09T12:00:21.760352: step 24114, loss 0.547054.
Train: 2018-08-09T12:00:21.854111: step 24115, loss 0.531206.
Train: 2018-08-09T12:00:21.947839: step 24116, loss 0.594704.
Train: 2018-08-09T12:00:22.041567: step 24117, loss 0.59492.
Train: 2018-08-09T12:00:22.135298: step 24118, loss 0.610627.
Train: 2018-08-09T12:00:22.229023: step 24119, loss 0.530794.
Train: 2018-08-09T12:00:22.322751: step 24120, loss 0.466806.
Test: 2018-08-09T12:00:22.822623: step 24120, loss 0.547786.
Train: 2018-08-09T12:00:22.916332: step 24121, loss 0.546793.
Train: 2018-08-09T12:00:23.010082: step 24122, loss 0.514388.
Train: 2018-08-09T12:00:23.088196: step 24123, loss 0.530379.
Train: 2018-08-09T12:00:23.181924: step 24124, loss 0.594901.
Train: 2018-08-09T12:00:23.275651: step 24125, loss 0.546165.
Train: 2018-08-09T12:00:23.369379: step 24126, loss 0.677018.
Train: 2018-08-09T12:00:23.463107: step 24127, loss 0.578655.
Train: 2018-08-09T12:00:23.556836: step 24128, loss 0.480874.
Train: 2018-08-09T12:00:23.650562: step 24129, loss 0.513308.
Train: 2018-08-09T12:00:23.744290: step 24130, loss 0.644243.
Test: 2018-08-09T12:00:24.228523: step 24130, loss 0.54846.
Train: 2018-08-09T12:00:24.322281: step 24131, loss 0.562289.
Train: 2018-08-09T12:00:24.416008: step 24132, loss 0.447774.
Train: 2018-08-09T12:00:24.509738: step 24133, loss 0.529673.
Train: 2018-08-09T12:00:24.603463: step 24134, loss 0.56199.
Train: 2018-08-09T12:00:24.697191: step 24135, loss 0.661932.
Train: 2018-08-09T12:00:24.790919: step 24136, loss 0.446911.
Train: 2018-08-09T12:00:24.868996: step 24137, loss 0.596427.
Train: 2018-08-09T12:00:24.965110: step 24138, loss 0.513104.
Train: 2018-08-09T12:00:25.058838: step 24139, loss 0.529017.
Train: 2018-08-09T12:00:25.152566: step 24140, loss 0.628722.
Test: 2018-08-09T12:00:25.652420: step 24140, loss 0.549917.
Train: 2018-08-09T12:00:25.746147: step 24141, loss 0.578828.
Train: 2018-08-09T12:00:25.824252: step 24142, loss 0.663574.
Train: 2018-08-09T12:00:25.917981: step 24143, loss 0.428748.
Train: 2018-08-09T12:00:26.011739: step 24144, loss 0.612818.
Train: 2018-08-09T12:00:26.105466: step 24145, loss 0.478501.
Train: 2018-08-09T12:00:26.199195: step 24146, loss 0.612116.
Train: 2018-08-09T12:00:26.277270: step 24147, loss 0.562592.
Train: 2018-08-09T12:00:26.371029: step 24148, loss 0.595953.
Train: 2018-08-09T12:00:26.464756: step 24149, loss 0.580345.
Train: 2018-08-09T12:00:26.558485: step 24150, loss 0.530016.
Test: 2018-08-09T12:00:27.060743: step 24150, loss 0.54933.
Train: 2018-08-09T12:00:27.138879: step 24151, loss 0.546423.
Train: 2018-08-09T12:00:27.232608: step 24152, loss 0.579552.
Train: 2018-08-09T12:00:27.326335: step 24153, loss 0.579345.
Train: 2018-08-09T12:00:27.420063: step 24154, loss 0.579573.
Train: 2018-08-09T12:00:27.498170: step 24155, loss 0.545373.
Train: 2018-08-09T12:00:27.591898: step 24156, loss 0.579281.
Train: 2018-08-09T12:00:27.685625: step 24157, loss 0.545608.
Train: 2018-08-09T12:00:27.779354: step 24158, loss 0.463159.
Train: 2018-08-09T12:00:27.857461: step 24159, loss 0.595097.
Train: 2018-08-09T12:00:27.951188: step 24160, loss 0.628551.
Test: 2018-08-09T12:00:28.451040: step 24160, loss 0.547009.
Train: 2018-08-09T12:00:28.544799: step 24161, loss 0.430431.
Train: 2018-08-09T12:00:28.622905: step 24162, loss 0.562373.
Train: 2018-08-09T12:00:28.716633: step 24163, loss 0.413381.
Train: 2018-08-09T12:00:28.810361: step 24164, loss 0.645321.
Train: 2018-08-09T12:00:28.904089: step 24165, loss 0.47915.
Train: 2018-08-09T12:00:28.983540: step 24166, loss 0.545776.
Train: 2018-08-09T12:00:29.077262: step 24167, loss 0.545551.
Train: 2018-08-09T12:00:29.170990: step 24168, loss 0.595922.
Train: 2018-08-09T12:00:29.264688: step 24169, loss 0.562191.
Train: 2018-08-09T12:00:29.342795: step 24170, loss 0.56269.
Test: 2018-08-09T12:00:29.842677: step 24170, loss 0.548484.
Train: 2018-08-09T12:00:29.936434: step 24171, loss 0.596696.
Train: 2018-08-09T12:00:30.014512: step 24172, loss 0.613036.
Train: 2018-08-09T12:00:30.108265: step 24173, loss 0.528129.
Train: 2018-08-09T12:00:30.201996: step 24174, loss 0.663547.
Train: 2018-08-09T12:00:30.280103: step 24175, loss 0.545361.
Train: 2018-08-09T12:00:30.373831: step 24176, loss 0.461685.
Train: 2018-08-09T12:00:30.467559: step 24177, loss 0.495176.
Train: 2018-08-09T12:00:30.545637: step 24178, loss 0.545157.
Train: 2018-08-09T12:00:30.639364: step 24179, loss 0.662729.
Train: 2018-08-09T12:00:30.733122: step 24180, loss 0.595949.
Test: 2018-08-09T12:00:31.218002: step 24180, loss 0.547287.
Train: 2018-08-09T12:00:31.311730: step 24181, loss 0.613006.
Train: 2018-08-09T12:00:31.405487: step 24182, loss 0.494761.
Train: 2018-08-09T12:00:31.483593: step 24183, loss 0.528763.
Train: 2018-08-09T12:00:31.577321: step 24184, loss 0.511557.
Train: 2018-08-09T12:00:31.671049: step 24185, loss 0.54494.
Train: 2018-08-09T12:00:31.764747: step 24186, loss 0.646027.
Train: 2018-08-09T12:00:31.842884: step 24187, loss 0.461766.
Train: 2018-08-09T12:00:31.936611: step 24188, loss 0.495197.
Train: 2018-08-09T12:00:32.014721: step 24189, loss 0.528856.
Train: 2018-08-09T12:00:32.108449: step 24190, loss 0.460149.
Test: 2018-08-09T12:00:32.608332: step 24190, loss 0.549019.
Train: 2018-08-09T12:00:32.702056: step 24191, loss 0.51031.
Train: 2018-08-09T12:00:32.780164: step 24192, loss 0.510169.
Train: 2018-08-09T12:00:32.873891: step 24193, loss 0.613372.
Train: 2018-08-09T12:00:32.967618: step 24194, loss 0.545197.
Train: 2018-08-09T12:00:33.045726: step 24195, loss 0.565581.
Train: 2018-08-09T12:00:33.139455: step 24196, loss 0.688092.
Train: 2018-08-09T12:00:33.233181: step 24197, loss 0.752552.
Train: 2018-08-09T12:00:33.311288: step 24198, loss 0.563654.
Train: 2018-08-09T12:00:33.405019: step 24199, loss 0.614442.
Train: 2018-08-09T12:00:33.498743: step 24200, loss 0.562367.
Test: 2018-08-09T12:00:34.001046: step 24200, loss 0.547804.
Train: 2018-08-09T12:00:34.553385: step 24201, loss 0.512066.
Train: 2018-08-09T12:00:34.631489: step 24202, loss 0.579385.
Train: 2018-08-09T12:00:34.725187: step 24203, loss 0.494829.
Train: 2018-08-09T12:00:34.818915: step 24204, loss 0.628933.
Train: 2018-08-09T12:00:34.897051: step 24205, loss 0.528917.
Train: 2018-08-09T12:00:34.990779: step 24206, loss 0.495643.
Train: 2018-08-09T12:00:35.068856: step 24207, loss 0.529169.
Train: 2018-08-09T12:00:35.162584: step 24208, loss 0.546252.
Train: 2018-08-09T12:00:35.256342: step 24209, loss 0.529186.
Train: 2018-08-09T12:00:35.334449: step 24210, loss 0.579067.
Test: 2018-08-09T12:00:35.834302: step 24210, loss 0.547503.
Train: 2018-08-09T12:00:35.928060: step 24211, loss 0.595686.
Train: 2018-08-09T12:00:36.006135: step 24212, loss 0.462776.
Train: 2018-08-09T12:00:36.099893: step 24213, loss 0.496032.
Train: 2018-08-09T12:00:36.193591: step 24214, loss 0.645856.
Train: 2018-08-09T12:00:36.271727: step 24215, loss 0.546094.
Train: 2018-08-09T12:00:36.365455: step 24216, loss 0.595606.
Train: 2018-08-09T12:00:36.459184: step 24217, loss 0.595974.
Train: 2018-08-09T12:00:36.537290: step 24218, loss 0.512345.
Train: 2018-08-09T12:00:36.631012: step 24219, loss 0.562638.
Train: 2018-08-09T12:00:36.724743: step 24220, loss 0.545743.
Test: 2018-08-09T12:00:37.208978: step 24220, loss 0.54875.
Train: 2018-08-09T12:00:37.302704: step 24221, loss 0.545772.
Train: 2018-08-09T12:00:37.396463: step 24222, loss 0.579002.
Train: 2018-08-09T12:00:37.474569: step 24223, loss 0.59568.
Train: 2018-08-09T12:00:37.568297: step 24224, loss 0.579242.
Train: 2018-08-09T12:00:37.646403: step 24225, loss 0.545999.
Train: 2018-08-09T12:00:37.740132: step 24226, loss 0.628847.
Train: 2018-08-09T12:00:37.833829: step 24227, loss 0.578955.
Train: 2018-08-09T12:00:37.914222: step 24228, loss 0.612041.
Train: 2018-08-09T12:00:38.007982: step 24229, loss 0.496546.
Train: 2018-08-09T12:00:38.086087: step 24230, loss 0.546018.
Test: 2018-08-09T12:00:38.585941: step 24230, loss 0.549573.
Train: 2018-08-09T12:00:38.664076: step 24231, loss 0.578923.
Train: 2018-08-09T12:00:38.757804: step 24232, loss 0.595453.
Train: 2018-08-09T12:00:38.835911: step 24233, loss 0.546098.
Train: 2018-08-09T12:00:38.929640: step 24234, loss 0.661072.
Train: 2018-08-09T12:00:39.023367: step 24235, loss 0.660868.
Train: 2018-08-09T12:00:39.101476: step 24236, loss 0.464683.
Train: 2018-08-09T12:00:39.195200: step 24237, loss 0.513799.
Train: 2018-08-09T12:00:39.273302: step 24238, loss 0.660298.
Train: 2018-08-09T12:00:39.367035: step 24239, loss 0.578888.
Train: 2018-08-09T12:00:39.460734: step 24240, loss 0.61132.
Test: 2018-08-09T12:00:39.946400: step 24240, loss 0.549394.
Train: 2018-08-09T12:00:40.040156: step 24241, loss 0.562674.
Train: 2018-08-09T12:00:40.118264: step 24242, loss 0.595039.
Train: 2018-08-09T12:00:40.211994: step 24243, loss 0.498437.
Train: 2018-08-09T12:00:40.305689: step 24244, loss 0.498497.
Train: 2018-08-09T12:00:40.383796: step 24245, loss 0.59497.
Train: 2018-08-09T12:00:40.477555: step 24246, loss 0.562814.
Train: 2018-08-09T12:00:40.555659: step 24247, loss 0.643149.
Train: 2018-08-09T12:00:40.649358: step 24248, loss 0.626957.
Train: 2018-08-09T12:00:40.727465: step 24249, loss 0.626851.
Train: 2018-08-09T12:00:40.821222: step 24250, loss 0.562922.
Test: 2018-08-09T12:00:41.321093: step 24250, loss 0.551052.
Train: 2018-08-09T12:00:41.399181: step 24251, loss 0.547029.
Train: 2018-08-09T12:00:41.492941: step 24252, loss 0.515352.
Train: 2018-08-09T12:00:41.571016: step 24253, loss 0.483693.
Train: 2018-08-09T12:00:41.664773: step 24254, loss 0.56299.
Train: 2018-08-09T12:00:41.742882: step 24255, loss 0.610603.
Train: 2018-08-09T12:00:41.836578: step 24256, loss 0.610594.
Train: 2018-08-09T12:00:41.916106: step 24257, loss 0.626416.
Train: 2018-08-09T12:00:42.009865: step 24258, loss 0.499708.
Train: 2018-08-09T12:00:42.103592: step 24259, loss 0.594705.
Train: 2018-08-09T12:00:42.181700: step 24260, loss 0.594676.
Test: 2018-08-09T12:00:42.681551: step 24260, loss 0.548862.
Train: 2018-08-09T12:00:42.759687: step 24261, loss 0.547278.
Train: 2018-08-09T12:00:42.853416: step 24262, loss 0.54728.
Train: 2018-08-09T12:00:42.947148: step 24263, loss 0.468381.
Train: 2018-08-09T12:00:43.025220: step 24264, loss 0.515634.
Train: 2018-08-09T12:00:43.118948: step 24265, loss 0.53135.
Train: 2018-08-09T12:00:43.197055: step 24266, loss 0.547095.
Train: 2018-08-09T12:00:43.290815: step 24267, loss 0.578861.
Train: 2018-08-09T12:00:43.368919: step 24268, loss 0.498935.
Train: 2018-08-09T12:00:43.462650: step 24269, loss 0.674997.
Train: 2018-08-09T12:00:43.556375: step 24270, loss 0.626911.
Test: 2018-08-09T12:00:44.042951: step 24270, loss 0.548997.
Train: 2018-08-09T12:00:44.136708: step 24271, loss 0.530383.
Train: 2018-08-09T12:00:44.230436: step 24272, loss 0.578839.
Train: 2018-08-09T12:00:44.308542: step 24273, loss 0.530851.
Train: 2018-08-09T12:00:44.402240: step 24274, loss 0.610125.
Train: 2018-08-09T12:00:44.480376: step 24275, loss 0.530846.
Train: 2018-08-09T12:00:44.574102: step 24276, loss 0.561557.
Train: 2018-08-09T12:00:44.667833: step 24277, loss 0.496947.
Train: 2018-08-09T12:00:44.745940: step 24278, loss 0.51194.
Train: 2018-08-09T12:00:44.839671: step 24279, loss 0.59706.
Train: 2018-08-09T12:00:44.917773: step 24280, loss 0.631315.
Test: 2018-08-09T12:00:45.417651: step 24280, loss 0.548486.
Train: 2018-08-09T12:00:45.511383: step 24281, loss 0.6791.
Train: 2018-08-09T12:00:45.589461: step 24282, loss 0.497611.
Train: 2018-08-09T12:00:45.683218: step 24283, loss 0.545925.
Train: 2018-08-09T12:00:45.761325: step 24284, loss 0.479481.
Train: 2018-08-09T12:00:45.855053: step 24285, loss 0.579152.
Train: 2018-08-09T12:00:45.934523: step 24286, loss 0.61103.
Train: 2018-08-09T12:00:46.028251: step 24287, loss 0.530038.
Train: 2018-08-09T12:00:46.106358: step 24288, loss 0.499403.
Train: 2018-08-09T12:00:46.200086: step 24289, loss 0.561439.
Train: 2018-08-09T12:00:46.293794: step 24290, loss 0.643219.
Test: 2018-08-09T12:00:46.793666: step 24290, loss 0.547758.
Train: 2018-08-09T12:00:46.871773: step 24291, loss 0.495969.
Train: 2018-08-09T12:00:46.965500: step 24292, loss 0.563517.
Train: 2018-08-09T12:00:47.043608: step 24293, loss 0.580649.
Train: 2018-08-09T12:00:47.137365: step 24294, loss 0.512453.
Train: 2018-08-09T12:00:47.215474: step 24295, loss 0.693215.
Train: 2018-08-09T12:00:47.309199: step 24296, loss 0.612309.
Train: 2018-08-09T12:00:47.402896: step 24297, loss 0.546124.
Train: 2018-08-09T12:00:47.481005: step 24298, loss 0.562544.
Train: 2018-08-09T12:00:47.574762: step 24299, loss 0.528612.
Train: 2018-08-09T12:00:47.668483: step 24300, loss 0.529691.
Test: 2018-08-09T12:00:48.155015: step 24300, loss 0.549895.
Train: 2018-08-09T12:00:48.717414: step 24301, loss 0.514735.
Train: 2018-08-09T12:00:48.795520: step 24302, loss 0.514212.
Train: 2018-08-09T12:00:48.889246: step 24303, loss 0.546043.
Train: 2018-08-09T12:00:48.982943: step 24304, loss 0.61011.
Train: 2018-08-09T12:00:49.061075: step 24305, loss 0.646869.
Train: 2018-08-09T12:00:49.154808: step 24306, loss 0.577613.
Train: 2018-08-09T12:00:49.232914: step 24307, loss 0.431221.
Train: 2018-08-09T12:00:49.326642: step 24308, loss 0.692974.
Train: 2018-08-09T12:00:49.420341: step 24309, loss 0.613421.
Train: 2018-08-09T12:00:49.498491: step 24310, loss 0.464734.
Test: 2018-08-09T12:00:49.999038: step 24310, loss 0.546679.
Train: 2018-08-09T12:00:50.077144: step 24311, loss 0.464317.
Train: 2018-08-09T12:00:50.170901: step 24312, loss 0.563861.
Train: 2018-08-09T12:00:50.248979: step 24313, loss 0.512651.
Train: 2018-08-09T12:00:50.342735: step 24314, loss 0.578989.
Train: 2018-08-09T12:00:50.436464: step 24315, loss 0.61381.
Train: 2018-08-09T12:00:50.514570: step 24316, loss 0.513814.
Train: 2018-08-09T12:00:50.608298: step 24317, loss 0.579663.
Train: 2018-08-09T12:00:50.686375: step 24318, loss 0.495218.
Train: 2018-08-09T12:00:50.780132: step 24319, loss 0.57873.
Train: 2018-08-09T12:00:50.873831: step 24320, loss 0.563886.
Test: 2018-08-09T12:00:51.373738: step 24320, loss 0.548893.
Train: 2018-08-09T12:00:51.451820: step 24321, loss 0.54604.
Train: 2018-08-09T12:00:51.545548: step 24322, loss 0.545577.
Train: 2018-08-09T12:00:51.639306: step 24323, loss 0.56222.
Train: 2018-08-09T12:00:51.717412: step 24324, loss 0.564232.
Train: 2018-08-09T12:00:51.811109: step 24325, loss 0.597897.
Train: 2018-08-09T12:00:51.894804: step 24326, loss 0.662311.
Train: 2018-08-09T12:00:51.988531: step 24327, loss 0.513523.
Train: 2018-08-09T12:00:52.082260: step 24328, loss 0.546302.
Train: 2018-08-09T12:00:52.160336: step 24329, loss 0.513732.
Train: 2018-08-09T12:00:52.254094: step 24330, loss 0.645299.
Test: 2018-08-09T12:00:52.758728: step 24330, loss 0.552828.
Train: 2018-08-09T12:00:52.836863: step 24331, loss 0.611553.
Train: 2018-08-09T12:00:52.930586: step 24332, loss 0.59515.
Train: 2018-08-09T12:00:53.024322: step 24333, loss 0.578978.
Train: 2018-08-09T12:00:53.102396: step 24334, loss 0.497791.
Train: 2018-08-09T12:00:53.196124: step 24335, loss 0.676062.
Train: 2018-08-09T12:00:53.289852: step 24336, loss 0.562757.
Train: 2018-08-09T12:00:53.383610: step 24337, loss 0.578898.
Train: 2018-08-09T12:00:53.461716: step 24338, loss 0.611044.
Train: 2018-08-09T12:00:53.555444: step 24339, loss 0.466529.
Train: 2018-08-09T12:00:53.649143: step 24340, loss 0.578866.
Test: 2018-08-09T12:00:54.149026: step 24340, loss 0.550249.
Train: 2018-08-09T12:00:54.242782: step 24341, loss 0.578863.
Train: 2018-08-09T12:00:54.336510: step 24342, loss 0.562853.
Train: 2018-08-09T12:00:54.424147: step 24343, loss 0.610863.
Train: 2018-08-09T12:00:54.517875: step 24344, loss 0.482985.
Train: 2018-08-09T12:00:54.611633: step 24345, loss 0.5469.
Train: 2018-08-09T12:00:54.689739: step 24346, loss 0.562878.
Train: 2018-08-09T12:00:54.783437: step 24347, loss 0.498908.
Train: 2018-08-09T12:00:54.877197: step 24348, loss 0.546839.
Train: 2018-08-09T12:00:54.967356: step 24349, loss 0.530764.
Train: 2018-08-09T12:00:55.061082: step 24350, loss 0.514617.
Test: 2018-08-09T12:00:55.560936: step 24350, loss 0.551951.
Train: 2018-08-09T12:00:55.639042: step 24351, loss 0.546672.
Train: 2018-08-09T12:00:55.732802: step 24352, loss 0.530469.
Train: 2018-08-09T12:00:55.826530: step 24353, loss 0.530349.
Train: 2018-08-09T12:00:55.921687: step 24354, loss 0.660043.
Train: 2018-08-09T12:00:56.015415: step 24355, loss 0.57891.
Train: 2018-08-09T12:00:56.093521: step 24356, loss 0.611433.
Train: 2018-08-09T12:00:56.187220: step 24357, loss 0.513873.
Train: 2018-08-09T12:00:56.280980: step 24358, loss 0.595187.
Train: 2018-08-09T12:00:56.374705: step 24359, loss 0.562637.
Train: 2018-08-09T12:00:56.468433: step 24360, loss 0.546363.
Test: 2018-08-09T12:00:56.952665: step 24360, loss 0.552296.
Train: 2018-08-09T12:00:57.046392: step 24361, loss 0.497469.
Train: 2018-08-09T12:00:57.140150: step 24362, loss 0.611555.
Train: 2018-08-09T12:00:57.233848: step 24363, loss 0.546281.
Train: 2018-08-09T12:00:57.327606: step 24364, loss 0.546293.
Train: 2018-08-09T12:00:57.421334: step 24365, loss 0.546252.
Train: 2018-08-09T12:00:57.515032: step 24366, loss 0.480799.
Train: 2018-08-09T12:00:57.593138: step 24367, loss 0.57898.
Train: 2018-08-09T12:00:57.686896: step 24368, loss 0.595377.
Train: 2018-08-09T12:00:57.780628: step 24369, loss 0.611855.
Train: 2018-08-09T12:00:57.874352: step 24370, loss 0.562549.
Test: 2018-08-09T12:00:58.376531: step 24370, loss 0.548387.
Train: 2018-08-09T12:00:58.470271: step 24371, loss 0.578972.
Train: 2018-08-09T12:00:58.563998: step 24372, loss 0.546114.
Train: 2018-08-09T12:00:58.657726: step 24373, loss 0.611835.
Train: 2018-08-09T12:00:58.751424: step 24374, loss 0.496866.
Train: 2018-08-09T12:00:58.845181: step 24375, loss 0.595398.
Train: 2018-08-09T12:00:58.938909: step 24376, loss 0.529701.
Train: 2018-08-09T12:00:59.032637: step 24377, loss 0.562548.
Train: 2018-08-09T12:00:59.126365: step 24378, loss 0.661123.
Train: 2018-08-09T12:00:59.220064: step 24379, loss 0.562556.
Train: 2018-08-09T12:00:59.313821: step 24380, loss 0.496997.
Test: 2018-08-09T12:00:59.813674: step 24380, loss 0.550929.
Train: 2018-08-09T12:00:59.891780: step 24381, loss 0.510118.
Train: 2018-08-09T12:00:59.987831: step 24382, loss 0.529761.
Train: 2018-08-09T12:01:00.081589: step 24383, loss 0.611799.
Train: 2018-08-09T12:01:00.175317: step 24384, loss 0.529712.
Train: 2018-08-09T12:01:00.269045: step 24385, loss 0.513257.
Train: 2018-08-09T12:01:00.362742: step 24386, loss 0.529639.
Train: 2018-08-09T12:01:00.456500: step 24387, loss 0.529575.
Train: 2018-08-09T12:01:00.550199: step 24388, loss 0.513.
Train: 2018-08-09T12:01:00.643956: step 24389, loss 0.678255.
Train: 2018-08-09T12:01:00.737684: step 24390, loss 0.562474.
Test: 2018-08-09T12:01:01.237538: step 24390, loss 0.545722.
Train: 2018-08-09T12:01:01.331296: step 24391, loss 0.512824.
Train: 2018-08-09T12:01:01.425022: step 24392, loss 0.579026.
Train: 2018-08-09T12:01:01.518721: step 24393, loss 0.579064.
Train: 2018-08-09T12:01:01.612479: step 24394, loss 0.545903.
Train: 2018-08-09T12:01:01.706205: step 24395, loss 0.612235.
Train: 2018-08-09T12:01:01.799934: step 24396, loss 0.545903.
Train: 2018-08-09T12:01:01.893631: step 24397, loss 0.529326.
Train: 2018-08-09T12:01:01.989861: step 24398, loss 0.579046.
Train: 2018-08-09T12:01:02.083588: step 24399, loss 0.512741.
Train: 2018-08-09T12:01:02.177285: step 24400, loss 0.562466.
Test: 2018-08-09T12:01:02.677169: step 24400, loss 0.547542.
Train: 2018-08-09T12:01:03.223921: step 24401, loss 0.57906.
Train: 2018-08-09T12:01:03.333293: step 24402, loss 0.595643.
Train: 2018-08-09T12:01:03.426991: step 24403, loss 0.579067.
Train: 2018-08-09T12:01:03.520749: step 24404, loss 0.661947.
Train: 2018-08-09T12:01:03.614478: step 24405, loss 0.51284.
Train: 2018-08-09T12:01:03.708205: step 24406, loss 0.512904.
Train: 2018-08-09T12:01:03.801902: step 24407, loss 0.645109.
Train: 2018-08-09T12:01:03.911568: step 24408, loss 0.644992.
Train: 2018-08-09T12:01:04.001688: step 24409, loss 0.546075.
Train: 2018-08-09T12:01:04.095445: step 24410, loss 0.431177.
Test: 2018-08-09T12:01:04.595297: step 24410, loss 0.549028.
Train: 2018-08-09T12:01:04.689051: step 24411, loss 0.513288.
Train: 2018-08-09T12:01:04.782783: step 24412, loss 0.611833.
Train: 2018-08-09T12:01:04.876484: step 24413, loss 0.562543.
Train: 2018-08-09T12:01:04.985861: step 24414, loss 0.562549.
Train: 2018-08-09T12:01:05.079559: step 24415, loss 0.546128.
Train: 2018-08-09T12:01:05.173317: step 24416, loss 0.513286.
Train: 2018-08-09T12:01:05.267015: step 24417, loss 0.595407.
Train: 2018-08-09T12:01:05.360772: step 24418, loss 0.496803.
Train: 2018-08-09T12:01:05.454500: step 24419, loss 0.496724.
Train: 2018-08-09T12:01:05.548199: step 24420, loss 0.546041.
Test: 2018-08-09T12:01:06.048082: step 24420, loss 0.552651.
Train: 2018-08-09T12:01:06.141808: step 24421, loss 0.595522.
Train: 2018-08-09T12:01:06.251190: step 24422, loss 0.529435.
Train: 2018-08-09T12:01:06.344915: step 24423, loss 0.496271.
Train: 2018-08-09T12:01:06.438643: step 24424, loss 0.595637.
Train: 2018-08-09T12:01:06.532371: step 24425, loss 0.562445.
Train: 2018-08-09T12:01:06.626102: step 24426, loss 0.562437.
Train: 2018-08-09T12:01:06.735448: step 24427, loss 0.579128.
Train: 2018-08-09T12:01:06.829176: step 24428, loss 0.579109.
Train: 2018-08-09T12:01:06.925290: step 24429, loss 0.445786.
Train: 2018-08-09T12:01:07.019019: step 24430, loss 0.612483.
Test: 2018-08-09T12:01:07.518871: step 24430, loss 0.548658.
Train: 2018-08-09T12:01:07.628250: step 24431, loss 0.545711.
Train: 2018-08-09T12:01:07.726308: step 24432, loss 0.595854.
Train: 2018-08-09T12:01:07.811865: step 24433, loss 0.612595.
Train: 2018-08-09T12:01:07.921214: step 24434, loss 0.629288.
Train: 2018-08-09T12:01:08.014938: step 24435, loss 0.629189.
Train: 2018-08-09T12:01:08.108641: step 24436, loss 0.462493.
Train: 2018-08-09T12:01:08.202399: step 24437, loss 0.495848.
Train: 2018-08-09T12:01:08.311742: step 24438, loss 0.66234.
Train: 2018-08-09T12:01:08.405481: step 24439, loss 0.579083.
Train: 2018-08-09T12:01:08.499205: step 24440, loss 0.562461.
Test: 2018-08-09T12:01:09.004285: step 24440, loss 0.548183.
Train: 2018-08-09T12:01:09.098042: step 24441, loss 0.628812.
Train: 2018-08-09T12:01:09.191769: step 24442, loss 0.595579.
Train: 2018-08-09T12:01:09.301090: step 24443, loss 0.529489.
Train: 2018-08-09T12:01:09.394846: step 24444, loss 0.578999.
Train: 2018-08-09T12:01:09.488577: step 24445, loss 0.595429.
Train: 2018-08-09T12:01:09.597895: step 24446, loss 0.562554.
Train: 2018-08-09T12:01:09.691657: step 24447, loss 0.578954.
Train: 2018-08-09T12:01:09.785380: step 24448, loss 0.562593.
Train: 2018-08-09T12:01:09.879108: step 24449, loss 0.578932.
Train: 2018-08-09T12:01:09.988457: step 24450, loss 0.497464.
Test: 2018-08-09T12:01:10.488310: step 24450, loss 0.548001.
Train: 2018-08-09T12:01:10.582067: step 24451, loss 0.513781.
Train: 2018-08-09T12:01:10.675798: step 24452, loss 0.49744.
Train: 2018-08-09T12:01:10.785115: step 24453, loss 0.529762.
Train: 2018-08-09T12:01:10.878842: step 24454, loss 0.447809.
Train: 2018-08-09T12:01:10.972601: step 24455, loss 0.678534.
Train: 2018-08-09T12:01:11.081950: step 24456, loss 0.544637.
Train: 2018-08-09T12:01:11.175677: step 24457, loss 0.545137.
Train: 2018-08-09T12:01:11.285026: step 24458, loss 0.560787.
Train: 2018-08-09T12:01:11.378725: step 24459, loss 0.611208.
Train: 2018-08-09T12:01:11.472482: step 24460, loss 0.545772.
Test: 2018-08-09T12:01:11.974784: step 24460, loss 0.547519.
Train: 2018-08-09T12:01:12.068491: step 24461, loss 0.683773.
Train: 2018-08-09T12:01:12.177852: step 24462, loss 0.525046.
Train: 2018-08-09T12:01:12.271580: step 24463, loss 0.564033.
Train: 2018-08-09T12:01:12.365308: step 24464, loss 0.50884.
Train: 2018-08-09T12:01:12.474656: step 24465, loss 0.564086.
Train: 2018-08-09T12:01:12.568385: step 24466, loss 0.578887.
Train: 2018-08-09T12:01:12.677734: step 24467, loss 0.528054.
Train: 2018-08-09T12:01:12.771462: step 24468, loss 0.625888.
Train: 2018-08-09T12:01:12.865160: step 24469, loss 0.529635.
Train: 2018-08-09T12:01:12.974538: step 24470, loss 0.626826.
Test: 2018-08-09T12:01:13.474422: step 24470, loss 0.5464.
Train: 2018-08-09T12:01:13.568152: step 24471, loss 0.542412.
Train: 2018-08-09T12:01:13.661877: step 24472, loss 0.594178.
Train: 2018-08-09T12:01:13.771196: step 24473, loss 0.511022.
Train: 2018-08-09T12:01:13.864953: step 24474, loss 0.478102.
Train: 2018-08-09T12:01:13.960076: step 24475, loss 0.595605.
Train: 2018-08-09T12:01:14.069427: step 24476, loss 0.496002.
Train: 2018-08-09T12:01:14.163156: step 24477, loss 0.704527.
Train: 2018-08-09T12:01:14.272473: step 24478, loss 0.564463.
Train: 2018-08-09T12:01:14.366231: step 24479, loss 0.544661.
Train: 2018-08-09T12:01:14.475580: step 24480, loss 0.718384.
Test: 2018-08-09T12:01:14.959812: step 24480, loss 0.548646.
Train: 2018-08-09T12:01:15.069190: step 24481, loss 0.601402.
Train: 2018-08-09T12:01:15.162888: step 24482, loss 0.612228.
Train: 2018-08-09T12:01:15.272237: step 24483, loss 0.513622.
Train: 2018-08-09T12:01:15.365996: step 24484, loss 0.578924.
Train: 2018-08-09T12:01:15.475344: step 24485, loss 0.642729.
Train: 2018-08-09T12:01:15.569073: step 24486, loss 0.531933.
Train: 2018-08-09T12:01:15.678424: step 24487, loss 0.547506.
Train: 2018-08-09T12:01:15.772149: step 24488, loss 0.531954.
Train: 2018-08-09T12:01:15.881500: step 24489, loss 0.547621.
Train: 2018-08-09T12:01:15.976583: step 24490, loss 0.53204.
Test: 2018-08-09T12:01:16.476467: step 24490, loss 0.548677.
Train: 2018-08-09T12:01:16.570194: step 24491, loss 0.594496.
Train: 2018-08-09T12:01:16.679544: step 24492, loss 0.594484.
Train: 2018-08-09T12:01:16.773301: step 24493, loss 0.625631.
Train: 2018-08-09T12:01:16.882650: step 24494, loss 0.610013.
Train: 2018-08-09T12:01:16.976378: step 24495, loss 0.594437.
Train: 2018-08-09T12:01:17.085727: step 24496, loss 0.578921.
Train: 2018-08-09T12:01:17.195047: step 24497, loss 0.486143.
Train: 2018-08-09T12:01:17.288805: step 24498, loss 0.47072.
Train: 2018-08-09T12:01:17.398154: step 24499, loss 0.594405.
Train: 2018-08-09T12:01:17.507472: step 24500, loss 0.687361.
Test: 2018-08-09T12:01:17.994048: step 24500, loss 0.550706.
Train: 2018-08-09T12:01:18.540824: step 24501, loss 0.532499.
Train: 2018-08-09T12:01:18.650174: step 24502, loss 0.578928.
Train: 2018-08-09T12:01:18.759511: step 24503, loss 0.501591.
Train: 2018-08-09T12:01:18.853250: step 24504, loss 0.532483.
Train: 2018-08-09T12:01:18.962602: step 24505, loss 0.594421.
Train: 2018-08-09T12:01:19.056327: step 24506, loss 0.54788.
Train: 2018-08-09T12:01:19.165676: step 24507, loss 0.563372.
Train: 2018-08-09T12:01:19.259405: step 24508, loss 0.687795.
Train: 2018-08-09T12:01:19.368723: step 24509, loss 0.485625.
Train: 2018-08-09T12:01:19.462485: step 24510, loss 0.532225.
Test: 2018-08-09T12:01:19.963689: step 24510, loss 0.553433.
Train: 2018-08-09T12:01:20.073037: step 24511, loss 0.516578.
Train: 2018-08-09T12:01:20.166795: step 24512, loss 0.500831.
Train: 2018-08-09T12:01:20.276115: step 24513, loss 0.594536.
Train: 2018-08-09T12:01:20.385494: step 24514, loss 0.484718.
Train: 2018-08-09T12:01:20.479224: step 24515, loss 0.547374.
Train: 2018-08-09T12:01:20.588571: step 24516, loss 0.642056.
Train: 2018-08-09T12:01:20.682299: step 24517, loss 0.578864.
Train: 2018-08-09T12:01:20.791619: step 24518, loss 0.547141.
Train: 2018-08-09T12:01:20.900997: step 24519, loss 0.594747.
Train: 2018-08-09T12:01:20.994729: step 24520, loss 0.499335.
Test: 2018-08-09T12:01:21.494578: step 24520, loss 0.549187.
Train: 2018-08-09T12:01:21.603927: step 24521, loss 0.594838.
Train: 2018-08-09T12:01:21.713276: step 24522, loss 0.546952.
Train: 2018-08-09T12:01:21.807034: step 24523, loss 0.514895.
Train: 2018-08-09T12:01:21.917818: step 24524, loss 0.498759.
Train: 2018-08-09T12:01:22.011576: step 24525, loss 0.610957.
Train: 2018-08-09T12:01:22.120895: step 24526, loss 0.546593.
Train: 2018-08-09T12:01:22.217510: step 24527, loss 0.530296.
Train: 2018-08-09T12:01:22.326861: step 24528, loss 0.562704.
Train: 2018-08-09T12:01:22.436240: step 24529, loss 0.595222.
Train: 2018-08-09T12:01:22.545589: step 24530, loss 0.481088.
Test: 2018-08-09T12:01:23.029821: step 24530, loss 0.550279.
Train: 2018-08-09T12:01:23.139199: step 24531, loss 0.545989.
Train: 2018-08-09T12:01:23.248550: step 24532, loss 0.612176.
Train: 2018-08-09T12:01:23.342247: step 24533, loss 0.628989.
Train: 2018-08-09T12:01:23.451625: step 24534, loss 0.545649.
Train: 2018-08-09T12:01:23.560945: step 24535, loss 0.512934.
Train: 2018-08-09T12:01:23.654702: step 24536, loss 0.562818.
Train: 2018-08-09T12:01:23.764022: step 24537, loss 0.544888.
Train: 2018-08-09T12:01:23.873403: step 24538, loss 0.613728.
Train: 2018-08-09T12:01:23.980344: step 24539, loss 0.496676.
Train: 2018-08-09T12:01:24.074074: step 24540, loss 0.644945.
Test: 2018-08-09T12:01:24.573925: step 24540, loss 0.550075.
Train: 2018-08-09T12:01:24.667685: step 24541, loss 0.561981.
Train: 2018-08-09T12:01:24.777031: step 24542, loss 0.562497.
Train: 2018-08-09T12:01:24.886380: step 24543, loss 0.662317.
Train: 2018-08-09T12:01:24.980112: step 24544, loss 0.645501.
Train: 2018-08-09T12:01:25.089457: step 24545, loss 0.529765.
Train: 2018-08-09T12:01:25.198777: step 24546, loss 0.546281.
Train: 2018-08-09T12:01:25.292538: step 24547, loss 0.530084.
Train: 2018-08-09T12:01:25.401855: step 24548, loss 0.546397.
Train: 2018-08-09T12:01:25.511203: step 24549, loss 0.546424.
Train: 2018-08-09T12:01:25.620582: step 24550, loss 0.627615.
Test: 2018-08-09T12:01:26.117466: step 24550, loss 0.552394.
Train: 2018-08-09T12:01:26.211222: step 24551, loss 0.562684.
Train: 2018-08-09T12:01:26.320571: step 24552, loss 0.49789.
Train: 2018-08-09T12:01:26.429921: step 24553, loss 0.481686.
Train: 2018-08-09T12:01:26.523649: step 24554, loss 0.660003.
Train: 2018-08-09T12:01:26.632999: step 24555, loss 0.562684.
Train: 2018-08-09T12:01:26.742350: step 24556, loss 0.530268.
Train: 2018-08-09T12:01:26.851697: step 24557, loss 0.578901.
Train: 2018-08-09T12:01:26.949737: step 24558, loss 0.530259.
Train: 2018-08-09T12:01:27.059087: step 24559, loss 0.562686.
Train: 2018-08-09T12:01:27.168436: step 24560, loss 0.6276.
Test: 2018-08-09T12:01:27.668323: step 24560, loss 0.549932.
Train: 2018-08-09T12:01:27.762077: step 24561, loss 0.578909.
Train: 2018-08-09T12:01:27.871426: step 24562, loss 0.627537.
Train: 2018-08-09T12:01:27.980775: step 24563, loss 0.530333.
Train: 2018-08-09T12:01:28.074503: step 24564, loss 0.530362.
Train: 2018-08-09T12:01:28.183853: step 24565, loss 0.449526.
Train: 2018-08-09T12:01:28.298321: step 24566, loss 0.611283.
Train: 2018-08-09T12:01:28.389549: step 24567, loss 0.595097.
Train: 2018-08-09T12:01:28.498899: step 24568, loss 0.546493.
Train: 2018-08-09T12:01:28.608247: step 24569, loss 0.595121.
Train: 2018-08-09T12:01:28.717596: step 24570, loss 0.611318.
Test: 2018-08-09T12:01:29.201829: step 24570, loss 0.548756.
Train: 2018-08-09T12:01:29.311177: step 24571, loss 0.595082.
Train: 2018-08-09T12:01:29.420556: step 24572, loss 0.578883.
Train: 2018-08-09T12:01:29.529906: step 24573, loss 0.595042.
Train: 2018-08-09T12:01:29.639224: step 24574, loss 0.627261.
Train: 2018-08-09T12:01:29.732982: step 24575, loss 0.54669.
Train: 2018-08-09T12:01:29.842332: step 24576, loss 0.514626.
Train: 2018-08-09T12:01:29.950386: step 24577, loss 0.49863.
Train: 2018-08-09T12:01:30.044085: step 24578, loss 0.610968.
Train: 2018-08-09T12:01:30.153434: step 24579, loss 0.562826.
Train: 2018-08-09T12:01:30.262816: step 24580, loss 0.562829.
Test: 2018-08-09T12:01:30.762665: step 24580, loss 0.549631.
Train: 2018-08-09T12:01:30.872044: step 24581, loss 0.546803.
Train: 2018-08-09T12:01:30.965771: step 24582, loss 0.659031.
Train: 2018-08-09T12:01:31.075091: step 24583, loss 0.674936.
Train: 2018-08-09T12:01:31.184470: step 24584, loss 0.562894.
Train: 2018-08-09T12:01:31.293819: step 24585, loss 0.499209.
Train: 2018-08-09T12:01:31.387518: step 24586, loss 0.515212.
Train: 2018-08-09T12:01:31.496896: step 24587, loss 0.499323.
Train: 2018-08-09T12:01:31.606246: step 24588, loss 0.467422.
Train: 2018-08-09T12:01:31.715596: step 24589, loss 0.562905.
Train: 2018-08-09T12:01:31.809323: step 24590, loss 0.578857.
Test: 2018-08-09T12:01:32.309183: step 24590, loss 0.54845.
Train: 2018-08-09T12:01:32.418554: step 24591, loss 0.626895.
Train: 2018-08-09T12:01:32.527903: step 24592, loss 0.434643.
Train: 2018-08-09T12:01:32.621631: step 24593, loss 0.498519.
Train: 2018-08-09T12:01:32.730981: step 24594, loss 0.546584.
Train: 2018-08-09T12:01:32.887198: step 24595, loss 0.513882.
Train: 2018-08-09T12:01:32.997942: step 24596, loss 0.578697.
Train: 2018-08-09T12:01:33.107289: step 24597, loss 0.448.
Train: 2018-08-09T12:01:33.201021: step 24598, loss 0.57896.
Train: 2018-08-09T12:01:33.310369: step 24599, loss 0.477611.
Train: 2018-08-09T12:01:33.419720: step 24600, loss 0.427171.
Test: 2018-08-09T12:01:33.919571: step 24600, loss 0.549242.
Train: 2018-08-09T12:01:34.466348: step 24601, loss 0.508444.
Train: 2018-08-09T12:01:34.575696: step 24602, loss 0.641929.
Train: 2018-08-09T12:01:34.685046: step 24603, loss 0.513586.
Train: 2018-08-09T12:01:34.778775: step 24604, loss 0.633707.
Train: 2018-08-09T12:01:34.888122: step 24605, loss 0.599662.
Train: 2018-08-09T12:01:34.999755: step 24606, loss 0.543982.
Train: 2018-08-09T12:01:35.109098: step 24607, loss 0.617318.
Train: 2018-08-09T12:01:35.202831: step 24608, loss 0.601167.
Train: 2018-08-09T12:01:35.312180: step 24609, loss 0.700254.
Train: 2018-08-09T12:01:35.421500: step 24610, loss 0.580047.
Test: 2018-08-09T12:01:35.921383: step 24610, loss 0.549827.
Train: 2018-08-09T12:01:36.030730: step 24611, loss 0.51242.
Train: 2018-08-09T12:01:36.124489: step 24612, loss 0.595939.
Train: 2018-08-09T12:01:36.233840: step 24613, loss 0.545832.
Train: 2018-08-09T12:01:36.343195: step 24614, loss 0.496069.
Train: 2018-08-09T12:01:36.452537: step 24615, loss 0.678557.
Train: 2018-08-09T12:01:36.561886: step 24616, loss 0.562483.
Train: 2018-08-09T12:01:36.671206: step 24617, loss 0.579012.
Train: 2018-08-09T12:01:36.780584: step 24618, loss 0.628422.
Train: 2018-08-09T12:01:36.874315: step 24619, loss 0.546117.
Train: 2018-08-09T12:01:36.984365: step 24620, loss 0.546179.
Test: 2018-08-09T12:01:37.484218: step 24620, loss 0.550355.
Train: 2018-08-09T12:01:37.593597: step 24621, loss 0.546231.
Train: 2018-08-09T12:01:37.702947: step 24622, loss 0.497277.
Train: 2018-08-09T12:01:37.812266: step 24623, loss 0.627905.
Train: 2018-08-09T12:01:37.921615: step 24624, loss 0.530021.
Train: 2018-08-09T12:01:38.030965: step 24625, loss 0.562633.
Train: 2018-08-09T12:01:38.140343: step 24626, loss 0.64403.
Train: 2018-08-09T12:01:38.249692: step 24627, loss 0.57891.
Train: 2018-08-09T12:01:38.343420: step 24628, loss 0.627565.
Train: 2018-08-09T12:01:38.452770: step 24629, loss 0.578892.
Train: 2018-08-09T12:01:38.562119: step 24630, loss 0.5466.
Test: 2018-08-09T12:01:39.061972: step 24630, loss 0.55011.
Train: 2018-08-09T12:01:39.171352: step 24631, loss 0.514437.
Train: 2018-08-09T12:01:39.280699: step 24632, loss 0.482316.
Train: 2018-08-09T12:01:39.390044: step 24633, loss 0.643266.
Train: 2018-08-09T12:01:39.499398: step 24634, loss 0.578877.
Train: 2018-08-09T12:01:39.608742: step 24635, loss 0.611017.
Train: 2018-08-09T12:01:39.718101: step 24636, loss 0.546777.
Train: 2018-08-09T12:01:39.827416: step 24637, loss 0.482713.
Train: 2018-08-09T12:01:39.922676: step 24638, loss 0.594896.
Train: 2018-08-09T12:01:40.031996: step 24639, loss 0.626946.
Train: 2018-08-09T12:01:40.141378: step 24640, loss 0.530841.
Test: 2018-08-09T12:01:40.641276: step 24640, loss 0.5515.
Train: 2018-08-09T12:01:40.750607: step 24641, loss 0.610866.
Train: 2018-08-09T12:01:40.844334: step 24642, loss 0.674771.
Train: 2018-08-09T12:01:40.953684: step 24643, loss 0.562912.
Train: 2018-08-09T12:01:41.063033: step 24644, loss 0.483385.
Train: 2018-08-09T12:01:41.182512: step 24645, loss 0.626555.
Train: 2018-08-09T12:01:41.285361: step 24646, loss 0.515349.
Train: 2018-08-09T12:01:41.394710: step 24647, loss 0.578866.
Train: 2018-08-09T12:01:41.504055: step 24648, loss 0.594719.
Train: 2018-08-09T12:01:41.613405: step 24649, loss 0.531334.
Train: 2018-08-09T12:01:41.722754: step 24650, loss 0.610541.
Test: 2018-08-09T12:01:42.222607: step 24650, loss 0.550001.
Train: 2018-08-09T12:01:42.316335: step 24651, loss 0.531379.
Train: 2018-08-09T12:01:42.425684: step 24652, loss 0.56303.
Train: 2018-08-09T12:01:42.535062: step 24653, loss 0.673787.
Train: 2018-08-09T12:01:42.644415: step 24654, loss 0.531479.
Train: 2018-08-09T12:01:42.753765: step 24655, loss 0.547299.
Train: 2018-08-09T12:01:42.863110: step 24656, loss 0.515772.
Train: 2018-08-09T12:01:42.972460: step 24657, loss 0.531514.
Train: 2018-08-09T12:01:43.081809: step 24658, loss 0.65784.
Train: 2018-08-09T12:01:43.191158: step 24659, loss 0.484129.
Train: 2018-08-09T12:01:43.300509: step 24660, loss 0.531445.
Test: 2018-08-09T12:01:43.784739: step 24660, loss 0.548801.
Train: 2018-08-09T12:01:43.909897: step 24661, loss 0.531423.
Train: 2018-08-09T12:01:44.004782: step 24662, loss 0.531345.
Train: 2018-08-09T12:01:44.114131: step 24663, loss 0.515441.
Train: 2018-08-09T12:01:44.223480: step 24664, loss 0.562948.
Train: 2018-08-09T12:01:44.332830: step 24665, loss 0.578743.
Train: 2018-08-09T12:01:44.442179: step 24666, loss 0.547042.
Train: 2018-08-09T12:01:44.551523: step 24667, loss 0.514687.
Train: 2018-08-09T12:01:44.660878: step 24668, loss 0.643249.
Train: 2018-08-09T12:01:44.770197: step 24669, loss 0.482253.
Train: 2018-08-09T12:01:44.879547: step 24670, loss 0.594967.
Test: 2018-08-09T12:01:45.379428: step 24670, loss 0.54816.
Train: 2018-08-09T12:01:45.488807: step 24671, loss 0.578866.
Train: 2018-08-09T12:01:45.598157: step 24672, loss 0.546619.
Train: 2018-08-09T12:01:45.707506: step 24673, loss 0.578721.
Train: 2018-08-09T12:01:45.816855: step 24674, loss 0.529949.
Train: 2018-08-09T12:01:45.912906: step 24675, loss 0.529624.
Train: 2018-08-09T12:01:46.022257: step 24676, loss 0.546127.
Train: 2018-08-09T12:01:46.131605: step 24677, loss 0.463785.
Train: 2018-08-09T12:01:46.240954: step 24678, loss 0.411936.
Train: 2018-08-09T12:01:46.350333: step 24679, loss 0.51255.
Train: 2018-08-09T12:01:46.459651: step 24680, loss 0.597489.
Test: 2018-08-09T12:01:46.959549: step 24680, loss 0.54858.
Train: 2018-08-09T12:01:47.068914: step 24681, loss 0.563786.
Train: 2018-08-09T12:01:47.178263: step 24682, loss 0.562163.
Train: 2018-08-09T12:01:47.287612: step 24683, loss 0.647057.
Train: 2018-08-09T12:01:47.381340: step 24684, loss 0.450781.
Train: 2018-08-09T12:01:47.490689: step 24685, loss 0.531479.
Train: 2018-08-09T12:01:47.600009: step 24686, loss 0.600966.
Train: 2018-08-09T12:01:47.709388: step 24687, loss 0.610054.
Train: 2018-08-09T12:01:47.818739: step 24688, loss 0.529335.
Train: 2018-08-09T12:01:47.930440: step 24689, loss 0.533509.
Train: 2018-08-09T12:01:48.039819: step 24690, loss 0.632933.
Test: 2018-08-09T12:01:48.524050: step 24690, loss 0.546671.
Train: 2018-08-09T12:01:48.633429: step 24691, loss 0.684018.
Train: 2018-08-09T12:01:48.742748: step 24692, loss 0.631097.
Train: 2018-08-09T12:01:48.852098: step 24693, loss 0.541815.
Train: 2018-08-09T12:01:48.961446: step 24694, loss 0.527722.
Train: 2018-08-09T12:01:49.070826: step 24695, loss 0.564322.
Train: 2018-08-09T12:01:49.164554: step 24696, loss 0.562654.
Train: 2018-08-09T12:01:49.273902: step 24697, loss 0.512834.
Train: 2018-08-09T12:01:49.383222: step 24698, loss 0.546043.
Train: 2018-08-09T12:01:49.492602: step 24699, loss 0.578507.
Train: 2018-08-09T12:01:49.601950: step 24700, loss 0.611905.
Test: 2018-08-09T12:01:50.103213: step 24700, loss 0.551483.
Train: 2018-08-09T12:01:50.665582: step 24701, loss 0.611485.
Train: 2018-08-09T12:01:50.774931: step 24702, loss 0.480632.
Train: 2018-08-09T12:01:50.884280: step 24703, loss 0.546286.
Train: 2018-08-09T12:01:50.993630: step 24704, loss 0.546219.
Train: 2018-08-09T12:01:51.103008: step 24705, loss 0.595449.
Train: 2018-08-09T12:01:51.212357: step 24706, loss 0.497254.
Train: 2018-08-09T12:01:51.306055: step 24707, loss 0.52992.
Train: 2018-08-09T12:01:51.415437: step 24708, loss 0.53001.
Train: 2018-08-09T12:01:51.524784: step 24709, loss 0.49727.
Train: 2018-08-09T12:01:51.634133: step 24710, loss 0.628067.
Test: 2018-08-09T12:01:52.120615: step 24710, loss 0.549698.
Train: 2018-08-09T12:01:52.261212: step 24711, loss 0.562545.
Train: 2018-08-09T12:01:52.370562: step 24712, loss 0.529807.
Train: 2018-08-09T12:01:52.479910: step 24713, loss 0.644611.
Train: 2018-08-09T12:01:52.590397: step 24714, loss 0.628171.
Train: 2018-08-09T12:01:52.695575: step 24715, loss 0.529793.
Train: 2018-08-09T12:01:52.804923: step 24716, loss 0.497065.
Train: 2018-08-09T12:01:52.898621: step 24717, loss 0.546147.
Train: 2018-08-09T12:01:53.007971: step 24718, loss 0.497002.
Train: 2018-08-09T12:01:53.132941: step 24719, loss 0.546282.
Train: 2018-08-09T12:01:53.226702: step 24720, loss 0.513104.
Test: 2018-08-09T12:01:53.726552: step 24720, loss 0.550773.
Train: 2018-08-09T12:01:53.835929: step 24721, loss 0.562393.
Train: 2018-08-09T12:01:53.950759: step 24722, loss 0.612244.
Train: 2018-08-09T12:01:54.044488: step 24723, loss 0.579161.
Train: 2018-08-09T12:01:54.153840: step 24724, loss 0.562714.
Train: 2018-08-09T12:01:54.263186: step 24725, loss 0.463166.
Train: 2018-08-09T12:01:54.372536: step 24726, loss 0.512726.
Train: 2018-08-09T12:01:54.466263: step 24727, loss 0.51229.
Train: 2018-08-09T12:01:54.575612: step 24728, loss 0.695969.
Train: 2018-08-09T12:01:54.684964: step 24729, loss 0.579057.
Train: 2018-08-09T12:01:54.794281: step 24730, loss 0.578773.
Test: 2018-08-09T12:01:55.294211: step 24730, loss 0.547998.
Train: 2018-08-09T12:01:55.387920: step 24731, loss 0.579738.
Train: 2018-08-09T12:01:55.497270: step 24732, loss 0.545474.
Train: 2018-08-09T12:01:55.606589: step 24733, loss 0.645585.
Train: 2018-08-09T12:01:55.700317: step 24734, loss 0.628957.
Train: 2018-08-09T12:01:55.809697: step 24735, loss 0.495807.
Train: 2018-08-09T12:01:55.919045: step 24736, loss 0.529144.
Train: 2018-08-09T12:01:56.012743: step 24737, loss 0.579589.
Train: 2018-08-09T12:01:56.122094: step 24738, loss 0.562002.
Train: 2018-08-09T12:01:56.231475: step 24739, loss 0.496132.
Train: 2018-08-09T12:01:56.340823: step 24740, loss 0.579827.
Test: 2018-08-09T12:01:56.825053: step 24740, loss 0.54943.
Train: 2018-08-09T12:01:56.936700: step 24741, loss 0.579342.
Train: 2018-08-09T12:01:57.046050: step 24742, loss 0.644998.
Train: 2018-08-09T12:01:57.139748: step 24743, loss 0.644898.
Train: 2018-08-09T12:01:57.249127: step 24744, loss 0.546239.
Train: 2018-08-09T12:01:57.358476: step 24745, loss 0.611731.
Train: 2018-08-09T12:01:57.452205: step 24746, loss 0.513564.
Train: 2018-08-09T12:01:57.561553: step 24747, loss 0.530059.
Train: 2018-08-09T12:01:57.655281: step 24748, loss 0.546367.
Train: 2018-08-09T12:01:57.764601: step 24749, loss 0.530116.
Train: 2018-08-09T12:01:57.873950: step 24750, loss 0.627718.
Test: 2018-08-09T12:01:58.373831: step 24750, loss 0.549284.
Train: 2018-08-09T12:01:58.467590: step 24751, loss 0.546424.
Train: 2018-08-09T12:01:58.576940: step 24752, loss 0.513968.
Train: 2018-08-09T12:01:58.670667: step 24753, loss 0.676288.
Train: 2018-08-09T12:01:58.780015: step 24754, loss 0.530287.
Train: 2018-08-09T12:01:58.873743: step 24755, loss 0.611281.
Train: 2018-08-09T12:01:58.983792: step 24756, loss 0.595067.
Train: 2018-08-09T12:01:59.093141: step 24757, loss 0.611166.
Train: 2018-08-09T12:01:59.186838: step 24758, loss 0.578876.
Train: 2018-08-09T12:01:59.296219: step 24759, loss 0.611006.
Train: 2018-08-09T12:01:59.394599: step 24760, loss 0.482706.
Test: 2018-08-09T12:01:59.894461: step 24760, loss 0.549669.
Train: 2018-08-09T12:02:00.003830: step 24761, loss 0.578862.
Train: 2018-08-09T12:02:00.097558: step 24762, loss 0.578845.
Train: 2018-08-09T12:02:00.206879: step 24763, loss 0.546934.
Train: 2018-08-09T12:02:00.316259: step 24764, loss 0.51499.
Train: 2018-08-09T12:02:00.409989: step 24765, loss 0.514966.
Train: 2018-08-09T12:02:00.519335: step 24766, loss 0.562914.
Train: 2018-08-09T12:02:00.613032: step 24767, loss 0.690966.
Train: 2018-08-09T12:02:00.722381: step 24768, loss 0.562722.
Train: 2018-08-09T12:02:00.816140: step 24769, loss 0.563192.
Train: 2018-08-09T12:02:00.925488: step 24770, loss 0.579046.
Test: 2018-08-09T12:02:01.409720: step 24770, loss 0.54921.
Train: 2018-08-09T12:02:01.519069: step 24771, loss 0.467331.
Train: 2018-08-09T12:02:01.628448: step 24772, loss 0.562982.
Train: 2018-08-09T12:02:01.729241: step 24773, loss 0.515089.
Train: 2018-08-09T12:02:01.822967: step 24774, loss 0.562896.
Train: 2018-08-09T12:02:01.932285: step 24775, loss 0.642784.
Train: 2018-08-09T12:02:02.026013: step 24776, loss 0.57886.
Train: 2018-08-09T12:02:02.135391: step 24777, loss 0.578868.
Train: 2018-08-09T12:02:02.229123: step 24778, loss 0.562887.
Train: 2018-08-09T12:02:02.338468: step 24779, loss 0.514971.
Train: 2018-08-09T12:02:02.432167: step 24780, loss 0.610837.
Test: 2018-08-09T12:02:02.932049: step 24780, loss 0.548513.
Train: 2018-08-09T12:02:03.025807: step 24781, loss 0.498957.
Train: 2018-08-09T12:02:03.135156: step 24782, loss 0.498891.
Train: 2018-08-09T12:02:03.228887: step 24783, loss 0.482726.
Train: 2018-08-09T12:02:03.338202: step 24784, loss 0.530667.
Train: 2018-08-09T12:02:03.431932: step 24785, loss 0.594992.
Train: 2018-08-09T12:02:03.541281: step 24786, loss 0.498106.
Train: 2018-08-09T12:02:03.635038: step 24787, loss 0.546495.
Train: 2018-08-09T12:02:03.744393: step 24788, loss 0.611428.
Train: 2018-08-09T12:02:03.838084: step 24789, loss 0.513748.
Train: 2018-08-09T12:02:03.949717: step 24790, loss 0.578966.
Test: 2018-08-09T12:02:04.433980: step 24790, loss 0.550952.
Train: 2018-08-09T12:02:04.543362: step 24791, loss 0.611686.
Train: 2018-08-09T12:02:04.637057: step 24792, loss 0.628036.
Train: 2018-08-09T12:02:04.746435: step 24793, loss 0.529775.
Train: 2018-08-09T12:02:04.840163: step 24794, loss 0.480216.
Train: 2018-08-09T12:02:04.949517: step 24795, loss 0.578774.
Train: 2018-08-09T12:02:05.043210: step 24796, loss 0.480095.
Train: 2018-08-09T12:02:05.136968: step 24797, loss 0.477669.
Train: 2018-08-09T12:02:05.246318: step 24798, loss 0.529927.
Train: 2018-08-09T12:02:05.355661: step 24799, loss 0.54515.
Train: 2018-08-09T12:02:05.449388: step 24800, loss 0.614385.
Test: 2018-08-09T12:02:05.936015: step 24800, loss 0.548042.
Train: 2018-08-09T12:02:06.560896: step 24801, loss 0.51144.
Train: 2018-08-09T12:02:06.670250: step 24802, loss 0.560541.
Train: 2018-08-09T12:02:06.763973: step 24803, loss 0.512064.
Train: 2018-08-09T12:02:06.857701: step 24804, loss 0.527776.
Train: 2018-08-09T12:02:06.967050: step 24805, loss 0.58704.
Train: 2018-08-09T12:02:07.060778: step 24806, loss 0.544096.
Train: 2018-08-09T12:02:07.154476: step 24807, loss 0.632486.
Train: 2018-08-09T12:02:07.263859: step 24808, loss 0.493082.
Train: 2018-08-09T12:02:07.357583: step 24809, loss 0.613388.
Train: 2018-08-09T12:02:07.466933: step 24810, loss 0.423753.
Test: 2018-08-09T12:02:07.951867: step 24810, loss 0.54812.
Train: 2018-08-09T12:02:08.061217: step 24811, loss 0.598931.
Train: 2018-08-09T12:02:08.154974: step 24812, loss 0.579306.
Train: 2018-08-09T12:02:08.248672: step 24813, loss 0.560515.
Train: 2018-08-09T12:02:08.358056: step 24814, loss 0.543808.
Train: 2018-08-09T12:02:08.451776: step 24815, loss 0.45914.
Train: 2018-08-09T12:02:08.561129: step 24816, loss 0.561419.
Train: 2018-08-09T12:02:08.654826: step 24817, loss 0.614324.
Train: 2018-08-09T12:02:08.764178: step 24818, loss 0.561396.
Train: 2018-08-09T12:02:08.857934: step 24819, loss 0.702147.
Train: 2018-08-09T12:02:08.951662: step 24820, loss 0.650056.
Test: 2018-08-09T12:02:09.451516: step 24820, loss 0.545053.
Train: 2018-08-09T12:02:09.560863: step 24821, loss 0.510531.
Train: 2018-08-09T12:02:09.654591: step 24822, loss 0.512633.
Train: 2018-08-09T12:02:09.748319: step 24823, loss 0.595326.
Train: 2018-08-09T12:02:09.857698: step 24824, loss 0.613045.
Train: 2018-08-09T12:02:09.951426: step 24825, loss 0.595164.
Train: 2018-08-09T12:02:10.045124: step 24826, loss 0.512174.
Train: 2018-08-09T12:02:10.154503: step 24827, loss 0.562.
Train: 2018-08-09T12:02:10.248231: step 24828, loss 0.479208.
Train: 2018-08-09T12:02:10.341962: step 24829, loss 0.712779.
Train: 2018-08-09T12:02:10.435689: step 24830, loss 0.529521.
Test: 2018-08-09T12:02:10.937839: step 24830, loss 0.5495.
Train: 2018-08-09T12:02:11.031598: step 24831, loss 0.59551.
Train: 2018-08-09T12:02:11.125328: step 24832, loss 0.545854.
Train: 2018-08-09T12:02:11.234677: step 24833, loss 0.595265.
Train: 2018-08-09T12:02:11.328403: step 24834, loss 0.579051.
Train: 2018-08-09T12:02:11.422131: step 24835, loss 0.579232.
Train: 2018-08-09T12:02:11.531481: step 24836, loss 0.627756.
Train: 2018-08-09T12:02:11.625211: step 24837, loss 0.595012.
Train: 2018-08-09T12:02:11.718935: step 24838, loss 0.530197.
Train: 2018-08-09T12:02:11.828285: step 24839, loss 0.498028.
Train: 2018-08-09T12:02:11.922013: step 24840, loss 0.562659.
Test: 2018-08-09T12:02:12.421867: step 24840, loss 0.551898.
Train: 2018-08-09T12:02:12.515593: step 24841, loss 0.482074.
Train: 2018-08-09T12:02:12.609351: step 24842, loss 0.627197.
Train: 2018-08-09T12:02:12.718695: step 24843, loss 0.514307.
Train: 2018-08-09T12:02:12.812428: step 24844, loss 0.546677.
Train: 2018-08-09T12:02:12.908464: step 24845, loss 0.578988.
Train: 2018-08-09T12:02:13.002193: step 24846, loss 0.578828.
Train: 2018-08-09T12:02:13.095921: step 24847, loss 0.562536.
Train: 2018-08-09T12:02:13.205300: step 24848, loss 0.530396.
Train: 2018-08-09T12:02:13.299028: step 24849, loss 0.546293.
Train: 2018-08-09T12:02:13.392756: step 24850, loss 0.692658.
Test: 2018-08-09T12:02:13.892608: step 24850, loss 0.547561.
Train: 2018-08-09T12:02:13.986366: step 24851, loss 0.546521.
Train: 2018-08-09T12:02:14.080094: step 24852, loss 0.562723.
Train: 2018-08-09T12:02:14.189437: step 24853, loss 0.595068.
Train: 2018-08-09T12:02:14.283171: step 24854, loss 0.579118.
Train: 2018-08-09T12:02:14.376899: step 24855, loss 0.466201.
Train: 2018-08-09T12:02:14.470626: step 24856, loss 0.594805.
Train: 2018-08-09T12:02:14.579976: step 24857, loss 0.579095.
Train: 2018-08-09T12:02:14.677332: step 24858, loss 0.530518.
Train: 2018-08-09T12:02:14.771069: step 24859, loss 0.62729.
Train: 2018-08-09T12:02:14.864797: step 24860, loss 0.578751.
Test: 2018-08-09T12:02:15.364650: step 24860, loss 0.550163.
Train: 2018-08-09T12:02:15.458376: step 24861, loss 0.482285.
Train: 2018-08-09T12:02:15.567750: step 24862, loss 0.514586.
Train: 2018-08-09T12:02:15.661484: step 24863, loss 0.594906.
Train: 2018-08-09T12:02:15.755182: step 24864, loss 0.675725.
Train: 2018-08-09T12:02:15.848940: step 24865, loss 0.562649.
Train: 2018-08-09T12:02:15.942668: step 24866, loss 0.627255.
Train: 2018-08-09T12:02:16.036396: step 24867, loss 0.594872.
Train: 2018-08-09T12:02:16.130124: step 24868, loss 0.594748.
Train: 2018-08-09T12:02:16.239449: step 24869, loss 0.546665.
Train: 2018-08-09T12:02:16.333201: step 24870, loss 0.658499.
Test: 2018-08-09T12:02:16.833065: step 24870, loss 0.548614.
Train: 2018-08-09T12:02:16.929158: step 24871, loss 0.674427.
Train: 2018-08-09T12:02:17.022886: step 24872, loss 0.547051.
Train: 2018-08-09T12:02:17.116644: step 24873, loss 0.500046.
Train: 2018-08-09T12:02:17.225962: step 24874, loss 0.626094.
Train: 2018-08-09T12:02:17.319691: step 24875, loss 0.468799.
Train: 2018-08-09T12:02:17.413419: step 24876, loss 0.563258.
Train: 2018-08-09T12:02:17.522767: step 24877, loss 0.610397.
Train: 2018-08-09T12:02:17.616526: step 24878, loss 0.594439.
Train: 2018-08-09T12:02:17.710253: step 24879, loss 0.469233.
Train: 2018-08-09T12:02:17.803981: step 24880, loss 0.500498.
Test: 2018-08-09T12:02:18.288214: step 24880, loss 0.549533.
Train: 2018-08-09T12:02:18.381970: step 24881, loss 0.657914.
Train: 2018-08-09T12:02:18.475699: step 24882, loss 0.453296.
Train: 2018-08-09T12:02:18.585048: step 24883, loss 0.531427.
Train: 2018-08-09T12:02:18.678776: step 24884, loss 0.531668.
Train: 2018-08-09T12:02:18.772503: step 24885, loss 0.547359.
Train: 2018-08-09T12:02:18.866231: step 24886, loss 0.595349.
Train: 2018-08-09T12:02:18.962213: step 24887, loss 0.546773.
Train: 2018-08-09T12:02:19.055945: step 24888, loss 0.611246.
Train: 2018-08-09T12:02:19.149669: step 24889, loss 0.514319.
Train: 2018-08-09T12:02:19.243397: step 24890, loss 0.547058.
Test: 2018-08-09T12:02:19.743249: step 24890, loss 0.551177.
Train: 2018-08-09T12:02:19.837006: step 24891, loss 0.611415.
Train: 2018-08-09T12:02:19.930739: step 24892, loss 0.643265.
Train: 2018-08-09T12:02:20.024463: step 24893, loss 0.611512.
Train: 2018-08-09T12:02:20.118190: step 24894, loss 0.626819.
Train: 2018-08-09T12:02:20.211919: step 24895, loss 0.515132.
Train: 2018-08-09T12:02:20.305650: step 24896, loss 0.546927.
Train: 2018-08-09T12:02:20.399374: step 24897, loss 0.611235.
Train: 2018-08-09T12:02:20.493105: step 24898, loss 0.483374.
Train: 2018-08-09T12:02:20.602452: step 24899, loss 0.531352.
Train: 2018-08-09T12:02:20.696180: step 24900, loss 0.451002.
Test: 2018-08-09T12:02:21.182678: step 24900, loss 0.550261.
Train: 2018-08-09T12:02:21.729394: step 24901, loss 0.514822.
Train: 2018-08-09T12:02:21.823145: step 24902, loss 0.531306.
Train: 2018-08-09T12:02:21.916849: step 24903, loss 0.59475.
Train: 2018-08-09T12:02:22.010609: step 24904, loss 0.561764.
Train: 2018-08-09T12:02:22.104305: step 24905, loss 0.611777.
Train: 2018-08-09T12:02:22.198063: step 24906, loss 0.562299.
Train: 2018-08-09T12:02:22.291791: step 24907, loss 0.628346.
Train: 2018-08-09T12:02:22.385490: step 24908, loss 0.628738.
Train: 2018-08-09T12:02:22.479246: step 24909, loss 0.595393.
Train: 2018-08-09T12:02:22.572974: step 24910, loss 0.562523.
Test: 2018-08-09T12:02:23.062153: step 24910, loss 0.549371.
Train: 2018-08-09T12:02:23.155880: step 24911, loss 0.643606.
Train: 2018-08-09T12:02:23.249607: step 24912, loss 0.562643.
Train: 2018-08-09T12:02:23.343305: step 24913, loss 0.482602.
Train: 2018-08-09T12:02:23.437035: step 24914, loss 0.530886.
Train: 2018-08-09T12:02:23.530762: step 24915, loss 0.498794.
Train: 2018-08-09T12:02:23.624520: step 24916, loss 0.562763.
Train: 2018-08-09T12:02:23.718250: step 24917, loss 0.594883.
Train: 2018-08-09T12:02:23.811945: step 24918, loss 0.594878.
Train: 2018-08-09T12:02:23.905705: step 24919, loss 0.498633.
Train: 2018-08-09T12:02:23.999431: step 24920, loss 0.450334.
Test: 2018-08-09T12:02:24.499299: step 24920, loss 0.548884.
Train: 2018-08-09T12:02:24.593039: step 24921, loss 0.546681.
Train: 2018-08-09T12:02:24.686770: step 24922, loss 0.595084.
Train: 2018-08-09T12:02:24.780498: step 24923, loss 0.578846.
Train: 2018-08-09T12:02:24.874225: step 24924, loss 0.595025.
Train: 2018-08-09T12:02:24.966507: step 24925, loss 0.64387.
Train: 2018-08-09T12:02:25.060265: step 24926, loss 0.611613.
Train: 2018-08-09T12:02:25.153992: step 24927, loss 0.578665.
Train: 2018-08-09T12:02:25.247721: step 24928, loss 0.562514.
Train: 2018-08-09T12:02:25.341448: step 24929, loss 0.643873.
Train: 2018-08-09T12:02:25.435176: step 24930, loss 0.56272.
Test: 2018-08-09T12:02:25.935030: step 24930, loss 0.551914.
Train: 2018-08-09T12:02:26.028757: step 24931, loss 0.578904.
Train: 2018-08-09T12:02:26.122515: step 24932, loss 0.594957.
Train: 2018-08-09T12:02:26.216242: step 24933, loss 0.562966.
Train: 2018-08-09T12:02:26.309971: step 24934, loss 0.594944.
Train: 2018-08-09T12:02:26.403701: step 24935, loss 0.690622.
Train: 2018-08-09T12:02:26.497396: step 24936, loss 0.499337.
Train: 2018-08-09T12:02:26.575534: step 24937, loss 0.515382.
Train: 2018-08-09T12:02:26.669261: step 24938, loss 0.64226.
Train: 2018-08-09T12:02:26.762989: step 24939, loss 0.626305.
Train: 2018-08-09T12:02:26.872355: step 24940, loss 0.641944.
Test: 2018-08-09T12:02:27.356570: step 24940, loss 0.547841.
Train: 2018-08-09T12:02:27.450297: step 24941, loss 0.468879.
Train: 2018-08-09T12:02:27.544054: step 24942, loss 0.610248.
Train: 2018-08-09T12:02:27.637782: step 24943, loss 0.657144.
Train: 2018-08-09T12:02:27.731513: step 24944, loss 0.547686.
Train: 2018-08-09T12:02:27.825238: step 24945, loss 0.547775.
Train: 2018-08-09T12:02:27.921269: step 24946, loss 0.578909.
Train: 2018-08-09T12:02:28.014997: step 24947, loss 0.516892.
Train: 2018-08-09T12:02:28.108755: step 24948, loss 0.547933.
Train: 2018-08-09T12:02:28.202483: step 24949, loss 0.532448.
Train: 2018-08-09T12:02:28.296214: step 24950, loss 0.609918.
Test: 2018-08-09T12:02:28.780443: step 24950, loss 0.551837.
Train: 2018-08-09T12:02:28.874203: step 24951, loss 0.516941.
Train: 2018-08-09T12:02:28.967897: step 24952, loss 0.67197.
Train: 2018-08-09T12:02:29.061655: step 24953, loss 0.578921.
Train: 2018-08-09T12:02:29.155383: step 24954, loss 0.609882.
Train: 2018-08-09T12:02:29.249116: step 24955, loss 0.501646.
Train: 2018-08-09T12:02:29.342840: step 24956, loss 0.563477.
Train: 2018-08-09T12:02:29.436567: step 24957, loss 0.532558.
Train: 2018-08-09T12:02:29.530295: step 24958, loss 0.563459.
Train: 2018-08-09T12:02:29.624023: step 24959, loss 0.516994.
Train: 2018-08-09T12:02:29.717751: step 24960, loss 0.547901.
Test: 2018-08-09T12:02:30.218215: step 24960, loss 0.5494.
Train: 2018-08-09T12:02:30.311971: step 24961, loss 0.470145.
Train: 2018-08-09T12:02:30.405689: step 24962, loss 0.516539.
Train: 2018-08-09T12:02:30.483776: step 24963, loss 0.516279.
Train: 2018-08-09T12:02:30.577535: step 24964, loss 0.578881.
Train: 2018-08-09T12:02:30.671262: step 24965, loss 0.547295.
Train: 2018-08-09T12:02:30.764991: step 24966, loss 0.467965.
Train: 2018-08-09T12:02:30.858718: step 24967, loss 0.467268.
Train: 2018-08-09T12:02:30.952446: step 24968, loss 0.498397.
Train: 2018-08-09T12:02:31.046175: step 24969, loss 0.497543.
Train: 2018-08-09T12:02:31.139902: step 24970, loss 0.44632.
Test: 2018-08-09T12:02:31.639755: step 24970, loss 0.545815.
Train: 2018-08-09T12:02:31.733514: step 24971, loss 0.529681.
Train: 2018-08-09T12:02:31.827210: step 24972, loss 0.578119.
Train: 2018-08-09T12:02:31.905316: step 24973, loss 0.615962.
Train: 2018-08-09T12:02:32.014696: step 24974, loss 0.417502.
Train: 2018-08-09T12:02:32.092802: step 24975, loss 0.562845.
Train: 2018-08-09T12:02:32.186530: step 24976, loss 0.568641.
Train: 2018-08-09T12:02:32.280258: step 24977, loss 0.577817.
Train: 2018-08-09T12:02:32.373987: step 24978, loss 0.639518.
Train: 2018-08-09T12:02:32.467739: step 24979, loss 0.687637.
Train: 2018-08-09T12:02:32.561411: step 24980, loss 0.532224.
Test: 2018-08-09T12:02:33.059015: step 24980, loss 0.545165.
Train: 2018-08-09T12:02:33.152742: step 24981, loss 0.440221.
Train: 2018-08-09T12:02:33.246501: step 24982, loss 0.489958.
Train: 2018-08-09T12:02:33.340229: step 24983, loss 0.508471.
Train: 2018-08-09T12:02:33.433958: step 24984, loss 0.474001.
Train: 2018-08-09T12:02:33.527684: step 24985, loss 0.527076.
Train: 2018-08-09T12:02:33.621412: step 24986, loss 0.615271.
Train: 2018-08-09T12:02:33.715140: step 24987, loss 0.668489.
Train: 2018-08-09T12:02:33.808871: step 24988, loss 0.491435.
Train: 2018-08-09T12:02:33.902596: step 24989, loss 0.59783.
Train: 2018-08-09T12:02:33.996324: step 24990, loss 0.58093.
Test: 2018-08-09T12:02:34.496176: step 24990, loss 0.548115.
Train: 2018-08-09T12:02:34.589933: step 24991, loss 0.561759.
Train: 2018-08-09T12:02:34.683664: step 24992, loss 0.563531.
Train: 2018-08-09T12:02:34.777359: step 24993, loss 0.511681.
Train: 2018-08-09T12:02:34.855466: step 24994, loss 0.494183.
Train: 2018-08-09T12:02:34.950555: step 24995, loss 0.544621.
Train: 2018-08-09T12:02:35.044287: step 24996, loss 0.664745.
Train: 2018-08-09T12:02:35.144011: step 24997, loss 0.528534.
Train: 2018-08-09T12:02:35.237739: step 24998, loss 0.544965.
Train: 2018-08-09T12:02:35.331467: step 24999, loss 0.663691.
Train: 2018-08-09T12:02:35.425195: step 25000, loss 0.579155.
Test: 2018-08-09T12:02:35.909426: step 25000, loss 0.547875.
Train: 2018-08-09T12:02:36.503066: step 25001, loss 0.545486.
Train: 2018-08-09T12:02:36.596794: step 25002, loss 0.612648.
Train: 2018-08-09T12:02:36.674901: step 25003, loss 0.579328.
Train: 2018-08-09T12:02:36.768628: step 25004, loss 0.645936.
Train: 2018-08-09T12:02:36.862356: step 25005, loss 0.695473.
Train: 2018-08-09T12:02:36.966457: step 25006, loss 0.562431.
Train: 2018-08-09T12:02:37.044532: step 25007, loss 0.595187.
Train: 2018-08-09T12:02:37.138290: step 25008, loss 0.481258.
Train: 2018-08-09T12:02:37.232019: step 25009, loss 0.61131.
Train: 2018-08-09T12:02:37.325716: step 25010, loss 0.563629.
Test: 2018-08-09T12:02:37.825600: step 25010, loss 0.548769.
Train: 2018-08-09T12:02:37.919327: step 25011, loss 0.530343.
Train: 2018-08-09T12:02:38.013084: step 25012, loss 0.578863.
Train: 2018-08-09T12:02:38.106812: step 25013, loss 0.54663.
Train: 2018-08-09T12:02:38.200541: step 25014, loss 0.595046.
Train: 2018-08-09T12:02:38.294268: step 25015, loss 0.659511.
Train: 2018-08-09T12:02:38.387997: step 25016, loss 0.498475.
Train: 2018-08-09T12:02:38.481724: step 25017, loss 0.4665.
Train: 2018-08-09T12:02:38.575453: step 25018, loss 0.498599.
Train: 2018-08-09T12:02:38.669174: step 25019, loss 0.611.
Train: 2018-08-09T12:02:38.762908: step 25020, loss 0.54677.
Test: 2018-08-09T12:02:39.262761: step 25020, loss 0.548929.
Train: 2018-08-09T12:02:39.356518: step 25021, loss 0.691514.
Train: 2018-08-09T12:02:39.450247: step 25022, loss 0.62712.
Train: 2018-08-09T12:02:39.543973: step 25023, loss 0.578771.
Train: 2018-08-09T12:02:39.637703: step 25024, loss 0.562957.
Train: 2018-08-09T12:02:39.731404: step 25025, loss 0.483167.
Train: 2018-08-09T12:02:39.825158: step 25026, loss 0.531026.
Train: 2018-08-09T12:02:39.919163: step 25027, loss 0.642627.
Train: 2018-08-09T12:02:39.999354: step 25028, loss 0.594775.
Train: 2018-08-09T12:02:40.093081: step 25029, loss 0.594796.
Train: 2018-08-09T12:02:40.186781: step 25030, loss 0.594715.
Test: 2018-08-09T12:02:40.686662: step 25030, loss 0.547554.
Train: 2018-08-09T12:02:40.780390: step 25031, loss 0.483746.
Train: 2018-08-09T12:02:40.874117: step 25032, loss 0.483759.
Train: 2018-08-09T12:02:40.967875: step 25033, loss 0.563021.
Train: 2018-08-09T12:02:41.061603: step 25034, loss 0.64241.
Train: 2018-08-09T12:02:41.155333: step 25035, loss 0.642436.
Train: 2018-08-09T12:02:41.249059: step 25036, loss 0.547161.
Train: 2018-08-09T12:02:41.342788: step 25037, loss 0.594683.
Train: 2018-08-09T12:02:41.436515: step 25038, loss 0.610503.
Train: 2018-08-09T12:02:41.530243: step 25039, loss 0.499903.
Train: 2018-08-09T12:02:41.623971: step 25040, loss 0.484107.
Test: 2018-08-09T12:02:42.124550: step 25040, loss 0.551833.
Train: 2018-08-09T12:02:42.218305: step 25041, loss 0.468209.
Train: 2018-08-09T12:02:42.312034: step 25042, loss 0.578835.
Train: 2018-08-09T12:02:42.405732: step 25043, loss 0.53119.
Train: 2018-08-09T12:02:42.499489: step 25044, loss 0.483374.
Train: 2018-08-09T12:02:42.593188: step 25045, loss 0.530897.
Train: 2018-08-09T12:02:42.686916: step 25046, loss 0.546764.
Train: 2018-08-09T12:02:42.780644: step 25047, loss 0.595149.
Train: 2018-08-09T12:02:42.874401: step 25048, loss 0.530395.
Train: 2018-08-09T12:02:42.968129: step 25049, loss 0.709397.
Train: 2018-08-09T12:02:43.061827: step 25050, loss 0.562589.
Test: 2018-08-09T12:02:43.561709: step 25050, loss 0.549344.
Train: 2018-08-09T12:02:43.655470: step 25051, loss 0.692487.
Train: 2018-08-09T12:02:43.749196: step 25052, loss 0.530208.
Train: 2018-08-09T12:02:43.842893: step 25053, loss 0.579115.
Train: 2018-08-09T12:02:43.936651: step 25054, loss 0.595058.
Train: 2018-08-09T12:02:44.030379: step 25055, loss 0.514543.
Train: 2018-08-09T12:02:44.124107: step 25056, loss 0.514681.
Train: 2018-08-09T12:02:44.217835: step 25057, loss 0.498467.
Train: 2018-08-09T12:02:44.311533: step 25058, loss 0.562768.
Train: 2018-08-09T12:02:44.405261: step 25059, loss 0.595005.
Train: 2018-08-09T12:02:44.499019: step 25060, loss 0.643433.
Test: 2018-08-09T12:02:45.001245: step 25060, loss 0.548847.
Train: 2018-08-09T12:02:45.106506: step 25061, loss 0.530481.
Train: 2018-08-09T12:02:45.200234: step 25062, loss 0.562733.
Train: 2018-08-09T12:02:45.293996: step 25063, loss 0.594992.
Train: 2018-08-09T12:02:45.387719: step 25064, loss 0.54662.
Train: 2018-08-09T12:02:45.481447: step 25065, loss 0.56275.
Train: 2018-08-09T12:02:45.575176: step 25066, loss 0.530492.
Train: 2018-08-09T12:02:45.668905: step 25067, loss 0.56271.
Train: 2018-08-09T12:02:45.762601: step 25068, loss 0.611192.
Train: 2018-08-09T12:02:45.856359: step 25069, loss 0.627355.
Train: 2018-08-09T12:02:45.950057: step 25070, loss 0.578911.
Test: 2018-08-09T12:02:46.449948: step 25070, loss 0.549494.
Train: 2018-08-09T12:02:46.543667: step 25071, loss 0.595032.
Train: 2018-08-09T12:02:46.637425: step 25072, loss 0.707589.
Train: 2018-08-09T12:02:46.731152: step 25073, loss 0.466702.
Train: 2018-08-09T12:02:46.824881: step 25074, loss 0.562869.
Train: 2018-08-09T12:02:46.934224: step 25075, loss 0.498989.
Train: 2018-08-09T12:02:47.027958: step 25076, loss 0.562889.
Train: 2018-08-09T12:02:47.121685: step 25077, loss 0.57885.
Train: 2018-08-09T12:02:47.215384: step 25078, loss 0.57886.
Train: 2018-08-09T12:02:47.309141: step 25079, loss 0.594798.
Train: 2018-08-09T12:02:47.402870: step 25080, loss 0.531062.
Test: 2018-08-09T12:02:47.902722: step 25080, loss 0.548558.
Train: 2018-08-09T12:02:47.996449: step 25081, loss 0.642738.
Train: 2018-08-09T12:02:48.090178: step 25082, loss 0.53101.
Train: 2018-08-09T12:02:48.191611: step 25083, loss 0.54706.
Train: 2018-08-09T12:02:48.285309: step 25084, loss 0.531125.
Train: 2018-08-09T12:02:48.379067: step 25085, loss 0.562958.
Train: 2018-08-09T12:02:48.488395: step 25086, loss 0.499255.
Train: 2018-08-09T12:02:48.582148: step 25087, loss 0.54699.
Train: 2018-08-09T12:02:48.675873: step 25088, loss 0.54695.
Train: 2018-08-09T12:02:48.769600: step 25089, loss 0.514809.
Train: 2018-08-09T12:02:48.863297: step 25090, loss 0.562765.
Test: 2018-08-09T12:02:49.372442: step 25090, loss 0.550093.
Train: 2018-08-09T12:02:49.466200: step 25091, loss 0.594972.
Train: 2018-08-09T12:02:49.559932: step 25092, loss 0.595092.
Train: 2018-08-09T12:02:49.653625: step 25093, loss 0.481575.
Train: 2018-08-09T12:02:49.762974: step 25094, loss 0.529969.
Train: 2018-08-09T12:02:49.856732: step 25095, loss 0.530365.
Train: 2018-08-09T12:02:49.950462: step 25096, loss 0.530926.
Train: 2018-08-09T12:02:50.059808: step 25097, loss 0.480331.
Train: 2018-08-09T12:02:50.153507: step 25098, loss 0.596147.
Train: 2018-08-09T12:02:50.247235: step 25099, loss 0.545263.
Train: 2018-08-09T12:02:50.356584: step 25100, loss 0.548257.
Test: 2018-08-09T12:02:50.840846: step 25100, loss 0.548463.
Train: 2018-08-09T12:02:51.434454: step 25101, loss 0.494112.
Train: 2018-08-09T12:02:51.543834: step 25102, loss 0.547672.
Train: 2018-08-09T12:02:51.637533: step 25103, loss 0.631393.
Train: 2018-08-09T12:02:51.731289: step 25104, loss 0.631849.
Train: 2018-08-09T12:02:51.825019: step 25105, loss 0.544365.
Train: 2018-08-09T12:02:51.921012: step 25106, loss 0.6119.
Train: 2018-08-09T12:02:52.014741: step 25107, loss 0.628767.
Train: 2018-08-09T12:02:52.124089: step 25108, loss 0.645552.
Train: 2018-08-09T12:02:52.217847: step 25109, loss 0.479882.
Train: 2018-08-09T12:02:52.311575: step 25110, loss 0.480012.
Test: 2018-08-09T12:02:52.811429: step 25110, loss 0.550169.
Train: 2018-08-09T12:02:52.905156: step 25111, loss 0.562373.
Train: 2018-08-09T12:02:53.014535: step 25112, loss 0.529367.
Train: 2018-08-09T12:02:53.108266: step 25113, loss 0.54603.
Train: 2018-08-09T12:02:53.201991: step 25114, loss 0.546142.
Train: 2018-08-09T12:02:53.295718: step 25115, loss 0.578997.
Train: 2018-08-09T12:02:53.405067: step 25116, loss 0.479749.
Train: 2018-08-09T12:02:53.506353: step 25117, loss 0.42995.
Train: 2018-08-09T12:02:53.600112: step 25118, loss 0.495841.
Train: 2018-08-09T12:02:53.693839: step 25119, loss 0.5795.
Train: 2018-08-09T12:02:53.787567: step 25120, loss 0.562648.
Test: 2018-08-09T12:02:54.296743: step 25120, loss 0.547877.
Train: 2018-08-09T12:02:54.390502: step 25121, loss 0.478135.
Train: 2018-08-09T12:02:54.484199: step 25122, loss 0.527964.
Train: 2018-08-09T12:02:54.593580: step 25123, loss 0.561645.
Train: 2018-08-09T12:02:54.687306: step 25124, loss 0.562288.
Train: 2018-08-09T12:02:54.781035: step 25125, loss 0.547614.
Train: 2018-08-09T12:02:54.890385: step 25126, loss 0.509933.
Train: 2018-08-09T12:02:54.984111: step 25127, loss 0.66566.
Train: 2018-08-09T12:02:55.077843: step 25128, loss 0.476566.
Train: 2018-08-09T12:02:55.187189: step 25129, loss 0.650314.
Train: 2018-08-09T12:02:55.280920: step 25130, loss 0.49248.
Test: 2018-08-09T12:02:55.780770: step 25130, loss 0.548732.
Train: 2018-08-09T12:02:55.874497: step 25131, loss 0.580607.
Train: 2018-08-09T12:02:55.983846: step 25132, loss 0.493037.
Train: 2018-08-09T12:02:56.077574: step 25133, loss 0.475973.
Train: 2018-08-09T12:02:56.171302: step 25134, loss 0.544959.
Train: 2018-08-09T12:02:56.265030: step 25135, loss 0.580171.
Train: 2018-08-09T12:02:56.374409: step 25136, loss 0.580595.
Train: 2018-08-09T12:02:56.468137: step 25137, loss 0.474907.
Train: 2018-08-09T12:02:56.561865: step 25138, loss 0.509557.
Train: 2018-08-09T12:02:56.671219: step 25139, loss 0.598897.
Train: 2018-08-09T12:02:56.764942: step 25140, loss 0.508827.
Test: 2018-08-09T12:02:57.266162: step 25140, loss 0.549202.
Train: 2018-08-09T12:02:57.359890: step 25141, loss 0.509538.
Train: 2018-08-09T12:02:57.469239: step 25142, loss 0.544711.
Train: 2018-08-09T12:02:57.562998: step 25143, loss 0.580221.
Train: 2018-08-09T12:02:57.672347: step 25144, loss 0.434157.
Train: 2018-08-09T12:02:57.766076: step 25145, loss 0.614172.
Train: 2018-08-09T12:02:57.875426: step 25146, loss 0.580553.
Train: 2018-08-09T12:02:57.969151: step 25147, loss 0.593689.
Train: 2018-08-09T12:02:58.078471: step 25148, loss 0.525618.
Train: 2018-08-09T12:02:58.172198: step 25149, loss 0.628592.
Train: 2018-08-09T12:02:58.281549: step 25150, loss 0.561661.
Test: 2018-08-09T12:02:58.765810: step 25150, loss 0.55052.
Train: 2018-08-09T12:02:58.875158: step 25151, loss 0.512658.
Train: 2018-08-09T12:02:58.971280: step 25152, loss 0.583804.
Train: 2018-08-09T12:02:59.080629: step 25153, loss 0.543311.
Train: 2018-08-09T12:02:59.174357: step 25154, loss 0.508803.
Train: 2018-08-09T12:02:59.268085: step 25155, loss 0.686313.
Train: 2018-08-09T12:02:59.377435: step 25156, loss 0.510685.
Train: 2018-08-09T12:02:59.471163: step 25157, loss 0.49392.
Train: 2018-08-09T12:02:59.580512: step 25158, loss 0.509689.
Train: 2018-08-09T12:02:59.674211: step 25159, loss 0.562255.
Train: 2018-08-09T12:02:59.783591: step 25160, loss 0.579045.
Test: 2018-08-09T12:03:00.283450: step 25160, loss 0.546214.
Train: 2018-08-09T12:03:00.377170: step 25161, loss 0.527926.
Train: 2018-08-09T12:03:00.470897: step 25162, loss 0.544825.
Train: 2018-08-09T12:03:00.580277: step 25163, loss 0.665406.
Train: 2018-08-09T12:03:00.674004: step 25164, loss 0.613145.
Train: 2018-08-09T12:03:00.783354: step 25165, loss 0.511552.
Train: 2018-08-09T12:03:00.892672: step 25166, loss 0.630736.
Train: 2018-08-09T12:03:00.988728: step 25167, loss 0.579581.
Train: 2018-08-09T12:03:01.082488: step 25168, loss 0.56235.
Train: 2018-08-09T12:03:01.191835: step 25169, loss 0.545669.
Train: 2018-08-09T12:03:01.301188: step 25170, loss 0.579153.
Test: 2018-08-09T12:03:01.785416: step 25170, loss 0.546765.
Train: 2018-08-09T12:03:01.894794: step 25171, loss 0.52893.
Train: 2018-08-09T12:03:01.988523: step 25172, loss 0.595802.
Train: 2018-08-09T12:03:02.097871: step 25173, loss 0.445995.
Train: 2018-08-09T12:03:02.191599: step 25174, loss 0.562447.
Train: 2018-08-09T12:03:02.300948: step 25175, loss 0.595687.
Train: 2018-08-09T12:03:02.394676: step 25176, loss 0.496013.
Train: 2018-08-09T12:03:02.503996: step 25177, loss 0.56246.
Train: 2018-08-09T12:03:02.597753: step 25178, loss 0.6123.
Train: 2018-08-09T12:03:02.707105: step 25179, loss 0.462786.
Train: 2018-08-09T12:03:02.800831: step 25180, loss 0.612386.
Test: 2018-08-09T12:03:03.303029: step 25180, loss 0.550642.
Train: 2018-08-09T12:03:03.412379: step 25181, loss 0.595685.
Train: 2018-08-09T12:03:03.506107: step 25182, loss 0.462744.
Train: 2018-08-09T12:03:03.615456: step 25183, loss 0.595723.
Train: 2018-08-09T12:03:03.709183: step 25184, loss 0.545756.
Train: 2018-08-09T12:03:03.818563: step 25185, loss 0.529026.
Train: 2018-08-09T12:03:03.912290: step 25186, loss 0.679234.
Train: 2018-08-09T12:03:04.021641: step 25187, loss 0.628898.
Train: 2018-08-09T12:03:04.115378: step 25188, loss 0.562633.
Train: 2018-08-09T12:03:04.224717: step 25189, loss 0.529438.
Train: 2018-08-09T12:03:04.334069: step 25190, loss 0.546018.
Test: 2018-08-09T12:03:04.818298: step 25190, loss 0.547595.
Train: 2018-08-09T12:03:04.929095: step 25191, loss 0.595665.
Train: 2018-08-09T12:03:05.022852: step 25192, loss 0.661647.
Train: 2018-08-09T12:03:05.132201: step 25193, loss 0.727221.
Train: 2018-08-09T12:03:05.241550: step 25194, loss 0.562554.
Train: 2018-08-09T12:03:05.335279: step 25195, loss 0.644195.
Train: 2018-08-09T12:03:05.444627: step 25196, loss 0.530201.
Train: 2018-08-09T12:03:05.538355: step 25197, loss 0.546546.
Train: 2018-08-09T12:03:05.647675: step 25198, loss 0.498285.
Train: 2018-08-09T12:03:05.757054: step 25199, loss 0.482355.
Train: 2018-08-09T12:03:05.850782: step 25200, loss 0.627101.
Test: 2018-08-09T12:03:06.345281: step 25200, loss 0.548989.
Train: 2018-08-09T12:03:06.892057: step 25201, loss 0.53071.
Train: 2018-08-09T12:03:06.992062: step 25202, loss 0.578868.
Train: 2018-08-09T12:03:07.085790: step 25203, loss 0.658998.
Train: 2018-08-09T12:03:07.195140: step 25204, loss 0.562867.
Train: 2018-08-09T12:03:07.304489: step 25205, loss 0.483081.
Train: 2018-08-09T12:03:07.398217: step 25206, loss 0.483123.
Train: 2018-08-09T12:03:07.507571: step 25207, loss 0.610803.
Train: 2018-08-09T12:03:07.616915: step 25208, loss 0.594837.
Train: 2018-08-09T12:03:07.726265: step 25209, loss 0.578865.
Train: 2018-08-09T12:03:07.819992: step 25210, loss 0.578864.
Test: 2018-08-09T12:03:08.319845: step 25210, loss 0.547361.
Train: 2018-08-09T12:03:08.429194: step 25211, loss 0.626709.
Train: 2018-08-09T12:03:08.546114: step 25212, loss 0.53107.
Train: 2018-08-09T12:03:08.649012: step 25213, loss 0.467433.
Train: 2018-08-09T12:03:08.742769: step 25214, loss 0.594788.
Train: 2018-08-09T12:03:08.867712: step 25215, loss 0.531042.
Train: 2018-08-09T12:03:08.961468: step 25216, loss 0.578849.
Train: 2018-08-09T12:03:09.070817: step 25217, loss 0.658689.
Train: 2018-08-09T12:03:09.180167: step 25218, loss 0.642692.
Train: 2018-08-09T12:03:09.289516: step 25219, loss 0.515144.
Train: 2018-08-09T12:03:09.383243: step 25220, loss 0.483378.
Test: 2018-08-09T12:03:09.883121: step 25220, loss 0.551025.
Train: 2018-08-09T12:03:09.992475: step 25221, loss 0.451466.
Train: 2018-08-09T12:03:10.101795: step 25222, loss 0.562902.
Train: 2018-08-09T12:03:10.195522: step 25223, loss 0.530806.
Train: 2018-08-09T12:03:10.304872: step 25224, loss 0.643271.
Train: 2018-08-09T12:03:10.414221: step 25225, loss 0.610878.
Train: 2018-08-09T12:03:10.523600: step 25226, loss 0.643019.
Train: 2018-08-09T12:03:10.632920: step 25227, loss 0.546746.
Train: 2018-08-09T12:03:10.726677: step 25228, loss 0.514362.
Train: 2018-08-09T12:03:10.836026: step 25229, loss 0.498391.
Train: 2018-08-09T12:03:10.945347: step 25230, loss 0.628301.
Test: 2018-08-09T12:03:11.445228: step 25230, loss 0.546906.
Train: 2018-08-09T12:03:11.554577: step 25231, loss 0.529787.
Train: 2018-08-09T12:03:11.648305: step 25232, loss 0.57809.
Train: 2018-08-09T12:03:11.757684: step 25233, loss 0.562605.
Train: 2018-08-09T12:03:11.867034: step 25234, loss 0.447707.
Train: 2018-08-09T12:03:11.978649: step 25235, loss 0.62958.
Train: 2018-08-09T12:03:12.087998: step 25236, loss 0.530655.
Train: 2018-08-09T12:03:12.181723: step 25237, loss 0.495947.
Train: 2018-08-09T12:03:12.291073: step 25238, loss 0.461707.
Train: 2018-08-09T12:03:12.400421: step 25239, loss 0.57421.
Train: 2018-08-09T12:03:12.509771: step 25240, loss 0.630904.
Test: 2018-08-09T12:03:13.009622: step 25240, loss 0.546629.
Train: 2018-08-09T12:03:13.119005: step 25241, loss 0.563971.
Train: 2018-08-09T12:03:13.212699: step 25242, loss 0.669968.
Train: 2018-08-09T12:03:13.322080: step 25243, loss 0.495271.
Train: 2018-08-09T12:03:13.431428: step 25244, loss 0.528541.
Train: 2018-08-09T12:03:13.540777: step 25245, loss 0.511201.
Train: 2018-08-09T12:03:13.650127: step 25246, loss 0.547499.
Train: 2018-08-09T12:03:13.759476: step 25247, loss 0.477478.
Train: 2018-08-09T12:03:13.853207: step 25248, loss 0.494994.
Train: 2018-08-09T12:03:13.964923: step 25249, loss 0.578499.
Train: 2018-08-09T12:03:14.074272: step 25250, loss 0.559191.
Test: 2018-08-09T12:03:14.574125: step 25250, loss 0.546131.
Train: 2018-08-09T12:03:14.683473: step 25251, loss 0.597603.
Train: 2018-08-09T12:03:14.777201: step 25252, loss 0.492892.
Train: 2018-08-09T12:03:14.886550: step 25253, loss 0.684488.
Train: 2018-08-09T12:03:14.995900: step 25254, loss 0.474005.
Train: 2018-08-09T12:03:15.105279: step 25255, loss 0.473882.
Train: 2018-08-09T12:03:15.214629: step 25256, loss 0.525699.
Train: 2018-08-09T12:03:15.323977: step 25257, loss 0.525393.
Train: 2018-08-09T12:03:15.433327: step 25258, loss 0.613721.
Train: 2018-08-09T12:03:15.542677: step 25259, loss 0.578417.
Train: 2018-08-09T12:03:15.652021: step 25260, loss 0.532042.
Test: 2018-08-09T12:03:16.138639: step 25260, loss 0.547188.
Train: 2018-08-09T12:03:16.248017: step 25261, loss 0.680174.
Train: 2018-08-09T12:03:16.357367: step 25262, loss 0.547085.
Train: 2018-08-09T12:03:16.466716: step 25263, loss 0.615688.
Train: 2018-08-09T12:03:16.576064: step 25264, loss 0.524164.
Train: 2018-08-09T12:03:16.685418: step 25265, loss 0.597718.
Train: 2018-08-09T12:03:16.779112: step 25266, loss 0.601576.
Train: 2018-08-09T12:03:16.888491: step 25267, loss 0.54261.
Train: 2018-08-09T12:03:16.997840: step 25268, loss 0.577575.
Train: 2018-08-09T12:03:17.107189: step 25269, loss 0.632666.
Train: 2018-08-09T12:03:17.216538: step 25270, loss 0.525954.
Test: 2018-08-09T12:03:17.716393: step 25270, loss 0.547494.
Train: 2018-08-09T12:03:17.825740: step 25271, loss 0.628752.
Train: 2018-08-09T12:03:17.936465: step 25272, loss 0.643276.
Train: 2018-08-09T12:03:18.045817: step 25273, loss 0.624858.
Train: 2018-08-09T12:03:18.155164: step 25274, loss 0.644273.
Train: 2018-08-09T12:03:18.264514: step 25275, loss 0.593141.
Train: 2018-08-09T12:03:18.358243: step 25276, loss 0.577217.
Train: 2018-08-09T12:03:18.467590: step 25277, loss 0.563857.
Train: 2018-08-09T12:03:18.576909: step 25278, loss 0.579355.
Train: 2018-08-09T12:03:18.686289: step 25279, loss 0.54871.
Train: 2018-08-09T12:03:18.795608: step 25280, loss 0.609574.
Test: 2018-08-09T12:03:19.295492: step 25280, loss 0.550352.
Train: 2018-08-09T12:03:19.404840: step 25281, loss 0.42198.
Train: 2018-08-09T12:03:19.514219: step 25282, loss 0.548261.
Train: 2018-08-09T12:03:19.623538: step 25283, loss 0.610322.
Train: 2018-08-09T12:03:19.732917: step 25284, loss 0.579218.
Train: 2018-08-09T12:03:19.842266: step 25285, loss 0.562881.
Train: 2018-08-09T12:03:19.952324: step 25286, loss 0.5322.
Train: 2018-08-09T12:03:20.061703: step 25287, loss 0.625655.
Train: 2018-08-09T12:03:20.171052: step 25288, loss 0.470302.
Train: 2018-08-09T12:03:20.280401: step 25289, loss 0.579015.
Train: 2018-08-09T12:03:20.389750: step 25290, loss 0.625351.
Test: 2018-08-09T12:03:20.889605: step 25290, loss 0.551045.
Train: 2018-08-09T12:03:20.998983: step 25291, loss 0.578315.
Train: 2018-08-09T12:03:21.092710: step 25292, loss 0.548677.
Train: 2018-08-09T12:03:21.217681: step 25293, loss 0.533872.
Train: 2018-08-09T12:03:21.327030: step 25294, loss 0.595117.
Train: 2018-08-09T12:03:21.436379: step 25295, loss 0.594588.
Train: 2018-08-09T12:03:21.545728: step 25296, loss 0.610927.
Train: 2018-08-09T12:03:21.655078: step 25297, loss 0.532178.
Train: 2018-08-09T12:03:21.764956: step 25298, loss 0.673026.
Train: 2018-08-09T12:03:21.874319: step 25299, loss 0.641284.
Train: 2018-08-09T12:03:21.968031: step 25300, loss 0.578832.
Test: 2018-08-09T12:03:22.467914: step 25300, loss 0.551589.
Train: 2018-08-09T12:03:23.061556: step 25301, loss 0.579121.
Train: 2018-08-09T12:03:23.170905: step 25302, loss 0.548171.
Train: 2018-08-09T12:03:23.280223: step 25303, loss 0.502724.
Train: 2018-08-09T12:03:23.389606: step 25304, loss 0.609571.
Train: 2018-08-09T12:03:23.498951: step 25305, loss 0.594498.
Train: 2018-08-09T12:03:23.608301: step 25306, loss 0.594388.
Train: 2018-08-09T12:03:23.717619: step 25307, loss 0.5792.
Train: 2018-08-09T12:03:23.827001: step 25308, loss 0.624547.
Train: 2018-08-09T12:03:23.936348: step 25309, loss 0.564014.
Train: 2018-08-09T12:03:24.042590: step 25310, loss 0.518626.
Test: 2018-08-09T12:03:24.542443: step 25310, loss 0.553253.
Train: 2018-08-09T12:03:24.651822: step 25311, loss 0.533764.
Train: 2018-08-09T12:03:24.761171: step 25312, loss 0.548822.
Train: 2018-08-09T12:03:24.870521: step 25313, loss 0.669918.
Train: 2018-08-09T12:03:24.979870: step 25314, loss 0.518566.
Train: 2018-08-09T12:03:25.089189: step 25315, loss 0.503394.
Train: 2018-08-09T12:03:25.198538: step 25316, loss 0.609393.
Train: 2018-08-09T12:03:25.307918: step 25317, loss 0.670126.
Train: 2018-08-09T12:03:25.417266: step 25318, loss 0.488051.
Train: 2018-08-09T12:03:25.526618: step 25319, loss 0.579059.
Train: 2018-08-09T12:03:25.635965: step 25320, loss 0.594248.
Test: 2018-08-09T12:03:26.135818: step 25320, loss 0.550172.
Train: 2018-08-09T12:03:26.245166: step 25321, loss 0.548639.
Train: 2018-08-09T12:03:26.354516: step 25322, loss 0.609476.
Train: 2018-08-09T12:03:26.463865: step 25323, loss 0.563813.
Train: 2018-08-09T12:03:26.573244: step 25324, loss 0.441951.
Train: 2018-08-09T12:03:26.682593: step 25325, loss 0.517924.
Train: 2018-08-09T12:03:26.791947: step 25326, loss 0.517695.
Train: 2018-08-09T12:03:26.901294: step 25327, loss 0.563559.
Train: 2018-08-09T12:03:27.013017: step 25328, loss 0.609801.
Train: 2018-08-09T12:03:27.122366: step 25329, loss 0.439466.
Train: 2018-08-09T12:03:27.231716: step 25330, loss 0.516446.
Test: 2018-08-09T12:03:27.715948: step 25330, loss 0.553171.
Train: 2018-08-09T12:03:27.825326: step 25331, loss 0.563108.
Train: 2018-08-09T12:03:27.934675: step 25332, loss 0.610191.
Train: 2018-08-09T12:03:28.044025: step 25333, loss 0.562391.
Train: 2018-08-09T12:03:28.153374: step 25334, loss 0.562238.
Train: 2018-08-09T12:03:28.262723: step 25335, loss 0.417044.
Train: 2018-08-09T12:03:28.372043: step 25336, loss 0.528956.
Train: 2018-08-09T12:03:28.481421: step 25337, loss 0.514228.
Train: 2018-08-09T12:03:28.590741: step 25338, loss 0.632483.
Train: 2018-08-09T12:03:28.700119: step 25339, loss 0.513314.
Train: 2018-08-09T12:03:28.809451: step 25340, loss 0.577954.
Test: 2018-08-09T12:03:29.311661: step 25340, loss 0.544728.
Train: 2018-08-09T12:03:29.421009: step 25341, loss 0.551902.
Train: 2018-08-09T12:03:29.514767: step 25342, loss 0.602235.
Train: 2018-08-09T12:03:29.624116: step 25343, loss 0.529668.
Train: 2018-08-09T12:03:29.733465: step 25344, loss 0.712179.
Train: 2018-08-09T12:03:29.842818: step 25345, loss 0.478416.
Train: 2018-08-09T12:03:29.952164: step 25346, loss 0.546757.
Train: 2018-08-09T12:03:30.061515: step 25347, loss 0.546185.
Train: 2018-08-09T12:03:30.170833: step 25348, loss 0.661593.
Train: 2018-08-09T12:03:30.280213: step 25349, loss 0.611464.
Train: 2018-08-09T12:03:30.373940: step 25350, loss 0.481027.
Test: 2018-08-09T12:03:30.889444: step 25350, loss 0.550417.
Train: 2018-08-09T12:03:30.983834: step 25351, loss 0.595237.
Train: 2018-08-09T12:03:31.093183: step 25352, loss 0.611409.
Train: 2018-08-09T12:03:31.202532: step 25353, loss 0.48138.
Train: 2018-08-09T12:03:31.311881: step 25354, loss 0.595126.
Train: 2018-08-09T12:03:31.421201: step 25355, loss 0.578715.
Train: 2018-08-09T12:03:31.530549: step 25356, loss 0.562805.
Train: 2018-08-09T12:03:31.639899: step 25357, loss 0.546223.
Train: 2018-08-09T12:03:31.749247: step 25358, loss 0.54621.
Train: 2018-08-09T12:03:31.843009: step 25359, loss 0.546656.
Train: 2018-08-09T12:03:31.952355: step 25360, loss 0.562707.
Test: 2018-08-09T12:03:32.452209: step 25360, loss 0.547228.
Train: 2018-08-09T12:03:32.561586: step 25361, loss 0.530061.
Train: 2018-08-09T12:03:32.670939: step 25362, loss 0.545829.
Train: 2018-08-09T12:03:32.764634: step 25363, loss 0.594945.
Train: 2018-08-09T12:03:32.920880: step 25364, loss 0.52948.
Train: 2018-08-09T12:03:33.030195: step 25365, loss 0.580061.
Train: 2018-08-09T12:03:33.139576: step 25366, loss 0.563809.
Train: 2018-08-09T12:03:33.248924: step 25367, loss 0.430624.
Train: 2018-08-09T12:03:33.358275: step 25368, loss 0.479595.
Train: 2018-08-09T12:03:33.467622: step 25369, loss 0.612194.
Train: 2018-08-09T12:03:33.561351: step 25370, loss 0.56348.
Test: 2018-08-09T12:03:34.063518: step 25370, loss 0.549227.
Train: 2018-08-09T12:03:34.172897: step 25371, loss 0.511589.
Train: 2018-08-09T12:03:34.282247: step 25372, loss 0.67902.
Train: 2018-08-09T12:03:34.391599: step 25373, loss 0.577781.
Train: 2018-08-09T12:03:34.500949: step 25374, loss 0.513896.
Train: 2018-08-09T12:03:34.610295: step 25375, loss 0.579161.
Train: 2018-08-09T12:03:34.704022: step 25376, loss 0.444987.
Train: 2018-08-09T12:03:34.813372: step 25377, loss 0.57799.
Train: 2018-08-09T12:03:34.922721: step 25378, loss 0.512084.
Train: 2018-08-09T12:03:35.032070: step 25379, loss 0.613879.
Train: 2018-08-09T12:03:35.141423: step 25380, loss 0.597062.
Test: 2018-08-09T12:03:35.625653: step 25380, loss 0.549646.
Train: 2018-08-09T12:03:35.734999: step 25381, loss 0.52962.
Train: 2018-08-09T12:03:35.844378: step 25382, loss 0.460394.
Train: 2018-08-09T12:03:35.955057: step 25383, loss 0.546376.
Train: 2018-08-09T12:03:36.048785: step 25384, loss 0.493809.
Train: 2018-08-09T12:03:36.158134: step 25385, loss 0.563005.
Train: 2018-08-09T12:03:36.267483: step 25386, loss 0.594999.
Train: 2018-08-09T12:03:36.376834: step 25387, loss 0.558232.
Train: 2018-08-09T12:03:36.470561: step 25388, loss 0.562224.
Train: 2018-08-09T12:03:36.579880: step 25389, loss 0.557209.
Train: 2018-08-09T12:03:36.689229: step 25390, loss 0.47112.
Test: 2018-08-09T12:03:37.189112: step 25390, loss 0.547844.
Train: 2018-08-09T12:03:37.282839: step 25391, loss 0.549366.
Train: 2018-08-09T12:03:37.392189: step 25392, loss 0.550032.
Train: 2018-08-09T12:03:37.501569: step 25393, loss 0.557488.
Train: 2018-08-09T12:03:37.610887: step 25394, loss 0.545166.
Train: 2018-08-09T12:03:37.704646: step 25395, loss 0.619259.
Train: 2018-08-09T12:03:37.813994: step 25396, loss 0.550883.
Train: 2018-08-09T12:03:37.909173: step 25397, loss 0.524101.
Train: 2018-08-09T12:03:38.018553: step 25398, loss 0.528489.
Train: 2018-08-09T12:03:38.127871: step 25399, loss 0.531109.
Train: 2018-08-09T12:03:38.221633: step 25400, loss 0.651596.
Test: 2018-08-09T12:03:38.737103: step 25400, loss 0.548091.
Train: 2018-08-09T12:03:39.330743: step 25401, loss 0.597981.
Train: 2018-08-09T12:03:39.424471: step 25402, loss 0.578082.
Train: 2018-08-09T12:03:39.533824: step 25403, loss 0.563586.
Train: 2018-08-09T12:03:39.643140: step 25404, loss 0.646869.
Train: 2018-08-09T12:03:39.753203: step 25405, loss 0.529188.
Train: 2018-08-09T12:03:39.846900: step 25406, loss 0.512741.
Train: 2018-08-09T12:03:39.956249: step 25407, loss 0.496289.
Train: 2018-08-09T12:03:40.065629: step 25408, loss 0.496421.
Train: 2018-08-09T12:03:40.159359: step 25409, loss 0.71106.
Train: 2018-08-09T12:03:40.268677: step 25410, loss 0.611796.
Test: 2018-08-09T12:03:40.752938: step 25410, loss 0.549064.
Train: 2018-08-09T12:03:40.862316: step 25411, loss 0.464141.
Train: 2018-08-09T12:03:40.971635: step 25412, loss 0.529677.
Train: 2018-08-09T12:03:41.065394: step 25413, loss 0.595404.
Train: 2018-08-09T12:03:41.174743: step 25414, loss 0.546154.
Train: 2018-08-09T12:03:41.268474: step 25415, loss 0.529809.
Train: 2018-08-09T12:03:41.377790: step 25416, loss 0.529795.
Train: 2018-08-09T12:03:41.471548: step 25417, loss 0.496975.
Train: 2018-08-09T12:03:41.580866: step 25418, loss 0.546155.
Train: 2018-08-09T12:03:41.690217: step 25419, loss 0.612056.
Train: 2018-08-09T12:03:41.783976: step 25420, loss 0.562517.
Test: 2018-08-09T12:03:42.291253: step 25420, loss 0.548343.
Train: 2018-08-09T12:03:42.385010: step 25421, loss 0.562506.
Train: 2018-08-09T12:03:42.494360: step 25422, loss 0.57896.
Train: 2018-08-09T12:03:42.588087: step 25423, loss 0.595476.
Train: 2018-08-09T12:03:42.697438: step 25424, loss 0.562472.
Train: 2018-08-09T12:03:42.806787: step 25425, loss 0.579047.
Train: 2018-08-09T12:03:42.900514: step 25426, loss 0.578902.
Train: 2018-08-09T12:03:43.009834: step 25427, loss 0.562499.
Train: 2018-08-09T12:03:43.103595: step 25428, loss 0.546198.
Train: 2018-08-09T12:03:43.212941: step 25429, loss 0.628325.
Train: 2018-08-09T12:03:43.306668: step 25430, loss 0.595304.
Test: 2018-08-09T12:03:43.806522: step 25430, loss 0.545961.
Train: 2018-08-09T12:03:43.915900: step 25431, loss 0.562475.
Train: 2018-08-09T12:03:44.009628: step 25432, loss 0.611895.
Train: 2018-08-09T12:03:44.118977: step 25433, loss 0.513807.
Train: 2018-08-09T12:03:44.212705: step 25434, loss 0.627809.
Train: 2018-08-09T12:03:44.322055: step 25435, loss 0.578913.
Train: 2018-08-09T12:03:44.415753: step 25436, loss 0.595152.
Train: 2018-08-09T12:03:44.525132: step 25437, loss 0.627259.
Train: 2018-08-09T12:03:44.618859: step 25438, loss 0.594831.
Train: 2018-08-09T12:03:44.712587: step 25439, loss 0.642855.
Train: 2018-08-09T12:03:44.821937: step 25440, loss 0.547115.
Test: 2018-08-09T12:03:45.321789: step 25440, loss 0.551903.
Train: 2018-08-09T12:03:45.415547: step 25441, loss 0.594428.
Train: 2018-08-09T12:03:45.524866: step 25442, loss 0.56437.
Train: 2018-08-09T12:03:45.618594: step 25443, loss 0.578972.
Train: 2018-08-09T12:03:45.727973: step 25444, loss 0.516966.
Train: 2018-08-09T12:03:45.821702: step 25445, loss 0.688965.
Train: 2018-08-09T12:03:45.933328: step 25446, loss 0.595398.
Train: 2018-08-09T12:03:46.027056: step 25447, loss 0.610322.
Train: 2018-08-09T12:03:46.136406: step 25448, loss 0.516145.
Train: 2018-08-09T12:03:46.230137: step 25449, loss 0.578986.
Train: 2018-08-09T12:03:46.339483: step 25450, loss 0.61003.
Test: 2018-08-09T12:03:46.823715: step 25450, loss 0.548278.
Train: 2018-08-09T12:03:46.933094: step 25451, loss 0.578879.
Train: 2018-08-09T12:03:47.026791: step 25452, loss 0.548147.
Train: 2018-08-09T12:03:47.136171: step 25453, loss 0.640928.
Train: 2018-08-09T12:03:47.229902: step 25454, loss 0.625407.
Train: 2018-08-09T12:03:47.339218: step 25455, loss 0.517368.
Train: 2018-08-09T12:03:47.432975: step 25456, loss 0.548158.
Train: 2018-08-09T12:03:47.526703: step 25457, loss 0.65593.
Train: 2018-08-09T12:03:47.636052: step 25458, loss 0.486865.
Train: 2018-08-09T12:03:47.729784: step 25459, loss 0.609709.
Train: 2018-08-09T12:03:47.839131: step 25460, loss 0.502375.
Test: 2018-08-09T12:03:48.325651: step 25460, loss 0.551624.
Train: 2018-08-09T12:03:48.435028: step 25461, loss 0.517727.
Train: 2018-08-09T12:03:48.528726: step 25462, loss 0.64035.
Train: 2018-08-09T12:03:48.638075: step 25463, loss 0.640334.
Train: 2018-08-09T12:03:48.731834: step 25464, loss 0.578958.
Train: 2018-08-09T12:03:48.841183: step 25465, loss 0.548398.
Train: 2018-08-09T12:03:48.934910: step 25466, loss 0.563699.
Train: 2018-08-09T12:03:49.028642: step 25467, loss 0.624932.
Train: 2018-08-09T12:03:49.137959: step 25468, loss 0.563751.
Train: 2018-08-09T12:03:49.231716: step 25469, loss 0.594262.
Train: 2018-08-09T12:03:49.341066: step 25470, loss 0.502742.
Test: 2018-08-09T12:03:49.825298: step 25470, loss 0.550032.
Train: 2018-08-09T12:03:49.936915: step 25471, loss 0.609499.
Train: 2018-08-09T12:03:50.030674: step 25472, loss 0.563727.
Train: 2018-08-09T12:03:50.124401: step 25473, loss 0.533022.
Train: 2018-08-09T12:03:50.233749: step 25474, loss 0.51756.
Train: 2018-08-09T12:03:50.327477: step 25475, loss 0.578376.
Train: 2018-08-09T12:03:50.436823: step 25476, loss 0.531711.
Train: 2018-08-09T12:03:50.530525: step 25477, loss 0.614387.
Train: 2018-08-09T12:03:50.624253: step 25478, loss 0.498973.
Train: 2018-08-09T12:03:50.733631: step 25479, loss 0.578479.
Train: 2018-08-09T12:03:50.827363: step 25480, loss 0.580176.
Test: 2018-08-09T12:03:51.327212: step 25480, loss 0.550837.
Train: 2018-08-09T12:03:51.420939: step 25481, loss 0.546418.
Train: 2018-08-09T12:03:51.514667: step 25482, loss 0.515374.
Train: 2018-08-09T12:03:51.624048: step 25483, loss 0.431159.
Train: 2018-08-09T12:03:51.717775: step 25484, loss 0.495093.
Train: 2018-08-09T12:03:51.811503: step 25485, loss 0.522194.
Train: 2018-08-09T12:03:51.921471: step 25486, loss 0.527406.
Train: 2018-08-09T12:03:52.015231: step 25487, loss 0.640738.
Train: 2018-08-09T12:03:52.124549: step 25488, loss 0.553499.
Train: 2018-08-09T12:03:52.218308: step 25489, loss 0.60515.
Train: 2018-08-09T12:03:52.312035: step 25490, loss 0.599188.
Test: 2018-08-09T12:03:52.811888: step 25490, loss 0.54628.
Train: 2018-08-09T12:03:52.921266: step 25491, loss 0.609057.
Train: 2018-08-09T12:03:53.030615: step 25492, loss 0.495001.
Train: 2018-08-09T12:03:53.124343: step 25493, loss 0.530193.
Train: 2018-08-09T12:03:53.218073: step 25494, loss 0.559213.
Train: 2018-08-09T12:03:53.327390: step 25495, loss 0.498279.
Train: 2018-08-09T12:03:53.421148: step 25496, loss 0.480291.
Train: 2018-08-09T12:03:53.514847: step 25497, loss 0.459557.
Train: 2018-08-09T12:03:53.608605: step 25498, loss 0.557235.
Train: 2018-08-09T12:03:53.717923: step 25499, loss 0.560835.
Train: 2018-08-09T12:03:53.811681: step 25500, loss 0.566479.
Test: 2018-08-09T12:03:54.311565: step 25500, loss 0.547366.
Train: 2018-08-09T12:03:54.921332: step 25501, loss 0.528172.
Train: 2018-08-09T12:03:55.015060: step 25502, loss 0.442072.
Train: 2018-08-09T12:03:55.124406: step 25503, loss 0.513118.
Train: 2018-08-09T12:03:55.223310: step 25504, loss 0.563669.
Train: 2018-08-09T12:03:55.319529: step 25505, loss 0.583508.
Train: 2018-08-09T12:03:55.413287: step 25506, loss 0.543606.
Train: 2018-08-09T12:03:55.506984: step 25507, loss 0.562685.
Train: 2018-08-09T12:03:55.600742: step 25508, loss 0.528863.
Train: 2018-08-09T12:03:55.710062: step 25509, loss 0.61485.
Train: 2018-08-09T12:03:55.803820: step 25510, loss 0.491373.
Test: 2018-08-09T12:03:56.303697: step 25510, loss 0.54651.
Train: 2018-08-09T12:03:56.397424: step 25511, loss 0.65158.
Train: 2018-08-09T12:03:56.491158: step 25512, loss 0.603996.
Train: 2018-08-09T12:03:56.584856: step 25513, loss 0.629349.
Train: 2018-08-09T12:03:56.694206: step 25514, loss 0.509693.
Train: 2018-08-09T12:03:56.787964: step 25515, loss 0.442957.
Train: 2018-08-09T12:03:56.881691: step 25516, loss 0.496135.
Train: 2018-08-09T12:03:56.975413: step 25517, loss 0.561304.
Train: 2018-08-09T12:03:57.084767: step 25518, loss 0.492454.
Train: 2018-08-09T12:03:57.178466: step 25519, loss 0.495611.
Train: 2018-08-09T12:03:57.272194: step 25520, loss 0.527694.
Test: 2018-08-09T12:03:57.772076: step 25520, loss 0.547331.
Train: 2018-08-09T12:03:57.865834: step 25521, loss 0.563276.
Train: 2018-08-09T12:03:57.959532: step 25522, loss 0.651471.
Train: 2018-08-09T12:03:58.068880: step 25523, loss 0.528444.
Train: 2018-08-09T12:03:58.162609: step 25524, loss 0.529994.
Train: 2018-08-09T12:03:58.256367: step 25525, loss 0.529182.
Train: 2018-08-09T12:03:58.350095: step 25526, loss 0.596875.
Train: 2018-08-09T12:03:58.459414: step 25527, loss 0.476552.
Train: 2018-08-09T12:03:58.553172: step 25528, loss 0.547532.
Train: 2018-08-09T12:03:58.646900: step 25529, loss 0.510715.
Train: 2018-08-09T12:03:58.740628: step 25530, loss 0.528651.
Test: 2018-08-09T12:03:59.242861: step 25530, loss 0.549956.
Train: 2018-08-09T12:03:59.336590: step 25531, loss 0.47584.
Train: 2018-08-09T12:03:59.430348: step 25532, loss 0.580881.
Train: 2018-08-09T12:03:59.524044: step 25533, loss 0.528243.
Train: 2018-08-09T12:03:59.633395: step 25534, loss 0.48986.
Train: 2018-08-09T12:03:59.727122: step 25535, loss 0.614512.
Train: 2018-08-09T12:03:59.820882: step 25536, loss 0.542305.
Train: 2018-08-09T12:03:59.914577: step 25537, loss 0.56217.
Train: 2018-08-09T12:04:00.008335: step 25538, loss 0.648324.
Train: 2018-08-09T12:04:00.117685: step 25539, loss 0.509002.
Train: 2018-08-09T12:04:00.211413: step 25540, loss 0.564526.
Test: 2018-08-09T12:04:00.711265: step 25540, loss 0.548499.
Train: 2018-08-09T12:04:00.805023: step 25541, loss 0.59823.
Train: 2018-08-09T12:04:00.898755: step 25542, loss 0.561741.
Train: 2018-08-09T12:04:00.993763: step 25543, loss 0.58067.
Train: 2018-08-09T12:04:01.087496: step 25544, loss 0.613669.
Train: 2018-08-09T12:04:01.181193: step 25545, loss 0.577977.
Train: 2018-08-09T12:04:01.290573: step 25546, loss 0.529201.
Train: 2018-08-09T12:04:01.384270: step 25547, loss 0.52944.
Train: 2018-08-09T12:04:01.478028: step 25548, loss 0.528763.
Train: 2018-08-09T12:04:01.571756: step 25549, loss 0.54524.
Train: 2018-08-09T12:04:01.681101: step 25550, loss 0.562186.
Test: 2018-08-09T12:04:02.180957: step 25550, loss 0.548099.
Train: 2018-08-09T12:04:02.274715: step 25551, loss 0.646223.
Train: 2018-08-09T12:04:02.368448: step 25552, loss 0.629267.
Train: 2018-08-09T12:04:02.462171: step 25553, loss 0.545967.
Train: 2018-08-09T12:04:02.555899: step 25554, loss 0.578917.
Train: 2018-08-09T12:04:02.649628: step 25555, loss 0.464417.
Train: 2018-08-09T12:04:02.758971: step 25556, loss 0.660855.
Train: 2018-08-09T12:04:02.852675: step 25557, loss 0.693086.
Train: 2018-08-09T12:04:02.947828: step 25558, loss 0.578947.
Train: 2018-08-09T12:04:03.041557: step 25559, loss 0.433205.
Train: 2018-08-09T12:04:03.135284: step 25560, loss 0.546564.
Test: 2018-08-09T12:04:03.635136: step 25560, loss 0.549414.
Train: 2018-08-09T12:04:03.728863: step 25561, loss 0.578901.
Train: 2018-08-09T12:04:03.822625: step 25562, loss 0.530386.
Train: 2018-08-09T12:04:03.931971: step 25563, loss 0.51422.
Train: 2018-08-09T12:04:04.025701: step 25564, loss 0.611422.
Train: 2018-08-09T12:04:04.119422: step 25565, loss 0.481734.
Train: 2018-08-09T12:04:04.213125: step 25566, loss 0.643722.
Train: 2018-08-09T12:04:04.322504: step 25567, loss 0.579269.
Train: 2018-08-09T12:04:04.416234: step 25568, loss 0.675991.
Train: 2018-08-09T12:04:04.509960: step 25569, loss 0.611091.
Train: 2018-08-09T12:04:04.603658: step 25570, loss 0.594927.
Test: 2018-08-09T12:04:05.104931: step 25570, loss 0.54843.
Train: 2018-08-09T12:04:05.198689: step 25571, loss 0.530801.
Train: 2018-08-09T12:04:05.292416: step 25572, loss 0.690855.
Train: 2018-08-09T12:04:05.386145: step 25573, loss 0.594818.
Train: 2018-08-09T12:04:05.479873: step 25574, loss 0.610601.
Train: 2018-08-09T12:04:05.589224: step 25575, loss 0.610465.
Train: 2018-08-09T12:04:05.682950: step 25576, loss 0.626142.
Train: 2018-08-09T12:04:05.776677: step 25577, loss 0.578843.
Train: 2018-08-09T12:04:05.870375: step 25578, loss 0.500904.
Train: 2018-08-09T12:04:05.964102: step 25579, loss 0.59451.
Train: 2018-08-09T12:04:06.057832: step 25580, loss 0.563366.
Test: 2018-08-09T12:04:06.557714: step 25580, loss 0.552407.
Train: 2018-08-09T12:04:06.651471: step 25581, loss 0.547989.
Train: 2018-08-09T12:04:06.760821: step 25582, loss 0.594448.
Train: 2018-08-09T12:04:06.854549: step 25583, loss 0.594375.
Train: 2018-08-09T12:04:06.949627: step 25584, loss 0.548124.
Train: 2018-08-09T12:04:07.043324: step 25585, loss 0.547099.
Train: 2018-08-09T12:04:07.137084: step 25586, loss 0.563583.
Train: 2018-08-09T12:04:07.246432: step 25587, loss 0.56356.
Train: 2018-08-09T12:04:07.340161: step 25588, loss 0.62513.
Train: 2018-08-09T12:04:07.433858: step 25589, loss 0.563565.
Train: 2018-08-09T12:04:07.527586: step 25590, loss 0.625059.
Test: 2018-08-09T12:04:08.027468: step 25590, loss 0.553328.
Train: 2018-08-09T12:04:08.121225: step 25591, loss 0.57895.
Train: 2018-08-09T12:04:08.214922: step 25592, loss 0.594302.
Train: 2018-08-09T12:04:08.324274: step 25593, loss 0.594302.
Train: 2018-08-09T12:04:08.418030: step 25594, loss 0.579029.
Train: 2018-08-09T12:04:08.511760: step 25595, loss 0.548571.
Train: 2018-08-09T12:04:08.605457: step 25596, loss 0.670437.
Train: 2018-08-09T12:04:08.714806: step 25597, loss 0.579038.
Train: 2018-08-09T12:04:08.808564: step 25598, loss 0.488062.
Train: 2018-08-09T12:04:08.902291: step 25599, loss 0.60941.
Train: 2018-08-09T12:04:08.998338: step 25600, loss 0.594217.
Test: 2018-08-09T12:04:09.498197: step 25600, loss 0.550335.
Train: 2018-08-09T12:04:10.091836: step 25601, loss 0.609401.
Train: 2018-08-09T12:04:10.185564: step 25602, loss 0.503504.
Train: 2018-08-09T12:04:10.279292: step 25603, loss 0.548855.
Train: 2018-08-09T12:04:10.388641: step 25604, loss 0.503482.
Train: 2018-08-09T12:04:10.482338: step 25605, loss 0.563862.
Train: 2018-08-09T12:04:10.576100: step 25606, loss 0.548732.
Train: 2018-08-09T12:04:10.669829: step 25607, loss 0.548604.
Train: 2018-08-09T12:04:10.763522: step 25608, loss 0.533197.
Train: 2018-08-09T12:04:10.872902: step 25609, loss 0.563931.
Train: 2018-08-09T12:04:10.968874: step 25610, loss 0.625278.
Test: 2018-08-09T12:04:11.453105: step 25610, loss 0.550239.
Train: 2018-08-09T12:04:11.546862: step 25611, loss 0.656213.
Train: 2018-08-09T12:04:11.656212: step 25612, loss 0.625041.
Train: 2018-08-09T12:04:11.749939: step 25613, loss 0.409989.
Train: 2018-08-09T12:04:11.843638: step 25614, loss 0.594487.
Train: 2018-08-09T12:04:11.937395: step 25615, loss 0.50184.
Train: 2018-08-09T12:04:12.031123: step 25616, loss 0.548064.
Train: 2018-08-09T12:04:12.140467: step 25617, loss 0.578838.
Train: 2018-08-09T12:04:12.234205: step 25618, loss 0.547335.
Train: 2018-08-09T12:04:12.327928: step 25619, loss 0.563183.
Train: 2018-08-09T12:04:12.421627: step 25620, loss 0.515665.
Test: 2018-08-09T12:04:12.922380: step 25620, loss 0.548169.
Train: 2018-08-09T12:04:13.016138: step 25621, loss 0.562963.
Train: 2018-08-09T12:04:13.109837: step 25622, loss 0.562587.
Train: 2018-08-09T12:04:13.204953: step 25623, loss 0.675691.
Train: 2018-08-09T12:04:13.314333: step 25624, loss 0.660046.
Train: 2018-08-09T12:04:13.408061: step 25625, loss 0.498465.
Train: 2018-08-09T12:04:13.501788: step 25626, loss 0.562891.
Train: 2018-08-09T12:04:13.595516: step 25627, loss 0.546844.
Train: 2018-08-09T12:04:13.689244: step 25628, loss 0.706394.
Train: 2018-08-09T12:04:13.798593: step 25629, loss 0.529654.
Train: 2018-08-09T12:04:13.892330: step 25630, loss 0.57806.
Test: 2018-08-09T12:04:14.392173: step 25630, loss 0.549787.
Train: 2018-08-09T12:04:14.485932: step 25631, loss 0.563381.
Train: 2018-08-09T12:04:14.579629: step 25632, loss 0.531686.
Train: 2018-08-09T12:04:14.673387: step 25633, loss 0.515795.
Train: 2018-08-09T12:04:14.767115: step 25634, loss 0.546999.
Train: 2018-08-09T12:04:14.860848: step 25635, loss 0.530111.
Train: 2018-08-09T12:04:14.970193: step 25636, loss 0.564319.
Train: 2018-08-09T12:04:15.063891: step 25637, loss 0.564631.
Train: 2018-08-09T12:04:15.157618: step 25638, loss 0.679314.
Train: 2018-08-09T12:04:15.251381: step 25639, loss 0.516167.
Train: 2018-08-09T12:04:15.345104: step 25640, loss 0.500111.
Test: 2018-08-09T12:04:15.852058: step 25640, loss 0.548674.
Train: 2018-08-09T12:04:15.945785: step 25641, loss 0.579399.
Train: 2018-08-09T12:04:16.039519: step 25642, loss 0.642317.
Train: 2018-08-09T12:04:16.133247: step 25643, loss 0.499777.
Train: 2018-08-09T12:04:16.226943: step 25644, loss 0.610451.
Train: 2018-08-09T12:04:16.320703: step 25645, loss 0.673627.
Train: 2018-08-09T12:04:16.430051: step 25646, loss 0.563115.
Train: 2018-08-09T12:04:16.523775: step 25647, loss 0.594636.
Train: 2018-08-09T12:04:16.617507: step 25648, loss 0.563177.
Train: 2018-08-09T12:04:16.711235: step 25649, loss 0.516097.
Train: 2018-08-09T12:04:16.804963: step 25650, loss 0.500426.
Test: 2018-08-09T12:04:17.304828: step 25650, loss 0.55083.
Train: 2018-08-09T12:04:17.398544: step 25651, loss 0.516057.
Train: 2018-08-09T12:04:17.492272: step 25652, loss 0.61031.
Train: 2018-08-09T12:04:17.601651: step 25653, loss 0.468637.
Train: 2018-08-09T12:04:17.695348: step 25654, loss 0.641933.
Train: 2018-08-09T12:04:17.789107: step 25655, loss 0.578844.
Train: 2018-08-09T12:04:17.882804: step 25656, loss 0.642119.
Train: 2018-08-09T12:04:17.978950: step 25657, loss 0.483849.
Train: 2018-08-09T12:04:18.072649: step 25658, loss 0.64247.
Train: 2018-08-09T12:04:18.182028: step 25659, loss 0.578949.
Train: 2018-08-09T12:04:18.275751: step 25660, loss 0.610426.
Test: 2018-08-09T12:04:18.775608: step 25660, loss 0.552393.
Train: 2018-08-09T12:04:18.869366: step 25661, loss 0.53128.
Train: 2018-08-09T12:04:18.963095: step 25662, loss 0.547357.
Train: 2018-08-09T12:04:19.056822: step 25663, loss 0.531445.
Train: 2018-08-09T12:04:19.166172: step 25664, loss 0.578836.
Train: 2018-08-09T12:04:19.259899: step 25665, loss 0.578689.
Train: 2018-08-09T12:04:19.353597: step 25666, loss 0.642344.
Train: 2018-08-09T12:04:19.447354: step 25667, loss 0.531679.
Train: 2018-08-09T12:04:19.541077: step 25668, loss 0.547205.
Train: 2018-08-09T12:04:19.650426: step 25669, loss 0.657687.
Train: 2018-08-09T12:04:19.744160: step 25670, loss 0.5472.
Test: 2018-08-09T12:04:20.246374: step 25670, loss 0.548275.
Train: 2018-08-09T12:04:20.340101: step 25671, loss 0.547391.
Train: 2018-08-09T12:04:20.433830: step 25672, loss 0.515557.
Train: 2018-08-09T12:04:20.527588: step 25673, loss 0.48393.
Train: 2018-08-09T12:04:20.636907: step 25674, loss 0.594411.
Train: 2018-08-09T12:04:20.730664: step 25675, loss 0.467662.
Train: 2018-08-09T12:04:20.824363: step 25676, loss 0.49864.
Train: 2018-08-09T12:04:20.918089: step 25677, loss 0.642563.
Train: 2018-08-09T12:04:21.027439: step 25678, loss 0.596341.
Train: 2018-08-09T12:04:21.121167: step 25679, loss 0.578294.
Train: 2018-08-09T12:04:21.214925: step 25680, loss 0.531921.
Test: 2018-08-09T12:04:21.714778: step 25680, loss 0.547275.
Train: 2018-08-09T12:04:21.808505: step 25681, loss 0.512637.
Train: 2018-08-09T12:04:21.902263: step 25682, loss 0.644316.
Train: 2018-08-09T12:04:22.014005: step 25683, loss 0.497696.
Train: 2018-08-09T12:04:22.107733: step 25684, loss 0.544404.
Train: 2018-08-09T12:04:22.201460: step 25685, loss 0.578372.
Train: 2018-08-09T12:04:22.295159: step 25686, loss 0.531872.
Train: 2018-08-09T12:04:22.388917: step 25687, loss 0.528164.
Train: 2018-08-09T12:04:22.498268: step 25688, loss 0.630642.
Train: 2018-08-09T12:04:22.591963: step 25689, loss 0.528268.
Train: 2018-08-09T12:04:22.685691: step 25690, loss 0.698869.
Test: 2018-08-09T12:04:23.185575: step 25690, loss 0.550681.
Train: 2018-08-09T12:04:23.279332: step 25691, loss 0.464444.
Train: 2018-08-09T12:04:23.373061: step 25692, loss 0.514018.
Train: 2018-08-09T12:04:23.482411: step 25693, loss 0.562742.
Train: 2018-08-09T12:04:23.576139: step 25694, loss 0.563522.
Train: 2018-08-09T12:04:23.669867: step 25695, loss 0.628756.
Train: 2018-08-09T12:04:23.779184: step 25696, loss 0.463436.
Train: 2018-08-09T12:04:23.872913: step 25697, loss 0.629486.
Train: 2018-08-09T12:04:23.968018: step 25698, loss 0.595302.
Train: 2018-08-09T12:04:24.061746: step 25699, loss 0.612054.
Train: 2018-08-09T12:04:24.171097: step 25700, loss 0.546789.
Test: 2018-08-09T12:04:24.655326: step 25700, loss 0.546356.
Train: 2018-08-09T12:04:25.236036: step 25701, loss 0.546593.
Train: 2018-08-09T12:04:25.345385: step 25702, loss 0.578779.
Train: 2018-08-09T12:04:25.439112: step 25703, loss 0.643348.
Train: 2018-08-09T12:04:25.532870: step 25704, loss 0.482342.
Train: 2018-08-09T12:04:25.626599: step 25705, loss 0.578873.
Train: 2018-08-09T12:04:25.735924: step 25706, loss 0.578872.
Train: 2018-08-09T12:04:25.829670: step 25707, loss 0.530669.
Train: 2018-08-09T12:04:25.937646: step 25708, loss 0.498532.
Train: 2018-08-09T12:04:26.031374: step 25709, loss 0.530624.
Train: 2018-08-09T12:04:26.125072: step 25710, loss 0.611099.
Test: 2018-08-09T12:04:26.624954: step 25710, loss 0.548881.
Train: 2018-08-09T12:04:26.718712: step 25711, loss 0.546658.
Train: 2018-08-09T12:04:26.828061: step 25712, loss 0.562753.
Train: 2018-08-09T12:04:26.921759: step 25713, loss 0.643441.
Train: 2018-08-09T12:04:27.015521: step 25714, loss 0.546619.
Train: 2018-08-09T12:04:27.124837: step 25715, loss 0.562752.
Train: 2018-08-09T12:04:27.218595: step 25716, loss 0.482113.
Train: 2018-08-09T12:04:27.312325: step 25717, loss 0.611176.
Train: 2018-08-09T12:04:27.421671: step 25718, loss 0.449676.
Train: 2018-08-09T12:04:27.515400: step 25719, loss 0.546532.
Train: 2018-08-09T12:04:27.609098: step 25720, loss 0.546476.
Test: 2018-08-09T12:04:28.109000: step 25720, loss 0.548053.
Train: 2018-08-09T12:04:28.202708: step 25721, loss 0.546416.
Train: 2018-08-09T12:04:28.312087: step 25722, loss 0.578917.
Train: 2018-08-09T12:04:28.405815: step 25723, loss 0.513692.
Train: 2018-08-09T12:04:28.515160: step 25724, loss 0.513566.
Train: 2018-08-09T12:04:28.608894: step 25725, loss 0.562574.
Train: 2018-08-09T12:04:28.702619: step 25726, loss 0.578964.
Train: 2018-08-09T12:04:28.816451: step 25727, loss 0.529612.
Train: 2018-08-09T12:04:28.901691: step 25728, loss 0.628461.
Train: 2018-08-09T12:04:29.011041: step 25729, loss 0.562477.
Train: 2018-08-09T12:04:29.104769: step 25730, loss 0.529429.
Test: 2018-08-09T12:04:29.604622: step 25730, loss 0.548219.
Train: 2018-08-09T12:04:29.698379: step 25731, loss 0.579178.
Train: 2018-08-09T12:04:29.807730: step 25732, loss 0.479649.
Train: 2018-08-09T12:04:29.901456: step 25733, loss 0.645403.
Train: 2018-08-09T12:04:30.010805: step 25734, loss 0.545776.
Train: 2018-08-09T12:04:30.104502: step 25735, loss 0.529296.
Train: 2018-08-09T12:04:30.198261: step 25736, loss 0.595655.
Train: 2018-08-09T12:04:30.307613: step 25737, loss 0.545841.
Train: 2018-08-09T12:04:30.401342: step 25738, loss 0.595856.
Train: 2018-08-09T12:04:30.510688: step 25739, loss 0.579076.
Train: 2018-08-09T12:04:30.604414: step 25740, loss 0.529286.
Test: 2018-08-09T12:04:31.105705: step 25740, loss 0.547514.
Train: 2018-08-09T12:04:31.199462: step 25741, loss 0.678808.
Train: 2018-08-09T12:04:31.308812: step 25742, loss 0.628785.
Train: 2018-08-09T12:04:31.402510: step 25743, loss 0.628637.
Train: 2018-08-09T12:04:31.511859: step 25744, loss 0.546027.
Train: 2018-08-09T12:04:31.605617: step 25745, loss 0.49676.
Train: 2018-08-09T12:04:31.699345: step 25746, loss 0.59539.
Train: 2018-08-09T12:04:31.808663: step 25747, loss 0.562568.
Train: 2018-08-09T12:04:31.918043: step 25748, loss 0.562584.
Train: 2018-08-09T12:04:32.011765: step 25749, loss 0.529919.
Train: 2018-08-09T12:04:32.121115: step 25750, loss 0.62791.
Test: 2018-08-09T12:04:32.605377: step 25750, loss 0.54674.
Train: 2018-08-09T12:04:32.714726: step 25751, loss 0.546329.
Train: 2018-08-09T12:04:32.808453: step 25752, loss 0.464984.
Train: 2018-08-09T12:04:32.920140: step 25753, loss 0.595198.
Train: 2018-08-09T12:04:33.013899: step 25754, loss 0.546368.
Train: 2018-08-09T12:04:33.123248: step 25755, loss 0.595192.
Train: 2018-08-09T12:04:33.216976: step 25756, loss 0.546381.
Train: 2018-08-09T12:04:33.326325: step 25757, loss 0.56265.
Train: 2018-08-09T12:04:33.420022: step 25758, loss 0.546391.
Train: 2018-08-09T12:04:33.529403: step 25759, loss 0.595176.
Train: 2018-08-09T12:04:33.623124: step 25760, loss 0.59517.
Test: 2018-08-09T12:04:34.123014: step 25760, loss 0.549284.
Train: 2018-08-09T12:04:34.216741: step 25761, loss 0.643891.
Train: 2018-08-09T12:04:34.326089: step 25762, loss 0.595117.
Train: 2018-08-09T12:04:34.435443: step 25763, loss 0.627434.
Train: 2018-08-09T12:04:34.529167: step 25764, loss 0.482059.
Train: 2018-08-09T12:04:34.622895: step 25765, loss 0.482176.
Train: 2018-08-09T12:04:34.732244: step 25766, loss 0.611126.
Train: 2018-08-09T12:04:34.841588: step 25767, loss 0.578878.
Train: 2018-08-09T12:04:34.936812: step 25768, loss 0.562772.
Train: 2018-08-09T12:04:35.046162: step 25769, loss 0.49844.
Train: 2018-08-09T12:04:35.139889: step 25770, loss 0.498382.
Test: 2018-08-09T12:04:35.639742: step 25770, loss 0.548245.
Train: 2018-08-09T12:04:35.749116: step 25771, loss 0.643407.
Train: 2018-08-09T12:04:35.842850: step 25772, loss 0.594964.
Train: 2018-08-09T12:04:35.952169: step 25773, loss 0.49823.
Train: 2018-08-09T12:04:36.045897: step 25774, loss 0.594962.
Train: 2018-08-09T12:04:36.155275: step 25775, loss 0.546504.
Train: 2018-08-09T12:04:36.248974: step 25776, loss 0.546574.
Train: 2018-08-09T12:04:36.358353: step 25777, loss 0.546537.
Train: 2018-08-09T12:04:36.467699: step 25778, loss 0.594335.
Train: 2018-08-09T12:04:36.561430: step 25779, loss 0.545467.
Train: 2018-08-09T12:04:36.670778: step 25780, loss 0.46112.
Test: 2018-08-09T12:04:37.155719: step 25780, loss 0.547327.
Train: 2018-08-09T12:04:37.265074: step 25781, loss 0.57953.
Train: 2018-08-09T12:04:37.358796: step 25782, loss 0.564107.
Train: 2018-08-09T12:04:37.468145: step 25783, loss 0.498928.
Train: 2018-08-09T12:04:37.577499: step 25784, loss 0.582819.
Train: 2018-08-09T12:04:37.671228: step 25785, loss 0.507632.
Train: 2018-08-09T12:04:37.780546: step 25786, loss 0.589873.
Train: 2018-08-09T12:04:37.889926: step 25787, loss 0.652441.
Train: 2018-08-09T12:04:37.983654: step 25788, loss 0.527911.
Train: 2018-08-09T12:04:38.092974: step 25789, loss 0.531295.
Train: 2018-08-09T12:04:38.186731: step 25790, loss 0.530814.
Test: 2018-08-09T12:04:38.686585: step 25790, loss 0.550115.
Train: 2018-08-09T12:04:38.795963: step 25791, loss 0.634184.
Train: 2018-08-09T12:04:38.905319: step 25792, loss 0.544959.
Train: 2018-08-09T12:04:38.999040: step 25793, loss 0.646534.
Train: 2018-08-09T12:04:39.108389: step 25794, loss 0.612872.
Train: 2018-08-09T12:04:39.202116: step 25795, loss 0.47806.
Train: 2018-08-09T12:04:39.311436: step 25796, loss 0.661398.
Train: 2018-08-09T12:04:39.420816: step 25797, loss 0.496935.
Train: 2018-08-09T12:04:39.514543: step 25798, loss 0.447202.
Train: 2018-08-09T12:04:39.623863: step 25799, loss 0.480866.
Train: 2018-08-09T12:04:39.733242: step 25800, loss 0.562616.
Test: 2018-08-09T12:04:40.219826: step 25800, loss 0.549509.
Train: 2018-08-09T12:04:40.797817: step 25801, loss 0.512831.
Train: 2018-08-09T12:04:40.907166: step 25802, loss 0.529361.
Train: 2018-08-09T12:04:41.016511: step 25803, loss 0.479677.
Train: 2018-08-09T12:04:41.125865: step 25804, loss 0.632075.
Train: 2018-08-09T12:04:41.219563: step 25805, loss 0.562146.
Train: 2018-08-09T12:04:41.328942: step 25806, loss 0.495551.
Train: 2018-08-09T12:04:41.438292: step 25807, loss 0.493716.
Train: 2018-08-09T12:04:41.532020: step 25808, loss 0.582258.
Train: 2018-08-09T12:04:41.641373: step 25809, loss 0.547478.
Train: 2018-08-09T12:04:41.750718: step 25810, loss 0.474988.
Test: 2018-08-09T12:04:42.237341: step 25810, loss 0.546749.
Train: 2018-08-09T12:04:42.346687: step 25811, loss 0.564788.
Train: 2018-08-09T12:04:42.456028: step 25812, loss 0.528021.
Train: 2018-08-09T12:04:42.565382: step 25813, loss 0.492938.
Train: 2018-08-09T12:04:42.659111: step 25814, loss 0.529849.
Train: 2018-08-09T12:04:42.768430: step 25815, loss 0.525272.
Train: 2018-08-09T12:04:42.877810: step 25816, loss 0.494786.
Train: 2018-08-09T12:04:42.987175: step 25817, loss 0.508658.
Train: 2018-08-09T12:04:43.096478: step 25818, loss 0.511789.
Train: 2018-08-09T12:04:43.190236: step 25819, loss 0.673798.
Train: 2018-08-09T12:04:43.299585: step 25820, loss 0.652221.
Test: 2018-08-09T12:04:43.799462: step 25820, loss 0.546408.
Train: 2018-08-09T12:04:43.910037: step 25821, loss 0.435524.
Train: 2018-08-09T12:04:44.004920: step 25822, loss 0.596224.
Train: 2018-08-09T12:04:44.114246: step 25823, loss 0.58.
Train: 2018-08-09T12:04:44.223625: step 25824, loss 0.561487.
Train: 2018-08-09T12:04:44.332974: step 25825, loss 0.691517.
Train: 2018-08-09T12:04:44.442293: step 25826, loss 0.528768.
Train: 2018-08-09T12:04:44.551672: step 25827, loss 0.630336.
Train: 2018-08-09T12:04:44.661022: step 25828, loss 0.476362.
Train: 2018-08-09T12:04:44.754719: step 25829, loss 0.54529.
Train: 2018-08-09T12:04:44.864069: step 25830, loss 0.56233.
Test: 2018-08-09T12:04:45.363951: step 25830, loss 0.551011.
Train: 2018-08-09T12:04:45.473331: step 25831, loss 0.629352.
Train: 2018-08-09T12:04:45.582679: step 25832, loss 0.562728.
Train: 2018-08-09T12:04:45.676407: step 25833, loss 0.613551.
Train: 2018-08-09T12:04:45.785757: step 25834, loss 0.545517.
Train: 2018-08-09T12:04:45.895107: step 25835, loss 0.562461.
Train: 2018-08-09T12:04:46.005745: step 25836, loss 0.612248.
Train: 2018-08-09T12:04:46.115096: step 25837, loss 0.578926.
Train: 2018-08-09T12:04:46.224446: step 25838, loss 0.579005.
Train: 2018-08-09T12:04:46.318204: step 25839, loss 0.497013.
Train: 2018-08-09T12:04:46.427521: step 25840, loss 0.595255.
Test: 2018-08-09T12:04:46.935032: step 25840, loss 0.549157.
Train: 2018-08-09T12:04:47.044400: step 25841, loss 0.644212.
Train: 2018-08-09T12:04:47.138133: step 25842, loss 0.562595.
Train: 2018-08-09T12:04:47.247485: step 25843, loss 0.562682.
Train: 2018-08-09T12:04:47.356833: step 25844, loss 0.530295.
Train: 2018-08-09T12:04:47.466152: step 25845, loss 0.659765.
Train: 2018-08-09T12:04:47.575501: step 25846, loss 0.611145.
Train: 2018-08-09T12:04:47.684879: step 25847, loss 0.562793.
Train: 2018-08-09T12:04:47.794231: step 25848, loss 0.482652.
Train: 2018-08-09T12:04:47.903548: step 25849, loss 0.530824.
Train: 2018-08-09T12:04:48.012898: step 25850, loss 0.658899.
Test: 2018-08-09T12:04:48.512788: step 25850, loss 0.550936.
Train: 2018-08-09T12:04:48.606507: step 25851, loss 0.546911.
Train: 2018-08-09T12:04:48.715888: step 25852, loss 0.499088.
Train: 2018-08-09T12:04:48.825238: step 25853, loss 0.546959.
Train: 2018-08-09T12:04:48.934556: step 25854, loss 0.594812.
Train: 2018-08-09T12:04:49.052036: step 25855, loss 0.49913.
Train: 2018-08-09T12:04:49.161416: step 25856, loss 0.53099.
Train: 2018-08-09T12:04:49.255147: step 25857, loss 0.48301.
Train: 2018-08-09T12:04:49.364464: step 25858, loss 0.594875.
Train: 2018-08-09T12:04:49.473812: step 25859, loss 0.626983.
Train: 2018-08-09T12:04:49.583192: step 25860, loss 0.530729.
Test: 2018-08-09T12:04:50.091313: step 25860, loss 0.550794.
Train: 2018-08-09T12:04:50.200686: step 25861, loss 0.498553.
Train: 2018-08-09T12:04:50.310035: step 25862, loss 0.61106.
Train: 2018-08-09T12:04:50.403770: step 25863, loss 0.594993.
Train: 2018-08-09T12:04:50.513117: step 25864, loss 0.578873.
Train: 2018-08-09T12:04:50.622468: step 25865, loss 0.466007.
Train: 2018-08-09T12:04:50.731787: step 25866, loss 0.595019.
Train: 2018-08-09T12:04:50.841136: step 25867, loss 0.514191.
Train: 2018-08-09T12:04:50.950516: step 25868, loss 0.546359.
Train: 2018-08-09T12:04:51.059864: step 25869, loss 0.546223.
Train: 2018-08-09T12:04:51.169183: step 25870, loss 0.52954.
Test: 2018-08-09T12:04:51.669067: step 25870, loss 0.550211.
Train: 2018-08-09T12:04:51.778414: step 25871, loss 0.54581.
Train: 2018-08-09T12:04:51.887788: step 25872, loss 0.563393.
Train: 2018-08-09T12:04:51.981522: step 25873, loss 0.545339.
Train: 2018-08-09T12:04:52.106463: step 25874, loss 0.562632.
Train: 2018-08-09T12:04:52.215842: step 25875, loss 0.562019.
Train: 2018-08-09T12:04:52.309570: step 25876, loss 0.630362.
Train: 2018-08-09T12:04:52.434541: step 25877, loss 0.660479.
Train: 2018-08-09T12:04:52.528267: step 25878, loss 0.56422.
Train: 2018-08-09T12:04:52.637617: step 25879, loss 0.527936.
Train: 2018-08-09T12:04:52.746967: step 25880, loss 0.613031.
Test: 2018-08-09T12:04:53.248318: step 25880, loss 0.549999.
Train: 2018-08-09T12:04:53.357696: step 25881, loss 0.530605.
Train: 2018-08-09T12:04:53.467015: step 25882, loss 0.462875.
Train: 2018-08-09T12:04:53.576365: step 25883, loss 0.529.
Train: 2018-08-09T12:04:53.685745: step 25884, loss 0.562621.
Train: 2018-08-09T12:04:53.795093: step 25885, loss 0.595441.
Train: 2018-08-09T12:04:53.888823: step 25886, loss 0.580373.
Train: 2018-08-09T12:04:54.013794: step 25887, loss 0.613084.
Train: 2018-08-09T12:04:54.107522: step 25888, loss 0.561458.
Train: 2018-08-09T12:04:54.216869: step 25889, loss 0.545978.
Train: 2018-08-09T12:04:54.326218: step 25890, loss 0.57921.
Test: 2018-08-09T12:04:54.826095: step 25890, loss 0.548433.
Train: 2018-08-09T12:04:54.937723: step 25891, loss 0.661028.
Train: 2018-08-09T12:04:55.047105: step 25892, loss 0.595149.
Train: 2018-08-09T12:04:55.156423: step 25893, loss 0.595345.
Train: 2018-08-09T12:04:55.265803: step 25894, loss 0.627597.
Train: 2018-08-09T12:04:55.375151: step 25895, loss 0.514225.
Train: 2018-08-09T12:04:55.484501: step 25896, loss 0.611114.
Train: 2018-08-09T12:04:55.593849: step 25897, loss 0.723457.
Train: 2018-08-09T12:04:55.705914: step 25898, loss 0.530879.
Train: 2018-08-09T12:04:55.815230: step 25899, loss 0.483253.
Train: 2018-08-09T12:04:55.924610: step 25900, loss 0.594768.
Test: 2018-08-09T12:04:56.408841: step 25900, loss 0.549923.
Train: 2018-08-09T12:04:57.048991: step 25901, loss 0.451923.
Train: 2018-08-09T12:04:57.158341: step 25902, loss 0.689916.
Train: 2018-08-09T12:04:57.267689: step 25903, loss 0.594683.
Train: 2018-08-09T12:04:57.361417: step 25904, loss 0.673647.
Train: 2018-08-09T12:04:57.470767: step 25905, loss 0.531652.
Train: 2018-08-09T12:04:57.580086: step 25906, loss 0.563176.
Train: 2018-08-09T12:04:57.705061: step 25907, loss 0.563216.
Train: 2018-08-09T12:04:57.798815: step 25908, loss 0.500721.
Train: 2018-08-09T12:04:57.908164: step 25909, loss 0.688246.
Train: 2018-08-09T12:04:58.017513: step 25910, loss 0.500963.
Test: 2018-08-09T12:04:58.517366: step 25910, loss 0.549328.
Train: 2018-08-09T12:04:58.626745: step 25911, loss 0.563325.
Train: 2018-08-09T12:04:58.736094: step 25912, loss 0.578909.
Train: 2018-08-09T12:04:58.845443: step 25913, loss 0.656641.
Train: 2018-08-09T12:04:58.954793: step 25914, loss 0.501331.
Train: 2018-08-09T12:04:59.064112: step 25915, loss 0.702972.
Train: 2018-08-09T12:04:59.173491: step 25916, loss 0.517054.
Train: 2018-08-09T12:04:59.282840: step 25917, loss 0.517159.
Train: 2018-08-09T12:04:59.392192: step 25918, loss 0.578946.
Train: 2018-08-09T12:04:59.501508: step 25919, loss 0.548073.
Train: 2018-08-09T12:04:59.610887: step 25920, loss 0.609814.
Test: 2018-08-09T12:05:00.096519: step 25920, loss 0.549064.
Train: 2018-08-09T12:05:00.205874: step 25921, loss 0.532664.
Train: 2018-08-09T12:05:00.315223: step 25922, loss 0.563518.
Train: 2018-08-09T12:05:00.424542: step 25923, loss 0.609806.
Train: 2018-08-09T12:05:00.533890: step 25924, loss 0.578942.
Train: 2018-08-09T12:05:00.643240: step 25925, loss 0.578945.
Train: 2018-08-09T12:05:00.752619: step 25926, loss 0.455557.
Train: 2018-08-09T12:05:00.861969: step 25927, loss 0.702528.
Train: 2018-08-09T12:05:00.971287: step 25928, loss 0.532601.
Train: 2018-08-09T12:05:01.065015: step 25929, loss 0.532601.
Train: 2018-08-09T12:05:01.174395: step 25930, loss 0.548024.
Test: 2018-08-09T12:05:01.674273: step 25930, loss 0.552455.
Train: 2018-08-09T12:05:01.783627: step 25931, loss 0.625388.
Train: 2018-08-09T12:05:01.892976: step 25932, loss 0.656314.
Train: 2018-08-09T12:05:02.003746: step 25933, loss 0.60987.
Train: 2018-08-09T12:05:02.113095: step 25934, loss 0.578951.
Train: 2018-08-09T12:05:02.222445: step 25935, loss 0.625176.
Train: 2018-08-09T12:05:02.316173: step 25936, loss 0.502054.
Train: 2018-08-09T12:05:02.434053: step 25937, loss 0.625094.
Train: 2018-08-09T12:05:02.543433: step 25938, loss 0.594322.
Train: 2018-08-09T12:05:02.652752: step 25939, loss 0.502377.
Train: 2018-08-09T12:05:02.762133: step 25940, loss 0.594308.
Test: 2018-08-09T12:05:03.261984: step 25940, loss 0.551073.
Train: 2018-08-09T12:05:03.371363: step 25941, loss 0.624925.
Train: 2018-08-09T12:05:03.465092: step 25942, loss 0.533123.
Train: 2018-08-09T12:05:03.574440: step 25943, loss 0.609585.
Train: 2018-08-09T12:05:03.683793: step 25944, loss 0.517917.
Train: 2018-08-09T12:05:03.793139: step 25945, loss 0.533181.
Train: 2018-08-09T12:05:03.902489: step 25946, loss 0.533136.
Train: 2018-08-09T12:05:04.011838: step 25947, loss 0.57901.
Train: 2018-08-09T12:05:04.121157: step 25948, loss 0.502296.
Train: 2018-08-09T12:05:04.214915: step 25949, loss 0.563604.
Train: 2018-08-09T12:05:04.324264: step 25950, loss 0.532696.
Test: 2018-08-09T12:05:04.824116: step 25950, loss 0.550115.
Train: 2018-08-09T12:05:04.933490: step 25951, loss 0.563344.
Train: 2018-08-09T12:05:05.042814: step 25952, loss 0.656623.
Train: 2018-08-09T12:05:05.152194: step 25953, loss 0.563445.
Train: 2018-08-09T12:05:05.261543: step 25954, loss 0.470031.
Train: 2018-08-09T12:05:05.370895: step 25955, loss 0.594379.
Train: 2018-08-09T12:05:05.480212: step 25956, loss 0.548122.
Train: 2018-08-09T12:05:05.573964: step 25957, loss 0.515242.
Train: 2018-08-09T12:05:05.683318: step 25958, loss 0.516335.
Train: 2018-08-09T12:05:05.792637: step 25959, loss 0.562726.
Train: 2018-08-09T12:05:05.902017: step 25960, loss 0.513008.
Test: 2018-08-09T12:05:06.404187: step 25960, loss 0.54758.
Train: 2018-08-09T12:05:06.497942: step 25961, loss 0.562776.
Train: 2018-08-09T12:05:06.607291: step 25962, loss 0.661467.
Train: 2018-08-09T12:05:06.716641: step 25963, loss 0.575831.
Train: 2018-08-09T12:05:06.810338: step 25964, loss 0.593668.
Train: 2018-08-09T12:05:06.919687: step 25965, loss 0.566145.
Train: 2018-08-09T12:05:07.029067: step 25966, loss 0.583287.
Train: 2018-08-09T12:05:07.138387: step 25967, loss 0.578261.
Train: 2018-08-09T12:05:07.247736: step 25968, loss 0.576721.
Train: 2018-08-09T12:05:07.357116: step 25969, loss 0.641729.
Train: 2018-08-09T12:05:07.450812: step 25970, loss 0.463916.
Test: 2018-08-09T12:05:07.952974: step 25970, loss 0.550375.
Train: 2018-08-09T12:05:08.062323: step 25971, loss 0.629967.
Train: 2018-08-09T12:05:08.156082: step 25972, loss 0.594642.
Train: 2018-08-09T12:05:08.265434: step 25973, loss 0.580101.
Train: 2018-08-09T12:05:08.374780: step 25974, loss 0.498928.
Train: 2018-08-09T12:05:08.484129: step 25975, loss 0.529723.
Train: 2018-08-09T12:05:08.577857: step 25976, loss 0.545898.
Train: 2018-08-09T12:05:08.702798: step 25977, loss 0.561978.
Train: 2018-08-09T12:05:08.812171: step 25978, loss 0.580765.
Train: 2018-08-09T12:05:08.905905: step 25979, loss 0.563038.
Train: 2018-08-09T12:05:09.030846: step 25980, loss 0.482681.
Test: 2018-08-09T12:05:09.515140: step 25980, loss 0.548804.
Train: 2018-08-09T12:05:09.624456: step 25981, loss 0.51544.
Train: 2018-08-09T12:05:09.733836: step 25982, loss 0.594801.
Train: 2018-08-09T12:05:09.843180: step 25983, loss 0.562751.
Train: 2018-08-09T12:05:09.953240: step 25984, loss 0.579036.
Train: 2018-08-09T12:05:10.062559: step 25985, loss 0.463959.
Train: 2018-08-09T12:05:10.156287: step 25986, loss 0.465309.
Train: 2018-08-09T12:05:10.265666: step 25987, loss 0.57762.
Train: 2018-08-09T12:05:10.374986: step 25988, loss 0.563218.
Train: 2018-08-09T12:05:10.484335: step 25989, loss 0.480509.
Train: 2018-08-09T12:05:10.578093: step 25990, loss 0.478098.
Test: 2018-08-09T12:05:11.077954: step 25990, loss 0.546552.
Train: 2018-08-09T12:05:11.187295: step 25991, loss 0.5457.
Train: 2018-08-09T12:05:11.281053: step 25992, loss 0.615087.
Train: 2018-08-09T12:05:11.390372: step 25993, loss 0.57612.
Train: 2018-08-09T12:05:11.499750: step 25994, loss 0.624003.
Train: 2018-08-09T12:05:11.593478: step 25995, loss 0.441599.
Train: 2018-08-09T12:05:11.702828: step 25996, loss 0.521746.
Train: 2018-08-09T12:05:11.812178: step 25997, loss 0.565968.
Train: 2018-08-09T12:05:11.921526: step 25998, loss 0.510523.
Train: 2018-08-09T12:05:12.015223: step 25999, loss 0.54988.
Train: 2018-08-09T12:05:12.124604: step 26000, loss 0.469541.
Test: 2018-08-09T12:05:12.624461: step 26000, loss 0.54652.
Train: 2018-08-09T12:05:13.219350: step 26001, loss 0.501117.
Train: 2018-08-09T12:05:13.328731: step 26002, loss 0.524925.
Train: 2018-08-09T12:05:13.422459: step 26003, loss 0.640161.
Train: 2018-08-09T12:05:13.531811: step 26004, loss 0.45248.
Train: 2018-08-09T12:05:13.641157: step 26005, loss 0.494683.
Train: 2018-08-09T12:05:13.750507: step 26006, loss 0.441702.
Train: 2018-08-09T12:05:13.844234: step 26007, loss 0.694687.
Train: 2018-08-09T12:05:13.953584: step 26008, loss 0.524336.
Train: 2018-08-09T12:05:14.062902: step 26009, loss 0.477564.
Train: 2018-08-09T12:05:14.156660: step 26010, loss 0.703569.
Test: 2018-08-09T12:05:14.656514: step 26010, loss 0.547877.
Train: 2018-08-09T12:05:14.765892: step 26011, loss 0.586123.
Train: 2018-08-09T12:05:14.859589: step 26012, loss 0.531492.
Train: 2018-08-09T12:05:14.971292: step 26013, loss 0.592839.
Train: 2018-08-09T12:05:15.080671: step 26014, loss 0.494601.
Train: 2018-08-09T12:05:15.174399: step 26015, loss 0.530363.
Train: 2018-08-09T12:05:15.283718: step 26016, loss 0.648826.
Train: 2018-08-09T12:05:15.377445: step 26017, loss 0.509326.
Train: 2018-08-09T12:05:15.486795: step 26018, loss 0.593564.
Train: 2018-08-09T12:05:15.596146: step 26019, loss 0.528248.
Train: 2018-08-09T12:05:15.689902: step 26020, loss 0.479923.
Test: 2018-08-09T12:05:16.189756: step 26020, loss 0.547274.
Train: 2018-08-09T12:05:16.299105: step 26021, loss 0.578348.
Train: 2018-08-09T12:05:16.392861: step 26022, loss 0.611174.
Train: 2018-08-09T12:05:16.502211: step 26023, loss 0.527779.
Train: 2018-08-09T12:05:16.595939: step 26024, loss 0.597325.
Train: 2018-08-09T12:05:16.705258: step 26025, loss 0.529585.
Train: 2018-08-09T12:05:16.799016: step 26026, loss 0.544581.
Train: 2018-08-09T12:05:16.910698: step 26027, loss 0.545881.
Train: 2018-08-09T12:05:17.020049: step 26028, loss 0.530446.
Train: 2018-08-09T12:05:17.113776: step 26029, loss 0.578114.
Train: 2018-08-09T12:05:17.223155: step 26030, loss 0.680362.
Test: 2018-08-09T12:05:17.707387: step 26030, loss 0.548886.
Train: 2018-08-09T12:05:17.816736: step 26031, loss 0.463628.
Train: 2018-08-09T12:05:17.926115: step 26032, loss 0.579506.
Train: 2018-08-09T12:05:18.019815: step 26033, loss 0.447898.
Train: 2018-08-09T12:05:18.129193: step 26034, loss 0.578858.
Train: 2018-08-09T12:05:18.238545: step 26035, loss 0.545071.
Train: 2018-08-09T12:05:18.332269: step 26036, loss 0.496689.
Train: 2018-08-09T12:05:18.441618: step 26037, loss 0.578302.
Train: 2018-08-09T12:05:18.535346: step 26038, loss 0.579595.
Train: 2018-08-09T12:05:18.644666: step 26039, loss 0.495274.
Train: 2018-08-09T12:05:18.738394: step 26040, loss 0.630939.
Test: 2018-08-09T12:05:19.239632: step 26040, loss 0.549113.
Train: 2018-08-09T12:05:19.348956: step 26041, loss 0.629932.
Train: 2018-08-09T12:05:19.458306: step 26042, loss 0.679798.
Train: 2018-08-09T12:05:19.552034: step 26043, loss 0.512772.
Train: 2018-08-09T12:05:19.661413: step 26044, loss 0.627715.
Train: 2018-08-09T12:05:19.755111: step 26045, loss 0.644295.
Train: 2018-08-09T12:05:19.864491: step 26046, loss 0.480905.
Train: 2018-08-09T12:05:19.958218: step 26047, loss 0.562402.
Train: 2018-08-09T12:05:20.067540: step 26048, loss 0.594732.
Train: 2018-08-09T12:05:20.176911: step 26049, loss 0.61056.
Train: 2018-08-09T12:05:20.279748: step 26050, loss 0.514243.
Test: 2018-08-09T12:05:20.771251: step 26050, loss 0.549422.
Train: 2018-08-09T12:05:20.864978: step 26051, loss 0.482008.
Train: 2018-08-09T12:05:20.974359: step 26052, loss 0.562931.
Train: 2018-08-09T12:05:21.068056: step 26053, loss 0.579214.
Train: 2018-08-09T12:05:21.177404: step 26054, loss 0.529716.
Train: 2018-08-09T12:05:21.271163: step 26055, loss 0.547111.
Train: 2018-08-09T12:05:21.380515: step 26056, loss 0.514028.
Train: 2018-08-09T12:05:21.474210: step 26057, loss 0.627407.
Train: 2018-08-09T12:05:21.583589: step 26058, loss 0.627862.
Train: 2018-08-09T12:05:21.677287: step 26059, loss 0.643754.
Train: 2018-08-09T12:05:21.786667: step 26060, loss 0.512403.
Test: 2018-08-09T12:05:22.286550: step 26060, loss 0.550676.
Train: 2018-08-09T12:05:22.380246: step 26061, loss 0.51391.
Train: 2018-08-09T12:05:22.489610: step 26062, loss 0.597001.
Train: 2018-08-09T12:05:22.590790: step 26063, loss 0.598471.
Train: 2018-08-09T12:05:22.685056: step 26064, loss 0.614006.
Train: 2018-08-09T12:05:22.778786: step 26065, loss 0.495312.
Train: 2018-08-09T12:05:22.888133: step 26066, loss 0.514194.
Train: 2018-08-09T12:05:22.981861: step 26067, loss 0.546264.
Train: 2018-08-09T12:05:23.091210: step 26068, loss 0.514876.
Train: 2018-08-09T12:05:23.184909: step 26069, loss 0.546044.
Train: 2018-08-09T12:05:23.294288: step 26070, loss 0.627692.
Test: 2018-08-09T12:05:23.778520: step 26070, loss 0.546946.
Train: 2018-08-09T12:05:23.887868: step 26071, loss 0.598774.
Train: 2018-08-09T12:05:23.981626: step 26072, loss 0.562401.
Train: 2018-08-09T12:05:24.090975: step 26073, loss 0.528493.
Train: 2018-08-09T12:05:24.184702: step 26074, loss 0.64541.
Train: 2018-08-09T12:05:24.294053: step 26075, loss 0.54552.
Train: 2018-08-09T12:05:24.387774: step 26076, loss 0.579922.
Train: 2018-08-09T12:05:24.497131: step 26077, loss 0.693629.
Train: 2018-08-09T12:05:24.590857: step 26078, loss 0.483807.
Train: 2018-08-09T12:05:24.700208: step 26079, loss 0.46751.
Train: 2018-08-09T12:05:24.793903: step 26080, loss 0.579239.
Test: 2018-08-09T12:05:25.296233: step 26080, loss 0.551037.
Train: 2018-08-09T12:05:25.389954: step 26081, loss 0.594729.
Train: 2018-08-09T12:05:25.499300: step 26082, loss 0.530965.
Train: 2018-08-09T12:05:25.593058: step 26083, loss 0.467723.
Train: 2018-08-09T12:05:25.702407: step 26084, loss 0.546861.
Train: 2018-08-09T12:05:25.796135: step 26085, loss 0.610696.
Train: 2018-08-09T12:05:25.889832: step 26086, loss 0.434279.
Train: 2018-08-09T12:05:25.999213: step 26087, loss 0.54612.
Train: 2018-08-09T12:05:26.092939: step 26088, loss 0.547286.
Train: 2018-08-09T12:05:26.202284: step 26089, loss 0.62937.
Train: 2018-08-09T12:05:26.299653: step 26090, loss 0.544818.
Test: 2018-08-09T12:05:26.799507: step 26090, loss 0.545811.
Train: 2018-08-09T12:05:26.893233: step 26091, loss 0.531309.
Train: 2018-08-09T12:05:26.999571: step 26092, loss 0.512019.
Train: 2018-08-09T12:05:27.093298: step 26093, loss 0.630265.
Train: 2018-08-09T12:05:27.202647: step 26094, loss 0.511958.
Train: 2018-08-09T12:05:27.296375: step 26095, loss 0.629588.
Train: 2018-08-09T12:05:27.405724: step 26096, loss 0.530113.
Train: 2018-08-09T12:05:27.499453: step 26097, loss 0.613538.
Train: 2018-08-09T12:05:27.608801: step 26098, loss 0.529643.
Train: 2018-08-09T12:05:27.702560: step 26099, loss 0.561485.
Train: 2018-08-09T12:05:27.796257: step 26100, loss 0.629414.
Test: 2018-08-09T12:05:28.296168: step 26100, loss 0.54831.
Train: 2018-08-09T12:05:28.905401: step 26101, loss 0.611976.
Train: 2018-08-09T12:05:28.999129: step 26102, loss 0.530148.
Train: 2018-08-09T12:05:29.108473: step 26103, loss 0.676079.
Train: 2018-08-09T12:05:29.202176: step 26104, loss 0.594441.
Train: 2018-08-09T12:05:29.295905: step 26105, loss 0.643678.
Train: 2018-08-09T12:05:29.405283: step 26106, loss 0.594797.
Train: 2018-08-09T12:05:29.499011: step 26107, loss 0.658059.
Train: 2018-08-09T12:05:29.608331: step 26108, loss 0.515005.
Train: 2018-08-09T12:05:29.702058: step 26109, loss 0.593952.
Train: 2018-08-09T12:05:29.795785: step 26110, loss 0.595571.
Test: 2018-08-09T12:05:30.298041: step 26110, loss 0.550956.
Train: 2018-08-09T12:05:30.391798: step 26111, loss 0.51998.
Train: 2018-08-09T12:05:30.501117: step 26112, loss 0.50332.
Train: 2018-08-09T12:05:30.594845: step 26113, loss 0.580299.
Train: 2018-08-09T12:05:30.688602: step 26114, loss 0.484173.
Train: 2018-08-09T12:05:30.797952: step 26115, loss 0.500919.
Train: 2018-08-09T12:05:30.891680: step 26116, loss 0.61053.
Train: 2018-08-09T12:05:31.001029: step 26117, loss 0.547451.
Train: 2018-08-09T12:05:31.094727: step 26118, loss 0.50009.
Train: 2018-08-09T12:05:31.188454: step 26119, loss 0.435968.
Train: 2018-08-09T12:05:31.297804: step 26120, loss 0.595083.
Test: 2018-08-09T12:05:31.797688: step 26120, loss 0.549653.
Train: 2018-08-09T12:05:31.891415: step 26121, loss 0.514619.
Train: 2018-08-09T12:05:31.986486: step 26122, loss 0.546085.
Train: 2018-08-09T12:05:32.095834: step 26123, loss 0.594359.
Train: 2018-08-09T12:05:32.189562: step 26124, loss 0.428807.
Train: 2018-08-09T12:05:32.283290: step 26125, loss 0.546589.
Train: 2018-08-09T12:05:32.392640: step 26126, loss 0.569383.
Train: 2018-08-09T12:05:32.486398: step 26127, loss 0.634776.
Train: 2018-08-09T12:05:32.595747: step 26128, loss 0.645435.
Train: 2018-08-09T12:05:32.689446: step 26129, loss 0.599442.
Train: 2018-08-09T12:05:32.783203: step 26130, loss 0.529178.
Test: 2018-08-09T12:05:33.329919: step 26130, loss 0.550571.
Train: 2018-08-09T12:05:33.439300: step 26131, loss 0.631601.
Train: 2018-08-09T12:05:33.533027: step 26132, loss 0.61123.
Train: 2018-08-09T12:05:33.626754: step 26133, loss 0.643307.
Train: 2018-08-09T12:05:33.736104: step 26134, loss 0.513344.
Train: 2018-08-09T12:05:33.829832: step 26135, loss 0.513639.
Train: 2018-08-09T12:05:33.925059: step 26136, loss 0.464939.
Train: 2018-08-09T12:05:34.034413: step 26137, loss 0.578847.
Train: 2018-08-09T12:05:34.128106: step 26138, loss 0.448914.
Train: 2018-08-09T12:05:34.221864: step 26139, loss 0.530349.
Train: 2018-08-09T12:05:34.331183: step 26140, loss 0.5472.
Test: 2018-08-09T12:05:34.831067: step 26140, loss 0.549717.
Train: 2018-08-09T12:05:34.924823: step 26141, loss 0.54572.
Train: 2018-08-09T12:05:35.018551: step 26142, loss 0.54615.
Train: 2018-08-09T12:05:35.127901: step 26143, loss 0.577843.
Train: 2018-08-09T12:05:35.221628: step 26144, loss 0.562872.
Train: 2018-08-09T12:05:35.330947: step 26145, loss 0.479747.
Train: 2018-08-09T12:05:35.424705: step 26146, loss 0.446253.
Train: 2018-08-09T12:05:35.518433: step 26147, loss 0.630863.
Train: 2018-08-09T12:05:35.627777: step 26148, loss 0.512436.
Train: 2018-08-09T12:05:35.721510: step 26149, loss 0.663266.
Train: 2018-08-09T12:05:35.830829: step 26150, loss 0.478411.
Test: 2018-08-09T12:05:36.324087: step 26150, loss 0.552244.
Train: 2018-08-09T12:05:36.417844: step 26151, loss 0.512388.
Train: 2018-08-09T12:05:36.527193: step 26152, loss 0.545678.
Train: 2018-08-09T12:05:36.620921: step 26153, loss 0.528257.
Train: 2018-08-09T12:05:36.714649: step 26154, loss 0.647676.
Train: 2018-08-09T12:05:36.823968: step 26155, loss 0.441962.
Train: 2018-08-09T12:05:36.917727: step 26156, loss 0.686151.
Train: 2018-08-09T12:05:37.011454: step 26157, loss 0.49133.
Train: 2018-08-09T12:05:37.120773: step 26158, loss 0.598098.
Train: 2018-08-09T12:05:37.214532: step 26159, loss 0.527597.
Train: 2018-08-09T12:05:37.308228: step 26160, loss 0.581494.
Test: 2018-08-09T12:05:37.808113: step 26160, loss 0.547529.
Train: 2018-08-09T12:05:37.917485: step 26161, loss 0.528243.
Train: 2018-08-09T12:05:38.011188: step 26162, loss 0.70008.
Train: 2018-08-09T12:05:38.104948: step 26163, loss 0.546481.
Train: 2018-08-09T12:05:38.214265: step 26164, loss 0.646471.
Train: 2018-08-09T12:05:38.307994: step 26165, loss 0.562403.
Train: 2018-08-09T12:05:38.401752: step 26166, loss 0.6124.
Train: 2018-08-09T12:05:38.511070: step 26167, loss 0.512838.
Train: 2018-08-09T12:05:38.604798: step 26168, loss 0.512861.
Train: 2018-08-09T12:05:38.698558: step 26169, loss 0.595462.
Train: 2018-08-09T12:05:38.807907: step 26170, loss 0.529739.
Test: 2018-08-09T12:05:39.307759: step 26170, loss 0.550228.
Train: 2018-08-09T12:05:39.401516: step 26171, loss 0.611877.
Train: 2018-08-09T12:05:39.495244: step 26172, loss 0.628255.
Train: 2018-08-09T12:05:39.604594: step 26173, loss 0.480686.
Train: 2018-08-09T12:05:39.698322: step 26174, loss 0.529885.
Train: 2018-08-09T12:05:39.792018: step 26175, loss 0.562548.
Train: 2018-08-09T12:05:39.901398: step 26176, loss 0.546226.
Train: 2018-08-09T12:05:39.997384: step 26177, loss 0.562562.
Train: 2018-08-09T12:05:40.091112: step 26178, loss 0.627972.
Train: 2018-08-09T12:05:40.184840: step 26179, loss 0.529945.
Train: 2018-08-09T12:05:40.294203: step 26180, loss 0.627896.
Test: 2018-08-09T12:05:40.794089: step 26180, loss 0.5492.
Train: 2018-08-09T12:05:40.887800: step 26181, loss 0.562664.
Train: 2018-08-09T12:05:40.981527: step 26182, loss 0.546377.
Train: 2018-08-09T12:05:41.090877: step 26183, loss 0.562658.
Train: 2018-08-09T12:05:41.184604: step 26184, loss 0.497617.
Train: 2018-08-09T12:05:41.278303: step 26185, loss 0.611423.
Train: 2018-08-09T12:05:41.387652: step 26186, loss 0.595164.
Train: 2018-08-09T12:05:41.481409: step 26187, loss 0.528007.
Train: 2018-08-09T12:05:41.575137: step 26188, loss 0.62763.
Train: 2018-08-09T12:05:41.684457: step 26189, loss 0.530239.
Train: 2018-08-09T12:05:41.778185: step 26190, loss 0.65994.
Test: 2018-08-09T12:05:42.279503: step 26190, loss 0.550001.
Train: 2018-08-09T12:05:42.373264: step 26191, loss 0.562697.
Train: 2018-08-09T12:05:42.466989: step 26192, loss 0.659647.
Train: 2018-08-09T12:05:42.576338: step 26193, loss 0.51445.
Train: 2018-08-09T12:05:42.670066: step 26194, loss 0.530648.
Train: 2018-08-09T12:05:42.763764: step 26195, loss 0.562823.
Train: 2018-08-09T12:05:42.873146: step 26196, loss 0.450507.
Train: 2018-08-09T12:05:42.966872: step 26197, loss 0.61098.
Train: 2018-08-09T12:05:43.060599: step 26198, loss 0.594933.
Train: 2018-08-09T12:05:43.169949: step 26199, loss 0.611001.
Train: 2018-08-09T12:05:43.263677: step 26200, loss 0.514742.
Test: 2018-08-09T12:05:43.763543: step 26200, loss 0.550834.
Train: 2018-08-09T12:05:44.310812: step 26201, loss 0.643025.
Train: 2018-08-09T12:05:44.420161: step 26202, loss 0.658957.
Train: 2018-08-09T12:05:44.513891: step 26203, loss 0.53095.
Train: 2018-08-09T12:05:44.607617: step 26204, loss 0.531011.
Train: 2018-08-09T12:05:44.716935: step 26205, loss 0.499195.
Train: 2018-08-09T12:05:44.810694: step 26206, loss 0.642619.
Train: 2018-08-09T12:05:44.904423: step 26207, loss 0.626626.
Train: 2018-08-09T12:05:45.013741: step 26208, loss 0.547063.
Train: 2018-08-09T12:05:45.107468: step 26209, loss 0.531242.
Train: 2018-08-09T12:05:45.201227: step 26210, loss 0.594719.
Test: 2018-08-09T12:05:45.712618: step 26210, loss 0.551153.
Train: 2018-08-09T12:05:45.806345: step 26211, loss 0.515457.
Train: 2018-08-09T12:05:45.900072: step 26212, loss 0.563004.
Train: 2018-08-09T12:05:46.009452: step 26213, loss 0.499597.
Train: 2018-08-09T12:05:46.103181: step 26214, loss 0.59473.
Train: 2018-08-09T12:05:46.196907: step 26215, loss 0.562995.
Train: 2018-08-09T12:05:46.306228: step 26216, loss 0.483528.
Train: 2018-08-09T12:05:46.399985: step 26217, loss 0.722148.
Train: 2018-08-09T12:05:46.493712: step 26218, loss 0.547038.
Train: 2018-08-09T12:05:46.603031: step 26219, loss 0.642462.
Train: 2018-08-09T12:05:46.696790: step 26220, loss 0.515323.
Test: 2018-08-09T12:05:47.196642: step 26220, loss 0.546304.
Train: 2018-08-09T12:05:47.290369: step 26221, loss 0.499464.
Train: 2018-08-09T12:05:47.399718: step 26222, loss 0.626547.
Train: 2018-08-09T12:05:47.493446: step 26223, loss 0.547103.
Train: 2018-08-09T12:05:47.602797: step 26224, loss 0.642395.
Train: 2018-08-09T12:05:47.696523: step 26225, loss 0.642319.
Train: 2018-08-09T12:05:47.790282: step 26226, loss 0.53135.
Train: 2018-08-09T12:05:47.899631: step 26227, loss 0.547226.
Train: 2018-08-09T12:05:47.993329: step 26228, loss 0.642081.
Train: 2018-08-09T12:05:48.087089: step 26229, loss 0.610418.
Train: 2018-08-09T12:05:48.196436: step 26230, loss 0.657575.
Test: 2018-08-09T12:05:48.680668: step 26230, loss 0.547888.
Train: 2018-08-09T12:05:48.790018: step 26231, loss 0.594568.
Train: 2018-08-09T12:05:48.883774: step 26232, loss 0.563243.
Train: 2018-08-09T12:05:48.979939: step 26233, loss 0.500915.
Train: 2018-08-09T12:05:49.089290: step 26234, loss 0.469887.
Train: 2018-08-09T12:05:49.183016: step 26235, loss 0.53217.
Train: 2018-08-09T12:05:49.276712: step 26236, loss 0.625663.
Train: 2018-08-09T12:05:49.386093: step 26237, loss 0.625661.
Train: 2018-08-09T12:05:49.479820: step 26238, loss 0.532174.
Train: 2018-08-09T12:05:49.573548: step 26239, loss 0.532186.
Train: 2018-08-09T12:05:49.667279: step 26240, loss 0.594478.
Test: 2018-08-09T12:05:50.167159: step 26240, loss 0.55048.
Train: 2018-08-09T12:05:50.276507: step 26241, loss 0.641228.
Train: 2018-08-09T12:05:50.370236: step 26242, loss 0.56333.
Train: 2018-08-09T12:05:50.463934: step 26243, loss 0.578904.
Train: 2018-08-09T12:05:50.573313: step 26244, loss 0.609998.
Train: 2018-08-09T12:05:50.667040: step 26245, loss 0.54786.
Train: 2018-08-09T12:05:50.760738: step 26246, loss 0.57891.
Train: 2018-08-09T12:05:50.870118: step 26247, loss 0.547899.
Train: 2018-08-09T12:05:50.965120: step 26248, loss 0.563411.
Train: 2018-08-09T12:05:51.058848: step 26249, loss 0.485897.
Train: 2018-08-09T12:05:51.168198: step 26250, loss 0.563387.
Test: 2018-08-09T12:05:51.652430: step 26250, loss 0.55172.
Train: 2018-08-09T12:05:51.761808: step 26251, loss 0.516669.
Train: 2018-08-09T12:05:51.855507: step 26252, loss 0.485322.
Train: 2018-08-09T12:05:51.964887: step 26253, loss 0.531753.
Train: 2018-08-09T12:05:52.058582: step 26254, loss 0.673912.
Train: 2018-08-09T12:05:52.152341: step 26255, loss 0.531416.
Train: 2018-08-09T12:05:52.261660: step 26256, loss 0.483442.
Train: 2018-08-09T12:05:52.355420: step 26257, loss 0.562274.
Train: 2018-08-09T12:05:52.464767: step 26258, loss 0.578578.
Train: 2018-08-09T12:05:52.558466: step 26259, loss 0.563624.
Train: 2018-08-09T12:05:52.652192: step 26260, loss 0.581654.
Test: 2018-08-09T12:05:53.153420: step 26260, loss 0.548437.
Train: 2018-08-09T12:05:53.294011: step 26261, loss 0.563072.
Train: 2018-08-09T12:05:53.387770: step 26262, loss 0.644083.
Train: 2018-08-09T12:05:53.481466: step 26263, loss 0.546076.
Train: 2018-08-09T12:05:53.590816: step 26264, loss 0.464706.
Train: 2018-08-09T12:05:53.684574: step 26265, loss 0.610237.
Train: 2018-08-09T12:05:53.793895: step 26266, loss 0.56215.
Train: 2018-08-09T12:05:53.911772: step 26267, loss 0.562903.
Train: 2018-08-09T12:05:54.005497: step 26268, loss 0.461877.
Train: 2018-08-09T12:05:54.099227: step 26269, loss 0.580366.
Train: 2018-08-09T12:05:54.208574: step 26270, loss 0.541607.
Test: 2018-08-09T12:05:54.705315: step 26270, loss 0.54769.
Train: 2018-08-09T12:05:54.799072: step 26271, loss 0.560186.
Train: 2018-08-09T12:05:54.909671: step 26272, loss 0.533061.
Train: 2018-08-09T12:05:54.997868: step 26273, loss 0.522636.
Train: 2018-08-09T12:05:55.107248: step 26274, loss 0.56356.
Train: 2018-08-09T12:05:55.200976: step 26275, loss 0.555004.
Train: 2018-08-09T12:05:55.294703: step 26276, loss 0.59939.
Train: 2018-08-09T12:05:55.404052: step 26277, loss 0.543095.
Train: 2018-08-09T12:05:55.497780: step 26278, loss 0.641868.
Train: 2018-08-09T12:05:55.607099: step 26279, loss 0.636125.
Train: 2018-08-09T12:05:55.700827: step 26280, loss 0.450642.
Test: 2018-08-09T12:05:56.205698: step 26280, loss 0.545815.
Train: 2018-08-09T12:05:56.299455: step 26281, loss 0.527735.
Train: 2018-08-09T12:05:56.408804: step 26282, loss 0.592835.
Train: 2018-08-09T12:05:56.502534: step 26283, loss 0.653443.
Train: 2018-08-09T12:05:56.596230: step 26284, loss 0.574796.
Train: 2018-08-09T12:05:56.705609: step 26285, loss 0.68245.
Train: 2018-08-09T12:05:56.799338: step 26286, loss 0.42776.
Train: 2018-08-09T12:05:56.907721: step 26287, loss 0.60011.
Train: 2018-08-09T12:05:57.017071: step 26288, loss 0.498448.
Train: 2018-08-09T12:05:57.110798: step 26289, loss 0.612573.
Train: 2018-08-09T12:05:57.204528: step 26290, loss 0.630801.
Test: 2018-08-09T12:05:57.704380: step 26290, loss 0.54778.
Train: 2018-08-09T12:05:57.813727: step 26291, loss 0.547386.
Train: 2018-08-09T12:05:57.911846: step 26292, loss 0.594803.
Train: 2018-08-09T12:05:58.021195: step 26293, loss 0.644226.
Train: 2018-08-09T12:05:58.114954: step 26294, loss 0.595457.
Train: 2018-08-09T12:05:58.224302: step 26295, loss 0.578328.
Train: 2018-08-09T12:05:58.318030: step 26296, loss 0.642553.
Train: 2018-08-09T12:05:58.411729: step 26297, loss 0.625623.
Train: 2018-08-09T12:05:58.521107: step 26298, loss 0.609731.
Train: 2018-08-09T12:05:58.630430: step 26299, loss 0.51789.
Train: 2018-08-09T12:05:58.724186: step 26300, loss 0.502874.
Test: 2018-08-09T12:05:59.224038: step 26300, loss 0.551305.
Train: 2018-08-09T12:05:59.833268: step 26301, loss 0.51817.
Train: 2018-08-09T12:05:59.943979: step 26302, loss 0.548632.
Train: 2018-08-09T12:06:00.037677: step 26303, loss 0.639808.
Train: 2018-08-09T12:06:00.147054: step 26304, loss 0.594212.
Train: 2018-08-09T12:06:00.240785: step 26305, loss 0.563891.
Train: 2018-08-09T12:06:00.350131: step 26306, loss 0.503162.
Train: 2018-08-09T12:06:00.443860: step 26307, loss 0.548648.
Train: 2018-08-09T12:06:00.553208: step 26308, loss 0.517834.
Train: 2018-08-09T12:06:00.646936: step 26309, loss 0.563658.
Train: 2018-08-09T12:06:00.756285: step 26310, loss 0.547613.
Test: 2018-08-09T12:06:01.256138: step 26310, loss 0.549571.
Train: 2018-08-09T12:06:01.349864: step 26311, loss 0.626657.
Train: 2018-08-09T12:06:01.443625: step 26312, loss 0.58075.
Train: 2018-08-09T12:06:01.552972: step 26313, loss 0.640522.
Train: 2018-08-09T12:06:01.662322: step 26314, loss 0.531626.
Train: 2018-08-09T12:06:01.756020: step 26315, loss 0.548317.
Train: 2018-08-09T12:06:01.865368: step 26316, loss 0.546888.
Train: 2018-08-09T12:06:01.960441: step 26317, loss 0.564084.
Train: 2018-08-09T12:06:02.069791: step 26318, loss 0.54808.
Train: 2018-08-09T12:06:02.163490: step 26319, loss 0.51587.
Train: 2018-08-09T12:06:02.272838: step 26320, loss 0.576246.
Test: 2018-08-09T12:06:02.772739: step 26320, loss 0.548633.
Train: 2018-08-09T12:06:02.866449: step 26321, loss 0.484756.
Train: 2018-08-09T12:06:02.975798: step 26322, loss 0.656665.
Train: 2018-08-09T12:06:03.069526: step 26323, loss 0.49435.
Train: 2018-08-09T12:06:03.178905: step 26324, loss 0.545749.
Train: 2018-08-09T12:06:03.272632: step 26325, loss 0.644395.
Train: 2018-08-09T12:06:03.381982: step 26326, loss 0.549513.
Train: 2018-08-09T12:06:03.491300: step 26327, loss 0.584499.
Train: 2018-08-09T12:06:03.585028: step 26328, loss 0.518059.
Train: 2018-08-09T12:06:03.694378: step 26329, loss 0.516454.
Train: 2018-08-09T12:06:03.803728: step 26330, loss 0.514416.
Test: 2018-08-09T12:06:04.305121: step 26330, loss 0.547522.
Train: 2018-08-09T12:06:04.398832: step 26331, loss 0.443746.
Train: 2018-08-09T12:06:04.508181: step 26332, loss 0.584293.
Train: 2018-08-09T12:06:04.601909: step 26333, loss 0.662142.
Train: 2018-08-09T12:06:04.711252: step 26334, loss 0.611286.
Train: 2018-08-09T12:06:04.804987: step 26335, loss 0.447446.
Train: 2018-08-09T12:06:04.914337: step 26336, loss 0.594531.
Train: 2018-08-09T12:06:05.023685: step 26337, loss 0.577078.
Train: 2018-08-09T12:06:05.117413: step 26338, loss 0.545841.
Train: 2018-08-09T12:06:05.226762: step 26339, loss 0.544365.
Train: 2018-08-09T12:06:05.320490: step 26340, loss 0.57878.
Test: 2018-08-09T12:06:05.820373: step 26340, loss 0.548615.
Train: 2018-08-09T12:06:05.929722: step 26341, loss 0.631805.
Train: 2018-08-09T12:06:06.023419: step 26342, loss 0.525496.
Train: 2018-08-09T12:06:06.132769: step 26343, loss 0.525622.
Train: 2018-08-09T12:06:06.242148: step 26344, loss 0.579802.
Train: 2018-08-09T12:06:06.335875: step 26345, loss 0.614449.
Train: 2018-08-09T12:06:06.445228: step 26346, loss 0.631597.
Train: 2018-08-09T12:06:06.538952: step 26347, loss 0.526172.
Train: 2018-08-09T12:06:06.648302: step 26348, loss 0.561376.
Train: 2018-08-09T12:06:06.757622: step 26349, loss 0.578639.
Train: 2018-08-09T12:06:06.851381: step 26350, loss 0.49615.
Test: 2018-08-09T12:06:07.352510: step 26350, loss 0.548778.
Train: 2018-08-09T12:06:07.461836: step 26351, loss 0.576417.
Train: 2018-08-09T12:06:07.555593: step 26352, loss 0.612341.
Train: 2018-08-09T12:06:07.664913: step 26353, loss 0.543361.
Train: 2018-08-09T12:06:07.774291: step 26354, loss 0.597529.
Train: 2018-08-09T12:06:07.883643: step 26355, loss 0.597572.
Train: 2018-08-09T12:06:07.977338: step 26356, loss 0.656294.
Train: 2018-08-09T12:06:08.086718: step 26357, loss 0.595473.
Train: 2018-08-09T12:06:08.196068: step 26358, loss 0.53142.
Train: 2018-08-09T12:06:08.289795: step 26359, loss 0.642838.
Train: 2018-08-09T12:06:08.399145: step 26360, loss 0.532111.
Test: 2018-08-09T12:06:08.898998: step 26360, loss 0.549732.
Train: 2018-08-09T12:06:08.995076: step 26361, loss 0.549988.
Train: 2018-08-09T12:06:09.120046: step 26362, loss 0.579219.
Train: 2018-08-09T12:06:09.213803: step 26363, loss 0.470516.
Train: 2018-08-09T12:06:09.323123: step 26364, loss 0.641184.
Train: 2018-08-09T12:06:09.432502: step 26365, loss 0.501667.
Train: 2018-08-09T12:06:09.526200: step 26366, loss 0.609109.
Train: 2018-08-09T12:06:09.635549: step 26367, loss 0.578904.
Train: 2018-08-09T12:06:09.740837: step 26368, loss 0.562951.
Train: 2018-08-09T12:06:09.850217: step 26369, loss 0.547529.
Train: 2018-08-09T12:06:09.959566: step 26370, loss 0.609909.
Test: 2018-08-09T12:06:10.459420: step 26370, loss 0.550799.
Train: 2018-08-09T12:06:10.553176: step 26371, loss 0.563758.
Train: 2018-08-09T12:06:10.662525: step 26372, loss 0.54744.
Train: 2018-08-09T12:06:10.771845: step 26373, loss 0.609604.
Train: 2018-08-09T12:06:10.881194: step 26374, loss 0.423078.
Train: 2018-08-09T12:06:10.981259: step 26375, loss 0.578776.
Train: 2018-08-09T12:06:11.090604: step 26376, loss 0.483689.
Train: 2018-08-09T12:06:11.199928: step 26377, loss 0.562019.
Train: 2018-08-09T12:06:11.293658: step 26378, loss 0.594402.
Train: 2018-08-09T12:06:11.403040: step 26379, loss 0.545655.
Train: 2018-08-09T12:06:11.512385: step 26380, loss 0.5318.
Test: 2018-08-09T12:06:12.012265: step 26380, loss 0.548037.
Train: 2018-08-09T12:06:12.121616: step 26381, loss 0.631028.
Train: 2018-08-09T12:06:12.230966: step 26382, loss 0.547102.
Train: 2018-08-09T12:06:12.340315: step 26383, loss 0.547072.
Train: 2018-08-09T12:06:12.434042: step 26384, loss 0.580922.
Train: 2018-08-09T12:06:12.543393: step 26385, loss 0.445418.
Train: 2018-08-09T12:06:12.652711: step 26386, loss 0.632386.
Train: 2018-08-09T12:06:12.762092: step 26387, loss 0.540039.
Train: 2018-08-09T12:06:12.871439: step 26388, loss 0.579279.
Train: 2018-08-09T12:06:12.965167: step 26389, loss 0.566636.
Train: 2018-08-09T12:06:13.074517: step 26390, loss 0.530316.
Test: 2018-08-09T12:06:13.574400: step 26390, loss 0.548221.
Train: 2018-08-09T12:06:13.683749: step 26391, loss 0.514223.
Train: 2018-08-09T12:06:13.793067: step 26392, loss 0.513483.
Train: 2018-08-09T12:06:13.902416: step 26393, loss 0.531585.
Train: 2018-08-09T12:06:13.998519: step 26394, loss 0.462487.
Train: 2018-08-09T12:06:14.107837: step 26395, loss 0.445223.
Train: 2018-08-09T12:06:14.217186: step 26396, loss 0.593781.
Train: 2018-08-09T12:06:14.326567: step 26397, loss 0.542813.
Train: 2018-08-09T12:06:14.435915: step 26398, loss 0.601804.
Train: 2018-08-09T12:06:14.545266: step 26399, loss 0.597322.
Train: 2018-08-09T12:06:14.654616: step 26400, loss 0.442633.
Test: 2018-08-09T12:06:15.154491: step 26400, loss 0.548739.
Train: 2018-08-09T12:06:15.716834: step 26401, loss 0.525954.
Train: 2018-08-09T12:06:15.826212: step 26402, loss 0.563289.
Train: 2018-08-09T12:06:15.936257: step 26403, loss 0.566001.
Train: 2018-08-09T12:06:16.045639: step 26404, loss 0.507784.
Train: 2018-08-09T12:06:16.139334: step 26405, loss 0.667834.
Train: 2018-08-09T12:06:16.248684: step 26406, loss 0.551012.
Train: 2018-08-09T12:06:16.358063: step 26407, loss 0.548715.
Train: 2018-08-09T12:06:16.467412: step 26408, loss 0.583405.
Train: 2018-08-09T12:06:16.576762: step 26409, loss 0.513596.
Train: 2018-08-09T12:06:16.686113: step 26410, loss 0.512818.
Test: 2018-08-09T12:06:17.185963: step 26410, loss 0.546058.
Train: 2018-08-09T12:06:17.295344: step 26411, loss 0.729765.
Train: 2018-08-09T12:06:17.389039: step 26412, loss 0.546128.
Train: 2018-08-09T12:06:17.498388: step 26413, loss 0.629597.
Train: 2018-08-09T12:06:17.607768: step 26414, loss 0.578579.
Train: 2018-08-09T12:06:17.717088: step 26415, loss 0.627849.
Train: 2018-08-09T12:06:17.826437: step 26416, loss 0.546404.
Train: 2018-08-09T12:06:17.935786: step 26417, loss 0.54735.
Train: 2018-08-09T12:06:18.045167: step 26418, loss 0.54665.
Train: 2018-08-09T12:06:18.154485: step 26419, loss 0.611211.
Train: 2018-08-09T12:06:18.263833: step 26420, loss 0.530804.
Test: 2018-08-09T12:06:18.763717: step 26420, loss 0.54911.
Train: 2018-08-09T12:06:18.857475: step 26421, loss 0.547066.
Train: 2018-08-09T12:06:18.968318: step 26422, loss 0.49903.
Train: 2018-08-09T12:06:19.077667: step 26423, loss 0.578893.
Train: 2018-08-09T12:06:19.186987: step 26424, loss 0.547132.
Train: 2018-08-09T12:06:19.296366: step 26425, loss 0.67467.
Train: 2018-08-09T12:06:19.405685: step 26426, loss 0.531013.
Train: 2018-08-09T12:06:19.515060: step 26427, loss 0.578857.
Train: 2018-08-09T12:06:19.624414: step 26428, loss 0.531146.
Train: 2018-08-09T12:06:19.733762: step 26429, loss 0.54704.
Train: 2018-08-09T12:06:19.843112: step 26430, loss 0.547003.
Test: 2018-08-09T12:06:20.342965: step 26430, loss 0.548602.
Train: 2018-08-09T12:06:20.452343: step 26431, loss 0.594765.
Train: 2018-08-09T12:06:20.561692: step 26432, loss 0.610731.
Train: 2018-08-09T12:06:20.671042: step 26433, loss 0.626649.
Train: 2018-08-09T12:06:20.780361: step 26434, loss 0.499346.
Train: 2018-08-09T12:06:20.874119: step 26435, loss 0.594793.
Train: 2018-08-09T12:06:20.984801: step 26436, loss 0.483526.
Train: 2018-08-09T12:06:21.094119: step 26437, loss 0.578854.
Train: 2018-08-09T12:06:21.203469: step 26438, loss 0.658413.
Train: 2018-08-09T12:06:21.312820: step 26439, loss 0.547033.
Train: 2018-08-09T12:06:21.422198: step 26440, loss 0.562958.
Test: 2018-08-09T12:06:21.906428: step 26440, loss 0.550479.
Train: 2018-08-09T12:06:22.015779: step 26441, loss 0.53117.
Train: 2018-08-09T12:06:22.125159: step 26442, loss 0.674317.
Train: 2018-08-09T12:06:22.234477: step 26443, loss 0.547184.
Train: 2018-08-09T12:06:22.343856: step 26444, loss 0.515386.
Train: 2018-08-09T12:06:22.453176: step 26445, loss 0.563042.
Train: 2018-08-09T12:06:22.562526: step 26446, loss 0.594712.
Train: 2018-08-09T12:06:22.671872: step 26447, loss 0.594725.
Train: 2018-08-09T12:06:22.781253: step 26448, loss 0.578844.
Train: 2018-08-09T12:06:22.890603: step 26449, loss 0.594709.
Train: 2018-08-09T12:06:23.001288: step 26450, loss 0.642125.
Test: 2018-08-09T12:06:23.485526: step 26450, loss 0.551882.
Train: 2018-08-09T12:06:23.594900: step 26451, loss 0.531538.
Train: 2018-08-09T12:06:23.704256: step 26452, loss 0.531594.
Train: 2018-08-09T12:06:23.813573: step 26453, loss 0.531622.
Train: 2018-08-09T12:06:23.922953: step 26454, loss 0.468604.
Train: 2018-08-09T12:06:24.032295: step 26455, loss 0.578877.
Train: 2018-08-09T12:06:24.141650: step 26456, loss 0.578844.
Train: 2018-08-09T12:06:24.250969: step 26457, loss 0.578864.
Train: 2018-08-09T12:06:24.360318: step 26458, loss 0.563082.
Train: 2018-08-09T12:06:24.469699: step 26459, loss 0.531312.
Train: 2018-08-09T12:06:24.579018: step 26460, loss 0.563019.
Test: 2018-08-09T12:06:25.081245: step 26460, loss 0.549889.
Train: 2018-08-09T12:06:25.190564: step 26461, loss 0.594744.
Train: 2018-08-09T12:06:25.299945: step 26462, loss 0.562958.
Train: 2018-08-09T12:06:25.409261: step 26463, loss 0.467522.
Train: 2018-08-09T12:06:25.518611: step 26464, loss 0.483189.
Train: 2018-08-09T12:06:25.627963: step 26465, loss 0.562907.
Train: 2018-08-09T12:06:25.737340: step 26466, loss 0.562761.
Train: 2018-08-09T12:06:25.846689: step 26467, loss 0.611287.
Train: 2018-08-09T12:06:25.956039: step 26468, loss 0.433591.
Train: 2018-08-09T12:06:26.065387: step 26469, loss 0.562311.
Train: 2018-08-09T12:06:26.174739: step 26470, loss 0.497829.
Test: 2018-08-09T12:06:26.674591: step 26470, loss 0.551514.
Train: 2018-08-09T12:06:26.768316: step 26471, loss 0.496925.
Train: 2018-08-09T12:06:26.877665: step 26472, loss 0.578214.
Train: 2018-08-09T12:06:26.989450: step 26473, loss 0.460895.
Train: 2018-08-09T12:06:27.098800: step 26474, loss 0.49403.
Train: 2018-08-09T12:06:27.208119: step 26475, loss 0.594993.
Train: 2018-08-09T12:06:27.317499: step 26476, loss 0.476723.
Train: 2018-08-09T12:06:27.426848: step 26477, loss 0.488759.
Train: 2018-08-09T12:06:27.528267: step 26478, loss 0.56482.
Train: 2018-08-09T12:06:27.639984: step 26479, loss 0.579365.
Train: 2018-08-09T12:06:27.749333: step 26480, loss 0.599346.
Test: 2018-08-09T12:06:28.233582: step 26480, loss 0.54674.
Train: 2018-08-09T12:06:28.342943: step 26481, loss 0.577036.
Train: 2018-08-09T12:06:28.452292: step 26482, loss 0.467111.
Train: 2018-08-09T12:06:28.561611: step 26483, loss 0.549421.
Train: 2018-08-09T12:06:28.670992: step 26484, loss 0.508158.
Train: 2018-08-09T12:06:28.780311: step 26485, loss 0.465692.
Train: 2018-08-09T12:06:28.889690: step 26486, loss 0.585837.
Train: 2018-08-09T12:06:28.989741: step 26487, loss 0.48704.
Train: 2018-08-09T12:06:29.099090: step 26488, loss 0.538743.
Train: 2018-08-09T12:06:29.208439: step 26489, loss 0.605794.
Train: 2018-08-09T12:06:29.317806: step 26490, loss 0.416338.
Test: 2018-08-09T12:06:29.815834: step 26490, loss 0.547098.
Train: 2018-08-09T12:06:29.925207: step 26491, loss 0.505862.
Train: 2018-08-09T12:06:30.034562: step 26492, loss 0.704586.
Train: 2018-08-09T12:06:30.128259: step 26493, loss 0.52938.
Train: 2018-08-09T12:06:30.237609: step 26494, loss 0.45846.
Train: 2018-08-09T12:06:30.346988: step 26495, loss 0.506602.
Train: 2018-08-09T12:06:30.456337: step 26496, loss 0.511267.
Train: 2018-08-09T12:06:30.565689: step 26497, loss 0.728788.
Train: 2018-08-09T12:06:30.675005: step 26498, loss 0.595064.
Train: 2018-08-09T12:06:30.784357: step 26499, loss 0.597468.
Train: 2018-08-09T12:06:30.893735: step 26500, loss 0.614007.
Test: 2018-08-09T12:06:31.393588: step 26500, loss 0.546215.
Train: 2018-08-09T12:06:31.958268: step 26501, loss 0.613155.
Train: 2018-08-09T12:06:32.067587: step 26502, loss 0.476847.
Train: 2018-08-09T12:06:32.176968: step 26503, loss 0.578939.
Train: 2018-08-09T12:06:32.286285: step 26504, loss 0.578637.
Train: 2018-08-09T12:06:32.395665: step 26505, loss 0.596576.
Train: 2018-08-09T12:06:32.505008: step 26506, loss 0.612941.
Train: 2018-08-09T12:06:32.598742: step 26507, loss 0.579195.
Train: 2018-08-09T12:06:32.708061: step 26508, loss 0.495868.
Train: 2018-08-09T12:06:32.817412: step 26509, loss 0.562455.
Train: 2018-08-09T12:06:32.926760: step 26510, loss 0.579073.
Test: 2018-08-09T12:06:33.426644: step 26510, loss 0.550734.
Train: 2018-08-09T12:06:33.536021: step 26511, loss 0.529385.
Train: 2018-08-09T12:06:33.645371: step 26512, loss 0.54597.
Train: 2018-08-09T12:06:33.739097: step 26513, loss 0.529504.
Train: 2018-08-09T12:06:33.848420: step 26514, loss 0.562515.
Train: 2018-08-09T12:06:33.959321: step 26515, loss 0.612045.
Train: 2018-08-09T12:06:34.068675: step 26516, loss 0.595533.
Train: 2018-08-09T12:06:34.178020: step 26517, loss 0.463775.
Train: 2018-08-09T12:06:34.287338: step 26518, loss 0.480132.
Train: 2018-08-09T12:06:34.381066: step 26519, loss 0.595382.
Train: 2018-08-09T12:06:34.490446: step 26520, loss 0.496575.
Test: 2018-08-09T12:06:34.990299: step 26520, loss 0.549422.
Train: 2018-08-09T12:06:35.099678: step 26521, loss 0.562564.
Train: 2018-08-09T12:06:35.208996: step 26522, loss 0.445514.
Train: 2018-08-09T12:06:35.302754: step 26523, loss 0.545759.
Train: 2018-08-09T12:06:35.412106: step 26524, loss 0.611522.
Train: 2018-08-09T12:06:35.521424: step 26525, loss 0.511632.
Train: 2018-08-09T12:06:35.630802: step 26526, loss 0.56191.
Train: 2018-08-09T12:06:35.740152: step 26527, loss 0.546596.
Train: 2018-08-09T12:06:35.833880: step 26528, loss 0.548495.
Train: 2018-08-09T12:06:35.944666: step 26529, loss 0.508961.
Train: 2018-08-09T12:06:36.054015: step 26530, loss 0.5795.
Test: 2018-08-09T12:06:36.553866: step 26530, loss 0.54658.
Train: 2018-08-09T12:06:36.647618: step 26531, loss 0.51115.
Train: 2018-08-09T12:06:36.756943: step 26532, loss 0.511174.
Train: 2018-08-09T12:06:36.866292: step 26533, loss 0.545745.
Train: 2018-08-09T12:06:36.975641: step 26534, loss 0.649856.
Train: 2018-08-09T12:06:37.085001: step 26535, loss 0.669329.
Train: 2018-08-09T12:06:37.178719: step 26536, loss 0.528394.
Train: 2018-08-09T12:06:37.288098: step 26537, loss 0.580173.
Train: 2018-08-09T12:06:37.397449: step 26538, loss 0.545493.
Train: 2018-08-09T12:06:37.506767: step 26539, loss 0.562308.
Train: 2018-08-09T12:06:37.616146: step 26540, loss 0.646474.
Test: 2018-08-09T12:06:38.102771: step 26540, loss 0.548069.
Train: 2018-08-09T12:06:38.212089: step 26541, loss 0.545813.
Train: 2018-08-09T12:06:38.321468: step 26542, loss 0.628947.
Train: 2018-08-09T12:06:38.430787: step 26543, loss 0.595626.
Train: 2018-08-09T12:06:38.524546: step 26544, loss 0.645155.
Train: 2018-08-09T12:06:38.633895: step 26545, loss 0.611932.
Train: 2018-08-09T12:06:38.743243: step 26546, loss 0.464139.
Train: 2018-08-09T12:06:38.852593: step 26547, loss 0.562583.
Train: 2018-08-09T12:06:38.961915: step 26548, loss 0.44829.
Train: 2018-08-09T12:06:39.071292: step 26549, loss 0.578935.
Train: 2018-08-09T12:06:39.165021: step 26550, loss 0.611564.
Test: 2018-08-09T12:06:39.664872: step 26550, loss 0.549821.
Train: 2018-08-09T12:06:39.774251: step 26551, loss 0.578925.
Train: 2018-08-09T12:06:39.883570: step 26552, loss 0.562641.
Train: 2018-08-09T12:06:39.979662: step 26553, loss 0.513876.
Train: 2018-08-09T12:06:40.089009: step 26554, loss 0.513896.
Train: 2018-08-09T12:06:40.198360: step 26555, loss 0.660215.
Train: 2018-08-09T12:06:40.292117: step 26556, loss 0.530175.
Train: 2018-08-09T12:06:40.401437: step 26557, loss 0.627619.
Train: 2018-08-09T12:06:40.510815: step 26558, loss 0.546467.
Train: 2018-08-09T12:06:40.604544: step 26559, loss 0.49788.
Train: 2018-08-09T12:06:40.713863: step 26560, loss 0.546493.
Test: 2018-08-09T12:06:41.213747: step 26560, loss 0.549949.
Train: 2018-08-09T12:06:41.323124: step 26561, loss 0.741042.
Train: 2018-08-09T12:06:41.416853: step 26562, loss 0.514193.
Train: 2018-08-09T12:06:41.526170: step 26563, loss 0.530425.
Train: 2018-08-09T12:06:41.635550: step 26564, loss 0.562739.
Train: 2018-08-09T12:06:41.729279: step 26565, loss 0.578883.
Train: 2018-08-09T12:06:41.838628: step 26566, loss 0.611136.
Train: 2018-08-09T12:06:41.949354: step 26567, loss 0.562768.
Train: 2018-08-09T12:06:42.043081: step 26568, loss 0.627136.
Train: 2018-08-09T12:06:42.152430: step 26569, loss 0.594924.
Train: 2018-08-09T12:06:42.261780: step 26570, loss 0.594878.
Test: 2018-08-09T12:06:42.746035: step 26570, loss 0.548487.
Train: 2018-08-09T12:06:42.855389: step 26571, loss 0.498902.
Train: 2018-08-09T12:06:42.964739: step 26572, loss 0.610799.
Train: 2018-08-09T12:06:43.074057: step 26573, loss 0.499022.
Train: 2018-08-09T12:06:43.167815: step 26574, loss 0.562797.
Train: 2018-08-09T12:06:43.277164: step 26575, loss 0.482967.
Train: 2018-08-09T12:06:43.386483: step 26576, loss 0.546473.
Train: 2018-08-09T12:06:43.481825: step 26577, loss 0.611163.
Train: 2018-08-09T12:06:43.591145: step 26578, loss 0.563232.
Train: 2018-08-09T12:06:43.700494: step 26579, loss 0.432944.
Train: 2018-08-09T12:06:43.794222: step 26580, loss 0.547151.
Test: 2018-08-09T12:06:44.294136: step 26580, loss 0.548363.
Train: 2018-08-09T12:06:44.403483: step 26581, loss 0.628313.
Train: 2018-08-09T12:06:44.497210: step 26582, loss 0.629672.
Train: 2018-08-09T12:06:44.606529: step 26583, loss 0.448053.
Train: 2018-08-09T12:06:44.715910: step 26584, loss 0.594248.
Train: 2018-08-09T12:06:44.809637: step 26585, loss 0.562391.
Train: 2018-08-09T12:06:44.918957: step 26586, loss 0.562907.
Train: 2018-08-09T12:06:45.028332: step 26587, loss 0.595603.
Train: 2018-08-09T12:06:45.137685: step 26588, loss 0.743142.
Train: 2018-08-09T12:06:45.231414: step 26589, loss 0.530097.
Train: 2018-08-09T12:06:45.340731: step 26590, loss 0.611816.
Test: 2018-08-09T12:06:45.840643: step 26590, loss 0.548177.
Train: 2018-08-09T12:06:45.935740: step 26591, loss 0.449341.
Train: 2018-08-09T12:06:46.045089: step 26592, loss 0.578629.
Train: 2018-08-09T12:06:46.138816: step 26593, loss 0.62766.
Train: 2018-08-09T12:06:46.248169: step 26594, loss 0.562352.
Train: 2018-08-09T12:06:46.357509: step 26595, loss 0.546831.
Train: 2018-08-09T12:06:46.451242: step 26596, loss 0.514848.
Train: 2018-08-09T12:06:46.560561: step 26597, loss 0.562973.
Train: 2018-08-09T12:06:46.654290: step 26598, loss 0.546898.
Train: 2018-08-09T12:06:46.763671: step 26599, loss 0.562987.
Train: 2018-08-09T12:06:46.872989: step 26600, loss 0.562735.
Test: 2018-08-09T12:06:47.357250: step 26600, loss 0.547239.
Train: 2018-08-09T12:06:47.952242: step 26601, loss 0.546811.
Train: 2018-08-09T12:06:48.045994: step 26602, loss 0.594813.
Train: 2018-08-09T12:06:48.155310: step 26603, loss 0.530897.
Train: 2018-08-09T12:06:48.264688: step 26604, loss 0.498618.
Train: 2018-08-09T12:06:48.358420: step 26605, loss 0.450215.
Train: 2018-08-09T12:06:48.467767: step 26606, loss 0.562814.
Train: 2018-08-09T12:06:48.577116: step 26607, loss 0.578621.
Train: 2018-08-09T12:06:48.686464: step 26608, loss 0.481093.
Train: 2018-08-09T12:06:48.780193: step 26609, loss 0.62814.
Train: 2018-08-09T12:06:48.889542: step 26610, loss 0.49625.
Test: 2018-08-09T12:06:49.389395: step 26610, loss 0.548808.
Train: 2018-08-09T12:06:49.483151: step 26611, loss 0.478775.
Train: 2018-08-09T12:06:49.576850: step 26612, loss 0.577183.
Train: 2018-08-09T12:06:49.686231: step 26613, loss 0.64808.
Train: 2018-08-09T12:06:49.795581: step 26614, loss 0.577121.
Train: 2018-08-09T12:06:49.889275: step 26615, loss 0.597673.
Train: 2018-08-09T12:06:49.999323: step 26616, loss 0.681872.
Train: 2018-08-09T12:06:50.093054: step 26617, loss 0.509156.
Train: 2018-08-09T12:06:50.202372: step 26618, loss 0.47763.
Train: 2018-08-09T12:06:50.296123: step 26619, loss 0.495761.
Train: 2018-08-09T12:06:50.405477: step 26620, loss 0.631922.
Test: 2018-08-09T12:06:50.905329: step 26620, loss 0.547553.
Train: 2018-08-09T12:06:50.999089: step 26621, loss 0.598656.
Train: 2018-08-09T12:06:51.108407: step 26622, loss 0.494218.
Train: 2018-08-09T12:06:51.202164: step 26623, loss 0.615356.
Train: 2018-08-09T12:06:51.311513: step 26624, loss 0.611777.
Train: 2018-08-09T12:06:51.405211: step 26625, loss 0.56322.
Train: 2018-08-09T12:06:51.514590: step 26626, loss 0.544942.
Train: 2018-08-09T12:06:51.608319: step 26627, loss 0.545633.
Train: 2018-08-09T12:06:51.717638: step 26628, loss 0.677258.
Train: 2018-08-09T12:06:51.811366: step 26629, loss 0.497025.
Train: 2018-08-09T12:06:51.920715: step 26630, loss 0.544954.
Test: 2018-08-09T12:06:52.420598: step 26630, loss 0.551308.
Train: 2018-08-09T12:06:52.514325: step 26631, loss 0.547477.
Train: 2018-08-09T12:06:52.623705: step 26632, loss 0.479666.
Train: 2018-08-09T12:06:52.717401: step 26633, loss 0.629163.
Train: 2018-08-09T12:06:52.826781: step 26634, loss 0.56272.
Train: 2018-08-09T12:06:52.921898: step 26635, loss 0.562986.
Train: 2018-08-09T12:06:53.031247: step 26636, loss 0.595093.
Train: 2018-08-09T12:06:53.124946: step 26637, loss 0.629234.
Train: 2018-08-09T12:06:53.234325: step 26638, loss 0.693755.
Train: 2018-08-09T12:06:53.328052: step 26639, loss 0.465008.
Train: 2018-08-09T12:06:53.437402: step 26640, loss 0.530782.
Test: 2018-08-09T12:06:53.921634: step 26640, loss 0.545784.
Train: 2018-08-09T12:06:54.031012: step 26641, loss 0.595582.
Train: 2018-08-09T12:06:54.124743: step 26642, loss 0.482431.
Train: 2018-08-09T12:06:54.234058: step 26643, loss 0.530082.
Train: 2018-08-09T12:06:54.343408: step 26644, loss 0.498541.
Train: 2018-08-09T12:06:54.437136: step 26645, loss 0.562711.
Train: 2018-08-09T12:06:54.546516: step 26646, loss 0.546966.
Train: 2018-08-09T12:06:54.640249: step 26647, loss 0.595436.
Train: 2018-08-09T12:06:54.749562: step 26648, loss 0.546819.
Train: 2018-08-09T12:06:54.843320: step 26649, loss 0.546112.
Train: 2018-08-09T12:06:54.953999: step 26650, loss 0.530256.
Test: 2018-08-09T12:06:55.438250: step 26650, loss 0.548651.
Train: 2018-08-09T12:06:55.547610: step 26651, loss 0.513508.
Train: 2018-08-09T12:06:55.641341: step 26652, loss 0.628496.
Train: 2018-08-09T12:06:55.750681: step 26653, loss 0.644851.
Train: 2018-08-09T12:06:55.844414: step 26654, loss 0.546408.
Train: 2018-08-09T12:06:55.953733: step 26655, loss 0.546149.
Train: 2018-08-09T12:06:56.047492: step 26656, loss 0.546321.
Train: 2018-08-09T12:06:56.156841: step 26657, loss 0.562475.
Train: 2018-08-09T12:06:56.250538: step 26658, loss 0.465012.
Train: 2018-08-09T12:06:56.359888: step 26659, loss 0.530171.
Train: 2018-08-09T12:06:56.453617: step 26660, loss 0.513297.
Test: 2018-08-09T12:06:56.955859: step 26660, loss 0.547752.
Train: 2018-08-09T12:06:57.065210: step 26661, loss 0.612221.
Train: 2018-08-09T12:06:57.158932: step 26662, loss 0.677987.
Train: 2018-08-09T12:06:57.268287: step 26663, loss 0.611561.
Train: 2018-08-09T12:06:57.362014: step 26664, loss 0.52987.
Train: 2018-08-09T12:06:57.471364: step 26665, loss 0.546018.
Train: 2018-08-09T12:06:57.565092: step 26666, loss 0.480534.
Train: 2018-08-09T12:06:57.674444: step 26667, loss 0.480708.
Train: 2018-08-09T12:06:57.768169: step 26668, loss 0.579063.
Train: 2018-08-09T12:06:57.877512: step 26669, loss 0.677729.
Train: 2018-08-09T12:06:57.971216: step 26670, loss 0.579162.
Test: 2018-08-09T12:06:58.464104: step 26670, loss 0.548369.
Train: 2018-08-09T12:06:58.573482: step 26671, loss 0.529265.
Train: 2018-08-09T12:06:58.667181: step 26672, loss 0.513366.
Train: 2018-08-09T12:06:58.776560: step 26673, loss 0.595705.
Train: 2018-08-09T12:06:58.870287: step 26674, loss 0.562869.
Train: 2018-08-09T12:06:58.979637: step 26675, loss 0.562916.
Train: 2018-08-09T12:06:59.073365: step 26676, loss 0.480139.
Train: 2018-08-09T12:06:59.182683: step 26677, loss 0.578541.
Train: 2018-08-09T12:06:59.276442: step 26678, loss 0.562396.
Train: 2018-08-09T12:06:59.370139: step 26679, loss 0.578692.
Train: 2018-08-09T12:06:59.479519: step 26680, loss 0.595615.
Test: 2018-08-09T12:06:59.979371: step 26680, loss 0.548885.
Train: 2018-08-09T12:07:00.088721: step 26681, loss 0.545691.
Train: 2018-08-09T12:07:00.182478: step 26682, loss 0.463085.
Train: 2018-08-09T12:07:00.291828: step 26683, loss 0.546129.
Train: 2018-08-09T12:07:00.385556: step 26684, loss 0.51292.
Train: 2018-08-09T12:07:00.494905: step 26685, loss 0.662113.
Train: 2018-08-09T12:07:00.588633: step 26686, loss 0.528942.
Train: 2018-08-09T12:07:00.697977: step 26687, loss 0.578677.
Train: 2018-08-09T12:07:00.791710: step 26688, loss 0.495373.
Train: 2018-08-09T12:07:00.885438: step 26689, loss 0.562831.
Train: 2018-08-09T12:07:00.997007: step 26690, loss 0.495169.
Test: 2018-08-09T12:07:01.494732: step 26690, loss 0.549111.
Train: 2018-08-09T12:07:01.588482: step 26691, loss 0.579052.
Train: 2018-08-09T12:07:01.697802: step 26692, loss 0.578701.
Train: 2018-08-09T12:07:01.791561: step 26693, loss 0.596686.
Train: 2018-08-09T12:07:01.900909: step 26694, loss 0.493207.
Train: 2018-08-09T12:07:01.994605: step 26695, loss 0.527454.
Train: 2018-08-09T12:07:02.088364: step 26696, loss 0.495215.
Train: 2018-08-09T12:07:02.197713: step 26697, loss 0.457471.
Train: 2018-08-09T12:07:02.291441: step 26698, loss 0.652948.
Train: 2018-08-09T12:07:02.400761: step 26699, loss 0.596869.
Train: 2018-08-09T12:07:02.494518: step 26700, loss 0.565774.
Test: 2018-08-09T12:07:02.998738: step 26700, loss 0.545923.
Train: 2018-08-09T12:07:03.547912: step 26701, loss 0.668153.
Train: 2018-08-09T12:07:03.641638: step 26702, loss 0.613074.
Train: 2018-08-09T12:07:03.735368: step 26703, loss 0.493277.
Train: 2018-08-09T12:07:03.844750: step 26704, loss 0.632504.
Train: 2018-08-09T12:07:03.938444: step 26705, loss 0.529004.
Train: 2018-08-09T12:07:04.047823: step 26706, loss 0.511571.
Train: 2018-08-09T12:07:04.141520: step 26707, loss 0.561806.
Train: 2018-08-09T12:07:04.250902: step 26708, loss 0.563513.
Train: 2018-08-09T12:07:04.344629: step 26709, loss 0.546479.
Train: 2018-08-09T12:07:04.438358: step 26710, loss 0.62923.
Test: 2018-08-09T12:07:04.952462: step 26710, loss 0.548185.
Train: 2018-08-09T12:07:05.046220: step 26711, loss 0.661848.
Train: 2018-08-09T12:07:05.155568: step 26712, loss 0.513617.
Train: 2018-08-09T12:07:05.249299: step 26713, loss 0.529588.
Train: 2018-08-09T12:07:05.358617: step 26714, loss 0.578947.
Train: 2018-08-09T12:07:05.452343: step 26715, loss 0.628034.
Train: 2018-08-09T12:07:05.546101: step 26716, loss 0.611565.
Train: 2018-08-09T12:07:05.655451: step 26717, loss 0.611471.
Train: 2018-08-09T12:07:05.749178: step 26718, loss 0.643819.
Train: 2018-08-09T12:07:05.858528: step 26719, loss 0.546556.
Train: 2018-08-09T12:07:05.952256: step 26720, loss 0.546648.
Test: 2018-08-09T12:07:06.452109: step 26720, loss 0.551994.
Train: 2018-08-09T12:07:06.561487: step 26721, loss 0.659236.
Train: 2018-08-09T12:07:06.655216: step 26722, loss 0.578865.
Train: 2018-08-09T12:07:06.764535: step 26723, loss 0.530986.
Train: 2018-08-09T12:07:06.858292: step 26724, loss 0.642523.
Train: 2018-08-09T12:07:06.967641: step 26725, loss 0.499552.
Train: 2018-08-09T12:07:07.061338: step 26726, loss 0.468061.
Train: 2018-08-09T12:07:07.170719: step 26727, loss 0.594687.
Train: 2018-08-09T12:07:07.264451: step 26728, loss 0.594669.
Train: 2018-08-09T12:07:07.358174: step 26729, loss 0.563064.
Train: 2018-08-09T12:07:07.467494: step 26730, loss 0.547291.
Test: 2018-08-09T12:07:07.970137: step 26730, loss 0.55188.
Train: 2018-08-09T12:07:08.063895: step 26731, loss 0.515752.
Train: 2018-08-09T12:07:08.157622: step 26732, loss 0.531463.
Train: 2018-08-09T12:07:08.266971: step 26733, loss 0.658068.
Train: 2018-08-09T12:07:08.360698: step 26734, loss 0.626266.
Train: 2018-08-09T12:07:08.470018: step 26735, loss 0.626215.
Train: 2018-08-09T12:07:08.563777: step 26736, loss 0.531628.
Train: 2018-08-09T12:07:08.673095: step 26737, loss 0.62604.
Train: 2018-08-09T12:07:08.782469: step 26738, loss 0.578861.
Train: 2018-08-09T12:07:08.876202: step 26739, loss 0.437913.
Train: 2018-08-09T12:07:08.985552: step 26740, loss 0.625891.
Test: 2018-08-09T12:07:09.485404: step 26740, loss 0.552093.
Train: 2018-08-09T12:07:09.579164: step 26741, loss 0.547546.
Train: 2018-08-09T12:07:09.672889: step 26742, loss 0.578861.
Train: 2018-08-09T12:07:09.782239: step 26743, loss 0.56324.
Train: 2018-08-09T12:07:09.875967: step 26744, loss 0.578906.
Train: 2018-08-09T12:07:09.985316: step 26745, loss 0.563207.
Train: 2018-08-09T12:07:10.079044: step 26746, loss 0.578905.
Train: 2018-08-09T12:07:10.172771: step 26747, loss 0.484928.
Train: 2018-08-09T12:07:10.282121: step 26748, loss 0.594553.
Train: 2018-08-09T12:07:10.375819: step 26749, loss 0.563176.
Train: 2018-08-09T12:07:10.485199: step 26750, loss 0.531758.
Test: 2018-08-09T12:07:10.985051: step 26750, loss 0.551971.
Train: 2018-08-09T12:07:11.078808: step 26751, loss 0.563086.
Train: 2018-08-09T12:07:11.188159: step 26752, loss 0.594653.
Train: 2018-08-09T12:07:11.281856: step 26753, loss 0.421267.
Train: 2018-08-09T12:07:11.391205: step 26754, loss 0.642043.
Train: 2018-08-09T12:07:11.484962: step 26755, loss 0.5632.
Train: 2018-08-09T12:07:11.578691: step 26756, loss 0.547215.
Train: 2018-08-09T12:07:11.688041: step 26757, loss 0.690104.
Train: 2018-08-09T12:07:11.797388: step 26758, loss 0.531265.
Train: 2018-08-09T12:07:11.891116: step 26759, loss 0.531074.
Train: 2018-08-09T12:07:12.002790: step 26760, loss 0.562963.
Test: 2018-08-09T12:07:12.487052: step 26760, loss 0.550433.
Train: 2018-08-09T12:07:12.596432: step 26761, loss 0.467531.
Train: 2018-08-09T12:07:12.690158: step 26762, loss 0.62674.
Train: 2018-08-09T12:07:12.799478: step 26763, loss 0.578877.
Train: 2018-08-09T12:07:12.893235: step 26764, loss 0.530929.
Train: 2018-08-09T12:07:13.002584: step 26765, loss 0.626843.
Train: 2018-08-09T12:07:13.096312: step 26766, loss 0.530686.
Train: 2018-08-09T12:07:13.205661: step 26767, loss 0.498741.
Train: 2018-08-09T12:07:13.299393: step 26768, loss 0.530472.
Train: 2018-08-09T12:07:13.408739: step 26769, loss 0.530423.
Train: 2018-08-09T12:07:13.502467: step 26770, loss 0.530481.
Test: 2018-08-09T12:07:14.004620: step 26770, loss 0.550331.
Train: 2018-08-09T12:07:14.098376: step 26771, loss 0.595681.
Train: 2018-08-09T12:07:14.207727: step 26772, loss 0.595234.
Train: 2018-08-09T12:07:14.301423: step 26773, loss 0.513197.
Train: 2018-08-09T12:07:14.410773: step 26774, loss 0.711424.
Train: 2018-08-09T12:07:14.504501: step 26775, loss 0.612302.
Train: 2018-08-09T12:07:14.613850: step 26776, loss 0.480626.
Train: 2018-08-09T12:07:14.707609: step 26777, loss 0.644539.
Train: 2018-08-09T12:07:14.801306: step 26778, loss 0.611247.
Train: 2018-08-09T12:07:14.910685: step 26779, loss 0.54647.
Train: 2018-08-09T12:07:15.004413: step 26780, loss 0.578739.
Test: 2018-08-09T12:07:15.504296: step 26780, loss 0.548243.
Train: 2018-08-09T12:07:15.613644: step 26781, loss 0.562646.
Train: 2018-08-09T12:07:15.707372: step 26782, loss 0.450004.
Train: 2018-08-09T12:07:15.816722: step 26783, loss 0.579171.
Train: 2018-08-09T12:07:15.912726: step 26784, loss 0.579382.
Train: 2018-08-09T12:07:16.022107: step 26785, loss 0.530309.
Train: 2018-08-09T12:07:16.115834: step 26786, loss 0.595116.
Train: 2018-08-09T12:07:16.225183: step 26787, loss 0.530672.
Train: 2018-08-09T12:07:16.318880: step 26788, loss 0.562894.
Train: 2018-08-09T12:07:16.428260: step 26789, loss 0.459542.
Train: 2018-08-09T12:07:16.521988: step 26790, loss 0.594995.
Test: 2018-08-09T12:07:17.021842: step 26790, loss 0.546295.
Train: 2018-08-09T12:07:17.115568: step 26791, loss 0.481663.
Train: 2018-08-09T12:07:17.219830: step 26792, loss 0.579031.
Train: 2018-08-09T12:07:17.313559: step 26793, loss 0.497438.
Train: 2018-08-09T12:07:17.422908: step 26794, loss 0.595746.
Train: 2018-08-09T12:07:17.516606: step 26795, loss 0.562274.
Train: 2018-08-09T12:07:17.625954: step 26796, loss 0.513132.
Train: 2018-08-09T12:07:17.735335: step 26797, loss 0.546498.
Train: 2018-08-09T12:07:17.829062: step 26798, loss 0.579486.
Train: 2018-08-09T12:07:17.945732: step 26799, loss 0.496825.
Train: 2018-08-09T12:07:18.039468: step 26800, loss 0.662196.
Test: 2018-08-09T12:07:18.539317: step 26800, loss 0.548203.
Train: 2018-08-09T12:07:19.070472: step 26801, loss 0.546016.
Train: 2018-08-09T12:07:19.164199: step 26802, loss 0.612212.
Train: 2018-08-09T12:07:19.273548: step 26803, loss 0.628562.
Train: 2018-08-09T12:07:19.367276: step 26804, loss 0.413825.
Train: 2018-08-09T12:07:19.476625: step 26805, loss 0.711043.
Train: 2018-08-09T12:07:19.570323: step 26806, loss 0.578761.
Train: 2018-08-09T12:07:19.679702: step 26807, loss 0.644777.
Train: 2018-08-09T12:07:19.773400: step 26808, loss 0.513096.
Train: 2018-08-09T12:07:19.882779: step 26809, loss 0.546018.
Train: 2018-08-09T12:07:19.992100: step 26810, loss 0.496947.
Test: 2018-08-09T12:07:20.476361: step 26810, loss 0.547678.
Train: 2018-08-09T12:07:20.585740: step 26811, loss 0.496176.
Train: 2018-08-09T12:07:20.679467: step 26812, loss 0.628697.
Train: 2018-08-09T12:07:20.788816: step 26813, loss 0.529185.
Train: 2018-08-09T12:07:20.882543: step 26814, loss 0.580365.
Train: 2018-08-09T12:07:20.994229: step 26815, loss 0.529188.
Train: 2018-08-09T12:07:21.103585: step 26816, loss 0.54681.
Train: 2018-08-09T12:07:21.197312: step 26817, loss 0.611764.
Train: 2018-08-09T12:07:21.306661: step 26818, loss 0.594944.
Train: 2018-08-09T12:07:21.400389: step 26819, loss 0.496023.
Train: 2018-08-09T12:07:21.509738: step 26820, loss 0.612166.
Test: 2018-08-09T12:07:22.009598: step 26820, loss 0.546952.
Train: 2018-08-09T12:07:22.103349: step 26821, loss 0.513842.
Train: 2018-08-09T12:07:22.212698: step 26822, loss 0.577968.
Train: 2018-08-09T12:07:22.306396: step 26823, loss 0.5798.
Train: 2018-08-09T12:07:22.415745: step 26824, loss 0.513294.
Train: 2018-08-09T12:07:22.509472: step 26825, loss 0.562247.
Train: 2018-08-09T12:07:22.618821: step 26826, loss 0.546885.
Train: 2018-08-09T12:07:22.712581: step 26827, loss 0.495482.
Train: 2018-08-09T12:07:22.821929: step 26828, loss 0.612784.
Train: 2018-08-09T12:07:22.917032: step 26829, loss 0.662756.
Train: 2018-08-09T12:07:23.026382: step 26830, loss 0.611561.
Test: 2018-08-09T12:07:23.510642: step 26830, loss 0.547109.
Train: 2018-08-09T12:07:23.620022: step 26831, loss 0.529328.
Train: 2018-08-09T12:07:23.729342: step 26832, loss 0.628162.
Train: 2018-08-09T12:07:23.823099: step 26833, loss 0.513303.
Train: 2018-08-09T12:07:23.932448: step 26834, loss 0.644061.
Train: 2018-08-09T12:07:24.026176: step 26835, loss 0.578958.
Train: 2018-08-09T12:07:24.135525: step 26836, loss 0.595209.
Train: 2018-08-09T12:07:24.229223: step 26837, loss 0.594942.
Train: 2018-08-09T12:07:24.338602: step 26838, loss 0.595023.
Train: 2018-08-09T12:07:24.432330: step 26839, loss 0.611158.
Train: 2018-08-09T12:07:24.541649: step 26840, loss 0.546722.
Test: 2018-08-09T12:07:25.043806: step 26840, loss 0.549016.
Train: 2018-08-09T12:07:25.137555: step 26841, loss 0.482654.
Train: 2018-08-09T12:07:25.246904: step 26842, loss 0.594826.
Train: 2018-08-09T12:07:25.340637: step 26843, loss 0.498846.
Train: 2018-08-09T12:07:25.449982: step 26844, loss 0.482743.
Train: 2018-08-09T12:07:25.559331: step 26845, loss 0.514841.
Train: 2018-08-09T12:07:25.653059: step 26846, loss 0.626931.
Train: 2018-08-09T12:07:25.762408: step 26847, loss 0.546604.
Train: 2018-08-09T12:07:25.856139: step 26848, loss 0.643328.
Train: 2018-08-09T12:07:25.965456: step 26849, loss 0.659557.
Train: 2018-08-09T12:07:26.074804: step 26850, loss 0.626798.
Test: 2018-08-09T12:07:26.559067: step 26850, loss 0.547247.
Train: 2018-08-09T12:07:26.668445: step 26851, loss 0.723077.
Train: 2018-08-09T12:07:26.762172: step 26852, loss 0.515022.
Train: 2018-08-09T12:07:26.871523: step 26853, loss 0.515203.
Train: 2018-08-09T12:07:26.982272: step 26854, loss 0.579016.
Train: 2018-08-09T12:07:27.075998: step 26855, loss 0.610363.
Train: 2018-08-09T12:07:27.185347: step 26856, loss 0.547296.
Train: 2018-08-09T12:07:27.279045: step 26857, loss 0.578839.
Train: 2018-08-09T12:07:27.388420: step 26858, loss 0.563174.
Train: 2018-08-09T12:07:27.497776: step 26859, loss 0.516102.
Train: 2018-08-09T12:07:27.591471: step 26860, loss 0.484896.
Test: 2018-08-09T12:07:28.091354: step 26860, loss 0.551436.
Train: 2018-08-09T12:07:28.200702: step 26861, loss 0.657277.
Train: 2018-08-09T12:07:28.310051: step 26862, loss 0.563217.
Train: 2018-08-09T12:07:28.403780: step 26863, loss 0.578976.
Train: 2018-08-09T12:07:28.513159: step 26864, loss 0.673112.
Train: 2018-08-09T12:07:28.606887: step 26865, loss 0.579001.
Train: 2018-08-09T12:07:28.716205: step 26866, loss 0.516522.
Train: 2018-08-09T12:07:28.825585: step 26867, loss 0.594439.
Train: 2018-08-09T12:07:28.920792: step 26868, loss 0.563306.
Train: 2018-08-09T12:07:29.022142: step 26869, loss 0.625552.
Train: 2018-08-09T12:07:29.131466: step 26870, loss 0.501283.
Test: 2018-08-09T12:07:29.615729: step 26870, loss 0.550025.
Train: 2018-08-09T12:07:29.725077: step 26871, loss 0.516848.
Train: 2018-08-09T12:07:29.834456: step 26872, loss 0.485727.
Train: 2018-08-09T12:07:29.928155: step 26873, loss 0.641139.
Train: 2018-08-09T12:07:30.037534: step 26874, loss 0.610029.
Train: 2018-08-09T12:07:30.146883: step 26875, loss 0.687867.
Train: 2018-08-09T12:07:30.240610: step 26876, loss 0.578909.
Train: 2018-08-09T12:07:30.349960: step 26877, loss 0.485817.
Train: 2018-08-09T12:07:30.459309: step 26878, loss 0.454798.
Train: 2018-08-09T12:07:30.553037: step 26879, loss 0.501207.
Train: 2018-08-09T12:07:30.662387: step 26880, loss 0.532153.
Test: 2018-08-09T12:07:31.170937: step 26880, loss 0.54921.
Train: 2018-08-09T12:07:31.264695: step 26881, loss 0.469507.
Train: 2018-08-09T12:07:31.374014: step 26882, loss 0.641636.
Train: 2018-08-09T12:07:31.483362: step 26883, loss 0.594604.
Train: 2018-08-09T12:07:31.577120: step 26884, loss 0.515808.
Train: 2018-08-09T12:07:31.686469: step 26885, loss 0.642091.
Train: 2018-08-09T12:07:31.795819: step 26886, loss 0.531379.
Train: 2018-08-09T12:07:31.889547: step 26887, loss 0.531295.
Train: 2018-08-09T12:07:31.998896: step 26888, loss 0.531196.
Train: 2018-08-09T12:07:32.108246: step 26889, loss 0.642564.
Train: 2018-08-09T12:07:32.201973: step 26890, loss 0.483195.
Test: 2018-08-09T12:07:32.701859: step 26890, loss 0.550325.
Train: 2018-08-09T12:07:32.811204: step 26891, loss 0.482974.
Train: 2018-08-09T12:07:32.967389: step 26892, loss 0.562837.
Train: 2018-08-09T12:07:33.076737: step 26893, loss 0.562802.
Train: 2018-08-09T12:07:33.170465: step 26894, loss 0.595006.
Train: 2018-08-09T12:07:33.279814: step 26895, loss 0.514281.
Train: 2018-08-09T12:07:33.389194: step 26896, loss 0.530341.
Train: 2018-08-09T12:07:33.498542: step 26897, loss 0.57893.
Train: 2018-08-09T12:07:33.607892: step 26898, loss 0.595174.
Train: 2018-08-09T12:07:33.701589: step 26899, loss 0.595257.
Train: 2018-08-09T12:07:33.810939: step 26900, loss 0.546277.
Test: 2018-08-09T12:07:34.313114: step 26900, loss 0.547913.
Train: 2018-08-09T12:07:34.880310: step 26901, loss 0.59526.
Train: 2018-08-09T12:07:34.989628: step 26902, loss 0.578922.
Train: 2018-08-09T12:07:35.098979: step 26903, loss 0.529923.
Train: 2018-08-09T12:07:35.192736: step 26904, loss 0.595305.
Train: 2018-08-09T12:07:35.302085: step 26905, loss 0.562592.
Train: 2018-08-09T12:07:35.411434: step 26906, loss 0.595327.
Train: 2018-08-09T12:07:35.520785: step 26907, loss 0.611693.
Train: 2018-08-09T12:07:35.614511: step 26908, loss 0.595269.
Train: 2018-08-09T12:07:35.723861: step 26909, loss 0.578941.
Train: 2018-08-09T12:07:35.833213: step 26910, loss 0.611495.
Test: 2018-08-09T12:07:36.333062: step 26910, loss 0.549282.
Train: 2018-08-09T12:07:36.426820: step 26911, loss 0.513921.
Train: 2018-08-09T12:07:36.536169: step 26912, loss 0.481527.
Train: 2018-08-09T12:07:36.645488: step 26913, loss 0.578905.
Train: 2018-08-09T12:07:36.754867: step 26914, loss 0.627599.
Train: 2018-08-09T12:07:36.864214: step 26915, loss 0.659981.
Train: 2018-08-09T12:07:36.957948: step 26916, loss 0.627436.
Train: 2018-08-09T12:07:37.074392: step 26917, loss 0.514342.
Train: 2018-08-09T12:07:37.168042: step 26918, loss 0.466141.
Train: 2018-08-09T12:07:37.277389: step 26919, loss 0.562775.
Train: 2018-08-09T12:07:37.386709: step 26920, loss 0.514486.
Test: 2018-08-09T12:07:37.886592: step 26920, loss 0.547681.
Train: 2018-08-09T12:07:37.995970: step 26921, loss 0.594982.
Train: 2018-08-09T12:07:38.105323: step 26922, loss 0.627197.
Train: 2018-08-09T12:07:38.199047: step 26923, loss 0.643249.
Train: 2018-08-09T12:07:38.308366: step 26924, loss 0.530678.
Train: 2018-08-09T12:07:38.417716: step 26925, loss 0.482589.
Train: 2018-08-09T12:07:38.527099: step 26926, loss 0.56282.
Train: 2018-08-09T12:07:38.636444: step 26927, loss 0.530714.
Train: 2018-08-09T12:07:38.745794: step 26928, loss 0.62706.
Train: 2018-08-09T12:07:38.839521: step 26929, loss 0.643103.
Train: 2018-08-09T12:07:38.948870: step 26930, loss 0.450566.
Test: 2018-08-09T12:07:39.448723: step 26930, loss 0.550216.
Train: 2018-08-09T12:07:39.558071: step 26931, loss 0.562821.
Train: 2018-08-09T12:07:39.667451: step 26932, loss 0.56281.
Train: 2018-08-09T12:07:39.776800: step 26933, loss 0.482494.
Train: 2018-08-09T12:07:39.886150: step 26934, loss 0.578858.
Train: 2018-08-09T12:07:39.982169: step 26935, loss 0.530459.
Train: 2018-08-09T12:07:40.091547: step 26936, loss 0.530472.
Train: 2018-08-09T12:07:40.200867: step 26937, loss 0.546112.
Train: 2018-08-09T12:07:40.310216: step 26938, loss 0.562273.
Train: 2018-08-09T12:07:40.419565: step 26939, loss 0.531303.
Train: 2018-08-09T12:07:40.528938: step 26940, loss 0.512436.
Test: 2018-08-09T12:07:41.028808: step 26940, loss 0.546891.
Train: 2018-08-09T12:07:41.122525: step 26941, loss 0.529264.
Train: 2018-08-09T12:07:41.231874: step 26942, loss 0.529289.
Train: 2018-08-09T12:07:41.341253: step 26943, loss 0.57799.
Train: 2018-08-09T12:07:41.450602: step 26944, loss 0.649453.
Train: 2018-08-09T12:07:41.559951: step 26945, loss 0.613724.
Train: 2018-08-09T12:07:41.669300: step 26946, loss 0.577776.
Train: 2018-08-09T12:07:41.778650: step 26947, loss 0.512733.
Train: 2018-08-09T12:07:41.872378: step 26948, loss 0.512839.
Train: 2018-08-09T12:07:41.984025: step 26949, loss 0.613588.
Train: 2018-08-09T12:07:42.093375: step 26950, loss 0.614071.
Test: 2018-08-09T12:07:42.593283: step 26950, loss 0.548207.
Train: 2018-08-09T12:07:42.702607: step 26951, loss 0.512636.
Train: 2018-08-09T12:07:42.811956: step 26952, loss 0.644534.
Train: 2018-08-09T12:07:42.921305: step 26953, loss 0.513198.
Train: 2018-08-09T12:07:43.030685: step 26954, loss 0.513192.
Train: 2018-08-09T12:07:43.140033: step 26955, loss 0.628242.
Train: 2018-08-09T12:07:43.249383: step 26956, loss 0.628092.
Train: 2018-08-09T12:07:43.358733: step 26957, loss 0.49723.
Train: 2018-08-09T12:07:43.452460: step 26958, loss 0.644264.
Train: 2018-08-09T12:07:43.561809: step 26959, loss 0.497404.
Train: 2018-08-09T12:07:43.671158: step 26960, loss 0.497446.
Test: 2018-08-09T12:07:44.171651: step 26960, loss 0.549198.
Train: 2018-08-09T12:07:44.281030: step 26961, loss 0.497415.
Train: 2018-08-09T12:07:44.390379: step 26962, loss 0.595256.
Train: 2018-08-09T12:07:44.499729: step 26963, loss 0.578937.
Train: 2018-08-09T12:07:44.609077: step 26964, loss 0.546259.
Train: 2018-08-09T12:07:44.718397: step 26965, loss 0.513547.
Train: 2018-08-09T12:07:44.812154: step 26966, loss 0.497109.
Train: 2018-08-09T12:07:44.921504: step 26967, loss 0.562562.
Train: 2018-08-09T12:07:45.030853: step 26968, loss 0.578973.
Train: 2018-08-09T12:07:45.140202: step 26969, loss 0.628331.
Train: 2018-08-09T12:07:45.249552: step 26970, loss 0.513172.
Test: 2018-08-09T12:07:45.749403: step 26970, loss 0.548962.
Train: 2018-08-09T12:07:45.858782: step 26971, loss 0.595458.
Train: 2018-08-09T12:07:45.968102: step 26972, loss 0.529575.
Train: 2018-08-09T12:07:46.077481: step 26973, loss 0.611969.
Train: 2018-08-09T12:07:46.186830: step 26974, loss 0.694384.
Train: 2018-08-09T12:07:46.296150: step 26975, loss 0.513179.
Train: 2018-08-09T12:07:46.389908: step 26976, loss 0.496806.
Train: 2018-08-09T12:07:46.499257: step 26977, loss 0.529677.
Train: 2018-08-09T12:07:46.608577: step 26978, loss 0.546101.
Train: 2018-08-09T12:07:46.717955: step 26979, loss 0.546088.
Train: 2018-08-09T12:07:46.827275: step 26980, loss 0.644818.
Test: 2018-08-09T12:07:47.329624: step 26980, loss 0.548988.
Train: 2018-08-09T12:07:47.438972: step 26981, loss 0.513188.
Train: 2018-08-09T12:07:47.548352: step 26982, loss 0.496728.
Train: 2018-08-09T12:07:47.657702: step 26983, loss 0.463718.
Train: 2018-08-09T12:07:47.767051: step 26984, loss 0.595515.
Train: 2018-08-09T12:07:47.876400: step 26985, loss 0.545964.
Train: 2018-08-09T12:07:47.985747: step 26986, loss 0.562481.
Train: 2018-08-09T12:07:48.079477: step 26987, loss 0.512742.
Train: 2018-08-09T12:07:48.188829: step 26988, loss 0.54584.
Train: 2018-08-09T12:07:48.298175: step 26989, loss 0.628993.
Train: 2018-08-09T12:07:48.407525: step 26990, loss 0.512494.
Test: 2018-08-09T12:07:48.909704: step 26990, loss 0.549964.
Train: 2018-08-09T12:07:49.019084: step 26991, loss 0.562406.
Train: 2018-08-09T12:07:49.128433: step 26992, loss 0.529055.
Train: 2018-08-09T12:07:49.237783: step 26993, loss 0.545723.
Train: 2018-08-09T12:07:49.347138: step 26994, loss 0.612625.
Train: 2018-08-09T12:07:49.440859: step 26995, loss 0.6294.
Train: 2018-08-09T12:07:49.565824: step 26996, loss 0.545689.
Train: 2018-08-09T12:07:49.675150: step 26997, loss 0.528979.
Train: 2018-08-09T12:07:49.784529: step 26998, loss 0.528962.
Train: 2018-08-09T12:07:49.893877: step 26999, loss 0.512233.
Train: 2018-08-09T12:07:49.987575: step 27000, loss 0.579136.
Test: 2018-08-09T12:07:50.487459: step 27000, loss 0.548602.
Train: 2018-08-09T12:07:51.081730: step 27001, loss 0.629395.
Train: 2018-08-09T12:07:51.191073: step 27002, loss 0.562402.
Train: 2018-08-09T12:07:51.300429: step 27003, loss 0.512135.
Train: 2018-08-09T12:07:51.409778: step 27004, loss 0.478746.
Train: 2018-08-09T12:07:51.519130: step 27005, loss 0.562146.
Train: 2018-08-09T12:07:51.612825: step 27006, loss 0.528959.
Train: 2018-08-09T12:07:51.737819: step 27007, loss 0.427458.
Train: 2018-08-09T12:07:51.831522: step 27008, loss 0.57966.
Train: 2018-08-09T12:07:51.940874: step 27009, loss 0.698803.
Train: 2018-08-09T12:07:52.050253: step 27010, loss 0.528336.
Test: 2018-08-09T12:07:52.550105: step 27010, loss 0.546463.
Train: 2018-08-09T12:07:52.659454: step 27011, loss 0.544911.
Train: 2018-08-09T12:07:52.768803: step 27012, loss 0.561939.
Train: 2018-08-09T12:07:52.878182: step 27013, loss 0.562303.
Train: 2018-08-09T12:07:52.987532: step 27014, loss 0.579191.
Train: 2018-08-09T12:07:53.081228: step 27015, loss 0.494343.
Train: 2018-08-09T12:07:53.190608: step 27016, loss 0.613588.
Train: 2018-08-09T12:07:53.299957: step 27017, loss 0.544808.
Train: 2018-08-09T12:07:53.409276: step 27018, loss 0.528264.
Train: 2018-08-09T12:07:53.518659: step 27019, loss 0.45957.
Train: 2018-08-09T12:07:53.628005: step 27020, loss 0.459699.
Test: 2018-08-09T12:07:54.130104: step 27020, loss 0.548113.
Train: 2018-08-09T12:07:54.239482: step 27021, loss 0.527349.
Train: 2018-08-09T12:07:54.348834: step 27022, loss 0.529974.
Train: 2018-08-09T12:07:54.458180: step 27023, loss 0.507805.
Train: 2018-08-09T12:07:54.567533: step 27024, loss 0.423742.
Train: 2018-08-09T12:07:54.676848: step 27025, loss 0.613159.
Train: 2018-08-09T12:07:54.786228: step 27026, loss 0.582142.
Train: 2018-08-09T12:07:54.895577: step 27027, loss 0.554023.
Train: 2018-08-09T12:07:54.989275: step 27028, loss 0.566754.
Train: 2018-08-09T12:07:55.114277: step 27029, loss 0.526641.
Train: 2018-08-09T12:07:55.208004: step 27030, loss 0.417468.
Test: 2018-08-09T12:07:55.707856: step 27030, loss 0.547796.
Train: 2018-08-09T12:07:55.817238: step 27031, loss 0.620325.
Train: 2018-08-09T12:07:55.928056: step 27032, loss 0.528973.
Train: 2018-08-09T12:07:56.037408: step 27033, loss 0.575677.
Train: 2018-08-09T12:07:56.146754: step 27034, loss 0.543756.
Train: 2018-08-09T12:07:56.256107: step 27035, loss 0.582044.
Train: 2018-08-09T12:07:56.365453: step 27036, loss 0.505317.
Train: 2018-08-09T12:07:56.459181: step 27037, loss 0.510916.
Train: 2018-08-09T12:07:56.568531: step 27038, loss 0.589926.
Train: 2018-08-09T12:07:56.677879: step 27039, loss 0.526398.
Train: 2018-08-09T12:07:56.787229: step 27040, loss 0.569582.
Test: 2018-08-09T12:07:57.287082: step 27040, loss 0.546415.
Train: 2018-08-09T12:07:57.396430: step 27041, loss 0.541548.
Train: 2018-08-09T12:07:57.490188: step 27042, loss 0.544573.
Train: 2018-08-09T12:07:57.599537: step 27043, loss 0.579477.
Train: 2018-08-09T12:07:57.708887: step 27044, loss 0.56041.
Train: 2018-08-09T12:07:57.818236: step 27045, loss 0.650856.
Train: 2018-08-09T12:07:57.929848: step 27046, loss 0.649815.
Train: 2018-08-09T12:07:58.039231: step 27047, loss 0.631897.
Train: 2018-08-09T12:07:58.148570: step 27048, loss 0.613994.
Train: 2018-08-09T12:07:58.242304: step 27049, loss 0.682485.
Train: 2018-08-09T12:07:58.351653: step 27050, loss 0.646962.
Test: 2018-08-09T12:07:58.851507: step 27050, loss 0.546658.
Train: 2018-08-09T12:07:58.960885: step 27051, loss 0.528925.
Train: 2018-08-09T12:07:59.054612: step 27052, loss 0.562406.
Train: 2018-08-09T12:07:59.163966: step 27053, loss 0.579034.
Train: 2018-08-09T12:07:59.273311: step 27054, loss 0.5956.
Train: 2018-08-09T12:07:59.382660: step 27055, loss 0.480415.
Train: 2018-08-09T12:07:59.492009: step 27056, loss 0.578933.
Train: 2018-08-09T12:07:59.585708: step 27057, loss 0.611582.
Train: 2018-08-09T12:07:59.695057: step 27058, loss 0.481295.
Train: 2018-08-09T12:07:59.797433: step 27059, loss 0.497715.
Train: 2018-08-09T12:07:59.915058: step 27060, loss 0.660023.
Test: 2018-08-09T12:08:00.399331: step 27060, loss 0.549985.
Train: 2018-08-09T12:08:00.508669: step 27061, loss 0.595084.
Train: 2018-08-09T12:08:00.618048: step 27062, loss 0.530431.
Train: 2018-08-09T12:08:00.727399: step 27063, loss 0.530506.
Train: 2018-08-09T12:08:00.836746: step 27064, loss 0.594995.
Train: 2018-08-09T12:08:00.946066: step 27065, loss 0.530612.
Train: 2018-08-09T12:08:01.055450: step 27066, loss 0.562796.
Train: 2018-08-09T12:08:01.149173: step 27067, loss 0.594937.
Train: 2018-08-09T12:08:01.258491: step 27068, loss 0.514656.
Train: 2018-08-09T12:08:01.367871: step 27069, loss 0.514671.
Train: 2018-08-09T12:08:01.477224: step 27070, loss 0.594955.
Test: 2018-08-09T12:08:01.961453: step 27070, loss 0.549572.
Train: 2018-08-09T12:08:02.070830: step 27071, loss 0.562829.
Train: 2018-08-09T12:08:02.180180: step 27072, loss 0.739606.
Train: 2018-08-09T12:08:02.289529: step 27073, loss 0.594895.
Train: 2018-08-09T12:08:02.383257: step 27074, loss 0.546887.
Train: 2018-08-09T12:08:02.492606: step 27075, loss 0.530989.
Train: 2018-08-09T12:08:02.601957: step 27076, loss 0.610735.
Train: 2018-08-09T12:08:02.711304: step 27077, loss 0.578859.
Train: 2018-08-09T12:08:02.820654: step 27078, loss 0.467673.
Train: 2018-08-09T12:08:02.915724: step 27079, loss 0.674152.
Train: 2018-08-09T12:08:03.025104: step 27080, loss 0.610572.
Test: 2018-08-09T12:08:03.524956: step 27080, loss 0.548807.
Train: 2018-08-09T12:08:03.634306: step 27081, loss 0.53139.
Train: 2018-08-09T12:08:03.728063: step 27082, loss 0.594654.
Train: 2018-08-09T12:08:03.837413: step 27083, loss 0.610433.
Train: 2018-08-09T12:08:03.946762: step 27084, loss 0.500156.
Train: 2018-08-09T12:08:04.056111: step 27085, loss 0.563129.
Train: 2018-08-09T12:08:04.165460: step 27086, loss 0.547401.
Train: 2018-08-09T12:08:04.259189: step 27087, loss 0.468753.
Train: 2018-08-09T12:08:04.368537: step 27088, loss 0.578845.
Train: 2018-08-09T12:08:04.477858: step 27089, loss 0.578823.
Train: 2018-08-09T12:08:04.571615: step 27090, loss 0.546265.
Test: 2018-08-09T12:08:05.072818: step 27090, loss 0.548226.
Train: 2018-08-09T12:08:05.182196: step 27091, loss 0.563093.
Train: 2018-08-09T12:08:05.291516: step 27092, loss 0.578839.
Train: 2018-08-09T12:08:05.400865: step 27093, loss 0.594731.
Train: 2018-08-09T12:08:05.494626: step 27094, loss 0.59467.
Train: 2018-08-09T12:08:05.603941: step 27095, loss 0.610523.
Train: 2018-08-09T12:08:05.713292: step 27096, loss 0.54713.
Train: 2018-08-09T12:08:05.807020: step 27097, loss 0.563021.
Train: 2018-08-09T12:08:05.916369: step 27098, loss 0.54713.
Train: 2018-08-09T12:08:06.025748: step 27099, loss 0.531218.
Train: 2018-08-09T12:08:06.135097: step 27100, loss 0.562872.
Test: 2018-08-09T12:08:06.634961: step 27100, loss 0.548015.
Train: 2018-08-09T12:08:07.214684: step 27101, loss 0.49898.
Train: 2018-08-09T12:08:07.324004: step 27102, loss 0.563292.
Train: 2018-08-09T12:08:07.433383: step 27103, loss 0.595134.
Train: 2018-08-09T12:08:07.527111: step 27104, loss 0.579981.
Train: 2018-08-09T12:08:07.636461: step 27105, loss 0.563278.
Train: 2018-08-09T12:08:07.745779: step 27106, loss 0.530109.
Train: 2018-08-09T12:08:07.839537: step 27107, loss 0.54682.
Train: 2018-08-09T12:08:07.948857: step 27108, loss 0.530383.
Train: 2018-08-09T12:08:08.058237: step 27109, loss 0.562363.
Train: 2018-08-09T12:08:08.151963: step 27110, loss 0.562418.
Test: 2018-08-09T12:08:08.653483: step 27110, loss 0.548594.
Train: 2018-08-09T12:08:08.762831: step 27111, loss 0.578855.
Train: 2018-08-09T12:08:08.872181: step 27112, loss 0.563089.
Train: 2018-08-09T12:08:08.981559: step 27113, loss 0.597043.
Train: 2018-08-09T12:08:09.075258: step 27114, loss 0.54561.
Train: 2018-08-09T12:08:09.200257: step 27115, loss 0.530575.
Train: 2018-08-09T12:08:09.293986: step 27116, loss 0.546189.
Train: 2018-08-09T12:08:09.403335: step 27117, loss 0.49753.
Train: 2018-08-09T12:08:09.512684: step 27118, loss 0.530126.
Train: 2018-08-09T12:08:09.606412: step 27119, loss 0.662352.
Train: 2018-08-09T12:08:09.715764: step 27120, loss 0.710706.
Test: 2018-08-09T12:08:10.215648: step 27120, loss 0.549181.
Train: 2018-08-09T12:08:10.324993: step 27121, loss 0.480947.
Train: 2018-08-09T12:08:10.418721: step 27122, loss 0.57827.
Train: 2018-08-09T12:08:10.528041: step 27123, loss 0.514188.
Train: 2018-08-09T12:08:10.637419: step 27124, loss 0.579187.
Train: 2018-08-09T12:08:10.731148: step 27125, loss 0.54571.
Train: 2018-08-09T12:08:10.846496: step 27126, loss 0.595266.
Train: 2018-08-09T12:08:10.951658: step 27127, loss 0.513845.
Train: 2018-08-09T12:08:11.045386: step 27128, loss 0.545473.
Train: 2018-08-09T12:08:11.154739: step 27129, loss 0.562602.
Train: 2018-08-09T12:08:11.264084: step 27130, loss 0.629099.
Test: 2018-08-09T12:08:11.763966: step 27130, loss 0.549029.
Train: 2018-08-09T12:08:11.857695: step 27131, loss 0.644692.
Train: 2018-08-09T12:08:11.967044: step 27132, loss 0.514224.
Train: 2018-08-09T12:08:12.060772: step 27133, loss 0.611089.
Train: 2018-08-09T12:08:12.170121: step 27134, loss 0.562554.
Train: 2018-08-09T12:08:12.279474: step 27135, loss 0.530777.
Train: 2018-08-09T12:08:12.373198: step 27136, loss 0.562688.
Train: 2018-08-09T12:08:12.482547: step 27137, loss 0.546527.
Train: 2018-08-09T12:08:12.576275: step 27138, loss 0.497997.
Train: 2018-08-09T12:08:12.685619: step 27139, loss 0.514438.
Train: 2018-08-09T12:08:12.794974: step 27140, loss 0.562789.
Test: 2018-08-09T12:08:13.294827: step 27140, loss 0.549931.
Train: 2018-08-09T12:08:13.388554: step 27141, loss 0.61125.
Train: 2018-08-09T12:08:13.497933: step 27142, loss 0.611261.
Train: 2018-08-09T12:08:13.607282: step 27143, loss 0.562772.
Train: 2018-08-09T12:08:13.701010: step 27144, loss 0.675993.
Train: 2018-08-09T12:08:13.810359: step 27145, loss 0.56284.
Train: 2018-08-09T12:08:13.919709: step 27146, loss 0.578841.
Train: 2018-08-09T12:08:14.013439: step 27147, loss 0.595069.
Train: 2018-08-09T12:08:14.122755: step 27148, loss 0.51464.
Train: 2018-08-09T12:08:14.216514: step 27149, loss 0.530761.
Train: 2018-08-09T12:08:14.325863: step 27150, loss 0.546814.
Test: 2018-08-09T12:08:14.825715: step 27150, loss 0.550256.
Train: 2018-08-09T12:08:14.919473: step 27151, loss 0.546823.
Train: 2018-08-09T12:08:15.028826: step 27152, loss 0.642949.
Train: 2018-08-09T12:08:15.138142: step 27153, loss 0.546844.
Train: 2018-08-09T12:08:15.231899: step 27154, loss 0.610866.
Train: 2018-08-09T12:08:15.341248: step 27155, loss 0.56288.
Train: 2018-08-09T12:08:15.450598: step 27156, loss 0.610788.
Train: 2018-08-09T12:08:15.544326: step 27157, loss 0.642624.
Train: 2018-08-09T12:08:15.653646: step 27158, loss 0.562935.
Train: 2018-08-09T12:08:15.747403: step 27159, loss 0.515386.
Train: 2018-08-09T12:08:15.856723: step 27160, loss 0.51554.
Test: 2018-08-09T12:08:16.358988: step 27160, loss 0.549948.
Train: 2018-08-09T12:08:16.452745: step 27161, loss 0.515447.
Train: 2018-08-09T12:08:16.562095: step 27162, loss 0.531289.
Train: 2018-08-09T12:08:16.655822: step 27163, loss 0.547098.
Train: 2018-08-09T12:08:16.765172: step 27164, loss 0.546951.
Train: 2018-08-09T12:08:16.874490: step 27165, loss 0.642692.
Train: 2018-08-09T12:08:16.968250: step 27166, loss 0.64271.
Train: 2018-08-09T12:08:17.077568: step 27167, loss 0.562925.
Train: 2018-08-09T12:08:17.171330: step 27168, loss 0.578838.
Train: 2018-08-09T12:08:17.280675: step 27169, loss 0.531116.
Train: 2018-08-09T12:08:17.390024: step 27170, loss 0.467459.
Test: 2018-08-09T12:08:17.889878: step 27170, loss 0.549171.
Train: 2018-08-09T12:08:17.984258: step 27171, loss 0.419322.
Train: 2018-08-09T12:08:18.093608: step 27172, loss 0.562778.
Train: 2018-08-09T12:08:18.187306: step 27173, loss 0.594405.
Train: 2018-08-09T12:08:18.296655: step 27174, loss 0.610333.
Train: 2018-08-09T12:08:18.406034: step 27175, loss 0.595259.
Train: 2018-08-09T12:08:18.499732: step 27176, loss 0.479245.
Train: 2018-08-09T12:08:18.609081: step 27177, loss 0.679138.
Train: 2018-08-09T12:08:18.702838: step 27178, loss 0.548523.
Train: 2018-08-09T12:08:18.812182: step 27179, loss 0.610642.
Train: 2018-08-09T12:08:18.921537: step 27180, loss 0.646389.
Test: 2018-08-09T12:08:19.421390: step 27180, loss 0.550697.
Train: 2018-08-09T12:08:19.515117: step 27181, loss 0.512713.
Train: 2018-08-09T12:08:19.624467: step 27182, loss 0.594932.
Train: 2018-08-09T12:08:19.718194: step 27183, loss 0.547459.
Train: 2018-08-09T12:08:19.827573: step 27184, loss 0.578604.
Train: 2018-08-09T12:08:19.936923: step 27185, loss 0.562319.
Train: 2018-08-09T12:08:20.030651: step 27186, loss 0.562522.
Train: 2018-08-09T12:08:20.140000: step 27187, loss 0.579219.
Train: 2018-08-09T12:08:20.233728: step 27188, loss 0.498404.
Train: 2018-08-09T12:08:20.343083: step 27189, loss 0.643508.
Train: 2018-08-09T12:08:20.452426: step 27190, loss 0.594597.
Test: 2018-08-09T12:08:20.938955: step 27190, loss 0.547276.
Train: 2018-08-09T12:08:21.048333: step 27191, loss 0.642602.
Train: 2018-08-09T12:08:21.157682: step 27192, loss 0.579001.
Train: 2018-08-09T12:08:21.251412: step 27193, loss 0.483587.
Train: 2018-08-09T12:08:21.360759: step 27194, loss 0.499382.
Train: 2018-08-09T12:08:21.470103: step 27195, loss 0.562545.
Train: 2018-08-09T12:08:21.563838: step 27196, loss 0.562556.
Train: 2018-08-09T12:08:21.673185: step 27197, loss 0.594381.
Train: 2018-08-09T12:08:21.766913: step 27198, loss 0.547666.
Train: 2018-08-09T12:08:21.876264: step 27199, loss 0.561052.
Train: 2018-08-09T12:08:21.969961: step 27200, loss 0.530491.
Test: 2018-08-09T12:08:22.469843: step 27200, loss 0.548019.
Train: 2018-08-09T12:08:23.032241: step 27201, loss 0.548289.
Train: 2018-08-09T12:08:23.141560: step 27202, loss 0.510013.
Train: 2018-08-09T12:08:23.235318: step 27203, loss 0.608533.
Train: 2018-08-09T12:08:23.344666: step 27204, loss 0.682554.
Train: 2018-08-09T12:08:23.438391: step 27205, loss 0.631135.
Train: 2018-08-09T12:08:23.547744: step 27206, loss 0.528859.
Train: 2018-08-09T12:08:23.657093: step 27207, loss 0.594999.
Train: 2018-08-09T12:08:23.750823: step 27208, loss 0.563936.
Train: 2018-08-09T12:08:23.860140: step 27209, loss 0.529251.
Train: 2018-08-09T12:08:23.953898: step 27210, loss 0.549241.
Test: 2018-08-09T12:08:24.453751: step 27210, loss 0.548713.
Train: 2018-08-09T12:08:24.563100: step 27211, loss 0.597088.
Train: 2018-08-09T12:08:24.673950: step 27212, loss 0.594968.
Train: 2018-08-09T12:08:24.776786: step 27213, loss 0.597659.
Train: 2018-08-09T12:08:24.870511: step 27214, loss 0.531511.
Train: 2018-08-09T12:08:24.979862: step 27215, loss 0.594778.
Train: 2018-08-09T12:08:25.073588: step 27216, loss 0.594449.
Train: 2018-08-09T12:08:25.182937: step 27217, loss 0.57917.
Train: 2018-08-09T12:08:25.276667: step 27218, loss 0.563194.
Train: 2018-08-09T12:08:25.386009: step 27219, loss 0.531998.
Train: 2018-08-09T12:08:25.479742: step 27220, loss 0.516431.
Test: 2018-08-09T12:08:25.979595: step 27220, loss 0.548645.
Train: 2018-08-09T12:08:26.088944: step 27221, loss 0.469601.
Train: 2018-08-09T12:08:26.182672: step 27222, loss 0.531949.
Train: 2018-08-09T12:08:26.292051: step 27223, loss 0.59455.
Train: 2018-08-09T12:08:26.401371: step 27224, loss 0.547433.
Train: 2018-08-09T12:08:26.495129: step 27225, loss 0.578864.
Train: 2018-08-09T12:08:26.604448: step 27226, loss 0.547273.
Train: 2018-08-09T12:08:26.698205: step 27227, loss 0.64209.
Train: 2018-08-09T12:08:26.807554: step 27228, loss 0.594749.
Train: 2018-08-09T12:08:26.901284: step 27229, loss 0.515408.
Train: 2018-08-09T12:08:27.013170: step 27230, loss 0.467952.
Test: 2018-08-09T12:08:27.497431: step 27230, loss 0.549832.
Train: 2018-08-09T12:08:27.606779: step 27231, loss 0.531312.
Train: 2018-08-09T12:08:27.716140: step 27232, loss 0.562759.
Train: 2018-08-09T12:08:27.809886: step 27233, loss 0.546753.
Train: 2018-08-09T12:08:27.919236: step 27234, loss 0.530074.
Train: 2018-08-09T12:08:28.012933: step 27235, loss 0.725423.
Train: 2018-08-09T12:08:28.122313: step 27236, loss 0.580073.
Train: 2018-08-09T12:08:28.216045: step 27237, loss 0.513895.
Train: 2018-08-09T12:08:28.325361: step 27238, loss 0.481813.
Train: 2018-08-09T12:08:28.434740: step 27239, loss 0.578544.
Train: 2018-08-09T12:08:28.528467: step 27240, loss 0.611667.
Test: 2018-08-09T12:08:29.030718: step 27240, loss 0.548077.
Train: 2018-08-09T12:08:29.124444: step 27241, loss 0.465488.
Train: 2018-08-09T12:08:29.233793: step 27242, loss 0.64499.
Train: 2018-08-09T12:08:29.343142: step 27243, loss 0.676583.
Train: 2018-08-09T12:08:29.436901: step 27244, loss 0.498007.
Train: 2018-08-09T12:08:29.546252: step 27245, loss 0.578831.
Train: 2018-08-09T12:08:29.639977: step 27246, loss 0.530435.
Train: 2018-08-09T12:08:29.749328: step 27247, loss 0.530568.
Train: 2018-08-09T12:08:29.843055: step 27248, loss 0.627418.
Train: 2018-08-09T12:08:29.952405: step 27249, loss 0.482082.
Train: 2018-08-09T12:08:30.061747: step 27250, loss 0.498205.
Test: 2018-08-09T12:08:30.555489: step 27250, loss 0.549398.
Train: 2018-08-09T12:08:30.664868: step 27251, loss 0.530365.
Train: 2018-08-09T12:08:30.758598: step 27252, loss 0.51407.
Train: 2018-08-09T12:08:30.867945: step 27253, loss 0.595138.
Train: 2018-08-09T12:08:30.968979: step 27254, loss 0.562348.
Train: 2018-08-09T12:08:31.062707: step 27255, loss 0.530091.
Train: 2018-08-09T12:08:31.172056: step 27256, loss 0.595567.
Train: 2018-08-09T12:08:31.265786: step 27257, loss 0.595155.
Train: 2018-08-09T12:08:31.375104: step 27258, loss 0.496523.
Train: 2018-08-09T12:08:31.468863: step 27259, loss 0.513028.
Train: 2018-08-09T12:08:31.578211: step 27260, loss 0.579762.
Test: 2018-08-09T12:08:32.062442: step 27260, loss 0.547542.
Train: 2018-08-09T12:08:32.171821: step 27261, loss 0.628813.
Train: 2018-08-09T12:08:32.265519: step 27262, loss 0.479159.
Train: 2018-08-09T12:08:32.374901: step 27263, loss 0.645081.
Train: 2018-08-09T12:08:32.468626: step 27264, loss 0.562488.
Train: 2018-08-09T12:08:32.577975: step 27265, loss 0.613486.
Train: 2018-08-09T12:08:32.671702: step 27266, loss 0.545012.
Train: 2018-08-09T12:08:32.781053: step 27267, loss 0.52918.
Train: 2018-08-09T12:08:32.890371: step 27268, loss 0.7119.
Train: 2018-08-09T12:08:32.984129: step 27269, loss 0.612708.
Train: 2018-08-09T12:08:33.093478: step 27270, loss 0.463678.
Test: 2018-08-09T12:08:33.593333: step 27270, loss 0.548981.
Train: 2018-08-09T12:08:33.687089: step 27271, loss 0.627909.
Train: 2018-08-09T12:08:33.796438: step 27272, loss 0.513541.
Train: 2018-08-09T12:08:33.907146: step 27273, loss 0.644775.
Train: 2018-08-09T12:08:34.000875: step 27274, loss 0.5952.
Train: 2018-08-09T12:08:34.110253: step 27275, loss 0.546353.
Train: 2018-08-09T12:08:34.203981: step 27276, loss 0.530305.
Train: 2018-08-09T12:08:34.313333: step 27277, loss 0.643907.
Train: 2018-08-09T12:08:34.407059: step 27278, loss 0.546547.
Train: 2018-08-09T12:08:34.516408: step 27279, loss 0.546566.
Train: 2018-08-09T12:08:34.625757: step 27280, loss 0.627216.
Test: 2018-08-09T12:08:35.109989: step 27280, loss 0.547126.
Train: 2018-08-09T12:08:35.219338: step 27281, loss 0.54673.
Train: 2018-08-09T12:08:35.313095: step 27282, loss 0.57887.
Train: 2018-08-09T12:08:35.422445: step 27283, loss 0.610908.
Train: 2018-08-09T12:08:35.531793: step 27284, loss 0.530894.
Train: 2018-08-09T12:08:35.625521: step 27285, loss 0.594829.
Train: 2018-08-09T12:08:35.734871: step 27286, loss 0.499149.
Train: 2018-08-09T12:08:35.828569: step 27287, loss 0.626664.
Train: 2018-08-09T12:08:35.939386: step 27288, loss 0.562944.
Train: 2018-08-09T12:08:36.033085: step 27289, loss 0.547061.
Train: 2018-08-09T12:08:36.142457: step 27290, loss 0.451755.
Test: 2018-08-09T12:08:36.642317: step 27290, loss 0.551056.
Train: 2018-08-09T12:08:36.736074: step 27291, loss 0.578862.
Train: 2018-08-09T12:08:36.845394: step 27292, loss 0.515179.
Train: 2018-08-09T12:08:36.939150: step 27293, loss 0.578864.
Train: 2018-08-09T12:08:37.048503: step 27294, loss 0.451134.
Train: 2018-08-09T12:08:37.157850: step 27295, loss 0.514831.
Train: 2018-08-09T12:08:37.251577: step 27296, loss 0.594895.
Train: 2018-08-09T12:08:37.360928: step 27297, loss 0.57882.
Train: 2018-08-09T12:08:37.454654: step 27298, loss 0.514164.
Train: 2018-08-09T12:08:37.564003: step 27299, loss 0.497508.
Train: 2018-08-09T12:08:37.657731: step 27300, loss 0.529922.
Test: 2018-08-09T12:08:38.158925: step 27300, loss 0.547506.
Train: 2018-08-09T12:08:38.721322: step 27301, loss 0.629116.
Train: 2018-08-09T12:08:38.815050: step 27302, loss 0.662934.
Train: 2018-08-09T12:08:38.924368: step 27303, loss 0.662318.
Train: 2018-08-09T12:08:39.033748: step 27304, loss 0.611694.
Train: 2018-08-09T12:08:39.127476: step 27305, loss 0.529532.
Train: 2018-08-09T12:08:39.236825: step 27306, loss 0.595169.
Train: 2018-08-09T12:08:39.330552: step 27307, loss 0.595218.
Train: 2018-08-09T12:08:39.439903: step 27308, loss 0.579243.
Train: 2018-08-09T12:08:39.549252: step 27309, loss 0.611594.
Train: 2018-08-09T12:08:39.642980: step 27310, loss 0.611131.
Test: 2018-08-09T12:08:40.143349: step 27310, loss 0.548878.
Train: 2018-08-09T12:08:40.252697: step 27311, loss 0.546691.
Train: 2018-08-09T12:08:40.346426: step 27312, loss 0.530653.
Train: 2018-08-09T12:08:40.455804: step 27313, loss 0.675137.
Train: 2018-08-09T12:08:40.549531: step 27314, loss 0.578857.
Train: 2018-08-09T12:08:40.658882: step 27315, loss 0.61076.
Train: 2018-08-09T12:08:40.752609: step 27316, loss 0.499355.
Train: 2018-08-09T12:08:40.861958: step 27317, loss 0.594731.
Train: 2018-08-09T12:08:40.971279: step 27318, loss 0.57886.
Train: 2018-08-09T12:08:41.065035: step 27319, loss 0.610481.
Train: 2018-08-09T12:08:41.174385: step 27320, loss 0.673502.
Test: 2018-08-09T12:08:41.674268: step 27320, loss 0.551401.
Train: 2018-08-09T12:08:41.767996: step 27321, loss 0.500292.
Train: 2018-08-09T12:08:41.877344: step 27322, loss 0.578876.
Train: 2018-08-09T12:08:41.973483: step 27323, loss 0.578882.
Train: 2018-08-09T12:08:42.082863: step 27324, loss 0.500809.
Train: 2018-08-09T12:08:42.194959: step 27325, loss 0.454054.
Train: 2018-08-09T12:08:42.297983: step 27326, loss 0.485144.
Train: 2018-08-09T12:08:42.391711: step 27327, loss 0.500546.
Train: 2018-08-09T12:08:42.501059: step 27328, loss 0.54739.
Train: 2018-08-09T12:08:42.594788: step 27329, loss 0.531367.
Train: 2018-08-09T12:08:42.704132: step 27330, loss 0.610161.
Test: 2018-08-09T12:08:43.188384: step 27330, loss 0.550217.
Train: 2018-08-09T12:08:43.297747: step 27331, loss 0.547646.
Train: 2018-08-09T12:08:43.391475: step 27332, loss 0.515122.
Train: 2018-08-09T12:08:43.500795: step 27333, loss 0.51333.
Train: 2018-08-09T12:08:43.610144: step 27334, loss 0.51178.
Train: 2018-08-09T12:08:43.703872: step 27335, loss 0.509095.
Train: 2018-08-09T12:08:43.813251: step 27336, loss 0.477068.
Train: 2018-08-09T12:08:43.912397: step 27337, loss 0.550438.
Train: 2018-08-09T12:08:44.021771: step 27338, loss 0.545716.
Train: 2018-08-09T12:08:44.131095: step 27339, loss 0.642906.
Train: 2018-08-09T12:08:44.224854: step 27340, loss 0.580786.
Test: 2018-08-09T12:08:44.726809: step 27340, loss 0.545681.
Train: 2018-08-09T12:08:44.820536: step 27341, loss 0.491754.
Train: 2018-08-09T12:08:44.929915: step 27342, loss 0.49001.
Train: 2018-08-09T12:08:45.039236: step 27343, loss 0.566833.
Train: 2018-08-09T12:08:45.148613: step 27344, loss 0.580073.
Train: 2018-08-09T12:08:45.242345: step 27345, loss 0.513909.
Train: 2018-08-09T12:08:45.351694: step 27346, loss 0.560672.
Train: 2018-08-09T12:08:45.445418: step 27347, loss 0.598125.
Train: 2018-08-09T12:08:45.554762: step 27348, loss 0.457147.
Train: 2018-08-09T12:08:45.664111: step 27349, loss 0.542948.
Train: 2018-08-09T12:08:45.757845: step 27350, loss 0.513803.
Test: 2018-08-09T12:08:46.256092: step 27350, loss 0.545979.
Train: 2018-08-09T12:08:46.365440: step 27351, loss 0.545751.
Train: 2018-08-09T12:08:46.459198: step 27352, loss 0.634075.
Train: 2018-08-09T12:08:46.568518: step 27353, loss 0.664583.
Train: 2018-08-09T12:08:46.662275: step 27354, loss 0.52611.
Train: 2018-08-09T12:08:46.771625: step 27355, loss 0.595005.
Train: 2018-08-09T12:08:46.880944: step 27356, loss 0.545503.
Train: 2018-08-09T12:08:46.979036: step 27357, loss 0.598462.
Train: 2018-08-09T12:08:47.088356: step 27358, loss 0.598557.
Train: 2018-08-09T12:08:47.182112: step 27359, loss 0.444381.
Train: 2018-08-09T12:08:47.291462: step 27360, loss 0.59538.
Test: 2018-08-09T12:08:47.791315: step 27360, loss 0.546051.
Train: 2018-08-09T12:08:47.885042: step 27361, loss 0.512542.
Train: 2018-08-09T12:08:47.994392: step 27362, loss 0.630388.
Train: 2018-08-09T12:08:48.103740: step 27363, loss 0.613302.
Train: 2018-08-09T12:08:48.197468: step 27364, loss 0.612686.
Train: 2018-08-09T12:08:48.306818: step 27365, loss 0.496046.
Train: 2018-08-09T12:08:48.400545: step 27366, loss 0.480477.
Train: 2018-08-09T12:08:48.509895: step 27367, loss 0.51347.
Train: 2018-08-09T12:08:48.619244: step 27368, loss 0.529923.
Train: 2018-08-09T12:08:48.713002: step 27369, loss 0.530134.
Train: 2018-08-09T12:08:48.822351: step 27370, loss 0.464106.
Test: 2018-08-09T12:08:49.324543: step 27370, loss 0.548957.
Train: 2018-08-09T12:08:49.418300: step 27371, loss 0.611985.
Train: 2018-08-09T12:08:49.527620: step 27372, loss 0.562618.
Train: 2018-08-09T12:08:49.621377: step 27373, loss 0.578963.
Train: 2018-08-09T12:08:49.730726: step 27374, loss 0.529291.
Train: 2018-08-09T12:08:49.840076: step 27375, loss 0.545782.
Train: 2018-08-09T12:08:49.933773: step 27376, loss 0.546247.
Train: 2018-08-09T12:08:50.043154: step 27377, loss 0.595958.
Train: 2018-08-09T12:08:50.152506: step 27378, loss 0.529331.
Train: 2018-08-09T12:08:50.246201: step 27379, loss 0.613145.
Train: 2018-08-09T12:08:50.355580: step 27380, loss 0.545784.
Test: 2018-08-09T12:08:50.855432: step 27380, loss 0.551243.
Train: 2018-08-09T12:08:50.951479: step 27381, loss 0.612575.
Train: 2018-08-09T12:08:51.060828: step 27382, loss 0.578885.
Train: 2018-08-09T12:08:51.170178: step 27383, loss 0.628928.
Train: 2018-08-09T12:08:51.279527: step 27384, loss 0.529459.
Train: 2018-08-09T12:08:51.373284: step 27385, loss 0.546104.
Train: 2018-08-09T12:08:51.482638: step 27386, loss 0.546131.
Train: 2018-08-09T12:08:51.576361: step 27387, loss 0.579049.
Train: 2018-08-09T12:08:51.685711: step 27388, loss 0.430794.
Train: 2018-08-09T12:08:51.795060: step 27389, loss 0.529514.
Train: 2018-08-09T12:08:51.904409: step 27390, loss 0.545943.
Test: 2018-08-09T12:08:52.388641: step 27390, loss 0.547618.
Train: 2018-08-09T12:08:52.498015: step 27391, loss 0.491869.
Train: 2018-08-09T12:08:52.591747: step 27392, loss 0.612237.
Train: 2018-08-09T12:08:52.701096: step 27393, loss 0.562461.
Train: 2018-08-09T12:08:52.810416: step 27394, loss 0.62909.
Train: 2018-08-09T12:08:52.919940: step 27395, loss 0.579088.
Train: 2018-08-09T12:08:53.015830: step 27396, loss 0.545758.
Train: 2018-08-09T12:08:53.125179: step 27397, loss 0.512652.
Train: 2018-08-09T12:08:53.234530: step 27398, loss 0.579013.
Train: 2018-08-09T12:08:53.328257: step 27399, loss 0.562508.
Train: 2018-08-09T12:08:53.437606: step 27400, loss 0.579161.
Test: 2018-08-09T12:08:53.937490: step 27400, loss 0.549369.
Train: 2018-08-09T12:08:54.468643: step 27401, loss 0.662331.
Train: 2018-08-09T12:08:54.577992: step 27402, loss 0.496022.
Train: 2018-08-09T12:08:54.671690: step 27403, loss 0.595768.
Train: 2018-08-09T12:08:54.781039: step 27404, loss 0.545877.
Train: 2018-08-09T12:08:54.890389: step 27405, loss 0.496255.
Train: 2018-08-09T12:08:55.000360: step 27406, loss 0.578997.
Train: 2018-08-09T12:08:55.094088: step 27407, loss 0.628677.
Train: 2018-08-09T12:08:55.203438: step 27408, loss 0.661686.
Train: 2018-08-09T12:08:55.312787: step 27409, loss 0.595501.
Train: 2018-08-09T12:08:55.422136: step 27410, loss 0.628304.
Test: 2018-08-09T12:08:55.921990: step 27410, loss 0.54844.
Train: 2018-08-09T12:08:56.031362: step 27411, loss 0.628117.
Train: 2018-08-09T12:08:56.140716: step 27412, loss 0.562601.
Train: 2018-08-09T12:08:56.234444: step 27413, loss 0.546362.
Train: 2018-08-09T12:08:56.343794: step 27414, loss 0.546441.
Train: 2018-08-09T12:08:56.453144: step 27415, loss 0.497952.
Train: 2018-08-09T12:08:56.562492: step 27416, loss 0.562738.
Train: 2018-08-09T12:08:56.656216: step 27417, loss 0.530407.
Train: 2018-08-09T12:08:56.765569: step 27418, loss 0.449443.
Train: 2018-08-09T12:08:56.874918: step 27419, loss 0.498073.
Train: 2018-08-09T12:08:56.984238: step 27420, loss 0.660388.
Test: 2018-08-09T12:08:57.484121: step 27420, loss 0.550464.
Train: 2018-08-09T12:08:57.593469: step 27421, loss 0.530178.
Train: 2018-08-09T12:08:57.687197: step 27422, loss 0.44866.
Train: 2018-08-09T12:08:57.796546: step 27423, loss 0.561874.
Train: 2018-08-09T12:08:57.908208: step 27424, loss 0.595889.
Train: 2018-08-09T12:08:58.017588: step 27425, loss 0.595685.
Train: 2018-08-09T12:08:58.111315: step 27426, loss 0.578454.
Train: 2018-08-09T12:08:58.220668: step 27427, loss 0.580627.
Train: 2018-08-09T12:08:58.330013: step 27428, loss 0.661541.
Train: 2018-08-09T12:08:58.439333: step 27429, loss 0.595128.
Train: 2018-08-09T12:08:58.533643: step 27430, loss 0.661294.
Test: 2018-08-09T12:08:59.033495: step 27430, loss 0.549862.
Train: 2018-08-09T12:08:59.142876: step 27431, loss 0.611755.
Train: 2018-08-09T12:08:59.252223: step 27432, loss 0.578902.
Train: 2018-08-09T12:08:59.361573: step 27433, loss 0.595016.
Train: 2018-08-09T12:08:59.470893: step 27434, loss 0.514548.
Train: 2018-08-09T12:08:59.564620: step 27435, loss 0.562823.
Train: 2018-08-09T12:08:59.674000: step 27436, loss 0.498789.
Train: 2018-08-09T12:08:59.783318: step 27437, loss 0.674883.
Train: 2018-08-09T12:08:59.892699: step 27438, loss 0.467088.
Train: 2018-08-09T12:08:59.986426: step 27439, loss 0.578862.
Train: 2018-08-09T12:09:00.095776: step 27440, loss 0.546962.
Test: 2018-08-09T12:09:00.595628: step 27440, loss 0.54858.
Train: 2018-08-09T12:09:00.705007: step 27441, loss 0.578852.
Train: 2018-08-09T12:09:00.814326: step 27442, loss 0.594796.
Train: 2018-08-09T12:09:00.908084: step 27443, loss 0.562935.
Train: 2018-08-09T12:09:01.017435: step 27444, loss 0.531108.
Train: 2018-08-09T12:09:01.126784: step 27445, loss 0.578829.
Train: 2018-08-09T12:09:01.236132: step 27446, loss 0.563002.
Train: 2018-08-09T12:09:01.345934: step 27447, loss 0.610699.
Train: 2018-08-09T12:09:01.452769: step 27448, loss 0.642471.
Train: 2018-08-09T12:09:01.546501: step 27449, loss 0.467746.
Train: 2018-08-09T12:09:01.671438: step 27450, loss 0.563009.
Test: 2018-08-09T12:09:02.159048: step 27450, loss 0.549915.
Train: 2018-08-09T12:09:02.268425: step 27451, loss 0.610608.
Train: 2018-08-09T12:09:02.377775: step 27452, loss 0.483677.
Train: 2018-08-09T12:09:02.487094: step 27453, loss 0.721708.
Train: 2018-08-09T12:09:02.596473: step 27454, loss 0.499623.
Train: 2018-08-09T12:09:02.705823: step 27455, loss 0.610561.
Train: 2018-08-09T12:09:02.815171: step 27456, loss 0.499737.
Train: 2018-08-09T12:09:02.924515: step 27457, loss 0.499687.
Train: 2018-08-09T12:09:03.018249: step 27458, loss 0.56304.
Train: 2018-08-09T12:09:03.127598: step 27459, loss 0.483626.
Train: 2018-08-09T12:09:03.236948: step 27460, loss 0.578914.
Test: 2018-08-09T12:09:03.736799: step 27460, loss 0.549179.
Train: 2018-08-09T12:09:03.846179: step 27461, loss 0.547085.
Train: 2018-08-09T12:09:03.956930: step 27462, loss 0.498957.
Train: 2018-08-09T12:09:04.066279: step 27463, loss 0.546871.
Train: 2018-08-09T12:09:04.160007: step 27464, loss 0.482563.
Train: 2018-08-09T12:09:04.269357: step 27465, loss 0.595071.
Train: 2018-08-09T12:09:04.378707: step 27466, loss 0.562671.
Train: 2018-08-09T12:09:04.488024: step 27467, loss 0.514058.
Train: 2018-08-09T12:09:04.597405: step 27468, loss 0.546526.
Train: 2018-08-09T12:09:04.706754: step 27469, loss 0.513427.
Train: 2018-08-09T12:09:04.800481: step 27470, loss 0.579091.
Test: 2018-08-09T12:09:05.300335: step 27470, loss 0.549585.
Train: 2018-08-09T12:09:05.409712: step 27471, loss 0.562317.
Train: 2018-08-09T12:09:05.519062: step 27472, loss 0.628653.
Train: 2018-08-09T12:09:05.628382: step 27473, loss 0.512977.
Train: 2018-08-09T12:09:05.737731: step 27474, loss 0.529922.
Train: 2018-08-09T12:09:05.847109: step 27475, loss 0.562318.
Train: 2018-08-09T12:09:05.942175: step 27476, loss 0.579198.
Train: 2018-08-09T12:09:06.051494: step 27477, loss 0.562737.
Train: 2018-08-09T12:09:06.160842: step 27478, loss 0.645673.
Train: 2018-08-09T12:09:06.270222: step 27479, loss 0.545893.
Train: 2018-08-09T12:09:06.379572: step 27480, loss 0.529326.
Test: 2018-08-09T12:09:06.879425: step 27480, loss 0.549438.
Train: 2018-08-09T12:09:06.973184: step 27481, loss 0.529304.
Train: 2018-08-09T12:09:07.082501: step 27482, loss 0.711778.
Train: 2018-08-09T12:09:07.191850: step 27483, loss 0.496188.
Train: 2018-08-09T12:09:07.301229: step 27484, loss 0.446547.
Train: 2018-08-09T12:09:07.410580: step 27485, loss 0.529322.
Train: 2018-08-09T12:09:07.519927: step 27486, loss 0.628836.
Train: 2018-08-09T12:09:07.629277: step 27487, loss 0.579095.
Train: 2018-08-09T12:09:07.738626: step 27488, loss 0.51257.
Train: 2018-08-09T12:09:07.847978: step 27489, loss 0.529239.
Train: 2018-08-09T12:09:07.958616: step 27490, loss 0.612384.
Test: 2018-08-09T12:09:08.458469: step 27490, loss 0.546837.
Train: 2018-08-09T12:09:08.552226: step 27491, loss 0.462555.
Train: 2018-08-09T12:09:08.661575: step 27492, loss 0.579094.
Train: 2018-08-09T12:09:08.770919: step 27493, loss 0.579093.
Train: 2018-08-09T12:09:08.895866: step 27494, loss 0.512226.
Train: 2018-08-09T12:09:09.005246: step 27495, loss 0.529004.
Train: 2018-08-09T12:09:09.114564: step 27496, loss 0.662924.
Train: 2018-08-09T12:09:09.223914: step 27497, loss 0.528663.
Train: 2018-08-09T12:09:09.333292: step 27498, loss 0.478447.
Train: 2018-08-09T12:09:09.442643: step 27499, loss 0.545629.
Train: 2018-08-09T12:09:09.536370: step 27500, loss 0.595648.
Test: 2018-08-09T12:09:10.038610: step 27500, loss 0.548475.
Train: 2018-08-09T12:09:10.647841: step 27501, loss 0.545122.
Train: 2018-08-09T12:09:10.741598: step 27502, loss 0.511785.
Train: 2018-08-09T12:09:10.850947: step 27503, loss 0.563275.
Train: 2018-08-09T12:09:10.960297: step 27504, loss 0.460185.
Train: 2018-08-09T12:09:11.069650: step 27505, loss 0.61371.
Train: 2018-08-09T12:09:11.178995: step 27506, loss 0.510979.
Train: 2018-08-09T12:09:11.288314: step 27507, loss 0.596176.
Train: 2018-08-09T12:09:11.397693: step 27508, loss 0.630911.
Train: 2018-08-09T12:09:11.507036: step 27509, loss 0.52716.
Train: 2018-08-09T12:09:11.600770: step 27510, loss 0.49415.
Test: 2018-08-09T12:09:12.102991: step 27510, loss 0.549437.
Train: 2018-08-09T12:09:12.212368: step 27511, loss 0.646818.
Train: 2018-08-09T12:09:12.321718: step 27512, loss 0.493383.
Train: 2018-08-09T12:09:12.431067: step 27513, loss 0.631376.
Train: 2018-08-09T12:09:12.540416: step 27514, loss 0.599343.
Train: 2018-08-09T12:09:12.649766: step 27515, loss 0.632312.
Train: 2018-08-09T12:09:12.759088: step 27516, loss 0.613378.
Train: 2018-08-09T12:09:12.868467: step 27517, loss 0.579321.
Train: 2018-08-09T12:09:12.977814: step 27518, loss 0.528764.
Train: 2018-08-09T12:09:13.087163: step 27519, loss 0.528759.
Train: 2018-08-09T12:09:13.180890: step 27520, loss 0.562393.
Test: 2018-08-09T12:09:13.680744: step 27520, loss 0.546833.
Train: 2018-08-09T12:09:13.790122: step 27521, loss 0.612471.
Train: 2018-08-09T12:09:13.899442: step 27522, loss 0.545825.
Train: 2018-08-09T12:09:14.011259: step 27523, loss 0.579057.
Train: 2018-08-09T12:09:14.120612: step 27524, loss 0.529454.
Train: 2018-08-09T12:09:14.229957: step 27525, loss 0.579.
Train: 2018-08-09T12:09:14.323686: step 27526, loss 0.61193.
Train: 2018-08-09T12:09:14.433035: step 27527, loss 0.578976.
Train: 2018-08-09T12:09:14.542385: step 27528, loss 0.578963.
Train: 2018-08-09T12:09:14.651734: step 27529, loss 0.529851.
Train: 2018-08-09T12:09:14.761083: step 27530, loss 0.627969.
Test: 2018-08-09T12:09:15.260935: step 27530, loss 0.549192.
Train: 2018-08-09T12:09:15.370317: step 27531, loss 0.448474.
Train: 2018-08-09T12:09:15.479663: step 27532, loss 0.530029.
Train: 2018-08-09T12:09:15.589013: step 27533, loss 0.513725.
Train: 2018-08-09T12:09:15.698332: step 27534, loss 0.529987.
Train: 2018-08-09T12:09:15.807711: step 27535, loss 0.562606.
Train: 2018-08-09T12:09:15.901439: step 27536, loss 0.578935.
Train: 2018-08-09T12:09:16.016887: step 27537, loss 0.578968.
Train: 2018-08-09T12:09:16.126231: step 27538, loss 0.546201.
Train: 2018-08-09T12:09:16.235581: step 27539, loss 0.644467.
Train: 2018-08-09T12:09:16.329309: step 27540, loss 0.497126.
Test: 2018-08-09T12:09:16.829161: step 27540, loss 0.548479.
Train: 2018-08-09T12:09:16.938510: step 27541, loss 0.562576.
Train: 2018-08-09T12:09:17.047859: step 27542, loss 0.578952.
Train: 2018-08-09T12:09:17.157209: step 27543, loss 0.546197.
Train: 2018-08-09T12:09:17.266588: step 27544, loss 0.546203.
Train: 2018-08-09T12:09:17.375937: step 27545, loss 0.628111.
Train: 2018-08-09T12:09:17.485286: step 27546, loss 0.628093.
Train: 2018-08-09T12:09:17.594606: step 27547, loss 0.562598.
Train: 2018-08-09T12:09:17.703985: step 27548, loss 0.562607.
Train: 2018-08-09T12:09:17.813333: step 27549, loss 0.497378.
Train: 2018-08-09T12:09:17.920261: step 27550, loss 0.595232.
Test: 2018-08-09T12:09:18.413473: step 27550, loss 0.547354.
Train: 2018-08-09T12:09:18.522822: step 27551, loss 0.595221.
Train: 2018-08-09T12:09:18.632171: step 27552, loss 0.546369.
Train: 2018-08-09T12:09:18.741551: step 27553, loss 0.660277.
Train: 2018-08-09T12:09:18.850874: step 27554, loss 0.513949.
Train: 2018-08-09T12:09:18.960251: step 27555, loss 0.611356.
Train: 2018-08-09T12:09:19.069568: step 27556, loss 0.530302.
Train: 2018-08-09T12:09:19.178916: step 27557, loss 0.675984.
Train: 2018-08-09T12:09:19.288300: step 27558, loss 0.498171.
Train: 2018-08-09T12:09:19.397647: step 27559, loss 0.627247.
Train: 2018-08-09T12:09:19.506995: step 27560, loss 0.514518.
Test: 2018-08-09T12:09:19.991227: step 27560, loss 0.549564.
Train: 2018-08-09T12:09:20.100605: step 27561, loss 0.562795.
Train: 2018-08-09T12:09:20.209954: step 27562, loss 0.610981.
Train: 2018-08-09T12:09:20.319274: step 27563, loss 0.530772.
Train: 2018-08-09T12:09:20.428655: step 27564, loss 0.658961.
Train: 2018-08-09T12:09:20.522381: step 27565, loss 0.578851.
Train: 2018-08-09T12:09:20.631730: step 27566, loss 0.546939.
Train: 2018-08-09T12:09:20.741079: step 27567, loss 0.594793.
Train: 2018-08-09T12:09:20.850399: step 27568, loss 0.515241.
Train: 2018-08-09T12:09:20.962022: step 27569, loss 0.594783.
Train: 2018-08-09T12:09:21.055783: step 27570, loss 0.578873.
Test: 2018-08-09T12:09:21.555633: step 27570, loss 0.548144.
Train: 2018-08-09T12:09:21.665013: step 27571, loss 0.563003.
Train: 2018-08-09T12:09:21.774365: step 27572, loss 0.578872.
Train: 2018-08-09T12:09:21.883679: step 27573, loss 0.610531.
Train: 2018-08-09T12:09:21.993060: step 27574, loss 0.594657.
Train: 2018-08-09T12:09:22.086787: step 27575, loss 0.531564.
Train: 2018-08-09T12:09:22.196136: step 27576, loss 0.468585.
Train: 2018-08-09T12:09:22.305485: step 27577, loss 0.531563.
Train: 2018-08-09T12:09:22.414836: step 27578, loss 0.641995.
Train: 2018-08-09T12:09:22.524183: step 27579, loss 0.642.
Train: 2018-08-09T12:09:22.617912: step 27580, loss 0.531557.
Test: 2018-08-09T12:09:23.120238: step 27580, loss 0.551317.
Train: 2018-08-09T12:09:23.229617: step 27581, loss 0.610401.
Train: 2018-08-09T12:09:23.338961: step 27582, loss 0.563123.
Train: 2018-08-09T12:09:23.432665: step 27583, loss 0.563123.
Train: 2018-08-09T12:09:23.542044: step 27584, loss 0.594614.
Train: 2018-08-09T12:09:23.651393: step 27585, loss 0.547434.
Train: 2018-08-09T12:09:23.760742: step 27586, loss 0.578851.
Train: 2018-08-09T12:09:23.870092: step 27587, loss 0.50039.
Train: 2018-08-09T12:09:23.979412: step 27588, loss 0.594525.
Train: 2018-08-09T12:09:24.088790: step 27589, loss 0.468855.
Train: 2018-08-09T12:09:24.182518: step 27590, loss 0.5315.
Test: 2018-08-09T12:09:24.682372: step 27590, loss 0.549365.
Train: 2018-08-09T12:09:24.791719: step 27591, loss 0.562633.
Train: 2018-08-09T12:09:24.901099: step 27592, loss 0.658755.
Train: 2018-08-09T12:09:25.012826: step 27593, loss 0.626516.
Train: 2018-08-09T12:09:25.106586: step 27594, loss 0.59573.
Train: 2018-08-09T12:09:25.215936: step 27595, loss 0.531301.
Train: 2018-08-09T12:09:25.325254: step 27596, loss 0.531387.
Train: 2018-08-09T12:09:25.434603: step 27597, loss 0.563106.
Train: 2018-08-09T12:09:25.543983: step 27598, loss 0.49967.
Train: 2018-08-09T12:09:25.637710: step 27599, loss 0.547122.
Train: 2018-08-09T12:09:25.747059: step 27600, loss 0.578774.
Test: 2018-08-09T12:09:26.246912: step 27600, loss 0.551085.
Train: 2018-08-09T12:09:26.809308: step 27601, loss 0.483458.
Train: 2018-08-09T12:09:26.920076: step 27602, loss 0.530985.
Train: 2018-08-09T12:09:27.029450: step 27603, loss 0.53091.
Train: 2018-08-09T12:09:27.123184: step 27604, loss 0.54667.
Train: 2018-08-09T12:09:27.232503: step 27605, loss 0.530328.
Train: 2018-08-09T12:09:27.341883: step 27606, loss 0.595406.
Train: 2018-08-09T12:09:27.435610: step 27607, loss 0.595563.
Train: 2018-08-09T12:09:27.544959: step 27608, loss 0.530057.
Train: 2018-08-09T12:09:27.654308: step 27609, loss 0.530265.
Train: 2018-08-09T12:09:27.763657: step 27610, loss 0.547071.
Test: 2018-08-09T12:09:28.247889: step 27610, loss 0.547067.
Train: 2018-08-09T12:09:28.357271: step 27611, loss 0.480236.
Train: 2018-08-09T12:09:28.466587: step 27612, loss 0.479961.
Train: 2018-08-09T12:09:28.575936: step 27613, loss 0.612942.
Train: 2018-08-09T12:09:28.685285: step 27614, loss 0.544947.
Train: 2018-08-09T12:09:28.794637: step 27615, loss 0.630084.
Train: 2018-08-09T12:09:28.888362: step 27616, loss 0.44493.
Train: 2018-08-09T12:09:28.998671: step 27617, loss 0.563096.
Train: 2018-08-09T12:09:29.108015: step 27618, loss 0.630204.
Train: 2018-08-09T12:09:29.217368: step 27619, loss 0.579091.
Train: 2018-08-09T12:09:29.326718: step 27620, loss 0.511451.
Test: 2018-08-09T12:09:29.810950: step 27620, loss 0.549764.
Train: 2018-08-09T12:09:29.920327: step 27621, loss 0.69837.
Train: 2018-08-09T12:09:30.029679: step 27622, loss 0.595855.
Train: 2018-08-09T12:09:30.139035: step 27623, loss 0.562071.
Train: 2018-08-09T12:09:30.248377: step 27624, loss 0.578747.
Train: 2018-08-09T12:09:30.342104: step 27625, loss 0.562601.
Train: 2018-08-09T12:09:30.451453: step 27626, loss 0.546061.
Train: 2018-08-09T12:09:30.560772: step 27627, loss 0.612259.
Train: 2018-08-09T12:09:30.670122: step 27628, loss 0.513006.
Train: 2018-08-09T12:09:30.763850: step 27629, loss 0.546119.
Train: 2018-08-09T12:09:30.873229: step 27630, loss 0.661245.
Test: 2018-08-09T12:09:31.373082: step 27630, loss 0.55091.
Train: 2018-08-09T12:09:31.482460: step 27631, loss 0.578961.
Train: 2018-08-09T12:09:31.591810: step 27632, loss 0.578964.
Train: 2018-08-09T12:09:31.685507: step 27633, loss 0.448296.
Train: 2018-08-09T12:09:31.794886: step 27634, loss 0.513625.
Train: 2018-08-09T12:09:31.905555: step 27635, loss 0.546269.
Train: 2018-08-09T12:09:31.999283: step 27636, loss 0.627972.
Train: 2018-08-09T12:09:32.108632: step 27637, loss 0.595276.
Train: 2018-08-09T12:09:32.208336: step 27638, loss 0.529954.
Train: 2018-08-09T12:09:32.317273: step 27639, loss 0.562604.
Train: 2018-08-09T12:09:32.426653: step 27640, loss 0.546298.
Test: 2018-08-09T12:09:32.926506: step 27640, loss 0.547317.
Train: 2018-08-09T12:09:33.020233: step 27641, loss 0.529961.
Train: 2018-08-09T12:09:33.160826: step 27642, loss 0.595271.
Train: 2018-08-09T12:09:33.270199: step 27643, loss 0.513622.
Train: 2018-08-09T12:09:33.363932: step 27644, loss 0.578932.
Train: 2018-08-09T12:09:33.473281: step 27645, loss 0.546233.
Train: 2018-08-09T12:09:33.582632: step 27646, loss 0.529868.
Train: 2018-08-09T12:09:33.691980: step 27647, loss 0.54618.
Train: 2018-08-09T12:09:33.785707: step 27648, loss 0.529673.
Train: 2018-08-09T12:09:33.895058: step 27649, loss 0.529709.
Train: 2018-08-09T12:09:34.004377: step 27650, loss 0.562395.
Test: 2018-08-09T12:09:34.488638: step 27650, loss 0.548237.
Train: 2018-08-09T12:09:34.598016: step 27651, loss 0.595137.
Train: 2018-08-09T12:09:34.707366: step 27652, loss 0.545909.
Train: 2018-08-09T12:09:34.801064: step 27653, loss 0.61258.
Train: 2018-08-09T12:09:34.910443: step 27654, loss 0.612138.
Train: 2018-08-09T12:09:35.019793: step 27655, loss 0.429016.
Train: 2018-08-09T12:09:35.129112: step 27656, loss 0.596412.
Train: 2018-08-09T12:09:35.222868: step 27657, loss 0.57874.
Train: 2018-08-09T12:09:35.332189: step 27658, loss 0.52746.
Train: 2018-08-09T12:09:35.441538: step 27659, loss 0.545041.
Train: 2018-08-09T12:09:35.535295: step 27660, loss 0.596462.
Test: 2018-08-09T12:09:36.037407: step 27660, loss 0.548225.
Train: 2018-08-09T12:09:36.146755: step 27661, loss 0.700388.
Train: 2018-08-09T12:09:36.240483: step 27662, loss 0.628756.
Train: 2018-08-09T12:09:36.349833: step 27663, loss 0.596289.
Train: 2018-08-09T12:09:36.459211: step 27664, loss 0.412458.
Train: 2018-08-09T12:09:36.552939: step 27665, loss 0.562679.
Train: 2018-08-09T12:09:36.662288: step 27666, loss 0.57852.
Train: 2018-08-09T12:09:36.771642: step 27667, loss 0.595999.
Train: 2018-08-09T12:09:36.865366: step 27668, loss 0.495633.
Train: 2018-08-09T12:09:36.974694: step 27669, loss 0.579401.
Train: 2018-08-09T12:09:37.084064: step 27670, loss 0.462265.
Test: 2018-08-09T12:09:37.583917: step 27670, loss 0.549278.
Train: 2018-08-09T12:09:37.677674: step 27671, loss 0.629742.
Train: 2018-08-09T12:09:37.787023: step 27672, loss 0.645657.
Train: 2018-08-09T12:09:37.896372: step 27673, loss 0.512098.
Train: 2018-08-09T12:09:37.992410: step 27674, loss 0.427808.
Train: 2018-08-09T12:09:38.101760: step 27675, loss 0.510871.
Train: 2018-08-09T12:09:38.195487: step 27676, loss 0.511574.
Train: 2018-08-09T12:09:38.304837: step 27677, loss 0.494242.
Train: 2018-08-09T12:09:38.414186: step 27678, loss 0.564008.
Train: 2018-08-09T12:09:38.523530: step 27679, loss 0.565786.
Train: 2018-08-09T12:09:38.617263: step 27680, loss 0.598794.
Test: 2018-08-09T12:09:39.117116: step 27680, loss 0.54719.
Train: 2018-08-09T12:09:39.226495: step 27681, loss 0.619078.
Train: 2018-08-09T12:09:39.320223: step 27682, loss 0.513442.
Train: 2018-08-09T12:09:39.429570: step 27683, loss 0.54744.
Train: 2018-08-09T12:09:39.538901: step 27684, loss 0.612707.
Train: 2018-08-09T12:09:39.632653: step 27685, loss 0.4934.
Train: 2018-08-09T12:09:39.741968: step 27686, loss 0.545688.
Train: 2018-08-09T12:09:39.851318: step 27687, loss 0.562196.
Train: 2018-08-09T12:09:39.946387: step 27688, loss 0.497884.
Train: 2018-08-09T12:09:40.055739: step 27689, loss 0.546606.
Train: 2018-08-09T12:09:40.165089: step 27690, loss 0.629003.
Test: 2018-08-09T12:09:40.649345: step 27690, loss 0.547314.
Train: 2018-08-09T12:09:40.758696: step 27691, loss 0.529732.
Train: 2018-08-09T12:09:40.868045: step 27692, loss 0.526546.
Train: 2018-08-09T12:09:40.961773: step 27693, loss 0.596426.
Train: 2018-08-09T12:09:41.071123: step 27694, loss 0.595962.
Train: 2018-08-09T12:09:41.180472: step 27695, loss 0.579063.
Train: 2018-08-09T12:09:41.274200: step 27696, loss 0.562413.
Train: 2018-08-09T12:09:41.383548: step 27697, loss 0.578642.
Train: 2018-08-09T12:09:41.492900: step 27698, loss 0.529409.
Train: 2018-08-09T12:09:41.586626: step 27699, loss 0.562498.
Train: 2018-08-09T12:09:41.695975: step 27700, loss 0.595348.
Test: 2018-08-09T12:09:42.197271: step 27700, loss 0.547113.
Train: 2018-08-09T12:09:42.744045: step 27701, loss 0.578906.
Train: 2018-08-09T12:09:42.853365: step 27702, loss 0.611878.
Train: 2018-08-09T12:09:42.947124: step 27703, loss 0.546178.
Train: 2018-08-09T12:09:43.056474: step 27704, loss 0.480755.
Train: 2018-08-09T12:09:43.165821: step 27705, loss 0.513413.
Train: 2018-08-09T12:09:43.259550: step 27706, loss 0.611685.
Train: 2018-08-09T12:09:43.368902: step 27707, loss 0.562569.
Train: 2018-08-09T12:09:43.478243: step 27708, loss 0.595328.
Train: 2018-08-09T12:09:43.587597: step 27709, loss 0.529797.
Train: 2018-08-09T12:09:43.681324: step 27710, loss 0.578948.
Test: 2018-08-09T12:09:44.183569: step 27710, loss 0.55158.
Train: 2018-08-09T12:09:44.277268: step 27711, loss 0.578976.
Train: 2018-08-09T12:09:44.386618: step 27712, loss 0.44808.
Train: 2018-08-09T12:09:44.495997: step 27713, loss 0.595342.
Train: 2018-08-09T12:09:44.589725: step 27714, loss 0.56259.
Train: 2018-08-09T12:09:44.699074: step 27715, loss 0.546215.
Train: 2018-08-09T12:09:44.808423: step 27716, loss 0.513322.
Train: 2018-08-09T12:09:44.902120: step 27717, loss 0.480378.
Train: 2018-08-09T12:09:45.011502: step 27718, loss 0.414305.
Train: 2018-08-09T12:09:45.120850: step 27719, loss 0.529375.
Train: 2018-08-09T12:09:45.214578: step 27720, loss 0.579113.
Test: 2018-08-09T12:09:45.714430: step 27720, loss 0.546783.
Train: 2018-08-09T12:09:45.823809: step 27721, loss 0.6793.
Train: 2018-08-09T12:09:45.918907: step 27722, loss 0.512399.
Train: 2018-08-09T12:09:46.028285: step 27723, loss 0.5792.
Train: 2018-08-09T12:09:46.137635: step 27724, loss 0.562482.
Train: 2018-08-09T12:09:46.231363: step 27725, loss 0.62943.
Train: 2018-08-09T12:09:46.340713: step 27726, loss 0.528862.
Train: 2018-08-09T12:09:46.450062: step 27727, loss 0.545614.
Train: 2018-08-09T12:09:46.559405: step 27728, loss 0.612717.
Train: 2018-08-09T12:09:46.653139: step 27729, loss 0.545676.
Train: 2018-08-09T12:09:46.762489: step 27730, loss 0.545633.
Test: 2018-08-09T12:09:47.262341: step 27730, loss 0.546072.
Train: 2018-08-09T12:09:47.356099: step 27731, loss 0.595895.
Train: 2018-08-09T12:09:47.465418: step 27732, loss 0.646142.
Train: 2018-08-09T12:09:47.574768: step 27733, loss 0.495581.
Train: 2018-08-09T12:09:47.668495: step 27734, loss 0.545732.
Train: 2018-08-09T12:09:47.777843: step 27735, loss 0.57911.
Train: 2018-08-09T12:09:47.887223: step 27736, loss 0.56242.
Train: 2018-08-09T12:09:47.983263: step 27737, loss 0.612428.
Train: 2018-08-09T12:09:48.092612: step 27738, loss 0.645642.
Train: 2018-08-09T12:09:48.186310: step 27739, loss 0.595665.
Train: 2018-08-09T12:09:48.295689: step 27740, loss 0.463157.
Test: 2018-08-09T12:09:48.795560: step 27740, loss 0.550747.
Train: 2018-08-09T12:09:48.889269: step 27741, loss 0.463281.
Train: 2018-08-09T12:09:48.998619: step 27742, loss 0.612107.
Train: 2018-08-09T12:09:49.107998: step 27743, loss 0.595555.
Train: 2018-08-09T12:09:49.201727: step 27744, loss 0.579015.
Train: 2018-08-09T12:09:49.311076: step 27745, loss 0.529507.
Train: 2018-08-09T12:09:49.404773: step 27746, loss 0.562509.
Train: 2018-08-09T12:09:49.514153: step 27747, loss 0.480096.
Train: 2018-08-09T12:09:49.623503: step 27748, loss 0.611983.
Train: 2018-08-09T12:09:49.730451: step 27749, loss 0.578993.
Train: 2018-08-09T12:09:49.824209: step 27750, loss 0.513093.
Test: 2018-08-09T12:09:50.328448: step 27750, loss 0.549542.
Train: 2018-08-09T12:09:50.422193: step 27751, loss 0.496488.
Train: 2018-08-09T12:09:50.531544: step 27752, loss 0.479916.
Train: 2018-08-09T12:09:50.640893: step 27753, loss 0.595841.
Train: 2018-08-09T12:09:50.734621: step 27754, loss 0.545823.
Train: 2018-08-09T12:09:50.843969: step 27755, loss 0.595588.
Train: 2018-08-09T12:09:50.953301: step 27756, loss 0.546027.
Train: 2018-08-09T12:09:51.047018: step 27757, loss 0.545832.
Train: 2018-08-09T12:09:51.156397: step 27758, loss 0.662279.
Train: 2018-08-09T12:09:51.265747: step 27759, loss 0.462721.
Train: 2018-08-09T12:09:51.359473: step 27760, loss 0.579081.
Test: 2018-08-09T12:09:51.859326: step 27760, loss 0.549995.
Train: 2018-08-09T12:09:51.969295: step 27761, loss 0.529155.
Train: 2018-08-09T12:09:52.064913: step 27762, loss 0.56244.
Train: 2018-08-09T12:09:52.174262: step 27763, loss 0.629087.
Train: 2018-08-09T12:09:52.283612: step 27764, loss 0.629071.
Train: 2018-08-09T12:09:52.377339: step 27765, loss 0.545807.
Train: 2018-08-09T12:09:52.486688: step 27766, loss 0.612306.
Train: 2018-08-09T12:09:52.580447: step 27767, loss 0.711777.
Train: 2018-08-09T12:09:52.689795: step 27768, loss 0.595552.
Train: 2018-08-09T12:09:52.799147: step 27769, loss 0.480177.
Train: 2018-08-09T12:09:52.892872: step 27770, loss 0.628272.
Test: 2018-08-09T12:09:53.392757: step 27770, loss 0.550938.
Train: 2018-08-09T12:09:53.502106: step 27771, loss 0.497037.
Train: 2018-08-09T12:09:53.611453: step 27772, loss 0.464462.
Train: 2018-08-09T12:09:53.705181: step 27773, loss 0.480839.
Train: 2018-08-09T12:09:53.814530: step 27774, loss 0.611679.
Train: 2018-08-09T12:09:53.923882: step 27775, loss 0.529847.
Train: 2018-08-09T12:09:54.017608: step 27776, loss 0.726341.
Train: 2018-08-09T12:09:54.126927: step 27777, loss 0.415468.
Train: 2018-08-09T12:09:54.220688: step 27778, loss 0.480818.
Train: 2018-08-09T12:09:54.330034: step 27779, loss 0.529817.
Train: 2018-08-09T12:09:54.439354: step 27780, loss 0.578965.
Test: 2018-08-09T12:09:54.925953: step 27780, loss 0.547149.
Train: 2018-08-09T12:09:55.082166: step 27781, loss 0.562544.
Train: 2018-08-09T12:09:55.175925: step 27782, loss 0.562532.
Train: 2018-08-09T12:09:55.285272: step 27783, loss 0.513117.
Train: 2018-08-09T12:09:55.394622: step 27784, loss 0.562492.
Train: 2018-08-09T12:09:55.488344: step 27785, loss 0.67818.
Train: 2018-08-09T12:09:55.597699: step 27786, loss 0.546.
Train: 2018-08-09T12:09:55.691428: step 27787, loss 0.512999.
Train: 2018-08-09T12:09:55.800776: step 27788, loss 0.52946.
Train: 2018-08-09T12:09:55.910125: step 27789, loss 0.595553.
Train: 2018-08-09T12:09:56.003855: step 27790, loss 0.579024.
Test: 2018-08-09T12:09:56.503705: step 27790, loss 0.549495.
Train: 2018-08-09T12:09:56.613085: step 27791, loss 0.579026.
Train: 2018-08-09T12:09:56.706813: step 27792, loss 0.744346.
Train: 2018-08-09T12:09:56.816163: step 27793, loss 0.546026.
Train: 2018-08-09T12:09:56.911263: step 27794, loss 0.578983.
Train: 2018-08-09T12:09:57.020613: step 27795, loss 0.595379.
Train: 2018-08-09T12:09:57.129991: step 27796, loss 0.644431.
Train: 2018-08-09T12:09:57.223720: step 27797, loss 0.546302.
Train: 2018-08-09T12:09:57.333039: step 27798, loss 0.627717.
Train: 2018-08-09T12:09:57.442388: step 27799, loss 0.578899.
Train: 2018-08-09T12:09:57.536116: step 27800, loss 0.595042.
Test: 2018-08-09T12:09:58.036000: step 27800, loss 0.548909.
Train: 2018-08-09T12:09:58.567153: step 27801, loss 0.498378.
Train: 2018-08-09T12:09:58.660880: step 27802, loss 0.64313.
Train: 2018-08-09T12:09:58.770231: step 27803, loss 0.498785.
Train: 2018-08-09T12:09:58.863927: step 27804, loss 0.54689.
Train: 2018-08-09T12:09:58.974680: step 27805, loss 0.674659.
Train: 2018-08-09T12:09:59.084000: step 27806, loss 0.626627.
Train: 2018-08-09T12:09:59.177758: step 27807, loss 0.515378.
Train: 2018-08-09T12:09:59.287106: step 27808, loss 0.563022.
Train: 2018-08-09T12:09:59.380835: step 27809, loss 0.547249.
Train: 2018-08-09T12:09:59.490183: step 27810, loss 0.515721.
Test: 2018-08-09T12:09:59.990038: step 27810, loss 0.550087.
Train: 2018-08-09T12:10:00.083764: step 27811, loss 0.610419.
Train: 2018-08-09T12:10:00.193113: step 27812, loss 0.515755.
Train: 2018-08-09T12:10:00.302462: step 27813, loss 0.563231.
Train: 2018-08-09T12:10:00.396220: step 27814, loss 0.515644.
Train: 2018-08-09T12:10:00.505540: step 27815, loss 0.515439.
Train: 2018-08-09T12:10:00.599298: step 27816, loss 0.531173.
Train: 2018-08-09T12:10:00.708646: step 27817, loss 0.499117.
Train: 2018-08-09T12:10:00.817999: step 27818, loss 0.628475.
Train: 2018-08-09T12:10:00.912980: step 27819, loss 0.627537.
Train: 2018-08-09T12:10:01.022330: step 27820, loss 0.530481.
Test: 2018-08-09T12:10:01.522212: step 27820, loss 0.54949.
Train: 2018-08-09T12:10:01.615940: step 27821, loss 0.579471.
Train: 2018-08-09T12:10:01.725289: step 27822, loss 0.62707.
Train: 2018-08-09T12:10:01.834668: step 27823, loss 0.611142.
Train: 2018-08-09T12:10:01.928398: step 27824, loss 0.51512.
Train: 2018-08-09T12:10:02.037715: step 27825, loss 0.642554.
Train: 2018-08-09T12:10:02.147090: step 27826, loss 0.531112.
Train: 2018-08-09T12:10:02.240822: step 27827, loss 0.483573.
Train: 2018-08-09T12:10:02.350142: step 27828, loss 0.562965.
Train: 2018-08-09T12:10:02.443900: step 27829, loss 0.51523.
Train: 2018-08-09T12:10:02.553250: step 27830, loss 0.59479.
Test: 2018-08-09T12:10:03.055400: step 27830, loss 0.550987.
Train: 2018-08-09T12:10:03.150675: step 27831, loss 0.57886.
Train: 2018-08-09T12:10:03.260024: step 27832, loss 0.483117.
Train: 2018-08-09T12:10:03.369373: step 27833, loss 0.562876.
Train: 2018-08-09T12:10:03.463101: step 27834, loss 0.498793.
Train: 2018-08-09T12:10:03.572450: step 27835, loss 0.530705.
Train: 2018-08-09T12:10:03.681794: step 27836, loss 0.659373.
Train: 2018-08-09T12:10:03.775528: step 27837, loss 0.530525.
Train: 2018-08-09T12:10:03.884877: step 27838, loss 0.562741.
Train: 2018-08-09T12:10:03.978574: step 27839, loss 0.595054.
Train: 2018-08-09T12:10:04.087954: step 27840, loss 0.659788.
Test: 2018-08-09T12:10:04.587806: step 27840, loss 0.549404.
Train: 2018-08-09T12:10:04.697155: step 27841, loss 0.433364.
Train: 2018-08-09T12:10:04.790883: step 27842, loss 0.627468.
Train: 2018-08-09T12:10:04.900233: step 27843, loss 0.611287.
Train: 2018-08-09T12:10:05.009612: step 27844, loss 0.546516.
Train: 2018-08-09T12:10:05.103340: step 27845, loss 0.611268.
Train: 2018-08-09T12:10:05.212689: step 27846, loss 0.562719.
Train: 2018-08-09T12:10:05.322039: step 27847, loss 0.578887.
Train: 2018-08-09T12:10:05.415766: step 27848, loss 0.578888.
Train: 2018-08-09T12:10:05.525115: step 27849, loss 0.514346.
Train: 2018-08-09T12:10:05.634465: step 27850, loss 0.498211.
Test: 2018-08-09T12:10:06.118696: step 27850, loss 0.550658.
Train: 2018-08-09T12:10:06.222046: step 27851, loss 0.611184.
Train: 2018-08-09T12:10:06.331396: step 27852, loss 0.578886.
Train: 2018-08-09T12:10:06.440745: step 27853, loss 0.611185.
Train: 2018-08-09T12:10:06.534473: step 27854, loss 0.514332.
Train: 2018-08-09T12:10:06.643823: step 27855, loss 0.49819.
Train: 2018-08-09T12:10:06.737550: step 27856, loss 0.417341.
Train: 2018-08-09T12:10:06.846900: step 27857, loss 0.481682.
Train: 2018-08-09T12:10:06.948024: step 27858, loss 0.546386.
Train: 2018-08-09T12:10:07.057341: step 27859, loss 0.660553.
Train: 2018-08-09T12:10:07.166722: step 27860, loss 0.546233.
Test: 2018-08-09T12:10:07.666574: step 27860, loss 0.548455.
Train: 2018-08-09T12:10:07.760332: step 27861, loss 0.51341.
Train: 2018-08-09T12:10:07.869683: step 27862, loss 0.644678.
Train: 2018-08-09T12:10:07.979030: step 27863, loss 0.513207.
Train: 2018-08-09T12:10:08.072757: step 27864, loss 0.546052.
Train: 2018-08-09T12:10:08.182107: step 27865, loss 0.529515.
Train: 2018-08-09T12:10:08.291427: step 27866, loss 0.529456.
Train: 2018-08-09T12:10:08.385184: step 27867, loss 0.496238.
Train: 2018-08-09T12:10:08.494536: step 27868, loss 0.562468.
Train: 2018-08-09T12:10:08.603883: step 27869, loss 0.462566.
Train: 2018-08-09T12:10:08.697611: step 27870, loss 0.579121.
Test: 2018-08-09T12:10:09.197464: step 27870, loss 0.550509.
Train: 2018-08-09T12:10:09.322435: step 27871, loss 0.562402.
Train: 2018-08-09T12:10:09.416191: step 27872, loss 0.629555.
Train: 2018-08-09T12:10:09.525540: step 27873, loss 0.528777.
Train: 2018-08-09T12:10:09.634860: step 27874, loss 0.461447.
Train: 2018-08-09T12:10:09.728618: step 27875, loss 0.596093.
Train: 2018-08-09T12:10:09.837937: step 27876, loss 0.51166.
Train: 2018-08-09T12:10:09.948679: step 27877, loss 0.477654.
Train: 2018-08-09T12:10:10.042407: step 27878, loss 0.715294.
Train: 2018-08-09T12:10:10.151758: step 27879, loss 0.528362.
Train: 2018-08-09T12:10:10.261105: step 27880, loss 0.664381.
Test: 2018-08-09T12:10:10.760959: step 27880, loss 0.547069.
Train: 2018-08-09T12:10:10.870333: step 27881, loss 0.664282.
Train: 2018-08-09T12:10:10.964036: step 27882, loss 0.647123.
Train: 2018-08-09T12:10:11.073415: step 27883, loss 0.579264.
Train: 2018-08-09T12:10:11.182764: step 27884, loss 0.528679.
Train: 2018-08-09T12:10:11.276461: step 27885, loss 0.646415.
Train: 2018-08-09T12:10:11.385810: step 27886, loss 0.495406.
Train: 2018-08-09T12:10:11.495189: step 27887, loss 0.579126.
Train: 2018-08-09T12:10:11.588917: step 27888, loss 0.595772.
Train: 2018-08-09T12:10:11.698267: step 27889, loss 0.562447.
Train: 2018-08-09T12:10:11.807616: step 27890, loss 0.545876.
Test: 2018-08-09T12:10:12.310046: step 27890, loss 0.54822.
Train: 2018-08-09T12:10:12.403775: step 27891, loss 0.56248.
Train: 2018-08-09T12:10:12.513153: step 27892, loss 0.545971.
Train: 2018-08-09T12:10:12.622473: step 27893, loss 0.579006.
Train: 2018-08-09T12:10:12.716201: step 27894, loss 0.595465.
Train: 2018-08-09T12:10:12.825550: step 27895, loss 0.529657.
Train: 2018-08-09T12:10:12.934899: step 27896, loss 0.496878.
Train: 2018-08-09T12:10:13.044248: step 27897, loss 0.611795.
Train: 2018-08-09T12:10:13.138006: step 27898, loss 0.611757.
Train: 2018-08-09T12:10:13.247355: step 27899, loss 0.628068.
Train: 2018-08-09T12:10:13.356705: step 27900, loss 0.627934.
Test: 2018-08-09T12:10:13.856558: step 27900, loss 0.549842.
Train: 2018-08-09T12:10:14.436991: step 27901, loss 0.530077.
Train: 2018-08-09T12:10:14.530719: step 27902, loss 0.513914.
Train: 2018-08-09T12:10:14.640098: step 27903, loss 0.595131.
Train: 2018-08-09T12:10:14.749447: step 27904, loss 0.578897.
Train: 2018-08-09T12:10:14.858797: step 27905, loss 0.578892.
Train: 2018-08-09T12:10:14.952525: step 27906, loss 0.46586.
Train: 2018-08-09T12:10:15.061877: step 27907, loss 0.627313.
Train: 2018-08-09T12:10:15.171193: step 27908, loss 0.562767.
Train: 2018-08-09T12:10:15.280573: step 27909, loss 0.675557.
Train: 2018-08-09T12:10:15.374300: step 27910, loss 0.594947.
Test: 2018-08-09T12:10:15.874154: step 27910, loss 0.550839.
Train: 2018-08-09T12:10:15.984089: step 27911, loss 0.498691.
Train: 2018-08-09T12:10:16.077817: step 27912, loss 0.498801.
Train: 2018-08-09T12:10:16.187166: step 27913, loss 0.610883.
Train: 2018-08-09T12:10:16.296515: step 27914, loss 0.514875.
Train: 2018-08-09T12:10:16.405835: step 27915, loss 0.674858.
Train: 2018-08-09T12:10:16.499592: step 27916, loss 0.578859.
Train: 2018-08-09T12:10:16.608942: step 27917, loss 0.610755.
Train: 2018-08-09T12:10:16.718261: step 27918, loss 0.610687.
Train: 2018-08-09T12:10:16.827610: step 27919, loss 0.64236.
Train: 2018-08-09T12:10:16.921337: step 27920, loss 0.594684.
Test: 2018-08-09T12:10:17.421222: step 27920, loss 0.548913.
Train: 2018-08-09T12:10:17.530602: step 27921, loss 0.578864.
Train: 2018-08-09T12:10:17.639952: step 27922, loss 0.405931.
Train: 2018-08-09T12:10:17.749297: step 27923, loss 0.547439.
Train: 2018-08-09T12:10:17.843026: step 27924, loss 0.57887.
Train: 2018-08-09T12:10:17.952375: step 27925, loss 0.641726.
Train: 2018-08-09T12:10:18.061694: step 27926, loss 0.547485.
Train: 2018-08-09T12:10:18.171073: step 27927, loss 0.56319.
Train: 2018-08-09T12:10:18.280417: step 27928, loss 0.625907.
Train: 2018-08-09T12:10:18.374151: step 27929, loss 0.469281.
Train: 2018-08-09T12:10:18.483500: step 27930, loss 0.531887.
Test: 2018-08-09T12:10:18.985840: step 27930, loss 0.549099.
Train: 2018-08-09T12:10:19.079571: step 27931, loss 0.578877.
Train: 2018-08-09T12:10:19.188916: step 27932, loss 0.641639.
Train: 2018-08-09T12:10:19.298262: step 27933, loss 0.625931.
Train: 2018-08-09T12:10:19.407615: step 27934, loss 0.51621.
Train: 2018-08-09T12:10:19.516964: step 27935, loss 0.50056.
Train: 2018-08-09T12:10:19.626313: step 27936, loss 0.625909.
Train: 2018-08-09T12:10:19.735663: step 27937, loss 0.469137.
Train: 2018-08-09T12:10:19.829390: step 27938, loss 0.531771.
Train: 2018-08-09T12:10:19.938740: step 27939, loss 0.610332.
Train: 2018-08-09T12:10:20.048059: step 27940, loss 0.578869.
Test: 2018-08-09T12:10:20.547942: step 27940, loss 0.548332.
Train: 2018-08-09T12:10:20.641699: step 27941, loss 0.594628.
Train: 2018-08-09T12:10:20.751048: step 27942, loss 0.578864.
Train: 2018-08-09T12:10:20.860398: step 27943, loss 0.484215.
Train: 2018-08-09T12:10:20.972045: step 27944, loss 0.515668.
Train: 2018-08-09T12:10:21.065770: step 27945, loss 0.515528.
Train: 2018-08-09T12:10:21.175119: step 27946, loss 0.53123.
Train: 2018-08-09T12:10:21.284468: step 27947, loss 0.499237.
Train: 2018-08-09T12:10:21.393817: step 27948, loss 0.578865.
Train: 2018-08-09T12:10:21.503167: step 27949, loss 0.546791.
Train: 2018-08-09T12:10:21.612518: step 27950, loss 0.578847.
Test: 2018-08-09T12:10:22.096748: step 27950, loss 0.549454.
Train: 2018-08-09T12:10:22.206095: step 27951, loss 0.595048.
Train: 2018-08-09T12:10:22.315475: step 27952, loss 0.465708.
Train: 2018-08-09T12:10:22.424824: step 27953, loss 0.611332.
Train: 2018-08-09T12:10:22.534174: step 27954, loss 0.513807.
Train: 2018-08-09T12:10:22.643523: step 27955, loss 0.546235.
Train: 2018-08-09T12:10:22.752843: step 27956, loss 0.497016.
Train: 2018-08-09T12:10:22.846600: step 27957, loss 0.52966.
Train: 2018-08-09T12:10:22.958273: step 27958, loss 0.613251.
Train: 2018-08-09T12:10:23.067598: step 27959, loss 0.428525.
Train: 2018-08-09T12:10:23.176946: step 27960, loss 0.478284.
Test: 2018-08-09T12:10:23.674412: step 27960, loss 0.550278.
Train: 2018-08-09T12:10:23.783754: step 27961, loss 0.663138.
Train: 2018-08-09T12:10:23.877482: step 27962, loss 0.513478.
Train: 2018-08-09T12:10:23.986861: step 27963, loss 0.579933.
Train: 2018-08-09T12:10:24.096210: step 27964, loss 0.666993.
Train: 2018-08-09T12:10:24.205560: step 27965, loss 0.596454.
Train: 2018-08-09T12:10:24.314909: step 27966, loss 0.528611.
Train: 2018-08-09T12:10:24.424257: step 27967, loss 0.595427.
Train: 2018-08-09T12:10:24.533608: step 27968, loss 0.595772.
Train: 2018-08-09T12:10:24.642929: step 27969, loss 0.595827.
Train: 2018-08-09T12:10:24.736684: step 27970, loss 0.545693.
Test: 2018-08-09T12:10:25.239818: step 27970, loss 0.548027.
Train: 2018-08-09T12:10:25.349197: step 27971, loss 0.579108.
Train: 2018-08-09T12:10:25.458517: step 27972, loss 0.54611.
Train: 2018-08-09T12:10:25.567864: step 27973, loss 0.462636.
Train: 2018-08-09T12:10:25.677245: step 27974, loss 0.56239.
Train: 2018-08-09T12:10:25.786595: step 27975, loss 0.545771.
Train: 2018-08-09T12:10:25.884253: step 27976, loss 0.728953.
Train: 2018-08-09T12:10:25.993632: step 27977, loss 0.529299.
Train: 2018-08-09T12:10:26.102981: step 27978, loss 0.496187.
Train: 2018-08-09T12:10:26.212331: step 27979, loss 0.562479.
Train: 2018-08-09T12:10:26.321679: step 27980, loss 0.595594.
Test: 2018-08-09T12:10:26.821543: step 27980, loss 0.548234.
Train: 2018-08-09T12:10:26.915289: step 27981, loss 0.430128.
Train: 2018-08-09T12:10:27.024609: step 27982, loss 0.496233.
Train: 2018-08-09T12:10:27.133988: step 27983, loss 0.595642.
Train: 2018-08-09T12:10:27.243337: step 27984, loss 0.562457.
Train: 2018-08-09T12:10:27.352686: step 27985, loss 0.495965.
Train: 2018-08-09T12:10:27.462038: step 27986, loss 0.545792.
Train: 2018-08-09T12:10:27.571385: step 27987, loss 0.595777.
Train: 2018-08-09T12:10:27.665113: step 27988, loss 0.545733.
Train: 2018-08-09T12:10:27.774464: step 27989, loss 0.495588.
Train: 2018-08-09T12:10:27.883812: step 27990, loss 0.545671.
Test: 2018-08-09T12:10:28.383665: step 27990, loss 0.547959.
Train: 2018-08-09T12:10:28.493012: step 27991, loss 0.545634.
Train: 2018-08-09T12:10:28.602363: step 27992, loss 0.56239.
Train: 2018-08-09T12:10:28.711741: step 27993, loss 0.652056.
Train: 2018-08-09T12:10:28.821091: step 27994, loss 0.528756.
Train: 2018-08-09T12:10:28.931875: step 27995, loss 0.495104.
Train: 2018-08-09T12:10:29.041223: step 27996, loss 0.528701.
Train: 2018-08-09T12:10:29.150573: step 27997, loss 0.697254.
Train: 2018-08-09T12:10:29.259901: step 27998, loss 0.478126.
Train: 2018-08-09T12:10:29.353653: step 27999, loss 0.427525.
Train: 2018-08-09T12:10:29.462999: step 28000, loss 0.545467.
Test: 2018-08-09T12:10:29.962853: step 28000, loss 0.54842.
Train: 2018-08-09T12:10:30.525219: step 28001, loss 0.494668.
Train: 2018-08-09T12:10:30.634598: step 28002, loss 0.630216.
Train: 2018-08-09T12:10:30.743947: step 28003, loss 0.562339.
Train: 2018-08-09T12:10:30.853299: step 28004, loss 0.51127.
Train: 2018-08-09T12:10:30.949299: step 28005, loss 0.613449.
Train: 2018-08-09T12:10:31.058649: step 28006, loss 0.61346.
Train: 2018-08-09T12:10:31.168028: step 28007, loss 0.51127.
Train: 2018-08-09T12:10:31.277348: step 28008, loss 0.57939.
Train: 2018-08-09T12:10:31.386727: step 28009, loss 0.494101.
Train: 2018-08-09T12:10:31.496076: step 28010, loss 0.681695.
Test: 2018-08-09T12:10:31.980321: step 28010, loss 0.547.
Train: 2018-08-09T12:10:32.089656: step 28011, loss 0.494178.
Train: 2018-08-09T12:10:32.199006: step 28012, loss 0.528309.
Train: 2018-08-09T12:10:32.308355: step 28013, loss 0.613702.
Train: 2018-08-09T12:10:32.417734: step 28014, loss 0.477158.
Train: 2018-08-09T12:10:32.527083: step 28015, loss 0.494137.
Train: 2018-08-09T12:10:32.636403: step 28016, loss 0.562216.
Train: 2018-08-09T12:10:32.745782: step 28017, loss 0.561977.
Train: 2018-08-09T12:10:32.855132: step 28018, loss 0.613887.
Train: 2018-08-09T12:10:32.966750: step 28019, loss 0.494166.
Train: 2018-08-09T12:10:33.060508: step 28020, loss 0.579959.
Test: 2018-08-09T12:10:33.560380: step 28020, loss 0.546929.
Train: 2018-08-09T12:10:33.669710: step 28021, loss 0.545232.
Train: 2018-08-09T12:10:33.779088: step 28022, loss 0.5281.
Train: 2018-08-09T12:10:33.888440: step 28023, loss 0.528222.
Train: 2018-08-09T12:10:33.997355: step 28024, loss 0.59647.
Train: 2018-08-09T12:10:34.106735: step 28025, loss 0.562329.
Train: 2018-08-09T12:10:34.216084: step 28026, loss 0.528247.
Train: 2018-08-09T12:10:34.325433: step 28027, loss 0.476846.
Train: 2018-08-09T12:10:34.419163: step 28028, loss 0.511101.
Train: 2018-08-09T12:10:34.528480: step 28029, loss 0.579503.
Train: 2018-08-09T12:10:34.637829: step 28030, loss 0.562398.
Test: 2018-08-09T12:10:35.139314: step 28030, loss 0.5456.
Train: 2018-08-09T12:10:35.248656: step 28031, loss 0.545222.
Train: 2018-08-09T12:10:35.358035: step 28032, loss 0.579527.
Train: 2018-08-09T12:10:35.467382: step 28033, loss 0.527997.
Train: 2018-08-09T12:10:35.576730: step 28034, loss 0.631016.
Train: 2018-08-09T12:10:35.686049: step 28035, loss 0.562332.
Train: 2018-08-09T12:10:35.795428: step 28036, loss 0.528051.
Train: 2018-08-09T12:10:35.904747: step 28037, loss 0.442255.
Train: 2018-08-09T12:10:36.014127: step 28038, loss 0.59669.
Train: 2018-08-09T12:10:36.123471: step 28039, loss 0.648247.
Train: 2018-08-09T12:10:36.232825: step 28040, loss 0.545152.
Test: 2018-08-09T12:10:36.732679: step 28040, loss 0.550157.
Train: 2018-08-09T12:10:36.826436: step 28041, loss 0.545209.
Train: 2018-08-09T12:10:36.936536: step 28042, loss 0.579463.
Train: 2018-08-09T12:10:37.045891: step 28043, loss 0.64797.
Train: 2018-08-09T12:10:37.155204: step 28044, loss 0.681947.
Train: 2018-08-09T12:10:37.264584: step 28045, loss 0.579364.
Train: 2018-08-09T12:10:37.373934: step 28046, loss 0.579314.
Train: 2018-08-09T12:10:37.483283: step 28047, loss 0.596172.
Train: 2018-08-09T12:10:37.592603: step 28048, loss 0.579219.
Train: 2018-08-09T12:10:37.701981: step 28049, loss 0.679877.
Train: 2018-08-09T12:10:37.811330: step 28050, loss 0.64593.
Test: 2018-08-09T12:10:38.311184: step 28050, loss 0.548147.
Train: 2018-08-09T12:10:38.404911: step 28051, loss 0.496014.
Train: 2018-08-09T12:10:38.514260: step 28052, loss 0.628648.
Train: 2018-08-09T12:10:38.623609: step 28053, loss 0.694228.
Train: 2018-08-09T12:10:38.732959: step 28054, loss 0.59531.
Train: 2018-08-09T12:10:38.842338: step 28055, loss 0.562649.
Train: 2018-08-09T12:10:38.951656: step 28056, loss 0.530366.
Train: 2018-08-09T12:10:39.061037: step 28057, loss 0.51444.
Train: 2018-08-09T12:10:39.170385: step 28058, loss 0.450412.
Train: 2018-08-09T12:10:39.279736: step 28059, loss 0.626978.
Train: 2018-08-09T12:10:39.389083: step 28060, loss 0.466803.
Test: 2018-08-09T12:10:39.888936: step 28060, loss 0.548465.
Train: 2018-08-09T12:10:39.984943: step 28061, loss 0.642888.
Train: 2018-08-09T12:10:40.090864: step 28062, loss 0.610822.
Train: 2018-08-09T12:10:40.200214: step 28063, loss 0.530987.
Train: 2018-08-09T12:10:40.325186: step 28064, loss 0.515067.
Train: 2018-08-09T12:10:40.434504: step 28065, loss 0.562752.
Train: 2018-08-09T12:10:40.543853: step 28066, loss 0.562949.
Train: 2018-08-09T12:10:40.653232: step 28067, loss 0.530763.
Train: 2018-08-09T12:10:40.746960: step 28068, loss 0.482339.
Train: 2018-08-09T12:10:40.856309: step 28069, loss 0.643636.
Train: 2018-08-09T12:10:40.965659: step 28070, loss 0.546664.
Test: 2018-08-09T12:10:41.465511: step 28070, loss 0.548521.
Train: 2018-08-09T12:10:41.574889: step 28071, loss 0.626455.
Train: 2018-08-09T12:10:41.684238: step 28072, loss 0.596797.
Train: 2018-08-09T12:10:41.793587: step 28073, loss 0.528702.
Train: 2018-08-09T12:10:41.902937: step 28074, loss 0.627389.
Train: 2018-08-09T12:10:42.012287: step 28075, loss 0.706371.
Train: 2018-08-09T12:10:42.121637: step 28076, loss 0.67356.
Train: 2018-08-09T12:10:42.230954: step 28077, loss 0.532225.
Train: 2018-08-09T12:10:42.324707: step 28078, loss 0.578673.
Train: 2018-08-09T12:10:42.434062: step 28079, loss 0.578397.
Train: 2018-08-09T12:10:42.543411: step 28080, loss 0.580209.
Test: 2018-08-09T12:10:43.043265: step 28080, loss 0.548975.
Train: 2018-08-09T12:10:43.152642: step 28081, loss 0.533026.
Train: 2018-08-09T12:10:43.246371: step 28082, loss 0.610145.
Train: 2018-08-09T12:10:43.355720: step 28083, loss 0.547805.
Train: 2018-08-09T12:10:43.465069: step 28084, loss 0.579033.
Train: 2018-08-09T12:10:43.574420: step 28085, loss 0.501677.
Train: 2018-08-09T12:10:43.683738: step 28086, loss 0.517133.
Train: 2018-08-09T12:10:43.793099: step 28087, loss 0.57893.
Train: 2018-08-09T12:10:43.886845: step 28088, loss 0.517065.
Train: 2018-08-09T12:10:43.998693: step 28089, loss 0.516986.
Train: 2018-08-09T12:10:44.108072: step 28090, loss 0.501343.
Test: 2018-08-09T12:10:44.607925: step 28090, loss 0.548767.
Train: 2018-08-09T12:10:44.717303: step 28091, loss 0.454438.
Train: 2018-08-09T12:10:44.826622: step 28092, loss 0.532012.
Train: 2018-08-09T12:10:44.920380: step 28093, loss 0.56318.
Train: 2018-08-09T12:10:45.029730: step 28094, loss 0.500072.
Train: 2018-08-09T12:10:45.139078: step 28095, loss 0.658024.
Train: 2018-08-09T12:10:45.248428: step 28096, loss 0.531218.
Train: 2018-08-09T12:10:45.342156: step 28097, loss 0.562931.
Train: 2018-08-09T12:10:45.451499: step 28098, loss 0.626801.
Train: 2018-08-09T12:10:45.560854: step 28099, loss 0.434798.
Train: 2018-08-09T12:10:45.670203: step 28100, loss 0.562815.
Test: 2018-08-09T12:10:46.170685: step 28100, loss 0.547654.
Train: 2018-08-09T12:10:46.764319: step 28101, loss 0.546659.
Train: 2018-08-09T12:10:46.858052: step 28102, loss 0.708326.
Train: 2018-08-09T12:10:46.967401: step 28103, loss 0.497988.
Train: 2018-08-09T12:10:47.076751: step 28104, loss 0.595107.
Train: 2018-08-09T12:10:47.186099: step 28105, loss 0.481583.
Train: 2018-08-09T12:10:47.295448: step 28106, loss 0.611403.
Train: 2018-08-09T12:10:47.389146: step 28107, loss 0.513824.
Train: 2018-08-09T12:10:47.498527: step 28108, loss 0.660403.
Train: 2018-08-09T12:10:47.607844: step 28109, loss 0.595285.
Train: 2018-08-09T12:10:47.717224: step 28110, loss 0.578885.
Test: 2018-08-09T12:10:48.201471: step 28110, loss 0.551064.
Train: 2018-08-09T12:10:48.310834: step 28111, loss 0.578943.
Train: 2018-08-09T12:10:48.420184: step 28112, loss 0.530092.
Train: 2018-08-09T12:10:48.529534: step 28113, loss 0.529942.
Train: 2018-08-09T12:10:48.623262: step 28114, loss 0.497503.
Train: 2018-08-09T12:10:48.732610: step 28115, loss 0.59516.
Train: 2018-08-09T12:10:48.841959: step 28116, loss 0.578323.
Train: 2018-08-09T12:10:48.952762: step 28117, loss 0.611681.
Train: 2018-08-09T12:10:49.062082: step 28118, loss 0.578489.
Train: 2018-08-09T12:10:49.155840: step 28119, loss 0.512636.
Train: 2018-08-09T12:10:49.265158: step 28120, loss 0.645948.
Test: 2018-08-09T12:10:49.765042: step 28120, loss 0.546831.
Train: 2018-08-09T12:10:49.874389: step 28121, loss 0.647043.
Train: 2018-08-09T12:10:49.968147: step 28122, loss 0.643784.
Train: 2018-08-09T12:10:50.077497: step 28123, loss 0.578402.
Train: 2018-08-09T12:10:50.186847: step 28124, loss 0.465628.
Train: 2018-08-09T12:10:50.296196: step 28125, loss 0.628412.
Train: 2018-08-09T12:10:50.405539: step 28126, loss 0.465319.
Train: 2018-08-09T12:10:50.499242: step 28127, loss 0.546622.
Train: 2018-08-09T12:10:50.608621: step 28128, loss 0.578783.
Train: 2018-08-09T12:10:50.717972: step 28129, loss 0.65939.
Train: 2018-08-09T12:10:50.811668: step 28130, loss 0.627073.
Test: 2018-08-09T12:10:51.312992: step 28130, loss 0.549014.
Train: 2018-08-09T12:10:51.422339: step 28131, loss 0.578896.
Train: 2018-08-09T12:10:51.531688: step 28132, loss 0.562866.
Train: 2018-08-09T12:10:51.625419: step 28133, loss 0.642599.
Train: 2018-08-09T12:10:51.734765: step 28134, loss 0.610604.
Train: 2018-08-09T12:10:51.844084: step 28135, loss 0.610532.
Train: 2018-08-09T12:10:51.953433: step 28136, loss 0.452567.
Train: 2018-08-09T12:10:52.047161: step 28137, loss 0.594619.
Train: 2018-08-09T12:10:52.156535: step 28138, loss 0.578944.
Train: 2018-08-09T12:10:52.265891: step 28139, loss 0.625997.
Train: 2018-08-09T12:10:52.375243: step 28140, loss 0.56315.
Test: 2018-08-09T12:10:52.875091: step 28140, loss 0.550928.
Train: 2018-08-09T12:10:52.970242: step 28141, loss 0.610155.
Train: 2018-08-09T12:10:53.079590: step 28142, loss 0.578917.
Train: 2018-08-09T12:10:53.188970: step 28143, loss 0.672431.
Train: 2018-08-09T12:10:53.298319: step 28144, loss 0.641037.
Train: 2018-08-09T12:10:53.407668: step 28145, loss 0.532552.
Train: 2018-08-09T12:10:53.501396: step 28146, loss 0.594364.
Train: 2018-08-09T12:10:53.610745: step 28147, loss 0.517539.
Train: 2018-08-09T12:10:53.720095: step 28148, loss 0.56366.
Train: 2018-08-09T12:10:53.829415: step 28149, loss 0.578996.
Train: 2018-08-09T12:10:53.942029: step 28150, loss 0.517852.
Test: 2018-08-09T12:10:54.437036: step 28150, loss 0.551138.
Train: 2018-08-09T12:10:54.530796: step 28151, loss 0.594289.
Train: 2018-08-09T12:10:54.640112: step 28152, loss 0.548462.
Train: 2018-08-09T12:10:54.749492: step 28153, loss 0.487373.
Train: 2018-08-09T12:10:54.858844: step 28154, loss 0.624881.
Train: 2018-08-09T12:10:54.954895: step 28155, loss 0.563694.
Train: 2018-08-09T12:10:55.064245: step 28156, loss 0.594302.
Train: 2018-08-09T12:10:55.173593: step 28157, loss 0.609633.
Train: 2018-08-09T12:10:55.267322: step 28158, loss 0.517735.
Train: 2018-08-09T12:10:55.376671: step 28159, loss 0.410414.
Train: 2018-08-09T12:10:55.486023: step 28160, loss 0.563576.
Test: 2018-08-09T12:10:55.984171: step 28160, loss 0.549032.
Train: 2018-08-09T12:10:56.093554: step 28161, loss 0.532522.
Train: 2018-08-09T12:10:56.187278: step 28162, loss 0.57871.
Train: 2018-08-09T12:10:56.296597: step 28163, loss 0.56251.
Train: 2018-08-09T12:10:56.405976: step 28164, loss 0.609136.
Train: 2018-08-09T12:10:56.499675: step 28165, loss 0.531293.
Train: 2018-08-09T12:10:56.609054: step 28166, loss 0.610617.
Train: 2018-08-09T12:10:56.718406: step 28167, loss 0.709348.
Train: 2018-08-09T12:10:56.827723: step 28168, loss 0.59504.
Train: 2018-08-09T12:10:56.924885: step 28169, loss 0.513714.
Train: 2018-08-09T12:10:57.034234: step 28170, loss 0.46303.
Test: 2018-08-09T12:10:57.536718: step 28170, loss 0.549356.
Train: 2018-08-09T12:10:57.630476: step 28171, loss 0.563567.
Train: 2018-08-09T12:10:57.739826: step 28172, loss 0.448427.
Train: 2018-08-09T12:10:57.849175: step 28173, loss 0.594301.
Train: 2018-08-09T12:10:57.942903: step 28174, loss 0.724729.
Train: 2018-08-09T12:10:58.052252: step 28175, loss 0.550041.
Train: 2018-08-09T12:10:58.161572: step 28176, loss 0.560643.
Train: 2018-08-09T12:10:58.270921: step 28177, loss 0.566553.
Train: 2018-08-09T12:10:58.380270: step 28178, loss 0.592036.
Train: 2018-08-09T12:10:58.489648: step 28179, loss 0.546412.
Train: 2018-08-09T12:10:58.583377: step 28180, loss 0.463626.
Test: 2018-08-09T12:10:59.083230: step 28180, loss 0.547615.
Train: 2018-08-09T12:10:59.192608: step 28181, loss 0.591501.
Train: 2018-08-09T12:10:59.301928: step 28182, loss 0.633021.
Train: 2018-08-09T12:10:59.395685: step 28183, loss 0.563291.
Train: 2018-08-09T12:10:59.505035: step 28184, loss 0.517056.
Train: 2018-08-09T12:10:59.614384: step 28185, loss 0.613954.
Train: 2018-08-09T12:10:59.715226: step 28186, loss 0.481941.
Train: 2018-08-09T12:10:59.824546: step 28187, loss 0.581454.
Train: 2018-08-09T12:10:59.918304: step 28188, loss 0.465819.
Train: 2018-08-09T12:11:00.027655: step 28189, loss 0.546345.
Train: 2018-08-09T12:11:00.137002: step 28190, loss 0.54745.
Test: 2018-08-09T12:11:00.636856: step 28190, loss 0.546284.
Train: 2018-08-09T12:11:00.730582: step 28191, loss 0.528824.
Train: 2018-08-09T12:11:00.839962: step 28192, loss 0.54692.
Train: 2018-08-09T12:11:00.949314: step 28193, loss 0.594557.
Train: 2018-08-09T12:11:01.043038: step 28194, loss 0.564183.
Train: 2018-08-09T12:11:01.152389: step 28195, loss 0.599575.
Train: 2018-08-09T12:11:01.261708: step 28196, loss 0.545163.
Train: 2018-08-09T12:11:01.371086: step 28197, loss 0.614126.
Train: 2018-08-09T12:11:01.464814: step 28198, loss 0.612807.
Train: 2018-08-09T12:11:01.574167: step 28199, loss 0.48234.
Train: 2018-08-09T12:11:01.683513: step 28200, loss 0.612043.
Test: 2018-08-09T12:11:02.183394: step 28200, loss 0.548221.
Train: 2018-08-09T12:11:02.730141: step 28201, loss 0.627124.
Train: 2018-08-09T12:11:02.839490: step 28202, loss 0.530298.
Train: 2018-08-09T12:11:02.934662: step 28203, loss 0.579085.
Train: 2018-08-09T12:11:03.044010: step 28204, loss 0.515656.
Train: 2018-08-09T12:11:03.153361: step 28205, loss 0.499028.
Train: 2018-08-09T12:11:03.262709: step 28206, loss 0.530728.
Train: 2018-08-09T12:11:03.356438: step 28207, loss 0.626922.
Train: 2018-08-09T12:11:03.465757: step 28208, loss 0.467083.
Train: 2018-08-09T12:11:03.575136: step 28209, loss 0.578914.
Train: 2018-08-09T12:11:03.684455: step 28210, loss 0.578936.
Test: 2018-08-09T12:11:04.177993: step 28210, loss 0.550765.
Train: 2018-08-09T12:11:04.287371: step 28211, loss 0.627355.
Train: 2018-08-09T12:11:04.396722: step 28212, loss 0.466396.
Train: 2018-08-09T12:11:04.490419: step 28213, loss 0.627061.
Train: 2018-08-09T12:11:04.599797: step 28214, loss 0.530747.
Train: 2018-08-09T12:11:04.709149: step 28215, loss 0.578537.
Train: 2018-08-09T12:11:04.802880: step 28216, loss 0.627735.
Train: 2018-08-09T12:11:04.921439: step 28217, loss 0.67543.
Train: 2018-08-09T12:11:05.014702: step 28218, loss 0.546917.
Train: 2018-08-09T12:11:05.124051: step 28219, loss 0.5789.
Train: 2018-08-09T12:11:05.233401: step 28220, loss 0.530848.
Test: 2018-08-09T12:11:05.717632: step 28220, loss 0.550911.
Train: 2018-08-09T12:11:05.827011: step 28221, loss 0.594902.
Train: 2018-08-09T12:11:05.936361: step 28222, loss 0.610801.
Train: 2018-08-09T12:11:06.030058: step 28223, loss 0.515083.
Train: 2018-08-09T12:11:06.139436: step 28224, loss 0.531055.
Train: 2018-08-09T12:11:06.248757: step 28225, loss 0.531055.
Train: 2018-08-09T12:11:06.342514: step 28226, loss 0.403482.
Train: 2018-08-09T12:11:06.451864: step 28227, loss 0.562868.
Train: 2018-08-09T12:11:06.561213: step 28228, loss 0.53069.
Train: 2018-08-09T12:11:06.670561: step 28229, loss 0.546674.
Train: 2018-08-09T12:11:06.764290: step 28230, loss 0.530553.
Test: 2018-08-09T12:11:07.264142: step 28230, loss 0.549287.
Train: 2018-08-09T12:11:07.373492: step 28231, loss 0.562757.
Train: 2018-08-09T12:11:07.467220: step 28232, loss 0.612112.
Train: 2018-08-09T12:11:07.576569: step 28233, loss 0.595149.
Train: 2018-08-09T12:11:07.670327: step 28234, loss 0.643966.
Train: 2018-08-09T12:11:07.779675: step 28235, loss 0.497558.
Train: 2018-08-09T12:11:07.888995: step 28236, loss 0.497492.
Train: 2018-08-09T12:11:07.985052: step 28237, loss 0.513742.
Train: 2018-08-09T12:11:08.094372: step 28238, loss 0.546313.
Train: 2018-08-09T12:11:08.203751: step 28239, loss 0.513468.
Train: 2018-08-09T12:11:08.297478: step 28240, loss 0.546169.
Test: 2018-08-09T12:11:08.797332: step 28240, loss 0.549603.
Train: 2018-08-09T12:11:08.906711: step 28241, loss 0.546028.
Train: 2018-08-09T12:11:09.016041: step 28242, loss 0.562449.
Train: 2018-08-09T12:11:09.125408: step 28243, loss 0.579122.
Train: 2018-08-09T12:11:09.234753: step 28244, loss 0.579113.
Train: 2018-08-09T12:11:09.344077: step 28245, loss 0.512598.
Train: 2018-08-09T12:11:09.453457: step 28246, loss 0.578988.
Train: 2018-08-09T12:11:09.547184: step 28247, loss 0.628991.
Train: 2018-08-09T12:11:09.656503: step 28248, loss 0.56224.
Train: 2018-08-09T12:11:09.765883: step 28249, loss 0.496085.
Train: 2018-08-09T12:11:09.875201: step 28250, loss 0.578795.
Test: 2018-08-09T12:11:10.361846: step 28250, loss 0.549224.
Train: 2018-08-09T12:11:10.471194: step 28251, loss 0.477393.
Train: 2018-08-09T12:11:10.564952: step 28252, loss 0.545772.
Train: 2018-08-09T12:11:10.674301: step 28253, loss 0.477584.
Train: 2018-08-09T12:11:10.783621: step 28254, loss 0.543656.
Train: 2018-08-09T12:11:10.877379: step 28255, loss 0.511753.
Train: 2018-08-09T12:11:10.986698: step 28256, loss 0.580431.
Train: 2018-08-09T12:11:11.096079: step 28257, loss 0.544385.
Train: 2018-08-09T12:11:11.189805: step 28258, loss 0.547285.
Train: 2018-08-09T12:11:11.299123: step 28259, loss 0.668713.
Train: 2018-08-09T12:11:11.408474: step 28260, loss 0.550884.
Test: 2018-08-09T12:11:11.892736: step 28260, loss 0.549214.
Train: 2018-08-09T12:11:12.003511: step 28261, loss 0.546656.
Train: 2018-08-09T12:11:12.112888: step 28262, loss 0.579085.
Train: 2018-08-09T12:11:12.206618: step 28263, loss 0.527673.
Train: 2018-08-09T12:11:12.315967: step 28264, loss 0.579337.
Train: 2018-08-09T12:11:12.409699: step 28265, loss 0.578235.
Train: 2018-08-09T12:11:12.519015: step 28266, loss 0.545618.
Train: 2018-08-09T12:11:12.628364: step 28267, loss 0.649126.
Train: 2018-08-09T12:11:12.737743: step 28268, loss 0.511214.
Train: 2018-08-09T12:11:12.831471: step 28269, loss 0.477243.
Train: 2018-08-09T12:11:12.940820: step 28270, loss 0.529516.
Test: 2018-08-09T12:11:13.440672: step 28270, loss 0.549135.
Train: 2018-08-09T12:11:13.550022: step 28271, loss 0.562077.
Train: 2018-08-09T12:11:13.643780: step 28272, loss 0.562604.
Train: 2018-08-09T12:11:13.753129: step 28273, loss 0.528306.
Train: 2018-08-09T12:11:13.862478: step 28274, loss 0.512954.
Train: 2018-08-09T12:11:13.971766: step 28275, loss 0.512139.
Train: 2018-08-09T12:11:14.081145: step 28276, loss 0.49467.
Train: 2018-08-09T12:11:14.174873: step 28277, loss 0.494752.
Train: 2018-08-09T12:11:14.284223: step 28278, loss 0.545621.
Train: 2018-08-09T12:11:14.393541: step 28279, loss 0.629754.
Train: 2018-08-09T12:11:14.487302: step 28280, loss 0.57952.
Test: 2018-08-09T12:11:15.002773: step 28280, loss 0.550215.
Train: 2018-08-09T12:11:15.096531: step 28281, loss 0.647468.
Train: 2018-08-09T12:11:15.205880: step 28282, loss 0.510961.
Train: 2018-08-09T12:11:15.315230: step 28283, loss 0.495016.
Train: 2018-08-09T12:11:15.408958: step 28284, loss 0.681288.
Train: 2018-08-09T12:11:15.518299: step 28285, loss 0.647404.
Train: 2018-08-09T12:11:15.627626: step 28286, loss 0.477197.
Train: 2018-08-09T12:11:15.736999: step 28287, loss 0.562496.
Train: 2018-08-09T12:11:15.830734: step 28288, loss 0.596499.
Train: 2018-08-09T12:11:15.942422: step 28289, loss 0.528697.
Train: 2018-08-09T12:11:16.036120: step 28290, loss 0.561842.
Test: 2018-08-09T12:11:16.536002: step 28290, loss 0.547872.
Train: 2018-08-09T12:11:16.645385: step 28291, loss 0.612631.
Train: 2018-08-09T12:11:16.754751: step 28292, loss 0.512092.
Train: 2018-08-09T12:11:16.848459: step 28293, loss 0.511811.
Train: 2018-08-09T12:11:16.957779: step 28294, loss 0.526471.
Train: 2018-08-09T12:11:17.051536: step 28295, loss 0.562665.
Train: 2018-08-09T12:11:17.160856: step 28296, loss 0.495157.
Train: 2018-08-09T12:11:17.270205: step 28297, loss 0.680053.
Train: 2018-08-09T12:11:17.379554: step 28298, loss 0.596112.
Train: 2018-08-09T12:11:17.488904: step 28299, loss 0.629116.
Train: 2018-08-09T12:11:17.582632: step 28300, loss 0.612812.
Test: 2018-08-09T12:11:18.084856: step 28300, loss 0.547423.
Train: 2018-08-09T12:11:18.672314: step 28301, loss 0.462014.
Train: 2018-08-09T12:11:18.781694: step 28302, loss 0.545768.
Train: 2018-08-09T12:11:18.875422: step 28303, loss 0.529132.
Train: 2018-08-09T12:11:18.984772: step 28304, loss 0.579261.
Train: 2018-08-09T12:11:19.094120: step 28305, loss 0.612263.
Train: 2018-08-09T12:11:19.187848: step 28306, loss 0.545795.
Train: 2018-08-09T12:11:19.297197: step 28307, loss 0.496129.
Train: 2018-08-09T12:11:19.406517: step 28308, loss 0.529203.
Train: 2018-08-09T12:11:19.500245: step 28309, loss 0.628121.
Train: 2018-08-09T12:11:19.609593: step 28310, loss 0.562012.
Test: 2018-08-09T12:11:20.101098: step 28310, loss 0.548129.
Train: 2018-08-09T12:11:20.210447: step 28311, loss 0.595357.
Train: 2018-08-09T12:11:20.319825: step 28312, loss 0.579295.
Train: 2018-08-09T12:11:20.429175: step 28313, loss 0.47863.
Train: 2018-08-09T12:11:20.522903: step 28314, loss 0.562615.
Train: 2018-08-09T12:11:20.632223: step 28315, loss 0.563769.
Train: 2018-08-09T12:11:20.741572: step 28316, loss 0.62987.
Train: 2018-08-09T12:11:20.835329: step 28317, loss 0.56299.
Train: 2018-08-09T12:11:20.944679: step 28318, loss 0.61168.
Train: 2018-08-09T12:11:21.054029: step 28319, loss 0.628606.
Train: 2018-08-09T12:11:21.147726: step 28320, loss 0.479998.
Test: 2018-08-09T12:11:21.647608: step 28320, loss 0.550225.
Train: 2018-08-09T12:11:21.756957: step 28321, loss 0.544955.
Train: 2018-08-09T12:11:21.850685: step 28322, loss 0.595685.
Train: 2018-08-09T12:11:21.960035: step 28323, loss 0.660831.
Train: 2018-08-09T12:11:22.069413: step 28324, loss 0.546297.
Train: 2018-08-09T12:11:22.163141: step 28325, loss 0.594858.
Train: 2018-08-09T12:11:22.272491: step 28326, loss 0.611559.
Train: 2018-08-09T12:11:22.381840: step 28327, loss 0.579331.
Train: 2018-08-09T12:11:22.475568: step 28328, loss 0.562586.
Train: 2018-08-09T12:11:22.584916: step 28329, loss 0.610713.
Train: 2018-08-09T12:11:22.694269: step 28330, loss 0.595036.
Test: 2018-08-09T12:11:23.180787: step 28330, loss 0.547757.
Train: 2018-08-09T12:11:23.290164: step 28331, loss 0.611201.
Train: 2018-08-09T12:11:23.399514: step 28332, loss 0.434873.
Train: 2018-08-09T12:11:23.493212: step 28333, loss 0.435226.
Train: 2018-08-09T12:11:23.602591: step 28334, loss 0.562547.
Train: 2018-08-09T12:11:23.711910: step 28335, loss 0.498334.
Train: 2018-08-09T12:11:23.805669: step 28336, loss 0.482463.
Train: 2018-08-09T12:11:23.914988: step 28337, loss 0.627373.
Train: 2018-08-09T12:11:24.024367: step 28338, loss 0.49829.
Train: 2018-08-09T12:11:24.133720: step 28339, loss 0.579262.
Train: 2018-08-09T12:11:24.227444: step 28340, loss 0.480703.
Test: 2018-08-09T12:11:24.727297: step 28340, loss 0.548404.
Train: 2018-08-09T12:11:24.836646: step 28341, loss 0.546426.
Train: 2018-08-09T12:11:24.931898: step 28342, loss 0.530201.
Train: 2018-08-09T12:11:25.041248: step 28343, loss 0.545899.
Train: 2018-08-09T12:11:25.150594: step 28344, loss 0.630062.
Train: 2018-08-09T12:11:25.244325: step 28345, loss 0.564473.
Train: 2018-08-09T12:11:25.353674: step 28346, loss 0.561489.
Train: 2018-08-09T12:11:25.463024: step 28347, loss 0.528447.
Train: 2018-08-09T12:11:25.556755: step 28348, loss 0.546312.
Train: 2018-08-09T12:11:25.666101: step 28349, loss 0.477112.
Train: 2018-08-09T12:11:25.775420: step 28350, loss 0.562287.
Test: 2018-08-09T12:11:26.259686: step 28350, loss 0.550299.
Train: 2018-08-09T12:11:26.369060: step 28351, loss 0.545404.
Train: 2018-08-09T12:11:26.478409: step 28352, loss 0.54487.
Train: 2018-08-09T12:11:26.572138: step 28353, loss 0.562231.
Train: 2018-08-09T12:11:26.681486: step 28354, loss 0.754081.
Train: 2018-08-09T12:11:26.790836: step 28355, loss 0.562914.
Train: 2018-08-09T12:11:26.884565: step 28356, loss 0.581101.
Train: 2018-08-09T12:11:26.996165: step 28357, loss 0.476387.
Train: 2018-08-09T12:11:27.105544: step 28358, loss 0.561332.
Train: 2018-08-09T12:11:27.199275: step 28359, loss 0.563416.
Train: 2018-08-09T12:11:27.308621: step 28360, loss 0.510433.
Test: 2018-08-09T12:11:27.808487: step 28360, loss 0.548372.
Train: 2018-08-09T12:11:27.902201: step 28361, loss 0.579734.
Train: 2018-08-09T12:11:28.011580: step 28362, loss 0.631602.
Train: 2018-08-09T12:11:28.120925: step 28363, loss 0.478506.
Train: 2018-08-09T12:11:28.230279: step 28364, loss 0.646274.
Train: 2018-08-09T12:11:28.339628: step 28365, loss 0.630235.
Train: 2018-08-09T12:11:28.433356: step 28366, loss 0.57958.
Train: 2018-08-09T12:11:28.542706: step 28367, loss 0.595182.
Train: 2018-08-09T12:11:28.652055: step 28368, loss 0.513133.
Train: 2018-08-09T12:11:28.745782: step 28369, loss 0.611365.
Train: 2018-08-09T12:11:28.855132: step 28370, loss 0.628343.
Test: 2018-08-09T12:11:29.357288: step 28370, loss 0.551109.
Train: 2018-08-09T12:11:29.451032: step 28371, loss 0.464893.
Train: 2018-08-09T12:11:29.560351: step 28372, loss 0.497868.
Train: 2018-08-09T12:11:29.669732: step 28373, loss 0.513999.
Train: 2018-08-09T12:11:29.779080: step 28374, loss 0.724874.
Train: 2018-08-09T12:11:29.872809: step 28375, loss 0.546665.
Train: 2018-08-09T12:11:29.982158: step 28376, loss 0.611139.
Train: 2018-08-09T12:11:30.091477: step 28377, loss 0.5467.
Train: 2018-08-09T12:11:30.200856: step 28378, loss 0.578867.
Train: 2018-08-09T12:11:30.294584: step 28379, loss 0.514689.
Train: 2018-08-09T12:11:30.403930: step 28380, loss 0.514732.
Test: 2018-08-09T12:11:30.903786: step 28380, loss 0.550833.
Train: 2018-08-09T12:11:31.014614: step 28381, loss 0.562834.
Train: 2018-08-09T12:11:31.123960: step 28382, loss 0.482613.
Train: 2018-08-09T12:11:31.223993: step 28383, loss 0.514593.
Train: 2018-08-09T12:11:31.333372: step 28384, loss 0.546621.
Train: 2018-08-09T12:11:31.442727: step 28385, loss 0.595046.
Train: 2018-08-09T12:11:31.552071: step 28386, loss 0.627562.
Train: 2018-08-09T12:11:31.661420: step 28387, loss 0.546564.
Train: 2018-08-09T12:11:31.755147: step 28388, loss 0.578921.
Train: 2018-08-09T12:11:31.864497: step 28389, loss 0.595167.
Train: 2018-08-09T12:11:31.973847: step 28390, loss 0.627393.
Test: 2018-08-09T12:11:32.473699: step 28390, loss 0.550003.
Train: 2018-08-09T12:11:32.583078: step 28391, loss 0.449553.
Train: 2018-08-09T12:11:32.676800: step 28392, loss 0.481947.
Train: 2018-08-09T12:11:32.786155: step 28393, loss 0.562709.
Train: 2018-08-09T12:11:32.895504: step 28394, loss 0.660327.
Train: 2018-08-09T12:11:32.999598: step 28395, loss 0.546649.
Train: 2018-08-09T12:11:33.108947: step 28396, loss 0.481552.
Train: 2018-08-09T12:11:33.265130: step 28397, loss 0.497678.
Train: 2018-08-09T12:11:33.358889: step 28398, loss 0.595194.
Train: 2018-08-09T12:11:33.468239: step 28399, loss 0.61158.
Train: 2018-08-09T12:11:33.574250: step 28400, loss 0.61159.
Test: 2018-08-09T12:11:34.074134: step 28400, loss 0.549169.
Train: 2018-08-09T12:11:34.636529: step 28401, loss 0.51362.
Train: 2018-08-09T12:11:34.745879: step 28402, loss 0.546273.
Train: 2018-08-09T12:11:34.855198: step 28403, loss 0.611586.
Train: 2018-08-09T12:11:34.964577: step 28404, loss 0.513564.
Train: 2018-08-09T12:11:35.073926: step 28405, loss 0.595397.
Train: 2018-08-09T12:11:35.167624: step 28406, loss 0.726292.
Train: 2018-08-09T12:11:35.277006: step 28407, loss 0.513635.
Train: 2018-08-09T12:11:35.386353: step 28408, loss 0.627839.
Train: 2018-08-09T12:11:35.495702: step 28409, loss 0.595172.
Train: 2018-08-09T12:11:35.589430: step 28410, loss 0.643824.
Test: 2018-08-09T12:11:36.090981: step 28410, loss 0.54877.
Train: 2018-08-09T12:11:36.200306: step 28411, loss 0.546521.
Train: 2018-08-09T12:11:36.309684: step 28412, loss 0.627319.
Train: 2018-08-09T12:11:36.419034: step 28413, loss 0.578883.
Train: 2018-08-09T12:11:36.528383: step 28414, loss 0.530705.
Train: 2018-08-09T12:11:36.622114: step 28415, loss 0.514812.
Train: 2018-08-09T12:11:36.731460: step 28416, loss 0.498949.
Train: 2018-08-09T12:11:36.840809: step 28417, loss 0.514975.
Train: 2018-08-09T12:11:36.950158: step 28418, loss 0.546909.
Train: 2018-08-09T12:11:37.059508: step 28419, loss 0.610985.
Train: 2018-08-09T12:11:37.168858: step 28420, loss 0.562919.
Test: 2018-08-09T12:11:37.653119: step 28420, loss 0.550836.
Train: 2018-08-09T12:11:37.762467: step 28421, loss 0.530705.
Train: 2018-08-09T12:11:37.871819: step 28422, loss 0.546762.
Train: 2018-08-09T12:11:37.981165: step 28423, loss 0.546713.
Train: 2018-08-09T12:11:38.074863: step 28424, loss 0.643047.
Train: 2018-08-09T12:11:38.184242: step 28425, loss 0.514373.
Train: 2018-08-09T12:11:38.293591: step 28426, loss 0.562556.
Train: 2018-08-09T12:11:38.402941: step 28427, loss 0.562611.
Train: 2018-08-09T12:11:38.512290: step 28428, loss 0.546666.
Train: 2018-08-09T12:11:38.606018: step 28429, loss 0.627384.
Train: 2018-08-09T12:11:38.715367: step 28430, loss 0.529937.
Test: 2018-08-09T12:11:39.216537: step 28430, loss 0.547735.
Train: 2018-08-09T12:11:39.325917: step 28431, loss 0.547337.
Train: 2018-08-09T12:11:39.435265: step 28432, loss 0.613673.
Train: 2018-08-09T12:11:39.528994: step 28433, loss 0.561905.
Train: 2018-08-09T12:11:39.638313: step 28434, loss 0.579312.
Train: 2018-08-09T12:11:39.747692: step 28435, loss 0.497628.
Train: 2018-08-09T12:11:39.857041: step 28436, loss 0.59537.
Train: 2018-08-09T12:11:39.966391: step 28437, loss 0.546312.
Train: 2018-08-09T12:11:40.075739: step 28438, loss 0.57921.
Train: 2018-08-09T12:11:40.169467: step 28439, loss 0.465836.
Train: 2018-08-09T12:11:40.278787: step 28440, loss 0.611597.
Test: 2018-08-09T12:11:40.778670: step 28440, loss 0.551105.
Train: 2018-08-09T12:11:40.888051: step 28441, loss 0.692819.
Train: 2018-08-09T12:11:40.999668: step 28442, loss 0.57905.
Train: 2018-08-09T12:11:41.093393: step 28443, loss 0.530542.
Train: 2018-08-09T12:11:41.202742: step 28444, loss 0.643178.
Train: 2018-08-09T12:11:41.312090: step 28445, loss 0.466677.
Train: 2018-08-09T12:11:41.421440: step 28446, loss 0.594874.
Train: 2018-08-09T12:11:41.530792: step 28447, loss 0.610878.
Train: 2018-08-09T12:11:41.624520: step 28448, loss 0.658786.
Train: 2018-08-09T12:11:41.733867: step 28449, loss 0.515067.
Train: 2018-08-09T12:11:41.843216: step 28450, loss 0.610713.
Test: 2018-08-09T12:11:42.343068: step 28450, loss 0.550469.
Train: 2018-08-09T12:11:42.452417: step 28451, loss 0.515266.
Train: 2018-08-09T12:11:42.561767: step 28452, loss 0.658282.
Train: 2018-08-09T12:11:42.671116: step 28453, loss 0.483747.
Train: 2018-08-09T12:11:42.780495: step 28454, loss 0.57886.
Train: 2018-08-09T12:11:42.889814: step 28455, loss 0.547198.
Train: 2018-08-09T12:11:42.999775: step 28456, loss 0.594686.
Train: 2018-08-09T12:11:43.093532: step 28457, loss 0.499783.
Train: 2018-08-09T12:11:43.202882: step 28458, loss 0.642149.
Train: 2018-08-09T12:11:43.312231: step 28459, loss 0.515616.
Train: 2018-08-09T12:11:43.421581: step 28460, loss 0.563045.
Test: 2018-08-09T12:11:43.921432: step 28460, loss 0.55121.
Train: 2018-08-09T12:11:44.015191: step 28461, loss 0.673774.
Train: 2018-08-09T12:11:44.132991: step 28462, loss 0.531467.
Train: 2018-08-09T12:11:44.242341: step 28463, loss 0.626235.
Train: 2018-08-09T12:11:44.336070: step 28464, loss 0.468485.
Train: 2018-08-09T12:11:44.445389: step 28465, loss 0.563095.
Train: 2018-08-09T12:11:44.554738: step 28466, loss 0.578852.
Train: 2018-08-09T12:11:44.664117: step 28467, loss 0.531512.
Train: 2018-08-09T12:11:44.773466: step 28468, loss 0.499858.
Train: 2018-08-09T12:11:44.882815: step 28469, loss 0.578794.
Train: 2018-08-09T12:11:44.992134: step 28470, loss 0.54711.
Test: 2018-08-09T12:11:45.492018: step 28470, loss 0.549804.
Train: 2018-08-09T12:11:45.585744: step 28471, loss 0.483277.
Train: 2018-08-09T12:11:45.695125: step 28472, loss 0.563257.
Train: 2018-08-09T12:11:45.804473: step 28473, loss 0.595112.
Train: 2018-08-09T12:11:45.913822: step 28474, loss 0.466035.
Train: 2018-08-09T12:11:46.023172: step 28475, loss 0.626982.
Train: 2018-08-09T12:11:46.132490: step 28476, loss 0.5958.
Train: 2018-08-09T12:11:46.241869: step 28477, loss 0.513518.
Train: 2018-08-09T12:11:46.351220: step 28478, loss 0.578212.
Train: 2018-08-09T12:11:46.456429: step 28479, loss 0.577175.
Train: 2018-08-09T12:11:46.565807: step 28480, loss 0.562838.
Test: 2018-08-09T12:11:47.065661: step 28480, loss 0.546606.
Train: 2018-08-09T12:11:47.159418: step 28481, loss 0.494318.
Train: 2018-08-09T12:11:47.268767: step 28482, loss 0.583566.
Train: 2018-08-09T12:11:47.378117: step 28483, loss 0.595054.
Train: 2018-08-09T12:11:47.487436: step 28484, loss 0.493469.
Train: 2018-08-09T12:11:47.596813: step 28485, loss 0.581694.
Train: 2018-08-09T12:11:47.706164: step 28486, loss 0.612064.
Train: 2018-08-09T12:11:47.815513: step 28487, loss 0.514006.
Train: 2018-08-09T12:11:47.914744: step 28488, loss 0.546363.
Train: 2018-08-09T12:11:48.024094: step 28489, loss 0.527505.
Train: 2018-08-09T12:11:48.133443: step 28490, loss 0.544905.
Test: 2018-08-09T12:11:48.633326: step 28490, loss 0.548338.
Train: 2018-08-09T12:11:48.742675: step 28491, loss 0.529604.
Train: 2018-08-09T12:11:48.852054: step 28492, loss 0.546505.
Train: 2018-08-09T12:11:48.961404: step 28493, loss 0.729475.
Train: 2018-08-09T12:11:49.070752: step 28494, loss 0.610585.
Train: 2018-08-09T12:11:49.180071: step 28495, loss 0.545154.
Train: 2018-08-09T12:11:49.273800: step 28496, loss 0.530586.
Train: 2018-08-09T12:11:49.383149: step 28497, loss 0.561735.
Train: 2018-08-09T12:11:49.492528: step 28498, loss 0.612738.
Train: 2018-08-09T12:11:49.601877: step 28499, loss 0.54528.
Train: 2018-08-09T12:11:49.711227: step 28500, loss 0.563828.
Test: 2018-08-09T12:11:50.211080: step 28500, loss 0.548541.
Train: 2018-08-09T12:11:50.804719: step 28501, loss 0.514765.
Train: 2018-08-09T12:11:50.915487: step 28502, loss 0.497404.
Train: 2018-08-09T12:11:51.024836: step 28503, loss 0.563041.
Train: 2018-08-09T12:11:51.134216: step 28504, loss 0.660875.
Train: 2018-08-09T12:11:51.243566: step 28505, loss 0.529376.
Train: 2018-08-09T12:11:51.352914: step 28506, loss 0.54648.
Train: 2018-08-09T12:11:51.446642: step 28507, loss 0.481914.
Train: 2018-08-09T12:11:51.555992: step 28508, loss 0.643656.
Train: 2018-08-09T12:11:51.665343: step 28509, loss 0.482213.
Train: 2018-08-09T12:11:51.774693: step 28510, loss 0.49719.
Test: 2018-08-09T12:11:52.274544: step 28510, loss 0.549221.
Train: 2018-08-09T12:11:52.368271: step 28511, loss 0.57984.
Train: 2018-08-09T12:11:52.477620: step 28512, loss 0.563323.
Train: 2018-08-09T12:11:52.586969: step 28513, loss 0.612402.
Train: 2018-08-09T12:11:52.696319: step 28514, loss 0.611534.
Train: 2018-08-09T12:11:52.805668: step 28515, loss 0.513566.
Train: 2018-08-09T12:11:52.917310: step 28516, loss 0.579424.
Train: 2018-08-09T12:11:53.026660: step 28517, loss 0.449092.
Train: 2018-08-09T12:11:53.136039: step 28518, loss 0.643397.
Train: 2018-08-09T12:11:53.229767: step 28519, loss 0.44886.
Train: 2018-08-09T12:11:53.339116: step 28520, loss 0.611448.
Test: 2018-08-09T12:11:53.838968: step 28520, loss 0.550336.
Train: 2018-08-09T12:11:53.948318: step 28521, loss 0.480129.
Train: 2018-08-09T12:11:54.057666: step 28522, loss 0.513282.
Train: 2018-08-09T12:11:54.167046: step 28523, loss 0.561752.
Train: 2018-08-09T12:11:54.276395: step 28524, loss 0.546161.
Train: 2018-08-09T12:11:54.385744: step 28525, loss 0.546157.
Train: 2018-08-09T12:11:54.495094: step 28526, loss 0.647703.
Train: 2018-08-09T12:11:54.588792: step 28527, loss 0.546147.
Train: 2018-08-09T12:11:54.698142: step 28528, loss 0.596818.
Train: 2018-08-09T12:11:54.807519: step 28529, loss 0.579645.
Train: 2018-08-09T12:11:54.919268: step 28530, loss 0.564639.
Test: 2018-08-09T12:11:55.403530: step 28530, loss 0.545536.
Train: 2018-08-09T12:11:55.544151: step 28531, loss 0.6301.
Train: 2018-08-09T12:11:55.653500: step 28532, loss 0.529141.
Train: 2018-08-09T12:11:55.762849: step 28533, loss 0.628034.
Train: 2018-08-09T12:11:55.872169: step 28534, loss 0.579089.
Train: 2018-08-09T12:11:55.965927: step 28535, loss 0.628041.
Train: 2018-08-09T12:11:56.075275: step 28536, loss 0.710247.
Train: 2018-08-09T12:11:56.184624: step 28537, loss 0.530249.
Train: 2018-08-09T12:11:56.293973: step 28538, loss 0.54641.
Train: 2018-08-09T12:11:56.403323: step 28539, loss 0.530424.
Train: 2018-08-09T12:11:56.512672: step 28540, loss 0.611085.
Test: 2018-08-09T12:11:57.012524: step 28540, loss 0.548969.
Train: 2018-08-09T12:11:57.121874: step 28541, loss 0.627061.
Train: 2018-08-09T12:11:57.215602: step 28542, loss 0.482777.
Train: 2018-08-09T12:11:57.324981: step 28543, loss 0.578865.
Train: 2018-08-09T12:11:57.434333: step 28544, loss 0.499037.
Train: 2018-08-09T12:11:57.543679: step 28545, loss 0.562903.
Train: 2018-08-09T12:11:57.652999: step 28546, loss 0.499098.
Train: 2018-08-09T12:11:57.762378: step 28547, loss 0.514999.
Train: 2018-08-09T12:11:57.871727: step 28548, loss 0.626826.
Train: 2018-08-09T12:11:57.965455: step 28549, loss 0.578896.
Train: 2018-08-09T12:11:58.074774: step 28550, loss 0.482888.
Test: 2018-08-09T12:11:58.574657: step 28550, loss 0.549049.
Train: 2018-08-09T12:11:58.684036: step 28551, loss 0.57889.
Train: 2018-08-09T12:11:58.793385: step 28552, loss 0.643006.
Train: 2018-08-09T12:11:58.902734: step 28553, loss 0.578865.
Train: 2018-08-09T12:11:58.998817: step 28554, loss 0.578861.
Train: 2018-08-09T12:11:59.108167: step 28555, loss 0.434698.
Train: 2018-08-09T12:11:59.217516: step 28556, loss 0.530661.
Train: 2018-08-09T12:11:59.326865: step 28557, loss 0.5789.
Train: 2018-08-09T12:11:59.436216: step 28558, loss 0.594963.
Train: 2018-08-09T12:11:59.545563: step 28559, loss 0.578541.
Train: 2018-08-09T12:11:59.654883: step 28560, loss 0.676556.
Test: 2018-08-09T12:12:00.154766: step 28560, loss 0.548133.
Train: 2018-08-09T12:12:00.264144: step 28561, loss 0.481654.
Train: 2018-08-09T12:12:00.357876: step 28562, loss 0.530715.
Train: 2018-08-09T12:12:00.467221: step 28563, loss 0.578852.
Train: 2018-08-09T12:12:00.576571: step 28564, loss 0.595152.
Train: 2018-08-09T12:12:00.685922: step 28565, loss 0.49759.
Train: 2018-08-09T12:12:00.795269: step 28566, loss 0.465289.
Train: 2018-08-09T12:12:00.907845: step 28567, loss 0.49729.
Train: 2018-08-09T12:12:01.014663: step 28568, loss 0.627933.
Train: 2018-08-09T12:12:01.108391: step 28569, loss 0.613769.
Train: 2018-08-09T12:12:01.217741: step 28570, loss 0.529171.
Test: 2018-08-09T12:12:01.717594: step 28570, loss 0.54944.
Train: 2018-08-09T12:12:01.826942: step 28571, loss 0.613128.
Train: 2018-08-09T12:12:01.936292: step 28572, loss 0.563086.
Train: 2018-08-09T12:12:02.045665: step 28573, loss 0.545826.
Train: 2018-08-09T12:12:02.155019: step 28574, loss 0.447116.
Train: 2018-08-09T12:12:02.264369: step 28575, loss 0.562855.
Train: 2018-08-09T12:12:02.373718: step 28576, loss 0.612259.
Train: 2018-08-09T12:12:02.467447: step 28577, loss 0.613349.
Train: 2018-08-09T12:12:02.576795: step 28578, loss 0.546384.
Train: 2018-08-09T12:12:02.686144: step 28579, loss 0.595571.
Train: 2018-08-09T12:12:02.795494: step 28580, loss 0.611885.
Test: 2018-08-09T12:12:03.295347: step 28580, loss 0.547874.
Train: 2018-08-09T12:12:03.404695: step 28581, loss 0.464421.
Train: 2018-08-09T12:12:03.498423: step 28582, loss 0.513533.
Train: 2018-08-09T12:12:03.607772: step 28583, loss 0.513423.
Train: 2018-08-09T12:12:03.717122: step 28584, loss 0.529713.
Train: 2018-08-09T12:12:03.826501: step 28585, loss 0.595327.
Train: 2018-08-09T12:12:03.938045: step 28586, loss 0.562619.
Train: 2018-08-09T12:12:04.047400: step 28587, loss 0.546044.
Train: 2018-08-09T12:12:04.156749: step 28588, loss 0.430158.
Train: 2018-08-09T12:12:04.266069: step 28589, loss 0.578553.
Train: 2018-08-09T12:12:04.375447: step 28590, loss 0.562065.
Test: 2018-08-09T12:12:04.875319: step 28590, loss 0.547276.
Train: 2018-08-09T12:12:04.969028: step 28591, loss 0.494603.
Train: 2018-08-09T12:12:05.078377: step 28592, loss 0.513285.
Train: 2018-08-09T12:12:05.187445: step 28593, loss 0.61352.
Train: 2018-08-09T12:12:05.296797: step 28594, loss 0.596768.
Train: 2018-08-09T12:12:05.406142: step 28595, loss 0.5268.
Train: 2018-08-09T12:12:05.515486: step 28596, loss 0.613172.
Train: 2018-08-09T12:12:05.609224: step 28597, loss 0.596696.
Train: 2018-08-09T12:12:05.718538: step 28598, loss 0.594263.
Train: 2018-08-09T12:12:05.827918: step 28599, loss 0.613896.
Train: 2018-08-09T12:12:05.938880: step 28600, loss 0.630164.
Test: 2018-08-09T12:12:06.438763: step 28600, loss 0.550411.
Train: 2018-08-09T12:12:07.000492: step 28601, loss 0.494837.
Train: 2018-08-09T12:12:07.109807: step 28602, loss 0.595325.
Train: 2018-08-09T12:12:07.219187: step 28603, loss 0.545741.
Train: 2018-08-09T12:12:07.328535: step 28604, loss 0.680126.
Train: 2018-08-09T12:12:07.428963: step 28605, loss 0.611951.
Train: 2018-08-09T12:12:07.538343: step 28606, loss 0.529321.
Train: 2018-08-09T12:12:07.647692: step 28607, loss 0.578821.
Train: 2018-08-09T12:12:07.757041: step 28608, loss 0.529699.
Train: 2018-08-09T12:12:07.850769: step 28609, loss 0.480741.
Train: 2018-08-09T12:12:07.960088: step 28610, loss 0.578979.
Test: 2018-08-09T12:12:08.459971: step 28610, loss 0.549773.
Train: 2018-08-09T12:12:08.569319: step 28611, loss 0.627836.
Train: 2018-08-09T12:12:08.678669: step 28612, loss 0.562593.
Train: 2018-08-09T12:12:08.772397: step 28613, loss 0.497475.
Train: 2018-08-09T12:12:08.881776: step 28614, loss 0.530116.
Train: 2018-08-09T12:12:08.991126: step 28615, loss 0.562649.
Train: 2018-08-09T12:12:09.100471: step 28616, loss 0.627839.
Train: 2018-08-09T12:12:09.209795: step 28617, loss 0.530145.
Train: 2018-08-09T12:12:09.319173: step 28618, loss 0.562617.
Train: 2018-08-09T12:12:09.428495: step 28619, loss 0.611459.
Train: 2018-08-09T12:12:09.537842: step 28620, loss 0.465205.
Test: 2018-08-09T12:12:10.024403: step 28620, loss 0.549895.
Train: 2018-08-09T12:12:10.133781: step 28621, loss 0.481466.
Train: 2018-08-09T12:12:10.243125: step 28622, loss 0.432385.
Train: 2018-08-09T12:12:10.352450: step 28623, loss 0.660782.
Train: 2018-08-09T12:12:10.461830: step 28624, loss 0.480609.
Train: 2018-08-09T12:12:10.571182: step 28625, loss 0.660995.
Train: 2018-08-09T12:12:10.680528: step 28626, loss 0.628354.
Train: 2018-08-09T12:12:10.774226: step 28627, loss 0.595395.
Train: 2018-08-09T12:12:10.883575: step 28628, loss 0.595169.
Train: 2018-08-09T12:12:10.992924: step 28629, loss 0.628015.
Train: 2018-08-09T12:12:11.102305: step 28630, loss 0.61177.
Test: 2018-08-09T12:12:11.602181: step 28630, loss 0.55039.
Train: 2018-08-09T12:12:11.695883: step 28631, loss 0.481016.
Train: 2018-08-09T12:12:11.805233: step 28632, loss 0.562937.
Train: 2018-08-09T12:12:11.916084: step 28633, loss 0.627964.
Train: 2018-08-09T12:12:12.025434: step 28634, loss 0.497749.
Train: 2018-08-09T12:12:12.119187: step 28635, loss 0.578955.
Train: 2018-08-09T12:12:12.228541: step 28636, loss 0.562678.
Train: 2018-08-09T12:12:12.337861: step 28637, loss 0.497788.
Train: 2018-08-09T12:12:12.447240: step 28638, loss 0.546455.
Train: 2018-08-09T12:12:12.540967: step 28639, loss 0.676284.
Train: 2018-08-09T12:12:12.650286: step 28640, loss 0.546468.
Test: 2018-08-09T12:12:13.150170: step 28640, loss 0.548124.
Train: 2018-08-09T12:12:13.259548: step 28641, loss 0.546516.
Train: 2018-08-09T12:12:13.368868: step 28642, loss 0.627491.
Train: 2018-08-09T12:12:13.462595: step 28643, loss 0.562701.
Train: 2018-08-09T12:12:13.571945: step 28644, loss 0.498106.
Train: 2018-08-09T12:12:13.681295: step 28645, loss 0.5627.
Train: 2018-08-09T12:12:13.775052: step 28646, loss 0.595027.
Train: 2018-08-09T12:12:13.884402: step 28647, loss 0.595045.
Train: 2018-08-09T12:12:13.996084: step 28648, loss 0.627327.
Train: 2018-08-09T12:12:14.105455: step 28649, loss 0.498233.
Train: 2018-08-09T12:12:14.214785: step 28650, loss 0.546724.
Test: 2018-08-09T12:12:14.699041: step 28650, loss 0.54889.
Train: 2018-08-09T12:12:14.808363: step 28651, loss 0.514466.
Train: 2018-08-09T12:12:14.917742: step 28652, loss 0.514333.
Train: 2018-08-09T12:12:15.027091: step 28653, loss 0.611488.
Train: 2018-08-09T12:12:15.120819: step 28654, loss 0.562967.
Train: 2018-08-09T12:12:15.230168: step 28655, loss 0.546543.
Train: 2018-08-09T12:12:15.339519: step 28656, loss 0.530462.
Train: 2018-08-09T12:12:15.448866: step 28657, loss 0.49807.
Train: 2018-08-09T12:12:15.558216: step 28658, loss 0.562703.
Train: 2018-08-09T12:12:15.651943: step 28659, loss 0.627573.
Train: 2018-08-09T12:12:15.776911: step 28660, loss 0.513951.
Test: 2018-08-09T12:12:16.263505: step 28660, loss 0.548035.
Train: 2018-08-09T12:12:16.372853: step 28661, loss 0.660164.
Train: 2018-08-09T12:12:16.482201: step 28662, loss 0.676466.
Train: 2018-08-09T12:12:16.591550: step 28663, loss 0.643792.
Train: 2018-08-09T12:12:16.685278: step 28664, loss 0.465664.
Train: 2018-08-09T12:12:16.794660: step 28665, loss 0.46577.
Train: 2018-08-09T12:12:16.904008: step 28666, loss 0.481882.
Train: 2018-08-09T12:12:17.013327: step 28667, loss 0.530325.
Train: 2018-08-09T12:12:17.107085: step 28668, loss 0.611332.
Train: 2018-08-09T12:12:17.216405: step 28669, loss 0.595147.
Train: 2018-08-09T12:12:17.325787: step 28670, loss 0.643929.
Test: 2018-08-09T12:12:17.825637: step 28670, loss 0.54866.
Train: 2018-08-09T12:12:17.920798: step 28671, loss 0.530103.
Train: 2018-08-09T12:12:18.030148: step 28672, loss 0.562833.
Train: 2018-08-09T12:12:18.139529: step 28673, loss 0.562686.
Train: 2018-08-09T12:12:18.248877: step 28674, loss 0.481377.
Train: 2018-08-09T12:12:18.358195: step 28675, loss 0.546456.
Train: 2018-08-09T12:12:18.451953: step 28676, loss 0.611492.
Train: 2018-08-09T12:12:18.561304: step 28677, loss 0.611558.
Train: 2018-08-09T12:12:18.670653: step 28678, loss 0.497612.
Train: 2018-08-09T12:12:18.779972: step 28679, loss 0.546397.
Train: 2018-08-09T12:12:18.873700: step 28680, loss 0.497479.
Test: 2018-08-09T12:12:19.373610: step 28680, loss 0.5498.
Train: 2018-08-09T12:12:19.482963: step 28681, loss 0.578944.
Train: 2018-08-09T12:12:19.592281: step 28682, loss 0.513606.
Train: 2018-08-09T12:12:19.686039: step 28683, loss 0.644411.
Train: 2018-08-09T12:12:19.795387: step 28684, loss 0.660807.
Train: 2018-08-09T12:12:19.911995: step 28685, loss 0.627997.
Train: 2018-08-09T12:12:20.014768: step 28686, loss 0.513646.
Train: 2018-08-09T12:12:20.124116: step 28687, loss 0.595235.
Train: 2018-08-09T12:12:20.217844: step 28688, loss 0.530065.
Train: 2018-08-09T12:12:20.327224: step 28689, loss 0.546368.
Train: 2018-08-09T12:12:20.436573: step 28690, loss 0.643991.
Test: 2018-08-09T12:12:20.936427: step 28690, loss 0.546828.
Train: 2018-08-09T12:12:21.030184: step 28691, loss 0.562665.
Train: 2018-08-09T12:12:21.139502: step 28692, loss 0.53023.
Train: 2018-08-09T12:12:21.248882: step 28693, loss 0.627538.
Train: 2018-08-09T12:12:21.358231: step 28694, loss 0.659838.
Train: 2018-08-09T12:12:21.451959: step 28695, loss 0.562742.
Train: 2018-08-09T12:12:21.561308: step 28696, loss 0.611091.
Train: 2018-08-09T12:12:21.670628: step 28697, loss 0.610994.
Train: 2018-08-09T12:12:21.780007: step 28698, loss 0.498805.
Train: 2018-08-09T12:12:21.876353: step 28699, loss 0.530915.
Train: 2018-08-09T12:12:21.985703: step 28700, loss 0.610788.
Test: 2018-08-09T12:12:22.485585: step 28700, loss 0.548593.
Train: 2018-08-09T12:12:23.079195: step 28701, loss 0.610735.
Train: 2018-08-09T12:12:23.188574: step 28702, loss 0.594762.
Train: 2018-08-09T12:12:23.297893: step 28703, loss 0.515391.
Train: 2018-08-09T12:12:23.391622: step 28704, loss 0.610551.
Train: 2018-08-09T12:12:23.501000: step 28705, loss 0.483953.
Train: 2018-08-09T12:12:23.610351: step 28706, loss 0.578861.
Train: 2018-08-09T12:12:23.719699: step 28707, loss 0.468224.
Train: 2018-08-09T12:12:23.813427: step 28708, loss 0.51557.
Train: 2018-08-09T12:12:23.922747: step 28709, loss 0.547157.
Train: 2018-08-09T12:12:24.032095: step 28710, loss 0.547108.
Test: 2018-08-09T12:12:24.531978: step 28710, loss 0.54984.
Train: 2018-08-09T12:12:24.641327: step 28711, loss 0.499293.
Train: 2018-08-09T12:12:24.750676: step 28712, loss 0.530967.
Train: 2018-08-09T12:12:24.860024: step 28713, loss 0.594923.
Train: 2018-08-09T12:12:24.956055: step 28714, loss 0.546775.
Train: 2018-08-09T12:12:25.065435: step 28715, loss 0.514458.
Train: 2018-08-09T12:12:25.159133: step 28716, loss 0.546649.
Train: 2018-08-09T12:12:25.268513: step 28717, loss 0.562554.
Train: 2018-08-09T12:12:25.377864: step 28718, loss 0.562317.
Train: 2018-08-09T12:12:25.487181: step 28719, loss 0.627038.
Train: 2018-08-09T12:12:25.589287: step 28720, loss 0.644338.
Test: 2018-08-09T12:12:26.089195: step 28720, loss 0.548294.
Train: 2018-08-09T12:12:26.182928: step 28721, loss 0.528953.
Train: 2018-08-09T12:12:26.292277: step 28722, loss 0.663523.
Train: 2018-08-09T12:12:26.401626: step 28723, loss 0.595357.
Train: 2018-08-09T12:12:26.495354: step 28724, loss 0.762667.
Train: 2018-08-09T12:12:26.604673: step 28725, loss 0.480595.
Train: 2018-08-09T12:12:26.714053: step 28726, loss 0.514187.
Train: 2018-08-09T12:12:26.823371: step 28727, loss 0.514485.
Train: 2018-08-09T12:12:26.917130: step 28728, loss 0.546128.
Train: 2018-08-09T12:12:27.026449: step 28729, loss 0.546583.
Train: 2018-08-09T12:12:27.135828: step 28730, loss 0.611106.
Test: 2018-08-09T12:12:27.635711: step 28730, loss 0.548315.
Train: 2018-08-09T12:12:27.729440: step 28731, loss 0.578999.
Train: 2018-08-09T12:12:27.838793: step 28732, loss 0.562758.
Train: 2018-08-09T12:12:27.948107: step 28733, loss 0.59485.
Train: 2018-08-09T12:12:28.057485: step 28734, loss 0.546767.
Train: 2018-08-09T12:12:28.151217: step 28735, loss 0.530828.
Train: 2018-08-09T12:12:28.260564: step 28736, loss 0.64289.
Train: 2018-08-09T12:12:28.369883: step 28737, loss 0.514893.
Train: 2018-08-09T12:12:28.463611: step 28738, loss 0.610829.
Train: 2018-08-09T12:12:28.572989: step 28739, loss 0.562912.
Train: 2018-08-09T12:12:28.682333: step 28740, loss 0.515005.
Test: 2018-08-09T12:12:29.167997: step 28740, loss 0.549751.
Train: 2018-08-09T12:12:29.277375: step 28741, loss 0.530972.
Train: 2018-08-09T12:12:29.386693: step 28742, loss 0.530947.
Train: 2018-08-09T12:12:29.480452: step 28743, loss 0.498883.
Train: 2018-08-09T12:12:29.589770: step 28744, loss 0.578918.
Train: 2018-08-09T12:12:29.699151: step 28745, loss 0.514627.
Train: 2018-08-09T12:12:29.792881: step 28746, loss 0.627167.
Train: 2018-08-09T12:12:29.902228: step 28747, loss 0.579057.
Train: 2018-08-09T12:12:30.011545: step 28748, loss 0.562295.
Train: 2018-08-09T12:12:30.120926: step 28749, loss 0.496944.
Train: 2018-08-09T12:12:30.214624: step 28750, loss 0.562393.
Test: 2018-08-09T12:12:30.714506: step 28750, loss 0.548907.
Train: 2018-08-09T12:12:30.823885: step 28751, loss 0.496878.
Train: 2018-08-09T12:12:30.919861: step 28752, loss 0.545097.
Train: 2018-08-09T12:12:31.029242: step 28753, loss 0.532218.
Train: 2018-08-09T12:12:31.138560: step 28754, loss 0.533377.
Train: 2018-08-09T12:12:31.232288: step 28755, loss 0.652553.
Train: 2018-08-09T12:12:31.341638: step 28756, loss 0.494787.
Train: 2018-08-09T12:12:31.450988: step 28757, loss 0.478115.
Train: 2018-08-09T12:12:31.560337: step 28758, loss 0.580412.
Train: 2018-08-09T12:12:31.654095: step 28759, loss 0.594373.
Train: 2018-08-09T12:12:31.763447: step 28760, loss 0.580825.
Test: 2018-08-09T12:12:32.263321: step 28760, loss 0.549039.
Train: 2018-08-09T12:12:32.357056: step 28761, loss 0.561285.
Train: 2018-08-09T12:12:32.466403: step 28762, loss 0.644017.
Train: 2018-08-09T12:12:32.575752: step 28763, loss 0.461938.
Train: 2018-08-09T12:12:32.685071: step 28764, loss 0.650242.
Train: 2018-08-09T12:12:32.778830: step 28765, loss 0.543677.
Train: 2018-08-09T12:12:32.888149: step 28766, loss 0.547936.
Train: 2018-08-09T12:12:32.999859: step 28767, loss 0.718793.
Train: 2018-08-09T12:12:33.093587: step 28768, loss 0.425979.
Train: 2018-08-09T12:12:33.202937: step 28769, loss 0.62612.
Train: 2018-08-09T12:12:33.312255: step 28770, loss 0.595077.
Test: 2018-08-09T12:12:33.796518: step 28770, loss 0.547349.
Train: 2018-08-09T12:12:33.905897: step 28771, loss 0.597473.
Train: 2018-08-09T12:12:34.015217: step 28772, loss 0.579699.
Train: 2018-08-09T12:12:34.108942: step 28773, loss 0.611937.
Train: 2018-08-09T12:12:34.218322: step 28774, loss 0.578769.
Train: 2018-08-09T12:12:34.327672: step 28775, loss 0.578305.
Train: 2018-08-09T12:12:34.421400: step 28776, loss 0.498349.
Train: 2018-08-09T12:12:34.530750: step 28777, loss 0.62727.
Train: 2018-08-09T12:12:34.640069: step 28778, loss 0.626914.
Train: 2018-08-09T12:12:34.749446: step 28779, loss 0.546989.
Train: 2018-08-09T12:12:34.843144: step 28780, loss 0.562268.
Test: 2018-08-09T12:12:35.343862: step 28780, loss 0.550436.
Train: 2018-08-09T12:12:35.453240: step 28781, loss 0.65854.
Train: 2018-08-09T12:12:35.562589: step 28782, loss 0.499805.
Train: 2018-08-09T12:12:35.671958: step 28783, loss 0.484233.
Train: 2018-08-09T12:12:35.765667: step 28784, loss 0.562875.
Train: 2018-08-09T12:12:35.875016: step 28785, loss 0.562994.
Train: 2018-08-09T12:12:35.968743: step 28786, loss 0.547237.
Train: 2018-08-09T12:12:36.078093: step 28787, loss 0.531811.
Train: 2018-08-09T12:12:36.187443: step 28788, loss 0.531454.
Train: 2018-08-09T12:12:36.281139: step 28789, loss 0.547287.
Train: 2018-08-09T12:12:36.390522: step 28790, loss 0.610592.
Test: 2018-08-09T12:12:36.890371: step 28790, loss 0.549284.
Train: 2018-08-09T12:12:36.999720: step 28791, loss 0.562879.
Train: 2018-08-09T12:12:37.093449: step 28792, loss 0.578924.
Train: 2018-08-09T12:12:37.202798: step 28793, loss 0.515253.
Train: 2018-08-09T12:12:37.312177: step 28794, loss 0.467538.
Train: 2018-08-09T12:12:37.405874: step 28795, loss 0.626617.
Train: 2018-08-09T12:12:37.515254: step 28796, loss 0.610858.
Train: 2018-08-09T12:12:37.624605: step 28797, loss 0.530749.
Train: 2018-08-09T12:12:37.733923: step 28798, loss 0.546668.
Train: 2018-08-09T12:12:37.827680: step 28799, loss 0.514378.
Train: 2018-08-09T12:12:37.936112: step 28800, loss 0.546586.
Test: 2018-08-09T12:12:38.435994: step 28800, loss 0.54882.
Train: 2018-08-09T12:12:39.015451: step 28801, loss 0.578958.
Train: 2018-08-09T12:12:39.114478: step 28802, loss 0.594717.
Train: 2018-08-09T12:12:39.223828: step 28803, loss 0.530285.
Train: 2018-08-09T12:12:39.317525: step 28804, loss 0.643489.
Train: 2018-08-09T12:12:39.426904: step 28805, loss 0.595239.
Train: 2018-08-09T12:12:39.536224: step 28806, loss 0.660302.
Train: 2018-08-09T12:12:39.629952: step 28807, loss 0.563337.
Train: 2018-08-09T12:12:39.739331: step 28808, loss 0.57815.
Train: 2018-08-09T12:12:39.848681: step 28809, loss 0.530336.
Train: 2018-08-09T12:12:39.958030: step 28810, loss 0.547568.
Test: 2018-08-09T12:12:40.442260: step 28810, loss 0.549449.
Train: 2018-08-09T12:12:40.551639: step 28811, loss 0.659208.
Train: 2018-08-09T12:12:40.660958: step 28812, loss 0.499012.
Train: 2018-08-09T12:12:40.770338: step 28813, loss 0.514288.
Train: 2018-08-09T12:12:40.864066: step 28814, loss 0.610858.
Train: 2018-08-09T12:12:40.973385: step 28815, loss 0.466129.
Train: 2018-08-09T12:12:41.082765: step 28816, loss 0.546745.
Train: 2018-08-09T12:12:41.176492: step 28817, loss 0.530982.
Train: 2018-08-09T12:12:41.285843: step 28818, loss 0.628359.
Train: 2018-08-09T12:12:41.388885: step 28819, loss 0.578872.
Train: 2018-08-09T12:12:41.498235: step 28820, loss 0.466225.
Test: 2018-08-09T12:12:41.989783: step 28820, loss 0.550638.
Train: 2018-08-09T12:12:42.099126: step 28821, loss 0.514686.
Train: 2018-08-09T12:12:42.208481: step 28822, loss 0.610969.
Train: 2018-08-09T12:12:42.302208: step 28823, loss 0.546549.
Train: 2018-08-09T12:12:42.411557: step 28824, loss 0.464854.
Train: 2018-08-09T12:12:42.520901: step 28825, loss 0.611293.
Train: 2018-08-09T12:12:42.630227: step 28826, loss 0.578711.
Train: 2018-08-09T12:12:42.723984: step 28827, loss 0.594683.
Train: 2018-08-09T12:12:42.833334: step 28828, loss 0.611768.
Train: 2018-08-09T12:12:42.942682: step 28829, loss 0.578288.
Train: 2018-08-09T12:12:43.052001: step 28830, loss 0.529218.
Test: 2018-08-09T12:12:43.551884: step 28830, loss 0.548445.
Train: 2018-08-09T12:12:43.661257: step 28831, loss 0.561995.
Train: 2018-08-09T12:12:43.754992: step 28832, loss 0.579329.
Train: 2018-08-09T12:12:43.864334: step 28833, loss 0.529799.
Train: 2018-08-09T12:12:43.973690: step 28834, loss 0.529089.
Train: 2018-08-09T12:12:44.083040: step 28835, loss 0.547352.
Train: 2018-08-09T12:12:44.192357: step 28836, loss 0.4456.
Train: 2018-08-09T12:12:44.286116: step 28837, loss 0.528147.
Train: 2018-08-09T12:12:44.395465: step 28838, loss 0.629044.
Train: 2018-08-09T12:12:44.504814: step 28839, loss 0.614091.
Train: 2018-08-09T12:12:44.614134: step 28840, loss 0.494404.
Test: 2018-08-09T12:12:45.100672: step 28840, loss 0.545775.
Train: 2018-08-09T12:12:45.210050: step 28841, loss 0.444456.
Train: 2018-08-09T12:12:45.319399: step 28842, loss 0.5123.
Train: 2018-08-09T12:12:45.428750: step 28843, loss 0.598694.
Train: 2018-08-09T12:12:45.522446: step 28844, loss 0.598125.
Train: 2018-08-09T12:12:45.631827: step 28845, loss 0.579111.
Train: 2018-08-09T12:12:45.741176: step 28846, loss 0.474014.
Train: 2018-08-09T12:12:45.850495: step 28847, loss 0.47408.
Train: 2018-08-09T12:12:45.959874: step 28848, loss 0.580105.
Train: 2018-08-09T12:12:46.053572: step 28849, loss 0.527783.
Train: 2018-08-09T12:12:46.162921: step 28850, loss 0.489505.
Test: 2018-08-09T12:12:46.662827: step 28850, loss 0.548605.
Train: 2018-08-09T12:12:46.772152: step 28851, loss 0.495749.
Train: 2018-08-09T12:12:46.865880: step 28852, loss 0.500309.
Train: 2018-08-09T12:12:46.977520: step 28853, loss 0.59936.
Train: 2018-08-09T12:12:47.086898: step 28854, loss 0.610374.
Train: 2018-08-09T12:12:47.196248: step 28855, loss 0.539022.
Train: 2018-08-09T12:12:47.289976: step 28856, loss 0.598616.
Train: 2018-08-09T12:12:47.399296: step 28857, loss 0.595022.
Train: 2018-08-09T12:12:47.508645: step 28858, loss 0.545333.
Train: 2018-08-09T12:12:47.617995: step 28859, loss 0.544244.
Train: 2018-08-09T12:12:47.711751: step 28860, loss 0.494201.
Test: 2018-08-09T12:12:48.211629: step 28860, loss 0.551439.
Train: 2018-08-09T12:12:48.320983: step 28861, loss 0.685358.
Train: 2018-08-09T12:12:48.430333: step 28862, loss 0.595606.
Train: 2018-08-09T12:12:48.524061: step 28863, loss 0.530198.
Train: 2018-08-09T12:12:48.633413: step 28864, loss 0.528702.
Train: 2018-08-09T12:12:48.742728: step 28865, loss 0.478811.
Train: 2018-08-09T12:12:48.852079: step 28866, loss 0.612014.
Train: 2018-08-09T12:12:48.948228: step 28867, loss 0.645858.
Train: 2018-08-09T12:12:49.057578: step 28868, loss 0.578995.
Train: 2018-08-09T12:12:49.166957: step 28869, loss 0.595341.
Train: 2018-08-09T12:12:49.276306: step 28870, loss 0.578651.
Test: 2018-08-09T12:12:49.776159: step 28870, loss 0.547209.
Train: 2018-08-09T12:12:49.885507: step 28871, loss 0.513392.
Train: 2018-08-09T12:12:49.979268: step 28872, loss 0.579108.
Train: 2018-08-09T12:12:50.088614: step 28873, loss 0.57874.
Train: 2018-08-09T12:12:50.197964: step 28874, loss 0.546323.
Train: 2018-08-09T12:12:50.307283: step 28875, loss 0.546336.
Train: 2018-08-09T12:12:50.401040: step 28876, loss 0.611249.
Train: 2018-08-09T12:12:50.510359: step 28877, loss 0.660055.
Train: 2018-08-09T12:12:50.619742: step 28878, loss 0.676129.
Train: 2018-08-09T12:12:50.729089: step 28879, loss 0.530498.
Train: 2018-08-09T12:12:50.822787: step 28880, loss 0.482466.
Test: 2018-08-09T12:12:51.322669: step 28880, loss 0.549568.
Train: 2018-08-09T12:12:51.432047: step 28881, loss 0.498538.
Train: 2018-08-09T12:12:51.541397: step 28882, loss 0.49854.
Train: 2018-08-09T12:12:51.650746: step 28883, loss 0.562778.
Train: 2018-08-09T12:12:51.744444: step 28884, loss 0.530736.
Train: 2018-08-09T12:12:51.853826: step 28885, loss 0.546681.
Train: 2018-08-09T12:12:51.963173: step 28886, loss 0.497977.
Train: 2018-08-09T12:12:52.072521: step 28887, loss 0.530123.
Train: 2018-08-09T12:12:52.181874: step 28888, loss 0.562059.
Train: 2018-08-09T12:12:52.275570: step 28889, loss 0.531522.
Train: 2018-08-09T12:12:52.384918: step 28890, loss 0.495574.
Test: 2018-08-09T12:12:52.884826: step 28890, loss 0.547909.
Train: 2018-08-09T12:12:52.979893: step 28891, loss 0.479905.
Train: 2018-08-09T12:12:53.089245: step 28892, loss 0.476482.
Train: 2018-08-09T12:12:53.198592: step 28893, loss 0.666631.
Train: 2018-08-09T12:12:53.307941: step 28894, loss 0.705352.
Train: 2018-08-09T12:12:53.417291: step 28895, loss 0.511842.
Train: 2018-08-09T12:12:53.511018: step 28896, loss 0.507179.
Train: 2018-08-09T12:12:53.620368: step 28897, loss 0.559254.
Train: 2018-08-09T12:12:53.729717: step 28898, loss 0.617671.
Train: 2018-08-09T12:12:53.839068: step 28899, loss 0.579738.
Train: 2018-08-09T12:12:53.932795: step 28900, loss 0.543738.
Test: 2018-08-09T12:12:54.448269: step 28900, loss 0.549053.
Train: 2018-08-09T12:12:55.026287: step 28901, loss 0.580068.
Train: 2018-08-09T12:12:55.135636: step 28902, loss 0.562893.
Train: 2018-08-09T12:12:55.244985: step 28903, loss 0.512236.
Train: 2018-08-09T12:12:55.354335: step 28904, loss 0.427693.
Train: 2018-08-09T12:12:55.448061: step 28905, loss 0.612593.
Train: 2018-08-09T12:12:55.557411: step 28906, loss 0.562214.
Train: 2018-08-09T12:12:55.666731: step 28907, loss 0.562782.
Train: 2018-08-09T12:12:55.776110: step 28908, loss 0.512335.
Train: 2018-08-09T12:12:55.884124: step 28909, loss 0.611831.
Train: 2018-08-09T12:12:55.993443: step 28910, loss 0.64628.
Test: 2018-08-09T12:12:56.477705: step 28910, loss 0.550399.
Train: 2018-08-09T12:12:56.587054: step 28911, loss 0.56319.
Train: 2018-08-09T12:12:56.696432: step 28912, loss 0.629437.
Train: 2018-08-09T12:12:56.805752: step 28913, loss 0.628504.
Train: 2018-08-09T12:12:56.899512: step 28914, loss 0.479613.
Train: 2018-08-09T12:12:57.011323: step 28915, loss 0.595681.
Train: 2018-08-09T12:12:57.120673: step 28916, loss 0.530119.
Train: 2018-08-09T12:12:57.230024: step 28917, loss 0.513009.
Train: 2018-08-09T12:12:57.339372: step 28918, loss 0.446894.
Train: 2018-08-09T12:12:57.433134: step 28919, loss 0.612207.
Train: 2018-08-09T12:12:57.542448: step 28920, loss 0.578878.
Test: 2018-08-09T12:12:58.042332: step 28920, loss 0.546403.
Train: 2018-08-09T12:12:58.151681: step 28921, loss 0.595322.
Train: 2018-08-09T12:12:58.261031: step 28922, loss 0.47987.
Train: 2018-08-09T12:12:58.354787: step 28923, loss 0.496107.
Train: 2018-08-09T12:12:58.464137: step 28924, loss 0.578974.
Train: 2018-08-09T12:12:58.573486: step 28925, loss 0.679198.
Train: 2018-08-09T12:12:58.682840: step 28926, loss 0.546503.
Train: 2018-08-09T12:12:58.776563: step 28927, loss 0.595527.
Train: 2018-08-09T12:12:58.885882: step 28928, loss 0.495707.
Train: 2018-08-09T12:12:58.995261: step 28929, loss 0.512763.
Train: 2018-08-09T12:12:59.104612: step 28930, loss 0.479457.
Test: 2018-08-09T12:12:59.604463: step 28930, loss 0.54675.
Train: 2018-08-09T12:12:59.713842: step 28931, loss 0.546253.
Train: 2018-08-09T12:12:59.807574: step 28932, loss 0.595711.
Train: 2018-08-09T12:12:59.916919: step 28933, loss 0.59626.
Train: 2018-08-09T12:13:00.026240: step 28934, loss 0.427196.
Train: 2018-08-09T12:13:00.135589: step 28935, loss 0.545227.
Train: 2018-08-09T12:13:00.244967: step 28936, loss 0.424718.
Train: 2018-08-09T12:13:00.354319: step 28937, loss 0.543575.
Train: 2018-08-09T12:13:00.463666: step 28938, loss 0.530187.
Train: 2018-08-09T12:13:00.572985: step 28939, loss 0.485205.
Train: 2018-08-09T12:13:00.682353: step 28940, loss 0.425634.
Test: 2018-08-09T12:13:01.168025: step 28940, loss 0.548173.
Train: 2018-08-09T12:13:01.277373: step 28941, loss 0.520342.
Train: 2018-08-09T12:13:01.386724: step 28942, loss 0.589435.
Train: 2018-08-09T12:13:01.496106: step 28943, loss 0.568417.
Train: 2018-08-09T12:13:01.605452: step 28944, loss 0.642787.
Train: 2018-08-09T12:13:01.714801: step 28945, loss 0.431975.
Train: 2018-08-09T12:13:01.824144: step 28946, loss 0.662776.
Train: 2018-08-09T12:13:01.917878: step 28947, loss 0.656439.
Train: 2018-08-09T12:13:02.027227: step 28948, loss 0.596792.
Train: 2018-08-09T12:13:02.136547: step 28949, loss 0.579688.
Train: 2018-08-09T12:13:02.245930: step 28950, loss 0.510529.
Test: 2018-08-09T12:13:02.745782: step 28950, loss 0.547716.
Train: 2018-08-09T12:13:02.855128: step 28951, loss 0.494408.
Train: 2018-08-09T12:13:02.950232: step 28952, loss 0.613458.
Train: 2018-08-09T12:13:03.059582: step 28953, loss 0.493515.
Train: 2018-08-09T12:13:03.168932: step 28954, loss 0.564002.
Train: 2018-08-09T12:13:03.278280: step 28955, loss 0.475598.
Train: 2018-08-09T12:13:03.387600: step 28956, loss 0.491368.
Train: 2018-08-09T12:13:03.496978: step 28957, loss 0.630047.
Train: 2018-08-09T12:13:03.590707: step 28958, loss 0.455517.
Train: 2018-08-09T12:13:03.700056: step 28959, loss 0.58075.
Train: 2018-08-09T12:13:03.809406: step 28960, loss 0.510244.
Test: 2018-08-09T12:13:04.309285: step 28960, loss 0.548496.
Train: 2018-08-09T12:13:04.418607: step 28961, loss 0.491978.
Train: 2018-08-09T12:13:04.527985: step 28962, loss 0.565352.
Train: 2018-08-09T12:13:04.621683: step 28963, loss 0.547566.
Train: 2018-08-09T12:13:04.731063: step 28964, loss 0.544251.
Train: 2018-08-09T12:13:04.840412: step 28965, loss 0.562037.
Train: 2018-08-09T12:13:04.951156: step 28966, loss 0.510713.
Train: 2018-08-09T12:13:05.060500: step 28967, loss 0.600435.
Train: 2018-08-09T12:13:05.169855: step 28968, loss 0.421537.
Train: 2018-08-09T12:13:05.279203: step 28969, loss 0.545335.
Train: 2018-08-09T12:13:05.372901: step 28970, loss 0.543718.
Test: 2018-08-09T12:13:05.872783: step 28970, loss 0.54982.
Train: 2018-08-09T12:13:05.982163: step 28971, loss 0.559859.
Train: 2018-08-09T12:13:06.091481: step 28972, loss 0.585818.
Train: 2018-08-09T12:13:06.200862: step 28973, loss 0.671498.
Train: 2018-08-09T12:13:06.310211: step 28974, loss 0.615698.
Train: 2018-08-09T12:13:06.419559: step 28975, loss 0.526418.
Train: 2018-08-09T12:13:06.513287: step 28976, loss 0.511655.
Train: 2018-08-09T12:13:06.622637: step 28977, loss 0.561943.
Train: 2018-08-09T12:13:06.731987: step 28978, loss 0.598273.
Train: 2018-08-09T12:13:06.841338: step 28979, loss 0.682822.
Train: 2018-08-09T12:13:06.952164: step 28980, loss 0.545493.
Test: 2018-08-09T12:13:07.452016: step 28980, loss 0.549337.
Train: 2018-08-09T12:13:07.545775: step 28981, loss 0.579022.
Train: 2018-08-09T12:13:07.655095: step 28982, loss 0.612249.
Train: 2018-08-09T12:13:07.764443: step 28983, loss 0.661164.
Train: 2018-08-09T12:13:07.873824: step 28984, loss 0.545963.
Train: 2018-08-09T12:13:07.983172: step 28985, loss 0.676802.
Train: 2018-08-09T12:13:08.092490: step 28986, loss 0.465725.
Train: 2018-08-09T12:13:08.201870: step 28987, loss 0.546546.
Train: 2018-08-09T12:13:08.295598: step 28988, loss 0.514616.
Train: 2018-08-09T12:13:08.404916: step 28989, loss 0.578915.
Train: 2018-08-09T12:13:08.514267: step 28990, loss 0.578802.
Test: 2018-08-09T12:13:09.010864: step 28990, loss 0.550243.
Train: 2018-08-09T12:13:09.120213: step 28991, loss 0.65906.
Train: 2018-08-09T12:13:09.229592: step 28992, loss 0.51494.
Train: 2018-08-09T12:13:09.338941: step 28993, loss 0.562905.
Train: 2018-08-09T12:13:09.448260: step 28994, loss 0.594761.
Train: 2018-08-09T12:13:09.557640: step 28995, loss 0.531133.
Train: 2018-08-09T12:13:09.666989: step 28996, loss 0.531173.
Train: 2018-08-09T12:13:09.776338: step 28997, loss 0.578866.
Train: 2018-08-09T12:13:09.885688: step 28998, loss 0.626542.
Train: 2018-08-09T12:13:09.995043: step 28999, loss 0.56299.
Train: 2018-08-09T12:13:10.104385: step 29000, loss 0.642226.
Test: 2018-08-09T12:13:10.604263: step 29000, loss 0.550031.
Train: 2018-08-09T12:13:11.213470: step 29001, loss 0.531428.
Train: 2018-08-09T12:13:11.307227: step 29002, loss 0.594649.
Train: 2018-08-09T12:13:11.416576: step 29003, loss 0.578884.
Train: 2018-08-09T12:13:11.525926: step 29004, loss 0.515881.
Train: 2018-08-09T12:13:11.635275: step 29005, loss 0.515927.
Train: 2018-08-09T12:13:11.744595: step 29006, loss 0.531615.
Train: 2018-08-09T12:13:11.853973: step 29007, loss 0.515774.
Train: 2018-08-09T12:13:11.965634: step 29008, loss 0.610323.
Train: 2018-08-09T12:13:12.059409: step 29009, loss 0.673747.
Train: 2018-08-09T12:13:12.168742: step 29010, loss 0.578845.
Test: 2018-08-09T12:13:12.668593: step 29010, loss 0.549345.
Train: 2018-08-09T12:13:12.777973: step 29011, loss 0.531061.
Train: 2018-08-09T12:13:12.887323: step 29012, loss 0.595072.
Train: 2018-08-09T12:13:12.990047: step 29013, loss 0.499001.
Train: 2018-08-09T12:13:13.099397: step 29014, loss 0.579306.
Train: 2018-08-09T12:13:13.208778: step 29015, loss 0.610688.
Train: 2018-08-09T12:13:13.318125: step 29016, loss 0.515305.
Train: 2018-08-09T12:13:13.427445: step 29017, loss 0.498522.
Train: 2018-08-09T12:13:13.521173: step 29018, loss 0.594624.
Train: 2018-08-09T12:13:13.630552: step 29019, loss 0.496656.
Train: 2018-08-09T12:13:13.739901: step 29020, loss 0.644533.
Test: 2018-08-09T12:13:14.247086: step 29020, loss 0.548327.
Train: 2018-08-09T12:13:14.340783: step 29021, loss 0.594001.
Train: 2018-08-09T12:13:14.450132: step 29022, loss 0.568398.
Train: 2018-08-09T12:13:14.559482: step 29023, loss 0.463523.
Train: 2018-08-09T12:13:14.668832: step 29024, loss 0.598857.
Train: 2018-08-09T12:13:14.778211: step 29025, loss 0.562986.
Train: 2018-08-09T12:13:14.887530: step 29026, loss 0.478104.
Train: 2018-08-09T12:13:14.996909: step 29027, loss 0.480293.
Train: 2018-08-09T12:13:15.106253: step 29028, loss 0.677863.
Train: 2018-08-09T12:13:15.199988: step 29029, loss 0.598567.
Train: 2018-08-09T12:13:15.311381: step 29030, loss 0.514023.
Test: 2018-08-09T12:13:15.811234: step 29030, loss 0.548424.
Train: 2018-08-09T12:13:15.920582: step 29031, loss 0.661105.
Train: 2018-08-09T12:13:16.029962: step 29032, loss 0.562356.
Train: 2018-08-09T12:13:16.139312: step 29033, loss 0.595179.
Train: 2018-08-09T12:13:16.233010: step 29034, loss 0.56325.
Train: 2018-08-09T12:13:16.342388: step 29035, loss 0.513956.
Train: 2018-08-09T12:13:16.451707: step 29036, loss 0.563817.
Train: 2018-08-09T12:13:16.561087: step 29037, loss 0.578282.
Train: 2018-08-09T12:13:16.670436: step 29038, loss 0.595403.
Train: 2018-08-09T12:13:16.779756: step 29039, loss 0.498694.
Train: 2018-08-09T12:13:16.889105: step 29040, loss 0.498141.
Test: 2018-08-09T12:13:17.387639: step 29040, loss 0.548932.
Train: 2018-08-09T12:13:17.497018: step 29041, loss 0.594447.
Train: 2018-08-09T12:13:17.606367: step 29042, loss 0.530358.
Train: 2018-08-09T12:13:17.700066: step 29043, loss 0.579612.
Train: 2018-08-09T12:13:17.809443: step 29044, loss 0.562353.
Train: 2018-08-09T12:13:17.918793: step 29045, loss 0.561842.
Train: 2018-08-09T12:13:18.028145: step 29046, loss 0.628321.
Train: 2018-08-09T12:13:18.137491: step 29047, loss 0.545408.
Train: 2018-08-09T12:13:18.246810: step 29048, loss 0.578627.
Train: 2018-08-09T12:13:18.356159: step 29049, loss 0.562435.
Train: 2018-08-09T12:13:18.449918: step 29050, loss 0.658697.
Test: 2018-08-09T12:13:18.949798: step 29050, loss 0.549092.
Train: 2018-08-09T12:13:19.059119: step 29051, loss 0.496227.
Train: 2018-08-09T12:13:19.168469: step 29052, loss 0.564803.
Train: 2018-08-09T12:13:19.277850: step 29053, loss 0.57954.
Train: 2018-08-09T12:13:19.371577: step 29054, loss 0.643212.
Train: 2018-08-09T12:13:19.480894: step 29055, loss 0.527028.
Train: 2018-08-09T12:13:19.590274: step 29056, loss 0.54567.
Train: 2018-08-09T12:13:19.699624: step 29057, loss 0.57821.
Train: 2018-08-09T12:13:19.808942: step 29058, loss 0.609927.
Train: 2018-08-09T12:13:19.919699: step 29059, loss 0.480739.
Train: 2018-08-09T12:13:20.029081: step 29060, loss 0.609871.
Test: 2018-08-09T12:13:20.513310: step 29060, loss 0.549158.
Train: 2018-08-09T12:13:20.622683: step 29061, loss 0.595238.
Train: 2018-08-09T12:13:20.732037: step 29062, loss 0.49233.
Train: 2018-08-09T12:13:20.841388: step 29063, loss 0.541105.
Train: 2018-08-09T12:13:20.950736: step 29064, loss 0.512075.
Train: 2018-08-09T12:13:21.060086: step 29065, loss 0.532143.
Train: 2018-08-09T12:13:21.169405: step 29066, loss 0.600353.
Train: 2018-08-09T12:13:21.278754: step 29067, loss 0.60254.
Train: 2018-08-09T12:13:21.372512: step 29068, loss 0.664645.
Train: 2018-08-09T12:13:21.481831: step 29069, loss 0.677763.
Train: 2018-08-09T12:13:21.591210: step 29070, loss 0.561515.
Test: 2018-08-09T12:13:22.093378: step 29070, loss 0.550828.
Train: 2018-08-09T12:13:22.202758: step 29071, loss 0.581318.
Train: 2018-08-09T12:13:22.312075: step 29072, loss 0.532465.
Train: 2018-08-09T12:13:22.421454: step 29073, loss 0.564377.
Train: 2018-08-09T12:13:22.530774: step 29074, loss 0.4833.
Train: 2018-08-09T12:13:22.640122: step 29075, loss 0.514653.
Train: 2018-08-09T12:13:22.733881: step 29076, loss 0.499651.
Train: 2018-08-09T12:13:22.843230: step 29077, loss 0.529317.
Train: 2018-08-09T12:13:22.952548: step 29078, loss 0.593377.
Train: 2018-08-09T12:13:23.061899: step 29079, loss 0.613631.
Train: 2018-08-09T12:13:23.171247: step 29080, loss 0.594133.
Test: 2018-08-09T12:13:23.655533: step 29080, loss 0.550251.
Train: 2018-08-09T12:13:23.764858: step 29081, loss 0.560766.
Train: 2018-08-09T12:13:23.874240: step 29082, loss 0.532007.
Train: 2018-08-09T12:13:23.984238: step 29083, loss 0.49885.
Train: 2018-08-09T12:13:24.093590: step 29084, loss 0.593092.
Train: 2018-08-09T12:13:24.202907: step 29085, loss 0.563427.
Train: 2018-08-09T12:13:24.312286: step 29086, loss 0.496523.
Train: 2018-08-09T12:13:24.421636: step 29087, loss 0.608762.
Train: 2018-08-09T12:13:24.515363: step 29088, loss 0.549486.
Train: 2018-08-09T12:13:24.624706: step 29089, loss 0.527008.
Train: 2018-08-09T12:13:24.734032: step 29090, loss 0.547023.
Test: 2018-08-09T12:13:25.233915: step 29090, loss 0.546583.
Train: 2018-08-09T12:13:25.343295: step 29091, loss 0.460964.
Train: 2018-08-09T12:13:25.452642: step 29092, loss 0.528973.
Train: 2018-08-09T12:13:25.546370: step 29093, loss 0.599554.
Train: 2018-08-09T12:13:25.655720: step 29094, loss 0.554713.
Train: 2018-08-09T12:13:25.765068: step 29095, loss 0.604367.
Train: 2018-08-09T12:13:25.874388: step 29096, loss 0.606332.
Train: 2018-08-09T12:13:25.983768: step 29097, loss 0.541812.
Train: 2018-08-09T12:13:26.093086: step 29098, loss 0.59175.
Train: 2018-08-09T12:13:26.186815: step 29099, loss 0.711434.
Train: 2018-08-09T12:13:26.296164: step 29100, loss 0.51231.
Test: 2018-08-09T12:13:26.796045: step 29100, loss 0.546733.
Train: 2018-08-09T12:13:27.345478: step 29101, loss 0.576859.
Train: 2018-08-09T12:13:27.454828: step 29102, loss 0.477454.
Train: 2018-08-09T12:13:27.564178: step 29103, loss 0.566852.
Train: 2018-08-09T12:13:27.657906: step 29104, loss 0.41418.
Train: 2018-08-09T12:13:27.767254: step 29105, loss 0.511402.
Train: 2018-08-09T12:13:27.876574: step 29106, loss 0.685341.
Train: 2018-08-09T12:13:27.985955: step 29107, loss 0.597939.
Train: 2018-08-09T12:13:28.095301: step 29108, loss 0.576964.
Train: 2018-08-09T12:13:28.204651: step 29109, loss 0.595079.
Train: 2018-08-09T12:13:28.314002: step 29110, loss 0.600611.
Test: 2018-08-09T12:13:28.798257: step 29110, loss 0.548518.
Train: 2018-08-09T12:13:28.909977: step 29111, loss 0.640968.
Train: 2018-08-09T12:13:29.019357: step 29112, loss 0.627385.
Train: 2018-08-09T12:13:29.128707: step 29113, loss 0.562047.
Train: 2018-08-09T12:13:29.238050: step 29114, loss 0.624901.
Train: 2018-08-09T12:13:29.347405: step 29115, loss 0.531511.
Train: 2018-08-09T12:13:29.456755: step 29116, loss 0.59511.
Train: 2018-08-09T12:13:29.566073: step 29117, loss 0.500317.
Train: 2018-08-09T12:13:29.659835: step 29118, loss 0.609246.
Train: 2018-08-09T12:13:29.769181: step 29119, loss 0.656837.
Train: 2018-08-09T12:13:29.884948: step 29120, loss 0.594156.
Test: 2018-08-09T12:13:30.372344: step 29120, loss 0.550202.
Train: 2018-08-09T12:13:30.481669: step 29121, loss 0.501788.
Train: 2018-08-09T12:13:30.591047: step 29122, loss 0.578731.
Train: 2018-08-09T12:13:30.700366: step 29123, loss 0.547775.
Train: 2018-08-09T12:13:30.794128: step 29124, loss 0.579382.
Train: 2018-08-09T12:13:30.903474: step 29125, loss 0.502282.
Train: 2018-08-09T12:13:31.012823: step 29126, loss 0.486283.
Train: 2018-08-09T12:13:31.122173: step 29127, loss 0.609586.
Train: 2018-08-09T12:13:31.231521: step 29128, loss 0.547648.
Train: 2018-08-09T12:13:31.340840: step 29129, loss 0.562789.
Train: 2018-08-09T12:13:31.434600: step 29130, loss 0.625847.
Test: 2018-08-09T12:13:31.934452: step 29130, loss 0.54905.
Train: 2018-08-09T12:13:32.043833: step 29131, loss 0.531423.
Train: 2018-08-09T12:13:32.153180: step 29132, loss 0.470032.
Train: 2018-08-09T12:13:32.262498: step 29133, loss 0.642954.
Train: 2018-08-09T12:13:32.371877: step 29134, loss 0.50004.
Train: 2018-08-09T12:13:32.481227: step 29135, loss 0.594967.
Train: 2018-08-09T12:13:32.574956: step 29136, loss 0.547353.
Train: 2018-08-09T12:13:32.684304: step 29137, loss 0.514556.
Train: 2018-08-09T12:13:32.793656: step 29138, loss 0.580741.
Train: 2018-08-09T12:13:32.902973: step 29139, loss 0.64517.
Train: 2018-08-09T12:13:32.999110: step 29140, loss 0.498536.
Test: 2018-08-09T12:13:33.498989: step 29140, loss 0.551874.
Train: 2018-08-09T12:13:33.608342: step 29141, loss 0.594665.
Train: 2018-08-09T12:13:33.733308: step 29142, loss 0.611126.
Train: 2018-08-09T12:13:33.842662: step 29143, loss 0.549206.
Train: 2018-08-09T12:13:33.952012: step 29144, loss 0.516731.
Train: 2018-08-09T12:13:34.045739: step 29145, loss 0.546365.
Train: 2018-08-09T12:13:34.155088: step 29146, loss 0.629322.
Train: 2018-08-09T12:13:34.264408: step 29147, loss 0.611586.
Train: 2018-08-09T12:13:34.373787: step 29148, loss 0.546536.
Train: 2018-08-09T12:13:34.483136: step 29149, loss 0.467399.
Train: 2018-08-09T12:13:34.576834: step 29150, loss 0.691388.
Test: 2018-08-09T12:13:35.077724: step 29150, loss 0.55156.
Train: 2018-08-09T12:13:35.187048: step 29151, loss 0.593495.
Train: 2018-08-09T12:13:35.296427: step 29152, loss 0.673234.
Train: 2018-08-09T12:13:35.405777: step 29153, loss 0.562127.
Train: 2018-08-09T12:13:35.515126: step 29154, loss 0.563419.
Train: 2018-08-09T12:13:35.608825: step 29155, loss 0.469264.
Train: 2018-08-09T12:13:35.718203: step 29156, loss 0.672849.
Train: 2018-08-09T12:13:35.827552: step 29157, loss 0.580032.
Train: 2018-08-09T12:13:35.921280: step 29158, loss 0.626257.
Train: 2018-08-09T12:13:36.030630: step 29159, loss 0.531803.
Train: 2018-08-09T12:13:36.139949: step 29160, loss 0.56527.
Test: 2018-08-09T12:13:36.639832: step 29160, loss 0.551226.
Train: 2018-08-09T12:13:36.733588: step 29161, loss 0.579656.
Train: 2018-08-09T12:13:36.842937: step 29162, loss 0.594156.
Train: 2018-08-09T12:13:36.952287: step 29163, loss 0.564345.
Train: 2018-08-09T12:13:37.061637: step 29164, loss 0.547956.
Train: 2018-08-09T12:13:37.155364: step 29165, loss 0.440087.
Train: 2018-08-09T12:13:37.264714: step 29166, loss 0.64034.
Train: 2018-08-09T12:13:37.374063: step 29167, loss 0.579065.
Train: 2018-08-09T12:13:37.483381: step 29168, loss 0.517014.
Train: 2018-08-09T12:13:37.592732: step 29169, loss 0.516116.
Train: 2018-08-09T12:13:37.686490: step 29170, loss 0.595283.
Test: 2018-08-09T12:13:38.188668: step 29170, loss 0.551513.
Train: 2018-08-09T12:13:38.298041: step 29171, loss 0.436596.
Train: 2018-08-09T12:13:38.407396: step 29172, loss 0.56218.
Train: 2018-08-09T12:13:38.516745: step 29173, loss 0.597432.
Train: 2018-08-09T12:13:38.610443: step 29174, loss 0.612025.
Train: 2018-08-09T12:13:38.719793: step 29175, loss 0.452242.
Train: 2018-08-09T12:13:38.829142: step 29176, loss 0.531094.
Train: 2018-08-09T12:13:38.938491: step 29177, loss 0.577658.
Train: 2018-08-09T12:13:39.047839: step 29178, loss 0.592754.
Train: 2018-08-09T12:13:39.157190: step 29179, loss 0.56151.
Train: 2018-08-09T12:13:39.250951: step 29180, loss 0.578981.
Test: 2018-08-09T12:13:39.750800: step 29180, loss 0.548346.
Train: 2018-08-09T12:13:39.860148: step 29181, loss 0.65881.
Train: 2018-08-09T12:13:39.955311: step 29182, loss 0.578562.
Train: 2018-08-09T12:13:40.076777: step 29183, loss 0.643296.
Train: 2018-08-09T12:13:40.178598: step 29184, loss 0.449673.
Train: 2018-08-09T12:13:40.287946: step 29185, loss 0.59498.
Train: 2018-08-09T12:13:40.381675: step 29186, loss 0.565084.
Train: 2018-08-09T12:13:40.491057: step 29187, loss 0.611496.
Train: 2018-08-09T12:13:40.600374: step 29188, loss 0.527469.
Train: 2018-08-09T12:13:40.709722: step 29189, loss 0.625381.
Train: 2018-08-09T12:13:40.803481: step 29190, loss 0.479638.
Test: 2018-08-09T12:13:41.303365: step 29190, loss 0.549802.
Train: 2018-08-09T12:13:41.412715: step 29191, loss 0.547115.
Train: 2018-08-09T12:13:41.522061: step 29192, loss 0.496905.
Train: 2018-08-09T12:13:41.615759: step 29193, loss 0.609817.
Train: 2018-08-09T12:13:41.725139: step 29194, loss 0.547523.
Train: 2018-08-09T12:13:41.834488: step 29195, loss 0.576512.
Train: 2018-08-09T12:13:41.943831: step 29196, loss 0.594558.
Train: 2018-08-09T12:13:42.037565: step 29197, loss 0.52867.
Train: 2018-08-09T12:13:42.146885: step 29198, loss 0.580638.
Train: 2018-08-09T12:13:42.256234: step 29199, loss 0.493034.
Train: 2018-08-09T12:13:42.349992: step 29200, loss 0.395395.
Test: 2018-08-09T12:13:42.849845: step 29200, loss 0.545586.
Train: 2018-08-09T12:13:43.474726: step 29201, loss 0.578741.
Train: 2018-08-09T12:13:43.568455: step 29202, loss 0.68399.
Train: 2018-08-09T12:13:43.677774: step 29203, loss 0.602584.
Train: 2018-08-09T12:13:43.787153: step 29204, loss 0.511094.
Train: 2018-08-09T12:13:43.880881: step 29205, loss 0.442505.
Train: 2018-08-09T12:13:43.991568: step 29206, loss 0.540253.
Train: 2018-08-09T12:13:44.100913: step 29207, loss 0.512334.
Train: 2018-08-09T12:13:44.210238: step 29208, loss 0.512261.
Train: 2018-08-09T12:13:44.303966: step 29209, loss 0.548796.
Train: 2018-08-09T12:13:44.413315: step 29210, loss 0.547828.
Test: 2018-08-09T12:13:44.913198: step 29210, loss 0.546453.
Train: 2018-08-09T12:13:45.022576: step 29211, loss 0.569519.
Train: 2018-08-09T12:13:45.131926: step 29212, loss 0.44478.
Train: 2018-08-09T12:13:45.241274: step 29213, loss 0.480235.
Train: 2018-08-09T12:13:45.335002: step 29214, loss 0.58159.
Train: 2018-08-09T12:13:45.444354: step 29215, loss 0.665193.
Train: 2018-08-09T12:13:45.553701: step 29216, loss 0.562993.
Train: 2018-08-09T12:13:45.663050: step 29217, loss 0.644246.
Train: 2018-08-09T12:13:45.772393: step 29218, loss 0.526574.
Train: 2018-08-09T12:13:45.866096: step 29219, loss 0.543067.
Train: 2018-08-09T12:13:45.977794: step 29220, loss 0.662495.
Test: 2018-08-09T12:13:46.477647: step 29220, loss 0.549301.
Train: 2018-08-09T12:13:46.571374: step 29221, loss 0.580274.
Train: 2018-08-09T12:13:46.680752: step 29222, loss 0.464467.
Train: 2018-08-09T12:13:46.790103: step 29223, loss 0.562612.
Train: 2018-08-09T12:13:46.883830: step 29224, loss 0.545167.
Train: 2018-08-09T12:13:47.002838: step 29225, loss 0.629573.
Train: 2018-08-09T12:13:47.096584: step 29226, loss 0.561143.
Train: 2018-08-09T12:13:47.205904: step 29227, loss 0.560147.
Train: 2018-08-09T12:13:47.315285: step 29228, loss 0.529043.
Train: 2018-08-09T12:13:47.409011: step 29229, loss 0.54473.
Train: 2018-08-09T12:13:47.518355: step 29230, loss 0.714971.
Test: 2018-08-09T12:13:48.010855: step 29230, loss 0.547627.
Train: 2018-08-09T12:13:48.120234: step 29231, loss 0.482011.
Train: 2018-08-09T12:13:48.229589: step 29232, loss 0.513187.
Train: 2018-08-09T12:13:48.338901: step 29233, loss 0.54743.
Train: 2018-08-09T12:13:48.448250: step 29234, loss 0.564448.
Train: 2018-08-09T12:13:48.542008: step 29235, loss 0.580002.
Train: 2018-08-09T12:13:48.651357: step 29236, loss 0.495929.
Train: 2018-08-09T12:13:48.760708: step 29237, loss 0.530136.
Train: 2018-08-09T12:13:48.870026: step 29238, loss 0.462249.
Train: 2018-08-09T12:13:48.963785: step 29239, loss 0.695455.
Train: 2018-08-09T12:13:49.073133: step 29240, loss 0.579461.
Test: 2018-08-09T12:13:49.574013: step 29240, loss 0.549528.
Train: 2018-08-09T12:13:49.683376: step 29241, loss 0.626582.
Train: 2018-08-09T12:13:49.777108: step 29242, loss 0.62904.
Train: 2018-08-09T12:13:49.886457: step 29243, loss 0.563708.
Train: 2018-08-09T12:13:49.995776: step 29244, loss 0.626038.
Train: 2018-08-09T12:13:50.105134: step 29245, loss 0.595277.
Train: 2018-08-09T12:13:50.198883: step 29246, loss 0.546855.
Train: 2018-08-09T12:13:50.308232: step 29247, loss 0.514659.
Train: 2018-08-09T12:13:50.417581: step 29248, loss 0.546154.
Train: 2018-08-09T12:13:50.511310: step 29249, loss 0.593708.
Train: 2018-08-09T12:13:50.620628: step 29250, loss 0.547626.
Test: 2018-08-09T12:13:51.120514: step 29250, loss 0.552136.
Train: 2018-08-09T12:13:51.229859: step 29251, loss 0.450809.
Train: 2018-08-09T12:13:51.323618: step 29252, loss 0.482818.
Train: 2018-08-09T12:13:51.432937: step 29253, loss 0.481644.
Train: 2018-08-09T12:13:51.542317: step 29254, loss 0.545824.
Train: 2018-08-09T12:13:51.651637: step 29255, loss 0.579478.
Train: 2018-08-09T12:13:51.761015: step 29256, loss 0.51458.
Train: 2018-08-09T12:13:51.854743: step 29257, loss 0.562401.
Train: 2018-08-09T12:13:51.964092: step 29258, loss 0.613969.
Train: 2018-08-09T12:13:52.073411: step 29259, loss 0.614218.
Train: 2018-08-09T12:13:52.167169: step 29260, loss 0.594711.
Test: 2018-08-09T12:13:52.667023: step 29260, loss 0.549636.
Train: 2018-08-09T12:13:52.776406: step 29261, loss 0.5954.
Train: 2018-08-09T12:13:52.885750: step 29262, loss 0.6602.
Train: 2018-08-09T12:13:52.980961: step 29263, loss 0.579545.
Train: 2018-08-09T12:13:53.090311: step 29264, loss 0.643433.
Train: 2018-08-09T12:13:53.199660: step 29265, loss 0.482335.
Train: 2018-08-09T12:13:53.293387: step 29266, loss 0.593823.
Train: 2018-08-09T12:13:53.402736: step 29267, loss 0.610097.
Train: 2018-08-09T12:13:53.512056: step 29268, loss 0.498256.
Train: 2018-08-09T12:13:53.621436: step 29269, loss 0.546182.
Train: 2018-08-09T12:13:53.715163: step 29270, loss 0.547035.
Test: 2018-08-09T12:13:54.215046: step 29270, loss 0.550713.
Train: 2018-08-09T12:13:54.324364: step 29271, loss 0.466133.
Train: 2018-08-09T12:13:54.418122: step 29272, loss 0.532033.
Train: 2018-08-09T12:13:54.527441: step 29273, loss 0.496942.
Train: 2018-08-09T12:13:54.636821: step 29274, loss 0.497387.
Train: 2018-08-09T12:13:54.746170: step 29275, loss 0.596266.
Train: 2018-08-09T12:13:54.839868: step 29276, loss 0.531419.
Train: 2018-08-09T12:13:54.951616: step 29277, loss 0.578394.
Train: 2018-08-09T12:13:55.060995: step 29278, loss 0.564233.
Train: 2018-08-09T12:13:55.170314: step 29279, loss 0.496388.
Train: 2018-08-09T12:13:55.264041: step 29280, loss 0.545204.
Test: 2018-08-09T12:13:55.763933: step 29280, loss 0.547705.
Train: 2018-08-09T12:13:55.920167: step 29281, loss 0.683576.
Train: 2018-08-09T12:13:56.029518: step 29282, loss 0.561889.
Train: 2018-08-09T12:13:56.123213: step 29283, loss 0.632141.
Train: 2018-08-09T12:13:56.232594: step 29284, loss 0.562443.
Train: 2018-08-09T12:13:56.341913: step 29285, loss 0.563202.
Train: 2018-08-09T12:13:56.451293: step 29286, loss 0.546745.
Train: 2018-08-09T12:13:56.544990: step 29287, loss 0.512919.
Train: 2018-08-09T12:13:56.654371: step 29288, loss 0.64552.
Train: 2018-08-09T12:13:56.763688: step 29289, loss 0.56256.
Train: 2018-08-09T12:13:56.873037: step 29290, loss 0.497712.
Test: 2018-08-09T12:13:57.373589: step 29290, loss 0.549186.
Train: 2018-08-09T12:13:57.467346: step 29291, loss 0.464777.
Train: 2018-08-09T12:13:57.576696: step 29292, loss 0.480837.
Train: 2018-08-09T12:13:57.686045: step 29293, loss 0.5626.
Train: 2018-08-09T12:13:57.795398: step 29294, loss 0.496794.
Train: 2018-08-09T12:13:57.889121: step 29295, loss 0.613243.
Train: 2018-08-09T12:13:57.998439: step 29296, loss 0.545128.
Train: 2018-08-09T12:13:58.107790: step 29297, loss 0.56217.
Train: 2018-08-09T12:13:58.201518: step 29298, loss 0.512188.
Train: 2018-08-09T12:13:58.310897: step 29299, loss 0.561676.
Train: 2018-08-09T12:13:58.420246: step 29300, loss 0.648529.
Test: 2018-08-09T12:13:58.920099: step 29300, loss 0.547068.
Train: 2018-08-09T12:13:59.544951: step 29301, loss 0.595206.
Train: 2018-08-09T12:13:59.638709: step 29302, loss 0.613296.
Train: 2018-08-09T12:13:59.748058: step 29303, loss 0.647712.
Train: 2018-08-09T12:13:59.857378: step 29304, loss 0.546125.
Train: 2018-08-09T12:13:59.953392: step 29305, loss 0.429462.
Train: 2018-08-09T12:14:00.062740: step 29306, loss 0.578777.
Train: 2018-08-09T12:14:00.172119: step 29307, loss 0.662539.
Train: 2018-08-09T12:14:00.281439: step 29308, loss 0.447028.
Train: 2018-08-09T12:14:00.390789: step 29309, loss 0.562048.
Train: 2018-08-09T12:14:00.484545: step 29310, loss 0.595197.
Test: 2018-08-09T12:14:00.984399: step 29310, loss 0.545751.
Train: 2018-08-09T12:14:01.078156: step 29311, loss 0.530575.
Train: 2018-08-09T12:14:01.187505: step 29312, loss 0.546282.
Train: 2018-08-09T12:14:01.296856: step 29313, loss 0.660812.
Train: 2018-08-09T12:14:01.406204: step 29314, loss 0.596662.
Train: 2018-08-09T12:14:01.499934: step 29315, loss 0.464009.
Train: 2018-08-09T12:14:01.609281: step 29316, loss 0.529709.
Train: 2018-08-09T12:14:01.718630: step 29317, loss 0.579076.
Train: 2018-08-09T12:14:01.827949: step 29318, loss 0.644841.
Train: 2018-08-09T12:14:01.923081: step 29319, loss 0.611552.
Train: 2018-08-09T12:14:02.032424: step 29320, loss 0.611781.
Test: 2018-08-09T12:14:02.532283: step 29320, loss 0.54981.
Train: 2018-08-09T12:14:02.641632: step 29321, loss 0.48112.
Train: 2018-08-09T12:14:02.735360: step 29322, loss 0.595534.
Train: 2018-08-09T12:14:02.844709: step 29323, loss 0.660085.
Train: 2018-08-09T12:14:02.954088: step 29324, loss 0.708604.
Train: 2018-08-09T12:14:03.063437: step 29325, loss 0.594938.
Train: 2018-08-09T12:14:03.157165: step 29326, loss 0.482455.
Train: 2018-08-09T12:14:03.266514: step 29327, loss 0.546906.
Train: 2018-08-09T12:14:03.375864: step 29328, loss 0.610664.
Train: 2018-08-09T12:14:03.469591: step 29329, loss 0.467747.
Train: 2018-08-09T12:14:03.578941: step 29330, loss 0.578542.
Test: 2018-08-09T12:14:04.074812: step 29330, loss 0.550498.
Train: 2018-08-09T12:14:04.184160: step 29331, loss 0.547148.
Train: 2018-08-09T12:14:04.293542: step 29332, loss 0.531087.
Train: 2018-08-09T12:14:04.402860: step 29333, loss 0.578553.
Train: 2018-08-09T12:14:04.496620: step 29334, loss 0.610225.
Train: 2018-08-09T12:14:04.605936: step 29335, loss 0.515437.
Train: 2018-08-09T12:14:04.715316: step 29336, loss 0.578329.
Train: 2018-08-09T12:14:04.824636: step 29337, loss 0.562627.
Train: 2018-08-09T12:14:04.933996: step 29338, loss 0.529251.
Train: 2018-08-09T12:14:05.027766: step 29339, loss 0.529518.
Train: 2018-08-09T12:14:05.137094: step 29340, loss 0.495302.
Test: 2018-08-09T12:14:05.636957: step 29340, loss 0.545679.
Train: 2018-08-09T12:14:05.730702: step 29341, loss 0.510178.
Train: 2018-08-09T12:14:05.840022: step 29342, loss 0.545629.
Train: 2018-08-09T12:14:05.949370: step 29343, loss 0.64157.
Train: 2018-08-09T12:14:06.058720: step 29344, loss 0.616997.
Train: 2018-08-09T12:14:06.168098: step 29345, loss 0.63716.
Train: 2018-08-09T12:14:06.277419: step 29346, loss 0.583755.
Train: 2018-08-09T12:14:06.371176: step 29347, loss 0.510631.
Train: 2018-08-09T12:14:06.480526: step 29348, loss 0.511439.
Train: 2018-08-09T12:14:06.589874: step 29349, loss 0.459423.
Train: 2018-08-09T12:14:06.699223: step 29350, loss 0.570261.
Test: 2018-08-09T12:14:07.199107: step 29350, loss 0.545354.
Train: 2018-08-09T12:14:07.292803: step 29351, loss 0.575039.
Train: 2018-08-09T12:14:07.402183: step 29352, loss 0.522729.
Train: 2018-08-09T12:14:07.511534: step 29353, loss 0.516864.
Train: 2018-08-09T12:14:07.620851: step 29354, loss 0.58762.
Train: 2018-08-09T12:14:07.714580: step 29355, loss 0.475746.
Train: 2018-08-09T12:14:07.823959: step 29356, loss 0.533724.
Train: 2018-08-09T12:14:07.935622: step 29357, loss 0.642202.
Train: 2018-08-09T12:14:08.045003: step 29358, loss 0.526566.
Train: 2018-08-09T12:14:08.154321: step 29359, loss 0.63286.
Train: 2018-08-09T12:14:08.248049: step 29360, loss 0.643814.
Test: 2018-08-09T12:14:08.747932: step 29360, loss 0.552835.
Train: 2018-08-09T12:14:08.857311: step 29361, loss 0.548989.
Train: 2018-08-09T12:14:08.966660: step 29362, loss 0.531538.
Train: 2018-08-09T12:14:09.060359: step 29363, loss 0.529889.
Train: 2018-08-09T12:14:09.169708: step 29364, loss 0.530098.
Train: 2018-08-09T12:14:09.279056: step 29365, loss 0.482517.
Train: 2018-08-09T12:14:09.388439: step 29366, loss 0.627828.
Train: 2018-08-09T12:14:09.497785: step 29367, loss 0.5465.
Train: 2018-08-09T12:14:09.622726: step 29368, loss 0.56262.
Train: 2018-08-09T12:14:09.716483: step 29369, loss 0.530011.
Train: 2018-08-09T12:14:09.825833: step 29370, loss 0.627071.
Test: 2018-08-09T12:14:10.327045: step 29370, loss 0.54749.
Train: 2018-08-09T12:14:10.436422: step 29371, loss 0.514669.
Train: 2018-08-09T12:14:10.530147: step 29372, loss 0.530857.
Train: 2018-08-09T12:14:10.639469: step 29373, loss 0.661651.
Train: 2018-08-09T12:14:10.748849: step 29374, loss 0.579003.
Train: 2018-08-09T12:14:10.858199: step 29375, loss 0.546322.
Train: 2018-08-09T12:14:10.951896: step 29376, loss 0.466263.
Train: 2018-08-09T12:14:11.061276: step 29377, loss 0.578881.
Train: 2018-08-09T12:14:11.170625: step 29378, loss 0.611294.
Train: 2018-08-09T12:14:11.282892: step 29379, loss 0.611856.
Train: 2018-08-09T12:14:11.376614: step 29380, loss 0.546677.
Test: 2018-08-09T12:14:11.876500: step 29380, loss 0.548876.
Train: 2018-08-09T12:14:11.983462: step 29381, loss 0.497909.
Train: 2018-08-09T12:14:12.092811: step 29382, loss 0.643834.
Train: 2018-08-09T12:14:12.202167: step 29383, loss 0.482395.
Train: 2018-08-09T12:14:12.311539: step 29384, loss 0.595451.
Train: 2018-08-09T12:14:12.420859: step 29385, loss 0.610807.
Train: 2018-08-09T12:14:12.530238: step 29386, loss 0.595032.
Train: 2018-08-09T12:14:12.623965: step 29387, loss 0.450116.
Train: 2018-08-09T12:14:12.733315: step 29388, loss 0.594782.
Train: 2018-08-09T12:14:12.842633: step 29389, loss 0.54687.
Train: 2018-08-09T12:14:12.952013: step 29390, loss 0.643832.
Test: 2018-08-09T12:14:13.451866: step 29390, loss 0.550158.
Train: 2018-08-09T12:14:13.545593: step 29391, loss 0.530826.
Train: 2018-08-09T12:14:13.654942: step 29392, loss 0.57935.
Train: 2018-08-09T12:14:13.764323: step 29393, loss 0.626688.
Train: 2018-08-09T12:14:13.873672: step 29394, loss 0.578181.
Train: 2018-08-09T12:14:13.983021: step 29395, loss 0.722961.
Train: 2018-08-09T12:14:14.092340: step 29396, loss 0.546754.
Train: 2018-08-09T12:14:14.186092: step 29397, loss 0.578985.
Train: 2018-08-09T12:14:14.295447: step 29398, loss 0.467962.
Train: 2018-08-09T12:14:14.404767: step 29399, loss 0.578333.
Train: 2018-08-09T12:14:14.514146: step 29400, loss 0.531364.
Test: 2018-08-09T12:14:15.016310: step 29400, loss 0.550567.
Train: 2018-08-09T12:14:15.578677: step 29401, loss 0.531158.
Train: 2018-08-09T12:14:15.688026: step 29402, loss 0.500404.
Train: 2018-08-09T12:14:15.797375: step 29403, loss 0.530914.
Train: 2018-08-09T12:14:15.906754: step 29404, loss 0.579345.
Train: 2018-08-09T12:14:16.000481: step 29405, loss 0.562852.
Train: 2018-08-09T12:14:16.109831: step 29406, loss 0.563458.
Train: 2018-08-09T12:14:16.219181: step 29407, loss 0.563376.
Train: 2018-08-09T12:14:16.328500: step 29408, loss 0.579423.
Train: 2018-08-09T12:14:16.437848: step 29409, loss 0.643326.
Train: 2018-08-09T12:14:16.547228: step 29410, loss 0.611263.
Test: 2018-08-09T12:14:17.034270: step 29410, loss 0.549718.
Train: 2018-08-09T12:14:17.143588: step 29411, loss 0.562883.
Train: 2018-08-09T12:14:17.252962: step 29412, loss 0.578736.
Train: 2018-08-09T12:14:17.362320: step 29413, loss 0.594611.
Train: 2018-08-09T12:14:17.471668: step 29414, loss 0.530712.
Train: 2018-08-09T12:14:17.581014: step 29415, loss 0.483361.
Train: 2018-08-09T12:14:17.690363: step 29416, loss 0.435523.
Train: 2018-08-09T12:14:17.799684: step 29417, loss 0.530872.
Train: 2018-08-09T12:14:17.893411: step 29418, loss 0.562768.
Train: 2018-08-09T12:14:18.002790: step 29419, loss 0.562829.
Train: 2018-08-09T12:14:18.112110: step 29420, loss 0.514424.
Test: 2018-08-09T12:14:18.611992: step 29420, loss 0.550595.
Train: 2018-08-09T12:14:18.721371: step 29421, loss 0.514004.
Train: 2018-08-09T12:14:18.815070: step 29422, loss 0.563168.
Train: 2018-08-09T12:14:18.924418: step 29423, loss 0.563546.
Train: 2018-08-09T12:14:19.033767: step 29424, loss 0.546905.
Train: 2018-08-09T12:14:19.143146: step 29425, loss 0.430821.
Train: 2018-08-09T12:14:19.252465: step 29426, loss 0.612148.
Train: 2018-08-09T12:14:19.361814: step 29427, loss 0.530452.
Train: 2018-08-09T12:14:19.471163: step 29428, loss 0.695512.
Train: 2018-08-09T12:14:19.564922: step 29429, loss 0.528575.
Train: 2018-08-09T12:14:19.674271: step 29430, loss 0.495525.
Test: 2018-08-09T12:14:20.174155: step 29430, loss 0.549309.
Train: 2018-08-09T12:14:20.283503: step 29431, loss 0.678394.
Train: 2018-08-09T12:14:20.392856: step 29432, loss 0.512949.
Train: 2018-08-09T12:14:20.502172: step 29433, loss 0.546143.
Train: 2018-08-09T12:14:20.595932: step 29434, loss 0.545446.
Train: 2018-08-09T12:14:20.705249: step 29435, loss 0.597499.
Train: 2018-08-09T12:14:20.814628: step 29436, loss 0.579102.
Train: 2018-08-09T12:14:20.932646: step 29437, loss 0.578247.
Train: 2018-08-09T12:14:21.026404: step 29438, loss 0.530345.
Train: 2018-08-09T12:14:21.135756: step 29439, loss 0.562826.
Train: 2018-08-09T12:14:21.245103: step 29440, loss 0.578478.
Test: 2018-08-09T12:14:21.744956: step 29440, loss 0.546193.
Train: 2018-08-09T12:14:21.854329: step 29441, loss 0.512411.
Train: 2018-08-09T12:14:21.963654: step 29442, loss 0.595513.
Train: 2018-08-09T12:14:22.073004: step 29443, loss 0.646206.
Train: 2018-08-09T12:14:22.182352: step 29444, loss 0.496142.
Train: 2018-08-09T12:14:22.276109: step 29445, loss 0.67815.
Train: 2018-08-09T12:14:22.401081: step 29446, loss 0.596025.
Train: 2018-08-09T12:14:22.494808: step 29447, loss 0.496462.
Train: 2018-08-09T12:14:22.604159: step 29448, loss 0.644718.
Train: 2018-08-09T12:14:22.713508: step 29449, loss 0.611914.
Train: 2018-08-09T12:14:22.822856: step 29450, loss 0.546221.
Test: 2018-08-09T12:14:23.317041: step 29450, loss 0.547977.
Train: 2018-08-09T12:14:23.426420: step 29451, loss 0.513673.
Train: 2018-08-09T12:14:23.535739: step 29452, loss 0.61163.
Train: 2018-08-09T12:14:23.645087: step 29453, loss 0.578994.
Train: 2018-08-09T12:14:23.738817: step 29454, loss 0.578901.
Train: 2018-08-09T12:14:23.848194: step 29455, loss 0.497972.
Train: 2018-08-09T12:14:23.963863: step 29456, loss 0.627364.
Train: 2018-08-09T12:14:24.073244: step 29457, loss 0.466051.
Train: 2018-08-09T12:14:24.182592: step 29458, loss 0.465903.
Train: 2018-08-09T12:14:24.291912: step 29459, loss 0.627318.
Train: 2018-08-09T12:14:24.385640: step 29460, loss 0.57898.
Test: 2018-08-09T12:14:24.885523: step 29460, loss 0.548814.
Train: 2018-08-09T12:14:24.994901: step 29461, loss 0.562683.
Train: 2018-08-09T12:14:25.104220: step 29462, loss 0.627352.
Train: 2018-08-09T12:14:25.197979: step 29463, loss 0.546651.
Train: 2018-08-09T12:14:25.307297: step 29464, loss 0.530483.
Train: 2018-08-09T12:14:25.416677: step 29465, loss 0.498136.
Train: 2018-08-09T12:14:25.526025: step 29466, loss 0.562734.
Train: 2018-08-09T12:14:25.635379: step 29467, loss 0.546576.
Train: 2018-08-09T12:14:25.744695: step 29468, loss 0.659864.
Train: 2018-08-09T12:14:25.854044: step 29469, loss 0.692216.
Train: 2018-08-09T12:14:25.947772: step 29470, loss 0.562712.
Test: 2018-08-09T12:14:26.447653: step 29470, loss 0.548862.
Train: 2018-08-09T12:14:26.557033: step 29471, loss 0.627312.
Train: 2018-08-09T12:14:26.666353: step 29472, loss 0.675335.
Train: 2018-08-09T12:14:26.775731: step 29473, loss 0.562838.
Train: 2018-08-09T12:14:26.885085: step 29474, loss 0.546971.
Train: 2018-08-09T12:14:26.980191: step 29475, loss 0.531122.
Train: 2018-08-09T12:14:27.089543: step 29476, loss 0.690053.
Train: 2018-08-09T12:14:27.198890: step 29477, loss 0.547228.
Train: 2018-08-09T12:14:27.308209: step 29478, loss 0.499928.
Train: 2018-08-09T12:14:27.417558: step 29479, loss 0.452808.
Train: 2018-08-09T12:14:27.526937: step 29480, loss 0.610394.
Test: 2018-08-09T12:14:28.011169: step 29480, loss 0.547745.
Train: 2018-08-09T12:14:28.120547: step 29481, loss 0.563103.
Train: 2018-08-09T12:14:28.229897: step 29482, loss 0.57887.
Train: 2018-08-09T12:14:28.339215: step 29483, loss 0.578861.
Train: 2018-08-09T12:14:28.448594: step 29484, loss 0.563129.
Train: 2018-08-09T12:14:28.557945: step 29485, loss 0.626101.
Train: 2018-08-09T12:14:28.667293: step 29486, loss 0.641758.
Train: 2018-08-09T12:14:28.776643: step 29487, loss 0.594579.
Train: 2018-08-09T12:14:28.885992: step 29488, loss 0.484909.
Train: 2018-08-09T12:14:28.996724: step 29489, loss 0.563224.
Train: 2018-08-09T12:14:29.090452: step 29490, loss 0.547589.
Test: 2018-08-09T12:14:29.590334: step 29490, loss 0.549164.
Train: 2018-08-09T12:14:29.699716: step 29491, loss 0.57886.
Train: 2018-08-09T12:14:29.809062: step 29492, loss 0.563233.
Train: 2018-08-09T12:14:29.918414: step 29493, loss 0.610156.
Train: 2018-08-09T12:14:30.027761: step 29494, loss 0.563255.
Train: 2018-08-09T12:14:30.137080: step 29495, loss 0.594544.
Train: 2018-08-09T12:14:30.246429: step 29496, loss 0.547648.
Train: 2018-08-09T12:14:30.340188: step 29497, loss 0.563262.
Train: 2018-08-09T12:14:30.449539: step 29498, loss 0.529959.
Train: 2018-08-09T12:14:30.558886: step 29499, loss 0.547637.
Train: 2018-08-09T12:14:30.668205: step 29500, loss 0.547568.
Test: 2018-08-09T12:14:31.170356: step 29500, loss 0.550285.
Train: 2018-08-09T12:14:31.748368: step 29501, loss 0.641608.
Train: 2018-08-09T12:14:31.857717: step 29502, loss 0.563129.
Train: 2018-08-09T12:14:31.967070: step 29503, loss 0.468895.
Train: 2018-08-09T12:14:32.076418: step 29504, loss 0.610197.
Train: 2018-08-09T12:14:32.185768: step 29505, loss 0.515739.
Train: 2018-08-09T12:14:32.295114: step 29506, loss 0.548566.
Train: 2018-08-09T12:14:32.388813: step 29507, loss 0.578094.
Train: 2018-08-09T12:14:32.498162: step 29508, loss 0.561527.
Train: 2018-08-09T12:14:32.607510: step 29509, loss 0.641787.
Train: 2018-08-09T12:14:32.716860: step 29510, loss 0.498337.
Test: 2018-08-09T12:14:33.217381: step 29510, loss 0.548558.
Train: 2018-08-09T12:14:33.326706: step 29511, loss 0.612743.
Train: 2018-08-09T12:14:33.420432: step 29512, loss 0.498051.
Train: 2018-08-09T12:14:33.529782: step 29513, loss 0.545741.
Train: 2018-08-09T12:14:33.654755: step 29514, loss 0.678107.
Train: 2018-08-09T12:14:33.748481: step 29515, loss 0.54816.
Train: 2018-08-09T12:14:33.857829: step 29516, loss 0.661279.
Train: 2018-08-09T12:14:33.967209: step 29517, loss 0.417357.
Train: 2018-08-09T12:14:34.076528: step 29518, loss 0.595781.
Train: 2018-08-09T12:14:34.185907: step 29519, loss 0.449189.
Train: 2018-08-09T12:14:34.295226: step 29520, loss 0.626426.
Test: 2018-08-09T12:14:34.795122: step 29520, loss 0.55065.
Train: 2018-08-09T12:14:34.888837: step 29521, loss 0.561702.
Train: 2018-08-09T12:14:34.998217: step 29522, loss 0.579103.
Train: 2018-08-09T12:14:35.107566: step 29523, loss 0.61181.
Train: 2018-08-09T12:14:35.216885: step 29524, loss 0.612824.
Train: 2018-08-09T12:14:35.326264: step 29525, loss 0.564348.
Train: 2018-08-09T12:14:35.435613: step 29526, loss 0.626089.
Train: 2018-08-09T12:14:35.544963: step 29527, loss 0.514091.
Train: 2018-08-09T12:14:35.638690: step 29528, loss 0.547881.
Train: 2018-08-09T12:14:35.748040: step 29529, loss 0.514932.
Train: 2018-08-09T12:14:35.857359: step 29530, loss 0.579429.
Test: 2018-08-09T12:14:36.358538: step 29530, loss 0.549241.
Train: 2018-08-09T12:14:36.467887: step 29531, loss 0.594491.
Train: 2018-08-09T12:14:36.577236: step 29532, loss 0.515221.
Train: 2018-08-09T12:14:36.670964: step 29533, loss 0.451414.
Train: 2018-08-09T12:14:36.780282: step 29534, loss 0.610397.
Train: 2018-08-09T12:14:36.889663: step 29535, loss 0.578877.
Train: 2018-08-09T12:14:36.999011: step 29536, loss 0.465117.
Train: 2018-08-09T12:14:37.108361: step 29537, loss 0.611342.
Train: 2018-08-09T12:14:37.217710: step 29538, loss 0.463635.
Train: 2018-08-09T12:14:37.327053: step 29539, loss 0.645217.
Train: 2018-08-09T12:14:37.436409: step 29540, loss 0.448269.
Test: 2018-08-09T12:14:37.921964: step 29540, loss 0.54855.
Train: 2018-08-09T12:14:38.038901: step 29541, loss 0.644269.
Train: 2018-08-09T12:14:38.148251: step 29542, loss 0.528377.
Train: 2018-08-09T12:14:38.241978: step 29543, loss 0.651066.
Train: 2018-08-09T12:14:38.351328: step 29544, loss 0.645267.
Train: 2018-08-09T12:14:38.460677: step 29545, loss 0.696832.
Train: 2018-08-09T12:14:38.570026: step 29546, loss 0.497194.
Train: 2018-08-09T12:14:38.679377: step 29547, loss 0.530021.
Train: 2018-08-09T12:14:38.788696: step 29548, loss 0.54599.
Train: 2018-08-09T12:14:38.898043: step 29549, loss 0.548033.
Train: 2018-08-09T12:14:39.007412: step 29550, loss 0.448839.
Test: 2018-08-09T12:14:39.507275: step 29550, loss 0.549345.
Train: 2018-08-09T12:14:39.601004: step 29551, loss 0.611239.
Train: 2018-08-09T12:14:39.710352: step 29552, loss 0.547149.
Train: 2018-08-09T12:14:39.819732: step 29553, loss 0.482454.
Train: 2018-08-09T12:14:39.929083: step 29554, loss 0.691959.
Train: 2018-08-09T12:14:40.038430: step 29555, loss 0.595279.
Train: 2018-08-09T12:14:40.147749: step 29556, loss 0.595371.
Train: 2018-08-09T12:14:40.257129: step 29557, loss 0.611364.
Train: 2018-08-09T12:14:40.366447: step 29558, loss 0.546824.
Train: 2018-08-09T12:14:40.475798: step 29559, loss 0.514876.
Train: 2018-08-09T12:14:40.569555: step 29560, loss 0.514905.
Test: 2018-08-09T12:14:41.069408: step 29560, loss 0.55029.
Train: 2018-08-09T12:14:41.178806: step 29561, loss 0.595077.
Train: 2018-08-09T12:14:41.288136: step 29562, loss 0.56297.
Train: 2018-08-09T12:14:41.381833: step 29563, loss 0.530677.
Train: 2018-08-09T12:14:41.491183: step 29564, loss 0.59512.
Train: 2018-08-09T12:14:41.600533: step 29565, loss 0.610902.
Train: 2018-08-09T12:14:41.709882: step 29566, loss 0.610924.
Train: 2018-08-09T12:14:41.819261: step 29567, loss 0.626769.
Train: 2018-08-09T12:14:41.930068: step 29568, loss 0.515095.
Train: 2018-08-09T12:14:42.023764: step 29569, loss 0.53114.
Train: 2018-08-09T12:14:42.133143: step 29570, loss 0.483469.
Test: 2018-08-09T12:14:42.635838: step 29570, loss 0.550431.
Train: 2018-08-09T12:14:42.729597: step 29571, loss 0.594811.
Train: 2018-08-09T12:14:42.838947: step 29572, loss 0.578821.
Train: 2018-08-09T12:14:42.948294: step 29573, loss 0.546993.
Train: 2018-08-09T12:14:43.057644: step 29574, loss 0.594821.
Train: 2018-08-09T12:14:43.166993: step 29575, loss 0.547004.
Train: 2018-08-09T12:14:43.276344: step 29576, loss 0.578889.
Train: 2018-08-09T12:14:43.385692: step 29577, loss 0.49913.
Train: 2018-08-09T12:14:43.479420: step 29578, loss 0.435125.
Train: 2018-08-09T12:14:43.588771: step 29579, loss 0.578902.
Train: 2018-08-09T12:14:43.698120: step 29580, loss 0.546722.
Test: 2018-08-09T12:14:44.197970: step 29580, loss 0.548874.
Train: 2018-08-09T12:14:44.307352: step 29581, loss 0.5949.
Train: 2018-08-09T12:14:44.416668: step 29582, loss 0.546611.
Train: 2018-08-09T12:14:44.510427: step 29583, loss 0.643788.
Train: 2018-08-09T12:14:44.619776: step 29584, loss 0.595091.
Train: 2018-08-09T12:14:44.729120: step 29585, loss 0.513915.
Train: 2018-08-09T12:14:44.838476: step 29586, loss 0.513965.
Train: 2018-08-09T12:14:44.947795: step 29587, loss 0.643457.
Train: 2018-08-09T12:14:45.057173: step 29588, loss 0.594455.
Train: 2018-08-09T12:14:45.166492: step 29589, loss 0.576917.
Train: 2018-08-09T12:14:45.275872: step 29590, loss 0.580082.
Test: 2018-08-09T12:14:45.760127: step 29590, loss 0.549971.
Train: 2018-08-09T12:14:45.869481: step 29591, loss 0.462328.
Train: 2018-08-09T12:14:45.980075: step 29592, loss 0.64574.
Train: 2018-08-09T12:14:46.089454: step 29593, loss 0.630123.
Train: 2018-08-09T12:14:46.198803: step 29594, loss 0.645694.
Train: 2018-08-09T12:14:46.308152: step 29595, loss 0.550041.
Train: 2018-08-09T12:14:46.401880: step 29596, loss 0.563274.
Train: 2018-08-09T12:14:46.511229: step 29597, loss 0.547297.
Train: 2018-08-09T12:14:46.620549: step 29598, loss 0.691065.
Train: 2018-08-09T12:14:46.729930: step 29599, loss 0.562754.
Train: 2018-08-09T12:14:46.839247: step 29600, loss 0.6108.
Test: 2018-08-09T12:14:47.334567: step 29600, loss 0.547446.
Train: 2018-08-09T12:14:47.934614: step 29601, loss 0.467485.
Train: 2018-08-09T12:14:48.043956: step 29602, loss 0.562917.
Train: 2018-08-09T12:14:48.153312: step 29603, loss 0.515631.
Train: 2018-08-09T12:14:48.262631: step 29604, loss 0.531407.
Train: 2018-08-09T12:14:48.371980: step 29605, loss 0.56305.
Train: 2018-08-09T12:14:48.465738: step 29606, loss 0.610486.
Train: 2018-08-09T12:14:48.575091: step 29607, loss 0.59467.
Train: 2018-08-09T12:14:48.684436: step 29608, loss 0.515657.
Train: 2018-08-09T12:14:48.793756: step 29609, loss 0.578858.
Train: 2018-08-09T12:14:48.903106: step 29610, loss 0.594671.
Test: 2018-08-09T12:14:49.387366: step 29610, loss 0.551243.
Train: 2018-08-09T12:14:49.496744: step 29611, loss 0.578866.
Train: 2018-08-09T12:14:49.606094: step 29612, loss 0.594657.
Train: 2018-08-09T12:14:49.715443: step 29613, loss 0.61043.
Train: 2018-08-09T12:14:49.809141: step 29614, loss 0.484284.
Train: 2018-08-09T12:14:49.918491: step 29615, loss 0.641928.
Train: 2018-08-09T12:14:50.027839: step 29616, loss 0.657626.
Train: 2018-08-09T12:14:50.137189: step 29617, loss 0.484557.
Train: 2018-08-09T12:14:50.246568: step 29618, loss 0.578872.
Train: 2018-08-09T12:14:50.355911: step 29619, loss 0.516082.
Train: 2018-08-09T12:14:50.449615: step 29620, loss 0.531765.
Test: 2018-08-09T12:14:50.951780: step 29620, loss 0.551996.
Train: 2018-08-09T12:14:51.061160: step 29621, loss 0.578872.
Train: 2018-08-09T12:14:51.170479: step 29622, loss 0.500246.
Train: 2018-08-09T12:14:51.279858: step 29623, loss 0.437078.
Train: 2018-08-09T12:14:51.373555: step 29624, loss 0.594693.
Train: 2018-08-09T12:14:51.482935: step 29625, loss 0.547083.
Train: 2018-08-09T12:14:51.592284: step 29626, loss 0.467326.
Train: 2018-08-09T12:14:51.701604: step 29627, loss 0.62615.
Train: 2018-08-09T12:14:51.810983: step 29628, loss 0.531032.
Train: 2018-08-09T12:14:51.920303: step 29629, loss 0.513296.
Train: 2018-08-09T12:14:52.014029: step 29630, loss 0.479946.
Test: 2018-08-09T12:14:52.513913: step 29630, loss 0.549775.
Train: 2018-08-09T12:14:52.623291: step 29631, loss 0.581241.
Train: 2018-08-09T12:14:52.732611: step 29632, loss 0.645773.
Train: 2018-08-09T12:14:52.841990: step 29633, loss 0.47668.
Train: 2018-08-09T12:14:52.936350: step 29634, loss 0.629939.
Train: 2018-08-09T12:14:53.045699: step 29635, loss 0.471185.
Train: 2018-08-09T12:14:53.155079: step 29636, loss 0.508083.
Train: 2018-08-09T12:14:53.264429: step 29637, loss 0.619044.
Train: 2018-08-09T12:14:53.373748: step 29638, loss 0.533458.
Train: 2018-08-09T12:14:53.483127: step 29639, loss 0.563925.
Train: 2018-08-09T12:14:53.576857: step 29640, loss 0.58079.
Test: 2018-08-09T12:14:54.076732: step 29640, loss 0.54794.
Train: 2018-08-09T12:14:54.186086: step 29641, loss 0.509751.
Train: 2018-08-09T12:14:54.295405: step 29642, loss 0.520808.
Train: 2018-08-09T12:14:54.404785: step 29643, loss 0.558636.
Train: 2018-08-09T12:14:54.514133: step 29644, loss 0.526084.
Train: 2018-08-09T12:14:54.623452: step 29645, loss 0.488435.
Train: 2018-08-09T12:14:54.717181: step 29646, loss 0.563526.
Train: 2018-08-09T12:14:54.826563: step 29647, loss 0.632332.
Train: 2018-08-09T12:14:54.936942: step 29648, loss 0.483912.
Train: 2018-08-09T12:14:55.046291: step 29649, loss 0.514887.
Train: 2018-08-09T12:14:55.155671: step 29650, loss 0.53167.
Test: 2018-08-09T12:14:55.639902: step 29650, loss 0.546367.
Train: 2018-08-09T12:14:55.749251: step 29651, loss 0.640083.
Train: 2018-08-09T12:14:55.858600: step 29652, loss 0.612901.
Train: 2018-08-09T12:14:55.952328: step 29653, loss 0.538537.
Train: 2018-08-09T12:14:56.061707: step 29654, loss 0.56427.
Train: 2018-08-09T12:14:56.171026: step 29655, loss 0.455527.
Train: 2018-08-09T12:14:56.280376: step 29656, loss 0.65082.
Train: 2018-08-09T12:14:56.389778: step 29657, loss 0.492446.
Train: 2018-08-09T12:14:56.499074: step 29658, loss 0.51068.
Train: 2018-08-09T12:14:56.608424: step 29659, loss 0.528778.
Train: 2018-08-09T12:14:56.702152: step 29660, loss 0.600301.
Test: 2018-08-09T12:14:57.202034: step 29660, loss 0.549351.
Train: 2018-08-09T12:14:57.305204: step 29661, loss 0.511176.
Train: 2018-08-09T12:14:57.414553: step 29662, loss 0.61697.
Train: 2018-08-09T12:14:57.523933: step 29663, loss 0.56474.
Train: 2018-08-09T12:14:57.633283: step 29664, loss 0.495317.
Train: 2018-08-09T12:14:57.742631: step 29665, loss 0.49642.
Train: 2018-08-09T12:14:57.836359: step 29666, loss 0.580671.
Train: 2018-08-09T12:14:57.945709: step 29667, loss 0.627109.
Train: 2018-08-09T12:14:58.055061: step 29668, loss 0.644463.
Train: 2018-08-09T12:14:58.164407: step 29669, loss 0.562271.
Train: 2018-08-09T12:14:58.273751: step 29670, loss 0.596905.
Test: 2018-08-09T12:14:58.757988: step 29670, loss 0.547613.
Train: 2018-08-09T12:14:58.867360: step 29671, loss 0.545954.
Train: 2018-08-09T12:14:58.976716: step 29672, loss 0.546119.
Train: 2018-08-09T12:14:59.086035: step 29673, loss 0.479559.
Train: 2018-08-09T12:14:59.195414: step 29674, loss 0.480858.
Train: 2018-08-09T12:14:59.304768: step 29675, loss 0.562789.
Train: 2018-08-09T12:14:59.398492: step 29676, loss 0.513332.
Train: 2018-08-09T12:14:59.507840: step 29677, loss 0.71156.
Train: 2018-08-09T12:14:59.617190: step 29678, loss 0.57908.
Train: 2018-08-09T12:14:59.726539: step 29679, loss 0.48046.
Train: 2018-08-09T12:14:59.835882: step 29680, loss 0.546352.
Test: 2018-08-09T12:15:00.321495: step 29680, loss 0.549095.
Train: 2018-08-09T12:15:00.430843: step 29681, loss 0.562004.
Train: 2018-08-09T12:15:00.540192: step 29682, loss 0.563401.
Train: 2018-08-09T12:15:00.633891: step 29683, loss 0.57797.
Train: 2018-08-09T12:15:00.743272: step 29684, loss 0.660184.
Train: 2018-08-09T12:15:00.852619: step 29685, loss 0.579202.
Train: 2018-08-09T12:15:00.961967: step 29686, loss 0.547049.
Train: 2018-08-09T12:15:01.071288: step 29687, loss 0.53001.
Train: 2018-08-09T12:15:01.165015: step 29688, loss 0.513305.
Train: 2018-08-09T12:15:01.274364: step 29689, loss 0.578608.
Train: 2018-08-09T12:15:01.383744: step 29690, loss 0.628421.
Test: 2018-08-09T12:15:01.883596: step 29690, loss 0.548636.
Train: 2018-08-09T12:15:01.994304: step 29691, loss 0.513904.
Train: 2018-08-09T12:15:02.103622: step 29692, loss 0.464843.
Train: 2018-08-09T12:15:02.197380: step 29693, loss 0.481248.
Train: 2018-08-09T12:15:02.306700: step 29694, loss 0.563195.
Train: 2018-08-09T12:15:02.416049: step 29695, loss 0.464698.
Train: 2018-08-09T12:15:02.525397: step 29696, loss 0.513513.
Train: 2018-08-09T12:15:02.634776: step 29697, loss 0.562124.
Train: 2018-08-09T12:15:02.728507: step 29698, loss 0.595171.
Train: 2018-08-09T12:15:02.837854: step 29699, loss 0.644833.
Train: 2018-08-09T12:15:02.947173: step 29700, loss 0.596762.
Test: 2018-08-09T12:15:03.447082: step 29700, loss 0.54877.
Train: 2018-08-09T12:15:03.980433: step 29701, loss 0.479645.
Train: 2018-08-09T12:15:04.089811: step 29702, loss 0.579327.
Train: 2018-08-09T12:15:04.199161: step 29703, loss 0.52859.
Train: 2018-08-09T12:15:04.308510: step 29704, loss 0.628772.
Train: 2018-08-09T12:15:04.417860: step 29705, loss 0.529764.
Train: 2018-08-09T12:15:04.511589: step 29706, loss 0.528578.
Train: 2018-08-09T12:15:04.620938: step 29707, loss 0.512243.
Train: 2018-08-09T12:15:04.730287: step 29708, loss 0.579972.
Train: 2018-08-09T12:15:04.839635: step 29709, loss 0.596617.
Train: 2018-08-09T12:15:04.948953: step 29710, loss 0.527959.
Test: 2018-08-09T12:15:05.448836: step 29710, loss 0.547928.
Train: 2018-08-09T12:15:05.542594: step 29711, loss 0.579114.
Train: 2018-08-09T12:15:05.651912: step 29712, loss 0.495994.
Train: 2018-08-09T12:15:05.761262: step 29713, loss 0.561751.
Train: 2018-08-09T12:15:05.854990: step 29714, loss 0.545585.
Train: 2018-08-09T12:15:05.966594: step 29715, loss 0.545517.
Train: 2018-08-09T12:15:06.075968: step 29716, loss 0.528784.
Train: 2018-08-09T12:15:06.185323: step 29717, loss 0.646745.
Train: 2018-08-09T12:15:06.279053: step 29718, loss 0.697205.
Train: 2018-08-09T12:15:06.388401: step 29719, loss 0.410217.
Train: 2018-08-09T12:15:06.497750: step 29720, loss 0.544288.
Test: 2018-08-09T12:15:06.997603: step 29720, loss 0.548414.
Train: 2018-08-09T12:15:07.091359: step 29721, loss 0.49445.
Train: 2018-08-09T12:15:07.200712: step 29722, loss 0.614034.
Train: 2018-08-09T12:15:07.310058: step 29723, loss 0.544821.
Train: 2018-08-09T12:15:07.403788: step 29724, loss 0.614177.
Train: 2018-08-09T12:15:07.513135: step 29725, loss 0.633764.
Train: 2018-08-09T12:15:07.622484: step 29726, loss 0.56304.
Train: 2018-08-09T12:15:07.731833: step 29727, loss 0.612995.
Train: 2018-08-09T12:15:07.841183: step 29728, loss 0.579521.
Train: 2018-08-09T12:15:07.936221: step 29729, loss 0.612769.
Train: 2018-08-09T12:15:08.045545: step 29730, loss 0.495811.
Test: 2018-08-09T12:15:08.545427: step 29730, loss 0.548875.
Train: 2018-08-09T12:15:08.654809: step 29731, loss 0.59534.
Train: 2018-08-09T12:15:08.748535: step 29732, loss 0.479692.
Train: 2018-08-09T12:15:08.857885: step 29733, loss 0.496554.
Train: 2018-08-09T12:15:08.967233: step 29734, loss 0.529488.
Train: 2018-08-09T12:15:09.060961: step 29735, loss 0.661647.
Train: 2018-08-09T12:15:09.170310: step 29736, loss 0.595579.
Train: 2018-08-09T12:15:09.279630: step 29737, loss 0.579442.
Train: 2018-08-09T12:15:09.389008: step 29738, loss 0.513459.
Train: 2018-08-09T12:15:09.498358: step 29739, loss 0.431175.
Train: 2018-08-09T12:15:09.607678: step 29740, loss 0.496875.
Test: 2018-08-09T12:15:10.108177: step 29740, loss 0.548968.
Train: 2018-08-09T12:15:10.201934: step 29741, loss 0.611885.
Train: 2018-08-09T12:15:10.311253: step 29742, loss 0.612443.
Train: 2018-08-09T12:15:10.420602: step 29743, loss 0.595501.
Train: 2018-08-09T12:15:10.529981: step 29744, loss 0.644634.
Train: 2018-08-09T12:15:10.623679: step 29745, loss 0.529624.
Train: 2018-08-09T12:15:10.733058: step 29746, loss 0.480257.
Train: 2018-08-09T12:15:10.842377: step 29747, loss 0.579034.
Train: 2018-08-09T12:15:10.951726: step 29748, loss 0.496709.
Train: 2018-08-09T12:15:11.061106: step 29749, loss 0.710793.
Train: 2018-08-09T12:15:11.154835: step 29750, loss 0.59546.
Test: 2018-08-09T12:15:11.654700: step 29750, loss 0.549054.
Train: 2018-08-09T12:15:11.764036: step 29751, loss 0.546093.
Train: 2018-08-09T12:15:11.857793: step 29752, loss 0.529751.
Train: 2018-08-09T12:15:11.967143: step 29753, loss 0.529814.
Train: 2018-08-09T12:15:12.076492: step 29754, loss 0.61157.
Train: 2018-08-09T12:15:12.186453: step 29755, loss 0.480897.
Train: 2018-08-09T12:15:12.280150: step 29756, loss 0.61163.
Train: 2018-08-09T12:15:12.389530: step 29757, loss 0.57902.
Train: 2018-08-09T12:15:12.498879: step 29758, loss 0.562686.
Train: 2018-08-09T12:15:12.608199: step 29759, loss 0.578909.
Train: 2018-08-09T12:15:12.717574: step 29760, loss 0.611699.
Test: 2018-08-09T12:15:13.201809: step 29760, loss 0.547963.
Train: 2018-08-09T12:15:13.311157: step 29761, loss 0.513756.
Train: 2018-08-09T12:15:13.420508: step 29762, loss 0.578828.
Train: 2018-08-09T12:15:13.530979: step 29763, loss 0.481268.
Train: 2018-08-09T12:15:13.627896: step 29764, loss 0.546244.
Train: 2018-08-09T12:15:13.737215: step 29765, loss 0.562668.
Train: 2018-08-09T12:15:13.846566: step 29766, loss 0.464668.
Train: 2018-08-09T12:15:13.940323: step 29767, loss 0.644445.
Train: 2018-08-09T12:15:14.049673: step 29768, loss 0.627924.
Train: 2018-08-09T12:15:14.159025: step 29769, loss 0.578986.
Train: 2018-08-09T12:15:14.268371: step 29770, loss 0.5789.
Test: 2018-08-09T12:15:14.768222: step 29770, loss 0.550394.
Train: 2018-08-09T12:15:14.861983: step 29771, loss 0.497361.
Train: 2018-08-09T12:15:14.971299: step 29772, loss 0.677072.
Train: 2018-08-09T12:15:15.080648: step 29773, loss 0.562648.
Train: 2018-08-09T12:15:15.189999: step 29774, loss 0.562685.
Train: 2018-08-09T12:15:15.299348: step 29775, loss 0.69279.
Train: 2018-08-09T12:15:15.393076: step 29776, loss 0.465373.
Train: 2018-08-09T12:15:15.502425: step 29777, loss 0.562674.
Train: 2018-08-09T12:15:15.611774: step 29778, loss 0.498022.
Train: 2018-08-09T12:15:15.721122: step 29779, loss 0.676026.
Train: 2018-08-09T12:15:15.814881: step 29780, loss 0.546583.
Test: 2018-08-09T12:15:16.314758: step 29780, loss 0.547623.
Train: 2018-08-09T12:15:16.424112: step 29781, loss 0.708011.
Train: 2018-08-09T12:15:16.533465: step 29782, loss 0.643237.
Train: 2018-08-09T12:15:16.642780: step 29783, loss 0.482684.
Train: 2018-08-09T12:15:16.752131: step 29784, loss 0.530884.
Train: 2018-08-09T12:15:16.845888: step 29785, loss 0.610799.
Train: 2018-08-09T12:15:16.956596: step 29786, loss 0.546984.
Train: 2018-08-09T12:15:17.065915: step 29787, loss 0.562943.
Train: 2018-08-09T12:15:17.159673: step 29788, loss 0.62656.
Train: 2018-08-09T12:15:17.268992: step 29789, loss 0.547127.
Train: 2018-08-09T12:15:17.378372: step 29790, loss 0.547178.
Test: 2018-08-09T12:15:17.878225: step 29790, loss 0.548801.
Train: 2018-08-09T12:15:17.987603: step 29791, loss 0.483895.
Train: 2018-08-09T12:15:18.081301: step 29792, loss 0.53136.
Train: 2018-08-09T12:15:18.190650: step 29793, loss 0.547168.
Train: 2018-08-09T12:15:18.300030: step 29794, loss 0.562995.
Train: 2018-08-09T12:15:18.409347: step 29795, loss 0.578856.
Train: 2018-08-09T12:15:18.503106: step 29796, loss 0.674224.
Train: 2018-08-09T12:15:18.612424: step 29797, loss 0.610615.
Train: 2018-08-09T12:15:18.721804: step 29798, loss 0.578863.
Train: 2018-08-09T12:15:18.831153: step 29799, loss 0.613705.
Train: 2018-08-09T12:15:18.926172: step 29800, loss 0.499819.
Test: 2018-08-09T12:15:19.426030: step 29800, loss 0.548859.
Train: 2018-08-09T12:15:20.035291: step 29801, loss 0.610458.
Train: 2018-08-09T12:15:20.128988: step 29802, loss 0.594645.
Train: 2018-08-09T12:15:20.238337: step 29803, loss 0.468559.
Train: 2018-08-09T12:15:20.347687: step 29804, loss 0.484298.
Train: 2018-08-09T12:15:20.457067: step 29805, loss 0.499942.
Train: 2018-08-09T12:15:20.550795: step 29806, loss 0.563038.
Train: 2018-08-09T12:15:20.660114: step 29807, loss 0.547141.
Train: 2018-08-09T12:15:20.769462: step 29808, loss 0.515308.
Train: 2018-08-09T12:15:20.878837: step 29809, loss 0.594819.
Train: 2018-08-09T12:15:20.974842: step 29810, loss 0.578881.
Test: 2018-08-09T12:15:21.474725: step 29810, loss 0.550897.
Train: 2018-08-09T12:15:21.584104: step 29811, loss 0.594878.
Train: 2018-08-09T12:15:21.693423: step 29812, loss 0.562853.
Train: 2018-08-09T12:15:21.787151: step 29813, loss 0.546788.
Train: 2018-08-09T12:15:21.896500: step 29814, loss 0.498604.
Train: 2018-08-09T12:15:22.005850: step 29815, loss 0.514506.
Train: 2018-08-09T12:15:22.115199: step 29816, loss 0.659558.
Train: 2018-08-09T12:15:22.208926: step 29817, loss 0.546357.
Train: 2018-08-09T12:15:22.318306: step 29818, loss 0.51423.
Train: 2018-08-09T12:15:22.427625: step 29819, loss 0.578712.
Train: 2018-08-09T12:15:22.537004: step 29820, loss 0.497404.
Test: 2018-08-09T12:15:23.038325: step 29820, loss 0.549053.
Train: 2018-08-09T12:15:23.132051: step 29821, loss 0.579146.
Train: 2018-08-09T12:15:23.241369: step 29822, loss 0.560564.
Train: 2018-08-09T12:15:23.350719: step 29823, loss 0.528669.
Train: 2018-08-09T12:15:23.460100: step 29824, loss 0.479473.
Train: 2018-08-09T12:15:23.553825: step 29825, loss 0.498046.
Train: 2018-08-09T12:15:23.663157: step 29826, loss 0.582276.
Train: 2018-08-09T12:15:23.772524: step 29827, loss 0.542879.
Train: 2018-08-09T12:15:23.881873: step 29828, loss 0.529122.
Train: 2018-08-09T12:15:23.991223: step 29829, loss 0.558167.
Train: 2018-08-09T12:15:24.100542: step 29830, loss 0.510343.
Test: 2018-08-09T12:15:24.600425: step 29830, loss 0.547161.
Train: 2018-08-09T12:15:24.694184: step 29831, loss 0.566748.
Train: 2018-08-09T12:15:24.803531: step 29832, loss 0.493085.
Train: 2018-08-09T12:15:24.915133: step 29833, loss 0.4789.
Train: 2018-08-09T12:15:25.024513: step 29834, loss 0.573557.
Train: 2018-08-09T12:15:25.118243: step 29835, loss 0.596751.
Train: 2018-08-09T12:15:25.227561: step 29836, loss 0.443622.
Train: 2018-08-09T12:15:25.336940: step 29837, loss 0.525907.
Train: 2018-08-09T12:15:25.446289: step 29838, loss 0.503339.
Train: 2018-08-09T12:15:25.555609: step 29839, loss 0.601253.
Train: 2018-08-09T12:15:25.649367: step 29840, loss 0.48713.
Test: 2018-08-09T12:15:26.149243: step 29840, loss 0.548113.
Train: 2018-08-09T12:15:26.258567: step 29841, loss 0.608539.
Train: 2018-08-09T12:15:26.367917: step 29842, loss 0.51071.
Train: 2018-08-09T12:15:26.461675: step 29843, loss 0.608492.
Train: 2018-08-09T12:15:26.571023: step 29844, loss 0.5437.
Train: 2018-08-09T12:15:26.680373: step 29845, loss 0.636073.
Train: 2018-08-09T12:15:26.789722: step 29846, loss 0.512428.
Train: 2018-08-09T12:15:26.899059: step 29847, loss 0.53363.
Train: 2018-08-09T12:15:26.994144: step 29848, loss 0.523376.
Train: 2018-08-09T12:15:27.103526: step 29849, loss 0.740766.
Train: 2018-08-09T12:15:27.212868: step 29850, loss 0.54557.
Test: 2018-08-09T12:15:27.712756: step 29850, loss 0.545729.
Train: 2018-08-09T12:15:27.806484: step 29851, loss 0.512001.
Train: 2018-08-09T12:15:27.915832: step 29852, loss 0.529158.
Train: 2018-08-09T12:15:28.025151: step 29853, loss 0.596009.
Train: 2018-08-09T12:15:28.134530: step 29854, loss 0.495564.
Train: 2018-08-09T12:15:28.290715: step 29855, loss 0.645297.
Train: 2018-08-09T12:15:28.384472: step 29856, loss 0.579777.
Train: 2018-08-09T12:15:28.493790: step 29857, loss 0.545573.
Train: 2018-08-09T12:15:28.603172: step 29858, loss 0.611828.
Train: 2018-08-09T12:15:28.712519: step 29859, loss 0.628383.
Train: 2018-08-09T12:15:28.806247: step 29860, loss 0.49719.
Test: 2018-08-09T12:15:29.312026: step 29860, loss 0.548605.
Train: 2018-08-09T12:15:29.421404: step 29861, loss 0.497477.
Train: 2018-08-09T12:15:29.515133: step 29862, loss 0.546388.
Train: 2018-08-09T12:15:29.624483: step 29863, loss 0.562665.
Train: 2018-08-09T12:15:29.733830: step 29864, loss 0.643937.
Train: 2018-08-09T12:15:29.843150: step 29865, loss 0.627611.
Train: 2018-08-09T12:15:29.952529: step 29866, loss 0.578897.
Train: 2018-08-09T12:15:30.061850: step 29867, loss 0.643565.
Train: 2018-08-09T12:15:30.155607: step 29868, loss 0.595002.
Train: 2018-08-09T12:15:30.264949: step 29869, loss 0.482434.
Train: 2018-08-09T12:15:30.374306: step 29870, loss 0.562823.
Test: 2018-08-09T12:15:30.874157: step 29870, loss 0.550249.
Train: 2018-08-09T12:15:30.980206: step 29871, loss 0.578865.
Train: 2018-08-09T12:15:31.089531: step 29872, loss 0.482849.
Train: 2018-08-09T12:15:31.198916: step 29873, loss 0.594863.
Train: 2018-08-09T12:15:31.304763: step 29874, loss 0.546875.
Train: 2018-08-09T12:15:31.414142: step 29875, loss 0.514893.
Train: 2018-08-09T12:15:31.507873: step 29876, loss 0.530858.
Train: 2018-08-09T12:15:31.617219: step 29877, loss 0.578876.
Train: 2018-08-09T12:15:31.726570: step 29878, loss 0.659018.
Train: 2018-08-09T12:15:31.835918: step 29879, loss 0.610902.
Train: 2018-08-09T12:15:31.945267: step 29880, loss 0.674883.
Test: 2018-08-09T12:15:32.445145: step 29880, loss 0.549752.
Train: 2018-08-09T12:15:32.538877: step 29881, loss 0.515011.
Train: 2018-08-09T12:15:32.648197: step 29882, loss 0.451365.
Train: 2018-08-09T12:15:32.757579: step 29883, loss 0.499142.
Train: 2018-08-09T12:15:32.866925: step 29884, loss 0.51502.
Train: 2018-08-09T12:15:32.976245: step 29885, loss 0.578918.
Train: 2018-08-09T12:15:33.085594: step 29886, loss 0.562865.
Train: 2018-08-09T12:15:33.179352: step 29887, loss 0.578916.
Train: 2018-08-09T12:15:33.288701: step 29888, loss 0.643124.
Train: 2018-08-09T12:15:33.398050: step 29889, loss 0.450499.
Train: 2018-08-09T12:15:33.507402: step 29890, loss 0.57887.
Test: 2018-08-09T12:15:34.009517: step 29890, loss 0.548322.
Train: 2018-08-09T12:15:34.118891: step 29891, loss 0.562786.
Train: 2018-08-09T12:15:34.306321: step 29892, loss 0.594989.
Train: 2018-08-09T12:15:34.415702: step 29893, loss 0.466072.
Train: 2018-08-09T12:15:34.525052: step 29894, loss 0.62731.
Train: 2018-08-09T12:15:34.634398: step 29895, loss 0.578874.
Train: 2018-08-09T12:15:34.728127: step 29896, loss 0.627372.
Train: 2018-08-09T12:15:34.837478: step 29897, loss 0.54659.
Train: 2018-08-09T12:15:34.946826: step 29898, loss 0.659785.
Train: 2018-08-09T12:15:35.056146: step 29899, loss 0.530431.
Train: 2018-08-09T12:15:35.165525: step 29900, loss 0.578876.
Test: 2018-08-09T12:15:35.665378: step 29900, loss 0.548273.
Train: 2018-08-09T12:15:36.229305: step 29901, loss 0.562793.
Train: 2018-08-09T12:15:36.338654: step 29902, loss 0.546702.
Train: 2018-08-09T12:15:36.447972: step 29903, loss 0.627151.
Train: 2018-08-09T12:15:36.557323: step 29904, loss 0.627086.
Train: 2018-08-09T12:15:36.651051: step 29905, loss 0.434476.
Train: 2018-08-09T12:15:36.760399: step 29906, loss 0.610943.
Train: 2018-08-09T12:15:36.869778: step 29907, loss 0.594912.
Train: 2018-08-09T12:15:36.979128: step 29908, loss 0.562843.
Train: 2018-08-09T12:15:37.088479: step 29909, loss 0.594871.
Train: 2018-08-09T12:15:37.197826: step 29910, loss 0.594881.
Test: 2018-08-09T12:15:37.697678: step 29910, loss 0.550341.
Train: 2018-08-09T12:15:37.807029: step 29911, loss 0.467061.
Train: 2018-08-09T12:15:37.900788: step 29912, loss 0.467032.
Train: 2018-08-09T12:15:38.011071: step 29913, loss 0.626884.
Train: 2018-08-09T12:15:38.120389: step 29914, loss 0.642935.
Train: 2018-08-09T12:15:38.229772: step 29915, loss 0.594868.
Train: 2018-08-09T12:15:38.339113: step 29916, loss 0.594838.
Train: 2018-08-09T12:15:38.448468: step 29917, loss 0.594828.
Train: 2018-08-09T12:15:38.557787: step 29918, loss 0.499149.
Train: 2018-08-09T12:15:38.651515: step 29919, loss 0.562912.
Train: 2018-08-09T12:15:38.760865: step 29920, loss 0.515131.
Test: 2018-08-09T12:15:39.260747: step 29920, loss 0.550993.
Train: 2018-08-09T12:15:39.370126: step 29921, loss 0.435372.
Train: 2018-08-09T12:15:39.479475: step 29922, loss 0.578863.
Train: 2018-08-09T12:15:39.588795: step 29923, loss 0.578844.
Train: 2018-08-09T12:15:39.698143: step 29924, loss 0.691206.
Train: 2018-08-09T12:15:39.791902: step 29925, loss 0.514709.
Train: 2018-08-09T12:15:39.901253: step 29926, loss 0.594911.
Train: 2018-08-09T12:15:40.012944: step 29927, loss 0.450495.
Train: 2018-08-09T12:15:40.122294: step 29928, loss 0.530641.
Train: 2018-08-09T12:15:40.231644: step 29929, loss 0.578823.
Train: 2018-08-09T12:15:40.340992: step 29930, loss 0.643393.
Test: 2018-08-09T12:15:40.825225: step 29930, loss 0.549431.
Train: 2018-08-09T12:15:40.950232: step 29931, loss 0.611355.
Train: 2018-08-09T12:15:41.059544: step 29932, loss 0.562718.
Train: 2018-08-09T12:15:41.168917: step 29933, loss 0.530413.
Train: 2018-08-09T12:15:41.278245: step 29934, loss 0.562773.
Train: 2018-08-09T12:15:41.371993: step 29935, loss 0.611031.
Train: 2018-08-09T12:15:41.481349: step 29936, loss 0.692016.
Train: 2018-08-09T12:15:41.590698: step 29937, loss 0.514549.
Train: 2018-08-09T12:15:41.700048: step 29938, loss 0.739662.
Train: 2018-08-09T12:15:41.809396: step 29939, loss 0.514751.
Train: 2018-08-09T12:15:41.921095: step 29940, loss 0.562938.
Test: 2018-08-09T12:15:42.405357: step 29940, loss 0.549187.
Train: 2018-08-09T12:15:42.514706: step 29941, loss 0.62675.
Train: 2018-08-09T12:15:42.624055: step 29942, loss 0.499445.
Train: 2018-08-09T12:15:42.733434: step 29943, loss 0.547129.
Train: 2018-08-09T12:15:42.842783: step 29944, loss 0.642251.
Train: 2018-08-09T12:15:42.952132: step 29945, loss 0.610487.
Train: 2018-08-09T12:15:43.045868: step 29946, loss 0.594639.
Train: 2018-08-09T12:15:43.155209: step 29947, loss 0.531658.
Train: 2018-08-09T12:15:43.264561: step 29948, loss 0.51603.
Train: 2018-08-09T12:15:43.373908: step 29949, loss 0.563174.
Train: 2018-08-09T12:15:43.483257: step 29950, loss 0.610256.
Test: 2018-08-09T12:15:43.983811: step 29950, loss 0.550887.
Train: 2018-08-09T12:15:44.093145: step 29951, loss 0.500504.
Train: 2018-08-09T12:15:44.202464: step 29952, loss 0.594553.
Train: 2018-08-09T12:15:44.296193: step 29953, loss 0.625894.
Train: 2018-08-09T12:15:44.405572: step 29954, loss 0.594537.
Train: 2018-08-09T12:15:44.514923: step 29955, loss 0.547611.
Train: 2018-08-09T12:15:44.624270: step 29956, loss 0.610138.
Train: 2018-08-09T12:15:44.741082: step 29957, loss 0.578892.
Train: 2018-08-09T12:15:44.837041: step 29958, loss 0.688001.
Train: 2018-08-09T12:15:44.946420: step 29959, loss 0.563363.
Train: 2018-08-09T12:15:45.055739: step 29960, loss 0.532397.
Test: 2018-08-09T12:15:45.555623: step 29960, loss 0.550105.
Train: 2018-08-09T12:15:45.665002: step 29961, loss 0.516991.
Train: 2018-08-09T12:15:45.758698: step 29962, loss 0.625353.
Train: 2018-08-09T12:15:45.868048: step 29963, loss 0.609847.
Train: 2018-08-09T12:15:45.977427: step 29964, loss 0.47092.
Train: 2018-08-09T12:15:46.086776: step 29965, loss 0.53264.
Train: 2018-08-09T12:15:46.196096: step 29966, loss 0.625278.
Train: 2018-08-09T12:15:46.298352: step 29967, loss 0.625269.
Train: 2018-08-09T12:15:46.407701: step 29968, loss 0.532651.
Train: 2018-08-09T12:15:46.517022: step 29969, loss 0.548088.
Train: 2018-08-09T12:15:46.626400: step 29970, loss 0.548083.
Test: 2018-08-09T12:15:47.134529: step 29970, loss 0.551945.
Train: 2018-08-09T12:15:47.228273: step 29971, loss 0.532606.
Train: 2018-08-09T12:15:47.337592: step 29972, loss 0.517068.
Train: 2018-08-09T12:15:47.446974: step 29973, loss 0.516947.
Train: 2018-08-09T12:15:47.556290: step 29974, loss 0.594425.
Train: 2018-08-09T12:15:47.665670: step 29975, loss 0.485425.
Train: 2018-08-09T12:15:47.775018: step 29976, loss 0.64145.
Train: 2018-08-09T12:15:47.868746: step 29977, loss 0.625847.
Train: 2018-08-09T12:15:47.978095: step 29978, loss 0.516054.
Train: 2018-08-09T12:15:48.087445: step 29979, loss 0.563087.
Train: 2018-08-09T12:15:48.196797: step 29980, loss 0.579133.
Test: 2018-08-09T12:15:48.696647: step 29980, loss 0.548884.
Train: 2018-08-09T12:15:48.806020: step 29981, loss 0.578884.
Train: 2018-08-09T12:15:48.915374: step 29982, loss 0.562855.
Train: 2018-08-09T12:15:49.009102: step 29983, loss 0.531505.
Train: 2018-08-09T12:15:49.118422: step 29984, loss 0.610232.
Train: 2018-08-09T12:15:49.227801: step 29985, loss 0.483547.
Train: 2018-08-09T12:15:49.337150: step 29986, loss 0.562638.
Train: 2018-08-09T12:15:49.446502: step 29987, loss 0.580466.
Train: 2018-08-09T12:15:49.555819: step 29988, loss 0.514036.
Train: 2018-08-09T12:15:49.649577: step 29989, loss 0.594784.
Train: 2018-08-09T12:15:49.758926: step 29990, loss 0.642426.
Test: 2018-08-09T12:15:50.261211: step 29990, loss 0.55057.
Train: 2018-08-09T12:15:50.370563: step 29991, loss 0.59591.
Train: 2018-08-09T12:15:50.479905: step 29992, loss 0.497905.
Train: 2018-08-09T12:15:50.589258: step 29993, loss 0.610914.
Train: 2018-08-09T12:15:50.682985: step 29994, loss 0.562107.
Train: 2018-08-09T12:15:50.792326: step 29995, loss 0.592354.
Train: 2018-08-09T12:15:50.901686: step 29996, loss 0.562675.
Train: 2018-08-09T12:15:51.011047: step 29997, loss 0.645661.
Train: 2018-08-09T12:15:51.120380: step 29998, loss 0.593968.
Train: 2018-08-09T12:15:51.229699: step 29999, loss 0.433872.
Train: 2018-08-09T12:15:51.323458: step 30000, loss 0.644115.
Test: 2018-08-09T12:15:51.823309: step 30000, loss 0.552367.
Train: 2018-08-09T12:15:52.387394: step 30001, loss 0.61005.
Train: 2018-08-09T12:15:52.496742: step 30002, loss 0.512187.
Train: 2018-08-09T12:15:52.606123: step 30003, loss 0.563356.
Train: 2018-08-09T12:15:52.715466: step 30004, loss 0.545044.
Train: 2018-08-09T12:15:52.824821: step 30005, loss 0.611126.
Train: 2018-08-09T12:15:52.934170: step 30006, loss 0.641568.
Train: 2018-08-09T12:15:53.027867: step 30007, loss 0.59422.
Train: 2018-08-09T12:15:53.137218: step 30008, loss 0.64311.
Train: 2018-08-09T12:15:53.246596: step 30009, loss 0.496417.
Train: 2018-08-09T12:15:53.355946: step 30010, loss 0.49595.
Test: 2018-08-09T12:15:53.855798: step 30010, loss 0.549147.
Train: 2018-08-09T12:15:53.966590: step 30011, loss 0.563935.
Train: 2018-08-09T12:15:54.075937: step 30012, loss 0.541698.
Train: 2018-08-09T12:15:54.169666: step 30013, loss 0.648665.
Train: 2018-08-09T12:15:54.279015: step 30014, loss 0.634438.
Train: 2018-08-09T12:15:54.388365: step 30015, loss 0.545651.
Train: 2018-08-09T12:15:54.497714: step 30016, loss 0.557761.
Train: 2018-08-09T12:15:54.607033: step 30017, loss 0.548205.
Train: 2018-08-09T12:15:54.716381: step 30018, loss 0.496469.
Train: 2018-08-09T12:15:54.825761: step 30019, loss 0.448192.
Train: 2018-08-09T12:15:54.935081: step 30020, loss 0.478708.
Test: 2018-08-09T12:15:55.434964: step 30020, loss 0.548303.
Train: 2018-08-09T12:15:55.528721: step 30021, loss 0.491037.
Train: 2018-08-09T12:15:55.638070: step 30022, loss 0.488574.
Train: 2018-08-09T12:15:55.747388: step 30023, loss 0.673254.
Train: 2018-08-09T12:15:55.856768: step 30024, loss 0.574705.
Train: 2018-08-09T12:15:55.967517: step 30025, loss 0.545123.
Train: 2018-08-09T12:15:56.076836: step 30026, loss 0.480251.
Train: 2018-08-09T12:15:56.170594: step 30027, loss 0.593946.
Train: 2018-08-09T12:15:56.279947: step 30028, loss 0.55525.
Train: 2018-08-09T12:15:56.389294: step 30029, loss 0.447343.
Train: 2018-08-09T12:15:56.498641: step 30030, loss 0.574459.
Test: 2018-08-09T12:15:56.998495: step 30030, loss 0.548458.
Train: 2018-08-09T12:15:57.154707: step 30031, loss 0.574367.
Train: 2018-08-09T12:15:57.264056: step 30032, loss 0.546069.
Train: 2018-08-09T12:15:57.357814: step 30033, loss 0.579772.
Train: 2018-08-09T12:15:57.467163: step 30034, loss 0.584676.
Train: 2018-08-09T12:15:57.576508: step 30035, loss 0.636647.
Train: 2018-08-09T12:15:57.685832: step 30036, loss 0.528208.
Train: 2018-08-09T12:15:57.795211: step 30037, loss 0.547963.
Train: 2018-08-09T12:15:57.913776: step 30038, loss 0.494177.
Train: 2018-08-09T12:15:58.014532: step 30039, loss 0.543053.
Train: 2018-08-09T12:15:58.123883: step 30040, loss 0.596491.
Test: 2018-08-09T12:15:58.623789: step 30040, loss 0.548103.
Train: 2018-08-09T12:15:58.733114: step 30041, loss 0.514329.
Train: 2018-08-09T12:15:58.826872: step 30042, loss 0.494625.
Train: 2018-08-09T12:15:58.936221: step 30043, loss 0.559213.
Train: 2018-08-09T12:15:59.045570: step 30044, loss 0.547486.
Train: 2018-08-09T12:15:59.154919: step 30045, loss 0.567705.
Train: 2018-08-09T12:15:59.264269: step 30046, loss 0.581782.
Train: 2018-08-09T12:15:59.373589: step 30047, loss 0.56403.
Train: 2018-08-09T12:15:59.482969: step 30048, loss 0.546782.
Train: 2018-08-09T12:15:59.592287: step 30049, loss 0.531368.
Train: 2018-08-09T12:15:59.686044: step 30050, loss 0.547859.
Test: 2018-08-09T12:16:00.185921: step 30050, loss 0.547986.
Train: 2018-08-09T12:16:00.295276: step 30051, loss 0.628268.
Train: 2018-08-09T12:16:00.404594: step 30052, loss 0.564059.
Train: 2018-08-09T12:16:00.513975: step 30053, loss 0.545088.
Train: 2018-08-09T12:16:00.623324: step 30054, loss 0.594805.
Train: 2018-08-09T12:16:00.732643: step 30055, loss 0.514384.
Train: 2018-08-09T12:16:00.842022: step 30056, loss 0.595827.
Train: 2018-08-09T12:16:00.937992: step 30057, loss 0.659154.
Train: 2018-08-09T12:16:01.047370: step 30058, loss 0.595877.
Train: 2018-08-09T12:16:01.156720: step 30059, loss 0.642661.
Train: 2018-08-09T12:16:01.266042: step 30060, loss 0.546818.
Test: 2018-08-09T12:16:01.765921: step 30060, loss 0.549335.
Train: 2018-08-09T12:16:01.875271: step 30061, loss 0.515583.
Train: 2018-08-09T12:16:01.984662: step 30062, loss 0.467601.
Train: 2018-08-09T12:16:02.093969: step 30063, loss 0.563473.
Train: 2018-08-09T12:16:02.203318: step 30064, loss 0.594863.
Train: 2018-08-09T12:16:02.312698: step 30065, loss 0.610799.
Train: 2018-08-09T12:16:02.406425: step 30066, loss 0.563031.
Train: 2018-08-09T12:16:02.515774: step 30067, loss 0.578586.
Train: 2018-08-09T12:16:02.625094: step 30068, loss 0.595355.
Train: 2018-08-09T12:16:02.734443: step 30069, loss 0.483754.
Train: 2018-08-09T12:16:02.843792: step 30070, loss 0.562552.
Test: 2018-08-09T12:16:03.339033: step 30070, loss 0.549365.
Train: 2018-08-09T12:16:03.448387: step 30071, loss 0.562701.
Train: 2018-08-09T12:16:03.557737: step 30072, loss 0.530767.
Train: 2018-08-09T12:16:03.667086: step 30073, loss 0.62635.
Train: 2018-08-09T12:16:03.776437: step 30074, loss 0.531085.
Train: 2018-08-09T12:16:03.885755: step 30075, loss 0.610718.
Train: 2018-08-09T12:16:03.985017: step 30076, loss 0.451838.
Train: 2018-08-09T12:16:04.094366: step 30077, loss 0.579494.
Train: 2018-08-09T12:16:04.203715: step 30078, loss 0.658713.
Train: 2018-08-09T12:16:04.313064: step 30079, loss 0.659313.
Train: 2018-08-09T12:16:04.422446: step 30080, loss 0.67449.
Test: 2018-08-09T12:16:04.908905: step 30080, loss 0.548732.
Train: 2018-08-09T12:16:05.018256: step 30081, loss 0.467732.
Train: 2018-08-09T12:16:05.127635: step 30082, loss 0.562871.
Train: 2018-08-09T12:16:05.236985: step 30083, loss 0.563138.
Train: 2018-08-09T12:16:05.346688: step 30084, loss 0.531393.
Train: 2018-08-09T12:16:05.442215: step 30085, loss 0.547183.
Train: 2018-08-09T12:16:05.551565: step 30086, loss 0.64152.
Train: 2018-08-09T12:16:05.660944: step 30087, loss 0.690921.
Train: 2018-08-09T12:16:05.770264: step 30088, loss 0.48399.
Train: 2018-08-09T12:16:05.879642: step 30089, loss 0.515962.
Train: 2018-08-09T12:16:05.973370: step 30090, loss 0.547307.
Test: 2018-08-09T12:16:06.473223: step 30090, loss 0.550074.
Train: 2018-08-09T12:16:06.582573: step 30091, loss 0.610641.
Train: 2018-08-09T12:16:06.691951: step 30092, loss 0.531871.
Train: 2018-08-09T12:16:06.801301: step 30093, loss 0.547391.
Train: 2018-08-09T12:16:06.910619: step 30094, loss 0.579068.
Train: 2018-08-09T12:16:07.019999: step 30095, loss 0.51592.
Train: 2018-08-09T12:16:07.113729: step 30096, loss 0.484137.
Train: 2018-08-09T12:16:07.223046: step 30097, loss 0.578895.
Train: 2018-08-09T12:16:07.332425: step 30098, loss 0.451912.
Train: 2018-08-09T12:16:07.441744: step 30099, loss 0.531241.
Train: 2018-08-09T12:16:07.551094: step 30100, loss 0.529266.
Test: 2018-08-09T12:16:08.050976: step 30100, loss 0.550187.
