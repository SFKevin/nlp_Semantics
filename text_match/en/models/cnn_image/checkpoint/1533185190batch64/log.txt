Train: 2018-08-02T12:46:35.473594: step 1, loss 0.727859.
Train: 2018-08-02T12:46:35.645428: step 2, loss 2.02051.
Train: 2018-08-02T12:46:35.786020: step 3, loss 0.670385.
Train: 2018-08-02T12:46:35.942202: step 4, loss 0.63891.
Train: 2018-08-02T12:46:36.082795: step 5, loss 0.638453.
Train: 2018-08-02T12:46:36.239014: step 6, loss 0.599156.
Train: 2018-08-02T12:46:36.379601: step 7, loss 0.523014.
Train: 2018-08-02T12:46:36.535813: step 8, loss 0.519165.
Train: 2018-08-02T12:46:36.676437: step 9, loss 0.741764.
Train: 2018-08-02T12:46:36.832620: step 10, loss 0.486379.
Test: 2018-08-02T12:46:37.473094: step 10, loss 1.4408.
Train: 2018-08-02T12:46:37.613721: step 11, loss 0.631011.
Train: 2018-08-02T12:46:37.769931: step 12, loss 0.515065.
Train: 2018-08-02T12:46:37.910493: step 13, loss 0.523095.
Train: 2018-08-02T12:46:38.051114: step 14, loss 0.530172.
Train: 2018-08-02T12:46:38.207328: step 15, loss 0.541895.
Train: 2018-08-02T12:46:38.347890: step 16, loss 0.540769.
Train: 2018-08-02T12:46:38.488515: step 17, loss 0.567255.
Train: 2018-08-02T12:46:38.644725: step 18, loss 0.586333.
Train: 2018-08-02T12:46:38.800911: step 19, loss 0.458381.
Train: 2018-08-02T12:46:38.941531: step 20, loss 0.553713.
Test: 2018-08-02T12:46:39.410172: step 20, loss 0.640031.
Train: 2018-08-02T12:46:39.550763: step 21, loss 0.550466.
Train: 2018-08-02T12:46:39.691356: step 22, loss 0.533088.
Train: 2018-08-02T12:46:39.847568: step 23, loss 0.599735.
Train: 2018-08-02T12:46:39.988165: step 24, loss 0.592506.
Train: 2018-08-02T12:46:40.128752: step 25, loss 0.601098.
Train: 2018-08-02T12:46:40.284966: step 26, loss 0.571663.
Train: 2018-08-02T12:46:40.425558: step 27, loss 0.569004.
Train: 2018-08-02T12:46:40.581776: step 28, loss 0.548152.
Train: 2018-08-02T12:46:40.722334: step 29, loss 0.558822.
Train: 2018-08-02T12:46:40.862959: step 30, loss 0.628868.
Test: 2018-08-02T12:46:41.331594: step 30, loss 0.637744.
Train: 2018-08-02T12:46:41.487804: step 31, loss 0.555777.
Train: 2018-08-02T12:46:41.628373: step 32, loss 0.586744.
Train: 2018-08-02T12:46:41.784609: step 33, loss 0.527051.
Train: 2018-08-02T12:46:41.925209: step 34, loss 0.633029.
Train: 2018-08-02T12:46:42.065770: step 35, loss 0.567021.
Train: 2018-08-02T12:46:42.222013: step 36, loss 0.535778.
Train: 2018-08-02T12:46:42.362575: step 37, loss 0.575926.
Train: 2018-08-02T12:46:42.503167: step 38, loss 0.466559.
Train: 2018-08-02T12:46:42.659410: step 39, loss 0.506762.
Train: 2018-08-02T12:46:42.799972: step 40, loss 0.521189.
Test: 2018-08-02T12:46:43.252991: step 40, loss 0.627899.
Train: 2018-08-02T12:46:43.409206: step 41, loss 0.495769.
Train: 2018-08-02T12:46:43.549826: step 42, loss 0.54455.
Train: 2018-08-02T12:46:43.690418: step 43, loss 0.561691.
Train: 2018-08-02T12:46:43.846632: step 44, loss 0.512455.
Train: 2018-08-02T12:46:43.987228: step 45, loss 0.476246.
Train: 2018-08-02T12:46:44.143407: step 46, loss 0.585744.
Train: 2018-08-02T12:46:44.283999: step 47, loss 0.48526.
Train: 2018-08-02T12:46:44.424591: step 48, loss 0.485836.
Train: 2018-08-02T12:46:44.580829: step 49, loss 0.582744.
Train: 2018-08-02T12:46:44.721427: step 50, loss 0.522559.
Test: 2018-08-02T12:46:45.190038: step 50, loss 0.627302.
Train: 2018-08-02T12:46:45.330660: step 51, loss 0.487918.
Train: 2018-08-02T12:46:45.471251: step 52, loss 0.517369.
Train: 2018-08-02T12:46:45.627436: step 53, loss 0.567693.
Train: 2018-08-02T12:46:45.768057: step 54, loss 0.546129.
Train: 2018-08-02T12:46:45.924271: step 55, loss 0.622722.
Train: 2018-08-02T12:46:46.064863: step 56, loss 0.4934.
Train: 2018-08-02T12:46:46.221076: step 57, loss 0.570226.
Train: 2018-08-02T12:46:46.361638: step 58, loss 0.477523.
Train: 2018-08-02T12:46:46.517851: step 59, loss 0.567059.
Train: 2018-08-02T12:46:46.658474: step 60, loss 0.52177.
Test: 2018-08-02T12:46:47.127085: step 60, loss 0.621782.
Train: 2018-08-02T12:46:47.267706: step 61, loss 0.572583.
Train: 2018-08-02T12:46:47.423889: step 62, loss 0.568648.
Train: 2018-08-02T12:46:47.564512: step 63, loss 0.523035.
Train: 2018-08-02T12:46:47.720695: step 64, loss 0.54244.
Train: 2018-08-02T12:46:47.861286: step 65, loss 0.540339.
Train: 2018-08-02T12:46:48.017501: step 66, loss 0.576444.
Train: 2018-08-02T12:46:48.158092: step 67, loss 0.536268.
Train: 2018-08-02T12:46:48.314336: step 68, loss 0.610993.
Train: 2018-08-02T12:46:48.454898: step 69, loss 0.598142.
Train: 2018-08-02T12:46:48.611145: step 70, loss 0.505227.
Test: 2018-08-02T12:46:49.064155: step 70, loss 0.616759.
Train: 2018-08-02T12:46:49.204753: step 71, loss 0.582511.
Train: 2018-08-02T12:46:49.360969: step 72, loss 0.524731.
Train: 2018-08-02T12:46:49.501558: step 73, loss 0.488022.
Train: 2018-08-02T12:46:49.657772: step 74, loss 0.618165.
Train: 2018-08-02T12:46:49.798335: step 75, loss 0.666608.
Train: 2018-08-02T12:46:49.954547: step 76, loss 0.596535.
Train: 2018-08-02T12:46:50.095169: step 77, loss 0.579118.
Train: 2018-08-02T12:46:50.251382: step 78, loss 0.528699.
Train: 2018-08-02T12:46:50.391975: step 79, loss 0.548463.
Train: 2018-08-02T12:46:50.548187: step 80, loss 0.536733.
Test: 2018-08-02T12:46:51.001176: step 80, loss 0.609034.
Train: 2018-08-02T12:46:51.157389: step 81, loss 0.553756.
Train: 2018-08-02T12:46:51.298013: step 82, loss 0.539211.
Train: 2018-08-02T12:46:51.454225: step 83, loss 0.57156.
Train: 2018-08-02T12:46:51.594817: step 84, loss 0.535432.
Train: 2018-08-02T12:46:51.751031: step 85, loss 0.507074.
Train: 2018-08-02T12:46:51.891617: step 86, loss 0.50384.
Train: 2018-08-02T12:46:52.047837: step 87, loss 0.594257.
Train: 2018-08-02T12:46:52.188398: step 88, loss 0.575374.
Train: 2018-08-02T12:46:52.344646: step 89, loss 0.5446.
Train: 2018-08-02T12:46:52.485238: step 90, loss 0.566977.
Test: 2018-08-02T12:46:52.938248: step 90, loss 0.601288.
Train: 2018-08-02T12:46:53.094437: step 91, loss 0.589013.
Train: 2018-08-02T12:46:53.235062: step 92, loss 0.521581.
Train: 2018-08-02T12:46:53.391242: step 93, loss 0.594116.
Train: 2018-08-02T12:46:53.531864: step 94, loss 0.62077.
Train: 2018-08-02T12:46:53.688047: step 95, loss 0.503282.
Train: 2018-08-02T12:46:53.828670: step 96, loss 0.529414.
Train: 2018-08-02T12:46:53.984853: step 97, loss 0.56233.
Train: 2018-08-02T12:46:54.125445: step 98, loss 0.519177.
Train: 2018-08-02T12:46:54.266067: step 99, loss 0.558868.
Train: 2018-08-02T12:46:54.422281: step 100, loss 0.522326.
Test: 2018-08-02T12:46:54.875294: step 100, loss 0.58905.
Train: 2018-08-02T12:46:55.547017: step 101, loss 0.545671.
Train: 2018-08-02T12:46:55.703231: step 102, loss 0.463598.
Train: 2018-08-02T12:46:55.843792: step 103, loss 0.509521.
Train: 2018-08-02T12:46:56.000036: step 104, loss 0.553067.
Train: 2018-08-02T12:46:56.156220: step 105, loss 0.548078.
Train: 2018-08-02T12:46:56.296812: step 106, loss 0.50039.
Train: 2018-08-02T12:46:56.437403: step 107, loss 0.532334.
Train: 2018-08-02T12:46:56.593617: step 108, loss 0.514192.
Train: 2018-08-02T12:46:56.749855: step 109, loss 0.64629.
Train: 2018-08-02T12:46:56.890423: step 110, loss 0.562204.
Test: 2018-08-02T12:46:57.343467: step 110, loss 0.591178.
Train: 2018-08-02T12:46:57.499685: step 111, loss 0.557156.
Train: 2018-08-02T12:46:57.640277: step 112, loss 0.591036.
Train: 2018-08-02T12:46:57.780869: step 113, loss 0.544111.
Train: 2018-08-02T12:46:57.937053: step 114, loss 0.612468.
Train: 2018-08-02T12:46:58.093300: step 115, loss 0.55673.
Train: 2018-08-02T12:46:58.233891: step 116, loss 0.525263.
Train: 2018-08-02T12:46:58.390101: step 117, loss 0.581399.
Train: 2018-08-02T12:46:58.530694: step 118, loss 0.531202.
Train: 2018-08-02T12:46:58.686876: step 119, loss 0.622679.
Train: 2018-08-02T12:46:58.827499: step 120, loss 0.481517.
Test: 2018-08-02T12:46:59.280489: step 120, loss 0.591819.
Train: 2018-08-02T12:46:59.436731: step 121, loss 0.544967.
Train: 2018-08-02T12:46:59.592948: step 122, loss 0.602488.
Train: 2018-08-02T12:46:59.733540: step 123, loss 0.490967.
Train: 2018-08-02T12:46:59.889750: step 124, loss 0.555547.
Train: 2018-08-02T12:47:00.030313: step 125, loss 0.532475.
Train: 2018-08-02T12:47:00.186556: step 126, loss 0.487717.
Train: 2018-08-02T12:47:00.327118: step 127, loss 0.588177.
Train: 2018-08-02T12:47:00.483331: step 128, loss 0.509939.
Train: 2018-08-02T12:47:00.623923: step 129, loss 0.478938.
Train: 2018-08-02T12:47:00.780136: step 130, loss 0.557817.
Test: 2018-08-02T12:47:01.233157: step 130, loss 0.570713.
Train: 2018-08-02T12:47:01.389399: step 131, loss 0.469633.
Train: 2018-08-02T12:47:01.529962: step 132, loss 0.619352.
Train: 2018-08-02T12:47:01.686205: step 133, loss 0.558014.
Train: 2018-08-02T12:47:01.826798: step 134, loss 0.549056.
Train: 2018-08-02T12:47:01.983014: step 135, loss 0.569764.
Train: 2018-08-02T12:47:02.139195: step 136, loss 0.553328.
Train: 2018-08-02T12:47:02.279817: step 137, loss 0.556128.
Train: 2018-08-02T12:47:02.436031: step 138, loss 0.58598.
Train: 2018-08-02T12:47:02.576591: step 139, loss 0.527004.
Train: 2018-08-02T12:47:02.732838: step 140, loss 0.566232.
Test: 2018-08-02T12:47:03.185823: step 140, loss 0.566066.
Train: 2018-08-02T12:47:03.342067: step 141, loss 0.558959.
Train: 2018-08-02T12:47:03.482630: step 142, loss 0.549599.
Train: 2018-08-02T12:47:03.638843: step 143, loss 0.514152.
Train: 2018-08-02T12:47:03.795085: step 144, loss 0.491234.
Train: 2018-08-02T12:47:03.935647: step 145, loss 0.47957.
Train: 2018-08-02T12:47:04.091861: step 146, loss 0.515419.
Train: 2018-08-02T12:47:04.248105: step 147, loss 0.528693.
Train: 2018-08-02T12:47:04.388667: step 148, loss 0.511031.
Train: 2018-08-02T12:47:04.544910: step 149, loss 0.562891.
Train: 2018-08-02T12:47:04.685502: step 150, loss 0.567177.
Test: 2018-08-02T12:47:05.138493: step 150, loss 0.560174.
Train: 2018-08-02T12:47:05.435321: step 151, loss 0.461879.
Train: 2018-08-02T12:47:05.591539: step 152, loss 0.526357.
Train: 2018-08-02T12:47:05.732136: step 153, loss 0.600436.
Train: 2018-08-02T12:47:05.888346: step 154, loss 0.526486.
Train: 2018-08-02T12:47:06.028938: step 155, loss 0.531146.
Train: 2018-08-02T12:47:06.185151: step 156, loss 0.561431.
Train: 2018-08-02T12:47:06.341364: step 157, loss 0.539531.
Train: 2018-08-02T12:47:06.481960: step 158, loss 0.507645.
Train: 2018-08-02T12:47:06.638139: step 159, loss 0.495582.
Train: 2018-08-02T12:47:06.778762: step 160, loss 0.5592.
Test: 2018-08-02T12:47:07.247373: step 160, loss 0.550873.
Train: 2018-08-02T12:47:07.387963: step 161, loss 0.608673.
Train: 2018-08-02T12:47:07.544203: step 162, loss 0.51292.
Train: 2018-08-02T12:47:07.700422: step 163, loss 0.561415.
Train: 2018-08-02T12:47:07.841013: step 164, loss 0.561423.
Train: 2018-08-02T12:47:07.997196: step 165, loss 0.559122.
Train: 2018-08-02T12:47:08.153411: step 166, loss 0.593414.
Train: 2018-08-02T12:47:08.294032: step 167, loss 0.569012.
Train: 2018-08-02T12:47:08.450240: step 168, loss 0.587617.
Train: 2018-08-02T12:47:08.590808: step 169, loss 0.573555.
Train: 2018-08-02T12:47:08.747054: step 170, loss 0.581673.
Test: 2018-08-02T12:47:09.200065: step 170, loss 0.590044.
Train: 2018-08-02T12:47:09.356307: step 171, loss 0.547132.
Train: 2018-08-02T12:47:09.512497: step 172, loss 0.550188.
Train: 2018-08-02T12:47:09.653058: step 173, loss 0.540177.
Train: 2018-08-02T12:47:09.809303: step 174, loss 0.53256.
Train: 2018-08-02T12:47:09.949895: step 175, loss 0.53017.
Train: 2018-08-02T12:47:10.106112: step 176, loss 0.535314.
Train: 2018-08-02T12:47:10.246700: step 177, loss 0.516921.
Train: 2018-08-02T12:47:10.402914: step 178, loss 0.523724.
Train: 2018-08-02T12:47:10.559127: step 179, loss 0.514268.
Train: 2018-08-02T12:47:10.699688: step 180, loss 0.492144.
Test: 2018-08-02T12:47:11.168354: step 180, loss 0.569084.
Train: 2018-08-02T12:47:11.308951: step 181, loss 0.572475.
Train: 2018-08-02T12:47:11.465164: step 182, loss 0.51131.
Train: 2018-08-02T12:47:11.605760: step 183, loss 0.537707.
Train: 2018-08-02T12:47:11.761941: step 184, loss 0.582548.
Train: 2018-08-02T12:47:11.902562: step 185, loss 0.431127.
Train: 2018-08-02T12:47:12.058776: step 186, loss 0.495771.
Train: 2018-08-02T12:47:12.214989: step 187, loss 0.509085.
Train: 2018-08-02T12:47:12.355585: step 188, loss 0.432808.
Train: 2018-08-02T12:47:12.511797: step 189, loss 0.625264.
Train: 2018-08-02T12:47:12.652356: step 190, loss 0.515716.
Test: 2018-08-02T12:47:13.121022: step 190, loss 0.550932.
Train: 2018-08-02T12:47:13.261619: step 191, loss 0.582095.
Train: 2018-08-02T12:47:13.417834: step 192, loss 0.516216.
Train: 2018-08-02T12:47:13.558424: step 193, loss 0.570812.
Train: 2018-08-02T12:47:13.714609: step 194, loss 0.533637.
Train: 2018-08-02T12:47:13.870822: step 195, loss 0.524754.
Train: 2018-08-02T12:47:14.011444: step 196, loss 0.529756.
Train: 2018-08-02T12:47:14.167656: step 197, loss 0.602721.
Train: 2018-08-02T12:47:14.308249: step 198, loss 0.585438.
Train: 2018-08-02T12:47:14.464465: step 199, loss 0.530829.
Train: 2018-08-02T12:47:14.605049: step 200, loss 0.4945.
Test: 2018-08-02T12:47:15.073697: step 200, loss 0.550627.
Train: 2018-08-02T12:47:15.682927: step 201, loss 0.551677.
Train: 2018-08-02T12:47:15.839140: step 202, loss 0.526074.
Train: 2018-08-02T12:47:15.995354: step 203, loss 0.585863.
Train: 2018-08-02T12:47:16.135946: step 204, loss 0.566157.
Train: 2018-08-02T12:47:16.292128: step 205, loss 0.564031.
Train: 2018-08-02T12:47:16.432751: step 206, loss 0.517805.
Train: 2018-08-02T12:47:16.588965: step 207, loss 0.606262.
Train: 2018-08-02T12:47:16.745179: step 208, loss 0.511295.
Train: 2018-08-02T12:47:16.885770: step 209, loss 0.560734.
Train: 2018-08-02T12:47:17.041985: step 210, loss 0.561988.
Test: 2018-08-02T12:47:17.494977: step 210, loss 0.56166.
Train: 2018-08-02T12:47:17.651210: step 211, loss 0.555498.
Train: 2018-08-02T12:47:17.807400: step 212, loss 0.512598.
Train: 2018-08-02T12:47:17.948022: step 213, loss 0.530858.
Train: 2018-08-02T12:47:18.104204: step 214, loss 0.486889.
Train: 2018-08-02T12:47:18.244797: step 215, loss 0.500238.
Train: 2018-08-02T12:47:18.401010: step 216, loss 0.531426.
Train: 2018-08-02T12:47:18.541633: step 217, loss 0.582684.
Train: 2018-08-02T12:47:18.697816: step 218, loss 0.534326.
Train: 2018-08-02T12:47:18.854028: step 219, loss 0.507101.
Train: 2018-08-02T12:47:18.994621: step 220, loss 0.485247.
Test: 2018-08-02T12:47:19.463262: step 220, loss 0.548183.
Train: 2018-08-02T12:47:19.603887: step 221, loss 0.542029.
Train: 2018-08-02T12:47:19.760101: step 222, loss 0.516612.
Train: 2018-08-02T12:47:19.900689: step 223, loss 0.48803.
Train: 2018-08-02T12:47:20.056903: step 224, loss 0.54718.
Train: 2018-08-02T12:47:20.213085: step 225, loss 0.641164.
Train: 2018-08-02T12:47:20.353712: step 226, loss 0.50816.
Train: 2018-08-02T12:47:20.509921: step 227, loss 0.551713.
Train: 2018-08-02T12:47:20.650483: step 228, loss 0.521436.
Train: 2018-08-02T12:47:20.806698: step 229, loss 0.578508.
Train: 2018-08-02T12:47:20.947289: step 230, loss 0.579412.
Test: 2018-08-02T12:47:21.415929: step 230, loss 0.552547.
Train: 2018-08-02T12:47:21.556552: step 231, loss 0.571982.
Train: 2018-08-02T12:47:21.712766: step 232, loss 0.585522.
Train: 2018-08-02T12:47:21.853327: step 233, loss 0.532096.
Train: 2018-08-02T12:47:22.009571: step 234, loss 0.607397.
Train: 2018-08-02T12:47:22.165784: step 235, loss 0.531186.
Train: 2018-08-02T12:47:22.306376: step 236, loss 0.528083.
Train: 2018-08-02T12:47:22.462591: step 237, loss 0.562013.
Train: 2018-08-02T12:47:22.603185: step 238, loss 0.456901.
Train: 2018-08-02T12:47:22.759366: step 239, loss 0.537688.
Train: 2018-08-02T12:47:22.915608: step 240, loss 0.517933.
Test: 2018-08-02T12:47:23.368597: step 240, loss 0.548107.
Train: 2018-08-02T12:47:23.524810: step 241, loss 0.503658.
Train: 2018-08-02T12:47:23.665432: step 242, loss 0.612535.
Train: 2018-08-02T12:47:23.821617: step 243, loss 0.579806.
Train: 2018-08-02T12:47:23.977863: step 244, loss 0.532246.
Train: 2018-08-02T12:47:24.118422: step 245, loss 0.544518.
Train: 2018-08-02T12:47:24.274666: step 246, loss 0.504454.
Train: 2018-08-02T12:47:24.415257: step 247, loss 0.508524.
Train: 2018-08-02T12:47:24.571471: step 248, loss 0.548916.
Train: 2018-08-02T12:47:24.727653: step 249, loss 0.5746.
Train: 2018-08-02T12:47:24.868246: step 250, loss 0.60389.
Test: 2018-08-02T12:47:25.336886: step 250, loss 0.554186.
Train: 2018-08-02T12:47:25.477512: step 251, loss 0.591339.
Train: 2018-08-02T12:47:25.633693: step 252, loss 0.50807.
Train: 2018-08-02T12:47:25.774283: step 253, loss 0.534571.
Train: 2018-08-02T12:47:25.930531: step 254, loss 0.526355.
Train: 2018-08-02T12:47:26.086711: step 255, loss 0.535445.
Train: 2018-08-02T12:47:26.242954: step 256, loss 0.573352.
Train: 2018-08-02T12:47:26.383547: step 257, loss 0.508441.
Train: 2018-08-02T12:47:26.539760: step 258, loss 0.525106.
Train: 2018-08-02T12:47:26.695943: step 259, loss 0.567116.
Train: 2018-08-02T12:47:26.836566: step 260, loss 0.502465.
Test: 2018-08-02T12:47:27.289580: step 260, loss 0.546742.
Train: 2018-08-02T12:47:27.445797: step 261, loss 0.561465.
Train: 2018-08-02T12:47:27.602015: step 262, loss 0.629951.
Train: 2018-08-02T12:47:27.742603: step 263, loss 0.526184.
Train: 2018-08-02T12:47:27.898816: step 264, loss 0.578293.
Train: 2018-08-02T12:47:28.055030: step 265, loss 0.585697.
Train: 2018-08-02T12:47:28.195621: step 266, loss 0.476322.
Train: 2018-08-02T12:47:28.351838: step 267, loss 0.585646.
Train: 2018-08-02T12:47:28.492398: step 268, loss 0.563091.
Train: 2018-08-02T12:47:28.648640: step 269, loss 0.529464.
Train: 2018-08-02T12:47:28.789232: step 270, loss 0.482753.
Test: 2018-08-02T12:47:29.242222: step 270, loss 0.551877.
Train: 2018-08-02T12:47:29.398435: step 271, loss 0.550715.
Train: 2018-08-02T12:47:29.554678: step 272, loss 0.604115.
Train: 2018-08-02T12:47:29.695270: step 273, loss 0.615102.
Train: 2018-08-02T12:47:29.851455: step 274, loss 0.596929.
Train: 2018-08-02T12:47:30.007668: step 275, loss 0.49786.
Train: 2018-08-02T12:47:30.148289: step 276, loss 0.521368.
Train: 2018-08-02T12:47:30.304503: step 277, loss 0.552934.
Train: 2018-08-02T12:47:30.460688: step 278, loss 0.522375.
Train: 2018-08-02T12:47:30.601312: step 279, loss 0.523706.
Train: 2018-08-02T12:47:30.757523: step 280, loss 0.523337.
Test: 2018-08-02T12:47:31.210537: step 280, loss 0.550019.
Train: 2018-08-02T12:47:31.366758: step 281, loss 0.513423.
Train: 2018-08-02T12:47:31.507347: step 282, loss 0.542405.
Train: 2018-08-02T12:47:31.663561: step 283, loss 0.499607.
Train: 2018-08-02T12:47:31.819749: step 284, loss 0.537384.
Train: 2018-08-02T12:47:31.960365: step 285, loss 0.513466.
Train: 2018-08-02T12:47:32.116549: step 286, loss 0.533512.
Train: 2018-08-02T12:47:32.257141: step 287, loss 0.459555.
Train: 2018-08-02T12:47:32.413384: step 288, loss 0.482212.
Train: 2018-08-02T12:47:32.569598: step 289, loss 0.577231.
Train: 2018-08-02T12:47:32.710159: step 290, loss 0.549873.
Test: 2018-08-02T12:47:33.178800: step 290, loss 0.550339.
Train: 2018-08-02T12:47:33.319392: step 291, loss 0.53177.
Train: 2018-08-02T12:47:33.475639: step 292, loss 0.526081.
Train: 2018-08-02T12:47:33.631850: step 293, loss 0.542673.
Train: 2018-08-02T12:47:33.772441: step 294, loss 0.481165.
Train: 2018-08-02T12:47:33.928624: step 295, loss 0.500687.
Train: 2018-08-02T12:47:34.084868: step 296, loss 0.487438.
Train: 2018-08-02T12:47:34.241051: step 297, loss 0.52682.
Train: 2018-08-02T12:47:34.381673: step 298, loss 0.422902.
Train: 2018-08-02T12:47:34.537889: step 299, loss 0.526205.
Train: 2018-08-02T12:47:34.678478: step 300, loss 0.541725.
Test: 2018-08-02T12:47:35.147120: step 300, loss 0.55022.
Train: 2018-08-02T12:47:35.756351: step 301, loss 0.444865.
Train: 2018-08-02T12:47:35.865702: step 302, loss 0.565868.
Train: 2018-08-02T12:47:36.021914: step 303, loss 0.472541.
Train: 2018-08-02T12:47:36.178127: step 304, loss 0.561771.
Train: 2018-08-02T12:47:36.318719: step 305, loss 0.529021.
Train: 2018-08-02T12:47:36.474904: step 306, loss 0.529576.
Train: 2018-08-02T12:47:36.631146: step 307, loss 0.600229.
Train: 2018-08-02T12:47:36.787330: step 308, loss 0.567899.
Train: 2018-08-02T12:47:36.927922: step 309, loss 0.513982.
Train: 2018-08-02T12:47:37.084136: step 310, loss 0.605687.
Test: 2018-08-02T12:47:37.537155: step 310, loss 0.559758.
Train: 2018-08-02T12:47:37.693397: step 311, loss 0.507308.
Train: 2018-08-02T12:47:37.833989: step 312, loss 0.580933.
Train: 2018-08-02T12:47:37.990205: step 313, loss 0.54535.
Train: 2018-08-02T12:47:38.130795: step 314, loss 0.530966.
Train: 2018-08-02T12:47:38.287009: step 315, loss 0.625051.
Train: 2018-08-02T12:47:38.443222: step 316, loss 0.506759.
Train: 2018-08-02T12:47:38.599432: step 317, loss 0.472286.
Train: 2018-08-02T12:47:38.755650: step 318, loss 0.567209.
Train: 2018-08-02T12:47:38.896241: step 319, loss 0.529965.
Train: 2018-08-02T12:47:39.052423: step 320, loss 0.500136.
Test: 2018-08-02T12:47:39.505444: step 320, loss 0.548912.
Train: 2018-08-02T12:47:39.661687: step 321, loss 0.527355.
Train: 2018-08-02T12:47:39.817900: step 322, loss 0.565233.
Train: 2018-08-02T12:47:39.958491: step 323, loss 0.499669.
Train: 2018-08-02T12:47:40.114706: step 324, loss 0.585799.
Train: 2018-08-02T12:47:40.270919: step 325, loss 0.566312.
Train: 2018-08-02T12:47:40.427132: step 326, loss 0.494214.
Train: 2018-08-02T12:47:40.583316: step 327, loss 0.494556.
Train: 2018-08-02T12:47:40.739528: step 328, loss 0.57601.
Train: 2018-08-02T12:47:40.880151: step 329, loss 0.541814.
Train: 2018-08-02T12:47:41.036365: step 330, loss 0.54866.
Test: 2018-08-02T12:47:41.489379: step 330, loss 0.552123.
Train: 2018-08-02T12:47:41.645598: step 331, loss 0.583169.
Train: 2018-08-02T12:47:41.801810: step 332, loss 0.530345.
Train: 2018-08-02T12:47:41.942373: step 333, loss 0.538467.
Train: 2018-08-02T12:47:42.098619: step 334, loss 0.515956.
Train: 2018-08-02T12:47:42.254829: step 335, loss 0.499511.
Train: 2018-08-02T12:47:42.395421: step 336, loss 0.480127.
Train: 2018-08-02T12:47:42.551605: step 337, loss 0.586558.
Train: 2018-08-02T12:47:42.707848: step 338, loss 0.615787.
Train: 2018-08-02T12:47:42.848444: step 339, loss 0.553229.
Train: 2018-08-02T12:47:43.004648: step 340, loss 0.540676.
Test: 2018-08-02T12:47:43.473264: step 340, loss 0.563038.
Train: 2018-08-02T12:47:43.613887: step 341, loss 0.600932.
Train: 2018-08-02T12:47:43.770099: step 342, loss 0.561862.
Train: 2018-08-02T12:47:43.910693: step 343, loss 0.532908.
Train: 2018-08-02T12:47:44.066875: step 344, loss 0.547461.
Train: 2018-08-02T12:47:44.223089: step 345, loss 0.529698.
Train: 2018-08-02T12:47:44.363711: step 346, loss 0.45312.
Train: 2018-08-02T12:47:44.519924: step 347, loss 0.503629.
Train: 2018-08-02T12:47:44.676107: step 348, loss 0.530121.
Train: 2018-08-02T12:47:44.816700: step 349, loss 0.489821.
Train: 2018-08-02T12:47:44.972947: step 350, loss 0.587394.
Test: 2018-08-02T12:47:45.425963: step 350, loss 0.549125.
Train: 2018-08-02T12:47:45.582171: step 351, loss 0.529769.
Train: 2018-08-02T12:47:45.738389: step 352, loss 0.553547.
Train: 2018-08-02T12:47:45.878951: step 353, loss 0.48041.
Train: 2018-08-02T12:47:46.035194: step 354, loss 0.5604.
Train: 2018-08-02T12:47:46.191378: step 355, loss 0.551434.
Train: 2018-08-02T12:47:46.332001: step 356, loss 0.486251.
Train: 2018-08-02T12:47:46.488214: step 357, loss 0.532634.
Train: 2018-08-02T12:47:46.644427: step 358, loss 0.501194.
Train: 2018-08-02T12:47:46.784989: step 359, loss 0.456489.
Train: 2018-08-02T12:47:46.941232: step 360, loss 0.629583.
Test: 2018-08-02T12:47:47.394246: step 360, loss 0.548746.
Train: 2018-08-02T12:47:47.550465: step 361, loss 0.541158.
Train: 2018-08-02T12:47:47.706682: step 362, loss 0.518284.
Train: 2018-08-02T12:47:47.847239: step 363, loss 0.482615.
Train: 2018-08-02T12:47:48.003453: step 364, loss 0.567236.
Train: 2018-08-02T12:47:48.144076: step 365, loss 0.485471.
Train: 2018-08-02T12:47:48.300259: step 366, loss 0.494918.
Train: 2018-08-02T12:47:48.456496: step 367, loss 0.428961.
Train: 2018-08-02T12:47:48.597098: step 368, loss 0.431806.
Train: 2018-08-02T12:47:48.753307: step 369, loss 0.488026.
Train: 2018-08-02T12:47:48.893869: step 370, loss 0.544647.
Test: 2018-08-02T12:47:49.362541: step 370, loss 0.556353.
Train: 2018-08-02T12:47:49.503101: step 371, loss 0.615484.
Train: 2018-08-02T12:47:49.659316: step 372, loss 0.51014.
Train: 2018-08-02T12:47:49.815558: step 373, loss 0.494944.
Train: 2018-08-02T12:47:49.956151: step 374, loss 0.546844.
Train: 2018-08-02T12:47:50.112335: step 375, loss 0.582315.
Train: 2018-08-02T12:47:50.268578: step 376, loss 0.486477.
Train: 2018-08-02T12:47:50.409170: step 377, loss 0.491529.
Train: 2018-08-02T12:47:50.565354: step 378, loss 0.565444.
Train: 2018-08-02T12:47:50.721567: step 379, loss 0.532685.
Train: 2018-08-02T12:47:50.862161: step 380, loss 0.59503.
Test: 2018-08-02T12:47:51.330802: step 380, loss 0.550562.
Train: 2018-08-02T12:47:51.471392: step 381, loss 0.521246.
Train: 2018-08-02T12:47:51.627630: step 382, loss 0.536787.
Train: 2018-08-02T12:47:51.768228: step 383, loss 0.507172.
Train: 2018-08-02T12:47:51.924410: step 384, loss 0.552429.
Train: 2018-08-02T12:47:52.080624: step 385, loss 0.512174.
Train: 2018-08-02T12:47:52.221216: step 386, loss 0.463598.
Train: 2018-08-02T12:47:52.377459: step 387, loss 0.505135.
Train: 2018-08-02T12:47:52.533672: step 388, loss 0.433066.
Train: 2018-08-02T12:47:52.689858: step 389, loss 0.519116.
Train: 2018-08-02T12:47:52.830478: step 390, loss 0.565769.
Test: 2018-08-02T12:47:53.283516: step 390, loss 0.54874.
Train: 2018-08-02T12:47:53.439681: step 391, loss 0.546564.
Train: 2018-08-02T12:47:53.595893: step 392, loss 0.506497.
Train: 2018-08-02T12:47:53.736485: step 393, loss 0.68809.
Train: 2018-08-02T12:47:53.892733: step 394, loss 0.487403.
Train: 2018-08-02T12:47:54.033322: step 395, loss 0.50273.
Train: 2018-08-02T12:47:54.189535: step 396, loss 0.529661.
Train: 2018-08-02T12:47:54.345752: step 397, loss 0.581977.
Train: 2018-08-02T12:47:54.486341: step 398, loss 0.525587.
Train: 2018-08-02T12:47:54.642554: step 399, loss 0.583633.
Train: 2018-08-02T12:47:54.798736: step 400, loss 0.554835.
Test: 2018-08-02T12:47:55.251787: step 400, loss 0.551494.
Train: 2018-08-02T12:47:55.876639: step 401, loss 0.577926.
Train: 2018-08-02T12:47:56.032854: step 402, loss 0.509969.
Train: 2018-08-02T12:47:56.189066: step 403, loss 0.566312.
Train: 2018-08-02T12:47:56.329628: step 404, loss 0.569783.
Train: 2018-08-02T12:47:56.485841: step 405, loss 0.505587.
Train: 2018-08-02T12:47:56.642085: step 406, loss 0.574657.
Train: 2018-08-02T12:47:56.782684: step 407, loss 0.538221.
Train: 2018-08-02T12:47:56.938891: step 408, loss 0.582797.
Train: 2018-08-02T12:47:57.095075: step 409, loss 0.494682.
Train: 2018-08-02T12:47:57.235697: step 410, loss 0.522048.
Test: 2018-08-02T12:47:57.688692: step 410, loss 0.548595.
Train: 2018-08-02T12:47:57.844932: step 411, loss 0.521902.
Train: 2018-08-02T12:47:57.985521: step 412, loss 0.528998.
Train: 2018-08-02T12:47:58.141734: step 413, loss 0.618.
Train: 2018-08-02T12:47:58.297917: step 414, loss 0.529373.
Train: 2018-08-02T12:47:58.438534: step 415, loss 0.55462.
Train: 2018-08-02T12:47:58.594756: step 416, loss 0.556784.
Train: 2018-08-02T12:47:58.750968: step 417, loss 0.595038.
Train: 2018-08-02T12:47:58.891562: step 418, loss 0.489165.
Train: 2018-08-02T12:47:59.047775: step 419, loss 0.523733.
Train: 2018-08-02T12:47:59.203985: step 420, loss 0.509587.
Test: 2018-08-02T12:47:59.656974: step 420, loss 0.547143.
Train: 2018-08-02T12:47:59.797596: step 421, loss 0.539384.
Train: 2018-08-02T12:47:59.953812: step 422, loss 0.459571.
Train: 2018-08-02T12:48:00.110023: step 423, loss 0.552505.
Train: 2018-08-02T12:48:00.250615: step 424, loss 0.639099.
Train: 2018-08-02T12:48:00.406829: step 425, loss 0.525151.
Train: 2018-08-02T12:48:00.563014: step 426, loss 0.588969.
Train: 2018-08-02T12:48:00.703604: step 427, loss 0.535912.
Train: 2018-08-02T12:48:00.859817: step 428, loss 0.524724.
Train: 2018-08-02T12:48:01.000440: step 429, loss 0.536764.
Train: 2018-08-02T12:48:01.156653: step 430, loss 0.549499.
Test: 2018-08-02T12:48:01.609642: step 430, loss 0.550177.
Train: 2018-08-02T12:48:01.765886: step 431, loss 0.499955.
Train: 2018-08-02T12:48:01.922071: step 432, loss 0.531701.
Train: 2018-08-02T12:48:02.062685: step 433, loss 0.579651.
Train: 2018-08-02T12:48:02.218874: step 434, loss 0.602876.
Train: 2018-08-02T12:48:02.375118: step 435, loss 0.527483.
Train: 2018-08-02T12:48:02.515710: step 436, loss 0.545142.
Train: 2018-08-02T12:48:02.671923: step 437, loss 0.550556.
Train: 2018-08-02T12:48:02.812515: step 438, loss 0.550189.
Train: 2018-08-02T12:48:02.968725: step 439, loss 0.54165.
Train: 2018-08-02T12:48:03.124943: step 440, loss 0.648646.
Test: 2018-08-02T12:48:03.577956: step 440, loss 0.559162.
Train: 2018-08-02T12:48:03.734174: step 441, loss 0.505556.
Train: 2018-08-02T12:48:03.874737: step 442, loss 0.563737.
Train: 2018-08-02T12:48:04.030980: step 443, loss 0.591062.
Train: 2018-08-02T12:48:04.187193: step 444, loss 0.502998.
Train: 2018-08-02T12:48:04.327786: step 445, loss 0.54923.
Train: 2018-08-02T12:48:04.483968: step 446, loss 0.541675.
Train: 2018-08-02T12:48:04.624585: step 447, loss 0.537416.
Train: 2018-08-02T12:48:04.780804: step 448, loss 0.577209.
Train: 2018-08-02T12:48:04.937018: step 449, loss 0.492989.
Train: 2018-08-02T12:48:05.093202: step 450, loss 0.586087.
Test: 2018-08-02T12:48:05.546220: step 450, loss 0.54915.
Train: 2018-08-02T12:48:05.702463: step 451, loss 0.532083.
Train: 2018-08-02T12:48:05.843059: step 452, loss 0.553579.
Train: 2018-08-02T12:48:05.952405: step 453, loss 0.449912.
Train: 2018-08-02T12:48:06.108621: step 454, loss 0.515702.
Train: 2018-08-02T12:48:06.264802: step 455, loss 0.502474.
Train: 2018-08-02T12:48:06.405424: step 456, loss 0.587834.
Train: 2018-08-02T12:48:06.561638: step 457, loss 0.500544.
Train: 2018-08-02T12:48:06.702229: step 458, loss 0.524042.
Train: 2018-08-02T12:48:06.858443: step 459, loss 0.541469.
Train: 2018-08-02T12:48:07.014657: step 460, loss 0.435775.
Test: 2018-08-02T12:48:07.467670: step 460, loss 0.551122.
Train: 2018-08-02T12:48:07.623889: step 461, loss 0.473128.
Train: 2018-08-02T12:48:07.764450: step 462, loss 0.452132.
Train: 2018-08-02T12:48:07.920665: step 463, loss 0.587881.
Train: 2018-08-02T12:48:08.061281: step 464, loss 0.428399.
Train: 2018-08-02T12:48:08.217470: step 465, loss 0.580878.
Train: 2018-08-02T12:48:08.373713: step 466, loss 0.606051.
Train: 2018-08-02T12:48:08.514306: step 467, loss 0.51712.
Train: 2018-08-02T12:48:08.670489: step 468, loss 0.524689.
Train: 2018-08-02T12:48:08.826732: step 469, loss 0.545521.
Train: 2018-08-02T12:48:08.967328: step 470, loss 0.555428.
Test: 2018-08-02T12:48:09.420313: step 470, loss 0.56468.
Train: 2018-08-02T12:48:09.576560: step 471, loss 0.533849.
Train: 2018-08-02T12:48:09.732739: step 472, loss 0.560594.
Train: 2018-08-02T12:48:09.873362: step 473, loss 0.573537.
Train: 2018-08-02T12:48:10.029544: step 474, loss 0.470959.
Train: 2018-08-02T12:48:10.185768: step 475, loss 0.44992.
Train: 2018-08-02T12:48:10.326380: step 476, loss 0.59203.
Train: 2018-08-02T12:48:10.482594: step 477, loss 0.506067.
Train: 2018-08-02T12:48:10.638808: step 478, loss 0.463681.
Train: 2018-08-02T12:48:10.779369: step 479, loss 0.532161.
Train: 2018-08-02T12:48:10.935584: step 480, loss 0.487508.
Test: 2018-08-02T12:48:11.388602: step 480, loss 0.551137.
Train: 2018-08-02T12:48:11.544846: step 481, loss 0.492375.
Train: 2018-08-02T12:48:11.685407: step 482, loss 0.529549.
Train: 2018-08-02T12:48:11.841622: step 483, loss 0.555034.
Train: 2018-08-02T12:48:11.997868: step 484, loss 0.568856.
Train: 2018-08-02T12:48:12.138457: step 485, loss 0.543264.
Train: 2018-08-02T12:48:12.294670: step 486, loss 0.529466.
Train: 2018-08-02T12:48:12.450865: step 487, loss 0.533471.
Train: 2018-08-02T12:48:12.591479: step 488, loss 0.515761.
Train: 2018-08-02T12:48:12.747689: step 489, loss 0.613898.
Train: 2018-08-02T12:48:12.888282: step 490, loss 0.566268.
Test: 2018-08-02T12:48:13.341309: step 490, loss 0.553527.
Train: 2018-08-02T12:48:13.497482: step 491, loss 0.501175.
Train: 2018-08-02T12:48:13.638105: step 492, loss 0.556066.
Train: 2018-08-02T12:48:13.794290: step 493, loss 0.544262.
Train: 2018-08-02T12:48:13.950503: step 494, loss 0.494827.
Train: 2018-08-02T12:48:14.091094: step 495, loss 0.490061.
Train: 2018-08-02T12:48:14.247338: step 496, loss 0.511685.
Train: 2018-08-02T12:48:14.403554: step 497, loss 0.51477.
Train: 2018-08-02T12:48:14.544113: step 498, loss 0.485804.
Train: 2018-08-02T12:48:14.700326: step 499, loss 0.502248.
Train: 2018-08-02T12:48:14.840918: step 500, loss 0.487262.
Test: 2018-08-02T12:48:15.309584: step 500, loss 0.548457.
Train: 2018-08-02T12:48:15.934442: step 501, loss 0.474365.
Train: 2018-08-02T12:48:16.090659: step 502, loss 0.47823.
Train: 2018-08-02T12:48:16.231217: step 503, loss 0.538761.
Train: 2018-08-02T12:48:16.387456: step 504, loss 0.493322.
Train: 2018-08-02T12:48:16.543674: step 505, loss 0.563758.
Train: 2018-08-02T12:48:16.699882: step 506, loss 0.608236.
Train: 2018-08-02T12:48:16.840484: step 507, loss 0.525672.
Train: 2018-08-02T12:48:16.996694: step 508, loss 0.548695.
Train: 2018-08-02T12:48:17.137257: step 509, loss 0.541573.
Train: 2018-08-02T12:48:17.293500: step 510, loss 0.519967.
Test: 2018-08-02T12:48:17.746489: step 510, loss 0.55367.
Train: 2018-08-02T12:48:17.902703: step 511, loss 0.527628.
Train: 2018-08-02T12:48:18.058946: step 512, loss 0.496716.
Train: 2018-08-02T12:48:18.199537: step 513, loss 0.576077.
Train: 2018-08-02T12:48:18.355721: step 514, loss 0.472898.
Train: 2018-08-02T12:48:18.511935: step 515, loss 0.595818.
Train: 2018-08-02T12:48:18.652526: step 516, loss 0.607194.
Train: 2018-08-02T12:48:18.808770: step 517, loss 0.542945.
Train: 2018-08-02T12:48:18.949331: step 518, loss 0.529118.
Train: 2018-08-02T12:48:19.105577: step 519, loss 0.531813.
Train: 2018-08-02T12:48:19.246167: step 520, loss 0.537595.
Test: 2018-08-02T12:48:19.714811: step 520, loss 0.559824.
Train: 2018-08-02T12:48:19.855400: step 521, loss 0.519503.
Train: 2018-08-02T12:48:20.011614: step 522, loss 0.532811.
Train: 2018-08-02T12:48:20.152205: step 523, loss 0.483988.
Train: 2018-08-02T12:48:20.308418: step 524, loss 0.567517.
Train: 2018-08-02T12:48:20.464601: step 525, loss 0.493106.
Train: 2018-08-02T12:48:20.605194: step 526, loss 0.576074.
Train: 2018-08-02T12:48:20.761437: step 527, loss 0.513431.
Train: 2018-08-02T12:48:20.917650: step 528, loss 0.565891.
Train: 2018-08-02T12:48:21.058243: step 529, loss 0.614905.
Train: 2018-08-02T12:48:21.214427: step 530, loss 0.553069.
Test: 2018-08-02T12:48:21.667470: step 530, loss 0.570184.
Train: 2018-08-02T12:48:21.823688: step 531, loss 0.596989.
Train: 2018-08-02T12:48:21.979903: step 532, loss 0.515996.
Train: 2018-08-02T12:48:22.120463: step 533, loss 0.538338.
Train: 2018-08-02T12:48:22.276707: step 534, loss 0.52963.
Train: 2018-08-02T12:48:22.417299: step 535, loss 0.551888.
Train: 2018-08-02T12:48:22.573484: step 536, loss 0.484293.
Train: 2018-08-02T12:48:22.729726: step 537, loss 0.509758.
Train: 2018-08-02T12:48:22.870318: step 538, loss 0.574594.
Train: 2018-08-02T12:48:23.026526: step 539, loss 0.552831.
Train: 2018-08-02T12:48:23.182740: step 540, loss 0.52555.
Test: 2018-08-02T12:48:23.635734: step 540, loss 0.548409.
Train: 2018-08-02T12:48:23.791978: step 541, loss 0.5404.
Train: 2018-08-02T12:48:23.932570: step 542, loss 0.523117.
Train: 2018-08-02T12:48:24.088754: step 543, loss 0.532322.
Train: 2018-08-02T12:48:24.244997: step 544, loss 0.589757.
Train: 2018-08-02T12:48:24.385590: step 545, loss 0.51576.
Train: 2018-08-02T12:48:24.541802: step 546, loss 0.63351.
Train: 2018-08-02T12:48:24.682394: step 547, loss 0.560967.
Train: 2018-08-02T12:48:24.838577: step 548, loss 0.537534.
Train: 2018-08-02T12:48:24.994824: step 549, loss 0.484152.
Train: 2018-08-02T12:48:25.135415: step 550, loss 0.550884.
Test: 2018-08-02T12:48:25.604023: step 550, loss 0.552535.
Train: 2018-08-02T12:48:25.744646: step 551, loss 0.48559.
Train: 2018-08-02T12:48:25.900829: step 552, loss 0.536422.
Train: 2018-08-02T12:48:26.057066: step 553, loss 0.534247.
Train: 2018-08-02T12:48:26.197665: step 554, loss 0.446866.
Train: 2018-08-02T12:48:26.353848: step 555, loss 0.510151.
Train: 2018-08-02T12:48:26.494470: step 556, loss 0.536068.
Train: 2018-08-02T12:48:26.650652: step 557, loss 0.463358.
Train: 2018-08-02T12:48:26.806866: step 558, loss 0.528147.
Train: 2018-08-02T12:48:26.947489: step 559, loss 0.554783.
Train: 2018-08-02T12:48:27.103702: step 560, loss 0.45905.
Test: 2018-08-02T12:48:27.556692: step 560, loss 0.546911.
Train: 2018-08-02T12:48:27.712929: step 561, loss 0.525453.
Train: 2018-08-02T12:48:27.869118: step 562, loss 0.499373.
Train: 2018-08-02T12:48:28.009734: step 563, loss 0.569408.
Train: 2018-08-02T12:48:28.165956: step 564, loss 0.530885.
Train: 2018-08-02T12:48:28.322167: step 565, loss 0.537989.
Train: 2018-08-02T12:48:28.462763: step 566, loss 0.544329.
Train: 2018-08-02T12:48:28.618974: step 567, loss 0.524012.
Train: 2018-08-02T12:48:28.775186: step 568, loss 0.553625.
Train: 2018-08-02T12:48:28.915777: step 569, loss 0.522233.
Train: 2018-08-02T12:48:29.071993: step 570, loss 0.550435.
Test: 2018-08-02T12:48:29.524983: step 570, loss 0.549833.
Train: 2018-08-02T12:48:29.681193: step 571, loss 0.478582.
Train: 2018-08-02T12:48:29.837408: step 572, loss 0.549213.
Train: 2018-08-02T12:48:29.977999: step 573, loss 0.505421.
Train: 2018-08-02T12:48:30.134242: step 574, loss 0.547382.
Train: 2018-08-02T12:48:30.274804: step 575, loss 0.502857.
Train: 2018-08-02T12:48:30.431048: step 576, loss 0.575812.
Train: 2018-08-02T12:48:30.587265: step 577, loss 0.57552.
Train: 2018-08-02T12:48:30.727853: step 578, loss 0.475929.
Train: 2018-08-02T12:48:30.884067: step 579, loss 0.549948.
Train: 2018-08-02T12:48:31.055904: step 580, loss 0.491849.
Test: 2018-08-02T12:48:31.508909: step 580, loss 0.547862.
Train: 2018-08-02T12:48:31.712001: step 581, loss 0.531412.
Train: 2018-08-02T12:48:31.868212: step 582, loss 0.530995.
Train: 2018-08-02T12:48:32.024395: step 583, loss 0.509025.
Train: 2018-08-02T12:48:32.165016: step 584, loss 0.535074.
Train: 2018-08-02T12:48:32.321230: step 585, loss 0.52949.
Train: 2018-08-02T12:48:32.477439: step 586, loss 0.560041.
Train: 2018-08-02T12:48:32.618039: step 587, loss 0.475138.
Train: 2018-08-02T12:48:32.774249: step 588, loss 0.511148.
Train: 2018-08-02T12:48:32.914841: step 589, loss 0.483643.
Train: 2018-08-02T12:48:33.071054: step 590, loss 0.440373.
Test: 2018-08-02T12:48:33.524044: step 590, loss 0.554445.
Train: 2018-08-02T12:48:33.680286: step 591, loss 0.503825.
Train: 2018-08-02T12:48:33.836504: step 592, loss 0.492946.
Train: 2018-08-02T12:48:33.977092: step 593, loss 0.486295.
Train: 2018-08-02T12:48:34.133276: step 594, loss 0.65622.
Train: 2018-08-02T12:48:34.273898: step 595, loss 0.474544.
Train: 2018-08-02T12:48:34.430111: step 596, loss 0.493566.
Train: 2018-08-02T12:48:34.586328: step 597, loss 0.577094.
Train: 2018-08-02T12:48:34.726917: step 598, loss 0.538343.
Train: 2018-08-02T12:48:34.883100: step 599, loss 0.5733.
Train: 2018-08-02T12:48:35.039344: step 600, loss 0.55537.
Test: 2018-08-02T12:48:35.492357: step 600, loss 0.557991.
Train: 2018-08-02T12:48:36.148429: step 601, loss 0.551167.
Train: 2018-08-02T12:48:36.289051: step 602, loss 0.621351.
Train: 2018-08-02T12:48:36.445234: step 603, loss 0.524037.
Train: 2018-08-02T12:48:36.554583: step 604, loss 0.539287.
Train: 2018-08-02T12:48:36.710796: step 605, loss 0.527414.
Train: 2018-08-02T12:48:36.851420: step 606, loss 0.498465.
Train: 2018-08-02T12:48:37.007604: step 607, loss 0.526887.
Train: 2018-08-02T12:48:37.163816: step 608, loss 0.505837.
Train: 2018-08-02T12:48:37.320030: step 609, loss 0.531676.
Train: 2018-08-02T12:48:37.460655: step 610, loss 0.616714.
Test: 2018-08-02T12:48:37.929262: step 610, loss 0.552413.
Train: 2018-08-02T12:48:38.069854: step 611, loss 0.564521.
Train: 2018-08-02T12:48:38.226097: step 612, loss 0.528371.
Train: 2018-08-02T12:48:38.366690: step 613, loss 0.538207.
Train: 2018-08-02T12:48:38.522903: step 614, loss 0.521178.
Train: 2018-08-02T12:48:38.679116: step 615, loss 0.511781.
Train: 2018-08-02T12:48:38.835333: step 616, loss 0.537149.
Train: 2018-08-02T12:48:38.975917: step 617, loss 0.532503.
Train: 2018-08-02T12:48:39.147753: step 618, loss 0.544666.
Train: 2018-08-02T12:48:39.288318: step 619, loss 0.54618.
Train: 2018-08-02T12:48:39.444562: step 620, loss 0.575584.
Test: 2018-08-02T12:48:39.897551: step 620, loss 0.548175.
Train: 2018-08-02T12:48:40.053789: step 621, loss 0.536399.
Train: 2018-08-02T12:48:40.210012: step 622, loss 0.603382.
Train: 2018-08-02T12:48:40.350570: step 623, loss 0.598533.
Train: 2018-08-02T12:48:40.506813: step 624, loss 0.533389.
Train: 2018-08-02T12:48:40.662997: step 625, loss 0.543089.
Train: 2018-08-02T12:48:40.803620: step 626, loss 0.504169.
Train: 2018-08-02T12:48:40.959836: step 627, loss 0.522778.
Train: 2018-08-02T12:48:41.116041: step 628, loss 0.579678.
Train: 2018-08-02T12:48:41.256638: step 629, loss 0.494783.
Train: 2018-08-02T12:48:41.412850: step 630, loss 0.473528.
Test: 2018-08-02T12:48:41.865840: step 630, loss 0.548142.
Train: 2018-08-02T12:48:42.022083: step 631, loss 0.576302.
Train: 2018-08-02T12:48:42.178296: step 632, loss 0.557493.
Train: 2018-08-02T12:48:42.318889: step 633, loss 0.523029.
Train: 2018-08-02T12:48:42.475102: step 634, loss 0.541053.
Train: 2018-08-02T12:48:42.631316: step 635, loss 0.523012.
Train: 2018-08-02T12:48:42.787530: step 636, loss 0.512535.
Train: 2018-08-02T12:48:42.928091: step 637, loss 0.50355.
Train: 2018-08-02T12:48:43.084338: step 638, loss 0.52388.
Train: 2018-08-02T12:48:43.224897: step 639, loss 0.610256.
Train: 2018-08-02T12:48:43.381140: step 640, loss 0.56262.
Test: 2018-08-02T12:48:43.849760: step 640, loss 0.552936.
Train: 2018-08-02T12:48:43.990343: step 641, loss 0.575981.
Train: 2018-08-02T12:48:44.146586: step 642, loss 0.495981.
Train: 2018-08-02T12:48:44.287178: step 643, loss 0.534147.
Train: 2018-08-02T12:48:44.443362: step 644, loss 0.506266.
Train: 2018-08-02T12:48:44.583987: step 645, loss 0.576174.
Train: 2018-08-02T12:48:44.740196: step 646, loss 0.429176.
Train: 2018-08-02T12:48:44.896380: step 647, loss 0.530735.
Train: 2018-08-02T12:48:45.052594: step 648, loss 0.465953.
Train: 2018-08-02T12:48:45.193216: step 649, loss 0.421543.
Train: 2018-08-02T12:48:45.349430: step 650, loss 0.493012.
Test: 2018-08-02T12:48:45.802444: step 650, loss 0.563146.
Train: 2018-08-02T12:48:45.943041: step 651, loss 0.440569.
Train: 2018-08-02T12:48:46.099268: step 652, loss 0.494926.
Train: 2018-08-02T12:48:46.255468: step 653, loss 0.594919.
Train: 2018-08-02T12:48:46.396030: step 654, loss 0.542978.
Train: 2018-08-02T12:48:46.552273: step 655, loss 0.561339.
Train: 2018-08-02T12:48:46.708489: step 656, loss 0.55316.
Train: 2018-08-02T12:48:46.849082: step 657, loss 0.540612.
Train: 2018-08-02T12:48:47.005263: step 658, loss 0.558138.
Train: 2018-08-02T12:48:47.161476: step 659, loss 0.552567.
Train: 2018-08-02T12:48:47.317690: step 660, loss 0.536147.
Test: 2018-08-02T12:48:47.770709: step 660, loss 0.559756.
Train: 2018-08-02T12:48:47.911329: step 661, loss 0.534072.
Train: 2018-08-02T12:48:48.067543: step 662, loss 0.563952.
Train: 2018-08-02T12:48:48.208105: step 663, loss 0.518389.
Train: 2018-08-02T12:48:48.364318: step 664, loss 0.506938.
Train: 2018-08-02T12:48:48.520561: step 665, loss 0.460909.
Train: 2018-08-02T12:48:48.676745: step 666, loss 0.469057.
Train: 2018-08-02T12:48:48.817367: step 667, loss 0.457935.
Train: 2018-08-02T12:48:48.973580: step 668, loss 0.613149.
Train: 2018-08-02T12:48:49.129794: step 669, loss 0.493887.
Train: 2018-08-02T12:48:49.270355: step 670, loss 0.511859.
Test: 2018-08-02T12:48:49.723375: step 670, loss 0.547129.
Train: 2018-08-02T12:48:49.879588: step 671, loss 0.4992.
Train: 2018-08-02T12:48:50.035831: step 672, loss 0.591629.
Train: 2018-08-02T12:48:50.192046: step 673, loss 0.540455.
Train: 2018-08-02T12:48:50.332641: step 674, loss 0.50998.
Train: 2018-08-02T12:48:50.488851: step 675, loss 0.536672.
Train: 2018-08-02T12:48:50.629443: step 676, loss 0.541419.
Train: 2018-08-02T12:48:50.785626: step 677, loss 0.570845.
Train: 2018-08-02T12:48:50.941869: step 678, loss 0.596854.
Train: 2018-08-02T12:48:51.082465: step 679, loss 0.468082.
Train: 2018-08-02T12:48:51.238679: step 680, loss 0.528717.
Test: 2018-08-02T12:48:51.691676: step 680, loss 0.549415.
Train: 2018-08-02T12:48:51.847907: step 681, loss 0.535673.
Train: 2018-08-02T12:48:51.988500: step 682, loss 0.505392.
Train: 2018-08-02T12:48:52.144683: step 683, loss 0.555507.
Train: 2018-08-02T12:48:52.300897: step 684, loss 0.530612.
Train: 2018-08-02T12:48:52.441490: step 685, loss 0.500345.
Train: 2018-08-02T12:48:52.597702: step 686, loss 0.57659.
Train: 2018-08-02T12:48:52.753945: step 687, loss 0.552279.
Train: 2018-08-02T12:48:52.910128: step 688, loss 0.54883.
Train: 2018-08-02T12:48:53.050751: step 689, loss 0.552523.
Train: 2018-08-02T12:48:53.206933: step 690, loss 0.446139.
Test: 2018-08-02T12:48:53.659978: step 690, loss 0.549089.
Train: 2018-08-02T12:48:53.816197: step 691, loss 0.490157.
Train: 2018-08-02T12:48:53.972410: step 692, loss 0.562717.
Train: 2018-08-02T12:48:54.112971: step 693, loss 0.589846.
Train: 2018-08-02T12:48:54.269219: step 694, loss 0.56093.
Train: 2018-08-02T12:48:54.425429: step 695, loss 0.514674.
Train: 2018-08-02T12:48:54.581613: step 696, loss 0.596793.
Train: 2018-08-02T12:48:54.722234: step 697, loss 0.558357.
Train: 2018-08-02T12:48:54.878448: step 698, loss 0.506869.
Train: 2018-08-02T12:48:55.019041: step 699, loss 0.550999.
Train: 2018-08-02T12:48:55.175255: step 700, loss 0.554222.
Test: 2018-08-02T12:48:55.628267: step 700, loss 0.549293.
Train: 2018-08-02T12:48:56.268747: step 701, loss 0.504003.
Train: 2018-08-02T12:48:56.424930: step 702, loss 0.54151.
Train: 2018-08-02T12:48:56.581144: step 703, loss 0.538324.
Train: 2018-08-02T12:48:56.737388: step 704, loss 0.485695.
Train: 2018-08-02T12:48:56.877950: step 705, loss 0.509198.
Train: 2018-08-02T12:48:57.034193: step 706, loss 0.609522.
Train: 2018-08-02T12:48:57.190377: step 707, loss 0.529266.
Train: 2018-08-02T12:48:57.331002: step 708, loss 0.528692.
Train: 2018-08-02T12:48:57.487182: step 709, loss 0.52433.
Train: 2018-08-02T12:48:57.643396: step 710, loss 0.581963.
Test: 2018-08-02T12:48:58.096440: step 710, loss 0.552688.
Train: 2018-08-02T12:48:58.252658: step 711, loss 0.511017.
Train: 2018-08-02T12:48:58.408871: step 712, loss 0.593042.
Train: 2018-08-02T12:48:58.549466: step 713, loss 0.52435.
Train: 2018-08-02T12:48:58.705680: step 714, loss 0.528178.
Train: 2018-08-02T12:48:58.861889: step 715, loss 0.4536.
Train: 2018-08-02T12:48:59.002482: step 716, loss 0.577528.
Train: 2018-08-02T12:48:59.158666: step 717, loss 0.577584.
Train: 2018-08-02T12:48:59.314909: step 718, loss 0.496883.
Train: 2018-08-02T12:48:59.455504: step 719, loss 0.553257.
Train: 2018-08-02T12:48:59.611718: step 720, loss 0.507664.
Test: 2018-08-02T12:49:00.064703: step 720, loss 0.550292.
Train: 2018-08-02T12:49:00.220947: step 721, loss 0.530761.
Train: 2018-08-02T12:49:00.377129: step 722, loss 0.558548.
Train: 2018-08-02T12:49:00.517753: step 723, loss 0.520225.
Train: 2018-08-02T12:49:00.673967: step 724, loss 0.438816.
Train: 2018-08-02T12:49:00.830182: step 725, loss 0.478898.
Train: 2018-08-02T12:49:00.986363: step 726, loss 0.627481.
Train: 2018-08-02T12:49:01.126985: step 727, loss 0.555002.
Train: 2018-08-02T12:49:01.283199: step 728, loss 0.435004.
Train: 2018-08-02T12:49:01.439382: step 729, loss 0.548669.
Train: 2018-08-02T12:49:01.579974: step 730, loss 0.491944.
Test: 2018-08-02T12:49:02.048624: step 730, loss 0.547309.
Train: 2018-08-02T12:49:02.189236: step 731, loss 0.495344.
Train: 2018-08-02T12:49:02.345450: step 732, loss 0.441107.
Train: 2018-08-02T12:49:02.486012: step 733, loss 0.522272.
Train: 2018-08-02T12:49:02.642255: step 734, loss 0.530279.
Train: 2018-08-02T12:49:02.798438: step 735, loss 0.588202.
Train: 2018-08-02T12:49:02.954685: step 736, loss 0.575762.
Train: 2018-08-02T12:49:03.095274: step 737, loss 0.469814.
Train: 2018-08-02T12:49:03.251487: step 738, loss 0.518818.
Train: 2018-08-02T12:49:03.407700: step 739, loss 0.505098.
Train: 2018-08-02T12:49:03.548262: step 740, loss 0.533955.
Test: 2018-08-02T12:49:04.016903: step 740, loss 0.5493.
Train: 2018-08-02T12:49:04.157525: step 741, loss 0.541929.
Train: 2018-08-02T12:49:04.313709: step 742, loss 0.536496.
Train: 2018-08-02T12:49:04.469952: step 743, loss 0.513139.
Train: 2018-08-02T12:49:04.610513: step 744, loss 0.540441.
Train: 2018-08-02T12:49:04.766757: step 745, loss 0.486953.
Train: 2018-08-02T12:49:04.938586: step 746, loss 0.567499.
Train: 2018-08-02T12:49:05.079154: step 747, loss 0.492334.
Train: 2018-08-02T12:49:05.235397: step 748, loss 0.54694.
Train: 2018-08-02T12:49:05.375989: step 749, loss 0.534023.
Train: 2018-08-02T12:49:05.532203: step 750, loss 0.525563.
Test: 2018-08-02T12:49:05.985217: step 750, loss 0.549356.
Train: 2018-08-02T12:49:06.141436: step 751, loss 0.547685.
Train: 2018-08-02T12:49:06.297650: step 752, loss 0.506101.
Train: 2018-08-02T12:49:06.438241: step 753, loss 0.585588.
Train: 2018-08-02T12:49:06.594455: step 754, loss 0.575971.
Train: 2018-08-02T12:49:06.703773: step 755, loss 0.565789.
Train: 2018-08-02T12:49:06.844365: step 756, loss 0.589182.
Train: 2018-08-02T12:49:07.000579: step 757, loss 0.492051.
Train: 2018-08-02T12:49:07.156793: step 758, loss 0.519035.
Train: 2018-08-02T12:49:07.313033: step 759, loss 0.546914.
Train: 2018-08-02T12:49:07.453599: step 760, loss 0.538174.
Test: 2018-08-02T12:49:07.906618: step 760, loss 0.549023.
Train: 2018-08-02T12:49:08.062831: step 761, loss 0.520974.
Train: 2018-08-02T12:49:08.219074: step 762, loss 0.495971.
Train: 2018-08-02T12:49:08.359666: step 763, loss 0.555879.
Train: 2018-08-02T12:49:08.515879: step 764, loss 0.543275.
Train: 2018-08-02T12:49:08.672062: step 765, loss 0.524071.
Train: 2018-08-02T12:49:08.812685: step 766, loss 0.522925.
Train: 2018-08-02T12:49:08.968898: step 767, loss 0.504416.
Train: 2018-08-02T12:49:09.125081: step 768, loss 0.542028.
Train: 2018-08-02T12:49:09.265703: step 769, loss 0.538675.
Train: 2018-08-02T12:49:09.421917: step 770, loss 0.578559.
Test: 2018-08-02T12:49:09.874906: step 770, loss 0.549021.
Train: 2018-08-02T12:49:10.031150: step 771, loss 0.530371.
Train: 2018-08-02T12:49:10.187363: step 772, loss 0.472431.
Train: 2018-08-02T12:49:10.343573: step 773, loss 0.500112.
Train: 2018-08-02T12:49:10.484139: step 774, loss 0.516604.
Train: 2018-08-02T12:49:10.640381: step 775, loss 0.552384.
Train: 2018-08-02T12:49:10.796595: step 776, loss 0.559067.
Train: 2018-08-02T12:49:10.937158: step 777, loss 0.511491.
Train: 2018-08-02T12:49:11.093401: step 778, loss 0.585293.
Train: 2018-08-02T12:49:11.245031: step 779, loss 0.55199.
Train: 2018-08-02T12:49:11.385593: step 780, loss 0.521535.
Test: 2018-08-02T12:49:11.854234: step 780, loss 0.552222.
Train: 2018-08-02T12:49:11.994856: step 781, loss 0.520223.
Train: 2018-08-02T12:49:12.151069: step 782, loss 0.583885.
Train: 2018-08-02T12:49:12.291632: step 783, loss 0.522375.
Train: 2018-08-02T12:49:12.447874: step 784, loss 0.525289.
Train: 2018-08-02T12:49:12.604059: step 785, loss 0.52586.
Train: 2018-08-02T12:49:12.760301: step 786, loss 0.522864.
Train: 2018-08-02T12:49:12.900864: step 787, loss 0.538618.
Train: 2018-08-02T12:49:13.057110: step 788, loss 0.553898.
Train: 2018-08-02T12:49:13.213320: step 789, loss 0.586335.
Train: 2018-08-02T12:49:13.353882: step 790, loss 0.544612.
Test: 2018-08-02T12:49:13.822547: step 790, loss 0.54857.
Train: 2018-08-02T12:49:13.963146: step 791, loss 0.537459.
Train: 2018-08-02T12:49:14.119359: step 792, loss 0.493229.
Train: 2018-08-02T12:49:14.275542: step 793, loss 0.492856.
Train: 2018-08-02T12:49:14.416166: step 794, loss 0.520515.
Train: 2018-08-02T12:49:14.572377: step 795, loss 0.558583.
Train: 2018-08-02T12:49:14.728560: step 796, loss 0.511542.
Train: 2018-08-02T12:49:14.869183: step 797, loss 0.510874.
Train: 2018-08-02T12:49:15.025396: step 798, loss 0.484587.
Train: 2018-08-02T12:49:15.181579: step 799, loss 0.479963.
Train: 2018-08-02T12:49:15.322202: step 800, loss 0.559816.
Test: 2018-08-02T12:49:15.775191: step 800, loss 0.549063.
Train: 2018-08-02T12:49:16.415667: step 801, loss 0.529723.
Train: 2018-08-02T12:49:16.556288: step 802, loss 0.473722.
Train: 2018-08-02T12:49:16.712501: step 803, loss 0.59313.
Train: 2018-08-02T12:49:16.868685: step 804, loss 0.533131.
Train: 2018-08-02T12:49:17.009307: step 805, loss 0.551208.
Train: 2018-08-02T12:49:17.165520: step 806, loss 0.544737.
Train: 2018-08-02T12:49:17.321733: step 807, loss 0.562823.
Train: 2018-08-02T12:49:17.462296: step 808, loss 0.588828.
Train: 2018-08-02T12:49:17.618539: step 809, loss 0.443394.
Train: 2018-08-02T12:49:17.774746: step 810, loss 0.439073.
Test: 2018-08-02T12:49:18.227742: step 810, loss 0.54777.
Train: 2018-08-02T12:49:18.383985: step 811, loss 0.555079.
Train: 2018-08-02T12:49:18.524576: step 812, loss 0.491734.
Train: 2018-08-02T12:49:18.680760: step 813, loss 0.539401.
Train: 2018-08-02T12:49:18.837003: step 814, loss 0.620754.
Train: 2018-08-02T12:49:18.977595: step 815, loss 0.503405.
Train: 2018-08-02T12:49:19.133779: step 816, loss 0.557988.
Train: 2018-08-02T12:49:19.290022: step 817, loss 0.492469.
Train: 2018-08-02T12:49:19.430585: step 818, loss 0.518681.
Train: 2018-08-02T12:49:19.586828: step 819, loss 0.546885.
Train: 2018-08-02T12:49:19.743041: step 820, loss 0.535893.
Test: 2018-08-02T12:49:20.196032: step 820, loss 0.554631.
Train: 2018-08-02T12:49:20.352274: step 821, loss 0.593656.
Train: 2018-08-02T12:49:20.492866: step 822, loss 0.492388.
Train: 2018-08-02T12:49:20.649079: step 823, loss 0.556816.
Train: 2018-08-02T12:49:20.805287: step 824, loss 0.537073.
Train: 2018-08-02T12:49:20.961476: step 825, loss 0.458593.
Train: 2018-08-02T12:49:21.102098: step 826, loss 0.513826.
Train: 2018-08-02T12:49:21.258313: step 827, loss 0.484912.
Train: 2018-08-02T12:49:21.414495: step 828, loss 0.509365.
Train: 2018-08-02T12:49:21.570707: step 829, loss 0.531587.
Train: 2018-08-02T12:49:21.711301: step 830, loss 0.595252.
Test: 2018-08-02T12:49:22.179941: step 830, loss 0.546737.
Train: 2018-08-02T12:49:22.320563: step 831, loss 0.431482.
Train: 2018-08-02T12:49:22.476776: step 832, loss 0.455696.
Train: 2018-08-02T12:49:22.632990: step 833, loss 0.533839.
Train: 2018-08-02T12:49:22.773582: step 834, loss 0.544171.
Train: 2018-08-02T12:49:22.929798: step 835, loss 0.489143.
Train: 2018-08-02T12:49:23.086008: step 836, loss 0.462846.
Train: 2018-08-02T12:49:23.226601: step 837, loss 0.548347.
Train: 2018-08-02T12:49:23.382810: step 838, loss 0.563095.
Train: 2018-08-02T12:49:23.538998: step 839, loss 0.444121.
Train: 2018-08-02T12:49:23.679619: step 840, loss 0.537005.
Test: 2018-08-02T12:49:24.148230: step 840, loss 0.547025.
Train: 2018-08-02T12:49:24.288852: step 841, loss 0.596649.
Train: 2018-08-02T12:49:24.445059: step 842, loss 0.498419.
Train: 2018-08-02T12:49:24.601280: step 843, loss 0.595698.
Train: 2018-08-02T12:49:24.757492: step 844, loss 0.492535.
Train: 2018-08-02T12:49:24.898055: step 845, loss 0.512981.
Train: 2018-08-02T12:49:25.054268: step 846, loss 0.480842.
Train: 2018-08-02T12:49:25.210516: step 847, loss 0.540231.
Train: 2018-08-02T12:49:25.351074: step 848, loss 0.526953.
Train: 2018-08-02T12:49:25.507316: step 849, loss 0.539782.
Train: 2018-08-02T12:49:25.663499: step 850, loss 0.504435.
Test: 2018-08-02T12:49:26.116545: step 850, loss 0.547985.
Train: 2018-08-02T12:49:26.272732: step 851, loss 0.549582.
Train: 2018-08-02T12:49:26.413354: step 852, loss 0.451901.
Train: 2018-08-02T12:49:26.569539: step 853, loss 0.535979.
Train: 2018-08-02T12:49:26.725781: step 854, loss 0.491722.
Train: 2018-08-02T12:49:26.866373: step 855, loss 0.599712.
Train: 2018-08-02T12:49:27.022586: step 856, loss 0.501184.
Train: 2018-08-02T12:49:27.178801: step 857, loss 0.569297.
Train: 2018-08-02T12:49:27.319363: step 858, loss 0.466205.
Train: 2018-08-02T12:49:27.475575: step 859, loss 0.562686.
Train: 2018-08-02T12:49:27.631788: step 860, loss 0.571536.
Test: 2018-08-02T12:49:28.084809: step 860, loss 0.553324.
Train: 2018-08-02T12:49:28.241051: step 861, loss 0.477465.
Train: 2018-08-02T12:49:28.381643: step 862, loss 0.509729.
Train: 2018-08-02T12:49:28.537852: step 863, loss 0.63292.
Train: 2018-08-02T12:49:28.694070: step 864, loss 0.536908.
Train: 2018-08-02T12:49:28.850283: step 865, loss 0.518234.
Train: 2018-08-02T12:49:28.990876: step 866, loss 0.660286.
Train: 2018-08-02T12:49:29.147089: step 867, loss 0.458462.
Train: 2018-08-02T12:49:29.303302: step 868, loss 0.502206.
Train: 2018-08-02T12:49:29.443864: step 869, loss 0.569936.
Train: 2018-08-02T12:49:29.600079: step 870, loss 0.570551.
Test: 2018-08-02T12:49:30.053123: step 870, loss 0.549395.
Train: 2018-08-02T12:49:30.209341: step 871, loss 0.494224.
Train: 2018-08-02T12:49:30.365554: step 872, loss 0.438626.
Train: 2018-08-02T12:49:30.506147: step 873, loss 0.572931.
Train: 2018-08-02T12:49:30.662359: step 874, loss 0.468717.
Train: 2018-08-02T12:49:30.818572: step 875, loss 0.487114.
Train: 2018-08-02T12:49:30.959165: step 876, loss 0.52138.
Train: 2018-08-02T12:49:31.115379: step 877, loss 0.484545.
Train: 2018-08-02T12:49:31.271591: step 878, loss 0.523336.
Train: 2018-08-02T12:49:31.412154: step 879, loss 0.499446.
Train: 2018-08-02T12:49:31.568399: step 880, loss 0.497359.
Test: 2018-08-02T12:49:32.021386: step 880, loss 0.549854.
Train: 2018-08-02T12:49:32.177624: step 881, loss 0.495721.
Train: 2018-08-02T12:49:32.333843: step 882, loss 0.469511.
Train: 2018-08-02T12:49:32.474405: step 883, loss 0.578068.
Train: 2018-08-02T12:49:32.630649: step 884, loss 0.502169.
Train: 2018-08-02T12:49:32.786862: step 885, loss 0.603323.
Train: 2018-08-02T12:49:32.927454: step 886, loss 0.650005.
Train: 2018-08-02T12:49:33.083668: step 887, loss 0.533206.
Train: 2018-08-02T12:49:33.239881: step 888, loss 0.540744.
Train: 2018-08-02T12:49:33.380473: step 889, loss 0.591577.
Train: 2018-08-02T12:49:33.536656: step 890, loss 0.56228.
Test: 2018-08-02T12:49:33.989676: step 890, loss 0.562921.
Train: 2018-08-02T12:49:34.145920: step 891, loss 0.525449.
Train: 2018-08-02T12:49:34.302132: step 892, loss 0.546088.
Train: 2018-08-02T12:49:34.442693: step 893, loss 0.549541.
Train: 2018-08-02T12:49:34.598937: step 894, loss 0.570382.
Train: 2018-08-02T12:49:34.755151: step 895, loss 0.533962.
Train: 2018-08-02T12:49:34.911358: step 896, loss 0.45186.
Train: 2018-08-02T12:49:35.051927: step 897, loss 0.485942.
Train: 2018-08-02T12:49:35.208169: step 898, loss 0.601191.
Train: 2018-08-02T12:49:35.348732: step 899, loss 0.454106.
Train: 2018-08-02T12:49:35.504946: step 900, loss 0.576973.
Test: 2018-08-02T12:49:35.957989: step 900, loss 0.548131.
Train: 2018-08-02T12:49:36.598440: step 901, loss 0.558893.
Train: 2018-08-02T12:49:36.754683: step 902, loss 0.485634.
Train: 2018-08-02T12:49:36.895245: step 903, loss 0.56644.
Train: 2018-08-02T12:49:37.051488: step 904, loss 0.572228.
Train: 2018-08-02T12:49:37.207702: step 905, loss 0.52083.
Train: 2018-08-02T12:49:37.317021: step 906, loss 0.510456.
Train: 2018-08-02T12:49:37.457612: step 907, loss 0.60644.
Train: 2018-08-02T12:49:37.613857: step 908, loss 0.527963.
Train: 2018-08-02T12:49:37.770040: step 909, loss 0.568884.
Train: 2018-08-02T12:49:37.910663: step 910, loss 0.479145.
Test: 2018-08-02T12:49:38.379274: step 910, loss 0.550209.
Train: 2018-08-02T12:49:38.519897: step 911, loss 0.567877.
Train: 2018-08-02T12:49:38.676078: step 912, loss 0.523056.
Train: 2018-08-02T12:49:38.816670: step 913, loss 0.564306.
Train: 2018-08-02T12:49:38.988505: step 914, loss 0.481733.
Train: 2018-08-02T12:49:39.129127: step 915, loss 0.543769.
Train: 2018-08-02T12:49:39.285312: step 916, loss 0.523752.
Train: 2018-08-02T12:49:39.441554: step 917, loss 0.573654.
Train: 2018-08-02T12:49:39.597737: step 918, loss 0.492009.
Train: 2018-08-02T12:49:39.738328: step 919, loss 0.479089.
Train: 2018-08-02T12:49:39.894543: step 920, loss 0.525528.
Test: 2018-08-02T12:49:40.347587: step 920, loss 0.548181.
Train: 2018-08-02T12:49:40.503775: step 921, loss 0.474666.
Train: 2018-08-02T12:49:40.659988: step 922, loss 0.633993.
Train: 2018-08-02T12:49:40.800610: step 923, loss 0.51326.
Train: 2018-08-02T12:49:40.956823: step 924, loss 0.622679.
Train: 2018-08-02T12:49:41.097416: step 925, loss 0.50473.
Train: 2018-08-02T12:49:41.253630: step 926, loss 0.569349.
Train: 2018-08-02T12:49:41.409812: step 927, loss 0.542037.
Train: 2018-08-02T12:49:41.550434: step 928, loss 0.563874.
Train: 2018-08-02T12:49:41.706647: step 929, loss 0.588131.
Train: 2018-08-02T12:49:41.862862: step 930, loss 0.553431.
Test: 2018-08-02T12:49:42.315850: step 930, loss 0.561705.
Train: 2018-08-02T12:49:42.456472: step 931, loss 0.542908.
Train: 2018-08-02T12:49:42.612656: step 932, loss 0.565796.
Train: 2018-08-02T12:49:42.768900: step 933, loss 0.51141.
Train: 2018-08-02T12:49:42.909491: step 934, loss 0.504195.
Train: 2018-08-02T12:49:43.065704: step 935, loss 0.530347.
Train: 2018-08-02T12:49:43.221889: step 936, loss 0.449882.
Train: 2018-08-02T12:49:43.362510: step 937, loss 0.525344.
Train: 2018-08-02T12:49:43.518724: step 938, loss 0.548407.
Train: 2018-08-02T12:49:43.674937: step 939, loss 0.476983.
Train: 2018-08-02T12:49:43.815530: step 940, loss 0.540197.
Test: 2018-08-02T12:49:44.284164: step 940, loss 0.547772.
Train: 2018-08-02T12:49:44.424761: step 941, loss 0.588485.
Train: 2018-08-02T12:49:44.580975: step 942, loss 0.592686.
Train: 2018-08-02T12:49:44.737188: step 943, loss 0.533136.
Train: 2018-08-02T12:49:44.877782: step 944, loss 0.462438.
Train: 2018-08-02T12:49:45.033989: step 945, loss 0.544756.
Train: 2018-08-02T12:49:45.190207: step 946, loss 0.557325.
Train: 2018-08-02T12:49:45.346391: step 947, loss 0.520699.
Train: 2018-08-02T12:49:45.486982: step 948, loss 0.545767.
Train: 2018-08-02T12:49:45.643226: step 949, loss 0.633233.
Train: 2018-08-02T12:49:45.799410: step 950, loss 0.4757.
Test: 2018-08-02T12:49:46.252459: step 950, loss 0.550053.
Train: 2018-08-02T12:49:46.393051: step 951, loss 0.604259.
Train: 2018-08-02T12:49:46.549264: step 952, loss 0.49687.
Train: 2018-08-02T12:49:46.705448: step 953, loss 0.503529.
Train: 2018-08-02T12:49:46.846069: step 954, loss 0.573015.
Train: 2018-08-02T12:49:47.002282: step 955, loss 0.542027.
Train: 2018-08-02T12:49:47.158518: step 956, loss 0.50133.
Train: 2018-08-02T12:49:47.299059: step 957, loss 0.596939.
Train: 2018-08-02T12:49:47.455271: step 958, loss 0.554688.
Train: 2018-08-02T12:49:47.595869: step 959, loss 0.606797.
Train: 2018-08-02T12:49:47.752078: step 960, loss 0.474201.
Test: 2018-08-02T12:49:48.205122: step 960, loss 0.552964.
Train: 2018-08-02T12:49:48.361340: step 961, loss 0.552506.
Train: 2018-08-02T12:49:48.517524: step 962, loss 0.608079.
Train: 2018-08-02T12:49:48.658115: step 963, loss 0.526602.
Train: 2018-08-02T12:49:48.814359: step 964, loss 0.521957.
Train: 2018-08-02T12:49:48.970565: step 965, loss 0.490833.
Train: 2018-08-02T12:49:49.111164: step 966, loss 0.520063.
Train: 2018-08-02T12:49:49.267378: step 967, loss 0.595344.
Train: 2018-08-02T12:49:49.407939: step 968, loss 0.486918.
Train: 2018-08-02T12:49:49.564183: step 969, loss 0.492556.
Train: 2018-08-02T12:49:49.720397: step 970, loss 0.419678.
Test: 2018-08-02T12:49:50.173394: step 970, loss 0.547713.
Train: 2018-08-02T12:49:50.313976: step 971, loss 0.600608.
Train: 2018-08-02T12:49:50.470220: step 972, loss 0.440681.
Train: 2018-08-02T12:49:50.626434: step 973, loss 0.534865.
Train: 2018-08-02T12:49:50.767026: step 974, loss 0.587873.
Train: 2018-08-02T12:49:50.923210: step 975, loss 0.496623.
Train: 2018-08-02T12:49:51.079422: step 976, loss 0.507792.
Train: 2018-08-02T12:49:51.235670: step 977, loss 0.513215.
Train: 2018-08-02T12:49:51.376260: step 978, loss 0.562665.
Train: 2018-08-02T12:49:51.532472: step 979, loss 0.545826.
Train: 2018-08-02T12:49:51.688686: step 980, loss 0.592068.
Test: 2018-08-02T12:49:52.141675: step 980, loss 0.556849.
Train: 2018-08-02T12:49:52.282296: step 981, loss 0.492004.
Train: 2018-08-02T12:49:52.438511: step 982, loss 0.559785.
Train: 2018-08-02T12:49:52.594717: step 983, loss 0.541993.
Train: 2018-08-02T12:49:52.750938: step 984, loss 0.528409.
Train: 2018-08-02T12:49:52.891529: step 985, loss 0.52634.
Train: 2018-08-02T12:49:53.047712: step 986, loss 0.487532.
Train: 2018-08-02T12:49:53.188334: step 987, loss 0.585176.
Train: 2018-08-02T12:49:53.344547: step 988, loss 0.448335.
Train: 2018-08-02T12:49:53.500773: step 989, loss 0.521218.
Train: 2018-08-02T12:49:53.641347: step 990, loss 0.51547.
Test: 2018-08-02T12:49:54.109964: step 990, loss 0.549027.
Train: 2018-08-02T12:49:54.250586: step 991, loss 0.487319.
Train: 2018-08-02T12:49:54.406770: step 992, loss 0.569013.
Train: 2018-08-02T12:49:54.563006: step 993, loss 0.535372.
Train: 2018-08-02T12:49:54.719229: step 994, loss 0.453733.
Train: 2018-08-02T12:49:54.859818: step 995, loss 0.41159.
Train: 2018-08-02T12:49:55.016032: step 996, loss 0.538434.
Train: 2018-08-02T12:49:55.156623: step 997, loss 0.476185.
Train: 2018-08-02T12:49:55.312806: step 998, loss 0.490137.
Train: 2018-08-02T12:49:55.469050: step 999, loss 0.442997.
Train: 2018-08-02T12:49:55.609612: step 1000, loss 0.556857.
Test: 2018-08-02T12:49:56.062631: step 1000, loss 0.54858.
Train: 2018-08-02T12:49:56.750001: step 1001, loss 0.487434.
Train: 2018-08-02T12:49:56.906214: step 1002, loss 0.579384.
Train: 2018-08-02T12:49:57.046805: step 1003, loss 0.443046.
Train: 2018-08-02T12:49:57.202989: step 1004, loss 0.576407.
Train: 2018-08-02T12:49:57.359233: step 1005, loss 0.508816.
Train: 2018-08-02T12:49:57.499824: step 1006, loss 0.540701.
Train: 2018-08-02T12:49:57.656041: step 1007, loss 0.550054.
Train: 2018-08-02T12:49:57.812251: step 1008, loss 0.491269.
Train: 2018-08-02T12:49:57.952813: step 1009, loss 0.518854.
Train: 2018-08-02T12:49:58.109051: step 1010, loss 0.5888.
Test: 2018-08-02T12:49:58.562046: step 1010, loss 0.552032.
Train: 2018-08-02T12:49:58.718259: step 1011, loss 0.502834.
Train: 2018-08-02T12:49:58.874473: step 1012, loss 0.509489.
Train: 2018-08-02T12:49:59.015066: step 1013, loss 0.488496.
Train: 2018-08-02T12:49:59.171303: step 1014, loss 0.536763.
Train: 2018-08-02T12:49:59.327521: step 1015, loss 0.459459.
Train: 2018-08-02T12:49:59.483706: step 1016, loss 0.548253.
Train: 2018-08-02T12:49:59.624327: step 1017, loss 0.530971.
Train: 2018-08-02T12:49:59.780542: step 1018, loss 0.526075.
Train: 2018-08-02T12:49:59.936754: step 1019, loss 0.514501.
Train: 2018-08-02T12:50:00.077346: step 1020, loss 0.601954.
Test: 2018-08-02T12:50:00.545989: step 1020, loss 0.549552.
Train: 2018-08-02T12:50:00.686578: step 1021, loss 0.454009.
Train: 2018-08-02T12:50:00.842791: step 1022, loss 0.593795.
Train: 2018-08-02T12:50:00.999000: step 1023, loss 0.569039.
Train: 2018-08-02T12:50:01.139568: step 1024, loss 0.502211.
Train: 2018-08-02T12:50:01.295811: step 1025, loss 0.532186.
Train: 2018-08-02T12:50:01.452018: step 1026, loss 0.550799.
Train: 2018-08-02T12:50:01.592616: step 1027, loss 0.540542.
Train: 2018-08-02T12:50:01.748830: step 1028, loss 0.558976.
Train: 2018-08-02T12:50:01.889421: step 1029, loss 0.489269.
Train: 2018-08-02T12:50:02.045636: step 1030, loss 0.478177.
Test: 2018-08-02T12:50:02.498649: step 1030, loss 0.548401.
Train: 2018-08-02T12:50:02.654867: step 1031, loss 0.516423.
Train: 2018-08-02T12:50:02.795459: step 1032, loss 0.466144.
Train: 2018-08-02T12:50:02.951643: step 1033, loss 0.514298.
Train: 2018-08-02T12:50:03.107856: step 1034, loss 0.500758.
Train: 2018-08-02T12:50:03.248478: step 1035, loss 0.542131.
Train: 2018-08-02T12:50:03.404692: step 1036, loss 0.561077.
Train: 2018-08-02T12:50:03.545254: step 1037, loss 0.510263.
Train: 2018-08-02T12:50:03.701497: step 1038, loss 0.546623.
Train: 2018-08-02T12:50:03.857680: step 1039, loss 0.568026.
Train: 2018-08-02T12:50:04.013894: step 1040, loss 0.507373.
Test: 2018-08-02T12:50:04.466913: step 1040, loss 0.551561.
Train: 2018-08-02T12:50:04.607536: step 1041, loss 0.57478.
Train: 2018-08-02T12:50:04.763743: step 1042, loss 0.560898.
Train: 2018-08-02T12:50:04.919962: step 1043, loss 0.519997.
Train: 2018-08-02T12:50:05.060554: step 1044, loss 0.452298.
Train: 2018-08-02T12:50:05.216768: step 1045, loss 0.546779.
Train: 2018-08-02T12:50:05.372952: step 1046, loss 0.551564.
Train: 2018-08-02T12:50:05.513573: step 1047, loss 0.457269.
Train: 2018-08-02T12:50:05.669783: step 1048, loss 0.46552.
Train: 2018-08-02T12:50:05.826000: step 1049, loss 0.550023.
Train: 2018-08-02T12:50:05.982182: step 1050, loss 0.554045.
Test: 2018-08-02T12:50:06.435233: step 1050, loss 0.548539.
Train: 2018-08-02T12:50:06.591415: step 1051, loss 0.565484.
Train: 2018-08-02T12:50:06.732038: step 1052, loss 0.556396.
Train: 2018-08-02T12:50:06.888250: step 1053, loss 0.600446.
Train: 2018-08-02T12:50:07.044435: step 1054, loss 0.509362.
Train: 2018-08-02T12:50:07.185043: step 1055, loss 0.567285.
Train: 2018-08-02T12:50:07.341240: step 1056, loss 0.551806.
Train: 2018-08-02T12:50:07.450620: step 1057, loss 0.519104.
Train: 2018-08-02T12:50:07.606833: step 1058, loss 0.538139.
Train: 2018-08-02T12:50:07.747428: step 1059, loss 0.512561.
Train: 2018-08-02T12:50:07.903607: step 1060, loss 0.48656.
Test: 2018-08-02T12:50:08.356627: step 1060, loss 0.546226.
Train: 2018-08-02T12:50:08.512870: step 1061, loss 0.55811.
Train: 2018-08-02T12:50:08.669053: step 1062, loss 0.461751.
Train: 2018-08-02T12:50:08.809676: step 1063, loss 0.52517.
Train: 2018-08-02T12:50:08.965890: step 1064, loss 0.432667.
Train: 2018-08-02T12:50:09.106482: step 1065, loss 0.465606.
Train: 2018-08-02T12:50:09.262695: step 1066, loss 0.516009.
Train: 2018-08-02T12:50:09.418878: step 1067, loss 0.556966.
Train: 2018-08-02T12:50:09.559471: step 1068, loss 0.492273.
Train: 2018-08-02T12:50:09.715715: step 1069, loss 0.451733.
Train: 2018-08-02T12:50:09.871927: step 1070, loss 0.578254.
Test: 2018-08-02T12:50:10.324942: step 1070, loss 0.548477.
Train: 2018-08-02T12:50:10.481160: step 1071, loss 0.579252.
Train: 2018-08-02T12:50:10.621751: step 1072, loss 0.594171.
Train: 2018-08-02T12:50:10.777965: step 1073, loss 0.57499.
Train: 2018-08-02T12:50:10.934178: step 1074, loss 0.485079.
Train: 2018-08-02T12:50:11.074771: step 1075, loss 0.539143.
Train: 2018-08-02T12:50:11.230984: step 1076, loss 0.538271.
Train: 2018-08-02T12:50:11.387197: step 1077, loss 0.496609.
Train: 2018-08-02T12:50:11.527790: step 1078, loss 0.544903.
Train: 2018-08-02T12:50:11.684003: step 1079, loss 0.612918.
Train: 2018-08-02T12:50:11.840211: step 1080, loss 0.512106.
Test: 2018-08-02T12:50:12.308857: step 1080, loss 0.551125.
Train: 2018-08-02T12:50:12.449418: step 1081, loss 0.479853.
Train: 2018-08-02T12:50:12.605663: step 1082, loss 0.510779.
Train: 2018-08-02T12:50:12.746254: step 1083, loss 0.567915.
Train: 2018-08-02T12:50:12.902438: step 1084, loss 0.619862.
Train: 2018-08-02T12:50:13.058682: step 1085, loss 0.508704.
Train: 2018-08-02T12:50:13.199273: step 1086, loss 0.507862.
Train: 2018-08-02T12:50:13.355487: step 1087, loss 0.508659.
Train: 2018-08-02T12:50:13.511670: step 1088, loss 0.483256.
Train: 2018-08-02T12:50:13.667884: step 1089, loss 0.518824.
Train: 2018-08-02T12:50:13.808506: step 1090, loss 0.508458.
Test: 2018-08-02T12:50:14.261495: step 1090, loss 0.548668.
Train: 2018-08-02T12:50:14.417738: step 1091, loss 0.544303.
Train: 2018-08-02T12:50:14.573922: step 1092, loss 0.465847.
Train: 2018-08-02T12:50:14.714547: step 1093, loss 0.472232.
Train: 2018-08-02T12:50:14.870726: step 1094, loss 0.566419.
Train: 2018-08-02T12:50:15.026940: step 1095, loss 0.488953.
Train: 2018-08-02T12:50:15.167562: step 1096, loss 0.594606.
Train: 2018-08-02T12:50:15.323776: step 1097, loss 0.496748.
Train: 2018-08-02T12:50:15.464338: step 1098, loss 0.511216.
Train: 2018-08-02T12:50:15.620581: step 1099, loss 0.512786.
Train: 2018-08-02T12:50:15.776794: step 1100, loss 0.636597.
Test: 2018-08-02T12:50:16.229814: step 1100, loss 0.558122.
Train: 2018-08-02T12:50:16.889388: step 1101, loss 0.497322.
Train: 2018-08-02T12:50:17.045601: step 1102, loss 0.507594.
Train: 2018-08-02T12:50:17.201846: step 1103, loss 0.50385.
Train: 2018-08-02T12:50:17.342407: step 1104, loss 0.564857.
Train: 2018-08-02T12:50:17.498622: step 1105, loss 0.45575.
Train: 2018-08-02T12:50:17.654834: step 1106, loss 0.544827.
Train: 2018-08-02T12:50:17.795456: step 1107, loss 0.551091.
Train: 2018-08-02T12:50:17.951671: step 1108, loss 0.547165.
Train: 2018-08-02T12:50:18.107884: step 1109, loss 0.587964.
Train: 2018-08-02T12:50:18.248446: step 1110, loss 0.58813.
Test: 2018-08-02T12:50:18.717117: step 1110, loss 0.553491.
Train: 2018-08-02T12:50:18.857708: step 1111, loss 0.50591.
Train: 2018-08-02T12:50:19.013921: step 1112, loss 0.531351.
Train: 2018-08-02T12:50:19.170105: step 1113, loss 0.5202.
Train: 2018-08-02T12:50:19.326342: step 1114, loss 0.556102.
Train: 2018-08-02T12:50:19.466940: step 1115, loss 0.494956.
Train: 2018-08-02T12:50:19.623154: step 1116, loss 0.492497.
Train: 2018-08-02T12:50:19.779361: step 1117, loss 0.614717.
Train: 2018-08-02T12:50:19.919929: step 1118, loss 0.516673.
Train: 2018-08-02T12:50:20.076144: step 1119, loss 0.518681.
Train: 2018-08-02T12:50:20.232386: step 1120, loss 0.518696.
Test: 2018-08-02T12:50:20.685374: step 1120, loss 0.550103.
Train: 2018-08-02T12:50:20.841619: step 1121, loss 0.45725.
Train: 2018-08-02T12:50:20.997801: step 1122, loss 0.580902.
Train: 2018-08-02T12:50:21.154015: step 1123, loss 0.523188.
Train: 2018-08-02T12:50:21.294638: step 1124, loss 0.46796.
Train: 2018-08-02T12:50:21.450852: step 1125, loss 0.573198.
Train: 2018-08-02T12:50:21.607063: step 1126, loss 0.517154.
Train: 2018-08-02T12:50:21.747656: step 1127, loss 0.550081.
Train: 2018-08-02T12:50:21.903870: step 1128, loss 0.49455.
Train: 2018-08-02T12:50:22.060052: step 1129, loss 0.52116.
Train: 2018-08-02T12:50:22.216290: step 1130, loss 0.469318.
Test: 2018-08-02T12:50:22.669286: step 1130, loss 0.55062.
Train: 2018-08-02T12:50:22.809907: step 1131, loss 0.465104.
Train: 2018-08-02T12:50:22.966091: step 1132, loss 0.432522.
Train: 2018-08-02T12:50:23.122328: step 1133, loss 0.4493.
Train: 2018-08-02T12:50:23.278547: step 1134, loss 0.472892.
Train: 2018-08-02T12:50:23.419139: step 1135, loss 0.518131.
Train: 2018-08-02T12:50:23.575354: step 1136, loss 0.486587.
Train: 2018-08-02T12:50:23.731537: step 1137, loss 0.582739.
Train: 2018-08-02T12:50:23.872129: step 1138, loss 0.509765.
Train: 2018-08-02T12:50:24.028373: step 1139, loss 0.478477.
Train: 2018-08-02T12:50:24.184555: step 1140, loss 0.521807.
Test: 2018-08-02T12:50:24.637574: step 1140, loss 0.555781.
Train: 2018-08-02T12:50:24.793788: step 1141, loss 0.445778.
Train: 2018-08-02T12:50:24.934379: step 1142, loss 0.563397.
Train: 2018-08-02T12:50:25.090593: step 1143, loss 0.522937.
Train: 2018-08-02T12:50:25.246837: step 1144, loss 0.49854.
Train: 2018-08-02T12:50:25.403020: step 1145, loss 0.529898.
Train: 2018-08-02T12:50:25.543612: step 1146, loss 0.591429.
Train: 2018-08-02T12:50:25.699826: step 1147, loss 0.509456.
Train: 2018-08-02T12:50:25.856068: step 1148, loss 0.552244.
Train: 2018-08-02T12:50:25.996631: step 1149, loss 0.573845.
Train: 2018-08-02T12:50:26.152874: step 1150, loss 0.541402.
Test: 2018-08-02T12:50:26.605863: step 1150, loss 0.557062.
Train: 2018-08-02T12:50:26.762078: step 1151, loss 0.545032.
Train: 2018-08-02T12:50:26.918321: step 1152, loss 0.503439.
Train: 2018-08-02T12:50:27.074534: step 1153, loss 0.645993.
Train: 2018-08-02T12:50:27.215126: step 1154, loss 0.577228.
Train: 2018-08-02T12:50:27.371309: step 1155, loss 0.513634.
Train: 2018-08-02T12:50:27.511901: step 1156, loss 0.583867.
Train: 2018-08-02T12:50:27.668144: step 1157, loss 0.522319.
Train: 2018-08-02T12:50:27.824359: step 1158, loss 0.497515.
Train: 2018-08-02T12:50:27.980571: step 1159, loss 0.493404.
Train: 2018-08-02T12:50:28.136755: step 1160, loss 0.56324.
Test: 2018-08-02T12:50:28.589774: step 1160, loss 0.548197.
Train: 2018-08-02T12:50:28.746016: step 1161, loss 0.55009.
Train: 2018-08-02T12:50:28.886579: step 1162, loss 0.541343.
Train: 2018-08-02T12:50:29.042793: step 1163, loss 0.522472.
Train: 2018-08-02T12:50:29.199037: step 1164, loss 0.532733.
Train: 2018-08-02T12:50:29.355255: step 1165, loss 0.493.
Train: 2018-08-02T12:50:29.495841: step 1166, loss 0.475.
Train: 2018-08-02T12:50:29.652055: step 1167, loss 0.487878.
Train: 2018-08-02T12:50:29.808268: step 1168, loss 0.550248.
Train: 2018-08-02T12:50:29.948861: step 1169, loss 0.505855.
Train: 2018-08-02T12:50:30.105073: step 1170, loss 0.59079.
Test: 2018-08-02T12:50:30.573683: step 1170, loss 0.550945.
Train: 2018-08-02T12:50:30.714306: step 1171, loss 0.587033.
Train: 2018-08-02T12:50:30.870519: step 1172, loss 0.530633.
Train: 2018-08-02T12:50:31.026703: step 1173, loss 0.59474.
Train: 2018-08-02T12:50:31.261053: step 1174, loss 0.544748.
Train: 2018-08-02T12:50:31.401645: step 1175, loss 0.532635.
Train: 2018-08-02T12:50:31.557858: step 1176, loss 0.582726.
Train: 2018-08-02T12:50:31.714042: step 1177, loss 0.54641.
Train: 2018-08-02T12:50:31.854664: step 1178, loss 0.499351.
Train: 2018-08-02T12:50:32.010846: step 1179, loss 0.551817.
Train: 2018-08-02T12:50:32.167090: step 1180, loss 0.49136.
Test: 2018-08-02T12:50:32.620104: step 1180, loss 0.5513.
Train: 2018-08-02T12:50:32.823188: step 1181, loss 0.486997.
Train: 2018-08-02T12:50:32.979372: step 1182, loss 0.507559.
Train: 2018-08-02T12:50:33.119996: step 1183, loss 0.538792.
Train: 2018-08-02T12:50:33.276176: step 1184, loss 0.49158.
Train: 2018-08-02T12:50:33.416798: step 1185, loss 0.488919.
Train: 2018-08-02T12:50:33.573013: step 1186, loss 0.531194.
Train: 2018-08-02T12:50:33.729195: step 1187, loss 0.46637.
Train: 2018-08-02T12:50:33.869787: step 1188, loss 0.492066.
Train: 2018-08-02T12:50:34.026001: step 1189, loss 0.508624.
Train: 2018-08-02T12:50:34.182244: step 1190, loss 0.490012.
Test: 2018-08-02T12:50:34.635260: step 1190, loss 0.549062.
Train: 2018-08-02T12:50:34.791476: step 1191, loss 0.465318.
Train: 2018-08-02T12:50:34.947660: step 1192, loss 0.623235.
Train: 2018-08-02T12:50:35.088282: step 1193, loss 0.503083.
Train: 2018-08-02T12:50:35.244465: step 1194, loss 0.515513.
Train: 2018-08-02T12:50:35.400703: step 1195, loss 0.556684.
Train: 2018-08-02T12:50:35.556911: step 1196, loss 0.504111.
Train: 2018-08-02T12:50:35.697484: step 1197, loss 0.528776.
Train: 2018-08-02T12:50:35.853727: step 1198, loss 0.567296.
Train: 2018-08-02T12:50:36.009936: step 1199, loss 0.517497.
Train: 2018-08-02T12:50:36.150534: step 1200, loss 0.539372.
Test: 2018-08-02T12:50:36.619168: step 1200, loss 0.555677.
Train: 2018-08-02T12:50:37.259648: step 1201, loss 0.592807.
Train: 2018-08-02T12:50:37.415862: step 1202, loss 0.524191.
Train: 2018-08-02T12:50:37.556453: step 1203, loss 0.600832.
Train: 2018-08-02T12:50:37.712637: step 1204, loss 0.450919.
Train: 2018-08-02T12:50:37.868880: step 1205, loss 0.454682.
Train: 2018-08-02T12:50:38.009473: step 1206, loss 0.61789.
Train: 2018-08-02T12:50:38.165655: step 1207, loss 0.566542.
Train: 2018-08-02T12:50:38.275036: step 1208, loss 0.495962.
Train: 2018-08-02T12:50:38.431219: step 1209, loss 0.535895.
Train: 2018-08-02T12:50:38.571810: step 1210, loss 0.506254.
Test: 2018-08-02T12:50:39.040452: step 1210, loss 0.549654.
Train: 2018-08-02T12:50:39.196696: step 1211, loss 0.551777.
Train: 2018-08-02T12:50:39.337256: step 1212, loss 0.584389.
Train: 2018-08-02T12:50:39.509122: step 1213, loss 0.525901.
Train: 2018-08-02T12:50:39.649684: step 1214, loss 0.569209.
Train: 2018-08-02T12:50:39.805927: step 1215, loss 0.492612.
Train: 2018-08-02T12:50:39.962141: step 1216, loss 0.530398.
Train: 2018-08-02T12:50:40.102702: step 1217, loss 0.439033.
Train: 2018-08-02T12:50:40.258946: step 1218, loss 0.475288.
Train: 2018-08-02T12:50:40.415130: step 1219, loss 0.514498.
Train: 2018-08-02T12:50:40.555722: step 1220, loss 0.540478.
Test: 2018-08-02T12:50:41.024394: step 1220, loss 0.557703.
Train: 2018-08-02T12:50:41.180605: step 1221, loss 0.599831.
Train: 2018-08-02T12:50:41.321166: step 1222, loss 0.553645.
Train: 2018-08-02T12:50:41.477381: step 1223, loss 0.5266.
Train: 2018-08-02T12:50:41.633593: step 1224, loss 0.557706.
Train: 2018-08-02T12:50:41.789833: step 1225, loss 0.563693.
Train: 2018-08-02T12:50:41.930429: step 1226, loss 0.513624.
Train: 2018-08-02T12:50:42.086643: step 1227, loss 0.566484.
Train: 2018-08-02T12:50:42.242826: step 1228, loss 0.540761.
Train: 2018-08-02T12:50:42.383418: step 1229, loss 0.505329.
Train: 2018-08-02T12:50:42.539661: step 1230, loss 0.500766.
Test: 2018-08-02T12:50:43.008274: step 1230, loss 0.548511.
Train: 2018-08-02T12:50:43.148894: step 1231, loss 0.49068.
Train: 2018-08-02T12:50:43.305107: step 1232, loss 0.532986.
Train: 2018-08-02T12:50:43.461290: step 1233, loss 0.45441.
Train: 2018-08-02T12:50:43.601883: step 1234, loss 0.52833.
Train: 2018-08-02T12:50:43.758127: step 1235, loss 0.576402.
Train: 2018-08-02T12:50:43.914340: step 1236, loss 0.428874.
Train: 2018-08-02T12:50:44.054932: step 1237, loss 0.540381.
Train: 2018-08-02T12:50:44.211114: step 1238, loss 0.449093.
Train: 2018-08-02T12:50:44.367360: step 1239, loss 0.51746.
Train: 2018-08-02T12:50:44.507951: step 1240, loss 0.463693.
Test: 2018-08-02T12:50:44.976561: step 1240, loss 0.549517.
Train: 2018-08-02T12:50:45.117177: step 1241, loss 0.511272.
Train: 2018-08-02T12:50:45.273367: step 1242, loss 0.543067.
Train: 2018-08-02T12:50:45.429579: step 1243, loss 0.527165.
Train: 2018-08-02T12:50:45.570203: step 1244, loss 0.49965.
Train: 2018-08-02T12:50:45.726415: step 1245, loss 0.504874.
Train: 2018-08-02T12:50:45.882628: step 1246, loss 0.528455.
Train: 2018-08-02T12:50:46.038842: step 1247, loss 0.526012.
Train: 2018-08-02T12:50:46.179404: step 1248, loss 0.528298.
Train: 2018-08-02T12:50:46.335648: step 1249, loss 0.539824.
Train: 2018-08-02T12:50:46.491861: step 1250, loss 0.529998.
Test: 2018-08-02T12:50:46.944850: step 1250, loss 0.549335.
Train: 2018-08-02T12:50:47.101093: step 1251, loss 0.535202.
Train: 2018-08-02T12:50:47.241655: step 1252, loss 0.472706.
Train: 2018-08-02T12:50:47.397869: step 1253, loss 0.480763.
Train: 2018-08-02T12:50:47.554112: step 1254, loss 0.487811.
Train: 2018-08-02T12:50:47.694675: step 1255, loss 0.626751.
Train: 2018-08-02T12:50:47.850918: step 1256, loss 0.518029.
Train: 2018-08-02T12:50:48.007100: step 1257, loss 0.569779.
Train: 2018-08-02T12:50:48.147694: step 1258, loss 0.495765.
Train: 2018-08-02T12:50:48.303937: step 1259, loss 0.52159.
Train: 2018-08-02T12:50:48.460151: step 1260, loss 0.539804.
Test: 2018-08-02T12:50:48.913139: step 1260, loss 0.549407.
Train: 2018-08-02T12:50:49.053761: step 1261, loss 0.562723.
Train: 2018-08-02T12:50:49.209975: step 1262, loss 0.486389.
Train: 2018-08-02T12:50:49.366158: step 1263, loss 0.483688.
Train: 2018-08-02T12:50:49.506780: step 1264, loss 0.524302.
Train: 2018-08-02T12:50:49.662994: step 1265, loss 0.475056.
Train: 2018-08-02T12:50:49.819177: step 1266, loss 0.530337.
Train: 2018-08-02T12:50:49.959800: step 1267, loss 0.481683.
Train: 2018-08-02T12:50:50.115983: step 1268, loss 0.567949.
Train: 2018-08-02T12:50:50.272226: step 1269, loss 0.491478.
Train: 2018-08-02T12:50:50.428410: step 1270, loss 0.547279.
Test: 2018-08-02T12:50:50.881429: step 1270, loss 0.549576.
Train: 2018-08-02T12:50:51.037671: step 1271, loss 0.480121.
Train: 2018-08-02T12:50:51.178263: step 1272, loss 0.572821.
Train: 2018-08-02T12:50:51.334448: step 1273, loss 0.554913.
Train: 2018-08-02T12:50:51.490691: step 1274, loss 0.499745.
Train: 2018-08-02T12:50:51.631283: step 1275, loss 0.550263.
Train: 2018-08-02T12:50:51.787496: step 1276, loss 0.563752.
Train: 2018-08-02T12:50:51.943678: step 1277, loss 0.57508.
Train: 2018-08-02T12:50:52.099923: step 1278, loss 0.50487.
Train: 2018-08-02T12:50:52.240485: step 1279, loss 0.53125.
Train: 2018-08-02T12:50:52.396729: step 1280, loss 0.511038.
Test: 2018-08-02T12:50:52.849717: step 1280, loss 0.55373.
Train: 2018-08-02T12:50:53.005961: step 1281, loss 0.563636.
Train: 2018-08-02T12:50:53.162175: step 1282, loss 0.546778.
Train: 2018-08-02T12:50:53.318385: step 1283, loss 0.460583.
Train: 2018-08-02T12:50:53.458980: step 1284, loss 0.491299.
Train: 2018-08-02T12:50:53.615193: step 1285, loss 0.437189.
Train: 2018-08-02T12:50:53.771407: step 1286, loss 0.552476.
Train: 2018-08-02T12:50:53.911999: step 1287, loss 0.573196.
Train: 2018-08-02T12:50:54.068213: step 1288, loss 0.509767.
Train: 2018-08-02T12:50:54.224425: step 1289, loss 0.449947.
Train: 2018-08-02T12:50:54.365017: step 1290, loss 0.506211.
Test: 2018-08-02T12:50:54.833628: step 1290, loss 0.548542.
Train: 2018-08-02T12:50:54.989872: step 1291, loss 0.480893.
Train: 2018-08-02T12:50:55.146082: step 1292, loss 0.461777.
Train: 2018-08-02T12:50:55.286647: step 1293, loss 0.507968.
Train: 2018-08-02T12:50:55.442859: step 1294, loss 0.490187.
Train: 2018-08-02T12:50:55.599104: step 1295, loss 0.498387.
Train: 2018-08-02T12:50:55.755317: step 1296, loss 0.537927.
Train: 2018-08-02T12:50:55.895909: step 1297, loss 0.487102.
Train: 2018-08-02T12:50:56.052123: step 1298, loss 0.545746.
Train: 2018-08-02T12:50:56.208305: step 1299, loss 0.51073.
Train: 2018-08-02T12:50:56.348898: step 1300, loss 0.535067.
Test: 2018-08-02T12:50:56.817570: step 1300, loss 0.552581.
Train: 2018-08-02T12:50:57.458044: step 1301, loss 0.5391.
Train: 2018-08-02T12:50:57.614227: step 1302, loss 0.553624.
Train: 2018-08-02T12:50:57.770470: step 1303, loss 0.478906.
Train: 2018-08-02T12:50:57.911062: step 1304, loss 0.600199.
Train: 2018-08-02T12:50:58.067245: step 1305, loss 0.507845.
Train: 2018-08-02T12:50:58.223460: step 1306, loss 0.585006.
Train: 2018-08-02T12:50:58.364051: step 1307, loss 0.567772.
Train: 2018-08-02T12:50:58.520264: step 1308, loss 0.517334.
Train: 2018-08-02T12:50:58.676508: step 1309, loss 0.528158.
Train: 2018-08-02T12:50:58.817071: step 1310, loss 0.487655.
Test: 2018-08-02T12:50:59.285711: step 1310, loss 0.548382.
Train: 2018-08-02T12:50:59.426303: step 1311, loss 0.557723.
Train: 2018-08-02T12:50:59.582547: step 1312, loss 0.511291.
Train: 2018-08-02T12:50:59.738730: step 1313, loss 0.584565.
Train: 2018-08-02T12:50:59.879351: step 1314, loss 0.551292.
Train: 2018-08-02T12:51:00.035534: step 1315, loss 0.59953.
Train: 2018-08-02T12:51:00.191779: step 1316, loss 0.536695.
Train: 2018-08-02T12:51:00.332371: step 1317, loss 0.556888.
Train: 2018-08-02T12:51:00.488583: step 1318, loss 0.500461.
Train: 2018-08-02T12:51:00.644797: step 1319, loss 0.477973.
Train: 2018-08-02T12:51:00.801011: step 1320, loss 0.508154.
Test: 2018-08-02T12:51:01.254025: step 1320, loss 0.54834.
Train: 2018-08-02T12:51:01.410243: step 1321, loss 0.487862.
Train: 2018-08-02T12:51:01.550835: step 1322, loss 0.600337.
Train: 2018-08-02T12:51:01.707049: step 1323, loss 0.528282.
Train: 2018-08-02T12:51:01.863262: step 1324, loss 0.526017.
Train: 2018-08-02T12:51:02.019445: step 1325, loss 0.548973.
Train: 2018-08-02T12:51:02.160067: step 1326, loss 0.433392.
Train: 2018-08-02T12:51:02.316250: step 1327, loss 0.561965.
Train: 2018-08-02T12:51:02.472463: step 1328, loss 0.507139.
Train: 2018-08-02T12:51:02.613056: step 1329, loss 0.485934.
Train: 2018-08-02T12:51:02.769301: step 1330, loss 0.576928.
Test: 2018-08-02T12:51:03.222289: step 1330, loss 0.548821.
Train: 2018-08-02T12:51:03.378503: step 1331, loss 0.591065.
Train: 2018-08-02T12:51:03.534746: step 1332, loss 0.55341.
Train: 2018-08-02T12:51:03.690960: step 1333, loss 0.533338.
Train: 2018-08-02T12:51:03.831521: step 1334, loss 0.555306.
Train: 2018-08-02T12:51:03.987733: step 1335, loss 0.56669.
Train: 2018-08-02T12:51:04.143948: step 1336, loss 0.589199.
Train: 2018-08-02T12:51:04.284569: step 1337, loss 0.547516.
Train: 2018-08-02T12:51:04.440784: step 1338, loss 0.552511.
Train: 2018-08-02T12:51:04.596997: step 1339, loss 0.517098.
Train: 2018-08-02T12:51:04.753210: step 1340, loss 0.487657.
Test: 2018-08-02T12:51:05.206199: step 1340, loss 0.551666.
Train: 2018-08-02T12:51:05.362442: step 1341, loss 0.532745.
Train: 2018-08-02T12:51:05.503035: step 1342, loss 0.479826.
Train: 2018-08-02T12:51:05.659218: step 1343, loss 0.539252.
Train: 2018-08-02T12:51:05.815462: step 1344, loss 0.612512.
Train: 2018-08-02T12:51:05.956054: step 1345, loss 0.491492.
Train: 2018-08-02T12:51:06.112237: step 1346, loss 0.525924.
Train: 2018-08-02T12:51:06.268480: step 1347, loss 0.558285.
Train: 2018-08-02T12:51:06.409071: step 1348, loss 0.518164.
Train: 2018-08-02T12:51:06.565256: step 1349, loss 0.533854.
Train: 2018-08-02T12:51:06.721500: step 1350, loss 0.587283.
Test: 2018-08-02T12:51:07.174489: step 1350, loss 0.552142.
Train: 2018-08-02T12:51:07.330701: step 1351, loss 0.555075.
Train: 2018-08-02T12:51:07.471324: step 1352, loss 0.498413.
Train: 2018-08-02T12:51:07.627537: step 1353, loss 0.574391.
Train: 2018-08-02T12:51:07.783750: step 1354, loss 0.513796.
Train: 2018-08-02T12:51:07.924311: step 1355, loss 0.501926.
Train: 2018-08-02T12:51:08.080556: step 1356, loss 0.507863.
Train: 2018-08-02T12:51:08.236769: step 1357, loss 0.516129.
Train: 2018-08-02T12:51:08.377361: step 1358, loss 0.505293.
Train: 2018-08-02T12:51:08.486711: step 1359, loss 0.615281.
Train: 2018-08-02T12:51:08.642894: step 1360, loss 0.495447.
Test: 2018-08-02T12:51:09.095913: step 1360, loss 0.547798.
Train: 2018-08-02T12:51:09.252156: step 1361, loss 0.544303.
Train: 2018-08-02T12:51:09.408339: step 1362, loss 0.498521.
Train: 2018-08-02T12:51:09.564579: step 1363, loss 0.509233.
Train: 2018-08-02T12:51:09.705176: step 1364, loss 0.497718.
Train: 2018-08-02T12:51:09.861389: step 1365, loss 0.522822.
Train: 2018-08-02T12:51:10.017602: step 1366, loss 0.536096.
Train: 2018-08-02T12:51:10.173816: step 1367, loss 0.543731.
Train: 2018-08-02T12:51:10.314409: step 1368, loss 0.599354.
Train: 2018-08-02T12:51:10.470621: step 1369, loss 0.574645.
Train: 2018-08-02T12:51:10.626834: step 1370, loss 0.567478.
Test: 2018-08-02T12:51:11.079823: step 1370, loss 0.558301.
Train: 2018-08-02T12:51:11.236066: step 1371, loss 0.527396.
Train: 2018-08-02T12:51:11.376659: step 1372, loss 0.526808.
Train: 2018-08-02T12:51:11.532843: step 1373, loss 0.491011.
Train: 2018-08-02T12:51:11.689055: step 1374, loss 0.459787.
Train: 2018-08-02T12:51:11.829647: step 1375, loss 0.433303.
Train: 2018-08-02T12:51:11.985891: step 1376, loss 0.533891.
Train: 2018-08-02T12:51:12.142104: step 1377, loss 0.589352.
Train: 2018-08-02T12:51:12.282697: step 1378, loss 0.713332.
Train: 2018-08-02T12:51:12.438911: step 1379, loss 0.466058.
Train: 2018-08-02T12:51:12.595094: step 1380, loss 0.508884.
Test: 2018-08-02T12:51:13.048114: step 1380, loss 0.558314.
Train: 2018-08-02T12:51:13.204357: step 1381, loss 0.508781.
Train: 2018-08-02T12:51:13.360569: step 1382, loss 0.542625.
Train: 2018-08-02T12:51:13.501162: step 1383, loss 0.562226.
Train: 2018-08-02T12:51:13.657351: step 1384, loss 0.623518.
Train: 2018-08-02T12:51:13.813589: step 1385, loss 0.509497.
Train: 2018-08-02T12:51:13.954181: step 1386, loss 0.527284.
Train: 2018-08-02T12:51:14.110364: step 1387, loss 0.508946.
Train: 2018-08-02T12:51:14.266608: step 1388, loss 0.524674.
Train: 2018-08-02T12:51:14.407201: step 1389, loss 0.574397.
Train: 2018-08-02T12:51:14.563414: step 1390, loss 0.543203.
Test: 2018-08-02T12:51:15.016402: step 1390, loss 0.54962.
Train: 2018-08-02T12:51:15.172645: step 1391, loss 0.599981.
Train: 2018-08-02T12:51:15.328828: step 1392, loss 0.606231.
Train: 2018-08-02T12:51:15.469421: step 1393, loss 0.536612.
Train: 2018-08-02T12:51:15.625664: step 1394, loss 0.522928.
Train: 2018-08-02T12:51:15.781848: step 1395, loss 0.53583.
Train: 2018-08-02T12:51:15.938096: step 1396, loss 0.504643.
Train: 2018-08-02T12:51:16.078683: step 1397, loss 0.496145.
Train: 2018-08-02T12:51:16.234896: step 1398, loss 0.534434.
Train: 2018-08-02T12:51:16.391109: step 1399, loss 0.494155.
Train: 2018-08-02T12:51:16.531671: step 1400, loss 0.510993.
Test: 2018-08-02T12:51:16.984690: step 1400, loss 0.546895.
Train: 2018-08-02T12:51:17.625195: step 1401, loss 0.49269.
Train: 2018-08-02T12:51:17.781379: step 1402, loss 0.497887.
Train: 2018-08-02T12:51:17.937593: step 1403, loss 0.51076.
Train: 2018-08-02T12:51:18.078185: step 1404, loss 0.520291.
Train: 2018-08-02T12:51:18.234428: step 1405, loss 0.518663.
Train: 2018-08-02T12:51:18.390642: step 1406, loss 0.521282.
Train: 2018-08-02T12:51:18.531233: step 1407, loss 0.427797.
Train: 2018-08-02T12:51:18.687448: step 1408, loss 0.479606.
Train: 2018-08-02T12:51:18.843632: step 1409, loss 0.552124.
Train: 2018-08-02T12:51:18.999843: step 1410, loss 0.534338.
Test: 2018-08-02T12:51:19.452863: step 1410, loss 0.548766.
Train: 2018-08-02T12:51:19.593486: step 1411, loss 0.492388.
Train: 2018-08-02T12:51:19.749699: step 1412, loss 0.44304.
Train: 2018-08-02T12:51:19.905881: step 1413, loss 0.522923.
Train: 2018-08-02T12:51:20.046504: step 1414, loss 0.548416.
Train: 2018-08-02T12:51:20.202717: step 1415, loss 0.473362.
Train: 2018-08-02T12:51:20.358900: step 1416, loss 0.489751.
Train: 2018-08-02T12:51:20.499525: step 1417, loss 0.551805.
Train: 2018-08-02T12:51:20.655736: step 1418, loss 0.513983.
Train: 2018-08-02T12:51:20.811950: step 1419, loss 0.601445.
Train: 2018-08-02T12:51:20.952542: step 1420, loss 0.535534.
Test: 2018-08-02T12:51:21.421183: step 1420, loss 0.552858.
Train: 2018-08-02T12:51:21.577396: step 1421, loss 0.479166.
Train: 2018-08-02T12:51:21.717988: step 1422, loss 0.560662.
Train: 2018-08-02T12:51:21.874200: step 1423, loss 0.517254.
Train: 2018-08-02T12:51:22.030418: step 1424, loss 0.508838.
Train: 2018-08-02T12:51:22.171000: step 1425, loss 0.538996.
Train: 2018-08-02T12:51:22.327190: step 1426, loss 0.493827.
Train: 2018-08-02T12:51:22.483433: step 1427, loss 0.508584.
Train: 2018-08-02T12:51:22.639647: step 1428, loss 0.536693.
Train: 2018-08-02T12:51:22.780238: step 1429, loss 0.535719.
Train: 2018-08-02T12:51:22.936453: step 1430, loss 0.497789.
Test: 2018-08-02T12:51:23.389441: step 1430, loss 0.547989.
Train: 2018-08-02T12:51:23.545684: step 1431, loss 0.498687.
Train: 2018-08-02T12:51:23.686282: step 1432, loss 0.515771.
Train: 2018-08-02T12:51:23.842489: step 1433, loss 0.464551.
Train: 2018-08-02T12:51:23.998698: step 1434, loss 0.505643.
Train: 2018-08-02T12:51:24.139296: step 1435, loss 0.538706.
Train: 2018-08-02T12:51:24.295479: step 1436, loss 0.471088.
Train: 2018-08-02T12:51:24.451722: step 1437, loss 0.559535.
Train: 2018-08-02T12:51:24.592284: step 1438, loss 0.483021.
Train: 2018-08-02T12:51:24.748527: step 1439, loss 0.601354.
Train: 2018-08-02T12:51:24.904741: step 1440, loss 0.454894.
Test: 2018-08-02T12:51:25.357731: step 1440, loss 0.552999.
Train: 2018-08-02T12:51:25.513974: step 1441, loss 0.523773.
Train: 2018-08-02T12:51:25.654536: step 1442, loss 0.544644.
Train: 2018-08-02T12:51:25.810773: step 1443, loss 0.580867.
Train: 2018-08-02T12:51:25.966961: step 1444, loss 0.550752.
Train: 2018-08-02T12:51:26.107584: step 1445, loss 0.53624.
Train: 2018-08-02T12:51:26.263798: step 1446, loss 0.492287.
Train: 2018-08-02T12:51:26.420005: step 1447, loss 0.538631.
Train: 2018-08-02T12:51:26.576225: step 1448, loss 0.548679.
Train: 2018-08-02T12:51:26.716816: step 1449, loss 0.526862.
Train: 2018-08-02T12:51:26.872999: step 1450, loss 0.562575.
Test: 2018-08-02T12:51:27.326026: step 1450, loss 0.549631.
Train: 2018-08-02T12:51:27.482262: step 1451, loss 0.53831.
Train: 2018-08-02T12:51:27.622825: step 1452, loss 0.4654.
Train: 2018-08-02T12:51:27.779062: step 1453, loss 0.555227.
Train: 2018-08-02T12:51:27.935282: step 1454, loss 0.557045.
Train: 2018-08-02T12:51:28.091470: step 1455, loss 0.593666.
Train: 2018-08-02T12:51:28.232057: step 1456, loss 0.507041.
Train: 2018-08-02T12:51:28.388300: step 1457, loss 0.396078.
Train: 2018-08-02T12:51:28.544518: step 1458, loss 0.51377.
Train: 2018-08-02T12:51:28.685107: step 1459, loss 0.576343.
Train: 2018-08-02T12:51:28.841288: step 1460, loss 0.52457.
Test: 2018-08-02T12:51:29.294334: step 1460, loss 0.548318.
Train: 2018-08-02T12:51:29.450521: step 1461, loss 0.508511.
Train: 2018-08-02T12:51:29.606734: step 1462, loss 0.501781.
Train: 2018-08-02T12:51:29.747357: step 1463, loss 0.567892.
Train: 2018-08-02T12:51:29.903570: step 1464, loss 0.526578.
Train: 2018-08-02T12:51:30.059785: step 1465, loss 0.556886.
Train: 2018-08-02T12:51:30.216003: step 1466, loss 0.503579.
Train: 2018-08-02T12:51:30.356559: step 1467, loss 0.520915.
Train: 2018-08-02T12:51:30.512803: step 1468, loss 0.566163.
Train: 2018-08-02T12:51:30.668985: step 1469, loss 0.567089.
Train: 2018-08-02T12:51:30.809609: step 1470, loss 0.505616.
Test: 2018-08-02T12:51:31.278243: step 1470, loss 0.555185.
Train: 2018-08-02T12:51:31.418840: step 1471, loss 0.52282.
Train: 2018-08-02T12:51:31.575023: step 1472, loss 0.535777.
Train: 2018-08-02T12:51:31.731241: step 1473, loss 0.530269.
Train: 2018-08-02T12:51:31.871861: step 1474, loss 0.545263.
Train: 2018-08-02T12:51:32.028073: step 1475, loss 0.602072.
Train: 2018-08-02T12:51:32.184287: step 1476, loss 0.576325.
Train: 2018-08-02T12:51:32.324879: step 1477, loss 0.538641.
Train: 2018-08-02T12:51:32.481061: step 1478, loss 0.519.
Train: 2018-08-02T12:51:32.637306: step 1479, loss 0.511778.
Train: 2018-08-02T12:51:32.777897: step 1480, loss 0.474905.
Test: 2018-08-02T12:51:33.246507: step 1480, loss 0.546938.
Train: 2018-08-02T12:51:33.387100: step 1481, loss 0.483673.
Train: 2018-08-02T12:51:33.543343: step 1482, loss 0.560976.
Train: 2018-08-02T12:51:33.699557: step 1483, loss 0.707549.
Train: 2018-08-02T12:51:33.855764: step 1484, loss 0.48414.
Train: 2018-08-02T12:51:33.996362: step 1485, loss 0.604634.
Train: 2018-08-02T12:51:34.152576: step 1486, loss 0.484734.
Train: 2018-08-02T12:51:34.308758: step 1487, loss 0.56512.
Train: 2018-08-02T12:51:34.465002: step 1488, loss 0.533436.
Train: 2018-08-02T12:51:34.605595: step 1489, loss 0.600939.
Train: 2018-08-02T12:51:34.761778: step 1490, loss 0.540346.
Test: 2018-08-02T12:51:35.214798: step 1490, loss 0.558781.
Train: 2018-08-02T12:51:35.371043: step 1491, loss 0.509952.
Train: 2018-08-02T12:51:35.527223: step 1492, loss 0.537761.
Train: 2018-08-02T12:51:35.667845: step 1493, loss 0.533274.
Train: 2018-08-02T12:51:35.824029: step 1494, loss 0.560209.
Train: 2018-08-02T12:51:35.980272: step 1495, loss 0.4543.
Train: 2018-08-02T12:51:36.120864: step 1496, loss 0.590572.
Train: 2018-08-02T12:51:36.277048: step 1497, loss 0.487934.
Train: 2018-08-02T12:51:36.433292: step 1498, loss 0.49125.
Train: 2018-08-02T12:51:36.573884: step 1499, loss 0.503999.
Train: 2018-08-02T12:51:36.730096: step 1500, loss 0.566069.
Test: 2018-08-02T12:51:37.183086: step 1500, loss 0.547077.
Train: 2018-08-02T12:51:37.823591: step 1501, loss 0.600033.
Train: 2018-08-02T12:51:37.979773: step 1502, loss 0.470805.
Train: 2018-08-02T12:51:38.136018: step 1503, loss 0.509065.
Train: 2018-08-02T12:51:38.276609: step 1504, loss 0.588402.
Train: 2018-08-02T12:51:38.432818: step 1505, loss 0.470805.
Train: 2018-08-02T12:51:38.589037: step 1506, loss 0.517278.
Train: 2018-08-02T12:51:38.745221: step 1507, loss 0.597265.
Train: 2018-08-02T12:51:38.885841: step 1508, loss 0.510733.
Train: 2018-08-02T12:51:39.042056: step 1509, loss 0.562673.
Train: 2018-08-02T12:51:39.151405: step 1510, loss 0.577741.
Test: 2018-08-02T12:51:39.604394: step 1510, loss 0.556176.
Train: 2018-08-02T12:51:39.760637: step 1511, loss 0.486871.
Train: 2018-08-02T12:51:39.916821: step 1512, loss 0.590283.
Train: 2018-08-02T12:51:40.073034: step 1513, loss 0.435088.
Train: 2018-08-02T12:51:40.213656: step 1514, loss 0.531077.
Train: 2018-08-02T12:51:40.369869: step 1515, loss 0.521662.
Train: 2018-08-02T12:51:40.526083: step 1516, loss 0.560146.
Train: 2018-08-02T12:51:40.682297: step 1517, loss 0.408041.
Train: 2018-08-02T12:51:40.822889: step 1518, loss 0.55737.
Train: 2018-08-02T12:51:40.979102: step 1519, loss 0.50967.
Train: 2018-08-02T12:51:41.135285: step 1520, loss 0.573291.
Test: 2018-08-02T12:51:41.588305: step 1520, loss 0.54611.
Train: 2018-08-02T12:51:41.744549: step 1521, loss 0.607554.
Train: 2018-08-02T12:51:41.885110: step 1522, loss 0.448776.
Train: 2018-08-02T12:51:42.041355: step 1523, loss 0.566741.
Train: 2018-08-02T12:51:42.197535: step 1524, loss 0.596758.
Train: 2018-08-02T12:51:42.338158: step 1525, loss 0.50228.
Train: 2018-08-02T12:51:42.494372: step 1526, loss 0.541227.
Train: 2018-08-02T12:51:42.650555: step 1527, loss 0.524919.
Train: 2018-08-02T12:51:42.791148: step 1528, loss 0.569422.
Train: 2018-08-02T12:51:42.947392: step 1529, loss 0.547359.
Train: 2018-08-02T12:51:43.103604: step 1530, loss 0.527181.
Test: 2018-08-02T12:51:43.556593: step 1530, loss 0.549523.
Train: 2018-08-02T12:51:43.712807: step 1531, loss 0.551768.
Train: 2018-08-02T12:51:43.869069: step 1532, loss 0.488412.
Train: 2018-08-02T12:51:44.009642: step 1533, loss 0.484874.
Train: 2018-08-02T12:51:44.165855: step 1534, loss 0.542783.
Train: 2018-08-02T12:51:44.322071: step 1535, loss 0.58661.
Train: 2018-08-02T12:51:44.462661: step 1536, loss 0.510105.
Train: 2018-08-02T12:51:44.618845: step 1537, loss 0.524547.
Train: 2018-08-02T12:51:44.775088: step 1538, loss 0.469893.
Train: 2018-08-02T12:51:44.915680: step 1539, loss 0.522457.
Train: 2018-08-02T12:51:45.071894: step 1540, loss 0.440106.
Test: 2018-08-02T12:51:45.524882: step 1540, loss 0.549388.
Train: 2018-08-02T12:51:45.681127: step 1541, loss 0.517044.
Train: 2018-08-02T12:51:45.837339: step 1542, loss 0.425226.
Train: 2018-08-02T12:51:45.993522: step 1543, loss 0.529808.
Train: 2018-08-02T12:51:46.134115: step 1544, loss 0.527893.
Train: 2018-08-02T12:51:46.290327: step 1545, loss 0.648556.
Train: 2018-08-02T12:51:46.446572: step 1546, loss 0.539903.
Train: 2018-08-02T12:51:46.587164: step 1547, loss 0.517803.
Train: 2018-08-02T12:51:46.743378: step 1548, loss 0.56316.
Train: 2018-08-02T12:51:46.899590: step 1549, loss 0.516355.
Train: 2018-08-02T12:51:47.055798: step 1550, loss 0.561178.
Test: 2018-08-02T12:51:47.508794: step 1550, loss 0.563684.
Train: 2018-08-02T12:51:47.665039: step 1551, loss 0.536997.
Train: 2018-08-02T12:51:47.805597: step 1552, loss 0.522824.
Train: 2018-08-02T12:51:47.961842: step 1553, loss 0.496442.
Train: 2018-08-02T12:51:48.118055: step 1554, loss 0.605834.
Train: 2018-08-02T12:51:48.274269: step 1555, loss 0.507906.
Train: 2018-08-02T12:51:48.414861: step 1556, loss 0.5765.
Train: 2018-08-02T12:51:48.571075: step 1557, loss 0.501389.
Train: 2018-08-02T12:51:48.727287: step 1558, loss 0.534459.
Train: 2018-08-02T12:51:48.867850: step 1559, loss 0.588634.
Train: 2018-08-02T12:51:49.024062: step 1560, loss 0.535171.
Test: 2018-08-02T12:51:49.477082: step 1560, loss 0.550566.
Train: 2018-08-02T12:51:49.617703: step 1561, loss 0.478512.
Train: 2018-08-02T12:51:49.773912: step 1562, loss 0.556477.
Train: 2018-08-02T12:51:49.930132: step 1563, loss 0.446762.
Train: 2018-08-02T12:51:50.086314: step 1564, loss 0.4813.
Train: 2018-08-02T12:51:50.226936: step 1565, loss 0.471456.
Train: 2018-08-02T12:51:50.383119: step 1566, loss 0.5526.
Train: 2018-08-02T12:51:50.539332: step 1567, loss 0.511139.
Train: 2018-08-02T12:51:50.679956: step 1568, loss 0.520815.
Train: 2018-08-02T12:51:50.836138: step 1569, loss 0.54068.
Train: 2018-08-02T12:51:50.992382: step 1570, loss 0.477349.
Test: 2018-08-02T12:51:51.445396: step 1570, loss 0.555602.
Train: 2018-08-02T12:51:51.601584: step 1571, loss 0.494293.
Train: 2018-08-02T12:51:51.757797: step 1572, loss 0.505897.
Train: 2018-08-02T12:51:51.898419: step 1573, loss 0.47228.
Train: 2018-08-02T12:51:52.054603: step 1574, loss 0.467662.
Train: 2018-08-02T12:51:52.210846: step 1575, loss 0.495781.
Train: 2018-08-02T12:51:52.351439: step 1576, loss 0.543272.
Train: 2018-08-02T12:51:52.507622: step 1577, loss 0.511508.
Train: 2018-08-02T12:51:52.663865: step 1578, loss 0.546494.
Train: 2018-08-02T12:51:52.804458: step 1579, loss 0.541718.
Train: 2018-08-02T12:51:52.960640: step 1580, loss 0.54794.
Test: 2018-08-02T12:51:53.413659: step 1580, loss 0.552906.
Train: 2018-08-02T12:51:53.569904: step 1581, loss 0.488293.
Train: 2018-08-02T12:51:53.710466: step 1582, loss 0.618763.
Train: 2018-08-02T12:51:53.866708: step 1583, loss 0.508856.
Train: 2018-08-02T12:51:54.022923: step 1584, loss 0.517801.
Train: 2018-08-02T12:51:54.163517: step 1585, loss 0.548226.
Train: 2018-08-02T12:51:54.319697: step 1586, loss 0.499281.
Train: 2018-08-02T12:51:54.475943: step 1587, loss 0.52744.
Train: 2018-08-02T12:51:54.616533: step 1588, loss 0.497905.
Train: 2018-08-02T12:51:54.772717: step 1589, loss 0.45353.
Train: 2018-08-02T12:51:54.928969: step 1590, loss 0.526528.
Test: 2018-08-02T12:51:55.381974: step 1590, loss 0.548727.
Train: 2018-08-02T12:51:55.538192: step 1591, loss 0.501883.
Train: 2018-08-02T12:51:55.678785: step 1592, loss 0.524932.
Train: 2018-08-02T12:51:55.834997: step 1593, loss 0.564203.
Train: 2018-08-02T12:51:55.991211: step 1594, loss 0.416192.
Train: 2018-08-02T12:51:56.147394: step 1595, loss 0.550905.
Train: 2018-08-02T12:51:56.303632: step 1596, loss 0.432753.
Train: 2018-08-02T12:51:56.444201: step 1597, loss 0.489761.
Train: 2018-08-02T12:51:56.600443: step 1598, loss 0.521406.
Train: 2018-08-02T12:51:56.756626: step 1599, loss 0.578443.
Train: 2018-08-02T12:51:56.897249: step 1600, loss 0.562226.
Test: 2018-08-02T12:51:57.350238: step 1600, loss 0.553072.
Train: 2018-08-02T12:51:57.975122: step 1601, loss 0.533705.
Train: 2018-08-02T12:51:58.115714: step 1602, loss 0.608003.
Train: 2018-08-02T12:51:58.271896: step 1603, loss 0.505474.
Train: 2018-08-02T12:51:58.428142: step 1604, loss 0.550977.
Train: 2018-08-02T12:51:58.568733: step 1605, loss 0.534213.
Train: 2018-08-02T12:51:58.724915: step 1606, loss 0.530501.
Train: 2018-08-02T12:51:58.881159: step 1607, loss 0.493267.
Train: 2018-08-02T12:51:59.021751: step 1608, loss 0.614136.
Train: 2018-08-02T12:51:59.177971: step 1609, loss 0.586941.
Train: 2018-08-02T12:51:59.334179: step 1610, loss 0.527097.
Test: 2018-08-02T12:51:59.787168: step 1610, loss 0.549154.
Train: 2018-08-02T12:51:59.943412: step 1611, loss 0.580873.
Train: 2018-08-02T12:52:00.099594: step 1612, loss 0.535773.
Train: 2018-08-02T12:52:00.240216: step 1613, loss 0.520885.
Train: 2018-08-02T12:52:00.396430: step 1614, loss 0.553071.
Train: 2018-08-02T12:52:00.552644: step 1615, loss 0.489267.
Train: 2018-08-02T12:52:00.708827: step 1616, loss 0.613272.
Train: 2018-08-02T12:52:00.849449: step 1617, loss 0.59048.
Train: 2018-08-02T12:52:01.005632: step 1618, loss 0.561641.
Train: 2018-08-02T12:52:01.161875: step 1619, loss 0.553228.
Train: 2018-08-02T12:52:01.302468: step 1620, loss 0.593051.
Test: 2018-08-02T12:52:01.771083: step 1620, loss 0.559551.
Train: 2018-08-02T12:52:01.911669: step 1621, loss 0.540218.
Train: 2018-08-02T12:52:02.067907: step 1622, loss 0.527833.
Train: 2018-08-02T12:52:02.224123: step 1623, loss 0.555853.
Train: 2018-08-02T12:52:02.380340: step 1624, loss 0.606053.
Train: 2018-08-02T12:52:02.520932: step 1625, loss 0.46428.
Train: 2018-08-02T12:52:02.677116: step 1626, loss 0.522753.
Train: 2018-08-02T12:52:02.833329: step 1627, loss 0.551144.
Train: 2018-08-02T12:52:02.989543: step 1628, loss 0.571989.
Train: 2018-08-02T12:52:03.130164: step 1629, loss 0.513684.
Train: 2018-08-02T12:52:03.286378: step 1630, loss 0.449.
Test: 2018-08-02T12:52:03.739376: step 1630, loss 0.548429.
Train: 2018-08-02T12:52:03.895610: step 1631, loss 0.518758.
Train: 2018-08-02T12:52:04.051825: step 1632, loss 0.601046.
Train: 2018-08-02T12:52:04.192387: step 1633, loss 0.491673.
Train: 2018-08-02T12:52:04.348629: step 1634, loss 0.55685.
Train: 2018-08-02T12:52:04.504842: step 1635, loss 0.578452.
Train: 2018-08-02T12:52:04.661025: step 1636, loss 0.543731.
Train: 2018-08-02T12:52:04.801649: step 1637, loss 0.554353.
Train: 2018-08-02T12:52:04.957864: step 1638, loss 0.531367.
Train: 2018-08-02T12:52:05.098423: step 1639, loss 0.53692.
Train: 2018-08-02T12:52:05.254662: step 1640, loss 0.493107.
Test: 2018-08-02T12:52:05.707681: step 1640, loss 0.553562.
Train: 2018-08-02T12:52:05.863893: step 1641, loss 0.534018.
Train: 2018-08-02T12:52:06.020113: step 1642, loss 0.542919.
Train: 2018-08-02T12:52:06.160705: step 1643, loss 0.553547.
Train: 2018-08-02T12:52:06.316919: step 1644, loss 0.418214.
Train: 2018-08-02T12:52:06.473132: step 1645, loss 0.509382.
Train: 2018-08-02T12:52:06.613694: step 1646, loss 0.546033.
Train: 2018-08-02T12:52:06.769938: step 1647, loss 0.580507.
Train: 2018-08-02T12:52:06.926152: step 1648, loss 0.543446.
Train: 2018-08-02T12:52:07.066743: step 1649, loss 0.584555.
Train: 2018-08-02T12:52:07.222956: step 1650, loss 0.489897.
Test: 2018-08-02T12:52:07.675945: step 1650, loss 0.563258.
Train: 2018-08-02T12:52:07.832189: step 1651, loss 0.532865.
Train: 2018-08-02T12:52:07.988402: step 1652, loss 0.50147.
Train: 2018-08-02T12:52:08.128964: step 1653, loss 0.509143.
Train: 2018-08-02T12:52:08.285207: step 1654, loss 0.506864.
Train: 2018-08-02T12:52:08.441390: step 1655, loss 0.493012.
Train: 2018-08-02T12:52:08.581983: step 1656, loss 0.481342.
Train: 2018-08-02T12:52:08.738195: step 1657, loss 0.458194.
Train: 2018-08-02T12:52:08.894409: step 1658, loss 0.497398.
Train: 2018-08-02T12:52:09.035032: step 1659, loss 0.411923.
Train: 2018-08-02T12:52:09.191246: step 1660, loss 0.579726.
Test: 2018-08-02T12:52:09.644235: step 1660, loss 0.551988.
Train: 2018-08-02T12:52:09.753613: step 1661, loss 0.522688.
Train: 2018-08-02T12:52:09.909827: step 1662, loss 0.430766.
Train: 2018-08-02T12:52:10.066040: step 1663, loss 0.486959.
Train: 2018-08-02T12:52:10.206633: step 1664, loss 0.440786.
Train: 2018-08-02T12:52:10.362822: step 1665, loss 0.586351.
Train: 2018-08-02T12:52:10.519059: step 1666, loss 0.55589.
Train: 2018-08-02T12:52:10.659651: step 1667, loss 0.473342.
Train: 2018-08-02T12:52:10.815834: step 1668, loss 0.483873.
Train: 2018-08-02T12:52:10.972049: step 1669, loss 0.483963.
Train: 2018-08-02T12:52:11.112639: step 1670, loss 0.523533.
Test: 2018-08-02T12:52:11.581311: step 1670, loss 0.553115.
Train: 2018-08-02T12:52:11.721902: step 1671, loss 0.498095.
Train: 2018-08-02T12:52:11.878086: step 1672, loss 0.554976.
Train: 2018-08-02T12:52:12.034332: step 1673, loss 0.500077.
Train: 2018-08-02T12:52:12.190514: step 1674, loss 0.501189.
Train: 2018-08-02T12:52:12.331135: step 1675, loss 0.511017.
Train: 2018-08-02T12:52:12.487348: step 1676, loss 0.599652.
Train: 2018-08-02T12:52:12.643562: step 1677, loss 0.533203.
Train: 2018-08-02T12:52:12.799770: step 1678, loss 0.557889.
Train: 2018-08-02T12:52:12.940337: step 1679, loss 0.561258.
Train: 2018-08-02T12:52:13.096581: step 1680, loss 0.572314.
Test: 2018-08-02T12:52:13.565205: step 1680, loss 0.558651.
Train: 2018-08-02T12:52:13.705782: step 1681, loss 0.533058.
Train: 2018-08-02T12:52:13.862026: step 1682, loss 0.54849.
Train: 2018-08-02T12:52:14.018210: step 1683, loss 0.593407.
Train: 2018-08-02T12:52:14.158832: step 1684, loss 0.589822.
Train: 2018-08-02T12:52:14.315046: step 1685, loss 0.523028.
Train: 2018-08-02T12:52:14.471253: step 1686, loss 0.562892.
Train: 2018-08-02T12:52:14.611851: step 1687, loss 0.510698.
Train: 2018-08-02T12:52:14.768064: step 1688, loss 0.57428.
Train: 2018-08-02T12:52:14.924259: step 1689, loss 0.610061.
Train: 2018-08-02T12:52:15.064867: step 1690, loss 0.501412.
Test: 2018-08-02T12:52:15.517860: step 1690, loss 0.555289.
Train: 2018-08-02T12:52:15.674103: step 1691, loss 0.523805.
Train: 2018-08-02T12:52:15.830315: step 1692, loss 0.554681.
Train: 2018-08-02T12:52:15.970908: step 1693, loss 0.509635.
Train: 2018-08-02T12:52:16.127090: step 1694, loss 0.532931.
Train: 2018-08-02T12:52:16.283336: step 1695, loss 0.501223.
Train: 2018-08-02T12:52:16.423927: step 1696, loss 0.501838.
Train: 2018-08-02T12:52:16.580140: step 1697, loss 0.471931.
Train: 2018-08-02T12:52:16.736353: step 1698, loss 0.473829.
Train: 2018-08-02T12:52:16.892536: step 1699, loss 0.519889.
Train: 2018-08-02T12:52:17.048756: step 1700, loss 0.522501.
Test: 2018-08-02T12:52:17.501770: step 1700, loss 0.546731.
Train: 2018-08-02T12:52:18.126627: step 1701, loss 0.550547.
Train: 2018-08-02T12:52:18.267245: step 1702, loss 0.540262.
Train: 2018-08-02T12:52:18.423459: step 1703, loss 0.504954.
Train: 2018-08-02T12:52:18.579642: step 1704, loss 0.504573.
Train: 2018-08-02T12:52:18.720264: step 1705, loss 0.444613.
Train: 2018-08-02T12:52:18.876478: step 1706, loss 0.581705.
Train: 2018-08-02T12:52:19.032691: step 1707, loss 0.468187.
Train: 2018-08-02T12:52:19.188906: step 1708, loss 0.538788.
Train: 2018-08-02T12:52:19.329496: step 1709, loss 0.571283.
Train: 2018-08-02T12:52:19.485709: step 1710, loss 0.585118.
Test: 2018-08-02T12:52:19.938698: step 1710, loss 0.552183.
Train: 2018-08-02T12:52:20.094941: step 1711, loss 0.57739.
Train: 2018-08-02T12:52:20.235534: step 1712, loss 0.552881.
Train: 2018-08-02T12:52:20.391748: step 1713, loss 0.551286.
Train: 2018-08-02T12:52:20.547930: step 1714, loss 0.575401.
Train: 2018-08-02T12:52:20.704169: step 1715, loss 0.562656.
Train: 2018-08-02T12:52:20.844766: step 1716, loss 0.545612.
Train: 2018-08-02T12:52:21.000979: step 1717, loss 0.559312.
Train: 2018-08-02T12:52:21.157194: step 1718, loss 0.492585.
Train: 2018-08-02T12:52:21.297786: step 1719, loss 0.502419.
Train: 2018-08-02T12:52:21.453999: step 1720, loss 0.527095.
Test: 2018-08-02T12:52:21.922633: step 1720, loss 0.547005.
Train: 2018-08-02T12:52:22.063233: step 1721, loss 0.460683.
Train: 2018-08-02T12:52:22.219444: step 1722, loss 0.435242.
Train: 2018-08-02T12:52:22.375658: step 1723, loss 0.446767.
Train: 2018-08-02T12:52:22.516244: step 1724, loss 0.54238.
Train: 2018-08-02T12:52:22.672434: step 1725, loss 0.541794.
Train: 2018-08-02T12:52:22.828677: step 1726, loss 0.518256.
Train: 2018-08-02T12:52:22.969269: step 1727, loss 0.577199.
Train: 2018-08-02T12:52:23.125452: step 1728, loss 0.500207.
Train: 2018-08-02T12:52:23.281696: step 1729, loss 0.583815.
Train: 2018-08-02T12:52:23.422288: step 1730, loss 0.558828.
Test: 2018-08-02T12:52:23.890899: step 1730, loss 0.566249.
Train: 2018-08-02T12:52:24.047112: step 1731, loss 0.570777.
Train: 2018-08-02T12:52:24.187703: step 1732, loss 0.548523.
Train: 2018-08-02T12:52:24.343916: step 1733, loss 0.582519.
Train: 2018-08-02T12:52:24.484539: step 1734, loss 0.53042.
Train: 2018-08-02T12:52:24.640722: step 1735, loss 0.522215.
Train: 2018-08-02T12:52:24.796967: step 1736, loss 0.512997.
Train: 2018-08-02T12:52:24.937558: step 1737, loss 0.51916.
Train: 2018-08-02T12:52:25.093771: step 1738, loss 0.589845.
Train: 2018-08-02T12:52:25.249954: step 1739, loss 0.504957.
Train: 2018-08-02T12:52:25.390577: step 1740, loss 0.526403.
Test: 2018-08-02T12:52:25.859187: step 1740, loss 0.549661.
Train: 2018-08-02T12:52:25.999808: step 1741, loss 0.480022.
Train: 2018-08-02T12:52:26.156022: step 1742, loss 0.520356.
Train: 2018-08-02T12:52:26.312206: step 1743, loss 0.505373.
Train: 2018-08-02T12:52:26.452827: step 1744, loss 0.523888.
Train: 2018-08-02T12:52:26.609012: step 1745, loss 0.546178.
Train: 2018-08-02T12:52:26.765254: step 1746, loss 0.476213.
Train: 2018-08-02T12:52:26.921439: step 1747, loss 0.50526.
Train: 2018-08-02T12:52:27.062061: step 1748, loss 0.397353.
Train: 2018-08-02T12:52:27.218273: step 1749, loss 0.582325.
Train: 2018-08-02T12:52:27.374491: step 1750, loss 0.516118.
Test: 2018-08-02T12:52:27.827476: step 1750, loss 0.546299.
Train: 2018-08-02T12:52:27.983720: step 1751, loss 0.459592.
Train: 2018-08-02T12:52:28.139932: step 1752, loss 0.526521.
Train: 2018-08-02T12:52:28.280532: step 1753, loss 0.50911.
Train: 2018-08-02T12:52:28.436709: step 1754, loss 0.486121.
Train: 2018-08-02T12:52:28.592951: step 1755, loss 0.545834.
Train: 2018-08-02T12:52:28.733516: step 1756, loss 0.507354.
Train: 2018-08-02T12:52:28.889758: step 1757, loss 0.53762.
Train: 2018-08-02T12:52:29.045972: step 1758, loss 0.543356.
Train: 2018-08-02T12:52:29.186564: step 1759, loss 0.497844.
Train: 2018-08-02T12:52:29.342776: step 1760, loss 0.481107.
Test: 2018-08-02T12:52:29.795766: step 1760, loss 0.554177.
Train: 2018-08-02T12:52:29.951978: step 1761, loss 0.533612.
Train: 2018-08-02T12:52:30.108191: step 1762, loss 0.496828.
Train: 2018-08-02T12:52:30.264405: step 1763, loss 0.588166.
Train: 2018-08-02T12:52:30.404997: step 1764, loss 0.598912.
Train: 2018-08-02T12:52:30.561241: step 1765, loss 0.510344.
Train: 2018-08-02T12:52:30.717454: step 1766, loss 0.484382.
Train: 2018-08-02T12:52:30.858017: step 1767, loss 0.591476.
Train: 2018-08-02T12:52:31.014230: step 1768, loss 0.514.
Train: 2018-08-02T12:52:31.170474: step 1769, loss 0.485189.
Train: 2018-08-02T12:52:31.373521: step 1770, loss 0.527269.
Test: 2018-08-02T12:52:31.826539: step 1770, loss 0.553429.
Train: 2018-08-02T12:52:31.982778: step 1771, loss 0.546884.
Train: 2018-08-02T12:52:32.138965: step 1772, loss 0.483343.
Train: 2018-08-02T12:52:32.279589: step 1773, loss 0.489657.
Train: 2018-08-02T12:52:32.435802: step 1774, loss 0.531087.
Train: 2018-08-02T12:52:32.592015: step 1775, loss 0.520171.
Train: 2018-08-02T12:52:32.732577: step 1776, loss 0.533113.
Train: 2018-08-02T12:52:32.888821: step 1777, loss 0.50622.
Train: 2018-08-02T12:52:33.045005: step 1778, loss 0.561156.
Train: 2018-08-02T12:52:33.185627: step 1779, loss 0.524581.
Train: 2018-08-02T12:52:33.341840: step 1780, loss 0.474209.
Test: 2018-08-02T12:52:33.794857: step 1780, loss 0.550816.
Train: 2018-08-02T12:52:33.997937: step 1781, loss 0.482916.
Train: 2018-08-02T12:52:34.154149: step 1782, loss 0.609636.
Train: 2018-08-02T12:52:34.310363: step 1783, loss 0.601893.
Train: 2018-08-02T12:52:34.450956: step 1784, loss 0.505783.
Train: 2018-08-02T12:52:34.607139: step 1785, loss 0.567065.
Train: 2018-08-02T12:52:34.763352: step 1786, loss 0.51287.
Train: 2018-08-02T12:52:34.903974: step 1787, loss 0.500749.
Train: 2018-08-02T12:52:35.060157: step 1788, loss 0.500689.
Train: 2018-08-02T12:52:35.216395: step 1789, loss 0.50676.
Train: 2018-08-02T12:52:35.372615: step 1790, loss 0.437263.
Test: 2018-08-02T12:52:35.825634: step 1790, loss 0.547913.
Train: 2018-08-02T12:52:35.966225: step 1791, loss 0.605743.
Train: 2018-08-02T12:52:36.122409: step 1792, loss 0.505851.
Train: 2018-08-02T12:52:36.278652: step 1793, loss 0.54917.
Train: 2018-08-02T12:52:36.419245: step 1794, loss 0.544121.
Train: 2018-08-02T12:52:36.575428: step 1795, loss 0.482449.
Train: 2018-08-02T12:52:36.731690: step 1796, loss 0.524691.
Train: 2018-08-02T12:52:36.872257: step 1797, loss 0.485647.
Train: 2018-08-02T12:52:37.028476: step 1798, loss 0.488613.
Train: 2018-08-02T12:52:37.184691: step 1799, loss 0.589513.
Train: 2018-08-02T12:52:37.340898: step 1800, loss 0.52701.
Test: 2018-08-02T12:52:37.793899: step 1800, loss 0.548262.
Train: 2018-08-02T12:52:38.450019: step 1801, loss 0.554263.
Train: 2018-08-02T12:52:38.590610: step 1802, loss 0.497192.
Train: 2018-08-02T12:52:38.746824: step 1803, loss 0.585443.
Train: 2018-08-02T12:52:38.903037: step 1804, loss 0.549362.
Train: 2018-08-02T12:52:39.043599: step 1805, loss 0.493833.
Train: 2018-08-02T12:52:39.199842: step 1806, loss 0.517607.
Train: 2018-08-02T12:52:39.356026: step 1807, loss 0.498396.
Train: 2018-08-02T12:52:39.512270: step 1808, loss 0.534815.
Train: 2018-08-02T12:52:39.652862: step 1809, loss 0.492246.
Train: 2018-08-02T12:52:39.824667: step 1810, loss 0.466516.
Test: 2018-08-02T12:52:40.277686: step 1810, loss 0.547364.
Train: 2018-08-02T12:52:40.418308: step 1811, loss 0.518478.
Train: 2018-08-02T12:52:40.527664: step 1812, loss 0.5348.
Train: 2018-08-02T12:52:40.683840: step 1813, loss 0.556118.
Train: 2018-08-02T12:52:40.840083: step 1814, loss 0.538959.
Train: 2018-08-02T12:52:40.996297: step 1815, loss 0.486896.
Train: 2018-08-02T12:52:41.136890: step 1816, loss 0.518394.
Train: 2018-08-02T12:52:41.293073: step 1817, loss 0.485548.
Train: 2018-08-02T12:52:41.449286: step 1818, loss 0.496251.
Train: 2018-08-02T12:52:41.589908: step 1819, loss 0.498627.
Train: 2018-08-02T12:52:41.746122: step 1820, loss 0.447988.
Test: 2018-08-02T12:52:42.199135: step 1820, loss 0.548598.
Train: 2018-08-02T12:52:42.355349: step 1821, loss 0.637917.
Train: 2018-08-02T12:52:42.511568: step 1822, loss 0.503542.
Train: 2018-08-02T12:52:42.652130: step 1823, loss 0.482956.
Train: 2018-08-02T12:52:42.808343: step 1824, loss 0.495727.
Train: 2018-08-02T12:52:42.964587: step 1825, loss 0.498153.
Train: 2018-08-02T12:52:43.105179: step 1826, loss 0.551889.
Train: 2018-08-02T12:52:43.261361: step 1827, loss 0.443388.
Train: 2018-08-02T12:52:43.417600: step 1828, loss 0.505572.
Train: 2018-08-02T12:52:43.573820: step 1829, loss 0.518978.
Train: 2018-08-02T12:52:43.714412: step 1830, loss 0.453409.
Test: 2018-08-02T12:52:44.183032: step 1830, loss 0.547338.
Train: 2018-08-02T12:52:44.323644: step 1831, loss 0.503995.
Train: 2018-08-02T12:52:44.479856: step 1832, loss 0.555344.
Train: 2018-08-02T12:52:44.636069: step 1833, loss 0.515162.
Train: 2018-08-02T12:52:44.776662: step 1834, loss 0.520851.
Train: 2018-08-02T12:52:44.932876: step 1835, loss 0.551506.
Train: 2018-08-02T12:52:45.089090: step 1836, loss 0.526819.
Train: 2018-08-02T12:52:45.229681: step 1837, loss 0.505187.
Train: 2018-08-02T12:52:45.385895: step 1838, loss 0.527016.
Train: 2018-08-02T12:52:45.542109: step 1839, loss 0.506186.
Train: 2018-08-02T12:52:45.682700: step 1840, loss 0.528329.
Test: 2018-08-02T12:52:46.151311: step 1840, loss 0.55025.
Train: 2018-08-02T12:52:46.291902: step 1841, loss 0.526973.
Train: 2018-08-02T12:52:46.448145: step 1842, loss 0.466881.
Train: 2018-08-02T12:52:46.604360: step 1843, loss 0.544419.
Train: 2018-08-02T12:52:46.760573: step 1844, loss 0.490522.
Train: 2018-08-02T12:52:46.916780: step 1845, loss 0.497041.
Train: 2018-08-02T12:52:47.057379: step 1846, loss 0.491193.
Train: 2018-08-02T12:52:47.213592: step 1847, loss 0.541275.
Train: 2018-08-02T12:52:47.354154: step 1848, loss 0.540842.
Train: 2018-08-02T12:52:47.510367: step 1849, loss 0.51729.
Train: 2018-08-02T12:52:47.666611: step 1850, loss 0.514024.
Test: 2018-08-02T12:52:48.119600: step 1850, loss 0.54957.
Train: 2018-08-02T12:52:48.275813: step 1851, loss 0.473815.
Train: 2018-08-02T12:52:48.416436: step 1852, loss 0.592439.
Train: 2018-08-02T12:52:48.572618: step 1853, loss 0.539871.
Train: 2018-08-02T12:52:48.728832: step 1854, loss 0.57245.
Train: 2018-08-02T12:52:48.885075: step 1855, loss 0.570304.
Train: 2018-08-02T12:52:49.025667: step 1856, loss 0.544963.
Train: 2018-08-02T12:52:49.181883: step 1857, loss 0.613714.
Train: 2018-08-02T12:52:49.338093: step 1858, loss 0.501062.
Train: 2018-08-02T12:52:49.478655: step 1859, loss 0.56196.
Train: 2018-08-02T12:52:49.634895: step 1860, loss 0.508734.
Test: 2018-08-02T12:52:50.103509: step 1860, loss 0.555976.
Train: 2018-08-02T12:52:50.244131: step 1861, loss 0.486587.
Train: 2018-08-02T12:52:50.400344: step 1862, loss 0.555022.
Train: 2018-08-02T12:52:50.540906: step 1863, loss 0.566726.
Train: 2018-08-02T12:52:50.697150: step 1864, loss 0.458739.
Train: 2018-08-02T12:52:50.853334: step 1865, loss 0.557061.
Train: 2018-08-02T12:52:50.993956: step 1866, loss 0.485944.
Train: 2018-08-02T12:52:51.150139: step 1867, loss 0.482773.
Train: 2018-08-02T12:52:51.306384: step 1868, loss 0.435236.
Train: 2018-08-02T12:52:51.462596: step 1869, loss 0.627026.
Train: 2018-08-02T12:52:51.603157: step 1870, loss 0.473863.
Test: 2018-08-02T12:52:52.071830: step 1870, loss 0.548729.
Train: 2018-08-02T12:52:52.212421: step 1871, loss 0.546191.
Train: 2018-08-02T12:52:52.368604: step 1872, loss 0.506759.
Train: 2018-08-02T12:52:52.524849: step 1873, loss 0.547119.
Train: 2018-08-02T12:52:52.681062: step 1874, loss 0.529796.
Train: 2018-08-02T12:52:52.821623: step 1875, loss 0.499014.
Train: 2018-08-02T12:52:52.977866: step 1876, loss 0.558937.
Train: 2018-08-02T12:52:53.134079: step 1877, loss 0.466345.
Train: 2018-08-02T12:52:53.290294: step 1878, loss 0.540416.
Train: 2018-08-02T12:52:53.430886: step 1879, loss 0.461538.
Train: 2018-08-02T12:52:53.587101: step 1880, loss 0.541824.
Test: 2018-08-02T12:52:54.040088: step 1880, loss 0.547541.
Train: 2018-08-02T12:52:54.196331: step 1881, loss 0.506335.
Train: 2018-08-02T12:52:54.336923: step 1882, loss 0.51592.
Train: 2018-08-02T12:52:54.493137: step 1883, loss 0.499724.
Train: 2018-08-02T12:52:54.649320: step 1884, loss 0.53676.
Train: 2018-08-02T12:52:54.789944: step 1885, loss 0.579524.
Train: 2018-08-02T12:52:54.946126: step 1886, loss 0.571296.
Train: 2018-08-02T12:52:55.102386: step 1887, loss 0.526037.
Train: 2018-08-02T12:52:55.242961: step 1888, loss 0.524319.
Train: 2018-08-02T12:52:55.399176: step 1889, loss 0.552732.
Train: 2018-08-02T12:52:55.555388: step 1890, loss 0.486338.
Test: 2018-08-02T12:52:56.008377: step 1890, loss 0.553412.
Train: 2018-08-02T12:52:56.164591: step 1891, loss 0.588239.
Train: 2018-08-02T12:52:56.305183: step 1892, loss 0.49924.
Train: 2018-08-02T12:52:56.461429: step 1893, loss 0.523354.
Train: 2018-08-02T12:52:56.617636: step 1894, loss 0.492792.
Train: 2018-08-02T12:52:56.773853: step 1895, loss 0.505904.
Train: 2018-08-02T12:52:56.914446: step 1896, loss 0.500853.
Train: 2018-08-02T12:52:57.070653: step 1897, loss 0.514906.
Train: 2018-08-02T12:52:57.226873: step 1898, loss 0.492238.
Train: 2018-08-02T12:52:57.367463: step 1899, loss 0.482243.
Train: 2018-08-02T12:52:57.523646: step 1900, loss 0.499796.
Test: 2018-08-02T12:52:57.976697: step 1900, loss 0.54592.
Train: 2018-08-02T12:52:58.601549: step 1901, loss 0.480663.
Train: 2018-08-02T12:52:58.757764: step 1902, loss 0.47205.
Train: 2018-08-02T12:52:58.913977: step 1903, loss 0.535882.
Train: 2018-08-02T12:52:59.054537: step 1904, loss 0.534636.
Train: 2018-08-02T12:52:59.210751: step 1905, loss 0.469869.
Train: 2018-08-02T12:52:59.366995: step 1906, loss 0.526753.
Train: 2018-08-02T12:52:59.507558: step 1907, loss 0.497231.
Train: 2018-08-02T12:52:59.663771: step 1908, loss 0.512718.
Train: 2018-08-02T12:52:59.804393: step 1909, loss 0.522798.
Train: 2018-08-02T12:52:59.960607: step 1910, loss 0.482248.
Test: 2018-08-02T12:53:00.413596: step 1910, loss 0.548331.
Train: 2018-08-02T12:53:00.569839: step 1911, loss 0.542177.
Train: 2018-08-02T12:53:00.726023: step 1912, loss 0.551677.
Train: 2018-08-02T12:53:00.866614: step 1913, loss 0.532961.
Train: 2018-08-02T12:53:01.022857: step 1914, loss 0.504915.
Train: 2018-08-02T12:53:01.179040: step 1915, loss 0.555409.
Train: 2018-08-02T12:53:01.319663: step 1916, loss 0.497061.
Train: 2018-08-02T12:53:01.475847: step 1917, loss 0.521966.
Train: 2018-08-02T12:53:01.632059: step 1918, loss 0.462227.
Train: 2018-08-02T12:53:01.772652: step 1919, loss 0.602956.
Train: 2018-08-02T12:53:01.928896: step 1920, loss 0.599948.
Test: 2018-08-02T12:53:02.381909: step 1920, loss 0.553697.
Train: 2018-08-02T12:53:02.538097: step 1921, loss 0.502878.
Train: 2018-08-02T12:53:02.694341: step 1922, loss 0.533268.
Train: 2018-08-02T12:53:02.850525: step 1923, loss 0.548912.
Train: 2018-08-02T12:53:02.991146: step 1924, loss 0.543004.
Train: 2018-08-02T12:53:03.147354: step 1925, loss 0.50903.
Train: 2018-08-02T12:53:03.303574: step 1926, loss 0.500906.
Train: 2018-08-02T12:53:03.444166: step 1927, loss 0.560409.
Train: 2018-08-02T12:53:03.600349: step 1928, loss 0.527393.
Train: 2018-08-02T12:53:03.756562: step 1929, loss 0.570526.
Train: 2018-08-02T12:53:03.897155: step 1930, loss 0.516244.
Test: 2018-08-02T12:53:04.365819: step 1930, loss 0.549221.
Train: 2018-08-02T12:53:04.522009: step 1931, loss 0.509955.
Train: 2018-08-02T12:53:04.662601: step 1932, loss 0.554855.
Train: 2018-08-02T12:53:04.818814: step 1933, loss 0.549484.
Train: 2018-08-02T12:53:04.975033: step 1934, loss 0.450411.
Train: 2018-08-02T12:53:05.115664: step 1935, loss 0.50838.
Train: 2018-08-02T12:53:05.271863: step 1936, loss 0.441703.
Train: 2018-08-02T12:53:05.412455: step 1937, loss 0.520482.
Train: 2018-08-02T12:53:05.568639: step 1938, loss 0.592837.
Train: 2018-08-02T12:53:05.724881: step 1939, loss 0.601721.
Train: 2018-08-02T12:53:05.865477: step 1940, loss 0.55672.
Test: 2018-08-02T12:53:06.334118: step 1940, loss 0.557322.
Train: 2018-08-02T12:53:06.474710: step 1941, loss 0.451842.
Train: 2018-08-02T12:53:06.630889: step 1942, loss 0.514144.
Train: 2018-08-02T12:53:06.787102: step 1943, loss 0.551838.
Train: 2018-08-02T12:53:06.927725: step 1944, loss 0.53183.
Train: 2018-08-02T12:53:07.083938: step 1945, loss 0.532492.
Train: 2018-08-02T12:53:07.240121: step 1946, loss 0.580571.
Train: 2018-08-02T12:53:07.396360: step 1947, loss 0.499047.
Train: 2018-08-02T12:53:07.536958: step 1948, loss 0.536505.
Train: 2018-08-02T12:53:07.693170: step 1949, loss 0.491278.
Train: 2018-08-02T12:53:07.849380: step 1950, loss 0.51879.
Test: 2018-08-02T12:53:08.302404: step 1950, loss 0.547664.
Train: 2018-08-02T12:53:08.458585: step 1951, loss 0.550168.
Train: 2018-08-02T12:53:08.599208: step 1952, loss 0.555005.
Train: 2018-08-02T12:53:08.755422: step 1953, loss 0.520583.
Train: 2018-08-02T12:53:08.911605: step 1954, loss 0.601265.
Train: 2018-08-02T12:53:09.052196: step 1955, loss 0.53488.
Train: 2018-08-02T12:53:09.208441: step 1956, loss 0.493587.
Train: 2018-08-02T12:53:09.364654: step 1957, loss 0.51612.
Train: 2018-08-02T12:53:09.520837: step 1958, loss 0.543773.
Train: 2018-08-02T12:53:09.661431: step 1959, loss 0.463237.
Train: 2018-08-02T12:53:09.817674: step 1960, loss 0.485601.
Test: 2018-08-02T12:53:10.270687: step 1960, loss 0.547924.
Train: 2018-08-02T12:53:10.426874: step 1961, loss 0.547473.
Train: 2018-08-02T12:53:10.567497: step 1962, loss 0.518389.
Train: 2018-08-02T12:53:10.676847: step 1963, loss 0.62182.
Train: 2018-08-02T12:53:10.833031: step 1964, loss 0.538447.
Train: 2018-08-02T12:53:10.989273: step 1965, loss 0.504302.
Train: 2018-08-02T12:53:11.129836: step 1966, loss 0.474451.
Train: 2018-08-02T12:53:11.286049: step 1967, loss 0.52001.
Train: 2018-08-02T12:53:11.442292: step 1968, loss 0.498651.
Train: 2018-08-02T12:53:11.598506: step 1969, loss 0.438913.
Train: 2018-08-02T12:53:11.739068: step 1970, loss 0.488345.
Test: 2018-08-02T12:53:12.207709: step 1970, loss 0.546803.
Train: 2018-08-02T12:53:12.348333: step 1971, loss 0.53376.
Train: 2018-08-02T12:53:12.504513: step 1972, loss 0.493125.
Train: 2018-08-02T12:53:12.660758: step 1973, loss 0.495958.
Train: 2018-08-02T12:53:12.801350: step 1974, loss 0.434534.
Train: 2018-08-02T12:53:12.957533: step 1975, loss 0.425118.
Train: 2018-08-02T12:53:13.113777: step 1976, loss 0.435742.
Train: 2018-08-02T12:53:13.269992: step 1977, loss 0.525206.
Train: 2018-08-02T12:53:13.410552: step 1978, loss 0.482833.
Train: 2018-08-02T12:53:13.566795: step 1979, loss 0.45571.
Train: 2018-08-02T12:53:13.723009: step 1980, loss 0.55705.
Test: 2018-08-02T12:53:14.176022: step 1980, loss 0.55501.
Train: 2018-08-02T12:53:14.332237: step 1981, loss 0.451185.
Train: 2018-08-02T12:53:14.472803: step 1982, loss 0.441351.
Train: 2018-08-02T12:53:14.629046: step 1983, loss 0.538818.
Train: 2018-08-02T12:53:14.785261: step 1984, loss 0.616923.
Train: 2018-08-02T12:53:14.925853: step 1985, loss 0.46325.
Train: 2018-08-02T12:53:15.082065: step 1986, loss 0.464563.
Train: 2018-08-02T12:53:15.238278: step 1987, loss 0.495422.
Train: 2018-08-02T12:53:15.394486: step 1988, loss 0.579815.
Train: 2018-08-02T12:53:15.535054: step 1989, loss 0.576683.
Train: 2018-08-02T12:53:15.691268: step 1990, loss 0.612902.
Test: 2018-08-02T12:53:16.144286: step 1990, loss 0.555402.
Train: 2018-08-02T12:53:16.300530: step 1991, loss 0.519192.
Train: 2018-08-02T12:53:16.456748: step 1992, loss 0.541703.
Train: 2018-08-02T12:53:16.597305: step 1993, loss 0.566536.
Train: 2018-08-02T12:53:16.753549: step 1994, loss 0.543298.
Train: 2018-08-02T12:53:16.909763: step 1995, loss 0.515682.
Train: 2018-08-02T12:53:17.050355: step 1996, loss 0.518307.
Train: 2018-08-02T12:53:17.206538: step 1997, loss 0.548034.
Train: 2018-08-02T12:53:17.358198: step 1998, loss 0.541782.
Train: 2018-08-02T12:53:17.514381: step 1999, loss 0.557801.
Train: 2018-08-02T12:53:17.670595: step 2000, loss 0.628295.
Test: 2018-08-02T12:53:18.123613: step 2000, loss 0.549044.
Train: 2018-08-02T12:53:18.779709: step 2001, loss 0.464318.
Train: 2018-08-02T12:53:18.935923: step 2002, loss 0.54871.
Train: 2018-08-02T12:53:19.092137: step 2003, loss 0.494823.
Train: 2018-08-02T12:53:19.232759: step 2004, loss 0.583445.
Train: 2018-08-02T12:53:19.388942: step 2005, loss 0.557827.
Train: 2018-08-02T12:53:19.545156: step 2006, loss 0.538703.
Train: 2018-08-02T12:53:19.701399: step 2007, loss 0.521693.
Train: 2018-08-02T12:53:19.857583: step 2008, loss 0.46183.
Train: 2018-08-02T12:53:19.998199: step 2009, loss 0.530641.
Train: 2018-08-02T12:53:20.170039: step 2010, loss 0.586024.
Test: 2018-08-02T12:53:20.623031: step 2010, loss 0.549346.
Train: 2018-08-02T12:53:20.779241: step 2011, loss 0.510551.
Train: 2018-08-02T12:53:20.919858: step 2012, loss 0.452764.
Train: 2018-08-02T12:53:21.076077: step 2013, loss 0.546817.
Train: 2018-08-02T12:53:21.232291: step 2014, loss 0.472738.
Train: 2018-08-02T12:53:21.372883: step 2015, loss 0.548505.
Train: 2018-08-02T12:53:21.529102: step 2016, loss 0.613001.
Train: 2018-08-02T12:53:21.685312: step 2017, loss 0.535437.
Train: 2018-08-02T12:53:21.825901: step 2018, loss 0.476011.
Train: 2018-08-02T12:53:21.982115: step 2019, loss 0.533248.
Train: 2018-08-02T12:53:22.138328: step 2020, loss 0.468615.
Test: 2018-08-02T12:53:22.591317: step 2020, loss 0.549874.
Train: 2018-08-02T12:53:22.731908: step 2021, loss 0.487909.
Train: 2018-08-02T12:53:22.888123: step 2022, loss 0.478298.
Train: 2018-08-02T12:53:23.044336: step 2023, loss 0.528472.
Train: 2018-08-02T12:53:23.184958: step 2024, loss 0.411383.
Train: 2018-08-02T12:53:23.341174: step 2025, loss 0.4727.
Train: 2018-08-02T12:53:23.497385: step 2026, loss 0.518491.
Train: 2018-08-02T12:53:23.653598: step 2027, loss 0.5696.
Train: 2018-08-02T12:53:23.794191: step 2028, loss 0.54255.
Train: 2018-08-02T12:53:23.950375: step 2029, loss 0.482419.
Train: 2018-08-02T12:53:24.106621: step 2030, loss 0.562336.
Test: 2018-08-02T12:53:24.559622: step 2030, loss 0.54853.
Train: 2018-08-02T12:53:24.700198: step 2031, loss 0.503536.
Train: 2018-08-02T12:53:24.856411: step 2032, loss 0.462686.
Train: 2018-08-02T12:53:25.012655: step 2033, loss 0.525407.
Train: 2018-08-02T12:53:25.153247: step 2034, loss 0.432773.
Train: 2018-08-02T12:53:25.309461: step 2035, loss 0.46967.
Train: 2018-08-02T12:53:25.465677: step 2036, loss 0.548667.
Train: 2018-08-02T12:53:25.606266: step 2037, loss 0.60877.
Train: 2018-08-02T12:53:25.762480: step 2038, loss 0.467887.
Train: 2018-08-02T12:53:25.918693: step 2039, loss 0.538929.
Train: 2018-08-02T12:53:26.059285: step 2040, loss 0.554704.
Test: 2018-08-02T12:53:26.512275: step 2040, loss 0.55015.
Train: 2018-08-02T12:53:26.668518: step 2041, loss 0.524406.
Train: 2018-08-02T12:53:26.824700: step 2042, loss 0.539453.
Train: 2018-08-02T12:53:26.980916: step 2043, loss 0.505554.
Train: 2018-08-02T12:53:27.121506: step 2044, loss 0.488058.
Train: 2018-08-02T12:53:27.277719: step 2045, loss 0.565193.
Train: 2018-08-02T12:53:27.433963: step 2046, loss 0.557896.
Train: 2018-08-02T12:53:27.574555: step 2047, loss 0.518829.
Train: 2018-08-02T12:53:27.730765: step 2048, loss 0.566736.
Train: 2018-08-02T12:53:27.886982: step 2049, loss 0.494246.
Train: 2018-08-02T12:53:28.043195: step 2050, loss 0.455845.
Test: 2018-08-02T12:53:28.496212: step 2050, loss 0.546454.
Train: 2018-08-02T12:53:28.652398: step 2051, loss 0.511446.
Train: 2018-08-02T12:53:28.792990: step 2052, loss 0.669748.
Train: 2018-08-02T12:53:28.949204: step 2053, loss 0.631478.
Train: 2018-08-02T12:53:29.105422: step 2054, loss 0.574776.
Train: 2018-08-02T12:53:29.246046: step 2055, loss 0.506224.
Train: 2018-08-02T12:53:29.402252: step 2056, loss 0.561316.
Train: 2018-08-02T12:53:29.542815: step 2057, loss 0.589759.
Train: 2018-08-02T12:53:29.699058: step 2058, loss 0.563239.
Train: 2018-08-02T12:53:29.855271: step 2059, loss 0.543598.
Train: 2018-08-02T12:53:29.995863: step 2060, loss 0.518185.
Test: 2018-08-02T12:53:30.464498: step 2060, loss 0.561003.
Train: 2018-08-02T12:53:30.605095: step 2061, loss 0.558351.
Train: 2018-08-02T12:53:30.761309: step 2062, loss 0.518051.
Train: 2018-08-02T12:53:30.901901: step 2063, loss 0.563316.
Train: 2018-08-02T12:53:31.058085: step 2064, loss 0.500173.
Train: 2018-08-02T12:53:31.214298: step 2065, loss 0.588879.
Train: 2018-08-02T12:53:31.370511: step 2066, loss 0.475795.
Train: 2018-08-02T12:53:31.511133: step 2067, loss 0.630091.
Train: 2018-08-02T12:53:31.667317: step 2068, loss 0.559374.
Train: 2018-08-02T12:53:31.823564: step 2069, loss 0.516082.
Train: 2018-08-02T12:53:31.964155: step 2070, loss 0.541949.
Test: 2018-08-02T12:53:32.417169: step 2070, loss 0.552099.
Train: 2018-08-02T12:53:32.573385: step 2071, loss 0.463721.
Train: 2018-08-02T12:53:32.729598: step 2072, loss 0.518386.
Train: 2018-08-02T12:53:32.885811: step 2073, loss 0.509401.
Train: 2018-08-02T12:53:33.026404: step 2074, loss 0.533685.
Train: 2018-08-02T12:53:33.182618: step 2075, loss 0.517079.
Train: 2018-08-02T12:53:33.323211: step 2076, loss 0.538499.
Train: 2018-08-02T12:53:33.479422: step 2077, loss 0.578446.
Train: 2018-08-02T12:53:33.635606: step 2078, loss 0.540697.
Train: 2018-08-02T12:53:33.776197: step 2079, loss 0.556194.
Train: 2018-08-02T12:53:33.932441: step 2080, loss 0.438807.
Test: 2018-08-02T12:53:34.385431: step 2080, loss 0.549553.
Train: 2018-08-02T12:53:34.541673: step 2081, loss 0.571795.
Train: 2018-08-02T12:53:34.682235: step 2082, loss 0.512572.
Train: 2018-08-02T12:53:34.838448: step 2083, loss 0.579097.
Train: 2018-08-02T12:53:34.994694: step 2084, loss 0.507607.
Train: 2018-08-02T12:53:35.135285: step 2085, loss 0.526007.
Train: 2018-08-02T12:53:35.291495: step 2086, loss 0.449609.
Train: 2018-08-02T12:53:35.447709: step 2087, loss 0.594991.
Train: 2018-08-02T12:53:35.603896: step 2088, loss 0.472078.
Train: 2018-08-02T12:53:35.744517: step 2089, loss 0.49006.
Train: 2018-08-02T12:53:35.900730: step 2090, loss 0.463203.
Test: 2018-08-02T12:53:36.353720: step 2090, loss 0.548172.
Train: 2018-08-02T12:53:36.509958: step 2091, loss 0.46739.
Train: 2018-08-02T12:53:36.666145: step 2092, loss 0.572737.
Train: 2018-08-02T12:53:36.806737: step 2093, loss 0.558025.
Train: 2018-08-02T12:53:36.962984: step 2094, loss 0.545074.
Train: 2018-08-02T12:53:37.119165: step 2095, loss 0.448736.
Train: 2018-08-02T12:53:37.275404: step 2096, loss 0.620405.
Train: 2018-08-02T12:53:37.416000: step 2097, loss 0.557729.
Train: 2018-08-02T12:53:37.572217: step 2098, loss 0.539913.
Train: 2018-08-02T12:53:37.728428: step 2099, loss 0.509566.
Train: 2018-08-02T12:53:37.869019: step 2100, loss 0.562963.
Test: 2018-08-02T12:53:38.337635: step 2100, loss 0.559233.
Train: 2018-08-02T12:53:38.962516: step 2101, loss 0.548533.
Train: 2018-08-02T12:53:39.118727: step 2102, loss 0.559664.
Train: 2018-08-02T12:53:39.274911: step 2103, loss 0.548254.
Train: 2018-08-02T12:53:39.415502: step 2104, loss 0.544936.
Train: 2018-08-02T12:53:39.571716: step 2105, loss 0.455521.
Train: 2018-08-02T12:53:39.727960: step 2106, loss 0.495158.
Train: 2018-08-02T12:53:39.884142: step 2107, loss 0.544833.
Train: 2018-08-02T12:53:40.024765: step 2108, loss 0.445088.
Train: 2018-08-02T12:53:40.180978: step 2109, loss 0.498413.
Train: 2018-08-02T12:53:40.337194: step 2110, loss 0.491219.
Test: 2018-08-02T12:53:40.790211: step 2110, loss 0.558658.
Train: 2018-08-02T12:53:40.946395: step 2111, loss 0.484545.
Train: 2018-08-02T12:53:41.102609: step 2112, loss 0.505609.
Train: 2018-08-02T12:53:41.243199: step 2113, loss 0.633652.
Train: 2018-08-02T12:53:41.352579: step 2114, loss 0.531358.
Train: 2018-08-02T12:53:41.508763: step 2115, loss 0.482807.
Train: 2018-08-02T12:53:41.649355: step 2116, loss 0.493743.
Train: 2018-08-02T12:53:41.805598: step 2117, loss 0.573466.
Train: 2018-08-02T12:53:41.961814: step 2118, loss 0.501556.
Train: 2018-08-02T12:53:42.102403: step 2119, loss 0.460653.
Train: 2018-08-02T12:53:42.258617: step 2120, loss 0.48562.
Test: 2018-08-02T12:53:42.711618: step 2120, loss 0.54708.
Train: 2018-08-02T12:53:42.867849: step 2121, loss 0.49018.
Train: 2018-08-02T12:53:43.024062: step 2122, loss 0.458238.
Train: 2018-08-02T12:53:43.164626: step 2123, loss 0.590797.
Train: 2018-08-02T12:53:43.320868: step 2124, loss 0.500316.
Train: 2018-08-02T12:53:43.477082: step 2125, loss 0.467349.
Train: 2018-08-02T12:53:43.617673: step 2126, loss 0.497688.
Train: 2018-08-02T12:53:43.773887: step 2127, loss 0.51463.
Train: 2018-08-02T12:53:43.930100: step 2128, loss 0.524269.
Train: 2018-08-02T12:53:44.070692: step 2129, loss 0.568634.
Train: 2018-08-02T12:53:44.226906: step 2130, loss 0.517518.
Test: 2018-08-02T12:53:44.695515: step 2130, loss 0.549262.
Train: 2018-08-02T12:53:44.836141: step 2131, loss 0.469001.
Train: 2018-08-02T12:53:44.992321: step 2132, loss 0.504306.
Train: 2018-08-02T12:53:45.148535: step 2133, loss 0.444539.
Train: 2018-08-02T12:53:45.289127: step 2134, loss 0.497263.
Train: 2018-08-02T12:53:45.445370: step 2135, loss 0.489564.
Train: 2018-08-02T12:53:45.601584: step 2136, loss 0.49183.
Train: 2018-08-02T12:53:45.742176: step 2137, loss 0.628958.
Train: 2018-08-02T12:53:45.898390: step 2138, loss 0.449678.
Train: 2018-08-02T12:53:46.054606: step 2139, loss 0.537383.
Train: 2018-08-02T12:53:46.210812: step 2140, loss 0.490971.
Test: 2018-08-02T12:53:46.663811: step 2140, loss 0.549093.
Train: 2018-08-02T12:53:46.820019: step 2141, loss 0.505565.
Train: 2018-08-02T12:53:46.960643: step 2142, loss 0.492894.
Train: 2018-08-02T12:53:47.116854: step 2143, loss 0.475778.
Train: 2018-08-02T12:53:47.273067: step 2144, loss 0.599196.
Train: 2018-08-02T12:53:47.413659: step 2145, loss 0.574176.
Train: 2018-08-02T12:53:47.569876: step 2146, loss 0.49867.
Train: 2018-08-02T12:53:47.726087: step 2147, loss 0.490963.
Train: 2018-08-02T12:53:47.866649: step 2148, loss 0.581586.
Train: 2018-08-02T12:53:48.022862: step 2149, loss 0.545244.
Train: 2018-08-02T12:53:48.179105: step 2150, loss 0.491176.
Test: 2018-08-02T12:53:48.632119: step 2150, loss 0.552494.
Train: 2018-08-02T12:53:48.772716: step 2151, loss 0.459711.
Train: 2018-08-02T12:53:48.928900: step 2152, loss 0.611917.
Train: 2018-08-02T12:53:49.085113: step 2153, loss 0.564369.
Train: 2018-08-02T12:53:49.225735: step 2154, loss 0.505709.
Train: 2018-08-02T12:53:49.381919: step 2155, loss 0.565438.
Train: 2018-08-02T12:53:49.538132: step 2156, loss 0.570964.
Train: 2018-08-02T12:53:49.678725: step 2157, loss 0.524982.
Train: 2018-08-02T12:53:49.834938: step 2158, loss 0.546609.
Train: 2018-08-02T12:53:49.991151: step 2159, loss 0.526918.
Train: 2018-08-02T12:53:50.131773: step 2160, loss 0.49511.
Test: 2018-08-02T12:53:50.600383: step 2160, loss 0.552264.
Train: 2018-08-02T12:53:50.756596: step 2161, loss 0.531514.
Train: 2018-08-02T12:53:50.897221: step 2162, loss 0.542951.
Train: 2018-08-02T12:53:51.053402: step 2163, loss 0.472313.
Train: 2018-08-02T12:53:51.209614: step 2164, loss 0.530132.
Train: 2018-08-02T12:53:51.350207: step 2165, loss 0.617599.
Train: 2018-08-02T12:53:51.506451: step 2166, loss 0.52931.
Train: 2018-08-02T12:53:51.662664: step 2167, loss 0.471294.
Train: 2018-08-02T12:53:51.818848: step 2168, loss 0.481592.
Train: 2018-08-02T12:53:51.959440: step 2169, loss 0.625003.
Train: 2018-08-02T12:53:52.115683: step 2170, loss 0.569258.
Test: 2018-08-02T12:53:52.568673: step 2170, loss 0.548057.
Train: 2018-08-02T12:53:52.724886: step 2171, loss 0.489358.
Train: 2018-08-02T12:53:52.865477: step 2172, loss 0.52981.
Train: 2018-08-02T12:53:53.021721: step 2173, loss 0.509238.
Train: 2018-08-02T12:53:53.177934: step 2174, loss 0.529666.
Train: 2018-08-02T12:53:53.334142: step 2175, loss 0.536324.
Train: 2018-08-02T12:53:53.474742: step 2176, loss 0.508822.
Train: 2018-08-02T12:53:53.630948: step 2177, loss 0.494543.
Train: 2018-08-02T12:53:53.787166: step 2178, loss 0.453805.
Train: 2018-08-02T12:53:53.943381: step 2179, loss 0.51452.
Train: 2018-08-02T12:53:54.083966: step 2180, loss 0.567662.
Test: 2018-08-02T12:53:54.552583: step 2180, loss 0.546523.
Train: 2018-08-02T12:53:54.693205: step 2181, loss 0.509734.
Train: 2018-08-02T12:53:54.849389: step 2182, loss 0.471889.
Train: 2018-08-02T12:53:55.005631: step 2183, loss 0.498388.
Train: 2018-08-02T12:53:55.161845: step 2184, loss 0.474236.
Train: 2018-08-02T12:53:55.302437: step 2185, loss 0.543769.
Train: 2018-08-02T12:53:55.458653: step 2186, loss 0.544241.
Train: 2018-08-02T12:53:55.614864: step 2187, loss 0.482463.
Train: 2018-08-02T12:53:55.755456: step 2188, loss 0.530172.
Train: 2018-08-02T12:53:55.911669: step 2189, loss 0.595045.
Train: 2018-08-02T12:53:56.067853: step 2190, loss 0.529408.
Test: 2018-08-02T12:53:56.520903: step 2190, loss 0.548316.
Train: 2018-08-02T12:53:56.661494: step 2191, loss 0.466255.
Train: 2018-08-02T12:53:56.817678: step 2192, loss 0.505466.
Train: 2018-08-02T12:53:56.973890: step 2193, loss 0.54189.
Train: 2018-08-02T12:53:57.130103: step 2194, loss 0.537193.
Train: 2018-08-02T12:53:57.270726: step 2195, loss 0.526862.
Train: 2018-08-02T12:53:57.426940: step 2196, loss 0.465783.
Train: 2018-08-02T12:53:57.567532: step 2197, loss 0.516581.
Train: 2018-08-02T12:53:57.723744: step 2198, loss 0.61638.
Train: 2018-08-02T12:53:57.879928: step 2199, loss 0.520034.
Train: 2018-08-02T12:53:58.020551: step 2200, loss 0.549134.
Test: 2018-08-02T12:53:58.489191: step 2200, loss 0.551016.
Train: 2018-08-02T12:53:59.160908: step 2201, loss 0.505041.
Train: 2018-08-02T12:53:59.317122: step 2202, loss 0.552207.
Train: 2018-08-02T12:53:59.473306: step 2203, loss 0.481358.
Train: 2018-08-02T12:53:59.613927: step 2204, loss 0.521733.
Train: 2018-08-02T12:53:59.770111: step 2205, loss 0.507741.
Train: 2018-08-02T12:53:59.926354: step 2206, loss 0.558266.
Train: 2018-08-02T12:54:00.066940: step 2207, loss 0.493219.
Train: 2018-08-02T12:54:00.223160: step 2208, loss 0.554333.
Train: 2018-08-02T12:54:00.379373: step 2209, loss 0.488033.
Train: 2018-08-02T12:54:00.519936: step 2210, loss 0.568823.
Test: 2018-08-02T12:54:00.988600: step 2210, loss 0.550554.
Train: 2018-08-02T12:54:01.129197: step 2211, loss 0.481244.
Train: 2018-08-02T12:54:01.285412: step 2212, loss 0.540287.
Train: 2018-08-02T12:54:01.441625: step 2213, loss 0.573861.
Train: 2018-08-02T12:54:01.582220: step 2214, loss 0.533618.
Train: 2018-08-02T12:54:01.738425: step 2215, loss 0.625613.
Train: 2018-08-02T12:54:01.894643: step 2216, loss 0.518361.
Train: 2018-08-02T12:54:02.050857: step 2217, loss 0.531739.
Train: 2018-08-02T12:54:02.191449: step 2218, loss 0.506329.
Train: 2018-08-02T12:54:02.347658: step 2219, loss 0.509498.
Train: 2018-08-02T12:54:02.503875: step 2220, loss 0.498443.
Test: 2018-08-02T12:54:02.956865: step 2220, loss 0.551513.
Train: 2018-08-02T12:54:03.113108: step 2221, loss 0.435069.
Train: 2018-08-02T12:54:03.253701: step 2222, loss 0.548477.
Train: 2018-08-02T12:54:03.409884: step 2223, loss 0.534577.
Train: 2018-08-02T12:54:03.566126: step 2224, loss 0.520615.
Train: 2018-08-02T12:54:03.706690: step 2225, loss 0.48801.
Train: 2018-08-02T12:54:03.862932: step 2226, loss 0.509131.
Train: 2018-08-02T12:54:04.019116: step 2227, loss 0.494016.
Train: 2018-08-02T12:54:04.159738: step 2228, loss 0.541409.
Train: 2018-08-02T12:54:04.315951: step 2229, loss 0.449381.
Train: 2018-08-02T12:54:04.472137: step 2230, loss 0.580293.
Test: 2018-08-02T12:54:04.925154: step 2230, loss 0.547948.
Train: 2018-08-02T12:54:05.081391: step 2231, loss 0.554527.
Train: 2018-08-02T12:54:05.221959: step 2232, loss 0.562694.
Train: 2018-08-02T12:54:05.378203: step 2233, loss 0.471398.
Train: 2018-08-02T12:54:05.534416: step 2234, loss 0.526991.
Train: 2018-08-02T12:54:05.674978: step 2235, loss 0.57503.
Train: 2018-08-02T12:54:05.831191: step 2236, loss 0.53012.
Train: 2018-08-02T12:54:05.987434: step 2237, loss 0.519089.
Train: 2018-08-02T12:54:06.128027: step 2238, loss 0.463241.
Train: 2018-08-02T12:54:06.284240: step 2239, loss 0.511356.
Train: 2018-08-02T12:54:06.440449: step 2240, loss 0.518369.
Test: 2018-08-02T12:54:06.893444: step 2240, loss 0.548181.
Train: 2018-08-02T12:54:07.049680: step 2241, loss 0.509935.
Train: 2018-08-02T12:54:07.190278: step 2242, loss 0.510393.
Train: 2018-08-02T12:54:07.346491: step 2243, loss 0.496502.
Train: 2018-08-02T12:54:07.502708: step 2244, loss 0.620101.
Train: 2018-08-02T12:54:07.643266: step 2245, loss 0.57914.
Train: 2018-08-02T12:54:07.799511: step 2246, loss 0.470338.
Train: 2018-08-02T12:54:07.955694: step 2247, loss 0.497585.
Train: 2018-08-02T12:54:08.111938: step 2248, loss 0.492554.
Train: 2018-08-02T12:54:08.252500: step 2249, loss 0.563164.
Train: 2018-08-02T12:54:08.408743: step 2250, loss 0.596438.
Test: 2018-08-02T12:54:08.861733: step 2250, loss 0.554423.
Train: 2018-08-02T12:54:09.017979: step 2251, loss 0.532393.
Train: 2018-08-02T12:54:09.158567: step 2252, loss 0.491472.
Train: 2018-08-02T12:54:09.314781: step 2253, loss 0.449963.
Train: 2018-08-02T12:54:09.470988: step 2254, loss 0.500887.
Train: 2018-08-02T12:54:09.611586: step 2255, loss 0.562428.
Train: 2018-08-02T12:54:09.767773: step 2256, loss 0.534551.
Train: 2018-08-02T12:54:09.923999: step 2257, loss 0.566486.
Train: 2018-08-02T12:54:10.064582: step 2258, loss 0.487246.
Train: 2018-08-02T12:54:10.220819: step 2259, loss 0.506271.
Train: 2018-08-02T12:54:10.377032: step 2260, loss 0.534293.
Test: 2018-08-02T12:54:10.830021: step 2260, loss 0.548369.
Train: 2018-08-02T12:54:10.986264: step 2261, loss 0.505378.
Train: 2018-08-02T12:54:11.126856: step 2262, loss 0.489467.
Train: 2018-08-02T12:54:11.283040: step 2263, loss 0.454463.
Train: 2018-08-02T12:54:11.439283: step 2264, loss 0.507344.
Train: 2018-08-02T12:54:11.533011: step 2265, loss 0.500204.
Train: 2018-08-02T12:54:11.689226: step 2266, loss 0.498611.
Train: 2018-08-02T12:54:11.845438: step 2267, loss 0.475945.
Train: 2018-08-02T12:54:11.986030: step 2268, loss 0.568665.
Train: 2018-08-02T12:54:12.142243: step 2269, loss 0.457197.
Train: 2018-08-02T12:54:12.298457: step 2270, loss 0.570858.
Test: 2018-08-02T12:54:12.751447: step 2270, loss 0.549374.
Train: 2018-08-02T12:54:12.907690: step 2271, loss 0.481277.
Train: 2018-08-02T12:54:13.048282: step 2272, loss 0.479802.
Train: 2018-08-02T12:54:13.204498: step 2273, loss 0.454062.
Train: 2018-08-02T12:54:13.360709: step 2274, loss 0.617834.
Train: 2018-08-02T12:54:13.501300: step 2275, loss 0.528665.
Train: 2018-08-02T12:54:13.657484: step 2276, loss 0.585855.
Train: 2018-08-02T12:54:13.798075: step 2277, loss 0.522149.
Train: 2018-08-02T12:54:13.954290: step 2278, loss 0.539219.
Train: 2018-08-02T12:54:14.110528: step 2279, loss 0.484697.
Train: 2018-08-02T12:54:14.266715: step 2280, loss 0.570797.
Test: 2018-08-02T12:54:14.719760: step 2280, loss 0.55458.
Train: 2018-08-02T12:54:14.860328: step 2281, loss 0.459468.
Train: 2018-08-02T12:54:15.016571: step 2282, loss 0.422653.
Train: 2018-08-02T12:54:15.172784: step 2283, loss 0.480682.
Train: 2018-08-02T12:54:15.329001: step 2284, loss 0.44909.
Train: 2018-08-02T12:54:15.469584: step 2285, loss 0.488819.
Train: 2018-08-02T12:54:15.625804: step 2286, loss 0.523475.
Train: 2018-08-02T12:54:15.782017: step 2287, loss 0.494789.
Train: 2018-08-02T12:54:15.922578: step 2288, loss 0.437021.
Train: 2018-08-02T12:54:16.078824: step 2289, loss 0.559269.
Train: 2018-08-02T12:54:16.235030: step 2290, loss 0.473151.
Test: 2018-08-02T12:54:16.688049: step 2290, loss 0.551175.
Train: 2018-08-02T12:54:16.844267: step 2291, loss 0.545769.
Train: 2018-08-02T12:54:16.984860: step 2292, loss 0.552619.
Train: 2018-08-02T12:54:17.141072: step 2293, loss 0.49195.
Train: 2018-08-02T12:54:17.297286: step 2294, loss 0.475763.
Train: 2018-08-02T12:54:17.437878: step 2295, loss 0.459579.
Train: 2018-08-02T12:54:17.594092: step 2296, loss 0.525232.
Train: 2018-08-02T12:54:17.750275: step 2297, loss 0.571754.
Train: 2018-08-02T12:54:17.890898: step 2298, loss 0.558357.
Train: 2018-08-02T12:54:18.047114: step 2299, loss 0.507326.
Train: 2018-08-02T12:54:18.203327: step 2300, loss 0.552738.
Test: 2018-08-02T12:54:18.656313: step 2300, loss 0.550742.
Train: 2018-08-02T12:54:19.328063: step 2301, loss 0.539111.
Train: 2018-08-02T12:54:19.484276: step 2302, loss 0.566081.
Train: 2018-08-02T12:54:19.640457: step 2303, loss 0.442345.
Train: 2018-08-02T12:54:19.781080: step 2304, loss 0.511187.
Train: 2018-08-02T12:54:19.937264: step 2305, loss 0.537842.
Train: 2018-08-02T12:54:20.093507: step 2306, loss 0.51226.
Train: 2018-08-02T12:54:20.234099: step 2307, loss 0.556307.
Train: 2018-08-02T12:54:20.390312: step 2308, loss 0.564108.
Train: 2018-08-02T12:54:20.530908: step 2309, loss 0.47601.
Train: 2018-08-02T12:54:20.687117: step 2310, loss 0.629081.
Test: 2018-08-02T12:54:21.155728: step 2310, loss 0.550702.
Train: 2018-08-02T12:54:21.296320: step 2311, loss 0.586153.
Train: 2018-08-02T12:54:21.452566: step 2312, loss 0.559253.
Train: 2018-08-02T12:54:21.608777: step 2313, loss 0.553989.
Train: 2018-08-02T12:54:21.749369: step 2314, loss 0.506506.
Train: 2018-08-02T12:54:21.905583: step 2315, loss 0.54985.
Train: 2018-08-02T12:54:22.061766: step 2316, loss 0.479149.
Train: 2018-08-02T12:54:22.202390: step 2317, loss 0.523776.
Train: 2018-08-02T12:54:22.358573: step 2318, loss 0.448773.
Train: 2018-08-02T12:54:22.514784: step 2319, loss 0.510109.
Train: 2018-08-02T12:54:22.671028: step 2320, loss 0.439431.
Test: 2018-08-02T12:54:23.124019: step 2320, loss 0.548475.
Train: 2018-08-02T12:54:23.280261: step 2321, loss 0.516072.
Train: 2018-08-02T12:54:23.420852: step 2322, loss 0.49093.
Train: 2018-08-02T12:54:23.577069: step 2323, loss 0.547592.
Train: 2018-08-02T12:54:23.733282: step 2324, loss 0.496737.
Train: 2018-08-02T12:54:23.873841: step 2325, loss 0.605489.
Train: 2018-08-02T12:54:24.030085: step 2326, loss 0.482845.
Train: 2018-08-02T12:54:24.186298: step 2327, loss 0.483721.
Train: 2018-08-02T12:54:24.326893: step 2328, loss 0.606342.
Train: 2018-08-02T12:54:24.483103: step 2329, loss 0.55936.
Train: 2018-08-02T12:54:24.639318: step 2330, loss 0.539916.
Test: 2018-08-02T12:54:25.092307: step 2330, loss 0.551979.
Train: 2018-08-02T12:54:25.248544: step 2331, loss 0.550089.
Train: 2018-08-02T12:54:25.389112: step 2332, loss 0.540678.
Train: 2018-08-02T12:54:25.545324: step 2333, loss 0.522967.
Train: 2018-08-02T12:54:25.701571: step 2334, loss 0.568958.
Train: 2018-08-02T12:54:25.842131: step 2335, loss 0.479697.
Train: 2018-08-02T12:54:25.998374: step 2336, loss 0.643668.
Train: 2018-08-02T12:54:26.154558: step 2337, loss 0.489278.
Train: 2018-08-02T12:54:26.310801: step 2338, loss 0.540723.
Train: 2018-08-02T12:54:26.451396: step 2339, loss 0.5535.
Train: 2018-08-02T12:54:26.607606: step 2340, loss 0.498654.
Test: 2018-08-02T12:54:27.060635: step 2340, loss 0.548235.
Train: 2018-08-02T12:54:27.216839: step 2341, loss 0.507819.
Train: 2018-08-02T12:54:27.373052: step 2342, loss 0.495612.
Train: 2018-08-02T12:54:27.529265: step 2343, loss 0.566083.
Train: 2018-08-02T12:54:27.669857: step 2344, loss 0.409964.
Train: 2018-08-02T12:54:27.826041: step 2345, loss 0.426905.
Train: 2018-08-02T12:54:27.982284: step 2346, loss 0.557999.
Train: 2018-08-02T12:54:28.122877: step 2347, loss 0.520249.
Train: 2018-08-02T12:54:28.279059: step 2348, loss 0.555145.
Train: 2018-08-02T12:54:28.435273: step 2349, loss 0.530954.
Train: 2018-08-02T12:54:28.591487: step 2350, loss 0.585609.
Test: 2018-08-02T12:54:29.044507: step 2350, loss 0.560469.
Train: 2018-08-02T12:54:29.185098: step 2351, loss 0.468925.
Train: 2018-08-02T12:54:29.341341: step 2352, loss 0.573415.
Train: 2018-08-02T12:54:29.497554: step 2353, loss 0.566052.
Train: 2018-08-02T12:54:29.638147: step 2354, loss 0.543331.
Train: 2018-08-02T12:54:29.794360: step 2355, loss 0.493885.
Train: 2018-08-02T12:54:29.950542: step 2356, loss 0.52097.
Train: 2018-08-02T12:54:30.091135: step 2357, loss 0.535736.
Train: 2018-08-02T12:54:30.247379: step 2358, loss 0.525335.
Train: 2018-08-02T12:54:30.403563: step 2359, loss 0.493039.
Train: 2018-08-02T12:54:30.544184: step 2360, loss 0.567555.
Test: 2018-08-02T12:54:31.012794: step 2360, loss 0.548591.
Train: 2018-08-02T12:54:31.153411: step 2361, loss 0.518106.
Train: 2018-08-02T12:54:31.309600: step 2362, loss 0.501364.
Train: 2018-08-02T12:54:31.512677: step 2363, loss 0.476771.
Train: 2018-08-02T12:54:31.668892: step 2364, loss 0.512955.
Train: 2018-08-02T12:54:31.809482: step 2365, loss 0.540131.
Train: 2018-08-02T12:54:31.965696: step 2366, loss 0.54628.
Train: 2018-08-02T12:54:32.121909: step 2367, loss 0.548074.
Train: 2018-08-02T12:54:32.262532: step 2368, loss 0.547317.
Train: 2018-08-02T12:54:32.418715: step 2369, loss 0.613368.
Train: 2018-08-02T12:54:32.574961: step 2370, loss 0.552983.
Test: 2018-08-02T12:54:33.027979: step 2370, loss 0.564346.
Train: 2018-08-02T12:54:33.184185: step 2371, loss 0.521056.
Train: 2018-08-02T12:54:33.324786: step 2372, loss 0.505445.
Train: 2018-08-02T12:54:33.480996: step 2373, loss 0.488397.
Train: 2018-08-02T12:54:33.621589: step 2374, loss 0.592581.
Train: 2018-08-02T12:54:33.777802: step 2375, loss 0.573463.
Train: 2018-08-02T12:54:33.934015: step 2376, loss 0.546558.
Train: 2018-08-02T12:54:34.090229: step 2377, loss 0.499691.
Train: 2018-08-02T12:54:34.230815: step 2378, loss 0.503988.
Train: 2018-08-02T12:54:34.387035: step 2379, loss 0.468006.
Train: 2018-08-02T12:54:34.543217: step 2380, loss 0.484107.
Test: 2018-08-02T12:54:34.996237: step 2380, loss 0.548982.
Train: 2018-08-02T12:54:35.199347: step 2381, loss 0.560304.
Train: 2018-08-02T12:54:35.355528: step 2382, loss 0.550309.
Train: 2018-08-02T12:54:35.496150: step 2383, loss 0.530396.
Train: 2018-08-02T12:54:35.652364: step 2384, loss 0.523272.
Train: 2018-08-02T12:54:35.808577: step 2385, loss 0.623332.
Train: 2018-08-02T12:54:35.964790: step 2386, loss 0.558202.
Train: 2018-08-02T12:54:36.105383: step 2387, loss 0.567239.
Train: 2018-08-02T12:54:36.261596: step 2388, loss 0.518532.
Train: 2018-08-02T12:54:36.417810: step 2389, loss 0.549698.
Train: 2018-08-02T12:54:36.558401: step 2390, loss 0.558374.
Test: 2018-08-02T12:54:37.027039: step 2390, loss 0.568281.
Train: 2018-08-02T12:54:37.167633: step 2391, loss 0.551775.
Train: 2018-08-02T12:54:37.323816: step 2392, loss 0.502648.
Train: 2018-08-02T12:54:37.480060: step 2393, loss 0.472985.
Train: 2018-08-02T12:54:37.636274: step 2394, loss 0.526466.
Train: 2018-08-02T12:54:37.776865: step 2395, loss 0.626164.
Train: 2018-08-02T12:54:37.933079: step 2396, loss 0.469297.
Train: 2018-08-02T12:54:38.089263: step 2397, loss 0.540856.
Train: 2018-08-02T12:54:38.229885: step 2398, loss 0.482846.
Train: 2018-08-02T12:54:38.386067: step 2399, loss 0.547077.
Train: 2018-08-02T12:54:38.542311: step 2400, loss 0.45578.
Test: 2018-08-02T12:54:38.995324: step 2400, loss 0.548766.
Train: 2018-08-02T12:54:39.635805: step 2401, loss 0.545164.
Train: 2018-08-02T12:54:39.791989: step 2402, loss 0.519336.
Train: 2018-08-02T12:54:39.948235: step 2403, loss 0.572351.
Train: 2018-08-02T12:54:40.104447: step 2404, loss 0.526155.
Train: 2018-08-02T12:54:40.245038: step 2405, loss 0.581005.
Train: 2018-08-02T12:54:40.401251: step 2406, loss 0.499356.
Train: 2018-08-02T12:54:40.557465: step 2407, loss 0.60508.
Train: 2018-08-02T12:54:40.698059: step 2408, loss 0.537736.
Train: 2018-08-02T12:54:40.854240: step 2409, loss 0.546547.
Train: 2018-08-02T12:54:41.010483: step 2410, loss 0.524237.
Test: 2018-08-02T12:54:41.463473: step 2410, loss 0.56638.
Train: 2018-08-02T12:54:41.619711: step 2411, loss 0.534716.
Train: 2018-08-02T12:54:41.760304: step 2412, loss 0.511771.
Train: 2018-08-02T12:54:41.916517: step 2413, loss 0.562496.
Train: 2018-08-02T12:54:42.072738: step 2414, loss 0.521875.
Train: 2018-08-02T12:54:42.228948: step 2415, loss 0.507291.
Train: 2018-08-02T12:54:42.338292: step 2416, loss 0.428818.
Train: 2018-08-02T12:54:42.494511: step 2417, loss 0.575783.
Train: 2018-08-02T12:54:42.635103: step 2418, loss 0.584464.
Train: 2018-08-02T12:54:42.791316: step 2419, loss 0.484349.
Train: 2018-08-02T12:54:42.947500: step 2420, loss 0.458869.
Test: 2018-08-02T12:54:43.400519: step 2420, loss 0.55274.
Train: 2018-08-02T12:54:43.556762: step 2421, loss 0.573053.
Train: 2018-08-02T12:54:43.697356: step 2422, loss 0.485842.
Train: 2018-08-02T12:54:43.853537: step 2423, loss 0.473307.
Train: 2018-08-02T12:54:44.009776: step 2424, loss 0.462662.
Train: 2018-08-02T12:54:44.165995: step 2425, loss 0.609017.
Train: 2018-08-02T12:54:44.306557: step 2426, loss 0.507693.
Train: 2018-08-02T12:54:44.462769: step 2427, loss 0.482992.
Train: 2018-08-02T12:54:44.619013: step 2428, loss 0.578345.
Train: 2018-08-02T12:54:44.759575: step 2429, loss 0.520555.
Train: 2018-08-02T12:54:44.915820: step 2430, loss 0.500482.
Test: 2018-08-02T12:54:45.368809: step 2430, loss 0.560138.
Train: 2018-08-02T12:54:45.525051: step 2431, loss 0.553868.
Train: 2018-08-02T12:54:45.681265: step 2432, loss 0.569266.
Train: 2018-08-02T12:54:45.821827: step 2433, loss 0.50564.
Train: 2018-08-02T12:54:45.978070: step 2434, loss 0.471048.
Train: 2018-08-02T12:54:46.134254: step 2435, loss 0.518874.
Train: 2018-08-02T12:54:46.274875: step 2436, loss 0.552172.
Train: 2018-08-02T12:54:46.431089: step 2437, loss 0.57307.
Train: 2018-08-02T12:54:46.587302: step 2438, loss 0.504995.
Train: 2018-08-02T12:54:46.743486: step 2439, loss 0.498727.
Train: 2018-08-02T12:54:46.884078: step 2440, loss 0.47062.
Test: 2018-08-02T12:54:47.337097: step 2440, loss 0.554896.
Train: 2018-08-02T12:54:47.493340: step 2441, loss 0.453268.
Train: 2018-08-02T12:54:47.649524: step 2442, loss 0.59643.
Train: 2018-08-02T12:54:47.805762: step 2443, loss 0.488596.
Train: 2018-08-02T12:54:47.946328: step 2444, loss 0.395431.
Train: 2018-08-02T12:54:48.102542: step 2445, loss 0.503637.
Train: 2018-08-02T12:54:48.258787: step 2446, loss 0.530327.
Train: 2018-08-02T12:54:48.414999: step 2447, loss 0.500821.
Train: 2018-08-02T12:54:48.555592: step 2448, loss 0.478419.
Train: 2018-08-02T12:54:48.711805: step 2449, loss 0.55647.
Train: 2018-08-02T12:54:48.868018: step 2450, loss 0.580889.
Test: 2018-08-02T12:54:49.321008: step 2450, loss 0.551982.
Train: 2018-08-02T12:54:49.461629: step 2451, loss 0.451804.
Train: 2018-08-02T12:54:49.617843: step 2452, loss 0.498431.
Train: 2018-08-02T12:54:49.774056: step 2453, loss 0.494033.
Train: 2018-08-02T12:54:49.930239: step 2454, loss 0.603561.
Train: 2018-08-02T12:54:50.070864: step 2455, loss 0.497758.
Train: 2018-08-02T12:54:50.227075: step 2456, loss 0.481383.
Train: 2018-08-02T12:54:50.383289: step 2457, loss 0.482694.
Train: 2018-08-02T12:54:50.523880: step 2458, loss 0.536323.
Train: 2018-08-02T12:54:50.680088: step 2459, loss 0.472346.
Train: 2018-08-02T12:54:50.836307: step 2460, loss 0.466358.
Test: 2018-08-02T12:54:51.289344: step 2460, loss 0.549662.
Train: 2018-08-02T12:54:51.445542: step 2461, loss 0.404603.
Train: 2018-08-02T12:54:51.601751: step 2462, loss 0.543156.
Train: 2018-08-02T12:54:51.742344: step 2463, loss 0.566552.
Train: 2018-08-02T12:54:51.898529: step 2464, loss 0.495636.
Train: 2018-08-02T12:54:52.054772: step 2465, loss 0.534186.
Train: 2018-08-02T12:54:52.195334: step 2466, loss 0.616726.
Train: 2018-08-02T12:54:52.351577: step 2467, loss 0.540171.
Train: 2018-08-02T12:54:52.507790: step 2468, loss 0.565945.
Train: 2018-08-02T12:54:52.648384: step 2469, loss 0.562912.
Train: 2018-08-02T12:54:52.804566: step 2470, loss 0.610985.
Test: 2018-08-02T12:54:53.257586: step 2470, loss 0.580508.
Train: 2018-08-02T12:54:53.413829: step 2471, loss 0.528641.
Train: 2018-08-02T12:54:53.570013: step 2472, loss 0.525091.
Train: 2018-08-02T12:54:53.710634: step 2473, loss 0.52551.
Train: 2018-08-02T12:54:53.866817: step 2474, loss 0.549069.
Train: 2018-08-02T12:54:54.023062: step 2475, loss 0.531119.
Train: 2018-08-02T12:54:54.179274: step 2476, loss 0.49694.
Train: 2018-08-02T12:54:54.319862: step 2477, loss 0.516993.
Train: 2018-08-02T12:54:54.476050: step 2478, loss 0.602036.
Train: 2018-08-02T12:54:54.632263: step 2479, loss 0.536338.
Train: 2018-08-02T12:54:54.772890: step 2480, loss 0.554047.
Test: 2018-08-02T12:54:55.241496: step 2480, loss 0.555632.
Train: 2018-08-02T12:54:55.397710: step 2481, loss 0.571926.
Train: 2018-08-02T12:54:55.553953: step 2482, loss 0.532157.
Train: 2018-08-02T12:54:55.694547: step 2483, loss 0.489709.
Train: 2018-08-02T12:54:55.850759: step 2484, loss 0.545244.
Train: 2018-08-02T12:54:56.006971: step 2485, loss 0.492848.
Train: 2018-08-02T12:54:56.147534: step 2486, loss 0.562675.
Train: 2018-08-02T12:54:56.303747: step 2487, loss 0.537517.
Train: 2018-08-02T12:54:56.459961: step 2488, loss 0.570105.
Train: 2018-08-02T12:54:56.600553: step 2489, loss 0.513008.
Train: 2018-08-02T12:54:56.756797: step 2490, loss 0.517173.
Test: 2018-08-02T12:54:57.209816: step 2490, loss 0.553794.
Train: 2018-08-02T12:54:57.366028: step 2491, loss 0.467409.
Train: 2018-08-02T12:54:57.522242: step 2492, loss 0.515596.
Train: 2018-08-02T12:54:57.662834: step 2493, loss 0.55621.
Train: 2018-08-02T12:54:57.819050: step 2494, loss 0.519623.
Train: 2018-08-02T12:54:57.975255: step 2495, loss 0.53494.
Train: 2018-08-02T12:54:58.115853: step 2496, loss 0.538857.
Train: 2018-08-02T12:54:58.272035: step 2497, loss 0.592365.
Train: 2018-08-02T12:54:58.428250: step 2498, loss 0.50712.
Train: 2018-08-02T12:54:58.568874: step 2499, loss 0.516536.
Train: 2018-08-02T12:54:58.725085: step 2500, loss 0.557858.
Test: 2018-08-02T12:54:59.193697: step 2500, loss 0.550207.
Train: 2018-08-02T12:54:59.834200: step 2501, loss 0.452113.
Train: 2018-08-02T12:54:59.990414: step 2502, loss 0.567117.
Train: 2018-08-02T12:55:00.131006: step 2503, loss 0.553808.
Train: 2018-08-02T12:55:00.287220: step 2504, loss 0.541313.
Train: 2018-08-02T12:55:00.443436: step 2505, loss 0.528351.
Train: 2018-08-02T12:55:00.599646: step 2506, loss 0.485842.
Train: 2018-08-02T12:55:00.740209: step 2507, loss 0.547609.
Train: 2018-08-02T12:55:00.896421: step 2508, loss 0.571054.
Train: 2018-08-02T12:55:01.052667: step 2509, loss 0.525994.
Train: 2018-08-02T12:55:01.193260: step 2510, loss 0.432186.
Test: 2018-08-02T12:55:01.661867: step 2510, loss 0.550266.
Train: 2018-08-02T12:55:01.802459: step 2511, loss 0.51158.
Train: 2018-08-02T12:55:01.958703: step 2512, loss 0.540427.
Train: 2018-08-02T12:55:02.114916: step 2513, loss 0.58494.
Train: 2018-08-02T12:55:02.255479: step 2514, loss 0.551167.
Train: 2018-08-02T12:55:02.411692: step 2515, loss 0.542157.
Train: 2018-08-02T12:55:02.567944: step 2516, loss 0.503551.
Train: 2018-08-02T12:55:02.724149: step 2517, loss 0.481826.
Train: 2018-08-02T12:55:02.864711: step 2518, loss 0.48958.
Train: 2018-08-02T12:55:03.020954: step 2519, loss 0.5056.
Train: 2018-08-02T12:55:03.177138: step 2520, loss 0.45361.
Test: 2018-08-02T12:55:03.630182: step 2520, loss 0.54768.
Train: 2018-08-02T12:55:03.786369: step 2521, loss 0.530338.
Train: 2018-08-02T12:55:03.926991: step 2522, loss 0.632715.
Train: 2018-08-02T12:55:04.083176: step 2523, loss 0.49475.
Train: 2018-08-02T12:55:04.239420: step 2524, loss 0.533047.
Train: 2018-08-02T12:55:04.380011: step 2525, loss 0.440065.
Train: 2018-08-02T12:55:04.536228: step 2526, loss 0.566719.
Train: 2018-08-02T12:55:04.692440: step 2527, loss 0.527825.
Train: 2018-08-02T12:55:04.833031: step 2528, loss 0.521011.
Train: 2018-08-02T12:55:04.989244: step 2529, loss 0.510245.
Train: 2018-08-02T12:55:05.145457: step 2530, loss 0.576739.
Test: 2018-08-02T12:55:05.598471: step 2530, loss 0.553206.
Train: 2018-08-02T12:55:05.754659: step 2531, loss 0.503219.
Train: 2018-08-02T12:55:05.895250: step 2532, loss 0.534085.
Train: 2018-08-02T12:55:06.051489: step 2533, loss 0.546154.
Train: 2018-08-02T12:55:06.207708: step 2534, loss 0.494299.
Train: 2018-08-02T12:55:06.363921: step 2535, loss 0.514484.
Train: 2018-08-02T12:55:06.504513: step 2536, loss 0.472625.
Train: 2018-08-02T12:55:06.660697: step 2537, loss 0.515881.
Train: 2018-08-02T12:55:06.816943: step 2538, loss 0.641979.
Train: 2018-08-02T12:55:06.957532: step 2539, loss 0.580194.
Train: 2018-08-02T12:55:07.113746: step 2540, loss 0.539856.
Test: 2018-08-02T12:55:07.566735: step 2540, loss 0.562195.
Train: 2018-08-02T12:55:07.722979: step 2541, loss 0.56262.
Train: 2018-08-02T12:55:07.879191: step 2542, loss 0.503073.
Train: 2018-08-02T12:55:08.035375: step 2543, loss 0.481891.
Train: 2018-08-02T12:55:08.175997: step 2544, loss 0.491155.
Train: 2018-08-02T12:55:08.332212: step 2545, loss 0.529436.
Train: 2018-08-02T12:55:08.488424: step 2546, loss 0.495702.
Train: 2018-08-02T12:55:08.629015: step 2547, loss 0.608604.
Train: 2018-08-02T12:55:08.785230: step 2548, loss 0.533577.
Train: 2018-08-02T12:55:08.941445: step 2549, loss 0.564381.
Train: 2018-08-02T12:55:09.082004: step 2550, loss 0.55135.
Test: 2018-08-02T12:55:09.550669: step 2550, loss 0.551222.
Train: 2018-08-02T12:55:09.706858: step 2551, loss 0.509538.
Train: 2018-08-02T12:55:09.847451: step 2552, loss 0.571407.
Train: 2018-08-02T12:55:10.003694: step 2553, loss 0.534402.
Train: 2018-08-02T12:55:10.144286: step 2554, loss 0.488731.
Train: 2018-08-02T12:55:10.300500: step 2555, loss 0.466464.
Train: 2018-08-02T12:55:10.456713: step 2556, loss 0.510036.
Train: 2018-08-02T12:55:10.612927: step 2557, loss 0.51254.
Train: 2018-08-02T12:55:10.753489: step 2558, loss 0.437083.
Train: 2018-08-02T12:55:10.909734: step 2559, loss 0.522316.
Train: 2018-08-02T12:55:11.065916: step 2560, loss 0.444711.
Test: 2018-08-02T12:55:11.518935: step 2560, loss 0.551914.
Train: 2018-08-02T12:55:11.675177: step 2561, loss 0.452875.
Train: 2018-08-02T12:55:11.815772: step 2562, loss 0.478286.
Train: 2018-08-02T12:55:11.971983: step 2563, loss 0.562847.
Train: 2018-08-02T12:55:12.128196: step 2564, loss 0.508441.
Train: 2018-08-02T12:55:12.268788: step 2565, loss 0.514471.
Train: 2018-08-02T12:55:12.424972: step 2566, loss 0.510154.
Train: 2018-08-02T12:55:12.534352: step 2567, loss 0.553729.
Train: 2018-08-02T12:55:12.690568: step 2568, loss 0.565334.
Train: 2018-08-02T12:55:12.846779: step 2569, loss 0.563755.
Train: 2018-08-02T12:55:12.987371: step 2570, loss 0.555898.
Test: 2018-08-02T12:55:13.456005: step 2570, loss 0.554533.
Train: 2018-08-02T12:55:13.596572: step 2571, loss 0.471378.
Train: 2018-08-02T12:55:13.752816: step 2572, loss 0.54099.
Train: 2018-08-02T12:55:13.909037: step 2573, loss 0.511855.
Train: 2018-08-02T12:55:14.065246: step 2574, loss 0.53075.
Train: 2018-08-02T12:55:14.221425: step 2575, loss 0.479165.
Train: 2018-08-02T12:55:14.362048: step 2576, loss 0.512348.
Train: 2018-08-02T12:55:14.518262: step 2577, loss 0.590974.
Train: 2018-08-02T12:55:14.658824: step 2578, loss 0.435174.
Train: 2018-08-02T12:55:14.815038: step 2579, loss 0.479312.
Train: 2018-08-02T12:55:14.971281: step 2580, loss 0.461985.
Test: 2018-08-02T12:55:15.439892: step 2580, loss 0.548907.
Train: 2018-08-02T12:55:15.580513: step 2581, loss 0.509647.
Train: 2018-08-02T12:55:15.736726: step 2582, loss 0.477398.
Train: 2018-08-02T12:55:15.892939: step 2583, loss 0.603937.
Train: 2018-08-02T12:55:16.033532: step 2584, loss 0.536611.
Train: 2018-08-02T12:55:16.189748: step 2585, loss 0.475425.
Train: 2018-08-02T12:55:16.345959: step 2586, loss 0.48384.
Train: 2018-08-02T12:55:16.486551: step 2587, loss 0.523044.
Train: 2018-08-02T12:55:16.642765: step 2588, loss 0.529034.
Train: 2018-08-02T12:55:16.798978: step 2589, loss 0.422384.
Train: 2018-08-02T12:55:16.939539: step 2590, loss 0.48509.
Test: 2018-08-02T12:55:17.392558: step 2590, loss 0.547171.
Train: 2018-08-02T12:55:17.548803: step 2591, loss 0.530937.
Train: 2018-08-02T12:55:17.705015: step 2592, loss 0.552003.
Train: 2018-08-02T12:55:17.861229: step 2593, loss 0.563352.
Train: 2018-08-02T12:55:18.001822: step 2594, loss 0.542052.
Train: 2018-08-02T12:55:18.158034: step 2595, loss 0.500006.
Train: 2018-08-02T12:55:18.314250: step 2596, loss 0.592334.
Train: 2018-08-02T12:55:18.454810: step 2597, loss 0.524379.
Train: 2018-08-02T12:55:18.611058: step 2598, loss 0.561493.
Train: 2018-08-02T12:55:18.767267: step 2599, loss 0.523013.
Train: 2018-08-02T12:55:18.907858: step 2600, loss 0.524189.
Test: 2018-08-02T12:55:19.376469: step 2600, loss 0.560952.
Train: 2018-08-02T12:55:19.985702: step 2601, loss 0.536604.
Train: 2018-08-02T12:55:20.141945: step 2602, loss 0.515241.
Train: 2018-08-02T12:55:20.298158: step 2603, loss 0.577091.
Train: 2018-08-02T12:55:20.454372: step 2604, loss 0.550395.
Train: 2018-08-02T12:55:20.594934: step 2605, loss 0.603795.
Train: 2018-08-02T12:55:20.751177: step 2606, loss 0.475076.
Train: 2018-08-02T12:55:20.907390: step 2607, loss 0.557431.
Train: 2018-08-02T12:55:21.047983: step 2608, loss 0.53389.
Train: 2018-08-02T12:55:21.204197: step 2609, loss 0.500632.
Train: 2018-08-02T12:55:21.360380: step 2610, loss 0.482525.
Test: 2018-08-02T12:55:21.813432: step 2610, loss 0.553341.
Train: 2018-08-02T12:55:21.969612: step 2611, loss 0.556614.
Train: 2018-08-02T12:55:22.110204: step 2612, loss 0.510117.
Train: 2018-08-02T12:55:22.266448: step 2613, loss 0.44695.
Train: 2018-08-02T12:55:22.422661: step 2614, loss 0.493314.
Train: 2018-08-02T12:55:22.563255: step 2615, loss 0.458069.
Train: 2018-08-02T12:55:22.719467: step 2616, loss 0.488193.
Train: 2018-08-02T12:55:22.875650: step 2617, loss 0.47487.
Train: 2018-08-02T12:55:23.031862: step 2618, loss 0.538782.
Train: 2018-08-02T12:55:23.172485: step 2619, loss 0.597556.
Train: 2018-08-02T12:55:23.328698: step 2620, loss 0.505927.
Test: 2018-08-02T12:55:23.781714: step 2620, loss 0.5567.
Train: 2018-08-02T12:55:23.937901: step 2621, loss 0.454116.
Train: 2018-08-02T12:55:24.094115: step 2622, loss 0.483262.
Train: 2018-08-02T12:55:24.234736: step 2623, loss 0.489628.
Train: 2018-08-02T12:55:24.390919: step 2624, loss 0.55569.
Train: 2018-08-02T12:55:24.547163: step 2625, loss 0.533901.
Train: 2018-08-02T12:55:24.703378: step 2626, loss 0.528075.
Train: 2018-08-02T12:55:24.843938: step 2627, loss 0.567148.
Train: 2018-08-02T12:55:25.000182: step 2628, loss 0.549008.
Train: 2018-08-02T12:55:25.156395: step 2629, loss 0.554557.
Train: 2018-08-02T12:55:25.296957: step 2630, loss 0.486564.
Test: 2018-08-02T12:55:25.765597: step 2630, loss 0.555687.
Train: 2018-08-02T12:55:25.906214: step 2631, loss 0.528981.
Train: 2018-08-02T12:55:26.062436: step 2632, loss 0.458452.
Train: 2018-08-02T12:55:26.218647: step 2633, loss 0.497118.
Train: 2018-08-02T12:55:26.359239: step 2634, loss 0.502667.
Train: 2018-08-02T12:55:26.515423: step 2635, loss 0.527483.
Train: 2018-08-02T12:55:26.671636: step 2636, loss 0.580824.
Train: 2018-08-02T12:55:26.827850: step 2637, loss 0.53771.
Train: 2018-08-02T12:55:26.968473: step 2638, loss 0.501427.
Train: 2018-08-02T12:55:27.124684: step 2639, loss 0.497429.
Train: 2018-08-02T12:55:27.280869: step 2640, loss 0.435119.
Test: 2018-08-02T12:55:27.733918: step 2640, loss 0.551718.
Train: 2018-08-02T12:55:27.890100: step 2641, loss 0.536769.
Train: 2018-08-02T12:55:28.030693: step 2642, loss 0.467121.
Train: 2018-08-02T12:55:28.186938: step 2643, loss 0.499633.
Train: 2018-08-02T12:55:28.343119: step 2644, loss 0.626622.
Train: 2018-08-02T12:55:28.499363: step 2645, loss 0.62621.
Train: 2018-08-02T12:55:28.639955: step 2646, loss 0.571004.
Train: 2018-08-02T12:55:28.796171: step 2647, loss 0.478788.
Train: 2018-08-02T12:55:28.952381: step 2648, loss 0.51618.
Train: 2018-08-02T12:55:29.092973: step 2649, loss 0.564527.
Train: 2018-08-02T12:55:29.249188: step 2650, loss 0.51789.
Test: 2018-08-02T12:55:29.702176: step 2650, loss 0.564784.
Train: 2018-08-02T12:55:29.858390: step 2651, loss 0.54144.
Train: 2018-08-02T12:55:30.014633: step 2652, loss 0.550187.
Train: 2018-08-02T12:55:30.170815: step 2653, loss 0.491936.
Train: 2018-08-02T12:55:30.311441: step 2654, loss 0.594492.
Train: 2018-08-02T12:55:30.467652: step 2655, loss 0.548546.
Train: 2018-08-02T12:55:30.623866: step 2656, loss 0.469455.
Train: 2018-08-02T12:55:30.764428: step 2657, loss 0.477361.
Train: 2018-08-02T12:55:30.920671: step 2658, loss 0.551994.
Train: 2018-08-02T12:55:31.076884: step 2659, loss 0.489493.
Train: 2018-08-02T12:55:31.217476: step 2660, loss 0.452287.
Test: 2018-08-02T12:55:31.686091: step 2660, loss 0.54671.
Train: 2018-08-02T12:55:31.842330: step 2661, loss 0.602537.
Train: 2018-08-02T12:55:31.982922: step 2662, loss 0.611463.
Train: 2018-08-02T12:55:32.139106: step 2663, loss 0.55761.
Train: 2018-08-02T12:55:32.295349: step 2664, loss 0.543283.
Train: 2018-08-02T12:55:32.435911: step 2665, loss 0.513723.
Train: 2018-08-02T12:55:32.592154: step 2666, loss 0.590968.
Train: 2018-08-02T12:55:32.748367: step 2667, loss 0.487329.
Train: 2018-08-02T12:55:32.904581: step 2668, loss 0.512584.
Train: 2018-08-02T12:55:33.045143: step 2669, loss 0.546455.
Train: 2018-08-02T12:55:33.201387: step 2670, loss 0.497875.
Test: 2018-08-02T12:55:33.670025: step 2670, loss 0.553423.
Train: 2018-08-02T12:55:33.810589: step 2671, loss 0.573562.
Train: 2018-08-02T12:55:33.966833: step 2672, loss 0.55068.
Train: 2018-08-02T12:55:34.123046: step 2673, loss 0.479881.
Train: 2018-08-02T12:55:34.263608: step 2674, loss 0.503103.
Train: 2018-08-02T12:55:34.419821: step 2675, loss 0.489869.
Train: 2018-08-02T12:55:34.576064: step 2676, loss 0.577477.
Train: 2018-08-02T12:55:34.716627: step 2677, loss 0.558599.
Train: 2018-08-02T12:55:34.872870: step 2678, loss 0.490257.
Train: 2018-08-02T12:55:35.029055: step 2679, loss 0.556814.
Train: 2018-08-02T12:55:35.169676: step 2680, loss 0.57121.
Test: 2018-08-02T12:55:35.638311: step 2680, loss 0.556687.
Train: 2018-08-02T12:55:35.778908: step 2681, loss 0.550252.
Train: 2018-08-02T12:55:35.935116: step 2682, loss 0.48572.
Train: 2018-08-02T12:55:36.091335: step 2683, loss 0.495131.
Train: 2018-08-02T12:55:36.231896: step 2684, loss 0.49139.
Train: 2018-08-02T12:55:36.388141: step 2685, loss 0.44578.
Train: 2018-08-02T12:55:36.544354: step 2686, loss 0.560573.
Train: 2018-08-02T12:55:36.700567: step 2687, loss 0.573107.
Train: 2018-08-02T12:55:36.841160: step 2688, loss 0.477405.
Train: 2018-08-02T12:55:36.997374: step 2689, loss 0.5077.
Train: 2018-08-02T12:55:37.153586: step 2690, loss 0.422595.
Test: 2018-08-02T12:55:37.606574: step 2690, loss 0.546142.
Train: 2018-08-02T12:55:37.762789: step 2691, loss 0.623631.
Train: 2018-08-02T12:55:37.903380: step 2692, loss 0.51314.
Train: 2018-08-02T12:55:38.059594: step 2693, loss 0.564067.
Train: 2018-08-02T12:55:38.215838: step 2694, loss 0.513876.
Train: 2018-08-02T12:55:38.356429: step 2695, loss 0.531613.
Train: 2018-08-02T12:55:38.512644: step 2696, loss 0.502944.
Train: 2018-08-02T12:55:38.668827: step 2697, loss 0.548019.
Train: 2018-08-02T12:55:38.825065: step 2698, loss 0.530591.
Train: 2018-08-02T12:55:38.981283: step 2699, loss 0.495128.
Train: 2018-08-02T12:55:39.121844: step 2700, loss 0.550968.
Test: 2018-08-02T12:55:39.590488: step 2700, loss 0.548039.
Train: 2018-08-02T12:55:40.230994: step 2701, loss 0.621144.
Train: 2018-08-02T12:55:40.387173: step 2702, loss 0.481379.
Train: 2018-08-02T12:55:40.543388: step 2703, loss 0.561642.
Train: 2018-08-02T12:55:40.699601: step 2704, loss 0.506749.
Train: 2018-08-02T12:55:40.840222: step 2705, loss 0.592546.
Train: 2018-08-02T12:55:40.996406: step 2706, loss 0.596878.
Train: 2018-08-02T12:55:41.152650: step 2707, loss 0.495355.
Train: 2018-08-02T12:55:41.293242: step 2708, loss 0.503263.
Train: 2018-08-02T12:55:41.449455: step 2709, loss 0.535491.
Train: 2018-08-02T12:55:41.605668: step 2710, loss 0.466025.
Test: 2018-08-02T12:55:42.058657: step 2710, loss 0.550653.
Train: 2018-08-02T12:55:42.214901: step 2711, loss 0.515565.
Train: 2018-08-02T12:55:42.355493: step 2712, loss 0.609526.
Train: 2018-08-02T12:55:42.511707: step 2713, loss 0.583775.
Train: 2018-08-02T12:55:42.667919: step 2714, loss 0.522591.
Train: 2018-08-02T12:55:42.824133: step 2715, loss 0.485316.
Train: 2018-08-02T12:55:42.964726: step 2716, loss 0.578893.
Train: 2018-08-02T12:55:43.120938: step 2717, loss 0.574953.
Train: 2018-08-02T12:55:43.230288: step 2718, loss 0.441852.
Train: 2018-08-02T12:55:43.386501: step 2719, loss 0.500939.
Train: 2018-08-02T12:55:43.527094: step 2720, loss 0.552955.
Test: 2018-08-02T12:55:43.980084: step 2720, loss 0.547558.
Train: 2018-08-02T12:55:44.136326: step 2721, loss 0.50885.
Train: 2018-08-02T12:55:44.292510: step 2722, loss 0.476583.
Train: 2018-08-02T12:55:44.433134: step 2723, loss 0.519402.
Train: 2018-08-02T12:55:44.589345: step 2724, loss 0.522421.
Train: 2018-08-02T12:55:44.745558: step 2725, loss 0.613055.
Train: 2018-08-02T12:55:44.886151: step 2726, loss 0.571871.
Train: 2018-08-02T12:55:45.042364: step 2727, loss 0.516624.
Train: 2018-08-02T12:55:45.198577: step 2728, loss 0.547662.
Train: 2018-08-02T12:55:45.339139: step 2729, loss 0.520032.
Train: 2018-08-02T12:55:45.495385: step 2730, loss 0.52125.
Test: 2018-08-02T12:55:45.948375: step 2730, loss 0.563566.
Train: 2018-08-02T12:55:46.104584: step 2731, loss 0.550184.
Train: 2018-08-02T12:55:46.245207: step 2732, loss 0.480772.
Train: 2018-08-02T12:55:46.401421: step 2733, loss 0.605382.
Train: 2018-08-02T12:55:46.557636: step 2734, loss 0.552868.
Train: 2018-08-02T12:55:46.713848: step 2735, loss 0.511475.
Train: 2018-08-02T12:55:46.854443: step 2736, loss 0.638738.
Train: 2018-08-02T12:55:47.010622: step 2737, loss 0.5131.
Train: 2018-08-02T12:55:47.166837: step 2738, loss 0.592779.
Train: 2018-08-02T12:55:47.307429: step 2739, loss 0.503066.
Train: 2018-08-02T12:55:47.463672: step 2740, loss 0.536759.
Test: 2018-08-02T12:55:47.916662: step 2740, loss 0.558123.
Train: 2018-08-02T12:55:48.072905: step 2741, loss 0.520327.
Train: 2018-08-02T12:55:48.229118: step 2742, loss 0.453928.
Train: 2018-08-02T12:55:48.369709: step 2743, loss 0.522213.
Train: 2018-08-02T12:55:48.525918: step 2744, loss 0.474117.
Train: 2018-08-02T12:55:48.682139: step 2745, loss 0.535479.
Train: 2018-08-02T12:55:48.822713: step 2746, loss 0.630211.
Train: 2018-08-02T12:55:48.978942: step 2747, loss 0.524688.
Train: 2018-08-02T12:55:49.135155: step 2748, loss 0.535742.
Train: 2018-08-02T12:55:49.291363: step 2749, loss 0.540022.
Train: 2018-08-02T12:55:49.431931: step 2750, loss 0.573702.
Test: 2018-08-02T12:55:49.900601: step 2750, loss 0.562045.
Train: 2018-08-02T12:55:50.041163: step 2751, loss 0.512457.
Train: 2018-08-02T12:55:50.197406: step 2752, loss 0.590219.
Train: 2018-08-02T12:55:50.353589: step 2753, loss 0.502657.
Train: 2018-08-02T12:55:50.494212: step 2754, loss 0.55465.
Train: 2018-08-02T12:55:50.650426: step 2755, loss 0.519675.
Train: 2018-08-02T12:55:50.806608: step 2756, loss 0.465272.
Train: 2018-08-02T12:55:50.947231: step 2757, loss 0.551182.
Train: 2018-08-02T12:55:51.103446: step 2758, loss 0.518371.
Train: 2018-08-02T12:55:51.259658: step 2759, loss 0.439885.
Train: 2018-08-02T12:55:51.400250: step 2760, loss 0.437571.
Test: 2018-08-02T12:55:51.868886: step 2760, loss 0.550481.
Train: 2018-08-02T12:55:52.025073: step 2761, loss 0.569482.
Train: 2018-08-02T12:55:52.165666: step 2762, loss 0.448303.
Train: 2018-08-02T12:55:52.321908: step 2763, loss 0.649514.
Train: 2018-08-02T12:55:52.478122: step 2764, loss 0.484199.
Train: 2018-08-02T12:55:52.618714: step 2765, loss 0.522136.
Train: 2018-08-02T12:55:52.774928: step 2766, loss 0.477163.
Train: 2018-08-02T12:55:52.931144: step 2767, loss 0.530141.
Train: 2018-08-02T12:55:53.087351: step 2768, loss 0.558097.
Train: 2018-08-02T12:55:53.227951: step 2769, loss 0.482513.
Train: 2018-08-02T12:55:53.384161: step 2770, loss 0.519881.
Test: 2018-08-02T12:55:53.837149: step 2770, loss 0.548224.
Train: 2018-08-02T12:55:53.993392: step 2771, loss 0.531191.
Train: 2018-08-02T12:55:54.149576: step 2772, loss 0.590091.
Train: 2018-08-02T12:55:54.290167: step 2773, loss 0.501193.
Train: 2018-08-02T12:55:54.446412: step 2774, loss 0.523017.
Train: 2018-08-02T12:55:54.602625: step 2775, loss 0.56476.
Train: 2018-08-02T12:55:54.743218: step 2776, loss 0.490882.
Train: 2018-08-02T12:55:54.899431: step 2777, loss 0.468962.
Train: 2018-08-02T12:55:55.055646: step 2778, loss 0.580732.
Train: 2018-08-02T12:55:55.196235: step 2779, loss 0.473878.
Train: 2018-08-02T12:55:55.352449: step 2780, loss 0.44755.
Test: 2018-08-02T12:55:55.821059: step 2780, loss 0.547679.
Train: 2018-08-02T12:55:55.961682: step 2781, loss 0.472179.
Train: 2018-08-02T12:55:56.117896: step 2782, loss 0.55514.
Train: 2018-08-02T12:55:56.274109: step 2783, loss 0.514103.
Train: 2018-08-02T12:55:56.414700: step 2784, loss 0.474215.
Train: 2018-08-02T12:55:56.570914: step 2785, loss 0.509117.
Train: 2018-08-02T12:55:56.727127: step 2786, loss 0.528268.
Train: 2018-08-02T12:55:56.867719: step 2787, loss 0.545654.
Train: 2018-08-02T12:55:57.023933: step 2788, loss 0.549114.
Train: 2018-08-02T12:55:57.180150: step 2789, loss 0.46166.
Train: 2018-08-02T12:55:57.320708: step 2790, loss 0.491518.
Test: 2018-08-02T12:55:57.773728: step 2790, loss 0.551177.
Train: 2018-08-02T12:55:57.929974: step 2791, loss 0.520484.
Train: 2018-08-02T12:55:58.086183: step 2792, loss 0.515918.
Train: 2018-08-02T12:55:58.242393: step 2793, loss 0.496211.
Train: 2018-08-02T12:55:58.382960: step 2794, loss 0.475043.
Train: 2018-08-02T12:55:58.539203: step 2795, loss 0.436242.
Train: 2018-08-02T12:55:58.695417: step 2796, loss 0.449091.
Train: 2018-08-02T12:55:58.836009: step 2797, loss 0.491005.
Train: 2018-08-02T12:55:58.992192: step 2798, loss 0.559492.
Train: 2018-08-02T12:55:59.148436: step 2799, loss 0.506121.
Train: 2018-08-02T12:55:59.304619: step 2800, loss 0.505812.
Test: 2018-08-02T12:55:59.757663: step 2800, loss 0.545717.
Train: 2018-08-02T12:56:00.398113: step 2801, loss 0.582052.
Train: 2018-08-02T12:56:00.554359: step 2802, loss 0.483241.
Train: 2018-08-02T12:56:00.710540: step 2803, loss 0.546486.
Train: 2018-08-02T12:56:00.866753: step 2804, loss 0.570134.
Train: 2018-08-02T12:56:01.007346: step 2805, loss 0.508796.
Train: 2018-08-02T12:56:01.163559: step 2806, loss 0.600761.
Train: 2018-08-02T12:56:01.319802: step 2807, loss 0.594599.
Train: 2018-08-02T12:56:01.475984: step 2808, loss 0.545429.
Train: 2018-08-02T12:56:01.616607: step 2809, loss 0.513121.
Train: 2018-08-02T12:56:01.772791: step 2810, loss 0.501559.
Test: 2018-08-02T12:56:02.225810: step 2810, loss 0.555866.
Train: 2018-08-02T12:56:02.382023: step 2811, loss 0.497657.
Train: 2018-08-02T12:56:02.538267: step 2812, loss 0.582208.
Train: 2018-08-02T12:56:02.678829: step 2813, loss 0.530137.
Train: 2018-08-02T12:56:02.835072: step 2814, loss 0.507004.
Train: 2018-08-02T12:56:02.991256: step 2815, loss 0.583941.
Train: 2018-08-02T12:56:03.131879: step 2816, loss 0.505871.
Train: 2018-08-02T12:56:03.288061: step 2817, loss 0.541053.
Train: 2018-08-02T12:56:03.444275: step 2818, loss 0.444795.
Train: 2018-08-02T12:56:03.600512: step 2819, loss 0.531668.
Train: 2018-08-02T12:56:03.741080: step 2820, loss 0.543895.
Test: 2018-08-02T12:56:04.194099: step 2820, loss 0.548463.
Train: 2018-08-02T12:56:04.350312: step 2821, loss 0.488054.
Train: 2018-08-02T12:56:04.506525: step 2822, loss 0.477334.
Train: 2018-08-02T12:56:04.647148: step 2823, loss 0.547042.
Train: 2018-08-02T12:56:04.803364: step 2824, loss 0.569658.
Train: 2018-08-02T12:56:04.959574: step 2825, loss 0.46525.
Train: 2018-08-02T12:56:05.100136: step 2826, loss 0.561693.
Train: 2018-08-02T12:56:05.256349: step 2827, loss 0.490252.
Train: 2018-08-02T12:56:05.412593: step 2828, loss 0.477767.
Train: 2018-08-02T12:56:05.553189: step 2829, loss 0.626244.
Train: 2018-08-02T12:56:05.709370: step 2830, loss 0.509555.
Test: 2018-08-02T12:56:06.162413: step 2830, loss 0.548403.
Train: 2018-08-02T12:56:06.318631: step 2831, loss 0.503583.
Train: 2018-08-02T12:56:06.474845: step 2832, loss 0.50005.
Train: 2018-08-02T12:56:06.631029: step 2833, loss 0.481149.
Train: 2018-08-02T12:56:06.771621: step 2834, loss 0.535968.
Train: 2018-08-02T12:56:06.927834: step 2835, loss 0.549067.
Train: 2018-08-02T12:56:07.084077: step 2836, loss 0.576194.
Train: 2018-08-02T12:56:07.224669: step 2837, loss 0.548567.
Train: 2018-08-02T12:56:07.380852: step 2838, loss 0.527729.
Train: 2018-08-02T12:56:07.537066: step 2839, loss 0.520861.
Train: 2018-08-02T12:56:07.693279: step 2840, loss 0.5662.
Test: 2018-08-02T12:56:08.146299: step 2840, loss 0.552605.
Train: 2018-08-02T12:56:08.302545: step 2841, loss 0.631302.
Train: 2018-08-02T12:56:08.443135: step 2842, loss 0.523188.
Train: 2018-08-02T12:56:08.599347: step 2843, loss 0.48631.
Train: 2018-08-02T12:56:08.755561: step 2844, loss 0.487353.
Train: 2018-08-02T12:56:08.911768: step 2845, loss 0.587338.
Train: 2018-08-02T12:56:09.052335: step 2846, loss 0.582847.
Train: 2018-08-02T12:56:09.208580: step 2847, loss 0.495089.
Train: 2018-08-02T12:56:09.364793: step 2848, loss 0.539273.
Train: 2018-08-02T12:56:09.505387: step 2849, loss 0.518546.
Train: 2018-08-02T12:56:09.661569: step 2850, loss 0.577059.
Test: 2018-08-02T12:56:10.114616: step 2850, loss 0.548034.
Train: 2018-08-02T12:56:10.270831: step 2851, loss 0.596015.
Train: 2018-08-02T12:56:10.427047: step 2852, loss 0.526465.
Train: 2018-08-02T12:56:10.567607: step 2853, loss 0.503064.
Train: 2018-08-02T12:56:10.723849: step 2854, loss 0.590564.
Train: 2018-08-02T12:56:10.880057: step 2855, loss 0.545459.
Train: 2018-08-02T12:56:11.020625: step 2856, loss 0.545047.
Train: 2018-08-02T12:56:11.176871: step 2857, loss 0.499032.
Train: 2018-08-02T12:56:11.333082: step 2858, loss 0.484957.
Train: 2018-08-02T12:56:11.473644: step 2859, loss 0.56358.
Train: 2018-08-02T12:56:11.629889: step 2860, loss 0.434117.
Test: 2018-08-02T12:56:12.098497: step 2860, loss 0.548986.
Train: 2018-08-02T12:56:12.239114: step 2861, loss 0.57931.
Train: 2018-08-02T12:56:12.395304: step 2862, loss 0.630844.
Train: 2018-08-02T12:56:12.535926: step 2863, loss 0.515529.
Train: 2018-08-02T12:56:12.692140: step 2864, loss 0.52664.
Train: 2018-08-02T12:56:12.848322: step 2865, loss 0.515626.
Train: 2018-08-02T12:56:13.004535: step 2866, loss 0.510431.
Train: 2018-08-02T12:56:13.160749: step 2867, loss 0.558341.
Train: 2018-08-02T12:56:13.301374: step 2868, loss 0.516015.
Train: 2018-08-02T12:56:13.410691: step 2869, loss 0.571088.
Train: 2018-08-02T12:56:13.566934: step 2870, loss 0.478849.
Test: 2018-08-02T12:56:14.019957: step 2870, loss 0.549202.
Train: 2018-08-02T12:56:14.176136: step 2871, loss 0.514018.
Train: 2018-08-02T12:56:14.332350: step 2872, loss 0.389815.
Train: 2018-08-02T12:56:14.472941: step 2873, loss 0.492005.
Train: 2018-08-02T12:56:14.629186: step 2874, loss 0.559298.
Train: 2018-08-02T12:56:14.785399: step 2875, loss 0.49785.
Train: 2018-08-02T12:56:14.925962: step 2876, loss 0.528193.
Train: 2018-08-02T12:56:15.082173: step 2877, loss 0.570629.
Train: 2018-08-02T12:56:15.238387: step 2878, loss 0.495392.
Train: 2018-08-02T12:56:15.379010: step 2879, loss 0.595899.
Train: 2018-08-02T12:56:15.535223: step 2880, loss 0.566757.
Test: 2018-08-02T12:56:15.988212: step 2880, loss 0.560975.
Train: 2018-08-02T12:56:16.144456: step 2881, loss 0.55409.
Train: 2018-08-02T12:56:16.300671: step 2882, loss 0.520666.
Train: 2018-08-02T12:56:16.441231: step 2883, loss 0.562371.
Train: 2018-08-02T12:56:16.597475: step 2884, loss 0.552713.
Train: 2018-08-02T12:56:16.753688: step 2885, loss 0.512191.
Train: 2018-08-02T12:56:16.894283: step 2886, loss 0.487644.
Train: 2018-08-02T12:56:17.050496: step 2887, loss 0.665473.
Train: 2018-08-02T12:56:17.206707: step 2888, loss 0.558408.
Train: 2018-08-02T12:56:17.362890: step 2889, loss 0.589622.
Train: 2018-08-02T12:56:17.503512: step 2890, loss 0.523022.
Test: 2018-08-02T12:56:17.967585: step 2890, loss 0.553787.
Train: 2018-08-02T12:56:18.123803: step 2891, loss 0.569189.
Train: 2018-08-02T12:56:18.264395: step 2892, loss 0.551377.
Train: 2018-08-02T12:56:18.420609: step 2893, loss 0.559893.
Train: 2018-08-02T12:56:18.561201: step 2894, loss 0.481645.
Train: 2018-08-02T12:56:18.717385: step 2895, loss 0.517113.
Train: 2018-08-02T12:56:18.873627: step 2896, loss 0.441074.
Train: 2018-08-02T12:56:19.029841: step 2897, loss 0.492454.
Train: 2018-08-02T12:56:19.170436: step 2898, loss 0.506869.
Train: 2018-08-02T12:56:19.326617: step 2899, loss 0.418483.
Train: 2018-08-02T12:56:19.482860: step 2900, loss 0.53423.
Test: 2018-08-02T12:56:19.935850: step 2900, loss 0.552978.
Train: 2018-08-02T12:56:20.576353: step 2901, loss 0.625934.
Train: 2018-08-02T12:56:20.716949: step 2902, loss 0.491883.
Train: 2018-08-02T12:56:20.873160: step 2903, loss 0.453968.
Train: 2018-08-02T12:56:21.029343: step 2904, loss 0.566375.
Train: 2018-08-02T12:56:21.185587: step 2905, loss 0.529537.
Train: 2018-08-02T12:56:21.326178: step 2906, loss 0.542142.
Train: 2018-08-02T12:56:21.482362: step 2907, loss 0.508067.
Train: 2018-08-02T12:56:21.638608: step 2908, loss 0.524607.
Train: 2018-08-02T12:56:21.779198: step 2909, loss 0.535862.
Train: 2018-08-02T12:56:21.935411: step 2910, loss 0.482476.
Test: 2018-08-02T12:56:22.388399: step 2910, loss 0.548034.
Train: 2018-08-02T12:56:22.544643: step 2911, loss 0.542861.
Train: 2018-08-02T12:56:22.700825: step 2912, loss 0.500444.
Train: 2018-08-02T12:56:22.857061: step 2913, loss 0.52513.
Train: 2018-08-02T12:56:23.013256: step 2914, loss 0.515756.
Train: 2018-08-02T12:56:23.169467: step 2915, loss 0.600946.
Train: 2018-08-02T12:56:23.310089: step 2916, loss 0.568589.
Train: 2018-08-02T12:56:23.466273: step 2917, loss 0.461088.
Train: 2018-08-02T12:56:23.622515: step 2918, loss 0.509639.
Train: 2018-08-02T12:56:23.763107: step 2919, loss 0.524131.
Train: 2018-08-02T12:56:23.919321: step 2920, loss 0.487955.
Test: 2018-08-02T12:56:24.372309: step 2920, loss 0.549217.
Train: 2018-08-02T12:56:24.528554: step 2921, loss 0.48394.
Train: 2018-08-02T12:56:24.669146: step 2922, loss 0.505052.
Train: 2018-08-02T12:56:24.825361: step 2923, loss 0.48424.
Train: 2018-08-02T12:56:24.981543: step 2924, loss 0.517024.
Train: 2018-08-02T12:56:25.122136: step 2925, loss 0.486042.
Train: 2018-08-02T12:56:25.278347: step 2926, loss 0.503146.
Train: 2018-08-02T12:56:25.434562: step 2927, loss 0.509253.
Train: 2018-08-02T12:56:25.575183: step 2928, loss 0.44692.
Train: 2018-08-02T12:56:25.731400: step 2929, loss 0.489095.
Train: 2018-08-02T12:56:25.887610: step 2930, loss 0.549597.
Test: 2018-08-02T12:56:26.340600: step 2930, loss 0.54787.
Train: 2018-08-02T12:56:26.496842: step 2931, loss 0.522014.
Train: 2018-08-02T12:56:26.637435: step 2932, loss 0.579655.
Train: 2018-08-02T12:56:26.793648: step 2933, loss 0.476168.
Train: 2018-08-02T12:56:26.949830: step 2934, loss 0.529239.
Train: 2018-08-02T12:56:27.106045: step 2935, loss 0.585339.
Train: 2018-08-02T12:56:27.246667: step 2936, loss 0.480824.
Train: 2018-08-02T12:56:27.402880: step 2937, loss 0.556971.
Train: 2018-08-02T12:56:27.559064: step 2938, loss 0.50577.
Train: 2018-08-02T12:56:27.699686: step 2939, loss 0.53456.
Train: 2018-08-02T12:56:27.855900: step 2940, loss 0.465832.
Test: 2018-08-02T12:56:28.308913: step 2940, loss 0.547076.
Train: 2018-08-02T12:56:28.465133: step 2941, loss 0.44234.
Train: 2018-08-02T12:56:28.621345: step 2942, loss 0.567914.
Train: 2018-08-02T12:56:28.761931: step 2943, loss 0.534306.
Train: 2018-08-02T12:56:28.918151: step 2944, loss 0.522198.
Train: 2018-08-02T12:56:29.074364: step 2945, loss 0.475535.
Train: 2018-08-02T12:56:29.214955: step 2946, loss 0.605078.
Train: 2018-08-02T12:56:29.371169: step 2947, loss 0.553297.
Train: 2018-08-02T12:56:29.527383: step 2948, loss 0.495457.
Train: 2018-08-02T12:56:29.667975: step 2949, loss 0.528211.
Train: 2018-08-02T12:56:29.824189: step 2950, loss 0.587538.
Test: 2018-08-02T12:56:30.292829: step 2950, loss 0.555693.
Train: 2018-08-02T12:56:30.433390: step 2951, loss 0.511335.
Train: 2018-08-02T12:56:30.589634: step 2952, loss 0.557528.
Train: 2018-08-02T12:56:30.745848: step 2953, loss 0.539895.
Train: 2018-08-02T12:56:30.902061: step 2954, loss 0.536473.
Train: 2018-08-02T12:56:31.042653: step 2955, loss 0.613435.
Train: 2018-08-02T12:56:31.198867: step 2956, loss 0.531586.
Train: 2018-08-02T12:56:31.355050: step 2957, loss 0.496167.
Train: 2018-08-02T12:56:31.558157: step 2958, loss 0.551251.
Train: 2018-08-02T12:56:31.698753: step 2959, loss 0.555219.
Train: 2018-08-02T12:56:31.854962: step 2960, loss 0.467475.
Test: 2018-08-02T12:56:32.307952: step 2960, loss 0.550069.
Train: 2018-08-02T12:56:32.464195: step 2961, loss 0.571529.
Train: 2018-08-02T12:56:32.620409: step 2962, loss 0.471316.
Train: 2018-08-02T12:56:32.776605: step 2963, loss 0.585855.
Train: 2018-08-02T12:56:32.917208: step 2964, loss 0.493335.
Train: 2018-08-02T12:56:33.073427: step 2965, loss 0.499439.
Train: 2018-08-02T12:56:33.229641: step 2966, loss 0.500721.
Train: 2018-08-02T12:56:33.370233: step 2967, loss 0.509168.
Train: 2018-08-02T12:56:33.526447: step 2968, loss 0.520054.
Train: 2018-08-02T12:56:33.682660: step 2969, loss 0.570073.
Train: 2018-08-02T12:56:33.823252: step 2970, loss 0.463134.
Test: 2018-08-02T12:56:34.291862: step 2970, loss 0.548856.
Train: 2018-08-02T12:56:34.448105: step 2971, loss 0.529516.
Train: 2018-08-02T12:56:34.588697: step 2972, loss 0.447222.
Train: 2018-08-02T12:56:34.744911: step 2973, loss 0.469316.
Train: 2018-08-02T12:56:34.901125: step 2974, loss 0.514644.
Train: 2018-08-02T12:56:35.041716: step 2975, loss 0.439238.
Train: 2018-08-02T12:56:35.197930: step 2976, loss 0.454091.
Train: 2018-08-02T12:56:35.354149: step 2977, loss 0.515833.
Train: 2018-08-02T12:56:35.510327: step 2978, loss 0.496074.
Train: 2018-08-02T12:56:35.650918: step 2979, loss 0.554563.
Train: 2018-08-02T12:56:35.807162: step 2980, loss 0.53402.
Test: 2018-08-02T12:56:36.260151: step 2980, loss 0.550636.
Train: 2018-08-02T12:56:36.432016: step 2981, loss 0.52085.
Train: 2018-08-02T12:56:36.588229: step 2982, loss 0.55199.
Train: 2018-08-02T12:56:36.744443: step 2983, loss 0.523168.
Train: 2018-08-02T12:56:36.885004: step 2984, loss 0.500727.
Train: 2018-08-02T12:56:37.041248: step 2985, loss 0.533026.
Train: 2018-08-02T12:56:37.197431: step 2986, loss 0.506731.
Train: 2018-08-02T12:56:37.338054: step 2987, loss 0.454201.
Train: 2018-08-02T12:56:37.494267: step 2988, loss 0.586321.
Train: 2018-08-02T12:56:37.650481: step 2989, loss 0.612752.
Train: 2018-08-02T12:56:37.806664: step 2990, loss 0.576405.
Test: 2018-08-02T12:56:38.259683: step 2990, loss 0.551907.
Train: 2018-08-02T12:56:38.415927: step 2991, loss 0.516538.
Train: 2018-08-02T12:56:38.556518: step 2992, loss 0.480863.
Train: 2018-08-02T12:56:38.712732: step 2993, loss 0.596685.
Train: 2018-08-02T12:56:38.868915: step 2994, loss 0.560941.
Train: 2018-08-02T12:56:39.009508: step 2995, loss 0.496788.
Train: 2018-08-02T12:56:39.165720: step 2996, loss 0.507644.
Train: 2018-08-02T12:56:39.321964: step 2997, loss 0.532906.
Train: 2018-08-02T12:56:39.478176: step 2998, loss 0.490277.
Train: 2018-08-02T12:56:39.618769: step 2999, loss 0.505911.
Train: 2018-08-02T12:56:39.774978: step 3000, loss 0.545793.
Test: 2018-08-02T12:56:40.243593: step 3000, loss 0.548263.
Train: 2018-08-02T12:56:40.852826: step 3001, loss 0.521508.
Train: 2018-08-02T12:56:41.009069: step 3002, loss 0.542452.
Train: 2018-08-02T12:56:41.149662: step 3003, loss 0.438595.
Train: 2018-08-02T12:56:41.305845: step 3004, loss 0.481852.
Train: 2018-08-02T12:56:41.462092: step 3005, loss 0.567674.
Train: 2018-08-02T12:56:41.618296: step 3006, loss 0.461745.
Train: 2018-08-02T12:56:41.758864: step 3007, loss 0.589656.
Train: 2018-08-02T12:56:41.915107: step 3008, loss 0.493323.
Train: 2018-08-02T12:56:42.071320: step 3009, loss 0.596159.
Train: 2018-08-02T12:56:42.211913: step 3010, loss 0.553784.
Test: 2018-08-02T12:56:42.664926: step 3010, loss 0.554775.
Train: 2018-08-02T12:56:42.821145: step 3011, loss 0.486366.
Train: 2018-08-02T12:56:42.977358: step 3012, loss 0.498704.
Train: 2018-08-02T12:56:43.133571: step 3013, loss 0.526347.
Train: 2018-08-02T12:56:43.274164: step 3014, loss 0.589161.
Train: 2018-08-02T12:56:43.430347: step 3015, loss 0.471641.
Train: 2018-08-02T12:56:43.586590: step 3016, loss 0.564368.
Train: 2018-08-02T12:56:43.727182: step 3017, loss 0.45362.
Train: 2018-08-02T12:56:43.883396: step 3018, loss 0.518593.
Train: 2018-08-02T12:56:44.039610: step 3019, loss 0.545424.
Train: 2018-08-02T12:56:44.148930: step 3020, loss 0.515503.
Test: 2018-08-02T12:56:44.601978: step 3020, loss 0.550686.
Train: 2018-08-02T12:56:44.758162: step 3021, loss 0.480953.
Train: 2018-08-02T12:56:44.898784: step 3022, loss 0.444425.
Train: 2018-08-02T12:56:45.054994: step 3023, loss 0.43122.
Train: 2018-08-02T12:56:45.211210: step 3024, loss 0.454763.
Train: 2018-08-02T12:56:45.367423: step 3025, loss 0.468687.
Train: 2018-08-02T12:56:45.507986: step 3026, loss 0.460755.
Train: 2018-08-02T12:56:45.664235: step 3027, loss 0.431885.
Train: 2018-08-02T12:56:45.820442: step 3028, loss 0.494329.
Train: 2018-08-02T12:56:45.976656: step 3029, loss 0.435974.
Train: 2018-08-02T12:56:46.117247: step 3030, loss 0.511764.
Test: 2018-08-02T12:56:46.585889: step 3030, loss 0.550939.
Train: 2018-08-02T12:56:46.742101: step 3031, loss 0.561511.
Train: 2018-08-02T12:56:46.882694: step 3032, loss 0.555382.
Train: 2018-08-02T12:56:47.038876: step 3033, loss 0.595248.
Train: 2018-08-02T12:56:47.195122: step 3034, loss 0.552118.
Train: 2018-08-02T12:56:47.335719: step 3035, loss 0.523476.
Train: 2018-08-02T12:56:47.491896: step 3036, loss 0.54072.
Train: 2018-08-02T12:56:47.648140: step 3037, loss 0.524355.
Train: 2018-08-02T12:56:47.788701: step 3038, loss 0.516788.
Train: 2018-08-02T12:56:47.944945: step 3039, loss 0.506976.
Train: 2018-08-02T12:56:48.101128: step 3040, loss 0.514421.
Test: 2018-08-02T12:56:48.554152: step 3040, loss 0.548961.
Train: 2018-08-02T12:56:48.694769: step 3041, loss 0.57962.
Train: 2018-08-02T12:56:48.850982: step 3042, loss 0.548115.
Train: 2018-08-02T12:56:49.007165: step 3043, loss 0.637194.
Train: 2018-08-02T12:56:49.147757: step 3044, loss 0.541194.
Train: 2018-08-02T12:56:49.304002: step 3045, loss 0.530238.
Train: 2018-08-02T12:56:49.460215: step 3046, loss 0.553258.
Train: 2018-08-02T12:56:49.600776: step 3047, loss 0.48998.
Train: 2018-08-02T12:56:49.757021: step 3048, loss 0.525817.
Train: 2018-08-02T12:56:49.913204: step 3049, loss 0.551386.
Train: 2018-08-02T12:56:50.053795: step 3050, loss 0.521494.
Test: 2018-08-02T12:56:50.522462: step 3050, loss 0.548801.
Train: 2018-08-02T12:56:50.663059: step 3051, loss 0.465737.
Train: 2018-08-02T12:56:50.819259: step 3052, loss 0.498059.
Train: 2018-08-02T12:56:50.975454: step 3053, loss 0.493813.
Train: 2018-08-02T12:56:51.116077: step 3054, loss 0.467419.
Train: 2018-08-02T12:56:51.272291: step 3055, loss 0.507018.
Train: 2018-08-02T12:56:51.428503: step 3056, loss 0.554495.
Train: 2018-08-02T12:56:51.569096: step 3057, loss 0.473756.
Train: 2018-08-02T12:56:51.725310: step 3058, loss 0.547803.
Train: 2018-08-02T12:56:51.881493: step 3059, loss 0.549869.
Train: 2018-08-02T12:56:52.037730: step 3060, loss 0.515319.
Test: 2018-08-02T12:56:52.490725: step 3060, loss 0.557158.
Train: 2018-08-02T12:56:52.646938: step 3061, loss 0.550777.
Train: 2018-08-02T12:56:52.787561: step 3062, loss 0.464257.
Train: 2018-08-02T12:56:52.943775: step 3063, loss 0.536788.
Train: 2018-08-02T12:56:53.099988: step 3064, loss 0.586345.
Train: 2018-08-02T12:56:53.240580: step 3065, loss 0.513898.
Train: 2018-08-02T12:56:53.396793: step 3066, loss 0.573048.
Train: 2018-08-02T12:56:53.553007: step 3067, loss 0.552556.
Train: 2018-08-02T12:56:53.693598: step 3068, loss 0.506293.
Train: 2018-08-02T12:56:53.849812: step 3069, loss 0.482307.
Train: 2018-08-02T12:56:54.005996: step 3070, loss 0.468173.
Test: 2018-08-02T12:56:54.459040: step 3070, loss 0.546913.
Train: 2018-08-02T12:56:54.615258: step 3071, loss 0.521919.
Train: 2018-08-02T12:56:54.755849: step 3072, loss 0.528623.
Train: 2018-08-02T12:56:54.912033: step 3073, loss 0.580461.
Train: 2018-08-02T12:56:55.068276: step 3074, loss 0.510635.
Train: 2018-08-02T12:56:55.208870: step 3075, loss 0.644015.
Train: 2018-08-02T12:56:55.365082: step 3076, loss 0.588649.
Train: 2018-08-02T12:56:55.521296: step 3077, loss 0.531972.
Train: 2018-08-02T12:56:55.677509: step 3078, loss 0.535197.
Train: 2018-08-02T12:56:55.818101: step 3079, loss 0.551098.
Train: 2018-08-02T12:56:55.974284: step 3080, loss 0.544468.
Test: 2018-08-02T12:56:56.427331: step 3080, loss 0.561397.
Train: 2018-08-02T12:56:56.583517: step 3081, loss 0.5496.
Train: 2018-08-02T12:56:56.739730: step 3082, loss 0.550293.
Train: 2018-08-02T12:56:56.880352: step 3083, loss 0.491045.
Train: 2018-08-02T12:56:57.036566: step 3084, loss 0.478862.
Train: 2018-08-02T12:56:57.192780: step 3085, loss 0.509022.
Train: 2018-08-02T12:56:57.348992: step 3086, loss 0.505364.
Train: 2018-08-02T12:56:57.489586: step 3087, loss 0.521929.
Train: 2018-08-02T12:56:57.645799: step 3088, loss 0.475669.
Train: 2018-08-02T12:56:57.801982: step 3089, loss 0.506368.
Train: 2018-08-02T12:56:57.958195: step 3090, loss 0.462661.
Test: 2018-08-02T12:56:58.411239: step 3090, loss 0.5463.
Train: 2018-08-02T12:56:58.567426: step 3091, loss 0.434003.
Train: 2018-08-02T12:56:58.708050: step 3092, loss 0.539633.
Train: 2018-08-02T12:56:58.864232: step 3093, loss 0.513915.
Train: 2018-08-02T12:56:59.020446: step 3094, loss 0.530403.
Train: 2018-08-02T12:56:59.161069: step 3095, loss 0.597652.
Train: 2018-08-02T12:56:59.317282: step 3096, loss 0.500846.
Train: 2018-08-02T12:56:59.473495: step 3097, loss 0.574963.
Train: 2018-08-02T12:56:59.614087: step 3098, loss 0.57642.
Train: 2018-08-02T12:56:59.770301: step 3099, loss 0.477971.
Train: 2018-08-02T12:56:59.926485: step 3100, loss 0.519228.
Test: 2018-08-02T12:57:00.379528: step 3100, loss 0.556481.
Train: 2018-08-02T12:57:01.020008: step 3101, loss 0.499369.
Train: 2018-08-02T12:57:01.176191: step 3102, loss 0.522599.
Train: 2018-08-02T12:57:01.332406: step 3103, loss 0.514375.
Train: 2018-08-02T12:57:01.473027: step 3104, loss 0.484489.
Train: 2018-08-02T12:57:01.629211: step 3105, loss 0.571884.
Train: 2018-08-02T12:57:01.785454: step 3106, loss 0.598763.
Train: 2018-08-02T12:57:01.926046: step 3107, loss 0.509523.
Train: 2018-08-02T12:57:02.082260: step 3108, loss 0.544741.
Train: 2018-08-02T12:57:02.238472: step 3109, loss 0.459969.
Train: 2018-08-02T12:57:02.379065: step 3110, loss 0.473187.
Test: 2018-08-02T12:57:02.847674: step 3110, loss 0.547339.
Train: 2018-08-02T12:57:02.988297: step 3111, loss 0.510458.
Train: 2018-08-02T12:57:03.144511: step 3112, loss 0.525866.
Train: 2018-08-02T12:57:03.285071: step 3113, loss 0.426624.
Train: 2018-08-02T12:57:03.441315: step 3114, loss 0.5949.
Train: 2018-08-02T12:57:03.597500: step 3115, loss 0.417041.
Train: 2018-08-02T12:57:03.753743: step 3116, loss 0.533018.
Train: 2018-08-02T12:57:03.894304: step 3117, loss 0.445018.
Train: 2018-08-02T12:57:04.050517: step 3118, loss 0.547972.
Train: 2018-08-02T12:57:04.206762: step 3119, loss 0.535693.
Train: 2018-08-02T12:57:04.347323: step 3120, loss 0.549101.
Test: 2018-08-02T12:57:04.815964: step 3120, loss 0.549899.
Train: 2018-08-02T12:57:04.972203: step 3121, loss 0.608049.
Train: 2018-08-02T12:57:05.112769: step 3122, loss 0.604106.
Train: 2018-08-02T12:57:05.268984: step 3123, loss 0.529788.
Train: 2018-08-02T12:57:05.425226: step 3124, loss 0.532344.
Train: 2018-08-02T12:57:05.581439: step 3125, loss 0.558495.
Train: 2018-08-02T12:57:05.722002: step 3126, loss 0.516831.
Train: 2018-08-02T12:57:05.878216: step 3127, loss 0.494129.
Train: 2018-08-02T12:57:06.034460: step 3128, loss 0.525907.
Train: 2018-08-02T12:57:06.175020: step 3129, loss 0.545951.
Train: 2018-08-02T12:57:06.331264: step 3130, loss 0.597947.
Test: 2018-08-02T12:57:06.784278: step 3130, loss 0.54898.
Train: 2018-08-02T12:57:06.940466: step 3131, loss 0.552656.
Train: 2018-08-02T12:57:07.096680: step 3132, loss 0.538072.
Train: 2018-08-02T12:57:07.252924: step 3133, loss 0.567209.
Train: 2018-08-02T12:57:07.393516: step 3134, loss 0.507576.
Train: 2018-08-02T12:57:07.549732: step 3135, loss 0.568791.
Train: 2018-08-02T12:57:07.705943: step 3136, loss 0.54741.
Train: 2018-08-02T12:57:07.846534: step 3137, loss 0.519354.
Train: 2018-08-02T12:57:08.002748: step 3138, loss 0.516511.
Train: 2018-08-02T12:57:08.158932: step 3139, loss 0.59405.
Train: 2018-08-02T12:57:08.299553: step 3140, loss 0.54136.
Test: 2018-08-02T12:57:08.768188: step 3140, loss 0.55145.
Train: 2018-08-02T12:57:08.908780: step 3141, loss 0.533457.
Train: 2018-08-02T12:57:09.064970: step 3142, loss 0.517398.
Train: 2018-08-02T12:57:09.221215: step 3143, loss 0.52851.
Train: 2018-08-02T12:57:09.377423: step 3144, loss 0.548155.
Train: 2018-08-02T12:57:09.518018: step 3145, loss 0.483298.
Train: 2018-08-02T12:57:09.674231: step 3146, loss 0.481189.
Train: 2018-08-02T12:57:09.830445: step 3147, loss 0.462122.
Train: 2018-08-02T12:57:09.971006: step 3148, loss 0.607208.
Train: 2018-08-02T12:57:10.127221: step 3149, loss 0.384753.
Train: 2018-08-02T12:57:10.283466: step 3150, loss 0.472453.
Test: 2018-08-02T12:57:10.736479: step 3150, loss 0.54786.
Train: 2018-08-02T12:57:10.892697: step 3151, loss 0.581562.
Train: 2018-08-02T12:57:11.048879: step 3152, loss 0.577674.
Train: 2018-08-02T12:57:11.189501: step 3153, loss 0.471266.
Train: 2018-08-02T12:57:11.345715: step 3154, loss 0.562213.
Train: 2018-08-02T12:57:11.501898: step 3155, loss 0.592864.
Train: 2018-08-02T12:57:11.658113: step 3156, loss 0.488492.
Train: 2018-08-02T12:57:11.798734: step 3157, loss 0.513153.
Train: 2018-08-02T12:57:11.954948: step 3158, loss 0.544119.
Train: 2018-08-02T12:57:12.111161: step 3159, loss 0.551809.
Train: 2018-08-02T12:57:12.251753: step 3160, loss 0.561766.
Test: 2018-08-02T12:57:12.720362: step 3160, loss 0.547521.
Train: 2018-08-02T12:57:12.860985: step 3161, loss 0.613833.
Train: 2018-08-02T12:57:13.017199: step 3162, loss 0.470879.
Train: 2018-08-02T12:57:13.173412: step 3163, loss 0.584819.
Train: 2018-08-02T12:57:13.314003: step 3164, loss 0.560398.
Train: 2018-08-02T12:57:13.470189: step 3165, loss 0.5772.
Train: 2018-08-02T12:57:13.626431: step 3166, loss 0.560649.
Train: 2018-08-02T12:57:13.767028: step 3167, loss 0.553237.
Train: 2018-08-02T12:57:13.923236: step 3168, loss 0.563484.
Train: 2018-08-02T12:57:14.079450: step 3169, loss 0.556071.
Train: 2018-08-02T12:57:14.220013: step 3170, loss 0.548967.
Test: 2018-08-02T12:57:14.673046: step 3170, loss 0.549376.
Train: 2018-08-02T12:57:14.782379: step 3171, loss 0.591442.
Train: 2018-08-02T12:57:14.938624: step 3172, loss 0.583255.
Train: 2018-08-02T12:57:15.094837: step 3173, loss 0.482903.
Train: 2018-08-02T12:57:15.235430: step 3174, loss 0.534442.
Train: 2018-08-02T12:57:15.391643: step 3175, loss 0.439684.
Train: 2018-08-02T12:57:15.547856: step 3176, loss 0.463069.
Train: 2018-08-02T12:57:15.688448: step 3177, loss 0.496548.
Train: 2018-08-02T12:57:15.844661: step 3178, loss 0.700724.
Train: 2018-08-02T12:57:16.000845: step 3179, loss 0.521756.
Train: 2018-08-02T12:57:16.157088: step 3180, loss 0.580367.
Test: 2018-08-02T12:57:16.610077: step 3180, loss 0.564245.
Train: 2018-08-02T12:57:16.766290: step 3181, loss 0.499492.
Train: 2018-08-02T12:57:16.906883: step 3182, loss 0.517496.
Train: 2018-08-02T12:57:17.063126: step 3183, loss 0.56966.
Train: 2018-08-02T12:57:17.219340: step 3184, loss 0.525745.
Train: 2018-08-02T12:57:17.359902: step 3185, loss 0.512624.
Train: 2018-08-02T12:57:17.516150: step 3186, loss 0.480176.
Train: 2018-08-02T12:57:17.672328: step 3187, loss 0.484821.
Train: 2018-08-02T12:57:17.828572: step 3188, loss 0.524829.
Train: 2018-08-02T12:57:17.969164: step 3189, loss 0.547877.
Train: 2018-08-02T12:57:18.125378: step 3190, loss 0.475709.
Test: 2018-08-02T12:57:18.578391: step 3190, loss 0.549133.
Train: 2018-08-02T12:57:18.734609: step 3191, loss 0.545689.
Train: 2018-08-02T12:57:18.890823: step 3192, loss 0.599247.
Train: 2018-08-02T12:57:19.031415: step 3193, loss 0.539602.
Train: 2018-08-02T12:57:19.187628: step 3194, loss 0.499697.
Train: 2018-08-02T12:57:19.343812: step 3195, loss 0.569187.
Train: 2018-08-02T12:57:19.500055: step 3196, loss 0.602878.
Train: 2018-08-02T12:57:19.640648: step 3197, loss 0.545089.
Train: 2018-08-02T12:57:19.796862: step 3198, loss 0.5224.
Train: 2018-08-02T12:57:19.953075: step 3199, loss 0.518019.
Train: 2018-08-02T12:57:20.093666: step 3200, loss 0.573942.
Test: 2018-08-02T12:57:20.562302: step 3200, loss 0.563756.
Train: 2018-08-02T12:57:21.218403: step 3201, loss 0.540429.
Train: 2018-08-02T12:57:21.358964: step 3202, loss 0.562691.
Train: 2018-08-02T12:57:21.515179: step 3203, loss 0.532194.
Train: 2018-08-02T12:57:21.671391: step 3204, loss 0.518431.
Train: 2018-08-02T12:57:21.827635: step 3205, loss 0.559835.
Train: 2018-08-02T12:57:21.968196: step 3206, loss 0.537825.
Train: 2018-08-02T12:57:22.124440: step 3207, loss 0.528259.
Train: 2018-08-02T12:57:22.280655: step 3208, loss 0.50098.
Train: 2018-08-02T12:57:22.421246: step 3209, loss 0.532779.
Train: 2018-08-02T12:57:22.577460: step 3210, loss 0.549624.
Test: 2018-08-02T12:57:23.030448: step 3210, loss 0.552075.
Train: 2018-08-02T12:57:23.186661: step 3211, loss 0.473191.
Train: 2018-08-02T12:57:23.342906: step 3212, loss 0.541205.
Train: 2018-08-02T12:57:23.499123: step 3213, loss 0.534715.
Train: 2018-08-02T12:57:23.639711: step 3214, loss 0.49931.
Train: 2018-08-02T12:57:23.795895: step 3215, loss 0.476144.
Train: 2018-08-02T12:57:23.952109: step 3216, loss 0.505893.
Train: 2018-08-02T12:57:24.108322: step 3217, loss 0.479124.
Train: 2018-08-02T12:57:24.248914: step 3218, loss 0.525908.
Train: 2018-08-02T12:57:24.405127: step 3219, loss 0.514458.
Train: 2018-08-02T12:57:24.561340: step 3220, loss 0.511434.
Test: 2018-08-02T12:57:25.014360: step 3220, loss 0.547322.
Train: 2018-08-02T12:57:25.170605: step 3221, loss 0.511051.
Train: 2018-08-02T12:57:25.326816: step 3222, loss 0.530542.
Train: 2018-08-02T12:57:25.467408: step 3223, loss 0.588519.
Train: 2018-08-02T12:57:25.623592: step 3224, loss 0.505772.
Train: 2018-08-02T12:57:25.779804: step 3225, loss 0.46812.
Train: 2018-08-02T12:57:25.920427: step 3226, loss 0.507862.
Train: 2018-08-02T12:57:26.076640: step 3227, loss 0.463923.
Train: 2018-08-02T12:57:26.232824: step 3228, loss 0.520839.
Train: 2018-08-02T12:57:26.389037: step 3229, loss 0.53588.
Train: 2018-08-02T12:57:26.529629: step 3230, loss 0.496577.
Test: 2018-08-02T12:57:26.998273: step 3230, loss 0.552324.
Train: 2018-08-02T12:57:27.138861: step 3231, loss 0.4403.
Train: 2018-08-02T12:57:27.295105: step 3232, loss 0.45889.
Train: 2018-08-02T12:57:27.451318: step 3233, loss 0.554034.
Train: 2018-08-02T12:57:27.607501: step 3234, loss 0.525873.
Train: 2018-08-02T12:57:27.748124: step 3235, loss 0.527396.
Train: 2018-08-02T12:57:27.904338: step 3236, loss 0.600179.
Train: 2018-08-02T12:57:28.060551: step 3237, loss 0.52652.
Train: 2018-08-02T12:57:28.201112: step 3238, loss 0.51008.
Train: 2018-08-02T12:57:28.357325: step 3239, loss 0.589126.
Train: 2018-08-02T12:57:28.513570: step 3240, loss 0.547448.
Test: 2018-08-02T12:57:28.966559: step 3240, loss 0.54959.
Train: 2018-08-02T12:57:29.122802: step 3241, loss 0.488696.
Train: 2018-08-02T12:57:29.278984: step 3242, loss 0.575764.
Train: 2018-08-02T12:57:29.435229: step 3243, loss 0.505913.
Train: 2018-08-02T12:57:29.591426: step 3244, loss 0.505598.
Train: 2018-08-02T12:57:29.732035: step 3245, loss 0.578836.
Train: 2018-08-02T12:57:29.888248: step 3246, loss 0.634992.
Train: 2018-08-02T12:57:30.044431: step 3247, loss 0.563062.
Train: 2018-08-02T12:57:30.185023: step 3248, loss 0.475462.
Train: 2018-08-02T12:57:30.341267: step 3249, loss 0.49101.
Train: 2018-08-02T12:57:30.497480: step 3250, loss 0.542547.
Test: 2018-08-02T12:57:30.950469: step 3250, loss 0.559124.
Train: 2018-08-02T12:57:31.106712: step 3251, loss 0.614755.
Train: 2018-08-02T12:57:31.262926: step 3252, loss 0.502185.
Train: 2018-08-02T12:57:31.403518: step 3253, loss 0.534823.
Train: 2018-08-02T12:57:31.559731: step 3254, loss 0.519415.
Train: 2018-08-02T12:57:31.715944: step 3255, loss 0.478727.
Train: 2018-08-02T12:57:31.856537: step 3256, loss 0.561181.
Train: 2018-08-02T12:57:32.012751: step 3257, loss 0.471283.
Train: 2018-08-02T12:57:32.168934: step 3258, loss 0.554813.
Train: 2018-08-02T12:57:32.309556: step 3259, loss 0.493654.
Train: 2018-08-02T12:57:32.465739: step 3260, loss 0.49794.
Test: 2018-08-02T12:57:32.918758: step 3260, loss 0.549707.
Train: 2018-08-02T12:57:33.074971: step 3261, loss 0.496067.
Train: 2018-08-02T12:57:33.231214: step 3262, loss 0.484415.
Train: 2018-08-02T12:57:33.387426: step 3263, loss 0.562481.
Train: 2018-08-02T12:57:33.528020: step 3264, loss 0.595243.
Train: 2018-08-02T12:57:33.684234: step 3265, loss 0.538597.
Train: 2018-08-02T12:57:33.840417: step 3266, loss 0.543717.
Train: 2018-08-02T12:57:33.981040: step 3267, loss 0.472771.
Train: 2018-08-02T12:57:34.137254: step 3268, loss 0.561782.
Train: 2018-08-02T12:57:34.293467: step 3269, loss 0.492456.
Train: 2018-08-02T12:57:34.449680: step 3270, loss 0.43616.
Test: 2018-08-02T12:57:34.902673: step 3270, loss 0.548597.
Train: 2018-08-02T12:57:35.058912: step 3271, loss 0.615417.
Train: 2018-08-02T12:57:35.215126: step 3272, loss 0.482239.
Train: 2018-08-02T12:57:35.355717: step 3273, loss 0.496705.
Train: 2018-08-02T12:57:35.511900: step 3274, loss 0.540069.
Train: 2018-08-02T12:57:35.668115: step 3275, loss 0.514057.
Train: 2018-08-02T12:57:35.808705: step 3276, loss 0.55444.
Train: 2018-08-02T12:57:35.964950: step 3277, loss 0.546221.
Train: 2018-08-02T12:57:36.121163: step 3278, loss 0.481969.
Train: 2018-08-02T12:57:36.261725: step 3279, loss 0.460835.
Train: 2018-08-02T12:57:36.417968: step 3280, loss 0.460694.
Test: 2018-08-02T12:57:36.886579: step 3280, loss 0.546726.
Train: 2018-08-02T12:57:37.027201: step 3281, loss 0.478665.
Train: 2018-08-02T12:57:37.183415: step 3282, loss 0.509257.
Train: 2018-08-02T12:57:37.339628: step 3283, loss 0.528628.
Train: 2018-08-02T12:57:37.480219: step 3284, loss 0.578606.
Train: 2018-08-02T12:57:37.636403: step 3285, loss 0.56783.
Train: 2018-08-02T12:57:37.792647: step 3286, loss 0.49484.
Train: 2018-08-02T12:57:37.948830: step 3287, loss 0.477567.
Train: 2018-08-02T12:57:38.089452: step 3288, loss 0.515258.
Train: 2018-08-02T12:57:38.245643: step 3289, loss 0.526439.
Train: 2018-08-02T12:57:38.401879: step 3290, loss 0.544448.
Test: 2018-08-02T12:57:38.854868: step 3290, loss 0.555921.
Train: 2018-08-02T12:57:39.011112: step 3291, loss 0.524843.
Train: 2018-08-02T12:57:39.151703: step 3292, loss 0.565712.
Train: 2018-08-02T12:57:39.307888: step 3293, loss 0.47633.
Train: 2018-08-02T12:57:39.464099: step 3294, loss 0.51089.
Train: 2018-08-02T12:57:39.620315: step 3295, loss 0.512345.
Train: 2018-08-02T12:57:39.776576: step 3296, loss 0.614125.
Train: 2018-08-02T12:57:39.917120: step 3297, loss 0.486984.
Train: 2018-08-02T12:57:40.073362: step 3298, loss 0.517667.
Train: 2018-08-02T12:57:40.229576: step 3299, loss 0.481605.
Train: 2018-08-02T12:57:40.385790: step 3300, loss 0.508367.
Test: 2018-08-02T12:57:40.838813: step 3300, loss 0.548936.
Train: 2018-08-02T12:57:41.479283: step 3301, loss 0.507206.
Train: 2018-08-02T12:57:41.635497: step 3302, loss 0.439411.
Train: 2018-08-02T12:57:41.791710: step 3303, loss 0.530291.
Train: 2018-08-02T12:57:41.947894: step 3304, loss 0.533051.
Train: 2018-08-02T12:57:42.088515: step 3305, loss 0.58995.
Train: 2018-08-02T12:57:42.244728: step 3306, loss 0.548302.
Train: 2018-08-02T12:57:42.400942: step 3307, loss 0.529565.
Train: 2018-08-02T12:57:42.557155: step 3308, loss 0.614129.
Train: 2018-08-02T12:57:42.697749: step 3309, loss 0.536391.
Train: 2018-08-02T12:57:42.853932: step 3310, loss 0.540787.
Test: 2018-08-02T12:57:43.322571: step 3310, loss 0.563393.
Train: 2018-08-02T12:57:43.463194: step 3311, loss 0.549651.
Train: 2018-08-02T12:57:43.619378: step 3312, loss 0.492171.
Train: 2018-08-02T12:57:43.775590: step 3313, loss 0.539208.
Train: 2018-08-02T12:57:43.916212: step 3314, loss 0.487027.
Train: 2018-08-02T12:57:44.072397: step 3315, loss 0.529708.
Train: 2018-08-02T12:57:44.228610: step 3316, loss 0.464598.
Train: 2018-08-02T12:57:44.369232: step 3317, loss 0.520283.
Train: 2018-08-02T12:57:44.525414: step 3318, loss 0.426965.
Train: 2018-08-02T12:57:44.681658: step 3319, loss 0.526447.
Train: 2018-08-02T12:57:44.822251: step 3320, loss 0.481326.
Test: 2018-08-02T12:57:45.290861: step 3320, loss 0.550615.
Train: 2018-08-02T12:57:45.447075: step 3321, loss 0.570738.
Train: 2018-08-02T12:57:45.556454: step 3322, loss 0.55807.
Train: 2018-08-02T12:57:45.697046: step 3323, loss 0.534869.
Train: 2018-08-02T12:57:45.853259: step 3324, loss 0.534551.
Train: 2018-08-02T12:57:46.009473: step 3325, loss 0.469295.
Train: 2018-08-02T12:57:46.165686: step 3326, loss 0.487145.
Train: 2018-08-02T12:57:46.306277: step 3327, loss 0.51684.
Train: 2018-08-02T12:57:46.462462: step 3328, loss 0.617467.
Train: 2018-08-02T12:57:46.618705: step 3329, loss 0.512313.
Train: 2018-08-02T12:57:46.759297: step 3330, loss 0.598471.
Test: 2018-08-02T12:57:47.212287: step 3330, loss 0.556679.
Train: 2018-08-02T12:57:47.368530: step 3331, loss 0.519002.
Train: 2018-08-02T12:57:47.524725: step 3332, loss 0.515329.
Train: 2018-08-02T12:57:47.680956: step 3333, loss 0.462817.
Train: 2018-08-02T12:57:47.837164: step 3334, loss 0.591772.
Train: 2018-08-02T12:57:47.977762: step 3335, loss 0.573041.
Train: 2018-08-02T12:57:48.133976: step 3336, loss 0.498811.
Train: 2018-08-02T12:57:48.274567: step 3337, loss 0.482358.
Train: 2018-08-02T12:57:48.430780: step 3338, loss 0.579401.
Train: 2018-08-02T12:57:48.586994: step 3339, loss 0.559131.
Train: 2018-08-02T12:57:48.743178: step 3340, loss 0.571605.
Test: 2018-08-02T12:57:49.196196: step 3340, loss 0.551269.
Train: 2018-08-02T12:57:49.352440: step 3341, loss 0.511388.
Train: 2018-08-02T12:57:49.508623: step 3342, loss 0.482324.
Train: 2018-08-02T12:57:49.649214: step 3343, loss 0.506762.
Train: 2018-08-02T12:57:49.805458: step 3344, loss 0.531979.
Train: 2018-08-02T12:57:49.961641: step 3345, loss 0.584904.
Train: 2018-08-02T12:57:50.102265: step 3346, loss 0.480854.
Train: 2018-08-02T12:57:50.258448: step 3347, loss 0.495516.
Train: 2018-08-02T12:57:50.414661: step 3348, loss 0.506596.
Train: 2018-08-02T12:57:50.555283: step 3349, loss 0.55909.
Train: 2018-08-02T12:57:50.711496: step 3350, loss 0.556862.
Test: 2018-08-02T12:57:51.164486: step 3350, loss 0.549462.
Train: 2018-08-02T12:57:51.320699: step 3351, loss 0.500316.
Train: 2018-08-02T12:57:51.476911: step 3352, loss 0.449034.
Train: 2018-08-02T12:57:51.633162: step 3353, loss 0.509009.
Train: 2018-08-02T12:57:51.789345: step 3354, loss 0.493996.
Train: 2018-08-02T12:57:51.929961: step 3355, loss 0.474748.
Train: 2018-08-02T12:57:52.086175: step 3356, loss 0.520453.
Train: 2018-08-02T12:57:52.242358: step 3357, loss 0.494936.
Train: 2018-08-02T12:57:52.382980: step 3358, loss 0.480106.
Train: 2018-08-02T12:57:52.539194: step 3359, loss 0.5155.
Train: 2018-08-02T12:57:52.695377: step 3360, loss 0.517382.
Test: 2018-08-02T12:57:53.148426: step 3360, loss 0.549076.
Train: 2018-08-02T12:57:53.304633: step 3361, loss 0.515639.
Train: 2018-08-02T12:57:53.460822: step 3362, loss 0.520394.
Train: 2018-08-02T12:57:53.601445: step 3363, loss 0.518866.
Train: 2018-08-02T12:57:53.757628: step 3364, loss 0.538919.
Train: 2018-08-02T12:57:53.913842: step 3365, loss 0.526212.
Train: 2018-08-02T12:57:54.054463: step 3366, loss 0.574466.
Train: 2018-08-02T12:57:54.210678: step 3367, loss 0.542664.
Train: 2018-08-02T12:57:54.366861: step 3368, loss 0.581997.
Train: 2018-08-02T12:57:54.523074: step 3369, loss 0.623125.
Train: 2018-08-02T12:57:54.663666: step 3370, loss 0.547942.
Test: 2018-08-02T12:57:55.132331: step 3370, loss 0.569947.
Train: 2018-08-02T12:57:55.272899: step 3371, loss 0.529707.
Train: 2018-08-02T12:57:55.429142: step 3372, loss 0.531102.
Train: 2018-08-02T12:57:55.585356: step 3373, loss 0.524485.
Train: 2018-08-02T12:57:55.725917: step 3374, loss 0.52876.
Train: 2018-08-02T12:57:55.882130: step 3375, loss 0.536937.
Train: 2018-08-02T12:57:56.038374: step 3376, loss 0.595083.
Train: 2018-08-02T12:57:56.194558: step 3377, loss 0.536618.
Train: 2018-08-02T12:57:56.335180: step 3378, loss 0.572922.
Train: 2018-08-02T12:57:56.491362: step 3379, loss 0.593849.
Train: 2018-08-02T12:57:56.647606: step 3380, loss 0.502127.
Test: 2018-08-02T12:57:57.100620: step 3380, loss 0.562776.
Train: 2018-08-02T12:57:57.256839: step 3381, loss 0.561682.
Train: 2018-08-02T12:57:57.397400: step 3382, loss 0.525443.
Train: 2018-08-02T12:57:57.553648: step 3383, loss 0.49483.
Train: 2018-08-02T12:57:57.709857: step 3384, loss 0.450543.
Train: 2018-08-02T12:57:57.866040: step 3385, loss 0.474361.
Train: 2018-08-02T12:57:58.006633: step 3386, loss 0.470976.
Train: 2018-08-02T12:57:58.162876: step 3387, loss 0.47563.
Train: 2018-08-02T12:57:58.319091: step 3388, loss 0.573361.
Train: 2018-08-02T12:57:58.459651: step 3389, loss 0.578365.
Train: 2018-08-02T12:57:58.615895: step 3390, loss 0.480363.
Test: 2018-08-02T12:57:59.068886: step 3390, loss 0.552091.
Train: 2018-08-02T12:57:59.225128: step 3391, loss 0.495569.
Train: 2018-08-02T12:57:59.381341: step 3392, loss 0.548786.
Train: 2018-08-02T12:57:59.521934: step 3393, loss 0.584297.
Train: 2018-08-02T12:57:59.678147: step 3394, loss 0.48418.
Train: 2018-08-02T12:57:59.834359: step 3395, loss 0.602597.
Train: 2018-08-02T12:57:59.974953: step 3396, loss 0.545694.
Train: 2018-08-02T12:58:00.131136: step 3397, loss 0.542611.
Train: 2018-08-02T12:58:00.287379: step 3398, loss 0.540073.
Train: 2018-08-02T12:58:00.443562: step 3399, loss 0.541003.
Train: 2018-08-02T12:58:00.584155: step 3400, loss 0.491007.
Test: 2018-08-02T12:58:01.052797: step 3400, loss 0.553902.
Train: 2018-08-02T12:58:01.693303: step 3401, loss 0.521973.
Train: 2018-08-02T12:58:01.849484: step 3402, loss 0.570893.
Train: 2018-08-02T12:58:02.005697: step 3403, loss 0.489503.
Train: 2018-08-02T12:58:02.161940: step 3404, loss 0.567151.
Train: 2018-08-02T12:58:02.302532: step 3405, loss 0.471234.
Train: 2018-08-02T12:58:02.458745: step 3406, loss 0.549467.
Train: 2018-08-02T12:58:02.614959: step 3407, loss 0.509597.
Train: 2018-08-02T12:58:02.755521: step 3408, loss 0.490897.
Train: 2018-08-02T12:58:02.911735: step 3409, loss 0.547976.
Train: 2018-08-02T12:58:03.067972: step 3410, loss 0.488272.
Test: 2018-08-02T12:58:03.520997: step 3410, loss 0.54845.
Train: 2018-08-02T12:58:03.677211: step 3411, loss 0.528969.
Train: 2018-08-02T12:58:03.817802: step 3412, loss 0.572545.
Train: 2018-08-02T12:58:03.973986: step 3413, loss 0.541232.
Train: 2018-08-02T12:58:04.130235: step 3414, loss 0.569959.
Train: 2018-08-02T12:58:04.270790: step 3415, loss 0.551046.
Train: 2018-08-02T12:58:04.427004: step 3416, loss 0.550174.
Train: 2018-08-02T12:58:04.583243: step 3417, loss 0.561931.
Train: 2018-08-02T12:58:04.739432: step 3418, loss 0.557445.
Train: 2018-08-02T12:58:04.880054: step 3419, loss 0.533837.
Train: 2018-08-02T12:58:05.036267: step 3420, loss 0.552462.
Test: 2018-08-02T12:58:05.489257: step 3420, loss 0.558059.
Train: 2018-08-02T12:58:05.645500: step 3421, loss 0.508955.
Train: 2018-08-02T12:58:05.801682: step 3422, loss 0.528733.
Train: 2018-08-02T12:58:05.957926: step 3423, loss 0.487288.
Train: 2018-08-02T12:58:06.098518: step 3424, loss 0.548331.
Train: 2018-08-02T12:58:06.254701: step 3425, loss 0.545566.
Train: 2018-08-02T12:58:06.410946: step 3426, loss 0.542899.
Train: 2018-08-02T12:58:06.567163: step 3427, loss 0.491283.
Train: 2018-08-02T12:58:06.707750: step 3428, loss 0.596204.
Train: 2018-08-02T12:58:06.863964: step 3429, loss 0.479431.
Train: 2018-08-02T12:58:07.020177: step 3430, loss 0.46233.
Test: 2018-08-02T12:58:07.473191: step 3430, loss 0.550079.
Train: 2018-08-02T12:58:07.629410: step 3431, loss 0.524988.
Train: 2018-08-02T12:58:07.785623: step 3432, loss 0.585899.
Train: 2018-08-02T12:58:07.941807: step 3433, loss 0.526544.
Train: 2018-08-02T12:58:08.082428: step 3434, loss 0.526235.
Train: 2018-08-02T12:58:08.238637: step 3435, loss 0.478511.
Train: 2018-08-02T12:58:08.394855: step 3436, loss 0.509542.
Train: 2018-08-02T12:58:08.551069: step 3437, loss 0.537364.
Train: 2018-08-02T12:58:08.691662: step 3438, loss 0.53007.
Train: 2018-08-02T12:58:08.847874: step 3439, loss 0.486778.
Train: 2018-08-02T12:58:09.004088: step 3440, loss 0.53708.
Test: 2018-08-02T12:58:09.457077: step 3440, loss 0.552271.
Train: 2018-08-02T12:58:09.613289: step 3441, loss 0.500807.
Train: 2018-08-02T12:58:09.753882: step 3442, loss 0.485814.
Train: 2018-08-02T12:58:09.910126: step 3443, loss 0.531077.
Train: 2018-08-02T12:58:10.066339: step 3444, loss 0.451025.
Train: 2018-08-02T12:58:10.206932: step 3445, loss 0.455958.
Train: 2018-08-02T12:58:10.363145: step 3446, loss 0.556005.
Train: 2018-08-02T12:58:10.519359: step 3447, loss 0.524843.
Train: 2018-08-02T12:58:10.675571: step 3448, loss 0.417297.
Train: 2018-08-02T12:58:10.816163: step 3449, loss 0.506994.
Train: 2018-08-02T12:58:10.972377: step 3450, loss 0.504342.
Test: 2018-08-02T12:58:11.425365: step 3450, loss 0.548.
Train: 2018-08-02T12:58:11.581603: step 3451, loss 0.493697.
Train: 2018-08-02T12:58:11.737793: step 3452, loss 0.477377.
Train: 2018-08-02T12:58:11.878415: step 3453, loss 0.523039.
Train: 2018-08-02T12:58:12.034628: step 3454, loss 0.547082.
Train: 2018-08-02T12:58:12.190842: step 3455, loss 0.467585.
Train: 2018-08-02T12:58:12.347025: step 3456, loss 0.559257.
Train: 2018-08-02T12:58:12.487647: step 3457, loss 0.511884.
Train: 2018-08-02T12:58:12.643861: step 3458, loss 0.530412.
Train: 2018-08-02T12:58:12.800074: step 3459, loss 0.512912.
Train: 2018-08-02T12:58:12.956282: step 3460, loss 0.567228.
Test: 2018-08-02T12:58:13.409278: step 3460, loss 0.547463.
Train: 2018-08-02T12:58:13.565520: step 3461, loss 0.508681.
Train: 2018-08-02T12:58:13.706081: step 3462, loss 0.432448.
Train: 2018-08-02T12:58:13.862325: step 3463, loss 0.537063.
Train: 2018-08-02T12:58:14.018538: step 3464, loss 0.480437.
Train: 2018-08-02T12:58:14.159101: step 3465, loss 0.535748.
Train: 2018-08-02T12:58:14.315314: step 3466, loss 0.522401.
Train: 2018-08-02T12:58:14.471560: step 3467, loss 0.482382.
Train: 2018-08-02T12:58:14.627771: step 3468, loss 0.5421.
Train: 2018-08-02T12:58:14.783978: step 3469, loss 0.483101.
Train: 2018-08-02T12:58:14.924577: step 3470, loss 0.483932.
Test: 2018-08-02T12:58:15.377590: step 3470, loss 0.547253.
Train: 2018-08-02T12:58:15.533810: step 3471, loss 0.53002.
Train: 2018-08-02T12:58:15.689991: step 3472, loss 0.46825.
Train: 2018-08-02T12:58:15.799372: step 3473, loss 0.621607.
Train: 2018-08-02T12:58:15.955585: step 3474, loss 0.575721.
Train: 2018-08-02T12:58:16.096177: step 3475, loss 0.536107.
Train: 2018-08-02T12:58:16.252391: step 3476, loss 0.547152.
Train: 2018-08-02T12:58:16.408604: step 3477, loss 0.501924.
Train: 2018-08-02T12:58:16.564817: step 3478, loss 0.542499.
Train: 2018-08-02T12:58:16.705409: step 3479, loss 0.479576.
Train: 2018-08-02T12:58:16.861592: step 3480, loss 0.481547.
Test: 2018-08-02T12:58:17.314612: step 3480, loss 0.54926.
Train: 2018-08-02T12:58:17.470826: step 3481, loss 0.455393.
Train: 2018-08-02T12:58:17.627069: step 3482, loss 0.465886.
Train: 2018-08-02T12:58:17.767661: step 3483, loss 0.478651.
Train: 2018-08-02T12:58:17.923845: step 3484, loss 0.437678.
Train: 2018-08-02T12:58:18.080087: step 3485, loss 0.446958.
Train: 2018-08-02T12:58:18.236300: step 3486, loss 0.53005.
Train: 2018-08-02T12:58:18.376862: step 3487, loss 0.469977.
Train: 2018-08-02T12:58:18.533110: step 3488, loss 0.467468.
Train: 2018-08-02T12:58:18.689319: step 3489, loss 0.614888.
Train: 2018-08-02T12:58:18.829913: step 3490, loss 0.517562.
Test: 2018-08-02T12:58:19.298522: step 3490, loss 0.551189.
Train: 2018-08-02T12:58:19.439146: step 3491, loss 0.502719.
Train: 2018-08-02T12:58:19.595357: step 3492, loss 0.533928.
Train: 2018-08-02T12:58:19.751572: step 3493, loss 0.542035.
Train: 2018-08-02T12:58:19.892164: step 3494, loss 0.564939.
Train: 2018-08-02T12:58:20.048377: step 3495, loss 0.55306.
Train: 2018-08-02T12:58:20.204560: step 3496, loss 0.532266.
Train: 2018-08-02T12:58:20.360803: step 3497, loss 0.532372.
Train: 2018-08-02T12:58:20.517015: step 3498, loss 0.50498.
Train: 2018-08-02T12:58:20.657609: step 3499, loss 0.537576.
Train: 2018-08-02T12:58:20.813822: step 3500, loss 0.462008.
Test: 2018-08-02T12:58:21.266836: step 3500, loss 0.547201.
Train: 2018-08-02T12:58:21.954150: step 3501, loss 0.570778.
Train: 2018-08-02T12:58:22.110364: step 3502, loss 0.537576.
Train: 2018-08-02T12:58:22.266607: step 3503, loss 0.497143.
Train: 2018-08-02T12:58:22.407199: step 3504, loss 0.592725.
Train: 2018-08-02T12:58:22.563412: step 3505, loss 0.499495.
Train: 2018-08-02T12:58:22.719596: step 3506, loss 0.57892.
Train: 2018-08-02T12:58:22.875840: step 3507, loss 0.512805.
Train: 2018-08-02T12:58:23.032052: step 3508, loss 0.506243.
Train: 2018-08-02T12:58:23.172645: step 3509, loss 0.527024.
Train: 2018-08-02T12:58:23.328858: step 3510, loss 0.45911.
Test: 2018-08-02T12:58:23.797493: step 3510, loss 0.548461.
Train: 2018-08-02T12:58:23.938091: step 3511, loss 0.470529.
Train: 2018-08-02T12:58:24.094304: step 3512, loss 0.611048.
Train: 2018-08-02T12:58:24.250518: step 3513, loss 0.563021.
Train: 2018-08-02T12:58:24.406725: step 3514, loss 0.632381.
Train: 2018-08-02T12:58:24.547325: step 3515, loss 0.54779.
Train: 2018-08-02T12:58:24.703536: step 3516, loss 0.576143.
Train: 2018-08-02T12:58:24.859721: step 3517, loss 0.538571.
Train: 2018-08-02T12:58:25.015963: step 3518, loss 0.529425.
Train: 2018-08-02T12:58:25.172177: step 3519, loss 0.561564.
Train: 2018-08-02T12:58:25.328391: step 3520, loss 0.518307.
Test: 2018-08-02T12:58:25.781404: step 3520, loss 0.552338.
Train: 2018-08-02T12:58:25.922001: step 3521, loss 0.535349.
Train: 2018-08-02T12:58:26.078216: step 3522, loss 0.536232.
Train: 2018-08-02T12:58:26.234422: step 3523, loss 0.521726.
Train: 2018-08-02T12:58:26.390641: step 3524, loss 0.535882.
Train: 2018-08-02T12:58:26.531202: step 3525, loss 0.491706.
Train: 2018-08-02T12:58:26.687441: step 3526, loss 0.474339.
Train: 2018-08-02T12:58:26.843660: step 3527, loss 0.46796.
Train: 2018-08-02T12:58:26.999874: step 3528, loss 0.443851.
Train: 2018-08-02T12:58:27.140435: step 3529, loss 0.542417.
Train: 2018-08-02T12:58:27.296679: step 3530, loss 0.547587.
Test: 2018-08-02T12:58:27.765290: step 3530, loss 0.545863.
Train: 2018-08-02T12:58:27.905911: step 3531, loss 0.506481.
Train: 2018-08-02T12:58:28.062125: step 3532, loss 0.448624.
Train: 2018-08-02T12:58:28.218338: step 3533, loss 0.550943.
Train: 2018-08-02T12:58:28.358901: step 3534, loss 0.541368.
Train: 2018-08-02T12:58:28.515144: step 3535, loss 0.559679.
Train: 2018-08-02T12:58:28.671357: step 3536, loss 0.56293.
Train: 2018-08-02T12:58:28.811949: step 3537, loss 0.526309.
Train: 2018-08-02T12:58:28.968132: step 3538, loss 0.501541.
Train: 2018-08-02T12:58:29.124376: step 3539, loss 0.552831.
Train: 2018-08-02T12:58:29.280590: step 3540, loss 0.528715.
Test: 2018-08-02T12:58:29.733610: step 3540, loss 0.549935.
Train: 2018-08-02T12:58:29.889823: step 3541, loss 0.545901.
Train: 2018-08-02T12:58:30.030415: step 3542, loss 0.488527.
Train: 2018-08-02T12:58:30.186627: step 3543, loss 0.483815.
Train: 2018-08-02T12:58:30.342840: step 3544, loss 0.408564.
Train: 2018-08-02T12:58:30.499025: step 3545, loss 0.636335.
Train: 2018-08-02T12:58:30.639616: step 3546, loss 0.577188.
Train: 2018-08-02T12:58:30.795860: step 3547, loss 0.523417.
Train: 2018-08-02T12:58:30.952043: step 3548, loss 0.504496.
Train: 2018-08-02T12:58:31.108257: step 3549, loss 0.533492.
Train: 2018-08-02T12:58:31.248879: step 3550, loss 0.501042.
Test: 2018-08-02T12:58:31.717493: step 3550, loss 0.555544.
Train: 2018-08-02T12:58:31.873728: step 3551, loss 0.49391.
Train: 2018-08-02T12:58:32.076780: step 3552, loss 0.592239.
Train: 2018-08-02T12:58:32.232992: step 3553, loss 0.545506.
Train: 2018-08-02T12:58:32.389236: step 3554, loss 0.494548.
Train: 2018-08-02T12:58:32.529829: step 3555, loss 0.515662.
Train: 2018-08-02T12:58:32.686036: step 3556, loss 0.437718.
Train: 2018-08-02T12:58:32.842226: step 3557, loss 0.569193.
Train: 2018-08-02T12:58:32.998438: step 3558, loss 0.513254.
Train: 2018-08-02T12:58:33.154653: step 3559, loss 0.502879.
Train: 2018-08-02T12:58:33.295274: step 3560, loss 0.601339.
Test: 2018-08-02T12:58:33.748289: step 3560, loss 0.547636.
Train: 2018-08-02T12:58:33.904507: step 3561, loss 0.507844.
Train: 2018-08-02T12:58:34.060720: step 3562, loss 0.52301.
Train: 2018-08-02T12:58:34.216934: step 3563, loss 0.541311.
Train: 2018-08-02T12:58:34.357525: step 3564, loss 0.515482.
Train: 2018-08-02T12:58:34.513739: step 3565, loss 0.552013.
Train: 2018-08-02T12:58:34.669953: step 3566, loss 0.55793.
Train: 2018-08-02T12:58:34.810515: step 3567, loss 0.564139.
Train: 2018-08-02T12:58:34.966758: step 3568, loss 0.556231.
Train: 2018-08-02T12:58:35.122977: step 3569, loss 0.518652.
Train: 2018-08-02T12:58:35.279185: step 3570, loss 0.546839.
Test: 2018-08-02T12:58:35.732175: step 3570, loss 0.55481.
Train: 2018-08-02T12:58:35.888388: step 3571, loss 0.550232.
Train: 2018-08-02T12:58:36.029009: step 3572, loss 0.454441.
Train: 2018-08-02T12:58:36.185223: step 3573, loss 0.490417.
Train: 2018-08-02T12:58:36.341436: step 3574, loss 0.501992.
Train: 2018-08-02T12:58:36.497650: step 3575, loss 0.633583.
Train: 2018-08-02T12:58:36.638241: step 3576, loss 0.568523.
Train: 2018-08-02T12:58:36.794455: step 3577, loss 0.59518.
Train: 2018-08-02T12:58:36.950663: step 3578, loss 0.546129.
Train: 2018-08-02T12:58:37.106852: step 3579, loss 0.559464.
Train: 2018-08-02T12:58:37.247474: step 3580, loss 0.530897.
Test: 2018-08-02T12:58:37.716125: step 3580, loss 0.568617.
Train: 2018-08-02T12:58:37.934807: step 3581, loss 0.510231.
Train: 2018-08-02T12:58:38.091026: step 3582, loss 0.506343.
Train: 2018-08-02T12:58:38.247240: step 3583, loss 0.515321.
Train: 2018-08-02T12:58:38.403423: step 3584, loss 0.585954.
Train: 2018-08-02T12:58:38.544045: step 3585, loss 0.542534.
Train: 2018-08-02T12:58:38.700258: step 3586, loss 0.567917.
Train: 2018-08-02T12:58:38.856443: step 3587, loss 0.530332.
Train: 2018-08-02T12:58:38.997065: step 3588, loss 0.508121.
Train: 2018-08-02T12:58:39.153277: step 3589, loss 0.528445.
Train: 2018-08-02T12:58:39.309491: step 3590, loss 0.560528.
Test: 2018-08-02T12:58:39.762505: step 3590, loss 0.546918.
Train: 2018-08-02T12:58:39.918724: step 3591, loss 0.510579.
Train: 2018-08-02T12:58:40.074937: step 3592, loss 0.546974.
Train: 2018-08-02T12:58:40.215528: step 3593, loss 0.540034.
Train: 2018-08-02T12:58:40.371743: step 3594, loss 0.606509.
Train: 2018-08-02T12:58:40.527926: step 3595, loss 0.507225.
Train: 2018-08-02T12:58:40.684170: step 3596, loss 0.53645.
Train: 2018-08-02T12:58:40.840354: step 3597, loss 0.542302.
Train: 2018-08-02T12:58:40.996567: step 3598, loss 0.557676.
Train: 2018-08-02T12:58:41.152812: step 3599, loss 0.528363.
Train: 2018-08-02T12:58:41.309023: step 3600, loss 0.572541.
Test: 2018-08-02T12:58:41.762011: step 3600, loss 0.552897.
Train: 2018-08-02T12:58:42.433754: step 3601, loss 0.530433.
Train: 2018-08-02T12:58:42.589943: step 3602, loss 0.561327.
Train: 2018-08-02T12:58:42.730565: step 3603, loss 0.542678.
Train: 2018-08-02T12:58:42.886779: step 3604, loss 0.525306.
Train: 2018-08-02T12:58:43.042962: step 3605, loss 0.533515.
Train: 2018-08-02T12:58:43.199201: step 3606, loss 0.518401.
Train: 2018-08-02T12:58:43.355418: step 3607, loss 0.491062.
Train: 2018-08-02T12:58:43.495981: step 3608, loss 0.495261.
Train: 2018-08-02T12:58:43.652195: step 3609, loss 0.495206.
Train: 2018-08-02T12:58:43.808437: step 3610, loss 0.438068.
Test: 2018-08-02T12:58:44.261451: step 3610, loss 0.551695.
Train: 2018-08-02T12:58:44.417640: step 3611, loss 0.554266.
Train: 2018-08-02T12:58:44.573853: step 3612, loss 0.594904.
Train: 2018-08-02T12:58:44.714476: step 3613, loss 0.507749.
Train: 2018-08-02T12:58:44.870689: step 3614, loss 0.511828.
Train: 2018-08-02T12:58:45.026873: step 3615, loss 0.46565.
Train: 2018-08-02T12:58:45.183086: step 3616, loss 0.488951.
Train: 2018-08-02T12:58:45.323707: step 3617, loss 0.605658.
Train: 2018-08-02T12:58:45.479922: step 3618, loss 0.567588.
Train: 2018-08-02T12:58:45.636135: step 3619, loss 0.514531.
Train: 2018-08-02T12:58:45.776727: step 3620, loss 0.512395.
Test: 2018-08-02T12:58:46.245337: step 3620, loss 0.554462.
Train: 2018-08-02T12:58:46.401551: step 3621, loss 0.58408.
Train: 2018-08-02T12:58:46.542172: step 3622, loss 0.463108.
Train: 2018-08-02T12:58:46.698355: step 3623, loss 0.591092.
Train: 2018-08-02T12:58:46.807704: step 3624, loss 0.434881.
Train: 2018-08-02T12:58:46.963919: step 3625, loss 0.439069.
Train: 2018-08-02T12:58:47.120132: step 3626, loss 0.550123.
Train: 2018-08-02T12:58:47.260754: step 3627, loss 0.551574.
Train: 2018-08-02T12:58:47.416968: step 3628, loss 0.475096.
Train: 2018-08-02T12:58:47.573181: step 3629, loss 0.504976.
Train: 2018-08-02T12:58:47.713775: step 3630, loss 0.474056.
Test: 2018-08-02T12:58:48.166764: step 3630, loss 0.549151.
Train: 2018-08-02T12:58:48.323006: step 3631, loss 0.525484.
Train: 2018-08-02T12:58:48.479222: step 3632, loss 0.498988.
Train: 2018-08-02T12:58:48.635432: step 3633, loss 0.545634.
Train: 2018-08-02T12:58:48.775994: step 3634, loss 0.514714.
Train: 2018-08-02T12:58:48.932238: step 3635, loss 0.491545.
Train: 2018-08-02T12:58:49.088452: step 3636, loss 0.497548.
Train: 2018-08-02T12:58:49.229012: step 3637, loss 0.558122.
Train: 2018-08-02T12:58:49.385253: step 3638, loss 0.503183.
Train: 2018-08-02T12:58:49.541440: step 3639, loss 0.500599.
Train: 2018-08-02T12:58:49.697683: step 3640, loss 0.58003.
Test: 2018-08-02T12:58:50.150672: step 3640, loss 0.549692.
Train: 2018-08-02T12:58:50.306917: step 3641, loss 0.549746.
Train: 2018-08-02T12:58:50.463124: step 3642, loss 0.480549.
Train: 2018-08-02T12:58:50.603721: step 3643, loss 0.470303.
Train: 2018-08-02T12:58:50.759935: step 3644, loss 0.458957.
Train: 2018-08-02T12:58:50.916148: step 3645, loss 0.547036.
Train: 2018-08-02T12:58:51.072363: step 3646, loss 0.538067.
Train: 2018-08-02T12:58:51.212953: step 3647, loss 0.487193.
Train: 2018-08-02T12:58:51.369168: step 3648, loss 0.459178.
Train: 2018-08-02T12:58:51.525380: step 3649, loss 0.486068.
Train: 2018-08-02T12:58:51.665972: step 3650, loss 0.449559.
Test: 2018-08-02T12:58:52.134607: step 3650, loss 0.547002.
Train: 2018-08-02T12:58:52.290826: step 3651, loss 0.564221.
Train: 2018-08-02T12:58:52.431419: step 3652, loss 0.447723.
Train: 2018-08-02T12:58:52.587602: step 3653, loss 0.44598.
Train: 2018-08-02T12:58:52.743846: step 3654, loss 0.589748.
Train: 2018-08-02T12:58:52.900059: step 3655, loss 0.541026.
Train: 2018-08-02T12:58:53.056272: step 3656, loss 0.574548.
Train: 2018-08-02T12:58:53.196834: step 3657, loss 0.519273.
Train: 2018-08-02T12:58:53.353048: step 3658, loss 0.53932.
Train: 2018-08-02T12:58:53.509262: step 3659, loss 0.55745.
Train: 2018-08-02T12:58:53.665504: step 3660, loss 0.541144.
Test: 2018-08-02T12:58:54.118522: step 3660, loss 0.564715.
Train: 2018-08-02T12:58:54.274707: step 3661, loss 0.550724.
Train: 2018-08-02T12:58:54.430920: step 3662, loss 0.580863.
Train: 2018-08-02T12:58:54.571542: step 3663, loss 0.444583.
Train: 2018-08-02T12:58:54.727755: step 3664, loss 0.560419.
Train: 2018-08-02T12:58:54.883939: step 3665, loss 0.535041.
Train: 2018-08-02T12:58:55.040182: step 3666, loss 0.51124.
Train: 2018-08-02T12:58:55.180775: step 3667, loss 0.489864.
Train: 2018-08-02T12:58:55.336988: step 3668, loss 0.547115.
Train: 2018-08-02T12:58:55.493202: step 3669, loss 0.521972.
Train: 2018-08-02T12:58:55.633765: step 3670, loss 0.560384.
Test: 2018-08-02T12:58:56.102406: step 3670, loss 0.549645.
Train: 2018-08-02T12:58:56.242995: step 3671, loss 0.586368.
Train: 2018-08-02T12:58:56.399239: step 3672, loss 0.600649.
Train: 2018-08-02T12:58:56.555422: step 3673, loss 0.54163.
Train: 2018-08-02T12:58:56.711666: step 3674, loss 0.549719.
Train: 2018-08-02T12:58:56.852258: step 3675, loss 0.555872.
Train: 2018-08-02T12:58:57.008471: step 3676, loss 0.543064.
Train: 2018-08-02T12:58:57.164669: step 3677, loss 0.512514.
Train: 2018-08-02T12:58:57.305277: step 3678, loss 0.484337.
Train: 2018-08-02T12:58:57.461491: step 3679, loss 0.676875.
Train: 2018-08-02T12:58:57.617705: step 3680, loss 0.541766.
Test: 2018-08-02T12:58:58.070718: step 3680, loss 0.554239.
Train: 2018-08-02T12:58:58.226936: step 3681, loss 0.499865.
Train: 2018-08-02T12:58:58.383149: step 3682, loss 0.550673.
Train: 2018-08-02T12:58:58.523742: step 3683, loss 0.544964.
Train: 2018-08-02T12:58:58.679956: step 3684, loss 0.569931.
Train: 2018-08-02T12:58:58.836172: step 3685, loss 0.513357.
Train: 2018-08-02T12:58:58.992351: step 3686, loss 0.531276.
Train: 2018-08-02T12:58:59.132974: step 3687, loss 0.549619.
Train: 2018-08-02T12:58:59.289187: step 3688, loss 0.452113.
Train: 2018-08-02T12:58:59.445401: step 3689, loss 0.541089.
Train: 2018-08-02T12:58:59.585962: step 3690, loss 0.552009.
Test: 2018-08-02T12:59:00.054634: step 3690, loss 0.547417.
Train: 2018-08-02T12:59:00.210846: step 3691, loss 0.542884.
Train: 2018-08-02T12:59:00.351439: step 3692, loss 0.568201.
Train: 2018-08-02T12:59:00.507652: step 3693, loss 0.441018.
Train: 2018-08-02T12:59:00.663865: step 3694, loss 0.534171.
Train: 2018-08-02T12:59:00.804457: step 3695, loss 0.53012.
Train: 2018-08-02T12:59:00.960671: step 3696, loss 0.466956.
Train: 2018-08-02T12:59:01.116856: step 3697, loss 0.598238.
Train: 2018-08-02T12:59:01.273097: step 3698, loss 0.469427.
Train: 2018-08-02T12:59:01.413690: step 3699, loss 0.506848.
Train: 2018-08-02T12:59:01.569905: step 3700, loss 0.501263.
Test: 2018-08-02T12:59:02.022925: step 3700, loss 0.549536.
Train: 2018-08-02T12:59:02.632154: step 3701, loss 0.531106.
Train: 2018-08-02T12:59:02.788346: step 3702, loss 0.569487.
Train: 2018-08-02T12:59:02.944585: step 3703, loss 0.654049.
Train: 2018-08-02T12:59:03.085174: step 3704, loss 0.529752.
Train: 2018-08-02T12:59:03.241357: step 3705, loss 0.503491.
Train: 2018-08-02T12:59:03.397601: step 3706, loss 0.559962.
Train: 2018-08-02T12:59:03.553784: step 3707, loss 0.559659.
Train: 2018-08-02T12:59:03.710027: step 3708, loss 0.538489.
Train: 2018-08-02T12:59:03.850619: step 3709, loss 0.521116.
Train: 2018-08-02T12:59:04.006803: step 3710, loss 0.600327.
Test: 2018-08-02T12:59:04.459822: step 3710, loss 0.554438.
Train: 2018-08-02T12:59:04.616065: step 3711, loss 0.484462.
Train: 2018-08-02T12:59:04.772279: step 3712, loss 0.489601.
Train: 2018-08-02T12:59:04.912870: step 3713, loss 0.516688.
Train: 2018-08-02T12:59:05.069085: step 3714, loss 0.539389.
Train: 2018-08-02T12:59:05.225296: step 3715, loss 0.57727.
Train: 2018-08-02T12:59:05.381482: step 3716, loss 0.461407.
Train: 2018-08-02T12:59:05.537695: step 3717, loss 0.610398.
Train: 2018-08-02T12:59:05.693932: step 3718, loss 0.555668.
Train: 2018-08-02T12:59:05.850146: step 3719, loss 0.428901.
Train: 2018-08-02T12:59:06.021982: step 3720, loss 0.600897.
Test: 2018-08-02T12:59:06.474976: step 3720, loss 0.555051.
Train: 2018-08-02T12:59:06.631189: step 3721, loss 0.514115.
Train: 2018-08-02T12:59:06.787431: step 3722, loss 0.522191.
Train: 2018-08-02T12:59:06.943640: step 3723, loss 0.492628.
Train: 2018-08-02T12:59:07.099858: step 3724, loss 0.534773.
Train: 2018-08-02T12:59:07.240451: step 3725, loss 0.446618.
Train: 2018-08-02T12:59:07.396664: step 3726, loss 0.477956.
Train: 2018-08-02T12:59:07.552877: step 3727, loss 0.551215.
Train: 2018-08-02T12:59:07.693469: step 3728, loss 0.547916.
Train: 2018-08-02T12:59:07.849682: step 3729, loss 0.437797.
Train: 2018-08-02T12:59:08.005896: step 3730, loss 0.573874.
Test: 2018-08-02T12:59:08.458901: step 3730, loss 0.547703.
Train: 2018-08-02T12:59:08.615129: step 3731, loss 0.476098.
Train: 2018-08-02T12:59:08.755720: step 3732, loss 0.501738.
Train: 2018-08-02T12:59:08.911934: step 3733, loss 0.396549.
Train: 2018-08-02T12:59:09.068147: step 3734, loss 0.498603.
Train: 2018-08-02T12:59:09.224336: step 3735, loss 0.548961.
Train: 2018-08-02T12:59:09.364923: step 3736, loss 0.474539.
Train: 2018-08-02T12:59:09.521136: step 3737, loss 0.587865.
Train: 2018-08-02T12:59:09.677380: step 3738, loss 0.522071.
Train: 2018-08-02T12:59:09.817972: step 3739, loss 0.605435.
Train: 2018-08-02T12:59:09.974185: step 3740, loss 0.462946.
Test: 2018-08-02T12:59:10.427199: step 3740, loss 0.557819.
Train: 2018-08-02T12:59:10.583388: step 3741, loss 0.584691.
Train: 2018-08-02T12:59:10.739601: step 3742, loss 0.511182.
Train: 2018-08-02T12:59:10.880223: step 3743, loss 0.521306.
Train: 2018-08-02T12:59:11.036431: step 3744, loss 0.543633.
Train: 2018-08-02T12:59:11.192650: step 3745, loss 0.507542.
Train: 2018-08-02T12:59:11.348834: step 3746, loss 0.531101.
Train: 2018-08-02T12:59:11.505047: step 3747, loss 0.568409.
Train: 2018-08-02T12:59:11.645672: step 3748, loss 0.529603.
Train: 2018-08-02T12:59:11.801877: step 3749, loss 0.475788.
Train: 2018-08-02T12:59:11.958095: step 3750, loss 0.549496.
Test: 2018-08-02T12:59:12.411115: step 3750, loss 0.550078.
Train: 2018-08-02T12:59:12.551707: step 3751, loss 0.550697.
Train: 2018-08-02T12:59:12.707920: step 3752, loss 0.476894.
Train: 2018-08-02T12:59:12.864127: step 3753, loss 0.545022.
Train: 2018-08-02T12:59:13.004696: step 3754, loss 0.55238.
Train: 2018-08-02T12:59:13.160939: step 3755, loss 0.501591.
Train: 2018-08-02T12:59:13.317122: step 3756, loss 0.495332.
Train: 2018-08-02T12:59:13.457744: step 3757, loss 0.525822.
Train: 2018-08-02T12:59:13.613958: step 3758, loss 0.492502.
Train: 2018-08-02T12:59:13.770171: step 3759, loss 0.562568.
Train: 2018-08-02T12:59:13.910734: step 3760, loss 0.486245.
Test: 2018-08-02T12:59:14.379373: step 3760, loss 0.547408.
Train: 2018-08-02T12:59:14.519966: step 3761, loss 0.501703.
Train: 2018-08-02T12:59:14.676179: step 3762, loss 0.582024.
Train: 2018-08-02T12:59:14.832422: step 3763, loss 0.457752.
Train: 2018-08-02T12:59:14.973015: step 3764, loss 0.501557.
Train: 2018-08-02T12:59:15.129228: step 3765, loss 0.503021.
Train: 2018-08-02T12:59:15.285441: step 3766, loss 0.603762.
Train: 2018-08-02T12:59:15.426028: step 3767, loss 0.522784.
Train: 2018-08-02T12:59:15.582216: step 3768, loss 0.494902.
Train: 2018-08-02T12:59:15.738431: step 3769, loss 0.469894.
Train: 2018-08-02T12:59:15.879053: step 3770, loss 0.485695.
Test: 2018-08-02T12:59:16.347687: step 3770, loss 0.547633.
Train: 2018-08-02T12:59:16.488286: step 3771, loss 0.532105.
Train: 2018-08-02T12:59:16.644498: step 3772, loss 0.55835.
Train: 2018-08-02T12:59:16.800711: step 3773, loss 0.557952.
Train: 2018-08-02T12:59:16.941303: step 3774, loss 0.498592.
Train: 2018-08-02T12:59:17.050653: step 3775, loss 0.579063.
Train: 2018-08-02T12:59:17.206866: step 3776, loss 0.54475.
Train: 2018-08-02T12:59:17.363080: step 3777, loss 0.517036.
Train: 2018-08-02T12:59:17.503673: step 3778, loss 0.490792.
Train: 2018-08-02T12:59:17.659886: step 3779, loss 0.491234.
Train: 2018-08-02T12:59:17.816099: step 3780, loss 0.477238.
Test: 2018-08-02T12:59:18.264501: step 3780, loss 0.54782.
Train: 2018-08-02T12:59:18.420745: step 3781, loss 0.53393.
Train: 2018-08-02T12:59:18.576958: step 3782, loss 0.55804.
Train: 2018-08-02T12:59:18.733140: step 3783, loss 0.534968.
Train: 2018-08-02T12:59:18.873762: step 3784, loss 0.41226.
Train: 2018-08-02T12:59:19.029975: step 3785, loss 0.576866.
Train: 2018-08-02T12:59:19.186190: step 3786, loss 0.453504.
Train: 2018-08-02T12:59:19.326782: step 3787, loss 0.531396.
Train: 2018-08-02T12:59:19.482995: step 3788, loss 0.482568.
Train: 2018-08-02T12:59:19.639208: step 3789, loss 0.459571.
Train: 2018-08-02T12:59:19.795404: step 3790, loss 0.44616.
Test: 2018-08-02T12:59:20.248435: step 3790, loss 0.547169.
Train: 2018-08-02T12:59:20.389033: step 3791, loss 0.508863.
Train: 2018-08-02T12:59:20.545246: step 3792, loss 0.500414.
Train: 2018-08-02T12:59:20.701429: step 3793, loss 0.514187.
Train: 2018-08-02T12:59:20.842052: step 3794, loss 0.518546.
Train: 2018-08-02T12:59:20.998267: step 3795, loss 0.484872.
Train: 2018-08-02T12:59:21.154478: step 3796, loss 0.464454.
Train: 2018-08-02T12:59:21.295040: step 3797, loss 0.559108.
Train: 2018-08-02T12:59:21.451283: step 3798, loss 0.465385.
Train: 2018-08-02T12:59:21.607497: step 3799, loss 0.561046.
Train: 2018-08-02T12:59:21.748089: step 3800, loss 0.544102.
Test: 2018-08-02T12:59:22.216723: step 3800, loss 0.557309.
Train: 2018-08-02T12:59:22.888447: step 3801, loss 0.504673.
Train: 2018-08-02T12:59:23.029009: step 3802, loss 0.574015.
Train: 2018-08-02T12:59:23.185223: step 3803, loss 0.52041.
Train: 2018-08-02T12:59:23.341436: step 3804, loss 0.652275.
Train: 2018-08-02T12:59:23.482058: step 3805, loss 0.608358.
Train: 2018-08-02T12:59:23.638272: step 3806, loss 0.55474.
Train: 2018-08-02T12:59:23.794454: step 3807, loss 0.551957.
Train: 2018-08-02T12:59:23.935078: step 3808, loss 0.569022.
Train: 2018-08-02T12:59:24.091290: step 3809, loss 0.474673.
Train: 2018-08-02T12:59:24.247474: step 3810, loss 0.616234.
Test: 2018-08-02T12:59:24.716114: step 3810, loss 0.558588.
Train: 2018-08-02T12:59:24.856739: step 3811, loss 0.527532.
Train: 2018-08-02T12:59:25.012951: step 3812, loss 0.509061.
Train: 2018-08-02T12:59:25.169162: step 3813, loss 0.460379.
Train: 2018-08-02T12:59:25.309756: step 3814, loss 0.504569.
Train: 2018-08-02T12:59:25.465963: step 3815, loss 0.631428.
Train: 2018-08-02T12:59:25.622151: step 3816, loss 0.434902.
Train: 2018-08-02T12:59:25.778396: step 3817, loss 0.559587.
Train: 2018-08-02T12:59:25.918989: step 3818, loss 0.487949.
Train: 2018-08-02T12:59:26.075201: step 3819, loss 0.495965.
Train: 2018-08-02T12:59:26.231384: step 3820, loss 0.500289.
Test: 2018-08-02T12:59:26.684428: step 3820, loss 0.548278.
Train: 2018-08-02T12:59:26.840647: step 3821, loss 0.642222.
Train: 2018-08-02T12:59:26.981239: step 3822, loss 0.537663.
Train: 2018-08-02T12:59:27.137421: step 3823, loss 0.518173.
Train: 2018-08-02T12:59:27.293635: step 3824, loss 0.518216.
Train: 2018-08-02T12:59:27.449873: step 3825, loss 0.516237.
Train: 2018-08-02T12:59:27.590471: step 3826, loss 0.468603.
Train: 2018-08-02T12:59:27.746684: step 3827, loss 0.498311.
Train: 2018-08-02T12:59:27.902900: step 3828, loss 0.522179.
Train: 2018-08-02T12:59:28.043489: step 3829, loss 0.474702.
Train: 2018-08-02T12:59:28.199703: step 3830, loss 0.450639.
Test: 2018-08-02T12:59:28.652693: step 3830, loss 0.548218.
Train: 2018-08-02T12:59:28.808937: step 3831, loss 0.542431.
Train: 2018-08-02T12:59:28.965149: step 3832, loss 0.453796.
Train: 2018-08-02T12:59:29.105710: step 3833, loss 0.523212.
Train: 2018-08-02T12:59:29.261954: step 3834, loss 0.597603.
Train: 2018-08-02T12:59:29.418170: step 3835, loss 0.546284.
Train: 2018-08-02T12:59:29.558758: step 3836, loss 0.543141.
Train: 2018-08-02T12:59:29.714974: step 3837, loss 0.551728.
Train: 2018-08-02T12:59:29.871187: step 3838, loss 0.508308.
Train: 2018-08-02T12:59:30.011749: step 3839, loss 0.57804.
Train: 2018-08-02T12:59:30.167994: step 3840, loss 0.591236.
Test: 2018-08-02T12:59:30.620990: step 3840, loss 0.558343.
Train: 2018-08-02T12:59:30.777224: step 3841, loss 0.522077.
Train: 2018-08-02T12:59:30.933440: step 3842, loss 0.540737.
Train: 2018-08-02T12:59:31.074030: step 3843, loss 0.492907.
Train: 2018-08-02T12:59:31.230214: step 3844, loss 0.465203.
Train: 2018-08-02T12:59:31.386457: step 3845, loss 0.528406.
Train: 2018-08-02T12:59:31.527019: step 3846, loss 0.508835.
Train: 2018-08-02T12:59:31.683264: step 3847, loss 0.44664.
Train: 2018-08-02T12:59:31.839445: step 3848, loss 0.61757.
Train: 2018-08-02T12:59:31.980068: step 3849, loss 0.507089.
Train: 2018-08-02T12:59:32.136282: step 3850, loss 0.514594.
Test: 2018-08-02T12:59:32.604916: step 3850, loss 0.548501.
Train: 2018-08-02T12:59:32.745508: step 3851, loss 0.535292.
Train: 2018-08-02T12:59:32.901697: step 3852, loss 0.510143.
Train: 2018-08-02T12:59:33.057935: step 3853, loss 0.621538.
Train: 2018-08-02T12:59:33.198503: step 3854, loss 0.536466.
Train: 2018-08-02T12:59:33.354746: step 3855, loss 0.590517.
Train: 2018-08-02T12:59:33.510959: step 3856, loss 0.548106.
Train: 2018-08-02T12:59:33.651522: step 3857, loss 0.562838.
Train: 2018-08-02T12:59:33.807759: step 3858, loss 0.533931.
Train: 2018-08-02T12:59:33.963949: step 3859, loss 0.576582.
Train: 2018-08-02T12:59:34.120161: step 3860, loss 0.537824.
Test: 2018-08-02T12:59:34.573212: step 3860, loss 0.560694.
Train: 2018-08-02T12:59:34.729425: step 3861, loss 0.513121.
Train: 2018-08-02T12:59:34.870016: step 3862, loss 0.45644.
Train: 2018-08-02T12:59:35.026229: step 3863, loss 0.615143.
Train: 2018-08-02T12:59:35.182412: step 3864, loss 0.469564.
Train: 2018-08-02T12:59:35.323035: step 3865, loss 0.549856.
Train: 2018-08-02T12:59:35.479249: step 3866, loss 0.502378.
Train: 2018-08-02T12:59:35.635462: step 3867, loss 0.49387.
Train: 2018-08-02T12:59:35.791647: step 3868, loss 0.574562.
Train: 2018-08-02T12:59:35.932238: step 3869, loss 0.50979.
Train: 2018-08-02T12:59:36.088482: step 3870, loss 0.462365.
Test: 2018-08-02T12:59:36.541471: step 3870, loss 0.548832.
Train: 2018-08-02T12:59:36.697713: step 3871, loss 0.486398.
Train: 2018-08-02T12:59:36.853926: step 3872, loss 0.475932.
Train: 2018-08-02T12:59:36.994489: step 3873, loss 0.547562.
Train: 2018-08-02T12:59:37.150733: step 3874, loss 0.460322.
Train: 2018-08-02T12:59:37.306935: step 3875, loss 0.518285.
Train: 2018-08-02T12:59:37.447537: step 3876, loss 0.593621.
Train: 2018-08-02T12:59:37.603750: step 3877, loss 0.534476.
Train: 2018-08-02T12:59:37.759933: step 3878, loss 0.531478.
Train: 2018-08-02T12:59:37.900557: step 3879, loss 0.603511.
Train: 2018-08-02T12:59:38.056771: step 3880, loss 0.519982.
Test: 2018-08-02T12:59:38.509759: step 3880, loss 0.565113.
Train: 2018-08-02T12:59:38.665972: step 3881, loss 0.551699.
Train: 2018-08-02T12:59:38.822185: step 3882, loss 0.536731.
Train: 2018-08-02T12:59:38.962808: step 3883, loss 0.526679.
Train: 2018-08-02T12:59:39.118990: step 3884, loss 0.503583.
Train: 2018-08-02T12:59:39.275234: step 3885, loss 0.548279.
Train: 2018-08-02T12:59:39.415825: step 3886, loss 0.495489.
Train: 2018-08-02T12:59:39.572040: step 3887, loss 0.41133.
Train: 2018-08-02T12:59:39.728248: step 3888, loss 0.500407.
Train: 2018-08-02T12:59:39.868846: step 3889, loss 0.490227.
Train: 2018-08-02T12:59:40.025059: step 3890, loss 0.580858.
Test: 2018-08-02T12:59:40.478048: step 3890, loss 0.552512.
Train: 2018-08-02T12:59:40.634262: step 3891, loss 0.526079.
Train: 2018-08-02T12:59:40.790504: step 3892, loss 0.499965.
Train: 2018-08-02T12:59:40.946722: step 3893, loss 0.556303.
Train: 2018-08-02T12:59:41.087310: step 3894, loss 0.509184.
Train: 2018-08-02T12:59:41.243524: step 3895, loss 0.457947.
Train: 2018-08-02T12:59:41.399737: step 3896, loss 0.622069.
Train: 2018-08-02T12:59:41.540328: step 3897, loss 0.593059.
Train: 2018-08-02T12:59:41.696512: step 3898, loss 0.537813.
Train: 2018-08-02T12:59:41.852756: step 3899, loss 0.452122.
Train: 2018-08-02T12:59:42.008940: step 3900, loss 0.565657.
Test: 2018-08-02T12:59:42.461989: step 3900, loss 0.549609.
Train: 2018-08-02T12:59:43.118055: step 3901, loss 0.539358.
Train: 2018-08-02T12:59:43.274291: step 3902, loss 0.484323.
Train: 2018-08-02T12:59:43.414859: step 3903, loss 0.579569.
Train: 2018-08-02T12:59:43.571104: step 3904, loss 0.556183.
Train: 2018-08-02T12:59:43.711695: step 3905, loss 0.565477.
Train: 2018-08-02T12:59:43.867879: step 3906, loss 0.531811.
Train: 2018-08-02T12:59:44.024094: step 3907, loss 0.538672.
Train: 2018-08-02T12:59:44.180306: step 3908, loss 0.525987.
Train: 2018-08-02T12:59:44.320928: step 3909, loss 0.572832.
Train: 2018-08-02T12:59:44.477141: step 3910, loss 0.542562.
Test: 2018-08-02T12:59:44.945751: step 3910, loss 0.549775.
Train: 2018-08-02T12:59:45.086344: step 3911, loss 0.499992.
Train: 2018-08-02T12:59:45.242556: step 3912, loss 0.490767.
Train: 2018-08-02T12:59:45.398771: step 3913, loss 0.552225.
Train: 2018-08-02T12:59:45.539392: step 3914, loss 0.47925.
Train: 2018-08-02T12:59:45.695576: step 3915, loss 0.453788.
Train: 2018-08-02T12:59:45.851820: step 3916, loss 0.563671.
Train: 2018-08-02T12:59:46.008033: step 3917, loss 0.621787.
Train: 2018-08-02T12:59:46.148625: step 3918, loss 0.497931.
Train: 2018-08-02T12:59:46.304840: step 3919, loss 0.562331.
Train: 2018-08-02T12:59:46.461052: step 3920, loss 0.59728.
Test: 2018-08-02T12:59:46.914065: step 3920, loss 0.557393.
Train: 2018-08-02T12:59:47.070287: step 3921, loss 0.547236.
Train: 2018-08-02T12:59:47.210876: step 3922, loss 0.532944.
Train: 2018-08-02T12:59:47.367060: step 3923, loss 0.494433.
Train: 2018-08-02T12:59:47.523274: step 3924, loss 0.487852.
Train: 2018-08-02T12:59:47.663897: step 3925, loss 0.454718.
Train: 2018-08-02T12:59:47.773249: step 3926, loss 0.611147.
Train: 2018-08-02T12:59:47.929428: step 3927, loss 0.467537.
Train: 2018-08-02T12:59:48.085671: step 3928, loss 0.502602.
Train: 2018-08-02T12:59:48.226263: step 3929, loss 0.453888.
Train: 2018-08-02T12:59:48.382446: step 3930, loss 0.583546.
Test: 2018-08-02T12:59:48.835467: step 3930, loss 0.548056.
Train: 2018-08-02T12:59:48.991709: step 3931, loss 0.583706.
Train: 2018-08-02T12:59:49.147923: step 3932, loss 0.510829.
Train: 2018-08-02T12:59:49.288514: step 3933, loss 0.543016.
Train: 2018-08-02T12:59:49.444697: step 3934, loss 0.555378.
Train: 2018-08-02T12:59:49.600912: step 3935, loss 0.599382.
Train: 2018-08-02T12:59:49.757125: step 3936, loss 0.502861.
Train: 2018-08-02T12:59:49.897717: step 3937, loss 0.509545.
Train: 2018-08-02T12:59:50.053962: step 3938, loss 0.505144.
Train: 2018-08-02T12:59:50.210144: step 3939, loss 0.60259.
Train: 2018-08-02T12:59:50.350765: step 3940, loss 0.61924.
Test: 2018-08-02T12:59:50.819375: step 3940, loss 0.553144.
Train: 2018-08-02T12:59:50.959998: step 3941, loss 0.496539.
Train: 2018-08-02T12:59:51.116212: step 3942, loss 0.582595.
Train: 2018-08-02T12:59:51.272425: step 3943, loss 0.53352.
Train: 2018-08-02T12:59:51.428609: step 3944, loss 0.567332.
Train: 2018-08-02T12:59:51.569200: step 3945, loss 0.578739.
Train: 2018-08-02T12:59:51.725445: step 3946, loss 0.518295.
Train: 2018-08-02T12:59:51.881657: step 3947, loss 0.48887.
Train: 2018-08-02T12:59:52.022250: step 3948, loss 0.553839.
Train: 2018-08-02T12:59:52.178432: step 3949, loss 0.529153.
Train: 2018-08-02T12:59:52.334646: step 3950, loss 0.501391.
Test: 2018-08-02T12:59:52.787666: step 3950, loss 0.550947.
Train: 2018-08-02T12:59:52.943909: step 3951, loss 0.520998.
Train: 2018-08-02T12:59:53.084501: step 3952, loss 0.52242.
Train: 2018-08-02T12:59:53.240714: step 3953, loss 0.532845.
Train: 2018-08-02T12:59:53.396928: step 3954, loss 0.458761.
Train: 2018-08-02T12:59:53.537519: step 3955, loss 0.541667.
Train: 2018-08-02T12:59:53.693733: step 3956, loss 0.54864.
Train: 2018-08-02T12:59:53.849946: step 3957, loss 0.621344.
Train: 2018-08-02T12:59:54.006141: step 3958, loss 0.607586.
Train: 2018-08-02T12:59:54.146752: step 3959, loss 0.464523.
Train: 2018-08-02T12:59:54.302967: step 3960, loss 0.581925.
Test: 2018-08-02T12:59:54.755986: step 3960, loss 0.566125.
Train: 2018-08-02T12:59:54.912167: step 3961, loss 0.486867.
Train: 2018-08-02T12:59:55.052759: step 3962, loss 0.511062.
Train: 2018-08-02T12:59:55.209003: step 3963, loss 0.593187.
Train: 2018-08-02T12:59:55.365225: step 3964, loss 0.567232.
Train: 2018-08-02T12:59:55.505810: step 3965, loss 0.459732.
Train: 2018-08-02T12:59:55.662022: step 3966, loss 0.416025.
Train: 2018-08-02T12:59:55.818235: step 3967, loss 0.560394.
Train: 2018-08-02T12:59:55.958830: step 3968, loss 0.495597.
Train: 2018-08-02T12:59:56.115011: step 3969, loss 0.446832.
Train: 2018-08-02T12:59:56.271255: step 3970, loss 0.417697.
Test: 2018-08-02T12:59:56.724244: step 3970, loss 0.547392.
Train: 2018-08-02T12:59:56.880489: step 3971, loss 0.580423.
Train: 2018-08-02T12:59:57.021079: step 3972, loss 0.516239.
Train: 2018-08-02T12:59:57.177294: step 3973, loss 0.562119.
Train: 2018-08-02T12:59:57.333476: step 3974, loss 0.537205.
Train: 2018-08-02T12:59:57.474067: step 3975, loss 0.515789.
Train: 2018-08-02T12:59:57.630313: step 3976, loss 0.549922.
Train: 2018-08-02T12:59:57.786524: step 3977, loss 0.483139.
Train: 2018-08-02T12:59:57.942709: step 3978, loss 0.552893.
Train: 2018-08-02T12:59:58.083331: step 3979, loss 0.487844.
Train: 2018-08-02T12:59:58.239514: step 3980, loss 0.50029.
Test: 2018-08-02T12:59:58.692532: step 3980, loss 0.552146.
Train: 2018-08-02T12:59:58.848776: step 3981, loss 0.467876.
Train: 2018-08-02T12:59:58.989369: step 3982, loss 0.528495.
Train: 2018-08-02T12:59:59.161205: step 3983, loss 0.463017.
Train: 2018-08-02T12:59:59.301796: step 3984, loss 0.546244.
Train: 2018-08-02T12:59:59.458011: step 3985, loss 0.464703.
Train: 2018-08-02T12:59:59.614191: step 3986, loss 0.46096.
Train: 2018-08-02T12:59:59.754814: step 3987, loss 0.492188.
Train: 2018-08-02T12:59:59.911022: step 3988, loss 0.447338.
Train: 2018-08-02T13:00:00.067211: step 3989, loss 0.48597.
Train: 2018-08-02T13:00:00.207832: step 3990, loss 0.52856.
Test: 2018-08-02T13:00:00.676442: step 3990, loss 0.545471.
Train: 2018-08-02T13:00:00.817035: step 3991, loss 0.57372.
Train: 2018-08-02T13:00:00.973278: step 3992, loss 0.491032.
Train: 2018-08-02T13:00:01.129492: step 3993, loss 0.476601.
Train: 2018-08-02T13:00:01.285676: step 3994, loss 0.511712.
Train: 2018-08-02T13:00:01.426268: step 3995, loss 0.56704.
Train: 2018-08-02T13:00:01.582510: step 3996, loss 0.574596.
Train: 2018-08-02T13:00:01.738694: step 3997, loss 0.581502.
Train: 2018-08-02T13:00:01.879287: step 3998, loss 0.480765.
Train: 2018-08-02T13:00:02.035529: step 3999, loss 0.519991.
Train: 2018-08-02T13:00:02.191743: step 4000, loss 0.519017.
Test: 2018-08-02T13:00:02.644733: step 4000, loss 0.557263.
Train: 2018-08-02T13:00:03.285207: step 4001, loss 0.483736.
Train: 2018-08-02T13:00:03.441451: step 4002, loss 0.459443.
Train: 2018-08-02T13:00:03.597664: step 4003, loss 0.471055.
Train: 2018-08-02T13:00:03.753877: step 4004, loss 0.587718.
Train: 2018-08-02T13:00:03.894470: step 4005, loss 0.416309.
Train: 2018-08-02T13:00:04.050684: step 4006, loss 0.471788.
Train: 2018-08-02T13:00:04.206866: step 4007, loss 0.606243.
Train: 2018-08-02T13:00:04.347488: step 4008, loss 0.503907.
Train: 2018-08-02T13:00:04.503701: step 4009, loss 0.620698.
Train: 2018-08-02T13:00:04.659915: step 4010, loss 0.537065.
Test: 2018-08-02T13:00:05.128526: step 4010, loss 0.562315.
Train: 2018-08-02T13:00:05.269147: step 4011, loss 0.515312.
Train: 2018-08-02T13:00:05.425363: step 4012, loss 0.526695.
Train: 2018-08-02T13:00:05.581544: step 4013, loss 0.485908.
Train: 2018-08-02T13:00:05.722137: step 4014, loss 0.518362.
Train: 2018-08-02T13:00:05.878380: step 4015, loss 0.529057.
Train: 2018-08-02T13:00:06.018941: step 4016, loss 0.498331.
Train: 2018-08-02T13:00:06.175186: step 4017, loss 0.499379.
Train: 2018-08-02T13:00:06.331392: step 4018, loss 0.526597.
Train: 2018-08-02T13:00:06.487582: step 4019, loss 0.455654.
Train: 2018-08-02T13:00:06.628174: step 4020, loss 0.565779.
Test: 2018-08-02T13:00:07.096839: step 4020, loss 0.548896.
Train: 2018-08-02T13:00:07.237436: step 4021, loss 0.55262.
Train: 2018-08-02T13:00:07.393649: step 4022, loss 0.499559.
Train: 2018-08-02T13:00:07.549865: step 4023, loss 0.482667.
Train: 2018-08-02T13:00:07.706047: step 4024, loss 0.525575.
Train: 2018-08-02T13:00:07.846639: step 4025, loss 0.463417.
Train: 2018-08-02T13:00:08.002880: step 4026, loss 0.555438.
Train: 2018-08-02T13:00:08.159097: step 4027, loss 0.486564.
Train: 2018-08-02T13:00:08.299689: step 4028, loss 0.505792.
Train: 2018-08-02T13:00:08.455871: step 4029, loss 0.602852.
Train: 2018-08-02T13:00:08.612084: step 4030, loss 0.544707.
Test: 2018-08-02T13:00:09.065103: step 4030, loss 0.552884.
Train: 2018-08-02T13:00:09.221317: step 4031, loss 0.514444.
Train: 2018-08-02T13:00:09.361939: step 4032, loss 0.526941.
Train: 2018-08-02T13:00:09.518154: step 4033, loss 0.489489.
Train: 2018-08-02T13:00:09.674367: step 4034, loss 0.484605.
Train: 2018-08-02T13:00:09.830549: step 4035, loss 0.525922.
Train: 2018-08-02T13:00:09.971142: step 4036, loss 0.560593.
Train: 2018-08-02T13:00:10.127354: step 4037, loss 0.488529.
Train: 2018-08-02T13:00:10.283574: step 4038, loss 0.562262.
Train: 2018-08-02T13:00:10.424191: step 4039, loss 0.531774.
Train: 2018-08-02T13:00:10.580373: step 4040, loss 0.53265.
Test: 2018-08-02T13:00:11.033392: step 4040, loss 0.549635.
Train: 2018-08-02T13:00:11.189636: step 4041, loss 0.533875.
Train: 2018-08-02T13:00:11.345849: step 4042, loss 0.502522.
Train: 2018-08-02T13:00:11.486441: step 4043, loss 0.580727.
Train: 2018-08-02T13:00:11.642655: step 4044, loss 0.52163.
Train: 2018-08-02T13:00:11.798837: step 4045, loss 0.482614.
Train: 2018-08-02T13:00:11.955083: step 4046, loss 0.581107.
Train: 2018-08-02T13:00:12.095674: step 4047, loss 0.532976.
Train: 2018-08-02T13:00:12.251857: step 4048, loss 0.534264.
Train: 2018-08-02T13:00:12.408100: step 4049, loss 0.528513.
Train: 2018-08-02T13:00:12.548694: step 4050, loss 0.532949.
Test: 2018-08-02T13:00:13.017304: step 4050, loss 0.557282.
Train: 2018-08-02T13:00:13.157924: step 4051, loss 0.477267.
Train: 2018-08-02T13:00:13.314139: step 4052, loss 0.499895.
Train: 2018-08-02T13:00:13.470323: step 4053, loss 0.499855.
Train: 2018-08-02T13:00:13.610944: step 4054, loss 0.537787.
Train: 2018-08-02T13:00:13.767128: step 4055, loss 0.498157.
Train: 2018-08-02T13:00:13.923368: step 4056, loss 0.5438.
Train: 2018-08-02T13:00:14.063934: step 4057, loss 0.500523.
Train: 2018-08-02T13:00:14.220147: step 4058, loss 0.485674.
Train: 2018-08-02T13:00:14.376390: step 4059, loss 0.603744.
Train: 2018-08-02T13:00:14.516952: step 4060, loss 0.480567.
Test: 2018-08-02T13:00:14.969971: step 4060, loss 0.557236.
Train: 2018-08-02T13:00:15.126214: step 4061, loss 0.508165.
Train: 2018-08-02T13:00:15.282398: step 4062, loss 0.572172.
Train: 2018-08-02T13:00:15.438641: step 4063, loss 0.573716.
Train: 2018-08-02T13:00:15.579234: step 4064, loss 0.531902.
Train: 2018-08-02T13:00:15.735447: step 4065, loss 0.496052.
Train: 2018-08-02T13:00:15.891659: step 4066, loss 0.541633.
Train: 2018-08-02T13:00:16.047844: step 4067, loss 0.48665.
Train: 2018-08-02T13:00:16.188434: step 4068, loss 0.642654.
Train: 2018-08-02T13:00:16.344679: step 4069, loss 0.557018.
Train: 2018-08-02T13:00:16.485271: step 4070, loss 0.570043.
Test: 2018-08-02T13:00:16.953905: step 4070, loss 0.559731.
Train: 2018-08-02T13:00:17.094506: step 4071, loss 0.559506.
Train: 2018-08-02T13:00:17.250716: step 4072, loss 0.467267.
Train: 2018-08-02T13:00:17.406931: step 4073, loss 0.494526.
Train: 2018-08-02T13:00:17.547524: step 4074, loss 0.527978.
Train: 2018-08-02T13:00:17.703736: step 4075, loss 0.489811.
Train: 2018-08-02T13:00:17.859919: step 4076, loss 0.441532.
Train: 2018-08-02T13:00:17.953677: step 4077, loss 0.650645.
Train: 2018-08-02T13:00:18.109890: step 4078, loss 0.522655.
Train: 2018-08-02T13:00:18.266073: step 4079, loss 0.499204.
Train: 2018-08-02T13:00:18.422319: step 4080, loss 0.521808.
Test: 2018-08-02T13:00:18.875310: step 4080, loss 0.549536.
Train: 2018-08-02T13:00:19.031550: step 4081, loss 0.511019.
Train: 2018-08-02T13:00:19.172143: step 4082, loss 0.501676.
Train: 2018-08-02T13:00:19.328356: step 4083, loss 0.475304.
Train: 2018-08-02T13:00:19.484568: step 4084, loss 0.515265.
Train: 2018-08-02T13:00:19.625161: step 4085, loss 0.590834.
Train: 2018-08-02T13:00:19.781373: step 4086, loss 0.472228.
Train: 2018-08-02T13:00:19.937556: step 4087, loss 0.462943.
Train: 2018-08-02T13:00:20.078152: step 4088, loss 0.450562.
Train: 2018-08-02T13:00:20.234363: step 4089, loss 0.508651.
Train: 2018-08-02T13:00:20.390577: step 4090, loss 0.435566.
Test: 2018-08-02T13:00:20.843620: step 4090, loss 0.546067.
Train: 2018-08-02T13:00:20.999838: step 4091, loss 0.439964.
Train: 2018-08-02T13:00:21.156051: step 4092, loss 0.516801.
Train: 2018-08-02T13:00:21.296645: step 4093, loss 0.542447.
Train: 2018-08-02T13:00:21.452853: step 4094, loss 0.532322.
Train: 2018-08-02T13:00:21.609074: step 4095, loss 0.545004.
Train: 2018-08-02T13:00:21.765253: step 4096, loss 0.542878.
Train: 2018-08-02T13:00:21.905878: step 4097, loss 0.522658.
Train: 2018-08-02T13:00:22.062061: step 4098, loss 0.60783.
Train: 2018-08-02T13:00:22.218273: step 4099, loss 0.551198.
Train: 2018-08-02T13:00:22.358866: step 4100, loss 0.559087.
Test: 2018-08-02T13:00:22.827530: step 4100, loss 0.555427.
Train: 2018-08-02T13:00:23.467981: step 4101, loss 0.441021.
Train: 2018-08-02T13:00:23.608573: step 4102, loss 0.515966.
Train: 2018-08-02T13:00:23.764816: step 4103, loss 0.578165.
Train: 2018-08-02T13:00:23.921031: step 4104, loss 0.519016.
Train: 2018-08-02T13:00:24.061592: step 4105, loss 0.47178.
Train: 2018-08-02T13:00:24.217805: step 4106, loss 0.495339.
Train: 2018-08-02T13:00:24.374049: step 4107, loss 0.538766.
Train: 2018-08-02T13:00:24.530264: step 4108, loss 0.498584.
Train: 2018-08-02T13:00:24.670855: step 4109, loss 0.558433.
Train: 2018-08-02T13:00:24.827038: step 4110, loss 0.53577.
Test: 2018-08-02T13:00:25.280057: step 4110, loss 0.551152.
Train: 2018-08-02T13:00:25.436299: step 4111, loss 0.479021.
Train: 2018-08-02T13:00:25.592513: step 4112, loss 0.538045.
Train: 2018-08-02T13:00:25.733075: step 4113, loss 0.421774.
Train: 2018-08-02T13:00:25.889319: step 4114, loss 0.534575.
Train: 2018-08-02T13:00:26.045534: step 4115, loss 0.486445.
Train: 2018-08-02T13:00:26.186124: step 4116, loss 0.510041.
Train: 2018-08-02T13:00:26.342307: step 4117, loss 0.507415.
Train: 2018-08-02T13:00:26.498551: step 4118, loss 0.533358.
Train: 2018-08-02T13:00:26.654735: step 4119, loss 0.564439.
Train: 2018-08-02T13:00:26.795327: step 4120, loss 0.463428.
Test: 2018-08-02T13:00:27.263967: step 4120, loss 0.551216.
Train: 2018-08-02T13:00:27.404588: step 4121, loss 0.536576.
Train: 2018-08-02T13:00:27.560803: step 4122, loss 0.476002.
Train: 2018-08-02T13:00:27.717016: step 4123, loss 0.539749.
Train: 2018-08-02T13:00:27.857607: step 4124, loss 0.487228.
Train: 2018-08-02T13:00:28.013821: step 4125, loss 0.434935.
Train: 2018-08-02T13:00:28.170005: step 4126, loss 0.535515.
Train: 2018-08-02T13:00:28.326223: step 4127, loss 0.533173.
Train: 2018-08-02T13:00:28.466836: step 4128, loss 0.593832.
Train: 2018-08-02T13:00:28.623054: step 4129, loss 0.496297.
Train: 2018-08-02T13:00:28.779267: step 4130, loss 0.499504.
Test: 2018-08-02T13:00:29.247876: step 4130, loss 0.553295.
Train: 2018-08-02T13:00:29.388501: step 4131, loss 0.549435.
Train: 2018-08-02T13:00:29.544682: step 4132, loss 0.505182.
Train: 2018-08-02T13:00:29.700926: step 4133, loss 0.463733.
Train: 2018-08-02T13:00:29.841518: step 4134, loss 0.468097.
Train: 2018-08-02T13:00:29.997701: step 4135, loss 0.636933.
Train: 2018-08-02T13:00:30.153947: step 4136, loss 0.46382.
Train: 2018-08-02T13:00:30.294538: step 4137, loss 0.459903.
Train: 2018-08-02T13:00:30.450721: step 4138, loss 0.459676.
Train: 2018-08-02T13:00:30.606935: step 4139, loss 0.626481.
Train: 2018-08-02T13:00:30.763178: step 4140, loss 0.496777.
Test: 2018-08-02T13:00:31.216167: step 4140, loss 0.547558.
Train: 2018-08-02T13:00:31.372410: step 4141, loss 0.580053.
Train: 2018-08-02T13:00:31.528594: step 4142, loss 0.52159.
Train: 2018-08-02T13:00:31.669217: step 4143, loss 0.596257.
Train: 2018-08-02T13:00:31.825399: step 4144, loss 0.552988.
Train: 2018-08-02T13:00:31.981642: step 4145, loss 0.562487.
Train: 2018-08-02T13:00:32.137857: step 4146, loss 0.545702.
Train: 2018-08-02T13:00:32.294070: step 4147, loss 0.505046.
Train: 2018-08-02T13:00:32.450256: step 4148, loss 0.511183.
Train: 2018-08-02T13:00:32.606465: step 4149, loss 0.484388.
Train: 2018-08-02T13:00:32.762709: step 4150, loss 0.492981.
Test: 2018-08-02T13:00:33.215726: step 4150, loss 0.545643.
Train: 2018-08-02T13:00:33.356319: step 4151, loss 0.502357.
Train: 2018-08-02T13:00:33.528157: step 4152, loss 0.518882.
Train: 2018-08-02T13:00:33.668750: step 4153, loss 0.546792.
Train: 2018-08-02T13:00:33.824960: step 4154, loss 0.576762.
Train: 2018-08-02T13:00:33.981180: step 4155, loss 0.46957.
Train: 2018-08-02T13:00:34.121766: step 4156, loss 0.512391.
Train: 2018-08-02T13:00:34.277981: step 4157, loss 0.528647.
Train: 2018-08-02T13:00:34.434163: step 4158, loss 0.564805.
Train: 2018-08-02T13:00:34.574784: step 4159, loss 0.553759.
Train: 2018-08-02T13:00:34.730999: step 4160, loss 0.546324.
Test: 2018-08-02T13:00:35.184013: step 4160, loss 0.552903.
Train: 2018-08-02T13:00:35.340201: step 4161, loss 0.593377.
Train: 2018-08-02T13:00:35.496444: step 4162, loss 0.523574.
Train: 2018-08-02T13:00:35.652646: step 4163, loss 0.539501.
Train: 2018-08-02T13:00:35.793251: step 4164, loss 0.524507.
Train: 2018-08-02T13:00:35.949463: step 4165, loss 0.585555.
Train: 2018-08-02T13:00:36.105677: step 4166, loss 0.482243.
Train: 2018-08-02T13:00:36.246268: step 4167, loss 0.525698.
Train: 2018-08-02T13:00:36.402481: step 4168, loss 0.465159.
Train: 2018-08-02T13:00:36.558666: step 4169, loss 0.524599.
Train: 2018-08-02T13:00:36.699287: step 4170, loss 0.571525.
Test: 2018-08-02T13:00:37.152301: step 4170, loss 0.548449.
Train: 2018-08-02T13:00:37.308490: step 4171, loss 0.570748.
Train: 2018-08-02T13:00:37.464702: step 4172, loss 0.508902.
Train: 2018-08-02T13:00:37.620947: step 4173, loss 0.539465.
Train: 2018-08-02T13:00:37.761508: step 4174, loss 0.551486.
Train: 2018-08-02T13:00:37.917753: step 4175, loss 0.504621.
Train: 2018-08-02T13:00:38.073935: step 4176, loss 0.556574.
Train: 2018-08-02T13:00:38.214527: step 4177, loss 0.504426.
Train: 2018-08-02T13:00:38.370741: step 4178, loss 0.56573.
Train: 2018-08-02T13:00:38.526955: step 4179, loss 0.519655.
Train: 2018-08-02T13:00:38.683197: step 4180, loss 0.484422.
Test: 2018-08-02T13:00:39.136211: step 4180, loss 0.5481.
Train: 2018-08-02T13:00:39.339263: step 4181, loss 0.513865.
Train: 2018-08-02T13:00:39.495478: step 4182, loss 0.523524.
Train: 2018-08-02T13:00:39.651729: step 4183, loss 0.526623.
Train: 2018-08-02T13:00:39.792314: step 4184, loss 0.482417.
Train: 2018-08-02T13:00:39.948496: step 4185, loss 0.638384.
Train: 2018-08-02T13:00:40.104709: step 4186, loss 0.570875.
Train: 2018-08-02T13:00:40.245332: step 4187, loss 0.558361.
Train: 2018-08-02T13:00:40.401515: step 4188, loss 0.497673.
Train: 2018-08-02T13:00:40.557759: step 4189, loss 0.518919.
Train: 2018-08-02T13:00:40.698355: step 4190, loss 0.534614.
Test: 2018-08-02T13:00:41.166969: step 4190, loss 0.551543.
Train: 2018-08-02T13:00:41.307584: step 4191, loss 0.544516.
Train: 2018-08-02T13:00:41.463766: step 4192, loss 0.561489.
Train: 2018-08-02T13:00:41.620009: step 4193, loss 0.475901.
Train: 2018-08-02T13:00:41.760598: step 4194, loss 0.467284.
Train: 2018-08-02T13:00:41.916817: step 4195, loss 0.577739.
Train: 2018-08-02T13:00:42.073029: step 4196, loss 0.490782.
Train: 2018-08-02T13:00:42.229243: step 4197, loss 0.561131.
Train: 2018-08-02T13:00:42.369805: step 4198, loss 0.489853.
Train: 2018-08-02T13:00:42.526050: step 4199, loss 0.535455.
Train: 2018-08-02T13:00:42.682261: step 4200, loss 0.567246.
Test: 2018-08-02T13:00:43.135252: step 4200, loss 0.548548.
Train: 2018-08-02T13:00:43.760134: step 4201, loss 0.466213.
Train: 2018-08-02T13:00:43.916350: step 4202, loss 0.568574.
Train: 2018-08-02T13:00:44.056910: step 4203, loss 0.45563.
Train: 2018-08-02T13:00:44.213153: step 4204, loss 0.544843.
Train: 2018-08-02T13:00:44.369366: step 4205, loss 0.507754.
Train: 2018-08-02T13:00:44.525573: step 4206, loss 0.527676.
Train: 2018-08-02T13:00:44.666173: step 4207, loss 0.519186.
Train: 2018-08-02T13:00:44.822386: step 4208, loss 0.487362.
Train: 2018-08-02T13:00:44.962977: step 4209, loss 0.499597.
Train: 2018-08-02T13:00:45.119161: step 4210, loss 0.552098.
Test: 2018-08-02T13:00:45.587800: step 4210, loss 0.548485.
Train: 2018-08-02T13:00:45.728422: step 4211, loss 0.542458.
Train: 2018-08-02T13:00:45.884616: step 4212, loss 0.459425.
Train: 2018-08-02T13:00:46.040851: step 4213, loss 0.519383.
Train: 2018-08-02T13:00:46.197034: step 4214, loss 0.504768.
Train: 2018-08-02T13:00:46.337655: step 4215, loss 0.518533.
Train: 2018-08-02T13:00:46.493868: step 4216, loss 0.540293.
Train: 2018-08-02T13:00:46.650076: step 4217, loss 0.438959.
Train: 2018-08-02T13:00:46.790675: step 4218, loss 0.518459.
Train: 2018-08-02T13:00:46.946888: step 4219, loss 0.525499.
Train: 2018-08-02T13:00:47.103101: step 4220, loss 0.579944.
Test: 2018-08-02T13:00:47.556115: step 4220, loss 0.551438.
Train: 2018-08-02T13:00:47.712333: step 4221, loss 0.528093.
Train: 2018-08-02T13:00:47.868517: step 4222, loss 0.464946.
Train: 2018-08-02T13:00:48.024729: step 4223, loss 0.525048.
Train: 2018-08-02T13:00:48.165354: step 4224, loss 0.533843.
Train: 2018-08-02T13:00:48.321567: step 4225, loss 0.45062.
Train: 2018-08-02T13:00:48.477779: step 4226, loss 0.451905.
Train: 2018-08-02T13:00:48.618340: step 4227, loss 0.619618.
Train: 2018-08-02T13:00:48.727721: step 4228, loss 0.642869.
Train: 2018-08-02T13:00:48.883904: step 4229, loss 0.54913.
Train: 2018-08-02T13:00:49.040117: step 4230, loss 0.523425.
Test: 2018-08-02T13:00:49.493161: step 4230, loss 0.563862.
Train: 2018-08-02T13:00:49.649382: step 4231, loss 0.537607.
Train: 2018-08-02T13:00:49.789973: step 4232, loss 0.584174.
Train: 2018-08-02T13:00:49.946185: step 4233, loss 0.518329.
Train: 2018-08-02T13:00:50.102369: step 4234, loss 0.545041.
Train: 2018-08-02T13:00:50.258614: step 4235, loss 0.482559.
Train: 2018-08-02T13:00:50.399175: step 4236, loss 0.538151.
Train: 2018-08-02T13:00:50.555417: step 4237, loss 0.469476.
Train: 2018-08-02T13:00:50.711601: step 4238, loss 0.605255.
Train: 2018-08-02T13:00:50.852223: step 4239, loss 0.569861.
Train: 2018-08-02T13:00:51.008438: step 4240, loss 0.479992.
Test: 2018-08-02T13:00:51.461425: step 4240, loss 0.549817.
Train: 2018-08-02T13:00:51.617668: step 4241, loss 0.482414.
Train: 2018-08-02T13:00:51.773881: step 4242, loss 0.53465.
Train: 2018-08-02T13:00:51.914476: step 4243, loss 0.49265.
Train: 2018-08-02T13:00:52.070658: step 4244, loss 0.503193.
Train: 2018-08-02T13:00:52.226872: step 4245, loss 0.540234.
Train: 2018-08-02T13:00:52.367493: step 4246, loss 0.557327.
Train: 2018-08-02T13:00:52.523708: step 4247, loss 0.504832.
Train: 2018-08-02T13:00:52.679920: step 4248, loss 0.577277.
Train: 2018-08-02T13:00:52.820482: step 4249, loss 0.502314.
Train: 2018-08-02T13:00:52.976725: step 4250, loss 0.517724.
Test: 2018-08-02T13:00:53.429716: step 4250, loss 0.549752.
Train: 2018-08-02T13:00:53.585958: step 4251, loss 0.505833.
Train: 2018-08-02T13:00:53.742166: step 4252, loss 0.565029.
Train: 2018-08-02T13:00:53.898384: step 4253, loss 0.481772.
Train: 2018-08-02T13:00:54.038946: step 4254, loss 0.506928.
Train: 2018-08-02T13:00:54.195191: step 4255, loss 0.459525.
Train: 2018-08-02T13:00:54.351374: step 4256, loss 0.520913.
Train: 2018-08-02T13:00:54.491996: step 4257, loss 0.478828.
Train: 2018-08-02T13:00:54.648179: step 4258, loss 0.487162.
Train: 2018-08-02T13:00:54.804422: step 4259, loss 0.487833.
Train: 2018-08-02T13:00:54.960636: step 4260, loss 0.562874.
Test: 2018-08-02T13:00:55.413625: step 4260, loss 0.548444.
Train: 2018-08-02T13:00:55.569838: step 4261, loss 0.555602.
Train: 2018-08-02T13:00:55.726082: step 4262, loss 0.514748.
Train: 2018-08-02T13:00:55.882266: step 4263, loss 0.524226.
Train: 2018-08-02T13:00:56.022889: step 4264, loss 0.501844.
Train: 2018-08-02T13:00:56.179100: step 4265, loss 0.502085.
Train: 2018-08-02T13:00:56.335314: step 4266, loss 0.54281.
Train: 2018-08-02T13:00:56.475875: step 4267, loss 0.521062.
Train: 2018-08-02T13:00:56.632120: step 4268, loss 0.56398.
Train: 2018-08-02T13:00:56.788302: step 4269, loss 0.594495.
Train: 2018-08-02T13:00:56.928894: step 4270, loss 0.514441.
Test: 2018-08-02T13:00:57.397562: step 4270, loss 0.553267.
Train: 2018-08-02T13:00:57.538159: step 4271, loss 0.561393.
Train: 2018-08-02T13:00:57.694371: step 4272, loss 0.525018.
Train: 2018-08-02T13:00:57.850584: step 4273, loss 0.52391.
Train: 2018-08-02T13:00:58.006792: step 4274, loss 0.517708.
Train: 2018-08-02T13:00:58.147389: step 4275, loss 0.454775.
Train: 2018-08-02T13:00:58.303603: step 4276, loss 0.557156.
Train: 2018-08-02T13:00:58.459811: step 4277, loss 0.476252.
Train: 2018-08-02T13:00:58.616026: step 4278, loss 0.528059.
Train: 2018-08-02T13:00:58.756591: step 4279, loss 0.530317.
Train: 2018-08-02T13:00:58.912837: step 4280, loss 0.584372.
Test: 2018-08-02T13:00:59.365857: step 4280, loss 0.553634.
Train: 2018-08-02T13:00:59.522075: step 4281, loss 0.529275.
Train: 2018-08-02T13:00:59.678250: step 4282, loss 0.535284.
Train: 2018-08-02T13:00:59.834494: step 4283, loss 0.502066.
Train: 2018-08-02T13:00:59.975087: step 4284, loss 0.528591.
Train: 2018-08-02T13:01:00.131273: step 4285, loss 0.499971.
Train: 2018-08-02T13:01:00.287484: step 4286, loss 0.518172.
Train: 2018-08-02T13:01:00.443727: step 4287, loss 0.561235.
Train: 2018-08-02T13:01:00.584319: step 4288, loss 0.548751.
Train: 2018-08-02T13:01:00.740527: step 4289, loss 0.458292.
Train: 2018-08-02T13:01:00.896745: step 4290, loss 0.50346.
Test: 2018-08-02T13:01:01.349735: step 4290, loss 0.549593.
Train: 2018-08-02T13:01:01.505978: step 4291, loss 0.544692.
Train: 2018-08-02T13:01:01.662193: step 4292, loss 0.475516.
Train: 2018-08-02T13:01:01.820379: step 4293, loss 0.426081.
Train: 2018-08-02T13:01:01.961002: step 4294, loss 0.519765.
Train: 2018-08-02T13:01:02.117214: step 4295, loss 0.601588.
Train: 2018-08-02T13:01:02.273430: step 4296, loss 0.528953.
Train: 2018-08-02T13:01:02.413990: step 4297, loss 0.513607.
Train: 2018-08-02T13:01:02.570234: step 4298, loss 0.479487.
Train: 2018-08-02T13:01:02.726441: step 4299, loss 0.504022.
Train: 2018-08-02T13:01:02.867040: step 4300, loss 0.443061.
Test: 2018-08-02T13:01:03.335679: step 4300, loss 0.546489.
Train: 2018-08-02T13:01:03.944913: step 4301, loss 0.534492.
Train: 2018-08-02T13:01:04.101125: step 4302, loss 0.514215.
Train: 2018-08-02T13:01:04.257339: step 4303, loss 0.524759.
Train: 2018-08-02T13:01:04.413555: step 4304, loss 0.528822.
Train: 2018-08-02T13:01:04.554144: step 4305, loss 0.509546.
Train: 2018-08-02T13:01:04.710357: step 4306, loss 0.547582.
Train: 2018-08-02T13:01:04.866571: step 4307, loss 0.520242.
Train: 2018-08-02T13:01:05.022802: step 4308, loss 0.546885.
Train: 2018-08-02T13:01:05.163379: step 4309, loss 0.467799.
Train: 2018-08-02T13:01:05.319559: step 4310, loss 0.567761.
Test: 2018-08-02T13:01:05.772579: step 4310, loss 0.552201.
Train: 2018-08-02T13:01:05.928826: step 4311, loss 0.526394.
Train: 2018-08-02T13:01:06.085036: step 4312, loss 0.627764.
Train: 2018-08-02T13:01:06.225627: step 4313, loss 0.540278.
Train: 2018-08-02T13:01:06.381810: step 4314, loss 0.542589.
Train: 2018-08-02T13:01:06.538057: step 4315, loss 0.556544.
Train: 2018-08-02T13:01:06.694268: step 4316, loss 0.59176.
Train: 2018-08-02T13:01:06.834860: step 4317, loss 0.517291.
Train: 2018-08-02T13:01:06.991073: step 4318, loss 0.546382.
Train: 2018-08-02T13:01:07.147286: step 4319, loss 0.574775.
Train: 2018-08-02T13:01:07.287881: step 4320, loss 0.495785.
Test: 2018-08-02T13:01:07.740869: step 4320, loss 0.550112.
Train: 2018-08-02T13:01:07.897111: step 4321, loss 0.538563.
Train: 2018-08-02T13:01:08.053324: step 4322, loss 0.526032.
Train: 2018-08-02T13:01:08.193916: step 4323, loss 0.463081.
Train: 2018-08-02T13:01:08.350130: step 4324, loss 0.530959.
Train: 2018-08-02T13:01:08.506343: step 4325, loss 0.549502.
Train: 2018-08-02T13:01:08.662527: step 4326, loss 0.677515.
Train: 2018-08-02T13:01:08.803149: step 4327, loss 0.530318.
Train: 2018-08-02T13:01:08.959363: step 4328, loss 0.518124.
Train: 2018-08-02T13:01:09.099924: step 4329, loss 0.500269.
Train: 2018-08-02T13:01:09.256168: step 4330, loss 0.527935.
Test: 2018-08-02T13:01:09.709183: step 4330, loss 0.557436.
Train: 2018-08-02T13:01:09.865370: step 4331, loss 0.553382.
Train: 2018-08-02T13:01:10.021613: step 4332, loss 0.512731.
Train: 2018-08-02T13:01:10.162208: step 4333, loss 0.547906.
Train: 2018-08-02T13:01:10.318414: step 4334, loss 0.555711.
Train: 2018-08-02T13:01:10.474632: step 4335, loss 0.569386.
Train: 2018-08-02T13:01:10.630850: step 4336, loss 0.548236.
Train: 2018-08-02T13:01:10.771437: step 4337, loss 0.484114.
Train: 2018-08-02T13:01:10.927655: step 4338, loss 0.490135.
Train: 2018-08-02T13:01:11.083836: step 4339, loss 0.50677.
Train: 2018-08-02T13:01:11.240048: step 4340, loss 0.528667.
Test: 2018-08-02T13:01:11.693067: step 4340, loss 0.549768.
Train: 2018-08-02T13:01:11.849280: step 4341, loss 0.468275.
Train: 2018-08-02T13:01:11.989905: step 4342, loss 0.496268.
Train: 2018-08-02T13:01:12.146116: step 4343, loss 0.539338.
Train: 2018-08-02T13:01:12.302330: step 4344, loss 0.504675.
Train: 2018-08-02T13:01:12.442892: step 4345, loss 0.513755.
Train: 2018-08-02T13:01:12.599135: step 4346, loss 0.529833.
Train: 2018-08-02T13:01:12.755317: step 4347, loss 0.528021.
Train: 2018-08-02T13:01:12.895910: step 4348, loss 0.586951.
Train: 2018-08-02T13:01:13.052155: step 4349, loss 0.455993.
Train: 2018-08-02T13:01:13.192745: step 4350, loss 0.50563.
Test: 2018-08-02T13:01:13.661389: step 4350, loss 0.554798.
Train: 2018-08-02T13:01:13.801978: step 4351, loss 0.512421.
Train: 2018-08-02T13:01:13.958192: step 4352, loss 0.528929.
Train: 2018-08-02T13:01:14.114405: step 4353, loss 0.609258.
Train: 2018-08-02T13:01:14.254997: step 4354, loss 0.493279.
Train: 2018-08-02T13:01:14.411213: step 4355, loss 0.606019.
Train: 2018-08-02T13:01:14.567426: step 4356, loss 0.547005.
Train: 2018-08-02T13:01:14.707987: step 4357, loss 0.560656.
Train: 2018-08-02T13:01:14.864200: step 4358, loss 0.53987.
Train: 2018-08-02T13:01:15.020443: step 4359, loss 0.509076.
Train: 2018-08-02T13:01:15.176625: step 4360, loss 0.620682.
Test: 2018-08-02T13:01:15.629650: step 4360, loss 0.560034.
Train: 2018-08-02T13:01:15.785892: step 4361, loss 0.516545.
Train: 2018-08-02T13:01:15.926480: step 4362, loss 0.498795.
Train: 2018-08-02T13:01:16.082665: step 4363, loss 0.555202.
Train: 2018-08-02T13:01:16.238908: step 4364, loss 0.474151.
Train: 2018-08-02T13:01:16.379469: step 4365, loss 0.56064.
Train: 2018-08-02T13:01:16.535716: step 4366, loss 0.524496.
Train: 2018-08-02T13:01:16.691926: step 4367, loss 0.572854.
Train: 2018-08-02T13:01:16.848109: step 4368, loss 0.628732.
Train: 2018-08-02T13:01:16.998742: step 4369, loss 0.480954.
Train: 2018-08-02T13:01:17.154985: step 4370, loss 0.502759.
Test: 2018-08-02T13:01:17.608005: step 4370, loss 0.550744.
Train: 2018-08-02T13:01:17.764218: step 4371, loss 0.534792.
Train: 2018-08-02T13:01:17.904809: step 4372, loss 0.638601.
Train: 2018-08-02T13:01:18.060993: step 4373, loss 0.513619.
Train: 2018-08-02T13:01:18.217238: step 4374, loss 0.479883.
Train: 2018-08-02T13:01:18.357810: step 4375, loss 0.491665.
Train: 2018-08-02T13:01:18.514043: step 4376, loss 0.64882.
Train: 2018-08-02T13:01:18.670250: step 4377, loss 0.491512.
Train: 2018-08-02T13:01:18.810817: step 4378, loss 0.458832.
Train: 2018-08-02T13:01:18.920166: step 4379, loss 0.554098.
Train: 2018-08-02T13:01:19.076410: step 4380, loss 0.501252.
Test: 2018-08-02T13:01:19.529399: step 4380, loss 0.550835.
Train: 2018-08-02T13:01:19.685613: step 4381, loss 0.517052.
Train: 2018-08-02T13:01:19.826235: step 4382, loss 0.411983.
Train: 2018-08-02T13:01:19.982448: step 4383, loss 0.528788.
Train: 2018-08-02T13:01:20.138631: step 4384, loss 0.517724.
Train: 2018-08-02T13:01:20.279223: step 4385, loss 0.579506.
Train: 2018-08-02T13:01:20.435438: step 4386, loss 0.457859.
Train: 2018-08-02T13:01:20.591681: step 4387, loss 0.523555.
Train: 2018-08-02T13:01:20.732272: step 4388, loss 0.492462.
Train: 2018-08-02T13:01:20.888487: step 4389, loss 0.583314.
Train: 2018-08-02T13:01:21.044699: step 4390, loss 0.488629.
Test: 2018-08-02T13:01:21.497720: step 4390, loss 0.554791.
Train: 2018-08-02T13:01:21.653901: step 4391, loss 0.522056.
Train: 2018-08-02T13:01:21.794524: step 4392, loss 0.522856.
Train: 2018-08-02T13:01:21.950707: step 4393, loss 0.472403.
Train: 2018-08-02T13:01:22.106920: step 4394, loss 0.503754.
Train: 2018-08-02T13:01:22.263164: step 4395, loss 0.570478.
Train: 2018-08-02T13:01:22.403725: step 4396, loss 0.596034.
Train: 2018-08-02T13:01:22.559970: step 4397, loss 0.561706.
Train: 2018-08-02T13:01:22.716183: step 4398, loss 0.489841.
Train: 2018-08-02T13:01:22.856776: step 4399, loss 0.493967.
Train: 2018-08-02T13:01:23.012959: step 4400, loss 0.456374.
Test: 2018-08-02T13:01:23.466003: step 4400, loss 0.551664.
Train: 2018-08-02T13:01:24.106484: step 4401, loss 0.451009.
Train: 2018-08-02T13:01:24.262696: step 4402, loss 0.485614.
Train: 2018-08-02T13:01:24.418910: step 4403, loss 0.546585.
Train: 2018-08-02T13:01:24.559472: step 4404, loss 0.562951.
Train: 2018-08-02T13:01:24.715684: step 4405, loss 0.632873.
Train: 2018-08-02T13:01:24.871899: step 4406, loss 0.510994.
Train: 2018-08-02T13:01:25.012491: step 4407, loss 0.483248.
Train: 2018-08-02T13:01:25.168734: step 4408, loss 0.49844.
Train: 2018-08-02T13:01:25.324947: step 4409, loss 0.528375.
Train: 2018-08-02T13:01:25.465539: step 4410, loss 0.545334.
Test: 2018-08-02T13:01:25.934150: step 4410, loss 0.557942.
Train: 2018-08-02T13:01:26.090393: step 4411, loss 0.599472.
Train: 2018-08-02T13:01:26.230955: step 4412, loss 0.575689.
Train: 2018-08-02T13:01:26.387168: step 4413, loss 0.474539.
Train: 2018-08-02T13:01:26.543412: step 4414, loss 0.511008.
Train: 2018-08-02T13:01:26.684008: step 4415, loss 0.507931.
Train: 2018-08-02T13:01:26.840218: step 4416, loss 0.508874.
Train: 2018-08-02T13:01:26.996431: step 4417, loss 0.52494.
Train: 2018-08-02T13:01:27.152614: step 4418, loss 0.548943.
Train: 2018-08-02T13:01:27.293207: step 4419, loss 0.557777.
Train: 2018-08-02T13:01:27.449419: step 4420, loss 0.517359.
Test: 2018-08-02T13:01:27.902451: step 4420, loss 0.554038.
Train: 2018-08-02T13:01:28.058651: step 4421, loss 0.481966.
Train: 2018-08-02T13:01:28.214866: step 4422, loss 0.518935.
Train: 2018-08-02T13:01:28.355488: step 4423, loss 0.473802.
Train: 2018-08-02T13:01:28.511671: step 4424, loss 0.434808.
Train: 2018-08-02T13:01:28.667915: step 4425, loss 0.540199.
Train: 2018-08-02T13:01:28.808506: step 4426, loss 0.556774.
Train: 2018-08-02T13:01:28.964719: step 4427, loss 0.607095.
Train: 2018-08-02T13:01:29.120934: step 4428, loss 0.545599.
Train: 2018-08-02T13:01:29.277141: step 4429, loss 0.598489.
Train: 2018-08-02T13:01:29.417709: step 4430, loss 0.529628.
Test: 2018-08-02T13:01:29.870753: step 4430, loss 0.565469.
Train: 2018-08-02T13:01:30.026940: step 4431, loss 0.589402.
Train: 2018-08-02T13:01:30.183185: step 4432, loss 0.568508.
Train: 2018-08-02T13:01:30.339392: step 4433, loss 0.481293.
Train: 2018-08-02T13:01:30.495584: step 4434, loss 0.558173.
Train: 2018-08-02T13:01:30.636203: step 4435, loss 0.543219.
Train: 2018-08-02T13:01:30.792417: step 4436, loss 0.523313.
Train: 2018-08-02T13:01:30.948631: step 4437, loss 0.496307.
Train: 2018-08-02T13:01:31.089223: step 4438, loss 0.517479.
Train: 2018-08-02T13:01:31.245436: step 4439, loss 0.495226.
Train: 2018-08-02T13:01:31.401649: step 4440, loss 0.600301.
Test: 2018-08-02T13:01:31.854672: step 4440, loss 0.549088.
Train: 2018-08-02T13:01:32.010882: step 4441, loss 0.528177.
Train: 2018-08-02T13:01:32.151444: step 4442, loss 0.572999.
Train: 2018-08-02T13:01:32.307656: step 4443, loss 0.487069.
Train: 2018-08-02T13:01:32.463900: step 4444, loss 0.552979.
Train: 2018-08-02T13:01:32.604492: step 4445, loss 0.490768.
Train: 2018-08-02T13:01:32.760706: step 4446, loss 0.498551.
Train: 2018-08-02T13:01:32.916920: step 4447, loss 0.571902.
Train: 2018-08-02T13:01:33.073132: step 4448, loss 0.502346.
Train: 2018-08-02T13:01:33.213694: step 4449, loss 0.511316.
Train: 2018-08-02T13:01:33.369939: step 4450, loss 0.54027.
Test: 2018-08-02T13:01:33.822928: step 4450, loss 0.554061.
Train: 2018-08-02T13:01:33.979171: step 4451, loss 0.520062.
Train: 2018-08-02T13:01:34.135384: step 4452, loss 0.529473.
Train: 2018-08-02T13:01:34.275977: step 4453, loss 0.507035.
Train: 2018-08-02T13:01:34.432191: step 4454, loss 0.522934.
Train: 2018-08-02T13:01:34.588403: step 4455, loss 0.531286.
Train: 2018-08-02T13:01:34.728995: step 4456, loss 0.465494.
Train: 2018-08-02T13:01:34.885209: step 4457, loss 0.52241.
Train: 2018-08-02T13:01:35.041422: step 4458, loss 0.54835.
Train: 2018-08-02T13:01:35.181984: step 4459, loss 0.48389.
Train: 2018-08-02T13:01:35.338198: step 4460, loss 0.578335.
Test: 2018-08-02T13:01:35.791216: step 4460, loss 0.55336.
Train: 2018-08-02T13:01:35.947460: step 4461, loss 0.562019.
Train: 2018-08-02T13:01:36.103674: step 4462, loss 0.487932.
Train: 2018-08-02T13:01:36.244265: step 4463, loss 0.513181.
Train: 2018-08-02T13:01:36.400479: step 4464, loss 0.507805.
Train: 2018-08-02T13:01:36.556692: step 4465, loss 0.556219.
Train: 2018-08-02T13:01:36.712875: step 4466, loss 0.518682.
Train: 2018-08-02T13:01:36.853497: step 4467, loss 0.51142.
Train: 2018-08-02T13:01:37.009711: step 4468, loss 0.519681.
Train: 2018-08-02T13:01:37.150303: step 4469, loss 0.521206.
Train: 2018-08-02T13:01:37.306487: step 4470, loss 0.393975.
Test: 2018-08-02T13:01:37.759506: step 4470, loss 0.547207.
Train: 2018-08-02T13:01:37.915749: step 4471, loss 0.533651.
Train: 2018-08-02T13:01:38.071933: step 4472, loss 0.46752.
Train: 2018-08-02T13:01:38.212525: step 4473, loss 0.520999.
Train: 2018-08-02T13:01:38.368768: step 4474, loss 0.561164.
Train: 2018-08-02T13:01:38.524981: step 4475, loss 0.433539.
Train: 2018-08-02T13:01:38.681164: step 4476, loss 0.522316.
Train: 2018-08-02T13:01:38.821787: step 4477, loss 0.519698.
Train: 2018-08-02T13:01:38.977970: step 4478, loss 0.538203.
Train: 2018-08-02T13:01:39.134183: step 4479, loss 0.540258.
Train: 2018-08-02T13:01:39.290427: step 4480, loss 0.512658.
Test: 2018-08-02T13:01:39.743416: step 4480, loss 0.551849.
Train: 2018-08-02T13:01:39.884038: step 4481, loss 0.656241.
Train: 2018-08-02T13:01:40.040251: step 4482, loss 0.481594.
Train: 2018-08-02T13:01:40.196465: step 4483, loss 0.54228.
Train: 2018-08-02T13:01:40.352649: step 4484, loss 0.499299.
Train: 2018-08-02T13:01:40.493265: step 4485, loss 0.564804.
Train: 2018-08-02T13:01:40.649453: step 4486, loss 0.612349.
Train: 2018-08-02T13:01:40.805666: step 4487, loss 0.537635.
Train: 2018-08-02T13:01:40.961880: step 4488, loss 0.512417.
Train: 2018-08-02T13:01:41.102503: step 4489, loss 0.514605.
Train: 2018-08-02T13:01:41.258716: step 4490, loss 0.479171.
Test: 2018-08-02T13:01:41.711706: step 4490, loss 0.555116.
Train: 2018-08-02T13:01:41.867944: step 4491, loss 0.524066.
Train: 2018-08-02T13:01:42.024132: step 4492, loss 0.59401.
Train: 2018-08-02T13:01:42.164754: step 4493, loss 0.54726.
Train: 2018-08-02T13:01:42.320968: step 4494, loss 0.544653.
Train: 2018-08-02T13:01:42.477151: step 4495, loss 0.539173.
Train: 2018-08-02T13:01:42.617773: step 4496, loss 0.486276.
Train: 2018-08-02T13:01:42.773986: step 4497, loss 0.62379.
Train: 2018-08-02T13:01:42.930199: step 4498, loss 0.567648.
Train: 2018-08-02T13:01:43.070792: step 4499, loss 0.492252.
Train: 2018-08-02T13:01:43.227005: step 4500, loss 0.484533.
Test: 2018-08-02T13:01:43.680019: step 4500, loss 0.555548.
Train: 2018-08-02T13:01:44.351742: step 4501, loss 0.502773.
Train: 2018-08-02T13:01:44.507926: step 4502, loss 0.434843.
Train: 2018-08-02T13:01:44.664168: step 4503, loss 0.469113.
Train: 2018-08-02T13:01:44.820382: step 4504, loss 0.529095.
Train: 2018-08-02T13:01:44.960974: step 4505, loss 0.487711.
Train: 2018-08-02T13:01:45.117157: step 4506, loss 0.573856.
Train: 2018-08-02T13:01:45.257783: step 4507, loss 0.502404.
Train: 2018-08-02T13:01:45.413977: step 4508, loss 0.537248.
Train: 2018-08-02T13:01:45.570206: step 4509, loss 0.533275.
Train: 2018-08-02T13:01:45.710799: step 4510, loss 0.525903.
Test: 2018-08-02T13:01:46.163794: step 4510, loss 0.558931.
Train: 2018-08-02T13:01:46.320030: step 4511, loss 0.419681.
Train: 2018-08-02T13:01:46.476244: step 4512, loss 0.55381.
Train: 2018-08-02T13:01:46.632428: step 4513, loss 0.551921.
Train: 2018-08-02T13:01:46.773049: step 4514, loss 0.503361.
Train: 2018-08-02T13:01:46.929263: step 4515, loss 0.511535.
Train: 2018-08-02T13:01:47.085471: step 4516, loss 0.556479.
Train: 2018-08-02T13:01:47.226068: step 4517, loss 0.505162.
Train: 2018-08-02T13:01:47.382252: step 4518, loss 0.506826.
Train: 2018-08-02T13:01:47.522874: step 4519, loss 0.528168.
Train: 2018-08-02T13:01:47.679089: step 4520, loss 0.509689.
Test: 2018-08-02T13:01:48.132077: step 4520, loss 0.549291.
Train: 2018-08-02T13:01:48.288320: step 4521, loss 0.607487.
Train: 2018-08-02T13:01:48.444534: step 4522, loss 0.498675.
Train: 2018-08-02T13:01:48.585125: step 4523, loss 0.534165.
Train: 2018-08-02T13:01:48.741339: step 4524, loss 0.487142.
Train: 2018-08-02T13:01:48.897521: step 4525, loss 0.567537.
Train: 2018-08-02T13:01:49.038145: step 4526, loss 0.442963.
Train: 2018-08-02T13:01:49.194358: step 4527, loss 0.523249.
Train: 2018-08-02T13:01:49.334950: step 4528, loss 0.471912.
Train: 2018-08-02T13:01:49.491157: step 4529, loss 0.513848.
Train: 2018-08-02T13:01:49.600514: step 4530, loss 0.683572.
Test: 2018-08-02T13:01:50.053532: step 4530, loss 0.552979.
Train: 2018-08-02T13:01:50.209715: step 4531, loss 0.487685.
Train: 2018-08-02T13:01:50.365959: step 4532, loss 0.608079.
Train: 2018-08-02T13:01:50.506550: step 4533, loss 0.472096.
Train: 2018-08-02T13:01:50.662764: step 4534, loss 0.515203.
Train: 2018-08-02T13:01:50.818949: step 4535, loss 0.482787.
Train: 2018-08-02T13:01:50.959569: step 4536, loss 0.490389.
Train: 2018-08-02T13:01:51.115753: step 4537, loss 0.514162.
Train: 2018-08-02T13:01:51.271996: step 4538, loss 0.517492.
Train: 2018-08-02T13:01:51.428179: step 4539, loss 0.469271.
Train: 2018-08-02T13:01:51.584393: step 4540, loss 0.443359.
Test: 2018-08-02T13:01:52.037412: step 4540, loss 0.550263.
Train: 2018-08-02T13:01:52.193649: step 4541, loss 0.492586.
Train: 2018-08-02T13:01:52.334248: step 4542, loss 0.549081.
Train: 2018-08-02T13:01:52.490461: step 4543, loss 0.468921.
Train: 2018-08-02T13:01:52.646678: step 4544, loss 0.562208.
Train: 2018-08-02T13:01:52.787261: step 4545, loss 0.491173.
Train: 2018-08-02T13:01:52.943474: step 4546, loss 0.508016.
Train: 2018-08-02T13:01:53.099693: step 4547, loss 0.497594.
Train: 2018-08-02T13:01:53.240286: step 4548, loss 0.566465.
Train: 2018-08-02T13:01:53.396499: step 4549, loss 0.57639.
Train: 2018-08-02T13:01:53.552713: step 4550, loss 0.496253.
Test: 2018-08-02T13:01:54.005701: step 4550, loss 0.54979.
Train: 2018-08-02T13:01:54.161913: step 4551, loss 0.473698.
Train: 2018-08-02T13:01:54.318161: step 4552, loss 0.536029.
Train: 2018-08-02T13:01:54.458750: step 4553, loss 0.486716.
Train: 2018-08-02T13:01:54.614964: step 4554, loss 0.51371.
Train: 2018-08-02T13:01:54.771146: step 4555, loss 0.59128.
Train: 2018-08-02T13:01:54.911770: step 4556, loss 0.567957.
Train: 2018-08-02T13:01:55.067953: step 4557, loss 0.497782.
Train: 2018-08-02T13:01:55.224196: step 4558, loss 0.511738.
Train: 2018-08-02T13:01:55.364788: step 4559, loss 0.557253.
Train: 2018-08-02T13:01:55.521001: step 4560, loss 0.55398.
Test: 2018-08-02T13:01:55.973990: step 4560, loss 0.555335.
Train: 2018-08-02T13:01:56.130233: step 4561, loss 0.539419.
Train: 2018-08-02T13:01:56.286447: step 4562, loss 0.567898.
Train: 2018-08-02T13:01:56.427009: step 4563, loss 0.556183.
Train: 2018-08-02T13:01:56.583250: step 4564, loss 0.527152.
Train: 2018-08-02T13:01:56.739460: step 4565, loss 0.59546.
Train: 2018-08-02T13:01:56.880044: step 4566, loss 0.469988.
Train: 2018-08-02T13:01:57.036272: step 4567, loss 0.506649.
Train: 2018-08-02T13:01:57.192485: step 4568, loss 0.489155.
Train: 2018-08-02T13:01:57.348698: step 4569, loss 0.458399.
Train: 2018-08-02T13:01:57.489259: step 4570, loss 0.456264.
Test: 2018-08-02T13:01:57.957900: step 4570, loss 0.549021.
Train: 2018-08-02T13:01:58.098523: step 4571, loss 0.453492.
Train: 2018-08-02T13:01:58.254736: step 4572, loss 0.55146.
Train: 2018-08-02T13:01:58.410949: step 4573, loss 0.606323.
Train: 2018-08-02T13:01:58.551542: step 4574, loss 0.542353.
Train: 2018-08-02T13:01:58.707725: step 4575, loss 0.543903.
Train: 2018-08-02T13:01:58.863968: step 4576, loss 0.613628.
Train: 2018-08-02T13:01:59.020151: step 4577, loss 0.525681.
Train: 2018-08-02T13:01:59.160775: step 4578, loss 0.580358.
Train: 2018-08-02T13:01:59.316987: step 4579, loss 0.571071.
Train: 2018-08-02T13:01:59.473201: step 4580, loss 0.536371.
Test: 2018-08-02T13:01:59.926220: step 4580, loss 0.560529.
Train: 2018-08-02T13:02:00.082432: step 4581, loss 0.518221.
Train: 2018-08-02T13:02:00.223025: step 4582, loss 0.522271.
Train: 2018-08-02T13:02:00.379239: step 4583, loss 0.519451.
Train: 2018-08-02T13:02:00.535452: step 4584, loss 0.61283.
Train: 2018-08-02T13:02:00.691636: step 4585, loss 0.464266.
Train: 2018-08-02T13:02:00.832258: step 4586, loss 0.566717.
Train: 2018-08-02T13:02:00.988471: step 4587, loss 0.561006.
Train: 2018-08-02T13:02:01.144681: step 4588, loss 0.548128.
Train: 2018-08-02T13:02:01.300868: step 4589, loss 0.504164.
Train: 2018-08-02T13:02:01.457113: step 4590, loss 0.495424.
Test: 2018-08-02T13:02:01.910126: step 4590, loss 0.548936.
Train: 2018-08-02T13:02:02.050722: step 4591, loss 0.464183.
Train: 2018-08-02T13:02:02.206936: step 4592, loss 0.554566.
Train: 2018-08-02T13:02:02.363118: step 4593, loss 0.592177.
Train: 2018-08-02T13:02:02.503711: step 4594, loss 0.499525.
Train: 2018-08-02T13:02:02.659954: step 4595, loss 0.523173.
Train: 2018-08-02T13:02:02.816168: step 4596, loss 0.535897.
Train: 2018-08-02T13:02:02.972383: step 4597, loss 0.587324.
Train: 2018-08-02T13:02:03.112974: step 4598, loss 0.556803.
Train: 2018-08-02T13:02:03.269181: step 4599, loss 0.580368.
Train: 2018-08-02T13:02:03.425370: step 4600, loss 0.552999.
Test: 2018-08-02T13:02:03.878421: step 4600, loss 0.559916.
Train: 2018-08-02T13:02:04.534484: step 4601, loss 0.486781.
Train: 2018-08-02T13:02:04.675108: step 4602, loss 0.568275.
Train: 2018-08-02T13:02:04.831291: step 4603, loss 0.530032.
Train: 2018-08-02T13:02:04.987534: step 4604, loss 0.556712.
Train: 2018-08-02T13:02:05.128127: step 4605, loss 0.577754.
Train: 2018-08-02T13:02:05.284340: step 4606, loss 0.554865.
Train: 2018-08-02T13:02:05.440554: step 4607, loss 0.605911.
Train: 2018-08-02T13:02:05.596767: step 4608, loss 0.530006.
Train: 2018-08-02T13:02:05.737359: step 4609, loss 0.569324.
Train: 2018-08-02T13:02:05.893572: step 4610, loss 0.580585.
Test: 2018-08-02T13:02:06.346562: step 4610, loss 0.560152.
Train: 2018-08-02T13:02:06.502805: step 4611, loss 0.480833.
Train: 2018-08-02T13:02:06.659018: step 4612, loss 0.499933.
Train: 2018-08-02T13:02:06.815225: step 4613, loss 0.588987.
Train: 2018-08-02T13:02:06.955823: step 4614, loss 0.557782.
Train: 2018-08-02T13:02:07.112037: step 4615, loss 0.536886.
Train: 2018-08-02T13:02:07.268250: step 4616, loss 0.419431.
Train: 2018-08-02T13:02:07.408843: step 4617, loss 0.482382.
Train: 2018-08-02T13:02:07.565056: step 4618, loss 0.546387.
Train: 2018-08-02T13:02:07.721269: step 4619, loss 0.501956.
Train: 2018-08-02T13:02:07.877483: step 4620, loss 0.477856.
Test: 2018-08-02T13:02:08.330472: step 4620, loss 0.549351.
Train: 2018-08-02T13:02:08.486684: step 4621, loss 0.537969.
Train: 2018-08-02T13:02:08.627278: step 4622, loss 0.521908.
Train: 2018-08-02T13:02:08.783491: step 4623, loss 0.431477.
Train: 2018-08-02T13:02:08.939733: step 4624, loss 0.632978.
Train: 2018-08-02T13:02:09.080296: step 4625, loss 0.611126.
Train: 2018-08-02T13:02:09.236540: step 4626, loss 0.534468.
Train: 2018-08-02T13:02:09.392754: step 4627, loss 0.534097.
Train: 2018-08-02T13:02:09.548967: step 4628, loss 0.531273.
Train: 2018-08-02T13:02:09.689558: step 4629, loss 0.538705.
Train: 2018-08-02T13:02:09.845773: step 4630, loss 0.574346.
Test: 2018-08-02T13:02:10.298762: step 4630, loss 0.550716.
Train: 2018-08-02T13:02:10.455004: step 4631, loss 0.597077.
Train: 2018-08-02T13:02:10.611218: step 4632, loss 0.603558.
Train: 2018-08-02T13:02:10.751809: step 4633, loss 0.510774.
Train: 2018-08-02T13:02:10.908023: step 4634, loss 0.532361.
Train: 2018-08-02T13:02:11.064207: step 4635, loss 0.540312.
Train: 2018-08-02T13:02:11.204829: step 4636, loss 0.549256.
Train: 2018-08-02T13:02:11.361012: step 4637, loss 0.469918.
Train: 2018-08-02T13:02:11.517225: step 4638, loss 0.584987.
Train: 2018-08-02T13:02:11.673468: step 4639, loss 0.562076.
Train: 2018-08-02T13:02:11.814030: step 4640, loss 0.531518.
Test: 2018-08-02T13:02:12.282684: step 4640, loss 0.548918.
Train: 2018-08-02T13:02:12.423264: step 4641, loss 0.510509.
Train: 2018-08-02T13:02:12.579507: step 4642, loss 0.620122.
Train: 2018-08-02T13:02:12.735720: step 4643, loss 0.457267.
Train: 2018-08-02T13:02:12.876283: step 4644, loss 0.537501.
Train: 2018-08-02T13:02:13.032526: step 4645, loss 0.484932.
Train: 2018-08-02T13:02:13.188738: step 4646, loss 0.552901.
Train: 2018-08-02T13:02:13.329331: step 4647, loss 0.551502.
Train: 2018-08-02T13:02:13.485545: step 4648, loss 0.496416.
Train: 2018-08-02T13:02:13.641728: step 4649, loss 0.516783.
Train: 2018-08-02T13:02:13.797972: step 4650, loss 0.495261.
Test: 2018-08-02T13:02:14.250961: step 4650, loss 0.546549.
Train: 2018-08-02T13:02:14.391553: step 4651, loss 0.572807.
Train: 2018-08-02T13:02:14.547796: step 4652, loss 0.591123.
Train: 2018-08-02T13:02:14.704003: step 4653, loss 0.522684.
Train: 2018-08-02T13:02:14.860222: step 4654, loss 0.491892.
Train: 2018-08-02T13:02:15.000785: step 4655, loss 0.575904.
Train: 2018-08-02T13:02:15.157028: step 4656, loss 0.548032.
Train: 2018-08-02T13:02:15.313212: step 4657, loss 0.496265.
Train: 2018-08-02T13:02:15.453804: step 4658, loss 0.500297.
Train: 2018-08-02T13:02:15.610017: step 4659, loss 0.594748.
Train: 2018-08-02T13:02:15.766260: step 4660, loss 0.571573.
Test: 2018-08-02T13:02:16.219274: step 4660, loss 0.55338.
Train: 2018-08-02T13:02:16.375463: step 4661, loss 0.479371.
Train: 2018-08-02T13:02:16.531706: step 4662, loss 0.53204.
Train: 2018-08-02T13:02:16.672298: step 4663, loss 0.451288.
Train: 2018-08-02T13:02:16.828511: step 4664, loss 0.549296.
Train: 2018-08-02T13:02:16.969103: step 4665, loss 0.5403.
Train: 2018-08-02T13:02:17.125317: step 4666, loss 0.548258.
Train: 2018-08-02T13:02:17.281530: step 4667, loss 0.490505.
Train: 2018-08-02T13:02:17.437744: step 4668, loss 0.532093.
Train: 2018-08-02T13:02:17.578336: step 4669, loss 0.534475.
Train: 2018-08-02T13:02:17.734520: step 4670, loss 0.509734.
Test: 2018-08-02T13:02:18.187538: step 4670, loss 0.549417.
Train: 2018-08-02T13:02:18.343782: step 4671, loss 0.507376.
Train: 2018-08-02T13:02:18.484343: step 4672, loss 0.446101.
Train: 2018-08-02T13:02:18.640556: step 4673, loss 0.449507.
Train: 2018-08-02T13:02:18.796801: step 4674, loss 0.477578.
Train: 2018-08-02T13:02:18.953014: step 4675, loss 0.49564.
Train: 2018-08-02T13:02:19.109221: step 4676, loss 0.435707.
Train: 2018-08-02T13:02:19.249821: step 4677, loss 0.57745.
Train: 2018-08-02T13:02:19.406032: step 4678, loss 0.636663.
Train: 2018-08-02T13:02:19.562217: step 4679, loss 0.464667.
Train: 2018-08-02T13:02:19.718460: step 4680, loss 0.546575.
Test: 2018-08-02T13:02:20.155852: step 4680, loss 0.546488.
Train: 2018-08-02T13:02:20.265207: step 4681, loss 0.456618.
Train: 2018-08-02T13:02:20.421390: step 4682, loss 0.495114.
Train: 2018-08-02T13:02:20.577633: step 4683, loss 0.512121.
Train: 2018-08-02T13:02:20.718197: step 4684, loss 0.51979.
Train: 2018-08-02T13:02:20.874409: step 4685, loss 0.490027.
Train: 2018-08-02T13:02:21.030652: step 4686, loss 0.417453.
Train: 2018-08-02T13:02:21.186866: step 4687, loss 0.489242.
Train: 2018-08-02T13:02:21.327457: step 4688, loss 0.516808.
Train: 2018-08-02T13:02:21.483671: step 4689, loss 0.412225.
Train: 2018-08-02T13:02:21.639855: step 4690, loss 0.527873.
Test: 2018-08-02T13:02:22.092873: step 4690, loss 0.554579.
Train: 2018-08-02T13:02:22.249091: step 4691, loss 0.52527.
Train: 2018-08-02T13:02:22.389680: step 4692, loss 0.477222.
Train: 2018-08-02T13:02:22.545892: step 4693, loss 0.510973.
Train: 2018-08-02T13:02:22.686515: step 4694, loss 0.480031.
Train: 2018-08-02T13:02:22.842699: step 4695, loss 0.461794.
Train: 2018-08-02T13:02:22.998941: step 4696, loss 0.58158.
Train: 2018-08-02T13:02:23.139534: step 4697, loss 0.51202.
Train: 2018-08-02T13:02:23.295747: step 4698, loss 0.561332.
Train: 2018-08-02T13:02:23.451955: step 4699, loss 0.49658.
Train: 2018-08-02T13:02:23.608174: step 4700, loss 0.485002.
Test: 2018-08-02T13:02:24.061188: step 4700, loss 0.552668.
Train: 2018-08-02T13:02:24.686047: step 4701, loss 0.497663.
Train: 2018-08-02T13:02:24.842258: step 4702, loss 0.548034.
Train: 2018-08-02T13:02:24.998473: step 4703, loss 0.492759.
Train: 2018-08-02T13:02:25.139060: step 4704, loss 0.528553.
Train: 2018-08-02T13:02:25.295248: step 4705, loss 0.458715.
Train: 2018-08-02T13:02:25.451492: step 4706, loss 0.521251.
Train: 2018-08-02T13:02:25.607706: step 4707, loss 0.452162.
Train: 2018-08-02T13:02:25.748292: step 4708, loss 0.571665.
Train: 2018-08-02T13:02:25.904482: step 4709, loss 0.477739.
Train: 2018-08-02T13:02:26.060720: step 4710, loss 0.499176.
Test: 2018-08-02T13:02:26.513744: step 4710, loss 0.551716.
Train: 2018-08-02T13:02:26.669957: step 4711, loss 0.584807.
Train: 2018-08-02T13:02:26.826170: step 4712, loss 0.587792.
Train: 2018-08-02T13:02:26.966733: step 4713, loss 0.489661.
Train: 2018-08-02T13:02:27.122976: step 4714, loss 0.480262.
Train: 2018-08-02T13:02:27.279191: step 4715, loss 0.470378.
Train: 2018-08-02T13:02:27.419782: step 4716, loss 0.465989.
Train: 2018-08-02T13:02:27.575995: step 4717, loss 0.517733.
Train: 2018-08-02T13:02:27.732208: step 4718, loss 0.602437.
Train: 2018-08-02T13:02:27.872770: step 4719, loss 0.612402.
Train: 2018-08-02T13:02:28.029013: step 4720, loss 0.478262.
Test: 2018-08-02T13:02:28.482027: step 4720, loss 0.556145.
Train: 2018-08-02T13:02:28.638248: step 4721, loss 0.469998.
Train: 2018-08-02T13:02:28.794454: step 4722, loss 0.494042.
Train: 2018-08-02T13:02:28.935021: step 4723, loss 0.546079.
Train: 2018-08-02T13:02:29.091234: step 4724, loss 0.566555.
Train: 2018-08-02T13:02:29.247447: step 4725, loss 0.521899.
Train: 2018-08-02T13:02:29.388070: step 4726, loss 0.519613.
Train: 2018-08-02T13:02:29.544284: step 4727, loss 0.524776.
Train: 2018-08-02T13:02:29.700497: step 4728, loss 0.522755.
Train: 2018-08-02T13:02:29.841090: step 4729, loss 0.520957.
Train: 2018-08-02T13:02:29.997273: step 4730, loss 0.515349.
Test: 2018-08-02T13:02:30.450317: step 4730, loss 0.549241.
Train: 2018-08-02T13:02:30.606536: step 4731, loss 0.46632.
Train: 2018-08-02T13:02:30.762749: step 4732, loss 0.457981.
Train: 2018-08-02T13:02:30.903340: step 4733, loss 0.470248.
Train: 2018-08-02T13:02:31.059555: step 4734, loss 0.586661.
Train: 2018-08-02T13:02:31.215768: step 4735, loss 0.578611.
Train: 2018-08-02T13:02:31.371981: step 4736, loss 0.570119.
Train: 2018-08-02T13:02:31.512573: step 4737, loss 0.572277.
Train: 2018-08-02T13:02:31.668756: step 4738, loss 0.558609.
Train: 2018-08-02T13:02:31.824969: step 4739, loss 0.517199.
Train: 2018-08-02T13:02:31.965562: step 4740, loss 0.538896.
Test: 2018-08-02T13:02:32.434227: step 4740, loss 0.571418.
Train: 2018-08-02T13:02:32.574827: step 4741, loss 0.553907.
Train: 2018-08-02T13:02:32.777872: step 4742, loss 0.555287.
Train: 2018-08-02T13:02:32.934086: step 4743, loss 0.536154.
Train: 2018-08-02T13:02:33.090298: step 4744, loss 0.545212.
Train: 2018-08-02T13:02:33.230920: step 4745, loss 0.550287.
Train: 2018-08-02T13:02:33.387134: step 4746, loss 0.558407.
Train: 2018-08-02T13:02:33.543348: step 4747, loss 0.518224.
Train: 2018-08-02T13:02:33.683942: step 4748, loss 0.518418.
Train: 2018-08-02T13:02:33.840155: step 4749, loss 0.477324.
Train: 2018-08-02T13:02:33.996366: step 4750, loss 0.541464.
Test: 2018-08-02T13:02:34.449386: step 4750, loss 0.546478.
Train: 2018-08-02T13:02:34.605599: step 4751, loss 0.636304.
Train: 2018-08-02T13:02:34.746191: step 4752, loss 0.530989.
Train: 2018-08-02T13:02:34.902399: step 4753, loss 0.545786.
Train: 2018-08-02T13:02:35.058618: step 4754, loss 0.523187.
Train: 2018-08-02T13:02:35.199209: step 4755, loss 0.453083.
Train: 2018-08-02T13:02:35.355423: step 4756, loss 0.57578.
Train: 2018-08-02T13:02:35.511637: step 4757, loss 0.518789.
Train: 2018-08-02T13:02:35.652199: step 4758, loss 0.48676.
Train: 2018-08-02T13:02:35.808442: step 4759, loss 0.474557.
Train: 2018-08-02T13:02:35.964655: step 4760, loss 0.578596.
Test: 2018-08-02T13:02:36.417644: step 4760, loss 0.547566.
Train: 2018-08-02T13:02:36.558266: step 4761, loss 0.53969.
Train: 2018-08-02T13:02:36.714479: step 4762, loss 0.466094.
Train: 2018-08-02T13:02:36.870693: step 4763, loss 0.555056.
Train: 2018-08-02T13:02:37.026901: step 4764, loss 0.470099.
Train: 2018-08-02T13:02:37.167498: step 4765, loss 0.530978.
Train: 2018-08-02T13:02:37.323712: step 4766, loss 0.493023.
Train: 2018-08-02T13:02:37.479931: step 4767, loss 0.506403.
Train: 2018-08-02T13:02:37.620487: step 4768, loss 0.559624.
Train: 2018-08-02T13:02:37.776701: step 4769, loss 0.556895.
Train: 2018-08-02T13:02:37.932945: step 4770, loss 0.49309.
Test: 2018-08-02T13:02:38.385960: step 4770, loss 0.548814.
Train: 2018-08-02T13:02:38.542178: step 4771, loss 0.49582.
Train: 2018-08-02T13:02:38.682769: step 4772, loss 0.512838.
Train: 2018-08-02T13:02:38.838953: step 4773, loss 0.558858.
Train: 2018-08-02T13:02:38.995196: step 4774, loss 0.521662.
Train: 2018-08-02T13:02:39.135787: step 4775, loss 0.461645.
Train: 2018-08-02T13:02:39.292001: step 4776, loss 0.524906.
Train: 2018-08-02T13:02:39.448215: step 4777, loss 0.500113.
Train: 2018-08-02T13:02:39.604397: step 4778, loss 0.585759.
Train: 2018-08-02T13:02:39.760611: step 4779, loss 0.56282.
Train: 2018-08-02T13:02:39.901203: step 4780, loss 0.57065.
Test: 2018-08-02T13:02:40.369843: step 4780, loss 0.548096.
Train: 2018-08-02T13:02:40.572952: step 4781, loss 0.498697.
Train: 2018-08-02T13:02:40.729165: step 4782, loss 0.462068.
Train: 2018-08-02T13:02:40.869757: step 4783, loss 0.601168.
Train: 2018-08-02T13:02:41.025970: step 4784, loss 0.491582.
Train: 2018-08-02T13:02:41.182179: step 4785, loss 0.562292.
Train: 2018-08-02T13:02:41.338393: step 4786, loss 0.527757.
Train: 2018-08-02T13:02:41.494582: step 4787, loss 0.540464.
Train: 2018-08-02T13:02:41.650794: step 4788, loss 0.5193.
Train: 2018-08-02T13:02:41.807007: step 4789, loss 0.541945.
Train: 2018-08-02T13:02:41.947599: step 4790, loss 0.545618.
Test: 2018-08-02T13:02:42.416240: step 4790, loss 0.547745.
Train: 2018-08-02T13:02:42.556861: step 4791, loss 0.529783.
Train: 2018-08-02T13:02:42.713045: step 4792, loss 0.550082.
Train: 2018-08-02T13:02:42.869289: step 4793, loss 0.472169.
Train: 2018-08-02T13:02:43.009850: step 4794, loss 0.466442.
Train: 2018-08-02T13:02:43.166063: step 4795, loss 0.632273.
Train: 2018-08-02T13:02:43.322308: step 4796, loss 0.485505.
Train: 2018-08-02T13:02:43.478521: step 4797, loss 0.578567.
Train: 2018-08-02T13:02:43.619113: step 4798, loss 0.511954.
Train: 2018-08-02T13:02:43.775295: step 4799, loss 0.563185.
Train: 2018-08-02T13:02:43.931540: step 4800, loss 0.545386.
Test: 2018-08-02T13:02:44.384530: step 4800, loss 0.552064.
Train: 2018-08-02T13:02:45.040626: step 4801, loss 0.551564.
Train: 2018-08-02T13:02:45.196866: step 4802, loss 0.548258.
Train: 2018-08-02T13:02:45.337461: step 4803, loss 0.525676.
Train: 2018-08-02T13:02:45.493651: step 4804, loss 0.516449.
Train: 2018-08-02T13:02:45.649887: step 4805, loss 0.496617.
Train: 2018-08-02T13:02:45.806070: step 4806, loss 0.543152.
Train: 2018-08-02T13:02:45.946693: step 4807, loss 0.555043.
Train: 2018-08-02T13:02:46.102876: step 4808, loss 0.526241.
Train: 2018-08-02T13:02:46.259120: step 4809, loss 0.539871.
Train: 2018-08-02T13:02:46.415334: step 4810, loss 0.453451.
Test: 2018-08-02T13:02:46.868323: step 4810, loss 0.548985.
Train: 2018-08-02T13:02:47.008944: step 4811, loss 0.597052.
Train: 2018-08-02T13:02:47.165127: step 4812, loss 0.579985.
Train: 2018-08-02T13:02:47.321371: step 4813, loss 0.501527.
Train: 2018-08-02T13:02:47.477585: step 4814, loss 0.470252.
Train: 2018-08-02T13:02:47.618176: step 4815, loss 0.501726.
Train: 2018-08-02T13:02:47.774390: step 4816, loss 0.565227.
Train: 2018-08-02T13:02:47.930603: step 4817, loss 0.492106.
Train: 2018-08-02T13:02:48.086816: step 4818, loss 0.490946.
Train: 2018-08-02T13:02:48.227379: step 4819, loss 0.571955.
Train: 2018-08-02T13:02:48.383591: step 4820, loss 0.439338.
Test: 2018-08-02T13:02:48.836663: step 4820, loss 0.548623.
Train: 2018-08-02T13:02:48.992854: step 4821, loss 0.551232.
Train: 2018-08-02T13:02:49.149038: step 4822, loss 0.509016.
Train: 2018-08-02T13:02:49.289661: step 4823, loss 0.509644.
Train: 2018-08-02T13:02:49.445844: step 4824, loss 0.523566.
Train: 2018-08-02T13:02:49.602087: step 4825, loss 0.516997.
Train: 2018-08-02T13:02:49.758271: step 4826, loss 0.530183.
Train: 2018-08-02T13:02:49.914484: step 4827, loss 0.529512.
Train: 2018-08-02T13:02:50.055105: step 4828, loss 0.57014.
Train: 2018-08-02T13:02:50.211289: step 4829, loss 0.51107.
Train: 2018-08-02T13:02:50.351912: step 4830, loss 0.585527.
Test: 2018-08-02T13:02:50.820552: step 4830, loss 0.560991.
Train: 2018-08-02T13:02:50.976734: step 4831, loss 0.517156.
Train: 2018-08-02T13:02:51.086116: step 4832, loss 0.564669.
Train: 2018-08-02T13:02:51.226676: step 4833, loss 0.481151.
Train: 2018-08-02T13:02:51.382920: step 4834, loss 0.481636.
Train: 2018-08-02T13:02:51.539104: step 4835, loss 0.547926.
Train: 2018-08-02T13:02:51.695317: step 4836, loss 0.547205.
Train: 2018-08-02T13:02:51.835908: step 4837, loss 0.489041.
Train: 2018-08-02T13:02:51.992122: step 4838, loss 0.517323.
Train: 2018-08-02T13:02:52.148366: step 4839, loss 0.439015.
Train: 2018-08-02T13:02:52.288958: step 4840, loss 0.596584.
Test: 2018-08-02T13:02:52.757568: step 4840, loss 0.547243.
Train: 2018-08-02T13:02:52.898190: step 4841, loss 0.54889.
Train: 2018-08-02T13:02:53.054403: step 4842, loss 0.578932.
Train: 2018-08-02T13:02:53.210617: step 4843, loss 0.485182.
Train: 2018-08-02T13:02:53.351209: step 4844, loss 0.616506.
Train: 2018-08-02T13:02:53.507393: step 4845, loss 0.582209.
Train: 2018-08-02T13:02:53.663605: step 4846, loss 0.554902.
Train: 2018-08-02T13:02:53.819818: step 4847, loss 0.547078.
Train: 2018-08-02T13:02:53.960412: step 4848, loss 0.537487.
Train: 2018-08-02T13:02:54.116655: step 4849, loss 0.521804.
Train: 2018-08-02T13:02:54.272868: step 4850, loss 0.53849.
Test: 2018-08-02T13:02:54.725882: step 4850, loss 0.553745.
Train: 2018-08-02T13:02:54.882101: step 4851, loss 0.481806.
Train: 2018-08-02T13:02:55.022693: step 4852, loss 0.450619.
Train: 2018-08-02T13:02:55.178876: step 4853, loss 0.475658.
Train: 2018-08-02T13:02:55.335122: step 4854, loss 0.441878.
Train: 2018-08-02T13:02:55.475712: step 4855, loss 0.528349.
Train: 2018-08-02T13:02:55.631894: step 4856, loss 0.431033.
Train: 2018-08-02T13:02:55.788133: step 4857, loss 0.6199.
Train: 2018-08-02T13:02:55.944351: step 4858, loss 0.530081.
Train: 2018-08-02T13:02:56.100565: step 4859, loss 0.505608.
Train: 2018-08-02T13:02:56.241128: step 4860, loss 0.513086.
Test: 2018-08-02T13:02:56.694147: step 4860, loss 0.552382.
Train: 2018-08-02T13:02:56.850394: step 4861, loss 0.575772.
Train: 2018-08-02T13:02:57.006603: step 4862, loss 0.500985.
Train: 2018-08-02T13:02:57.162816: step 4863, loss 0.572595.
Train: 2018-08-02T13:02:57.303408: step 4864, loss 0.566826.
Train: 2018-08-02T13:02:57.459622: step 4865, loss 0.514869.
Train: 2018-08-02T13:02:57.615805: step 4866, loss 0.521419.
Train: 2018-08-02T13:02:57.756427: step 4867, loss 0.525311.
Train: 2018-08-02T13:02:57.912640: step 4868, loss 0.58237.
Train: 2018-08-02T13:02:58.068856: step 4869, loss 0.496134.
Train: 2018-08-02T13:02:58.209416: step 4870, loss 0.491438.
Test: 2018-08-02T13:02:58.678056: step 4870, loss 0.553596.
Train: 2018-08-02T13:02:58.818679: step 4871, loss 0.518568.
Train: 2018-08-02T13:02:58.974861: step 4872, loss 0.578541.
Train: 2018-08-02T13:02:59.131076: step 4873, loss 0.547233.
Train: 2018-08-02T13:02:59.271697: step 4874, loss 0.491497.
Train: 2018-08-02T13:02:59.427880: step 4875, loss 0.519473.
Train: 2018-08-02T13:02:59.584124: step 4876, loss 0.456675.
Train: 2018-08-02T13:02:59.740308: step 4877, loss 0.553585.
Train: 2018-08-02T13:02:59.880930: step 4878, loss 0.466331.
Train: 2018-08-02T13:03:00.037113: step 4879, loss 0.482159.
Train: 2018-08-02T13:03:00.193357: step 4880, loss 0.488759.
Test: 2018-08-02T13:03:00.646345: step 4880, loss 0.546863.
Train: 2018-08-02T13:03:00.802559: step 4881, loss 0.483574.
Train: 2018-08-02T13:03:00.958772: step 4882, loss 0.45523.
Train: 2018-08-02T13:03:01.114986: step 4883, loss 0.636729.
Train: 2018-08-02T13:03:01.255607: step 4884, loss 0.542048.
Train: 2018-08-02T13:03:01.411821: step 4885, loss 0.500141.
Train: 2018-08-02T13:03:01.568035: step 4886, loss 0.466121.
Train: 2018-08-02T13:03:01.708627: step 4887, loss 0.488335.
Train: 2018-08-02T13:03:01.864840: step 4888, loss 0.599995.
Train: 2018-08-02T13:03:02.021023: step 4889, loss 0.488438.
Train: 2018-08-02T13:03:02.177261: step 4890, loss 0.582553.
Test: 2018-08-02T13:03:02.630257: step 4890, loss 0.556753.
Train: 2018-08-02T13:03:02.786499: step 4891, loss 0.434473.
Train: 2018-08-02T13:03:02.927092: step 4892, loss 0.473222.
Train: 2018-08-02T13:03:03.083275: step 4893, loss 0.487401.
Train: 2018-08-02T13:03:03.239518: step 4894, loss 0.627931.
Train: 2018-08-02T13:03:03.380081: step 4895, loss 0.484241.
Train: 2018-08-02T13:03:03.536324: step 4896, loss 0.465263.
Train: 2018-08-02T13:03:03.692537: step 4897, loss 0.528314.
Train: 2018-08-02T13:03:03.848751: step 4898, loss 0.519823.
Train: 2018-08-02T13:03:03.989342: step 4899, loss 0.533693.
Train: 2018-08-02T13:03:04.145526: step 4900, loss 0.5343.
Test: 2018-08-02T13:03:04.598545: step 4900, loss 0.549232.
Train: 2018-08-02T13:03:05.207807: step 4901, loss 0.537394.
Train: 2018-08-02T13:03:05.364022: step 4902, loss 0.510285.
Train: 2018-08-02T13:03:05.520234: step 4903, loss 0.511124.
Train: 2018-08-02T13:03:05.660796: step 4904, loss 0.512491.
Train: 2018-08-02T13:03:05.817009: step 4905, loss 0.602496.
Train: 2018-08-02T13:03:05.973253: step 4906, loss 0.614715.
Train: 2018-08-02T13:03:06.129437: step 4907, loss 0.531528.
Train: 2018-08-02T13:03:06.270052: step 4908, loss 0.518985.
Train: 2018-08-02T13:03:06.426272: step 4909, loss 0.523146.
Train: 2018-08-02T13:03:06.582485: step 4910, loss 0.529977.
Test: 2018-08-02T13:03:07.035525: step 4910, loss 0.553899.
Train: 2018-08-02T13:03:07.176097: step 4911, loss 0.535193.
Train: 2018-08-02T13:03:07.332311: step 4912, loss 0.466288.
Train: 2018-08-02T13:03:07.488492: step 4913, loss 0.592221.
Train: 2018-08-02T13:03:07.629117: step 4914, loss 0.507378.
Train: 2018-08-02T13:03:07.785298: step 4915, loss 0.544597.
Train: 2018-08-02T13:03:07.941512: step 4916, loss 0.548325.
Train: 2018-08-02T13:03:08.082136: step 4917, loss 0.535348.
Train: 2018-08-02T13:03:08.238348: step 4918, loss 0.503876.
Train: 2018-08-02T13:03:08.394561: step 4919, loss 0.489486.
Train: 2018-08-02T13:03:08.550774: step 4920, loss 0.56954.
Test: 2018-08-02T13:03:09.003791: step 4920, loss 0.55089.
Train: 2018-08-02T13:03:09.159977: step 4921, loss 0.513169.
Train: 2018-08-02T13:03:09.300600: step 4922, loss 0.565456.
Train: 2018-08-02T13:03:09.456812: step 4923, loss 0.599026.
Train: 2018-08-02T13:03:09.613020: step 4924, loss 0.516112.
Train: 2018-08-02T13:03:09.753618: step 4925, loss 0.459364.
Train: 2018-08-02T13:03:09.909831: step 4926, loss 0.47154.
Train: 2018-08-02T13:03:10.066044: step 4927, loss 0.609707.
Train: 2018-08-02T13:03:10.222228: step 4928, loss 0.530641.
Train: 2018-08-02T13:03:10.362819: step 4929, loss 0.560761.
Train: 2018-08-02T13:03:10.519063: step 4930, loss 0.538248.
Test: 2018-08-02T13:03:10.972084: step 4930, loss 0.552361.
Train: 2018-08-02T13:03:11.128296: step 4931, loss 0.503188.
Train: 2018-08-02T13:03:11.268888: step 4932, loss 0.519697.
Train: 2018-08-02T13:03:11.425072: step 4933, loss 0.522227.
Train: 2018-08-02T13:03:11.581317: step 4934, loss 0.525873.
Train: 2018-08-02T13:03:11.721907: step 4935, loss 0.496525.
Train: 2018-08-02T13:03:11.878121: step 4936, loss 0.553814.
Train: 2018-08-02T13:03:12.034335: step 4937, loss 0.511241.
Train: 2018-08-02T13:03:12.174926: step 4938, loss 0.625288.
Train: 2018-08-02T13:03:12.331139: step 4939, loss 0.533984.
Train: 2018-08-02T13:03:12.487352: step 4940, loss 0.46487.
Test: 2018-08-02T13:03:12.940367: step 4940, loss 0.557196.
Train: 2018-08-02T13:03:13.096554: step 4941, loss 0.512442.
Train: 2018-08-02T13:03:13.252798: step 4942, loss 0.542781.
Train: 2018-08-02T13:03:13.393390: step 4943, loss 0.48267.
Train: 2018-08-02T13:03:13.549604: step 4944, loss 0.524249.
Train: 2018-08-02T13:03:13.705817: step 4945, loss 0.554422.
Train: 2018-08-02T13:03:13.846380: step 4946, loss 0.577824.
Train: 2018-08-02T13:03:14.002623: step 4947, loss 0.44342.
Train: 2018-08-02T13:03:14.158836: step 4948, loss 0.546492.
Train: 2018-08-02T13:03:14.315019: step 4949, loss 0.546638.
Train: 2018-08-02T13:03:14.455611: step 4950, loss 0.577777.
Test: 2018-08-02T13:03:14.908641: step 4950, loss 0.552699.
Train: 2018-08-02T13:03:15.064873: step 4951, loss 0.633773.
Train: 2018-08-02T13:03:15.221056: step 4952, loss 0.464806.
Train: 2018-08-02T13:03:15.361650: step 4953, loss 0.499868.
Train: 2018-08-02T13:03:15.517893: step 4954, loss 0.462858.
Train: 2018-08-02T13:03:15.674107: step 4955, loss 0.509855.
Train: 2018-08-02T13:03:15.814668: step 4956, loss 0.436338.
Train: 2018-08-02T13:03:15.970913: step 4957, loss 0.49659.
Train: 2018-08-02T13:03:16.127125: step 4958, loss 0.574871.
Train: 2018-08-02T13:03:16.283309: step 4959, loss 0.458037.
Train: 2018-08-02T13:03:16.423931: step 4960, loss 0.461264.
Test: 2018-08-02T13:03:16.892540: step 4960, loss 0.547342.
Train: 2018-08-02T13:03:17.048754: step 4961, loss 0.552292.
Train: 2018-08-02T13:03:17.189377: step 4962, loss 0.527915.
Train: 2018-08-02T13:03:17.345590: step 4963, loss 0.491535.
Train: 2018-08-02T13:03:17.501803: step 4964, loss 0.56932.
Train: 2018-08-02T13:03:17.642395: step 4965, loss 0.565711.
Train: 2018-08-02T13:03:17.798579: step 4966, loss 0.486825.
Train: 2018-08-02T13:03:17.954822: step 4967, loss 0.554408.
Train: 2018-08-02T13:03:18.095414: step 4968, loss 0.573116.
Train: 2018-08-02T13:03:18.251628: step 4969, loss 0.5182.
Train: 2018-08-02T13:03:18.407812: step 4970, loss 0.514624.
Test: 2018-08-02T13:03:18.860830: step 4970, loss 0.552476.
Train: 2018-08-02T13:03:19.017074: step 4971, loss 0.553967.
Train: 2018-08-02T13:03:19.157665: step 4972, loss 0.490358.
Train: 2018-08-02T13:03:19.313848: step 4973, loss 0.62143.
Train: 2018-08-02T13:03:19.470088: step 4974, loss 0.500741.
Train: 2018-08-02T13:03:19.626307: step 4975, loss 0.549642.
Train: 2018-08-02T13:03:19.782519: step 4976, loss 0.512649.
Train: 2018-08-02T13:03:19.923111: step 4977, loss 0.489479.
Train: 2018-08-02T13:03:20.079295: step 4978, loss 0.536093.
Train: 2018-08-02T13:03:20.235538: step 4979, loss 0.539895.
Train: 2018-08-02T13:03:20.376099: step 4980, loss 0.453618.
Test: 2018-08-02T13:03:20.844751: step 4980, loss 0.548936.
Train: 2018-08-02T13:03:20.985357: step 4981, loss 0.483601.
Train: 2018-08-02T13:03:21.141576: step 4982, loss 0.491021.
Train: 2018-08-02T13:03:21.250896: step 4983, loss 0.411075.
Train: 2018-08-02T13:03:21.407138: step 4984, loss 0.416861.
Train: 2018-08-02T13:03:21.563346: step 4985, loss 0.464356.
Train: 2018-08-02T13:03:21.703915: step 4986, loss 0.55479.
Train: 2018-08-02T13:03:21.860128: step 4987, loss 0.519323.
Train: 2018-08-02T13:03:22.000750: step 4988, loss 0.558117.
Train: 2018-08-02T13:03:22.156966: step 4989, loss 0.605767.
Train: 2018-08-02T13:03:22.313177: step 4990, loss 0.533879.
Test: 2018-08-02T13:03:22.766165: step 4990, loss 0.595485.
Train: 2018-08-02T13:03:22.922378: step 4991, loss 0.564914.
Train: 2018-08-02T13:03:23.062972: step 4992, loss 0.560557.
Train: 2018-08-02T13:03:23.219215: step 4993, loss 0.524783.
Train: 2018-08-02T13:03:23.375397: step 4994, loss 0.562228.
Train: 2018-08-02T13:03:23.515990: step 4995, loss 0.463059.
Train: 2018-08-02T13:03:23.672205: step 4996, loss 0.608339.
Train: 2018-08-02T13:03:23.828417: step 4997, loss 0.537051.
Train: 2018-08-02T13:03:23.984630: step 4998, loss 0.64866.
Train: 2018-08-02T13:03:24.140874: step 4999, loss 0.552128.
Train: 2018-08-02T13:03:24.297087: step 5000, loss 0.524629.
Test: 2018-08-02T13:03:24.750101: step 5000, loss 0.594209.
Train: 2018-08-02T13:03:25.390581: step 5001, loss 0.570328.
Train: 2018-08-02T13:03:25.546765: step 5002, loss 0.535175.
Train: 2018-08-02T13:03:25.687386: step 5003, loss 0.551933.
Train: 2018-08-02T13:03:25.843600: step 5004, loss 0.501255.
Train: 2018-08-02T13:03:25.999784: step 5005, loss 0.543015.
Train: 2018-08-02T13:03:26.140376: step 5006, loss 0.525405.
Train: 2018-08-02T13:03:26.296619: step 5007, loss 0.583484.
Train: 2018-08-02T13:03:26.452803: step 5008, loss 0.552379.
Train: 2018-08-02T13:03:26.609015: step 5009, loss 0.512443.
Train: 2018-08-02T13:03:26.749638: step 5010, loss 0.49411.
Test: 2018-08-02T13:03:27.202627: step 5010, loss 0.580495.
Train: 2018-08-02T13:03:27.358871: step 5011, loss 0.485991.
Train: 2018-08-02T13:03:27.515054: step 5012, loss 0.471787.
Train: 2018-08-02T13:03:27.671267: step 5013, loss 0.499198.
Train: 2018-08-02T13:03:27.811889: step 5014, loss 0.558823.
Train: 2018-08-02T13:03:27.968103: step 5015, loss 0.468147.
Train: 2018-08-02T13:03:28.124316: step 5016, loss 0.532254.
Train: 2018-08-02T13:03:28.264908: step 5017, loss 0.447306.
Train: 2018-08-02T13:03:28.421125: step 5018, loss 0.564548.
Train: 2018-08-02T13:03:28.577335: step 5019, loss 0.566358.
Train: 2018-08-02T13:03:28.717897: step 5020, loss 0.526317.
Test: 2018-08-02T13:03:29.186562: step 5020, loss 0.576166.
Train: 2018-08-02T13:03:29.342751: step 5021, loss 0.570405.
Train: 2018-08-02T13:03:29.483373: step 5022, loss 0.473008.
Train: 2018-08-02T13:03:29.639556: step 5023, loss 0.488637.
Train: 2018-08-02T13:03:29.795794: step 5024, loss 0.496494.
Train: 2018-08-02T13:03:29.936391: step 5025, loss 0.480962.
Train: 2018-08-02T13:03:30.092605: step 5026, loss 0.496273.
Train: 2018-08-02T13:03:30.248819: step 5027, loss 0.61586.
Train: 2018-08-02T13:03:30.405026: step 5028, loss 0.510461.
Train: 2018-08-02T13:03:30.545624: step 5029, loss 0.455405.
Train: 2018-08-02T13:03:30.701831: step 5030, loss 0.534272.
Test: 2018-08-02T13:03:31.154828: step 5030, loss 0.56328.
Train: 2018-08-02T13:03:31.311040: step 5031, loss 0.48979.
Train: 2018-08-02T13:03:31.467283: step 5032, loss 0.494529.
Train: 2018-08-02T13:03:31.607877: step 5033, loss 0.548245.
Train: 2018-08-02T13:03:31.764091: step 5034, loss 0.559739.
Train: 2018-08-02T13:03:31.920302: step 5035, loss 0.585046.
Train: 2018-08-02T13:03:32.076486: step 5036, loss 0.575118.
Train: 2018-08-02T13:03:32.217076: step 5037, loss 0.48316.
Train: 2018-08-02T13:03:32.373321: step 5038, loss 0.547981.
Train: 2018-08-02T13:03:32.529534: step 5039, loss 0.507232.
Train: 2018-08-02T13:03:32.670126: step 5040, loss 0.537402.
Test: 2018-08-02T13:03:33.123123: step 5040, loss 0.563213.
Train: 2018-08-02T13:03:33.279329: step 5041, loss 0.50171.
Train: 2018-08-02T13:03:33.435572: step 5042, loss 0.503852.
Train: 2018-08-02T13:03:33.591785: step 5043, loss 0.562627.
Train: 2018-08-02T13:03:33.732377: step 5044, loss 0.520023.
Train: 2018-08-02T13:03:33.888591: step 5045, loss 0.488996.
Train: 2018-08-02T13:03:34.044804: step 5046, loss 0.556651.
Train: 2018-08-02T13:03:34.201013: step 5047, loss 0.498351.
Train: 2018-08-02T13:03:34.357231: step 5048, loss 0.477572.
Train: 2018-08-02T13:03:34.497794: step 5049, loss 0.563662.
Train: 2018-08-02T13:03:34.654006: step 5050, loss 0.434127.
Test: 2018-08-02T13:03:35.107050: step 5050, loss 0.554057.
Train: 2018-08-02T13:03:35.263269: step 5051, loss 0.535566.
Train: 2018-08-02T13:03:35.419478: step 5052, loss 0.529816.
Train: 2018-08-02T13:03:35.575696: step 5053, loss 0.518546.
Train: 2018-08-02T13:03:35.716288: step 5054, loss 0.565432.
Train: 2018-08-02T13:03:35.872501: step 5055, loss 0.432659.
Train: 2018-08-02T13:03:36.028714: step 5056, loss 0.527051.
Train: 2018-08-02T13:03:36.184928: step 5057, loss 0.498116.
Train: 2018-08-02T13:03:36.325520: step 5058, loss 0.433036.
Train: 2018-08-02T13:03:36.481734: step 5059, loss 0.48248.
Train: 2018-08-02T13:03:36.637946: step 5060, loss 0.510076.
Test: 2018-08-02T13:03:37.090935: step 5060, loss 0.548538.
Train: 2018-08-02T13:03:37.231527: step 5061, loss 0.475114.
Train: 2018-08-02T13:03:37.387772: step 5062, loss 0.534697.
Train: 2018-08-02T13:03:37.543954: step 5063, loss 0.543491.
Train: 2018-08-02T13:03:37.700199: step 5064, loss 0.519254.
Train: 2018-08-02T13:03:37.840791: step 5065, loss 0.51271.
Train: 2018-08-02T13:03:37.996974: step 5066, loss 0.49466.
Train: 2018-08-02T13:03:38.153188: step 5067, loss 0.5009.
Train: 2018-08-02T13:03:38.309433: step 5068, loss 0.460002.
Train: 2018-08-02T13:03:38.450023: step 5069, loss 0.529157.
Train: 2018-08-02T13:03:38.606237: step 5070, loss 0.540329.
Test: 2018-08-02T13:03:39.059225: step 5070, loss 0.55241.
Train: 2018-08-02T13:03:39.215469: step 5071, loss 0.519629.
Train: 2018-08-02T13:03:39.371682: step 5072, loss 0.573941.
Train: 2018-08-02T13:03:39.512269: step 5073, loss 0.531754.
Train: 2018-08-02T13:03:39.668487: step 5074, loss 0.558368.
Train: 2018-08-02T13:03:39.824701: step 5075, loss 0.470643.
Train: 2018-08-02T13:03:39.965293: step 5076, loss 0.560829.
Train: 2018-08-02T13:03:40.121477: step 5077, loss 0.507962.
Train: 2018-08-02T13:03:40.277720: step 5078, loss 0.464888.
Train: 2018-08-02T13:03:40.433934: step 5079, loss 0.497712.
Train: 2018-08-02T13:03:40.574525: step 5080, loss 0.600456.
Test: 2018-08-02T13:03:41.043136: step 5080, loss 0.548831.
Train: 2018-08-02T13:03:41.183728: step 5081, loss 0.608773.
Train: 2018-08-02T13:03:41.339942: step 5082, loss 0.467973.
Train: 2018-08-02T13:03:41.496185: step 5083, loss 0.549259.
Train: 2018-08-02T13:03:41.652370: step 5084, loss 0.564108.
Train: 2018-08-02T13:03:41.808581: step 5085, loss 0.511244.
Train: 2018-08-02T13:03:41.964794: step 5086, loss 0.560703.
Train: 2018-08-02T13:03:42.105417: step 5087, loss 0.561563.
Train: 2018-08-02T13:03:42.261630: step 5088, loss 0.540894.
Train: 2018-08-02T13:03:42.417844: step 5089, loss 0.521111.
Train: 2018-08-02T13:03:42.558435: step 5090, loss 0.618408.
Test: 2018-08-02T13:03:43.027058: step 5090, loss 0.559833.
Train: 2018-08-02T13:03:43.183259: step 5091, loss 0.541083.
Train: 2018-08-02T13:03:43.323882: step 5092, loss 0.520851.
Train: 2018-08-02T13:03:43.480095: step 5093, loss 0.601416.
Train: 2018-08-02T13:03:43.620656: step 5094, loss 0.587372.
Train: 2018-08-02T13:03:43.776901: step 5095, loss 0.534343.
Train: 2018-08-02T13:03:43.933115: step 5096, loss 0.591377.
Train: 2018-08-02T13:03:44.073706: step 5097, loss 0.565791.
Train: 2018-08-02T13:03:44.229890: step 5098, loss 0.557616.
Train: 2018-08-02T13:03:44.386102: step 5099, loss 0.49442.
Train: 2018-08-02T13:03:44.542349: step 5100, loss 0.504833.
Test: 2018-08-02T13:03:44.995366: step 5100, loss 0.554093.
Train: 2018-08-02T13:03:45.635811: step 5101, loss 0.530364.
Train: 2018-08-02T13:03:45.776432: step 5102, loss 0.470946.
Train: 2018-08-02T13:03:45.932645: step 5103, loss 0.577714.
Train: 2018-08-02T13:03:46.088829: step 5104, loss 0.520258.
Train: 2018-08-02T13:03:46.229451: step 5105, loss 0.501028.
Train: 2018-08-02T13:03:46.385664: step 5106, loss 0.467925.
Train: 2018-08-02T13:03:46.541848: step 5107, loss 0.621943.
Train: 2018-08-02T13:03:46.682470: step 5108, loss 0.568902.
Train: 2018-08-02T13:03:46.838683: step 5109, loss 0.545169.
Train: 2018-08-02T13:03:46.994868: step 5110, loss 0.50634.
Test: 2018-08-02T13:03:47.447916: step 5110, loss 0.563347.
Train: 2018-08-02T13:03:47.588478: step 5111, loss 0.506674.
Train: 2018-08-02T13:03:47.744691: step 5112, loss 0.47463.
Train: 2018-08-02T13:03:47.900934: step 5113, loss 0.579443.
Train: 2018-08-02T13:03:48.041526: step 5114, loss 0.524919.
Train: 2018-08-02T13:03:48.197710: step 5115, loss 0.530822.
Train: 2018-08-02T13:03:48.353953: step 5116, loss 0.550643.
Train: 2018-08-02T13:03:48.510168: step 5117, loss 0.536706.
Train: 2018-08-02T13:03:48.650760: step 5118, loss 0.521003.
Train: 2018-08-02T13:03:48.806973: step 5119, loss 0.570559.
Train: 2018-08-02T13:03:48.963186: step 5120, loss 0.492722.
Test: 2018-08-02T13:03:49.416174: step 5120, loss 0.548676.
Train: 2018-08-02T13:03:49.556797: step 5121, loss 0.566044.
Train: 2018-08-02T13:03:49.713011: step 5122, loss 0.490972.
Train: 2018-08-02T13:03:49.853573: step 5123, loss 0.426719.
Train: 2018-08-02T13:03:50.009785: step 5124, loss 0.575466.
Train: 2018-08-02T13:03:50.166029: step 5125, loss 0.500222.
Train: 2018-08-02T13:03:50.306621: step 5126, loss 0.475807.
Train: 2018-08-02T13:03:50.462835: step 5127, loss 0.519806.
Train: 2018-08-02T13:03:50.619049: step 5128, loss 0.528962.
Train: 2018-08-02T13:03:50.759640: step 5129, loss 0.451263.
Train: 2018-08-02T13:03:50.915854: step 5130, loss 0.569287.
Test: 2018-08-02T13:03:51.368844: step 5130, loss 0.552401.
Train: 2018-08-02T13:03:51.525057: step 5131, loss 0.561745.
Train: 2018-08-02T13:03:51.681299: step 5132, loss 0.541027.
Train: 2018-08-02T13:03:51.821891: step 5133, loss 0.513654.
Train: 2018-08-02T13:03:51.931242: step 5134, loss 0.433337.
Train: 2018-08-02T13:03:52.087455: step 5135, loss 0.524758.
Train: 2018-08-02T13:03:52.243637: step 5136, loss 0.446902.
Train: 2018-08-02T13:03:52.384230: step 5137, loss 0.508535.
Train: 2018-08-02T13:03:52.540473: step 5138, loss 0.567475.
Train: 2018-08-02T13:03:52.696686: step 5139, loss 0.409392.
Train: 2018-08-02T13:03:52.837279: step 5140, loss 0.530631.
Test: 2018-08-02T13:03:53.290293: step 5140, loss 0.547723.
Train: 2018-08-02T13:03:53.446481: step 5141, loss 0.560231.
Train: 2018-08-02T13:03:53.602719: step 5142, loss 0.567595.
Train: 2018-08-02T13:03:53.743285: step 5143, loss 0.494217.
Train: 2018-08-02T13:03:53.899501: step 5144, loss 0.532432.
Train: 2018-08-02T13:03:54.055743: step 5145, loss 0.663772.
Train: 2018-08-02T13:03:54.211927: step 5146, loss 0.59477.
Train: 2018-08-02T13:03:54.352549: step 5147, loss 0.545936.
Train: 2018-08-02T13:03:54.508762: step 5148, loss 0.567361.
Train: 2018-08-02T13:03:54.664946: step 5149, loss 0.595635.
Train: 2018-08-02T13:03:54.805567: step 5150, loss 0.582567.
Test: 2018-08-02T13:03:55.258587: step 5150, loss 0.563972.
Train: 2018-08-02T13:03:55.414801: step 5151, loss 0.592785.
Train: 2018-08-02T13:03:55.571014: step 5152, loss 0.532524.
Train: 2018-08-02T13:03:55.727227: step 5153, loss 0.526839.
Train: 2018-08-02T13:03:55.867789: step 5154, loss 0.516649.
Train: 2018-08-02T13:03:56.024002: step 5155, loss 0.539387.
Train: 2018-08-02T13:03:56.180246: step 5156, loss 0.61166.
Train: 2018-08-02T13:03:56.320837: step 5157, loss 0.577496.
Train: 2018-08-02T13:03:56.477051: step 5158, loss 0.552292.
Train: 2018-08-02T13:03:56.633265: step 5159, loss 0.488617.
Train: 2018-08-02T13:03:56.773857: step 5160, loss 0.489742.
Test: 2018-08-02T13:03:57.242484: step 5160, loss 0.549043.
Train: 2018-08-02T13:03:57.383091: step 5161, loss 0.559389.
Train: 2018-08-02T13:03:57.539303: step 5162, loss 0.483296.
Train: 2018-08-02T13:03:57.695516: step 5163, loss 0.61602.
Train: 2018-08-02T13:03:57.836108: step 5164, loss 0.611785.
Train: 2018-08-02T13:03:57.992321: step 5165, loss 0.468851.
Train: 2018-08-02T13:03:58.148535: step 5166, loss 0.524015.
Train: 2018-08-02T13:03:58.289127: step 5167, loss 0.528122.
Train: 2018-08-02T13:03:58.445310: step 5168, loss 0.500812.
Train: 2018-08-02T13:03:58.601555: step 5169, loss 0.502507.
Train: 2018-08-02T13:03:58.742146: step 5170, loss 0.48227.
Test: 2018-08-02T13:03:59.210756: step 5170, loss 0.54826.
Train: 2018-08-02T13:03:59.351378: step 5171, loss 0.432053.
Train: 2018-08-02T13:03:59.507562: step 5172, loss 0.630866.
Train: 2018-08-02T13:03:59.663805: step 5173, loss 0.550913.
Train: 2018-08-02T13:03:59.804368: step 5174, loss 0.480353.
Train: 2018-08-02T13:03:59.960611: step 5175, loss 0.550986.
Train: 2018-08-02T13:04:00.101203: step 5176, loss 0.584779.
Train: 2018-08-02T13:04:00.257387: step 5177, loss 0.522273.
Train: 2018-08-02T13:04:00.413632: step 5178, loss 0.581014.
Train: 2018-08-02T13:04:00.554221: step 5179, loss 0.537076.
Train: 2018-08-02T13:04:00.710405: step 5180, loss 0.489902.
Test: 2018-08-02T13:04:01.179045: step 5180, loss 0.557956.
Train: 2018-08-02T13:04:01.319667: step 5181, loss 0.586636.
Train: 2018-08-02T13:04:01.475881: step 5182, loss 0.502681.
Train: 2018-08-02T13:04:01.632094: step 5183, loss 0.524636.
Train: 2018-08-02T13:04:01.788277: step 5184, loss 0.537131.
Train: 2018-08-02T13:04:01.928900: step 5185, loss 0.556541.
Train: 2018-08-02T13:04:02.085113: step 5186, loss 0.520001.
Train: 2018-08-02T13:04:02.241296: step 5187, loss 0.507661.
Train: 2018-08-02T13:04:02.381919: step 5188, loss 0.526843.
Train: 2018-08-02T13:04:02.538126: step 5189, loss 0.544962.
Train: 2018-08-02T13:04:02.694347: step 5190, loss 0.497059.
Test: 2018-08-02T13:04:03.147335: step 5190, loss 0.552763.
Train: 2018-08-02T13:04:03.303547: step 5191, loss 0.499013.
Train: 2018-08-02T13:04:03.459791: step 5192, loss 0.521207.
Train: 2018-08-02T13:04:03.600383: step 5193, loss 0.513226.
Train: 2018-08-02T13:04:03.756568: step 5194, loss 0.514214.
Train: 2018-08-02T13:04:03.912780: step 5195, loss 0.403728.
Train: 2018-08-02T13:04:04.053402: step 5196, loss 0.563918.
Train: 2018-08-02T13:04:04.209616: step 5197, loss 0.530081.
Train: 2018-08-02T13:04:04.365799: step 5198, loss 0.515748.
Train: 2018-08-02T13:04:04.522036: step 5199, loss 0.51932.
Train: 2018-08-02T13:04:04.662634: step 5200, loss 0.55121.
Test: 2018-08-02T13:04:05.115624: step 5200, loss 0.551823.
Train: 2018-08-02T13:04:05.756128: step 5201, loss 0.551461.
Train: 2018-08-02T13:04:05.912342: step 5202, loss 0.542386.
Train: 2018-08-02T13:04:06.068526: step 5203, loss 0.533404.
Train: 2018-08-02T13:04:06.209147: step 5204, loss 0.531944.
Train: 2018-08-02T13:04:06.365361: step 5205, loss 0.466619.
Train: 2018-08-02T13:04:06.521543: step 5206, loss 0.525207.
Train: 2018-08-02T13:04:06.662137: step 5207, loss 0.510501.
Train: 2018-08-02T13:04:06.818380: step 5208, loss 0.565078.
Train: 2018-08-02T13:04:06.974562: step 5209, loss 0.462755.
Train: 2018-08-02T13:04:07.115154: step 5210, loss 0.459343.
Test: 2018-08-02T13:04:07.568174: step 5210, loss 0.548104.
Train: 2018-08-02T13:04:07.724417: step 5211, loss 0.511438.
Train: 2018-08-02T13:04:07.880631: step 5212, loss 0.535452.
Train: 2018-08-02T13:04:08.036814: step 5213, loss 0.46854.
Train: 2018-08-02T13:04:08.177436: step 5214, loss 0.540569.
Train: 2018-08-02T13:04:08.333650: step 5215, loss 0.459616.
Train: 2018-08-02T13:04:08.489857: step 5216, loss 0.539951.
Train: 2018-08-02T13:04:08.630455: step 5217, loss 0.539128.
Train: 2018-08-02T13:04:08.786668: step 5218, loss 0.522974.
Train: 2018-08-02T13:04:08.942882: step 5219, loss 0.488459.
Train: 2018-08-02T13:04:09.083470: step 5220, loss 0.470982.
Test: 2018-08-02T13:04:09.552085: step 5220, loss 0.548261.
Train: 2018-08-02T13:04:09.692706: step 5221, loss 0.486807.
Train: 2018-08-02T13:04:09.848920: step 5222, loss 0.570454.
Train: 2018-08-02T13:04:10.005135: step 5223, loss 0.493426.
Train: 2018-08-02T13:04:10.145695: step 5224, loss 0.53259.
Train: 2018-08-02T13:04:10.301908: step 5225, loss 0.53628.
Train: 2018-08-02T13:04:10.458153: step 5226, loss 0.595532.
Train: 2018-08-02T13:04:10.614336: step 5227, loss 0.516255.
Train: 2018-08-02T13:04:10.754958: step 5228, loss 0.53285.
Train: 2018-08-02T13:04:10.911171: step 5229, loss 0.515878.
Train: 2018-08-02T13:04:11.067384: step 5230, loss 0.51684.
Test: 2018-08-02T13:04:11.520398: step 5230, loss 0.552854.
Train: 2018-08-02T13:04:11.676587: step 5231, loss 0.529726.
Train: 2018-08-02T13:04:11.817179: step 5232, loss 0.523408.
Train: 2018-08-02T13:04:11.973423: step 5233, loss 0.494026.
Train: 2018-08-02T13:04:12.129636: step 5234, loss 0.450323.
Train: 2018-08-02T13:04:12.270228: step 5235, loss 0.522252.
Train: 2018-08-02T13:04:12.426410: step 5236, loss 0.499547.
Train: 2018-08-02T13:04:12.582655: step 5237, loss 0.547568.
Train: 2018-08-02T13:04:12.723247: step 5238, loss 0.491355.
Train: 2018-08-02T13:04:12.879460: step 5239, loss 0.510402.
Train: 2018-08-02T13:04:13.035674: step 5240, loss 0.48278.
Test: 2018-08-02T13:04:13.488663: step 5240, loss 0.548971.
Train: 2018-08-02T13:04:13.629285: step 5241, loss 0.521609.
Train: 2018-08-02T13:04:13.785498: step 5242, loss 0.439801.
Train: 2018-08-02T13:04:13.941682: step 5243, loss 0.556002.
Train: 2018-08-02T13:04:14.097921: step 5244, loss 0.535589.
Train: 2018-08-02T13:04:14.254134: step 5245, loss 0.593934.
Train: 2018-08-02T13:04:14.394700: step 5246, loss 0.513566.
Train: 2018-08-02T13:04:14.550914: step 5247, loss 0.536664.
Train: 2018-08-02T13:04:14.707162: step 5248, loss 0.560425.
Train: 2018-08-02T13:04:14.847749: step 5249, loss 0.531717.
Train: 2018-08-02T13:04:15.003962: step 5250, loss 0.497697.
Test: 2018-08-02T13:04:15.456951: step 5250, loss 0.559545.
Train: 2018-08-02T13:04:15.613166: step 5251, loss 0.495305.
Train: 2018-08-02T13:04:15.753787: step 5252, loss 0.45726.
Train: 2018-08-02T13:04:15.910000: step 5253, loss 0.424874.
Train: 2018-08-02T13:04:16.066184: step 5254, loss 0.53274.
Train: 2018-08-02T13:04:16.206806: step 5255, loss 0.506003.
Train: 2018-08-02T13:04:16.363019: step 5256, loss 0.497116.
Train: 2018-08-02T13:04:16.519202: step 5257, loss 0.610748.
Train: 2018-08-02T13:04:16.675446: step 5258, loss 0.485628.
Train: 2018-08-02T13:04:16.816009: step 5259, loss 0.483147.
Train: 2018-08-02T13:04:16.972252: step 5260, loss 0.579004.
Test: 2018-08-02T13:04:17.421138: step 5260, loss 0.559346.
Train: 2018-08-02T13:04:17.577350: step 5261, loss 0.59972.
Train: 2018-08-02T13:04:17.733596: step 5262, loss 0.582941.
Train: 2018-08-02T13:04:17.874180: step 5263, loss 0.574999.
Train: 2018-08-02T13:04:18.030399: step 5264, loss 0.574571.
Train: 2018-08-02T13:04:18.186613: step 5265, loss 0.523205.
Train: 2018-08-02T13:04:18.342797: step 5266, loss 0.556211.
Train: 2018-08-02T13:04:18.483420: step 5267, loss 0.482825.
Train: 2018-08-02T13:04:18.639632: step 5268, loss 0.537683.
Train: 2018-08-02T13:04:18.795845: step 5269, loss 0.544396.
Train: 2018-08-02T13:04:18.936408: step 5270, loss 0.515685.
Test: 2018-08-02T13:04:19.389457: step 5270, loss 0.549696.
Train: 2018-08-02T13:04:19.545669: step 5271, loss 0.499078.
Train: 2018-08-02T13:04:19.701852: step 5272, loss 0.458841.
Train: 2018-08-02T13:04:19.842475: step 5273, loss 0.594713.
Train: 2018-08-02T13:04:19.998658: step 5274, loss 0.578615.
Train: 2018-08-02T13:04:20.154902: step 5275, loss 0.481272.
Train: 2018-08-02T13:04:20.311116: step 5276, loss 0.520556.
Train: 2018-08-02T13:04:20.451678: step 5277, loss 0.498521.
Train: 2018-08-02T13:04:20.607923: step 5278, loss 0.543322.
Train: 2018-08-02T13:04:20.764105: step 5279, loss 0.489617.
Train: 2018-08-02T13:04:20.904726: step 5280, loss 0.549921.
Test: 2018-08-02T13:04:21.373359: step 5280, loss 0.548789.
Train: 2018-08-02T13:04:21.529550: step 5281, loss 0.515862.
Train: 2018-08-02T13:04:21.670173: step 5282, loss 0.523945.
Train: 2018-08-02T13:04:21.826363: step 5283, loss 0.483683.
Train: 2018-08-02T13:04:21.982595: step 5284, loss 0.51537.
Train: 2018-08-02T13:04:22.091918: step 5285, loss 0.412178.
Train: 2018-08-02T13:04:22.248131: step 5286, loss 0.547073.
Train: 2018-08-02T13:04:22.404345: step 5287, loss 0.539504.
Train: 2018-08-02T13:04:22.567571: step 5288, loss 0.448331.
Train: 2018-08-02T13:04:22.723783: step 5289, loss 0.51256.
Train: 2018-08-02T13:04:22.879996: step 5290, loss 0.565492.
Test: 2018-08-02T13:04:23.333016: step 5290, loss 0.546369.
Train: 2018-08-02T13:04:23.504852: step 5291, loss 0.511831.
Train: 2018-08-02T13:04:23.661065: step 5292, loss 0.429424.
Train: 2018-08-02T13:04:23.817279: step 5293, loss 0.528853.
Train: 2018-08-02T13:04:23.973490: step 5294, loss 0.501684.
Train: 2018-08-02T13:04:24.129703: step 5295, loss 0.461159.
Train: 2018-08-02T13:04:24.285916: step 5296, loss 0.552758.
Train: 2018-08-02T13:04:24.442132: step 5297, loss 0.465543.
Train: 2018-08-02T13:04:24.598344: step 5298, loss 0.492611.
Train: 2018-08-02T13:04:24.754587: step 5299, loss 0.559337.
Train: 2018-08-02T13:04:24.895180: step 5300, loss 0.533962.
Test: 2018-08-02T13:04:25.363821: step 5300, loss 0.547789.
Train: 2018-08-02T13:04:26.051155: step 5301, loss 0.597738.
Train: 2018-08-02T13:04:26.207373: step 5302, loss 0.576242.
Train: 2018-08-02T13:04:26.363587: step 5303, loss 0.615011.
Train: 2018-08-02T13:04:26.519800: step 5304, loss 0.54511.
Train: 2018-08-02T13:04:26.676013: step 5305, loss 0.55923.
Train: 2018-08-02T13:04:26.832227: step 5306, loss 0.600371.
Train: 2018-08-02T13:04:26.988434: step 5307, loss 0.535228.
Train: 2018-08-02T13:04:27.144653: step 5308, loss 0.593255.
Train: 2018-08-02T13:04:27.300860: step 5309, loss 0.502878.
Train: 2018-08-02T13:04:27.441458: step 5310, loss 0.528189.
Test: 2018-08-02T13:04:27.910070: step 5310, loss 0.546537.
Train: 2018-08-02T13:04:28.066312: step 5311, loss 0.572239.
Train: 2018-08-02T13:04:28.238117: step 5312, loss 0.523841.
Train: 2018-08-02T13:04:28.394331: step 5313, loss 0.583493.
Train: 2018-08-02T13:04:28.581787: step 5314, loss 0.539301.
Train: 2018-08-02T13:04:28.753624: step 5315, loss 0.581721.
Train: 2018-08-02T13:04:28.941078: step 5316, loss 0.57964.
Train: 2018-08-02T13:04:29.097290: step 5317, loss 0.515307.
Train: 2018-08-02T13:04:29.269150: step 5318, loss 0.552366.
Train: 2018-08-02T13:04:29.417477: step 5319, loss 0.493001.
Train: 2018-08-02T13:04:29.589286: step 5320, loss 0.5043.
Test: 2018-08-02T13:04:30.042305: step 5320, loss 0.548499.
Train: 2018-08-02T13:04:30.214139: step 5321, loss 0.529507.
Train: 2018-08-02T13:04:30.370378: step 5322, loss 0.523194.
Train: 2018-08-02T13:04:30.526565: step 5323, loss 0.621563.
Train: 2018-08-02T13:04:30.682170: step 5324, loss 0.517273.
Train: 2018-08-02T13:04:30.845482: step 5325, loss 0.541764.
Train: 2018-08-02T13:04:30.995513: step 5326, loss 0.619047.
Train: 2018-08-02T13:04:31.152208: step 5327, loss 0.501749.
Train: 2018-08-02T13:04:31.305773: step 5328, loss 0.508413.
Train: 2018-08-02T13:04:31.451973: step 5329, loss 0.501753.
Train: 2018-08-02T13:04:31.615955: step 5330, loss 0.510337.
Test: 2018-08-02T13:04:32.073877: step 5330, loss 0.547144.
Train: 2018-08-02T13:04:32.214501: step 5331, loss 0.636434.
Train: 2018-08-02T13:04:32.370681: step 5332, loss 0.586745.
Train: 2018-08-02T13:04:32.526925: step 5333, loss 0.479925.
Train: 2018-08-02T13:04:32.683109: step 5334, loss 0.443542.
Train: 2018-08-02T13:04:32.886217: step 5335, loss 0.624471.
Train: 2018-08-02T13:04:33.042431: step 5336, loss 0.533543.
Train: 2018-08-02T13:04:33.182991: step 5337, loss 0.494819.
Train: 2018-08-02T13:04:33.339205: step 5338, loss 0.469099.
Train: 2018-08-02T13:04:33.495449: step 5339, loss 0.575074.
Train: 2018-08-02T13:04:33.651662: step 5340, loss 0.472917.
Test: 2018-08-02T13:04:34.104652: step 5340, loss 0.548285.
Train: 2018-08-02T13:04:34.260865: step 5341, loss 0.550529.
Train: 2018-08-02T13:04:34.417107: step 5342, loss 0.500314.
Train: 2018-08-02T13:04:34.557669: step 5343, loss 0.484039.
Train: 2018-08-02T13:04:34.713908: step 5344, loss 0.508151.
Train: 2018-08-02T13:04:34.870098: step 5345, loss 0.538428.
Train: 2018-08-02T13:04:35.026340: step 5346, loss 0.46581.
Train: 2018-08-02T13:04:35.182548: step 5347, loss 0.590686.
Train: 2018-08-02T13:04:35.323148: step 5348, loss 0.604282.
Train: 2018-08-02T13:04:35.494951: step 5349, loss 0.535156.
Train: 2018-08-02T13:04:35.635543: step 5350, loss 0.513265.
Test: 2018-08-02T13:04:36.104182: step 5350, loss 0.558515.
Train: 2018-08-02T13:04:36.260426: step 5351, loss 0.55023.
Train: 2018-08-02T13:04:36.416638: step 5352, loss 0.534291.
Train: 2018-08-02T13:04:36.572823: step 5353, loss 0.489893.
Train: 2018-08-02T13:04:36.729066: step 5354, loss 0.535944.
Train: 2018-08-02T13:04:36.869658: step 5355, loss 0.482799.
Train: 2018-08-02T13:04:37.025842: step 5356, loss 0.525986.
Train: 2018-08-02T13:04:37.182085: step 5357, loss 0.459739.
Train: 2018-08-02T13:04:37.338272: step 5358, loss 0.526066.
Train: 2018-08-02T13:04:37.494512: step 5359, loss 0.441254.
Train: 2018-08-02T13:04:37.635104: step 5360, loss 0.580287.
Test: 2018-08-02T13:04:38.103715: step 5360, loss 0.549174.
Train: 2018-08-02T13:04:38.259952: step 5361, loss 0.494571.
Train: 2018-08-02T13:04:38.400550: step 5362, loss 0.50777.
Train: 2018-08-02T13:04:38.556763: step 5363, loss 0.554816.
Train: 2018-08-02T13:04:38.712978: step 5364, loss 0.487867.
Train: 2018-08-02T13:04:38.869161: step 5365, loss 0.544153.
Train: 2018-08-02T13:04:39.025374: step 5366, loss 0.512626.
Train: 2018-08-02T13:04:39.165996: step 5367, loss 0.538616.
Train: 2018-08-02T13:04:39.322209: step 5368, loss 0.542935.
Train: 2018-08-02T13:04:39.478422: step 5369, loss 0.555468.
Train: 2018-08-02T13:04:39.634630: step 5370, loss 0.555611.
Test: 2018-08-02T13:04:40.087625: step 5370, loss 0.553046.
Train: 2018-08-02T13:04:40.243863: step 5371, loss 0.565879.
Train: 2018-08-02T13:04:40.400082: step 5372, loss 0.517496.
Train: 2018-08-02T13:04:40.556295: step 5373, loss 0.516262.
Train: 2018-08-02T13:04:40.712478: step 5374, loss 0.52431.
Train: 2018-08-02T13:04:40.853103: step 5375, loss 0.429358.
Train: 2018-08-02T13:04:41.009314: step 5376, loss 0.594109.
Train: 2018-08-02T13:04:41.165498: step 5377, loss 0.499082.
Train: 2018-08-02T13:04:41.321710: step 5378, loss 0.557281.
Train: 2018-08-02T13:04:41.477957: step 5379, loss 0.605736.
Train: 2018-08-02T13:04:41.634167: step 5380, loss 0.522096.
Test: 2018-08-02T13:04:42.087157: step 5380, loss 0.556092.
Train: 2018-08-02T13:04:42.290264: step 5381, loss 0.555381.
Train: 2018-08-02T13:04:42.430826: step 5382, loss 0.537111.
Train: 2018-08-02T13:04:42.587039: step 5383, loss 0.516748.
Train: 2018-08-02T13:04:42.743283: step 5384, loss 0.529506.
Train: 2018-08-02T13:04:42.899467: step 5385, loss 0.544498.
Train: 2018-08-02T13:04:43.040088: step 5386, loss 0.529558.
Train: 2018-08-02T13:04:43.196304: step 5387, loss 0.45588.
Train: 2018-08-02T13:04:43.352515: step 5388, loss 0.516313.
Train: 2018-08-02T13:04:43.508698: step 5389, loss 0.602359.
Train: 2018-08-02T13:04:43.649321: step 5390, loss 0.527824.
Test: 2018-08-02T13:04:44.117932: step 5390, loss 0.5474.
Train: 2018-08-02T13:04:44.258554: step 5391, loss 0.52637.
Train: 2018-08-02T13:04:44.414766: step 5392, loss 0.461928.
Train: 2018-08-02T13:04:44.570949: step 5393, loss 0.50033.
Train: 2018-08-02T13:04:44.727188: step 5394, loss 0.550125.
Train: 2018-08-02T13:04:44.867786: step 5395, loss 0.477943.
Train: 2018-08-02T13:04:45.024002: step 5396, loss 0.517982.
Train: 2018-08-02T13:04:45.180213: step 5397, loss 0.550376.
Train: 2018-08-02T13:04:45.336427: step 5398, loss 0.5044.
Train: 2018-08-02T13:04:45.492639: step 5399, loss 0.554271.
Train: 2018-08-02T13:04:45.633235: step 5400, loss 0.517715.
Test: 2018-08-02T13:04:46.101866: step 5400, loss 0.551359.
Train: 2018-08-02T13:04:46.742346: step 5401, loss 0.56499.
Train: 2018-08-02T13:04:46.882938: step 5402, loss 0.563009.
Train: 2018-08-02T13:04:47.039152: step 5403, loss 0.554962.
Train: 2018-08-02T13:04:47.195336: step 5404, loss 0.52907.
Train: 2018-08-02T13:04:47.351579: step 5405, loss 0.462123.
Train: 2018-08-02T13:04:47.507792: step 5406, loss 0.565545.
Train: 2018-08-02T13:04:47.648355: step 5407, loss 0.480899.
Train: 2018-08-02T13:04:47.804598: step 5408, loss 0.494786.
Train: 2018-08-02T13:04:47.960813: step 5409, loss 0.425853.
Train: 2018-08-02T13:04:48.117020: step 5410, loss 0.535576.
Test: 2018-08-02T13:04:48.570042: step 5410, loss 0.551386.
Train: 2018-08-02T13:04:48.726227: step 5411, loss 0.480437.
Train: 2018-08-02T13:04:48.882440: step 5412, loss 0.548636.
Train: 2018-08-02T13:04:49.023032: step 5413, loss 0.509803.
Train: 2018-08-02T13:04:49.194872: step 5414, loss 0.469272.
Train: 2018-08-02T13:04:49.335492: step 5415, loss 0.523755.
Train: 2018-08-02T13:04:49.491703: step 5416, loss 0.521038.
Train: 2018-08-02T13:04:49.647917: step 5417, loss 0.502423.
Train: 2018-08-02T13:04:49.804130: step 5418, loss 0.539425.
Train: 2018-08-02T13:04:49.944724: step 5419, loss 0.49039.
Train: 2018-08-02T13:04:50.100905: step 5420, loss 0.521945.
Test: 2018-08-02T13:04:50.569544: step 5420, loss 0.548113.
Train: 2018-08-02T13:04:50.710169: step 5421, loss 0.462919.
Train: 2018-08-02T13:04:50.866378: step 5422, loss 0.508479.
Train: 2018-08-02T13:04:51.022565: step 5423, loss 0.494732.
Train: 2018-08-02T13:04:51.163186: step 5424, loss 0.498229.
Train: 2018-08-02T13:04:51.319370: step 5425, loss 0.475441.
Train: 2018-08-02T13:04:51.475613: step 5426, loss 0.479539.
Train: 2018-08-02T13:04:51.631826: step 5427, loss 0.487592.
Train: 2018-08-02T13:04:51.772419: step 5428, loss 0.473069.
Train: 2018-08-02T13:04:51.928602: step 5429, loss 0.601741.
Train: 2018-08-02T13:04:52.084848: step 5430, loss 0.492719.
Test: 2018-08-02T13:04:52.537835: step 5430, loss 0.547842.
Train: 2018-08-02T13:04:52.690031: step 5431, loss 0.54637.
Train: 2018-08-02T13:04:52.846244: step 5432, loss 0.556663.
Train: 2018-08-02T13:04:53.002458: step 5433, loss 0.520021.
Train: 2018-08-02T13:04:53.143050: step 5434, loss 0.515666.
Train: 2018-08-02T13:04:53.299264: step 5435, loss 0.521386.
Train: 2018-08-02T13:04:53.408582: step 5436, loss 0.6214.
Train: 2018-08-02T13:04:53.564797: step 5437, loss 0.440233.
Train: 2018-08-02T13:04:53.721040: step 5438, loss 0.578965.
Train: 2018-08-02T13:04:53.877223: step 5439, loss 0.47049.
Train: 2018-08-02T13:04:54.017816: step 5440, loss 0.457646.
Test: 2018-08-02T13:04:54.486457: step 5440, loss 0.559145.
Train: 2018-08-02T13:04:54.642699: step 5441, loss 0.529751.
Train: 2018-08-02T13:04:54.783262: step 5442, loss 0.483558.
Train: 2018-08-02T13:04:54.939473: step 5443, loss 0.542479.
Train: 2018-08-02T13:04:55.095722: step 5444, loss 0.560115.
Train: 2018-08-02T13:04:55.251904: step 5445, loss 0.453178.
Train: 2018-08-02T13:04:55.392523: step 5446, loss 0.528489.
Train: 2018-08-02T13:04:55.548718: step 5447, loss 0.501334.
Train: 2018-08-02T13:04:55.704919: step 5448, loss 0.491788.
Train: 2018-08-02T13:04:55.861164: step 5449, loss 0.479719.
Train: 2018-08-02T13:04:56.001726: step 5450, loss 0.557636.
Test: 2018-08-02T13:04:56.470391: step 5450, loss 0.550139.
Train: 2018-08-02T13:04:56.626609: step 5451, loss 0.621143.
Train: 2018-08-02T13:04:56.767172: step 5452, loss 0.60437.
Train: 2018-08-02T13:04:56.923385: step 5453, loss 0.485157.
Train: 2018-08-02T13:04:57.079629: step 5454, loss 0.516273.
Train: 2018-08-02T13:04:57.235812: step 5455, loss 0.535536.
Train: 2018-08-02T13:04:57.392054: step 5456, loss 0.498085.
Train: 2018-08-02T13:04:57.532617: step 5457, loss 0.502359.
Train: 2018-08-02T13:04:57.688861: step 5458, loss 0.462277.
Train: 2018-08-02T13:04:57.845045: step 5459, loss 0.503534.
Train: 2018-08-02T13:04:58.001271: step 5460, loss 0.467032.
Test: 2018-08-02T13:04:58.454307: step 5460, loss 0.547849.
Train: 2018-08-02T13:04:58.610490: step 5461, loss 0.506733.
Train: 2018-08-02T13:04:58.751112: step 5462, loss 0.490471.
Train: 2018-08-02T13:04:58.907295: step 5463, loss 0.494441.
Train: 2018-08-02T13:04:59.063542: step 5464, loss 0.602655.
Train: 2018-08-02T13:04:59.219752: step 5465, loss 0.472411.
Train: 2018-08-02T13:04:59.375966: step 5466, loss 0.489417.
Train: 2018-08-02T13:04:59.532180: step 5467, loss 0.643156.
Train: 2018-08-02T13:04:59.672772: step 5468, loss 0.551062.
Train: 2018-08-02T13:04:59.828978: step 5469, loss 0.505713.
Train: 2018-08-02T13:04:59.985198: step 5470, loss 0.477831.
Test: 2018-08-02T13:05:00.438212: step 5470, loss 0.551338.
Train: 2018-08-02T13:05:00.594430: step 5471, loss 0.524977.
Train: 2018-08-02T13:05:00.750612: step 5472, loss 0.597182.
Train: 2018-08-02T13:05:00.891237: step 5473, loss 0.512722.
Train: 2018-08-02T13:05:01.047449: step 5474, loss 0.541266.
Train: 2018-08-02T13:05:01.203662: step 5475, loss 0.415291.
Train: 2018-08-02T13:05:01.359877: step 5476, loss 0.495931.
Train: 2018-08-02T13:05:01.516086: step 5477, loss 0.462222.
Train: 2018-08-02T13:05:01.656651: step 5478, loss 0.537722.
Train: 2018-08-02T13:05:01.812895: step 5479, loss 0.548554.
Train: 2018-08-02T13:05:01.969108: step 5480, loss 0.506787.
Test: 2018-08-02T13:05:02.422122: step 5480, loss 0.548383.
Train: 2018-08-02T13:05:02.578341: step 5481, loss 0.579747.
Train: 2018-08-02T13:05:02.734554: step 5482, loss 0.504676.
Train: 2018-08-02T13:05:02.875146: step 5483, loss 0.578122.
Train: 2018-08-02T13:05:03.031330: step 5484, loss 0.580981.
Train: 2018-08-02T13:05:03.187573: step 5485, loss 0.543341.
Train: 2018-08-02T13:05:03.343756: step 5486, loss 0.507903.
Train: 2018-08-02T13:05:03.484349: step 5487, loss 0.501369.
Train: 2018-08-02T13:05:03.640592: step 5488, loss 0.554385.
Train: 2018-08-02T13:05:03.796806: step 5489, loss 0.550887.
Train: 2018-08-02T13:05:03.952990: step 5490, loss 0.545375.
Test: 2018-08-02T13:05:04.406008: step 5490, loss 0.550828.
Train: 2018-08-02T13:05:04.562222: step 5491, loss 0.518327.
Train: 2018-08-02T13:05:04.702843: step 5492, loss 0.473889.
Train: 2018-08-02T13:05:04.859026: step 5493, loss 0.60091.
Train: 2018-08-02T13:05:05.015240: step 5494, loss 0.51022.
Train: 2018-08-02T13:05:05.155831: step 5495, loss 0.475098.
Train: 2018-08-02T13:05:05.312075: step 5496, loss 0.504995.
Train: 2018-08-02T13:05:05.468289: step 5497, loss 0.570321.
Train: 2018-08-02T13:05:05.624473: step 5498, loss 0.629815.
Train: 2018-08-02T13:05:05.765094: step 5499, loss 0.47781.
Train: 2018-08-02T13:05:05.921278: step 5500, loss 0.514852.
Test: 2018-08-02T13:05:06.374296: step 5500, loss 0.553956.
Train: 2018-08-02T13:05:06.999180: step 5501, loss 0.570797.
Train: 2018-08-02T13:05:07.139772: step 5502, loss 0.574944.
Train: 2018-08-02T13:05:07.295986: step 5503, loss 0.571212.
Train: 2018-08-02T13:05:07.452193: step 5504, loss 0.546504.
Train: 2018-08-02T13:05:07.608382: step 5505, loss 0.509203.
Train: 2018-08-02T13:05:07.749005: step 5506, loss 0.55415.
Train: 2018-08-02T13:05:07.905188: step 5507, loss 0.516111.
Train: 2018-08-02T13:05:08.061401: step 5508, loss 0.51697.
Train: 2018-08-02T13:05:08.202023: step 5509, loss 0.597794.
Train: 2018-08-02T13:05:08.358237: step 5510, loss 0.545843.
Test: 2018-08-02T13:05:08.826877: step 5510, loss 0.550899.
Train: 2018-08-02T13:05:08.967470: step 5511, loss 0.518971.
Train: 2018-08-02T13:05:09.123683: step 5512, loss 0.513245.
Train: 2018-08-02T13:05:09.279866: step 5513, loss 0.466783.
Train: 2018-08-02T13:05:09.436110: step 5514, loss 0.406379.
Train: 2018-08-02T13:05:09.576702: step 5515, loss 0.518325.
Train: 2018-08-02T13:05:09.732916: step 5516, loss 0.465973.
Train: 2018-08-02T13:05:09.889128: step 5517, loss 0.494195.
Train: 2018-08-02T13:05:10.029720: step 5518, loss 0.548817.
Train: 2018-08-02T13:05:10.185930: step 5519, loss 0.540239.
Train: 2018-08-02T13:05:10.342118: step 5520, loss 0.483618.
Test: 2018-08-02T13:05:10.795163: step 5520, loss 0.550363.
Train: 2018-08-02T13:05:10.951380: step 5521, loss 0.526418.
Train: 2018-08-02T13:05:11.091942: step 5522, loss 0.45951.
Train: 2018-08-02T13:05:11.248156: step 5523, loss 0.497081.
Train: 2018-08-02T13:05:11.404398: step 5524, loss 0.575071.
Train: 2018-08-02T13:05:11.560583: step 5525, loss 0.54148.
Train: 2018-08-02T13:05:11.701204: step 5526, loss 0.472062.
Train: 2018-08-02T13:05:11.857387: step 5527, loss 0.566264.
Train: 2018-08-02T13:05:12.013602: step 5528, loss 0.517307.
Train: 2018-08-02T13:05:12.169845: step 5529, loss 0.537683.
Train: 2018-08-02T13:05:12.310407: step 5530, loss 0.480411.
Test: 2018-08-02T13:05:12.779048: step 5530, loss 0.548563.
Train: 2018-08-02T13:05:12.919670: step 5531, loss 0.477359.
Train: 2018-08-02T13:05:13.075882: step 5532, loss 0.477949.
Train: 2018-08-02T13:05:13.232065: step 5533, loss 0.504111.
Train: 2018-08-02T13:05:13.372689: step 5534, loss 0.518051.
Train: 2018-08-02T13:05:13.528901: step 5535, loss 0.496422.
Train: 2018-08-02T13:05:13.685115: step 5536, loss 0.572079.
Train: 2018-08-02T13:05:13.841331: step 5537, loss 0.52642.
Train: 2018-08-02T13:05:13.981889: step 5538, loss 0.481353.
Train: 2018-08-02T13:05:14.138133: step 5539, loss 0.517501.
Train: 2018-08-02T13:05:14.294316: step 5540, loss 0.469755.
Test: 2018-08-02T13:05:14.747362: step 5540, loss 0.547276.
Train: 2018-08-02T13:05:14.903579: step 5541, loss 0.533142.
Train: 2018-08-02T13:05:15.059793: step 5542, loss 0.427732.
Train: 2018-08-02T13:05:15.216008: step 5543, loss 0.448905.
Train: 2018-08-02T13:05:15.372190: step 5544, loss 0.548151.
Train: 2018-08-02T13:05:15.512781: step 5545, loss 0.559459.
Train: 2018-08-02T13:05:15.669026: step 5546, loss 0.582986.
Train: 2018-08-02T13:05:15.825238: step 5547, loss 0.491907.
Train: 2018-08-02T13:05:15.981452: step 5548, loss 0.524841.
Train: 2018-08-02T13:05:16.122045: step 5549, loss 0.615676.
Train: 2018-08-02T13:05:16.278257: step 5550, loss 0.536995.
Test: 2018-08-02T13:05:16.746902: step 5550, loss 0.560172.
Train: 2018-08-02T13:05:16.887490: step 5551, loss 0.526057.
Train: 2018-08-02T13:05:17.043704: step 5552, loss 0.545725.
Train: 2018-08-02T13:05:17.199916: step 5553, loss 0.495925.
Train: 2018-08-02T13:05:17.340478: step 5554, loss 0.543483.
Train: 2018-08-02T13:05:17.496724: step 5555, loss 0.491128.
Train: 2018-08-02T13:05:17.652905: step 5556, loss 0.473757.
Train: 2018-08-02T13:05:17.809149: step 5557, loss 0.44574.
Train: 2018-08-02T13:05:17.949741: step 5558, loss 0.612108.
Train: 2018-08-02T13:05:18.105955: step 5559, loss 0.625351.
Train: 2018-08-02T13:05:18.262168: step 5560, loss 0.530771.
Test: 2018-08-02T13:05:18.715157: step 5560, loss 0.562509.
Train: 2018-08-02T13:05:18.871370: step 5561, loss 0.522458.
Train: 2018-08-02T13:05:19.011992: step 5562, loss 0.473095.
Train: 2018-08-02T13:05:19.168206: step 5563, loss 0.52062.
Train: 2018-08-02T13:05:19.324419: step 5564, loss 0.49911.
Train: 2018-08-02T13:05:19.480633: step 5565, loss 0.55319.
Train: 2018-08-02T13:05:19.621225: step 5566, loss 0.534415.
Train: 2018-08-02T13:05:19.777407: step 5567, loss 0.562792.
Train: 2018-08-02T13:05:19.933620: step 5568, loss 0.471513.
Train: 2018-08-02T13:05:20.089865: step 5569, loss 0.57658.
Train: 2018-08-02T13:05:20.246074: step 5570, loss 0.489294.
Test: 2018-08-02T13:05:20.699069: step 5570, loss 0.548011.
Train: 2018-08-02T13:05:20.855310: step 5571, loss 0.568996.
Train: 2018-08-02T13:05:20.995902: step 5572, loss 0.52617.
Train: 2018-08-02T13:05:21.152116: step 5573, loss 0.624102.
Train: 2018-08-02T13:05:21.308300: step 5574, loss 0.558437.
Train: 2018-08-02T13:05:21.464512: step 5575, loss 0.638408.
Train: 2018-08-02T13:05:21.605135: step 5576, loss 0.568753.
Train: 2018-08-02T13:05:21.761348: step 5577, loss 0.484833.
Train: 2018-08-02T13:05:21.917562: step 5578, loss 0.530168.
Train: 2018-08-02T13:05:22.073745: step 5579, loss 0.560099.
Train: 2018-08-02T13:05:22.214367: step 5580, loss 0.544607.
Test: 2018-08-02T13:05:22.683002: step 5580, loss 0.553129.
Train: 2018-08-02T13:05:22.823600: step 5581, loss 0.525351.
Train: 2018-08-02T13:05:22.979813: step 5582, loss 0.522435.
Train: 2018-08-02T13:05:23.136026: step 5583, loss 0.571412.
Train: 2018-08-02T13:05:23.292241: step 5584, loss 0.468053.
Train: 2018-08-02T13:05:23.432835: step 5585, loss 0.495252.
Train: 2018-08-02T13:05:23.589046: step 5586, loss 0.485948.
Train: 2018-08-02T13:05:23.698365: step 5587, loss 0.487042.
Train: 2018-08-02T13:05:23.854609: step 5588, loss 0.551179.
Train: 2018-08-02T13:05:24.010822: step 5589, loss 0.524481.
Train: 2018-08-02T13:05:24.151414: step 5590, loss 0.520854.
Test: 2018-08-02T13:05:24.620025: step 5590, loss 0.548636.
Train: 2018-08-02T13:05:24.776238: step 5591, loss 0.49155.
Train: 2018-08-02T13:05:24.916860: step 5592, loss 0.501768.
Train: 2018-08-02T13:05:25.073072: step 5593, loss 0.485784.
Train: 2018-08-02T13:05:25.229293: step 5594, loss 0.460631.
Train: 2018-08-02T13:05:25.385500: step 5595, loss 0.445589.
Train: 2018-08-02T13:05:25.526092: step 5596, loss 0.499144.
Train: 2018-08-02T13:05:25.682306: step 5597, loss 0.492472.
Train: 2018-08-02T13:05:25.838489: step 5598, loss 0.535.
Train: 2018-08-02T13:05:25.994732: step 5599, loss 0.566317.
Train: 2018-08-02T13:05:26.150946: step 5600, loss 0.551219.
Test: 2018-08-02T13:05:26.603960: step 5600, loss 0.558176.
Train: 2018-08-02T13:05:27.228818: step 5601, loss 0.506242.
Train: 2018-08-02T13:05:27.385001: step 5602, loss 0.534627.
Train: 2018-08-02T13:05:27.541246: step 5603, loss 0.558646.
Train: 2018-08-02T13:05:27.697458: step 5604, loss 0.553823.
Train: 2018-08-02T13:05:27.838050: step 5605, loss 0.534638.
Train: 2018-08-02T13:05:27.994264: step 5606, loss 0.477641.
Train: 2018-08-02T13:05:28.150446: step 5607, loss 0.471467.
Train: 2018-08-02T13:05:28.291070: step 5608, loss 0.669204.
Train: 2018-08-02T13:05:28.447252: step 5609, loss 0.507768.
Train: 2018-08-02T13:05:28.603466: step 5610, loss 0.522993.
Test: 2018-08-02T13:05:29.056486: step 5610, loss 0.552743.
Train: 2018-08-02T13:05:29.212699: step 5611, loss 0.571559.
Train: 2018-08-02T13:05:29.368942: step 5612, loss 0.549662.
Train: 2018-08-02T13:05:29.509533: step 5613, loss 0.513786.
Train: 2018-08-02T13:05:29.665748: step 5614, loss 0.517668.
Train: 2018-08-02T13:05:29.821955: step 5615, loss 0.529183.
Train: 2018-08-02T13:05:29.978143: step 5616, loss 0.471008.
Train: 2018-08-02T13:05:30.118767: step 5617, loss 0.532926.
Train: 2018-08-02T13:05:30.274949: step 5618, loss 0.607469.
Train: 2018-08-02T13:05:30.431194: step 5619, loss 0.543913.
Train: 2018-08-02T13:05:30.587407: step 5620, loss 0.503509.
Test: 2018-08-02T13:05:31.040426: step 5620, loss 0.547408.
Train: 2018-08-02T13:05:31.196639: step 5621, loss 0.552418.
Train: 2018-08-02T13:05:31.337202: step 5622, loss 0.52856.
Train: 2018-08-02T13:05:31.493444: step 5623, loss 0.547216.
Train: 2018-08-02T13:05:31.649660: step 5624, loss 0.488931.
Train: 2018-08-02T13:05:31.805872: step 5625, loss 0.497508.
Train: 2018-08-02T13:05:31.946434: step 5626, loss 0.522532.
Train: 2018-08-02T13:05:32.102677: step 5627, loss 0.457648.
Train: 2018-08-02T13:05:32.258890: step 5628, loss 0.514965.
Train: 2018-08-02T13:05:32.415104: step 5629, loss 0.416277.
Train: 2018-08-02T13:05:32.555696: step 5630, loss 0.589876.
Test: 2018-08-02T13:05:33.024332: step 5630, loss 0.548141.
Train: 2018-08-02T13:05:33.164929: step 5631, loss 0.479328.
Train: 2018-08-02T13:05:33.321142: step 5632, loss 0.575708.
Train: 2018-08-02T13:05:33.477350: step 5633, loss 0.5341.
Train: 2018-08-02T13:05:33.633569: step 5634, loss 0.494098.
Train: 2018-08-02T13:05:33.789776: step 5635, loss 0.556753.
Train: 2018-08-02T13:05:33.930345: step 5636, loss 0.544597.
Train: 2018-08-02T13:05:34.086556: step 5637, loss 0.479613.
Train: 2018-08-02T13:05:34.242771: step 5638, loss 0.562728.
Train: 2018-08-02T13:05:34.399014: step 5639, loss 0.504995.
Train: 2018-08-02T13:05:34.555227: step 5640, loss 0.511437.
Test: 2018-08-02T13:05:35.008216: step 5640, loss 0.547077.
Train: 2018-08-02T13:05:35.164460: step 5641, loss 0.467127.
Train: 2018-08-02T13:05:35.305054: step 5642, loss 0.489786.
Train: 2018-08-02T13:05:35.461265: step 5643, loss 0.461545.
Train: 2018-08-02T13:05:35.617456: step 5644, loss 0.474699.
Train: 2018-08-02T13:05:35.773668: step 5645, loss 0.41042.
Train: 2018-08-02T13:05:35.929906: step 5646, loss 0.528747.
Train: 2018-08-02T13:05:36.070498: step 5647, loss 0.473138.
Train: 2018-08-02T13:05:36.226711: step 5648, loss 0.590797.
Train: 2018-08-02T13:05:36.382925: step 5649, loss 0.523664.
Train: 2018-08-02T13:05:36.539138: step 5650, loss 0.542189.
Test: 2018-08-02T13:05:36.992131: step 5650, loss 0.566098.
Train: 2018-08-02T13:05:37.132749: step 5651, loss 0.503426.
Train: 2018-08-02T13:05:37.288963: step 5652, loss 0.499525.
Train: 2018-08-02T13:05:37.445145: step 5653, loss 0.515213.
Train: 2018-08-02T13:05:37.601367: step 5654, loss 0.500616.
Train: 2018-08-02T13:05:37.757573: step 5655, loss 0.562461.
Train: 2018-08-02T13:05:37.898195: step 5656, loss 0.512082.
Train: 2018-08-02T13:05:38.054414: step 5657, loss 0.531731.
Train: 2018-08-02T13:05:38.210621: step 5658, loss 0.499613.
Train: 2018-08-02T13:05:38.366831: step 5659, loss 0.534175.
Train: 2018-08-02T13:05:38.507427: step 5660, loss 0.493777.
Test: 2018-08-02T13:05:38.976062: step 5660, loss 0.553541.
Train: 2018-08-02T13:05:39.116659: step 5661, loss 0.54359.
Train: 2018-08-02T13:05:39.272842: step 5662, loss 0.461887.
Train: 2018-08-02T13:05:39.429086: step 5663, loss 0.435584.
Train: 2018-08-02T13:05:39.585270: step 5664, loss 0.467086.
Train: 2018-08-02T13:05:39.741513: step 5665, loss 0.536956.
Train: 2018-08-02T13:05:39.882105: step 5666, loss 0.501102.
Train: 2018-08-02T13:05:40.038319: step 5667, loss 0.440177.
Train: 2018-08-02T13:05:40.194532: step 5668, loss 0.557364.
Train: 2018-08-02T13:05:40.350716: step 5669, loss 0.570801.
Train: 2018-08-02T13:05:40.491337: step 5670, loss 0.504107.
Test: 2018-08-02T13:05:40.944351: step 5670, loss 0.553953.
Train: 2018-08-02T13:05:41.100564: step 5671, loss 0.513593.
Train: 2018-08-02T13:05:41.256784: step 5672, loss 0.516943.
Train: 2018-08-02T13:05:41.412967: step 5673, loss 0.485352.
Train: 2018-08-02T13:05:41.553589: step 5674, loss 0.558757.
Train: 2018-08-02T13:05:41.709802: step 5675, loss 0.48339.
Train: 2018-08-02T13:05:41.866015: step 5676, loss 0.479648.
Train: 2018-08-02T13:05:42.037822: step 5677, loss 0.496555.
Train: 2018-08-02T13:05:42.178445: step 5678, loss 0.586972.
Train: 2018-08-02T13:05:42.334626: step 5679, loss 0.561348.
Train: 2018-08-02T13:05:42.490839: step 5680, loss 0.503419.
Test: 2018-08-02T13:05:42.943883: step 5680, loss 0.550276.
Train: 2018-08-02T13:05:43.100072: step 5681, loss 0.453069.
Train: 2018-08-02T13:05:43.256317: step 5682, loss 0.480483.
Train: 2018-08-02T13:05:43.412499: step 5683, loss 0.578654.
Train: 2018-08-02T13:05:43.553120: step 5684, loss 0.491612.
Train: 2018-08-02T13:05:43.709334: step 5685, loss 0.497326.
Train: 2018-08-02T13:05:43.865517: step 5686, loss 0.526509.
Train: 2018-08-02T13:05:44.021732: step 5687, loss 0.51286.
Train: 2018-08-02T13:05:44.162323: step 5688, loss 0.49158.
Train: 2018-08-02T13:05:44.318535: step 5689, loss 0.510367.
Train: 2018-08-02T13:05:44.474780: step 5690, loss 0.518011.
Test: 2018-08-02T13:05:44.927770: step 5690, loss 0.548251.
Train: 2018-08-02T13:05:45.084011: step 5691, loss 0.492816.
Train: 2018-08-02T13:05:45.240226: step 5692, loss 0.531711.
Train: 2018-08-02T13:05:45.380819: step 5693, loss 0.491635.
Train: 2018-08-02T13:05:45.537031: step 5694, loss 0.550105.
Train: 2018-08-02T13:05:45.693244: step 5695, loss 0.465637.
Train: 2018-08-02T13:05:45.849453: step 5696, loss 0.517706.
Train: 2018-08-02T13:05:45.990050: step 5697, loss 0.608773.
Train: 2018-08-02T13:05:46.146234: step 5698, loss 0.454071.
Train: 2018-08-02T13:05:46.302477: step 5699, loss 0.537141.
Train: 2018-08-02T13:05:46.458689: step 5700, loss 0.480678.
Test: 2018-08-02T13:05:46.927300: step 5700, loss 0.558246.
Train: 2018-08-02T13:05:47.536563: step 5701, loss 0.495499.
Train: 2018-08-02T13:05:47.692776: step 5702, loss 0.519894.
Train: 2018-08-02T13:05:47.848959: step 5703, loss 0.534447.
Train: 2018-08-02T13:05:48.005173: step 5704, loss 0.527775.
Train: 2018-08-02T13:05:48.145795: step 5705, loss 0.576215.
Train: 2018-08-02T13:05:48.302009: step 5706, loss 0.532439.
Train: 2018-08-02T13:05:48.458222: step 5707, loss 0.509714.
Train: 2018-08-02T13:05:48.614405: step 5708, loss 0.530408.
Train: 2018-08-02T13:05:48.755027: step 5709, loss 0.491822.
Train: 2018-08-02T13:05:48.911241: step 5710, loss 0.610769.
Test: 2018-08-02T13:05:49.364233: step 5710, loss 0.550678.
Train: 2018-08-02T13:05:49.520473: step 5711, loss 0.486157.
Train: 2018-08-02T13:05:49.676686: step 5712, loss 0.544272.
Train: 2018-08-02T13:05:49.817248: step 5713, loss 0.515041.
Train: 2018-08-02T13:05:49.973487: step 5714, loss 0.554915.
Train: 2018-08-02T13:05:50.129706: step 5715, loss 0.506462.
Train: 2018-08-02T13:05:50.285889: step 5716, loss 0.472424.
Train: 2018-08-02T13:05:50.442132: step 5717, loss 0.518328.
Train: 2018-08-02T13:05:50.582725: step 5718, loss 0.498782.
Train: 2018-08-02T13:05:50.738908: step 5719, loss 0.506286.
Train: 2018-08-02T13:05:50.895151: step 5720, loss 0.518292.
Test: 2018-08-02T13:05:51.348139: step 5720, loss 0.552023.
Train: 2018-08-02T13:05:51.504383: step 5721, loss 0.610432.
Train: 2018-08-02T13:05:51.660597: step 5722, loss 0.567321.
Train: 2018-08-02T13:05:51.816780: step 5723, loss 0.54842.
Train: 2018-08-02T13:05:51.957403: step 5724, loss 0.549296.
Train: 2018-08-02T13:05:52.113616: step 5725, loss 0.507497.
Train: 2018-08-02T13:05:52.269799: step 5726, loss 0.556985.
Train: 2018-08-02T13:05:52.410421: step 5727, loss 0.528218.
Train: 2018-08-02T13:05:52.566635: step 5728, loss 0.538842.
Train: 2018-08-02T13:05:52.722843: step 5729, loss 0.550992.
Train: 2018-08-02T13:05:52.879061: step 5730, loss 0.47577.
Test: 2018-08-02T13:05:53.332076: step 5730, loss 0.549231.
Train: 2018-08-02T13:05:53.488292: step 5731, loss 0.560672.
Train: 2018-08-02T13:05:53.628886: step 5732, loss 0.557059.
Train: 2018-08-02T13:05:53.785100: step 5733, loss 0.532208.
Train: 2018-08-02T13:05:53.941307: step 5734, loss 0.606583.
Train: 2018-08-02T13:05:54.097497: step 5735, loss 0.52976.
Train: 2018-08-02T13:05:54.253710: step 5736, loss 0.550501.
Train: 2018-08-02T13:05:54.394331: step 5737, loss 0.560043.
Train: 2018-08-02T13:05:54.503682: step 5738, loss 0.44574.
Train: 2018-08-02T13:05:54.659864: step 5739, loss 0.453375.
Train: 2018-08-02T13:05:54.816108: step 5740, loss 0.507956.
Test: 2018-08-02T13:05:55.269121: step 5740, loss 0.549686.
Train: 2018-08-02T13:05:55.425341: step 5741, loss 0.519883.
Train: 2018-08-02T13:05:55.581553: step 5742, loss 0.472248.
Train: 2018-08-02T13:05:55.737767: step 5743, loss 0.540144.
Train: 2018-08-02T13:05:55.878359: step 5744, loss 0.493026.
Train: 2018-08-02T13:05:56.034573: step 5745, loss 0.449859.
Train: 2018-08-02T13:05:56.190785: step 5746, loss 0.50759.
Train: 2018-08-02T13:05:56.346969: step 5747, loss 0.477034.
Train: 2018-08-02T13:05:56.503213: step 5748, loss 0.601098.
Train: 2018-08-02T13:05:56.643775: step 5749, loss 0.488235.
Train: 2018-08-02T13:05:56.800018: step 5750, loss 0.505825.
Test: 2018-08-02T13:05:57.253006: step 5750, loss 0.552658.
Train: 2018-08-02T13:05:57.409221: step 5751, loss 0.520298.
Train: 2018-08-02T13:05:57.565465: step 5752, loss 0.509074.
Train: 2018-08-02T13:05:57.721677: step 5753, loss 0.56398.
Train: 2018-08-02T13:05:57.877861: step 5754, loss 0.515416.
Train: 2018-08-02T13:05:58.018483: step 5755, loss 0.543346.
Train: 2018-08-02T13:05:58.174696: step 5756, loss 0.501637.
Train: 2018-08-02T13:05:58.330880: step 5757, loss 0.543948.
Train: 2018-08-02T13:05:58.471471: step 5758, loss 0.575467.
Train: 2018-08-02T13:05:58.627686: step 5759, loss 0.597685.
Train: 2018-08-02T13:05:58.783929: step 5760, loss 0.521561.
Test: 2018-08-02T13:05:59.236942: step 5760, loss 0.560374.
Train: 2018-08-02T13:05:59.393164: step 5761, loss 0.555242.
Train: 2018-08-02T13:05:59.549345: step 5762, loss 0.522896.
Train: 2018-08-02T13:05:59.705587: step 5763, loss 0.538252.
Train: 2018-08-02T13:05:59.861801: step 5764, loss 0.518434.
Train: 2018-08-02T13:06:00.002393: step 5765, loss 0.519409.
Train: 2018-08-02T13:06:00.158576: step 5766, loss 0.53925.
Train: 2018-08-02T13:06:00.314820: step 5767, loss 0.552016.
Train: 2018-08-02T13:06:00.471004: step 5768, loss 0.585806.
Train: 2018-08-02T13:06:00.627247: step 5769, loss 0.553701.
Train: 2018-08-02T13:06:00.767808: step 5770, loss 0.531307.
Test: 2018-08-02T13:06:01.236449: step 5770, loss 0.551772.
Train: 2018-08-02T13:06:01.392692: step 5771, loss 0.541922.
Train: 2018-08-02T13:06:01.533285: step 5772, loss 0.470232.
Train: 2018-08-02T13:06:01.689499: step 5773, loss 0.444907.
Train: 2018-08-02T13:06:01.845681: step 5774, loss 0.562855.
Train: 2018-08-02T13:06:02.001896: step 5775, loss 0.567919.
Train: 2018-08-02T13:06:02.142486: step 5776, loss 0.637353.
Train: 2018-08-02T13:06:02.298731: step 5777, loss 0.433099.
Train: 2018-08-02T13:06:02.454944: step 5778, loss 0.5291.
Train: 2018-08-02T13:06:02.595536: step 5779, loss 0.514199.
Train: 2018-08-02T13:06:02.751749: step 5780, loss 0.534983.
Test: 2018-08-02T13:06:03.220360: step 5780, loss 0.552683.
Train: 2018-08-02T13:06:03.360951: step 5781, loss 0.475211.
Train: 2018-08-02T13:06:03.517168: step 5782, loss 0.573769.
Train: 2018-08-02T13:06:03.673409: step 5783, loss 0.523133.
Train: 2018-08-02T13:06:03.813970: step 5784, loss 0.586622.
Train: 2018-08-02T13:06:03.970184: step 5785, loss 0.519537.
Train: 2018-08-02T13:06:04.126397: step 5786, loss 0.461479.
Train: 2018-08-02T13:06:04.282611: step 5787, loss 0.529724.
Train: 2018-08-02T13:06:04.438825: step 5788, loss 0.536992.
Train: 2018-08-02T13:06:04.595070: step 5789, loss 0.461235.
Train: 2018-08-02T13:06:04.735659: step 5790, loss 0.523382.
Test: 2018-08-02T13:06:05.204275: step 5790, loss 0.547409.
Train: 2018-08-02T13:06:05.344890: step 5791, loss 0.51277.
Train: 2018-08-02T13:06:05.501106: step 5792, loss 0.513835.
Train: 2018-08-02T13:06:05.657289: step 5793, loss 0.46657.
Train: 2018-08-02T13:06:05.797882: step 5794, loss 0.575769.
Train: 2018-08-02T13:06:05.954128: step 5795, loss 0.48934.
Train: 2018-08-02T13:06:06.110338: step 5796, loss 0.485996.
Train: 2018-08-02T13:06:06.250899: step 5797, loss 0.619609.
Train: 2018-08-02T13:06:06.407144: step 5798, loss 0.489876.
Train: 2018-08-02T13:06:06.563357: step 5799, loss 0.487935.
Train: 2018-08-02T13:06:06.719540: step 5800, loss 0.462494.
Test: 2018-08-02T13:06:07.172561: step 5800, loss 0.546357.
Train: 2018-08-02T13:06:07.797443: step 5801, loss 0.440581.
Train: 2018-08-02T13:06:07.953656: step 5802, loss 0.431699.
Train: 2018-08-02T13:06:08.094248: step 5803, loss 0.539788.
Train: 2018-08-02T13:06:08.250462: step 5804, loss 0.429221.
Train: 2018-08-02T13:06:08.406645: step 5805, loss 0.582086.
Train: 2018-08-02T13:06:08.562859: step 5806, loss 0.503702.
Train: 2018-08-02T13:06:08.703481: step 5807, loss 0.510356.
Train: 2018-08-02T13:06:08.859694: step 5808, loss 0.461264.
Train: 2018-08-02T13:06:09.015907: step 5809, loss 0.433183.
Train: 2018-08-02T13:06:09.156500: step 5810, loss 0.523947.
Test: 2018-08-02T13:06:09.625140: step 5810, loss 0.548623.
Train: 2018-08-02T13:06:09.765701: step 5811, loss 0.435162.
Train: 2018-08-02T13:06:09.921946: step 5812, loss 0.488951.
Train: 2018-08-02T13:06:10.078162: step 5813, loss 0.534254.
Train: 2018-08-02T13:06:10.234341: step 5814, loss 0.507368.
Train: 2018-08-02T13:06:10.374964: step 5815, loss 0.479726.
Train: 2018-08-02T13:06:10.531179: step 5816, loss 0.50245.
Train: 2018-08-02T13:06:10.687391: step 5817, loss 0.457357.
Train: 2018-08-02T13:06:10.843574: step 5818, loss 0.4761.
Train: 2018-08-02T13:06:10.984167: step 5819, loss 0.452436.
Train: 2018-08-02T13:06:11.140409: step 5820, loss 0.508833.
Test: 2018-08-02T13:06:11.593400: step 5820, loss 0.548527.
Train: 2018-08-02T13:06:11.749642: step 5821, loss 0.630725.
Train: 2018-08-02T13:06:11.905856: step 5822, loss 0.466053.
Train: 2018-08-02T13:06:12.062041: step 5823, loss 0.537959.
Train: 2018-08-02T13:06:12.202662: step 5824, loss 0.574829.
Train: 2018-08-02T13:06:12.358845: step 5825, loss 0.532644.
Train: 2018-08-02T13:06:12.499466: step 5826, loss 0.529009.
Train: 2018-08-02T13:06:12.655650: step 5827, loss 0.537333.
Train: 2018-08-02T13:06:12.811888: step 5828, loss 0.54761.
Train: 2018-08-02T13:06:12.968103: step 5829, loss 0.54694.
Train: 2018-08-02T13:06:13.124324: step 5830, loss 0.491893.
Test: 2018-08-02T13:06:13.577335: step 5830, loss 0.554511.
Train: 2018-08-02T13:06:13.733523: step 5831, loss 0.570088.
Train: 2018-08-02T13:06:13.889766: step 5832, loss 0.572784.
Train: 2018-08-02T13:06:14.030359: step 5833, loss 0.534055.
Train: 2018-08-02T13:06:14.186541: step 5834, loss 0.548443.
Train: 2018-08-02T13:06:14.327158: step 5835, loss 0.490792.
Train: 2018-08-02T13:06:14.483379: step 5836, loss 0.486999.
Train: 2018-08-02T13:06:14.639591: step 5837, loss 0.615924.
Train: 2018-08-02T13:06:14.795804: step 5838, loss 0.556905.
Train: 2018-08-02T13:06:14.936396: step 5839, loss 0.508802.
Train: 2018-08-02T13:06:15.092610: step 5840, loss 0.497807.
Test: 2018-08-02T13:06:15.545599: step 5840, loss 0.554093.
Train: 2018-08-02T13:06:15.701841: step 5841, loss 0.527392.
Train: 2018-08-02T13:06:15.858055: step 5842, loss 0.522671.
Train: 2018-08-02T13:06:15.998647: step 5843, loss 0.508521.
Train: 2018-08-02T13:06:16.154830: step 5844, loss 0.588736.
Train: 2018-08-02T13:06:16.311044: step 5845, loss 0.547827.
Train: 2018-08-02T13:06:16.451666: step 5846, loss 0.462201.
Train: 2018-08-02T13:06:16.607880: step 5847, loss 0.490365.
Train: 2018-08-02T13:06:16.764094: step 5848, loss 0.460868.
Train: 2018-08-02T13:06:16.904686: step 5849, loss 0.577765.
Train: 2018-08-02T13:06:17.060898: step 5850, loss 0.540609.
Test: 2018-08-02T13:06:17.529534: step 5850, loss 0.54739.
Train: 2018-08-02T13:06:17.670131: step 5851, loss 0.539621.
Train: 2018-08-02T13:06:17.826315: step 5852, loss 0.496677.
Train: 2018-08-02T13:06:17.982527: step 5853, loss 0.560032.
Train: 2018-08-02T13:06:18.138771: step 5854, loss 0.482225.
Train: 2018-08-02T13:06:18.279332: step 5855, loss 0.568301.
Train: 2018-08-02T13:06:18.435547: step 5856, loss 0.478248.
Train: 2018-08-02T13:06:18.591791: step 5857, loss 0.498108.
Train: 2018-08-02T13:06:18.732383: step 5858, loss 0.519902.
Train: 2018-08-02T13:06:18.888596: step 5859, loss 0.6415.
Train: 2018-08-02T13:06:19.044778: step 5860, loss 0.585596.
Test: 2018-08-02T13:06:19.513419: step 5860, loss 0.5559.
Train: 2018-08-02T13:06:19.654042: step 5861, loss 0.607527.
Train: 2018-08-02T13:06:19.810224: step 5862, loss 0.540977.
Train: 2018-08-02T13:06:19.966468: step 5863, loss 0.520779.
Train: 2018-08-02T13:06:20.107031: step 5864, loss 0.528324.
Train: 2018-08-02T13:06:20.263273: step 5865, loss 0.468627.
Train: 2018-08-02T13:06:20.419487: step 5866, loss 0.603658.
Train: 2018-08-02T13:06:20.560079: step 5867, loss 0.513861.
Train: 2018-08-02T13:06:20.716293: step 5868, loss 0.508289.
Train: 2018-08-02T13:06:20.872477: step 5869, loss 0.471424.
Train: 2018-08-02T13:06:21.013100: step 5870, loss 0.505387.
Test: 2018-08-02T13:06:21.466087: step 5870, loss 0.548707.
Train: 2018-08-02T13:06:21.622324: step 5871, loss 0.532977.
Train: 2018-08-02T13:06:21.778514: step 5872, loss 0.518143.
Train: 2018-08-02T13:06:21.934727: step 5873, loss 0.606161.
Train: 2018-08-02T13:06:22.090970: step 5874, loss 0.572564.
Train: 2018-08-02T13:06:22.231563: step 5875, loss 0.537366.
Train: 2018-08-02T13:06:22.387746: step 5876, loss 0.526885.
Train: 2018-08-02T13:06:22.543990: step 5877, loss 0.492981.
Train: 2018-08-02T13:06:22.684582: step 5878, loss 0.484967.
Train: 2018-08-02T13:06:22.840795: step 5879, loss 0.627423.
Train: 2018-08-02T13:06:22.997009: step 5880, loss 0.604981.
Test: 2018-08-02T13:06:23.450028: step 5880, loss 0.556769.
Train: 2018-08-02T13:06:23.606235: step 5881, loss 0.473604.
Train: 2018-08-02T13:06:23.762425: step 5882, loss 0.550793.
Train: 2018-08-02T13:06:23.903047: step 5883, loss 0.506662.
Train: 2018-08-02T13:06:24.059260: step 5884, loss 0.462467.
Train: 2018-08-02T13:06:24.215473: step 5885, loss 0.581879.
Train: 2018-08-02T13:06:24.356065: step 5886, loss 0.516704.
Train: 2018-08-02T13:06:24.512249: step 5887, loss 0.550571.
Train: 2018-08-02T13:06:24.668462: step 5888, loss 0.47651.
Train: 2018-08-02T13:06:24.762220: step 5889, loss 0.599436.
Train: 2018-08-02T13:06:24.918405: step 5890, loss 0.508788.
Test: 2018-08-02T13:06:25.387074: step 5890, loss 0.558435.
Train: 2018-08-02T13:06:25.527635: step 5891, loss 0.583982.
Train: 2018-08-02T13:06:25.683880: step 5892, loss 0.49598.
Train: 2018-08-02T13:06:25.840092: step 5893, loss 0.465255.
Train: 2018-08-02T13:06:25.996275: step 5894, loss 0.466942.
Train: 2018-08-02T13:06:26.136898: step 5895, loss 0.566579.
Train: 2018-08-02T13:06:26.293114: step 5896, loss 0.482537.
Train: 2018-08-02T13:06:26.433704: step 5897, loss 0.580929.
Train: 2018-08-02T13:06:26.589888: step 5898, loss 0.519192.
Train: 2018-08-02T13:06:26.746131: step 5899, loss 0.533164.
Train: 2018-08-02T13:06:26.886722: step 5900, loss 0.507237.
Test: 2018-08-02T13:06:27.355333: step 5900, loss 0.550332.
Train: 2018-08-02T13:06:27.964595: step 5901, loss 0.55485.
Train: 2018-08-02T13:06:28.120779: step 5902, loss 0.55473.
Train: 2018-08-02T13:06:28.277027: step 5903, loss 0.469194.
Train: 2018-08-02T13:06:28.417614: step 5904, loss 0.584498.
Train: 2018-08-02T13:06:28.573828: step 5905, loss 0.563189.
Train: 2018-08-02T13:06:28.730011: step 5906, loss 0.506048.
Train: 2018-08-02T13:06:28.870633: step 5907, loss 0.57373.
Train: 2018-08-02T13:06:29.026847: step 5908, loss 0.473949.
Train: 2018-08-02T13:06:29.183061: step 5909, loss 0.528794.
Train: 2018-08-02T13:06:29.323622: step 5910, loss 0.466867.
Test: 2018-08-02T13:06:29.792287: step 5910, loss 0.547969.
Train: 2018-08-02T13:06:29.932885: step 5911, loss 0.510017.
Train: 2018-08-02T13:06:30.089099: step 5912, loss 0.499721.
Train: 2018-08-02T13:06:30.245311: step 5913, loss 0.586848.
Train: 2018-08-02T13:06:30.401525: step 5914, loss 0.559752.
Train: 2018-08-02T13:06:30.542117: step 5915, loss 0.548312.
Train: 2018-08-02T13:06:30.698330: step 5916, loss 0.51713.
Train: 2018-08-02T13:06:30.854514: step 5917, loss 0.627692.
Train: 2018-08-02T13:06:30.995136: step 5918, loss 0.53289.
Train: 2018-08-02T13:06:31.151350: step 5919, loss 0.537964.
Train: 2018-08-02T13:06:31.307532: step 5920, loss 0.564226.
Test: 2018-08-02T13:06:31.760576: step 5920, loss 0.556651.
Train: 2018-08-02T13:06:31.916765: step 5921, loss 0.49412.
Train: 2018-08-02T13:06:32.057356: step 5922, loss 0.557525.
Train: 2018-08-02T13:06:32.213569: step 5923, loss 0.47541.
Train: 2018-08-02T13:06:32.369814: step 5924, loss 0.552026.
Train: 2018-08-02T13:06:32.510406: step 5925, loss 0.514974.
Train: 2018-08-02T13:06:32.666590: step 5926, loss 0.571089.
Train: 2018-08-02T13:06:32.822833: step 5927, loss 0.565122.
Train: 2018-08-02T13:06:33.025911: step 5928, loss 0.537469.
Train: 2018-08-02T13:06:33.182093: step 5929, loss 0.489388.
Train: 2018-08-02T13:06:33.338337: step 5930, loss 0.520012.
Test: 2018-08-02T13:06:33.791326: step 5930, loss 0.551689.
Train: 2018-08-02T13:06:33.947538: step 5931, loss 0.50549.
Train: 2018-08-02T13:06:34.088161: step 5932, loss 0.533434.
Train: 2018-08-02T13:06:34.244374: step 5933, loss 0.445498.
Train: 2018-08-02T13:06:34.400589: step 5934, loss 0.490871.
Train: 2018-08-02T13:06:34.541180: step 5935, loss 0.550849.
Train: 2018-08-02T13:06:34.697394: step 5936, loss 0.390389.
Train: 2018-08-02T13:06:34.853576: step 5937, loss 0.515386.
Train: 2018-08-02T13:06:34.994199: step 5938, loss 0.519611.
Train: 2018-08-02T13:06:35.150413: step 5939, loss 0.671333.
Train: 2018-08-02T13:06:35.306626: step 5940, loss 0.506437.
Test: 2018-08-02T13:06:35.759614: step 5940, loss 0.550709.
Train: 2018-08-02T13:06:35.915858: step 5941, loss 0.519113.
Train: 2018-08-02T13:06:36.072042: step 5942, loss 0.536641.
Train: 2018-08-02T13:06:36.212664: step 5943, loss 0.485684.
Train: 2018-08-02T13:06:36.368848: step 5944, loss 0.535406.
Train: 2018-08-02T13:06:36.525091: step 5945, loss 0.581403.
Train: 2018-08-02T13:06:36.665653: step 5946, loss 0.564046.
Train: 2018-08-02T13:06:36.821867: step 5947, loss 0.546897.
Train: 2018-08-02T13:06:36.978110: step 5948, loss 0.542755.
Train: 2018-08-02T13:06:37.118702: step 5949, loss 0.502279.
Train: 2018-08-02T13:06:37.274916: step 5950, loss 0.523092.
Test: 2018-08-02T13:06:37.727905: step 5950, loss 0.551486.
Train: 2018-08-02T13:06:37.884148: step 5951, loss 0.532364.
Train: 2018-08-02T13:06:38.040330: step 5952, loss 0.44326.
Train: 2018-08-02T13:06:38.196544: step 5953, loss 0.635079.
Train: 2018-08-02T13:06:38.337166: step 5954, loss 0.534763.
Train: 2018-08-02T13:06:38.493380: step 5955, loss 0.482462.
Train: 2018-08-02T13:06:38.649562: step 5956, loss 0.520445.
Train: 2018-08-02T13:06:38.790154: step 5957, loss 0.501893.
Train: 2018-08-02T13:06:38.946399: step 5958, loss 0.460452.
Train: 2018-08-02T13:06:39.102582: step 5959, loss 0.638532.
Train: 2018-08-02T13:06:39.243204: step 5960, loss 0.469182.
Test: 2018-08-02T13:06:39.711816: step 5960, loss 0.551183.
Train: 2018-08-02T13:06:39.852406: step 5961, loss 0.537947.
Train: 2018-08-02T13:06:40.008651: step 5962, loss 0.570103.
Train: 2018-08-02T13:06:40.164865: step 5963, loss 0.436702.
Train: 2018-08-02T13:06:40.321077: step 5964, loss 0.521664.
Train: 2018-08-02T13:06:40.461668: step 5965, loss 0.421553.
Train: 2018-08-02T13:06:40.617876: step 5966, loss 0.578718.
Train: 2018-08-02T13:06:40.774096: step 5967, loss 0.498171.
Train: 2018-08-02T13:06:40.914659: step 5968, loss 0.631774.
Train: 2018-08-02T13:06:41.070872: step 5969, loss 0.500578.
Train: 2018-08-02T13:06:41.227117: step 5970, loss 0.538226.
Test: 2018-08-02T13:06:41.680103: step 5970, loss 0.553113.
Train: 2018-08-02T13:06:41.836317: step 5971, loss 0.530884.
Train: 2018-08-02T13:06:41.992554: step 5972, loss 0.56191.
Train: 2018-08-02T13:06:42.148770: step 5973, loss 0.532488.
Train: 2018-08-02T13:06:42.304987: step 5974, loss 0.516647.
Train: 2018-08-02T13:06:42.461201: step 5975, loss 0.533322.
Train: 2018-08-02T13:06:42.617385: step 5976, loss 0.525452.
Train: 2018-08-02T13:06:42.758006: step 5977, loss 0.512983.
Train: 2018-08-02T13:06:42.914220: step 5978, loss 0.588227.
Train: 2018-08-02T13:06:43.054811: step 5979, loss 0.578122.
Train: 2018-08-02T13:06:43.211024: step 5980, loss 0.53568.
Test: 2018-08-02T13:06:43.664015: step 5980, loss 0.555444.
Train: 2018-08-02T13:06:43.867121: step 5981, loss 0.52689.
Train: 2018-08-02T13:06:44.023336: step 5982, loss 0.455674.
Train: 2018-08-02T13:06:44.179518: step 5983, loss 0.47257.
Train: 2018-08-02T13:06:44.320140: step 5984, loss 0.556955.
Train: 2018-08-02T13:06:44.476353: step 5985, loss 0.559026.
Train: 2018-08-02T13:06:44.632567: step 5986, loss 0.505894.
Train: 2018-08-02T13:06:44.773128: step 5987, loss 0.515698.
Train: 2018-08-02T13:06:44.929372: step 5988, loss 0.509639.
Train: 2018-08-02T13:06:45.085556: step 5989, loss 0.544963.
Train: 2018-08-02T13:06:45.226178: step 5990, loss 0.604691.
Test: 2018-08-02T13:06:45.694792: step 5990, loss 0.550975.
Train: 2018-08-02T13:06:45.835410: step 5991, loss 0.519208.
Train: 2018-08-02T13:06:45.991593: step 5992, loss 0.500653.
Train: 2018-08-02T13:06:46.147838: step 5993, loss 0.586604.
Train: 2018-08-02T13:06:46.288429: step 5994, loss 0.525352.
Train: 2018-08-02T13:06:46.444637: step 5995, loss 0.484947.
Train: 2018-08-02T13:06:46.600856: step 5996, loss 0.539688.
Train: 2018-08-02T13:06:46.741448: step 5997, loss 0.524524.
Train: 2018-08-02T13:06:46.897662: step 5998, loss 0.550797.
Train: 2018-08-02T13:06:47.053875: step 5999, loss 0.563219.
Train: 2018-08-02T13:06:47.194467: step 6000, loss 0.532317.
Test: 2018-08-02T13:06:47.663108: step 6000, loss 0.550662.
Train: 2018-08-02T13:06:48.350417: step 6001, loss 0.553122.
Train: 2018-08-02T13:06:48.491038: step 6002, loss 0.515155.
Train: 2018-08-02T13:06:48.647250: step 6003, loss 0.532235.
Train: 2018-08-02T13:06:48.803435: step 6004, loss 0.530928.
Train: 2018-08-02T13:06:48.959649: step 6005, loss 0.452165.
Train: 2018-08-02T13:06:49.100271: step 6006, loss 0.51263.
Train: 2018-08-02T13:06:49.256453: step 6007, loss 0.617025.
Train: 2018-08-02T13:06:49.412692: step 6008, loss 0.523539.
Train: 2018-08-02T13:06:49.553290: step 6009, loss 0.475646.
Train: 2018-08-02T13:06:49.709504: step 6010, loss 0.484968.
Test: 2018-08-02T13:06:50.162526: step 6010, loss 0.553128.
Train: 2018-08-02T13:06:50.318735: step 6011, loss 0.509191.
Train: 2018-08-02T13:06:50.474949: step 6012, loss 0.481925.
Train: 2018-08-02T13:06:50.615510: step 6013, loss 0.611177.
Train: 2018-08-02T13:06:50.771725: step 6014, loss 0.580472.
Train: 2018-08-02T13:06:50.927938: step 6015, loss 0.467801.
Train: 2018-08-02T13:06:51.068559: step 6016, loss 0.549594.
Train: 2018-08-02T13:06:51.224745: step 6017, loss 0.501234.
Train: 2018-08-02T13:06:51.380987: step 6018, loss 0.477418.
Train: 2018-08-02T13:06:51.521579: step 6019, loss 0.493214.
Train: 2018-08-02T13:06:51.677792: step 6020, loss 0.481442.
Test: 2018-08-02T13:06:52.130806: step 6020, loss 0.547154.
Train: 2018-08-02T13:06:52.287025: step 6021, loss 0.475037.
Train: 2018-08-02T13:06:52.443238: step 6022, loss 0.554229.
Train: 2018-08-02T13:06:52.583799: step 6023, loss 0.444781.
Train: 2018-08-02T13:06:52.740013: step 6024, loss 0.486725.
Train: 2018-08-02T13:06:52.896258: step 6025, loss 0.565137.
Train: 2018-08-02T13:06:53.052439: step 6026, loss 0.49651.
Train: 2018-08-02T13:06:53.193063: step 6027, loss 0.485734.
Train: 2018-08-02T13:06:53.349276: step 6028, loss 0.522948.
Train: 2018-08-02T13:06:53.489868: step 6029, loss 0.424395.
Train: 2018-08-02T13:06:53.646087: step 6030, loss 0.48852.
Test: 2018-08-02T13:06:54.099071: step 6030, loss 0.547975.
Train: 2018-08-02T13:06:54.255313: step 6031, loss 0.544455.
Train: 2018-08-02T13:06:54.395906: step 6032, loss 0.499793.
Train: 2018-08-02T13:06:54.552123: step 6033, loss 0.523956.
Train: 2018-08-02T13:06:54.708303: step 6034, loss 0.565096.
Train: 2018-08-02T13:06:54.864546: step 6035, loss 0.441235.
Train: 2018-08-02T13:06:55.020729: step 6036, loss 0.55258.
Train: 2018-08-02T13:06:55.161352: step 6037, loss 0.528191.
Train: 2018-08-02T13:06:55.317535: step 6038, loss 0.492697.
Train: 2018-08-02T13:06:55.473778: step 6039, loss 0.523891.
Train: 2018-08-02T13:06:55.567506: step 6040, loss 0.396903.
Test: 2018-08-02T13:06:56.036117: step 6040, loss 0.548338.
Train: 2018-08-02T13:06:56.176741: step 6041, loss 0.548584.
Train: 2018-08-02T13:06:56.332955: step 6042, loss 0.539046.
Train: 2018-08-02T13:06:56.489167: step 6043, loss 0.496715.
Train: 2018-08-02T13:06:56.629757: step 6044, loss 0.506827.
Train: 2018-08-02T13:06:56.785971: step 6045, loss 0.550541.
Train: 2018-08-02T13:06:56.942155: step 6046, loss 0.571608.
Train: 2018-08-02T13:06:57.082747: step 6047, loss 0.576711.
Train: 2018-08-02T13:06:57.238959: step 6048, loss 0.502924.
Train: 2018-08-02T13:06:57.395204: step 6049, loss 0.486137.
Train: 2018-08-02T13:06:57.535796: step 6050, loss 0.519042.
Test: 2018-08-02T13:06:58.004436: step 6050, loss 0.554969.
Train: 2018-08-02T13:06:58.160643: step 6051, loss 0.586091.
Train: 2018-08-02T13:06:58.301241: step 6052, loss 0.455499.
Train: 2018-08-02T13:06:58.457457: step 6053, loss 0.497566.
Train: 2018-08-02T13:06:58.613667: step 6054, loss 0.441508.
Train: 2018-08-02T13:06:58.754260: step 6055, loss 0.492279.
Train: 2018-08-02T13:06:58.910474: step 6056, loss 0.493928.
Train: 2018-08-02T13:06:59.066658: step 6057, loss 0.568409.
Train: 2018-08-02T13:06:59.222871: step 6058, loss 0.550045.
Train: 2018-08-02T13:06:59.363492: step 6059, loss 0.486382.
Train: 2018-08-02T13:06:59.519706: step 6060, loss 0.452554.
Test: 2018-08-02T13:06:59.972720: step 6060, loss 0.549486.
Train: 2018-08-02T13:07:00.128938: step 6061, loss 0.539422.
Train: 2018-08-02T13:07:00.285122: step 6062, loss 0.499242.
Train: 2018-08-02T13:07:00.425744: step 6063, loss 0.442194.
Train: 2018-08-02T13:07:00.597578: step 6064, loss 0.512132.
Train: 2018-08-02T13:07:00.738141: step 6065, loss 0.533765.
Train: 2018-08-02T13:07:00.894353: step 6066, loss 0.537993.
Train: 2018-08-02T13:07:01.050567: step 6067, loss 0.53832.
Train: 2018-08-02T13:07:01.191189: step 6068, loss 0.553709.
Train: 2018-08-02T13:07:01.347403: step 6069, loss 0.506466.
Train: 2018-08-02T13:07:01.503616: step 6070, loss 0.48528.
Test: 2018-08-02T13:07:01.956631: step 6070, loss 0.550084.
Train: 2018-08-02T13:07:02.112848: step 6071, loss 0.505639.
Train: 2018-08-02T13:07:02.253441: step 6072, loss 0.56092.
Train: 2018-08-02T13:07:02.409654: step 6073, loss 0.493859.
Train: 2018-08-02T13:07:02.565873: step 6074, loss 0.562123.
Train: 2018-08-02T13:07:02.706459: step 6075, loss 0.536839.
Train: 2018-08-02T13:07:02.862643: step 6076, loss 0.576155.
Train: 2018-08-02T13:07:03.018892: step 6077, loss 0.513538.
Train: 2018-08-02T13:07:03.159448: step 6078, loss 0.592025.
Train: 2018-08-02T13:07:03.315692: step 6079, loss 0.449593.
Train: 2018-08-02T13:07:03.471905: step 6080, loss 0.479041.
Test: 2018-08-02T13:07:03.924914: step 6080, loss 0.555573.
Train: 2018-08-02T13:07:04.081138: step 6081, loss 0.52793.
Train: 2018-08-02T13:07:04.237351: step 6082, loss 0.474218.
Train: 2018-08-02T13:07:04.377943: step 6083, loss 0.578427.
Train: 2018-08-02T13:07:04.534152: step 6084, loss 0.546343.
Train: 2018-08-02T13:07:04.690371: step 6085, loss 0.514162.
Train: 2018-08-02T13:07:04.846577: step 6086, loss 0.546106.
Train: 2018-08-02T13:07:04.987146: step 6087, loss 0.540021.
Train: 2018-08-02T13:07:05.143390: step 6088, loss 0.48266.
Train: 2018-08-02T13:07:05.299572: step 6089, loss 0.455016.
Train: 2018-08-02T13:07:05.455786: step 6090, loss 0.580213.
Test: 2018-08-02T13:07:05.908836: step 6090, loss 0.548705.
Train: 2018-08-02T13:07:06.049396: step 6091, loss 0.467106.
Train: 2018-08-02T13:07:06.205640: step 6092, loss 0.600549.
Train: 2018-08-02T13:07:06.361823: step 6093, loss 0.512592.
Train: 2018-08-02T13:07:06.518068: step 6094, loss 0.572619.
Train: 2018-08-02T13:07:06.658629: step 6095, loss 0.497892.
Train: 2018-08-02T13:07:06.814874: step 6096, loss 0.503598.
Train: 2018-08-02T13:07:06.971086: step 6097, loss 0.56774.
Train: 2018-08-02T13:07:07.111679: step 6098, loss 0.610594.
Train: 2018-08-02T13:07:07.267861: step 6099, loss 0.527551.
Train: 2018-08-02T13:07:07.424104: step 6100, loss 0.517366.
Test: 2018-08-02T13:07:07.877119: step 6100, loss 0.559213.
Train: 2018-08-02T13:07:08.533221: step 6101, loss 0.538047.
Train: 2018-08-02T13:07:08.689433: step 6102, loss 0.548731.
Train: 2018-08-02T13:07:08.845646: step 6103, loss 0.549058.
Train: 2018-08-02T13:07:08.986239: step 6104, loss 0.562262.
Train: 2018-08-02T13:07:09.142423: step 6105, loss 0.523637.
Train: 2018-08-02T13:07:09.298666: step 6106, loss 0.567264.
Train: 2018-08-02T13:07:09.454879: step 6107, loss 0.549523.
Train: 2018-08-02T13:07:09.611092: step 6108, loss 0.529504.
Train: 2018-08-02T13:07:09.751654: step 6109, loss 0.50071.
Train: 2018-08-02T13:07:09.907899: step 6110, loss 0.550347.
Test: 2018-08-02T13:07:10.360912: step 6110, loss 0.550893.
Train: 2018-08-02T13:07:10.517126: step 6111, loss 0.52261.
Train: 2018-08-02T13:07:10.673314: step 6112, loss 0.502491.
Train: 2018-08-02T13:07:10.829552: step 6113, loss 0.448865.
Train: 2018-08-02T13:07:10.985741: step 6114, loss 0.451951.
Train: 2018-08-02T13:07:11.126363: step 6115, loss 0.52786.
Train: 2018-08-02T13:07:11.282576: step 6116, loss 0.532609.
Train: 2018-08-02T13:07:11.438760: step 6117, loss 0.53248.
Train: 2018-08-02T13:07:11.595005: step 6118, loss 0.472457.
Train: 2018-08-02T13:07:11.735566: step 6119, loss 0.612092.
Train: 2018-08-02T13:07:11.891779: step 6120, loss 0.601713.
Test: 2018-08-02T13:07:12.360419: step 6120, loss 0.552225.
Train: 2018-08-02T13:07:12.501041: step 6121, loss 0.545431.
Train: 2018-08-02T13:07:12.657254: step 6122, loss 0.510345.
Train: 2018-08-02T13:07:12.813470: step 6123, loss 0.554148.
Train: 2018-08-02T13:07:12.954060: step 6124, loss 0.533071.
Train: 2018-08-02T13:07:13.110273: step 6125, loss 0.474587.
Train: 2018-08-02T13:07:13.266482: step 6126, loss 0.54968.
Train: 2018-08-02T13:07:13.422700: step 6127, loss 0.507691.
Train: 2018-08-02T13:07:13.563263: step 6128, loss 0.543577.
Train: 2018-08-02T13:07:13.719475: step 6129, loss 0.438193.
Train: 2018-08-02T13:07:13.875719: step 6130, loss 0.609503.
Test: 2018-08-02T13:07:14.328708: step 6130, loss 0.546713.
Train: 2018-08-02T13:07:14.484920: step 6131, loss 0.443535.
Train: 2018-08-02T13:07:14.641165: step 6132, loss 0.558066.
Train: 2018-08-02T13:07:14.781726: step 6133, loss 0.497841.
Train: 2018-08-02T13:07:14.937972: step 6134, loss 0.582926.
Train: 2018-08-02T13:07:15.094154: step 6135, loss 0.555981.
Train: 2018-08-02T13:07:15.234775: step 6136, loss 0.44345.
Train: 2018-08-02T13:07:15.390990: step 6137, loss 0.604385.
Train: 2018-08-02T13:07:15.547186: step 6138, loss 0.545502.
Train: 2018-08-02T13:07:15.703419: step 6139, loss 0.558819.
Train: 2018-08-02T13:07:15.844008: step 6140, loss 0.513524.
Test: 2018-08-02T13:07:16.312618: step 6140, loss 0.550737.
Train: 2018-08-02T13:07:16.468861: step 6141, loss 0.517316.
Train: 2018-08-02T13:07:16.609426: step 6142, loss 0.509081.
Train: 2018-08-02T13:07:16.765668: step 6143, loss 0.474205.
Train: 2018-08-02T13:07:16.921880: step 6144, loss 0.489832.
Train: 2018-08-02T13:07:17.078088: step 6145, loss 0.649058.
Train: 2018-08-02T13:07:17.234308: step 6146, loss 0.474574.
Train: 2018-08-02T13:07:17.374899: step 6147, loss 0.435778.
Train: 2018-08-02T13:07:17.531114: step 6148, loss 0.57927.
Train: 2018-08-02T13:07:17.687326: step 6149, loss 0.42515.
Train: 2018-08-02T13:07:17.843540: step 6150, loss 0.547046.
Test: 2018-08-02T13:07:18.296554: step 6150, loss 0.550317.
Train: 2018-08-02T13:07:18.452741: step 6151, loss 0.493523.
Train: 2018-08-02T13:07:18.608986: step 6152, loss 0.509526.
Train: 2018-08-02T13:07:18.765200: step 6153, loss 0.499374.
Train: 2018-08-02T13:07:18.915729: step 6154, loss 0.525598.
Train: 2018-08-02T13:07:19.088373: step 6155, loss 0.492153.
Train: 2018-08-02T13:07:19.243859: step 6156, loss 0.525451.
Train: 2018-08-02T13:07:19.399661: step 6157, loss 0.543065.
Train: 2018-08-02T13:07:19.569594: step 6158, loss 0.505576.
Train: 2018-08-02T13:07:19.729755: step 6159, loss 0.491646.
Train: 2018-08-02T13:07:19.900006: step 6160, loss 0.657067.
Test: 2018-08-02T13:07:20.359521: step 6160, loss 0.554179.
Train: 2018-08-02T13:07:20.519929: step 6161, loss 0.472128.
Train: 2018-08-02T13:07:20.683956: step 6162, loss 0.548679.
Train: 2018-08-02T13:07:20.840836: step 6163, loss 0.50337.
Train: 2018-08-02T13:07:20.997060: step 6164, loss 0.518657.
Train: 2018-08-02T13:07:21.137646: step 6165, loss 0.476224.
Train: 2018-08-02T13:07:21.293890: step 6166, loss 0.51919.
Train: 2018-08-02T13:07:21.450103: step 6167, loss 0.465545.
Train: 2018-08-02T13:07:21.606317: step 6168, loss 0.477973.
Train: 2018-08-02T13:07:21.762500: step 6169, loss 0.502234.
Train: 2018-08-02T13:07:21.918738: step 6170, loss 0.478935.
Test: 2018-08-02T13:07:22.371758: step 6170, loss 0.548916.
Train: 2018-08-02T13:07:22.527980: step 6171, loss 0.448912.
Train: 2018-08-02T13:07:22.684159: step 6172, loss 0.48565.
Train: 2018-08-02T13:07:22.840402: step 6173, loss 0.495183.
Train: 2018-08-02T13:07:22.996617: step 6174, loss 0.533644.
Train: 2018-08-02T13:07:23.152834: step 6175, loss 0.536116.
Train: 2018-08-02T13:07:23.309013: step 6176, loss 0.506451.
Train: 2018-08-02T13:07:23.465226: step 6177, loss 0.479503.
Train: 2018-08-02T13:07:23.621439: step 6178, loss 0.48665.
Train: 2018-08-02T13:07:23.777678: step 6179, loss 0.524519.
Train: 2018-08-02T13:07:23.933868: step 6180, loss 0.575359.
Test: 2018-08-02T13:07:24.386885: step 6180, loss 0.5514.
Train: 2018-08-02T13:07:24.543099: step 6181, loss 0.453403.
Train: 2018-08-02T13:07:24.699312: step 6182, loss 0.503525.
Train: 2018-08-02T13:07:24.855556: step 6183, loss 0.484008.
Train: 2018-08-02T13:07:25.011740: step 6184, loss 0.502876.
Train: 2018-08-02T13:07:25.167976: step 6185, loss 0.490044.
Train: 2018-08-02T13:07:25.308545: step 6186, loss 0.561293.
Train: 2018-08-02T13:07:25.464761: step 6187, loss 0.513376.
Train: 2018-08-02T13:07:25.620997: step 6188, loss 0.551872.
Train: 2018-08-02T13:07:25.777214: step 6189, loss 0.538532.
Train: 2018-08-02T13:07:25.933397: step 6190, loss 0.564889.
Test: 2018-08-02T13:07:26.386443: step 6190, loss 0.555522.
Train: 2018-08-02T13:07:26.495803: step 6191, loss 0.571173.
Train: 2018-08-02T13:07:26.667627: step 6192, loss 0.49971.
Train: 2018-08-02T13:07:26.823840: step 6193, loss 0.517324.
Train: 2018-08-02T13:07:27.026893: step 6194, loss 0.501151.
Train: 2018-08-02T13:07:27.198727: step 6195, loss 0.529427.
Train: 2018-08-02T13:07:27.370562: step 6196, loss 0.551097.
Train: 2018-08-02T13:07:27.526800: step 6197, loss 0.516879.
Train: 2018-08-02T13:07:27.682998: step 6198, loss 0.578694.
Train: 2018-08-02T13:07:27.839202: step 6199, loss 0.425161.
Train: 2018-08-02T13:07:27.995445: step 6200, loss 0.508781.
Test: 2018-08-02T13:07:28.448435: step 6200, loss 0.547443.
Train: 2018-08-02T13:07:29.057697: step 6201, loss 0.519425.
Train: 2018-08-02T13:07:29.213881: step 6202, loss 0.498498.
Train: 2018-08-02T13:07:29.370124: step 6203, loss 0.473741.
Train: 2018-08-02T13:07:29.526337: step 6204, loss 0.51172.
Train: 2018-08-02T13:07:29.682523: step 6205, loss 0.532223.
Train: 2018-08-02T13:07:29.838767: step 6206, loss 0.512383.
Train: 2018-08-02T13:07:29.994986: step 6207, loss 0.523786.
Train: 2018-08-02T13:07:30.151191: step 6208, loss 0.490312.
Train: 2018-08-02T13:07:30.291782: step 6209, loss 0.525499.
Train: 2018-08-02T13:07:30.447996: step 6210, loss 0.485009.
Test: 2018-08-02T13:07:30.900984: step 6210, loss 0.548163.
Train: 2018-08-02T13:07:31.057229: step 6211, loss 0.479122.
Train: 2018-08-02T13:07:31.213441: step 6212, loss 0.663535.
Train: 2018-08-02T13:07:31.369655: step 6213, loss 0.536735.
Train: 2018-08-02T13:07:31.510247: step 6214, loss 0.562125.
Train: 2018-08-02T13:07:31.666431: step 6215, loss 0.54825.
Train: 2018-08-02T13:07:31.822675: step 6216, loss 0.606982.
Train: 2018-08-02T13:07:31.978888: step 6217, loss 0.537768.
Train: 2018-08-02T13:07:32.135073: step 6218, loss 0.500152.
Train: 2018-08-02T13:07:32.291284: step 6219, loss 0.553631.
Train: 2018-08-02T13:07:32.447521: step 6220, loss 0.517033.
Test: 2018-08-02T13:07:32.900544: step 6220, loss 0.563976.
Train: 2018-08-02T13:07:33.056762: step 6221, loss 0.544361.
Train: 2018-08-02T13:07:33.197322: step 6222, loss 0.493545.
Train: 2018-08-02T13:07:33.353565: step 6223, loss 0.519835.
Train: 2018-08-02T13:07:33.509779: step 6224, loss 0.493032.
Train: 2018-08-02T13:07:33.665962: step 6225, loss 0.561757.
Train: 2018-08-02T13:07:33.806584: step 6226, loss 0.475546.
Train: 2018-08-02T13:07:33.962798: step 6227, loss 0.553997.
Train: 2018-08-02T13:07:34.119007: step 6228, loss 0.554226.
Train: 2018-08-02T13:07:34.275194: step 6229, loss 0.596932.
Train: 2018-08-02T13:07:34.431407: step 6230, loss 0.534575.
Test: 2018-08-02T13:07:34.884452: step 6230, loss 0.563246.
Train: 2018-08-02T13:07:35.040641: step 6231, loss 0.549436.
Train: 2018-08-02T13:07:35.203013: step 6232, loss 0.471737.
Train: 2018-08-02T13:07:35.359225: step 6233, loss 0.550759.
Train: 2018-08-02T13:07:35.519886: step 6234, loss 0.545729.
Train: 2018-08-02T13:07:35.675405: step 6235, loss 0.5906.
Train: 2018-08-02T13:07:35.829649: step 6236, loss 0.531833.
Train: 2018-08-02T13:07:35.975964: step 6237, loss 0.500857.
Train: 2018-08-02T13:07:36.135674: step 6238, loss 0.614523.
Train: 2018-08-02T13:07:36.290444: step 6239, loss 0.578552.
Train: 2018-08-02T13:07:36.445477: step 6240, loss 0.541558.
Test: 2018-08-02T13:07:36.896061: step 6240, loss 0.560191.
Train: 2018-08-02T13:07:37.052274: step 6241, loss 0.558762.
Train: 2018-08-02T13:07:37.208489: step 6242, loss 0.521759.
Train: 2018-08-02T13:07:37.349080: step 6243, loss 0.534159.
Train: 2018-08-02T13:07:37.505293: step 6244, loss 0.530815.
Train: 2018-08-02T13:07:37.661536: step 6245, loss 0.575326.
Train: 2018-08-02T13:07:37.817750: step 6246, loss 0.553158.
Train: 2018-08-02T13:07:37.958346: step 6247, loss 0.520297.
Train: 2018-08-02T13:07:38.114557: step 6248, loss 0.454306.
Train: 2018-08-02T13:07:38.270769: step 6249, loss 0.51991.
Train: 2018-08-02T13:07:38.426982: step 6250, loss 0.547576.
Test: 2018-08-02T13:07:38.879988: step 6250, loss 0.54757.
Train: 2018-08-02T13:07:39.036209: step 6251, loss 0.468456.
Train: 2018-08-02T13:07:39.192399: step 6252, loss 0.560808.
Train: 2018-08-02T13:07:39.348612: step 6253, loss 0.619454.
Train: 2018-08-02T13:07:39.504838: step 6254, loss 0.549136.
Train: 2018-08-02T13:07:39.645418: step 6255, loss 0.504233.
Train: 2018-08-02T13:07:39.801661: step 6256, loss 0.53233.
Train: 2018-08-02T13:07:39.957877: step 6257, loss 0.540633.
Train: 2018-08-02T13:07:40.098436: step 6258, loss 0.509429.
Train: 2018-08-02T13:07:40.254679: step 6259, loss 0.56957.
Train: 2018-08-02T13:07:40.410862: step 6260, loss 0.463809.
Test: 2018-08-02T13:07:40.863907: step 6260, loss 0.552949.
Train: 2018-08-02T13:07:41.020120: step 6261, loss 0.560078.
Train: 2018-08-02T13:07:41.176339: step 6262, loss 0.5714.
Train: 2018-08-02T13:07:41.332556: step 6263, loss 0.503895.
Train: 2018-08-02T13:07:41.473144: step 6264, loss 0.524632.
Train: 2018-08-02T13:07:41.629357: step 6265, loss 0.542984.
Train: 2018-08-02T13:07:41.785571: step 6266, loss 0.522749.
Train: 2018-08-02T13:07:41.941784: step 6267, loss 0.524059.
Train: 2018-08-02T13:07:42.082380: step 6268, loss 0.459272.
Train: 2018-08-02T13:07:42.238590: step 6269, loss 0.584994.
Train: 2018-08-02T13:07:42.394773: step 6270, loss 0.60613.
Test: 2018-08-02T13:07:42.863413: step 6270, loss 0.550005.
Train: 2018-08-02T13:07:43.019627: step 6271, loss 0.537513.
Train: 2018-08-02T13:07:43.160218: step 6272, loss 0.509864.
Train: 2018-08-02T13:07:43.316462: step 6273, loss 0.542148.
Train: 2018-08-02T13:07:43.472675: step 6274, loss 0.598567.
Train: 2018-08-02T13:07:43.628889: step 6275, loss 0.525172.
Train: 2018-08-02T13:07:43.769484: step 6276, loss 0.488966.
Train: 2018-08-02T13:07:43.925665: step 6277, loss 0.506805.
Train: 2018-08-02T13:07:44.081912: step 6278, loss 0.565307.
Train: 2018-08-02T13:07:44.238091: step 6279, loss 0.540914.
Train: 2018-08-02T13:07:44.378683: step 6280, loss 0.414262.
Test: 2018-08-02T13:07:44.847324: step 6280, loss 0.547543.
Train: 2018-08-02T13:07:45.003567: step 6281, loss 0.597854.
Train: 2018-08-02T13:07:45.144159: step 6282, loss 0.506602.
Train: 2018-08-02T13:07:45.300342: step 6283, loss 0.433867.
Train: 2018-08-02T13:07:45.456556: step 6284, loss 0.528981.
Train: 2018-08-02T13:07:45.612769: step 6285, loss 0.5293.
Train: 2018-08-02T13:07:45.769013: step 6286, loss 0.415394.
Train: 2018-08-02T13:07:45.909605: step 6287, loss 0.46698.
Train: 2018-08-02T13:07:46.065820: step 6288, loss 0.445877.
Train: 2018-08-02T13:07:46.222002: step 6289, loss 0.583919.
Train: 2018-08-02T13:07:46.378246: step 6290, loss 0.446682.
Test: 2018-08-02T13:07:46.831234: step 6290, loss 0.54713.
Train: 2018-08-02T13:07:46.987477: step 6291, loss 0.46834.
Train: 2018-08-02T13:07:47.143661: step 6292, loss 0.543694.
Train: 2018-08-02T13:07:47.284283: step 6293, loss 0.536634.
Train: 2018-08-02T13:07:47.440496: step 6294, loss 0.460883.
Train: 2018-08-02T13:07:47.596679: step 6295, loss 0.556602.
Train: 2018-08-02T13:07:47.737302: step 6296, loss 0.504569.
Train: 2018-08-02T13:07:47.893485: step 6297, loss 0.449613.
Train: 2018-08-02T13:07:48.049699: step 6298, loss 0.541129.
Train: 2018-08-02T13:07:48.205946: step 6299, loss 0.533795.
Train: 2018-08-02T13:07:48.346535: step 6300, loss 0.471628.
Test: 2018-08-02T13:07:48.815144: step 6300, loss 0.548801.
Train: 2018-08-02T13:07:49.471275: step 6301, loss 0.498514.
Train: 2018-08-02T13:07:49.611833: step 6302, loss 0.558346.
Train: 2018-08-02T13:07:49.768076: step 6303, loss 0.554597.
Train: 2018-08-02T13:07:49.924290: step 6304, loss 0.46543.
Train: 2018-08-02T13:07:50.080473: step 6305, loss 0.556948.
Train: 2018-08-02T13:07:50.221095: step 6306, loss 0.484926.
Train: 2018-08-02T13:07:50.377309: step 6307, loss 0.492446.
Train: 2018-08-02T13:07:50.533526: step 6308, loss 0.492936.
Train: 2018-08-02T13:07:50.689706: step 6309, loss 0.635112.
Train: 2018-08-02T13:07:50.830331: step 6310, loss 0.499459.
Test: 2018-08-02T13:07:51.298939: step 6310, loss 0.551414.
Train: 2018-08-02T13:07:51.439529: step 6311, loss 0.492585.
Train: 2018-08-02T13:07:51.595774: step 6312, loss 0.574774.
Train: 2018-08-02T13:07:51.751988: step 6313, loss 0.549426.
Train: 2018-08-02T13:07:51.908199: step 6314, loss 0.548191.
Train: 2018-08-02T13:07:52.048793: step 6315, loss 0.483538.
Train: 2018-08-02T13:07:52.205009: step 6316, loss 0.452966.
Train: 2018-08-02T13:07:52.361188: step 6317, loss 0.504495.
Train: 2018-08-02T13:07:52.517403: step 6318, loss 0.578847.
Train: 2018-08-02T13:07:52.673646: step 6319, loss 0.477754.
Train: 2018-08-02T13:07:52.814208: step 6320, loss 0.540653.
Test: 2018-08-02T13:07:53.278779: step 6320, loss 0.548289.
Train: 2018-08-02T13:07:53.419371: step 6321, loss 0.488446.
Train: 2018-08-02T13:07:53.575584: step 6322, loss 0.536645.
Train: 2018-08-02T13:07:53.731768: step 6323, loss 0.500333.
Train: 2018-08-02T13:07:53.888011: step 6324, loss 0.456385.
Train: 2018-08-02T13:07:54.028574: step 6325, loss 0.470719.
Train: 2018-08-02T13:07:54.184816: step 6326, loss 0.512932.
Train: 2018-08-02T13:07:54.341032: step 6327, loss 0.566536.
Train: 2018-08-02T13:07:54.481622: step 6328, loss 0.448874.
Train: 2018-08-02T13:07:54.637806: step 6329, loss 0.49182.
Train: 2018-08-02T13:07:54.794049: step 6330, loss 0.538822.
Test: 2018-08-02T13:07:55.262660: step 6330, loss 0.553711.
Train: 2018-08-02T13:07:55.403281: step 6331, loss 0.470147.
Train: 2018-08-02T13:07:55.559464: step 6332, loss 0.499221.
Train: 2018-08-02T13:07:55.715710: step 6333, loss 0.513792.
Train: 2018-08-02T13:07:55.856300: step 6334, loss 0.574119.
Train: 2018-08-02T13:07:56.012486: step 6335, loss 0.476231.
Train: 2018-08-02T13:07:56.168727: step 6336, loss 0.517581.
Train: 2018-08-02T13:07:56.324941: step 6337, loss 0.550026.
Train: 2018-08-02T13:07:56.465534: step 6338, loss 0.53106.
Train: 2018-08-02T13:07:56.621746: step 6339, loss 0.524291.
Train: 2018-08-02T13:07:56.777930: step 6340, loss 0.472055.
Test: 2018-08-02T13:07:57.230948: step 6340, loss 0.54792.
Train: 2018-08-02T13:07:57.387162: step 6341, loss 0.482477.
Train: 2018-08-02T13:07:57.496541: step 6342, loss 0.500564.
Train: 2018-08-02T13:07:57.652754: step 6343, loss 0.434606.
Train: 2018-08-02T13:07:57.808963: step 6344, loss 0.501007.
Train: 2018-08-02T13:07:57.949560: step 6345, loss 0.564473.
Train: 2018-08-02T13:07:58.105773: step 6346, loss 0.513275.
Train: 2018-08-02T13:07:58.261988: step 6347, loss 0.498814.
Train: 2018-08-02T13:07:58.418200: step 6348, loss 0.517933.
Train: 2018-08-02T13:07:58.574383: step 6349, loss 0.476509.
Train: 2018-08-02T13:07:58.714975: step 6350, loss 0.493477.
Test: 2018-08-02T13:07:59.183624: step 6350, loss 0.548285.
Train: 2018-08-02T13:07:59.324241: step 6351, loss 0.464142.
Train: 2018-08-02T13:07:59.480452: step 6352, loss 0.584261.
Train: 2018-08-02T13:07:59.636635: step 6353, loss 0.521207.
Train: 2018-08-02T13:07:59.792882: step 6354, loss 0.611461.
Train: 2018-08-02T13:07:59.933471: step 6355, loss 0.539176.
Train: 2018-08-02T13:08:00.089653: step 6356, loss 0.502363.
Train: 2018-08-02T13:08:00.245868: step 6357, loss 0.495985.
Train: 2018-08-02T13:08:00.386490: step 6358, loss 0.598457.
Train: 2018-08-02T13:08:00.542674: step 6359, loss 0.540419.
Train: 2018-08-02T13:08:00.698915: step 6360, loss 0.563958.
Test: 2018-08-02T13:08:01.151906: step 6360, loss 0.556197.
Train: 2018-08-02T13:08:01.308148: step 6361, loss 0.51087.
Train: 2018-08-02T13:08:01.464333: step 6362, loss 0.620638.
Train: 2018-08-02T13:08:01.604953: step 6363, loss 0.583248.
Train: 2018-08-02T13:08:01.761170: step 6364, loss 0.495685.
Train: 2018-08-02T13:08:01.917381: step 6365, loss 0.522037.
Train: 2018-08-02T13:08:02.073594: step 6366, loss 0.488348.
Train: 2018-08-02T13:08:02.214157: step 6367, loss 0.499399.
Train: 2018-08-02T13:08:02.370399: step 6368, loss 0.498975.
Train: 2018-08-02T13:08:02.526613: step 6369, loss 0.506313.
Train: 2018-08-02T13:08:02.682826: step 6370, loss 0.508202.
Test: 2018-08-02T13:08:03.135816: step 6370, loss 0.548186.
Train: 2018-08-02T13:08:03.292059: step 6371, loss 0.519486.
Train: 2018-08-02T13:08:03.432651: step 6372, loss 0.492556.
Train: 2018-08-02T13:08:03.588867: step 6373, loss 0.482527.
Train: 2018-08-02T13:08:03.745078: step 6374, loss 0.562687.
Train: 2018-08-02T13:08:03.901293: step 6375, loss 0.498763.
Train: 2018-08-02T13:08:04.041883: step 6376, loss 0.61575.
Train: 2018-08-02T13:08:04.198099: step 6377, loss 0.521804.
Train: 2018-08-02T13:08:04.354310: step 6378, loss 0.486609.
Train: 2018-08-02T13:08:04.494902: step 6379, loss 0.475258.
Train: 2018-08-02T13:08:04.651086: step 6380, loss 0.569797.
Test: 2018-08-02T13:08:05.119756: step 6380, loss 0.551216.
Train: 2018-08-02T13:08:05.260351: step 6381, loss 0.488701.
Train: 2018-08-02T13:08:05.416561: step 6382, loss 0.462489.
Train: 2018-08-02T13:08:05.572778: step 6383, loss 0.498423.
Train: 2018-08-02T13:08:05.713369: step 6384, loss 0.57386.
Train: 2018-08-02T13:08:05.869580: step 6385, loss 0.502368.
Train: 2018-08-02T13:08:06.025794: step 6386, loss 0.501395.
Train: 2018-08-02T13:08:06.181977: step 6387, loss 0.518153.
Train: 2018-08-02T13:08:06.322602: step 6388, loss 0.47304.
Train: 2018-08-02T13:08:06.478813: step 6389, loss 0.480616.
Train: 2018-08-02T13:08:06.634997: step 6390, loss 0.572392.
Test: 2018-08-02T13:08:07.088022: step 6390, loss 0.548084.
Train: 2018-08-02T13:08:07.244228: step 6391, loss 0.544846.
Train: 2018-08-02T13:08:07.400471: step 6392, loss 0.547002.
Train: 2018-08-02T13:08:07.541064: step 6393, loss 0.495303.
Train: 2018-08-02T13:08:07.697280: step 6394, loss 0.515627.
Train: 2018-08-02T13:08:07.853491: step 6395, loss 0.445106.
Train: 2018-08-02T13:08:08.009705: step 6396, loss 0.512466.
Train: 2018-08-02T13:08:08.150296: step 6397, loss 0.608349.
Train: 2018-08-02T13:08:08.306510: step 6398, loss 0.521209.
Train: 2018-08-02T13:08:08.462725: step 6399, loss 0.618737.
Train: 2018-08-02T13:08:08.618936: step 6400, loss 0.525695.
Test: 2018-08-02T13:08:09.071954: step 6400, loss 0.550503.
Train: 2018-08-02T13:08:09.728052: step 6401, loss 0.501268.
Train: 2018-08-02T13:08:09.884265: step 6402, loss 0.484122.
Train: 2018-08-02T13:08:10.024857: step 6403, loss 0.574653.
Train: 2018-08-02T13:08:10.181066: step 6404, loss 0.522441.
Train: 2018-08-02T13:08:10.337287: step 6405, loss 0.51205.
Train: 2018-08-02T13:08:10.493497: step 6406, loss 0.492871.
Train: 2018-08-02T13:08:10.634089: step 6407, loss 0.527825.
Train: 2018-08-02T13:08:10.790305: step 6408, loss 0.520288.
Train: 2018-08-02T13:08:10.946516: step 6409, loss 0.482912.
Train: 2018-08-02T13:08:11.102700: step 6410, loss 0.536072.
Test: 2018-08-02T13:08:11.555719: step 6410, loss 0.548528.
Train: 2018-08-02T13:08:11.711964: step 6411, loss 0.45957.
Train: 2018-08-02T13:08:11.852525: step 6412, loss 0.496009.
Train: 2018-08-02T13:08:12.008771: step 6413, loss 0.55069.
Train: 2018-08-02T13:08:12.164950: step 6414, loss 0.530098.
Train: 2018-08-02T13:08:12.321194: step 6415, loss 0.593076.
Train: 2018-08-02T13:08:12.477378: step 6416, loss 0.567788.
Train: 2018-08-02T13:08:12.633591: step 6417, loss 0.494578.
Train: 2018-08-02T13:08:12.774213: step 6418, loss 0.518655.
Train: 2018-08-02T13:08:12.930427: step 6419, loss 0.517995.
Train: 2018-08-02T13:08:13.086642: step 6420, loss 0.469923.
Test: 2018-08-02T13:08:13.539654: step 6420, loss 0.558964.
Train: 2018-08-02T13:08:13.695873: step 6421, loss 0.526013.
Train: 2018-08-02T13:08:13.852086: step 6422, loss 0.539089.
Train: 2018-08-02T13:08:13.992678: step 6423, loss 0.58243.
Train: 2018-08-02T13:08:14.148891: step 6424, loss 0.496529.
Train: 2018-08-02T13:08:14.305113: step 6425, loss 0.492123.
Train: 2018-08-02T13:08:14.461320: step 6426, loss 0.574056.
Train: 2018-08-02T13:08:14.601910: step 6427, loss 0.463661.
Train: 2018-08-02T13:08:14.758099: step 6428, loss 0.543274.
Train: 2018-08-02T13:08:14.914337: step 6429, loss 0.58364.
Train: 2018-08-02T13:08:15.070551: step 6430, loss 0.594827.
Test: 2018-08-02T13:08:15.523570: step 6430, loss 0.55014.
Train: 2018-08-02T13:08:15.679805: step 6431, loss 0.590865.
Train: 2018-08-02T13:08:15.820375: step 6432, loss 0.541659.
Train: 2018-08-02T13:08:15.976589: step 6433, loss 0.549166.
Train: 2018-08-02T13:08:16.132803: step 6434, loss 0.479053.
Train: 2018-08-02T13:08:16.289015: step 6435, loss 0.56767.
Train: 2018-08-02T13:08:16.445198: step 6436, loss 0.541149.
Train: 2018-08-02T13:08:16.585823: step 6437, loss 0.519014.
Train: 2018-08-02T13:08:16.742034: step 6438, loss 0.546318.
Train: 2018-08-02T13:08:16.898218: step 6439, loss 0.55287.
Train: 2018-08-02T13:08:17.038809: step 6440, loss 0.52537.
Test: 2018-08-02T13:08:17.507496: step 6440, loss 0.559246.
Train: 2018-08-02T13:08:17.648072: step 6441, loss 0.541614.
Train: 2018-08-02T13:08:17.804256: step 6442, loss 0.57267.
Train: 2018-08-02T13:08:17.960469: step 6443, loss 0.564318.
Train: 2018-08-02T13:08:18.116715: step 6444, loss 0.533416.
Train: 2018-08-02T13:08:18.257305: step 6445, loss 0.52031.
Train: 2018-08-02T13:08:18.413517: step 6446, loss 0.582503.
Train: 2018-08-02T13:08:18.569739: step 6447, loss 0.597347.
Train: 2018-08-02T13:08:18.725915: step 6448, loss 0.534896.
Train: 2018-08-02T13:08:18.866536: step 6449, loss 0.501385.
Train: 2018-08-02T13:08:19.022750: step 6450, loss 0.460226.
Test: 2018-08-02T13:08:19.475740: step 6450, loss 0.556994.
Train: 2018-08-02T13:08:19.631983: step 6451, loss 0.561066.
Train: 2018-08-02T13:08:19.788196: step 6452, loss 0.499105.
Train: 2018-08-02T13:08:19.944409: step 6453, loss 0.498088.
Train: 2018-08-02T13:08:20.085004: step 6454, loss 0.512472.
Train: 2018-08-02T13:08:20.241215: step 6455, loss 0.527478.
Train: 2018-08-02T13:08:20.397429: step 6456, loss 0.60137.
Train: 2018-08-02T13:08:20.553611: step 6457, loss 0.503385.
Train: 2018-08-02T13:08:20.709857: step 6458, loss 0.516937.
Train: 2018-08-02T13:08:20.850447: step 6459, loss 0.505644.
Train: 2018-08-02T13:08:21.006660: step 6460, loss 0.460662.
Test: 2018-08-02T13:08:21.475271: step 6460, loss 0.553437.
Train: 2018-08-02T13:08:21.615894: step 6461, loss 0.550385.
Train: 2018-08-02T13:08:21.772106: step 6462, loss 0.503686.
Train: 2018-08-02T13:08:21.928290: step 6463, loss 0.49185.
Train: 2018-08-02T13:08:22.068912: step 6464, loss 0.410279.
Train: 2018-08-02T13:08:22.225094: step 6465, loss 0.497619.
Train: 2018-08-02T13:08:22.381339: step 6466, loss 0.608808.
Train: 2018-08-02T13:08:22.537552: step 6467, loss 0.509434.
Train: 2018-08-02T13:08:22.693765: step 6468, loss 0.509156.
Train: 2018-08-02T13:08:22.834360: step 6469, loss 0.481712.
Train: 2018-08-02T13:08:22.990566: step 6470, loss 0.544207.
Test: 2018-08-02T13:08:23.459182: step 6470, loss 0.551316.
Train: 2018-08-02T13:08:23.615418: step 6471, loss 0.565187.
Train: 2018-08-02T13:08:23.771634: step 6472, loss 0.566122.
Train: 2018-08-02T13:08:23.912230: step 6473, loss 0.45397.
Train: 2018-08-02T13:08:24.068444: step 6474, loss 0.582186.
Train: 2018-08-02T13:08:24.224657: step 6475, loss 0.486508.
Train: 2018-08-02T13:08:24.380840: step 6476, loss 0.490813.
Train: 2018-08-02T13:08:24.537084: step 6477, loss 0.534951.
Train: 2018-08-02T13:08:24.693268: step 6478, loss 0.528704.
Train: 2018-08-02T13:08:24.833891: step 6479, loss 0.425626.
Train: 2018-08-02T13:08:24.990102: step 6480, loss 0.435138.
Test: 2018-08-02T13:08:25.443130: step 6480, loss 0.548984.
Train: 2018-08-02T13:08:25.599335: step 6481, loss 0.569618.
Train: 2018-08-02T13:08:25.755548: step 6482, loss 0.503853.
Train: 2018-08-02T13:08:25.911731: step 6483, loss 0.53883.
Train: 2018-08-02T13:08:26.067947: step 6484, loss 0.524464.
Train: 2018-08-02T13:08:26.208538: step 6485, loss 0.484623.
Train: 2018-08-02T13:08:26.364781: step 6486, loss 0.591991.
Train: 2018-08-02T13:08:26.520994: step 6487, loss 0.532049.
Train: 2018-08-02T13:08:26.677203: step 6488, loss 0.511257.
Train: 2018-08-02T13:08:26.817800: step 6489, loss 0.58056.
Train: 2018-08-02T13:08:26.974013: step 6490, loss 0.455472.
Test: 2018-08-02T13:08:27.442623: step 6490, loss 0.551569.
Train: 2018-08-02T13:08:27.583247: step 6491, loss 0.575126.
Train: 2018-08-02T13:08:27.739430: step 6492, loss 0.478539.
Train: 2018-08-02T13:08:27.848778: step 6493, loss 0.429649.
Train: 2018-08-02T13:08:28.004991: step 6494, loss 0.559496.
Train: 2018-08-02T13:08:28.161235: step 6495, loss 0.471509.
Train: 2018-08-02T13:08:28.317418: step 6496, loss 0.590903.
Train: 2018-08-02T13:08:28.473661: step 6497, loss 0.498615.
Train: 2018-08-02T13:08:28.614254: step 6498, loss 0.507193.
Train: 2018-08-02T13:08:28.786090: step 6499, loss 0.446072.
Train: 2018-08-02T13:08:28.926681: step 6500, loss 0.454461.
Test: 2018-08-02T13:08:29.395317: step 6500, loss 0.54929.
Train: 2018-08-02T13:08:30.020174: step 6501, loss 0.520276.
Train: 2018-08-02T13:08:30.176389: step 6502, loss 0.474218.
Train: 2018-08-02T13:08:30.332572: step 6503, loss 0.551639.
Train: 2018-08-02T13:08:30.473194: step 6504, loss 0.58384.
Train: 2018-08-02T13:08:30.629408: step 6505, loss 0.551576.
Train: 2018-08-02T13:08:30.785590: step 6506, loss 0.523587.
Train: 2018-08-02T13:08:30.941836: step 6507, loss 0.587609.
Train: 2018-08-02T13:08:31.098047: step 6508, loss 0.49756.
Train: 2018-08-02T13:08:31.254261: step 6509, loss 0.501727.
Train: 2018-08-02T13:08:31.394852: step 6510, loss 0.49888.
Test: 2018-08-02T13:08:31.863488: step 6510, loss 0.557215.
Train: 2018-08-02T13:08:32.004080: step 6511, loss 0.414131.
Train: 2018-08-02T13:08:32.160299: step 6512, loss 0.610601.
Train: 2018-08-02T13:08:32.316482: step 6513, loss 0.468537.
Train: 2018-08-02T13:08:32.472696: step 6514, loss 0.541045.
Train: 2018-08-02T13:08:32.613288: step 6515, loss 0.420668.
Train: 2018-08-02T13:08:32.769531: step 6516, loss 0.677759.
Train: 2018-08-02T13:08:32.925744: step 6517, loss 0.483948.
Train: 2018-08-02T13:08:33.128791: step 6518, loss 0.500577.
Train: 2018-08-02T13:08:33.285036: step 6519, loss 0.580334.
Train: 2018-08-02T13:08:33.441249: step 6520, loss 0.511174.
Test: 2018-08-02T13:08:33.894244: step 6520, loss 0.547714.
Train: 2018-08-02T13:08:34.050480: step 6521, loss 0.542469.
Train: 2018-08-02T13:08:34.191042: step 6522, loss 0.527832.
Train: 2018-08-02T13:08:34.347256: step 6523, loss 0.593988.
Train: 2018-08-02T13:08:34.503495: step 6524, loss 0.558198.
Train: 2018-08-02T13:08:34.659684: step 6525, loss 0.481682.
Train: 2018-08-02T13:08:34.815927: step 6526, loss 0.559841.
Train: 2018-08-02T13:08:34.972140: step 6527, loss 0.558731.
Train: 2018-08-02T13:08:35.128324: step 6528, loss 0.539807.
Train: 2018-08-02T13:08:35.268916: step 6529, loss 0.490295.
Train: 2018-08-02T13:08:35.425159: step 6530, loss 0.486822.
Test: 2018-08-02T13:08:35.893770: step 6530, loss 0.547765.
Train: 2018-08-02T13:08:36.034391: step 6531, loss 0.422077.
Train: 2018-08-02T13:08:36.190604: step 6532, loss 0.549196.
Train: 2018-08-02T13:08:36.346787: step 6533, loss 0.557811.
Train: 2018-08-02T13:08:36.503032: step 6534, loss 0.494448.
Train: 2018-08-02T13:08:36.643623: step 6535, loss 0.458701.
Train: 2018-08-02T13:08:36.799837: step 6536, loss 0.533209.
Train: 2018-08-02T13:08:36.956053: step 6537, loss 0.48821.
Train: 2018-08-02T13:08:37.112235: step 6538, loss 0.540849.
Train: 2018-08-02T13:08:37.252857: step 6539, loss 0.522672.
Train: 2018-08-02T13:08:37.409070: step 6540, loss 0.564745.
Test: 2018-08-02T13:08:37.877679: step 6540, loss 0.552106.
Train: 2018-08-02T13:08:38.033919: step 6541, loss 0.514337.
Train: 2018-08-02T13:08:38.174530: step 6542, loss 0.515398.
Train: 2018-08-02T13:08:38.330731: step 6543, loss 0.574177.
Train: 2018-08-02T13:08:38.486912: step 6544, loss 0.534893.
Train: 2018-08-02T13:08:38.643156: step 6545, loss 0.51039.
Train: 2018-08-02T13:08:38.783748: step 6546, loss 0.574914.
Train: 2018-08-02T13:08:38.939961: step 6547, loss 0.489113.
Train: 2018-08-02T13:08:39.096174: step 6548, loss 0.493273.
Train: 2018-08-02T13:08:39.252389: step 6549, loss 0.439696.
Train: 2018-08-02T13:08:39.408602: step 6550, loss 0.493819.
Test: 2018-08-02T13:08:39.861642: step 6550, loss 0.547083.
Train: 2018-08-02T13:08:40.017834: step 6551, loss 0.630331.
Train: 2018-08-02T13:08:40.174017: step 6552, loss 0.480639.
Train: 2018-08-02T13:08:40.314610: step 6553, loss 0.591751.
Train: 2018-08-02T13:08:40.470853: step 6554, loss 0.398996.
Train: 2018-08-02T13:08:40.627066: step 6555, loss 0.502605.
Train: 2018-08-02T13:08:40.783280: step 6556, loss 0.506327.
Train: 2018-08-02T13:08:40.923871: step 6557, loss 0.558593.
Train: 2018-08-02T13:08:41.080087: step 6558, loss 0.458997.
Train: 2018-08-02T13:08:41.236269: step 6559, loss 0.465153.
Train: 2018-08-02T13:08:41.392482: step 6560, loss 0.546861.
Test: 2018-08-02T13:08:41.845501: step 6560, loss 0.548822.
Train: 2018-08-02T13:08:42.001744: step 6561, loss 0.5031.
Train: 2018-08-02T13:08:42.173550: step 6562, loss 0.556481.
Train: 2018-08-02T13:08:42.314173: step 6563, loss 0.504524.
Train: 2018-08-02T13:08:42.470386: step 6564, loss 0.57101.
Train: 2018-08-02T13:08:42.626598: step 6565, loss 0.49487.
Train: 2018-08-02T13:08:42.782811: step 6566, loss 0.613584.
Train: 2018-08-02T13:08:42.939024: step 6567, loss 0.529576.
Train: 2018-08-02T13:08:43.079619: step 6568, loss 0.61382.
Train: 2018-08-02T13:08:43.235830: step 6569, loss 0.526576.
Train: 2018-08-02T13:08:43.392043: step 6570, loss 0.505183.
Test: 2018-08-02T13:08:43.845081: step 6570, loss 0.559451.
Train: 2018-08-02T13:08:44.048110: step 6571, loss 0.609085.
Train: 2018-08-02T13:08:44.204349: step 6572, loss 0.501485.
Train: 2018-08-02T13:08:44.360567: step 6573, loss 0.518866.
Train: 2018-08-02T13:08:44.516749: step 6574, loss 0.530098.
Train: 2018-08-02T13:08:44.657372: step 6575, loss 0.527297.
Train: 2018-08-02T13:08:44.813586: step 6576, loss 0.521698.
Train: 2018-08-02T13:08:44.969799: step 6577, loss 0.564209.
Train: 2018-08-02T13:08:45.125982: step 6578, loss 0.500125.
Train: 2018-08-02T13:08:45.282229: step 6579, loss 0.585558.
Train: 2018-08-02T13:08:45.422818: step 6580, loss 0.547651.
Test: 2018-08-02T13:08:45.891453: step 6580, loss 0.5518.
Train: 2018-08-02T13:08:46.047641: step 6581, loss 0.519159.
Train: 2018-08-02T13:08:46.188233: step 6582, loss 0.558464.
Train: 2018-08-02T13:08:46.344477: step 6583, loss 0.47986.
Train: 2018-08-02T13:08:46.500691: step 6584, loss 0.529251.
Train: 2018-08-02T13:08:46.656874: step 6585, loss 0.484742.
Train: 2018-08-02T13:08:46.797496: step 6586, loss 0.520369.
Train: 2018-08-02T13:08:46.953705: step 6587, loss 0.523514.
Train: 2018-08-02T13:08:47.109923: step 6588, loss 0.506103.
Train: 2018-08-02T13:08:47.266136: step 6589, loss 0.496756.
Train: 2018-08-02T13:08:47.422336: step 6590, loss 0.462677.
Test: 2018-08-02T13:08:47.875339: step 6590, loss 0.54972.
Train: 2018-08-02T13:08:48.031576: step 6591, loss 0.515774.
Train: 2018-08-02T13:08:48.172174: step 6592, loss 0.547382.
Train: 2018-08-02T13:08:48.328389: step 6593, loss 0.479241.
Train: 2018-08-02T13:08:48.484601: step 6594, loss 0.488623.
Train: 2018-08-02T13:08:48.640814: step 6595, loss 0.503506.
Train: 2018-08-02T13:08:48.796998: step 6596, loss 0.549609.
Train: 2018-08-02T13:08:48.937619: step 6597, loss 0.559493.
Train: 2018-08-02T13:08:49.093833: step 6598, loss 0.480061.
Train: 2018-08-02T13:08:49.250046: step 6599, loss 0.566277.
Train: 2018-08-02T13:08:49.406262: step 6600, loss 0.452286.
Test: 2018-08-02T13:08:49.859251: step 6600, loss 0.552201.
Train: 2018-08-02T13:08:50.546588: step 6601, loss 0.485145.
Train: 2018-08-02T13:08:50.702802: step 6602, loss 0.478975.
Train: 2018-08-02T13:08:50.843424: step 6603, loss 0.516193.
Train: 2018-08-02T13:08:50.999637: step 6604, loss 0.481306.
Train: 2018-08-02T13:08:51.155850: step 6605, loss 0.490523.
Train: 2018-08-02T13:08:51.312064: step 6606, loss 0.59553.
Train: 2018-08-02T13:08:51.468247: step 6607, loss 0.549502.
Train: 2018-08-02T13:08:51.608840: step 6608, loss 0.510515.
Train: 2018-08-02T13:08:51.765082: step 6609, loss 0.531854.
Train: 2018-08-02T13:08:51.921266: step 6610, loss 0.49788.
Test: 2018-08-02T13:08:52.374285: step 6610, loss 0.556223.
Train: 2018-08-02T13:08:52.530529: step 6611, loss 0.537925.
Train: 2018-08-02T13:08:52.686742: step 6612, loss 0.508219.
Train: 2018-08-02T13:08:52.827305: step 6613, loss 0.498333.
Train: 2018-08-02T13:08:52.983517: step 6614, loss 0.443872.
Train: 2018-08-02T13:08:53.139761: step 6615, loss 0.551294.
Train: 2018-08-02T13:08:53.295968: step 6616, loss 0.514839.
Train: 2018-08-02T13:08:53.436568: step 6617, loss 0.513918.
Train: 2018-08-02T13:08:53.592779: step 6618, loss 0.540223.
Train: 2018-08-02T13:08:53.748993: step 6619, loss 0.456498.
Train: 2018-08-02T13:08:53.905176: step 6620, loss 0.574679.
Test: 2018-08-02T13:08:54.358195: step 6620, loss 0.552532.
Train: 2018-08-02T13:08:54.514411: step 6621, loss 0.519991.
Train: 2018-08-02T13:08:54.670623: step 6622, loss 0.579415.
Train: 2018-08-02T13:08:54.811215: step 6623, loss 0.482725.
Train: 2018-08-02T13:08:54.967458: step 6624, loss 0.541298.
Train: 2018-08-02T13:08:55.123671: step 6625, loss 0.543616.
Train: 2018-08-02T13:08:55.279855: step 6626, loss 0.467117.
Train: 2018-08-02T13:08:55.436098: step 6627, loss 0.493892.
Train: 2018-08-02T13:08:55.592312: step 6628, loss 0.451555.
Train: 2018-08-02T13:08:55.732904: step 6629, loss 0.505017.
Train: 2018-08-02T13:08:55.889118: step 6630, loss 0.410809.
Test: 2018-08-02T13:08:56.342105: step 6630, loss 0.552628.
Train: 2018-08-02T13:08:56.498350: step 6631, loss 0.470152.
Train: 2018-08-02T13:08:56.654563: step 6632, loss 0.468994.
Train: 2018-08-02T13:08:56.810776: step 6633, loss 0.466837.
Train: 2018-08-02T13:08:56.951370: step 6634, loss 0.461303.
Train: 2018-08-02T13:08:57.107582: step 6635, loss 0.472963.
Train: 2018-08-02T13:08:57.263795: step 6636, loss 0.513197.
Train: 2018-08-02T13:08:57.420008: step 6637, loss 0.481875.
Train: 2018-08-02T13:08:57.560605: step 6638, loss 0.600932.
Train: 2018-08-02T13:08:57.716784: step 6639, loss 0.56054.
Train: 2018-08-02T13:08:57.873027: step 6640, loss 0.516615.
Test: 2018-08-02T13:08:58.326018: step 6640, loss 0.556374.
Train: 2018-08-02T13:08:58.482260: step 6641, loss 0.564778.
Train: 2018-08-02T13:08:58.638468: step 6642, loss 0.614432.
Train: 2018-08-02T13:08:58.794689: step 6643, loss 0.557785.
Train: 2018-08-02T13:08:58.904005: step 6644, loss 0.584365.
Train: 2018-08-02T13:08:59.044628: step 6645, loss 0.545765.
Train: 2018-08-02T13:08:59.200843: step 6646, loss 0.598509.
Train: 2018-08-02T13:08:59.357055: step 6647, loss 0.557271.
Train: 2018-08-02T13:08:59.513268: step 6648, loss 0.457867.
Train: 2018-08-02T13:08:59.653829: step 6649, loss 0.565029.
Train: 2018-08-02T13:08:59.810073: step 6650, loss 0.524033.
Test: 2018-08-02T13:09:00.278684: step 6650, loss 0.548076.
Train: 2018-08-02T13:09:00.434927: step 6651, loss 0.535439.
Train: 2018-08-02T13:09:00.575521: step 6652, loss 0.527604.
Train: 2018-08-02T13:09:00.731733: step 6653, loss 0.583131.
Train: 2018-08-02T13:09:00.887917: step 6654, loss 0.639029.
Train: 2018-08-02T13:09:01.044159: step 6655, loss 0.499614.
Train: 2018-08-02T13:09:01.184752: step 6656, loss 0.596134.
Train: 2018-08-02T13:09:01.340968: step 6657, loss 0.526372.
Train: 2018-08-02T13:09:01.497178: step 6658, loss 0.473767.
Train: 2018-08-02T13:09:01.653393: step 6659, loss 0.52886.
Train: 2018-08-02T13:09:01.793954: step 6660, loss 0.54393.
Test: 2018-08-02T13:09:02.262594: step 6660, loss 0.547945.
Train: 2018-08-02T13:09:02.403187: step 6661, loss 0.610585.
Train: 2018-08-02T13:09:02.559400: step 6662, loss 0.623396.
Train: 2018-08-02T13:09:02.715644: step 6663, loss 0.482687.
Train: 2018-08-02T13:09:02.871827: step 6664, loss 0.509773.
Train: 2018-08-02T13:09:03.028039: step 6665, loss 0.554241.
Train: 2018-08-02T13:09:03.168662: step 6666, loss 0.541431.
Train: 2018-08-02T13:09:03.324878: step 6667, loss 0.560772.
Train: 2018-08-02T13:09:03.481058: step 6668, loss 0.512512.
Train: 2018-08-02T13:09:03.637273: step 6669, loss 0.529697.
Train: 2018-08-02T13:09:03.777897: step 6670, loss 0.551516.
Test: 2018-08-02T13:09:04.246530: step 6670, loss 0.549345.
Train: 2018-08-02T13:09:04.402749: step 6671, loss 0.54104.
Train: 2018-08-02T13:09:04.543340: step 6672, loss 0.517154.
Train: 2018-08-02T13:09:04.699556: step 6673, loss 0.618376.
Train: 2018-08-02T13:09:04.855738: step 6674, loss 0.540529.
Train: 2018-08-02T13:09:05.011951: step 6675, loss 0.558585.
Train: 2018-08-02T13:09:05.152573: step 6676, loss 0.51794.
Train: 2018-08-02T13:09:05.308789: step 6677, loss 0.513224.
Train: 2018-08-02T13:09:05.465002: step 6678, loss 0.590878.
Train: 2018-08-02T13:09:05.621212: step 6679, loss 0.554992.
Train: 2018-08-02T13:09:05.777426: step 6680, loss 0.545201.
Test: 2018-08-02T13:09:06.230416: step 6680, loss 0.555699.
Train: 2018-08-02T13:09:06.386658: step 6681, loss 0.543373.
Train: 2018-08-02T13:09:06.527221: step 6682, loss 0.467854.
Train: 2018-08-02T13:09:06.683464: step 6683, loss 0.577809.
Train: 2018-08-02T13:09:06.839678: step 6684, loss 0.489018.
Train: 2018-08-02T13:09:06.995861: step 6685, loss 0.53337.
Train: 2018-08-02T13:09:07.136452: step 6686, loss 0.589292.
Train: 2018-08-02T13:09:07.292667: step 6687, loss 0.525743.
Train: 2018-08-02T13:09:07.448912: step 6688, loss 0.497371.
Train: 2018-08-02T13:09:07.605118: step 6689, loss 0.544137.
Train: 2018-08-02T13:09:07.761337: step 6690, loss 0.50028.
Test: 2018-08-02T13:09:08.214325: step 6690, loss 0.553395.
Train: 2018-08-02T13:09:08.354947: step 6691, loss 0.509395.
Train: 2018-08-02T13:09:08.511161: step 6692, loss 0.45609.
Train: 2018-08-02T13:09:08.667374: step 6693, loss 0.468211.
Train: 2018-08-02T13:09:08.823573: step 6694, loss 0.532911.
Train: 2018-08-02T13:09:08.979801: step 6695, loss 0.531939.
Train: 2018-08-02T13:09:09.120394: step 6696, loss 0.526877.
Train: 2018-08-02T13:09:09.276610: step 6697, loss 0.522048.
Train: 2018-08-02T13:09:09.432789: step 6698, loss 0.530532.
Train: 2018-08-02T13:09:09.573414: step 6699, loss 0.508524.
Train: 2018-08-02T13:09:09.729626: step 6700, loss 0.562622.
Test: 2018-08-02T13:09:10.198236: step 6700, loss 0.558748.
Train: 2018-08-02T13:09:10.838710: step 6701, loss 0.482173.
Train: 2018-08-02T13:09:10.979333: step 6702, loss 0.481936.
Train: 2018-08-02T13:09:11.135547: step 6703, loss 0.519774.
Train: 2018-08-02T13:09:11.291730: step 6704, loss 0.485587.
Train: 2018-08-02T13:09:11.447976: step 6705, loss 0.536631.
Train: 2018-08-02T13:09:11.588565: step 6706, loss 0.504629.
Train: 2018-08-02T13:09:11.744778: step 6707, loss 0.441417.
Train: 2018-08-02T13:09:11.900992: step 6708, loss 0.499118.
Train: 2018-08-02T13:09:12.057206: step 6709, loss 0.522341.
Train: 2018-08-02T13:09:12.213390: step 6710, loss 0.49453.
Test: 2018-08-02T13:09:12.666433: step 6710, loss 0.548504.
Train: 2018-08-02T13:09:12.822654: step 6711, loss 0.489002.
Train: 2018-08-02T13:09:12.963244: step 6712, loss 0.53199.
Train: 2018-08-02T13:09:13.119451: step 6713, loss 0.518317.
Train: 2018-08-02T13:09:13.275670: step 6714, loss 0.5496.
Train: 2018-08-02T13:09:13.416262: step 6715, loss 0.510371.
Train: 2018-08-02T13:09:13.572478: step 6716, loss 0.544668.
Train: 2018-08-02T13:09:13.728689: step 6717, loss 0.631172.
Train: 2018-08-02T13:09:13.884903: step 6718, loss 0.52376.
Train: 2018-08-02T13:09:14.025495: step 6719, loss 0.452393.
Train: 2018-08-02T13:09:14.181679: step 6720, loss 0.515339.
Test: 2018-08-02T13:09:14.650318: step 6720, loss 0.551809.
Train: 2018-08-02T13:09:14.790941: step 6721, loss 0.550078.
Train: 2018-08-02T13:09:14.947149: step 6722, loss 0.531048.
Train: 2018-08-02T13:09:15.103367: step 6723, loss 0.47762.
Train: 2018-08-02T13:09:15.259550: step 6724, loss 0.46755.
Train: 2018-08-02T13:09:15.400143: step 6725, loss 0.532956.
Train: 2018-08-02T13:09:15.556358: step 6726, loss 0.562205.
Train: 2018-08-02T13:09:15.712569: step 6727, loss 0.477987.
Train: 2018-08-02T13:09:15.868813: step 6728, loss 0.541531.
Train: 2018-08-02T13:09:16.009406: step 6729, loss 0.530236.
Train: 2018-08-02T13:09:16.165596: step 6730, loss 0.473795.
Test: 2018-08-02T13:09:16.634254: step 6730, loss 0.54909.
Train: 2018-08-02T13:09:16.774851: step 6731, loss 0.425487.
Train: 2018-08-02T13:09:16.931064: step 6732, loss 0.494083.
Train: 2018-08-02T13:09:17.087247: step 6733, loss 0.415541.
Train: 2018-08-02T13:09:17.243491: step 6734, loss 0.500201.
Train: 2018-08-02T13:09:17.384084: step 6735, loss 0.513211.
Train: 2018-08-02T13:09:17.540267: step 6736, loss 0.436135.
Train: 2018-08-02T13:09:17.696512: step 6737, loss 0.597935.
Train: 2018-08-02T13:09:17.852717: step 6738, loss 0.560687.
Train: 2018-08-02T13:09:17.993316: step 6739, loss 0.474732.
Train: 2018-08-02T13:09:18.149499: step 6740, loss 0.482691.
Test: 2018-08-02T13:09:18.602518: step 6740, loss 0.552629.
Train: 2018-08-02T13:09:18.758761: step 6741, loss 0.551505.
Train: 2018-08-02T13:09:18.914945: step 6742, loss 0.466409.
Train: 2018-08-02T13:09:19.055567: step 6743, loss 0.514204.
Train: 2018-08-02T13:09:19.211781: step 6744, loss 0.492361.
Train: 2018-08-02T13:09:19.367993: step 6745, loss 0.653975.
Train: 2018-08-02T13:09:19.524207: step 6746, loss 0.483715.
Train: 2018-08-02T13:09:19.680421: step 6747, loss 0.546041.
Train: 2018-08-02T13:09:19.821015: step 6748, loss 0.477547.
Train: 2018-08-02T13:09:19.977196: step 6749, loss 0.467704.
Train: 2018-08-02T13:09:20.133443: step 6750, loss 0.514534.
Test: 2018-08-02T13:09:20.586428: step 6750, loss 0.548446.
Train: 2018-08-02T13:09:20.742642: step 6751, loss 0.621234.
Train: 2018-08-02T13:09:20.898881: step 6752, loss 0.519854.
Train: 2018-08-02T13:09:21.039478: step 6753, loss 0.557622.
Train: 2018-08-02T13:09:21.195661: step 6754, loss 0.521422.
Train: 2018-08-02T13:09:21.351905: step 6755, loss 0.523053.
Train: 2018-08-02T13:09:21.508119: step 6756, loss 0.457347.
Train: 2018-08-02T13:09:21.648710: step 6757, loss 0.610008.
Train: 2018-08-02T13:09:21.804923: step 6758, loss 0.548412.
Train: 2018-08-02T13:09:21.961136: step 6759, loss 0.536713.
Train: 2018-08-02T13:09:22.101698: step 6760, loss 0.484824.
Test: 2018-08-02T13:09:22.570340: step 6760, loss 0.551435.
Train: 2018-08-02T13:09:22.710961: step 6761, loss 0.586985.
Train: 2018-08-02T13:09:22.867177: step 6762, loss 0.475781.
Train: 2018-08-02T13:09:23.023388: step 6763, loss 0.581106.
Train: 2018-08-02T13:09:23.163980: step 6764, loss 0.502615.
Train: 2018-08-02T13:09:23.320196: step 6765, loss 0.555486.
Train: 2018-08-02T13:09:23.476409: step 6766, loss 0.50125.
Train: 2018-08-02T13:09:23.632622: step 6767, loss 0.484638.
Train: 2018-08-02T13:09:23.773212: step 6768, loss 0.487993.
Train: 2018-08-02T13:09:23.929426: step 6769, loss 0.531244.
Train: 2018-08-02T13:09:24.085642: step 6770, loss 0.528463.
Test: 2018-08-02T13:09:24.538635: step 6770, loss 0.548623.
Train: 2018-08-02T13:09:24.694841: step 6771, loss 0.460864.
Train: 2018-08-02T13:09:24.851055: step 6772, loss 0.507499.
Train: 2018-08-02T13:09:24.991677: step 6773, loss 0.534186.
Train: 2018-08-02T13:09:25.147890: step 6774, loss 0.574351.
Train: 2018-08-02T13:09:25.304074: step 6775, loss 0.539282.
Train: 2018-08-02T13:09:25.460317: step 6776, loss 0.588147.
Train: 2018-08-02T13:09:25.600879: step 6777, loss 0.57462.
Train: 2018-08-02T13:09:25.757123: step 6778, loss 0.50715.
Train: 2018-08-02T13:09:25.913337: step 6779, loss 0.539911.
Train: 2018-08-02T13:09:26.069551: step 6780, loss 0.550592.
Test: 2018-08-02T13:09:26.522564: step 6780, loss 0.558209.
Train: 2018-08-02T13:09:26.678784: step 6781, loss 0.521101.
Train: 2018-08-02T13:09:26.819376: step 6782, loss 0.519708.
Train: 2018-08-02T13:09:26.975587: step 6783, loss 0.503742.
Train: 2018-08-02T13:09:27.131801: step 6784, loss 0.500979.
Train: 2018-08-02T13:09:27.272393: step 6785, loss 0.63103.
Train: 2018-08-02T13:09:27.428609: step 6786, loss 0.618769.
Train: 2018-08-02T13:09:27.584819: step 6787, loss 0.57357.
Train: 2018-08-02T13:09:27.741033: step 6788, loss 0.516103.
Train: 2018-08-02T13:09:27.881626: step 6789, loss 0.478263.
Train: 2018-08-02T13:09:28.037838: step 6790, loss 0.491264.
Test: 2018-08-02T13:09:28.490853: step 6790, loss 0.552943.
Train: 2018-08-02T13:09:28.647071: step 6791, loss 0.517117.
Train: 2018-08-02T13:09:28.803254: step 6792, loss 0.478468.
Train: 2018-08-02T13:09:28.943879: step 6793, loss 0.488925.
Train: 2018-08-02T13:09:29.100059: step 6794, loss 0.515577.
Train: 2018-08-02T13:09:29.209439: step 6795, loss 0.632594.
Train: 2018-08-02T13:09:29.365653: step 6796, loss 0.564784.
Train: 2018-08-02T13:09:29.506245: step 6797, loss 0.45249.
Train: 2018-08-02T13:09:29.662458: step 6798, loss 0.57278.
Train: 2018-08-02T13:09:29.818673: step 6799, loss 0.580728.
Train: 2018-08-02T13:09:29.974885: step 6800, loss 0.499977.
Test: 2018-08-02T13:09:30.427877: step 6800, loss 0.552069.
Train: 2018-08-02T13:09:31.037136: step 6801, loss 0.608152.
Train: 2018-08-02T13:09:31.193352: step 6802, loss 0.496298.
Train: 2018-08-02T13:09:31.349564: step 6803, loss 0.503073.
Train: 2018-08-02T13:09:31.490125: step 6804, loss 0.524635.
Train: 2018-08-02T13:09:31.646368: step 6805, loss 0.478026.
Train: 2018-08-02T13:09:31.802582: step 6806, loss 0.47586.
Train: 2018-08-02T13:09:31.958766: step 6807, loss 0.542528.
Train: 2018-08-02T13:09:32.099382: step 6808, loss 0.461751.
Train: 2018-08-02T13:09:32.255600: step 6809, loss 0.498908.
Train: 2018-08-02T13:09:32.411814: step 6810, loss 0.537243.
Test: 2018-08-02T13:09:32.864828: step 6810, loss 0.549547.
Train: 2018-08-02T13:09:33.021017: step 6811, loss 0.54887.
Train: 2018-08-02T13:09:33.161639: step 6812, loss 0.409947.
Train: 2018-08-02T13:09:33.317823: step 6813, loss 0.629678.
Train: 2018-08-02T13:09:33.474036: step 6814, loss 0.516906.
Train: 2018-08-02T13:09:33.614660: step 6815, loss 0.534515.
Train: 2018-08-02T13:09:33.770871: step 6816, loss 0.551017.
Train: 2018-08-02T13:09:33.927054: step 6817, loss 0.583176.
Train: 2018-08-02T13:09:34.083298: step 6818, loss 0.564951.
Train: 2018-08-02T13:09:34.223893: step 6819, loss 0.517311.
Train: 2018-08-02T13:09:34.380104: step 6820, loss 0.543456.
Test: 2018-08-02T13:09:34.833117: step 6820, loss 0.554972.
Train: 2018-08-02T13:09:34.989335: step 6821, loss 0.514286.
Train: 2018-08-02T13:09:35.145549: step 6822, loss 0.52042.
Train: 2018-08-02T13:09:35.301765: step 6823, loss 0.487695.
Train: 2018-08-02T13:09:35.442356: step 6824, loss 0.493718.
Train: 2018-08-02T13:09:35.598568: step 6825, loss 0.485931.
Train: 2018-08-02T13:09:35.754753: step 6826, loss 0.414372.
Train: 2018-08-02T13:09:35.910965: step 6827, loss 0.634738.
Train: 2018-08-02T13:09:36.051587: step 6828, loss 0.482924.
Train: 2018-08-02T13:09:36.207801: step 6829, loss 0.465346.
Train: 2018-08-02T13:09:36.363984: step 6830, loss 0.489441.
Test: 2018-08-02T13:09:36.817032: step 6830, loss 0.549227.
Train: 2018-08-02T13:09:36.973246: step 6831, loss 0.531181.
Train: 2018-08-02T13:09:37.129459: step 6832, loss 0.507547.
Train: 2018-08-02T13:09:37.270051: step 6833, loss 0.566646.
Train: 2018-08-02T13:09:37.426260: step 6834, loss 0.485111.
Train: 2018-08-02T13:09:37.582478: step 6835, loss 0.572756.
Train: 2018-08-02T13:09:37.723048: step 6836, loss 0.510373.
Train: 2018-08-02T13:09:37.879283: step 6837, loss 0.492933.
Train: 2018-08-02T13:09:38.035468: step 6838, loss 0.595347.
Train: 2018-08-02T13:09:38.176089: step 6839, loss 0.495667.
Train: 2018-08-02T13:09:38.332303: step 6840, loss 0.47883.
Test: 2018-08-02T13:09:38.800941: step 6840, loss 0.549439.
Train: 2018-08-02T13:09:38.941504: step 6841, loss 0.452671.
Train: 2018-08-02T13:09:39.097749: step 6842, loss 0.514515.
Train: 2018-08-02T13:09:39.238341: step 6843, loss 0.535187.
Train: 2018-08-02T13:09:39.394553: step 6844, loss 0.465444.
Train: 2018-08-02T13:09:39.550738: step 6845, loss 0.580471.
Train: 2018-08-02T13:09:39.691330: step 6846, loss 0.506386.
Train: 2018-08-02T13:09:39.847573: step 6847, loss 0.524104.
Train: 2018-08-02T13:09:40.003782: step 6848, loss 0.530929.
Train: 2018-08-02T13:09:40.160000: step 6849, loss 0.553993.
Train: 2018-08-02T13:09:40.316215: step 6850, loss 0.5528.
Test: 2018-08-02T13:09:40.769202: step 6850, loss 0.55144.
Train: 2018-08-02T13:09:40.925445: step 6851, loss 0.564028.
Train: 2018-08-02T13:09:41.066008: step 6852, loss 0.548253.
Train: 2018-08-02T13:09:41.222245: step 6853, loss 0.545792.
Train: 2018-08-02T13:09:41.378437: step 6854, loss 0.53272.
Train: 2018-08-02T13:09:41.534679: step 6855, loss 0.545822.
Train: 2018-08-02T13:09:41.690893: step 6856, loss 0.518528.
Train: 2018-08-02T13:09:41.831484: step 6857, loss 0.569103.
Train: 2018-08-02T13:09:41.987697: step 6858, loss 0.487379.
Train: 2018-08-02T13:09:42.143910: step 6859, loss 0.529749.
Train: 2018-08-02T13:09:42.300095: step 6860, loss 0.51794.
Test: 2018-08-02T13:09:42.753113: step 6860, loss 0.547692.
Train: 2018-08-02T13:09:42.909356: step 6861, loss 0.552413.
Train: 2018-08-02T13:09:43.049948: step 6862, loss 0.50875.
Train: 2018-08-02T13:09:43.206163: step 6863, loss 0.513587.
Train: 2018-08-02T13:09:43.362377: step 6864, loss 0.478853.
Train: 2018-08-02T13:09:43.502967: step 6865, loss 0.470012.
Train: 2018-08-02T13:09:43.659182: step 6866, loss 0.540601.
Train: 2018-08-02T13:09:43.815396: step 6867, loss 0.544962.
Train: 2018-08-02T13:09:43.971607: step 6868, loss 0.616452.
Train: 2018-08-02T13:09:44.112199: step 6869, loss 0.539331.
Train: 2018-08-02T13:09:44.268383: step 6870, loss 0.504053.
Test: 2018-08-02T13:09:44.721426: step 6870, loss 0.548322.
Train: 2018-08-02T13:09:44.877645: step 6871, loss 0.56043.
Train: 2018-08-02T13:09:45.033830: step 6872, loss 0.577535.
Train: 2018-08-02T13:09:45.190074: step 6873, loss 0.557138.
Train: 2018-08-02T13:09:45.330664: step 6874, loss 0.565471.
Train: 2018-08-02T13:09:45.486847: step 6875, loss 0.543958.
Train: 2018-08-02T13:09:45.643060: step 6876, loss 0.624847.
Train: 2018-08-02T13:09:45.783652: step 6877, loss 0.600683.
Train: 2018-08-02T13:09:45.939898: step 6878, loss 0.56074.
Train: 2018-08-02T13:09:46.096081: step 6879, loss 0.557949.
Train: 2018-08-02T13:09:46.252318: step 6880, loss 0.531354.
Test: 2018-08-02T13:09:46.705313: step 6880, loss 0.548933.
Train: 2018-08-02T13:09:46.861558: step 6881, loss 0.487768.
Train: 2018-08-02T13:09:47.017739: step 6882, loss 0.615327.
Train: 2018-08-02T13:09:47.158361: step 6883, loss 0.529414.
Train: 2018-08-02T13:09:47.314546: step 6884, loss 0.566353.
Train: 2018-08-02T13:09:47.470787: step 6885, loss 0.508016.
Train: 2018-08-02T13:09:47.611380: step 6886, loss 0.514001.
Train: 2018-08-02T13:09:47.767593: step 6887, loss 0.5345.
Train: 2018-08-02T13:09:47.923807: step 6888, loss 0.501262.
Train: 2018-08-02T13:09:48.064370: step 6889, loss 0.532042.
Train: 2018-08-02T13:09:48.220612: step 6890, loss 0.527253.
Test: 2018-08-02T13:09:48.689222: step 6890, loss 0.547101.
Train: 2018-08-02T13:09:48.845466: step 6891, loss 0.497457.
Train: 2018-08-02T13:09:48.986058: step 6892, loss 0.546053.
Train: 2018-08-02T13:09:49.142272: step 6893, loss 0.476192.
Train: 2018-08-02T13:09:49.298455: step 6894, loss 0.515416.
Train: 2018-08-02T13:09:49.439079: step 6895, loss 0.531551.
Train: 2018-08-02T13:09:49.595260: step 6896, loss 0.549099.
Train: 2018-08-02T13:09:49.751473: step 6897, loss 0.455838.
Train: 2018-08-02T13:09:49.892065: step 6898, loss 0.508684.
Train: 2018-08-02T13:09:50.048278: step 6899, loss 0.632007.
Train: 2018-08-02T13:09:50.204522: step 6900, loss 0.525516.
Test: 2018-08-02T13:09:50.657513: step 6900, loss 0.552333.
Train: 2018-08-02T13:09:51.344881: step 6901, loss 0.519866.
Train: 2018-08-02T13:09:51.501089: step 6902, loss 0.519038.
Train: 2018-08-02T13:09:51.641686: step 6903, loss 0.520825.
Train: 2018-08-02T13:09:51.797899: step 6904, loss 0.447588.
Train: 2018-08-02T13:09:51.954083: step 6905, loss 0.625879.
Train: 2018-08-02T13:09:52.094676: step 6906, loss 0.524262.
Train: 2018-08-02T13:09:52.250918: step 6907, loss 0.585021.
Train: 2018-08-02T13:09:52.407131: step 6908, loss 0.483768.
Train: 2018-08-02T13:09:52.563347: step 6909, loss 0.596358.
Train: 2018-08-02T13:09:52.719553: step 6910, loss 0.549089.
Test: 2018-08-02T13:09:53.172575: step 6910, loss 0.553544.
Train: 2018-08-02T13:09:53.313171: step 6911, loss 0.54324.
Train: 2018-08-02T13:09:53.469353: step 6912, loss 0.515573.
Train: 2018-08-02T13:09:53.625596: step 6913, loss 0.511891.
Train: 2018-08-02T13:09:53.766189: step 6914, loss 0.475221.
Train: 2018-08-02T13:09:53.922402: step 6915, loss 0.586301.
Train: 2018-08-02T13:09:54.078616: step 6916, loss 0.606823.
Train: 2018-08-02T13:09:54.219179: step 6917, loss 0.474905.
Train: 2018-08-02T13:09:54.375421: step 6918, loss 0.461264.
Train: 2018-08-02T13:09:54.531604: step 6919, loss 0.439215.
Train: 2018-08-02T13:09:54.672226: step 6920, loss 0.449656.
Test: 2018-08-02T13:09:55.140836: step 6920, loss 0.547048.
Train: 2018-08-02T13:09:55.281459: step 6921, loss 0.577896.
Train: 2018-08-02T13:09:55.437674: step 6922, loss 0.470882.
Train: 2018-08-02T13:09:55.593856: step 6923, loss 0.507787.
Train: 2018-08-02T13:09:55.750075: step 6924, loss 0.477803.
Train: 2018-08-02T13:09:55.890692: step 6925, loss 0.582003.
Train: 2018-08-02T13:09:56.046904: step 6926, loss 0.477736.
Train: 2018-08-02T13:09:56.203117: step 6927, loss 0.497375.
Train: 2018-08-02T13:09:56.343711: step 6928, loss 0.494603.
Train: 2018-08-02T13:09:56.499923: step 6929, loss 0.499841.
Train: 2018-08-02T13:09:56.656137: step 6930, loss 0.566513.
Test: 2018-08-02T13:09:57.109162: step 6930, loss 0.548259.
Train: 2018-08-02T13:09:57.265386: step 6931, loss 0.424073.
Train: 2018-08-02T13:09:57.421582: step 6932, loss 0.52458.
Train: 2018-08-02T13:09:57.577766: step 6933, loss 0.513184.
Train: 2018-08-02T13:09:57.718357: step 6934, loss 0.510858.
Train: 2018-08-02T13:09:57.874571: step 6935, loss 0.532642.
Train: 2018-08-02T13:09:58.030820: step 6936, loss 0.464571.
Train: 2018-08-02T13:09:58.187030: step 6937, loss 0.500399.
Train: 2018-08-02T13:09:58.343243: step 6938, loss 0.60038.
Train: 2018-08-02T13:09:58.483833: step 6939, loss 0.528768.
Train: 2018-08-02T13:09:58.640017: step 6940, loss 0.538507.
Test: 2018-08-02T13:09:59.093036: step 6940, loss 0.54691.
Train: 2018-08-02T13:09:59.249281: step 6941, loss 0.514063.
Train: 2018-08-02T13:09:59.405493: step 6942, loss 0.559044.
Train: 2018-08-02T13:09:59.561709: step 6943, loss 0.581324.
Train: 2018-08-02T13:09:59.702299: step 6944, loss 0.521269.
Train: 2018-08-02T13:09:59.858512: step 6945, loss 0.586754.
Train: 2018-08-02T13:09:59.967862: step 6946, loss 0.523389.
Train: 2018-08-02T13:10:00.124046: step 6947, loss 0.551045.
Train: 2018-08-02T13:10:00.280258: step 6948, loss 0.536241.
Train: 2018-08-02T13:10:00.420881: step 6949, loss 0.576813.
Train: 2018-08-02T13:10:00.577093: step 6950, loss 0.568843.
Test: 2018-08-02T13:10:01.030083: step 6950, loss 0.551639.
Train: 2018-08-02T13:10:01.186328: step 6951, loss 0.533067.
Train: 2018-08-02T13:10:01.342540: step 6952, loss 0.48315.
Train: 2018-08-02T13:10:01.483132: step 6953, loss 0.595695.
Train: 2018-08-02T13:10:01.639315: step 6954, loss 0.474039.
Train: 2018-08-02T13:10:01.795559: step 6955, loss 0.509905.
Train: 2018-08-02T13:10:01.951773: step 6956, loss 0.516836.
Train: 2018-08-02T13:10:02.092367: step 6957, loss 0.474674.
Train: 2018-08-02T13:10:02.248577: step 6958, loss 0.554953.
Train: 2018-08-02T13:10:02.404785: step 6959, loss 0.575457.
Train: 2018-08-02T13:10:02.561006: step 6960, loss 0.494781.
Test: 2018-08-02T13:10:03.014019: step 6960, loss 0.548502.
Train: 2018-08-02T13:10:03.170206: step 6961, loss 0.512741.
Train: 2018-08-02T13:10:03.326444: step 6962, loss 0.47591.
Train: 2018-08-02T13:10:03.467042: step 6963, loss 0.536069.
Train: 2018-08-02T13:10:03.623255: step 6964, loss 0.543149.
Train: 2018-08-02T13:10:03.779469: step 6965, loss 0.508918.
Train: 2018-08-02T13:10:03.920061: step 6966, loss 0.571842.
Train: 2018-08-02T13:10:04.076244: step 6967, loss 0.487152.
Train: 2018-08-02T13:10:04.232487: step 6968, loss 0.556682.
Train: 2018-08-02T13:10:04.388701: step 6969, loss 0.520296.
Train: 2018-08-02T13:10:04.544914: step 6970, loss 0.577285.
Test: 2018-08-02T13:10:04.997930: step 6970, loss 0.553.
Train: 2018-08-02T13:10:05.154147: step 6971, loss 0.479502.
Train: 2018-08-02T13:10:05.310330: step 6972, loss 0.507777.
Train: 2018-08-02T13:10:05.450953: step 6973, loss 0.535718.
Train: 2018-08-02T13:10:05.622758: step 6974, loss 0.477969.
Train: 2018-08-02T13:10:05.763379: step 6975, loss 0.48373.
Train: 2018-08-02T13:10:05.919592: step 6976, loss 0.524372.
Train: 2018-08-02T13:10:06.075806: step 6977, loss 0.530174.
Train: 2018-08-02T13:10:06.231990: step 6978, loss 0.605187.
Train: 2018-08-02T13:10:06.372611: step 6979, loss 0.526055.
Train: 2018-08-02T13:10:06.528826: step 6980, loss 0.502018.
Test: 2018-08-02T13:10:06.997466: step 6980, loss 0.552789.
Train: 2018-08-02T13:10:07.138057: step 6981, loss 0.484083.
Train: 2018-08-02T13:10:07.294270: step 6982, loss 0.503692.
Train: 2018-08-02T13:10:07.450453: step 6983, loss 0.520034.
Train: 2018-08-02T13:10:07.606697: step 6984, loss 0.491846.
Train: 2018-08-02T13:10:07.747292: step 6985, loss 0.465645.
Train: 2018-08-02T13:10:07.903504: step 6986, loss 0.508474.
Train: 2018-08-02T13:10:08.059686: step 6987, loss 0.595585.
Train: 2018-08-02T13:10:08.215930: step 6988, loss 0.53944.
Train: 2018-08-02T13:10:08.356522: step 6989, loss 0.511752.
Train: 2018-08-02T13:10:08.512704: step 6990, loss 0.528684.
Test: 2018-08-02T13:10:08.965725: step 6990, loss 0.552253.
Train: 2018-08-02T13:10:09.121969: step 6991, loss 0.522178.
Train: 2018-08-02T13:10:09.278181: step 6992, loss 0.49487.
Train: 2018-08-02T13:10:09.434396: step 6993, loss 0.438422.
Train: 2018-08-02T13:10:09.590608: step 6994, loss 0.571636.
Train: 2018-08-02T13:10:09.731200: step 6995, loss 0.657021.
Train: 2018-08-02T13:10:09.887416: step 6996, loss 0.609359.
Train: 2018-08-02T13:10:10.043629: step 6997, loss 0.498477.
Train: 2018-08-02T13:10:10.199812: step 6998, loss 0.556587.
Train: 2018-08-02T13:10:10.340401: step 6999, loss 0.500723.
Train: 2018-08-02T13:10:10.496616: step 7000, loss 0.586928.
Test: 2018-08-02T13:10:10.949666: step 7000, loss 0.55993.
Train: 2018-08-02T13:10:11.605761: step 7001, loss 0.47723.
Train: 2018-08-02T13:10:11.761974: step 7002, loss 0.512677.
Train: 2018-08-02T13:10:11.918187: step 7003, loss 0.557523.
Train: 2018-08-02T13:10:12.058782: step 7004, loss 0.559777.
Train: 2018-08-02T13:10:12.214993: step 7005, loss 0.499882.
Train: 2018-08-02T13:10:12.371206: step 7006, loss 0.530959.
Train: 2018-08-02T13:10:12.527391: step 7007, loss 0.542593.
Train: 2018-08-02T13:10:12.683630: step 7008, loss 0.501896.
Train: 2018-08-02T13:10:12.824226: step 7009, loss 0.468978.
Train: 2018-08-02T13:10:12.980439: step 7010, loss 0.485056.
Test: 2018-08-02T13:10:13.433468: step 7010, loss 0.550162.
Train: 2018-08-02T13:10:13.589641: step 7011, loss 0.590454.
Train: 2018-08-02T13:10:13.745884: step 7012, loss 0.552345.
Train: 2018-08-02T13:10:13.902101: step 7013, loss 0.526629.
Train: 2018-08-02T13:10:14.058314: step 7014, loss 0.510571.
Train: 2018-08-02T13:10:14.198903: step 7015, loss 0.482105.
Train: 2018-08-02T13:10:14.355117: step 7016, loss 0.53028.
Train: 2018-08-02T13:10:14.511301: step 7017, loss 0.47024.
Train: 2018-08-02T13:10:14.667542: step 7018, loss 0.520757.
Train: 2018-08-02T13:10:14.823752: step 7019, loss 0.49824.
Train: 2018-08-02T13:10:14.979970: step 7020, loss 0.594255.
Test: 2018-08-02T13:10:15.432986: step 7020, loss 0.548652.
Train: 2018-08-02T13:10:15.589203: step 7021, loss 0.526064.
Train: 2018-08-02T13:10:15.729795: step 7022, loss 0.506813.
Train: 2018-08-02T13:10:15.886009: step 7023, loss 0.524202.
Train: 2018-08-02T13:10:16.042225: step 7024, loss 0.570901.
Train: 2018-08-02T13:10:16.198435: step 7025, loss 0.537788.
Train: 2018-08-02T13:10:16.339027: step 7026, loss 0.503742.
Train: 2018-08-02T13:10:16.495211: step 7027, loss 0.525942.
Train: 2018-08-02T13:10:16.651425: step 7028, loss 0.491147.
Train: 2018-08-02T13:10:16.807670: step 7029, loss 0.505682.
Train: 2018-08-02T13:10:16.948260: step 7030, loss 0.459412.
Test: 2018-08-02T13:10:17.416870: step 7030, loss 0.553641.
Train: 2018-08-02T13:10:17.557496: step 7031, loss 0.508612.
Train: 2018-08-02T13:10:17.713676: step 7032, loss 0.481676.
Train: 2018-08-02T13:10:17.869914: step 7033, loss 0.549468.
Train: 2018-08-02T13:10:18.026103: step 7034, loss 0.484267.
Train: 2018-08-02T13:10:18.182341: step 7035, loss 0.547611.
Train: 2018-08-02T13:10:18.322938: step 7036, loss 0.536963.
Train: 2018-08-02T13:10:18.479153: step 7037, loss 0.577471.
Train: 2018-08-02T13:10:18.635365: step 7038, loss 0.54402.
Train: 2018-08-02T13:10:18.775957: step 7039, loss 0.526598.
Train: 2018-08-02T13:10:18.932172: step 7040, loss 0.542442.
Test: 2018-08-02T13:10:19.385191: step 7040, loss 0.55134.
Train: 2018-08-02T13:10:19.541372: step 7041, loss 0.543471.
Train: 2018-08-02T13:10:19.697617: step 7042, loss 0.528797.
Train: 2018-08-02T13:10:19.838178: step 7043, loss 0.549286.
Train: 2018-08-02T13:10:19.994422: step 7044, loss 0.593953.
Train: 2018-08-02T13:10:20.150636: step 7045, loss 0.473732.
Train: 2018-08-02T13:10:20.306842: step 7046, loss 0.447071.
Train: 2018-08-02T13:10:20.447440: step 7047, loss 0.546901.
Train: 2018-08-02T13:10:20.603654: step 7048, loss 0.560661.
Train: 2018-08-02T13:10:20.759867: step 7049, loss 0.466551.
Train: 2018-08-02T13:10:20.916084: step 7050, loss 0.512082.
Test: 2018-08-02T13:10:21.369070: step 7050, loss 0.54869.
Train: 2018-08-02T13:10:21.525283: step 7051, loss 0.539252.
Train: 2018-08-02T13:10:21.665905: step 7052, loss 0.545332.
Train: 2018-08-02T13:10:21.822119: step 7053, loss 0.557828.
Train: 2018-08-02T13:10:21.978301: step 7054, loss 0.472235.
Train: 2018-08-02T13:10:22.118924: step 7055, loss 0.52371.
Train: 2018-08-02T13:10:22.275138: step 7056, loss 0.541519.
Train: 2018-08-02T13:10:22.431353: step 7057, loss 0.489115.
Train: 2018-08-02T13:10:22.587564: step 7058, loss 0.495741.
Train: 2018-08-02T13:10:22.743747: step 7059, loss 0.572873.
Train: 2018-08-02T13:10:22.884370: step 7060, loss 0.479881.
Test: 2018-08-02T13:10:23.352980: step 7060, loss 0.54847.
Train: 2018-08-02T13:10:23.493603: step 7061, loss 0.53207.
Train: 2018-08-02T13:10:23.649815: step 7062, loss 0.458195.
Train: 2018-08-02T13:10:23.806031: step 7063, loss 0.616769.
Train: 2018-08-02T13:10:23.946621: step 7064, loss 0.484662.
Train: 2018-08-02T13:10:24.102835: step 7065, loss 0.474854.
Train: 2018-08-02T13:10:24.259042: step 7066, loss 0.474031.
Train: 2018-08-02T13:10:24.415261: step 7067, loss 0.544659.
Train: 2018-08-02T13:10:24.555855: step 7068, loss 0.482746.
Train: 2018-08-02T13:10:24.712036: step 7069, loss 0.516555.
Train: 2018-08-02T13:10:24.868280: step 7070, loss 0.46641.
Test: 2018-08-02T13:10:25.321270: step 7070, loss 0.546323.
Train: 2018-08-02T13:10:25.477487: step 7071, loss 0.518986.
Train: 2018-08-02T13:10:25.633695: step 7072, loss 0.413428.
Train: 2018-08-02T13:10:25.789937: step 7073, loss 0.464718.
Train: 2018-08-02T13:10:25.961775: step 7074, loss 0.565137.
Train: 2018-08-02T13:10:26.102366: step 7075, loss 0.505723.
Train: 2018-08-02T13:10:26.274195: step 7076, loss 0.43003.
Train: 2018-08-02T13:10:26.430410: step 7077, loss 0.559448.
Train: 2018-08-02T13:10:26.586600: step 7078, loss 0.57654.
Train: 2018-08-02T13:10:26.739758: step 7079, loss 0.584069.
Train: 2018-08-02T13:10:26.926262: step 7080, loss 0.553079.
Test: 2018-08-02T13:10:27.390265: step 7080, loss 0.560693.
Train: 2018-08-02T13:10:27.573533: step 7081, loss 0.572953.
Train: 2018-08-02T13:10:27.739983: step 7082, loss 0.536081.
Train: 2018-08-02T13:10:27.902030: step 7083, loss 0.543073.
Train: 2018-08-02T13:10:28.059604: step 7084, loss 0.459059.
Train: 2018-08-02T13:10:28.220115: step 7085, loss 0.508086.
Train: 2018-08-02T13:10:28.379959: step 7086, loss 0.493699.
Train: 2018-08-02T13:10:28.540001: step 7087, loss 0.61618.
Train: 2018-08-02T13:10:28.690212: step 7088, loss 0.530413.
Train: 2018-08-02T13:10:28.850067: step 7089, loss 0.508985.
Train: 2018-08-02T13:10:29.003810: step 7090, loss 0.524194.
Test: 2018-08-02T13:10:29.464184: step 7090, loss 0.552686.
Train: 2018-08-02T13:10:29.619887: step 7091, loss 0.571846.
Train: 2018-08-02T13:10:29.770180: step 7092, loss 0.475428.
Train: 2018-08-02T13:10:29.923587: step 7093, loss 0.455818.
Train: 2018-08-02T13:10:30.079435: step 7094, loss 0.471335.
Train: 2018-08-02T13:10:30.229879: step 7095, loss 0.598138.
Train: 2018-08-02T13:10:30.394397: step 7096, loss 0.503523.
Train: 2018-08-02T13:10:30.501451: step 7097, loss 0.431116.
Train: 2018-08-02T13:10:30.650096: step 7098, loss 0.437753.
Train: 2018-08-02T13:10:30.801729: step 7099, loss 0.525787.
Train: 2018-08-02T13:10:30.964498: step 7100, loss 0.529326.
Test: 2018-08-02T13:10:31.419007: step 7100, loss 0.548574.
Train: 2018-08-02T13:10:32.036256: step 7101, loss 0.667827.
Train: 2018-08-02T13:10:32.205478: step 7102, loss 0.52066.
Train: 2018-08-02T13:10:32.352615: step 7103, loss 0.486225.
Train: 2018-08-02T13:10:32.506288: step 7104, loss 0.515864.
Train: 2018-08-02T13:10:32.653457: step 7105, loss 0.490191.
Train: 2018-08-02T13:10:32.807092: step 7106, loss 0.484301.
Train: 2018-08-02T13:10:32.969877: step 7107, loss 0.413523.
Train: 2018-08-02T13:10:33.170535: step 7108, loss 0.528313.
Train: 2018-08-02T13:10:33.324235: step 7109, loss 0.474046.
Train: 2018-08-02T13:10:33.471311: step 7110, loss 0.560465.
Test: 2018-08-02T13:10:33.941365: step 7110, loss 0.546846.
Train: 2018-08-02T13:10:34.104125: step 7111, loss 0.527464.
Train: 2018-08-02T13:10:34.257738: step 7112, loss 0.487257.
Train: 2018-08-02T13:10:34.411419: step 7113, loss 0.548326.
Train: 2018-08-02T13:10:34.558552: step 7114, loss 0.482727.
Train: 2018-08-02T13:10:34.712219: step 7115, loss 0.485309.
Train: 2018-08-02T13:10:34.875038: step 7116, loss 0.523195.
Train: 2018-08-02T13:10:35.028616: step 7117, loss 0.496046.
Train: 2018-08-02T13:10:35.184862: step 7118, loss 0.512019.
Train: 2018-08-02T13:10:35.341043: step 7119, loss 0.50073.
Train: 2018-08-02T13:10:35.497285: step 7120, loss 0.51497.
Test: 2018-08-02T13:10:35.950275: step 7120, loss 0.549916.
Train: 2018-08-02T13:10:36.106513: step 7121, loss 0.483271.
Train: 2018-08-02T13:10:36.278326: step 7122, loss 0.535588.
Train: 2018-08-02T13:10:36.434561: step 7123, loss 0.499938.
Train: 2018-08-02T13:10:36.590750: step 7124, loss 0.50676.
Train: 2018-08-02T13:10:36.746964: step 7125, loss 0.538319.
Train: 2018-08-02T13:10:36.903176: step 7126, loss 0.56497.
Train: 2018-08-02T13:10:37.075042: step 7127, loss 0.499027.
Train: 2018-08-02T13:10:37.215604: step 7128, loss 0.531709.
Train: 2018-08-02T13:10:37.387440: step 7129, loss 0.507978.
Train: 2018-08-02T13:10:37.543653: step 7130, loss 0.555986.
Test: 2018-08-02T13:10:37.996696: step 7130, loss 0.551682.
Train: 2018-08-02T13:10:38.152914: step 7131, loss 0.470676.
Train: 2018-08-02T13:10:38.309098: step 7132, loss 0.55231.
Train: 2018-08-02T13:10:38.449719: step 7133, loss 0.521285.
Train: 2018-08-02T13:10:38.605933: step 7134, loss 0.593621.
Train: 2018-08-02T13:10:38.762147: step 7135, loss 0.527281.
Train: 2018-08-02T13:10:38.918360: step 7136, loss 0.570514.
Train: 2018-08-02T13:10:39.074561: step 7137, loss 0.551292.
Train: 2018-08-02T13:10:39.246379: step 7138, loss 0.482768.
Train: 2018-08-02T13:10:39.402590: step 7139, loss 0.477313.
Train: 2018-08-02T13:10:39.558828: step 7140, loss 0.495492.
Test: 2018-08-02T13:10:40.011824: step 7140, loss 0.547656.
Train: 2018-08-02T13:10:40.168062: step 7141, loss 0.495302.
Train: 2018-08-02T13:10:40.324280: step 7142, loss 0.4774.
Train: 2018-08-02T13:10:40.480493: step 7143, loss 0.653543.
Train: 2018-08-02T13:10:40.636708: step 7144, loss 0.528825.
Train: 2018-08-02T13:10:40.792921: step 7145, loss 0.529885.
Train: 2018-08-02T13:10:40.949104: step 7146, loss 0.512095.
Train: 2018-08-02T13:10:41.105347: step 7147, loss 0.529326.
Train: 2018-08-02T13:10:41.261561: step 7148, loss 0.514406.
Train: 2018-08-02T13:10:41.402154: step 7149, loss 0.469959.
Train: 2018-08-02T13:10:41.558367: step 7150, loss 0.482221.
Test: 2018-08-02T13:10:42.026977: step 7150, loss 0.550717.
Train: 2018-08-02T13:10:42.183219: step 7151, loss 0.558007.
Train: 2018-08-02T13:10:42.339403: step 7152, loss 0.486258.
Train: 2018-08-02T13:10:42.495645: step 7153, loss 0.551276.
Train: 2018-08-02T13:10:42.651831: step 7154, loss 0.532397.
Train: 2018-08-02T13:10:42.808051: step 7155, loss 0.475288.
Train: 2018-08-02T13:10:42.964258: step 7156, loss 0.641364.
Train: 2018-08-02T13:10:43.120473: step 7157, loss 0.478373.
Train: 2018-08-02T13:10:43.276714: step 7158, loss 0.523733.
Train: 2018-08-02T13:10:43.432928: step 7159, loss 0.487934.
Train: 2018-08-02T13:10:43.589110: step 7160, loss 0.574736.
Test: 2018-08-02T13:10:44.042161: step 7160, loss 0.553959.
Train: 2018-08-02T13:10:44.229616: step 7161, loss 0.496583.
Train: 2018-08-02T13:10:44.385800: step 7162, loss 0.443424.
Train: 2018-08-02T13:10:44.542045: step 7163, loss 0.511557.
Train: 2018-08-02T13:10:44.698256: step 7164, loss 0.528427.
Train: 2018-08-02T13:10:44.854440: step 7165, loss 0.470537.
Train: 2018-08-02T13:10:45.010654: step 7166, loss 0.485537.
Train: 2018-08-02T13:10:45.182488: step 7167, loss 0.437815.
Train: 2018-08-02T13:10:45.323080: step 7168, loss 0.484372.
Train: 2018-08-02T13:10:45.479323: step 7169, loss 0.481504.
Train: 2018-08-02T13:10:45.635537: step 7170, loss 0.401542.
Test: 2018-08-02T13:10:46.104173: step 7170, loss 0.557738.
Train: 2018-08-02T13:10:46.260360: step 7171, loss 0.611393.
Train: 2018-08-02T13:10:46.416573: step 7172, loss 0.507568.
Train: 2018-08-02T13:10:46.572788: step 7173, loss 0.539613.
Train: 2018-08-02T13:10:46.729032: step 7174, loss 0.540977.
Train: 2018-08-02T13:10:46.885247: step 7175, loss 0.566357.
Train: 2018-08-02T13:10:47.041428: step 7176, loss 0.518583.
Train: 2018-08-02T13:10:47.197641: step 7177, loss 0.532801.
Train: 2018-08-02T13:10:47.353884: step 7178, loss 0.569563.
Train: 2018-08-02T13:10:47.510069: step 7179, loss 0.559965.
Train: 2018-08-02T13:10:47.666282: step 7180, loss 0.551799.
Test: 2018-08-02T13:10:48.119300: step 7180, loss 0.560718.
Train: 2018-08-02T13:10:48.275515: step 7181, loss 0.543355.
Train: 2018-08-02T13:10:48.431757: step 7182, loss 0.580426.
Train: 2018-08-02T13:10:48.587973: step 7183, loss 0.504259.
Train: 2018-08-02T13:10:48.744155: step 7184, loss 0.595924.
Train: 2018-08-02T13:10:48.900398: step 7185, loss 0.51629.
Train: 2018-08-02T13:10:49.056582: step 7186, loss 0.505151.
Train: 2018-08-02T13:10:49.212796: step 7187, loss 0.57442.
Train: 2018-08-02T13:10:49.369007: step 7188, loss 0.508336.
Train: 2018-08-02T13:10:49.525251: step 7189, loss 0.463729.
Train: 2018-08-02T13:10:49.681465: step 7190, loss 0.533805.
Test: 2018-08-02T13:10:50.134453: step 7190, loss 0.548776.
Train: 2018-08-02T13:10:50.290696: step 7191, loss 0.522934.
Train: 2018-08-02T13:10:50.446909: step 7192, loss 0.519125.
Train: 2018-08-02T13:10:50.603094: step 7193, loss 0.574122.
Train: 2018-08-02T13:10:50.759336: step 7194, loss 0.472639.
Train: 2018-08-02T13:10:50.915544: step 7195, loss 0.604104.
Train: 2018-08-02T13:10:51.071763: step 7196, loss 0.544783.
Train: 2018-08-02T13:10:51.227977: step 7197, loss 0.467088.
Train: 2018-08-02T13:10:51.368569: step 7198, loss 0.601841.
Train: 2018-08-02T13:10:51.524755: step 7199, loss 0.501474.
Train: 2018-08-02T13:10:51.680996: step 7200, loss 0.524509.
Test: 2018-08-02T13:10:52.149606: step 7200, loss 0.555008.
Train: 2018-08-02T13:10:52.758864: step 7201, loss 0.476357.
Train: 2018-08-02T13:10:52.915077: step 7202, loss 0.550421.
Train: 2018-08-02T13:10:53.071295: step 7203, loss 0.492549.
Train: 2018-08-02T13:10:53.227509: step 7204, loss 0.46389.
Train: 2018-08-02T13:10:53.383723: step 7205, loss 0.558919.
Train: 2018-08-02T13:10:53.551451: step 7206, loss 0.516036.
Train: 2018-08-02T13:10:53.707667: step 7207, loss 0.476785.
Train: 2018-08-02T13:10:53.863911: step 7208, loss 0.540775.
Train: 2018-08-02T13:10:54.020095: step 7209, loss 0.610066.
Train: 2018-08-02T13:10:54.176339: step 7210, loss 0.542053.
Test: 2018-08-02T13:10:54.644974: step 7210, loss 0.554087.
Train: 2018-08-02T13:10:54.801187: step 7211, loss 0.472894.
Train: 2018-08-02T13:10:54.957375: step 7212, loss 0.535787.
Train: 2018-08-02T13:10:55.113619: step 7213, loss 0.489134.
Train: 2018-08-02T13:10:55.269834: step 7214, loss 0.538028.
Train: 2018-08-02T13:10:55.426016: step 7215, loss 0.606037.
Train: 2018-08-02T13:10:55.582229: step 7216, loss 0.52733.
Train: 2018-08-02T13:10:55.738473: step 7217, loss 0.525786.
Train: 2018-08-02T13:10:55.894655: step 7218, loss 0.538256.
Train: 2018-08-02T13:10:56.050869: step 7219, loss 0.510972.
Train: 2018-08-02T13:10:56.207083: step 7220, loss 0.569543.
Test: 2018-08-02T13:10:56.675723: step 7220, loss 0.551193.
Train: 2018-08-02T13:10:56.831937: step 7221, loss 0.51995.
Train: 2018-08-02T13:10:56.988180: step 7222, loss 0.619795.
Train: 2018-08-02T13:10:57.144364: step 7223, loss 0.525898.
Train: 2018-08-02T13:10:57.300577: step 7224, loss 0.494881.
Train: 2018-08-02T13:10:57.456791: step 7225, loss 0.583703.
Train: 2018-08-02T13:10:57.613033: step 7226, loss 0.509168.
Train: 2018-08-02T13:10:57.769241: step 7227, loss 0.495936.
Train: 2018-08-02T13:10:57.925455: step 7228, loss 0.468572.
Train: 2018-08-02T13:10:58.081644: step 7229, loss 0.466746.
Train: 2018-08-02T13:10:58.237857: step 7230, loss 0.562204.
Test: 2018-08-02T13:10:58.706522: step 7230, loss 0.546897.
Train: 2018-08-02T13:10:58.862742: step 7231, loss 0.53615.
Train: 2018-08-02T13:10:59.018954: step 7232, loss 0.541566.
Train: 2018-08-02T13:10:59.190760: step 7233, loss 0.614345.
Train: 2018-08-02T13:10:59.346973: step 7234, loss 0.515568.
Train: 2018-08-02T13:10:59.503185: step 7235, loss 0.553857.
Train: 2018-08-02T13:10:59.659424: step 7236, loss 0.593031.
Train: 2018-08-02T13:10:59.815642: step 7237, loss 0.520877.
Train: 2018-08-02T13:10:59.971850: step 7238, loss 0.583484.
Train: 2018-08-02T13:11:00.128063: step 7239, loss 0.541677.
Train: 2018-08-02T13:11:00.284253: step 7240, loss 0.521724.
Test: 2018-08-02T13:11:00.752918: step 7240, loss 0.56717.
Train: 2018-08-02T13:11:00.909105: step 7241, loss 0.592937.
Train: 2018-08-02T13:11:01.049702: step 7242, loss 0.537277.
Train: 2018-08-02T13:11:01.205912: step 7243, loss 0.44584.
Train: 2018-08-02T13:11:01.362126: step 7244, loss 0.379735.
Train: 2018-08-02T13:11:01.518364: step 7245, loss 0.421544.
Train: 2018-08-02T13:11:01.674578: step 7246, loss 0.512042.
Train: 2018-08-02T13:11:01.830790: step 7247, loss 0.406507.
Train: 2018-08-02T13:11:01.940117: step 7248, loss 0.608789.
Train: 2018-08-02T13:11:02.111952: step 7249, loss 0.521206.
Train: 2018-08-02T13:11:02.268164: step 7250, loss 0.584905.
Test: 2018-08-02T13:11:02.721200: step 7250, loss 0.572946.
Train: 2018-08-02T13:11:02.877395: step 7251, loss 0.485951.
Train: 2018-08-02T13:11:03.049255: step 7252, loss 0.508476.
Train: 2018-08-02T13:11:03.205443: step 7253, loss 0.468286.
Train: 2018-08-02T13:11:03.361690: step 7254, loss 0.503924.
Train: 2018-08-02T13:11:03.517897: step 7255, loss 0.561447.
Train: 2018-08-02T13:11:03.674085: step 7256, loss 0.468124.
Train: 2018-08-02T13:11:03.830298: step 7257, loss 0.64315.
Train: 2018-08-02T13:11:03.986541: step 7258, loss 0.51377.
Train: 2018-08-02T13:11:04.142748: step 7259, loss 0.551466.
Train: 2018-08-02T13:11:04.298967: step 7260, loss 0.537704.
Test: 2018-08-02T13:11:04.767608: step 7260, loss 0.560467.
Train: 2018-08-02T13:11:04.923822: step 7261, loss 0.480872.
Train: 2018-08-02T13:11:05.080036: step 7262, loss 0.565095.
Train: 2018-08-02T13:11:05.236248: step 7263, loss 0.558798.
Train: 2018-08-02T13:11:05.392462: step 7264, loss 0.533649.
Train: 2018-08-02T13:11:05.548683: step 7265, loss 0.476093.
Train: 2018-08-02T13:11:05.720481: step 7266, loss 0.466935.
Train: 2018-08-02T13:11:05.876727: step 7267, loss 0.615518.
Train: 2018-08-02T13:11:06.032936: step 7268, loss 0.557354.
Train: 2018-08-02T13:11:06.189151: step 7269, loss 0.509236.
Train: 2018-08-02T13:11:06.345344: step 7270, loss 0.537405.
Test: 2018-08-02T13:11:06.798380: step 7270, loss 0.547679.
Train: 2018-08-02T13:11:06.954599: step 7271, loss 0.519779.
Train: 2018-08-02T13:11:07.110810: step 7272, loss 0.581069.
Train: 2018-08-02T13:11:07.267022: step 7273, loss 0.480913.
Train: 2018-08-02T13:11:07.423207: step 7274, loss 0.482853.
Train: 2018-08-02T13:11:07.579452: step 7275, loss 0.538278.
Train: 2018-08-02T13:11:07.735662: step 7276, loss 0.464509.
Train: 2018-08-02T13:11:07.876256: step 7277, loss 0.519412.
Train: 2018-08-02T13:11:08.032469: step 7278, loss 0.570046.
Train: 2018-08-02T13:11:08.188653: step 7279, loss 0.501623.
Train: 2018-08-02T13:11:08.344865: step 7280, loss 0.490165.
Test: 2018-08-02T13:11:08.797917: step 7280, loss 0.549384.
Train: 2018-08-02T13:11:08.954100: step 7281, loss 0.47247.
Train: 2018-08-02T13:11:09.110337: step 7282, loss 0.522946.
Train: 2018-08-02T13:11:09.266568: step 7283, loss 0.503102.
Train: 2018-08-02T13:11:09.422746: step 7284, loss 0.422123.
Train: 2018-08-02T13:11:09.594574: step 7285, loss 0.535794.
Train: 2018-08-02T13:11:09.735166: step 7286, loss 0.552623.
Train: 2018-08-02T13:11:09.907030: step 7287, loss 0.568263.
Train: 2018-08-02T13:11:10.047621: step 7288, loss 0.454301.
Train: 2018-08-02T13:11:10.203838: step 7289, loss 0.471868.
Train: 2018-08-02T13:11:10.360047: step 7290, loss 0.555132.
Test: 2018-08-02T13:11:10.828659: step 7290, loss 0.550386.
Train: 2018-08-02T13:11:10.969280: step 7291, loss 0.564092.
Train: 2018-08-02T13:11:11.125494: step 7292, loss 0.450391.
Train: 2018-08-02T13:11:11.281708: step 7293, loss 0.43086.
Train: 2018-08-02T13:11:11.437890: step 7294, loss 0.576936.
Train: 2018-08-02T13:11:11.594104: step 7295, loss 0.586735.
Train: 2018-08-02T13:11:11.750348: step 7296, loss 0.517096.
Train: 2018-08-02T13:11:11.906561: step 7297, loss 0.491293.
Train: 2018-08-02T13:11:12.062775: step 7298, loss 0.510398.
Train: 2018-08-02T13:11:12.218988: step 7299, loss 0.496583.
Train: 2018-08-02T13:11:12.375206: step 7300, loss 0.473277.
Test: 2018-08-02T13:11:12.843836: step 7300, loss 0.549801.
Train: 2018-08-02T13:11:13.499933: step 7301, loss 0.519414.
Train: 2018-08-02T13:11:13.656153: step 7302, loss 0.48896.
Train: 2018-08-02T13:11:13.812335: step 7303, loss 0.567005.
Train: 2018-08-02T13:11:13.968579: step 7304, loss 0.505897.
Train: 2018-08-02T13:11:14.124792: step 7305, loss 0.501303.
Train: 2018-08-02T13:11:14.280975: step 7306, loss 0.453073.
Train: 2018-08-02T13:11:14.421596: step 7307, loss 0.505023.
Train: 2018-08-02T13:11:14.577813: step 7308, loss 0.489288.
Train: 2018-08-02T13:11:14.734032: step 7309, loss 0.525097.
Train: 2018-08-02T13:11:14.905828: step 7310, loss 0.496144.
Test: 2018-08-02T13:11:15.358879: step 7310, loss 0.549454.
Train: 2018-08-02T13:11:15.515085: step 7311, loss 0.624938.
Train: 2018-08-02T13:11:15.671301: step 7312, loss 0.540736.
Train: 2018-08-02T13:11:15.827521: step 7313, loss 0.577673.
Train: 2018-08-02T13:11:15.968079: step 7314, loss 0.575671.
Train: 2018-08-02T13:11:16.124324: step 7315, loss 0.578011.
Train: 2018-08-02T13:11:16.280545: step 7316, loss 0.514813.
Train: 2018-08-02T13:11:16.436722: step 7317, loss 0.588515.
Train: 2018-08-02T13:11:16.592935: step 7318, loss 0.538988.
Train: 2018-08-02T13:11:16.749179: step 7319, loss 0.494118.
Train: 2018-08-02T13:11:16.905361: step 7320, loss 0.564057.
Test: 2018-08-02T13:11:17.374032: step 7320, loss 0.550661.
Train: 2018-08-02T13:11:17.530215: step 7321, loss 0.580973.
Train: 2018-08-02T13:11:17.670806: step 7322, loss 0.458125.
Train: 2018-08-02T13:11:17.827053: step 7323, loss 0.54179.
Train: 2018-08-02T13:11:17.983257: step 7324, loss 0.485297.
Train: 2018-08-02T13:11:18.139476: step 7325, loss 0.562476.
Train: 2018-08-02T13:11:18.295717: step 7326, loss 0.448978.
Train: 2018-08-02T13:11:18.451907: step 7327, loss 0.48888.
Train: 2018-08-02T13:11:18.608118: step 7328, loss 0.554003.
Train: 2018-08-02T13:11:18.764301: step 7329, loss 0.573928.
Train: 2018-08-02T13:11:18.920514: step 7330, loss 0.676504.
Test: 2018-08-02T13:11:19.389155: step 7330, loss 0.560083.
Train: 2018-08-02T13:11:19.545367: step 7331, loss 0.528154.
Train: 2018-08-02T13:11:19.701581: step 7332, loss 0.444992.
Train: 2018-08-02T13:11:19.857824: step 7333, loss 0.568996.
Train: 2018-08-02T13:11:20.014037: step 7334, loss 0.556491.
Train: 2018-08-02T13:11:20.170251: step 7335, loss 0.56538.
Train: 2018-08-02T13:11:20.310843: step 7336, loss 0.458558.
Train: 2018-08-02T13:11:20.467050: step 7337, loss 0.486348.
Train: 2018-08-02T13:11:20.623239: step 7338, loss 0.498532.
Train: 2018-08-02T13:11:20.779483: step 7339, loss 0.533168.
Train: 2018-08-02T13:11:20.935701: step 7340, loss 0.486832.
Test: 2018-08-02T13:11:21.388685: step 7340, loss 0.545891.
Train: 2018-08-02T13:11:21.544929: step 7341, loss 0.473203.
Train: 2018-08-02T13:11:21.701137: step 7342, loss 0.535848.
Train: 2018-08-02T13:11:21.857350: step 7343, loss 0.620587.
Train: 2018-08-02T13:11:22.013569: step 7344, loss 0.534485.
Train: 2018-08-02T13:11:22.169783: step 7345, loss 0.584083.
Train: 2018-08-02T13:11:22.326000: step 7346, loss 0.551073.
Train: 2018-08-02T13:11:22.482209: step 7347, loss 0.484187.
Train: 2018-08-02T13:11:22.622801: step 7348, loss 0.480689.
Train: 2018-08-02T13:11:22.779009: step 7349, loss 0.443891.
Train: 2018-08-02T13:11:22.935197: step 7350, loss 0.605398.
Test: 2018-08-02T13:11:23.403869: step 7350, loss 0.549169.
Train: 2018-08-02T13:11:23.560051: step 7351, loss 0.468499.
Train: 2018-08-02T13:11:23.716265: step 7352, loss 0.506359.
Train: 2018-08-02T13:11:23.856857: step 7353, loss 0.506015.
Train: 2018-08-02T13:11:24.013102: step 7354, loss 0.545791.
Train: 2018-08-02T13:11:24.169315: step 7355, loss 0.522754.
Train: 2018-08-02T13:11:24.325531: step 7356, loss 0.494076.
Train: 2018-08-02T13:11:24.481742: step 7357, loss 0.478557.
Train: 2018-08-02T13:11:24.637926: step 7358, loss 0.548201.
Train: 2018-08-02T13:11:24.794139: step 7359, loss 0.544022.
Train: 2018-08-02T13:11:24.950376: step 7360, loss 0.512158.
Test: 2018-08-02T13:11:25.403372: step 7360, loss 0.550431.
Train: 2018-08-02T13:11:25.559624: step 7361, loss 0.482437.
Train: 2018-08-02T13:11:25.715823: step 7362, loss 0.479193.
Train: 2018-08-02T13:11:25.872042: step 7363, loss 0.565652.
Train: 2018-08-02T13:11:26.028231: step 7364, loss 0.477734.
Train: 2018-08-02T13:11:26.184438: step 7365, loss 0.618212.
Train: 2018-08-02T13:11:26.340676: step 7366, loss 0.539156.
Train: 2018-08-02T13:11:26.512505: step 7367, loss 0.560077.
Train: 2018-08-02T13:11:26.653109: step 7368, loss 0.493528.
Train: 2018-08-02T13:11:26.824937: step 7369, loss 0.504803.
Train: 2018-08-02T13:11:26.981157: step 7370, loss 0.518334.
Test: 2018-08-02T13:11:27.434146: step 7370, loss 0.553532.
Train: 2018-08-02T13:11:27.590389: step 7371, loss 0.532322.
Train: 2018-08-02T13:11:27.746602: step 7372, loss 0.55361.
Train: 2018-08-02T13:11:27.902787: step 7373, loss 0.581398.
Train: 2018-08-02T13:11:28.074621: step 7374, loss 0.580839.
Train: 2018-08-02T13:11:28.230835: step 7375, loss 0.467186.
Train: 2018-08-02T13:11:28.387048: step 7376, loss 0.48728.
Train: 2018-08-02T13:11:28.543260: step 7377, loss 0.516202.
Train: 2018-08-02T13:11:28.699474: step 7378, loss 0.481115.
Train: 2018-08-02T13:11:28.855717: step 7379, loss 0.571288.
Train: 2018-08-02T13:11:28.996311: step 7380, loss 0.494276.
Test: 2018-08-02T13:11:29.464951: step 7380, loss 0.547596.
Train: 2018-08-02T13:11:29.621166: step 7381, loss 0.512177.
Train: 2018-08-02T13:11:29.777376: step 7382, loss 0.505698.
Train: 2018-08-02T13:11:29.933559: step 7383, loss 0.541754.
Train: 2018-08-02T13:11:30.089774: step 7384, loss 0.484189.
Train: 2018-08-02T13:11:30.245987: step 7385, loss 0.562119.
Train: 2018-08-02T13:11:30.402231: step 7386, loss 0.539753.
Train: 2018-08-02T13:11:30.558414: step 7387, loss 0.479861.
Train: 2018-08-02T13:11:30.714627: step 7388, loss 0.515095.
Train: 2018-08-02T13:11:30.870864: step 7389, loss 0.502946.
Train: 2018-08-02T13:11:31.027078: step 7390, loss 0.527905.
Test: 2018-08-02T13:11:31.480107: step 7390, loss 0.550604.
Train: 2018-08-02T13:11:31.636317: step 7391, loss 0.437674.
Train: 2018-08-02T13:11:31.792533: step 7392, loss 0.542874.
Train: 2018-08-02T13:11:31.948713: step 7393, loss 0.511251.
Train: 2018-08-02T13:11:32.104957: step 7394, loss 0.523177.
Train: 2018-08-02T13:11:32.276761: step 7395, loss 0.573824.
Train: 2018-08-02T13:11:32.417352: step 7396, loss 0.485469.
Train: 2018-08-02T13:11:32.573600: step 7397, loss 0.461533.
Train: 2018-08-02T13:11:32.745433: step 7398, loss 0.455356.
Train: 2018-08-02T13:11:32.854780: step 7399, loss 0.423755.
Train: 2018-08-02T13:11:33.010965: step 7400, loss 0.571051.
Test: 2018-08-02T13:11:33.463988: step 7400, loss 0.548016.
Train: 2018-08-02T13:11:34.151353: step 7401, loss 0.628409.
Train: 2018-08-02T13:11:34.307564: step 7402, loss 0.541655.
Train: 2018-08-02T13:11:34.463780: step 7403, loss 0.520928.
Train: 2018-08-02T13:11:34.619988: step 7404, loss 0.517909.
Train: 2018-08-02T13:11:34.776206: step 7405, loss 0.472493.
Train: 2018-08-02T13:11:34.932390: step 7406, loss 0.538321.
Train: 2018-08-02T13:11:35.088633: step 7407, loss 0.40763.
Train: 2018-08-02T13:11:35.229198: step 7408, loss 0.56638.
Train: 2018-08-02T13:11:35.385438: step 7409, loss 0.559981.
Train: 2018-08-02T13:11:35.541622: step 7410, loss 0.568788.
Test: 2018-08-02T13:11:36.010292: step 7410, loss 0.554798.
Train: 2018-08-02T13:11:36.150854: step 7411, loss 0.501385.
Train: 2018-08-02T13:11:36.322689: step 7412, loss 0.512759.
Train: 2018-08-02T13:11:36.463310: step 7413, loss 0.517588.
Train: 2018-08-02T13:11:36.619495: step 7414, loss 0.469743.
Train: 2018-08-02T13:11:36.775738: step 7415, loss 0.520111.
Train: 2018-08-02T13:11:36.931953: step 7416, loss 0.484378.
Train: 2018-08-02T13:11:37.072513: step 7417, loss 0.526665.
Train: 2018-08-02T13:11:37.228756: step 7418, loss 0.527222.
Train: 2018-08-02T13:11:37.384969: step 7419, loss 0.603447.
Train: 2018-08-02T13:11:37.541183: step 7420, loss 0.506222.
Test: 2018-08-02T13:11:37.994172: step 7420, loss 0.548491.
Train: 2018-08-02T13:11:38.150415: step 7421, loss 0.542843.
Train: 2018-08-02T13:11:38.306598: step 7422, loss 0.507526.
Train: 2018-08-02T13:11:38.462813: step 7423, loss 0.482618.
Train: 2018-08-02T13:11:38.619025: step 7424, loss 0.432147.
Train: 2018-08-02T13:11:38.775269: step 7425, loss 0.522326.
Train: 2018-08-02T13:11:38.931482: step 7426, loss 0.567226.
Train: 2018-08-02T13:11:39.087696: step 7427, loss 0.586381.
Train: 2018-08-02T13:11:39.243911: step 7428, loss 0.531533.
Train: 2018-08-02T13:11:39.415713: step 7429, loss 0.521113.
Train: 2018-08-02T13:11:39.556306: step 7430, loss 0.539174.
Test: 2018-08-02T13:11:40.024977: step 7430, loss 0.555576.
Train: 2018-08-02T13:11:40.181160: step 7431, loss 0.508936.
Train: 2018-08-02T13:11:40.337375: step 7432, loss 0.508012.
Train: 2018-08-02T13:11:40.493598: step 7433, loss 0.53012.
Train: 2018-08-02T13:11:40.649799: step 7434, loss 0.534933.
Train: 2018-08-02T13:11:40.806044: step 7435, loss 0.434612.
Train: 2018-08-02T13:11:40.962253: step 7436, loss 0.47807.
Train: 2018-08-02T13:11:41.118470: step 7437, loss 0.587042.
Train: 2018-08-02T13:11:41.274719: step 7438, loss 0.482203.
Train: 2018-08-02T13:11:41.430892: step 7439, loss 0.554651.
Train: 2018-08-02T13:11:41.587114: step 7440, loss 0.57157.
Test: 2018-08-02T13:11:42.055727: step 7440, loss 0.549347.
Train: 2018-08-02T13:11:42.211933: step 7441, loss 0.495021.
Train: 2018-08-02T13:11:42.368147: step 7442, loss 0.492628.
Train: 2018-08-02T13:11:42.524361: step 7443, loss 0.535924.
Train: 2018-08-02T13:11:42.680575: step 7444, loss 0.466643.
Train: 2018-08-02T13:11:42.836818: step 7445, loss 0.455775.
Train: 2018-08-02T13:11:42.993001: step 7446, loss 0.513201.
Train: 2018-08-02T13:11:43.149245: step 7447, loss 0.574323.
Train: 2018-08-02T13:11:43.305461: step 7448, loss 0.559055.
Train: 2018-08-02T13:11:43.461671: step 7449, loss 0.524015.
Train: 2018-08-02T13:11:43.617885: step 7450, loss 0.578761.
Test: 2018-08-02T13:11:44.070899: step 7450, loss 0.549965.
Train: 2018-08-02T13:11:44.227087: step 7451, loss 0.559371.
Train: 2018-08-02T13:11:44.383331: step 7452, loss 0.484664.
Train: 2018-08-02T13:11:44.539516: step 7453, loss 0.557555.
Train: 2018-08-02T13:11:44.695762: step 7454, loss 0.468177.
Train: 2018-08-02T13:11:44.851967: step 7455, loss 0.466349.
Train: 2018-08-02T13:11:45.008185: step 7456, loss 0.549728.
Train: 2018-08-02T13:11:45.164368: step 7457, loss 0.483358.
Train: 2018-08-02T13:11:45.351850: step 7458, loss 0.572591.
Train: 2018-08-02T13:11:45.508068: step 7459, loss 0.652832.
Train: 2018-08-02T13:11:45.664252: step 7460, loss 0.471125.
Test: 2018-08-02T13:11:46.117269: step 7460, loss 0.551489.
Train: 2018-08-02T13:11:46.273482: step 7461, loss 0.588519.
Train: 2018-08-02T13:11:46.429695: step 7462, loss 0.554314.
Train: 2018-08-02T13:11:46.585911: step 7463, loss 0.604477.
Train: 2018-08-02T13:11:46.742125: step 7464, loss 0.530857.
Train: 2018-08-02T13:11:46.898367: step 7465, loss 0.54838.
Train: 2018-08-02T13:11:47.054579: step 7466, loss 0.548473.
Train: 2018-08-02T13:11:47.210793: step 7467, loss 0.579109.
Train: 2018-08-02T13:11:47.366976: step 7468, loss 0.522405.
Train: 2018-08-02T13:11:47.523220: step 7469, loss 0.509227.
Train: 2018-08-02T13:11:47.679404: step 7470, loss 0.515863.
Test: 2018-08-02T13:11:48.132454: step 7470, loss 0.547706.
Train: 2018-08-02T13:11:48.288661: step 7471, loss 0.649596.
Train: 2018-08-02T13:11:48.444851: step 7472, loss 0.519379.
Train: 2018-08-02T13:11:48.601064: step 7473, loss 0.478428.
Train: 2018-08-02T13:11:48.757307: step 7474, loss 0.580884.
Train: 2018-08-02T13:11:48.913533: step 7475, loss 0.509062.
Train: 2018-08-02T13:11:49.069737: step 7476, loss 0.466298.
Train: 2018-08-02T13:11:49.225947: step 7477, loss 0.531596.
Train: 2018-08-02T13:11:49.382130: step 7478, loss 0.583371.
Train: 2018-08-02T13:11:49.538374: step 7479, loss 0.492232.
Train: 2018-08-02T13:11:49.694582: step 7480, loss 0.510319.
Test: 2018-08-02T13:11:50.147576: step 7480, loss 0.549926.
Train: 2018-08-02T13:11:50.303791: step 7481, loss 0.549573.
Train: 2018-08-02T13:11:50.475623: step 7482, loss 0.459402.
Train: 2018-08-02T13:11:50.631869: step 7483, loss 0.567965.
Train: 2018-08-02T13:11:50.788082: step 7484, loss 0.513632.
Train: 2018-08-02T13:11:50.944265: step 7485, loss 0.547145.
Train: 2018-08-02T13:11:51.100479: step 7486, loss 0.425432.
Train: 2018-08-02T13:11:51.256691: step 7487, loss 0.559204.
Train: 2018-08-02T13:11:51.397283: step 7488, loss 0.532822.
Train: 2018-08-02T13:11:51.553527: step 7489, loss 0.468361.
Train: 2018-08-02T13:11:51.709741: step 7490, loss 0.514237.
Test: 2018-08-02T13:11:52.178351: step 7490, loss 0.547216.
Train: 2018-08-02T13:11:52.334595: step 7491, loss 0.545126.
Train: 2018-08-02T13:11:52.490810: step 7492, loss 0.472275.
Train: 2018-08-02T13:11:52.646992: step 7493, loss 0.512536.
Train: 2018-08-02T13:11:52.803206: step 7494, loss 0.473326.
Train: 2018-08-02T13:11:52.959442: step 7495, loss 0.494278.
Train: 2018-08-02T13:11:53.115662: step 7496, loss 0.615373.
Train: 2018-08-02T13:11:53.271860: step 7497, loss 0.516779.
Train: 2018-08-02T13:11:53.428058: step 7498, loss 0.514496.
Train: 2018-08-02T13:11:53.584302: step 7499, loss 0.514361.
Train: 2018-08-02T13:11:53.740515: step 7500, loss 0.547157.
Test: 2018-08-02T13:11:54.193537: step 7500, loss 0.555634.
Train: 2018-08-02T13:11:54.834010: step 7501, loss 0.53159.
Train: 2018-08-02T13:11:54.990219: step 7502, loss 0.462964.
Train: 2018-08-02T13:11:55.146432: step 7503, loss 0.507843.
Train: 2018-08-02T13:11:55.302648: step 7504, loss 0.553628.
Train: 2018-08-02T13:11:55.458858: step 7505, loss 0.538685.
Train: 2018-08-02T13:11:55.615071: step 7506, loss 0.545892.
Train: 2018-08-02T13:11:55.771259: step 7507, loss 0.481981.
Train: 2018-08-02T13:11:55.927506: step 7508, loss 0.51031.
Train: 2018-08-02T13:11:56.083717: step 7509, loss 0.454644.
Train: 2018-08-02T13:11:56.239930: step 7510, loss 0.459562.
Test: 2018-08-02T13:11:56.708565: step 7510, loss 0.548404.
Train: 2018-08-02T13:11:56.864783: step 7511, loss 0.505132.
Train: 2018-08-02T13:11:57.020968: step 7512, loss 0.567131.
Train: 2018-08-02T13:11:57.177208: step 7513, loss 0.625039.
Train: 2018-08-02T13:11:57.333420: step 7514, loss 0.512691.
Train: 2018-08-02T13:11:57.489608: step 7515, loss 0.559801.
Train: 2018-08-02T13:11:57.645850: step 7516, loss 0.507332.
Train: 2018-08-02T13:11:57.786412: step 7517, loss 0.545043.
Train: 2018-08-02T13:11:57.942626: step 7518, loss 0.521391.
Train: 2018-08-02T13:11:58.098869: step 7519, loss 0.503182.
Train: 2018-08-02T13:11:58.255052: step 7520, loss 0.497735.
Test: 2018-08-02T13:11:58.723693: step 7520, loss 0.552784.
Train: 2018-08-02T13:11:58.879931: step 7521, loss 0.457665.
Train: 2018-08-02T13:11:59.020498: step 7522, loss 0.511974.
Train: 2018-08-02T13:11:59.176741: step 7523, loss 0.529814.
Train: 2018-08-02T13:11:59.332950: step 7524, loss 0.453398.
Train: 2018-08-02T13:11:59.489168: step 7525, loss 0.623103.
Train: 2018-08-02T13:11:59.645382: step 7526, loss 0.54572.
Train: 2018-08-02T13:11:59.785974: step 7527, loss 0.56506.
Train: 2018-08-02T13:11:59.957810: step 7528, loss 0.527113.
Train: 2018-08-02T13:12:00.114022: step 7529, loss 0.516.
Train: 2018-08-02T13:12:00.270207: step 7530, loss 0.529997.
Test: 2018-08-02T13:12:00.723225: step 7530, loss 0.558181.
Train: 2018-08-02T13:12:00.879438: step 7531, loss 0.569811.
Train: 2018-08-02T13:12:01.035681: step 7532, loss 0.528087.
Train: 2018-08-02T13:12:01.191898: step 7533, loss 0.578331.
Train: 2018-08-02T13:12:01.348108: step 7534, loss 0.515706.
Train: 2018-08-02T13:12:01.504322: step 7535, loss 0.535916.
Train: 2018-08-02T13:12:01.660505: step 7536, loss 0.513927.
Train: 2018-08-02T13:12:01.816732: step 7537, loss 0.470629.
Train: 2018-08-02T13:12:01.957349: step 7538, loss 0.504069.
Train: 2018-08-02T13:12:02.113554: step 7539, loss 0.568204.
Train: 2018-08-02T13:12:02.285393: step 7540, loss 0.544433.
Test: 2018-08-02T13:12:02.738402: step 7540, loss 0.547701.
Train: 2018-08-02T13:12:02.894615: step 7541, loss 0.443264.
Train: 2018-08-02T13:12:03.066425: step 7542, loss 0.453353.
Train: 2018-08-02T13:12:03.222670: step 7543, loss 0.472973.
Train: 2018-08-02T13:12:03.378853: step 7544, loss 0.529677.
Train: 2018-08-02T13:12:03.535091: step 7545, loss 0.520292.
Train: 2018-08-02T13:12:03.691313: step 7546, loss 0.509633.
Train: 2018-08-02T13:12:03.847523: step 7547, loss 0.617434.
Train: 2018-08-02T13:12:04.003737: step 7548, loss 0.528656.
Train: 2018-08-02T13:12:04.159950: step 7549, loss 0.594064.
Train: 2018-08-02T13:12:04.269269: step 7550, loss 0.518495.
Test: 2018-08-02T13:12:04.722288: step 7550, loss 0.564153.
Train: 2018-08-02T13:12:04.878532: step 7551, loss 0.622085.
Train: 2018-08-02T13:12:05.034745: step 7552, loss 0.564153.
Train: 2018-08-02T13:12:05.190967: step 7553, loss 0.524529.
Train: 2018-08-02T13:12:05.347142: step 7554, loss 0.513985.
Train: 2018-08-02T13:12:05.503356: step 7555, loss 0.540633.
Train: 2018-08-02T13:12:05.659568: step 7556, loss 0.469202.
Train: 2018-08-02T13:12:05.815814: step 7557, loss 0.529975.
Train: 2018-08-02T13:12:05.971995: step 7558, loss 0.587236.
Train: 2018-08-02T13:12:06.112587: step 7559, loss 0.43667.
Train: 2018-08-02T13:12:06.284456: step 7560, loss 0.42784.
Test: 2018-08-02T13:12:06.737441: step 7560, loss 0.547128.
Train: 2018-08-02T13:12:06.893684: step 7561, loss 0.428768.
Train: 2018-08-02T13:12:07.049901: step 7562, loss 0.398186.
Train: 2018-08-02T13:12:07.206111: step 7563, loss 0.529771.
Train: 2018-08-02T13:12:07.362321: step 7564, loss 0.601995.
Train: 2018-08-02T13:12:07.518538: step 7565, loss 0.618613.
Train: 2018-08-02T13:12:07.674753: step 7566, loss 0.55142.
Train: 2018-08-02T13:12:07.830935: step 7567, loss 0.562677.
Train: 2018-08-02T13:12:07.987173: step 7568, loss 0.55466.
Train: 2018-08-02T13:12:08.143387: step 7569, loss 0.52307.
Train: 2018-08-02T13:12:08.299606: step 7570, loss 0.558703.
Test: 2018-08-02T13:12:08.752597: step 7570, loss 0.557447.
Train: 2018-08-02T13:12:08.908841: step 7571, loss 0.529451.
Train: 2018-08-02T13:12:09.065022: step 7572, loss 0.543264.
Train: 2018-08-02T13:12:09.221260: step 7573, loss 0.526445.
Train: 2018-08-02T13:12:09.377478: step 7574, loss 0.515098.
Train: 2018-08-02T13:12:09.533691: step 7575, loss 0.466225.
Train: 2018-08-02T13:12:09.689908: step 7576, loss 0.530386.
Train: 2018-08-02T13:12:09.846118: step 7577, loss 0.55535.
Train: 2018-08-02T13:12:10.002332: step 7578, loss 0.511742.
Train: 2018-08-02T13:12:10.142924: step 7579, loss 0.510225.
Train: 2018-08-02T13:12:10.299135: step 7580, loss 0.475881.
Test: 2018-08-02T13:12:10.767747: step 7580, loss 0.547516.
Train: 2018-08-02T13:12:10.924004: step 7581, loss 0.500387.
Train: 2018-08-02T13:12:11.080174: step 7582, loss 0.582977.
Train: 2018-08-02T13:12:11.236387: step 7583, loss 0.501167.
Train: 2018-08-02T13:12:11.392631: step 7584, loss 0.572256.
Train: 2018-08-02T13:12:11.548848: step 7585, loss 0.50803.
Train: 2018-08-02T13:12:11.689440: step 7586, loss 0.486333.
Train: 2018-08-02T13:12:11.845650: step 7587, loss 0.447143.
Train: 2018-08-02T13:12:12.001863: step 7588, loss 0.562295.
Train: 2018-08-02T13:12:12.158047: step 7589, loss 0.538719.
Train: 2018-08-02T13:12:12.314293: step 7590, loss 0.469985.
Test: 2018-08-02T13:12:12.782933: step 7590, loss 0.549616.
Train: 2018-08-02T13:12:12.939147: step 7591, loss 0.484979.
Train: 2018-08-02T13:12:13.095361: step 7592, loss 0.466134.
Train: 2018-08-02T13:12:13.251571: step 7593, loss 0.568625.
Train: 2018-08-02T13:12:13.407784: step 7594, loss 0.428443.
Train: 2018-08-02T13:12:13.563966: step 7595, loss 0.606229.
Train: 2018-08-02T13:12:13.704589: step 7596, loss 0.482344.
Train: 2018-08-02T13:12:13.876397: step 7597, loss 0.500978.
Train: 2018-08-02T13:12:14.032609: step 7598, loss 0.512578.
Train: 2018-08-02T13:12:14.188870: step 7599, loss 0.51693.
Train: 2018-08-02T13:12:14.345060: step 7600, loss 0.573525.
Test: 2018-08-02T13:12:14.798053: step 7600, loss 0.548564.
Train: 2018-08-02T13:12:15.438532: step 7601, loss 0.501948.
Train: 2018-08-02T13:12:15.610394: step 7602, loss 0.441973.
Train: 2018-08-02T13:12:15.766584: step 7603, loss 0.546817.
Train: 2018-08-02T13:12:15.907198: step 7604, loss 0.450235.
Train: 2018-08-02T13:12:16.063415: step 7605, loss 0.559588.
Train: 2018-08-02T13:12:16.219596: step 7606, loss 0.608053.
Train: 2018-08-02T13:12:16.375841: step 7607, loss 0.539787.
Train: 2018-08-02T13:12:16.547678: step 7608, loss 0.524746.
Train: 2018-08-02T13:12:16.703857: step 7609, loss 0.510993.
Train: 2018-08-02T13:12:16.844479: step 7610, loss 0.49109.
Test: 2018-08-02T13:12:17.313092: step 7610, loss 0.550359.
Train: 2018-08-02T13:12:17.469333: step 7611, loss 0.615039.
Train: 2018-08-02T13:12:17.609924: step 7612, loss 0.5841.
Train: 2018-08-02T13:12:17.766138: step 7613, loss 0.590784.
Train: 2018-08-02T13:12:17.922356: step 7614, loss 0.530875.
Train: 2018-08-02T13:12:18.078565: step 7615, loss 0.516491.
Train: 2018-08-02T13:12:18.250402: step 7616, loss 0.483285.
Train: 2018-08-02T13:12:18.406589: step 7617, loss 0.529123.
Train: 2018-08-02T13:12:18.562830: step 7618, loss 0.451579.
Train: 2018-08-02T13:12:18.719041: step 7619, loss 0.492969.
Train: 2018-08-02T13:12:18.875255: step 7620, loss 0.477329.
Test: 2018-08-02T13:12:19.328279: step 7620, loss 0.562952.
Train: 2018-08-02T13:12:19.484488: step 7621, loss 0.632352.
Train: 2018-08-02T13:12:19.640708: step 7622, loss 0.472831.
Train: 2018-08-02T13:12:19.812506: step 7623, loss 0.49722.
Train: 2018-08-02T13:12:19.968748: step 7624, loss 0.552468.
Train: 2018-08-02T13:12:20.124962: step 7625, loss 0.454329.
Train: 2018-08-02T13:12:20.281146: step 7626, loss 0.479748.
Train: 2018-08-02T13:12:20.437387: step 7627, loss 0.575021.
Train: 2018-08-02T13:12:20.593570: step 7628, loss 0.615781.
Train: 2018-08-02T13:12:20.749800: step 7629, loss 0.5014.
Train: 2018-08-02T13:12:20.906028: step 7630, loss 0.607053.
Test: 2018-08-02T13:12:21.359018: step 7630, loss 0.547092.
Train: 2018-08-02T13:12:21.515279: step 7631, loss 0.512846.
Train: 2018-08-02T13:12:21.671444: step 7632, loss 0.502031.
Train: 2018-08-02T13:12:21.827681: step 7633, loss 0.574656.
Train: 2018-08-02T13:12:21.983897: step 7634, loss 0.578312.
Train: 2018-08-02T13:12:22.140085: step 7635, loss 0.563869.
Train: 2018-08-02T13:12:22.296333: step 7636, loss 0.487159.
Train: 2018-08-02T13:12:22.452541: step 7637, loss 0.518551.
Train: 2018-08-02T13:12:22.608754: step 7638, loss 0.549958.
Train: 2018-08-02T13:12:22.764939: step 7639, loss 0.538847.
Train: 2018-08-02T13:12:22.921180: step 7640, loss 0.479875.
Test: 2018-08-02T13:12:23.389791: step 7640, loss 0.547851.
Train: 2018-08-02T13:12:23.546035: step 7641, loss 0.540114.
Train: 2018-08-02T13:12:23.702255: step 7642, loss 0.508729.
Train: 2018-08-02T13:12:23.858433: step 7643, loss 0.49055.
Train: 2018-08-02T13:12:24.014675: step 7644, loss 0.530555.
Train: 2018-08-02T13:12:24.155263: step 7645, loss 0.511875.
Train: 2018-08-02T13:12:24.311452: step 7646, loss 0.571681.
Train: 2018-08-02T13:12:24.467664: step 7647, loss 0.543303.
Train: 2018-08-02T13:12:24.623907: step 7648, loss 0.48862.
Train: 2018-08-02T13:12:24.780121: step 7649, loss 0.510275.
Train: 2018-08-02T13:12:24.936304: step 7650, loss 0.548079.
Test: 2018-08-02T13:12:25.404945: step 7650, loss 0.547499.
Train: 2018-08-02T13:12:25.561187: step 7651, loss 0.550082.
Train: 2018-08-02T13:12:25.717405: step 7652, loss 0.560961.
Train: 2018-08-02T13:12:25.873619: step 7653, loss 0.513586.
Train: 2018-08-02T13:12:26.029832: step 7654, loss 0.598521.
Train: 2018-08-02T13:12:26.186046: step 7655, loss 0.475538.
Train: 2018-08-02T13:12:26.342252: step 7656, loss 0.516619.
Train: 2018-08-02T13:12:26.498492: step 7657, loss 0.547682.
Train: 2018-08-02T13:12:26.654653: step 7658, loss 0.581753.
Train: 2018-08-02T13:12:26.810867: step 7659, loss 0.570745.
Train: 2018-08-02T13:12:26.967080: step 7660, loss 0.522839.
Test: 2018-08-02T13:12:27.435719: step 7660, loss 0.548575.
Train: 2018-08-02T13:12:27.591959: step 7661, loss 0.560424.
Train: 2018-08-02T13:12:27.748176: step 7662, loss 0.565663.
Train: 2018-08-02T13:12:27.904359: step 7663, loss 0.533952.
Train: 2018-08-02T13:12:28.060574: step 7664, loss 0.476744.
Train: 2018-08-02T13:12:28.216786: step 7665, loss 0.47368.
Train: 2018-08-02T13:12:28.372999: step 7666, loss 0.485523.
Train: 2018-08-02T13:12:28.529239: step 7667, loss 0.583229.
Train: 2018-08-02T13:12:28.685461: step 7668, loss 0.544887.
Train: 2018-08-02T13:12:28.857261: step 7669, loss 0.498884.
Train: 2018-08-02T13:12:28.997883: step 7670, loss 0.460113.
Test: 2018-08-02T13:12:29.466495: step 7670, loss 0.547754.
Train: 2018-08-02T13:12:29.622737: step 7671, loss 0.58673.
Train: 2018-08-02T13:12:29.778950: step 7672, loss 0.477607.
Train: 2018-08-02T13:12:29.935133: step 7673, loss 0.540528.
Train: 2018-08-02T13:12:30.091346: step 7674, loss 0.465088.
Train: 2018-08-02T13:12:30.247560: step 7675, loss 0.609205.
Train: 2018-08-02T13:12:30.403807: step 7676, loss 0.558871.
Train: 2018-08-02T13:12:30.559989: step 7677, loss 0.511662.
Train: 2018-08-02T13:12:30.716232: step 7678, loss 0.526273.
Train: 2018-08-02T13:12:30.872415: step 7679, loss 0.501532.
Train: 2018-08-02T13:12:31.028664: step 7680, loss 0.440637.
Test: 2018-08-02T13:12:31.481646: step 7680, loss 0.546849.
Train: 2018-08-02T13:12:31.653514: step 7681, loss 0.587551.
Train: 2018-08-02T13:12:31.794098: step 7682, loss 0.48389.
Train: 2018-08-02T13:12:31.950287: step 7683, loss 0.497856.
Train: 2018-08-02T13:12:32.106530: step 7684, loss 0.577206.
Train: 2018-08-02T13:12:32.262744: step 7685, loss 0.51743.
Train: 2018-08-02T13:12:32.418953: step 7686, loss 0.565744.
Train: 2018-08-02T13:12:32.575171: step 7687, loss 0.438638.
Train: 2018-08-02T13:12:32.731354: step 7688, loss 0.596295.
Train: 2018-08-02T13:12:32.887567: step 7689, loss 0.49986.
Train: 2018-08-02T13:12:33.043781: step 7690, loss 0.520867.
Test: 2018-08-02T13:12:33.528044: step 7690, loss 0.548098.
Train: 2018-08-02T13:12:33.684290: step 7691, loss 0.465801.
Train: 2018-08-02T13:12:33.840469: step 7692, loss 0.453645.
Train: 2018-08-02T13:12:33.996713: step 7693, loss 0.537325.
Train: 2018-08-02T13:12:34.152929: step 7694, loss 0.48419.
Train: 2018-08-02T13:12:34.309147: step 7695, loss 0.552559.
Train: 2018-08-02T13:12:34.465324: step 7696, loss 0.552848.
Train: 2018-08-02T13:12:34.621566: step 7697, loss 0.563363.
Train: 2018-08-02T13:12:34.777750: step 7698, loss 0.450696.
Train: 2018-08-02T13:12:34.933964: step 7699, loss 0.539037.
Train: 2018-08-02T13:12:35.090230: step 7700, loss 0.527372.
Test: 2018-08-02T13:12:35.558817: step 7700, loss 0.547065.
Train: 2018-08-02T13:12:36.183704: step 7701, loss 0.499971.
Train: 2018-08-02T13:12:36.339913: step 7702, loss 0.51681.
Train: 2018-08-02T13:12:36.496097: step 7703, loss 0.563096.
Train: 2018-08-02T13:12:36.652340: step 7704, loss 0.502523.
Train: 2018-08-02T13:12:36.808558: step 7705, loss 0.510061.
Train: 2018-08-02T13:12:36.964737: step 7706, loss 0.48923.
Train: 2018-08-02T13:12:37.120981: step 7707, loss 0.496141.
Train: 2018-08-02T13:12:37.277186: step 7708, loss 0.592957.
Train: 2018-08-02T13:12:37.433412: step 7709, loss 0.447826.
Train: 2018-08-02T13:12:37.574000: step 7710, loss 0.512377.
Test: 2018-08-02T13:12:38.042611: step 7710, loss 0.549271.
Train: 2018-08-02T13:12:38.198854: step 7711, loss 0.478469.
Train: 2018-08-02T13:12:38.355067: step 7712, loss 0.531732.
Train: 2018-08-02T13:12:38.511249: step 7713, loss 0.449615.
Train: 2018-08-02T13:12:38.651843: step 7714, loss 0.577419.
Train: 2018-08-02T13:12:38.823677: step 7715, loss 0.571036.
Train: 2018-08-02T13:12:38.979890: step 7716, loss 0.569245.
Train: 2018-08-02T13:12:39.120517: step 7717, loss 0.470207.
Train: 2018-08-02T13:12:39.292346: step 7718, loss 0.49896.
Train: 2018-08-02T13:12:39.448532: step 7719, loss 0.490571.
Train: 2018-08-02T13:12:39.604775: step 7720, loss 0.524567.
Test: 2018-08-02T13:12:40.057766: step 7720, loss 0.547962.
Train: 2018-08-02T13:12:40.213978: step 7721, loss 0.522057.
Train: 2018-08-02T13:12:40.370190: step 7722, loss 0.460648.
Train: 2018-08-02T13:12:40.526437: step 7723, loss 0.543994.
Train: 2018-08-02T13:12:40.682646: step 7724, loss 0.55015.
Train: 2018-08-02T13:12:40.838861: step 7725, loss 0.507261.
Train: 2018-08-02T13:12:40.995078: step 7726, loss 0.47498.
Train: 2018-08-02T13:12:41.151258: step 7727, loss 0.523119.
Train: 2018-08-02T13:12:41.323122: step 7728, loss 0.519934.
Train: 2018-08-02T13:12:41.479345: step 7729, loss 0.491158.
Train: 2018-08-02T13:12:41.619897: step 7730, loss 0.534619.
Test: 2018-08-02T13:12:42.088538: step 7730, loss 0.548672.
Train: 2018-08-02T13:12:42.244777: step 7731, loss 0.575699.
Train: 2018-08-02T13:12:42.400966: step 7732, loss 0.556112.
Train: 2018-08-02T13:12:42.557217: step 7733, loss 0.59588.
Train: 2018-08-02T13:12:42.713422: step 7734, loss 0.482014.
Train: 2018-08-02T13:12:42.885226: step 7735, loss 0.563854.
Train: 2018-08-02T13:12:43.025853: step 7736, loss 0.522866.
Train: 2018-08-02T13:12:43.197653: step 7737, loss 0.567864.
Train: 2018-08-02T13:12:43.353898: step 7738, loss 0.477039.
Train: 2018-08-02T13:12:43.510110: step 7739, loss 0.516748.
Train: 2018-08-02T13:12:43.666327: step 7740, loss 0.52661.
Test: 2018-08-02T13:12:44.134934: step 7740, loss 0.549267.
Train: 2018-08-02T13:12:44.338011: step 7741, loss 0.523695.
Train: 2018-08-02T13:12:44.494249: step 7742, loss 0.564489.
Train: 2018-08-02T13:12:44.650468: step 7743, loss 0.493041.
Train: 2018-08-02T13:12:44.791066: step 7744, loss 0.519212.
Train: 2018-08-02T13:12:44.947267: step 7745, loss 0.464244.
Train: 2018-08-02T13:12:45.103487: step 7746, loss 0.552389.
Train: 2018-08-02T13:12:45.259671: step 7747, loss 0.5783.
Train: 2018-08-02T13:12:45.415914: step 7748, loss 0.390411.
Train: 2018-08-02T13:12:45.572122: step 7749, loss 0.574752.
Train: 2018-08-02T13:12:45.728349: step 7750, loss 0.569798.
Test: 2018-08-02T13:12:46.196981: step 7750, loss 0.547296.
Train: 2018-08-02T13:12:46.353172: step 7751, loss 0.570449.
Train: 2018-08-02T13:12:46.509409: step 7752, loss 0.556868.
Train: 2018-08-02T13:12:46.665591: step 7753, loss 0.50384.
Train: 2018-08-02T13:12:46.821835: step 7754, loss 0.526445.
Train: 2018-08-02T13:12:46.978019: step 7755, loss 0.524005.
Train: 2018-08-02T13:12:47.134261: step 7756, loss 0.538392.
Train: 2018-08-02T13:12:47.290445: step 7757, loss 0.547664.
Train: 2018-08-02T13:12:47.446658: step 7758, loss 0.504564.
Train: 2018-08-02T13:12:47.602901: step 7759, loss 0.480053.
Train: 2018-08-02T13:12:47.759086: step 7760, loss 0.512881.
Test: 2018-08-02T13:12:48.212134: step 7760, loss 0.547018.
Train: 2018-08-02T13:12:48.368348: step 7761, loss 0.471644.
Train: 2018-08-02T13:12:48.524531: step 7762, loss 0.429825.
Train: 2018-08-02T13:12:48.680743: step 7763, loss 0.52323.
Train: 2018-08-02T13:12:48.836989: step 7764, loss 0.53219.
Train: 2018-08-02T13:12:48.993201: step 7765, loss 0.486271.
Train: 2018-08-02T13:12:49.149385: step 7766, loss 0.495932.
Train: 2018-08-02T13:12:49.305642: step 7767, loss 0.561283.
Train: 2018-08-02T13:12:49.461841: step 7768, loss 0.57582.
Train: 2018-08-02T13:12:49.618055: step 7769, loss 0.461612.
Train: 2018-08-02T13:12:49.774268: step 7770, loss 0.456321.
Test: 2018-08-02T13:12:50.242909: step 7770, loss 0.547114.
Train: 2018-08-02T13:12:50.399116: step 7771, loss 0.471891.
Train: 2018-08-02T13:12:50.555339: step 7772, loss 0.515258.
Train: 2018-08-02T13:12:50.711550: step 7773, loss 0.564388.
Train: 2018-08-02T13:12:50.867766: step 7774, loss 0.495176.
Train: 2018-08-02T13:12:51.023975: step 7775, loss 0.501513.
Train: 2018-08-02T13:12:51.180192: step 7776, loss 0.514961.
Train: 2018-08-02T13:12:51.320783: step 7777, loss 0.518047.
Train: 2018-08-02T13:12:51.492610: step 7778, loss 0.636.
Train: 2018-08-02T13:12:51.633208: step 7779, loss 0.543951.
Train: 2018-08-02T13:12:51.789397: step 7780, loss 0.499902.
Test: 2018-08-02T13:12:52.242411: step 7780, loss 0.549469.
Train: 2018-08-02T13:12:52.398655: step 7781, loss 0.504208.
Train: 2018-08-02T13:12:52.554875: step 7782, loss 0.501839.
Train: 2018-08-02T13:12:52.711082: step 7783, loss 0.510342.
Train: 2018-08-02T13:12:52.867296: step 7784, loss 0.487563.
Train: 2018-08-02T13:12:53.023507: step 7785, loss 0.521874.
Train: 2018-08-02T13:12:53.179724: step 7786, loss 0.542445.
Train: 2018-08-02T13:12:53.335903: step 7787, loss 0.487147.
Train: 2018-08-02T13:12:53.492148: step 7788, loss 0.57094.
Train: 2018-08-02T13:12:53.648364: step 7789, loss 0.512721.
Train: 2018-08-02T13:12:53.820190: step 7790, loss 0.522251.
Test: 2018-08-02T13:12:54.273209: step 7790, loss 0.549435.
Train: 2018-08-02T13:12:54.429398: step 7791, loss 0.548546.
Train: 2018-08-02T13:12:54.585641: step 7792, loss 0.622807.
Train: 2018-08-02T13:12:54.726202: step 7793, loss 0.512109.
Train: 2018-08-02T13:12:54.882447: step 7794, loss 0.595585.
Train: 2018-08-02T13:12:55.038660: step 7795, loss 0.579975.
Train: 2018-08-02T13:12:55.194843: step 7796, loss 0.503396.
Train: 2018-08-02T13:12:55.351059: step 7797, loss 0.540131.
Train: 2018-08-02T13:12:55.507301: step 7798, loss 0.568237.
Train: 2018-08-02T13:12:55.663485: step 7799, loss 0.531692.
Train: 2018-08-02T13:12:55.819727: step 7800, loss 0.523846.
Test: 2018-08-02T13:12:56.272759: step 7800, loss 0.54948.
Train: 2018-08-02T13:12:56.881980: step 7801, loss 0.52282.
Train: 2018-08-02T13:12:57.038203: step 7802, loss 0.585496.
Train: 2018-08-02T13:12:57.194405: step 7803, loss 0.503403.
Train: 2018-08-02T13:12:57.350620: step 7804, loss 0.560418.
Train: 2018-08-02T13:12:57.506833: step 7805, loss 0.464817.
Train: 2018-08-02T13:12:57.647428: step 7806, loss 0.538345.
Train: 2018-08-02T13:12:57.803609: step 7807, loss 0.472362.
Train: 2018-08-02T13:12:57.959851: step 7808, loss 0.521176.
Train: 2018-08-02T13:12:58.116061: step 7809, loss 0.547856.
Train: 2018-08-02T13:12:58.272274: step 7810, loss 0.542665.
Test: 2018-08-02T13:12:58.740919: step 7810, loss 0.548048.
Train: 2018-08-02T13:12:58.897126: step 7811, loss 0.421931.
Train: 2018-08-02T13:12:59.053340: step 7812, loss 0.482005.
Train: 2018-08-02T13:12:59.193931: step 7813, loss 0.495254.
Train: 2018-08-02T13:12:59.350121: step 7814, loss 0.490984.
Train: 2018-08-02T13:12:59.506333: step 7815, loss 0.485724.
Train: 2018-08-02T13:12:59.662548: step 7816, loss 0.518603.
Train: 2018-08-02T13:12:59.818786: step 7817, loss 0.512574.
Train: 2018-08-02T13:12:59.990595: step 7818, loss 0.510373.
Train: 2018-08-02T13:13:00.146810: step 7819, loss 0.509833.
Train: 2018-08-02T13:13:00.303047: step 7820, loss 0.584777.
Test: 2018-08-02T13:13:00.756066: step 7820, loss 0.546229.
Train: 2018-08-02T13:13:00.927923: step 7821, loss 0.502601.
Train: 2018-08-02T13:13:01.084089: step 7822, loss 0.552548.
Train: 2018-08-02T13:13:01.240303: step 7823, loss 0.48549.
Train: 2018-08-02T13:13:01.396518: step 7824, loss 0.581818.
Train: 2018-08-02T13:13:01.552729: step 7825, loss 0.569442.
Train: 2018-08-02T13:13:01.693321: step 7826, loss 0.529051.
Train: 2018-08-02T13:13:01.849535: step 7827, loss 0.541034.
Train: 2018-08-02T13:13:02.021407: step 7828, loss 0.522082.
Train: 2018-08-02T13:13:02.177612: step 7829, loss 0.546461.
Train: 2018-08-02T13:13:02.333814: step 7830, loss 0.524336.
Test: 2018-08-02T13:13:02.786844: step 7830, loss 0.549254.
Train: 2018-08-02T13:13:02.943029: step 7831, loss 0.553017.
Train: 2018-08-02T13:13:03.099277: step 7832, loss 0.543629.
Train: 2018-08-02T13:13:03.255457: step 7833, loss 0.452166.
Train: 2018-08-02T13:13:03.427320: step 7834, loss 0.569599.
Train: 2018-08-02T13:13:03.583506: step 7835, loss 0.51233.
Train: 2018-08-02T13:13:03.739748: step 7836, loss 0.544179.
Train: 2018-08-02T13:13:03.880340: step 7837, loss 0.521066.
Train: 2018-08-02T13:13:04.036552: step 7838, loss 0.420996.
Train: 2018-08-02T13:13:04.192766: step 7839, loss 0.549659.
Train: 2018-08-02T13:13:04.348979: step 7840, loss 0.502826.
Test: 2018-08-02T13:13:04.817590: step 7840, loss 0.548757.
Train: 2018-08-02T13:13:04.973803: step 7841, loss 0.444112.
Train: 2018-08-02T13:13:05.130047: step 7842, loss 0.512525.
Train: 2018-08-02T13:13:05.286261: step 7843, loss 0.569329.
Train: 2018-08-02T13:13:05.426856: step 7844, loss 0.460576.
Train: 2018-08-02T13:13:05.598658: step 7845, loss 0.474514.
Train: 2018-08-02T13:13:05.754879: step 7846, loss 0.473981.
Train: 2018-08-02T13:13:05.911085: step 7847, loss 0.507465.
Train: 2018-08-02T13:13:06.051709: step 7848, loss 0.498514.
Train: 2018-08-02T13:13:06.207914: step 7849, loss 0.485526.
Train: 2018-08-02T13:13:06.379724: step 7850, loss 0.520848.
Test: 2018-08-02T13:13:06.832774: step 7850, loss 0.546497.
Train: 2018-08-02T13:13:06.988986: step 7851, loss 0.539947.
Train: 2018-08-02T13:13:07.098305: step 7852, loss 0.649369.
Train: 2018-08-02T13:13:07.254519: step 7853, loss 0.461344.
Train: 2018-08-02T13:13:07.410732: step 7854, loss 0.48366.
Train: 2018-08-02T13:13:07.566977: step 7855, loss 0.520145.
Train: 2018-08-02T13:13:07.723191: step 7856, loss 0.538445.
Train: 2018-08-02T13:13:07.879374: step 7857, loss 0.417632.
Train: 2018-08-02T13:13:08.035617: step 7858, loss 0.454359.
Train: 2018-08-02T13:13:08.191802: step 7859, loss 0.463563.
Train: 2018-08-02T13:13:08.348039: step 7860, loss 0.517479.
Test: 2018-08-02T13:13:08.801058: step 7860, loss 0.555245.
Train: 2018-08-02T13:13:08.957275: step 7861, loss 0.483425.
Train: 2018-08-02T13:13:09.113459: step 7862, loss 0.523331.
Train: 2018-08-02T13:13:09.269703: step 7863, loss 0.491316.
Train: 2018-08-02T13:13:09.425920: step 7864, loss 0.472827.
Train: 2018-08-02T13:13:09.582130: step 7865, loss 0.483922.
Train: 2018-08-02T13:13:09.738365: step 7866, loss 0.474926.
Train: 2018-08-02T13:13:09.894557: step 7867, loss 0.516385.
Train: 2018-08-02T13:13:10.050765: step 7868, loss 0.52756.
Train: 2018-08-02T13:13:10.206954: step 7869, loss 0.543834.
Train: 2018-08-02T13:13:10.363167: step 7870, loss 0.526547.
Test: 2018-08-02T13:13:10.831807: step 7870, loss 0.553031.
Train: 2018-08-02T13:13:10.988046: step 7871, loss 0.580654.
Train: 2018-08-02T13:13:11.144263: step 7872, loss 0.516828.
Train: 2018-08-02T13:13:11.284825: step 7873, loss 0.56426.
Train: 2018-08-02T13:13:11.441070: step 7874, loss 0.564438.
Train: 2018-08-02T13:13:11.597283: step 7875, loss 0.510682.
Train: 2018-08-02T13:13:11.753466: step 7876, loss 0.521171.
Train: 2018-08-02T13:13:11.909680: step 7877, loss 0.558425.
Train: 2018-08-02T13:13:12.050304: step 7878, loss 0.555948.
Train: 2018-08-02T13:13:12.222107: step 7879, loss 0.527607.
Train: 2018-08-02T13:13:12.378323: step 7880, loss 0.558245.
Test: 2018-08-02T13:13:12.831338: step 7880, loss 0.551564.
Train: 2018-08-02T13:13:12.987582: step 7881, loss 0.504373.
Train: 2018-08-02T13:13:13.143766: step 7882, loss 0.520903.
Train: 2018-08-02T13:13:13.299978: step 7883, loss 0.516934.
Train: 2018-08-02T13:13:13.456222: step 7884, loss 0.491024.
Train: 2018-08-02T13:13:13.596784: step 7885, loss 0.550188.
Train: 2018-08-02T13:13:13.753028: step 7886, loss 0.524599.
Train: 2018-08-02T13:13:13.909241: step 7887, loss 0.494762.
Train: 2018-08-02T13:13:14.065455: step 7888, loss 0.497665.
Train: 2018-08-02T13:13:14.221666: step 7889, loss 0.526689.
Train: 2018-08-02T13:13:14.377851: step 7890, loss 0.469868.
Test: 2018-08-02T13:13:14.830892: step 7890, loss 0.548551.
Train: 2018-08-02T13:13:14.987109: step 7891, loss 0.459185.
Train: 2018-08-02T13:13:15.143327: step 7892, loss 0.627.
Train: 2018-08-02T13:13:15.299544: step 7893, loss 0.526587.
Train: 2018-08-02T13:13:15.455754: step 7894, loss 0.451045.
Train: 2018-08-02T13:13:15.611937: step 7895, loss 0.494836.
Train: 2018-08-02T13:13:15.768185: step 7896, loss 0.488939.
Train: 2018-08-02T13:13:15.924398: step 7897, loss 0.507025.
Train: 2018-08-02T13:13:16.080579: step 7898, loss 0.523315.
Train: 2018-08-02T13:13:16.236821: step 7899, loss 0.483856.
Train: 2018-08-02T13:13:16.377413: step 7900, loss 0.450767.
Test: 2018-08-02T13:13:16.846048: step 7900, loss 0.548622.
Train: 2018-08-02T13:13:17.548984: step 7901, loss 0.641469.
Train: 2018-08-02T13:13:17.689606: step 7902, loss 0.604791.
Train: 2018-08-02T13:13:17.845819: step 7903, loss 0.50515.
Train: 2018-08-02T13:13:18.002033: step 7904, loss 0.611385.
Train: 2018-08-02T13:13:18.158216: step 7905, loss 0.499276.
Train: 2018-08-02T13:13:18.298807: step 7906, loss 0.558095.
Train: 2018-08-02T13:13:18.455052: step 7907, loss 0.581352.
Train: 2018-08-02T13:13:18.611265: step 7908, loss 0.49854.
Train: 2018-08-02T13:13:18.767478: step 7909, loss 0.504733.
Train: 2018-08-02T13:13:18.923695: step 7910, loss 0.535095.
Test: 2018-08-02T13:13:19.376729: step 7910, loss 0.548247.
Train: 2018-08-02T13:13:19.532925: step 7911, loss 0.521704.
Train: 2018-08-02T13:13:19.689109: step 7912, loss 0.495943.
Train: 2018-08-02T13:13:19.845320: step 7913, loss 0.514556.
Train: 2018-08-02T13:13:20.001533: step 7914, loss 0.524237.
Train: 2018-08-02T13:13:20.157747: step 7915, loss 0.505785.
Train: 2018-08-02T13:13:20.298374: step 7916, loss 0.599684.
Train: 2018-08-02T13:13:20.470192: step 7917, loss 0.509617.
Train: 2018-08-02T13:13:20.610772: step 7918, loss 0.542073.
Train: 2018-08-02T13:13:20.767016: step 7919, loss 0.576993.
Train: 2018-08-02T13:13:20.923223: step 7920, loss 0.516777.
Test: 2018-08-02T13:13:21.391859: step 7920, loss 0.554437.
Train: 2018-08-02T13:13:21.548081: step 7921, loss 0.515664.
Train: 2018-08-02T13:13:21.704262: step 7922, loss 0.475271.
Train: 2018-08-02T13:13:21.860502: step 7923, loss 0.58558.
Train: 2018-08-02T13:13:22.016725: step 7924, loss 0.466462.
Train: 2018-08-02T13:13:22.157279: step 7925, loss 0.532588.
Train: 2018-08-02T13:13:22.313523: step 7926, loss 0.540229.
Train: 2018-08-02T13:13:22.469736: step 7927, loss 0.554932.
Train: 2018-08-02T13:13:22.625950: step 7928, loss 0.474983.
Train: 2018-08-02T13:13:22.782163: step 7929, loss 0.525621.
Train: 2018-08-02T13:13:22.938376: step 7930, loss 0.528256.
Test: 2018-08-02T13:13:23.406987: step 7930, loss 0.549739.
Train: 2018-08-02T13:13:23.563232: step 7931, loss 0.440336.
Train: 2018-08-02T13:13:23.703822: step 7932, loss 0.52961.
Train: 2018-08-02T13:13:23.860035: step 7933, loss 0.478518.
Train: 2018-08-02T13:13:24.016219: step 7934, loss 0.508232.
Train: 2018-08-02T13:13:24.172458: step 7935, loss 0.506074.
Train: 2018-08-02T13:13:24.328676: step 7936, loss 0.50904.
Train: 2018-08-02T13:13:24.484890: step 7937, loss 0.51148.
Train: 2018-08-02T13:13:24.641074: step 7938, loss 0.504862.
Train: 2018-08-02T13:13:24.797319: step 7939, loss 0.487232.
Train: 2018-08-02T13:13:24.953531: step 7940, loss 0.503004.
Test: 2018-08-02T13:13:25.406543: step 7940, loss 0.549136.
Train: 2018-08-02T13:13:25.562757: step 7941, loss 0.497912.
Train: 2018-08-02T13:13:25.718945: step 7942, loss 0.506765.
Train: 2018-08-02T13:13:25.875189: step 7943, loss 0.487512.
Train: 2018-08-02T13:13:26.031373: step 7944, loss 0.507092.
Train: 2018-08-02T13:13:26.187616: step 7945, loss 0.540533.
Train: 2018-08-02T13:13:26.343829: step 7946, loss 0.517474.
Train: 2018-08-02T13:13:26.500012: step 7947, loss 0.515923.
Train: 2018-08-02T13:13:26.656257: step 7948, loss 0.472791.
Train: 2018-08-02T13:13:26.812471: step 7949, loss 0.513348.
Train: 2018-08-02T13:13:26.968659: step 7950, loss 0.489954.
Test: 2018-08-02T13:13:27.421706: step 7950, loss 0.54794.
Train: 2018-08-02T13:13:27.577885: step 7951, loss 0.633661.
Train: 2018-08-02T13:13:27.734099: step 7952, loss 0.4897.
Train: 2018-08-02T13:13:27.890312: step 7953, loss 0.527688.
Train: 2018-08-02T13:13:28.030939: step 7954, loss 0.535536.
Train: 2018-08-02T13:13:28.202764: step 7955, loss 0.511108.
Train: 2018-08-02T13:13:28.358952: step 7956, loss 0.484189.
Train: 2018-08-02T13:13:28.515165: step 7957, loss 0.51383.
Train: 2018-08-02T13:13:28.671408: step 7958, loss 0.572499.
Train: 2018-08-02T13:13:28.827594: step 7959, loss 0.509436.
Train: 2018-08-02T13:13:28.983837: step 7960, loss 0.510869.
Test: 2018-08-02T13:13:29.452478: step 7960, loss 0.548862.
Train: 2018-08-02T13:13:29.608699: step 7961, loss 0.442273.
Train: 2018-08-02T13:13:29.764874: step 7962, loss 0.468606.
Train: 2018-08-02T13:13:29.921122: step 7963, loss 0.480261.
Train: 2018-08-02T13:13:30.077301: step 7964, loss 0.496557.
Train: 2018-08-02T13:13:30.233537: step 7965, loss 0.479023.
Train: 2018-08-02T13:13:30.389729: step 7966, loss 0.589606.
Train: 2018-08-02T13:13:30.561598: step 7967, loss 0.532736.
Train: 2018-08-02T13:13:30.717775: step 7968, loss 0.508775.
Train: 2018-08-02T13:13:30.858398: step 7969, loss 0.497091.
Train: 2018-08-02T13:13:31.014580: step 7970, loss 0.527904.
Test: 2018-08-02T13:13:31.483251: step 7970, loss 0.556492.
Train: 2018-08-02T13:13:31.639443: step 7971, loss 0.520908.
Train: 2018-08-02T13:13:31.795677: step 7972, loss 0.530145.
Train: 2018-08-02T13:13:31.951862: step 7973, loss 0.490606.
Train: 2018-08-02T13:13:32.108105: step 7974, loss 0.568277.
Train: 2018-08-02T13:13:32.264321: step 7975, loss 0.584526.
Train: 2018-08-02T13:13:32.420501: step 7976, loss 0.590231.
Train: 2018-08-02T13:13:32.576745: step 7977, loss 0.505096.
Train: 2018-08-02T13:13:32.732958: step 7978, loss 0.527412.
Train: 2018-08-02T13:13:32.889167: step 7979, loss 0.589936.
Train: 2018-08-02T13:13:33.045356: step 7980, loss 0.465925.
Test: 2018-08-02T13:13:33.514002: step 7980, loss 0.551965.
Train: 2018-08-02T13:13:33.654588: step 7981, loss 0.526115.
Train: 2018-08-02T13:13:33.810825: step 7982, loss 0.425256.
Train: 2018-08-02T13:13:33.967013: step 7983, loss 0.610701.
Train: 2018-08-02T13:13:34.138874: step 7984, loss 0.511365.
Train: 2018-08-02T13:13:34.295096: step 7985, loss 0.587205.
Train: 2018-08-02T13:13:34.451307: step 7986, loss 0.486529.
Train: 2018-08-02T13:13:34.607519: step 7987, loss 0.488751.
Train: 2018-08-02T13:13:34.763703: step 7988, loss 0.482671.
Train: 2018-08-02T13:13:34.919916: step 7989, loss 0.522225.
Train: 2018-08-02T13:13:35.076129: step 7990, loss 0.509862.
Test: 2018-08-02T13:13:35.529178: step 7990, loss 0.548011.
Train: 2018-08-02T13:13:35.685367: step 7991, loss 0.50526.
Train: 2018-08-02T13:13:35.841576: step 7992, loss 0.564147.
Train: 2018-08-02T13:13:35.997788: step 7993, loss 0.423245.
Train: 2018-08-02T13:13:36.154018: step 7994, loss 0.569143.
Train: 2018-08-02T13:13:36.310246: step 7995, loss 0.530254.
Train: 2018-08-02T13:13:36.466479: step 7996, loss 0.508986.
Train: 2018-08-02T13:13:36.622673: step 7997, loss 0.507946.
Train: 2018-08-02T13:13:36.778883: step 7998, loss 0.476179.
Train: 2018-08-02T13:13:36.935105: step 7999, loss 0.594904.
Train: 2018-08-02T13:13:37.091282: step 8000, loss 0.58847.
Test: 2018-08-02T13:13:37.544301: step 8000, loss 0.55111.
Train: 2018-08-02T13:13:38.184776: step 8001, loss 0.48526.
Train: 2018-08-02T13:13:38.340990: step 8002, loss 0.522747.
Train: 2018-08-02T13:13:38.450369: step 8003, loss 0.474845.
Train: 2018-08-02T13:13:38.606552: step 8004, loss 0.494178.
Train: 2018-08-02T13:13:38.762767: step 8005, loss 0.497285.
Train: 2018-08-02T13:13:38.919012: step 8006, loss 0.528919.
Train: 2018-08-02T13:13:39.075223: step 8007, loss 0.551893.
Train: 2018-08-02T13:13:39.231407: step 8008, loss 0.512017.
Train: 2018-08-02T13:13:39.387643: step 8009, loss 0.475695.
Train: 2018-08-02T13:13:39.528241: step 8010, loss 0.513114.
Test: 2018-08-02T13:13:39.996877: step 8010, loss 0.548254.
Train: 2018-08-02T13:13:40.199954: step 8011, loss 0.514511.
Train: 2018-08-02T13:13:40.371764: step 8012, loss 0.480472.
Train: 2018-08-02T13:13:40.528011: step 8013, loss 0.480898.
Train: 2018-08-02T13:13:40.684222: step 8014, loss 0.467781.
Train: 2018-08-02T13:13:40.840441: step 8015, loss 0.500109.
Train: 2018-08-02T13:13:40.996619: step 8016, loss 0.510936.
Train: 2018-08-02T13:13:41.152859: step 8017, loss 0.492023.
Train: 2018-08-02T13:13:41.309075: step 8018, loss 0.422718.
Train: 2018-08-02T13:13:41.465258: step 8019, loss 0.453025.
Train: 2018-08-02T13:13:41.621502: step 8020, loss 0.495232.
Test: 2018-08-02T13:13:42.074490: step 8020, loss 0.565248.
Train: 2018-08-02T13:13:42.230704: step 8021, loss 0.514574.
Train: 2018-08-02T13:13:42.386947: step 8022, loss 0.435052.
Train: 2018-08-02T13:13:42.543131: step 8023, loss 0.56415.
Train: 2018-08-02T13:13:42.699354: step 8024, loss 0.434947.
Train: 2018-08-02T13:13:42.855559: step 8025, loss 0.481244.
Train: 2018-08-02T13:13:43.027392: step 8026, loss 0.519661.
Train: 2018-08-02T13:13:43.183637: step 8027, loss 0.598671.
Train: 2018-08-02T13:13:43.339821: step 8028, loss 0.490894.
Train: 2018-08-02T13:13:43.496059: step 8029, loss 0.504116.
Train: 2018-08-02T13:13:43.652274: step 8030, loss 0.564207.
Test: 2018-08-02T13:13:44.105266: step 8030, loss 0.551539.
Train: 2018-08-02T13:13:44.261479: step 8031, loss 0.55814.
Train: 2018-08-02T13:13:44.417725: step 8032, loss 0.555465.
Train: 2018-08-02T13:13:44.573905: step 8033, loss 0.47688.
Train: 2018-08-02T13:13:44.714527: step 8034, loss 0.499262.
Train: 2018-08-02T13:13:44.870741: step 8035, loss 0.507125.
Train: 2018-08-02T13:13:45.026954: step 8036, loss 0.475007.
Train: 2018-08-02T13:13:45.183137: step 8037, loss 0.50971.
Train: 2018-08-02T13:13:45.339356: step 8038, loss 0.580392.
Train: 2018-08-02T13:13:45.495564: step 8039, loss 0.513139.
Train: 2018-08-02T13:13:45.636157: step 8040, loss 0.500122.
Test: 2018-08-02T13:13:46.104797: step 8040, loss 0.550818.
Train: 2018-08-02T13:13:46.261038: step 8041, loss 0.502518.
Train: 2018-08-02T13:13:46.417257: step 8042, loss 0.504694.
Train: 2018-08-02T13:13:46.573437: step 8043, loss 0.52066.
Train: 2018-08-02T13:13:46.729680: step 8044, loss 0.489637.
Train: 2018-08-02T13:13:46.885894: step 8045, loss 0.546737.
Train: 2018-08-02T13:13:47.042077: step 8046, loss 0.527552.
Train: 2018-08-02T13:13:47.198321: step 8047, loss 0.549608.
Train: 2018-08-02T13:13:47.354535: step 8048, loss 0.529375.
Train: 2018-08-02T13:13:47.510742: step 8049, loss 0.492727.
Train: 2018-08-02T13:13:47.666931: step 8050, loss 0.46869.
Test: 2018-08-02T13:13:48.135571: step 8050, loss 0.548607.
Train: 2018-08-02T13:13:48.291818: step 8051, loss 0.571373.
Train: 2018-08-02T13:13:48.432407: step 8052, loss 0.500109.
Train: 2018-08-02T13:13:48.588620: step 8053, loss 0.522041.
Train: 2018-08-02T13:13:48.744833: step 8054, loss 0.580672.
Train: 2018-08-02T13:13:48.901054: step 8055, loss 0.51985.
Train: 2018-08-02T13:13:49.057260: step 8056, loss 0.529439.
Train: 2018-08-02T13:13:49.213475: step 8057, loss 0.485415.
Train: 2018-08-02T13:13:49.369708: step 8058, loss 0.573379.
Train: 2018-08-02T13:13:49.525900: step 8059, loss 0.565245.
Train: 2018-08-02T13:13:49.682085: step 8060, loss 0.573185.
Test: 2018-08-02T13:13:50.135134: step 8060, loss 0.55653.
Train: 2018-08-02T13:13:50.291350: step 8061, loss 0.49216.
Train: 2018-08-02T13:13:50.447577: step 8062, loss 0.499097.
Train: 2018-08-02T13:13:50.603774: step 8063, loss 0.520042.
Train: 2018-08-02T13:13:50.759982: step 8064, loss 0.480257.
Train: 2018-08-02T13:13:50.916199: step 8065, loss 0.562594.
Train: 2018-08-02T13:13:51.072415: step 8066, loss 0.593208.
Train: 2018-08-02T13:13:51.244219: step 8067, loss 0.456239.
Train: 2018-08-02T13:13:51.400465: step 8068, loss 0.495674.
Train: 2018-08-02T13:13:51.556676: step 8069, loss 0.550489.
Train: 2018-08-02T13:13:51.712860: step 8070, loss 0.485429.
Test: 2018-08-02T13:13:52.165902: step 8070, loss 0.549214.
Train: 2018-08-02T13:13:52.337743: step 8071, loss 0.529743.
Train: 2018-08-02T13:13:52.493957: step 8072, loss 0.569937.
Train: 2018-08-02T13:13:52.634547: step 8073, loss 0.502556.
Train: 2018-08-02T13:13:52.790764: step 8074, loss 0.530952.
Train: 2018-08-02T13:13:52.946974: step 8075, loss 0.619444.
Train: 2018-08-02T13:13:53.103172: step 8076, loss 0.530967.
Train: 2018-08-02T13:13:53.259372: step 8077, loss 0.500552.
Train: 2018-08-02T13:13:53.431208: step 8078, loss 0.544038.
Train: 2018-08-02T13:13:53.587450: step 8079, loss 0.555831.
Train: 2018-08-02T13:13:53.743667: step 8080, loss 0.45702.
Test: 2018-08-02T13:13:54.208169: step 8080, loss 0.547269.
Train: 2018-08-02T13:13:54.364410: step 8081, loss 0.523057.
Train: 2018-08-02T13:13:54.504997: step 8082, loss 0.53688.
Train: 2018-08-02T13:13:54.661210: step 8083, loss 0.553904.
Train: 2018-08-02T13:13:54.817424: step 8084, loss 0.479591.
Train: 2018-08-02T13:13:54.973632: step 8085, loss 0.448765.
Train: 2018-08-02T13:13:55.129851: step 8086, loss 0.538271.
Train: 2018-08-02T13:13:55.286065: step 8087, loss 0.490116.
Train: 2018-08-02T13:13:55.442278: step 8088, loss 0.456693.
Train: 2018-08-02T13:13:55.598493: step 8089, loss 0.537488.
Train: 2018-08-02T13:13:55.754674: step 8090, loss 0.515861.
Test: 2018-08-02T13:13:56.223322: step 8090, loss 0.546964.
Train: 2018-08-02T13:13:56.379558: step 8091, loss 0.493349.
Train: 2018-08-02T13:13:56.535742: step 8092, loss 0.472524.
Train: 2018-08-02T13:13:56.691985: step 8093, loss 0.487602.
Train: 2018-08-02T13:13:56.832580: step 8094, loss 0.495845.
Train: 2018-08-02T13:13:56.988790: step 8095, loss 0.489008.
Train: 2018-08-02T13:13:57.144994: step 8096, loss 0.474557.
Train: 2018-08-02T13:13:57.316808: step 8097, loss 0.604402.
Train: 2018-08-02T13:13:57.473024: step 8098, loss 0.541867.
Train: 2018-08-02T13:13:57.629261: step 8099, loss 0.536417.
Train: 2018-08-02T13:13:57.785480: step 8100, loss 0.479884.
Test: 2018-08-02T13:13:58.238469: step 8100, loss 0.550536.
Train: 2018-08-02T13:13:58.894596: step 8101, loss 0.443311.
Train: 2018-08-02T13:13:59.050780: step 8102, loss 0.461624.
Train: 2018-08-02T13:13:59.207024: step 8103, loss 0.448033.
Train: 2018-08-02T13:13:59.363205: step 8104, loss 0.618164.
Train: 2018-08-02T13:13:59.519419: step 8105, loss 0.490989.
Train: 2018-08-02T13:13:59.675662: step 8106, loss 0.484942.
Train: 2018-08-02T13:13:59.831845: step 8107, loss 0.529634.
Train: 2018-08-02T13:13:59.988058: step 8108, loss 0.54837.
Train: 2018-08-02T13:14:00.128651: step 8109, loss 0.484429.
Train: 2018-08-02T13:14:00.284895: step 8110, loss 0.576971.
Test: 2018-08-02T13:14:00.753533: step 8110, loss 0.552188.
Train: 2018-08-02T13:14:00.909748: step 8111, loss 0.569223.
Train: 2018-08-02T13:14:01.065960: step 8112, loss 0.547561.
Train: 2018-08-02T13:14:01.206524: step 8113, loss 0.54244.
Train: 2018-08-02T13:14:01.362736: step 8114, loss 0.472763.
Train: 2018-08-02T13:14:01.518976: step 8115, loss 0.582704.
Train: 2018-08-02T13:14:01.675188: step 8116, loss 0.55641.
Train: 2018-08-02T13:14:01.831409: step 8117, loss 0.501694.
Train: 2018-08-02T13:14:01.987622: step 8118, loss 0.57402.
Train: 2018-08-02T13:14:02.143803: step 8119, loss 0.522007.
Train: 2018-08-02T13:14:02.315669: step 8120, loss 0.591258.
Test: 2018-08-02T13:14:02.768696: step 8120, loss 0.552879.
Train: 2018-08-02T13:14:02.924870: step 8121, loss 0.511164.
Train: 2018-08-02T13:14:03.081116: step 8122, loss 0.544484.
Train: 2018-08-02T13:14:03.237322: step 8123, loss 0.514642.
Train: 2018-08-02T13:14:03.377920: step 8124, loss 0.534834.
Train: 2018-08-02T13:14:03.534104: step 8125, loss 0.524969.
Train: 2018-08-02T13:14:03.690344: step 8126, loss 0.525688.
Train: 2018-08-02T13:14:03.846530: step 8127, loss 0.530544.
Train: 2018-08-02T13:14:04.002769: step 8128, loss 0.465301.
Train: 2018-08-02T13:14:04.158983: step 8129, loss 0.487651.
Train: 2018-08-02T13:14:04.330798: step 8130, loss 0.483175.
Test: 2018-08-02T13:14:04.783841: step 8130, loss 0.549504.
Train: 2018-08-02T13:14:04.940026: step 8131, loss 0.536602.
Train: 2018-08-02T13:14:05.096237: step 8132, loss 0.45366.
Train: 2018-08-02T13:14:05.252451: step 8133, loss 0.541831.
Train: 2018-08-02T13:14:05.408691: step 8134, loss 0.480307.
Train: 2018-08-02T13:14:05.564878: step 8135, loss 0.476496.
Train: 2018-08-02T13:14:05.721120: step 8136, loss 0.576823.
Train: 2018-08-02T13:14:05.877305: step 8137, loss 0.50361.
Train: 2018-08-02T13:14:06.033544: step 8138, loss 0.470409.
Train: 2018-08-02T13:14:06.189732: step 8139, loss 0.623163.
Train: 2018-08-02T13:14:06.345947: step 8140, loss 0.458964.
Test: 2018-08-02T13:14:06.814585: step 8140, loss 0.550217.
Train: 2018-08-02T13:14:06.970798: step 8141, loss 0.529945.
Train: 2018-08-02T13:14:07.111421: step 8142, loss 0.582591.
Train: 2018-08-02T13:14:07.267634: step 8143, loss 0.519977.
Train: 2018-08-02T13:14:07.423850: step 8144, loss 0.514433.
Train: 2018-08-02T13:14:07.580063: step 8145, loss 0.556103.
Train: 2018-08-02T13:14:07.751867: step 8146, loss 0.486625.
Train: 2018-08-02T13:14:07.908079: step 8147, loss 0.560062.
Train: 2018-08-02T13:14:08.064321: step 8148, loss 0.48887.
Train: 2018-08-02T13:14:08.220507: step 8149, loss 0.487994.
Train: 2018-08-02T13:14:08.376721: step 8150, loss 0.517825.
Test: 2018-08-02T13:14:08.829739: step 8150, loss 0.547372.
Train: 2018-08-02T13:14:08.985982: step 8151, loss 0.554991.
Train: 2018-08-02T13:14:09.142164: step 8152, loss 0.496215.
Train: 2018-08-02T13:14:09.298409: step 8153, loss 0.50437.
Train: 2018-08-02T13:14:09.407727: step 8154, loss 0.533282.
Train: 2018-08-02T13:14:09.563941: step 8155, loss 0.453047.
Train: 2018-08-02T13:14:09.720155: step 8156, loss 0.516502.
Train: 2018-08-02T13:14:09.876368: step 8157, loss 0.510222.
Train: 2018-08-02T13:14:10.032582: step 8158, loss 0.572903.
Train: 2018-08-02T13:14:10.188825: step 8159, loss 0.592874.
Train: 2018-08-02T13:14:10.345033: step 8160, loss 0.528073.
Test: 2018-08-02T13:14:10.798061: step 8160, loss 0.56758.
Train: 2018-08-02T13:14:10.954242: step 8161, loss 0.494501.
Train: 2018-08-02T13:14:11.110484: step 8162, loss 0.531473.
Train: 2018-08-02T13:14:11.266698: step 8163, loss 0.495067.
Train: 2018-08-02T13:14:11.422911: step 8164, loss 0.51184.
Train: 2018-08-02T13:14:11.579127: step 8165, loss 0.504894.
Train: 2018-08-02T13:14:11.735309: step 8166, loss 0.429679.
Train: 2018-08-02T13:14:11.891521: step 8167, loss 0.613365.
Train: 2018-08-02T13:14:12.047735: step 8168, loss 0.501274.
Train: 2018-08-02T13:14:12.188331: step 8169, loss 0.568284.
Train: 2018-08-02T13:14:12.344571: step 8170, loss 0.561079.
Test: 2018-08-02T13:14:12.813208: step 8170, loss 0.553569.
Train: 2018-08-02T13:14:12.969394: step 8171, loss 0.562205.
Train: 2018-08-02T13:14:13.125612: step 8172, loss 0.486062.
Train: 2018-08-02T13:14:13.266231: step 8173, loss 0.495318.
Train: 2018-08-02T13:14:13.422446: step 8174, loss 0.592893.
Train: 2018-08-02T13:14:13.578627: step 8175, loss 0.520084.
Train: 2018-08-02T13:14:13.734870: step 8176, loss 0.494386.
Train: 2018-08-02T13:14:13.891083: step 8177, loss 0.555285.
Train: 2018-08-02T13:14:14.047305: step 8178, loss 0.490376.
Train: 2018-08-02T13:14:14.203508: step 8179, loss 0.568407.
Train: 2018-08-02T13:14:14.359724: step 8180, loss 0.507889.
Test: 2018-08-02T13:14:14.828335: step 8180, loss 0.552694.
Train: 2018-08-02T13:14:14.968955: step 8181, loss 0.575062.
Train: 2018-08-02T13:14:15.125168: step 8182, loss 0.505912.
Train: 2018-08-02T13:14:15.281382: step 8183, loss 0.514278.
Train: 2018-08-02T13:14:15.437566: step 8184, loss 0.539054.
Train: 2018-08-02T13:14:15.593809: step 8185, loss 0.535257.
Train: 2018-08-02T13:14:15.750024: step 8186, loss 0.496049.
Train: 2018-08-02T13:14:15.906235: step 8187, loss 0.485818.
Train: 2018-08-02T13:14:16.046799: step 8188, loss 0.59547.
Train: 2018-08-02T13:14:16.203044: step 8189, loss 0.524922.
Train: 2018-08-02T13:14:16.359255: step 8190, loss 0.473332.
Test: 2018-08-02T13:14:16.812244: step 8190, loss 0.54669.
Train: 2018-08-02T13:14:16.968487: step 8191, loss 0.602158.
Train: 2018-08-02T13:14:17.124701: step 8192, loss 0.508622.
Train: 2018-08-02T13:14:17.296535: step 8193, loss 0.518543.
Train: 2018-08-02T13:14:17.452720: step 8194, loss 0.500652.
Train: 2018-08-02T13:14:17.608963: step 8195, loss 0.551629.
Train: 2018-08-02T13:14:17.765146: step 8196, loss 0.494545.
Train: 2018-08-02T13:14:17.921383: step 8197, loss 0.476646.
Train: 2018-08-02T13:14:18.077575: step 8198, loss 0.537301.
Train: 2018-08-02T13:14:18.233815: step 8199, loss 0.547377.
Train: 2018-08-02T13:14:18.390029: step 8200, loss 0.516374.
Test: 2018-08-02T13:14:18.843020: step 8200, loss 0.548026.
Train: 2018-08-02T13:14:19.530382: step 8201, loss 0.479517.
Train: 2018-08-02T13:14:19.686600: step 8202, loss 0.55808.
Train: 2018-08-02T13:14:19.842785: step 8203, loss 0.494356.
Train: 2018-08-02T13:14:19.983378: step 8204, loss 0.438491.
Train: 2018-08-02T13:14:20.139590: step 8205, loss 0.504224.
Train: 2018-08-02T13:14:20.295804: step 8206, loss 0.541995.
Train: 2018-08-02T13:14:20.452017: step 8207, loss 0.475665.
Train: 2018-08-02T13:14:20.623882: step 8208, loss 0.459247.
Train: 2018-08-02T13:14:20.780095: step 8209, loss 0.455164.
Train: 2018-08-02T13:14:20.936308: step 8210, loss 0.549531.
Test: 2018-08-02T13:14:21.389323: step 8210, loss 0.548416.
Train: 2018-08-02T13:14:21.545512: step 8211, loss 0.53054.
Train: 2018-08-02T13:14:21.717376: step 8212, loss 0.444656.
Train: 2018-08-02T13:14:21.873560: step 8213, loss 0.498423.
Train: 2018-08-02T13:14:22.029796: step 8214, loss 0.529009.
Train: 2018-08-02T13:14:22.185987: step 8215, loss 0.530561.
Train: 2018-08-02T13:14:22.342229: step 8216, loss 0.516192.
Train: 2018-08-02T13:14:22.498447: step 8217, loss 0.487234.
Train: 2018-08-02T13:14:22.654651: step 8218, loss 0.536817.
Train: 2018-08-02T13:14:22.810884: step 8219, loss 0.51522.
Train: 2018-08-02T13:14:22.967083: step 8220, loss 0.487083.
Test: 2018-08-02T13:14:23.435718: step 8220, loss 0.548186.
Train: 2018-08-02T13:14:23.591907: step 8221, loss 0.517046.
Train: 2018-08-02T13:14:23.748147: step 8222, loss 0.534036.
Train: 2018-08-02T13:14:23.904363: step 8223, loss 0.574845.
Train: 2018-08-02T13:14:24.060577: step 8224, loss 0.515189.
Train: 2018-08-02T13:14:24.216790: step 8225, loss 0.521859.
Train: 2018-08-02T13:14:24.372973: step 8226, loss 0.487297.
Train: 2018-08-02T13:14:24.529218: step 8227, loss 0.480371.
Train: 2018-08-02T13:14:24.685428: step 8228, loss 0.540218.
Train: 2018-08-02T13:14:24.841643: step 8229, loss 0.532427.
Train: 2018-08-02T13:14:24.997828: step 8230, loss 0.506193.
Test: 2018-08-02T13:14:25.450871: step 8230, loss 0.548433.
Train: 2018-08-02T13:14:25.607085: step 8231, loss 0.462826.
Train: 2018-08-02T13:14:25.763274: step 8232, loss 0.483979.
Train: 2018-08-02T13:14:25.935107: step 8233, loss 0.448032.
Train: 2018-08-02T13:14:26.091352: step 8234, loss 0.525568.
Train: 2018-08-02T13:14:26.247565: step 8235, loss 0.606385.
Train: 2018-08-02T13:14:26.403748: step 8236, loss 0.562676.
Train: 2018-08-02T13:14:26.559992: step 8237, loss 0.53273.
Train: 2018-08-02T13:14:26.716176: step 8238, loss 0.508225.
Train: 2018-08-02T13:14:26.872412: step 8239, loss 0.499471.
Train: 2018-08-02T13:14:27.013010: step 8240, loss 0.509338.
Test: 2018-08-02T13:14:27.481653: step 8240, loss 0.54834.
Train: 2018-08-02T13:14:27.637866: step 8241, loss 0.585973.
Train: 2018-08-02T13:14:27.794077: step 8242, loss 0.444701.
Train: 2018-08-02T13:14:27.950286: step 8243, loss 0.569773.
Train: 2018-08-02T13:14:28.106476: step 8244, loss 0.493032.
Train: 2018-08-02T13:14:28.262719: step 8245, loss 0.523278.
Train: 2018-08-02T13:14:28.418927: step 8246, loss 0.49533.
Train: 2018-08-02T13:14:28.575115: step 8247, loss 0.620538.
Train: 2018-08-02T13:14:28.715707: step 8248, loss 0.482419.
Train: 2018-08-02T13:14:28.871950: step 8249, loss 0.530591.
Train: 2018-08-02T13:14:29.043790: step 8250, loss 0.489305.
Test: 2018-08-02T13:14:29.496804: step 8250, loss 0.547994.
Train: 2018-08-02T13:14:29.652987: step 8251, loss 0.49198.
Train: 2018-08-02T13:14:29.809201: step 8252, loss 0.477948.
Train: 2018-08-02T13:14:29.965440: step 8253, loss 0.509821.
Train: 2018-08-02T13:14:30.121627: step 8254, loss 0.543508.
Train: 2018-08-02T13:14:30.293493: step 8255, loss 0.481734.
Train: 2018-08-02T13:14:30.449675: step 8256, loss 0.514911.
Train: 2018-08-02T13:14:30.605914: step 8257, loss 0.576246.
Train: 2018-08-02T13:14:30.762104: step 8258, loss 0.516725.
Train: 2018-08-02T13:14:30.918350: step 8259, loss 0.446796.
Train: 2018-08-02T13:14:31.074561: step 8260, loss 0.489226.
Test: 2018-08-02T13:14:31.527547: step 8260, loss 0.549291.
Train: 2018-08-02T13:14:31.683761: step 8261, loss 0.532917.
Train: 2018-08-02T13:14:31.824386: step 8262, loss 0.521493.
Train: 2018-08-02T13:14:31.980597: step 8263, loss 0.488324.
Train: 2018-08-02T13:14:32.152442: step 8264, loss 0.589831.
Train: 2018-08-02T13:14:32.308616: step 8265, loss 0.643832.
Train: 2018-08-02T13:14:32.464829: step 8266, loss 0.542913.
Train: 2018-08-02T13:14:32.621067: step 8267, loss 0.527527.
Train: 2018-08-02T13:14:32.777285: step 8268, loss 0.525023.
Train: 2018-08-02T13:14:32.933468: step 8269, loss 0.566294.
Train: 2018-08-02T13:14:33.089712: step 8270, loss 0.510152.
Test: 2018-08-02T13:14:33.573969: step 8270, loss 0.54978.
Train: 2018-08-02T13:14:33.730187: step 8271, loss 0.537902.
Train: 2018-08-02T13:14:33.886402: step 8272, loss 0.564165.
Train: 2018-08-02T13:14:34.042610: step 8273, loss 0.547974.
Train: 2018-08-02T13:14:34.198831: step 8274, loss 0.499722.
Train: 2018-08-02T13:14:34.355041: step 8275, loss 0.478129.
Train: 2018-08-02T13:14:34.511255: step 8276, loss 0.54217.
Train: 2018-08-02T13:14:34.667439: step 8277, loss 0.565873.
Train: 2018-08-02T13:14:34.823684: step 8278, loss 0.508936.
Train: 2018-08-02T13:14:34.979893: step 8279, loss 0.505484.
Train: 2018-08-02T13:14:35.136102: step 8280, loss 0.491809.
Test: 2018-08-02T13:14:35.589098: step 8280, loss 0.550647.
Train: 2018-08-02T13:14:35.745341: step 8281, loss 0.527726.
Train: 2018-08-02T13:14:35.901534: step 8282, loss 0.514097.
Train: 2018-08-02T13:14:36.057767: step 8283, loss 0.603838.
Train: 2018-08-02T13:14:36.213981: step 8284, loss 0.539955.
Train: 2018-08-02T13:14:36.370189: step 8285, loss 0.558946.
Train: 2018-08-02T13:14:36.542038: step 8286, loss 0.53835.
Train: 2018-08-02T13:14:36.698245: step 8287, loss 0.5719.
Train: 2018-08-02T13:14:36.854427: step 8288, loss 0.521778.
Train: 2018-08-02T13:14:37.010670: step 8289, loss 0.540881.
Train: 2018-08-02T13:14:37.166871: step 8290, loss 0.516408.
Test: 2018-08-02T13:14:37.635492: step 8290, loss 0.549787.
Train: 2018-08-02T13:14:37.791750: step 8291, loss 0.596998.
Train: 2018-08-02T13:14:37.947919: step 8292, loss 0.562001.
Train: 2018-08-02T13:14:38.104134: step 8293, loss 0.483479.
Train: 2018-08-02T13:14:38.260377: step 8294, loss 0.492396.
Train: 2018-08-02T13:14:38.400968: step 8295, loss 0.503718.
Train: 2018-08-02T13:14:38.557152: step 8296, loss 0.486337.
Train: 2018-08-02T13:14:38.729017: step 8297, loss 0.55219.
Train: 2018-08-02T13:14:38.885226: step 8298, loss 0.465651.
Train: 2018-08-02T13:14:39.041414: step 8299, loss 0.564558.
Train: 2018-08-02T13:14:39.197660: step 8300, loss 0.490014.
Test: 2018-08-02T13:14:39.650671: step 8300, loss 0.548974.
Train: 2018-08-02T13:14:40.275531: step 8301, loss 0.45918.
Train: 2018-08-02T13:14:40.431714: step 8302, loss 0.574783.
Train: 2018-08-02T13:14:40.603576: step 8303, loss 0.507414.
Train: 2018-08-02T13:14:40.749707: step 8304, loss 0.508919.
Train: 2018-08-02T13:14:40.859060: step 8305, loss 0.48999.
Train: 2018-08-02T13:14:41.030894: step 8306, loss 0.448692.
Train: 2018-08-02T13:14:41.171512: step 8307, loss 0.485089.
Train: 2018-08-02T13:14:41.327730: step 8308, loss 0.556748.
Train: 2018-08-02T13:14:41.483943: step 8309, loss 0.540745.
Train: 2018-08-02T13:14:41.640152: step 8310, loss 0.46722.
Test: 2018-08-02T13:14:42.108768: step 8310, loss 0.549798.
Train: 2018-08-02T13:14:42.264982: step 8311, loss 0.528661.
Train: 2018-08-02T13:14:42.421193: step 8312, loss 0.544231.
Train: 2018-08-02T13:14:42.577437: step 8313, loss 0.542328.
Train: 2018-08-02T13:14:42.733653: step 8314, loss 0.516513.
Train: 2018-08-02T13:14:42.889871: step 8315, loss 0.477958.
Train: 2018-08-02T13:14:43.046072: step 8316, loss 0.513489.
Train: 2018-08-02T13:14:43.217907: step 8317, loss 0.472539.
Train: 2018-08-02T13:14:43.374100: step 8318, loss 0.491818.
Train: 2018-08-02T13:14:43.530309: step 8319, loss 0.547673.
Train: 2018-08-02T13:14:43.686546: step 8320, loss 0.483926.
Test: 2018-08-02T13:14:44.155193: step 8320, loss 0.548267.
Train: 2018-08-02T13:14:44.358269: step 8321, loss 0.522719.
Train: 2018-08-02T13:14:44.514484: step 8322, loss 0.492244.
Train: 2018-08-02T13:14:44.670698: step 8323, loss 0.409237.
Train: 2018-08-02T13:14:44.842502: step 8324, loss 0.429486.
Train: 2018-08-02T13:14:44.998714: step 8325, loss 0.597248.
Train: 2018-08-02T13:14:45.139337: step 8326, loss 0.577795.
Train: 2018-08-02T13:14:45.295550: step 8327, loss 0.431798.
Train: 2018-08-02T13:14:45.467385: step 8328, loss 0.514408.
Train: 2018-08-02T13:14:45.623568: step 8329, loss 0.589371.
Train: 2018-08-02T13:14:45.779813: step 8330, loss 0.535377.
Test: 2018-08-02T13:14:46.232800: step 8330, loss 0.54843.
Train: 2018-08-02T13:14:46.404661: step 8331, loss 0.513963.
Train: 2018-08-02T13:14:46.560851: step 8332, loss 0.51471.
Train: 2018-08-02T13:14:46.717087: step 8333, loss 0.500707.
Train: 2018-08-02T13:14:46.873279: step 8334, loss 0.559994.
Train: 2018-08-02T13:14:47.029516: step 8335, loss 0.520153.
Train: 2018-08-02T13:14:47.185733: step 8336, loss 0.481837.
Train: 2018-08-02T13:14:47.341941: step 8337, loss 0.418759.
Train: 2018-08-02T13:14:47.498154: step 8338, loss 0.565176.
Train: 2018-08-02T13:14:47.654344: step 8339, loss 0.529064.
Train: 2018-08-02T13:14:47.810588: step 8340, loss 0.521725.
Test: 2018-08-02T13:14:48.263601: step 8340, loss 0.5488.
Train: 2018-08-02T13:14:48.419787: step 8341, loss 0.53593.
Train: 2018-08-02T13:14:48.576032: step 8342, loss 0.537809.
Train: 2018-08-02T13:14:48.732245: step 8343, loss 0.529033.
Train: 2018-08-02T13:14:48.888459: step 8344, loss 0.464033.
Train: 2018-08-02T13:14:49.044675: step 8345, loss 0.502645.
Train: 2018-08-02T13:14:49.200885: step 8346, loss 0.541029.
Train: 2018-08-02T13:14:49.357110: step 8347, loss 0.575456.
Train: 2018-08-02T13:14:49.513309: step 8348, loss 0.553683.
Train: 2018-08-02T13:14:49.685117: step 8349, loss 0.45642.
Train: 2018-08-02T13:14:49.841332: step 8350, loss 0.510897.
Test: 2018-08-02T13:14:50.294374: step 8350, loss 0.546282.
Train: 2018-08-02T13:14:50.450562: step 8351, loss 0.576265.
Train: 2018-08-02T13:14:50.606806: step 8352, loss 0.553704.
Train: 2018-08-02T13:14:50.763020: step 8353, loss 0.483564.
Train: 2018-08-02T13:14:50.919211: step 8354, loss 0.548065.
Train: 2018-08-02T13:14:51.075447: step 8355, loss 0.490957.
Train: 2018-08-02T13:14:51.231659: step 8356, loss 0.558472.
Train: 2018-08-02T13:14:51.387843: step 8357, loss 0.584246.
Train: 2018-08-02T13:14:51.544057: step 8358, loss 0.632876.
Train: 2018-08-02T13:14:51.700300: step 8359, loss 0.500717.
Train: 2018-08-02T13:14:51.856514: step 8360, loss 0.46941.
Test: 2018-08-02T13:14:52.309528: step 8360, loss 0.548811.
Train: 2018-08-02T13:14:52.465746: step 8361, loss 0.543712.
Train: 2018-08-02T13:14:52.621961: step 8362, loss 0.436059.
Train: 2018-08-02T13:14:52.778173: step 8363, loss 0.563891.
Train: 2018-08-02T13:14:52.934381: step 8364, loss 0.605541.
Train: 2018-08-02T13:14:53.090569: step 8365, loss 0.566067.
Train: 2018-08-02T13:14:53.246814: step 8366, loss 0.617996.
Train: 2018-08-02T13:14:53.403001: step 8367, loss 0.508216.
Train: 2018-08-02T13:14:53.559212: step 8368, loss 0.565657.
Train: 2018-08-02T13:14:53.715454: step 8369, loss 0.493236.
Train: 2018-08-02T13:14:53.871638: step 8370, loss 0.535529.
Test: 2018-08-02T13:14:54.340278: step 8370, loss 0.553625.
Train: 2018-08-02T13:14:54.496517: step 8371, loss 0.507556.
Train: 2018-08-02T13:14:54.652703: step 8372, loss 0.531245.
Train: 2018-08-02T13:14:54.808918: step 8373, loss 0.463974.
Train: 2018-08-02T13:14:54.980753: step 8374, loss 0.541427.
Train: 2018-08-02T13:14:55.121375: step 8375, loss 0.504247.
Train: 2018-08-02T13:14:55.277582: step 8376, loss 0.532479.
Train: 2018-08-02T13:14:55.433795: step 8377, loss 0.64665.
Train: 2018-08-02T13:14:55.590008: step 8378, loss 0.527941.
Train: 2018-08-02T13:14:55.746228: step 8379, loss 0.51297.
Train: 2018-08-02T13:14:55.902441: step 8380, loss 0.537484.
Test: 2018-08-02T13:14:56.355430: step 8380, loss 0.551364.
Train: 2018-08-02T13:14:56.511675: step 8381, loss 0.489054.
Train: 2018-08-02T13:14:56.667883: step 8382, loss 0.499594.
Train: 2018-08-02T13:14:56.824071: step 8383, loss 0.533225.
Train: 2018-08-02T13:14:56.995937: step 8384, loss 0.492712.
Train: 2018-08-02T13:14:57.152153: step 8385, loss 0.546495.
Train: 2018-08-02T13:14:57.308357: step 8386, loss 0.529978.
Train: 2018-08-02T13:14:57.464572: step 8387, loss 0.454494.
Train: 2018-08-02T13:14:57.620794: step 8388, loss 0.547431.
Train: 2018-08-02T13:14:57.777002: step 8389, loss 0.597016.
Train: 2018-08-02T13:14:57.933216: step 8390, loss 0.567697.
Test: 2018-08-02T13:14:58.386236: step 8390, loss 0.548886.
Train: 2018-08-02T13:14:58.542419: step 8391, loss 0.509771.
Train: 2018-08-02T13:14:58.698662: step 8392, loss 0.516467.
Train: 2018-08-02T13:14:58.854870: step 8393, loss 0.506846.
Train: 2018-08-02T13:14:59.011089: step 8394, loss 0.47474.
Train: 2018-08-02T13:14:59.167307: step 8395, loss 0.494204.
Train: 2018-08-02T13:14:59.323511: step 8396, loss 0.521128.
Train: 2018-08-02T13:14:59.479724: step 8397, loss 0.543263.
Train: 2018-08-02T13:14:59.635913: step 8398, loss 0.502946.
Train: 2018-08-02T13:14:59.792159: step 8399, loss 0.546231.
Train: 2018-08-02T13:14:59.948340: step 8400, loss 0.509343.
Test: 2018-08-02T13:15:00.401386: step 8400, loss 0.546916.
Train: 2018-08-02T13:15:01.026241: step 8401, loss 0.462019.
Train: 2018-08-02T13:15:01.182455: step 8402, loss 0.549604.
Train: 2018-08-02T13:15:01.338639: step 8403, loss 0.533631.
Train: 2018-08-02T13:15:01.494883: step 8404, loss 0.487905.
Train: 2018-08-02T13:15:01.651091: step 8405, loss 0.489667.
Train: 2018-08-02T13:15:01.807308: step 8406, loss 0.48112.
Train: 2018-08-02T13:15:01.947896: step 8407, loss 0.496999.
Train: 2018-08-02T13:15:02.104115: step 8408, loss 0.608023.
Train: 2018-08-02T13:15:02.260328: step 8409, loss 0.444509.
Train: 2018-08-02T13:15:02.416557: step 8410, loss 0.578845.
Test: 2018-08-02T13:15:02.885177: step 8410, loss 0.549331.
Train: 2018-08-02T13:15:03.041395: step 8411, loss 0.565749.
Train: 2018-08-02T13:15:03.181989: step 8412, loss 0.466813.
Train: 2018-08-02T13:15:03.338200: step 8413, loss 0.511118.
Train: 2018-08-02T13:15:03.494384: step 8414, loss 0.570787.
Train: 2018-08-02T13:15:03.650626: step 8415, loss 0.512745.
Train: 2018-08-02T13:15:03.806841: step 8416, loss 0.49614.
Train: 2018-08-02T13:15:03.963054: step 8417, loss 0.558286.
Train: 2018-08-02T13:15:04.119268: step 8418, loss 0.512329.
Train: 2018-08-02T13:15:04.275450: step 8419, loss 0.512158.
Train: 2018-08-02T13:15:04.416073: step 8420, loss 0.528411.
Test: 2018-08-02T13:15:04.884714: step 8420, loss 0.54776.
Train: 2018-08-02T13:15:05.040897: step 8421, loss 0.487331.
Train: 2018-08-02T13:15:05.197135: step 8422, loss 0.620523.
Train: 2018-08-02T13:15:05.353353: step 8423, loss 0.566726.
Train: 2018-08-02T13:15:05.509567: step 8424, loss 0.484437.
Train: 2018-08-02T13:15:05.665751: step 8425, loss 0.567706.
Train: 2018-08-02T13:15:05.806373: step 8426, loss 0.520575.
Train: 2018-08-02T13:15:05.962555: step 8427, loss 0.57223.
Train: 2018-08-02T13:15:06.118800: step 8428, loss 0.484083.
Train: 2018-08-02T13:15:06.275012: step 8429, loss 0.59085.
Train: 2018-08-02T13:15:06.431196: step 8430, loss 0.622686.
Test: 2018-08-02T13:15:06.899868: step 8430, loss 0.551422.
Train: 2018-08-02T13:15:07.056079: step 8431, loss 0.516769.
Train: 2018-08-02T13:15:07.212263: step 8432, loss 0.529135.
Train: 2018-08-02T13:15:07.368507: step 8433, loss 0.598048.
Train: 2018-08-02T13:15:07.524720: step 8434, loss 0.528398.
Train: 2018-08-02T13:15:07.680934: step 8435, loss 0.531932.
Train: 2018-08-02T13:15:07.837117: step 8436, loss 0.533345.
Train: 2018-08-02T13:15:07.993357: step 8437, loss 0.481145.
Train: 2018-08-02T13:15:08.149543: step 8438, loss 0.443073.
Train: 2018-08-02T13:15:08.305787: step 8439, loss 0.584184.
Train: 2018-08-02T13:15:08.462006: step 8440, loss 0.522596.
Test: 2018-08-02T13:15:08.915020: step 8440, loss 0.548566.
Train: 2018-08-02T13:15:09.071233: step 8441, loss 0.50776.
Train: 2018-08-02T13:15:09.227449: step 8442, loss 0.503616.
Train: 2018-08-02T13:15:09.383656: step 8443, loss 0.535902.
Train: 2018-08-02T13:15:09.539869: step 8444, loss 0.485835.
Train: 2018-08-02T13:15:09.696057: step 8445, loss 0.526859.
Train: 2018-08-02T13:15:09.852300: step 8446, loss 0.482549.
Train: 2018-08-02T13:15:10.008514: step 8447, loss 0.468253.
Train: 2018-08-02T13:15:10.164722: step 8448, loss 0.425334.
Train: 2018-08-02T13:15:10.320940: step 8449, loss 0.664426.
Train: 2018-08-02T13:15:10.477156: step 8450, loss 0.48521.
Test: 2018-08-02T13:15:10.945769: step 8450, loss 0.548767.
Train: 2018-08-02T13:15:11.086385: step 8451, loss 0.532225.
Train: 2018-08-02T13:15:11.242601: step 8452, loss 0.595866.
Train: 2018-08-02T13:15:11.414430: step 8453, loss 0.55687.
Train: 2018-08-02T13:15:11.570651: step 8454, loss 0.532973.
Train: 2018-08-02T13:15:11.726878: step 8455, loss 0.521065.
Train: 2018-08-02T13:15:11.836210: step 8456, loss 0.530084.
Train: 2018-08-02T13:15:11.992424: step 8457, loss 0.491477.
Train: 2018-08-02T13:15:12.148607: step 8458, loss 0.531668.
Train: 2018-08-02T13:15:12.304850: step 8459, loss 0.493417.
Train: 2018-08-02T13:15:12.461065: step 8460, loss 0.530128.
Test: 2018-08-02T13:15:12.929705: step 8460, loss 0.547714.
Train: 2018-08-02T13:15:13.085919: step 8461, loss 0.496859.
Train: 2018-08-02T13:15:13.242102: step 8462, loss 0.56264.
Train: 2018-08-02T13:15:13.398316: step 8463, loss 0.574308.
Train: 2018-08-02T13:15:13.570150: step 8464, loss 0.529469.
Train: 2018-08-02T13:15:13.726393: step 8465, loss 0.620816.
Train: 2018-08-02T13:15:13.882612: step 8466, loss 0.654345.
Train: 2018-08-02T13:15:14.038789: step 8467, loss 0.540897.
Train: 2018-08-02T13:15:14.195033: step 8468, loss 0.576194.
Train: 2018-08-02T13:15:14.351246: step 8469, loss 0.537094.
Train: 2018-08-02T13:15:14.507460: step 8470, loss 0.574078.
Test: 2018-08-02T13:15:14.960448: step 8470, loss 0.563298.
Train: 2018-08-02T13:15:15.132283: step 8471, loss 0.555535.
Train: 2018-08-02T13:15:15.288496: step 8472, loss 0.574444.
Train: 2018-08-02T13:15:15.444740: step 8473, loss 0.548192.
Train: 2018-08-02T13:15:15.600953: step 8474, loss 0.500644.
Train: 2018-08-02T13:15:15.757139: step 8475, loss 0.482437.
Train: 2018-08-02T13:15:15.913375: step 8476, loss 0.490113.
Train: 2018-08-02T13:15:16.069565: step 8477, loss 0.503884.
Train: 2018-08-02T13:15:16.225783: step 8478, loss 0.470322.
Train: 2018-08-02T13:15:16.382024: step 8479, loss 0.425902.
Train: 2018-08-02T13:15:16.538234: step 8480, loss 0.448661.
Test: 2018-08-02T13:15:17.006877: step 8480, loss 0.552714.
Train: 2018-08-02T13:15:17.163088: step 8481, loss 0.55097.
Train: 2018-08-02T13:15:17.319271: step 8482, loss 0.587779.
Train: 2018-08-02T13:15:17.475510: step 8483, loss 0.583804.
Train: 2018-08-02T13:15:17.631724: step 8484, loss 0.547904.
Train: 2018-08-02T13:15:17.787943: step 8485, loss 0.574013.
Train: 2018-08-02T13:15:17.944125: step 8486, loss 0.523925.
Train: 2018-08-02T13:15:18.115963: step 8487, loss 0.500313.
Train: 2018-08-02T13:15:18.272173: step 8488, loss 0.591363.
Train: 2018-08-02T13:15:18.428417: step 8489, loss 0.518496.
Train: 2018-08-02T13:15:18.584629: step 8490, loss 0.472788.
Test: 2018-08-02T13:15:19.037643: step 8490, loss 0.553822.
Train: 2018-08-02T13:15:19.193843: step 8491, loss 0.629359.
Train: 2018-08-02T13:15:19.350045: step 8492, loss 0.487445.
Train: 2018-08-02T13:15:19.506258: step 8493, loss 0.557545.
Train: 2018-08-02T13:15:19.662477: step 8494, loss 0.533101.
Train: 2018-08-02T13:15:19.818716: step 8495, loss 0.520444.
Train: 2018-08-02T13:15:19.974929: step 8496, loss 0.480064.
Train: 2018-08-02T13:15:20.131112: step 8497, loss 0.53864.
Train: 2018-08-02T13:15:20.287356: step 8498, loss 0.564153.
Train: 2018-08-02T13:15:20.443569: step 8499, loss 0.57525.
Train: 2018-08-02T13:15:20.599783: step 8500, loss 0.544693.
Test: 2018-08-02T13:15:21.052771: step 8500, loss 0.55933.
Train: 2018-08-02T13:15:21.740110: step 8501, loss 0.536059.
Train: 2018-08-02T13:15:21.896365: step 8502, loss 0.498469.
Train: 2018-08-02T13:15:22.052537: step 8503, loss 0.528006.
Train: 2018-08-02T13:15:22.208752: step 8504, loss 0.510094.
Train: 2018-08-02T13:15:22.364995: step 8505, loss 0.575715.
Train: 2018-08-02T13:15:22.521203: step 8506, loss 0.478849.
Train: 2018-08-02T13:15:22.677417: step 8507, loss 0.466936.
Train: 2018-08-02T13:15:22.833634: step 8508, loss 0.501173.
Train: 2018-08-02T13:15:22.989818: step 8509, loss 0.505676.
Train: 2018-08-02T13:15:23.146031: step 8510, loss 0.44269.
Test: 2018-08-02T13:15:23.599050: step 8510, loss 0.547499.
Train: 2018-08-02T13:15:23.755283: step 8511, loss 0.500658.
Train: 2018-08-02T13:15:23.911508: step 8512, loss 0.4807.
Train: 2018-08-02T13:15:24.067690: step 8513, loss 0.556741.
Train: 2018-08-02T13:15:24.239557: step 8514, loss 0.541649.
Train: 2018-08-02T13:15:24.395770: step 8515, loss 0.50592.
Train: 2018-08-02T13:15:24.551952: step 8516, loss 0.581148.
Train: 2018-08-02T13:15:24.708196: step 8517, loss 0.546182.
Train: 2018-08-02T13:15:24.864410: step 8518, loss 0.518621.
Train: 2018-08-02T13:15:25.020617: step 8519, loss 0.492262.
Train: 2018-08-02T13:15:25.192456: step 8520, loss 0.520196.
Test: 2018-08-02T13:15:25.645446: step 8520, loss 0.554504.
Train: 2018-08-02T13:15:25.817282: step 8521, loss 0.499558.
Train: 2018-08-02T13:15:25.973494: step 8522, loss 0.47764.
Train: 2018-08-02T13:15:26.114087: step 8523, loss 0.437846.
Train: 2018-08-02T13:15:26.270331: step 8524, loss 0.507445.
Train: 2018-08-02T13:15:26.426543: step 8525, loss 0.535347.
Train: 2018-08-02T13:15:26.582756: step 8526, loss 0.553192.
Train: 2018-08-02T13:15:26.738974: step 8527, loss 0.509177.
Train: 2018-08-02T13:15:26.895154: step 8528, loss 0.479982.
Train: 2018-08-02T13:15:27.051393: step 8529, loss 0.486395.
Train: 2018-08-02T13:15:27.207622: step 8530, loss 0.48911.
Test: 2018-08-02T13:15:27.676251: step 8530, loss 0.549348.
Train: 2018-08-02T13:15:27.832435: step 8531, loss 0.588362.
Train: 2018-08-02T13:15:27.988674: step 8532, loss 0.58542.
Train: 2018-08-02T13:15:28.144887: step 8533, loss 0.508085.
Train: 2018-08-02T13:15:28.301075: step 8534, loss 0.54283.
Train: 2018-08-02T13:15:28.472942: step 8535, loss 0.537287.
Train: 2018-08-02T13:15:28.629154: step 8536, loss 0.552641.
Train: 2018-08-02T13:15:28.785366: step 8537, loss 0.522095.
Train: 2018-08-02T13:15:28.925958: step 8538, loss 0.522587.
Train: 2018-08-02T13:15:29.082142: step 8539, loss 0.490344.
Train: 2018-08-02T13:15:29.238357: step 8540, loss 0.496116.
Test: 2018-08-02T13:15:29.707020: step 8540, loss 0.549801.
Train: 2018-08-02T13:15:29.863236: step 8541, loss 0.471458.
Train: 2018-08-02T13:15:30.019452: step 8542, loss 0.569197.
Train: 2018-08-02T13:15:30.175650: step 8543, loss 0.4781.
Train: 2018-08-02T13:15:30.331849: step 8544, loss 0.542911.
Train: 2018-08-02T13:15:30.488063: step 8545, loss 0.469115.
Train: 2018-08-02T13:15:30.659904: step 8546, loss 0.516656.
Train: 2018-08-02T13:15:30.800519: step 8547, loss 0.525493.
Train: 2018-08-02T13:15:30.956732: step 8548, loss 0.492993.
Train: 2018-08-02T13:15:31.112915: step 8549, loss 0.526859.
Train: 2018-08-02T13:15:31.284775: step 8550, loss 0.490049.
Test: 2018-08-02T13:15:31.737794: step 8550, loss 0.549071.
Train: 2018-08-02T13:15:31.893983: step 8551, loss 0.567495.
Train: 2018-08-02T13:15:32.050196: step 8552, loss 0.545995.
Train: 2018-08-02T13:15:32.206440: step 8553, loss 0.456193.
Train: 2018-08-02T13:15:32.378276: step 8554, loss 0.456787.
Train: 2018-08-02T13:15:32.534459: step 8555, loss 0.533078.
Train: 2018-08-02T13:15:32.675080: step 8556, loss 0.495965.
Train: 2018-08-02T13:15:32.831294: step 8557, loss 0.507235.
Train: 2018-08-02T13:15:33.003122: step 8558, loss 0.528184.
Train: 2018-08-02T13:15:33.143721: step 8559, loss 0.478748.
Train: 2018-08-02T13:15:33.299903: step 8560, loss 0.559196.
Test: 2018-08-02T13:15:33.768569: step 8560, loss 0.548108.
Train: 2018-08-02T13:15:33.924783: step 8561, loss 0.482413.
Train: 2018-08-02T13:15:34.081002: step 8562, loss 0.460715.
Train: 2018-08-02T13:15:34.237211: step 8563, loss 0.533558.
Train: 2018-08-02T13:15:34.393408: step 8564, loss 0.521624.
Train: 2018-08-02T13:15:34.549611: step 8565, loss 0.481292.
Train: 2018-08-02T13:15:34.705855: step 8566, loss 0.521671.
Train: 2018-08-02T13:15:34.862068: step 8567, loss 0.553373.
Train: 2018-08-02T13:15:35.018281: step 8568, loss 0.501461.
Train: 2018-08-02T13:15:35.174465: step 8569, loss 0.448786.
Train: 2018-08-02T13:15:35.330709: step 8570, loss 0.46115.
Test: 2018-08-02T13:15:35.799319: step 8570, loss 0.547148.
Train: 2018-08-02T13:15:35.955561: step 8571, loss 0.565222.
Train: 2018-08-02T13:15:36.111776: step 8572, loss 0.442877.
Train: 2018-08-02T13:15:36.267959: step 8573, loss 0.483341.
Train: 2018-08-02T13:15:36.424202: step 8574, loss 0.535204.
Train: 2018-08-02T13:15:36.580385: step 8575, loss 0.605974.
Train: 2018-08-02T13:15:36.736650: step 8576, loss 0.506325.
Train: 2018-08-02T13:15:36.892842: step 8577, loss 0.529977.
Train: 2018-08-02T13:15:37.049025: step 8578, loss 0.491913.
Train: 2018-08-02T13:15:37.205275: step 8579, loss 0.482411.
Train: 2018-08-02T13:15:37.361479: step 8580, loss 0.526103.
Test: 2018-08-02T13:15:37.830093: step 8580, loss 0.550707.
Train: 2018-08-02T13:15:37.986337: step 8581, loss 0.463765.
Train: 2018-08-02T13:15:38.142521: step 8582, loss 0.526061.
Train: 2018-08-02T13:15:38.298763: step 8583, loss 0.583215.
Train: 2018-08-02T13:15:38.454976: step 8584, loss 0.5648.
Train: 2018-08-02T13:15:38.611186: step 8585, loss 0.495809.
Train: 2018-08-02T13:15:38.767373: step 8586, loss 0.472548.
Train: 2018-08-02T13:15:38.923617: step 8587, loss 0.485525.
Train: 2018-08-02T13:15:39.095458: step 8588, loss 0.50255.
Train: 2018-08-02T13:15:39.251665: step 8589, loss 0.540838.
Train: 2018-08-02T13:15:39.392227: step 8590, loss 0.511276.
Test: 2018-08-02T13:15:39.860909: step 8590, loss 0.549187.
Train: 2018-08-02T13:15:40.017111: step 8591, loss 0.486293.
Train: 2018-08-02T13:15:40.173328: step 8592, loss 0.509315.
Train: 2018-08-02T13:15:40.313887: step 8593, loss 0.476472.
Train: 2018-08-02T13:15:40.470101: step 8594, loss 0.460956.
Train: 2018-08-02T13:15:40.641959: step 8595, loss 0.519891.
Train: 2018-08-02T13:15:40.798174: step 8596, loss 0.525824.
Train: 2018-08-02T13:15:40.954392: step 8597, loss 0.543527.
Train: 2018-08-02T13:15:41.110600: step 8598, loss 0.512853.
Train: 2018-08-02T13:15:41.266818: step 8599, loss 0.571379.
Train: 2018-08-02T13:15:41.423032: step 8600, loss 0.504222.
Test: 2018-08-02T13:15:41.891674: step 8600, loss 0.55356.
Train: 2018-08-02T13:15:42.579011: step 8601, loss 0.558537.
Train: 2018-08-02T13:15:42.735219: step 8602, loss 0.467115.
Train: 2018-08-02T13:15:42.891438: step 8603, loss 0.489558.
Train: 2018-08-02T13:15:43.047646: step 8604, loss 0.610063.
Train: 2018-08-02T13:15:43.203860: step 8605, loss 0.473864.
Train: 2018-08-02T13:15:43.360049: step 8606, loss 0.477118.
Train: 2018-08-02T13:15:43.469397: step 8607, loss 0.465426.
Train: 2018-08-02T13:15:43.625641: step 8608, loss 0.533927.
Train: 2018-08-02T13:15:43.797444: step 8609, loss 0.546712.
Train: 2018-08-02T13:15:43.953658: step 8610, loss 0.552535.
Test: 2018-08-02T13:15:44.406702: step 8610, loss 0.54736.
Train: 2018-08-02T13:15:44.562915: step 8611, loss 0.536242.
Train: 2018-08-02T13:15:44.719104: step 8612, loss 0.472258.
Train: 2018-08-02T13:15:44.875342: step 8613, loss 0.569573.
Train: 2018-08-02T13:15:45.031562: step 8614, loss 0.448595.
Train: 2018-08-02T13:15:45.187776: step 8615, loss 0.501459.
Train: 2018-08-02T13:15:45.343984: step 8616, loss 0.523459.
Train: 2018-08-02T13:15:45.515792: step 8617, loss 0.574733.
Train: 2018-08-02T13:15:45.672005: step 8618, loss 0.558617.
Train: 2018-08-02T13:15:45.828220: step 8619, loss 0.554348.
Train: 2018-08-02T13:15:45.984457: step 8620, loss 0.555345.
Test: 2018-08-02T13:15:46.437452: step 8620, loss 0.55315.
Train: 2018-08-02T13:15:46.593665: step 8621, loss 0.521433.
Train: 2018-08-02T13:15:46.749909: step 8622, loss 0.557261.
Train: 2018-08-02T13:15:46.906093: step 8623, loss 0.499302.
Train: 2018-08-02T13:15:47.062335: step 8624, loss 0.510525.
Train: 2018-08-02T13:15:47.218549: step 8625, loss 0.496171.
Train: 2018-08-02T13:15:47.374733: step 8626, loss 0.548033.
Train: 2018-08-02T13:15:47.530977: step 8627, loss 0.490068.
Train: 2018-08-02T13:15:47.687189: step 8628, loss 0.520357.
Train: 2018-08-02T13:15:47.843373: step 8629, loss 0.44.
Train: 2018-08-02T13:15:47.999612: step 8630, loss 0.506467.
Test: 2018-08-02T13:15:48.468227: step 8630, loss 0.547793.
Train: 2018-08-02T13:15:48.608819: step 8631, loss 0.533264.
Train: 2018-08-02T13:15:48.765062: step 8632, loss 0.467088.
Train: 2018-08-02T13:15:48.936866: step 8633, loss 0.569842.
Train: 2018-08-02T13:15:49.093106: step 8634, loss 0.507222.
Train: 2018-08-02T13:15:49.249296: step 8635, loss 0.490359.
Train: 2018-08-02T13:15:49.405537: step 8636, loss 0.568748.
Train: 2018-08-02T13:15:49.546130: step 8637, loss 0.532198.
Train: 2018-08-02T13:15:49.702343: step 8638, loss 0.550121.
Train: 2018-08-02T13:15:49.858557: step 8639, loss 0.506716.
Train: 2018-08-02T13:15:50.014770: step 8640, loss 0.499157.
Test: 2018-08-02T13:15:50.467760: step 8640, loss 0.554198.
Train: 2018-08-02T13:15:50.624002: step 8641, loss 0.476354.
Train: 2018-08-02T13:15:50.780226: step 8642, loss 0.486066.
Train: 2018-08-02T13:15:50.952046: step 8643, loss 0.533082.
Train: 2018-08-02T13:15:51.108233: step 8644, loss 0.456031.
Train: 2018-08-02T13:15:51.264480: step 8645, loss 0.528054.
Train: 2018-08-02T13:15:51.420692: step 8646, loss 0.508519.
Train: 2018-08-02T13:15:51.576894: step 8647, loss 0.50582.
Train: 2018-08-02T13:15:51.733120: step 8648, loss 0.553061.
Train: 2018-08-02T13:15:51.889330: step 8649, loss 0.456605.
Train: 2018-08-02T13:15:52.045538: step 8650, loss 0.608018.
Test: 2018-08-02T13:15:52.514172: step 8650, loss 0.54967.
Train: 2018-08-02T13:15:52.670367: step 8651, loss 0.519743.
Train: 2018-08-02T13:15:52.826612: step 8652, loss 0.56559.
Train: 2018-08-02T13:15:52.982795: step 8653, loss 0.526517.
Train: 2018-08-02T13:15:53.139022: step 8654, loss 0.562226.
Train: 2018-08-02T13:15:53.295255: step 8655, loss 0.566882.
Train: 2018-08-02T13:15:53.451434: step 8656, loss 0.509873.
Train: 2018-08-02T13:15:53.607649: step 8657, loss 0.519075.
Train: 2018-08-02T13:15:53.763892: step 8658, loss 0.446189.
Train: 2018-08-02T13:15:53.920076: step 8659, loss 0.552016.
Train: 2018-08-02T13:15:54.076314: step 8660, loss 0.543011.
Test: 2018-08-02T13:15:54.529311: step 8660, loss 0.547565.
Train: 2018-08-02T13:15:54.685546: step 8661, loss 0.449903.
Train: 2018-08-02T13:15:54.857381: step 8662, loss 0.440526.
Train: 2018-08-02T13:15:54.997952: step 8663, loss 0.428496.
Train: 2018-08-02T13:15:55.154186: step 8664, loss 0.602007.
Train: 2018-08-02T13:15:55.310375: step 8665, loss 0.560202.
Train: 2018-08-02T13:15:55.466589: step 8666, loss 0.465781.
Train: 2018-08-02T13:15:55.622810: step 8667, loss 0.468836.
Train: 2018-08-02T13:15:55.779045: step 8668, loss 0.461178.
Train: 2018-08-02T13:15:55.935229: step 8669, loss 0.49635.
Train: 2018-08-02T13:15:56.091443: step 8670, loss 0.456378.
Test: 2018-08-02T13:15:56.560082: step 8670, loss 0.547616.
Train: 2018-08-02T13:15:56.716321: step 8671, loss 0.545662.
Train: 2018-08-02T13:15:56.872510: step 8672, loss 0.551061.
Train: 2018-08-02T13:15:57.028769: step 8673, loss 0.598007.
Train: 2018-08-02T13:15:57.184967: step 8674, loss 0.500129.
Train: 2018-08-02T13:15:57.341180: step 8675, loss 0.478231.
Train: 2018-08-02T13:15:57.497379: step 8676, loss 0.482137.
Train: 2018-08-02T13:15:57.653601: step 8677, loss 0.486521.
Train: 2018-08-02T13:15:57.809820: step 8678, loss 0.484859.
Train: 2018-08-02T13:15:57.981625: step 8679, loss 0.467226.
Train: 2018-08-02T13:15:58.137862: step 8680, loss 0.564993.
Test: 2018-08-02T13:15:58.590856: step 8680, loss 0.550429.
Train: 2018-08-02T13:15:58.747098: step 8681, loss 0.466594.
Train: 2018-08-02T13:15:58.903314: step 8682, loss 0.563429.
Train: 2018-08-02T13:15:59.059522: step 8683, loss 0.51183.
Train: 2018-08-02T13:15:59.215711: step 8684, loss 0.480469.
Train: 2018-08-02T13:15:59.371923: step 8685, loss 0.561649.
Train: 2018-08-02T13:15:59.528168: step 8686, loss 0.495502.
Train: 2018-08-02T13:15:59.684350: step 8687, loss 0.547662.
Train: 2018-08-02T13:15:59.840563: step 8688, loss 0.469113.
Train: 2018-08-02T13:15:59.996778: step 8689, loss 0.449427.
Train: 2018-08-02T13:16:00.153022: step 8690, loss 0.555605.
Test: 2018-08-02T13:16:00.606009: step 8690, loss 0.546974.
Train: 2018-08-02T13:16:00.777857: step 8691, loss 0.546453.
Train: 2018-08-02T13:16:00.934057: step 8692, loss 0.448518.
Train: 2018-08-02T13:16:01.090272: step 8693, loss 0.545893.
Train: 2018-08-02T13:16:01.246485: step 8694, loss 0.540382.
Train: 2018-08-02T13:16:01.402730: step 8695, loss 0.486093.
Train: 2018-08-02T13:16:01.558936: step 8696, loss 0.577484.
Train: 2018-08-02T13:16:01.715157: step 8697, loss 0.480625.
Train: 2018-08-02T13:16:01.871367: step 8698, loss 0.567138.
Train: 2018-08-02T13:16:02.027582: step 8699, loss 0.577159.
Train: 2018-08-02T13:16:02.183790: step 8700, loss 0.455664.
Test: 2018-08-02T13:16:02.652431: step 8700, loss 0.557082.
Train: 2018-08-02T13:16:03.355365: step 8701, loss 0.52589.
Train: 2018-08-02T13:16:03.527201: step 8702, loss 0.496172.
Train: 2018-08-02T13:16:03.683413: step 8703, loss 0.474491.
Train: 2018-08-02T13:16:03.839626: step 8704, loss 0.476407.
Train: 2018-08-02T13:16:03.995871: step 8705, loss 0.576288.
Train: 2018-08-02T13:16:04.152055: step 8706, loss 0.53961.
Train: 2018-08-02T13:16:04.308298: step 8707, loss 0.498776.
Train: 2018-08-02T13:16:04.480102: step 8708, loss 0.554466.
Train: 2018-08-02T13:16:04.636348: step 8709, loss 0.570662.
Train: 2018-08-02T13:16:04.792554: step 8710, loss 0.562278.
Test: 2018-08-02T13:16:05.245573: step 8710, loss 0.55435.
Train: 2018-08-02T13:16:05.401793: step 8711, loss 0.517414.
Train: 2018-08-02T13:16:05.557976: step 8712, loss 0.513473.
Train: 2018-08-02T13:16:05.729815: step 8713, loss 0.594292.
Train: 2018-08-02T13:16:05.870425: step 8714, loss 0.497593.
Train: 2018-08-02T13:16:06.026644: step 8715, loss 0.537217.
Train: 2018-08-02T13:16:06.182828: step 8716, loss 0.533363.
Train: 2018-08-02T13:16:06.339072: step 8717, loss 0.557003.
Train: 2018-08-02T13:16:06.495285: step 8718, loss 0.469458.
Train: 2018-08-02T13:16:06.651500: step 8719, loss 0.482736.
Train: 2018-08-02T13:16:06.807712: step 8720, loss 0.548204.
Test: 2018-08-02T13:16:07.260701: step 8720, loss 0.547963.
Train: 2018-08-02T13:16:07.416940: step 8721, loss 0.5443.
Train: 2018-08-02T13:16:07.588749: step 8722, loss 0.518103.
Train: 2018-08-02T13:16:07.744962: step 8723, loss 0.451838.
Train: 2018-08-02T13:16:07.901207: step 8724, loss 0.53093.
Train: 2018-08-02T13:16:08.057396: step 8725, loss 0.478562.
Train: 2018-08-02T13:16:08.213603: step 8726, loss 0.499775.
Train: 2018-08-02T13:16:08.369842: step 8727, loss 0.455922.
Train: 2018-08-02T13:16:08.526059: step 8728, loss 0.465053.
Train: 2018-08-02T13:16:08.682273: step 8729, loss 0.500366.
Train: 2018-08-02T13:16:08.838486: step 8730, loss 0.450982.
Test: 2018-08-02T13:16:09.307121: step 8730, loss 0.553689.
Train: 2018-08-02T13:16:09.447688: step 8731, loss 0.475025.
Train: 2018-08-02T13:16:09.603902: step 8732, loss 0.461351.
Train: 2018-08-02T13:16:09.760146: step 8733, loss 0.543817.
Train: 2018-08-02T13:16:09.916359: step 8734, loss 0.538256.
Train: 2018-08-02T13:16:10.072573: step 8735, loss 0.522436.
Train: 2018-08-02T13:16:10.228757: step 8736, loss 0.519525.
Train: 2018-08-02T13:16:10.384999: step 8737, loss 0.500548.
Train: 2018-08-02T13:16:10.541208: step 8738, loss 0.538581.
Train: 2018-08-02T13:16:10.697427: step 8739, loss 0.515636.
Train: 2018-08-02T13:16:10.853641: step 8740, loss 0.530661.
Test: 2018-08-02T13:16:11.306628: step 8740, loss 0.552143.
Train: 2018-08-02T13:16:11.462868: step 8741, loss 0.48531.
Train: 2018-08-02T13:16:11.619086: step 8742, loss 0.497496.
Train: 2018-08-02T13:16:11.775320: step 8743, loss 0.569104.
Train: 2018-08-02T13:16:11.931482: step 8744, loss 0.474254.
Train: 2018-08-02T13:16:12.087696: step 8745, loss 0.532954.
Train: 2018-08-02T13:16:12.243910: step 8746, loss 0.473272.
Train: 2018-08-02T13:16:12.400123: step 8747, loss 0.466135.
Train: 2018-08-02T13:16:12.556362: step 8748, loss 0.517767.
Train: 2018-08-02T13:16:12.712550: step 8749, loss 0.498514.
Train: 2018-08-02T13:16:12.868794: step 8750, loss 0.489042.
Test: 2018-08-02T13:16:13.337403: step 8750, loss 0.548807.
Train: 2018-08-02T13:16:13.493616: step 8751, loss 0.46616.
Train: 2018-08-02T13:16:13.649861: step 8752, loss 0.534169.
Train: 2018-08-02T13:16:13.806043: step 8753, loss 0.476419.
Train: 2018-08-02T13:16:13.962287: step 8754, loss 0.50085.
Train: 2018-08-02T13:16:14.118500: step 8755, loss 0.499346.
Train: 2018-08-02T13:16:14.259092: step 8756, loss 0.524647.
Train: 2018-08-02T13:16:14.415309: step 8757, loss 0.447677.
Train: 2018-08-02T13:16:14.524655: step 8758, loss 0.674526.
Train: 2018-08-02T13:16:14.696490: step 8759, loss 0.438167.
Train: 2018-08-02T13:16:14.837084: step 8760, loss 0.494389.
Test: 2018-08-02T13:16:15.305722: step 8760, loss 0.548882.
Train: 2018-08-02T13:16:15.461936: step 8761, loss 0.551691.
Train: 2018-08-02T13:16:15.618150: step 8762, loss 0.483797.
Train: 2018-08-02T13:16:15.774357: step 8763, loss 0.5062.
Train: 2018-08-02T13:16:15.946198: step 8764, loss 0.455592.
Train: 2018-08-02T13:16:16.102412: step 8765, loss 0.415073.
Train: 2018-08-02T13:16:16.258630: step 8766, loss 0.617285.
Train: 2018-08-02T13:16:16.414839: step 8767, loss 0.556304.
Train: 2018-08-02T13:16:16.571021: step 8768, loss 0.545511.
Train: 2018-08-02T13:16:16.727270: step 8769, loss 0.486617.
Train: 2018-08-02T13:16:16.883448: step 8770, loss 0.529889.
Test: 2018-08-02T13:16:17.336469: step 8770, loss 0.552268.
Train: 2018-08-02T13:16:17.492710: step 8771, loss 0.609769.
Train: 2018-08-02T13:16:17.664541: step 8772, loss 0.517513.
Train: 2018-08-02T13:16:17.820762: step 8773, loss 0.504193.
Train: 2018-08-02T13:16:17.976962: step 8774, loss 0.47655.
Train: 2018-08-02T13:16:18.133180: step 8775, loss 0.558903.
Train: 2018-08-02T13:16:18.289399: step 8776, loss 0.544075.
Train: 2018-08-02T13:16:18.445612: step 8777, loss 0.517134.
Train: 2018-08-02T13:16:18.601821: step 8778, loss 0.49847.
Train: 2018-08-02T13:16:18.758040: step 8779, loss 0.469764.
Train: 2018-08-02T13:16:18.914223: step 8780, loss 0.549177.
Test: 2018-08-02T13:16:19.382894: step 8780, loss 0.546279.
Train: 2018-08-02T13:16:19.539078: step 8781, loss 0.518239.
Train: 2018-08-02T13:16:19.695320: step 8782, loss 0.548224.
Train: 2018-08-02T13:16:19.851534: step 8783, loss 0.53728.
Train: 2018-08-02T13:16:20.023368: step 8784, loss 0.514366.
Train: 2018-08-02T13:16:20.179550: step 8785, loss 0.546768.
Train: 2018-08-02T13:16:20.335765: step 8786, loss 0.538891.
Train: 2018-08-02T13:16:20.491977: step 8787, loss 0.502617.
Train: 2018-08-02T13:16:20.648190: step 8788, loss 0.524108.
Train: 2018-08-02T13:16:20.804404: step 8789, loss 0.492354.
Train: 2018-08-02T13:16:20.960648: step 8790, loss 0.431278.
Test: 2018-08-02T13:16:21.429257: step 8790, loss 0.550673.
Train: 2018-08-02T13:16:21.569881: step 8791, loss 0.569461.
Train: 2018-08-02T13:16:21.726095: step 8792, loss 0.514997.
Train: 2018-08-02T13:16:21.882307: step 8793, loss 0.492056.
Train: 2018-08-02T13:16:22.054111: step 8794, loss 0.490465.
Train: 2018-08-02T13:16:22.210325: step 8795, loss 0.548847.
Train: 2018-08-02T13:16:22.366569: step 8796, loss 0.525414.
Train: 2018-08-02T13:16:22.522776: step 8797, loss 0.543979.
Train: 2018-08-02T13:16:22.663374: step 8798, loss 0.599461.
Train: 2018-08-02T13:16:22.819583: step 8799, loss 0.517008.
Train: 2018-08-02T13:16:22.975770: step 8800, loss 0.634349.
Test: 2018-08-02T13:16:23.444458: step 8800, loss 0.564449.
Train: 2018-08-02T13:16:24.084887: step 8801, loss 0.529659.
Train: 2018-08-02T13:16:24.241127: step 8802, loss 0.508713.
Train: 2018-08-02T13:16:24.397346: step 8803, loss 0.487424.
Train: 2018-08-02T13:16:24.553552: step 8804, loss 0.429777.
Train: 2018-08-02T13:16:24.709769: step 8805, loss 0.600043.
Train: 2018-08-02T13:16:24.865954: step 8806, loss 0.538629.
Train: 2018-08-02T13:16:25.022198: step 8807, loss 0.432.
Train: 2018-08-02T13:16:25.178413: step 8808, loss 0.498464.
Train: 2018-08-02T13:16:25.334623: step 8809, loss 0.472676.
Train: 2018-08-02T13:16:25.490837: step 8810, loss 0.562163.
Test: 2018-08-02T13:16:25.943827: step 8810, loss 0.547962.
Train: 2018-08-02T13:16:26.100063: step 8811, loss 0.561747.
Train: 2018-08-02T13:16:26.271876: step 8812, loss 0.530698.
Train: 2018-08-02T13:16:26.428118: step 8813, loss 0.461854.
Train: 2018-08-02T13:16:26.584332: step 8814, loss 0.561495.
Train: 2018-08-02T13:16:26.740545: step 8815, loss 0.589283.
Train: 2018-08-02T13:16:26.896757: step 8816, loss 0.525784.
Train: 2018-08-02T13:16:27.052972: step 8817, loss 0.484961.
Train: 2018-08-02T13:16:27.209184: step 8818, loss 0.521208.
Train: 2018-08-02T13:16:27.365398: step 8819, loss 0.479958.
Train: 2018-08-02T13:16:27.521611: step 8820, loss 0.530347.
Test: 2018-08-02T13:16:27.974625: step 8820, loss 0.546884.
Train: 2018-08-02T13:16:28.130830: step 8821, loss 0.513194.
Train: 2018-08-02T13:16:28.287053: step 8822, loss 0.517079.
Train: 2018-08-02T13:16:28.443242: step 8823, loss 0.547393.
Train: 2018-08-02T13:16:28.599486: step 8824, loss 0.467659.
Train: 2018-08-02T13:16:28.755699: step 8825, loss 0.458951.
Train: 2018-08-02T13:16:28.911880: step 8826, loss 0.528559.
Train: 2018-08-02T13:16:29.068095: step 8827, loss 0.526421.
Train: 2018-08-02T13:16:29.224338: step 8828, loss 0.564974.
Train: 2018-08-02T13:16:29.380522: step 8829, loss 0.432088.
Train: 2018-08-02T13:16:29.536764: step 8830, loss 0.574105.
Test: 2018-08-02T13:16:30.005375: step 8830, loss 0.553287.
Train: 2018-08-02T13:16:30.161619: step 8831, loss 0.509025.
Train: 2018-08-02T13:16:30.317801: step 8832, loss 0.56063.
Train: 2018-08-02T13:16:30.474046: step 8833, loss 0.459887.
Train: 2018-08-02T13:16:30.630257: step 8834, loss 0.468532.
Train: 2018-08-02T13:16:30.786450: step 8835, loss 0.561387.
Train: 2018-08-02T13:16:30.942662: step 8836, loss 0.529491.
Train: 2018-08-02T13:16:31.098896: step 8837, loss 0.58298.
Train: 2018-08-02T13:16:31.255082: step 8838, loss 0.543643.
Train: 2018-08-02T13:16:31.411327: step 8839, loss 0.51785.
Train: 2018-08-02T13:16:31.567509: step 8840, loss 0.552653.
Test: 2018-08-02T13:16:32.036174: step 8840, loss 0.549942.
Train: 2018-08-02T13:16:32.192364: step 8841, loss 0.455987.
Train: 2018-08-02T13:16:32.348576: step 8842, loss 0.505539.
Train: 2018-08-02T13:16:32.504815: step 8843, loss 0.507073.
Train: 2018-08-02T13:16:32.661002: step 8844, loss 0.505042.
Train: 2018-08-02T13:16:32.817216: step 8845, loss 0.516611.
Train: 2018-08-02T13:16:32.973430: step 8846, loss 0.502316.
Train: 2018-08-02T13:16:33.129643: step 8847, loss 0.542226.
Train: 2018-08-02T13:16:33.332750: step 8848, loss 0.567391.
Train: 2018-08-02T13:16:33.488967: step 8849, loss 0.52065.
Train: 2018-08-02T13:16:33.629556: step 8850, loss 0.577722.
Test: 2018-08-02T13:16:34.098198: step 8850, loss 0.557588.
Train: 2018-08-02T13:16:34.254383: step 8851, loss 0.564563.
Train: 2018-08-02T13:16:34.410618: step 8852, loss 0.521121.
Train: 2018-08-02T13:16:34.566837: step 8853, loss 0.526518.
Train: 2018-08-02T13:16:34.723032: step 8854, loss 0.503057.
Train: 2018-08-02T13:16:34.879263: step 8855, loss 0.462406.
Train: 2018-08-02T13:16:35.035448: step 8856, loss 0.541842.
Train: 2018-08-02T13:16:35.207312: step 8857, loss 0.469557.
Train: 2018-08-02T13:16:35.363519: step 8858, loss 0.448164.
Train: 2018-08-02T13:16:35.504117: step 8859, loss 0.522967.
Train: 2018-08-02T13:16:35.660301: step 8860, loss 0.44115.
Test: 2018-08-02T13:16:36.128942: step 8860, loss 0.546717.
Train: 2018-08-02T13:16:36.285185: step 8861, loss 0.569501.
Train: 2018-08-02T13:16:36.441394: step 8862, loss 0.488655.
Train: 2018-08-02T13:16:36.597580: step 8863, loss 0.491616.
Train: 2018-08-02T13:16:36.753794: step 8864, loss 0.474052.
Train: 2018-08-02T13:16:36.910009: step 8865, loss 0.540985.
Train: 2018-08-02T13:16:37.066221: step 8866, loss 0.535556.
Train: 2018-08-02T13:16:37.222466: step 8867, loss 0.475939.
Train: 2018-08-02T13:16:37.378647: step 8868, loss 0.534594.
Train: 2018-08-02T13:16:37.550483: step 8869, loss 0.473415.
Train: 2018-08-02T13:16:37.706727: step 8870, loss 0.483514.
Test: 2018-08-02T13:16:38.159740: step 8870, loss 0.546811.
Train: 2018-08-02T13:16:38.315959: step 8871, loss 0.582149.
Train: 2018-08-02T13:16:38.472144: step 8872, loss 0.635159.
Train: 2018-08-02T13:16:38.628385: step 8873, loss 0.47606.
Train: 2018-08-02T13:16:38.800214: step 8874, loss 0.492329.
Train: 2018-08-02T13:16:38.940812: step 8875, loss 0.504221.
Train: 2018-08-02T13:16:39.097027: step 8876, loss 0.456303.
Train: 2018-08-02T13:16:39.253209: step 8877, loss 0.524944.
Train: 2018-08-02T13:16:39.425074: step 8878, loss 0.578983.
Train: 2018-08-02T13:16:39.565668: step 8879, loss 0.564744.
Train: 2018-08-02T13:16:39.721849: step 8880, loss 0.462961.
Test: 2018-08-02T13:16:40.190514: step 8880, loss 0.548357.
Train: 2018-08-02T13:16:40.346734: step 8881, loss 0.511859.
Train: 2018-08-02T13:16:40.502947: step 8882, loss 0.504642.
Train: 2018-08-02T13:16:40.659155: step 8883, loss 0.600089.
Train: 2018-08-02T13:16:40.815374: step 8884, loss 0.554053.
Train: 2018-08-02T13:16:40.971588: step 8885, loss 0.530006.
Train: 2018-08-02T13:16:41.127801: step 8886, loss 0.470358.
Train: 2018-08-02T13:16:41.284014: step 8887, loss 0.448317.
Train: 2018-08-02T13:16:41.440228: step 8888, loss 0.459608.
Train: 2018-08-02T13:16:41.596462: step 8889, loss 0.490547.
Train: 2018-08-02T13:16:41.752655: step 8890, loss 0.594948.
Test: 2018-08-02T13:16:42.221289: step 8890, loss 0.548019.
Train: 2018-08-02T13:16:42.377478: step 8891, loss 0.566406.
Train: 2018-08-02T13:16:42.533691: step 8892, loss 0.432782.
Train: 2018-08-02T13:16:42.689906: step 8893, loss 0.626199.
Train: 2018-08-02T13:16:42.846148: step 8894, loss 0.472766.
Train: 2018-08-02T13:16:43.002331: step 8895, loss 0.502687.
Train: 2018-08-02T13:16:43.158577: step 8896, loss 0.507052.
Train: 2018-08-02T13:16:43.314788: step 8897, loss 0.551871.
Train: 2018-08-02T13:16:43.470971: step 8898, loss 0.521508.
Train: 2018-08-02T13:16:43.627216: step 8899, loss 0.579887.
Train: 2018-08-02T13:16:43.783427: step 8900, loss 0.579463.
Test: 2018-08-02T13:16:44.236418: step 8900, loss 0.556285.
Train: 2018-08-02T13:16:44.892515: step 8901, loss 0.449233.
Train: 2018-08-02T13:16:45.048758: step 8902, loss 0.527125.
Train: 2018-08-02T13:16:45.204971: step 8903, loss 0.529122.
Train: 2018-08-02T13:16:45.361185: step 8904, loss 0.520114.
Train: 2018-08-02T13:16:45.517368: step 8905, loss 0.516328.
Train: 2018-08-02T13:16:45.673606: step 8906, loss 0.534833.
Train: 2018-08-02T13:16:45.829824: step 8907, loss 0.415308.
Train: 2018-08-02T13:16:45.986008: step 8908, loss 0.483428.
Train: 2018-08-02T13:16:46.095388: step 8909, loss 0.385997.
Train: 2018-08-02T13:16:46.267221: step 8910, loss 0.549219.
Test: 2018-08-02T13:16:46.720210: step 8910, loss 0.558741.
Train: 2018-08-02T13:16:46.876436: step 8911, loss 0.569837.
Train: 2018-08-02T13:16:47.032668: step 8912, loss 0.53296.
Train: 2018-08-02T13:16:47.188881: step 8913, loss 0.49291.
Train: 2018-08-02T13:16:47.345091: step 8914, loss 0.5339.
Train: 2018-08-02T13:16:47.485686: step 8915, loss 0.597339.
Train: 2018-08-02T13:16:47.657491: step 8916, loss 0.524901.
Train: 2018-08-02T13:16:47.813738: step 8917, loss 0.527942.
Train: 2018-08-02T13:16:47.969943: step 8918, loss 0.477855.
Train: 2018-08-02T13:16:48.126163: step 8919, loss 0.506488.
Train: 2018-08-02T13:16:48.282380: step 8920, loss 0.571615.
Test: 2018-08-02T13:16:48.750984: step 8920, loss 0.548394.
Train: 2018-08-02T13:16:48.907202: step 8921, loss 0.500766.
Train: 2018-08-02T13:16:49.063442: step 8922, loss 0.521805.
Train: 2018-08-02T13:16:49.219626: step 8923, loss 0.48495.
Train: 2018-08-02T13:16:49.360248: step 8924, loss 0.57566.
Train: 2018-08-02T13:16:49.532052: step 8925, loss 0.57857.
Train: 2018-08-02T13:16:49.688267: step 8926, loss 0.53987.
Train: 2018-08-02T13:16:49.844480: step 8927, loss 0.53743.
Train: 2018-08-02T13:16:50.000723: step 8928, loss 0.548957.
Train: 2018-08-02T13:16:50.156906: step 8929, loss 0.468706.
Train: 2018-08-02T13:16:50.313150: step 8930, loss 0.4389.
Test: 2018-08-02T13:16:50.766139: step 8930, loss 0.549943.
Train: 2018-08-02T13:16:50.922384: step 8931, loss 0.541884.
Train: 2018-08-02T13:16:51.078595: step 8932, loss 0.526219.
Train: 2018-08-02T13:16:51.234805: step 8933, loss 0.490036.
Train: 2018-08-02T13:16:51.391022: step 8934, loss 0.509005.
Train: 2018-08-02T13:16:51.547205: step 8935, loss 0.436366.
Train: 2018-08-02T13:16:51.719082: step 8936, loss 0.464568.
Train: 2018-08-02T13:16:51.875284: step 8937, loss 0.627696.
Train: 2018-08-02T13:16:52.031466: step 8938, loss 0.440917.
Train: 2018-08-02T13:16:52.187681: step 8939, loss 0.501562.
Train: 2018-08-02T13:16:52.343911: step 8940, loss 0.444441.
Test: 2018-08-02T13:16:52.812544: step 8940, loss 0.546921.
Train: 2018-08-02T13:16:52.968779: step 8941, loss 0.408425.
Train: 2018-08-02T13:16:53.124961: step 8942, loss 0.48755.
Train: 2018-08-02T13:16:53.281174: step 8943, loss 0.51089.
Train: 2018-08-02T13:16:53.437413: step 8944, loss 0.588203.
Train: 2018-08-02T13:16:53.578013: step 8945, loss 0.547002.
Train: 2018-08-02T13:16:53.749840: step 8946, loss 0.573387.
Train: 2018-08-02T13:16:53.906033: step 8947, loss 0.486409.
Train: 2018-08-02T13:16:54.062243: step 8948, loss 0.485641.
Train: 2018-08-02T13:16:54.218481: step 8949, loss 0.506561.
Train: 2018-08-02T13:16:54.374667: step 8950, loss 0.510975.
Test: 2018-08-02T13:16:54.827687: step 8950, loss 0.550893.
Train: 2018-08-02T13:16:54.983931: step 8951, loss 0.481074.
Train: 2018-08-02T13:16:55.140146: step 8952, loss 0.543269.
Train: 2018-08-02T13:16:55.296353: step 8953, loss 0.539055.
Train: 2018-08-02T13:16:55.452541: step 8954, loss 0.570518.
Train: 2018-08-02T13:16:55.624377: step 8955, loss 0.477406.
Train: 2018-08-02T13:16:55.780614: step 8956, loss 0.479922.
Train: 2018-08-02T13:16:55.936808: step 8957, loss 0.423759.
Train: 2018-08-02T13:16:56.093015: step 8958, loss 0.487658.
Train: 2018-08-02T13:16:56.249258: step 8959, loss 0.443348.
Train: 2018-08-02T13:16:56.405473: step 8960, loss 0.503813.
Test: 2018-08-02T13:16:56.858463: step 8960, loss 0.549204.
Train: 2018-08-02T13:16:57.014704: step 8961, loss 0.524361.
Train: 2018-08-02T13:16:57.170918: step 8962, loss 0.463337.
Train: 2018-08-02T13:16:57.327131: step 8963, loss 0.441655.
Train: 2018-08-02T13:16:57.483345: step 8964, loss 0.473482.
Train: 2018-08-02T13:16:57.639558: step 8965, loss 0.435781.
Train: 2018-08-02T13:16:57.795772: step 8966, loss 0.516112.
Train: 2018-08-02T13:16:57.951980: step 8967, loss 0.584965.
Train: 2018-08-02T13:16:58.123792: step 8968, loss 0.490796.
Train: 2018-08-02T13:16:58.280029: step 8969, loss 0.521682.
Train: 2018-08-02T13:16:58.436248: step 8970, loss 0.600533.
Test: 2018-08-02T13:16:58.889236: step 8970, loss 0.551036.
Train: 2018-08-02T13:16:59.045448: step 8971, loss 0.535761.
Train: 2018-08-02T13:16:59.201693: step 8972, loss 0.503363.
Train: 2018-08-02T13:16:59.357906: step 8973, loss 0.599644.
Train: 2018-08-02T13:16:59.514119: step 8974, loss 0.588825.
Train: 2018-08-02T13:16:59.670302: step 8975, loss 0.567772.
Train: 2018-08-02T13:16:59.826546: step 8976, loss 0.477456.
Train: 2018-08-02T13:16:59.982729: step 8977, loss 0.526192.
Train: 2018-08-02T13:17:00.138974: step 8978, loss 0.534846.
Train: 2018-08-02T13:17:00.295186: step 8979, loss 0.508224.
Train: 2018-08-02T13:17:00.451399: step 8980, loss 0.561238.
Test: 2018-08-02T13:17:00.920034: step 8980, loss 0.556491.
Train: 2018-08-02T13:17:01.076254: step 8981, loss 0.635042.
Train: 2018-08-02T13:17:01.232467: step 8982, loss 0.580782.
Train: 2018-08-02T13:17:01.388651: step 8983, loss 0.597721.
Train: 2018-08-02T13:17:01.544889: step 8984, loss 0.571889.
Train: 2018-08-02T13:17:01.701128: step 8985, loss 0.512749.
Train: 2018-08-02T13:17:01.857331: step 8986, loss 0.546099.
Train: 2018-08-02T13:17:02.013534: step 8987, loss 0.52607.
Train: 2018-08-02T13:17:02.169748: step 8988, loss 0.560164.
Train: 2018-08-02T13:17:02.325968: step 8989, loss 0.486879.
Train: 2018-08-02T13:17:02.482144: step 8990, loss 0.539984.
Test: 2018-08-02T13:17:02.950784: step 8990, loss 0.546947.
Train: 2018-08-02T13:17:03.091408: step 8991, loss 0.587935.
Train: 2018-08-02T13:17:03.247620: step 8992, loss 0.540433.
Train: 2018-08-02T13:17:03.403833: step 8993, loss 0.609384.
Train: 2018-08-02T13:17:03.560048: step 8994, loss 0.555649.
Train: 2018-08-02T13:17:03.716230: step 8995, loss 0.527947.
Train: 2018-08-02T13:17:03.872474: step 8996, loss 0.560039.
Train: 2018-08-02T13:17:04.028687: step 8997, loss 0.505698.
Train: 2018-08-02T13:17:04.184875: step 8998, loss 0.500125.
Train: 2018-08-02T13:17:04.341108: step 8999, loss 0.592006.
Train: 2018-08-02T13:17:04.497328: step 9000, loss 0.473498.
Test: 2018-08-02T13:17:04.950341: step 9000, loss 0.548851.
Train: 2018-08-02T13:17:05.575214: step 9001, loss 0.426926.
Train: 2018-08-02T13:17:05.731415: step 9002, loss 0.457322.
Train: 2018-08-02T13:17:05.887598: step 9003, loss 0.502724.
Train: 2018-08-02T13:17:06.043843: step 9004, loss 0.445406.
Train: 2018-08-02T13:17:06.200031: step 9005, loss 0.521418.
Train: 2018-08-02T13:17:06.356264: step 9006, loss 0.538527.
Train: 2018-08-02T13:17:06.512450: step 9007, loss 0.52503.
Train: 2018-08-02T13:17:06.668663: step 9008, loss 0.528728.
Train: 2018-08-02T13:17:06.824877: step 9009, loss 0.556865.
Train: 2018-08-02T13:17:06.981115: step 9010, loss 0.50522.
Test: 2018-08-02T13:17:07.449756: step 9010, loss 0.555358.
Train: 2018-08-02T13:17:07.605945: step 9011, loss 0.48924.
Train: 2018-08-02T13:17:07.762188: step 9012, loss 0.458319.
Train: 2018-08-02T13:17:07.918401: step 9013, loss 0.619481.
Train: 2018-08-02T13:17:08.074585: step 9014, loss 0.443077.
Train: 2018-08-02T13:17:08.230830: step 9015, loss 0.591367.
Train: 2018-08-02T13:17:08.387038: step 9016, loss 0.509244.
Train: 2018-08-02T13:17:08.543256: step 9017, loss 0.509471.
Train: 2018-08-02T13:17:08.699438: step 9018, loss 0.537089.
Train: 2018-08-02T13:17:08.855678: step 9019, loss 0.530784.
Train: 2018-08-02T13:17:09.011896: step 9020, loss 0.536338.
Test: 2018-08-02T13:17:09.464885: step 9020, loss 0.55636.
Train: 2018-08-02T13:17:09.621098: step 9021, loss 0.465232.
Train: 2018-08-02T13:17:09.792964: step 9022, loss 0.526526.
Train: 2018-08-02T13:17:09.949146: step 9023, loss 0.534988.
Train: 2018-08-02T13:17:10.105392: step 9024, loss 0.461387.
Train: 2018-08-02T13:17:10.261603: step 9025, loss 0.506029.
Train: 2018-08-02T13:17:10.417816: step 9026, loss 0.508109.
Train: 2018-08-02T13:17:10.574038: step 9027, loss 0.541641.
Train: 2018-08-02T13:17:10.714622: step 9028, loss 0.570247.
Train: 2018-08-02T13:17:10.870835: step 9029, loss 0.576396.
Train: 2018-08-02T13:17:11.027019: step 9030, loss 0.487539.
Test: 2018-08-02T13:17:11.480037: step 9030, loss 0.551877.
Train: 2018-08-02T13:17:11.651903: step 9031, loss 0.570944.
Train: 2018-08-02T13:17:11.808111: step 9032, loss 0.528888.
Train: 2018-08-02T13:17:11.964326: step 9033, loss 0.52865.
Train: 2018-08-02T13:17:12.120530: step 9034, loss 0.501594.
Train: 2018-08-02T13:17:12.276726: step 9035, loss 0.541811.
Train: 2018-08-02T13:17:12.432939: step 9036, loss 0.448591.
Train: 2018-08-02T13:17:12.589188: step 9037, loss 0.504938.
Train: 2018-08-02T13:17:12.745395: step 9038, loss 0.421708.
Train: 2018-08-02T13:17:12.901582: step 9039, loss 0.542881.
Train: 2018-08-02T13:17:13.057794: step 9040, loss 0.506125.
Test: 2018-08-02T13:17:13.526434: step 9040, loss 0.548715.
Train: 2018-08-02T13:17:13.682676: step 9041, loss 0.57237.
Train: 2018-08-02T13:17:13.823238: step 9042, loss 0.524938.
Train: 2018-08-02T13:17:13.979478: step 9043, loss 0.54271.
Train: 2018-08-02T13:17:14.135691: step 9044, loss 0.590358.
Train: 2018-08-02T13:17:14.291881: step 9045, loss 0.575466.
Train: 2018-08-02T13:17:14.448118: step 9046, loss 0.515752.
Train: 2018-08-02T13:17:14.604336: step 9047, loss 0.57925.
Train: 2018-08-02T13:17:14.760519: step 9048, loss 0.527416.
Train: 2018-08-02T13:17:14.916742: step 9049, loss 0.569635.
Train: 2018-08-02T13:17:15.072979: step 9050, loss 0.506036.
Test: 2018-08-02T13:17:15.541586: step 9050, loss 0.558265.
Train: 2018-08-02T13:17:15.697830: step 9051, loss 0.631214.
Train: 2018-08-02T13:17:15.854044: step 9052, loss 0.529768.
Train: 2018-08-02T13:17:16.010257: step 9053, loss 0.530032.
Train: 2018-08-02T13:17:16.166470: step 9054, loss 0.462643.
Train: 2018-08-02T13:17:16.322679: step 9055, loss 0.547188.
Train: 2018-08-02T13:17:16.478867: step 9056, loss 0.482469.
Train: 2018-08-02T13:17:16.635111: step 9057, loss 0.519257.
Train: 2018-08-02T13:17:16.791324: step 9058, loss 0.604896.
Train: 2018-08-02T13:17:16.947532: step 9059, loss 0.534943.
Train: 2018-08-02T13:17:17.056887: step 9060, loss 0.52013.
Test: 2018-08-02T13:17:17.525523: step 9060, loss 0.561414.
Train: 2018-08-02T13:17:17.681722: step 9061, loss 0.55372.
Train: 2018-08-02T13:17:17.822332: step 9062, loss 0.475544.
Train: 2018-08-02T13:17:17.978542: step 9063, loss 0.49332.
Train: 2018-08-02T13:17:18.134730: step 9064, loss 0.5104.
Train: 2018-08-02T13:17:18.290968: step 9065, loss 0.502767.
Train: 2018-08-02T13:17:18.447156: step 9066, loss 0.549997.
Train: 2018-08-02T13:17:18.603405: step 9067, loss 0.575641.
Train: 2018-08-02T13:17:18.759591: step 9068, loss 0.427329.
Train: 2018-08-02T13:17:18.931417: step 9069, loss 0.512274.
Train: 2018-08-02T13:17:19.087657: step 9070, loss 0.509022.
Test: 2018-08-02T13:17:19.540650: step 9070, loss 0.550063.
Train: 2018-08-02T13:17:19.696894: step 9071, loss 0.669586.
Train: 2018-08-02T13:17:19.853077: step 9072, loss 0.470551.
Train: 2018-08-02T13:17:20.009316: step 9073, loss 0.430771.
Train: 2018-08-02T13:17:20.165530: step 9074, loss 0.522841.
Train: 2018-08-02T13:17:20.321747: step 9075, loss 0.512916.
Train: 2018-08-02T13:17:20.477932: step 9076, loss 0.492871.
Train: 2018-08-02T13:17:20.634175: step 9077, loss 0.607573.
Train: 2018-08-02T13:17:20.790388: step 9078, loss 0.447296.
Train: 2018-08-02T13:17:20.946603: step 9079, loss 0.508903.
Train: 2018-08-02T13:17:21.102784: step 9080, loss 0.51851.
Test: 2018-08-02T13:17:21.555834: step 9080, loss 0.561381.
Train: 2018-08-02T13:17:21.712047: step 9081, loss 0.543397.
Train: 2018-08-02T13:17:21.868229: step 9082, loss 0.517269.
Train: 2018-08-02T13:17:22.024474: step 9083, loss 0.47461.
Train: 2018-08-02T13:17:22.180656: step 9084, loss 0.554103.
Train: 2018-08-02T13:17:22.336906: step 9085, loss 0.501767.
Train: 2018-08-02T13:17:22.493114: step 9086, loss 0.490078.
Train: 2018-08-02T13:17:22.649297: step 9087, loss 0.46537.
Train: 2018-08-02T13:17:22.805540: step 9088, loss 0.508983.
Train: 2018-08-02T13:17:22.961754: step 9089, loss 0.566621.
Train: 2018-08-02T13:17:23.117968: step 9090, loss 0.420597.
Test: 2018-08-02T13:17:23.586602: step 9090, loss 0.553147.
Train: 2018-08-02T13:17:23.742791: step 9091, loss 0.542774.
Train: 2018-08-02T13:17:23.899004: step 9092, loss 0.614413.
Train: 2018-08-02T13:17:24.055248: step 9093, loss 0.553839.
Train: 2018-08-02T13:17:24.211431: step 9094, loss 0.463694.
Train: 2018-08-02T13:17:24.367674: step 9095, loss 0.600464.
Train: 2018-08-02T13:17:24.523858: step 9096, loss 0.624712.
Train: 2018-08-02T13:17:24.680073: step 9097, loss 0.519567.
Train: 2018-08-02T13:17:24.836285: step 9098, loss 0.525691.
Train: 2018-08-02T13:17:24.992528: step 9099, loss 0.564742.
Train: 2018-08-02T13:17:25.148735: step 9100, loss 0.560971.
Test: 2018-08-02T13:17:25.617352: step 9100, loss 0.562923.
Train: 2018-08-02T13:17:26.289110: step 9101, loss 0.462175.
Train: 2018-08-02T13:17:26.445284: step 9102, loss 0.486458.
Train: 2018-08-02T13:17:26.601522: step 9103, loss 0.570736.
Train: 2018-08-02T13:17:26.757709: step 9104, loss 0.521673.
Train: 2018-08-02T13:17:26.913954: step 9105, loss 0.591865.
Train: 2018-08-02T13:17:27.070138: step 9106, loss 0.503989.
Train: 2018-08-02T13:17:27.226350: step 9107, loss 0.59801.
Train: 2018-08-02T13:17:27.398209: step 9108, loss 0.546757.
Train: 2018-08-02T13:17:27.554429: step 9109, loss 0.513451.
Train: 2018-08-02T13:17:27.710611: step 9110, loss 0.477682.
Test: 2018-08-02T13:17:28.163630: step 9110, loss 0.556758.
Train: 2018-08-02T13:17:28.319846: step 9111, loss 0.594046.
Train: 2018-08-02T13:17:28.476057: step 9112, loss 0.514959.
Train: 2018-08-02T13:17:28.632295: step 9113, loss 0.50001.
Train: 2018-08-02T13:17:28.788514: step 9114, loss 0.51267.
Train: 2018-08-02T13:17:28.944727: step 9115, loss 0.52274.
Train: 2018-08-02T13:17:29.100942: step 9116, loss 0.419139.
Train: 2018-08-02T13:17:29.257124: step 9117, loss 0.567853.
Train: 2018-08-02T13:17:29.413369: step 9118, loss 0.533602.
Train: 2018-08-02T13:17:29.569582: step 9119, loss 0.476448.
Train: 2018-08-02T13:17:29.725795: step 9120, loss 0.530965.
Test: 2018-08-02T13:17:30.194405: step 9120, loss 0.552801.
Train: 2018-08-02T13:17:30.350643: step 9121, loss 0.576464.
Train: 2018-08-02T13:17:30.506856: step 9122, loss 0.595969.
Train: 2018-08-02T13:17:30.663047: step 9123, loss 0.511913.
Train: 2018-08-02T13:17:30.819302: step 9124, loss 0.608813.
Train: 2018-08-02T13:17:30.975503: step 9125, loss 0.501202.
Train: 2018-08-02T13:17:31.131716: step 9126, loss 0.515754.
Train: 2018-08-02T13:17:31.287923: step 9127, loss 0.579565.
Train: 2018-08-02T13:17:31.444144: step 9128, loss 0.496835.
Train: 2018-08-02T13:17:31.600352: step 9129, loss 0.529762.
Train: 2018-08-02T13:17:31.756538: step 9130, loss 0.548249.
Test: 2018-08-02T13:17:32.209558: step 9130, loss 0.553401.
Train: 2018-08-02T13:17:32.365802: step 9131, loss 0.452048.
Train: 2018-08-02T13:17:32.521986: step 9132, loss 0.557791.
Train: 2018-08-02T13:17:32.662607: step 9133, loss 0.569809.
Train: 2018-08-02T13:17:32.818816: step 9134, loss 0.425573.
Train: 2018-08-02T13:17:32.975005: step 9135, loss 0.444019.
Train: 2018-08-02T13:17:33.131218: step 9136, loss 0.4205.
Train: 2018-08-02T13:17:33.287461: step 9137, loss 0.446571.
Train: 2018-08-02T13:17:33.443645: step 9138, loss 0.553064.
Train: 2018-08-02T13:17:33.599883: step 9139, loss 0.557134.
Train: 2018-08-02T13:17:33.756101: step 9140, loss 0.554359.
Test: 2018-08-02T13:17:34.209115: step 9140, loss 0.549649.
Train: 2018-08-02T13:17:34.365333: step 9141, loss 0.484942.
Train: 2018-08-02T13:17:34.521541: step 9142, loss 0.637853.
Train: 2018-08-02T13:17:34.693353: step 9143, loss 0.489415.
Train: 2018-08-02T13:17:34.849566: step 9144, loss 0.511217.
Train: 2018-08-02T13:17:35.005808: step 9145, loss 0.564528.
Train: 2018-08-02T13:17:35.162023: step 9146, loss 0.476794.
Train: 2018-08-02T13:17:35.318236: step 9147, loss 0.532832.
Train: 2018-08-02T13:17:35.474445: step 9148, loss 0.555413.
Train: 2018-08-02T13:17:35.630662: step 9149, loss 0.540357.
Train: 2018-08-02T13:17:35.786870: step 9150, loss 0.511914.
Test: 2018-08-02T13:17:36.255485: step 9150, loss 0.550666.
Train: 2018-08-02T13:17:36.411725: step 9151, loss 0.505328.
Train: 2018-08-02T13:17:36.567914: step 9152, loss 0.465708.
Train: 2018-08-02T13:17:36.724126: step 9153, loss 0.484152.
Train: 2018-08-02T13:17:36.880365: step 9154, loss 0.519698.
Train: 2018-08-02T13:17:37.036584: step 9155, loss 0.485507.
Train: 2018-08-02T13:17:37.192766: step 9156, loss 0.509191.
Train: 2018-08-02T13:17:37.348981: step 9157, loss 0.506836.
Train: 2018-08-02T13:17:37.505227: step 9158, loss 0.500115.
Train: 2018-08-02T13:17:37.661432: step 9159, loss 0.559695.
Train: 2018-08-02T13:17:37.817620: step 9160, loss 0.491794.
Test: 2018-08-02T13:17:38.270638: step 9160, loss 0.549042.
Train: 2018-08-02T13:17:38.426889: step 9161, loss 0.480899.
Train: 2018-08-02T13:17:38.598713: step 9162, loss 0.504373.
Train: 2018-08-02T13:17:38.754932: step 9163, loss 0.493306.
Train: 2018-08-02T13:17:38.911145: step 9164, loss 0.447199.
Train: 2018-08-02T13:17:39.067328: step 9165, loss 0.588393.
Train: 2018-08-02T13:17:39.223572: step 9166, loss 0.544334.
Train: 2018-08-02T13:17:39.379780: step 9167, loss 0.495799.
Train: 2018-08-02T13:17:39.535997: step 9168, loss 0.516372.
Train: 2018-08-02T13:17:39.692217: step 9169, loss 0.465021.
Train: 2018-08-02T13:17:39.848425: step 9170, loss 0.582432.
Test: 2018-08-02T13:17:40.301413: step 9170, loss 0.554696.
Train: 2018-08-02T13:17:40.457627: step 9171, loss 0.541054.
Train: 2018-08-02T13:17:40.613870: step 9172, loss 0.465972.
Train: 2018-08-02T13:17:40.770083: step 9173, loss 0.512808.
Train: 2018-08-02T13:17:40.926266: step 9174, loss 0.532784.
Train: 2018-08-02T13:17:41.082511: step 9175, loss 0.475388.
Train: 2018-08-02T13:17:41.251733: step 9176, loss 0.482826.
Train: 2018-08-02T13:17:41.407922: step 9177, loss 0.557338.
Train: 2018-08-02T13:17:41.564167: step 9178, loss 0.52141.
Train: 2018-08-02T13:17:41.720348: step 9179, loss 0.466135.
Train: 2018-08-02T13:17:41.876563: step 9180, loss 0.535908.
Test: 2018-08-02T13:17:42.329614: step 9180, loss 0.549169.
Train: 2018-08-02T13:17:42.485795: step 9181, loss 0.549432.
Train: 2018-08-02T13:17:42.642008: step 9182, loss 0.538146.
Train: 2018-08-02T13:17:42.813868: step 9183, loss 0.537244.
Train: 2018-08-02T13:17:42.970086: step 9184, loss 0.48678.
Train: 2018-08-02T13:17:43.126269: step 9185, loss 0.49199.
Train: 2018-08-02T13:17:43.282514: step 9186, loss 0.521834.
Train: 2018-08-02T13:17:43.438726: step 9187, loss 0.566461.
Train: 2018-08-02T13:17:43.594935: step 9188, loss 0.527443.
Train: 2018-08-02T13:17:43.751124: step 9189, loss 0.558962.
Train: 2018-08-02T13:17:43.907338: step 9190, loss 0.524719.
Test: 2018-08-02T13:17:44.360355: step 9190, loss 0.551309.
Train: 2018-08-02T13:17:44.516599: step 9191, loss 0.507044.
Train: 2018-08-02T13:17:44.672813: step 9192, loss 0.54902.
Train: 2018-08-02T13:17:44.829023: step 9193, loss 0.485093.
Train: 2018-08-02T13:17:45.000861: step 9194, loss 0.501592.
Train: 2018-08-02T13:17:45.157075: step 9195, loss 0.490226.
Train: 2018-08-02T13:17:45.313288: step 9196, loss 0.581833.
Train: 2018-08-02T13:17:45.469506: step 9197, loss 0.463315.
Train: 2018-08-02T13:17:45.625715: step 9198, loss 0.516267.
Train: 2018-08-02T13:17:45.781928: step 9199, loss 0.566767.
Train: 2018-08-02T13:17:45.938144: step 9200, loss 0.488564.
Test: 2018-08-02T13:17:46.406775: step 9200, loss 0.550928.
Train: 2018-08-02T13:17:47.094089: step 9201, loss 0.594766.
Train: 2018-08-02T13:17:47.250302: step 9202, loss 0.471051.
Train: 2018-08-02T13:17:47.406516: step 9203, loss 0.514835.
Train: 2018-08-02T13:17:47.562766: step 9204, loss 0.540349.
Train: 2018-08-02T13:17:47.718978: step 9205, loss 0.47171.
Train: 2018-08-02T13:17:47.875187: step 9206, loss 0.53002.
Train: 2018-08-02T13:17:48.031395: step 9207, loss 0.548333.
Train: 2018-08-02T13:17:48.187584: step 9208, loss 0.549538.
Train: 2018-08-02T13:17:48.343827: step 9209, loss 0.581324.
Train: 2018-08-02T13:17:48.515632: step 9210, loss 0.506865.
Test: 2018-08-02T13:17:48.968652: step 9210, loss 0.556158.
Train: 2018-08-02T13:17:49.078001: step 9211, loss 0.520416.
Train: 2018-08-02T13:17:49.249863: step 9212, loss 0.490793.
Train: 2018-08-02T13:17:49.390457: step 9213, loss 0.511378.
Train: 2018-08-02T13:17:49.546670: step 9214, loss 0.482487.
Train: 2018-08-02T13:17:49.702860: step 9215, loss 0.4729.
Train: 2018-08-02T13:17:49.859093: step 9216, loss 0.633086.
Train: 2018-08-02T13:17:50.015280: step 9217, loss 0.456954.
Train: 2018-08-02T13:17:50.171504: step 9218, loss 0.485164.
Train: 2018-08-02T13:17:50.343346: step 9219, loss 0.528988.
Train: 2018-08-02T13:17:50.499543: step 9220, loss 0.456131.
Test: 2018-08-02T13:17:50.952563: step 9220, loss 0.557136.
Train: 2018-08-02T13:17:51.108805: step 9221, loss 0.516833.
Train: 2018-08-02T13:17:51.264987: step 9222, loss 0.439595.
Train: 2018-08-02T13:17:51.421202: step 9223, loss 0.561251.
Train: 2018-08-02T13:17:51.577415: step 9224, loss 0.501258.
Train: 2018-08-02T13:17:51.733658: step 9225, loss 0.578168.
Train: 2018-08-02T13:17:51.889873: step 9226, loss 0.458305.
Train: 2018-08-02T13:17:52.046085: step 9227, loss 0.560122.
Train: 2018-08-02T13:17:52.202298: step 9228, loss 0.486342.
Train: 2018-08-02T13:17:52.358507: step 9229, loss 0.613525.
Train: 2018-08-02T13:17:52.514745: step 9230, loss 0.600755.
Test: 2018-08-02T13:17:52.967739: step 9230, loss 0.551006.
Train: 2018-08-02T13:17:53.123958: step 9231, loss 0.52376.
Train: 2018-08-02T13:17:53.280167: step 9232, loss 0.492181.
Train: 2018-08-02T13:17:53.436385: step 9233, loss 0.450096.
Train: 2018-08-02T13:17:53.592598: step 9234, loss 0.590152.
Train: 2018-08-02T13:17:53.748813: step 9235, loss 0.571533.
Train: 2018-08-02T13:17:53.904995: step 9236, loss 0.523507.
Train: 2018-08-02T13:17:54.061210: step 9237, loss 0.568412.
Train: 2018-08-02T13:17:54.217447: step 9238, loss 0.53546.
Train: 2018-08-02T13:17:54.389282: step 9239, loss 0.504167.
Train: 2018-08-02T13:17:54.545507: step 9240, loss 0.515054.
Test: 2018-08-02T13:17:55.014110: step 9240, loss 0.552691.
Train: 2018-08-02T13:17:55.170355: step 9241, loss 0.491885.
Train: 2018-08-02T13:17:55.326538: step 9242, loss 0.53315.
Train: 2018-08-02T13:17:55.482751: step 9243, loss 0.50881.
Train: 2018-08-02T13:17:55.623342: step 9244, loss 0.560371.
Train: 2018-08-02T13:17:55.779587: step 9245, loss 0.633371.
Train: 2018-08-02T13:17:55.935770: step 9246, loss 0.513138.
Train: 2018-08-02T13:17:56.092008: step 9247, loss 0.53038.
Train: 2018-08-02T13:17:56.248196: step 9248, loss 0.553138.
Train: 2018-08-02T13:17:56.404440: step 9249, loss 0.451.
Train: 2018-08-02T13:17:56.545031: step 9250, loss 0.527758.
Test: 2018-08-02T13:17:57.013643: step 9250, loss 0.557755.
Train: 2018-08-02T13:17:57.169857: step 9251, loss 0.551747.
Train: 2018-08-02T13:17:57.326099: step 9252, loss 0.503493.
Train: 2018-08-02T13:17:57.497904: step 9253, loss 0.534212.
Train: 2018-08-02T13:17:57.654143: step 9254, loss 0.485031.
Train: 2018-08-02T13:17:57.810330: step 9255, loss 0.512031.
Train: 2018-08-02T13:17:57.966543: step 9256, loss 0.526136.
Train: 2018-08-02T13:17:58.122760: step 9257, loss 0.515172.
Train: 2018-08-02T13:17:58.278972: step 9258, loss 0.52831.
Train: 2018-08-02T13:17:58.435209: step 9259, loss 0.430785.
Train: 2018-08-02T13:17:58.591397: step 9260, loss 0.530855.
Test: 2018-08-02T13:17:59.060072: step 9260, loss 0.553195.
Train: 2018-08-02T13:17:59.216282: step 9261, loss 0.549028.
Train: 2018-08-02T13:17:59.372476: step 9262, loss 0.459745.
Train: 2018-08-02T13:17:59.528709: step 9263, loss 0.500798.
Train: 2018-08-02T13:17:59.684892: step 9264, loss 0.487732.
Train: 2018-08-02T13:17:59.841136: step 9265, loss 0.489757.
Train: 2018-08-02T13:17:59.997343: step 9266, loss 0.534012.
Train: 2018-08-02T13:18:00.153531: step 9267, loss 0.614987.
Train: 2018-08-02T13:18:00.309778: step 9268, loss 0.536786.
Train: 2018-08-02T13:18:00.481610: step 9269, loss 0.537619.
Train: 2018-08-02T13:18:00.622172: step 9270, loss 0.460749.
Test: 2018-08-02T13:18:01.090848: step 9270, loss 0.552194.
Train: 2018-08-02T13:18:01.247042: step 9271, loss 0.507204.
Train: 2018-08-02T13:18:01.403270: step 9272, loss 0.485209.
Train: 2018-08-02T13:18:01.559478: step 9273, loss 0.530788.
Train: 2018-08-02T13:18:01.715667: step 9274, loss 0.439716.
Train: 2018-08-02T13:18:01.871879: step 9275, loss 0.599321.
Train: 2018-08-02T13:18:02.028118: step 9276, loss 0.540251.
Train: 2018-08-02T13:18:02.184339: step 9277, loss 0.438094.
Train: 2018-08-02T13:18:02.340545: step 9278, loss 0.486297.
Train: 2018-08-02T13:18:02.512380: step 9279, loss 0.487006.
Train: 2018-08-02T13:18:02.668575: step 9280, loss 0.523203.
Test: 2018-08-02T13:18:03.121587: step 9280, loss 0.54857.
Train: 2018-08-02T13:18:03.277830: step 9281, loss 0.534028.
Train: 2018-08-02T13:18:03.434043: step 9282, loss 0.550718.
Train: 2018-08-02T13:18:03.590257: step 9283, loss 0.524829.
Train: 2018-08-02T13:18:03.746441: step 9284, loss 0.519475.
Train: 2018-08-02T13:18:03.902653: step 9285, loss 0.474905.
Train: 2018-08-02T13:18:04.058893: step 9286, loss 0.540934.
Train: 2018-08-02T13:18:04.215082: step 9287, loss 0.513563.
Train: 2018-08-02T13:18:04.371326: step 9288, loss 0.511169.
Train: 2018-08-02T13:18:04.527509: step 9289, loss 0.45687.
Train: 2018-08-02T13:18:04.683757: step 9290, loss 0.562865.
Test: 2018-08-02T13:18:05.152361: step 9290, loss 0.548395.
Train: 2018-08-02T13:18:05.308576: step 9291, loss 0.574726.
Train: 2018-08-02T13:18:05.464818: step 9292, loss 0.519087.
Train: 2018-08-02T13:18:05.621032: step 9293, loss 0.538428.
Train: 2018-08-02T13:18:05.777216: step 9294, loss 0.46632.
Train: 2018-08-02T13:18:05.933459: step 9295, loss 0.509109.
Train: 2018-08-02T13:18:06.089677: step 9296, loss 0.409794.
Train: 2018-08-02T13:18:06.245857: step 9297, loss 0.510141.
Train: 2018-08-02T13:18:06.402093: step 9298, loss 0.551132.
Train: 2018-08-02T13:18:06.573936: step 9299, loss 0.503485.
Train: 2018-08-02T13:18:06.730148: step 9300, loss 0.435174.
Test: 2018-08-02T13:18:07.183136: step 9300, loss 0.548826.
Train: 2018-08-02T13:18:07.823610: step 9301, loss 0.508872.
Train: 2018-08-02T13:18:07.979854: step 9302, loss 0.466147.
Train: 2018-08-02T13:18:08.136038: step 9303, loss 0.603117.
Train: 2018-08-02T13:18:08.292251: step 9304, loss 0.548119.
Train: 2018-08-02T13:18:08.448494: step 9305, loss 0.567622.
Train: 2018-08-02T13:18:08.604708: step 9306, loss 0.512231.
Train: 2018-08-02T13:18:08.776514: step 9307, loss 0.539149.
Train: 2018-08-02T13:18:08.932760: step 9308, loss 0.568461.
Train: 2018-08-02T13:18:09.088974: step 9309, loss 0.510005.
Train: 2018-08-02T13:18:09.245154: step 9310, loss 0.521614.
Test: 2018-08-02T13:18:09.698172: step 9310, loss 0.559171.
Train: 2018-08-02T13:18:09.854386: step 9311, loss 0.608116.
Train: 2018-08-02T13:18:10.026221: step 9312, loss 0.511895.
Train: 2018-08-02T13:18:10.182463: step 9313, loss 0.534301.
Train: 2018-08-02T13:18:10.338661: step 9314, loss 0.439921.
Train: 2018-08-02T13:18:10.494890: step 9315, loss 0.544899.
Train: 2018-08-02T13:18:10.651099: step 9316, loss 0.575301.
Train: 2018-08-02T13:18:10.807318: step 9317, loss 0.583449.
Train: 2018-08-02T13:18:10.963530: step 9318, loss 0.537031.
Train: 2018-08-02T13:18:11.119743: step 9319, loss 0.535933.
Train: 2018-08-02T13:18:11.275957: step 9320, loss 0.484342.
Test: 2018-08-02T13:18:11.728947: step 9320, loss 0.556319.
Train: 2018-08-02T13:18:11.885190: step 9321, loss 0.543678.
Train: 2018-08-02T13:18:12.057024: step 9322, loss 0.523493.
Train: 2018-08-02T13:18:12.213243: step 9323, loss 0.540632.
Train: 2018-08-02T13:18:12.369452: step 9324, loss 0.508097.
Train: 2018-08-02T13:18:12.525665: step 9325, loss 0.507862.
Train: 2018-08-02T13:18:12.681879: step 9326, loss 0.521818.
Train: 2018-08-02T13:18:12.838091: step 9327, loss 0.53069.
Train: 2018-08-02T13:18:13.009898: step 9328, loss 0.421097.
Train: 2018-08-02T13:18:13.166129: step 9329, loss 0.644113.
Train: 2018-08-02T13:18:13.322322: step 9330, loss 0.612176.
Test: 2018-08-02T13:18:13.775343: step 9330, loss 0.549739.
Train: 2018-08-02T13:18:13.931581: step 9331, loss 0.559106.
Train: 2018-08-02T13:18:14.087794: step 9332, loss 0.463051.
Train: 2018-08-02T13:18:14.243983: step 9333, loss 0.474275.
Train: 2018-08-02T13:18:14.400195: step 9334, loss 0.571295.
Train: 2018-08-02T13:18:14.556439: step 9335, loss 0.541205.
Train: 2018-08-02T13:18:14.712647: step 9336, loss 0.591554.
Train: 2018-08-02T13:18:14.868837: step 9337, loss 0.55226.
Train: 2018-08-02T13:18:15.025080: step 9338, loss 0.580397.
Train: 2018-08-02T13:18:15.181296: step 9339, loss 0.479236.
Train: 2018-08-02T13:18:15.337511: step 9340, loss 0.562309.
Test: 2018-08-02T13:18:15.806116: step 9340, loss 0.56048.
Train: 2018-08-02T13:18:15.962331: step 9341, loss 0.46332.
Train: 2018-08-02T13:18:16.118545: step 9342, loss 0.60625.
Train: 2018-08-02T13:18:16.274756: step 9343, loss 0.532559.
Train: 2018-08-02T13:18:16.430996: step 9344, loss 0.474957.
Train: 2018-08-02T13:18:16.587191: step 9345, loss 0.549299.
Train: 2018-08-02T13:18:16.759017: step 9346, loss 0.589493.
Train: 2018-08-02T13:18:16.915262: step 9347, loss 0.489585.
Train: 2018-08-02T13:18:17.071474: step 9348, loss 0.535814.
Train: 2018-08-02T13:18:17.227689: step 9349, loss 0.454785.
Train: 2018-08-02T13:18:17.383902: step 9350, loss 0.533213.
Test: 2018-08-02T13:18:17.836918: step 9350, loss 0.554802.
Train: 2018-08-02T13:18:17.993139: step 9351, loss 0.469212.
Train: 2018-08-02T13:18:18.149349: step 9352, loss 0.675903.
Train: 2018-08-02T13:18:18.305532: step 9353, loss 0.549625.
Train: 2018-08-02T13:18:18.477365: step 9354, loss 0.47587.
Train: 2018-08-02T13:18:18.633610: step 9355, loss 0.508489.
Train: 2018-08-02T13:18:18.789794: step 9356, loss 0.469596.
Train: 2018-08-02T13:18:18.946006: step 9357, loss 0.488007.
Train: 2018-08-02T13:18:19.117872: step 9358, loss 0.617636.
Train: 2018-08-02T13:18:19.274086: step 9359, loss 0.541951.
Train: 2018-08-02T13:18:19.430293: step 9360, loss 0.525908.
Test: 2018-08-02T13:18:19.883330: step 9360, loss 0.549659.
Train: 2018-08-02T13:18:20.039530: step 9361, loss 0.510168.
Train: 2018-08-02T13:18:20.164472: step 9362, loss 0.568658.
Train: 2018-08-02T13:18:20.320714: step 9363, loss 0.556555.
Train: 2018-08-02T13:18:20.476927: step 9364, loss 0.540337.
Train: 2018-08-02T13:18:20.633135: step 9365, loss 0.583768.
Train: 2018-08-02T13:18:20.789325: step 9366, loss 0.494816.
Train: 2018-08-02T13:18:20.945567: step 9367, loss 0.558379.
Train: 2018-08-02T13:18:21.101781: step 9368, loss 0.589004.
Train: 2018-08-02T13:18:21.257995: step 9369, loss 0.502131.
Train: 2018-08-02T13:18:21.414207: step 9370, loss 0.475574.
Test: 2018-08-02T13:18:21.867228: step 9370, loss 0.552029.
Train: 2018-08-02T13:18:22.023410: step 9371, loss 0.50439.
Train: 2018-08-02T13:18:22.179625: step 9372, loss 0.498675.
Train: 2018-08-02T13:18:22.335867: step 9373, loss 0.590431.
Train: 2018-08-02T13:18:22.507697: step 9374, loss 0.474582.
Train: 2018-08-02T13:18:22.663885: step 9375, loss 0.522168.
Train: 2018-08-02T13:18:22.820100: step 9376, loss 0.488591.
Train: 2018-08-02T13:18:22.976343: step 9377, loss 0.523409.
Train: 2018-08-02T13:18:23.132525: step 9378, loss 0.591587.
Train: 2018-08-02T13:18:23.304386: step 9379, loss 0.512137.
Train: 2018-08-02T13:18:23.444982: step 9380, loss 0.506444.
Test: 2018-08-02T13:18:23.913624: step 9380, loss 0.553612.
Train: 2018-08-02T13:18:24.069807: step 9381, loss 0.51641.
Train: 2018-08-02T13:18:24.226049: step 9382, loss 0.510226.
Train: 2018-08-02T13:18:24.382263: step 9383, loss 0.46513.
Train: 2018-08-02T13:18:24.538476: step 9384, loss 0.525331.
Train: 2018-08-02T13:18:24.710287: step 9385, loss 0.447779.
Train: 2018-08-02T13:18:24.866526: step 9386, loss 0.556653.
Train: 2018-08-02T13:18:25.022733: step 9387, loss 0.478273.
Train: 2018-08-02T13:18:25.178952: step 9388, loss 0.505719.
Train: 2018-08-02T13:18:25.335193: step 9389, loss 0.426423.
Train: 2018-08-02T13:18:25.491348: step 9390, loss 0.532223.
Test: 2018-08-02T13:18:25.960019: step 9390, loss 0.547964.
Train: 2018-08-02T13:18:26.116232: step 9391, loss 0.543182.
Train: 2018-08-02T13:18:26.272447: step 9392, loss 0.652484.
Train: 2018-08-02T13:18:26.428655: step 9393, loss 0.500417.
Train: 2018-08-02T13:18:26.584872: step 9394, loss 0.55738.
Train: 2018-08-02T13:18:26.741088: step 9395, loss 0.56255.
Train: 2018-08-02T13:18:26.897299: step 9396, loss 0.559588.
Train: 2018-08-02T13:18:27.053513: step 9397, loss 0.46636.
Train: 2018-08-02T13:18:27.209727: step 9398, loss 0.522505.
Train: 2018-08-02T13:18:27.365911: step 9399, loss 0.480615.
Train: 2018-08-02T13:18:27.522153: step 9400, loss 0.481523.
Test: 2018-08-02T13:18:27.990764: step 9400, loss 0.55071.
Train: 2018-08-02T13:18:28.631268: step 9401, loss 0.587176.
Train: 2018-08-02T13:18:28.787482: step 9402, loss 0.500114.
Train: 2018-08-02T13:18:28.959287: step 9403, loss 0.531715.
Train: 2018-08-02T13:18:29.115500: step 9404, loss 0.553855.
Train: 2018-08-02T13:18:29.271714: step 9405, loss 0.51733.
Train: 2018-08-02T13:18:29.427956: step 9406, loss 0.564071.
Train: 2018-08-02T13:18:29.584140: step 9407, loss 0.522594.
Train: 2018-08-02T13:18:29.740383: step 9408, loss 0.526152.
Train: 2018-08-02T13:18:29.896597: step 9409, loss 0.596078.
Train: 2018-08-02T13:18:30.052810: step 9410, loss 0.474946.
Test: 2018-08-02T13:18:30.505830: step 9410, loss 0.553875.
Train: 2018-08-02T13:18:30.662013: step 9411, loss 0.554096.
Train: 2018-08-02T13:18:30.818226: step 9412, loss 0.564458.
Train: 2018-08-02T13:18:30.974469: step 9413, loss 0.502248.
Train: 2018-08-02T13:18:31.130683: step 9414, loss 0.570138.
Train: 2018-08-02T13:18:31.286897: step 9415, loss 0.60631.
Train: 2018-08-02T13:18:31.443079: step 9416, loss 0.529128.
Train: 2018-08-02T13:18:31.614916: step 9417, loss 0.51663.
Train: 2018-08-02T13:18:31.771159: step 9418, loss 0.491413.
Train: 2018-08-02T13:18:31.927393: step 9419, loss 0.549984.
Train: 2018-08-02T13:18:32.083579: step 9420, loss 0.528814.
Test: 2018-08-02T13:18:32.552195: step 9420, loss 0.562512.
Train: 2018-08-02T13:18:32.708409: step 9421, loss 0.519456.
Train: 2018-08-02T13:18:32.864621: step 9422, loss 0.55964.
Train: 2018-08-02T13:18:33.020836: step 9423, loss 0.513491.
Train: 2018-08-02T13:18:33.177049: step 9424, loss 0.550391.
Train: 2018-08-02T13:18:33.411399: step 9425, loss 0.555708.
Train: 2018-08-02T13:18:33.567581: step 9426, loss 0.629613.
Train: 2018-08-02T13:18:33.723826: step 9427, loss 0.472765.
Train: 2018-08-02T13:18:33.880039: step 9428, loss 0.463908.
Train: 2018-08-02T13:18:34.036252: step 9429, loss 0.455124.
Train: 2018-08-02T13:18:34.192436: step 9430, loss 0.482651.
Test: 2018-08-02T13:18:34.661076: step 9430, loss 0.5521.
Train: 2018-08-02T13:18:34.817290: step 9431, loss 0.548625.
Train: 2018-08-02T13:18:34.973527: step 9432, loss 0.521802.
Train: 2018-08-02T13:18:35.145361: step 9433, loss 0.481341.
Train: 2018-08-02T13:18:35.301581: step 9434, loss 0.593362.
Train: 2018-08-02T13:18:35.442160: step 9435, loss 0.474691.
Train: 2018-08-02T13:18:35.613978: step 9436, loss 0.485948.
Train: 2018-08-02T13:18:35.770221: step 9437, loss 0.556606.
Train: 2018-08-02T13:18:35.926405: step 9438, loss 0.504794.
Train: 2018-08-02T13:18:36.082648: step 9439, loss 0.530028.
Train: 2018-08-02T13:18:36.238831: step 9440, loss 0.510868.
Test: 2018-08-02T13:18:36.691850: step 9440, loss 0.550755.
Train: 2018-08-02T13:18:36.863720: step 9441, loss 0.507075.
Train: 2018-08-02T13:18:37.019930: step 9442, loss 0.54456.
Train: 2018-08-02T13:18:37.176142: step 9443, loss 0.479627.
Train: 2018-08-02T13:18:37.332326: step 9444, loss 0.535955.
Train: 2018-08-02T13:18:37.488572: step 9445, loss 0.447221.
Train: 2018-08-02T13:18:37.644751: step 9446, loss 0.606455.
Train: 2018-08-02T13:18:37.800970: step 9447, loss 0.521946.
Train: 2018-08-02T13:18:37.957209: step 9448, loss 0.521063.
Train: 2018-08-02T13:18:38.113423: step 9449, loss 0.52728.
Train: 2018-08-02T13:18:38.269609: step 9450, loss 0.525658.
Test: 2018-08-02T13:18:38.738247: step 9450, loss 0.548727.
Train: 2018-08-02T13:18:38.894490: step 9451, loss 0.56902.
Train: 2018-08-02T13:18:39.050673: step 9452, loss 0.51784.
Train: 2018-08-02T13:18:39.206921: step 9453, loss 0.542764.
Train: 2018-08-02T13:18:39.363100: step 9454, loss 0.581412.
Train: 2018-08-02T13:18:39.519343: step 9455, loss 0.605357.
Train: 2018-08-02T13:18:39.675557: step 9456, loss 0.571147.
Train: 2018-08-02T13:18:39.831775: step 9457, loss 0.505689.
Train: 2018-08-02T13:18:39.987955: step 9458, loss 0.570952.
Train: 2018-08-02T13:18:40.159790: step 9459, loss 0.482054.
Train: 2018-08-02T13:18:40.316001: step 9460, loss 0.506962.
Test: 2018-08-02T13:18:40.769020: step 9460, loss 0.552381.
Train: 2018-08-02T13:18:40.925268: step 9461, loss 0.518854.
Train: 2018-08-02T13:18:41.097069: step 9462, loss 0.555246.
Train: 2018-08-02T13:18:41.253316: step 9463, loss 0.545701.
Train: 2018-08-02T13:18:41.409521: step 9464, loss 0.574017.
Train: 2018-08-02T13:18:41.565741: step 9465, loss 0.47469.
Train: 2018-08-02T13:18:41.737575: step 9466, loss 0.485125.
Train: 2018-08-02T13:18:41.893783: step 9467, loss 0.596496.
Train: 2018-08-02T13:18:42.049972: step 9468, loss 0.539823.
Train: 2018-08-02T13:18:42.206214: step 9469, loss 0.555078.
Train: 2018-08-02T13:18:42.378019: step 9470, loss 0.515173.
Test: 2018-08-02T13:18:42.831037: step 9470, loss 0.553045.
Train: 2018-08-02T13:18:42.987251: step 9471, loss 0.555849.
Train: 2018-08-02T13:18:43.143495: step 9472, loss 0.522703.
Train: 2018-08-02T13:18:43.299696: step 9473, loss 0.544008.
Train: 2018-08-02T13:18:43.455927: step 9474, loss 0.498865.
Train: 2018-08-02T13:18:43.612136: step 9475, loss 0.512811.
Train: 2018-08-02T13:18:43.768349: step 9476, loss 0.461956.
Train: 2018-08-02T13:18:43.940177: step 9477, loss 0.518908.
Train: 2018-08-02T13:18:44.096368: step 9478, loss 0.518629.
Train: 2018-08-02T13:18:44.252579: step 9479, loss 0.480389.
Train: 2018-08-02T13:18:44.408793: step 9480, loss 0.533277.
Test: 2018-08-02T13:18:44.877433: step 9480, loss 0.547609.
Train: 2018-08-02T13:18:45.080536: step 9481, loss 0.485569.
Train: 2018-08-02T13:18:45.252346: step 9482, loss 0.50002.
Train: 2018-08-02T13:18:45.408559: step 9483, loss 0.52616.
Train: 2018-08-02T13:18:45.564798: step 9484, loss 0.494916.
Train: 2018-08-02T13:18:45.721012: step 9485, loss 0.459946.
Train: 2018-08-02T13:18:45.877225: step 9486, loss 0.551143.
Train: 2018-08-02T13:18:46.033443: step 9487, loss 0.536642.
Train: 2018-08-02T13:18:46.189654: step 9488, loss 0.556838.
Train: 2018-08-02T13:18:46.345870: step 9489, loss 0.522997.
Train: 2018-08-02T13:18:46.502083: step 9490, loss 0.563097.
Test: 2018-08-02T13:18:46.955073: step 9490, loss 0.559568.
Train: 2018-08-02T13:18:47.126918: step 9491, loss 0.577126.
Train: 2018-08-02T13:18:47.283120: step 9492, loss 0.510281.
Train: 2018-08-02T13:18:47.439333: step 9493, loss 0.511496.
Train: 2018-08-02T13:18:47.595579: step 9494, loss 0.459356.
Train: 2018-08-02T13:18:47.751787: step 9495, loss 0.492307.
Train: 2018-08-02T13:18:47.908005: step 9496, loss 0.626175.
Train: 2018-08-02T13:18:48.048565: step 9497, loss 0.487044.
Train: 2018-08-02T13:18:48.204779: step 9498, loss 0.50856.
Train: 2018-08-02T13:18:48.360992: step 9499, loss 0.489727.
Train: 2018-08-02T13:18:48.532828: step 9500, loss 0.545669.
Test: 2018-08-02T13:18:48.985846: step 9500, loss 0.550932.
Train: 2018-08-02T13:18:49.657564: step 9501, loss 0.592438.
Train: 2018-08-02T13:18:49.813779: step 9502, loss 0.499006.
Train: 2018-08-02T13:18:49.985612: step 9503, loss 0.531798.
Train: 2018-08-02T13:18:50.141855: step 9504, loss 0.58814.
Train: 2018-08-02T13:18:50.298069: step 9505, loss 0.561726.
Train: 2018-08-02T13:18:50.454252: step 9506, loss 0.527345.
Train: 2018-08-02T13:18:50.610496: step 9507, loss 0.50266.
Train: 2018-08-02T13:18:50.766709: step 9508, loss 0.455179.
Train: 2018-08-02T13:18:50.907272: step 9509, loss 0.500344.
Train: 2018-08-02T13:18:51.063486: step 9510, loss 0.50168.
Test: 2018-08-02T13:18:51.532125: step 9510, loss 0.546936.
Train: 2018-08-02T13:18:51.688367: step 9511, loss 0.526732.
Train: 2018-08-02T13:18:51.844553: step 9512, loss 0.491368.
Train: 2018-08-02T13:18:51.969554: step 9513, loss 0.387646.
Train: 2018-08-02T13:18:52.125738: step 9514, loss 0.545446.
Train: 2018-08-02T13:18:52.281951: step 9515, loss 0.552111.
Train: 2018-08-02T13:18:52.438191: step 9516, loss 0.510319.
Train: 2018-08-02T13:18:52.610031: step 9517, loss 0.530186.
Train: 2018-08-02T13:18:52.766242: step 9518, loss 0.503703.
Train: 2018-08-02T13:18:52.922456: step 9519, loss 0.512474.
Train: 2018-08-02T13:18:53.078668: step 9520, loss 0.555354.
Test: 2018-08-02T13:18:53.531656: step 9520, loss 0.56257.
Train: 2018-08-02T13:18:53.687899: step 9521, loss 0.505199.
Train: 2018-08-02T13:18:53.844114: step 9522, loss 0.534864.
Train: 2018-08-02T13:18:54.000323: step 9523, loss 0.55751.
Train: 2018-08-02T13:18:54.156510: step 9524, loss 0.545269.
Train: 2018-08-02T13:18:54.312723: step 9525, loss 0.449849.
Train: 2018-08-02T13:18:54.484560: step 9526, loss 0.495501.
Train: 2018-08-02T13:18:54.625152: step 9527, loss 0.502393.
Train: 2018-08-02T13:18:54.781365: step 9528, loss 0.542184.
Train: 2018-08-02T13:18:54.937606: step 9529, loss 0.475314.
Train: 2018-08-02T13:18:55.093817: step 9530, loss 0.441996.
Test: 2018-08-02T13:18:55.562460: step 9530, loss 0.547845.
Train: 2018-08-02T13:18:55.718646: step 9531, loss 0.583116.
Train: 2018-08-02T13:18:55.874889: step 9532, loss 0.520861.
Train: 2018-08-02T13:18:56.031072: step 9533, loss 0.538901.
Train: 2018-08-02T13:18:56.187310: step 9534, loss 0.502598.
Train: 2018-08-02T13:18:56.343524: step 9535, loss 0.448997.
Train: 2018-08-02T13:18:56.499713: step 9536, loss 0.531403.
Train: 2018-08-02T13:18:56.671556: step 9537, loss 0.514855.
Train: 2018-08-02T13:18:56.827768: step 9538, loss 0.556887.
Train: 2018-08-02T13:18:56.984004: step 9539, loss 0.487791.
Train: 2018-08-02T13:18:57.140210: step 9540, loss 0.522663.
Test: 2018-08-02T13:18:57.593213: step 9540, loss 0.551104.
Train: 2018-08-02T13:18:57.749451: step 9541, loss 0.48682.
Train: 2018-08-02T13:18:57.905665: step 9542, loss 0.673385.
Train: 2018-08-02T13:18:58.061877: step 9543, loss 0.529842.
Train: 2018-08-02T13:18:58.218060: step 9544, loss 0.547927.
Train: 2018-08-02T13:18:58.374303: step 9545, loss 0.525939.
Train: 2018-08-02T13:18:58.530516: step 9546, loss 0.460239.
Train: 2018-08-02T13:18:58.686731: step 9547, loss 0.505798.
Train: 2018-08-02T13:18:58.842913: step 9548, loss 0.534105.
Train: 2018-08-02T13:18:58.999156: step 9549, loss 0.453224.
Train: 2018-08-02T13:18:59.155341: step 9550, loss 0.605389.
Test: 2018-08-02T13:18:59.608360: step 9550, loss 0.548666.
Train: 2018-08-02T13:18:59.780224: step 9551, loss 0.489764.
Train: 2018-08-02T13:18:59.936441: step 9552, loss 0.462038.
Train: 2018-08-02T13:19:00.092645: step 9553, loss 0.55822.
Train: 2018-08-02T13:19:00.248860: step 9554, loss 0.496465.
Train: 2018-08-02T13:19:00.405047: step 9555, loss 0.478616.
Train: 2018-08-02T13:19:00.576917: step 9556, loss 0.501005.
Train: 2018-08-02T13:19:00.733126: step 9557, loss 0.531252.
Train: 2018-08-02T13:19:00.889310: step 9558, loss 0.515539.
Train: 2018-08-02T13:19:01.029926: step 9559, loss 0.607144.
Train: 2018-08-02T13:19:01.201766: step 9560, loss 0.546401.
Test: 2018-08-02T13:19:01.654755: step 9560, loss 0.560298.
Train: 2018-08-02T13:19:01.810969: step 9561, loss 0.511273.
Train: 2018-08-02T13:19:01.967182: step 9562, loss 0.482723.
Train: 2018-08-02T13:19:02.123426: step 9563, loss 0.569364.
Train: 2018-08-02T13:19:02.295264: step 9564, loss 0.527927.
Train: 2018-08-02T13:19:02.451443: step 9565, loss 0.584323.
Train: 2018-08-02T13:19:02.607686: step 9566, loss 0.497157.
Train: 2018-08-02T13:19:02.763900: step 9567, loss 0.477253.
Train: 2018-08-02T13:19:02.920113: step 9568, loss 0.491945.
Train: 2018-08-02T13:19:03.076327: step 9569, loss 0.641858.
Train: 2018-08-02T13:19:03.232541: step 9570, loss 0.564707.
Test: 2018-08-02T13:19:03.701150: step 9570, loss 0.549538.
Train: 2018-08-02T13:19:03.857394: step 9571, loss 0.604175.
Train: 2018-08-02T13:19:04.013577: step 9572, loss 0.605827.
Train: 2018-08-02T13:19:04.169820: step 9573, loss 0.496879.
Train: 2018-08-02T13:19:04.326038: step 9574, loss 0.543324.
Train: 2018-08-02T13:19:04.482248: step 9575, loss 0.46615.
Train: 2018-08-02T13:19:04.638456: step 9576, loss 0.470093.
Train: 2018-08-02T13:19:04.794684: step 9577, loss 0.520896.
Train: 2018-08-02T13:19:04.950857: step 9578, loss 0.569682.
Train: 2018-08-02T13:19:05.122723: step 9579, loss 0.551352.
Train: 2018-08-02T13:19:05.278938: step 9580, loss 0.551974.
Test: 2018-08-02T13:19:05.731950: step 9580, loss 0.547249.
Train: 2018-08-02T13:19:05.888169: step 9581, loss 0.609494.
Train: 2018-08-02T13:19:06.060002: step 9582, loss 0.546832.
Train: 2018-08-02T13:19:06.216186: step 9583, loss 0.567798.
Train: 2018-08-02T13:19:06.372401: step 9584, loss 0.493684.
Train: 2018-08-02T13:19:06.528615: step 9585, loss 0.470203.
Train: 2018-08-02T13:19:06.684828: step 9586, loss 0.465818.
Train: 2018-08-02T13:19:06.841041: step 9587, loss 0.619733.
Train: 2018-08-02T13:19:07.012874: step 9588, loss 0.500273.
Train: 2018-08-02T13:19:07.169122: step 9589, loss 0.447435.
Train: 2018-08-02T13:19:07.325332: step 9590, loss 0.488024.
Test: 2018-08-02T13:19:07.793942: step 9590, loss 0.547837.
Train: 2018-08-02T13:19:07.950180: step 9591, loss 0.587602.
Train: 2018-08-02T13:19:08.106403: step 9592, loss 0.448155.
Train: 2018-08-02T13:19:08.262608: step 9593, loss 0.540413.
Train: 2018-08-02T13:19:08.418821: step 9594, loss 0.552706.
Train: 2018-08-02T13:19:08.575039: step 9595, loss 0.516933.
Train: 2018-08-02T13:19:08.731223: step 9596, loss 0.544769.
Train: 2018-08-02T13:19:08.887466: step 9597, loss 0.512028.
Train: 2018-08-02T13:19:09.043654: step 9598, loss 0.554229.
Train: 2018-08-02T13:19:09.199893: step 9599, loss 0.504011.
Train: 2018-08-02T13:19:09.356106: step 9600, loss 0.415326.
Test: 2018-08-02T13:19:09.809098: step 9600, loss 0.5515.
Train: 2018-08-02T13:19:10.433974: step 9601, loss 0.533721.
Train: 2018-08-02T13:19:10.590194: step 9602, loss 0.517427.
Train: 2018-08-02T13:19:10.746403: step 9603, loss 0.520233.
Train: 2018-08-02T13:19:10.902598: step 9604, loss 0.502452.
Train: 2018-08-02T13:19:11.058802: step 9605, loss 0.546846.
Train: 2018-08-02T13:19:11.215047: step 9606, loss 0.453067.
Train: 2018-08-02T13:19:11.371230: step 9607, loss 0.478892.
Train: 2018-08-02T13:19:11.527473: step 9608, loss 0.457677.
Train: 2018-08-02T13:19:11.683657: step 9609, loss 0.548919.
Train: 2018-08-02T13:19:11.839900: step 9610, loss 0.548369.
Test: 2018-08-02T13:19:12.308511: step 9610, loss 0.549136.
Train: 2018-08-02T13:19:12.464724: step 9611, loss 0.545328.
Train: 2018-08-02T13:19:12.620937: step 9612, loss 0.445192.
Train: 2018-08-02T13:19:12.777180: step 9613, loss 0.559151.
Train: 2018-08-02T13:19:12.933366: step 9614, loss 0.536567.
Train: 2018-08-02T13:19:13.089579: step 9615, loss 0.511693.
Train: 2018-08-02T13:19:13.245790: step 9616, loss 0.432978.
Train: 2018-08-02T13:19:13.402035: step 9617, loss 0.664372.
Train: 2018-08-02T13:19:13.573841: step 9618, loss 0.479515.
Train: 2018-08-02T13:19:13.730082: step 9619, loss 0.510077.
Train: 2018-08-02T13:19:13.886265: step 9620, loss 0.519652.
Test: 2018-08-02T13:19:14.354906: step 9620, loss 0.553175.
Train: 2018-08-02T13:19:14.511149: step 9621, loss 0.564726.
Train: 2018-08-02T13:19:14.667362: step 9622, loss 0.560647.
Train: 2018-08-02T13:19:14.823547: step 9623, loss 0.522918.
Train: 2018-08-02T13:19:14.979758: step 9624, loss 0.51301.
Train: 2018-08-02T13:19:15.135974: step 9625, loss 0.490543.
Train: 2018-08-02T13:19:15.292216: step 9626, loss 0.536942.
Train: 2018-08-02T13:19:15.448400: step 9627, loss 0.451754.
Train: 2018-08-02T13:19:15.604612: step 9628, loss 0.540792.
Train: 2018-08-02T13:19:15.760826: step 9629, loss 0.473697.
Train: 2018-08-02T13:19:15.917071: step 9630, loss 0.547125.
Test: 2018-08-02T13:19:16.370060: step 9630, loss 0.550231.
Train: 2018-08-02T13:19:16.526302: step 9631, loss 0.491396.
Train: 2018-08-02T13:19:16.682486: step 9632, loss 0.540701.
Train: 2018-08-02T13:19:16.838729: step 9633, loss 0.487192.
Train: 2018-08-02T13:19:16.994912: step 9634, loss 0.595159.
Train: 2018-08-02T13:19:17.151127: step 9635, loss 0.547598.
Train: 2018-08-02T13:19:17.322991: step 9636, loss 0.472758.
Train: 2018-08-02T13:19:17.479203: step 9637, loss 0.54822.
Train: 2018-08-02T13:19:17.635424: step 9638, loss 0.541421.
Train: 2018-08-02T13:19:17.791626: step 9639, loss 0.544722.
Train: 2018-08-02T13:19:17.947844: step 9640, loss 0.466597.
Test: 2018-08-02T13:19:18.416455: step 9640, loss 0.55289.
Train: 2018-08-02T13:19:18.572703: step 9641, loss 0.51876.
Train: 2018-08-02T13:19:18.728911: step 9642, loss 0.576882.
Train: 2018-08-02T13:19:18.869475: step 9643, loss 0.451458.
Train: 2018-08-02T13:19:19.025689: step 9644, loss 0.472673.
Train: 2018-08-02T13:19:19.197523: step 9645, loss 0.486493.
Train: 2018-08-02T13:19:19.353760: step 9646, loss 0.534215.
Train: 2018-08-02T13:19:19.509947: step 9647, loss 0.494751.
Train: 2018-08-02T13:19:19.666202: step 9648, loss 0.491931.
Train: 2018-08-02T13:19:19.822406: step 9649, loss 0.495626.
Train: 2018-08-02T13:19:19.978589: step 9650, loss 0.542051.
Test: 2018-08-02T13:19:20.447254: step 9650, loss 0.548505.
Train: 2018-08-02T13:19:20.603468: step 9651, loss 0.55523.
Train: 2018-08-02T13:19:20.759685: step 9652, loss 0.45692.
Train: 2018-08-02T13:19:20.915872: step 9653, loss 0.555749.
Train: 2018-08-02T13:19:21.072113: step 9654, loss 0.565581.
Train: 2018-08-02T13:19:21.228299: step 9655, loss 0.508849.
Train: 2018-08-02T13:19:21.384541: step 9656, loss 0.484062.
Train: 2018-08-02T13:19:21.556344: step 9657, loss 0.481671.
Train: 2018-08-02T13:19:21.712557: step 9658, loss 0.546408.
Train: 2018-08-02T13:19:21.868802: step 9659, loss 0.488308.
Train: 2018-08-02T13:19:22.024992: step 9660, loss 0.578259.
Test: 2018-08-02T13:19:22.493625: step 9660, loss 0.548509.
Train: 2018-08-02T13:19:22.634247: step 9661, loss 0.474555.
Train: 2018-08-02T13:19:22.790430: step 9662, loss 0.518636.
Train: 2018-08-02T13:19:22.946674: step 9663, loss 0.552373.
Train: 2018-08-02T13:19:23.071649: step 9664, loss 0.589497.
Train: 2018-08-02T13:19:23.227835: step 9665, loss 0.515017.
Train: 2018-08-02T13:19:23.384041: step 9666, loss 0.511875.
Train: 2018-08-02T13:19:23.540285: step 9667, loss 0.508642.
Train: 2018-08-02T13:19:23.696503: step 9668, loss 0.527509.
Train: 2018-08-02T13:19:23.852682: step 9669, loss 0.457071.
Train: 2018-08-02T13:19:24.008926: step 9670, loss 0.517314.
Test: 2018-08-02T13:19:24.461914: step 9670, loss 0.549888.
Train: 2018-08-02T13:19:24.618158: step 9671, loss 0.448098.
Train: 2018-08-02T13:19:24.774341: step 9672, loss 0.512081.
Train: 2018-08-02T13:19:24.930584: step 9673, loss 0.5402.
Train: 2018-08-02T13:19:25.086768: step 9674, loss 0.482242.
Train: 2018-08-02T13:19:25.243012: step 9675, loss 0.524519.
Train: 2018-08-02T13:19:25.399222: step 9676, loss 0.565586.
Train: 2018-08-02T13:19:25.571064: step 9677, loss 0.493192.
Train: 2018-08-02T13:19:25.711652: step 9678, loss 0.493927.
Train: 2018-08-02T13:19:25.867864: step 9679, loss 0.441055.
Train: 2018-08-02T13:19:26.024048: step 9680, loss 0.540854.
Test: 2018-08-02T13:19:26.492688: step 9680, loss 0.549131.
Train: 2018-08-02T13:19:26.648902: step 9681, loss 0.489698.
Train: 2018-08-02T13:19:26.805116: step 9682, loss 0.502022.
Train: 2018-08-02T13:19:26.961364: step 9683, loss 0.438139.
Train: 2018-08-02T13:19:27.117572: step 9684, loss 0.478018.
Train: 2018-08-02T13:19:27.273785: step 9685, loss 0.470386.
Train: 2018-08-02T13:19:27.429968: step 9686, loss 0.552301.
Train: 2018-08-02T13:19:27.586214: step 9687, loss 0.553766.
Train: 2018-08-02T13:19:27.758043: step 9688, loss 0.544635.
Train: 2018-08-02T13:19:27.898639: step 9689, loss 0.508452.
Train: 2018-08-02T13:19:28.054852: step 9690, loss 0.553276.
Test: 2018-08-02T13:19:28.523493: step 9690, loss 0.548465.
Train: 2018-08-02T13:19:28.679700: step 9691, loss 0.470662.
Train: 2018-08-02T13:19:28.835922: step 9692, loss 0.540047.
Train: 2018-08-02T13:19:28.976511: step 9693, loss 0.499511.
Train: 2018-08-02T13:19:29.148351: step 9694, loss 0.508247.
Train: 2018-08-02T13:19:29.304561: step 9695, loss 0.532799.
Train: 2018-08-02T13:19:29.460774: step 9696, loss 0.575429.
Train: 2018-08-02T13:19:29.616988: step 9697, loss 0.529779.
Train: 2018-08-02T13:19:29.773170: step 9698, loss 0.532746.
Train: 2018-08-02T13:19:29.929413: step 9699, loss 0.524355.
Train: 2018-08-02T13:19:30.085597: step 9700, loss 0.53496.
Test: 2018-08-02T13:19:30.538616: step 9700, loss 0.549306.
Train: 2018-08-02T13:19:31.194712: step 9701, loss 0.531722.
Train: 2018-08-02T13:19:31.350956: step 9702, loss 0.565164.
Train: 2018-08-02T13:19:31.507139: step 9703, loss 0.468282.
Train: 2018-08-02T13:19:31.663378: step 9704, loss 0.482364.
Train: 2018-08-02T13:19:31.819567: step 9705, loss 0.557454.
Train: 2018-08-02T13:19:31.975810: step 9706, loss 0.535525.
Train: 2018-08-02T13:19:32.132024: step 9707, loss 0.52294.
Train: 2018-08-02T13:19:32.288206: step 9708, loss 0.516954.
Train: 2018-08-02T13:19:32.444421: step 9709, loss 0.502792.
Train: 2018-08-02T13:19:32.600632: step 9710, loss 0.520066.
Test: 2018-08-02T13:19:33.069304: step 9710, loss 0.555311.
Train: 2018-08-02T13:19:33.225487: step 9711, loss 0.47529.
Train: 2018-08-02T13:19:33.381731: step 9712, loss 0.543855.
Train: 2018-08-02T13:19:33.537949: step 9713, loss 0.448714.
Train: 2018-08-02T13:19:33.694162: step 9714, loss 0.502624.
Train: 2018-08-02T13:19:33.850341: step 9715, loss 0.509108.
Train: 2018-08-02T13:19:34.006553: step 9716, loss 0.555509.
Train: 2018-08-02T13:19:34.162800: step 9717, loss 0.546139.
Train: 2018-08-02T13:19:34.318982: step 9718, loss 0.514588.
Train: 2018-08-02T13:19:34.475225: step 9719, loss 0.514737.
Train: 2018-08-02T13:19:34.631433: step 9720, loss 0.488474.
Test: 2018-08-02T13:19:35.100073: step 9720, loss 0.549391.
Train: 2018-08-02T13:19:35.256292: step 9721, loss 0.50347.
Train: 2018-08-02T13:19:35.412500: step 9722, loss 0.471497.
Train: 2018-08-02T13:19:35.568718: step 9723, loss 0.476856.
Train: 2018-08-02T13:19:35.724926: step 9724, loss 0.561972.
Train: 2018-08-02T13:19:35.881146: step 9725, loss 0.571096.
Train: 2018-08-02T13:19:36.037359: step 9726, loss 0.507413.
Train: 2018-08-02T13:19:36.193543: step 9727, loss 0.513652.
Train: 2018-08-02T13:19:36.334163: step 9728, loss 0.57824.
Train: 2018-08-02T13:19:36.490347: step 9729, loss 0.509678.
Train: 2018-08-02T13:19:36.646561: step 9730, loss 0.497922.
Test: 2018-08-02T13:19:37.115225: step 9730, loss 0.55492.
Train: 2018-08-02T13:19:37.271413: step 9731, loss 0.488547.
Train: 2018-08-02T13:19:37.427657: step 9732, loss 0.53991.
Train: 2018-08-02T13:19:37.583841: step 9733, loss 0.530234.
Train: 2018-08-02T13:19:37.740084: step 9734, loss 0.662344.
Train: 2018-08-02T13:19:37.880677: step 9735, loss 0.589516.
Train: 2018-08-02T13:19:38.036861: step 9736, loss 0.530337.
Train: 2018-08-02T13:19:38.208695: step 9737, loss 0.49363.
Train: 2018-08-02T13:19:38.349312: step 9738, loss 0.53013.
Train: 2018-08-02T13:19:38.505531: step 9739, loss 0.547049.
Train: 2018-08-02T13:19:38.661747: step 9740, loss 0.532644.
Test: 2018-08-02T13:19:39.130379: step 9740, loss 0.553175.
Train: 2018-08-02T13:19:39.286597: step 9741, loss 0.544027.
Train: 2018-08-02T13:19:39.442810: step 9742, loss 0.470229.
Train: 2018-08-02T13:19:39.583429: step 9743, loss 0.45226.
Train: 2018-08-02T13:19:39.755239: step 9744, loss 0.522598.
Train: 2018-08-02T13:19:39.895829: step 9745, loss 0.478202.
Train: 2018-08-02T13:19:40.052021: step 9746, loss 0.469116.
Train: 2018-08-02T13:19:40.223849: step 9747, loss 0.510228.
Train: 2018-08-02T13:19:40.380091: step 9748, loss 0.53239.
Train: 2018-08-02T13:19:40.536305: step 9749, loss 0.48735.
Train: 2018-08-02T13:19:40.692502: step 9750, loss 0.546108.
Test: 2018-08-02T13:19:41.145507: step 9750, loss 0.547914.
Train: 2018-08-02T13:19:41.301751: step 9751, loss 0.564366.
Train: 2018-08-02T13:19:41.457935: step 9752, loss 0.551959.
Train: 2018-08-02T13:19:41.614178: step 9753, loss 0.447885.
Train: 2018-08-02T13:19:41.770392: step 9754, loss 0.508247.
Train: 2018-08-02T13:19:41.942195: step 9755, loss 0.47958.
Train: 2018-08-02T13:19:42.098441: step 9756, loss 0.568935.
Train: 2018-08-02T13:19:42.239000: step 9757, loss 0.541155.
Train: 2018-08-02T13:19:42.395213: step 9758, loss 0.536907.
Train: 2018-08-02T13:19:42.551457: step 9759, loss 0.50694.
Train: 2018-08-02T13:19:42.707671: step 9760, loss 0.388318.
Test: 2018-08-02T13:19:43.176282: step 9760, loss 0.548172.
Train: 2018-08-02T13:19:43.316903: step 9761, loss 0.443821.
Train: 2018-08-02T13:19:43.473117: step 9762, loss 0.683215.
Train: 2018-08-02T13:19:43.629330: step 9763, loss 0.434128.
Train: 2018-08-02T13:19:43.785539: step 9764, loss 0.494601.
Train: 2018-08-02T13:19:43.941757: step 9765, loss 0.601079.
Train: 2018-08-02T13:19:44.097970: step 9766, loss 0.480018.
Train: 2018-08-02T13:19:44.254184: step 9767, loss 0.540007.
Train: 2018-08-02T13:19:44.410397: step 9768, loss 0.520957.
Train: 2018-08-02T13:19:44.566610: step 9769, loss 0.533885.
Train: 2018-08-02T13:19:44.722819: step 9770, loss 0.530358.
Test: 2018-08-02T13:19:45.191434: step 9770, loss 0.55093.
Train: 2018-08-02T13:19:45.332059: step 9771, loss 0.500811.
Train: 2018-08-02T13:19:45.488242: step 9772, loss 0.560597.
Train: 2018-08-02T13:19:45.660074: step 9773, loss 0.47312.
Train: 2018-08-02T13:19:45.816288: step 9774, loss 0.463535.
Train: 2018-08-02T13:19:45.972527: step 9775, loss 0.503509.
Train: 2018-08-02T13:19:46.128717: step 9776, loss 0.573813.
Train: 2018-08-02T13:19:46.284968: step 9777, loss 0.553734.
Train: 2018-08-02T13:19:46.441143: step 9778, loss 0.506501.
Train: 2018-08-02T13:19:46.597357: step 9779, loss 0.541166.
Train: 2018-08-02T13:19:46.753568: step 9780, loss 0.519957.
Test: 2018-08-02T13:19:47.222208: step 9780, loss 0.553238.
Train: 2018-08-02T13:19:47.362827: step 9781, loss 0.490843.
Train: 2018-08-02T13:19:47.519045: step 9782, loss 0.534989.
Train: 2018-08-02T13:19:47.690849: step 9783, loss 0.552452.
Train: 2018-08-02T13:19:47.847088: step 9784, loss 0.469779.
Train: 2018-08-02T13:19:48.003306: step 9785, loss 0.533563.
Train: 2018-08-02T13:19:48.159523: step 9786, loss 0.477368.
Train: 2018-08-02T13:19:48.315733: step 9787, loss 0.504764.
Train: 2018-08-02T13:19:48.471942: step 9788, loss 0.436657.
Train: 2018-08-02T13:19:48.628129: step 9789, loss 0.61604.
Train: 2018-08-02T13:19:48.800000: step 9790, loss 0.553742.
Test: 2018-08-02T13:19:49.252985: step 9790, loss 0.547042.
Train: 2018-08-02T13:19:49.409227: step 9791, loss 0.509218.
Train: 2018-08-02T13:19:49.565409: step 9792, loss 0.607054.
Train: 2018-08-02T13:19:49.721654: step 9793, loss 0.558268.
Train: 2018-08-02T13:19:49.877868: step 9794, loss 0.571819.
Train: 2018-08-02T13:19:50.034052: step 9795, loss 0.563825.
Train: 2018-08-02T13:19:50.190298: step 9796, loss 0.56356.
Train: 2018-08-02T13:19:50.362099: step 9797, loss 0.554055.
Train: 2018-08-02T13:19:50.518337: step 9798, loss 0.519173.
Train: 2018-08-02T13:19:50.674553: step 9799, loss 0.578938.
Train: 2018-08-02T13:19:50.830739: step 9800, loss 0.557774.
Test: 2018-08-02T13:19:51.283784: step 9800, loss 0.558768.
Train: 2018-08-02T13:19:51.924263: step 9801, loss 0.528384.
Train: 2018-08-02T13:19:52.080476: step 9802, loss 0.518706.
Train: 2018-08-02T13:19:52.236690: step 9803, loss 0.515841.
Train: 2018-08-02T13:19:52.392903: step 9804, loss 0.503961.
Train: 2018-08-02T13:19:52.549117: step 9805, loss 0.440417.
Train: 2018-08-02T13:19:52.705301: step 9806, loss 0.570288.
Train: 2018-08-02T13:19:52.861525: step 9807, loss 0.550441.
Train: 2018-08-02T13:19:53.017727: step 9808, loss 0.592762.
Train: 2018-08-02T13:19:53.173970: step 9809, loss 0.589732.
Train: 2018-08-02T13:19:53.330184: step 9810, loss 0.537556.
Test: 2018-08-02T13:19:53.783173: step 9810, loss 0.564097.
Train: 2018-08-02T13:19:53.939386: step 9811, loss 0.496773.
Train: 2018-08-02T13:19:54.095600: step 9812, loss 0.551752.
Train: 2018-08-02T13:19:54.267460: step 9813, loss 0.474032.
Train: 2018-08-02T13:19:54.423648: step 9814, loss 0.539905.
Train: 2018-08-02T13:19:54.533030: step 9815, loss 0.504622.
Train: 2018-08-02T13:19:54.689241: step 9816, loss 0.573806.
Train: 2018-08-02T13:19:54.845454: step 9817, loss 0.489019.
Train: 2018-08-02T13:19:55.017281: step 9818, loss 0.499118.
Train: 2018-08-02T13:19:55.173473: step 9819, loss 0.629283.
Train: 2018-08-02T13:19:55.329686: step 9820, loss 0.565039.
Test: 2018-08-02T13:19:55.782705: step 9820, loss 0.551664.
Train: 2018-08-02T13:19:55.938917: step 9821, loss 0.523187.
Train: 2018-08-02T13:19:56.095161: step 9822, loss 0.545034.
Train: 2018-08-02T13:19:56.251375: step 9823, loss 0.489737.
Train: 2018-08-02T13:19:56.407588: step 9824, loss 0.458018.
Train: 2018-08-02T13:19:56.563801: step 9825, loss 0.508014.
Train: 2018-08-02T13:19:56.719986: step 9826, loss 0.502019.
Train: 2018-08-02T13:19:56.891820: step 9827, loss 0.500622.
Train: 2018-08-02T13:19:57.048033: step 9828, loss 0.521144.
Train: 2018-08-02T13:19:57.204246: step 9829, loss 0.593076.
Train: 2018-08-02T13:19:57.360489: step 9830, loss 0.541457.
Test: 2018-08-02T13:19:57.829131: step 9830, loss 0.551368.
Train: 2018-08-02T13:19:57.985344: step 9831, loss 0.52042.
Train: 2018-08-02T13:19:58.141526: step 9832, loss 0.547938.
Train: 2018-08-02T13:19:58.297741: step 9833, loss 0.537972.
Train: 2018-08-02T13:19:58.453979: step 9834, loss 0.510971.
Train: 2018-08-02T13:19:58.610168: step 9835, loss 0.514782.
Train: 2018-08-02T13:19:58.766411: step 9836, loss 0.46988.
Train: 2018-08-02T13:19:58.922594: step 9837, loss 0.571887.
Train: 2018-08-02T13:19:59.078839: step 9838, loss 0.511297.
Train: 2018-08-02T13:19:59.235052: step 9839, loss 0.52559.
Train: 2018-08-02T13:19:59.391242: step 9840, loss 0.519302.
Test: 2018-08-02T13:19:59.859875: step 9840, loss 0.546677.
Train: 2018-08-02T13:20:00.016088: step 9841, loss 0.50413.
Train: 2018-08-02T13:20:00.172310: step 9842, loss 0.459926.
Train: 2018-08-02T13:20:00.328540: step 9843, loss 0.60275.
Train: 2018-08-02T13:20:00.484729: step 9844, loss 0.495977.
Train: 2018-08-02T13:20:00.640968: step 9845, loss 0.536647.
Train: 2018-08-02T13:20:00.797213: step 9846, loss 0.447153.
Train: 2018-08-02T13:20:00.953368: step 9847, loss 0.524496.
Train: 2018-08-02T13:20:01.109587: step 9848, loss 0.520443.
Train: 2018-08-02T13:20:01.265821: step 9849, loss 0.536608.
Train: 2018-08-02T13:20:01.422034: step 9850, loss 0.442732.
Test: 2018-08-02T13:20:01.890675: step 9850, loss 0.547233.
Train: 2018-08-02T13:20:02.046891: step 9851, loss 0.521939.
Train: 2018-08-02T13:20:02.203076: step 9852, loss 0.493708.
Train: 2018-08-02T13:20:02.359290: step 9853, loss 0.574666.
Train: 2018-08-02T13:20:02.515504: step 9854, loss 0.459854.
Train: 2018-08-02T13:20:02.671750: step 9855, loss 0.525241.
Train: 2018-08-02T13:20:02.827955: step 9856, loss 0.52204.
Train: 2018-08-02T13:20:02.984174: step 9857, loss 0.546058.
Train: 2018-08-02T13:20:03.155979: step 9858, loss 0.539199.
Train: 2018-08-02T13:20:03.312227: step 9859, loss 0.543543.
Train: 2018-08-02T13:20:03.468434: step 9860, loss 0.621994.
Test: 2018-08-02T13:20:03.937049: step 9860, loss 0.561128.
Train: 2018-08-02T13:20:04.093259: step 9861, loss 0.514382.
Train: 2018-08-02T13:20:04.249502: step 9862, loss 0.54667.
Train: 2018-08-02T13:20:04.405684: step 9863, loss 0.55501.
Train: 2018-08-02T13:20:04.561914: step 9864, loss 0.559791.
Train: 2018-08-02T13:20:04.702521: step 9865, loss 0.525843.
Train: 2018-08-02T13:20:04.858735: step 9866, loss 0.511314.
Train: 2018-08-02T13:20:05.014931: step 9867, loss 0.459701.
Train: 2018-08-02T13:20:05.171159: step 9868, loss 0.499683.
Train: 2018-08-02T13:20:05.327346: step 9869, loss 0.505868.
Train: 2018-08-02T13:20:05.483585: step 9870, loss 0.51526.
Test: 2018-08-02T13:20:05.952199: step 9870, loss 0.547265.
Train: 2018-08-02T13:20:06.108442: step 9871, loss 0.512937.
Train: 2018-08-02T13:20:06.264625: step 9872, loss 0.521887.
Train: 2018-08-02T13:20:06.436461: step 9873, loss 0.430069.
Train: 2018-08-02T13:20:06.592699: step 9874, loss 0.470126.
Train: 2018-08-02T13:20:06.748918: step 9875, loss 0.664862.
Train: 2018-08-02T13:20:06.905125: step 9876, loss 0.459044.
Train: 2018-08-02T13:20:07.061313: step 9877, loss 0.535571.
Train: 2018-08-02T13:20:07.217557: step 9878, loss 0.46873.
Train: 2018-08-02T13:20:07.373741: step 9879, loss 0.483601.
Train: 2018-08-02T13:20:07.529995: step 9880, loss 0.524582.
Test: 2018-08-02T13:20:07.998594: step 9880, loss 0.549659.
Train: 2018-08-02T13:20:08.139186: step 9881, loss 0.480416.
Train: 2018-08-02T13:20:08.295430: step 9882, loss 0.468863.
Train: 2018-08-02T13:20:08.451613: step 9883, loss 0.596264.
Train: 2018-08-02T13:20:08.607857: step 9884, loss 0.384807.
Train: 2018-08-02T13:20:08.764039: step 9885, loss 0.557391.
Train: 2018-08-02T13:20:08.920281: step 9886, loss 0.541009.
Train: 2018-08-02T13:20:09.076466: step 9887, loss 0.494439.
Train: 2018-08-02T13:20:09.232681: step 9888, loss 0.441045.
Train: 2018-08-02T13:20:09.388949: step 9889, loss 0.591225.
Train: 2018-08-02T13:20:09.545137: step 9890, loss 0.508002.
Test: 2018-08-02T13:20:10.013746: step 9890, loss 0.547176.
Train: 2018-08-02T13:20:10.169962: step 9891, loss 0.51321.
Train: 2018-08-02T13:20:10.326175: step 9892, loss 0.530587.
Train: 2018-08-02T13:20:10.482389: step 9893, loss 0.461331.
Train: 2018-08-02T13:20:10.638600: step 9894, loss 0.55584.
Train: 2018-08-02T13:20:10.794842: step 9895, loss 0.490333.
Train: 2018-08-02T13:20:10.951058: step 9896, loss 0.498109.
Train: 2018-08-02T13:20:11.107267: step 9897, loss 0.521522.
Train: 2018-08-02T13:20:11.279119: step 9898, loss 0.494098.
Train: 2018-08-02T13:20:11.435314: step 9899, loss 0.485061.
Train: 2018-08-02T13:20:11.591503: step 9900, loss 0.483706.
Test: 2018-08-02T13:20:12.044522: step 9900, loss 0.54633.
Train: 2018-08-02T13:20:12.747512: step 9901, loss 0.59994.
Train: 2018-08-02T13:20:12.903725: step 9902, loss 0.582314.
Train: 2018-08-02T13:20:13.059939: step 9903, loss 0.457885.
Train: 2018-08-02T13:20:13.216123: step 9904, loss 0.501374.
Train: 2018-08-02T13:20:13.372366: step 9905, loss 0.489556.
Train: 2018-08-02T13:20:13.544172: step 9906, loss 0.4326.
Train: 2018-08-02T13:20:13.700414: step 9907, loss 0.515635.
Train: 2018-08-02T13:20:13.856596: step 9908, loss 0.446099.
Train: 2018-08-02T13:20:14.012840: step 9909, loss 0.498786.
Train: 2018-08-02T13:20:14.169057: step 9910, loss 0.435767.
Test: 2018-08-02T13:20:14.637711: step 9910, loss 0.550493.
Train: 2018-08-02T13:20:14.793922: step 9911, loss 0.480839.
Train: 2018-08-02T13:20:14.950121: step 9912, loss 0.573264.
Train: 2018-08-02T13:20:15.106334: step 9913, loss 0.460993.
Train: 2018-08-02T13:20:15.262548: step 9914, loss 0.477102.
Train: 2018-08-02T13:20:15.418760: step 9915, loss 0.554267.
Train: 2018-08-02T13:20:15.574976: step 9916, loss 0.498501.
Train: 2018-08-02T13:20:15.746813: step 9917, loss 0.575575.
Train: 2018-08-02T13:20:15.903026: step 9918, loss 0.504931.
Train: 2018-08-02T13:20:16.059206: step 9919, loss 0.587865.
Train: 2018-08-02T13:20:16.215420: step 9920, loss 0.503746.
Test: 2018-08-02T13:20:16.684060: step 9920, loss 0.556919.
Train: 2018-08-02T13:20:16.840299: step 9921, loss 0.537583.
Train: 2018-08-02T13:20:16.996486: step 9922, loss 0.473003.
Train: 2018-08-02T13:20:17.152735: step 9923, loss 0.570247.
Train: 2018-08-02T13:20:17.308939: step 9924, loss 0.553654.
Train: 2018-08-02T13:20:17.465127: step 9925, loss 0.525375.
Train: 2018-08-02T13:20:17.621374: step 9926, loss 0.5271.
Train: 2018-08-02T13:20:17.793204: step 9927, loss 0.531489.
Train: 2018-08-02T13:20:17.949419: step 9928, loss 0.559591.
Train: 2018-08-02T13:20:18.105642: step 9929, loss 0.526219.
Train: 2018-08-02T13:20:18.261840: step 9930, loss 0.495992.
Test: 2018-08-02T13:20:18.714835: step 9930, loss 0.556474.
Train: 2018-08-02T13:20:18.886670: step 9931, loss 0.495432.
Train: 2018-08-02T13:20:19.042883: step 9932, loss 0.46709.
Train: 2018-08-02T13:20:19.199097: step 9933, loss 0.547427.
Train: 2018-08-02T13:20:19.355310: step 9934, loss 0.541685.
Train: 2018-08-02T13:20:19.511523: step 9935, loss 0.513272.
Train: 2018-08-02T13:20:19.683358: step 9936, loss 0.509045.
Train: 2018-08-02T13:20:19.839602: step 9937, loss 0.691949.
Train: 2018-08-02T13:20:19.995821: step 9938, loss 0.462985.
Train: 2018-08-02T13:20:20.151997: step 9939, loss 0.490349.
Train: 2018-08-02T13:20:20.308241: step 9940, loss 0.564031.
Test: 2018-08-02T13:20:20.776877: step 9940, loss 0.550852.
Train: 2018-08-02T13:20:20.933095: step 9941, loss 0.482975.
Train: 2018-08-02T13:20:21.089308: step 9942, loss 0.512739.
Train: 2018-08-02T13:20:21.245521: step 9943, loss 0.697063.
Train: 2018-08-02T13:20:21.401734: step 9944, loss 0.496441.
Train: 2018-08-02T13:20:21.557918: step 9945, loss 0.51591.
Train: 2018-08-02T13:20:21.714132: step 9946, loss 0.550038.
Train: 2018-08-02T13:20:21.886009: step 9947, loss 0.547177.
Train: 2018-08-02T13:20:22.042210: step 9948, loss 0.587091.
Train: 2018-08-02T13:20:22.198419: step 9949, loss 0.510954.
Train: 2018-08-02T13:20:22.339016: step 9950, loss 0.52726.
Test: 2018-08-02T13:20:22.807650: step 9950, loss 0.5594.
Train: 2018-08-02T13:20:22.963839: step 9951, loss 0.525363.
Train: 2018-08-02T13:20:23.120085: step 9952, loss 0.560294.
Train: 2018-08-02T13:20:23.276297: step 9953, loss 0.517087.
Train: 2018-08-02T13:20:23.432480: step 9954, loss 0.540201.
Train: 2018-08-02T13:20:23.588694: step 9955, loss 0.635867.
Train: 2018-08-02T13:20:23.760528: step 9956, loss 0.48956.
Train: 2018-08-02T13:20:23.916743: step 9957, loss 0.542359.
Train: 2018-08-02T13:20:24.072985: step 9958, loss 0.609931.
Train: 2018-08-02T13:20:24.229167: step 9959, loss 0.515008.
Train: 2018-08-02T13:20:24.385381: step 9960, loss 0.525142.
Test: 2018-08-02T13:20:24.854022: step 9960, loss 0.552179.
Train: 2018-08-02T13:20:25.010265: step 9961, loss 0.563011.
Train: 2018-08-02T13:20:25.150857: step 9962, loss 0.495522.
Train: 2018-08-02T13:20:25.307041: step 9963, loss 0.464029.
Train: 2018-08-02T13:20:25.478908: step 9964, loss 0.528014.
Train: 2018-08-02T13:20:25.635113: step 9965, loss 0.469449.
Train: 2018-08-02T13:20:25.744443: step 9966, loss 0.506113.
Train: 2018-08-02T13:20:25.900677: step 9967, loss 0.534959.
Train: 2018-08-02T13:20:26.072512: step 9968, loss 0.67053.
Train: 2018-08-02T13:20:26.228725: step 9969, loss 0.496398.
Train: 2018-08-02T13:20:26.384944: step 9970, loss 0.540207.
Test: 2018-08-02T13:20:26.853553: step 9970, loss 0.549157.
Train: 2018-08-02T13:20:27.009798: step 9971, loss 0.55086.
Train: 2018-08-02T13:20:27.166012: step 9972, loss 0.543141.
Train: 2018-08-02T13:20:27.337842: step 9973, loss 0.489889.
Train: 2018-08-02T13:20:27.494059: step 9974, loss 0.508193.
Train: 2018-08-02T13:20:27.650274: step 9975, loss 0.583011.
Train: 2018-08-02T13:20:27.806485: step 9976, loss 0.507722.
Train: 2018-08-02T13:20:27.962701: step 9977, loss 0.448124.
Train: 2018-08-02T13:20:28.118881: step 9978, loss 0.483453.
Train: 2018-08-02T13:20:28.275095: step 9979, loss 0.45326.
Train: 2018-08-02T13:20:28.431339: step 9980, loss 0.459973.
Test: 2018-08-02T13:20:28.899949: step 9980, loss 0.553318.
Train: 2018-08-02T13:20:29.056163: step 9981, loss 0.46154.
Train: 2018-08-02T13:20:29.196786: step 9982, loss 0.51423.
Train: 2018-08-02T13:20:29.368622: step 9983, loss 0.482985.
Train: 2018-08-02T13:20:29.524834: step 9984, loss 0.540281.
Train: 2018-08-02T13:20:29.681016: step 9985, loss 0.530672.
Train: 2018-08-02T13:20:29.821639: step 9986, loss 0.5236.
Train: 2018-08-02T13:20:29.977853: step 9987, loss 0.512975.
Train: 2018-08-02T13:20:30.149656: step 9988, loss 0.578824.
Train: 2018-08-02T13:20:30.305869: step 9989, loss 0.55826.
Train: 2018-08-02T13:20:30.462112: step 9990, loss 0.561258.
Test: 2018-08-02T13:20:30.930723: step 9990, loss 0.552424.
Train: 2018-08-02T13:20:31.086968: step 9991, loss 0.519978.
Train: 2018-08-02T13:20:31.243169: step 9992, loss 0.55882.
Train: 2018-08-02T13:20:31.399365: step 9993, loss 0.505178.
Train: 2018-08-02T13:20:31.555602: step 9994, loss 0.534328.
Train: 2018-08-02T13:20:31.711824: step 9995, loss 0.522405.
Train: 2018-08-02T13:20:31.883627: step 9996, loss 0.512865.
Train: 2018-08-02T13:20:32.039867: step 9997, loss 0.616542.
Train: 2018-08-02T13:20:32.196054: step 9998, loss 0.565936.
Train: 2018-08-02T13:20:32.352296: step 9999, loss 0.536082.
Train: 2018-08-02T13:20:32.508508: step 10000, loss 0.477563.
Test: 2018-08-02T13:20:32.961515: step 10000, loss 0.56038.
Train: 2018-08-02T13:20:33.648866: step 10001, loss 0.517101.
Train: 2018-08-02T13:20:33.820701: step 10002, loss 0.578869.
Train: 2018-08-02T13:20:33.976915: step 10003, loss 0.524228.
Train: 2018-08-02T13:20:34.133129: step 10004, loss 0.560077.
Train: 2018-08-02T13:20:34.273691: step 10005, loss 0.549418.
Train: 2018-08-02T13:20:34.429934: step 10006, loss 0.507711.
Train: 2018-08-02T13:20:34.586147: step 10007, loss 0.487065.
Train: 2018-08-02T13:20:34.757974: step 10008, loss 0.487448.
Train: 2018-08-02T13:20:34.914201: step 10009, loss 0.462706.
Train: 2018-08-02T13:20:35.070410: step 10010, loss 0.505731.
Test: 2018-08-02T13:20:35.523398: step 10010, loss 0.549291.
Train: 2018-08-02T13:20:35.679612: step 10011, loss 0.509928.
Train: 2018-08-02T13:20:35.835855: step 10012, loss 0.51377.
Train: 2018-08-02T13:20:35.992038: step 10013, loss 0.534298.
Train: 2018-08-02T13:20:36.148283: step 10014, loss 0.49922.
Train: 2018-08-02T13:20:36.320093: step 10015, loss 0.461226.
Train: 2018-08-02T13:20:36.460679: step 10016, loss 0.534608.
Train: 2018-08-02T13:20:36.616922: step 10017, loss 0.545479.
Train: 2018-08-02T13:20:36.773135: step 10018, loss 0.549315.
Train: 2018-08-02T13:20:36.929319: step 10019, loss 0.545166.
Train: 2018-08-02T13:20:37.085532: step 10020, loss 0.484305.
Test: 2018-08-02T13:20:37.554173: step 10020, loss 0.549914.
Train: 2018-08-02T13:20:37.710416: step 10021, loss 0.468251.
Train: 2018-08-02T13:20:37.851008: step 10022, loss 0.501209.
Train: 2018-08-02T13:20:38.007221: step 10023, loss 0.557068.
Train: 2018-08-02T13:20:38.163438: step 10024, loss 0.56021.
Train: 2018-08-02T13:20:38.319649: step 10025, loss 0.572042.
Train: 2018-08-02T13:20:38.491484: step 10026, loss 0.501151.
Train: 2018-08-02T13:20:38.647692: step 10027, loss 0.526808.
Train: 2018-08-02T13:20:38.803881: step 10028, loss 0.456909.
Train: 2018-08-02T13:20:38.960094: step 10029, loss 0.585851.
Train: 2018-08-02T13:20:39.116308: step 10030, loss 0.533511.
Test: 2018-08-02T13:20:39.584973: step 10030, loss 0.550643.
Train: 2018-08-02T13:20:39.741161: step 10031, loss 0.516552.
Train: 2018-08-02T13:20:39.897404: step 10032, loss 0.495692.
Train: 2018-08-02T13:20:40.053612: step 10033, loss 0.528111.
Train: 2018-08-02T13:20:40.209834: step 10034, loss 0.427061.
Train: 2018-08-02T13:20:40.366045: step 10035, loss 0.578326.
Train: 2018-08-02T13:20:40.522258: step 10036, loss 0.557825.
Train: 2018-08-02T13:20:40.678470: step 10037, loss 0.4578.
Train: 2018-08-02T13:20:40.834684: step 10038, loss 0.519082.
Train: 2018-08-02T13:20:40.990869: step 10039, loss 0.564109.
Train: 2018-08-02T13:20:41.147111: step 10040, loss 0.456147.
Test: 2018-08-02T13:20:41.615746: step 10040, loss 0.554366.
Train: 2018-08-02T13:20:41.767885: step 10041, loss 0.472236.
Train: 2018-08-02T13:20:41.924100: step 10042, loss 0.494635.
Train: 2018-08-02T13:20:42.080312: step 10043, loss 0.586935.
Train: 2018-08-02T13:20:42.236525: step 10044, loss 0.517994.
Train: 2018-08-02T13:20:42.392733: step 10045, loss 0.475921.
Train: 2018-08-02T13:20:42.548952: step 10046, loss 0.516142.
Train: 2018-08-02T13:20:42.705166: step 10047, loss 0.483268.
Train: 2018-08-02T13:20:42.861379: step 10048, loss 0.578254.
Train: 2018-08-02T13:20:43.017564: step 10049, loss 0.576388.
Train: 2018-08-02T13:20:43.173803: step 10050, loss 0.585588.
Test: 2018-08-02T13:20:43.626795: step 10050, loss 0.555758.
Train: 2018-08-02T13:20:43.783008: step 10051, loss 0.616509.
Train: 2018-08-02T13:20:43.954861: step 10052, loss 0.511168.
Train: 2018-08-02T13:20:44.111090: step 10053, loss 0.536059.
Train: 2018-08-02T13:20:44.267301: step 10054, loss 0.516454.
Train: 2018-08-02T13:20:44.423514: step 10055, loss 0.528999.
Train: 2018-08-02T13:20:44.595351: step 10056, loss 0.557544.
Train: 2018-08-02T13:20:44.751561: step 10057, loss 0.490621.
Train: 2018-08-02T13:20:44.892124: step 10058, loss 0.583769.
Train: 2018-08-02T13:20:45.048367: step 10059, loss 0.504329.
Train: 2018-08-02T13:20:45.204581: step 10060, loss 0.607169.
Test: 2018-08-02T13:20:45.673190: step 10060, loss 0.549029.
Train: 2018-08-02T13:20:45.907542: step 10061, loss 0.5591.
Train: 2018-08-02T13:20:46.079345: step 10062, loss 0.525994.
Train: 2018-08-02T13:20:46.235589: step 10063, loss 0.536811.
Train: 2018-08-02T13:20:46.391803: step 10064, loss 0.540957.
Train: 2018-08-02T13:20:46.548011: step 10065, loss 0.544683.
Train: 2018-08-02T13:20:46.704230: step 10066, loss 0.457957.
Train: 2018-08-02T13:20:46.876034: step 10067, loss 0.474607.
Train: 2018-08-02T13:20:47.032249: step 10068, loss 0.530544.
Train: 2018-08-02T13:20:47.188490: step 10069, loss 0.470784.
Train: 2018-08-02T13:20:47.344675: step 10070, loss 0.543153.
Test: 2018-08-02T13:20:47.797718: step 10070, loss 0.548139.
Train: 2018-08-02T13:20:47.953936: step 10071, loss 0.597833.
Train: 2018-08-02T13:20:48.110150: step 10072, loss 0.550135.
Train: 2018-08-02T13:20:48.266363: step 10073, loss 0.499143.
Train: 2018-08-02T13:20:48.422577: step 10074, loss 0.519357.
Train: 2018-08-02T13:20:48.578791: step 10075, loss 0.512408.
Train: 2018-08-02T13:20:48.734975: step 10076, loss 0.567335.
Train: 2018-08-02T13:20:48.891187: step 10077, loss 0.512854.
Train: 2018-08-02T13:20:49.047431: step 10078, loss 0.515739.
Train: 2018-08-02T13:20:49.203645: step 10079, loss 0.586975.
Train: 2018-08-02T13:20:49.359857: step 10080, loss 0.542976.
Test: 2018-08-02T13:20:49.828469: step 10080, loss 0.555445.
Train: 2018-08-02T13:20:49.984693: step 10081, loss 0.517593.
Train: 2018-08-02T13:20:50.140896: step 10082, loss 0.53981.
Train: 2018-08-02T13:20:50.297139: step 10083, loss 0.455406.
Train: 2018-08-02T13:20:50.453323: step 10084, loss 0.515581.
Train: 2018-08-02T13:20:50.609542: step 10085, loss 0.532278.
Train: 2018-08-02T13:20:50.765780: step 10086, loss 0.63014.
Train: 2018-08-02T13:20:50.937613: step 10087, loss 0.546625.
Train: 2018-08-02T13:20:51.093827: step 10088, loss 0.612804.
Train: 2018-08-02T13:20:51.250045: step 10089, loss 0.548519.
Train: 2018-08-02T13:20:51.406248: step 10090, loss 0.561677.
Test: 2018-08-02T13:20:51.859288: step 10090, loss 0.57002.
Train: 2018-08-02T13:20:52.031107: step 10091, loss 0.519911.
Train: 2018-08-02T13:20:52.171699: step 10092, loss 0.514562.
Train: 2018-08-02T13:20:52.343505: step 10093, loss 0.554138.
Train: 2018-08-02T13:20:52.499735: step 10094, loss 0.508729.
Train: 2018-08-02T13:20:52.655961: step 10095, loss 0.546687.
Train: 2018-08-02T13:20:52.827791: step 10096, loss 0.516468.
Train: 2018-08-02T13:20:52.983980: step 10097, loss 0.45519.
Train: 2018-08-02T13:20:53.140193: step 10098, loss 0.584962.
Train: 2018-08-02T13:20:53.296436: step 10099, loss 0.468475.
Train: 2018-08-02T13:20:53.452650: step 10100, loss 0.513585.
Test: 2018-08-02T13:20:53.921259: step 10100, loss 0.550546.
Train: 2018-08-02T13:20:54.561760: step 10101, loss 0.520819.
Train: 2018-08-02T13:20:54.717979: step 10102, loss 0.473442.
Train: 2018-08-02T13:20:54.874190: step 10103, loss 0.497538.
Train: 2018-08-02T13:20:55.030405: step 10104, loss 0.455252.
Train: 2018-08-02T13:20:55.186588: step 10105, loss 0.49803.
Train: 2018-08-02T13:20:55.342834: step 10106, loss 0.574708.
Train: 2018-08-02T13:20:55.499049: step 10107, loss 0.612357.
Train: 2018-08-02T13:20:55.655253: step 10108, loss 0.465817.
Train: 2018-08-02T13:20:55.811441: step 10109, loss 0.513325.
Train: 2018-08-02T13:20:55.967687: step 10110, loss 0.547928.
Test: 2018-08-02T13:20:56.436327: step 10110, loss 0.552357.
Train: 2018-08-02T13:20:56.592539: step 10111, loss 0.476527.
Train: 2018-08-02T13:20:56.748748: step 10112, loss 0.5073.
Train: 2018-08-02T13:20:56.904966: step 10113, loss 0.492811.
Train: 2018-08-02T13:20:57.061175: step 10114, loss 0.517121.
Train: 2018-08-02T13:20:57.217393: step 10115, loss 0.455563.
Train: 2018-08-02T13:20:57.373575: step 10116, loss 0.591308.
Train: 2018-08-02T13:20:57.498548: step 10117, loss 0.484074.
Train: 2018-08-02T13:20:57.654792: step 10118, loss 0.471704.
Train: 2018-08-02T13:20:57.810974: step 10119, loss 0.615997.
Train: 2018-08-02T13:20:57.967211: step 10120, loss 0.548544.
Test: 2018-08-02T13:20:58.435851: step 10120, loss 0.554086.
Train: 2018-08-02T13:20:58.592066: step 10121, loss 0.463264.
Train: 2018-08-02T13:20:58.748284: step 10122, loss 0.539383.
Train: 2018-08-02T13:20:58.904468: step 10123, loss 0.542379.
Train: 2018-08-02T13:20:59.076333: step 10124, loss 0.535589.
Train: 2018-08-02T13:20:59.232517: step 10125, loss 0.52579.
Train: 2018-08-02T13:20:59.388760: step 10126, loss 0.511773.
Train: 2018-08-02T13:20:59.544943: step 10127, loss 0.49053.
Train: 2018-08-02T13:20:59.701181: step 10128, loss 0.475271.
Train: 2018-08-02T13:20:59.857370: step 10129, loss 0.473788.
Train: 2018-08-02T13:21:00.013616: step 10130, loss 0.566272.
Test: 2018-08-02T13:21:00.466626: step 10130, loss 0.54901.
Train: 2018-08-02T13:21:00.622815: step 10131, loss 0.484492.
Train: 2018-08-02T13:21:00.779059: step 10132, loss 0.445418.
Train: 2018-08-02T13:21:00.935271: step 10133, loss 0.517817.
Train: 2018-08-02T13:21:01.091458: step 10134, loss 0.489455.
Train: 2018-08-02T13:21:01.247698: step 10135, loss 0.597502.
Train: 2018-08-02T13:21:01.403912: step 10136, loss 0.500586.
Train: 2018-08-02T13:21:01.560106: step 10137, loss 0.548884.
Train: 2018-08-02T13:21:01.716309: step 10138, loss 0.57618.
Train: 2018-08-02T13:21:01.872557: step 10139, loss 0.506795.
Train: 2018-08-02T13:21:02.028758: step 10140, loss 0.505207.
Test: 2018-08-02T13:21:02.497401: step 10140, loss 0.55124.
Train: 2018-08-02T13:21:02.653614: step 10141, loss 0.499457.
Train: 2018-08-02T13:21:02.825450: step 10142, loss 0.496001.
Train: 2018-08-02T13:21:02.981672: step 10143, loss 0.503501.
Train: 2018-08-02T13:21:03.137876: step 10144, loss 0.512799.
Train: 2018-08-02T13:21:03.294095: step 10145, loss 0.59145.
Train: 2018-08-02T13:21:03.450308: step 10146, loss 0.557345.
Train: 2018-08-02T13:21:03.606520: step 10147, loss 0.479007.
Train: 2018-08-02T13:21:03.778356: step 10148, loss 0.463027.
Train: 2018-08-02T13:21:03.934570: step 10149, loss 0.586924.
Train: 2018-08-02T13:21:04.090784: step 10150, loss 0.648088.
Test: 2018-08-02T13:21:04.543772: step 10150, loss 0.551876.
Train: 2018-08-02T13:21:04.699986: step 10151, loss 0.589404.
Train: 2018-08-02T13:21:04.856199: step 10152, loss 0.541047.
Train: 2018-08-02T13:21:05.028035: step 10153, loss 0.494773.
Train: 2018-08-02T13:21:05.184277: step 10154, loss 0.497065.
Train: 2018-08-02T13:21:05.340474: step 10155, loss 0.518359.
Train: 2018-08-02T13:21:05.496673: step 10156, loss 0.546513.
Train: 2018-08-02T13:21:05.652918: step 10157, loss 0.532148.
Train: 2018-08-02T13:21:05.809131: step 10158, loss 0.542878.
Train: 2018-08-02T13:21:05.980959: step 10159, loss 0.470485.
Train: 2018-08-02T13:21:06.121557: step 10160, loss 0.508355.
Test: 2018-08-02T13:21:06.590169: step 10160, loss 0.547169.
Train: 2018-08-02T13:21:06.746380: step 10161, loss 0.4657.
Train: 2018-08-02T13:21:06.902626: step 10162, loss 0.490621.
Train: 2018-08-02T13:21:07.058838: step 10163, loss 0.485143.
Train: 2018-08-02T13:21:07.215052: step 10164, loss 0.531073.
Train: 2018-08-02T13:21:07.371264: step 10165, loss 0.494787.
Train: 2018-08-02T13:21:07.527478: step 10166, loss 0.503542.
Train: 2018-08-02T13:21:07.683661: step 10167, loss 0.50739.
Train: 2018-08-02T13:21:07.839876: step 10168, loss 0.480291.
Train: 2018-08-02T13:21:07.996121: step 10169, loss 0.627308.
Train: 2018-08-02T13:21:08.152334: step 10170, loss 0.505985.
Test: 2018-08-02T13:21:08.620967: step 10170, loss 0.551199.
Train: 2018-08-02T13:21:08.777157: step 10171, loss 0.474039.
Train: 2018-08-02T13:21:08.933368: step 10172, loss 0.585819.
Train: 2018-08-02T13:21:09.105203: step 10173, loss 0.484745.
Train: 2018-08-02T13:21:09.261418: step 10174, loss 0.517581.
Train: 2018-08-02T13:21:09.417660: step 10175, loss 0.553551.
Train: 2018-08-02T13:21:09.573874: step 10176, loss 0.556739.
Train: 2018-08-02T13:21:09.730088: step 10177, loss 0.542929.
Train: 2018-08-02T13:21:09.886302: step 10178, loss 0.546434.
Train: 2018-08-02T13:21:10.042513: step 10179, loss 0.474679.
Train: 2018-08-02T13:21:10.198728: step 10180, loss 0.492294.
Test: 2018-08-02T13:21:10.667368: step 10180, loss 0.546489.
Train: 2018-08-02T13:21:10.823582: step 10181, loss 0.508771.
Train: 2018-08-02T13:21:10.979766: step 10182, loss 0.405417.
Train: 2018-08-02T13:21:11.136006: step 10183, loss 0.497766.
Train: 2018-08-02T13:21:11.292191: step 10184, loss 0.498283.
Train: 2018-08-02T13:21:11.448406: step 10185, loss 0.480356.
Train: 2018-08-02T13:21:11.604646: step 10186, loss 0.542396.
Train: 2018-08-02T13:21:11.760862: step 10187, loss 0.545591.
Train: 2018-08-02T13:21:11.917075: step 10188, loss 0.481606.
Train: 2018-08-02T13:21:12.073288: step 10189, loss 0.585592.
Train: 2018-08-02T13:21:12.229502: step 10190, loss 0.527587.
Test: 2018-08-02T13:21:12.682521: step 10190, loss 0.554133.
Train: 2018-08-02T13:21:12.838735: step 10191, loss 0.576756.
Train: 2018-08-02T13:21:12.994948: step 10192, loss 0.531674.
Train: 2018-08-02T13:21:13.151156: step 10193, loss 0.608823.
Train: 2018-08-02T13:21:13.322996: step 10194, loss 0.536166.
Train: 2018-08-02T13:21:13.479180: step 10195, loss 0.492447.
Train: 2018-08-02T13:21:13.635418: step 10196, loss 0.466498.
Train: 2018-08-02T13:21:13.791630: step 10197, loss 0.514844.
Train: 2018-08-02T13:21:13.947850: step 10198, loss 0.580137.
Train: 2018-08-02T13:21:14.104033: step 10199, loss 0.532975.
Train: 2018-08-02T13:21:14.260277: step 10200, loss 0.494231.
Test: 2018-08-02T13:21:14.713290: step 10200, loss 0.548207.
Train: 2018-08-02T13:21:15.353754: step 10201, loss 0.429723.
Train: 2018-08-02T13:21:15.509984: step 10202, loss 0.405089.
Train: 2018-08-02T13:21:15.681790: step 10203, loss 0.519384.
Train: 2018-08-02T13:21:15.838005: step 10204, loss 0.488731.
Train: 2018-08-02T13:21:15.994229: step 10205, loss 0.631884.
Train: 2018-08-02T13:21:16.150459: step 10206, loss 0.523874.
Train: 2018-08-02T13:21:16.306643: step 10207, loss 0.509557.
Train: 2018-08-02T13:21:16.462857: step 10208, loss 0.574504.
Train: 2018-08-02T13:21:16.619098: step 10209, loss 0.468035.
Train: 2018-08-02T13:21:16.790903: step 10210, loss 0.506244.
Test: 2018-08-02T13:21:17.243952: step 10210, loss 0.554689.
Train: 2018-08-02T13:21:17.400166: step 10211, loss 0.49692.
Train: 2018-08-02T13:21:17.556381: step 10212, loss 0.500557.
Train: 2018-08-02T13:21:17.712593: step 10213, loss 0.556958.
Train: 2018-08-02T13:21:17.868806: step 10214, loss 0.514524.
Train: 2018-08-02T13:21:18.025020: step 10215, loss 0.532634.
Train: 2018-08-02T13:21:18.165581: step 10216, loss 0.486473.
Train: 2018-08-02T13:21:18.321825: step 10217, loss 0.463649.
Train: 2018-08-02T13:21:18.493631: step 10218, loss 0.516436.
Train: 2018-08-02T13:21:18.649869: step 10219, loss 0.47654.
Train: 2018-08-02T13:21:18.806064: step 10220, loss 0.537437.
Test: 2018-08-02T13:21:19.259076: step 10220, loss 0.549359.
Train: 2018-08-02T13:21:19.415313: step 10221, loss 0.50313.
Train: 2018-08-02T13:21:19.587126: step 10222, loss 0.62568.
Train: 2018-08-02T13:21:19.743372: step 10223, loss 0.475045.
Train: 2018-08-02T13:21:19.899576: step 10224, loss 0.551044.
Train: 2018-08-02T13:21:20.055763: step 10225, loss 0.570093.
Train: 2018-08-02T13:21:20.212010: step 10226, loss 0.501292.
Train: 2018-08-02T13:21:20.368226: step 10227, loss 0.513295.
Train: 2018-08-02T13:21:20.540029: step 10228, loss 0.501842.
Train: 2018-08-02T13:21:20.696240: step 10229, loss 0.47152.
Train: 2018-08-02T13:21:20.852487: step 10230, loss 0.520628.
Test: 2018-08-02T13:21:21.305497: step 10230, loss 0.548046.
Train: 2018-08-02T13:21:21.461714: step 10231, loss 0.532695.
Train: 2018-08-02T13:21:21.617899: step 10232, loss 0.542884.
Train: 2018-08-02T13:21:21.774111: step 10233, loss 0.520209.
Train: 2018-08-02T13:21:21.930326: step 10234, loss 0.529533.
Train: 2018-08-02T13:21:22.086568: step 10235, loss 0.539031.
Train: 2018-08-02T13:21:22.242751: step 10236, loss 0.57757.
Train: 2018-08-02T13:21:22.398966: step 10237, loss 0.545021.
Train: 2018-08-02T13:21:22.555208: step 10238, loss 0.499293.
Train: 2018-08-02T13:21:22.711423: step 10239, loss 0.532086.
Train: 2018-08-02T13:21:22.867640: step 10240, loss 0.576358.
Test: 2018-08-02T13:21:23.320649: step 10240, loss 0.55128.
Train: 2018-08-02T13:21:23.476868: step 10241, loss 0.465786.
Train: 2018-08-02T13:21:23.633082: step 10242, loss 0.483671.
Train: 2018-08-02T13:21:23.789265: step 10243, loss 0.55324.
Train: 2018-08-02T13:21:23.945504: step 10244, loss 0.573129.
Train: 2018-08-02T13:21:24.101722: step 10245, loss 0.5444.
Train: 2018-08-02T13:21:24.257906: step 10246, loss 0.523642.
Train: 2018-08-02T13:21:24.414149: step 10247, loss 0.520037.
Train: 2018-08-02T13:21:24.570362: step 10248, loss 0.491217.
Train: 2018-08-02T13:21:24.726576: step 10249, loss 0.496705.
Train: 2018-08-02T13:21:24.882790: step 10250, loss 0.474551.
Test: 2018-08-02T13:21:25.335777: step 10250, loss 0.54822.
Train: 2018-08-02T13:21:25.507644: step 10251, loss 0.4578.
Train: 2018-08-02T13:21:25.663852: step 10252, loss 0.513929.
Train: 2018-08-02T13:21:25.804442: step 10253, loss 0.528456.
Train: 2018-08-02T13:21:25.960653: step 10254, loss 0.498998.
Train: 2018-08-02T13:21:26.116869: step 10255, loss 0.576284.
Train: 2018-08-02T13:21:26.288679: step 10256, loss 0.453935.
Train: 2018-08-02T13:21:26.444924: step 10257, loss 0.489023.
Train: 2018-08-02T13:21:26.601106: step 10258, loss 0.469334.
Train: 2018-08-02T13:21:26.757350: step 10259, loss 0.531449.
Train: 2018-08-02T13:21:26.913559: step 10260, loss 0.618394.
Test: 2018-08-02T13:21:27.382173: step 10260, loss 0.548978.
Train: 2018-08-02T13:21:27.538413: step 10261, loss 0.494359.
Train: 2018-08-02T13:21:27.694600: step 10262, loss 0.526832.
Train: 2018-08-02T13:21:27.850842: step 10263, loss 0.527193.
Train: 2018-08-02T13:21:28.007027: step 10264, loss 0.507419.
Train: 2018-08-02T13:21:28.163241: step 10265, loss 0.565684.
Train: 2018-08-02T13:21:28.319484: step 10266, loss 0.464418.
Train: 2018-08-02T13:21:28.475668: step 10267, loss 0.544634.
Train: 2018-08-02T13:21:28.585047: step 10268, loss 0.471947.
Train: 2018-08-02T13:21:28.741260: step 10269, loss 0.447831.
Train: 2018-08-02T13:21:28.897475: step 10270, loss 0.559493.
Test: 2018-08-02T13:21:29.366084: step 10270, loss 0.550192.
Train: 2018-08-02T13:21:29.522298: step 10271, loss 0.496256.
Train: 2018-08-02T13:21:29.678536: step 10272, loss 0.526191.
Train: 2018-08-02T13:21:29.834724: step 10273, loss 0.500244.
Train: 2018-08-02T13:21:29.990972: step 10274, loss 0.508775.
Train: 2018-08-02T13:21:30.147187: step 10275, loss 0.52592.
Train: 2018-08-02T13:21:30.303395: step 10276, loss 0.526626.
Train: 2018-08-02T13:21:30.459602: step 10277, loss 0.473661.
Train: 2018-08-02T13:21:30.615818: step 10278, loss 0.418984.
Train: 2018-08-02T13:21:30.772034: step 10279, loss 0.556805.
Train: 2018-08-02T13:21:30.928251: step 10280, loss 0.640632.
Test: 2018-08-02T13:21:31.396859: step 10280, loss 0.54786.
Train: 2018-08-02T13:21:31.553072: step 10281, loss 0.558044.
Train: 2018-08-02T13:21:31.709315: step 10282, loss 0.498323.
Train: 2018-08-02T13:21:31.865539: step 10283, loss 0.538014.
Train: 2018-08-02T13:21:32.021742: step 10284, loss 0.566428.
Train: 2018-08-02T13:21:32.177956: step 10285, loss 0.455161.
Train: 2018-08-02T13:21:32.334169: step 10286, loss 0.603779.
Train: 2018-08-02T13:21:32.490386: step 10287, loss 0.549013.
Train: 2018-08-02T13:21:32.646595: step 10288, loss 0.538769.
Train: 2018-08-02T13:21:32.787157: step 10289, loss 0.474532.
Train: 2018-08-02T13:21:32.943370: step 10290, loss 0.52467.
Test: 2018-08-02T13:21:33.412037: step 10290, loss 0.549589.
Train: 2018-08-02T13:21:33.568225: step 10291, loss 0.433375.
Train: 2018-08-02T13:21:33.724466: step 10292, loss 0.541043.
Train: 2018-08-02T13:21:33.896273: step 10293, loss 0.590829.
Train: 2018-08-02T13:21:34.052487: step 10294, loss 0.489549.
Train: 2018-08-02T13:21:34.208701: step 10295, loss 0.484636.
Train: 2018-08-02T13:21:34.349322: step 10296, loss 0.489047.
Train: 2018-08-02T13:21:34.505537: step 10297, loss 0.463932.
Train: 2018-08-02T13:21:34.677370: step 10298, loss 0.449604.
Train: 2018-08-02T13:21:34.833591: step 10299, loss 0.567303.
Train: 2018-08-02T13:21:34.989769: step 10300, loss 0.524063.
Test: 2018-08-02T13:21:35.442786: step 10300, loss 0.547021.
Train: 2018-08-02T13:21:36.083293: step 10301, loss 0.511096.
Train: 2018-08-02T13:21:36.239500: step 10302, loss 0.549922.
Train: 2018-08-02T13:21:36.395724: step 10303, loss 0.562478.
Train: 2018-08-02T13:21:36.551931: step 10304, loss 0.474508.
Train: 2018-08-02T13:21:36.708145: step 10305, loss 0.594262.
Train: 2018-08-02T13:21:36.864352: step 10306, loss 0.529921.
Train: 2018-08-02T13:21:37.020542: step 10307, loss 0.493346.
Train: 2018-08-02T13:21:37.176754: step 10308, loss 0.523275.
Train: 2018-08-02T13:21:37.332998: step 10309, loss 0.518353.
Train: 2018-08-02T13:21:37.489207: step 10310, loss 0.517821.
Test: 2018-08-02T13:21:37.942203: step 10310, loss 0.551817.
Train: 2018-08-02T13:21:38.098441: step 10311, loss 0.444133.
Train: 2018-08-02T13:21:38.254658: step 10312, loss 0.541897.
Train: 2018-08-02T13:21:38.410842: step 10313, loss 0.498794.
Train: 2018-08-02T13:21:38.582703: step 10314, loss 0.548911.
Train: 2018-08-02T13:21:38.738923: step 10315, loss 0.514598.
Train: 2018-08-02T13:21:38.895132: step 10316, loss 0.557421.
Train: 2018-08-02T13:21:39.051346: step 10317, loss 0.458244.
Train: 2018-08-02T13:21:39.207529: step 10318, loss 0.550196.
Train: 2018-08-02T13:21:39.363777: step 10319, loss 0.48639.
Train: 2018-08-02T13:21:39.504365: step 10320, loss 0.553058.
Test: 2018-08-02T13:21:39.973001: step 10320, loss 0.550882.
Train: 2018-08-02T13:21:40.129213: step 10321, loss 0.533405.
Train: 2018-08-02T13:21:40.285428: step 10322, loss 0.525367.
Train: 2018-08-02T13:21:40.441616: step 10323, loss 0.535146.
Train: 2018-08-02T13:21:40.597860: step 10324, loss 0.484618.
Train: 2018-08-02T13:21:40.754073: step 10325, loss 0.466406.
Train: 2018-08-02T13:21:40.910282: step 10326, loss 0.436705.
Train: 2018-08-02T13:21:41.066499: step 10327, loss 0.570106.
Train: 2018-08-02T13:21:41.222715: step 10328, loss 0.495409.
Train: 2018-08-02T13:21:41.378927: step 10329, loss 0.517868.
Train: 2018-08-02T13:21:41.535144: step 10330, loss 0.528894.
Test: 2018-08-02T13:21:42.003783: step 10330, loss 0.548661.
Train: 2018-08-02T13:21:42.159962: step 10331, loss 0.490671.
Train: 2018-08-02T13:21:42.316201: step 10332, loss 0.515138.
Train: 2018-08-02T13:21:42.488025: step 10333, loss 0.599346.
Train: 2018-08-02T13:21:42.644255: step 10334, loss 0.515772.
Train: 2018-08-02T13:21:42.800469: step 10335, loss 0.621232.
Train: 2018-08-02T13:21:42.956653: step 10336, loss 0.558461.
Train: 2018-08-02T13:21:43.112867: step 10337, loss 0.502785.
Train: 2018-08-02T13:21:43.269109: step 10338, loss 0.572758.
Train: 2018-08-02T13:21:43.425322: step 10339, loss 0.50111.
Train: 2018-08-02T13:21:43.581507: step 10340, loss 0.467597.
Test: 2018-08-02T13:21:44.050146: step 10340, loss 0.548086.
Train: 2018-08-02T13:21:44.206389: step 10341, loss 0.55066.
Train: 2018-08-02T13:21:44.362574: step 10342, loss 0.491707.
Train: 2018-08-02T13:21:44.518816: step 10343, loss 0.518535.
Train: 2018-08-02T13:21:44.675023: step 10344, loss 0.475122.
Train: 2018-08-02T13:21:44.831242: step 10345, loss 0.575975.
Train: 2018-08-02T13:21:44.987456: step 10346, loss 0.522283.
Train: 2018-08-02T13:21:45.143640: step 10347, loss 0.461884.
Train: 2018-08-02T13:21:45.299853: step 10348, loss 0.465251.
Train: 2018-08-02T13:21:45.456097: step 10349, loss 0.481322.
Train: 2018-08-02T13:21:45.612305: step 10350, loss 0.539492.
Test: 2018-08-02T13:21:46.065299: step 10350, loss 0.547543.
Train: 2018-08-02T13:21:46.221537: step 10351, loss 0.556572.
Train: 2018-08-02T13:21:46.393373: step 10352, loss 0.48295.
Train: 2018-08-02T13:21:46.549566: step 10353, loss 0.5645.
Train: 2018-08-02T13:21:46.705780: step 10354, loss 0.564849.
Train: 2018-08-02T13:21:46.861987: step 10355, loss 0.540491.
Train: 2018-08-02T13:21:47.018201: step 10356, loss 0.595476.
Train: 2018-08-02T13:21:47.174422: step 10357, loss 0.505282.
Train: 2018-08-02T13:21:47.330628: step 10358, loss 0.53263.
Train: 2018-08-02T13:21:47.486840: step 10359, loss 0.506198.
Train: 2018-08-02T13:21:47.643084: step 10360, loss 0.572675.
Test: 2018-08-02T13:21:48.111724: step 10360, loss 0.549115.
Train: 2018-08-02T13:21:48.267907: step 10361, loss 0.520493.
Train: 2018-08-02T13:21:48.424151: step 10362, loss 0.517413.
Train: 2018-08-02T13:21:48.580365: step 10363, loss 0.474704.
Train: 2018-08-02T13:21:48.736547: step 10364, loss 0.556393.
Train: 2018-08-02T13:21:48.877140: step 10365, loss 0.476279.
Train: 2018-08-02T13:21:49.033388: step 10366, loss 0.4892.
Train: 2018-08-02T13:21:49.189597: step 10367, loss 0.425285.
Train: 2018-08-02T13:21:49.345812: step 10368, loss 0.53168.
Train: 2018-08-02T13:21:49.502019: step 10369, loss 0.543847.
Train: 2018-08-02T13:21:49.658212: step 10370, loss 0.547494.
Test: 2018-08-02T13:21:50.126872: step 10370, loss 0.547883.
Train: 2018-08-02T13:21:50.283067: step 10371, loss 0.561748.
Train: 2018-08-02T13:21:50.439305: step 10372, loss 0.533359.
Train: 2018-08-02T13:21:50.595521: step 10373, loss 0.524449.
Train: 2018-08-02T13:21:50.751732: step 10374, loss 0.61206.
Train: 2018-08-02T13:21:50.923561: step 10375, loss 0.492754.
Train: 2018-08-02T13:21:51.079780: step 10376, loss 0.509154.
Train: 2018-08-02T13:21:51.235992: step 10377, loss 0.536035.
Train: 2018-08-02T13:21:51.392175: step 10378, loss 0.540763.
Train: 2018-08-02T13:21:51.548390: step 10379, loss 0.487933.
Train: 2018-08-02T13:21:51.704602: step 10380, loss 0.47429.
Test: 2018-08-02T13:21:52.157622: step 10380, loss 0.548347.
Train: 2018-08-02T13:21:52.313866: step 10381, loss 0.440793.
Train: 2018-08-02T13:21:52.470092: step 10382, loss 0.472775.
Train: 2018-08-02T13:21:52.641914: step 10383, loss 0.658685.
Train: 2018-08-02T13:21:52.798097: step 10384, loss 0.483047.
Train: 2018-08-02T13:21:52.954344: step 10385, loss 0.533482.
Train: 2018-08-02T13:21:53.110525: step 10386, loss 0.547554.
Train: 2018-08-02T13:21:53.266738: step 10387, loss 0.550454.
Train: 2018-08-02T13:21:53.422950: step 10388, loss 0.502972.
Train: 2018-08-02T13:21:53.579196: step 10389, loss 0.496505.
Train: 2018-08-02T13:21:53.735411: step 10390, loss 0.519393.
Test: 2018-08-02T13:21:54.188421: step 10390, loss 0.554139.
Train: 2018-08-02T13:21:54.344640: step 10391, loss 0.509666.
Train: 2018-08-02T13:21:54.500822: step 10392, loss 0.50249.
Train: 2018-08-02T13:21:54.657094: step 10393, loss 0.474615.
Train: 2018-08-02T13:21:54.813282: step 10394, loss 0.507078.
Train: 2018-08-02T13:21:54.969502: step 10395, loss 0.436485.
Train: 2018-08-02T13:21:55.125701: step 10396, loss 0.548974.
Train: 2018-08-02T13:21:55.297511: step 10397, loss 0.531749.
Train: 2018-08-02T13:21:55.438134: step 10398, loss 0.553209.
Train: 2018-08-02T13:21:55.594345: step 10399, loss 0.543808.
Train: 2018-08-02T13:21:55.750556: step 10400, loss 0.506173.
Test: 2018-08-02T13:21:56.219196: step 10400, loss 0.548715.
Train: 2018-08-02T13:21:56.875291: step 10401, loss 0.453964.
Train: 2018-08-02T13:21:57.031482: step 10402, loss 0.509805.
Train: 2018-08-02T13:21:57.187724: step 10403, loss 0.562113.
Train: 2018-08-02T13:21:57.343908: step 10404, loss 0.48083.
Train: 2018-08-02T13:21:57.500122: step 10405, loss 0.549464.
Train: 2018-08-02T13:21:57.656345: step 10406, loss 0.502476.
Train: 2018-08-02T13:21:57.812548: step 10407, loss 0.500175.
Train: 2018-08-02T13:21:57.968760: step 10408, loss 0.523564.
Train: 2018-08-02T13:21:58.125004: step 10409, loss 0.526772.
Train: 2018-08-02T13:21:58.281233: step 10410, loss 0.458826.
Test: 2018-08-02T13:21:58.749829: step 10410, loss 0.55058.
Train: 2018-08-02T13:21:58.906042: step 10411, loss 0.5284.
Train: 2018-08-02T13:21:59.062279: step 10412, loss 0.570252.
Train: 2018-08-02T13:21:59.218498: step 10413, loss 0.501894.
Train: 2018-08-02T13:21:59.374682: step 10414, loss 0.476239.
Train: 2018-08-02T13:21:59.530894: step 10415, loss 0.543425.
Train: 2018-08-02T13:21:59.687108: step 10416, loss 0.564305.
Train: 2018-08-02T13:21:59.843352: step 10417, loss 0.548797.
Train: 2018-08-02T13:21:59.999559: step 10418, loss 0.501908.
Train: 2018-08-02T13:22:00.108919: step 10419, loss 0.437955.
Train: 2018-08-02T13:22:00.265100: step 10420, loss 0.448501.
Test: 2018-08-02T13:22:00.718118: step 10420, loss 0.54675.
Train: 2018-08-02T13:22:00.874355: step 10421, loss 0.551669.
Train: 2018-08-02T13:22:01.030545: step 10422, loss 0.508518.
Train: 2018-08-02T13:22:01.186757: step 10423, loss 0.593714.
Train: 2018-08-02T13:22:01.343005: step 10424, loss 0.394049.
Train: 2018-08-02T13:22:01.499214: step 10425, loss 0.449296.
Train: 2018-08-02T13:22:01.655435: step 10426, loss 0.493484.
Train: 2018-08-02T13:22:01.811653: step 10427, loss 0.586571.
Train: 2018-08-02T13:22:01.967858: step 10428, loss 0.490097.
Train: 2018-08-02T13:22:02.139659: step 10429, loss 0.509722.
Train: 2018-08-02T13:22:02.295902: step 10430, loss 0.53996.
Test: 2018-08-02T13:22:02.748892: step 10430, loss 0.550044.
Train: 2018-08-02T13:22:02.905136: step 10431, loss 0.535159.
Train: 2018-08-02T13:22:03.061319: step 10432, loss 0.531054.
Train: 2018-08-02T13:22:03.217568: step 10433, loss 0.473316.
Train: 2018-08-02T13:22:03.373747: step 10434, loss 0.471997.
Train: 2018-08-02T13:22:03.529989: step 10435, loss 0.478518.
Train: 2018-08-02T13:22:03.670551: step 10436, loss 0.482493.
Train: 2018-08-02T13:22:03.826794: step 10437, loss 0.392495.
Train: 2018-08-02T13:22:03.983011: step 10438, loss 0.526315.
Train: 2018-08-02T13:22:04.139221: step 10439, loss 0.541808.
Train: 2018-08-02T13:22:04.295434: step 10440, loss 0.426613.
Test: 2018-08-02T13:22:04.748424: step 10440, loss 0.546924.
Train: 2018-08-02T13:22:04.904641: step 10441, loss 0.515554.
Train: 2018-08-02T13:22:05.060849: step 10442, loss 0.546746.
Train: 2018-08-02T13:22:05.217064: step 10443, loss 0.49586.
Train: 2018-08-02T13:22:05.373307: step 10444, loss 0.507645.
Train: 2018-08-02T13:22:05.529490: step 10445, loss 0.484074.
Train: 2018-08-02T13:22:05.685730: step 10446, loss 0.4699.
Train: 2018-08-02T13:22:05.841948: step 10447, loss 0.508473.
Train: 2018-08-02T13:22:05.998160: step 10448, loss 0.531033.
Train: 2018-08-02T13:22:06.154374: step 10449, loss 0.539416.
Train: 2018-08-02T13:22:06.294965: step 10450, loss 0.517867.
Test: 2018-08-02T13:22:06.763578: step 10450, loss 0.554348.
Train: 2018-08-02T13:22:06.919815: step 10451, loss 0.487548.
Train: 2018-08-02T13:22:07.076004: step 10452, loss 0.55264.
Train: 2018-08-02T13:22:07.232252: step 10453, loss 0.505965.
Train: 2018-08-02T13:22:07.388432: step 10454, loss 0.504843.
Train: 2018-08-02T13:22:07.544644: step 10455, loss 0.537664.
Train: 2018-08-02T13:22:07.700889: step 10456, loss 0.463069.
Train: 2018-08-02T13:22:07.857099: step 10457, loss 0.550975.
Train: 2018-08-02T13:22:08.013285: step 10458, loss 0.542081.
Train: 2018-08-02T13:22:08.169527: step 10459, loss 0.467953.
Train: 2018-08-02T13:22:08.325740: step 10460, loss 0.498176.
Test: 2018-08-02T13:22:08.778730: step 10460, loss 0.547862.
Train: 2018-08-02T13:22:08.934943: step 10461, loss 0.525306.
Train: 2018-08-02T13:22:09.091158: step 10462, loss 0.465866.
Train: 2018-08-02T13:22:09.247400: step 10463, loss 0.489414.
Train: 2018-08-02T13:22:09.403614: step 10464, loss 0.583536.
Train: 2018-08-02T13:22:09.559822: step 10465, loss 0.423015.
Train: 2018-08-02T13:22:09.716035: step 10466, loss 0.572122.
Train: 2018-08-02T13:22:09.872249: step 10467, loss 0.473813.
Train: 2018-08-02T13:22:10.028438: step 10468, loss 0.466768.
Train: 2018-08-02T13:22:10.184681: step 10469, loss 0.444698.
Train: 2018-08-02T13:22:10.340865: step 10470, loss 0.574444.
Test: 2018-08-02T13:22:10.793883: step 10470, loss 0.547394.
Train: 2018-08-02T13:22:10.950097: step 10471, loss 0.517434.
Train: 2018-08-02T13:22:11.121932: step 10472, loss 0.525703.
Train: 2018-08-02T13:22:11.278146: step 10473, loss 0.507253.
Train: 2018-08-02T13:22:11.434383: step 10474, loss 0.499158.
Train: 2018-08-02T13:22:11.590612: step 10475, loss 0.575191.
Train: 2018-08-02T13:22:11.746784: step 10476, loss 0.509352.
Train: 2018-08-02T13:22:11.903029: step 10477, loss 0.523204.
Train: 2018-08-02T13:22:12.059250: step 10478, loss 0.494836.
Train: 2018-08-02T13:22:12.215426: step 10479, loss 0.475089.
Train: 2018-08-02T13:22:12.371638: step 10480, loss 0.592021.
Test: 2018-08-02T13:22:12.824664: step 10480, loss 0.546816.
Train: 2018-08-02T13:22:12.980895: step 10481, loss 0.518007.
Train: 2018-08-02T13:22:13.137114: step 10482, loss 0.456017.
Train: 2018-08-02T13:22:13.293323: step 10483, loss 0.598719.
Train: 2018-08-02T13:22:13.449514: step 10484, loss 0.521184.
Train: 2018-08-02T13:22:13.605754: step 10485, loss 0.530963.
Train: 2018-08-02T13:22:13.761940: step 10486, loss 0.626051.
Train: 2018-08-02T13:22:13.933773: step 10487, loss 0.551868.
Train: 2018-08-02T13:22:14.089987: step 10488, loss 0.523398.
Train: 2018-08-02T13:22:14.246232: step 10489, loss 0.532814.
Train: 2018-08-02T13:22:14.386817: step 10490, loss 0.519416.
Test: 2018-08-02T13:22:14.855466: step 10490, loss 0.560857.
Train: 2018-08-02T13:22:15.011675: step 10491, loss 0.547515.
Train: 2018-08-02T13:22:15.167859: step 10492, loss 0.578971.
Train: 2018-08-02T13:22:15.324102: step 10493, loss 0.526924.
Train: 2018-08-02T13:22:15.480285: step 10494, loss 0.572527.
Train: 2018-08-02T13:22:15.636499: step 10495, loss 0.466338.
Train: 2018-08-02T13:22:15.792712: step 10496, loss 0.545688.
Train: 2018-08-02T13:22:15.948926: step 10497, loss 0.509678.
Train: 2018-08-02T13:22:16.105140: step 10498, loss 0.469798.
Train: 2018-08-02T13:22:16.261383: step 10499, loss 0.517475.
Train: 2018-08-02T13:22:16.417566: step 10500, loss 0.443653.
Test: 2018-08-02T13:22:16.870616: step 10500, loss 0.547272.
Train: 2018-08-02T13:22:17.620410: step 10501, loss 0.514998.
Train: 2018-08-02T13:22:17.776647: step 10502, loss 0.541731.
Train: 2018-08-02T13:22:17.932837: step 10503, loss 0.583202.
Train: 2018-08-02T13:22:18.089075: step 10504, loss 0.591887.
Train: 2018-08-02T13:22:18.245262: step 10505, loss 0.519464.
Train: 2018-08-02T13:22:18.401502: step 10506, loss 0.541316.
Train: 2018-08-02T13:22:18.557720: step 10507, loss 0.512662.
Train: 2018-08-02T13:22:18.713927: step 10508, loss 0.599146.
Train: 2018-08-02T13:22:18.870117: step 10509, loss 0.59592.
Train: 2018-08-02T13:22:19.026360: step 10510, loss 0.54023.
Test: 2018-08-02T13:22:19.494993: step 10510, loss 0.558215.
Train: 2018-08-02T13:22:19.651215: step 10511, loss 0.495984.
Train: 2018-08-02T13:22:19.807427: step 10512, loss 0.541218.
Train: 2018-08-02T13:22:19.963646: step 10513, loss 0.469488.
Train: 2018-08-02T13:22:20.119848: step 10514, loss 0.574439.
Train: 2018-08-02T13:22:20.276038: step 10515, loss 0.566378.
Train: 2018-08-02T13:22:20.432281: step 10516, loss 0.529374.
Train: 2018-08-02T13:22:20.588494: step 10517, loss 0.542261.
Train: 2018-08-02T13:22:20.744707: step 10518, loss 0.582123.
Train: 2018-08-02T13:22:20.900891: step 10519, loss 0.546617.
Train: 2018-08-02T13:22:21.057131: step 10520, loss 0.503585.
Test: 2018-08-02T13:22:21.525748: step 10520, loss 0.551324.
Train: 2018-08-02T13:22:21.681988: step 10521, loss 0.553389.
Train: 2018-08-02T13:22:21.838196: step 10522, loss 0.586682.
Train: 2018-08-02T13:22:21.994410: step 10523, loss 0.507198.
Train: 2018-08-02T13:22:22.166251: step 10524, loss 0.487638.
Train: 2018-08-02T13:22:22.322464: step 10525, loss 0.496371.
Train: 2018-08-02T13:22:22.478673: step 10526, loss 0.54515.
Train: 2018-08-02T13:22:22.634864: step 10527, loss 0.494501.
Train: 2018-08-02T13:22:22.791103: step 10528, loss 0.549961.
Train: 2018-08-02T13:22:22.947317: step 10529, loss 0.576343.
Train: 2018-08-02T13:22:23.103500: step 10530, loss 0.557873.
Test: 2018-08-02T13:22:23.572140: step 10530, loss 0.546931.
Train: 2018-08-02T13:22:23.728355: step 10531, loss 0.506715.
Train: 2018-08-02T13:22:23.884569: step 10532, loss 0.565897.
Train: 2018-08-02T13:22:24.040812: step 10533, loss 0.514453.
Train: 2018-08-02T13:22:24.197024: step 10534, loss 0.446044.
Train: 2018-08-02T13:22:24.353208: step 10535, loss 0.495235.
Train: 2018-08-02T13:22:24.509422: step 10536, loss 0.512577.
Train: 2018-08-02T13:22:24.665636: step 10537, loss 0.465916.
Train: 2018-08-02T13:22:24.821848: step 10538, loss 0.5242.
Train: 2018-08-02T13:22:24.993708: step 10539, loss 0.512124.
Train: 2018-08-02T13:22:25.149926: step 10540, loss 0.522746.
Test: 2018-08-02T13:22:25.602915: step 10540, loss 0.549636.
Train: 2018-08-02T13:22:25.759159: step 10541, loss 0.594214.
Train: 2018-08-02T13:22:25.930964: step 10542, loss 0.482248.
Train: 2018-08-02T13:22:26.087177: step 10543, loss 0.546785.
Train: 2018-08-02T13:22:26.243419: step 10544, loss 0.536832.
Train: 2018-08-02T13:22:26.399633: step 10545, loss 0.510188.
Train: 2018-08-02T13:22:26.555848: step 10546, loss 0.570144.
Train: 2018-08-02T13:22:26.696439: step 10547, loss 0.549629.
Train: 2018-08-02T13:22:26.868244: step 10548, loss 0.593992.
Train: 2018-08-02T13:22:27.024487: step 10549, loss 0.523472.
Train: 2018-08-02T13:22:27.180700: step 10550, loss 0.548272.
Test: 2018-08-02T13:22:27.633720: step 10550, loss 0.557153.
Train: 2018-08-02T13:22:27.789928: step 10551, loss 0.643119.
Train: 2018-08-02T13:22:27.946146: step 10552, loss 0.570813.
Train: 2018-08-02T13:22:28.102329: step 10553, loss 0.515204.
Train: 2018-08-02T13:22:28.258573: step 10554, loss 0.546542.
Train: 2018-08-02T13:22:28.414786: step 10555, loss 0.555566.
Train: 2018-08-02T13:22:28.570970: step 10556, loss 0.522109.
Train: 2018-08-02T13:22:28.727184: step 10557, loss 0.429639.
Train: 2018-08-02T13:22:28.883426: step 10558, loss 0.534388.
Train: 2018-08-02T13:22:29.039611: step 10559, loss 0.579836.
Train: 2018-08-02T13:22:29.195856: step 10560, loss 0.531955.
Test: 2018-08-02T13:22:29.664464: step 10560, loss 0.550259.
Train: 2018-08-02T13:22:29.805087: step 10561, loss 0.600289.
Train: 2018-08-02T13:22:29.976921: step 10562, loss 0.669383.
Train: 2018-08-02T13:22:30.133134: step 10563, loss 0.502729.
Train: 2018-08-02T13:22:30.289317: step 10564, loss 0.491066.
Train: 2018-08-02T13:22:30.445568: step 10565, loss 0.505624.
Train: 2018-08-02T13:22:30.601744: step 10566, loss 0.568322.
Train: 2018-08-02T13:22:30.757983: step 10567, loss 0.550983.
Train: 2018-08-02T13:22:30.929823: step 10568, loss 0.561654.
Train: 2018-08-02T13:22:31.086006: step 10569, loss 0.48245.
Train: 2018-08-02T13:22:31.195381: step 10570, loss 0.607838.
Test: 2018-08-02T13:22:31.648404: step 10570, loss 0.564147.
Train: 2018-08-02T13:22:31.804617: step 10571, loss 0.495789.
Train: 2018-08-02T13:22:31.960831: step 10572, loss 0.50779.
Train: 2018-08-02T13:22:32.117014: step 10573, loss 0.555919.
Train: 2018-08-02T13:22:32.257636: step 10574, loss 0.461725.
Train: 2018-08-02T13:22:32.413820: step 10575, loss 0.544709.
Train: 2018-08-02T13:22:32.570063: step 10576, loss 0.540672.
Train: 2018-08-02T13:22:32.741869: step 10577, loss 0.517981.
Train: 2018-08-02T13:22:32.898106: step 10578, loss 0.504596.
Train: 2018-08-02T13:22:33.054326: step 10579, loss 0.597735.
Train: 2018-08-02T13:22:33.210539: step 10580, loss 0.514469.
Test: 2018-08-02T13:22:33.663529: step 10580, loss 0.550784.
Train: 2018-08-02T13:22:33.819770: step 10581, loss 0.464299.
Train: 2018-08-02T13:22:34.038440: step 10582, loss 0.447753.
Train: 2018-08-02T13:22:34.194683: step 10583, loss 0.603885.
Train: 2018-08-02T13:22:34.350900: step 10584, loss 0.515855.
Train: 2018-08-02T13:22:34.507106: step 10585, loss 0.49037.
Train: 2018-08-02T13:22:34.663318: step 10586, loss 0.49214.
Train: 2018-08-02T13:22:34.835128: step 10587, loss 0.508363.
Train: 2018-08-02T13:22:34.991372: step 10588, loss 0.495025.
Train: 2018-08-02T13:22:35.147580: step 10589, loss 0.499982.
Train: 2018-08-02T13:22:35.303798: step 10590, loss 0.556004.
Test: 2018-08-02T13:22:35.772410: step 10590, loss 0.552668.
Train: 2018-08-02T13:22:35.928652: step 10591, loss 0.546878.
Train: 2018-08-02T13:22:36.084863: step 10592, loss 0.622246.
Train: 2018-08-02T13:22:36.241050: step 10593, loss 0.514334.
Train: 2018-08-02T13:22:36.397263: step 10594, loss 0.50385.
Train: 2018-08-02T13:22:36.553505: step 10595, loss 0.486741.
Train: 2018-08-02T13:22:36.709719: step 10596, loss 0.526593.
Train: 2018-08-02T13:22:36.865932: step 10597, loss 0.450741.
Train: 2018-08-02T13:22:37.022145: step 10598, loss 0.546044.
Train: 2018-08-02T13:22:37.178359: step 10599, loss 0.509361.
Train: 2018-08-02T13:22:37.334572: step 10600, loss 0.567561.
Test: 2018-08-02T13:22:37.787593: step 10600, loss 0.547588.
Train: 2018-08-02T13:22:38.443657: step 10601, loss 0.507108.
Train: 2018-08-02T13:22:38.599900: step 10602, loss 0.455111.
Train: 2018-08-02T13:22:38.756086: step 10603, loss 0.545926.
Train: 2018-08-02T13:22:38.912330: step 10604, loss 0.470486.
Train: 2018-08-02T13:22:39.068512: step 10605, loss 0.535703.
Train: 2018-08-02T13:22:39.224765: step 10606, loss 0.5387.
Train: 2018-08-02T13:22:39.380969: step 10607, loss 0.478237.
Train: 2018-08-02T13:22:39.552804: step 10608, loss 0.499077.
Train: 2018-08-02T13:22:39.708994: step 10609, loss 0.493004.
Train: 2018-08-02T13:22:39.865231: step 10610, loss 0.523572.
Test: 2018-08-02T13:22:40.318218: step 10610, loss 0.547527.
Train: 2018-08-02T13:22:40.474461: step 10611, loss 0.489656.
Train: 2018-08-02T13:22:40.630679: step 10612, loss 0.539078.
Train: 2018-08-02T13:22:40.786859: step 10613, loss 0.479283.
Train: 2018-08-02T13:22:40.943073: step 10614, loss 0.617851.
Train: 2018-08-02T13:22:41.114938: step 10615, loss 0.507148.
Train: 2018-08-02T13:22:41.271126: step 10616, loss 0.502585.
Train: 2018-08-02T13:22:41.427333: step 10617, loss 0.503353.
Train: 2018-08-02T13:22:41.583576: step 10618, loss 0.497779.
Train: 2018-08-02T13:22:41.739792: step 10619, loss 0.479479.
Train: 2018-08-02T13:22:41.895974: step 10620, loss 0.546993.
Test: 2018-08-02T13:22:42.364619: step 10620, loss 0.549059.
Train: 2018-08-02T13:22:42.505237: step 10621, loss 0.535453.
Train: 2018-08-02T13:22:42.661420: step 10622, loss 0.532276.
Train: 2018-08-02T13:22:42.833285: step 10623, loss 0.607035.
Train: 2018-08-02T13:22:42.973881: step 10624, loss 0.517869.
Train: 2018-08-02T13:22:43.145682: step 10625, loss 0.485188.
Train: 2018-08-02T13:22:43.301925: step 10626, loss 0.55113.
Train: 2018-08-02T13:22:43.458136: step 10627, loss 0.526049.
Train: 2018-08-02T13:22:43.614356: step 10628, loss 0.565528.
Train: 2018-08-02T13:22:43.770567: step 10629, loss 0.534686.
Train: 2018-08-02T13:22:43.926775: step 10630, loss 0.569342.
Test: 2018-08-02T13:22:44.379769: step 10630, loss 0.55449.
Train: 2018-08-02T13:22:44.551627: step 10631, loss 0.511136.
Train: 2018-08-02T13:22:44.707844: step 10632, loss 0.493272.
Train: 2018-08-02T13:22:44.864041: step 10633, loss 0.484606.
Train: 2018-08-02T13:22:45.035894: step 10634, loss 0.472791.
Train: 2018-08-02T13:22:45.192108: step 10635, loss 0.515426.
Train: 2018-08-02T13:22:45.348291: step 10636, loss 0.42968.
Train: 2018-08-02T13:22:45.504504: step 10637, loss 0.663001.
Train: 2018-08-02T13:22:45.660748: step 10638, loss 0.565977.
Train: 2018-08-02T13:22:45.816941: step 10639, loss 0.534796.
Train: 2018-08-02T13:22:45.973180: step 10640, loss 0.581641.
Test: 2018-08-02T13:22:46.441786: step 10640, loss 0.562622.
Train: 2018-08-02T13:22:46.644892: step 10641, loss 0.519137.
Train: 2018-08-02T13:22:46.801105: step 10642, loss 0.554129.
Train: 2018-08-02T13:22:46.957319: step 10643, loss 0.556802.
Train: 2018-08-02T13:22:47.113533: step 10644, loss 0.499308.
Train: 2018-08-02T13:22:47.269716: step 10645, loss 0.500384.
Train: 2018-08-02T13:22:47.425959: step 10646, loss 0.574921.
Train: 2018-08-02T13:22:47.582167: step 10647, loss 0.539971.
Train: 2018-08-02T13:22:47.738380: step 10648, loss 0.476631.
Train: 2018-08-02T13:22:47.894570: step 10649, loss 0.554522.
Train: 2018-08-02T13:22:48.050782: step 10650, loss 0.532566.
Test: 2018-08-02T13:22:48.503803: step 10650, loss 0.546925.
Train: 2018-08-02T13:22:48.660016: step 10651, loss 0.597275.
Train: 2018-08-02T13:22:48.816237: step 10652, loss 0.458141.
Train: 2018-08-02T13:22:48.972468: step 10653, loss 0.5346.
Train: 2018-08-02T13:22:49.128655: step 10654, loss 0.566708.
Train: 2018-08-02T13:22:49.284893: step 10655, loss 0.482556.
Train: 2018-08-02T13:22:49.441115: step 10656, loss 0.463217.
Train: 2018-08-02T13:22:49.612948: step 10657, loss 0.466517.
Train: 2018-08-02T13:22:49.769131: step 10658, loss 0.56278.
Train: 2018-08-02T13:22:49.925343: step 10659, loss 0.54613.
Train: 2018-08-02T13:22:50.081598: step 10660, loss 0.47148.
Test: 2018-08-02T13:22:50.550197: step 10660, loss 0.549183.
Train: 2018-08-02T13:22:50.706411: step 10661, loss 0.548851.
Train: 2018-08-02T13:22:50.862648: step 10662, loss 0.582264.
Train: 2018-08-02T13:22:51.018837: step 10663, loss 0.460775.
Train: 2018-08-02T13:22:51.175081: step 10664, loss 0.526579.
Train: 2018-08-02T13:22:51.331294: step 10665, loss 0.556743.
Train: 2018-08-02T13:22:51.487477: step 10666, loss 0.50091.
Train: 2018-08-02T13:22:51.643691: step 10667, loss 0.472191.
Train: 2018-08-02T13:22:51.799904: step 10668, loss 0.604002.
Train: 2018-08-02T13:22:51.956149: step 10669, loss 0.486681.
Train: 2018-08-02T13:22:52.127995: step 10670, loss 0.487444.
Test: 2018-08-02T13:22:52.580972: step 10670, loss 0.547906.
Train: 2018-08-02T13:22:52.737209: step 10671, loss 0.353201.
Train: 2018-08-02T13:22:52.893429: step 10672, loss 0.557802.
Train: 2018-08-02T13:22:53.049642: step 10673, loss 0.495194.
Train: 2018-08-02T13:22:53.221477: step 10674, loss 0.563242.
Train: 2018-08-02T13:22:53.377661: step 10675, loss 0.473512.
Train: 2018-08-02T13:22:53.518283: step 10676, loss 0.464316.
Train: 2018-08-02T13:22:53.674500: step 10677, loss 0.463919.
Train: 2018-08-02T13:22:53.830714: step 10678, loss 0.575885.
Train: 2018-08-02T13:22:53.986924: step 10679, loss 0.527808.
Train: 2018-08-02T13:22:54.158728: step 10680, loss 0.577858.
Test: 2018-08-02T13:22:54.611772: step 10680, loss 0.55533.
Train: 2018-08-02T13:22:54.767990: step 10681, loss 0.510109.
Train: 2018-08-02T13:22:54.924174: step 10682, loss 0.547333.
Train: 2018-08-02T13:22:55.080387: step 10683, loss 0.557726.
Train: 2018-08-02T13:22:55.236630: step 10684, loss 0.519859.
Train: 2018-08-02T13:22:55.392837: step 10685, loss 0.556539.
Train: 2018-08-02T13:22:55.549057: step 10686, loss 0.505206.
Train: 2018-08-02T13:22:55.705271: step 10687, loss 0.561417.
Train: 2018-08-02T13:22:55.861485: step 10688, loss 0.526188.
Train: 2018-08-02T13:22:56.017675: step 10689, loss 0.527986.
Train: 2018-08-02T13:22:56.173882: step 10690, loss 0.487686.
Test: 2018-08-02T13:22:56.626900: step 10690, loss 0.547945.
Train: 2018-08-02T13:22:56.783113: step 10691, loss 0.504629.
Train: 2018-08-02T13:22:56.939326: step 10692, loss 0.577592.
Train: 2018-08-02T13:22:57.111162: step 10693, loss 0.601358.
Train: 2018-08-02T13:22:57.267405: step 10694, loss 0.562284.
Train: 2018-08-02T13:22:57.423589: step 10695, loss 0.487253.
Train: 2018-08-02T13:22:57.579832: step 10696, loss 0.539765.
Train: 2018-08-02T13:22:57.736017: step 10697, loss 0.565283.
Train: 2018-08-02T13:22:57.892264: step 10698, loss 0.4686.
Train: 2018-08-02T13:22:58.048472: step 10699, loss 0.537063.
Train: 2018-08-02T13:22:58.220311: step 10700, loss 0.535203.
Test: 2018-08-02T13:22:58.673298: step 10700, loss 0.549621.
Train: 2018-08-02T13:22:59.266937: step 10701, loss 0.512152.
Train: 2018-08-02T13:22:59.438772: step 10702, loss 0.558671.
Train: 2018-08-02T13:22:59.594985: step 10703, loss 0.60338.
Train: 2018-08-02T13:22:59.751198: step 10704, loss 0.528912.
Train: 2018-08-02T13:22:59.907411: step 10705, loss 0.478616.
Train: 2018-08-02T13:23:00.063596: step 10706, loss 0.510428.
Train: 2018-08-02T13:23:00.219833: step 10707, loss 0.574952.
Train: 2018-08-02T13:23:00.376021: step 10708, loss 0.506069.
Train: 2018-08-02T13:23:00.532236: step 10709, loss 0.516936.
Train: 2018-08-02T13:23:00.688480: step 10710, loss 0.446076.
Test: 2018-08-02T13:23:01.141467: step 10710, loss 0.547162.
Train: 2018-08-02T13:23:01.297690: step 10711, loss 0.511037.
Train: 2018-08-02T13:23:01.453924: step 10712, loss 0.450852.
Train: 2018-08-02T13:23:01.610134: step 10713, loss 0.488068.
Train: 2018-08-02T13:23:01.766353: step 10714, loss 0.528838.
Train: 2018-08-02T13:23:01.922590: step 10715, loss 0.503333.
Train: 2018-08-02T13:23:02.094369: step 10716, loss 0.577438.
Train: 2018-08-02T13:23:02.234991: step 10717, loss 0.452904.
Train: 2018-08-02T13:23:02.391206: step 10718, loss 0.419218.
Train: 2018-08-02T13:23:02.547387: step 10719, loss 0.510234.
Train: 2018-08-02T13:23:02.703631: step 10720, loss 0.505585.
Test: 2018-08-02T13:23:03.156652: step 10720, loss 0.546985.
Train: 2018-08-02T13:23:03.265969: step 10721, loss 0.556366.
Train: 2018-08-02T13:23:03.437835: step 10722, loss 0.473914.
Train: 2018-08-02T13:23:03.594053: step 10723, loss 0.491333.
Train: 2018-08-02T13:23:03.750262: step 10724, loss 0.550938.
Train: 2018-08-02T13:23:03.906475: step 10725, loss 0.504756.
Train: 2018-08-02T13:23:04.062690: step 10726, loss 0.50997.
Train: 2018-08-02T13:23:04.234493: step 10727, loss 0.50913.
Train: 2018-08-02T13:23:04.390706: step 10728, loss 0.486356.
Train: 2018-08-02T13:23:04.546920: step 10729, loss 0.399579.
Train: 2018-08-02T13:23:04.703162: step 10730, loss 0.423364.
Test: 2018-08-02T13:23:05.156183: step 10730, loss 0.557171.
Train: 2018-08-02T13:23:05.312396: step 10731, loss 0.637104.
Train: 2018-08-02T13:23:05.468579: step 10732, loss 0.527366.
Train: 2018-08-02T13:23:05.624823: step 10733, loss 0.601505.
Train: 2018-08-02T13:23:05.796628: step 10734, loss 0.462536.
Train: 2018-08-02T13:23:05.952869: step 10735, loss 0.569712.
Train: 2018-08-02T13:23:06.109055: step 10736, loss 0.564062.
Train: 2018-08-02T13:23:06.265298: step 10737, loss 0.529986.
Train: 2018-08-02T13:23:06.421513: step 10738, loss 0.571963.
Train: 2018-08-02T13:23:06.577694: step 10739, loss 0.492388.
Train: 2018-08-02T13:23:06.733910: step 10740, loss 0.535765.
Test: 2018-08-02T13:23:07.202580: step 10740, loss 0.552971.
Train: 2018-08-02T13:23:07.358762: step 10741, loss 0.48756.
Train: 2018-08-02T13:23:07.514975: step 10742, loss 0.519026.
Train: 2018-08-02T13:23:07.671227: step 10743, loss 0.534133.
Train: 2018-08-02T13:23:07.827402: step 10744, loss 0.540569.
Train: 2018-08-02T13:23:07.983615: step 10745, loss 0.511107.
Train: 2018-08-02T13:23:08.139859: step 10746, loss 0.596971.
Train: 2018-08-02T13:23:08.296042: step 10747, loss 0.565394.
Train: 2018-08-02T13:23:08.452257: step 10748, loss 0.553806.
Train: 2018-08-02T13:23:08.608500: step 10749, loss 0.495671.
Train: 2018-08-02T13:23:08.764712: step 10750, loss 0.511929.
Test: 2018-08-02T13:23:09.217702: step 10750, loss 0.552654.
Train: 2018-08-02T13:23:09.373945: step 10751, loss 0.502871.
Train: 2018-08-02T13:23:09.545751: step 10752, loss 0.565789.
Train: 2018-08-02T13:23:09.701964: step 10753, loss 0.512256.
Train: 2018-08-02T13:23:09.858178: step 10754, loss 0.429677.
Train: 2018-08-02T13:23:10.014420: step 10755, loss 0.525782.
Train: 2018-08-02T13:23:10.170633: step 10756, loss 0.51658.
Train: 2018-08-02T13:23:10.326848: step 10757, loss 0.600187.
Train: 2018-08-02T13:23:10.483060: step 10758, loss 0.573408.
Train: 2018-08-02T13:23:10.639276: step 10759, loss 0.568943.
Train: 2018-08-02T13:23:10.795463: step 10760, loss 0.548861.
Test: 2018-08-02T13:23:11.248477: step 10760, loss 0.559205.
Train: 2018-08-02T13:23:11.404729: step 10761, loss 0.502156.
Train: 2018-08-02T13:23:11.560933: step 10762, loss 0.478944.
Train: 2018-08-02T13:23:11.717117: step 10763, loss 0.511463.
Train: 2018-08-02T13:23:11.888982: step 10764, loss 0.517831.
Train: 2018-08-02T13:23:12.045164: step 10765, loss 0.486308.
Train: 2018-08-02T13:23:12.201406: step 10766, loss 0.534697.
Train: 2018-08-02T13:23:12.357591: step 10767, loss 0.513001.
Train: 2018-08-02T13:23:12.513805: step 10768, loss 0.624206.
Train: 2018-08-02T13:23:12.670020: step 10769, loss 0.50389.
Train: 2018-08-02T13:23:12.826260: step 10770, loss 0.498118.
Test: 2018-08-02T13:23:13.294880: step 10770, loss 0.548643.
Train: 2018-08-02T13:23:13.451117: step 10771, loss 0.529276.
Train: 2018-08-02T13:23:13.607330: step 10772, loss 0.536091.
Train: 2018-08-02T13:23:13.763513: step 10773, loss 0.529277.
Train: 2018-08-02T13:23:13.919751: step 10774, loss 0.483468.
Train: 2018-08-02T13:23:14.075969: step 10775, loss 0.531028.
Train: 2018-08-02T13:23:14.232177: step 10776, loss 0.498106.
Train: 2018-08-02T13:23:14.388366: step 10777, loss 0.570921.
Train: 2018-08-02T13:23:14.544610: step 10778, loss 0.507237.
Train: 2018-08-02T13:23:14.700820: step 10779, loss 0.489269.
Train: 2018-08-02T13:23:14.857011: step 10780, loss 0.483882.
Test: 2018-08-02T13:23:15.310025: step 10780, loss 0.549415.
Train: 2018-08-02T13:23:15.481890: step 10781, loss 0.497345.
Train: 2018-08-02T13:23:15.638074: step 10782, loss 0.558253.
Train: 2018-08-02T13:23:15.794317: step 10783, loss 0.513267.
Train: 2018-08-02T13:23:15.950501: step 10784, loss 0.527508.
Train: 2018-08-02T13:23:16.106738: step 10785, loss 0.433488.
Train: 2018-08-02T13:23:16.262954: step 10786, loss 0.520606.
Train: 2018-08-02T13:23:16.419171: step 10787, loss 0.570508.
Train: 2018-08-02T13:23:16.575353: step 10788, loss 0.491507.
Train: 2018-08-02T13:23:16.731568: step 10789, loss 0.536028.
Train: 2018-08-02T13:23:16.887803: step 10790, loss 0.453461.
Test: 2018-08-02T13:23:17.340799: step 10790, loss 0.547309.
Train: 2018-08-02T13:23:17.512636: step 10791, loss 0.533324.
Train: 2018-08-02T13:23:17.668847: step 10792, loss 0.553136.
Train: 2018-08-02T13:23:17.825090: step 10793, loss 0.476538.
Train: 2018-08-02T13:23:17.981300: step 10794, loss 0.518201.
Train: 2018-08-02T13:23:18.137488: step 10795, loss 0.4374.
Train: 2018-08-02T13:23:18.293732: step 10796, loss 0.483682.
Train: 2018-08-02T13:23:18.449914: step 10797, loss 0.569978.
Train: 2018-08-02T13:23:18.606129: step 10798, loss 0.51379.
Train: 2018-08-02T13:23:18.762354: step 10799, loss 0.532424.
Train: 2018-08-02T13:23:18.918580: step 10800, loss 0.543716.
Test: 2018-08-02T13:23:19.371575: step 10800, loss 0.55572.
Train: 2018-08-02T13:23:19.980830: step 10801, loss 0.536406.
Train: 2018-08-02T13:23:20.137050: step 10802, loss 0.540021.
Train: 2018-08-02T13:23:20.293263: step 10803, loss 0.487203.
Train: 2018-08-02T13:23:20.465067: step 10804, loss 0.468989.
Train: 2018-08-02T13:23:20.621311: step 10805, loss 0.55769.
Train: 2018-08-02T13:23:20.777495: step 10806, loss 0.559105.
Train: 2018-08-02T13:23:20.933707: step 10807, loss 0.550629.
Train: 2018-08-02T13:23:21.089946: step 10808, loss 0.498151.
Train: 2018-08-02T13:23:21.246165: step 10809, loss 0.569796.
Train: 2018-08-02T13:23:21.418000: step 10810, loss 0.528846.
Test: 2018-08-02T13:23:21.871019: step 10810, loss 0.553445.
Train: 2018-08-02T13:23:22.027202: step 10811, loss 0.500408.
Train: 2018-08-02T13:23:22.183445: step 10812, loss 0.530926.
Train: 2018-08-02T13:23:22.339654: step 10813, loss 0.457664.
Train: 2018-08-02T13:23:22.495843: step 10814, loss 0.633463.
Train: 2018-08-02T13:23:22.652055: step 10815, loss 0.456955.
Train: 2018-08-02T13:23:22.808299: step 10816, loss 0.516578.
Train: 2018-08-02T13:23:22.964511: step 10817, loss 0.513082.
Train: 2018-08-02T13:23:23.120725: step 10818, loss 0.533465.
Train: 2018-08-02T13:23:23.276908: step 10819, loss 0.471267.
Train: 2018-08-02T13:23:23.433152: step 10820, loss 0.490249.
Test: 2018-08-02T13:23:23.901778: step 10820, loss 0.547148.
Train: 2018-08-02T13:23:24.057977: step 10821, loss 0.46037.
Train: 2018-08-02T13:23:24.214190: step 10822, loss 0.50496.
Train: 2018-08-02T13:23:24.370404: step 10823, loss 0.54271.
Train: 2018-08-02T13:23:24.526646: step 10824, loss 0.464246.
Train: 2018-08-02T13:23:24.698479: step 10825, loss 0.515897.
Train: 2018-08-02T13:23:24.854696: step 10826, loss 0.527151.
Train: 2018-08-02T13:23:25.010904: step 10827, loss 0.521302.
Train: 2018-08-02T13:23:25.167118: step 10828, loss 0.496598.
Train: 2018-08-02T13:23:25.323333: step 10829, loss 0.598052.
Train: 2018-08-02T13:23:25.479549: step 10830, loss 0.418534.
Test: 2018-08-02T13:23:25.948185: step 10830, loss 0.549801.
Train: 2018-08-02T13:23:26.104402: step 10831, loss 0.583911.
Train: 2018-08-02T13:23:26.260616: step 10832, loss 0.566645.
Train: 2018-08-02T13:23:26.416829: step 10833, loss 0.550863.
Train: 2018-08-02T13:23:26.573041: step 10834, loss 0.493531.
Train: 2018-08-02T13:23:26.729226: step 10835, loss 0.605703.
Train: 2018-08-02T13:23:26.885464: step 10836, loss 0.538169.
Train: 2018-08-02T13:23:27.041683: step 10837, loss 0.531831.
Train: 2018-08-02T13:23:27.197866: step 10838, loss 0.551769.
Train: 2018-08-02T13:23:27.354110: step 10839, loss 0.467569.
Train: 2018-08-02T13:23:27.510323: step 10840, loss 0.520944.
Test: 2018-08-02T13:23:27.978934: step 10840, loss 0.548405.
Train: 2018-08-02T13:23:28.135178: step 10841, loss 0.546642.
Train: 2018-08-02T13:23:28.291392: step 10842, loss 0.516759.
Train: 2018-08-02T13:23:28.463194: step 10843, loss 0.539547.
Train: 2018-08-02T13:23:28.603817: step 10844, loss 0.514518.
Train: 2018-08-02T13:23:28.760026: step 10845, loss 0.49135.
Train: 2018-08-02T13:23:28.916244: step 10846, loss 0.45228.
Train: 2018-08-02T13:23:29.088082: step 10847, loss 0.536242.
Train: 2018-08-02T13:23:29.244262: step 10848, loss 0.542197.
Train: 2018-08-02T13:23:29.384883: step 10849, loss 0.533199.
Train: 2018-08-02T13:23:29.541091: step 10850, loss 0.520307.
Test: 2018-08-02T13:23:30.009732: step 10850, loss 0.547795.
Train: 2018-08-02T13:23:30.165946: step 10851, loss 0.496369.
Train: 2018-08-02T13:23:30.322164: step 10852, loss 0.511607.
Train: 2018-08-02T13:23:30.478377: step 10853, loss 0.488832.
Train: 2018-08-02T13:23:30.634590: step 10854, loss 0.537509.
Train: 2018-08-02T13:23:30.790775: step 10855, loss 0.562312.
Train: 2018-08-02T13:23:30.962609: step 10856, loss 0.504155.
Train: 2018-08-02T13:23:31.118852: step 10857, loss 0.492091.
Train: 2018-08-02T13:23:31.275035: step 10858, loss 0.463788.
Train: 2018-08-02T13:23:31.431283: step 10859, loss 0.538216.
Train: 2018-08-02T13:23:31.587488: step 10860, loss 0.633412.
Test: 2018-08-02T13:23:32.056103: step 10860, loss 0.549409.
Train: 2018-08-02T13:23:32.212316: step 10861, loss 0.455945.
Train: 2018-08-02T13:23:32.368530: step 10862, loss 0.541468.
Train: 2018-08-02T13:23:32.524774: step 10863, loss 0.535013.
Train: 2018-08-02T13:23:32.680983: step 10864, loss 0.463931.
Train: 2018-08-02T13:23:32.837200: step 10865, loss 0.510017.
Train: 2018-08-02T13:23:32.993384: step 10866, loss 0.46263.
Train: 2018-08-02T13:23:33.165244: step 10867, loss 0.520063.
Train: 2018-08-02T13:23:33.321457: step 10868, loss 0.467381.
Train: 2018-08-02T13:23:33.477675: step 10869, loss 0.582957.
Train: 2018-08-02T13:23:33.633889: step 10870, loss 0.533165.
Test: 2018-08-02T13:23:34.086912: step 10870, loss 0.550793.
Train: 2018-08-02T13:23:34.243124: step 10871, loss 0.578393.
Train: 2018-08-02T13:23:34.352442: step 10872, loss 0.478505.
Train: 2018-08-02T13:23:34.508684: step 10873, loss 0.523709.
Train: 2018-08-02T13:23:34.664894: step 10874, loss 0.526845.
Train: 2018-08-02T13:23:34.821080: step 10875, loss 0.512833.
Train: 2018-08-02T13:23:34.992916: step 10876, loss 0.558619.
Train: 2018-08-02T13:23:35.149129: step 10877, loss 0.519794.
Train: 2018-08-02T13:23:35.305366: step 10878, loss 0.561711.
Train: 2018-08-02T13:23:35.461570: step 10879, loss 0.556623.
Train: 2018-08-02T13:23:35.602149: step 10880, loss 0.526179.
Test: 2018-08-02T13:23:36.070807: step 10880, loss 0.551709.
Train: 2018-08-02T13:23:36.227003: step 10881, loss 0.541028.
Train: 2018-08-02T13:23:36.383245: step 10882, loss 0.590254.
Train: 2018-08-02T13:23:36.539456: step 10883, loss 0.531217.
Train: 2018-08-02T13:23:36.695669: step 10884, loss 0.527834.
Train: 2018-08-02T13:23:36.851879: step 10885, loss 0.504584.
Train: 2018-08-02T13:23:37.023689: step 10886, loss 0.53748.
Train: 2018-08-02T13:23:37.179905: step 10887, loss 0.471211.
Train: 2018-08-02T13:23:37.336116: step 10888, loss 0.542231.
Train: 2018-08-02T13:23:37.492329: step 10889, loss 0.466634.
Train: 2018-08-02T13:23:37.648569: step 10890, loss 0.540972.
Test: 2018-08-02T13:23:38.117184: step 10890, loss 0.548368.
Train: 2018-08-02T13:23:38.273427: step 10891, loss 0.507255.
Train: 2018-08-02T13:23:38.429641: step 10892, loss 0.440366.
Train: 2018-08-02T13:23:38.585854: step 10893, loss 0.448237.
Train: 2018-08-02T13:23:38.742037: step 10894, loss 0.530833.
Train: 2018-08-02T13:23:38.898282: step 10895, loss 0.630521.
Train: 2018-08-02T13:23:39.054496: step 10896, loss 0.516964.
Train: 2018-08-02T13:23:39.226300: step 10897, loss 0.552139.
Train: 2018-08-02T13:23:39.382512: step 10898, loss 0.611499.
Train: 2018-08-02T13:23:39.538726: step 10899, loss 0.607381.
Train: 2018-08-02T13:23:39.694940: step 10900, loss 0.51785.
Test: 2018-08-02T13:23:40.147983: step 10900, loss 0.563802.
Train: 2018-08-02T13:23:40.788465: step 10901, loss 0.54609.
Train: 2018-08-02T13:23:40.944678: step 10902, loss 0.549038.
Train: 2018-08-02T13:23:41.100860: step 10903, loss 0.580436.
Train: 2018-08-02T13:23:41.272726: step 10904, loss 0.533711.
Train: 2018-08-02T13:23:41.428933: step 10905, loss 0.569399.
Train: 2018-08-02T13:23:41.585156: step 10906, loss 0.513364.
Train: 2018-08-02T13:23:41.741367: step 10907, loss 0.512908.
Train: 2018-08-02T13:23:41.897583: step 10908, loss 0.476984.
Train: 2018-08-02T13:23:42.053787: step 10909, loss 0.528123.
Train: 2018-08-02T13:23:42.205397: step 10910, loss 0.545661.
Test: 2018-08-02T13:23:42.674028: step 10910, loss 0.546976.
Train: 2018-08-02T13:23:42.830251: step 10911, loss 0.447028.
Train: 2018-08-02T13:23:42.986460: step 10912, loss 0.573663.
Train: 2018-08-02T13:23:43.142644: step 10913, loss 0.512463.
Train: 2018-08-02T13:23:43.283265: step 10914, loss 0.57271.
Train: 2018-08-02T13:23:43.439479: step 10915, loss 0.464434.
Train: 2018-08-02T13:23:43.595696: step 10916, loss 0.502428.
Train: 2018-08-02T13:23:43.751890: step 10917, loss 0.471339.
Train: 2018-08-02T13:23:43.923712: step 10918, loss 0.512739.
Train: 2018-08-02T13:23:44.079925: step 10919, loss 0.58914.
Train: 2018-08-02T13:23:44.236137: step 10920, loss 0.476189.
Test: 2018-08-02T13:23:44.689157: step 10920, loss 0.54741.
Train: 2018-08-02T13:23:44.845369: step 10921, loss 0.542248.
Train: 2018-08-02T13:23:45.017205: step 10922, loss 0.545275.
Train: 2018-08-02T13:23:45.173419: step 10923, loss 0.481007.
Train: 2018-08-02T13:23:45.329631: step 10924, loss 0.497962.
Train: 2018-08-02T13:23:45.485876: step 10925, loss 0.519396.
Train: 2018-08-02T13:23:45.642060: step 10926, loss 0.506752.
Train: 2018-08-02T13:23:45.798302: step 10927, loss 0.425944.
Train: 2018-08-02T13:23:45.954514: step 10928, loss 0.497281.
Train: 2018-08-02T13:23:46.110699: step 10929, loss 0.519226.
Train: 2018-08-02T13:23:46.282560: step 10930, loss 0.560614.
Test: 2018-08-02T13:23:46.735589: step 10930, loss 0.549203.
Train: 2018-08-02T13:23:46.891765: step 10931, loss 0.520408.
Train: 2018-08-02T13:23:47.047980: step 10932, loss 0.511116.
Train: 2018-08-02T13:23:47.204220: step 10933, loss 0.463769.
Train: 2018-08-02T13:23:47.360432: step 10934, loss 0.478423.
Train: 2018-08-02T13:23:47.516622: step 10935, loss 0.518545.
Train: 2018-08-02T13:23:47.672832: step 10936, loss 0.563213.
Train: 2018-08-02T13:23:47.829080: step 10937, loss 0.449078.
Train: 2018-08-02T13:23:47.985291: step 10938, loss 0.411016.
Train: 2018-08-02T13:23:48.141474: step 10939, loss 0.533303.
Train: 2018-08-02T13:23:48.297686: step 10940, loss 0.545825.
Test: 2018-08-02T13:23:48.750729: step 10940, loss 0.548053.
Train: 2018-08-02T13:23:48.906949: step 10941, loss 0.588768.
Train: 2018-08-02T13:23:49.078753: step 10942, loss 0.572176.
Train: 2018-08-02T13:23:49.234998: step 10943, loss 0.512677.
Train: 2018-08-02T13:23:49.391181: step 10944, loss 0.567508.
Train: 2018-08-02T13:23:49.547422: step 10945, loss 0.543055.
Train: 2018-08-02T13:23:49.703607: step 10946, loss 0.532267.
Train: 2018-08-02T13:23:49.859851: step 10947, loss 0.499893.
Train: 2018-08-02T13:23:50.031686: step 10948, loss 0.535372.
Train: 2018-08-02T13:23:50.187868: step 10949, loss 0.568979.
Train: 2018-08-02T13:23:50.344116: step 10950, loss 0.441507.
Test: 2018-08-02T13:23:50.797151: step 10950, loss 0.5488.
Train: 2018-08-02T13:23:50.953343: step 10951, loss 0.490887.
Train: 2018-08-02T13:23:51.109528: step 10952, loss 0.46864.
Train: 2018-08-02T13:23:51.265773: step 10953, loss 0.550284.
Train: 2018-08-02T13:23:51.421980: step 10954, loss 0.529705.
Train: 2018-08-02T13:23:51.578169: step 10955, loss 0.428394.
Train: 2018-08-02T13:23:51.734407: step 10956, loss 0.474.
Train: 2018-08-02T13:23:51.890626: step 10957, loss 0.551121.
Train: 2018-08-02T13:23:52.046839: step 10958, loss 0.520138.
Train: 2018-08-02T13:23:52.203047: step 10959, loss 0.502895.
Train: 2018-08-02T13:23:52.359266: step 10960, loss 0.583928.
Test: 2018-08-02T13:23:52.827876: step 10960, loss 0.550168.
Train: 2018-08-02T13:23:52.984120: step 10961, loss 0.514612.
Train: 2018-08-02T13:23:53.140333: step 10962, loss 0.55837.
Train: 2018-08-02T13:23:53.312179: step 10963, loss 0.587312.
Train: 2018-08-02T13:23:53.468351: step 10964, loss 0.592693.
Train: 2018-08-02T13:23:53.624594: step 10965, loss 0.516141.
Train: 2018-08-02T13:23:53.780778: step 10966, loss 0.587068.
Train: 2018-08-02T13:23:53.937021: step 10967, loss 0.524527.
Train: 2018-08-02T13:23:54.093238: step 10968, loss 0.514561.
Train: 2018-08-02T13:23:54.249418: step 10969, loss 0.508419.
Train: 2018-08-02T13:23:54.405632: step 10970, loss 0.476458.
Test: 2018-08-02T13:23:54.874279: step 10970, loss 0.546817.
Train: 2018-08-02T13:23:55.030485: step 10971, loss 0.483018.
Train: 2018-08-02T13:23:55.171102: step 10972, loss 0.580292.
Train: 2018-08-02T13:23:55.342913: step 10973, loss 0.497428.
Train: 2018-08-02T13:23:55.483534: step 10974, loss 0.497205.
Train: 2018-08-02T13:23:55.639748: step 10975, loss 0.57896.
Train: 2018-08-02T13:23:55.795970: step 10976, loss 0.490469.
Train: 2018-08-02T13:23:55.952145: step 10977, loss 0.443411.
Train: 2018-08-02T13:23:56.108389: step 10978, loss 0.467082.
Train: 2018-08-02T13:23:56.264571: step 10979, loss 0.54568.
Train: 2018-08-02T13:23:56.436433: step 10980, loss 0.520392.
Test: 2018-08-02T13:23:56.889440: step 10980, loss 0.54837.
Train: 2018-08-02T13:23:57.045669: step 10981, loss 0.495417.
Train: 2018-08-02T13:23:57.201879: step 10982, loss 0.493715.
Train: 2018-08-02T13:23:57.358095: step 10983, loss 0.613154.
Train: 2018-08-02T13:23:57.514309: step 10984, loss 0.574142.
Train: 2018-08-02T13:23:57.670520: step 10985, loss 0.547818.
Train: 2018-08-02T13:23:57.826735: step 10986, loss 0.617171.
Train: 2018-08-02T13:23:57.982945: step 10987, loss 0.532565.
Train: 2018-08-02T13:23:58.139132: step 10988, loss 0.518676.
Train: 2018-08-02T13:23:58.310966: step 10989, loss 0.547817.
Train: 2018-08-02T13:23:58.467205: step 10990, loss 0.5082.
Test: 2018-08-02T13:23:58.920198: step 10990, loss 0.559739.
Train: 2018-08-02T13:23:59.076413: step 10991, loss 0.5764.
Train: 2018-08-02T13:23:59.232627: step 10992, loss 0.509947.
Train: 2018-08-02T13:23:59.388870: step 10993, loss 0.546468.
Train: 2018-08-02T13:23:59.545055: step 10994, loss 0.611655.
Train: 2018-08-02T13:23:59.701296: step 10995, loss 0.520363.
Train: 2018-08-02T13:23:59.857505: step 10996, loss 0.588738.
Train: 2018-08-02T13:23:59.998080: step 10997, loss 0.564673.
Train: 2018-08-02T13:24:00.154315: step 10998, loss 0.542235.
Train: 2018-08-02T13:24:00.310523: step 10999, loss 0.562187.
Train: 2018-08-02T13:24:00.482359: step 11000, loss 0.52529.
Test: 2018-08-02T13:24:00.935353: step 11000, loss 0.554528.
Train: 2018-08-02T13:24:01.560230: step 11001, loss 0.579775.
Train: 2018-08-02T13:24:01.716452: step 11002, loss 0.528373.
Train: 2018-08-02T13:24:01.872662: step 11003, loss 0.614041.
Train: 2018-08-02T13:24:02.044470: step 11004, loss 0.505279.
Train: 2018-08-02T13:24:02.200682: step 11005, loss 0.522003.
Train: 2018-08-02T13:24:02.341297: step 11006, loss 0.503707.
Train: 2018-08-02T13:24:02.513138: step 11007, loss 0.648391.
Train: 2018-08-02T13:24:02.669321: step 11008, loss 0.538668.
Train: 2018-08-02T13:24:02.825534: step 11009, loss 0.52891.
Train: 2018-08-02T13:24:02.981754: step 11010, loss 0.481103.
Test: 2018-08-02T13:24:03.434768: step 11010, loss 0.548457.
Train: 2018-08-02T13:24:03.606603: step 11011, loss 0.537948.
Train: 2018-08-02T13:24:03.747223: step 11012, loss 0.537961.
Train: 2018-08-02T13:24:03.903407: step 11013, loss 0.513917.
Train: 2018-08-02T13:24:04.059650: step 11014, loss 0.494839.
Train: 2018-08-02T13:24:04.215867: step 11015, loss 0.500419.
Train: 2018-08-02T13:24:04.372049: step 11016, loss 0.524589.
Train: 2018-08-02T13:24:04.528291: step 11017, loss 0.571286.
Train: 2018-08-02T13:24:04.700124: step 11018, loss 0.503282.
Train: 2018-08-02T13:24:04.856309: step 11019, loss 0.540804.
Train: 2018-08-02T13:24:05.012522: step 11020, loss 0.554029.
Test: 2018-08-02T13:24:05.465542: step 11020, loss 0.550118.
Train: 2018-08-02T13:24:05.621785: step 11021, loss 0.463124.
Train: 2018-08-02T13:24:05.777998: step 11022, loss 0.523885.
Train: 2018-08-02T13:24:05.887347: step 11023, loss 0.477402.
Train: 2018-08-02T13:24:06.059152: step 11024, loss 0.537385.
Train: 2018-08-02T13:24:06.215399: step 11025, loss 0.563293.
Train: 2018-08-02T13:24:06.371579: step 11026, loss 0.504229.
Train: 2018-08-02T13:24:06.527822: step 11027, loss 0.477484.
Train: 2018-08-02T13:24:06.684030: step 11028, loss 0.561195.
Train: 2018-08-02T13:24:06.840244: step 11029, loss 0.566025.
Train: 2018-08-02T13:24:06.996433: step 11030, loss 0.519907.
Test: 2018-08-02T13:24:07.465087: step 11030, loss 0.550063.
Train: 2018-08-02T13:24:07.621288: step 11031, loss 0.544661.
Train: 2018-08-02T13:24:07.777531: step 11032, loss 0.50745.
Train: 2018-08-02T13:24:07.933739: step 11033, loss 0.519629.
Train: 2018-08-02T13:24:08.089951: step 11034, loss 0.543964.
Train: 2018-08-02T13:24:08.246170: step 11035, loss 0.524534.
Train: 2018-08-02T13:24:08.402379: step 11036, loss 0.569991.
Train: 2018-08-02T13:24:08.558597: step 11037, loss 0.543052.
Train: 2018-08-02T13:24:08.714814: step 11038, loss 0.567021.
Train: 2018-08-02T13:24:08.871023: step 11039, loss 0.457722.
Train: 2018-08-02T13:24:09.027232: step 11040, loss 0.555101.
Test: 2018-08-02T13:24:09.495849: step 11040, loss 0.551577.
Train: 2018-08-02T13:24:09.652061: step 11041, loss 0.494142.
Train: 2018-08-02T13:24:09.808280: step 11042, loss 0.514157.
Train: 2018-08-02T13:24:09.964489: step 11043, loss 0.595922.
Train: 2018-08-02T13:24:10.120731: step 11044, loss 0.473107.
Train: 2018-08-02T13:24:10.276945: step 11045, loss 0.553446.
Train: 2018-08-02T13:24:10.433128: step 11046, loss 0.499643.
Train: 2018-08-02T13:24:10.589374: step 11047, loss 0.548385.
Train: 2018-08-02T13:24:10.745585: step 11048, loss 0.510181.
Train: 2018-08-02T13:24:10.901799: step 11049, loss 0.573709.
Train: 2018-08-02T13:24:11.057983: step 11050, loss 0.499429.
Test: 2018-08-02T13:24:11.511001: step 11050, loss 0.548767.
Train: 2018-08-02T13:24:11.682869: step 11051, loss 0.528288.
Train: 2018-08-02T13:24:11.839049: step 11052, loss 0.457697.
Train: 2018-08-02T13:24:11.995315: step 11053, loss 0.565569.
Train: 2018-08-02T13:24:12.151475: step 11054, loss 0.569328.
Train: 2018-08-02T13:24:12.307690: step 11055, loss 0.558373.
Train: 2018-08-02T13:24:12.479524: step 11056, loss 0.507503.
Train: 2018-08-02T13:24:12.635769: step 11057, loss 0.459207.
Train: 2018-08-02T13:24:12.791951: step 11058, loss 0.560912.
Train: 2018-08-02T13:24:12.948164: step 11059, loss 0.531351.
Train: 2018-08-02T13:24:13.104407: step 11060, loss 0.501794.
Test: 2018-08-02T13:24:13.557429: step 11060, loss 0.546688.
Train: 2018-08-02T13:24:13.713610: step 11061, loss 0.549964.
Train: 2018-08-02T13:24:13.869849: step 11062, loss 0.55022.
Train: 2018-08-02T13:24:14.026067: step 11063, loss 0.492894.
Train: 2018-08-02T13:24:14.182250: step 11064, loss 0.581103.
Train: 2018-08-02T13:24:14.338512: step 11065, loss 0.556767.
Train: 2018-08-02T13:24:14.494702: step 11066, loss 0.578313.
Train: 2018-08-02T13:24:14.650890: step 11067, loss 0.572627.
Train: 2018-08-02T13:24:14.807106: step 11068, loss 0.495686.
Train: 2018-08-02T13:24:14.963318: step 11069, loss 0.521793.
Train: 2018-08-02T13:24:15.135153: step 11070, loss 0.520631.
Test: 2018-08-02T13:24:15.588205: step 11070, loss 0.551057.
Train: 2018-08-02T13:24:15.744410: step 11071, loss 0.54097.
Train: 2018-08-02T13:24:15.900629: step 11072, loss 0.509692.
Train: 2018-08-02T13:24:16.056841: step 11073, loss 0.599673.
Train: 2018-08-02T13:24:16.213058: step 11074, loss 0.50423.
Train: 2018-08-02T13:24:16.369268: step 11075, loss 0.473746.
Train: 2018-08-02T13:24:16.525477: step 11076, loss 0.473133.
Train: 2018-08-02T13:24:16.681694: step 11077, loss 0.453944.
Train: 2018-08-02T13:24:16.837911: step 11078, loss 0.476584.
Train: 2018-08-02T13:24:16.994121: step 11079, loss 0.594284.
Train: 2018-08-02T13:24:17.150338: step 11080, loss 0.517972.
Test: 2018-08-02T13:24:17.618946: step 11080, loss 0.548091.
Train: 2018-08-02T13:24:17.775159: step 11081, loss 0.520434.
Train: 2018-08-02T13:24:17.931373: step 11082, loss 0.477337.
Train: 2018-08-02T13:24:18.087587: step 11083, loss 0.504115.
Train: 2018-08-02T13:24:18.243824: step 11084, loss 0.594071.
Train: 2018-08-02T13:24:18.415665: step 11085, loss 0.481829.
Train: 2018-08-02T13:24:18.571872: step 11086, loss 0.511142.
Train: 2018-08-02T13:24:18.728091: step 11087, loss 0.538685.
Train: 2018-08-02T13:24:18.884274: step 11088, loss 0.511784.
Train: 2018-08-02T13:24:19.040520: step 11089, loss 0.486174.
Train: 2018-08-02T13:24:19.196728: step 11090, loss 0.530872.
Test: 2018-08-02T13:24:19.665366: step 11090, loss 0.557173.
Train: 2018-08-02T13:24:19.821583: step 11091, loss 0.565329.
Train: 2018-08-02T13:24:19.977798: step 11092, loss 0.502679.
Train: 2018-08-02T13:24:20.133981: step 11093, loss 0.531597.
Train: 2018-08-02T13:24:20.290225: step 11094, loss 0.545657.
Train: 2018-08-02T13:24:20.462030: step 11095, loss 0.491278.
Train: 2018-08-02T13:24:20.618273: step 11096, loss 0.50194.
Train: 2018-08-02T13:24:20.774457: step 11097, loss 0.554065.
Train: 2018-08-02T13:24:20.930670: step 11098, loss 0.563394.
Train: 2018-08-02T13:24:21.086883: step 11099, loss 0.494005.
Train: 2018-08-02T13:24:21.243096: step 11100, loss 0.471637.
Test: 2018-08-02T13:24:21.696144: step 11100, loss 0.549157.
Train: 2018-08-02T13:24:22.367833: step 11101, loss 0.567519.
Train: 2018-08-02T13:24:22.524072: step 11102, loss 0.51732.
Train: 2018-08-02T13:24:22.680300: step 11103, loss 0.510173.
Train: 2018-08-02T13:24:22.836507: step 11104, loss 0.475047.
Train: 2018-08-02T13:24:22.992686: step 11105, loss 0.49202.
Train: 2018-08-02T13:24:23.148900: step 11106, loss 0.481751.
Train: 2018-08-02T13:24:23.305144: step 11107, loss 0.537701.
Train: 2018-08-02T13:24:23.461328: step 11108, loss 0.525177.
Train: 2018-08-02T13:24:23.617571: step 11109, loss 0.516783.
Train: 2018-08-02T13:24:23.773754: step 11110, loss 0.514326.
Test: 2018-08-02T13:24:24.226774: step 11110, loss 0.552024.
Train: 2018-08-02T13:24:24.398634: step 11111, loss 0.518213.
Train: 2018-08-02T13:24:24.554822: step 11112, loss 0.481577.
Train: 2018-08-02T13:24:24.711040: step 11113, loss 0.542304.
Train: 2018-08-02T13:24:24.867250: step 11114, loss 0.537796.
Train: 2018-08-02T13:24:25.023488: step 11115, loss 0.519651.
Train: 2018-08-02T13:24:25.179701: step 11116, loss 0.500011.
Train: 2018-08-02T13:24:25.335917: step 11117, loss 0.468164.
Train: 2018-08-02T13:24:25.492133: step 11118, loss 0.43959.
Train: 2018-08-02T13:24:25.648346: step 11119, loss 0.452916.
Train: 2018-08-02T13:24:25.804564: step 11120, loss 0.501088.
Test: 2018-08-02T13:24:26.273169: step 11120, loss 0.551337.
Train: 2018-08-02T13:24:26.429415: step 11121, loss 0.529418.
Train: 2018-08-02T13:24:26.585624: step 11122, loss 0.513216.
Train: 2018-08-02T13:24:26.741839: step 11123, loss 0.46507.
Train: 2018-08-02T13:24:26.898031: step 11124, loss 0.527645.
Train: 2018-08-02T13:24:27.054269: step 11125, loss 0.489875.
Train: 2018-08-02T13:24:27.210480: step 11126, loss 0.49434.
Train: 2018-08-02T13:24:27.366688: step 11127, loss 0.456729.
Train: 2018-08-02T13:24:27.538523: step 11128, loss 0.496213.
Train: 2018-08-02T13:24:27.694712: step 11129, loss 0.561576.
Train: 2018-08-02T13:24:27.850950: step 11130, loss 0.492321.
Test: 2018-08-02T13:24:28.319564: step 11130, loss 0.545711.
Train: 2018-08-02T13:24:28.475780: step 11131, loss 0.512007.
Train: 2018-08-02T13:24:28.632021: step 11132, loss 0.471797.
Train: 2018-08-02T13:24:28.788208: step 11133, loss 0.506518.
Train: 2018-08-02T13:24:28.944442: step 11134, loss 0.486852.
Train: 2018-08-02T13:24:29.100632: step 11135, loss 0.555706.
Train: 2018-08-02T13:24:29.256851: step 11136, loss 0.430537.
Train: 2018-08-02T13:24:29.397462: step 11137, loss 0.534825.
Train: 2018-08-02T13:24:29.553680: step 11138, loss 0.556348.
Train: 2018-08-02T13:24:29.709897: step 11139, loss 0.589852.
Train: 2018-08-02T13:24:29.866077: step 11140, loss 0.51591.
Test: 2018-08-02T13:24:30.334718: step 11140, loss 0.552746.
Train: 2018-08-02T13:24:30.490932: step 11141, loss 0.49712.
Train: 2018-08-02T13:24:30.647177: step 11142, loss 0.515764.
Train: 2018-08-02T13:24:30.803391: step 11143, loss 0.523291.
Train: 2018-08-02T13:24:30.959605: step 11144, loss 0.513994.
Train: 2018-08-02T13:24:31.115816: step 11145, loss 0.648149.
Train: 2018-08-02T13:24:31.272028: step 11146, loss 0.500559.
Train: 2018-08-02T13:24:31.428236: step 11147, loss 0.537851.
Train: 2018-08-02T13:24:31.584458: step 11148, loss 0.480005.
Train: 2018-08-02T13:24:31.740674: step 11149, loss 0.532213.
Train: 2018-08-02T13:24:31.896883: step 11150, loss 0.512011.
Test: 2018-08-02T13:24:32.365517: step 11150, loss 0.550832.
Train: 2018-08-02T13:24:32.521736: step 11151, loss 0.577142.
Train: 2018-08-02T13:24:32.677953: step 11152, loss 0.528758.
Train: 2018-08-02T13:24:32.834165: step 11153, loss 0.549447.
Train: 2018-08-02T13:24:32.990346: step 11154, loss 0.466339.
Train: 2018-08-02T13:24:33.146559: step 11155, loss 0.506853.
Train: 2018-08-02T13:24:33.302773: step 11156, loss 0.516817.
Train: 2018-08-02T13:24:33.458988: step 11157, loss 0.555408.
Train: 2018-08-02T13:24:33.615201: step 11158, loss 0.480853.
Train: 2018-08-02T13:24:33.787064: step 11159, loss 0.481996.
Train: 2018-08-02T13:24:33.943275: step 11160, loss 0.572897.
Test: 2018-08-02T13:24:34.443131: step 11160, loss 0.547919.
Train: 2018-08-02T13:24:34.599375: step 11161, loss 0.540462.
Train: 2018-08-02T13:24:34.755557: step 11162, loss 0.510807.
Train: 2018-08-02T13:24:34.911773: step 11163, loss 0.482808.
Train: 2018-08-02T13:24:35.067998: step 11164, loss 0.534461.
Train: 2018-08-02T13:24:35.224222: step 11165, loss 0.516769.
Train: 2018-08-02T13:24:35.380440: step 11166, loss 0.467215.
Train: 2018-08-02T13:24:35.536642: step 11167, loss 0.497611.
Train: 2018-08-02T13:24:35.677246: step 11168, loss 0.463555.
Train: 2018-08-02T13:24:35.849051: step 11169, loss 0.503256.
Train: 2018-08-02T13:24:36.005291: step 11170, loss 0.579789.
Test: 2018-08-02T13:24:36.458283: step 11170, loss 0.54934.
Train: 2018-08-02T13:24:36.614497: step 11171, loss 0.509493.
Train: 2018-08-02T13:24:36.786332: step 11172, loss 0.51949.
Train: 2018-08-02T13:24:36.926953: step 11173, loss 0.517956.
Train: 2018-08-02T13:24:37.036303: step 11174, loss 0.422696.
Train: 2018-08-02T13:24:37.208142: step 11175, loss 0.556.
Train: 2018-08-02T13:24:37.364322: step 11176, loss 0.463258.
Train: 2018-08-02T13:24:37.520536: step 11177, loss 0.504611.
Train: 2018-08-02T13:24:37.676750: step 11178, loss 0.62958.
Train: 2018-08-02T13:24:37.832962: step 11179, loss 0.569454.
Train: 2018-08-02T13:24:37.989206: step 11180, loss 0.499291.
Test: 2018-08-02T13:24:38.442203: step 11180, loss 0.554347.
Train: 2018-08-02T13:24:38.598438: step 11181, loss 0.566708.
Train: 2018-08-02T13:24:38.754622: step 11182, loss 0.502355.
Train: 2018-08-02T13:24:38.910835: step 11183, loss 0.563849.
Train: 2018-08-02T13:24:39.067078: step 11184, loss 0.573871.
Train: 2018-08-02T13:24:39.223293: step 11185, loss 0.470883.
Train: 2018-08-02T13:24:39.395106: step 11186, loss 0.519444.
Train: 2018-08-02T13:24:39.535688: step 11187, loss 0.495776.
Train: 2018-08-02T13:24:39.707556: step 11188, loss 0.519127.
Train: 2018-08-02T13:24:39.863767: step 11189, loss 0.492363.
Train: 2018-08-02T13:24:40.019973: step 11190, loss 0.498013.
Test: 2018-08-02T13:24:40.472970: step 11190, loss 0.548218.
Train: 2018-08-02T13:24:40.644833: step 11191, loss 0.456042.
Train: 2018-08-02T13:24:40.801016: step 11192, loss 0.464752.
Train: 2018-08-02T13:24:40.957264: step 11193, loss 0.569947.
Train: 2018-08-02T13:24:41.113474: step 11194, loss 0.481081.
Train: 2018-08-02T13:24:41.269687: step 11195, loss 0.469542.
Train: 2018-08-02T13:24:41.425871: step 11196, loss 0.586899.
Train: 2018-08-02T13:24:41.582083: step 11197, loss 0.50803.
Train: 2018-08-02T13:24:41.738327: step 11198, loss 0.523458.
Train: 2018-08-02T13:24:41.894541: step 11199, loss 0.542197.
Train: 2018-08-02T13:24:42.050724: step 11200, loss 0.514921.
Test: 2018-08-02T13:24:42.503768: step 11200, loss 0.55377.
Train: 2018-08-02T13:24:43.191112: step 11201, loss 0.535717.
Train: 2018-08-02T13:24:43.347328: step 11202, loss 0.460795.
Train: 2018-08-02T13:24:43.503540: step 11203, loss 0.481917.
Train: 2018-08-02T13:24:43.659748: step 11204, loss 0.568374.
Train: 2018-08-02T13:24:43.815966: step 11205, loss 0.497836.
Train: 2018-08-02T13:24:43.972174: step 11206, loss 0.508665.
Train: 2018-08-02T13:24:44.128362: step 11207, loss 0.472888.
Train: 2018-08-02T13:24:44.268985: step 11208, loss 0.47333.
Train: 2018-08-02T13:24:44.425197: step 11209, loss 0.488627.
Train: 2018-08-02T13:24:44.581381: step 11210, loss 0.556875.
Test: 2018-08-02T13:24:45.050022: step 11210, loss 0.548734.
Train: 2018-08-02T13:24:45.206269: step 11211, loss 0.50379.
Train: 2018-08-02T13:24:45.362449: step 11212, loss 0.476758.
Train: 2018-08-02T13:24:45.518692: step 11213, loss 0.509666.
Train: 2018-08-02T13:24:45.674905: step 11214, loss 0.441224.
Train: 2018-08-02T13:24:45.831122: step 11215, loss 0.515815.
Train: 2018-08-02T13:24:45.987332: step 11216, loss 0.521453.
Train: 2018-08-02T13:24:46.143545: step 11217, loss 0.516225.
Train: 2018-08-02T13:24:46.299759: step 11218, loss 0.59502.
Train: 2018-08-02T13:24:46.455975: step 11219, loss 0.61582.
Train: 2018-08-02T13:24:46.612186: step 11220, loss 0.483109.
Test: 2018-08-02T13:24:47.065175: step 11220, loss 0.558585.
Train: 2018-08-02T13:24:47.283905: step 11221, loss 0.539307.
Train: 2018-08-02T13:24:47.440121: step 11222, loss 0.524566.
Train: 2018-08-02T13:24:47.596302: step 11223, loss 0.518172.
Train: 2018-08-02T13:24:47.752515: step 11224, loss 0.520817.
Train: 2018-08-02T13:24:47.908727: step 11225, loss 0.489933.
Train: 2018-08-02T13:24:48.064941: step 11226, loss 0.545207.
Train: 2018-08-02T13:24:48.221184: step 11227, loss 0.530985.
Train: 2018-08-02T13:24:48.377393: step 11228, loss 0.623909.
Train: 2018-08-02T13:24:48.533610: step 11229, loss 0.582248.
Train: 2018-08-02T13:24:48.689795: step 11230, loss 0.419049.
Test: 2018-08-02T13:24:49.158449: step 11230, loss 0.547323.
Train: 2018-08-02T13:24:49.314647: step 11231, loss 0.578109.
Train: 2018-08-02T13:24:49.470891: step 11232, loss 0.514442.
Train: 2018-08-02T13:24:49.627101: step 11233, loss 0.423095.
Train: 2018-08-02T13:24:49.783289: step 11234, loss 0.496483.
Train: 2018-08-02T13:24:49.939531: step 11235, loss 0.603528.
Train: 2018-08-02T13:24:50.095745: step 11236, loss 0.543024.
Train: 2018-08-02T13:24:50.251959: step 11237, loss 0.570497.
Train: 2018-08-02T13:24:50.392520: step 11238, loss 0.596374.
Train: 2018-08-02T13:24:50.548763: step 11239, loss 0.523801.
Train: 2018-08-02T13:24:50.704947: step 11240, loss 0.551476.
Test: 2018-08-02T13:24:51.173587: step 11240, loss 0.562744.
Train: 2018-08-02T13:24:51.329800: step 11241, loss 0.593788.
Train: 2018-08-02T13:24:51.486040: step 11242, loss 0.571737.
Train: 2018-08-02T13:24:51.626633: step 11243, loss 0.593178.
Train: 2018-08-02T13:24:51.782851: step 11244, loss 0.605646.
Train: 2018-08-02T13:24:51.939034: step 11245, loss 0.52845.
Train: 2018-08-02T13:24:52.095278: step 11246, loss 0.499752.
Train: 2018-08-02T13:24:52.251491: step 11247, loss 0.546769.
Train: 2018-08-02T13:24:52.423295: step 11248, loss 0.499944.
Train: 2018-08-02T13:24:52.579509: step 11249, loss 0.523057.
Train: 2018-08-02T13:24:52.735752: step 11250, loss 0.482426.
Test: 2018-08-02T13:24:53.188740: step 11250, loss 0.549812.
Train: 2018-08-02T13:24:53.344953: step 11251, loss 0.685471.
Train: 2018-08-02T13:24:53.501197: step 11252, loss 0.564896.
Train: 2018-08-02T13:24:53.641791: step 11253, loss 0.502926.
Train: 2018-08-02T13:24:53.798002: step 11254, loss 0.511255.
Train: 2018-08-02T13:24:53.954219: step 11255, loss 0.531925.
Train: 2018-08-02T13:24:54.110400: step 11256, loss 0.55204.
Train: 2018-08-02T13:24:54.266643: step 11257, loss 0.472322.
Train: 2018-08-02T13:24:54.422856: step 11258, loss 0.583155.
Train: 2018-08-02T13:24:54.579068: step 11259, loss 0.510673.
Train: 2018-08-02T13:24:54.735283: step 11260, loss 0.560654.
Test: 2018-08-02T13:24:55.188273: step 11260, loss 0.569657.
Train: 2018-08-02T13:24:55.344515: step 11261, loss 0.547392.
Train: 2018-08-02T13:24:55.500700: step 11262, loss 0.448323.
Train: 2018-08-02T13:24:55.656942: step 11263, loss 0.57428.
Train: 2018-08-02T13:24:55.813157: step 11264, loss 0.485127.
Train: 2018-08-02T13:24:55.969369: step 11265, loss 0.503967.
Train: 2018-08-02T13:24:56.125579: step 11266, loss 0.452386.
Train: 2018-08-02T13:24:56.281767: step 11267, loss 0.556513.
Train: 2018-08-02T13:24:56.437981: step 11268, loss 0.513996.
Train: 2018-08-02T13:24:56.594219: step 11269, loss 0.600673.
Train: 2018-08-02T13:24:56.750406: step 11270, loss 0.532122.
Test: 2018-08-02T13:24:57.203426: step 11270, loss 0.552901.
Train: 2018-08-02T13:24:57.375272: step 11271, loss 0.510276.
Train: 2018-08-02T13:24:57.531504: step 11272, loss 0.504698.
Train: 2018-08-02T13:24:57.687688: step 11273, loss 0.482117.
Train: 2018-08-02T13:24:57.843901: step 11274, loss 0.512732.
Train: 2018-08-02T13:24:58.000115: step 11275, loss 0.485626.
Train: 2018-08-02T13:24:58.156328: step 11276, loss 0.448903.
Train: 2018-08-02T13:24:58.296920: step 11277, loss 0.581989.
Train: 2018-08-02T13:24:58.468754: step 11278, loss 0.470625.
Train: 2018-08-02T13:24:58.624996: step 11279, loss 0.548277.
Train: 2018-08-02T13:24:58.781211: step 11280, loss 0.536269.
Test: 2018-08-02T13:24:59.234200: step 11280, loss 0.55028.
Train: 2018-08-02T13:24:59.390415: step 11281, loss 0.486453.
Train: 2018-08-02T13:24:59.546657: step 11282, loss 0.547866.
Train: 2018-08-02T13:24:59.702869: step 11283, loss 0.501125.
Train: 2018-08-02T13:24:59.859055: step 11284, loss 0.516037.
Train: 2018-08-02T13:25:00.015268: step 11285, loss 0.568223.
Train: 2018-08-02T13:25:00.171480: step 11286, loss 0.521889.
Train: 2018-08-02T13:25:00.327695: step 11287, loss 0.510916.
Train: 2018-08-02T13:25:00.499560: step 11288, loss 0.546053.
Train: 2018-08-02T13:25:00.655742: step 11289, loss 0.539971.
Train: 2018-08-02T13:25:00.811984: step 11290, loss 0.556171.
Test: 2018-08-02T13:25:01.264975: step 11290, loss 0.557552.
Train: 2018-08-02T13:25:01.421191: step 11291, loss 0.564062.
Train: 2018-08-02T13:25:01.577426: step 11292, loss 0.605752.
Train: 2018-08-02T13:25:01.733617: step 11293, loss 0.508781.
Train: 2018-08-02T13:25:01.889828: step 11294, loss 0.530452.
Train: 2018-08-02T13:25:02.046041: step 11295, loss 0.476306.
Train: 2018-08-02T13:25:02.202255: step 11296, loss 0.515656.
Train: 2018-08-02T13:25:02.358499: step 11297, loss 0.48009.
Train: 2018-08-02T13:25:02.514682: step 11298, loss 0.516174.
Train: 2018-08-02T13:25:02.670896: step 11299, loss 0.498073.
Train: 2018-08-02T13:25:02.827109: step 11300, loss 0.513628.
Test: 2018-08-02T13:25:03.295775: step 11300, loss 0.549744.
Train: 2018-08-02T13:25:03.873738: step 11301, loss 0.608847.
Train: 2018-08-02T13:25:04.029953: step 11302, loss 0.525177.
Train: 2018-08-02T13:25:04.201786: step 11303, loss 0.529974.
Train: 2018-08-02T13:25:04.358000: step 11304, loss 0.552631.
Train: 2018-08-02T13:25:04.514215: step 11305, loss 0.565139.
Train: 2018-08-02T13:25:04.670460: step 11306, loss 0.529908.
Train: 2018-08-02T13:25:04.826674: step 11307, loss 0.517559.
Train: 2018-08-02T13:25:04.982853: step 11308, loss 0.537646.
Train: 2018-08-02T13:25:05.139067: step 11309, loss 0.553163.
Train: 2018-08-02T13:25:05.295314: step 11310, loss 0.561001.
Test: 2018-08-02T13:25:05.763946: step 11310, loss 0.548719.
Train: 2018-08-02T13:25:05.920177: step 11311, loss 0.491463.
Train: 2018-08-02T13:25:06.076347: step 11312, loss 0.549231.
Train: 2018-08-02T13:25:06.232591: step 11313, loss 0.548353.
Train: 2018-08-02T13:25:06.388774: step 11314, loss 0.515604.
Train: 2018-08-02T13:25:06.545013: step 11315, loss 0.54489.
Train: 2018-08-02T13:25:06.701234: step 11316, loss 0.579197.
Train: 2018-08-02T13:25:06.857448: step 11317, loss 0.534438.
Train: 2018-08-02T13:25:06.998050: step 11318, loss 0.482443.
Train: 2018-08-02T13:25:07.154221: step 11319, loss 0.560397.
Train: 2018-08-02T13:25:07.310462: step 11320, loss 0.477104.
Test: 2018-08-02T13:25:07.779075: step 11320, loss 0.549036.
Train: 2018-08-02T13:25:07.935289: step 11321, loss 0.498089.
Train: 2018-08-02T13:25:08.091502: step 11322, loss 0.517733.
Train: 2018-08-02T13:25:08.263337: step 11323, loss 0.591665.
Train: 2018-08-02T13:25:08.419548: step 11324, loss 0.615936.
Train: 2018-08-02T13:25:08.528932: step 11325, loss 0.616843.
Train: 2018-08-02T13:25:08.685140: step 11326, loss 0.447668.
Train: 2018-08-02T13:25:08.841351: step 11327, loss 0.554902.
Train: 2018-08-02T13:25:08.997539: step 11328, loss 0.540926.
Train: 2018-08-02T13:25:09.153783: step 11329, loss 0.518983.
Train: 2018-08-02T13:25:09.309994: step 11330, loss 0.557487.
Test: 2018-08-02T13:25:09.778605: step 11330, loss 0.561168.
Train: 2018-08-02T13:25:09.934850: step 11331, loss 0.549879.
Train: 2018-08-02T13:25:10.091063: step 11332, loss 0.51472.
Train: 2018-08-02T13:25:10.247248: step 11333, loss 0.557198.
Train: 2018-08-02T13:25:10.403461: step 11334, loss 0.565136.
Train: 2018-08-02T13:25:10.559700: step 11335, loss 0.556731.
Train: 2018-08-02T13:25:10.715916: step 11336, loss 0.539629.
Train: 2018-08-02T13:25:10.872111: step 11337, loss 0.513211.
Train: 2018-08-02T13:25:11.028338: step 11338, loss 0.527522.
Train: 2018-08-02T13:25:11.184526: step 11339, loss 0.485225.
Train: 2018-08-02T13:25:11.340772: step 11340, loss 0.517077.
Test: 2018-08-02T13:25:11.793759: step 11340, loss 0.553125.
Train: 2018-08-02T13:25:11.950006: step 11341, loss 0.541678.
Train: 2018-08-02T13:25:12.106185: step 11342, loss 0.459194.
Train: 2018-08-02T13:25:12.278053: step 11343, loss 0.518454.
Train: 2018-08-02T13:25:12.434262: step 11344, loss 0.436176.
Train: 2018-08-02T13:25:12.590447: step 11345, loss 0.523196.
Train: 2018-08-02T13:25:12.746687: step 11346, loss 0.532776.
Train: 2018-08-02T13:25:12.902904: step 11347, loss 0.53491.
Train: 2018-08-02T13:25:13.059116: step 11348, loss 0.518315.
Train: 2018-08-02T13:25:13.199713: step 11349, loss 0.505399.
Train: 2018-08-02T13:25:13.371515: step 11350, loss 0.471024.
Test: 2018-08-02T13:25:13.824534: step 11350, loss 0.550158.
Train: 2018-08-02T13:25:13.980777: step 11351, loss 0.480754.
Train: 2018-08-02T13:25:14.136993: step 11352, loss 0.46188.
Train: 2018-08-02T13:25:14.277582: step 11353, loss 0.536038.
Train: 2018-08-02T13:25:14.433795: step 11354, loss 0.520351.
Train: 2018-08-02T13:25:14.590008: step 11355, loss 0.532369.
Train: 2018-08-02T13:25:14.746222: step 11356, loss 0.452447.
Train: 2018-08-02T13:25:14.902406: step 11357, loss 0.483393.
Train: 2018-08-02T13:25:15.058619: step 11358, loss 0.530474.
Train: 2018-08-02T13:25:15.214862: step 11359, loss 0.490554.
Train: 2018-08-02T13:25:15.371070: step 11360, loss 0.473392.
Test: 2018-08-02T13:25:15.824090: step 11360, loss 0.547563.
Train: 2018-08-02T13:25:15.980308: step 11361, loss 0.52764.
Train: 2018-08-02T13:25:16.136521: step 11362, loss 0.502244.
Train: 2018-08-02T13:25:16.292705: step 11363, loss 0.524814.
Train: 2018-08-02T13:25:16.448926: step 11364, loss 0.466466.
Train: 2018-08-02T13:25:16.605162: step 11365, loss 0.500545.
Train: 2018-08-02T13:25:16.761346: step 11366, loss 0.483152.
Train: 2018-08-02T13:25:16.917584: step 11367, loss 0.496167.
Train: 2018-08-02T13:25:17.073805: step 11368, loss 0.573901.
Train: 2018-08-02T13:25:17.230015: step 11369, loss 0.622764.
Train: 2018-08-02T13:25:17.386223: step 11370, loss 0.499554.
Test: 2018-08-02T13:25:17.854865: step 11370, loss 0.558406.
Train: 2018-08-02T13:25:18.011083: step 11371, loss 0.544587.
Train: 2018-08-02T13:25:18.167296: step 11372, loss 0.520993.
Train: 2018-08-02T13:25:18.323513: step 11373, loss 0.513424.
Train: 2018-08-02T13:25:18.479717: step 11374, loss 0.549303.
Train: 2018-08-02T13:25:18.620315: step 11375, loss 0.565836.
Train: 2018-08-02T13:25:18.792155: step 11376, loss 0.536818.
Train: 2018-08-02T13:25:18.948367: step 11377, loss 0.51471.
Train: 2018-08-02T13:25:19.104577: step 11378, loss 0.527607.
Train: 2018-08-02T13:25:19.260790: step 11379, loss 0.539262.
Train: 2018-08-02T13:25:19.417003: step 11380, loss 0.513912.
Test: 2018-08-02T13:25:19.869994: step 11380, loss 0.54952.
Train: 2018-08-02T13:25:20.026238: step 11381, loss 0.476552.
Train: 2018-08-02T13:25:20.198042: step 11382, loss 0.663479.
Train: 2018-08-02T13:25:20.354255: step 11383, loss 0.553125.
Train: 2018-08-02T13:25:20.510517: step 11384, loss 0.547428.
Train: 2018-08-02T13:25:20.666707: step 11385, loss 0.503065.
Train: 2018-08-02T13:25:20.822895: step 11386, loss 0.508758.
Train: 2018-08-02T13:25:20.979138: step 11387, loss 0.515965.
Train: 2018-08-02T13:25:21.135351: step 11388, loss 0.575845.
Train: 2018-08-02T13:25:21.291535: step 11389, loss 0.572827.
Train: 2018-08-02T13:25:21.447781: step 11390, loss 0.589658.
Test: 2018-08-02T13:25:21.916387: step 11390, loss 0.553151.
Train: 2018-08-02T13:25:22.057011: step 11391, loss 0.552676.
Train: 2018-08-02T13:25:22.213195: step 11392, loss 0.485209.
Train: 2018-08-02T13:25:22.385028: step 11393, loss 0.509194.
Train: 2018-08-02T13:25:22.541272: step 11394, loss 0.483549.
Train: 2018-08-02T13:25:22.697489: step 11395, loss 0.501918.
Train: 2018-08-02T13:25:22.853670: step 11396, loss 0.619235.
Train: 2018-08-02T13:25:23.009915: step 11397, loss 0.539642.
Train: 2018-08-02T13:25:23.166145: step 11398, loss 0.537132.
Train: 2018-08-02T13:25:23.322342: step 11399, loss 0.560873.
Train: 2018-08-02T13:25:23.478529: step 11400, loss 0.507414.
Test: 2018-08-02T13:25:23.931566: step 11400, loss 0.553577.
Train: 2018-08-02T13:25:24.650153: step 11401, loss 0.466048.
Train: 2018-08-02T13:25:24.806366: step 11402, loss 0.535705.
Train: 2018-08-02T13:25:24.962580: step 11403, loss 0.431043.
Train: 2018-08-02T13:25:25.118793: step 11404, loss 0.586711.
Train: 2018-08-02T13:25:25.275007: step 11405, loss 0.559918.
Train: 2018-08-02T13:25:25.431235: step 11406, loss 0.576852.
Train: 2018-08-02T13:25:25.587403: step 11407, loss 0.479658.
Train: 2018-08-02T13:25:25.731031: step 11408, loss 0.582055.
Train: 2018-08-02T13:25:25.887273: step 11409, loss 0.486242.
Train: 2018-08-02T13:25:26.043486: step 11410, loss 0.45261.
Test: 2018-08-02T13:25:26.512096: step 11410, loss 0.552034.
Train: 2018-08-02T13:25:26.668335: step 11411, loss 0.604545.
Train: 2018-08-02T13:25:26.808902: step 11412, loss 0.559987.
Train: 2018-08-02T13:25:26.965116: step 11413, loss 0.507576.
Train: 2018-08-02T13:25:27.121359: step 11414, loss 0.500874.
Train: 2018-08-02T13:25:27.277542: step 11415, loss 0.516323.
Train: 2018-08-02T13:25:27.433756: step 11416, loss 0.486368.
Train: 2018-08-02T13:25:27.589971: step 11417, loss 0.557782.
Train: 2018-08-02T13:25:27.746212: step 11418, loss 0.559769.
Train: 2018-08-02T13:25:27.902429: step 11419, loss 0.551304.
Train: 2018-08-02T13:25:28.058639: step 11420, loss 0.476229.
Test: 2018-08-02T13:25:28.511653: step 11420, loss 0.553936.
Train: 2018-08-02T13:25:28.667871: step 11421, loss 0.525722.
Train: 2018-08-02T13:25:28.824084: step 11422, loss 0.531044.
Train: 2018-08-02T13:25:28.980298: step 11423, loss 0.487499.
Train: 2018-08-02T13:25:29.136482: step 11424, loss 0.486921.
Train: 2018-08-02T13:25:29.277104: step 11425, loss 0.529261.
Train: 2018-08-02T13:25:29.433321: step 11426, loss 0.523881.
Train: 2018-08-02T13:25:29.589527: step 11427, loss 0.523395.
Train: 2018-08-02T13:25:29.745715: step 11428, loss 0.511304.
Train: 2018-08-02T13:25:29.901927: step 11429, loss 0.474024.
Train: 2018-08-02T13:25:30.058141: step 11430, loss 0.543869.
Test: 2018-08-02T13:25:30.511186: step 11430, loss 0.548066.
Train: 2018-08-02T13:25:30.667404: step 11431, loss 0.499805.
Train: 2018-08-02T13:25:30.823617: step 11432, loss 0.529735.
Train: 2018-08-02T13:25:30.979830: step 11433, loss 0.522784.
Train: 2018-08-02T13:25:31.136013: step 11434, loss 0.589781.
Train: 2018-08-02T13:25:31.292228: step 11435, loss 0.509801.
Train: 2018-08-02T13:25:31.448471: step 11436, loss 0.562593.
Train: 2018-08-02T13:25:31.604684: step 11437, loss 0.59691.
Train: 2018-08-02T13:25:31.760897: step 11438, loss 0.545802.
Train: 2018-08-02T13:25:31.917111: step 11439, loss 0.542432.
Train: 2018-08-02T13:25:32.073295: step 11440, loss 0.473651.
Test: 2018-08-02T13:25:32.541936: step 11440, loss 0.550532.
Train: 2018-08-02T13:25:32.698178: step 11441, loss 0.516484.
Train: 2018-08-02T13:25:32.854362: step 11442, loss 0.592021.
Train: 2018-08-02T13:25:33.010575: step 11443, loss 0.50946.
Train: 2018-08-02T13:25:33.166818: step 11444, loss 0.508218.
Train: 2018-08-02T13:25:33.323026: step 11445, loss 0.543936.
Train: 2018-08-02T13:25:33.479215: step 11446, loss 0.577552.
Train: 2018-08-02T13:25:33.619838: step 11447, loss 0.504641.
Train: 2018-08-02T13:25:33.791665: step 11448, loss 0.51253.
Train: 2018-08-02T13:25:33.932268: step 11449, loss 0.557046.
Train: 2018-08-02T13:25:34.088483: step 11450, loss 0.4842.
Test: 2018-08-02T13:25:34.541466: step 11450, loss 0.547991.
Train: 2018-08-02T13:25:34.697681: step 11451, loss 0.554383.
Train: 2018-08-02T13:25:34.853919: step 11452, loss 0.559283.
Train: 2018-08-02T13:25:35.010124: step 11453, loss 0.470668.
Train: 2018-08-02T13:25:35.166351: step 11454, loss 0.566048.
Train: 2018-08-02T13:25:35.322564: step 11455, loss 0.612593.
Train: 2018-08-02T13:25:35.478747: step 11456, loss 0.534184.
Train: 2018-08-02T13:25:35.634997: step 11457, loss 0.502081.
Train: 2018-08-02T13:25:35.791173: step 11458, loss 0.597247.
Train: 2018-08-02T13:25:35.947418: step 11459, loss 0.51989.
Train: 2018-08-02T13:25:36.103630: step 11460, loss 0.504018.
Test: 2018-08-02T13:25:36.556644: step 11460, loss 0.548442.
Train: 2018-08-02T13:25:36.712857: step 11461, loss 0.529616.
Train: 2018-08-02T13:25:36.869048: step 11462, loss 0.526808.
Train: 2018-08-02T13:25:37.025290: step 11463, loss 0.551668.
Train: 2018-08-02T13:25:37.181507: step 11464, loss 0.585184.
Train: 2018-08-02T13:25:37.322095: step 11465, loss 0.500242.
Train: 2018-08-02T13:25:37.478309: step 11466, loss 0.61389.
Train: 2018-08-02T13:25:37.634523: step 11467, loss 0.509432.
Train: 2018-08-02T13:25:37.790705: step 11468, loss 0.520057.
Train: 2018-08-02T13:25:37.931331: step 11469, loss 0.505779.
Train: 2018-08-02T13:25:38.087541: step 11470, loss 0.52616.
Test: 2018-08-02T13:25:38.556178: step 11470, loss 0.55157.
Train: 2018-08-02T13:25:38.696774: step 11471, loss 0.533055.
Train: 2018-08-02T13:25:38.853007: step 11472, loss 0.609286.
Train: 2018-08-02T13:25:39.009200: step 11473, loss 0.495926.
Train: 2018-08-02T13:25:39.165413: step 11474, loss 0.565366.
Train: 2018-08-02T13:25:39.321627: step 11475, loss 0.561952.
Train: 2018-08-02T13:25:39.430980: step 11476, loss 0.510018.
Train: 2018-08-02T13:25:39.587190: step 11477, loss 0.51468.
Train: 2018-08-02T13:25:39.743404: step 11478, loss 0.507209.
Train: 2018-08-02T13:25:39.899617: step 11479, loss 0.473621.
Train: 2018-08-02T13:25:40.055834: step 11480, loss 0.432741.
Test: 2018-08-02T13:25:40.508850: step 11480, loss 0.547079.
Train: 2018-08-02T13:25:40.665035: step 11481, loss 0.54583.
Train: 2018-08-02T13:25:40.822788: step 11482, loss 0.474274.
Train: 2018-08-02T13:25:40.979026: step 11483, loss 0.440791.
Train: 2018-08-02T13:25:41.135213: step 11484, loss 0.534142.
Train: 2018-08-02T13:25:41.291427: step 11485, loss 0.550781.
Train: 2018-08-02T13:25:41.447665: step 11486, loss 0.570855.
Train: 2018-08-02T13:25:41.603884: step 11487, loss 0.59554.
Train: 2018-08-02T13:25:41.760097: step 11488, loss 0.529951.
Train: 2018-08-02T13:25:41.916310: step 11489, loss 0.5069.
Train: 2018-08-02T13:25:42.072526: step 11490, loss 0.540043.
Test: 2018-08-02T13:25:42.525514: step 11490, loss 0.566458.
Train: 2018-08-02T13:25:42.681752: step 11491, loss 0.574261.
Train: 2018-08-02T13:25:42.837970: step 11492, loss 0.587523.
Train: 2018-08-02T13:25:42.994186: step 11493, loss 0.495588.
Train: 2018-08-02T13:25:43.150398: step 11494, loss 0.535512.
Train: 2018-08-02T13:25:43.306605: step 11495, loss 0.564354.
Train: 2018-08-02T13:25:43.462793: step 11496, loss 0.545553.
Train: 2018-08-02T13:25:43.603415: step 11497, loss 0.533038.
Train: 2018-08-02T13:25:43.775222: step 11498, loss 0.522154.
Train: 2018-08-02T13:25:43.931465: step 11499, loss 0.458289.
Train: 2018-08-02T13:25:44.087646: step 11500, loss 0.500233.
Test: 2018-08-02T13:25:44.540701: step 11500, loss 0.548541.
Train: 2018-08-02T13:25:45.228005: step 11501, loss 0.530465.
Train: 2018-08-02T13:25:45.399840: step 11502, loss 0.47857.
Train: 2018-08-02T13:25:45.540472: step 11503, loss 0.525912.
Train: 2018-08-02T13:25:45.712293: step 11504, loss 0.494545.
Train: 2018-08-02T13:25:45.852883: step 11505, loss 0.488227.
Train: 2018-08-02T13:25:46.009072: step 11506, loss 0.538909.
Train: 2018-08-02T13:25:46.165314: step 11507, loss 0.560107.
Train: 2018-08-02T13:25:46.305907: step 11508, loss 0.482552.
Train: 2018-08-02T13:25:46.462090: step 11509, loss 0.476087.
Train: 2018-08-02T13:25:46.618306: step 11510, loss 0.555814.
Test: 2018-08-02T13:25:47.071323: step 11510, loss 0.552013.
Train: 2018-08-02T13:25:47.243183: step 11511, loss 0.541635.
Train: 2018-08-02T13:25:47.383781: step 11512, loss 0.505418.
Train: 2018-08-02T13:25:47.539963: step 11513, loss 0.472607.
Train: 2018-08-02T13:25:47.696207: step 11514, loss 0.574688.
Train: 2018-08-02T13:25:47.868037: step 11515, loss 0.492261.
Train: 2018-08-02T13:25:48.008634: step 11516, loss 0.521679.
Train: 2018-08-02T13:25:48.164848: step 11517, loss 0.502403.
Train: 2018-08-02T13:25:48.321061: step 11518, loss 0.536699.
Train: 2018-08-02T13:25:48.477275: step 11519, loss 0.445227.
Train: 2018-08-02T13:25:48.633487: step 11520, loss 0.507742.
Test: 2018-08-02T13:25:49.102099: step 11520, loss 0.547192.
Train: 2018-08-02T13:25:49.258341: step 11521, loss 0.518771.
Train: 2018-08-02T13:25:49.414525: step 11522, loss 0.504958.
Train: 2018-08-02T13:25:49.570738: step 11523, loss 0.475299.
Train: 2018-08-02T13:25:49.726981: step 11524, loss 0.53069.
Train: 2018-08-02T13:25:49.867573: step 11525, loss 0.561472.
Train: 2018-08-02T13:25:50.023781: step 11526, loss 0.498909.
Train: 2018-08-02T13:25:50.179970: step 11527, loss 0.484887.
Train: 2018-08-02T13:25:50.336214: step 11528, loss 0.526135.
Train: 2018-08-02T13:25:50.492430: step 11529, loss 0.546915.
Train: 2018-08-02T13:25:50.632998: step 11530, loss 0.467336.
Test: 2018-08-02T13:25:51.086009: step 11530, loss 0.552464.
Train: 2018-08-02T13:25:51.242252: step 11531, loss 0.533982.
Train: 2018-08-02T13:25:51.398465: step 11532, loss 0.424214.
Train: 2018-08-02T13:25:51.554678: step 11533, loss 0.570202.
Train: 2018-08-02T13:25:51.710862: step 11534, loss 0.610335.
Train: 2018-08-02T13:25:51.851453: step 11535, loss 0.474186.
Train: 2018-08-02T13:25:52.007697: step 11536, loss 0.567418.
Train: 2018-08-02T13:25:52.163910: step 11537, loss 0.485422.
Train: 2018-08-02T13:25:52.320124: step 11538, loss 0.475793.
Train: 2018-08-02T13:25:52.476337: step 11539, loss 0.535323.
Train: 2018-08-02T13:25:52.616929: step 11540, loss 0.540369.
Test: 2018-08-02T13:25:53.085540: step 11540, loss 0.55092.
Train: 2018-08-02T13:25:53.226131: step 11541, loss 0.553169.
Train: 2018-08-02T13:25:53.382375: step 11542, loss 0.601892.
Train: 2018-08-02T13:25:53.538567: step 11543, loss 0.54072.
Train: 2018-08-02T13:25:53.694803: step 11544, loss 0.455734.
Train: 2018-08-02T13:25:53.851015: step 11545, loss 0.526073.
Train: 2018-08-02T13:25:53.991608: step 11546, loss 0.519915.
Train: 2018-08-02T13:25:54.147821: step 11547, loss 0.532348.
Train: 2018-08-02T13:25:54.304006: step 11548, loss 0.523319.
Train: 2018-08-02T13:25:54.460247: step 11549, loss 0.675814.
Train: 2018-08-02T13:25:54.616465: step 11550, loss 0.555037.
Test: 2018-08-02T13:25:55.069476: step 11550, loss 0.550449.
Train: 2018-08-02T13:25:55.225689: step 11551, loss 0.465789.
Train: 2018-08-02T13:25:55.381907: step 11552, loss 0.580116.
Train: 2018-08-02T13:25:55.522500: step 11553, loss 0.545972.
Train: 2018-08-02T13:25:55.678714: step 11554, loss 0.595088.
Train: 2018-08-02T13:25:55.834922: step 11555, loss 0.527043.
Train: 2018-08-02T13:25:55.991139: step 11556, loss 0.586173.
Train: 2018-08-02T13:25:56.147353: step 11557, loss 0.531263.
Train: 2018-08-02T13:25:56.287949: step 11558, loss 0.555222.
Train: 2018-08-02T13:25:56.444158: step 11559, loss 0.536968.
Train: 2018-08-02T13:25:56.600367: step 11560, loss 0.581329.
Test: 2018-08-02T13:25:57.053392: step 11560, loss 0.552254.
Train: 2018-08-02T13:25:57.209604: step 11561, loss 0.541001.
Train: 2018-08-02T13:25:57.365788: step 11562, loss 0.544704.
Train: 2018-08-02T13:25:57.522032: step 11563, loss 0.53431.
Train: 2018-08-02T13:25:57.662623: step 11564, loss 0.468155.
Train: 2018-08-02T13:25:57.818837: step 11565, loss 0.470552.
Train: 2018-08-02T13:25:57.975021: step 11566, loss 0.515244.
Train: 2018-08-02T13:25:58.131233: step 11567, loss 0.440826.
Train: 2018-08-02T13:25:58.287471: step 11568, loss 0.553273.
Train: 2018-08-02T13:25:58.443690: step 11569, loss 0.520725.
Train: 2018-08-02T13:25:58.599903: step 11570, loss 0.533708.
Test: 2018-08-02T13:25:59.052894: step 11570, loss 0.548735.
Train: 2018-08-02T13:25:59.209105: step 11571, loss 0.447885.
Train: 2018-08-02T13:25:59.365349: step 11572, loss 0.553031.
Train: 2018-08-02T13:25:59.505911: step 11573, loss 0.487869.
Train: 2018-08-02T13:25:59.662125: step 11574, loss 0.53307.
Train: 2018-08-02T13:25:59.818369: step 11575, loss 0.581491.
Train: 2018-08-02T13:25:59.974582: step 11576, loss 0.5547.
Train: 2018-08-02T13:26:00.130777: step 11577, loss 0.608529.
Train: 2018-08-02T13:26:00.287013: step 11578, loss 0.47425.
Train: 2018-08-02T13:26:00.427600: step 11579, loss 0.548582.
Train: 2018-08-02T13:26:00.583814: step 11580, loss 0.602176.
Test: 2018-08-02T13:26:01.052449: step 11580, loss 0.555348.
Train: 2018-08-02T13:26:01.208662: step 11581, loss 0.481741.
Train: 2018-08-02T13:26:01.349229: step 11582, loss 0.486761.
Train: 2018-08-02T13:26:01.505473: step 11583, loss 0.53013.
Train: 2018-08-02T13:26:01.661692: step 11584, loss 0.611245.
Train: 2018-08-02T13:26:01.817901: step 11585, loss 0.521974.
Train: 2018-08-02T13:26:01.974114: step 11586, loss 0.625518.
Train: 2018-08-02T13:26:02.130296: step 11587, loss 0.506215.
Train: 2018-08-02T13:26:02.286511: step 11588, loss 0.445523.
Train: 2018-08-02T13:26:02.427132: step 11589, loss 0.536301.
Train: 2018-08-02T13:26:02.583315: step 11590, loss 0.524408.
Test: 2018-08-02T13:26:03.051956: step 11590, loss 0.549663.
Train: 2018-08-02T13:26:03.192585: step 11591, loss 0.500369.
Train: 2018-08-02T13:26:03.348792: step 11592, loss 0.53187.
Train: 2018-08-02T13:26:03.505008: step 11593, loss 0.419162.
Train: 2018-08-02T13:26:03.661189: step 11594, loss 0.561441.
Train: 2018-08-02T13:26:03.817431: step 11595, loss 0.520741.
Train: 2018-08-02T13:26:03.958025: step 11596, loss 0.486592.
Train: 2018-08-02T13:26:04.114207: step 11597, loss 0.53407.
Train: 2018-08-02T13:26:04.270420: step 11598, loss 0.534943.
Train: 2018-08-02T13:26:04.426665: step 11599, loss 0.491318.
Train: 2018-08-02T13:26:04.582878: step 11600, loss 0.519376.
Test: 2018-08-02T13:26:05.051513: step 11600, loss 0.552095.
Train: 2018-08-02T13:26:05.676371: step 11601, loss 0.516807.
Train: 2018-08-02T13:26:05.832588: step 11602, loss 0.592705.
Train: 2018-08-02T13:26:05.988794: step 11603, loss 0.47441.
Train: 2018-08-02T13:26:06.129361: step 11604, loss 0.526446.
Train: 2018-08-02T13:26:06.285603: step 11605, loss 0.491202.
Train: 2018-08-02T13:26:06.441787: step 11606, loss 0.49341.
Train: 2018-08-02T13:26:06.598001: step 11607, loss 0.471103.
Train: 2018-08-02T13:26:06.754243: step 11608, loss 0.576633.
Train: 2018-08-02T13:26:06.894836: step 11609, loss 0.571165.
Train: 2018-08-02T13:26:07.051050: step 11610, loss 0.543134.
Test: 2018-08-02T13:26:07.504063: step 11610, loss 0.551296.
Train: 2018-08-02T13:26:07.660251: step 11611, loss 0.544378.
Train: 2018-08-02T13:26:07.816496: step 11612, loss 0.500467.
Train: 2018-08-02T13:26:07.988299: step 11613, loss 0.529654.
Train: 2018-08-02T13:26:08.144514: step 11614, loss 0.545751.
Train: 2018-08-02T13:26:08.300757: step 11615, loss 0.528565.
Train: 2018-08-02T13:26:08.456940: step 11616, loss 0.549946.
Train: 2018-08-02T13:26:08.613187: step 11617, loss 0.449332.
Train: 2018-08-02T13:26:08.769367: step 11618, loss 0.615129.
Train: 2018-08-02T13:26:08.925604: step 11619, loss 0.531545.
Train: 2018-08-02T13:26:09.066203: step 11620, loss 0.534724.
Test: 2018-08-02T13:26:09.534812: step 11620, loss 0.549425.
Train: 2018-08-02T13:26:09.675404: step 11621, loss 0.489935.
Train: 2018-08-02T13:26:09.831649: step 11622, loss 0.493699.
Train: 2018-08-02T13:26:09.987863: step 11623, loss 0.543486.
Train: 2018-08-02T13:26:10.144074: step 11624, loss 0.582826.
Train: 2018-08-02T13:26:10.300283: step 11625, loss 0.5273.
Train: 2018-08-02T13:26:10.472119: step 11626, loss 0.476242.
Train: 2018-08-02T13:26:10.581474: step 11627, loss 0.543095.
Train: 2018-08-02T13:26:10.737687: step 11628, loss 0.510657.
Train: 2018-08-02T13:26:10.893903: step 11629, loss 0.560302.
Train: 2018-08-02T13:26:11.050114: step 11630, loss 0.506669.
Test: 2018-08-02T13:26:11.503101: step 11630, loss 0.549315.
Train: 2018-08-02T13:26:11.659320: step 11631, loss 0.578876.
Train: 2018-08-02T13:26:11.815559: step 11632, loss 0.480035.
Train: 2018-08-02T13:26:11.971772: step 11633, loss 0.475046.
Train: 2018-08-02T13:26:12.127956: step 11634, loss 0.488339.
Train: 2018-08-02T13:26:12.284198: step 11635, loss 0.435363.
Train: 2018-08-02T13:26:12.424791: step 11636, loss 0.566243.
Train: 2018-08-02T13:26:12.581005: step 11637, loss 0.526134.
Train: 2018-08-02T13:26:12.737218: step 11638, loss 0.536662.
Train: 2018-08-02T13:26:12.893432: step 11639, loss 0.512299.
Train: 2018-08-02T13:26:13.034028: step 11640, loss 0.460683.
Test: 2018-08-02T13:26:13.502636: step 11640, loss 0.556562.
Train: 2018-08-02T13:26:13.658873: step 11641, loss 0.516668.
Train: 2018-08-02T13:26:13.815094: step 11642, loss 0.513814.
Train: 2018-08-02T13:26:13.971275: step 11643, loss 0.487583.
Train: 2018-08-02T13:26:14.127487: step 11644, loss 0.539326.
Train: 2018-08-02T13:26:14.283731: step 11645, loss 0.549851.
Train: 2018-08-02T13:26:14.439914: step 11646, loss 0.598238.
Train: 2018-08-02T13:26:14.596182: step 11647, loss 0.503227.
Train: 2018-08-02T13:26:14.752372: step 11648, loss 0.525489.
Train: 2018-08-02T13:26:14.908554: step 11649, loss 0.591555.
Train: 2018-08-02T13:26:15.064798: step 11650, loss 0.500957.
Test: 2018-08-02T13:26:15.533433: step 11650, loss 0.548277.
Train: 2018-08-02T13:26:15.689652: step 11651, loss 0.549707.
Train: 2018-08-02T13:26:15.830244: step 11652, loss 0.499513.
Train: 2018-08-02T13:26:15.986457: step 11653, loss 0.510852.
Train: 2018-08-02T13:26:16.142671: step 11654, loss 0.477401.
Train: 2018-08-02T13:26:16.298878: step 11655, loss 0.512487.
Train: 2018-08-02T13:26:16.455101: step 11656, loss 0.595029.
Train: 2018-08-02T13:26:16.611281: step 11657, loss 0.509764.
Train: 2018-08-02T13:26:16.767495: step 11658, loss 0.50847.
Train: 2018-08-02T13:26:16.923710: step 11659, loss 0.589219.
Train: 2018-08-02T13:26:17.079951: step 11660, loss 0.487123.
Test: 2018-08-02T13:26:17.548560: step 11660, loss 0.555585.
Train: 2018-08-02T13:26:17.689184: step 11661, loss 0.494098.
Train: 2018-08-02T13:26:17.845367: step 11662, loss 0.473217.
Train: 2018-08-02T13:26:18.001610: step 11663, loss 0.543866.
Train: 2018-08-02T13:26:18.157794: step 11664, loss 0.578473.
Train: 2018-08-02T13:26:18.298416: step 11665, loss 0.499945.
Train: 2018-08-02T13:26:18.454600: step 11666, loss 0.473491.
Train: 2018-08-02T13:26:18.610843: step 11667, loss 0.547844.
Train: 2018-08-02T13:26:18.767057: step 11668, loss 0.543139.
Train: 2018-08-02T13:26:18.923240: step 11669, loss 0.483719.
Train: 2018-08-02T13:26:19.079452: step 11670, loss 0.507397.
Test: 2018-08-02T13:26:19.548094: step 11670, loss 0.548396.
Train: 2018-08-02T13:26:19.704307: step 11671, loss 0.46998.
Train: 2018-08-02T13:26:19.860521: step 11672, loss 0.52675.
Train: 2018-08-02T13:26:20.016762: step 11673, loss 0.426635.
Train: 2018-08-02T13:26:20.172977: step 11674, loss 0.490007.
Train: 2018-08-02T13:26:20.329160: step 11675, loss 0.543364.
Train: 2018-08-02T13:26:20.485398: step 11676, loss 0.579226.
Train: 2018-08-02T13:26:20.641617: step 11677, loss 0.657913.
Train: 2018-08-02T13:26:20.782209: step 11678, loss 0.497777.
Train: 2018-08-02T13:26:20.938394: step 11679, loss 0.565998.
Train: 2018-08-02T13:26:21.094631: step 11680, loss 0.556858.
Test: 2018-08-02T13:26:21.547626: step 11680, loss 0.567528.
Train: 2018-08-02T13:26:21.703869: step 11681, loss 0.550765.
Train: 2018-08-02T13:26:21.860082: step 11682, loss 0.536793.
Train: 2018-08-02T13:26:22.016282: step 11683, loss 0.537508.
Train: 2018-08-02T13:26:22.172479: step 11684, loss 0.52301.
Train: 2018-08-02T13:26:22.328693: step 11685, loss 0.558664.
Train: 2018-08-02T13:26:22.484935: step 11686, loss 0.512785.
Train: 2018-08-02T13:26:22.641149: step 11687, loss 0.460565.
Train: 2018-08-02T13:26:22.797362: step 11688, loss 0.491666.
Train: 2018-08-02T13:26:22.953580: step 11689, loss 0.480418.
Train: 2018-08-02T13:26:23.109790: step 11690, loss 0.431529.
Test: 2018-08-02T13:26:23.562803: step 11690, loss 0.55361.
Train: 2018-08-02T13:26:23.719022: step 11691, loss 0.541748.
Train: 2018-08-02T13:26:23.890860: step 11692, loss 0.57166.
Train: 2018-08-02T13:26:24.031420: step 11693, loss 0.506693.
Train: 2018-08-02T13:26:24.187662: step 11694, loss 0.539977.
Train: 2018-08-02T13:26:24.343869: step 11695, loss 0.47285.
Train: 2018-08-02T13:26:24.500088: step 11696, loss 0.546433.
Train: 2018-08-02T13:26:24.656302: step 11697, loss 0.509323.
Train: 2018-08-02T13:26:24.812515: step 11698, loss 0.500534.
Train: 2018-08-02T13:26:24.968728: step 11699, loss 0.459438.
Train: 2018-08-02T13:26:25.124943: step 11700, loss 0.656984.
Test: 2018-08-02T13:26:25.577932: step 11700, loss 0.547823.
Train: 2018-08-02T13:26:26.249650: step 11701, loss 0.616439.
Train: 2018-08-02T13:26:26.405862: step 11702, loss 0.513226.
Train: 2018-08-02T13:26:26.562106: step 11703, loss 0.507635.
Train: 2018-08-02T13:26:26.718315: step 11704, loss 0.541305.
Train: 2018-08-02T13:26:26.874504: step 11705, loss 0.558822.
Train: 2018-08-02T13:26:27.030745: step 11706, loss 0.514945.
Train: 2018-08-02T13:26:27.186959: step 11707, loss 0.553543.
Train: 2018-08-02T13:26:27.343173: step 11708, loss 0.511199.
Train: 2018-08-02T13:26:27.499358: step 11709, loss 0.541031.
Train: 2018-08-02T13:26:27.655599: step 11710, loss 0.528511.
Test: 2018-08-02T13:26:28.108588: step 11710, loss 0.54992.
Train: 2018-08-02T13:26:28.264832: step 11711, loss 0.493387.
Train: 2018-08-02T13:26:28.421045: step 11712, loss 0.509734.
Train: 2018-08-02T13:26:28.577276: step 11713, loss 0.439913.
Train: 2018-08-02T13:26:28.733472: step 11714, loss 0.44238.
Train: 2018-08-02T13:26:28.889655: step 11715, loss 0.577949.
Train: 2018-08-02T13:26:29.045899: step 11716, loss 0.60605.
Train: 2018-08-02T13:26:29.202108: step 11717, loss 0.463105.
Train: 2018-08-02T13:26:29.358326: step 11718, loss 0.504472.
Train: 2018-08-02T13:26:29.530155: step 11719, loss 0.63286.
Train: 2018-08-02T13:26:29.670753: step 11720, loss 0.566084.
Test: 2018-08-02T13:26:30.139390: step 11720, loss 0.564808.
Train: 2018-08-02T13:26:30.279954: step 11721, loss 0.552164.
Train: 2018-08-02T13:26:30.436199: step 11722, loss 0.550222.
Train: 2018-08-02T13:26:30.592413: step 11723, loss 0.530803.
Train: 2018-08-02T13:26:30.748625: step 11724, loss 0.473674.
Train: 2018-08-02T13:26:30.904838: step 11725, loss 0.51316.
Train: 2018-08-02T13:26:31.061021: step 11726, loss 0.501766.
Train: 2018-08-02T13:26:31.201643: step 11727, loss 0.537721.
Train: 2018-08-02T13:26:31.373475: step 11728, loss 0.546151.
Train: 2018-08-02T13:26:31.529664: step 11729, loss 0.506492.
Train: 2018-08-02T13:26:31.685885: step 11730, loss 0.548252.
Test: 2018-08-02T13:26:32.138894: step 11730, loss 0.554085.
Train: 2018-08-02T13:26:32.295109: step 11731, loss 0.501456.
Train: 2018-08-02T13:26:32.451354: step 11732, loss 0.522224.
Train: 2018-08-02T13:26:32.607566: step 11733, loss 0.549925.
Train: 2018-08-02T13:26:32.763781: step 11734, loss 0.531243.
Train: 2018-08-02T13:26:32.919992: step 11735, loss 0.516642.
Train: 2018-08-02T13:26:33.076205: step 11736, loss 0.493329.
Train: 2018-08-02T13:26:33.232389: step 11737, loss 0.507461.
Train: 2018-08-02T13:26:33.388603: step 11738, loss 0.525741.
Train: 2018-08-02T13:26:33.544845: step 11739, loss 0.533532.
Train: 2018-08-02T13:26:33.701042: step 11740, loss 0.509368.
Test: 2018-08-02T13:26:34.154072: step 11740, loss 0.550036.
Train: 2018-08-02T13:26:34.310287: step 11741, loss 0.498992.
Train: 2018-08-02T13:26:34.513340: step 11742, loss 0.449272.
Train: 2018-08-02T13:26:34.669552: step 11743, loss 0.540097.
Train: 2018-08-02T13:26:34.825765: step 11744, loss 0.560132.
Train: 2018-08-02T13:26:34.982010: step 11745, loss 0.486595.
Train: 2018-08-02T13:26:35.138192: step 11746, loss 0.552288.
Train: 2018-08-02T13:26:35.294406: step 11747, loss 0.579518.
Train: 2018-08-02T13:26:35.450644: step 11748, loss 0.517622.
Train: 2018-08-02T13:26:35.606832: step 11749, loss 0.571348.
Train: 2018-08-02T13:26:35.763074: step 11750, loss 0.538683.
Test: 2018-08-02T13:26:36.216066: step 11750, loss 0.557627.
Train: 2018-08-02T13:26:36.372279: step 11751, loss 0.528472.
Train: 2018-08-02T13:26:36.528522: step 11752, loss 0.598007.
Train: 2018-08-02T13:26:36.684711: step 11753, loss 0.585605.
Train: 2018-08-02T13:26:36.856569: step 11754, loss 0.531655.
Train: 2018-08-02T13:26:37.012790: step 11755, loss 0.509657.
Train: 2018-08-02T13:26:37.168967: step 11756, loss 0.581875.
Train: 2018-08-02T13:26:37.325211: step 11757, loss 0.570128.
Train: 2018-08-02T13:26:37.465802: step 11758, loss 0.552049.
Train: 2018-08-02T13:26:37.622015: step 11759, loss 0.549134.
Train: 2018-08-02T13:26:37.778199: step 11760, loss 0.595845.
Test: 2018-08-02T13:26:38.231221: step 11760, loss 0.560789.
Train: 2018-08-02T13:26:38.387457: step 11761, loss 0.483869.
Train: 2018-08-02T13:26:38.543645: step 11762, loss 0.498211.
Train: 2018-08-02T13:26:38.699892: step 11763, loss 0.607384.
Train: 2018-08-02T13:26:38.856096: step 11764, loss 0.530318.
Train: 2018-08-02T13:26:39.012313: step 11765, loss 0.553736.
Train: 2018-08-02T13:26:39.168498: step 11766, loss 0.458297.
Train: 2018-08-02T13:26:39.324711: step 11767, loss 0.530515.
Train: 2018-08-02T13:26:39.480956: step 11768, loss 0.481119.
Train: 2018-08-02T13:26:39.652785: step 11769, loss 0.489588.
Train: 2018-08-02T13:26:39.793378: step 11770, loss 0.489271.
Test: 2018-08-02T13:26:40.262021: step 11770, loss 0.547339.
Train: 2018-08-02T13:26:40.418232: step 11771, loss 0.612146.
Train: 2018-08-02T13:26:40.574450: step 11772, loss 0.517783.
Train: 2018-08-02T13:26:40.730663: step 11773, loss 0.467715.
Train: 2018-08-02T13:26:40.871255: step 11774, loss 0.471656.
Train: 2018-08-02T13:26:41.027467: step 11775, loss 0.59979.
Train: 2018-08-02T13:26:41.183682: step 11776, loss 0.501677.
Train: 2018-08-02T13:26:41.339895: step 11777, loss 0.52921.
Train: 2018-08-02T13:26:41.449244: step 11778, loss 0.532477.
Train: 2018-08-02T13:26:41.605459: step 11779, loss 0.542405.
Train: 2018-08-02T13:26:41.761640: step 11780, loss 0.542891.
Test: 2018-08-02T13:26:42.214685: step 11780, loss 0.548833.
Train: 2018-08-02T13:26:42.370874: step 11781, loss 0.495796.
Train: 2018-08-02T13:26:42.527117: step 11782, loss 0.615297.
Train: 2018-08-02T13:26:42.683325: step 11783, loss 0.530708.
Train: 2018-08-02T13:26:42.839515: step 11784, loss 0.512317.
Train: 2018-08-02T13:26:43.011378: step 11785, loss 0.560253.
Train: 2018-08-02T13:26:43.167590: step 11786, loss 0.468581.
Train: 2018-08-02T13:26:43.308184: step 11787, loss 0.477139.
Train: 2018-08-02T13:26:43.479989: step 11788, loss 0.49897.
Train: 2018-08-02T13:26:43.620581: step 11789, loss 0.593451.
Train: 2018-08-02T13:26:43.776794: step 11790, loss 0.411466.
Test: 2018-08-02T13:26:44.229814: step 11790, loss 0.551538.
Train: 2018-08-02T13:26:44.386027: step 11791, loss 0.439983.
Train: 2018-08-02T13:26:44.542270: step 11792, loss 0.533044.
Train: 2018-08-02T13:26:44.682863: step 11793, loss 0.573796.
Train: 2018-08-02T13:26:44.854698: step 11794, loss 0.484961.
Train: 2018-08-02T13:26:45.010894: step 11795, loss 0.545047.
Train: 2018-08-02T13:26:45.151502: step 11796, loss 0.48475.
Train: 2018-08-02T13:26:45.323306: step 11797, loss 0.566826.
Train: 2018-08-02T13:26:45.479551: step 11798, loss 0.602482.
Train: 2018-08-02T13:26:45.635735: step 11799, loss 0.509862.
Train: 2018-08-02T13:26:45.791947: step 11800, loss 0.483293.
Test: 2018-08-02T13:26:46.260616: step 11800, loss 0.551563.
Train: 2018-08-02T13:26:46.885471: step 11801, loss 0.506724.
Train: 2018-08-02T13:26:47.041690: step 11802, loss 0.419249.
Train: 2018-08-02T13:26:47.182248: step 11803, loss 0.497806.
Train: 2018-08-02T13:26:47.338491: step 11804, loss 0.520828.
Train: 2018-08-02T13:26:47.494703: step 11805, loss 0.421752.
Train: 2018-08-02T13:26:47.650917: step 11806, loss 0.542312.
Train: 2018-08-02T13:26:47.807101: step 11807, loss 0.486248.
Train: 2018-08-02T13:26:47.963339: step 11808, loss 0.549989.
Train: 2018-08-02T13:26:48.119559: step 11809, loss 0.509028.
Train: 2018-08-02T13:26:48.275742: step 11810, loss 0.492422.
Test: 2018-08-02T13:26:48.744414: step 11810, loss 0.550873.
Train: 2018-08-02T13:26:48.978727: step 11811, loss 0.51327.
Train: 2018-08-02T13:26:49.134947: step 11812, loss 0.465621.
Train: 2018-08-02T13:26:49.291159: step 11813, loss 0.587057.
Train: 2018-08-02T13:26:49.447371: step 11814, loss 0.461792.
Train: 2018-08-02T13:26:49.603556: step 11815, loss 0.582952.
Train: 2018-08-02T13:26:49.759799: step 11816, loss 0.543116.
Train: 2018-08-02T13:26:49.915982: step 11817, loss 0.486912.
Train: 2018-08-02T13:26:50.072225: step 11818, loss 0.553243.
Train: 2018-08-02T13:26:50.228410: step 11819, loss 0.509928.
Train: 2018-08-02T13:26:50.369002: step 11820, loss 0.486346.
Test: 2018-08-02T13:26:50.837642: step 11820, loss 0.552492.
Train: 2018-08-02T13:26:50.993854: step 11821, loss 0.513273.
Train: 2018-08-02T13:26:51.150098: step 11822, loss 0.537991.
Train: 2018-08-02T13:26:51.306282: step 11823, loss 0.576934.
Train: 2018-08-02T13:26:51.446872: step 11824, loss 0.517512.
Train: 2018-08-02T13:26:51.603117: step 11825, loss 0.467083.
Train: 2018-08-02T13:26:51.759330: step 11826, loss 0.464462.
Train: 2018-08-02T13:26:51.915543: step 11827, loss 0.495226.
Train: 2018-08-02T13:26:52.071757: step 11828, loss 0.483491.
Train: 2018-08-02T13:26:52.212349: step 11829, loss 0.542899.
Train: 2018-08-02T13:26:52.368562: step 11830, loss 0.536978.
Test: 2018-08-02T13:26:52.821551: step 11830, loss 0.549003.
Train: 2018-08-02T13:26:52.977795: step 11831, loss 0.528549.
Train: 2018-08-02T13:26:53.149629: step 11832, loss 0.543244.
Train: 2018-08-02T13:26:53.305844: step 11833, loss 0.530268.
Train: 2018-08-02T13:26:53.462026: step 11834, loss 0.49491.
Train: 2018-08-02T13:26:53.618241: step 11835, loss 0.509987.
Train: 2018-08-02T13:26:53.774453: step 11836, loss 0.517844.
Train: 2018-08-02T13:26:53.930667: step 11837, loss 0.423706.
Train: 2018-08-02T13:26:54.086904: step 11838, loss 0.536194.
Train: 2018-08-02T13:26:54.243093: step 11839, loss 0.568288.
Train: 2018-08-02T13:26:54.399307: step 11840, loss 0.549156.
Test: 2018-08-02T13:26:54.852352: step 11840, loss 0.547567.
Train: 2018-08-02T13:26:55.008569: step 11841, loss 0.437658.
Train: 2018-08-02T13:26:55.164783: step 11842, loss 0.477713.
Train: 2018-08-02T13:26:55.320996: step 11843, loss 0.471883.
Train: 2018-08-02T13:26:55.461589: step 11844, loss 0.49795.
Train: 2018-08-02T13:26:55.617803: step 11845, loss 0.521947.
Train: 2018-08-02T13:26:55.789607: step 11846, loss 0.5226.
Train: 2018-08-02T13:26:55.945851: step 11847, loss 0.538911.
Train: 2018-08-02T13:26:56.102033: step 11848, loss 0.548407.
Train: 2018-08-02T13:26:56.258248: step 11849, loss 0.502708.
Train: 2018-08-02T13:26:56.414461: step 11850, loss 0.554446.
Test: 2018-08-02T13:26:56.867480: step 11850, loss 0.551348.
Train: 2018-08-02T13:26:57.023732: step 11851, loss 0.569939.
Train: 2018-08-02T13:26:57.179936: step 11852, loss 0.488679.
Train: 2018-08-02T13:26:57.336150: step 11853, loss 0.466212.
Train: 2018-08-02T13:26:57.492357: step 11854, loss 0.509557.
Train: 2018-08-02T13:26:57.664193: step 11855, loss 0.614427.
Train: 2018-08-02T13:26:57.804791: step 11856, loss 0.604009.
Train: 2018-08-02T13:26:57.976619: step 11857, loss 0.590611.
Train: 2018-08-02T13:26:58.117216: step 11858, loss 0.540281.
Train: 2018-08-02T13:26:58.289046: step 11859, loss 0.527666.
Train: 2018-08-02T13:26:58.429643: step 11860, loss 0.528037.
Test: 2018-08-02T13:26:58.882657: step 11860, loss 0.558466.
Train: 2018-08-02T13:26:59.038876: step 11861, loss 0.566174.
Train: 2018-08-02T13:26:59.210714: step 11862, loss 0.507573.
Train: 2018-08-02T13:26:59.366924: step 11863, loss 0.581234.
Train: 2018-08-02T13:26:59.523107: step 11864, loss 0.468221.
Train: 2018-08-02T13:26:59.679330: step 11865, loss 0.538737.
Train: 2018-08-02T13:26:59.835533: step 11866, loss 0.515662.
Train: 2018-08-02T13:26:59.991778: step 11867, loss 0.588378.
Train: 2018-08-02T13:27:00.147991: step 11868, loss 0.523212.
Train: 2018-08-02T13:27:00.304175: step 11869, loss 0.528957.
Train: 2018-08-02T13:27:00.460419: step 11870, loss 0.548509.
Test: 2018-08-02T13:27:00.929029: step 11870, loss 0.550847.
Train: 2018-08-02T13:27:01.085272: step 11871, loss 0.547854.
Train: 2018-08-02T13:27:01.225834: step 11872, loss 0.51301.
Train: 2018-08-02T13:27:01.382077: step 11873, loss 0.508961.
Train: 2018-08-02T13:27:01.538290: step 11874, loss 0.514085.
Train: 2018-08-02T13:27:01.694506: step 11875, loss 0.560786.
Train: 2018-08-02T13:27:01.850695: step 11876, loss 0.534592.
Train: 2018-08-02T13:27:02.006935: step 11877, loss 0.479189.
Train: 2018-08-02T13:27:02.163144: step 11878, loss 0.602746.
Train: 2018-08-02T13:27:02.334975: step 11879, loss 0.480207.
Train: 2018-08-02T13:27:02.491192: step 11880, loss 0.515139.
Test: 2018-08-02T13:27:02.944212: step 11880, loss 0.551355.
Train: 2018-08-02T13:27:03.100424: step 11881, loss 0.525632.
Train: 2018-08-02T13:27:03.256636: step 11882, loss 0.549944.
Train: 2018-08-02T13:27:03.412852: step 11883, loss 0.570237.
Train: 2018-08-02T13:27:03.569061: step 11884, loss 0.529387.
Train: 2018-08-02T13:27:03.725248: step 11885, loss 0.556905.
Train: 2018-08-02T13:27:03.881492: step 11886, loss 0.531285.
Train: 2018-08-02T13:27:04.037674: step 11887, loss 0.539248.
Train: 2018-08-02T13:27:04.193888: step 11888, loss 0.560154.
Train: 2018-08-02T13:27:04.350127: step 11889, loss 0.607446.
Train: 2018-08-02T13:27:04.506345: step 11890, loss 0.576202.
Test: 2018-08-02T13:27:04.959333: step 11890, loss 0.554442.
Train: 2018-08-02T13:27:05.115578: step 11891, loss 0.580943.
Train: 2018-08-02T13:27:05.271796: step 11892, loss 0.551745.
Train: 2018-08-02T13:27:05.427999: step 11893, loss 0.503087.
Train: 2018-08-02T13:27:05.584218: step 11894, loss 0.576636.
Train: 2018-08-02T13:27:05.740431: step 11895, loss 0.50794.
Train: 2018-08-02T13:27:05.896644: step 11896, loss 0.486453.
Train: 2018-08-02T13:27:06.052860: step 11897, loss 0.507331.
Train: 2018-08-02T13:27:06.209073: step 11898, loss 0.447472.
Train: 2018-08-02T13:27:06.365290: step 11899, loss 0.482767.
Train: 2018-08-02T13:27:06.521502: step 11900, loss 0.571356.
Test: 2018-08-02T13:27:06.974513: step 11900, loss 0.546913.
Train: 2018-08-02T13:27:07.599370: step 11901, loss 0.491017.
Train: 2018-08-02T13:27:07.755553: step 11902, loss 0.44533.
Train: 2018-08-02T13:27:07.896145: step 11903, loss 0.537042.
Train: 2018-08-02T13:27:08.068012: step 11904, loss 0.510202.
Train: 2018-08-02T13:27:08.224220: step 11905, loss 0.454964.
Train: 2018-08-02T13:27:08.380443: step 11906, loss 0.612931.
Train: 2018-08-02T13:27:08.536620: step 11907, loss 0.540227.
Train: 2018-08-02T13:27:08.692865: step 11908, loss 0.466898.
Train: 2018-08-02T13:27:08.849079: step 11909, loss 0.535155.
Train: 2018-08-02T13:27:09.005292: step 11910, loss 0.527933.
Test: 2018-08-02T13:27:09.458312: step 11910, loss 0.548557.
Train: 2018-08-02T13:27:09.614524: step 11911, loss 0.52165.
Train: 2018-08-02T13:27:09.770742: step 11912, loss 0.579997.
Train: 2018-08-02T13:27:09.926951: step 11913, loss 0.488818.
Train: 2018-08-02T13:27:10.083164: step 11914, loss 0.498833.
Train: 2018-08-02T13:27:10.239378: step 11915, loss 0.522904.
Train: 2018-08-02T13:27:10.395590: step 11916, loss 0.511062.
Train: 2018-08-02T13:27:10.536183: step 11917, loss 0.529343.
Train: 2018-08-02T13:27:10.692366: step 11918, loss 0.524166.
Train: 2018-08-02T13:27:10.848609: step 11919, loss 0.441079.
Train: 2018-08-02T13:27:11.004826: step 11920, loss 0.489528.
Test: 2018-08-02T13:27:11.457843: step 11920, loss 0.54824.
Train: 2018-08-02T13:27:11.614057: step 11921, loss 0.549794.
Train: 2018-08-02T13:27:11.770240: step 11922, loss 0.556588.
Train: 2018-08-02T13:27:11.926455: step 11923, loss 0.553792.
Train: 2018-08-02T13:27:12.082666: step 11924, loss 0.481437.
Train: 2018-08-02T13:27:12.254526: step 11925, loss 0.471643.
Train: 2018-08-02T13:27:12.410714: step 11926, loss 0.44908.
Train: 2018-08-02T13:27:12.566958: step 11927, loss 0.53131.
Train: 2018-08-02T13:27:12.723172: step 11928, loss 0.525029.
Train: 2018-08-02T13:27:12.848112: step 11929, loss 0.574389.
Train: 2018-08-02T13:27:13.004374: step 11930, loss 0.434968.
Test: 2018-08-02T13:27:13.457370: step 11930, loss 0.546894.
Train: 2018-08-02T13:27:13.613587: step 11931, loss 0.506049.
Train: 2018-08-02T13:27:13.769800: step 11932, loss 0.458971.
Train: 2018-08-02T13:27:13.926014: step 11933, loss 0.510367.
Train: 2018-08-02T13:27:14.066576: step 11934, loss 0.429144.
Train: 2018-08-02T13:27:14.222789: step 11935, loss 0.493401.
Train: 2018-08-02T13:27:14.379053: step 11936, loss 0.602612.
Train: 2018-08-02T13:27:14.535241: step 11937, loss 0.497648.
Train: 2018-08-02T13:27:14.691461: step 11938, loss 0.49503.
Train: 2018-08-02T13:27:14.863289: step 11939, loss 0.632161.
Train: 2018-08-02T13:27:15.019508: step 11940, loss 0.530979.
Test: 2018-08-02T13:27:15.472522: step 11940, loss 0.554368.
Train: 2018-08-02T13:27:15.628736: step 11941, loss 0.531892.
Train: 2018-08-02T13:27:15.784954: step 11942, loss 0.509431.
Train: 2018-08-02T13:27:15.956782: step 11943, loss 0.52154.
Train: 2018-08-02T13:27:16.097381: step 11944, loss 0.467682.
Train: 2018-08-02T13:27:16.253564: step 11945, loss 0.532907.
Train: 2018-08-02T13:27:16.409808: step 11946, loss 0.569849.
Train: 2018-08-02T13:27:16.581615: step 11947, loss 0.513.
Train: 2018-08-02T13:27:16.722205: step 11948, loss 0.497043.
Train: 2018-08-02T13:27:16.878448: step 11949, loss 0.520165.
Train: 2018-08-02T13:27:17.034661: step 11950, loss 0.456709.
Test: 2018-08-02T13:27:17.503271: step 11950, loss 0.54715.
Train: 2018-08-02T13:27:17.659486: step 11951, loss 0.510608.
Train: 2018-08-02T13:27:17.815700: step 11952, loss 0.554539.
Train: 2018-08-02T13:27:17.971942: step 11953, loss 0.527455.
Train: 2018-08-02T13:27:18.128155: step 11954, loss 0.546309.
Train: 2018-08-02T13:27:18.284340: step 11955, loss 0.521531.
Train: 2018-08-02T13:27:18.440553: step 11956, loss 0.494091.
Train: 2018-08-02T13:27:18.596767: step 11957, loss 0.53018.
Train: 2018-08-02T13:27:18.753009: step 11958, loss 0.562336.
Train: 2018-08-02T13:27:18.909223: step 11959, loss 0.589981.
Train: 2018-08-02T13:27:19.065431: step 11960, loss 0.519087.
Test: 2018-08-02T13:27:19.534046: step 11960, loss 0.549476.
Train: 2018-08-02T13:27:19.674638: step 11961, loss 0.519791.
Train: 2018-08-02T13:27:19.830882: step 11962, loss 0.546759.
Train: 2018-08-02T13:27:19.987065: step 11963, loss 0.618811.
Train: 2018-08-02T13:27:20.143314: step 11964, loss 0.60347.
Train: 2018-08-02T13:27:20.299522: step 11965, loss 0.494936.
Train: 2018-08-02T13:27:20.455736: step 11966, loss 0.504943.
Train: 2018-08-02T13:27:20.611945: step 11967, loss 0.520205.
Train: 2018-08-02T13:27:20.768162: step 11968, loss 0.467339.
Train: 2018-08-02T13:27:20.939993: step 11969, loss 0.479363.
Train: 2018-08-02T13:27:21.096181: step 11970, loss 0.57417.
Test: 2018-08-02T13:27:21.549224: step 11970, loss 0.552086.
Train: 2018-08-02T13:27:21.705438: step 11971, loss 0.616551.
Train: 2018-08-02T13:27:21.861627: step 11972, loss 0.496981.
Train: 2018-08-02T13:27:22.017869: step 11973, loss 0.552792.
Train: 2018-08-02T13:27:22.174052: step 11974, loss 0.544274.
Train: 2018-08-02T13:27:22.330266: step 11975, loss 0.520682.
Train: 2018-08-02T13:27:22.486511: step 11976, loss 0.4976.
Train: 2018-08-02T13:27:22.627101: step 11977, loss 0.532615.
Train: 2018-08-02T13:27:22.783285: step 11978, loss 0.498696.
Train: 2018-08-02T13:27:22.939524: step 11979, loss 0.415975.
Train: 2018-08-02T13:27:23.095712: step 11980, loss 0.503546.
Test: 2018-08-02T13:27:23.564352: step 11980, loss 0.567796.
Train: 2018-08-02T13:27:23.704974: step 11981, loss 0.466862.
Train: 2018-08-02T13:27:23.876810: step 11982, loss 0.539029.
Train: 2018-08-02T13:27:24.033004: step 11983, loss 0.613638.
Train: 2018-08-02T13:27:24.189232: step 11984, loss 0.533543.
Train: 2018-08-02T13:27:24.345423: step 11985, loss 0.470213.
Train: 2018-08-02T13:27:24.501664: step 11986, loss 0.506076.
Train: 2018-08-02T13:27:24.657877: step 11987, loss 0.477609.
Train: 2018-08-02T13:27:24.814090: step 11988, loss 0.461933.
Train: 2018-08-02T13:27:24.970303: step 11989, loss 0.517721.
Train: 2018-08-02T13:27:25.110895: step 11990, loss 0.60247.
Test: 2018-08-02T13:27:25.579505: step 11990, loss 0.550702.
Train: 2018-08-02T13:27:25.720128: step 11991, loss 0.509899.
Train: 2018-08-02T13:27:25.876341: step 11992, loss 0.602753.
Train: 2018-08-02T13:27:26.032524: step 11993, loss 0.465587.
Train: 2018-08-02T13:27:26.188768: step 11994, loss 0.573024.
Train: 2018-08-02T13:27:26.344951: step 11995, loss 0.515752.
Train: 2018-08-02T13:27:26.501175: step 11996, loss 0.519536.
Train: 2018-08-02T13:27:26.641786: step 11997, loss 0.527155.
Train: 2018-08-02T13:27:26.798001: step 11998, loss 0.503268.
Train: 2018-08-02T13:27:26.954182: step 11999, loss 0.610515.
Train: 2018-08-02T13:27:27.110430: step 12000, loss 0.53708.
Test: 2018-08-02T13:27:27.563415: step 12000, loss 0.54727.
Train: 2018-08-02T13:27:28.203891: step 12001, loss 0.54086.
Train: 2018-08-02T13:27:28.360135: step 12002, loss 0.546685.
Train: 2018-08-02T13:27:28.516317: step 12003, loss 0.541074.
Train: 2018-08-02T13:27:28.672531: step 12004, loss 0.510551.
Train: 2018-08-02T13:27:28.828799: step 12005, loss 0.515983.
Train: 2018-08-02T13:27:28.984987: step 12006, loss 0.531299.
Train: 2018-08-02T13:27:29.141215: step 12007, loss 0.446762.
Train: 2018-08-02T13:27:29.297420: step 12008, loss 0.577267.
Train: 2018-08-02T13:27:29.453628: step 12009, loss 0.45591.
Train: 2018-08-02T13:27:29.609812: step 12010, loss 0.47425.
Test: 2018-08-02T13:27:30.062830: step 12010, loss 0.550668.
Train: 2018-08-02T13:27:30.219074: step 12011, loss 0.536849.
Train: 2018-08-02T13:27:30.375289: step 12012, loss 0.603853.
Train: 2018-08-02T13:27:30.531501: step 12013, loss 0.528799.
Train: 2018-08-02T13:27:30.687683: step 12014, loss 0.513399.
Train: 2018-08-02T13:27:30.843928: step 12015, loss 0.477619.
Train: 2018-08-02T13:27:31.000141: step 12016, loss 0.555792.
Train: 2018-08-02T13:27:31.156355: step 12017, loss 0.474177.
Train: 2018-08-02T13:27:31.312537: step 12018, loss 0.625455.
Train: 2018-08-02T13:27:31.468753: step 12019, loss 0.430434.
Train: 2018-08-02T13:27:31.624993: step 12020, loss 0.56262.
Test: 2018-08-02T13:27:32.093606: step 12020, loss 0.546953.
Train: 2018-08-02T13:27:32.234197: step 12021, loss 0.536136.
Train: 2018-08-02T13:27:32.406031: step 12022, loss 0.484294.
Train: 2018-08-02T13:27:32.546653: step 12023, loss 0.484836.
Train: 2018-08-02T13:27:32.702867: step 12024, loss 0.501096.
Train: 2018-08-02T13:27:32.859080: step 12025, loss 0.534623.
Train: 2018-08-02T13:27:33.015294: step 12026, loss 0.519213.
Train: 2018-08-02T13:27:33.171508: step 12027, loss 0.504521.
Train: 2018-08-02T13:27:33.327721: step 12028, loss 0.519404.
Train: 2018-08-02T13:27:33.468313: step 12029, loss 0.587829.
Train: 2018-08-02T13:27:33.624526: step 12030, loss 0.470712.
Test: 2018-08-02T13:27:34.093138: step 12030, loss 0.547017.
Train: 2018-08-02T13:27:34.233759: step 12031, loss 0.490657.
Train: 2018-08-02T13:27:34.389977: step 12032, loss 0.4525.
Train: 2018-08-02T13:27:34.546155: step 12033, loss 0.615623.
Train: 2018-08-02T13:27:34.702413: step 12034, loss 0.515248.
Train: 2018-08-02T13:27:34.858607: step 12035, loss 0.555987.
Train: 2018-08-02T13:27:35.014826: step 12036, loss 0.47199.
Train: 2018-08-02T13:27:35.171010: step 12037, loss 0.548177.
Train: 2018-08-02T13:27:35.342844: step 12038, loss 0.579563.
Train: 2018-08-02T13:27:35.499057: step 12039, loss 0.456079.
Train: 2018-08-02T13:27:35.655295: step 12040, loss 0.494704.
Test: 2018-08-02T13:27:36.108323: step 12040, loss 0.548793.
Train: 2018-08-02T13:27:36.264528: step 12041, loss 0.528705.
Train: 2018-08-02T13:27:36.420746: step 12042, loss 0.518113.
Train: 2018-08-02T13:27:36.561339: step 12043, loss 0.537899.
Train: 2018-08-02T13:27:36.717552: step 12044, loss 0.517837.
Train: 2018-08-02T13:27:36.873766: step 12045, loss 0.555775.
Train: 2018-08-02T13:27:37.029979: step 12046, loss 0.477571.
Train: 2018-08-02T13:27:37.186193: step 12047, loss 0.543761.
Train: 2018-08-02T13:27:37.342375: step 12048, loss 0.535032.
Train: 2018-08-02T13:27:37.498614: step 12049, loss 0.503896.
Train: 2018-08-02T13:27:37.654802: step 12050, loss 0.500237.
Test: 2018-08-02T13:27:38.123442: step 12050, loss 0.546752.
Train: 2018-08-02T13:27:38.279657: step 12051, loss 0.54883.
Train: 2018-08-02T13:27:38.435869: step 12052, loss 0.423047.
Train: 2018-08-02T13:27:38.592114: step 12053, loss 0.511383.
Train: 2018-08-02T13:27:38.748326: step 12054, loss 0.513881.
Train: 2018-08-02T13:27:38.888918: step 12055, loss 0.607944.
Train: 2018-08-02T13:27:39.045132: step 12056, loss 0.475525.
Train: 2018-08-02T13:27:39.201314: step 12057, loss 0.503283.
Train: 2018-08-02T13:27:39.357530: step 12058, loss 0.47637.
Train: 2018-08-02T13:27:39.513771: step 12059, loss 0.633252.
Train: 2018-08-02T13:27:39.669968: step 12060, loss 0.563612.
Test: 2018-08-02T13:27:40.122993: step 12060, loss 0.551042.
Train: 2018-08-02T13:27:40.279217: step 12061, loss 0.515592.
Train: 2018-08-02T13:27:40.419810: step 12062, loss 0.50894.
Train: 2018-08-02T13:27:40.576023: step 12063, loss 0.547637.
Train: 2018-08-02T13:27:40.732239: step 12064, loss 0.523121.
Train: 2018-08-02T13:27:40.888454: step 12065, loss 0.501598.
Train: 2018-08-02T13:27:41.029042: step 12066, loss 0.465452.
Train: 2018-08-02T13:27:41.185255: step 12067, loss 0.504515.
Train: 2018-08-02T13:27:41.341469: step 12068, loss 0.559426.
Train: 2018-08-02T13:27:41.497684: step 12069, loss 0.509814.
Train: 2018-08-02T13:27:41.653866: step 12070, loss 0.481174.
Test: 2018-08-02T13:27:42.106885: step 12070, loss 0.547404.
Train: 2018-08-02T13:27:42.263128: step 12071, loss 0.572478.
Train: 2018-08-02T13:27:42.419342: step 12072, loss 0.523597.
Train: 2018-08-02T13:27:42.575526: step 12073, loss 0.510592.
Train: 2018-08-02T13:27:42.731740: step 12074, loss 0.530224.
Train: 2018-08-02T13:27:42.887987: step 12075, loss 0.542386.
Train: 2018-08-02T13:27:43.044197: step 12076, loss 0.510261.
Train: 2018-08-02T13:27:43.200380: step 12077, loss 0.597382.
Train: 2018-08-02T13:27:43.372213: step 12078, loss 0.608332.
Train: 2018-08-02T13:27:43.528427: step 12079, loss 0.531678.
Train: 2018-08-02T13:27:43.622184: step 12080, loss 0.510047.
Test: 2018-08-02T13:27:44.090838: step 12080, loss 0.554022.
Train: 2018-08-02T13:27:44.247039: step 12081, loss 0.442381.
Train: 2018-08-02T13:27:44.403223: step 12082, loss 0.552625.
Train: 2018-08-02T13:27:44.543843: step 12083, loss 0.49662.
Train: 2018-08-02T13:27:44.700028: step 12084, loss 0.453899.
Train: 2018-08-02T13:27:44.856266: step 12085, loss 0.527833.
Train: 2018-08-02T13:27:45.012454: step 12086, loss 0.459459.
Train: 2018-08-02T13:27:45.168668: step 12087, loss 0.573614.
Train: 2018-08-02T13:27:45.324912: step 12088, loss 0.525133.
Train: 2018-08-02T13:27:45.481121: step 12089, loss 0.553438.
Train: 2018-08-02T13:27:45.637343: step 12090, loss 0.485567.
Test: 2018-08-02T13:27:46.090328: step 12090, loss 0.5485.
Train: 2018-08-02T13:27:46.246571: step 12091, loss 0.521315.
Train: 2018-08-02T13:27:46.418377: step 12092, loss 0.598227.
Train: 2018-08-02T13:27:46.558998: step 12093, loss 0.537304.
Train: 2018-08-02T13:27:46.715211: step 12094, loss 0.550871.
Train: 2018-08-02T13:27:46.871424: step 12095, loss 0.485448.
Train: 2018-08-02T13:27:47.027609: step 12096, loss 0.532213.
Train: 2018-08-02T13:27:47.183851: step 12097, loss 0.510308.
Train: 2018-08-02T13:27:47.340035: step 12098, loss 0.581958.
Train: 2018-08-02T13:27:47.496277: step 12099, loss 0.531759.
Train: 2018-08-02T13:27:47.652486: step 12100, loss 0.521334.
Test: 2018-08-02T13:27:48.105505: step 12100, loss 0.546375.
Train: 2018-08-02T13:27:48.745985: step 12101, loss 0.475339.
Train: 2018-08-02T13:27:48.902194: step 12102, loss 0.4825.
Train: 2018-08-02T13:27:49.058412: step 12103, loss 0.498233.
Train: 2018-08-02T13:27:49.214620: step 12104, loss 0.632301.
Train: 2018-08-02T13:27:49.370810: step 12105, loss 0.533832.
Train: 2018-08-02T13:27:49.542674: step 12106, loss 0.565975.
Train: 2018-08-02T13:27:49.683266: step 12107, loss 0.478305.
Train: 2018-08-02T13:27:49.839479: step 12108, loss 0.522713.
Train: 2018-08-02T13:27:49.995692: step 12109, loss 0.584065.
Train: 2018-08-02T13:27:50.151907: step 12110, loss 0.527851.
Test: 2018-08-02T13:27:50.604896: step 12110, loss 0.551068.
Train: 2018-08-02T13:27:50.761138: step 12111, loss 0.483068.
Train: 2018-08-02T13:27:50.917323: step 12112, loss 0.530122.
Train: 2018-08-02T13:27:51.073565: step 12113, loss 0.565026.
Train: 2018-08-02T13:27:51.229748: step 12114, loss 0.545832.
Train: 2018-08-02T13:27:51.401584: step 12115, loss 0.524891.
Train: 2018-08-02T13:27:51.557796: step 12116, loss 0.581313.
Train: 2018-08-02T13:27:51.714010: step 12117, loss 0.423478.
Train: 2018-08-02T13:27:51.870252: step 12118, loss 0.491774.
Train: 2018-08-02T13:27:52.026468: step 12119, loss 0.551614.
Train: 2018-08-02T13:27:52.182681: step 12120, loss 0.517643.
Test: 2018-08-02T13:27:52.635671: step 12120, loss 0.55004.
Train: 2018-08-02T13:27:52.791883: step 12121, loss 0.443706.
Train: 2018-08-02T13:27:52.948126: step 12122, loss 0.489834.
Train: 2018-08-02T13:27:53.104311: step 12123, loss 0.446092.
Train: 2018-08-02T13:27:53.260523: step 12124, loss 0.530843.
Train: 2018-08-02T13:27:53.416736: step 12125, loss 0.570727.
Train: 2018-08-02T13:27:53.557330: step 12126, loss 0.576766.
Train: 2018-08-02T13:27:53.713572: step 12127, loss 0.494939.
Train: 2018-08-02T13:27:53.869784: step 12128, loss 0.481811.
Train: 2018-08-02T13:27:54.025996: step 12129, loss 0.498647.
Train: 2018-08-02T13:27:54.182213: step 12130, loss 0.536892.
Test: 2018-08-02T13:27:54.635202: step 12130, loss 0.546956.
Train: 2018-08-02T13:27:54.791439: step 12131, loss 0.57267.
Train: 2018-08-02T13:27:54.947658: step 12132, loss 0.505205.
Train: 2018-08-02T13:27:55.103842: step 12133, loss 0.490893.
Train: 2018-08-02T13:27:55.260055: step 12134, loss 0.510725.
Train: 2018-08-02T13:27:55.416269: step 12135, loss 0.544063.
Train: 2018-08-02T13:27:55.572513: step 12136, loss 0.468813.
Train: 2018-08-02T13:27:55.744318: step 12137, loss 0.540361.
Train: 2018-08-02T13:27:55.884938: step 12138, loss 0.500046.
Train: 2018-08-02T13:27:56.041122: step 12139, loss 0.488265.
Train: 2018-08-02T13:27:56.197365: step 12140, loss 0.522686.
Test: 2018-08-02T13:27:56.650356: step 12140, loss 0.547736.
Train: 2018-08-02T13:27:56.806598: step 12141, loss 0.455303.
Train: 2018-08-02T13:27:56.962811: step 12142, loss 0.516268.
Train: 2018-08-02T13:27:57.119025: step 12143, loss 0.449841.
Train: 2018-08-02T13:27:57.275238: step 12144, loss 0.409269.
Train: 2018-08-02T13:27:57.431451: step 12145, loss 0.46379.
Train: 2018-08-02T13:27:57.572043: step 12146, loss 0.623711.
Train: 2018-08-02T13:27:57.728257: step 12147, loss 0.529487.
Train: 2018-08-02T13:27:57.884469: step 12148, loss 0.480977.
Train: 2018-08-02T13:27:58.040684: step 12149, loss 0.527707.
Train: 2018-08-02T13:27:58.181276: step 12150, loss 0.522663.
Test: 2018-08-02T13:27:58.649887: step 12150, loss 0.553247.
Train: 2018-08-02T13:27:58.806099: step 12151, loss 0.578408.
Train: 2018-08-02T13:27:58.962313: step 12152, loss 0.533336.
Train: 2018-08-02T13:27:59.118525: step 12153, loss 0.528538.
Train: 2018-08-02T13:27:59.274741: step 12154, loss 0.499844.
Train: 2018-08-02T13:27:59.430983: step 12155, loss 0.514788.
Train: 2018-08-02T13:27:59.587197: step 12156, loss 0.547832.
Train: 2018-08-02T13:27:59.743408: step 12157, loss 0.436568.
Train: 2018-08-02T13:27:59.899595: step 12158, loss 0.533477.
Train: 2018-08-02T13:28:00.055838: step 12159, loss 0.463448.
Train: 2018-08-02T13:28:00.212050: step 12160, loss 0.611447.
Test: 2018-08-02T13:28:00.680709: step 12160, loss 0.547443.
Train: 2018-08-02T13:28:00.821283: step 12161, loss 0.466724.
Train: 2018-08-02T13:28:00.977466: step 12162, loss 0.477688.
Train: 2018-08-02T13:28:01.133709: step 12163, loss 0.441757.
Train: 2018-08-02T13:28:01.274301: step 12164, loss 0.571469.
Train: 2018-08-02T13:28:01.430516: step 12165, loss 0.560062.
Train: 2018-08-02T13:28:01.586728: step 12166, loss 0.415236.
Train: 2018-08-02T13:28:01.742942: step 12167, loss 0.533707.
Train: 2018-08-02T13:28:01.899151: step 12168, loss 0.474628.
Train: 2018-08-02T13:28:02.055369: step 12169, loss 0.565431.
Train: 2018-08-02T13:28:02.211583: step 12170, loss 0.565276.
Test: 2018-08-02T13:28:02.680202: step 12170, loss 0.549747.
Train: 2018-08-02T13:28:02.836406: step 12171, loss 0.54367.
Train: 2018-08-02T13:28:02.992650: step 12172, loss 0.425708.
Train: 2018-08-02T13:28:03.148833: step 12173, loss 0.523798.
Train: 2018-08-02T13:28:03.305076: step 12174, loss 0.485519.
Train: 2018-08-02T13:28:03.445639: step 12175, loss 0.522321.
Train: 2018-08-02T13:28:03.601881: step 12176, loss 0.558779.
Train: 2018-08-02T13:28:03.758094: step 12177, loss 0.547434.
Train: 2018-08-02T13:28:03.898657: step 12178, loss 0.488092.
Train: 2018-08-02T13:28:04.054901: step 12179, loss 0.488524.
Train: 2018-08-02T13:28:04.211083: step 12180, loss 0.555588.
Test: 2018-08-02T13:28:04.679724: step 12180, loss 0.549698.
Train: 2018-08-02T13:28:04.835972: step 12181, loss 0.560364.
Train: 2018-08-02T13:28:04.992186: step 12182, loss 0.546336.
Train: 2018-08-02T13:28:05.148395: step 12183, loss 0.508834.
Train: 2018-08-02T13:28:05.304579: step 12184, loss 0.593723.
Train: 2018-08-02T13:28:05.460793: step 12185, loss 0.560564.
Train: 2018-08-02T13:28:05.617035: step 12186, loss 0.533235.
Train: 2018-08-02T13:28:05.773250: step 12187, loss 0.516899.
Train: 2018-08-02T13:28:05.929430: step 12188, loss 0.537149.
Train: 2018-08-02T13:28:06.085676: step 12189, loss 0.52743.
Train: 2018-08-02T13:28:06.241858: step 12190, loss 0.515373.
Test: 2018-08-02T13:28:06.710498: step 12190, loss 0.547006.
Train: 2018-08-02T13:28:06.851121: step 12191, loss 0.627061.
Train: 2018-08-02T13:28:07.007334: step 12192, loss 0.536328.
Train: 2018-08-02T13:28:07.163518: step 12193, loss 0.538812.
Train: 2018-08-02T13:28:07.319760: step 12194, loss 0.60598.
Train: 2018-08-02T13:28:07.475944: step 12195, loss 0.561488.
Train: 2018-08-02T13:28:07.632183: step 12196, loss 0.48503.
Train: 2018-08-02T13:28:07.788379: step 12197, loss 0.506617.
Train: 2018-08-02T13:28:07.944617: step 12198, loss 0.472658.
Train: 2018-08-02T13:28:08.100831: step 12199, loss 0.481846.
Train: 2018-08-02T13:28:08.257039: step 12200, loss 0.488264.
Test: 2018-08-02T13:28:08.710030: step 12200, loss 0.548966.
Train: 2018-08-02T13:28:09.319293: step 12201, loss 0.681609.
Train: 2018-08-02T13:28:09.475505: step 12202, loss 0.508023.
Train: 2018-08-02T13:28:09.631691: step 12203, loss 0.551176.
Train: 2018-08-02T13:28:09.787934: step 12204, loss 0.491897.
Train: 2018-08-02T13:28:09.928524: step 12205, loss 0.537645.
Train: 2018-08-02T13:28:10.084708: step 12206, loss 0.516612.
Train: 2018-08-02T13:28:10.240921: step 12207, loss 0.621224.
Train: 2018-08-02T13:28:10.397135: step 12208, loss 0.480445.
Train: 2018-08-02T13:28:10.553350: step 12209, loss 0.56744.
Train: 2018-08-02T13:28:10.709592: step 12210, loss 0.545273.
Test: 2018-08-02T13:28:11.178204: step 12210, loss 0.551117.
Train: 2018-08-02T13:28:11.334446: step 12211, loss 0.549736.
Train: 2018-08-02T13:28:11.490660: step 12212, loss 0.488397.
Train: 2018-08-02T13:28:11.646868: step 12213, loss 0.53372.
Train: 2018-08-02T13:28:11.803087: step 12214, loss 0.559612.
Train: 2018-08-02T13:28:11.959295: step 12215, loss 0.45851.
Train: 2018-08-02T13:28:12.115510: step 12216, loss 0.486812.
Train: 2018-08-02T13:28:12.271726: step 12217, loss 0.500689.
Train: 2018-08-02T13:28:12.427939: step 12218, loss 0.579476.
Train: 2018-08-02T13:28:12.584123: step 12219, loss 0.563731.
Train: 2018-08-02T13:28:12.740366: step 12220, loss 0.494044.
Test: 2018-08-02T13:28:13.193380: step 12220, loss 0.548005.
Train: 2018-08-02T13:28:13.349599: step 12221, loss 0.497543.
Train: 2018-08-02T13:28:13.505817: step 12222, loss 0.523034.
Train: 2018-08-02T13:28:13.662020: step 12223, loss 0.557766.
Train: 2018-08-02T13:28:13.818239: step 12224, loss 0.539444.
Train: 2018-08-02T13:28:13.974453: step 12225, loss 0.535072.
Train: 2018-08-02T13:28:14.115044: step 12226, loss 0.517195.
Train: 2018-08-02T13:28:14.271254: step 12227, loss 0.579474.
Train: 2018-08-02T13:28:14.427443: step 12228, loss 0.496398.
Train: 2018-08-02T13:28:14.583685: step 12229, loss 0.528818.
Train: 2018-08-02T13:28:14.739898: step 12230, loss 0.486157.
Test: 2018-08-02T13:28:15.192888: step 12230, loss 0.547756.
Train: 2018-08-02T13:28:15.302237: step 12231, loss 0.507561.
Train: 2018-08-02T13:28:15.474096: step 12232, loss 0.540238.
Train: 2018-08-02T13:28:15.630315: step 12233, loss 0.562714.
Train: 2018-08-02T13:28:15.786497: step 12234, loss 0.426026.
Train: 2018-08-02T13:28:15.942711: step 12235, loss 0.511535.
Train: 2018-08-02T13:28:16.098955: step 12236, loss 0.556956.
Train: 2018-08-02T13:28:16.255169: step 12237, loss 0.511728.
Train: 2018-08-02T13:28:16.411382: step 12238, loss 0.475197.
Train: 2018-08-02T13:28:16.567595: step 12239, loss 0.452536.
Train: 2018-08-02T13:28:16.723804: step 12240, loss 0.530772.
Test: 2018-08-02T13:28:17.176799: step 12240, loss 0.551477.
Train: 2018-08-02T13:28:17.333041: step 12241, loss 0.566475.
Train: 2018-08-02T13:28:17.489223: step 12242, loss 0.532485.
Train: 2018-08-02T13:28:17.645467: step 12243, loss 0.46224.
Train: 2018-08-02T13:28:17.801682: step 12244, loss 0.540746.
Train: 2018-08-02T13:28:17.942267: step 12245, loss 0.494458.
Train: 2018-08-02T13:28:18.098486: step 12246, loss 0.429741.
Train: 2018-08-02T13:28:18.254700: step 12247, loss 0.574845.
Train: 2018-08-02T13:28:18.410909: step 12248, loss 0.504383.
Train: 2018-08-02T13:28:18.567129: step 12249, loss 0.497912.
Train: 2018-08-02T13:28:18.723310: step 12250, loss 0.47093.
Test: 2018-08-02T13:28:19.176361: step 12250, loss 0.546495.
Train: 2018-08-02T13:28:19.332544: step 12251, loss 0.484079.
Train: 2018-08-02T13:28:19.488787: step 12252, loss 0.508804.
Train: 2018-08-02T13:28:19.645000: step 12253, loss 0.488463.
Train: 2018-08-02T13:28:19.801213: step 12254, loss 0.467078.
Train: 2018-08-02T13:28:19.957414: step 12255, loss 0.601704.
Train: 2018-08-02T13:28:20.113609: step 12256, loss 0.53036.
Train: 2018-08-02T13:28:20.285469: step 12257, loss 0.445183.
Train: 2018-08-02T13:28:20.441659: step 12258, loss 0.582158.
Train: 2018-08-02T13:28:20.597901: step 12259, loss 0.56231.
Train: 2018-08-02T13:28:20.738493: step 12260, loss 0.502178.
Test: 2018-08-02T13:28:21.207128: step 12260, loss 0.550126.
Train: 2018-08-02T13:28:21.347727: step 12261, loss 0.566074.
Train: 2018-08-02T13:28:21.503911: step 12262, loss 0.443559.
Train: 2018-08-02T13:28:21.660152: step 12263, loss 0.497614.
Train: 2018-08-02T13:28:21.816367: step 12264, loss 0.542564.
Train: 2018-08-02T13:28:21.972583: step 12265, loss 0.518934.
Train: 2018-08-02T13:28:22.128792: step 12266, loss 0.507687.
Train: 2018-08-02T13:28:22.285002: step 12267, loss 0.51902.
Train: 2018-08-02T13:28:22.441215: step 12268, loss 0.523949.
Train: 2018-08-02T13:28:22.597436: step 12269, loss 0.512059.
Train: 2018-08-02T13:28:22.753616: step 12270, loss 0.508529.
Test: 2018-08-02T13:28:23.206643: step 12270, loss 0.54864.
Train: 2018-08-02T13:28:23.362879: step 12271, loss 0.501497.
Train: 2018-08-02T13:28:23.519062: step 12272, loss 0.468171.
Train: 2018-08-02T13:28:23.675306: step 12273, loss 0.533571.
Train: 2018-08-02T13:28:23.831519: step 12274, loss 0.44676.
Train: 2018-08-02T13:28:23.987703: step 12275, loss 0.51619.
Train: 2018-08-02T13:28:24.143946: step 12276, loss 0.526136.
Train: 2018-08-02T13:28:24.300160: step 12277, loss 0.45118.
Train: 2018-08-02T13:28:24.456372: step 12278, loss 0.556069.
Train: 2018-08-02T13:28:24.612585: step 12279, loss 0.544778.
Train: 2018-08-02T13:28:24.753178: step 12280, loss 0.511042.
Test: 2018-08-02T13:28:25.221827: step 12280, loss 0.546496.
Train: 2018-08-02T13:28:25.378002: step 12281, loss 0.467089.
Train: 2018-08-02T13:28:25.534246: step 12282, loss 0.534053.
Train: 2018-08-02T13:28:25.690428: step 12283, loss 0.498614.
Train: 2018-08-02T13:28:25.846672: step 12284, loss 0.539811.
Train: 2018-08-02T13:28:26.002885: step 12285, loss 0.538043.
Train: 2018-08-02T13:28:26.159069: step 12286, loss 0.457507.
Train: 2018-08-02T13:28:26.315313: step 12287, loss 0.486141.
Train: 2018-08-02T13:28:26.471530: step 12288, loss 0.562437.
Train: 2018-08-02T13:28:26.627739: step 12289, loss 0.59174.
Train: 2018-08-02T13:28:26.783953: step 12290, loss 0.512686.
Test: 2018-08-02T13:28:27.252562: step 12290, loss 0.552108.
Train: 2018-08-02T13:28:27.393185: step 12291, loss 0.556061.
Train: 2018-08-02T13:28:27.549398: step 12292, loss 0.515415.
Train: 2018-08-02T13:28:27.705581: step 12293, loss 0.528873.
Train: 2018-08-02T13:28:27.861825: step 12294, loss 0.50018.
Train: 2018-08-02T13:28:28.018009: step 12295, loss 0.505845.
Train: 2018-08-02T13:28:28.174252: step 12296, loss 0.582557.
Train: 2018-08-02T13:28:28.330437: step 12297, loss 0.535149.
Train: 2018-08-02T13:28:28.486650: step 12298, loss 0.512493.
Train: 2018-08-02T13:28:28.658484: step 12299, loss 0.513522.
Train: 2018-08-02T13:28:28.814728: step 12300, loss 0.489161.
Test: 2018-08-02T13:28:29.267746: step 12300, loss 0.547979.
Train: 2018-08-02T13:28:29.892600: step 12301, loss 0.511014.
Train: 2018-08-02T13:28:30.048812: step 12302, loss 0.477174.
Train: 2018-08-02T13:28:30.205027: step 12303, loss 0.575976.
Train: 2018-08-02T13:28:30.361238: step 12304, loss 0.558962.
Train: 2018-08-02T13:28:30.517455: step 12305, loss 0.451851.
Train: 2018-08-02T13:28:30.673667: step 12306, loss 0.524543.
Train: 2018-08-02T13:28:30.814259: step 12307, loss 0.507817.
Train: 2018-08-02T13:28:30.970472: step 12308, loss 0.470152.
Train: 2018-08-02T13:28:31.126685: step 12309, loss 0.473428.
Train: 2018-08-02T13:28:31.282899: step 12310, loss 0.452811.
Test: 2018-08-02T13:28:31.735888: step 12310, loss 0.547943.
Train: 2018-08-02T13:28:31.892131: step 12311, loss 0.510298.
Train: 2018-08-02T13:28:32.048314: step 12312, loss 0.508435.
Train: 2018-08-02T13:28:32.188907: step 12313, loss 0.487666.
Train: 2018-08-02T13:28:32.345119: step 12314, loss 0.520699.
Train: 2018-08-02T13:28:32.501335: step 12315, loss 0.509473.
Train: 2018-08-02T13:28:32.657573: step 12316, loss 0.525083.
Train: 2018-08-02T13:28:32.829408: step 12317, loss 0.541652.
Train: 2018-08-02T13:28:32.985647: step 12318, loss 0.469079.
Train: 2018-08-02T13:28:33.141810: step 12319, loss 0.522999.
Train: 2018-08-02T13:28:33.298060: step 12320, loss 0.524002.
Test: 2018-08-02T13:28:33.751072: step 12320, loss 0.554866.
Train: 2018-08-02T13:28:33.907286: step 12321, loss 0.598236.
Train: 2018-08-02T13:28:34.063498: step 12322, loss 0.518172.
Train: 2018-08-02T13:28:34.204090: step 12323, loss 0.507916.
Train: 2018-08-02T13:28:34.360272: step 12324, loss 0.567199.
Train: 2018-08-02T13:28:34.594595: step 12325, loss 0.54825.
Train: 2018-08-02T13:28:34.750841: step 12326, loss 0.508955.
Train: 2018-08-02T13:28:34.907051: step 12327, loss 0.545239.
Train: 2018-08-02T13:28:35.063235: step 12328, loss 0.558854.
Train: 2018-08-02T13:28:35.219448: step 12329, loss 0.611305.
Train: 2018-08-02T13:28:35.375686: step 12330, loss 0.572141.
Test: 2018-08-02T13:28:35.828680: step 12330, loss 0.558634.
Train: 2018-08-02T13:28:35.984893: step 12331, loss 0.492917.
Train: 2018-08-02T13:28:36.156729: step 12332, loss 0.506726.
Train: 2018-08-02T13:28:36.297350: step 12333, loss 0.579293.
Train: 2018-08-02T13:28:36.453559: step 12334, loss 0.616067.
Train: 2018-08-02T13:28:36.625398: step 12335, loss 0.509149.
Train: 2018-08-02T13:28:36.765990: step 12336, loss 0.521716.
Train: 2018-08-02T13:28:36.922204: step 12337, loss 0.555269.
Train: 2018-08-02T13:28:37.078387: step 12338, loss 0.515263.
Train: 2018-08-02T13:28:37.234625: step 12339, loss 0.537186.
Train: 2018-08-02T13:28:37.390844: step 12340, loss 0.4868.
Test: 2018-08-02T13:28:37.843834: step 12340, loss 0.550862.
Train: 2018-08-02T13:28:38.000076: step 12341, loss 0.496461.
Train: 2018-08-02T13:28:38.156276: step 12342, loss 0.526164.
Train: 2018-08-02T13:28:38.312506: step 12343, loss 0.399847.
Train: 2018-08-02T13:28:38.468687: step 12344, loss 0.498936.
Train: 2018-08-02T13:28:38.624929: step 12345, loss 0.557509.
Train: 2018-08-02T13:28:38.796760: step 12346, loss 0.488778.
Train: 2018-08-02T13:28:38.937356: step 12347, loss 0.490945.
Train: 2018-08-02T13:28:39.093571: step 12348, loss 0.571646.
Train: 2018-08-02T13:28:39.265401: step 12349, loss 0.478567.
Train: 2018-08-02T13:28:39.421589: step 12350, loss 0.501461.
Test: 2018-08-02T13:28:39.874638: step 12350, loss 0.547325.
Train: 2018-08-02T13:28:40.030850: step 12351, loss 0.531777.
Train: 2018-08-02T13:28:40.171442: step 12352, loss 0.572846.
Train: 2018-08-02T13:28:40.327625: step 12353, loss 0.564211.
Train: 2018-08-02T13:28:40.483838: step 12354, loss 0.513584.
Train: 2018-08-02T13:28:40.624432: step 12355, loss 0.496514.
Train: 2018-08-02T13:28:40.780657: step 12356, loss 0.548739.
Train: 2018-08-02T13:28:40.936889: step 12357, loss 0.54106.
Train: 2018-08-02T13:28:41.093102: step 12358, loss 0.540619.
Train: 2018-08-02T13:28:41.245172: step 12359, loss 0.494778.
Train: 2018-08-02T13:28:41.417038: step 12360, loss 0.535284.
Test: 2018-08-02T13:28:41.870040: step 12360, loss 0.550529.
Train: 2018-08-02T13:28:42.026271: step 12361, loss 0.623979.
Train: 2018-08-02T13:28:42.182484: step 12362, loss 0.517283.
Train: 2018-08-02T13:28:42.338693: step 12363, loss 0.547517.
Train: 2018-08-02T13:28:42.494880: step 12364, loss 0.550371.
Train: 2018-08-02T13:28:42.651123: step 12365, loss 0.550077.
Train: 2018-08-02T13:28:42.807337: step 12366, loss 0.502557.
Train: 2018-08-02T13:28:42.963521: step 12367, loss 0.545797.
Train: 2018-08-02T13:28:43.119759: step 12368, loss 0.473531.
Train: 2018-08-02T13:28:43.260356: step 12369, loss 0.536774.
Train: 2018-08-02T13:28:43.416569: step 12370, loss 0.483637.
Test: 2018-08-02T13:28:43.885182: step 12370, loss 0.546917.
Train: 2018-08-02T13:28:44.041418: step 12371, loss 0.481292.
Train: 2018-08-02T13:28:44.197637: step 12372, loss 0.611786.
Train: 2018-08-02T13:28:44.353820: step 12373, loss 0.567112.
Train: 2018-08-02T13:28:44.510035: step 12374, loss 0.471957.
Train: 2018-08-02T13:28:44.666248: step 12375, loss 0.541483.
Train: 2018-08-02T13:28:44.822461: step 12376, loss 0.501489.
Train: 2018-08-02T13:28:44.994294: step 12377, loss 0.53295.
Train: 2018-08-02T13:28:45.150540: step 12378, loss 0.515272.
Train: 2018-08-02T13:28:45.306723: step 12379, loss 0.481521.
Train: 2018-08-02T13:28:45.462965: step 12380, loss 0.603424.
Test: 2018-08-02T13:28:45.915955: step 12380, loss 0.552407.
Train: 2018-08-02T13:28:46.072198: step 12381, loss 0.546592.
Train: 2018-08-02T13:28:46.181516: step 12382, loss 0.515011.
Train: 2018-08-02T13:28:46.337761: step 12383, loss 0.498714.
Train: 2018-08-02T13:28:46.493943: step 12384, loss 0.519644.
Train: 2018-08-02T13:28:46.650185: step 12385, loss 0.46402.
Train: 2018-08-02T13:28:46.806401: step 12386, loss 0.508202.
Train: 2018-08-02T13:28:46.962583: step 12387, loss 0.532819.
Train: 2018-08-02T13:28:47.118828: step 12388, loss 0.420695.
Train: 2018-08-02T13:28:47.275040: step 12389, loss 0.433212.
Train: 2018-08-02T13:28:47.431255: step 12390, loss 0.533536.
Test: 2018-08-02T13:28:47.899890: step 12390, loss 0.550137.
Train: 2018-08-02T13:28:48.056108: step 12391, loss 0.517346.
Train: 2018-08-02T13:28:48.196670: step 12392, loss 0.563419.
Train: 2018-08-02T13:28:48.352914: step 12393, loss 0.566384.
Train: 2018-08-02T13:28:48.509097: step 12394, loss 0.470236.
Train: 2018-08-02T13:28:48.665340: step 12395, loss 0.479724.
Train: 2018-08-02T13:28:48.821553: step 12396, loss 0.475627.
Train: 2018-08-02T13:28:48.977767: step 12397, loss 0.463389.
Train: 2018-08-02T13:28:49.133976: step 12398, loss 0.525617.
Train: 2018-08-02T13:28:49.290203: step 12399, loss 0.488105.
Train: 2018-08-02T13:28:49.446377: step 12400, loss 0.510911.
Test: 2018-08-02T13:28:49.899425: step 12400, loss 0.548232.
Train: 2018-08-02T13:28:50.555526: step 12401, loss 0.488629.
Train: 2018-08-02T13:28:50.711732: step 12402, loss 0.487399.
Train: 2018-08-02T13:28:50.867950: step 12403, loss 0.518656.
Train: 2018-08-02T13:28:51.024157: step 12404, loss 0.563046.
Train: 2018-08-02T13:28:51.180377: step 12405, loss 0.493158.
Train: 2018-08-02T13:28:51.336589: step 12406, loss 0.540564.
Train: 2018-08-02T13:28:51.492803: step 12407, loss 0.520653.
Train: 2018-08-02T13:28:51.649017: step 12408, loss 0.482696.
Train: 2018-08-02T13:28:51.805230: step 12409, loss 0.524874.
Train: 2018-08-02T13:28:51.961443: step 12410, loss 0.573697.
Test: 2018-08-02T13:28:52.414442: step 12410, loss 0.551595.
Train: 2018-08-02T13:28:52.570676: step 12411, loss 0.471683.
Train: 2018-08-02T13:28:52.726866: step 12412, loss 0.534152.
Train: 2018-08-02T13:28:52.883102: step 12413, loss 0.504789.
Train: 2018-08-02T13:28:53.039316: step 12414, loss 0.485247.
Train: 2018-08-02T13:28:53.195524: step 12415, loss 0.515502.
Train: 2018-08-02T13:28:53.351714: step 12416, loss 0.537563.
Train: 2018-08-02T13:28:53.507952: step 12417, loss 0.490923.
Train: 2018-08-02T13:28:53.664139: step 12418, loss 0.473697.
Train: 2018-08-02T13:28:53.820355: step 12419, loss 0.558013.
Train: 2018-08-02T13:28:53.976596: step 12420, loss 0.48685.
Test: 2018-08-02T13:28:54.429589: step 12420, loss 0.547609.
Train: 2018-08-02T13:28:54.585829: step 12421, loss 0.49066.
Train: 2018-08-02T13:28:54.742042: step 12422, loss 0.557963.
Train: 2018-08-02T13:28:54.882635: step 12423, loss 0.531593.
Train: 2018-08-02T13:28:55.038819: step 12424, loss 0.473146.
Train: 2018-08-02T13:28:55.195061: step 12425, loss 0.490755.
Train: 2018-08-02T13:28:55.351245: step 12426, loss 0.502684.
Train: 2018-08-02T13:28:55.507459: step 12427, loss 0.513007.
Train: 2018-08-02T13:28:55.679319: step 12428, loss 0.560873.
Train: 2018-08-02T13:28:55.835506: step 12429, loss 0.540771.
Train: 2018-08-02T13:28:55.991751: step 12430, loss 0.518014.
Test: 2018-08-02T13:28:56.444739: step 12430, loss 0.550935.
Train: 2018-08-02T13:28:56.600983: step 12431, loss 0.543331.
Train: 2018-08-02T13:28:56.757196: step 12432, loss 0.462497.
Train: 2018-08-02T13:28:56.913403: step 12433, loss 0.510989.
Train: 2018-08-02T13:28:57.053971: step 12434, loss 0.484779.
Train: 2018-08-02T13:28:57.225831: step 12435, loss 0.487397.
Train: 2018-08-02T13:28:57.366428: step 12436, loss 0.532326.
Train: 2018-08-02T13:28:57.522641: step 12437, loss 0.406359.
Train: 2018-08-02T13:28:57.678826: step 12438, loss 0.453823.
Train: 2018-08-02T13:28:57.835062: step 12439, loss 0.565757.
Train: 2018-08-02T13:28:57.991282: step 12440, loss 0.538826.
Test: 2018-08-02T13:28:58.444270: step 12440, loss 0.548381.
Train: 2018-08-02T13:28:58.600484: step 12441, loss 0.496246.
Train: 2018-08-02T13:28:58.756728: step 12442, loss 0.481103.
Train: 2018-08-02T13:28:58.912911: step 12443, loss 0.484601.
Train: 2018-08-02T13:28:59.069154: step 12444, loss 0.486344.
Train: 2018-08-02T13:28:59.225337: step 12445, loss 0.515268.
Train: 2018-08-02T13:28:59.381550: step 12446, loss 0.490525.
Train: 2018-08-02T13:28:59.537795: step 12447, loss 0.563428.
Train: 2018-08-02T13:28:59.694006: step 12448, loss 0.471651.
Train: 2018-08-02T13:28:59.850221: step 12449, loss 0.523881.
Train: 2018-08-02T13:29:00.006434: step 12450, loss 0.552828.
Test: 2018-08-02T13:29:00.459424: step 12450, loss 0.548118.
Train: 2018-08-02T13:29:00.615669: step 12451, loss 0.569962.
Train: 2018-08-02T13:29:00.771880: step 12452, loss 0.508746.
Train: 2018-08-02T13:29:00.912472: step 12453, loss 0.569724.
Train: 2018-08-02T13:29:01.068657: step 12454, loss 0.525374.
Train: 2018-08-02T13:29:01.224894: step 12455, loss 0.499891.
Train: 2018-08-02T13:29:01.381114: step 12456, loss 0.562358.
Train: 2018-08-02T13:29:01.537326: step 12457, loss 0.589167.
Train: 2018-08-02T13:29:01.693510: step 12458, loss 0.547254.
Train: 2018-08-02T13:29:01.849754: step 12459, loss 0.527253.
Train: 2018-08-02T13:29:02.005937: step 12460, loss 0.438693.
Test: 2018-08-02T13:29:02.458980: step 12460, loss 0.548591.
Train: 2018-08-02T13:29:02.615199: step 12461, loss 0.517896.
Train: 2018-08-02T13:29:02.771413: step 12462, loss 0.526582.
Train: 2018-08-02T13:29:02.927626: step 12463, loss 0.446176.
Train: 2018-08-02T13:29:03.083839: step 12464, loss 0.54867.
Train: 2018-08-02T13:29:03.240024: step 12465, loss 0.56088.
Train: 2018-08-02T13:29:03.396266: step 12466, loss 0.575038.
Train: 2018-08-02T13:29:03.536828: step 12467, loss 0.494406.
Train: 2018-08-02T13:29:03.693071: step 12468, loss 0.487778.
Train: 2018-08-02T13:29:03.849284: step 12469, loss 0.565103.
Train: 2018-08-02T13:29:04.005500: step 12470, loss 0.569059.
Test: 2018-08-02T13:29:04.458512: step 12470, loss 0.551073.
Train: 2018-08-02T13:29:04.614732: step 12471, loss 0.537159.
Train: 2018-08-02T13:29:04.770945: step 12472, loss 0.48471.
Train: 2018-08-02T13:29:04.927157: step 12473, loss 0.5185.
Train: 2018-08-02T13:29:05.083348: step 12474, loss 0.504759.
Train: 2018-08-02T13:29:05.239556: step 12475, loss 0.444478.
Train: 2018-08-02T13:29:05.395798: step 12476, loss 0.493764.
Train: 2018-08-02T13:29:05.552011: step 12477, loss 0.449086.
Train: 2018-08-02T13:29:05.708224: step 12478, loss 0.510274.
Train: 2018-08-02T13:29:05.864439: step 12479, loss 0.56038.
Train: 2018-08-02T13:29:06.020651: step 12480, loss 0.500339.
Test: 2018-08-02T13:29:06.473665: step 12480, loss 0.550028.
Train: 2018-08-02T13:29:06.614262: step 12481, loss 0.475236.
Train: 2018-08-02T13:29:06.786091: step 12482, loss 0.46654.
Train: 2018-08-02T13:29:06.942310: step 12483, loss 0.525744.
Train: 2018-08-02T13:29:07.082872: step 12484, loss 0.473978.
Train: 2018-08-02T13:29:07.239116: step 12485, loss 0.397217.
Train: 2018-08-02T13:29:07.395329: step 12486, loss 0.453382.
Train: 2018-08-02T13:29:07.551543: step 12487, loss 0.467214.
Train: 2018-08-02T13:29:07.692135: step 12488, loss 0.532712.
Train: 2018-08-02T13:29:07.848318: step 12489, loss 0.597792.
Train: 2018-08-02T13:29:08.004562: step 12490, loss 0.562479.
Test: 2018-08-02T13:29:08.457562: step 12490, loss 0.547648.
Train: 2018-08-02T13:29:08.613795: step 12491, loss 0.580972.
Train: 2018-08-02T13:29:08.754389: step 12492, loss 0.568224.
Train: 2018-08-02T13:29:08.910598: step 12493, loss 0.542965.
Train: 2018-08-02T13:29:09.066813: step 12494, loss 0.537764.
Train: 2018-08-02T13:29:09.223026: step 12495, loss 0.511917.
Train: 2018-08-02T13:29:09.379211: step 12496, loss 0.519028.
Train: 2018-08-02T13:29:09.535448: step 12497, loss 0.445352.
Train: 2018-08-02T13:29:09.676046: step 12498, loss 0.464594.
Train: 2018-08-02T13:29:09.832229: step 12499, loss 0.603921.
Train: 2018-08-02T13:29:09.988442: step 12500, loss 0.60445.
Test: 2018-08-02T13:29:10.441462: step 12500, loss 0.5468.
Train: 2018-08-02T13:29:11.035097: step 12501, loss 0.580167.
Train: 2018-08-02T13:29:11.191316: step 12502, loss 0.492088.
Train: 2018-08-02T13:29:11.347499: step 12503, loss 0.551196.
Train: 2018-08-02T13:29:11.503742: step 12504, loss 0.526668.
Train: 2018-08-02T13:29:11.644335: step 12505, loss 0.476516.
Train: 2018-08-02T13:29:11.800517: step 12506, loss 0.54409.
Train: 2018-08-02T13:29:11.956730: step 12507, loss 0.505976.
Train: 2018-08-02T13:29:12.112974: step 12508, loss 0.52864.
Train: 2018-08-02T13:29:12.253567: step 12509, loss 0.522225.
Train: 2018-08-02T13:29:12.409780: step 12510, loss 0.488285.
Test: 2018-08-02T13:29:12.878421: step 12510, loss 0.547147.
Train: 2018-08-02T13:29:13.034657: step 12511, loss 0.524834.
Train: 2018-08-02T13:29:13.175226: step 12512, loss 0.593861.
Train: 2018-08-02T13:29:13.331435: step 12513, loss 0.529563.
Train: 2018-08-02T13:29:13.487623: step 12514, loss 0.563148.
Train: 2018-08-02T13:29:13.643865: step 12515, loss 0.58408.
Train: 2018-08-02T13:29:13.784458: step 12516, loss 0.533514.
Train: 2018-08-02T13:29:13.940672: step 12517, loss 0.537297.
Train: 2018-08-02T13:29:14.096885: step 12518, loss 0.587042.
Train: 2018-08-02T13:29:14.253068: step 12519, loss 0.515341.
Train: 2018-08-02T13:29:14.409316: step 12520, loss 0.533081.
Test: 2018-08-02T13:29:14.862313: step 12520, loss 0.551753.
Train: 2018-08-02T13:29:15.018515: step 12521, loss 0.495848.
Train: 2018-08-02T13:29:15.174727: step 12522, loss 0.466011.
Train: 2018-08-02T13:29:15.315354: step 12523, loss 0.601173.
Train: 2018-08-02T13:29:15.471563: step 12524, loss 0.501881.
Train: 2018-08-02T13:29:15.627776: step 12525, loss 0.458487.
Train: 2018-08-02T13:29:15.783990: step 12526, loss 0.534206.
Train: 2018-08-02T13:29:15.924551: step 12527, loss 0.44103.
Train: 2018-08-02T13:29:16.080765: step 12528, loss 0.612724.
Train: 2018-08-02T13:29:16.237010: step 12529, loss 0.502736.
Train: 2018-08-02T13:29:16.393222: step 12530, loss 0.541891.
Test: 2018-08-02T13:29:16.846211: step 12530, loss 0.548221.
Train: 2018-08-02T13:29:17.002455: step 12531, loss 0.551189.
Train: 2018-08-02T13:29:17.158669: step 12532, loss 0.544083.
Train: 2018-08-02T13:29:17.268018: step 12533, loss 0.513077.
Train: 2018-08-02T13:29:17.424231: step 12534, loss 0.480907.
Train: 2018-08-02T13:29:17.580440: step 12535, loss 0.536575.
Train: 2018-08-02T13:29:17.736657: step 12536, loss 0.584942.
Train: 2018-08-02T13:29:17.877250: step 12537, loss 0.496763.
Train: 2018-08-02T13:29:18.033463: step 12538, loss 0.47723.
Train: 2018-08-02T13:29:18.189672: step 12539, loss 0.570224.
Train: 2018-08-02T13:29:18.345886: step 12540, loss 0.480486.
Test: 2018-08-02T13:29:18.814527: step 12540, loss 0.548137.
Train: 2018-08-02T13:29:18.955093: step 12541, loss 0.489342.
Train: 2018-08-02T13:29:19.111336: step 12542, loss 0.47947.
Train: 2018-08-02T13:29:19.267552: step 12543, loss 0.46885.
Train: 2018-08-02T13:29:19.423732: step 12544, loss 0.463299.
Train: 2018-08-02T13:29:19.579970: step 12545, loss 0.477895.
Train: 2018-08-02T13:29:19.720569: step 12546, loss 0.555523.
Train: 2018-08-02T13:29:19.876781: step 12547, loss 0.513992.
Train: 2018-08-02T13:29:20.032994: step 12548, loss 0.431004.
Train: 2018-08-02T13:29:20.189208: step 12549, loss 0.540076.
Train: 2018-08-02T13:29:20.345421: step 12550, loss 0.468074.
Test: 2018-08-02T13:29:20.798441: step 12550, loss 0.547651.
Train: 2018-08-02T13:29:20.954654: step 12551, loss 0.481651.
Train: 2018-08-02T13:29:21.095217: step 12552, loss 0.570398.
Train: 2018-08-02T13:29:21.251459: step 12553, loss 0.448258.
Train: 2018-08-02T13:29:21.407643: step 12554, loss 0.487471.
Train: 2018-08-02T13:29:21.563886: step 12555, loss 0.485253.
Train: 2018-08-02T13:29:21.720100: step 12556, loss 0.48568.
Train: 2018-08-02T13:29:21.860691: step 12557, loss 0.459156.
Train: 2018-08-02T13:29:22.016905: step 12558, loss 0.527253.
Train: 2018-08-02T13:29:22.173119: step 12559, loss 0.418108.
Train: 2018-08-02T13:29:22.329333: step 12560, loss 0.430183.
Test: 2018-08-02T13:29:22.782322: step 12560, loss 0.54941.
Train: 2018-08-02T13:29:22.938534: step 12561, loss 0.592273.
Train: 2018-08-02T13:29:23.094779: step 12562, loss 0.547415.
Train: 2018-08-02T13:29:23.235372: step 12563, loss 0.511034.
Train: 2018-08-02T13:29:23.391583: step 12564, loss 0.477705.
Train: 2018-08-02T13:29:23.547798: step 12565, loss 0.554708.
Train: 2018-08-02T13:29:23.704029: step 12566, loss 0.516043.
Train: 2018-08-02T13:29:23.860224: step 12567, loss 0.542089.
Train: 2018-08-02T13:29:24.016439: step 12568, loss 0.564842.
Train: 2018-08-02T13:29:24.157028: step 12569, loss 0.5628.
Train: 2018-08-02T13:29:24.313242: step 12570, loss 0.499604.
Test: 2018-08-02T13:29:24.766257: step 12570, loss 0.550742.
Train: 2018-08-02T13:29:24.922475: step 12571, loss 0.477504.
Train: 2018-08-02T13:29:25.078658: step 12572, loss 0.524566.
Train: 2018-08-02T13:29:25.234871: step 12573, loss 0.487975.
Train: 2018-08-02T13:29:25.375464: step 12574, loss 0.596696.
Train: 2018-08-02T13:29:25.531677: step 12575, loss 0.589851.
Train: 2018-08-02T13:29:25.687920: step 12576, loss 0.563652.
Train: 2018-08-02T13:29:25.844134: step 12577, loss 0.555504.
Train: 2018-08-02T13:29:26.000318: step 12578, loss 0.504532.
Train: 2018-08-02T13:29:26.140940: step 12579, loss 0.52703.
Train: 2018-08-02T13:29:26.297157: step 12580, loss 0.562652.
Test: 2018-08-02T13:29:26.750142: step 12580, loss 0.55949.
Train: 2018-08-02T13:29:26.906385: step 12581, loss 0.595837.
Train: 2018-08-02T13:29:27.062599: step 12582, loss 0.544972.
Train: 2018-08-02T13:29:27.218812: step 12583, loss 0.566177.
Train: 2018-08-02T13:29:27.359404: step 12584, loss 0.50927.
Train: 2018-08-02T13:29:27.515587: step 12585, loss 0.585415.
Train: 2018-08-02T13:29:27.671830: step 12586, loss 0.477504.
Train: 2018-08-02T13:29:27.828015: step 12587, loss 0.49834.
Train: 2018-08-02T13:29:27.968606: step 12588, loss 0.488292.
Train: 2018-08-02T13:29:28.124851: step 12589, loss 0.503839.
Train: 2018-08-02T13:29:28.281068: step 12590, loss 0.537879.
Test: 2018-08-02T13:29:28.734077: step 12590, loss 0.549735.
Train: 2018-08-02T13:29:28.890291: step 12591, loss 0.461259.
Train: 2018-08-02T13:29:29.046510: step 12592, loss 0.620535.
Train: 2018-08-02T13:29:29.202723: step 12593, loss 0.549688.
Train: 2018-08-02T13:29:29.358905: step 12594, loss 0.480131.
Train: 2018-08-02T13:29:29.515150: step 12595, loss 0.511325.
Train: 2018-08-02T13:29:29.655746: step 12596, loss 0.530174.
Train: 2018-08-02T13:29:29.811924: step 12597, loss 0.530759.
Train: 2018-08-02T13:29:29.968164: step 12598, loss 0.551732.
Train: 2018-08-02T13:29:30.124353: step 12599, loss 0.625191.
Train: 2018-08-02T13:29:30.280600: step 12600, loss 0.515011.
Test: 2018-08-02T13:29:30.733584: step 12600, loss 0.555639.
Train: 2018-08-02T13:29:31.342846: step 12601, loss 0.54518.
Train: 2018-08-02T13:29:31.499060: step 12602, loss 0.495732.
Train: 2018-08-02T13:29:31.655273: step 12603, loss 0.537895.
Train: 2018-08-02T13:29:31.795866: step 12604, loss 0.46705.
Train: 2018-08-02T13:29:31.952079: step 12605, loss 0.509723.
Train: 2018-08-02T13:29:32.108287: step 12606, loss 0.528375.
Train: 2018-08-02T13:29:32.264476: step 12607, loss 0.463219.
Train: 2018-08-02T13:29:32.405102: step 12608, loss 0.505358.
Train: 2018-08-02T13:29:32.561311: step 12609, loss 0.523184.
Train: 2018-08-02T13:29:32.717524: step 12610, loss 0.519209.
Test: 2018-08-02T13:29:33.170515: step 12610, loss 0.549925.
Train: 2018-08-02T13:29:33.326758: step 12611, loss 0.553862.
Train: 2018-08-02T13:29:33.482970: step 12612, loss 0.518774.
Train: 2018-08-02T13:29:33.639184: step 12613, loss 0.502521.
Train: 2018-08-02T13:29:33.779780: step 12614, loss 0.525505.
Train: 2018-08-02T13:29:33.935989: step 12615, loss 0.486755.
Train: 2018-08-02T13:29:34.092203: step 12616, loss 0.522909.
Train: 2018-08-02T13:29:34.248392: step 12617, loss 0.584715.
Train: 2018-08-02T13:29:34.389008: step 12618, loss 0.545133.
Train: 2018-08-02T13:29:34.545222: step 12619, loss 0.548643.
Train: 2018-08-02T13:29:34.701404: step 12620, loss 0.466538.
Test: 2018-08-02T13:29:35.154423: step 12620, loss 0.555213.
Train: 2018-08-02T13:29:35.310665: step 12621, loss 0.558228.
Train: 2018-08-02T13:29:35.466881: step 12622, loss 0.467203.
Train: 2018-08-02T13:29:35.623094: step 12623, loss 0.498736.
Train: 2018-08-02T13:29:35.779307: step 12624, loss 0.468807.
Train: 2018-08-02T13:29:35.919903: step 12625, loss 0.501612.
Train: 2018-08-02T13:29:36.076083: step 12626, loss 0.579002.
Train: 2018-08-02T13:29:36.232297: step 12627, loss 0.509819.
Train: 2018-08-02T13:29:36.388541: step 12628, loss 0.559193.
Train: 2018-08-02T13:29:36.544753: step 12629, loss 0.549779.
Train: 2018-08-02T13:29:36.685345: step 12630, loss 0.575446.
Test: 2018-08-02T13:29:37.138361: step 12630, loss 0.557105.
Train: 2018-08-02T13:29:37.294577: step 12631, loss 0.508744.
Train: 2018-08-02T13:29:37.450761: step 12632, loss 0.569938.
Train: 2018-08-02T13:29:37.607004: step 12633, loss 0.487169.
Train: 2018-08-02T13:29:37.763218: step 12634, loss 0.442249.
Train: 2018-08-02T13:29:37.903780: step 12635, loss 0.563556.
Train: 2018-08-02T13:29:38.059994: step 12636, loss 0.499882.
Train: 2018-08-02T13:29:38.216207: step 12637, loss 0.565507.
Train: 2018-08-02T13:29:38.372420: step 12638, loss 0.526071.
Train: 2018-08-02T13:29:38.528668: step 12639, loss 0.588811.
Train: 2018-08-02T13:29:38.669256: step 12640, loss 0.459975.
Test: 2018-08-02T13:29:39.137894: step 12640, loss 0.548196.
Train: 2018-08-02T13:29:39.294109: step 12641, loss 0.483362.
Train: 2018-08-02T13:29:39.434671: step 12642, loss 0.504255.
Train: 2018-08-02T13:29:39.590915: step 12643, loss 0.520394.
Train: 2018-08-02T13:29:39.747128: step 12644, loss 0.407372.
Train: 2018-08-02T13:29:39.903346: step 12645, loss 0.455163.
Train: 2018-08-02T13:29:40.059555: step 12646, loss 0.555442.
Train: 2018-08-02T13:29:40.200147: step 12647, loss 0.509244.
Train: 2018-08-02T13:29:40.356361: step 12648, loss 0.465109.
Train: 2018-08-02T13:29:40.512572: step 12649, loss 0.49806.
Train: 2018-08-02T13:29:40.668788: step 12650, loss 0.597044.
Test: 2018-08-02T13:29:41.121807: step 12650, loss 0.551493.
Train: 2018-08-02T13:29:41.278017: step 12651, loss 0.544707.
Train: 2018-08-02T13:29:41.434233: step 12652, loss 0.475314.
Train: 2018-08-02T13:29:41.574825: step 12653, loss 0.473749.
Train: 2018-08-02T13:29:41.731039: step 12654, loss 0.590127.
Train: 2018-08-02T13:29:41.887252: step 12655, loss 0.524015.
Train: 2018-08-02T13:29:42.043435: step 12656, loss 0.520592.
Train: 2018-08-02T13:29:42.199679: step 12657, loss 0.46987.
Train: 2018-08-02T13:29:42.355863: step 12658, loss 0.522934.
Train: 2018-08-02T13:29:42.496485: step 12659, loss 0.569049.
Train: 2018-08-02T13:29:42.652697: step 12660, loss 0.575816.
Test: 2018-08-02T13:29:43.105686: step 12660, loss 0.55168.
Train: 2018-08-02T13:29:43.261900: step 12661, loss 0.484103.
Train: 2018-08-02T13:29:43.418114: step 12662, loss 0.568866.
Train: 2018-08-02T13:29:43.574357: step 12663, loss 0.499541.
Train: 2018-08-02T13:29:43.746198: step 12664, loss 0.586008.
Train: 2018-08-02T13:29:43.917996: step 12665, loss 0.504412.
Train: 2018-08-02T13:29:44.074211: step 12666, loss 0.529455.
Train: 2018-08-02T13:29:44.230453: step 12667, loss 0.48335.
Train: 2018-08-02T13:29:44.371045: step 12668, loss 0.530168.
Train: 2018-08-02T13:29:44.527229: step 12669, loss 0.558043.
Train: 2018-08-02T13:29:44.683473: step 12670, loss 0.464362.
Test: 2018-08-02T13:29:45.136461: step 12670, loss 0.548338.
Train: 2018-08-02T13:29:45.292705: step 12671, loss 0.464434.
Train: 2018-08-02T13:29:45.448918: step 12672, loss 0.501631.
Train: 2018-08-02T13:29:45.605132: step 12673, loss 0.464972.
Train: 2018-08-02T13:29:45.761315: step 12674, loss 0.557376.
Train: 2018-08-02T13:29:45.917529: step 12675, loss 0.443473.
Train: 2018-08-02T13:29:46.073744: step 12676, loss 0.521188.
Train: 2018-08-02T13:29:46.229986: step 12677, loss 0.425925.
Train: 2018-08-02T13:29:46.370574: step 12678, loss 0.523643.
Train: 2018-08-02T13:29:46.526786: step 12679, loss 0.552563.
Train: 2018-08-02T13:29:46.683004: step 12680, loss 0.447583.
Test: 2018-08-02T13:29:47.136018: step 12680, loss 0.547859.
Train: 2018-08-02T13:29:47.292238: step 12681, loss 0.523717.
Train: 2018-08-02T13:29:47.448449: step 12682, loss 0.531826.
Train: 2018-08-02T13:29:47.604666: step 12683, loss 0.582599.
Train: 2018-08-02T13:29:47.714013: step 12684, loss 0.399053.
Train: 2018-08-02T13:29:47.854605: step 12685, loss 0.586194.
Train: 2018-08-02T13:29:48.010818: step 12686, loss 0.498983.
Train: 2018-08-02T13:29:48.167033: step 12687, loss 0.458177.
Train: 2018-08-02T13:29:48.323245: step 12688, loss 0.47649.
Train: 2018-08-02T13:29:48.479459: step 12689, loss 0.592394.
Train: 2018-08-02T13:29:48.620051: step 12690, loss 0.43012.
Test: 2018-08-02T13:29:49.088661: step 12690, loss 0.5475.
Train: 2018-08-02T13:29:49.229283: step 12691, loss 0.596042.
Train: 2018-08-02T13:29:49.385496: step 12692, loss 0.572439.
Train: 2018-08-02T13:29:49.541680: step 12693, loss 0.567915.
Train: 2018-08-02T13:29:49.697894: step 12694, loss 0.52868.
Train: 2018-08-02T13:29:49.838515: step 12695, loss 0.505914.
Train: 2018-08-02T13:29:49.994728: step 12696, loss 0.495203.
Train: 2018-08-02T13:29:50.150937: step 12697, loss 0.557933.
Train: 2018-08-02T13:29:50.307155: step 12698, loss 0.5329.
Train: 2018-08-02T13:29:50.463338: step 12699, loss 0.575141.
Train: 2018-08-02T13:29:50.603932: step 12700, loss 0.483036.
Test: 2018-08-02T13:29:51.072596: step 12700, loss 0.555037.
Train: 2018-08-02T13:29:51.713076: step 12701, loss 0.526723.
Train: 2018-08-02T13:29:51.853668: step 12702, loss 0.490834.
Train: 2018-08-02T13:29:52.009882: step 12703, loss 0.498199.
Train: 2018-08-02T13:29:52.166095: step 12704, loss 0.552474.
Train: 2018-08-02T13:29:52.322278: step 12705, loss 0.530456.
Train: 2018-08-02T13:29:52.462900: step 12706, loss 0.535858.
Train: 2018-08-02T13:29:52.619114: step 12707, loss 0.45878.
Train: 2018-08-02T13:29:52.775322: step 12708, loss 0.526515.
Train: 2018-08-02T13:29:52.931541: step 12709, loss 0.481871.
Train: 2018-08-02T13:29:53.087754: step 12710, loss 0.480766.
Test: 2018-08-02T13:29:53.540744: step 12710, loss 0.54818.
Train: 2018-08-02T13:29:53.696956: step 12711, loss 0.514646.
Train: 2018-08-02T13:29:53.853200: step 12712, loss 0.526606.
Train: 2018-08-02T13:29:54.009414: step 12713, loss 0.512545.
Train: 2018-08-02T13:29:54.149975: step 12714, loss 0.505506.
Train: 2018-08-02T13:29:54.306189: step 12715, loss 0.513132.
Train: 2018-08-02T13:29:54.462432: step 12716, loss 0.650138.
Train: 2018-08-02T13:29:54.618615: step 12717, loss 0.524039.
Train: 2018-08-02T13:29:54.774864: step 12718, loss 0.514789.
Train: 2018-08-02T13:29:54.931073: step 12719, loss 0.562967.
Train: 2018-08-02T13:29:55.087289: step 12720, loss 0.536011.
Test: 2018-08-02T13:29:55.540305: step 12720, loss 0.555005.
Train: 2018-08-02T13:29:55.696519: step 12721, loss 0.546033.
Train: 2018-08-02T13:29:55.852733: step 12722, loss 0.536897.
Train: 2018-08-02T13:29:55.993324: step 12723, loss 0.530378.
Train: 2018-08-02T13:29:56.149537: step 12724, loss 0.591107.
Train: 2018-08-02T13:29:56.305751: step 12725, loss 0.550716.
Train: 2018-08-02T13:29:56.461959: step 12726, loss 0.599185.
Train: 2018-08-02T13:29:56.602556: step 12727, loss 0.630097.
Train: 2018-08-02T13:29:56.758771: step 12728, loss 0.530554.
Train: 2018-08-02T13:29:56.914985: step 12729, loss 0.520393.
Train: 2018-08-02T13:29:57.071196: step 12730, loss 0.547307.
Test: 2018-08-02T13:29:57.539807: step 12730, loss 0.563122.
Train: 2018-08-02T13:29:57.696056: step 12731, loss 0.539466.
Train: 2018-08-02T13:29:57.852265: step 12732, loss 0.539259.
Train: 2018-08-02T13:29:58.008478: step 12733, loss 0.584127.
Train: 2018-08-02T13:29:58.164691: step 12734, loss 0.531315.
Train: 2018-08-02T13:29:58.336495: step 12735, loss 0.508643.
Train: 2018-08-02T13:29:58.492708: step 12736, loss 0.534363.
Train: 2018-08-02T13:29:58.648957: step 12737, loss 0.437509.
Train: 2018-08-02T13:29:58.789544: step 12738, loss 0.558229.
Train: 2018-08-02T13:29:58.945757: step 12739, loss 0.468052.
Train: 2018-08-02T13:29:59.101941: step 12740, loss 0.444763.
Test: 2018-08-02T13:29:59.554960: step 12740, loss 0.549311.
Train: 2018-08-02T13:29:59.711204: step 12741, loss 0.464182.
Train: 2018-08-02T13:29:59.867416: step 12742, loss 0.439176.
Train: 2018-08-02T13:30:00.023627: step 12743, loss 0.489001.
Train: 2018-08-02T13:30:00.164221: step 12744, loss 0.408052.
Train: 2018-08-02T13:30:00.320436: step 12745, loss 0.49036.
Train: 2018-08-02T13:30:00.476650: step 12746, loss 0.502885.
Train: 2018-08-02T13:30:00.632834: step 12747, loss 0.657624.
Train: 2018-08-02T13:30:00.789048: step 12748, loss 0.502406.
Train: 2018-08-02T13:30:00.945290: step 12749, loss 0.489797.
Train: 2018-08-02T13:30:01.101472: step 12750, loss 0.511575.
Test: 2018-08-02T13:30:01.554492: step 12750, loss 0.5615.
Train: 2018-08-02T13:30:01.726356: step 12751, loss 0.497333.
Train: 2018-08-02T13:30:01.882570: step 12752, loss 0.463851.
Train: 2018-08-02T13:30:02.038786: step 12753, loss 0.535553.
Train: 2018-08-02T13:30:02.179359: step 12754, loss 0.483406.
Train: 2018-08-02T13:30:02.335589: step 12755, loss 0.516039.
Train: 2018-08-02T13:30:02.491802: step 12756, loss 0.525954.
Train: 2018-08-02T13:30:02.648014: step 12757, loss 0.500897.
Train: 2018-08-02T13:30:02.804230: step 12758, loss 0.554589.
Train: 2018-08-02T13:30:02.960437: step 12759, loss 0.418827.
Train: 2018-08-02T13:30:03.116650: step 12760, loss 0.475086.
Test: 2018-08-02T13:30:03.569670: step 12760, loss 0.548886.
Train: 2018-08-02T13:30:03.725888: step 12761, loss 0.455452.
Train: 2018-08-02T13:30:03.882102: step 12762, loss 0.542294.
Train: 2018-08-02T13:30:04.053936: step 12763, loss 0.484063.
Train: 2018-08-02T13:30:04.194528: step 12764, loss 0.607665.
Train: 2018-08-02T13:30:04.350711: step 12765, loss 0.453056.
Train: 2018-08-02T13:30:04.506955: step 12766, loss 0.456831.
Train: 2018-08-02T13:30:04.663169: step 12767, loss 0.567687.
Train: 2018-08-02T13:30:04.803761: step 12768, loss 0.524964.
Train: 2018-08-02T13:30:04.959973: step 12769, loss 0.54845.
Train: 2018-08-02T13:30:05.116188: step 12770, loss 0.533749.
Test: 2018-08-02T13:30:05.569176: step 12770, loss 0.54789.
Train: 2018-08-02T13:30:05.725419: step 12771, loss 0.633687.
Train: 2018-08-02T13:30:05.881635: step 12772, loss 0.544578.
Train: 2018-08-02T13:30:06.022225: step 12773, loss 0.626731.
Train: 2018-08-02T13:30:06.178440: step 12774, loss 0.511656.
Train: 2018-08-02T13:30:06.334653: step 12775, loss 0.538206.
Train: 2018-08-02T13:30:06.490862: step 12776, loss 0.53126.
Train: 2018-08-02T13:30:06.647049: step 12777, loss 0.473391.
Train: 2018-08-02T13:30:06.803264: step 12778, loss 0.591637.
Train: 2018-08-02T13:30:06.959478: step 12779, loss 0.504365.
Train: 2018-08-02T13:30:07.115720: step 12780, loss 0.533908.
Test: 2018-08-02T13:30:07.568733: step 12780, loss 0.547543.
Train: 2018-08-02T13:30:07.724952: step 12781, loss 0.468228.
Train: 2018-08-02T13:30:07.896786: step 12782, loss 0.473124.
Train: 2018-08-02T13:30:08.053000: step 12783, loss 0.474208.
Train: 2018-08-02T13:30:08.193592: step 12784, loss 0.572187.
Train: 2018-08-02T13:30:08.349805: step 12785, loss 0.590431.
Train: 2018-08-02T13:30:08.506019: step 12786, loss 0.565722.
Train: 2018-08-02T13:30:08.662244: step 12787, loss 0.577428.
Train: 2018-08-02T13:30:08.802825: step 12788, loss 0.574888.
Train: 2018-08-02T13:30:08.959038: step 12789, loss 0.553487.
Train: 2018-08-02T13:30:09.115222: step 12790, loss 0.543238.
Test: 2018-08-02T13:30:09.583862: step 12790, loss 0.568135.
Train: 2018-08-02T13:30:09.724484: step 12791, loss 0.564195.
Train: 2018-08-02T13:30:09.880666: step 12792, loss 0.553687.
Train: 2018-08-02T13:30:10.036910: step 12793, loss 0.518663.
Train: 2018-08-02T13:30:10.193123: step 12794, loss 0.454072.
Train: 2018-08-02T13:30:10.333686: step 12795, loss 0.481205.
Train: 2018-08-02T13:30:10.489930: step 12796, loss 0.682021.
Train: 2018-08-02T13:30:10.646143: step 12797, loss 0.486571.
Train: 2018-08-02T13:30:10.802358: step 12798, loss 0.560707.
Train: 2018-08-02T13:30:10.958571: step 12799, loss 0.587921.
Train: 2018-08-02T13:30:11.114754: step 12800, loss 0.499033.
Test: 2018-08-02T13:30:11.567772: step 12800, loss 0.56208.
Train: 2018-08-02T13:30:12.192655: step 12801, loss 0.532423.
Train: 2018-08-02T13:30:12.348869: step 12802, loss 0.47139.
Train: 2018-08-02T13:30:12.505082: step 12803, loss 0.519807.
Train: 2018-08-02T13:30:12.661297: step 12804, loss 0.496049.
Train: 2018-08-02T13:30:12.817504: step 12805, loss 0.549729.
Train: 2018-08-02T13:30:12.973723: step 12806, loss 0.533281.
Train: 2018-08-02T13:30:13.129939: step 12807, loss 0.479255.
Train: 2018-08-02T13:30:13.286150: step 12808, loss 0.554714.
Train: 2018-08-02T13:30:13.442332: step 12809, loss 0.575993.
Train: 2018-08-02T13:30:13.598576: step 12810, loss 0.483898.
Test: 2018-08-02T13:30:14.051589: step 12810, loss 0.55101.
Train: 2018-08-02T13:30:14.207809: step 12811, loss 0.547645.
Train: 2018-08-02T13:30:14.364022: step 12812, loss 0.582317.
Train: 2018-08-02T13:30:14.520231: step 12813, loss 0.526599.
Train: 2018-08-02T13:30:14.676449: step 12814, loss 0.533523.
Train: 2018-08-02T13:30:14.832657: step 12815, loss 0.46037.
Train: 2018-08-02T13:30:14.988845: step 12816, loss 0.629906.
Train: 2018-08-02T13:30:15.145059: step 12817, loss 0.470226.
Train: 2018-08-02T13:30:15.301273: step 12818, loss 0.533411.
Train: 2018-08-02T13:30:15.457515: step 12819, loss 0.479617.
Train: 2018-08-02T13:30:15.613700: step 12820, loss 0.501201.
Test: 2018-08-02T13:30:16.066719: step 12820, loss 0.54831.
Train: 2018-08-02T13:30:16.222962: step 12821, loss 0.459813.
Train: 2018-08-02T13:30:16.379174: step 12822, loss 0.451932.
Train: 2018-08-02T13:30:16.519736: step 12823, loss 0.453604.
Train: 2018-08-02T13:30:16.675981: step 12824, loss 0.516178.
Train: 2018-08-02T13:30:16.832194: step 12825, loss 0.481366.
Train: 2018-08-02T13:30:16.988407: step 12826, loss 0.523731.
Train: 2018-08-02T13:30:17.144620: step 12827, loss 0.492357.
Train: 2018-08-02T13:30:17.300829: step 12828, loss 0.492149.
Train: 2018-08-02T13:30:17.457049: step 12829, loss 0.494899.
Train: 2018-08-02T13:30:17.613231: step 12830, loss 0.568248.
Test: 2018-08-02T13:30:18.066250: step 12830, loss 0.551655.
Train: 2018-08-02T13:30:18.222463: step 12831, loss 0.494524.
Train: 2018-08-02T13:30:18.378707: step 12832, loss 0.553177.
Train: 2018-08-02T13:30:18.534921: step 12833, loss 0.475228.
Train: 2018-08-02T13:30:18.691133: step 12834, loss 0.45775.
Train: 2018-08-02T13:30:18.800484: step 12835, loss 0.553465.
Train: 2018-08-02T13:30:18.956697: step 12836, loss 0.599218.
Train: 2018-08-02T13:30:19.112905: step 12837, loss 0.446559.
Train: 2018-08-02T13:30:19.269093: step 12838, loss 0.488581.
Train: 2018-08-02T13:30:19.409686: step 12839, loss 0.500645.
Train: 2018-08-02T13:30:19.565930: step 12840, loss 0.556314.
Test: 2018-08-02T13:30:20.034541: step 12840, loss 0.547563.
Train: 2018-08-02T13:30:20.190783: step 12841, loss 0.535628.
Train: 2018-08-02T13:30:20.346966: step 12842, loss 0.565963.
Train: 2018-08-02T13:30:20.487588: step 12843, loss 0.39928.
Train: 2018-08-02T13:30:20.643797: step 12844, loss 0.441177.
Train: 2018-08-02T13:30:20.800014: step 12845, loss 0.491391.
Train: 2018-08-02T13:30:20.956228: step 12846, loss 0.542933.
Train: 2018-08-02T13:30:21.112442: step 12847, loss 0.528762.
Train: 2018-08-02T13:30:21.268655: step 12848, loss 0.47726.
Train: 2018-08-02T13:30:21.424839: step 12849, loss 0.575121.
Train: 2018-08-02T13:30:21.565462: step 12850, loss 0.454129.
Test: 2018-08-02T13:30:22.034070: step 12850, loss 0.550034.
Train: 2018-08-02T13:30:22.174693: step 12851, loss 0.561468.
Train: 2018-08-02T13:30:22.330906: step 12852, loss 0.48917.
Train: 2018-08-02T13:30:22.487120: step 12853, loss 0.424036.
Train: 2018-08-02T13:30:22.643333: step 12854, loss 0.507651.
Train: 2018-08-02T13:30:22.815163: step 12855, loss 0.594865.
Train: 2018-08-02T13:30:22.971382: step 12856, loss 0.452396.
Train: 2018-08-02T13:30:23.127564: step 12857, loss 0.425617.
Train: 2018-08-02T13:30:23.283778: step 12858, loss 0.48316.
Train: 2018-08-02T13:30:23.424400: step 12859, loss 0.430599.
Train: 2018-08-02T13:30:23.580613: step 12860, loss 0.527336.
Test: 2018-08-02T13:30:24.049257: step 12860, loss 0.55046.
Train: 2018-08-02T13:30:24.189816: step 12861, loss 0.458532.
Train: 2018-08-02T13:30:24.361675: step 12862, loss 0.405351.
Train: 2018-08-02T13:30:24.517864: step 12863, loss 0.540308.
Train: 2018-08-02T13:30:24.674078: step 12864, loss 0.518322.
Train: 2018-08-02T13:30:24.830324: step 12865, loss 0.448732.
Train: 2018-08-02T13:30:24.986534: step 12866, loss 0.474445.
Train: 2018-08-02T13:30:25.142752: step 12867, loss 0.501127.
Train: 2018-08-02T13:30:25.283344: step 12868, loss 0.520715.
Train: 2018-08-02T13:30:25.439524: step 12869, loss 0.459733.
Train: 2018-08-02T13:30:25.595737: step 12870, loss 0.535266.
Test: 2018-08-02T13:30:26.048756: step 12870, loss 0.547076.
Train: 2018-08-02T13:30:26.205000: step 12871, loss 0.554414.
Train: 2018-08-02T13:30:26.361215: step 12872, loss 0.571914.
Train: 2018-08-02T13:30:26.517426: step 12873, loss 0.562509.
Train: 2018-08-02T13:30:26.673640: step 12874, loss 0.536262.
Train: 2018-08-02T13:30:26.829871: step 12875, loss 0.537571.
Train: 2018-08-02T13:30:26.986061: step 12876, loss 0.511957.
Train: 2018-08-02T13:30:27.142250: step 12877, loss 0.546857.
Train: 2018-08-02T13:30:27.314086: step 12878, loss 0.490509.
Train: 2018-08-02T13:30:27.470297: step 12879, loss 0.477811.
Train: 2018-08-02T13:30:27.626558: step 12880, loss 0.556744.
Test: 2018-08-02T13:30:28.079544: step 12880, loss 0.547476.
Train: 2018-08-02T13:30:28.235773: step 12881, loss 0.672956.
Train: 2018-08-02T13:30:28.391991: step 12882, loss 0.469735.
Train: 2018-08-02T13:30:28.548199: step 12883, loss 0.567025.
Train: 2018-08-02T13:30:28.704416: step 12884, loss 0.513867.
Train: 2018-08-02T13:30:28.860627: step 12885, loss 0.532481.
Train: 2018-08-02T13:30:29.016840: step 12886, loss 0.521545.
Train: 2018-08-02T13:30:29.173055: step 12887, loss 0.511291.
Train: 2018-08-02T13:30:29.329263: step 12888, loss 0.548066.
Train: 2018-08-02T13:30:29.485476: step 12889, loss 0.556368.
Train: 2018-08-02T13:30:29.641679: step 12890, loss 0.509907.
Test: 2018-08-02T13:30:30.094709: step 12890, loss 0.553452.
Train: 2018-08-02T13:30:30.250926: step 12891, loss 0.565089.
Train: 2018-08-02T13:30:30.407140: step 12892, loss 0.529353.
Train: 2018-08-02T13:30:30.563355: step 12893, loss 0.51466.
Train: 2018-08-02T13:30:30.719567: step 12894, loss 0.657109.
Train: 2018-08-02T13:30:30.875781: step 12895, loss 0.460846.
Train: 2018-08-02T13:30:31.016377: step 12896, loss 0.480331.
Train: 2018-08-02T13:30:31.172582: step 12897, loss 0.45224.
Train: 2018-08-02T13:30:31.328799: step 12898, loss 0.569995.
Train: 2018-08-02T13:30:31.485007: step 12899, loss 0.467567.
Train: 2018-08-02T13:30:31.641226: step 12900, loss 0.43463.
Test: 2018-08-02T13:30:32.094215: step 12900, loss 0.548106.
Train: 2018-08-02T13:30:32.719101: step 12901, loss 0.636531.
Train: 2018-08-02T13:30:32.875309: step 12902, loss 0.466287.
Train: 2018-08-02T13:30:33.031526: step 12903, loss 0.549435.
Train: 2018-08-02T13:30:33.187739: step 12904, loss 0.497656.
Train: 2018-08-02T13:30:33.343953: step 12905, loss 0.519751.
Train: 2018-08-02T13:30:33.500138: step 12906, loss 0.475541.
Train: 2018-08-02T13:30:33.656350: step 12907, loss 0.48573.
Train: 2018-08-02T13:30:33.812592: step 12908, loss 0.555978.
Train: 2018-08-02T13:30:33.953185: step 12909, loss 0.533953.
Train: 2018-08-02T13:30:34.109398: step 12910, loss 0.591779.
Test: 2018-08-02T13:30:34.562388: step 12910, loss 0.556009.
Train: 2018-08-02T13:30:34.718630: step 12911, loss 0.533728.
Train: 2018-08-02T13:30:34.921708: step 12912, loss 0.512249.
Train: 2018-08-02T13:30:35.077921: step 12913, loss 0.534735.
Train: 2018-08-02T13:30:35.234137: step 12914, loss 0.510531.
Train: 2018-08-02T13:30:35.374727: step 12915, loss 0.572163.
Train: 2018-08-02T13:30:35.530941: step 12916, loss 0.555217.
Train: 2018-08-02T13:30:35.702776: step 12917, loss 0.572909.
Train: 2018-08-02T13:30:35.858984: step 12918, loss 0.514806.
Train: 2018-08-02T13:30:36.015197: step 12919, loss 0.520214.
Train: 2018-08-02T13:30:36.171385: step 12920, loss 0.501291.
Test: 2018-08-02T13:30:36.624404: step 12920, loss 0.556386.
Train: 2018-08-02T13:30:36.780648: step 12921, loss 0.523303.
Train: 2018-08-02T13:30:36.936830: step 12922, loss 0.51204.
Train: 2018-08-02T13:30:37.108699: step 12923, loss 0.540382.
Train: 2018-08-02T13:30:37.264904: step 12924, loss 0.556999.
Train: 2018-08-02T13:30:37.421118: step 12925, loss 0.435191.
Train: 2018-08-02T13:30:37.577305: step 12926, loss 0.450843.
Train: 2018-08-02T13:30:37.733519: step 12927, loss 0.466958.
Train: 2018-08-02T13:30:37.889762: step 12928, loss 0.513108.
Train: 2018-08-02T13:30:38.030355: step 12929, loss 0.458392.
Train: 2018-08-02T13:30:38.186568: step 12930, loss 0.417515.
Test: 2018-08-02T13:30:38.655179: step 12930, loss 0.549651.
Train: 2018-08-02T13:30:38.811422: step 12931, loss 0.534971.
Train: 2018-08-02T13:30:38.967641: step 12932, loss 0.480887.
Train: 2018-08-02T13:30:39.108199: step 12933, loss 0.410059.
Train: 2018-08-02T13:30:39.264441: step 12934, loss 0.479522.
Train: 2018-08-02T13:30:39.436276: step 12935, loss 0.418321.
Train: 2018-08-02T13:30:39.592489: step 12936, loss 0.589098.
Train: 2018-08-02T13:30:39.748673: step 12937, loss 0.547078.
Train: 2018-08-02T13:30:39.904887: step 12938, loss 0.48598.
Train: 2018-08-02T13:30:40.061130: step 12939, loss 0.494215.
Train: 2018-08-02T13:30:40.217343: step 12940, loss 0.553362.
Test: 2018-08-02T13:30:40.670332: step 12940, loss 0.553584.
Train: 2018-08-02T13:30:40.826548: step 12941, loss 0.482265.
Train: 2018-08-02T13:30:40.982788: step 12942, loss 0.483939.
Train: 2018-08-02T13:30:41.139007: step 12943, loss 0.617374.
Train: 2018-08-02T13:30:41.295184: step 12944, loss 0.548917.
Train: 2018-08-02T13:30:41.435811: step 12945, loss 0.446779.
Train: 2018-08-02T13:30:41.591990: step 12946, loss 0.516512.
Train: 2018-08-02T13:30:41.748235: step 12947, loss 0.528029.
Train: 2018-08-02T13:30:41.920070: step 12948, loss 0.61315.
Train: 2018-08-02T13:30:42.076254: step 12949, loss 0.42301.
Train: 2018-08-02T13:30:42.232500: step 12950, loss 0.474374.
Test: 2018-08-02T13:30:42.685486: step 12950, loss 0.548234.
Train: 2018-08-02T13:30:42.841728: step 12951, loss 0.578905.
Train: 2018-08-02T13:30:42.997942: step 12952, loss 0.502769.
Train: 2018-08-02T13:30:43.154155: step 12953, loss 0.517226.
Train: 2018-08-02T13:30:43.294747: step 12954, loss 0.482757.
Train: 2018-08-02T13:30:43.450930: step 12955, loss 0.579098.
Train: 2018-08-02T13:30:43.607145: step 12956, loss 0.534736.
Train: 2018-08-02T13:30:43.763358: step 12957, loss 0.54208.
Train: 2018-08-02T13:30:43.919600: step 12958, loss 0.53468.
Train: 2018-08-02T13:30:44.060194: step 12959, loss 0.530078.
Train: 2018-08-02T13:30:44.216406: step 12960, loss 0.618857.
Test: 2018-08-02T13:30:44.669395: step 12960, loss 0.559575.
Train: 2018-08-02T13:30:44.825635: step 12961, loss 0.54161.
Train: 2018-08-02T13:30:44.997468: step 12962, loss 0.461947.
Train: 2018-08-02T13:30:45.153682: step 12963, loss 0.532972.
Train: 2018-08-02T13:30:45.309901: step 12964, loss 0.509315.
Train: 2018-08-02T13:30:45.466113: step 12965, loss 0.505543.
Train: 2018-08-02T13:30:45.622298: step 12966, loss 0.499276.
Train: 2018-08-02T13:30:45.762919: step 12967, loss 0.532326.
Train: 2018-08-02T13:30:45.934724: step 12968, loss 0.617958.
Train: 2018-08-02T13:30:46.090968: step 12969, loss 0.521901.
Train: 2018-08-02T13:30:46.247176: step 12970, loss 0.52585.
Test: 2018-08-02T13:30:46.700170: step 12970, loss 0.552499.
Train: 2018-08-02T13:30:46.856384: step 12971, loss 0.53281.
Train: 2018-08-02T13:30:47.012626: step 12972, loss 0.507264.
Train: 2018-08-02T13:30:47.153218: step 12973, loss 0.454606.
Train: 2018-08-02T13:30:47.309432: step 12974, loss 0.509567.
Train: 2018-08-02T13:30:47.465614: step 12975, loss 0.537622.
Train: 2018-08-02T13:30:47.621859: step 12976, loss 0.502759.
Train: 2018-08-02T13:30:47.778042: step 12977, loss 0.55453.
Train: 2018-08-02T13:30:47.934281: step 12978, loss 0.539681.
Train: 2018-08-02T13:30:48.090494: step 12979, loss 0.535036.
Train: 2018-08-02T13:30:48.262303: step 12980, loss 0.57471.
Test: 2018-08-02T13:30:48.715323: step 12980, loss 0.551209.
Train: 2018-08-02T13:30:48.871569: step 12981, loss 0.559282.
Train: 2018-08-02T13:30:49.027784: step 12982, loss 0.536556.
Train: 2018-08-02T13:30:49.183988: step 12983, loss 0.519325.
Train: 2018-08-02T13:30:49.340178: step 12984, loss 0.583017.
Train: 2018-08-02T13:30:49.496391: step 12985, loss 0.49204.
Train: 2018-08-02T13:30:49.605770: step 12986, loss 0.463507.
Train: 2018-08-02T13:30:49.761953: step 12987, loss 0.505098.
Train: 2018-08-02T13:30:49.918196: step 12988, loss 0.484952.
Train: 2018-08-02T13:30:50.074409: step 12989, loss 0.539576.
Train: 2018-08-02T13:30:50.246243: step 12990, loss 0.619508.
Test: 2018-08-02T13:30:50.699264: step 12990, loss 0.549697.
Train: 2018-08-02T13:30:50.902310: step 12991, loss 0.566295.
Train: 2018-08-02T13:30:51.058525: step 12992, loss 0.488773.
Train: 2018-08-02T13:30:51.214747: step 12993, loss 0.514838.
Train: 2018-08-02T13:30:51.370977: step 12994, loss 0.545504.
Train: 2018-08-02T13:30:51.527166: step 12995, loss 0.558704.
Train: 2018-08-02T13:30:51.683377: step 12996, loss 0.536571.
Train: 2018-08-02T13:30:51.839622: step 12997, loss 0.515141.
Train: 2018-08-02T13:30:51.995806: step 12998, loss 0.597355.
Train: 2018-08-02T13:30:52.167670: step 12999, loss 0.501058.
Train: 2018-08-02T13:30:52.323878: step 13000, loss 0.462498.
Test: 2018-08-02T13:30:52.776873: step 13000, loss 0.551.
Train: 2018-08-02T13:30:53.417376: step 13001, loss 0.465676.
Train: 2018-08-02T13:30:53.573591: step 13002, loss 0.491308.
Train: 2018-08-02T13:30:53.729775: step 13003, loss 0.47208.
Train: 2018-08-02T13:30:53.886030: step 13004, loss 0.406671.
Train: 2018-08-02T13:30:54.042233: step 13005, loss 0.453615.
Train: 2018-08-02T13:30:54.182792: step 13006, loss 0.504847.
Train: 2018-08-02T13:30:54.339036: step 13007, loss 0.522166.
Train: 2018-08-02T13:30:54.495249: step 13008, loss 0.512865.
Train: 2018-08-02T13:30:54.651460: step 13009, loss 0.506052.
Train: 2018-08-02T13:30:54.807676: step 13010, loss 0.539651.
Test: 2018-08-02T13:30:55.260666: step 13010, loss 0.549734.
Train: 2018-08-02T13:30:55.416911: step 13011, loss 0.656532.
Train: 2018-08-02T13:30:55.573122: step 13012, loss 0.522336.
Train: 2018-08-02T13:30:55.729331: step 13013, loss 0.482303.
Train: 2018-08-02T13:30:55.885520: step 13014, loss 0.529325.
Train: 2018-08-02T13:30:56.041765: step 13015, loss 0.497087.
Train: 2018-08-02T13:30:56.197946: step 13016, loss 0.49705.
Train: 2018-08-02T13:30:56.369782: step 13017, loss 0.49415.
Train: 2018-08-02T13:30:56.526003: step 13018, loss 0.521512.
Train: 2018-08-02T13:30:56.682206: step 13019, loss 0.533059.
Train: 2018-08-02T13:30:56.838452: step 13020, loss 0.530117.
Test: 2018-08-02T13:30:57.291464: step 13020, loss 0.550605.
Train: 2018-08-02T13:30:57.447678: step 13021, loss 0.602801.
Train: 2018-08-02T13:30:57.603896: step 13022, loss 0.526997.
Train: 2018-08-02T13:30:57.760104: step 13023, loss 0.531452.
Train: 2018-08-02T13:30:57.916323: step 13024, loss 0.529626.
Train: 2018-08-02T13:30:58.072536: step 13025, loss 0.58323.
Train: 2018-08-02T13:30:58.228749: step 13026, loss 0.539711.
Train: 2018-08-02T13:30:58.384933: step 13027, loss 0.569917.
Train: 2018-08-02T13:30:58.525556: step 13028, loss 0.529485.
Train: 2018-08-02T13:30:58.681769: step 13029, loss 0.576503.
Train: 2018-08-02T13:30:58.837984: step 13030, loss 0.57889.
Test: 2018-08-02T13:30:59.291002: step 13030, loss 0.560985.
Train: 2018-08-02T13:30:59.447215: step 13031, loss 0.523306.
Train: 2018-08-02T13:30:59.619048: step 13032, loss 0.507984.
Train: 2018-08-02T13:30:59.775234: step 13033, loss 0.569945.
Train: 2018-08-02T13:30:59.931447: step 13034, loss 0.504863.
Train: 2018-08-02T13:31:00.072068: step 13035, loss 0.493607.
Train: 2018-08-02T13:31:00.228281: step 13036, loss 0.445587.
Train: 2018-08-02T13:31:00.384495: step 13037, loss 0.410947.
Train: 2018-08-02T13:31:00.540704: step 13038, loss 0.496622.
Train: 2018-08-02T13:31:00.696891: step 13039, loss 0.537504.
Train: 2018-08-02T13:31:00.853135: step 13040, loss 0.55342.
Test: 2018-08-02T13:31:01.306131: step 13040, loss 0.55047.
Train: 2018-08-02T13:31:01.462364: step 13041, loss 0.530655.
Train: 2018-08-02T13:31:01.618579: step 13042, loss 0.56099.
Train: 2018-08-02T13:31:01.774790: step 13043, loss 0.505606.
Train: 2018-08-02T13:31:01.931004: step 13044, loss 0.484875.
Train: 2018-08-02T13:31:02.087218: step 13045, loss 0.541849.
Train: 2018-08-02T13:31:02.243434: step 13046, loss 0.440775.
Train: 2018-08-02T13:31:02.399644: step 13047, loss 0.476596.
Train: 2018-08-02T13:31:02.555862: step 13048, loss 0.587587.
Train: 2018-08-02T13:31:02.712076: step 13049, loss 0.554015.
Train: 2018-08-02T13:31:02.868259: step 13050, loss 0.461876.
Test: 2018-08-02T13:31:03.321277: step 13050, loss 0.547121.
Train: 2018-08-02T13:31:03.477521: step 13051, loss 0.59084.
Train: 2018-08-02T13:31:03.633734: step 13052, loss 0.4816.
Train: 2018-08-02T13:31:03.789942: step 13053, loss 0.628136.
Train: 2018-08-02T13:31:03.946131: step 13054, loss 0.502921.
Train: 2018-08-02T13:31:04.102370: step 13055, loss 0.465332.
Train: 2018-08-02T13:31:04.258588: step 13056, loss 0.488029.
Train: 2018-08-02T13:31:04.414772: step 13057, loss 0.570572.
Train: 2018-08-02T13:31:04.571016: step 13058, loss 0.572715.
Train: 2018-08-02T13:31:04.727229: step 13059, loss 0.505756.
Train: 2018-08-02T13:31:04.883442: step 13060, loss 0.509516.
Test: 2018-08-02T13:31:05.352077: step 13060, loss 0.549569.
Train: 2018-08-02T13:31:05.492675: step 13061, loss 0.533426.
Train: 2018-08-02T13:31:05.648889: step 13062, loss 0.57101.
Train: 2018-08-02T13:31:05.805097: step 13063, loss 0.573991.
Train: 2018-08-02T13:31:05.961314: step 13064, loss 0.475305.
Train: 2018-08-02T13:31:06.101906: step 13065, loss 0.526744.
Train: 2018-08-02T13:31:06.258119: step 13066, loss 0.579049.
Train: 2018-08-02T13:31:06.414333: step 13067, loss 0.50132.
Train: 2018-08-02T13:31:06.570516: step 13068, loss 0.446122.
Train: 2018-08-02T13:31:06.726730: step 13069, loss 0.53851.
Train: 2018-08-02T13:31:06.882944: step 13070, loss 0.473861.
Test: 2018-08-02T13:31:07.351583: step 13070, loss 0.551247.
Train: 2018-08-02T13:31:07.507821: step 13071, loss 0.469681.
Train: 2018-08-02T13:31:07.664042: step 13072, loss 0.561768.
Train: 2018-08-02T13:31:07.820224: step 13073, loss 0.511834.
Train: 2018-08-02T13:31:07.976437: step 13074, loss 0.536585.
Train: 2018-08-02T13:31:08.132680: step 13075, loss 0.530681.
Train: 2018-08-02T13:31:08.288894: step 13076, loss 0.521951.
Train: 2018-08-02T13:31:08.445103: step 13077, loss 0.503389.
Train: 2018-08-02T13:31:08.601323: step 13078, loss 0.533377.
Train: 2018-08-02T13:31:08.741883: step 13079, loss 0.526098.
Train: 2018-08-02T13:31:08.898126: step 13080, loss 0.546119.
Test: 2018-08-02T13:31:09.366737: step 13080, loss 0.547777.
Train: 2018-08-02T13:31:09.522981: step 13081, loss 0.515743.
Train: 2018-08-02T13:31:09.679195: step 13082, loss 0.540716.
Train: 2018-08-02T13:31:09.835407: step 13083, loss 0.476887.
Train: 2018-08-02T13:31:09.991620: step 13084, loss 0.604436.
Train: 2018-08-02T13:31:10.147805: step 13085, loss 0.478268.
Train: 2018-08-02T13:31:10.304018: step 13086, loss 0.562378.
Train: 2018-08-02T13:31:10.444609: step 13087, loss 0.49454.
Train: 2018-08-02T13:31:10.600851: step 13088, loss 0.574683.
Train: 2018-08-02T13:31:10.772688: step 13089, loss 0.511558.
Train: 2018-08-02T13:31:10.928903: step 13090, loss 0.488043.
Test: 2018-08-02T13:31:11.381915: step 13090, loss 0.552635.
Train: 2018-08-02T13:31:11.538103: step 13091, loss 0.509299.
Train: 2018-08-02T13:31:11.694316: step 13092, loss 0.562271.
Train: 2018-08-02T13:31:11.850529: step 13093, loss 0.642925.
Train: 2018-08-02T13:31:12.006778: step 13094, loss 0.496018.
Train: 2018-08-02T13:31:12.162958: step 13095, loss 0.431639.
Train: 2018-08-02T13:31:12.319201: step 13096, loss 0.508041.
Train: 2018-08-02T13:31:12.475414: step 13097, loss 0.491527.
Train: 2018-08-02T13:31:12.631597: step 13098, loss 0.50757.
Train: 2018-08-02T13:31:12.787842: step 13099, loss 0.499777.
Train: 2018-08-02T13:31:12.944024: step 13100, loss 0.462105.
Test: 2018-08-02T13:31:13.397046: step 13100, loss 0.545905.
Train: 2018-08-02T13:31:14.006304: step 13101, loss 0.550964.
Train: 2018-08-02T13:31:14.162519: step 13102, loss 0.611069.
Train: 2018-08-02T13:31:14.318702: step 13103, loss 0.51869.
Train: 2018-08-02T13:31:14.474916: step 13104, loss 0.513011.
Train: 2018-08-02T13:31:14.631153: step 13105, loss 0.540151.
Train: 2018-08-02T13:31:14.771751: step 13106, loss 0.537989.
Train: 2018-08-02T13:31:14.927965: step 13107, loss 0.53474.
Train: 2018-08-02T13:31:15.084181: step 13108, loss 0.527118.
Train: 2018-08-02T13:31:15.240363: step 13109, loss 0.492394.
Train: 2018-08-02T13:31:15.396607: step 13110, loss 0.600704.
Test: 2018-08-02T13:31:15.865232: step 13110, loss 0.549933.
Train: 2018-08-02T13:31:16.021454: step 13111, loss 0.493644.
Train: 2018-08-02T13:31:16.177672: step 13112, loss 0.478385.
Train: 2018-08-02T13:31:16.333886: step 13113, loss 0.514054.
Train: 2018-08-02T13:31:16.490068: step 13114, loss 0.582449.
Train: 2018-08-02T13:31:16.646312: step 13115, loss 0.562906.
Train: 2018-08-02T13:31:16.802495: step 13116, loss 0.44534.
Train: 2018-08-02T13:31:16.974360: step 13117, loss 0.467866.
Train: 2018-08-02T13:31:17.130557: step 13118, loss 0.476844.
Train: 2018-08-02T13:31:17.271165: step 13119, loss 0.493952.
Train: 2018-08-02T13:31:17.427348: step 13120, loss 0.424015.
Test: 2018-08-02T13:31:17.880369: step 13120, loss 0.548409.
Train: 2018-08-02T13:31:18.036614: step 13121, loss 0.577198.
Train: 2018-08-02T13:31:18.192825: step 13122, loss 0.442526.
Train: 2018-08-02T13:31:18.349039: step 13123, loss 0.545351.
Train: 2018-08-02T13:31:18.489599: step 13124, loss 0.562663.
Train: 2018-08-02T13:31:18.645845: step 13125, loss 0.430019.
Train: 2018-08-02T13:31:18.817685: step 13126, loss 0.563271.
Train: 2018-08-02T13:31:18.973862: step 13127, loss 0.530379.
Train: 2018-08-02T13:31:19.130100: step 13128, loss 0.470397.
Train: 2018-08-02T13:31:19.286315: step 13129, loss 0.471088.
Train: 2018-08-02T13:31:19.442502: step 13130, loss 0.56342.
Test: 2018-08-02T13:31:19.911143: step 13130, loss 0.548388.
Train: 2018-08-02T13:31:20.067387: step 13131, loss 0.46615.
Train: 2018-08-02T13:31:20.223603: step 13132, loss 0.51291.
Train: 2018-08-02T13:31:20.379782: step 13133, loss 0.520074.
Train: 2018-08-02T13:31:20.536028: step 13134, loss 0.445946.
Train: 2018-08-02T13:31:20.692239: step 13135, loss 0.495749.
Train: 2018-08-02T13:31:20.848453: step 13136, loss 0.481207.
Train: 2018-08-02T13:31:20.942150: step 13137, loss 0.511116.
Train: 2018-08-02T13:31:21.113986: step 13138, loss 0.539172.
Train: 2018-08-02T13:31:21.270231: step 13139, loss 0.516978.
Train: 2018-08-02T13:31:21.426412: step 13140, loss 0.432934.
Test: 2018-08-02T13:31:21.879462: step 13140, loss 0.547849.
Train: 2018-08-02T13:31:22.035662: step 13141, loss 0.500878.
Train: 2018-08-02T13:31:22.191859: step 13142, loss 0.586176.
Train: 2018-08-02T13:31:22.348097: step 13143, loss 0.41945.
Train: 2018-08-02T13:31:22.504333: step 13144, loss 0.524747.
Train: 2018-08-02T13:31:22.676151: step 13145, loss 0.617887.
Train: 2018-08-02T13:31:22.832335: step 13146, loss 0.462655.
Train: 2018-08-02T13:31:22.988548: step 13147, loss 0.497506.
Train: 2018-08-02T13:31:23.144762: step 13148, loss 0.491296.
Train: 2018-08-02T13:31:23.301000: step 13149, loss 0.560587.
Train: 2018-08-02T13:31:23.457212: step 13150, loss 0.462862.
Test: 2018-08-02T13:31:23.925827: step 13150, loss 0.548441.
Train: 2018-08-02T13:31:24.066419: step 13151, loss 0.473756.
Train: 2018-08-02T13:31:24.222662: step 13152, loss 0.489631.
Train: 2018-08-02T13:31:24.378875: step 13153, loss 0.479115.
Train: 2018-08-02T13:31:24.535060: step 13154, loss 0.525914.
Train: 2018-08-02T13:31:24.691297: step 13155, loss 0.552409.
Train: 2018-08-02T13:31:24.847487: step 13156, loss 0.485082.
Train: 2018-08-02T13:31:25.003726: step 13157, loss 0.501822.
Train: 2018-08-02T13:31:25.144322: step 13158, loss 0.448196.
Train: 2018-08-02T13:31:25.300505: step 13159, loss 0.476243.
Train: 2018-08-02T13:31:25.472340: step 13160, loss 0.544387.
Test: 2018-08-02T13:31:25.925384: step 13160, loss 0.546144.
Train: 2018-08-02T13:31:26.081574: step 13161, loss 0.636457.
Train: 2018-08-02T13:31:26.237786: step 13162, loss 0.498417.
Train: 2018-08-02T13:31:26.394030: step 13163, loss 0.502874.
Train: 2018-08-02T13:31:26.550242: step 13164, loss 0.529522.
Train: 2018-08-02T13:31:26.706457: step 13165, loss 0.506887.
Train: 2018-08-02T13:31:26.862669: step 13166, loss 0.521605.
Train: 2018-08-02T13:31:27.018883: step 13167, loss 0.530825.
Train: 2018-08-02T13:31:27.175068: step 13168, loss 0.534866.
Train: 2018-08-02T13:31:27.331310: step 13169, loss 0.603721.
Train: 2018-08-02T13:31:27.487493: step 13170, loss 0.521718.
Test: 2018-08-02T13:31:27.940512: step 13170, loss 0.55147.
Train: 2018-08-02T13:31:28.096744: step 13171, loss 0.509286.
Train: 2018-08-02T13:31:28.252969: step 13172, loss 0.505444.
Train: 2018-08-02T13:31:28.409178: step 13173, loss 0.557258.
Train: 2018-08-02T13:31:28.565396: step 13174, loss 0.526449.
Train: 2018-08-02T13:31:28.721604: step 13175, loss 0.616641.
Train: 2018-08-02T13:31:28.877792: step 13176, loss 0.574492.
Train: 2018-08-02T13:31:29.034036: step 13177, loss 0.488728.
Train: 2018-08-02T13:31:29.174628: step 13178, loss 0.566206.
Train: 2018-08-02T13:31:29.330841: step 13179, loss 0.531834.
Train: 2018-08-02T13:31:29.487053: step 13180, loss 0.531501.
Test: 2018-08-02T13:31:29.955666: step 13180, loss 0.555653.
Train: 2018-08-02T13:31:30.111888: step 13181, loss 0.458867.
Train: 2018-08-02T13:31:30.268093: step 13182, loss 0.554982.
Train: 2018-08-02T13:31:30.424340: step 13183, loss 0.426154.
Train: 2018-08-02T13:31:30.580550: step 13184, loss 0.506175.
Train: 2018-08-02T13:31:30.736763: step 13185, loss 0.525662.
Train: 2018-08-02T13:31:30.892993: step 13186, loss 0.531242.
Train: 2018-08-02T13:31:31.049191: step 13187, loss 0.508649.
Train: 2018-08-02T13:31:31.205372: step 13188, loss 0.495797.
Train: 2018-08-02T13:31:31.361615: step 13189, loss 0.523874.
Train: 2018-08-02T13:31:31.502178: step 13190, loss 0.474887.
Test: 2018-08-02T13:31:31.970818: step 13190, loss 0.548178.
Train: 2018-08-02T13:31:32.111441: step 13191, loss 0.5576.
Train: 2018-08-02T13:31:32.283247: step 13192, loss 0.456879.
Train: 2018-08-02T13:31:32.439459: step 13193, loss 0.529221.
Train: 2018-08-02T13:31:32.580081: step 13194, loss 0.529676.
Train: 2018-08-02T13:31:32.751885: step 13195, loss 0.562262.
Train: 2018-08-02T13:31:32.892510: step 13196, loss 0.465068.
Train: 2018-08-02T13:31:33.048690: step 13197, loss 0.541514.
Train: 2018-08-02T13:31:33.204904: step 13198, loss 0.572309.
Train: 2018-08-02T13:31:33.361120: step 13199, loss 0.543634.
Train: 2018-08-02T13:31:33.532952: step 13200, loss 0.501746.
Test: 2018-08-02T13:31:33.985971: step 13200, loss 0.553868.
Train: 2018-08-02T13:31:34.595256: step 13201, loss 0.547019.
Train: 2018-08-02T13:31:34.751452: step 13202, loss 0.580586.
Train: 2018-08-02T13:31:34.923253: step 13203, loss 0.515434.
Train: 2018-08-02T13:31:35.079490: step 13204, loss 0.536103.
Train: 2018-08-02T13:31:35.235709: step 13205, loss 0.551111.
Train: 2018-08-02T13:31:35.391917: step 13206, loss 0.502562.
Train: 2018-08-02T13:31:35.548107: step 13207, loss 0.541233.
Train: 2018-08-02T13:31:35.704350: step 13208, loss 0.510037.
Train: 2018-08-02T13:31:35.860561: step 13209, loss 0.515931.
Train: 2018-08-02T13:31:36.016777: step 13210, loss 0.462708.
Test: 2018-08-02T13:31:36.469765: step 13210, loss 0.547542.
Train: 2018-08-02T13:31:36.626008: step 13211, loss 0.662759.
Train: 2018-08-02T13:31:36.782224: step 13212, loss 0.460638.
Train: 2018-08-02T13:31:36.938431: step 13213, loss 0.43471.
Train: 2018-08-02T13:31:37.094648: step 13214, loss 0.500676.
Train: 2018-08-02T13:31:37.250862: step 13215, loss 0.512475.
Train: 2018-08-02T13:31:37.407046: step 13216, loss 0.421794.
Train: 2018-08-02T13:31:37.563290: step 13217, loss 0.512015.
Train: 2018-08-02T13:31:37.719497: step 13218, loss 0.447842.
Train: 2018-08-02T13:31:37.875710: step 13219, loss 0.429697.
Train: 2018-08-02T13:31:38.031929: step 13220, loss 0.562671.
Test: 2018-08-02T13:31:38.500565: step 13220, loss 0.547369.
Train: 2018-08-02T13:31:38.641161: step 13221, loss 0.528517.
Train: 2018-08-02T13:31:38.797344: step 13222, loss 0.501683.
Train: 2018-08-02T13:31:38.953588: step 13223, loss 0.475703.
Train: 2018-08-02T13:31:39.109799: step 13224, loss 0.454274.
Train: 2018-08-02T13:31:39.266015: step 13225, loss 0.507843.
Train: 2018-08-02T13:31:39.422229: step 13226, loss 0.493522.
Train: 2018-08-02T13:31:39.578441: step 13227, loss 0.515233.
Train: 2018-08-02T13:31:39.734654: step 13228, loss 0.517544.
Train: 2018-08-02T13:31:39.890864: step 13229, loss 0.478602.
Train: 2018-08-02T13:31:40.031460: step 13230, loss 0.564065.
Test: 2018-08-02T13:31:40.500070: step 13230, loss 0.546864.
Train: 2018-08-02T13:31:40.640664: step 13231, loss 0.448995.
Train: 2018-08-02T13:31:40.796906: step 13232, loss 0.463961.
Train: 2018-08-02T13:31:40.953120: step 13233, loss 0.524506.
Train: 2018-08-02T13:31:41.109304: step 13234, loss 0.520978.
Train: 2018-08-02T13:31:41.249901: step 13235, loss 0.47653.
Train: 2018-08-02T13:31:41.406108: step 13236, loss 0.51935.
Train: 2018-08-02T13:31:41.562352: step 13237, loss 0.52853.
Train: 2018-08-02T13:31:41.714514: step 13238, loss 0.500478.
Train: 2018-08-02T13:31:41.870730: step 13239, loss 0.501656.
Train: 2018-08-02T13:31:42.026942: step 13240, loss 0.546166.
Test: 2018-08-02T13:31:42.479930: step 13240, loss 0.550344.
Train: 2018-08-02T13:31:42.636169: step 13241, loss 0.475847.
Train: 2018-08-02T13:31:42.792388: step 13242, loss 0.488475.
Train: 2018-08-02T13:31:42.964193: step 13243, loss 0.519536.
Train: 2018-08-02T13:31:43.120407: step 13244, loss 0.562608.
Train: 2018-08-02T13:31:43.276620: step 13245, loss 0.50433.
Train: 2018-08-02T13:31:43.432833: step 13246, loss 0.477279.
Train: 2018-08-02T13:31:43.589076: step 13247, loss 0.525821.
Train: 2018-08-02T13:31:43.745258: step 13248, loss 0.550656.
Train: 2018-08-02T13:31:43.901473: step 13249, loss 0.511233.
Train: 2018-08-02T13:31:44.057716: step 13250, loss 0.493729.
Test: 2018-08-02T13:31:44.526328: step 13250, loss 0.548623.
Train: 2018-08-02T13:31:44.682541: step 13251, loss 0.522072.
Train: 2018-08-02T13:31:44.838787: step 13252, loss 0.552773.
Train: 2018-08-02T13:31:44.994966: step 13253, loss 0.511295.
Train: 2018-08-02T13:31:45.135588: step 13254, loss 0.498966.
Train: 2018-08-02T13:31:45.291801: step 13255, loss 0.479242.
Train: 2018-08-02T13:31:45.448015: step 13256, loss 0.633613.
Train: 2018-08-02T13:31:45.604224: step 13257, loss 0.530102.
Train: 2018-08-02T13:31:45.760442: step 13258, loss 0.520228.
Train: 2018-08-02T13:31:45.901034: step 13259, loss 0.508136.
Train: 2018-08-02T13:31:46.057248: step 13260, loss 0.502454.
Test: 2018-08-02T13:31:46.525858: step 13260, loss 0.550858.
Train: 2018-08-02T13:31:46.682097: step 13261, loss 0.526729.
Train: 2018-08-02T13:31:46.838318: step 13262, loss 0.52714.
Train: 2018-08-02T13:31:46.994523: step 13263, loss 0.447196.
Train: 2018-08-02T13:31:47.150742: step 13264, loss 0.504012.
Train: 2018-08-02T13:31:47.306955: step 13265, loss 0.557326.
Train: 2018-08-02T13:31:47.478761: step 13266, loss 0.576771.
Train: 2018-08-02T13:31:47.650624: step 13267, loss 0.454185.
Train: 2018-08-02T13:31:47.806808: step 13268, loss 0.499693.
Train: 2018-08-02T13:31:47.963052: step 13269, loss 0.473284.
Train: 2018-08-02T13:31:48.119263: step 13270, loss 0.484027.
Test: 2018-08-02T13:31:48.572285: step 13270, loss 0.546065.
Train: 2018-08-02T13:31:48.728468: step 13271, loss 0.576643.
Train: 2018-08-02T13:31:48.884680: step 13272, loss 0.514989.
Train: 2018-08-02T13:31:49.040951: step 13273, loss 0.546669.
Train: 2018-08-02T13:31:49.197108: step 13274, loss 0.479417.
Train: 2018-08-02T13:31:49.353349: step 13275, loss 0.494462.
Train: 2018-08-02T13:31:49.509565: step 13276, loss 0.449024.
Train: 2018-08-02T13:31:49.681399: step 13277, loss 0.474118.
Train: 2018-08-02T13:31:49.837607: step 13278, loss 0.471474.
Train: 2018-08-02T13:31:49.993828: step 13279, loss 0.466824.
Train: 2018-08-02T13:31:50.150039: step 13280, loss 0.499595.
Test: 2018-08-02T13:31:50.603046: step 13280, loss 0.547641.
Train: 2018-08-02T13:31:50.759271: step 13281, loss 0.512218.
Train: 2018-08-02T13:31:50.915454: step 13282, loss 0.566543.
Train: 2018-08-02T13:31:51.056076: step 13283, loss 0.535563.
Train: 2018-08-02T13:31:51.212290: step 13284, loss 0.480267.
Train: 2018-08-02T13:31:51.368506: step 13285, loss 0.508629.
Train: 2018-08-02T13:31:51.524688: step 13286, loss 0.506432.
Train: 2018-08-02T13:31:51.680901: step 13287, loss 0.451871.
Train: 2018-08-02T13:31:51.790249: step 13288, loss 0.512703.
Train: 2018-08-02T13:31:51.962116: step 13289, loss 0.543539.
Train: 2018-08-02T13:31:52.118331: step 13290, loss 0.568185.
Test: 2018-08-02T13:31:52.571342: step 13290, loss 0.548721.
Train: 2018-08-02T13:31:52.727560: step 13291, loss 0.537317.
Train: 2018-08-02T13:31:52.883775: step 13292, loss 0.51825.
Train: 2018-08-02T13:31:53.039987: step 13293, loss 0.466122.
Train: 2018-08-02T13:31:53.196171: step 13294, loss 0.459644.
Train: 2018-08-02T13:31:53.352414: step 13295, loss 0.480695.
Train: 2018-08-02T13:31:53.508598: step 13296, loss 0.508619.
Train: 2018-08-02T13:31:53.664841: step 13297, loss 0.536481.
Train: 2018-08-02T13:31:53.821054: step 13298, loss 0.496393.
Train: 2018-08-02T13:31:53.977266: step 13299, loss 0.55272.
Train: 2018-08-02T13:31:54.133477: step 13300, loss 0.524737.
Test: 2018-08-02T13:31:54.602127: step 13300, loss 0.547687.
Train: 2018-08-02T13:31:55.273840: step 13301, loss 0.500093.
Train: 2018-08-02T13:31:55.430054: step 13302, loss 0.495216.
Train: 2018-08-02T13:31:55.586266: step 13303, loss 0.537374.
Train: 2018-08-02T13:31:55.742473: step 13304, loss 0.501253.
Train: 2018-08-02T13:31:55.883071: step 13305, loss 0.565091.
Train: 2018-08-02T13:31:56.039285: step 13306, loss 0.468384.
Train: 2018-08-02T13:31:56.195498: step 13307, loss 0.46827.
Train: 2018-08-02T13:31:56.351711: step 13308, loss 0.457341.
Train: 2018-08-02T13:31:56.523518: step 13309, loss 0.506197.
Train: 2018-08-02T13:31:56.679731: step 13310, loss 0.591781.
Test: 2018-08-02T13:31:57.132780: step 13310, loss 0.547632.
Train: 2018-08-02T13:31:57.288992: step 13311, loss 0.578325.
Train: 2018-08-02T13:31:57.429584: step 13312, loss 0.510392.
Train: 2018-08-02T13:31:57.585797: step 13313, loss 0.493891.
Train: 2018-08-02T13:31:57.742014: step 13314, loss 0.453935.
Train: 2018-08-02T13:31:57.898224: step 13315, loss 0.485245.
Train: 2018-08-02T13:31:58.054434: step 13316, loss 0.481985.
Train: 2018-08-02T13:31:58.210646: step 13317, loss 0.494764.
Train: 2018-08-02T13:31:58.366867: step 13318, loss 0.462344.
Train: 2018-08-02T13:31:58.523078: step 13319, loss 0.524677.
Train: 2018-08-02T13:31:58.679292: step 13320, loss 0.460138.
Test: 2018-08-02T13:31:59.132306: step 13320, loss 0.547195.
Train: 2018-08-02T13:31:59.272902: step 13321, loss 0.587623.
Train: 2018-08-02T13:31:59.429087: step 13322, loss 0.594258.
Train: 2018-08-02T13:31:59.585330: step 13323, loss 0.519503.
Train: 2018-08-02T13:31:59.741542: step 13324, loss 0.494957.
Train: 2018-08-02T13:31:59.897737: step 13325, loss 0.530168.
Train: 2018-08-02T13:32:00.053968: step 13326, loss 0.530666.
Train: 2018-08-02T13:32:00.210154: step 13327, loss 0.52695.
Train: 2018-08-02T13:32:00.366399: step 13328, loss 0.476774.
Train: 2018-08-02T13:32:00.522613: step 13329, loss 0.488564.
Train: 2018-08-02T13:32:00.678801: step 13330, loss 0.571363.
Test: 2018-08-02T13:32:01.131813: step 13330, loss 0.548011.
Train: 2018-08-02T13:32:01.288025: step 13331, loss 0.547936.
Train: 2018-08-02T13:32:01.444269: step 13332, loss 0.489246.
Train: 2018-08-02T13:32:01.600483: step 13333, loss 0.500261.
Train: 2018-08-02T13:32:01.756696: step 13334, loss 0.528858.
Train: 2018-08-02T13:32:01.912910: step 13335, loss 0.522155.
Train: 2018-08-02T13:32:02.069093: step 13336, loss 0.525416.
Train: 2018-08-02T13:32:02.225307: step 13337, loss 0.520121.
Train: 2018-08-02T13:32:02.381549: step 13338, loss 0.461721.
Train: 2018-08-02T13:32:02.537763: step 13339, loss 0.499515.
Train: 2018-08-02T13:32:02.693976: step 13340, loss 0.452975.
Test: 2018-08-02T13:32:03.146982: step 13340, loss 0.549338.
Train: 2018-08-02T13:32:03.303181: step 13341, loss 0.509262.
Train: 2018-08-02T13:32:03.459422: step 13342, loss 0.496122.
Train: 2018-08-02T13:32:03.615638: step 13343, loss 0.530042.
Train: 2018-08-02T13:32:03.756228: step 13344, loss 0.470234.
Train: 2018-08-02T13:32:03.912441: step 13345, loss 0.549326.
Train: 2018-08-02T13:32:04.068654: step 13346, loss 0.525578.
Train: 2018-08-02T13:32:04.224868: step 13347, loss 0.54766.
Train: 2018-08-02T13:32:04.381084: step 13348, loss 0.536179.
Train: 2018-08-02T13:32:04.537289: step 13349, loss 0.548161.
Train: 2018-08-02T13:32:04.693477: step 13350, loss 0.54679.
Test: 2018-08-02T13:32:05.146522: step 13350, loss 0.561243.
Train: 2018-08-02T13:32:05.302741: step 13351, loss 0.594391.
Train: 2018-08-02T13:32:05.458955: step 13352, loss 0.571088.
Train: 2018-08-02T13:32:05.599546: step 13353, loss 0.511578.
Train: 2018-08-02T13:32:05.755760: step 13354, loss 0.514735.
Train: 2018-08-02T13:32:05.911973: step 13355, loss 0.53383.
Train: 2018-08-02T13:32:06.068186: step 13356, loss 0.511365.
Train: 2018-08-02T13:32:06.224400: step 13357, loss 0.500764.
Train: 2018-08-02T13:32:06.380609: step 13358, loss 0.474001.
Train: 2018-08-02T13:32:06.536798: step 13359, loss 0.363746.
Train: 2018-08-02T13:32:06.693035: step 13360, loss 0.519151.
Test: 2018-08-02T13:32:07.146030: step 13360, loss 0.558339.
Train: 2018-08-02T13:32:07.302272: step 13361, loss 0.602016.
Train: 2018-08-02T13:32:07.458456: step 13362, loss 0.540839.
Train: 2018-08-02T13:32:07.599078: step 13363, loss 0.538501.
Train: 2018-08-02T13:32:07.755292: step 13364, loss 0.540815.
Train: 2018-08-02T13:32:07.911506: step 13365, loss 0.674487.
Train: 2018-08-02T13:32:08.067688: step 13366, loss 0.563809.
Train: 2018-08-02T13:32:08.223931: step 13367, loss 0.495586.
Train: 2018-08-02T13:32:08.380114: step 13368, loss 0.521697.
Train: 2018-08-02T13:32:08.536328: step 13369, loss 0.598266.
Train: 2018-08-02T13:32:08.676922: step 13370, loss 0.543525.
Test: 2018-08-02T13:32:09.145561: step 13370, loss 0.563965.
Train: 2018-08-02T13:32:09.301804: step 13371, loss 0.564176.
Train: 2018-08-02T13:32:09.458018: step 13372, loss 0.576863.
Train: 2018-08-02T13:32:09.598579: step 13373, loss 0.52205.
Train: 2018-08-02T13:32:09.754823: step 13374, loss 0.537397.
Train: 2018-08-02T13:32:09.911006: step 13375, loss 0.526999.
Train: 2018-08-02T13:32:10.067219: step 13376, loss 0.537228.
Train: 2018-08-02T13:32:10.223432: step 13377, loss 0.473001.
Train: 2018-08-02T13:32:10.379677: step 13378, loss 0.471175.
Train: 2018-08-02T13:32:10.520269: step 13379, loss 0.477731.
Train: 2018-08-02T13:32:10.676483: step 13380, loss 0.524137.
Test: 2018-08-02T13:32:11.145093: step 13380, loss 0.553186.
Train: 2018-08-02T13:32:11.285715: step 13381, loss 0.536166.
Train: 2018-08-02T13:32:11.441928: step 13382, loss 0.56074.
Train: 2018-08-02T13:32:11.598142: step 13383, loss 0.570947.
Train: 2018-08-02T13:32:11.754357: step 13384, loss 0.556145.
Train: 2018-08-02T13:32:11.910568: step 13385, loss 0.54509.
Train: 2018-08-02T13:32:12.066781: step 13386, loss 0.508529.
Train: 2018-08-02T13:32:12.222995: step 13387, loss 0.49065.
Train: 2018-08-02T13:32:12.363590: step 13388, loss 0.520407.
Train: 2018-08-02T13:32:12.519771: step 13389, loss 0.519146.
Train: 2018-08-02T13:32:12.676008: step 13390, loss 0.466597.
Test: 2018-08-02T13:32:13.129028: step 13390, loss 0.546866.
Train: 2018-08-02T13:32:13.285247: step 13391, loss 0.435955.
Train: 2018-08-02T13:32:13.441460: step 13392, loss 0.49425.
Train: 2018-08-02T13:32:13.597673: step 13393, loss 0.592296.
Train: 2018-08-02T13:32:13.753887: step 13394, loss 0.417072.
Train: 2018-08-02T13:32:13.894478: step 13395, loss 0.508824.
Train: 2018-08-02T13:32:14.050692: step 13396, loss 0.548388.
Train: 2018-08-02T13:32:14.206909: step 13397, loss 0.507297.
Train: 2018-08-02T13:32:14.363122: step 13398, loss 0.415892.
Train: 2018-08-02T13:32:14.519327: step 13399, loss 0.531624.
Train: 2018-08-02T13:32:14.675546: step 13400, loss 0.596209.
Test: 2018-08-02T13:32:15.128565: step 13400, loss 0.547595.
Train: 2018-08-02T13:32:15.800253: step 13401, loss 0.545923.
Train: 2018-08-02T13:32:15.956496: step 13402, loss 0.513596.
Train: 2018-08-02T13:32:16.112703: step 13403, loss 0.462035.
Train: 2018-08-02T13:32:16.268893: step 13404, loss 0.563866.
Train: 2018-08-02T13:32:16.425136: step 13405, loss 0.572073.
Train: 2018-08-02T13:32:16.581349: step 13406, loss 0.595086.
Train: 2018-08-02T13:32:16.721942: step 13407, loss 0.499163.
Train: 2018-08-02T13:32:16.878155: step 13408, loss 0.4468.
Train: 2018-08-02T13:32:17.034338: step 13409, loss 0.555747.
Train: 2018-08-02T13:32:17.190582: step 13410, loss 0.522396.
Test: 2018-08-02T13:32:17.643570: step 13410, loss 0.546105.
Train: 2018-08-02T13:32:17.799784: step 13411, loss 0.514931.
Train: 2018-08-02T13:32:17.956031: step 13412, loss 0.505451.
Train: 2018-08-02T13:32:18.112243: step 13413, loss 0.510855.
Train: 2018-08-02T13:32:18.268454: step 13414, loss 0.57014.
Train: 2018-08-02T13:32:18.409016: step 13415, loss 0.547335.
Train: 2018-08-02T13:32:18.565260: step 13416, loss 0.514023.
Train: 2018-08-02T13:32:18.721444: step 13417, loss 0.482647.
Train: 2018-08-02T13:32:18.877682: step 13418, loss 0.515564.
Train: 2018-08-02T13:32:19.033900: step 13419, loss 0.501265.
Train: 2018-08-02T13:32:19.190114: step 13420, loss 0.516861.
Test: 2018-08-02T13:32:19.643104: step 13420, loss 0.548675.
Train: 2018-08-02T13:32:19.799340: step 13421, loss 0.52083.
Train: 2018-08-02T13:32:19.955562: step 13422, loss 0.600083.
Train: 2018-08-02T13:32:20.111775: step 13423, loss 0.45014.
Train: 2018-08-02T13:32:20.267986: step 13424, loss 0.463624.
Train: 2018-08-02T13:32:20.424200: step 13425, loss 0.517602.
Train: 2018-08-02T13:32:20.580413: step 13426, loss 0.482076.
Train: 2018-08-02T13:32:20.736598: step 13427, loss 0.522092.
Train: 2018-08-02T13:32:20.877188: step 13428, loss 0.439346.
Train: 2018-08-02T13:32:21.033401: step 13429, loss 0.535594.
Train: 2018-08-02T13:32:21.189645: step 13430, loss 0.493306.
Test: 2018-08-02T13:32:21.642634: step 13430, loss 0.547391.
Train: 2018-08-02T13:32:21.798878: step 13431, loss 0.525768.
Train: 2018-08-02T13:32:21.955091: step 13432, loss 0.485003.
Train: 2018-08-02T13:32:22.111307: step 13433, loss 0.469989.
Train: 2018-08-02T13:32:22.251896: step 13434, loss 0.46167.
Train: 2018-08-02T13:32:22.408110: step 13435, loss 0.502595.
Train: 2018-08-02T13:32:22.564323: step 13436, loss 0.610476.
Train: 2018-08-02T13:32:22.720538: step 13437, loss 0.459627.
Train: 2018-08-02T13:32:22.876750: step 13438, loss 0.573054.
Train: 2018-08-02T13:32:22.986102: step 13439, loss 0.529422.
Train: 2018-08-02T13:32:23.142313: step 13440, loss 0.485133.
Test: 2018-08-02T13:32:23.610923: step 13440, loss 0.550343.
Train: 2018-08-02T13:32:23.751546: step 13441, loss 0.506523.
Train: 2018-08-02T13:32:23.907759: step 13442, loss 0.587663.
Train: 2018-08-02T13:32:24.063943: step 13443, loss 0.479845.
Train: 2018-08-02T13:32:24.220183: step 13444, loss 0.460341.
Train: 2018-08-02T13:32:24.376399: step 13445, loss 0.526576.
Train: 2018-08-02T13:32:24.532613: step 13446, loss 0.453348.
Train: 2018-08-02T13:32:24.688826: step 13447, loss 0.55223.
Train: 2018-08-02T13:32:24.845039: step 13448, loss 0.479478.
Train: 2018-08-02T13:32:25.001248: step 13449, loss 0.51366.
Train: 2018-08-02T13:32:25.157436: step 13450, loss 0.47669.
Test: 2018-08-02T13:32:25.610480: step 13450, loss 0.546334.
Train: 2018-08-02T13:32:25.766698: step 13451, loss 0.498931.
Train: 2018-08-02T13:32:25.922912: step 13452, loss 0.510059.
Train: 2018-08-02T13:32:26.079125: step 13453, loss 0.591168.
Train: 2018-08-02T13:32:26.235342: step 13454, loss 0.486344.
Train: 2018-08-02T13:32:26.391552: step 13455, loss 0.406456.
Train: 2018-08-02T13:32:26.532144: step 13456, loss 0.522174.
Train: 2018-08-02T13:32:26.688353: step 13457, loss 0.461754.
Train: 2018-08-02T13:32:26.844571: step 13458, loss 0.573231.
Train: 2018-08-02T13:32:27.000780: step 13459, loss 0.506769.
Train: 2018-08-02T13:32:27.156998: step 13460, loss 0.487451.
Test: 2018-08-02T13:32:27.610013: step 13460, loss 0.548783.
Train: 2018-08-02T13:32:27.766230: step 13461, loss 0.569423.
Train: 2018-08-02T13:32:27.922439: step 13462, loss 0.501831.
Train: 2018-08-02T13:32:28.078657: step 13463, loss 0.567248.
Train: 2018-08-02T13:32:28.234873: step 13464, loss 0.518376.
Train: 2018-08-02T13:32:28.391084: step 13465, loss 0.46418.
Train: 2018-08-02T13:32:28.547297: step 13466, loss 0.510374.
Train: 2018-08-02T13:32:28.703510: step 13467, loss 0.558421.
Train: 2018-08-02T13:32:28.859726: step 13468, loss 0.484858.
Train: 2018-08-02T13:32:29.015907: step 13469, loss 0.573572.
Train: 2018-08-02T13:32:29.172152: step 13470, loss 0.458169.
Test: 2018-08-02T13:32:29.625168: step 13470, loss 0.546992.
Train: 2018-08-02T13:32:29.781383: step 13471, loss 0.56381.
Train: 2018-08-02T13:32:29.937594: step 13472, loss 0.467186.
Train: 2018-08-02T13:32:30.093827: step 13473, loss 0.517423.
Train: 2018-08-02T13:32:30.250023: step 13474, loss 0.560752.
Train: 2018-08-02T13:32:30.406207: step 13475, loss 0.517328.
Train: 2018-08-02T13:32:30.546829: step 13476, loss 0.611662.
Train: 2018-08-02T13:32:30.703044: step 13477, loss 0.552223.
Train: 2018-08-02T13:32:30.859259: step 13478, loss 0.511909.
Train: 2018-08-02T13:32:31.015438: step 13479, loss 0.519207.
Train: 2018-08-02T13:32:31.171653: step 13480, loss 0.572401.
Test: 2018-08-02T13:32:31.624696: step 13480, loss 0.562028.
Train: 2018-08-02T13:32:31.780916: step 13481, loss 0.495423.
Train: 2018-08-02T13:32:31.937129: step 13482, loss 0.520201.
Train: 2018-08-02T13:32:32.093342: step 13483, loss 0.502729.
Train: 2018-08-02T13:32:32.249555: step 13484, loss 0.573964.
Train: 2018-08-02T13:32:32.405769: step 13485, loss 0.453553.
Train: 2018-08-02T13:32:32.561953: step 13486, loss 0.492463.
Train: 2018-08-02T13:32:32.702545: step 13487, loss 0.4717.
Train: 2018-08-02T13:32:32.858791: step 13488, loss 0.477843.
Train: 2018-08-02T13:32:33.015002: step 13489, loss 0.499801.
Train: 2018-08-02T13:32:33.171214: step 13490, loss 0.507369.
Test: 2018-08-02T13:32:33.624204: step 13490, loss 0.547878.
Train: 2018-08-02T13:32:33.780447: step 13491, loss 0.442413.
Train: 2018-08-02T13:32:33.936629: step 13492, loss 0.467603.
Train: 2018-08-02T13:32:34.092843: step 13493, loss 0.511047.
Train: 2018-08-02T13:32:34.249087: step 13494, loss 0.523655.
Train: 2018-08-02T13:32:34.405301: step 13495, loss 0.494751.
Train: 2018-08-02T13:32:34.561514: step 13496, loss 0.490899.
Train: 2018-08-02T13:32:34.702105: step 13497, loss 0.457898.
Train: 2018-08-02T13:32:34.858289: step 13498, loss 0.508708.
Train: 2018-08-02T13:32:35.092611: step 13499, loss 0.520477.
Train: 2018-08-02T13:32:35.248853: step 13500, loss 0.476396.
Test: 2018-08-02T13:32:35.717463: step 13500, loss 0.548285.
Train: 2018-08-02T13:32:36.373589: step 13501, loss 0.578614.
Train: 2018-08-02T13:32:36.529803: step 13502, loss 0.501591.
Train: 2018-08-02T13:32:36.685988: step 13503, loss 0.585195.
Train: 2018-08-02T13:32:36.842201: step 13504, loss 0.568019.
Train: 2018-08-02T13:32:36.998441: step 13505, loss 0.583528.
Train: 2018-08-02T13:32:37.154626: step 13506, loss 0.538174.
Train: 2018-08-02T13:32:37.310871: step 13507, loss 0.50048.
Train: 2018-08-02T13:32:37.467085: step 13508, loss 0.608614.
Train: 2018-08-02T13:32:37.623298: step 13509, loss 0.580105.
Train: 2018-08-02T13:32:37.779506: step 13510, loss 0.471763.
Test: 2018-08-02T13:32:38.232499: step 13510, loss 0.548247.
Train: 2018-08-02T13:32:38.388712: step 13511, loss 0.521115.
Train: 2018-08-02T13:32:38.544925: step 13512, loss 0.513495.
Train: 2018-08-02T13:32:38.701170: step 13513, loss 0.640081.
Train: 2018-08-02T13:32:38.857353: step 13514, loss 0.561032.
Train: 2018-08-02T13:32:39.013591: step 13515, loss 0.5321.
Train: 2018-08-02T13:32:39.169787: step 13516, loss 0.543571.
Train: 2018-08-02T13:32:39.326026: step 13517, loss 0.527943.
Train: 2018-08-02T13:32:39.497859: step 13518, loss 0.569567.
Train: 2018-08-02T13:32:39.654041: step 13519, loss 0.57072.
Train: 2018-08-02T13:32:39.810281: step 13520, loss 0.524963.
Test: 2018-08-02T13:32:40.263274: step 13520, loss 0.554442.
Train: 2018-08-02T13:32:40.419518: step 13521, loss 0.512766.
Train: 2018-08-02T13:32:40.575725: step 13522, loss 0.574481.
Train: 2018-08-02T13:32:40.716322: step 13523, loss 0.498823.
Train: 2018-08-02T13:32:40.872536: step 13524, loss 0.575894.
Train: 2018-08-02T13:32:41.028749: step 13525, loss 0.478493.
Train: 2018-08-02T13:32:41.184964: step 13526, loss 0.46602.
Train: 2018-08-02T13:32:41.341176: step 13527, loss 0.492592.
Train: 2018-08-02T13:32:41.497391: step 13528, loss 0.506538.
Train: 2018-08-02T13:32:41.653572: step 13529, loss 0.466322.
Train: 2018-08-02T13:32:41.809814: step 13530, loss 0.51189.
Test: 2018-08-02T13:32:42.262830: step 13530, loss 0.548559.
Train: 2018-08-02T13:32:42.419050: step 13531, loss 0.524715.
Train: 2018-08-02T13:32:42.575233: step 13532, loss 0.546923.
Train: 2018-08-02T13:32:42.715825: step 13533, loss 0.478253.
Train: 2018-08-02T13:32:42.872068: step 13534, loss 0.516207.
Train: 2018-08-02T13:32:43.028250: step 13535, loss 0.454234.
Train: 2018-08-02T13:32:43.168873: step 13536, loss 0.414437.
Train: 2018-08-02T13:32:43.325087: step 13537, loss 0.493734.
Train: 2018-08-02T13:32:43.481303: step 13538, loss 0.463527.
Train: 2018-08-02T13:32:43.637483: step 13539, loss 0.443266.
Train: 2018-08-02T13:32:43.809343: step 13540, loss 0.624842.
Test: 2018-08-02T13:32:44.262362: step 13540, loss 0.54769.
Train: 2018-08-02T13:32:44.434177: step 13541, loss 0.523365.
Train: 2018-08-02T13:32:44.590416: step 13542, loss 0.516575.
Train: 2018-08-02T13:32:44.746600: step 13543, loss 0.527495.
Train: 2018-08-02T13:32:44.887221: step 13544, loss 0.533309.
Train: 2018-08-02T13:32:45.043405: step 13545, loss 0.492848.
Train: 2018-08-02T13:32:45.199618: step 13546, loss 0.489551.
Train: 2018-08-02T13:32:45.355831: step 13547, loss 0.548811.
Train: 2018-08-02T13:32:45.512045: step 13548, loss 0.461835.
Train: 2018-08-02T13:32:45.652637: step 13549, loss 0.547571.
Train: 2018-08-02T13:32:45.808883: step 13550, loss 0.42122.
Test: 2018-08-02T13:32:46.277491: step 13550, loss 0.551627.
Train: 2018-08-02T13:32:46.433728: step 13551, loss 0.52512.
Train: 2018-08-02T13:32:46.589942: step 13552, loss 0.536355.
Train: 2018-08-02T13:32:46.746164: step 13553, loss 0.475218.
Train: 2018-08-02T13:32:46.902374: step 13554, loss 0.408911.
Train: 2018-08-02T13:32:47.058589: step 13555, loss 0.54189.
Train: 2018-08-02T13:32:47.214772: step 13556, loss 0.57277.
Train: 2018-08-02T13:32:47.370986: step 13557, loss 0.408489.
Train: 2018-08-02T13:32:47.527228: step 13558, loss 0.453111.
Train: 2018-08-02T13:32:47.699034: step 13559, loss 0.445083.
Train: 2018-08-02T13:32:47.839655: step 13560, loss 0.510416.
Test: 2018-08-02T13:32:48.308303: step 13560, loss 0.550166.
Train: 2018-08-02T13:32:48.448887: step 13561, loss 0.487575.
Train: 2018-08-02T13:32:48.605103: step 13562, loss 0.504584.
Train: 2018-08-02T13:32:48.761285: step 13563, loss 0.43188.
Train: 2018-08-02T13:32:48.917528: step 13564, loss 0.588997.
Train: 2018-08-02T13:32:49.073742: step 13565, loss 0.552473.
Train: 2018-08-02T13:32:49.229957: step 13566, loss 0.509042.
Train: 2018-08-02T13:32:49.386168: step 13567, loss 0.481883.
Train: 2018-08-02T13:32:49.542384: step 13568, loss 0.565917.
Train: 2018-08-02T13:32:49.698595: step 13569, loss 0.502992.
Train: 2018-08-02T13:32:49.854803: step 13570, loss 0.520873.
Test: 2018-08-02T13:32:50.307796: step 13570, loss 0.550409.
Train: 2018-08-02T13:32:50.464040: step 13571, loss 0.546255.
Train: 2018-08-02T13:32:50.620223: step 13572, loss 0.529114.
Train: 2018-08-02T13:32:50.776467: step 13573, loss 0.471585.
Train: 2018-08-02T13:32:50.932681: step 13574, loss 0.605783.
Train: 2018-08-02T13:32:51.088894: step 13575, loss 0.564985.
Train: 2018-08-02T13:32:51.229456: step 13576, loss 0.532835.
Train: 2018-08-02T13:32:51.385698: step 13577, loss 0.472239.
Train: 2018-08-02T13:32:51.541912: step 13578, loss 0.547332.
Train: 2018-08-02T13:32:51.698097: step 13579, loss 0.553755.
Train: 2018-08-02T13:32:51.854309: step 13580, loss 0.486854.
Test: 2018-08-02T13:32:52.322951: step 13580, loss 0.546382.
Train: 2018-08-02T13:32:52.526028: step 13581, loss 0.538338.
Train: 2018-08-02T13:32:52.682241: step 13582, loss 0.479033.
Train: 2018-08-02T13:32:52.838479: step 13583, loss 0.590587.
Train: 2018-08-02T13:32:52.994697: step 13584, loss 0.478382.
Train: 2018-08-02T13:32:53.150881: step 13585, loss 0.45908.
Train: 2018-08-02T13:32:53.307100: step 13586, loss 0.538298.
Train: 2018-08-02T13:32:53.463310: step 13587, loss 0.594064.
Train: 2018-08-02T13:32:53.619520: step 13588, loss 0.514905.
Train: 2018-08-02T13:32:53.775764: step 13589, loss 0.530296.
Train: 2018-08-02T13:32:53.885114: step 13590, loss 0.511945.
Test: 2018-08-02T13:32:54.338128: step 13590, loss 0.549535.
Train: 2018-08-02T13:32:54.494316: step 13591, loss 0.492826.
Train: 2018-08-02T13:32:54.650531: step 13592, loss 0.506506.
Train: 2018-08-02T13:32:54.806773: step 13593, loss 0.592545.
Train: 2018-08-02T13:32:54.962957: step 13594, loss 0.578005.
Train: 2018-08-02T13:32:55.119171: step 13595, loss 0.506152.
Train: 2018-08-02T13:32:55.275414: step 13596, loss 0.499508.
Train: 2018-08-02T13:32:55.431627: step 13597, loss 0.501386.
Train: 2018-08-02T13:32:55.587810: step 13598, loss 0.503672.
Train: 2018-08-02T13:32:55.744053: step 13599, loss 0.464017.
Train: 2018-08-02T13:32:55.884645: step 13600, loss 0.458247.
Test: 2018-08-02T13:32:56.353281: step 13600, loss 0.550711.
Train: 2018-08-02T13:32:57.025004: step 13601, loss 0.483701.
Train: 2018-08-02T13:32:57.181217: step 13602, loss 0.560282.
Train: 2018-08-02T13:32:57.337431: step 13603, loss 0.523123.
Train: 2018-08-02T13:32:57.493614: step 13604, loss 0.516396.
Train: 2018-08-02T13:32:57.649857: step 13605, loss 0.525409.
Train: 2018-08-02T13:32:57.806072: step 13606, loss 0.505525.
Train: 2018-08-02T13:32:57.962295: step 13607, loss 0.548577.
Train: 2018-08-02T13:32:58.118468: step 13608, loss 0.484125.
Train: 2018-08-02T13:32:58.274681: step 13609, loss 0.488465.
Train: 2018-08-02T13:32:58.430925: step 13610, loss 0.547647.
Test: 2018-08-02T13:32:58.899565: step 13610, loss 0.548101.
Train: 2018-08-02T13:32:59.055748: step 13611, loss 0.54834.
Train: 2018-08-02T13:32:59.211992: step 13612, loss 0.504183.
Train: 2018-08-02T13:32:59.352554: step 13613, loss 0.4845.
Train: 2018-08-02T13:32:59.508797: step 13614, loss 0.506434.
Train: 2018-08-02T13:32:59.665010: step 13615, loss 0.509427.
Train: 2018-08-02T13:32:59.821223: step 13616, loss 0.533917.
Train: 2018-08-02T13:32:59.977408: step 13617, loss 0.591826.
Train: 2018-08-02T13:33:00.133651: step 13618, loss 0.485687.
Train: 2018-08-02T13:33:00.289865: step 13619, loss 0.53839.
Train: 2018-08-02T13:33:00.446073: step 13620, loss 0.50232.
Test: 2018-08-02T13:33:00.899091: step 13620, loss 0.550612.
Train: 2018-08-02T13:33:01.055311: step 13621, loss 0.492516.
Train: 2018-08-02T13:33:01.227148: step 13622, loss 0.471261.
Train: 2018-08-02T13:33:01.383359: step 13623, loss 0.459892.
Train: 2018-08-02T13:33:01.539544: step 13624, loss 0.502912.
Train: 2018-08-02T13:33:01.695785: step 13625, loss 0.564766.
Train: 2018-08-02T13:33:01.851968: step 13626, loss 0.547905.
Train: 2018-08-02T13:33:02.008181: step 13627, loss 0.454648.
Train: 2018-08-02T13:33:02.164430: step 13628, loss 0.485348.
Train: 2018-08-02T13:33:02.320638: step 13629, loss 0.550441.
Train: 2018-08-02T13:33:02.476855: step 13630, loss 0.419003.
Test: 2018-08-02T13:33:02.929866: step 13630, loss 0.548383.
Train: 2018-08-02T13:33:03.086055: step 13631, loss 0.592355.
Train: 2018-08-02T13:33:03.226679: step 13632, loss 0.493987.
Train: 2018-08-02T13:33:03.382889: step 13633, loss 0.532978.
Train: 2018-08-02T13:33:03.539104: step 13634, loss 0.447014.
Train: 2018-08-02T13:33:03.695317: step 13635, loss 0.440774.
Train: 2018-08-02T13:33:03.851533: step 13636, loss 0.469893.
Train: 2018-08-02T13:33:04.023360: step 13637, loss 0.643998.
Train: 2018-08-02T13:33:04.179578: step 13638, loss 0.455802.
Train: 2018-08-02T13:33:04.335792: step 13639, loss 0.502343.
Train: 2018-08-02T13:33:04.491975: step 13640, loss 0.553924.
Test: 2018-08-02T13:33:04.944994: step 13640, loss 0.546569.
Train: 2018-08-02T13:33:05.101240: step 13641, loss 0.516182.
Train: 2018-08-02T13:33:05.241830: step 13642, loss 0.507818.
Train: 2018-08-02T13:33:05.413634: step 13643, loss 0.513219.
Train: 2018-08-02T13:33:05.569873: step 13644, loss 0.460023.
Train: 2018-08-02T13:33:05.726062: step 13645, loss 0.601703.
Train: 2018-08-02T13:33:05.882275: step 13646, loss 0.459417.
Train: 2018-08-02T13:33:06.038489: step 13647, loss 0.56005.
Train: 2018-08-02T13:33:06.179110: step 13648, loss 0.475138.
Train: 2018-08-02T13:33:06.335294: step 13649, loss 0.526933.
Train: 2018-08-02T13:33:06.491507: step 13650, loss 0.537362.
Test: 2018-08-02T13:33:06.960172: step 13650, loss 0.549536.
Train: 2018-08-02T13:33:07.116393: step 13651, loss 0.492346.
Train: 2018-08-02T13:33:07.272575: step 13652, loss 0.50821.
Train: 2018-08-02T13:33:07.428818: step 13653, loss 0.591434.
Train: 2018-08-02T13:33:07.585028: step 13654, loss 0.510455.
Train: 2018-08-02T13:33:07.741244: step 13655, loss 0.542972.
Train: 2018-08-02T13:33:07.897427: step 13656, loss 0.511136.
Train: 2018-08-02T13:33:08.053671: step 13657, loss 0.554871.
Train: 2018-08-02T13:33:08.209887: step 13658, loss 0.424103.
Train: 2018-08-02T13:33:08.366067: step 13659, loss 0.502156.
Train: 2018-08-02T13:33:08.522307: step 13660, loss 0.48185.
Test: 2018-08-02T13:33:08.975300: step 13660, loss 0.549711.
Train: 2018-08-02T13:33:09.131514: step 13661, loss 0.467304.
Train: 2018-08-02T13:33:09.287757: step 13662, loss 0.584293.
Train: 2018-08-02T13:33:09.443940: step 13663, loss 0.441635.
Train: 2018-08-02T13:33:09.600186: step 13664, loss 0.502822.
Train: 2018-08-02T13:33:09.756391: step 13665, loss 0.481402.
Train: 2018-08-02T13:33:09.896989: step 13666, loss 0.563442.
Train: 2018-08-02T13:33:10.053172: step 13667, loss 0.592602.
Train: 2018-08-02T13:33:10.209388: step 13668, loss 0.493064.
Train: 2018-08-02T13:33:10.381221: step 13669, loss 0.469462.
Train: 2018-08-02T13:33:10.521843: step 13670, loss 0.508888.
Test: 2018-08-02T13:33:10.990454: step 13670, loss 0.550343.
Train: 2018-08-02T13:33:11.146698: step 13671, loss 0.523346.
Train: 2018-08-02T13:33:11.302881: step 13672, loss 0.529606.
Train: 2018-08-02T13:33:11.459121: step 13673, loss 0.428239.
Train: 2018-08-02T13:33:11.615333: step 13674, loss 0.545588.
Train: 2018-08-02T13:33:11.771551: step 13675, loss 0.48379.
Train: 2018-08-02T13:33:11.927764: step 13676, loss 0.476044.
Train: 2018-08-02T13:33:12.083977: step 13677, loss 0.695016.
Train: 2018-08-02T13:33:12.240162: step 13678, loss 0.556707.
Train: 2018-08-02T13:33:12.396373: step 13679, loss 0.52403.
Train: 2018-08-02T13:33:12.536996: step 13680, loss 0.522321.
Test: 2018-08-02T13:33:13.005607: step 13680, loss 0.567604.
Train: 2018-08-02T13:33:13.161819: step 13681, loss 0.543646.
Train: 2018-08-02T13:33:13.318063: step 13682, loss 0.530613.
Train: 2018-08-02T13:33:13.474280: step 13683, loss 0.51321.
Train: 2018-08-02T13:33:13.630490: step 13684, loss 0.548778.
Train: 2018-08-02T13:33:13.786698: step 13685, loss 0.517151.
Train: 2018-08-02T13:33:13.942918: step 13686, loss 0.465202.
Train: 2018-08-02T13:33:14.099133: step 13687, loss 0.589552.
Train: 2018-08-02T13:33:14.255344: step 13688, loss 0.457365.
Train: 2018-08-02T13:33:14.411562: step 13689, loss 0.638925.
Train: 2018-08-02T13:33:14.567772: step 13690, loss 0.495818.
Test: 2018-08-02T13:33:15.020764: step 13690, loss 0.549159.
Train: 2018-08-02T13:33:15.177002: step 13691, loss 0.474056.
Train: 2018-08-02T13:33:15.333214: step 13692, loss 0.456084.
Train: 2018-08-02T13:33:15.489400: step 13693, loss 0.541515.
Train: 2018-08-02T13:33:15.645643: step 13694, loss 0.521827.
Train: 2018-08-02T13:33:15.786235: step 13695, loss 0.551958.
Train: 2018-08-02T13:33:15.958065: step 13696, loss 0.587606.
Train: 2018-08-02T13:33:16.114253: step 13697, loss 0.484379.
Train: 2018-08-02T13:33:16.270497: step 13698, loss 0.430096.
Train: 2018-08-02T13:33:16.426711: step 13699, loss 0.583203.
Train: 2018-08-02T13:33:16.582895: step 13700, loss 0.606627.
Test: 2018-08-02T13:33:17.035914: step 13700, loss 0.549743.
Train: 2018-08-02T13:33:17.645175: step 13701, loss 0.51008.
Train: 2018-08-02T13:33:17.801384: step 13702, loss 0.50264.
Train: 2018-08-02T13:33:17.957571: step 13703, loss 0.528081.
Train: 2018-08-02T13:33:18.113815: step 13704, loss 0.514108.
Train: 2018-08-02T13:33:18.254403: step 13705, loss 0.485864.
Train: 2018-08-02T13:33:18.410617: step 13706, loss 0.553631.
Train: 2018-08-02T13:33:18.566830: step 13707, loss 0.556024.
Train: 2018-08-02T13:33:18.723051: step 13708, loss 0.457177.
Train: 2018-08-02T13:33:18.879262: step 13709, loss 0.472309.
Train: 2018-08-02T13:33:19.051097: step 13710, loss 0.484168.
Test: 2018-08-02T13:33:19.504129: step 13710, loss 0.54776.
Train: 2018-08-02T13:33:19.660298: step 13711, loss 0.6138.
Train: 2018-08-02T13:33:19.816513: step 13712, loss 0.55241.
Train: 2018-08-02T13:33:19.972724: step 13713, loss 0.520979.
Train: 2018-08-02T13:33:20.128938: step 13714, loss 0.552501.
Train: 2018-08-02T13:33:20.285182: step 13715, loss 0.522215.
Train: 2018-08-02T13:33:20.441364: step 13716, loss 0.500121.
Train: 2018-08-02T13:33:20.581987: step 13717, loss 0.525936.
Train: 2018-08-02T13:33:20.738203: step 13718, loss 0.533589.
Train: 2018-08-02T13:33:20.894414: step 13719, loss 0.459029.
Train: 2018-08-02T13:33:21.050627: step 13720, loss 0.510774.
Test: 2018-08-02T13:33:21.503616: step 13720, loss 0.551707.
Train: 2018-08-02T13:33:21.659860: step 13721, loss 0.486762.
Train: 2018-08-02T13:33:21.816043: step 13722, loss 0.480595.
Train: 2018-08-02T13:33:21.972290: step 13723, loss 0.456612.
Train: 2018-08-02T13:33:22.128501: step 13724, loss 0.455842.
Train: 2018-08-02T13:33:22.284683: step 13725, loss 0.501468.
Train: 2018-08-02T13:33:22.440928: step 13726, loss 0.541613.
Train: 2018-08-02T13:33:22.612775: step 13727, loss 0.427001.
Train: 2018-08-02T13:33:22.768977: step 13728, loss 0.503404.
Train: 2018-08-02T13:33:22.925185: step 13729, loss 0.514251.
Train: 2018-08-02T13:33:23.081372: step 13730, loss 0.468935.
Test: 2018-08-02T13:33:23.534416: step 13730, loss 0.546292.
Train: 2018-08-02T13:33:23.690634: step 13731, loss 0.590487.
Train: 2018-08-02T13:33:23.846818: step 13732, loss 0.559587.
Train: 2018-08-02T13:33:24.003061: step 13733, loss 0.526824.
Train: 2018-08-02T13:33:24.143653: step 13734, loss 0.47348.
Train: 2018-08-02T13:33:24.299867: step 13735, loss 0.498562.
Train: 2018-08-02T13:33:24.456080: step 13736, loss 0.489031.
Train: 2018-08-02T13:33:24.612294: step 13737, loss 0.509027.
Train: 2018-08-02T13:33:24.768507: step 13738, loss 0.545274.
Train: 2018-08-02T13:33:24.924715: step 13739, loss 0.562601.
Train: 2018-08-02T13:33:25.080937: step 13740, loss 0.552249.
Test: 2018-08-02T13:33:25.549544: step 13740, loss 0.554308.
Train: 2018-08-02T13:33:25.658894: step 13741, loss 0.558469.
Train: 2018-08-02T13:33:25.815136: step 13742, loss 0.496308.
Train: 2018-08-02T13:33:25.955728: step 13743, loss 0.476929.
Train: 2018-08-02T13:33:26.111942: step 13744, loss 0.505843.
Train: 2018-08-02T13:33:26.268150: step 13745, loss 0.482609.
Train: 2018-08-02T13:33:26.424338: step 13746, loss 0.523653.
Train: 2018-08-02T13:33:26.580554: step 13747, loss 0.451198.
Train: 2018-08-02T13:33:26.736796: step 13748, loss 0.553317.
Train: 2018-08-02T13:33:26.892980: step 13749, loss 0.49421.
Train: 2018-08-02T13:33:27.049224: step 13750, loss 0.493471.
Test: 2018-08-02T13:33:27.502211: step 13750, loss 0.547149.
Train: 2018-08-02T13:33:27.658425: step 13751, loss 0.444951.
Train: 2018-08-02T13:33:27.814669: step 13752, loss 0.471383.
Train: 2018-08-02T13:33:27.970885: step 13753, loss 0.56408.
Train: 2018-08-02T13:33:28.127095: step 13754, loss 0.490419.
Train: 2018-08-02T13:33:28.283278: step 13755, loss 0.475415.
Train: 2018-08-02T13:33:28.439518: step 13756, loss 0.552448.
Train: 2018-08-02T13:33:28.595739: step 13757, loss 0.470738.
Train: 2018-08-02T13:33:28.751949: step 13758, loss 0.531247.
Train: 2018-08-02T13:33:28.892541: step 13759, loss 0.54835.
Train: 2018-08-02T13:33:29.048754: step 13760, loss 0.510029.
Test: 2018-08-02T13:33:29.517366: step 13760, loss 0.548646.
Train: 2018-08-02T13:33:29.673582: step 13761, loss 0.48238.
Train: 2018-08-02T13:33:29.814170: step 13762, loss 0.544071.
Train: 2018-08-02T13:33:29.970413: step 13763, loss 0.468166.
Train: 2018-08-02T13:33:30.126623: step 13764, loss 0.480213.
Train: 2018-08-02T13:33:30.282841: step 13765, loss 0.526232.
Train: 2018-08-02T13:33:30.439050: step 13766, loss 0.473208.
Train: 2018-08-02T13:33:30.595271: step 13767, loss 0.613951.
Train: 2018-08-02T13:33:30.767083: step 13768, loss 0.540828.
Train: 2018-08-02T13:33:30.923312: step 13769, loss 0.508919.
Train: 2018-08-02T13:33:31.079524: step 13770, loss 0.492305.
Test: 2018-08-02T13:33:31.532550: step 13770, loss 0.554335.
Train: 2018-08-02T13:33:31.688764: step 13771, loss 0.497177.
Train: 2018-08-02T13:33:31.844969: step 13772, loss 0.44089.
Train: 2018-08-02T13:33:32.016810: step 13773, loss 0.537647.
Train: 2018-08-02T13:33:32.173019: step 13774, loss 0.531829.
Train: 2018-08-02T13:33:32.313615: step 13775, loss 0.451705.
Train: 2018-08-02T13:33:32.469831: step 13776, loss 0.57494.
Train: 2018-08-02T13:33:32.626012: step 13777, loss 0.513093.
Train: 2018-08-02T13:33:32.782256: step 13778, loss 0.535926.
Train: 2018-08-02T13:33:32.938469: step 13779, loss 0.499746.
Train: 2018-08-02T13:33:33.079061: step 13780, loss 0.499436.
Test: 2018-08-02T13:33:33.532050: step 13780, loss 0.546498.
Train: 2018-08-02T13:33:33.688294: step 13781, loss 0.511736.
Train: 2018-08-02T13:33:33.844505: step 13782, loss 0.59817.
Train: 2018-08-02T13:33:34.000720: step 13783, loss 0.560556.
Train: 2018-08-02T13:33:34.156904: step 13784, loss 0.479753.
Train: 2018-08-02T13:33:34.328738: step 13785, loss 0.538164.
Train: 2018-08-02T13:33:34.484982: step 13786, loss 0.438066.
Train: 2018-08-02T13:33:34.641196: step 13787, loss 0.512284.
Train: 2018-08-02T13:33:34.797409: step 13788, loss 0.551179.
Train: 2018-08-02T13:33:34.953591: step 13789, loss 0.50542.
Train: 2018-08-02T13:33:35.109836: step 13790, loss 0.47436.
Test: 2018-08-02T13:33:35.562824: step 13790, loss 0.5486.
Train: 2018-08-02T13:33:35.719070: step 13791, loss 0.558875.
Train: 2018-08-02T13:33:35.875251: step 13792, loss 0.507905.
Train: 2018-08-02T13:33:36.015873: step 13793, loss 0.521176.
Train: 2018-08-02T13:33:36.172086: step 13794, loss 0.466034.
Train: 2018-08-02T13:33:36.328300: step 13795, loss 0.646874.
Train: 2018-08-02T13:33:36.484513: step 13796, loss 0.455199.
Train: 2018-08-02T13:33:36.640695: step 13797, loss 0.581288.
Train: 2018-08-02T13:33:36.796941: step 13798, loss 0.48955.
Train: 2018-08-02T13:33:36.953154: step 13799, loss 0.488924.
Train: 2018-08-02T13:33:37.109362: step 13800, loss 0.440015.
Test: 2018-08-02T13:33:37.562356: step 13800, loss 0.547346.
Train: 2018-08-02T13:33:38.187240: step 13801, loss 0.462215.
Train: 2018-08-02T13:33:38.343455: step 13802, loss 0.493415.
Train: 2018-08-02T13:33:38.499668: step 13803, loss 0.459629.
Train: 2018-08-02T13:33:38.655851: step 13804, loss 0.426364.
Train: 2018-08-02T13:33:38.812064: step 13805, loss 0.522366.
Train: 2018-08-02T13:33:38.952685: step 13806, loss 0.51788.
Train: 2018-08-02T13:33:39.108869: step 13807, loss 0.485041.
Train: 2018-08-02T13:33:39.265113: step 13808, loss 0.514518.
Train: 2018-08-02T13:33:39.421295: step 13809, loss 0.512833.
Train: 2018-08-02T13:33:39.577511: step 13810, loss 0.549109.
Test: 2018-08-02T13:33:40.046150: step 13810, loss 0.547907.
Train: 2018-08-02T13:33:40.202387: step 13811, loss 0.494166.
Train: 2018-08-02T13:33:40.358607: step 13812, loss 0.423262.
Train: 2018-08-02T13:33:40.514791: step 13813, loss 0.514557.
Train: 2018-08-02T13:33:40.671034: step 13814, loss 0.471739.
Train: 2018-08-02T13:33:40.827247: step 13815, loss 0.592973.
Train: 2018-08-02T13:33:40.967841: step 13816, loss 0.477841.
Train: 2018-08-02T13:33:41.124052: step 13817, loss 0.528374.
Train: 2018-08-02T13:33:41.295882: step 13818, loss 0.517727.
Train: 2018-08-02T13:33:41.436478: step 13819, loss 0.510514.
Train: 2018-08-02T13:33:41.592694: step 13820, loss 0.506614.
Test: 2018-08-02T13:33:42.045682: step 13820, loss 0.551078.
Train: 2018-08-02T13:33:42.201894: step 13821, loss 0.562627.
Train: 2018-08-02T13:33:42.358138: step 13822, loss 0.424197.
Train: 2018-08-02T13:33:42.514322: step 13823, loss 0.541212.
Train: 2018-08-02T13:33:42.670565: step 13824, loss 0.527774.
Train: 2018-08-02T13:33:42.826778: step 13825, loss 0.610768.
Train: 2018-08-02T13:33:42.982992: step 13826, loss 0.520932.
Train: 2018-08-02T13:33:43.154846: step 13827, loss 0.503819.
Train: 2018-08-02T13:33:43.311011: step 13828, loss 0.464722.
Train: 2018-08-02T13:33:43.467256: step 13829, loss 0.504228.
Train: 2018-08-02T13:33:43.623467: step 13830, loss 0.505501.
Test: 2018-08-02T13:33:44.076480: step 13830, loss 0.546911.
Train: 2018-08-02T13:33:44.232670: step 13831, loss 0.553765.
Train: 2018-08-02T13:33:44.388912: step 13832, loss 0.53283.
Train: 2018-08-02T13:33:44.545121: step 13833, loss 0.523812.
Train: 2018-08-02T13:33:44.701339: step 13834, loss 0.479656.
Train: 2018-08-02T13:33:44.857553: step 13835, loss 0.49651.
Train: 2018-08-02T13:33:45.013768: step 13836, loss 0.487612.
Train: 2018-08-02T13:33:45.154327: step 13837, loss 0.465698.
Train: 2018-08-02T13:33:45.326173: step 13838, loss 0.599122.
Train: 2018-08-02T13:33:45.466787: step 13839, loss 0.503581.
Train: 2018-08-02T13:33:45.622969: step 13840, loss 0.492007.
Test: 2018-08-02T13:33:46.076016: step 13840, loss 0.549354.
Train: 2018-08-02T13:33:46.232200: step 13841, loss 0.524027.
Train: 2018-08-02T13:33:46.388444: step 13842, loss 0.472185.
Train: 2018-08-02T13:33:46.544657: step 13843, loss 0.483679.
Train: 2018-08-02T13:33:46.685250: step 13844, loss 0.521762.
Train: 2018-08-02T13:33:46.857055: step 13845, loss 0.43127.
Train: 2018-08-02T13:33:47.013298: step 13846, loss 0.530875.
Train: 2018-08-02T13:33:47.169481: step 13847, loss 0.511571.
Train: 2018-08-02T13:33:47.325727: step 13848, loss 0.517004.
Train: 2018-08-02T13:33:47.481910: step 13849, loss 0.522776.
Train: 2018-08-02T13:33:47.638146: step 13850, loss 0.608191.
Test: 2018-08-02T13:33:48.091141: step 13850, loss 0.555042.
Train: 2018-08-02T13:33:48.247384: step 13851, loss 0.496743.
Train: 2018-08-02T13:33:48.403598: step 13852, loss 0.46956.
Train: 2018-08-02T13:33:48.559811: step 13853, loss 0.500247.
Train: 2018-08-02T13:33:48.716023: step 13854, loss 0.491319.
Train: 2018-08-02T13:33:48.872238: step 13855, loss 0.516131.
Train: 2018-08-02T13:33:49.044041: step 13856, loss 0.545112.
Train: 2018-08-02T13:33:49.200255: step 13857, loss 0.515135.
Train: 2018-08-02T13:33:49.356494: step 13858, loss 0.538463.
Train: 2018-08-02T13:33:49.512712: step 13859, loss 0.511957.
Train: 2018-08-02T13:33:49.668925: step 13860, loss 0.524443.
Test: 2018-08-02T13:33:50.121914: step 13860, loss 0.549401.
Train: 2018-08-02T13:33:50.278129: step 13861, loss 0.532213.
Train: 2018-08-02T13:33:50.434342: step 13862, loss 0.458054.
Train: 2018-08-02T13:33:50.590555: step 13863, loss 0.499488.
Train: 2018-08-02T13:33:50.746818: step 13864, loss 0.548241.
Train: 2018-08-02T13:33:50.903008: step 13865, loss 0.513109.
Train: 2018-08-02T13:33:51.059221: step 13866, loss 0.4747.
Train: 2018-08-02T13:33:51.215410: step 13867, loss 0.516662.
Train: 2018-08-02T13:33:51.371652: step 13868, loss 0.465368.
Train: 2018-08-02T13:33:51.527870: step 13869, loss 0.529359.
Train: 2018-08-02T13:33:51.668428: step 13870, loss 0.527628.
Test: 2018-08-02T13:33:52.137067: step 13870, loss 0.546914.
Train: 2018-08-02T13:33:52.293314: step 13871, loss 0.42642.
Train: 2018-08-02T13:33:52.449528: step 13872, loss 0.529432.
Train: 2018-08-02T13:33:52.605738: step 13873, loss 0.6012.
Train: 2018-08-02T13:33:52.761951: step 13874, loss 0.556608.
Train: 2018-08-02T13:33:52.902512: step 13875, loss 0.500862.
Train: 2018-08-02T13:33:53.058757: step 13876, loss 0.551353.
Train: 2018-08-02T13:33:53.214973: step 13877, loss 0.552269.
Train: 2018-08-02T13:33:53.371183: step 13878, loss 0.555052.
Train: 2018-08-02T13:33:53.511745: step 13879, loss 0.576094.
Train: 2018-08-02T13:33:53.667989: step 13880, loss 0.541364.
Test: 2018-08-02T13:33:54.120979: step 13880, loss 0.556946.
Train: 2018-08-02T13:33:54.277191: step 13881, loss 0.53138.
Train: 2018-08-02T13:33:54.433431: step 13882, loss 0.549729.
Train: 2018-08-02T13:33:54.589652: step 13883, loss 0.531475.
Train: 2018-08-02T13:33:54.745873: step 13884, loss 0.503751.
Train: 2018-08-02T13:33:54.917697: step 13885, loss 0.535613.
Train: 2018-08-02T13:33:55.073905: step 13886, loss 0.557444.
Train: 2018-08-02T13:33:55.230123: step 13887, loss 0.504882.
Train: 2018-08-02T13:33:55.386337: step 13888, loss 0.504477.
Train: 2018-08-02T13:33:55.542520: step 13889, loss 0.49643.
Train: 2018-08-02T13:33:55.683113: step 13890, loss 0.480911.
Test: 2018-08-02T13:33:56.151754: step 13890, loss 0.548228.
Train: 2018-08-02T13:33:56.292346: step 13891, loss 0.45823.
Train: 2018-08-02T13:33:56.401724: step 13892, loss 0.501843.
Train: 2018-08-02T13:33:56.573557: step 13893, loss 0.54552.
Train: 2018-08-02T13:33:56.729772: step 13894, loss 0.461791.
Train: 2018-08-02T13:33:56.885956: step 13895, loss 0.55232.
Train: 2018-08-02T13:33:57.042199: step 13896, loss 0.437136.
Train: 2018-08-02T13:33:57.198397: step 13897, loss 0.467414.
Train: 2018-08-02T13:33:57.354627: step 13898, loss 0.532455.
Train: 2018-08-02T13:33:57.510865: step 13899, loss 0.560511.
Train: 2018-08-02T13:33:57.667024: step 13900, loss 0.535882.
Test: 2018-08-02T13:33:58.120041: step 13900, loss 0.548561.
Train: 2018-08-02T13:33:58.807412: step 13901, loss 0.491234.
Train: 2018-08-02T13:33:58.963626: step 13902, loss 0.491337.
Train: 2018-08-02T13:33:59.119838: step 13903, loss 0.44497.
Train: 2018-08-02T13:33:59.276052: step 13904, loss 0.477349.
Train: 2018-08-02T13:33:59.432265: step 13905, loss 0.468891.
Train: 2018-08-02T13:33:59.588475: step 13906, loss 0.377733.
Train: 2018-08-02T13:33:59.760284: step 13907, loss 0.494072.
Train: 2018-08-02T13:33:59.916496: step 13908, loss 0.527093.
Train: 2018-08-02T13:34:00.072743: step 13909, loss 0.506768.
Train: 2018-08-02T13:34:00.244575: step 13910, loss 0.503956.
Test: 2018-08-02T13:34:00.697563: step 13910, loss 0.547604.
Train: 2018-08-02T13:34:00.869424: step 13911, loss 0.507506.
Train: 2018-08-02T13:34:01.010020: step 13912, loss 0.412243.
Train: 2018-08-02T13:34:01.166203: step 13913, loss 0.461468.
Train: 2018-08-02T13:34:01.322447: step 13914, loss 0.646874.
Train: 2018-08-02T13:34:01.478660: step 13915, loss 0.549821.
Train: 2018-08-02T13:34:01.650496: step 13916, loss 0.522775.
Train: 2018-08-02T13:34:01.806709: step 13917, loss 0.509209.
Train: 2018-08-02T13:34:01.962894: step 13918, loss 0.550401.
Train: 2018-08-02T13:34:02.119136: step 13919, loss 0.514991.
Train: 2018-08-02T13:34:02.275318: step 13920, loss 0.531203.
Test: 2018-08-02T13:34:02.728337: step 13920, loss 0.55645.
Train: 2018-08-02T13:34:02.884552: step 13921, loss 0.516612.
Train: 2018-08-02T13:34:03.040765: step 13922, loss 0.568934.
Train: 2018-08-02T13:34:03.196977: step 13923, loss 0.451818.
Train: 2018-08-02T13:34:03.353191: step 13924, loss 0.528331.
Train: 2018-08-02T13:34:03.509434: step 13925, loss 0.511281.
Train: 2018-08-02T13:34:03.665648: step 13926, loss 0.501402.
Train: 2018-08-02T13:34:03.821861: step 13927, loss 0.603613.
Train: 2018-08-02T13:34:03.978045: step 13928, loss 0.432643.
Train: 2018-08-02T13:34:04.118667: step 13929, loss 0.489137.
Train: 2018-08-02T13:34:04.274880: step 13930, loss 0.607784.
Test: 2018-08-02T13:34:04.743490: step 13930, loss 0.55154.
Train: 2018-08-02T13:34:04.899706: step 13931, loss 0.522348.
Train: 2018-08-02T13:34:05.055942: step 13932, loss 0.535644.
Train: 2018-08-02T13:34:05.212164: step 13933, loss 0.597351.
Train: 2018-08-02T13:34:05.368375: step 13934, loss 0.55357.
Train: 2018-08-02T13:34:05.524583: step 13935, loss 0.530408.
Train: 2018-08-02T13:34:05.680773: step 13936, loss 0.600877.
Train: 2018-08-02T13:34:05.821393: step 13937, loss 0.516837.
Train: 2018-08-02T13:34:05.977606: step 13938, loss 0.559484.
Train: 2018-08-02T13:34:06.133790: step 13939, loss 0.52546.
Train: 2018-08-02T13:34:06.290027: step 13940, loss 0.614736.
Test: 2018-08-02T13:34:06.743024: step 13940, loss 0.554127.
Train: 2018-08-02T13:34:06.899266: step 13941, loss 0.461385.
Train: 2018-08-02T13:34:07.055451: step 13942, loss 0.534171.
Train: 2018-08-02T13:34:07.211691: step 13943, loss 0.538316.
Train: 2018-08-02T13:34:07.367877: step 13944, loss 0.471691.
Train: 2018-08-02T13:34:07.524121: step 13945, loss 0.515988.
Train: 2018-08-02T13:34:07.680333: step 13946, loss 0.454379.
Train: 2018-08-02T13:34:07.836549: step 13947, loss 0.537806.
Train: 2018-08-02T13:34:07.992760: step 13948, loss 0.435852.
Train: 2018-08-02T13:34:08.148942: step 13949, loss 0.567675.
Train: 2018-08-02T13:34:08.305187: step 13950, loss 0.493242.
Test: 2018-08-02T13:34:08.758175: step 13950, loss 0.548655.
Train: 2018-08-02T13:34:08.914413: step 13951, loss 0.554207.
Train: 2018-08-02T13:34:09.086257: step 13952, loss 0.497518.
Train: 2018-08-02T13:34:09.242467: step 13953, loss 0.51447.
Train: 2018-08-02T13:34:09.398681: step 13954, loss 0.6118.
Train: 2018-08-02T13:34:09.539273: step 13955, loss 0.49147.
Train: 2018-08-02T13:34:09.695455: step 13956, loss 0.494906.
Train: 2018-08-02T13:34:09.851699: step 13957, loss 0.512022.
Train: 2018-08-02T13:34:10.007883: step 13958, loss 0.566857.
Train: 2018-08-02T13:34:10.164127: step 13959, loss 0.490901.
Train: 2018-08-02T13:34:10.320340: step 13960, loss 0.506135.
Test: 2018-08-02T13:34:10.773355: step 13960, loss 0.547519.
Train: 2018-08-02T13:34:10.929542: step 13961, loss 0.423614.
Train: 2018-08-02T13:34:11.085780: step 13962, loss 0.598389.
Train: 2018-08-02T13:34:11.241971: step 13963, loss 0.436462.
Train: 2018-08-02T13:34:11.398220: step 13964, loss 0.530535.
Train: 2018-08-02T13:34:11.570045: step 13965, loss 0.49219.
Train: 2018-08-02T13:34:11.726262: step 13966, loss 0.495189.
Train: 2018-08-02T13:34:11.882475: step 13967, loss 0.578393.
Train: 2018-08-02T13:34:12.038658: step 13968, loss 0.497676.
Train: 2018-08-02T13:34:12.194871: step 13969, loss 0.527854.
Train: 2018-08-02T13:34:12.351114: step 13970, loss 0.528508.
Test: 2018-08-02T13:34:12.804137: step 13970, loss 0.551796.
Train: 2018-08-02T13:34:12.960317: step 13971, loss 0.520874.
Train: 2018-08-02T13:34:13.100934: step 13972, loss 0.468269.
Train: 2018-08-02T13:34:13.257152: step 13973, loss 0.527959.
Train: 2018-08-02T13:34:13.413361: step 13974, loss 0.490989.
Train: 2018-08-02T13:34:13.569582: step 13975, loss 0.608166.
Train: 2018-08-02T13:34:13.725763: step 13976, loss 0.422681.
Train: 2018-08-02T13:34:13.882007: step 13977, loss 0.474697.
Train: 2018-08-02T13:34:14.038189: step 13978, loss 0.567441.
Train: 2018-08-02T13:34:14.210026: step 13979, loss 0.574491.
Train: 2018-08-02T13:34:14.366268: step 13980, loss 0.476299.
Test: 2018-08-02T13:34:14.819257: step 13980, loss 0.564073.
Train: 2018-08-02T13:34:14.975501: step 13981, loss 0.480621.
Train: 2018-08-02T13:34:15.131714: step 13982, loss 0.534519.
Train: 2018-08-02T13:34:15.287898: step 13983, loss 0.511286.
Train: 2018-08-02T13:34:15.444140: step 13984, loss 0.482256.
Train: 2018-08-02T13:34:15.600322: step 13985, loss 0.524016.
Train: 2018-08-02T13:34:15.740915: step 13986, loss 0.543762.
Train: 2018-08-02T13:34:15.897158: step 13987, loss 0.555911.
Train: 2018-08-02T13:34:16.053342: step 13988, loss 0.469493.
Train: 2018-08-02T13:34:16.209555: step 13989, loss 0.546359.
Train: 2018-08-02T13:34:16.365799: step 13990, loss 0.446294.
Test: 2018-08-02T13:34:16.818816: step 13990, loss 0.554374.
Train: 2018-08-02T13:34:16.975034: step 13991, loss 0.438732.
Train: 2018-08-02T13:34:17.131216: step 13992, loss 0.532693.
Train: 2018-08-02T13:34:17.287453: step 13993, loss 0.50447.
Train: 2018-08-02T13:34:17.443642: step 13994, loss 0.52893.
Train: 2018-08-02T13:34:17.599881: step 13995, loss 0.483706.
Train: 2018-08-02T13:34:17.756094: step 13996, loss 0.467605.
Train: 2018-08-02T13:34:17.912307: step 13997, loss 0.476269.
Train: 2018-08-02T13:34:18.068542: step 13998, loss 0.503012.
Train: 2018-08-02T13:34:18.224740: step 13999, loss 0.561403.
Train: 2018-08-02T13:34:18.380935: step 14000, loss 0.500868.
Test: 2018-08-02T13:34:18.833974: step 14000, loss 0.549101.
Train: 2018-08-02T13:34:19.443203: step 14001, loss 0.567445.
Train: 2018-08-02T13:34:19.599417: step 14002, loss 0.472873.
Train: 2018-08-02T13:34:19.755631: step 14003, loss 0.548757.
Train: 2018-08-02T13:34:19.911845: step 14004, loss 0.487416.
Train: 2018-08-02T13:34:20.068060: step 14005, loss 0.498377.
Train: 2018-08-02T13:34:20.224241: step 14006, loss 0.536755.
Train: 2018-08-02T13:34:20.396077: step 14007, loss 0.54298.
Train: 2018-08-02T13:34:20.552297: step 14008, loss 0.592067.
Train: 2018-08-02T13:34:20.708535: step 14009, loss 0.502727.
Train: 2018-08-02T13:34:20.864741: step 14010, loss 0.536098.
Test: 2018-08-02T13:34:21.317734: step 14010, loss 0.554188.
Train: 2018-08-02T13:34:21.473948: step 14011, loss 0.518262.
Train: 2018-08-02T13:34:21.630162: step 14012, loss 0.534626.
Train: 2018-08-02T13:34:21.786400: step 14013, loss 0.50803.
Train: 2018-08-02T13:34:21.958211: step 14014, loss 0.497058.
Train: 2018-08-02T13:34:22.114424: step 14015, loss 0.594533.
Train: 2018-08-02T13:34:22.255046: step 14016, loss 0.485304.
Train: 2018-08-02T13:34:22.411258: step 14017, loss 0.55735.
Train: 2018-08-02T13:34:22.567471: step 14018, loss 0.466676.
Train: 2018-08-02T13:34:22.723685: step 14019, loss 0.554486.
Train: 2018-08-02T13:34:22.864277: step 14020, loss 0.461333.
Test: 2018-08-02T13:34:23.332887: step 14020, loss 0.547865.
Train: 2018-08-02T13:34:23.489132: step 14021, loss 0.556957.
Train: 2018-08-02T13:34:23.645314: step 14022, loss 0.534414.
Train: 2018-08-02T13:34:23.801558: step 14023, loss 0.564172.
Train: 2018-08-02T13:34:23.957775: step 14024, loss 0.537245.
Train: 2018-08-02T13:34:24.113980: step 14025, loss 0.495452.
Train: 2018-08-02T13:34:24.270200: step 14026, loss 0.509716.
Train: 2018-08-02T13:34:24.426414: step 14027, loss 0.545003.
Train: 2018-08-02T13:34:24.582596: step 14028, loss 0.541675.
Train: 2018-08-02T13:34:24.738839: step 14029, loss 0.475643.
Train: 2018-08-02T13:34:24.895023: step 14030, loss 0.396713.
Test: 2018-08-02T13:34:25.363686: step 14030, loss 0.553074.
Train: 2018-08-02T13:34:25.519900: step 14031, loss 0.539146.
Train: 2018-08-02T13:34:25.660468: step 14032, loss 0.495058.
Train: 2018-08-02T13:34:25.816711: step 14033, loss 0.417985.
Train: 2018-08-02T13:34:25.972894: step 14034, loss 0.474277.
Train: 2018-08-02T13:34:26.113516: step 14035, loss 0.52125.
Train: 2018-08-02T13:34:26.269730: step 14036, loss 0.581611.
Train: 2018-08-02T13:34:26.425944: step 14037, loss 0.451131.
Train: 2018-08-02T13:34:26.582159: step 14038, loss 0.520568.
Train: 2018-08-02T13:34:26.738370: step 14039, loss 0.529047.
Train: 2018-08-02T13:34:26.894579: step 14040, loss 0.527797.
Test: 2018-08-02T13:34:27.347574: step 14040, loss 0.5534.
Train: 2018-08-02T13:34:27.503817: step 14041, loss 0.503871.
Train: 2018-08-02T13:34:27.660030: step 14042, loss 0.50509.
Train: 2018-08-02T13:34:27.769349: step 14043, loss 0.530607.
Train: 2018-08-02T13:34:27.941211: step 14044, loss 0.49928.
Train: 2018-08-02T13:34:28.081808: step 14045, loss 0.471446.
Train: 2018-08-02T13:34:28.238018: step 14046, loss 0.437561.
Train: 2018-08-02T13:34:28.394232: step 14047, loss 0.507904.
Train: 2018-08-02T13:34:28.566039: step 14048, loss 0.468019.
Train: 2018-08-02T13:34:28.722281: step 14049, loss 0.48011.
Train: 2018-08-02T13:34:28.862872: step 14050, loss 0.478631.
Test: 2018-08-02T13:34:29.331512: step 14050, loss 0.547962.
Train: 2018-08-02T13:34:29.487727: step 14051, loss 0.546457.
Train: 2018-08-02T13:34:29.643935: step 14052, loss 0.467053.
Train: 2018-08-02T13:34:29.800154: step 14053, loss 0.471343.
Train: 2018-08-02T13:34:29.956369: step 14054, loss 0.504367.
Train: 2018-08-02T13:34:30.112574: step 14055, loss 0.511112.
Train: 2018-08-02T13:34:30.268794: step 14056, loss 0.565696.
Train: 2018-08-02T13:34:30.425011: step 14057, loss 0.520163.
Train: 2018-08-02T13:34:30.596813: step 14058, loss 0.498975.
Train: 2018-08-02T13:34:30.753049: step 14059, loss 0.476879.
Train: 2018-08-02T13:34:30.893647: step 14060, loss 0.513603.
Test: 2018-08-02T13:34:31.362257: step 14060, loss 0.549212.
Train: 2018-08-02T13:34:31.502879: step 14061, loss 0.42658.
Train: 2018-08-02T13:34:31.659094: step 14062, loss 0.490532.
Train: 2018-08-02T13:34:31.815305: step 14063, loss 0.488699.
Train: 2018-08-02T13:34:31.971520: step 14064, loss 0.417137.
Train: 2018-08-02T13:34:32.127733: step 14065, loss 0.442972.
Train: 2018-08-02T13:34:32.299568: step 14066, loss 0.474007.
Train: 2018-08-02T13:34:32.455784: step 14067, loss 0.483861.
Train: 2018-08-02T13:34:32.611974: step 14068, loss 0.612036.
Train: 2018-08-02T13:34:32.768179: step 14069, loss 0.59978.
Train: 2018-08-02T13:34:32.924393: step 14070, loss 0.575609.
Test: 2018-08-02T13:34:33.393044: step 14070, loss 0.562765.
Train: 2018-08-02T13:34:33.533624: step 14071, loss 0.48602.
Train: 2018-08-02T13:34:33.689838: step 14072, loss 0.486029.
Train: 2018-08-02T13:34:33.846080: step 14073, loss 0.5226.
Train: 2018-08-02T13:34:34.002294: step 14074, loss 0.520475.
Train: 2018-08-02T13:34:34.158507: step 14075, loss 0.505722.
Train: 2018-08-02T13:34:34.299070: step 14076, loss 0.466494.
Train: 2018-08-02T13:34:34.455287: step 14077, loss 0.575658.
Train: 2018-08-02T13:34:34.611528: step 14078, loss 0.519836.
Train: 2018-08-02T13:34:34.767743: step 14079, loss 0.523592.
Train: 2018-08-02T13:34:34.923922: step 14080, loss 0.427233.
Test: 2018-08-02T13:34:35.439461: step 14080, loss 0.547156.
Train: 2018-08-02T13:34:35.595671: step 14081, loss 0.41977.
Train: 2018-08-02T13:34:35.751880: step 14082, loss 0.453395.
Train: 2018-08-02T13:34:35.892480: step 14083, loss 0.569129.
Train: 2018-08-02T13:34:36.048689: step 14084, loss 0.459583.
Train: 2018-08-02T13:34:36.204906: step 14085, loss 0.573688.
Train: 2018-08-02T13:34:36.376738: step 14086, loss 0.554104.
Train: 2018-08-02T13:34:36.532952: step 14087, loss 0.515302.
Train: 2018-08-02T13:34:36.689165: step 14088, loss 0.512073.
Train: 2018-08-02T13:34:36.845373: step 14089, loss 0.55772.
Train: 2018-08-02T13:34:37.001593: step 14090, loss 0.572879.
Test: 2018-08-02T13:34:37.454581: step 14090, loss 0.554918.
Train: 2018-08-02T13:34:37.595173: step 14091, loss 0.53028.
Train: 2018-08-02T13:34:37.751387: step 14092, loss 0.522211.
Train: 2018-08-02T13:34:37.923221: step 14093, loss 0.484371.
Train: 2018-08-02T13:34:38.079456: step 14094, loss 0.427001.
Train: 2018-08-02T13:34:38.220056: step 14095, loss 0.611111.
Train: 2018-08-02T13:34:38.376240: step 14096, loss 0.505848.
Train: 2018-08-02T13:34:38.532483: step 14097, loss 0.490333.
Train: 2018-08-02T13:34:38.688667: step 14098, loss 0.547439.
Train: 2018-08-02T13:34:38.844881: step 14099, loss 0.512096.
Train: 2018-08-02T13:34:39.001124: step 14100, loss 0.520304.
Test: 2018-08-02T13:34:39.454112: step 14100, loss 0.550455.
Train: 2018-08-02T13:34:40.063344: step 14101, loss 0.49146.
Train: 2018-08-02T13:34:40.219559: step 14102, loss 0.498861.
Train: 2018-08-02T13:34:40.375803: step 14103, loss 0.557435.
Train: 2018-08-02T13:34:40.532020: step 14104, loss 0.534476.
Train: 2018-08-02T13:34:40.688229: step 14105, loss 0.488631.
Train: 2018-08-02T13:34:40.860033: step 14106, loss 0.472113.
Train: 2018-08-02T13:34:41.016257: step 14107, loss 0.520186.
Train: 2018-08-02T13:34:41.172465: step 14108, loss 0.526013.
Train: 2018-08-02T13:34:41.328707: step 14109, loss 0.515527.
Train: 2018-08-02T13:34:41.484916: step 14110, loss 0.616702.
Test: 2018-08-02T13:34:41.937906: step 14110, loss 0.547414.
Train: 2018-08-02T13:34:42.094150: step 14111, loss 0.570601.
Train: 2018-08-02T13:34:42.245750: step 14112, loss 0.487243.
Train: 2018-08-02T13:34:42.401934: step 14113, loss 0.568558.
Train: 2018-08-02T13:34:42.558176: step 14114, loss 0.565108.
Train: 2018-08-02T13:34:42.714360: step 14115, loss 0.544317.
Train: 2018-08-02T13:34:42.854952: step 14116, loss 0.517579.
Train: 2018-08-02T13:34:43.011191: step 14117, loss 0.507323.
Train: 2018-08-02T13:34:43.167412: step 14118, loss 0.528223.
Train: 2018-08-02T13:34:43.339240: step 14119, loss 0.533524.
Train: 2018-08-02T13:34:43.495453: step 14120, loss 0.527844.
Test: 2018-08-02T13:34:43.948445: step 14120, loss 0.547461.
Train: 2018-08-02T13:34:44.104691: step 14121, loss 0.616763.
Train: 2018-08-02T13:34:44.260899: step 14122, loss 0.517903.
Train: 2018-08-02T13:34:44.432709: step 14123, loss 0.566388.
Train: 2018-08-02T13:34:44.588954: step 14124, loss 0.454555.
Train: 2018-08-02T13:34:44.745158: step 14125, loss 0.658796.
Train: 2018-08-02T13:34:44.901349: step 14126, loss 0.487182.
Train: 2018-08-02T13:34:45.057562: step 14127, loss 0.461937.
Train: 2018-08-02T13:34:45.213807: step 14128, loss 0.504739.
Train: 2018-08-02T13:34:45.369989: step 14129, loss 0.674454.
Train: 2018-08-02T13:34:45.510606: step 14130, loss 0.511751.
Test: 2018-08-02T13:34:45.979221: step 14130, loss 0.549625.
Train: 2018-08-02T13:34:46.135459: step 14131, loss 0.463686.
Train: 2018-08-02T13:34:46.291649: step 14132, loss 0.474764.
Train: 2018-08-02T13:34:46.447891: step 14133, loss 0.558127.
Train: 2018-08-02T13:34:46.619695: step 14134, loss 0.502979.
Train: 2018-08-02T13:34:46.775944: step 14135, loss 0.516827.
Train: 2018-08-02T13:34:46.932153: step 14136, loss 0.467678.
Train: 2018-08-02T13:34:47.088368: step 14137, loss 0.453323.
Train: 2018-08-02T13:34:47.244550: step 14138, loss 0.49606.
Train: 2018-08-02T13:34:47.400793: step 14139, loss 0.458329.
Train: 2018-08-02T13:34:47.557012: step 14140, loss 0.556358.
Test: 2018-08-02T13:34:48.025642: step 14140, loss 0.54917.
Train: 2018-08-02T13:34:48.181830: step 14141, loss 0.517214.
Train: 2018-08-02T13:34:48.322451: step 14142, loss 0.486459.
Train: 2018-08-02T13:34:48.478634: step 14143, loss 0.493654.
Train: 2018-08-02T13:34:48.634848: step 14144, loss 0.53104.
Train: 2018-08-02T13:34:48.791089: step 14145, loss 0.503178.
Train: 2018-08-02T13:34:48.947305: step 14146, loss 0.540303.
Train: 2018-08-02T13:34:49.103519: step 14147, loss 0.515047.
Train: 2018-08-02T13:34:49.259733: step 14148, loss 0.537834.
Train: 2018-08-02T13:34:49.415946: step 14149, loss 0.571272.
Train: 2018-08-02T13:34:49.572160: step 14150, loss 0.56118.
Test: 2018-08-02T13:34:50.040770: step 14150, loss 0.563055.
Train: 2018-08-02T13:34:50.197013: step 14151, loss 0.547945.
Train: 2018-08-02T13:34:50.353196: step 14152, loss 0.585372.
Train: 2018-08-02T13:34:50.509439: step 14153, loss 0.529258.
Train: 2018-08-02T13:34:50.650031: step 14154, loss 0.594705.
Train: 2018-08-02T13:34:50.806246: step 14155, loss 0.607213.
Train: 2018-08-02T13:34:50.978078: step 14156, loss 0.618676.
Train: 2018-08-02T13:34:51.118671: step 14157, loss 0.567479.
Train: 2018-08-02T13:34:51.274885: step 14158, loss 0.53979.
Train: 2018-08-02T13:34:51.431101: step 14159, loss 0.579259.
Train: 2018-08-02T13:34:51.587314: step 14160, loss 0.527389.
Test: 2018-08-02T13:34:52.040332: step 14160, loss 0.599251.
Train: 2018-08-02T13:34:52.196545: step 14161, loss 0.524357.
Train: 2018-08-02T13:34:52.352729: step 14162, loss 0.584546.
Train: 2018-08-02T13:34:52.524593: step 14163, loss 0.558801.
Train: 2018-08-02T13:34:52.665185: step 14164, loss 0.559476.
Train: 2018-08-02T13:34:52.821369: step 14165, loss 0.532786.
Train: 2018-08-02T13:34:52.977612: step 14166, loss 0.546017.
Train: 2018-08-02T13:34:53.133825: step 14167, loss 0.617131.
Train: 2018-08-02T13:34:53.290039: step 14168, loss 0.53839.
Train: 2018-08-02T13:34:53.446252: step 14169, loss 0.551893.
Train: 2018-08-02T13:34:53.618056: step 14170, loss 0.53668.
Test: 2018-08-02T13:34:54.071076: step 14170, loss 0.576158.
Train: 2018-08-02T13:34:54.274179: step 14171, loss 0.491076.
Train: 2018-08-02T13:34:54.430397: step 14172, loss 0.567085.
Train: 2018-08-02T13:34:54.586613: step 14173, loss 0.516354.
Train: 2018-08-02T13:34:54.742824: step 14174, loss 0.460103.
Train: 2018-08-02T13:34:54.899037: step 14175, loss 0.5021.
Train: 2018-08-02T13:34:55.070842: step 14176, loss 0.588306.
Train: 2018-08-02T13:34:55.227089: step 14177, loss 0.471955.
Train: 2018-08-02T13:34:55.383298: step 14178, loss 0.541301.
Train: 2018-08-02T13:34:55.539511: step 14179, loss 0.495398.
Train: 2018-08-02T13:34:55.680104: step 14180, loss 0.65851.
Test: 2018-08-02T13:34:56.148713: step 14180, loss 0.548078.
Train: 2018-08-02T13:34:56.304958: step 14181, loss 0.529124.
Train: 2018-08-02T13:34:56.445549: step 14182, loss 0.480333.
Train: 2018-08-02T13:34:56.601763: step 14183, loss 0.628344.
Train: 2018-08-02T13:34:56.757977: step 14184, loss 0.509791.
Train: 2018-08-02T13:34:56.914159: step 14185, loss 0.542043.
Train: 2018-08-02T13:34:57.070404: step 14186, loss 0.505463.
Train: 2018-08-02T13:34:57.226586: step 14187, loss 0.525006.
Train: 2018-08-02T13:34:57.398452: step 14188, loss 0.481179.
Train: 2018-08-02T13:34:57.554634: step 14189, loss 0.510551.
Train: 2018-08-02T13:34:57.710881: step 14190, loss 0.499538.
Test: 2018-08-02T13:34:58.163868: step 14190, loss 0.564562.
Train: 2018-08-02T13:34:58.320111: step 14191, loss 0.499351.
Train: 2018-08-02T13:34:58.476309: step 14192, loss 0.526394.
Train: 2018-08-02T13:34:58.632527: step 14193, loss 0.50923.
Train: 2018-08-02T13:34:58.757510: step 14194, loss 0.484408.
Train: 2018-08-02T13:34:58.913716: step 14195, loss 0.523686.
Train: 2018-08-02T13:34:59.069935: step 14196, loss 0.529155.
Train: 2018-08-02T13:34:59.226143: step 14197, loss 0.522349.
Train: 2018-08-02T13:34:59.382363: step 14198, loss 0.607864.
Train: 2018-08-02T13:34:59.538547: step 14199, loss 0.496152.
Train: 2018-08-02T13:34:59.694788: step 14200, loss 0.519322.
Test: 2018-08-02T13:35:00.163425: step 14200, loss 0.557805.
Train: 2018-08-02T13:35:00.835149: step 14201, loss 0.46493.
Train: 2018-08-02T13:35:00.991356: step 14202, loss 0.482185.
Train: 2018-08-02T13:35:01.147574: step 14203, loss 0.544842.
Train: 2018-08-02T13:35:01.319403: step 14204, loss 0.456174.
Train: 2018-08-02T13:35:01.475622: step 14205, loss 0.553463.
Train: 2018-08-02T13:35:01.631835: step 14206, loss 0.482517.
Train: 2018-08-02T13:35:01.788035: step 14207, loss 0.584797.
Train: 2018-08-02T13:35:01.944262: step 14208, loss 0.466683.
Train: 2018-08-02T13:35:02.084852: step 14209, loss 0.599978.
Train: 2018-08-02T13:35:02.256690: step 14210, loss 0.492753.
Test: 2018-08-02T13:35:02.709679: step 14210, loss 0.558153.
Train: 2018-08-02T13:35:02.865921: step 14211, loss 0.532107.
Train: 2018-08-02T13:35:03.006513: step 14212, loss 0.490502.
Train: 2018-08-02T13:35:03.178348: step 14213, loss 0.589284.
Train: 2018-08-02T13:35:03.318940: step 14214, loss 0.538106.
Train: 2018-08-02T13:35:03.475153: step 14215, loss 0.519698.
Train: 2018-08-02T13:35:03.631336: step 14216, loss 0.502203.
Train: 2018-08-02T13:35:03.787549: step 14217, loss 0.52657.
Train: 2018-08-02T13:35:03.943765: step 14218, loss 0.582469.
Train: 2018-08-02T13:35:04.100008: step 14219, loss 0.468031.
Train: 2018-08-02T13:35:04.256222: step 14220, loss 0.480685.
Test: 2018-08-02T13:35:04.724831: step 14220, loss 0.546867.
Train: 2018-08-02T13:35:04.881075: step 14221, loss 0.484539.
Train: 2018-08-02T13:35:05.037258: step 14222, loss 0.46765.
Train: 2018-08-02T13:35:05.193470: step 14223, loss 0.599598.
Train: 2018-08-02T13:35:05.349714: step 14224, loss 0.452935.
Train: 2018-08-02T13:35:05.505928: step 14225, loss 0.597515.
Train: 2018-08-02T13:35:05.662141: step 14226, loss 0.559471.
Train: 2018-08-02T13:35:05.818349: step 14227, loss 0.481123.
Train: 2018-08-02T13:35:05.974568: step 14228, loss 0.482517.
Train: 2018-08-02T13:35:06.130782: step 14229, loss 0.480563.
Train: 2018-08-02T13:35:06.286994: step 14230, loss 0.52389.
Test: 2018-08-02T13:35:06.740009: step 14230, loss 0.548723.
Train: 2018-08-02T13:35:06.896198: step 14231, loss 0.455893.
Train: 2018-08-02T13:35:07.052441: step 14232, loss 0.506211.
Train: 2018-08-02T13:35:07.208656: step 14233, loss 0.662936.
Train: 2018-08-02T13:35:07.364869: step 14234, loss 0.520539.
Train: 2018-08-02T13:35:07.521082: step 14235, loss 0.540449.
Train: 2018-08-02T13:35:07.692887: step 14236, loss 0.505804.
Train: 2018-08-02T13:35:07.849098: step 14237, loss 0.501996.
Train: 2018-08-02T13:35:07.989691: step 14238, loss 0.505231.
Train: 2018-08-02T13:35:08.145934: step 14239, loss 0.545745.
Train: 2018-08-02T13:35:08.302149: step 14240, loss 0.527423.
Test: 2018-08-02T13:35:08.770759: step 14240, loss 0.552445.
Train: 2018-08-02T13:35:08.911351: step 14241, loss 0.491536.
Train: 2018-08-02T13:35:09.067596: step 14242, loss 0.528836.
Train: 2018-08-02T13:35:09.239423: step 14243, loss 0.482802.
Train: 2018-08-02T13:35:09.395643: step 14244, loss 0.496463.
Train: 2018-08-02T13:35:09.551828: step 14245, loss 0.602617.
Train: 2018-08-02T13:35:09.708071: step 14246, loss 0.520588.
Train: 2018-08-02T13:35:09.864278: step 14247, loss 0.57033.
Train: 2018-08-02T13:35:10.020465: step 14248, loss 0.515802.
Train: 2018-08-02T13:35:10.176710: step 14249, loss 0.552126.
Train: 2018-08-02T13:35:10.332926: step 14250, loss 0.522558.
Test: 2018-08-02T13:35:10.801532: step 14250, loss 0.560546.
Train: 2018-08-02T13:35:10.957778: step 14251, loss 0.51454.
Train: 2018-08-02T13:35:11.098337: step 14252, loss 0.533466.
Train: 2018-08-02T13:35:11.254582: step 14253, loss 0.568988.
Train: 2018-08-02T13:35:11.410795: step 14254, loss 0.50797.
Train: 2018-08-02T13:35:11.567008: step 14255, loss 0.574265.
Train: 2018-08-02T13:35:11.723191: step 14256, loss 0.504777.
Train: 2018-08-02T13:35:11.879436: step 14257, loss 0.462442.
Train: 2018-08-02T13:35:12.035643: step 14258, loss 0.521188.
Train: 2018-08-02T13:35:12.207454: step 14259, loss 0.479015.
Train: 2018-08-02T13:35:12.363667: step 14260, loss 0.614309.
Test: 2018-08-02T13:35:12.816687: step 14260, loss 0.549186.
Train: 2018-08-02T13:35:12.972930: step 14261, loss 0.442026.
Train: 2018-08-02T13:35:13.129143: step 14262, loss 0.462105.
Train: 2018-08-02T13:35:13.285325: step 14263, loss 0.52077.
Train: 2018-08-02T13:35:13.441539: step 14264, loss 0.512801.
Train: 2018-08-02T13:35:13.597753: step 14265, loss 0.480966.
Train: 2018-08-02T13:35:13.753997: step 14266, loss 0.517811.
Train: 2018-08-02T13:35:13.910180: step 14267, loss 0.546583.
Train: 2018-08-02T13:35:14.066423: step 14268, loss 0.476464.
Train: 2018-08-02T13:35:14.222631: step 14269, loss 0.550887.
Train: 2018-08-02T13:35:14.378851: step 14270, loss 0.52951.
Test: 2018-08-02T13:35:14.847485: step 14270, loss 0.550967.
Train: 2018-08-02T13:35:15.003701: step 14271, loss 0.563341.
Train: 2018-08-02T13:35:15.159917: step 14272, loss 0.504981.
Train: 2018-08-02T13:35:15.316126: step 14273, loss 0.46691.
Train: 2018-08-02T13:35:15.472343: step 14274, loss 0.490341.
Train: 2018-08-02T13:35:15.644179: step 14275, loss 0.531669.
Train: 2018-08-02T13:35:15.800392: step 14276, loss 0.553758.
Train: 2018-08-02T13:35:15.956608: step 14277, loss 0.50736.
Train: 2018-08-02T13:35:16.097197: step 14278, loss 0.58763.
Train: 2018-08-02T13:35:16.253413: step 14279, loss 0.486382.
Train: 2018-08-02T13:35:16.409624: step 14280, loss 0.516234.
Test: 2018-08-02T13:35:16.862643: step 14280, loss 0.549594.
Train: 2018-08-02T13:35:17.018856: step 14281, loss 0.431945.
Train: 2018-08-02T13:35:17.175042: step 14282, loss 0.571172.
Train: 2018-08-02T13:35:17.331286: step 14283, loss 0.628617.
Train: 2018-08-02T13:35:17.487497: step 14284, loss 0.470203.
Train: 2018-08-02T13:35:17.643713: step 14285, loss 0.498857.
Train: 2018-08-02T13:35:17.799924: step 14286, loss 0.554891.
Train: 2018-08-02T13:35:17.956108: step 14287, loss 0.536149.
Train: 2018-08-02T13:35:18.096700: step 14288, loss 0.523562.
Train: 2018-08-02T13:35:18.252943: step 14289, loss 0.489404.
Train: 2018-08-02T13:35:18.409127: step 14290, loss 0.471853.
Test: 2018-08-02T13:35:18.862145: step 14290, loss 0.55072.
Train: 2018-08-02T13:35:19.018389: step 14291, loss 0.538173.
Train: 2018-08-02T13:35:19.174602: step 14292, loss 0.535321.
Train: 2018-08-02T13:35:19.330813: step 14293, loss 0.451664.
Train: 2018-08-02T13:35:19.502645: step 14294, loss 0.483918.
Train: 2018-08-02T13:35:19.658843: step 14295, loss 0.518195.
Train: 2018-08-02T13:35:19.815075: step 14296, loss 0.505062.
Train: 2018-08-02T13:35:19.971285: step 14297, loss 0.488228.
Train: 2018-08-02T13:35:20.127476: step 14298, loss 0.524416.
Train: 2018-08-02T13:35:20.283689: step 14299, loss 0.535704.
Train: 2018-08-02T13:35:20.439929: step 14300, loss 0.482757.
Test: 2018-08-02T13:35:20.892919: step 14300, loss 0.548668.
Train: 2018-08-02T13:35:21.502181: step 14301, loss 0.447129.
Train: 2018-08-02T13:35:21.658395: step 14302, loss 0.594811.
Train: 2018-08-02T13:35:21.814580: step 14303, loss 0.480706.
Train: 2018-08-02T13:35:21.970793: step 14304, loss 0.479302.
Train: 2018-08-02T13:35:22.127031: step 14305, loss 0.561823.
Train: 2018-08-02T13:35:22.283219: step 14306, loss 0.473481.
Train: 2018-08-02T13:35:22.439461: step 14307, loss 0.522389.
Train: 2018-08-02T13:35:22.595679: step 14308, loss 0.492101.
Train: 2018-08-02T13:35:22.751890: step 14309, loss 0.492421.
Train: 2018-08-02T13:35:22.908072: step 14310, loss 0.471801.
Test: 2018-08-02T13:35:23.361130: step 14310, loss 0.547852.
Train: 2018-08-02T13:35:23.517335: step 14311, loss 0.535158.
Train: 2018-08-02T13:35:23.673548: step 14312, loss 0.575435.
Train: 2018-08-02T13:35:23.829764: step 14313, loss 0.575543.
Train: 2018-08-02T13:35:23.970354: step 14314, loss 0.516795.
Train: 2018-08-02T13:35:24.126568: step 14315, loss 0.528499.
Train: 2018-08-02T13:35:24.282781: step 14316, loss 0.549534.
Train: 2018-08-02T13:35:24.454618: step 14317, loss 0.542247.
Train: 2018-08-02T13:35:24.610830: step 14318, loss 0.512496.
Train: 2018-08-02T13:35:24.767043: step 14319, loss 0.546309.
Train: 2018-08-02T13:35:24.923258: step 14320, loss 0.454588.
Test: 2018-08-02T13:35:25.376245: step 14320, loss 0.550548.
Train: 2018-08-02T13:35:25.532488: step 14321, loss 0.53144.
Train: 2018-08-02T13:35:25.688671: step 14322, loss 0.548845.
Train: 2018-08-02T13:35:25.844915: step 14323, loss 0.634067.
Train: 2018-08-02T13:35:26.001129: step 14324, loss 0.457461.
Train: 2018-08-02T13:35:26.141720: step 14325, loss 0.564903.
Train: 2018-08-02T13:35:26.297928: step 14326, loss 0.591739.
Train: 2018-08-02T13:35:26.454142: step 14327, loss 0.461763.
Train: 2018-08-02T13:35:26.610355: step 14328, loss 0.450983.
Train: 2018-08-02T13:35:26.766544: step 14329, loss 0.471565.
Train: 2018-08-02T13:35:26.922791: step 14330, loss 0.498163.
Test: 2018-08-02T13:35:27.375776: step 14330, loss 0.547378.
Train: 2018-08-02T13:35:27.532006: step 14331, loss 0.547257.
Train: 2018-08-02T13:35:27.688236: step 14332, loss 0.559504.
Train: 2018-08-02T13:35:27.844446: step 14333, loss 0.459199.
Train: 2018-08-02T13:35:28.000630: step 14334, loss 0.473751.
Train: 2018-08-02T13:35:28.156873: step 14335, loss 0.487028.
Train: 2018-08-02T13:35:28.313083: step 14336, loss 0.466647.
Train: 2018-08-02T13:35:28.469270: step 14337, loss 0.531604.
Train: 2018-08-02T13:35:28.625514: step 14338, loss 0.493994.
Train: 2018-08-02T13:35:28.781727: step 14339, loss 0.495687.
Train: 2018-08-02T13:35:28.922320: step 14340, loss 0.512055.
Test: 2018-08-02T13:35:29.390929: step 14340, loss 0.550917.
Train: 2018-08-02T13:35:29.531551: step 14341, loss 0.476571.
Train: 2018-08-02T13:35:29.687734: step 14342, loss 0.558778.
Train: 2018-08-02T13:35:29.843957: step 14343, loss 0.492813.
Train: 2018-08-02T13:35:30.000162: step 14344, loss 0.537479.
Train: 2018-08-02T13:35:30.125158: step 14345, loss 0.688307.
Train: 2018-08-02T13:35:30.281346: step 14346, loss 0.544174.
Train: 2018-08-02T13:35:30.437588: step 14347, loss 0.514462.
Train: 2018-08-02T13:35:30.593773: step 14348, loss 0.556933.
Train: 2018-08-02T13:35:30.749987: step 14349, loss 0.526428.
Train: 2018-08-02T13:35:30.906225: step 14350, loss 0.547465.
Test: 2018-08-02T13:35:31.374842: step 14350, loss 0.566696.
Train: 2018-08-02T13:35:31.531084: step 14351, loss 0.466842.
Train: 2018-08-02T13:35:31.671675: step 14352, loss 0.509494.
Train: 2018-08-02T13:35:31.843506: step 14353, loss 0.557301.
Train: 2018-08-02T13:35:31.999724: step 14354, loss 0.49784.
Train: 2018-08-02T13:35:32.140316: step 14355, loss 0.673014.
Train: 2018-08-02T13:35:32.296529: step 14356, loss 0.530082.
Train: 2018-08-02T13:35:32.452745: step 14357, loss 0.487226.
Train: 2018-08-02T13:35:32.624599: step 14358, loss 0.537738.
Train: 2018-08-02T13:35:32.780760: step 14359, loss 0.461751.
Train: 2018-08-02T13:35:32.937008: step 14360, loss 0.512993.
Test: 2018-08-02T13:35:33.405614: step 14360, loss 0.549241.
Train: 2018-08-02T13:35:33.561856: step 14361, loss 0.575994.
Train: 2018-08-02T13:35:33.718073: step 14362, loss 0.486208.
Train: 2018-08-02T13:35:33.874280: step 14363, loss 0.5558.
Train: 2018-08-02T13:35:34.030471: step 14364, loss 0.43555.
Train: 2018-08-02T13:35:34.186681: step 14365, loss 0.510388.
Train: 2018-08-02T13:35:34.342922: step 14366, loss 0.553276.
Train: 2018-08-02T13:35:34.514755: step 14367, loss 0.555674.
Train: 2018-08-02T13:35:34.670976: step 14368, loss 0.548329.
Train: 2018-08-02T13:35:34.827188: step 14369, loss 0.586145.
Train: 2018-08-02T13:35:34.983400: step 14370, loss 0.539886.
Test: 2018-08-02T13:35:35.436389: step 14370, loss 0.552965.
Train: 2018-08-02T13:35:35.592601: step 14371, loss 0.509947.
Train: 2018-08-02T13:35:35.748846: step 14372, loss 0.542913.
Train: 2018-08-02T13:35:35.889437: step 14373, loss 0.553517.
Train: 2018-08-02T13:35:36.045652: step 14374, loss 0.536613.
Train: 2018-08-02T13:35:36.201849: step 14375, loss 0.473853.
Train: 2018-08-02T13:35:36.373670: step 14376, loss 0.596892.
Train: 2018-08-02T13:35:36.529908: step 14377, loss 0.476996.
Train: 2018-08-02T13:35:36.686125: step 14378, loss 0.555078.
Train: 2018-08-02T13:35:36.842340: step 14379, loss 0.497493.
Train: 2018-08-02T13:35:36.998553: step 14380, loss 0.529444.
Test: 2018-08-02T13:35:37.451542: step 14380, loss 0.547549.
Train: 2018-08-02T13:35:37.623377: step 14381, loss 0.481499.
Train: 2018-08-02T13:35:37.779616: step 14382, loss 0.562011.
Train: 2018-08-02T13:35:37.935804: step 14383, loss 0.479967.
Train: 2018-08-02T13:35:38.076394: step 14384, loss 0.481739.
Train: 2018-08-02T13:35:38.232639: step 14385, loss 0.592502.
Train: 2018-08-02T13:35:38.388823: step 14386, loss 0.517174.
Train: 2018-08-02T13:35:38.545066: step 14387, loss 0.448876.
Train: 2018-08-02T13:35:38.701281: step 14388, loss 0.532191.
Train: 2018-08-02T13:35:38.857492: step 14389, loss 0.619083.
Train: 2018-08-02T13:35:38.998084: step 14390, loss 0.512919.
Test: 2018-08-02T13:35:39.466695: step 14390, loss 0.552441.
Train: 2018-08-02T13:35:39.622909: step 14391, loss 0.527573.
Train: 2018-08-02T13:35:39.779152: step 14392, loss 0.518225.
Train: 2018-08-02T13:35:39.935366: step 14393, loss 0.486505.
Train: 2018-08-02T13:35:40.091579: step 14394, loss 0.452742.
Train: 2018-08-02T13:35:40.247761: step 14395, loss 0.486377.
Train: 2018-08-02T13:35:40.404001: step 14396, loss 0.548603.
Train: 2018-08-02T13:35:40.560191: step 14397, loss 0.504369.
Train: 2018-08-02T13:35:40.716402: step 14398, loss 0.507934.
Train: 2018-08-02T13:35:40.872645: step 14399, loss 0.499256.
Train: 2018-08-02T13:35:41.028859: step 14400, loss 0.586886.
Test: 2018-08-02T13:35:41.481848: step 14400, loss 0.547004.
Train: 2018-08-02T13:35:42.091080: step 14401, loss 0.444212.
Train: 2018-08-02T13:35:42.247294: step 14402, loss 0.451851.
Train: 2018-08-02T13:35:42.403537: step 14403, loss 0.478462.
Train: 2018-08-02T13:35:42.559721: step 14404, loss 0.457786.
Train: 2018-08-02T13:35:42.715934: step 14405, loss 0.525447.
Train: 2018-08-02T13:35:42.872148: step 14406, loss 0.526584.
Train: 2018-08-02T13:35:43.028361: step 14407, loss 0.535804.
Train: 2018-08-02T13:35:43.184605: step 14408, loss 0.513534.
Train: 2018-08-02T13:35:43.340818: step 14409, loss 0.508875.
Train: 2018-08-02T13:35:43.497032: step 14410, loss 0.555363.
Test: 2018-08-02T13:35:43.965653: step 14410, loss 0.555457.
Train: 2018-08-02T13:35:44.106263: step 14411, loss 0.440669.
Train: 2018-08-02T13:35:44.262448: step 14412, loss 0.459088.
Train: 2018-08-02T13:35:44.418661: step 14413, loss 0.521914.
Train: 2018-08-02T13:35:44.574907: step 14414, loss 0.481772.
Train: 2018-08-02T13:35:44.731087: step 14415, loss 0.597531.
Train: 2018-08-02T13:35:44.887331: step 14416, loss 0.530563.
Train: 2018-08-02T13:35:45.043547: step 14417, loss 0.433373.
Train: 2018-08-02T13:35:45.199752: step 14418, loss 0.512452.
Train: 2018-08-02T13:35:45.355941: step 14419, loss 0.527176.
Train: 2018-08-02T13:35:45.512184: step 14420, loss 0.459611.
Test: 2018-08-02T13:35:45.965199: step 14420, loss 0.549128.
Train: 2018-08-02T13:35:46.121416: step 14421, loss 0.506349.
Train: 2018-08-02T13:35:46.277633: step 14422, loss 0.517683.
Train: 2018-08-02T13:35:46.418223: step 14423, loss 0.49483.
Train: 2018-08-02T13:35:46.574438: step 14424, loss 0.420741.
Train: 2018-08-02T13:35:46.730648: step 14425, loss 0.469582.
Train: 2018-08-02T13:35:46.886857: step 14426, loss 0.535676.
Train: 2018-08-02T13:35:47.043077: step 14427, loss 0.528766.
Train: 2018-08-02T13:35:47.199260: step 14428, loss 0.593749.
Train: 2018-08-02T13:35:47.355503: step 14429, loss 0.474604.
Train: 2018-08-02T13:35:47.511687: step 14430, loss 0.41355.
Test: 2018-08-02T13:35:47.980326: step 14430, loss 0.550638.
Train: 2018-08-02T13:35:48.120949: step 14431, loss 0.537133.
Train: 2018-08-02T13:35:48.277162: step 14432, loss 0.439919.
Train: 2018-08-02T13:35:48.433375: step 14433, loss 0.509604.
Train: 2018-08-02T13:35:48.589560: step 14434, loss 0.497924.
Train: 2018-08-02T13:35:48.745803: step 14435, loss 0.528317.
Train: 2018-08-02T13:35:48.902011: step 14436, loss 0.470913.
Train: 2018-08-02T13:35:49.058229: step 14437, loss 0.546739.
Train: 2018-08-02T13:35:49.214412: step 14438, loss 0.578627.
Train: 2018-08-02T13:35:49.370656: step 14439, loss 0.640452.
Train: 2018-08-02T13:35:49.526840: step 14440, loss 0.523437.
Test: 2018-08-02T13:35:49.979858: step 14440, loss 0.558613.
Train: 2018-08-02T13:35:50.136072: step 14441, loss 0.500415.
Train: 2018-08-02T13:35:50.292285: step 14442, loss 0.532522.
Train: 2018-08-02T13:35:50.448537: step 14443, loss 0.555264.
Train: 2018-08-02T13:35:50.604713: step 14444, loss 0.541085.
Train: 2018-08-02T13:35:50.760951: step 14445, loss 0.469707.
Train: 2018-08-02T13:35:50.917170: step 14446, loss 0.500255.
Train: 2018-08-02T13:35:51.073384: step 14447, loss 0.520169.
Train: 2018-08-02T13:35:51.229565: step 14448, loss 0.518298.
Train: 2018-08-02T13:35:51.385809: step 14449, loss 0.530962.
Train: 2018-08-02T13:35:51.541991: step 14450, loss 0.575138.
Test: 2018-08-02T13:35:51.995012: step 14450, loss 0.548604.
Train: 2018-08-02T13:35:52.151225: step 14451, loss 0.514109.
Train: 2018-08-02T13:35:52.307439: step 14452, loss 0.565071.
Train: 2018-08-02T13:35:52.463677: step 14453, loss 0.486604.
Train: 2018-08-02T13:35:52.619896: step 14454, loss 0.464024.
Train: 2018-08-02T13:35:52.776114: step 14455, loss 0.519323.
Train: 2018-08-02T13:35:52.932317: step 14456, loss 0.432858.
Train: 2018-08-02T13:35:53.088529: step 14457, loss 0.548344.
Train: 2018-08-02T13:35:53.244717: step 14458, loss 0.601266.
Train: 2018-08-02T13:35:53.385341: step 14459, loss 0.548221.
Train: 2018-08-02T13:35:53.541554: step 14460, loss 0.490597.
Test: 2018-08-02T13:35:53.994543: step 14460, loss 0.550987.
Train: 2018-08-02T13:35:54.150756: step 14461, loss 0.472677.
Train: 2018-08-02T13:35:54.307000: step 14462, loss 0.448005.
Train: 2018-08-02T13:35:54.463213: step 14463, loss 0.516426.
Train: 2018-08-02T13:35:54.619396: step 14464, loss 0.452435.
Train: 2018-08-02T13:35:54.760018: step 14465, loss 0.530183.
Train: 2018-08-02T13:35:54.916232: step 14466, loss 0.43178.
Train: 2018-08-02T13:35:55.072448: step 14467, loss 0.548508.
Train: 2018-08-02T13:35:55.228655: step 14468, loss 0.573285.
Train: 2018-08-02T13:35:55.384867: step 14469, loss 0.663081.
Train: 2018-08-02T13:35:55.525464: step 14470, loss 0.462587.
Test: 2018-08-02T13:35:55.978454: step 14470, loss 0.550851.
Train: 2018-08-02T13:35:56.134697: step 14471, loss 0.501591.
Train: 2018-08-02T13:35:56.290904: step 14472, loss 0.577053.
Train: 2018-08-02T13:35:56.447125: step 14473, loss 0.579744.
Train: 2018-08-02T13:35:56.603337: step 14474, loss 0.517707.
Train: 2018-08-02T13:35:56.759551: step 14475, loss 0.584505.
Train: 2018-08-02T13:35:56.915735: step 14476, loss 0.504593.
Train: 2018-08-02T13:35:57.071947: step 14477, loss 0.548247.
Train: 2018-08-02T13:35:57.228160: step 14478, loss 0.558179.
Train: 2018-08-02T13:35:57.368752: step 14479, loss 0.535348.
Train: 2018-08-02T13:35:57.524996: step 14480, loss 0.531093.
Test: 2018-08-02T13:35:57.977986: step 14480, loss 0.553422.
Train: 2018-08-02T13:35:58.134224: step 14481, loss 0.495362.
Train: 2018-08-02T13:35:58.290442: step 14482, loss 0.488941.
Train: 2018-08-02T13:35:58.446658: step 14483, loss 0.550657.
Train: 2018-08-02T13:35:58.602870: step 14484, loss 0.548841.
Train: 2018-08-02T13:35:58.759082: step 14485, loss 0.524726.
Train: 2018-08-02T13:35:58.915265: step 14486, loss 0.438561.
Train: 2018-08-02T13:35:59.071511: step 14487, loss 0.461096.
Train: 2018-08-02T13:35:59.212102: step 14488, loss 0.425818.
Train: 2018-08-02T13:35:59.368285: step 14489, loss 0.577219.
Train: 2018-08-02T13:35:59.524529: step 14490, loss 0.536507.
Test: 2018-08-02T13:35:59.977549: step 14490, loss 0.548179.
Train: 2018-08-02T13:36:00.133756: step 14491, loss 0.549683.
Train: 2018-08-02T13:36:00.289975: step 14492, loss 0.513586.
Train: 2018-08-02T13:36:00.446190: step 14493, loss 0.465781.
Train: 2018-08-02T13:36:00.602372: step 14494, loss 0.552455.
Train: 2018-08-02T13:36:00.758585: step 14495, loss 0.514804.
Train: 2018-08-02T13:36:00.867963: step 14496, loss 0.578595.
Train: 2018-08-02T13:36:01.024177: step 14497, loss 0.472492.
Train: 2018-08-02T13:36:01.180390: step 14498, loss 0.497884.
Train: 2018-08-02T13:36:01.336574: step 14499, loss 0.457146.
Train: 2018-08-02T13:36:01.492817: step 14500, loss 0.506241.
Test: 2018-08-02T13:36:01.961427: step 14500, loss 0.546452.
Train: 2018-08-02T13:36:02.570659: step 14501, loss 0.524132.
Train: 2018-08-02T13:36:02.726874: step 14502, loss 0.487043.
Train: 2018-08-02T13:36:02.898709: step 14503, loss 0.56225.
Train: 2018-08-02T13:36:03.054927: step 14504, loss 0.549141.
Train: 2018-08-02T13:36:03.195545: step 14505, loss 0.506805.
Train: 2018-08-02T13:36:03.351757: step 14506, loss 0.481512.
Train: 2018-08-02T13:36:03.507964: step 14507, loss 0.515278.
Train: 2018-08-02T13:36:03.664184: step 14508, loss 0.433576.
Train: 2018-08-02T13:36:03.820399: step 14509, loss 0.500822.
Train: 2018-08-02T13:36:03.960993: step 14510, loss 0.526879.
Test: 2018-08-02T13:36:04.429601: step 14510, loss 0.548173.
Train: 2018-08-02T13:36:04.585842: step 14511, loss 0.485934.
Train: 2018-08-02T13:36:04.726435: step 14512, loss 0.400078.
Train: 2018-08-02T13:36:04.882648: step 14513, loss 0.552677.
Train: 2018-08-02T13:36:05.038861: step 14514, loss 0.449408.
Train: 2018-08-02T13:36:05.195077: step 14515, loss 0.500856.
Train: 2018-08-02T13:36:05.335637: step 14516, loss 0.450478.
Train: 2018-08-02T13:36:05.491880: step 14517, loss 0.520707.
Train: 2018-08-02T13:36:05.648094: step 14518, loss 0.521654.
Train: 2018-08-02T13:36:05.804303: step 14519, loss 0.568925.
Train: 2018-08-02T13:36:05.960522: step 14520, loss 0.551513.
Test: 2018-08-02T13:36:06.413543: step 14520, loss 0.554073.
Train: 2018-08-02T13:36:06.569751: step 14521, loss 0.480036.
Train: 2018-08-02T13:36:06.725936: step 14522, loss 0.57315.
Train: 2018-08-02T13:36:06.882149: step 14523, loss 0.502557.
Train: 2018-08-02T13:36:07.038393: step 14524, loss 0.555728.
Train: 2018-08-02T13:36:07.194609: step 14525, loss 0.539508.
Train: 2018-08-02T13:36:07.350789: step 14526, loss 0.476778.
Train: 2018-08-02T13:36:07.507004: step 14527, loss 0.504219.
Train: 2018-08-02T13:36:07.663249: step 14528, loss 0.512281.
Train: 2018-08-02T13:36:07.819463: step 14529, loss 0.525199.
Train: 2018-08-02T13:36:07.975674: step 14530, loss 0.463183.
Test: 2018-08-02T13:36:08.428691: step 14530, loss 0.548172.
Train: 2018-08-02T13:36:08.584907: step 14531, loss 0.521242.
Train: 2018-08-02T13:36:08.741120: step 14532, loss 0.528141.
Train: 2018-08-02T13:36:08.897333: step 14533, loss 0.576479.
Train: 2018-08-02T13:36:09.053545: step 14534, loss 0.471254.
Train: 2018-08-02T13:36:09.209760: step 14535, loss 0.567131.
Train: 2018-08-02T13:36:09.381595: step 14536, loss 0.499133.
Train: 2018-08-02T13:36:09.537778: step 14537, loss 0.52988.
Train: 2018-08-02T13:36:09.678400: step 14538, loss 0.443691.
Train: 2018-08-02T13:36:09.834614: step 14539, loss 0.494818.
Train: 2018-08-02T13:36:09.990827: step 14540, loss 0.548861.
Test: 2018-08-02T13:36:10.443818: step 14540, loss 0.547483.
Train: 2018-08-02T13:36:10.615681: step 14541, loss 0.591961.
Train: 2018-08-02T13:36:10.771864: step 14542, loss 0.554027.
Train: 2018-08-02T13:36:10.928108: step 14543, loss 0.459278.
Train: 2018-08-02T13:36:11.084316: step 14544, loss 0.54139.
Train: 2018-08-02T13:36:11.240504: step 14545, loss 0.493457.
Train: 2018-08-02T13:36:11.396749: step 14546, loss 0.566934.
Train: 2018-08-02T13:36:11.552931: step 14547, loss 0.549913.
Train: 2018-08-02T13:36:11.709145: step 14548, loss 0.593731.
Train: 2018-08-02T13:36:11.865357: step 14549, loss 0.502721.
Train: 2018-08-02T13:36:12.021572: step 14550, loss 0.550516.
Test: 2018-08-02T13:36:12.474616: step 14550, loss 0.551426.
Train: 2018-08-02T13:36:12.630804: step 14551, loss 0.578499.
Train: 2018-08-02T13:36:12.771395: step 14552, loss 0.552271.
Train: 2018-08-02T13:36:12.927639: step 14553, loss 0.549254.
Train: 2018-08-02T13:36:13.083855: step 14554, loss 0.475424.
Train: 2018-08-02T13:36:13.240060: step 14555, loss 0.4836.
Train: 2018-08-02T13:36:13.396279: step 14556, loss 0.477582.
Train: 2018-08-02T13:36:13.552493: step 14557, loss 0.572132.
Train: 2018-08-02T13:36:13.708706: step 14558, loss 0.49533.
Train: 2018-08-02T13:36:13.864891: step 14559, loss 0.505919.
Train: 2018-08-02T13:36:14.021104: step 14560, loss 0.552632.
Test: 2018-08-02T13:36:14.474122: step 14560, loss 0.547541.
Train: 2018-08-02T13:36:14.630335: step 14561, loss 0.482216.
Train: 2018-08-02T13:36:14.786602: step 14562, loss 0.526058.
Train: 2018-08-02T13:36:14.942790: step 14563, loss 0.529137.
Train: 2018-08-02T13:36:15.098976: step 14564, loss 0.503926.
Train: 2018-08-02T13:36:15.255219: step 14565, loss 0.426677.
Train: 2018-08-02T13:36:15.395811: step 14566, loss 0.545735.
Train: 2018-08-02T13:36:15.552025: step 14567, loss 0.593641.
Train: 2018-08-02T13:36:15.708210: step 14568, loss 0.504134.
Train: 2018-08-02T13:36:15.864452: step 14569, loss 0.424097.
Train: 2018-08-02T13:36:16.020665: step 14570, loss 0.65448.
Test: 2018-08-02T13:36:16.473655: step 14570, loss 0.548741.
Train: 2018-08-02T13:36:16.629898: step 14571, loss 0.461585.
Train: 2018-08-02T13:36:16.786114: step 14572, loss 0.532382.
Train: 2018-08-02T13:36:16.942295: step 14573, loss 0.516019.
Train: 2018-08-02T13:36:17.098552: step 14574, loss 0.508601.
Train: 2018-08-02T13:36:17.254751: step 14575, loss 0.494489.
Train: 2018-08-02T13:36:17.426584: step 14576, loss 0.496708.
Train: 2018-08-02T13:36:17.582800: step 14577, loss 0.611806.
Train: 2018-08-02T13:36:17.739013: step 14578, loss 0.507123.
Train: 2018-08-02T13:36:17.895227: step 14579, loss 0.535251.
Train: 2018-08-02T13:36:18.051409: step 14580, loss 0.472043.
Test: 2018-08-02T13:36:18.520050: step 14580, loss 0.548819.
Train: 2018-08-02T13:36:18.676263: step 14581, loss 0.482951.
Train: 2018-08-02T13:36:18.832478: step 14582, loss 0.526296.
Train: 2018-08-02T13:36:18.988720: step 14583, loss 0.479478.
Train: 2018-08-02T13:36:19.144903: step 14584, loss 0.447193.
Train: 2018-08-02T13:36:19.301118: step 14585, loss 0.625331.
Train: 2018-08-02T13:36:19.441741: step 14586, loss 0.493912.
Train: 2018-08-02T13:36:19.597952: step 14587, loss 0.524279.
Train: 2018-08-02T13:36:19.754165: step 14588, loss 0.454027.
Train: 2018-08-02T13:36:19.910349: step 14589, loss 0.509389.
Train: 2018-08-02T13:36:20.066595: step 14590, loss 0.588128.
Test: 2018-08-02T13:36:20.535203: step 14590, loss 0.549832.
Train: 2018-08-02T13:36:20.691449: step 14591, loss 0.438793.
Train: 2018-08-02T13:36:20.847629: step 14592, loss 0.525888.
Train: 2018-08-02T13:36:21.003858: step 14593, loss 0.561208.
Train: 2018-08-02T13:36:21.160058: step 14594, loss 0.498852.
Train: 2018-08-02T13:36:21.316302: step 14595, loss 0.601697.
Train: 2018-08-02T13:36:21.472509: step 14596, loss 0.524159.
Train: 2018-08-02T13:36:21.644319: step 14597, loss 0.626748.
Train: 2018-08-02T13:36:21.800562: step 14598, loss 0.506183.
Train: 2018-08-02T13:36:21.956776: step 14599, loss 0.53514.
Train: 2018-08-02T13:36:22.112988: step 14600, loss 0.475232.
Test: 2018-08-02T13:36:22.566002: step 14600, loss 0.556419.
Train: 2018-08-02T13:36:23.190863: step 14601, loss 0.501731.
Train: 2018-08-02T13:36:23.347045: step 14602, loss 0.485013.
Train: 2018-08-02T13:36:23.503259: step 14603, loss 0.522185.
Train: 2018-08-02T13:36:23.659502: step 14604, loss 0.456337.
Train: 2018-08-02T13:36:23.815718: step 14605, loss 0.504492.
Train: 2018-08-02T13:36:23.987551: step 14606, loss 0.523609.
Train: 2018-08-02T13:36:24.143764: step 14607, loss 0.541766.
Train: 2018-08-02T13:36:24.299948: step 14608, loss 0.457644.
Train: 2018-08-02T13:36:24.456160: step 14609, loss 0.52756.
Train: 2018-08-02T13:36:24.612374: step 14610, loss 0.490673.
Test: 2018-08-02T13:36:25.065417: step 14610, loss 0.549147.
Train: 2018-08-02T13:36:25.221635: step 14611, loss 0.508029.
Train: 2018-08-02T13:36:25.377850: step 14612, loss 0.540078.
Train: 2018-08-02T13:36:25.534062: step 14613, loss 0.517108.
Train: 2018-08-02T13:36:25.690246: step 14614, loss 0.567912.
Train: 2018-08-02T13:36:25.830863: step 14615, loss 0.498664.
Train: 2018-08-02T13:36:25.987081: step 14616, loss 0.527455.
Train: 2018-08-02T13:36:26.143295: step 14617, loss 0.49443.
Train: 2018-08-02T13:36:26.299477: step 14618, loss 0.446734.
Train: 2018-08-02T13:36:26.440102: step 14619, loss 0.453833.
Train: 2018-08-02T13:36:26.596308: step 14620, loss 0.457268.
Test: 2018-08-02T13:36:27.064925: step 14620, loss 0.547738.
Train: 2018-08-02T13:36:27.221168: step 14621, loss 0.518332.
Train: 2018-08-02T13:36:27.377375: step 14622, loss 0.487287.
Train: 2018-08-02T13:36:27.533594: step 14623, loss 0.489019.
Train: 2018-08-02T13:36:27.689801: step 14624, loss 0.491645.
Train: 2018-08-02T13:36:27.846022: step 14625, loss 0.481042.
Train: 2018-08-02T13:36:27.986613: step 14626, loss 0.622817.
Train: 2018-08-02T13:36:28.142827: step 14627, loss 0.494786.
Train: 2018-08-02T13:36:28.299039: step 14628, loss 0.566641.
Train: 2018-08-02T13:36:28.455223: step 14629, loss 0.499061.
Train: 2018-08-02T13:36:28.611466: step 14630, loss 0.576596.
Test: 2018-08-02T13:36:29.064456: step 14630, loss 0.559527.
Train: 2018-08-02T13:36:29.220699: step 14631, loss 0.484084.
Train: 2018-08-02T13:36:29.376913: step 14632, loss 0.546785.
Train: 2018-08-02T13:36:29.533121: step 14633, loss 0.529025.
Train: 2018-08-02T13:36:29.689322: step 14634, loss 0.548499.
Train: 2018-08-02T13:36:29.845525: step 14635, loss 0.49322.
Train: 2018-08-02T13:36:30.001767: step 14636, loss 0.537546.
Train: 2018-08-02T13:36:30.173606: step 14637, loss 0.547885.
Train: 2018-08-02T13:36:30.329810: step 14638, loss 0.477423.
Train: 2018-08-02T13:36:30.486022: step 14639, loss 0.463422.
Train: 2018-08-02T13:36:30.642236: step 14640, loss 0.462843.
Test: 2018-08-02T13:36:31.095229: step 14640, loss 0.548213.
Train: 2018-08-02T13:36:31.251443: step 14641, loss 0.577733.
Train: 2018-08-02T13:36:31.407657: step 14642, loss 0.446417.
Train: 2018-08-02T13:36:31.563902: step 14643, loss 0.61091.
Train: 2018-08-02T13:36:31.704463: step 14644, loss 0.566557.
Train: 2018-08-02T13:36:31.860675: step 14645, loss 0.613156.
Train: 2018-08-02T13:36:32.016888: step 14646, loss 0.576093.
Train: 2018-08-02T13:36:32.126270: step 14647, loss 0.574338.
Train: 2018-08-02T13:36:32.282483: step 14648, loss 0.554952.
Train: 2018-08-02T13:36:32.454287: step 14649, loss 0.563991.
Train: 2018-08-02T13:36:32.594909: step 14650, loss 0.506395.
Test: 2018-08-02T13:36:33.047899: step 14650, loss 0.568102.
Train: 2018-08-02T13:36:33.219732: step 14651, loss 0.498421.
Train: 2018-08-02T13:36:33.375977: step 14652, loss 0.563794.
Train: 2018-08-02T13:36:33.532159: step 14653, loss 0.475752.
Train: 2018-08-02T13:36:33.688405: step 14654, loss 0.549684.
Train: 2018-08-02T13:36:33.844586: step 14655, loss 0.514555.
Train: 2018-08-02T13:36:34.000831: step 14656, loss 0.54.
Train: 2018-08-02T13:36:34.157012: step 14657, loss 0.492448.
Train: 2018-08-02T13:36:34.313258: step 14658, loss 0.591193.
Train: 2018-08-02T13:36:34.469469: step 14659, loss 0.456744.
Train: 2018-08-02T13:36:34.625679: step 14660, loss 0.507854.
Test: 2018-08-02T13:36:35.078697: step 14660, loss 0.547374.
Train: 2018-08-02T13:36:35.234916: step 14661, loss 0.544333.
Train: 2018-08-02T13:36:35.406750: step 14662, loss 0.462753.
Train: 2018-08-02T13:36:35.562967: step 14663, loss 0.517601.
Train: 2018-08-02T13:36:35.719173: step 14664, loss 0.612895.
Train: 2018-08-02T13:36:35.875391: step 14665, loss 0.508978.
Train: 2018-08-02T13:36:36.031604: step 14666, loss 0.469119.
Train: 2018-08-02T13:36:36.187787: step 14667, loss 0.463747.
Train: 2018-08-02T13:36:36.359630: step 14668, loss 0.463842.
Train: 2018-08-02T13:36:36.515835: step 14669, loss 0.55288.
Train: 2018-08-02T13:36:36.672079: step 14670, loss 0.555311.
Test: 2018-08-02T13:36:37.125069: step 14670, loss 0.547163.
Train: 2018-08-02T13:36:37.281307: step 14671, loss 0.435055.
Train: 2018-08-02T13:36:37.437520: step 14672, loss 0.60506.
Train: 2018-08-02T13:36:37.593709: step 14673, loss 0.587871.
Train: 2018-08-02T13:36:37.749952: step 14674, loss 0.516296.
Train: 2018-08-02T13:36:37.906165: step 14675, loss 0.437449.
Train: 2018-08-02T13:36:38.062380: step 14676, loss 0.491421.
Train: 2018-08-02T13:36:38.218591: step 14677, loss 0.606351.
Train: 2018-08-02T13:36:38.374776: step 14678, loss 0.54869.
Train: 2018-08-02T13:36:38.515397: step 14679, loss 0.490792.
Train: 2018-08-02T13:36:38.671611: step 14680, loss 0.481303.
Test: 2018-08-02T13:36:39.140221: step 14680, loss 0.549989.
Train: 2018-08-02T13:36:39.296436: step 14681, loss 0.506576.
Train: 2018-08-02T13:36:39.452679: step 14682, loss 0.60826.
Train: 2018-08-02T13:36:39.608887: step 14683, loss 0.549344.
Train: 2018-08-02T13:36:39.765103: step 14684, loss 0.531533.
Train: 2018-08-02T13:36:39.921316: step 14685, loss 0.54906.
Train: 2018-08-02T13:36:40.077501: step 14686, loss 0.496385.
Train: 2018-08-02T13:36:40.233715: step 14687, loss 0.459883.
Train: 2018-08-02T13:36:40.405583: step 14688, loss 0.535206.
Train: 2018-08-02T13:36:40.561796: step 14689, loss 0.520334.
Train: 2018-08-02T13:36:40.718007: step 14690, loss 0.586141.
Test: 2018-08-02T13:36:41.171020: step 14690, loss 0.548239.
Train: 2018-08-02T13:36:41.327239: step 14691, loss 0.545717.
Train: 2018-08-02T13:36:41.483453: step 14692, loss 0.63329.
Train: 2018-08-02T13:36:41.639690: step 14693, loss 0.487224.
Train: 2018-08-02T13:36:41.795881: step 14694, loss 0.532874.
Train: 2018-08-02T13:36:41.952093: step 14695, loss 0.493395.
Train: 2018-08-02T13:36:42.108306: step 14696, loss 0.498311.
Train: 2018-08-02T13:36:42.248869: step 14697, loss 0.541888.
Train: 2018-08-02T13:36:42.405111: step 14698, loss 0.45156.
Train: 2018-08-02T13:36:42.561295: step 14699, loss 0.563803.
Train: 2018-08-02T13:36:42.717509: step 14700, loss 0.568055.
Test: 2018-08-02T13:36:43.186182: step 14700, loss 0.548195.
Train: 2018-08-02T13:36:43.873488: step 14701, loss 0.606556.
Train: 2018-08-02T13:36:44.029727: step 14702, loss 0.474905.
Train: 2018-08-02T13:36:44.185947: step 14703, loss 0.552275.
Train: 2018-08-02T13:36:44.342154: step 14704, loss 0.455619.
Train: 2018-08-02T13:36:44.498371: step 14705, loss 0.558928.
Train: 2018-08-02T13:36:44.654556: step 14706, loss 0.482402.
Train: 2018-08-02T13:36:44.810769: step 14707, loss 0.514286.
Train: 2018-08-02T13:36:44.966982: step 14708, loss 0.453262.
Train: 2018-08-02T13:36:45.123194: step 14709, loss 0.418822.
Train: 2018-08-02T13:36:45.279438: step 14710, loss 0.549118.
Test: 2018-08-02T13:36:45.732459: step 14710, loss 0.549822.
Train: 2018-08-02T13:36:45.888642: step 14711, loss 0.515889.
Train: 2018-08-02T13:36:46.044855: step 14712, loss 0.570285.
Train: 2018-08-02T13:36:46.201094: step 14713, loss 0.49407.
Train: 2018-08-02T13:36:46.357283: step 14714, loss 0.565542.
Train: 2018-08-02T13:36:46.513495: step 14715, loss 0.495371.
Train: 2018-08-02T13:36:46.685362: step 14716, loss 0.560675.
Train: 2018-08-02T13:36:46.841568: step 14717, loss 0.5203.
Train: 2018-08-02T13:36:46.997790: step 14718, loss 0.534418.
Train: 2018-08-02T13:36:47.153999: step 14719, loss 0.560216.
Train: 2018-08-02T13:36:47.310184: step 14720, loss 0.54458.
Test: 2018-08-02T13:36:47.778824: step 14720, loss 0.558392.
Train: 2018-08-02T13:36:47.935069: step 14721, loss 0.49766.
Train: 2018-08-02T13:36:48.075659: step 14722, loss 0.521938.
Train: 2018-08-02T13:36:48.231872: step 14723, loss 0.558756.
Train: 2018-08-02T13:36:48.403676: step 14724, loss 0.532129.
Train: 2018-08-02T13:36:48.559918: step 14725, loss 0.516586.
Train: 2018-08-02T13:36:48.716105: step 14726, loss 0.548222.
Train: 2018-08-02T13:36:48.872319: step 14727, loss 0.512724.
Train: 2018-08-02T13:36:49.028564: step 14728, loss 0.436002.
Train: 2018-08-02T13:36:49.184769: step 14729, loss 0.515714.
Train: 2018-08-02T13:36:49.340965: step 14730, loss 0.453257.
Test: 2018-08-02T13:36:49.809598: step 14730, loss 0.547312.
Train: 2018-08-02T13:36:49.965837: step 14731, loss 0.583256.
Train: 2018-08-02T13:36:50.122024: step 14732, loss 0.435139.
Train: 2018-08-02T13:36:50.278267: step 14733, loss 0.545227.
Train: 2018-08-02T13:36:50.434451: step 14734, loss 0.49414.
Train: 2018-08-02T13:36:50.575042: step 14735, loss 0.494494.
Train: 2018-08-02T13:36:50.731287: step 14736, loss 0.489087.
Train: 2018-08-02T13:36:50.887500: step 14737, loss 0.469945.
Train: 2018-08-02T13:36:51.043716: step 14738, loss 0.4995.
Train: 2018-08-02T13:36:51.199927: step 14739, loss 0.57224.
Train: 2018-08-02T13:36:51.356117: step 14740, loss 0.480373.
Test: 2018-08-02T13:36:51.809153: step 14740, loss 0.548713.
Train: 2018-08-02T13:36:51.965372: step 14741, loss 0.610138.
Train: 2018-08-02T13:36:52.121585: step 14742, loss 0.514159.
Train: 2018-08-02T13:36:52.277772: step 14743, loss 0.427357.
Train: 2018-08-02T13:36:52.434015: step 14744, loss 0.572032.
Train: 2018-08-02T13:36:52.590226: step 14745, loss 0.487624.
Train: 2018-08-02T13:36:52.746440: step 14746, loss 0.525716.
Train: 2018-08-02T13:36:52.902653: step 14747, loss 0.540455.
Train: 2018-08-02T13:36:53.058838: step 14748, loss 0.595965.
Train: 2018-08-02T13:36:53.215065: step 14749, loss 0.475866.
Train: 2018-08-02T13:36:53.371294: step 14750, loss 0.486978.
Test: 2018-08-02T13:36:53.824282: step 14750, loss 0.551582.
Train: 2018-08-02T13:36:53.980522: step 14751, loss 0.49411.
Train: 2018-08-02T13:36:54.136739: step 14752, loss 0.530197.
Train: 2018-08-02T13:36:54.292952: step 14753, loss 0.451772.
Train: 2018-08-02T13:36:54.449166: step 14754, loss 0.555576.
Train: 2018-08-02T13:36:54.605380: step 14755, loss 0.531145.
Train: 2018-08-02T13:36:54.761593: step 14756, loss 0.5156.
Train: 2018-08-02T13:36:54.917801: step 14757, loss 0.536895.
Train: 2018-08-02T13:36:55.074020: step 14758, loss 0.436537.
Train: 2018-08-02T13:36:55.230234: step 14759, loss 0.50217.
Train: 2018-08-02T13:36:55.386416: step 14760, loss 0.629535.
Test: 2018-08-02T13:36:55.839435: step 14760, loss 0.547954.
Train: 2018-08-02T13:36:56.073810: step 14761, loss 0.557622.
Train: 2018-08-02T13:36:56.230003: step 14762, loss 0.467842.
Train: 2018-08-02T13:36:56.386213: step 14763, loss 0.491913.
Train: 2018-08-02T13:36:56.558041: step 14764, loss 0.520747.
Train: 2018-08-02T13:36:56.714257: step 14765, loss 0.528649.
Train: 2018-08-02T13:36:56.870444: step 14766, loss 0.573384.
Train: 2018-08-02T13:36:57.026690: step 14767, loss 0.552199.
Train: 2018-08-02T13:36:57.182871: step 14768, loss 0.495927.
Train: 2018-08-02T13:36:57.339085: step 14769, loss 0.495799.
Train: 2018-08-02T13:36:57.495299: step 14770, loss 0.517295.
Test: 2018-08-02T13:36:57.948347: step 14770, loss 0.553356.
Train: 2018-08-02T13:36:58.104560: step 14771, loss 0.502883.
Train: 2018-08-02T13:36:58.260773: step 14772, loss 0.52065.
Train: 2018-08-02T13:36:58.416987: step 14773, loss 0.523068.
Train: 2018-08-02T13:36:58.573200: step 14774, loss 0.512192.
Train: 2018-08-02T13:36:58.729383: step 14775, loss 0.476573.
Train: 2018-08-02T13:36:58.885627: step 14776, loss 0.458241.
Train: 2018-08-02T13:36:59.041837: step 14777, loss 0.473738.
Train: 2018-08-02T13:36:59.198058: step 14778, loss 0.507855.
Train: 2018-08-02T13:36:59.354263: step 14779, loss 0.481017.
Train: 2018-08-02T13:36:59.510450: step 14780, loss 0.442834.
Test: 2018-08-02T13:36:59.963469: step 14780, loss 0.549638.
Train: 2018-08-02T13:37:00.119684: step 14781, loss 0.501533.
Train: 2018-08-02T13:37:00.275928: step 14782, loss 0.458971.
Train: 2018-08-02T13:37:00.432109: step 14783, loss 0.457719.
Train: 2018-08-02T13:37:00.603975: step 14784, loss 0.500775.
Train: 2018-08-02T13:37:00.760189: step 14785, loss 0.531257.
Train: 2018-08-02T13:37:00.916400: step 14786, loss 0.562118.
Train: 2018-08-02T13:37:01.072587: step 14787, loss 0.486848.
Train: 2018-08-02T13:37:01.228829: step 14788, loss 0.513571.
Train: 2018-08-02T13:37:01.385042: step 14789, loss 0.435436.
Train: 2018-08-02T13:37:01.541273: step 14790, loss 0.515249.
Test: 2018-08-02T13:37:01.994275: step 14790, loss 0.548734.
Train: 2018-08-02T13:37:02.150488: step 14791, loss 0.459709.
Train: 2018-08-02T13:37:02.306670: step 14792, loss 0.514977.
Train: 2018-08-02T13:37:02.447263: step 14793, loss 0.426372.
Train: 2018-08-02T13:37:02.603477: step 14794, loss 0.416746.
Train: 2018-08-02T13:37:02.775344: step 14795, loss 0.5034.
Train: 2018-08-02T13:37:02.931526: step 14796, loss 0.563882.
Train: 2018-08-02T13:37:03.087769: step 14797, loss 0.450591.
Train: 2018-08-02T13:37:03.197119: step 14798, loss 0.466185.
Train: 2018-08-02T13:37:03.353332: step 14799, loss 0.428318.
Train: 2018-08-02T13:37:03.509514: step 14800, loss 0.484147.
Test: 2018-08-02T13:37:03.978154: step 14800, loss 0.553132.
Train: 2018-08-02T13:37:04.587417: step 14801, loss 0.48788.
Train: 2018-08-02T13:37:04.743626: step 14802, loss 0.537643.
Train: 2018-08-02T13:37:04.915437: step 14803, loss 0.543983.
Train: 2018-08-02T13:37:05.071649: step 14804, loss 0.487546.
Train: 2018-08-02T13:37:05.227862: step 14805, loss 0.553293.
Train: 2018-08-02T13:37:05.384106: step 14806, loss 0.484127.
Train: 2018-08-02T13:37:05.524697: step 14807, loss 0.505454.
Train: 2018-08-02T13:37:05.696503: step 14808, loss 0.519944.
Train: 2018-08-02T13:37:05.852746: step 14809, loss 0.472265.
Train: 2018-08-02T13:37:06.008960: step 14810, loss 0.510881.
Test: 2018-08-02T13:37:06.461979: step 14810, loss 0.552187.
Train: 2018-08-02T13:37:06.618192: step 14811, loss 0.470151.
Train: 2018-08-02T13:37:06.774375: step 14812, loss 0.620077.
Train: 2018-08-02T13:37:06.930616: step 14813, loss 0.532268.
Train: 2018-08-02T13:37:07.086831: step 14814, loss 0.524685.
Train: 2018-08-02T13:37:07.243016: step 14815, loss 0.583401.
Train: 2018-08-02T13:37:07.399258: step 14816, loss 0.52283.
Train: 2018-08-02T13:37:07.555441: step 14817, loss 0.555214.
Train: 2018-08-02T13:37:07.696034: step 14818, loss 0.497312.
Train: 2018-08-02T13:37:07.852277: step 14819, loss 0.570145.
Train: 2018-08-02T13:37:08.008491: step 14820, loss 0.610466.
Test: 2018-08-02T13:37:08.461505: step 14820, loss 0.556483.
Train: 2018-08-02T13:37:08.617724: step 14821, loss 0.49324.
Train: 2018-08-02T13:37:08.773906: step 14822, loss 0.557051.
Train: 2018-08-02T13:37:08.945766: step 14823, loss 0.506352.
Train: 2018-08-02T13:37:09.101985: step 14824, loss 0.526515.
Train: 2018-08-02T13:37:09.258198: step 14825, loss 0.514346.
Train: 2018-08-02T13:37:09.414412: step 14826, loss 0.549365.
Train: 2018-08-02T13:37:09.570626: step 14827, loss 0.553673.
Train: 2018-08-02T13:37:09.726839: step 14828, loss 0.558066.
Train: 2018-08-02T13:37:09.883051: step 14829, loss 0.475664.
Train: 2018-08-02T13:37:10.039260: step 14830, loss 0.550002.
Test: 2018-08-02T13:37:10.492256: step 14830, loss 0.551388.
Train: 2018-08-02T13:37:10.648498: step 14831, loss 0.469096.
Train: 2018-08-02T13:37:10.804713: step 14832, loss 0.529758.
Train: 2018-08-02T13:37:10.960923: step 14833, loss 0.470012.
Train: 2018-08-02T13:37:11.117138: step 14834, loss 0.515506.
Train: 2018-08-02T13:37:11.273321: step 14835, loss 0.438692.
Train: 2018-08-02T13:37:11.429567: step 14836, loss 0.539846.
Train: 2018-08-02T13:37:11.585778: step 14837, loss 0.424849.
Train: 2018-08-02T13:37:11.726370: step 14838, loss 0.41525.
Train: 2018-08-02T13:37:11.882584: step 14839, loss 0.503839.
Train: 2018-08-02T13:37:12.038767: step 14840, loss 0.451346.
Test: 2018-08-02T13:37:12.507437: step 14840, loss 0.552978.
Train: 2018-08-02T13:37:12.648029: step 14841, loss 0.512317.
Train: 2018-08-02T13:37:12.804246: step 14842, loss 0.555292.
Train: 2018-08-02T13:37:12.976048: step 14843, loss 0.427506.
Train: 2018-08-02T13:37:13.132294: step 14844, loss 0.466758.
Train: 2018-08-02T13:37:13.288507: step 14845, loss 0.518118.
Train: 2018-08-02T13:37:13.444732: step 14846, loss 0.440787.
Train: 2018-08-02T13:37:13.600903: step 14847, loss 0.514789.
Train: 2018-08-02T13:37:13.757115: step 14848, loss 0.502472.
Train: 2018-08-02T13:37:13.897736: step 14849, loss 0.623954.
Train: 2018-08-02T13:37:14.053919: step 14850, loss 0.569247.
Test: 2018-08-02T13:37:14.506939: step 14850, loss 0.556935.
Train: 2018-08-02T13:37:14.663153: step 14851, loss 0.541787.
Train: 2018-08-02T13:37:14.819397: step 14852, loss 0.551601.
Train: 2018-08-02T13:37:14.975580: step 14853, loss 0.56163.
Train: 2018-08-02T13:37:15.131824: step 14854, loss 0.574698.
Train: 2018-08-02T13:37:15.288005: step 14855, loss 0.51119.
Train: 2018-08-02T13:37:15.444250: step 14856, loss 0.476304.
Train: 2018-08-02T13:37:15.584842: step 14857, loss 0.563293.
Train: 2018-08-02T13:37:15.741072: step 14858, loss 0.50272.
Train: 2018-08-02T13:37:15.897239: step 14859, loss 0.455541.
Train: 2018-08-02T13:37:16.053484: step 14860, loss 0.58383.
Test: 2018-08-02T13:37:16.522093: step 14860, loss 0.547235.
Train: 2018-08-02T13:37:16.678331: step 14861, loss 0.500829.
Train: 2018-08-02T13:37:16.834568: step 14862, loss 0.555322.
Train: 2018-08-02T13:37:16.990757: step 14863, loss 0.505076.
Train: 2018-08-02T13:37:17.146977: step 14864, loss 0.473218.
Train: 2018-08-02T13:37:17.303192: step 14865, loss 0.520893.
Train: 2018-08-02T13:37:17.459374: step 14866, loss 0.503617.
Train: 2018-08-02T13:37:17.631239: step 14867, loss 0.620075.
Train: 2018-08-02T13:37:17.787449: step 14868, loss 0.52085.
Train: 2018-08-02T13:37:17.943665: step 14869, loss 0.532379.
Train: 2018-08-02T13:37:18.084256: step 14870, loss 0.503481.
Test: 2018-08-02T13:37:18.552891: step 14870, loss 0.554543.
Train: 2018-08-02T13:37:18.693488: step 14871, loss 0.56128.
Train: 2018-08-02T13:37:18.849671: step 14872, loss 0.461145.
Train: 2018-08-02T13:37:19.005916: step 14873, loss 0.50828.
Train: 2018-08-02T13:37:19.162127: step 14874, loss 0.568841.
Train: 2018-08-02T13:37:19.318312: step 14875, loss 0.525837.
Train: 2018-08-02T13:37:19.474556: step 14876, loss 0.519627.
Train: 2018-08-02T13:37:19.630770: step 14877, loss 0.501656.
Train: 2018-08-02T13:37:19.786983: step 14878, loss 0.511044.
Train: 2018-08-02T13:37:19.943173: step 14879, loss 0.473198.
Train: 2018-08-02T13:37:20.099410: step 14880, loss 0.559821.
Test: 2018-08-02T13:37:20.568019: step 14880, loss 0.550531.
Train: 2018-08-02T13:37:20.724264: step 14881, loss 0.533279.
Train: 2018-08-02T13:37:20.880472: step 14882, loss 0.510889.
Train: 2018-08-02T13:37:21.052314: step 14883, loss 0.484152.
Train: 2018-08-02T13:37:21.208523: step 14884, loss 0.494299.
Train: 2018-08-02T13:37:21.364732: step 14885, loss 0.551185.
Train: 2018-08-02T13:37:21.505330: step 14886, loss 0.491883.
Train: 2018-08-02T13:37:21.661514: step 14887, loss 0.597875.
Train: 2018-08-02T13:37:21.817757: step 14888, loss 0.526398.
Train: 2018-08-02T13:37:21.973970: step 14889, loss 0.566912.
Train: 2018-08-02T13:37:22.114556: step 14890, loss 0.449994.
Test: 2018-08-02T13:37:22.583198: step 14890, loss 0.551591.
Train: 2018-08-02T13:37:22.723795: step 14891, loss 0.46378.
Train: 2018-08-02T13:37:22.880008: step 14892, loss 0.464842.
Train: 2018-08-02T13:37:23.036221: step 14893, loss 0.49369.
Train: 2018-08-02T13:37:23.192435: step 14894, loss 0.476084.
Train: 2018-08-02T13:37:23.333029: step 14895, loss 0.508646.
Train: 2018-08-02T13:37:23.489237: step 14896, loss 0.412187.
Train: 2018-08-02T13:37:23.645455: step 14897, loss 0.481256.
Train: 2018-08-02T13:37:23.801665: step 14898, loss 0.612041.
Train: 2018-08-02T13:37:23.957852: step 14899, loss 0.496201.
Train: 2018-08-02T13:37:24.114090: step 14900, loss 0.581265.
Test: 2018-08-02T13:37:24.582714: step 14900, loss 0.55653.
Train: 2018-08-02T13:37:25.207558: step 14901, loss 0.562949.
Train: 2018-08-02T13:37:25.363797: step 14902, loss 0.534862.
Train: 2018-08-02T13:37:25.520015: step 14903, loss 0.566017.
Train: 2018-08-02T13:37:25.676231: step 14904, loss 0.481446.
Train: 2018-08-02T13:37:25.832413: step 14905, loss 0.503573.
Train: 2018-08-02T13:37:25.988626: step 14906, loss 0.498996.
Train: 2018-08-02T13:37:26.144868: step 14907, loss 0.480594.
Train: 2018-08-02T13:37:26.285461: step 14908, loss 0.480423.
Train: 2018-08-02T13:37:26.441674: step 14909, loss 0.386162.
Train: 2018-08-02T13:37:26.613480: step 14910, loss 0.419048.
Test: 2018-08-02T13:37:27.066498: step 14910, loss 0.564784.
Train: 2018-08-02T13:37:27.222742: step 14911, loss 0.515975.
Train: 2018-08-02T13:37:27.378956: step 14912, loss 0.63159.
Train: 2018-08-02T13:37:27.535163: step 14913, loss 0.460601.
Train: 2018-08-02T13:37:27.691381: step 14914, loss 0.578274.
Train: 2018-08-02T13:37:27.847595: step 14915, loss 0.572272.
Train: 2018-08-02T13:37:28.003808: step 14916, loss 0.543402.
Train: 2018-08-02T13:37:28.160021: step 14917, loss 0.618525.
Train: 2018-08-02T13:37:28.316235: step 14918, loss 0.555291.
Train: 2018-08-02T13:37:28.456827: step 14919, loss 0.503356.
Train: 2018-08-02T13:37:28.613041: step 14920, loss 0.472577.
Test: 2018-08-02T13:37:29.066029: step 14920, loss 0.547608.
Train: 2018-08-02T13:37:29.222254: step 14921, loss 0.468199.
Train: 2018-08-02T13:37:29.378487: step 14922, loss 0.530014.
Train: 2018-08-02T13:37:29.534700: step 14923, loss 0.472806.
Train: 2018-08-02T13:37:29.706536: step 14924, loss 0.595043.
Train: 2018-08-02T13:37:29.862719: step 14925, loss 0.488862.
Train: 2018-08-02T13:37:30.018931: step 14926, loss 0.622251.
Train: 2018-08-02T13:37:30.175178: step 14927, loss 0.530862.
Train: 2018-08-02T13:37:30.331395: step 14928, loss 0.538761.
Train: 2018-08-02T13:37:30.487574: step 14929, loss 0.51035.
Train: 2018-08-02T13:37:30.643814: step 14930, loss 0.508549.
Test: 2018-08-02T13:37:31.096804: step 14930, loss 0.55508.
Train: 2018-08-02T13:37:31.253018: step 14931, loss 0.507735.
Train: 2018-08-02T13:37:31.409263: step 14932, loss 0.53743.
Train: 2018-08-02T13:37:31.565444: step 14933, loss 0.494191.
Train: 2018-08-02T13:37:31.721658: step 14934, loss 0.605545.
Train: 2018-08-02T13:37:31.877873: step 14935, loss 0.483248.
Train: 2018-08-02T13:37:32.049738: step 14936, loss 0.584614.
Train: 2018-08-02T13:37:32.205920: step 14937, loss 0.521966.
Train: 2018-08-02T13:37:32.362166: step 14938, loss 0.526083.
Train: 2018-08-02T13:37:32.518347: step 14939, loss 0.514808.
Train: 2018-08-02T13:37:32.674559: step 14940, loss 0.495558.
Test: 2018-08-02T13:37:33.127604: step 14940, loss 0.552765.
Train: 2018-08-02T13:37:33.283792: step 14941, loss 0.524217.
Train: 2018-08-02T13:37:33.440030: step 14942, loss 0.549369.
Train: 2018-08-02T13:37:33.596249: step 14943, loss 0.47144.
Train: 2018-08-02T13:37:33.752456: step 14944, loss 0.50505.
Train: 2018-08-02T13:37:33.893054: step 14945, loss 0.466398.
Train: 2018-08-02T13:37:34.064889: step 14946, loss 0.486196.
Train: 2018-08-02T13:37:34.221097: step 14947, loss 0.531808.
Train: 2018-08-02T13:37:34.377311: step 14948, loss 0.467832.
Train: 2018-08-02T13:37:34.486665: step 14949, loss 0.630256.
Train: 2018-08-02T13:37:34.642880: step 14950, loss 0.476813.
Test: 2018-08-02T13:37:35.111488: step 14950, loss 0.549472.
Train: 2018-08-02T13:37:35.267732: step 14951, loss 0.547928.
Train: 2018-08-02T13:37:35.423948: step 14952, loss 0.496252.
Train: 2018-08-02T13:37:35.580159: step 14953, loss 0.441241.
Train: 2018-08-02T13:37:35.736344: step 14954, loss 0.516422.
Train: 2018-08-02T13:37:35.892557: step 14955, loss 0.490679.
Train: 2018-08-02T13:37:36.048802: step 14956, loss 0.428746.
Train: 2018-08-02T13:37:36.220605: step 14957, loss 0.419123.
Train: 2018-08-02T13:37:36.376848: step 14958, loss 0.504684.
Train: 2018-08-02T13:37:36.533060: step 14959, loss 0.432199.
Train: 2018-08-02T13:37:36.689275: step 14960, loss 0.535135.
Test: 2018-08-02T13:37:37.142288: step 14960, loss 0.551301.
Train: 2018-08-02T13:37:37.298509: step 14961, loss 0.497906.
Train: 2018-08-02T13:37:37.454720: step 14962, loss 0.566202.
Train: 2018-08-02T13:37:37.610934: step 14963, loss 0.490068.
Train: 2018-08-02T13:37:37.767151: step 14964, loss 0.448418.
Train: 2018-08-02T13:37:37.923359: step 14965, loss 0.517535.
Train: 2018-08-02T13:37:38.095165: step 14966, loss 0.485837.
Train: 2018-08-02T13:37:38.251409: step 14967, loss 0.446561.
Train: 2018-08-02T13:37:38.407617: step 14968, loss 0.473125.
Train: 2018-08-02T13:37:38.563835: step 14969, loss 0.500622.
Train: 2018-08-02T13:37:38.720020: step 14970, loss 0.41693.
Test: 2018-08-02T13:37:39.173063: step 14970, loss 0.549288.
Train: 2018-08-02T13:37:39.329251: step 14971, loss 0.529215.
Train: 2018-08-02T13:37:39.485464: step 14972, loss 0.501439.
Train: 2018-08-02T13:37:39.641679: step 14973, loss 0.525028.
Train: 2018-08-02T13:37:39.813527: step 14974, loss 0.474172.
Train: 2018-08-02T13:37:39.954135: step 14975, loss 0.479647.
Train: 2018-08-02T13:37:40.125941: step 14976, loss 0.448472.
Train: 2018-08-02T13:37:40.282153: step 14977, loss 0.539842.
Train: 2018-08-02T13:37:40.438397: step 14978, loss 0.494101.
Train: 2018-08-02T13:37:40.594604: step 14979, loss 0.509829.
Train: 2018-08-02T13:37:40.750809: step 14980, loss 0.524185.
Test: 2018-08-02T13:37:41.203813: step 14980, loss 0.547542.
Train: 2018-08-02T13:37:41.360055: step 14981, loss 0.501811.
Train: 2018-08-02T13:37:41.516269: step 14982, loss 0.526483.
Train: 2018-08-02T13:37:41.672485: step 14983, loss 0.547351.
Train: 2018-08-02T13:37:41.844317: step 14984, loss 0.544945.
Train: 2018-08-02T13:37:42.000531: step 14985, loss 0.6146.
Train: 2018-08-02T13:37:42.172369: step 14986, loss 0.462722.
Train: 2018-08-02T13:37:42.328576: step 14987, loss 0.591325.
Train: 2018-08-02T13:37:42.484789: step 14988, loss 0.548468.
Train: 2018-08-02T13:37:42.636889: step 14989, loss 0.497472.
Train: 2018-08-02T13:37:42.793137: step 14990, loss 0.450384.
Test: 2018-08-02T13:37:43.261747: step 14990, loss 0.55058.
Train: 2018-08-02T13:37:43.417990: step 14991, loss 0.455467.
Train: 2018-08-02T13:37:43.574205: step 14992, loss 0.550053.
Train: 2018-08-02T13:37:43.730417: step 14993, loss 0.475254.
Train: 2018-08-02T13:37:43.886631: step 14994, loss 0.603966.
Train: 2018-08-02T13:37:44.042843: step 14995, loss 0.464854.
Train: 2018-08-02T13:37:44.183406: step 14996, loss 0.483978.
Train: 2018-08-02T13:37:44.339620: step 14997, loss 0.468863.
Train: 2018-08-02T13:37:44.495863: step 14998, loss 0.556566.
Train: 2018-08-02T13:37:44.652072: step 14999, loss 0.590501.
Train: 2018-08-02T13:37:44.808258: step 15000, loss 0.503247.
Test: 2018-08-02T13:37:45.261303: step 15000, loss 0.553846.
Train: 2018-08-02T13:37:45.901784: step 15001, loss 0.655313.
Train: 2018-08-02T13:37:46.057997: step 15002, loss 0.527263.
Train: 2018-08-02T13:37:46.214211: step 15003, loss 0.57384.
Train: 2018-08-02T13:37:46.370418: step 15004, loss 0.584063.
Train: 2018-08-02T13:37:46.526638: step 15005, loss 0.525721.
Train: 2018-08-02T13:37:46.682820: step 15006, loss 0.534061.
Train: 2018-08-02T13:37:46.854681: step 15007, loss 0.525642.
Train: 2018-08-02T13:37:47.010893: step 15008, loss 0.438988.
Train: 2018-08-02T13:37:47.167112: step 15009, loss 0.474669.
Train: 2018-08-02T13:37:47.323326: step 15010, loss 0.457678.
Test: 2018-08-02T13:37:47.791966: step 15010, loss 0.565835.
Train: 2018-08-02T13:37:47.948179: step 15011, loss 0.618953.
Train: 2018-08-02T13:37:48.104393: step 15012, loss 0.533952.
Train: 2018-08-02T13:37:48.244955: step 15013, loss 0.482641.
Train: 2018-08-02T13:37:48.401193: step 15014, loss 0.50694.
Train: 2018-08-02T13:37:48.557412: step 15015, loss 0.573016.
Train: 2018-08-02T13:37:48.729245: step 15016, loss 0.536953.
Train: 2018-08-02T13:37:48.885454: step 15017, loss 0.561588.
Train: 2018-08-02T13:37:49.041644: step 15018, loss 0.540171.
Train: 2018-08-02T13:37:49.197886: step 15019, loss 0.564181.
Train: 2018-08-02T13:37:49.354070: step 15020, loss 0.578111.
Test: 2018-08-02T13:37:49.807089: step 15020, loss 0.552649.
Train: 2018-08-02T13:37:49.963332: step 15021, loss 0.526269.
Train: 2018-08-02T13:37:50.119546: step 15022, loss 0.526765.
Train: 2018-08-02T13:37:50.275728: step 15023, loss 0.529734.
Train: 2018-08-02T13:37:50.431943: step 15024, loss 0.518852.
Train: 2018-08-02T13:37:50.588181: step 15025, loss 0.539331.
Train: 2018-08-02T13:37:50.744399: step 15026, loss 0.583648.
Train: 2018-08-02T13:37:50.900608: step 15027, loss 0.585097.
Train: 2018-08-02T13:37:51.056796: step 15028, loss 0.550782.
Train: 2018-08-02T13:37:51.213041: step 15029, loss 0.52999.
Train: 2018-08-02T13:37:51.369254: step 15030, loss 0.481956.
Test: 2018-08-02T13:37:51.822241: step 15030, loss 0.554104.
Train: 2018-08-02T13:37:51.978485: step 15031, loss 0.497719.
Train: 2018-08-02T13:37:52.134699: step 15032, loss 0.591905.
Train: 2018-08-02T13:37:52.290882: step 15033, loss 0.60212.
Train: 2018-08-02T13:37:52.447125: step 15034, loss 0.508321.
Train: 2018-08-02T13:37:52.603308: step 15035, loss 0.44588.
Train: 2018-08-02T13:37:52.743932: step 15036, loss 0.436363.
Train: 2018-08-02T13:37:52.900147: step 15037, loss 0.476742.
Train: 2018-08-02T13:37:53.056329: step 15038, loss 0.596446.
Train: 2018-08-02T13:37:53.212541: step 15039, loss 0.530363.
Train: 2018-08-02T13:37:53.368789: step 15040, loss 0.529721.
Test: 2018-08-02T13:37:53.821774: step 15040, loss 0.547581.
Train: 2018-08-02T13:37:53.978017: step 15041, loss 0.523254.
Train: 2018-08-02T13:37:54.134231: step 15042, loss 0.507389.
Train: 2018-08-02T13:37:54.290445: step 15043, loss 0.470582.
Train: 2018-08-02T13:37:54.462248: step 15044, loss 0.53597.
Train: 2018-08-02T13:37:54.618492: step 15045, loss 0.475497.
Train: 2018-08-02T13:37:54.774676: step 15046, loss 0.603964.
Train: 2018-08-02T13:37:54.915298: step 15047, loss 0.534222.
Train: 2018-08-02T13:37:55.071511: step 15048, loss 0.504395.
Train: 2018-08-02T13:37:55.227695: step 15049, loss 0.527305.
Train: 2018-08-02T13:37:55.383937: step 15050, loss 0.54021.
Test: 2018-08-02T13:37:55.836927: step 15050, loss 0.551999.
Train: 2018-08-02T13:37:55.993166: step 15051, loss 0.461166.
Train: 2018-08-02T13:37:56.149379: step 15052, loss 0.488408.
Train: 2018-08-02T13:37:56.305568: step 15053, loss 0.473732.
Train: 2018-08-02T13:37:56.461811: step 15054, loss 0.535294.
Train: 2018-08-02T13:37:56.618025: step 15055, loss 0.540373.
Train: 2018-08-02T13:37:56.789860: step 15056, loss 0.555672.
Train: 2018-08-02T13:37:56.946067: step 15057, loss 0.573009.
Train: 2018-08-02T13:37:57.102281: step 15058, loss 0.492032.
Train: 2018-08-02T13:37:57.258470: step 15059, loss 0.5901.
Train: 2018-08-02T13:37:57.414714: step 15060, loss 0.561322.
Test: 2018-08-02T13:37:57.883353: step 15060, loss 0.557243.
Train: 2018-08-02T13:37:58.039565: step 15061, loss 0.536974.
Train: 2018-08-02T13:37:58.180159: step 15062, loss 0.496189.
Train: 2018-08-02T13:37:58.351965: step 15063, loss 0.458273.
Train: 2018-08-02T13:37:58.492585: step 15064, loss 0.575563.
Train: 2018-08-02T13:37:58.648798: step 15065, loss 0.503586.
Train: 2018-08-02T13:37:58.805012: step 15066, loss 0.518326.
Train: 2018-08-02T13:37:58.961225: step 15067, loss 0.491079.
Train: 2018-08-02T13:37:59.117439: step 15068, loss 0.553364.
Train: 2018-08-02T13:37:59.273654: step 15069, loss 0.558765.
Train: 2018-08-02T13:37:59.429837: step 15070, loss 0.48655.
Test: 2018-08-02T13:37:59.898476: step 15070, loss 0.547608.
Train: 2018-08-02T13:38:00.054719: step 15071, loss 0.605543.
Train: 2018-08-02T13:38:00.210928: step 15072, loss 0.497742.
Train: 2018-08-02T13:38:00.367146: step 15073, loss 0.554375.
Train: 2018-08-02T13:38:00.523328: step 15074, loss 0.482675.
Train: 2018-08-02T13:38:00.679544: step 15075, loss 0.558216.
Train: 2018-08-02T13:38:00.835787: step 15076, loss 0.53684.
Train: 2018-08-02T13:38:00.991969: step 15077, loss 0.563898.
Train: 2018-08-02T13:38:01.148184: step 15078, loss 0.572248.
Train: 2018-08-02T13:38:01.320048: step 15079, loss 0.478743.
Train: 2018-08-02T13:38:01.476262: step 15080, loss 0.544083.
Test: 2018-08-02T13:38:01.929251: step 15080, loss 0.549894.
Train: 2018-08-02T13:38:02.085462: step 15081, loss 0.480946.
Train: 2018-08-02T13:38:02.241676: step 15082, loss 0.546792.
Train: 2018-08-02T13:38:02.397889: step 15083, loss 0.504484.
Train: 2018-08-02T13:38:02.538483: step 15084, loss 0.503681.
Train: 2018-08-02T13:38:02.710319: step 15085, loss 0.49196.
Train: 2018-08-02T13:38:02.866530: step 15086, loss 0.533987.
Train: 2018-08-02T13:38:03.022743: step 15087, loss 0.437422.
Train: 2018-08-02T13:38:03.178990: step 15088, loss 0.515026.
Train: 2018-08-02T13:38:03.335170: step 15089, loss 0.484409.
Train: 2018-08-02T13:38:03.475792: step 15090, loss 0.483872.
Test: 2018-08-02T13:38:03.944403: step 15090, loss 0.546552.
Train: 2018-08-02T13:38:04.085025: step 15091, loss 0.528822.
Train: 2018-08-02T13:38:04.241239: step 15092, loss 0.498165.
Train: 2018-08-02T13:38:04.397452: step 15093, loss 0.486059.
Train: 2018-08-02T13:38:04.553666: step 15094, loss 0.492107.
Train: 2018-08-02T13:38:04.709849: step 15095, loss 0.535856.
Train: 2018-08-02T13:38:04.881714: step 15096, loss 0.553656.
Train: 2018-08-02T13:38:05.037899: step 15097, loss 0.474705.
Train: 2018-08-02T13:38:05.194141: step 15098, loss 0.501962.
Train: 2018-08-02T13:38:05.350349: step 15099, loss 0.540441.
Train: 2018-08-02T13:38:05.459673: step 15100, loss 0.652204.
Test: 2018-08-02T13:38:05.928345: step 15100, loss 0.549931.
