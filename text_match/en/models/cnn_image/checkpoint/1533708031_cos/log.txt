Train: 2018-08-08T14:00:39.093558: step 1, loss 0.590502.
Train: 2018-08-08T14:00:42.158707: step 2, loss 1.51798.
Train: 2018-08-08T14:00:45.201798: step 3, loss 0.827132.
Train: 2018-08-08T14:00:48.290009: step 4, loss 0.679126.
Train: 2018-08-08T14:00:51.351147: step 5, loss 0.69987.
Train: 2018-08-08T14:00:54.428329: step 6, loss 0.667443.
Train: 2018-08-08T14:00:57.488465: step 7, loss 0.634174.
Train: 2018-08-08T14:01:00.555620: step 8, loss 0.590365.
Train: 2018-08-08T14:01:03.608737: step 9, loss 0.472199.
Train: 2018-08-08T14:01:06.704969: step 10, loss 0.616851.
Test: 2018-08-08T14:01:26.222862: step 10, loss 0.698539.
Train: 2018-08-08T14:01:29.246902: step 11, loss 0.569416.
Train: 2018-08-08T14:01:32.294003: step 12, loss 0.522268.
Train: 2018-08-08T14:01:35.401265: step 13, loss 0.5203.
Train: 2018-08-08T14:01:38.454382: step 14, loss 0.545184.
Train: 2018-08-08T14:01:41.502486: step 15, loss 0.56831.
Train: 2018-08-08T14:01:44.548585: step 16, loss 0.45529.
Train: 2018-08-08T14:01:47.579644: step 17, loss 0.659118.
Train: 2018-08-08T14:01:50.610703: step 18, loss 0.655712.
Train: 2018-08-08T14:01:53.666828: step 19, loss 0.381124.
Train: 2018-08-08T14:01:56.731977: step 20, loss 0.613647.
Test: 2018-08-08T14:02:16.494520: step 20, loss 0.64189.
Train: 2018-08-08T14:02:19.554656: step 21, loss 0.63693.
Train: 2018-08-08T14:02:22.617800: step 22, loss 0.61218.
Train: 2018-08-08T14:02:25.678939: step 23, loss 0.522957.
Train: 2018-08-08T14:02:28.737070: step 24, loss 0.539492.
Train: 2018-08-08T14:02:31.817259: step 25, loss 0.536063.
Train: 2018-08-08T14:02:34.865363: step 26, loss 0.495985.
Train: 2018-08-08T14:02:38.004710: step 27, loss 0.513649.
Train: 2018-08-08T14:02:41.062840: step 28, loss 0.59545.
Train: 2018-08-08T14:02:44.160075: step 29, loss 0.619467.
Train: 2018-08-08T14:02:47.247283: step 30, loss 0.46333.
Test: 2018-08-08T14:03:07.147192: step 30, loss 0.626435.
Train: 2018-08-08T14:03:10.282528: step 31, loss 0.574888.
Train: 2018-08-08T14:03:13.359709: step 32, loss 0.481701.
Train: 2018-08-08T14:03:16.432880: step 33, loss 0.541151.
Train: 2018-08-08T14:03:19.512066: step 34, loss 0.517952.
Train: 2018-08-08T14:03:22.587242: step 35, loss 0.500148.
Train: 2018-08-08T14:03:25.697512: step 36, loss 0.609124.
Train: 2018-08-08T14:03:28.784720: step 37, loss 0.45162.
Train: 2018-08-08T14:03:31.881954: step 38, loss 0.423285.
Train: 2018-08-08T14:03:34.968160: step 39, loss 0.50251.
Train: 2018-08-08T14:03:38.132573: step 40, loss 0.574339.
Test: 2018-08-08T14:03:58.120716: step 40, loss 0.627967.
Train: 2018-08-08T14:04:01.224970: step 41, loss 0.520936.
Train: 2018-08-08T14:04:04.334236: step 42, loss 0.555482.
Train: 2018-08-08T14:04:07.420441: step 43, loss 0.540484.
Train: 2018-08-08T14:04:10.537730: step 44, loss 0.549488.
Train: 2018-08-08T14:04:13.672063: step 45, loss 0.62488.
Train: 2018-08-08T14:04:16.753255: step 46, loss 0.528769.
Train: 2018-08-08T14:04:19.832442: step 47, loss 0.642268.
Train: 2018-08-08T14:04:22.930679: step 48, loss 0.574798.
Train: 2018-08-08T14:04:26.015882: step 49, loss 0.612327.
Train: 2018-08-08T14:04:29.079026: step 50, loss 0.560614.
Test: 2018-08-08T14:04:49.222582: step 50, loss 0.628449.
Train: 2018-08-08T14:04:52.334857: step 51, loss 0.553739.
Train: 2018-08-08T14:04:55.491249: step 52, loss 0.490283.
Train: 2018-08-08T14:04:58.556398: step 53, loss 0.616069.
Train: 2018-08-08T14:05:01.666667: step 54, loss 0.608578.
Train: 2018-08-08T14:05:04.794984: step 55, loss 0.463813.
Train: 2018-08-08T14:05:07.915280: step 56, loss 0.594232.
Train: 2018-08-08T14:05:10.999481: step 57, loss 0.538809.
Train: 2018-08-08T14:05:14.220043: step 58, loss 0.547412.
Train: 2018-08-08T14:05:17.293214: step 59, loss 0.615915.
Train: 2018-08-08T14:05:20.363377: step 60, loss 0.565386.
Test: 2018-08-08T14:05:41.095765: step 60, loss 0.612762.
Train: 2018-08-08T14:05:44.581604: step 61, loss 0.511856.
Train: 2018-08-08T14:05:47.882636: step 62, loss 0.601333.
Train: 2018-08-08T14:05:51.026794: step 63, loss 0.665434.
Train: 2018-08-08T14:05:54.110994: step 64, loss 0.541888.
Train: 2018-08-08T14:05:57.197200: step 65, loss 0.502452.
Train: 2018-08-08T14:06:00.313485: step 66, loss 0.510278.
Train: 2018-08-08T14:06:03.578165: step 67, loss 0.701355.
Train: 2018-08-08T14:06:06.830580: step 68, loss 0.584545.
Train: 2018-08-08T14:06:10.344407: step 69, loss 0.608236.
Train: 2018-08-08T14:06:13.701164: step 70, loss 0.527631.
Test: 2018-08-08T14:06:34.266775: step 70, loss 0.610888.
Train: 2018-08-08T14:06:37.317886: step 71, loss 0.522154.
Train: 2018-08-08T14:06:40.377020: step 72, loss 0.495602.
Train: 2018-08-08T14:06:43.435150: step 73, loss 0.584107.
Train: 2018-08-08T14:06:46.512332: step 74, loss 0.575703.
Train: 2018-08-08T14:06:49.540383: step 75, loss 0.442468.
Train: 2018-08-08T14:06:52.579463: step 76, loss 0.525914.
Train: 2018-08-08T14:06:55.717807: step 77, loss 0.458341.
Train: 2018-08-08T14:06:58.798999: step 78, loss 0.59248.
Train: 2018-08-08T14:07:02.078719: step 79, loss 0.429413.
Train: 2018-08-08T14:07:05.179964: step 80, loss 0.569295.
Test: 2018-08-08T14:07:25.315499: step 80, loss 0.601207.
Train: 2018-08-08T14:07:28.388670: step 81, loss 0.641838.
Train: 2018-08-08T14:07:31.437776: step 82, loss 0.441665.
Train: 2018-08-08T14:07:34.503928: step 83, loss 0.508934.
Train: 2018-08-08T14:07:37.541003: step 84, loss 0.605391.
Train: 2018-08-08T14:07:40.606153: step 85, loss 0.532284.
Train: 2018-08-08T14:07:43.627184: step 86, loss 0.541575.
Train: 2018-08-08T14:07:46.704366: step 87, loss 0.454054.
Train: 2018-08-08T14:07:49.745451: step 88, loss 0.589557.
Train: 2018-08-08T14:07:52.798569: step 89, loss 0.465926.
Train: 2018-08-08T14:07:55.817595: step 90, loss 0.467363.
Test: 2018-08-08T14:08:15.915029: step 90, loss 0.580426.
Train: 2018-08-08T14:08:19.037330: step 91, loss 0.652273.
Train: 2018-08-08T14:08:22.090448: step 92, loss 0.563955.
Train: 2018-08-08T14:08:25.118498: step 93, loss 0.479886.
Train: 2018-08-08T14:08:28.264063: step 94, loss 0.502194.
Train: 2018-08-08T14:08:31.419453: step 95, loss 0.470789.
Train: 2018-08-08T14:08:34.466554: step 96, loss 0.462623.
Train: 2018-08-08T14:08:37.483575: step 97, loss 0.563578.
Train: 2018-08-08T14:08:40.524661: step 98, loss 0.62665.
Train: 2018-08-08T14:08:43.568754: step 99, loss 0.57094.
Train: 2018-08-08T14:08:46.616858: step 100, loss 0.502341.
Test: 2018-08-08T14:09:06.705268: step 100, loss 0.588945.
Train: 2018-08-08T14:09:11.116997: step 101, loss 0.501231.
Train: 2018-08-08T14:09:14.184152: step 102, loss 0.515399.
Train: 2018-08-08T14:09:17.230251: step 103, loss 0.483637.
Train: 2018-08-08T14:09:20.287379: step 104, loss 0.544498.
Train: 2018-08-08T14:09:23.337488: step 105, loss 0.624213.
Train: 2018-08-08T14:09:26.397624: step 106, loss 0.499202.
Train: 2018-08-08T14:09:29.465781: step 107, loss 0.564891.
Train: 2018-08-08T14:09:32.515891: step 108, loss 0.567208.
Train: 2018-08-08T14:09:35.551963: step 109, loss 0.646835.
Train: 2018-08-08T14:09:38.666243: step 110, loss 0.591621.
Test: 2018-08-08T14:09:58.718557: step 110, loss 0.584696.
Train: 2018-08-08T14:10:01.963183: step 111, loss 0.463866.
Train: 2018-08-08T14:10:05.033346: step 112, loss 0.51543.
Train: 2018-08-08T14:10:08.107519: step 113, loss 0.592477.
Train: 2018-08-08T14:10:11.133565: step 114, loss 0.510242.
Train: 2018-08-08T14:10:14.195706: step 115, loss 0.478686.
Train: 2018-08-08T14:10:17.217741: step 116, loss 0.501818.
Train: 2018-08-08T14:10:20.259829: step 117, loss 0.598165.
Train: 2018-08-08T14:10:23.301917: step 118, loss 0.506452.
Train: 2018-08-08T14:10:26.327962: step 119, loss 0.578446.
Train: 2018-08-08T14:10:29.367042: step 120, loss 0.493475.
Test: 2018-08-08T14:10:50.133254: step 120, loss 0.566388.
Train: 2018-08-08T14:10:53.576408: step 121, loss 0.558202.
Train: 2018-08-08T14:10:56.656598: step 122, loss 0.601217.
Train: 2018-08-08T14:10:59.701694: step 123, loss 0.592691.
Train: 2018-08-08T14:11:02.736763: step 124, loss 0.560879.
Train: 2018-08-08T14:11:05.842019: step 125, loss 0.476474.
Train: 2018-08-08T14:11:08.891126: step 126, loss 0.598802.
Train: 2018-08-08T14:11:11.962291: step 127, loss 0.525912.
Train: 2018-08-08T14:11:15.135729: step 128, loss 0.568403.
Train: 2018-08-08T14:11:18.194862: step 129, loss 0.522933.
Train: 2018-08-08T14:11:21.272043: step 130, loss 0.544465.
Test: 2018-08-08T14:11:41.445680: step 130, loss 0.557893.
Train: 2018-08-08T14:11:44.503810: step 131, loss 0.581842.
Train: 2018-08-08T14:11:47.615082: step 132, loss 0.584531.
Train: 2018-08-08T14:11:50.825618: step 133, loss 0.50821.
Train: 2018-08-08T14:11:53.962960: step 134, loss 0.649906.
Train: 2018-08-08T14:11:57.070221: step 135, loss 0.597385.
Train: 2018-08-08T14:12:00.181493: step 136, loss 0.642593.
Train: 2018-08-08T14:12:03.328861: step 137, loss 0.488908.
Train: 2018-08-08T14:12:06.427098: step 138, loss 0.647803.
Train: 2018-08-08T14:12:09.668852: step 139, loss 0.529795.
Train: 2018-08-08T14:12:12.850311: step 140, loss 0.504089.
Test: 2018-08-08T14:12:33.251552: step 140, loss 0.570855.
Train: 2018-08-08T14:12:36.365074: step 141, loss 0.583547.
Train: 2018-08-08T14:12:39.488378: step 142, loss 0.647125.
Train: 2018-08-08T14:12:42.598647: step 143, loss 0.526759.
Train: 2018-08-08T14:12:45.706912: step 144, loss 0.563925.
Train: 2018-08-08T14:12:48.796125: step 145, loss 0.556064.
Train: 2018-08-08T14:12:51.892357: step 146, loss 0.480273.
Train: 2018-08-08T14:12:55.019671: step 147, loss 0.563066.
Train: 2018-08-08T14:12:58.111893: step 148, loss 0.650314.
Train: 2018-08-08T14:13:01.243218: step 149, loss 0.514306.
Train: 2018-08-08T14:13:04.350479: step 150, loss 0.763415.
Test: 2018-08-08T14:13:24.701587: step 150, loss 0.558558.
Train: 2018-08-08T14:13:27.851066: step 151, loss 0.526275.
Train: 2018-08-08T14:13:31.023500: step 152, loss 0.689197.
Train: 2018-08-08T14:13:34.120735: step 153, loss 0.568865.
Train: 2018-08-08T14:13:37.215964: step 154, loss 0.621243.
Train: 2018-08-08T14:13:40.334255: step 155, loss 0.508073.
Train: 2018-08-08T14:13:43.429485: step 156, loss 0.514524.
Train: 2018-08-08T14:13:46.542762: step 157, loss 0.501456.
Train: 2018-08-08T14:13:49.692135: step 158, loss 0.604721.
Train: 2018-08-08T14:13:52.796388: step 159, loss 0.532592.
Train: 2018-08-08T14:13:55.911671: step 160, loss 0.538748.
Test: 2018-08-08T14:14:16.413179: step 160, loss 0.556466.
Train: 2018-08-08T14:14:19.573581: step 161, loss 0.626336.
Train: 2018-08-08T14:14:22.718944: step 162, loss 0.52094.
Train: 2018-08-08T14:14:25.832222: step 163, loss 0.537333.
Train: 2018-08-08T14:14:28.954523: step 164, loss 0.57903.
Train: 2018-08-08T14:14:32.075822: step 165, loss 0.681762.
Train: 2018-08-08T14:14:35.170048: step 166, loss 0.459359.
Train: 2018-08-08T14:14:38.266280: step 167, loss 0.494392.
Train: 2018-08-08T14:14:41.327419: step 168, loss 0.59543.
Train: 2018-08-08T14:14:44.399587: step 169, loss 0.426005.
Train: 2018-08-08T14:14:47.509857: step 170, loss 0.672657.
Test: 2018-08-08T14:15:07.723599: step 170, loss 0.54914.
Train: 2018-08-08T14:15:10.787746: step 171, loss 0.5485.
Train: 2018-08-08T14:15:13.870943: step 172, loss 0.495019.
Train: 2018-08-08T14:15:16.926066: step 173, loss 0.647695.
Train: 2018-08-08T14:15:19.978181: step 174, loss 0.550055.
Train: 2018-08-08T14:15:23.051352: step 175, loss 0.664715.
Train: 2018-08-08T14:15:26.139562: step 176, loss 0.488153.
Train: 2018-08-08T14:15:29.205714: step 177, loss 0.55023.
Train: 2018-08-08T14:15:32.311973: step 178, loss 0.557825.
Train: 2018-08-08T14:15:35.373111: step 179, loss 0.61365.
Train: 2018-08-08T14:15:38.405173: step 180, loss 0.516617.
Test: 2018-08-08T14:15:58.643982: step 180, loss 0.555942.
Train: 2018-08-08T14:16:01.757260: step 181, loss 0.607269.
Train: 2018-08-08T14:16:04.859508: step 182, loss 0.589272.
Train: 2018-08-08T14:16:07.919644: step 183, loss 0.562118.
Train: 2018-08-08T14:16:11.009860: step 184, loss 0.515998.
Train: 2018-08-08T14:16:14.111105: step 185, loss 0.636209.
Train: 2018-08-08T14:16:17.168233: step 186, loss 0.545896.
Train: 2018-08-08T14:16:20.242407: step 187, loss 0.674195.
Train: 2018-08-08T14:16:23.326607: step 188, loss 0.647625.
Train: 2018-08-08T14:16:26.436876: step 189, loss 0.472506.
Train: 2018-08-08T14:16:29.511050: step 190, loss 0.553111.
Test: 2018-08-08T14:16:49.795349: step 190, loss 0.548489.
Train: 2018-08-08T14:16:52.997864: step 191, loss 0.532341.
Train: 2018-08-08T14:16:56.083066: step 192, loss 0.539269.
Train: 2018-08-08T14:16:59.164258: step 193, loss 0.589369.
Train: 2018-08-08T14:17:02.240437: step 194, loss 0.616163.
Train: 2018-08-08T14:17:05.311602: step 195, loss 0.476358.
Train: 2018-08-08T14:17:08.396805: step 196, loss 0.574002.
Train: 2018-08-08T14:17:11.485016: step 197, loss 0.495995.
Train: 2018-08-08T14:17:14.585258: step 198, loss 0.651499.
Train: 2018-08-08T14:17:17.668456: step 199, loss 0.609235.
Train: 2018-08-08T14:17:20.740624: step 200, loss 0.498185.
Test: 2018-08-08T14:17:41.107775: step 200, loss 0.553648.
Train: 2018-08-08T14:17:45.375120: step 201, loss 0.545781.
Train: 2018-08-08T14:17:48.456312: step 202, loss 0.533865.
Train: 2018-08-08T14:17:51.491382: step 203, loss 0.498335.
Train: 2018-08-08T14:17:54.570568: step 204, loss 0.441586.
Train: 2018-08-08T14:17:57.687856: step 205, loss 0.510932.
Train: 2018-08-08T14:18:00.761027: step 206, loss 0.520475.
Train: 2018-08-08T14:18:03.807126: step 207, loss 0.651169.
Train: 2018-08-08T14:18:06.877288: step 208, loss 0.485889.
Train: 2018-08-08T14:18:09.925392: step 209, loss 0.574646.
Train: 2018-08-08T14:18:13.023630: step 210, loss 0.507874.
Test: 2018-08-08T14:18:33.196263: step 210, loss 0.549033.
Train: 2018-08-08T14:18:36.299514: step 211, loss 0.523036.
Train: 2018-08-08T14:18:39.486993: step 212, loss 0.574591.
Train: 2018-08-08T14:18:42.535097: step 213, loss 0.529712.
Train: 2018-08-08T14:18:45.621302: step 214, loss 0.565228.
Train: 2018-08-08T14:18:48.702494: step 215, loss 0.595985.
Train: 2018-08-08T14:18:51.764635: step 216, loss 0.480815.
Train: 2018-08-08T14:18:54.853849: step 217, loss 0.659899.
Train: 2018-08-08T14:18:57.930028: step 218, loss 0.612266.
Train: 2018-08-08T14:19:00.986153: step 219, loss 0.558296.
Train: 2018-08-08T14:19:04.072358: step 220, loss 0.546913.
Test: 2018-08-08T14:19:24.283093: step 220, loss 0.556341.
Train: 2018-08-08T14:19:27.359272: step 221, loss 0.610544.
Train: 2018-08-08T14:19:30.413392: step 222, loss 0.541962.
Train: 2018-08-08T14:19:33.478541: step 223, loss 0.593306.
Train: 2018-08-08T14:19:36.570763: step 224, loss 0.591639.
Train: 2018-08-08T14:19:39.636915: step 225, loss 0.534603.
Train: 2018-08-08T14:19:42.712091: step 226, loss 0.534423.
Train: 2018-08-08T14:19:45.745155: step 227, loss 0.646632.
Train: 2018-08-08T14:19:48.851413: step 228, loss 0.631894.
Train: 2018-08-08T14:19:51.922579: step 229, loss 0.577282.
Train: 2018-08-08T14:19:55.007781: step 230, loss 0.540423.
Test: 2018-08-08T14:20:15.293849: step 230, loss 0.554141.
Train: 2018-08-08T14:20:18.362006: step 231, loss 0.558653.
Train: 2018-08-08T14:20:21.539455: step 232, loss 0.53651.
Train: 2018-08-08T14:20:24.670779: step 233, loss 0.57008.
Train: 2018-08-08T14:20:27.777038: step 234, loss 0.604069.
Train: 2018-08-08T14:20:30.843190: step 235, loss 0.525326.
Train: 2018-08-08T14:20:33.886281: step 236, loss 0.578234.
Train: 2018-08-08T14:20:36.950427: step 237, loss 0.674768.
Train: 2018-08-08T14:20:40.000537: step 238, loss 0.588683.
Train: 2018-08-08T14:20:43.095766: step 239, loss 0.501091.
Train: 2018-08-08T14:20:46.175956: step 240, loss 0.524196.
Test: 2018-08-08T14:21:06.339565: step 240, loss 0.550451.
Train: 2018-08-08T14:21:09.451840: step 241, loss 0.549898.
Train: 2018-08-08T14:21:12.528018: step 242, loss 0.542648.
Train: 2018-08-08T14:21:15.629264: step 243, loss 0.646018.
Train: 2018-08-08T14:21:18.703437: step 244, loss 0.568328.
Train: 2018-08-08T14:21:21.757557: step 245, loss 0.452542.
Train: 2018-08-08T14:21:24.843762: step 246, loss 0.56385.
Train: 2018-08-08T14:21:27.906950: step 247, loss 0.598696.
Train: 2018-08-08T14:21:30.995161: step 248, loss 0.531767.
Train: 2018-08-08T14:21:34.069335: step 249, loss 0.534435.
Train: 2018-08-08T14:21:37.128468: step 250, loss 0.566979.
Test: 2018-08-08T14:21:57.274030: step 250, loss 0.548376.
Train: 2018-08-08T14:22:00.334165: step 251, loss 0.464345.
Train: 2018-08-08T14:22:03.459475: step 252, loss 0.536029.
Train: 2018-08-08T14:22:06.533648: step 253, loss 0.590042.
Train: 2018-08-08T14:22:09.624867: step 254, loss 0.647208.
Train: 2018-08-08T14:22:12.709067: step 255, loss 0.514775.
Train: 2018-08-08T14:22:15.764190: step 256, loss 0.485052.
Train: 2018-08-08T14:22:18.838363: step 257, loss 0.521163.
Train: 2018-08-08T14:22:21.911534: step 258, loss 0.454709.
Train: 2018-08-08T14:22:25.012779: step 259, loss 0.529111.
Train: 2018-08-08T14:22:28.084947: step 260, loss 0.521171.
Test: 2018-08-08T14:22:48.290669: step 260, loss 0.546459.
Train: 2018-08-08T14:22:51.361834: step 261, loss 0.476927.
Train: 2018-08-08T14:22:54.407933: step 262, loss 0.525067.
Train: 2018-08-08T14:22:57.468069: step 263, loss 0.552595.
Train: 2018-08-08T14:23:00.531213: step 264, loss 0.689019.
Train: 2018-08-08T14:23:03.598368: step 265, loss 0.544099.
Train: 2018-08-08T14:23:06.687581: step 266, loss 0.623935.
Train: 2018-08-08T14:23:09.761754: step 267, loss 0.550866.
Train: 2018-08-08T14:23:12.855981: step 268, loss 0.573132.
Train: 2018-08-08T14:23:15.919125: step 269, loss 0.547473.
Train: 2018-08-08T14:23:18.982269: step 270, loss 0.644612.
Test: 2018-08-08T14:23:39.179969: step 270, loss 0.561975.
Train: 2018-08-08T14:23:42.273193: step 271, loss 0.576308.
Train: 2018-08-08T14:23:45.335335: step 272, loss 0.531658.
Train: 2018-08-08T14:23:48.440591: step 273, loss 0.513613.
Train: 2018-08-08T14:23:51.514764: step 274, loss 0.60745.
Train: 2018-08-08T14:23:54.552841: step 275, loss 0.553557.
Train: 2018-08-08T14:23:57.629020: step 276, loss 0.656802.
Train: 2018-08-08T14:24:00.700186: step 277, loss 0.516839.
Train: 2018-08-08T14:24:03.831511: step 278, loss 0.557416.
Train: 2018-08-08T14:24:06.913705: step 279, loss 0.63794.
Train: 2018-08-08T14:24:10.006930: step 280, loss 0.547707.
Test: 2018-08-08T14:24:30.261782: step 280, loss 0.554368.
Train: 2018-08-08T14:24:33.404136: step 281, loss 0.616362.
Train: 2018-08-08T14:24:36.486959: step 282, loss 0.547475.
Train: 2018-08-08T14:24:39.561132: step 283, loss 0.55388.
Train: 2018-08-08T14:24:42.625279: step 284, loss 0.544369.
Train: 2018-08-08T14:24:45.712487: step 285, loss 0.51714.
Train: 2018-08-08T14:24:48.776634: step 286, loss 0.508918.
Train: 2018-08-08T14:24:51.857826: step 287, loss 0.538065.
Train: 2018-08-08T14:24:54.963082: step 288, loss 0.495148.
Train: 2018-08-08T14:24:58.062322: step 289, loss 0.448721.
Train: 2018-08-08T14:25:01.148527: step 290, loss 0.545086.
Test: 2018-08-08T14:25:21.315145: step 290, loss 0.547844.
Train: 2018-08-08T14:25:24.402353: step 291, loss 0.602322.
Train: 2018-08-08T14:25:27.496580: step 292, loss 0.498503.
Train: 2018-08-08T14:25:30.538668: step 293, loss 0.551095.
Train: 2018-08-08T14:25:33.610836: step 294, loss 0.508374.
Train: 2018-08-08T14:25:36.757201: step 295, loss 0.45635.
Train: 2018-08-08T14:25:39.821348: step 296, loss 0.497778.
Train: 2018-08-08T14:25:42.887500: step 297, loss 0.594606.
Train: 2018-08-08T14:25:45.946633: step 298, loss 0.593759.
Train: 2018-08-08T14:25:49.021809: step 299, loss 0.539153.
Train: 2018-08-08T14:25:52.164164: step 300, loss 0.592659.
Test: 2018-08-08T14:26:12.325769: step 300, loss 0.550594.
Train: 2018-08-08T14:26:16.414639: step 301, loss 0.475364.
Train: 2018-08-08T14:26:19.472770: step 302, loss 0.612665.
Train: 2018-08-08T14:26:22.524884: step 303, loss 0.585195.
Train: 2018-08-08T14:26:25.583015: step 304, loss 0.631218.
Train: 2018-08-08T14:26:28.652175: step 305, loss 0.50089.
Train: 2018-08-08T14:26:31.785506: step 306, loss 0.482306.
Train: 2018-08-08T14:26:34.842634: step 307, loss 0.539016.
Train: 2018-08-08T14:26:37.903773: step 308, loss 0.557159.
Train: 2018-08-08T14:26:40.961903: step 309, loss 0.496355.
Train: 2018-08-08T14:26:44.027053: step 310, loss 0.673778.
Test: 2018-08-08T14:27:04.173617: step 310, loss 0.547252.
Train: 2018-08-08T14:27:07.245785: step 311, loss 0.549035.
Train: 2018-08-08T14:27:10.322967: step 312, loss 0.515815.
Train: 2018-08-08T14:27:13.396137: step 313, loss 0.56737.
Train: 2018-08-08T14:27:16.450257: step 314, loss 0.585198.
Train: 2018-08-08T14:27:19.497359: step 315, loss 0.58643.
Train: 2018-08-08T14:27:22.550476: step 316, loss 0.572696.
Train: 2018-08-08T14:27:25.631668: step 317, loss 0.545756.
Train: 2018-08-08T14:27:28.720881: step 318, loss 0.60914.
Train: 2018-08-08T14:27:31.824132: step 319, loss 0.623959.
Train: 2018-08-08T14:27:34.882263: step 320, loss 0.506658.
Test: 2018-08-08T14:27:55.008774: step 320, loss 0.55997.
Train: 2018-08-08T14:27:58.362691: step 321, loss 0.6149.
Train: 2018-08-08T14:28:01.498028: step 322, loss 0.53691.
Train: 2018-08-08T14:28:04.559166: step 323, loss 0.584269.
Train: 2018-08-08T14:28:07.629328: step 324, loss 0.535374.
Train: 2018-08-08T14:28:10.727566: step 325, loss 0.554526.
Train: 2018-08-08T14:28:13.803744: step 326, loss 0.565214.
Train: 2018-08-08T14:28:16.862878: step 327, loss 0.530026.
Train: 2018-08-08T14:28:19.938054: step 328, loss 0.645418.
Train: 2018-08-08T14:28:23.000195: step 329, loss 0.519554.
Train: 2018-08-08T14:28:26.038273: step 330, loss 0.546439.
Test: 2018-08-08T14:28:46.135706: step 330, loss 0.553585.
Train: 2018-08-08T14:28:49.233943: step 331, loss 0.538329.
Train: 2018-08-08T14:28:52.315136: step 332, loss 0.543334.
Train: 2018-08-08T14:28:55.361234: step 333, loss 0.601128.
Train: 2018-08-08T14:28:58.443429: step 334, loss 0.499395.
Train: 2018-08-08T14:29:01.534648: step 335, loss 0.506396.
Train: 2018-08-08T14:29:04.618848: step 336, loss 0.537297.
Train: 2018-08-08T14:29:07.699037: step 337, loss 0.409647.
Train: 2018-08-08T14:29:10.774213: step 338, loss 0.466963.
Train: 2018-08-08T14:29:13.818306: step 339, loss 0.592258.
Train: 2018-08-08T14:29:16.861397: step 340, loss 0.529227.
Test: 2018-08-08T14:29:36.969860: step 340, loss 0.548658.
Train: 2018-08-08T14:29:40.030999: step 341, loss 0.525505.
Train: 2018-08-08T14:29:43.110185: step 342, loss 0.410651.
Train: 2018-08-08T14:29:46.172327: step 343, loss 0.524187.
Train: 2018-08-08T14:29:49.242489: step 344, loss 0.6738.
Train: 2018-08-08T14:29:52.321676: step 345, loss 0.625175.
Train: 2018-08-08T14:29:55.381812: step 346, loss 0.54782.
Train: 2018-08-08T14:29:58.445959: step 347, loss 0.519177.
Train: 2018-08-08T14:30:01.697487: step 348, loss 0.591915.
Train: 2018-08-08T14:30:04.765645: step 349, loss 0.501599.
Train: 2018-08-08T14:30:07.845834: step 350, loss 0.560465.
Test: 2018-08-08T14:30:28.037518: step 350, loss 0.555925.
Train: 2018-08-08T14:30:31.099660: step 351, loss 0.654272.
Train: 2018-08-08T14:30:34.206921: step 352, loss 0.440844.
Train: 2018-08-08T14:30:37.287110: step 353, loss 0.526617.
Train: 2018-08-08T14:30:40.351257: step 354, loss 0.635309.
Train: 2018-08-08T14:30:43.410391: step 355, loss 0.543324.
Train: 2018-08-08T14:30:46.469524: step 356, loss 0.546182.
Train: 2018-08-08T14:30:49.560742: step 357, loss 0.475753.
Train: 2018-08-08T14:30:52.643940: step 358, loss 0.43116.
Train: 2018-08-08T14:30:55.703073: step 359, loss 0.520282.
Train: 2018-08-08T14:30:58.783263: step 360, loss 0.520792.
Test: 2018-08-08T14:31:18.954893: step 360, loss 0.550829.
Train: 2018-08-08T14:31:22.056138: step 361, loss 0.599242.
Train: 2018-08-08T14:31:25.117277: step 362, loss 0.58551.
Train: 2018-08-08T14:31:28.172400: step 363, loss 0.43888.
Train: 2018-08-08T14:31:31.275651: step 364, loss 0.474984.
Train: 2018-08-08T14:31:34.329771: step 365, loss 0.387195.
Train: 2018-08-08T14:31:37.402942: step 366, loss 0.529845.
Train: 2018-08-08T14:31:40.463078: step 367, loss 0.47053.
Train: 2018-08-08T14:31:43.538254: step 368, loss 0.463868.
Train: 2018-08-08T14:31:46.575328: step 369, loss 0.542587.
Train: 2018-08-08T14:31:49.655518: step 370, loss 0.468491.
Test: 2018-08-08T14:32:09.893325: step 370, loss 0.552196.
Train: 2018-08-08T14:32:12.941429: step 371, loss 0.625151.
Train: 2018-08-08T14:32:15.995549: step 372, loss 0.531533.
Train: 2018-08-08T14:32:19.060698: step 373, loss 0.574214.
Train: 2018-08-08T14:32:22.122840: step 374, loss 0.538702.
Train: 2018-08-08T14:32:25.183978: step 375, loss 0.54922.
Train: 2018-08-08T14:32:28.263165: step 376, loss 0.561846.
Train: 2018-08-08T14:32:31.314277: step 377, loss 0.522764.
Train: 2018-08-08T14:32:34.385442: step 378, loss 0.610771.
Train: 2018-08-08T14:32:37.421514: step 379, loss 0.516427.
Train: 2018-08-08T14:32:40.499698: step 380, loss 0.580194.
Test: 2018-08-08T14:33:00.711436: step 380, loss 0.560635.
Train: 2018-08-08T14:33:03.777588: step 381, loss 0.513895.
Train: 2018-08-08T14:33:06.843740: step 382, loss 0.524922.
Train: 2018-08-08T14:33:09.907887: step 383, loss 0.500786.
Train: 2018-08-08T14:33:13.006124: step 384, loss 0.535903.
Train: 2018-08-08T14:33:16.097343: step 385, loss 0.493977.
Train: 2018-08-08T14:33:19.160487: step 386, loss 0.488958.
Train: 2018-08-08T14:33:22.233658: step 387, loss 0.435692.
Train: 2018-08-08T14:33:25.335906: step 388, loss 0.476568.
Train: 2018-08-08T14:33:28.417098: step 389, loss 0.398385.
Train: 2018-08-08T14:33:31.513330: step 390, loss 0.570062.
Test: 2018-08-08T14:33:52.001803: step 390, loss 0.556417.
Train: 2018-08-08T14:33:55.173235: step 391, loss 0.443715.
Train: 2018-08-08T14:33:58.262448: step 392, loss 0.669427.
Train: 2018-08-08T14:34:01.334616: step 393, loss 0.435513.
Train: 2018-08-08T14:34:04.467947: step 394, loss 0.565814.
Train: 2018-08-08T14:34:07.532093: step 395, loss 0.552181.
Train: 2018-08-08T14:34:10.653392: step 396, loss 0.687505.
Train: 2018-08-08T14:34:13.815801: step 397, loss 0.495614.
Train: 2018-08-08T14:34:16.911029: step 398, loss 0.498509.
Train: 2018-08-08T14:34:20.049373: step 399, loss 0.49654.
Train: 2018-08-08T14:34:23.190726: step 400, loss 0.6268.
Test: 2018-08-08T14:34:43.414280: step 400, loss 0.552766.
Train: 2018-08-08T14:34:47.643525: step 401, loss 0.48964.
Train: 2018-08-08T14:34:50.749783: step 402, loss 0.540825.
Train: 2018-08-08T14:34:53.849024: step 403, loss 0.607432.
Train: 2018-08-08T14:34:56.915177: step 404, loss 0.573514.
Train: 2018-08-08T14:35:00.017424: step 405, loss 0.590507.
Train: 2018-08-08T14:35:03.112654: step 406, loss 0.541938.
Train: 2018-08-08T14:35:06.201866: step 407, loss 0.511728.
Train: 2018-08-08T14:35:09.288072: step 408, loss 0.615101.
Train: 2018-08-08T14:35:12.404357: step 409, loss 0.538218.
Train: 2018-08-08T14:35:15.602861: step 410, loss 0.423667.
Test: 2018-08-08T14:35:35.830641: step 410, loss 0.54801.
Train: 2018-08-08T14:35:38.866713: step 411, loss 0.664927.
Train: 2018-08-08T14:35:41.941889: step 412, loss 0.522368.
Train: 2018-08-08T14:35:45.003028: step 413, loss 0.542919.
Train: 2018-08-08T14:35:48.080209: step 414, loss 0.640954.
Train: 2018-08-08T14:35:51.131321: step 415, loss 0.653751.
Train: 2018-08-08T14:35:54.221537: step 416, loss 0.511345.
Train: 2018-08-08T14:35:57.326793: step 417, loss 0.53455.
Train: 2018-08-08T14:36:00.397959: step 418, loss 0.491585.
Train: 2018-08-08T14:36:03.445060: step 419, loss 0.673037.
Train: 2018-08-08T14:36:06.510209: step 420, loss 0.543819.
Test: 2018-08-08T14:36:26.739995: step 420, loss 0.552837.
Train: 2018-08-08T14:36:29.924461: step 421, loss 0.498336.
Train: 2018-08-08T14:36:33.208192: step 422, loss 0.572483.
Train: 2018-08-08T14:36:36.320467: step 423, loss 0.554695.
Train: 2018-08-08T14:36:39.395643: step 424, loss 0.472247.
Train: 2018-08-08T14:36:42.467811: step 425, loss 0.487634.
Train: 2018-08-08T14:36:45.543989: step 426, loss 0.480553.
Train: 2018-08-08T14:36:48.909939: step 427, loss 0.590151.
Train: 2018-08-08T14:36:52.044272: step 428, loss 0.568117.
Train: 2018-08-08T14:36:55.136493: step 429, loss 0.484086.
Train: 2018-08-08T14:36:58.248768: step 430, loss 0.458474.
Test: 2018-08-08T14:37:18.616921: step 430, loss 0.548916.
Train: 2018-08-08T14:37:21.735212: step 431, loss 0.553697.
Train: 2018-08-08T14:37:24.876564: step 432, loss 0.541318.
Train: 2018-08-08T14:37:28.003879: step 433, loss 0.500886.
Train: 2018-08-08T14:37:31.146233: step 434, loss 0.565549.
Train: 2018-08-08T14:37:34.478092: step 435, loss 0.528311.
Train: 2018-08-08T14:37:37.766835: step 436, loss 0.453425.
Train: 2018-08-08T14:37:41.037531: step 437, loss 0.470392.
Train: 2018-08-08T14:37:44.323267: step 438, loss 0.513231.
Train: 2018-08-08T14:37:47.612011: step 439, loss 0.609686.
Train: 2018-08-08T14:37:50.880702: step 440, loss 0.511587.
Test: 2018-08-08T14:38:11.699106: step 440, loss 0.549816.
Train: 2018-08-08T14:38:14.759242: step 441, loss 0.546083.
Train: 2018-08-08T14:38:17.840434: step 442, loss 0.478665.
Train: 2018-08-08T14:38:20.871493: step 443, loss 0.437591.
Train: 2018-08-08T14:38:23.949677: step 444, loss 0.42548.
Train: 2018-08-08T14:38:27.013824: step 445, loss 0.423154.
Train: 2018-08-08T14:38:30.078973: step 446, loss 0.521015.
Train: 2018-08-08T14:38:33.147130: step 447, loss 0.713387.
Train: 2018-08-08T14:38:36.214285: step 448, loss 0.491198.
Train: 2018-08-08T14:38:39.302496: step 449, loss 0.500261.
Train: 2018-08-08T14:38:42.366642: step 450, loss 0.45083.
Test: 2018-08-08T14:39:02.554316: step 450, loss 0.547191.
Train: 2018-08-08T14:39:05.662580: step 451, loss 0.616595.
Train: 2018-08-08T14:39:08.770844: step 452, loss 0.479459.
Train: 2018-08-08T14:39:11.844015: step 453, loss 0.512948.
Train: 2018-08-08T14:39:14.920194: step 454, loss 0.551815.
Train: 2018-08-08T14:39:18.050516: step 455, loss 0.53643.
Train: 2018-08-08T14:39:21.108647: step 456, loss 0.580972.
Train: 2018-08-08T14:39:24.147727: step 457, loss 0.533674.
Train: 2018-08-08T14:39:27.184802: step 458, loss 0.525246.
Train: 2018-08-08T14:39:30.267999: step 459, loss 0.640855.
Train: 2018-08-08T14:39:33.352199: step 460, loss 0.503814.
Test: 2018-08-08T14:39:53.552907: step 460, loss 0.551261.
Train: 2018-08-08T14:39:56.621065: step 461, loss 0.484104.
Train: 2018-08-08T14:39:59.709275: step 462, loss 0.493095.
Train: 2018-08-08T14:40:02.918497: step 463, loss 0.527764.
Train: 2018-08-08T14:40:06.021748: step 464, loss 0.50568.
Train: 2018-08-08T14:40:09.124999: step 465, loss 0.576837.
Train: 2018-08-08T14:40:12.165081: step 466, loss 0.505749.
Train: 2018-08-08T14:40:15.250284: step 467, loss 0.70661.
Train: 2018-08-08T14:40:18.336490: step 468, loss 0.443535.
Train: 2018-08-08T14:40:21.386599: step 469, loss 0.544444.
Train: 2018-08-08T14:40:24.478820: step 470, loss 0.58382.
Test: 2018-08-08T14:40:44.678526: step 470, loss 0.548961.
Train: 2018-08-08T14:40:47.752699: step 471, loss 0.589877.
Train: 2018-08-08T14:40:50.831886: step 472, loss 0.533023.
Train: 2018-08-08T14:40:53.897035: step 473, loss 0.546372.
Train: 2018-08-08T14:40:56.994270: step 474, loss 0.594603.
Train: 2018-08-08T14:41:00.071452: step 475, loss 0.505465.
Train: 2018-08-08T14:41:03.250904: step 476, loss 0.516695.
Train: 2018-08-08T14:41:06.309035: step 477, loss 0.454315.
Train: 2018-08-08T14:41:09.419305: step 478, loss 0.54648.
Train: 2018-08-08T14:41:12.497489: step 479, loss 0.47346.
Train: 2018-08-08T14:41:15.586702: step 480, loss 0.636755.
Test: 2018-08-08T14:41:35.754322: step 480, loss 0.546905.
Train: 2018-08-08T14:41:38.860581: step 481, loss 0.592893.
Train: 2018-08-08T14:41:41.924728: step 482, loss 0.437159.
Train: 2018-08-08T14:41:45.002912: step 483, loss 0.680714.
Train: 2018-08-08T14:41:48.066056: step 484, loss 0.632065.
Train: 2018-08-08T14:41:51.152261: step 485, loss 0.582317.
Train: 2018-08-08T14:41:54.278573: step 486, loss 0.578804.
Train: 2018-08-08T14:41:57.398869: step 487, loss 0.508732.
Train: 2018-08-08T14:42:00.435944: step 488, loss 0.527968.
Train: 2018-08-08T14:42:03.500091: step 489, loss 0.630883.
Train: 2018-08-08T14:42:06.572259: step 490, loss 0.542716.
Test: 2018-08-08T14:42:26.913340: step 490, loss 0.555201.
Train: 2018-08-08T14:42:29.963449: step 491, loss 0.515181.
Train: 2018-08-08T14:42:33.045644: step 492, loss 0.550849.
Train: 2018-08-08T14:42:36.237130: step 493, loss 0.564975.
Train: 2018-08-08T14:42:39.518802: step 494, loss 0.554103.
Train: 2018-08-08T14:42:42.582949: step 495, loss 0.570448.
Train: 2018-08-08T14:42:45.665143: step 496, loss 0.562504.
Train: 2018-08-08T14:42:48.789974: step 497, loss 0.579916.
Train: 2018-08-08T14:42:52.027082: step 498, loss 0.654973.
Train: 2018-08-08T14:42:55.095239: step 499, loss 0.475943.
Train: 2018-08-08T14:42:58.208517: step 500, loss 0.542951.
Test: 2018-08-08T14:43:18.551249: step 500, loss 0.548297.
Train: 2018-08-08T14:43:22.776483: step 501, loss 0.608181.
Train: 2018-08-08T14:43:25.904800: step 502, loss 0.53956.
Train: 2018-08-08T14:43:28.954909: step 503, loss 0.576496.
Train: 2018-08-08T14:43:31.984966: step 504, loss 0.5796.
Train: 2018-08-08T14:43:35.048110: step 505, loss 0.514016.
Train: 2018-08-08T14:43:38.129302: step 506, loss 0.577526.
Train: 2018-08-08T14:43:41.240574: step 507, loss 0.579906.
Train: 2018-08-08T14:43:44.287675: step 508, loss 0.483916.
Train: 2018-08-08T14:43:47.352824: step 509, loss 0.530771.
Train: 2018-08-08T14:43:50.441035: step 510, loss 0.540042.
Test: 2018-08-08T14:44:10.609658: step 510, loss 0.548016.
Train: 2018-08-08T14:44:13.822199: step 511, loss 0.531379.
Train: 2018-08-08T14:44:16.862282: step 512, loss 0.525588.
Train: 2018-08-08T14:44:19.938461: step 513, loss 0.574817.
Train: 2018-08-08T14:44:23.015642: step 514, loss 0.520027.
Train: 2018-08-08T14:44:26.121901: step 515, loss 0.559833.
Train: 2018-08-08T14:44:29.171007: step 516, loss 0.463073.
Train: 2018-08-08T14:44:32.229138: step 517, loss 0.537204.
Train: 2018-08-08T14:44:35.292282: step 518, loss 0.564521.
Train: 2018-08-08T14:44:38.341389: step 519, loss 0.499155.
Train: 2018-08-08T14:44:41.370442: step 520, loss 0.476145.
Test: 2018-08-08T14:45:01.522020: step 520, loss 0.547442.
Train: 2018-08-08T14:45:04.579148: step 521, loss 0.430466.
Train: 2018-08-08T14:45:07.643295: step 522, loss 0.458997.
Train: 2018-08-08T14:45:10.732508: step 523, loss 0.712234.
Train: 2018-08-08T14:45:13.805679: step 524, loss 0.753225.
Train: 2018-08-08T14:45:16.882860: step 525, loss 0.54908.
Train: 2018-08-08T14:45:19.953023: step 526, loss 0.605988.
Train: 2018-08-08T14:45:23.041233: step 527, loss 0.552419.
Train: 2018-08-08T14:45:26.184591: step 528, loss 0.542568.
Train: 2018-08-08T14:45:29.235704: step 529, loss 0.53025.
Train: 2018-08-08T14:45:32.342964: step 530, loss 0.600042.
Test: 2018-08-08T14:45:52.505571: step 530, loss 0.570148.
Train: 2018-08-08T14:45:55.573728: step 531, loss 0.543821.
Train: 2018-08-08T14:45:58.662942: step 532, loss 0.520055.
Train: 2018-08-08T14:46:01.782235: step 533, loss 0.46293.
Train: 2018-08-08T14:46:04.850392: step 534, loss 0.531407.
Train: 2018-08-08T14:46:07.934593: step 535, loss 0.563253.
Train: 2018-08-08T14:46:11.027817: step 536, loss 0.520274.
Train: 2018-08-08T14:46:14.120038: step 537, loss 0.551833.
Train: 2018-08-08T14:46:17.205240: step 538, loss 0.445722.
Train: 2018-08-08T14:46:20.267382: step 539, loss 0.555076.
Train: 2018-08-08T14:46:23.352585: step 540, loss 0.720855.
Test: 2018-08-08T14:46:43.507170: step 540, loss 0.548001.
Train: 2018-08-08T14:46:46.597386: step 541, loss 0.59119.
Train: 2018-08-08T14:46:49.681586: step 542, loss 0.575184.
Train: 2018-08-08T14:46:52.746736: step 543, loss 0.518788.
Train: 2018-08-08T14:46:55.815896: step 544, loss 0.509635.
Train: 2018-08-08T14:46:58.911125: step 545, loss 0.524704.
Train: 2018-08-08T14:47:02.015378: step 546, loss 0.525509.
Train: 2018-08-08T14:47:05.082533: step 547, loss 0.568742.
Train: 2018-08-08T14:47:08.161720: step 548, loss 0.66918.
Train: 2018-08-08T14:47:11.256949: step 549, loss 0.567735.
Train: 2018-08-08T14:47:14.355186: step 550, loss 0.584363.
Test: 2018-08-08T14:47:34.497741: step 550, loss 0.556773.
Train: 2018-08-08T14:47:37.528799: step 551, loss 0.58478.
Train: 2018-08-08T14:47:40.643079: step 552, loss 0.596384.
Train: 2018-08-08T14:47:43.719257: step 553, loss 0.507699.
Train: 2018-08-08T14:47:46.800449: step 554, loss 0.550252.
Train: 2018-08-08T14:47:49.908713: step 555, loss 0.64213.
Train: 2018-08-08T14:47:53.001938: step 556, loss 0.535177.
Train: 2018-08-08T14:47:56.101178: step 557, loss 0.558989.
Train: 2018-08-08T14:47:59.185378: step 558, loss 0.65452.
Train: 2018-08-08T14:48:02.229471: step 559, loss 0.628115.
Train: 2018-08-08T14:48:05.287602: step 560, loss 0.471863.
Test: 2018-08-08T14:48:25.530422: step 560, loss 0.555624.
Train: 2018-08-08T14:48:28.737951: step 561, loss 0.531033.
Train: 2018-08-08T14:48:31.796080: step 562, loss 0.551546.
Train: 2018-08-08T14:48:34.963502: step 563, loss 0.521443.
Train: 2018-08-08T14:48:38.088811: step 564, loss 0.60964.
Train: 2018-08-08T14:48:41.164990: step 565, loss 0.552314.
Train: 2018-08-08T14:48:44.258214: step 566, loss 0.510261.
Train: 2018-08-08T14:48:47.320355: step 567, loss 0.528445.
Train: 2018-08-08T14:48:50.399542: step 568, loss 0.44455.
Train: 2018-08-08T14:48:53.500787: step 569, loss 0.623088.
Train: 2018-08-08T14:48:56.601030: step 570, loss 0.547633.
Test: 2018-08-08T14:49:17.466506: step 570, loss 0.54632.
Train: 2018-08-08T14:49:20.653980: step 571, loss 0.662311.
Train: 2018-08-08T14:49:23.760239: step 572, loss 0.558071.
Train: 2018-08-08T14:49:26.885548: step 573, loss 0.640386.
Train: 2018-08-08T14:49:30.022889: step 574, loss 0.585164.
Train: 2018-08-08T14:49:33.143185: step 575, loss 0.604066.
Train: 2018-08-08T14:49:36.333668: step 576, loss 0.528182.
Train: 2018-08-08T14:49:39.476760: step 577, loss 0.520997.
Train: 2018-08-08T14:49:42.690304: step 578, loss 0.56574.
Train: 2018-08-08T14:49:45.793555: step 579, loss 0.576779.
Train: 2018-08-08T14:49:48.877755: step 580, loss 0.481657.
Test: 2018-08-08T14:50:09.184234: step 580, loss 0.554701.
Train: 2018-08-08T14:50:12.239355: step 581, loss 0.630653.
Train: 2018-08-08T14:50:15.332580: step 582, loss 0.548136.
Train: 2018-08-08T14:50:18.399734: step 583, loss 0.540388.
Train: 2018-08-08T14:50:21.432798: step 584, loss 0.515297.
Train: 2018-08-08T14:50:24.529031: step 585, loss 0.512875.
Train: 2018-08-08T14:50:27.629273: step 586, loss 0.63921.
Train: 2018-08-08T14:50:30.702444: step 587, loss 0.551815.
Train: 2018-08-08T14:50:33.778623: step 588, loss 0.438208.
Train: 2018-08-08T14:50:36.850790: step 589, loss 0.541762.
Train: 2018-08-08T14:50:39.899897: step 590, loss 0.438232.
Test: 2018-08-08T14:51:00.040446: step 590, loss 0.549892.
Train: 2018-08-08T14:51:03.131664: step 591, loss 0.458583.
Train: 2018-08-08T14:51:06.183779: step 592, loss 0.512789.
Train: 2018-08-08T14:51:09.219851: step 593, loss 0.637987.
Train: 2018-08-08T14:51:12.264947: step 594, loss 0.623565.
Train: 2018-08-08T14:51:15.356166: step 595, loss 0.554528.
Train: 2018-08-08T14:51:18.471984: step 596, loss 0.595524.
Train: 2018-08-08T14:51:21.546158: step 597, loss 0.527366.
Train: 2018-08-08T14:51:24.666454: step 598, loss 0.570935.
Train: 2018-08-08T14:51:27.776723: step 599, loss 0.544698.
Train: 2018-08-08T14:51:30.872955: step 600, loss 0.516993.
Test: 2018-08-08T14:51:51.067647: step 600, loss 0.555778.
Train: 2018-08-08T14:51:55.223697: step 601, loss 0.555907.
Train: 2018-08-08T14:51:58.129422: step 602, loss 0.533583.
Train: 2018-08-08T14:52:01.204599: step 603, loss 0.417017.
Train: 2018-08-08T14:52:04.282782: step 604, loss 0.458734.
Train: 2018-08-08T14:52:07.332892: step 605, loss 0.47424.
Train: 2018-08-08T14:52:10.473241: step 606, loss 0.463219.
Train: 2018-08-08T14:52:13.514327: step 607, loss 0.570046.
Train: 2018-08-08T14:52:16.611561: step 608, loss 0.539812.
Train: 2018-08-08T14:52:19.687740: step 609, loss 0.443363.
Train: 2018-08-08T14:52:22.739855: step 610, loss 0.495435.
Test: 2018-08-08T14:52:43.292691: step 610, loss 0.547181.
Train: 2018-08-08T14:52:46.472140: step 611, loss 0.403358.
Train: 2018-08-08T14:52:49.677663: step 612, loss 0.56204.
Train: 2018-08-08T14:52:52.877914: step 613, loss 0.487269.
Train: 2018-08-08T14:52:56.021271: step 614, loss 0.512885.
Train: 2018-08-08T14:52:59.282169: step 615, loss 0.54917.
Train: 2018-08-08T14:53:02.475659: step 616, loss 0.543816.
Train: 2018-08-08T14:53:05.586931: step 617, loss 0.543296.
Train: 2018-08-08T14:53:08.670129: step 618, loss 0.598312.
Train: 2018-08-08T14:53:11.735278: step 619, loss 0.508223.
Train: 2018-08-08T14:53:14.818475: step 620, loss 0.485508.
Test: 2018-08-08T14:53:35.260997: step 620, loss 0.551206.
Train: 2018-08-08T14:53:38.342189: step 621, loss 0.597702.
Train: 2018-08-08T14:53:41.518096: step 622, loss 0.625308.
Train: 2018-08-08T14:53:44.686238: step 623, loss 0.564405.
Train: 2018-08-08T14:53:47.969937: step 624, loss 0.601041.
Train: 2018-08-08T14:53:51.184752: step 625, loss 0.47586.
Train: 2018-08-08T14:53:54.450215: step 626, loss 0.543709.
Train: 2018-08-08T14:53:57.761674: step 627, loss 0.573359.
Train: 2018-08-08T14:54:01.157472: step 628, loss 0.493881.
Train: 2018-08-08T14:54:04.490333: step 629, loss 0.575395.
Train: 2018-08-08T14:54:07.757325: step 630, loss 0.429606.
Test: 2018-08-08T14:54:28.413488: step 630, loss 0.551561.
Train: 2018-08-08T14:54:31.649090: step 631, loss 0.478026.
Train: 2018-08-08T14:54:34.757354: step 632, loss 0.68672.
Train: 2018-08-08T14:54:37.854589: step 633, loss 0.518976.
Train: 2018-08-08T14:54:40.954832: step 634, loss 0.5373.
Train: 2018-08-08T14:54:44.019981: step 635, loss 0.448345.
Train: 2018-08-08T14:54:47.125237: step 636, loss 0.5562.
Train: 2018-08-08T14:54:50.231496: step 637, loss 0.61025.
Train: 2018-08-08T14:54:53.352795: step 638, loss 0.436118.
Train: 2018-08-08T14:54:56.465069: step 639, loss 0.48619.
Train: 2018-08-08T14:54:59.577344: step 640, loss 0.667747.
Test: 2018-08-08T14:55:20.020477: step 640, loss 0.546271.
Train: 2018-08-08T14:55:23.105679: step 641, loss 0.557325.
Train: 2018-08-08T14:55:26.246029: step 642, loss 0.58322.
Train: 2018-08-08T14:55:29.342261: step 643, loss 0.601363.
Train: 2018-08-08T14:55:32.469575: step 644, loss 0.568852.
Train: 2018-08-08T14:55:35.588869: step 645, loss 0.536672.
Train: 2018-08-08T14:55:38.701143: step 646, loss 0.565825.
Train: 2018-08-08T14:55:41.799381: step 647, loss 0.535169.
Train: 2018-08-08T14:55:44.896615: step 648, loss 0.61565.
Train: 2018-08-08T14:55:47.977808: step 649, loss 0.545514.
Train: 2018-08-08T14:55:51.072034: step 650, loss 0.554582.
Test: 2018-08-08T14:56:11.398076: step 650, loss 0.554723.
Train: 2018-08-08T14:56:14.466233: step 651, loss 0.467727.
Train: 2018-08-08T14:56:17.580513: step 652, loss 0.573355.
Train: 2018-08-08T14:56:20.706825: step 653, loss 0.514825.
Train: 2018-08-08T14:56:23.801052: step 654, loss 0.590925.
Train: 2018-08-08T14:56:27.041863: step 655, loss 0.494964.
Train: 2018-08-08T14:56:30.209285: step 656, loss 0.508699.
Train: 2018-08-08T14:56:33.305517: step 657, loss 0.680456.
Train: 2018-08-08T14:56:36.400746: step 658, loss 0.541054.
Train: 2018-08-08T14:56:39.483944: step 659, loss 0.556577.
Train: 2018-08-08T14:56:42.611258: step 660, loss 0.573623.
Test: 2018-08-08T14:57:02.978409: step 660, loss 0.55201.
Train: 2018-08-08T14:57:06.107729: step 661, loss 0.598246.
Train: 2018-08-08T14:57:09.291959: step 662, loss 0.559698.
Train: 2018-08-08T14:57:12.387188: step 663, loss 0.500805.
Train: 2018-08-08T14:57:15.478407: step 664, loss 0.617901.
Train: 2018-08-08T14:57:18.574639: step 665, loss 0.544631.
Train: 2018-08-08T14:57:21.683905: step 666, loss 0.558359.
Train: 2018-08-08T14:57:24.772116: step 667, loss 0.465325.
Train: 2018-08-08T14:57:27.867346: step 668, loss 0.475391.
Train: 2018-08-08T14:57:30.984634: step 669, loss 0.436925.
Train: 2018-08-08T14:57:34.078860: step 670, loss 0.506874.
Test: 2018-08-08T14:57:54.356774: step 670, loss 0.559404.
Train: 2018-08-08T14:57:57.465038: step 671, loss 0.463171.
Train: 2018-08-08T14:58:00.559265: step 672, loss 0.6602.
Train: 2018-08-08T14:58:03.667529: step 673, loss 0.566413.
Train: 2018-08-08T14:58:06.784817: step 674, loss 0.46204.
Train: 2018-08-08T14:58:09.865006: step 675, loss 0.493836.
Train: 2018-08-08T14:58:12.973270: step 676, loss 0.572525.
Train: 2018-08-08T14:58:16.070505: step 677, loss 0.585854.
Train: 2018-08-08T14:58:19.205840: step 678, loss 0.539283.
Train: 2018-08-08T14:58:22.355214: step 679, loss 0.523584.
Train: 2018-08-08T14:58:25.465483: step 680, loss 0.468639.
Test: 2018-08-08T14:58:45.713325: step 680, loss 0.555368.
Train: 2018-08-08T14:58:48.801527: step 681, loss 0.552742.
Train: 2018-08-08T14:58:51.902773: step 682, loss 0.556541.
Train: 2018-08-08T14:58:55.002012: step 683, loss 0.574826.
Train: 2018-08-08T14:58:58.092229: step 684, loss 0.571222.
Train: 2018-08-08T14:59:01.198487: step 685, loss 0.496606.
Train: 2018-08-08T14:59:04.292714: step 686, loss 0.543049.
Train: 2018-08-08T14:59:07.411005: step 687, loss 0.509541.
Train: 2018-08-08T14:59:10.537317: step 688, loss 0.481745.
Train: 2018-08-08T14:59:13.632546: step 689, loss 0.359014.
Train: 2018-08-08T14:59:16.724767: step 690, loss 0.530471.
Test: 2018-08-08T14:59:37.150073: step 690, loss 0.55258.
Train: 2018-08-08T14:59:40.268363: step 691, loss 0.493842.
Train: 2018-08-08T14:59:43.426761: step 692, loss 0.603944.
Train: 2018-08-08T14:59:46.633286: step 693, loss 0.519842.
Train: 2018-08-08T14:59:49.766617: step 694, loss 0.512851.
Train: 2018-08-08T14:59:53.135573: step 695, loss 0.484259.
Train: 2018-08-08T14:59:56.277928: step 696, loss 0.476898.
Train: 2018-08-08T14:59:59.480445: step 697, loss 0.541885.
Train: 2018-08-08T15:00:02.723454: step 698, loss 0.434916.
Train: 2018-08-08T15:00:05.821691: step 699, loss 0.497035.
Train: 2018-08-08T15:00:08.891854: step 700, loss 0.596323.
Test: 2018-08-08T15:00:29.165757: step 700, loss 0.546459.
Train: 2018-08-08T15:00:33.444132: step 701, loss 0.512381.
Train: 2018-08-08T15:00:36.684748: step 702, loss 0.404109.
Train: 2018-08-08T15:00:39.776969: step 703, loss 0.553026.
Train: 2018-08-08T15:00:42.877212: step 704, loss 0.614989.
Train: 2018-08-08T15:00:45.970436: step 705, loss 0.499526.
Train: 2018-08-08T15:00:49.078700: step 706, loss 0.583998.
Train: 2018-08-08T15:00:52.148862: step 707, loss 0.610649.
Train: 2018-08-08T15:00:55.243089: step 708, loss 0.488714.
Train: 2018-08-08T15:00:58.328292: step 709, loss 0.487747.
Train: 2018-08-08T15:01:01.411489: step 710, loss 0.463601.
Test: 2018-08-08T15:01:21.610192: step 710, loss 0.548491.
Train: 2018-08-08T15:01:24.707427: step 711, loss 0.537838.
Train: 2018-08-08T15:01:27.804662: step 712, loss 0.563904.
Train: 2018-08-08T15:01:30.932979: step 713, loss 0.521074.
Train: 2018-08-08T15:01:34.018181: step 714, loss 0.626975.
Train: 2018-08-08T15:01:37.119427: step 715, loss 0.555854.
Train: 2018-08-08T15:01:40.214656: step 716, loss 0.57873.
Train: 2018-08-08T15:01:43.307880: step 717, loss 0.58474.
Train: 2018-08-08T15:01:46.384059: step 718, loss 0.656523.
Train: 2018-08-08T15:01:49.471267: step 719, loss 0.459155.
Train: 2018-08-08T15:01:52.562486: step 720, loss 0.593186.
Test: 2018-08-08T15:02:12.805306: step 720, loss 0.553247.
Train: 2018-08-08T15:02:15.915575: step 721, loss 0.56861.
Train: 2018-08-08T15:02:19.008799: step 722, loss 0.672847.
Train: 2018-08-08T15:02:22.105031: step 723, loss 0.55065.
Train: 2018-08-08T15:02:25.226330: step 724, loss 0.566401.
Train: 2018-08-08T15:02:28.296492: step 725, loss 0.627144.
Train: 2018-08-08T15:02:31.381695: step 726, loss 0.51034.
Train: 2018-08-08T15:02:34.454866: step 727, loss 0.554818.
Train: 2018-08-08T15:02:37.561125: step 728, loss 0.516748.
Train: 2018-08-08T15:02:40.665378: step 729, loss 0.581318.
Train: 2018-08-08T15:02:43.772640: step 730, loss 0.483081.
Test: 2018-08-08T15:03:04.151822: step 730, loss 0.546213.
Train: 2018-08-08T15:03:07.231009: step 731, loss 0.485239.
Train: 2018-08-08T15:03:10.340275: step 732, loss 0.619298.
Train: 2018-08-08T15:03:13.428486: step 733, loss 0.445637.
Train: 2018-08-08T15:03:16.530734: step 734, loss 0.67033.
Train: 2018-08-08T15:03:19.623958: step 735, loss 0.579187.
Train: 2018-08-08T15:03:22.717182: step 736, loss 0.621345.
Train: 2018-08-08T15:03:25.864550: step 737, loss 0.599338.
Train: 2018-08-08T15:03:28.961785: step 738, loss 0.603205.
Train: 2018-08-08T15:03:32.041974: step 739, loss 0.588567.
Train: 2018-08-08T15:03:35.136201: step 740, loss 0.564904.
Test: 2018-08-08T15:03:55.708898: step 740, loss 0.571852.
Train: 2018-08-08T15:03:58.906399: step 741, loss 0.536719.
Train: 2018-08-08T15:04:02.028701: step 742, loss 0.604553.
Train: 2018-08-08T15:04:05.115908: step 743, loss 0.54551.
Train: 2018-08-08T15:04:08.238210: step 744, loss 0.51483.
Train: 2018-08-08T15:04:11.346474: step 745, loss 0.486426.
Train: 2018-08-08T15:04:14.434685: step 746, loss 0.539091.
Train: 2018-08-08T15:04:17.540943: step 747, loss 0.42894.
Train: 2018-08-08T15:04:20.656226: step 748, loss 0.475308.
Train: 2018-08-08T15:04:23.791562: step 749, loss 0.572479.
Train: 2018-08-08T15:04:26.877767: step 750, loss 0.697292.
Test: 2018-08-08T15:04:47.064438: step 750, loss 0.550731.
Train: 2018-08-08T15:04:50.212809: step 751, loss 0.453912.
Train: 2018-08-08T15:04:53.337115: step 752, loss 0.42698.
Train: 2018-08-08T15:04:56.423321: step 753, loss 0.528168.
Train: 2018-08-08T15:04:59.499500: step 754, loss 0.551176.
Train: 2018-08-08T15:05:02.610772: step 755, loss 0.449036.
Train: 2018-08-08T15:05:05.741094: step 756, loss 0.577093.
Train: 2018-08-08T15:05:08.851363: step 757, loss 0.510347.
Train: 2018-08-08T15:05:11.952609: step 758, loss 0.492154.
Train: 2018-08-08T15:05:15.106996: step 759, loss 0.465422.
Train: 2018-08-08T15:05:18.232571: step 760, loss 0.472959.
Test: 2018-08-08T15:05:38.510484: step 760, loss 0.547479.
Train: 2018-08-08T15:05:41.621756: step 761, loss 0.62363.
Train: 2018-08-08T15:05:44.738041: step 762, loss 0.580448.
Train: 2018-08-08T15:05:47.843297: step 763, loss 0.615877.
Train: 2018-08-08T15:05:50.967604: step 764, loss 0.521448.
Train: 2018-08-08T15:05:54.042780: step 765, loss 0.532452.
Train: 2018-08-08T15:05:57.153049: step 766, loss 0.544909.
Train: 2018-08-08T15:06:00.227223: step 767, loss 0.463762.
Train: 2018-08-08T15:06:03.318441: step 768, loss 0.561283.
Train: 2018-08-08T15:06:06.405649: step 769, loss 0.482232.
Train: 2018-08-08T15:06:09.514916: step 770, loss 0.46839.
Test: 2018-08-08T15:06:29.723646: step 770, loss 0.547373.
Train: 2018-08-08T15:06:32.798822: step 771, loss 0.409607.
Train: 2018-08-08T15:06:35.891043: step 772, loss 0.439094.
Train: 2018-08-08T15:06:38.985270: step 773, loss 0.56396.
Train: 2018-08-08T15:06:42.060446: step 774, loss 0.549061.
Train: 2018-08-08T15:06:45.245915: step 775, loss 0.683018.
Train: 2018-08-08T15:06:48.434392: step 776, loss 0.465028.
Train: 2018-08-08T15:06:51.518592: step 777, loss 0.459967.
Train: 2018-08-08T15:06:54.634877: step 778, loss 0.661721.
Train: 2018-08-08T15:06:57.729104: step 779, loss 0.489761.
Train: 2018-08-08T15:07:00.828344: step 780, loss 0.549872.
Test: 2018-08-08T15:07:21.601575: step 780, loss 0.555863.
Train: 2018-08-08T15:07:24.755961: step 781, loss 0.52274.
Train: 2018-08-08T15:07:27.877260: step 782, loss 0.55285.
Train: 2018-08-08T15:07:30.967476: step 783, loss 0.536637.
Train: 2018-08-08T15:07:34.068721: step 784, loss 0.580156.
Train: 2018-08-08T15:07:37.186009: step 785, loss 0.607412.
Train: 2018-08-08T15:07:40.286252: step 786, loss 0.540563.
Train: 2018-08-08T15:07:43.383487: step 787, loss 0.586605.
Train: 2018-08-08T15:07:46.462673: step 788, loss 0.569204.
Train: 2018-08-08T15:07:49.558905: step 789, loss 0.51436.
Train: 2018-08-08T15:07:52.644108: step 790, loss 0.431895.
Test: 2018-08-08T15:08:12.853840: step 790, loss 0.549527.
Train: 2018-08-08T15:08:15.941048: step 791, loss 0.531438.
Train: 2018-08-08T15:08:19.029259: step 792, loss 0.467126.
Train: 2018-08-08T15:08:22.122483: step 793, loss 0.524176.
Train: 2018-08-08T15:08:25.217712: step 794, loss 0.70189.
Train: 2018-08-08T15:08:28.316952: step 795, loss 0.526396.
Train: 2018-08-08T15:08:31.408171: step 796, loss 0.603892.
Train: 2018-08-08T15:08:34.498387: step 797, loss 0.581276.
Train: 2018-08-08T15:08:37.590608: step 798, loss 0.573835.
Train: 2018-08-08T15:08:40.689848: step 799, loss 0.553899.
Train: 2018-08-08T15:08:43.779062: step 800, loss 0.535132.
Test: 2018-08-08T15:09:03.992805: step 800, loss 0.558517.
Train: 2018-08-08T15:09:08.250124: step 801, loss 0.566087.
Train: 2018-08-08T15:09:11.390473: step 802, loss 0.501278.
Train: 2018-08-08T15:09:14.496732: step 803, loss 0.567852.
Train: 2018-08-08T15:09:17.599982: step 804, loss 0.567472.
Train: 2018-08-08T15:09:20.700225: step 805, loss 0.526783.
Train: 2018-08-08T15:09:23.790441: step 806, loss 0.589475.
Train: 2018-08-08T15:09:26.878652: step 807, loss 0.482143.
Train: 2018-08-08T15:09:29.985913: step 808, loss 0.605062.
Train: 2018-08-08T15:09:33.067105: step 809, loss 0.541837.
Train: 2018-08-08T15:09:36.161332: step 810, loss 0.45322.
Test: 2018-08-08T15:09:56.339981: step 810, loss nan.
Train: 2018-08-08T15:09:59.459274: step 811, loss 0.463324.
Train: 2018-08-08T15:10:02.701896: step 812, loss 0.498906.
Train: 2018-08-08T15:10:05.782086: step 813, loss 0.684387.
Train: 2018-08-08T15:10:08.889346: step 814, loss 0.471518.
Train: 2018-08-08T15:10:11.967531: step 815, loss 0.402141.
Train: 2018-08-08T15:10:15.068776: step 816, loss 0.516321.
Train: 2018-08-08T15:10:18.115877: step 817, loss 0.704794.
Train: 2018-08-08T15:10:21.236173: step 818, loss 0.620948.
Train: 2018-08-08T15:10:24.357472: step 819, loss 0.669557.
Train: 2018-08-08T15:10:27.437661: step 820, loss 0.541103.
Test: 2018-08-08T15:10:47.655415: step 820, loss nan.
Train: 2018-08-08T15:10:50.744628: step 821, loss 0.612727.
Train: 2018-08-08T15:10:53.843868: step 822, loss 0.571926.
Train: 2018-08-08T15:10:56.927065: step 823, loss 0.586778.
Train: 2018-08-08T15:11:00.039340: step 824, loss 0.581712.
Train: 2018-08-08T15:11:03.146602: step 825, loss 0.560865.
Train: 2018-08-08T15:11:06.227794: step 826, loss 0.575327.
Train: 2018-08-08T15:11:09.316004: step 827, loss 0.515879.
Train: 2018-08-08T15:11:12.418252: step 828, loss 0.543669.
Train: 2018-08-08T15:11:15.510474: step 829, loss 0.52553.
Train: 2018-08-08T15:11:18.610716: step 830, loss 0.52152.
Test: 2018-08-08T15:11:38.961824: step 830, loss nan.
Train: 2018-08-08T15:11:42.057054: step 831, loss 0.48449.
Train: 2018-08-08T15:11:45.135238: step 832, loss 0.564211.
Train: 2018-08-08T15:11:48.239491: step 833, loss 0.590229.
Train: 2018-08-08T15:11:51.322688: step 834, loss 0.593725.
Train: 2018-08-08T15:11:54.414910: step 835, loss 0.674709.
Train: 2018-08-08T15:11:57.503120: step 836, loss 0.526452.
Train: 2018-08-08T15:12:00.580302: step 837, loss 0.609219.
Train: 2018-08-08T15:12:03.673526: step 838, loss 0.449384.
Train: 2018-08-08T15:12:06.760734: step 839, loss 0.586244.
Train: 2018-08-08T15:12:09.840923: step 840, loss 0.456866.
Test: 2018-08-08T15:12:30.096778: step 840, loss nan.
Train: 2018-08-08T15:12:33.222088: step 841, loss 0.517799.
Train: 2018-08-08T15:12:36.333359: step 842, loss 0.594783.
Train: 2018-08-08T15:12:39.426583: step 843, loss 0.518387.
Train: 2018-08-08T15:12:42.501759: step 844, loss 0.58138.
Train: 2018-08-08T15:12:45.602002: step 845, loss 0.506684.
Train: 2018-08-08T15:12:48.694223: step 846, loss 0.539912.
Train: 2018-08-08T15:12:51.776418: step 847, loss 0.502889.
Train: 2018-08-08T15:12:54.878666: step 848, loss 0.491391.
Train: 2018-08-08T15:12:57.968882: step 849, loss 0.542988.
Train: 2018-08-08T15:13:01.084165: step 850, loss 0.529821.
Test: 2018-08-08T15:13:21.579657: step 850, loss nan.
Train: 2018-08-08T15:13:24.750086: step 851, loss 0.442155.
Train: 2018-08-08T15:13:27.924526: step 852, loss 0.433892.
Train: 2018-08-08T15:13:31.042817: step 853, loss 0.535578.
Train: 2018-08-08T15:13:34.205225: step 854, loss 0.479547.
Train: 2018-08-08T15:13:37.348582: step 855, loss 0.476529.
Train: 2018-08-08T15:13:40.489934: step 856, loss 0.616804.
Train: 2018-08-08T15:13:43.602209: step 857, loss 0.51673.
Train: 2018-08-08T15:13:46.707465: step 858, loss 0.496033.
Train: 2018-08-08T15:13:49.818737: step 859, loss 0.461342.
Train: 2018-08-08T15:13:52.919982: step 860, loss 0.528861.
