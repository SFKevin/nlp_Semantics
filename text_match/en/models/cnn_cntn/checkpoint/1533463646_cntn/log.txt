Train: 2018-08-05T18:07:30.584251: step 1, loss 0.62178.
Train: 2018-08-05T18:07:30.787358: step 2, loss 4.53321.
Train: 2018-08-05T18:07:31.006027: step 3, loss 4.78506.
Train: 2018-08-05T18:07:31.209104: step 4, loss 4.28137.
Train: 2018-08-05T18:07:31.396590: step 5, loss 3.65176.
Train: 2018-08-05T18:07:31.584046: step 6, loss 3.9036.
Train: 2018-08-05T18:07:31.818366: step 7, loss 3.65176.
Train: 2018-08-05T18:07:32.021444: step 8, loss 3.9036.
Train: 2018-08-05T18:07:32.208900: step 9, loss 3.77768.
Train: 2018-08-05T18:07:32.396356: step 10, loss 3.14807.
Test: 2018-08-05T18:07:33.536714: step 10, loss 3.81632.
Train: 2018-08-05T18:07:33.739792: step 11, loss 4.78506.
Train: 2018-08-05T18:07:33.942869: step 12, loss 3.65176.
Train: 2018-08-05T18:07:34.130326: step 13, loss 3.77768.
Train: 2018-08-05T18:07:34.333373: step 14, loss 3.65176.
Train: 2018-08-05T18:07:34.520860: step 15, loss 3.9033.
Train: 2018-08-05T18:07:34.739527: step 16, loss 1.25005.
Train: 2018-08-05T18:07:34.942635: step 17, loss 1.80766.
Train: 2018-08-05T18:07:35.130091: step 18, loss 1.19467.
Train: 2018-08-05T18:07:35.333168: step 19, loss 0.482451.
Train: 2018-08-05T18:07:35.536245: step 20, loss 1.29557.
Test: 2018-08-05T18:07:36.582876: step 20, loss 0.556669.
Train: 2018-08-05T18:07:36.801574: step 21, loss 0.590034.
Train: 2018-08-05T18:07:37.004652: step 22, loss 0.556151.
Train: 2018-08-05T18:07:37.207729: step 23, loss 0.57453.
Train: 2018-08-05T18:07:37.410807: step 24, loss 0.58015.
Train: 2018-08-05T18:07:37.613885: step 25, loss 0.601438.
Train: 2018-08-05T18:07:37.832583: step 26, loss 0.545502.
Train: 2018-08-05T18:07:38.020008: step 27, loss 0.870961.
Train: 2018-08-05T18:07:38.207495: step 28, loss 0.590492.
Train: 2018-08-05T18:07:38.410573: step 29, loss 0.617266.
Train: 2018-08-05T18:07:38.629272: step 30, loss 0.642669.
Test: 2018-08-05T18:07:39.675872: step 30, loss 0.610298.
Train: 2018-08-05T18:07:39.878981: step 31, loss 0.618844.
Train: 2018-08-05T18:07:40.082056: step 32, loss 0.611179.
Train: 2018-08-05T18:07:40.285105: step 33, loss 0.525011.
Train: 2018-08-05T18:07:40.503833: step 34, loss 0.62743.
Train: 2018-08-05T18:07:40.722532: step 35, loss 0.621175.
Train: 2018-08-05T18:07:40.925609: step 36, loss 0.593322.
Train: 2018-08-05T18:07:41.113066: step 37, loss 0.617621.
Train: 2018-08-05T18:07:41.316113: step 38, loss 0.511707.
Train: 2018-08-05T18:07:41.534841: step 39, loss 0.548661.
Train: 2018-08-05T18:07:41.737888: step 40, loss 0.530763.
Test: 2018-08-05T18:07:42.784519: step 40, loss 0.551.
Train: 2018-08-05T18:07:42.987596: step 41, loss 0.515255.
Train: 2018-08-05T18:07:43.190674: step 42, loss 0.562499.
Train: 2018-08-05T18:07:43.409402: step 43, loss 0.54826.
Train: 2018-08-05T18:07:43.628101: step 44, loss 0.498474.
Train: 2018-08-05T18:07:43.831149: step 45, loss 0.480677.
Train: 2018-08-05T18:07:44.034256: step 46, loss 0.592823.
Train: 2018-08-05T18:07:44.268576: step 47, loss 0.531666.
Train: 2018-08-05T18:07:44.487275: step 48, loss 0.53957.
Train: 2018-08-05T18:07:44.690323: step 49, loss 0.602998.
Train: 2018-08-05T18:07:44.893429: step 50, loss 0.57108.
Test: 2018-08-05T18:07:45.940030: step 50, loss 0.554886.
Train: 2018-08-05T18:07:46.143136: step 51, loss 0.516875.
Train: 2018-08-05T18:07:46.361836: step 52, loss 0.538833.
Train: 2018-08-05T18:07:46.564913: step 53, loss 0.517015.
Train: 2018-08-05T18:07:46.767991: step 54, loss 0.562289.
Train: 2018-08-05T18:07:46.986690: step 55, loss 0.629656.
Train: 2018-08-05T18:07:47.205388: step 56, loss 0.500501.
Train: 2018-08-05T18:07:47.424086: step 57, loss 0.586831.
Train: 2018-08-05T18:07:47.627165: step 58, loss 0.513454.
Train: 2018-08-05T18:07:47.845863: step 59, loss 0.598458.
Train: 2018-08-05T18:07:48.064532: step 60, loss 0.496547.
Test: 2018-08-05T18:07:49.111162: step 60, loss 0.55148.
Train: 2018-08-05T18:07:49.329860: step 61, loss 0.60093.
Train: 2018-08-05T18:07:49.517346: step 62, loss 0.582333.
Train: 2018-08-05T18:07:49.736046: step 63, loss 0.554208.
Train: 2018-08-05T18:07:49.954744: step 64, loss 0.553624.
Train: 2018-08-05T18:07:50.173443: step 65, loss 0.553751.
Train: 2018-08-05T18:07:50.360900: step 66, loss 0.570682.
Train: 2018-08-05T18:07:50.579567: step 67, loss 0.565097.
Train: 2018-08-05T18:07:50.798298: step 68, loss 0.62856.
Train: 2018-08-05T18:07:51.001375: step 69, loss 0.575108.
Train: 2018-08-05T18:07:51.204452: step 70, loss 0.527581.
Test: 2018-08-05T18:07:52.251051: step 70, loss 0.554095.
Train: 2018-08-05T18:07:52.469751: step 71, loss 0.615475.
Train: 2018-08-05T18:07:52.704070: step 72, loss 0.548011.
Train: 2018-08-05T18:07:52.907178: step 73, loss 0.544522.
Train: 2018-08-05T18:07:53.110256: step 74, loss 0.626787.
Train: 2018-08-05T18:07:53.328954: step 75, loss 0.65842.
Train: 2018-08-05T18:07:53.563275: step 76, loss 0.609857.
Train: 2018-08-05T18:07:53.766352: step 77, loss 0.605886.
Train: 2018-08-05T18:07:53.985051: step 78, loss 0.536344.
Train: 2018-08-05T18:07:54.203720: step 79, loss 0.569235.
Train: 2018-08-05T18:07:54.422448: step 80, loss 0.533485.
Test: 2018-08-05T18:07:55.469048: step 80, loss 0.550226.
Train: 2018-08-05T18:07:55.687778: step 81, loss 0.605345.
Train: 2018-08-05T18:07:55.890854: step 82, loss 0.593677.
Train: 2018-08-05T18:07:56.109559: step 83, loss 0.581965.
Train: 2018-08-05T18:07:56.328252: step 84, loss 0.56047.
Train: 2018-08-05T18:07:56.531329: step 85, loss 0.562676.
Train: 2018-08-05T18:07:56.749998: step 86, loss 0.525565.
Train: 2018-08-05T18:07:56.968727: step 87, loss 0.622371.
Train: 2018-08-05T18:07:57.171804: step 88, loss 0.578958.
Train: 2018-08-05T18:07:57.390503: step 89, loss 0.556632.
Train: 2018-08-05T18:07:57.609203: step 90, loss 0.569897.
Test: 2018-08-05T18:07:58.655831: step 90, loss 0.554546.
Train: 2018-08-05T18:07:58.874502: step 91, loss 0.608311.
Train: 2018-08-05T18:07:59.077579: step 92, loss 0.543807.
Train: 2018-08-05T18:07:59.296307: step 93, loss 0.611752.
Train: 2018-08-05T18:07:59.530598: step 94, loss 0.676138.
Train: 2018-08-05T18:07:59.749296: step 95, loss 0.527679.
Train: 2018-08-05T18:07:59.968025: step 96, loss 0.561089.
Train: 2018-08-05T18:08:00.186723: step 97, loss 0.579829.
Train: 2018-08-05T18:08:00.421044: step 98, loss 0.548295.
Train: 2018-08-05T18:08:00.624121: step 99, loss 0.587921.
Train: 2018-08-05T18:08:00.858442: step 100, loss 0.553615.
Test: 2018-08-05T18:08:01.905071: step 100, loss 0.546281.
Train: 2018-08-05T18:08:02.904837: step 101, loss 0.572135.
Train: 2018-08-05T18:08:03.123506: step 102, loss 0.472118.
Train: 2018-08-05T18:08:03.326582: step 103, loss 0.555053.
Train: 2018-08-05T18:08:03.560902: step 104, loss 0.566864.
Train: 2018-08-05T18:08:03.779632: step 105, loss 0.545369.
Train: 2018-08-05T18:08:03.982710: step 106, loss 0.554394.
Train: 2018-08-05T18:08:04.201408: step 107, loss 0.553826.
Train: 2018-08-05T18:08:04.435730: step 108, loss 0.553672.
Train: 2018-08-05T18:08:04.638775: step 109, loss 0.637537.
Train: 2018-08-05T18:08:04.857505: step 110, loss 0.565266.
Test: 2018-08-05T18:08:05.904135: step 110, loss 0.556412.
Train: 2018-08-05T18:08:06.122803: step 111, loss 0.581029.
Train: 2018-08-05T18:08:06.325910: step 112, loss 0.601454.
Train: 2018-08-05T18:08:06.544610: step 113, loss 0.55456.
Train: 2018-08-05T18:08:06.763308: step 114, loss 0.651162.
Train: 2018-08-05T18:08:06.982007: step 115, loss 0.5877.
Train: 2018-08-05T18:08:07.200676: step 116, loss 0.574315.
Train: 2018-08-05T18:08:07.403786: step 117, loss 0.60824.
Train: 2018-08-05T18:08:07.638098: step 118, loss 0.580633.
Train: 2018-08-05T18:08:07.841184: step 119, loss 0.636698.
Train: 2018-08-05T18:08:08.059880: step 120, loss 0.545015.
Test: 2018-08-05T18:08:09.106510: step 120, loss 0.552713.
Train: 2018-08-05T18:08:09.325208: step 121, loss 0.572314.
Train: 2018-08-05T18:08:09.559529: step 122, loss 0.617774.
Train: 2018-08-05T18:08:09.778197: step 123, loss 0.508693.
Train: 2018-08-05T18:08:09.981305: step 124, loss 0.579657.
Train: 2018-08-05T18:08:10.199973: step 125, loss 0.553671.
Train: 2018-08-05T18:08:10.403081: step 126, loss 0.498911.
Train: 2018-08-05T18:08:10.621780: step 127, loss 0.656028.
Train: 2018-08-05T18:08:10.856101: step 128, loss 0.49789.
Train: 2018-08-05T18:08:11.059177: step 129, loss 0.490496.
Train: 2018-08-05T18:08:11.277847: step 130, loss 0.544638.
Test: 2018-08-05T18:08:12.324506: step 130, loss 0.546844.
Train: 2018-08-05T18:08:12.543175: step 131, loss 0.496442.
Train: 2018-08-05T18:08:12.761898: step 132, loss 0.637857.
Train: 2018-08-05T18:08:12.964980: step 133, loss 0.572591.
Train: 2018-08-05T18:08:13.183650: step 134, loss 0.553765.
Train: 2018-08-05T18:08:13.402379: step 135, loss 0.603322.
Train: 2018-08-05T18:08:13.621047: step 136, loss 0.556057.
Train: 2018-08-05T18:08:13.839776: step 137, loss 0.557089.
Train: 2018-08-05T18:08:14.058476: step 138, loss 0.601626.
Train: 2018-08-05T18:08:14.277143: step 139, loss 0.549761.
Train: 2018-08-05T18:08:14.495873: step 140, loss 0.594463.
Test: 2018-08-05T18:08:15.542473: step 140, loss 0.549977.
Train: 2018-08-05T18:08:15.776823: step 141, loss 0.602505.
Train: 2018-08-05T18:08:15.979870: step 142, loss 0.550116.
Train: 2018-08-05T18:08:16.198599: step 143, loss 0.52403.
Train: 2018-08-05T18:08:16.417297: step 144, loss 0.527128.
Train: 2018-08-05T18:08:16.635997: step 145, loss 0.504784.
Train: 2018-08-05T18:08:16.854696: step 146, loss 0.597067.
Train: 2018-08-05T18:08:17.073394: step 147, loss 0.526642.
Train: 2018-08-05T18:08:17.292093: step 148, loss 0.497921.
Train: 2018-08-05T18:08:17.495141: step 149, loss 0.589251.
Train: 2018-08-05T18:08:17.729492: step 150, loss 0.578872.
Test: 2018-08-05T18:08:18.776120: step 150, loss 0.552075.
Train: 2018-08-05T18:08:18.963576: step 151, loss 0.485323.
Train: 2018-08-05T18:08:19.182276: step 152, loss 0.554533.
Train: 2018-08-05T18:08:19.400974: step 153, loss 0.637407.
Train: 2018-08-05T18:08:19.619644: step 154, loss 0.554087.
Train: 2018-08-05T18:08:19.838373: step 155, loss 0.571052.
Train: 2018-08-05T18:08:20.072692: step 156, loss 0.586666.
Train: 2018-08-05T18:08:20.291361: step 157, loss 0.572102.
Train: 2018-08-05T18:08:20.510089: step 158, loss 0.542773.
Train: 2018-08-05T18:08:20.728759: step 159, loss 0.525623.
Train: 2018-08-05T18:08:20.947490: step 160, loss 0.581005.
Test: 2018-08-05T18:08:21.994087: step 160, loss 0.548636.
Train: 2018-08-05T18:08:22.212815: step 161, loss 0.611285.
Train: 2018-08-05T18:08:22.431485: step 162, loss 0.545377.
Train: 2018-08-05T18:08:22.665836: step 163, loss 0.578919.
Train: 2018-08-05T18:08:22.884533: step 164, loss 0.563162.
Train: 2018-08-05T18:08:23.103202: step 165, loss 0.594525.
Train: 2018-08-05T18:08:23.321931: step 166, loss 0.633061.
Train: 2018-08-05T18:08:23.540629: step 167, loss 0.601755.
Train: 2018-08-05T18:08:23.759328: step 168, loss 0.608728.
Train: 2018-08-05T18:08:23.962406: step 169, loss 0.608386.
Train: 2018-08-05T18:08:24.181106: step 170, loss 0.62865.
Test: 2018-08-05T18:08:25.227705: step 170, loss 0.558655.
Train: 2018-08-05T18:08:25.446434: step 171, loss 0.582002.
Train: 2018-08-05T18:08:25.665132: step 172, loss 0.595482.
Train: 2018-08-05T18:08:25.883801: step 173, loss 0.589288.
Train: 2018-08-05T18:08:26.102500: step 174, loss 0.595648.
Train: 2018-08-05T18:08:26.321228: step 175, loss 0.556913.
Train: 2018-08-05T18:08:26.539897: step 176, loss 0.58192.
Train: 2018-08-05T18:08:26.758625: step 177, loss 0.553976.
Train: 2018-08-05T18:08:26.961703: step 178, loss 0.544653.
Train: 2018-08-05T18:08:27.180402: step 179, loss 0.564046.
Train: 2018-08-05T18:08:27.399101: step 180, loss 0.497547.
Test: 2018-08-05T18:08:28.445701: step 180, loss 0.547767.
Train: 2018-08-05T18:08:28.664430: step 181, loss 0.572201.
Train: 2018-08-05T18:08:28.883128: step 182, loss 0.555513.
Train: 2018-08-05T18:08:29.101828: step 183, loss 0.535919.
Train: 2018-08-05T18:08:29.336117: step 184, loss 0.639969.
Train: 2018-08-05T18:08:29.554846: step 185, loss 0.457274.
Train: 2018-08-05T18:08:29.773516: step 186, loss 0.506066.
Train: 2018-08-05T18:08:29.992214: step 187, loss 0.515599.
Train: 2018-08-05T18:08:30.210913: step 188, loss 0.465824.
Train: 2018-08-05T18:08:30.414020: step 189, loss 0.639391.
Train: 2018-08-05T18:08:30.632719: step 190, loss 0.54554.
Test: 2018-08-05T18:08:31.679348: step 190, loss 0.548387.
Train: 2018-08-05T18:08:31.898047: step 191, loss 0.593874.
Train: 2018-08-05T18:08:32.116717: step 192, loss 0.526206.
Train: 2018-08-05T18:08:32.335445: step 193, loss 0.589048.
Train: 2018-08-05T18:08:32.554144: step 194, loss 0.511825.
Train: 2018-08-05T18:08:32.772812: step 195, loss 0.528956.
Train: 2018-08-05T18:08:32.991541: step 196, loss 0.553877.
Train: 2018-08-05T18:08:33.210240: step 197, loss 0.630178.
Train: 2018-08-05T18:08:33.428939: step 198, loss 0.62927.
Train: 2018-08-05T18:08:33.647638: step 199, loss 0.578825.
Train: 2018-08-05T18:08:33.866337: step 200, loss 0.547093.
Test: 2018-08-05T18:08:34.912937: step 200, loss 0.549404.
Train: 2018-08-05T18:08:35.912732: step 201, loss 0.563204.
Train: 2018-08-05T18:08:36.131431: step 202, loss 0.563332.
Train: 2018-08-05T18:08:36.350130: step 203, loss 0.625695.
Train: 2018-08-05T18:08:36.568823: step 204, loss 0.617384.
Train: 2018-08-05T18:08:36.787528: step 205, loss 0.594149.
Train: 2018-08-05T18:08:37.006227: step 206, loss 0.543157.
Train: 2018-08-05T18:08:37.224896: step 207, loss 0.58683.
Train: 2018-08-05T18:08:37.443624: step 208, loss 0.528319.
Train: 2018-08-05T18:08:37.662322: step 209, loss 0.579.
Train: 2018-08-05T18:08:37.896614: step 210, loss 0.562862.
Test: 2018-08-05T18:08:38.943272: step 210, loss 0.547629.
Train: 2018-08-05T18:08:39.161973: step 211, loss 0.562353.
Train: 2018-08-05T18:08:39.380670: step 212, loss 0.53546.
Train: 2018-08-05T18:08:39.599370: step 213, loss 0.5456.
Train: 2018-08-05T18:08:39.818037: step 214, loss 0.505568.
Train: 2018-08-05T18:08:40.036766: step 215, loss 0.535614.
Train: 2018-08-05T18:08:40.255465: step 216, loss 0.545588.
Train: 2018-08-05T18:08:40.474165: step 217, loss 0.613112.
Train: 2018-08-05T18:08:40.692863: step 218, loss 0.571464.
Train: 2018-08-05T18:08:40.911561: step 219, loss 0.529444.
Train: 2018-08-05T18:08:41.130230: step 220, loss 0.49654.
Test: 2018-08-05T18:08:42.176860: step 220, loss 0.547204.
Train: 2018-08-05T18:08:42.395589: step 221, loss 0.553748.
Train: 2018-08-05T18:08:42.614289: step 222, loss 0.536076.
Train: 2018-08-05T18:08:42.832986: step 223, loss 0.499542.
Train: 2018-08-05T18:08:43.051656: step 224, loss 0.554219.
Train: 2018-08-05T18:08:43.270354: step 225, loss 0.653904.
Train: 2018-08-05T18:08:43.489084: step 226, loss 0.535081.
Train: 2018-08-05T18:08:43.692131: step 227, loss 0.572143.
Train: 2018-08-05T18:08:43.910859: step 228, loss 0.535987.
Train: 2018-08-05T18:08:44.129559: step 229, loss 0.605069.
Train: 2018-08-05T18:08:44.363849: step 230, loss 0.595175.
Test: 2018-08-05T18:08:45.409915: step 230, loss 0.550376.
Train: 2018-08-05T18:08:45.644265: step 231, loss 0.602081.
Train: 2018-08-05T18:08:45.862955: step 232, loss 0.601317.
Train: 2018-08-05T18:08:46.081654: step 233, loss 0.552407.
Train: 2018-08-05T18:08:46.300322: step 234, loss 0.635539.
Train: 2018-08-05T18:08:46.519051: step 235, loss 0.567872.
Train: 2018-08-05T18:08:46.737750: step 236, loss 0.574644.
Train: 2018-08-05T18:08:46.956449: step 237, loss 0.601326.
Train: 2018-08-05T18:08:47.175148: step 238, loss 0.51947.
Train: 2018-08-05T18:08:47.393817: step 239, loss 0.565892.
Train: 2018-08-05T18:08:47.596924: step 240, loss 0.550021.
Test: 2018-08-05T18:08:48.643524: step 240, loss 0.550141.
Train: 2018-08-05T18:08:48.877875: step 241, loss 0.563311.
Train: 2018-08-05T18:08:49.096573: step 242, loss 0.630194.
Train: 2018-08-05T18:08:49.299650: step 243, loss 0.629831.
Train: 2018-08-05T18:08:49.518351: step 244, loss 0.53749.
Train: 2018-08-05T18:08:49.737048: step 245, loss 0.537212.
Train: 2018-08-05T18:08:49.955747: step 246, loss 0.485377.
Train: 2018-08-05T18:08:50.174445: step 247, loss 0.517774.
Train: 2018-08-05T18:08:50.393144: step 248, loss 0.56351.
Train: 2018-08-05T18:08:50.596192: step 249, loss 0.584409.
Train: 2018-08-05T18:08:50.814921: step 250, loss 0.604995.
Test: 2018-08-05T18:08:51.877141: step 250, loss 0.546696.
Train: 2018-08-05T18:08:52.080249: step 251, loss 0.612892.
Train: 2018-08-05T18:08:52.298948: step 252, loss 0.499085.
Train: 2018-08-05T18:08:52.517649: step 253, loss 0.526955.
Train: 2018-08-05T18:08:52.736346: step 254, loss 0.527115.
Train: 2018-08-05T18:08:52.955015: step 255, loss 0.54473.
Train: 2018-08-05T18:08:53.173743: step 256, loss 0.589258.
Train: 2018-08-05T18:08:53.408058: step 257, loss 0.518277.
Train: 2018-08-05T18:08:53.626762: step 258, loss 0.526961.
Train: 2018-08-05T18:08:53.829840: step 259, loss 0.589528.
Train: 2018-08-05T18:08:54.048508: step 260, loss 0.517834.
Test: 2018-08-05T18:08:55.095139: step 260, loss 0.546314.
Train: 2018-08-05T18:08:55.313867: step 261, loss 0.571655.
Train: 2018-08-05T18:08:55.532536: step 262, loss 0.634902.
Train: 2018-08-05T18:08:55.735643: step 263, loss 0.536055.
Train: 2018-08-05T18:08:55.954342: step 264, loss 0.588199.
Train: 2018-08-05T18:08:56.173041: step 265, loss 0.579196.
Train: 2018-08-05T18:08:56.376118: step 266, loss 0.464304.
Train: 2018-08-05T18:08:56.610407: step 267, loss 0.595566.
Train: 2018-08-05T18:08:56.829137: step 268, loss 0.579035.
Train: 2018-08-05T18:08:57.032216: step 269, loss 0.537682.
Train: 2018-08-05T18:08:57.250884: step 270, loss 0.504056.
Test: 2018-08-05T18:08:58.297544: step 270, loss 0.549207.
Train: 2018-08-05T18:08:58.500590: step 271, loss 0.562335.
Train: 2018-08-05T18:08:58.719319: step 272, loss 0.62395.
Train: 2018-08-05T18:08:58.938018: step 273, loss 0.606243.
Train: 2018-08-05T18:08:59.141095: step 274, loss 0.613794.
Train: 2018-08-05T18:08:59.344176: step 275, loss 0.505361.
Train: 2018-08-05T18:08:59.547250: step 276, loss 0.546128.
Train: 2018-08-05T18:08:59.765949: step 277, loss 0.570743.
Train: 2018-08-05T18:08:59.969027: step 278, loss 0.545823.
Train: 2018-08-05T18:09:00.187720: step 279, loss 0.537167.
Train: 2018-08-05T18:09:00.390805: step 280, loss 0.553778.
Test: 2018-08-05T18:09:01.437402: step 280, loss 0.547603.
Train: 2018-08-05T18:09:01.640512: step 281, loss 0.553645.
Train: 2018-08-05T18:09:01.843588: step 282, loss 0.562472.
Train: 2018-08-05T18:09:02.046665: step 283, loss 0.535606.
Train: 2018-08-05T18:09:02.249742: step 284, loss 0.581152.
Train: 2018-08-05T18:09:02.452791: step 285, loss 0.507744.
Train: 2018-08-05T18:09:02.655899: step 286, loss 0.581902.
Train: 2018-08-05T18:09:02.858975: step 287, loss 0.516473.
Train: 2018-08-05T18:09:03.062053: step 288, loss 0.497369.
Train: 2018-08-05T18:09:03.265132: step 289, loss 0.59321.
Train: 2018-08-05T18:09:03.468177: step 290, loss 0.602654.
Test: 2018-08-05T18:09:04.514841: step 290, loss 0.548081.
Train: 2018-08-05T18:09:04.749162: step 291, loss 0.563355.
Train: 2018-08-05T18:09:04.983480: step 292, loss 0.571894.
Train: 2018-08-05T18:09:05.186556: step 293, loss 0.562408.
Train: 2018-08-05T18:09:05.374011: step 294, loss 0.51973.
Train: 2018-08-05T18:09:05.561468: step 295, loss 0.545592.
Train: 2018-08-05T18:09:05.748923: step 296, loss 0.520666.
Train: 2018-08-05T18:09:05.952001: step 297, loss 0.587704.
Train: 2018-08-05T18:09:06.155080: step 298, loss 0.435549.
Train: 2018-08-05T18:09:06.326914: step 299, loss 0.562456.
Train: 2018-08-05T18:09:06.514376: step 300, loss 0.562886.
Test: 2018-08-05T18:09:07.560999: step 300, loss 0.549178.
Train: 2018-08-05T18:09:08.513900: step 301, loss 0.469224.
Train: 2018-08-05T18:09:08.654494: step 302, loss 0.585839.
Train: 2018-08-05T18:09:08.826298: step 303, loss 0.495142.
Train: 2018-08-05T18:09:08.998162: step 304, loss 0.587625.
Train: 2018-08-05T18:09:09.169997: step 305, loss 0.577128.
Train: 2018-08-05T18:09:09.341802: step 306, loss 0.54552.
Train: 2018-08-05T18:09:09.513666: step 307, loss 0.662683.
Train: 2018-08-05T18:09:09.685471: step 308, loss 0.572092.
Train: 2018-08-05T18:09:09.841715: step 309, loss 0.536509.
Train: 2018-08-05T18:09:10.013549: step 310, loss 0.636387.
Test: 2018-08-05T18:09:11.060179: step 310, loss 0.550291.
Train: 2018-08-05T18:09:11.232014: step 311, loss 0.524973.
Train: 2018-08-05T18:09:11.388230: step 312, loss 0.609256.
Train: 2018-08-05T18:09:11.560062: step 313, loss 0.579424.
Train: 2018-08-05T18:09:11.731898: step 314, loss 0.579737.
Train: 2018-08-05T18:09:11.903733: step 315, loss 0.651024.
Train: 2018-08-05T18:09:12.075537: step 316, loss 0.504077.
Train: 2018-08-05T18:09:12.231751: step 317, loss 0.532087.
Train: 2018-08-05T18:09:12.387996: step 318, loss 0.573361.
Train: 2018-08-05T18:09:12.559828: step 319, loss 0.558836.
Train: 2018-08-05T18:09:12.716041: step 320, loss 0.529239.
Test: 2018-08-05T18:09:13.778263: step 320, loss 0.551847.
Train: 2018-08-05T18:09:13.934507: step 321, loss 0.549565.
Train: 2018-08-05T18:09:14.090690: step 322, loss 0.555546.
Train: 2018-08-05T18:09:14.262554: step 323, loss 0.529011.
Train: 2018-08-05T18:09:14.418739: step 324, loss 0.627647.
Train: 2018-08-05T18:09:14.590574: step 325, loss 0.599141.
Train: 2018-08-05T18:09:14.762437: step 326, loss 0.500848.
Train: 2018-08-05T18:09:14.918622: step 327, loss 0.52712.
Train: 2018-08-05T18:09:15.074835: step 328, loss 0.607361.
Train: 2018-08-05T18:09:15.246699: step 329, loss 0.571511.
Train: 2018-08-05T18:09:15.418534: step 330, loss 0.580392.
Test: 2018-08-05T18:09:16.465133: step 330, loss 0.547843.
Train: 2018-08-05T18:09:16.621378: step 331, loss 0.624626.
Train: 2018-08-05T18:09:16.793212: step 332, loss 0.544894.
Train: 2018-08-05T18:09:16.949425: step 333, loss 0.545026.
Train: 2018-08-05T18:09:17.121260: step 334, loss 0.536512.
Train: 2018-08-05T18:09:17.277468: step 335, loss 0.519297.
Train: 2018-08-05T18:09:17.433662: step 336, loss 0.527667.
Train: 2018-08-05T18:09:17.605492: step 337, loss 0.606289.
Train: 2018-08-05T18:09:17.761735: step 338, loss 0.623813.
Train: 2018-08-05T18:09:17.933564: step 339, loss 0.588386.
Train: 2018-08-05T18:09:18.089783: step 340, loss 0.579425.
Test: 2018-08-05T18:09:19.152022: step 340, loss 0.548346.
Train: 2018-08-05T18:09:19.308219: step 341, loss 0.621189.
Train: 2018-08-05T18:09:19.464431: step 342, loss 0.603497.
Train: 2018-08-05T18:09:19.636294: step 343, loss 0.547014.
Train: 2018-08-05T18:09:19.792480: step 344, loss 0.563185.
Train: 2018-08-05T18:09:19.948693: step 345, loss 0.555762.
Train: 2018-08-05T18:09:20.104937: step 346, loss 0.486685.
Train: 2018-08-05T18:09:20.261152: step 347, loss 0.53991.
Train: 2018-08-05T18:09:20.417334: step 348, loss 0.531267.
Train: 2018-08-05T18:09:20.589169: step 349, loss 0.522153.
Train: 2018-08-05T18:09:20.761032: step 350, loss 0.629092.
Test: 2018-08-05T18:09:21.807633: step 350, loss 0.548428.
Train: 2018-08-05T18:09:21.963877: step 351, loss 0.503157.
Train: 2018-08-05T18:09:22.120090: step 352, loss 0.562343.
Train: 2018-08-05T18:09:22.276274: step 353, loss 0.48288.
Train: 2018-08-05T18:09:22.432517: step 354, loss 0.580986.
Train: 2018-08-05T18:09:22.604346: step 355, loss 0.563143.
Train: 2018-08-05T18:09:22.760535: step 356, loss 0.497237.
Train: 2018-08-05T18:09:22.916778: step 357, loss 0.544766.
Train: 2018-08-05T18:09:23.072991: step 358, loss 0.515495.
Train: 2018-08-05T18:09:23.229208: step 359, loss 0.475255.
Train: 2018-08-05T18:09:23.385418: step 360, loss 0.638221.
Test: 2018-08-05T18:09:24.432018: step 360, loss 0.550076.
Train: 2018-08-05T18:09:24.588261: step 361, loss 0.576767.
Train: 2018-08-05T18:09:24.760103: step 362, loss 0.545725.
Train: 2018-08-05T18:09:24.900659: step 363, loss 0.495211.
Train: 2018-08-05T18:09:25.072493: step 364, loss 0.575183.
Train: 2018-08-05T18:09:25.213115: step 365, loss 0.495856.
Train: 2018-08-05T18:09:25.369329: step 366, loss 0.515622.
Train: 2018-08-05T18:09:25.541164: step 367, loss 0.457454.
Train: 2018-08-05T18:09:25.697348: step 368, loss 0.466502.
Train: 2018-08-05T18:09:25.853591: step 369, loss 0.525365.
Train: 2018-08-05T18:09:26.009774: step 370, loss 0.556928.
Test: 2018-08-05T18:09:27.056404: step 370, loss 0.550277.
Train: 2018-08-05T18:09:27.275102: step 371, loss 0.639045.
Train: 2018-08-05T18:09:27.462588: step 372, loss 0.535342.
Train: 2018-08-05T18:09:27.618802: step 373, loss 0.515478.
Train: 2018-08-05T18:09:27.775010: step 374, loss 0.593527.
Train: 2018-08-05T18:09:27.915578: step 375, loss 0.62089.
Train: 2018-08-05T18:09:28.071815: step 376, loss 0.498371.
Train: 2018-08-05T18:09:28.228005: step 377, loss 0.499437.
Train: 2018-08-05T18:09:28.399870: step 378, loss 0.580363.
Train: 2018-08-05T18:09:28.540462: step 379, loss 0.527225.
Train: 2018-08-05T18:09:28.696674: step 380, loss 0.632114.
Test: 2018-08-05T18:09:29.758896: step 380, loss 0.548059.
Train: 2018-08-05T18:09:29.915110: step 381, loss 0.528142.
Train: 2018-08-05T18:09:30.071323: step 382, loss 0.553918.
Train: 2018-08-05T18:09:30.227536: step 383, loss 0.537298.
Train: 2018-08-05T18:09:30.383749: step 384, loss 0.579107.
Train: 2018-08-05T18:09:30.539964: step 385, loss 0.554155.
Train: 2018-08-05T18:09:30.696206: step 386, loss 0.50447.
Train: 2018-08-05T18:09:30.852415: step 387, loss 0.56241.
Train: 2018-08-05T18:09:31.024225: step 388, loss 0.469584.
Train: 2018-08-05T18:09:31.180417: step 389, loss 0.579609.
Train: 2018-08-05T18:09:31.336630: step 390, loss 0.615148.
Test: 2018-08-05T18:09:32.383231: step 390, loss 0.546847.
Train: 2018-08-05T18:09:32.555095: step 391, loss 0.571285.
Train: 2018-08-05T18:09:32.711278: step 392, loss 0.535828.
Train: 2018-08-05T18:09:32.851900: step 393, loss 0.705345.
Train: 2018-08-05T18:09:33.008114: step 394, loss 0.509839.
Train: 2018-08-05T18:09:33.164327: step 395, loss 0.51899.
Train: 2018-08-05T18:09:33.320510: step 396, loss 0.527741.
Train: 2018-08-05T18:09:33.476754: step 397, loss 0.562342.
Train: 2018-08-05T18:09:33.632937: step 398, loss 0.544948.
Train: 2018-08-05T18:09:33.789186: step 399, loss 0.632306.
Train: 2018-08-05T18:09:33.945395: step 400, loss 0.562345.
Test: 2018-08-05T18:09:34.991994: step 400, loss 0.548358.
Train: 2018-08-05T18:09:35.898062: step 401, loss 0.579546.
Train: 2018-08-05T18:09:36.054275: step 402, loss 0.536778.
Train: 2018-08-05T18:09:36.194867: step 403, loss 0.57931.
Train: 2018-08-05T18:09:36.351080: step 404, loss 0.61292.
Train: 2018-08-05T18:09:36.507294: step 405, loss 0.529196.
Train: 2018-08-05T18:09:36.663507: step 406, loss 0.595548.
Train: 2018-08-05T18:09:36.819720: step 407, loss 0.546199.
Train: 2018-08-05T18:09:36.975904: step 408, loss 0.611537.
Train: 2018-08-05T18:09:37.132147: step 409, loss 0.506229.
Train: 2018-08-05T18:09:37.288361: step 410, loss 0.546531.
Test: 2018-08-05T18:09:38.334990: step 410, loss 0.549976.
Train: 2018-08-05T18:09:38.491175: step 411, loss 0.570774.
Train: 2018-08-05T18:09:38.647417: step 412, loss 0.554378.
Train: 2018-08-05T18:09:38.803631: step 413, loss 0.628584.
Train: 2018-08-05T18:09:38.975466: step 414, loss 0.578998.
Train: 2018-08-05T18:09:39.131650: step 415, loss 0.562536.
Train: 2018-08-05T18:09:39.272271: step 416, loss 0.595385.
Train: 2018-08-05T18:09:39.428484: step 417, loss 0.644282.
Train: 2018-08-05T18:09:39.584669: step 418, loss 0.538684.
Train: 2018-08-05T18:09:39.756536: step 419, loss 0.562918.
Train: 2018-08-05T18:09:39.912747: step 420, loss 0.539224.
Test: 2018-08-05T18:09:40.959376: step 420, loss 0.549351.
Train: 2018-08-05T18:09:41.115589: step 421, loss 0.586798.
Train: 2018-08-05T18:09:41.287394: step 422, loss 0.523267.
Train: 2018-08-05T18:09:41.443608: step 423, loss 0.54681.
Train: 2018-08-05T18:09:41.639575: step 424, loss 0.578905.
Train: 2018-08-05T18:09:41.795790: step 425, loss 0.554343.
Train: 2018-08-05T18:09:41.952004: step 426, loss 0.587384.
Train: 2018-08-05T18:09:42.123843: step 427, loss 0.554013.
Train: 2018-08-05T18:09:42.280054: step 428, loss 0.520036.
Train: 2018-08-05T18:09:42.451889: step 429, loss 0.553732.
Train: 2018-08-05T18:09:42.623723: step 430, loss 0.553632.
Test: 2018-08-05T18:09:43.670378: step 430, loss 0.546422.
Train: 2018-08-05T18:09:43.826565: step 431, loss 0.526983.
Train: 2018-08-05T18:09:43.982779: step 432, loss 0.562634.
Train: 2018-08-05T18:09:44.154640: step 433, loss 0.599452.
Train: 2018-08-05T18:09:44.295237: step 434, loss 0.608848.
Train: 2018-08-05T18:09:44.467041: step 435, loss 0.553655.
Train: 2018-08-05T18:09:44.623279: step 436, loss 0.580795.
Train: 2018-08-05T18:09:44.779469: step 437, loss 0.589395.
Train: 2018-08-05T18:09:44.935680: step 438, loss 0.571209.
Train: 2018-08-05T18:09:45.091928: step 439, loss 0.56234.
Train: 2018-08-05T18:09:45.248109: step 440, loss 0.689944.
Test: 2018-08-05T18:09:46.295801: step 440, loss 0.550158.
Train: 2018-08-05T18:09:46.467610: step 441, loss 0.529703.
Train: 2018-08-05T18:09:46.623823: step 442, loss 0.594882.
Train: 2018-08-05T18:09:46.780037: step 443, loss 0.56327.
Train: 2018-08-05T18:09:46.951876: step 444, loss 0.540716.
Train: 2018-08-05T18:09:47.108085: step 445, loss 0.594227.
Train: 2018-08-05T18:09:47.295565: step 446, loss 0.57918.
Train: 2018-08-05T18:09:47.451779: step 447, loss 0.534639.
Train: 2018-08-05T18:09:47.607969: step 448, loss 0.638879.
Train: 2018-08-05T18:09:47.764207: step 449, loss 0.534854.
Train: 2018-08-05T18:09:47.920426: step 450, loss 0.623905.
Test: 2018-08-05T18:09:48.967025: step 450, loss 0.550844.
Train: 2018-08-05T18:09:49.123238: step 451, loss 0.564442.
Train: 2018-08-05T18:09:49.295072: step 452, loss 0.631377.
Train: 2018-08-05T18:09:49.420048: step 453, loss 0.501207.
Train: 2018-08-05T18:09:49.576257: step 454, loss 0.549201.
Train: 2018-08-05T18:09:49.748117: step 455, loss 0.571411.
Train: 2018-08-05T18:09:49.904305: step 456, loss 0.602167.
Train: 2018-08-05T18:09:50.060519: step 457, loss 0.539645.
Train: 2018-08-05T18:09:50.216733: step 458, loss 0.57886.
Train: 2018-08-05T18:09:50.372977: step 459, loss 0.530219.
Train: 2018-08-05T18:09:50.529159: step 460, loss 0.504444.
Test: 2018-08-05T18:09:51.591410: step 460, loss 0.547086.
Train: 2018-08-05T18:09:51.747627: step 461, loss 0.528163.
Train: 2018-08-05T18:09:51.903862: step 462, loss 0.50929.
Train: 2018-08-05T18:09:52.060051: step 463, loss 0.618675.
Train: 2018-08-05T18:09:52.216296: step 464, loss 0.459438.
Train: 2018-08-05T18:09:52.372502: step 465, loss 0.604119.
Train: 2018-08-05T18:09:52.528691: step 466, loss 0.615248.
Train: 2018-08-05T18:09:52.700550: step 467, loss 0.564775.
Train: 2018-08-05T18:09:52.856738: step 468, loss 0.544816.
Train: 2018-08-05T18:09:53.012983: step 469, loss 0.59282.
Train: 2018-08-05T18:09:53.169193: step 470, loss 0.601294.
Test: 2018-08-05T18:09:54.215827: step 470, loss 0.546701.
Train: 2018-08-05T18:09:54.387631: step 471, loss 0.590565.
Train: 2018-08-05T18:09:54.543845: step 472, loss 0.625045.
Train: 2018-08-05T18:09:54.700059: step 473, loss 0.596687.
Train: 2018-08-05T18:09:54.856271: step 474, loss 0.480063.
Train: 2018-08-05T18:09:55.028106: step 475, loss 0.530494.
Train: 2018-08-05T18:09:55.184343: step 476, loss 0.586821.
Train: 2018-08-05T18:09:55.340533: step 477, loss 0.492409.
Train: 2018-08-05T18:09:55.496777: step 478, loss 0.492212.
Train: 2018-08-05T18:09:55.652962: step 479, loss 0.562859.
Train: 2018-08-05T18:09:55.809173: step 480, loss 0.538246.
Test: 2018-08-05T18:09:56.871450: step 480, loss 0.547833.
Train: 2018-08-05T18:09:57.027637: step 481, loss 0.537558.
Train: 2018-08-05T18:09:57.183850: step 482, loss 0.562346.
Train: 2018-08-05T18:09:57.340063: step 483, loss 0.597051.
Train: 2018-08-05T18:09:57.511931: step 484, loss 0.579981.
Train: 2018-08-05T18:09:57.652516: step 485, loss 0.553592.
Train: 2018-08-05T18:09:57.808704: step 486, loss 0.562514.
Train: 2018-08-05T18:09:57.964944: step 487, loss 0.562565.
Train: 2018-08-05T18:09:58.136757: step 488, loss 0.553598.
Train: 2018-08-05T18:09:58.292999: step 489, loss 0.661785.
Train: 2018-08-05T18:09:58.449179: step 490, loss 0.615712.
Test: 2018-08-05T18:09:59.511431: step 490, loss 0.547474.
Train: 2018-08-05T18:09:59.667664: step 491, loss 0.553679.
Train: 2018-08-05T18:09:59.823882: step 492, loss 0.536877.
Train: 2018-08-05T18:09:59.980073: step 493, loss 0.562404.
Train: 2018-08-05T18:10:00.136285: step 494, loss 0.554212.
Train: 2018-08-05T18:10:00.292499: step 495, loss 0.54615.
Train: 2018-08-05T18:10:00.464333: step 496, loss 0.570767.
Train: 2018-08-05T18:10:00.620546: step 497, loss 0.530019.
Train: 2018-08-05T18:10:00.776777: step 498, loss 0.521659.
Train: 2018-08-05T18:10:00.932997: step 499, loss 0.554199.
Train: 2018-08-05T18:10:01.104806: step 500, loss 0.512083.
Test: 2018-08-05T18:10:02.151438: step 500, loss 0.547261.
Train: 2018-08-05T18:10:03.057475: step 501, loss 0.528115.
Train: 2018-08-05T18:10:03.213688: step 502, loss 0.518591.
Train: 2018-08-05T18:10:03.369926: step 503, loss 0.5536.
Train: 2018-08-05T18:10:03.526143: step 504, loss 0.54451.
Train: 2018-08-05T18:10:03.682330: step 505, loss 0.601093.
Train: 2018-08-05T18:10:03.854163: step 506, loss 0.649246.
Train: 2018-08-05T18:10:04.010377: step 507, loss 0.56341.
Train: 2018-08-05T18:10:04.182217: step 508, loss 0.572461.
Train: 2018-08-05T18:10:04.338450: step 509, loss 0.572033.
Train: 2018-08-05T18:10:04.494639: step 510, loss 0.535586.
Test: 2018-08-05T18:10:05.541271: step 510, loss 0.548659.
Train: 2018-08-05T18:10:05.697481: step 511, loss 0.562473.
Train: 2018-08-05T18:10:05.869317: step 512, loss 0.527337.
Train: 2018-08-05T18:10:06.041151: step 513, loss 0.571047.
Train: 2018-08-05T18:10:06.197376: step 514, loss 0.527873.
Train: 2018-08-05T18:10:06.337984: step 515, loss 0.605289.
Train: 2018-08-05T18:10:06.494195: step 516, loss 0.638934.
Train: 2018-08-05T18:10:06.650383: step 517, loss 0.604128.
Train: 2018-08-05T18:10:06.806601: step 518, loss 0.54634.
Train: 2018-08-05T18:10:06.962850: step 519, loss 0.538828.
Train: 2018-08-05T18:10:07.119024: step 520, loss 0.570928.
Test: 2018-08-05T18:10:08.165653: step 520, loss 0.549431.
Train: 2018-08-05T18:10:08.321868: step 521, loss 0.563122.
Train: 2018-08-05T18:10:08.493708: step 522, loss 0.555355.
Train: 2018-08-05T18:10:08.649917: step 523, loss 0.547506.
Train: 2018-08-05T18:10:08.806128: step 524, loss 0.586748.
Train: 2018-08-05T18:10:08.962369: step 525, loss 0.563033.
Train: 2018-08-05T18:10:09.118581: step 526, loss 0.618658.
Train: 2018-08-05T18:10:09.274794: step 527, loss 0.515243.
Train: 2018-08-05T18:10:09.431007: step 528, loss 0.546781.
Train: 2018-08-05T18:10:09.587220: step 529, loss 0.66013.
Train: 2018-08-05T18:10:09.743410: step 530, loss 0.587004.
Test: 2018-08-05T18:10:10.790070: step 530, loss 0.549625.
Train: 2018-08-05T18:10:10.946253: step 531, loss 0.603106.
Train: 2018-08-05T18:10:11.118114: step 532, loss 0.506767.
Train: 2018-08-05T18:10:11.274334: step 533, loss 0.562792.
Train: 2018-08-05T18:10:11.430541: step 534, loss 0.522302.
Train: 2018-08-05T18:10:11.571107: step 535, loss 0.595331.
Train: 2018-08-05T18:10:11.727320: step 536, loss 0.496461.
Train: 2018-08-05T18:10:11.883536: step 537, loss 0.570791.
Train: 2018-08-05T18:10:12.055371: step 538, loss 0.58798.
Train: 2018-08-05T18:10:12.211582: step 539, loss 0.588256.
Train: 2018-08-05T18:10:12.367794: step 540, loss 0.56235.
Test: 2018-08-05T18:10:13.414424: step 540, loss 0.548603.
Train: 2018-08-05T18:10:13.570637: step 541, loss 0.562364.
Train: 2018-08-05T18:10:13.742473: step 542, loss 0.553629.
Train: 2018-08-05T18:10:13.914310: step 543, loss 0.579945.
Train: 2018-08-05T18:10:14.070523: step 544, loss 0.632556.
Train: 2018-08-05T18:10:14.226759: step 545, loss 0.501709.
Train: 2018-08-05T18:10:14.398595: step 546, loss 0.640135.
Train: 2018-08-05T18:10:14.554782: step 547, loss 0.596421.
Train: 2018-08-05T18:10:14.710998: step 548, loss 0.562407.
Train: 2018-08-05T18:10:14.882857: step 549, loss 0.488377.
Train: 2018-08-05T18:10:15.039075: step 550, loss 0.562529.
Test: 2018-08-05T18:10:16.085674: step 550, loss 0.54916.
Train: 2018-08-05T18:10:16.257512: step 551, loss 0.513142.
Train: 2018-08-05T18:10:16.413724: step 552, loss 0.562452.
Train: 2018-08-05T18:10:16.569936: step 553, loss 0.562395.
Train: 2018-08-05T18:10:16.726176: step 554, loss 0.494661.
Train: 2018-08-05T18:10:16.882394: step 555, loss 0.553712.
Train: 2018-08-05T18:10:17.038601: step 556, loss 0.562398.
Train: 2018-08-05T18:10:17.194793: step 557, loss 0.508963.
Train: 2018-08-05T18:10:17.351007: step 558, loss 0.55365.
Train: 2018-08-05T18:10:17.522838: step 559, loss 0.572349.
Train: 2018-08-05T18:10:17.694707: step 560, loss 0.506989.
Test: 2018-08-05T18:10:18.741329: step 560, loss 0.547116.
Train: 2018-08-05T18:10:18.897540: step 561, loss 0.563699.
Train: 2018-08-05T18:10:19.053760: step 562, loss 0.515835.
Train: 2018-08-05T18:10:19.209969: step 563, loss 0.642254.
Train: 2018-08-05T18:10:19.381808: step 564, loss 0.573767.
Train: 2018-08-05T18:10:19.537990: step 565, loss 0.563713.
Train: 2018-08-05T18:10:19.694207: step 566, loss 0.619619.
Train: 2018-08-05T18:10:19.850420: step 567, loss 0.590193.
Train: 2018-08-05T18:10:20.006655: step 568, loss 0.615555.
Train: 2018-08-05T18:10:20.162871: step 569, loss 0.553816.
Train: 2018-08-05T18:10:20.319060: step 570, loss 0.612025.
Test: 2018-08-05T18:10:21.381310: step 570, loss 0.550906.
Train: 2018-08-05T18:10:21.537534: step 571, loss 0.539028.
Train: 2018-08-05T18:10:21.678118: step 572, loss 0.617733.
Train: 2018-08-05T18:10:21.849950: step 573, loss 0.548924.
Train: 2018-08-05T18:10:22.006163: step 574, loss 0.557123.
Train: 2018-08-05T18:10:22.162400: step 575, loss 0.535617.
Train: 2018-08-05T18:10:22.334212: step 576, loss 0.572238.
Train: 2018-08-05T18:10:22.490455: step 577, loss 0.623358.
Train: 2018-08-05T18:10:22.646665: step 578, loss 0.521296.
Train: 2018-08-05T18:10:22.787231: step 579, loss 0.542736.
Train: 2018-08-05T18:10:22.959065: step 580, loss 0.534529.
Test: 2018-08-05T18:10:24.005695: step 580, loss 0.551232.
Train: 2018-08-05T18:10:24.146286: step 581, loss 0.563784.
Train: 2018-08-05T18:10:24.302503: step 582, loss 0.547675.
Train: 2018-08-05T18:10:24.458713: step 583, loss 0.53077.
Train: 2018-08-05T18:10:24.614953: step 584, loss 0.57903.
Train: 2018-08-05T18:10:24.771141: step 585, loss 0.536846.
Train: 2018-08-05T18:10:24.942976: step 586, loss 0.614869.
Train: 2018-08-05T18:10:25.099191: step 587, loss 0.518014.
Train: 2018-08-05T18:10:25.255428: step 588, loss 0.544551.
Train: 2018-08-05T18:10:25.411648: step 589, loss 0.507486.
Train: 2018-08-05T18:10:25.583479: step 590, loss 0.47826.
Test: 2018-08-05T18:10:26.630112: step 590, loss 0.548361.
Train: 2018-08-05T18:10:26.786323: step 591, loss 0.56446.
Train: 2018-08-05T18:10:26.942538: step 592, loss 0.535316.
Train: 2018-08-05T18:10:27.098720: step 593, loss 0.515221.
Train: 2018-08-05T18:10:27.254965: step 594, loss 0.63927.
Train: 2018-08-05T18:10:27.426771: step 595, loss 0.535731.
Train: 2018-08-05T18:10:27.582982: step 596, loss 0.505058.
Train: 2018-08-05T18:10:27.739221: step 597, loss 0.616237.
Train: 2018-08-05T18:10:27.911062: step 598, loss 0.57471.
Train: 2018-08-05T18:10:28.082866: step 599, loss 0.650348.
Train: 2018-08-05T18:10:28.239103: step 600, loss 0.572093.
Test: 2018-08-05T18:10:29.285707: step 600, loss 0.547501.
Train: 2018-08-05T18:10:30.160504: step 601, loss 0.562411.
Train: 2018-08-05T18:10:30.332369: step 602, loss 0.698203.
Train: 2018-08-05T18:10:30.488553: step 603, loss 0.554695.
Train: 2018-08-05T18:10:30.613548: step 604, loss 0.563496.
Train: 2018-08-05T18:10:30.769735: step 605, loss 0.556895.
Train: 2018-08-05T18:10:30.925950: step 606, loss 0.557869.
Train: 2018-08-05T18:10:31.082187: step 607, loss 0.565731.
Train: 2018-08-05T18:10:31.238401: step 608, loss 0.538037.
Train: 2018-08-05T18:10:31.394590: step 609, loss 0.58729.
Train: 2018-08-05T18:10:31.550804: step 610, loss 0.636335.
Test: 2018-08-05T18:10:32.597433: step 610, loss 0.554515.
Train: 2018-08-05T18:10:32.753645: step 611, loss 0.587379.
Train: 2018-08-05T18:10:32.909860: step 612, loss 0.580472.
Train: 2018-08-05T18:10:33.066073: step 613, loss 0.566558.
Train: 2018-08-05T18:10:33.222287: step 614, loss 0.552406.
Train: 2018-08-05T18:10:33.394128: step 615, loss 0.566001.
Train: 2018-08-05T18:10:33.550335: step 616, loss 0.587016.
Train: 2018-08-05T18:10:33.722203: step 617, loss 0.56509.
Train: 2018-08-05T18:10:33.878383: step 618, loss 0.557222.
Train: 2018-08-05T18:10:34.034621: step 619, loss 0.601723.
Train: 2018-08-05T18:10:34.175214: step 620, loss 0.594295.
Test: 2018-08-05T18:10:35.237439: step 620, loss 0.55049.
Train: 2018-08-05T18:10:35.393655: step 621, loss 0.571174.
Train: 2018-08-05T18:10:35.549898: step 622, loss 0.594576.
Train: 2018-08-05T18:10:35.706104: step 623, loss 0.610586.
Train: 2018-08-05T18:10:35.862294: step 624, loss 0.546974.
Train: 2018-08-05T18:10:36.018532: step 625, loss 0.562793.
Train: 2018-08-05T18:10:36.190373: step 626, loss 0.522095.
Train: 2018-08-05T18:10:36.362200: step 627, loss 0.554274.
Train: 2018-08-05T18:10:36.518416: step 628, loss 0.604277.
Train: 2018-08-05T18:10:36.674635: step 629, loss 0.536996.
Train: 2018-08-05T18:10:36.830817: step 630, loss 0.519536.
Test: 2018-08-05T18:10:37.877447: step 630, loss 0.547617.
Train: 2018-08-05T18:10:38.033660: step 631, loss 0.614597.
Train: 2018-08-05T18:10:38.189872: step 632, loss 0.615027.
Train: 2018-08-05T18:10:38.361707: step 633, loss 0.553622.
Train: 2018-08-05T18:10:38.502325: step 634, loss 0.579898.
Train: 2018-08-05T18:10:38.658539: step 635, loss 0.527463.
Train: 2018-08-05T18:10:38.830348: step 636, loss 0.536167.
Train: 2018-08-05T18:10:38.986561: step 637, loss 0.544852.
Train: 2018-08-05T18:10:39.142777: step 638, loss 0.580037.
Train: 2018-08-05T18:10:39.283398: step 639, loss 0.641815.
Train: 2018-08-05T18:10:39.439585: step 640, loss 0.588548.
Test: 2018-08-05T18:10:40.486209: step 640, loss 0.548753.
Train: 2018-08-05T18:10:40.642426: step 641, loss 0.61398.
Train: 2018-08-05T18:10:40.798636: step 642, loss 0.537044.
Train: 2018-08-05T18:10:40.954878: step 643, loss 0.595739.
Train: 2018-08-05T18:10:41.126686: step 644, loss 0.537988.
Train: 2018-08-05T18:10:41.282924: step 645, loss 0.587017.
Train: 2018-08-05T18:10:41.439142: step 646, loss 0.4745.
Train: 2018-08-05T18:10:41.595329: step 647, loss 0.546641.
Train: 2018-08-05T18:10:41.751570: step 648, loss 0.530197.
Train: 2018-08-05T18:10:41.923392: step 649, loss 0.488572.
Train: 2018-08-05T18:10:42.079618: step 650, loss 0.503571.
Test: 2018-08-05T18:10:43.126221: step 650, loss 0.547507.
Train: 2018-08-05T18:10:43.282436: step 651, loss 0.493189.
Train: 2018-08-05T18:10:43.438643: step 652, loss 0.544641.
Train: 2018-08-05T18:10:43.594858: step 653, loss 0.581485.
Train: 2018-08-05T18:10:43.751096: step 654, loss 0.620335.
Train: 2018-08-05T18:10:43.907310: step 655, loss 0.59249.
Train: 2018-08-05T18:10:44.079119: step 656, loss 0.611716.
Train: 2018-08-05T18:10:44.235334: step 657, loss 0.592028.
Train: 2018-08-05T18:10:44.407168: step 658, loss 0.609869.
Train: 2018-08-05T18:10:44.563380: step 659, loss 0.535418.
Train: 2018-08-05T18:10:44.719618: step 660, loss 0.526769.
Test: 2018-08-05T18:10:45.781844: step 660, loss 0.547288.
Train: 2018-08-05T18:10:45.922463: step 661, loss 0.571234.
Train: 2018-08-05T18:10:46.078675: step 662, loss 0.623164.
Train: 2018-08-05T18:10:46.234890: step 663, loss 0.570844.
Train: 2018-08-05T18:10:46.406699: step 664, loss 0.529139.
Train: 2018-08-05T18:10:46.562937: step 665, loss 0.529681.
Train: 2018-08-05T18:10:46.719125: step 666, loss 0.538113.
Train: 2018-08-05T18:10:46.875369: step 667, loss 0.529995.
Train: 2018-08-05T18:10:47.047206: step 668, loss 0.587152.
Train: 2018-08-05T18:10:47.187792: step 669, loss 0.529698.
Train: 2018-08-05T18:10:47.359601: step 670, loss 0.521109.
Test: 2018-08-05T18:10:48.406234: step 670, loss 0.547579.
Train: 2018-08-05T18:10:48.562445: step 671, loss 0.495168.
Train: 2018-08-05T18:10:48.718657: step 672, loss 0.605478.
Train: 2018-08-05T18:10:48.874873: step 673, loss 0.562387.
Train: 2018-08-05T18:10:49.031084: step 674, loss 0.500274.
Train: 2018-08-05T18:10:49.187299: step 675, loss 0.56272.
Train: 2018-08-05T18:10:49.343544: step 676, loss 0.544509.
Train: 2018-08-05T18:10:49.499755: step 677, loss 0.610296.
Train: 2018-08-05T18:10:49.655939: step 678, loss 0.61029.
Train: 2018-08-05T18:10:49.812152: step 679, loss 0.479771.
Train: 2018-08-05T18:10:49.968365: step 680, loss 0.535198.
Test: 2018-08-05T18:10:51.030647: step 680, loss 0.549227.
Train: 2018-08-05T18:10:51.186834: step 681, loss 0.582443.
Train: 2018-08-05T18:10:51.343044: step 682, loss 0.544512.
Train: 2018-08-05T18:10:51.499256: step 683, loss 0.590231.
Train: 2018-08-05T18:10:51.655485: step 684, loss 0.553603.
Train: 2018-08-05T18:10:51.811687: step 685, loss 0.509116.
Train: 2018-08-05T18:10:51.983544: step 686, loss 0.589038.
Train: 2018-08-05T18:10:52.155379: step 687, loss 0.597501.
Train: 2018-08-05T18:10:52.311597: step 688, loss 0.54501.
Train: 2018-08-05T18:10:52.467779: step 689, loss 0.570927.
Train: 2018-08-05T18:10:52.624018: step 690, loss 0.460172.
Test: 2018-08-05T18:10:53.670622: step 690, loss 0.547643.
Train: 2018-08-05T18:10:53.826836: step 691, loss 0.502404.
Train: 2018-08-05T18:10:53.983075: step 692, loss 0.597043.
Train: 2018-08-05T18:10:54.154886: step 693, loss 0.632221.
Train: 2018-08-05T18:10:54.311101: step 694, loss 0.597139.
Train: 2018-08-05T18:10:54.467313: step 695, loss 0.570957.
Train: 2018-08-05T18:10:54.623550: step 696, loss 0.630648.
Train: 2018-08-05T18:10:54.779762: step 697, loss 0.579133.
Train: 2018-08-05T18:10:54.935954: step 698, loss 0.538336.
Train: 2018-08-05T18:10:55.092165: step 699, loss 0.595205.
Train: 2018-08-05T18:10:55.248410: step 700, loss 0.586955.
Test: 2018-08-05T18:10:56.310663: step 700, loss 0.548475.
Train: 2018-08-05T18:10:57.279153: step 701, loss 0.530986.
Train: 2018-08-05T18:10:57.435391: step 702, loss 0.570919.
Train: 2018-08-05T18:10:57.591604: step 703, loss 0.547232.
Train: 2018-08-05T18:10:57.747818: step 704, loss 0.523477.
Train: 2018-08-05T18:10:57.904010: step 705, loss 0.554947.
Train: 2018-08-05T18:10:58.075866: step 706, loss 0.651296.
Train: 2018-08-05T18:10:58.232054: step 707, loss 0.546714.
Train: 2018-08-05T18:10:58.388268: step 708, loss 0.538567.
Train: 2018-08-05T18:10:58.544514: step 709, loss 0.587028.
Train: 2018-08-05T18:10:58.700695: step 710, loss 0.611568.
Test: 2018-08-05T18:10:59.762949: step 710, loss 0.549189.
Train: 2018-08-05T18:10:59.919161: step 711, loss 0.570774.
Train: 2018-08-05T18:11:00.075405: step 712, loss 0.635993.
Train: 2018-08-05T18:11:00.231587: step 713, loss 0.538459.
Train: 2018-08-05T18:11:00.387800: step 714, loss 0.578881.
Train: 2018-08-05T18:11:00.544030: step 715, loss 0.498477.
Train: 2018-08-05T18:11:00.700257: step 716, loss 0.611278.
Train: 2018-08-05T18:11:00.856478: step 717, loss 0.562683.
Train: 2018-08-05T18:11:01.012654: step 718, loss 0.505718.
Train: 2018-08-05T18:11:01.168867: step 719, loss 0.570758.
Train: 2018-08-05T18:11:01.325080: step 720, loss 0.529221.
Test: 2018-08-05T18:11:02.371735: step 720, loss 0.5483.
Train: 2018-08-05T18:11:02.527924: step 721, loss 0.570799.
Train: 2018-08-05T18:11:02.684168: step 722, loss 0.579399.
Train: 2018-08-05T18:11:02.840381: step 723, loss 0.579547.
Train: 2018-08-05T18:11:02.996563: step 724, loss 0.484454.
Train: 2018-08-05T18:11:03.152802: step 725, loss 0.500908.
Train: 2018-08-05T18:11:03.308991: step 726, loss 0.625336.
Train: 2018-08-05T18:11:03.465205: step 727, loss 0.589809.
Train: 2018-08-05T18:11:03.621418: step 728, loss 0.481048.
Train: 2018-08-05T18:11:03.777631: step 729, loss 0.562868.
Train: 2018-08-05T18:11:03.933870: step 730, loss 0.52601.
Test: 2018-08-05T18:11:04.980500: step 730, loss 0.547172.
Train: 2018-08-05T18:11:05.136718: step 731, loss 0.507167.
Train: 2018-08-05T18:11:05.292901: step 732, loss 0.478336.
Train: 2018-08-05T18:11:05.433531: step 733, loss 0.496478.
Train: 2018-08-05T18:11:05.589734: step 734, loss 0.554963.
Train: 2018-08-05T18:11:05.761573: step 735, loss 0.655942.
Train: 2018-08-05T18:11:05.917786: step 736, loss 0.63548.
Train: 2018-08-05T18:11:06.073992: step 737, loss 0.505675.
Train: 2018-08-05T18:11:06.245837: step 738, loss 0.525405.
Train: 2018-08-05T18:11:06.402055: step 739, loss 0.573456.
Train: 2018-08-05T18:11:06.558257: step 740, loss 0.53512.
Test: 2018-08-05T18:11:07.604860: step 740, loss 0.549307.
Train: 2018-08-05T18:11:07.761072: step 741, loss 0.581796.
Train: 2018-08-05T18:11:07.932908: step 742, loss 0.56284.
Train: 2018-08-05T18:11:08.089152: step 743, loss 0.562599.
Train: 2018-08-05T18:11:08.245334: step 744, loss 0.527064.
Train: 2018-08-05T18:11:08.401550: step 745, loss 0.553634.
Train: 2018-08-05T18:11:08.542140: step 746, loss 0.596935.
Train: 2018-08-05T18:11:08.698379: step 747, loss 0.528227.
Train: 2018-08-05T18:11:08.854570: step 748, loss 0.570818.
Train: 2018-08-05T18:11:09.010812: step 749, loss 0.570783.
Train: 2018-08-05T18:11:09.166994: step 750, loss 0.537454.
Test: 2018-08-05T18:11:10.229244: step 750, loss 0.548401.
Train: 2018-08-05T18:11:10.369842: step 751, loss 0.579061.
Train: 2018-08-05T18:11:10.526075: step 752, loss 0.496335.
Train: 2018-08-05T18:11:10.697885: step 753, loss 0.63732.
Train: 2018-08-05T18:11:10.854124: step 754, loss 0.595616.
Train: 2018-08-05T18:11:10.979085: step 755, loss 0.597651.
Train: 2018-08-05T18:11:11.135284: step 756, loss 0.61156.
Train: 2018-08-05T18:11:11.291497: step 757, loss 0.506412.
Train: 2018-08-05T18:11:11.447741: step 758, loss 0.562817.
Train: 2018-08-05T18:11:11.603955: step 759, loss 0.562834.
Train: 2018-08-05T18:11:11.775758: step 760, loss 0.562826.
Test: 2018-08-05T18:11:12.822388: step 760, loss 0.548631.
Train: 2018-08-05T18:11:12.978604: step 761, loss 0.594949.
Train: 2018-08-05T18:11:13.134841: step 762, loss 0.538695.
Train: 2018-08-05T18:11:13.306680: step 763, loss 0.595022.
Train: 2018-08-05T18:11:13.447243: step 764, loss 0.603129.
Train: 2018-08-05T18:11:13.603480: step 765, loss 0.57888.
Train: 2018-08-05T18:11:13.759672: step 766, loss 0.538666.
Train: 2018-08-05T18:11:13.931503: step 767, loss 0.546639.
Train: 2018-08-05T18:11:14.087716: step 768, loss 0.546476.
Train: 2018-08-05T18:11:14.243929: step 769, loss 0.562592.
Train: 2018-08-05T18:11:14.400143: step 770, loss 0.603754.
Test: 2018-08-05T18:11:15.462397: step 770, loss 0.548454.
Train: 2018-08-05T18:11:15.618641: step 771, loss 0.562475.
Train: 2018-08-05T18:11:15.774821: step 772, loss 0.487583.
Train: 2018-08-05T18:11:15.931035: step 773, loss 0.520181.
Train: 2018-08-05T18:11:16.087273: step 774, loss 0.536545.
Train: 2018-08-05T18:11:16.243463: step 775, loss 0.579932.
Train: 2018-08-05T18:11:16.399704: step 776, loss 0.562493.
Train: 2018-08-05T18:11:16.555891: step 777, loss 0.562621.
Train: 2018-08-05T18:11:16.712127: step 778, loss 0.608265.
Train: 2018-08-05T18:11:16.868342: step 779, loss 0.571864.
Train: 2018-08-05T18:11:17.024532: step 780, loss 0.580886.
Test: 2018-08-05T18:11:18.071159: step 780, loss 0.54756.
Train: 2018-08-05T18:11:18.242994: step 781, loss 0.56263.
Train: 2018-08-05T18:11:18.399207: step 782, loss 0.616269.
Train: 2018-08-05T18:11:18.555420: step 783, loss 0.553602.
Train: 2018-08-05T18:11:18.711661: step 784, loss 0.562355.
Train: 2018-08-05T18:11:18.867847: step 785, loss 0.570923.
Train: 2018-08-05T18:11:19.024069: step 786, loss 0.579313.
Train: 2018-08-05T18:11:19.180300: step 787, loss 0.579144.
Train: 2018-08-05T18:11:19.336490: step 788, loss 0.587273.
Train: 2018-08-05T18:11:19.492729: step 789, loss 0.61149.
Train: 2018-08-05T18:11:19.664573: step 790, loss 0.578864.
Test: 2018-08-05T18:11:20.711169: step 790, loss 0.549057.
Train: 2018-08-05T18:11:20.867381: step 791, loss 0.531615.
Train: 2018-08-05T18:11:21.023620: step 792, loss 0.555457.
Train: 2018-08-05T18:11:21.179805: step 793, loss 0.516688.
Train: 2018-08-05T18:11:21.336019: step 794, loss 0.547648.
Train: 2018-08-05T18:11:21.492233: step 795, loss 0.586739.
Train: 2018-08-05T18:11:21.648470: step 796, loss 0.547169.
Train: 2018-08-05T18:11:21.804660: step 797, loss 0.530867.
Train: 2018-08-05T18:11:21.960873: step 798, loss 0.546443.
Train: 2018-08-05T18:11:22.117087: step 799, loss 0.529507.
Train: 2018-08-05T18:11:22.273301: step 800, loss 0.570794.
Test: 2018-08-05T18:11:23.319930: step 800, loss 0.549016.
Train: 2018-08-05T18:11:24.272863: step 801, loss 0.570901.
Train: 2018-08-05T18:11:24.444667: step 802, loss 0.501507.
Train: 2018-08-05T18:11:24.600885: step 803, loss 0.642329.
Train: 2018-08-05T18:11:24.772714: step 804, loss 0.544652.
Train: 2018-08-05T18:11:24.928954: step 805, loss 0.598618.
Train: 2018-08-05T18:11:25.085143: step 806, loss 0.571615.
Train: 2018-08-05T18:11:25.241384: step 807, loss 0.571565.
Train: 2018-08-05T18:11:25.413192: step 808, loss 0.616173.
Train: 2018-08-05T18:11:25.569408: step 809, loss 0.465331.
Train: 2018-08-05T18:11:25.725617: step 810, loss 0.456118.
Test: 2018-08-05T18:11:26.772247: step 810, loss 0.547762.
Train: 2018-08-05T18:11:26.928490: step 811, loss 0.59883.
Train: 2018-08-05T18:11:27.084699: step 812, loss 0.553661.
Train: 2018-08-05T18:11:27.256508: step 813, loss 0.599725.
Train: 2018-08-05T18:11:27.412722: step 814, loss 0.617913.
Train: 2018-08-05T18:11:27.600180: step 815, loss 0.571674.
Train: 2018-08-05T18:11:27.772012: step 816, loss 0.589082.
Train: 2018-08-05T18:11:27.928250: step 817, loss 0.492894.
Train: 2018-08-05T18:11:28.100092: step 818, loss 0.570945.
Train: 2018-08-05T18:11:28.256305: step 819, loss 0.587932.
Train: 2018-08-05T18:11:28.428108: step 820, loss 0.579228.
Test: 2018-08-05T18:11:29.474763: step 820, loss 0.549888.
Train: 2018-08-05T18:11:29.693438: step 821, loss 0.629.
Train: 2018-08-05T18:11:29.849682: step 822, loss 0.497431.
Train: 2018-08-05T18:11:30.005866: step 823, loss 0.586984.
Train: 2018-08-05T18:11:30.162079: step 824, loss 0.570843.
Train: 2018-08-05T18:11:30.318318: step 825, loss 0.507037.
Train: 2018-08-05T18:11:30.490157: step 826, loss 0.546839.
Train: 2018-08-05T18:11:30.646339: step 827, loss 0.50635.
Train: 2018-08-05T18:11:30.818179: step 828, loss 0.51355.
Train: 2018-08-05T18:11:30.974388: step 829, loss 0.562427.
Train: 2018-08-05T18:11:31.130626: step 830, loss 0.587837.
Test: 2018-08-05T18:11:32.192851: step 830, loss 0.546955.
Train: 2018-08-05T18:11:32.364686: step 831, loss 0.467555.
Train: 2018-08-05T18:11:32.520901: step 832, loss 0.500624.
Train: 2018-08-05T18:11:32.677117: step 833, loss 0.580931.
Train: 2018-08-05T18:11:32.833326: step 834, loss 0.618846.
Train: 2018-08-05T18:11:33.005191: step 835, loss 0.535154.
Train: 2018-08-05T18:11:33.161376: step 836, loss 0.5067.
Train: 2018-08-05T18:11:33.317590: step 837, loss 0.583043.
Train: 2018-08-05T18:11:33.489423: step 838, loss 0.612207.
Train: 2018-08-05T18:11:33.645638: step 839, loss 0.515922.
Train: 2018-08-05T18:11:33.817496: step 840, loss 0.535091.
Test: 2018-08-05T18:11:34.864101: step 840, loss 0.546895.
Train: 2018-08-05T18:11:35.035939: step 841, loss 0.649499.
Train: 2018-08-05T18:11:35.207807: step 842, loss 0.525783.
Train: 2018-08-05T18:11:35.364016: step 843, loss 0.655522.
Train: 2018-08-05T18:11:35.535857: step 844, loss 0.517548.
Train: 2018-08-05T18:11:35.692063: step 845, loss 0.562438.
Train: 2018-08-05T18:11:35.848278: step 846, loss 0.501559.
Train: 2018-08-05T18:11:36.004459: step 847, loss 0.613967.
Train: 2018-08-05T18:11:36.176295: step 848, loss 0.562351.
Train: 2018-08-05T18:11:36.332532: step 849, loss 0.554031.
Train: 2018-08-05T18:11:36.488722: step 850, loss 0.562468.
Test: 2018-08-05T18:11:37.550972: step 850, loss 0.548993.
Train: 2018-08-05T18:11:37.707185: step 851, loss 0.636538.
Train: 2018-08-05T18:11:37.863398: step 852, loss 0.489789.
Train: 2018-08-05T18:11:38.019614: step 853, loss 0.570812.
Train: 2018-08-05T18:11:38.191447: step 854, loss 0.554726.
Train: 2018-08-05T18:11:38.347688: step 855, loss 0.594975.
Train: 2018-08-05T18:11:38.503873: step 856, loss 0.554777.
Train: 2018-08-05T18:11:38.675740: step 857, loss 0.562802.
Train: 2018-08-05T18:11:38.831923: step 858, loss 0.498376.
Train: 2018-08-05T18:11:39.003757: step 859, loss 0.57078.
Train: 2018-08-05T18:11:39.159973: step 860, loss 0.595416.
Test: 2018-08-05T18:11:40.222223: step 860, loss 0.548865.
Train: 2018-08-05T18:11:40.378434: step 861, loss 0.529406.
Train: 2018-08-05T18:11:40.550269: step 862, loss 0.545701.
Train: 2018-08-05T18:11:40.706484: step 863, loss 0.672307.
Train: 2018-08-05T18:11:40.862697: step 864, loss 0.579245.
Train: 2018-08-05T18:11:41.018936: step 865, loss 0.57079.
Train: 2018-08-05T18:11:41.175157: step 866, loss 0.696423.
Train: 2018-08-05T18:11:41.346962: step 867, loss 0.505.
Train: 2018-08-05T18:11:41.503174: step 868, loss 0.538195.
Train: 2018-08-05T18:11:41.659385: step 869, loss 0.627577.
Train: 2018-08-05T18:11:41.815600: step 870, loss 0.659195.
Test: 2018-08-05T18:11:42.862229: step 870, loss 0.549734.
Train: 2018-08-05T18:11:43.018442: step 871, loss 0.539426.
Train: 2018-08-05T18:11:43.190285: step 872, loss 0.485349.
Train: 2018-08-05T18:11:43.346515: step 873, loss 0.610083.
Train: 2018-08-05T18:11:43.502707: step 874, loss 0.516708.
Train: 2018-08-05T18:11:43.674541: step 875, loss 0.524209.
Train: 2018-08-05T18:11:43.830757: step 876, loss 0.586757.
Train: 2018-08-05T18:11:43.986965: step 877, loss 0.52311.
Train: 2018-08-05T18:11:44.158802: step 878, loss 0.530424.
Train: 2018-08-05T18:11:44.330634: step 879, loss 0.513204.
Train: 2018-08-05T18:11:44.486878: step 880, loss 0.511911.
Test: 2018-08-05T18:11:45.533477: step 880, loss 0.549102.
Train: 2018-08-05T18:11:45.705314: step 881, loss 0.545057.
Train: 2018-08-05T18:11:45.861528: step 882, loss 0.509234.
Train: 2018-08-05T18:11:46.033361: step 883, loss 0.581064.
Train: 2018-08-05T18:11:46.189604: step 884, loss 0.525842.
Train: 2018-08-05T18:11:46.345819: step 885, loss 0.60192.
Train: 2018-08-05T18:11:46.517648: step 886, loss 0.689621.
Train: 2018-08-05T18:11:46.673836: step 887, loss 0.53509.
Train: 2018-08-05T18:11:46.830051: step 888, loss 0.573069.
Train: 2018-08-05T18:11:47.001916: step 889, loss 0.619575.
Train: 2018-08-05T18:11:47.173719: step 890, loss 0.581285.
Test: 2018-08-05T18:11:48.220351: step 890, loss 0.54778.
Train: 2018-08-05T18:11:48.376593: step 891, loss 0.517637.
Train: 2018-08-05T18:11:48.548424: step 892, loss 0.580137.
Train: 2018-08-05T18:11:48.704611: step 893, loss 0.562354.
Train: 2018-08-05T18:11:48.876470: step 894, loss 0.588013.
Train: 2018-08-05T18:11:49.032659: step 895, loss 0.562379.
Train: 2018-08-05T18:11:49.204523: step 896, loss 0.479562.
Train: 2018-08-05T18:11:49.360706: step 897, loss 0.529442.
Train: 2018-08-05T18:11:49.516951: step 898, loss 0.587308.
Train: 2018-08-05T18:11:49.688754: step 899, loss 0.521138.
Train: 2018-08-05T18:11:49.844994: step 900, loss 0.57076.
Test: 2018-08-05T18:11:50.891624: step 900, loss 0.548873.
Train: 2018-08-05T18:11:51.828887: step 901, loss 0.595797.
Train: 2018-08-05T18:11:51.985094: step 902, loss 0.529025.
Train: 2018-08-05T18:11:52.141335: step 903, loss 0.579171.
Train: 2018-08-05T18:11:52.313171: step 904, loss 0.612862.
Train: 2018-08-05T18:11:52.484975: step 905, loss 0.528819.
Train: 2018-08-05T18:11:52.609946: step 906, loss 0.544442.
Train: 2018-08-05T18:11:52.781811: step 907, loss 0.646803.
Train: 2018-08-05T18:11:52.937993: step 908, loss 0.562387.
Train: 2018-08-05T18:11:53.094209: step 909, loss 0.604225.
Train: 2018-08-05T18:11:53.266042: step 910, loss 0.520975.
Test: 2018-08-05T18:11:54.312702: step 910, loss 0.549216.
Train: 2018-08-05T18:11:54.468885: step 911, loss 0.579039.
Train: 2018-08-05T18:11:54.625101: step 912, loss 0.579016.
Train: 2018-08-05T18:11:54.796934: step 913, loss 0.595452.
Train: 2018-08-05T18:11:54.968768: step 914, loss 0.505292.
Train: 2018-08-05T18:11:55.124982: step 915, loss 0.611768.
Train: 2018-08-05T18:11:55.296844: step 916, loss 0.554404.
Train: 2018-08-05T18:11:55.453061: step 917, loss 0.595302.
Train: 2018-08-05T18:11:55.624890: step 918, loss 0.538152.
Train: 2018-08-05T18:11:55.796703: step 919, loss 0.48095.
Train: 2018-08-05T18:11:55.952912: step 920, loss 0.5212.
Test: 2018-08-05T18:11:56.999567: step 920, loss 0.548114.
Train: 2018-08-05T18:11:57.155760: step 921, loss 0.477153.
Train: 2018-08-05T18:11:57.327623: step 922, loss 0.626611.
Train: 2018-08-05T18:11:57.499425: step 923, loss 0.571735.
Train: 2018-08-05T18:11:57.655663: step 924, loss 0.661038.
Train: 2018-08-05T18:11:57.811852: step 925, loss 0.527397.
Train: 2018-08-05T18:11:57.968090: step 926, loss 0.579829.
Train: 2018-08-05T18:11:58.139900: step 927, loss 0.579768.
Train: 2018-08-05T18:11:58.296146: step 928, loss 0.622973.
Train: 2018-08-05T18:11:58.467949: step 929, loss 0.639376.
Train: 2018-08-05T18:11:58.624194: step 930, loss 0.587582.
Test: 2018-08-05T18:11:59.686440: step 930, loss 0.548001.
Train: 2018-08-05T18:11:59.842658: step 931, loss 0.570757.
Train: 2018-08-05T18:11:59.998844: step 932, loss 0.586994.
Train: 2018-08-05T18:12:00.170675: step 933, loss 0.507179.
Train: 2018-08-05T18:12:00.342511: step 934, loss 0.531433.
Train: 2018-08-05T18:12:00.498723: step 935, loss 0.578863.
Train: 2018-08-05T18:12:00.670587: step 936, loss 0.484321.
Train: 2018-08-05T18:12:00.826803: step 937, loss 0.578858.
Train: 2018-08-05T18:12:00.998605: step 938, loss 0.586872.
Train: 2018-08-05T18:12:01.154851: step 939, loss 0.514445.
Train: 2018-08-05T18:12:01.326655: step 940, loss 0.578927.
Test: 2018-08-05T18:12:02.388930: step 940, loss 0.548197.
Train: 2018-08-05T18:12:02.545119: step 941, loss 0.595466.
Train: 2018-08-05T18:12:02.732575: step 942, loss 0.612166.
Train: 2018-08-05T18:12:02.904412: step 943, loss 0.562468.
Train: 2018-08-05T18:12:03.076254: step 944, loss 0.454509.
Train: 2018-08-05T18:12:03.248082: step 945, loss 0.553957.
Train: 2018-08-05T18:12:03.419916: step 946, loss 0.570874.
Train: 2018-08-05T18:12:03.576128: step 947, loss 0.536434.
Train: 2018-08-05T18:12:03.747962: step 948, loss 0.562377.
Train: 2018-08-05T18:12:03.904176: step 949, loss 0.606682.
Train: 2018-08-05T18:12:04.076011: step 950, loss 0.491385.
Test: 2018-08-05T18:12:05.122640: step 950, loss 0.547576.
Train: 2018-08-05T18:12:05.278879: step 951, loss 0.625464.
Train: 2018-08-05T18:12:05.450736: step 952, loss 0.517615.
Train: 2018-08-05T18:12:05.606929: step 953, loss 0.51745.
Train: 2018-08-05T18:12:05.763142: step 954, loss 0.626565.
Train: 2018-08-05T18:12:05.934980: step 955, loss 0.562739.
Train: 2018-08-05T18:12:06.106784: step 956, loss 0.526417.
Train: 2018-08-05T18:12:06.263024: step 957, loss 0.617085.
Train: 2018-08-05T18:12:06.419243: step 958, loss 0.598561.
Train: 2018-08-05T18:12:06.591073: step 959, loss 0.642358.
Train: 2018-08-05T18:12:06.747260: step 960, loss 0.493926.
Test: 2018-08-05T18:12:07.793916: step 960, loss 0.548588.
Train: 2018-08-05T18:12:07.965726: step 961, loss 0.588093.
Train: 2018-08-05T18:12:08.121964: step 962, loss 0.638716.
Train: 2018-08-05T18:12:08.293775: step 963, loss 0.554121.
Train: 2018-08-05T18:12:08.450012: step 964, loss 0.529822.
Train: 2018-08-05T18:12:08.621822: step 965, loss 0.514039.
Train: 2018-08-05T18:12:08.778035: step 966, loss 0.554632.
Train: 2018-08-05T18:12:08.934250: step 967, loss 0.611222.
Train: 2018-08-05T18:12:09.106115: step 968, loss 0.554725.
Train: 2018-08-05T18:12:09.262321: step 969, loss 0.54672.
Train: 2018-08-05T18:12:09.434158: step 970, loss 0.474206.
Test: 2018-08-05T18:12:10.480760: step 970, loss 0.548183.
Train: 2018-08-05T18:12:10.652598: step 971, loss 0.611964.
Train: 2018-08-05T18:12:10.808840: step 972, loss 0.521549.
Train: 2018-08-05T18:12:10.965058: step 973, loss 0.562468.
Train: 2018-08-05T18:12:11.121240: step 974, loss 0.62943.
Train: 2018-08-05T18:12:11.293071: step 975, loss 0.537172.
Train: 2018-08-05T18:12:11.480526: step 976, loss 0.570814.
Train: 2018-08-05T18:12:11.636766: step 977, loss 0.536871.
Train: 2018-08-05T18:12:11.792953: step 978, loss 0.639309.
Train: 2018-08-05T18:12:11.964822: step 979, loss 0.570875.
Train: 2018-08-05T18:12:12.121007: step 980, loss 0.587886.
Test: 2018-08-05T18:12:13.183253: step 980, loss 0.548007.
Train: 2018-08-05T18:12:13.339493: step 981, loss 0.545407.
Train: 2018-08-05T18:12:13.511331: step 982, loss 0.579267.
Train: 2018-08-05T18:12:13.683166: step 983, loss 0.537107.
Train: 2018-08-05T18:12:13.839374: step 984, loss 0.553955.
Train: 2018-08-05T18:12:14.011211: step 985, loss 0.570801.
Train: 2018-08-05T18:12:14.167397: step 986, loss 0.520206.
Train: 2018-08-05T18:12:14.339233: step 987, loss 0.579305.
Train: 2018-08-05T18:12:14.495446: step 988, loss 0.519832.
Train: 2018-08-05T18:12:14.667281: step 989, loss 0.579468.
Train: 2018-08-05T18:12:14.839117: step 990, loss 0.553728.
Test: 2018-08-05T18:12:15.885750: step 990, loss 0.548688.
Train: 2018-08-05T18:12:16.057611: step 991, loss 0.52773.
Train: 2018-08-05T18:12:16.229415: step 992, loss 0.588537.
Train: 2018-08-05T18:12:16.385628: step 993, loss 0.562382.
Train: 2018-08-05T18:12:16.557489: step 994, loss 0.474544.
Train: 2018-08-05T18:12:16.713677: step 995, loss 0.45581.
Train: 2018-08-05T18:12:16.885511: step 996, loss 0.599041.
Train: 2018-08-05T18:12:17.041749: step 997, loss 0.507682.
Train: 2018-08-05T18:12:17.213586: step 998, loss 0.535166.
Train: 2018-08-05T18:12:17.369775: step 999, loss 0.506569.
Train: 2018-08-05T18:12:17.541639: step 1000, loss 0.602859.
Test: 2018-08-05T18:12:18.588238: step 1000, loss 0.548099.
Train: 2018-08-05T18:12:19.509897: step 1001, loss 0.544881.
Train: 2018-08-05T18:12:19.666112: step 1002, loss 0.613586.
Train: 2018-08-05T18:12:19.837945: step 1003, loss 0.46687.
Train: 2018-08-05T18:12:20.009779: step 1004, loss 0.613339.
Train: 2018-08-05T18:12:20.165993: step 1005, loss 0.554508.
Train: 2018-08-05T18:12:20.337852: step 1006, loss 0.525481.
Train: 2018-08-05T18:12:20.494046: step 1007, loss 0.592309.
Train: 2018-08-05T18:12:20.665878: step 1008, loss 0.47868.
Train: 2018-08-05T18:12:20.822124: step 1009, loss 0.55389.
Train: 2018-08-05T18:12:20.993925: step 1010, loss 0.637578.
Test: 2018-08-05T18:12:22.040553: step 1010, loss 0.547331.
Train: 2018-08-05T18:12:22.212414: step 1011, loss 0.553678.
Train: 2018-08-05T18:12:22.384254: step 1012, loss 0.562622.
Train: 2018-08-05T18:12:22.556082: step 1013, loss 0.553589.
Train: 2018-08-05T18:12:22.727896: step 1014, loss 0.562385.
Train: 2018-08-05T18:12:22.884130: step 1015, loss 0.51908.
Train: 2018-08-05T18:12:23.055972: step 1016, loss 0.553743.
Train: 2018-08-05T18:12:23.212154: step 1017, loss 0.545247.
Train: 2018-08-05T18:12:23.384020: step 1018, loss 0.562341.
Train: 2018-08-05T18:12:23.540202: step 1019, loss 0.528362.
Train: 2018-08-05T18:12:23.712064: step 1020, loss 0.630406.
Test: 2018-08-05T18:12:24.758694: step 1020, loss 0.547649.
Train: 2018-08-05T18:12:24.930501: step 1021, loss 0.503151.
Train: 2018-08-05T18:12:25.102337: step 1022, loss 0.647044.
Train: 2018-08-05T18:12:25.258550: step 1023, loss 0.562384.
Train: 2018-08-05T18:12:25.430385: step 1024, loss 0.52901.
Train: 2018-08-05T18:12:25.586600: step 1025, loss 0.579103.
Train: 2018-08-05T18:12:25.758435: step 1026, loss 0.595708.
Train: 2018-08-05T18:12:25.914683: step 1027, loss 0.587301.
Train: 2018-08-05T18:12:26.086482: step 1028, loss 0.595415.
Train: 2018-08-05T18:12:26.258317: step 1029, loss 0.546324.
Train: 2018-08-05T18:12:26.430156: step 1030, loss 0.530233.
Test: 2018-08-05T18:12:27.476780: step 1030, loss 0.549688.
Train: 2018-08-05T18:12:27.648616: step 1031, loss 0.538339.
Train: 2018-08-05T18:12:27.804831: step 1032, loss 0.562634.
Train: 2018-08-05T18:12:27.976664: step 1033, loss 0.529865.
Train: 2018-08-05T18:12:28.148503: step 1034, loss 0.52951.
Train: 2018-08-05T18:12:28.304716: step 1035, loss 0.579115.
Train: 2018-08-05T18:12:28.492169: step 1036, loss 0.579226.
Train: 2018-08-05T18:12:28.664003: step 1037, loss 0.553861.
Train: 2018-08-05T18:12:28.835838: step 1038, loss 0.605095.
Train: 2018-08-05T18:12:29.007705: step 1039, loss 0.579477.
Train: 2018-08-05T18:12:29.179534: step 1040, loss 0.596625.
Test: 2018-08-05T18:12:30.226162: step 1040, loss 0.548072.
Train: 2018-08-05T18:12:30.397971: step 1041, loss 0.605047.
Train: 2018-08-05T18:12:30.585427: step 1042, loss 0.604734.
Train: 2018-08-05T18:12:30.741668: step 1043, loss 0.545633.
Train: 2018-08-05T18:12:30.913476: step 1044, loss 0.52086.
Train: 2018-08-05T18:12:31.085310: step 1045, loss 0.562459.
Train: 2018-08-05T18:12:31.257148: step 1046, loss 0.579048.
Train: 2018-08-05T18:12:31.428979: step 1047, loss 0.504568.
Train: 2018-08-05T18:12:31.585196: step 1048, loss 0.495929.
Train: 2018-08-05T18:12:31.757054: step 1049, loss 0.579209.
Train: 2018-08-05T18:12:31.928894: step 1050, loss 0.570843.
Test: 2018-08-05T18:12:32.991114: step 1050, loss 0.548626.
Train: 2018-08-05T18:12:33.147353: step 1051, loss 0.570899.
Train: 2018-08-05T18:12:33.319193: step 1052, loss 0.605417.
Train: 2018-08-05T18:12:33.475376: step 1053, loss 0.622694.
Train: 2018-08-05T18:12:33.647211: step 1054, loss 0.493769.
Train: 2018-08-05T18:12:33.819077: step 1055, loss 0.596683.
Train: 2018-08-05T18:12:33.975259: step 1056, loss 0.545197.
Train: 2018-08-05T18:12:34.115851: step 1057, loss 0.507495.
Train: 2018-08-05T18:12:34.272066: step 1058, loss 0.570955.
Train: 2018-08-05T18:12:34.443901: step 1059, loss 0.570997.
Train: 2018-08-05T18:12:34.631385: step 1060, loss 0.544997.
Test: 2018-08-05T18:12:35.677987: step 1060, loss 0.546011.
Train: 2018-08-05T18:12:35.849819: step 1061, loss 0.597192.
Train: 2018-08-05T18:12:36.006060: step 1062, loss 0.536253.
Train: 2018-08-05T18:12:36.177896: step 1063, loss 0.579793.
Train: 2018-08-05T18:12:36.349705: step 1064, loss 0.527526.
Train: 2018-08-05T18:12:36.521538: step 1065, loss 0.571099.
Train: 2018-08-05T18:12:36.693399: step 1066, loss 0.579848.
Train: 2018-08-05T18:12:36.849589: step 1067, loss 0.571089.
Train: 2018-08-05T18:12:37.021421: step 1068, loss 0.536262.
Train: 2018-08-05T18:12:37.177635: step 1069, loss 0.492787.
Train: 2018-08-05T18:12:37.349469: step 1070, loss 0.597409.
Test: 2018-08-05T18:12:38.411721: step 1070, loss 0.548546.
Train: 2018-08-05T18:12:38.583588: step 1071, loss 0.579937.
Train: 2018-08-05T18:12:38.755420: step 1072, loss 0.597447.
Train: 2018-08-05T18:12:38.911630: step 1073, loss 0.579782.
Train: 2018-08-05T18:12:39.083469: step 1074, loss 0.468201.
Train: 2018-08-05T18:12:39.239679: step 1075, loss 0.571052.
Train: 2018-08-05T18:12:39.411490: step 1076, loss 0.571072.
Train: 2018-08-05T18:12:39.583353: step 1077, loss 0.501346.
Train: 2018-08-05T18:12:39.739565: step 1078, loss 0.571168.
Train: 2018-08-05T18:12:39.911395: step 1079, loss 0.632941.
Train: 2018-08-05T18:12:40.083212: step 1080, loss 0.544841.
Test: 2018-08-05T18:12:41.129833: step 1080, loss 0.547159.
Train: 2018-08-05T18:12:41.301699: step 1081, loss 0.536126.
Train: 2018-08-05T18:12:41.457884: step 1082, loss 0.562377.
Train: 2018-08-05T18:12:41.629716: step 1083, loss 0.588601.
Train: 2018-08-05T18:12:41.801552: step 1084, loss 0.614595.
Train: 2018-08-05T18:12:41.973387: step 1085, loss 0.510616.
Train: 2018-08-05T18:12:42.129599: step 1086, loss 0.553744.
Train: 2018-08-05T18:12:42.301434: step 1087, loss 0.553764.
Train: 2018-08-05T18:12:42.457672: step 1088, loss 0.510966.
Train: 2018-08-05T18:12:42.629482: step 1089, loss 0.536532.
Train: 2018-08-05T18:12:42.785696: step 1090, loss 0.553684.
Test: 2018-08-05T18:12:43.847947: step 1090, loss 0.546597.
Train: 2018-08-05T18:12:44.004200: step 1091, loss 0.588522.
Train: 2018-08-05T18:12:44.176026: step 1092, loss 0.501168.
Train: 2018-08-05T18:12:44.332211: step 1093, loss 0.518317.
Train: 2018-08-05T18:12:44.504043: step 1094, loss 0.589302.
Train: 2018-08-05T18:12:44.660259: step 1095, loss 0.544608.
Train: 2018-08-05T18:12:44.832092: step 1096, loss 0.607917.
Train: 2018-08-05T18:12:45.003926: step 1097, loss 0.526481.
Train: 2018-08-05T18:12:45.160172: step 1098, loss 0.571748.
Train: 2018-08-05T18:12:45.331974: step 1099, loss 0.562675.
Train: 2018-08-05T18:12:45.503809: step 1100, loss 0.689139.
Test: 2018-08-05T18:12:46.566060: step 1100, loss 0.547848.
Train: 2018-08-05T18:12:47.468020: step 1101, loss 0.518093.
Train: 2018-08-05T18:12:47.639851: step 1102, loss 0.544852.
Train: 2018-08-05T18:12:47.811687: step 1103, loss 0.553666.
Train: 2018-08-05T18:12:47.967898: step 1104, loss 0.631268.
Train: 2018-08-05T18:12:48.139757: step 1105, loss 0.494436.
Train: 2018-08-05T18:12:48.311568: step 1106, loss 0.570807.
Train: 2018-08-05T18:12:48.483402: step 1107, loss 0.579176.
Train: 2018-08-05T18:12:48.639617: step 1108, loss 0.612454.
Train: 2018-08-05T18:12:48.811482: step 1109, loss 0.587256.
Train: 2018-08-05T18:12:48.983284: step 1110, loss 0.6034.
Test: 2018-08-05T18:12:50.029919: step 1110, loss 0.549337.
Train: 2018-08-05T18:12:50.201749: step 1111, loss 0.586924.
Train: 2018-08-05T18:12:50.357963: step 1112, loss 0.547086.
Train: 2018-08-05T18:12:50.529798: step 1113, loss 0.570991.
Train: 2018-08-05T18:12:50.701658: step 1114, loss 0.586705.
Train: 2018-08-05T18:12:50.857881: step 1115, loss 0.571137.
Train: 2018-08-05T18:12:51.014060: step 1116, loss 0.563464.
Train: 2018-08-05T18:12:51.185900: step 1117, loss 0.64067.
Train: 2018-08-05T18:12:51.357754: step 1118, loss 0.586648.
Train: 2018-08-05T18:12:51.513968: step 1119, loss 0.563845.
Train: 2018-08-05T18:12:51.685777: step 1120, loss 0.594225.
Test: 2018-08-05T18:12:52.732407: step 1120, loss 0.552156.
Train: 2018-08-05T18:12:52.904272: step 1121, loss 0.50375.
Train: 2018-08-05T18:12:53.076108: step 1122, loss 0.571509.
Train: 2018-08-05T18:12:53.247914: step 1123, loss 0.540951.
Train: 2018-08-05T18:12:53.404125: step 1124, loss 0.532814.
Train: 2018-08-05T18:12:53.575992: step 1125, loss 0.602297.
Train: 2018-08-05T18:12:53.732199: step 1126, loss 0.547325.
Train: 2018-08-05T18:12:53.904035: step 1127, loss 0.602834.
Train: 2018-08-05T18:12:54.060222: step 1128, loss 0.506341.
Train: 2018-08-05T18:12:54.232095: step 1129, loss 0.546209.
Train: 2018-08-05T18:12:54.388272: step 1130, loss 0.504147.
Test: 2018-08-05T18:12:55.450520: step 1130, loss 0.547223.
Train: 2018-08-05T18:12:55.606736: step 1131, loss 0.485153.
Train: 2018-08-05T18:12:55.778569: step 1132, loss 0.489426.
Train: 2018-08-05T18:12:55.950430: step 1133, loss 0.50484.
Train: 2018-08-05T18:12:56.106618: step 1134, loss 0.506781.
Train: 2018-08-05T18:12:56.278479: step 1135, loss 0.581.
Train: 2018-08-05T18:12:56.450313: step 1136, loss 0.528095.
Train: 2018-08-05T18:12:56.622122: step 1137, loss 0.620922.
Train: 2018-08-05T18:12:56.793988: step 1138, loss 0.584111.
Train: 2018-08-05T18:12:56.950196: step 1139, loss 0.525703.
Train: 2018-08-05T18:12:57.122005: step 1140, loss 0.535189.
Test: 2018-08-05T18:12:58.168661: step 1140, loss 0.548409.
Train: 2018-08-05T18:12:58.340468: step 1141, loss 0.461323.
Train: 2018-08-05T18:12:58.496685: step 1142, loss 0.600062.
Train: 2018-08-05T18:12:58.668542: step 1143, loss 0.544512.
Train: 2018-08-05T18:12:58.824733: step 1144, loss 0.562835.
Train: 2018-08-05T18:12:58.996566: step 1145, loss 0.489944.
Train: 2018-08-05T18:12:59.168425: step 1146, loss 0.608287.
Train: 2018-08-05T18:12:59.340240: step 1147, loss 0.553617.
Train: 2018-08-05T18:12:59.496481: step 1148, loss 0.571595.
Train: 2018-08-05T18:12:59.668282: step 1149, loss 0.598233.
Train: 2018-08-05T18:12:59.840121: step 1150, loss 0.580069.
Test: 2018-08-05T18:13:00.886747: step 1150, loss 0.550223.
Train: 2018-08-05T18:13:01.058583: step 1151, loss 0.597182.
Train: 2018-08-05T18:13:01.230416: step 1152, loss 0.536634.
Train: 2018-08-05T18:13:01.386630: step 1153, loss 0.680898.
Train: 2018-08-05T18:13:01.558493: step 1154, loss 0.620434.
Train: 2018-08-05T18:13:01.714679: step 1155, loss 0.538512.
Train: 2018-08-05T18:13:01.870892: step 1156, loss 0.626411.
Train: 2018-08-05T18:13:02.042729: step 1157, loss 0.563396.
Train: 2018-08-05T18:13:02.198967: step 1158, loss 0.510365.
Train: 2018-08-05T18:13:02.370807: step 1159, loss 0.518548.
Train: 2018-08-05T18:13:02.527019: step 1160, loss 0.60178.
Test: 2018-08-05T18:13:03.589240: step 1160, loss 0.551242.
Train: 2018-08-05T18:13:03.761075: step 1161, loss 0.58666.
Train: 2018-08-05T18:13:03.917288: step 1162, loss 0.556529.
Train: 2018-08-05T18:13:04.089123: step 1163, loss 0.579116.
Train: 2018-08-05T18:13:04.245339: step 1164, loss 0.586657.
Train: 2018-08-05T18:13:04.401550: step 1165, loss 0.533622.
Train: 2018-08-05T18:13:04.573387: step 1166, loss 0.502722.
Train: 2018-08-05T18:13:04.745222: step 1167, loss 0.52471.
Train: 2018-08-05T18:13:04.901434: step 1168, loss 0.570938.
Train: 2018-08-05T18:13:05.073267: step 1169, loss 0.505703.
Train: 2018-08-05T18:13:05.229481: step 1170, loss 0.605595.
Test: 2018-08-05T18:13:06.291735: step 1170, loss 0.547768.
Train: 2018-08-05T18:13:06.447945: step 1171, loss 0.614859.
Train: 2018-08-05T18:13:06.619806: step 1172, loss 0.562335.
Train: 2018-08-05T18:13:06.776022: step 1173, loss 0.647696.
Train: 2018-08-05T18:13:06.932238: step 1174, loss 0.570827.
Train: 2018-08-05T18:13:07.104042: step 1175, loss 0.528671.
Train: 2018-08-05T18:13:07.260260: step 1176, loss 0.629747.
Train: 2018-08-05T18:13:07.432094: step 1177, loss 0.612601.
Train: 2018-08-05T18:13:07.588304: step 1178, loss 0.512774.
Train: 2018-08-05T18:13:07.744518: step 1179, loss 0.579014.
Train: 2018-08-05T18:13:07.916354: step 1180, loss 0.521369.
Test: 2018-08-05T18:13:08.963007: step 1180, loss 0.549116.
Train: 2018-08-05T18:13:09.134816: step 1181, loss 0.546015.
Train: 2018-08-05T18:13:09.291039: step 1182, loss 0.554195.
Train: 2018-08-05T18:13:09.462869: step 1183, loss 0.587409.
Train: 2018-08-05T18:13:09.634699: step 1184, loss 0.537387.
Train: 2018-08-05T18:13:09.790914: step 1185, loss 0.537217.
Train: 2018-08-05T18:13:09.947151: step 1186, loss 0.604654.
Train: 2018-08-05T18:13:10.118961: step 1187, loss 0.536895.
Train: 2018-08-05T18:13:10.290799: step 1188, loss 0.485547.
Train: 2018-08-05T18:13:10.447011: step 1189, loss 0.545046.
Train: 2018-08-05T18:13:10.618846: step 1190, loss 0.492212.
Test: 2018-08-05T18:13:11.665476: step 1190, loss 0.546757.
Train: 2018-08-05T18:13:11.837310: step 1191, loss 0.508748.
Train: 2018-08-05T18:13:11.993550: step 1192, loss 0.636831.
Train: 2018-08-05T18:13:12.165357: step 1193, loss 0.544512.
Train: 2018-08-05T18:13:12.321573: step 1194, loss 0.553882.
Train: 2018-08-05T18:13:12.493405: step 1195, loss 0.600857.
Train: 2018-08-05T18:13:12.665266: step 1196, loss 0.563157.
Train: 2018-08-05T18:13:12.821456: step 1197, loss 0.572235.
Train: 2018-08-05T18:13:12.977701: step 1198, loss 0.645149.
Train: 2018-08-05T18:13:13.149502: step 1199, loss 0.571287.
Train: 2018-08-05T18:13:13.305715: step 1200, loss 0.562559.
Test: 2018-08-05T18:13:14.367966: step 1200, loss 0.549161.
Train: 2018-08-05T18:13:15.352111: step 1201, loss 0.595103.
Train: 2018-08-05T18:13:15.508324: step 1202, loss 0.564217.
Train: 2018-08-05T18:13:15.680159: step 1203, loss 0.602242.
Train: 2018-08-05T18:13:15.836372: step 1204, loss 0.479784.
Train: 2018-08-05T18:13:15.992585: step 1205, loss 0.505131.
Train: 2018-08-05T18:13:16.148800: step 1206, loss 0.631394.
Train: 2018-08-05T18:13:16.320635: step 1207, loss 0.596776.
Train: 2018-08-05T18:13:16.445606: step 1208, loss 0.544033.
Train: 2018-08-05T18:13:16.617470: step 1209, loss 0.579476.
Train: 2018-08-05T18:13:16.789305: step 1210, loss 0.553788.
Test: 2018-08-05T18:13:17.835903: step 1210, loss 0.547101.
Train: 2018-08-05T18:13:18.007769: step 1211, loss 0.605017.
Train: 2018-08-05T18:13:18.163983: step 1212, loss 0.596312.
Train: 2018-08-05T18:13:18.320166: step 1213, loss 0.537094.
Train: 2018-08-05T18:13:18.492003: step 1214, loss 0.595953.
Train: 2018-08-05T18:13:18.663835: step 1215, loss 0.520752.
Train: 2018-08-05T18:13:18.820073: step 1216, loss 0.554117.
Train: 2018-08-05T18:13:18.991882: step 1217, loss 0.48755.
Train: 2018-08-05T18:13:19.163748: step 1218, loss 0.503728.
Train: 2018-08-05T18:13:19.319930: step 1219, loss 0.511442.
Train: 2018-08-05T18:13:19.491766: step 1220, loss 0.562337.
Test: 2018-08-05T18:13:20.538396: step 1220, loss 0.546965.
Train: 2018-08-05T18:13:20.710230: step 1221, loss 0.606102.
Train: 2018-08-05T18:13:20.866446: step 1222, loss 0.580056.
Train: 2018-08-05T18:13:21.038280: step 1223, loss 0.535865.
Train: 2018-08-05T18:13:21.194492: step 1224, loss 0.589268.
Train: 2018-08-05T18:13:21.366357: step 1225, loss 0.598287.
Train: 2018-08-05T18:13:21.538162: step 1226, loss 0.535754.
Train: 2018-08-05T18:13:21.710021: step 1227, loss 0.598147.
Train: 2018-08-05T18:13:21.866237: step 1228, loss 0.553591.
Train: 2018-08-05T18:13:22.038044: step 1229, loss 0.52711.
Train: 2018-08-05T18:13:22.194261: step 1230, loss 0.544783.
Test: 2018-08-05T18:13:23.256510: step 1230, loss 0.547288.
Train: 2018-08-05T18:13:23.428375: step 1231, loss 0.562418.
Train: 2018-08-05T18:13:23.600180: step 1232, loss 0.562413.
Train: 2018-08-05T18:13:23.772039: step 1233, loss 0.536013.
Train: 2018-08-05T18:13:23.943875: step 1234, loss 0.553606.
Train: 2018-08-05T18:13:24.100089: step 1235, loss 0.606461.
Train: 2018-08-05T18:13:24.271923: step 1236, loss 0.483434.
Train: 2018-08-05T18:13:24.443732: step 1237, loss 0.571209.
Train: 2018-08-05T18:13:24.599979: step 1238, loss 0.500723.
Train: 2018-08-05T18:13:24.771780: step 1239, loss 0.518108.
Train: 2018-08-05T18:13:24.943614: step 1240, loss 0.517763.
Test: 2018-08-05T18:13:26.005867: step 1240, loss 0.549427.
Train: 2018-08-05T18:13:26.177726: step 1241, loss 0.544556.
Train: 2018-08-05T18:13:26.349536: step 1242, loss 0.535339.
Train: 2018-08-05T18:13:26.505748: step 1243, loss 0.581657.
Train: 2018-08-05T18:13:26.661987: step 1244, loss 0.516487.
Train: 2018-08-05T18:13:26.833824: step 1245, loss 0.582255.
Train: 2018-08-05T18:13:26.990011: step 1246, loss 0.572916.
Train: 2018-08-05T18:13:27.146223: step 1247, loss 0.572857.
Train: 2018-08-05T18:13:27.318059: step 1248, loss 0.610194.
Train: 2018-08-05T18:13:27.474273: step 1249, loss 0.525992.
Train: 2018-08-05T18:13:27.692970: step 1250, loss 0.553695.
Test: 2018-08-05T18:13:28.739631: step 1250, loss 0.547543.
Train: 2018-08-05T18:13:28.911462: step 1251, loss 0.571853.
Train: 2018-08-05T18:13:29.083305: step 1252, loss 0.490489.
Train: 2018-08-05T18:13:29.239514: step 1253, loss 0.517637.
Train: 2018-08-05T18:13:29.411350: step 1254, loss 0.535608.
Train: 2018-08-05T18:13:29.583158: step 1255, loss 0.625704.
Train: 2018-08-05T18:13:29.739366: step 1256, loss 0.562555.
Train: 2018-08-05T18:13:29.926854: step 1257, loss 0.580314.
Train: 2018-08-05T18:13:30.098658: step 1258, loss 0.500574.
Train: 2018-08-05T18:13:30.254873: step 1259, loss 0.544783.
Train: 2018-08-05T18:13:30.426706: step 1260, loss 0.588847.
Test: 2018-08-05T18:13:31.473334: step 1260, loss 0.548139.
Train: 2018-08-05T18:13:31.676439: step 1261, loss 0.58872.
Train: 2018-08-05T18:13:31.848273: step 1262, loss 0.536208.
Train: 2018-08-05T18:13:32.020112: step 1263, loss 0.527613.
Train: 2018-08-05T18:13:32.176298: step 1264, loss 0.579708.
Train: 2018-08-05T18:13:32.348130: step 1265, loss 0.519052.
Train: 2018-08-05T18:13:32.504368: step 1266, loss 0.571019.
Train: 2018-08-05T18:13:32.660582: step 1267, loss 0.544999.
Train: 2018-08-05T18:13:32.832416: step 1268, loss 0.579729.
Train: 2018-08-05T18:13:32.988630: step 1269, loss 0.544983.
Train: 2018-08-05T18:13:33.160472: step 1270, loss 0.597117.
Test: 2018-08-05T18:13:34.222702: step 1270, loss 0.547872.
Train: 2018-08-05T18:13:34.378936: step 1271, loss 0.545013.
Train: 2018-08-05T18:13:34.550771: step 1272, loss 0.60562.
Train: 2018-08-05T18:13:34.722574: step 1273, loss 0.588161.
Train: 2018-08-05T18:13:34.878791: step 1274, loss 0.587972.
Train: 2018-08-05T18:13:35.050653: step 1275, loss 0.621632.
Train: 2018-08-05T18:13:35.206836: step 1276, loss 0.595774.
Train: 2018-08-05T18:13:35.378670: step 1277, loss 0.562947.
Train: 2018-08-05T18:13:35.534889: step 1278, loss 0.530827.
Train: 2018-08-05T18:13:35.691123: step 1279, loss 0.586984.
Train: 2018-08-05T18:13:35.862961: step 1280, loss 0.586908.
Test: 2018-08-05T18:13:36.925208: step 1280, loss 0.550316.
Train: 2018-08-05T18:13:37.097017: step 1281, loss 0.586838.
Train: 2018-08-05T18:13:37.268853: step 1282, loss 0.61048.
Train: 2018-08-05T18:13:37.425091: step 1283, loss 0.517919.
Train: 2018-08-05T18:13:37.581280: step 1284, loss 0.539833.
Train: 2018-08-05T18:13:37.753140: step 1285, loss 0.508249.
Train: 2018-08-05T18:13:37.909353: step 1286, loss 0.523505.
Train: 2018-08-05T18:13:38.081187: step 1287, loss 0.570862.
Train: 2018-08-05T18:13:38.252997: step 1288, loss 0.586984.
Train: 2018-08-05T18:13:38.424835: step 1289, loss 0.521787.
Train: 2018-08-05T18:13:38.581072: step 1290, loss 0.570756.
Test: 2018-08-05T18:13:39.643296: step 1290, loss 0.548832.
Train: 2018-08-05T18:13:39.799542: step 1291, loss 0.562411.
Train: 2018-08-05T18:13:39.971349: step 1292, loss 0.520109.
Train: 2018-08-05T18:13:40.127584: step 1293, loss 0.545201.
Train: 2018-08-05T18:13:40.299394: step 1294, loss 0.518914.
Train: 2018-08-05T18:13:40.455608: step 1295, loss 0.518275.
Train: 2018-08-05T18:13:40.627444: step 1296, loss 0.544605.
Train: 2018-08-05T18:13:40.799277: step 1297, loss 0.535374.
Train: 2018-08-05T18:13:40.971150: step 1298, loss 0.591026.
Train: 2018-08-05T18:13:41.127325: step 1299, loss 0.563336.
Train: 2018-08-05T18:13:41.283538: step 1300, loss 0.554037.
Test: 2018-08-05T18:13:42.345790: step 1300, loss 0.549709.
Train: 2018-08-05T18:13:43.220586: step 1301, loss 0.563613.
Train: 2018-08-05T18:13:43.392446: step 1302, loss 0.592199.
Train: 2018-08-05T18:13:43.548633: step 1303, loss 0.516158.
Train: 2018-08-05T18:13:43.704846: step 1304, loss 0.639152.
Train: 2018-08-05T18:13:43.876713: step 1305, loss 0.507128.
Train: 2018-08-05T18:13:44.032897: step 1306, loss 0.60021.
Train: 2018-08-05T18:13:44.204760: step 1307, loss 0.590386.
Train: 2018-08-05T18:13:44.360944: step 1308, loss 0.562651.
Train: 2018-08-05T18:13:44.532778: step 1309, loss 0.580314.
Train: 2018-08-05T18:13:44.688990: step 1310, loss 0.500987.
Test: 2018-08-05T18:13:45.751241: step 1310, loss 0.547432.
Train: 2018-08-05T18:13:45.907456: step 1311, loss 0.58844.
Train: 2018-08-05T18:13:46.063668: step 1312, loss 0.57094.
Train: 2018-08-05T18:13:46.235534: step 1313, loss 0.587891.
Train: 2018-08-05T18:13:46.407339: step 1314, loss 0.604476.
Train: 2018-08-05T18:13:46.579175: step 1315, loss 0.628911.
Train: 2018-08-05T18:13:46.735412: step 1316, loss 0.554439.
Train: 2018-08-05T18:13:46.907221: step 1317, loss 0.570823.
Train: 2018-08-05T18:13:47.079082: step 1318, loss 0.523149.
Train: 2018-08-05T18:13:47.235269: step 1319, loss 0.531405.
Train: 2018-08-05T18:13:47.407107: step 1320, loss 0.578862.
Test: 2018-08-05T18:13:48.453733: step 1320, loss 0.549746.
Train: 2018-08-05T18:13:48.625600: step 1321, loss 0.555211.
Train: 2018-08-05T18:13:48.797435: step 1322, loss 0.594642.
Train: 2018-08-05T18:13:48.969241: step 1323, loss 0.523673.
Train: 2018-08-05T18:13:49.125455: step 1324, loss 0.570944.
Train: 2018-08-05T18:13:49.297290: step 1325, loss 0.586807.
Train: 2018-08-05T18:13:49.453526: step 1326, loss 0.499134.
Train: 2018-08-05T18:13:49.625365: step 1327, loss 0.586919.
Train: 2018-08-05T18:13:49.781551: step 1328, loss 0.554589.
Train: 2018-08-05T18:13:49.937788: step 1329, loss 0.521755.
Train: 2018-08-05T18:13:50.109622: step 1330, loss 0.636861.
Test: 2018-08-05T18:13:51.171849: step 1330, loss 0.54859.
Train: 2018-08-05T18:13:51.343681: step 1331, loss 0.612273.
Train: 2018-08-05T18:13:51.515519: step 1332, loss 0.603995.
Train: 2018-08-05T18:13:51.671730: step 1333, loss 0.562467.
Train: 2018-08-05T18:13:51.843591: step 1334, loss 0.612146.
Train: 2018-08-05T18:13:52.015403: step 1335, loss 0.620185.
Train: 2018-08-05T18:13:52.187237: step 1336, loss 0.611615.
Train: 2018-08-05T18:13:52.343482: step 1337, loss 0.570806.
Train: 2018-08-05T18:13:52.515282: step 1338, loss 0.56285.
Train: 2018-08-05T18:13:52.687120: step 1339, loss 0.555009.
Train: 2018-08-05T18:13:52.843358: step 1340, loss 0.491822.
Test: 2018-08-05T18:13:53.905612: step 1340, loss 0.548828.
Train: 2018-08-05T18:13:54.061796: step 1341, loss 0.594726.
Train: 2018-08-05T18:13:54.233634: step 1342, loss 0.523261.
Train: 2018-08-05T18:13:54.405467: step 1343, loss 0.562891.
Train: 2018-08-05T18:13:54.577327: step 1344, loss 0.602968.
Train: 2018-08-05T18:13:54.749162: step 1345, loss 0.570821.
Train: 2018-08-05T18:13:54.921002: step 1346, loss 0.570805.
Train: 2018-08-05T18:13:55.077185: step 1347, loss 0.587012.
Train: 2018-08-05T18:13:55.249048: step 1348, loss 0.546411.
Train: 2018-08-05T18:13:55.420851: step 1349, loss 0.546307.
Train: 2018-08-05T18:13:55.592723: step 1350, loss 0.619968.
Test: 2018-08-05T18:13:56.639318: step 1350, loss 0.549028.
Train: 2018-08-05T18:13:56.811155: step 1351, loss 0.56255.
Train: 2018-08-05T18:13:56.982986: step 1352, loss 0.513192.
Train: 2018-08-05T18:13:57.154821: step 1353, loss 0.603856.
Train: 2018-08-05T18:13:57.311068: step 1354, loss 0.56246.
Train: 2018-08-05T18:13:57.482903: step 1355, loss 0.529146.
Train: 2018-08-05T18:13:57.639108: step 1356, loss 0.55403.
Train: 2018-08-05T18:13:57.810922: step 1357, loss 0.553945.
Train: 2018-08-05T18:13:57.982784: step 1358, loss 0.579319.
Train: 2018-08-05T18:13:58.107725: step 1359, loss 0.635093.
Train: 2018-08-05T18:13:58.279584: step 1360, loss 0.528297.
Test: 2018-08-05T18:13:59.326189: step 1360, loss 0.548105.
Train: 2018-08-05T18:13:59.498023: step 1361, loss 0.596435.
Train: 2018-08-05T18:13:59.669885: step 1362, loss 0.553832.
Train: 2018-08-05T18:13:59.857313: step 1363, loss 0.553838.
Train: 2018-08-05T18:14:00.013531: step 1364, loss 0.5198.
Train: 2018-08-05T18:14:00.200994: step 1365, loss 0.57088.
Train: 2018-08-05T18:14:00.372818: step 1366, loss 0.562335.
Train: 2018-08-05T18:14:00.544677: step 1367, loss 0.5881.
Train: 2018-08-05T18:14:00.700898: step 1368, loss 0.622453.
Train: 2018-08-05T18:14:00.872701: step 1369, loss 0.63069.
Train: 2018-08-05T18:14:01.044537: step 1370, loss 0.621552.
Test: 2018-08-05T18:14:02.091167: step 1370, loss 0.548885.
Train: 2018-08-05T18:14:02.263000: step 1371, loss 0.604119.
Train: 2018-08-05T18:14:02.434836: step 1372, loss 0.554339.
Train: 2018-08-05T18:14:02.591049: step 1373, loss 0.538358.
Train: 2018-08-05T18:14:02.762883: step 1374, loss 0.498458.
Train: 2018-08-05T18:14:02.919096: step 1375, loss 0.482497.
Train: 2018-08-05T18:14:03.090965: step 1376, loss 0.546561.
Train: 2018-08-05T18:14:03.262765: step 1377, loss 0.619665.
Train: 2018-08-05T18:14:03.418979: step 1378, loss 0.627977.
Train: 2018-08-05T18:14:03.590818: step 1379, loss 0.54629.
Train: 2018-08-05T18:14:03.762674: step 1380, loss 0.546269.
Test: 2018-08-05T18:14:04.809278: step 1380, loss 0.549651.
Train: 2018-08-05T18:14:04.981114: step 1381, loss 0.546195.
Train: 2018-08-05T18:14:05.137351: step 1382, loss 0.603666.
Train: 2018-08-05T18:14:05.309167: step 1383, loss 0.578996.
Train: 2018-08-05T18:14:05.481027: step 1384, loss 0.636711.
Train: 2018-08-05T18:14:05.637210: step 1385, loss 0.505121.
Train: 2018-08-05T18:14:05.809046: step 1386, loss 0.578973.
Train: 2018-08-05T18:14:05.965257: step 1387, loss 0.537896.
Train: 2018-08-05T18:14:06.137123: step 1388, loss 0.562517.
Train: 2018-08-05T18:14:06.308952: step 1389, loss 0.579023.
Train: 2018-08-05T18:14:06.480763: step 1390, loss 0.595612.
Test: 2018-08-05T18:14:07.527392: step 1390, loss 0.549973.
Train: 2018-08-05T18:14:07.699253: step 1391, loss 0.562473.
Train: 2018-08-05T18:14:07.855440: step 1392, loss 0.628768.
Train: 2018-08-05T18:14:08.027276: step 1393, loss 0.537754.
Train: 2018-08-05T18:14:08.199136: step 1394, loss 0.5378.
Train: 2018-08-05T18:14:08.355326: step 1395, loss 0.554255.
Train: 2018-08-05T18:14:08.511537: step 1396, loss 0.545941.
Train: 2018-08-05T18:14:08.683398: step 1397, loss 0.512613.
Train: 2018-08-05T18:14:08.855242: step 1398, loss 0.595907.
Train: 2018-08-05T18:14:09.011444: step 1399, loss 0.545536.
Train: 2018-08-05T18:14:09.183254: step 1400, loss 0.55388.
Test: 2018-08-05T18:14:10.245506: step 1400, loss 0.547308.
Train: 2018-08-05T18:14:11.151546: step 1401, loss 0.52822.
Train: 2018-08-05T18:14:11.323412: step 1402, loss 0.510701.
Train: 2018-08-05T18:14:11.495216: step 1403, loss 0.544937.
Train: 2018-08-05T18:14:11.667073: step 1404, loss 0.553601.
Train: 2018-08-05T18:14:11.838882: step 1405, loss 0.535753.
Train: 2018-08-05T18:14:12.010748: step 1406, loss 0.544585.
Train: 2018-08-05T18:14:12.182553: step 1407, loss 0.480687.
Train: 2018-08-05T18:14:12.354390: step 1408, loss 0.535243.
Train: 2018-08-05T18:14:12.510633: step 1409, loss 0.572766.
Train: 2018-08-05T18:14:12.682435: step 1410, loss 0.582633.
Test: 2018-08-05T18:14:13.729064: step 1410, loss 0.548021.
Train: 2018-08-05T18:14:13.900927: step 1411, loss 0.515978.
Train: 2018-08-05T18:14:14.072762: step 1412, loss 0.467754.
Train: 2018-08-05T18:14:14.244569: step 1413, loss 0.535119.
Train: 2018-08-05T18:14:14.416407: step 1414, loss 0.584371.
Train: 2018-08-05T18:14:14.588239: step 1415, loss 0.515449.
Train: 2018-08-05T18:14:14.760100: step 1416, loss 0.515398.
Train: 2018-08-05T18:14:14.916317: step 1417, loss 0.585171.
Train: 2018-08-05T18:14:15.103770: step 1418, loss 0.535282.
Train: 2018-08-05T18:14:15.259981: step 1419, loss 0.634637.
Train: 2018-08-05T18:14:15.431830: step 1420, loss 0.574401.
Test: 2018-08-05T18:14:16.494044: step 1420, loss 0.549372.
Train: 2018-08-05T18:14:16.650288: step 1421, loss 0.515932.
Train: 2018-08-05T18:14:16.822125: step 1422, loss 0.611407.
Train: 2018-08-05T18:14:16.993930: step 1423, loss 0.56325.
Train: 2018-08-05T18:14:17.150164: step 1424, loss 0.571933.
Train: 2018-08-05T18:14:17.321976: step 1425, loss 0.571022.
Train: 2018-08-05T18:14:17.493844: step 1426, loss 0.539847.
Train: 2018-08-05T18:14:17.650026: step 1427, loss 0.545626.
Train: 2018-08-05T18:14:17.821891: step 1428, loss 0.5711.
Train: 2018-08-05T18:14:17.993690: step 1429, loss 0.571038.
Train: 2018-08-05T18:14:18.165526: step 1430, loss 0.528053.
Test: 2018-08-05T18:14:19.227778: step 1430, loss 0.54849.
Train: 2018-08-05T18:14:19.399615: step 1431, loss 0.460008.
Train: 2018-08-05T18:14:19.571471: step 1432, loss 0.545218.
Train: 2018-08-05T18:14:19.743312: step 1433, loss 0.502141.
Train: 2018-08-05T18:14:19.915141: step 1434, loss 0.544996.
Train: 2018-08-05T18:14:20.086954: step 1435, loss 0.553625.
Train: 2018-08-05T18:14:20.258785: step 1436, loss 0.500613.
Train: 2018-08-05T18:14:20.430627: step 1437, loss 0.60722.
Train: 2018-08-05T18:14:20.602480: step 1438, loss 0.499624.
Train: 2018-08-05T18:14:20.774291: step 1439, loss 0.644492.
Train: 2018-08-05T18:14:20.946124: step 1440, loss 0.471787.
Test: 2018-08-05T18:14:22.008375: step 1440, loss 0.548807.
Train: 2018-08-05T18:14:22.180213: step 1441, loss 0.572.
Train: 2018-08-05T18:14:22.352078: step 1442, loss 0.618108.
Train: 2018-08-05T18:14:22.523908: step 1443, loss 0.599589.
Train: 2018-08-05T18:14:22.695742: step 1444, loss 0.58101.
Train: 2018-08-05T18:14:22.867551: step 1445, loss 0.571693.
Train: 2018-08-05T18:14:23.055018: step 1446, loss 0.517886.
Train: 2018-08-05T18:14:23.226841: step 1447, loss 0.535781.
Train: 2018-08-05T18:14:23.398710: step 1448, loss 0.58907.
Train: 2018-08-05T18:14:23.586132: step 1449, loss 0.562417.
Train: 2018-08-05T18:14:23.757993: step 1450, loss 0.623709.
Test: 2018-08-05T18:14:24.804621: step 1450, loss 0.546674.
Train: 2018-08-05T18:14:24.976432: step 1451, loss 0.588347.
Train: 2018-08-05T18:14:25.163922: step 1452, loss 0.49382.
Train: 2018-08-05T18:14:25.335721: step 1453, loss 0.613451.
Train: 2018-08-05T18:14:25.507583: step 1454, loss 0.562364.
Train: 2018-08-05T18:14:25.695037: step 1455, loss 0.629443.
Train: 2018-08-05T18:14:25.866879: step 1456, loss 0.554188.
Train: 2018-08-05T18:14:26.054305: step 1457, loss 0.464045.
Train: 2018-08-05T18:14:26.241790: step 1458, loss 0.562554.
Train: 2018-08-05T18:14:26.429240: step 1459, loss 0.611809.
Train: 2018-08-05T18:14:26.616673: step 1460, loss 0.554388.
Test: 2018-08-05T18:14:27.663303: step 1460, loss 0.549116.
Train: 2018-08-05T18:14:27.850759: step 1461, loss 0.562587.
Train: 2018-08-05T18:14:28.022595: step 1462, loss 0.51353.
Train: 2018-08-05T18:14:28.210050: step 1463, loss 0.57076.
Train: 2018-08-05T18:14:28.397504: step 1464, loss 0.546036.
Train: 2018-08-05T18:14:28.569342: step 1465, loss 0.595604.
Train: 2018-08-05T18:14:28.756823: step 1466, loss 0.562455.
Train: 2018-08-05T18:14:28.944282: step 1467, loss 0.545779.
Train: 2018-08-05T18:14:29.131732: step 1468, loss 0.5875.
Train: 2018-08-05T18:14:29.303569: step 1469, loss 0.604311.
Train: 2018-08-05T18:14:29.491030: step 1470, loss 0.537271.
Test: 2018-08-05T18:14:30.553251: step 1470, loss 0.548961.
Train: 2018-08-05T18:14:30.740706: step 1471, loss 0.554003.
Train: 2018-08-05T18:14:30.912542: step 1472, loss 0.545563.
Train: 2018-08-05T18:14:31.115649: step 1473, loss 0.596125.
Train: 2018-08-05T18:14:31.287485: step 1474, loss 0.579259.
Train: 2018-08-05T18:14:31.474936: step 1475, loss 0.655268.
Train: 2018-08-05T18:14:31.662396: step 1476, loss 0.612717.
Train: 2018-08-05T18:14:31.849821: step 1477, loss 0.587372.
Train: 2018-08-05T18:14:32.052901: step 1478, loss 0.562536.
Train: 2018-08-05T18:14:32.224736: step 1479, loss 0.521852.
Train: 2018-08-05T18:14:32.427812: step 1480, loss 0.481441.
Test: 2018-08-05T18:14:33.474466: step 1480, loss 0.548267.
Train: 2018-08-05T18:14:33.661927: step 1481, loss 0.562627.
Train: 2018-08-05T18:14:33.849380: step 1482, loss 0.578946.
Train: 2018-08-05T18:14:34.036809: step 1483, loss 0.652781.
Train: 2018-08-05T18:14:34.239886: step 1484, loss 0.538073.
Train: 2018-08-05T18:14:34.427342: step 1485, loss 0.63612.
Train: 2018-08-05T18:14:34.614799: step 1486, loss 0.522027.
Train: 2018-08-05T18:14:34.802287: step 1487, loss 0.570788.
Train: 2018-08-05T18:14:34.989711: step 1488, loss 0.54645.
Train: 2018-08-05T18:14:35.192788: step 1489, loss 0.660157.
Train: 2018-08-05T18:14:35.380245: step 1490, loss 0.554638.
Test: 2018-08-05T18:14:36.426874: step 1490, loss 0.548005.
Train: 2018-08-05T18:14:36.614335: step 1491, loss 0.562762.
Train: 2018-08-05T18:14:36.817410: step 1492, loss 0.570831.
Train: 2018-08-05T18:14:37.004889: step 1493, loss 0.570839.
Train: 2018-08-05T18:14:37.192321: step 1494, loss 0.635043.
Train: 2018-08-05T18:14:37.395399: step 1495, loss 0.514987.
Train: 2018-08-05T18:14:37.582880: step 1496, loss 0.562894.
Train: 2018-08-05T18:14:37.785962: step 1497, loss 0.506937.
Train: 2018-08-05T18:14:37.989009: step 1498, loss 0.514291.
Train: 2018-08-05T18:14:38.176490: step 1499, loss 0.553891.
Train: 2018-08-05T18:14:38.379543: step 1500, loss 0.582361.
Test: 2018-08-05T18:14:39.426173: step 1500, loss 0.546965.
Train: 2018-08-05T18:14:40.504070: step 1501, loss 0.616365.
Train: 2018-08-05T18:14:40.707147: step 1502, loss 0.537063.
Train: 2018-08-05T18:14:40.894584: step 1503, loss 0.562474.
Train: 2018-08-05T18:14:41.097659: step 1504, loss 0.628775.
Train: 2018-08-05T18:14:41.285139: step 1505, loss 0.488037.
Train: 2018-08-05T18:14:41.488191: step 1506, loss 0.53753.
Train: 2018-08-05T18:14:41.691299: step 1507, loss 0.595847.
Train: 2018-08-05T18:14:41.894346: step 1508, loss 0.537232.
Train: 2018-08-05T18:14:42.097447: step 1509, loss 0.579233.
Train: 2018-08-05T18:14:42.253667: step 1510, loss 0.652629.
Test: 2018-08-05T18:14:43.315887: step 1510, loss 0.548086.
Train: 2018-08-05T18:14:43.518994: step 1511, loss 0.528625.
Train: 2018-08-05T18:14:43.722067: step 1512, loss 0.629843.
Train: 2018-08-05T18:14:43.925149: step 1513, loss 0.478464.
Train: 2018-08-05T18:14:44.128221: step 1514, loss 0.587611.
Train: 2018-08-05T18:14:44.331309: step 1515, loss 0.56238.
Train: 2018-08-05T18:14:44.534350: step 1516, loss 0.596043.
Train: 2018-08-05T18:14:44.737433: step 1517, loss 0.486774.
Train: 2018-08-05T18:14:44.956162: step 1518, loss 0.58768.
Train: 2018-08-05T18:14:45.159229: step 1519, loss 0.545448.
Train: 2018-08-05T18:14:45.377903: step 1520, loss 0.604773.
Test: 2018-08-05T18:14:46.424536: step 1520, loss 0.546812.
Train: 2018-08-05T18:14:46.627614: step 1521, loss 0.596289.
Train: 2018-08-05T18:14:46.830714: step 1522, loss 0.503104.
Train: 2018-08-05T18:14:47.049390: step 1523, loss 0.56235.
Train: 2018-08-05T18:14:47.268110: step 1524, loss 0.698425.
Train: 2018-08-05T18:14:47.471165: step 1525, loss 0.570805.
Train: 2018-08-05T18:14:47.689894: step 1526, loss 0.562403.
Train: 2018-08-05T18:14:47.892964: step 1527, loss 0.57908.
Train: 2018-08-05T18:14:48.127263: step 1528, loss 0.570756.
Train: 2018-08-05T18:14:48.330362: step 1529, loss 0.537903.
Train: 2018-08-05T18:14:48.549037: step 1530, loss 0.570764.
Test: 2018-08-05T18:14:49.611319: step 1530, loss 0.549336.
Train: 2018-08-05T18:14:49.829989: step 1531, loss 0.595274.
Train: 2018-08-05T18:14:50.048684: step 1532, loss 0.53824.
Train: 2018-08-05T18:14:50.267384: step 1533, loss 0.538279.
Train: 2018-08-05T18:14:50.486108: step 1534, loss 0.587058.
Train: 2018-08-05T18:14:50.720434: step 1535, loss 0.619632.
Train: 2018-08-05T18:14:50.939136: step 1536, loss 0.562671.
Train: 2018-08-05T18:14:51.157800: step 1537, loss 0.562694.
Train: 2018-08-05T18:14:51.376502: step 1538, loss 0.48986.
Train: 2018-08-05T18:14:51.595224: step 1539, loss 0.587054.
Train: 2018-08-05T18:14:51.813921: step 1540, loss 0.505429.
Test: 2018-08-05T18:14:52.876148: step 1540, loss 0.548957.
Train: 2018-08-05T18:14:53.094873: step 1541, loss 0.537815.
Train: 2018-08-05T18:14:53.313545: step 1542, loss 0.462637.
Train: 2018-08-05T18:14:53.547866: step 1543, loss 0.553901.
Train: 2018-08-05T18:14:53.766596: step 1544, loss 0.579523.
Train: 2018-08-05T18:14:53.985267: step 1545, loss 0.623306.
Train: 2018-08-05T18:14:54.219588: step 1546, loss 0.588684.
Train: 2018-08-05T18:14:54.422662: step 1547, loss 0.571204.
Train: 2018-08-05T18:14:54.641391: step 1548, loss 0.588868.
Train: 2018-08-05T18:14:54.875680: step 1549, loss 0.527172.
Train: 2018-08-05T18:14:55.094380: step 1550, loss 0.624191.
Test: 2018-08-05T18:14:56.141008: step 1550, loss 0.547316.
Train: 2018-08-05T18:14:56.359739: step 1551, loss 0.579979.
Train: 2018-08-05T18:14:56.594032: step 1552, loss 0.553631.
Train: 2018-08-05T18:14:56.812753: step 1553, loss 0.544951.
Train: 2018-08-05T18:14:57.031425: step 1554, loss 0.623084.
Train: 2018-08-05T18:14:57.265746: step 1555, loss 0.553725.
Train: 2018-08-05T18:14:57.484443: step 1556, loss 0.605123.
Train: 2018-08-05T18:14:57.703142: step 1557, loss 0.545385.
Train: 2018-08-05T18:14:57.921847: step 1558, loss 0.579227.
Train: 2018-08-05T18:14:58.140540: step 1559, loss 0.637733.
Train: 2018-08-05T18:14:58.359271: step 1560, loss 0.554205.
Test: 2018-08-05T18:14:59.421490: step 1560, loss 0.548296.
Train: 2018-08-05T18:14:59.640214: step 1561, loss 0.488744.
Train: 2018-08-05T18:14:59.858918: step 1562, loss 0.546213.
Train: 2018-08-05T18:15:00.093208: step 1563, loss 0.505299.
Train: 2018-08-05T18:15:00.311932: step 1564, loss 0.496765.
Train: 2018-08-05T18:15:00.530608: step 1565, loss 0.512659.
Train: 2018-08-05T18:15:00.749331: step 1566, loss 0.553979.
Train: 2018-08-05T18:15:00.968002: step 1567, loss 0.545326.
Train: 2018-08-05T18:15:01.186737: step 1568, loss 0.553717.
Train: 2018-08-05T18:15:01.405406: step 1569, loss 0.544871.
Train: 2018-08-05T18:15:01.608480: step 1570, loss 0.489998.
Test: 2018-08-05T18:15:02.670754: step 1570, loss 0.546466.
Train: 2018-08-05T18:15:02.889433: step 1571, loss 0.515486.
Train: 2018-08-05T18:15:03.123776: step 1572, loss 0.560462.
Train: 2018-08-05T18:15:03.342447: step 1573, loss 0.515351.
Train: 2018-08-05T18:15:03.561149: step 1574, loss 0.53561.
Train: 2018-08-05T18:15:03.795466: step 1575, loss 0.55467.
Train: 2018-08-05T18:15:03.998568: step 1576, loss 0.591691.
Train: 2018-08-05T18:15:04.217266: step 1577, loss 0.553745.
Train: 2018-08-05T18:15:04.451589: step 1578, loss 0.59983.
Train: 2018-08-05T18:15:04.670286: step 1579, loss 0.590317.
Train: 2018-08-05T18:15:04.888987: step 1580, loss 0.571777.
Test: 2018-08-05T18:15:05.951212: step 1580, loss 0.54799.
Train: 2018-08-05T18:15:06.169914: step 1581, loss 0.535628.
Train: 2018-08-05T18:15:06.388633: step 1582, loss 0.642721.
Train: 2018-08-05T18:15:06.607310: step 1583, loss 0.527232.
Train: 2018-08-05T18:15:06.826006: step 1584, loss 0.544949.
Train: 2018-08-05T18:15:07.044707: step 1585, loss 0.570973.
Train: 2018-08-05T18:15:07.263435: step 1586, loss 0.528075.
Train: 2018-08-05T18:15:07.482102: step 1587, loss 0.60496.
Train: 2018-08-05T18:15:07.700805: step 1588, loss 0.511583.
Train: 2018-08-05T18:15:07.919527: step 1589, loss 0.511735.
Train: 2018-08-05T18:15:08.153846: step 1590, loss 0.553914.
Test: 2018-08-05T18:15:09.200451: step 1590, loss 0.549195.
Train: 2018-08-05T18:15:09.419175: step 1591, loss 0.553893.
Train: 2018-08-05T18:15:09.653502: step 1592, loss 0.56235.
Train: 2018-08-05T18:15:09.872192: step 1593, loss 0.60486.
Train: 2018-08-05T18:15:10.090866: step 1594, loss 0.477387.
Train: 2018-08-05T18:15:10.325223: step 1595, loss 0.613579.
Train: 2018-08-05T18:15:10.543889: step 1596, loss 0.485386.
Train: 2018-08-05T18:15:10.762585: step 1597, loss 0.510704.
Train: 2018-08-05T18:15:10.981307: step 1598, loss 0.571037.
Train: 2018-08-05T18:15:11.199982: step 1599, loss 0.597385.
Train: 2018-08-05T18:15:11.418680: step 1600, loss 0.597542.
Test: 2018-08-05T18:15:12.480931: step 1600, loss 0.548124.
Train: 2018-08-05T18:15:13.543214: step 1601, loss 0.579982.
Train: 2018-08-05T18:15:13.761912: step 1602, loss 0.615073.
Train: 2018-08-05T18:15:13.996205: step 1603, loss 0.527436.
Train: 2018-08-05T18:15:14.214903: step 1604, loss 0.57978.
Train: 2018-08-05T18:15:14.449226: step 1605, loss 0.527644.
Train: 2018-08-05T18:15:14.667921: step 1606, loss 0.510357.
Train: 2018-08-05T18:15:14.886618: step 1607, loss 0.510245.
Train: 2018-08-05T18:15:15.105317: step 1608, loss 0.623495.
Train: 2018-08-05T18:15:15.324017: step 1609, loss 0.614775.
Train: 2018-08-05T18:15:15.558361: step 1610, loss 0.544955.
Test: 2018-08-05T18:15:16.620620: step 1610, loss 0.547456.
Train: 2018-08-05T18:15:16.839286: step 1611, loss 0.562349.
Train: 2018-08-05T18:15:17.057986: step 1612, loss 0.571.
Train: 2018-08-05T18:15:17.276709: step 1613, loss 0.536446.
Train: 2018-08-05T18:15:17.495413: step 1614, loss 0.570959.
Train: 2018-08-05T18:15:17.729735: step 1615, loss 0.519303.
Train: 2018-08-05T18:15:17.948436: step 1616, loss 0.665753.
Train: 2018-08-05T18:15:18.167100: step 1617, loss 0.596599.
Train: 2018-08-05T18:15:18.370204: step 1618, loss 0.604841.
Train: 2018-08-05T18:15:18.604501: step 1619, loss 0.553962.
Train: 2018-08-05T18:15:18.823231: step 1620, loss 0.645901.
Test: 2018-08-05T18:15:19.885451: step 1620, loss 0.548554.
Train: 2018-08-05T18:15:20.104146: step 1621, loss 0.562512.
Train: 2018-08-05T18:15:20.338468: step 1622, loss 0.55446.
Train: 2018-08-05T18:15:20.557192: step 1623, loss 0.603156.
Train: 2018-08-05T18:15:20.775868: step 1624, loss 0.642961.
Train: 2018-08-05T18:15:20.994565: step 1625, loss 0.499772.
Train: 2018-08-05T18:15:21.213263: step 1626, loss 0.563153.
Train: 2018-08-05T18:15:21.431985: step 1627, loss 0.57106.
Train: 2018-08-05T18:15:21.650659: step 1628, loss 0.563303.
Train: 2018-08-05T18:15:21.884980: step 1629, loss 0.56334.
Train: 2018-08-05T18:15:22.103679: step 1630, loss 0.501134.
Test: 2018-08-05T18:15:23.165929: step 1630, loss 0.550383.
Train: 2018-08-05T18:15:23.384653: step 1631, loss 0.578885.
Train: 2018-08-05T18:15:23.603328: step 1632, loss 0.63383.
Train: 2018-08-05T18:15:23.822026: step 1633, loss 0.531737.
Train: 2018-08-05T18:15:24.040724: step 1634, loss 0.578864.
Train: 2018-08-05T18:15:24.243824: step 1635, loss 0.602595.
Train: 2018-08-05T18:15:24.462504: step 1636, loss 0.602629.
Train: 2018-08-05T18:15:24.681229: step 1637, loss 0.602623.
Train: 2018-08-05T18:15:24.915522: step 1638, loss 0.55514.
Train: 2018-08-05T18:15:25.134225: step 1639, loss 0.547235.
Train: 2018-08-05T18:15:25.352952: step 1640, loss 0.54717.
Test: 2018-08-05T18:15:26.399574: step 1640, loss 0.548207.
Train: 2018-08-05T18:15:26.618276: step 1641, loss 0.547047.
Train: 2018-08-05T18:15:26.852569: step 1642, loss 0.554873.
Train: 2018-08-05T18:15:27.071299: step 1643, loss 0.603017.
Train: 2018-08-05T18:15:27.289991: step 1644, loss 0.473864.
Train: 2018-08-05T18:15:27.508689: step 1645, loss 0.578932.
Train: 2018-08-05T18:15:27.789846: step 1646, loss 0.537833.
Train: 2018-08-05T18:15:28.008545: step 1647, loss 0.595701.
Train: 2018-08-05T18:15:28.227245: step 1648, loss 0.604267.
Train: 2018-08-05T18:15:28.445977: step 1649, loss 0.612818.
Train: 2018-08-05T18:15:28.664667: step 1650, loss 0.4951.
Test: 2018-08-05T18:15:29.711274: step 1650, loss 0.549411.
Train: 2018-08-05T18:15:29.930007: step 1651, loss 0.545455.
Train: 2018-08-05T18:15:30.148671: step 1652, loss 0.536837.
Train: 2018-08-05T18:15:30.367400: step 1653, loss 0.562336.
Train: 2018-08-05T18:15:30.586098: step 1654, loss 0.570953.
Train: 2018-08-05T18:15:30.804799: step 1655, loss 0.536365.
Train: 2018-08-05T18:15:31.023490: step 1656, loss 0.536225.
Train: 2018-08-05T18:15:31.242164: step 1657, loss 0.527303.
Train: 2018-08-05T18:15:31.460863: step 1658, loss 0.562438.
Train: 2018-08-05T18:15:31.679587: step 1659, loss 0.500169.
Train: 2018-08-05T18:15:31.898262: step 1660, loss 0.580566.
Test: 2018-08-05T18:15:32.944890: step 1660, loss 0.547969.
Train: 2018-08-05T18:15:33.179246: step 1661, loss 0.54336.
Train: 2018-08-05T18:15:33.397935: step 1662, loss 0.517214.
Train: 2018-08-05T18:15:33.632229: step 1663, loss 0.553695.
Train: 2018-08-05T18:15:33.850928: step 1664, loss 0.516791.
Train: 2018-08-05T18:15:34.069661: step 1665, loss 0.59107.
Train: 2018-08-05T18:15:34.288356: step 1666, loss 0.591242.
Train: 2018-08-05T18:15:34.507024: step 1667, loss 0.525839.
Train: 2018-08-05T18:15:34.725728: step 1668, loss 0.525827.
Train: 2018-08-05T18:15:34.944455: step 1669, loss 0.535164.
Train: 2018-08-05T18:15:35.163145: step 1670, loss 0.591446.
Test: 2018-08-05T18:15:36.225372: step 1670, loss 0.547834.
Train: 2018-08-05T18:15:36.444074: step 1671, loss 0.525799.
Train: 2018-08-05T18:15:36.647149: step 1672, loss 0.591318.
Train: 2018-08-05T18:15:36.865847: step 1673, loss 0.516558.
Train: 2018-08-05T18:15:37.084572: step 1674, loss 0.563115.
Train: 2018-08-05T18:15:37.303277: step 1675, loss 0.525968.
Train: 2018-08-05T18:15:37.521968: step 1676, loss 0.618546.
Train: 2018-08-05T18:15:37.756268: step 1677, loss 0.572091.
Train: 2018-08-05T18:15:37.959344: step 1678, loss 0.581013.
Train: 2018-08-05T18:15:38.193694: step 1679, loss 0.616855.
Train: 2018-08-05T18:15:38.412384: step 1680, loss 0.589263.
Test: 2018-08-05T18:15:39.474611: step 1680, loss 0.547918.
Train: 2018-08-05T18:15:39.677691: step 1681, loss 0.509636.
Train: 2018-08-05T18:15:39.912039: step 1682, loss 0.553649.
Train: 2018-08-05T18:15:40.130707: step 1683, loss 0.640098.
Train: 2018-08-05T18:15:40.349437: step 1684, loss 0.61353.
Train: 2018-08-05T18:15:40.568138: step 1685, loss 0.537155.
Train: 2018-08-05T18:15:40.786804: step 1686, loss 0.595707.
Train: 2018-08-05T18:15:41.005502: step 1687, loss 0.546104.
Train: 2018-08-05T18:15:41.224227: step 1688, loss 0.595212.
Train: 2018-08-05T18:15:41.458554: step 1689, loss 0.619224.
Train: 2018-08-05T18:15:41.661623: step 1690, loss 0.515039.
Test: 2018-08-05T18:15:42.723849: step 1690, loss 0.550127.
Train: 2018-08-05T18:15:42.942549: step 1691, loss 0.507505.
Train: 2018-08-05T18:15:43.161247: step 1692, loss 0.586779.
Train: 2018-08-05T18:15:43.379976: step 1693, loss 0.523509.
Train: 2018-08-05T18:15:43.598675: step 1694, loss 0.578859.
Train: 2018-08-05T18:15:43.817379: step 1695, loss 0.539164.
Train: 2018-08-05T18:15:44.036069: step 1696, loss 0.546974.
Train: 2018-08-05T18:15:44.254741: step 1697, loss 0.498708.
Train: 2018-08-05T18:15:44.473439: step 1698, loss 0.530328.
Train: 2018-08-05T18:15:44.676521: step 1699, loss 0.554389.
Train: 2018-08-05T18:15:44.895248: step 1700, loss 0.579038.
Test: 2018-08-05T18:15:45.957469: step 1700, loss 0.547679.
Train: 2018-08-05T18:15:46.957234: step 1701, loss 0.53733.
Train: 2018-08-05T18:15:47.173821: step 1702, loss 0.58771.
Train: 2018-08-05T18:15:47.392547: step 1703, loss 0.553827.
Train: 2018-08-05T18:15:47.611251: step 1704, loss 0.553754.
Train: 2018-08-05T18:15:47.829945: step 1705, loss 0.458607.
Train: 2018-08-05T18:15:48.048621: step 1706, loss 0.606155.
Train: 2018-08-05T18:15:48.267342: step 1707, loss 0.518282.
Train: 2018-08-05T18:15:48.486017: step 1708, loss 0.535756.
Train: 2018-08-05T18:15:48.689095: step 1709, loss 0.598619.
Train: 2018-08-05T18:15:48.907792: step 1710, loss 0.598904.
Test: 2018-08-05T18:15:49.970069: step 1710, loss 0.547547.
Train: 2018-08-05T18:15:50.188773: step 1711, loss 0.599016.
Train: 2018-08-05T18:15:50.407468: step 1712, loss 0.571761.
Train: 2018-08-05T18:15:50.610546: step 1713, loss 0.589805.
Train: 2018-08-05T18:15:50.829251: step 1714, loss 0.607641.
Train: 2018-08-05T18:15:51.032319: step 1715, loss 0.580408.
Train: 2018-08-05T18:15:51.251018: step 1716, loss 0.562456.
Train: 2018-08-05T18:15:51.454076: step 1717, loss 0.597576.
Train: 2018-08-05T18:15:51.672796: step 1718, loss 0.484206.
Train: 2018-08-05T18:15:51.891470: step 1719, loss 0.527661.
Train: 2018-08-05T18:15:52.094546: step 1720, loss 0.570996.
Test: 2018-08-05T18:15:53.156798: step 1720, loss 0.547128.
Train: 2018-08-05T18:15:53.375522: step 1721, loss 0.501914.
Train: 2018-08-05T18:15:53.594226: step 1722, loss 0.519131.
Train: 2018-08-05T18:15:53.797274: step 1723, loss 0.518991.
Train: 2018-08-05T18:15:54.015972: step 1724, loss 0.571081.
Train: 2018-08-05T18:15:54.234675: step 1725, loss 0.562379.
Train: 2018-08-05T18:15:54.437750: step 1726, loss 0.571178.
Train: 2018-08-05T18:15:54.656448: step 1727, loss 0.641617.
Train: 2018-08-05T18:15:54.859549: step 1728, loss 0.509748.
Train: 2018-08-05T18:15:55.078223: step 1729, loss 0.632573.
Train: 2018-08-05T18:15:55.281301: step 1730, loss 0.614769.
Test: 2018-08-05T18:15:56.343587: step 1730, loss 0.547869.
Train: 2018-08-05T18:15:56.562250: step 1731, loss 0.60569.
Train: 2018-08-05T18:15:56.765328: step 1732, loss 0.59668.
Train: 2018-08-05T18:15:56.984053: step 1733, loss 0.5878.
Train: 2018-08-05T18:15:57.202756: step 1734, loss 0.587424.
Train: 2018-08-05T18:15:57.405802: step 1735, loss 0.546728.
Train: 2018-08-05T18:15:57.624502: step 1736, loss 0.522266.
Train: 2018-08-05T18:15:57.827610: step 1737, loss 0.545972.
Train: 2018-08-05T18:15:58.030659: step 1738, loss 0.620424.
Train: 2018-08-05T18:15:58.233761: step 1739, loss 0.538168.
Train: 2018-08-05T18:15:58.452432: step 1740, loss 0.595174.
Test: 2018-08-05T18:15:59.514685: step 1740, loss 0.549167.
Train: 2018-08-05T18:15:59.717786: step 1741, loss 0.554595.
Train: 2018-08-05T18:15:59.920870: step 1742, loss 0.538472.
Train: 2018-08-05T18:16:00.139537: step 1743, loss 0.530381.
Train: 2018-08-05T18:16:00.342614: step 1744, loss 0.554579.
Train: 2018-08-05T18:16:00.561315: step 1745, loss 0.578917.
Train: 2018-08-05T18:16:00.764417: step 1746, loss 0.521812.
Train: 2018-08-05T18:16:00.967499: step 1747, loss 0.562555.
Train: 2018-08-05T18:16:01.170573: step 1748, loss 0.446993.
Train: 2018-08-05T18:16:01.373623: step 1749, loss 0.604173.
Train: 2018-08-05T18:16:01.576732: step 1750, loss 0.570798.
Test: 2018-08-05T18:16:02.638978: step 1750, loss 0.547002.
Train: 2018-08-05T18:16:02.842029: step 1751, loss 0.545374.
Train: 2018-08-05T18:16:03.060729: step 1752, loss 0.579444.
Train: 2018-08-05T18:16:03.248184: step 1753, loss 0.562336.
Train: 2018-08-05T18:16:03.451288: step 1754, loss 0.519086.
Train: 2018-08-05T18:16:03.654340: step 1755, loss 0.579785.
Train: 2018-08-05T18:16:03.857444: step 1756, loss 0.562381.
Train: 2018-08-05T18:16:04.076117: step 1757, loss 0.553609.
Train: 2018-08-05T18:16:04.263573: step 1758, loss 0.597744.
Train: 2018-08-05T18:16:04.466651: step 1759, loss 0.500562.
Train: 2018-08-05T18:16:04.669753: step 1760, loss 0.500341.
Test: 2018-08-05T18:16:05.731977: step 1760, loss 0.547603.
Train: 2018-08-05T18:16:05.935056: step 1761, loss 0.562523.
Train: 2018-08-05T18:16:06.138159: step 1762, loss 0.544614.
Train: 2018-08-05T18:16:06.325591: step 1763, loss 0.58972.
Train: 2018-08-05T18:16:06.528668: step 1764, loss 0.626012.
Train: 2018-08-05T18:16:06.747391: step 1765, loss 0.544579.
Train: 2018-08-05T18:16:06.934848: step 1766, loss 0.517546.
Train: 2018-08-05T18:16:07.137899: step 1767, loss 0.625741.
Train: 2018-08-05T18:16:07.341003: step 1768, loss 0.562576.
Train: 2018-08-05T18:16:07.544083: step 1769, loss 0.544647.
Train: 2018-08-05T18:16:07.731513: step 1770, loss 0.580324.
Test: 2018-08-05T18:16:08.793761: step 1770, loss 0.546217.
Train: 2018-08-05T18:16:08.981217: step 1771, loss 0.589073.
Train: 2018-08-05T18:16:09.184294: step 1772, loss 0.553602.
Train: 2018-08-05T18:16:09.387396: step 1773, loss 0.536085.
Train: 2018-08-05T18:16:09.590480: step 1774, loss 0.588573.
Train: 2018-08-05T18:16:09.777905: step 1775, loss 0.605797.
Train: 2018-08-05T18:16:09.980985: step 1776, loss 0.562338.
Train: 2018-08-05T18:16:10.184099: step 1777, loss 0.588036.
Train: 2018-08-05T18:16:10.371545: step 1778, loss 0.553843.
Train: 2018-08-05T18:16:10.574593: step 1779, loss 0.520118.
Train: 2018-08-05T18:16:10.777670: step 1780, loss 0.553952.
Test: 2018-08-05T18:16:11.824302: step 1780, loss 0.548348.
Train: 2018-08-05T18:16:12.027406: step 1781, loss 0.537171.
Train: 2018-08-05T18:16:12.214837: step 1782, loss 0.629602.
Train: 2018-08-05T18:16:12.417942: step 1783, loss 0.63773.
Train: 2018-08-05T18:16:12.605372: step 1784, loss 0.562453.
Train: 2018-08-05T18:16:12.808472: step 1785, loss 0.612017.
Train: 2018-08-05T18:16:12.995901: step 1786, loss 0.546214.
Train: 2018-08-05T18:16:13.183358: step 1787, loss 0.554511.
Train: 2018-08-05T18:16:13.386466: step 1788, loss 0.554597.
Train: 2018-08-05T18:16:13.573922: step 1789, loss 0.562732.
Train: 2018-08-05T18:16:13.776995: step 1790, loss 0.506316.
Test: 2018-08-05T18:16:14.839248: step 1790, loss 0.549226.
Train: 2018-08-05T18:16:15.026678: step 1791, loss 0.635464.
Train: 2018-08-05T18:16:15.214158: step 1792, loss 0.546601.
Train: 2018-08-05T18:16:15.401590: step 1793, loss 0.595032.
Train: 2018-08-05T18:16:15.589045: step 1794, loss 0.595015.
Train: 2018-08-05T18:16:15.792149: step 1795, loss 0.522522.
Train: 2018-08-05T18:16:15.979579: step 1796, loss 0.57888.
Train: 2018-08-05T18:16:16.167039: step 1797, loss 0.46592.
Train: 2018-08-05T18:16:16.354515: step 1798, loss 0.52202.
Train: 2018-08-05T18:16:16.541972: step 1799, loss 0.61999.
Train: 2018-08-05T18:16:16.729430: step 1800, loss 0.512996.
Test: 2018-08-05T18:16:17.776063: step 1800, loss 0.547778.
Train: 2018-08-05T18:16:18.744580: step 1801, loss 0.579083.
Train: 2018-08-05T18:16:18.932042: step 1802, loss 0.545643.
Train: 2018-08-05T18:16:19.119471: step 1803, loss 0.613009.
Train: 2018-08-05T18:16:19.306955: step 1804, loss 0.604721.
Train: 2018-08-05T18:16:19.494380: step 1805, loss 0.545383.
Train: 2018-08-05T18:16:19.681869: step 1806, loss 0.570847.
Train: 2018-08-05T18:16:19.853697: step 1807, loss 0.528284.
Train: 2018-08-05T18:16:20.041128: step 1808, loss 0.596509.
Train: 2018-08-05T18:16:20.228583: step 1809, loss 0.553785.
Train: 2018-08-05T18:16:20.400417: step 1810, loss 0.493824.
Test: 2018-08-05T18:16:21.462669: step 1810, loss 0.547762.
Train: 2018-08-05T18:16:21.650152: step 1811, loss 0.579551.
Train: 2018-08-05T18:16:21.790749: step 1812, loss 0.525484.
Train: 2018-08-05T18:16:21.978174: step 1813, loss 0.597078.
Train: 2018-08-05T18:16:22.165631: step 1814, loss 0.588466.
Train: 2018-08-05T18:16:22.353086: step 1815, loss 0.544944.
Train: 2018-08-05T18:16:22.540566: step 1816, loss 0.553645.
Train: 2018-08-05T18:16:22.728002: step 1817, loss 0.544911.
Train: 2018-08-05T18:16:22.899858: step 1818, loss 0.562375.
Train: 2018-08-05T18:16:23.087292: step 1819, loss 0.588649.
Train: 2018-08-05T18:16:23.259124: step 1820, loss 0.483618.
Test: 2018-08-05T18:16:24.321408: step 1820, loss 0.547526.
Train: 2018-08-05T18:16:24.493209: step 1821, loss 0.659002.
Train: 2018-08-05T18:16:24.680665: step 1822, loss 0.588664.
Train: 2018-08-05T18:16:24.852501: step 1823, loss 0.544916.
Train: 2018-08-05T18:16:25.039957: step 1824, loss 0.562356.
Train: 2018-08-05T18:16:25.211793: step 1825, loss 0.518979.
Train: 2018-08-05T18:16:25.383652: step 1826, loss 0.605698.
Train: 2018-08-05T18:16:25.555463: step 1827, loss 0.475895.
Train: 2018-08-05T18:16:25.742917: step 1828, loss 0.605655.
Train: 2018-08-05T18:16:25.914784: step 1829, loss 0.614275.
Train: 2018-08-05T18:16:26.102238: step 1830, loss 0.484733.
Test: 2018-08-05T18:16:27.148837: step 1830, loss 0.547727.
Train: 2018-08-05T18:16:27.336293: step 1831, loss 0.596857.
Train: 2018-08-05T18:16:27.508133: step 1832, loss 0.579574.
Train: 2018-08-05T18:16:27.679990: step 1833, loss 0.545138.
Train: 2018-08-05T18:16:27.867446: step 1834, loss 0.570924.
Train: 2018-08-05T18:16:28.039259: step 1835, loss 0.596635.
Train: 2018-08-05T18:16:28.211114: step 1836, loss 0.587974.
Train: 2018-08-05T18:16:28.382957: step 1837, loss 0.54533.
Train: 2018-08-05T18:16:28.554758: step 1838, loss 0.587793.
Train: 2018-08-05T18:16:28.726593: step 1839, loss 0.545473.
Train: 2018-08-05T18:16:28.898428: step 1840, loss 0.553951.
Test: 2018-08-05T18:16:29.945057: step 1840, loss 0.548531.
Train: 2018-08-05T18:16:30.132514: step 1841, loss 0.520336.
Train: 2018-08-05T18:16:30.288732: step 1842, loss 0.545541.
Train: 2018-08-05T18:16:30.460561: step 1843, loss 0.621419.
Train: 2018-08-05T18:16:30.648018: step 1844, loss 0.537099.
Train: 2018-08-05T18:16:30.819883: step 1845, loss 0.562372.
Train: 2018-08-05T18:16:30.976067: step 1846, loss 0.53707.
Train: 2018-08-05T18:16:31.147933: step 1847, loss 0.570813.
Train: 2018-08-05T18:16:31.319738: step 1848, loss 0.587749.
Train: 2018-08-05T18:16:31.491596: step 1849, loss 0.596215.
Train: 2018-08-05T18:16:31.647788: step 1850, loss 0.520114.
Test: 2018-08-05T18:16:32.710036: step 1850, loss 0.547844.
Train: 2018-08-05T18:16:32.881872: step 1851, loss 0.545443.
Train: 2018-08-05T18:16:33.053729: step 1852, loss 0.6132.
Train: 2018-08-05T18:16:33.225571: step 1853, loss 0.553891.
Train: 2018-08-05T18:16:33.412998: step 1854, loss 0.553894.
Train: 2018-08-05T18:16:33.569212: step 1855, loss 0.587752.
Train: 2018-08-05T18:16:33.741046: step 1856, loss 0.56236.
Train: 2018-08-05T18:16:33.912905: step 1857, loss 0.613057.
Train: 2018-08-05T18:16:34.084744: step 1858, loss 0.495022.
Train: 2018-08-05T18:16:34.256548: step 1859, loss 0.604506.
Train: 2018-08-05T18:16:34.428409: step 1860, loss 0.54555.
Test: 2018-08-05T18:16:35.490635: step 1860, loss 0.548523.
Train: 2018-08-05T18:16:35.662471: step 1861, loss 0.53714.
Train: 2018-08-05T18:16:35.834303: step 1862, loss 0.570799.
Train: 2018-08-05T18:16:35.990517: step 1863, loss 0.596107.
Train: 2018-08-05T18:16:36.162352: step 1864, loss 0.494952.
Train: 2018-08-05T18:16:36.334189: step 1865, loss 0.587726.
Train: 2018-08-05T18:16:36.521669: step 1866, loss 0.520009.
Train: 2018-08-05T18:16:36.693505: step 1867, loss 0.545338.
Train: 2018-08-05T18:16:36.849722: step 1868, loss 0.493994.
Train: 2018-08-05T18:16:37.021552: step 1869, loss 0.614009.
Train: 2018-08-05T18:16:37.193386: step 1870, loss 0.536396.
Test: 2018-08-05T18:16:38.239991: step 1870, loss 0.547836.
Train: 2018-08-05T18:16:38.411828: step 1871, loss 0.562353.
Train: 2018-08-05T18:16:38.583686: step 1872, loss 0.544907.
Train: 2018-08-05T18:16:38.755495: step 1873, loss 0.57116.
Train: 2018-08-05T18:16:38.927355: step 1874, loss 0.544807.
Train: 2018-08-05T18:16:39.099191: step 1875, loss 0.562429.
Train: 2018-08-05T18:16:39.255377: step 1876, loss 0.589018.
Train: 2018-08-05T18:16:39.427237: step 1877, loss 0.482705.
Train: 2018-08-05T18:16:39.599048: step 1878, loss 0.58029.
Train: 2018-08-05T18:16:39.755286: step 1879, loss 0.482206.
Train: 2018-08-05T18:16:39.927126: step 1880, loss 0.607461.
Test: 2018-08-05T18:16:40.973725: step 1880, loss 0.54757.
Train: 2018-08-05T18:16:41.145587: step 1881, loss 0.607593.
Train: 2018-08-05T18:16:41.317420: step 1882, loss 0.562586.
Train: 2018-08-05T18:16:41.473608: step 1883, loss 0.553594.
Train: 2018-08-05T18:16:41.645476: step 1884, loss 0.616307.
Train: 2018-08-05T18:16:41.817302: step 1885, loss 0.607069.
Train: 2018-08-05T18:16:41.989113: step 1886, loss 0.597821.
Train: 2018-08-05T18:16:42.160948: step 1887, loss 0.588672.
Train: 2018-08-05T18:16:42.332787: step 1888, loss 0.545001.
Train: 2018-08-05T18:16:42.504617: step 1889, loss 0.622533.
Train: 2018-08-05T18:16:42.676476: step 1890, loss 0.519825.
Test: 2018-08-05T18:16:43.723081: step 1890, loss 0.548466.
Train: 2018-08-05T18:16:43.894953: step 1891, loss 0.655203.
Train: 2018-08-05T18:16:44.066781: step 1892, loss 0.520734.
Train: 2018-08-05T18:16:44.222991: step 1893, loss 0.587299.
Train: 2018-08-05T18:16:44.394804: step 1894, loss 0.521549.
Train: 2018-08-05T18:16:44.566659: step 1895, loss 0.546279.
Train: 2018-08-05T18:16:44.738468: step 1896, loss 0.554493.
Train: 2018-08-05T18:16:44.925926: step 1897, loss 0.562651.
Train: 2018-08-05T18:16:45.082168: step 1898, loss 0.546403.
Train: 2018-08-05T18:16:45.253977: step 1899, loss 0.546373.
Train: 2018-08-05T18:16:45.425832: step 1900, loss 0.546304.
Test: 2018-08-05T18:16:46.472438: step 1900, loss 0.547207.
Train: 2018-08-05T18:16:47.456607: step 1901, loss 0.529827.
Train: 2018-08-05T18:16:47.628443: step 1902, loss 0.521343.
Train: 2018-08-05T18:16:47.800252: step 1903, loss 0.562456.
Train: 2018-08-05T18:16:47.972117: step 1904, loss 0.570774.
Train: 2018-08-05T18:16:48.143951: step 1905, loss 0.520264.
Train: 2018-08-05T18:16:48.300134: step 1906, loss 0.579336.
Train: 2018-08-05T18:16:48.471969: step 1907, loss 0.528128.
Train: 2018-08-05T18:16:48.643836: step 1908, loss 0.553717.
Train: 2018-08-05T18:16:48.815640: step 1909, loss 0.579723.
Train: 2018-08-05T18:16:48.971878: step 1910, loss 0.544901.
Test: 2018-08-05T18:16:50.034104: step 1910, loss 0.547524.
Train: 2018-08-05T18:16:50.190316: step 1911, loss 0.562397.
Train: 2018-08-05T18:16:50.362151: step 1912, loss 0.59773.
Train: 2018-08-05T18:16:50.533986: step 1913, loss 0.562438.
Train: 2018-08-05T18:16:50.690200: step 1914, loss 0.553593.
Train: 2018-08-05T18:16:50.862034: step 1915, loss 0.57132.
Train: 2018-08-05T18:16:51.033871: step 1916, loss 0.535861.
Train: 2018-08-05T18:16:51.205738: step 1917, loss 0.589088.
Train: 2018-08-05T18:16:51.361919: step 1918, loss 0.509265.
Train: 2018-08-05T18:16:51.533785: step 1919, loss 0.624621.
Train: 2018-08-05T18:16:51.689992: step 1920, loss 0.651012.
Test: 2018-08-05T18:16:52.752217: step 1920, loss 0.549745.
Train: 2018-08-05T18:16:52.924084: step 1921, loss 0.518456.
Train: 2018-08-05T18:16:53.080291: step 1922, loss 0.579869.
Train: 2018-08-05T18:16:53.267722: step 1923, loss 0.623236.
Train: 2018-08-05T18:16:53.439583: step 1924, loss 0.501985.
Train: 2018-08-05T18:16:53.595770: step 1925, loss 0.536588.
Train: 2018-08-05T18:16:53.767629: step 1926, loss 0.570894.
Train: 2018-08-05T18:16:53.955062: step 1927, loss 0.587938.
Train: 2018-08-05T18:16:54.111273: step 1928, loss 0.528354.
Train: 2018-08-05T18:16:54.283108: step 1929, loss 0.596283.
Train: 2018-08-05T18:16:54.439324: step 1930, loss 0.553906.
Test: 2018-08-05T18:16:55.501601: step 1930, loss 0.548284.
Train: 2018-08-05T18:16:55.673407: step 1931, loss 0.537068.
Train: 2018-08-05T18:16:55.829625: step 1932, loss 0.612943.
Train: 2018-08-05T18:16:56.001458: step 1933, loss 0.587589.
Train: 2018-08-05T18:16:56.173293: step 1934, loss 0.487114.
Train: 2018-08-05T18:16:56.329505: step 1935, loss 0.55403.
Train: 2018-08-05T18:16:56.501338: step 1936, loss 0.512084.
Train: 2018-08-05T18:16:56.657584: step 1937, loss 0.520261.
Train: 2018-08-05T18:16:56.845010: step 1938, loss 0.562352.
Train: 2018-08-05T18:16:57.016843: step 1939, loss 0.639103.
Train: 2018-08-05T18:16:57.188679: step 1940, loss 0.570876.
Test: 2018-08-05T18:16:58.250928: step 1940, loss 0.548464.
Train: 2018-08-05T18:16:58.407143: step 1941, loss 0.476914.
Train: 2018-08-05T18:16:58.578981: step 1942, loss 0.553748.
Train: 2018-08-05T18:16:58.735191: step 1943, loss 0.553707.
Train: 2018-08-05T18:16:58.907026: step 1944, loss 0.544999.
Train: 2018-08-05T18:16:59.078860: step 1945, loss 0.579803.
Train: 2018-08-05T18:16:59.235098: step 1946, loss 0.623628.
Train: 2018-08-05T18:16:59.406939: step 1947, loss 0.527396.
Train: 2018-08-05T18:16:59.563123: step 1948, loss 0.518608.
Train: 2018-08-05T18:16:59.734956: step 1949, loss 0.58874.
Train: 2018-08-05T18:16:59.906817: step 1950, loss 0.588771.
Test: 2018-08-05T18:17:00.953423: step 1950, loss 0.547729.
Train: 2018-08-05T18:17:01.125256: step 1951, loss 0.579959.
Train: 2018-08-05T18:17:01.297091: step 1952, loss 0.57991.
Train: 2018-08-05T18:17:01.468952: step 1953, loss 0.553636.
Train: 2018-08-05T18:17:01.640760: step 1954, loss 0.649458.
Train: 2018-08-05T18:17:01.796974: step 1955, loss 0.562341.
Train: 2018-08-05T18:17:01.968836: step 1956, loss 0.50225.
Train: 2018-08-05T18:17:02.125049: step 1957, loss 0.528104.
Train: 2018-08-05T18:17:02.296861: step 1958, loss 0.570888.
Train: 2018-08-05T18:17:02.468693: step 1959, loss 0.502552.
Train: 2018-08-05T18:17:02.624905: step 1960, loss 0.510973.
Test: 2018-08-05T18:17:03.687181: step 1960, loss 0.547968.
Train: 2018-08-05T18:17:03.859016: step 1961, loss 0.596747.
Train: 2018-08-05T18:17:04.030857: step 1962, loss 0.536466.
Train: 2018-08-05T18:17:04.171418: step 1963, loss 0.617727.
Train: 2018-08-05T18:17:04.343253: step 1964, loss 0.579648.
Train: 2018-08-05T18:17:04.499491: step 1965, loss 0.56234.
Train: 2018-08-05T18:17:04.671331: step 1966, loss 0.519184.
Train: 2018-08-05T18:17:04.843138: step 1967, loss 0.553699.
Train: 2018-08-05T18:17:05.014972: step 1968, loss 0.562342.
Train: 2018-08-05T18:17:05.171184: step 1969, loss 0.484388.
Train: 2018-08-05T18:17:05.343045: step 1970, loss 0.527533.
Test: 2018-08-05T18:17:06.405271: step 1970, loss 0.546743.
Train: 2018-08-05T18:17:06.561515: step 1971, loss 0.588669.
Train: 2018-08-05T18:17:06.748966: step 1972, loss 0.544813.
Train: 2018-08-05T18:17:06.905154: step 1973, loss 0.580093.
Train: 2018-08-05T18:17:07.077012: step 1974, loss 0.527038.
Train: 2018-08-05T18:17:07.248847: step 1975, loss 0.50029.
Train: 2018-08-05T18:17:07.405060: step 1976, loss 0.499965.
Train: 2018-08-05T18:17:07.576870: step 1977, loss 0.607674.
Train: 2018-08-05T18:17:07.748735: step 1978, loss 0.481212.
Train: 2018-08-05T18:17:07.904918: step 1979, loss 0.57189.
Train: 2018-08-05T18:17:08.076779: step 1980, loss 0.590359.
Test: 2018-08-05T18:17:09.123382: step 1980, loss 0.547757.
Train: 2018-08-05T18:17:09.295248: step 1981, loss 0.535323.
Train: 2018-08-05T18:17:09.467056: step 1982, loss 0.54451.
Train: 2018-08-05T18:17:09.623267: step 1983, loss 0.544509.
Train: 2018-08-05T18:17:09.795104: step 1984, loss 0.627827.
Train: 2018-08-05T18:17:09.966935: step 1985, loss 0.479876.
Train: 2018-08-05T18:17:10.123176: step 1986, loss 0.562995.
Train: 2018-08-05T18:17:10.279390: step 1987, loss 0.562993.
Train: 2018-08-05T18:17:10.451202: step 1988, loss 0.627596.
Train: 2018-08-05T18:17:10.623033: step 1989, loss 0.572059.
Train: 2018-08-05T18:17:10.779247: step 1990, loss 0.672247.
Test: 2018-08-05T18:17:11.841496: step 1990, loss 0.548395.
Train: 2018-08-05T18:17:12.013334: step 1991, loss 0.580629.
Train: 2018-08-05T18:17:12.185166: step 1992, loss 0.562481.
Train: 2018-08-05T18:17:12.357001: step 1993, loss 0.623882.
Train: 2018-08-05T18:17:12.513215: step 1994, loss 0.562343.
Train: 2018-08-05T18:17:12.685052: step 1995, loss 0.545265.
Train: 2018-08-05T18:17:12.856886: step 1996, loss 0.545485.
Train: 2018-08-05T18:17:13.028718: step 1997, loss 0.562409.
Train: 2018-08-05T18:17:13.184957: step 1998, loss 0.554164.
Train: 2018-08-05T18:17:13.356797: step 1999, loss 0.59549.
Train: 2018-08-05T18:17:13.513007: step 2000, loss 0.619875.
Test: 2018-08-05T18:17:14.575232: step 2000, loss 0.549328.
Train: 2018-08-05T18:17:15.496921: step 2001, loss 0.497814.
Train: 2018-08-05T18:17:15.668725: step 2002, loss 0.578886.
Train: 2018-08-05T18:17:15.840560: step 2003, loss 0.514507.
Train: 2018-08-05T18:17:15.996776: step 2004, loss 0.59497.
Train: 2018-08-05T18:17:16.168612: step 2005, loss 0.586912.
Train: 2018-08-05T18:17:16.356095: step 2006, loss 0.55479.
Train: 2018-08-05T18:17:16.527901: step 2007, loss 0.554794.
Train: 2018-08-05T18:17:16.684114: step 2008, loss 0.498535.
Train: 2018-08-05T18:17:16.855979: step 2009, loss 0.554652.
Train: 2018-08-05T18:17:17.027784: step 2010, loss 0.578912.
Test: 2018-08-05T18:17:18.090033: step 2010, loss 0.548576.
Train: 2018-08-05T18:17:18.246249: step 2011, loss 0.554428.
Train: 2018-08-05T18:17:18.433705: step 2012, loss 0.496803.
Train: 2018-08-05T18:17:18.589941: step 2013, loss 0.595643.
Train: 2018-08-05T18:17:18.761783: step 2014, loss 0.529008.
Train: 2018-08-05T18:17:18.917991: step 2015, loss 0.57922.
Train: 2018-08-05T18:17:19.089825: step 2016, loss 0.613228.
Train: 2018-08-05T18:17:19.246013: step 2017, loss 0.570851.
Train: 2018-08-05T18:17:19.417848: step 2018, loss 0.502642.
Train: 2018-08-05T18:17:19.589683: step 2019, loss 0.579488.
Train: 2018-08-05T18:17:19.745926: step 2020, loss 0.476227.
Test: 2018-08-05T18:17:20.808177: step 2020, loss 0.548449.
Train: 2018-08-05T18:17:20.964394: step 2021, loss 0.536302.
Train: 2018-08-05T18:17:21.136221: step 2022, loss 0.527349.
Train: 2018-08-05T18:17:21.308057: step 2023, loss 0.562436.
Train: 2018-08-05T18:17:21.464244: step 2024, loss 0.473387.
Train: 2018-08-05T18:17:21.636082: step 2025, loss 0.508538.
Train: 2018-08-05T18:17:21.792291: step 2026, loss 0.526281.
Train: 2018-08-05T18:17:21.964153: step 2027, loss 0.581455.
Train: 2018-08-05T18:17:22.135960: step 2028, loss 0.591112.
Train: 2018-08-05T18:17:22.292175: step 2029, loss 0.54453.
Train: 2018-08-05T18:17:22.464040: step 2030, loss 0.629269.
Test: 2018-08-05T18:17:23.510669: step 2030, loss 0.545918.
Train: 2018-08-05T18:17:23.682476: step 2031, loss 0.535137.
Train: 2018-08-05T18:17:23.854339: step 2032, loss 0.516325.
Train: 2018-08-05T18:17:24.026168: step 2033, loss 0.553966.
Train: 2018-08-05T18:17:24.198008: step 2034, loss 0.478634.
Train: 2018-08-05T18:17:24.354192: step 2035, loss 0.525674.
Train: 2018-08-05T18:17:24.526028: step 2036, loss 0.582502.
Train: 2018-08-05T18:17:24.682240: step 2037, loss 0.601509.
Train: 2018-08-05T18:17:24.854080: step 2038, loss 0.506728.
Train: 2018-08-05T18:17:25.025908: step 2039, loss 0.591844.
Train: 2018-08-05T18:17:25.182123: step 2040, loss 0.619929.
Test: 2018-08-05T18:17:26.244399: step 2040, loss 0.547611.
Train: 2018-08-05T18:17:26.416211: step 2041, loss 0.572586.
Train: 2018-08-05T18:17:26.572422: step 2042, loss 0.572341.
Train: 2018-08-05T18:17:26.744284: step 2043, loss 0.562901.
Train: 2018-08-05T18:17:26.916095: step 2044, loss 0.526315.
Train: 2018-08-05T18:17:27.087929: step 2045, loss 0.607894.
Train: 2018-08-05T18:17:27.244142: step 2046, loss 0.589432.
Train: 2018-08-05T18:17:27.416005: step 2047, loss 0.580186.
Train: 2018-08-05T18:17:27.572189: step 2048, loss 0.579919.
Train: 2018-08-05T18:17:27.790918: step 2049, loss 0.536344.
Train: 2018-08-05T18:17:27.962748: step 2050, loss 0.476443.
Test: 2018-08-05T18:17:29.024987: step 2050, loss 0.546464.
Train: 2018-08-05T18:17:29.181185: step 2051, loss 0.545216.
Train: 2018-08-05T18:17:29.353020: step 2052, loss 0.707543.
Train: 2018-08-05T18:17:29.524887: step 2053, loss 0.655374.
Train: 2018-08-05T18:17:29.696691: step 2054, loss 0.620782.
Train: 2018-08-05T18:17:29.868525: step 2055, loss 0.554353.
Train: 2018-08-05T18:17:30.024738: step 2056, loss 0.562706.
Train: 2018-08-05T18:17:30.196578: step 2057, loss 0.634869.
Train: 2018-08-05T18:17:30.352787: step 2058, loss 0.586753.
Train: 2018-08-05T18:17:30.524624: step 2059, loss 0.555538.
Train: 2018-08-05T18:17:30.680835: step 2060, loss 0.54812.
Test: 2018-08-05T18:17:31.743087: step 2060, loss 0.552494.
Train: 2018-08-05T18:17:31.899299: step 2061, loss 0.578995.
Train: 2018-08-05T18:17:32.071159: step 2062, loss 0.548613.
Train: 2018-08-05T18:17:32.242994: step 2063, loss 0.579068.
Train: 2018-08-05T18:17:32.414807: step 2064, loss 0.563954.
Train: 2018-08-05T18:17:32.586641: step 2065, loss 0.594219.
Train: 2018-08-05T18:17:32.758476: step 2066, loss 0.533764.
Train: 2018-08-05T18:17:32.930311: step 2067, loss 0.609381.
Train: 2018-08-05T18:17:33.102174: step 2068, loss 0.571486.
Train: 2018-08-05T18:17:33.258357: step 2069, loss 0.56386.
Train: 2018-08-05T18:17:33.430217: step 2070, loss 0.586648.
Test: 2018-08-05T18:17:34.492469: step 2070, loss 0.549911.
Train: 2018-08-05T18:17:34.679898: step 2071, loss 0.525523.
Train: 2018-08-05T18:17:34.851735: step 2072, loss 0.571276.
Train: 2018-08-05T18:17:35.007949: step 2073, loss 0.547979.
Train: 2018-08-05T18:17:35.179783: step 2074, loss 0.586691.
Train: 2018-08-05T18:17:35.351617: step 2075, loss 0.563169.
Train: 2018-08-05T18:17:35.507855: step 2076, loss 0.586768.
Train: 2018-08-05T18:17:35.679667: step 2077, loss 0.594767.
Train: 2018-08-05T18:17:35.851529: step 2078, loss 0.594838.
Train: 2018-08-05T18:17:36.007743: step 2079, loss 0.546825.
Train: 2018-08-05T18:17:36.179553: step 2080, loss 0.474303.
Test: 2018-08-05T18:17:37.226208: step 2080, loss 0.549658.
Train: 2018-08-05T18:17:37.398043: step 2081, loss 0.611401.
Train: 2018-08-05T18:17:37.554226: step 2082, loss 0.570767.
Train: 2018-08-05T18:17:37.726090: step 2083, loss 0.587208.
Train: 2018-08-05T18:17:37.897927: step 2084, loss 0.545974.
Train: 2018-08-05T18:17:38.069760: step 2085, loss 0.55415.
Train: 2018-08-05T18:17:38.225946: step 2086, loss 0.49561.
Train: 2018-08-05T18:17:38.397780: step 2087, loss 0.663502.
Train: 2018-08-05T18:17:38.569639: step 2088, loss 0.511667.
Train: 2018-08-05T18:17:38.741450: step 2089, loss 0.502886.
Train: 2018-08-05T18:17:38.913285: step 2090, loss 0.502385.
Test: 2018-08-05T18:17:39.959956: step 2090, loss 0.548485.
Train: 2018-08-05T18:17:40.131773: step 2091, loss 0.501751.
Train: 2018-08-05T18:17:40.319233: step 2092, loss 0.597453.
Train: 2018-08-05T18:17:40.475419: step 2093, loss 0.588983.
Train: 2018-08-05T18:17:40.647251: step 2094, loss 0.580297.
Train: 2018-08-05T18:17:40.819087: step 2095, loss 0.499947.
Train: 2018-08-05T18:17:40.990946: step 2096, loss 0.643587.
Train: 2018-08-05T18:17:41.162757: step 2097, loss 0.571615.
Train: 2018-08-05T18:17:41.334591: step 2098, loss 0.571607.
Train: 2018-08-05T18:17:41.506425: step 2099, loss 0.526628.
Train: 2018-08-05T18:17:41.662637: step 2100, loss 0.598542.
Test: 2018-08-05T18:17:42.724914: step 2100, loss 0.548617.
Train: 2018-08-05T18:17:43.630952: step 2101, loss 0.61635.
Train: 2018-08-05T18:17:43.787141: step 2102, loss 0.62489.
Train: 2018-08-05T18:17:43.958978: step 2103, loss 0.588922.
Train: 2018-08-05T18:17:44.130842: step 2104, loss 0.571115.
Train: 2018-08-05T18:17:44.287025: step 2105, loss 0.475801.
Train: 2018-08-05T18:17:44.458858: step 2106, loss 0.562337.
Train: 2018-08-05T18:17:44.615104: step 2107, loss 0.588078.
Train: 2018-08-05T18:17:44.786908: step 2108, loss 0.485509.
Train: 2018-08-05T18:17:44.958742: step 2109, loss 0.579406.
Train: 2018-08-05T18:17:45.130601: step 2110, loss 0.553818.
Test: 2018-08-05T18:17:46.177206: step 2110, loss 0.547921.
Train: 2018-08-05T18:17:46.349067: step 2111, loss 0.511237.
Train: 2018-08-05T18:17:46.505279: step 2112, loss 0.545264.
Train: 2018-08-05T18:17:46.677090: step 2113, loss 0.639407.
Train: 2018-08-05T18:17:46.817681: step 2114, loss 0.580576.
Train: 2018-08-05T18:17:46.989519: step 2115, loss 0.562339.
Train: 2018-08-05T18:17:47.161350: step 2116, loss 0.553833.
Train: 2018-08-05T18:17:47.333187: step 2117, loss 0.604833.
Train: 2018-08-05T18:17:47.489401: step 2118, loss 0.536957.
Train: 2018-08-05T18:17:47.661234: step 2119, loss 0.528555.
Train: 2018-08-05T18:17:47.817478: step 2120, loss 0.56236.
Test: 2018-08-05T18:17:48.879730: step 2120, loss 0.547644.
Train: 2018-08-05T18:17:49.051534: step 2121, loss 0.536977.
Train: 2018-08-05T18:17:49.207748: step 2122, loss 0.511485.
Train: 2018-08-05T18:17:49.379585: step 2123, loss 0.621973.
Train: 2018-08-05T18:17:49.551442: step 2124, loss 0.56234.
Train: 2018-08-05T18:17:49.707630: step 2125, loss 0.511127.
Train: 2018-08-05T18:17:49.879466: step 2126, loss 0.57947.
Train: 2018-08-05T18:17:50.051298: step 2127, loss 0.570922.
Train: 2018-08-05T18:17:50.223158: step 2128, loss 0.562335.
Train: 2018-08-05T18:17:50.379348: step 2129, loss 0.596784.
Train: 2018-08-05T18:17:50.551185: step 2130, loss 0.57094.
Test: 2018-08-05T18:17:51.613435: step 2130, loss 0.547193.
Train: 2018-08-05T18:17:51.785267: step 2131, loss 0.51937.
Train: 2018-08-05T18:17:51.941506: step 2132, loss 0.579541.
Train: 2018-08-05T18:17:52.113341: step 2133, loss 0.519324.
Train: 2018-08-05T18:17:52.285177: step 2134, loss 0.553715.
Train: 2018-08-05T18:17:52.441366: step 2135, loss 0.536412.
Train: 2018-08-05T18:17:52.613223: step 2136, loss 0.518983.
Train: 2018-08-05T18:17:52.769436: step 2137, loss 0.666992.
Train: 2018-08-05T18:17:52.941249: step 2138, loss 0.544943.
Train: 2018-08-05T18:17:53.113082: step 2139, loss 0.605879.
Train: 2018-08-05T18:17:53.284948: step 2140, loss 0.544993.
Test: 2018-08-05T18:17:54.331572: step 2140, loss 0.54788.
Train: 2018-08-05T18:17:54.503383: step 2141, loss 0.571005.
Train: 2018-08-05T18:17:54.675216: step 2142, loss 0.527781.
Train: 2018-08-05T18:17:54.847051: step 2143, loss 0.510523.
Train: 2018-08-05T18:17:55.018917: step 2144, loss 0.64891.
Train: 2018-08-05T18:17:55.190751: step 2145, loss 0.665922.
Train: 2018-08-05T18:17:55.346965: step 2146, loss 0.579454.
Train: 2018-08-05T18:17:55.518769: step 2147, loss 0.587805.
Train: 2018-08-05T18:17:55.690604: step 2148, loss 0.596012.
Train: 2018-08-05T18:17:55.846820: step 2149, loss 0.637349.
Train: 2018-08-05T18:17:56.018683: step 2150, loss 0.546115.
Test: 2018-08-05T18:17:57.080903: step 2150, loss 0.548328.
Train: 2018-08-05T18:17:57.252742: step 2151, loss 0.505746.
Train: 2018-08-05T18:17:57.424574: step 2152, loss 0.651661.
Train: 2018-08-05T18:17:57.580815: step 2153, loss 0.586874.
Train: 2018-08-05T18:17:57.752621: step 2154, loss 0.539181.
Train: 2018-08-05T18:17:57.924482: step 2155, loss 0.649856.
Train: 2018-08-05T18:17:58.096291: step 2156, loss 0.617945.
Train: 2018-08-05T18:17:58.252528: step 2157, loss 0.57121.
Train: 2018-08-05T18:17:58.424338: step 2158, loss 0.601958.
Train: 2018-08-05T18:17:58.596206: step 2159, loss 0.586652.
Train: 2018-08-05T18:17:58.752414: step 2160, loss 0.511484.
Test: 2018-08-05T18:17:59.814637: step 2160, loss 0.552352.
Train: 2018-08-05T18:17:59.986503: step 2161, loss 0.556686.
Train: 2018-08-05T18:18:00.142686: step 2162, loss 0.60167.
Train: 2018-08-05T18:18:00.314545: step 2163, loss 0.526792.
Train: 2018-08-05T18:18:00.486355: step 2164, loss 0.564147.
Train: 2018-08-05T18:18:00.658190: step 2165, loss 0.631906.
Train: 2018-08-05T18:18:00.830025: step 2166, loss 0.564015.
Train: 2018-08-05T18:18:00.986238: step 2167, loss 0.510988.
Train: 2018-08-05T18:18:01.158077: step 2168, loss 0.540928.
Train: 2018-08-05T18:18:01.329940: step 2169, loss 0.663553.
Train: 2018-08-05T18:18:01.517364: step 2170, loss 0.609803.
Test: 2018-08-05T18:18:02.579646: step 2170, loss 0.549696.
Train: 2018-08-05T18:18:02.751474: step 2171, loss 0.548037.
Train: 2018-08-05T18:18:02.907687: step 2172, loss 0.563417.
Train: 2018-08-05T18:18:03.079525: step 2173, loss 0.524415.
Train: 2018-08-05T18:18:03.251335: step 2174, loss 0.563194.
Train: 2018-08-05T18:18:03.423198: step 2175, loss 0.610466.
Train: 2018-08-05T18:18:03.595004: step 2176, loss 0.578858.
Train: 2018-08-05T18:18:03.766837: step 2177, loss 0.515064.
Train: 2018-08-05T18:18:03.938673: step 2178, loss 0.522621.
Train: 2018-08-05T18:18:04.110510: step 2179, loss 0.570788.
Train: 2018-08-05T18:18:04.266724: step 2180, loss 0.529767.
Test: 2018-08-05T18:18:05.328970: step 2180, loss 0.547.
Train: 2018-08-05T18:18:05.500834: step 2181, loss 0.570787.
Train: 2018-08-05T18:18:05.657021: step 2182, loss 0.502281.
Train: 2018-08-05T18:18:05.828854: step 2183, loss 0.53526.
Train: 2018-08-05T18:18:06.000690: step 2184, loss 0.494463.
Train: 2018-08-05T18:18:06.156903: step 2185, loss 0.595859.
Train: 2018-08-05T18:18:06.328763: step 2186, loss 0.617616.
Train: 2018-08-05T18:18:06.516193: step 2187, loss 0.53524.
Train: 2018-08-05T18:18:06.672407: step 2188, loss 0.589905.
Train: 2018-08-05T18:18:06.844241: step 2189, loss 0.648198.
Train: 2018-08-05T18:18:07.016076: step 2190, loss 0.562372.
Test: 2018-08-05T18:18:08.062706: step 2190, loss 0.547631.
Train: 2018-08-05T18:18:08.234566: step 2191, loss 0.503737.
Train: 2018-08-05T18:18:08.390755: step 2192, loss 0.562424.
Train: 2018-08-05T18:18:08.562613: step 2193, loss 0.56245.
Train: 2018-08-05T18:18:08.734424: step 2194, loss 0.570757.
Train: 2018-08-05T18:18:08.906290: step 2195, loss 0.554235.
Train: 2018-08-05T18:18:09.062505: step 2196, loss 0.546017.
Train: 2018-08-05T18:18:09.234337: step 2197, loss 0.570757.
Train: 2018-08-05T18:18:09.421796: step 2198, loss 0.628464.
Train: 2018-08-05T18:18:09.578002: step 2199, loss 0.57076.
Train: 2018-08-05T18:18:09.749814: step 2200, loss 0.554387.
Test: 2018-08-05T18:18:10.812062: step 2200, loss 0.549321.
Train: 2018-08-05T18:18:11.796239: step 2201, loss 0.529902.
Train: 2018-08-05T18:18:11.968041: step 2202, loss 0.538048.
Train: 2018-08-05T18:18:12.139877: step 2203, loss 0.496947.
Train: 2018-08-05T18:18:12.327333: step 2204, loss 0.570756.
Train: 2018-08-05T18:18:12.483546: step 2205, loss 0.545839.
Train: 2018-08-05T18:18:12.655380: step 2206, loss 0.604222.
Train: 2018-08-05T18:18:12.827220: step 2207, loss 0.52881.
Train: 2018-08-05T18:18:12.999053: step 2208, loss 0.604576.
Train: 2018-08-05T18:18:13.170886: step 2209, loss 0.553891.
Train: 2018-08-05T18:18:13.342744: step 2210, loss 0.57084.
Test: 2018-08-05T18:18:14.405002: step 2210, loss 0.548519.
Train: 2018-08-05T18:18:14.561184: step 2211, loss 0.536807.
Train: 2018-08-05T18:18:14.733020: step 2212, loss 0.587966.
Train: 2018-08-05T18:18:14.904853: step 2213, loss 0.605128.
Train: 2018-08-05T18:18:15.076720: step 2214, loss 0.613645.
Train: 2018-08-05T18:18:15.248548: step 2215, loss 0.596423.
Train: 2018-08-05T18:18:15.420390: step 2216, loss 0.579305.
Train: 2018-08-05T18:18:15.592196: step 2217, loss 0.604519.
Train: 2018-08-05T18:18:15.748433: step 2218, loss 0.587512.
Train: 2018-08-05T18:18:15.920258: step 2219, loss 0.570759.
Train: 2018-08-05T18:18:16.092102: step 2220, loss 0.562511.
Test: 2018-08-05T18:18:17.138710: step 2220, loss 0.549445.
Train: 2018-08-05T18:18:17.310565: step 2221, loss 0.505195.
Train: 2018-08-05T18:18:17.482375: step 2222, loss 0.595316.
Train: 2018-08-05T18:18:17.654236: step 2223, loss 0.562611.
Train: 2018-08-05T18:18:17.810449: step 2224, loss 0.538195.
Train: 2018-08-05T18:18:17.982282: step 2225, loss 0.578924.
Train: 2018-08-05T18:18:18.154120: step 2226, loss 0.570776.
Train: 2018-08-05T18:18:18.325930: step 2227, loss 0.530038.
Train: 2018-08-05T18:18:18.482168: step 2228, loss 0.529927.
Train: 2018-08-05T18:18:18.653975: step 2229, loss 0.513307.
Train: 2018-08-05T18:18:18.825837: step 2230, loss 0.60384.
Test: 2018-08-05T18:18:19.872442: step 2230, loss 0.548377.
Train: 2018-08-05T18:18:20.044308: step 2231, loss 0.57076.
Train: 2018-08-05T18:18:20.216140: step 2232, loss 0.595808.
Train: 2018-08-05T18:18:20.387944: step 2233, loss 0.528948.
Train: 2018-08-05T18:18:20.559781: step 2234, loss 0.570786.
Train: 2018-08-05T18:18:20.747266: step 2235, loss 0.638232.
Train: 2018-08-05T18:18:20.919102: step 2236, loss 0.579216.
Train: 2018-08-05T18:18:21.075316: step 2237, loss 0.537168.
Train: 2018-08-05T18:18:21.262742: step 2238, loss 0.461515.
Train: 2018-08-05T18:18:21.434605: step 2239, loss 0.528533.
Train: 2018-08-05T18:18:21.606436: step 2240, loss 0.545301.
Test: 2018-08-05T18:18:22.653042: step 2240, loss 0.548588.
Train: 2018-08-05T18:18:22.840497: step 2241, loss 0.476473.
Train: 2018-08-05T18:18:23.012356: step 2242, loss 0.544964.
Train: 2018-08-05T18:18:23.184165: step 2243, loss 0.544691.
Train: 2018-08-05T18:18:23.355999: step 2244, loss 0.610815.
Train: 2018-08-05T18:18:23.527860: step 2245, loss 0.589902.
Train: 2018-08-05T18:18:23.699694: step 2246, loss 0.464096.
Train: 2018-08-05T18:18:23.871505: step 2247, loss 0.499524.
Train: 2018-08-05T18:18:24.043343: step 2248, loss 0.47178.
Train: 2018-08-05T18:18:24.215205: step 2249, loss 0.562924.
Train: 2018-08-05T18:18:24.371389: step 2250, loss 0.618893.
Test: 2018-08-05T18:18:25.433668: step 2250, loss 0.547819.
Train: 2018-08-05T18:18:25.605504: step 2251, loss 0.581894.
Train: 2018-08-05T18:18:25.777308: step 2252, loss 0.469617.
Train: 2018-08-05T18:18:25.949146: step 2253, loss 0.450381.
Train: 2018-08-05T18:18:26.120980: step 2254, loss 0.535099.
Train: 2018-08-05T18:18:26.292812: step 2255, loss 0.602217.
Train: 2018-08-05T18:18:26.464648: step 2256, loss 0.583247.
Train: 2018-08-05T18:18:26.636507: step 2257, loss 0.564025.
Train: 2018-08-05T18:18:26.792721: step 2258, loss 0.525452.
Train: 2018-08-05T18:18:26.964530: step 2259, loss 0.564015.
Train: 2018-08-05T18:18:27.136367: step 2260, loss 0.563968.
Test: 2018-08-05T18:18:28.198646: step 2260, loss 0.547632.
Train: 2018-08-05T18:18:28.370475: step 2261, loss 0.554284.
Train: 2018-08-05T18:18:28.542288: step 2262, loss 0.53509.
Train: 2018-08-05T18:18:28.698499: step 2263, loss 0.477917.
Train: 2018-08-05T18:18:28.870334: step 2264, loss 0.554152.
Train: 2018-08-05T18:18:29.010957: step 2265, loss 0.543346.
Train: 2018-08-05T18:18:29.182787: step 2266, loss 0.554112.
Train: 2018-08-05T18:18:29.339004: step 2267, loss 0.516133.
Train: 2018-08-05T18:18:29.510808: step 2268, loss 0.601456.
Train: 2018-08-05T18:18:29.682674: step 2269, loss 0.525683.
Train: 2018-08-05T18:18:29.854477: step 2270, loss 0.61984.
Test: 2018-08-05T18:18:30.916730: step 2270, loss 0.547176.
Train: 2018-08-05T18:18:31.088563: step 2271, loss 0.525829.
Train: 2018-08-05T18:18:31.260425: step 2272, loss 0.507327.
Train: 2018-08-05T18:18:31.432260: step 2273, loss 0.50744.
Train: 2018-08-05T18:18:31.604071: step 2274, loss 0.609317.
Train: 2018-08-05T18:18:31.775935: step 2275, loss 0.581383.
Train: 2018-08-05T18:18:31.947764: step 2276, loss 0.60867.
Train: 2018-08-05T18:18:32.119574: step 2277, loss 0.599061.
Train: 2018-08-05T18:18:32.291408: step 2278, loss 0.571578.
Train: 2018-08-05T18:18:32.447620: step 2279, loss 0.518011.
Train: 2018-08-05T18:18:32.619458: step 2280, loss 0.544773.
Test: 2018-08-05T18:18:33.681707: step 2280, loss 0.549358.
Train: 2018-08-05T18:18:33.853568: step 2281, loss 0.492236.
Train: 2018-08-05T18:18:34.025376: step 2282, loss 0.466141.
Train: 2018-08-05T18:18:34.197242: step 2283, loss 0.518519.
Train: 2018-08-05T18:18:34.369076: step 2284, loss 0.500707.
Train: 2018-08-05T18:18:34.556503: step 2285, loss 0.562469.
Train: 2018-08-05T18:18:34.728336: step 2286, loss 0.535724.
Train: 2018-08-05T18:18:34.900196: step 2287, loss 0.544609.
Train: 2018-08-05T18:18:35.072032: step 2288, loss 0.517448.
Train: 2018-08-05T18:18:35.243871: step 2289, loss 0.590062.
Train: 2018-08-05T18:18:35.415677: step 2290, loss 0.544526.
Test: 2018-08-05T18:18:36.477945: step 2290, loss 0.547755.
Train: 2018-08-05T18:18:36.649792: step 2291, loss 0.608738.
Train: 2018-08-05T18:18:36.821621: step 2292, loss 0.553691.
Train: 2018-08-05T18:18:36.977810: step 2293, loss 0.553687.
Train: 2018-08-05T18:18:37.149670: step 2294, loss 0.526197.
Train: 2018-08-05T18:18:37.321481: step 2295, loss 0.535355.
Train: 2018-08-05T18:18:37.508938: step 2296, loss 0.562859.
Train: 2018-08-05T18:18:37.680775: step 2297, loss 0.617867.
Train: 2018-08-05T18:18:37.852606: step 2298, loss 0.581055.
Train: 2018-08-05T18:18:38.024467: step 2299, loss 0.535468.
Train: 2018-08-05T18:18:38.211921: step 2300, loss 0.562656.
Test: 2018-08-05T18:18:39.258526: step 2300, loss 0.54674.
Train: 2018-08-05T18:18:40.305156: step 2301, loss 0.580603.
Train: 2018-08-05T18:18:40.477017: step 2302, loss 0.607283.
Train: 2018-08-05T18:18:40.648856: step 2303, loss 0.482591.
Train: 2018-08-05T18:18:40.820686: step 2304, loss 0.562438.
Train: 2018-08-05T18:18:40.992531: step 2305, loss 0.571224.
Train: 2018-08-05T18:18:41.164331: step 2306, loss 0.544844.
Train: 2018-08-05T18:18:41.336164: step 2307, loss 0.606102.
Train: 2018-08-05T18:18:41.523622: step 2308, loss 0.631926.
Train: 2018-08-05T18:18:41.679836: step 2309, loss 0.510635.
Train: 2018-08-05T18:18:41.851695: step 2310, loss 0.639462.
Test: 2018-08-05T18:18:42.913955: step 2310, loss 0.548365.
Train: 2018-08-05T18:18:43.085755: step 2311, loss 0.638755.
Train: 2018-08-05T18:18:43.257589: step 2312, loss 0.612688.
Train: 2018-08-05T18:18:43.413833: step 2313, loss 0.562493.
Train: 2018-08-05T18:18:43.585637: step 2314, loss 0.554451.
Train: 2018-08-05T18:18:43.757472: step 2315, loss 0.562732.
Train: 2018-08-05T18:18:43.929310: step 2316, loss 0.514799.
Train: 2018-08-05T18:18:44.101142: step 2317, loss 0.546955.
Train: 2018-08-05T18:18:44.272976: step 2318, loss 0.491278.
Train: 2018-08-05T18:18:44.429222: step 2319, loss 0.55489.
Train: 2018-08-05T18:18:44.616648: step 2320, loss 0.498596.
Test: 2018-08-05T18:18:45.663275: step 2320, loss 0.548431.
Train: 2018-08-05T18:18:45.835149: step 2321, loss 0.5627.
Train: 2018-08-05T18:18:46.006945: step 2322, loss 0.513601.
Train: 2018-08-05T18:18:46.194433: step 2323, loss 0.562498.
Train: 2018-08-05T18:18:46.366237: step 2324, loss 0.503979.
Train: 2018-08-05T18:18:46.538072: step 2325, loss 0.605562.
Train: 2018-08-05T18:18:46.709939: step 2326, loss 0.511134.
Train: 2018-08-05T18:18:46.881766: step 2327, loss 0.510605.
Train: 2018-08-05T18:18:47.053578: step 2328, loss 0.614755.
Train: 2018-08-05T18:18:47.225435: step 2329, loss 0.588813.
Train: 2018-08-05T18:18:47.397245: step 2330, loss 0.553594.
Test: 2018-08-05T18:18:48.454921: step 2330, loss 0.548653.
Train: 2018-08-05T18:18:48.626756: step 2331, loss 0.589159.
Train: 2018-08-05T18:18:48.782991: step 2332, loss 0.598158.
Train: 2018-08-05T18:18:48.954802: step 2333, loss 0.580322.
Train: 2018-08-05T18:18:49.126635: step 2334, loss 0.606959.
Train: 2018-08-05T18:18:49.314092: step 2335, loss 0.518176.
Train: 2018-08-05T18:18:49.485927: step 2336, loss 0.668454.
Train: 2018-08-05T18:18:49.657762: step 2337, loss 0.527333.
Train: 2018-08-05T18:18:49.829629: step 2338, loss 0.605922.
Train: 2018-08-05T18:18:50.001433: step 2339, loss 0.562341.
Train: 2018-08-05T18:18:50.173270: step 2340, loss 0.536593.
Test: 2018-08-05T18:18:51.235518: step 2340, loss 0.547886.
Train: 2018-08-05T18:18:51.407376: step 2341, loss 0.562338.
Train: 2018-08-05T18:18:51.579190: step 2342, loss 0.545355.
Train: 2018-08-05T18:18:51.751051: step 2343, loss 0.579293.
Train: 2018-08-05T18:18:51.938508: step 2344, loss 0.461122.
Train: 2018-08-05T18:18:52.110345: step 2345, loss 0.486227.
Train: 2018-08-05T18:18:52.282147: step 2346, loss 0.545308.
Train: 2018-08-05T18:18:52.454007: step 2347, loss 0.562335.
Train: 2018-08-05T18:18:52.625821: step 2348, loss 0.562339.
Train: 2018-08-05T18:18:52.797653: step 2349, loss 0.588389.
Train: 2018-08-05T18:18:52.985142: step 2350, loss 0.623317.
Test: 2018-08-05T18:18:54.031738: step 2350, loss 0.548023.
Train: 2018-08-05T18:18:54.219194: step 2351, loss 0.457927.
Train: 2018-08-05T18:18:54.391029: step 2352, loss 0.597359.
Train: 2018-08-05T18:18:54.562862: step 2353, loss 0.579916.
Train: 2018-08-05T18:18:54.734702: step 2354, loss 0.571161.
Train: 2018-08-05T18:18:54.906533: step 2355, loss 0.51854.
Train: 2018-08-05T18:18:55.078368: step 2356, loss 0.553612.
Train: 2018-08-05T18:18:55.250203: step 2357, loss 0.571211.
Train: 2018-08-05T18:18:55.422041: step 2358, loss 0.562413.
Train: 2018-08-05T18:18:55.593872: step 2359, loss 0.518355.
Train: 2018-08-05T18:18:55.765707: step 2360, loss 0.597759.
Test: 2018-08-05T18:18:56.827957: step 2360, loss 0.548698.
Train: 2018-08-05T18:18:56.984171: step 2361, loss 0.562428.
Train: 2018-08-05T18:18:57.156007: step 2362, loss 0.52713.
Train: 2018-08-05T18:18:57.343489: step 2363, loss 0.535936.
Train: 2018-08-05T18:18:57.515328: step 2364, loss 0.544747.
Train: 2018-08-05T18:18:57.687156: step 2365, loss 0.58019.
Train: 2018-08-05T18:18:57.874614: step 2366, loss 0.606815.
Train: 2018-08-05T18:18:58.030805: step 2367, loss 0.58014.
Train: 2018-08-05T18:18:58.202636: step 2368, loss 0.650582.
Train: 2018-08-05T18:18:58.390091: step 2369, loss 0.693488.
Train: 2018-08-05T18:18:58.561926: step 2370, loss 0.58816.
Test: 2018-08-05T18:18:59.608582: step 2370, loss 0.548392.
Train: 2018-08-05T18:18:59.796015: step 2371, loss 0.562352.
Train: 2018-08-05T18:18:59.967848: step 2372, loss 0.528976.
Train: 2018-08-05T18:19:00.139682: step 2373, loss 0.529393.
Train: 2018-08-05T18:19:00.327138: step 2374, loss 0.620059.
Train: 2018-08-05T18:19:00.530246: step 2375, loss 0.611473.
Train: 2018-08-05T18:19:00.686461: step 2376, loss 0.594981.
Train: 2018-08-05T18:19:00.873910: step 2377, loss 0.554964.
Train: 2018-08-05T18:19:01.045722: step 2378, loss 0.555169.
Train: 2018-08-05T18:19:01.217555: step 2379, loss 0.500384.
Train: 2018-08-05T18:19:01.389391: step 2380, loss 0.516125.
Test: 2018-08-05T18:19:02.451642: step 2380, loss 0.54944.
Train: 2018-08-05T18:19:02.623476: step 2381, loss 0.555257.
Train: 2018-08-05T18:19:02.795313: step 2382, loss 0.578861.
Train: 2018-08-05T18:19:02.967147: step 2383, loss 0.539168.
Train: 2018-08-05T18:19:03.138980: step 2384, loss 0.530945.
Train: 2018-08-05T18:19:03.310815: step 2385, loss 0.643288.
Train: 2018-08-05T18:19:03.498274: step 2386, loss 0.586964.
Train: 2018-08-05T18:19:03.670108: step 2387, loss 0.627464.
Train: 2018-08-05T18:19:03.841966: step 2388, loss 0.546551.
Train: 2018-08-05T18:19:04.013777: step 2389, loss 0.603158.
Train: 2018-08-05T18:19:04.185640: step 2390, loss 0.578887.
Test: 2018-08-05T18:19:05.232242: step 2390, loss 0.548718.
Train: 2018-08-05T18:19:05.404100: step 2391, loss 0.595017.
Train: 2018-08-05T18:19:05.575939: step 2392, loss 0.522545.
Train: 2018-08-05T18:19:05.763368: step 2393, loss 0.490276.
Train: 2018-08-05T18:19:05.935199: step 2394, loss 0.554591.
Train: 2018-08-05T18:19:06.107036: step 2395, loss 0.627858.
Train: 2018-08-05T18:19:06.278870: step 2396, loss 0.505365.
Train: 2018-08-05T18:19:06.450705: step 2397, loss 0.529628.
Train: 2018-08-05T18:19:06.638185: step 2398, loss 0.512716.
Train: 2018-08-05T18:19:06.809999: step 2399, loss 0.587534.
Train: 2018-08-05T18:19:06.997456: step 2400, loss 0.503239.
Test: 2018-08-05T18:19:08.044082: step 2400, loss 0.547686.
Train: 2018-08-05T18:19:09.028226: step 2401, loss 0.59649.
Train: 2018-08-05T18:19:09.200091: step 2402, loss 0.57094.
Train: 2018-08-05T18:19:09.387517: step 2403, loss 0.622957.
Train: 2018-08-05T18:19:09.559355: step 2404, loss 0.571026.
Train: 2018-08-05T18:19:09.731186: step 2405, loss 0.623171.
Train: 2018-08-05T18:19:09.903047: step 2406, loss 0.553679.
Train: 2018-08-05T18:19:10.090487: step 2407, loss 0.596941.
Train: 2018-08-05T18:19:10.262314: step 2408, loss 0.562337.
Train: 2018-08-05T18:19:10.434147: step 2409, loss 0.588092.
Train: 2018-08-05T18:19:10.621602: step 2410, loss 0.553792.
Test: 2018-08-05T18:19:11.668232: step 2410, loss 0.548519.
Train: 2018-08-05T18:19:11.840101: step 2411, loss 0.587879.
Train: 2018-08-05T18:19:12.027523: step 2412, loss 0.545413.
Train: 2018-08-05T18:19:12.199385: step 2413, loss 0.579251.
Train: 2018-08-05T18:19:12.371220: step 2414, loss 0.587613.
Train: 2018-08-05T18:19:12.558648: step 2415, loss 0.554033.
Train: 2018-08-05T18:19:12.714862: step 2416, loss 0.473444.
Train: 2018-08-05T18:19:12.886722: step 2417, loss 0.595856.
Train: 2018-08-05T18:19:13.074186: step 2418, loss 0.646057.
Train: 2018-08-05T18:19:13.246018: step 2419, loss 0.562434.
Train: 2018-08-05T18:19:13.433446: step 2420, loss 0.520954.
Test: 2018-08-05T18:19:14.480074: step 2420, loss 0.547838.
Train: 2018-08-05T18:19:14.667531: step 2421, loss 0.645439.
Train: 2018-08-05T18:19:14.854988: step 2422, loss 0.570756.
Train: 2018-08-05T18:19:15.042442: step 2423, loss 0.537871.
Train: 2018-08-05T18:19:15.214277: step 2424, loss 0.521529.
Train: 2018-08-05T18:19:15.401733: step 2425, loss 0.620065.
Train: 2018-08-05T18:19:15.573573: step 2426, loss 0.570762.
Train: 2018-08-05T18:19:15.776647: step 2427, loss 0.521656.
Train: 2018-08-05T18:19:15.948505: step 2428, loss 0.644533.
Train: 2018-08-05T18:19:16.135961: step 2429, loss 0.595284.
Train: 2018-08-05T18:19:16.323418: step 2430, loss 0.546365.
Test: 2018-08-05T18:19:17.385675: step 2430, loss 0.547608.
Train: 2018-08-05T18:19:17.557511: step 2431, loss 0.587029.
Train: 2018-08-05T18:19:17.744937: step 2432, loss 0.635587.
Train: 2018-08-05T18:19:17.932415: step 2433, loss 0.522534.
Train: 2018-08-05T18:19:18.119848: step 2434, loss 0.538722.
Train: 2018-08-05T18:19:18.307327: step 2435, loss 0.55478.
Train: 2018-08-05T18:19:18.494763: step 2436, loss 0.554751.
Train: 2018-08-05T18:19:18.697868: step 2437, loss 0.603061.
Train: 2018-08-05T18:19:18.885317: step 2438, loss 0.578881.
Train: 2018-08-05T18:19:19.057160: step 2439, loss 0.554684.
Train: 2018-08-05T18:19:19.244610: step 2440, loss 0.570809.
Test: 2018-08-05T18:19:20.306840: step 2440, loss 0.549018.
Train: 2018-08-05T18:19:20.478671: step 2441, loss 0.457557.
Train: 2018-08-05T18:19:20.681749: step 2442, loss 0.619711.
Train: 2018-08-05T18:19:20.869203: step 2443, loss 0.546181.
Train: 2018-08-05T18:19:21.056660: step 2444, loss 0.463631.
Train: 2018-08-05T18:19:21.244141: step 2445, loss 0.562431.
Train: 2018-08-05T18:19:21.431596: step 2446, loss 0.587639.
Train: 2018-08-05T18:19:21.619059: step 2447, loss 0.5199.
Train: 2018-08-05T18:19:21.806508: step 2448, loss 0.54519.
Train: 2018-08-05T18:19:21.993942: step 2449, loss 0.640244.
Train: 2018-08-05T18:19:22.197052: step 2450, loss 0.597118.
Test: 2018-08-05T18:19:23.243647: step 2450, loss 0.54742.
Train: 2018-08-05T18:19:23.446727: step 2451, loss 0.484016.
Train: 2018-08-05T18:19:23.634185: step 2452, loss 0.571131.
Train: 2018-08-05T18:19:23.821636: step 2453, loss 0.518456.
Train: 2018-08-05T18:19:24.009119: step 2454, loss 0.606633.
Train: 2018-08-05T18:19:24.212172: step 2455, loss 0.553592.
Train: 2018-08-05T18:19:24.399626: step 2456, loss 0.518076.
Train: 2018-08-05T18:19:24.602732: step 2457, loss 0.509027.
Train: 2018-08-05T18:19:24.790190: step 2458, loss 0.562556.
Train: 2018-08-05T18:19:24.977620: step 2459, loss 0.535591.
Train: 2018-08-05T18:19:25.180695: step 2460, loss 0.51742.
Test: 2018-08-05T18:19:26.227324: step 2460, loss 0.548381.
Train: 2018-08-05T18:19:26.430402: step 2461, loss 0.4535.
Train: 2018-08-05T18:19:26.617858: step 2462, loss 0.590508.
Train: 2018-08-05T18:19:26.805313: step 2463, loss 0.590824.
Train: 2018-08-05T18:19:27.008418: step 2464, loss 0.544512.
Train: 2018-08-05T18:19:27.211468: step 2465, loss 0.572498.
Train: 2018-08-05T18:19:27.398924: step 2466, loss 0.637905.
Train: 2018-08-05T18:19:27.602004: step 2467, loss 0.58172.
Train: 2018-08-05T18:19:27.836323: step 2468, loss 0.563007.
Train: 2018-08-05T18:19:28.023803: step 2469, loss 0.608863.
Train: 2018-08-05T18:19:28.211237: step 2470, loss 0.590092.
Test: 2018-08-05T18:19:29.273485: step 2470, loss 0.547978.
Train: 2018-08-05T18:19:29.476564: step 2471, loss 0.553604.
Train: 2018-08-05T18:19:29.679643: step 2472, loss 0.553589.
Train: 2018-08-05T18:19:29.882751: step 2473, loss 0.553594.
Train: 2018-08-05T18:19:30.085797: step 2474, loss 0.588723.
Train: 2018-08-05T18:19:30.288896: step 2475, loss 0.571002.
Train: 2018-08-05T18:19:30.476328: step 2476, loss 0.520143.
Train: 2018-08-05T18:19:30.679407: step 2477, loss 0.588094.
Train: 2018-08-05T18:19:30.882513: step 2478, loss 0.596476.
Train: 2018-08-05T18:19:31.085590: step 2479, loss 0.57082.
Train: 2018-08-05T18:19:31.288643: step 2480, loss 0.58759.
Test: 2018-08-05T18:19:32.350890: step 2480, loss 0.549084.
Train: 2018-08-05T18:19:32.553966: step 2481, loss 0.604106.
Train: 2018-08-05T18:19:32.757069: step 2482, loss 0.587271.
Train: 2018-08-05T18:19:32.960123: step 2483, loss 0.546268.
Train: 2018-08-05T18:19:33.163198: step 2484, loss 0.562654.
Train: 2018-08-05T18:19:33.366277: step 2485, loss 0.514179.
Train: 2018-08-05T18:19:33.569357: step 2486, loss 0.612106.
Train: 2018-08-05T18:19:33.788053: step 2487, loss 0.56268.
Train: 2018-08-05T18:19:33.991131: step 2488, loss 0.619051.
Train: 2018-08-05T18:19:34.209828: step 2489, loss 0.51548.
Train: 2018-08-05T18:19:34.428528: step 2490, loss 0.547001.
Test: 2018-08-05T18:19:35.475159: step 2490, loss 0.549046.
Train: 2018-08-05T18:19:35.725098: step 2491, loss 0.531019.
Train: 2018-08-05T18:19:35.943797: step 2492, loss 0.538845.
Train: 2018-08-05T18:19:36.146876: step 2493, loss 0.578876.
Train: 2018-08-05T18:19:36.365601: step 2494, loss 0.554625.
Train: 2018-08-05T18:19:36.568651: step 2495, loss 0.554513.
Train: 2018-08-05T18:19:36.787352: step 2496, loss 0.562582.
Train: 2018-08-05T18:19:36.990453: step 2497, loss 0.60369.
Train: 2018-08-05T18:19:37.224774: step 2498, loss 0.529449.
Train: 2018-08-05T18:19:37.427849: step 2499, loss 0.520919.
Train: 2018-08-05T18:19:37.646555: step 2500, loss 0.579146.
Test: 2018-08-05T18:19:38.708775: step 2500, loss 0.546563.
Train: 2018-08-05T18:19:39.661679: step 2501, loss 0.494992.
Train: 2018-08-05T18:19:39.880377: step 2502, loss 0.604865.
Train: 2018-08-05T18:19:40.099108: step 2503, loss 0.630794.
Train: 2018-08-05T18:19:40.317776: step 2504, loss 0.545192.
Train: 2018-08-05T18:19:40.536496: step 2505, loss 0.553743.
Train: 2018-08-05T18:19:40.770825: step 2506, loss 0.527877.
Train: 2018-08-05T18:19:40.989493: step 2507, loss 0.588294.
Train: 2018-08-05T18:19:41.223835: step 2508, loss 0.588354.
Train: 2018-08-05T18:19:41.442509: step 2509, loss 0.571018.
Train: 2018-08-05T18:19:41.661208: step 2510, loss 0.458338.
Test: 2018-08-05T18:19:42.707840: step 2510, loss 0.549013.
Train: 2018-08-05T18:19:42.942161: step 2511, loss 0.571074.
Train: 2018-08-05T18:19:43.160858: step 2512, loss 0.58862.
Train: 2018-08-05T18:19:43.379557: step 2513, loss 0.623729.
Train: 2018-08-05T18:19:43.598254: step 2514, loss 0.579864.
Train: 2018-08-05T18:19:43.832611: step 2515, loss 0.605945.
Train: 2018-08-05T18:19:44.051275: step 2516, loss 0.553678.
Train: 2018-08-05T18:19:44.285595: step 2517, loss 0.501952.
Train: 2018-08-05T18:19:44.504293: step 2518, loss 0.493393.
Train: 2018-08-05T18:19:44.722995: step 2519, loss 0.56234.
Train: 2018-08-05T18:19:44.957314: step 2520, loss 0.519025.
Test: 2018-08-05T18:19:46.003943: step 2520, loss 0.547622.
Train: 2018-08-05T18:19:46.238263: step 2521, loss 0.588465.
Train: 2018-08-05T18:19:46.456962: step 2522, loss 0.64087.
Train: 2018-08-05T18:19:46.675660: step 2523, loss 0.544956.
Train: 2018-08-05T18:19:46.894389: step 2524, loss 0.579723.
Train: 2018-08-05T18:19:47.113082: step 2525, loss 0.536356.
Train: 2018-08-05T18:19:47.331757: step 2526, loss 0.596957.
Train: 2018-08-05T18:19:47.550485: step 2527, loss 0.588217.
Train: 2018-08-05T18:19:47.784810: step 2528, loss 0.570923.
Train: 2018-08-05T18:19:48.019126: step 2529, loss 0.570886.
Train: 2018-08-05T18:19:48.237797: step 2530, loss 0.630424.
Test: 2018-08-05T18:19:49.300045: step 2530, loss 0.546712.
Train: 2018-08-05T18:19:49.518749: step 2531, loss 0.520155.
Train: 2018-08-05T18:19:49.737442: step 2532, loss 0.553982.
Train: 2018-08-05T18:19:49.956146: step 2533, loss 0.637764.
Train: 2018-08-05T18:19:50.174840: step 2534, loss 0.545823.
Train: 2018-08-05T18:19:50.409167: step 2535, loss 0.537682.
Train: 2018-08-05T18:19:50.627891: step 2536, loss 0.488308.
Train: 2018-08-05T18:19:50.846583: step 2537, loss 0.57902.
Train: 2018-08-05T18:19:51.065256: step 2538, loss 0.645238.
Train: 2018-08-05T18:19:51.283982: step 2539, loss 0.603755.
Train: 2018-08-05T18:19:51.518277: step 2540, loss 0.562551.
Test: 2018-08-05T18:19:52.564905: step 2540, loss 0.547426.
Train: 2018-08-05T18:19:52.799252: step 2541, loss 0.595301.
Train: 2018-08-05T18:19:53.033577: step 2542, loss 0.521953.
Train: 2018-08-05T18:19:53.252247: step 2543, loss 0.513897.
Train: 2018-08-05T18:19:53.470971: step 2544, loss 0.530048.
Train: 2018-08-05T18:19:53.689668: step 2545, loss 0.529846.
Train: 2018-08-05T18:19:53.908365: step 2546, loss 0.537802.
Train: 2018-08-05T18:19:54.127072: step 2547, loss 0.595672.
Train: 2018-08-05T18:19:54.345743: step 2548, loss 0.537368.
Train: 2018-08-05T18:19:54.564469: step 2549, loss 0.570789.
Train: 2018-08-05T18:19:54.783140: step 2550, loss 0.587725.
Test: 2018-08-05T18:19:55.845388: step 2550, loss 0.54837.
Train: 2018-08-05T18:19:56.079710: step 2551, loss 0.562349.
Train: 2018-08-05T18:19:56.298436: step 2552, loss 0.579375.
Train: 2018-08-05T18:19:56.532758: step 2553, loss 0.553803.
Train: 2018-08-05T18:19:56.751427: step 2554, loss 0.493881.
Train: 2018-08-05T18:19:56.970125: step 2555, loss 0.510679.
Train: 2018-08-05T18:19:57.188853: step 2556, loss 0.56235.
Train: 2018-08-05T18:19:57.423143: step 2557, loss 0.57112.
Train: 2018-08-05T18:19:57.641868: step 2558, loss 0.492044.
Train: 2018-08-05T18:19:57.876166: step 2559, loss 0.571334.
Train: 2018-08-05T18:19:58.094860: step 2560, loss 0.517861.
Test: 2018-08-05T18:19:59.157137: step 2560, loss 0.54736.
Train: 2018-08-05T18:19:59.375837: step 2561, loss 0.535594.
Train: 2018-08-05T18:19:59.610165: step 2562, loss 0.499177.
Train: 2018-08-05T18:19:59.828859: step 2563, loss 0.535355.
Train: 2018-08-05T18:20:00.047532: step 2564, loss 0.535259.
Train: 2018-08-05T18:20:00.266251: step 2565, loss 0.56318.
Train: 2018-08-05T18:20:00.500549: step 2566, loss 0.563326.
Train: 2018-08-05T18:20:00.688034: step 2567, loss 0.583572.
Train: 2018-08-05T18:20:00.906732: step 2568, loss 0.63913.
Train: 2018-08-05T18:20:01.141053: step 2569, loss 0.629349.
Train: 2018-08-05T18:20:01.359721: step 2570, loss 0.619296.
Test: 2018-08-05T18:20:02.421973: step 2570, loss 0.547345.
Train: 2018-08-05T18:20:02.640671: step 2571, loss 0.526027.
Train: 2018-08-05T18:20:02.859370: step 2572, loss 0.571986.
Train: 2018-08-05T18:20:03.093692: step 2573, loss 0.598947.
Train: 2018-08-05T18:20:03.312388: step 2574, loss 0.57151.
Train: 2018-08-05T18:20:03.531114: step 2575, loss 0.527028.
Train: 2018-08-05T18:20:03.749786: step 2576, loss 0.588716.
Train: 2018-08-05T18:20:03.968517: step 2577, loss 0.649221.
Train: 2018-08-05T18:20:04.187183: step 2578, loss 0.502387.
Train: 2018-08-05T18:20:04.421505: step 2579, loss 0.519934.
Train: 2018-08-05T18:20:04.640236: step 2580, loss 0.528641.
Test: 2018-08-05T18:20:05.686833: step 2580, loss 0.549122.
Train: 2018-08-05T18:20:05.921157: step 2581, loss 0.55398.
Train: 2018-08-05T18:20:06.139852: step 2582, loss 0.528854.
Train: 2018-08-05T18:20:06.358575: step 2583, loss 0.64626.
Train: 2018-08-05T18:20:06.577249: step 2584, loss 0.587465.
Train: 2018-08-05T18:20:06.811604: step 2585, loss 0.529236.
Train: 2018-08-05T18:20:07.030295: step 2586, loss 0.529335.
Train: 2018-08-05T18:20:07.248967: step 2587, loss 0.595614.
Train: 2018-08-05T18:20:07.467693: step 2588, loss 0.529388.
Train: 2018-08-05T18:20:07.686367: step 2589, loss 0.463082.
Train: 2018-08-05T18:20:07.920688: step 2590, loss 0.495681.
Test: 2018-08-05T18:20:08.967315: step 2590, loss 0.547899.
Train: 2018-08-05T18:20:09.186014: step 2591, loss 0.537073.
Train: 2018-08-05T18:20:09.420334: step 2592, loss 0.553812.
Train: 2018-08-05T18:20:09.639057: step 2593, loss 0.58819.
Train: 2018-08-05T18:20:09.857731: step 2594, loss 0.588408.
Train: 2018-08-05T18:20:10.076454: step 2595, loss 0.544904.
Train: 2018-08-05T18:20:10.295129: step 2596, loss 0.597517.
Train: 2018-08-05T18:20:10.513852: step 2597, loss 0.544802.
Train: 2018-08-05T18:20:10.732528: step 2598, loss 0.597747.
Train: 2018-08-05T18:20:10.951251: step 2599, loss 0.544766.
Train: 2018-08-05T18:20:11.169954: step 2600, loss 0.535918.
Test: 2018-08-05T18:20:12.232202: step 2600, loss 0.548272.
Train: 2018-08-05T18:20:13.200698: step 2601, loss 0.544739.
Train: 2018-08-05T18:20:13.435050: step 2602, loss 0.544718.
Train: 2018-08-05T18:20:13.653717: step 2603, loss 0.606948.
Train: 2018-08-05T18:20:13.872417: step 2604, loss 0.615787.
Train: 2018-08-05T18:20:14.091140: step 2605, loss 0.650918.
Train: 2018-08-05T18:20:14.325469: step 2606, loss 0.501023.
Train: 2018-08-05T18:20:14.544134: step 2607, loss 0.614687.
Train: 2018-08-05T18:20:14.762859: step 2608, loss 0.570994.
Train: 2018-08-05T18:20:14.981561: step 2609, loss 0.519415.
Train: 2018-08-05T18:20:15.200230: step 2610, loss 0.519619.
Test: 2018-08-05T18:20:16.262480: step 2610, loss 0.549274.
Train: 2018-08-05T18:20:16.481181: step 2611, loss 0.579397.
Train: 2018-08-05T18:20:16.715502: step 2612, loss 0.54533.
Train: 2018-08-05T18:20:16.918578: step 2613, loss 0.485883.
Train: 2018-08-05T18:20:17.137280: step 2614, loss 0.570865.
Train: 2018-08-05T18:20:17.356003: step 2615, loss 0.528153.
Train: 2018-08-05T18:20:17.590295: step 2616, loss 0.510841.
Train: 2018-08-05T18:20:17.808996: step 2617, loss 0.527781.
Train: 2018-08-05T18:20:18.043315: step 2618, loss 0.571066.
Train: 2018-08-05T18:20:18.262038: step 2619, loss 0.650007.
Train: 2018-08-05T18:20:18.465092: step 2620, loss 0.562386.
Test: 2018-08-05T18:20:19.527341: step 2620, loss 0.548957.
Train: 2018-08-05T18:20:19.746067: step 2621, loss 0.492261.
Train: 2018-08-05T18:20:19.964740: step 2622, loss 0.544811.
Train: 2018-08-05T18:20:20.183439: step 2623, loss 0.509444.
Train: 2018-08-05T18:20:20.402169: step 2624, loss 0.589123.
Train: 2018-08-05T18:20:20.620835: step 2625, loss 0.553588.
Train: 2018-08-05T18:20:20.839566: step 2626, loss 0.598288.
Train: 2018-08-05T18:20:21.058233: step 2627, loss 0.607232.
Train: 2018-08-05T18:20:21.276931: step 2628, loss 0.624898.
Train: 2018-08-05T18:20:21.495630: step 2629, loss 0.597865.
Train: 2018-08-05T18:20:21.714332: step 2630, loss 0.536056.
Test: 2018-08-05T18:20:22.776581: step 2630, loss 0.547597.
Train: 2018-08-05T18:20:22.995279: step 2631, loss 0.571087.
Train: 2018-08-05T18:20:23.213981: step 2632, loss 0.48437.
Train: 2018-08-05T18:20:23.432676: step 2633, loss 0.553693.
Train: 2018-08-05T18:20:23.651378: step 2634, loss 0.536427.
Train: 2018-08-05T18:20:23.870074: step 2635, loss 0.596895.
Train: 2018-08-05T18:20:24.104396: step 2636, loss 0.605447.
Train: 2018-08-05T18:20:24.323094: step 2637, loss 0.588089.
Train: 2018-08-05T18:20:24.541827: step 2638, loss 0.587951.
Train: 2018-08-05T18:20:24.760516: step 2639, loss 0.56235.
Train: 2018-08-05T18:20:24.979217: step 2640, loss 0.520194.
Test: 2018-08-05T18:20:26.041440: step 2640, loss 0.548134.
Train: 2018-08-05T18:20:26.260142: step 2641, loss 0.604447.
Train: 2018-08-05T18:20:26.478839: step 2642, loss 0.503773.
Train: 2018-08-05T18:20:26.697538: step 2643, loss 0.528915.
Train: 2018-08-05T18:20:26.916267: step 2644, loss 0.621108.
Train: 2018-08-05T18:20:27.134935: step 2645, loss 0.604277.
Train: 2018-08-05T18:20:27.353660: step 2646, loss 0.620837.
Train: 2018-08-05T18:20:27.572357: step 2647, loss 0.512715.
Train: 2018-08-05T18:20:27.806652: step 2648, loss 0.554213.
Train: 2018-08-05T18:20:28.025352: step 2649, loss 0.612062.
Train: 2018-08-05T18:20:28.244083: step 2650, loss 0.554296.
Test: 2018-08-05T18:20:29.306302: step 2650, loss 0.548268.
Train: 2018-08-05T18:20:29.525003: step 2651, loss 0.562549.
Train: 2018-08-05T18:20:29.743703: step 2652, loss 0.611753.
Train: 2018-08-05T18:20:29.978019: step 2653, loss 0.529939.
Train: 2018-08-05T18:20:30.196719: step 2654, loss 0.603406.
Train: 2018-08-05T18:20:30.415417: step 2655, loss 0.562644.
Train: 2018-08-05T18:20:30.649737: step 2656, loss 0.530182.
Train: 2018-08-05T18:20:30.868436: step 2657, loss 0.497624.
Train: 2018-08-05T18:20:31.087134: step 2658, loss 0.562595.
Train: 2018-08-05T18:20:31.305836: step 2659, loss 0.546106.
Train: 2018-08-05T18:20:31.524538: step 2660, loss 0.512866.
Test: 2018-08-05T18:20:32.571163: step 2660, loss 0.547716.
Train: 2018-08-05T18:20:32.805483: step 2661, loss 0.612495.
Train: 2018-08-05T18:20:33.024211: step 2662, loss 0.621131.
Train: 2018-08-05T18:20:33.242881: step 2663, loss 0.612817.
Train: 2018-08-05T18:20:33.461613: step 2664, loss 0.587575.
Train: 2018-08-05T18:20:33.680304: step 2665, loss 0.554026.
Train: 2018-08-05T18:20:33.898975: step 2666, loss 0.637683.
Train: 2018-08-05T18:20:34.117705: step 2667, loss 0.529169.
Train: 2018-08-05T18:20:34.351995: step 2668, loss 0.579055.
Train: 2018-08-05T18:20:34.555097: step 2669, loss 0.587302.
Train: 2018-08-05T18:20:34.773771: step 2670, loss 0.54603.
Test: 2018-08-05T18:20:35.836052: step 2670, loss 0.548225.
Train: 2018-08-05T18:20:36.054752: step 2671, loss 0.562532.
Train: 2018-08-05T18:20:36.273446: step 2672, loss 0.595408.
Train: 2018-08-05T18:20:36.492145: step 2673, loss 0.521593.
Train: 2018-08-05T18:20:36.710818: step 2674, loss 0.53796.
Train: 2018-08-05T18:20:36.929517: step 2675, loss 0.513205.
Train: 2018-08-05T18:20:37.148216: step 2676, loss 0.595565.
Train: 2018-08-05T18:20:37.366915: step 2677, loss 0.603958.
Train: 2018-08-05T18:20:37.585613: step 2678, loss 0.554141.
Train: 2018-08-05T18:20:37.788715: step 2679, loss 0.579088.
Train: 2018-08-05T18:20:38.007391: step 2680, loss 0.587433.
Test: 2018-08-05T18:20:39.069639: step 2680, loss 0.548516.
Train: 2018-08-05T18:20:39.288341: step 2681, loss 0.604094.
Train: 2018-08-05T18:20:39.507038: step 2682, loss 0.520883.
Train: 2018-08-05T18:20:39.725737: step 2683, loss 0.51253.
Train: 2018-08-05T18:20:39.944465: step 2684, loss 0.554066.
Train: 2018-08-05T18:20:40.163160: step 2685, loss 0.486902.
Train: 2018-08-05T18:20:40.381860: step 2686, loss 0.613118.
Train: 2018-08-05T18:20:40.584936: step 2687, loss 0.630354.
Train: 2018-08-05T18:20:40.834853: step 2688, loss 0.570849.
Train: 2018-08-05T18:20:41.053550: step 2689, loss 0.545335.
Train: 2018-08-05T18:20:41.256629: step 2690, loss 0.443159.
Test: 2018-08-05T18:20:42.318881: step 2690, loss 0.546623.
Train: 2018-08-05T18:20:42.537581: step 2691, loss 0.605236.
Train: 2018-08-05T18:20:42.756278: step 2692, loss 0.570956.
Train: 2018-08-05T18:20:42.974978: step 2693, loss 0.605586.
Train: 2018-08-05T18:20:43.178057: step 2694, loss 0.570995.
Train: 2018-08-05T18:20:43.396779: step 2695, loss 0.588294.
Train: 2018-08-05T18:20:43.615455: step 2696, loss 0.519167.
Train: 2018-08-05T18:20:43.834175: step 2697, loss 0.50186.
Train: 2018-08-05T18:20:44.037228: step 2698, loss 0.510306.
Train: 2018-08-05T18:20:44.255926: step 2699, loss 0.527455.
Train: 2018-08-05T18:20:44.474624: step 2700, loss 0.562402.
Test: 2018-08-05T18:20:45.521255: step 2700, loss 0.54726.
Train: 2018-08-05T18:20:46.474160: step 2701, loss 0.633213.
Train: 2018-08-05T18:20:46.677260: step 2702, loss 0.50931.
Train: 2018-08-05T18:20:46.895936: step 2703, loss 0.589139.
Train: 2018-08-05T18:20:47.114632: step 2704, loss 0.517998.
Train: 2018-08-05T18:20:47.317709: step 2705, loss 0.589285.
Train: 2018-08-05T18:20:47.520787: step 2706, loss 0.616095.
Train: 2018-08-05T18:20:47.739514: step 2707, loss 0.526881.
Train: 2018-08-05T18:20:47.942597: step 2708, loss 0.491351.
Train: 2018-08-05T18:20:48.161261: step 2709, loss 0.535765.
Train: 2018-08-05T18:20:48.364341: step 2710, loss 0.46421.
Test: 2018-08-05T18:20:49.426590: step 2710, loss 0.547774.
Train: 2018-08-05T18:20:49.645292: step 2711, loss 0.49957.
Train: 2018-08-05T18:20:49.848367: step 2712, loss 0.608182.
Train: 2018-08-05T18:20:50.051446: step 2713, loss 0.608496.
Train: 2018-08-05T18:20:50.270172: step 2714, loss 0.535372.
Train: 2018-08-05T18:20:50.473225: step 2715, loss 0.507852.
Train: 2018-08-05T18:20:50.676297: step 2716, loss 0.60891.
Train: 2018-08-05T18:20:50.895021: step 2717, loss 0.608897.
Train: 2018-08-05T18:20:51.066857: step 2718, loss 0.445545.
Train: 2018-08-05T18:20:51.285535: step 2719, loss 0.553698.
Train: 2018-08-05T18:20:51.504256: step 2720, loss 0.5813.
Test: 2018-08-05T18:20:52.566483: step 2720, loss 0.547546.
Train: 2018-08-05T18:20:52.769582: step 2721, loss 0.581276.
Train: 2018-08-05T18:20:52.972662: step 2722, loss 0.517017.
Train: 2018-08-05T18:20:53.175713: step 2723, loss 0.55368.
Train: 2018-08-05T18:20:53.394411: step 2724, loss 0.562822.
Train: 2018-08-05T18:20:53.597491: step 2725, loss 0.626718.
Train: 2018-08-05T18:20:53.800593: step 2726, loss 0.599012.
Train: 2018-08-05T18:20:54.019295: step 2727, loss 0.580606.
Train: 2018-08-05T18:20:54.237991: step 2728, loss 0.571428.
Train: 2018-08-05T18:20:54.441043: step 2729, loss 0.562433.
Train: 2018-08-05T18:20:54.644118: step 2730, loss 0.579898.
Test: 2018-08-05T18:20:55.706369: step 2730, loss 0.548257.
Train: 2018-08-05T18:20:55.909449: step 2731, loss 0.597054.
Train: 2018-08-05T18:20:56.112526: step 2732, loss 0.527998.
Train: 2018-08-05T18:20:56.315626: step 2733, loss 0.61345.
Train: 2018-08-05T18:20:56.518704: step 2734, loss 0.621407.
Train: 2018-08-05T18:20:56.721787: step 2735, loss 0.579095.
Train: 2018-08-05T18:20:56.924836: step 2736, loss 0.669555.
Train: 2018-08-05T18:20:57.127938: step 2737, loss 0.530312.
Train: 2018-08-05T18:20:57.330992: step 2738, loss 0.610852.
Train: 2018-08-05T18:20:57.549690: step 2739, loss 0.55519.
Train: 2018-08-05T18:20:57.752766: step 2740, loss 0.547658.
Test: 2018-08-05T18:20:58.799395: step 2740, loss 0.5503.
Train: 2018-08-05T18:20:59.002503: step 2741, loss 0.524672.
Train: 2018-08-05T18:20:59.221203: step 2742, loss 0.470769.
Train: 2018-08-05T18:20:59.424248: step 2743, loss 0.555631.
Train: 2018-08-05T18:20:59.627331: step 2744, loss 0.524256.
Train: 2018-08-05T18:20:59.830436: step 2745, loss 0.547375.
Train: 2018-08-05T18:21:00.033514: step 2746, loss 0.61066.
Train: 2018-08-05T18:21:00.220939: step 2747, loss 0.57086.
Train: 2018-08-05T18:21:00.424039: step 2748, loss 0.570822.
Train: 2018-08-05T18:21:00.611470: step 2749, loss 0.595107.
Train: 2018-08-05T18:21:00.814580: step 2750, loss 0.603329.
Test: 2018-08-05T18:21:01.876799: step 2750, loss 0.54807.
Train: 2018-08-05T18:21:02.079876: step 2751, loss 0.546318.
Train: 2018-08-05T18:21:02.267333: step 2752, loss 0.652552.
Train: 2018-08-05T18:21:02.470412: step 2753, loss 0.562609.
Train: 2018-08-05T18:21:02.673518: step 2754, loss 0.554469.
Train: 2018-08-05T18:21:02.876566: step 2755, loss 0.554469.
Train: 2018-08-05T18:21:03.064022: step 2756, loss 0.513638.
Train: 2018-08-05T18:21:03.267127: step 2757, loss 0.570762.
Train: 2018-08-05T18:21:03.454587: step 2758, loss 0.587223.
Train: 2018-08-05T18:21:03.657657: step 2759, loss 0.504718.
Train: 2018-08-05T18:21:03.845091: step 2760, loss 0.479383.
Test: 2018-08-05T18:21:04.907340: step 2760, loss 0.548362.
Train: 2018-08-05T18:21:05.094820: step 2761, loss 0.579184.
Train: 2018-08-05T18:21:05.282253: step 2762, loss 0.511509.
Train: 2018-08-05T18:21:05.485331: step 2763, loss 0.648023.
Train: 2018-08-05T18:21:05.688432: step 2764, loss 0.519278.
Train: 2018-08-05T18:21:05.875892: step 2765, loss 0.562347.
Train: 2018-08-05T18:21:06.063324: step 2766, loss 0.510031.
Train: 2018-08-05T18:21:06.250776: step 2767, loss 0.55361.
Train: 2018-08-05T18:21:06.438258: step 2768, loss 0.615589.
Train: 2018-08-05T18:21:06.641339: step 2769, loss 0.526936.
Train: 2018-08-05T18:21:06.828767: step 2770, loss 0.56251.
Test: 2018-08-05T18:21:07.891048: step 2770, loss 0.54718.
Train: 2018-08-05T18:21:08.078472: step 2771, loss 0.508834.
Train: 2018-08-05T18:21:08.265955: step 2772, loss 0.616588.
Train: 2018-08-05T18:21:08.453386: step 2773, loss 0.517569.
Train: 2018-08-05T18:21:08.640842: step 2774, loss 0.580709.
Train: 2018-08-05T18:21:08.828322: step 2775, loss 0.589771.
Train: 2018-08-05T18:21:09.015777: step 2776, loss 0.517506.
Train: 2018-08-05T18:21:09.203210: step 2777, loss 0.517494.
Train: 2018-08-05T18:21:09.390665: step 2778, loss 0.589803.
Train: 2018-08-05T18:21:09.578122: step 2779, loss 0.499354.
Train: 2018-08-05T18:21:09.781203: step 2780, loss 0.481107.
Test: 2018-08-05T18:21:10.827830: step 2780, loss 0.547122.
Train: 2018-08-05T18:21:11.015286: step 2781, loss 0.553651.
Train: 2018-08-05T18:21:11.202774: step 2782, loss 0.572003.
Train: 2018-08-05T18:21:11.390196: step 2783, loss 0.553702.
Train: 2018-08-05T18:21:11.562033: step 2784, loss 0.572133.
Train: 2018-08-05T18:21:11.749515: step 2785, loss 0.544511.
Train: 2018-08-05T18:21:11.952566: step 2786, loss 0.572155.
Train: 2018-08-05T18:21:12.124426: step 2787, loss 0.56292.
Train: 2018-08-05T18:21:12.311880: step 2788, loss 0.617997.
Train: 2018-08-05T18:21:12.483691: step 2789, loss 0.489734.
Train: 2018-08-05T18:21:12.671177: step 2790, loss 0.517196.
Test: 2018-08-05T18:21:13.733426: step 2790, loss 0.547333.
Train: 2018-08-05T18:21:13.920856: step 2791, loss 0.544537.
Train: 2018-08-05T18:21:14.092688: step 2792, loss 0.544537.
Train: 2018-08-05T18:21:14.280146: step 2793, loss 0.526318.
Train: 2018-08-05T18:21:14.451979: step 2794, loss 0.535413.
Train: 2018-08-05T18:21:14.639462: step 2795, loss 0.471448.
Train: 2018-08-05T18:21:14.811273: step 2796, loss 0.489399.
Train: 2018-08-05T18:21:14.998727: step 2797, loss 0.544509.
Train: 2018-08-05T18:21:15.186213: step 2798, loss 0.581818.
Train: 2018-08-05T18:21:15.358020: step 2799, loss 0.563257.
Train: 2018-08-05T18:21:15.529879: step 2800, loss 0.544537.
Test: 2018-08-05T18:21:16.592106: step 2800, loss 0.545484.
Train: 2018-08-05T18:21:17.560652: step 2801, loss 0.610395.
Train: 2018-08-05T18:21:17.748083: step 2802, loss 0.497594.
Train: 2018-08-05T18:21:17.935540: step 2803, loss 0.61029.
Train: 2018-08-05T18:21:18.107376: step 2804, loss 0.591335.
Train: 2018-08-05T18:21:18.294829: step 2805, loss 0.535206.
Train: 2018-08-05T18:21:18.482300: step 2806, loss 0.646404.
Train: 2018-08-05T18:21:18.654151: step 2807, loss 0.599549.
Train: 2018-08-05T18:21:18.825980: step 2808, loss 0.59894.
Train: 2018-08-05T18:21:18.997790: step 2809, loss 0.553589.
Train: 2018-08-05T18:21:19.169628: step 2810, loss 0.500609.
Test: 2018-08-05T18:21:20.231877: step 2810, loss 0.547754.
Train: 2018-08-05T18:21:20.403710: step 2811, loss 0.536104.
Train: 2018-08-05T18:21:20.575545: step 2812, loss 0.61459.
Train: 2018-08-05T18:21:20.747381: step 2813, loss 0.545081.
Train: 2018-08-05T18:21:20.919215: step 2814, loss 0.545201.
Train: 2018-08-05T18:21:21.106671: step 2815, loss 0.613463.
Train: 2018-08-05T18:21:21.278505: step 2816, loss 0.553908.
Train: 2018-08-05T18:21:21.450341: step 2817, loss 0.604379.
Train: 2018-08-05T18:21:21.622200: step 2818, loss 0.470802.
Train: 2018-08-05T18:21:21.794015: step 2819, loss 0.595712.
Train: 2018-08-05T18:21:21.965845: step 2820, loss 0.579049.
Test: 2018-08-05T18:21:23.028095: step 2820, loss 0.549071.
Train: 2018-08-05T18:21:23.199933: step 2821, loss 0.554228.
Train: 2018-08-05T18:21:23.371770: step 2822, loss 0.537771.
Train: 2018-08-05T18:21:23.543605: step 2823, loss 0.603744.
Train: 2018-08-05T18:21:23.731056: step 2824, loss 0.578988.
Train: 2018-08-05T18:21:23.887295: step 2825, loss 0.488645.
Train: 2018-08-05T18:21:24.059106: step 2826, loss 0.611933.
Train: 2018-08-05T18:21:24.230966: step 2827, loss 0.546049.
Train: 2018-08-05T18:21:24.402800: step 2828, loss 0.529513.
Train: 2018-08-05T18:21:24.574609: step 2829, loss 0.628719.
Train: 2018-08-05T18:21:24.746477: step 2830, loss 0.52936.
Test: 2018-08-05T18:21:25.793075: step 2830, loss 0.547838.
Train: 2018-08-05T18:21:25.964913: step 2831, loss 0.570758.
Train: 2018-08-05T18:21:26.136769: step 2832, loss 0.520877.
Train: 2018-08-05T18:21:26.308608: step 2833, loss 0.520658.
Train: 2018-08-05T18:21:26.480437: step 2834, loss 0.5792.
Train: 2018-08-05T18:21:26.652274: step 2835, loss 0.56236.
Train: 2018-08-05T18:21:26.824108: step 2836, loss 0.587835.
Train: 2018-08-05T18:21:26.995917: step 2837, loss 0.596429.
Train: 2018-08-05T18:21:27.152131: step 2838, loss 0.528225.
Train: 2018-08-05T18:21:27.323966: step 2839, loss 0.579439.
Train: 2018-08-05T18:21:27.495825: step 2840, loss 0.588026.
Test: 2018-08-05T18:21:28.558053: step 2840, loss 0.547447.
Train: 2018-08-05T18:21:28.714290: step 2841, loss 0.58802.
Train: 2018-08-05T18:21:28.979828: step 2842, loss 0.545244.
Train: 2018-08-05T18:21:29.151681: step 2843, loss 0.51109.
Train: 2018-08-05T18:21:29.323498: step 2844, loss 0.545211.
Train: 2018-08-05T18:21:29.495365: step 2845, loss 0.62246.
Train: 2018-08-05T18:21:29.667194: step 2846, loss 0.570918.
Train: 2018-08-05T18:21:29.839036: step 2847, loss 0.519471.
Train: 2018-08-05T18:21:30.010840: step 2848, loss 0.545166.
Train: 2018-08-05T18:21:30.182673: step 2849, loss 0.562335.
Train: 2018-08-05T18:21:30.354531: step 2850, loss 0.60543.
Test: 2018-08-05T18:21:31.416782: step 2850, loss 0.546965.
Train: 2018-08-05T18:21:31.572970: step 2851, loss 0.596778.
Train: 2018-08-05T18:21:31.760451: step 2852, loss 0.53658.
Train: 2018-08-05T18:21:31.932263: step 2853, loss 0.528038.
Train: 2018-08-05T18:21:32.104124: step 2854, loss 0.622404.
Train: 2018-08-05T18:21:32.275931: step 2855, loss 0.57945.
Train: 2018-08-05T18:21:32.447766: step 2856, loss 0.553813.
Train: 2018-08-05T18:21:32.635252: step 2857, loss 0.562345.
Train: 2018-08-05T18:21:32.822713: step 2858, loss 0.502963.
Train: 2018-08-05T18:21:32.994544: step 2859, loss 0.596331.
Train: 2018-08-05T18:21:33.166348: step 2860, loss 0.47744.
Test: 2018-08-05T18:21:34.213001: step 2860, loss 0.548488.
Train: 2018-08-05T18:21:34.384812: step 2861, loss 0.613514.
Train: 2018-08-05T18:21:34.556681: step 2862, loss 0.630644.
Train: 2018-08-05T18:21:34.728511: step 2863, loss 0.545318.
Train: 2018-08-05T18:21:34.900316: step 2864, loss 0.570844.
Train: 2018-08-05T18:21:35.072183: step 2865, loss 0.562351.
Train: 2018-08-05T18:21:35.228397: step 2866, loss 0.562355.
Train: 2018-08-05T18:21:35.415847: step 2867, loss 0.613094.
Train: 2018-08-05T18:21:35.587656: step 2868, loss 0.545533.
Train: 2018-08-05T18:21:35.712626: step 2869, loss 0.634064.
Train: 2018-08-05T18:21:35.900114: step 2870, loss 0.51234.
Test: 2018-08-05T18:21:36.946712: step 2870, loss 0.549872.
Train: 2018-08-05T18:21:37.165416: step 2871, loss 0.562436.
Train: 2018-08-05T18:21:37.352892: step 2872, loss 0.471023.
Train: 2018-08-05T18:21:37.524703: step 2873, loss 0.562421.
Train: 2018-08-05T18:21:37.696538: step 2874, loss 0.562398.
Train: 2018-08-05T18:21:37.868373: step 2875, loss 0.528729.
Train: 2018-08-05T18:21:38.040238: step 2876, loss 0.604661.
Train: 2018-08-05T18:21:38.212071: step 2877, loss 0.545382.
Train: 2018-08-05T18:21:38.383907: step 2878, loss 0.519772.
Train: 2018-08-05T18:21:38.555727: step 2879, loss 0.596587.
Train: 2018-08-05T18:21:38.727545: step 2880, loss 0.596693.
Test: 2018-08-05T18:21:39.789796: step 2880, loss 0.547782.
Train: 2018-08-05T18:21:39.946010: step 2881, loss 0.570931.
Train: 2018-08-05T18:21:40.117845: step 2882, loss 0.510758.
Train: 2018-08-05T18:21:40.289706: step 2883, loss 0.579581.
Train: 2018-08-05T18:21:40.461513: step 2884, loss 0.579609.
Train: 2018-08-05T18:21:40.633348: step 2885, loss 0.52779.
Train: 2018-08-05T18:21:40.820804: step 2886, loss 0.527722.
Train: 2018-08-05T18:21:40.992669: step 2887, loss 0.69266.
Train: 2018-08-05T18:21:41.164477: step 2888, loss 0.596938.
Train: 2018-08-05T18:21:41.336335: step 2889, loss 0.605316.
Train: 2018-08-05T18:21:41.508144: step 2890, loss 0.58792.
Test: 2018-08-05T18:21:42.554774: step 2890, loss 0.548441.
Train: 2018-08-05T18:21:42.726608: step 2891, loss 0.579265.
Train: 2018-08-05T18:21:42.898468: step 2892, loss 0.621042.
Train: 2018-08-05T18:21:43.070279: step 2893, loss 0.595608.
Train: 2018-08-05T18:21:43.242116: step 2894, loss 0.529831.
Train: 2018-08-05T18:21:43.398352: step 2895, loss 0.554545.
Train: 2018-08-05T18:21:43.585782: step 2896, loss 0.490104.
Train: 2018-08-05T18:21:43.757618: step 2897, loss 0.54661.
Train: 2018-08-05T18:21:43.929452: step 2898, loss 0.570808.
Train: 2018-08-05T18:21:44.101288: step 2899, loss 0.473698.
Train: 2018-08-05T18:21:44.273127: step 2900, loss 0.554469.
Test: 2018-08-05T18:21:45.335372: step 2900, loss 0.548819.
Train: 2018-08-05T18:21:46.319542: step 2901, loss 0.603627.
Train: 2018-08-05T18:21:46.491383: step 2902, loss 0.529464.
Train: 2018-08-05T18:21:46.678808: step 2903, loss 0.52918.
Train: 2018-08-05T18:21:46.850642: step 2904, loss 0.579169.
Train: 2018-08-05T18:21:47.022477: step 2905, loss 0.562364.
Train: 2018-08-05T18:21:47.194314: step 2906, loss 0.553846.
Train: 2018-08-05T18:21:47.366152: step 2907, loss 0.553783.
Train: 2018-08-05T18:21:47.538008: step 2908, loss 0.527912.
Train: 2018-08-05T18:21:47.709817: step 2909, loss 0.562346.
Train: 2018-08-05T18:21:47.881651: step 2910, loss 0.53619.
Test: 2018-08-05T18:21:48.939824: step 2910, loss 0.547926.
Train: 2018-08-05T18:21:49.111635: step 2911, loss 0.571185.
Train: 2018-08-05T18:21:49.267850: step 2912, loss 0.562431.
Train: 2018-08-05T18:21:49.439686: step 2913, loss 0.571336.
Train: 2018-08-05T18:21:49.611518: step 2914, loss 0.544689.
Train: 2018-08-05T18:21:49.783352: step 2915, loss 0.633929.
Train: 2018-08-05T18:21:49.955213: step 2916, loss 0.615963.
Train: 2018-08-05T18:21:50.142643: step 2917, loss 0.473811.
Train: 2018-08-05T18:21:50.298856: step 2918, loss 0.553592.
Train: 2018-08-05T18:21:50.470692: step 2919, loss 0.527007.
Train: 2018-08-05T18:21:50.658147: step 2920, loss 0.535843.
Test: 2018-08-05T18:21:51.704776: step 2920, loss 0.54722.
Train: 2018-08-05T18:21:51.876611: step 2921, loss 0.535801.
Train: 2018-08-05T18:21:52.048477: step 2922, loss 0.544668.
Train: 2018-08-05T18:21:52.220312: step 2923, loss 0.535694.
Train: 2018-08-05T18:21:52.392141: step 2924, loss 0.571557.
Train: 2018-08-05T18:21:52.563984: step 2925, loss 0.5446.
Train: 2018-08-05T18:21:52.735813: step 2926, loss 0.571645.
Train: 2018-08-05T18:21:52.907622: step 2927, loss 0.553608.
Train: 2018-08-05T18:21:53.079457: step 2928, loss 0.508456.
Train: 2018-08-05T18:21:53.235671: step 2929, loss 0.553619.
Train: 2018-08-05T18:21:53.407517: step 2930, loss 0.58085.
Test: 2018-08-05T18:21:54.469754: step 2930, loss 0.547338.
Train: 2018-08-05T18:21:54.641619: step 2931, loss 0.571778.
Train: 2018-08-05T18:21:54.829080: step 2932, loss 0.626133.
Train: 2018-08-05T18:21:55.000879: step 2933, loss 0.544589.
Train: 2018-08-05T18:21:55.172717: step 2934, loss 0.562567.
Train: 2018-08-05T18:21:55.344575: step 2935, loss 0.669678.
Train: 2018-08-05T18:21:55.516386: step 2936, loss 0.5271.
Train: 2018-08-05T18:21:55.688218: step 2937, loss 0.614935.
Train: 2018-08-05T18:21:55.860055: step 2938, loss 0.536354.
Train: 2018-08-05T18:21:56.016291: step 2939, loss 0.562335.
Train: 2018-08-05T18:21:56.188104: step 2940, loss 0.528248.
Test: 2018-08-05T18:21:57.250353: step 2940, loss 0.547212.
Train: 2018-08-05T18:21:57.422212: step 2941, loss 0.502988.
Train: 2018-08-05T18:21:57.594025: step 2942, loss 0.638607.
Train: 2018-08-05T18:21:57.781505: step 2943, loss 0.511801.
Train: 2018-08-05T18:21:57.953313: step 2944, loss 0.587625.
Train: 2018-08-05T18:21:58.125162: step 2945, loss 0.495248.
Train: 2018-08-05T18:21:58.297009: step 2946, loss 0.604422.
Train: 2018-08-05T18:21:58.468850: step 2947, loss 0.537178.
Train: 2018-08-05T18:21:58.640655: step 2948, loss 0.537145.
Train: 2018-08-05T18:21:58.812487: step 2949, loss 0.562369.
Train: 2018-08-05T18:21:58.984323: step 2950, loss 0.663837.
Test: 2018-08-05T18:22:00.046573: step 2950, loss 0.550045.
Train: 2018-08-05T18:22:00.218439: step 2951, loss 0.537093.
Train: 2018-08-05T18:22:00.390243: step 2952, loss 0.570793.
Train: 2018-08-05T18:22:00.562104: step 2953, loss 0.562387.
Train: 2018-08-05T18:22:00.733940: step 2954, loss 0.58756.
Train: 2018-08-05T18:22:00.890128: step 2955, loss 0.646091.
Train: 2018-08-05T18:22:01.061961: step 2956, loss 0.579071.
Train: 2018-08-05T18:22:01.233796: step 2957, loss 0.554249.
Train: 2018-08-05T18:22:01.421284: step 2958, loss 0.58718.
Train: 2018-08-05T18:22:01.593118: step 2959, loss 0.578934.
Train: 2018-08-05T18:22:01.764921: step 2960, loss 0.489619.
Test: 2018-08-05T18:22:02.827206: step 2960, loss 0.548552.
Train: 2018-08-05T18:22:02.999007: step 2961, loss 0.627614.
Train: 2018-08-05T18:22:03.170871: step 2962, loss 0.522252.
Train: 2018-08-05T18:22:03.327091: step 2963, loss 0.603176.
Train: 2018-08-05T18:22:03.498889: step 2964, loss 0.5304.
Train: 2018-08-05T18:22:03.670729: step 2965, loss 0.554617.
Train: 2018-08-05T18:22:03.842559: step 2966, loss 0.56268.
Train: 2018-08-05T18:22:04.014421: step 2967, loss 0.562647.
Train: 2018-08-05T18:22:04.186231: step 2968, loss 0.570772.
Train: 2018-08-05T18:22:04.358064: step 2969, loss 0.603503.
Train: 2018-08-05T18:22:04.529925: step 2970, loss 0.480673.
Test: 2018-08-05T18:22:05.576529: step 2970, loss 0.548184.
Train: 2018-08-05T18:22:05.748363: step 2971, loss 0.570757.
Train: 2018-08-05T18:22:05.920230: step 2972, loss 0.529325.
Train: 2018-08-05T18:22:06.092033: step 2973, loss 0.537378.
Train: 2018-08-05T18:22:06.263895: step 2974, loss 0.562377.
Train: 2018-08-05T18:22:06.435726: step 2975, loss 0.519935.
Train: 2018-08-05T18:22:06.607569: step 2976, loss 0.519515.
Train: 2018-08-05T18:22:06.779401: step 2977, loss 0.579661.
Train: 2018-08-05T18:22:06.935587: step 2978, loss 0.492498.
Train: 2018-08-05T18:22:07.107420: step 2979, loss 0.59777.
Train: 2018-08-05T18:22:07.279255: step 2980, loss 0.5803.
Test: 2018-08-05T18:22:08.341509: step 2980, loss 0.549446.
Train: 2018-08-05T18:22:08.513341: step 2981, loss 0.562543.
Train: 2018-08-05T18:22:08.669554: step 2982, loss 0.598551.
Train: 2018-08-05T18:22:08.841389: step 2983, loss 0.526597.
Train: 2018-08-05T18:22:09.028847: step 2984, loss 0.517518.
Train: 2018-08-05T18:22:09.185063: step 2985, loss 0.589848.
Train: 2018-08-05T18:22:09.372541: step 2986, loss 0.490149.
Train: 2018-08-05T18:22:09.544350: step 2987, loss 0.471683.
Train: 2018-08-05T18:22:09.716210: step 2988, loss 0.627136.
Train: 2018-08-05T18:22:09.872397: step 2989, loss 0.608945.
Train: 2018-08-05T18:22:10.044237: step 2990, loss 0.627266.
Test: 2018-08-05T18:22:11.090862: step 2990, loss 0.548804.
Train: 2018-08-05T18:22:11.278318: step 2991, loss 0.562813.
Train: 2018-08-05T18:22:11.450157: step 2992, loss 0.508182.
Train: 2018-08-05T18:22:11.621990: step 2993, loss 0.662372.
Train: 2018-08-05T18:22:11.793823: step 2994, loss 0.607477.
Train: 2018-08-05T18:22:11.965657: step 2995, loss 0.509199.
Train: 2018-08-05T18:22:12.137491: step 2996, loss 0.527178.
Train: 2018-08-05T18:22:12.293708: step 2997, loss 0.562383.
Train: 2018-08-05T18:22:12.481193: step 2998, loss 0.544933.
Train: 2018-08-05T18:22:12.652999: step 2999, loss 0.536311.
Train: 2018-08-05T18:22:12.824856: step 3000, loss 0.58832.
Test: 2018-08-05T18:22:13.871460: step 3000, loss 0.548329.
Train: 2018-08-05T18:22:14.808777: step 3001, loss 0.596842.
Train: 2018-08-05T18:22:14.996229: step 3002, loss 0.588069.
Train: 2018-08-05T18:22:15.168064: step 3003, loss 0.511211.
Train: 2018-08-05T18:22:15.339897: step 3004, loss 0.553849.
Train: 2018-08-05T18:22:15.511702: step 3005, loss 0.613228.
Train: 2018-08-05T18:22:15.683570: step 3006, loss 0.52861.
Train: 2018-08-05T18:22:15.839752: step 3007, loss 0.604481.
Train: 2018-08-05T18:22:16.011609: step 3008, loss 0.545624.
Train: 2018-08-05T18:22:16.183419: step 3009, loss 0.5875.
Train: 2018-08-05T18:22:16.355282: step 3010, loss 0.595767.
Test: 2018-08-05T18:22:17.417530: step 3010, loss 0.548805.
Train: 2018-08-05T18:22:17.589353: step 3011, loss 0.512705.
Train: 2018-08-05T18:22:17.761179: step 3012, loss 0.579042.
Train: 2018-08-05T18:22:17.933012: step 3013, loss 0.587301.
Train: 2018-08-05T18:22:18.104844: step 3014, loss 0.620266.
Train: 2018-08-05T18:22:18.276706: step 3015, loss 0.538043.
Train: 2018-08-05T18:22:18.432927: step 3016, loss 0.611696.
Train: 2018-08-05T18:22:18.604729: step 3017, loss 0.538191.
Train: 2018-08-05T18:22:18.792210: step 3018, loss 0.562656.
Train: 2018-08-05T18:22:18.979671: step 3019, loss 0.554557.
Train: 2018-08-05T18:22:19.104612: step 3020, loss 0.562674.
Test: 2018-08-05T18:22:20.166886: step 3020, loss 0.550415.
Train: 2018-08-05T18:22:20.323110: step 3021, loss 0.513947.
Train: 2018-08-05T18:22:20.510532: step 3022, loss 0.505541.
Train: 2018-08-05T18:22:20.682365: step 3023, loss 0.496792.
Train: 2018-08-05T18:22:20.854201: step 3024, loss 0.537504.
Train: 2018-08-05T18:22:21.026039: step 3025, loss 0.562378.
Train: 2018-08-05T18:22:21.197896: step 3026, loss 0.511273.
Train: 2018-08-05T18:22:21.369709: step 3027, loss 0.492448.
Train: 2018-08-05T18:22:21.541570: step 3028, loss 0.563488.
Train: 2018-08-05T18:22:21.729021: step 3029, loss 0.50645.
Train: 2018-08-05T18:22:21.900831: step 3030, loss 0.566041.
Test: 2018-08-05T18:22:22.963082: step 3030, loss 0.546512.
Train: 2018-08-05T18:22:23.134917: step 3031, loss 0.604305.
Train: 2018-08-05T18:22:23.291155: step 3032, loss 0.609798.
Train: 2018-08-05T18:22:23.462964: step 3033, loss 0.635332.
Train: 2018-08-05T18:22:23.634825: step 3034, loss 0.544581.
Train: 2018-08-05T18:22:23.806660: step 3035, loss 0.553595.
Train: 2018-08-05T18:22:23.978472: step 3036, loss 0.580376.
Train: 2018-08-05T18:22:24.134707: step 3037, loss 0.56238.
Train: 2018-08-05T18:22:24.306520: step 3038, loss 0.596042.
Train: 2018-08-05T18:22:24.478384: step 3039, loss 0.541742.
Train: 2018-08-05T18:22:24.650190: step 3040, loss 0.530723.
Test: 2018-08-05T18:22:25.696817: step 3040, loss 0.547929.
Train: 2018-08-05T18:22:25.868652: step 3041, loss 0.614107.
Train: 2018-08-05T18:22:26.040486: step 3042, loss 0.588603.
Train: 2018-08-05T18:22:26.212321: step 3043, loss 0.667056.
Train: 2018-08-05T18:22:26.384185: step 3044, loss 0.545089.
Train: 2018-08-05T18:22:26.556015: step 3045, loss 0.553786.
Train: 2018-08-05T18:22:26.727856: step 3046, loss 0.604794.
Train: 2018-08-05T18:22:26.899664: step 3047, loss 0.48666.
Train: 2018-08-05T18:22:27.071519: step 3048, loss 0.545626.
Train: 2018-08-05T18:22:27.243355: step 3049, loss 0.604254.
Train: 2018-08-05T18:22:27.415190: step 3050, loss 0.562428.
Test: 2018-08-05T18:22:28.461825: step 3050, loss 0.54876.
Train: 2018-08-05T18:22:28.633661: step 3051, loss 0.512583.
Train: 2018-08-05T18:22:28.805496: step 3052, loss 0.562446.
Train: 2018-08-05T18:22:28.977299: step 3053, loss 0.495864.
Train: 2018-08-05T18:22:29.149134: step 3054, loss 0.54568.
Train: 2018-08-05T18:22:29.305371: step 3055, loss 0.553966.
Train: 2018-08-05T18:22:29.477184: step 3056, loss 0.604673.
Train: 2018-08-05T18:22:29.649046: step 3057, loss 0.494444.
Train: 2018-08-05T18:22:29.820854: step 3058, loss 0.562337.
Train: 2018-08-05T18:22:29.977065: step 3059, loss 0.639709.
Train: 2018-08-05T18:22:30.148900: step 3060, loss 0.562336.
Test: 2018-08-05T18:22:31.211151: step 3060, loss 0.548552.
Train: 2018-08-05T18:22:31.383012: step 3061, loss 0.588165.
Train: 2018-08-05T18:22:31.554820: step 3062, loss 0.49353.
Train: 2018-08-05T18:22:31.711033: step 3063, loss 0.553711.
Train: 2018-08-05T18:22:31.898520: step 3064, loss 0.622911.
Train: 2018-08-05T18:22:32.070360: step 3065, loss 0.545052.
Train: 2018-08-05T18:22:32.242160: step 3066, loss 0.596919.
Train: 2018-08-05T18:22:32.413993: step 3067, loss 0.614093.
Train: 2018-08-05T18:22:32.585828: step 3068, loss 0.528007.
Train: 2018-08-05T18:22:32.757665: step 3069, loss 0.493862.
Train: 2018-08-05T18:22:32.945119: step 3070, loss 0.510898.
Test: 2018-08-05T18:22:33.991750: step 3070, loss 0.547361.
Train: 2018-08-05T18:22:34.163583: step 3071, loss 0.579557.
Train: 2018-08-05T18:22:34.335419: step 3072, loss 0.545069.
Train: 2018-08-05T18:22:34.507255: step 3073, loss 0.631661.
Train: 2018-08-05T18:22:34.679088: step 3074, loss 0.562343.
Train: 2018-08-05T18:22:34.850925: step 3075, loss 0.683358.
Train: 2018-08-05T18:22:35.022759: step 3076, loss 0.588054.
Train: 2018-08-05T18:22:35.194626: step 3077, loss 0.536858.
Train: 2018-08-05T18:22:35.366454: step 3078, loss 0.537045.
Train: 2018-08-05T18:22:35.538293: step 3079, loss 0.570788.
Train: 2018-08-05T18:22:35.710098: step 3080, loss 0.587514.
Test: 2018-08-05T18:22:36.772348: step 3080, loss 0.549294.
Train: 2018-08-05T18:22:36.944183: step 3081, loss 0.554107.
Train: 2018-08-05T18:22:37.116049: step 3082, loss 0.554163.
Train: 2018-08-05T18:22:37.287853: step 3083, loss 0.512815.
Train: 2018-08-05T18:22:37.475310: step 3084, loss 0.537607.
Train: 2018-08-05T18:22:37.647171: step 3085, loss 0.520885.
Train: 2018-08-05T18:22:37.818980: step 3086, loss 0.512259.
Train: 2018-08-05T18:22:37.975194: step 3087, loss 0.5708.
Train: 2018-08-05T18:22:38.162648: step 3088, loss 0.528396.
Train: 2018-08-05T18:22:38.334484: step 3089, loss 0.545215.
Train: 2018-08-05T18:22:38.506345: step 3090, loss 0.510537.
Test: 2018-08-05T18:22:39.568568: step 3090, loss 0.54659.
Train: 2018-08-05T18:22:39.740404: step 3091, loss 0.475117.
Train: 2018-08-05T18:22:39.912238: step 3092, loss 0.58899.
Train: 2018-08-05T18:22:40.084073: step 3093, loss 0.55359.
Train: 2018-08-05T18:22:40.255931: step 3094, loss 0.562637.
Train: 2018-08-05T18:22:40.427743: step 3095, loss 0.644624.
Train: 2018-08-05T18:22:40.583980: step 3096, loss 0.517198.
Train: 2018-08-05T18:22:40.771436: step 3097, loss 0.617649.
Train: 2018-08-05T18:22:40.943247: step 3098, loss 0.608436.
Train: 2018-08-05T18:22:41.115080: step 3099, loss 0.508191.
Train: 2018-08-05T18:22:41.271294: step 3100, loss 0.508266.
Test: 2018-08-05T18:22:42.333545: step 3100, loss 0.547547.
Train: 2018-08-05T18:22:43.286448: step 3101, loss 0.508244.
Train: 2018-08-05T18:22:43.458313: step 3102, loss 0.544541.
Train: 2018-08-05T18:22:43.630118: step 3103, loss 0.58103.
Train: 2018-08-05T18:22:43.801986: step 3104, loss 0.526273.
Train: 2018-08-05T18:22:43.973811: step 3105, loss 0.599377.
Train: 2018-08-05T18:22:44.161273: step 3106, loss 0.654061.
Train: 2018-08-05T18:22:44.333082: step 3107, loss 0.580804.
Train: 2018-08-05T18:22:44.504943: step 3108, loss 0.571566.
Train: 2018-08-05T18:22:44.676748: step 3109, loss 0.509057.
Train: 2018-08-05T18:22:44.848582: step 3110, loss 0.518162.
Test: 2018-08-05T18:22:45.895212: step 3110, loss 0.546257.
Train: 2018-08-05T18:22:46.067046: step 3111, loss 0.588924.
Train: 2018-08-05T18:22:46.254529: step 3112, loss 0.588775.
Train: 2018-08-05T18:22:46.426339: step 3113, loss 0.509942.
Train: 2018-08-05T18:22:46.582552: step 3114, loss 0.623363.
Train: 2018-08-05T18:22:46.770007: step 3115, loss 0.449761.
Train: 2018-08-05T18:22:46.941846: step 3116, loss 0.571012.
Train: 2018-08-05T18:22:47.098059: step 3117, loss 0.501685.
Train: 2018-08-05T18:22:47.269922: step 3118, loss 0.59713.
Train: 2018-08-05T18:22:47.441725: step 3119, loss 0.553656.
Train: 2018-08-05T18:22:47.613586: step 3120, loss 0.579774.
Test: 2018-08-05T18:22:48.675810: step 3120, loss 0.547421.
Train: 2018-08-05T18:22:48.847648: step 3121, loss 0.692909.
Train: 2018-08-05T18:22:49.019480: step 3122, loss 0.640033.
Train: 2018-08-05T18:22:49.191318: step 3123, loss 0.587928.
Train: 2018-08-05T18:22:49.363174: step 3124, loss 0.596072.
Train: 2018-08-05T18:22:49.550637: step 3125, loss 0.554131.
Train: 2018-08-05T18:22:49.722471: step 3126, loss 0.529637.
Train: 2018-08-05T18:22:49.878686: step 3127, loss 0.554441.
Train: 2018-08-05T18:22:50.050489: step 3128, loss 0.578907.
Train: 2018-08-05T18:22:50.206732: step 3129, loss 0.522357.
Train: 2018-08-05T18:22:50.378567: step 3130, loss 0.619192.
Test: 2018-08-05T18:22:51.440822: step 3130, loss 0.548666.
Train: 2018-08-05T18:22:51.612653: step 3131, loss 0.586898.
Train: 2018-08-05T18:22:51.768836: step 3132, loss 0.57087.
Train: 2018-08-05T18:22:51.940671: step 3133, loss 0.586821.
Train: 2018-08-05T18:22:52.112536: step 3134, loss 0.531272.
Train: 2018-08-05T18:22:52.268749: step 3135, loss 0.563006.
Train: 2018-08-05T18:22:52.440590: step 3136, loss 0.54714.
Train: 2018-08-05T18:22:52.612419: step 3137, loss 0.555014.
Train: 2018-08-05T18:22:52.784224: step 3138, loss 0.538978.
Train: 2018-08-05T18:22:52.956088: step 3139, loss 0.610951.
Train: 2018-08-05T18:22:53.112303: step 3140, loss 0.538658.
Test: 2018-08-05T18:22:54.174553: step 3140, loss 0.549785.
Train: 2018-08-05T18:22:54.346387: step 3141, loss 0.562725.
Train: 2018-08-05T18:22:54.518222: step 3142, loss 0.554541.
Train: 2018-08-05T18:22:54.674436: step 3143, loss 0.595276.
Train: 2018-08-05T18:22:54.846271: step 3144, loss 0.578961.
Train: 2018-08-05T18:22:55.018076: step 3145, loss 0.521427.
Train: 2018-08-05T18:22:55.174319: step 3146, loss 0.521151.
Train: 2018-08-05T18:22:55.346154: step 3147, loss 0.512433.
Train: 2018-08-05T18:22:55.517988: step 3148, loss 0.562376.
Train: 2018-08-05T18:22:55.689827: step 3149, loss 0.477342.
Train: 2018-08-05T18:22:55.846007: step 3150, loss 0.55372.
Test: 2018-08-05T18:22:56.908258: step 3150, loss 0.547996.
Train: 2018-08-05T18:22:57.080123: step 3151, loss 0.597264.
Train: 2018-08-05T18:22:57.236337: step 3152, loss 0.580014.
Train: 2018-08-05T18:22:57.408140: step 3153, loss 0.491567.
Train: 2018-08-05T18:22:57.564388: step 3154, loss 0.625146.
Train: 2018-08-05T18:22:57.736218: step 3155, loss 0.616479.
Train: 2018-08-05T18:22:57.908024: step 3156, loss 0.544611.
Train: 2018-08-05T18:22:58.079883: step 3157, loss 0.553597.
Train: 2018-08-05T18:22:58.251718: step 3158, loss 0.571582.
Train: 2018-08-05T18:22:58.423558: step 3159, loss 0.580548.
Train: 2018-08-05T18:22:58.595393: step 3160, loss 0.60736.
Test: 2018-08-05T18:22:59.642022: step 3160, loss 0.547411.
Train: 2018-08-05T18:22:59.845097: step 3161, loss 0.598152.
Train: 2018-08-05T18:23:00.048178: step 3162, loss 0.473976.
Train: 2018-08-05T18:23:00.251252: step 3163, loss 0.588907.
Train: 2018-08-05T18:23:00.454329: step 3164, loss 0.58878.
Train: 2018-08-05T18:23:00.657407: step 3165, loss 0.58861.
Train: 2018-08-05T18:23:00.860485: step 3166, loss 0.597097.
Train: 2018-08-05T18:23:01.063565: step 3167, loss 0.588181.
Train: 2018-08-05T18:23:01.266644: step 3168, loss 0.579411.
Train: 2018-08-05T18:23:01.469691: step 3169, loss 0.570817.
Train: 2018-08-05T18:23:01.672768: step 3170, loss 0.579164.
Test: 2018-08-05T18:23:02.719429: step 3170, loss 0.548375.
Train: 2018-08-05T18:23:02.891259: step 3171, loss 0.615645.
Train: 2018-08-05T18:23:03.094340: step 3172, loss 0.6201.
Train: 2018-08-05T18:23:03.250553: step 3173, loss 0.546428.
Train: 2018-08-05T18:23:03.422388: step 3174, loss 0.546712.
Train: 2018-08-05T18:23:03.594222: step 3175, loss 0.491043.
Train: 2018-08-05T18:23:03.766057: step 3176, loss 0.531008.
Train: 2018-08-05T18:23:03.937895: step 3177, loss 0.570871.
Train: 2018-08-05T18:23:04.109726: step 3178, loss 0.658948.
Train: 2018-08-05T18:23:04.281561: step 3179, loss 0.562896.
Train: 2018-08-05T18:23:04.453396: step 3180, loss 0.594791.
Test: 2018-08-05T18:23:05.500026: step 3180, loss 0.548977.
Train: 2018-08-05T18:23:05.671831: step 3181, loss 0.515318.
Train: 2018-08-05T18:23:05.843695: step 3182, loss 0.539094.
Train: 2018-08-05T18:23:06.015500: step 3183, loss 0.570879.
Train: 2018-08-05T18:23:06.202981: step 3184, loss 0.522794.
Train: 2018-08-05T18:23:06.374821: step 3185, loss 0.546629.
Train: 2018-08-05T18:23:06.546626: step 3186, loss 0.497664.
Train: 2018-08-05T18:23:06.718491: step 3187, loss 0.537886.
Train: 2018-08-05T18:23:06.890295: step 3188, loss 0.595721.
Train: 2018-08-05T18:23:07.062164: step 3189, loss 0.587581.
Train: 2018-08-05T18:23:07.249617: step 3190, loss 0.511615.
Test: 2018-08-05T18:23:08.296216: step 3190, loss 0.549258.
Train: 2018-08-05T18:23:08.468050: step 3191, loss 0.545262.
Train: 2018-08-05T18:23:08.639915: step 3192, loss 0.631292.
Train: 2018-08-05T18:23:08.827342: step 3193, loss 0.571001.
Train: 2018-08-05T18:23:08.999206: step 3194, loss 0.536294.
Train: 2018-08-05T18:23:09.171011: step 3195, loss 0.623421.
Train: 2018-08-05T18:23:09.342846: step 3196, loss 0.605976.
Train: 2018-08-05T18:23:09.514711: step 3197, loss 0.579751.
Train: 2018-08-05T18:23:09.702167: step 3198, loss 0.571011.
Train: 2018-08-05T18:23:09.873972: step 3199, loss 0.579598.
Train: 2018-08-05T18:23:10.045807: step 3200, loss 0.605277.
Test: 2018-08-05T18:23:11.108091: step 3200, loss 0.547899.
Train: 2018-08-05T18:23:12.060960: step 3201, loss 0.579397.
Train: 2018-08-05T18:23:12.232824: step 3202, loss 0.570823.
Train: 2018-08-05T18:23:12.404658: step 3203, loss 0.562382.
Train: 2018-08-05T18:23:12.592110: step 3204, loss 0.57077.
Train: 2018-08-05T18:23:12.779571: step 3205, loss 0.554144.
Train: 2018-08-05T18:23:12.967027: step 3206, loss 0.55421.
Train: 2018-08-05T18:23:13.138833: step 3207, loss 0.554256.
Train: 2018-08-05T18:23:13.326318: step 3208, loss 0.537809.
Train: 2018-08-05T18:23:13.498153: step 3209, loss 0.570757.
Train: 2018-08-05T18:23:13.685612: step 3210, loss 0.562512.
Test: 2018-08-05T18:23:14.732239: step 3210, loss 0.549101.
Train: 2018-08-05T18:23:14.919695: step 3211, loss 0.512987.
Train: 2018-08-05T18:23:15.091530: step 3212, loss 0.562451.
Train: 2018-08-05T18:23:15.278986: step 3213, loss 0.595937.
Train: 2018-08-05T18:23:15.450820: step 3214, loss 0.52054.
Train: 2018-08-05T18:23:15.638276: step 3215, loss 0.528457.
Train: 2018-08-05T18:23:15.810115: step 3216, loss 0.571134.
Train: 2018-08-05T18:23:15.997567: step 3217, loss 0.562533.
Train: 2018-08-05T18:23:16.185027: step 3218, loss 0.598851.
Train: 2018-08-05T18:23:16.356858: step 3219, loss 0.553644.
Train: 2018-08-05T18:23:16.559939: step 3220, loss 0.545184.
Test: 2018-08-05T18:23:17.606536: step 3220, loss 0.547306.
Train: 2018-08-05T18:23:17.794022: step 3221, loss 0.579401.
Train: 2018-08-05T18:23:17.981478: step 3222, loss 0.519745.
Train: 2018-08-05T18:23:18.168935: step 3223, loss 0.630687.
Train: 2018-08-05T18:23:18.356361: step 3224, loss 0.579401.
Train: 2018-08-05T18:23:18.543846: step 3225, loss 0.545321.
Train: 2018-08-05T18:23:18.731303: step 3226, loss 0.485826.
Train: 2018-08-05T18:23:18.918758: step 3227, loss 0.519672.
Train: 2018-08-05T18:23:19.106214: step 3228, loss 0.553752.
Train: 2018-08-05T18:23:19.278049: step 3229, loss 0.570971.
Train: 2018-08-05T18:23:19.481121: step 3230, loss 0.536335.
Test: 2018-08-05T18:23:20.527756: step 3230, loss 0.547604.
Train: 2018-08-05T18:23:20.715213: step 3231, loss 0.49263.
Train: 2018-08-05T18:23:20.902640: step 3232, loss 0.509654.
Train: 2018-08-05T18:23:21.090096: step 3233, loss 0.580236.
Train: 2018-08-05T18:23:21.277584: step 3234, loss 0.562541.
Train: 2018-08-05T18:23:21.465037: step 3235, loss 0.562608.
Train: 2018-08-05T18:23:21.652488: step 3236, loss 0.680331.
Train: 2018-08-05T18:23:21.839949: step 3237, loss 0.571656.
Train: 2018-08-05T18:23:22.027376: step 3238, loss 0.562586.
Train: 2018-08-05T18:23:22.214862: step 3239, loss 0.643104.
Train: 2018-08-05T18:23:22.417909: step 3240, loss 0.571333.
Test: 2018-08-05T18:23:23.464538: step 3240, loss 0.548934.
Train: 2018-08-05T18:23:23.652025: step 3241, loss 0.527238.
Train: 2018-08-05T18:23:23.839481: step 3242, loss 0.588567.
Train: 2018-08-05T18:23:24.042528: step 3243, loss 0.545016.
Train: 2018-08-05T18:23:24.245638: step 3244, loss 0.553725.
Train: 2018-08-05T18:23:24.433062: step 3245, loss 0.579468.
Train: 2018-08-05T18:23:24.620548: step 3246, loss 0.673062.
Train: 2018-08-05T18:23:24.808008: step 3247, loss 0.579212.
Train: 2018-08-05T18:23:25.011082: step 3248, loss 0.529153.
Train: 2018-08-05T18:23:25.198507: step 3249, loss 0.545989.
Train: 2018-08-05T18:23:25.385994: step 3250, loss 0.603593.
Test: 2018-08-05T18:23:26.448240: step 3250, loss 0.549392.
Train: 2018-08-05T18:23:26.651324: step 3251, loss 0.660418.
Train: 2018-08-05T18:23:26.838778: step 3252, loss 0.530562.
Train: 2018-08-05T18:23:27.041854: step 3253, loss 0.578861.
Train: 2018-08-05T18:23:27.229313: step 3254, loss 0.547141.
Train: 2018-08-05T18:23:27.416769: step 3255, loss 0.539401.
Train: 2018-08-05T18:23:27.619846: step 3256, loss 0.610382.
Train: 2018-08-05T18:23:27.822923: step 3257, loss 0.547466.
Train: 2018-08-05T18:23:28.026001: step 3258, loss 0.578875.
Train: 2018-08-05T18:23:28.229078: step 3259, loss 0.563203.
Train: 2018-08-05T18:23:28.432160: step 3260, loss 0.555357.
Test: 2018-08-05T18:23:29.478786: step 3260, loss 0.550405.
Train: 2018-08-05T18:23:29.681867: step 3261, loss 0.539597.
Train: 2018-08-05T18:23:29.931808: step 3262, loss 0.531529.
Train: 2018-08-05T18:23:30.119231: step 3263, loss 0.586802.
Train: 2018-08-05T18:23:30.322309: step 3264, loss 0.586851.
Train: 2018-08-05T18:23:30.525385: step 3265, loss 0.578869.
Train: 2018-08-05T18:23:30.728493: step 3266, loss 0.562767.
Train: 2018-08-05T18:23:30.931574: step 3267, loss 0.497999.
Train: 2018-08-05T18:23:31.134648: step 3268, loss 0.587089.
Train: 2018-08-05T18:23:31.353347: step 3269, loss 0.54612.
Train: 2018-08-05T18:23:31.556428: step 3270, loss 0.446625.
Test: 2018-08-05T18:23:32.618675: step 3270, loss 0.548372.
Train: 2018-08-05T18:23:32.821723: step 3271, loss 0.637933.
Train: 2018-08-05T18:23:33.040451: step 3272, loss 0.528499.
Train: 2018-08-05T18:23:33.243529: step 3273, loss 0.511059.
Train: 2018-08-05T18:23:33.446606: step 3274, loss 0.579634.
Train: 2018-08-05T18:23:33.649684: step 3275, loss 0.57982.
Train: 2018-08-05T18:23:33.868383: step 3276, loss 0.571188.
Train: 2018-08-05T18:23:34.087081: step 3277, loss 0.562433.
Train: 2018-08-05T18:23:34.305784: step 3278, loss 0.518091.
Train: 2018-08-05T18:23:34.508857: step 3279, loss 0.500021.
Train: 2018-08-05T18:23:34.727556: step 3280, loss 0.535596.
Test: 2018-08-05T18:23:35.774186: step 3280, loss 0.546711.
Train: 2018-08-05T18:23:35.977264: step 3281, loss 0.517322.
Train: 2018-08-05T18:23:36.195964: step 3282, loss 0.526201.
Train: 2018-08-05T18:23:36.399010: step 3283, loss 0.599974.
Train: 2018-08-05T18:23:36.617738: step 3284, loss 0.590973.
Train: 2018-08-05T18:23:36.836438: step 3285, loss 0.572442.
Train: 2018-08-05T18:23:37.055140: step 3286, loss 0.544509.
Train: 2018-08-05T18:23:37.273835: step 3287, loss 0.507841.
Train: 2018-08-05T18:23:37.476914: step 3288, loss 0.563182.
Train: 2018-08-05T18:23:37.695611: step 3289, loss 0.572535.
Train: 2018-08-05T18:23:37.914310: step 3290, loss 0.591157.
Test: 2018-08-05T18:23:38.976532: step 3290, loss 0.548861.
Train: 2018-08-05T18:23:39.226503: step 3291, loss 0.609564.
Train: 2018-08-05T18:23:39.445172: step 3292, loss 0.618334.
Train: 2018-08-05T18:23:39.663900: step 3293, loss 0.526264.
Train: 2018-08-05T18:23:39.882600: step 3294, loss 0.52645.
Train: 2018-08-05T18:23:40.085647: step 3295, loss 0.562599.
Train: 2018-08-05T18:23:40.304346: step 3296, loss 0.651956.
Train: 2018-08-05T18:23:40.523074: step 3297, loss 0.535915.
Train: 2018-08-05T18:23:40.741773: step 3298, loss 0.562382.
Train: 2018-08-05T18:23:40.960441: step 3299, loss 0.544985.
Train: 2018-08-05T18:23:41.179141: step 3300, loss 0.527854.
Test: 2018-08-05T18:23:42.225803: step 3300, loss 0.548003.
Train: 2018-08-05T18:23:43.163081: step 3301, loss 0.562335.
Train: 2018-08-05T18:23:43.381781: step 3302, loss 0.493966.
Train: 2018-08-05T18:23:43.600479: step 3303, loss 0.605082.
Train: 2018-08-05T18:23:43.803560: step 3304, loss 0.596458.
Train: 2018-08-05T18:23:44.022255: step 3305, loss 0.604822.
Train: 2018-08-05T18:23:44.240950: step 3306, loss 0.528592.
Train: 2018-08-05T18:23:44.444031: step 3307, loss 0.579209.
Train: 2018-08-05T18:23:44.662730: step 3308, loss 0.612694.
Train: 2018-08-05T18:23:44.865808: step 3309, loss 0.579095.
Train: 2018-08-05T18:23:45.084507: step 3310, loss 0.562478.
Test: 2018-08-05T18:23:46.146758: step 3310, loss 0.548202.
Train: 2018-08-05T18:23:46.365456: step 3311, loss 0.595461.
Train: 2018-08-05T18:23:46.568504: step 3312, loss 0.538036.
Train: 2018-08-05T18:23:46.787232: step 3313, loss 0.58708.
Train: 2018-08-05T18:23:47.005931: step 3314, loss 0.530194.
Train: 2018-08-05T18:23:47.224631: step 3315, loss 0.570792.
Train: 2018-08-05T18:23:47.443329: step 3316, loss 0.522172.
Train: 2018-08-05T18:23:47.662032: step 3317, loss 0.538289.
Train: 2018-08-05T18:23:47.865105: step 3318, loss 0.464687.
Train: 2018-08-05T18:23:48.083804: step 3319, loss 0.562509.
Train: 2018-08-05T18:23:48.302506: step 3320, loss 0.537443.
Test: 2018-08-05T18:23:49.364754: step 3320, loss 0.549281.
Train: 2018-08-05T18:23:49.583422: step 3321, loss 0.570796.
Train: 2018-08-05T18:23:49.755288: step 3322, loss 0.598611.
Train: 2018-08-05T18:23:49.973986: step 3323, loss 0.562336.
Train: 2018-08-05T18:23:50.192685: step 3324, loss 0.570937.
Train: 2018-08-05T18:23:50.411384: step 3325, loss 0.49322.
Train: 2018-08-05T18:23:50.630082: step 3326, loss 0.536234.
Train: 2018-08-05T18:23:50.833163: step 3327, loss 0.536051.
Train: 2018-08-05T18:23:51.051862: step 3328, loss 0.651023.
Train: 2018-08-05T18:23:51.301804: step 3329, loss 0.535837.
Train: 2018-08-05T18:23:51.520499: step 3330, loss 0.624811.
Test: 2018-08-05T18:23:52.567099: step 3330, loss 0.547837.
Train: 2018-08-05T18:23:52.785827: step 3331, loss 0.535809.
Train: 2018-08-05T18:23:53.004526: step 3332, loss 0.571363.
Train: 2018-08-05T18:23:53.223226: step 3333, loss 0.50034.
Train: 2018-08-05T18:23:53.441924: step 3334, loss 0.642502.
Train: 2018-08-05T18:23:53.660593: step 3335, loss 0.633338.
Train: 2018-08-05T18:23:53.879322: step 3336, loss 0.553609.
Train: 2018-08-05T18:23:54.098020: step 3337, loss 0.518703.
Train: 2018-08-05T18:23:54.316719: step 3338, loss 0.62325.
Train: 2018-08-05T18:23:54.535418: step 3339, loss 0.622789.
Train: 2018-08-05T18:23:54.754117: step 3340, loss 0.630714.
Test: 2018-08-05T18:23:55.816341: step 3340, loss 0.547508.
Train: 2018-08-05T18:23:56.019445: step 3341, loss 0.553936.
Train: 2018-08-05T18:23:56.238144: step 3342, loss 0.49573.
Train: 2018-08-05T18:23:56.456844: step 3343, loss 0.537607.
Train: 2018-08-05T18:23:56.675546: step 3344, loss 0.587274.
Train: 2018-08-05T18:23:56.894240: step 3345, loss 0.587206.
Train: 2018-08-05T18:23:57.112939: step 3346, loss 0.50529.
Train: 2018-08-05T18:23:57.331638: step 3347, loss 0.538036.
Train: 2018-08-05T18:23:57.550340: step 3348, loss 0.562565.
Train: 2018-08-05T18:23:57.769036: step 3349, loss 0.595406.
Train: 2018-08-05T18:23:57.987705: step 3350, loss 0.587198.
Test: 2018-08-05T18:23:59.034364: step 3350, loss 0.549014.
Train: 2018-08-05T18:23:59.253063: step 3351, loss 0.546115.
Train: 2018-08-05T18:23:59.471762: step 3352, loss 0.488528.
Train: 2018-08-05T18:23:59.690461: step 3353, loss 0.554212.
Train: 2018-08-05T18:23:59.909159: step 3354, loss 0.54579.
Train: 2018-08-05T18:24:00.127829: step 3355, loss 0.545633.
Train: 2018-08-05T18:24:00.346558: step 3356, loss 0.537028.
Train: 2018-08-05T18:24:00.565226: step 3357, loss 0.536792.
Train: 2018-08-05T18:24:00.783925: step 3358, loss 0.536552.
Train: 2018-08-05T18:24:00.987032: step 3359, loss 0.553671.
Train: 2018-08-05T18:24:01.205731: step 3360, loss 0.579889.
Test: 2018-08-05T18:24:02.267953: step 3360, loss 0.547089.
Train: 2018-08-05T18:24:02.533515: step 3361, loss 0.553603.
Train: 2018-08-05T18:24:02.799080: step 3362, loss 0.606786.
Train: 2018-08-05T18:24:03.064667: step 3363, loss 0.571363.
Train: 2018-08-05T18:24:03.283340: step 3364, loss 0.553588.
Train: 2018-08-05T18:24:03.502038: step 3365, loss 0.562493.
Train: 2018-08-05T18:24:03.720767: step 3366, loss 0.580311.
Train: 2018-08-05T18:24:03.939465: step 3367, loss 0.562484.
Train: 2018-08-05T18:24:04.142543: step 3368, loss 0.58911.
Train: 2018-08-05T18:24:04.361242: step 3369, loss 0.677467.
Train: 2018-08-05T18:24:04.579941: step 3370, loss 0.588652.
Test: 2018-08-05T18:24:05.642192: step 3370, loss 0.547683.
Train: 2018-08-05T18:24:05.860891: step 3371, loss 0.571003.
Train: 2018-08-05T18:24:06.063968: step 3372, loss 0.545204.
Train: 2018-08-05T18:24:06.282667: step 3373, loss 0.553858.
Train: 2018-08-05T18:24:06.501366: step 3374, loss 0.587654.
Train: 2018-08-05T18:24:06.720065: step 3375, loss 0.587491.
Train: 2018-08-05T18:24:06.938763: step 3376, loss 0.587335.
Train: 2018-08-05T18:24:07.141841: step 3377, loss 0.578975.
Train: 2018-08-05T18:24:07.360510: step 3378, loss 0.578923.
Train: 2018-08-05T18:24:07.579238: step 3379, loss 0.627375.
Train: 2018-08-05T18:24:07.797941: step 3380, loss 0.506902.
Test: 2018-08-05T18:24:08.860159: step 3380, loss 0.549116.
Train: 2018-08-05T18:24:09.063235: step 3381, loss 0.626585.
Train: 2018-08-05T18:24:09.281965: step 3382, loss 0.555185.
Train: 2018-08-05T18:24:09.500664: step 3383, loss 0.50823.
Train: 2018-08-05T18:24:09.719333: step 3384, loss 0.508252.
Train: 2018-08-05T18:24:09.938061: step 3385, loss 0.531577.
Train: 2018-08-05T18:24:10.156761: step 3386, loss 0.491553.
Train: 2018-08-05T18:24:10.375459: step 3387, loss 0.546747.
Train: 2018-08-05T18:24:10.578537: step 3388, loss 0.570783.
Train: 2018-08-05T18:24:10.797206: step 3389, loss 0.587203.
Train: 2018-08-05T18:24:11.000312: step 3390, loss 0.520978.
Test: 2018-08-05T18:24:12.062534: step 3390, loss 0.548577.
Train: 2018-08-05T18:24:12.281262: step 3391, loss 0.554005.
Train: 2018-08-05T18:24:12.499961: step 3392, loss 0.553874.
Train: 2018-08-05T18:24:12.703038: step 3393, loss 0.630844.
Train: 2018-08-05T18:24:12.921741: step 3394, loss 0.484881.
Train: 2018-08-05T18:24:13.140436: step 3395, loss 0.597082.
Train: 2018-08-05T18:24:13.359135: step 3396, loss 0.579835.
Train: 2018-08-05T18:24:13.562213: step 3397, loss 0.597452.
Train: 2018-08-05T18:24:13.780911: step 3398, loss 0.579943.
Train: 2018-08-05T18:24:13.999610: step 3399, loss 0.56239.
Train: 2018-08-05T18:24:14.218312: step 3400, loss 0.518558.
Test: 2018-08-05T18:24:15.264935: step 3400, loss 0.547328.
Train: 2018-08-05T18:24:16.249083: step 3401, loss 0.58873.
Train: 2018-08-05T18:24:16.467782: step 3402, loss 0.606263.
Train: 2018-08-05T18:24:16.670863: step 3403, loss 0.544885.
Train: 2018-08-05T18:24:16.889558: step 3404, loss 0.605987.
Train: 2018-08-05T18:24:17.108257: step 3405, loss 0.536306.
Train: 2018-08-05T18:24:17.311334: step 3406, loss 0.596959.
Train: 2018-08-05T18:24:17.530034: step 3407, loss 0.527894.
Train: 2018-08-05T18:24:17.748733: step 3408, loss 0.519396.
Train: 2018-08-05T18:24:17.951809: step 3409, loss 0.579512.
Train: 2018-08-05T18:24:18.170512: step 3410, loss 0.545175.
Test: 2018-08-05T18:24:19.232737: step 3410, loss 0.548597.
Train: 2018-08-05T18:24:19.435837: step 3411, loss 0.536593.
Train: 2018-08-05T18:24:19.654507: step 3412, loss 0.588116.
Train: 2018-08-05T18:24:19.873235: step 3413, loss 0.605292.
Train: 2018-08-05T18:24:20.076312: step 3414, loss 0.579469.
Train: 2018-08-05T18:24:20.310631: step 3415, loss 0.62209.
Train: 2018-08-05T18:24:20.513709: step 3416, loss 0.587785.
Train: 2018-08-05T18:24:20.732408: step 3417, loss 0.604447.
Train: 2018-08-05T18:24:20.951108: step 3418, loss 0.595774.
Train: 2018-08-05T18:24:21.154185: step 3419, loss 0.595514.
Train: 2018-08-05T18:24:21.372883: step 3420, loss 0.5871.
Test: 2018-08-05T18:24:22.419484: step 3420, loss 0.548494.
Train: 2018-08-05T18:24:22.638212: step 3421, loss 0.546573.
Train: 2018-08-05T18:24:22.856911: step 3422, loss 0.546804.
Train: 2018-08-05T18:24:23.059988: step 3423, loss 0.546965.
Train: 2018-08-05T18:24:23.278657: step 3424, loss 0.594762.
Train: 2018-08-05T18:24:23.497355: step 3425, loss 0.55509.
Train: 2018-08-05T18:24:23.716055: step 3426, loss 0.570951.
Train: 2018-08-05T18:24:23.934783: step 3427, loss 0.539358.
Train: 2018-08-05T18:24:24.153483: step 3428, loss 0.634253.
Train: 2018-08-05T18:24:24.372181: step 3429, loss 0.547277.
Train: 2018-08-05T18:24:24.575259: step 3430, loss 0.523582.
Test: 2018-08-05T18:24:25.637510: step 3430, loss 0.549221.
Train: 2018-08-05T18:24:25.840587: step 3431, loss 0.55508.
Train: 2018-08-05T18:24:26.059286: step 3432, loss 0.602753.
Train: 2018-08-05T18:24:26.262363: step 3433, loss 0.586847.
Train: 2018-08-05T18:24:26.481063: step 3434, loss 0.578863.
Train: 2018-08-05T18:24:26.699761: step 3435, loss 0.530795.
Train: 2018-08-05T18:24:26.918463: step 3436, loss 0.594965.
Train: 2018-08-05T18:24:27.137129: step 3437, loss 0.570816.
Train: 2018-08-05T18:24:27.355857: step 3438, loss 0.570804.
Train: 2018-08-05T18:24:27.558904: step 3439, loss 0.522159.
Train: 2018-08-05T18:24:27.777634: step 3440, loss 0.546324.
Test: 2018-08-05T18:24:28.839884: step 3440, loss 0.5498.
Train: 2018-08-05T18:24:29.058587: step 3441, loss 0.546149.
Train: 2018-08-05T18:24:29.261631: step 3442, loss 0.521164.
Train: 2018-08-05T18:24:29.480360: step 3443, loss 0.612495.
Train: 2018-08-05T18:24:29.699061: step 3444, loss 0.512019.
Train: 2018-08-05T18:24:29.902107: step 3445, loss 0.528486.
Train: 2018-08-05T18:24:30.120835: step 3446, loss 0.579439.
Train: 2018-08-05T18:24:30.323912: step 3447, loss 0.553718.
Train: 2018-08-05T18:24:30.542614: step 3448, loss 0.510256.
Train: 2018-08-05T18:24:30.761309: step 3449, loss 0.527326.
Train: 2018-08-05T18:24:30.964387: step 3450, loss 0.544738.
Test: 2018-08-05T18:24:32.026639: step 3450, loss 0.54801.
Train: 2018-08-05T18:24:32.229716: step 3451, loss 0.544649.
Train: 2018-08-05T18:24:32.448384: step 3452, loss 0.544584.
Train: 2018-08-05T18:24:32.667117: step 3453, loss 0.526346.
Train: 2018-08-05T18:24:32.870160: step 3454, loss 0.590405.
Train: 2018-08-05T18:24:33.088890: step 3455, loss 0.498377.
Train: 2018-08-05T18:24:33.291971: step 3456, loss 0.618864.
Train: 2018-08-05T18:24:33.510667: step 3457, loss 0.553831.
Train: 2018-08-05T18:24:33.713744: step 3458, loss 0.553846.
Train: 2018-08-05T18:24:33.932442: step 3459, loss 0.535186.
Train: 2018-08-05T18:24:34.135489: step 3460, loss 0.572543.
Test: 2018-08-05T18:24:35.197741: step 3460, loss 0.546309.
Train: 2018-08-05T18:24:35.400848: step 3461, loss 0.535188.
Train: 2018-08-05T18:24:35.619517: step 3462, loss 0.49789.
Train: 2018-08-05T18:24:35.822624: step 3463, loss 0.591235.
Train: 2018-08-05T18:24:36.025702: step 3464, loss 0.479193.
Train: 2018-08-05T18:24:36.244401: step 3465, loss 0.563235.
Train: 2018-08-05T18:24:36.463069: step 3466, loss 0.535165.
Train: 2018-08-05T18:24:36.666181: step 3467, loss 0.516412.
Train: 2018-08-05T18:24:36.884876: step 3468, loss 0.591514.
Train: 2018-08-05T18:24:37.103575: step 3469, loss 0.516376.
Train: 2018-08-05T18:24:37.322274: step 3470, loss 0.516363.
Test: 2018-08-05T18:24:38.384494: step 3470, loss 0.546134.
Train: 2018-08-05T18:24:38.587572: step 3471, loss 0.553954.
Train: 2018-08-05T18:24:38.806300: step 3472, loss 0.516304.
Train: 2018-08-05T18:24:38.978136: step 3473, loss 0.623798.
Train: 2018-08-05T18:24:39.196834: step 3474, loss 0.610381.
Train: 2018-08-05T18:24:39.399912: step 3475, loss 0.581895.
Train: 2018-08-05T18:24:39.602989: step 3476, loss 0.572309.
Train: 2018-08-05T18:24:39.806066: step 3477, loss 0.489419.
Train: 2018-08-05T18:24:40.024735: step 3478, loss 0.581071.
Train: 2018-08-05T18:24:40.227843: step 3479, loss 0.471941.
Train: 2018-08-05T18:24:40.430890: step 3480, loss 0.562684.
Test: 2018-08-05T18:24:41.493142: step 3480, loss 0.549012.
Train: 2018-08-05T18:24:41.696218: step 3481, loss 0.517447.
Train: 2018-08-05T18:24:41.914918: step 3482, loss 0.499374.
Train: 2018-08-05T18:24:42.117996: step 3483, loss 0.535496.
Train: 2018-08-05T18:24:42.321073: step 3484, loss 0.489995.
Train: 2018-08-05T18:24:42.524180: step 3485, loss 0.498791.
Train: 2018-08-05T18:24:42.727258: step 3486, loss 0.562951.
Train: 2018-08-05T18:24:42.945956: step 3487, loss 0.535232.
Train: 2018-08-05T18:24:43.149034: step 3488, loss 0.516519.
Train: 2018-08-05T18:24:43.352111: step 3489, loss 0.647901.
Train: 2018-08-05T18:24:43.555188: step 3490, loss 0.591525.
Test: 2018-08-05T18:24:44.601789: step 3490, loss 0.548485.
Train: 2018-08-05T18:24:44.804896: step 3491, loss 0.53516.
Train: 2018-08-05T18:24:45.007977: step 3492, loss 0.58192.
Train: 2018-08-05T18:24:45.211051: step 3493, loss 0.544514.
Train: 2018-08-05T18:24:45.429749: step 3494, loss 0.563051.
Train: 2018-08-05T18:24:45.632830: step 3495, loss 0.581414.
Train: 2018-08-05T18:24:45.835874: step 3496, loss 0.553685.
Train: 2018-08-05T18:24:46.038986: step 3497, loss 0.580975.
Train: 2018-08-05T18:24:46.242030: step 3498, loss 0.50841.
Train: 2018-08-05T18:24:46.429515: step 3499, loss 0.526601.
Train: 2018-08-05T18:24:46.632593: step 3500, loss 0.481794.
Test: 2018-08-05T18:24:47.694844: step 3500, loss 0.54716.
Train: 2018-08-05T18:24:48.628047: step 3501, loss 0.625504.
Train: 2018-08-05T18:24:48.831095: step 3502, loss 0.580477.
Train: 2018-08-05T18:24:49.034197: step 3503, loss 0.544666.
Train: 2018-08-05T18:24:49.221657: step 3504, loss 0.615829.
Train: 2018-08-05T18:24:49.440357: step 3505, loss 0.509443.
Train: 2018-08-05T18:24:49.643434: step 3506, loss 0.615207.
Train: 2018-08-05T18:24:49.846514: step 3507, loss 0.562373.
Train: 2018-08-05T18:24:50.049559: step 3508, loss 0.492862.
Train: 2018-08-05T18:24:50.237047: step 3509, loss 0.553676.
Train: 2018-08-05T18:24:50.440093: step 3510, loss 0.501732.
Test: 2018-08-05T18:24:51.502344: step 3510, loss 0.547656.
Train: 2018-08-05T18:24:51.689830: step 3511, loss 0.536314.
Train: 2018-08-05T18:24:51.892878: step 3512, loss 0.623308.
Train: 2018-08-05T18:24:52.080364: step 3513, loss 0.588454.
Train: 2018-08-05T18:24:52.283411: step 3514, loss 0.666471.
Train: 2018-08-05T18:24:52.486513: step 3515, loss 0.553734.
Train: 2018-08-05T18:24:52.673974: step 3516, loss 0.579413.
Train: 2018-08-05T18:24:52.877022: step 3517, loss 0.545412.
Train: 2018-08-05T18:24:53.064507: step 3518, loss 0.553954.
Train: 2018-08-05T18:24:53.267587: step 3519, loss 0.612693.
Train: 2018-08-05T18:24:53.455042: step 3520, loss 0.554114.
Test: 2018-08-05T18:24:54.517293: step 3520, loss 0.550176.
Train: 2018-08-05T18:24:54.704749: step 3521, loss 0.570757.
Train: 2018-08-05T18:24:54.892206: step 3522, loss 0.578995.
Train: 2018-08-05T18:24:55.095282: step 3523, loss 0.529785.
Train: 2018-08-05T18:24:55.282709: step 3524, loss 0.554407.
Train: 2018-08-05T18:24:55.485818: step 3525, loss 0.505376.
Train: 2018-08-05T18:24:55.688874: step 3526, loss 0.513337.
Train: 2018-08-05T18:24:55.876349: step 3527, loss 0.512937.
Train: 2018-08-05T18:24:56.063806: step 3528, loss 0.504045.
Train: 2018-08-05T18:24:56.251262: step 3529, loss 0.545473.
Train: 2018-08-05T18:24:56.454340: step 3530, loss 0.553785.
Test: 2018-08-05T18:24:57.500969: step 3530, loss 0.546301.
Train: 2018-08-05T18:24:57.688425: step 3531, loss 0.536388.
Train: 2018-08-05T18:24:57.891503: step 3532, loss 0.50109.
Train: 2018-08-05T18:24:58.078958: step 3533, loss 0.580485.
Train: 2018-08-05T18:24:58.266409: step 3534, loss 0.580508.
Train: 2018-08-05T18:24:58.453841: step 3535, loss 0.571694.
Train: 2018-08-05T18:24:58.656951: step 3536, loss 0.608192.
Train: 2018-08-05T18:24:58.844407: step 3537, loss 0.599186.
Train: 2018-08-05T18:24:59.047481: step 3538, loss 0.544543.
Train: 2018-08-05T18:24:59.234933: step 3539, loss 0.571803.
Train: 2018-08-05T18:24:59.422394: step 3540, loss 0.535496.
Test: 2018-08-05T18:25:00.469024: step 3540, loss 0.547136.
Train: 2018-08-05T18:25:00.656480: step 3541, loss 0.598871.
Train: 2018-08-05T18:25:00.859559: step 3542, loss 0.54459.
Train: 2018-08-05T18:25:01.046984: step 3543, loss 0.544613.
Train: 2018-08-05T18:25:01.234440: step 3544, loss 0.464014.
Train: 2018-08-05T18:25:01.421926: step 3545, loss 0.62544.
Train: 2018-08-05T18:25:01.609352: step 3546, loss 0.598405.
Train: 2018-08-05T18:25:01.796808: step 3547, loss 0.571432.
Train: 2018-08-05T18:25:01.968642: step 3548, loss 0.589092.
Train: 2018-08-05T18:25:02.156129: step 3549, loss 0.571233.
Train: 2018-08-05T18:25:02.343583: step 3550, loss 0.501107.
Test: 2018-08-05T18:25:03.405808: step 3550, loss 0.546588.
Train: 2018-08-05T18:25:03.624534: step 3551, loss 0.544914.
Train: 2018-08-05T18:25:03.843205: step 3552, loss 0.605899.
Train: 2018-08-05T18:25:04.077525: step 3553, loss 0.57968.
Train: 2018-08-05T18:25:04.265010: step 3554, loss 0.536478.
Train: 2018-08-05T18:25:04.452466: step 3555, loss 0.545156.
Train: 2018-08-05T18:25:04.624301: step 3556, loss 0.493772.
Train: 2018-08-05T18:25:04.811757: step 3557, loss 0.605277.
Train: 2018-08-05T18:25:04.983562: step 3558, loss 0.553753.
Train: 2018-08-05T18:25:05.171051: step 3559, loss 0.536596.
Train: 2018-08-05T18:25:05.358506: step 3560, loss 0.622468.
Test: 2018-08-05T18:25:06.420726: step 3560, loss 0.546053.
Train: 2018-08-05T18:25:06.592590: step 3561, loss 0.553767.
Train: 2018-08-05T18:25:06.764425: step 3562, loss 0.545231.
Train: 2018-08-05T18:25:06.951851: step 3563, loss 0.596526.
Train: 2018-08-05T18:25:07.123716: step 3564, loss 0.502668.
Train: 2018-08-05T18:25:07.311142: step 3565, loss 0.605015.
Train: 2018-08-05T18:25:07.483008: step 3566, loss 0.58791.
Train: 2018-08-05T18:25:07.670433: step 3567, loss 0.655834.
Train: 2018-08-05T18:25:07.842267: step 3568, loss 0.59609.
Train: 2018-08-05T18:25:08.014132: step 3569, loss 0.554066.
Train: 2018-08-05T18:25:08.201588: step 3570, loss 0.587335.
Test: 2018-08-05T18:25:09.248218: step 3570, loss 0.547667.
Train: 2018-08-05T18:25:09.420022: step 3571, loss 0.611873.
Train: 2018-08-05T18:25:09.607511: step 3572, loss 0.497496.
Train: 2018-08-05T18:25:09.779314: step 3573, loss 0.514027.
Train: 2018-08-05T18:25:09.966807: step 3574, loss 0.51402.
Train: 2018-08-05T18:25:10.138604: step 3575, loss 0.611488.
Train: 2018-08-05T18:25:10.310469: step 3576, loss 0.587076.
Train: 2018-08-05T18:25:10.482306: step 3577, loss 0.578926.
Train: 2018-08-05T18:25:10.654139: step 3578, loss 0.587071.
Train: 2018-08-05T18:25:10.825974: step 3579, loss 0.578917.
Train: 2018-08-05T18:25:10.997779: step 3580, loss 0.554536.
Test: 2018-08-05T18:25:12.060065: step 3580, loss 0.548722.
Train: 2018-08-05T18:25:12.231865: step 3581, loss 0.522046.
Train: 2018-08-05T18:25:12.403729: step 3582, loss 0.538182.
Train: 2018-08-05T18:25:12.575564: step 3583, loss 0.546197.
Train: 2018-08-05T18:25:12.747369: step 3584, loss 0.644906.
Train: 2018-08-05T18:25:12.919237: step 3585, loss 0.587247.
Train: 2018-08-05T18:25:13.091038: step 3586, loss 0.58724.
Train: 2018-08-05T18:25:13.262903: step 3587, loss 0.554298.
Train: 2018-08-05T18:25:13.434738: step 3588, loss 0.529619.
Train: 2018-08-05T18:25:13.606574: step 3589, loss 0.570756.
Train: 2018-08-05T18:25:13.778377: step 3590, loss 0.587284.
Test: 2018-08-05T18:25:14.825037: step 3590, loss 0.548104.
Train: 2018-08-05T18:25:14.996872: step 3591, loss 0.545946.
Train: 2018-08-05T18:25:15.168707: step 3592, loss 0.587333.
Train: 2018-08-05T18:25:15.340541: step 3593, loss 0.562464.
Train: 2018-08-05T18:25:15.496756: step 3594, loss 0.653792.
Train: 2018-08-05T18:25:15.668590: step 3595, loss 0.570756.
Train: 2018-08-05T18:25:15.840424: step 3596, loss 0.595461.
Train: 2018-08-05T18:25:16.012254: step 3597, loss 0.578955.
Train: 2018-08-05T18:25:16.184096: step 3598, loss 0.603375.
Train: 2018-08-05T18:25:16.355931: step 3599, loss 0.497935.
Train: 2018-08-05T18:25:16.527763: step 3600, loss 0.578891.
Test: 2018-08-05T18:25:17.574363: step 3600, loss 0.549801.
Train: 2018-08-05T18:25:18.511673: step 3601, loss 0.562733.
Train: 2018-08-05T18:25:18.683509: step 3602, loss 0.562738.
Train: 2018-08-05T18:25:18.839722: step 3603, loss 0.586963.
Train: 2018-08-05T18:25:19.011557: step 3604, loss 0.554665.
Train: 2018-08-05T18:25:19.183392: step 3605, loss 0.546567.
Train: 2018-08-05T18:25:19.355228: step 3606, loss 0.562695.
Train: 2018-08-05T18:25:19.511440: step 3607, loss 0.562659.
Train: 2018-08-05T18:25:19.683245: step 3608, loss 0.546315.
Train: 2018-08-05T18:25:19.855111: step 3609, loss 0.562572.
Train: 2018-08-05T18:25:20.026944: step 3610, loss 0.504908.
Test: 2018-08-05T18:25:21.073574: step 3610, loss 0.548403.
Train: 2018-08-05T18:25:21.245404: step 3611, loss 0.554157.
Train: 2018-08-05T18:25:21.417243: step 3612, loss 0.579147.
Train: 2018-08-05T18:25:21.573457: step 3613, loss 0.57923.
Train: 2018-08-05T18:25:21.745291: step 3614, loss 0.579301.
Train: 2018-08-05T18:25:21.917126: step 3615, loss 0.485789.
Train: 2018-08-05T18:25:22.073310: step 3616, loss 0.536608.
Train: 2018-08-05T18:25:22.245145: step 3617, loss 0.640206.
Train: 2018-08-05T18:25:22.416979: step 3618, loss 0.588382.
Train: 2018-08-05T18:25:22.588814: step 3619, loss 0.553665.
Train: 2018-08-05T18:25:22.745027: step 3620, loss 0.571051.
Test: 2018-08-05T18:25:23.807279: step 3620, loss 0.547828.
Train: 2018-08-05T18:25:23.963493: step 3621, loss 0.640641.
Train: 2018-08-05T18:25:24.135357: step 3622, loss 0.510398.
Train: 2018-08-05T18:25:24.307162: step 3623, loss 0.65745.
Train: 2018-08-05T18:25:24.432133: step 3624, loss 0.470757.
Train: 2018-08-05T18:25:24.603967: step 3625, loss 0.502291.
Train: 2018-08-05T18:25:24.775801: step 3626, loss 0.579538.
Train: 2018-08-05T18:25:24.947661: step 3627, loss 0.605402.
Train: 2018-08-05T18:25:25.103852: step 3628, loss 0.579536.
Train: 2018-08-05T18:25:25.275715: step 3629, loss 0.579493.
Train: 2018-08-05T18:25:25.447550: step 3630, loss 0.570887.
Test: 2018-08-05T18:25:26.494179: step 3630, loss 0.548896.
Train: 2018-08-05T18:25:26.666014: step 3631, loss 0.579381.
Train: 2018-08-05T18:25:26.837829: step 3632, loss 0.553866.
Train: 2018-08-05T18:25:26.994062: step 3633, loss 0.604646.
Train: 2018-08-05T18:25:27.165867: step 3634, loss 0.56238.
Train: 2018-08-05T18:25:27.322114: step 3635, loss 0.562402.
Train: 2018-08-05T18:25:27.493940: step 3636, loss 0.495695.
Train: 2018-08-05T18:25:27.665780: step 3637, loss 0.570768.
Train: 2018-08-05T18:25:27.821994: step 3638, loss 0.545705.
Train: 2018-08-05T18:25:27.993829: step 3639, loss 0.545661.
Train: 2018-08-05T18:25:28.165663: step 3640, loss 0.604373.
Test: 2018-08-05T18:25:29.212293: step 3640, loss 0.548357.
Train: 2018-08-05T18:25:29.384128: step 3641, loss 0.579187.
Train: 2018-08-05T18:25:29.540310: step 3642, loss 0.5372.
Train: 2018-08-05T18:25:29.712178: step 3643, loss 0.520343.
Train: 2018-08-05T18:25:29.930874: step 3644, loss 0.511714.
Train: 2018-08-05T18:25:30.102711: step 3645, loss 0.562345.
Train: 2018-08-05T18:25:30.258893: step 3646, loss 0.587998.
Train: 2018-08-05T18:25:30.430758: step 3647, loss 0.570923.
Train: 2018-08-05T18:25:30.586942: step 3648, loss 0.519263.
Train: 2018-08-05T18:25:30.758776: step 3649, loss 0.571004.
Train: 2018-08-05T18:25:30.915019: step 3650, loss 0.501494.
Test: 2018-08-05T18:25:31.977241: step 3650, loss 0.547154.
Train: 2018-08-05T18:25:32.133454: step 3651, loss 0.579891.
Train: 2018-08-05T18:25:32.305320: step 3652, loss 0.474427.
Train: 2018-08-05T18:25:32.477155: step 3653, loss 0.500317.
Train: 2018-08-05T18:25:32.648958: step 3654, loss 0.616453.
Train: 2018-08-05T18:25:32.805172: step 3655, loss 0.616868.
Train: 2018-08-05T18:25:32.977038: step 3656, loss 0.589828.
Train: 2018-08-05T18:25:33.148871: step 3657, loss 0.535524.
Train: 2018-08-05T18:25:33.305085: step 3658, loss 0.598844.
Train: 2018-08-05T18:25:33.476921: step 3659, loss 0.571646.
Train: 2018-08-05T18:25:33.633132: step 3660, loss 0.580554.
Test: 2018-08-05T18:25:34.695354: step 3660, loss 0.548422.
Train: 2018-08-05T18:25:34.851599: step 3661, loss 0.580407.
Train: 2018-08-05T18:25:35.023433: step 3662, loss 0.580236.
Train: 2018-08-05T18:25:35.195268: step 3663, loss 0.421343.
Train: 2018-08-05T18:25:35.351482: step 3664, loss 0.606569.
Train: 2018-08-05T18:25:35.523286: step 3665, loss 0.509545.
Train: 2018-08-05T18:25:35.695150: step 3666, loss 0.553601.
Train: 2018-08-05T18:25:35.866986: step 3667, loss 0.527113.
Train: 2018-08-05T18:25:36.038790: step 3668, loss 0.580145.
Train: 2018-08-05T18:25:36.195002: step 3669, loss 0.553593.
Train: 2018-08-05T18:25:36.366838: step 3670, loss 0.62449.
Test: 2018-08-05T18:25:37.429119: step 3670, loss 0.548087.
Train: 2018-08-05T18:25:37.585332: step 3671, loss 0.624244.
Train: 2018-08-05T18:25:37.757167: step 3672, loss 0.614985.
Train: 2018-08-05T18:25:37.913381: step 3673, loss 0.571027.
Train: 2018-08-05T18:25:38.085215: step 3674, loss 0.570928.
Train: 2018-08-05T18:25:38.241429: step 3675, loss 0.6049.
Train: 2018-08-05T18:25:38.413263: step 3676, loss 0.570794.
Train: 2018-08-05T18:25:38.585067: step 3677, loss 0.537447.
Train: 2018-08-05T18:25:38.741314: step 3678, loss 0.51289.
Train: 2018-08-05T18:25:38.913146: step 3679, loss 0.644916.
Train: 2018-08-05T18:25:39.084952: step 3680, loss 0.570767.
Test: 2018-08-05T18:25:40.131580: step 3680, loss 0.548541.
Train: 2018-08-05T18:25:40.365934: step 3681, loss 0.513936.
Train: 2018-08-05T18:25:40.537735: step 3682, loss 0.595103.
Train: 2018-08-05T18:25:40.693980: step 3683, loss 0.586959.
Train: 2018-08-05T18:25:40.865814: step 3684, loss 0.627127.
Train: 2018-08-05T18:25:41.037649: step 3685, loss 0.53094.
Train: 2018-08-05T18:25:41.209490: step 3686, loss 0.547016.
Train: 2018-08-05T18:25:41.381318: step 3687, loss 0.602718.
Train: 2018-08-05T18:25:41.537531: step 3688, loss 0.507458.
Train: 2018-08-05T18:25:41.709337: step 3689, loss 0.570906.
Train: 2018-08-05T18:25:41.881202: step 3690, loss 0.570887.
Test: 2018-08-05T18:25:42.927831: step 3690, loss 0.549154.
Train: 2018-08-05T18:25:43.099668: step 3691, loss 0.554879.
Train: 2018-08-05T18:25:43.255879: step 3692, loss 0.554793.
Train: 2018-08-05T18:25:43.427714: step 3693, loss 0.506304.
Train: 2018-08-05T18:25:43.599519: step 3694, loss 0.60332.
Train: 2018-08-05T18:25:43.771384: step 3695, loss 0.5544.
Train: 2018-08-05T18:25:43.927597: step 3696, loss 0.51311.
Train: 2018-08-05T18:25:44.099434: step 3697, loss 0.60401.
Train: 2018-08-05T18:25:44.271267: step 3698, loss 0.487137.
Train: 2018-08-05T18:25:44.443103: step 3699, loss 0.545459.
Train: 2018-08-05T18:25:44.599284: step 3700, loss 0.562338.
Test: 2018-08-05T18:25:45.661536: step 3700, loss 0.546946.
Train: 2018-08-05T18:25:46.598850: step 3701, loss 0.570959.
Train: 2018-08-05T18:25:46.770683: step 3702, loss 0.571042.
Train: 2018-08-05T18:25:46.926894: step 3703, loss 0.623583.
Train: 2018-08-05T18:25:47.098700: step 3704, loss 0.544865.
Train: 2018-08-05T18:25:47.270564: step 3705, loss 0.544838.
Train: 2018-08-05T18:25:47.426748: step 3706, loss 0.571208.
Train: 2018-08-05T18:25:47.598583: step 3707, loss 0.588856.
Train: 2018-08-05T18:25:47.754796: step 3708, loss 0.580026.
Train: 2018-08-05T18:25:47.926661: step 3709, loss 0.5624.
Train: 2018-08-05T18:25:48.098499: step 3710, loss 0.632534.
Test: 2018-08-05T18:25:49.145125: step 3710, loss 0.549216.
Train: 2018-08-05T18:25:49.316930: step 3711, loss 0.553649.
Train: 2018-08-05T18:25:49.488764: step 3712, loss 0.553683.
Train: 2018-08-05T18:25:49.645008: step 3713, loss 0.553718.
Train: 2018-08-05T18:25:49.816843: step 3714, loss 0.622431.
Train: 2018-08-05T18:25:49.988678: step 3715, loss 0.596431.
Train: 2018-08-05T18:25:50.144893: step 3716, loss 0.494769.
Train: 2018-08-05T18:25:50.316725: step 3717, loss 0.629734.
Train: 2018-08-05T18:25:50.488531: step 3718, loss 0.604212.
Train: 2018-08-05T18:25:50.644774: step 3719, loss 0.446451.
Train: 2018-08-05T18:25:50.816610: step 3720, loss 0.645327.
Test: 2018-08-05T18:25:51.863239: step 3720, loss 0.548174.
Train: 2018-08-05T18:25:52.035073: step 3721, loss 0.554267.
Train: 2018-08-05T18:25:52.206905: step 3722, loss 0.521463.
Train: 2018-08-05T18:25:52.363091: step 3723, loss 0.562543.
Train: 2018-08-05T18:25:52.550577: step 3724, loss 0.562539.
Train: 2018-08-05T18:25:52.722382: step 3725, loss 0.521388.
Train: 2018-08-05T18:25:52.878595: step 3726, loss 0.512929.
Train: 2018-08-05T18:25:53.050460: step 3727, loss 0.579082.
Train: 2018-08-05T18:25:53.222297: step 3728, loss 0.604246.
Train: 2018-08-05T18:25:53.378509: step 3729, loss 0.495259.
Train: 2018-08-05T18:25:53.550343: step 3730, loss 0.604611.
Test: 2018-08-05T18:25:54.596944: step 3730, loss 0.547991.
Train: 2018-08-05T18:25:54.768812: step 3731, loss 0.553869.
Train: 2018-08-05T18:25:54.940643: step 3732, loss 0.553826.
Train: 2018-08-05T18:25:55.112447: step 3733, loss 0.451175.
Train: 2018-08-05T18:25:55.284313: step 3734, loss 0.56234.
Train: 2018-08-05T18:25:55.440527: step 3735, loss 0.597243.
Train: 2018-08-05T18:25:55.612361: step 3736, loss 0.518533.
Train: 2018-08-05T18:25:55.784195: step 3737, loss 0.60662.
Train: 2018-08-05T18:25:55.940410: step 3738, loss 0.606809.
Train: 2018-08-05T18:25:56.112244: step 3739, loss 0.624561.
Train: 2018-08-05T18:25:56.284049: step 3740, loss 0.491745.
Test: 2018-08-05T18:25:57.346331: step 3740, loss 0.547269.
Train: 2018-08-05T18:25:57.518165: step 3741, loss 0.580107.
Train: 2018-08-05T18:25:57.674378: step 3742, loss 0.535951.
Train: 2018-08-05T18:25:57.846215: step 3743, loss 0.527132.
Train: 2018-08-05T18:25:58.018047: step 3744, loss 0.571268.
Train: 2018-08-05T18:25:58.174231: step 3745, loss 0.527081.
Train: 2018-08-05T18:25:58.346098: step 3746, loss 0.562448.
Train: 2018-08-05T18:25:58.517931: step 3747, loss 0.633387.
Train: 2018-08-05T18:25:58.689765: step 3748, loss 0.535928.
Train: 2018-08-05T18:25:58.861569: step 3749, loss 0.518339.
Train: 2018-08-05T18:25:59.017813: step 3750, loss 0.615327.
Test: 2018-08-05T18:26:00.080035: step 3750, loss 0.547924.
Train: 2018-08-05T18:26:00.251901: step 3751, loss 0.571188.
Train: 2018-08-05T18:26:00.423740: step 3752, loss 0.571134.
Train: 2018-08-05T18:26:00.595571: step 3753, loss 0.562362.
Train: 2018-08-05T18:26:00.751782: step 3754, loss 0.605744.
Train: 2018-08-05T18:26:00.923617: step 3755, loss 0.553716.
Train: 2018-08-05T18:26:01.095455: step 3756, loss 0.510895.
Train: 2018-08-05T18:26:01.251665: step 3757, loss 0.562336.
Train: 2018-08-05T18:26:01.423469: step 3758, loss 0.528175.
Train: 2018-08-05T18:26:01.595336: step 3759, loss 0.605059.
Train: 2018-08-05T18:26:01.767140: step 3760, loss 0.553815.
Test: 2018-08-05T18:26:02.813800: step 3760, loss 0.547145.
Train: 2018-08-05T18:26:03.001250: step 3761, loss 0.511263.
Train: 2018-08-05T18:26:03.157469: step 3762, loss 0.562339.
Train: 2018-08-05T18:26:03.329274: step 3763, loss 0.493989.
Train: 2018-08-05T18:26:03.501138: step 3764, loss 0.562335.
Train: 2018-08-05T18:26:03.672968: step 3765, loss 0.570974.
Train: 2018-08-05T18:26:03.829157: step 3766, loss 0.666366.
Train: 2018-08-05T18:26:04.001021: step 3767, loss 0.553698.
Train: 2018-08-05T18:26:04.172859: step 3768, loss 0.536471.
Train: 2018-08-05T18:26:04.344692: step 3769, loss 0.48479.
Train: 2018-08-05T18:26:04.516526: step 3770, loss 0.536391.
Test: 2018-08-05T18:26:05.563155: step 3770, loss 0.547035.
Train: 2018-08-05T18:26:05.734991: step 3771, loss 0.562353.
Train: 2018-08-05T18:26:05.906795: step 3772, loss 0.562368.
Train: 2018-08-05T18:26:06.063038: step 3773, loss 0.562383.
Train: 2018-08-05T18:26:06.234874: step 3774, loss 0.553613.
Train: 2018-08-05T18:26:06.375465: step 3775, loss 0.63756.
Train: 2018-08-05T18:26:06.547269: step 3776, loss 0.641457.
Train: 2018-08-05T18:26:06.719136: step 3777, loss 0.562364.
Train: 2018-08-05T18:26:06.890939: step 3778, loss 0.545023.
Train: 2018-08-05T18:26:07.047183: step 3779, loss 0.562336.
Train: 2018-08-05T18:26:07.219020: step 3780, loss 0.528051.
Test: 2018-08-05T18:26:08.265618: step 3780, loss 0.547861.
Train: 2018-08-05T18:26:08.437453: step 3781, loss 0.562337.
Train: 2018-08-05T18:26:08.609288: step 3782, loss 0.596463.
Train: 2018-08-05T18:26:08.781152: step 3783, loss 0.562346.
Train: 2018-08-05T18:26:08.937366: step 3784, loss 0.503096.
Train: 2018-08-05T18:26:09.109171: step 3785, loss 0.613175.
Train: 2018-08-05T18:26:09.265414: step 3786, loss 0.503223.
Train: 2018-08-05T18:26:09.437219: step 3787, loss 0.570819.
Train: 2018-08-05T18:26:09.609083: step 3788, loss 0.528478.
Train: 2018-08-05T18:26:09.780919: step 3789, loss 0.519871.
Train: 2018-08-05T18:26:09.952752: step 3790, loss 0.528172.
Test: 2018-08-05T18:26:11.014974: step 3790, loss 0.547378.
Train: 2018-08-05T18:26:11.171217: step 3791, loss 0.562335.
Train: 2018-08-05T18:26:11.343054: step 3792, loss 0.545035.
Train: 2018-08-05T18:26:11.514887: step 3793, loss 0.55365.
Train: 2018-08-05T18:26:11.686722: step 3794, loss 0.562383.
Train: 2018-08-05T18:26:11.842937: step 3795, loss 0.56241.
Train: 2018-08-05T18:26:12.014770: step 3796, loss 0.544755.
Train: 2018-08-05T18:26:12.186605: step 3797, loss 0.597981.
Train: 2018-08-05T18:26:12.342821: step 3798, loss 0.482479.
Train: 2018-08-05T18:26:12.514653: step 3799, loss 0.598264.
Train: 2018-08-05T18:26:12.686487: step 3800, loss 0.598353.
Test: 2018-08-05T18:26:13.748708: step 3800, loss 0.548832.
Train: 2018-08-05T18:26:14.701647: step 3801, loss 0.544646.
Train: 2018-08-05T18:26:14.873476: step 3802, loss 0.598287.
Train: 2018-08-05T18:26:15.045310: step 3803, loss 0.544677.
Train: 2018-08-05T18:26:15.217146: step 3804, loss 0.59804.
Train: 2018-08-05T18:26:15.373358: step 3805, loss 0.650919.
Train: 2018-08-05T18:26:15.545162: step 3806, loss 0.571142.
Train: 2018-08-05T18:26:15.717029: step 3807, loss 0.579693.
Train: 2018-08-05T18:26:15.888863: step 3808, loss 0.605256.
Train: 2018-08-05T18:26:16.060698: step 3809, loss 0.460566.
Train: 2018-08-05T18:26:16.216912: step 3810, loss 0.638364.
Test: 2018-08-05T18:26:17.279132: step 3810, loss 0.548228.
Train: 2018-08-05T18:26:17.450996: step 3811, loss 0.554031.
Train: 2018-08-05T18:26:17.607210: step 3812, loss 0.545816.
Train: 2018-08-05T18:26:17.779045: step 3813, loss 0.488011.
Train: 2018-08-05T18:26:17.950879: step 3814, loss 0.537634.
Train: 2018-08-05T18:26:18.122714: step 3815, loss 0.628877.
Train: 2018-08-05T18:26:18.294520: step 3816, loss 0.454684.
Train: 2018-08-05T18:26:18.466386: step 3817, loss 0.562426.
Train: 2018-08-05T18:26:18.622598: step 3818, loss 0.562394.
Train: 2018-08-05T18:26:18.794402: step 3819, loss 0.494924.
Train: 2018-08-05T18:26:18.966269: step 3820, loss 0.528315.
Test: 2018-08-05T18:26:20.028489: step 3820, loss 0.548575.
Train: 2018-08-05T18:26:20.184732: step 3821, loss 0.682659.
Train: 2018-08-05T18:26:20.356566: step 3822, loss 0.553726.
Train: 2018-08-05T18:26:20.528401: step 3823, loss 0.562338.
Train: 2018-08-05T18:26:20.700205: step 3824, loss 0.536412.
Train: 2018-08-05T18:26:20.872071: step 3825, loss 0.571014.
Train: 2018-08-05T18:26:21.043906: step 3826, loss 0.518928.
Train: 2018-08-05T18:26:21.200119: step 3827, loss 0.518764.
Train: 2018-08-05T18:26:21.371953: step 3828, loss 0.571162.
Train: 2018-08-05T18:26:21.559410: step 3829, loss 0.491924.
Train: 2018-08-05T18:26:21.715625: step 3830, loss 0.500313.
Test: 2018-08-05T18:26:22.777875: step 3830, loss 0.547583.
Train: 2018-08-05T18:26:22.949679: step 3831, loss 0.589467.
Train: 2018-08-05T18:26:23.121545: step 3832, loss 0.526521.
Train: 2018-08-05T18:26:23.293374: step 3833, loss 0.544543.
Train: 2018-08-05T18:26:23.465215: step 3834, loss 0.645252.
Train: 2018-08-05T18:26:23.637048: step 3835, loss 0.562843.
Train: 2018-08-05T18:26:23.808883: step 3836, loss 0.581146.
Train: 2018-08-05T18:26:23.980688: step 3837, loss 0.608461.
Train: 2018-08-05T18:26:24.152553: step 3838, loss 0.54455.
Train: 2018-08-05T18:26:24.324389: step 3839, loss 0.634927.
Train: 2018-08-05T18:26:24.496192: step 3840, loss 0.634142.
Test: 2018-08-05T18:26:25.542852: step 3840, loss 0.547883.
Train: 2018-08-05T18:26:25.714688: step 3841, loss 0.571261.
Train: 2018-08-05T18:26:25.886521: step 3842, loss 0.597231.
Train: 2018-08-05T18:26:26.058326: step 3843, loss 0.519363.
Train: 2018-08-05T18:26:26.230190: step 3844, loss 0.536826.
Train: 2018-08-05T18:26:26.402029: step 3845, loss 0.570807.
Train: 2018-08-05T18:26:26.573831: step 3846, loss 0.554021.
Train: 2018-08-05T18:26:26.745695: step 3847, loss 0.487468.
Train: 2018-08-05T18:26:26.917530: step 3848, loss 0.645707.
Train: 2018-08-05T18:26:27.089335: step 3849, loss 0.504479.
Train: 2018-08-05T18:26:27.276820: step 3850, loss 0.579037.
Test: 2018-08-05T18:26:28.323421: step 3850, loss 0.548102.
Train: 2018-08-05T18:26:28.495256: step 3851, loss 0.554215.
Train: 2018-08-05T18:26:28.667120: step 3852, loss 0.579027.
Train: 2018-08-05T18:26:28.854576: step 3853, loss 0.628611.
Train: 2018-08-05T18:26:29.026382: step 3854, loss 0.603676.
Train: 2018-08-05T18:26:29.198245: step 3855, loss 0.619852.
Train: 2018-08-05T18:26:29.385702: step 3856, loss 0.578903.
Train: 2018-08-05T18:26:29.557537: step 3857, loss 0.546682.
Train: 2018-08-05T18:26:29.729371: step 3858, loss 0.58687.
Train: 2018-08-05T18:26:29.901207: step 3859, loss 0.594782.
Train: 2018-08-05T18:26:30.088632: step 3860, loss 0.586773.
Test: 2018-08-05T18:26:31.135292: step 3860, loss 0.551269.
Train: 2018-08-05T18:26:31.307097: step 3861, loss 0.539541.
Train: 2018-08-05T18:26:31.478961: step 3862, loss 0.461221.
Train: 2018-08-05T18:26:31.650767: step 3863, loss 0.634061.
Train: 2018-08-05T18:26:31.838255: step 3864, loss 0.499915.
Train: 2018-08-05T18:26:32.010087: step 3865, loss 0.586802.
Train: 2018-08-05T18:26:32.181922: step 3866, loss 0.546926.
Train: 2018-08-05T18:26:32.369349: step 3867, loss 0.546731.
Train: 2018-08-05T18:26:32.541183: step 3868, loss 0.611281.
Train: 2018-08-05T18:26:32.713049: step 3869, loss 0.570782.
Train: 2018-08-05T18:26:32.900504: step 3870, loss 0.505439.
Test: 2018-08-05T18:26:33.947133: step 3870, loss 0.549348.
Train: 2018-08-05T18:26:34.134560: step 3871, loss 0.521371.
Train: 2018-08-05T18:26:34.306424: step 3872, loss 0.512555.
Train: 2018-08-05T18:26:34.493880: step 3873, loss 0.604475.
Train: 2018-08-05T18:26:34.665715: step 3874, loss 0.51138.
Train: 2018-08-05T18:26:34.853182: step 3875, loss 0.570924.
Train: 2018-08-05T18:26:35.025006: step 3876, loss 0.631701.
Train: 2018-08-05T18:26:35.212432: step 3877, loss 0.571062.
Train: 2018-08-05T18:26:35.399918: step 3878, loss 0.597285.
Train: 2018-08-05T18:26:35.587377: step 3879, loss 0.623487.
Train: 2018-08-05T18:26:35.774831: step 3880, loss 0.562356.
Test: 2018-08-05T18:26:36.821431: step 3880, loss 0.547471.
Train: 2018-08-05T18:26:37.008917: step 3881, loss 0.588349.
Train: 2018-08-05T18:26:37.180751: step 3882, loss 0.579587.
Train: 2018-08-05T18:26:37.368178: step 3883, loss 0.536608.
Train: 2018-08-05T18:26:37.555658: step 3884, loss 0.553792.
Train: 2018-08-05T18:26:37.727499: step 3885, loss 0.562341.
Train: 2018-08-05T18:26:37.914924: step 3886, loss 0.570846.
Train: 2018-08-05T18:26:38.102381: step 3887, loss 0.452142.
Train: 2018-08-05T18:26:38.289838: step 3888, loss 0.553827.
Train: 2018-08-05T18:26:38.477323: step 3889, loss 0.57089.
Train: 2018-08-05T18:26:38.664779: step 3890, loss 0.579502.
Test: 2018-08-05T18:26:39.711411: step 3890, loss 0.547773.
Train: 2018-08-05T18:26:39.898865: step 3891, loss 0.588139.
Train: 2018-08-05T18:26:40.086323: step 3892, loss 0.553733.
Train: 2018-08-05T18:26:40.273777: step 3893, loss 0.545119.
Train: 2018-08-05T18:26:40.461235: step 3894, loss 0.545095.
Train: 2018-08-05T18:26:40.648689: step 3895, loss 0.510499.
Train: 2018-08-05T18:26:40.836147: step 3896, loss 0.64918.
Train: 2018-08-05T18:26:41.023572: step 3897, loss 0.597047.
Train: 2018-08-05T18:26:41.211059: step 3898, loss 0.545046.
Train: 2018-08-05T18:26:41.398484: step 3899, loss 0.458768.
Train: 2018-08-05T18:26:41.585972: step 3900, loss 0.571014.
Test: 2018-08-05T18:26:42.632596: step 3900, loss 0.547833.
Train: 2018-08-05T18:26:43.538637: step 3901, loss 0.571049.
Train: 2018-08-05T18:26:43.726094: step 3902, loss 0.510084.
Train: 2018-08-05T18:26:43.913544: step 3903, loss 0.606147.
Train: 2018-08-05T18:26:44.116627: step 3904, loss 0.579915.
Train: 2018-08-05T18:26:44.304053: step 3905, loss 0.57991.
Train: 2018-08-05T18:26:44.491510: step 3906, loss 0.579874.
Train: 2018-08-05T18:26:44.678995: step 3907, loss 0.605987.
Train: 2018-08-05T18:26:44.866453: step 3908, loss 0.544994.
Train: 2018-08-05T18:26:45.053908: step 3909, loss 0.614195.
Train: 2018-08-05T18:26:45.256985: step 3910, loss 0.579501.
Test: 2018-08-05T18:26:46.303586: step 3910, loss 0.548504.
Train: 2018-08-05T18:26:46.537933: step 3911, loss 0.51974.
Train: 2018-08-05T18:26:46.772258: step 3912, loss 0.519919.
Train: 2018-08-05T18:26:47.022168: step 3913, loss 0.545394.
Train: 2018-08-05T18:26:47.209655: step 3914, loss 0.519823.
Train: 2018-08-05T18:26:47.397110: step 3915, loss 0.482708.
Train: 2018-08-05T18:26:47.600187: step 3916, loss 0.574733.
Train: 2018-08-05T18:26:47.803264: step 3917, loss 0.641921.
Train: 2018-08-05T18:26:47.990720: step 3918, loss 0.526681.
Train: 2018-08-05T18:26:48.193797: step 3919, loss 0.605696.
Train: 2018-08-05T18:26:48.396875: step 3920, loss 0.647498.
Test: 2018-08-05T18:26:49.443475: step 3920, loss 0.548853.
Train: 2018-08-05T18:26:49.646553: step 3921, loss 0.62145.
Train: 2018-08-05T18:26:49.834040: step 3922, loss 0.554078.
Train: 2018-08-05T18:26:50.037116: step 3923, loss 0.49637.
Train: 2018-08-05T18:26:50.224572: step 3924, loss 0.504904.
Train: 2018-08-05T18:26:50.443270: step 3925, loss 0.471935.
Train: 2018-08-05T18:26:50.615075: step 3926, loss 0.615517.
Train: 2018-08-05T18:26:50.818183: step 3927, loss 0.504253.
Train: 2018-08-05T18:26:51.021230: step 3928, loss 0.545669.
Train: 2018-08-05T18:26:51.224338: step 3929, loss 0.503369.
Train: 2018-08-05T18:26:51.427418: step 3930, loss 0.596404.
Test: 2018-08-05T18:26:52.474014: step 3930, loss 0.547814.
Train: 2018-08-05T18:26:52.677123: step 3931, loss 0.596641.
Train: 2018-08-05T18:26:52.880200: step 3932, loss 0.562336.
Train: 2018-08-05T18:26:53.083247: step 3933, loss 0.562341.
Train: 2018-08-05T18:26:53.286356: step 3934, loss 0.588369.
Train: 2018-08-05T18:26:53.489403: step 3935, loss 0.657873.
Train: 2018-08-05T18:26:53.692510: step 3936, loss 0.51913.
Train: 2018-08-05T18:26:53.895558: step 3937, loss 0.562338.
Train: 2018-08-05T18:26:54.098665: step 3938, loss 0.562336.
Train: 2018-08-05T18:26:54.301742: step 3939, loss 0.674048.
Train: 2018-08-05T18:26:54.520413: step 3940, loss 0.664602.
Test: 2018-08-05T18:26:55.567073: step 3940, loss 0.54757.
Train: 2018-08-05T18:26:55.770148: step 3941, loss 0.537165.
Train: 2018-08-05T18:26:55.973226: step 3942, loss 0.60403.
Train: 2018-08-05T18:26:56.176303: step 3943, loss 0.5461.
Train: 2018-08-05T18:26:56.395003: step 3944, loss 0.6115.
Train: 2018-08-05T18:26:56.598080: step 3945, loss 0.586936.
Train: 2018-08-05T18:26:56.816780: step 3946, loss 0.554938.
Train: 2018-08-05T18:26:57.019856: step 3947, loss 0.539311.
Train: 2018-08-05T18:26:57.222933: step 3948, loss 0.594612.
Train: 2018-08-05T18:26:57.441602: step 3949, loss 0.563216.
Train: 2018-08-05T18:26:57.644710: step 3950, loss 0.539872.
Test: 2018-08-05T18:26:58.706962: step 3950, loss 0.551523.
Train: 2018-08-05T18:26:58.910039: step 3951, loss 0.571092.
Train: 2018-08-05T18:26:59.128708: step 3952, loss 0.578891.
Train: 2018-08-05T18:26:59.347436: step 3953, loss 0.594499.
Train: 2018-08-05T18:26:59.550513: step 3954, loss 0.493087.
Train: 2018-08-05T18:26:59.769212: step 3955, loss 0.555349.
Train: 2018-08-05T18:26:59.987911: step 3956, loss 0.618325.
Train: 2018-08-05T18:27:00.190988: step 3957, loss 0.618448.
Train: 2018-08-05T18:27:00.409687: step 3958, loss 0.5947.
Train: 2018-08-05T18:27:00.612764: step 3959, loss 0.491809.
Train: 2018-08-05T18:27:00.831465: step 3960, loss 0.634531.
Test: 2018-08-05T18:27:01.878093: step 3960, loss 0.550747.
Train: 2018-08-05T18:27:02.096792: step 3961, loss 0.539063.
Train: 2018-08-05T18:27:02.299869: step 3962, loss 0.570877.
Train: 2018-08-05T18:27:02.518571: step 3963, loss 0.674984.
Train: 2018-08-05T18:27:02.721645: step 3964, loss 0.570878.
Train: 2018-08-05T18:27:02.940315: step 3965, loss 0.507208.
Train: 2018-08-05T18:27:03.159014: step 3966, loss 0.459195.
Train: 2018-08-05T18:27:03.377742: step 3967, loss 0.594982.
Train: 2018-08-05T18:27:03.580821: step 3968, loss 0.546462.
Train: 2018-08-05T18:27:03.799518: step 3969, loss 0.497187.
Train: 2018-08-05T18:27:04.002595: step 3970, loss 0.496298.
Test: 2018-08-05T18:27:05.064849: step 3970, loss 0.547588.
Train: 2018-08-05T18:27:05.283515: step 3971, loss 0.570786.
Train: 2018-08-05T18:27:05.486624: step 3972, loss 0.553833.
Train: 2018-08-05T18:27:05.720944: step 3973, loss 0.53649.
Train: 2018-08-05T18:27:05.924020: step 3974, loss 0.579812.
Train: 2018-08-05T18:27:06.142690: step 3975, loss 0.544794.
Train: 2018-08-05T18:27:06.361388: step 3976, loss 0.589162.
Train: 2018-08-05T18:27:06.580119: step 3977, loss 0.508839.
Train: 2018-08-05T18:27:06.798785: step 3978, loss 0.53556.
Train: 2018-08-05T18:27:07.001893: step 3979, loss 0.535446.
Train: 2018-08-05T18:27:07.220594: step 3980, loss 0.517012.
Test: 2018-08-05T18:27:08.282843: step 3980, loss 0.545431.
Train: 2018-08-05T18:27:08.485921: step 3981, loss 0.507514.
Train: 2018-08-05T18:27:08.704589: step 3982, loss 0.591217.
Train: 2018-08-05T18:27:08.923288: step 3983, loss 0.52575.
Train: 2018-08-05T18:27:09.141987: step 3984, loss 0.591835.
Train: 2018-08-05T18:27:09.345096: step 3985, loss 0.544585.
Train: 2018-08-05T18:27:09.563764: step 3986, loss 0.525607.
Train: 2018-08-05T18:27:09.782494: step 3987, loss 0.554132.
Train: 2018-08-05T18:27:10.001191: step 3988, loss 0.525568.
Train: 2018-08-05T18:27:10.219889: step 3989, loss 0.573261.
Train: 2018-08-05T18:27:10.422938: step 3990, loss 0.582773.
Test: 2018-08-05T18:27:11.485218: step 3990, loss 0.545554.
Train: 2018-08-05T18:27:11.703919: step 3991, loss 0.601641.
Train: 2018-08-05T18:27:11.906996: step 3992, loss 0.554012.
Train: 2018-08-05T18:27:12.141315: step 3993, loss 0.535152.
Train: 2018-08-05T18:27:12.360014: step 3994, loss 0.535188.
Train: 2018-08-05T18:27:12.563061: step 3995, loss 0.572365.
Train: 2018-08-05T18:27:12.781789: step 3996, loss 0.618342.
Train: 2018-08-05T18:27:13.016125: step 3997, loss 0.617654.
Train: 2018-08-05T18:27:13.219187: step 3998, loss 0.490425.
Train: 2018-08-05T18:27:13.437856: step 3999, loss 0.571497.
Train: 2018-08-05T18:27:13.656556: step 4000, loss 0.562467.
Test: 2018-08-05T18:27:14.718806: step 4000, loss 0.547503.
Train: 2018-08-05T18:27:15.702982: step 4001, loss 0.509581.
Train: 2018-08-05T18:27:15.921650: step 4002, loss 0.483515.
Train: 2018-08-05T18:27:16.140379: step 4003, loss 0.465991.
Train: 2018-08-05T18:27:16.343457: step 4004, loss 0.580031.
Train: 2018-08-05T18:27:16.562157: step 4005, loss 0.474057.
Train: 2018-08-05T18:27:16.780853: step 4006, loss 0.509065.
Train: 2018-08-05T18:27:16.999552: step 4007, loss 0.580559.
Train: 2018-08-05T18:27:17.218221: step 4008, loss 0.553615.
Train: 2018-08-05T18:27:17.436920: step 4009, loss 0.65372.
Train: 2018-08-05T18:27:17.655648: step 4010, loss 0.589996.
Test: 2018-08-05T18:27:18.702248: step 4010, loss 0.547967.
Train: 2018-08-05T18:27:18.920977: step 4011, loss 0.535499.
Train: 2018-08-05T18:27:19.139646: step 4012, loss 0.526487.
Train: 2018-08-05T18:27:19.358375: step 4013, loss 0.526503.
Train: 2018-08-05T18:27:19.577075: step 4014, loss 0.580741.
Train: 2018-08-05T18:27:19.795772: step 4015, loss 0.580705.
Train: 2018-08-05T18:27:20.014471: step 4016, loss 0.535589.
Train: 2018-08-05T18:27:20.233170: step 4017, loss 0.535617.
Train: 2018-08-05T18:27:20.436249: step 4018, loss 0.571563.
Train: 2018-08-05T18:27:20.670568: step 4019, loss 0.517725.
Train: 2018-08-05T18:27:20.889261: step 4020, loss 0.571531.
Test: 2018-08-05T18:27:21.935896: step 4020, loss 0.547588.
Train: 2018-08-05T18:27:22.154595: step 4021, loss 0.580472.
Train: 2018-08-05T18:27:22.373293: step 4022, loss 0.589337.
Train: 2018-08-05T18:27:22.591992: step 4023, loss 0.517999.
Train: 2018-08-05T18:27:22.810691: step 4024, loss 0.589107.
Train: 2018-08-05T18:27:23.029390: step 4025, loss 0.52706.
Train: 2018-08-05T18:27:23.248090: step 4026, loss 0.615395.
Train: 2018-08-05T18:27:23.466788: step 4027, loss 0.53605.
Train: 2018-08-05T18:27:23.685487: step 4028, loss 0.536127.
Train: 2018-08-05T18:27:23.904185: step 4029, loss 0.640963.
Train: 2018-08-05T18:27:24.107233: step 4030, loss 0.58837.
Test: 2018-08-05T18:27:25.169513: step 4030, loss 0.548358.
Train: 2018-08-05T18:27:25.388215: step 4031, loss 0.562336.
Train: 2018-08-05T18:27:25.591260: step 4032, loss 0.528157.
Train: 2018-08-05T18:27:25.809989: step 4033, loss 0.5113.
Train: 2018-08-05T18:27:26.028688: step 4034, loss 0.562345.
Train: 2018-08-05T18:27:26.247356: step 4035, loss 0.553843.
Train: 2018-08-05T18:27:26.466088: step 4036, loss 0.5709.
Train: 2018-08-05T18:27:26.684783: step 4037, loss 0.528275.
Train: 2018-08-05T18:27:26.903482: step 4038, loss 0.571068.
Train: 2018-08-05T18:27:27.122152: step 4039, loss 0.614373.
Train: 2018-08-05T18:27:27.340880: step 4040, loss 0.570833.
Test: 2018-08-05T18:27:28.403132: step 4040, loss 0.547651.
Train: 2018-08-05T18:27:28.621830: step 4041, loss 0.553902.
Train: 2018-08-05T18:27:28.840529: step 4042, loss 0.553926.
Train: 2018-08-05T18:27:29.059197: step 4043, loss 0.612957.
Train: 2018-08-05T18:27:29.262309: step 4044, loss 0.520397.
Train: 2018-08-05T18:27:29.496627: step 4045, loss 0.537215.
Train: 2018-08-05T18:27:29.699705: step 4046, loss 0.621195.
Train: 2018-08-05T18:27:29.918402: step 4047, loss 0.570779.
Train: 2018-08-05T18:27:30.183964: step 4048, loss 0.579133.
Train: 2018-08-05T18:27:30.402664: step 4049, loss 0.529079.
Train: 2018-08-05T18:27:30.621333: step 4050, loss 0.637447.
Test: 2018-08-05T18:27:31.683583: step 4050, loss 0.548603.
Train: 2018-08-05T18:27:31.886661: step 4051, loss 0.520972.
Train: 2018-08-05T18:27:32.105360: step 4052, loss 0.512737.
Train: 2018-08-05T18:27:32.324090: step 4053, loss 0.554141.
Train: 2018-08-05T18:27:32.542757: step 4054, loss 0.629111.
Train: 2018-08-05T18:27:32.745834: step 4055, loss 0.554109.
Train: 2018-08-05T18:27:32.964564: step 4056, loss 0.570763.
Train: 2018-08-05T18:27:33.183262: step 4057, loss 0.554112.
Train: 2018-08-05T18:27:33.386340: step 4058, loss 0.52911.
Train: 2018-08-05T18:27:33.605038: step 4059, loss 0.654329.
Train: 2018-08-05T18:27:33.823739: step 4060, loss 0.529096.
Test: 2018-08-05T18:27:34.885958: step 4060, loss 0.547746.
Train: 2018-08-05T18:27:35.104688: step 4061, loss 0.554097.
Train: 2018-08-05T18:27:35.307764: step 4062, loss 0.645831.
Train: 2018-08-05T18:27:35.526464: step 4063, loss 0.603987.
Train: 2018-08-05T18:27:35.745162: step 4064, loss 0.545981.
Train: 2018-08-05T18:27:35.948241: step 4065, loss 0.570758.
Train: 2018-08-05T18:27:36.166938: step 4066, loss 0.595358.
Train: 2018-08-05T18:27:36.385637: step 4067, loss 0.546289.
Train: 2018-08-05T18:27:36.588714: step 4068, loss 0.652182.
Train: 2018-08-05T18:27:36.807384: step 4069, loss 0.578888.
Train: 2018-08-05T18:27:37.026114: step 4070, loss 0.610972.
Test: 2018-08-05T18:27:38.072744: step 4070, loss 0.549285.
Train: 2018-08-05T18:27:38.291411: step 4071, loss 0.578859.
Train: 2018-08-05T18:27:38.510140: step 4072, loss 0.460409.
Train: 2018-08-05T18:27:38.728839: step 4073, loss 0.531445.
Train: 2018-08-05T18:27:38.947539: step 4074, loss 0.562994.
Train: 2018-08-05T18:27:39.166236: step 4075, loss 0.523094.
Train: 2018-08-05T18:27:39.384935: step 4076, loss 0.474527.
Train: 2018-08-05T18:27:39.556740: step 4077, loss 0.668314.
Train: 2018-08-05T18:27:39.775439: step 4078, loss 0.562593.
Train: 2018-08-05T18:27:39.994145: step 4079, loss 0.578979.
Train: 2018-08-05T18:27:40.212835: step 4080, loss 0.554243.
Test: 2018-08-05T18:27:41.259466: step 4080, loss 0.550132.
Train: 2018-08-05T18:27:41.493817: step 4081, loss 0.554164.
Train: 2018-08-05T18:27:41.712515: step 4082, loss 0.58745.
Train: 2018-08-05T18:27:41.931213: step 4083, loss 0.537289.
Train: 2018-08-05T18:27:42.134294: step 4084, loss 0.553965.
Train: 2018-08-05T18:27:42.352960: step 4085, loss 0.621562.
Train: 2018-08-05T18:27:42.571690: step 4086, loss 0.562355.
Train: 2018-08-05T18:27:42.790357: step 4087, loss 0.519959.
Train: 2018-08-05T18:27:42.993465: step 4088, loss 0.536812.
Train: 2018-08-05T18:27:43.212133: step 4089, loss 0.536678.
Train: 2018-08-05T18:27:43.446486: step 4090, loss 0.493505.
Test: 2018-08-05T18:27:44.493114: step 4090, loss 0.546448.
Train: 2018-08-05T18:27:44.711783: step 4091, loss 0.48419.
Train: 2018-08-05T18:27:44.930511: step 4092, loss 0.544813.
Train: 2018-08-05T18:27:45.133559: step 4093, loss 0.607011.
Train: 2018-08-05T18:27:45.367909: step 4094, loss 0.553593.
Train: 2018-08-05T18:27:45.586578: step 4095, loss 0.553609.
Train: 2018-08-05T18:27:45.805306: step 4096, loss 0.617205.
Train: 2018-08-05T18:27:46.008384: step 4097, loss 0.553638.
Train: 2018-08-05T18:27:46.227052: step 4098, loss 0.662849.
Train: 2018-08-05T18:27:46.445783: step 4099, loss 0.58076.
Train: 2018-08-05T18:27:46.648829: step 4100, loss 0.62545.
Test: 2018-08-05T18:27:47.711080: step 4100, loss 0.547402.
Train: 2018-08-05T18:27:48.726497: step 4101, loss 0.483905.
Train: 2018-08-05T18:27:48.945190: step 4102, loss 0.553595.
Train: 2018-08-05T18:27:49.161785: step 4103, loss 0.606415.
Train: 2018-08-05T18:27:49.380484: step 4104, loss 0.57111.
Train: 2018-08-05T18:27:49.599213: step 4105, loss 0.536326.
Train: 2018-08-05T18:27:49.817912: step 4106, loss 0.545083.
Train: 2018-08-05T18:27:50.020989: step 4107, loss 0.527959.
Train: 2018-08-05T18:27:50.239658: step 4108, loss 0.528013.
Train: 2018-08-05T18:27:50.473979: step 4109, loss 0.57092.
Train: 2018-08-05T18:27:50.677086: step 4110, loss 0.545166.
Test: 2018-08-05T18:27:51.723686: step 4110, loss 0.547787.
Train: 2018-08-05T18:27:51.942414: step 4111, loss 0.527966.
Train: 2018-08-05T18:27:52.161083: step 4112, loss 0.562337.
Train: 2018-08-05T18:27:52.364190: step 4113, loss 0.484611.
Train: 2018-08-05T18:27:52.582889: step 4114, loss 0.614513.
Train: 2018-08-05T18:27:52.801590: step 4115, loss 0.562362.
Train: 2018-08-05T18:27:53.004665: step 4116, loss 0.571104.
Train: 2018-08-05T18:27:53.223364: step 4117, loss 0.571117.
Train: 2018-08-05T18:27:53.442032: step 4118, loss 0.562375.
Train: 2018-08-05T18:27:53.676384: step 4119, loss 0.614825.
Train: 2018-08-05T18:27:53.895084: step 4120, loss 0.510096.
Test: 2018-08-05T18:27:54.941712: step 4120, loss 0.547817.
Train: 2018-08-05T18:27:55.160412: step 4121, loss 0.588479.
Train: 2018-08-05T18:27:55.379109: step 4122, loss 0.510224.
Train: 2018-08-05T18:27:55.582156: step 4123, loss 0.588441.
Train: 2018-08-05T18:27:55.800885: step 4124, loss 0.544978.
Train: 2018-08-05T18:27:56.019586: step 4125, loss 0.527607.
Train: 2018-08-05T18:27:56.222661: step 4126, loss 0.605863.
Train: 2018-08-05T18:27:56.441360: step 4127, loss 0.571043.
Train: 2018-08-05T18:27:56.660031: step 4128, loss 0.579696.
Train: 2018-08-05T18:27:56.863107: step 4129, loss 0.527747.
Train: 2018-08-05T18:27:57.081806: step 4130, loss 0.596909.
Test: 2018-08-05T18:27:58.128436: step 4130, loss 0.547747.
Train: 2018-08-05T18:27:58.347135: step 4131, loss 0.588189.
Train: 2018-08-05T18:27:58.550242: step 4132, loss 0.536593.
Train: 2018-08-05T18:27:58.768940: step 4133, loss 0.502414.
Train: 2018-08-05T18:27:58.972018: step 4134, loss 0.502328.
Train: 2018-08-05T18:27:59.206342: step 4135, loss 0.708772.
Train: 2018-08-05T18:27:59.409417: step 4136, loss 0.519458.
Train: 2018-08-05T18:27:59.628114: step 4137, loss 0.519525.
Train: 2018-08-05T18:27:59.846813: step 4138, loss 0.485189.
Train: 2018-08-05T18:28:00.049890: step 4139, loss 0.622675.
Train: 2018-08-05T18:28:00.268589: step 4140, loss 0.501924.
Test: 2018-08-05T18:28:01.315219: step 4140, loss 0.547468.
Train: 2018-08-05T18:28:01.533920: step 4141, loss 0.562347.
Train: 2018-08-05T18:28:01.736995: step 4142, loss 0.536245.
Train: 2018-08-05T18:28:01.955694: step 4143, loss 0.588614.
Train: 2018-08-05T18:28:02.158741: step 4144, loss 0.571155.
Train: 2018-08-05T18:28:02.361849: step 4145, loss 0.606291.
Train: 2018-08-05T18:28:02.564928: step 4146, loss 0.588678.
Train: 2018-08-05T18:28:02.783627: step 4147, loss 0.509964.
Train: 2018-08-05T18:28:02.986702: step 4148, loss 0.562369.
Train: 2018-08-05T18:28:03.189781: step 4149, loss 0.544914.
Train: 2018-08-05T18:28:03.408450: step 4150, loss 0.509998.
Test: 2018-08-05T18:28:04.455109: step 4150, loss 0.545946.
Train: 2018-08-05T18:28:04.658156: step 4151, loss 0.544872.
Train: 2018-08-05T18:28:04.876885: step 4152, loss 0.579963.
Train: 2018-08-05T18:28:05.079963: step 4153, loss 0.58.
Train: 2018-08-05T18:28:05.298658: step 4154, loss 0.615191.
Train: 2018-08-05T18:28:05.501739: step 4155, loss 0.544852.
Train: 2018-08-05T18:28:05.704816: step 4156, loss 0.536138.
Train: 2018-08-05T18:28:05.907863: step 4157, loss 0.53616.
Train: 2018-08-05T18:28:06.110971: step 4158, loss 0.606075.
Train: 2018-08-05T18:28:06.314049: step 4159, loss 0.579799.
Train: 2018-08-05T18:28:06.532749: step 4160, loss 0.544979.
Test: 2018-08-05T18:28:07.579347: step 4160, loss 0.547475.
Train: 2018-08-05T18:28:07.782425: step 4161, loss 0.648995.
Train: 2018-08-05T18:28:08.001125: step 4162, loss 0.562335.
Train: 2018-08-05T18:28:08.204201: step 4163, loss 0.553797.
Train: 2018-08-05T18:28:08.407279: step 4164, loss 0.553856.
Train: 2018-08-05T18:28:08.610386: step 4165, loss 0.655347.
Train: 2018-08-05T18:28:08.813433: step 4166, loss 0.520542.
Train: 2018-08-05T18:28:09.016540: step 4167, loss 0.562439.
Train: 2018-08-05T18:28:09.219618: step 4168, loss 0.512781.
Train: 2018-08-05T18:28:09.422696: step 4169, loss 0.587305.
Train: 2018-08-05T18:28:09.625773: step 4170, loss 0.636817.
Test: 2018-08-05T18:28:10.672373: step 4170, loss 0.549599.
Train: 2018-08-05T18:28:10.875480: step 4171, loss 0.636429.
Train: 2018-08-05T18:28:11.078558: step 4172, loss 0.578914.
Train: 2018-08-05T18:28:11.281637: step 4173, loss 0.57082.
Train: 2018-08-05T18:28:11.484713: step 4174, loss 0.586859.
Train: 2018-08-05T18:28:11.687790: step 4175, loss 0.539183.
Train: 2018-08-05T18:28:11.890837: step 4176, loss 0.539363.
Train: 2018-08-05T18:28:12.093945: step 4177, loss 0.563087.
Train: 2018-08-05T18:28:12.297022: step 4178, loss 0.610409.
Train: 2018-08-05T18:28:12.484478: step 4179, loss 0.570999.
Train: 2018-08-05T18:28:12.687557: step 4180, loss 0.516015.
Test: 2018-08-05T18:28:13.749778: step 4180, loss 0.55031.
Train: 2018-08-05T18:28:13.937264: step 4181, loss 0.594624.
Train: 2018-08-05T18:28:14.140340: step 4182, loss 0.602539.
Train: 2018-08-05T18:28:14.343419: step 4183, loss 0.570971.
Train: 2018-08-05T18:28:14.546496: step 4184, loss 0.523597.
Train: 2018-08-05T18:28:14.749573: step 4185, loss 0.6502.
Train: 2018-08-05T18:28:14.937030: step 4186, loss 0.594697.
Train: 2018-08-05T18:28:15.140106: step 4187, loss 0.578861.
Train: 2018-08-05T18:28:15.343184: step 4188, loss 0.499976.
Train: 2018-08-05T18:28:15.546261: step 4189, loss 0.55511.
Train: 2018-08-05T18:28:15.733718: step 4190, loss 0.578859.
Test: 2018-08-05T18:28:16.795939: step 4190, loss 0.549365.
Train: 2018-08-05T18:28:16.983426: step 4191, loss 0.546914.
Train: 2018-08-05T18:28:17.186502: step 4192, loss 0.578871.
Train: 2018-08-05T18:28:17.373959: step 4193, loss 0.514318.
Train: 2018-08-05T18:28:17.577036: step 4194, loss 0.505685.
Train: 2018-08-05T18:28:17.764494: step 4195, loss 0.620145.
Train: 2018-08-05T18:28:17.967569: step 4196, loss 0.554176.
Train: 2018-08-05T18:28:18.170647: step 4197, loss 0.554064.
Train: 2018-08-05T18:28:18.358104: step 4198, loss 0.562378.
Train: 2018-08-05T18:28:18.561181: step 4199, loss 0.5793.
Train: 2018-08-05T18:28:18.748607: step 4200, loss 0.630484.
Test: 2018-08-05T18:28:19.795237: step 4200, loss 0.547517.
Train: 2018-08-05T18:28:20.795032: step 4201, loss 0.519723.
Train: 2018-08-05T18:28:20.982488: step 4202, loss 0.630742.
Train: 2018-08-05T18:28:21.185579: step 4203, loss 0.519638.
Train: 2018-08-05T18:28:21.373022: step 4204, loss 0.570889.
Train: 2018-08-05T18:28:21.560479: step 4205, loss 0.553777.
Train: 2018-08-05T18:28:21.763558: step 4206, loss 0.588042.
Train: 2018-08-05T18:28:21.951012: step 4207, loss 0.562335.
Train: 2018-08-05T18:28:22.138438: step 4208, loss 0.528096.
Train: 2018-08-05T18:28:22.341545: step 4209, loss 0.519469.
Train: 2018-08-05T18:28:22.529003: step 4210, loss 0.596763.
Test: 2018-08-05T18:28:23.591223: step 4210, loss 0.548538.
Train: 2018-08-05T18:28:23.778710: step 4211, loss 0.545099.
Train: 2018-08-05T18:28:23.966165: step 4212, loss 0.527791.
Train: 2018-08-05T18:28:24.153622: step 4213, loss 0.579687.
Train: 2018-08-05T18:28:24.341077: step 4214, loss 0.544974.
Train: 2018-08-05T18:28:24.544155: step 4215, loss 0.544934.
Train: 2018-08-05T18:28:24.731581: step 4216, loss 0.579857.
Train: 2018-08-05T18:28:24.919066: step 4217, loss 0.48358.
Train: 2018-08-05T18:28:25.106523: step 4218, loss 0.571217.
Train: 2018-08-05T18:28:25.293979: step 4219, loss 0.580124.
Train: 2018-08-05T18:28:25.481435: step 4220, loss 0.597906.
Test: 2018-08-05T18:28:26.543686: step 4220, loss 0.548472.
Train: 2018-08-05T18:28:26.731143: step 4221, loss 0.535876.
Train: 2018-08-05T18:28:26.918598: step 4222, loss 0.491549.
Train: 2018-08-05T18:28:27.106054: step 4223, loss 0.553588.
Train: 2018-08-05T18:28:27.293506: step 4224, loss 0.535713.
Train: 2018-08-05T18:28:27.480967: step 4225, loss 0.472422.
Train: 2018-08-05T18:28:27.668394: step 4226, loss 0.468291.
Train: 2018-08-05T18:28:27.855881: step 4227, loss 0.669715.
Train: 2018-08-05T18:28:27.996441: step 4228, loss 0.673401.
Train: 2018-08-05T18:28:28.183928: step 4229, loss 0.572373.
Train: 2018-08-05T18:28:28.371383: step 4230, loss 0.562606.
Test: 2018-08-05T18:28:29.417982: step 4230, loss 0.548851.
Train: 2018-08-05T18:28:29.621091: step 4231, loss 0.562493.
Train: 2018-08-05T18:28:29.792925: step 4232, loss 0.614587.
Train: 2018-08-05T18:28:29.980381: step 4233, loss 0.570827.
Train: 2018-08-05T18:28:30.167838: step 4234, loss 0.571787.
Train: 2018-08-05T18:28:30.339673: step 4235, loss 0.509175.
Train: 2018-08-05T18:28:30.527130: step 4236, loss 0.545653.
Train: 2018-08-05T18:28:30.714585: step 4237, loss 0.536482.
Train: 2018-08-05T18:28:30.886422: step 4238, loss 0.631241.
Train: 2018-08-05T18:28:31.073875: step 4239, loss 0.562335.
Train: 2018-08-05T18:28:31.261301: step 4240, loss 0.510986.
Test: 2018-08-05T18:28:32.307932: step 4240, loss 0.549811.
Train: 2018-08-05T18:28:32.495417: step 4241, loss 0.536652.
Train: 2018-08-05T18:28:32.682875: step 4242, loss 0.596728.
Train: 2018-08-05T18:28:32.870331: step 4243, loss 0.553764.
Train: 2018-08-05T18:28:33.042164: step 4244, loss 0.553765.
Train: 2018-08-05T18:28:33.229621: step 4245, loss 0.579484.
Train: 2018-08-05T18:28:33.401457: step 4246, loss 0.58804.
Train: 2018-08-05T18:28:33.588913: step 4247, loss 0.536688.
Train: 2018-08-05T18:28:33.760717: step 4248, loss 0.630702.
Train: 2018-08-05T18:28:33.948203: step 4249, loss 0.519808.
Train: 2018-08-05T18:28:34.120037: step 4250, loss 0.587831.
Test: 2018-08-05T18:28:35.182257: step 4250, loss 0.548791.
Train: 2018-08-05T18:28:35.354123: step 4251, loss 0.587768.
Train: 2018-08-05T18:28:35.525961: step 4252, loss 0.612995.
Train: 2018-08-05T18:28:35.713414: step 4253, loss 0.50371.
Train: 2018-08-05T18:28:35.885248: step 4254, loss 0.528938.
Train: 2018-08-05T18:28:36.057083: step 4255, loss 0.520546.
Train: 2018-08-05T18:28:36.228919: step 4256, loss 0.621181.
Train: 2018-08-05T18:28:36.416392: step 4257, loss 0.545597.
Train: 2018-08-05T18:28:36.588210: step 4258, loss 0.520367.
Train: 2018-08-05T18:28:36.760014: step 4259, loss 0.503334.
Train: 2018-08-05T18:28:36.947470: step 4260, loss 0.53687.
Test: 2018-08-05T18:28:37.994130: step 4260, loss 0.548829.
Train: 2018-08-05T18:28:38.165934: step 4261, loss 0.596573.
Train: 2018-08-05T18:28:38.337799: step 4262, loss 0.553734.
Train: 2018-08-05T18:28:38.509634: step 4263, loss 0.570985.
Train: 2018-08-05T18:28:38.697090: step 4264, loss 0.518967.
Train: 2018-08-05T18:28:38.868925: step 4265, loss 0.571092.
Train: 2018-08-05T18:28:39.040760: step 4266, loss 0.571148.
Train: 2018-08-05T18:28:39.212594: step 4267, loss 0.536032.
Train: 2018-08-05T18:28:39.384429: step 4268, loss 0.571247.
Train: 2018-08-05T18:28:39.556265: step 4269, loss 0.597822.
Train: 2018-08-05T18:28:39.728099: step 4270, loss 0.580124.
Test: 2018-08-05T18:28:40.790320: step 4270, loss 0.546871.
Train: 2018-08-05T18:28:40.962185: step 4271, loss 0.580082.
Train: 2018-08-05T18:28:41.134019: step 4272, loss 0.509603.
Train: 2018-08-05T18:28:41.305824: step 4273, loss 0.544807.
Train: 2018-08-05T18:28:41.462067: step 4274, loss 0.553605.
Train: 2018-08-05T18:28:41.633902: step 4275, loss 0.474312.
Train: 2018-08-05T18:28:41.821328: step 4276, loss 0.580162.
Train: 2018-08-05T18:28:41.993193: step 4277, loss 0.51805.
Train: 2018-08-05T18:28:42.165028: step 4278, loss 0.553589.
Train: 2018-08-05T18:28:42.336863: step 4279, loss 0.571526.
Train: 2018-08-05T18:28:42.508700: step 4280, loss 0.589562.
Test: 2018-08-05T18:28:43.570919: step 4280, loss 0.546537.
Train: 2018-08-05T18:28:43.742784: step 4281, loss 0.562591.
Train: 2018-08-05T18:28:43.930210: step 4282, loss 0.58056.
Train: 2018-08-05T18:28:44.102046: step 4283, loss 0.517722.
Train: 2018-08-05T18:28:44.273911: step 4284, loss 0.56256.
Train: 2018-08-05T18:28:44.445744: step 4285, loss 0.544631.
Train: 2018-08-05T18:28:44.617579: step 4286, loss 0.580464.
Train: 2018-08-05T18:28:44.789414: step 4287, loss 0.58935.
Train: 2018-08-05T18:28:44.961248: step 4288, loss 0.598113.
Train: 2018-08-05T18:28:45.133081: step 4289, loss 0.500494.
Train: 2018-08-05T18:28:45.304918: step 4290, loss 0.4918.
Test: 2018-08-05T18:28:46.367140: step 4290, loss 0.548078.
Train: 2018-08-05T18:28:46.538975: step 4291, loss 0.597802.
Train: 2018-08-05T18:28:46.710838: step 4292, loss 0.518276.
Train: 2018-08-05T18:28:46.882673: step 4293, loss 0.474041.
Train: 2018-08-05T18:28:47.054479: step 4294, loss 0.553589.
Train: 2018-08-05T18:28:47.226343: step 4295, loss 0.63399.
Train: 2018-08-05T18:28:47.398148: step 4296, loss 0.535726.
Train: 2018-08-05T18:28:47.570013: step 4297, loss 0.571464.
Train: 2018-08-05T18:28:47.741847: step 4298, loss 0.491057.
Train: 2018-08-05T18:28:47.913682: step 4299, loss 0.553592.
Train: 2018-08-05T18:28:48.085487: step 4300, loss 0.499693.
Test: 2018-08-05T18:28:49.132117: step 4300, loss 0.546519.
Train: 2018-08-05T18:28:50.085048: step 4301, loss 0.580703.
Train: 2018-08-05T18:28:50.256854: step 4302, loss 0.526449.
Train: 2018-08-05T18:28:50.428689: step 4303, loss 0.56273.
Train: 2018-08-05T18:28:50.600553: step 4304, loss 0.590117.
Train: 2018-08-05T18:28:50.772389: step 4305, loss 0.590114.
Train: 2018-08-05T18:28:50.944193: step 4306, loss 0.608195.
Train: 2018-08-05T18:28:51.116027: step 4307, loss 0.517448.
Train: 2018-08-05T18:28:51.287891: step 4308, loss 0.571625.
Train: 2018-08-05T18:28:51.459727: step 4309, loss 0.463869.
Train: 2018-08-05T18:28:51.631531: step 4310, loss 0.589527.
Test: 2018-08-05T18:28:52.678162: step 4310, loss 0.547995.
Train: 2018-08-05T18:28:52.849996: step 4311, loss 0.544622.
Train: 2018-08-05T18:28:53.021862: step 4312, loss 0.625315.
Train: 2018-08-05T18:28:53.193695: step 4313, loss 0.589273.
Train: 2018-08-05T18:28:53.365500: step 4314, loss 0.58018.
Train: 2018-08-05T18:28:53.537365: step 4315, loss 0.597591.
Train: 2018-08-05T18:28:53.709200: step 4316, loss 0.623371.
Train: 2018-08-05T18:28:53.881036: step 4317, loss 0.519288.
Train: 2018-08-05T18:28:54.037248: step 4318, loss 0.553803.
Train: 2018-08-05T18:28:54.209054: step 4319, loss 0.604723.
Train: 2018-08-05T18:28:54.380917: step 4320, loss 0.503603.
Test: 2018-08-05T18:28:55.443146: step 4320, loss 0.548253.
Train: 2018-08-05T18:28:55.599382: step 4321, loss 0.56241.
Train: 2018-08-05T18:28:55.771187: step 4322, loss 0.545769.
Train: 2018-08-05T18:28:55.943052: step 4323, loss 0.495918.
Train: 2018-08-05T18:28:56.114857: step 4324, loss 0.562427.
Train: 2018-08-05T18:28:56.271102: step 4325, loss 0.587497.
Train: 2018-08-05T18:28:56.442935: step 4326, loss 0.637761.
Train: 2018-08-05T18:28:56.630388: step 4327, loss 0.554075.
Train: 2018-08-05T18:28:56.802225: step 4328, loss 0.529119.
Train: 2018-08-05T18:28:56.974055: step 4329, loss 0.554098.
Train: 2018-08-05T18:28:57.145896: step 4330, loss 0.520701.
Test: 2018-08-05T18:28:58.192495: step 4330, loss 0.546093.
Train: 2018-08-05T18:28:58.364360: step 4331, loss 0.579156.
Train: 2018-08-05T18:28:58.536195: step 4332, loss 0.553982.
Train: 2018-08-05T18:28:58.708030: step 4333, loss 0.579232.
Train: 2018-08-05T18:28:58.879866: step 4334, loss 0.604607.
Train: 2018-08-05T18:28:59.051694: step 4335, loss 0.621477.
Train: 2018-08-05T18:28:59.223533: step 4336, loss 0.579203.
Train: 2018-08-05T18:28:59.395338: step 4337, loss 0.528901.
Train: 2018-08-05T18:28:59.567203: step 4338, loss 0.554049.
Train: 2018-08-05T18:28:59.739009: step 4339, loss 0.562414.
Train: 2018-08-05T18:28:59.910873: step 4340, loss 0.57077.
Test: 2018-08-05T18:29:00.973094: step 4340, loss 0.550011.
Train: 2018-08-05T18:29:01.129339: step 4341, loss 0.537368.
Train: 2018-08-05T18:29:01.316793: step 4342, loss 0.56241.
Train: 2018-08-05T18:29:01.488628: step 4343, loss 0.587528.
Train: 2018-08-05T18:29:01.660463: step 4344, loss 0.587529.
Train: 2018-08-05T18:29:01.816676: step 4345, loss 0.562413.
Train: 2018-08-05T18:29:01.988511: step 4346, loss 0.562417.
Train: 2018-08-05T18:29:02.160316: step 4347, loss 0.545699.
Train: 2018-08-05T18:29:02.332181: step 4348, loss 0.620967.
Train: 2018-08-05T18:29:02.503986: step 4349, loss 0.478972.
Train: 2018-08-05T18:29:02.675851: step 4350, loss 0.554031.
Test: 2018-08-05T18:29:03.722480: step 4350, loss 0.547382.
Train: 2018-08-05T18:29:03.894284: step 4351, loss 0.562384.
Train: 2018-08-05T18:29:04.066149: step 4352, loss 0.545502.
Train: 2018-08-05T18:29:04.237955: step 4353, loss 0.60471.
Train: 2018-08-05T18:29:04.409819: step 4354, loss 0.56235.
Train: 2018-08-05T18:29:04.566003: step 4355, loss 0.638811.
Train: 2018-08-05T18:29:04.737867: step 4356, loss 0.545419.
Train: 2018-08-05T18:29:04.909672: step 4357, loss 0.537003.
Train: 2018-08-05T18:29:05.081538: step 4358, loss 0.562361.
Train: 2018-08-05T18:29:05.253371: step 4359, loss 0.520087.
Train: 2018-08-05T18:29:05.425176: step 4360, loss 0.638665.
Test: 2018-08-05T18:29:06.471836: step 4360, loss 0.548419.
Train: 2018-08-05T18:29:06.643672: step 4361, loss 0.545432.
Train: 2018-08-05T18:29:06.815507: step 4362, loss 0.562359.
Train: 2018-08-05T18:29:06.987340: step 4363, loss 0.596177.
Train: 2018-08-05T18:29:07.159175: step 4364, loss 0.52863.
Train: 2018-08-05T18:29:07.330979: step 4365, loss 0.612985.
Train: 2018-08-05T18:29:07.487224: step 4366, loss 0.553967.
Train: 2018-08-05T18:29:07.659058: step 4367, loss 0.595978.
Train: 2018-08-05T18:29:07.830893: step 4368, loss 0.646092.
Train: 2018-08-05T18:29:08.002730: step 4369, loss 0.529251.
Train: 2018-08-05T18:29:08.174563: step 4370, loss 0.521185.
Test: 2018-08-05T18:29:09.221161: step 4370, loss 0.549294.
Train: 2018-08-05T18:29:09.393028: step 4371, loss 0.570756.
Train: 2018-08-05T18:29:09.564862: step 4372, loss 0.628447.
Train: 2018-08-05T18:29:09.736696: step 4373, loss 0.578964.
Train: 2018-08-05T18:29:09.908532: step 4374, loss 0.513628.
Train: 2018-08-05T18:29:10.080366: step 4375, loss 0.521822.
Train: 2018-08-05T18:29:10.252202: step 4376, loss 0.668931.
Train: 2018-08-05T18:29:10.424036: step 4377, loss 0.546326.
Train: 2018-08-05T18:29:10.595870: step 4378, loss 0.497556.
Train: 2018-08-05T18:29:10.736462: step 4379, loss 0.632262.
Train: 2018-08-05T18:29:10.908268: step 4380, loss 0.538152.
Test: 2018-08-05T18:29:11.954898: step 4380, loss 0.548777.
Train: 2018-08-05T18:29:12.126732: step 4381, loss 0.57077.
Train: 2018-08-05T18:29:12.298597: step 4382, loss 0.505347.
Train: 2018-08-05T18:29:12.470432: step 4383, loss 0.570759.
Train: 2018-08-05T18:29:12.642267: step 4384, loss 0.53771.
Train: 2018-08-05T18:29:12.798480: step 4385, loss 0.570761.
Train: 2018-08-05T18:29:12.970315: step 4386, loss 0.52061.
Train: 2018-08-05T18:29:13.142150: step 4387, loss 0.587651.
Train: 2018-08-05T18:29:13.313984: step 4388, loss 0.536931.
Train: 2018-08-05T18:29:13.485819: step 4389, loss 0.630603.
Train: 2018-08-05T18:29:13.657653: step 4390, loss 0.553789.
Test: 2018-08-05T18:29:14.704254: step 4390, loss 0.547633.
Train: 2018-08-05T18:29:14.876118: step 4391, loss 0.588036.
Train: 2018-08-05T18:29:15.047953: step 4392, loss 0.588047.
Train: 2018-08-05T18:29:15.219788: step 4393, loss 0.519535.
Train: 2018-08-05T18:29:15.391624: step 4394, loss 0.553762.
Train: 2018-08-05T18:29:15.547836: step 4395, loss 0.622451.
Train: 2018-08-05T18:29:15.719671: step 4396, loss 0.570905.
Train: 2018-08-05T18:29:15.891507: step 4397, loss 0.622179.
Train: 2018-08-05T18:29:16.063335: step 4398, loss 0.519851.
Train: 2018-08-05T18:29:16.235175: step 4399, loss 0.562352.
Train: 2018-08-05T18:29:16.407011: step 4400, loss 0.486241.
Test: 2018-08-05T18:29:17.453639: step 4400, loss 0.548187.
Train: 2018-08-05T18:29:18.453405: step 4401, loss 0.519946.
Train: 2018-08-05T18:29:18.625239: step 4402, loss 0.570865.
Train: 2018-08-05T18:29:18.797075: step 4403, loss 0.579456.
Train: 2018-08-05T18:29:18.953289: step 4404, loss 0.562335.
Train: 2018-08-05T18:29:19.140714: step 4405, loss 0.60535.
Train: 2018-08-05T18:29:19.312580: step 4406, loss 0.553742.
Train: 2018-08-05T18:29:19.468762: step 4407, loss 0.579529.
Train: 2018-08-05T18:29:19.640629: step 4408, loss 0.519404.
Train: 2018-08-05T18:29:19.812467: step 4409, loss 0.553737.
Train: 2018-08-05T18:29:19.984297: step 4410, loss 0.596789.
Test: 2018-08-05T18:29:21.030927: step 4410, loss 0.547564.
Train: 2018-08-05T18:29:21.218353: step 4411, loss 0.639803.
Train: 2018-08-05T18:29:21.390219: step 4412, loss 0.613704.
Train: 2018-08-05T18:29:21.562047: step 4413, loss 0.502895.
Train: 2018-08-05T18:29:21.733887: step 4414, loss 0.57082.
Train: 2018-08-05T18:29:21.905722: step 4415, loss 0.553936.
Train: 2018-08-05T18:29:22.077527: step 4416, loss 0.528731.
Train: 2018-08-05T18:29:22.233742: step 4417, loss 0.604441.
Train: 2018-08-05T18:29:22.405575: step 4418, loss 0.621137.
Train: 2018-08-05T18:29:22.577440: step 4419, loss 0.604151.
Train: 2018-08-05T18:29:22.749274: step 4420, loss 0.562472.
Test: 2018-08-05T18:29:23.811527: step 4420, loss 0.547444.
Train: 2018-08-05T18:29:23.983361: step 4421, loss 0.529585.
Train: 2018-08-05T18:29:24.155195: step 4422, loss 0.56255.
Train: 2018-08-05T18:29:24.327024: step 4423, loss 0.529803.
Train: 2018-08-05T18:29:24.498864: step 4424, loss 0.513378.
Train: 2018-08-05T18:29:24.670669: step 4425, loss 0.570757.
Train: 2018-08-05T18:29:24.842535: step 4426, loss 0.579021.
Train: 2018-08-05T18:29:25.014370: step 4427, loss 0.637064.
Train: 2018-08-05T18:29:25.186204: step 4428, loss 0.545934.
Train: 2018-08-05T18:29:25.358039: step 4429, loss 0.661767.
Train: 2018-08-05T18:29:25.529874: step 4430, loss 0.49673.
Test: 2018-08-05T18:29:26.592124: step 4430, loss 0.548432.
Train: 2018-08-05T18:29:26.748340: step 4431, loss 0.636521.
Train: 2018-08-05T18:29:26.920142: step 4432, loss 0.611675.
Train: 2018-08-05T18:29:27.107629: step 4433, loss 0.538279.
Train: 2018-08-05T18:29:27.279434: step 4434, loss 0.611275.
Train: 2018-08-05T18:29:27.451299: step 4435, loss 0.586923.
Train: 2018-08-05T18:29:27.623104: step 4436, loss 0.562866.
Train: 2018-08-05T18:29:27.794970: step 4437, loss 0.570897.
Train: 2018-08-05T18:29:27.966803: step 4438, loss 0.562995.
Train: 2018-08-05T18:29:28.138639: step 4439, loss 0.563034.
Train: 2018-08-05T18:29:28.310473: step 4440, loss 0.586765.
Test: 2018-08-05T18:29:29.372703: step 4440, loss 0.549534.
Train: 2018-08-05T18:29:29.544559: step 4441, loss 0.563078.
Train: 2018-08-05T18:29:29.716393: step 4442, loss 0.610423.
Train: 2018-08-05T18:29:29.888229: step 4443, loss 0.531639.
Train: 2018-08-05T18:29:30.060062: step 4444, loss 0.602505.
Train: 2018-08-05T18:29:30.278764: step 4445, loss 0.55524.
Train: 2018-08-05T18:29:30.450596: step 4446, loss 0.547328.
Train: 2018-08-05T18:29:30.622430: step 4447, loss 0.634221.
Train: 2018-08-05T18:29:30.794266: step 4448, loss 0.507744.
Train: 2018-08-05T18:29:30.966071: step 4449, loss 0.531261.
Train: 2018-08-05T18:29:31.153557: step 4450, loss 0.594831.
Test: 2018-08-05T18:29:32.200157: step 4450, loss 0.550716.
Train: 2018-08-05T18:29:32.372023: step 4451, loss 0.554799.
Train: 2018-08-05T18:29:32.559447: step 4452, loss 0.562746.
Train: 2018-08-05T18:29:32.731312: step 4453, loss 0.554559.
Train: 2018-08-05T18:29:32.903147: step 4454, loss 0.546268.
Train: 2018-08-05T18:29:33.074981: step 4455, loss 0.554302.
Train: 2018-08-05T18:29:33.246818: step 4456, loss 0.496135.
Train: 2018-08-05T18:29:33.434273: step 4457, loss 0.579168.
Train: 2018-08-05T18:29:33.606108: step 4458, loss 0.621618.
Train: 2018-08-05T18:29:33.777945: step 4459, loss 0.519812.
Train: 2018-08-05T18:29:33.949777: step 4460, loss 0.579472.
Test: 2018-08-05T18:29:35.011998: step 4460, loss 0.547554.
Train: 2018-08-05T18:29:35.183833: step 4461, loss 0.596792.
Train: 2018-08-05T18:29:35.371318: step 4462, loss 0.501889.
Train: 2018-08-05T18:29:35.558769: step 4463, loss 0.536294.
Train: 2018-08-05T18:29:35.730580: step 4464, loss 0.571116.
Train: 2018-08-05T18:29:35.902414: step 4465, loss 0.606329.
Train: 2018-08-05T18:29:36.074248: step 4466, loss 0.606404.
Train: 2018-08-05T18:29:36.246113: step 4467, loss 0.500904.
Train: 2018-08-05T18:29:36.433570: step 4468, loss 0.553607.
Train: 2018-08-05T18:29:36.605405: step 4469, loss 0.474266.
Train: 2018-08-05T18:29:36.777239: step 4470, loss 0.456013.
Test: 2018-08-05T18:29:37.823870: step 4470, loss 0.547169.
Train: 2018-08-05T18:29:38.011327: step 4471, loss 0.562565.
Train: 2018-08-05T18:29:38.198781: step 4472, loss 0.544563.
Train: 2018-08-05T18:29:38.370616: step 4473, loss 0.517119.
Train: 2018-08-05T18:29:38.542451: step 4474, loss 0.562955.
Train: 2018-08-05T18:29:38.729877: step 4475, loss 0.479471.
Train: 2018-08-05T18:29:38.917333: step 4476, loss 0.563309.
Train: 2018-08-05T18:29:39.089169: step 4477, loss 0.544574.
Train: 2018-08-05T18:29:39.261034: step 4478, loss 0.592228.
Train: 2018-08-05T18:29:39.448489: step 4479, loss 0.592386.
Train: 2018-08-05T18:29:39.635946: step 4480, loss 0.592353.
Test: 2018-08-05T18:29:40.682575: step 4480, loss 0.548621.
Train: 2018-08-05T18:29:40.870033: step 4481, loss 0.649205.
Train: 2018-08-05T18:29:41.057488: step 4482, loss 0.497455.
Train: 2018-08-05T18:29:41.229322: step 4483, loss 0.591329.
Train: 2018-08-05T18:29:41.416778: step 4484, loss 0.498106.
Train: 2018-08-05T18:29:41.604234: step 4485, loss 0.58144.
Train: 2018-08-05T18:29:41.776038: step 4486, loss 0.654558.
Train: 2018-08-05T18:29:41.963525: step 4487, loss 0.55362.
Train: 2018-08-05T18:29:42.135361: step 4488, loss 0.526728.
Train: 2018-08-05T18:29:42.322816: step 4489, loss 0.500343.
Train: 2018-08-05T18:29:42.510274: step 4490, loss 0.474123.
Test: 2018-08-05T18:29:43.572494: step 4490, loss 0.548492.
Train: 2018-08-05T18:29:43.759981: step 4491, loss 0.571263.
Train: 2018-08-05T18:29:43.963057: step 4492, loss 0.641858.
Train: 2018-08-05T18:29:44.134893: step 4493, loss 0.571166.
Train: 2018-08-05T18:29:44.337968: step 4494, loss 0.571086.
Train: 2018-08-05T18:29:44.509774: step 4495, loss 0.553679.
Train: 2018-08-05T18:29:44.697260: step 4496, loss 0.579576.
Train: 2018-08-05T18:29:44.884718: step 4497, loss 0.64801.
Train: 2018-08-05T18:29:45.072172: step 4498, loss 0.613144.
Train: 2018-08-05T18:29:45.259631: step 4499, loss 0.562588.
Train: 2018-08-05T18:29:45.447084: step 4500, loss 0.546628.
Test: 2018-08-05T18:29:46.493721: step 4500, loss 0.549611.
Train: 2018-08-05T18:29:47.368509: step 4501, loss 0.513332.
Train: 2018-08-05T18:29:47.555966: step 4502, loss 0.497114.
Train: 2018-08-05T18:29:47.727800: step 4503, loss 0.546177.
Train: 2018-08-05T18:29:47.915257: step 4504, loss 0.554321.
Train: 2018-08-05T18:29:48.102682: step 4505, loss 0.554258.
Train: 2018-08-05T18:29:48.290168: step 4506, loss 0.595609.
Train: 2018-08-05T18:29:48.477624: step 4507, loss 0.579061.
Train: 2018-08-05T18:29:48.665083: step 4508, loss 0.537509.
Train: 2018-08-05T18:29:48.852506: step 4509, loss 0.570766.
Train: 2018-08-05T18:29:49.055585: step 4510, loss 0.587492.
Test: 2018-08-05T18:29:50.117865: step 4510, loss 0.549199.
Train: 2018-08-05T18:29:50.305321: step 4511, loss 0.478716.
Train: 2018-08-05T18:29:50.492777: step 4512, loss 0.604488.
Train: 2018-08-05T18:29:50.680204: step 4513, loss 0.579264.
Train: 2018-08-05T18:29:50.867660: step 4514, loss 0.520011.
Train: 2018-08-05T18:29:51.070737: step 4515, loss 0.553836.
Train: 2018-08-05T18:29:51.258193: step 4516, loss 0.545241.
Train: 2018-08-05T18:29:51.445679: step 4517, loss 0.553742.
Train: 2018-08-05T18:29:51.633137: step 4518, loss 0.570977.
Train: 2018-08-05T18:29:51.836213: step 4519, loss 0.597033.
Train: 2018-08-05T18:29:52.023669: step 4520, loss 0.544985.
Test: 2018-08-05T18:29:53.070269: step 4520, loss 0.547826.
Train: 2018-08-05T18:29:53.273376: step 4521, loss 0.605859.
Train: 2018-08-05T18:29:53.460832: step 4522, loss 0.518901.
Train: 2018-08-05T18:29:53.663881: step 4523, loss 0.553655.
Train: 2018-08-05T18:29:53.866989: step 4524, loss 0.501354.
Train: 2018-08-05T18:29:54.054443: step 4525, loss 0.579894.
Train: 2018-08-05T18:29:54.257522: step 4526, loss 0.483362.
Train: 2018-08-05T18:29:54.444977: step 4527, loss 0.562437.
Train: 2018-08-05T18:29:54.648056: step 4528, loss 0.518021.
Train: 2018-08-05T18:29:54.835510: step 4529, loss 0.517769.
Train: 2018-08-05T18:29:55.007316: step 4530, loss 0.658963.
Test: 2018-08-05T18:29:56.053975: step 4530, loss 0.547971.
Train: 2018-08-05T18:29:56.257024: step 4531, loss 0.535531.
Train: 2018-08-05T18:29:56.460130: step 4532, loss 0.644192.
Train: 2018-08-05T18:29:56.663208: step 4533, loss 0.526539.
Train: 2018-08-05T18:29:56.850664: step 4534, loss 0.553601.
Train: 2018-08-05T18:29:57.053742: step 4535, loss 0.499693.
Train: 2018-08-05T18:29:57.256820: step 4536, loss 0.535612.
Train: 2018-08-05T18:29:57.459896: step 4537, loss 0.562611.
Train: 2018-08-05T18:29:57.662944: step 4538, loss 0.580652.
Train: 2018-08-05T18:29:57.866050: step 4539, loss 0.544595.
Train: 2018-08-05T18:29:58.069128: step 4540, loss 0.499594.
Test: 2018-08-05T18:29:59.115728: step 4540, loss 0.547145.
Train: 2018-08-05T18:29:59.318832: step 4541, loss 0.553606.
Train: 2018-08-05T18:29:59.537535: step 4542, loss 0.553612.
Train: 2018-08-05T18:29:59.740582: step 4543, loss 0.526462.
Train: 2018-08-05T18:29:59.959311: step 4544, loss 0.599003.
Train: 2018-08-05T18:30:00.162388: step 4545, loss 0.526417.
Train: 2018-08-05T18:30:00.365465: step 4546, loss 0.526402.
Train: 2018-08-05T18:30:00.568544: step 4547, loss 0.544545.
Train: 2018-08-05T18:30:00.787242: step 4548, loss 0.562755.
Train: 2018-08-05T18:30:00.990319: step 4549, loss 0.608347.
Train: 2018-08-05T18:30:01.193366: step 4550, loss 0.526365.
Test: 2018-08-05T18:30:02.255618: step 4550, loss 0.547755.
Train: 2018-08-05T18:30:02.458697: step 4551, loss 0.49916.
Train: 2018-08-05T18:30:02.661803: step 4552, loss 0.56273.
Train: 2018-08-05T18:30:02.864882: step 4553, loss 0.535444.
Train: 2018-08-05T18:30:03.067958: step 4554, loss 0.544537.
Train: 2018-08-05T18:30:03.286627: step 4555, loss 0.608374.
Train: 2018-08-05T18:30:03.489732: step 4556, loss 0.626426.
Train: 2018-08-05T18:30:03.708433: step 4557, loss 0.571687.
Train: 2018-08-05T18:30:03.911510: step 4558, loss 0.553593.
Train: 2018-08-05T18:30:04.114558: step 4559, loss 0.589229.
Train: 2018-08-05T18:30:04.333256: step 4560, loss 0.588943.
Test: 2018-08-05T18:30:05.379918: step 4560, loss 0.54897.
Train: 2018-08-05T18:30:05.582994: step 4561, loss 0.571131.
Train: 2018-08-05T18:30:05.801692: step 4562, loss 0.623035.
Train: 2018-08-05T18:30:06.004772: step 4563, loss 0.588023.
Train: 2018-08-05T18:30:06.207848: step 4564, loss 0.553908.
Train: 2018-08-05T18:30:06.426516: step 4565, loss 0.587492.
Train: 2018-08-05T18:30:06.645245: step 4566, loss 0.512889.
Train: 2018-08-05T18:30:06.848323: step 4567, loss 0.562547.
Train: 2018-08-05T18:30:07.067022: step 4568, loss 0.529928.
Train: 2018-08-05T18:30:07.270098: step 4569, loss 0.481119.
Train: 2018-08-05T18:30:07.488768: step 4570, loss 0.513491.
Test: 2018-08-05T18:30:08.535398: step 4570, loss 0.548942.
Train: 2018-08-05T18:30:08.754127: step 4571, loss 0.488347.
Train: 2018-08-05T18:30:08.972825: step 4572, loss 0.570765.
Train: 2018-08-05T18:30:09.175874: step 4573, loss 0.629751.
Train: 2018-08-05T18:30:09.394602: step 4574, loss 0.562356.
Train: 2018-08-05T18:30:09.597679: step 4575, loss 0.57085.
Train: 2018-08-05T18:30:09.816378: step 4576, loss 0.656258.
Train: 2018-08-05T18:30:10.019426: step 4577, loss 0.57086.
Train: 2018-08-05T18:30:10.238124: step 4578, loss 0.562346.
Train: 2018-08-05T18:30:10.441233: step 4579, loss 0.545396.
Train: 2018-08-05T18:30:10.659900: step 4580, loss 0.570825.
Test: 2018-08-05T18:30:11.722164: step 4580, loss 0.548423.
Train: 2018-08-05T18:30:11.925229: step 4581, loss 0.528516.
Train: 2018-08-05T18:30:12.143928: step 4582, loss 0.562355.
Train: 2018-08-05T18:30:12.347034: step 4583, loss 0.562351.
Train: 2018-08-05T18:30:12.565735: step 4584, loss 0.638752.
Train: 2018-08-05T18:30:12.784432: step 4585, loss 0.486229.
Train: 2018-08-05T18:30:12.987510: step 4586, loss 0.579298.
Train: 2018-08-05T18:30:13.206208: step 4587, loss 0.579305.
Train: 2018-08-05T18:30:13.409286: step 4588, loss 0.579298.
Train: 2018-08-05T18:30:13.643577: step 4589, loss 0.545437.
Train: 2018-08-05T18:30:13.846654: step 4590, loss 0.511599.
Test: 2018-08-05T18:30:14.908936: step 4590, loss 0.547394.
Train: 2018-08-05T18:30:15.111983: step 4591, loss 0.536888.
Train: 2018-08-05T18:30:15.330682: step 4592, loss 0.545289.
Train: 2018-08-05T18:30:15.549411: step 4593, loss 0.596612.
Train: 2018-08-05T18:30:15.768109: step 4594, loss 0.545154.
Train: 2018-08-05T18:30:15.986807: step 4595, loss 0.545101.
Train: 2018-08-05T18:30:16.189886: step 4596, loss 0.545041.
Train: 2018-08-05T18:30:16.408583: step 4597, loss 0.562352.
Train: 2018-08-05T18:30:16.627285: step 4598, loss 0.597238.
Train: 2018-08-05T18:30:16.830360: step 4599, loss 0.632177.
Train: 2018-08-05T18:30:17.049059: step 4600, loss 0.579742.
Test: 2018-08-05T18:30:18.095689: step 4600, loss 0.548288.
Train: 2018-08-05T18:30:19.111046: step 4601, loss 0.49311.
Train: 2018-08-05T18:30:19.314154: step 4602, loss 0.5883.
Train: 2018-08-05T18:30:19.532852: step 4603, loss 0.579612.
Train: 2018-08-05T18:30:19.751522: step 4604, loss 0.570947.
Train: 2018-08-05T18:30:19.970220: step 4605, loss 0.622417.
Train: 2018-08-05T18:30:20.188948: step 4606, loss 0.587918.
Train: 2018-08-05T18:30:20.407647: step 4607, loss 0.62159.
Train: 2018-08-05T18:30:20.626317: step 4608, loss 0.570776.
Train: 2018-08-05T18:30:20.845014: step 4609, loss 0.603935.
Train: 2018-08-05T18:30:21.063744: step 4610, loss 0.619985.
Test: 2018-08-05T18:30:22.110344: step 4610, loss 0.549176.
Train: 2018-08-05T18:30:22.344695: step 4611, loss 0.497916.
Train: 2018-08-05T18:30:22.563392: step 4612, loss 0.522575.
Train: 2018-08-05T18:30:22.766440: step 4613, loss 0.610949.
Train: 2018-08-05T18:30:22.985139: step 4614, loss 0.61079.
Train: 2018-08-05T18:30:23.203838: step 4615, loss 0.594719.
Train: 2018-08-05T18:30:23.422566: step 4616, loss 0.468615.
Train: 2018-08-05T18:30:23.641265: step 4617, loss 0.515797.
Train: 2018-08-05T18:30:23.859963: step 4618, loss 0.555087.
Train: 2018-08-05T18:30:24.078663: step 4619, loss 0.531024.
Train: 2018-08-05T18:30:24.297361: step 4620, loss 0.53867.
Test: 2018-08-05T18:30:25.359583: step 4620, loss 0.550598.
Train: 2018-08-05T18:30:25.562690: step 4621, loss 0.570787.
Train: 2018-08-05T18:30:25.781389: step 4622, loss 0.53798.
Train: 2018-08-05T18:30:26.000058: step 4623, loss 0.471393.
Train: 2018-08-05T18:30:26.218756: step 4624, loss 0.638052.
Train: 2018-08-05T18:30:26.437456: step 4625, loss 0.630234.
Train: 2018-08-05T18:30:26.656186: step 4626, loss 0.528256.
Train: 2018-08-05T18:30:26.874884: step 4627, loss 0.545192.
Train: 2018-08-05T18:30:27.077960: step 4628, loss 0.536462.
Train: 2018-08-05T18:30:27.296659: step 4629, loss 0.544979.
Train: 2018-08-05T18:30:27.515357: step 4630, loss 0.606119.
Test: 2018-08-05T18:30:28.561958: step 4630, loss 0.54753.
Train: 2018-08-05T18:30:28.796280: step 4631, loss 0.615067.
Train: 2018-08-05T18:30:29.015010: step 4632, loss 0.615045.
Train: 2018-08-05T18:30:29.233705: step 4633, loss 0.553632.
Train: 2018-08-05T18:30:29.436783: step 4634, loss 0.571075.
Train: 2018-08-05T18:30:29.671072: step 4635, loss 0.597075.
Train: 2018-08-05T18:30:29.874182: step 4636, loss 0.562339.
Train: 2018-08-05T18:30:30.092850: step 4637, loss 0.502227.
Train: 2018-08-05T18:30:30.311578: step 4638, loss 0.639526.
Train: 2018-08-05T18:30:30.530277: step 4639, loss 0.596449.
Train: 2018-08-05T18:30:30.748946: step 4640, loss 0.553891.
Test: 2018-08-05T18:30:31.811196: step 4640, loss 0.547743.
Train: 2018-08-05T18:30:32.029925: step 4641, loss 0.520303.
Train: 2018-08-05T18:30:32.248594: step 4642, loss 0.621145.
Train: 2018-08-05T18:30:32.467323: step 4643, loss 0.48731.
Train: 2018-08-05T18:30:32.701615: step 4644, loss 0.554078.
Train: 2018-08-05T18:30:32.920312: step 4645, loss 0.487266.
Train: 2018-08-05T18:30:33.139041: step 4646, loss 0.579182.
Train: 2018-08-05T18:30:33.357741: step 4647, loss 0.612964.
Train: 2018-08-05T18:30:33.576438: step 4648, loss 0.528624.
Train: 2018-08-05T18:30:33.795137: step 4649, loss 0.536961.
Train: 2018-08-05T18:30:33.998219: step 4650, loss 0.562345.
Test: 2018-08-05T18:30:35.060436: step 4650, loss 0.5471.
Train: 2018-08-05T18:30:35.279173: step 4651, loss 0.579411.
Train: 2018-08-05T18:30:35.482213: step 4652, loss 0.613684.
Train: 2018-08-05T18:30:35.700910: step 4653, loss 0.519582.
Train: 2018-08-05T18:30:35.919640: step 4654, loss 0.528069.
Train: 2018-08-05T18:30:36.138338: step 4655, loss 0.639717.
Train: 2018-08-05T18:30:36.357037: step 4656, loss 0.596677.
Train: 2018-08-05T18:30:36.560084: step 4657, loss 0.536669.
Train: 2018-08-05T18:30:36.794435: step 4658, loss 0.545253.
Train: 2018-08-05T18:30:36.997514: step 4659, loss 0.656266.
Train: 2018-08-05T18:30:37.216181: step 4660, loss 0.647222.
Test: 2018-08-05T18:30:38.278432: step 4660, loss 0.547589.
Train: 2018-08-05T18:30:38.481542: step 4661, loss 0.511999.
Train: 2018-08-05T18:30:38.700209: step 4662, loss 0.595806.
Train: 2018-08-05T18:30:38.934561: step 4663, loss 0.487909.
Train: 2018-08-05T18:30:39.153228: step 4664, loss 0.5873.
Train: 2018-08-05T18:30:39.356335: step 4665, loss 0.562506.
Train: 2018-08-05T18:30:39.575035: step 4666, loss 0.578993.
Train: 2018-08-05T18:30:39.793733: step 4667, loss 0.537888.
Train: 2018-08-05T18:30:40.012432: step 4668, loss 0.587196.
Train: 2018-08-05T18:30:40.231101: step 4669, loss 0.595393.
Train: 2018-08-05T18:30:40.434208: step 4670, loss 0.54619.
Test: 2018-08-05T18:30:41.496460: step 4670, loss 0.550224.
Train: 2018-08-05T18:30:41.715158: step 4671, loss 0.578952.
Train: 2018-08-05T18:30:41.918235: step 4672, loss 0.513514.
Train: 2018-08-05T18:30:42.136934: step 4673, loss 0.48874.
Train: 2018-08-05T18:30:42.355632: step 4674, loss 0.537693.
Train: 2018-08-05T18:30:42.574331: step 4675, loss 0.520727.
Train: 2018-08-05T18:30:42.793030: step 4676, loss 0.486489.
Train: 2018-08-05T18:30:43.011730: step 4677, loss 0.60511.
Train: 2018-08-05T18:30:43.214806: step 4678, loss 0.588273.
Train: 2018-08-05T18:30:43.449130: step 4679, loss 0.501388.
Train: 2018-08-05T18:30:43.652203: step 4680, loss 0.588805.
Test: 2018-08-05T18:30:44.714426: step 4680, loss 0.546633.
Train: 2018-08-05T18:30:44.901911: step 4681, loss 0.486827.
Train: 2018-08-05T18:30:45.120581: step 4682, loss 0.535682.
Train: 2018-08-05T18:30:45.339304: step 4683, loss 0.571706.
Train: 2018-08-05T18:30:45.558008: step 4684, loss 0.553651.
Train: 2018-08-05T18:30:45.776706: step 4685, loss 0.526163.
Train: 2018-08-05T18:30:45.995406: step 4686, loss 0.470582.
Train: 2018-08-05T18:30:46.214104: step 4687, loss 0.535183.
Train: 2018-08-05T18:30:46.417182: step 4688, loss 0.553986.
Train: 2018-08-05T18:30:46.635880: step 4689, loss 0.459101.
Train: 2018-08-05T18:30:46.870204: step 4690, loss 0.563907.
Test: 2018-08-05T18:30:47.932422: step 4690, loss 0.547964.
Train: 2018-08-05T18:30:48.135500: step 4691, loss 0.583469.
Train: 2018-08-05T18:30:48.369859: step 4692, loss 0.525596.
Train: 2018-08-05T18:30:48.572927: step 4693, loss 0.573339.
Train: 2018-08-05T18:30:48.791625: step 4694, loss 0.544541.
Train: 2018-08-05T18:30:49.010295: step 4695, loss 0.510366.
Train: 2018-08-05T18:30:49.213402: step 4696, loss 0.600556.
Train: 2018-08-05T18:30:49.432071: step 4697, loss 0.535153.
Train: 2018-08-05T18:30:49.649154: step 4698, loss 0.601561.
Train: 2018-08-05T18:30:49.867852: step 4699, loss 0.544518.
Train: 2018-08-05T18:30:50.086580: step 4700, loss 0.53583.
Test: 2018-08-05T18:30:51.133210: step 4700, loss 0.548484.
Train: 2018-08-05T18:30:52.070491: step 4701, loss 0.544752.
Train: 2018-08-05T18:30:52.289160: step 4702, loss 0.607202.
Train: 2018-08-05T18:30:52.492267: step 4703, loss 0.553843.
Train: 2018-08-05T18:30:52.710937: step 4704, loss 0.603611.
Train: 2018-08-05T18:30:52.929665: step 4705, loss 0.556915.
Train: 2018-08-05T18:30:53.148334: step 4706, loss 0.57171.
Train: 2018-08-05T18:30:53.367063: step 4707, loss 0.523591.
Train: 2018-08-05T18:30:53.570140: step 4708, loss 0.642183.
Train: 2018-08-05T18:30:53.788839: step 4709, loss 0.562585.
Train: 2018-08-05T18:30:54.007537: step 4710, loss 0.571704.
Test: 2018-08-05T18:30:55.069791: step 4710, loss 0.549016.
Train: 2018-08-05T18:30:55.288488: step 4711, loss 0.589668.
Train: 2018-08-05T18:30:55.491536: step 4712, loss 0.650941.
Train: 2018-08-05T18:30:55.710266: step 4713, loss 0.537743.
Train: 2018-08-05T18:30:55.928963: step 4714, loss 0.563025.
Train: 2018-08-05T18:30:56.147632: step 4715, loss 0.516801.
Train: 2018-08-05T18:30:56.366361: step 4716, loss 0.497549.
Train: 2018-08-05T18:30:56.569438: step 4717, loss 0.544845.
Train: 2018-08-05T18:30:56.788136: step 4718, loss 0.581516.
Train: 2018-08-05T18:30:57.006836: step 4719, loss 0.676497.
Train: 2018-08-05T18:30:57.225505: step 4720, loss 0.507079.
Test: 2018-08-05T18:30:58.272135: step 4720, loss 0.547807.
Train: 2018-08-05T18:30:58.490862: step 4721, loss 0.488578.
Train: 2018-08-05T18:30:58.709563: step 4722, loss 0.544515.
Train: 2018-08-05T18:30:58.928232: step 4723, loss 0.609662.
Train: 2018-08-05T18:30:59.146929: step 4724, loss 0.59083.
Train: 2018-08-05T18:30:59.365658: step 4725, loss 0.553713.
Train: 2018-08-05T18:30:59.584357: step 4726, loss 0.581085.
Train: 2018-08-05T18:30:59.803026: step 4727, loss 0.571758.
Train: 2018-08-05T18:31:00.021749: step 4728, loss 0.580566.
Train: 2018-08-05T18:31:00.240453: step 4729, loss 0.580302.
Train: 2018-08-05T18:31:00.443500: step 4730, loss 0.535974.
Test: 2018-08-05T18:31:01.505782: step 4730, loss 0.547974.
Train: 2018-08-05T18:31:01.724482: step 4731, loss 0.49243.
Train: 2018-08-05T18:31:01.927558: step 4732, loss 0.501373.
Train: 2018-08-05T18:31:02.146226: step 4733, loss 0.501362.
Train: 2018-08-05T18:31:02.364955: step 4734, loss 0.58861.
Train: 2018-08-05T18:31:02.568033: step 4735, loss 0.571137.
Train: 2018-08-05T18:31:02.786731: step 4736, loss 0.588659.
Train: 2018-08-05T18:31:03.005430: step 4737, loss 0.588609.
Train: 2018-08-05T18:31:03.208508: step 4738, loss 0.588509.
Train: 2018-08-05T18:31:03.427207: step 4739, loss 0.527654.
Train: 2018-08-05T18:31:03.645905: step 4740, loss 0.562342.
Test: 2018-08-05T18:31:04.708127: step 4740, loss 0.547527.
Train: 2018-08-05T18:31:04.911205: step 4741, loss 0.588231.
Train: 2018-08-05T18:31:05.129933: step 4742, loss 0.553738.
Train: 2018-08-05T18:31:05.348633: step 4743, loss 0.579477.
Train: 2018-08-05T18:31:05.551709: step 4744, loss 0.587952.
Train: 2018-08-05T18:31:05.770408: step 4745, loss 0.545357.
Train: 2018-08-05T18:31:05.973487: step 4746, loss 0.59622.
Train: 2018-08-05T18:31:06.192184: step 4747, loss 0.545528.
Train: 2018-08-05T18:31:06.410884: step 4748, loss 0.545599.
Train: 2018-08-05T18:31:06.613963: step 4749, loss 0.486972.
Train: 2018-08-05T18:31:06.832659: step 4750, loss 0.579199.
Test: 2018-08-05T18:31:07.879260: step 4750, loss 0.547134.
Train: 2018-08-05T18:31:08.097958: step 4751, loss 0.671929.
Train: 2018-08-05T18:31:08.301065: step 4752, loss 0.595949.
Train: 2018-08-05T18:31:08.519764: step 4753, loss 0.595783.
Train: 2018-08-05T18:31:08.722843: step 4754, loss 0.570757.
Train: 2018-08-05T18:31:08.941541: step 4755, loss 0.472046.
Train: 2018-08-05T18:31:09.144588: step 4756, loss 0.595444.
Train: 2018-08-05T18:31:09.347697: step 4757, loss 0.578977.
Train: 2018-08-05T18:31:09.582017: step 4758, loss 0.537944.
Train: 2018-08-05T18:31:09.785094: step 4759, loss 0.521506.
Train: 2018-08-05T18:31:10.003793: step 4760, loss 0.595474.
Test: 2018-08-05T18:31:11.050421: step 4760, loss 0.54853.
Train: 2018-08-05T18:31:11.253499: step 4761, loss 0.545997.
Train: 2018-08-05T18:31:11.472168: step 4762, loss 0.496255.
Train: 2018-08-05T18:31:11.675275: step 4763, loss 0.59578.
Train: 2018-08-05T18:31:11.893974: step 4764, loss 0.528887.
Train: 2018-08-05T18:31:12.097022: step 4765, loss 0.570803.
Train: 2018-08-05T18:31:12.300129: step 4766, loss 0.528427.
Train: 2018-08-05T18:31:12.518828: step 4767, loss 0.519627.
Train: 2018-08-05T18:31:12.721907: step 4768, loss 0.562337.
Train: 2018-08-05T18:31:12.924983: step 4769, loss 0.605794.
Train: 2018-08-05T18:31:13.143651: step 4770, loss 0.527462.
Test: 2018-08-05T18:31:14.190312: step 4770, loss 0.547938.
Train: 2018-08-05T18:31:14.409010: step 4771, loss 0.509739.
Train: 2018-08-05T18:31:14.612058: step 4772, loss 0.580128.
Train: 2018-08-05T18:31:14.815165: step 4773, loss 0.571375.
Train: 2018-08-05T18:31:15.018242: step 4774, loss 0.544661.
Train: 2018-08-05T18:31:15.236912: step 4775, loss 0.481894.
Train: 2018-08-05T18:31:15.440019: step 4776, loss 0.52652.
Train: 2018-08-05T18:31:15.643068: step 4777, loss 0.54454.
Train: 2018-08-05T18:31:15.861765: step 4778, loss 0.608704.
Train: 2018-08-05T18:31:16.064872: step 4779, loss 0.608893.
Train: 2018-08-05T18:31:16.267953: step 4780, loss 0.636404.
Test: 2018-08-05T18:31:17.330201: step 4780, loss 0.547541.
Train: 2018-08-05T18:31:17.533278: step 4781, loss 0.535394.
Train: 2018-08-05T18:31:17.751948: step 4782, loss 0.471818.
Train: 2018-08-05T18:31:17.955054: step 4783, loss 0.662729.
Train: 2018-08-05T18:31:18.158102: step 4784, loss 0.544578.
Train: 2018-08-05T18:31:18.361210: step 4785, loss 0.616462.
Train: 2018-08-05T18:31:18.564288: step 4786, loss 0.500307.
Train: 2018-08-05T18:31:18.767364: step 4787, loss 0.571309.
Train: 2018-08-05T18:31:18.970444: step 4788, loss 0.562414.
Train: 2018-08-05T18:31:19.173519: step 4789, loss 0.597435.
Train: 2018-08-05T18:31:19.376567: step 4790, loss 0.579754.
Test: 2018-08-05T18:31:20.423197: step 4790, loss 0.547924.
Train: 2018-08-05T18:31:20.626304: step 4791, loss 0.562338.
Train: 2018-08-05T18:31:20.829352: step 4792, loss 0.596611.
Train: 2018-08-05T18:31:21.032428: step 4793, loss 0.528367.
Train: 2018-08-05T18:31:21.235536: step 4794, loss 0.520128.
Train: 2018-08-05T18:31:21.438614: step 4795, loss 0.655069.
Train: 2018-08-05T18:31:21.641662: step 4796, loss 0.50387.
Train: 2018-08-05T18:31:21.844771: step 4797, loss 0.620792.
Train: 2018-08-05T18:31:22.047846: step 4798, loss 0.554182.
Train: 2018-08-05T18:31:22.235303: step 4799, loss 0.620261.
Train: 2018-08-05T18:31:22.438350: step 4800, loss 0.562572.
Test: 2018-08-05T18:31:23.500600: step 4800, loss 0.548848.
Train: 2018-08-05T18:31:24.469156: step 4801, loss 0.562635.
Train: 2018-08-05T18:31:24.656610: step 4802, loss 0.595109.
Train: 2018-08-05T18:31:24.859658: step 4803, loss 0.554702.
Train: 2018-08-05T18:31:25.062765: step 4804, loss 0.57887.
Train: 2018-08-05T18:31:25.250191: step 4805, loss 0.538864.
Train: 2018-08-05T18:31:25.453301: step 4806, loss 0.54689.
Train: 2018-08-05T18:31:25.656378: step 4807, loss 0.578864.
Train: 2018-08-05T18:31:25.843834: step 4808, loss 0.594885.
Train: 2018-08-05T18:31:26.046911: step 4809, loss 0.602884.
Train: 2018-08-05T18:31:26.249957: step 4810, loss 0.482984.
Test: 2018-08-05T18:31:27.296617: step 4810, loss 0.549238.
Train: 2018-08-05T18:31:27.499665: step 4811, loss 0.643054.
Train: 2018-08-05T18:31:27.687150: step 4812, loss 0.594904.
Train: 2018-08-05T18:31:27.890198: step 4813, loss 0.570859.
Train: 2018-08-05T18:31:28.077686: step 4814, loss 0.498912.
Train: 2018-08-05T18:31:28.280762: step 4815, loss 0.562816.
Train: 2018-08-05T18:31:28.468219: step 4816, loss 0.603065.
Train: 2018-08-05T18:31:28.671295: step 4817, loss 0.514259.
Train: 2018-08-05T18:31:28.858751: step 4818, loss 0.56266.
Train: 2018-08-05T18:31:29.046177: step 4819, loss 0.587113.
Train: 2018-08-05T18:31:29.249285: step 4820, loss 0.496896.
Test: 2018-08-05T18:31:30.295915: step 4820, loss 0.549609.
Train: 2018-08-05T18:31:30.498992: step 4821, loss 0.570757.
Train: 2018-08-05T18:31:30.733313: step 4822, loss 0.545747.
Train: 2018-08-05T18:31:30.936359: step 4823, loss 0.579197.
Train: 2018-08-05T18:31:31.123816: step 4824, loss 0.553896.
Train: 2018-08-05T18:31:31.311303: step 4825, loss 0.528277.
Train: 2018-08-05T18:31:31.514374: step 4826, loss 0.579502.
Train: 2018-08-05T18:31:31.701806: step 4827, loss 0.614148.
Train: 2018-08-05T18:31:31.889294: step 4828, loss 0.553689.
Train: 2018-08-05T18:31:32.076747: step 4829, loss 0.553675.
Train: 2018-08-05T18:31:32.279825: step 4830, loss 0.6232.
Test: 2018-08-05T18:31:33.326426: step 4830, loss 0.547059.
Train: 2018-08-05T18:31:33.529527: step 4831, loss 0.544995.
Train: 2018-08-05T18:31:33.685747: step 4832, loss 0.654827.
Train: 2018-08-05T18:31:33.873203: step 4833, loss 0.502036.
Train: 2018-08-05T18:31:34.076301: step 4834, loss 0.510768.
Train: 2018-08-05T18:31:34.263736: step 4835, loss 0.562335.
Train: 2018-08-05T18:31:34.451191: step 4836, loss 0.553727.
Train: 2018-08-05T18:31:34.638648: step 4837, loss 0.562337.
Train: 2018-08-05T18:31:34.810483: step 4838, loss 0.545086.
Train: 2018-08-05T18:31:34.997939: step 4839, loss 0.475936.
Train: 2018-08-05T18:31:35.185394: step 4840, loss 0.623286.
Test: 2018-08-05T18:31:36.247617: step 4840, loss 0.547407.
Train: 2018-08-05T18:31:36.435072: step 4841, loss 0.597216.
Train: 2018-08-05T18:31:36.606906: step 4842, loss 0.605899.
Train: 2018-08-05T18:31:36.794393: step 4843, loss 0.519032.
Train: 2018-08-05T18:31:36.981852: step 4844, loss 0.649086.
Train: 2018-08-05T18:31:37.169305: step 4845, loss 0.622694.
Train: 2018-08-05T18:31:37.356761: step 4846, loss 0.587969.
Train: 2018-08-05T18:31:37.544188: step 4847, loss 0.536977.
Train: 2018-08-05T18:31:37.716053: step 4848, loss 0.570786.
Train: 2018-08-05T18:31:37.903508: step 4849, loss 0.562424.
Train: 2018-08-05T18:31:38.090966: step 4850, loss 0.579053.
Test: 2018-08-05T18:31:39.137563: step 4850, loss 0.54798.
Train: 2018-08-05T18:31:39.325019: step 4851, loss 0.504789.
Train: 2018-08-05T18:31:39.512506: step 4852, loss 0.455455.
Train: 2018-08-05T18:31:39.699962: step 4853, loss 0.504468.
Train: 2018-08-05T18:31:39.871797: step 4854, loss 0.503855.
Train: 2018-08-05T18:31:40.059253: step 4855, loss 0.528481.
Train: 2018-08-05T18:31:40.231087: step 4856, loss 0.485106.
Train: 2018-08-05T18:31:40.418545: step 4857, loss 0.640864.
Train: 2018-08-05T18:31:40.606000: step 4858, loss 0.59764.
Train: 2018-08-05T18:31:40.777837: step 4859, loss 0.553592.
Train: 2018-08-05T18:31:40.965293: step 4860, loss 0.5625.
Test: 2018-08-05T18:31:42.011892: step 4860, loss 0.548623.
Train: 2018-08-05T18:31:42.199347: step 4861, loss 0.625201.
Train: 2018-08-05T18:31:42.371212: step 4862, loss 0.526743.
Train: 2018-08-05T18:31:42.558668: step 4863, loss 0.652149.
Train: 2018-08-05T18:31:42.730473: step 4864, loss 0.633847.
Train: 2018-08-05T18:31:42.917958: step 4865, loss 0.588949.
Train: 2018-08-05T18:31:43.089795: step 4866, loss 0.571127.
Train: 2018-08-05T18:31:43.261629: step 4867, loss 0.588332.
Train: 2018-08-05T18:31:43.449091: step 4868, loss 0.613753.
Train: 2018-08-05T18:31:43.620920: step 4869, loss 0.528518.
Train: 2018-08-05T18:31:43.808376: step 4870, loss 0.545638.
Test: 2018-08-05T18:31:44.854976: step 4870, loss 0.549314.
Train: 2018-08-05T18:31:45.089297: step 4871, loss 0.537484.
Train: 2018-08-05T18:31:45.261160: step 4872, loss 0.628721.
Train: 2018-08-05T18:31:45.432995: step 4873, loss 0.578975.
Train: 2018-08-05T18:31:45.620451: step 4874, loss 0.521859.
Train: 2018-08-05T18:31:45.792285: step 4875, loss 0.554542.
Train: 2018-08-05T18:31:45.964122: step 4876, loss 0.514062.
Train: 2018-08-05T18:31:46.135925: step 4877, loss 0.587026.
Train: 2018-08-05T18:31:46.323382: step 4878, loss 0.538282.
Train: 2018-08-05T18:31:46.495246: step 4879, loss 0.52188.
Train: 2018-08-05T18:31:46.667051: step 4880, loss 0.546171.
Test: 2018-08-05T18:31:47.729302: step 4880, loss 0.547943.
Train: 2018-08-05T18:31:47.901138: step 4881, loss 0.537719.
Train: 2018-08-05T18:31:48.073003: step 4882, loss 0.51198.
Train: 2018-08-05T18:31:48.260428: step 4883, loss 0.643625.
Train: 2018-08-05T18:31:48.432263: step 4884, loss 0.579596.
Train: 2018-08-05T18:31:48.604098: step 4885, loss 0.536961.
Train: 2018-08-05T18:31:48.775963: step 4886, loss 0.52838.
Train: 2018-08-05T18:31:48.947768: step 4887, loss 0.562338.
Train: 2018-08-05T18:31:49.135255: step 4888, loss 0.622403.
Train: 2018-08-05T18:31:49.307058: step 4889, loss 0.545159.
Train: 2018-08-05T18:31:49.478892: step 4890, loss 0.605347.
Test: 2018-08-05T18:31:50.541144: step 4890, loss 0.548776.
Train: 2018-08-05T18:31:50.712978: step 4891, loss 0.493597.
Train: 2018-08-05T18:31:50.884843: step 4892, loss 0.536483.
Train: 2018-08-05T18:31:51.056649: step 4893, loss 0.536383.
Train: 2018-08-05T18:31:51.228513: step 4894, loss 0.63193.
Train: 2018-08-05T18:31:51.400348: step 4895, loss 0.527562.
Train: 2018-08-05T18:31:51.587805: step 4896, loss 0.518784.
Train: 2018-08-05T18:31:51.759638: step 4897, loss 0.57988.
Train: 2018-08-05T18:31:51.931443: step 4898, loss 0.562389.
Train: 2018-08-05T18:31:52.103308: step 4899, loss 0.544826.
Train: 2018-08-05T18:31:52.275144: step 4900, loss 0.588825.
Test: 2018-08-05T18:31:53.337364: step 4900, loss 0.547908.
Train: 2018-08-05T18:31:54.196570: step 4901, loss 0.588825.
Train: 2018-08-05T18:31:54.368402: step 4902, loss 0.527248.
Train: 2018-08-05T18:31:54.555828: step 4903, loss 0.571185.
Train: 2018-08-05T18:31:54.727664: step 4904, loss 0.544837.
Train: 2018-08-05T18:31:54.899528: step 4905, loss 0.597497.
Train: 2018-08-05T18:31:55.071365: step 4906, loss 0.62365.
Train: 2018-08-05T18:31:55.243198: step 4907, loss 0.571053.
Train: 2018-08-05T18:31:55.415002: step 4908, loss 0.536415.
Train: 2018-08-05T18:31:55.586869: step 4909, loss 0.562336.
Train: 2018-08-05T18:31:55.758703: step 4910, loss 0.562335.
Test: 2018-08-05T18:31:56.805301: step 4910, loss 0.547101.
Train: 2018-08-05T18:31:56.977166: step 4911, loss 0.553803.
Train: 2018-08-05T18:31:57.149001: step 4912, loss 0.485741.
Train: 2018-08-05T18:31:57.320836: step 4913, loss 0.545281.
Train: 2018-08-05T18:31:57.492672: step 4914, loss 0.570891.
Train: 2018-08-05T18:31:57.664506: step 4915, loss 0.57948.
Train: 2018-08-05T18:31:57.851961: step 4916, loss 0.605232.
Train: 2018-08-05T18:31:58.023796: step 4917, loss 0.622277.
Train: 2018-08-05T18:31:58.195634: step 4918, loss 0.528272.
Train: 2018-08-05T18:31:58.367466: step 4919, loss 0.536865.
Train: 2018-08-05T18:31:58.539297: step 4920, loss 0.604783.
Test: 2018-08-05T18:31:59.601522: step 4920, loss 0.548426.
Train: 2018-08-05T18:31:59.773381: step 4921, loss 0.520062.
Train: 2018-08-05T18:31:59.945224: step 4922, loss 0.621561.
Train: 2018-08-05T18:32:00.117056: step 4923, loss 0.646636.
Train: 2018-08-05T18:32:00.288860: step 4924, loss 0.537344.
Train: 2018-08-05T18:32:00.460727: step 4925, loss 0.512593.
Train: 2018-08-05T18:32:00.648182: step 4926, loss 0.50439.
Train: 2018-08-05T18:32:00.820017: step 4927, loss 0.62898.
Train: 2018-08-05T18:32:00.991822: step 4928, loss 0.595673.
Train: 2018-08-05T18:32:01.163686: step 4929, loss 0.545918.
Train: 2018-08-05T18:32:01.335522: step 4930, loss 0.570756.
Test: 2018-08-05T18:32:02.382151: step 4930, loss 0.547944.
Train: 2018-08-05T18:32:02.569609: step 4931, loss 0.496427.
Train: 2018-08-05T18:32:02.741442: step 4932, loss 0.56247.
Train: 2018-08-05T18:32:02.913279: step 4933, loss 0.545811.
Train: 2018-08-05T18:32:03.085113: step 4934, loss 0.528997.
Train: 2018-08-05T18:32:03.256946: step 4935, loss 0.52034.
Train: 2018-08-05T18:32:03.444402: step 4936, loss 0.545389.
Train: 2018-08-05T18:32:03.616237: step 4937, loss 0.536649.
Train: 2018-08-05T18:32:03.788071: step 4938, loss 0.658526.
Train: 2018-08-05T18:32:03.959908: step 4939, loss 0.553686.
Train: 2018-08-05T18:32:04.131741: step 4940, loss 0.49292.
Test: 2018-08-05T18:32:05.178371: step 4940, loss 0.548791.
Train: 2018-08-05T18:32:05.350200: step 4941, loss 0.579833.
Train: 2018-08-05T18:32:05.537662: step 4942, loss 0.60623.
Train: 2018-08-05T18:32:05.693875: step 4943, loss 0.500962.
Train: 2018-08-05T18:32:05.865710: step 4944, loss 0.553604.
Train: 2018-08-05T18:32:06.037545: step 4945, loss 0.615495.
Train: 2018-08-05T18:32:06.209381: step 4946, loss 0.65968.
Train: 2018-08-05T18:32:06.396805: step 4947, loss 0.448375.
Train: 2018-08-05T18:32:06.553050: step 4948, loss 0.588768.
Train: 2018-08-05T18:32:06.740506: step 4949, loss 0.588723.
Train: 2018-08-05T18:32:06.912340: step 4950, loss 0.61487.
Test: 2018-08-05T18:32:07.958971: step 4950, loss 0.548234.
Train: 2018-08-05T18:32:08.130806: step 4951, loss 0.657989.
Train: 2018-08-05T18:32:08.302640: step 4952, loss 0.510769.
Train: 2018-08-05T18:32:08.474474: step 4953, loss 0.528217.
Train: 2018-08-05T18:32:08.646280: step 4954, loss 0.519894.
Train: 2018-08-05T18:32:08.818144: step 4955, loss 0.596262.
Train: 2018-08-05T18:32:08.989978: step 4956, loss 0.477871.
Train: 2018-08-05T18:32:09.161814: step 4957, loss 0.562355.
Train: 2018-08-05T18:32:09.333619: step 4958, loss 0.587807.
Train: 2018-08-05T18:32:09.505484: step 4959, loss 0.502919.
Train: 2018-08-05T18:32:09.677318: step 4960, loss 0.536766.
Test: 2018-08-05T18:32:10.739555: step 4960, loss 0.547827.
Train: 2018-08-05T18:32:10.911405: step 4961, loss 0.630888.
Train: 2018-08-05T18:32:11.083208: step 4962, loss 0.536613.
Train: 2018-08-05T18:32:11.255042: step 4963, loss 0.562335.
Train: 2018-08-05T18:32:11.426877: step 4964, loss 0.596758.
Train: 2018-08-05T18:32:11.598743: step 4965, loss 0.570936.
Train: 2018-08-05T18:32:11.770578: step 4966, loss 0.545155.
Train: 2018-08-05T18:32:11.942383: step 4967, loss 0.622461.
Train: 2018-08-05T18:32:12.114247: step 4968, loss 0.639351.
Train: 2018-08-05T18:32:12.286083: step 4969, loss 0.519904.
Train: 2018-08-05T18:32:12.457887: step 4970, loss 0.545462.
Test: 2018-08-05T18:32:13.504546: step 4970, loss 0.548497.
Train: 2018-08-05T18:32:13.676381: step 4971, loss 0.545524.
Train: 2018-08-05T18:32:13.848186: step 4972, loss 0.520317.
Train: 2018-08-05T18:32:14.020050: step 4973, loss 0.638195.
Train: 2018-08-05T18:32:14.191885: step 4974, loss 0.545591.
Train: 2018-08-05T18:32:14.363721: step 4975, loss 0.621091.
Train: 2018-08-05T18:32:14.535525: step 4976, loss 0.545731.
Train: 2018-08-05T18:32:14.707391: step 4977, loss 0.520838.
Train: 2018-08-05T18:32:14.879224: step 4978, loss 0.587408.
Train: 2018-08-05T18:32:15.051059: step 4979, loss 0.570761.
Train: 2018-08-05T18:32:15.222864: step 4980, loss 0.529228.
Test: 2018-08-05T18:32:16.285116: step 4980, loss 0.548167.
Train: 2018-08-05T18:32:16.456981: step 4981, loss 0.554124.
Train: 2018-08-05T18:32:16.628814: step 4982, loss 0.562428.
Train: 2018-08-05T18:32:16.753785: step 4983, loss 0.473278.
Train: 2018-08-05T18:32:16.925621: step 4984, loss 0.511816.
Train: 2018-08-05T18:32:17.097455: step 4985, loss 0.545009.
Train: 2018-08-05T18:32:17.269289: step 4986, loss 0.611117.
Train: 2018-08-05T18:32:17.441094: step 4987, loss 0.58039.
Train: 2018-08-05T18:32:17.612954: step 4988, loss 0.588371.
Train: 2018-08-05T18:32:17.800415: step 4989, loss 0.588332.
Train: 2018-08-05T18:32:17.972251: step 4990, loss 0.588291.
Test: 2018-08-05T18:32:19.018850: step 4990, loss 0.548413.
Train: 2018-08-05T18:32:19.190717: step 4991, loss 0.605198.
Train: 2018-08-05T18:32:19.362550: step 4992, loss 0.562675.
Train: 2018-08-05T18:32:19.534385: step 4993, loss 0.522197.
Train: 2018-08-05T18:32:19.706220: step 4994, loss 0.553881.
Train: 2018-08-05T18:32:19.878024: step 4995, loss 0.493904.
Train: 2018-08-05T18:32:20.049889: step 4996, loss 0.648196.
Train: 2018-08-05T18:32:20.221723: step 4997, loss 0.57091.
Train: 2018-08-05T18:32:20.393559: step 4998, loss 0.656495.
Train: 2018-08-05T18:32:20.565392: step 4999, loss 0.587841.
Train: 2018-08-05T18:32:20.737198: step 5000, loss 0.55394.
Test: 2018-08-05T18:32:21.799448: step 5000, loss 0.547642.
Train: 2018-08-05T18:32:22.768002: step 5001, loss 0.595904.
Train: 2018-08-05T18:32:22.955429: step 5002, loss 0.554137.
Train: 2018-08-05T18:32:23.127293: step 5003, loss 0.587278.
Train: 2018-08-05T18:32:23.299097: step 5004, loss 0.505105.
Train: 2018-08-05T18:32:23.486584: step 5005, loss 0.562572.
Train: 2018-08-05T18:32:23.658419: step 5006, loss 0.554399.
Train: 2018-08-05T18:32:23.830223: step 5007, loss 0.611686.
Train: 2018-08-05T18:32:24.002058: step 5008, loss 0.578935.
Train: 2018-08-05T18:32:24.173923: step 5009, loss 0.521918.
Train: 2018-08-05T18:32:24.345751: step 5010, loss 0.513723.
Test: 2018-08-05T18:32:25.407979: step 5010, loss 0.548905.
Train: 2018-08-05T18:32:25.579845: step 5011, loss 0.513457.
Train: 2018-08-05T18:32:25.751649: step 5012, loss 0.52126.
Train: 2018-08-05T18:32:25.923484: step 5013, loss 0.53744.
Train: 2018-08-05T18:32:26.095351: step 5014, loss 0.587637.
Train: 2018-08-05T18:32:26.282805: step 5015, loss 0.519897.
Train: 2018-08-05T18:32:26.454609: step 5016, loss 0.553759.
Train: 2018-08-05T18:32:26.626476: step 5017, loss 0.493089.
Train: 2018-08-05T18:32:26.813932: step 5018, loss 0.606206.
Train: 2018-08-05T18:32:26.985764: step 5019, loss 0.597776.
Train: 2018-08-05T18:32:27.157570: step 5020, loss 0.571346.
Test: 2018-08-05T18:32:28.204200: step 5020, loss 0.547825.
Train: 2018-08-05T18:32:28.376034: step 5021, loss 0.589218.
Train: 2018-08-05T18:32:28.547898: step 5022, loss 0.535758.
Train: 2018-08-05T18:32:28.735355: step 5023, loss 0.562519.
Train: 2018-08-05T18:32:28.907190: step 5024, loss 0.571466.
Train: 2018-08-05T18:32:29.079026: step 5025, loss 0.499974.
Train: 2018-08-05T18:32:29.266482: step 5026, loss 0.535671.
Train: 2018-08-05T18:32:29.438284: step 5027, loss 0.643496.
Train: 2018-08-05T18:32:29.610150: step 5028, loss 0.562562.
Train: 2018-08-05T18:32:29.781984: step 5029, loss 0.499925.
Train: 2018-08-05T18:32:29.969441: step 5030, loss 0.598332.
Test: 2018-08-05T18:32:31.031692: step 5030, loss 0.546373.
Train: 2018-08-05T18:32:31.203528: step 5031, loss 0.553589.
Train: 2018-08-05T18:32:31.375362: step 5032, loss 0.526861.
Train: 2018-08-05T18:32:31.562817: step 5033, loss 0.589209.
Train: 2018-08-05T18:32:31.734653: step 5034, loss 0.544707.
Train: 2018-08-05T18:32:31.922108: step 5035, loss 0.58906.
Train: 2018-08-05T18:32:32.093914: step 5036, loss 0.659616.
Train: 2018-08-05T18:32:32.281400: step 5037, loss 0.527369.
Train: 2018-08-05T18:32:32.453204: step 5038, loss 0.640594.
Train: 2018-08-05T18:32:32.640661: step 5039, loss 0.59673.
Train: 2018-08-05T18:32:32.812524: step 5040, loss 0.579338.
Test: 2018-08-05T18:32:33.874776: step 5040, loss 0.547789.
Train: 2018-08-05T18:32:34.062232: step 5041, loss 0.545599.
Train: 2018-08-05T18:32:34.234068: step 5042, loss 0.545814.
Train: 2018-08-05T18:32:34.405904: step 5043, loss 0.554244.
Train: 2018-08-05T18:32:34.593358: step 5044, loss 0.595391.
Train: 2018-08-05T18:32:34.765163: step 5045, loss 0.554459.
Train: 2018-08-05T18:32:34.952650: step 5046, loss 0.570788.
Train: 2018-08-05T18:32:35.124484: step 5047, loss 0.538466.
Train: 2018-08-05T18:32:35.311939: step 5048, loss 0.530447.
Train: 2018-08-05T18:32:35.483775: step 5049, loss 0.570805.
Train: 2018-08-05T18:32:35.686854: step 5050, loss 0.506017.
Test: 2018-08-05T18:32:36.733482: step 5050, loss 0.549783.
Train: 2018-08-05T18:32:36.920938: step 5051, loss 0.562634.
Train: 2018-08-05T18:32:37.092743: step 5052, loss 0.562573.
Train: 2018-08-05T18:32:37.280198: step 5053, loss 0.537806.
Train: 2018-08-05T18:32:37.467684: step 5054, loss 0.595649.
Train: 2018-08-05T18:32:37.655142: step 5055, loss 0.470727.
Train: 2018-08-05T18:32:37.826977: step 5056, loss 0.562375.
Train: 2018-08-05T18:32:38.014431: step 5057, loss 0.545343.
Train: 2018-08-05T18:32:38.201890: step 5058, loss 0.502262.
Train: 2018-08-05T18:32:38.389344: step 5059, loss 0.544981.
Train: 2018-08-05T18:32:38.576801: step 5060, loss 0.553612.
Test: 2018-08-05T18:32:39.639022: step 5060, loss 0.548458.
Train: 2018-08-05T18:32:39.810885: step 5061, loss 0.535834.
Train: 2018-08-05T18:32:39.998342: step 5062, loss 0.571535.
Train: 2018-08-05T18:32:40.185769: step 5063, loss 0.54457.
Train: 2018-08-05T18:32:40.373255: step 5064, loss 0.544536.
Train: 2018-08-05T18:32:40.560712: step 5065, loss 0.599567.
Train: 2018-08-05T18:32:40.748167: step 5066, loss 0.526104.
Train: 2018-08-05T18:32:40.920001: step 5067, loss 0.52603.
Train: 2018-08-05T18:32:41.107458: step 5068, loss 0.460985.
Train: 2018-08-05T18:32:41.294913: step 5069, loss 0.535167.
Train: 2018-08-05T18:32:41.482370: step 5070, loss 0.582294.
Test: 2018-08-05T18:32:42.529000: step 5070, loss 0.547495.
Train: 2018-08-05T18:32:42.716455: step 5071, loss 0.52563.
Train: 2018-08-05T18:32:42.919504: step 5072, loss 0.582715.
Train: 2018-08-05T18:32:43.106989: step 5073, loss 0.582797.
Train: 2018-08-05T18:32:43.294441: step 5074, loss 0.601817.
Train: 2018-08-05T18:32:43.481872: step 5075, loss 0.516125.
Train: 2018-08-05T18:32:43.684978: step 5076, loss 0.582424.
Train: 2018-08-05T18:32:43.872435: step 5077, loss 0.535135.
Train: 2018-08-05T18:32:44.059890: step 5078, loss 0.460172.
Train: 2018-08-05T18:32:44.247347: step 5079, loss 0.535156.
Train: 2018-08-05T18:32:44.450426: step 5080, loss 0.58207.
Test: 2018-08-05T18:32:45.497055: step 5080, loss 0.54762.
Train: 2018-08-05T18:32:45.684481: step 5081, loss 0.647571.
Train: 2018-08-05T18:32:45.887588: step 5082, loss 0.525917.
Train: 2018-08-05T18:32:46.075046: step 5083, loss 0.609211.
Train: 2018-08-05T18:32:46.278122: step 5084, loss 0.572002.
Train: 2018-08-05T18:32:46.481170: step 5085, loss 0.571773.
Train: 2018-08-05T18:32:46.668655: step 5086, loss 0.616461.
Train: 2018-08-05T18:32:46.871733: step 5087, loss 0.597855.
Train: 2018-08-05T18:32:47.059159: step 5088, loss 0.53639.
Train: 2018-08-05T18:32:47.246615: step 5089, loss 0.562337.
Train: 2018-08-05T18:32:47.449724: step 5090, loss 0.647034.
Test: 2018-08-05T18:32:48.496353: step 5090, loss 0.5486.
Train: 2018-08-05T18:32:48.699431: step 5091, loss 0.530281.
Train: 2018-08-05T18:32:48.886885: step 5092, loss 0.562752.
Train: 2018-08-05T18:32:49.089934: step 5093, loss 0.62744.
Train: 2018-08-05T18:32:49.293010: step 5094, loss 0.61845.
Train: 2018-08-05T18:32:49.480498: step 5095, loss 0.5791.
Train: 2018-08-05T18:32:49.683543: step 5096, loss 0.608655.
Train: 2018-08-05T18:32:49.886652: step 5097, loss 0.601393.
Train: 2018-08-05T18:32:50.074109: step 5098, loss 0.588434.
Train: 2018-08-05T18:32:50.277185: step 5099, loss 0.532403.
Train: 2018-08-05T18:32:50.480262: step 5100, loss 0.541363.
Test: 2018-08-05T18:32:51.526863: step 5100, loss 0.549797.
Train: 2018-08-05T18:32:52.589144: step 5101, loss 0.546574.
Train: 2018-08-05T18:32:52.792221: step 5102, loss 0.536481.
Train: 2018-08-05T18:32:52.995299: step 5103, loss 0.563691.
Train: 2018-08-05T18:32:53.182754: step 5104, loss 0.554484.
Train: 2018-08-05T18:32:53.385803: step 5105, loss 0.544509.
Train: 2018-08-05T18:32:53.588880: step 5106, loss 0.536015.
Train: 2018-08-05T18:32:53.791987: step 5107, loss 0.624008.
Train: 2018-08-05T18:32:53.995064: step 5108, loss 0.586926.
Train: 2018-08-05T18:32:54.198143: step 5109, loss 0.563176.
Train: 2018-08-05T18:32:54.401219: step 5110, loss 0.539698.
Test: 2018-08-05T18:32:55.463440: step 5110, loss 0.54972.
Train: 2018-08-05T18:32:55.666548: step 5111, loss 0.531802.
Train: 2018-08-05T18:32:55.869625: step 5112, loss 0.500074.
Train: 2018-08-05T18:32:56.072706: step 5113, loss 0.578858.
Train: 2018-08-05T18:32:56.275780: step 5114, loss 0.554823.
Train: 2018-08-05T18:32:56.478858: step 5115, loss 0.562723.
Train: 2018-08-05T18:32:56.681907: step 5116, loss 0.595229.
Train: 2018-08-05T18:32:56.885013: step 5117, loss 0.595365.
Train: 2018-08-05T18:32:57.088091: step 5118, loss 0.55429.
Train: 2018-08-05T18:32:57.291168: step 5119, loss 0.59557.
Train: 2018-08-05T18:32:57.494245: step 5120, loss 0.529299.
Test: 2018-08-05T18:32:58.540844: step 5120, loss 0.54737.
Train: 2018-08-05T18:32:58.759545: step 5121, loss 0.587425.
Train: 2018-08-05T18:32:58.962652: step 5122, loss 0.562415.
Train: 2018-08-05T18:32:59.181350: step 5123, loss 0.478591.
Train: 2018-08-05T18:32:59.384428: step 5124, loss 0.613055.
Train: 2018-08-05T18:32:59.587505: step 5125, loss 0.553864.
Train: 2018-08-05T18:32:59.806203: step 5126, loss 0.519724.
Train: 2018-08-05T18:33:00.024873: step 5127, loss 0.553755.
Train: 2018-08-05T18:33:00.227979: step 5128, loss 0.588244.
Train: 2018-08-05T18:33:00.431057: step 5129, loss 0.536336.
Train: 2018-08-05T18:33:00.649756: step 5130, loss 0.597214.
Test: 2018-08-05T18:33:01.696356: step 5130, loss 0.546781.
Train: 2018-08-05T18:33:01.899463: step 5131, loss 0.579833.
Train: 2018-08-05T18:33:02.118164: step 5132, loss 0.571108.
Train: 2018-08-05T18:33:02.321241: step 5133, loss 0.553636.
Train: 2018-08-05T18:33:02.493074: step 5134, loss 0.450574.
Train: 2018-08-05T18:33:02.711773: step 5135, loss 0.509668.
Train: 2018-08-05T18:33:02.930474: step 5136, loss 0.518144.
Train: 2018-08-05T18:33:03.133519: step 5137, loss 0.562537.
Train: 2018-08-05T18:33:03.352248: step 5138, loss 0.598693.
Train: 2018-08-05T18:33:03.555328: step 5139, loss 0.508339.
Train: 2018-08-05T18:33:03.773995: step 5140, loss 0.517201.
Test: 2018-08-05T18:33:04.820623: step 5140, loss 0.547121.
Train: 2018-08-05T18:33:05.039353: step 5141, loss 0.590403.
Train: 2018-08-05T18:33:05.242431: step 5142, loss 0.562937.
Train: 2018-08-05T18:33:05.461129: step 5143, loss 0.544509.
Train: 2018-08-05T18:33:05.664207: step 5144, loss 0.544509.
Train: 2018-08-05T18:33:05.882905: step 5145, loss 0.67434.
Train: 2018-08-05T18:33:06.085953: step 5146, loss 0.553731.
Train: 2018-08-05T18:33:06.304681: step 5147, loss 0.53535.
Train: 2018-08-05T18:33:06.523380: step 5148, loss 0.59017.
Train: 2018-08-05T18:33:06.726458: step 5149, loss 0.626171.
Train: 2018-08-05T18:33:06.945158: step 5150, loss 0.616418.
Test: 2018-08-05T18:33:07.991756: step 5150, loss 0.547657.
Train: 2018-08-05T18:33:08.194866: step 5151, loss 0.59788.
Train: 2018-08-05T18:33:08.413563: step 5152, loss 0.544908.
Train: 2018-08-05T18:33:08.616640: step 5153, loss 0.553715.
Train: 2018-08-05T18:33:08.835334: step 5154, loss 0.536751.
Train: 2018-08-05T18:33:09.054009: step 5155, loss 0.570818.
Train: 2018-08-05T18:33:09.272708: step 5156, loss 0.629537.
Train: 2018-08-05T18:33:09.491436: step 5157, loss 0.587362.
Train: 2018-08-05T18:33:09.694514: step 5158, loss 0.56255.
Train: 2018-08-05T18:33:09.913212: step 5159, loss 0.505725.
Train: 2018-08-05T18:33:10.147536: step 5160, loss 0.514119.
Test: 2018-08-05T18:33:11.194131: step 5160, loss 0.549181.
Train: 2018-08-05T18:33:11.428485: step 5161, loss 0.570799.
Train: 2018-08-05T18:33:11.647181: step 5162, loss 0.530311.
Train: 2018-08-05T18:33:11.865880: step 5163, loss 0.651996.
Train: 2018-08-05T18:33:12.084549: step 5164, loss 0.619405.
Train: 2018-08-05T18:33:12.287656: step 5165, loss 0.506335.
Train: 2018-08-05T18:33:12.506355: step 5166, loss 0.538584.
Train: 2018-08-05T18:33:12.725053: step 5167, loss 0.554657.
Train: 2018-08-05T18:33:12.943723: step 5168, loss 0.514078.
Train: 2018-08-05T18:33:13.162451: step 5169, loss 0.52999.
Train: 2018-08-05T18:33:13.381153: step 5170, loss 0.546076.
Test: 2018-08-05T18:33:14.427779: step 5170, loss 0.548013.
Train: 2018-08-05T18:33:14.646479: step 5171, loss 0.47111.
Train: 2018-08-05T18:33:14.865147: step 5172, loss 0.62134.
Train: 2018-08-05T18:33:15.068256: step 5173, loss 0.570848.
Train: 2018-08-05T18:33:15.286923: step 5174, loss 0.528051.
Train: 2018-08-05T18:33:15.505652: step 5175, loss 0.579641.
Train: 2018-08-05T18:33:15.739973: step 5176, loss 0.623334.
Train: 2018-08-05T18:33:15.943053: step 5177, loss 0.536174.
Train: 2018-08-05T18:33:16.161752: step 5178, loss 0.606191.
Train: 2018-08-05T18:33:16.380447: step 5179, loss 0.571147.
Train: 2018-08-05T18:33:16.599146: step 5180, loss 0.483573.
Test: 2018-08-05T18:33:17.645776: step 5180, loss 0.547318.
Train: 2018-08-05T18:33:17.880068: step 5181, loss 0.641495.
Train: 2018-08-05T18:33:18.098765: step 5182, loss 0.553617.
Train: 2018-08-05T18:33:18.301872: step 5183, loss 0.571144.
Train: 2018-08-05T18:33:18.520571: step 5184, loss 0.59734.
Train: 2018-08-05T18:33:18.739270: step 5185, loss 0.605878.
Train: 2018-08-05T18:33:18.957970: step 5186, loss 0.553695.
Train: 2018-08-05T18:33:19.176667: step 5187, loss 0.588125.
Train: 2018-08-05T18:33:19.395368: step 5188, loss 0.570876.
Train: 2018-08-05T18:33:19.614035: step 5189, loss 0.587795.
Train: 2018-08-05T18:33:19.832764: step 5190, loss 0.537121.
Test: 2018-08-05T18:33:20.879394: step 5190, loss 0.548797.
Train: 2018-08-05T18:33:21.098093: step 5191, loss 0.537271.
Train: 2018-08-05T18:33:21.332414: step 5192, loss 0.529.
Train: 2018-08-05T18:33:21.551111: step 5193, loss 0.54571.
Train: 2018-08-05T18:33:21.769781: step 5194, loss 0.562409.
Train: 2018-08-05T18:33:21.988509: step 5195, loss 0.470267.
Train: 2018-08-05T18:33:22.207208: step 5196, loss 0.604549.
Train: 2018-08-05T18:33:22.410286: step 5197, loss 0.570825.
Train: 2018-08-05T18:33:22.628984: step 5198, loss 0.553848.
Train: 2018-08-05T18:33:22.863305: step 5199, loss 0.56234.
Train: 2018-08-05T18:33:23.082003: step 5200, loss 0.570891.
Test: 2018-08-05T18:33:24.128604: step 5200, loss 0.548015.
Train: 2018-08-05T18:33:25.159642: step 5201, loss 0.57091.
Train: 2018-08-05T18:33:25.378344: step 5202, loss 0.570922.
Train: 2018-08-05T18:33:25.597040: step 5203, loss 0.596711.
Train: 2018-08-05T18:33:25.815739: step 5204, loss 0.528009.
Train: 2018-08-05T18:33:26.034437: step 5205, loss 0.502223.
Train: 2018-08-05T18:33:26.253136: step 5206, loss 0.562337.
Train: 2018-08-05T18:33:26.471806: step 5207, loss 0.553688.
Train: 2018-08-05T18:33:26.690534: step 5208, loss 0.597092.
Train: 2018-08-05T18:33:26.909232: step 5209, loss 0.5015.
Train: 2018-08-05T18:33:27.127931: step 5210, loss 0.544908.
Test: 2018-08-05T18:33:28.174531: step 5210, loss 0.548146.
Train: 2018-08-05T18:33:28.393259: step 5211, loss 0.544849.
Train: 2018-08-05T18:33:28.611958: step 5212, loss 0.544793.
Train: 2018-08-05T18:33:28.830657: step 5213, loss 0.527032.
Train: 2018-08-05T18:33:29.064949: step 5214, loss 0.580308.
Train: 2018-08-05T18:33:29.283671: step 5215, loss 0.517831.
Train: 2018-08-05T18:33:29.502375: step 5216, loss 0.589545.
Train: 2018-08-05T18:33:29.721044: step 5217, loss 0.598651.
Train: 2018-08-05T18:33:29.939774: step 5218, loss 0.58962.
Train: 2018-08-05T18:33:30.158471: step 5219, loss 0.517677.
Train: 2018-08-05T18:33:30.377169: step 5220, loss 0.535646.
Test: 2018-08-05T18:33:31.439423: step 5220, loss 0.547372.
Train: 2018-08-05T18:33:31.658090: step 5221, loss 0.553594.
Train: 2018-08-05T18:33:31.876822: step 5222, loss 0.598486.
Train: 2018-08-05T18:33:32.095488: step 5223, loss 0.553591.
Train: 2018-08-05T18:33:32.329839: step 5224, loss 0.580384.
Train: 2018-08-05T18:33:32.548505: step 5225, loss 0.553588.
Train: 2018-08-05T18:33:32.767205: step 5226, loss 0.624511.
Train: 2018-08-05T18:33:32.985934: step 5227, loss 0.553607.
Train: 2018-08-05T18:33:33.204603: step 5228, loss 0.536148.
Train: 2018-08-05T18:33:33.423331: step 5229, loss 0.518851.
Train: 2018-08-05T18:33:33.642030: step 5230, loss 0.57972.
Test: 2018-08-05T18:33:34.704281: step 5230, loss 0.549079.
Train: 2018-08-05T18:33:34.922979: step 5231, loss 0.579662.
Train: 2018-08-05T18:33:35.141679: step 5232, loss 0.545087.
Train: 2018-08-05T18:33:35.360378: step 5233, loss 0.562335.
Train: 2018-08-05T18:33:35.579077: step 5234, loss 0.528009.
Train: 2018-08-05T18:33:35.797775: step 5235, loss 0.570915.
Train: 2018-08-05T18:33:36.016444: step 5236, loss 0.519468.
Train: 2018-08-05T18:33:36.235174: step 5237, loss 0.579514.
Train: 2018-08-05T18:33:36.453871: step 5238, loss 0.545147.
Train: 2018-08-05T18:33:36.672570: step 5239, loss 0.545122.
Train: 2018-08-05T18:33:36.891240: step 5240, loss 0.545085.
Test: 2018-08-05T18:33:37.937901: step 5240, loss 0.547893.
Train: 2018-08-05T18:33:38.156598: step 5241, loss 0.579645.
Train: 2018-08-05T18:33:38.375297: step 5242, loss 0.475706.
Train: 2018-08-05T18:33:38.593995: step 5243, loss 0.605955.
Train: 2018-08-05T18:33:38.812696: step 5244, loss 0.597344.
Train: 2018-08-05T18:33:39.031393: step 5245, loss 0.641062.
Train: 2018-08-05T18:33:39.250063: step 5246, loss 0.571056.
Train: 2018-08-05T18:33:39.468791: step 5247, loss 0.562343.
Train: 2018-08-05T18:33:39.687460: step 5248, loss 0.588172.
Train: 2018-08-05T18:33:39.906188: step 5249, loss 0.553776.
Train: 2018-08-05T18:33:40.124856: step 5250, loss 0.562342.
Test: 2018-08-05T18:33:41.171486: step 5250, loss 0.548388.
Train: 2018-08-05T18:33:41.390215: step 5251, loss 0.486047.
Train: 2018-08-05T18:33:41.608884: step 5252, loss 0.477504.
Train: 2018-08-05T18:33:41.827613: step 5253, loss 0.519662.
Train: 2018-08-05T18:33:42.061936: step 5254, loss 0.596749.
Train: 2018-08-05T18:33:42.280632: step 5255, loss 0.536403.
Train: 2018-08-05T18:33:42.499331: step 5256, loss 0.527568.
Train: 2018-08-05T18:33:42.718030: step 5257, loss 0.579899.
Train: 2018-08-05T18:33:42.936730: step 5258, loss 0.553606.
Train: 2018-08-05T18:33:43.155427: step 5259, loss 0.535911.
Train: 2018-08-05T18:33:43.374126: step 5260, loss 0.624694.
Test: 2018-08-05T18:33:44.420726: step 5260, loss 0.548449.
Train: 2018-08-05T18:33:44.655071: step 5261, loss 0.64251.
Train: 2018-08-05T18:33:44.873775: step 5262, loss 0.633237.
Train: 2018-08-05T18:33:45.092473: step 5263, loss 0.588697.
Train: 2018-08-05T18:33:45.311175: step 5264, loss 0.640485.
Train: 2018-08-05T18:33:45.529871: step 5265, loss 0.562336.
Train: 2018-08-05T18:33:45.748570: step 5266, loss 0.604622.
Train: 2018-08-05T18:33:45.967239: step 5267, loss 0.537427.
Train: 2018-08-05T18:33:46.185969: step 5268, loss 0.595491.
Train: 2018-08-05T18:33:46.404668: step 5269, loss 0.587078.
Train: 2018-08-05T18:33:46.623335: step 5270, loss 0.530517.
Test: 2018-08-05T18:33:47.669995: step 5270, loss 0.549128.
Train: 2018-08-05T18:33:47.935559: step 5271, loss 0.546856.
Train: 2018-08-05T18:33:48.154257: step 5272, loss 0.515142.
Train: 2018-08-05T18:33:48.372955: step 5273, loss 0.594786.
Train: 2018-08-05T18:33:48.591654: step 5274, loss 0.578859.
Train: 2018-08-05T18:33:48.810355: step 5275, loss 0.507366.
Train: 2018-08-05T18:33:49.029052: step 5276, loss 0.570888.
Train: 2018-08-05T18:33:49.247722: step 5277, loss 0.562863.
Train: 2018-08-05T18:33:49.466449: step 5278, loss 0.611002.
Train: 2018-08-05T18:33:49.685150: step 5279, loss 0.554743.
Train: 2018-08-05T18:33:49.903847: step 5280, loss 0.570817.
Test: 2018-08-05T18:33:50.962016: step 5280, loss 0.549403.
Train: 2018-08-05T18:33:51.165124: step 5281, loss 0.546549.
Train: 2018-08-05T18:33:51.383821: step 5282, loss 0.554551.
Train: 2018-08-05T18:33:51.602521: step 5283, loss 0.570773.
Train: 2018-08-05T18:33:51.821220: step 5284, loss 0.554371.
Train: 2018-08-05T18:33:52.008676: step 5285, loss 0.4393.
Train: 2018-08-05T18:33:52.227345: step 5286, loss 0.579653.
Train: 2018-08-05T18:33:52.446073: step 5287, loss 0.580026.
Train: 2018-08-05T18:33:52.664742: step 5288, loss 0.501674.
Train: 2018-08-05T18:33:52.883471: step 5289, loss 0.580358.
Train: 2018-08-05T18:33:53.102170: step 5290, loss 0.616202.
Test: 2018-08-05T18:33:54.148770: step 5290, loss 0.546586.
Train: 2018-08-05T18:33:54.383091: step 5291, loss 0.544912.
Train: 2018-08-05T18:33:54.586197: step 5292, loss 0.544957.
Train: 2018-08-05T18:33:54.804897: step 5293, loss 0.623516.
Train: 2018-08-05T18:33:55.023594: step 5294, loss 0.544907.
Train: 2018-08-05T18:33:55.242293: step 5295, loss 0.544903.
Train: 2018-08-05T18:33:55.445342: step 5296, loss 0.606086.
Train: 2018-08-05T18:33:55.679692: step 5297, loss 0.527467.
Train: 2018-08-05T18:33:55.898391: step 5298, loss 0.527467.
Train: 2018-08-05T18:33:56.101468: step 5299, loss 0.562373.
Train: 2018-08-05T18:33:56.320137: step 5300, loss 0.57988.
Test: 2018-08-05T18:33:57.382388: step 5300, loss 0.546759.
Train: 2018-08-05T18:33:58.335319: step 5301, loss 0.597373.
Train: 2018-08-05T18:33:58.554019: step 5302, loss 0.536188.
Train: 2018-08-05T18:33:58.772716: step 5303, loss 0.640811.
Train: 2018-08-05T18:33:58.991415: step 5304, loss 0.55368.
Train: 2018-08-05T18:33:59.194493: step 5305, loss 0.579579.
Train: 2018-08-05T18:33:59.413193: step 5306, loss 0.622337.
Train: 2018-08-05T18:33:59.631885: step 5307, loss 0.545355.
Train: 2018-08-05T18:33:59.850603: step 5308, loss 0.621429.
Train: 2018-08-05T18:34:00.069288: step 5309, loss 0.520637.
Train: 2018-08-05T18:34:00.287993: step 5310, loss 0.554146.
Test: 2018-08-05T18:34:01.350207: step 5310, loss 0.547723.
Train: 2018-08-05T18:34:01.553316: step 5311, loss 0.570756.
Train: 2018-08-05T18:34:01.772014: step 5312, loss 0.554284.
Train: 2018-08-05T18:34:01.990714: step 5313, loss 0.587189.
Train: 2018-08-05T18:34:02.209412: step 5314, loss 0.562578.
Train: 2018-08-05T18:34:02.428112: step 5315, loss 0.619767.
Train: 2018-08-05T18:34:02.631188: step 5316, loss 0.603277.
Train: 2018-08-05T18:34:02.849887: step 5317, loss 0.538541.
Train: 2018-08-05T18:34:03.052965: step 5318, loss 0.578872.
Train: 2018-08-05T18:34:03.271663: step 5319, loss 0.522798.
Train: 2018-08-05T18:34:03.490359: step 5320, loss 0.546819.
Test: 2018-08-05T18:34:04.536962: step 5320, loss 0.549774.
Train: 2018-08-05T18:34:04.755691: step 5321, loss 0.562812.
Train: 2018-08-05T18:34:04.958769: step 5322, loss 0.554724.
Train: 2018-08-05T18:34:05.177467: step 5323, loss 0.619296.
Train: 2018-08-05T18:34:05.396165: step 5324, loss 0.554633.
Train: 2018-08-05T18:34:05.599212: step 5325, loss 0.578897.
Train: 2018-08-05T18:34:05.817942: step 5326, loss 0.635669.
Train: 2018-08-05T18:34:06.021020: step 5327, loss 0.522288.
Train: 2018-08-05T18:34:06.239688: step 5328, loss 0.522248.
Train: 2018-08-05T18:34:06.442795: step 5329, loss 0.546411.
Train: 2018-08-05T18:34:06.661495: step 5330, loss 0.562602.
Test: 2018-08-05T18:34:07.723745: step 5330, loss 0.548454.
Train: 2018-08-05T18:34:07.926824: step 5331, loss 0.64467.
Train: 2018-08-05T18:34:08.145492: step 5332, loss 0.611827.
Train: 2018-08-05T18:34:08.348600: step 5333, loss 0.51341.
Train: 2018-08-05T18:34:08.567298: step 5334, loss 0.505118.
Train: 2018-08-05T18:34:08.770345: step 5335, loss 0.620261.
Train: 2018-08-05T18:34:08.989074: step 5336, loss 0.570756.
Train: 2018-08-05T18:34:09.192122: step 5337, loss 0.504531.
Train: 2018-08-05T18:34:09.395229: step 5338, loss 0.487529.
Train: 2018-08-05T18:34:09.598307: step 5339, loss 0.604414.
Train: 2018-08-05T18:34:09.817006: step 5340, loss 0.520053.
Test: 2018-08-05T18:34:10.863605: step 5340, loss 0.547301.
Train: 2018-08-05T18:34:11.082334: step 5341, loss 0.570872.
Train: 2018-08-05T18:34:11.285381: step 5342, loss 0.545145.
Train: 2018-08-05T18:34:11.504081: step 5343, loss 0.527709.
Train: 2018-08-05T18:34:11.707188: step 5344, loss 0.553636.
Train: 2018-08-05T18:34:11.910266: step 5345, loss 0.553606.
Train: 2018-08-05T18:34:12.128935: step 5346, loss 0.526997.
Train: 2018-08-05T18:34:12.332041: step 5347, loss 0.589333.
Train: 2018-08-05T18:34:12.535120: step 5348, loss 0.643401.
Train: 2018-08-05T18:34:12.738197: step 5349, loss 0.571538.
Train: 2018-08-05T18:34:12.956865: step 5350, loss 0.553591.
Test: 2018-08-05T18:34:14.003494: step 5350, loss 0.547807.
Train: 2018-08-05T18:34:14.206573: step 5351, loss 0.589333.
Train: 2018-08-05T18:34:14.409649: step 5352, loss 0.58029.
Train: 2018-08-05T18:34:14.628379: step 5353, loss 0.52703.
Train: 2018-08-05T18:34:14.831456: step 5354, loss 0.571253.
Train: 2018-08-05T18:34:15.034503: step 5355, loss 0.527229.
Train: 2018-08-05T18:34:15.237611: step 5356, loss 0.562393.
Train: 2018-08-05T18:34:15.440690: step 5357, loss 0.483531.
Train: 2018-08-05T18:34:15.643766: step 5358, loss 0.571181.
Train: 2018-08-05T18:34:15.846843: step 5359, loss 0.500826.
Train: 2018-08-05T18:34:16.049892: step 5360, loss 0.615456.
Test: 2018-08-05T18:34:17.096551: step 5360, loss 0.549096.
Train: 2018-08-05T18:34:17.299597: step 5361, loss 0.544754.
Train: 2018-08-05T18:34:17.502705: step 5362, loss 0.562444.
Train: 2018-08-05T18:34:17.705783: step 5363, loss 0.562447.
Train: 2018-08-05T18:34:17.908862: step 5364, loss 0.527036.
Train: 2018-08-05T18:34:18.127560: step 5365, loss 0.61565.
Train: 2018-08-05T18:34:18.330636: step 5366, loss 0.553595.
Train: 2018-08-05T18:34:18.518093: step 5367, loss 0.597732.
Train: 2018-08-05T18:34:18.736791: step 5368, loss 0.527248.
Train: 2018-08-05T18:34:18.939865: step 5369, loss 0.632529.
Train: 2018-08-05T18:34:19.142916: step 5370, loss 0.588489.
Test: 2018-08-05T18:34:20.205167: step 5370, loss 0.548902.
Train: 2018-08-05T18:34:20.392653: step 5371, loss 0.588266.
Train: 2018-08-05T18:34:20.595731: step 5372, loss 0.545203.
Train: 2018-08-05T18:34:20.798779: step 5373, loss 0.587864.
Train: 2018-08-05T18:34:20.986265: step 5374, loss 0.562366.
Train: 2018-08-05T18:34:21.204935: step 5375, loss 0.453421.
Train: 2018-08-05T18:34:21.392419: step 5376, loss 0.604342.
Train: 2018-08-05T18:34:21.595466: step 5377, loss 0.520517.
Train: 2018-08-05T18:34:21.798577: step 5378, loss 0.554004.
Train: 2018-08-05T18:34:21.986030: step 5379, loss 0.638038.
Train: 2018-08-05T18:34:22.189077: step 5380, loss 0.545628.
Test: 2018-08-05T18:34:23.251329: step 5380, loss 0.547065.
Train: 2018-08-05T18:34:23.438818: step 5381, loss 0.612652.
Train: 2018-08-05T18:34:23.641863: step 5382, loss 0.570767.
Train: 2018-08-05T18:34:23.844971: step 5383, loss 0.545823.
Train: 2018-08-05T18:34:24.048017: step 5384, loss 0.545867.
Train: 2018-08-05T18:34:24.235474: step 5385, loss 0.579053.
Train: 2018-08-05T18:34:24.438581: step 5386, loss 0.587333.
Train: 2018-08-05T18:34:24.626008: step 5387, loss 0.504584.
Train: 2018-08-05T18:34:24.829115: step 5388, loss 0.545884.
Train: 2018-08-05T18:34:25.016541: step 5389, loss 0.620686.
Train: 2018-08-05T18:34:25.219619: step 5390, loss 0.504203.
Test: 2018-08-05T18:34:26.266249: step 5390, loss 0.548083.
Train: 2018-08-05T18:34:26.500601: step 5391, loss 0.570769.
Train: 2018-08-05T18:34:26.766158: step 5392, loss 0.528877.
Train: 2018-08-05T18:34:27.000485: step 5393, loss 0.553948.
Train: 2018-08-05T18:34:27.203558: step 5394, loss 0.604712.
Train: 2018-08-05T18:34:27.390985: step 5395, loss 0.519887.
Train: 2018-08-05T18:34:27.578471: step 5396, loss 0.536737.
Train: 2018-08-05T18:34:27.781519: step 5397, loss 0.579504.
Train: 2018-08-05T18:34:27.969005: step 5398, loss 0.527852.
Train: 2018-08-05T18:34:28.156462: step 5399, loss 0.623045.
Train: 2018-08-05T18:34:28.343916: step 5400, loss 0.553668.
Test: 2018-08-05T18:34:29.406138: step 5400, loss 0.547432.
Train: 2018-08-05T18:34:30.327827: step 5401, loss 0.58844.
Train: 2018-08-05T18:34:30.515283: step 5402, loss 0.605808.
Train: 2018-08-05T18:34:30.718360: step 5403, loss 0.579669.
Train: 2018-08-05T18:34:30.905818: step 5404, loss 0.545086.
Train: 2018-08-05T18:34:31.093273: step 5405, loss 0.459128.
Train: 2018-08-05T18:34:31.280730: step 5406, loss 0.579601.
Train: 2018-08-05T18:34:31.468156: step 5407, loss 0.536395.
Train: 2018-08-05T18:34:31.655643: step 5408, loss 0.536318.
Train: 2018-08-05T18:34:31.843097: step 5409, loss 0.475224.
Train: 2018-08-05T18:34:32.030555: step 5410, loss 0.527239.
Test: 2018-08-05T18:34:33.077154: step 5410, loss 0.548052.
Train: 2018-08-05T18:34:33.264640: step 5411, loss 0.526966.
Train: 2018-08-05T18:34:33.452095: step 5412, loss 0.553592.
Train: 2018-08-05T18:34:33.655174: step 5413, loss 0.58075.
Train: 2018-08-05T18:34:33.842629: step 5414, loss 0.526339.
Train: 2018-08-05T18:34:34.030086: step 5415, loss 0.54452.
Train: 2018-08-05T18:34:34.217510: step 5416, loss 0.572165.
Train: 2018-08-05T18:34:34.404999: step 5417, loss 0.535255.
Train: 2018-08-05T18:34:34.592453: step 5418, loss 0.553802.
Train: 2018-08-05T18:34:34.779911: step 5419, loss 0.544516.
Train: 2018-08-05T18:34:34.967366: step 5420, loss 0.563201.
Test: 2018-08-05T18:34:36.029618: step 5420, loss 0.547823.
Train: 2018-08-05T18:34:36.217073: step 5421, loss 0.497782.
Train: 2018-08-05T18:34:36.404524: step 5422, loss 0.57267.
Train: 2018-08-05T18:34:36.591986: step 5423, loss 0.563315.
Train: 2018-08-05T18:34:36.779443: step 5424, loss 0.563306.
Train: 2018-08-05T18:34:36.966868: step 5425, loss 0.525792.
Train: 2018-08-05T18:34:37.138732: step 5426, loss 0.535165.
Train: 2018-08-05T18:34:37.326158: step 5427, loss 0.525808.
Train: 2018-08-05T18:34:37.513614: step 5428, loss 0.535164.
Train: 2018-08-05T18:34:37.701072: step 5429, loss 0.638236.
Train: 2018-08-05T18:34:37.888556: step 5430, loss 0.572492.
Test: 2018-08-05T18:34:38.935158: step 5430, loss 0.546927.
Train: 2018-08-05T18:34:39.153856: step 5431, loss 0.572311.
Train: 2018-08-05T18:34:39.372555: step 5432, loss 0.599714.
Train: 2018-08-05T18:34:39.591253: step 5433, loss 0.57187.
Train: 2018-08-05T18:34:39.809984: step 5434, loss 0.553605.
Train: 2018-08-05T18:34:40.028679: step 5435, loss 0.562522.
Train: 2018-08-05T18:34:40.184864: step 5436, loss 0.637973.
Train: 2018-08-05T18:34:40.372320: step 5437, loss 0.483768.
Train: 2018-08-05T18:34:40.544186: step 5438, loss 0.597014.
Train: 2018-08-05T18:34:40.731641: step 5439, loss 0.519397.
Train: 2018-08-05T18:34:40.903477: step 5440, loss 0.50257.
Test: 2018-08-05T18:34:41.965727: step 5440, loss 0.548102.
Train: 2018-08-05T18:34:42.137532: step 5441, loss 0.553815.
Train: 2018-08-05T18:34:42.325017: step 5442, loss 0.553823.
Train: 2018-08-05T18:34:42.496852: step 5443, loss 0.579377.
Train: 2018-08-05T18:34:42.668688: step 5444, loss 0.621896.
Train: 2018-08-05T18:34:42.856114: step 5445, loss 0.520017.
Train: 2018-08-05T18:34:43.043570: step 5446, loss 0.545449.
Train: 2018-08-05T18:34:43.215435: step 5447, loss 0.553906.
Train: 2018-08-05T18:34:43.402892: step 5448, loss 0.562359.
Train: 2018-08-05T18:34:43.574726: step 5449, loss 0.511571.
Train: 2018-08-05T18:34:43.762181: step 5450, loss 0.596341.
Test: 2018-08-05T18:34:44.808781: step 5450, loss 0.54872.
Train: 2018-08-05T18:34:44.996267: step 5451, loss 0.59638.
Train: 2018-08-05T18:34:45.183694: step 5452, loss 0.621863.
Train: 2018-08-05T18:34:45.355559: step 5453, loss 0.545424.
Train: 2018-08-05T18:34:45.527395: step 5454, loss 0.596136.
Train: 2018-08-05T18:34:45.699228: step 5455, loss 0.5876.
Train: 2018-08-05T18:34:45.886683: step 5456, loss 0.579133.
Train: 2018-08-05T18:34:46.058518: step 5457, loss 0.570761.
Train: 2018-08-05T18:34:46.245975: step 5458, loss 0.554214.
Train: 2018-08-05T18:34:46.417780: step 5459, loss 0.537795.
Train: 2018-08-05T18:34:46.589615: step 5460, loss 0.504919.
Test: 2018-08-05T18:34:47.651866: step 5460, loss 0.548144.
Train: 2018-08-05T18:34:47.823731: step 5461, loss 0.579011.
Train: 2018-08-05T18:34:48.011157: step 5462, loss 0.570756.
Train: 2018-08-05T18:34:48.182990: step 5463, loss 0.562469.
Train: 2018-08-05T18:34:48.370477: step 5464, loss 0.612288.
Train: 2018-08-05T18:34:48.542313: step 5465, loss 0.512671.
Train: 2018-08-05T18:34:48.714146: step 5466, loss 0.512514.
Train: 2018-08-05T18:34:48.901597: step 5467, loss 0.637745.
Train: 2018-08-05T18:34:49.073437: step 5468, loss 0.570778.
Train: 2018-08-05T18:34:49.245272: step 5469, loss 0.537246.
Train: 2018-08-05T18:34:49.417076: step 5470, loss 0.537175.
Test: 2018-08-05T18:34:50.479358: step 5470, loss 0.548085.
Train: 2018-08-05T18:34:50.651194: step 5471, loss 0.53706.
Train: 2018-08-05T18:34:50.823028: step 5472, loss 0.630197.
Train: 2018-08-05T18:34:51.010484: step 5473, loss 0.579321.
Train: 2018-08-05T18:34:51.182319: step 5474, loss 0.579315.
Train: 2018-08-05T18:34:51.354154: step 5475, loss 0.452276.
Train: 2018-08-05T18:34:51.525989: step 5476, loss 0.553821.
Train: 2018-08-05T18:34:51.713445: step 5477, loss 0.51949.
Train: 2018-08-05T18:34:51.885250: step 5478, loss 0.570975.
Train: 2018-08-05T18:34:52.057114: step 5479, loss 0.597112.
Train: 2018-08-05T18:34:52.228949: step 5480, loss 0.588518.
Test: 2018-08-05T18:34:53.291170: step 5480, loss 0.548595.
Train: 2018-08-05T18:34:53.463035: step 5481, loss 0.579822.
Train: 2018-08-05T18:34:53.634869: step 5482, loss 0.544917.
Train: 2018-08-05T18:34:53.822326: step 5483, loss 0.571095.
Train: 2018-08-05T18:34:53.994160: step 5484, loss 0.588536.
Train: 2018-08-05T18:34:54.165965: step 5485, loss 0.597172.
Train: 2018-08-05T18:34:54.353450: step 5486, loss 0.475697.
Train: 2018-08-05T18:34:54.525287: step 5487, loss 0.562348.
Train: 2018-08-05T18:34:54.697120: step 5488, loss 0.597072.
Train: 2018-08-05T18:34:54.868956: step 5489, loss 0.562346.
Train: 2018-08-05T18:34:55.040791: step 5490, loss 0.579645.
Test: 2018-08-05T18:34:56.087390: step 5490, loss 0.547532.
Train: 2018-08-05T18:34:56.274877: step 5491, loss 0.536456.
Train: 2018-08-05T18:34:56.446712: step 5492, loss 0.536482.
Train: 2018-08-05T18:34:56.618545: step 5493, loss 0.622697.
Train: 2018-08-05T18:34:56.790381: step 5494, loss 0.519367.
Train: 2018-08-05T18:34:56.962186: step 5495, loss 0.502208.
Train: 2018-08-05T18:34:57.134020: step 5496, loss 0.55372.
Train: 2018-08-05T18:34:57.305885: step 5497, loss 0.640148.
Train: 2018-08-05T18:34:57.477721: step 5498, loss 0.665875.
Train: 2018-08-05T18:34:57.649555: step 5499, loss 0.528108.
Train: 2018-08-05T18:34:57.821392: step 5500, loss 0.545321.
Test: 2018-08-05T18:34:58.883610: step 5500, loss 0.54741.
Train: 2018-08-05T18:34:59.805299: step 5501, loss 0.638663.
Train: 2018-08-05T18:34:59.992751: step 5502, loss 0.579203.
Train: 2018-08-05T18:35:00.164592: step 5503, loss 0.60414.
Train: 2018-08-05T18:35:00.336425: step 5504, loss 0.55423.
Train: 2018-08-05T18:35:00.508261: step 5505, loss 0.529766.
Train: 2018-08-05T18:35:00.680095: step 5506, loss 0.587099.
Train: 2018-08-05T18:35:00.851929: step 5507, loss 0.546412.
Train: 2018-08-05T18:35:01.023765: step 5508, loss 0.562692.
Train: 2018-08-05T18:35:01.211215: step 5509, loss 0.603161.
Train: 2018-08-05T18:35:01.383057: step 5510, loss 0.586943.
Test: 2018-08-05T18:35:02.429655: step 5510, loss 0.550504.
Train: 2018-08-05T18:35:02.601490: step 5511, loss 0.522652.
Train: 2018-08-05T18:35:02.773356: step 5512, loss 0.530682.
Train: 2018-08-05T18:35:02.960810: step 5513, loss 0.50638.
Train: 2018-08-05T18:35:03.132645: step 5514, loss 0.473433.
Train: 2018-08-05T18:35:03.320101: step 5515, loss 0.546104.
Train: 2018-08-05T18:35:03.491936: step 5516, loss 0.52913.
Train: 2018-08-05T18:35:03.663771: step 5517, loss 0.553919.
Train: 2018-08-05T18:35:03.835606: step 5518, loss 0.519542.
Train: 2018-08-05T18:35:04.007410: step 5519, loss 0.588402.
Train: 2018-08-05T18:35:04.179275: step 5520, loss 0.500924.
Test: 2018-08-05T18:35:05.241496: step 5520, loss 0.548854.
Train: 2018-08-05T18:35:05.413362: step 5521, loss 0.580286.
Train: 2018-08-05T18:35:05.585196: step 5522, loss 0.49066.
Train: 2018-08-05T18:35:05.757001: step 5523, loss 0.571856.
Train: 2018-08-05T18:35:05.928865: step 5524, loss 0.590496.
Train: 2018-08-05T18:35:06.100700: step 5525, loss 0.572273.
Train: 2018-08-05T18:35:06.288157: step 5526, loss 0.516637.
Train: 2018-08-05T18:35:06.459991: step 5527, loss 0.619227.
Train: 2018-08-05T18:35:06.631796: step 5528, loss 0.563199.
Train: 2018-08-05T18:35:06.803631: step 5529, loss 0.600485.
Train: 2018-08-05T18:35:06.975466: step 5530, loss 0.553797.
Test: 2018-08-05T18:35:08.037717: step 5530, loss 0.547771.
Train: 2018-08-05T18:35:08.209551: step 5531, loss 0.553752.
Train: 2018-08-05T18:35:08.397039: step 5532, loss 0.516915.
Train: 2018-08-05T18:35:08.584494: step 5533, loss 0.590386.
Train: 2018-08-05T18:35:08.756329: step 5534, loss 0.608423.
Train: 2018-08-05T18:35:08.928133: step 5535, loss 0.544565.
Train: 2018-08-05T18:35:09.099968: step 5536, loss 0.643488.
Train: 2018-08-05T18:35:09.287455: step 5537, loss 0.598002.
Train: 2018-08-05T18:35:09.459259: step 5538, loss 0.52733.
Train: 2018-08-05T18:35:09.631123: step 5539, loss 0.562347.
Train: 2018-08-05T18:35:09.802958: step 5540, loss 0.502209.
Test: 2018-08-05T18:35:10.849558: step 5540, loss 0.547475.
Train: 2018-08-05T18:35:11.037045: step 5541, loss 0.570883.
Train: 2018-08-05T18:35:11.208849: step 5542, loss 0.494329.
Train: 2018-08-05T18:35:11.380714: step 5543, loss 0.511352.
Train: 2018-08-05T18:35:11.552549: step 5544, loss 0.596436.
Train: 2018-08-05T18:35:11.740004: step 5545, loss 0.604982.
Train: 2018-08-05T18:35:11.911810: step 5546, loss 0.630426.
Train: 2018-08-05T18:35:12.083674: step 5547, loss 0.545441.
Train: 2018-08-05T18:35:12.271100: step 5548, loss 0.587642.
Train: 2018-08-05T18:35:12.442935: step 5549, loss 0.629424.
Train: 2018-08-05T18:35:12.614769: step 5550, loss 0.545844.
Test: 2018-08-05T18:35:13.677054: step 5550, loss 0.549862.
Train: 2018-08-05T18:35:13.848856: step 5551, loss 0.587261.
Train: 2018-08-05T18:35:14.020722: step 5552, loss 0.578958.
Train: 2018-08-05T18:35:14.208147: step 5553, loss 0.530079.
Train: 2018-08-05T18:35:14.380011: step 5554, loss 0.595129.
Train: 2018-08-05T18:35:14.551817: step 5555, loss 0.546577.
Train: 2018-08-05T18:35:14.739302: step 5556, loss 0.522459.
Train: 2018-08-05T18:35:14.911138: step 5557, loss 0.465873.
Train: 2018-08-05T18:35:15.082972: step 5558, loss 0.603341.
Train: 2018-08-05T18:35:15.270397: step 5559, loss 0.652626.
Train: 2018-08-05T18:35:15.442262: step 5560, loss 0.570765.
Test: 2018-08-05T18:35:16.504484: step 5560, loss 0.548915.
Train: 2018-08-05T18:35:16.676348: step 5561, loss 0.570766.
Train: 2018-08-05T18:35:16.848152: step 5562, loss 0.464384.
Train: 2018-08-05T18:35:17.020019: step 5563, loss 0.554278.
Train: 2018-08-05T18:35:17.207475: step 5564, loss 0.545863.
Train: 2018-08-05T18:35:17.379309: step 5565, loss 0.579137.
Train: 2018-08-05T18:35:17.566765: step 5566, loss 0.553961.
Train: 2018-08-05T18:35:17.738600: step 5567, loss 0.553884.
Train: 2018-08-05T18:35:17.926027: step 5568, loss 0.494141.
Train: 2018-08-05T18:35:18.097890: step 5569, loss 0.58828.
Train: 2018-08-05T18:35:18.269727: step 5570, loss 0.527662.
Test: 2018-08-05T18:35:19.331976: step 5570, loss 0.547973.
Train: 2018-08-05T18:35:19.519434: step 5571, loss 0.597348.
Train: 2018-08-05T18:35:19.691267: step 5572, loss 0.562399.
Train: 2018-08-05T18:35:19.863073: step 5573, loss 0.641835.
Train: 2018-08-05T18:35:20.050559: step 5574, loss 0.615276.
Train: 2018-08-05T18:35:20.222363: step 5575, loss 0.650056.
Train: 2018-08-05T18:35:20.409820: step 5576, loss 0.605744.
Train: 2018-08-05T18:35:20.581684: step 5577, loss 0.468001.
Train: 2018-08-05T18:35:20.769141: step 5578, loss 0.536742.
Train: 2018-08-05T18:35:20.940945: step 5579, loss 0.596371.
Train: 2018-08-05T18:35:21.112780: step 5580, loss 0.570822.
Test: 2018-08-05T18:35:22.175061: step 5580, loss 0.548687.
Train: 2018-08-05T18:35:22.346865: step 5581, loss 0.537092.
Train: 2018-08-05T18:35:22.534352: step 5582, loss 0.553977.
Train: 2018-08-05T18:35:22.706186: step 5583, loss 0.612751.
Train: 2018-08-05T18:35:22.893644: step 5584, loss 0.512265.
Train: 2018-08-05T18:35:23.081093: step 5585, loss 0.562415.
Train: 2018-08-05T18:35:23.252903: step 5586, loss 0.512289.
Train: 2018-08-05T18:35:23.393527: step 5587, loss 0.544513.
Train: 2018-08-05T18:35:23.580982: step 5588, loss 0.604477.
Train: 2018-08-05T18:35:23.768439: step 5589, loss 0.58767.
Train: 2018-08-05T18:35:23.955894: step 5590, loss 0.579236.
Test: 2018-08-05T18:35:25.018146: step 5590, loss 0.548688.
Train: 2018-08-05T18:35:25.205596: step 5591, loss 0.54552.
Train: 2018-08-05T18:35:25.377436: step 5592, loss 0.587661.
Train: 2018-08-05T18:35:25.564892: step 5593, loss 0.511845.
Train: 2018-08-05T18:35:25.752342: step 5594, loss 0.528596.
Train: 2018-08-05T18:35:25.939775: step 5595, loss 0.51147.
Train: 2018-08-05T18:35:26.127260: step 5596, loss 0.502544.
Train: 2018-08-05T18:35:26.314712: step 5597, loss 0.510563.
Train: 2018-08-05T18:35:26.502142: step 5598, loss 0.544902.
Train: 2018-08-05T18:35:26.689628: step 5599, loss 0.597776.
Train: 2018-08-05T18:35:26.877085: step 5600, loss 0.589207.
Test: 2018-08-05T18:35:27.939307: step 5600, loss 0.5478.
Train: 2018-08-05T18:35:28.845374: step 5601, loss 0.517798.
Train: 2018-08-05T18:35:29.032799: step 5602, loss 0.553601.
Train: 2018-08-05T18:35:29.220286: step 5603, loss 0.607942.
Train: 2018-08-05T18:35:29.392122: step 5604, loss 0.608025.
Train: 2018-08-05T18:35:29.579577: step 5605, loss 0.535582.
Train: 2018-08-05T18:35:29.767027: step 5606, loss 0.517287.
Train: 2018-08-05T18:35:29.970111: step 5607, loss 0.506642.
Train: 2018-08-05T18:35:30.157536: step 5608, loss 0.65498.
Train: 2018-08-05T18:35:30.345022: step 5609, loss 0.544856.
Train: 2018-08-05T18:35:30.532480: step 5610, loss 0.572035.
Test: 2018-08-05T18:35:31.594731: step 5610, loss 0.548245.
Train: 2018-08-05T18:35:31.782187: step 5611, loss 0.589151.
Train: 2018-08-05T18:35:32.016508: step 5612, loss 0.580044.
Train: 2018-08-05T18:35:32.219584: step 5613, loss 0.536173.
Train: 2018-08-05T18:35:32.407041: step 5614, loss 0.518997.
Train: 2018-08-05T18:35:32.594496: step 5615, loss 0.562339.
Train: 2018-08-05T18:35:32.781953: step 5616, loss 0.51072.
Train: 2018-08-05T18:35:32.969408: step 5617, loss 0.579668.
Train: 2018-08-05T18:35:33.172456: step 5618, loss 0.648188.
Train: 2018-08-05T18:35:33.359942: step 5619, loss 0.59645.
Train: 2018-08-05T18:35:33.547398: step 5620, loss 0.536985.
Test: 2018-08-05T18:35:34.594028: step 5620, loss 0.548142.
Train: 2018-08-05T18:35:34.797105: step 5621, loss 0.604432.
Train: 2018-08-05T18:35:34.984562: step 5622, loss 0.579115.
Train: 2018-08-05T18:35:35.172017: step 5623, loss 0.587326.
Train: 2018-08-05T18:35:35.375096: step 5624, loss 0.521449.
Train: 2018-08-05T18:35:35.578172: step 5625, loss 0.56258.
Train: 2018-08-05T18:35:35.765622: step 5626, loss 0.546292.
Train: 2018-08-05T18:35:35.968706: step 5627, loss 0.521868.
Train: 2018-08-05T18:35:36.156163: step 5628, loss 0.562599.
Train: 2018-08-05T18:35:36.359239: step 5629, loss 0.488849.
Train: 2018-08-05T18:35:36.546695: step 5630, loss 0.612029.
Test: 2018-08-05T18:35:37.608947: step 5630, loss 0.548434.
Train: 2018-08-05T18:35:37.796372: step 5631, loss 0.521022.
Train: 2018-08-05T18:35:37.983858: step 5632, loss 0.60415.
Train: 2018-08-05T18:35:38.186907: step 5633, loss 0.579155.
Train: 2018-08-05T18:35:38.389983: step 5634, loss 0.537188.
Train: 2018-08-05T18:35:38.593091: step 5635, loss 0.579238.
Train: 2018-08-05T18:35:38.780548: step 5636, loss 0.604653.
Train: 2018-08-05T18:35:38.983625: step 5637, loss 0.545439.
Train: 2018-08-05T18:35:39.171052: step 5638, loss 0.604706.
Train: 2018-08-05T18:35:39.374158: step 5639, loss 0.536983.
Train: 2018-08-05T18:35:39.577237: step 5640, loss 0.553895.
Test: 2018-08-05T18:35:40.639488: step 5640, loss 0.54684.
Train: 2018-08-05T18:35:40.826943: step 5641, loss 0.519995.
Train: 2018-08-05T18:35:41.030022: step 5642, loss 0.545338.
Train: 2018-08-05T18:35:41.217480: step 5643, loss 0.528173.
Train: 2018-08-05T18:35:41.420554: step 5644, loss 0.484995.
Train: 2018-08-05T18:35:41.639254: step 5645, loss 0.484225.
Train: 2018-08-05T18:35:41.842330: step 5646, loss 0.588802.
Train: 2018-08-05T18:35:42.029781: step 5647, loss 0.518037.
Train: 2018-08-05T18:35:42.232835: step 5648, loss 0.60752.
Train: 2018-08-05T18:35:42.435941: step 5649, loss 0.616944.
Train: 2018-08-05T18:35:42.638989: step 5650, loss 0.626143.
Test: 2018-08-05T18:35:43.701270: step 5650, loss 0.547347.
Train: 2018-08-05T18:35:43.904348: step 5651, loss 0.508411.
Train: 2018-08-05T18:35:44.107425: step 5652, loss 0.544573.
Train: 2018-08-05T18:35:44.310504: step 5653, loss 0.517449.
Train: 2018-08-05T18:35:44.497958: step 5654, loss 0.589857.
Train: 2018-08-05T18:35:44.716627: step 5655, loss 0.580779.
Train: 2018-08-05T18:35:44.919736: step 5656, loss 0.580705.
Train: 2018-08-05T18:35:45.122782: step 5657, loss 0.607571.
Train: 2018-08-05T18:35:45.310268: step 5658, loss 0.52679.
Train: 2018-08-05T18:35:45.513347: step 5659, loss 0.598039.
Train: 2018-08-05T18:35:45.716392: step 5660, loss 0.580082.
Test: 2018-08-05T18:35:46.778644: step 5660, loss 0.546951.
Train: 2018-08-05T18:35:46.981752: step 5661, loss 0.623678.
Train: 2018-08-05T18:35:47.184829: step 5662, loss 0.527911.
Train: 2018-08-05T18:35:47.387913: step 5663, loss 0.510794.
Train: 2018-08-05T18:35:47.590984: step 5664, loss 0.502444.
Train: 2018-08-05T18:35:47.794061: step 5665, loss 0.579446.
Train: 2018-08-05T18:35:47.997139: step 5666, loss 0.570882.
Train: 2018-08-05T18:35:48.215837: step 5667, loss 0.494094.
Train: 2018-08-05T18:35:48.418886: step 5668, loss 0.605107.
Train: 2018-08-05T18:35:48.621994: step 5669, loss 0.596548.
Train: 2018-08-05T18:35:48.825069: step 5670, loss 0.536738.
Test: 2018-08-05T18:35:49.887292: step 5670, loss 0.548094.
Train: 2018-08-05T18:35:50.121613: step 5671, loss 0.53675.
Train: 2018-08-05T18:35:50.324719: step 5672, loss 0.562338.
Train: 2018-08-05T18:35:50.527767: step 5673, loss 0.53669.
Train: 2018-08-05T18:35:50.746495: step 5674, loss 0.588047.
Train: 2018-08-05T18:35:50.949542: step 5675, loss 0.519454.
Train: 2018-08-05T18:35:51.152621: step 5676, loss 0.536526.
Train: 2018-08-05T18:35:51.371349: step 5677, loss 0.56234.
Train: 2018-08-05T18:35:51.574427: step 5678, loss 0.60571.
Train: 2018-08-05T18:35:51.777474: step 5679, loss 0.571025.
Train: 2018-08-05T18:35:51.996203: step 5680, loss 0.562348.
Test: 2018-08-05T18:35:53.042832: step 5680, loss 0.548466.
Train: 2018-08-05T18:35:53.261533: step 5681, loss 0.484321.
Train: 2018-08-05T18:35:53.464609: step 5682, loss 0.536241.
Train: 2018-08-05T18:35:53.667656: step 5683, loss 0.579875.
Train: 2018-08-05T18:35:53.886386: step 5684, loss 0.553616.
Train: 2018-08-05T18:35:54.089433: step 5685, loss 0.57121.
Train: 2018-08-05T18:35:54.308164: step 5686, loss 0.580052.
Train: 2018-08-05T18:35:54.511209: step 5687, loss 0.562419.
Train: 2018-08-05T18:35:54.714316: step 5688, loss 0.571229.
Train: 2018-08-05T18:35:54.933016: step 5689, loss 0.571209.
Train: 2018-08-05T18:35:55.151684: step 5690, loss 0.588743.
Test: 2018-08-05T18:35:56.198343: step 5690, loss 0.547566.
Train: 2018-08-05T18:35:56.417042: step 5691, loss 0.562376.
Train: 2018-08-05T18:35:56.620119: step 5692, loss 0.53622.
Train: 2018-08-05T18:35:56.838818: step 5693, loss 0.518879.
Train: 2018-08-05T18:35:57.041896: step 5694, loss 0.588451.
Train: 2018-08-05T18:35:57.260596: step 5695, loss 0.518921.
Train: 2018-08-05T18:35:57.463675: step 5696, loss 0.544963.
Train: 2018-08-05T18:35:57.682341: step 5697, loss 0.605915.
Train: 2018-08-05T18:35:57.885450: step 5698, loss 0.510158.
Train: 2018-08-05T18:35:58.104147: step 5699, loss 0.553647.
Train: 2018-08-05T18:35:58.322815: step 5700, loss 0.544907.
Test: 2018-08-05T18:35:59.369445: step 5700, loss 0.54736.
Train: 2018-08-05T18:36:00.353621: step 5701, loss 0.544876.
Train: 2018-08-05T18:36:00.572315: step 5702, loss 0.571169.
Train: 2018-08-05T18:36:00.790987: step 5703, loss 0.562401.
Train: 2018-08-05T18:36:00.994095: step 5704, loss 0.544806.
Train: 2018-08-05T18:36:01.212796: step 5705, loss 0.588862.
Train: 2018-08-05T18:36:01.431462: step 5706, loss 0.571224.
Train: 2018-08-05T18:36:01.634540: step 5707, loss 0.571202.
Train: 2018-08-05T18:36:01.853269: step 5708, loss 0.588722.
Train: 2018-08-05T18:36:02.071968: step 5709, loss 0.527409.
Train: 2018-08-05T18:36:02.290667: step 5710, loss 0.597267.
Test: 2018-08-05T18:36:03.337267: step 5710, loss 0.54964.
Train: 2018-08-05T18:36:03.555965: step 5711, loss 0.518904.
Train: 2018-08-05T18:36:03.759043: step 5712, loss 0.605742.
Train: 2018-08-05T18:36:03.977742: step 5713, loss 0.545053.
Train: 2018-08-05T18:36:04.196471: step 5714, loss 0.579577.
Train: 2018-08-05T18:36:04.415169: step 5715, loss 0.562335.
Train: 2018-08-05T18:36:04.633868: step 5716, loss 0.510977.
Train: 2018-08-05T18:36:04.852567: step 5717, loss 0.588018.
Train: 2018-08-05T18:36:05.055648: step 5718, loss 0.553789.
Train: 2018-08-05T18:36:05.274313: step 5719, loss 0.528176.
Train: 2018-08-05T18:36:05.493042: step 5720, loss 0.562336.
Test: 2018-08-05T18:36:06.539671: step 5720, loss 0.548824.
Train: 2018-08-05T18:36:06.758372: step 5721, loss 0.639398.
Train: 2018-08-05T18:36:06.977073: step 5722, loss 0.604992.
Train: 2018-08-05T18:36:07.195768: step 5723, loss 0.621707.
Train: 2018-08-05T18:36:07.414467: step 5724, loss 0.562385.
Train: 2018-08-05T18:36:07.633167: step 5725, loss 0.579101.
Train: 2018-08-05T18:36:07.851864: step 5726, loss 0.628644.
Train: 2018-08-05T18:36:08.070563: step 5727, loss 0.570795.
Train: 2018-08-05T18:36:08.289262: step 5728, loss 0.570839.
Train: 2018-08-05T18:36:08.507962: step 5729, loss 0.554775.
Train: 2018-08-05T18:36:08.726660: step 5730, loss 0.507027.
Test: 2018-08-05T18:36:09.773291: step 5730, loss 0.551064.
Train: 2018-08-05T18:36:09.991988: step 5731, loss 0.57886.
Train: 2018-08-05T18:36:10.210687: step 5732, loss 0.594791.
Train: 2018-08-05T18:36:10.429386: step 5733, loss 0.578858.
Train: 2018-08-05T18:36:10.648086: step 5734, loss 0.634388.
Train: 2018-08-05T18:36:10.866783: step 5735, loss 0.563085.
Train: 2018-08-05T18:36:11.085485: step 5736, loss 0.547425.
Train: 2018-08-05T18:36:11.288560: step 5737, loss 0.610635.
Train: 2018-08-05T18:36:11.476017: step 5738, loss 0.479818.
Train: 2018-08-05T18:36:11.694714: step 5739, loss 0.500365.
Train: 2018-08-05T18:36:11.913414: step 5740, loss 0.570939.
Test: 2018-08-05T18:36:12.975664: step 5740, loss 0.550284.
Train: 2018-08-05T18:36:13.194363: step 5741, loss 0.538926.
Train: 2018-08-05T18:36:13.397411: step 5742, loss 0.538545.
Train: 2018-08-05T18:36:13.631762: step 5743, loss 0.554453.
Train: 2018-08-05T18:36:13.850429: step 5744, loss 0.545998.
Train: 2018-08-05T18:36:14.069128: step 5745, loss 0.512303.
Train: 2018-08-05T18:36:14.287827: step 5746, loss 0.510845.
Train: 2018-08-05T18:36:14.506558: step 5747, loss 0.544525.
Train: 2018-08-05T18:36:14.725255: step 5748, loss 0.617234.
Train: 2018-08-05T18:36:14.943953: step 5749, loss 0.488031.
Train: 2018-08-05T18:36:15.178245: step 5750, loss 0.526002.
Test: 2018-08-05T18:36:16.224873: step 5750, loss 0.546912.
Train: 2018-08-05T18:36:16.443604: step 5751, loss 0.581315.
Train: 2018-08-05T18:36:16.677923: step 5752, loss 0.562753.
Train: 2018-08-05T18:36:16.896623: step 5753, loss 0.617016.
Train: 2018-08-05T18:36:17.115291: step 5754, loss 0.589703.
Train: 2018-08-05T18:36:17.334019: step 5755, loss 0.634615.
Train: 2018-08-05T18:36:17.552718: step 5756, loss 0.562526.
Train: 2018-08-05T18:36:17.771418: step 5757, loss 0.606828.
Train: 2018-08-05T18:36:17.990086: step 5758, loss 0.579973.
Train: 2018-08-05T18:36:18.224407: step 5759, loss 0.658027.
Train: 2018-08-05T18:36:18.443134: step 5760, loss 0.562336.
Test: 2018-08-05T18:36:19.489734: step 5760, loss 0.547902.
Train: 2018-08-05T18:36:19.708463: step 5761, loss 0.562371.
Train: 2018-08-05T18:36:19.927161: step 5762, loss 0.595733.
Train: 2018-08-05T18:36:20.145861: step 5763, loss 0.570777.
Train: 2018-08-05T18:36:20.364560: step 5764, loss 0.55481.
Train: 2018-08-05T18:36:20.583258: step 5765, loss 0.570821.
Train: 2018-08-05T18:36:20.817579: step 5766, loss 0.594848.
Train: 2018-08-05T18:36:21.036247: step 5767, loss 0.563077.
Train: 2018-08-05T18:36:21.254946: step 5768, loss 0.602417.
Train: 2018-08-05T18:36:21.473675: step 5769, loss 0.5789.
Train: 2018-08-05T18:36:21.692374: step 5770, loss 0.571198.
Test: 2018-08-05T18:36:22.738974: step 5770, loss 0.550322.
Train: 2018-08-05T18:36:22.957702: step 5771, loss 0.563543.
Train: 2018-08-05T18:36:23.176371: step 5772, loss 0.501982.
Train: 2018-08-05T18:36:23.395101: step 5773, loss 0.478472.
Train: 2018-08-05T18:36:23.613798: step 5774, loss 0.555479.
Train: 2018-08-05T18:36:23.832468: step 5775, loss 0.586751.
Train: 2018-08-05T18:36:24.051196: step 5776, loss 0.626619.
Train: 2018-08-05T18:36:24.269896: step 5777, loss 0.490927.
Train: 2018-08-05T18:36:24.488594: step 5778, loss 0.538498.
Train: 2018-08-05T18:36:24.707292: step 5779, loss 0.537614.
Train: 2018-08-05T18:36:24.925991: step 5780, loss 0.553614.
Test: 2018-08-05T18:36:25.988213: step 5780, loss 0.547035.
Train: 2018-08-05T18:36:26.206941: step 5781, loss 0.477211.
Train: 2018-08-05T18:36:26.425640: step 5782, loss 0.599162.
Train: 2018-08-05T18:36:26.644340: step 5783, loss 0.566936.
Train: 2018-08-05T18:36:26.863038: step 5784, loss 0.647751.
Train: 2018-08-05T18:36:27.081736: step 5785, loss 0.57096.
Train: 2018-08-05T18:36:27.300435: step 5786, loss 0.495111.
Train: 2018-08-05T18:36:27.519135: step 5787, loss 0.587596.
Train: 2018-08-05T18:36:27.737832: step 5788, loss 0.570782.
Train: 2018-08-05T18:36:27.956532: step 5789, loss 0.512174.
Train: 2018-08-05T18:36:28.175230: step 5790, loss 0.55401.
Test: 2018-08-05T18:36:29.237482: step 5790, loss 0.547961.
Train: 2018-08-05T18:36:29.440559: step 5791, loss 0.570789.
Train: 2018-08-05T18:36:29.659258: step 5792, loss 0.52028.
Train: 2018-08-05T18:36:29.877958: step 5793, loss 0.511621.
Train: 2018-08-05T18:36:30.112278: step 5794, loss 0.639003.
Train: 2018-08-05T18:36:30.330976: step 5795, loss 0.587939.
Train: 2018-08-05T18:36:30.549675: step 5796, loss 0.570872.
Train: 2018-08-05T18:36:30.768374: step 5797, loss 0.656166.
Train: 2018-08-05T18:36:30.987072: step 5798, loss 0.536922.
Train: 2018-08-05T18:36:31.205770: step 5799, loss 0.52859.
Train: 2018-08-05T18:36:31.424469: step 5800, loss 0.469606.
Test: 2018-08-05T18:36:32.486691: step 5800, loss 0.547031.
Train: 2018-08-05T18:36:33.533350: step 5801, loss 0.519984.
Train: 2018-08-05T18:36:33.752051: step 5802, loss 0.536737.
Train: 2018-08-05T18:36:33.955129: step 5803, loss 0.631149.
Train: 2018-08-05T18:36:34.189417: step 5804, loss 0.501957.
Train: 2018-08-05T18:36:34.408149: step 5805, loss 0.571027.
Train: 2018-08-05T18:36:34.626846: step 5806, loss 0.510049.
Train: 2018-08-05T18:36:34.845544: step 5807, loss 0.553614.
Train: 2018-08-05T18:36:35.079865: step 5808, loss 0.500568.
Train: 2018-08-05T18:36:35.298565: step 5809, loss 0.500079.
Train: 2018-08-05T18:36:35.517262: step 5810, loss 0.562623.
Test: 2018-08-05T18:36:36.563890: step 5810, loss 0.547334.
Train: 2018-08-05T18:36:36.782589: step 5811, loss 0.508134.
Train: 2018-08-05T18:36:37.001259: step 5812, loss 0.572106.
Train: 2018-08-05T18:36:37.219958: step 5813, loss 0.572321.
Train: 2018-08-05T18:36:37.438686: step 5814, loss 0.563157.
Train: 2018-08-05T18:36:37.657385: step 5815, loss 0.581941.
Train: 2018-08-05T18:36:37.876054: step 5816, loss 0.572615.
Train: 2018-08-05T18:36:38.094782: step 5817, loss 0.525819.
Train: 2018-08-05T18:36:38.329103: step 5818, loss 0.516469.
Train: 2018-08-05T18:36:38.532151: step 5819, loss 0.535164.
Train: 2018-08-05T18:36:38.750880: step 5820, loss 0.525779.
Test: 2018-08-05T18:36:39.813101: step 5820, loss 0.548506.
Train: 2018-08-05T18:36:40.031829: step 5821, loss 0.676094.
Train: 2018-08-05T18:36:40.250529: step 5822, loss 0.525837.
Train: 2018-08-05T18:36:40.469226: step 5823, loss 0.581717.
Train: 2018-08-05T18:36:40.687926: step 5824, loss 0.618455.
Train: 2018-08-05T18:36:40.906595: step 5825, loss 0.581131.
Train: 2018-08-05T18:36:41.125295: step 5826, loss 0.544566.
Train: 2018-08-05T18:36:41.344021: step 5827, loss 0.517734.
Train: 2018-08-05T18:36:41.562691: step 5828, loss 0.553588.
Train: 2018-08-05T18:36:41.781419: step 5829, loss 0.624382.
Train: 2018-08-05T18:36:42.000119: step 5830, loss 0.527335.
Test: 2018-08-05T18:36:43.062370: step 5830, loss 0.547025.
Train: 2018-08-05T18:36:43.265447: step 5831, loss 0.605857.
Train: 2018-08-05T18:36:43.484145: step 5832, loss 0.562337.
Train: 2018-08-05T18:36:43.702844: step 5833, loss 0.60504.
Train: 2018-08-05T18:36:43.921545: step 5834, loss 0.595743.
Train: 2018-08-05T18:36:44.140242: step 5835, loss 0.510054.
Train: 2018-08-05T18:36:44.358936: step 5836, loss 0.538383.
Train: 2018-08-05T18:36:44.577639: step 5837, loss 0.645572.
Train: 2018-08-05T18:36:44.796338: step 5838, loss 0.570757.
Train: 2018-08-05T18:36:45.015038: step 5839, loss 0.562572.
Train: 2018-08-05T18:36:45.218114: step 5840, loss 0.505578.
Test: 2018-08-05T18:36:46.280336: step 5840, loss 0.549399.
Train: 2018-08-05T18:36:46.499065: step 5841, loss 0.56263.
Train: 2018-08-05T18:36:46.702144: step 5842, loss 0.530028.
Train: 2018-08-05T18:36:46.920840: step 5843, loss 0.53807.
Train: 2018-08-05T18:36:47.139540: step 5844, loss 0.620065.
Train: 2018-08-05T18:36:47.358239: step 5845, loss 0.578983.
Train: 2018-08-05T18:36:47.576939: step 5846, loss 0.521378.
Train: 2018-08-05T18:36:47.795605: step 5847, loss 0.512934.
Train: 2018-08-05T18:36:48.014336: step 5848, loss 0.520856.
Train: 2018-08-05T18:36:48.217413: step 5849, loss 0.595966.
Train: 2018-08-05T18:36:48.436111: step 5850, loss 0.604602.
Test: 2018-08-05T18:36:49.498332: step 5850, loss 0.547418.
Train: 2018-08-05T18:36:49.717061: step 5851, loss 0.570828.
Train: 2018-08-05T18:36:49.935759: step 5852, loss 0.562346.
Train: 2018-08-05T18:36:50.154458: step 5853, loss 0.58789.
Train: 2018-08-05T18:36:50.357537: step 5854, loss 0.536779.
Train: 2018-08-05T18:36:50.572095: step 5855, loss 0.61357.
Train: 2018-08-05T18:36:50.790827: step 5856, loss 0.570868.
Train: 2018-08-05T18:36:51.009522: step 5857, loss 0.545314.
Train: 2018-08-05T18:36:51.228222: step 5858, loss 0.562343.
Train: 2018-08-05T18:36:51.446920: step 5859, loss 0.664442.
Train: 2018-08-05T18:36:51.649997: step 5860, loss 0.629983.
Test: 2018-08-05T18:36:52.712219: step 5860, loss 0.548236.
Train: 2018-08-05T18:36:52.915297: step 5861, loss 0.620988.
Train: 2018-08-05T18:36:53.134024: step 5862, loss 0.579022.
Train: 2018-08-05T18:36:53.352694: step 5863, loss 0.538098.
Train: 2018-08-05T18:36:53.555801: step 5864, loss 0.570798.
Train: 2018-08-05T18:36:53.774470: step 5865, loss 0.530645.
Train: 2018-08-05T18:36:53.993198: step 5866, loss 0.674948.
Train: 2018-08-05T18:36:54.196278: step 5867, loss 0.523365.
Train: 2018-08-05T18:36:54.414979: step 5868, loss 0.547323.
Train: 2018-08-05T18:36:54.618054: step 5869, loss 0.52382.
Train: 2018-08-05T18:36:54.836721: step 5870, loss 0.515871.
Test: 2018-08-05T18:36:55.898972: step 5870, loss 0.549074.
Train: 2018-08-05T18:36:56.102080: step 5871, loss 0.570942.
Train: 2018-08-05T18:36:56.305157: step 5872, loss 0.554979.
Train: 2018-08-05T18:36:56.523856: step 5873, loss 0.618916.
Train: 2018-08-05T18:36:56.742554: step 5874, loss 0.594937.
Train: 2018-08-05T18:36:56.945603: step 5875, loss 0.554744.
Train: 2018-08-05T18:36:57.164301: step 5876, loss 0.586945.
Train: 2018-08-05T18:36:57.367379: step 5877, loss 0.51428.
Train: 2018-08-05T18:36:57.570486: step 5878, loss 0.538316.
Train: 2018-08-05T18:36:57.789155: step 5879, loss 0.627991.
Train: 2018-08-05T18:36:57.992233: step 5880, loss 0.611745.
Test: 2018-08-05T18:36:59.054514: step 5880, loss 0.548128.
Train: 2018-08-05T18:36:59.257591: step 5881, loss 0.505205.
Train: 2018-08-05T18:36:59.476290: step 5882, loss 0.587214.
Train: 2018-08-05T18:36:59.679338: step 5883, loss 0.52951.
Train: 2018-08-05T18:36:59.898066: step 5884, loss 0.51273.
Train: 2018-08-05T18:37:00.101143: step 5885, loss 0.604193.
Train: 2018-08-05T18:37:00.304190: step 5886, loss 0.537201.
Train: 2018-08-05T18:37:00.522919: step 5887, loss 0.579258.
Train: 2018-08-05T18:37:00.741614: step 5888, loss 0.528403.
Train: 2018-08-05T18:37:00.913457: step 5889, loss 0.562338.
Train: 2018-08-05T18:37:01.116530: step 5890, loss 0.579512.
Test: 2018-08-05T18:37:02.163131: step 5890, loss 0.547738.
Train: 2018-08-05T18:37:02.381860: step 5891, loss 0.648567.
Train: 2018-08-05T18:37:02.584937: step 5892, loss 0.527907.
Train: 2018-08-05T18:37:02.788018: step 5893, loss 0.493454.
Train: 2018-08-05T18:37:03.006682: step 5894, loss 0.493156.
Train: 2018-08-05T18:37:03.209792: step 5895, loss 0.658253.
Train: 2018-08-05T18:37:03.412872: step 5896, loss 0.55364.
Train: 2018-08-05T18:37:03.615945: step 5897, loss 0.597321.
Train: 2018-08-05T18:37:03.819023: step 5898, loss 0.579818.
Train: 2018-08-05T18:37:04.037692: step 5899, loss 0.562357.
Train: 2018-08-05T18:37:04.240769: step 5900, loss 0.56235.
Test: 2018-08-05T18:37:05.303020: step 5900, loss 0.546685.
Train: 2018-08-05T18:37:06.318408: step 5901, loss 0.657602.
Train: 2018-08-05T18:37:06.521484: step 5902, loss 0.579513.
Train: 2018-08-05T18:37:06.724592: step 5903, loss 0.511254.
Train: 2018-08-05T18:37:06.927670: step 5904, loss 0.630168.
Train: 2018-08-05T18:37:07.130747: step 5905, loss 0.579198.
Train: 2018-08-05T18:37:07.333795: step 5906, loss 0.537411.
Train: 2018-08-05T18:37:07.536872: step 5907, loss 0.587343.
Train: 2018-08-05T18:37:07.739982: step 5908, loss 0.48834.
Train: 2018-08-05T18:37:07.943057: step 5909, loss 0.554279.
Train: 2018-08-05T18:37:08.146134: step 5910, loss 0.537776.
Test: 2018-08-05T18:37:09.192764: step 5910, loss 0.548105.
Train: 2018-08-05T18:37:09.411433: step 5911, loss 0.537678.
Train: 2018-08-05T18:37:09.614540: step 5912, loss 0.520816.
Train: 2018-08-05T18:37:09.817618: step 5913, loss 0.588263.
Train: 2018-08-05T18:37:10.020695: step 5914, loss 0.528317.
Train: 2018-08-05T18:37:10.208152: step 5915, loss 0.562338.
Train: 2018-08-05T18:37:10.411198: step 5916, loss 0.588577.
Train: 2018-08-05T18:37:10.614306: step 5917, loss 0.674671.
Train: 2018-08-05T18:37:10.817384: step 5918, loss 0.579323.
Train: 2018-08-05T18:37:11.004841: step 5919, loss 0.579249.
Train: 2018-08-05T18:37:11.207917: step 5920, loss 0.58758.
Test: 2018-08-05T18:37:12.270169: step 5920, loss 0.54906.
Train: 2018-08-05T18:37:12.473215: step 5921, loss 0.529038.
Train: 2018-08-05T18:37:12.660672: step 5922, loss 0.595721.
Train: 2018-08-05T18:37:12.863750: step 5923, loss 0.512783.
Train: 2018-08-05T18:37:13.051236: step 5924, loss 0.562479.
Train: 2018-08-05T18:37:13.254313: step 5925, loss 0.56248.
Train: 2018-08-05T18:37:13.457391: step 5926, loss 0.603877.
Train: 2018-08-05T18:37:13.644846: step 5927, loss 0.603816.
Train: 2018-08-05T18:37:13.847893: step 5928, loss 0.546058.
Train: 2018-08-05T18:37:14.051002: step 5929, loss 0.53789.
Train: 2018-08-05T18:37:14.238458: step 5930, loss 0.546097.
Test: 2018-08-05T18:37:15.300709: step 5930, loss 0.548956.
Train: 2018-08-05T18:37:15.503787: step 5931, loss 0.529576.
Train: 2018-08-05T18:37:15.691242: step 5932, loss 0.570756.
Train: 2018-08-05T18:37:15.894320: step 5933, loss 0.496031.
Train: 2018-08-05T18:37:16.097397: step 5934, loss 0.554036.
Train: 2018-08-05T18:37:16.284857: step 5935, loss 0.553932.
Train: 2018-08-05T18:37:16.487931: step 5936, loss 0.451812.
Train: 2018-08-05T18:37:16.675387: step 5937, loss 0.570958.
Train: 2018-08-05T18:37:16.878464: step 5938, loss 0.553642.
Train: 2018-08-05T18:37:17.065920: step 5939, loss 0.650584.
Train: 2018-08-05T18:37:17.268998: step 5940, loss 0.52704.
Test: 2018-08-05T18:37:18.315599: step 5940, loss 0.547832.
Train: 2018-08-05T18:37:18.518676: step 5941, loss 0.589176.
Train: 2018-08-05T18:37:18.706161: step 5942, loss 0.571426.
Train: 2018-08-05T18:37:18.909238: step 5943, loss 0.508945.
Train: 2018-08-05T18:37:19.096665: step 5944, loss 0.589432.
Train: 2018-08-05T18:37:19.299773: step 5945, loss 0.57153.
Train: 2018-08-05T18:37:19.487199: step 5946, loss 0.553592.
Train: 2018-08-05T18:37:19.690306: step 5947, loss 0.607356.
Train: 2018-08-05T18:37:19.877731: step 5948, loss 0.571443.
Train: 2018-08-05T18:37:20.065218: step 5949, loss 0.544702.
Train: 2018-08-05T18:37:20.252674: step 5950, loss 0.589015.
Test: 2018-08-05T18:37:21.314896: step 5950, loss 0.54973.
Train: 2018-08-05T18:37:21.502382: step 5951, loss 0.562413.
Train: 2018-08-05T18:37:21.689838: step 5952, loss 0.483535.
Train: 2018-08-05T18:37:21.877294: step 5953, loss 0.623729.
Train: 2018-08-05T18:37:22.064719: step 5954, loss 0.562365.
Train: 2018-08-05T18:37:22.252206: step 5955, loss 0.562351.
Train: 2018-08-05T18:37:22.439663: step 5956, loss 0.562342.
Train: 2018-08-05T18:37:22.627119: step 5957, loss 0.562337.
Train: 2018-08-05T18:37:22.814575: step 5958, loss 0.502221.
Train: 2018-08-05T18:37:23.002030: step 5959, loss 0.656837.
Train: 2018-08-05T18:37:23.189487: step 5960, loss 0.519617.
Test: 2018-08-05T18:37:24.251741: step 5960, loss 0.548497.
Train: 2018-08-05T18:37:24.439194: step 5961, loss 0.587913.
Train: 2018-08-05T18:37:24.626650: step 5962, loss 0.621799.
Train: 2018-08-05T18:37:24.814106: step 5963, loss 0.49489.
Train: 2018-08-05T18:37:25.001562: step 5964, loss 0.570796.
Train: 2018-08-05T18:37:25.188988: step 5965, loss 0.486762.
Train: 2018-08-05T18:37:25.376474: step 5966, loss 0.621374.
Train: 2018-08-05T18:37:25.563930: step 5967, loss 0.545527.
Train: 2018-08-05T18:37:25.751387: step 5968, loss 0.638231.
Train: 2018-08-05T18:37:25.938843: step 5969, loss 0.562389.
Train: 2018-08-05T18:37:26.126298: step 5970, loss 0.579142.
Test: 2018-08-05T18:37:27.172899: step 5970, loss 0.546971.
Train: 2018-08-05T18:37:27.360355: step 5971, loss 0.554092.
Train: 2018-08-05T18:37:27.547841: step 5972, loss 0.587391.
Train: 2018-08-05T18:37:27.719646: step 5973, loss 0.554185.
Train: 2018-08-05T18:37:27.907101: step 5974, loss 0.55422.
Train: 2018-08-05T18:37:28.094588: step 5975, loss 0.562497.
Train: 2018-08-05T18:37:28.282043: step 5976, loss 0.554245.
Train: 2018-08-05T18:37:28.469501: step 5977, loss 0.554235.
Train: 2018-08-05T18:37:28.641335: step 5978, loss 0.620394.
Train: 2018-08-05T18:37:28.828762: step 5979, loss 0.579013.
Train: 2018-08-05T18:37:29.031867: step 5980, loss 0.546042.
Test: 2018-08-05T18:37:30.078469: step 5980, loss 0.548771.
Train: 2018-08-05T18:37:30.265925: step 5981, loss 0.603694.
Train: 2018-08-05T18:37:30.453405: step 5982, loss 0.472206.
Train: 2018-08-05T18:37:30.640861: step 5983, loss 0.521284.
Train: 2018-08-05T18:37:30.812701: step 5984, loss 0.612261.
Train: 2018-08-05T18:37:31.000158: step 5985, loss 0.587418.
Train: 2018-08-05T18:37:31.187613: step 5986, loss 0.570766.
Train: 2018-08-05T18:37:31.375069: step 5987, loss 0.554065.
Train: 2018-08-05T18:37:31.546875: step 5988, loss 0.537299.
Train: 2018-08-05T18:37:31.734361: step 5989, loss 0.562387.
Train: 2018-08-05T18:37:31.921789: step 5990, loss 0.629805.
Test: 2018-08-05T18:37:33.030902: step 5990, loss 0.549085.
Train: 2018-08-05T18:37:33.202767: step 5991, loss 0.553953.
Train: 2018-08-05T18:37:33.390223: step 5992, loss 0.553953.
Train: 2018-08-05T18:37:33.562058: step 5993, loss 0.612936.
Train: 2018-08-05T18:37:33.749484: step 5994, loss 0.579197.
Train: 2018-08-05T18:37:33.936970: step 5995, loss 0.520484.
Train: 2018-08-05T18:37:34.108775: step 5996, loss 0.545627.
Train: 2018-08-05T18:37:34.296230: step 5997, loss 0.579182.
Train: 2018-08-05T18:37:34.468066: step 5998, loss 0.612794.
Train: 2018-08-05T18:37:34.655555: step 5999, loss 0.595922.
Train: 2018-08-05T18:37:34.827386: step 6000, loss 0.570768.
Test: 2018-08-05T18:37:35.889608: step 6000, loss 0.548751.
Train: 2018-08-05T18:37:36.795675: step 6001, loss 0.612335.
Train: 2018-08-05T18:37:36.983131: step 6002, loss 0.579018.
Train: 2018-08-05T18:37:37.170596: step 6003, loss 0.546154.
Train: 2018-08-05T18:37:37.342393: step 6004, loss 0.554411.
Train: 2018-08-05T18:37:37.529883: step 6005, loss 0.497355.
Train: 2018-08-05T18:37:37.701713: step 6006, loss 0.562588.
Train: 2018-08-05T18:37:37.889170: step 6007, loss 0.619974.
Train: 2018-08-05T18:37:38.061004: step 6008, loss 0.537971.
Train: 2018-08-05T18:37:38.248431: step 6009, loss 0.52149.
Train: 2018-08-05T18:37:38.420265: step 6010, loss 0.521256.
Test: 2018-08-05T18:37:39.482527: step 6010, loss 0.548762.
Train: 2018-08-05T18:37:39.654352: step 6011, loss 0.56245.
Train: 2018-08-05T18:37:39.841837: step 6012, loss 0.520563.
Train: 2018-08-05T18:37:40.013642: step 6013, loss 0.638495.
Train: 2018-08-05T18:37:40.201098: step 6014, loss 0.621676.
Train: 2018-08-05T18:37:40.372962: step 6015, loss 0.511525.
Train: 2018-08-05T18:37:40.560419: step 6016, loss 0.587836.
Train: 2018-08-05T18:37:40.732254: step 6017, loss 0.536826.
Train: 2018-08-05T18:37:40.904092: step 6018, loss 0.511168.
Train: 2018-08-05T18:37:41.091544: step 6019, loss 0.553759.
Train: 2018-08-05T18:37:41.263379: step 6020, loss 0.527845.
Test: 2018-08-05T18:37:42.325609: step 6020, loss 0.54745.
Train: 2018-08-05T18:37:42.497465: step 6021, loss 0.544985.
Train: 2018-08-05T18:37:42.669300: step 6022, loss 0.597347.
Train: 2018-08-05T18:37:42.841134: step 6023, loss 0.509731.
Train: 2018-08-05T18:37:43.012970: step 6024, loss 0.518271.
Train: 2018-08-05T18:37:43.200425: step 6025, loss 0.562489.
Train: 2018-08-05T18:37:43.387852: step 6026, loss 0.571506.
Train: 2018-08-05T18:37:43.575307: step 6027, loss 0.517608.
Train: 2018-08-05T18:37:43.762763: step 6028, loss 0.544566.
Train: 2018-08-05T18:37:43.934629: step 6029, loss 0.480821.
Train: 2018-08-05T18:37:44.122055: step 6030, loss 0.489413.
Test: 2018-08-05T18:37:45.168715: step 6030, loss 0.548002.
Train: 2018-08-05T18:37:45.356170: step 6031, loss 0.590943.
Train: 2018-08-05T18:37:45.527976: step 6032, loss 0.535171.
Train: 2018-08-05T18:37:45.699810: step 6033, loss 0.572801.
Train: 2018-08-05T18:37:45.871675: step 6034, loss 0.610766.
Train: 2018-08-05T18:37:46.059132: step 6035, loss 0.45004.
Train: 2018-08-05T18:37:46.230969: step 6036, loss 0.582584.
Train: 2018-08-05T18:37:46.402770: step 6037, loss 0.582654.
Train: 2018-08-05T18:37:46.590260: step 6038, loss 0.554101.
Train: 2018-08-05T18:37:46.762092: step 6039, loss 0.58251.
Train: 2018-08-05T18:37:46.902683: step 6040, loss 0.382211.
Test: 2018-08-05T18:37:47.964934: step 6040, loss 0.546846.
Train: 2018-08-05T18:37:48.136740: step 6041, loss 0.620461.
Train: 2018-08-05T18:37:48.308604: step 6042, loss 0.573028.
Train: 2018-08-05T18:37:48.496060: step 6043, loss 0.591865.
Train: 2018-08-05T18:37:48.667866: step 6044, loss 0.572777.
Train: 2018-08-05T18:37:48.839700: step 6045, loss 0.628666.
Train: 2018-08-05T18:37:49.027190: step 6046, loss 0.646243.
Train: 2018-08-05T18:37:49.198991: step 6047, loss 0.653669.
Train: 2018-08-05T18:37:49.386477: step 6048, loss 0.536448.
Train: 2018-08-05T18:37:49.558306: step 6049, loss 0.528042.
Train: 2018-08-05T18:37:49.730147: step 6050, loss 0.579626.
Test: 2018-08-05T18:37:50.792368: step 6050, loss 0.548269.
Train: 2018-08-05T18:37:51.057931: step 6051, loss 0.596503.
Train: 2018-08-05T18:37:51.245387: step 6052, loss 0.511849.
Train: 2018-08-05T18:37:51.417221: step 6053, loss 0.529059.
Train: 2018-08-05T18:37:51.589086: step 6054, loss 0.512728.
Train: 2018-08-05T18:37:51.776536: step 6055, loss 0.529392.
Train: 2018-08-05T18:37:51.948377: step 6056, loss 0.512807.
Train: 2018-08-05T18:37:52.120211: step 6057, loss 0.57076.
Train: 2018-08-05T18:37:52.292016: step 6058, loss 0.570767.
Train: 2018-08-05T18:37:52.463882: step 6059, loss 0.570774.
Train: 2018-08-05T18:37:52.651308: step 6060, loss 0.495286.
Test: 2018-08-05T18:37:53.697967: step 6060, loss 0.547873.
Train: 2018-08-05T18:37:53.869803: step 6061, loss 0.579253.
Train: 2018-08-05T18:37:54.057258: step 6062, loss 0.545375.
Train: 2018-08-05T18:37:54.229092: step 6063, loss 0.485514.
Train: 2018-08-05T18:37:54.416549: step 6064, loss 0.553688.
Train: 2018-08-05T18:37:54.588384: step 6065, loss 0.597902.
Train: 2018-08-05T18:37:54.760218: step 6066, loss 0.553632.
Train: 2018-08-05T18:37:54.947675: step 6067, loss 0.55361.
Train: 2018-08-05T18:37:55.119479: step 6068, loss 0.624263.
Train: 2018-08-05T18:37:55.306935: step 6069, loss 0.580101.
Train: 2018-08-05T18:37:55.478802: step 6070, loss 0.571245.
Test: 2018-08-05T18:37:56.541046: step 6070, loss 0.546491.
Train: 2018-08-05T18:37:56.712886: step 6071, loss 0.518392.
Train: 2018-08-05T18:37:56.900346: step 6072, loss 0.588826.
Train: 2018-08-05T18:37:57.072177: step 6073, loss 0.544821.
Train: 2018-08-05T18:37:57.259633: step 6074, loss 0.615081.
Train: 2018-08-05T18:37:57.431468: step 6075, loss 0.579859.
Train: 2018-08-05T18:37:57.603302: step 6076, loss 0.64062.
Train: 2018-08-05T18:37:57.790729: step 6077, loss 0.545113.
Train: 2018-08-05T18:37:57.962564: step 6078, loss 0.605057.
Train: 2018-08-05T18:37:58.150020: step 6079, loss 0.494669.
Train: 2018-08-05T18:37:58.321884: step 6080, loss 0.537105.
Test: 2018-08-05T18:37:59.384106: step 6080, loss 0.549123.
Train: 2018-08-05T18:37:59.571589: step 6081, loss 0.587596.
Train: 2018-08-05T18:37:59.743427: step 6082, loss 0.528901.
Train: 2018-08-05T18:37:59.930852: step 6083, loss 0.61262.
Train: 2018-08-05T18:38:00.102718: step 6084, loss 0.587445.
Train: 2018-08-05T18:38:00.290173: step 6085, loss 0.554153.
Train: 2018-08-05T18:38:00.462009: step 6086, loss 0.579034.
Train: 2018-08-05T18:38:00.649468: step 6087, loss 0.579006.
Train: 2018-08-05T18:38:00.836920: step 6088, loss 0.521439.
Train: 2018-08-05T18:38:01.024347: step 6089, loss 0.488571.
Train: 2018-08-05T18:38:01.196211: step 6090, loss 0.61206.
Test: 2018-08-05T18:38:02.242843: step 6090, loss 0.547896.
Train: 2018-08-05T18:38:02.430292: step 6091, loss 0.496268.
Train: 2018-08-05T18:38:02.602102: step 6092, loss 0.59575.
Train: 2018-08-05T18:38:02.789588: step 6093, loss 0.562411.
Train: 2018-08-05T18:38:02.977039: step 6094, loss 0.637931.
Train: 2018-08-05T18:38:03.148883: step 6095, loss 0.562396.
Train: 2018-08-05T18:38:03.336335: step 6096, loss 0.503745.
Train: 2018-08-05T18:38:03.523796: step 6097, loss 0.612823.
Train: 2018-08-05T18:38:03.695596: step 6098, loss 0.629641.
Train: 2018-08-05T18:38:03.883053: step 6099, loss 0.562402.
Train: 2018-08-05T18:38:04.070539: step 6100, loss 0.545726.
Test: 2018-08-05T18:38:05.117162: step 6100, loss 0.548895.
Train: 2018-08-05T18:38:06.007555: step 6101, loss 0.570765.
Train: 2018-08-05T18:38:06.195041: step 6102, loss 0.595724.
Train: 2018-08-05T18:38:06.382497: step 6103, loss 0.579052.
Train: 2018-08-05T18:38:06.569953: step 6104, loss 0.570756.
Train: 2018-08-05T18:38:06.741789: step 6105, loss 0.570757.
Train: 2018-08-05T18:38:06.929244: step 6106, loss 0.57076.
Train: 2018-08-05T18:38:07.116700: step 6107, loss 0.554378.
Train: 2018-08-05T18:38:07.288534: step 6108, loss 0.58713.
Train: 2018-08-05T18:38:07.475961: step 6109, loss 0.554442.
Train: 2018-08-05T18:38:07.647829: step 6110, loss 0.587088.
Test: 2018-08-05T18:38:08.710047: step 6110, loss 0.548657.
Train: 2018-08-05T18:38:08.897532: step 6111, loss 0.578922.
Train: 2018-08-05T18:38:09.069362: step 6112, loss 0.522011.
Train: 2018-08-05T18:38:09.256824: step 6113, loss 0.489364.
Train: 2018-08-05T18:38:09.444280: step 6114, loss 0.521582.
Train: 2018-08-05T18:38:09.631736: step 6115, loss 0.554211.
Train: 2018-08-05T18:38:09.819162: step 6116, loss 0.54572.
Train: 2018-08-05T18:38:10.006649: step 6117, loss 0.553941.
Train: 2018-08-05T18:38:10.194104: step 6118, loss 0.536816.
Train: 2018-08-05T18:38:10.381531: step 6119, loss 0.613919.
Train: 2018-08-05T18:38:10.553395: step 6120, loss 0.596915.
Test: 2018-08-05T18:38:11.615615: step 6120, loss 0.547071.
Train: 2018-08-05T18:38:11.818718: step 6121, loss 0.553678.
Train: 2018-08-05T18:38:11.990559: step 6122, loss 0.536273.
Train: 2018-08-05T18:38:12.178014: step 6123, loss 0.614735.
Train: 2018-08-05T18:38:12.365441: step 6124, loss 0.553639.
Train: 2018-08-05T18:38:12.552897: step 6125, loss 0.501249.
Train: 2018-08-05T18:38:12.740383: step 6126, loss 0.579911.
Train: 2018-08-05T18:38:12.927839: step 6127, loss 0.571174.
Train: 2018-08-05T18:38:13.115295: step 6128, loss 0.562398.
Train: 2018-08-05T18:38:13.302754: step 6129, loss 0.46572.
Train: 2018-08-05T18:38:13.490207: step 6130, loss 0.588951.
Test: 2018-08-05T18:38:14.552459: step 6130, loss 0.546631.
Train: 2018-08-05T18:38:14.739915: step 6131, loss 0.500399.
Train: 2018-08-05T18:38:14.927371: step 6132, loss 0.58034.
Train: 2018-08-05T18:38:15.114797: step 6133, loss 0.55359.
Train: 2018-08-05T18:38:15.302254: step 6134, loss 0.589505.
Train: 2018-08-05T18:38:15.489739: step 6135, loss 0.598507.
Train: 2018-08-05T18:38:15.692817: step 6136, loss 0.490865.
Train: 2018-08-05T18:38:15.880273: step 6137, loss 0.634363.
Train: 2018-08-05T18:38:16.067728: step 6138, loss 0.5983.
Train: 2018-08-05T18:38:16.255155: step 6139, loss 0.589145.
Train: 2018-08-05T18:38:16.458262: step 6140, loss 0.571245.
Test: 2018-08-05T18:38:17.504862: step 6140, loss 0.54776.
Train: 2018-08-05T18:38:17.692319: step 6141, loss 0.571134.
Train: 2018-08-05T18:38:17.879774: step 6142, loss 0.510241.
Train: 2018-08-05T18:38:18.067260: step 6143, loss 0.51045.
Train: 2018-08-05T18:38:18.270342: step 6144, loss 0.553699.
Train: 2018-08-05T18:38:18.457795: step 6145, loss 0.657359.
Train: 2018-08-05T18:38:18.660872: step 6146, loss 0.493661.
Train: 2018-08-05T18:38:18.848328: step 6147, loss 0.485195.
Train: 2018-08-05T18:38:19.035784: step 6148, loss 0.588133.
Train: 2018-08-05T18:38:19.238861: step 6149, loss 0.467623.
Train: 2018-08-05T18:38:19.441908: step 6150, loss 0.571015.
Test: 2018-08-05T18:38:20.488538: step 6150, loss 0.547405.
Train: 2018-08-05T18:38:20.691647: step 6151, loss 0.562362.
Train: 2018-08-05T18:38:20.879102: step 6152, loss 0.571135.
Train: 2018-08-05T18:38:21.082180: step 6153, loss 0.527264.
Train: 2018-08-05T18:38:21.285228: step 6154, loss 0.562423.
Train: 2018-08-05T18:38:21.472713: step 6155, loss 0.553593.
Train: 2018-08-05T18:38:21.675760: step 6156, loss 0.580245.
Train: 2018-08-05T18:38:21.878868: step 6157, loss 0.580281.
Train: 2018-08-05T18:38:22.066295: step 6158, loss 0.544694.
Train: 2018-08-05T18:38:22.269372: step 6159, loss 0.553588.
Train: 2018-08-05T18:38:22.472479: step 6160, loss 0.695936.
Test: 2018-08-05T18:38:23.519078: step 6160, loss 0.548703.
Train: 2018-08-05T18:38:23.722183: step 6161, loss 0.518308.
Train: 2018-08-05T18:38:23.909612: step 6162, loss 0.553616.
Train: 2018-08-05T18:38:24.112721: step 6163, loss 0.553635.
Train: 2018-08-05T18:38:24.300177: step 6164, loss 0.597156.
Train: 2018-08-05T18:38:24.503224: step 6165, loss 0.510451.
Train: 2018-08-05T18:38:24.706331: step 6166, loss 0.562338.
Train: 2018-08-05T18:38:24.909379: step 6167, loss 0.536513.
Train: 2018-08-05T18:38:25.096865: step 6168, loss 0.493519.
Train: 2018-08-05T18:38:25.299942: step 6169, loss 0.527809.
Train: 2018-08-05T18:38:25.503019: step 6170, loss 0.536319.
Test: 2018-08-05T18:38:26.549619: step 6170, loss 0.548996.
Train: 2018-08-05T18:38:26.752727: step 6171, loss 0.518727.
Train: 2018-08-05T18:38:26.955805: step 6172, loss 0.509635.
Train: 2018-08-05T18:38:27.158881: step 6173, loss 0.580227.
Train: 2018-08-05T18:38:27.361929: step 6174, loss 0.571472.
Train: 2018-08-05T18:38:27.549386: step 6175, loss 0.526635.
Train: 2018-08-05T18:38:27.768113: step 6176, loss 0.60786.
Train: 2018-08-05T18:38:27.971191: step 6177, loss 0.481149.
Train: 2018-08-05T18:38:28.174239: step 6178, loss 0.562757.
Train: 2018-08-05T18:38:28.377346: step 6179, loss 0.599412.
Train: 2018-08-05T18:38:28.580394: step 6180, loss 0.645221.
Test: 2018-08-05T18:38:29.627023: step 6180, loss 0.546704.
Train: 2018-08-05T18:38:29.830130: step 6181, loss 0.535429.
Train: 2018-08-05T18:38:30.033209: step 6182, loss 0.490115.
Train: 2018-08-05T18:38:30.236287: step 6183, loss 0.544555.
Train: 2018-08-05T18:38:30.439334: step 6184, loss 0.553627.
Train: 2018-08-05T18:38:30.642445: step 6185, loss 0.553626.
Train: 2018-08-05T18:38:30.845518: step 6186, loss 0.653336.
Train: 2018-08-05T18:38:31.064217: step 6187, loss 0.499568.
Train: 2018-08-05T18:38:31.267299: step 6188, loss 0.580531.
Train: 2018-08-05T18:38:31.470341: step 6189, loss 0.607223.
Train: 2018-08-05T18:38:31.673449: step 6190, loss 0.606837.
Test: 2018-08-05T18:38:32.735700: step 6190, loss 0.54671.
Train: 2018-08-05T18:38:32.907535: step 6191, loss 0.581149.
Train: 2018-08-05T18:38:33.110613: step 6192, loss 0.544958.
Train: 2018-08-05T18:38:33.313661: step 6193, loss 0.50196.
Train: 2018-08-05T18:38:33.516737: step 6194, loss 0.536566.
Train: 2018-08-05T18:38:33.719845: step 6195, loss 0.596616.
Train: 2018-08-05T18:38:33.922922: step 6196, loss 0.596477.
Train: 2018-08-05T18:38:34.125970: step 6197, loss 0.570835.
Train: 2018-08-05T18:38:34.329078: step 6198, loss 0.60455.
Train: 2018-08-05T18:38:34.547776: step 6199, loss 0.445193.
Train: 2018-08-05T18:38:34.750854: step 6200, loss 0.579157.
Test: 2018-08-05T18:38:35.813075: step 6200, loss 0.548216.
Train: 2018-08-05T18:38:36.766006: step 6201, loss 0.5624.
Train: 2018-08-05T18:38:36.984676: step 6202, loss 0.579157.
Train: 2018-08-05T18:38:37.187753: step 6203, loss 0.528909.
Train: 2018-08-05T18:38:37.390864: step 6204, loss 0.545617.
Train: 2018-08-05T18:38:37.593909: step 6205, loss 0.587617.
Train: 2018-08-05T18:38:37.797015: step 6206, loss 0.604489.
Train: 2018-08-05T18:38:38.015718: step 6207, loss 0.579204.
Train: 2018-08-05T18:38:38.234413: step 6208, loss 0.528809.
Train: 2018-08-05T18:38:38.437490: step 6209, loss 0.545588.
Train: 2018-08-05T18:38:38.656193: step 6210, loss 0.528723.
Test: 2018-08-05T18:38:39.718410: step 6210, loss 0.546508.
Train: 2018-08-05T18:38:39.921518: step 6211, loss 0.511686.
Train: 2018-08-05T18:38:40.124565: step 6212, loss 0.655882.
Train: 2018-08-05T18:38:40.343294: step 6213, loss 0.587864.
Train: 2018-08-05T18:38:40.546371: step 6214, loss 0.587836.
Train: 2018-08-05T18:38:40.749449: step 6215, loss 0.553878.
Train: 2018-08-05T18:38:40.968148: step 6216, loss 0.655426.
Train: 2018-08-05T18:38:41.171225: step 6217, loss 0.570787.
Train: 2018-08-05T18:38:41.389923: step 6218, loss 0.529045.
Train: 2018-08-05T18:38:41.592971: step 6219, loss 0.604017.
Train: 2018-08-05T18:38:41.811702: step 6220, loss 0.55422.
Test: 2018-08-05T18:38:42.858330: step 6220, loss 0.548391.
Train: 2018-08-05T18:38:43.077029: step 6221, loss 0.570757.
Train: 2018-08-05T18:38:43.280106: step 6222, loss 0.570761.
Train: 2018-08-05T18:38:43.498805: step 6223, loss 0.554412.
Train: 2018-08-05T18:38:43.701883: step 6224, loss 0.554446.
Train: 2018-08-05T18:38:43.920581: step 6225, loss 0.595247.
Train: 2018-08-05T18:38:44.123659: step 6226, loss 0.538208.
Train: 2018-08-05T18:38:44.342327: step 6227, loss 0.578923.
Train: 2018-08-05T18:38:44.545435: step 6228, loss 0.595216.
Train: 2018-08-05T18:38:44.764105: step 6229, loss 0.595186.
Train: 2018-08-05T18:38:44.982833: step 6230, loss 0.578903.
Test: 2018-08-05T18:38:46.045053: step 6230, loss 0.548642.
Train: 2018-08-05T18:38:46.263780: step 6231, loss 0.578892.
Train: 2018-08-05T18:38:46.466863: step 6232, loss 0.522412.
Train: 2018-08-05T18:38:46.685558: step 6233, loss 0.578885.
Train: 2018-08-05T18:38:46.888636: step 6234, loss 0.570809.
Train: 2018-08-05T18:38:47.107335: step 6235, loss 0.603139.
Train: 2018-08-05T18:38:47.326034: step 6236, loss 0.562739.
Train: 2018-08-05T18:38:47.529111: step 6237, loss 0.514324.
Train: 2018-08-05T18:38:47.747780: step 6238, loss 0.627489.
Train: 2018-08-05T18:38:47.966509: step 6239, loss 0.578895.
Train: 2018-08-05T18:38:48.169587: step 6240, loss 0.595078.
Test: 2018-08-05T18:38:49.231807: step 6240, loss 0.54887.
Train: 2018-08-05T18:38:49.434916: step 6241, loss 0.611195.
Train: 2018-08-05T18:38:49.653616: step 6242, loss 0.578874.
Train: 2018-08-05T18:38:49.872313: step 6243, loss 0.522774.
Train: 2018-08-05T18:38:50.075391: step 6244, loss 0.538801.
Train: 2018-08-05T18:38:50.309711: step 6245, loss 0.619033.
Train: 2018-08-05T18:38:50.528378: step 6246, loss 0.578869.
Train: 2018-08-05T18:38:50.747109: step 6247, loss 0.522723.
Train: 2018-08-05T18:38:50.950186: step 6248, loss 0.48236.
Train: 2018-08-05T18:38:51.168884: step 6249, loss 0.56268.
Train: 2018-08-05T18:38:51.387587: step 6250, loss 0.595305.
Test: 2018-08-05T18:38:52.434183: step 6250, loss 0.548598.
Train: 2018-08-05T18:38:52.652912: step 6251, loss 0.529616.
Train: 2018-08-05T18:38:52.871611: step 6252, loss 0.55417.
Train: 2018-08-05T18:38:53.090309: step 6253, loss 0.654384.
Train: 2018-08-05T18:38:53.309008: step 6254, loss 0.579153.
Train: 2018-08-05T18:38:53.527677: step 6255, loss 0.537249.
Train: 2018-08-05T18:38:53.746406: step 6256, loss 0.579192.
Train: 2018-08-05T18:38:53.949484: step 6257, loss 0.596044.
Train: 2018-08-05T18:38:54.168152: step 6258, loss 0.528734.
Train: 2018-08-05T18:38:54.386851: step 6259, loss 0.629786.
Train: 2018-08-05T18:38:54.605580: step 6260, loss 0.503537.
Test: 2018-08-05T18:38:55.667801: step 6260, loss 0.548893.
Train: 2018-08-05T18:38:55.886533: step 6261, loss 0.545533.
Train: 2018-08-05T18:38:56.105229: step 6262, loss 0.604588.
Train: 2018-08-05T18:38:56.308276: step 6263, loss 0.545474.
Train: 2018-08-05T18:38:56.542625: step 6264, loss 0.553904.
Train: 2018-08-05T18:38:56.761328: step 6265, loss 0.536942.
Train: 2018-08-05T18:38:56.964402: step 6266, loss 0.53685.
Train: 2018-08-05T18:38:57.183100: step 6267, loss 0.5538.
Train: 2018-08-05T18:38:57.417392: step 6268, loss 0.519446.
Train: 2018-08-05T18:38:57.620499: step 6269, loss 0.596883.
Train: 2018-08-05T18:38:57.839197: step 6270, loss 0.640366.
Test: 2018-08-05T18:38:58.901418: step 6270, loss 0.549083.
Train: 2018-08-05T18:38:59.104495: step 6271, loss 0.596966.
Train: 2018-08-05T18:38:59.323225: step 6272, loss 0.545089.
Train: 2018-08-05T18:38:59.541923: step 6273, loss 0.553732.
Train: 2018-08-05T18:38:59.760626: step 6274, loss 0.656815.
Train: 2018-08-05T18:38:59.994943: step 6275, loss 0.587917.
Train: 2018-08-05T18:39:00.213643: step 6276, loss 0.536992.
Train: 2018-08-05T18:39:00.416719: step 6277, loss 0.528752.
Train: 2018-08-05T18:39:00.635421: step 6278, loss 0.595931.
Train: 2018-08-05T18:39:00.869736: step 6279, loss 0.545728.
Train: 2018-08-05T18:39:01.104055: step 6280, loss 0.462539.
Test: 2018-08-05T18:39:02.150658: step 6280, loss 0.548256.
Train: 2018-08-05T18:39:02.369387: step 6281, loss 0.612575.
Train: 2018-08-05T18:39:02.588085: step 6282, loss 0.554037.
Train: 2018-08-05T18:39:02.806783: step 6283, loss 0.486947.
Train: 2018-08-05T18:39:03.025452: step 6284, loss 0.55393.
Train: 2018-08-05T18:39:03.244152: step 6285, loss 0.596319.
Train: 2018-08-05T18:39:03.462880: step 6286, loss 0.468559.
Train: 2018-08-05T18:39:03.681550: step 6287, loss 0.51071.
Train: 2018-08-05T18:39:03.900278: step 6288, loss 0.52755.
Train: 2018-08-05T18:39:04.118976: step 6289, loss 0.632858.
Train: 2018-08-05T18:39:04.337675: step 6290, loss 0.527014.
Test: 2018-08-05T18:39:05.399927: step 6290, loss 0.547199.
Train: 2018-08-05T18:39:05.618626: step 6291, loss 0.526823.
Train: 2018-08-05T18:39:05.837324: step 6292, loss 0.571581.
Train: 2018-08-05T18:39:06.056023: step 6293, loss 0.607874.
Train: 2018-08-05T18:39:06.274722: step 6294, loss 0.508325.
Train: 2018-08-05T18:39:06.493420: step 6295, loss 0.590016.
Train: 2018-08-05T18:39:06.712120: step 6296, loss 0.517227.
Train: 2018-08-05T18:39:06.930819: step 6297, loss 0.462374.
Train: 2018-08-05T18:39:07.149488: step 6298, loss 0.608874.
Train: 2018-08-05T18:39:07.383842: step 6299, loss 0.590612.
Train: 2018-08-05T18:39:07.602535: step 6300, loss 0.535291.
Test: 2018-08-05T18:39:08.649166: step 6300, loss 0.547765.
Train: 2018-08-05T18:39:09.570824: step 6301, loss 0.498392.
Train: 2018-08-05T18:39:09.805145: step 6302, loss 0.600014.
Train: 2018-08-05T18:39:10.023814: step 6303, loss 0.572246.
Train: 2018-08-05T18:39:10.242542: step 6304, loss 0.507605.
Train: 2018-08-05T18:39:10.461241: step 6305, loss 0.599874.
Train: 2018-08-05T18:39:10.679940: step 6306, loss 0.526113.
Train: 2018-08-05T18:39:10.898654: step 6307, loss 0.507779.
Train: 2018-08-05T18:39:11.132930: step 6308, loss 0.507756.
Train: 2018-08-05T18:39:11.351658: step 6309, loss 0.655077.
Train: 2018-08-05T18:39:11.570357: step 6310, loss 0.57205.
Test: 2018-08-05T18:39:12.616986: step 6310, loss 0.548172.
Train: 2018-08-05T18:39:12.851308: step 6311, loss 0.517133.
Train: 2018-08-05T18:39:13.070006: step 6312, loss 0.599173.
Train: 2018-08-05T18:39:13.288704: step 6313, loss 0.598895.
Train: 2018-08-05T18:39:13.491782: step 6314, loss 0.544612.
Train: 2018-08-05T18:39:13.710480: step 6315, loss 0.526821.
Train: 2018-08-05T18:39:13.929180: step 6316, loss 0.509185.
Train: 2018-08-05T18:39:14.147848: step 6317, loss 0.553591.
Train: 2018-08-05T18:39:14.366577: step 6318, loss 0.642129.
Train: 2018-08-05T18:39:14.585278: step 6319, loss 0.500832.
Train: 2018-08-05T18:39:14.803944: step 6320, loss 0.579936.
Test: 2018-08-05T18:39:15.866195: step 6320, loss 0.547173.
Train: 2018-08-05T18:39:16.084925: step 6321, loss 0.553633.
Train: 2018-08-05T18:39:16.303623: step 6322, loss 0.588496.
Train: 2018-08-05T18:39:16.522322: step 6323, loss 0.562347.
Train: 2018-08-05T18:39:16.741021: step 6324, loss 0.536443.
Train: 2018-08-05T18:39:16.959723: step 6325, loss 0.519289.
Train: 2018-08-05T18:39:17.194010: step 6326, loss 0.562336.
Train: 2018-08-05T18:39:17.412738: step 6327, loss 0.60539.
Train: 2018-08-05T18:39:17.631441: step 6328, loss 0.502222.
Train: 2018-08-05T18:39:17.850105: step 6329, loss 0.570933.
Train: 2018-08-05T18:39:18.068835: step 6330, loss 0.622542.
Test: 2018-08-05T18:39:19.115434: step 6330, loss 0.548019.
Train: 2018-08-05T18:39:19.334164: step 6331, loss 0.528048.
Train: 2018-08-05T18:39:19.552862: step 6332, loss 0.553772.
Train: 2018-08-05T18:39:19.771561: step 6333, loss 0.545216.
Train: 2018-08-05T18:39:19.990260: step 6334, loss 0.562335.
Train: 2018-08-05T18:39:20.208958: step 6335, loss 0.528056.
Train: 2018-08-05T18:39:20.427657: step 6336, loss 0.553743.
Train: 2018-08-05T18:39:20.646356: step 6337, loss 0.605422.
Train: 2018-08-05T18:39:20.865056: step 6338, loss 0.596785.
Train: 2018-08-05T18:39:21.099376: step 6339, loss 0.545153.
Train: 2018-08-05T18:39:21.318074: step 6340, loss 0.502274.
Test: 2018-08-05T18:39:22.380295: step 6340, loss 0.546783.
Train: 2018-08-05T18:39:22.599024: step 6341, loss 0.545133.
Train: 2018-08-05T18:39:22.770859: step 6342, loss 0.580746.
Train: 2018-08-05T18:39:23.005178: step 6343, loss 0.493203.
Train: 2018-08-05T18:39:23.208256: step 6344, loss 0.553663.
Train: 2018-08-05T18:39:23.442577: step 6345, loss 0.56237.
Train: 2018-08-05T18:39:23.661275: step 6346, loss 0.571163.
Train: 2018-08-05T18:39:23.879973: step 6347, loss 0.571205.
Train: 2018-08-05T18:39:24.114295: step 6348, loss 0.553602.
Train: 2018-08-05T18:39:24.364236: step 6349, loss 0.52711.
Train: 2018-08-05T18:39:24.582905: step 6350, loss 0.535878.
Test: 2018-08-05T18:39:25.645186: step 6350, loss 0.545993.
Train: 2018-08-05T18:39:25.863884: step 6351, loss 0.500238.
Train: 2018-08-05T18:39:26.082583: step 6352, loss 0.634157.
Train: 2018-08-05T18:39:26.301281: step 6353, loss 0.571513.
Train: 2018-08-05T18:39:26.519980: step 6354, loss 0.643175.
Train: 2018-08-05T18:39:26.723058: step 6355, loss 0.571405.
Train: 2018-08-05T18:39:26.957379: step 6356, loss 0.580147.
Train: 2018-08-05T18:39:27.176077: step 6357, loss 0.536205.
Train: 2018-08-05T18:39:27.394746: step 6358, loss 0.649835.
Train: 2018-08-05T18:39:27.613475: step 6359, loss 0.579657.
Train: 2018-08-05T18:39:27.832173: step 6360, loss 0.648003.
Test: 2018-08-05T18:39:28.878808: step 6360, loss 0.54788.
Train: 2018-08-05T18:39:29.113094: step 6361, loss 0.562366.
Train: 2018-08-05T18:39:29.331822: step 6362, loss 0.629075.
Train: 2018-08-05T18:39:29.550521: step 6363, loss 0.652754.
Train: 2018-08-05T18:39:29.769220: step 6364, loss 0.546712.
Train: 2018-08-05T18:39:29.987918: step 6365, loss 0.50764.
Train: 2018-08-05T18:39:30.206617: step 6366, loss 0.54753.
Train: 2018-08-05T18:39:30.409695: step 6367, loss 0.563328.
Train: 2018-08-05T18:39:30.628398: step 6368, loss 0.555672.
Train: 2018-08-05T18:39:30.862717: step 6369, loss 0.548009.
Train: 2018-08-05T18:39:31.065792: step 6370, loss 0.563466.
Test: 2018-08-05T18:39:32.128042: step 6370, loss 0.551036.
Train: 2018-08-05T18:39:32.346711: step 6371, loss 0.578923.
Train: 2018-08-05T18:39:32.612305: step 6372, loss 0.54013.
Train: 2018-08-05T18:39:32.815382: step 6373, loss 0.532152.
Train: 2018-08-05T18:39:33.034080: step 6374, loss 0.578874.
Train: 2018-08-05T18:39:33.252748: step 6375, loss 0.539367.
Train: 2018-08-05T18:39:33.471478: step 6376, loss 0.618692.
Train: 2018-08-05T18:39:33.674525: step 6377, loss 0.570859.
Train: 2018-08-05T18:39:33.893254: step 6378, loss 0.514522.
Train: 2018-08-05T18:39:34.111953: step 6379, loss 0.505908.
Train: 2018-08-05T18:39:34.315030: step 6380, loss 0.578967.
Test: 2018-08-05T18:39:35.377251: step 6380, loss 0.548818.
Train: 2018-08-05T18:39:35.595980: step 6381, loss 0.512739.
Train: 2018-08-05T18:39:35.799057: step 6382, loss 0.495252.
Train: 2018-08-05T18:39:36.017756: step 6383, loss 0.570864.
Train: 2018-08-05T18:39:36.220834: step 6384, loss 0.605534.
Train: 2018-08-05T18:39:36.439533: step 6385, loss 0.562363.
Train: 2018-08-05T18:39:36.642610: step 6386, loss 0.5624.
Train: 2018-08-05T18:39:36.861278: step 6387, loss 0.588988.
Train: 2018-08-05T18:39:37.064387: step 6388, loss 0.544705.
Train: 2018-08-05T18:39:37.283085: step 6389, loss 0.517905.
Train: 2018-08-05T18:39:37.501788: step 6390, loss 0.607425.
Test: 2018-08-05T18:39:38.548408: step 6390, loss 0.546126.
Train: 2018-08-05T18:39:38.767113: step 6391, loss 0.607527.
Train: 2018-08-05T18:39:38.985781: step 6392, loss 0.580513.
Train: 2018-08-05T18:39:39.188859: step 6393, loss 0.50887.
Train: 2018-08-05T18:39:39.407588: step 6394, loss 0.553589.
Train: 2018-08-05T18:39:39.610665: step 6395, loss 0.482105.
Train: 2018-08-05T18:39:39.829365: step 6396, loss 0.571529.
Train: 2018-08-05T18:39:40.032442: step 6397, loss 0.625488.
Train: 2018-08-05T18:39:40.251110: step 6398, loss 0.589447.
Train: 2018-08-05T18:39:40.469808: step 6399, loss 0.580359.
Train: 2018-08-05T18:39:40.672916: step 6400, loss 0.562463.
Test: 2018-08-05T18:39:41.735138: step 6400, loss 0.54708.
Train: 2018-08-05T18:39:42.750559: step 6401, loss 0.518313.
Train: 2018-08-05T18:39:42.969224: step 6402, loss 0.509631.
Train: 2018-08-05T18:39:43.187923: step 6403, loss 0.623977.
Train: 2018-08-05T18:39:43.406651: step 6404, loss 0.536101.
Train: 2018-08-05T18:39:43.609728: step 6405, loss 0.518674.
Train: 2018-08-05T18:39:43.828431: step 6406, loss 0.53615.
Train: 2018-08-05T18:39:44.031505: step 6407, loss 0.52735.
Train: 2018-08-05T18:39:44.250208: step 6408, loss 0.580113.
Train: 2018-08-05T18:39:44.453281: step 6409, loss 0.562405.
Train: 2018-08-05T18:39:44.656358: step 6410, loss 0.597581.
Test: 2018-08-05T18:39:45.718579: step 6410, loss 0.548138.
Train: 2018-08-05T18:39:45.921690: step 6411, loss 0.544838.
Train: 2018-08-05T18:39:46.124769: step 6412, loss 0.536083.
Train: 2018-08-05T18:39:46.343465: step 6413, loss 0.579937.
Train: 2018-08-05T18:39:46.546535: step 6414, loss 0.56238.
Train: 2018-08-05T18:39:46.749623: step 6415, loss 0.579833.
Train: 2018-08-05T18:39:46.968317: step 6416, loss 0.579419.
Train: 2018-08-05T18:39:47.171395: step 6417, loss 0.55526.
Train: 2018-08-05T18:39:47.374442: step 6418, loss 0.562433.
Train: 2018-08-05T18:39:47.577520: step 6419, loss 0.562336.
Train: 2018-08-05T18:39:47.796219: step 6420, loss 0.484081.
Test: 2018-08-05T18:39:48.842878: step 6420, loss 0.546426.
Train: 2018-08-05T18:39:49.061577: step 6421, loss 0.553591.
Train: 2018-08-05T18:39:49.264655: step 6422, loss 0.553615.
Train: 2018-08-05T18:39:49.467701: step 6423, loss 0.626879.
Train: 2018-08-05T18:39:49.670809: step 6424, loss 0.571087.
Train: 2018-08-05T18:39:49.873891: step 6425, loss 0.536267.
Train: 2018-08-05T18:39:50.092556: step 6426, loss 0.640504.
Train: 2018-08-05T18:39:50.295663: step 6427, loss 0.519189.
Train: 2018-08-05T18:39:50.498740: step 6428, loss 0.579541.
Train: 2018-08-05T18:39:50.701788: step 6429, loss 0.562335.
Train: 2018-08-05T18:39:50.920517: step 6430, loss 0.570876.
Test: 2018-08-05T18:39:51.963068: step 6430, loss 0.547748.
Train: 2018-08-05T18:39:52.213010: step 6431, loss 0.587861.
Train: 2018-08-05T18:39:52.416086: step 6432, loss 0.528503.
Train: 2018-08-05T18:39:52.619194: step 6433, loss 0.579254.
Train: 2018-08-05T18:39:52.806650: step 6434, loss 0.495014.
Train: 2018-08-05T18:39:53.009698: step 6435, loss 0.562368.
Train: 2018-08-05T18:39:53.212805: step 6436, loss 0.587713.
Train: 2018-08-05T18:39:53.415884: step 6437, loss 0.562361.
Train: 2018-08-05T18:39:53.618960: step 6438, loss 0.579269.
Train: 2018-08-05T18:39:53.822037: step 6439, loss 0.596159.
Train: 2018-08-05T18:39:54.025115: step 6440, loss 0.5708.
Test: 2018-08-05T18:39:55.071746: step 6440, loss 0.547185.
Train: 2018-08-05T18:39:55.274823: step 6441, loss 0.604411.
Train: 2018-08-05T18:39:55.477900: step 6442, loss 0.654427.
Train: 2018-08-05T18:39:55.680977: step 6443, loss 0.587324.
Train: 2018-08-05T18:39:55.884054: step 6444, loss 0.587162.
Train: 2018-08-05T18:39:56.087132: step 6445, loss 0.522091.
Train: 2018-08-05T18:39:56.290210: step 6446, loss 0.603095.
Train: 2018-08-05T18:39:56.493258: step 6447, loss 0.626946.
Train: 2018-08-05T18:39:56.680713: step 6448, loss 0.594729.
Train: 2018-08-05T18:39:56.883820: step 6449, loss 0.531731.
Train: 2018-08-05T18:39:57.086898: step 6450, loss 0.485147.
Test: 2018-08-05T18:39:58.133528: step 6450, loss 0.549996.
Train: 2018-08-05T18:39:58.336605: step 6451, loss 0.578883.
Train: 2018-08-05T18:39:58.524061: step 6452, loss 0.563215.
Train: 2018-08-05T18:39:58.727139: step 6453, loss 0.571022.
Train: 2018-08-05T18:39:58.930187: step 6454, loss 0.547379.
Train: 2018-08-05T18:39:59.117672: step 6455, loss 0.570952.
Train: 2018-08-05T18:39:59.320750: step 6456, loss 0.602694.
Train: 2018-08-05T18:39:59.523828: step 6457, loss 0.562928.
Train: 2018-08-05T18:39:59.711254: step 6458, loss 0.538905.
Train: 2018-08-05T18:39:59.914358: step 6459, loss 0.514594.
Train: 2018-08-05T18:40:00.101817: step 6460, loss 0.505947.
Test: 2018-08-05T18:40:01.164038: step 6460, loss 0.54885.
Train: 2018-08-05T18:40:01.367115: step 6461, loss 0.611792.
Train: 2018-08-05T18:40:01.554602: step 6462, loss 0.52938.
Train: 2018-08-05T18:40:01.742058: step 6463, loss 0.537335.
Train: 2018-08-05T18:40:01.945135: step 6464, loss 0.486307.
Train: 2018-08-05T18:40:02.132562: step 6465, loss 0.536611.
Train: 2018-08-05T18:40:02.335669: step 6466, loss 0.597159.
Train: 2018-08-05T18:40:02.523125: step 6467, loss 0.518433.
Train: 2018-08-05T18:40:02.710581: step 6468, loss 0.526894.
Train: 2018-08-05T18:40:02.913659: step 6469, loss 0.526583.
Train: 2018-08-05T18:40:03.101085: step 6470, loss 0.553651.
Test: 2018-08-05T18:40:04.163336: step 6470, loss 0.547974.
Train: 2018-08-05T18:40:04.350791: step 6471, loss 0.608999.
Train: 2018-08-05T18:40:04.553899: step 6472, loss 0.590836.
Train: 2018-08-05T18:40:04.741355: step 6473, loss 0.498073.
Train: 2018-08-05T18:40:04.944433: step 6474, loss 0.628498.
Train: 2018-08-05T18:40:05.147510: step 6475, loss 0.544517.
Train: 2018-08-05T18:40:05.334969: step 6476, loss 0.497936.
Train: 2018-08-05T18:40:05.538014: step 6477, loss 0.572514.
Train: 2018-08-05T18:40:05.725470: step 6478, loss 0.553847.
Train: 2018-08-05T18:40:05.912956: step 6479, loss 0.488595.
Train: 2018-08-05T18:40:06.116033: step 6480, loss 0.535181.
Test: 2018-08-05T18:40:07.162663: step 6480, loss 0.547831.
Train: 2018-08-05T18:40:07.350120: step 6481, loss 0.610053.
Train: 2018-08-05T18:40:07.537576: step 6482, loss 0.544522.
Train: 2018-08-05T18:40:07.725001: step 6483, loss 0.581815.
Train: 2018-08-05T18:40:07.928126: step 6484, loss 0.590944.
Train: 2018-08-05T18:40:08.115565: step 6485, loss 0.516828.
Train: 2018-08-05T18:40:08.303021: step 6486, loss 0.618016.
Train: 2018-08-05T18:40:08.490448: step 6487, loss 0.608334.
Train: 2018-08-05T18:40:08.677933: step 6488, loss 0.56262.
Train: 2018-08-05T18:40:08.865389: step 6489, loss 0.580352.
Train: 2018-08-05T18:40:09.052846: step 6490, loss 0.49185.
Test: 2018-08-05T18:40:10.115068: step 6490, loss 0.547545.
Train: 2018-08-05T18:40:10.302553: step 6491, loss 0.588682.
Train: 2018-08-05T18:40:10.490010: step 6492, loss 0.536261.
Train: 2018-08-05T18:40:10.630601: step 6493, loss 0.470082.
Train: 2018-08-05T18:40:10.833649: step 6494, loss 0.579648.
Train: 2018-08-05T18:40:11.021135: step 6495, loss 0.553696.
Train: 2018-08-05T18:40:11.208591: step 6496, loss 0.579622.
Train: 2018-08-05T18:40:11.396047: step 6497, loss 0.579592.
Train: 2018-08-05T18:40:11.583503: step 6498, loss 0.588148.
Train: 2018-08-05T18:40:11.770960: step 6499, loss 0.476644.
Train: 2018-08-05T18:40:11.958416: step 6500, loss 0.493676.
Test: 2018-08-05T18:40:13.005015: step 6500, loss 0.548521.
Train: 2018-08-05T18:40:13.973539: step 6501, loss 0.570968.
Train: 2018-08-05T18:40:14.161027: step 6502, loss 0.562346.
Train: 2018-08-05T18:40:14.348481: step 6503, loss 0.52756.
Train: 2018-08-05T18:40:14.535937: step 6504, loss 0.632325.
Train: 2018-08-05T18:40:14.723393: step 6505, loss 0.632331.
Train: 2018-08-05T18:40:14.910849: step 6506, loss 0.553652.
Train: 2018-08-05T18:40:15.098276: step 6507, loss 0.640401.
Train: 2018-08-05T18:40:15.285730: step 6508, loss 0.527933.
Train: 2018-08-05T18:40:15.473187: step 6509, loss 0.562336.
Train: 2018-08-05T18:40:15.660673: step 6510, loss 0.528302.
Test: 2018-08-05T18:40:16.707303: step 6510, loss 0.548951.
Train: 2018-08-05T18:40:16.894755: step 6511, loss 0.443488.
Train: 2018-08-05T18:40:17.082216: step 6512, loss 0.536733.
Train: 2018-08-05T18:40:17.254050: step 6513, loss 0.519389.
Train: 2018-08-05T18:40:17.441507: step 6514, loss 0.579666.
Train: 2018-08-05T18:40:17.628962: step 6515, loss 0.48394.
Train: 2018-08-05T18:40:17.800798: step 6516, loss 0.676811.
Train: 2018-08-05T18:40:18.003844: step 6517, loss 0.491897.
Train: 2018-08-05T18:40:18.191331: step 6518, loss 0.562453.
Train: 2018-08-05T18:40:18.378786: step 6519, loss 0.589183.
Train: 2018-08-05T18:40:18.566243: step 6520, loss 0.562501.
Test: 2018-08-05T18:40:19.628495: step 6520, loss 0.546584.
Train: 2018-08-05T18:40:19.815950: step 6521, loss 0.616035.
Train: 2018-08-05T18:40:20.003407: step 6522, loss 0.571378.
Train: 2018-08-05T18:40:20.190863: step 6523, loss 0.642206.
Train: 2018-08-05T18:40:20.362668: step 6524, loss 0.588754.
Train: 2018-08-05T18:40:20.565776: step 6525, loss 0.484053.
Train: 2018-08-05T18:40:20.737580: step 6526, loss 0.596995.
Train: 2018-08-05T18:40:20.925066: step 6527, loss 0.570945.
Train: 2018-08-05T18:40:21.112521: step 6528, loss 0.588003.
Train: 2018-08-05T18:40:21.284356: step 6529, loss 0.519878.
Train: 2018-08-05T18:40:21.471812: step 6530, loss 0.553896.
Test: 2018-08-05T18:40:22.518442: step 6530, loss 0.547887.
Train: 2018-08-05T18:40:22.705899: step 6531, loss 0.494866.
Train: 2018-08-05T18:40:22.893354: step 6532, loss 0.570814.
Train: 2018-08-05T18:40:23.065160: step 6533, loss 0.596212.
Train: 2018-08-05T18:40:23.252616: step 6534, loss 0.536989.
Train: 2018-08-05T18:40:23.440102: step 6535, loss 0.511568.
Train: 2018-08-05T18:40:23.627558: step 6536, loss 0.570845.
Train: 2018-08-05T18:40:23.799362: step 6537, loss 0.570867.
Train: 2018-08-05T18:40:23.986818: step 6538, loss 0.545241.
Train: 2018-08-05T18:40:24.174305: step 6539, loss 0.545184.
Train: 2018-08-05T18:40:24.346139: step 6540, loss 0.622596.
Test: 2018-08-05T18:40:25.408391: step 6540, loss 0.548359.
Train: 2018-08-05T18:40:25.580195: step 6541, loss 0.579548.
Train: 2018-08-05T18:40:25.767683: step 6542, loss 0.570929.
Train: 2018-08-05T18:40:25.955138: step 6543, loss 0.613801.
Train: 2018-08-05T18:40:26.126973: step 6544, loss 0.553803.
Train: 2018-08-05T18:40:26.314428: step 6545, loss 0.562345.
Train: 2018-08-05T18:40:26.486263: step 6546, loss 0.647087.
Train: 2018-08-05T18:40:26.673690: step 6547, loss 0.528769.
Train: 2018-08-05T18:40:26.861176: step 6548, loss 0.512246.
Train: 2018-08-05T18:40:27.048602: step 6549, loss 0.478908.
Train: 2018-08-05T18:40:27.220466: step 6550, loss 0.554005.
Test: 2018-08-05T18:40:28.282688: step 6550, loss 0.547519.
Train: 2018-08-05T18:40:28.454523: step 6551, loss 0.621372.
Train: 2018-08-05T18:40:28.641979: step 6552, loss 0.562368.
Train: 2018-08-05T18:40:28.813843: step 6553, loss 0.596138.
Train: 2018-08-05T18:40:29.001270: step 6554, loss 0.469599.
Train: 2018-08-05T18:40:29.173134: step 6555, loss 0.553878.
Train: 2018-08-05T18:40:29.360590: step 6556, loss 0.545306.
Train: 2018-08-05T18:40:29.532395: step 6557, loss 0.630862.
Train: 2018-08-05T18:40:29.719851: step 6558, loss 0.519471.
Train: 2018-08-05T18:40:29.891716: step 6559, loss 0.519331.
Train: 2018-08-05T18:40:30.079173: step 6560, loss 0.579638.
Test: 2018-08-05T18:40:31.141393: step 6560, loss 0.549052.
Train: 2018-08-05T18:40:31.313229: step 6561, loss 0.57103.
Train: 2018-08-05T18:40:31.500714: step 6562, loss 0.571059.
Train: 2018-08-05T18:40:31.672518: step 6563, loss 0.510066.
Train: 2018-08-05T18:40:31.860005: step 6564, loss 0.588638.
Train: 2018-08-05T18:40:32.047462: step 6565, loss 0.553619.
Train: 2018-08-05T18:40:32.234888: step 6566, loss 0.659044.
Train: 2018-08-05T18:40:32.406752: step 6567, loss 0.597364.
Train: 2018-08-05T18:40:32.594213: step 6568, loss 0.631878.
Train: 2018-08-05T18:40:32.781658: step 6569, loss 0.570938.
Train: 2018-08-05T18:40:32.953469: step 6570, loss 0.536789.
Test: 2018-08-05T18:40:34.015721: step 6570, loss 0.548629.
Train: 2018-08-05T18:40:34.187585: step 6571, loss 0.680732.
Train: 2018-08-05T18:40:34.375011: step 6572, loss 0.562427.
Train: 2018-08-05T18:40:34.562498: step 6573, loss 0.55428.
Train: 2018-08-05T18:40:34.749922: step 6574, loss 0.595243.
Train: 2018-08-05T18:40:34.921788: step 6575, loss 0.586955.
Train: 2018-08-05T18:40:35.109215: step 6576, loss 0.554897.
Train: 2018-08-05T18:40:35.296703: step 6577, loss 0.570934.
Train: 2018-08-05T18:40:35.484156: step 6578, loss 0.563121.
Train: 2018-08-05T18:40:35.655963: step 6579, loss 0.625884.
Train: 2018-08-05T18:40:35.843418: step 6580, loss 0.594456.
Test: 2018-08-05T18:40:36.890079: step 6580, loss 0.551683.
Train: 2018-08-05T18:40:37.077533: step 6581, loss 0.524918.
Train: 2018-08-05T18:40:37.249339: step 6582, loss 0.571263.
Train: 2018-08-05T18:40:37.436824: step 6583, loss 0.525191.
Train: 2018-08-05T18:40:37.624285: step 6584, loss 0.571252.
Train: 2018-08-05T18:40:37.811749: step 6585, loss 0.540307.
Train: 2018-08-05T18:40:37.983571: step 6586, loss 0.532289.
Train: 2018-08-05T18:40:38.170997: step 6587, loss 0.571042.
Train: 2018-08-05T18:40:38.342862: step 6588, loss 0.578862.
Train: 2018-08-05T18:40:38.530289: step 6589, loss 0.586818.
Train: 2018-08-05T18:40:38.717774: step 6590, loss 0.522817.
Test: 2018-08-05T18:40:39.764404: step 6590, loss 0.548868.
Train: 2018-08-05T18:40:39.951830: step 6591, loss 0.603121.
Train: 2018-08-05T18:40:40.139287: step 6592, loss 0.595167.
Train: 2018-08-05T18:40:40.311151: step 6593, loss 0.554451.
Train: 2018-08-05T18:40:40.498607: step 6594, loss 0.570762.
Train: 2018-08-05T18:40:40.686034: step 6595, loss 0.595463.
Train: 2018-08-05T18:40:40.873519: step 6596, loss 0.587265.
Train: 2018-08-05T18:40:41.045324: step 6597, loss 0.587281.
Train: 2018-08-05T18:40:41.232810: step 6598, loss 0.545975.
Train: 2018-08-05T18:40:41.420266: step 6599, loss 0.595569.
Train: 2018-08-05T18:40:41.607722: step 6600, loss 0.521155.
Test: 2018-08-05T18:40:42.654322: step 6600, loss 0.547673.
Train: 2018-08-05T18:40:43.591633: step 6601, loss 0.562469.
Train: 2018-08-05T18:40:43.779089: step 6602, loss 0.562449.
Train: 2018-08-05T18:40:43.966550: step 6603, loss 0.570765.
Train: 2018-08-05T18:40:44.154002: step 6604, loss 0.52064.
Train: 2018-08-05T18:40:44.341457: step 6605, loss 0.486811.
Train: 2018-08-05T18:40:44.528913: step 6606, loss 0.596262.
Train: 2018-08-05T18:40:44.700748: step 6607, loss 0.639134.
Train: 2018-08-05T18:40:44.888204: step 6608, loss 0.57088.
Train: 2018-08-05T18:40:45.075660: step 6609, loss 0.562337.
Train: 2018-08-05T18:40:45.263116: step 6610, loss 0.502492.
Test: 2018-08-05T18:40:46.309746: step 6610, loss 0.548392.
Train: 2018-08-05T18:40:46.497172: step 6611, loss 0.55375.
Train: 2018-08-05T18:40:46.684658: step 6612, loss 0.545096.
Train: 2018-08-05T18:40:46.872114: step 6613, loss 0.536364.
Train: 2018-08-05T18:40:47.059571: step 6614, loss 0.492696.
Train: 2018-08-05T18:40:47.247027: step 6615, loss 0.579969.
Train: 2018-08-05T18:40:47.434483: step 6616, loss 0.53591.
Train: 2018-08-05T18:40:47.621910: step 6617, loss 0.509067.
Train: 2018-08-05T18:40:47.809365: step 6618, loss 0.589527.
Train: 2018-08-05T18:40:47.981231: step 6619, loss 0.517477.
Train: 2018-08-05T18:40:48.168686: step 6620, loss 0.599116.
Test: 2018-08-05T18:40:49.230907: step 6620, loss 0.546491.
Train: 2018-08-05T18:40:49.402741: step 6621, loss 0.571904.
Train: 2018-08-05T18:40:49.590198: step 6622, loss 0.626762.
Train: 2018-08-05T18:40:49.793276: step 6623, loss 0.526325.
Train: 2018-08-05T18:40:49.980760: step 6624, loss 0.599086.
Train: 2018-08-05T18:40:50.152567: step 6625, loss 0.598856.
Train: 2018-08-05T18:40:50.340022: step 6626, loss 0.499691.
Train: 2018-08-05T18:40:50.527479: step 6627, loss 0.52673.
Train: 2018-08-05T18:40:50.714965: step 6628, loss 0.508895.
Train: 2018-08-05T18:40:50.902391: step 6629, loss 0.55359.
Train: 2018-08-05T18:40:51.105468: step 6630, loss 0.481947.
Test: 2018-08-05T18:40:52.152128: step 6630, loss 0.547363.
Train: 2018-08-05T18:40:52.355206: step 6631, loss 0.544601.
Train: 2018-08-05T18:40:52.542661: step 6632, loss 0.499372.
Train: 2018-08-05T18:40:52.730087: step 6633, loss 0.535435.
Train: 2018-08-05T18:40:52.933195: step 6634, loss 0.53535.
Train: 2018-08-05T18:40:53.120652: step 6635, loss 0.507586.
Train: 2018-08-05T18:40:53.308078: step 6636, loss 0.572434.
Train: 2018-08-05T18:40:53.495534: step 6637, loss 0.535167.
Train: 2018-08-05T18:40:53.698611: step 6638, loss 0.601004.
Train: 2018-08-05T18:40:53.886099: step 6639, loss 0.629351.
Train: 2018-08-05T18:40:54.073553: step 6640, loss 0.544536.
Test: 2018-08-05T18:40:55.135804: step 6640, loss 0.546533.
Train: 2018-08-05T18:40:55.323260: step 6641, loss 0.516474.
Train: 2018-08-05T18:40:55.510717: step 6642, loss 0.609835.
Train: 2018-08-05T18:40:55.713764: step 6643, loss 0.572343.
Train: 2018-08-05T18:40:55.870007: step 6644, loss 0.562939.
Train: 2018-08-05T18:40:56.073056: step 6645, loss 0.571966.
Train: 2018-08-05T18:40:56.260511: step 6646, loss 0.617144.
Train: 2018-08-05T18:40:56.447966: step 6647, loss 0.607424.
Train: 2018-08-05T18:40:56.651075: step 6648, loss 0.456232.
Train: 2018-08-05T18:40:56.838531: step 6649, loss 0.553609.
Train: 2018-08-05T18:40:57.041596: step 6650, loss 0.562375.
Test: 2018-08-05T18:40:58.088207: step 6650, loss 0.54703.
Train: 2018-08-05T18:40:58.275694: step 6651, loss 0.553658.
Train: 2018-08-05T18:40:58.478741: step 6652, loss 0.536374.
Train: 2018-08-05T18:40:58.666228: step 6653, loss 0.614131.
Train: 2018-08-05T18:40:58.853683: step 6654, loss 0.639571.
Train: 2018-08-05T18:40:59.056731: step 6655, loss 0.49438.
Train: 2018-08-05T18:40:59.244217: step 6656, loss 0.59618.
Train: 2018-08-05T18:40:59.447295: step 6657, loss 0.53718.
Train: 2018-08-05T18:40:59.634754: step 6658, loss 0.520559.
Train: 2018-08-05T18:40:59.837823: step 6659, loss 0.554044.
Train: 2018-08-05T18:41:00.025255: step 6660, loss 0.57914.
Test: 2018-08-05T18:41:01.087507: step 6660, loss 0.547867.
Train: 2018-08-05T18:41:01.274992: step 6661, loss 0.629307.
Train: 2018-08-05T18:41:01.478070: step 6662, loss 0.620716.
Train: 2018-08-05T18:41:01.681146: step 6663, loss 0.496369.
Train: 2018-08-05T18:41:01.868603: step 6664, loss 0.513011.
Train: 2018-08-05T18:41:02.071680: step 6665, loss 0.587286.
Train: 2018-08-05T18:41:02.259137: step 6666, loss 0.570756.
Train: 2018-08-05T18:41:02.462208: step 6667, loss 0.595577.
Train: 2018-08-05T18:41:02.665291: step 6668, loss 0.570756.
Train: 2018-08-05T18:41:02.852751: step 6669, loss 0.545998.
Train: 2018-08-05T18:41:03.055795: step 6670, loss 0.587269.
Test: 2018-08-05T18:41:04.102455: step 6670, loss 0.547398.
Train: 2018-08-05T18:41:04.305502: step 6671, loss 0.570756.
Train: 2018-08-05T18:41:04.492988: step 6672, loss 0.537778.
Train: 2018-08-05T18:41:04.696066: step 6673, loss 0.653321.
Train: 2018-08-05T18:41:04.883522: step 6674, loss 0.562535.
Train: 2018-08-05T18:41:05.086599: step 6675, loss 0.570762.
Train: 2018-08-05T18:41:05.289677: step 6676, loss 0.570768.
Train: 2018-08-05T18:41:05.477133: step 6677, loss 0.570774.
Train: 2018-08-05T18:41:05.680181: step 6678, loss 0.61961.
Train: 2018-08-05T18:41:05.883288: step 6679, loss 0.586995.
Train: 2018-08-05T18:41:06.086365: step 6680, loss 0.59499.
Test: 2018-08-05T18:41:07.132995: step 6680, loss 0.549661.
Train: 2018-08-05T18:41:07.336072: step 6681, loss 0.554843.
Train: 2018-08-05T18:41:07.539150: step 6682, loss 0.54696.
Train: 2018-08-05T18:41:07.726606: step 6683, loss 0.618666.
Train: 2018-08-05T18:41:07.929684: step 6684, loss 0.515437.
Train: 2018-08-05T18:41:08.132760: step 6685, loss 0.562998.
Train: 2018-08-05T18:41:08.335838: step 6686, loss 0.634446.
Train: 2018-08-05T18:41:08.538916: step 6687, loss 0.594701.
Train: 2018-08-05T18:41:08.726372: step 6688, loss 0.531498.
Train: 2018-08-05T18:41:08.945070: step 6689, loss 0.610442.
Train: 2018-08-05T18:41:09.132526: step 6690, loss 0.570986.
Test: 2018-08-05T18:41:10.194777: step 6690, loss 0.54926.
Train: 2018-08-05T18:41:10.397857: step 6691, loss 0.555258.
Train: 2018-08-05T18:41:10.600933: step 6692, loss 0.51587.
Train: 2018-08-05T18:41:10.803981: step 6693, loss 0.48379.
Train: 2018-08-05T18:41:11.007088: step 6694, loss 0.545998.
Train: 2018-08-05T18:41:11.210165: step 6695, loss 0.563218.
Train: 2018-08-05T18:41:11.413212: step 6696, loss 0.618125.
Train: 2018-08-05T18:41:11.616320: step 6697, loss 0.536743.
Train: 2018-08-05T18:41:11.819368: step 6698, loss 0.562443.
Train: 2018-08-05T18:41:12.022446: step 6699, loss 0.521214.
Train: 2018-08-05T18:41:12.241177: step 6700, loss 0.612294.
Test: 2018-08-05T18:41:13.287774: step 6700, loss 0.548507.
Train: 2018-08-05T18:41:14.256326: step 6701, loss 0.537421.
Train: 2018-08-05T18:41:14.459404: step 6702, loss 0.537265.
Train: 2018-08-05T18:41:14.678103: step 6703, loss 0.56237.
Train: 2018-08-05T18:41:14.881181: step 6704, loss 0.486015.
Train: 2018-08-05T18:41:15.084258: step 6705, loss 0.519496.
Train: 2018-08-05T18:41:15.287306: step 6706, loss 0.553678.
Train: 2018-08-05T18:41:15.490413: step 6707, loss 0.474769.
Train: 2018-08-05T18:41:15.709111: step 6708, loss 0.562478.
Train: 2018-08-05T18:41:15.912190: step 6709, loss 0.571599.
Train: 2018-08-05T18:41:16.115266: step 6710, loss 0.535457.
Test: 2018-08-05T18:41:17.161896: step 6710, loss 0.548389.
Train: 2018-08-05T18:41:17.364974: step 6711, loss 0.544517.
Train: 2018-08-05T18:41:17.568051: step 6712, loss 0.535254.
Train: 2018-08-05T18:41:17.786722: step 6713, loss 0.535188.
Train: 2018-08-05T18:41:17.989827: step 6714, loss 0.563347.
Train: 2018-08-05T18:41:18.192905: step 6715, loss 0.51621.
Train: 2018-08-05T18:41:18.395953: step 6716, loss 0.60168.
Train: 2018-08-05T18:41:18.599060: step 6717, loss 0.630388.
Train: 2018-08-05T18:41:18.802137: step 6718, loss 0.563589.
Train: 2018-08-05T18:41:19.020836: step 6719, loss 0.478417.
Train: 2018-08-05T18:41:19.223884: step 6720, loss 0.563447.
Test: 2018-08-05T18:41:20.286135: step 6720, loss 0.548094.
Train: 2018-08-05T18:41:20.489212: step 6721, loss 0.563393.
Train: 2018-08-05T18:41:20.707941: step 6722, loss 0.544537.
Train: 2018-08-05T18:41:20.910989: step 6723, loss 0.516457.
Train: 2018-08-05T18:41:21.114096: step 6724, loss 0.544521.
Train: 2018-08-05T18:41:21.317173: step 6725, loss 0.544517.
Train: 2018-08-05T18:41:21.535873: step 6726, loss 0.656209.
Train: 2018-08-05T18:41:21.738949: step 6727, loss 0.544509.
Train: 2018-08-05T18:41:21.957648: step 6728, loss 0.572016.
Train: 2018-08-05T18:41:22.160726: step 6729, loss 0.590001.
Train: 2018-08-05T18:41:22.363803: step 6730, loss 0.499587.
Test: 2018-08-05T18:41:23.426025: step 6730, loss 0.548006.
Train: 2018-08-05T18:41:23.629132: step 6731, loss 0.508845.
Train: 2018-08-05T18:41:23.847830: step 6732, loss 0.571436.
Train: 2018-08-05T18:41:24.050878: step 6733, loss 0.464682.
Train: 2018-08-05T18:41:24.253955: step 6734, loss 0.562496.
Train: 2018-08-05T18:41:24.472655: step 6735, loss 0.571423.
Train: 2018-08-05T18:41:24.675762: step 6736, loss 0.50009.
Train: 2018-08-05T18:41:24.894460: step 6737, loss 0.643019.
Train: 2018-08-05T18:41:25.113159: step 6738, loss 0.589263.
Train: 2018-08-05T18:41:25.316237: step 6739, loss 0.562468.
Train: 2018-08-05T18:41:25.534936: step 6740, loss 0.509417.
Test: 2018-08-05T18:41:26.581566: step 6740, loss 0.546876.
Train: 2018-08-05T18:41:26.800267: step 6741, loss 0.624177.
Train: 2018-08-05T18:41:27.018963: step 6742, loss 0.500977.
Train: 2018-08-05T18:41:27.237631: step 6743, loss 0.588658.
Train: 2018-08-05T18:41:27.440740: step 6744, loss 0.509997.
Train: 2018-08-05T18:41:27.659438: step 6745, loss 0.614723.
Train: 2018-08-05T18:41:27.862517: step 6746, loss 0.501496.
Train: 2018-08-05T18:41:28.081215: step 6747, loss 0.571049.
Train: 2018-08-05T18:41:28.299913: step 6748, loss 0.536285.
Train: 2018-08-05T18:41:28.518612: step 6749, loss 0.527572.
Train: 2018-08-05T18:41:28.737310: step 6750, loss 0.57108.
Test: 2018-08-05T18:41:29.783911: step 6750, loss 0.548391.
Train: 2018-08-05T18:41:29.987019: step 6751, loss 0.632209.
Train: 2018-08-05T18:41:30.205722: step 6752, loss 0.588463.
Train: 2018-08-05T18:41:30.424416: step 6753, loss 0.579666.
Train: 2018-08-05T18:41:30.643116: step 6754, loss 0.553723.
Train: 2018-08-05T18:41:30.861817: step 6755, loss 0.545187.
Train: 2018-08-05T18:41:31.064891: step 6756, loss 0.485409.
Train: 2018-08-05T18:41:31.283589: step 6757, loss 0.630846.
Train: 2018-08-05T18:41:31.502259: step 6758, loss 0.622133.
Train: 2018-08-05T18:41:31.721010: step 6759, loss 0.562348.
Train: 2018-08-05T18:41:31.924065: step 6760, loss 0.528577.
Test: 2018-08-05T18:41:32.986286: step 6760, loss 0.548108.
Train: 2018-08-05T18:41:33.204985: step 6761, loss 0.596075.
Train: 2018-08-05T18:41:33.470548: step 6762, loss 0.50366.
Train: 2018-08-05T18:41:33.689276: step 6763, loss 0.604349.
Train: 2018-08-05T18:41:33.892353: step 6764, loss 0.52054.
Train: 2018-08-05T18:41:34.111022: step 6765, loss 0.562399.
Train: 2018-08-05T18:41:34.329751: step 6766, loss 0.545616.
Train: 2018-08-05T18:41:34.548420: step 6767, loss 0.503529.
Train: 2018-08-05T18:41:34.767148: step 6768, loss 0.545445.
Train: 2018-08-05T18:41:34.970226: step 6769, loss 0.579367.
Train: 2018-08-05T18:41:35.188924: step 6770, loss 0.536676.
Test: 2018-08-05T18:41:36.251177: step 6770, loss 0.548957.
Train: 2018-08-05T18:41:36.454254: step 6771, loss 0.519317.
Train: 2018-08-05T18:41:36.688574: step 6772, loss 0.579689.
Train: 2018-08-05T18:41:36.907273: step 6773, loss 0.571085.
Train: 2018-08-05T18:41:37.125971: step 6774, loss 0.65873.
Train: 2018-08-05T18:41:37.329048: step 6775, loss 0.588591.
Train: 2018-08-05T18:41:37.547751: step 6776, loss 0.64941.
Train: 2018-08-05T18:41:37.766447: step 6777, loss 0.562338.
Train: 2018-08-05T18:41:37.985145: step 6778, loss 0.545225.
Train: 2018-08-05T18:41:38.188225: step 6779, loss 0.579345.
Train: 2018-08-05T18:41:38.406921: step 6780, loss 0.579249.
Test: 2018-08-05T18:41:39.469143: step 6780, loss 0.548976.
Train: 2018-08-05T18:41:39.687871: step 6781, loss 0.545632.
Train: 2018-08-05T18:41:39.906571: step 6782, loss 0.554086.
Train: 2018-08-05T18:41:40.109648: step 6783, loss 0.537524.
Train: 2018-08-05T18:41:40.328346: step 6784, loss 0.545867.
Train: 2018-08-05T18:41:40.547045: step 6785, loss 0.637148.
Train: 2018-08-05T18:41:40.765744: step 6786, loss 0.628605.
Train: 2018-08-05T18:41:40.984444: step 6787, loss 0.611773.
Train: 2018-08-05T18:41:41.203142: step 6788, loss 0.554534.
Train: 2018-08-05T18:41:41.421811: step 6789, loss 0.546616.
Train: 2018-08-05T18:41:41.640539: step 6790, loss 0.538728.
Test: 2018-08-05T18:41:42.702791: step 6790, loss 0.549456.
Train: 2018-08-05T18:41:42.921460: step 6791, loss 0.562839.
Train: 2018-08-05T18:41:43.124566: step 6792, loss 0.530821.
Train: 2018-08-05T18:41:43.343236: step 6793, loss 0.530627.
Train: 2018-08-05T18:41:43.561965: step 6794, loss 0.554043.
Train: 2018-08-05T18:41:43.749421: step 6795, loss 0.56361.
Train: 2018-08-05T18:41:43.968119: step 6796, loss 0.615669.
Train: 2018-08-05T18:41:44.186817: step 6797, loss 0.496363.
Train: 2018-08-05T18:41:44.405519: step 6798, loss 0.578952.
Train: 2018-08-05T18:41:44.624186: step 6799, loss 0.636192.
Train: 2018-08-05T18:41:44.842914: step 6800, loss 0.513612.
Test: 2018-08-05T18:41:45.905166: step 6800, loss 0.547403.
Train: 2018-08-05T18:41:46.889280: step 6801, loss 0.685356.
Train: 2018-08-05T18:41:47.107978: step 6802, loss 0.546349.
Train: 2018-08-05T18:41:47.326708: step 6803, loss 0.554549.
Train: 2018-08-05T18:41:47.529785: step 6804, loss 0.570793.
Train: 2018-08-05T18:41:47.748479: step 6805, loss 0.546502.
Train: 2018-08-05T18:41:47.967183: step 6806, loss 0.587004.
Train: 2018-08-05T18:41:48.185852: step 6807, loss 0.578898.
Train: 2018-08-05T18:41:48.404581: step 6808, loss 0.506.
Train: 2018-08-05T18:41:48.623279: step 6809, loss 0.546387.
Train: 2018-08-05T18:41:48.841977: step 6810, loss 0.603467.
Test: 2018-08-05T18:41:49.888607: step 6810, loss 0.54831.
Train: 2018-08-05T18:41:50.107277: step 6811, loss 0.587157.
Train: 2018-08-05T18:41:50.326005: step 6812, loss 0.464046.
Train: 2018-08-05T18:41:50.544704: step 6813, loss 0.579032.
Train: 2018-08-05T18:41:50.763403: step 6814, loss 0.562433.
Train: 2018-08-05T18:41:50.982072: step 6815, loss 0.57078.
Train: 2018-08-05T18:41:51.200771: step 6816, loss 0.596086.
Train: 2018-08-05T18:41:51.419499: step 6817, loss 0.613082.
Train: 2018-08-05T18:41:51.638169: step 6818, loss 0.579263.
Train: 2018-08-05T18:41:51.856897: step 6819, loss 0.562366.
Train: 2018-08-05T18:41:52.075595: step 6820, loss 0.579234.
Test: 2018-08-05T18:41:53.122226: step 6820, loss 0.549676.
Train: 2018-08-05T18:41:53.372168: step 6821, loss 0.52029.
Train: 2018-08-05T18:41:53.590865: step 6822, loss 0.553944.
Train: 2018-08-05T18:41:53.809564: step 6823, loss 0.486371.
Train: 2018-08-05T18:41:54.028263: step 6824, loss 0.579344.
Train: 2018-08-05T18:41:54.262584: step 6825, loss 0.485471.
Train: 2018-08-05T18:41:54.481282: step 6826, loss 0.46753.
Train: 2018-08-05T18:41:54.699981: step 6827, loss 0.597324.
Train: 2018-08-05T18:41:54.918679: step 6828, loss 0.483001.
Train: 2018-08-05T18:41:55.137349: step 6829, loss 0.517825.
Train: 2018-08-05T18:41:55.356077: step 6830, loss 0.562687.
Test: 2018-08-05T18:41:56.402707: step 6830, loss 0.549231.
Train: 2018-08-05T18:41:56.621376: step 6831, loss 0.572023.
Train: 2018-08-05T18:41:56.840105: step 6832, loss 0.581499.
Train: 2018-08-05T18:41:57.058803: step 6833, loss 0.544509.
Train: 2018-08-05T18:41:57.277503: step 6834, loss 0.544509.
Train: 2018-08-05T18:41:57.496201: step 6835, loss 0.599899.
Train: 2018-08-05T18:41:57.714870: step 6836, loss 0.527597.
Train: 2018-08-05T18:41:57.933599: step 6837, loss 0.52694.
Train: 2018-08-05T18:41:58.152297: step 6838, loss 0.581602.
Train: 2018-08-05T18:41:58.370967: step 6839, loss 0.516435.
Train: 2018-08-05T18:41:58.589695: step 6840, loss 0.516168.
Test: 2018-08-05T18:41:59.651917: step 6840, loss 0.549516.
Train: 2018-08-05T18:41:59.870645: step 6841, loss 0.487475.
Train: 2018-08-05T18:42:00.089344: step 6842, loss 0.554239.
Train: 2018-08-05T18:42:00.323671: step 6843, loss 0.573542.
Train: 2018-08-05T18:42:00.526741: step 6844, loss 0.535091.
Train: 2018-08-05T18:42:00.745441: step 6845, loss 0.6411.
Train: 2018-08-05T18:42:00.979762: step 6846, loss 0.554269.
Train: 2018-08-05T18:42:01.214053: step 6847, loss 0.535094.
Train: 2018-08-05T18:42:01.432753: step 6848, loss 0.592037.
Train: 2018-08-05T18:42:01.667101: step 6849, loss 0.582111.
Train: 2018-08-05T18:42:01.885799: step 6850, loss 0.616462.
Test: 2018-08-05T18:42:02.948032: step 6850, loss 0.54902.
Train: 2018-08-05T18:42:03.166742: step 6851, loss 0.594936.
Train: 2018-08-05T18:42:03.369825: step 6852, loss 0.59641.
Train: 2018-08-05T18:42:03.588495: step 6853, loss 0.570646.
Train: 2018-08-05T18:42:03.807223: step 6854, loss 0.54996.
Train: 2018-08-05T18:42:04.025923: step 6855, loss 0.595837.
Train: 2018-08-05T18:42:04.260243: step 6856, loss 0.509756.
Train: 2018-08-05T18:42:04.478912: step 6857, loss 0.624987.
Train: 2018-08-05T18:42:04.681989: step 6858, loss 0.526944.
Train: 2018-08-05T18:42:04.900717: step 6859, loss 0.562451.
Train: 2018-08-05T18:42:05.119417: step 6860, loss 0.597763.
Test: 2018-08-05T18:42:06.181638: step 6860, loss 0.547723.
Train: 2018-08-05T18:42:06.400336: step 6861, loss 0.615125.
Train: 2018-08-05T18:42:06.603443: step 6862, loss 0.562362.
Train: 2018-08-05T18:42:06.822142: step 6863, loss 0.579639.
Train: 2018-08-05T18:42:07.056433: step 6864, loss 0.553757.
Train: 2018-08-05T18:42:07.275161: step 6865, loss 0.519746.
Train: 2018-08-05T18:42:07.493830: step 6866, loss 0.638745.
Train: 2018-08-05T18:42:07.712558: step 6867, loss 0.604484.
Train: 2018-08-05T18:42:07.931227: step 6868, loss 0.645841.
Train: 2018-08-05T18:42:08.149927: step 6869, loss 0.529612.
Train: 2018-08-05T18:42:08.368655: step 6870, loss 0.554476.
Test: 2018-08-05T18:42:09.415256: step 6870, loss 0.548276.
Train: 2018-08-05T18:42:09.633954: step 6871, loss 0.643597.
Train: 2018-08-05T18:42:09.852682: step 6872, loss 0.58686.
Train: 2018-08-05T18:42:10.055730: step 6873, loss 0.570951.
Train: 2018-08-05T18:42:10.274430: step 6874, loss 0.578878.
Train: 2018-08-05T18:42:10.493157: step 6875, loss 0.547828.
Train: 2018-08-05T18:42:10.711857: step 6876, loss 0.648496.
Train: 2018-08-05T18:42:10.914904: step 6877, loss 0.57899.
Train: 2018-08-05T18:42:11.149254: step 6878, loss 0.579051.
Train: 2018-08-05T18:42:11.383578: step 6879, loss 0.58666.
Train: 2018-08-05T18:42:11.586653: step 6880, loss 0.52664.
Test: 2018-08-05T18:42:12.648902: step 6880, loss 0.552011.
Train: 2018-08-05T18:42:12.867602: step 6881, loss 0.489213.
Train: 2018-08-05T18:42:13.070649: step 6882, loss 0.586661.
Train: 2018-08-05T18:42:13.289377: step 6883, loss 0.548725.
Train: 2018-08-05T18:42:13.508077: step 6884, loss 0.586648.
Train: 2018-08-05T18:42:13.726776: step 6885, loss 0.532786.
Train: 2018-08-05T18:42:13.945444: step 6886, loss 0.540062.
Train: 2018-08-05T18:42:14.148522: step 6887, loss 0.555296.
Train: 2018-08-05T18:42:14.367220: step 6888, loss 0.539101.
Train: 2018-08-05T18:42:14.585950: step 6889, loss 0.562765.
Train: 2018-08-05T18:42:14.788997: step 6890, loss 0.53814.
Test: 2018-08-05T18:42:15.851278: step 6890, loss 0.548109.
Train: 2018-08-05T18:42:16.069947: step 6891, loss 0.529415.
Train: 2018-08-05T18:42:16.288675: step 6892, loss 0.579171.
Train: 2018-08-05T18:42:16.507375: step 6893, loss 0.502905.
Train: 2018-08-05T18:42:16.726073: step 6894, loss 0.536486.
Train: 2018-08-05T18:42:16.944772: step 6895, loss 0.571117.
Train: 2018-08-05T18:42:17.147849: step 6896, loss 0.571293.
Train: 2018-08-05T18:42:17.366548: step 6897, loss 0.499965.
Train: 2018-08-05T18:42:17.585247: step 6898, loss 0.526482.
Train: 2018-08-05T18:42:17.803946: step 6899, loss 0.608588.
Train: 2018-08-05T18:42:18.007023: step 6900, loss 0.572162.
Test: 2018-08-05T18:42:19.069243: step 6900, loss 0.546285.
Train: 2018-08-05T18:42:19.975282: step 6901, loss 0.525987.
Train: 2018-08-05T18:42:20.194011: step 6902, loss 0.553824.
Train: 2018-08-05T18:42:20.397089: step 6903, loss 0.563217.
Train: 2018-08-05T18:42:20.615787: step 6904, loss 0.478947.
Train: 2018-08-05T18:42:20.834487: step 6905, loss 0.648182.
Train: 2018-08-05T18:42:21.037563: step 6906, loss 0.553957.
Train: 2018-08-05T18:42:21.256263: step 6907, loss 0.600893.
Train: 2018-08-05T18:42:21.474961: step 6908, loss 0.51649.
Train: 2018-08-05T18:42:21.678039: step 6909, loss 0.619024.
Train: 2018-08-05T18:42:21.896737: step 6910, loss 0.590737.
Test: 2018-08-05T18:42:22.943367: step 6910, loss 0.547331.
Train: 2018-08-05T18:42:23.146415: step 6911, loss 0.58116.
Train: 2018-08-05T18:42:23.365144: step 6912, loss 0.580817.
Train: 2018-08-05T18:42:23.568191: step 6913, loss 0.571516.
Train: 2018-08-05T18:42:23.786920: step 6914, loss 0.527019.
Train: 2018-08-05T18:42:23.989999: step 6915, loss 0.606307.
Train: 2018-08-05T18:42:24.208665: step 6916, loss 0.631838.
Train: 2018-08-05T18:42:24.411766: step 6917, loss 0.50249.
Train: 2018-08-05T18:42:24.630442: step 6918, loss 0.452068.
Train: 2018-08-05T18:42:24.833520: step 6919, loss 0.486121.
Train: 2018-08-05T18:42:25.052219: step 6920, loss 0.502848.
Test: 2018-08-05T18:42:26.098851: step 6920, loss 0.547453.
Train: 2018-08-05T18:42:26.348817: step 6921, loss 0.613682.
Train: 2018-08-05T18:42:26.598733: step 6922, loss 0.545169.
Train: 2018-08-05T18:42:26.864321: step 6923, loss 0.519266.
Train: 2018-08-05T18:42:27.129888: step 6924, loss 0.510362.
Train: 2018-08-05T18:42:27.364208: step 6925, loss 0.588573.
Train: 2018-08-05T18:42:27.582905: step 6926, loss 0.536051.
Train: 2018-08-05T18:42:27.785983: step 6927, loss 0.509426.
Train: 2018-08-05T18:42:28.004682: step 6928, loss 0.562494.
Train: 2018-08-05T18:42:28.207760: step 6929, loss 0.517732.
Train: 2018-08-05T18:42:28.410837: step 6930, loss 0.634938.
Test: 2018-08-05T18:42:29.473058: step 6930, loss 0.54776.
Train: 2018-08-05T18:42:29.676165: step 6931, loss 0.435898.
Train: 2018-08-05T18:42:29.879214: step 6932, loss 0.617581.
Train: 2018-08-05T18:42:30.082321: step 6933, loss 0.553683.
Train: 2018-08-05T18:42:30.301022: step 6934, loss 0.553702.
Train: 2018-08-05T18:42:30.504067: step 6935, loss 0.608946.
Train: 2018-08-05T18:42:30.707145: step 6936, loss 0.516953.
Train: 2018-08-05T18:42:30.910222: step 6937, loss 0.5537.
Train: 2018-08-05T18:42:31.113299: step 6938, loss 0.617953.
Train: 2018-08-05T18:42:31.332027: step 6939, loss 0.59021.
Train: 2018-08-05T18:42:31.535105: step 6940, loss 0.580853.
Test: 2018-08-05T18:42:32.597351: step 6940, loss 0.546946.
Train: 2018-08-05T18:42:32.800434: step 6941, loss 0.535595.
Train: 2018-08-05T18:42:33.003511: step 6942, loss 0.598318.
Train: 2018-08-05T18:42:33.206588: step 6943, loss 0.65116.
Train: 2018-08-05T18:42:33.409668: step 6944, loss 0.536123.
Train: 2018-08-05T18:42:33.612744: step 6945, loss 0.62294.
Train: 2018-08-05T18:42:33.784547: step 6946, loss 0.580554.
Train: 2018-08-05T18:42:33.987657: step 6947, loss 0.604494.
Train: 2018-08-05T18:42:34.190733: step 6948, loss 0.587361.
Train: 2018-08-05T18:42:34.393781: step 6949, loss 0.587129.
Train: 2018-08-05T18:42:34.612509: step 6950, loss 0.595016.
Test: 2018-08-05T18:42:35.659110: step 6950, loss 0.549302.
Train: 2018-08-05T18:42:35.862217: step 6951, loss 0.60272.
Train: 2018-08-05T18:42:36.065265: step 6952, loss 0.524.
Train: 2018-08-05T18:42:36.268372: step 6953, loss 0.617743.
Train: 2018-08-05T18:42:36.471449: step 6954, loss 0.502136.
Train: 2018-08-05T18:42:36.674497: step 6955, loss 0.548392.
Train: 2018-08-05T18:42:36.861982: step 6956, loss 0.556082.
Train: 2018-08-05T18:42:37.065060: step 6957, loss 0.54076.
Train: 2018-08-05T18:42:37.268137: step 6958, loss 0.586651.
Train: 2018-08-05T18:42:37.471216: step 6959, loss 0.609762.
Train: 2018-08-05T18:42:37.674295: step 6960, loss 0.540388.
Test: 2018-08-05T18:42:38.736544: step 6960, loss 0.551043.
Train: 2018-08-05T18:42:38.924001: step 6961, loss 0.524727.
Train: 2018-08-05T18:42:39.127077: step 6962, loss 0.524287.
Train: 2018-08-05T18:42:39.330125: step 6963, loss 0.563098.
Train: 2018-08-05T18:42:39.517611: step 6964, loss 0.586825.
Train: 2018-08-05T18:42:39.720688: step 6965, loss 0.562799.
Train: 2018-08-05T18:42:39.908144: step 6966, loss 0.595113.
Train: 2018-08-05T18:42:40.111223: step 6967, loss 0.538138.
Train: 2018-08-05T18:42:40.314269: step 6968, loss 0.595429.
Train: 2018-08-05T18:42:40.517377: step 6969, loss 0.570756.
Train: 2018-08-05T18:42:40.704803: step 6970, loss 0.604001.
Test: 2018-08-05T18:42:41.767084: step 6970, loss 0.548528.
Train: 2018-08-05T18:42:41.954541: step 6971, loss 0.54578.
Train: 2018-08-05T18:42:42.157617: step 6972, loss 0.579127.
Train: 2018-08-05T18:42:42.360665: step 6973, loss 0.604277.
Train: 2018-08-05T18:42:42.563772: step 6974, loss 0.528913.
Train: 2018-08-05T18:42:42.766850: step 6975, loss 0.587563.
Train: 2018-08-05T18:42:42.954306: step 6976, loss 0.562389.
Train: 2018-08-05T18:42:43.157384: step 6977, loss 0.562386.
Train: 2018-08-05T18:42:43.344810: step 6978, loss 0.612832.
Train: 2018-08-05T18:42:43.547918: step 6979, loss 0.587564.
Train: 2018-08-05T18:42:43.750994: step 6980, loss 0.503865.
Test: 2018-08-05T18:42:44.797595: step 6980, loss 0.548999.
Train: 2018-08-05T18:42:45.000702: step 6981, loss 0.520543.
Train: 2018-08-05T18:42:45.188157: step 6982, loss 0.545576.
Train: 2018-08-05T18:42:45.391205: step 6983, loss 0.553921.
Train: 2018-08-05T18:42:45.594313: step 6984, loss 0.536895.
Train: 2018-08-05T18:42:45.781769: step 6985, loss 0.51112.
Train: 2018-08-05T18:42:45.969225: step 6986, loss 0.527897.
Train: 2018-08-05T18:42:46.187923: step 6987, loss 0.571047.
Train: 2018-08-05T18:42:46.375350: step 6988, loss 0.597433.
Train: 2018-08-05T18:42:46.578457: step 6989, loss 0.527198.
Train: 2018-08-05T18:42:46.765913: step 6990, loss 0.580156.
Test: 2018-08-05T18:42:47.828174: step 6990, loss 0.548248.
Train: 2018-08-05T18:42:48.015621: step 6991, loss 0.553589.
Train: 2018-08-05T18:42:48.203076: step 6992, loss 0.535756.
Train: 2018-08-05T18:42:48.406154: step 6993, loss 0.508833.
Train: 2018-08-05T18:42:48.593604: step 6994, loss 0.580618.
Train: 2018-08-05T18:42:48.796692: step 6995, loss 0.671108.
Train: 2018-08-05T18:42:48.984144: step 6996, loss 0.634633.
Train: 2018-08-05T18:42:49.171601: step 6997, loss 0.535731.
Train: 2018-08-05T18:42:49.359027: step 6998, loss 0.642289.
Train: 2018-08-05T18:42:49.562134: step 6999, loss 0.518534.
Train: 2018-08-05T18:42:49.749590: step 7000, loss 0.61457.
Test: 2018-08-05T18:42:50.811811: step 7000, loss 0.547757.
Train: 2018-08-05T18:42:51.791548: step 7001, loss 0.484836.
Train: 2018-08-05T18:42:51.994596: step 7002, loss 0.570904.
Train: 2018-08-05T18:42:52.197697: step 7003, loss 0.613497.
Train: 2018-08-05T18:42:52.385159: step 7004, loss 0.553896.
Train: 2018-08-05T18:42:52.588237: step 7005, loss 0.579199.
Train: 2018-08-05T18:42:52.775692: step 7006, loss 0.554061.
Train: 2018-08-05T18:42:52.963148: step 7007, loss 0.587386.
Train: 2018-08-05T18:42:53.150605: step 7008, loss 0.587287.
Train: 2018-08-05T18:42:53.353677: step 7009, loss 0.521477.
Train: 2018-08-05T18:42:53.541138: step 7010, loss 0.546184.
Test: 2018-08-05T18:42:54.603389: step 7010, loss 0.550034.
Train: 2018-08-05T18:42:54.790846: step 7011, loss 0.578952.
Train: 2018-08-05T18:42:54.978301: step 7012, loss 0.587124.
Train: 2018-08-05T18:42:55.181349: step 7013, loss 0.570771.
Train: 2018-08-05T18:42:55.368805: step 7014, loss 0.578925.
Train: 2018-08-05T18:42:55.556292: step 7015, loss 0.546379.
Train: 2018-08-05T18:42:55.743747: step 7016, loss 0.570781.
Train: 2018-08-05T18:42:55.931204: step 7017, loss 0.513835.
Train: 2018-08-05T18:42:56.118629: step 7018, loss 0.554436.
Train: 2018-08-05T18:42:56.306116: step 7019, loss 0.529734.
Train: 2018-08-05T18:42:56.493566: step 7020, loss 0.595541.
Test: 2018-08-05T18:42:57.555823: step 7020, loss 0.549556.
Train: 2018-08-05T18:42:57.743250: step 7021, loss 0.56246.
Train: 2018-08-05T18:42:57.930706: step 7022, loss 0.554094.
Train: 2018-08-05T18:42:58.118162: step 7023, loss 0.554026.
Train: 2018-08-05T18:42:58.305647: step 7024, loss 0.629727.
Train: 2018-08-05T18:42:58.493104: step 7025, loss 0.570798.
Train: 2018-08-05T18:42:58.680560: step 7026, loss 0.537096.
Train: 2018-08-05T18:42:58.868017: step 7027, loss 0.553923.
Train: 2018-08-05T18:42:59.055443: step 7028, loss 0.536962.
Train: 2018-08-05T18:42:59.242898: step 7029, loss 0.562338.
Train: 2018-08-05T18:42:59.430354: step 7030, loss 0.493058.
Test: 2018-08-05T18:43:00.477014: step 7030, loss 0.547983.
Train: 2018-08-05T18:43:00.680092: step 7031, loss 0.55375.
Train: 2018-08-05T18:43:00.867518: step 7032, loss 0.525362.
Train: 2018-08-05T18:43:01.055004: step 7033, loss 0.599451.
Train: 2018-08-05T18:43:01.242460: step 7034, loss 0.535108.
Train: 2018-08-05T18:43:01.445533: step 7035, loss 0.562864.
Train: 2018-08-05T18:43:01.632994: step 7036, loss 0.571173.
Train: 2018-08-05T18:43:01.820449: step 7037, loss 0.605543.
Train: 2018-08-05T18:43:02.007905: step 7038, loss 0.54512.
Train: 2018-08-05T18:43:02.195362: step 7039, loss 0.570914.
Train: 2018-08-05T18:43:02.382789: step 7040, loss 0.570885.
Test: 2018-08-05T18:43:03.429448: step 7040, loss 0.547139.
Train: 2018-08-05T18:43:03.616904: step 7041, loss 0.553826.
Train: 2018-08-05T18:43:03.804360: step 7042, loss 0.570839.
Train: 2018-08-05T18:43:03.991816: step 7043, loss 0.587754.
Train: 2018-08-05T18:43:04.163620: step 7044, loss 0.629818.
Train: 2018-08-05T18:43:04.351106: step 7045, loss 0.528947.
Train: 2018-08-05T18:43:04.538563: step 7046, loss 0.470845.
Train: 2018-08-05T18:43:04.725990: step 7047, loss 0.570767.
Train: 2018-08-05T18:43:04.913475: step 7048, loss 0.579126.
Train: 2018-08-05T18:43:05.100931: step 7049, loss 0.487162.
Train: 2018-08-05T18:43:05.288387: step 7050, loss 0.57079.
Test: 2018-08-05T18:43:06.350639: step 7050, loss 0.548255.
Train: 2018-08-05T18:43:06.538065: step 7051, loss 0.596153.
Train: 2018-08-05T18:43:06.725545: step 7052, loss 0.553889.
Train: 2018-08-05T18:43:06.928599: step 7053, loss 0.579328.
Train: 2018-08-05T18:43:07.116089: step 7054, loss 0.511332.
Train: 2018-08-05T18:43:07.303540: step 7055, loss 0.519634.
Train: 2018-08-05T18:43:07.490997: step 7056, loss 0.631124.
Train: 2018-08-05T18:43:07.662831: step 7057, loss 0.519272.
Train: 2018-08-05T18:43:07.865879: step 7058, loss 0.536398.
Train: 2018-08-05T18:43:08.037713: step 7059, loss 0.631885.
Train: 2018-08-05T18:43:08.225200: step 7060, loss 0.536277.
Test: 2018-08-05T18:43:09.287451: step 7060, loss 0.548021.
Train: 2018-08-05T18:43:09.474901: step 7061, loss 0.562358.
Train: 2018-08-05T18:43:09.662363: step 7062, loss 0.553648.
Train: 2018-08-05T18:43:09.849789: step 7063, loss 0.65832.
Train: 2018-08-05T18:43:10.037269: step 7064, loss 0.536311.
Train: 2018-08-05T18:43:10.224726: step 7065, loss 0.510432.
Train: 2018-08-05T18:43:10.412188: step 7066, loss 0.562342.
Train: 2018-08-05T18:43:10.599614: step 7067, loss 0.588297.
Train: 2018-08-05T18:43:10.771449: step 7068, loss 0.510525.
Train: 2018-08-05T18:43:10.958935: step 7069, loss 0.562341.
Train: 2018-08-05T18:43:11.162012: step 7070, loss 0.536374.
Test: 2018-08-05T18:43:12.224263: step 7070, loss 0.54666.
Train: 2018-08-05T18:43:12.396099: step 7071, loss 0.571025.
Train: 2018-08-05T18:43:12.583554: step 7072, loss 0.458093.
Train: 2018-08-05T18:43:12.786631: step 7073, loss 0.501117.
Train: 2018-08-05T18:43:12.974087: step 7074, loss 0.597787.
Train: 2018-08-05T18:43:13.145923: step 7075, loss 0.535802.
Train: 2018-08-05T18:43:13.333379: step 7076, loss 0.499875.
Train: 2018-08-05T18:43:13.520805: step 7077, loss 0.589742.
Train: 2018-08-05T18:43:13.708291: step 7078, loss 0.580886.
Train: 2018-08-05T18:43:13.880126: step 7079, loss 0.617442.
Train: 2018-08-05T18:43:14.067581: step 7080, loss 0.608256.
Test: 2018-08-05T18:43:15.129833: step 7080, loss 0.54755.
Train: 2018-08-05T18:43:15.301668: step 7081, loss 0.635165.
Train: 2018-08-05T18:43:15.504746: step 7082, loss 0.544617.
Train: 2018-08-05T18:43:15.692201: step 7083, loss 0.607027.
Train: 2018-08-05T18:43:15.864036: step 7084, loss 0.456645.
Train: 2018-08-05T18:43:16.051492: step 7085, loss 0.54483.
Train: 2018-08-05T18:43:16.238943: step 7086, loss 0.5361.
Train: 2018-08-05T18:43:16.426404: step 7087, loss 0.614891.
Train: 2018-08-05T18:43:16.613860: step 7088, loss 0.571074.
Train: 2018-08-05T18:43:16.785695: step 7089, loss 0.553675.
Train: 2018-08-05T18:43:16.973122: step 7090, loss 0.562339.
Test: 2018-08-05T18:43:18.035372: step 7090, loss 0.547174.
Train: 2018-08-05T18:43:18.222828: step 7091, loss 0.605357.
Train: 2018-08-05T18:43:18.410315: step 7092, loss 0.519578.
Train: 2018-08-05T18:43:18.582150: step 7093, loss 0.502653.
Train: 2018-08-05T18:43:18.769576: step 7094, loss 0.528196.
Train: 2018-08-05T18:43:18.957063: step 7095, loss 0.596583.
Train: 2018-08-05T18:43:19.144487: step 7096, loss 0.562335.
Train: 2018-08-05T18:43:19.285110: step 7097, loss 0.489192.
Train: 2018-08-05T18:43:19.472567: step 7098, loss 0.510656.
Train: 2018-08-05T18:43:19.660022: step 7099, loss 0.588381.
Train: 2018-08-05T18:43:19.847478: step 7100, loss 0.571083.
Test: 2018-08-05T18:43:20.894108: step 7100, loss 0.548368.
Train: 2018-08-05T18:43:21.878253: step 7101, loss 0.667384.
Train: 2018-08-05T18:43:22.065708: step 7102, loss 0.553643.
Train: 2018-08-05T18:43:22.253165: step 7103, loss 0.492789.
Train: 2018-08-05T18:43:22.456213: step 7104, loss 0.597189.
Train: 2018-08-05T18:43:22.643669: step 7105, loss 0.536262.
Train: 2018-08-05T18:43:22.831154: step 7106, loss 0.544955.
Train: 2018-08-05T18:43:23.018581: step 7107, loss 0.466555.
Train: 2018-08-05T18:43:23.206066: step 7108, loss 0.606214.
Train: 2018-08-05T18:43:23.393524: step 7109, loss 0.53603.
Train: 2018-08-05T18:43:23.596600: step 7110, loss 0.624181.
Test: 2018-08-05T18:43:24.658822: step 7110, loss 0.54668.
Train: 2018-08-05T18:43:24.830686: step 7111, loss 0.571233.
Train: 2018-08-05T18:43:25.018113: step 7112, loss 0.527204.
Train: 2018-08-05T18:43:25.205599: step 7113, loss 0.562409.
Train: 2018-08-05T18:43:25.393025: step 7114, loss 0.509607.
Train: 2018-08-05T18:43:25.596101: step 7115, loss 0.544779.
Train: 2018-08-05T18:43:25.783588: step 7116, loss 0.588982.
Train: 2018-08-05T18:43:25.971014: step 7117, loss 0.535891.
Train: 2018-08-05T18:43:26.158500: step 7118, loss 0.633372.
Train: 2018-08-05T18:43:26.345956: step 7119, loss 0.544764.
Train: 2018-08-05T18:43:26.549004: step 7120, loss 0.509349.
Test: 2018-08-05T18:43:27.611255: step 7120, loss 0.547765.
Train: 2018-08-05T18:43:27.783119: step 7121, loss 0.562956.
Train: 2018-08-05T18:43:27.986167: step 7122, loss 0.571864.
Train: 2018-08-05T18:43:28.173653: step 7123, loss 0.571268.
Train: 2018-08-05T18:43:28.361109: step 7124, loss 0.553619.
Train: 2018-08-05T18:43:28.548565: step 7125, loss 0.588618.
Train: 2018-08-05T18:43:28.736022: step 7126, loss 0.588495.
Train: 2018-08-05T18:43:28.923478: step 7127, loss 0.510364.
Train: 2018-08-05T18:43:29.110933: step 7128, loss 0.562341.
Train: 2018-08-05T18:43:29.298390: step 7129, loss 0.579595.
Train: 2018-08-05T18:43:29.501467: step 7130, loss 0.596746.
Test: 2018-08-05T18:43:30.548067: step 7130, loss 0.548039.
Train: 2018-08-05T18:43:30.751174: step 7131, loss 0.519533.
Train: 2018-08-05T18:43:30.938600: step 7132, loss 0.545247.
Train: 2018-08-05T18:43:31.126087: step 7133, loss 0.562338.
Train: 2018-08-05T18:43:31.313543: step 7134, loss 0.587944.
Train: 2018-08-05T18:43:31.500999: step 7135, loss 0.570859.
Train: 2018-08-05T18:43:31.704076: step 7136, loss 0.604835.
Train: 2018-08-05T18:43:31.891503: step 7137, loss 0.587734.
Train: 2018-08-05T18:43:32.078989: step 7138, loss 0.495097.
Train: 2018-08-05T18:43:32.282066: step 7139, loss 0.495146.
Train: 2018-08-05T18:43:32.469522: step 7140, loss 0.520184.
Test: 2018-08-05T18:43:33.516122: step 7140, loss 0.548171.
Train: 2018-08-05T18:43:33.719231: step 7141, loss 0.54537.
Train: 2018-08-05T18:43:33.969171: step 7142, loss 0.519605.
Train: 2018-08-05T18:43:34.156627: step 7143, loss 0.648541.
Train: 2018-08-05T18:43:34.344085: step 7144, loss 0.545059.
Train: 2018-08-05T18:43:34.547160: step 7145, loss 0.605679.
Train: 2018-08-05T18:43:34.734616: step 7146, loss 0.545014.
Train: 2018-08-05T18:43:34.922073: step 7147, loss 0.588364.
Train: 2018-08-05T18:43:35.125148: step 7148, loss 0.553682.
Train: 2018-08-05T18:43:35.328228: step 7149, loss 0.527717.
Train: 2018-08-05T18:43:35.515654: step 7150, loss 0.519007.
Test: 2018-08-05T18:43:36.577935: step 7150, loss 0.548428.
Train: 2018-08-05T18:43:36.765392: step 7151, loss 0.562356.
Train: 2018-08-05T18:43:36.952818: step 7152, loss 0.562366.
Train: 2018-08-05T18:43:37.155925: step 7153, loss 0.571121.
Train: 2018-08-05T18:43:37.343381: step 7154, loss 0.53611.
Train: 2018-08-05T18:43:37.546456: step 7155, loss 0.536059.
Train: 2018-08-05T18:43:37.733914: step 7156, loss 0.650486.
Train: 2018-08-05T18:43:37.921342: step 7157, loss 0.509694.
Train: 2018-08-05T18:43:38.140040: step 7158, loss 0.562399.
Train: 2018-08-05T18:43:38.327526: step 7159, loss 0.53604.
Train: 2018-08-05T18:43:38.530603: step 7160, loss 0.65916.
Test: 2018-08-05T18:43:39.577227: step 7160, loss 0.54716.
Train: 2018-08-05T18:43:39.780310: step 7161, loss 0.544877.
Train: 2018-08-05T18:43:39.983358: step 7162, loss 0.501352.
Train: 2018-08-05T18:43:40.170844: step 7163, loss 0.562361.
Train: 2018-08-05T18:43:40.373923: step 7164, loss 0.57978.
Train: 2018-08-05T18:43:40.561379: step 7165, loss 0.510183.
Train: 2018-08-05T18:43:40.764455: step 7166, loss 0.571066.
Train: 2018-08-05T18:43:40.951914: step 7167, loss 0.518806.
Train: 2018-08-05T18:43:41.154959: step 7168, loss 0.527431.
Train: 2018-08-05T18:43:41.358065: step 7169, loss 0.536074.
Train: 2018-08-05T18:43:41.545522: step 7170, loss 0.474257.
Test: 2018-08-05T18:43:42.607743: step 7170, loss 0.546806.
Train: 2018-08-05T18:43:42.795200: step 7171, loss 0.606979.
Train: 2018-08-05T18:43:42.998307: step 7172, loss 0.607256.
Train: 2018-08-05T18:43:43.201354: step 7173, loss 0.544635.
Train: 2018-08-05T18:43:43.388842: step 7174, loss 0.652273.
Train: 2018-08-05T18:43:43.591921: step 7175, loss 0.651826.
Train: 2018-08-05T18:43:43.795000: step 7176, loss 0.588965.
Train: 2018-08-05T18:43:43.982422: step 7177, loss 0.571121.
Train: 2018-08-05T18:43:44.201121: step 7178, loss 0.605598.
Train: 2018-08-05T18:43:44.404198: step 7179, loss 0.57088.
Train: 2018-08-05T18:43:44.591683: step 7180, loss 0.537042.
Test: 2018-08-05T18:43:45.653934: step 7180, loss 0.547861.
Train: 2018-08-05T18:43:45.856982: step 7181, loss 0.587503.
Train: 2018-08-05T18:43:46.044439: step 7182, loss 0.620472.
Train: 2018-08-05T18:43:46.263167: step 7183, loss 0.497072.
Train: 2018-08-05T18:43:46.466246: step 7184, loss 0.635896.
Train: 2018-08-05T18:43:46.669322: step 7185, loss 0.546628.
Train: 2018-08-05T18:43:46.856747: step 7186, loss 0.562846.
Train: 2018-08-05T18:43:47.059855: step 7187, loss 0.602764.
Train: 2018-08-05T18:43:47.262903: step 7188, loss 0.523435.
Train: 2018-08-05T18:43:47.466011: step 7189, loss 0.499847.
Train: 2018-08-05T18:43:47.669093: step 7190, loss 0.531291.
Test: 2018-08-05T18:43:48.731340: step 7190, loss 0.549402.
Train: 2018-08-05T18:43:48.934417: step 7191, loss 0.57886.
Train: 2018-08-05T18:43:49.137494: step 7192, loss 0.53877.
Train: 2018-08-05T18:43:49.340571: step 7193, loss 0.562731.
Train: 2018-08-05T18:43:49.543654: step 7194, loss 0.50569.
Train: 2018-08-05T18:43:49.746723: step 7195, loss 0.611881.
Train: 2018-08-05T18:43:49.949804: step 7196, loss 0.57904.
Train: 2018-08-05T18:43:50.152852: step 7197, loss 0.479131.
Train: 2018-08-05T18:43:50.355959: step 7198, loss 0.646571.
Train: 2018-08-05T18:43:50.559036: step 7199, loss 0.562359.
Train: 2018-08-05T18:43:50.762084: step 7200, loss 0.579334.
Test: 2018-08-05T18:43:51.808714: step 7200, loss 0.548509.
Train: 2018-08-05T18:43:52.824130: step 7201, loss 0.511238.
Train: 2018-08-05T18:43:53.027209: step 7202, loss 0.545204.
Train: 2018-08-05T18:43:53.230285: step 7203, loss 0.570955.
Train: 2018-08-05T18:43:53.433363: step 7204, loss 0.519043.
Train: 2018-08-05T18:43:53.636441: step 7205, loss 0.571082.
Train: 2018-08-05T18:43:53.839519: step 7206, loss 0.553621.
Train: 2018-08-05T18:43:54.042596: step 7207, loss 0.544796.
Train: 2018-08-05T18:43:54.245673: step 7208, loss 0.553593.
Train: 2018-08-05T18:43:54.448750: step 7209, loss 0.660301.
Train: 2018-08-05T18:43:54.651828: step 7210, loss 0.6157.
Test: 2018-08-05T18:43:55.698428: step 7210, loss 0.548298.
Train: 2018-08-05T18:43:55.964020: step 7211, loss 0.465376.
Train: 2018-08-05T18:43:56.167068: step 7212, loss 0.571248.
Train: 2018-08-05T18:43:56.370176: step 7213, loss 0.553602.
Train: 2018-08-05T18:43:56.573252: step 7214, loss 0.553603.
Train: 2018-08-05T18:43:56.776330: step 7215, loss 0.615251.
Train: 2018-08-05T18:43:56.979377: step 7216, loss 0.588703.
Train: 2018-08-05T18:43:57.198106: step 7217, loss 0.579809.
Train: 2018-08-05T18:43:57.401184: step 7218, loss 0.57967.
Train: 2018-08-05T18:43:57.604261: step 7219, loss 0.545197.
Train: 2018-08-05T18:43:57.807308: step 7220, loss 0.596551.
Test: 2018-08-05T18:43:58.869559: step 7220, loss 0.548.
Train: 2018-08-05T18:43:59.072668: step 7221, loss 0.545399.
Train: 2018-08-05T18:43:59.275745: step 7222, loss 0.621553.
Train: 2018-08-05T18:43:59.478793: step 7223, loss 0.554025.
Train: 2018-08-05T18:43:59.697521: step 7224, loss 0.529124.
Train: 2018-08-05T18:43:59.900599: step 7225, loss 0.604032.
Train: 2018-08-05T18:44:00.103676: step 7226, loss 0.579019.
Train: 2018-08-05T18:44:00.306753: step 7227, loss 0.570759.
Train: 2018-08-05T18:44:00.525452: step 7228, loss 0.529877.
Train: 2018-08-05T18:44:00.728529: step 7229, loss 0.521789.
Train: 2018-08-05T18:44:00.931609: step 7230, loss 0.578944.
Test: 2018-08-05T18:44:01.993829: step 7230, loss 0.548155.
Train: 2018-08-05T18:44:02.196936: step 7231, loss 0.570765.
Train: 2018-08-05T18:44:02.415604: step 7232, loss 0.554376.
Train: 2018-08-05T18:44:02.618715: step 7233, loss 0.6036.
Train: 2018-08-05T18:44:02.821790: step 7234, loss 0.554345.
Train: 2018-08-05T18:44:03.024867: step 7235, loss 0.562545.
Train: 2018-08-05T18:44:03.243536: step 7236, loss 0.62833.
Train: 2018-08-05T18:44:03.446643: step 7237, loss 0.562558.
Train: 2018-08-05T18:44:03.665320: step 7238, loss 0.660846.
Train: 2018-08-05T18:44:03.884054: step 7239, loss 0.578913.
Train: 2018-08-05T18:44:04.087118: step 7240, loss 0.570812.
Test: 2018-08-05T18:44:05.149339: step 7240, loss 0.549603.
Train: 2018-08-05T18:44:05.352447: step 7241, loss 0.643065.
Train: 2018-08-05T18:44:05.555495: step 7242, loss 0.578858.
Train: 2018-08-05T18:44:05.774223: step 7243, loss 0.484371.
Train: 2018-08-05T18:44:05.977300: step 7244, loss 0.437377.
Train: 2018-08-05T18:44:06.195999: step 7245, loss 0.515479.
Train: 2018-08-05T18:44:06.414668: step 7246, loss 0.506765.
Train: 2018-08-05T18:44:06.617775: step 7247, loss 0.505763.
Train: 2018-08-05T18:44:06.805232: step 7248, loss 0.562488.
Train: 2018-08-05T18:44:07.023900: step 7249, loss 0.612785.
Train: 2018-08-05T18:44:07.227009: step 7250, loss 0.630302.
Test: 2018-08-05T18:44:08.273637: step 7250, loss 0.547869.
Train: 2018-08-05T18:44:08.492337: step 7251, loss 0.536701.
Train: 2018-08-05T18:44:08.695414: step 7252, loss 0.545127.
Train: 2018-08-05T18:44:08.914113: step 7253, loss 0.562346.
Train: 2018-08-05T18:44:09.117191: step 7254, loss 0.536204.
Train: 2018-08-05T18:44:09.335889: step 7255, loss 0.562394.
Train: 2018-08-05T18:44:09.554588: step 7256, loss 0.518276.
Train: 2018-08-05T18:44:09.773287: step 7257, loss 0.606961.
Train: 2018-08-05T18:44:09.991955: step 7258, loss 0.562514.
Train: 2018-08-05T18:44:10.195032: step 7259, loss 0.58938.
Train: 2018-08-05T18:44:10.413762: step 7260, loss 0.616222.
Test: 2018-08-05T18:44:11.460392: step 7260, loss 0.546589.
Train: 2018-08-05T18:44:11.679091: step 7261, loss 0.526847.
Train: 2018-08-05T18:44:11.882168: step 7262, loss 0.624762.
Train: 2018-08-05T18:44:12.100867: step 7263, loss 0.580127.
Train: 2018-08-05T18:44:12.319566: step 7264, loss 0.553613.
Train: 2018-08-05T18:44:12.522613: step 7265, loss 0.536178.
Train: 2018-08-05T18:44:12.741313: step 7266, loss 0.5015.
Train: 2018-08-05T18:44:12.960041: step 7267, loss 0.605801.
Train: 2018-08-05T18:44:13.163088: step 7268, loss 0.614316.
Train: 2018-08-05T18:44:13.381787: step 7269, loss 0.553728.
Train: 2018-08-05T18:44:13.600516: step 7270, loss 0.562335.
Test: 2018-08-05T18:44:14.662737: step 7270, loss 0.548107.
Train: 2018-08-05T18:44:14.865844: step 7271, loss 0.570863.
Train: 2018-08-05T18:44:15.100135: step 7272, loss 0.596278.
Train: 2018-08-05T18:44:15.318863: step 7273, loss 0.545526.
Train: 2018-08-05T18:44:15.521911: step 7274, loss 0.545614.
Train: 2018-08-05T18:44:15.740639: step 7275, loss 0.579144.
Train: 2018-08-05T18:44:15.974931: step 7276, loss 0.545739.
Train: 2018-08-05T18:44:16.193657: step 7277, loss 0.612413.
Train: 2018-08-05T18:44:16.412357: step 7278, loss 0.637119.
Train: 2018-08-05T18:44:16.631025: step 7279, loss 0.562529.
Train: 2018-08-05T18:44:16.834104: step 7280, loss 0.570768.
Test: 2018-08-05T18:44:17.896355: step 7280, loss 0.548908.
Train: 2018-08-05T18:44:18.099462: step 7281, loss 0.522045.
Train: 2018-08-05T18:44:18.318131: step 7282, loss 0.5789.
Train: 2018-08-05T18:44:18.536830: step 7283, loss 0.554629.
Train: 2018-08-05T18:44:18.755558: step 7284, loss 0.498078.
Train: 2018-08-05T18:44:18.958636: step 7285, loss 0.57079.
Train: 2018-08-05T18:44:19.192928: step 7286, loss 0.578924.
Train: 2018-08-05T18:44:19.396034: step 7287, loss 0.587114.
Train: 2018-08-05T18:44:19.614732: step 7288, loss 0.50526.
Train: 2018-08-05T18:44:19.833400: step 7289, loss 0.513102.
Train: 2018-08-05T18:44:20.036508: step 7290, loss 0.57076.
Test: 2018-08-05T18:44:21.098729: step 7290, loss 0.549185.
Train: 2018-08-05T18:44:21.317428: step 7291, loss 0.570777.
Train: 2018-08-05T18:44:21.536157: step 7292, loss 0.520207.
Train: 2018-08-05T18:44:21.754855: step 7293, loss 0.477268.
Train: 2018-08-05T18:44:21.957934: step 7294, loss 0.605439.
Train: 2018-08-05T18:44:22.176632: step 7295, loss 0.614549.
Train: 2018-08-05T18:44:22.395331: step 7296, loss 0.509931.
Train: 2018-08-05T18:44:22.614029: step 7297, loss 0.553606.
Train: 2018-08-05T18:44:22.832728: step 7298, loss 0.527015.
Train: 2018-08-05T18:44:23.051398: step 7299, loss 0.517889.
Train: 2018-08-05T18:44:23.270097: step 7300, loss 0.508591.
Test: 2018-08-05T18:44:24.332348: step 7300, loss 0.548381.
Train: 2018-08-05T18:44:25.300871: step 7301, loss 0.553637.
Train: 2018-08-05T18:44:25.503979: step 7302, loss 0.526171.
Train: 2018-08-05T18:44:25.738298: step 7303, loss 0.600038.
Train: 2018-08-05T18:44:25.941345: step 7304, loss 0.563111.
Train: 2018-08-05T18:44:26.160074: step 7305, loss 0.544518.
Train: 2018-08-05T18:44:26.394390: step 7306, loss 0.553877.
Train: 2018-08-05T18:44:26.613087: step 7307, loss 0.544529.
Train: 2018-08-05T18:44:26.831792: step 7308, loss 0.535155.
Train: 2018-08-05T18:44:27.050491: step 7309, loss 0.59149.
Train: 2018-08-05T18:44:27.269189: step 7310, loss 0.56328.
Test: 2018-08-05T18:44:28.315789: step 7310, loss 0.546959.
Train: 2018-08-05T18:44:28.534518: step 7311, loss 0.665982.
Train: 2018-08-05T18:44:28.753218: step 7312, loss 0.562645.
Train: 2018-08-05T18:44:28.971917: step 7313, loss 0.578928.
Train: 2018-08-05T18:44:29.190584: step 7314, loss 0.601912.
Train: 2018-08-05T18:44:29.393692: step 7315, loss 0.615442.
Train: 2018-08-05T18:44:29.612392: step 7316, loss 0.535509.
Train: 2018-08-05T18:44:29.831085: step 7317, loss 0.617228.
Train: 2018-08-05T18:44:30.049789: step 7318, loss 0.578896.
Train: 2018-08-05T18:44:30.268487: step 7319, loss 0.495218.
Train: 2018-08-05T18:44:30.487186: step 7320, loss 0.571118.
Test: 2018-08-05T18:44:31.549437: step 7320, loss 0.545999.
Train: 2018-08-05T18:44:31.768136: step 7321, loss 0.642449.
Train: 2018-08-05T18:44:31.986835: step 7322, loss 0.500509.
Train: 2018-08-05T18:44:32.205504: step 7323, loss 0.571255.
Train: 2018-08-05T18:44:32.424232: step 7324, loss 0.544796.
Train: 2018-08-05T18:44:32.642931: step 7325, loss 0.606381.
Train: 2018-08-05T18:44:32.861630: step 7326, loss 0.509836.
Train: 2018-08-05T18:44:33.080298: step 7327, loss 0.518637.
Train: 2018-08-05T18:44:33.283406: step 7328, loss 0.571142.
Train: 2018-08-05T18:44:33.502106: step 7329, loss 0.641239.
Train: 2018-08-05T18:44:33.720803: step 7330, loss 0.69316.
Test: 2018-08-05T18:44:34.783026: step 7330, loss 0.54716.
Train: 2018-08-05T18:44:35.001754: step 7331, loss 0.570948.
Train: 2018-08-05T18:44:35.220422: step 7332, loss 0.47724.
Train: 2018-08-05T18:44:35.439122: step 7333, loss 0.621603.
Train: 2018-08-05T18:44:35.657821: step 7334, loss 0.587568.
Train: 2018-08-05T18:44:35.876549: step 7335, loss 0.579079.
Train: 2018-08-05T18:44:36.095250: step 7336, loss 0.513044.
Train: 2018-08-05T18:44:36.313917: step 7337, loss 0.513298.
Train: 2018-08-05T18:44:36.532645: step 7338, loss 0.513313.
Train: 2018-08-05T18:44:36.751344: step 7339, loss 0.55428.
Train: 2018-08-05T18:44:36.970043: step 7340, loss 0.520892.
Test: 2018-08-05T18:44:38.016643: step 7340, loss 0.54593.
Train: 2018-08-05T18:44:38.250994: step 7341, loss 0.501023.
Train: 2018-08-05T18:44:38.469661: step 7342, loss 0.595641.
Train: 2018-08-05T18:44:38.688390: step 7343, loss 0.639361.
Train: 2018-08-05T18:44:38.907089: step 7344, loss 0.579782.
Train: 2018-08-05T18:44:39.125758: step 7345, loss 0.579193.
Train: 2018-08-05T18:44:39.344456: step 7346, loss 0.562414.
Train: 2018-08-05T18:44:39.563156: step 7347, loss 0.512452.
Train: 2018-08-05T18:44:39.781856: step 7348, loss 0.537424.
Train: 2018-08-05T18:44:40.000584: step 7349, loss 0.495572.
Train: 2018-08-05T18:44:40.219252: step 7350, loss 0.629666.
Test: 2018-08-05T18:44:41.281533: step 7350, loss 0.548685.
Train: 2018-08-05T18:44:41.500202: step 7351, loss 0.520234.
Train: 2018-08-05T18:44:41.703309: step 7352, loss 0.520026.
Train: 2018-08-05T18:44:41.922009: step 7353, loss 0.56234.
Train: 2018-08-05T18:44:42.140707: step 7354, loss 0.605216.
Train: 2018-08-05T18:44:42.359406: step 7355, loss 0.527932.
Train: 2018-08-05T18:44:42.578104: step 7356, loss 0.5105.
Train: 2018-08-05T18:44:42.796804: step 7357, loss 0.527553.
Train: 2018-08-05T18:44:43.015502: step 7358, loss 0.579933.
Train: 2018-08-05T18:44:43.249823: step 7359, loss 0.588894.
Train: 2018-08-05T18:44:43.452900: step 7360, loss 0.562446.
Test: 2018-08-05T18:44:44.515154: step 7360, loss 0.547644.
Train: 2018-08-05T18:44:44.733819: step 7361, loss 0.544716.
Train: 2018-08-05T18:44:44.952549: step 7362, loss 0.553588.
Train: 2018-08-05T18:44:45.171218: step 7363, loss 0.580348.
Train: 2018-08-05T18:44:45.389946: step 7364, loss 0.51789.
Train: 2018-08-05T18:44:45.608645: step 7365, loss 0.616216.
Train: 2018-08-05T18:44:45.827345: step 7366, loss 0.562522.
Train: 2018-08-05T18:44:46.046043: step 7367, loss 0.589247.
Train: 2018-08-05T18:44:46.264742: step 7368, loss 0.54471.
Train: 2018-08-05T18:44:46.467818: step 7369, loss 0.527036.
Train: 2018-08-05T18:44:46.686518: step 7370, loss 0.562437.
Test: 2018-08-05T18:44:47.748739: step 7370, loss 0.54809.
Train: 2018-08-05T18:44:47.967468: step 7371, loss 0.580083.
Train: 2018-08-05T18:44:48.186167: step 7372, loss 0.553607.
Train: 2018-08-05T18:44:48.404836: step 7373, loss 0.650173.
Train: 2018-08-05T18:44:48.623564: step 7374, loss 0.622972.
Train: 2018-08-05T18:44:48.842263: step 7375, loss 0.463117.
Train: 2018-08-05T18:44:49.060932: step 7376, loss 0.528278.
Train: 2018-08-05T18:44:49.295283: step 7377, loss 0.57956.
Train: 2018-08-05T18:44:49.513983: step 7378, loss 0.528074.
Train: 2018-08-05T18:44:49.717058: step 7379, loss 0.605174.
Train: 2018-08-05T18:44:49.951379: step 7380, loss 0.536695.
Test: 2018-08-05T18:44:50.997978: step 7380, loss 0.548071.
Train: 2018-08-05T18:44:51.216708: step 7381, loss 0.528168.
Train: 2018-08-05T18:44:51.419754: step 7382, loss 0.553781.
Train: 2018-08-05T18:44:51.638483: step 7383, loss 0.58805.
Train: 2018-08-05T18:44:51.857182: step 7384, loss 0.519474.
Train: 2018-08-05T18:44:52.075881: step 7385, loss 0.579524.
Train: 2018-08-05T18:44:52.278958: step 7386, loss 0.596757.
Train: 2018-08-05T18:44:52.497656: step 7387, loss 0.545143.
Train: 2018-08-05T18:44:52.716355: step 7388, loss 0.570931.
Train: 2018-08-05T18:44:52.935055: step 7389, loss 0.562335.
Train: 2018-08-05T18:44:53.138102: step 7390, loss 0.596675.
Test: 2018-08-05T18:44:54.200353: step 7390, loss 0.547643.
Train: 2018-08-05T18:44:54.419082: step 7391, loss 0.502403.
Train: 2018-08-05T18:44:54.622130: step 7392, loss 0.553763.
Train: 2018-08-05T18:44:54.840858: step 7393, loss 0.579504.
Train: 2018-08-05T18:44:55.059557: step 7394, loss 0.562335.
Train: 2018-08-05T18:44:55.278256: step 7395, loss 0.639627.
Train: 2018-08-05T18:44:55.481302: step 7396, loss 0.553788.
Train: 2018-08-05T18:44:55.700032: step 7397, loss 0.511239.
Train: 2018-08-05T18:44:55.903109: step 7398, loss 0.519762.
Train: 2018-08-05T18:44:56.090565: step 7399, loss 0.489486.
Train: 2018-08-05T18:44:56.309265: step 7400, loss 0.639687.
Test: 2018-08-05T18:44:57.371486: step 7400, loss 0.548162.
Train: 2018-08-05T18:44:58.340010: step 7401, loss 0.656995.
Train: 2018-08-05T18:44:58.543116: step 7402, loss 0.51951.
Train: 2018-08-05T18:44:58.761785: step 7403, loss 0.613641.
Train: 2018-08-05T18:44:58.980514: step 7404, loss 0.579365.
Train: 2018-08-05T18:44:59.183590: step 7405, loss 0.494617.
Train: 2018-08-05T18:44:59.402290: step 7406, loss 0.604676.
Train: 2018-08-05T18:44:59.620988: step 7407, loss 0.452646.
Train: 2018-08-05T18:44:59.824035: step 7408, loss 0.604736.
Train: 2018-08-05T18:45:00.042765: step 7409, loss 0.587815.
Train: 2018-08-05T18:45:00.245842: step 7410, loss 0.621759.
Test: 2018-08-05T18:45:01.308094: step 7410, loss 0.548627.
Train: 2018-08-05T18:45:01.511140: step 7411, loss 0.570816.
Train: 2018-08-05T18:45:01.729870: step 7412, loss 0.562374.
Train: 2018-08-05T18:45:01.932947: step 7413, loss 0.512002.
Train: 2018-08-05T18:45:02.167267: step 7414, loss 0.486771.
Train: 2018-08-05T18:45:02.370345: step 7415, loss 0.587709.
Train: 2018-08-05T18:45:02.589044: step 7416, loss 0.553877.
Train: 2018-08-05T18:45:02.792091: step 7417, loss 0.545329.
Train: 2018-08-05T18:45:02.995199: step 7418, loss 0.528136.
Train: 2018-08-05T18:45:03.213897: step 7419, loss 0.657541.
Train: 2018-08-05T18:45:03.416975: step 7420, loss 0.562335.
Test: 2018-08-05T18:45:04.463606: step 7420, loss 0.547208.
Train: 2018-08-05T18:45:04.682304: step 7421, loss 0.570919.
Train: 2018-08-05T18:45:04.901002: step 7422, loss 0.53661.
Train: 2018-08-05T18:45:05.104049: step 7423, loss 0.528017.
Train: 2018-08-05T18:45:05.322779: step 7424, loss 0.476329.
Train: 2018-08-05T18:45:05.525856: step 7425, loss 0.588335.
Train: 2018-08-05T18:45:05.728904: step 7426, loss 0.571063.
Train: 2018-08-05T18:45:05.947602: step 7427, loss 0.614786.
Train: 2018-08-05T18:45:06.150710: step 7428, loss 0.58857.
Train: 2018-08-05T18:45:06.353757: step 7429, loss 0.571074.
Train: 2018-08-05T18:45:06.572486: step 7430, loss 0.597071.
Test: 2018-08-05T18:45:07.634707: step 7430, loss 0.547861.
Train: 2018-08-05T18:45:07.837815: step 7431, loss 0.528137.
Train: 2018-08-05T18:45:08.040893: step 7432, loss 0.536492.
Train: 2018-08-05T18:45:08.259591: step 7433, loss 0.579596.
Train: 2018-08-05T18:45:08.462668: step 7434, loss 0.58818.
Train: 2018-08-05T18:45:08.665746: step 7435, loss 0.467875.
Train: 2018-08-05T18:45:08.868823: step 7436, loss 0.493447.
Train: 2018-08-05T18:45:09.087521: step 7437, loss 0.640351.
Train: 2018-08-05T18:45:09.290599: step 7438, loss 0.562348.
Train: 2018-08-05T18:45:09.493677: step 7439, loss 0.579714.
Train: 2018-08-05T18:45:09.696754: step 7440, loss 0.640434.
Test: 2018-08-05T18:45:10.759007: step 7440, loss 0.548324.
Train: 2018-08-05T18:45:10.962083: step 7441, loss 0.527822.
Train: 2018-08-05T18:45:11.165131: step 7442, loss 0.545129.
Train: 2018-08-05T18:45:11.368207: step 7443, loss 0.596688.
Train: 2018-08-05T18:45:11.571315: step 7444, loss 0.519554.
Train: 2018-08-05T18:45:11.774389: step 7445, loss 0.528139.
Train: 2018-08-05T18:45:11.977469: step 7446, loss 0.553775.
Train: 2018-08-05T18:45:12.180547: step 7447, loss 0.622363.
Train: 2018-08-05T18:45:12.383628: step 7448, loss 0.622234.
Train: 2018-08-05T18:45:12.586702: step 7449, loss 0.545325.
Train: 2018-08-05T18:45:12.805402: step 7450, loss 0.570828.
Test: 2018-08-05T18:45:13.852001: step 7450, loss 0.547878.
Train: 2018-08-05T18:45:14.055109: step 7451, loss 0.596133.
Train: 2018-08-05T18:45:14.258186: step 7452, loss 0.537202.
Train: 2018-08-05T18:45:14.461264: step 7453, loss 0.595882.
Train: 2018-08-05T18:45:14.664341: step 7454, loss 0.512453.
Train: 2018-08-05T18:45:14.883040: step 7455, loss 0.52082.
Train: 2018-08-05T18:45:15.086117: step 7456, loss 0.562424.
Train: 2018-08-05T18:45:15.273573: step 7457, loss 0.528962.
Train: 2018-08-05T18:45:15.476654: step 7458, loss 0.612783.
Train: 2018-08-05T18:45:15.679728: step 7459, loss 0.654878.
Train: 2018-08-05T18:45:15.882807: step 7460, loss 0.520547.
Test: 2018-08-05T18:45:16.945057: step 7460, loss 0.547676.
Train: 2018-08-05T18:45:17.148104: step 7461, loss 0.604219.
Train: 2018-08-05T18:45:17.351211: step 7462, loss 0.570765.
Train: 2018-08-05T18:45:17.554260: step 7463, loss 0.620602.
Train: 2018-08-05T18:45:17.741716: step 7464, loss 0.595525.
Train: 2018-08-05T18:45:17.944818: step 7465, loss 0.603552.
Train: 2018-08-05T18:45:18.147900: step 7466, loss 0.578912.
Train: 2018-08-05T18:45:18.350978: step 7467, loss 0.611133.
Train: 2018-08-05T18:45:18.554024: step 7468, loss 0.530946.
Train: 2018-08-05T18:45:18.757102: step 7469, loss 0.523275.
Train: 2018-08-05T18:45:18.960181: step 7470, loss 0.563003.
Test: 2018-08-05T18:45:20.022461: step 7470, loss 0.548498.
Train: 2018-08-05T18:45:20.225538: step 7471, loss 0.673954.
Train: 2018-08-05T18:45:20.412995: step 7472, loss 0.555251.
Train: 2018-08-05T18:45:20.616072: step 7473, loss 0.531851.
Train: 2018-08-05T18:45:20.819149: step 7474, loss 0.594543.
Train: 2018-08-05T18:45:21.022198: step 7475, loss 0.563244.
Train: 2018-08-05T18:45:21.209683: step 7476, loss 0.531978.
Train: 2018-08-05T18:45:21.412731: step 7477, loss 0.571034.
Train: 2018-08-05T18:45:21.615838: step 7478, loss 0.610338.
Train: 2018-08-05T18:45:21.818915: step 7479, loss 0.53949.
Train: 2018-08-05T18:45:22.021993: step 7480, loss 0.555152.
Test: 2018-08-05T18:45:23.068593: step 7480, loss 0.550082.
Train: 2018-08-05T18:45:23.271700: step 7481, loss 0.586799.
Train: 2018-08-05T18:45:23.459157: step 7482, loss 0.507133.
Train: 2018-08-05T18:45:23.662204: step 7483, loss 0.635096.
Train: 2018-08-05T18:45:23.865311: step 7484, loss 0.546648.
Train: 2018-08-05T18:45:24.068389: step 7485, loss 0.595086.
Train: 2018-08-05T18:45:24.255844: step 7486, loss 0.4896.
Train: 2018-08-05T18:45:24.458893: step 7487, loss 0.562583.
Train: 2018-08-05T18:45:24.646378: step 7488, loss 0.562511.
Train: 2018-08-05T18:45:24.849455: step 7489, loss 0.520921.
Train: 2018-08-05T18:45:25.052533: step 7490, loss 0.545623.
Test: 2018-08-05T18:45:26.099134: step 7490, loss 0.547627.
Train: 2018-08-05T18:45:26.302240: step 7491, loss 0.596229.
Train: 2018-08-05T18:45:26.489696: step 7492, loss 0.502653.
Train: 2018-08-05T18:45:26.692774: step 7493, loss 0.588167.
Train: 2018-08-05T18:45:26.895822: step 7494, loss 0.510317.
Train: 2018-08-05T18:45:27.083308: step 7495, loss 0.52737.
Train: 2018-08-05T18:45:27.286385: step 7496, loss 0.615479.
Train: 2018-08-05T18:45:27.473841: step 7497, loss 0.562477.
Train: 2018-08-05T18:45:27.676889: step 7498, loss 0.500028.
Train: 2018-08-05T18:45:27.879967: step 7499, loss 0.526632.
Train: 2018-08-05T18:45:28.067453: step 7500, loss 0.544563.
Test: 2018-08-05T18:45:29.129674: step 7500, loss 0.547332.
Train: 2018-08-05T18:45:30.051363: step 7501, loss 0.544533.
Train: 2018-08-05T18:45:30.238788: step 7502, loss 0.498616.
Train: 2018-08-05T18:45:30.441896: step 7503, loss 0.572484.
Train: 2018-08-05T18:45:30.629352: step 7504, loss 0.600369.
Train: 2018-08-05T18:45:30.832430: step 7505, loss 0.572499.
Train: 2018-08-05T18:45:31.019886: step 7506, loss 0.535191.
Train: 2018-08-05T18:45:31.222963: step 7507, loss 0.535189.
Train: 2018-08-05T18:45:31.410389: step 7508, loss 0.516514.
Train: 2018-08-05T18:45:31.597875: step 7509, loss 0.497759.
Train: 2018-08-05T18:45:31.800922: step 7510, loss 0.516363.
Test: 2018-08-05T18:45:32.863182: step 7510, loss 0.547465.
Train: 2018-08-05T18:45:33.050661: step 7511, loss 0.572906.
Train: 2018-08-05T18:45:33.238116: step 7512, loss 0.582422.
Train: 2018-08-05T18:45:33.441163: step 7513, loss 0.620252.
Train: 2018-08-05T18:45:33.628650: step 7514, loss 0.601039.
Train: 2018-08-05T18:45:33.831728: step 7515, loss 0.581893.
Train: 2018-08-05T18:45:34.066048: step 7516, loss 0.572279.
Train: 2018-08-05T18:45:34.253474: step 7517, loss 0.617834.
Train: 2018-08-05T18:45:34.456585: step 7518, loss 0.571704.
Train: 2018-08-05T18:45:34.659661: step 7519, loss 0.562514.
Train: 2018-08-05T18:45:34.847114: step 7520, loss 0.57123.
Test: 2018-08-05T18:45:35.909365: step 7520, loss 0.548821.
Train: 2018-08-05T18:45:36.112444: step 7521, loss 0.527536.
Train: 2018-08-05T18:45:36.299899: step 7522, loss 0.605465.
Train: 2018-08-05T18:45:36.502977: step 7523, loss 0.56234.
Train: 2018-08-05T18:45:36.690434: step 7524, loss 0.49483.
Train: 2018-08-05T18:45:36.893510: step 7525, loss 0.595991.
Train: 2018-08-05T18:45:37.080961: step 7526, loss 0.579118.
Train: 2018-08-05T18:45:37.268423: step 7527, loss 0.579055.
Train: 2018-08-05T18:45:37.455849: step 7528, loss 0.579.
Train: 2018-08-05T18:45:37.658957: step 7529, loss 0.562572.
Train: 2018-08-05T18:45:37.846414: step 7530, loss 0.546326.
Test: 2018-08-05T18:45:38.893012: step 7530, loss 0.550772.
Train: 2018-08-05T18:45:39.096120: step 7531, loss 0.554534.
Train: 2018-08-05T18:45:39.283570: step 7532, loss 0.546449.
Train: 2018-08-05T18:45:39.471032: step 7533, loss 0.554555.
Train: 2018-08-05T18:45:39.674109: step 7534, loss 0.538264.
Train: 2018-08-05T18:45:39.861566: step 7535, loss 0.546294.
Train: 2018-08-05T18:45:40.049021: step 7536, loss 0.496965.
Train: 2018-08-05T18:45:40.236478: step 7537, loss 0.512833.
Train: 2018-08-05T18:45:40.423933: step 7538, loss 0.51216.
Train: 2018-08-05T18:45:40.611361: step 7539, loss 0.579521.
Train: 2018-08-05T18:45:40.814437: step 7540, loss 0.562335.
Test: 2018-08-05T18:45:41.861068: step 7540, loss 0.547666.
Train: 2018-08-05T18:45:42.048554: step 7541, loss 0.492976.
Train: 2018-08-05T18:45:42.236009: step 7542, loss 0.474548.
Train: 2018-08-05T18:45:42.423466: step 7543, loss 0.508797.
Train: 2018-08-05T18:45:42.610892: step 7544, loss 0.572598.
Train: 2018-08-05T18:45:42.798378: step 7545, loss 0.553878.
Train: 2018-08-05T18:45:42.985833: step 7546, loss 0.516501.
Train: 2018-08-05T18:45:43.173260: step 7547, loss 0.676443.
Train: 2018-08-05T18:45:43.376367: step 7548, loss 0.544562.
Train: 2018-08-05T18:45:43.563823: step 7549, loss 0.620227.
Train: 2018-08-05T18:45:43.720037: step 7550, loss 0.482943.
Test: 2018-08-05T18:45:44.782283: step 7550, loss 0.546588.
Train: 2018-08-05T18:45:44.969745: step 7551, loss 0.66723.
Train: 2018-08-05T18:45:45.157200: step 7552, loss 0.572655.
Train: 2018-08-05T18:45:45.344626: step 7553, loss 0.553815.
Train: 2018-08-05T18:45:45.532083: step 7554, loss 0.544509.
Train: 2018-08-05T18:45:45.719568: step 7555, loss 0.562852.
Train: 2018-08-05T18:45:45.907024: step 7556, loss 0.499038.
Train: 2018-08-05T18:45:46.094480: step 7557, loss 0.580821.
Train: 2018-08-05T18:45:46.297528: step 7558, loss 0.625749.
Train: 2018-08-05T18:45:46.484985: step 7559, loss 0.482103.
Train: 2018-08-05T18:45:46.672470: step 7560, loss 0.517996.
Test: 2018-08-05T18:45:47.719100: step 7560, loss 0.546411.
Train: 2018-08-05T18:45:47.922178: step 7561, loss 0.482533.
Train: 2018-08-05T18:45:48.109633: step 7562, loss 0.491258.
Train: 2018-08-05T18:45:48.297090: step 7563, loss 0.544635.
Train: 2018-08-05T18:45:48.484546: step 7564, loss 0.589636.
Train: 2018-08-05T18:45:48.671973: step 7565, loss 0.598762.
Train: 2018-08-05T18:45:48.859429: step 7566, loss 0.58069.
Train: 2018-08-05T18:45:49.046884: step 7567, loss 0.580626.
Train: 2018-08-05T18:45:49.234370: step 7568, loss 0.598469.
Train: 2018-08-05T18:45:49.421827: step 7569, loss 0.517906.
Train: 2018-08-05T18:45:49.609282: step 7570, loss 0.553589.
Test: 2018-08-05T18:45:50.671529: step 7570, loss 0.547246.
Train: 2018-08-05T18:45:50.858991: step 7571, loss 0.562453.
Train: 2018-08-05T18:45:51.046417: step 7572, loss 0.597755.
Train: 2018-08-05T18:45:51.233902: step 7573, loss 0.571178.
Train: 2018-08-05T18:45:51.421329: step 7574, loss 0.562368.
Train: 2018-08-05T18:45:51.608785: step 7575, loss 0.492901.
Train: 2018-08-05T18:45:51.796273: step 7576, loss 0.553676.
Train: 2018-08-05T18:45:51.995287: step 7577, loss 0.614331.
Train: 2018-08-05T18:45:52.182742: step 7578, loss 0.579598.
Train: 2018-08-05T18:45:52.370198: step 7579, loss 0.570924.
Train: 2018-08-05T18:45:52.557655: step 7580, loss 0.536696.
Test: 2018-08-05T18:45:53.619876: step 7580, loss 0.547519.
Train: 2018-08-05T18:45:53.791745: step 7581, loss 0.545296.
Train: 2018-08-05T18:45:53.994788: step 7582, loss 0.587872.
Train: 2018-08-05T18:45:54.182274: step 7583, loss 0.570835.
Train: 2018-08-05T18:45:54.369700: step 7584, loss 0.621575.
Train: 2018-08-05T18:45:54.557187: step 7585, loss 0.579197.
Train: 2018-08-05T18:45:54.744642: step 7586, loss 0.537364.
Train: 2018-08-05T18:45:54.932099: step 7587, loss 0.504201.
Train: 2018-08-05T18:45:55.119525: step 7588, loss 0.579086.
Train: 2018-08-05T18:45:55.307012: step 7589, loss 0.595725.
Train: 2018-08-05T18:45:55.494438: step 7590, loss 0.49602.
Test: 2018-08-05T18:45:56.556720: step 7590, loss 0.549104.
Train: 2018-08-05T18:45:56.822283: step 7591, loss 0.570763.
Train: 2018-08-05T18:45:57.056604: step 7592, loss 0.54573.
Train: 2018-08-05T18:45:57.244028: step 7593, loss 0.604274.
Train: 2018-08-05T18:45:57.431484: step 7594, loss 0.478594.
Train: 2018-08-05T18:45:57.618940: step 7595, loss 0.655118.
Train: 2018-08-05T18:45:57.822048: step 7596, loss 0.570801.
Train: 2018-08-05T18:45:58.025125: step 7597, loss 0.553944.
Train: 2018-08-05T18:45:58.212580: step 7598, loss 0.579232.
Train: 2018-08-05T18:45:58.415685: step 7599, loss 0.520242.
Train: 2018-08-05T18:45:58.603114: step 7600, loss 0.613036.
Test: 2018-08-05T18:45:59.649713: step 7600, loss 0.549053.
Train: 2018-08-05T18:46:00.696344: step 7601, loss 0.55393.
Train: 2018-08-05T18:46:00.899421: step 7602, loss 0.494874.
Train: 2018-08-05T18:46:01.086908: step 7603, loss 0.570826.
Train: 2018-08-05T18:46:01.274364: step 7604, loss 0.494341.
Train: 2018-08-05T18:46:01.477411: step 7605, loss 0.570898.
Train: 2018-08-05T18:46:01.664899: step 7606, loss 0.588175.
Train: 2018-08-05T18:46:01.852353: step 7607, loss 0.588274.
Train: 2018-08-05T18:46:02.055431: step 7608, loss 0.622948.
Train: 2018-08-05T18:46:02.242890: step 7609, loss 0.545066.
Train: 2018-08-05T18:46:02.430343: step 7610, loss 0.49333.
Test: 2018-08-05T18:46:03.492596: step 7610, loss 0.547695.
Train: 2018-08-05T18:46:03.680050: step 7611, loss 0.648852.
Train: 2018-08-05T18:46:03.867506: step 7612, loss 0.579591.
Train: 2018-08-05T18:46:04.054962: step 7613, loss 0.639689.
Train: 2018-08-05T18:46:04.258035: step 7614, loss 0.579392.
Train: 2018-08-05T18:46:04.445496: step 7615, loss 0.511627.
Train: 2018-08-05T18:46:04.632952: step 7616, loss 0.553957.
Train: 2018-08-05T18:46:04.820408: step 7617, loss 0.553997.
Train: 2018-08-05T18:46:05.023488: step 7618, loss 0.495381.
Train: 2018-08-05T18:46:05.210942: step 7619, loss 0.536937.
Train: 2018-08-05T18:46:05.414021: step 7620, loss 0.544586.
Test: 2018-08-05T18:46:06.460650: step 7620, loss 0.549537.
Train: 2018-08-05T18:46:06.648106: step 7621, loss 0.598261.
Train: 2018-08-05T18:46:06.835561: step 7622, loss 0.54451.
Train: 2018-08-05T18:46:07.038641: step 7623, loss 0.562384.
Train: 2018-08-05T18:46:07.226095: step 7624, loss 0.613264.
Train: 2018-08-05T18:46:07.429172: step 7625, loss 0.453373.
Train: 2018-08-05T18:46:07.616630: step 7626, loss 0.478227.
Train: 2018-08-05T18:46:07.819706: step 7627, loss 0.604764.
Train: 2018-08-05T18:46:08.007162: step 7628, loss 0.630521.
Train: 2018-08-05T18:46:08.210239: step 7629, loss 0.56234.
Train: 2018-08-05T18:46:08.413287: step 7630, loss 0.63054.
Test: 2018-08-05T18:46:09.459947: step 7630, loss 0.546212.
Train: 2018-08-05T18:46:09.647403: step 7631, loss 0.502908.
Train: 2018-08-05T18:46:09.850480: step 7632, loss 0.519882.
Train: 2018-08-05T18:46:10.037939: step 7633, loss 0.62262.
Train: 2018-08-05T18:46:10.225392: step 7634, loss 0.604869.
Train: 2018-08-05T18:46:10.428470: step 7635, loss 0.579301.
Train: 2018-08-05T18:46:10.615928: step 7636, loss 0.528614.
Train: 2018-08-05T18:46:10.803383: step 7637, loss 0.537098.
Train: 2018-08-05T18:46:11.006459: step 7638, loss 0.587655.
Train: 2018-08-05T18:46:11.193915: step 7639, loss 0.579214.
Train: 2018-08-05T18:46:11.381372: step 7640, loss 0.520369.
Test: 2018-08-05T18:46:12.443593: step 7640, loss 0.548523.
Train: 2018-08-05T18:46:12.646701: step 7641, loss 0.553966.
Train: 2018-08-05T18:46:12.834127: step 7642, loss 0.537089.
Train: 2018-08-05T18:46:13.037236: step 7643, loss 0.50317.
Train: 2018-08-05T18:46:13.224690: step 7644, loss 0.562343.
Train: 2018-08-05T18:46:13.412149: step 7645, loss 0.588028.
Train: 2018-08-05T18:46:13.615224: step 7646, loss 0.570933.
Train: 2018-08-05T18:46:13.818272: step 7647, loss 0.570959.
Train: 2018-08-05T18:46:14.005727: step 7648, loss 0.519145.
Train: 2018-08-05T18:46:14.208806: step 7649, loss 0.579698.
Train: 2018-08-05T18:46:14.411914: step 7650, loss 0.571051.
Test: 2018-08-05T18:46:15.458512: step 7650, loss 0.547012.
Train: 2018-08-05T18:46:15.661590: step 7651, loss 0.588488.
Train: 2018-08-05T18:46:15.849077: step 7652, loss 0.588474.
Train: 2018-08-05T18:46:16.052153: step 7653, loss 0.553665.
Train: 2018-08-05T18:46:16.239610: step 7654, loss 0.657726.
Train: 2018-08-05T18:46:16.442687: step 7655, loss 0.493484.
Train: 2018-08-05T18:46:16.630113: step 7656, loss 0.553752.
Train: 2018-08-05T18:46:16.833220: step 7657, loss 0.596601.
Train: 2018-08-05T18:46:17.036300: step 7658, loss 0.579405.
Train: 2018-08-05T18:46:17.223724: step 7659, loss 0.621809.
Train: 2018-08-05T18:46:17.426831: step 7660, loss 0.545511.
Test: 2018-08-05T18:46:18.473461: step 7660, loss 0.548012.
Train: 2018-08-05T18:46:18.676539: step 7661, loss 0.604308.
Train: 2018-08-05T18:46:18.879616: step 7662, loss 0.645641.
Train: 2018-08-05T18:46:19.082693: step 7663, loss 0.529637.
Train: 2018-08-05T18:46:19.270149: step 7664, loss 0.489173.
Train: 2018-08-05T18:46:19.473197: step 7665, loss 0.521908.
Train: 2018-08-05T18:46:19.676304: step 7666, loss 0.546301.
Train: 2018-08-05T18:46:19.879382: step 7667, loss 0.603495.
Train: 2018-08-05T18:46:20.066808: step 7668, loss 0.578953.
Train: 2018-08-05T18:46:20.269916: step 7669, loss 0.521626.
Train: 2018-08-05T18:46:20.472994: step 7670, loss 0.529675.
Test: 2018-08-05T18:46:21.519592: step 7670, loss 0.548885.
Train: 2018-08-05T18:46:21.707079: step 7671, loss 0.603807.
Train: 2018-08-05T18:46:21.925778: step 7672, loss 0.529326.
Train: 2018-08-05T18:46:22.128854: step 7673, loss 0.595747.
Train: 2018-08-05T18:46:22.316312: step 7674, loss 0.529012.
Train: 2018-08-05T18:46:22.519359: step 7675, loss 0.637921.
Train: 2018-08-05T18:46:22.722466: step 7676, loss 0.579173.
Train: 2018-08-05T18:46:22.925544: step 7677, loss 0.520526.
Train: 2018-08-05T18:46:23.128621: step 7678, loss 0.562385.
Train: 2018-08-05T18:46:23.331698: step 7679, loss 0.553954.
Train: 2018-08-05T18:46:23.534778: step 7680, loss 0.486376.
Test: 2018-08-05T18:46:24.581406: step 7680, loss 0.546744.
Train: 2018-08-05T18:46:24.784485: step 7681, loss 0.613447.
Train: 2018-08-05T18:46:24.987560: step 7682, loss 0.545275.
Train: 2018-08-05T18:46:25.190638: step 7683, loss 0.588035.
Train: 2018-08-05T18:46:25.393716: step 7684, loss 0.596672.
Train: 2018-08-05T18:46:25.596794: step 7685, loss 0.545171.
Train: 2018-08-05T18:46:25.799873: step 7686, loss 0.631039.
Train: 2018-08-05T18:46:26.002948: step 7687, loss 0.528106.
Train: 2018-08-05T18:46:26.205996: step 7688, loss 0.613622.
Train: 2018-08-05T18:46:26.409073: step 7689, loss 0.511271.
Train: 2018-08-05T18:46:26.612180: step 7690, loss 0.596371.
Test: 2018-08-05T18:46:27.658812: step 7690, loss 0.54857.
Train: 2018-08-05T18:46:27.861887: step 7691, loss 0.511439.
Train: 2018-08-05T18:46:28.064935: step 7692, loss 0.5029.
Train: 2018-08-05T18:46:28.268042: step 7693, loss 0.596462.
Train: 2018-08-05T18:46:28.471123: step 7694, loss 0.536695.
Train: 2018-08-05T18:46:28.674197: step 7695, loss 0.562335.
Train: 2018-08-05T18:46:28.877275: step 7696, loss 0.570936.
Train: 2018-08-05T18:46:29.080352: step 7697, loss 0.59681.
Train: 2018-08-05T18:46:29.283432: step 7698, loss 0.458956.
Train: 2018-08-05T18:46:29.486507: step 7699, loss 0.588339.
Train: 2018-08-05T18:46:29.689606: step 7700, loss 0.518884.
Test: 2018-08-05T18:46:30.736185: step 7700, loss 0.546568.
Train: 2018-08-05T18:46:31.626631: step 7701, loss 0.543726.
Train: 2018-08-05T18:46:31.829679: step 7702, loss 0.579981.
Train: 2018-08-05T18:46:32.032788: step 7703, loss 0.588889.
Train: 2018-08-05T18:46:32.235833: step 7704, loss 0.562431.
Train: 2018-08-05T18:46:32.438941: step 7705, loss 0.553596.
Train: 2018-08-05T18:46:32.642018: step 7706, loss 0.535904.
Train: 2018-08-05T18:46:32.845095: step 7707, loss 0.562452.
Train: 2018-08-05T18:46:33.048142: step 7708, loss 0.615673.
Train: 2018-08-05T18:46:33.266872: step 7709, loss 0.518216.
Train: 2018-08-05T18:46:33.469949: step 7710, loss 0.544754.
Test: 2018-08-05T18:46:34.532205: step 7710, loss 0.548076.
Train: 2018-08-05T18:46:34.782141: step 7711, loss 0.535907.
Train: 2018-08-05T18:46:35.032088: step 7712, loss 0.562448.
Train: 2018-08-05T18:46:35.266374: step 7713, loss 0.518147.
Train: 2018-08-05T18:46:35.469481: step 7714, loss 0.598015.
Train: 2018-08-05T18:46:35.672559: step 7715, loss 0.598007.
Train: 2018-08-05T18:46:35.891260: step 7716, loss 0.624462.
Train: 2018-08-05T18:46:36.094335: step 7717, loss 0.492007.
Train: 2018-08-05T18:46:36.313034: step 7718, loss 0.544834.
Train: 2018-08-05T18:46:36.516112: step 7719, loss 0.527315.
Train: 2018-08-05T18:46:36.719189: step 7720, loss 0.615023.
Test: 2018-08-05T18:46:37.781442: step 7720, loss 0.548172.
Train: 2018-08-05T18:46:37.984487: step 7721, loss 0.55363.
Train: 2018-08-05T18:46:38.187595: step 7722, loss 0.518749.
Train: 2018-08-05T18:46:38.390672: step 7723, loss 0.588535.
Train: 2018-08-05T18:46:38.609341: step 7724, loss 0.544944.
Train: 2018-08-05T18:46:38.812448: step 7725, loss 0.562356.
Train: 2018-08-05T18:46:39.015526: step 7726, loss 0.544972.
Train: 2018-08-05T18:46:39.234195: step 7727, loss 0.571041.
Train: 2018-08-05T18:46:39.452923: step 7728, loss 0.579712.
Train: 2018-08-05T18:46:39.655971: step 7729, loss 0.562345.
Train: 2018-08-05T18:46:39.874699: step 7730, loss 0.579632.
Test: 2018-08-05T18:46:40.921330: step 7730, loss 0.547744.
Train: 2018-08-05T18:46:41.140029: step 7731, loss 0.631291.
Train: 2018-08-05T18:46:41.343106: step 7732, loss 0.613689.
Train: 2018-08-05T18:46:41.561804: step 7733, loss 0.613219.
Train: 2018-08-05T18:46:41.780505: step 7734, loss 0.495388.
Train: 2018-08-05T18:46:41.983581: step 7735, loss 0.587422.
Train: 2018-08-05T18:46:42.202249: step 7736, loss 0.562483.
Train: 2018-08-05T18:46:42.405327: step 7737, loss 0.611894.
Train: 2018-08-05T18:46:42.608434: step 7738, loss 0.538114.
Train: 2018-08-05T18:46:42.827133: step 7739, loss 0.554534.
Train: 2018-08-05T18:46:43.030213: step 7740, loss 0.538396.
Test: 2018-08-05T18:46:44.076840: step 7740, loss 0.549928.
Train: 2018-08-05T18:46:44.295539: step 7741, loss 0.586991.
Train: 2018-08-05T18:46:44.498617: step 7742, loss 0.570805.
Train: 2018-08-05T18:46:44.717315: step 7743, loss 0.546577.
Train: 2018-08-05T18:46:44.920394: step 7744, loss 0.59506.
Train: 2018-08-05T18:46:45.139092: step 7745, loss 0.49808.
Train: 2018-08-05T18:46:45.342169: step 7746, loss 0.570788.
Train: 2018-08-05T18:46:45.545216: step 7747, loss 0.636022.
Train: 2018-08-05T18:46:45.763945: step 7748, loss 0.440411.
Train: 2018-08-05T18:46:45.967023: step 7749, loss 0.603615.
Train: 2018-08-05T18:46:46.185691: step 7750, loss 0.595527.
Test: 2018-08-05T18:46:47.232351: step 7750, loss 0.548071.
Train: 2018-08-05T18:46:47.451050: step 7751, loss 0.612167.
Train: 2018-08-05T18:46:47.654130: step 7752, loss 0.57904.
Train: 2018-08-05T18:46:47.872826: step 7753, loss 0.545923.
Train: 2018-08-05T18:46:48.075904: step 7754, loss 0.5459.
Train: 2018-08-05T18:46:48.294573: step 7755, loss 0.587369.
Train: 2018-08-05T18:46:48.513301: step 7756, loss 0.562448.
Train: 2018-08-05T18:46:48.716349: step 7757, loss 0.579084.
Train: 2018-08-05T18:46:48.935079: step 7758, loss 0.545786.
Train: 2018-08-05T18:46:49.138155: step 7759, loss 0.520728.
Train: 2018-08-05T18:46:49.356823: step 7760, loss 0.554023.
Test: 2018-08-05T18:46:50.403486: step 7760, loss 0.547739.
Train: 2018-08-05T18:46:50.622185: step 7761, loss 0.537126.
Train: 2018-08-05T18:46:50.825260: step 7762, loss 0.503076.
Train: 2018-08-05T18:46:51.043959: step 7763, loss 0.58804.
Train: 2018-08-05T18:46:51.262660: step 7764, loss 0.570939.
Train: 2018-08-05T18:46:51.465705: step 7765, loss 0.562342.
Train: 2018-08-05T18:46:51.684434: step 7766, loss 0.562352.
Train: 2018-08-05T18:46:51.903135: step 7767, loss 0.588534.
Train: 2018-08-05T18:46:52.106210: step 7768, loss 0.579847.
Train: 2018-08-05T18:46:52.324878: step 7769, loss 0.5012.
Train: 2018-08-05T18:46:52.527956: step 7770, loss 0.509774.
Test: 2018-08-05T18:46:53.590237: step 7770, loss 0.547081.
Train: 2018-08-05T18:46:53.793315: step 7771, loss 0.518318.
Train: 2018-08-05T18:46:54.012014: step 7772, loss 0.589134.
Train: 2018-08-05T18:46:54.230683: step 7773, loss 0.633928.
Train: 2018-08-05T18:46:54.433790: step 7774, loss 0.544668.
Train: 2018-08-05T18:46:54.652491: step 7775, loss 0.517913.
Train: 2018-08-05T18:46:54.871188: step 7776, loss 0.526783.
Train: 2018-08-05T18:46:55.089882: step 7777, loss 0.544629.
Train: 2018-08-05T18:46:55.292934: step 7778, loss 0.688472.
Train: 2018-08-05T18:46:55.511663: step 7779, loss 0.562535.
Train: 2018-08-05T18:46:55.730356: step 7780, loss 0.606979.
Test: 2018-08-05T18:46:56.776994: step 7780, loss 0.54748.
Train: 2018-08-05T18:46:56.995690: step 7781, loss 0.553599.
Train: 2018-08-05T18:46:57.214389: step 7782, loss 0.55362.
Train: 2018-08-05T18:46:57.417466: step 7783, loss 0.510093.
Train: 2018-08-05T18:46:57.636165: step 7784, loss 0.518917.
Train: 2018-08-05T18:46:57.839213: step 7785, loss 0.536294.
Train: 2018-08-05T18:46:58.057941: step 7786, loss 0.579746.
Train: 2018-08-05T18:46:58.276611: step 7787, loss 0.527577.
Train: 2018-08-05T18:46:58.495339: step 7788, loss 0.562359.
Train: 2018-08-05T18:46:58.714038: step 7789, loss 0.536206.
Train: 2018-08-05T18:46:58.932731: step 7790, loss 0.579852.
Test: 2018-08-05T18:46:59.979335: step 7790, loss 0.549581.
Train: 2018-08-05T18:47:00.198065: step 7791, loss 0.544883.
Train: 2018-08-05T18:47:00.401113: step 7792, loss 0.623694.
Train: 2018-08-05T18:47:00.619842: step 7793, loss 0.5449.
Train: 2018-08-05T18:47:00.838510: step 7794, loss 0.605964.
Train: 2018-08-05T18:47:01.057240: step 7795, loss 0.588394.
Train: 2018-08-05T18:47:01.275940: step 7796, loss 0.519183.
Train: 2018-08-05T18:47:01.494636: step 7797, loss 0.562336.
Train: 2018-08-05T18:47:01.697714: step 7798, loss 0.62243.
Train: 2018-08-05T18:47:01.916417: step 7799, loss 0.562339.
Train: 2018-08-05T18:47:02.150728: step 7800, loss 0.570836.
Test: 2018-08-05T18:47:03.197363: step 7800, loss 0.548851.
Train: 2018-08-05T18:47:04.181508: step 7801, loss 0.511719.
Train: 2018-08-05T18:47:04.400209: step 7802, loss 0.646653.
Train: 2018-08-05T18:47:04.618905: step 7803, loss 0.528905.
Train: 2018-08-05T18:47:04.837604: step 7804, loss 0.587458.
Train: 2018-08-05T18:47:05.040681: step 7805, loss 0.495977.
Train: 2018-08-05T18:47:05.259350: step 7806, loss 0.595704.
Train: 2018-08-05T18:47:05.478049: step 7807, loss 0.520931.
Train: 2018-08-05T18:47:05.696778: step 7808, loss 0.562441.
Train: 2018-08-05T18:47:05.915477: step 7809, loss 0.554089.
Train: 2018-08-05T18:47:06.134145: step 7810, loss 0.55405.
Test: 2018-08-05T18:47:07.180775: step 7810, loss 0.54877.
Train: 2018-08-05T18:47:07.399506: step 7811, loss 0.512063.
Train: 2018-08-05T18:47:07.618203: step 7812, loss 0.545483.
Train: 2018-08-05T18:47:07.836901: step 7813, loss 0.536847.
Train: 2018-08-05T18:47:08.055600: step 7814, loss 0.545204.
Train: 2018-08-05T18:47:08.274302: step 7815, loss 0.54507.
Train: 2018-08-05T18:47:08.477377: step 7816, loss 0.553651.
Train: 2018-08-05T18:47:08.696075: step 7817, loss 0.509697.
Train: 2018-08-05T18:47:08.914775: step 7818, loss 0.589945.
Train: 2018-08-05T18:47:09.133443: step 7819, loss 0.562529.
Train: 2018-08-05T18:47:09.352172: step 7820, loss 0.607281.
Test: 2018-08-05T18:47:10.398770: step 7820, loss 0.546146.
Train: 2018-08-05T18:47:10.617470: step 7821, loss 0.553591.
Train: 2018-08-05T18:47:10.836170: step 7822, loss 0.598416.
Train: 2018-08-05T18:47:11.054898: step 7823, loss 0.508864.
Train: 2018-08-05T18:47:11.273599: step 7824, loss 0.63416.
Train: 2018-08-05T18:47:11.492266: step 7825, loss 0.624901.
Train: 2018-08-05T18:47:11.710994: step 7826, loss 0.535911.
Train: 2018-08-05T18:47:11.929693: step 7827, loss 0.553611.
Train: 2018-08-05T18:47:12.148362: step 7828, loss 0.544893.
Train: 2018-08-05T18:47:12.351440: step 7829, loss 0.588465.
Train: 2018-08-05T18:47:12.570139: step 7830, loss 0.553689.
Test: 2018-08-05T18:47:13.632390: step 7830, loss 0.546366.
Train: 2018-08-05T18:47:13.851088: step 7831, loss 0.6054.
Train: 2018-08-05T18:47:14.069817: step 7832, loss 0.562336.
Train: 2018-08-05T18:47:14.272865: step 7833, loss 0.477361.
Train: 2018-08-05T18:47:14.507216: step 7834, loss 0.613315.
Train: 2018-08-05T18:47:14.710292: step 7835, loss 0.553891.
Train: 2018-08-05T18:47:14.928961: step 7836, loss 0.613032.
Train: 2018-08-05T18:47:15.147692: step 7837, loss 0.570787.
Train: 2018-08-05T18:47:15.366388: step 7838, loss 0.470483.
Train: 2018-08-05T18:47:15.585057: step 7839, loss 0.595891.
Train: 2018-08-05T18:47:15.803786: step 7840, loss 0.570774.
Test: 2018-08-05T18:47:16.866006: step 7840, loss 0.549405.
Train: 2018-08-05T18:47:17.069115: step 7841, loss 0.462053.
Train: 2018-08-05T18:47:17.287813: step 7842, loss 0.562379.
Train: 2018-08-05T18:47:17.506512: step 7843, loss 0.630037.
Train: 2018-08-05T18:47:17.725182: step 7844, loss 0.50308.
Train: 2018-08-05T18:47:17.943910: step 7845, loss 0.536822.
Train: 2018-08-05T18:47:18.162608: step 7846, loss 0.528109.
Train: 2018-08-05T18:47:18.365685: step 7847, loss 0.536443.
Train: 2018-08-05T18:47:18.584387: step 7848, loss 0.535895.
Train: 2018-08-05T18:47:18.803083: step 7849, loss 0.535096.
Train: 2018-08-05T18:47:19.021752: step 7850, loss 0.57746.
Test: 2018-08-05T18:47:20.068383: step 7850, loss 0.546674.
Train: 2018-08-05T18:47:20.287080: step 7851, loss 0.620884.
Train: 2018-08-05T18:47:20.474563: step 7852, loss 0.695275.
Train: 2018-08-05T18:47:20.693266: step 7853, loss 0.509972.
Train: 2018-08-05T18:47:20.911965: step 7854, loss 0.53632.
Train: 2018-08-05T18:47:21.115011: step 7855, loss 0.622793.
Train: 2018-08-05T18:47:21.333741: step 7856, loss 0.613743.
Train: 2018-08-05T18:47:21.552439: step 7857, loss 0.477552.
Train: 2018-08-05T18:47:21.771142: step 7858, loss 0.57081.
Train: 2018-08-05T18:47:21.989837: step 7859, loss 0.537145.
Train: 2018-08-05T18:47:22.208506: step 7860, loss 0.562389.
Test: 2018-08-05T18:47:23.270757: step 7860, loss 0.548201.
Train: 2018-08-05T18:47:23.473865: step 7861, loss 0.604315.
Train: 2018-08-05T18:47:23.708156: step 7862, loss 0.587476.
Train: 2018-08-05T18:47:23.926883: step 7863, loss 0.537499.
Train: 2018-08-05T18:47:24.161174: step 7864, loss 0.562461.
Train: 2018-08-05T18:47:24.364281: step 7865, loss 0.545904.
Train: 2018-08-05T18:47:24.582980: step 7866, loss 0.579042.
Train: 2018-08-05T18:47:24.817302: step 7867, loss 0.579037.
Train: 2018-08-05T18:47:25.020380: step 7868, loss 0.537675.
Train: 2018-08-05T18:47:25.239077: step 7869, loss 0.562478.
Train: 2018-08-05T18:47:25.457776: step 7870, loss 0.562469.
Test: 2018-08-05T18:47:26.519997: step 7870, loss 0.549739.
Train: 2018-08-05T18:47:26.723104: step 7871, loss 0.612268.
Train: 2018-08-05T18:47:26.941802: step 7872, loss 0.562467.
Train: 2018-08-05T18:47:27.160501: step 7873, loss 0.570757.
Train: 2018-08-05T18:47:27.363579: step 7874, loss 0.59559.
Train: 2018-08-05T18:47:27.582278: step 7875, loss 0.521209.
Train: 2018-08-05T18:47:27.800976: step 7876, loss 0.52943.
Train: 2018-08-05T18:47:28.004054: step 7877, loss 0.579051.
Train: 2018-08-05T18:47:28.222753: step 7878, loss 0.604011.
Train: 2018-08-05T18:47:28.441452: step 7879, loss 0.562449.
Train: 2018-08-05T18:47:28.644531: step 7880, loss 0.570761.
Test: 2018-08-05T18:47:29.706750: step 7880, loss 0.549134.
Train: 2018-08-05T18:47:29.909828: step 7881, loss 0.562446.
Train: 2018-08-05T18:47:30.128557: step 7882, loss 0.545806.
Train: 2018-08-05T18:47:30.347255: step 7883, loss 0.529097.
Train: 2018-08-05T18:47:30.550332: step 7884, loss 0.537305.
Train: 2018-08-05T18:47:30.769033: step 7885, loss 0.596032.
Train: 2018-08-05T18:47:30.987700: step 7886, loss 0.570806.
Train: 2018-08-05T18:47:31.190808: step 7887, loss 0.570817.
Train: 2018-08-05T18:47:31.409477: step 7888, loss 0.570827.
Train: 2018-08-05T18:47:31.628205: step 7889, loss 0.587801.
Train: 2018-08-05T18:47:31.831282: step 7890, loss 0.536912.
Test: 2018-08-05T18:47:32.893536: step 7890, loss 0.547777.
Train: 2018-08-05T18:47:33.112233: step 7891, loss 0.528385.
Train: 2018-08-05T18:47:33.315310: step 7892, loss 0.673087.
Train: 2018-08-05T18:47:33.534008: step 7893, loss 0.58781.
Train: 2018-08-05T18:47:33.737086: step 7894, loss 0.51169.
Train: 2018-08-05T18:47:33.955785: step 7895, loss 0.562368.
Train: 2018-08-05T18:47:34.174484: step 7896, loss 0.528655.
Train: 2018-08-05T18:47:34.393185: step 7897, loss 0.570808.
Train: 2018-08-05T18:47:34.596260: step 7898, loss 0.587713.
Train: 2018-08-05T18:47:34.799308: step 7899, loss 0.562364.
Train: 2018-08-05T18:47:35.018038: step 7900, loss 0.545478.
Test: 2018-08-05T18:47:36.064667: step 7900, loss 0.548053.
Train: 2018-08-05T18:47:37.017538: step 7901, loss 0.587715.
Train: 2018-08-05T18:47:37.236237: step 7902, loss 0.629934.
Train: 2018-08-05T18:47:37.439344: step 7903, loss 0.562382.
Train: 2018-08-05T18:47:37.658043: step 7904, loss 0.654527.
Train: 2018-08-05T18:47:37.861120: step 7905, loss 0.520959.
Train: 2018-08-05T18:47:38.079821: step 7906, loss 0.620302.
Train: 2018-08-05T18:47:38.282897: step 7907, loss 0.603536.
Train: 2018-08-05T18:47:38.501566: step 7908, loss 0.513956.
Train: 2018-08-05T18:47:38.704673: step 7909, loss 0.562723.
Train: 2018-08-05T18:47:38.907751: step 7910, loss 0.586936.
Test: 2018-08-05T18:47:39.970001: step 7910, loss 0.549224.
Train: 2018-08-05T18:47:40.173079: step 7911, loss 0.538733.
Train: 2018-08-05T18:47:40.391780: step 7912, loss 0.530748.
Train: 2018-08-05T18:47:40.610476: step 7913, loss 0.562798.
Train: 2018-08-05T18:47:40.813555: step 7914, loss 0.554703.
Train: 2018-08-05T18:47:41.032253: step 7915, loss 0.546535.
Train: 2018-08-05T18:47:41.235302: step 7916, loss 0.619568.
Train: 2018-08-05T18:47:41.454029: step 7917, loss 0.554489.
Train: 2018-08-05T18:47:41.657106: step 7918, loss 0.627931.
Train: 2018-08-05T18:47:41.860153: step 7919, loss 0.61971.
Train: 2018-08-05T18:47:42.078883: step 7920, loss 0.538295.
Test: 2018-08-05T18:47:43.141136: step 7920, loss 0.549321.
Train: 2018-08-05T18:47:43.344211: step 7921, loss 0.522126.
Train: 2018-08-05T18:47:43.547288: step 7922, loss 0.473257.
Train: 2018-08-05T18:47:43.765990: step 7923, loss 0.628122.
Train: 2018-08-05T18:47:43.984687: step 7924, loss 0.496735.
Train: 2018-08-05T18:47:44.187763: step 7925, loss 0.562467.
Train: 2018-08-05T18:47:44.390844: step 7926, loss 0.595833.
Train: 2018-08-05T18:47:44.593919: step 7927, loss 0.629561.
Train: 2018-08-05T18:47:44.796996: step 7928, loss 0.545582.
Train: 2018-08-05T18:47:45.015665: step 7929, loss 0.570794.
Train: 2018-08-05T18:47:45.218743: step 7930, loss 0.621367.
Test: 2018-08-05T18:47:46.280994: step 7930, loss 0.547952.
Train: 2018-08-05T18:47:46.484072: step 7931, loss 0.486706.
Train: 2018-08-05T18:47:46.687179: step 7932, loss 0.553937.
Train: 2018-08-05T18:47:46.890258: step 7933, loss 0.545439.
Train: 2018-08-05T18:47:47.093333: step 7934, loss 0.519875.
Train: 2018-08-05T18:47:47.296381: step 7935, loss 0.54524.
Train: 2018-08-05T18:47:47.515112: step 7936, loss 0.579548.
Train: 2018-08-05T18:47:47.733779: step 7937, loss 0.596935.
Train: 2018-08-05T18:47:47.936886: step 7938, loss 0.579679.
Train: 2018-08-05T18:47:48.139934: step 7939, loss 0.553675.
Train: 2018-08-05T18:47:48.343041: step 7940, loss 0.553669.
Test: 2018-08-05T18:47:49.405262: step 7940, loss 0.548039.
Train: 2018-08-05T18:47:49.608370: step 7941, loss 0.553662.
Train: 2018-08-05T18:47:49.811447: step 7942, loss 0.518848.
Train: 2018-08-05T18:47:50.014527: step 7943, loss 0.527439.
Train: 2018-08-05T18:47:50.217603: step 7944, loss 0.571174.
Train: 2018-08-05T18:47:50.420680: step 7945, loss 0.553605.
Train: 2018-08-05T18:47:50.623757: step 7946, loss 0.588941.
Train: 2018-08-05T18:47:50.826834: step 7947, loss 0.544751.
Train: 2018-08-05T18:47:51.045534: step 7948, loss 0.562449.
Train: 2018-08-05T18:47:51.248610: step 7949, loss 0.518141.
Train: 2018-08-05T18:47:51.451688: step 7950, loss 0.580251.
Test: 2018-08-05T18:47:52.498288: step 7950, loss 0.547423.
Train: 2018-08-05T18:47:52.701365: step 7951, loss 0.651437.
Train: 2018-08-05T18:47:52.904473: step 7952, loss 0.509339.
Train: 2018-08-05T18:47:53.107550: step 7953, loss 0.588935.
Train: 2018-08-05T18:47:53.310627: step 7954, loss 0.562408.
Train: 2018-08-05T18:47:53.513707: step 7955, loss 0.509776.
Train: 2018-08-05T18:47:53.716753: step 7956, loss 0.509806.
Train: 2018-08-05T18:47:53.935481: step 7957, loss 0.562396.
Train: 2018-08-05T18:47:54.122908: step 7958, loss 0.588795.
Train: 2018-08-05T18:47:54.326015: step 7959, loss 0.562402.
Train: 2018-08-05T18:47:54.529092: step 7960, loss 0.544828.
Test: 2018-08-05T18:47:55.591314: step 7960, loss 0.546919.
Train: 2018-08-05T18:47:55.794421: step 7961, loss 0.492129.
Train: 2018-08-05T18:47:55.981877: step 7962, loss 0.518339.
Train: 2018-08-05T18:47:56.184925: step 7963, loss 0.553591.
Train: 2018-08-05T18:47:56.388002: step 7964, loss 0.517955.
Train: 2018-08-05T18:47:56.591110: step 7965, loss 0.526694.
Train: 2018-08-05T18:47:56.794187: step 7966, loss 0.589751.
Train: 2018-08-05T18:47:56.997234: step 7967, loss 0.544556.
Train: 2018-08-05T18:47:57.200341: step 7968, loss 0.553643.
Train: 2018-08-05T18:47:57.403419: step 7969, loss 0.51713.
Train: 2018-08-05T18:47:57.606497: step 7970, loss 0.553691.
Test: 2018-08-05T18:47:58.653097: step 7970, loss 0.547761.
Train: 2018-08-05T18:47:58.887450: step 7971, loss 0.553718.
Train: 2018-08-05T18:47:59.090525: step 7972, loss 0.544509.
Train: 2018-08-05T18:47:59.293573: step 7973, loss 0.535255.
Train: 2018-08-05T18:47:59.496649: step 7974, loss 0.581621.
Train: 2018-08-05T18:47:59.684135: step 7975, loss 0.646577.
Train: 2018-08-05T18:47:59.887183: step 7976, loss 0.618308.
Train: 2018-08-05T18:48:00.090290: step 7977, loss 0.58099.
Train: 2018-08-05T18:48:00.293367: step 7978, loss 0.57108.
Train: 2018-08-05T18:48:00.496445: step 7979, loss 0.594319.
Train: 2018-08-05T18:48:00.683872: step 7980, loss 0.525165.
Test: 2018-08-05T18:48:01.746152: step 7980, loss 0.554264.
Train: 2018-08-05T18:48:01.949230: step 7981, loss 0.566526.
Train: 2018-08-05T18:48:02.136689: step 7982, loss 0.45786.
Train: 2018-08-05T18:48:02.339763: step 7983, loss 0.652829.
Train: 2018-08-05T18:48:02.542841: step 7984, loss 0.553589.
Train: 2018-08-05T18:48:02.745918: step 7985, loss 0.60739.
Train: 2018-08-05T18:48:02.948996: step 7986, loss 0.55359.
Train: 2018-08-05T18:48:03.152073: step 7987, loss 0.517868.
Train: 2018-08-05T18:48:03.339529: step 7988, loss 0.526793.
Train: 2018-08-05T18:48:03.542607: step 7989, loss 0.535697.
Train: 2018-08-05T18:48:03.745684: step 7990, loss 0.553593.
Test: 2018-08-05T18:48:04.792284: step 7990, loss 0.546748.
Train: 2018-08-05T18:48:04.995361: step 7991, loss 0.526638.
Train: 2018-08-05T18:48:05.182847: step 7992, loss 0.598678.
Train: 2018-08-05T18:48:05.385927: step 7993, loss 0.481484.
Train: 2018-08-05T18:48:05.573382: step 7994, loss 0.616973.
Train: 2018-08-05T18:48:05.776429: step 7995, loss 0.562663.
Train: 2018-08-05T18:48:05.979508: step 7996, loss 0.571686.
Train: 2018-08-05T18:48:06.182613: step 7997, loss 0.544588.
Train: 2018-08-05T18:48:06.370077: step 7998, loss 0.5716.
Train: 2018-08-05T18:48:06.573147: step 7999, loss 0.643335.
Train: 2018-08-05T18:48:06.760605: step 8000, loss 0.580295.
Test: 2018-08-05T18:48:07.822854: step 8000, loss 0.548103.
Train: 2018-08-05T18:48:08.885108: step 8001, loss 0.518349.
Train: 2018-08-05T18:48:09.088183: step 8002, loss 0.56239.
Train: 2018-08-05T18:48:09.244396: step 8003, loss 0.48792.
Train: 2018-08-05T18:48:09.447474: step 8004, loss 0.536214.
Train: 2018-08-05T18:48:09.634932: step 8005, loss 0.571084.
Train: 2018-08-05T18:48:09.838007: step 8006, loss 0.614665.
Train: 2018-08-05T18:48:10.041085: step 8007, loss 0.588404.
Train: 2018-08-05T18:48:10.228544: step 8008, loss 0.56234.
Train: 2018-08-05T18:48:10.431589: step 8009, loss 0.562335.
Train: 2018-08-05T18:48:10.619045: step 8010, loss 0.605133.
Test: 2018-08-05T18:48:11.681296: step 8010, loss 0.547362.
Train: 2018-08-05T18:48:11.868752: step 8011, loss 0.54534.
Train: 2018-08-05T18:48:12.056238: step 8012, loss 0.511593.
Train: 2018-08-05T18:48:12.259318: step 8013, loss 0.570813.
Train: 2018-08-05T18:48:12.446772: step 8014, loss 0.562367.
Train: 2018-08-05T18:48:12.634198: step 8015, loss 0.58766.
Train: 2018-08-05T18:48:12.837305: step 8016, loss 0.638066.
Train: 2018-08-05T18:48:13.024761: step 8017, loss 0.554062.
Train: 2018-08-05T18:48:13.227839: step 8018, loss 0.487656.
Train: 2018-08-05T18:48:13.415295: step 8019, loss 0.537505.
Train: 2018-08-05T18:48:13.602751: step 8020, loss 0.554098.
Test: 2018-08-05T18:48:14.665005: step 8020, loss 0.548841.
Train: 2018-08-05T18:48:14.852458: step 8021, loss 0.6042.
Train: 2018-08-05T18:48:15.039885: step 8022, loss 0.512251.
Train: 2018-08-05T18:48:15.242991: step 8023, loss 0.554.
Train: 2018-08-05T18:48:15.430448: step 8024, loss 0.528671.
Train: 2018-08-05T18:48:15.633519: step 8025, loss 0.528449.
Train: 2018-08-05T18:48:15.820952: step 8026, loss 0.605038.
Train: 2018-08-05T18:48:16.008440: step 8027, loss 0.588062.
Train: 2018-08-05T18:48:16.211515: step 8028, loss 0.52796.
Train: 2018-08-05T18:48:16.398971: step 8029, loss 0.562338.
Train: 2018-08-05T18:48:16.586397: step 8030, loss 0.614282.
Test: 2018-08-05T18:48:17.648649: step 8030, loss 0.546894.
Train: 2018-08-05T18:48:17.836105: step 8031, loss 0.553689.
Train: 2018-08-05T18:48:18.039213: step 8032, loss 0.553688.
Train: 2018-08-05T18:48:18.226668: step 8033, loss 0.553685.
Train: 2018-08-05T18:48:18.414125: step 8034, loss 0.527687.
Train: 2018-08-05T18:48:18.617171: step 8035, loss 0.571046.
Train: 2018-08-05T18:48:18.820280: step 8036, loss 0.518853.
Train: 2018-08-05T18:48:19.007737: step 8037, loss 0.553629.
Train: 2018-08-05T18:48:19.210813: step 8038, loss 0.624191.
Train: 2018-08-05T18:48:19.398268: step 8039, loss 0.571134.
Train: 2018-08-05T18:48:19.601348: step 8040, loss 0.544894.
Test: 2018-08-05T18:48:20.647976: step 8040, loss 0.548188.
Train: 2018-08-05T18:48:20.835432: step 8041, loss 0.571101.
Train: 2018-08-05T18:48:21.038509: step 8042, loss 0.544926.
Train: 2018-08-05T18:48:21.225965: step 8043, loss 0.562361.
Train: 2018-08-05T18:48:21.413422: step 8044, loss 0.510128.
Train: 2018-08-05T18:48:21.616499: step 8045, loss 0.588537.
Train: 2018-08-05T18:48:21.803955: step 8046, loss 0.562365.
Train: 2018-08-05T18:48:21.991382: step 8047, loss 0.614692.
Train: 2018-08-05T18:48:22.194488: step 8048, loss 0.571041.
Train: 2018-08-05T18:48:22.381914: step 8049, loss 0.57965.
Train: 2018-08-05T18:48:22.585023: step 8050, loss 0.562336.
Test: 2018-08-05T18:48:23.631655: step 8050, loss 0.549006.
Train: 2018-08-05T18:48:23.834730: step 8051, loss 0.596623.
Train: 2018-08-05T18:48:24.037777: step 8052, loss 0.562342.
Train: 2018-08-05T18:48:24.225263: step 8053, loss 0.528481.
Train: 2018-08-05T18:48:24.412719: step 8054, loss 0.553917.
Train: 2018-08-05T18:48:24.600176: step 8055, loss 0.612966.
Train: 2018-08-05T18:48:24.803255: step 8056, loss 0.570783.
Train: 2018-08-05T18:48:24.990679: step 8057, loss 0.537357.
Train: 2018-08-05T18:48:25.178136: step 8058, loss 0.604104.
Train: 2018-08-05T18:48:25.381245: step 8059, loss 0.620555.
Train: 2018-08-05T18:48:25.568669: step 8060, loss 0.587237.
Test: 2018-08-05T18:48:26.615299: step 8060, loss 0.549307.
Train: 2018-08-05T18:48:26.818406: step 8061, loss 0.505342.
Train: 2018-08-05T18:48:27.021479: step 8062, loss 0.5463.
Train: 2018-08-05T18:48:27.208939: step 8063, loss 0.578927.
Train: 2018-08-05T18:48:27.412017: step 8064, loss 0.489326.
Train: 2018-08-05T18:48:27.599473: step 8065, loss 0.595312.
Train: 2018-08-05T18:48:27.802552: step 8066, loss 0.595371.
Train: 2018-08-05T18:48:27.990006: step 8067, loss 0.521505.
Train: 2018-08-05T18:48:28.177462: step 8068, loss 0.546034.
Train: 2018-08-05T18:48:28.380542: step 8069, loss 0.570757.
Train: 2018-08-05T18:48:28.567996: step 8070, loss 0.537487.
Test: 2018-08-05T18:48:29.614596: step 8070, loss 0.548239.
Train: 2018-08-05T18:48:29.802083: step 8071, loss 0.537303.
Train: 2018-08-05T18:48:30.005130: step 8072, loss 0.587656.
Train: 2018-08-05T18:48:30.192586: step 8073, loss 0.536944.
Train: 2018-08-05T18:48:30.380074: step 8074, loss 0.56234.
Train: 2018-08-05T18:48:30.583149: step 8075, loss 0.630902.
Train: 2018-08-05T18:48:30.786196: step 8076, loss 0.562335.
Train: 2018-08-05T18:48:30.973683: step 8077, loss 0.510839.
Train: 2018-08-05T18:48:31.161140: step 8078, loss 0.570951.
Train: 2018-08-05T18:48:31.348595: step 8079, loss 0.58825.
Train: 2018-08-05T18:48:31.551673: step 8080, loss 0.484562.
Test: 2018-08-05T18:48:32.598273: step 8080, loss 0.547444.
Train: 2018-08-05T18:48:32.801350: step 8081, loss 0.5971.
Train: 2018-08-05T18:48:32.988836: step 8082, loss 0.632014.
Train: 2018-08-05T18:48:33.191914: step 8083, loss 0.54498.
Train: 2018-08-05T18:48:33.394991: step 8084, loss 0.545.
Train: 2018-08-05T18:48:33.582447: step 8085, loss 0.484313.
Train: 2018-08-05T18:48:33.785525: step 8086, loss 0.588478.
Train: 2018-08-05T18:48:33.972981: step 8087, loss 0.553642.
Train: 2018-08-05T18:48:34.176058: step 8088, loss 0.527412.
Train: 2018-08-05T18:48:34.379105: step 8089, loss 0.544847.
Train: 2018-08-05T18:48:34.566561: step 8090, loss 0.580021.
Test: 2018-08-05T18:48:35.628845: step 8090, loss 0.546876.
Train: 2018-08-05T18:48:35.816269: step 8091, loss 0.571246.
Train: 2018-08-05T18:48:36.019376: step 8092, loss 0.500618.
Train: 2018-08-05T18:48:36.206802: step 8093, loss 0.509266.
Train: 2018-08-05T18:48:36.409909: step 8094, loss 0.508987.
Train: 2018-08-05T18:48:36.597366: step 8095, loss 0.544605.
Train: 2018-08-05T18:48:36.800443: step 8096, loss 0.544559.
Train: 2018-08-05T18:48:36.987894: step 8097, loss 0.599305.
Train: 2018-08-05T18:48:37.175356: step 8098, loss 0.608613.
Train: 2018-08-05T18:48:37.378404: step 8099, loss 0.562822.
Train: 2018-08-05T18:48:37.565889: step 8100, loss 0.553663.
Test: 2018-08-05T18:48:38.628143: step 8100, loss 0.547542.
Train: 2018-08-05T18:48:39.612285: step 8101, loss 0.480706.
Train: 2018-08-05T18:48:39.815362: step 8102, loss 0.535388.
Train: 2018-08-05T18:48:40.002789: step 8103, loss 0.489549.
Train: 2018-08-05T18:48:40.205865: step 8104, loss 0.609001.
Train: 2018-08-05T18:48:40.393322: step 8105, loss 0.599858.
Train: 2018-08-05T18:48:40.596429: step 8106, loss 0.5261.
Train: 2018-08-05T18:48:40.783856: step 8107, loss 0.553711.
Train: 2018-08-05T18:48:40.971342: step 8108, loss 0.608823.
Train: 2018-08-05T18:48:41.174390: step 8109, loss 0.52624.
Train: 2018-08-05T18:48:41.361875: step 8110, loss 0.590106.
Test: 2018-08-05T18:48:42.424126: step 8110, loss 0.546506.
Train: 2018-08-05T18:48:42.611553: step 8111, loss 0.580819.
Train: 2018-08-05T18:48:42.814662: step 8112, loss 0.616631.
Train: 2018-08-05T18:48:43.017708: step 8113, loss 0.526845.
Train: 2018-08-05T18:48:43.205163: step 8114, loss 0.491663.
Train: 2018-08-05T18:48:43.408271: step 8115, loss 0.597702.
Train: 2018-08-05T18:48:43.595726: step 8116, loss 0.588711.
Train: 2018-08-05T18:48:43.798805: step 8117, loss 0.510077.
Train: 2018-08-05T18:48:44.001884: step 8118, loss 0.571037.
Train: 2018-08-05T18:48:44.204929: step 8119, loss 0.53638.
Train: 2018-08-05T18:48:44.392385: step 8120, loss 0.657355.
Test: 2018-08-05T18:48:45.454667: step 8120, loss 0.548809.
Train: 2018-08-05T18:48:45.642123: step 8121, loss 0.545192.
Train: 2018-08-05T18:48:45.845200: step 8122, loss 0.604945.
Train: 2018-08-05T18:48:46.032626: step 8123, loss 0.537002.
Train: 2018-08-05T18:48:46.235734: step 8124, loss 0.59601.
Train: 2018-08-05T18:48:46.423190: step 8125, loss 0.579118.
Train: 2018-08-05T18:48:46.626270: step 8126, loss 0.57905.
Train: 2018-08-05T18:48:46.813727: step 8127, loss 0.570757.
Train: 2018-08-05T18:48:47.016802: step 8128, loss 0.521666.
Train: 2018-08-05T18:48:47.219849: step 8129, loss 0.538113.
Train: 2018-08-05T18:48:47.422956: step 8130, loss 0.50523.
Test: 2018-08-05T18:48:48.469556: step 8130, loss 0.548189.
Train: 2018-08-05T18:48:48.657045: step 8131, loss 0.588102.
Train: 2018-08-05T18:48:48.860119: step 8132, loss 0.494648.
Train: 2018-08-05T18:48:49.047576: step 8133, loss 0.562514.
Train: 2018-08-05T18:48:49.266274: step 8134, loss 0.544518.
Train: 2018-08-05T18:48:49.469351: step 8135, loss 0.564267.
Train: 2018-08-05T18:48:49.656808: step 8136, loss 0.581931.
Train: 2018-08-05T18:48:49.859885: step 8137, loss 0.562385.
Train: 2018-08-05T18:48:50.062966: step 8138, loss 0.545503.
Train: 2018-08-05T18:48:50.250421: step 8139, loss 0.637564.
Train: 2018-08-05T18:48:50.453497: step 8140, loss 0.545812.
Test: 2018-08-05T18:48:51.500097: step 8140, loss 0.549178.
Train: 2018-08-05T18:48:51.703173: step 8141, loss 0.62054.
Train: 2018-08-05T18:48:51.906275: step 8142, loss 0.620269.
Train: 2018-08-05T18:48:52.093737: step 8143, loss 0.587136.
Train: 2018-08-05T18:48:52.296815: step 8144, loss 0.603257.
Train: 2018-08-05T18:48:52.495814: step 8145, loss 0.562793.
Train: 2018-08-05T18:48:52.683271: step 8146, loss 0.515037.
Train: 2018-08-05T18:48:52.886347: step 8147, loss 0.578859.
Train: 2018-08-05T18:48:53.089426: step 8148, loss 0.547124.
Train: 2018-08-05T18:48:53.292502: step 8149, loss 0.523341.
Train: 2018-08-05T18:48:53.479959: step 8150, loss 0.570866.
Test: 2018-08-05T18:48:54.542210: step 8150, loss 0.548409.
Train: 2018-08-05T18:48:54.745259: step 8151, loss 0.595655.
Train: 2018-08-05T18:48:54.948335: step 8152, loss 0.554772.
Train: 2018-08-05T18:48:55.151412: step 8153, loss 0.538803.
Train: 2018-08-05T18:48:55.307656: step 8154, loss 0.579951.
Train: 2018-08-05T18:48:55.510733: step 8155, loss 0.514212.
Train: 2018-08-05T18:48:55.713810: step 8156, loss 0.587068.
Train: 2018-08-05T18:48:55.916887: step 8157, loss 0.603533.
Train: 2018-08-05T18:48:56.119966: step 8158, loss 0.603629.
Train: 2018-08-05T18:48:56.323043: step 8159, loss 0.644708.
Train: 2018-08-05T18:48:56.526090: step 8160, loss 0.594893.
Test: 2018-08-05T18:48:57.588342: step 8160, loss 0.552877.
Train: 2018-08-05T18:48:57.775828: step 8161, loss 0.535688.
Train: 2018-08-05T18:48:57.978905: step 8162, loss 0.578893.
Train: 2018-08-05T18:48:58.181988: step 8163, loss 0.562744.
Train: 2018-08-05T18:48:58.385030: step 8164, loss 0.546331.
Train: 2018-08-05T18:48:58.588137: step 8165, loss 0.578935.
Train: 2018-08-05T18:48:58.775594: step 8166, loss 0.480828.
Train: 2018-08-05T18:48:58.978672: step 8167, loss 0.570757.
Train: 2018-08-05T18:48:59.181718: step 8168, loss 0.562467.
Train: 2018-08-05T18:48:59.384829: step 8169, loss 0.579112.
Train: 2018-08-05T18:48:59.587874: step 8170, loss 0.604313.
Test: 2018-08-05T18:49:00.634503: step 8170, loss 0.547974.
Train: 2018-08-05T18:49:00.837611: step 8171, loss 0.562387.
Train: 2018-08-05T18:49:01.040688: step 8172, loss 0.553963.
Train: 2018-08-05T18:49:01.243765: step 8173, loss 0.52862.
Train: 2018-08-05T18:49:01.446813: step 8174, loss 0.621683.
Train: 2018-08-05T18:49:01.649892: step 8175, loss 0.536911.
Train: 2018-08-05T18:49:01.852998: step 8176, loss 0.562346.
Train: 2018-08-05T18:49:02.056076: step 8177, loss 0.545311.
Train: 2018-08-05T18:49:02.259152: step 8178, loss 0.553797.
Train: 2018-08-05T18:49:02.462202: step 8179, loss 0.570903.
Train: 2018-08-05T18:49:02.665307: step 8180, loss 0.536575.
Test: 2018-08-05T18:49:03.727559: step 8180, loss 0.547549.
Train: 2018-08-05T18:49:03.930637: step 8181, loss 0.61404.
Train: 2018-08-05T18:49:04.133714: step 8182, loss 0.570953.
Train: 2018-08-05T18:49:04.336791: step 8183, loss 0.527899.
Train: 2018-08-05T18:49:04.539839: step 8184, loss 0.570958.
Train: 2018-08-05T18:49:04.742917: step 8185, loss 0.570963.
Train: 2018-08-05T18:49:04.946024: step 8186, loss 0.553714.
Train: 2018-08-05T18:49:05.149072: step 8187, loss 0.536461.
Train: 2018-08-05T18:49:05.352149: step 8188, loss 0.5969.
Train: 2018-08-05T18:49:05.570847: step 8189, loss 0.588238.
Train: 2018-08-05T18:49:05.773926: step 8190, loss 0.493442.
Test: 2018-08-05T18:49:06.820555: step 8190, loss 0.546736.
Train: 2018-08-05T18:49:07.039283: step 8191, loss 0.588223.
Train: 2018-08-05T18:49:07.242361: step 8192, loss 0.55371.
Train: 2018-08-05T18:49:07.445439: step 8193, loss 0.562339.
Train: 2018-08-05T18:49:07.648516: step 8194, loss 0.553706.
Train: 2018-08-05T18:49:07.867215: step 8195, loss 0.596892.
Train: 2018-08-05T18:49:08.070292: step 8196, loss 0.53647.
Train: 2018-08-05T18:49:08.273340: step 8197, loss 0.536473.
Train: 2018-08-05T18:49:08.476447: step 8198, loss 0.579605.
Train: 2018-08-05T18:49:08.695146: step 8199, loss 0.59687.
Train: 2018-08-05T18:49:08.898194: step 8200, loss 0.588175.
Test: 2018-08-05T18:49:09.944853: step 8200, loss 0.5484.
Train: 2018-08-05T18:49:10.913376: step 8201, loss 0.510854.
Train: 2018-08-05T18:49:11.116453: step 8202, loss 0.570913.
Train: 2018-08-05T18:49:11.319532: step 8203, loss 0.545194.
Train: 2018-08-05T18:49:11.522608: step 8204, loss 0.493753.
Train: 2018-08-05T18:49:11.741307: step 8205, loss 0.553725.
Train: 2018-08-05T18:49:11.944385: step 8206, loss 0.570994.
Train: 2018-08-05T18:49:12.147432: step 8207, loss 0.527637.
Train: 2018-08-05T18:49:12.350539: step 8208, loss 0.536191.
Train: 2018-08-05T18:49:12.569239: step 8209, loss 0.482922.
Train: 2018-08-05T18:49:12.787937: step 8210, loss 0.572798.
Test: 2018-08-05T18:49:13.834537: step 8210, loss 0.547582.
Train: 2018-08-05T18:49:14.068860: step 8211, loss 0.563119.
Train: 2018-08-05T18:49:14.271964: step 8212, loss 0.499019.
Train: 2018-08-05T18:49:14.475042: step 8213, loss 0.572073.
Train: 2018-08-05T18:49:14.678120: step 8214, loss 0.553679.
Train: 2018-08-05T18:49:14.912440: step 8215, loss 0.562792.
Train: 2018-08-05T18:49:15.115517: step 8216, loss 0.608328.
Train: 2018-08-05T18:49:15.318595: step 8217, loss 0.544543.
Train: 2018-08-05T18:49:15.521672: step 8218, loss 0.553633.
Train: 2018-08-05T18:49:15.724718: step 8219, loss 0.571772.
Train: 2018-08-05T18:49:15.943449: step 8220, loss 0.499325.
Test: 2018-08-05T18:49:17.005699: step 8220, loss 0.548177.
Train: 2018-08-05T18:49:17.208777: step 8221, loss 0.562672.
Train: 2018-08-05T18:49:17.427476: step 8222, loss 0.544567.
Train: 2018-08-05T18:49:17.630553: step 8223, loss 0.635065.
Train: 2018-08-05T18:49:17.833600: step 8224, loss 0.544599.
Train: 2018-08-05T18:49:18.052330: step 8225, loss 0.544629.
Train: 2018-08-05T18:49:18.271028: step 8226, loss 0.571452.
Train: 2018-08-05T18:49:18.474105: step 8227, loss 0.562481.
Train: 2018-08-05T18:49:18.677184: step 8228, loss 0.58015.
Train: 2018-08-05T18:49:18.895881: step 8229, loss 0.597617.
Train: 2018-08-05T18:49:19.098959: step 8230, loss 0.588573.
Test: 2018-08-05T18:49:20.145560: step 8230, loss 0.547686.
Train: 2018-08-05T18:49:20.364287: step 8231, loss 0.493089.
Train: 2018-08-05T18:49:20.567368: step 8232, loss 0.588206.
Train: 2018-08-05T18:49:20.786064: step 8233, loss 0.476561.
Train: 2018-08-05T18:49:20.989142: step 8234, loss 0.579499.
Train: 2018-08-05T18:49:21.192219: step 8235, loss 0.622371.
Train: 2018-08-05T18:49:21.410918: step 8236, loss 0.613571.
Train: 2018-08-05T18:49:21.629616: step 8237, loss 0.53692.
Train: 2018-08-05T18:49:21.832695: step 8238, loss 0.562368.
Train: 2018-08-05T18:49:22.035771: step 8239, loss 0.528776.
Train: 2018-08-05T18:49:22.254470: step 8240, loss 0.562392.
Test: 2018-08-05T18:49:23.301070: step 8240, loss 0.548201.
Train: 2018-08-05T18:49:23.519799: step 8241, loss 0.604316.
Train: 2018-08-05T18:49:23.722877: step 8242, loss 0.478864.
Train: 2018-08-05T18:49:23.941575: step 8243, loss 0.5624.
Train: 2018-08-05T18:49:24.144653: step 8244, loss 0.545585.
Train: 2018-08-05T18:49:24.363322: step 8245, loss 0.570802.
Train: 2018-08-05T18:49:24.566399: step 8246, loss 0.503157.
Train: 2018-08-05T18:49:24.785127: step 8247, loss 0.647476.
Train: 2018-08-05T18:49:24.988176: step 8248, loss 0.536786.
Train: 2018-08-05T18:49:25.206904: step 8249, loss 0.553801.
Train: 2018-08-05T18:49:25.409982: step 8250, loss 0.528102.
Test: 2018-08-05T18:49:26.472203: step 8250, loss 0.548375.
Train: 2018-08-05T18:49:26.675310: step 8251, loss 0.527951.
Train: 2018-08-05T18:49:26.894009: step 8252, loss 0.527753.
Train: 2018-08-05T18:49:27.097086: step 8253, loss 0.553649.
Train: 2018-08-05T18:49:27.315786: step 8254, loss 0.544847.
Train: 2018-08-05T18:49:27.534484: step 8255, loss 0.544764.
Train: 2018-08-05T18:49:27.737561: step 8256, loss 0.562498.
Train: 2018-08-05T18:49:27.956260: step 8257, loss 0.59829.
Train: 2018-08-05T18:49:28.174930: step 8258, loss 0.571508.
Train: 2018-08-05T18:49:28.378006: step 8259, loss 0.490844.
Train: 2018-08-05T18:49:28.612350: step 8260, loss 0.526595.
Test: 2018-08-05T18:49:29.658986: step 8260, loss 0.548594.
Train: 2018-08-05T18:49:29.862064: step 8261, loss 0.607906.
Train: 2018-08-05T18:49:30.080762: step 8262, loss 0.55362.
Train: 2018-08-05T18:49:30.299462: step 8263, loss 0.508299.
Train: 2018-08-05T18:49:30.502538: step 8264, loss 0.64456.
Train: 2018-08-05T18:49:30.721237: step 8265, loss 0.6624.
Train: 2018-08-05T18:49:30.939937: step 8266, loss 0.616447.
Train: 2018-08-05T18:49:31.143014: step 8267, loss 0.544724.
Train: 2018-08-05T18:49:31.361713: step 8268, loss 0.571161.
Train: 2018-08-05T18:49:31.580411: step 8269, loss 0.614419.
Train: 2018-08-05T18:49:31.799111: step 8270, loss 0.545205.
Test: 2018-08-05T18:49:32.861362: step 8270, loss 0.547814.
Train: 2018-08-05T18:49:33.080031: step 8271, loss 0.536937.
Train: 2018-08-05T18:49:33.283107: step 8272, loss 0.537172.
Train: 2018-08-05T18:49:33.501807: step 8273, loss 0.570771.
Train: 2018-08-05T18:49:33.720535: step 8274, loss 0.570761.
Train: 2018-08-05T18:49:33.939203: step 8275, loss 0.504558.
Train: 2018-08-05T18:49:34.142312: step 8276, loss 0.562483.
Train: 2018-08-05T18:49:34.423466: step 8277, loss 0.579033.
Train: 2018-08-05T18:49:34.642165: step 8278, loss 0.55421.
Train: 2018-08-05T18:49:34.845272: step 8279, loss 0.54592.
Train: 2018-08-05T18:49:35.063971: step 8280, loss 0.496087.
Test: 2018-08-05T18:49:36.126192: step 8280, loss 0.548629.
Train: 2018-08-05T18:49:36.329299: step 8281, loss 0.528945.
Train: 2018-08-05T18:49:36.547999: step 8282, loss 0.536553.
Train: 2018-08-05T18:49:36.766696: step 8283, loss 0.63954.
Train: 2018-08-05T18:49:36.985366: step 8284, loss 0.562368.
Train: 2018-08-05T18:49:37.188443: step 8285, loss 0.6219.
Train: 2018-08-05T18:49:37.407172: step 8286, loss 0.553859.
Train: 2018-08-05T18:49:37.625870: step 8287, loss 0.604807.
Train: 2018-08-05T18:49:37.828919: step 8288, loss 0.570817.
Train: 2018-08-05T18:49:38.047617: step 8289, loss 0.604494.
Train: 2018-08-05T18:49:38.266316: step 8290, loss 0.579148.
Test: 2018-08-05T18:49:39.312946: step 8290, loss 0.547973.
Train: 2018-08-05T18:49:39.531674: step 8291, loss 0.612361.
Train: 2018-08-05T18:49:39.734751: step 8292, loss 0.612001.
Train: 2018-08-05T18:49:39.953452: step 8293, loss 0.562608.
Train: 2018-08-05T18:49:40.172119: step 8294, loss 0.562711.
Train: 2018-08-05T18:49:40.390819: step 8295, loss 0.538707.
Train: 2018-08-05T18:49:40.609547: step 8296, loss 0.546804.
Train: 2018-08-05T18:49:40.828216: step 8297, loss 0.611808.
Train: 2018-08-05T18:49:41.031293: step 8298, loss 0.546952.
Train: 2018-08-05T18:49:41.249992: step 8299, loss 0.586796.
Train: 2018-08-05T18:49:41.468691: step 8300, loss 0.547178.
Test: 2018-08-05T18:49:42.515321: step 8300, loss 0.550699.
Train: 2018-08-05T18:49:43.515117: step 8301, loss 0.547174.
Train: 2018-08-05T18:49:43.733816: step 8302, loss 0.555041.
Train: 2018-08-05T18:49:43.952514: step 8303, loss 0.54699.
Train: 2018-08-05T18:49:44.171183: step 8304, loss 0.538819.
Train: 2018-08-05T18:49:44.358663: step 8305, loss 0.528327.
Train: 2018-08-05T18:49:44.577368: step 8306, loss 0.497475.
Train: 2018-08-05T18:49:44.780445: step 8307, loss 0.545986.
Train: 2018-08-05T18:49:44.999144: step 8308, loss 0.54561.
Train: 2018-08-05T18:49:45.217813: step 8309, loss 0.571109.
Train: 2018-08-05T18:49:45.436542: step 8310, loss 0.544969.
Test: 2018-08-05T18:49:46.483172: step 8310, loss 0.547448.
Train: 2018-08-05T18:49:46.701872: step 8311, loss 0.597912.
Train: 2018-08-05T18:49:46.920540: step 8312, loss 0.623575.
Train: 2018-08-05T18:49:47.139268: step 8313, loss 0.597387.
Train: 2018-08-05T18:49:47.342348: step 8314, loss 0.606112.
Train: 2018-08-05T18:49:47.561045: step 8315, loss 0.553649.
Train: 2018-08-05T18:49:47.779743: step 8316, loss 0.597077.
Train: 2018-08-05T18:49:47.998442: step 8317, loss 0.545301.
Train: 2018-08-05T18:49:48.217111: step 8318, loss 0.562335.
Train: 2018-08-05T18:49:48.420219: step 8319, loss 0.639643.
Train: 2018-08-05T18:49:48.638917: step 8320, loss 0.553819.
Test: 2018-08-05T18:49:49.685547: step 8320, loss 0.548598.
Train: 2018-08-05T18:49:49.904245: step 8321, loss 0.570825.
Train: 2018-08-05T18:49:50.122944: step 8322, loss 0.545542.
Train: 2018-08-05T18:49:50.341614: step 8323, loss 0.453445.
Train: 2018-08-05T18:49:50.560342: step 8324, loss 0.4867.
Train: 2018-08-05T18:49:50.779040: step 8325, loss 0.579306.
Train: 2018-08-05T18:49:50.982089: step 8326, loss 0.536749.
Train: 2018-08-05T18:49:51.200817: step 8327, loss 0.476415.
Train: 2018-08-05T18:49:51.419516: step 8328, loss 0.562354.
Train: 2018-08-05T18:49:51.638214: step 8329, loss 0.632668.
Train: 2018-08-05T18:49:51.856913: step 8330, loss 0.571246.
Test: 2018-08-05T18:49:52.903544: step 8330, loss 0.546238.
Train: 2018-08-05T18:49:53.122242: step 8331, loss 0.553594.
Train: 2018-08-05T18:49:53.340911: step 8332, loss 0.55359.
Train: 2018-08-05T18:49:53.543989: step 8333, loss 0.544692.
Train: 2018-08-05T18:49:53.762717: step 8334, loss 0.580351.
Train: 2018-08-05T18:49:53.981415: step 8335, loss 0.544661.
Train: 2018-08-05T18:49:54.200085: step 8336, loss 0.499963.
Train: 2018-08-05T18:49:54.403193: step 8337, loss 0.445897.
Train: 2018-08-05T18:49:54.621891: step 8338, loss 0.571752.
Train: 2018-08-05T18:49:54.840560: step 8339, loss 0.535395.
Train: 2018-08-05T18:49:55.043668: step 8340, loss 0.553713.
Test: 2018-08-05T18:49:56.105918: step 8340, loss 0.548842.
Train: 2018-08-05T18:49:56.324617: step 8341, loss 0.553766.
Train: 2018-08-05T18:49:56.527694: step 8342, loss 0.581724.
Train: 2018-08-05T18:49:56.746394: step 8343, loss 0.563158.
Train: 2018-08-05T18:49:56.965092: step 8344, loss 0.460744.
Train: 2018-08-05T18:49:57.183791: step 8345, loss 0.507027.
Train: 2018-08-05T18:49:57.402490: step 8346, loss 0.563438.
Train: 2018-08-05T18:49:57.621189: step 8347, loss 0.629909.
Train: 2018-08-05T18:49:57.839887: step 8348, loss 0.601387.
Train: 2018-08-05T18:49:58.042965: step 8349, loss 0.506863.
Train: 2018-08-05T18:49:58.261664: step 8350, loss 0.56334.
Test: 2018-08-05T18:49:59.308264: step 8350, loss 0.546325.
Train: 2018-08-05T18:49:59.589478: step 8351, loss 0.619446.
Train: 2018-08-05T18:49:59.808176: step 8352, loss 0.581677.
Train: 2018-08-05T18:50:00.026870: step 8353, loss 0.526103.
Train: 2018-08-05T18:50:00.245573: step 8354, loss 0.562801.
Train: 2018-08-05T18:50:00.464243: step 8355, loss 0.526423.
Train: 2018-08-05T18:50:00.682971: step 8356, loss 0.607702.
Train: 2018-08-05T18:50:00.901670: step 8357, loss 0.598287.
Train: 2018-08-05T18:50:01.120369: step 8358, loss 0.677439.
Train: 2018-08-05T18:50:01.323449: step 8359, loss 0.544961.
Train: 2018-08-05T18:50:01.542151: step 8360, loss 0.502338.
Test: 2018-08-05T18:50:02.604367: step 8360, loss 0.547192.
Train: 2018-08-05T18:50:02.807474: step 8361, loss 0.570838.
Train: 2018-08-05T18:50:03.041795: step 8362, loss 0.469832.
Train: 2018-08-05T18:50:03.260487: step 8363, loss 0.562389.
Train: 2018-08-05T18:50:03.463570: step 8364, loss 0.604309.
Train: 2018-08-05T18:50:03.682269: step 8365, loss 0.554069.
Train: 2018-08-05T18:50:03.885347: step 8366, loss 0.570763.
Train: 2018-08-05T18:50:04.104045: step 8367, loss 0.520919.
Train: 2018-08-05T18:50:04.322744: step 8368, loss 0.620638.
Train: 2018-08-05T18:50:04.541413: step 8369, loss 0.512723.
Train: 2018-08-05T18:50:04.760142: step 8370, loss 0.58736.
Test: 2018-08-05T18:50:05.806772: step 8370, loss 0.549744.
Train: 2018-08-05T18:50:06.025470: step 8371, loss 0.53756.
Train: 2018-08-05T18:50:06.244169: step 8372, loss 0.570761.
Train: 2018-08-05T18:50:06.447246: step 8373, loss 0.504137.
Train: 2018-08-05T18:50:06.665916: step 8374, loss 0.562402.
Train: 2018-08-05T18:50:06.884643: step 8375, loss 0.562377.
Train: 2018-08-05T18:50:07.103346: step 8376, loss 0.587735.
Train: 2018-08-05T18:50:07.306420: step 8377, loss 0.596283.
Train: 2018-08-05T18:50:07.525119: step 8378, loss 0.562349.
Train: 2018-08-05T18:50:07.743788: step 8379, loss 0.545361.
Train: 2018-08-05T18:50:07.946895: step 8380, loss 0.604882.
Test: 2018-08-05T18:50:09.009116: step 8380, loss 0.547567.
Train: 2018-08-05T18:50:09.212225: step 8381, loss 0.528353.
Train: 2018-08-05T18:50:09.415302: step 8382, loss 0.570852.
Train: 2018-08-05T18:50:09.634000: step 8383, loss 0.57937.
Train: 2018-08-05T18:50:09.837077: step 8384, loss 0.553833.
Train: 2018-08-05T18:50:10.055777: step 8385, loss 0.596388.
Train: 2018-08-05T18:50:10.258824: step 8386, loss 0.553852.
Train: 2018-08-05T18:50:10.477523: step 8387, loss 0.477506.
Train: 2018-08-05T18:50:10.680631: step 8388, loss 0.613476.
Train: 2018-08-05T18:50:10.899328: step 8389, loss 0.587927.
Train: 2018-08-05T18:50:11.102407: step 8390, loss 0.613471.
Test: 2018-08-05T18:50:12.164658: step 8390, loss 0.548564.
Train: 2018-08-05T18:50:12.367705: step 8391, loss 0.579325.
Train: 2018-08-05T18:50:12.586403: step 8392, loss 0.528566.
Train: 2018-08-05T18:50:12.789481: step 8393, loss 0.537068.
Train: 2018-08-05T18:50:13.008210: step 8394, loss 0.49489.
Train: 2018-08-05T18:50:13.211258: step 8395, loss 0.570828.
Train: 2018-08-05T18:50:13.429957: step 8396, loss 0.562344.
Train: 2018-08-05T18:50:13.633058: step 8397, loss 0.613542.
Train: 2018-08-05T18:50:13.851762: step 8398, loss 0.562339.
Train: 2018-08-05T18:50:14.054810: step 8399, loss 0.587933.
Train: 2018-08-05T18:50:14.273539: step 8400, loss 0.553825.
Test: 2018-08-05T18:50:15.320139: step 8400, loss 0.547546.
Train: 2018-08-05T18:50:16.273072: step 8401, loss 0.502783.
Train: 2018-08-05T18:50:16.491740: step 8402, loss 0.596523.
Train: 2018-08-05T18:50:16.694842: step 8403, loss 0.570876.
Train: 2018-08-05T18:50:16.913516: step 8404, loss 0.511117.
Train: 2018-08-05T18:50:17.116623: step 8405, loss 0.536644.
Train: 2018-08-05T18:50:17.335291: step 8406, loss 0.545133.
Train: 2018-08-05T18:50:17.538370: step 8407, loss 0.545053.
Train: 2018-08-05T18:50:17.741477: step 8408, loss 0.605803.
Train: 2018-08-05T18:50:17.960176: step 8409, loss 0.475296.
Train: 2018-08-05T18:50:18.163254: step 8410, loss 0.588681.
Test: 2018-08-05T18:50:19.225474: step 8410, loss 0.548317.
Train: 2018-08-05T18:50:19.428582: step 8411, loss 0.606413.
Train: 2018-08-05T18:50:19.631659: step 8412, loss 0.465548.
Train: 2018-08-05T18:50:19.850358: step 8413, loss 0.562451.
Train: 2018-08-05T18:50:20.069057: step 8414, loss 0.580293.
Train: 2018-08-05T18:50:20.272134: step 8415, loss 0.553588.
Train: 2018-08-05T18:50:20.475213: step 8416, loss 0.517805.
Train: 2018-08-05T18:50:20.678289: step 8417, loss 0.571562.
Train: 2018-08-05T18:50:20.896988: step 8418, loss 0.553601.
Train: 2018-08-05T18:50:21.100065: step 8419, loss 0.544584.
Train: 2018-08-05T18:50:21.303143: step 8420, loss 0.562655.
Test: 2018-08-05T18:50:22.365394: step 8420, loss 0.547761.
Train: 2018-08-05T18:50:22.568472: step 8421, loss 0.508363.
Train: 2018-08-05T18:50:22.771551: step 8422, loss 0.662604.
Train: 2018-08-05T18:50:22.974626: step 8423, loss 0.625989.
Train: 2018-08-05T18:50:23.177674: step 8424, loss 0.526671.
Train: 2018-08-05T18:50:23.396402: step 8425, loss 0.571434.
Train: 2018-08-05T18:50:23.599480: step 8426, loss 0.589055.
Train: 2018-08-05T18:50:23.802558: step 8427, loss 0.597588.
Train: 2018-08-05T18:50:24.005605: step 8428, loss 0.510095.
Train: 2018-08-05T18:50:24.224334: step 8429, loss 0.666265.
Train: 2018-08-05T18:50:24.427411: step 8430, loss 0.65647.
Test: 2018-08-05T18:50:25.474010: step 8430, loss 0.547741.
Train: 2018-08-05T18:50:25.692734: step 8431, loss 0.562378.
Train: 2018-08-05T18:50:25.895787: step 8432, loss 0.58734.
Train: 2018-08-05T18:50:26.098895: step 8433, loss 0.587109.
Train: 2018-08-05T18:50:26.317594: step 8434, loss 0.586931.
Train: 2018-08-05T18:50:26.520642: step 8435, loss 0.562972.
Train: 2018-08-05T18:50:26.723719: step 8436, loss 0.578872.
Train: 2018-08-05T18:50:26.926826: step 8437, loss 0.547819.
Train: 2018-08-05T18:50:27.129903: step 8438, loss 0.517198.
Train: 2018-08-05T18:50:27.332980: step 8439, loss 0.586656.
Train: 2018-08-05T18:50:27.536059: step 8440, loss 0.555885.
Test: 2018-08-05T18:50:28.598278: step 8440, loss 0.551247.
Train: 2018-08-05T18:50:28.801357: step 8441, loss 0.555872.
Train: 2018-08-05T18:50:29.004464: step 8442, loss 0.563517.
Train: 2018-08-05T18:50:29.207542: step 8443, loss 0.602149.
Train: 2018-08-05T18:50:29.410620: step 8444, loss 0.532384.
Train: 2018-08-05T18:50:29.613667: step 8445, loss 0.539907.
Train: 2018-08-05T18:50:29.816745: step 8446, loss 0.547351.
Train: 2018-08-05T18:50:30.035473: step 8447, loss 0.513939.
Train: 2018-08-05T18:50:30.222929: step 8448, loss 0.507684.
Train: 2018-08-05T18:50:30.426007: step 8449, loss 0.769622.
Train: 2018-08-05T18:50:30.644706: step 8450, loss 0.544914.
Test: 2018-08-05T18:50:31.691305: step 8450, loss 0.548757.
Train: 2018-08-05T18:50:31.894412: step 8451, loss 0.579545.
Train: 2018-08-05T18:50:32.097490: step 8452, loss 0.618299.
Train: 2018-08-05T18:50:32.300567: step 8453, loss 0.594444.
Train: 2018-08-05T18:50:32.503645: step 8454, loss 0.582318.
Train: 2018-08-05T18:50:32.706722: step 8455, loss 0.558413.
Train: 2018-08-05T18:50:32.878528: step 8456, loss 0.580005.
Train: 2018-08-05T18:50:33.081634: step 8457, loss 0.524521.
Train: 2018-08-05T18:50:33.300333: step 8458, loss 0.578895.
Train: 2018-08-05T18:50:33.503405: step 8459, loss 0.547495.
Train: 2018-08-05T18:50:33.690867: step 8460, loss 0.570756.
Test: 2018-08-05T18:50:34.753118: step 8460, loss 0.54764.
Train: 2018-08-05T18:50:34.956195: step 8461, loss 0.535832.
Train: 2018-08-05T18:50:35.159274: step 8462, loss 0.527609.
Train: 2018-08-05T18:50:35.362350: step 8463, loss 0.5737.
Train: 2018-08-05T18:50:35.565428: step 8464, loss 0.553912.
Train: 2018-08-05T18:50:35.752854: step 8465, loss 0.635635.
Train: 2018-08-05T18:50:35.971582: step 8466, loss 0.649794.
Train: 2018-08-05T18:50:36.159039: step 8467, loss 0.547541.
Train: 2018-08-05T18:50:36.362086: step 8468, loss 0.578896.
Train: 2018-08-05T18:50:36.580816: step 8469, loss 0.555647.
Train: 2018-08-05T18:50:36.783893: step 8470, loss 0.617617.
Test: 2018-08-05T18:50:37.830492: step 8470, loss 0.550502.
Train: 2018-08-05T18:50:38.033601: step 8471, loss 0.563545.
Train: 2018-08-05T18:50:38.236677: step 8472, loss 0.625068.
Train: 2018-08-05T18:50:38.439755: step 8473, loss 0.594292.
Train: 2018-08-05T18:50:38.658424: step 8474, loss 0.541025.
Train: 2018-08-05T18:50:38.861531: step 8475, loss 0.533504.
Train: 2018-08-05T18:50:39.064579: step 8476, loss 0.548599.
Train: 2018-08-05T18:50:39.267686: step 8477, loss 0.571356.
Train: 2018-08-05T18:50:39.470763: step 8478, loss 0.532853.
Train: 2018-08-05T18:50:39.658220: step 8479, loss 0.493681.
Train: 2018-08-05T18:50:39.876918: step 8480, loss 0.523836.
Test: 2018-08-05T18:50:40.923549: step 8480, loss 0.549349.
Train: 2018-08-05T18:50:41.126596: step 8481, loss 0.587219.
Train: 2018-08-05T18:50:41.329703: step 8482, loss 0.595425.
Train: 2018-08-05T18:50:41.517159: step 8483, loss 0.611603.
Train: 2018-08-05T18:50:41.720236: step 8484, loss 0.570759.
Train: 2018-08-05T18:50:41.923285: step 8485, loss 0.570756.
Train: 2018-08-05T18:50:42.126391: step 8486, loss 0.579068.
Train: 2018-08-05T18:50:42.329439: step 8487, loss 0.529067.
Train: 2018-08-05T18:50:42.516895: step 8488, loss 0.629514.
Train: 2018-08-05T18:50:42.735594: step 8489, loss 0.553986.
Train: 2018-08-05T18:50:42.923050: step 8490, loss 0.528714.
Test: 2018-08-05T18:50:43.985331: step 8490, loss 0.54786.
Train: 2018-08-05T18:50:44.188379: step 8491, loss 0.680671.
Train: 2018-08-05T18:50:44.375865: step 8492, loss 0.553959.
Train: 2018-08-05T18:50:44.578942: step 8493, loss 0.595969.
Train: 2018-08-05T18:50:44.781990: step 8494, loss 0.595846.
Train: 2018-08-05T18:50:44.969476: step 8495, loss 0.56245.
Train: 2018-08-05T18:50:45.172553: step 8496, loss 0.562486.
Train: 2018-08-05T18:50:45.375630: step 8497, loss 0.570757.
Train: 2018-08-05T18:50:45.578708: step 8498, loss 0.595392.
Train: 2018-08-05T18:50:45.766164: step 8499, loss 0.636143.
Train: 2018-08-05T18:50:45.969241: step 8500, loss 0.570796.
Test: 2018-08-05T18:50:47.031467: step 8500, loss 0.548811.
Train: 2018-08-05T18:50:48.046880: step 8501, loss 0.546714.
Train: 2018-08-05T18:50:48.234306: step 8502, loss 0.578863.
Train: 2018-08-05T18:50:48.437383: step 8503, loss 0.554965.
Train: 2018-08-05T18:50:48.624870: step 8504, loss 0.602695.
Train: 2018-08-05T18:50:48.827947: step 8505, loss 0.626343.
Train: 2018-08-05T18:50:49.031025: step 8506, loss 0.53173.
Train: 2018-08-05T18:50:49.234072: step 8507, loss 0.49269.
Train: 2018-08-05T18:50:49.421558: step 8508, loss 0.578868.
Train: 2018-08-05T18:50:49.624635: step 8509, loss 0.539365.
Train: 2018-08-05T18:50:49.812092: step 8510, loss 0.483475.
Test: 2018-08-05T18:50:50.874343: step 8510, loss 0.548978.
Train: 2018-08-05T18:50:51.077415: step 8511, loss 0.554739.
Train: 2018-08-05T18:50:51.264847: step 8512, loss 0.513778.
Train: 2018-08-05T18:50:51.452303: step 8513, loss 0.54596.
Train: 2018-08-05T18:50:51.655380: step 8514, loss 0.629528.
Train: 2018-08-05T18:50:51.858458: step 8515, loss 0.545427.
Train: 2018-08-05T18:50:52.045914: step 8516, loss 0.605051.
Train: 2018-08-05T18:50:52.248991: step 8517, loss 0.605246.
Train: 2018-08-05T18:50:52.436447: step 8518, loss 0.545165.
Train: 2018-08-05T18:50:52.639554: step 8519, loss 0.53654.
Train: 2018-08-05T18:50:52.827010: step 8520, loss 0.536442.
Test: 2018-08-05T18:50:53.889232: step 8520, loss 0.546957.
Train: 2018-08-05T18:50:54.092340: step 8521, loss 0.562378.
Train: 2018-08-05T18:50:54.295387: step 8522, loss 0.544808.
Train: 2018-08-05T18:50:54.482843: step 8523, loss 0.480716.
Train: 2018-08-05T18:50:54.670329: step 8524, loss 0.547825.
Train: 2018-08-05T18:50:54.873407: step 8525, loss 0.555555.
Train: 2018-08-05T18:50:55.076484: step 8526, loss 0.572546.
Train: 2018-08-05T18:50:55.279562: step 8527, loss 0.553594.
Train: 2018-08-05T18:50:55.467018: step 8528, loss 0.544707.
Train: 2018-08-05T18:50:55.670064: step 8529, loss 0.55359.
Train: 2018-08-05T18:50:55.857551: step 8530, loss 0.473669.
Test: 2018-08-05T18:50:56.919803: step 8530, loss 0.548425.
Train: 2018-08-05T18:50:57.122850: step 8531, loss 0.589324.
Train: 2018-08-05T18:50:57.310336: step 8532, loss 0.571505.
Train: 2018-08-05T18:50:57.497792: step 8533, loss 0.544628.
Train: 2018-08-05T18:50:57.700840: step 8534, loss 0.598468.
Train: 2018-08-05T18:50:57.903916: step 8535, loss 0.580451.
Train: 2018-08-05T18:50:58.107024: step 8536, loss 0.553588.
Train: 2018-08-05T18:50:58.294480: step 8537, loss 0.562468.
Train: 2018-08-05T18:50:58.497560: step 8538, loss 0.59775.
Train: 2018-08-05T18:50:58.684983: step 8539, loss 0.519515.
Train: 2018-08-05T18:50:58.888091: step 8540, loss 0.536151.
Test: 2018-08-05T18:50:59.934722: step 8540, loss 0.548162.
Train: 2018-08-05T18:51:00.137768: step 8541, loss 0.518605.
Train: 2018-08-05T18:51:00.325255: step 8542, loss 0.597468.
Train: 2018-08-05T18:51:00.512710: step 8543, loss 0.562382.
Train: 2018-08-05T18:51:00.715788: step 8544, loss 0.588612.
Train: 2018-08-05T18:51:00.918835: step 8545, loss 0.501355.
Train: 2018-08-05T18:51:01.106316: step 8546, loss 0.553643.
Train: 2018-08-05T18:51:01.309399: step 8547, loss 0.588555.
Train: 2018-08-05T18:51:01.496826: step 8548, loss 0.553645.
Train: 2018-08-05T18:51:01.699933: step 8549, loss 0.597203.
Train: 2018-08-05T18:51:01.903011: step 8550, loss 0.544986.
Test: 2018-08-05T18:51:02.949611: step 8550, loss 0.548474.
Train: 2018-08-05T18:51:03.152698: step 8551, loss 0.622991.
Train: 2018-08-05T18:51:03.355795: step 8552, loss 0.562336.
Train: 2018-08-05T18:51:03.558872: step 8553, loss 0.476711.
Train: 2018-08-05T18:51:03.746329: step 8554, loss 0.545198.
Train: 2018-08-05T18:51:03.949376: step 8555, loss 0.562335.
Train: 2018-08-05T18:51:04.152454: step 8556, loss 0.579528.
Train: 2018-08-05T18:51:04.355532: step 8557, loss 0.562335.
Train: 2018-08-05T18:51:04.543017: step 8558, loss 0.588128.
Train: 2018-08-05T18:51:04.746094: step 8559, loss 0.562335.
Train: 2018-08-05T18:51:04.949142: step 8560, loss 0.579472.
Test: 2018-08-05T18:51:05.995772: step 8560, loss 0.547869.
Train: 2018-08-05T18:51:06.198879: step 8561, loss 0.528155.
Train: 2018-08-05T18:51:06.386306: step 8562, loss 0.511071.
Train: 2018-08-05T18:51:06.589413: step 8563, loss 0.579479.
Train: 2018-08-05T18:51:06.776870: step 8564, loss 0.519403.
Train: 2018-08-05T18:51:06.979916: step 8565, loss 0.527847.
Train: 2018-08-05T18:51:07.167403: step 8566, loss 0.605711.
Train: 2018-08-05T18:51:07.370481: step 8567, loss 0.597122.
Train: 2018-08-05T18:51:07.573557: step 8568, loss 0.562352.
Train: 2018-08-05T18:51:07.761013: step 8569, loss 0.492882.
Train: 2018-08-05T18:51:07.964091: step 8570, loss 0.527505.
Test: 2018-08-05T18:51:09.010721: step 8570, loss 0.546748.
Train: 2018-08-05T18:51:09.213768: step 8571, loss 0.597413.
Train: 2018-08-05T18:51:09.401255: step 8572, loss 0.527291.
Train: 2018-08-05T18:51:09.604302: step 8573, loss 0.51838.
Train: 2018-08-05T18:51:09.807409: step 8574, loss 0.589008.
Train: 2018-08-05T18:51:09.994865: step 8575, loss 0.633492.
Train: 2018-08-05T18:51:10.197942: step 8576, loss 0.535878.
Train: 2018-08-05T18:51:10.385393: step 8577, loss 0.57129.
Train: 2018-08-05T18:51:10.588446: step 8578, loss 0.553598.
Train: 2018-08-05T18:51:10.775933: step 8579, loss 0.491894.
Train: 2018-08-05T18:51:10.978980: step 8580, loss 0.580103.
Test: 2018-08-05T18:51:12.041231: step 8580, loss 0.547876.
Train: 2018-08-05T18:51:12.228687: step 8581, loss 0.518237.
Train: 2018-08-05T18:51:12.431796: step 8582, loss 0.597906.
Train: 2018-08-05T18:51:12.634872: step 8583, loss 0.597894.
Train: 2018-08-05T18:51:12.837949: step 8584, loss 0.606604.
Train: 2018-08-05T18:51:13.025406: step 8585, loss 0.588748.
Train: 2018-08-05T18:51:13.228483: step 8586, loss 0.553643.
Train: 2018-08-05T18:51:13.431560: step 8587, loss 0.571011.
Train: 2018-08-05T18:51:13.618988: step 8588, loss 0.510672.
Train: 2018-08-05T18:51:13.822065: step 8589, loss 0.605273.
Train: 2018-08-05T18:51:14.025172: step 8590, loss 0.536708.
Test: 2018-08-05T18:51:15.071801: step 8590, loss 0.549294.
Train: 2018-08-05T18:51:15.274848: step 8591, loss 0.511239.
Train: 2018-08-05T18:51:15.462305: step 8592, loss 0.587904.
Train: 2018-08-05T18:51:15.665383: step 8593, loss 0.562343.
Train: 2018-08-05T18:51:15.868459: step 8594, loss 0.528332.
Train: 2018-08-05T18:51:16.055916: step 8595, loss 0.528284.
Train: 2018-08-05T18:51:16.259023: step 8596, loss 0.545251.
Train: 2018-08-05T18:51:16.462071: step 8597, loss 0.579492.
Train: 2018-08-05T18:51:16.649558: step 8598, loss 0.553735.
Train: 2018-08-05T18:51:16.852605: step 8599, loss 0.631328.
Train: 2018-08-05T18:51:17.040061: step 8600, loss 0.527908.
Test: 2018-08-05T18:51:18.102312: step 8600, loss 0.546571.
Train: 2018-08-05T18:51:19.039623: step 8601, loss 0.596773.
Train: 2018-08-05T18:51:19.242670: step 8602, loss 0.519374.
Train: 2018-08-05T18:51:19.430156: step 8603, loss 0.502144.
Train: 2018-08-05T18:51:19.633233: step 8604, loss 0.648701.
Train: 2018-08-05T18:51:19.836305: step 8605, loss 0.519225.
Train: 2018-08-05T18:51:20.023768: step 8606, loss 0.536443.
Train: 2018-08-05T18:51:20.195601: step 8607, loss 0.470044.
Train: 2018-08-05T18:51:20.398680: step 8608, loss 0.544924.
Train: 2018-08-05T18:51:20.586135: step 8609, loss 0.579977.
Train: 2018-08-05T18:51:20.789213: step 8610, loss 0.571267.
Test: 2018-08-05T18:51:21.851434: step 8610, loss 0.547445.
Train: 2018-08-05T18:51:22.038920: step 8611, loss 0.571326.
Train: 2018-08-05T18:51:22.241998: step 8612, loss 0.500255.
Train: 2018-08-05T18:51:22.429453: step 8613, loss 0.616142.
Train: 2018-08-05T18:51:22.632531: step 8614, loss 0.508867.
Train: 2018-08-05T18:51:22.835609: step 8615, loss 0.544619.
Train: 2018-08-05T18:51:23.038686: step 8616, loss 0.571612.
Train: 2018-08-05T18:51:23.241763: step 8617, loss 0.580669.
Train: 2018-08-05T18:51:23.429220: step 8618, loss 0.589681.
Train: 2018-08-05T18:51:23.632266: step 8619, loss 0.607572.
Train: 2018-08-05T18:51:23.835374: step 8620, loss 0.607254.
Test: 2018-08-05T18:51:24.881974: step 8620, loss 0.546219.
Train: 2018-08-05T18:51:25.085051: step 8621, loss 0.562459.
Train: 2018-08-05T18:51:25.288159: step 8622, loss 0.597588.
Train: 2018-08-05T18:51:25.491237: step 8623, loss 0.536234.
Train: 2018-08-05T18:51:25.678693: step 8624, loss 0.56234.
Train: 2018-08-05T18:51:25.881770: step 8625, loss 0.570915.
Train: 2018-08-05T18:51:26.084847: step 8626, loss 0.596422.
Train: 2018-08-05T18:51:26.287925: step 8627, loss 0.553914.
Train: 2018-08-05T18:51:26.491002: step 8628, loss 0.570782.
Train: 2018-08-05T18:51:26.694080: step 8629, loss 0.487415.
Train: 2018-08-05T18:51:26.897154: step 8630, loss 0.554106.
Test: 2018-08-05T18:51:27.943756: step 8630, loss 0.547563.
Train: 2018-08-05T18:51:28.146865: step 8631, loss 0.545773.
Train: 2018-08-05T18:51:28.349942: step 8632, loss 0.495677.
Train: 2018-08-05T18:51:28.553020: step 8633, loss 0.604358.
Train: 2018-08-05T18:51:28.740476: step 8634, loss 0.579214.
Train: 2018-08-05T18:51:28.943523: step 8635, loss 0.553935.
Train: 2018-08-05T18:51:29.146631: step 8636, loss 0.613083.
Train: 2018-08-05T18:51:29.349707: step 8637, loss 0.613038.
Train: 2018-08-05T18:51:29.552786: step 8638, loss 0.629681.
Train: 2018-08-05T18:51:29.740242: step 8639, loss 0.53738.
Train: 2018-08-05T18:51:29.943289: step 8640, loss 0.529218.
Test: 2018-08-05T18:51:31.005570: step 8640, loss 0.548807.
Train: 2018-08-05T18:51:31.208647: step 8641, loss 0.504419.
Train: 2018-08-05T18:51:31.396104: step 8642, loss 0.554136.
Train: 2018-08-05T18:51:31.599184: step 8643, loss 0.554089.
Train: 2018-08-05T18:51:31.802259: step 8644, loss 0.528932.
Train: 2018-08-05T18:51:32.005336: step 8645, loss 0.57921.
Train: 2018-08-05T18:51:32.208384: step 8646, loss 0.570813.
Train: 2018-08-05T18:51:32.411491: step 8647, loss 0.579313.
Train: 2018-08-05T18:51:32.614568: step 8648, loss 0.553847.
Train: 2018-08-05T18:51:32.817640: step 8649, loss 0.494181.
Train: 2018-08-05T18:51:33.005102: step 8650, loss 0.622369.
Test: 2018-08-05T18:51:34.067323: step 8650, loss 0.54739.
Train: 2018-08-05T18:51:34.270431: step 8651, loss 0.579522.
Train: 2018-08-05T18:51:34.520373: step 8652, loss 0.596735.
Train: 2018-08-05T18:51:34.723449: step 8653, loss 0.519403.
Train: 2018-08-05T18:51:34.926523: step 8654, loss 0.536546.
Train: 2018-08-05T18:51:35.113983: step 8655, loss 0.622666.
Train: 2018-08-05T18:51:35.332682: step 8656, loss 0.536521.
Train: 2018-08-05T18:51:35.535760: step 8657, loss 0.570941.
Train: 2018-08-05T18:51:35.738837: step 8658, loss 0.510729.
Train: 2018-08-05T18:51:35.941914: step 8659, loss 0.588208.
Train: 2018-08-05T18:51:36.144992: step 8660, loss 0.588224.
Test: 2018-08-05T18:51:37.191592: step 8660, loss 0.547547.
Train: 2018-08-05T18:51:37.410290: step 8661, loss 0.510627.
Train: 2018-08-05T18:51:37.613397: step 8662, loss 0.467349.
Train: 2018-08-05T18:51:37.816471: step 8663, loss 0.501447.
Train: 2018-08-05T18:51:38.019554: step 8664, loss 0.624043.
Train: 2018-08-05T18:51:38.222630: step 8665, loss 0.571262.
Train: 2018-08-05T18:51:38.425701: step 8666, loss 0.527001.
Train: 2018-08-05T18:51:38.644407: step 8667, loss 0.526872.
Train: 2018-08-05T18:51:38.847483: step 8668, loss 0.499851.
Train: 2018-08-05T18:51:39.050562: step 8669, loss 0.544579.
Train: 2018-08-05T18:51:39.253608: step 8670, loss 0.517241.
Test: 2018-08-05T18:51:40.300239: step 8670, loss 0.547333.
Train: 2018-08-05T18:51:40.503316: step 8671, loss 0.636304.
Train: 2018-08-05T18:51:40.706423: step 8672, loss 0.636524.
Train: 2018-08-05T18:51:40.909501: step 8673, loss 0.663759.
Train: 2018-08-05T18:51:41.128201: step 8674, loss 0.571801.
Train: 2018-08-05T18:51:41.331278: step 8675, loss 0.535613.
Train: 2018-08-05T18:51:41.549976: step 8676, loss 0.553588.
Train: 2018-08-05T18:51:41.753053: step 8677, loss 0.553594.
Train: 2018-08-05T18:51:41.971753: step 8678, loss 0.553611.
Train: 2018-08-05T18:51:42.174829: step 8679, loss 0.518686.
Train: 2018-08-05T18:51:42.377908: step 8680, loss 0.571069.
Test: 2018-08-05T18:51:43.424537: step 8680, loss 0.548455.
Train: 2018-08-05T18:51:43.627585: step 8681, loss 0.544992.
Train: 2018-08-05T18:51:43.830661: step 8682, loss 0.579658.
Train: 2018-08-05T18:51:44.033769: step 8683, loss 0.579594.
Train: 2018-08-05T18:51:44.236847: step 8684, loss 0.519382.
Train: 2018-08-05T18:51:44.439895: step 8685, loss 0.613812.
Train: 2018-08-05T18:51:44.658623: step 8686, loss 0.511092.
Train: 2018-08-05T18:51:44.861701: step 8687, loss 0.605014.
Train: 2018-08-05T18:51:45.064748: step 8688, loss 0.52832.
Train: 2018-08-05T18:51:45.267826: step 8689, loss 0.502855.
Train: 2018-08-05T18:51:45.470933: step 8690, loss 0.579389.
Test: 2018-08-05T18:51:46.533154: step 8690, loss 0.547488.
Train: 2018-08-05T18:51:46.751882: step 8691, loss 0.579416.
Train: 2018-08-05T18:51:46.954956: step 8692, loss 0.511081.
Train: 2018-08-05T18:51:47.158038: step 8693, loss 0.588055.
Train: 2018-08-05T18:51:47.361115: step 8694, loss 0.60527.
Train: 2018-08-05T18:51:47.564192: step 8695, loss 0.545183.
Train: 2018-08-05T18:51:47.782862: step 8696, loss 0.588058.
Train: 2018-08-05T18:51:47.985970: step 8697, loss 0.519542.
Train: 2018-08-05T18:51:48.189047: step 8698, loss 0.61374.
Train: 2018-08-05T18:51:48.392129: step 8699, loss 0.622169.
Train: 2018-08-05T18:51:48.595201: step 8700, loss 0.519858.
Test: 2018-08-05T18:51:49.657453: step 8700, loss 0.548587.
Train: 2018-08-05T18:51:50.641598: step 8701, loss 0.570829.
Train: 2018-08-05T18:51:50.844674: step 8702, loss 0.553908.
Train: 2018-08-05T18:51:51.047752: step 8703, loss 0.511738.
Train: 2018-08-05T18:51:51.250829: step 8704, loss 0.520095.
Train: 2018-08-05T18:51:51.469529: step 8705, loss 0.613291.
Train: 2018-08-05T18:51:51.672607: step 8706, loss 0.638824.
Train: 2018-08-05T18:51:51.875682: step 8707, loss 0.553893.
Train: 2018-08-05T18:51:52.078760: step 8708, loss 0.545488.
Train: 2018-08-05T18:51:52.297429: step 8709, loss 0.621365.
Train: 2018-08-05T18:51:52.500537: step 8710, loss 0.520458.
Test: 2018-08-05T18:51:53.558177: step 8710, loss 0.548606.
Train: 2018-08-05T18:51:53.761225: step 8711, loss 0.54565.
Train: 2018-08-05T18:51:53.979953: step 8712, loss 0.570777.
Train: 2018-08-05T18:51:54.183000: step 8713, loss 0.646146.
Train: 2018-08-05T18:51:54.386108: step 8714, loss 0.545772.
Train: 2018-08-05T18:51:54.604776: step 8715, loss 0.612275.
Train: 2018-08-05T18:51:54.807884: step 8716, loss 0.570756.
Train: 2018-08-05T18:51:55.026586: step 8717, loss 0.611819.
Train: 2018-08-05T18:51:55.229661: step 8718, loss 0.513718.
Train: 2018-08-05T18:51:55.432738: step 8719, loss 0.530144.
Train: 2018-08-05T18:51:55.635815: step 8720, loss 0.570783.
Test: 2018-08-05T18:51:56.698037: step 8720, loss 0.549253.
Train: 2018-08-05T18:51:56.901147: step 8721, loss 0.611447.
Train: 2018-08-05T18:51:57.104222: step 8722, loss 0.562674.
Train: 2018-08-05T18:51:57.307298: step 8723, loss 0.522164.
Train: 2018-08-05T18:51:57.526001: step 8724, loss 0.578909.
Train: 2018-08-05T18:51:57.729075: step 8725, loss 0.554507.
Train: 2018-08-05T18:51:57.947744: step 8726, loss 0.58709.
Train: 2018-08-05T18:51:58.150854: step 8727, loss 0.480891.
Train: 2018-08-05T18:51:58.353931: step 8728, loss 0.554298.
Train: 2018-08-05T18:51:58.572627: step 8729, loss 0.587342.
Train: 2018-08-05T18:51:58.775675: step 8730, loss 0.529073.
Test: 2018-08-05T18:51:59.837927: step 8730, loss 0.549709.
Train: 2018-08-05T18:52:00.072277: step 8731, loss 0.587589.
Train: 2018-08-05T18:52:00.275357: step 8732, loss 0.503242.
Train: 2018-08-05T18:52:00.494053: step 8733, loss 0.596418.
Train: 2018-08-05T18:52:00.697131: step 8734, loss 0.605174.
Train: 2018-08-05T18:52:00.900177: step 8735, loss 0.527982.
Train: 2018-08-05T18:52:01.118877: step 8736, loss 0.553713.
Train: 2018-08-05T18:52:01.321984: step 8737, loss 0.571005.
Train: 2018-08-05T18:52:01.540652: step 8738, loss 0.562351.
Train: 2018-08-05T18:52:01.743760: step 8739, loss 0.571067.
Train: 2018-08-05T18:52:01.946838: step 8740, loss 0.597247.
Test: 2018-08-05T18:52:03.009089: step 8740, loss 0.548815.
Train: 2018-08-05T18:52:03.212137: step 8741, loss 0.518807.
Train: 2018-08-05T18:52:03.430865: step 8742, loss 0.527475.
Train: 2018-08-05T18:52:03.633937: step 8743, loss 0.588625.
Train: 2018-08-05T18:52:03.852641: step 8744, loss 0.544868.
Train: 2018-08-05T18:52:04.055718: step 8745, loss 0.597464.
Train: 2018-08-05T18:52:04.274421: step 8746, loss 0.536107.
Train: 2018-08-05T18:52:04.493117: step 8747, loss 0.553623.
Train: 2018-08-05T18:52:04.696193: step 8748, loss 0.518534.
Train: 2018-08-05T18:52:04.914892: step 8749, loss 0.54459.
Train: 2018-08-05T18:52:05.117972: step 8750, loss 0.584543.
Test: 2018-08-05T18:52:06.180192: step 8750, loss 0.548172.
Train: 2018-08-05T18:52:06.398923: step 8751, loss 0.535407.
Train: 2018-08-05T18:52:06.601997: step 8752, loss 0.544712.
Train: 2018-08-05T18:52:06.820696: step 8753, loss 0.527206.
Train: 2018-08-05T18:52:07.039365: step 8754, loss 0.588875.
Train: 2018-08-05T18:52:07.258088: step 8755, loss 0.527153.
Train: 2018-08-05T18:52:07.461171: step 8756, loss 0.606574.
Train: 2018-08-05T18:52:07.664251: step 8757, loss 0.483098.
Train: 2018-08-05T18:52:07.851705: step 8758, loss 0.694402.
Train: 2018-08-05T18:52:08.054782: step 8759, loss 0.544829.
Train: 2018-08-05T18:52:08.273484: step 8760, loss 0.553631.
Test: 2018-08-05T18:52:09.335732: step 8760, loss 0.548018.
Train: 2018-08-05T18:52:09.538780: step 8761, loss 0.588479.
Train: 2018-08-05T18:52:09.757479: step 8762, loss 0.545026.
Train: 2018-08-05T18:52:09.960586: step 8763, loss 0.545091.
Train: 2018-08-05T18:52:10.163663: step 8764, loss 0.519339.
Train: 2018-08-05T18:52:10.382362: step 8765, loss 0.467651.
Train: 2018-08-05T18:52:10.601031: step 8766, loss 0.624427.
Train: 2018-08-05T18:52:10.804138: step 8767, loss 0.579716.
Train: 2018-08-05T18:52:11.022840: step 8768, loss 0.562343.
Train: 2018-08-05T18:52:11.225915: step 8769, loss 0.519102.
Train: 2018-08-05T18:52:11.444584: step 8770, loss 0.579674.
Test: 2018-08-05T18:52:12.491213: step 8770, loss 0.548068.
Train: 2018-08-05T18:52:12.709942: step 8771, loss 0.640365.
Train: 2018-08-05T18:52:12.928641: step 8772, loss 0.54508.
Train: 2018-08-05T18:52:13.131718: step 8773, loss 0.536531.
Train: 2018-08-05T18:52:13.350417: step 8774, loss 0.502208.
Train: 2018-08-05T18:52:13.553496: step 8775, loss 0.622608.
Train: 2018-08-05T18:52:13.772197: step 8776, loss 0.562335.
Train: 2018-08-05T18:52:13.975271: step 8777, loss 0.570917.
Train: 2018-08-05T18:52:14.193970: step 8778, loss 0.536642.
Train: 2018-08-05T18:52:14.412668: step 8779, loss 0.545212.
Train: 2018-08-05T18:52:14.631367: step 8780, loss 0.588041.
Test: 2018-08-05T18:52:15.677967: step 8780, loss 0.546857.
Train: 2018-08-05T18:52:15.896699: step 8781, loss 0.562336.
Train: 2018-08-05T18:52:16.099744: step 8782, loss 0.596548.
Train: 2018-08-05T18:52:16.318471: step 8783, loss 0.622034.
Train: 2018-08-05T18:52:16.537171: step 8784, loss 0.545406.
Train: 2018-08-05T18:52:16.755864: step 8785, loss 0.52863.
Train: 2018-08-05T18:52:16.958917: step 8786, loss 0.587635.
Train: 2018-08-05T18:52:17.177639: step 8787, loss 0.503633.
Train: 2018-08-05T18:52:17.380693: step 8788, loss 0.579193.
Train: 2018-08-05T18:52:17.599421: step 8789, loss 0.545569.
Train: 2018-08-05T18:52:17.818121: step 8790, loss 0.503439.
Test: 2018-08-05T18:52:18.864753: step 8790, loss 0.549194.
Train: 2018-08-05T18:52:19.083451: step 8791, loss 0.579286.
Train: 2018-08-05T18:52:19.286527: step 8792, loss 0.57934.
Train: 2018-08-05T18:52:19.505226: step 8793, loss 0.553824.
Train: 2018-08-05T18:52:19.723895: step 8794, loss 0.51963.
Train: 2018-08-05T18:52:19.942624: step 8795, loss 0.588092.
Train: 2018-08-05T18:52:20.145704: step 8796, loss 0.58817.
Train: 2018-08-05T18:52:20.364399: step 8797, loss 0.579576.
Train: 2018-08-05T18:52:20.583098: step 8798, loss 0.648501.
Train: 2018-08-05T18:52:20.801766: step 8799, loss 0.545202.
Train: 2018-08-05T18:52:21.004845: step 8800, loss 0.622055.
Test: 2018-08-05T18:52:22.067096: step 8800, loss 0.54919.
Train: 2018-08-05T18:52:23.051240: step 8801, loss 0.562356.
Train: 2018-08-05T18:52:23.269972: step 8802, loss 0.579202.
Train: 2018-08-05T18:52:23.473046: step 8803, loss 0.545711.
Train: 2018-08-05T18:52:23.691745: step 8804, loss 0.487633.
Train: 2018-08-05T18:52:23.894827: step 8805, loss 0.629.
Train: 2018-08-05T18:52:24.113522: step 8806, loss 0.570758.
Train: 2018-08-05T18:52:24.332220: step 8807, loss 0.488066.
Train: 2018-08-05T18:52:24.535298: step 8808, loss 0.545886.
Train: 2018-08-05T18:52:24.753968: step 8809, loss 0.554118.
Train: 2018-08-05T18:52:24.972696: step 8810, loss 0.629289.
Test: 2018-08-05T18:52:26.019330: step 8810, loss 0.547494.
Train: 2018-08-05T18:52:26.284885: step 8811, loss 0.579128.
Train: 2018-08-05T18:52:26.534832: step 8812, loss 0.587469.
Train: 2018-08-05T18:52:26.800389: step 8813, loss 0.504101.
Train: 2018-08-05T18:52:27.065952: step 8814, loss 0.604176.
Train: 2018-08-05T18:52:27.331515: step 8815, loss 0.670955.
Train: 2018-08-05T18:52:27.581456: step 8816, loss 0.570758.
Train: 2018-08-05T18:52:27.846994: step 8817, loss 0.529564.
Train: 2018-08-05T18:52:28.112582: step 8818, loss 0.595398.
Train: 2018-08-05T18:52:28.331278: step 8819, loss 0.529883.
Train: 2018-08-05T18:52:28.534362: step 8820, loss 0.578937.
Test: 2018-08-05T18:52:29.596583: step 8820, loss 0.548251.
Train: 2018-08-05T18:52:29.799690: step 8821, loss 0.603391.
Train: 2018-08-05T18:52:30.018392: step 8822, loss 0.570785.
Train: 2018-08-05T18:52:30.237058: step 8823, loss 0.570796.
Train: 2018-08-05T18:52:30.455786: step 8824, loss 0.530392.
Train: 2018-08-05T18:52:30.658864: step 8825, loss 0.514184.
Train: 2018-08-05T18:52:30.877562: step 8826, loss 0.595161.
Train: 2018-08-05T18:52:31.080640: step 8827, loss 0.587071.
Train: 2018-08-05T18:52:31.299339: step 8828, loss 0.578932.
Train: 2018-08-05T18:52:31.518008: step 8829, loss 0.48094.
Train: 2018-08-05T18:52:31.721115: step 8830, loss 0.620088.
Test: 2018-08-05T18:52:32.783337: step 8830, loss 0.548932.
Train: 2018-08-05T18:52:32.986444: step 8831, loss 0.562512.
Train: 2018-08-05T18:52:33.205142: step 8832, loss 0.620377.
Train: 2018-08-05T18:52:33.408220: step 8833, loss 0.479834.
Train: 2018-08-05T18:52:33.626919: step 8834, loss 0.529212.
Train: 2018-08-05T18:52:33.829996: step 8835, loss 0.579145.
Train: 2018-08-05T18:52:34.064317: step 8836, loss 0.596049.
Train: 2018-08-05T18:52:34.283015: step 8837, loss 0.629925.
Train: 2018-08-05T18:52:34.486092: step 8838, loss 0.562369.
Train: 2018-08-05T18:52:34.704791: step 8839, loss 0.537091.
Train: 2018-08-05T18:52:34.923491: step 8840, loss 0.587674.
Test: 2018-08-05T18:52:35.970091: step 8840, loss 0.549261.
Train: 2018-08-05T18:52:36.188788: step 8841, loss 0.478065.
Train: 2018-08-05T18:52:36.407517: step 8842, loss 0.528462.
Train: 2018-08-05T18:52:36.626216: step 8843, loss 0.562339.
Train: 2018-08-05T18:52:36.829295: step 8844, loss 0.562335.
Train: 2018-08-05T18:52:37.047992: step 8845, loss 0.55371.
Train: 2018-08-05T18:52:37.251070: step 8846, loss 0.588366.
Train: 2018-08-05T18:52:37.469769: step 8847, loss 0.597147.
Train: 2018-08-05T18:52:37.688467: step 8848, loss 0.562356.
Train: 2018-08-05T18:52:37.891549: step 8849, loss 0.571054.
Train: 2018-08-05T18:52:38.110214: step 8850, loss 0.623191.
Test: 2018-08-05T18:52:39.156874: step 8850, loss 0.548692.
Train: 2018-08-05T18:52:39.375573: step 8851, loss 0.64019.
Train: 2018-08-05T18:52:39.578651: step 8852, loss 0.553765.
Train: 2018-08-05T18:52:39.797349: step 8853, loss 0.57935.
Train: 2018-08-05T18:52:40.016018: step 8854, loss 0.570803.
Train: 2018-08-05T18:52:40.219114: step 8855, loss 0.503827.
Train: 2018-08-05T18:52:40.437794: step 8856, loss 0.612486.
Train: 2018-08-05T18:52:40.640904: step 8857, loss 0.520972.
Train: 2018-08-05T18:52:40.859600: step 8858, loss 0.504503.
Train: 2018-08-05T18:52:41.062647: step 8859, loss 0.595665.
Train: 2018-08-05T18:52:41.281347: step 8860, loss 0.496009.
Test: 2018-08-05T18:52:42.343627: step 8860, loss 0.547518.
Train: 2018-08-05T18:52:42.546675: step 8861, loss 0.570768.
Train: 2018-08-05T18:52:42.749753: step 8862, loss 0.545629.
Train: 2018-08-05T18:52:42.968481: step 8863, loss 0.50338.
Train: 2018-08-05T18:52:43.187150: step 8864, loss 0.536846.
Train: 2018-08-05T18:52:43.390260: step 8865, loss 0.570915.
Train: 2018-08-05T18:52:43.608956: step 8866, loss 0.596928.
Train: 2018-08-05T18:52:43.812034: step 8867, loss 0.51023.
Train: 2018-08-05T18:52:44.030733: step 8868, loss 0.588627.
Train: 2018-08-05T18:52:44.249402: step 8869, loss 0.553611.
Train: 2018-08-05T18:52:44.468130: step 8870, loss 0.535948.
Test: 2018-08-05T18:52:45.514730: step 8870, loss 0.546832.
Train: 2018-08-05T18:52:45.717806: step 8871, loss 0.624541.
Train: 2018-08-05T18:52:45.936506: step 8872, loss 0.695507.
Train: 2018-08-05T18:52:46.155205: step 8873, loss 0.527222.
Train: 2018-08-05T18:52:46.358315: step 8874, loss 0.536143.
Train: 2018-08-05T18:52:46.561390: step 8875, loss 0.55365.
Train: 2018-08-05T18:52:46.780088: step 8876, loss 0.4582.
Train: 2018-08-05T18:52:46.998788: step 8877, loss 0.605892.
Train: 2018-08-05T18:52:47.217486: step 8878, loss 0.631995.
Train: 2018-08-05T18:52:47.420534: step 8879, loss 0.614331.
Train: 2018-08-05T18:52:47.639233: step 8880, loss 0.493543.
Test: 2018-08-05T18:52:48.685863: step 8880, loss 0.545644.
Train: 2018-08-05T18:52:48.904561: step 8881, loss 0.536607.
Train: 2018-08-05T18:52:49.107668: step 8882, loss 0.545198.
Train: 2018-08-05T18:52:49.326338: step 8883, loss 0.596616.
Train: 2018-08-05T18:52:49.529444: step 8884, loss 0.613657.
Train: 2018-08-05T18:52:49.748144: step 8885, loss 0.562343.
Train: 2018-08-05T18:52:49.951191: step 8886, loss 0.511514.
Train: 2018-08-05T18:52:50.154298: step 8887, loss 0.477683.
Train: 2018-08-05T18:52:50.372998: step 8888, loss 0.528313.
Train: 2018-08-05T18:52:50.591690: step 8889, loss 0.553773.
Train: 2018-08-05T18:52:50.794744: step 8890, loss 0.605426.
Test: 2018-08-05T18:52:51.841403: step 8890, loss 0.54771.
Train: 2018-08-05T18:52:52.060102: step 8891, loss 0.666035.
Train: 2018-08-05T18:52:52.263183: step 8892, loss 0.48487.
Train: 2018-08-05T18:52:52.466257: step 8893, loss 0.57957.
Train: 2018-08-05T18:52:52.684956: step 8894, loss 0.527876.
Train: 2018-08-05T18:52:52.888004: step 8895, loss 0.536446.
Train: 2018-08-05T18:52:53.106733: step 8896, loss 0.536371.
Train: 2018-08-05T18:52:53.309809: step 8897, loss 0.55366.
Train: 2018-08-05T18:52:53.512890: step 8898, loss 0.54491.
Train: 2018-08-05T18:52:53.731586: step 8899, loss 0.60622.
Train: 2018-08-05T18:52:53.950284: step 8900, loss 0.606265.
Test: 2018-08-05T18:52:54.996912: step 8900, loss 0.548363.
Train: 2018-08-05T18:52:56.027923: step 8901, loss 0.492336.
Train: 2018-08-05T18:52:56.231000: step 8902, loss 0.588706.
Train: 2018-08-05T18:52:56.449699: step 8903, loss 0.571158.
Train: 2018-08-05T18:52:56.652777: step 8904, loss 0.571143.
Train: 2018-08-05T18:52:56.855854: step 8905, loss 0.544887.
Train: 2018-08-05T18:52:57.058902: step 8906, loss 0.553635.
Train: 2018-08-05T18:52:57.277631: step 8907, loss 0.483804.
Train: 2018-08-05T18:52:57.480679: step 8908, loss 0.509806.
Train: 2018-08-05T18:52:57.652543: step 8909, loss 0.411923.
Train: 2018-08-05T18:52:57.871241: step 8910, loss 0.59831.
Test: 2018-08-05T18:52:58.917841: step 8910, loss 0.546726.
Train: 2018-08-05T18:52:59.120948: step 8911, loss 0.571674.
Train: 2018-08-05T18:52:59.339647: step 8912, loss 0.653744.
Train: 2018-08-05T18:52:59.542725: step 8913, loss 0.544539.
Train: 2018-08-05T18:52:59.745802: step 8914, loss 0.617434.
Train: 2018-08-05T18:52:59.948880: step 8915, loss 0.644438.
Train: 2018-08-05T18:53:00.151958: step 8916, loss 0.517718.
Train: 2018-08-05T18:53:00.370656: step 8917, loss 0.544641.
Train: 2018-08-05T18:53:00.573733: step 8918, loss 0.491248.
Train: 2018-08-05T18:53:00.776811: step 8919, loss 0.553588.
Train: 2018-08-05T18:53:00.979859: step 8920, loss 0.571371.
Test: 2018-08-05T18:53:02.042140: step 8920, loss 0.54846.
Train: 2018-08-05T18:53:02.245217: step 8921, loss 0.571341.
Train: 2018-08-05T18:53:02.463917: step 8922, loss 0.597847.
Train: 2018-08-05T18:53:02.666993: step 8923, loss 0.536055.
Train: 2018-08-05T18:53:02.870071: step 8924, loss 0.632616.
Train: 2018-08-05T18:53:03.073148: step 8925, loss 0.605919.
Train: 2018-08-05T18:53:03.291816: step 8926, loss 0.579592.
Train: 2018-08-05T18:53:03.494895: step 8927, loss 0.570879.
Train: 2018-08-05T18:53:03.698002: step 8928, loss 0.570818.
Train: 2018-08-05T18:53:03.916671: step 8929, loss 0.512098.
Train: 2018-08-05T18:53:04.119778: step 8930, loss 0.504016.
Test: 2018-08-05T18:53:05.166408: step 8930, loss 0.548532.
Train: 2018-08-05T18:53:05.385076: step 8931, loss 0.629653.
Train: 2018-08-05T18:53:05.588184: step 8932, loss 0.562454.
Train: 2018-08-05T18:53:05.791232: step 8933, loss 0.56248.
Train: 2018-08-05T18:53:05.994309: step 8934, loss 0.554249.
Train: 2018-08-05T18:53:06.197416: step 8935, loss 0.504821.
Train: 2018-08-05T18:53:06.400494: step 8936, loss 0.521163.
Train: 2018-08-05T18:53:06.603572: step 8937, loss 0.645571.
Train: 2018-08-05T18:53:06.822270: step 8938, loss 0.479308.
Train: 2018-08-05T18:53:07.025318: step 8939, loss 0.554048.
Train: 2018-08-05T18:53:07.228425: step 8940, loss 0.511907.
Test: 2018-08-05T18:53:08.275025: step 8940, loss 0.547592.
Train: 2018-08-05T18:53:08.478132: step 8941, loss 0.477495.
Train: 2018-08-05T18:53:08.681210: step 8942, loss 0.527946.
Train: 2018-08-05T18:53:08.884290: step 8943, loss 0.536214.
Train: 2018-08-05T18:53:09.087336: step 8944, loss 0.650777.
Train: 2018-08-05T18:53:09.290442: step 8945, loss 0.615829.
Train: 2018-08-05T18:53:09.493520: step 8946, loss 0.651596.
Train: 2018-08-05T18:53:09.696566: step 8947, loss 0.544715.
Train: 2018-08-05T18:53:09.899675: step 8948, loss 0.500507.
Train: 2018-08-05T18:53:10.102752: step 8949, loss 0.53589.
Train: 2018-08-05T18:53:10.305800: step 8950, loss 0.544727.
Test: 2018-08-05T18:53:11.368061: step 8950, loss 0.547434.
Train: 2018-08-05T18:53:11.555507: step 8951, loss 0.509185.
Train: 2018-08-05T18:53:11.758617: step 8952, loss 0.589262.
Train: 2018-08-05T18:53:11.961662: step 8953, loss 0.580388.
Train: 2018-08-05T18:53:12.164740: step 8954, loss 0.598246.
Train: 2018-08-05T18:53:12.367847: step 8955, loss 0.553588.
Train: 2018-08-05T18:53:12.555273: step 8956, loss 0.500295.
Train: 2018-08-05T18:53:12.758383: step 8957, loss 0.482479.
Train: 2018-08-05T18:53:12.961458: step 8958, loss 0.544657.
Train: 2018-08-05T18:53:13.164505: step 8959, loss 0.508725.
Train: 2018-08-05T18:53:13.367613: step 8960, loss 0.535543.
Test: 2018-08-05T18:53:14.414213: step 8960, loss 0.547125.
Train: 2018-08-05T18:53:14.617290: step 8961, loss 0.571829.
Train: 2018-08-05T18:53:14.820397: step 8962, loss 0.517112.
Train: 2018-08-05T18:53:15.023445: step 8963, loss 0.498553.
Train: 2018-08-05T18:53:15.226552: step 8964, loss 0.544509.
Train: 2018-08-05T18:53:15.429599: step 8965, loss 0.516529.
Train: 2018-08-05T18:53:15.632678: step 8966, loss 0.535142.
Train: 2018-08-05T18:53:15.835785: step 8967, loss 0.620327.
Train: 2018-08-05T18:53:16.023241: step 8968, loss 0.544591.
Train: 2018-08-05T18:53:16.226318: step 8969, loss 0.573088.
Train: 2018-08-05T18:53:16.429366: step 8970, loss 0.611002.
Test: 2018-08-05T18:53:17.475996: step 8970, loss 0.546156.
Train: 2018-08-05T18:53:17.679103: step 8971, loss 0.563443.
Train: 2018-08-05T18:53:17.882184: step 8972, loss 0.535151.
Train: 2018-08-05T18:53:18.085228: step 8973, loss 0.619218.
Train: 2018-08-05T18:53:18.272714: step 8974, loss 0.563016.
Train: 2018-08-05T18:53:18.475791: step 8975, loss 0.562855.
Train: 2018-08-05T18:53:18.678869: step 8976, loss 0.480964.
Train: 2018-08-05T18:53:18.881949: step 8977, loss 0.544572.
Train: 2018-08-05T18:53:19.085024: step 8978, loss 0.544595.
Train: 2018-08-05T18:53:19.288101: step 8979, loss 0.517682.
Train: 2018-08-05T18:53:19.491179: step 8980, loss 0.544624.
Test: 2018-08-05T18:53:20.537808: step 8980, loss 0.547791.
Train: 2018-08-05T18:53:20.740886: step 8981, loss 0.652209.
Train: 2018-08-05T18:53:20.943934: step 8982, loss 0.607038.
Train: 2018-08-05T18:53:21.131422: step 8983, loss 0.641888.
Train: 2018-08-05T18:53:21.334466: step 8984, loss 0.571074.
Train: 2018-08-05T18:53:21.537545: step 8985, loss 0.519326.
Train: 2018-08-05T18:53:21.725030: step 8986, loss 0.570865.
Train: 2018-08-05T18:53:21.928077: step 8987, loss 0.562362.
Train: 2018-08-05T18:53:22.115564: step 8988, loss 0.579167.
Train: 2018-08-05T18:53:22.318641: step 8989, loss 0.479232.
Train: 2018-08-05T18:53:22.521718: step 8990, loss 0.554142.
Test: 2018-08-05T18:53:23.583970: step 8990, loss 0.548009.
Train: 2018-08-05T18:53:23.771426: step 8991, loss 0.603982.
Train: 2018-08-05T18:53:23.974504: step 8992, loss 0.545904.
Train: 2018-08-05T18:53:24.177581: step 8993, loss 0.628697.
Train: 2018-08-05T18:53:24.380662: step 8994, loss 0.578997.
Train: 2018-08-05T18:53:24.568085: step 8995, loss 0.56256.
Train: 2018-08-05T18:53:24.786813: step 8996, loss 0.578941.
Train: 2018-08-05T18:53:24.974273: step 8997, loss 0.538213.
Train: 2018-08-05T18:53:25.177347: step 8998, loss 0.530116.
Train: 2018-08-05T18:53:25.380428: step 8999, loss 0.63639.
Train: 2018-08-05T18:53:25.583473: step 9000, loss 0.505824.
Test: 2018-08-05T18:53:26.630102: step 9000, loss 0.549973.
Train: 2018-08-05T18:53:27.567413: step 9001, loss 0.505635.
Train: 2018-08-05T18:53:27.754871: step 9002, loss 0.521183.
Train: 2018-08-05T18:53:27.957946: step 9003, loss 0.527066.
Train: 2018-08-05T18:53:28.145401: step 9004, loss 0.525702.
Train: 2018-08-05T18:53:28.348450: step 9005, loss 0.591374.
Train: 2018-08-05T18:53:28.551527: step 9006, loss 0.5831.
Train: 2018-08-05T18:53:28.739013: step 9007, loss 0.536129.
Train: 2018-08-05T18:53:28.942090: step 9008, loss 0.579229.
Train: 2018-08-05T18:53:29.145168: step 9009, loss 0.587401.
Train: 2018-08-05T18:53:29.348239: step 9010, loss 0.578897.
Test: 2018-08-05T18:53:30.394846: step 9010, loss 0.55047.
Train: 2018-08-05T18:53:30.597953: step 9011, loss 0.516544.
Train: 2018-08-05T18:53:30.785379: step 9012, loss 0.496281.
Train: 2018-08-05T18:53:30.988456: step 9013, loss 0.648496.
Train: 2018-08-05T18:53:31.191534: step 9014, loss 0.486266.
Train: 2018-08-05T18:53:31.394642: step 9015, loss 0.647197.
Train: 2018-08-05T18:53:31.597718: step 9016, loss 0.562352.
Train: 2018-08-05T18:53:31.800796: step 9017, loss 0.570828.
Train: 2018-08-05T18:53:32.003877: step 9018, loss 0.570824.
Train: 2018-08-05T18:53:32.206951: step 9019, loss 0.536972.
Train: 2018-08-05T18:53:32.394408: step 9020, loss 0.587761.
Test: 2018-08-05T18:53:33.456628: step 9020, loss 0.549003.
Train: 2018-08-05T18:53:33.644114: step 9021, loss 0.494654.
Train: 2018-08-05T18:53:33.847192: step 9022, loss 0.596323.
Train: 2018-08-05T18:53:34.034618: step 9023, loss 0.638887.
Train: 2018-08-05T18:53:34.237725: step 9024, loss 0.511501.
Train: 2018-08-05T18:53:34.440773: step 9025, loss 0.579305.
Train: 2018-08-05T18:53:34.675123: step 9026, loss 0.570824.
Train: 2018-08-05T18:53:34.878204: step 9027, loss 0.579277.
Train: 2018-08-05T18:53:35.065627: step 9028, loss 0.596136.
Train: 2018-08-05T18:53:35.268733: step 9029, loss 0.604438.
Train: 2018-08-05T18:53:35.471811: step 9030, loss 0.528955.
Test: 2018-08-05T18:53:36.518441: step 9030, loss 0.547537.
Train: 2018-08-05T18:53:36.721519: step 9031, loss 0.579106.
Train: 2018-08-05T18:53:36.924567: step 9032, loss 0.570761.
Train: 2018-08-05T18:53:37.127677: step 9033, loss 0.587333.
Train: 2018-08-05T18:53:37.330751: step 9034, loss 0.545992.
Train: 2018-08-05T18:53:37.518207: step 9035, loss 0.578996.
Train: 2018-08-05T18:53:37.721255: step 9036, loss 0.505007.
Train: 2018-08-05T18:53:37.924362: step 9037, loss 0.521343.
Train: 2018-08-05T18:53:38.127456: step 9038, loss 0.512736.
Train: 2018-08-05T18:53:38.314895: step 9039, loss 0.502379.
Train: 2018-08-05T18:53:38.517973: step 9040, loss 0.516079.
Test: 2018-08-05T18:53:39.564604: step 9040, loss 0.558466.
Train: 2018-08-05T18:53:39.767680: step 9041, loss 0.589983.
Train: 2018-08-05T18:53:39.955107: step 9042, loss 0.536046.
Train: 2018-08-05T18:53:40.158214: step 9043, loss 0.573188.
Train: 2018-08-05T18:53:40.345670: step 9044, loss 0.597868.
Train: 2018-08-05T18:53:40.548747: step 9045, loss 0.595974.
Train: 2018-08-05T18:53:40.751819: step 9046, loss 0.547219.
Train: 2018-08-05T18:53:40.939251: step 9047, loss 0.611258.
Train: 2018-08-05T18:53:41.142359: step 9048, loss 0.56319.
Train: 2018-08-05T18:53:41.345436: step 9049, loss 0.602383.
Train: 2018-08-05T18:53:41.532892: step 9050, loss 0.563873.
Test: 2018-08-05T18:53:42.595114: step 9050, loss 0.550727.
Train: 2018-08-05T18:53:42.782570: step 9051, loss 0.663583.
Train: 2018-08-05T18:53:42.985676: step 9052, loss 0.580072.
Train: 2018-08-05T18:53:43.173133: step 9053, loss 0.574466.
Train: 2018-08-05T18:53:43.376181: step 9054, loss 0.517127.
Train: 2018-08-05T18:53:43.579291: step 9055, loss 0.586664.
Train: 2018-08-05T18:53:43.766744: step 9056, loss 0.570789.
Train: 2018-08-05T18:53:43.954200: step 9057, loss 0.554163.
Train: 2018-08-05T18:53:44.157277: step 9058, loss 0.645879.
Train: 2018-08-05T18:53:44.360325: step 9059, loss 0.562435.
Train: 2018-08-05T18:53:44.516538: step 9060, loss 0.562443.
Test: 2018-08-05T18:53:45.578790: step 9060, loss 0.54914.
Train: 2018-08-05T18:53:45.781897: step 9061, loss 0.604009.
Train: 2018-08-05T18:53:45.969353: step 9062, loss 0.521028.
Train: 2018-08-05T18:53:46.172430: step 9063, loss 0.537592.
Train: 2018-08-05T18:53:46.375511: step 9064, loss 0.612314.
Train: 2018-08-05T18:53:46.562964: step 9065, loss 0.53754.
Train: 2018-08-05T18:53:46.766042: step 9066, loss 0.595726.
Train: 2018-08-05T18:53:46.969118: step 9067, loss 0.595697.
Train: 2018-08-05T18:53:47.156575: step 9068, loss 0.504406.
Train: 2018-08-05T18:53:47.359647: step 9069, loss 0.562449.
Train: 2018-08-05T18:53:47.562730: step 9070, loss 0.545774.
Test: 2018-08-05T18:53:48.609330: step 9070, loss 0.547684.
Train: 2018-08-05T18:53:48.812437: step 9071, loss 0.662712.
Train: 2018-08-05T18:53:49.015514: step 9072, loss 0.529094.
Train: 2018-08-05T18:53:49.218614: step 9073, loss 0.470775.
Train: 2018-08-05T18:53:49.421670: step 9074, loss 0.587542.
Train: 2018-08-05T18:53:49.609096: step 9075, loss 0.553964.
Train: 2018-08-05T18:53:49.827824: step 9076, loss 0.537009.
Train: 2018-08-05T18:53:50.015280: step 9077, loss 0.664335.
Train: 2018-08-05T18:53:50.218328: step 9078, loss 0.468982.
Train: 2018-08-05T18:53:50.421406: step 9079, loss 0.545283.
Train: 2018-08-05T18:53:50.624483: step 9080, loss 0.562335.
Test: 2018-08-05T18:53:51.671112: step 9080, loss 0.549923.
Train: 2018-08-05T18:53:51.905436: step 9081, loss 0.596824.
Train: 2018-08-05T18:53:52.155408: step 9082, loss 0.562338.
Train: 2018-08-05T18:53:52.358477: step 9083, loss 0.484532.
Train: 2018-08-05T18:53:52.545938: step 9084, loss 0.597145.
Train: 2018-08-05T18:53:52.748986: step 9085, loss 0.536193.
Train: 2018-08-05T18:53:52.936442: step 9086, loss 0.527345.
Train: 2018-08-05T18:53:53.139549: step 9087, loss 0.509575.
Train: 2018-08-05T18:53:53.327005: step 9088, loss 0.544718.
Train: 2018-08-05T18:53:53.530077: step 9089, loss 0.589335.
Train: 2018-08-05T18:53:53.733160: step 9090, loss 0.499744.
Test: 2018-08-05T18:53:54.795381: step 9090, loss 0.548388.
Train: 2018-08-05T18:53:54.982868: step 9091, loss 0.598792.
Train: 2018-08-05T18:53:55.185915: step 9092, loss 0.653346.
Train: 2018-08-05T18:53:55.388993: step 9093, loss 0.598801.
Train: 2018-08-05T18:53:55.592099: step 9094, loss 0.490688.
Train: 2018-08-05T18:53:55.779555: step 9095, loss 0.652289.
Train: 2018-08-05T18:53:55.982636: step 9096, loss 0.633749.
Train: 2018-08-05T18:53:56.170090: step 9097, loss 0.588831.
Train: 2018-08-05T18:53:56.373170: step 9098, loss 0.553656.
Train: 2018-08-05T18:53:56.576244: step 9099, loss 0.55373.
Train: 2018-08-05T18:53:56.779321: step 9100, loss 0.570866.
Test: 2018-08-05T18:53:57.825922: step 9100, loss 0.548446.
Train: 2018-08-05T18:53:58.763235: step 9101, loss 0.520115.
Train: 2018-08-05T18:53:58.966309: step 9102, loss 0.528767.
Train: 2018-08-05T18:53:59.169387: step 9103, loss 0.587545.
Train: 2018-08-05T18:53:59.372468: step 9104, loss 0.545711.
Train: 2018-08-05T18:53:59.575542: step 9105, loss 0.629123.
Train: 2018-08-05T18:53:59.778590: step 9106, loss 0.521012.
Train: 2018-08-05T18:53:59.981666: step 9107, loss 0.628677.
Train: 2018-08-05T18:54:00.184745: step 9108, loss 0.62836.
Train: 2018-08-05T18:54:00.372233: step 9109, loss 0.513664.
Train: 2018-08-05T18:54:00.575308: step 9110, loss 0.522033.
Test: 2018-08-05T18:54:01.621908: step 9110, loss 0.549848.
Train: 2018-08-05T18:54:01.887504: step 9111, loss 0.619519.
Train: 2018-08-05T18:54:02.090578: step 9112, loss 0.562704.
Train: 2018-08-05T18:54:02.293626: step 9113, loss 0.538501.
Train: 2018-08-05T18:54:02.496736: step 9114, loss 0.522333.
Train: 2018-08-05T18:54:02.699781: step 9115, loss 0.562685.
Train: 2018-08-05T18:54:02.902888: step 9116, loss 0.473105.
Train: 2018-08-05T18:54:03.105965: step 9117, loss 0.587198.
Train: 2018-08-05T18:54:03.309013: step 9118, loss 0.55419.
Train: 2018-08-05T18:54:03.512090: step 9119, loss 0.520675.
Train: 2018-08-05T18:54:03.715197: step 9120, loss 0.545506.
Test: 2018-08-05T18:54:04.777449: step 9120, loss 0.550273.
Train: 2018-08-05T18:54:04.980497: step 9121, loss 0.613448.
Train: 2018-08-05T18:54:05.183603: step 9122, loss 0.60518.
Train: 2018-08-05T18:54:05.371063: step 9123, loss 0.54515.
Train: 2018-08-05T18:54:05.589759: step 9124, loss 0.639937.
Train: 2018-08-05T18:54:05.792807: step 9125, loss 0.527905.
Train: 2018-08-05T18:54:05.980292: step 9126, loss 0.562336.
Train: 2018-08-05T18:54:06.183369: step 9127, loss 0.631249.
Train: 2018-08-05T18:54:06.386446: step 9128, loss 0.536596.
Train: 2018-08-05T18:54:06.589495: step 9129, loss 0.562336.
Train: 2018-08-05T18:54:06.776950: step 9130, loss 0.570882.
Test: 2018-08-05T18:54:07.839233: step 9130, loss 0.549082.
Train: 2018-08-05T18:54:08.042309: step 9131, loss 0.494133.
Train: 2018-08-05T18:54:08.229765: step 9132, loss 0.605057.
Train: 2018-08-05T18:54:08.448464: step 9133, loss 0.587951.
Train: 2018-08-05T18:54:08.651541: step 9134, loss 0.485697.
Train: 2018-08-05T18:54:08.854619: step 9135, loss 0.502536.
Train: 2018-08-05T18:54:09.057696: step 9136, loss 0.459197.
Train: 2018-08-05T18:54:09.260743: step 9137, loss 0.501495.
Train: 2018-08-05T18:54:09.463851: step 9138, loss 0.54479.
Train: 2018-08-05T18:54:09.651308: step 9139, loss 0.571442.
Train: 2018-08-05T18:54:09.854385: step 9140, loss 0.580663.
Test: 2018-08-05T18:54:10.916606: step 9140, loss 0.547964.
Train: 2018-08-05T18:54:11.104092: step 9141, loss 0.526376.
Train: 2018-08-05T18:54:11.307169: step 9142, loss 0.645271.
Train: 2018-08-05T18:54:11.510247: step 9143, loss 0.489479.
Train: 2018-08-05T18:54:11.713324: step 9144, loss 0.498439.
Train: 2018-08-05T18:54:11.916402: step 9145, loss 0.609435.
Train: 2018-08-05T18:54:12.135095: step 9146, loss 0.460852.
Train: 2018-08-05T18:54:12.322557: step 9147, loss 0.581948.
Train: 2018-08-05T18:54:12.525635: step 9148, loss 0.600869.
Train: 2018-08-05T18:54:12.728712: step 9149, loss 0.619631.
Train: 2018-08-05T18:54:12.931789: step 9150, loss 0.507146.
Test: 2018-08-05T18:54:13.994044: step 9150, loss 0.548666.
Train: 2018-08-05T18:54:14.197118: step 9151, loss 0.572486.
Train: 2018-08-05T18:54:14.400195: step 9152, loss 0.553799.
Train: 2018-08-05T18:54:14.603273: step 9153, loss 0.526008.
Train: 2018-08-05T18:54:14.806350: step 9154, loss 0.544509.
Train: 2018-08-05T18:54:15.009428: step 9155, loss 0.590528.
Train: 2018-08-05T18:54:15.212475: step 9156, loss 0.590317.
Train: 2018-08-05T18:54:15.415582: step 9157, loss 0.562737.
Train: 2018-08-05T18:54:15.618663: step 9158, loss 0.544577.
Train: 2018-08-05T18:54:15.821738: step 9159, loss 0.607467.
Train: 2018-08-05T18:54:16.024815: step 9160, loss 0.535787.
Test: 2018-08-05T18:54:17.071415: step 9160, loss 0.548284.
Train: 2018-08-05T18:54:17.274522: step 9161, loss 0.491723.
Train: 2018-08-05T18:54:17.493224: step 9162, loss 0.588866.
Train: 2018-08-05T18:54:17.696269: step 9163, loss 0.536059.
Train: 2018-08-05T18:54:17.899346: step 9164, loss 0.509855.
Train: 2018-08-05T18:54:18.102453: step 9165, loss 0.606165.
Train: 2018-08-05T18:54:18.321152: step 9166, loss 0.606038.
Train: 2018-08-05T18:54:18.524229: step 9167, loss 0.544976.
Train: 2018-08-05T18:54:18.727307: step 9168, loss 0.570996.
Train: 2018-08-05T18:54:18.930385: step 9169, loss 0.545103.
Train: 2018-08-05T18:54:19.133462: step 9170, loss 0.605293.
Test: 2018-08-05T18:54:20.195684: step 9170, loss 0.548851.
Train: 2018-08-05T18:54:20.398790: step 9171, loss 0.587976.
Train: 2018-08-05T18:54:20.601868: step 9172, loss 0.519886.
Train: 2018-08-05T18:54:20.804945: step 9173, loss 0.55389.
Train: 2018-08-05T18:54:20.992405: step 9174, loss 0.562363.
Train: 2018-08-05T18:54:21.211101: step 9175, loss 0.537069.
Train: 2018-08-05T18:54:21.414149: step 9176, loss 0.537066.
Train: 2018-08-05T18:54:21.617258: step 9177, loss 0.570812.
Train: 2018-08-05T18:54:21.820303: step 9178, loss 0.579279.
Train: 2018-08-05T18:54:22.023410: step 9179, loss 0.536969.
Train: 2018-08-05T18:54:22.226487: step 9180, loss 0.630181.
Test: 2018-08-05T18:54:23.288739: step 9180, loss 0.547258.
Train: 2018-08-05T18:54:23.491819: step 9181, loss 0.587736.
Train: 2018-08-05T18:54:23.710509: step 9182, loss 0.528654.
Train: 2018-08-05T18:54:23.913593: step 9183, loss 0.604483.
Train: 2018-08-05T18:54:24.116673: step 9184, loss 0.570784.
Train: 2018-08-05T18:54:24.319747: step 9185, loss 0.528946.
Train: 2018-08-05T18:54:24.522825: step 9186, loss 0.554046.
Train: 2018-08-05T18:54:24.725872: step 9187, loss 0.587506.
Train: 2018-08-05T18:54:24.928950: step 9188, loss 0.545697.
Train: 2018-08-05T18:54:25.147679: step 9189, loss 0.595857.
Train: 2018-08-05T18:54:25.350756: step 9190, loss 0.562419.
Test: 2018-08-05T18:54:26.397356: step 9190, loss 0.548494.
Train: 2018-08-05T18:54:26.616088: step 9191, loss 0.537401.
Train: 2018-08-05T18:54:26.819162: step 9192, loss 0.61253.
Train: 2018-08-05T18:54:27.037831: step 9193, loss 0.520753.
Train: 2018-08-05T18:54:27.240941: step 9194, loss 0.587459.
Train: 2018-08-05T18:54:27.443987: step 9195, loss 0.537389.
Train: 2018-08-05T18:54:27.662715: step 9196, loss 0.646003.
Train: 2018-08-05T18:54:27.865795: step 9197, loss 0.512444.
Train: 2018-08-05T18:54:28.068869: step 9198, loss 0.570765.
Train: 2018-08-05T18:54:28.287571: step 9199, loss 0.620789.
Train: 2018-08-05T18:54:28.490649: step 9200, loss 0.529203.
Test: 2018-08-05T18:54:29.552868: step 9200, loss 0.54781.
Train: 2018-08-05T18:54:30.474556: step 9201, loss 0.587377.
Train: 2018-08-05T18:54:30.693225: step 9202, loss 0.537573.
Train: 2018-08-05T18:54:30.896332: step 9203, loss 0.537554.
Train: 2018-08-05T18:54:31.099410: step 9204, loss 0.595727.
Train: 2018-08-05T18:54:31.318078: step 9205, loss 0.512481.
Train: 2018-08-05T18:54:31.521156: step 9206, loss 0.57913.
Train: 2018-08-05T18:54:31.724233: step 9207, loss 0.604306.
Train: 2018-08-05T18:54:31.942933: step 9208, loss 0.579163.
Train: 2018-08-05T18:54:32.146039: step 9209, loss 0.604288.
Train: 2018-08-05T18:54:32.349121: step 9210, loss 0.587468.
Test: 2018-08-05T18:54:33.411368: step 9210, loss 0.548459.
Train: 2018-08-05T18:54:33.583203: step 9211, loss 0.527151.
Train: 2018-08-05T18:54:33.801901: step 9212, loss 0.562447.
Train: 2018-08-05T18:54:34.004980: step 9213, loss 0.537517.
Train: 2018-08-05T18:54:34.208057: step 9214, loss 0.537466.
Train: 2018-08-05T18:54:34.426756: step 9215, loss 0.545701.
Train: 2018-08-05T18:54:34.629833: step 9216, loss 0.663484.
Train: 2018-08-05T18:54:34.832880: step 9217, loss 0.537299.
Train: 2018-08-05T18:54:35.035988: step 9218, loss 0.570774.
Train: 2018-08-05T18:54:35.254656: step 9219, loss 0.579138.
Train: 2018-08-05T18:54:35.457764: step 9220, loss 0.554059.
Test: 2018-08-05T18:54:36.520015: step 9220, loss 0.548271.
Train: 2018-08-05T18:54:36.723065: step 9221, loss 0.562416.
Train: 2018-08-05T18:54:36.941793: step 9222, loss 0.528992.
Train: 2018-08-05T18:54:37.144838: step 9223, loss 0.621039.
Train: 2018-08-05T18:54:37.347946: step 9224, loss 0.554039.
Train: 2018-08-05T18:54:37.551025: step 9225, loss 0.604237.
Train: 2018-08-05T18:54:37.769722: step 9226, loss 0.504007.
Train: 2018-08-05T18:54:37.988425: step 9227, loss 0.579131.
Train: 2018-08-05T18:54:38.191499: step 9228, loss 0.512212.
Train: 2018-08-05T18:54:38.410198: step 9229, loss 0.612793.
Train: 2018-08-05T18:54:38.613278: step 9230, loss 0.629638.
Test: 2018-08-05T18:54:39.675526: step 9230, loss 0.548211.
Train: 2018-08-05T18:54:39.878604: step 9231, loss 0.562399.
Train: 2018-08-05T18:54:40.081652: step 9232, loss 0.545698.
Train: 2018-08-05T18:54:40.300380: step 9233, loss 0.512325.
Train: 2018-08-05T18:54:40.503457: step 9234, loss 0.637733.
Train: 2018-08-05T18:54:40.722156: step 9235, loss 0.620879.
Train: 2018-08-05T18:54:40.940858: step 9236, loss 0.529221.
Train: 2018-08-05T18:54:41.143933: step 9237, loss 0.595625.
Train: 2018-08-05T18:54:41.347010: step 9238, loss 0.537736.
Train: 2018-08-05T18:54:41.565708: step 9239, loss 0.546018.
Train: 2018-08-05T18:54:41.784408: step 9240, loss 0.513017.
Test: 2018-08-05T18:54:42.831008: step 9240, loss 0.548839.
Train: 2018-08-05T18:54:43.049736: step 9241, loss 0.545916.
Train: 2018-08-05T18:54:43.252814: step 9242, loss 0.604045.
Train: 2018-08-05T18:54:43.471514: step 9243, loss 0.562427.
Train: 2018-08-05T18:54:43.674590: step 9244, loss 0.587487.
Train: 2018-08-05T18:54:43.893289: step 9245, loss 0.662782.
Train: 2018-08-05T18:54:44.096368: step 9246, loss 0.55412.
Train: 2018-08-05T18:54:44.315066: step 9247, loss 0.587335.
Train: 2018-08-05T18:54:44.518142: step 9248, loss 0.620259.
Train: 2018-08-05T18:54:44.736845: step 9249, loss 0.456103.
Train: 2018-08-05T18:54:44.939918: step 9250, loss 0.570763.
Test: 2018-08-05T18:54:45.986518: step 9250, loss 0.549053.
Train: 2018-08-05T18:54:46.205247: step 9251, loss 0.587164.
Train: 2018-08-05T18:54:46.408325: step 9252, loss 0.562565.
Train: 2018-08-05T18:54:46.627023: step 9253, loss 0.554364.
Train: 2018-08-05T18:54:46.830101: step 9254, loss 0.513293.
Train: 2018-08-05T18:54:47.048800: step 9255, loss 0.570756.
Train: 2018-08-05T18:54:47.251877: step 9256, loss 0.570759.
Train: 2018-08-05T18:54:47.470546: step 9257, loss 0.579114.
Train: 2018-08-05T18:54:47.673623: step 9258, loss 0.579116.
Train: 2018-08-05T18:54:47.892352: step 9259, loss 0.487292.
Train: 2018-08-05T18:54:48.111046: step 9260, loss 0.537183.
Test: 2018-08-05T18:54:49.157681: step 9260, loss 0.547246.
Train: 2018-08-05T18:54:49.360761: step 9261, loss 0.579286.
Train: 2018-08-05T18:54:49.579457: step 9262, loss 0.536801.
Train: 2018-08-05T18:54:49.782534: step 9263, loss 0.570907.
Train: 2018-08-05T18:54:50.001233: step 9264, loss 0.553717.
Train: 2018-08-05T18:54:50.219933: step 9265, loss 0.553679.
Train: 2018-08-05T18:54:50.423010: step 9266, loss 0.614625.
Train: 2018-08-05T18:54:50.641708: step 9267, loss 0.562364.
Train: 2018-08-05T18:54:50.844756: step 9268, loss 0.562368.
Train: 2018-08-05T18:54:51.063454: step 9269, loss 0.588567.
Train: 2018-08-05T18:54:51.282183: step 9270, loss 0.510058.
Test: 2018-08-05T18:54:52.328813: step 9270, loss 0.545975.
Train: 2018-08-05T18:54:52.563134: step 9271, loss 0.571103.
Train: 2018-08-05T18:54:52.766211: step 9272, loss 0.509944.
Train: 2018-08-05T18:54:52.984913: step 9273, loss 0.588687.
Train: 2018-08-05T18:54:53.187982: step 9274, loss 0.457079.
Train: 2018-08-05T18:54:53.402577: step 9275, loss 0.597776.
Train: 2018-08-05T18:54:53.621275: step 9276, loss 0.571326.
Train: 2018-08-05T18:54:53.824353: step 9277, loss 0.500271.
Train: 2018-08-05T18:54:54.043021: step 9278, loss 0.517864.
Train: 2018-08-05T18:54:54.246100: step 9279, loss 0.553597.
Train: 2018-08-05T18:54:54.464797: step 9280, loss 0.562653.
Test: 2018-08-05T18:54:55.511458: step 9280, loss 0.547337.
Train: 2018-08-05T18:54:55.730125: step 9281, loss 0.55363.
Train: 2018-08-05T18:54:55.933233: step 9282, loss 0.580968.
Train: 2018-08-05T18:54:56.151902: step 9283, loss 0.580693.
Train: 2018-08-05T18:54:56.355004: step 9284, loss 0.570848.
Train: 2018-08-05T18:54:56.573708: step 9285, loss 0.514208.
Train: 2018-08-05T18:54:56.776756: step 9286, loss 0.546077.
Train: 2018-08-05T18:54:56.995484: step 9287, loss 0.562511.
Train: 2018-08-05T18:54:57.214183: step 9288, loss 0.535309.
Train: 2018-08-05T18:54:57.417261: step 9289, loss 0.516703.
Train: 2018-08-05T18:54:57.635959: step 9290, loss 0.572468.
Test: 2018-08-05T18:54:58.682560: step 9290, loss 0.548466.
Train: 2018-08-05T18:54:58.901289: step 9291, loss 0.628629.
Train: 2018-08-05T18:54:59.104367: step 9292, loss 0.57249.
Train: 2018-08-05T18:54:59.323064: step 9293, loss 0.590957.
Train: 2018-08-05T18:54:59.526143: step 9294, loss 0.470654.
Train: 2018-08-05T18:54:59.744840: step 9295, loss 0.590606.
Train: 2018-08-05T18:54:59.947919: step 9296, loss 0.443497.
Train: 2018-08-05T18:55:00.150997: step 9297, loss 0.544512.
Train: 2018-08-05T18:55:00.369695: step 9298, loss 0.590623.
Train: 2018-08-05T18:55:00.588393: step 9299, loss 0.535295.
Train: 2018-08-05T18:55:00.791470: step 9300, loss 0.498445.
Test: 2018-08-05T18:55:01.853691: step 9300, loss 0.548831.
Train: 2018-08-05T18:55:02.806624: step 9301, loss 0.535274.
Train: 2018-08-05T18:55:03.025324: step 9302, loss 0.52599.
Train: 2018-08-05T18:55:03.228399: step 9303, loss 0.637423.
Train: 2018-08-05T18:55:03.447099: step 9304, loss 0.59085.
Train: 2018-08-05T18:55:03.650146: step 9305, loss 0.57218.
Train: 2018-08-05T18:55:03.884495: step 9306, loss 0.52618.
Train: 2018-08-05T18:55:04.103195: step 9307, loss 0.571919.
Train: 2018-08-05T18:55:04.306276: step 9308, loss 0.580875.
Train: 2018-08-05T18:55:04.524973: step 9309, loss 0.535564.
Train: 2018-08-05T18:55:04.728049: step 9310, loss 0.598463.
Test: 2018-08-05T18:55:05.790270: step 9310, loss 0.5468.
Train: 2018-08-05T18:55:05.993379: step 9311, loss 0.580308.
Train: 2018-08-05T18:55:06.212076: step 9312, loss 0.527107.
Train: 2018-08-05T18:55:06.415154: step 9313, loss 0.553615.
Train: 2018-08-05T18:55:06.633852: step 9314, loss 0.466296.
Train: 2018-08-05T18:55:06.852522: step 9315, loss 0.597333.
Train: 2018-08-05T18:55:07.055630: step 9316, loss 0.614714.
Train: 2018-08-05T18:55:07.274298: step 9317, loss 0.588391.
Train: 2018-08-05T18:55:07.477405: step 9318, loss 0.605408.
Train: 2018-08-05T18:55:07.696104: step 9319, loss 0.529197.
Train: 2018-08-05T18:55:07.914802: step 9320, loss 0.477594.
Test: 2018-08-05T18:55:08.961433: step 9320, loss 0.548709.
Train: 2018-08-05T18:55:09.180102: step 9321, loss 0.579372.
Train: 2018-08-05T18:55:09.383179: step 9322, loss 0.553831.
Train: 2018-08-05T18:55:09.586285: step 9323, loss 0.562342.
Train: 2018-08-05T18:55:09.804985: step 9324, loss 0.536798.
Train: 2018-08-05T18:55:10.008061: step 9325, loss 0.553809.
Train: 2018-08-05T18:55:10.226730: step 9326, loss 0.553788.
Train: 2018-08-05T18:55:10.429838: step 9327, loss 0.553764.
Train: 2018-08-05T18:55:10.648538: step 9328, loss 0.467722.
Train: 2018-08-05T18:55:10.867206: step 9329, loss 0.642767.
Train: 2018-08-05T18:55:11.070314: step 9330, loss 0.649161.
Test: 2018-08-05T18:55:12.132560: step 9330, loss 0.547706.
Train: 2018-08-05T18:55:12.335643: step 9331, loss 0.570984.
Train: 2018-08-05T18:55:12.554342: step 9332, loss 0.519293.
Train: 2018-08-05T18:55:12.773041: step 9333, loss 0.519343.
Train: 2018-08-05T18:55:12.991739: step 9334, loss 0.631221.
Train: 2018-08-05T18:55:13.210408: step 9335, loss 0.58809.
Train: 2018-08-05T18:55:13.413515: step 9336, loss 0.613625.
Train: 2018-08-05T18:55:13.632213: step 9337, loss 0.60479.
Train: 2018-08-05T18:55:13.835262: step 9338, loss 0.596033.
Train: 2018-08-05T18:55:14.053989: step 9339, loss 0.520782.
Train: 2018-08-05T18:55:14.257066: step 9340, loss 0.579038.
Test: 2018-08-05T18:55:15.319288: step 9340, loss 0.548401.
Train: 2018-08-05T18:55:15.522366: step 9341, loss 0.488445.
Train: 2018-08-05T18:55:15.725473: step 9342, loss 0.644819.
Train: 2018-08-05T18:55:15.944172: step 9343, loss 0.570764.
Train: 2018-08-05T18:55:16.162840: step 9344, loss 0.489247.
Train: 2018-08-05T18:55:16.365951: step 9345, loss 0.554444.
Train: 2018-08-05T18:55:16.584647: step 9346, loss 0.619861.
Train: 2018-08-05T18:55:16.787695: step 9347, loss 0.538069.
Train: 2018-08-05T18:55:17.006423: step 9348, loss 0.587136.
Train: 2018-08-05T18:55:17.209501: step 9349, loss 0.521644.
Train: 2018-08-05T18:55:17.428200: step 9350, loss 0.587187.
Test: 2018-08-05T18:55:18.474830: step 9350, loss 0.549162.
Train: 2018-08-05T18:55:18.693528: step 9351, loss 0.513148.
Train: 2018-08-05T18:55:18.896607: step 9352, loss 0.636953.
Train: 2018-08-05T18:55:19.099683: step 9353, loss 0.57903.
Train: 2018-08-05T18:55:19.318382: step 9354, loss 0.48877.
Train: 2018-08-05T18:55:19.521429: step 9355, loss 0.554121.
Train: 2018-08-05T18:55:19.740159: step 9356, loss 0.495479.
Train: 2018-08-05T18:55:19.943235: step 9357, loss 0.553921.
Train: 2018-08-05T18:55:20.146315: step 9358, loss 0.656065.
Train: 2018-08-05T18:55:20.364982: step 9359, loss 0.622124.
Train: 2018-08-05T18:55:20.568090: step 9360, loss 0.56234.
Test: 2018-08-05T18:55:21.630311: step 9360, loss 0.547136.
Train: 2018-08-05T18:55:21.833418: step 9361, loss 0.553824.
Train: 2018-08-05T18:55:22.005252: step 9362, loss 0.653146.
Train: 2018-08-05T18:55:22.223952: step 9363, loss 0.638514.
Train: 2018-08-05T18:55:22.426999: step 9364, loss 0.528887.
Train: 2018-08-05T18:55:22.645729: step 9365, loss 0.59573.
Train: 2018-08-05T18:55:22.848805: step 9366, loss 0.479911.
Train: 2018-08-05T18:55:23.067504: step 9367, loss 0.587251.
Train: 2018-08-05T18:55:23.270583: step 9368, loss 0.620129.
Train: 2018-08-05T18:55:23.473662: step 9369, loss 0.554393.
Train: 2018-08-05T18:55:23.676737: step 9370, loss 0.497363.
Test: 2018-08-05T18:55:24.738958: step 9370, loss 0.549898.
Train: 2018-08-05T18:55:24.942066: step 9371, loss 0.529925.
Train: 2018-08-05T18:55:25.145144: step 9372, loss 0.529753.
Train: 2018-08-05T18:55:25.363841: step 9373, loss 0.612028.
Train: 2018-08-05T18:55:25.566918: step 9374, loss 0.554203.
Train: 2018-08-05T18:55:25.769997: step 9375, loss 0.579066.
Train: 2018-08-05T18:55:25.988695: step 9376, loss 0.545781.
Train: 2018-08-05T18:55:26.207365: step 9377, loss 0.579129.
Train: 2018-08-05T18:55:26.410441: step 9378, loss 0.595917.
Train: 2018-08-05T18:55:26.613549: step 9379, loss 0.554012.
Train: 2018-08-05T18:55:26.832241: step 9380, loss 0.56239.
Test: 2018-08-05T18:55:27.878879: step 9380, loss 0.548733.
Train: 2018-08-05T18:55:28.081955: step 9381, loss 0.570789.
Train: 2018-08-05T18:55:28.300623: step 9382, loss 0.579205.
Train: 2018-08-05T18:55:28.519354: step 9383, loss 0.511906.
Train: 2018-08-05T18:55:28.722431: step 9384, loss 0.553926.
Train: 2018-08-05T18:55:28.925507: step 9385, loss 0.52.
Train: 2018-08-05T18:55:29.128586: step 9386, loss 0.604948.
Train: 2018-08-05T18:55:29.362877: step 9387, loss 0.545247.
Train: 2018-08-05T18:55:29.565982: step 9388, loss 0.562335.
Train: 2018-08-05T18:55:29.769030: step 9389, loss 0.493517.
Train: 2018-08-05T18:55:29.972108: step 9390, loss 0.562344.
Test: 2018-08-05T18:55:31.034358: step 9390, loss 0.548211.
Train: 2018-08-05T18:55:31.237465: step 9391, loss 0.553648.
Train: 2018-08-05T18:55:31.440543: step 9392, loss 0.65.
Train: 2018-08-05T18:55:31.659242: step 9393, loss 0.553624.
Train: 2018-08-05T18:55:31.862320: step 9394, loss 0.57989.
Train: 2018-08-05T18:55:32.065397: step 9395, loss 0.606076.
Train: 2018-08-05T18:55:32.268476: step 9396, loss 0.588463.
Train: 2018-08-05T18:55:32.487184: step 9397, loss 0.458543.
Train: 2018-08-05T18:55:32.690251: step 9398, loss 0.553684.
Train: 2018-08-05T18:55:32.893330: step 9399, loss 0.536338.
Train: 2018-08-05T18:55:33.096406: step 9400, loss 0.501525.
Test: 2018-08-05T18:55:34.158645: step 9400, loss 0.548584.
Train: 2018-08-05T18:55:35.111558: step 9401, loss 0.614795.
Train: 2018-08-05T18:55:35.330257: step 9402, loss 0.579879.
Train: 2018-08-05T18:55:35.548956: step 9403, loss 0.518626.
Train: 2018-08-05T18:55:35.752004: step 9404, loss 0.562388.
Train: 2018-08-05T18:55:35.955111: step 9405, loss 0.536044.
Train: 2018-08-05T18:55:36.158188: step 9406, loss 0.553604.
Train: 2018-08-05T18:55:36.361266: step 9407, loss 0.553598.
Train: 2018-08-05T18:55:36.564344: step 9408, loss 0.544744.
Train: 2018-08-05T18:55:36.767422: step 9409, loss 0.633451.
Train: 2018-08-05T18:55:36.986120: step 9410, loss 0.509331.
Test: 2018-08-05T18:55:38.032749: step 9410, loss 0.547454.
Train: 2018-08-05T18:55:38.251418: step 9411, loss 0.597879.
Train: 2018-08-05T18:55:38.454525: step 9412, loss 0.588951.
Train: 2018-08-05T18:55:38.657603: step 9413, loss 0.535997.
Train: 2018-08-05T18:55:38.860681: step 9414, loss 0.536047.
Train: 2018-08-05T18:55:39.063759: step 9415, loss 0.623813.
Train: 2018-08-05T18:55:39.282457: step 9416, loss 0.579833.
Train: 2018-08-05T18:55:39.485534: step 9417, loss 0.553668.
Train: 2018-08-05T18:55:39.704204: step 9418, loss 0.510502.
Train: 2018-08-05T18:55:39.907280: step 9419, loss 0.579592.
Train: 2018-08-05T18:55:40.110389: step 9420, loss 0.579547.
Test: 2018-08-05T18:55:41.172639: step 9420, loss 0.548604.
Train: 2018-08-05T18:55:41.375686: step 9421, loss 0.545182.
Train: 2018-08-05T18:55:41.578794: step 9422, loss 0.596571.
Train: 2018-08-05T18:55:41.781841: step 9423, loss 0.519715.
Train: 2018-08-05T18:55:41.984950: step 9424, loss 0.570858.
Train: 2018-08-05T18:55:42.187997: step 9425, loss 0.553839.
Train: 2018-08-05T18:55:42.391105: step 9426, loss 0.638846.
Train: 2018-08-05T18:55:42.594182: step 9427, loss 0.520078.
Train: 2018-08-05T18:55:42.797259: step 9428, loss 0.503287.
Train: 2018-08-05T18:55:43.000335: step 9429, loss 0.477788.
Train: 2018-08-05T18:55:43.203384: step 9430, loss 0.519753.
Test: 2018-08-05T18:55:44.250014: step 9430, loss 0.548971.
Train: 2018-08-05T18:55:44.453121: step 9431, loss 0.605308.
Train: 2018-08-05T18:55:44.656199: step 9432, loss 0.553699.
Train: 2018-08-05T18:55:44.859276: step 9433, loss 0.527605.
Train: 2018-08-05T18:55:45.062353: step 9434, loss 0.614834.
Train: 2018-08-05T18:55:45.281022: step 9435, loss 0.527327.
Train: 2018-08-05T18:55:45.484129: step 9436, loss 0.553607.
Train: 2018-08-05T18:55:45.687208: step 9437, loss 0.571259.
Train: 2018-08-05T18:55:45.890286: step 9438, loss 0.553594.
Train: 2018-08-05T18:55:46.093363: step 9439, loss 0.597937.
Train: 2018-08-05T18:55:46.296440: step 9440, loss 0.509278.
Test: 2018-08-05T18:55:47.358660: step 9440, loss 0.548048.
Train: 2018-08-05T18:55:47.561768: step 9441, loss 0.55359.
Train: 2018-08-05T18:55:47.749226: step 9442, loss 0.571379.
Train: 2018-08-05T18:55:47.967923: step 9443, loss 0.535789.
Train: 2018-08-05T18:55:48.171001: step 9444, loss 0.571413.
Train: 2018-08-05T18:55:48.374077: step 9445, loss 0.500104.
Train: 2018-08-05T18:55:48.577155: step 9446, loss 0.651959.
Train: 2018-08-05T18:55:48.780232: step 9447, loss 0.535755.
Train: 2018-08-05T18:55:48.983305: step 9448, loss 0.553588.
Train: 2018-08-05T18:55:49.186358: step 9449, loss 0.535814.
Train: 2018-08-05T18:55:49.389466: step 9450, loss 0.562474.
Test: 2018-08-05T18:55:50.436095: step 9450, loss 0.548051.
Train: 2018-08-05T18:55:50.654789: step 9451, loss 0.606847.
Train: 2018-08-05T18:55:50.857871: step 9452, loss 0.553596.
Train: 2018-08-05T18:55:51.060918: step 9453, loss 0.588843.
Train: 2018-08-05T18:55:51.264027: step 9454, loss 0.597443.
Train: 2018-08-05T18:55:51.451453: step 9455, loss 0.623274.
Train: 2018-08-05T18:55:51.654530: step 9456, loss 0.631243.
Train: 2018-08-05T18:55:51.857606: step 9457, loss 0.511365.
Train: 2018-08-05T18:55:52.060715: step 9458, loss 0.570796.
Train: 2018-08-05T18:55:52.263763: step 9459, loss 0.495636.
Train: 2018-08-05T18:55:52.466873: step 9460, loss 0.537479.
Test: 2018-08-05T18:55:53.513500: step 9460, loss 0.548375.
Train: 2018-08-05T18:55:53.716580: step 9461, loss 0.562448.
Train: 2018-08-05T18:55:53.919650: step 9462, loss 0.579068.
Train: 2018-08-05T18:55:54.122726: step 9463, loss 0.570758.
Train: 2018-08-05T18:55:54.325810: step 9464, loss 0.645353.
Train: 2018-08-05T18:55:54.544527: step 9465, loss 0.537799.
Train: 2018-08-05T18:55:54.731933: step 9466, loss 0.529695.
Train: 2018-08-05T18:55:54.935042: step 9467, loss 0.611812.
Train: 2018-08-05T18:55:55.138120: step 9468, loss 0.578952.
Train: 2018-08-05T18:55:55.341197: step 9469, loss 0.562609.
Train: 2018-08-05T18:55:55.544244: step 9470, loss 0.530054.
Test: 2018-08-05T18:55:56.590904: step 9470, loss 0.548823.
Train: 2018-08-05T18:55:56.793981: step 9471, loss 0.595229.
Train: 2018-08-05T18:55:56.997060: step 9472, loss 0.554487.
Train: 2018-08-05T18:55:57.200130: step 9473, loss 0.595224.
Train: 2018-08-05T18:55:57.403214: step 9474, loss 0.546356.
Train: 2018-08-05T18:55:57.606291: step 9475, loss 0.538185.
Train: 2018-08-05T18:55:57.793747: step 9476, loss 0.489047.
Train: 2018-08-05T18:55:57.996824: step 9477, loss 0.570756.
Train: 2018-08-05T18:55:58.199873: step 9478, loss 0.562406.
Train: 2018-08-05T18:55:58.402949: step 9479, loss 0.545176.
Train: 2018-08-05T18:55:58.606057: step 9480, loss 0.582947.
Test: 2018-08-05T18:55:59.652687: step 9480, loss 0.546962.
Train: 2018-08-05T18:55:59.855764: step 9481, loss 0.553629.
Train: 2018-08-05T18:56:00.058842: step 9482, loss 0.56234.
Train: 2018-08-05T18:56:00.261889: step 9483, loss 0.604776.
Train: 2018-08-05T18:56:00.464996: step 9484, loss 0.562349.
Train: 2018-08-05T18:56:00.668074: step 9485, loss 0.536883.
Train: 2018-08-05T18:56:00.871151: step 9486, loss 0.587857.
Train: 2018-08-05T18:56:01.074231: step 9487, loss 0.579355.
Train: 2018-08-05T18:56:01.277306: step 9488, loss 0.596337.
Train: 2018-08-05T18:56:01.480385: step 9489, loss 0.528457.
Train: 2018-08-05T18:56:01.667842: step 9490, loss 0.621653.
Test: 2018-08-05T18:56:02.730061: step 9490, loss 0.547885.
Train: 2018-08-05T18:56:02.980003: step 9491, loss 0.604561.
Train: 2018-08-05T18:56:03.167489: step 9492, loss 0.545615.
Train: 2018-08-05T18:56:03.370565: step 9493, loss 0.587481.
Train: 2018-08-05T18:56:03.573643: step 9494, loss 0.470988.
Train: 2018-08-05T18:56:03.776721: step 9495, loss 0.529122.
Train: 2018-08-05T18:56:03.964178: step 9496, loss 0.637661.
Train: 2018-08-05T18:56:04.167255: step 9497, loss 0.554062.
Train: 2018-08-05T18:56:04.370332: step 9498, loss 0.545707.
Train: 2018-08-05T18:56:04.573410: step 9499, loss 0.520578.
Train: 2018-08-05T18:56:04.776487: step 9500, loss 0.587588.
Test: 2018-08-05T18:56:05.823117: step 9500, loss 0.548507.
Train: 2018-08-05T18:56:06.776019: step 9501, loss 0.621319.
Train: 2018-08-05T18:56:06.963445: step 9502, loss 0.53716.
Train: 2018-08-05T18:56:07.166552: step 9503, loss 0.579207.
Train: 2018-08-05T18:56:07.369629: step 9504, loss 0.65491.
Train: 2018-08-05T18:56:07.572707: step 9505, loss 0.604226.
Train: 2018-08-05T18:56:07.775784: step 9506, loss 0.587359.
Train: 2018-08-05T18:56:07.978862: step 9507, loss 0.578991.
Train: 2018-08-05T18:56:08.181939: step 9508, loss 0.489065.
Train: 2018-08-05T18:56:08.385019: step 9509, loss 0.538148.
Train: 2018-08-05T18:56:08.588096: step 9510, loss 0.513641.
Test: 2018-08-05T18:56:09.634695: step 9510, loss 0.548311.
Train: 2018-08-05T18:56:09.837801: step 9511, loss 0.570763.
Train: 2018-08-05T18:56:10.040849: step 9512, loss 0.537843.
Train: 2018-08-05T18:56:10.212713: step 9513, loss 0.421263.
Train: 2018-08-05T18:56:10.415791: step 9514, loss 0.562391.
Train: 2018-08-05T18:56:10.603248: step 9515, loss 0.587851.
Train: 2018-08-05T18:56:10.806325: step 9516, loss 0.553747.
Train: 2018-08-05T18:56:11.009404: step 9517, loss 0.597026.
Train: 2018-08-05T18:56:11.196858: step 9518, loss 0.553642.
Train: 2018-08-05T18:56:11.399936: step 9519, loss 0.544842.
Train: 2018-08-05T18:56:11.603013: step 9520, loss 0.615384.
Test: 2018-08-05T18:56:12.665235: step 9520, loss 0.547673.
Train: 2018-08-05T18:56:12.868344: step 9521, loss 0.518238.
Train: 2018-08-05T18:56:13.055798: step 9522, loss 0.58909.
Train: 2018-08-05T18:56:13.258877: step 9523, loss 0.580245.
Train: 2018-08-05T18:56:13.461954: step 9524, loss 0.535828.
Train: 2018-08-05T18:56:13.665031: step 9525, loss 0.482501.
Train: 2018-08-05T18:56:13.868107: step 9526, loss 0.491087.
Train: 2018-08-05T18:56:14.055533: step 9527, loss 0.571727.
Train: 2018-08-05T18:56:14.258643: step 9528, loss 0.626036.
Train: 2018-08-05T18:56:14.461719: step 9529, loss 0.490254.
Train: 2018-08-05T18:56:14.664768: step 9530, loss 0.508199.
Test: 2018-08-05T18:56:15.711427: step 9530, loss 0.547752.
Train: 2018-08-05T18:56:15.914504: step 9531, loss 0.617647.
Train: 2018-08-05T18:56:16.101959: step 9532, loss 0.581119.
Train: 2018-08-05T18:56:16.320659: step 9533, loss 0.553666.
Train: 2018-08-05T18:56:16.523736: step 9534, loss 0.553658.
Train: 2018-08-05T18:56:16.726784: step 9535, loss 0.498973.
Train: 2018-08-05T18:56:16.929860: step 9536, loss 0.590155.
Train: 2018-08-05T18:56:17.132962: step 9537, loss 0.544536.
Train: 2018-08-05T18:56:17.336015: step 9538, loss 0.562747.
Train: 2018-08-05T18:56:17.539094: step 9539, loss 0.526373.
Train: 2018-08-05T18:56:17.742200: step 9540, loss 0.571798.
Test: 2018-08-05T18:56:18.804422: step 9540, loss 0.547757.
Train: 2018-08-05T18:56:19.007531: step 9541, loss 0.526422.
Train: 2018-08-05T18:56:19.194987: step 9542, loss 0.707717.
Train: 2018-08-05T18:56:19.398032: step 9543, loss 0.580534.
Train: 2018-08-05T18:56:19.601140: step 9544, loss 0.59803.
Train: 2018-08-05T18:56:19.788596: step 9545, loss 0.597529.
Train: 2018-08-05T18:56:19.991675: step 9546, loss 0.484336.
Train: 2018-08-05T18:56:20.194752: step 9547, loss 0.51932.
Train: 2018-08-05T18:56:20.382209: step 9548, loss 0.6223.
Train: 2018-08-05T18:56:20.585286: step 9549, loss 0.519834.
Train: 2018-08-05T18:56:20.788362: step 9550, loss 0.63856.
Test: 2018-08-05T18:56:21.834992: step 9550, loss 0.548557.
Train: 2018-08-05T18:56:22.038069: step 9551, loss 0.55399.
Train: 2018-08-05T18:56:22.241148: step 9552, loss 0.512378.
Train: 2018-08-05T18:56:22.444225: step 9553, loss 0.579081.
Train: 2018-08-05T18:56:22.631682: step 9554, loss 0.562464.
Train: 2018-08-05T18:56:22.834729: step 9555, loss 0.512838.
Train: 2018-08-05T18:56:23.037836: step 9556, loss 0.562472.
Train: 2018-08-05T18:56:23.225292: step 9557, loss 0.537561.
Train: 2018-08-05T18:56:23.428369: step 9558, loss 0.545776.
Train: 2018-08-05T18:56:23.631417: step 9559, loss 0.621058.
Train: 2018-08-05T18:56:23.834524: step 9560, loss 0.562405.
Test: 2018-08-05T18:56:24.881155: step 9560, loss 0.548409.
Train: 2018-08-05T18:56:25.084231: step 9561, loss 0.520512.
Train: 2018-08-05T18:56:25.287304: step 9562, loss 0.503527.
Train: 2018-08-05T18:56:25.490357: step 9563, loss 0.596225.
Train: 2018-08-05T18:56:25.677843: step 9564, loss 0.545335.
Train: 2018-08-05T18:56:25.880920: step 9565, loss 0.622169.
Train: 2018-08-05T18:56:26.083997: step 9566, loss 0.536671.
Train: 2018-08-05T18:56:26.271453: step 9567, loss 0.53661.
Train: 2018-08-05T18:56:26.474531: step 9568, loss 0.519306.
Train: 2018-08-05T18:56:26.677608: step 9569, loss 0.640241.
Train: 2018-08-05T18:56:26.880682: step 9570, loss 0.579659.
Test: 2018-08-05T18:56:27.927316: step 9570, loss 0.5471.
Train: 2018-08-05T18:56:28.130394: step 9571, loss 0.59694.
Train: 2018-08-05T18:56:28.333470: step 9572, loss 0.605451.
Train: 2018-08-05T18:56:28.536547: step 9573, loss 0.528034.
Train: 2018-08-05T18:56:28.739627: step 9574, loss 0.536686.
Train: 2018-08-05T18:56:28.927085: step 9575, loss 0.459853.
Train: 2018-08-05T18:56:29.130160: step 9576, loss 0.476487.
Train: 2018-08-05T18:56:29.333236: step 9577, loss 0.562347.
Train: 2018-08-05T18:56:29.536314: step 9578, loss 0.571125.
Train: 2018-08-05T18:56:29.739362: step 9579, loss 0.580024.
Train: 2018-08-05T18:56:29.942469: step 9580, loss 0.588965.
Test: 2018-08-05T18:56:30.989098: step 9580, loss 0.547042.
Train: 2018-08-05T18:56:31.192147: step 9581, loss 0.633345.
Train: 2018-08-05T18:56:31.395254: step 9582, loss 0.562434.
Train: 2018-08-05T18:56:31.598331: step 9583, loss 0.606475.
Train: 2018-08-05T18:56:31.785757: step 9584, loss 0.544859.
Train: 2018-08-05T18:56:31.988864: step 9585, loss 0.483849.
Train: 2018-08-05T18:56:32.191913: step 9586, loss 0.466365.
Train: 2018-08-05T18:56:32.395019: step 9587, loss 0.694055.
Train: 2018-08-05T18:56:32.582475: step 9588, loss 0.553627.
Train: 2018-08-05T18:56:32.785552: step 9589, loss 0.492528.
Train: 2018-08-05T18:56:32.988629: step 9590, loss 0.544887.
Test: 2018-08-05T18:56:34.050852: step 9590, loss 0.546944.
Train: 2018-08-05T18:56:34.253930: step 9591, loss 0.641245.
Train: 2018-08-05T18:56:34.457006: step 9592, loss 0.527428.
Train: 2018-08-05T18:56:34.660113: step 9593, loss 0.562367.
Train: 2018-08-05T18:56:34.863193: step 9594, loss 0.605945.
Train: 2018-08-05T18:56:35.066269: step 9595, loss 0.588397.
Train: 2018-08-05T18:56:35.269347: step 9596, loss 0.545066.
Train: 2018-08-05T18:56:35.456802: step 9597, loss 0.527924.
Train: 2018-08-05T18:56:35.675501: step 9598, loss 0.596693.
Train: 2018-08-05T18:56:35.878580: step 9599, loss 0.562336.
Train: 2018-08-05T18:56:36.081656: step 9600, loss 0.494086.
Test: 2018-08-05T18:56:37.128287: step 9600, loss 0.548072.
Train: 2018-08-05T18:56:38.221780: step 9601, loss 0.596506.
Train: 2018-08-05T18:56:38.424827: step 9602, loss 0.553805.
Train: 2018-08-05T18:56:38.627936: step 9603, loss 0.553808.
Train: 2018-08-05T18:56:38.831012: step 9604, loss 0.587938.
Train: 2018-08-05T18:56:39.034090: step 9605, loss 0.604949.
Train: 2018-08-05T18:56:39.237166: step 9606, loss 0.528389.
Train: 2018-08-05T18:56:39.440214: step 9607, loss 0.536913.
Train: 2018-08-05T18:56:39.643322: step 9608, loss 0.528414.
Train: 2018-08-05T18:56:39.846370: step 9609, loss 0.613382.
Train: 2018-08-05T18:56:40.049477: step 9610, loss 0.528343.
Test: 2018-08-05T18:56:41.111698: step 9610, loss 0.548124.
Train: 2018-08-05T18:56:41.299155: step 9611, loss 0.587883.
Train: 2018-08-05T18:56:41.502232: step 9612, loss 0.494243.
Train: 2018-08-05T18:56:41.705339: step 9613, loss 0.579431.
Train: 2018-08-05T18:56:41.908417: step 9614, loss 0.553766.
Train: 2018-08-05T18:56:42.111493: step 9615, loss 0.51937.
Train: 2018-08-05T18:56:42.314572: step 9616, loss 0.475978.
Train: 2018-08-05T18:56:42.533241: step 9617, loss 0.693135.
Train: 2018-08-05T18:56:42.720727: step 9618, loss 0.527475.
Train: 2018-08-05T18:56:42.923803: step 9619, loss 0.536148.
Train: 2018-08-05T18:56:43.126882: step 9620, loss 0.579928.
Test: 2018-08-05T18:56:44.189103: step 9620, loss 0.547728.
Train: 2018-08-05T18:56:44.376590: step 9621, loss 0.615095.
Train: 2018-08-05T18:56:44.595288: step 9622, loss 0.623739.
Train: 2018-08-05T18:56:44.782743: step 9623, loss 0.553647.
Train: 2018-08-05T18:56:44.985823: step 9624, loss 0.536336.
Train: 2018-08-05T18:56:45.188900: step 9625, loss 0.553698.
Train: 2018-08-05T18:56:45.391976: step 9626, loss 0.579579.
Train: 2018-08-05T18:56:45.595023: step 9627, loss 0.519379.
Train: 2018-08-05T18:56:45.798130: step 9628, loss 0.596679.
Train: 2018-08-05T18:56:46.001209: step 9629, loss 0.553772.
Train: 2018-08-05T18:56:46.204288: step 9630, loss 0.60507.
Test: 2018-08-05T18:56:47.250886: step 9630, loss 0.549502.
Train: 2018-08-05T18:56:47.453994: step 9631, loss 0.553833.
Train: 2018-08-05T18:56:47.657040: step 9632, loss 0.587796.
Train: 2018-08-05T18:56:47.875770: step 9633, loss 0.537033.
Train: 2018-08-05T18:56:48.078846: step 9634, loss 0.604496.
Train: 2018-08-05T18:56:48.281924: step 9635, loss 0.604329.
Train: 2018-08-05T18:56:48.484972: step 9636, loss 0.504095.
Train: 2018-08-05T18:56:48.688049: step 9637, loss 0.612355.
Train: 2018-08-05T18:56:48.891156: step 9638, loss 0.554192.
Train: 2018-08-05T18:56:49.109854: step 9639, loss 0.579014.
Train: 2018-08-05T18:56:49.312932: step 9640, loss 0.529605.
Test: 2018-08-05T18:56:50.359533: step 9640, loss 0.549169.
Train: 2018-08-05T18:56:50.562640: step 9641, loss 0.578985.
Train: 2018-08-05T18:56:50.765717: step 9642, loss 0.587198.
Train: 2018-08-05T18:56:50.984417: step 9643, loss 0.529737.
Train: 2018-08-05T18:56:51.187494: step 9644, loss 0.562547.
Train: 2018-08-05T18:56:51.390541: step 9645, loss 0.554311.
Train: 2018-08-05T18:56:51.593619: step 9646, loss 0.587242.
Train: 2018-08-05T18:56:51.796726: step 9647, loss 0.521254.
Train: 2018-08-05T18:56:51.999804: step 9648, loss 0.570757.
Train: 2018-08-05T18:56:52.202881: step 9649, loss 0.579071.
Train: 2018-08-05T18:56:52.421580: step 9650, loss 0.620749.
Test: 2018-08-05T18:56:53.483800: step 9650, loss 0.548159.
Train: 2018-08-05T18:56:53.686908: step 9651, loss 0.579084.
Train: 2018-08-05T18:56:53.889956: step 9652, loss 0.520913.
Train: 2018-08-05T18:56:54.093067: step 9653, loss 0.587401.
Train: 2018-08-05T18:56:54.311762: step 9654, loss 0.612364.
Train: 2018-08-05T18:56:54.514840: step 9655, loss 0.512669.
Train: 2018-08-05T18:56:54.717917: step 9656, loss 0.529222.
Train: 2018-08-05T18:56:54.920994: step 9657, loss 0.487405.
Train: 2018-08-05T18:56:55.124073: step 9658, loss 0.595997.
Train: 2018-08-05T18:56:55.327150: step 9659, loss 0.537019.
Train: 2018-08-05T18:56:55.545848: step 9660, loss 0.579351.
Test: 2018-08-05T18:56:56.592478: step 9660, loss 0.548067.
Train: 2018-08-05T18:56:56.795555: step 9661, loss 0.511069.
Train: 2018-08-05T18:56:56.998634: step 9662, loss 0.579553.
Train: 2018-08-05T18:56:57.201682: step 9663, loss 0.588312.
Train: 2018-08-05T18:56:57.373546: step 9664, loss 0.617932.
Train: 2018-08-05T18:56:57.592244: step 9665, loss 0.55367.
Train: 2018-08-05T18:56:57.795321: step 9666, loss 0.536317.
Train: 2018-08-05T18:56:57.998400: step 9667, loss 0.571038.
Train: 2018-08-05T18:56:58.201477: step 9668, loss 0.553663.
Train: 2018-08-05T18:56:58.404553: step 9669, loss 0.484113.
Train: 2018-08-05T18:56:58.607602: step 9670, loss 0.544899.
Test: 2018-08-05T18:56:59.669882: step 9670, loss 0.548539.
Train: 2018-08-05T18:56:59.872961: step 9671, loss 0.483371.
Train: 2018-08-05T18:57:00.076037: step 9672, loss 0.571309.
Train: 2018-08-05T18:57:00.279085: step 9673, loss 0.571427.
Train: 2018-08-05T18:57:00.497808: step 9674, loss 0.553592.
Train: 2018-08-05T18:57:00.700891: step 9675, loss 0.544596.
Train: 2018-08-05T18:57:00.903938: step 9676, loss 0.580743.
Train: 2018-08-05T18:57:01.122669: step 9677, loss 0.54456.
Train: 2018-08-05T18:57:01.325714: step 9678, loss 0.56271.
Train: 2018-08-05T18:57:01.528822: step 9679, loss 0.490013.
Train: 2018-08-05T18:57:01.731893: step 9680, loss 0.590165.
Test: 2018-08-05T18:57:02.794150: step 9680, loss 0.548593.
Train: 2018-08-05T18:57:02.997230: step 9681, loss 0.544526.
Train: 2018-08-05T18:57:03.200300: step 9682, loss 0.517073.
Train: 2018-08-05T18:57:03.418975: step 9683, loss 0.535341.
Train: 2018-08-05T18:57:03.622053: step 9684, loss 0.507698.
Train: 2018-08-05T18:57:03.840776: step 9685, loss 0.553756.
Train: 2018-08-05T18:57:04.043858: step 9686, loss 0.58164.
Train: 2018-08-05T18:57:04.262557: step 9687, loss 0.618857.
Train: 2018-08-05T18:57:04.465605: step 9688, loss 0.563035.
Train: 2018-08-05T18:57:04.668712: step 9689, loss 0.581412.
Train: 2018-08-05T18:57:04.887381: step 9690, loss 0.562862.
Test: 2018-08-05T18:57:05.934011: step 9690, loss 0.547123.
Train: 2018-08-05T18:57:06.152739: step 9691, loss 0.544535.
Train: 2018-08-05T18:57:06.355787: step 9692, loss 0.562684.
Train: 2018-08-05T18:57:06.558894: step 9693, loss 0.535597.
Train: 2018-08-05T18:57:06.761972: step 9694, loss 0.535658.
Train: 2018-08-05T18:57:06.980672: step 9695, loss 0.55359.
Train: 2018-08-05T18:57:07.183748: step 9696, loss 0.616048.
Train: 2018-08-05T18:57:07.386825: step 9697, loss 0.580191.
Train: 2018-08-05T18:57:07.589902: step 9698, loss 0.571211.
Train: 2018-08-05T18:57:07.792980: step 9699, loss 0.571108.
Train: 2018-08-05T18:57:08.011678: step 9700, loss 0.527664.
Test: 2018-08-05T18:57:09.058309: step 9700, loss 0.547731.
Train: 2018-08-05T18:57:09.964347: step 9701, loss 0.562338.
Train: 2018-08-05T18:57:10.183016: step 9702, loss 0.61386.
Train: 2018-08-05T18:57:10.386123: step 9703, loss 0.528243.
Train: 2018-08-05T18:57:10.589200: step 9704, loss 0.536893.
Train: 2018-08-05T18:57:10.807899: step 9705, loss 0.579286.
Train: 2018-08-05T18:57:11.010976: step 9706, loss 0.579243.
Train: 2018-08-05T18:57:11.214054: step 9707, loss 0.57079.
Train: 2018-08-05T18:57:11.432753: step 9708, loss 0.554025.
Train: 2018-08-05T18:57:11.635801: step 9709, loss 0.545704.
Train: 2018-08-05T18:57:11.854531: step 9710, loss 0.554073.
Test: 2018-08-05T18:57:12.916780: step 9710, loss 0.548671.
Train: 2018-08-05T18:57:13.119828: step 9711, loss 0.512333.
Train: 2018-08-05T18:57:13.322935: step 9712, loss 0.579154.
Train: 2018-08-05T18:57:13.541604: step 9713, loss 0.5372.
Train: 2018-08-05T18:57:13.744713: step 9714, loss 0.545514.
Train: 2018-08-05T18:57:13.963410: step 9715, loss 0.520011.
Train: 2018-08-05T18:57:14.166489: step 9716, loss 0.587923.
Train: 2018-08-05T18:57:14.369566: step 9717, loss 0.570903.
Train: 2018-08-05T18:57:14.572642: step 9718, loss 0.527938.
Train: 2018-08-05T18:57:14.791312: step 9719, loss 0.527761.
Train: 2018-08-05T18:57:14.994420: step 9720, loss 0.544951.
Test: 2018-08-05T18:57:16.056639: step 9720, loss 0.547348.
Train: 2018-08-05T18:57:16.259747: step 9721, loss 0.553622.
Train: 2018-08-05T18:57:16.462824: step 9722, loss 0.509518.
Train: 2018-08-05T18:57:16.665873: step 9723, loss 0.535809.
Train: 2018-08-05T18:57:16.884601: step 9724, loss 0.616351.
Train: 2018-08-05T18:57:17.087649: step 9725, loss 0.616595.
Train: 2018-08-05T18:57:17.306378: step 9726, loss 0.580584.
Train: 2018-08-05T18:57:17.509455: step 9727, loss 0.535644.
Train: 2018-08-05T18:57:17.712536: step 9728, loss 0.598419.
Train: 2018-08-05T18:57:17.931231: step 9729, loss 0.553589.
Train: 2018-08-05T18:57:18.134308: step 9730, loss 0.517987.
Test: 2018-08-05T18:57:19.196530: step 9730, loss 0.548041.
Train: 2018-08-05T18:57:19.399637: step 9731, loss 0.491355.
Train: 2018-08-05T18:57:19.618306: step 9732, loss 0.553588.
Train: 2018-08-05T18:57:19.837035: step 9733, loss 0.580392.
Train: 2018-08-05T18:57:20.040112: step 9734, loss 0.678722.
Train: 2018-08-05T18:57:20.243159: step 9735, loss 0.597965.
Train: 2018-08-05T18:57:20.461888: step 9736, loss 0.579994.
Train: 2018-08-05T18:57:20.680589: step 9737, loss 0.518804.
Train: 2018-08-05T18:57:20.899285: step 9738, loss 0.588311.
Train: 2018-08-05T18:57:21.102365: step 9739, loss 0.562335.
Train: 2018-08-05T18:57:21.305411: step 9740, loss 0.579405.
Test: 2018-08-05T18:57:22.367661: step 9740, loss 0.548206.
Train: 2018-08-05T18:57:22.570771: step 9741, loss 0.579297.
Train: 2018-08-05T18:57:22.789469: step 9742, loss 0.51193.
Train: 2018-08-05T18:57:23.008167: step 9743, loss 0.503736.
Train: 2018-08-05T18:57:23.226865: step 9744, loss 0.545618.
Train: 2018-08-05T18:57:23.429943: step 9745, loss 0.553978.
Train: 2018-08-05T18:57:23.648642: step 9746, loss 0.511809.
Train: 2018-08-05T18:57:23.851719: step 9747, loss 0.553876.
Train: 2018-08-05T18:57:24.070420: step 9748, loss 0.587925.
Train: 2018-08-05T18:57:24.289117: step 9749, loss 0.536671.
Train: 2018-08-05T18:57:24.492196: step 9750, loss 0.553739.
Test: 2018-08-05T18:57:25.554415: step 9750, loss 0.547317.
Train: 2018-08-05T18:57:25.757523: step 9751, loss 0.596892.
Train: 2018-08-05T18:57:25.976221: step 9752, loss 0.579648.
Train: 2018-08-05T18:57:26.179301: step 9753, loss 0.493089.
Train: 2018-08-05T18:57:26.397998: step 9754, loss 0.579749.
Train: 2018-08-05T18:57:26.601046: step 9755, loss 0.510039.
Train: 2018-08-05T18:57:26.819778: step 9756, loss 0.58869.
Train: 2018-08-05T18:57:27.022852: step 9757, loss 0.544817.
Train: 2018-08-05T18:57:27.241521: step 9758, loss 0.562421.
Train: 2018-08-05T18:57:27.444599: step 9759, loss 0.562438.
Train: 2018-08-05T18:57:27.678950: step 9760, loss 0.429589.
Test: 2018-08-05T18:57:28.725578: step 9760, loss 0.548218.
Train: 2018-08-05T18:57:28.944277: step 9761, loss 0.482099.
Train: 2018-08-05T18:57:29.162946: step 9762, loss 0.689274.
Train: 2018-08-05T18:57:29.381674: step 9763, loss 0.49015.
Train: 2018-08-05T18:57:29.600344: step 9764, loss 0.581011.
Train: 2018-08-05T18:57:29.803451: step 9765, loss 0.654305.
Train: 2018-08-05T18:57:30.022120: step 9766, loss 0.517179.
Train: 2018-08-05T18:57:30.240849: step 9767, loss 0.580964.
Train: 2018-08-05T18:57:30.443926: step 9768, loss 0.580864.
Train: 2018-08-05T18:57:30.662595: step 9769, loss 0.562647.
Train: 2018-08-05T18:57:30.865703: step 9770, loss 0.562588.
Test: 2018-08-05T18:57:31.927948: step 9770, loss 0.547802.
Train: 2018-08-05T18:57:32.131030: step 9771, loss 0.517814.
Train: 2018-08-05T18:57:32.334078: step 9772, loss 0.624942.
Train: 2018-08-05T18:57:32.552808: step 9773, loss 0.509295.
Train: 2018-08-05T18:57:32.771476: step 9774, loss 0.527112.
Train: 2018-08-05T18:57:32.974554: step 9775, loss 0.571234.
Train: 2018-08-05T18:57:33.193282: step 9776, loss 0.597587.
Train: 2018-08-05T18:57:33.411984: step 9777, loss 0.579891.
Train: 2018-08-05T18:57:33.615059: step 9778, loss 0.553654.
Train: 2018-08-05T18:57:33.833759: step 9779, loss 0.545021.
Train: 2018-08-05T18:57:34.036834: step 9780, loss 0.579617.
Test: 2018-08-05T18:57:35.099056: step 9780, loss 0.547971.
Train: 2018-08-05T18:57:35.317784: step 9781, loss 0.502129.
Train: 2018-08-05T18:57:35.567726: step 9782, loss 0.622524.
Train: 2018-08-05T18:57:35.786425: step 9783, loss 0.588028.
Train: 2018-08-05T18:57:36.005123: step 9784, loss 0.511223.
Train: 2018-08-05T18:57:36.208201: step 9785, loss 0.604877.
Train: 2018-08-05T18:57:36.426900: step 9786, loss 0.519987.
Train: 2018-08-05T18:57:36.629977: step 9787, loss 0.613153.
Train: 2018-08-05T18:57:36.833025: step 9788, loss 0.486458.
Train: 2018-08-05T18:57:37.051723: step 9789, loss 0.629939.
Train: 2018-08-05T18:57:37.270453: step 9790, loss 0.570798.
Test: 2018-08-05T18:57:38.317052: step 9790, loss 0.547388.
Train: 2018-08-05T18:57:38.520159: step 9791, loss 0.562386.
Train: 2018-08-05T18:57:38.738860: step 9792, loss 0.595926.
Train: 2018-08-05T18:57:38.941938: step 9793, loss 0.562419.
Train: 2018-08-05T18:57:39.145014: step 9794, loss 0.654.
Train: 2018-08-05T18:57:39.363713: step 9795, loss 0.570756.
Train: 2018-08-05T18:57:39.566789: step 9796, loss 0.619925.
Train: 2018-08-05T18:57:39.785488: step 9797, loss 0.59513.
Train: 2018-08-05T18:57:39.988567: step 9798, loss 0.55478.
Train: 2018-08-05T18:57:40.207264: step 9799, loss 0.626657.
Train: 2018-08-05T18:57:40.425963: step 9800, loss 0.641934.
Test: 2018-08-05T18:57:41.472593: step 9800, loss 0.549464.
Train: 2018-08-05T18:57:42.441117: step 9801, loss 0.547793.
Train: 2018-08-05T18:57:42.644194: step 9802, loss 0.571254.
Train: 2018-08-05T18:57:42.847271: step 9803, loss 0.54846.
Train: 2018-08-05T18:57:43.065969: step 9804, loss 0.58665.
Train: 2018-08-05T18:57:43.284668: step 9805, loss 0.533661.
Train: 2018-08-05T18:57:43.487748: step 9806, loss 0.586649.
Train: 2018-08-05T18:57:43.690825: step 9807, loss 0.594334.
Train: 2018-08-05T18:57:43.893901: step 9808, loss 0.59432.
Train: 2018-08-05T18:57:44.112601: step 9809, loss 0.662599.
Train: 2018-08-05T18:57:44.315677: step 9810, loss 0.55671.
Test: 2018-08-05T18:57:45.377929: step 9810, loss 0.553533.
Train: 2018-08-05T18:57:45.581006: step 9811, loss 0.579224.
Train: 2018-08-05T18:57:45.799705: step 9812, loss 0.542007.
Train: 2018-08-05T18:57:46.002782: step 9813, loss 0.549412.
Train: 2018-08-05T18:57:46.221503: step 9814, loss 0.59417.
Train: 2018-08-05T18:57:46.393285: step 9815, loss 0.548127.
Train: 2018-08-05T18:57:46.596395: step 9816, loss 0.609368.
Train: 2018-08-05T18:57:46.815092: step 9817, loss 0.556346.
Train: 2018-08-05T18:57:47.018140: step 9818, loss 0.548565.
Train: 2018-08-05T18:57:47.221217: step 9819, loss 0.640356.
Train: 2018-08-05T18:57:47.439945: step 9820, loss 0.57896.
Test: 2018-08-05T18:57:48.502168: step 9820, loss 0.550451.
Train: 2018-08-05T18:57:48.705274: step 9821, loss 0.578943.
Train: 2018-08-05T18:57:48.923944: step 9822, loss 0.602139.
Train: 2018-08-05T18:57:49.127052: step 9823, loss 0.540186.
Train: 2018-08-05T18:57:49.345751: step 9824, loss 0.555568.
Train: 2018-08-05T18:57:49.548797: step 9825, loss 0.547602.
Train: 2018-08-05T18:57:49.751906: step 9826, loss 0.555238.
Train: 2018-08-05T18:57:49.970572: step 9827, loss 0.53121.
Train: 2018-08-05T18:57:50.173681: step 9828, loss 0.538584.
Train: 2018-08-05T18:57:50.376758: step 9829, loss 0.630596.
Train: 2018-08-05T18:57:50.595456: step 9830, loss 0.578945.
Test: 2018-08-05T18:57:51.642086: step 9830, loss 0.548536.
Train: 2018-08-05T18:57:51.860787: step 9831, loss 0.57895.
Train: 2018-08-05T18:57:52.063832: step 9832, loss 0.570757.
Train: 2018-08-05T18:57:52.282531: step 9833, loss 0.587284.
Train: 2018-08-05T18:57:52.485641: step 9834, loss 0.587318.
Train: 2018-08-05T18:57:52.704338: step 9835, loss 0.56247.
Train: 2018-08-05T18:57:52.907415: step 9836, loss 0.520985.
Train: 2018-08-05T18:57:53.126084: step 9837, loss 0.637392.
Train: 2018-08-05T18:57:53.329163: step 9838, loss 0.537474.
Train: 2018-08-05T18:57:53.532269: step 9839, loss 0.562432.
Train: 2018-08-05T18:57:53.750937: step 9840, loss 0.570767.
Test: 2018-08-05T18:57:54.808590: step 9840, loss 0.547505.
Train: 2018-08-05T18:57:55.011685: step 9841, loss 0.520652.
Train: 2018-08-05T18:57:55.214762: step 9842, loss 0.537236.
Train: 2018-08-05T18:57:55.433461: step 9843, loss 0.646725.
Train: 2018-08-05T18:57:55.652160: step 9844, loss 0.5708.
Train: 2018-08-05T18:57:55.855237: step 9845, loss 0.570797.
Train: 2018-08-05T18:57:56.058284: step 9846, loss 0.511887.
Train: 2018-08-05T18:57:56.277016: step 9847, loss 0.604552.
Train: 2018-08-05T18:57:56.480091: step 9848, loss 0.587675.
Train: 2018-08-05T18:57:56.683139: step 9849, loss 0.587642.
Train: 2018-08-05T18:57:56.901868: step 9850, loss 0.478393.
Test: 2018-08-05T18:57:57.948468: step 9850, loss 0.547916.
Train: 2018-08-05T18:57:58.151545: step 9851, loss 0.579222.
Train: 2018-08-05T18:57:58.370273: step 9852, loss 0.503289.
Train: 2018-08-05T18:57:58.573351: step 9853, loss 0.553864.
Train: 2018-08-05T18:57:58.776428: step 9854, loss 0.536741.
Train: 2018-08-05T18:57:58.995127: step 9855, loss 0.562335.
Train: 2018-08-05T18:57:59.198204: step 9856, loss 0.596893.
Train: 2018-08-05T18:57:59.401251: step 9857, loss 0.579673.
Train: 2018-08-05T18:57:59.619951: step 9858, loss 0.562348.
Train: 2018-08-05T18:57:59.838680: step 9859, loss 0.605776.
Train: 2018-08-05T18:58:00.041759: step 9860, loss 0.631693.
Test: 2018-08-05T18:58:01.104008: step 9860, loss 0.54656.
Train: 2018-08-05T18:58:01.307055: step 9861, loss 0.510644.
Train: 2018-08-05T18:58:01.510133: step 9862, loss 0.579523.
Train: 2018-08-05T18:58:01.713211: step 9863, loss 0.596595.
Train: 2018-08-05T18:58:01.931940: step 9864, loss 0.562341.
Train: 2018-08-05T18:58:02.135017: step 9865, loss 0.519946.
Train: 2018-08-05T18:58:02.353686: step 9866, loss 0.579292.
Train: 2018-08-05T18:58:02.556793: step 9867, loss 0.503217.
Train: 2018-08-05T18:58:02.759871: step 9868, loss 0.545428.
Train: 2018-08-05T18:58:02.962948: step 9869, loss 0.562349.
Train: 2018-08-05T18:58:03.181617: step 9870, loss 0.562343.
Test: 2018-08-05T18:58:04.228247: step 9870, loss 0.547504.
Train: 2018-08-05T18:58:04.493840: step 9871, loss 0.562339.
Train: 2018-08-05T18:58:04.696917: step 9872, loss 0.553788.
Train: 2018-08-05T18:58:04.899994: step 9873, loss 0.47664.
Train: 2018-08-05T18:58:05.118693: step 9874, loss 0.484641.
Train: 2018-08-05T18:58:05.321741: step 9875, loss 0.675861.
Train: 2018-08-05T18:58:05.524851: step 9876, loss 0.492336.
Train: 2018-08-05T18:58:05.743547: step 9877, loss 0.562415.
Train: 2018-08-05T18:58:05.946624: step 9878, loss 0.509296.
Train: 2018-08-05T18:58:06.149671: step 9879, loss 0.500036.
Train: 2018-08-05T18:58:06.352779: step 9880, loss 0.562616.
Test: 2018-08-05T18:58:07.399378: step 9880, loss 0.548173.
Train: 2018-08-05T18:58:07.602486: step 9881, loss 0.499124.
Train: 2018-08-05T18:58:07.821186: step 9882, loss 0.489461.
Train: 2018-08-05T18:58:08.024263: step 9883, loss 0.628105.
Train: 2018-08-05T18:58:08.227311: step 9884, loss 0.451084.
Train: 2018-08-05T18:58:08.430388: step 9885, loss 0.591775.
Train: 2018-08-05T18:58:08.633495: step 9886, loss 0.554105.
Train: 2018-08-05T18:58:08.852163: step 9887, loss 0.544639.
Train: 2018-08-05T18:58:09.055271: step 9888, loss 0.429694.
Train: 2018-08-05T18:58:09.273970: step 9889, loss 0.631793.
Train: 2018-08-05T18:58:09.477016: step 9890, loss 0.544798.
Test: 2018-08-05T18:58:10.523677: step 9890, loss 0.547351.
Train: 2018-08-05T18:58:10.726760: step 9891, loss 0.564237.
Train: 2018-08-05T18:58:10.929832: step 9892, loss 0.612747.
Train: 2018-08-05T18:58:11.148531: step 9893, loss 0.525439.
Train: 2018-08-05T18:58:11.351579: step 9894, loss 0.57356.
Train: 2018-08-05T18:58:11.554686: step 9895, loss 0.535092.
Train: 2018-08-05T18:58:11.757763: step 9896, loss 0.55405.
Train: 2018-08-05T18:58:11.960843: step 9897, loss 0.553734.
Train: 2018-08-05T18:58:12.179542: step 9898, loss 0.536692.
Train: 2018-08-05T18:58:12.382617: step 9899, loss 0.535476.
Train: 2018-08-05T18:58:12.585695: step 9900, loss 0.563188.
Test: 2018-08-05T18:58:13.632295: step 9900, loss 0.546305.
Train: 2018-08-05T18:58:14.632060: step 9901, loss 0.609779.
Train: 2018-08-05T18:58:14.835140: step 9902, loss 0.572284.
Train: 2018-08-05T18:58:15.053866: step 9903, loss 0.526145.
Train: 2018-08-05T18:58:15.256945: step 9904, loss 0.562787.
Train: 2018-08-05T18:58:15.460021: step 9905, loss 0.571765.
Train: 2018-08-05T18:58:15.678691: step 9906, loss 0.526587.
Train: 2018-08-05T18:58:15.881798: step 9907, loss 0.56255.
Train: 2018-08-05T18:58:16.084846: step 9908, loss 0.526856.
Train: 2018-08-05T18:58:16.287953: step 9909, loss 0.518062.
Train: 2018-08-05T18:58:16.491032: step 9910, loss 0.500345.
Test: 2018-08-05T18:58:17.553252: step 9910, loss 0.546604.
Train: 2018-08-05T18:58:17.756329: step 9911, loss 0.562484.
Train: 2018-08-05T18:58:17.959438: step 9912, loss 0.624869.
Train: 2018-08-05T18:58:18.162513: step 9913, loss 0.562474.
Train: 2018-08-05T18:58:18.365591: step 9914, loss 0.535881.
Train: 2018-08-05T18:58:18.584260: step 9915, loss 0.544755.
Train: 2018-08-05T18:58:18.787336: step 9916, loss 0.544766.
Train: 2018-08-05T18:58:18.990445: step 9917, loss 0.597741.
Train: 2018-08-05T18:58:19.193523: step 9918, loss 0.606424.
Train: 2018-08-05T18:58:19.412192: step 9919, loss 0.597392.
Train: 2018-08-05T18:58:19.615299: step 9920, loss 0.501543.
Test: 2018-08-05T18:58:20.661928: step 9920, loss 0.548682.
Train: 2018-08-05T18:58:20.865006: step 9921, loss 0.579659.
Train: 2018-08-05T18:58:21.068083: step 9922, loss 0.510612.
Train: 2018-08-05T18:58:21.271160: step 9923, loss 0.545109.
Train: 2018-08-05T18:58:21.474209: step 9924, loss 0.605413.
Train: 2018-08-05T18:58:21.677315: step 9925, loss 0.588115.
Train: 2018-08-05T18:58:21.880363: step 9926, loss 0.588014.
Train: 2018-08-05T18:58:22.083470: step 9927, loss 0.596403.
Train: 2018-08-05T18:58:22.302143: step 9928, loss 0.596191.
Train: 2018-08-05T18:58:22.505218: step 9929, loss 0.52044.
Train: 2018-08-05T18:58:22.708324: step 9930, loss 0.537355.
Test: 2018-08-05T18:58:23.754955: step 9930, loss 0.550233.
Train: 2018-08-05T18:58:23.973653: step 9931, loss 0.520747.
Train: 2018-08-05T18:58:24.176730: step 9932, loss 0.52904.
Train: 2018-08-05T18:58:24.379807: step 9933, loss 0.621027.
Train: 2018-08-05T18:58:24.582855: step 9934, loss 0.587517.
Train: 2018-08-05T18:58:24.785962: step 9935, loss 0.554056.
Train: 2018-08-05T18:58:24.989040: step 9936, loss 0.529009.
Train: 2018-08-05T18:58:25.192086: step 9937, loss 0.729758.
Train: 2018-08-05T18:58:25.395195: step 9938, loss 0.554166.
Train: 2018-08-05T18:58:25.598243: step 9939, loss 0.578997.
Train: 2018-08-05T18:58:25.801320: step 9940, loss 0.570765.
Test: 2018-08-05T18:58:26.863600: step 9940, loss 0.548869.
Train: 2018-08-05T18:58:27.066679: step 9941, loss 0.538236.
Train: 2018-08-05T18:58:27.269726: step 9942, loss 0.570792.
Train: 2018-08-05T18:58:27.457212: step 9943, loss 0.675957.
Train: 2018-08-05T18:58:27.675911: step 9944, loss 0.57085.
Train: 2018-08-05T18:58:27.878958: step 9945, loss 0.54704.
Train: 2018-08-05T18:58:28.082065: step 9946, loss 0.59469.
Train: 2018-08-05T18:58:28.300764: step 9947, loss 0.563125.
Train: 2018-08-05T18:58:28.503813: step 9948, loss 0.571036.
Train: 2018-08-05T18:58:28.706919: step 9949, loss 0.539799.
Train: 2018-08-05T18:58:28.909997: step 9950, loss 0.55543.
Test: 2018-08-05T18:58:29.972248: step 9950, loss 0.550131.
Train: 2018-08-05T18:58:30.175326: step 9951, loss 0.547547.
Train: 2018-08-05T18:58:30.378373: step 9952, loss 0.563137.
Train: 2018-08-05T18:58:30.581481: step 9953, loss 0.547203.
Train: 2018-08-05T18:58:30.784528: step 9954, loss 0.562771.
Train: 2018-08-05T18:58:30.987638: step 9955, loss 0.62131.
Train: 2018-08-05T18:58:31.190713: step 9956, loss 0.505961.
Train: 2018-08-05T18:58:31.393790: step 9957, loss 0.587106.
Train: 2018-08-05T18:58:31.596868: step 9958, loss 0.628031.
Train: 2018-08-05T18:58:31.799948: step 9959, loss 0.554548.
Train: 2018-08-05T18:58:32.003023: step 9960, loss 0.562658.
Test: 2018-08-05T18:58:33.049647: step 9960, loss 0.549405.
Train: 2018-08-05T18:58:33.268350: step 9961, loss 0.603357.
Train: 2018-08-05T18:58:33.471429: step 9962, loss 0.538198.
Train: 2018-08-05T18:58:33.658885: step 9963, loss 0.489134.
Train: 2018-08-05T18:58:33.877585: step 9964, loss 0.58721.
Train: 2018-08-05T18:58:34.080632: step 9965, loss 0.521117.
Train: 2018-08-05T18:58:34.252497: step 9966, loss 0.562424.
Train: 2018-08-05T18:58:34.455573: step 9967, loss 0.579201.
Train: 2018-08-05T18:58:34.658651: step 9968, loss 0.663912.
Train: 2018-08-05T18:58:34.861728: step 9969, loss 0.570813.
Train: 2018-08-05T18:58:35.064805: step 9970, loss 0.570804.
Test: 2018-08-05T18:58:36.111435: step 9970, loss 0.548315.
Train: 2018-08-05T18:58:36.314482: step 9971, loss 0.654989.
Train: 2018-08-05T18:58:36.517590: step 9972, loss 0.570757.
Train: 2018-08-05T18:58:36.720667: step 9973, loss 0.52236.
Train: 2018-08-05T18:58:36.923745: step 9974, loss 0.537712.
Train: 2018-08-05T18:58:37.126822: step 9975, loss 0.587384.
Train: 2018-08-05T18:58:37.329870: step 9976, loss 0.587309.
Train: 2018-08-05T18:58:37.532977: step 9977, loss 0.496432.
Train: 2018-08-05T18:58:37.720434: step 9978, loss 0.554195.
Train: 2018-08-05T18:58:37.923514: step 9979, loss 0.504281.
Train: 2018-08-05T18:58:38.126585: step 9980, loss 0.52892.
Test: 2018-08-05T18:58:39.188824: step 9980, loss 0.549226.
Train: 2018-08-05T18:58:39.376295: step 9981, loss 0.520122.
Train: 2018-08-05T18:58:39.579373: step 9982, loss 0.502522.
Train: 2018-08-05T18:58:39.782420: step 9983, loss 0.536334.
Train: 2018-08-05T18:58:39.985528: step 9984, loss 0.624035.
Train: 2018-08-05T18:58:40.188605: step 9985, loss 0.606848.
Train: 2018-08-05T18:58:40.376032: step 9986, loss 0.589203.
Train: 2018-08-05T18:58:40.579142: step 9987, loss 0.562492.
Train: 2018-08-05T18:58:40.782216: step 9988, loss 0.606975.
Train: 2018-08-05T18:58:40.985263: step 9989, loss 0.633333.
Train: 2018-08-05T18:58:41.188373: step 9990, loss 0.509717.
Test: 2018-08-05T18:58:42.234972: step 9990, loss 0.547369.
Train: 2018-08-05T18:58:42.438048: step 9991, loss 0.553631.
Train: 2018-08-05T18:58:42.641157: step 9992, loss 0.597212.
Train: 2018-08-05T18:58:42.844234: step 9993, loss 0.510348.
Train: 2018-08-05T18:58:43.047282: step 9994, loss 0.545039.
Train: 2018-08-05T18:58:43.234762: step 9995, loss 0.57099.
Train: 2018-08-05T18:58:43.437844: step 9996, loss 0.510501.
Train: 2018-08-05T18:58:43.640923: step 9997, loss 0.640359.
Train: 2018-08-05T18:58:43.843999: step 9998, loss 0.570972.
Train: 2018-08-05T18:58:44.047047: step 9999, loss 0.562336.
Train: 2018-08-05T18:58:44.250125: step 10000, loss 0.510878.
Test: 2018-08-05T18:58:45.296784: step 10000, loss 0.547214.
Train: 2018-08-05T18:58:46.296520: step 10001, loss 0.536592.
Train: 2018-08-05T18:58:46.499628: step 10002, loss 0.631138.
Train: 2018-08-05T18:58:46.687084: step 10003, loss 0.545184.
Train: 2018-08-05T18:58:46.890161: step 10004, loss 0.579461.
Train: 2018-08-05T18:58:47.093239: step 10005, loss 0.596501.
Train: 2018-08-05T18:58:47.296316: step 10006, loss 0.545345.
Train: 2018-08-05T18:58:47.515016: step 10007, loss 0.545404.
Train: 2018-08-05T18:58:47.702465: step 10008, loss 0.536968.
Train: 2018-08-05T18:58:47.905548: step 10009, loss 0.52848.
Train: 2018-08-05T18:58:48.108598: step 10010, loss 0.570843.
Test: 2018-08-05T18:58:49.155256: step 10010, loss 0.548511.
Train: 2018-08-05T18:58:49.358333: step 10011, loss 0.562342.
Train: 2018-08-05T18:58:49.561410: step 10012, loss 0.553803.
Train: 2018-08-05T18:58:49.764489: step 10013, loss 0.596569.
Train: 2018-08-05T18:58:49.967566: step 10014, loss 0.528111.
Train: 2018-08-05T18:58:50.170617: step 10015, loss 0.476568.
Train: 2018-08-05T18:58:50.358070: step 10016, loss 0.596954.
Train: 2018-08-05T18:58:50.561147: step 10017, loss 0.614533.
Train: 2018-08-05T18:58:50.764254: step 10018, loss 0.597131.
Train: 2018-08-05T18:58:50.967331: step 10019, loss 0.562344.
Train: 2018-08-05T18:58:51.170379: step 10020, loss 0.553716.
Test: 2018-08-05T18:58:52.217009: step 10020, loss 0.547752.
Train: 2018-08-05T18:58:52.420087: step 10021, loss 0.493426.
Train: 2018-08-05T18:58:52.623163: step 10022, loss 0.536413.
Train: 2018-08-05T18:58:52.826241: step 10023, loss 0.640483.
Train: 2018-08-05T18:58:53.013727: step 10024, loss 0.58833.
Train: 2018-08-05T18:58:53.216775: step 10025, loss 0.596839.
Train: 2018-08-05T18:58:53.419882: step 10026, loss 0.545194.
Train: 2018-08-05T18:58:53.622930: step 10027, loss 0.596468.
Train: 2018-08-05T18:58:53.826008: step 10028, loss 0.47756.
Train: 2018-08-05T18:58:54.029114: step 10029, loss 0.630202.
Train: 2018-08-05T18:58:54.232192: step 10030, loss 0.579257.
Test: 2018-08-05T18:58:55.278823: step 10030, loss 0.546986.
Train: 2018-08-05T18:58:55.481869: step 10031, loss 0.545567.
Train: 2018-08-05T18:58:55.669355: step 10032, loss 0.545627.
Train: 2018-08-05T18:58:55.872433: step 10033, loss 0.579152.
Train: 2018-08-05T18:58:56.075481: step 10034, loss 0.495531.
Train: 2018-08-05T18:58:56.278588: step 10035, loss 0.621089.
Train: 2018-08-05T18:58:56.481636: step 10036, loss 0.5624.
Train: 2018-08-05T18:58:56.669122: step 10037, loss 0.537278.
Train: 2018-08-05T18:58:56.872199: step 10038, loss 0.545621.
Train: 2018-08-05T18:58:57.075246: step 10039, loss 0.671696.
Train: 2018-08-05T18:58:57.278354: step 10040, loss 0.520537.
Test: 2018-08-05T18:58:58.324954: step 10040, loss 0.548437.
Train: 2018-08-05T18:58:58.528060: step 10041, loss 0.512215.
Train: 2018-08-05T18:58:58.731108: step 10042, loss 0.554007.
Train: 2018-08-05T18:58:58.918565: step 10043, loss 0.638097.
Train: 2018-08-05T18:58:59.121673: step 10044, loss 0.562387.
Train: 2018-08-05T18:58:59.324749: step 10045, loss 0.554006.
Train: 2018-08-05T18:58:59.527827: step 10046, loss 0.520473.
Train: 2018-08-05T18:58:59.730904: step 10047, loss 0.503545.
Train: 2018-08-05T18:58:59.933982: step 10048, loss 0.621557.
Train: 2018-08-05T18:59:00.152681: step 10049, loss 0.604721.
Train: 2018-08-05T18:59:00.355729: step 10050, loss 0.587758.
Test: 2018-08-05T18:59:01.402388: step 10050, loss 0.546695.
Train: 2018-08-05T18:59:01.605465: step 10051, loss 0.604611.
Train: 2018-08-05T18:59:01.808543: step 10052, loss 0.528727.
Train: 2018-08-05T18:59:02.011620: step 10053, loss 0.545588.
Train: 2018-08-05T18:59:02.214698: step 10054, loss 0.520399.
Train: 2018-08-05T18:59:02.417775: step 10055, loss 0.579216.
Train: 2018-08-05T18:59:02.605231: step 10056, loss 0.553936.
Train: 2018-08-05T18:59:02.808279: step 10057, loss 0.520108.
Train: 2018-08-05T18:59:03.011386: step 10058, loss 0.59631.
Train: 2018-08-05T18:59:03.230055: step 10059, loss 0.528311.
Train: 2018-08-05T18:59:03.417541: step 10060, loss 0.596509.
Test: 2018-08-05T18:59:04.479792: step 10060, loss 0.549228.
Train: 2018-08-05T18:59:04.682870: step 10061, loss 0.570892.
Train: 2018-08-05T18:59:04.885948: step 10062, loss 0.570899.
Train: 2018-08-05T18:59:05.073404: step 10063, loss 0.562335.
Train: 2018-08-05T18:59:05.276450: step 10064, loss 0.570902.
Train: 2018-08-05T18:59:05.479558: step 10065, loss 0.553772.
Train: 2018-08-05T18:59:05.682605: step 10066, loss 0.476692.
Train: 2018-08-05T18:59:05.885713: step 10067, loss 0.519288.
Train: 2018-08-05T18:59:06.088790: step 10068, loss 0.553675.
Train: 2018-08-05T18:59:06.291838: step 10069, loss 0.527442.
Train: 2018-08-05T18:59:06.479325: step 10070, loss 0.553606.
Test: 2018-08-05T18:59:07.541545: step 10070, loss 0.54785.
Train: 2018-08-05T18:59:07.744671: step 10071, loss 0.624564.
Train: 2018-08-05T18:59:07.932109: step 10072, loss 0.589124.
Train: 2018-08-05T18:59:08.135186: step 10073, loss 0.526941.
Train: 2018-08-05T18:59:08.338263: step 10074, loss 0.580277.
Train: 2018-08-05T18:59:08.541341: step 10075, loss 0.562482.
Train: 2018-08-05T18:59:08.744418: step 10076, loss 0.606905.
Train: 2018-08-05T18:59:08.947497: step 10077, loss 0.553594.
Train: 2018-08-05T18:59:09.150574: step 10078, loss 0.544786.
Train: 2018-08-05T18:59:09.338030: step 10079, loss 0.632764.
Train: 2018-08-05T18:59:09.556728: step 10080, loss 0.57106.
Test: 2018-08-05T18:59:10.603358: step 10080, loss 0.54805.
Train: 2018-08-05T18:59:10.806435: step 10081, loss 0.570891.
Train: 2018-08-05T18:59:11.009514: step 10082, loss 0.562459.
Train: 2018-08-05T18:59:11.228213: step 10083, loss 0.529932.
Train: 2018-08-05T18:59:11.431260: step 10084, loss 0.545184.
Train: 2018-08-05T18:59:11.634368: step 10085, loss 0.553608.
Train: 2018-08-05T18:59:11.837445: step 10086, loss 0.615589.
Train: 2018-08-05T18:59:12.040522: step 10087, loss 0.56235.
Train: 2018-08-05T18:59:12.243570: step 10088, loss 0.629952.
Train: 2018-08-05T18:59:12.446676: step 10089, loss 0.554014.
Train: 2018-08-05T18:59:12.649756: step 10090, loss 0.570764.
Test: 2018-08-05T18:59:13.696384: step 10090, loss 0.547881.
Train: 2018-08-05T18:59:13.899432: step 10091, loss 0.537629.
Train: 2018-08-05T18:59:14.102539: step 10092, loss 0.562501.
Train: 2018-08-05T18:59:14.305616: step 10093, loss 0.620165.
Train: 2018-08-05T18:59:14.508693: step 10094, loss 0.521624.
Train: 2018-08-05T18:59:14.711771: step 10095, loss 0.58712.
Train: 2018-08-05T18:59:14.914852: step 10096, loss 0.53815.
Train: 2018-08-05T18:59:15.117926: step 10097, loss 0.497375.
Train: 2018-08-05T18:59:15.320974: step 10098, loss 0.61993.
Train: 2018-08-05T18:59:15.524052: step 10099, loss 0.521536.
Train: 2018-08-05T18:59:15.727159: step 10100, loss 0.54604.
Test: 2018-08-05T18:59:16.789379: step 10100, loss 0.550356.
Train: 2018-08-05T18:59:17.773554: step 10101, loss 0.570757.
Train: 2018-08-05T18:59:17.976631: step 10102, loss 0.529152.
Train: 2018-08-05T18:59:18.179680: step 10103, loss 0.545645.
Train: 2018-08-05T18:59:18.382757: step 10104, loss 0.503294.
Train: 2018-08-05T18:59:18.601485: step 10105, loss 0.545286.
Train: 2018-08-05T18:59:18.804563: step 10106, loss 0.579569.
Train: 2018-08-05T18:59:19.007640: step 10107, loss 0.605779.
Train: 2018-08-05T18:59:19.226341: step 10108, loss 0.518745.
Train: 2018-08-05T18:59:19.429416: step 10109, loss 0.562394.
Train: 2018-08-05T18:59:19.648085: step 10110, loss 0.553599.
Test: 2018-08-05T18:59:20.694745: step 10110, loss 0.548669.
Train: 2018-08-05T18:59:20.897823: step 10111, loss 0.491515.
Train: 2018-08-05T18:59:21.116521: step 10112, loss 0.508894.
Train: 2018-08-05T18:59:21.319598: step 10113, loss 0.517504.
Train: 2018-08-05T18:59:21.522676: step 10114, loss 0.571896.
Train: 2018-08-05T18:59:21.725754: step 10115, loss 0.498541.
Train: 2018-08-05T18:59:21.944423: step 10116, loss 0.646645.
Train: 2018-08-05T18:59:22.116257: step 10117, loss 0.523408.
Train: 2018-08-05T18:59:22.334957: step 10118, loss 0.497797.
Train: 2018-08-05T18:59:22.538063: step 10119, loss 0.629123.
Train: 2018-08-05T18:59:22.741141: step 10120, loss 0.591533.
Test: 2018-08-05T18:59:23.803375: step 10120, loss 0.548701.
Train: 2018-08-05T18:59:24.006470: step 10121, loss 0.497675.
Train: 2018-08-05T18:59:24.209552: step 10122, loss 0.582005.
Train: 2018-08-05T18:59:24.412595: step 10123, loss 0.609936.
Train: 2018-08-05T18:59:24.615703: step 10124, loss 0.62809.
Train: 2018-08-05T18:59:24.818779: step 10125, loss 0.544521.
Train: 2018-08-05T18:59:25.021857: step 10126, loss 0.526521.
Train: 2018-08-05T18:59:25.240525: step 10127, loss 0.535534.
Train: 2018-08-05T18:59:25.443633: step 10128, loss 0.53562.
Train: 2018-08-05T18:59:25.646710: step 10129, loss 0.51779.
Train: 2018-08-05T18:59:25.849788: step 10130, loss 0.625054.
Test: 2018-08-05T18:59:26.896387: step 10130, loss 0.548048.
Train: 2018-08-05T18:59:27.099495: step 10131, loss 0.500311.
Train: 2018-08-05T18:59:27.318194: step 10132, loss 0.500434.
Train: 2018-08-05T18:59:27.521271: step 10133, loss 0.597939.
Train: 2018-08-05T18:59:27.724349: step 10134, loss 0.597863.
Train: 2018-08-05T18:59:27.943048: step 10135, loss 0.579895.
Train: 2018-08-05T18:59:28.146125: step 10136, loss 0.537653.
Train: 2018-08-05T18:59:28.349197: step 10137, loss 0.622505.
Train: 2018-08-05T18:59:28.552280: step 10138, loss 0.587753.
Train: 2018-08-05T18:59:28.770979: step 10139, loss 0.562588.
Train: 2018-08-05T18:59:28.989678: step 10140, loss 0.570853.
Test: 2018-08-05T18:59:30.036307: step 10140, loss 0.549604.
Train: 2018-08-05T18:59:30.239385: step 10141, loss 0.570888.
Train: 2018-08-05T18:59:30.442463: step 10142, loss 0.530332.
Train: 2018-08-05T18:59:30.645510: step 10143, loss 0.570809.
Train: 2018-08-05T18:59:30.848618: step 10144, loss 0.570888.
Train: 2018-08-05T18:59:31.051695: step 10145, loss 0.613545.
Train: 2018-08-05T18:59:31.254772: step 10146, loss 0.587699.
Train: 2018-08-05T18:59:31.457849: step 10147, loss 0.537451.
Train: 2018-08-05T18:59:31.676548: step 10148, loss 0.49538.
Train: 2018-08-05T18:59:31.879627: step 10149, loss 0.621152.
Train: 2018-08-05T18:59:32.082703: step 10150, loss 0.671284.
Test: 2018-08-05T18:59:33.144925: step 10150, loss 0.548771.
Train: 2018-08-05T18:59:33.348035: step 10151, loss 0.662136.
Train: 2018-08-05T18:59:33.566731: step 10152, loss 0.60355.
Train: 2018-08-05T18:59:33.769812: step 10153, loss 0.506153.
Train: 2018-08-05T18:59:33.988507: step 10154, loss 0.522766.
Train: 2018-08-05T18:59:34.191584: step 10155, loss 0.546935.
Train: 2018-08-05T18:59:34.394665: step 10156, loss 0.578859.
Train: 2018-08-05T18:59:34.597739: step 10157, loss 0.570904.
Train: 2018-08-05T18:59:34.816438: step 10158, loss 0.570913.
Train: 2018-08-05T18:59:35.019515: step 10159, loss 0.49944.
Train: 2018-08-05T18:59:35.222593: step 10160, loss 0.554923.
Test: 2018-08-05T18:59:36.284844: step 10160, loss 0.548679.
Train: 2018-08-05T18:59:36.487922: step 10161, loss 0.538746.
Train: 2018-08-05T18:59:36.706621: step 10162, loss 0.554633.
Train: 2018-08-05T18:59:36.909698: step 10163, loss 0.529967.
Train: 2018-08-05T18:59:37.112775: step 10164, loss 0.528538.
Train: 2018-08-05T18:59:37.315853: step 10165, loss 0.544819.
Train: 2018-08-05T18:59:37.534552: step 10166, loss 0.544776.
Train: 2018-08-05T18:59:37.737629: step 10167, loss 0.563301.
Train: 2018-08-05T18:59:37.940706: step 10168, loss 0.518039.
Train: 2018-08-05T18:59:38.159408: step 10169, loss 0.607023.
Train: 2018-08-05T18:59:38.378074: step 10170, loss 0.579706.
Test: 2018-08-05T18:59:39.424705: step 10170, loss 0.548174.
Train: 2018-08-05T18:59:39.627781: step 10171, loss 0.493553.
Train: 2018-08-05T18:59:39.846511: step 10172, loss 0.631486.
Train: 2018-08-05T18:59:40.049587: step 10173, loss 0.553695.
Train: 2018-08-05T18:59:40.252636: step 10174, loss 0.55369.
Train: 2018-08-05T18:59:40.455742: step 10175, loss 0.605647.
Train: 2018-08-05T18:59:40.658820: step 10176, loss 0.562341.
Train: 2018-08-05T18:59:40.877518: step 10177, loss 0.631355.
Train: 2018-08-05T18:59:41.080567: step 10178, loss 0.53661.
Train: 2018-08-05T18:59:41.299289: step 10179, loss 0.519632.
Train: 2018-08-05T18:59:41.502372: step 10180, loss 0.553806.
Test: 2018-08-05T18:59:42.564596: step 10180, loss 0.547111.
Train: 2018-08-05T18:59:42.767701: step 10181, loss 0.553809.
Train: 2018-08-05T18:59:42.970779: step 10182, loss 0.502609.
Train: 2018-08-05T18:59:43.189478: step 10183, loss 0.545203.
Train: 2018-08-05T18:59:43.392555: step 10184, loss 0.527911.
Train: 2018-08-05T18:59:43.611223: step 10185, loss 0.519009.
Train: 2018-08-05T18:59:43.829952: step 10186, loss 0.606667.
Train: 2018-08-05T18:59:44.048651: step 10187, loss 0.588652.
Train: 2018-08-05T18:59:44.251699: step 10188, loss 0.492202.
Train: 2018-08-05T18:59:44.470428: step 10189, loss 0.606519.
Train: 2018-08-05T18:59:44.673505: step 10190, loss 0.553597.
Test: 2018-08-05T18:59:45.735726: step 10190, loss 0.548277.
Train: 2018-08-05T18:59:45.938834: step 10191, loss 0.588986.
Train: 2018-08-05T18:59:46.157533: step 10192, loss 0.562437.
Train: 2018-08-05T18:59:46.360610: step 10193, loss 0.650753.
Train: 2018-08-05T18:59:46.563687: step 10194, loss 0.579937.
Train: 2018-08-05T18:59:46.766764: step 10195, loss 0.553651.
Train: 2018-08-05T18:59:46.969843: step 10196, loss 0.510432.
Train: 2018-08-05T18:59:47.188541: step 10197, loss 0.553711.
Train: 2018-08-05T18:59:47.391618: step 10198, loss 0.613984.
Train: 2018-08-05T18:59:47.594696: step 10199, loss 0.553773.
Train: 2018-08-05T18:59:47.813395: step 10200, loss 0.56234.
Test: 2018-08-05T18:59:48.860024: step 10200, loss 0.548748.
Train: 2018-08-05T18:59:49.828548: step 10201, loss 0.494393.
Train: 2018-08-05T18:59:50.047247: step 10202, loss 0.46033.
Train: 2018-08-05T18:59:50.250324: step 10203, loss 0.562336.
Train: 2018-08-05T18:59:50.453396: step 10204, loss 0.519258.
Train: 2018-08-05T18:59:50.656478: step 10205, loss 0.623155.
Train: 2018-08-05T18:59:50.875177: step 10206, loss 0.579794.
Train: 2018-08-05T18:59:51.078260: step 10207, loss 0.536175.
Train: 2018-08-05T18:59:51.281332: step 10208, loss 0.614918.
Train: 2018-08-05T18:59:51.500002: step 10209, loss 0.466148.
Train: 2018-08-05T18:59:51.703109: step 10210, loss 0.527247.
Test: 2018-08-05T18:59:52.765360: step 10210, loss 0.54747.
Train: 2018-08-05T18:59:52.968438: step 10211, loss 0.500561.
Train: 2018-08-05T18:59:53.171515: step 10212, loss 0.535761.
Train: 2018-08-05T18:59:53.390214: step 10213, loss 0.562587.
Train: 2018-08-05T18:59:53.593262: step 10214, loss 0.571721.
Train: 2018-08-05T18:59:53.811989: step 10215, loss 0.590022.
Train: 2018-08-05T18:59:54.015038: step 10216, loss 0.562762.
Train: 2018-08-05T18:59:54.218115: step 10217, loss 0.526291.
Train: 2018-08-05T18:59:54.436814: step 10218, loss 0.52625.
Train: 2018-08-05T18:59:54.639924: step 10219, loss 0.535354.
Train: 2018-08-05T18:59:54.858620: step 10220, loss 0.590484.
Test: 2018-08-05T18:59:55.905249: step 10220, loss 0.549028.
Train: 2018-08-05T18:59:56.123919: step 10221, loss 0.581297.
Train: 2018-08-05T18:59:56.326998: step 10222, loss 0.67302.
Train: 2018-08-05T18:59:56.530103: step 10223, loss 0.517251.
Train: 2018-08-05T18:59:56.748803: step 10224, loss 0.571691.
Train: 2018-08-05T18:59:56.951879: step 10225, loss 0.589497.
Train: 2018-08-05T18:59:57.170579: step 10226, loss 0.517994.
Train: 2018-08-05T18:59:57.389278: step 10227, loss 0.535899.
Train: 2018-08-05T18:59:57.592355: step 10228, loss 0.535974.
Train: 2018-08-05T18:59:57.811054: step 10229, loss 0.491833.
Train: 2018-08-05T18:59:58.014131: step 10230, loss 0.553731.
Test: 2018-08-05T18:59:59.060731: step 10230, loss 0.548129.
Train: 2018-08-05T18:59:59.279460: step 10231, loss 0.56349.
Train: 2018-08-05T18:59:59.482507: step 10232, loss 0.617455.
Train: 2018-08-05T18:59:59.701206: step 10233, loss 0.544859.
Train: 2018-08-05T18:59:59.904313: step 10234, loss 0.544917.
Train: 2018-08-05T19:00:00.123012: step 10235, loss 0.588464.
Train: 2018-08-05T19:00:00.326089: step 10236, loss 0.640337.
Train: 2018-08-05T19:00:00.544788: step 10237, loss 0.613881.
Train: 2018-08-05T19:00:00.747866: step 10238, loss 0.562346.
Train: 2018-08-05T19:00:00.950943: step 10239, loss 0.56238.
Train: 2018-08-05T19:00:01.169642: step 10240, loss 0.637477.
Test: 2018-08-05T19:00:02.231864: step 10240, loss 0.548195.
Train: 2018-08-05T19:00:02.434971: step 10241, loss 0.496626.
Train: 2018-08-05T19:00:02.653669: step 10242, loss 0.53803.
Train: 2018-08-05T19:00:02.856747: step 10243, loss 0.619697.
Train: 2018-08-05T19:00:03.075446: step 10244, loss 0.587.
Train: 2018-08-05T19:00:03.278493: step 10245, loss 0.554729.
Train: 2018-08-05T19:00:03.497222: step 10246, loss 0.626938.
Train: 2018-08-05T19:00:03.700299: step 10247, loss 0.586811.
Train: 2018-08-05T19:00:03.918998: step 10248, loss 0.563075.
Train: 2018-08-05T19:00:04.137697: step 10249, loss 0.555323.
Train: 2018-08-05T19:00:04.356366: step 10250, loss 0.571058.
Test: 2018-08-05T19:00:05.402996: step 10250, loss 0.550784.
Train: 2018-08-05T19:00:05.668588: step 10251, loss 0.532061.
Train: 2018-08-05T19:00:05.871666: step 10252, loss 0.563258.
Train: 2018-08-05T19:00:06.090365: step 10253, loss 0.571048.
Train: 2018-08-05T19:00:06.293443: step 10254, loss 0.539612.
Train: 2018-08-05T19:00:06.512111: step 10255, loss 0.578863.
Train: 2018-08-05T19:00:06.715218: step 10256, loss 0.539216.
Train: 2018-08-05T19:00:06.918296: step 10257, loss 0.514945.
Train: 2018-08-05T19:00:07.136965: step 10258, loss 0.504546.
Train: 2018-08-05T19:00:07.340071: step 10259, loss 0.553708.
Train: 2018-08-05T19:00:07.558741: step 10260, loss 0.676306.
Test: 2018-08-05T19:00:08.605401: step 10260, loss 0.545664.
Train: 2018-08-05T19:00:08.824099: step 10261, loss 0.553632.
Train: 2018-08-05T19:00:09.027176: step 10262, loss 0.579524.
Train: 2018-08-05T19:00:09.245846: step 10263, loss 0.579089.
Train: 2018-08-05T19:00:09.448958: step 10264, loss 0.545997.
Train: 2018-08-05T19:00:09.652031: step 10265, loss 0.6038.
Train: 2018-08-05T19:00:09.870730: step 10266, loss 0.504788.
Train: 2018-08-05T19:00:10.073807: step 10267, loss 0.58729.
Train: 2018-08-05T19:00:10.245612: step 10268, loss 0.491859.
Train: 2018-08-05T19:00:10.464340: step 10269, loss 0.537463.
Train: 2018-08-05T19:00:10.667417: step 10270, loss 0.545625.
Test: 2018-08-05T19:00:11.729669: step 10270, loss 0.549222.
Train: 2018-08-05T19:00:11.948368: step 10271, loss 0.545461.
Train: 2018-08-05T19:00:12.151446: step 10272, loss 0.57938.
Train: 2018-08-05T19:00:12.354524: step 10273, loss 0.553762.
Train: 2018-08-05T19:00:12.573192: step 10274, loss 0.570964.
Train: 2018-08-05T19:00:12.791890: step 10275, loss 0.579682.
Train: 2018-08-05T19:00:12.994968: step 10276, loss 0.597134.
Train: 2018-08-05T19:00:13.213696: step 10277, loss 0.562356.
Train: 2018-08-05T19:00:13.432395: step 10278, loss 0.492757.
Train: 2018-08-05T19:00:13.635443: step 10279, loss 0.579841.
Train: 2018-08-05T19:00:13.854143: step 10280, loss 0.667446.
Test: 2018-08-05T19:00:14.900796: step 10280, loss 0.546603.
Train: 2018-08-05T19:00:15.119471: step 10281, loss 0.640798.
Train: 2018-08-05T19:00:15.322578: step 10282, loss 0.553703.
Train: 2018-08-05T19:00:15.541277: step 10283, loss 0.622311.
Train: 2018-08-05T19:00:15.759975: step 10284, loss 0.630159.
Train: 2018-08-05T19:00:15.963052: step 10285, loss 0.495538.
Train: 2018-08-05T19:00:16.181722: step 10286, loss 0.603927.
Train: 2018-08-05T19:00:16.384829: step 10287, loss 0.554331.
Train: 2018-08-05T19:00:16.587906: step 10288, loss 0.578928.
Train: 2018-08-05T19:00:16.806576: step 10289, loss 0.530325.
Train: 2018-08-05T19:00:17.009682: step 10290, loss 0.522423.
Test: 2018-08-05T19:00:18.071904: step 10290, loss 0.549463.
Train: 2018-08-05T19:00:18.274981: step 10291, loss 0.474017.
Train: 2018-08-05T19:00:18.478089: step 10292, loss 0.570787.
Train: 2018-08-05T19:00:18.696787: step 10293, loss 0.611623.
Train: 2018-08-05T19:00:18.899859: step 10294, loss 0.546177.
Train: 2018-08-05T19:00:19.118564: step 10295, loss 0.529602.
Train: 2018-08-05T19:00:19.321641: step 10296, loss 0.537615.
Train: 2018-08-05T19:00:19.540340: step 10297, loss 0.512303.
Train: 2018-08-05T19:00:19.743417: step 10298, loss 0.503258.
Train: 2018-08-05T19:00:19.946495: step 10299, loss 0.570899.
Train: 2018-08-05T19:00:20.165194: step 10300, loss 0.545012.
Test: 2018-08-05T19:00:21.211824: step 10300, loss 0.546933.
Train: 2018-08-05T19:00:22.242832: step 10301, loss 0.536076.
Train: 2018-08-05T19:00:22.461532: step 10302, loss 0.571351.
Train: 2018-08-05T19:00:22.680199: step 10303, loss 0.571526.
Train: 2018-08-05T19:00:22.883308: step 10304, loss 0.517486.
Train: 2018-08-05T19:00:23.102006: step 10305, loss 0.617445.
Train: 2018-08-05T19:00:23.305054: step 10306, loss 0.571935.
Train: 2018-08-05T19:00:23.508160: step 10307, loss 0.535389.
Train: 2018-08-05T19:00:23.726859: step 10308, loss 0.562819.
Train: 2018-08-05T19:00:23.929937: step 10309, loss 0.57196.
Train: 2018-08-05T19:00:24.148636: step 10310, loss 0.562783.
Test: 2018-08-05T19:00:25.195237: step 10310, loss 0.547962.
Train: 2018-08-05T19:00:25.413964: step 10311, loss 0.499044.
Train: 2018-08-05T19:00:25.617042: step 10312, loss 0.544538.
Train: 2018-08-05T19:00:25.835741: step 10313, loss 0.571878.
Train: 2018-08-05T19:00:26.038818: step 10314, loss 0.562751.
Train: 2018-08-05T19:00:26.257518: step 10315, loss 0.553636.
Train: 2018-08-05T19:00:26.460595: step 10316, loss 0.589915.
Train: 2018-08-05T19:00:26.663674: step 10317, loss 0.535545.
Train: 2018-08-05T19:00:26.882340: step 10318, loss 0.580616.
Train: 2018-08-05T19:00:27.085448: step 10319, loss 0.526701.
Train: 2018-08-05T19:00:27.288496: step 10320, loss 0.589352.
Test: 2018-08-05T19:00:28.350776: step 10320, loss 0.548035.
Train: 2018-08-05T19:00:28.553854: step 10321, loss 0.589188.
Train: 2018-08-05T19:00:28.772553: step 10322, loss 0.588972.
Train: 2018-08-05T19:00:28.991252: step 10323, loss 0.606275.
Train: 2018-08-05T19:00:29.194329: step 10324, loss 0.501521.
Train: 2018-08-05T19:00:29.397408: step 10325, loss 0.527767.
Train: 2018-08-05T19:00:29.616105: step 10326, loss 0.50201.
Train: 2018-08-05T19:00:29.819188: step 10327, loss 0.60547.
Train: 2018-08-05T19:00:30.022261: step 10328, loss 0.553726.
Train: 2018-08-05T19:00:30.256584: step 10329, loss 0.527937.
Train: 2018-08-05T19:00:30.459658: step 10330, loss 0.536513.
Test: 2018-08-05T19:00:31.506287: step 10330, loss 0.54773.
Train: 2018-08-05T19:00:31.724987: step 10331, loss 0.536455.
Train: 2018-08-05T19:00:31.928034: step 10332, loss 0.605637.
Train: 2018-08-05T19:00:32.131142: step 10333, loss 0.648963.
Train: 2018-08-05T19:00:32.349810: step 10334, loss 0.527871.
Train: 2018-08-05T19:00:32.552918: step 10335, loss 0.656859.
Train: 2018-08-05T19:00:32.755996: step 10336, loss 0.562341.
Train: 2018-08-05T19:00:32.974664: step 10337, loss 0.52008.
Train: 2018-08-05T19:00:33.193393: step 10338, loss 0.61291.
Train: 2018-08-05T19:00:33.396471: step 10339, loss 0.562407.
Train: 2018-08-05T19:00:33.599547: step 10340, loss 0.529164.
Test: 2018-08-05T19:00:34.661769: step 10340, loss 0.548603.
Train: 2018-08-05T19:00:34.864876: step 10341, loss 0.603948.
Train: 2018-08-05T19:00:35.067954: step 10342, loss 0.537719.
Train: 2018-08-05T19:00:35.271031: step 10343, loss 0.53779.
Train: 2018-08-05T19:00:35.474109: step 10344, loss 0.554271.
Train: 2018-08-05T19:00:35.692807: step 10345, loss 0.587261.
Train: 2018-08-05T19:00:35.895884: step 10346, loss 0.512994.
Train: 2018-08-05T19:00:36.098932: step 10347, loss 0.529351.
Train: 2018-08-05T19:00:36.317661: step 10348, loss 0.52912.
Train: 2018-08-05T19:00:36.520709: step 10349, loss 0.528823.
Train: 2018-08-05T19:00:36.723816: step 10350, loss 0.579293.
Test: 2018-08-05T19:00:37.786039: step 10350, loss 0.548092.
Train: 2018-08-05T19:00:37.989114: step 10351, loss 0.596463.
Train: 2018-08-05T19:00:38.192222: step 10352, loss 0.545198.
Train: 2018-08-05T19:00:38.395299: step 10353, loss 0.57956.
Train: 2018-08-05T19:00:38.613999: step 10354, loss 0.527779.
Train: 2018-08-05T19:00:38.817081: step 10355, loss 0.579716.
Train: 2018-08-05T19:00:39.035775: step 10356, loss 0.649464.
Train: 2018-08-05T19:00:39.238852: step 10357, loss 0.536293.
Train: 2018-08-05T19:00:39.441929: step 10358, loss 0.597053.
Train: 2018-08-05T19:00:39.645008: step 10359, loss 0.519107.
Train: 2018-08-05T19:00:39.848088: step 10360, loss 0.588267.
Test: 2018-08-05T19:00:40.910305: step 10360, loss 0.547738.
Train: 2018-08-05T19:00:41.113413: step 10361, loss 0.588205.
Train: 2018-08-05T19:00:41.316496: step 10362, loss 0.553745.
Train: 2018-08-05T19:00:41.535189: step 10363, loss 0.510944.
Train: 2018-08-05T19:00:41.753857: step 10364, loss 0.622322.
Train: 2018-08-05T19:00:41.956966: step 10365, loss 0.528172.
Train: 2018-08-05T19:00:42.160043: step 10366, loss 0.528202.
Train: 2018-08-05T19:00:42.363120: step 10367, loss 0.502519.
Train: 2018-08-05T19:00:42.581819: step 10368, loss 0.562335.
Train: 2018-08-05T19:00:42.784897: step 10369, loss 0.553713.
Train: 2018-08-05T19:00:42.987974: step 10370, loss 0.596988.
Test: 2018-08-05T19:00:44.050196: step 10370, loss 0.548061.
Train: 2018-08-05T19:00:44.253273: step 10371, loss 0.579696.
Train: 2018-08-05T19:00:44.456381: step 10372, loss 0.614397.
Train: 2018-08-05T19:00:44.659458: step 10373, loss 0.588234.
Train: 2018-08-05T19:00:44.878157: step 10374, loss 0.5876.
Train: 2018-08-05T19:00:45.081237: step 10375, loss 0.54082.
Train: 2018-08-05T19:00:45.284311: step 10376, loss 0.530138.
Train: 2018-08-05T19:00:45.487389: step 10377, loss 0.570858.
Train: 2018-08-05T19:00:45.690466: step 10378, loss 0.587986.
Train: 2018-08-05T19:00:45.893544: step 10379, loss 0.562338.
Train: 2018-08-05T19:00:46.112243: step 10380, loss 0.511174.
Test: 2018-08-05T19:00:47.158872: step 10380, loss 0.546886.
Train: 2018-08-05T19:00:47.361950: step 10381, loss 0.476885.
Train: 2018-08-05T19:00:47.580649: step 10382, loss 0.527912.
Train: 2018-08-05T19:00:47.783726: step 10383, loss 0.614417.
Train: 2018-08-05T19:00:47.986774: step 10384, loss 0.562361.
Train: 2018-08-05T19:00:48.189881: step 10385, loss 0.588605.
Train: 2018-08-05T19:00:48.392959: step 10386, loss 0.597391.
Train: 2018-08-05T19:00:48.596036: step 10387, loss 0.562358.
Train: 2018-08-05T19:00:48.799083: step 10388, loss 0.570921.
Train: 2018-08-05T19:00:49.002191: step 10389, loss 0.570784.
Train: 2018-08-05T19:00:49.205238: step 10390, loss 0.562752.
Test: 2018-08-05T19:00:50.267519: step 10390, loss 0.547876.
Train: 2018-08-05T19:00:50.470597: step 10391, loss 0.545255.
Train: 2018-08-05T19:00:50.673675: step 10392, loss 0.562403.
Train: 2018-08-05T19:00:50.892373: step 10393, loss 0.51888.
Train: 2018-08-05T19:00:51.095450: step 10394, loss 0.579792.
Train: 2018-08-05T19:00:51.298528: step 10395, loss 0.536199.
Train: 2018-08-05T19:00:51.501605: step 10396, loss 0.579851.
Train: 2018-08-05T19:00:51.704652: step 10397, loss 0.57986.
Train: 2018-08-05T19:00:51.907763: step 10398, loss 0.60604.
Train: 2018-08-05T19:00:52.110837: step 10399, loss 0.605858.
Train: 2018-08-05T19:00:52.313920: step 10400, loss 0.51047.
Test: 2018-08-05T19:00:53.376137: step 10400, loss 0.548336.
Train: 2018-08-05T19:00:54.340579: step 10401, loss 0.51923.
Train: 2018-08-05T19:00:54.543656: step 10402, loss 0.553716.
Train: 2018-08-05T19:00:54.746735: step 10403, loss 0.605464.
Train: 2018-08-05T19:00:54.965432: step 10404, loss 0.527917.
Train: 2018-08-05T19:00:55.168509: step 10405, loss 0.605352.
Train: 2018-08-05T19:00:55.371587: step 10406, loss 0.570914.
Train: 2018-08-05T19:00:55.574664: step 10407, loss 0.545234.
Train: 2018-08-05T19:00:55.777742: step 10408, loss 0.613553.
Train: 2018-08-05T19:00:55.980818: step 10409, loss 0.553852.
Train: 2018-08-05T19:00:56.199488: step 10410, loss 0.536967.
Test: 2018-08-05T19:00:57.246148: step 10410, loss 0.548834.
Train: 2018-08-05T19:00:57.449226: step 10411, loss 0.553912.
Train: 2018-08-05T19:00:57.667918: step 10412, loss 0.579257.
Train: 2018-08-05T19:00:57.870972: step 10413, loss 0.553943.
Train: 2018-08-05T19:00:58.074078: step 10414, loss 0.545531.
Train: 2018-08-05T19:00:58.277157: step 10415, loss 0.545519.
Train: 2018-08-05T19:00:58.480238: step 10416, loss 0.613016.
Train: 2018-08-05T19:00:58.683281: step 10417, loss 0.570801.
Train: 2018-08-05T19:00:58.886389: step 10418, loss 0.553962.
Train: 2018-08-05T19:00:59.058223: step 10419, loss 0.49062.
Train: 2018-08-05T19:00:59.261271: step 10420, loss 0.503259.
Test: 2018-08-05T19:01:00.307901: step 10420, loss 0.547353.
Train: 2018-08-05T19:01:00.511008: step 10421, loss 0.570851.
Train: 2018-08-05T19:01:00.714086: step 10422, loss 0.510973.
Train: 2018-08-05T19:01:00.917164: step 10423, loss 0.64006.
Train: 2018-08-05T19:01:01.120241: step 10424, loss 0.449752.
Train: 2018-08-05T19:01:01.338939: step 10425, loss 0.501183.
Train: 2018-08-05T19:01:01.526395: step 10426, loss 0.580127.
Train: 2018-08-05T19:01:01.745094: step 10427, loss 0.580357.
Train: 2018-08-05T19:01:01.948173: step 10428, loss 0.562574.
Train: 2018-08-05T19:01:02.151249: step 10429, loss 0.598727.
Train: 2018-08-05T19:01:02.354326: step 10430, loss 0.607842.
Test: 2018-08-05T19:01:03.400956: step 10430, loss 0.546731.
Train: 2018-08-05T19:01:03.604033: step 10431, loss 0.580666.
Train: 2018-08-05T19:01:03.822733: step 10432, loss 0.607521.
Train: 2018-08-05T19:01:04.025810: step 10433, loss 0.508951.
Train: 2018-08-05T19:01:04.213266: step 10434, loss 0.491307.
Train: 2018-08-05T19:01:04.431965: step 10435, loss 0.571393.
Train: 2018-08-05T19:01:04.635012: step 10436, loss 0.562485.
Train: 2018-08-05T19:01:04.853712: step 10437, loss 0.473608.
Train: 2018-08-05T19:01:05.056818: step 10438, loss 0.553588.
Train: 2018-08-05T19:01:05.259896: step 10439, loss 0.55359.
Train: 2018-08-05T19:01:05.462974: step 10440, loss 0.454883.
Test: 2018-08-05T19:01:06.509574: step 10440, loss 0.546929.
Train: 2018-08-05T19:01:06.728307: step 10441, loss 0.535522.
Train: 2018-08-05T19:01:06.931380: step 10442, loss 0.590133.
Train: 2018-08-05T19:01:07.134457: step 10443, loss 0.526193.
Train: 2018-08-05T19:01:07.337535: step 10444, loss 0.572148.
Train: 2018-08-05T19:01:07.540612: step 10445, loss 0.489056.
Train: 2018-08-05T19:01:07.743659: step 10446, loss 0.507322.
Train: 2018-08-05T19:01:07.946768: step 10447, loss 0.563261.
Train: 2018-08-05T19:01:08.149844: step 10448, loss 0.54455.
Train: 2018-08-05T19:01:08.352921: step 10449, loss 0.572945.
Train: 2018-08-05T19:01:08.555999: step 10450, loss 0.58249.
Test: 2018-08-05T19:01:09.618250: step 10450, loss 0.549666.
Train: 2018-08-05T19:01:09.821329: step 10451, loss 0.487764.
Train: 2018-08-05T19:01:10.008784: step 10452, loss 0.620508.
Train: 2018-08-05T19:01:10.211861: step 10453, loss 0.544576.
Train: 2018-08-05T19:01:10.414940: step 10454, loss 0.54456.
Train: 2018-08-05T19:01:10.618016: step 10455, loss 0.553954.
Train: 2018-08-05T19:01:10.821094: step 10456, loss 0.507034.
Train: 2018-08-05T19:01:11.024172: step 10457, loss 0.563252.
Train: 2018-08-05T19:01:11.227219: step 10458, loss 0.581876.
Train: 2018-08-05T19:01:11.430326: step 10459, loss 0.535216.
Train: 2018-08-05T19:01:11.633404: step 10460, loss 0.516724.
Test: 2018-08-05T19:01:12.680005: step 10460, loss 0.547346.
Train: 2018-08-05T19:01:12.883110: step 10461, loss 0.599974.
Train: 2018-08-05T19:01:13.086188: step 10462, loss 0.526115.
Train: 2018-08-05T19:01:13.289236: step 10463, loss 0.53535.
Train: 2018-08-05T19:01:13.492343: step 10464, loss 0.635977.
Train: 2018-08-05T19:01:13.695420: step 10465, loss 0.471929.
Train: 2018-08-05T19:01:13.898498: step 10466, loss 0.562677.
Train: 2018-08-05T19:01:14.101546: step 10467, loss 0.526515.
Train: 2018-08-05T19:01:14.304623: step 10468, loss 0.490465.
Train: 2018-08-05T19:01:14.507731: step 10469, loss 0.499366.
Train: 2018-08-05T19:01:14.710808: step 10470, loss 0.644497.
Test: 2018-08-05T19:01:15.757408: step 10470, loss 0.54692.
Train: 2018-08-05T19:01:15.976136: step 10471, loss 0.580856.
Train: 2018-08-05T19:01:16.163593: step 10472, loss 0.598863.
Train: 2018-08-05T19:01:16.366670: step 10473, loss 0.5446.
Train: 2018-08-05T19:01:16.569749: step 10474, loss 0.553591.
Train: 2018-08-05T19:01:16.772825: step 10475, loss 0.607094.
Train: 2018-08-05T19:01:16.975902: step 10476, loss 0.54474.
Train: 2018-08-05T19:01:17.178980: step 10477, loss 0.544805.
Train: 2018-08-05T19:01:17.382028: step 10478, loss 0.536104.
Train: 2018-08-05T19:01:17.585135: step 10479, loss 0.509971.
Train: 2018-08-05T19:01:17.788182: step 10480, loss 0.614777.
Test: 2018-08-05T19:01:18.834842: step 10480, loss 0.546818.
Train: 2018-08-05T19:01:19.037920: step 10481, loss 0.536244.
Train: 2018-08-05T19:01:19.256618: step 10482, loss 0.510215.
Train: 2018-08-05T19:01:19.459697: step 10483, loss 0.614566.
Train: 2018-08-05T19:01:19.678394: step 10484, loss 0.588398.
Train: 2018-08-05T19:01:19.865851: step 10485, loss 0.545042.
Train: 2018-08-05T19:01:20.068899: step 10486, loss 0.665872.
Train: 2018-08-05T19:01:20.271976: step 10487, loss 0.596547.
Train: 2018-08-05T19:01:20.475083: step 10488, loss 0.553888.
Train: 2018-08-05T19:01:20.678160: step 10489, loss 0.553993.
Train: 2018-08-05T19:01:20.881238: step 10490, loss 0.562427.
Test: 2018-08-05T19:01:21.943459: step 10490, loss 0.548622.
Train: 2018-08-05T19:01:22.146568: step 10491, loss 0.587338.
Train: 2018-08-05T19:01:22.349644: step 10492, loss 0.595467.
Train: 2018-08-05T19:01:22.537071: step 10493, loss 0.546241.
Train: 2018-08-05T19:01:22.740171: step 10494, loss 0.554513.
Train: 2018-08-05T19:01:22.958876: step 10495, loss 0.522153.
Train: 2018-08-05T19:01:23.161954: step 10496, loss 0.603229.
Train: 2018-08-05T19:01:23.365032: step 10497, loss 0.546512.
Train: 2018-08-05T19:01:23.568108: step 10498, loss 0.51411.
Train: 2018-08-05T19:01:23.786808: step 10499, loss 0.60331.
Train: 2018-08-05T19:01:23.989885: step 10500, loss 0.473019.
Test: 2018-08-05T19:01:25.036514: step 10500, loss 0.548455.
Train: 2018-08-05T19:01:25.989416: step 10501, loss 0.546124.
Train: 2018-08-05T19:01:26.192495: step 10502, loss 0.587326.
Train: 2018-08-05T19:01:26.395571: step 10503, loss 0.595787.
Train: 2018-08-05T19:01:26.598649: step 10504, loss 0.612655.
Train: 2018-08-05T19:01:26.786075: step 10505, loss 0.562396.
Train: 2018-08-05T19:01:26.989183: step 10506, loss 0.595961.
Train: 2018-08-05T19:01:27.192260: step 10507, loss 0.55401.
Train: 2018-08-05T19:01:27.395307: step 10508, loss 0.612706.
Train: 2018-08-05T19:01:27.598414: step 10509, loss 0.620936.
Train: 2018-08-05T19:01:27.801493: step 10510, loss 0.57076.
Test: 2018-08-05T19:01:28.848093: step 10510, loss 0.549452.
Train: 2018-08-05T19:01:29.051200: step 10511, loss 0.521173.
Train: 2018-08-05T19:01:29.254248: step 10512, loss 0.628491.
Train: 2018-08-05T19:01:29.472976: step 10513, loss 0.537951.
Train: 2018-08-05T19:01:29.676053: step 10514, loss 0.587125.
Train: 2018-08-05T19:01:29.879126: step 10515, loss 0.611528.
Train: 2018-08-05T19:01:30.082177: step 10516, loss 0.587003.
Train: 2018-08-05T19:01:30.285256: step 10517, loss 0.578878.
Train: 2018-08-05T19:01:30.488363: step 10518, loss 0.610904.
Train: 2018-08-05T19:01:30.691441: step 10519, loss 0.547057.
Train: 2018-08-05T19:01:30.878897: step 10520, loss 0.53138.
Test: 2018-08-05T19:01:31.941148: step 10520, loss 0.550031.
Train: 2018-08-05T19:01:32.144225: step 10521, loss 0.594671.
Train: 2018-08-05T19:01:32.347307: step 10522, loss 0.586752.
Train: 2018-08-05T19:01:32.550380: step 10523, loss 0.539513.
Train: 2018-08-05T19:01:32.753428: step 10524, loss 0.555239.
Train: 2018-08-05T19:01:32.940914: step 10525, loss 0.51572.
Train: 2018-08-05T19:01:33.143961: step 10526, loss 0.562975.
Train: 2018-08-05T19:01:33.347068: step 10527, loss 0.522904.
Train: 2018-08-05T19:01:33.550146: step 10528, loss 0.570814.
Train: 2018-08-05T19:01:33.753193: step 10529, loss 0.58706.
Train: 2018-08-05T19:01:33.956301: step 10530, loss 0.562567.
Test: 2018-08-05T19:01:35.002930: step 10530, loss 0.548154.
Train: 2018-08-05T19:01:35.206008: step 10531, loss 0.53775.
Train: 2018-08-05T19:01:35.409086: step 10532, loss 0.620673.
Train: 2018-08-05T19:01:35.612163: step 10533, loss 0.570769.
Train: 2018-08-05T19:01:35.815241: step 10534, loss 0.495366.
Train: 2018-08-05T19:01:36.002697: step 10535, loss 0.520156.
Train: 2018-08-05T19:01:36.205743: step 10536, loss 0.511207.
Train: 2018-08-05T19:01:36.408852: step 10537, loss 0.493357.
Train: 2018-08-05T19:01:36.611900: step 10538, loss 0.571126.
Train: 2018-08-05T19:01:36.877463: step 10539, loss 0.535882.
Train: 2018-08-05T19:01:37.080539: step 10540, loss 0.553592.
Test: 2018-08-05T19:01:38.127170: step 10540, loss 0.547759.
Train: 2018-08-05T19:01:38.330277: step 10541, loss 0.598904.
Train: 2018-08-05T19:01:38.533325: step 10542, loss 0.526303.
Train: 2018-08-05T19:01:38.736432: step 10543, loss 0.572056.
Train: 2018-08-05T19:01:38.939509: step 10544, loss 0.562958.
Train: 2018-08-05T19:01:39.142588: step 10545, loss 0.57227.
Train: 2018-08-05T19:01:39.345668: step 10546, loss 0.627892.
Train: 2018-08-05T19:01:39.548742: step 10547, loss 0.581428.
Train: 2018-08-05T19:01:39.751819: step 10548, loss 0.636299.
Train: 2018-08-05T19:01:39.954897: step 10549, loss 0.580886.
Train: 2018-08-05T19:01:40.157976: step 10550, loss 0.571561.
Test: 2018-08-05T19:01:41.204604: step 10550, loss 0.548252.
Train: 2018-08-05T19:01:41.423303: step 10551, loss 0.660156.
Train: 2018-08-05T19:01:41.626350: step 10552, loss 0.623503.
Train: 2018-08-05T19:01:41.813837: step 10553, loss 0.528066.
Train: 2018-08-05T19:01:42.016913: step 10554, loss 0.596115.
Train: 2018-08-05T19:01:42.219961: step 10555, loss 0.554151.
Train: 2018-08-05T19:01:42.438660: step 10556, loss 0.554377.
Train: 2018-08-05T19:01:42.626146: step 10557, loss 0.481667.
Train: 2018-08-05T19:01:42.829228: step 10558, loss 0.619234.
Train: 2018-08-05T19:01:43.032271: step 10559, loss 0.626972.
Train: 2018-08-05T19:01:43.235348: step 10560, loss 0.53914.
Test: 2018-08-05T19:01:44.282008: step 10560, loss 0.549695.
Train: 2018-08-05T19:01:44.485056: step 10561, loss 0.570964.
Train: 2018-08-05T19:01:44.688163: step 10562, loss 0.68106.
Train: 2018-08-05T19:01:44.891240: step 10563, loss 0.547806.
Train: 2018-08-05T19:01:45.094323: step 10564, loss 0.540364.
Train: 2018-08-05T19:01:45.297366: step 10565, loss 0.532853.
Train: 2018-08-05T19:01:45.500473: step 10566, loss 0.571281.
Train: 2018-08-05T19:01:45.703550: step 10567, loss 0.571269.
Train: 2018-08-05T19:01:45.906627: step 10568, loss 0.602066.
Train: 2018-08-05T19:01:46.109705: step 10569, loss 0.525.
Train: 2018-08-05T19:01:46.281540: step 10570, loss 0.662516.
Test: 2018-08-05T19:01:47.328139: step 10570, loss 0.550217.
Train: 2018-08-05T19:01:47.531217: step 10571, loss 0.563479.
Train: 2018-08-05T19:01:47.734324: step 10572, loss 0.586662.
Train: 2018-08-05T19:01:47.937403: step 10573, loss 0.586661.
Train: 2018-08-05T19:01:48.140480: step 10574, loss 0.532611.
Train: 2018-08-05T19:01:48.343551: step 10575, loss 0.609901.
Train: 2018-08-05T19:01:48.546635: step 10576, loss 0.58667.
Train: 2018-08-05T19:01:48.749711: step 10577, loss 0.602183.
Train: 2018-08-05T19:01:48.952790: step 10578, loss 0.547923.
Train: 2018-08-05T19:01:49.155867: step 10579, loss 0.648767.
Train: 2018-08-05T19:01:49.358944: step 10580, loss 0.563451.
Test: 2018-08-05T19:01:50.421195: step 10580, loss 0.550931.
Train: 2018-08-05T19:01:50.624272: step 10581, loss 0.548026.
Train: 2018-08-05T19:01:50.827350: step 10582, loss 0.517041.
Train: 2018-08-05T19:01:51.030427: step 10583, loss 0.610017.
Train: 2018-08-05T19:01:51.233506: step 10584, loss 0.594496.
Train: 2018-08-05T19:01:51.436583: step 10585, loss 0.555434.
Train: 2018-08-05T19:01:51.639663: step 10586, loss 0.539657.
Train: 2018-08-05T19:01:51.842732: step 10587, loss 0.594642.
Train: 2018-08-05T19:01:52.045815: step 10588, loss 0.539249.
Train: 2018-08-05T19:01:52.248892: step 10589, loss 0.531025.
Train: 2018-08-05T19:01:52.451940: step 10590, loss 0.570831.
Test: 2018-08-05T19:01:53.514221: step 10590, loss 0.550078.
Train: 2018-08-05T19:01:53.717303: step 10591, loss 0.554577.
Train: 2018-08-05T19:01:53.935997: step 10592, loss 0.636184.
Train: 2018-08-05T19:01:54.123453: step 10593, loss 0.529732.
Train: 2018-08-05T19:01:54.342147: step 10594, loss 0.562503.
Train: 2018-08-05T19:01:54.545200: step 10595, loss 0.496055.
Train: 2018-08-05T19:01:54.748277: step 10596, loss 0.554016.
Train: 2018-08-05T19:01:54.951385: step 10597, loss 0.460792.
Train: 2018-08-05T19:01:55.170083: step 10598, loss 0.562367.
Train: 2018-08-05T19:01:55.373131: step 10599, loss 0.535551.
Train: 2018-08-05T19:01:55.576239: step 10600, loss 0.584142.
Test: 2018-08-05T19:01:56.622869: step 10600, loss 0.547165.
Train: 2018-08-05T19:01:57.622634: step 10601, loss 0.56317.
Train: 2018-08-05T19:01:57.810090: step 10602, loss 0.481644.
Train: 2018-08-05T19:01:58.028788: step 10603, loss 0.571634.
Train: 2018-08-05T19:01:58.231837: step 10604, loss 0.499162.
Train: 2018-08-05T19:01:58.434943: step 10605, loss 0.590326.
Train: 2018-08-05T19:01:58.638022: step 10606, loss 0.562936.
Train: 2018-08-05T19:01:58.841098: step 10607, loss 0.535259.
Train: 2018-08-05T19:01:59.044176: step 10608, loss 0.525936.
Train: 2018-08-05T19:01:59.247253: step 10609, loss 0.535189.
Train: 2018-08-05T19:01:59.450331: step 10610, loss 0.572644.
Test: 2018-08-05T19:02:00.512553: step 10610, loss 0.547205.
Train: 2018-08-05T19:02:00.700038: step 10611, loss 0.516365.
Train: 2018-08-05T19:02:00.903116: step 10612, loss 0.572817.
Train: 2018-08-05T19:02:01.121786: step 10613, loss 0.535127.
Train: 2018-08-05T19:02:01.324894: step 10614, loss 0.648409.
Train: 2018-08-05T19:02:01.527970: step 10615, loss 0.535147.
Train: 2018-08-05T19:02:01.746668: step 10616, loss 0.553873.
Train: 2018-08-05T19:02:01.949746: step 10617, loss 0.544513.
Train: 2018-08-05T19:02:02.152825: step 10618, loss 0.525983.
Train: 2018-08-05T19:02:02.355901: step 10619, loss 0.544509.
Train: 2018-08-05T19:02:02.558980: step 10620, loss 0.55372.
Test: 2018-08-05T19:02:03.605609: step 10620, loss 0.547967.
Train: 2018-08-05T19:02:03.824312: step 10621, loss 0.599597.
Train: 2018-08-05T19:02:04.027353: step 10622, loss 0.544531.
Train: 2018-08-05T19:02:04.230461: step 10623, loss 0.571787.
Train: 2018-08-05T19:02:04.433539: step 10624, loss 0.544583.
Train: 2018-08-05T19:02:04.636586: step 10625, loss 0.544618.
Train: 2018-08-05T19:02:04.839663: step 10626, loss 0.580402.
Train: 2018-08-05T19:02:05.058394: step 10627, loss 0.571364.
Train: 2018-08-05T19:02:05.261470: step 10628, loss 0.597761.
Train: 2018-08-05T19:02:05.464518: step 10629, loss 0.606186.
Train: 2018-08-05T19:02:05.667625: step 10630, loss 0.622989.
Test: 2018-08-05T19:02:06.729847: step 10630, loss 0.549559.
Train: 2018-08-05T19:02:06.995439: step 10631, loss 0.537257.
Train: 2018-08-05T19:02:07.198517: step 10632, loss 0.545506.
Train: 2018-08-05T19:02:07.401595: step 10633, loss 0.511981.
Train: 2018-08-05T19:02:07.604672: step 10634, loss 0.52893.
Train: 2018-08-05T19:02:07.823369: step 10635, loss 0.55405.
Train: 2018-08-05T19:02:08.042040: step 10636, loss 0.470366.
Train: 2018-08-05T19:02:08.245146: step 10637, loss 0.674827.
Train: 2018-08-05T19:02:08.463815: step 10638, loss 0.596032.
Train: 2018-08-05T19:02:08.666918: step 10639, loss 0.554043.
Train: 2018-08-05T19:02:08.870000: step 10640, loss 0.612497.
Test: 2018-08-05T19:02:09.916600: step 10640, loss 0.548203.
Train: 2018-08-05T19:02:10.119707: step 10641, loss 0.496018.
Train: 2018-08-05T19:02:10.338406: step 10642, loss 0.570759.
Train: 2018-08-05T19:02:10.541484: step 10643, loss 0.595676.
Train: 2018-08-05T19:02:10.744561: step 10644, loss 0.529302.
Train: 2018-08-05T19:02:10.947609: step 10645, loss 0.545861.
Train: 2018-08-05T19:02:11.150687: step 10646, loss 0.628989.
Train: 2018-08-05T19:02:11.353794: step 10647, loss 0.612276.
Train: 2018-08-05T19:02:11.556874: step 10648, loss 0.504619.
Train: 2018-08-05T19:02:11.775570: step 10649, loss 0.554219.
Train: 2018-08-05T19:02:11.978647: step 10650, loss 0.579035.
Test: 2018-08-05T19:02:13.025248: step 10650, loss 0.548073.
Train: 2018-08-05T19:02:13.228324: step 10651, loss 0.603883.
Train: 2018-08-05T19:02:13.447024: step 10652, loss 0.529425.
Train: 2018-08-05T19:02:13.650131: step 10653, loss 0.570756.
Train: 2018-08-05T19:02:13.868830: step 10654, loss 0.562477.
Train: 2018-08-05T19:02:14.071907: step 10655, loss 0.55418.
Train: 2018-08-05T19:02:14.274985: step 10656, loss 0.504324.
Train: 2018-08-05T19:02:14.493683: step 10657, loss 0.503949.
Train: 2018-08-05T19:02:14.696732: step 10658, loss 0.596089.
Train: 2018-08-05T19:02:14.899809: step 10659, loss 0.570833.
Train: 2018-08-05T19:02:15.102916: step 10660, loss 0.536758.
Test: 2018-08-05T19:02:16.165137: step 10660, loss 0.548201.
Train: 2018-08-05T19:02:16.368246: step 10661, loss 0.622402.
Train: 2018-08-05T19:02:16.571323: step 10662, loss 0.656906.
Train: 2018-08-05T19:02:16.790021: step 10663, loss 0.502406.
Train: 2018-08-05T19:02:16.993098: step 10664, loss 0.579457.
Train: 2018-08-05T19:02:17.196146: step 10665, loss 0.562337.
Train: 2018-08-05T19:02:17.399253: step 10666, loss 0.545256.
Train: 2018-08-05T19:02:17.617951: step 10667, loss 0.511088.
Train: 2018-08-05T19:02:17.821000: step 10668, loss 0.630889.
Train: 2018-08-05T19:02:18.039698: step 10669, loss 0.502432.
Train: 2018-08-05T19:02:18.242776: step 10670, loss 0.5366.
Test: 2018-08-05T19:02:19.289435: step 10670, loss 0.546569.
Train: 2018-08-05T19:02:19.492513: step 10671, loss 0.424571.
Train: 2018-08-05T19:02:19.711211: step 10672, loss 0.579781.
Train: 2018-08-05T19:02:19.914289: step 10673, loss 0.527242.
Train: 2018-08-05T19:02:20.117366: step 10674, loss 0.59797.
Train: 2018-08-05T19:02:20.336059: step 10675, loss 0.526797.
Train: 2018-08-05T19:02:20.539143: step 10676, loss 0.508631.
Train: 2018-08-05T19:02:20.742221: step 10677, loss 0.517339.
Train: 2018-08-05T19:02:20.945298: step 10678, loss 0.608634.
Train: 2018-08-05T19:02:21.148375: step 10679, loss 0.553716.
Train: 2018-08-05T19:02:21.367074: step 10680, loss 0.609197.
Test: 2018-08-05T19:02:22.413703: step 10680, loss 0.547132.
Train: 2018-08-05T19:02:22.616782: step 10681, loss 0.553748.
Train: 2018-08-05T19:02:22.835479: step 10682, loss 0.61837.
Train: 2018-08-05T19:02:23.038527: step 10683, loss 0.544514.
Train: 2018-08-05T19:02:23.257256: step 10684, loss 0.544524.
Train: 2018-08-05T19:02:23.460338: step 10685, loss 0.599204.
Train: 2018-08-05T19:02:23.663411: step 10686, loss 0.56267.
Train: 2018-08-05T19:02:23.882080: step 10687, loss 0.625513.
Train: 2018-08-05T19:02:24.100779: step 10688, loss 0.553624.
Train: 2018-08-05T19:02:24.303886: step 10689, loss 0.596641.
Train: 2018-08-05T19:02:24.506964: step 10690, loss 0.547453.
Test: 2018-08-05T19:02:25.569185: step 10690, loss 0.548676.
Train: 2018-08-05T19:02:25.772292: step 10691, loss 0.53009.
Train: 2018-08-05T19:02:25.990991: step 10692, loss 0.656227.
Train: 2018-08-05T19:02:26.194068: step 10693, loss 0.647495.
Train: 2018-08-05T19:02:26.397116: step 10694, loss 0.587598.
Train: 2018-08-05T19:02:26.615814: step 10695, loss 0.529311.
Train: 2018-08-05T19:02:26.818921: step 10696, loss 0.5872.
Train: 2018-08-05T19:02:27.021999: step 10697, loss 0.562629.
Train: 2018-08-05T19:02:27.225077: step 10698, loss 0.514254.
Train: 2018-08-05T19:02:27.443771: step 10699, loss 0.562778.
Train: 2018-08-05T19:02:27.646823: step 10700, loss 0.57887.
Test: 2018-08-05T19:02:28.709104: step 10700, loss 0.549288.
Train: 2018-08-05T19:02:29.786947: step 10701, loss 0.594882.
Train: 2018-08-05T19:02:30.005677: step 10702, loss 0.586841.
Train: 2018-08-05T19:02:30.208758: step 10703, loss 0.666315.
Train: 2018-08-05T19:02:30.427452: step 10704, loss 0.555234.
Train: 2018-08-05T19:02:30.630500: step 10705, loss 0.524111.
Train: 2018-08-05T19:02:30.833601: step 10706, loss 0.563268.
Train: 2018-08-05T19:02:31.052306: step 10707, loss 0.641513.
Train: 2018-08-05T19:02:31.255383: step 10708, loss 0.53234.
Train: 2018-08-05T19:02:31.474082: step 10709, loss 0.594425.
Train: 2018-08-05T19:02:31.677130: step 10710, loss 0.509252.
Test: 2018-08-05T19:02:32.723789: step 10710, loss 0.549862.
Train: 2018-08-05T19:02:32.942488: step 10711, loss 0.540067.
Train: 2018-08-05T19:02:33.145567: step 10712, loss 0.524183.
Train: 2018-08-05T19:02:33.348643: step 10713, loss 0.531546.
Train: 2018-08-05T19:02:33.567342: step 10714, loss 0.57886.
Train: 2018-08-05T19:02:33.770419: step 10715, loss 0.570822.
Train: 2018-08-05T19:02:33.973467: step 10716, loss 0.61144.
Train: 2018-08-05T19:02:34.192196: step 10717, loss 0.521692.
Train: 2018-08-05T19:02:34.395273: step 10718, loss 0.480023.
Train: 2018-08-05T19:02:34.598351: step 10719, loss 0.52062.
Train: 2018-08-05T19:02:34.801428: step 10720, loss 0.579316.
Test: 2018-08-05T19:02:35.863648: step 10720, loss 0.547004.
Train: 2018-08-05T19:02:36.035513: step 10721, loss 0.598976.
Train: 2018-08-05T19:02:36.238561: step 10722, loss 0.519032.
Train: 2018-08-05T19:02:36.457289: step 10723, loss 0.544875.
Train: 2018-08-05T19:02:36.660367: step 10724, loss 0.606627.
Train: 2018-08-05T19:02:36.879036: step 10725, loss 0.580256.
Train: 2018-08-05T19:02:37.082143: step 10726, loss 0.589268.
Train: 2018-08-05T19:02:37.300844: step 10727, loss 0.553589.
Train: 2018-08-05T19:02:37.503920: step 10728, loss 0.562523.
Train: 2018-08-05T19:02:37.722620: step 10729, loss 0.437436.
Train: 2018-08-05T19:02:37.941320: step 10730, loss 0.490628.
Test: 2018-08-05T19:02:38.987947: step 10730, loss 0.547755.
Train: 2018-08-05T19:02:39.191025: step 10731, loss 0.653542.
Train: 2018-08-05T19:02:39.409723: step 10732, loss 0.599167.
Train: 2018-08-05T19:02:39.628422: step 10733, loss 0.662803.
Train: 2018-08-05T19:02:39.831504: step 10734, loss 0.53555.
Train: 2018-08-05T19:02:40.065816: step 10735, loss 0.589499.
Train: 2018-08-05T19:02:40.268898: step 10736, loss 0.598132.
Train: 2018-08-05T19:02:40.487596: step 10737, loss 0.562424.
Train: 2018-08-05T19:02:40.690644: step 10738, loss 0.597359.
Train: 2018-08-05T19:02:40.893752: step 10739, loss 0.519071.
Train: 2018-08-05T19:02:41.096829: step 10740, loss 0.596709.
Test: 2018-08-05T19:02:42.159079: step 10740, loss 0.547719.
Train: 2018-08-05T19:02:42.377749: step 10741, loss 0.519739.
Train: 2018-08-05T19:02:42.580860: step 10742, loss 0.503009.
Train: 2018-08-05T19:02:42.783934: step 10743, loss 0.562354.
Train: 2018-08-05T19:02:43.002634: step 10744, loss 0.562356.
Train: 2018-08-05T19:02:43.205709: step 10745, loss 0.553892.
Train: 2018-08-05T19:02:43.424408: step 10746, loss 0.638563.
Train: 2018-08-05T19:02:43.627486: step 10747, loss 0.604524.
Train: 2018-08-05T19:02:43.830564: step 10748, loss 0.570777.
Train: 2018-08-05T19:02:44.049263: step 10749, loss 0.537461.
Train: 2018-08-05T19:02:44.252310: step 10750, loss 0.579054.
Test: 2018-08-05T19:02:45.314593: step 10750, loss 0.549261.
Train: 2018-08-05T19:02:45.517638: step 10751, loss 0.554228.
Train: 2018-08-05T19:02:45.736367: step 10752, loss 0.595488.
Train: 2018-08-05T19:02:45.939414: step 10753, loss 0.603608.
Train: 2018-08-05T19:02:46.142522: step 10754, loss 0.505443.
Train: 2018-08-05T19:02:46.345599: step 10755, loss 0.554452.
Train: 2018-08-05T19:02:46.564268: step 10756, loss 0.554442.
Train: 2018-08-05T19:02:46.767376: step 10757, loss 0.587122.
Train: 2018-08-05T19:02:46.970453: step 10758, loss 0.603492.
Train: 2018-08-05T19:02:47.189152: step 10759, loss 0.61161.
Train: 2018-08-05T19:02:47.392229: step 10760, loss 0.578916.
Test: 2018-08-05T19:02:48.454480: step 10760, loss 0.549163.
Train: 2018-08-05T19:02:48.657559: step 10761, loss 0.530287.
Train: 2018-08-05T19:02:48.891880: step 10762, loss 0.530322.
Train: 2018-08-05T19:02:49.094955: step 10763, loss 0.587016.
Train: 2018-08-05T19:02:49.313655: step 10764, loss 0.59515.
Train: 2018-08-05T19:02:49.516733: step 10765, loss 0.513968.
Train: 2018-08-05T19:02:49.735431: step 10766, loss 0.554484.
Train: 2018-08-05T19:02:49.938477: step 10767, loss 0.578965.
Train: 2018-08-05T19:02:50.157208: step 10768, loss 0.628385.
Train: 2018-08-05T19:02:50.360284: step 10769, loss 0.537957.
Train: 2018-08-05T19:02:50.578954: step 10770, loss 0.529691.
Test: 2018-08-05T19:02:51.641204: step 10770, loss 0.549496.
Train: 2018-08-05T19:02:51.844312: step 10771, loss 0.562509.
Train: 2018-08-05T19:02:52.063013: step 10772, loss 0.603882.
Train: 2018-08-05T19:02:52.266087: step 10773, loss 0.587344.
Train: 2018-08-05T19:02:52.469165: step 10774, loss 0.545877.
Train: 2018-08-05T19:02:52.687866: step 10775, loss 0.554146.
Train: 2018-08-05T19:02:52.890942: step 10776, loss 0.545783.
Train: 2018-08-05T19:02:53.094019: step 10777, loss 0.604201.
Train: 2018-08-05T19:02:53.312718: step 10778, loss 0.56241.
Train: 2018-08-05T19:02:53.515765: step 10779, loss 0.545661.
Train: 2018-08-05T19:02:53.718872: step 10780, loss 0.537219.
Test: 2018-08-05T19:02:54.781127: step 10780, loss 0.547916.
Train: 2018-08-05T19:02:54.984172: step 10781, loss 0.570798.
Train: 2018-08-05T19:02:55.187249: step 10782, loss 0.596169.
Train: 2018-08-05T19:02:55.405978: step 10783, loss 0.587735.
Train: 2018-08-05T19:02:55.609055: step 10784, loss 0.570813.
Train: 2018-08-05T19:02:55.827753: step 10785, loss 0.528594.
Train: 2018-08-05T19:02:56.030831: step 10786, loss 0.579269.
Train: 2018-08-05T19:02:56.233878: step 10787, loss 0.579273.
Train: 2018-08-05T19:02:56.452607: step 10788, loss 0.511656.
Train: 2018-08-05T19:02:56.655685: step 10789, loss 0.536928.
Train: 2018-08-05T19:02:56.858763: step 10790, loss 0.545318.
Test: 2018-08-05T19:02:57.921014: step 10790, loss 0.546865.
Train: 2018-08-05T19:02:58.124091: step 10791, loss 0.55378.
Train: 2018-08-05T19:02:58.342790: step 10792, loss 0.588137.
Train: 2018-08-05T19:02:58.545871: step 10793, loss 0.519214.
Train: 2018-08-05T19:02:58.764566: step 10794, loss 0.579688.
Train: 2018-08-05T19:02:58.967614: step 10795, loss 0.510155.
Train: 2018-08-05T19:02:59.170721: step 10796, loss 0.527357.
Train: 2018-08-05T19:02:59.389420: step 10797, loss 0.535949.
Train: 2018-08-05T19:02:59.608144: step 10798, loss 0.544688.
Train: 2018-08-05T19:02:59.811197: step 10799, loss 0.553594.
Train: 2018-08-05T19:03:00.014274: step 10800, loss 0.544571.
Test: 2018-08-05T19:03:01.076524: step 10800, loss 0.5488.
Train: 2018-08-05T19:03:02.091882: step 10801, loss 0.526325.
Train: 2018-08-05T19:03:02.294960: step 10802, loss 0.581242.
Train: 2018-08-05T19:03:02.513688: step 10803, loss 0.553731.
Train: 2018-08-05T19:03:02.732387: step 10804, loss 0.544509.
Train: 2018-08-05T19:03:02.935435: step 10805, loss 0.553783.
Train: 2018-08-05T19:03:03.138542: step 10806, loss 0.581653.
Train: 2018-08-05T19:03:03.357210: step 10807, loss 0.572304.
Train: 2018-08-05T19:03:03.575910: step 10808, loss 0.535286.
Train: 2018-08-05T19:03:03.779016: step 10809, loss 0.590492.
Train: 2018-08-05T19:03:03.997715: step 10810, loss 0.58103.
Test: 2018-08-05T19:03:05.044346: step 10810, loss 0.548056.
Train: 2018-08-05T19:03:05.278638: step 10811, loss 0.544721.
Train: 2018-08-05T19:03:05.481743: step 10812, loss 0.580102.
Train: 2018-08-05T19:03:05.684820: step 10813, loss 0.503576.
Train: 2018-08-05T19:03:05.903490: step 10814, loss 0.650191.
Train: 2018-08-05T19:03:06.106597: step 10815, loss 0.501361.
Train: 2018-08-05T19:03:06.325295: step 10816, loss 0.571185.
Train: 2018-08-05T19:03:06.528373: step 10817, loss 0.615155.
Train: 2018-08-05T19:03:06.747041: step 10818, loss 0.579858.
Train: 2018-08-05T19:03:06.950154: step 10819, loss 0.51026.
Train: 2018-08-05T19:03:07.168848: step 10820, loss 0.596948.
Test: 2018-08-05T19:03:08.215479: step 10820, loss 0.54757.
Train: 2018-08-05T19:03:08.434177: step 10821, loss 0.502107.
Train: 2018-08-05T19:03:08.637255: step 10822, loss 0.596709.
Train: 2018-08-05T19:03:08.855954: step 10823, loss 0.579466.
Train: 2018-08-05T19:03:09.074651: step 10824, loss 0.519688.
Train: 2018-08-05T19:03:09.277700: step 10825, loss 0.587924.
Train: 2018-08-05T19:03:09.480807: step 10826, loss 0.57935.
Train: 2018-08-05T19:03:09.683885: step 10827, loss 0.562353.
Train: 2018-08-05T19:03:09.902553: step 10828, loss 0.49475.
Train: 2018-08-05T19:03:10.105660: step 10829, loss 0.604708.
Train: 2018-08-05T19:03:10.308738: step 10830, loss 0.460779.
Test: 2018-08-05T19:03:11.370989: step 10830, loss 0.549483.
Train: 2018-08-05T19:03:11.574066: step 10831, loss 0.587905.
Train: 2018-08-05T19:03:11.792763: step 10832, loss 0.613668.
Train: 2018-08-05T19:03:11.995843: step 10833, loss 0.596121.
Train: 2018-08-05T19:03:12.198919: step 10834, loss 0.524109.
Train: 2018-08-05T19:03:12.401997: step 10835, loss 0.62894.
Train: 2018-08-05T19:03:12.620696: step 10836, loss 0.57077.
Train: 2018-08-05T19:03:12.823774: step 10837, loss 0.579154.
Train: 2018-08-05T19:03:13.026853: step 10838, loss 0.570771.
Train: 2018-08-05T19:03:13.245522: step 10839, loss 0.504068.
Train: 2018-08-05T19:03:13.448632: step 10840, loss 0.579244.
Test: 2018-08-05T19:03:14.510878: step 10840, loss 0.54839.
Train: 2018-08-05T19:03:14.713957: step 10841, loss 0.579306.
Train: 2018-08-05T19:03:14.932655: step 10842, loss 0.562352.
Train: 2018-08-05T19:03:15.135733: step 10843, loss 0.5793.
Train: 2018-08-05T19:03:15.338810: step 10844, loss 0.520053.
Train: 2018-08-05T19:03:15.541857: step 10845, loss 0.562352.
Train: 2018-08-05T19:03:15.760567: step 10846, loss 0.502906.
Train: 2018-08-05T19:03:15.963664: step 10847, loss 0.579422.
Train: 2018-08-05T19:03:16.166741: step 10848, loss 0.519448.
Train: 2018-08-05T19:03:16.385440: step 10849, loss 0.596883.
Train: 2018-08-05T19:03:16.588488: step 10850, loss 0.562345.
Test: 2018-08-05T19:03:17.650738: step 10850, loss 0.54764.
Train: 2018-08-05T19:03:17.853846: step 10851, loss 0.501524.
Train: 2018-08-05T19:03:18.056924: step 10852, loss 0.562377.
Train: 2018-08-05T19:03:18.259971: step 10853, loss 0.518407.
Train: 2018-08-05T19:03:18.478670: step 10854, loss 0.597952.
Train: 2018-08-05T19:03:18.681776: step 10855, loss 0.633742.
Train: 2018-08-05T19:03:18.884857: step 10856, loss 0.562468.
Train: 2018-08-05T19:03:19.087932: step 10857, loss 0.571289.
Train: 2018-08-05T19:03:19.306630: step 10858, loss 0.527178.
Train: 2018-08-05T19:03:19.509709: step 10859, loss 0.553609.
Train: 2018-08-05T19:03:19.712786: step 10860, loss 0.615076.
Test: 2018-08-05T19:03:20.775007: step 10860, loss 0.547786.
Train: 2018-08-05T19:03:20.978114: step 10861, loss 0.492512.
Train: 2018-08-05T19:03:21.181193: step 10862, loss 0.640932.
Train: 2018-08-05T19:03:21.384269: step 10863, loss 0.544995.
Train: 2018-08-05T19:03:21.587347: step 10864, loss 0.475957.
Train: 2018-08-05T19:03:21.806046: step 10865, loss 0.510427.
Train: 2018-08-05T19:03:22.009123: step 10866, loss 0.518885.
Train: 2018-08-05T19:03:22.212201: step 10867, loss 0.544872.
Train: 2018-08-05T19:03:22.415277: step 10868, loss 0.527156.
Train: 2018-08-05T19:03:22.618356: step 10869, loss 0.624693.
Train: 2018-08-05T19:03:22.837054: step 10870, loss 0.571409.
Test: 2018-08-05T19:03:23.883684: step 10870, loss 0.549665.
Train: 2018-08-05T19:03:24.086761: step 10871, loss 0.598183.
Train: 2018-08-05T19:03:24.258567: step 10872, loss 0.486546.
Train: 2018-08-05T19:03:24.477265: step 10873, loss 0.589273.
Train: 2018-08-05T19:03:24.680372: step 10874, loss 0.553588.
Train: 2018-08-05T19:03:24.883450: step 10875, loss 0.535756.
Train: 2018-08-05T19:03:25.086527: step 10876, loss 0.633896.
Train: 2018-08-05T19:03:25.289604: step 10877, loss 0.562472.
Train: 2018-08-05T19:03:25.492683: step 10878, loss 0.606638.
Train: 2018-08-05T19:03:25.695759: step 10879, loss 0.623808.
Train: 2018-08-05T19:03:25.914428: step 10880, loss 0.579704.
Test: 2018-08-05T19:03:26.961059: step 10880, loss 0.548793.
Train: 2018-08-05T19:03:27.164166: step 10881, loss 0.622406.
Train: 2018-08-05T19:03:27.382865: step 10882, loss 0.596213.
Train: 2018-08-05T19:03:27.601563: step 10883, loss 0.57911.
Train: 2018-08-05T19:03:27.804640: step 10884, loss 0.546071.
Train: 2018-08-05T19:03:28.007719: step 10885, loss 0.521914.
Train: 2018-08-05T19:03:28.226417: step 10886, loss 0.554613.
Train: 2018-08-05T19:03:28.445086: step 10887, loss 0.498284.
Train: 2018-08-05T19:03:28.648193: step 10888, loss 0.570813.
Train: 2018-08-05T19:03:28.851241: step 10889, loss 0.506138.
Train: 2018-08-05T19:03:29.054348: step 10890, loss 0.603315.
Test: 2018-08-05T19:03:30.116570: step 10890, loss 0.549165.
Train: 2018-08-05T19:03:30.319647: step 10891, loss 0.619745.
Train: 2018-08-05T19:03:30.522725: step 10892, loss 0.497315.
Train: 2018-08-05T19:03:30.725832: step 10893, loss 0.513327.
Train: 2018-08-05T19:03:30.944525: step 10894, loss 0.545921.
Train: 2018-08-05T19:03:31.147611: step 10895, loss 0.637659.
Train: 2018-08-05T19:03:31.350656: step 10896, loss 0.50365.
Train: 2018-08-05T19:03:31.553763: step 10897, loss 0.553899.
Train: 2018-08-05T19:03:31.756841: step 10898, loss 0.65616.
Train: 2018-08-05T19:03:31.959888: step 10899, loss 0.647631.
Train: 2018-08-05T19:03:32.162966: step 10900, loss 0.511512.
Test: 2018-08-05T19:03:33.209596: step 10900, loss 0.548039.
Train: 2018-08-05T19:03:34.115667: step 10901, loss 0.570817.
Train: 2018-08-05T19:03:34.318740: step 10902, loss 0.587687.
Train: 2018-08-05T19:03:34.521787: step 10903, loss 0.629662.
Train: 2018-08-05T19:03:34.724896: step 10904, loss 0.570767.
Train: 2018-08-05T19:03:34.927973: step 10905, loss 0.570757.
Train: 2018-08-05T19:03:35.131050: step 10906, loss 0.554294.
Train: 2018-08-05T19:03:35.334128: step 10907, loss 0.570763.
Train: 2018-08-05T19:03:35.537175: step 10908, loss 0.521811.
Train: 2018-08-05T19:03:35.740282: step 10909, loss 0.595247.
Train: 2018-08-05T19:03:35.943364: step 10910, loss 0.570778.
Test: 2018-08-05T19:03:37.005611: step 10910, loss 0.548506.
Train: 2018-08-05T19:03:37.208689: step 10911, loss 0.497589.
Train: 2018-08-05T19:03:37.458631: step 10912, loss 0.587098.
Train: 2018-08-05T19:03:37.661707: step 10913, loss 0.546216.
Train: 2018-08-05T19:03:37.880406: step 10914, loss 0.603619.
Train: 2018-08-05T19:03:38.083453: step 10915, loss 0.496728.
Train: 2018-08-05T19:03:38.286564: step 10916, loss 0.537654.
Train: 2018-08-05T19:03:38.489639: step 10917, loss 0.537413.
Train: 2018-08-05T19:03:38.692716: step 10918, loss 0.56238.
Train: 2018-08-05T19:03:38.895795: step 10919, loss 0.604751.
Train: 2018-08-05T19:03:39.098871: step 10920, loss 0.536784.
Test: 2018-08-05T19:03:40.161092: step 10920, loss 0.54763.
Train: 2018-08-05T19:03:40.364199: step 10921, loss 0.536628.
Train: 2018-08-05T19:03:40.567247: step 10922, loss 0.562338.
Train: 2018-08-05T19:03:40.770355: step 10923, loss 0.518945.
Train: 2018-08-05T19:03:40.973401: step 10924, loss 0.536125.
Train: 2018-08-05T19:03:41.176509: step 10925, loss 0.544772.
Train: 2018-08-05T19:03:41.363965: step 10926, loss 0.553588.
Train: 2018-08-05T19:03:41.567047: step 10927, loss 0.481822.
Train: 2018-08-05T19:03:41.770120: step 10928, loss 0.562887.
Train: 2018-08-05T19:03:41.973198: step 10929, loss 0.562879.
Train: 2018-08-05T19:03:42.191896: step 10930, loss 0.590611.
Test: 2018-08-05T19:03:43.238526: step 10930, loss 0.549049.
Train: 2018-08-05T19:03:43.441608: step 10931, loss 0.563.
Train: 2018-08-05T19:03:43.644682: step 10932, loss 0.535286.
Train: 2018-08-05T19:03:43.847758: step 10933, loss 0.507434.
Train: 2018-08-05T19:03:44.050839: step 10934, loss 0.525882.
Train: 2018-08-05T19:03:44.253914: step 10935, loss 0.535166.
Train: 2018-08-05T19:03:44.456991: step 10936, loss 0.563355.
Train: 2018-08-05T19:03:44.660038: step 10937, loss 0.51627.
Train: 2018-08-05T19:03:44.863146: step 10938, loss 0.478326.
Train: 2018-08-05T19:03:45.066223: step 10939, loss 0.592313.
Train: 2018-08-05T19:03:45.284893: step 10940, loss 0.573343.
Test: 2018-08-05T19:03:46.331552: step 10940, loss 0.548025.
Train: 2018-08-05T19:03:46.534630: step 10941, loss 0.630696.
Train: 2018-08-05T19:03:46.737677: step 10942, loss 0.563586.
Train: 2018-08-05T19:03:46.940784: step 10943, loss 0.535132.
Train: 2018-08-05T19:03:47.143862: step 10944, loss 0.581944.
Train: 2018-08-05T19:03:47.346909: step 10945, loss 0.609421.
Train: 2018-08-05T19:03:47.549987: step 10946, loss 0.553671.
Train: 2018-08-05T19:03:47.753065: step 10947, loss 0.517756.
Train: 2018-08-05T19:03:47.956141: step 10948, loss 0.517544.
Train: 2018-08-05T19:03:48.159249: step 10949, loss 0.625889.
Train: 2018-08-05T19:03:48.362297: step 10950, loss 0.482643.
Test: 2018-08-05T19:03:49.424548: step 10950, loss 0.54849.
Train: 2018-08-05T19:03:49.627656: step 10951, loss 0.562433.
Train: 2018-08-05T19:03:49.830737: step 10952, loss 0.527196.
Train: 2018-08-05T19:03:50.033810: step 10953, loss 0.606345.
Train: 2018-08-05T19:03:50.236888: step 10954, loss 0.544877.
Train: 2018-08-05T19:03:50.439970: step 10955, loss 0.483869.
Train: 2018-08-05T19:03:50.643012: step 10956, loss 0.5449.
Train: 2018-08-05T19:03:50.846119: step 10957, loss 0.571134.
Train: 2018-08-05T19:03:51.049197: step 10958, loss 0.562385.
Train: 2018-08-05T19:03:51.236653: step 10959, loss 0.571158.
Train: 2018-08-05T19:03:51.455352: step 10960, loss 0.632525.
Test: 2018-08-05T19:03:52.501982: step 10960, loss 0.547193.
Train: 2018-08-05T19:03:52.705060: step 10961, loss 0.536192.
Train: 2018-08-05T19:03:52.908137: step 10962, loss 0.57975.
Train: 2018-08-05T19:03:53.111210: step 10963, loss 0.605653.
Train: 2018-08-05T19:03:53.314296: step 10964, loss 0.656988.
Train: 2018-08-05T19:03:53.517370: step 10965, loss 0.528333.
Train: 2018-08-05T19:03:53.720447: step 10966, loss 0.596092.
Train: 2018-08-05T19:03:53.939145: step 10967, loss 0.570768.
Train: 2018-08-05T19:03:54.142223: step 10968, loss 0.545934.
Train: 2018-08-05T19:03:54.345270: step 10969, loss 0.55432.
Train: 2018-08-05T19:03:54.548378: step 10970, loss 0.513513.
Test: 2018-08-05T19:03:55.605984: step 10970, loss 0.548191.
Train: 2018-08-05T19:03:55.809092: step 10971, loss 0.538073.
Train: 2018-08-05T19:03:55.996547: step 10972, loss 0.636261.
Train: 2018-08-05T19:03:56.199595: step 10973, loss 0.554442.
Train: 2018-08-05T19:03:56.402703: step 10974, loss 0.530005.
Train: 2018-08-05T19:03:56.605780: step 10975, loss 0.619771.
Train: 2018-08-05T19:03:56.808857: step 10976, loss 0.538162.
Train: 2018-08-05T19:03:57.011935: step 10977, loss 0.529977.
Train: 2018-08-05T19:03:57.214982: step 10978, loss 0.513457.
Train: 2018-08-05T19:03:57.418089: step 10979, loss 0.587242.
Train: 2018-08-05T19:03:57.621167: step 10980, loss 0.587326.
Test: 2018-08-05T19:03:58.667766: step 10980, loss 0.54914.
Train: 2018-08-05T19:03:58.870874: step 10981, loss 0.520886.
Train: 2018-08-05T19:03:59.073921: step 10982, loss 0.537321.
Train: 2018-08-05T19:03:59.277030: step 10983, loss 0.629768.
Train: 2018-08-05T19:03:59.480077: step 10984, loss 0.570809.
Train: 2018-08-05T19:03:59.683184: step 10985, loss 0.520052.
Train: 2018-08-05T19:03:59.886261: step 10986, loss 0.613341.
Train: 2018-08-05T19:04:00.089340: step 10987, loss 0.553837.
Train: 2018-08-05T19:04:00.276765: step 10988, loss 0.51123.
Train: 2018-08-05T19:04:00.479873: step 10989, loss 0.57945.
Train: 2018-08-05T19:04:00.682950: step 10990, loss 0.536588.
Test: 2018-08-05T19:04:01.745172: step 10990, loss 0.549332.
Train: 2018-08-05T19:04:01.948280: step 10991, loss 0.570955.
Train: 2018-08-05T19:04:02.135735: step 10992, loss 0.527759.
Train: 2018-08-05T19:04:02.338813: step 10993, loss 0.518917.
Train: 2018-08-05T19:04:02.541890: step 10994, loss 0.606105.
Train: 2018-08-05T19:04:02.744967: step 10995, loss 0.518528.
Train: 2018-08-05T19:04:02.948046: step 10996, loss 0.606503.
Train: 2018-08-05T19:04:03.151121: step 10997, loss 0.597742.
Train: 2018-08-05T19:04:03.354199: step 10998, loss 0.58005.
Train: 2018-08-05T19:04:03.557276: step 10999, loss 0.579983.
Train: 2018-08-05T19:04:03.760324: step 11000, loss 0.54487.
Test: 2018-08-05T19:04:04.822576: step 11000, loss 0.546985.
Train: 2018-08-05T19:04:05.744265: step 11001, loss 0.597286.
Train: 2018-08-05T19:04:05.947341: step 11002, loss 0.59709.
Train: 2018-08-05T19:04:06.150389: step 11003, loss 0.665827.
Train: 2018-08-05T19:04:06.353497: step 11004, loss 0.545315.
Train: 2018-08-05T19:04:06.556574: step 11005, loss 0.553949.
Train: 2018-08-05T19:04:06.759652: step 11006, loss 0.529014.
Train: 2018-08-05T19:04:06.947077: step 11007, loss 0.645508.
Train: 2018-08-05T19:04:07.150156: step 11008, loss 0.55431.
Train: 2018-08-05T19:04:07.353263: step 11009, loss 0.554452.
Train: 2018-08-05T19:04:07.556340: step 11010, loss 0.497781.
Test: 2018-08-05T19:04:08.618562: step 11010, loss 0.548767.
Train: 2018-08-05T19:04:08.868533: step 11011, loss 0.562683.
Train: 2018-08-05T19:04:09.071582: step 11012, loss 0.578903.
Train: 2018-08-05T19:04:09.274658: step 11013, loss 0.554566.
Train: 2018-08-05T19:04:09.477736: step 11014, loss 0.546419.
Train: 2018-08-05T19:04:09.680843: step 11015, loss 0.538194.
Train: 2018-08-05T19:04:09.883891: step 11016, loss 0.554386.
Train: 2018-08-05T19:04:10.086999: step 11017, loss 0.595608.
Train: 2018-08-05T19:04:10.274424: step 11018, loss 0.545989.
Train: 2018-08-05T19:04:10.477531: step 11019, loss 0.545882.
Train: 2018-08-05T19:04:10.680578: step 11020, loss 0.595776.
Test: 2018-08-05T19:04:11.742848: step 11020, loss 0.548828.
Train: 2018-08-05T19:04:11.930286: step 11021, loss 0.503871.
Train: 2018-08-05T19:04:12.133394: step 11022, loss 0.562376.
Train: 2018-08-05T19:04:12.305228: step 11023, loss 0.50813.
Train: 2018-08-05T19:04:12.508276: step 11024, loss 0.587987.
Train: 2018-08-05T19:04:12.711354: step 11025, loss 0.613976.
Train: 2018-08-05T19:04:12.914461: step 11026, loss 0.536451.
Train: 2018-08-05T19:04:13.117538: step 11027, loss 0.536358.
Train: 2018-08-05T19:04:13.320617: step 11028, loss 0.597173.
Train: 2018-08-05T19:04:13.523694: step 11029, loss 0.63213.
Train: 2018-08-05T19:04:13.726770: step 11030, loss 0.571051.
Test: 2018-08-05T19:04:14.789022: step 11030, loss 0.546874.
Train: 2018-08-05T19:04:14.992099: step 11031, loss 0.57968.
Train: 2018-08-05T19:04:15.195147: step 11032, loss 0.553708.
Train: 2018-08-05T19:04:15.398254: step 11033, loss 0.562335.
Train: 2018-08-05T19:04:15.601301: step 11034, loss 0.605198.
Train: 2018-08-05T19:04:15.804409: step 11035, loss 0.553816.
Train: 2018-08-05T19:04:16.007486: step 11036, loss 0.604779.
Train: 2018-08-05T19:04:16.210565: step 11037, loss 0.562371.
Train: 2018-08-05T19:04:16.413641: step 11038, loss 0.637826.
Train: 2018-08-05T19:04:16.616719: step 11039, loss 0.520962.
Train: 2018-08-05T19:04:16.819798: step 11040, loss 0.554253.
Test: 2018-08-05T19:04:17.866396: step 11040, loss 0.549008.
Train: 2018-08-05T19:04:18.069503: step 11041, loss 0.546108.
Train: 2018-08-05T19:04:18.272551: step 11042, loss 0.529766.
Train: 2018-08-05T19:04:18.475658: step 11043, loss 0.595376.
Train: 2018-08-05T19:04:18.678735: step 11044, loss 0.48059.
Train: 2018-08-05T19:04:18.881813: step 11045, loss 0.587234.
Train: 2018-08-05T19:04:19.084892: step 11046, loss 0.529421.
Train: 2018-08-05T19:04:19.287938: step 11047, loss 0.529191.
Train: 2018-08-05T19:04:19.506667: step 11048, loss 0.512139.
Train: 2018-08-05T19:04:19.709745: step 11049, loss 0.604671.
Train: 2018-08-05T19:04:19.897200: step 11050, loss 0.545303.
Test: 2018-08-05T19:04:20.959422: step 11050, loss 0.548007.
Train: 2018-08-05T19:04:21.162498: step 11051, loss 0.562335.
Train: 2018-08-05T19:04:21.365607: step 11052, loss 0.484641.
Train: 2018-08-05T19:04:21.553033: step 11053, loss 0.571086.
Train: 2018-08-05T19:04:21.756140: step 11054, loss 0.562404.
Train: 2018-08-05T19:04:21.959219: step 11055, loss 0.597878.
Train: 2018-08-05T19:04:22.162295: step 11056, loss 0.553589.
Train: 2018-08-05T19:04:22.380993: step 11057, loss 0.482233.
Train: 2018-08-05T19:04:22.584072: step 11058, loss 0.562579.
Train: 2018-08-05T19:04:22.787119: step 11059, loss 0.562645.
Train: 2018-08-05T19:04:22.990227: step 11060, loss 0.544554.
Test: 2018-08-05T19:04:24.036826: step 11060, loss 0.546494.
Train: 2018-08-05T19:04:24.239935: step 11061, loss 0.553647.
Train: 2018-08-05T19:04:24.443013: step 11062, loss 0.599368.
Train: 2018-08-05T19:04:24.646088: step 11063, loss 0.535387.
Train: 2018-08-05T19:04:24.849166: step 11064, loss 0.617675.
Train: 2018-08-05T19:04:25.067864: step 11065, loss 0.562757.
Train: 2018-08-05T19:04:25.270942: step 11066, loss 0.598982.
Train: 2018-08-05T19:04:25.474021: step 11067, loss 0.625678.
Train: 2018-08-05T19:04:25.677067: step 11068, loss 0.526846.
Train: 2018-08-05T19:04:25.880175: step 11069, loss 0.535911.
Train: 2018-08-05T19:04:26.083221: step 11070, loss 0.536037.
Test: 2018-08-05T19:04:27.129883: step 11070, loss 0.547162.
Train: 2018-08-05T19:04:27.332960: step 11071, loss 0.597372.
Train: 2018-08-05T19:04:27.536037: step 11072, loss 0.544968.
Train: 2018-08-05T19:04:27.739114: step 11073, loss 0.631534.
Train: 2018-08-05T19:04:27.942198: step 11074, loss 0.536619.
Train: 2018-08-05T19:04:28.145269: step 11075, loss 0.502729.
Train: 2018-08-05T19:04:28.348346: step 11076, loss 0.528354.
Train: 2018-08-05T19:04:28.551424: step 11077, loss 0.536844.
Train: 2018-08-05T19:04:28.754502: step 11078, loss 0.528238.
Train: 2018-08-05T19:04:28.957580: step 11079, loss 0.649708.
Train: 2018-08-05T19:04:29.176272: step 11080, loss 0.596468.
Test: 2018-08-05T19:04:30.222907: step 11080, loss 0.547963.
Train: 2018-08-05T19:04:30.441606: step 11081, loss 0.570842.
Train: 2018-08-05T19:04:30.644683: step 11082, loss 0.494677.
Train: 2018-08-05T19:04:30.847760: step 11083, loss 0.528494.
Train: 2018-08-05T19:04:31.050839: step 11084, loss 0.587816.
Train: 2018-08-05T19:04:31.253916: step 11085, loss 0.519856.
Train: 2018-08-05T19:04:31.456995: step 11086, loss 0.536753.
Train: 2018-08-05T19:04:31.660040: step 11087, loss 0.562335.
Train: 2018-08-05T19:04:31.863148: step 11088, loss 0.553728.
Train: 2018-08-05T19:04:32.066196: step 11089, loss 0.545051.
Train: 2018-08-05T19:04:32.269303: step 11090, loss 0.562351.
Test: 2018-08-05T19:04:33.331525: step 11090, loss 0.547196.
Train: 2018-08-05T19:04:33.534602: step 11091, loss 0.614694.
Train: 2018-08-05T19:04:33.737709: step 11092, loss 0.527473.
Train: 2018-08-05T19:04:33.940786: step 11093, loss 0.579853.
Train: 2018-08-05T19:04:34.143860: step 11094, loss 0.614835.
Train: 2018-08-05T19:04:34.346942: step 11095, loss 0.536215.
Train: 2018-08-05T19:04:34.550018: step 11096, loss 0.527548.
Train: 2018-08-05T19:04:34.753067: step 11097, loss 0.63201.
Train: 2018-08-05T19:04:34.956175: step 11098, loss 0.649054.
Train: 2018-08-05T19:04:35.159251: step 11099, loss 0.553744.
Train: 2018-08-05T19:04:35.377950: step 11100, loss 0.545293.
Test: 2018-08-05T19:04:36.424580: step 11100, loss 0.549374.
Train: 2018-08-05T19:04:37.393106: step 11101, loss 0.562354.
Train: 2018-08-05T19:04:37.596150: step 11102, loss 0.5708.
Train: 2018-08-05T19:04:37.814881: step 11103, loss 0.579164.
Train: 2018-08-05T19:04:38.017958: step 11104, loss 0.529075.
Train: 2018-08-05T19:04:38.221034: step 11105, loss 0.537491.
Train: 2018-08-05T19:04:38.424081: step 11106, loss 0.579076.
Train: 2018-08-05T19:04:38.627189: step 11107, loss 0.57076.
Train: 2018-08-05T19:04:38.830268: step 11108, loss 0.570759.
Train: 2018-08-05T19:04:39.048966: step 11109, loss 0.529302.
Train: 2018-08-05T19:04:39.252013: step 11110, loss 0.595673.
Test: 2018-08-05T19:04:40.298673: step 11110, loss 0.549737.
Train: 2018-08-05T19:04:40.517371: step 11111, loss 0.529247.
Train: 2018-08-05T19:04:40.720420: step 11112, loss 0.520834.
Train: 2018-08-05T19:04:40.923526: step 11113, loss 0.570773.
Train: 2018-08-05T19:04:41.126604: step 11114, loss 0.604385.
Train: 2018-08-05T19:04:41.329652: step 11115, loss 0.579204.
Train: 2018-08-05T19:04:41.548350: step 11116, loss 0.537131.
Train: 2018-08-05T19:04:41.751460: step 11117, loss 0.545501.
Train: 2018-08-05T19:04:41.954506: step 11118, loss 0.494666.
Train: 2018-08-05T19:04:42.173234: step 11119, loss 0.477107.
Train: 2018-08-05T19:04:42.376312: step 11120, loss 0.570963.
Test: 2018-08-05T19:04:43.438564: step 11120, loss 0.547611.
Train: 2018-08-05T19:04:43.641640: step 11121, loss 0.605915.
Train: 2018-08-05T19:04:43.844718: step 11122, loss 0.527331.
Train: 2018-08-05T19:04:44.047795: step 11123, loss 0.535947.
Train: 2018-08-05T19:04:44.250872: step 11124, loss 0.580267.
Train: 2018-08-05T19:04:44.469571: step 11125, loss 0.562528.
Train: 2018-08-05T19:04:44.672648: step 11126, loss 0.571542.
Train: 2018-08-05T19:04:44.875697: step 11127, loss 0.508625.
Train: 2018-08-05T19:04:45.094425: step 11128, loss 0.544575.
Train: 2018-08-05T19:04:45.297503: step 11129, loss 0.562703.
Train: 2018-08-05T19:04:45.500580: step 11130, loss 0.54454.
Test: 2018-08-05T19:04:46.547180: step 11130, loss 0.54565.
Train: 2018-08-05T19:04:46.765909: step 11131, loss 0.571918.
Train: 2018-08-05T19:04:46.968986: step 11132, loss 0.553666.
Train: 2018-08-05T19:04:47.172063: step 11133, loss 0.562815.
Train: 2018-08-05T19:04:47.375111: step 11134, loss 0.562808.
Train: 2018-08-05T19:04:47.578219: step 11135, loss 0.562787.
Train: 2018-08-05T19:04:47.796917: step 11136, loss 0.489887.
Train: 2018-08-05T19:04:47.999994: step 11137, loss 0.581017.
Train: 2018-08-05T19:04:48.203073: step 11138, loss 0.608335.
Train: 2018-08-05T19:04:48.406149: step 11139, loss 0.599004.
Train: 2018-08-05T19:04:48.624819: step 11140, loss 0.571629.
Test: 2018-08-05T19:04:49.687098: step 11140, loss 0.548213.
Train: 2018-08-05T19:04:49.890177: step 11141, loss 0.517809.
Train: 2018-08-05T19:04:50.093224: step 11142, loss 0.535783.
Train: 2018-08-05T19:04:50.311954: step 11143, loss 0.597958.
Train: 2018-08-05T19:04:50.515030: step 11144, loss 0.571247.
Train: 2018-08-05T19:04:50.718108: step 11145, loss 0.658857.
Train: 2018-08-05T19:04:50.921185: step 11146, loss 0.510343.
Train: 2018-08-05T19:04:51.139854: step 11147, loss 0.579538.
Train: 2018-08-05T19:04:51.342961: step 11148, loss 0.528211.
Train: 2018-08-05T19:04:51.561660: step 11149, loss 0.587813.
Train: 2018-08-05T19:04:51.764738: step 11150, loss 0.520187.
Test: 2018-08-05T19:04:52.811337: step 11150, loss 0.549684.
Train: 2018-08-05T19:04:53.014445: step 11151, loss 0.612859.
Train: 2018-08-05T19:04:53.233145: step 11152, loss 0.570774.
Train: 2018-08-05T19:04:53.436221: step 11153, loss 0.612386.
Train: 2018-08-05T19:04:53.639299: step 11154, loss 0.529444.
Train: 2018-08-05T19:04:53.842376: step 11155, loss 0.578986.
Train: 2018-08-05T19:04:54.061044: step 11156, loss 0.587149.
Train: 2018-08-05T19:04:54.264122: step 11157, loss 0.603384.
Train: 2018-08-05T19:04:54.482851: step 11158, loss 0.554597.
Train: 2018-08-05T19:04:54.685928: step 11159, loss 0.546629.
Train: 2018-08-05T19:04:54.904598: step 11160, loss 0.594964.
Test: 2018-08-05T19:04:55.951259: step 11160, loss 0.549625.
Train: 2018-08-05T19:04:56.169956: step 11161, loss 0.578867.
Train: 2018-08-05T19:04:56.373034: step 11162, loss 0.546893.
Train: 2018-08-05T19:04:56.576113: step 11163, loss 0.570876.
Train: 2018-08-05T19:04:56.794810: step 11164, loss 0.602807.
Train: 2018-08-05T19:04:56.997887: step 11165, loss 0.570894.
Train: 2018-08-05T19:04:57.232177: step 11166, loss 0.51523.
Train: 2018-08-05T19:04:57.435284: step 11167, loss 0.562907.
Train: 2018-08-05T19:04:57.638362: step 11168, loss 0.530836.
Train: 2018-08-05T19:04:57.857061: step 11169, loss 0.538604.
Train: 2018-08-05T19:04:58.060138: step 11170, loss 0.603265.
Test: 2018-08-05T19:04:59.122360: step 11170, loss 0.550106.
Train: 2018-08-05T19:04:59.325437: step 11171, loss 0.56261.
Train: 2018-08-05T19:04:59.544166: step 11172, loss 0.529735.
Train: 2018-08-05T19:04:59.747243: step 11173, loss 0.56249.
Train: 2018-08-05T19:04:59.919078: step 11174, loss 0.455863.
Train: 2018-08-05T19:05:00.137777: step 11175, loss 0.579246.
Train: 2018-08-05T19:05:00.340855: step 11176, loss 0.511126.
Train: 2018-08-05T19:05:00.543932: step 11177, loss 0.545043.
Train: 2018-08-05T19:05:00.762631: step 11178, loss 0.64999.
Train: 2018-08-05T19:05:00.965678: step 11179, loss 0.606468.
Train: 2018-08-05T19:05:01.168757: step 11180, loss 0.535941.
Test: 2018-08-05T19:05:02.231038: step 11180, loss 0.547048.
Train: 2018-08-05T19:05:02.434085: step 11181, loss 0.580157.
Train: 2018-08-05T19:05:02.652782: step 11182, loss 0.518135.
Train: 2018-08-05T19:05:02.855890: step 11183, loss 0.633615.
Train: 2018-08-05T19:05:03.058969: step 11184, loss 0.633451.
Train: 2018-08-05T19:05:03.277666: step 11185, loss 0.535974.
Train: 2018-08-05T19:05:03.480733: step 11186, loss 0.571161.
Train: 2018-08-05T19:05:03.699444: step 11187, loss 0.536191.
Train: 2018-08-05T19:05:03.902520: step 11188, loss 0.553659.
Train: 2018-08-05T19:05:04.105597: step 11189, loss 0.553677.
Train: 2018-08-05T19:05:04.308675: step 11190, loss 0.545042.
Test: 2018-08-05T19:05:05.370926: step 11190, loss 0.54791.
Train: 2018-08-05T19:05:05.589625: step 11191, loss 0.501857.
Train: 2018-08-05T19:05:05.792673: step 11192, loss 0.54502.
Train: 2018-08-05T19:05:05.995780: step 11193, loss 0.605803.
Train: 2018-08-05T19:05:06.214448: step 11194, loss 0.527595.
Train: 2018-08-05T19:05:06.433180: step 11195, loss 0.492715.
Train: 2018-08-05T19:05:06.636255: step 11196, loss 0.667438.
Train: 2018-08-05T19:05:06.854954: step 11197, loss 0.606073.
Train: 2018-08-05T19:05:07.073623: step 11198, loss 0.562356.
Train: 2018-08-05T19:05:07.276701: step 11199, loss 0.605659.
Train: 2018-08-05T19:05:07.495430: step 11200, loss 0.519314.
Test: 2018-08-05T19:05:08.542028: step 11200, loss 0.548411.
Train: 2018-08-05T19:05:09.541825: step 11201, loss 0.536614.
Train: 2018-08-05T19:05:09.744902: step 11202, loss 0.519538.
Train: 2018-08-05T19:05:09.947981: step 11203, loss 0.553766.
Train: 2018-08-05T19:05:10.166679: step 11204, loss 0.613829.
Train: 2018-08-05T19:05:10.369756: step 11205, loss 0.570901.
Train: 2018-08-05T19:05:10.572835: step 11206, loss 0.545244.
Train: 2018-08-05T19:05:10.791532: step 11207, loss 0.536725.
Train: 2018-08-05T19:05:10.994609: step 11208, loss 0.493987.
Train: 2018-08-05T19:05:11.213279: step 11209, loss 0.545163.
Train: 2018-08-05T19:05:11.416386: step 11210, loss 0.6055.
Test: 2018-08-05T19:05:12.478626: step 11210, loss 0.548493.
Train: 2018-08-05T19:05:12.681714: step 11211, loss 0.553692.
Train: 2018-08-05T19:05:12.884762: step 11212, loss 0.53634.
Train: 2018-08-05T19:05:13.103461: step 11213, loss 0.553657.
Train: 2018-08-05T19:05:13.306539: step 11214, loss 0.518734.
Train: 2018-08-05T19:05:13.525267: step 11215, loss 0.571163.
Train: 2018-08-05T19:05:13.728344: step 11216, loss 0.544797.
Train: 2018-08-05T19:05:13.931424: step 11217, loss 0.535908.
Train: 2018-08-05T19:05:14.150121: step 11218, loss 0.606906.
Train: 2018-08-05T19:05:14.353198: step 11219, loss 0.642546.
Train: 2018-08-05T19:05:14.556275: step 11220, loss 0.553593.
Test: 2018-08-05T19:05:15.618528: step 11220, loss 0.548096.
Train: 2018-08-05T19:05:15.837225: step 11221, loss 0.606528.
Train: 2018-08-05T19:05:16.040303: step 11222, loss 0.562384.
Train: 2018-08-05T19:05:16.259003: step 11223, loss 0.553652.
Train: 2018-08-05T19:05:16.462079: step 11224, loss 0.553686.
Train: 2018-08-05T19:05:16.665151: step 11225, loss 0.510634.
Train: 2018-08-05T19:05:16.883827: step 11226, loss 0.562336.
Train: 2018-08-05T19:05:17.086927: step 11227, loss 0.579531.
Train: 2018-08-05T19:05:17.305634: step 11228, loss 0.622391.
Train: 2018-08-05T19:05:17.508711: step 11229, loss 0.639109.
Train: 2018-08-05T19:05:17.727409: step 11230, loss 0.477941.
Test: 2018-08-05T19:05:18.774038: step 11230, loss 0.54755.
Train: 2018-08-05T19:05:18.992737: step 11231, loss 0.612869.
Train: 2018-08-05T19:05:19.195816: step 11232, loss 0.554046.
Train: 2018-08-05T19:05:19.398892: step 11233, loss 0.487527.
Train: 2018-08-05T19:05:19.617561: step 11234, loss 0.529109.
Train: 2018-08-05T19:05:19.836289: step 11235, loss 0.612563.
Train: 2018-08-05T19:05:20.054983: step 11236, loss 0.528978.
Train: 2018-08-05T19:05:20.258036: step 11237, loss 0.554019.
Train: 2018-08-05T19:05:20.461143: step 11238, loss 0.612813.
Train: 2018-08-05T19:05:20.679812: step 11239, loss 0.553982.
Train: 2018-08-05T19:05:20.882919: step 11240, loss 0.562383.
Test: 2018-08-05T19:05:21.929520: step 11240, loss 0.548329.
Train: 2018-08-05T19:05:22.179489: step 11241, loss 0.596032.
Train: 2018-08-05T19:05:22.398190: step 11242, loss 0.595995.
Train: 2018-08-05T19:05:22.616888: step 11243, loss 0.570777.
Train: 2018-08-05T19:05:22.819967: step 11244, loss 0.595822.
Train: 2018-08-05T19:05:23.038664: step 11245, loss 0.529195.
Train: 2018-08-05T19:05:23.241711: step 11246, loss 0.51266.
Train: 2018-08-05T19:05:23.444821: step 11247, loss 0.545808.
Train: 2018-08-05T19:05:23.663518: step 11248, loss 0.537383.
Train: 2018-08-05T19:05:23.866565: step 11249, loss 0.545618.
Train: 2018-08-05T19:05:24.085294: step 11250, loss 0.503312.
Test: 2018-08-05T19:05:25.131895: step 11250, loss 0.54832.
Train: 2018-08-05T19:05:25.350623: step 11251, loss 0.630452.
Train: 2018-08-05T19:05:25.553702: step 11252, loss 0.579431.
Train: 2018-08-05T19:05:25.756778: step 11253, loss 0.510926.
Train: 2018-08-05T19:05:25.959856: step 11254, loss 0.51065.
Train: 2018-08-05T19:05:26.178556: step 11255, loss 0.544985.
Train: 2018-08-05T19:05:26.397222: step 11256, loss 0.588634.
Train: 2018-08-05T19:05:26.600330: step 11257, loss 0.492028.
Train: 2018-08-05T19:05:26.803407: step 11258, loss 0.615695.
Train: 2018-08-05T19:05:27.006485: step 11259, loss 0.544682.
Train: 2018-08-05T19:05:27.225154: step 11260, loss 0.589356.
Test: 2018-08-05T19:05:28.271784: step 11260, loss 0.547798.
Train: 2018-08-05T19:05:28.490516: step 11261, loss 0.598356.
Train: 2018-08-05T19:05:28.693559: step 11262, loss 0.464207.
Train: 2018-08-05T19:05:28.896667: step 11263, loss 0.607424.
Train: 2018-08-05T19:05:29.115366: step 11264, loss 0.53565.
Train: 2018-08-05T19:05:29.334065: step 11265, loss 0.553595.
Train: 2018-08-05T19:05:29.537112: step 11266, loss 0.481712.
Train: 2018-08-05T19:05:29.740190: step 11267, loss 0.58069.
Train: 2018-08-05T19:05:29.958889: step 11268, loss 0.508374.
Train: 2018-08-05T19:05:30.161996: step 11269, loss 0.653615.
Train: 2018-08-05T19:05:30.380695: step 11270, loss 0.562694.
Test: 2018-08-05T19:05:31.427324: step 11270, loss 0.548387.
Train: 2018-08-05T19:05:31.630402: step 11271, loss 0.535528.
Train: 2018-08-05T19:05:31.849102: step 11272, loss 0.544581.
Train: 2018-08-05T19:05:32.052178: step 11273, loss 0.517543.
Train: 2018-08-05T19:05:32.270877: step 11274, loss 0.517519.
Train: 2018-08-05T19:05:32.473924: step 11275, loss 0.49028.
Train: 2018-08-05T19:05:32.692655: step 11276, loss 0.507575.
Train: 2018-08-05T19:05:32.911354: step 11277, loss 0.615379.
Train: 2018-08-05T19:05:33.114430: step 11278, loss 0.487845.
Train: 2018-08-05T19:05:33.317509: step 11279, loss 0.553864.
Train: 2018-08-05T19:05:33.536206: step 11280, loss 0.581603.
Test: 2018-08-05T19:05:34.582838: step 11280, loss 0.546494.
Train: 2018-08-05T19:05:34.832779: step 11281, loss 0.535267.
Train: 2018-08-05T19:05:35.082690: step 11282, loss 0.553757.
Train: 2018-08-05T19:05:35.348253: step 11283, loss 0.52601.
Train: 2018-08-05T19:05:35.613814: step 11284, loss 0.563028.
Train: 2018-08-05T19:05:35.863757: step 11285, loss 0.618571.
Train: 2018-08-05T19:05:36.129342: step 11286, loss 0.562942.
Train: 2018-08-05T19:05:36.379288: step 11287, loss 0.544518.
Train: 2018-08-05T19:05:36.644852: step 11288, loss 0.562783.
Train: 2018-08-05T19:05:36.910420: step 11289, loss 0.544551.
Train: 2018-08-05T19:05:37.160362: step 11290, loss 0.571691.
Test: 2018-08-05T19:05:38.206957: step 11290, loss 0.548401.
Train: 2018-08-05T19:05:38.425686: step 11291, loss 0.607545.
Train: 2018-08-05T19:05:38.675628: step 11292, loss 0.642732.
Train: 2018-08-05T19:05:38.878704: step 11293, loss 0.55371.
Train: 2018-08-05T19:05:39.097404: step 11294, loss 0.596203.
Train: 2018-08-05T19:05:39.300451: step 11295, loss 0.516674.
Train: 2018-08-05T19:05:39.519180: step 11296, loss 0.537952.
Train: 2018-08-05T19:05:39.722257: step 11297, loss 0.519973.
Train: 2018-08-05T19:05:39.940957: step 11298, loss 0.553844.
Train: 2018-08-05T19:05:40.144004: step 11299, loss 0.519844.
Train: 2018-08-05T19:05:40.362732: step 11300, loss 0.553818.
Test: 2018-08-05T19:05:41.409363: step 11300, loss 0.548062.
Train: 2018-08-05T19:05:42.487205: step 11301, loss 0.664909.
Train: 2018-08-05T19:05:42.705933: step 11302, loss 0.570857.
Train: 2018-08-05T19:05:42.909011: step 11303, loss 0.562351.
Train: 2018-08-05T19:05:43.112088: step 11304, loss 0.553908.
Train: 2018-08-05T19:05:43.315136: step 11305, loss 0.604534.
Train: 2018-08-05T19:05:43.518214: step 11306, loss 0.56239.
Train: 2018-08-05T19:05:43.736913: step 11307, loss 0.54569.
Train: 2018-08-05T19:05:43.940021: step 11308, loss 0.587451.
Train: 2018-08-05T19:05:44.143068: step 11309, loss 0.562446.
Train: 2018-08-05T19:05:44.361796: step 11310, loss 0.570758.
Test: 2018-08-05T19:05:45.424020: step 11310, loss 0.547523.
Train: 2018-08-05T19:05:45.627124: step 11311, loss 0.512843.
Train: 2018-08-05T19:05:45.830203: step 11312, loss 0.587328.
Train: 2018-08-05T19:05:46.033280: step 11313, loss 0.61219.
Train: 2018-08-05T19:05:46.251978: step 11314, loss 0.579021.
Train: 2018-08-05T19:05:46.455057: step 11315, loss 0.570757.
Train: 2018-08-05T19:05:46.673756: step 11316, loss 0.62006.
Train: 2018-08-05T19:05:46.876802: step 11317, loss 0.578938.
Train: 2018-08-05T19:05:47.079910: step 11318, loss 0.505896.
Train: 2018-08-05T19:05:47.298579: step 11319, loss 0.578907.
Train: 2018-08-05T19:05:47.501656: step 11320, loss 0.514001.
Test: 2018-08-05T19:05:48.548286: step 11320, loss 0.550361.
Train: 2018-08-05T19:05:48.751393: step 11321, loss 0.578918.
Train: 2018-08-05T19:05:48.970063: step 11322, loss 0.562615.
Train: 2018-08-05T19:05:49.173171: step 11323, loss 0.587128.
Train: 2018-08-05T19:05:49.376247: step 11324, loss 0.644503.
Train: 2018-08-05T19:05:49.548052: step 11325, loss 0.632284.
Train: 2018-08-05T19:05:49.766780: step 11326, loss 0.489709.
Train: 2018-08-05T19:05:49.969828: step 11327, loss 0.562694.
Train: 2018-08-05T19:05:50.172905: step 11328, loss 0.562694.
Train: 2018-08-05T19:05:50.391628: step 11329, loss 0.530251.
Train: 2018-08-05T19:05:50.594682: step 11330, loss 0.578919.
Test: 2018-08-05T19:05:51.641341: step 11330, loss 0.549537.
Train: 2018-08-05T19:05:51.844419: step 11331, loss 0.562607.
Train: 2018-08-05T19:05:52.063088: step 11332, loss 0.554357.
Train: 2018-08-05T19:05:52.266165: step 11333, loss 0.587463.
Train: 2018-08-05T19:05:52.484864: step 11334, loss 0.603789.
Train: 2018-08-05T19:05:52.687972: step 11335, loss 0.570757.
Train: 2018-08-05T19:05:52.891049: step 11336, loss 0.587222.
Train: 2018-08-05T19:05:53.094096: step 11337, loss 0.537873.
Train: 2018-08-05T19:05:53.312826: step 11338, loss 0.554298.
Train: 2018-08-05T19:05:53.515904: step 11339, loss 0.521283.
Train: 2018-08-05T19:05:53.734601: step 11340, loss 0.570757.
Test: 2018-08-05T19:05:54.781201: step 11340, loss 0.549691.
Train: 2018-08-05T19:05:54.984308: step 11341, loss 0.56244.
Train: 2018-08-05T19:05:55.187387: step 11342, loss 0.520627.
Train: 2018-08-05T19:05:55.390463: step 11343, loss 0.562379.
Train: 2018-08-05T19:05:55.593541: step 11344, loss 0.503083.
Train: 2018-08-05T19:05:55.812234: step 11345, loss 0.587982.
Train: 2018-08-05T19:05:56.015317: step 11346, loss 0.605375.
Train: 2018-08-05T19:05:56.218365: step 11347, loss 0.588251.
Train: 2018-08-05T19:05:56.421474: step 11348, loss 0.579638.
Train: 2018-08-05T19:05:56.640170: step 11349, loss 0.545046.
Train: 2018-08-05T19:05:56.843248: step 11350, loss 0.475794.
Test: 2018-08-05T19:05:57.905499: step 11350, loss 0.547417.
Train: 2018-08-05T19:05:58.108577: step 11351, loss 0.510121.
Train: 2018-08-05T19:05:58.311656: step 11352, loss 0.536058.
Train: 2018-08-05T19:05:58.514732: step 11353, loss 0.571302.
Train: 2018-08-05T19:05:58.733431: step 11354, loss 0.571416.
Train: 2018-08-05T19:05:58.936509: step 11355, loss 0.562548.
Train: 2018-08-05T19:05:59.155178: step 11356, loss 0.490666.
Train: 2018-08-05T19:05:59.358284: step 11357, loss 0.490252.
Train: 2018-08-05T19:05:59.576983: step 11358, loss 0.553666.
Train: 2018-08-05T19:05:59.780061: step 11359, loss 0.52608.
Train: 2018-08-05T19:05:59.983137: step 11360, loss 0.507337.
Test: 2018-08-05T19:06:01.045390: step 11360, loss 0.547641.
Train: 2018-08-05T19:06:01.248437: step 11361, loss 0.563333.
Train: 2018-08-05T19:06:01.451513: step 11362, loss 0.58237.
Train: 2018-08-05T19:06:01.654621: step 11363, loss 0.544592.
Train: 2018-08-05T19:06:01.857698: step 11364, loss 0.535098.
Train: 2018-08-05T19:06:02.060778: step 11365, loss 0.563719.
Train: 2018-08-05T19:06:02.263854: step 11366, loss 0.544642.
Train: 2018-08-05T19:06:02.466902: step 11367, loss 0.535091.
Train: 2018-08-05T19:06:02.670008: step 11368, loss 0.621085.
Train: 2018-08-05T19:06:02.888707: step 11369, loss 0.620214.
Train: 2018-08-05T19:06:03.091785: step 11370, loss 0.51785.
Test: 2018-08-05T19:06:04.138385: step 11370, loss 0.548177.
Train: 2018-08-05T19:06:04.341463: step 11371, loss 0.598878.
Train: 2018-08-05T19:06:04.560161: step 11372, loss 0.579758.
Train: 2018-08-05T19:06:04.763268: step 11373, loss 0.547025.
Train: 2018-08-05T19:06:04.966346: step 11374, loss 0.611283.
Train: 2018-08-05T19:06:05.169422: step 11375, loss 0.617176.
Train: 2018-08-05T19:06:05.388092: step 11376, loss 0.575426.
Train: 2018-08-05T19:06:05.591199: step 11377, loss 0.554362.
Train: 2018-08-05T19:06:05.809868: step 11378, loss 0.541049.
Train: 2018-08-05T19:06:06.012976: step 11379, loss 0.579267.
Train: 2018-08-05T19:06:06.231675: step 11380, loss 0.535894.
Test: 2018-08-05T19:06:07.293895: step 11380, loss 0.54841.
Train: 2018-08-05T19:06:07.497003: step 11381, loss 0.526693.
Train: 2018-08-05T19:06:07.700080: step 11382, loss 0.616547.
Train: 2018-08-05T19:06:07.903158: step 11383, loss 0.580536.
Train: 2018-08-05T19:06:08.106236: step 11384, loss 0.562544.
Train: 2018-08-05T19:06:08.324934: step 11385, loss 0.553588.
Train: 2018-08-05T19:06:08.528012: step 11386, loss 0.509115.
Train: 2018-08-05T19:06:08.731089: step 11387, loss 0.562482.
Train: 2018-08-05T19:06:08.934168: step 11388, loss 0.598021.
Train: 2018-08-05T19:06:09.137244: step 11389, loss 0.606729.
Train: 2018-08-05T19:06:09.340322: step 11390, loss 0.624011.
Test: 2018-08-05T19:06:10.402542: step 11390, loss 0.549814.
Train: 2018-08-05T19:06:10.621272: step 11391, loss 0.597216.
Train: 2018-08-05T19:06:10.824350: step 11392, loss 0.519262.
Train: 2018-08-05T19:06:11.027427: step 11393, loss 0.562337.
Train: 2018-08-05T19:06:11.230505: step 11394, loss 0.545371.
Train: 2018-08-05T19:06:11.449173: step 11395, loss 0.545474.
Train: 2018-08-05T19:06:11.652280: step 11396, loss 0.638126.
Train: 2018-08-05T19:06:11.855361: step 11397, loss 0.570769.
Train: 2018-08-05T19:06:12.058436: step 11398, loss 0.521008.
Train: 2018-08-05T19:06:12.261482: step 11399, loss 0.612074.
Train: 2018-08-05T19:06:12.480211: step 11400, loss 0.554328.
Test: 2018-08-05T19:06:13.526841: step 11400, loss 0.549298.
Train: 2018-08-05T19:06:14.464122: step 11401, loss 0.529859.
Train: 2018-08-05T19:06:14.667200: step 11402, loss 0.587115.
Train: 2018-08-05T19:06:14.870277: step 11403, loss 0.464722.
Train: 2018-08-05T19:06:15.073353: step 11404, loss 0.611771.
Train: 2018-08-05T19:06:15.276433: step 11405, loss 0.595418.
Train: 2018-08-05T19:06:15.495130: step 11406, loss 0.611872.
Train: 2018-08-05T19:06:15.698208: step 11407, loss 0.505136.
Train: 2018-08-05T19:06:15.901285: step 11408, loss 0.628307.
Train: 2018-08-05T19:06:16.104365: step 11409, loss 0.529732.
Train: 2018-08-05T19:06:16.307410: step 11410, loss 0.505036.
Test: 2018-08-05T19:06:17.369691: step 11410, loss 0.549655.
Train: 2018-08-05T19:06:17.572739: step 11411, loss 0.645086.
Train: 2018-08-05T19:06:17.775846: step 11412, loss 0.612044.
Train: 2018-08-05T19:06:17.978894: step 11413, loss 0.521355.
Train: 2018-08-05T19:06:18.182003: step 11414, loss 0.55428.
Train: 2018-08-05T19:06:18.385078: step 11415, loss 0.521246.
Train: 2018-08-05T19:06:18.588157: step 11416, loss 0.554178.
Train: 2018-08-05T19:06:18.806856: step 11417, loss 0.554101.
Train: 2018-08-05T19:06:19.009933: step 11418, loss 0.570777.
Train: 2018-08-05T19:06:19.213010: step 11419, loss 0.587626.
Train: 2018-08-05T19:06:19.400465: step 11420, loss 0.53705.
Test: 2018-08-05T19:06:20.462717: step 11420, loss 0.547809.
Train: 2018-08-05T19:06:20.665796: step 11421, loss 0.562353.
Train: 2018-08-05T19:06:20.868872: step 11422, loss 0.536819.
Train: 2018-08-05T19:06:21.087541: step 11423, loss 0.493915.
Train: 2018-08-05T19:06:21.274997: step 11424, loss 0.53645.
Train: 2018-08-05T19:06:21.478104: step 11425, loss 0.579783.
Train: 2018-08-05T19:06:21.696803: step 11426, loss 0.562391.
Train: 2018-08-05T19:06:21.915504: step 11427, loss 0.571253.
Train: 2018-08-05T19:06:22.118579: step 11428, loss 0.589055.
Train: 2018-08-05T19:06:22.321628: step 11429, loss 0.50918.
Train: 2018-08-05T19:06:22.524734: step 11430, loss 0.624949.
Test: 2018-08-05T19:06:23.571334: step 11430, loss 0.547819.
Train: 2018-08-05T19:06:23.774411: step 11431, loss 0.553588.
Train: 2018-08-05T19:06:23.993111: step 11432, loss 0.535767.
Train: 2018-08-05T19:06:24.196217: step 11433, loss 0.598163.
Train: 2018-08-05T19:06:24.399295: step 11434, loss 0.624743.
Train: 2018-08-05T19:06:24.602373: step 11435, loss 0.553597.
Train: 2018-08-05T19:06:24.789828: step 11436, loss 0.579971.
Train: 2018-08-05T19:06:25.008498: step 11437, loss 0.597278.
Train: 2018-08-05T19:06:25.195954: step 11438, loss 0.536382.
Train: 2018-08-05T19:06:25.399061: step 11439, loss 0.545136.
Train: 2018-08-05T19:06:25.602138: step 11440, loss 0.502417.
Test: 2018-08-05T19:06:26.664391: step 11440, loss 0.549426.
Train: 2018-08-05T19:06:26.867436: step 11441, loss 0.545224.
Train: 2018-08-05T19:06:27.070544: step 11442, loss 0.570897.
Train: 2018-08-05T19:06:27.273593: step 11443, loss 0.562336.
Train: 2018-08-05T19:06:27.476670: step 11444, loss 0.528091.
Train: 2018-08-05T19:06:27.664126: step 11445, loss 0.588072.
Train: 2018-08-05T19:06:27.867234: step 11446, loss 0.605239.
Train: 2018-08-05T19:06:28.070281: step 11447, loss 0.53666.
Train: 2018-08-05T19:06:28.288980: step 11448, loss 0.553784.
Train: 2018-08-05T19:06:28.492087: step 11449, loss 0.579439.
Train: 2018-08-05T19:06:28.695164: step 11450, loss 0.511093.
Test: 2018-08-05T19:06:29.741794: step 11450, loss 0.548042.
Train: 2018-08-05T19:06:29.944842: step 11451, loss 0.536659.
Train: 2018-08-05T19:06:30.147949: step 11452, loss 0.562335.
Train: 2018-08-05T19:06:30.351027: step 11453, loss 0.519258.
Train: 2018-08-05T19:06:30.554105: step 11454, loss 0.562344.
Train: 2018-08-05T19:06:30.757181: step 11455, loss 0.658077.
Train: 2018-08-05T19:06:30.960259: step 11456, loss 0.553667.
Train: 2018-08-05T19:06:31.163307: step 11457, loss 0.518996.
Train: 2018-08-05T19:06:31.382035: step 11458, loss 0.640469.
Train: 2018-08-05T19:06:31.585112: step 11459, loss 0.562341.
Train: 2018-08-05T19:06:31.803812: step 11460, loss 0.553726.
Test: 2018-08-05T19:06:32.850412: step 11460, loss 0.548395.
Train: 2018-08-05T19:06:33.053520: step 11461, loss 0.545168.
Train: 2018-08-05T19:06:33.256566: step 11462, loss 0.570904.
Train: 2018-08-05T19:06:33.459644: step 11463, loss 0.613635.
Train: 2018-08-05T19:06:33.662753: step 11464, loss 0.596365.
Train: 2018-08-05T19:06:33.865799: step 11465, loss 0.537021.
Train: 2018-08-05T19:06:34.068906: step 11466, loss 0.621253.
Train: 2018-08-05T19:06:34.271953: step 11467, loss 0.562421.
Train: 2018-08-05T19:06:34.490678: step 11468, loss 0.520997.
Train: 2018-08-05T19:06:34.693730: step 11469, loss 0.504588.
Train: 2018-08-05T19:06:34.881216: step 11470, loss 0.554186.
Test: 2018-08-05T19:06:35.943437: step 11470, loss 0.548005.
Train: 2018-08-05T19:06:36.146545: step 11471, loss 0.595681.
Train: 2018-08-05T19:06:36.349621: step 11472, loss 0.662189.
Train: 2018-08-05T19:06:36.552700: step 11473, loss 0.521165.
Train: 2018-08-05T19:06:36.755777: step 11474, loss 0.570756.
Train: 2018-08-05T19:06:36.958855: step 11475, loss 0.603698.
Train: 2018-08-05T19:06:37.115068: step 11476, loss 0.527552.
Train: 2018-08-05T19:06:37.333736: step 11477, loss 0.546162.
Train: 2018-08-05T19:06:37.536845: step 11478, loss 0.57076.
Train: 2018-08-05T19:06:37.739922: step 11479, loss 0.537885.
Train: 2018-08-05T19:06:37.942998: step 11480, loss 0.513047.
Test: 2018-08-05T19:06:38.989599: step 11480, loss 0.547455.
Train: 2018-08-05T19:06:39.208297: step 11481, loss 0.545864.
Train: 2018-08-05T19:06:39.411404: step 11482, loss 0.495536.
Train: 2018-08-05T19:06:39.614482: step 11483, loss 0.503186.
Train: 2018-08-05T19:06:39.817530: step 11484, loss 0.536619.
Train: 2018-08-05T19:06:40.020637: step 11485, loss 0.553661.
Train: 2018-08-05T19:06:40.223714: step 11486, loss 0.624044.
Train: 2018-08-05T19:06:40.426793: step 11487, loss 0.597936.
Train: 2018-08-05T19:06:40.629869: step 11488, loss 0.517976.
Train: 2018-08-05T19:06:40.832916: step 11489, loss 0.571495.
Train: 2018-08-05T19:06:41.036026: step 11490, loss 0.54461.
Test: 2018-08-05T19:06:42.082653: step 11490, loss 0.547354.
Train: 2018-08-05T19:06:42.301354: step 11491, loss 0.616743.
Train: 2018-08-05T19:06:42.504431: step 11492, loss 0.634725.
Train: 2018-08-05T19:06:42.707502: step 11493, loss 0.517743.
Train: 2018-08-05T19:06:42.910587: step 11494, loss 0.553589.
Train: 2018-08-05T19:06:43.098041: step 11495, loss 0.580312.
Train: 2018-08-05T19:06:43.301120: step 11496, loss 0.56246.
Train: 2018-08-05T19:06:43.519788: step 11497, loss 0.580088.
Train: 2018-08-05T19:06:43.722866: step 11498, loss 0.571176.
Train: 2018-08-05T19:06:43.925974: step 11499, loss 0.483809.
Train: 2018-08-05T19:06:44.129019: step 11500, loss 0.544921.
Test: 2018-08-05T19:06:45.175681: step 11500, loss 0.5482.
Train: 2018-08-05T19:06:46.284765: step 11501, loss 0.562364.
Train: 2018-08-05T19:06:46.534735: step 11502, loss 0.510046.
Train: 2018-08-05T19:06:46.784650: step 11503, loss 0.536145.
Train: 2018-08-05T19:06:47.034590: step 11504, loss 0.518513.
Train: 2018-08-05T19:06:47.300181: step 11505, loss 0.562425.
Train: 2018-08-05T19:06:47.534502: step 11506, loss 0.571324.
Train: 2018-08-05T19:06:47.784448: step 11507, loss 0.615842.
Train: 2018-08-05T19:06:48.034357: step 11508, loss 0.52695.
Train: 2018-08-05T19:06:48.268676: step 11509, loss 0.500075.
Train: 2018-08-05T19:06:48.471784: step 11510, loss 0.618553.
Test: 2018-08-05T19:06:49.534029: step 11510, loss 0.549025.
Train: 2018-08-05T19:06:49.737112: step 11511, loss 0.607463.
Train: 2018-08-05T19:06:49.940158: step 11512, loss 0.55359.
Train: 2018-08-05T19:06:50.143268: step 11513, loss 0.535921.
Train: 2018-08-05T19:06:50.346344: step 11514, loss 0.615313.
Train: 2018-08-05T19:06:50.549423: step 11515, loss 0.52733.
Train: 2018-08-05T19:06:50.752500: step 11516, loss 0.606031.
Train: 2018-08-05T19:06:50.971198: step 11517, loss 0.544993.
Train: 2018-08-05T19:06:51.174276: step 11518, loss 0.562339.
Train: 2018-08-05T19:06:51.377353: step 11519, loss 0.519344.
Train: 2018-08-05T19:06:51.580399: step 11520, loss 0.527991.
Test: 2018-08-05T19:06:52.642651: step 11520, loss 0.547589.
Train: 2018-08-05T19:06:52.845728: step 11521, loss 0.545149.
Train: 2018-08-05T19:06:53.048838: step 11522, loss 0.605375.
Train: 2018-08-05T19:06:53.251913: step 11523, loss 0.545141.
Train: 2018-08-05T19:06:53.454991: step 11524, loss 0.562335.
Train: 2018-08-05T19:06:53.673691: step 11525, loss 0.579522.
Train: 2018-08-05T19:06:53.876761: step 11526, loss 0.536589.
Train: 2018-08-05T19:06:54.079845: step 11527, loss 0.510827.
Train: 2018-08-05T19:06:54.282924: step 11528, loss 0.527879.
Train: 2018-08-05T19:06:54.485999: step 11529, loss 0.562344.
Train: 2018-08-05T19:06:54.689077: step 11530, loss 0.510171.
Test: 2018-08-05T19:06:55.747249: step 11530, loss 0.548763.
Train: 2018-08-05T19:06:55.950326: step 11531, loss 0.553624.
Train: 2018-08-05T19:06:56.153403: step 11532, loss 0.45664.
Train: 2018-08-05T19:06:56.356451: step 11533, loss 0.589266.
Train: 2018-08-05T19:06:56.559529: step 11534, loss 0.625547.
Train: 2018-08-05T19:06:56.778257: step 11535, loss 0.517526.
Train: 2018-08-05T19:06:56.981305: step 11536, loss 0.62611.
Train: 2018-08-05T19:06:57.184412: step 11537, loss 0.526451.
Train: 2018-08-05T19:06:57.387489: step 11538, loss 0.499242.
Train: 2018-08-05T19:06:57.590536: step 11539, loss 0.580929.
Train: 2018-08-05T19:06:57.793645: step 11540, loss 0.59008.
Test: 2018-08-05T19:06:58.840244: step 11540, loss 0.546916.
Train: 2018-08-05T19:06:59.043351: step 11541, loss 0.599119.
Train: 2018-08-05T19:06:59.262050: step 11542, loss 0.626074.
Train: 2018-08-05T19:06:59.449477: step 11543, loss 0.598495.
Train: 2018-08-05T19:06:59.668205: step 11544, loss 0.500277.
Train: 2018-08-05T19:06:59.871284: step 11545, loss 0.571256.
Train: 2018-08-05T19:07:00.074356: step 11546, loss 0.483463.
Train: 2018-08-05T19:07:00.277437: step 11547, loss 0.571136.
Train: 2018-08-05T19:07:00.496106: step 11548, loss 0.588575.
Train: 2018-08-05T19:07:00.699214: step 11549, loss 0.649352.
Train: 2018-08-05T19:07:00.902294: step 11550, loss 0.588194.
Test: 2018-08-05T19:07:01.964543: step 11550, loss 0.546127.
Train: 2018-08-05T19:07:02.167620: step 11551, loss 0.528212.
Train: 2018-08-05T19:07:02.370698: step 11552, loss 0.596242.
Train: 2018-08-05T19:07:02.573745: step 11553, loss 0.587589.
Train: 2018-08-05T19:07:02.776852: step 11554, loss 0.654025.
Train: 2018-08-05T19:07:02.979931: step 11555, loss 0.57076.
Train: 2018-08-05T19:07:03.183007: step 11556, loss 0.635686.
Train: 2018-08-05T19:07:03.386085: step 11557, loss 0.546939.
Train: 2018-08-05T19:07:03.589162: step 11558, loss 0.578863.
Train: 2018-08-05T19:07:03.807861: step 11559, loss 0.547649.
Train: 2018-08-05T19:07:04.010908: step 11560, loss 0.602174.
Test: 2018-08-05T19:07:05.057568: step 11560, loss 0.549854.
Train: 2018-08-05T19:07:05.260616: step 11561, loss 0.586653.
Train: 2018-08-05T19:07:05.463723: step 11562, loss 0.601914.
Train: 2018-08-05T19:07:05.666800: step 11563, loss 0.57908.
Train: 2018-08-05T19:07:05.885489: step 11564, loss 0.533998.
Train: 2018-08-05T19:07:06.088577: step 11565, loss 0.534108.
Train: 2018-08-05T19:07:06.291654: step 11566, loss 0.549041.
Train: 2018-08-05T19:07:06.494701: step 11567, loss 0.488366.
Train: 2018-08-05T19:07:06.697809: step 11568, loss 0.556063.
Train: 2018-08-05T19:07:06.900886: step 11569, loss 0.547964.
Train: 2018-08-05T19:07:07.103933: step 11570, loss 0.578866.
Test: 2018-08-05T19:07:08.166215: step 11570, loss 0.550251.
Train: 2018-08-05T19:07:08.369263: step 11571, loss 0.490301.
Train: 2018-08-05T19:07:08.572370: step 11572, loss 0.579808.
Train: 2018-08-05T19:07:08.775418: step 11573, loss 0.508484.
Train: 2018-08-05T19:07:08.978527: step 11574, loss 0.545786.
Train: 2018-08-05T19:07:09.181602: step 11575, loss 0.587653.
Train: 2018-08-05T19:07:09.400301: step 11576, loss 0.563578.
Train: 2018-08-05T19:07:09.619001: step 11577, loss 0.606758.
Train: 2018-08-05T19:07:09.822077: step 11578, loss 0.511528.
Train: 2018-08-05T19:07:10.025155: step 11579, loss 0.604775.
Train: 2018-08-05T19:07:10.228203: step 11580, loss 0.647219.
Test: 2018-08-05T19:07:11.290484: step 11580, loss 0.548845.
Train: 2018-08-05T19:07:11.493531: step 11581, loss 0.528588.
Train: 2018-08-05T19:07:11.696639: step 11582, loss 0.537098.
Train: 2018-08-05T19:07:11.899716: step 11583, loss 0.579221.
Train: 2018-08-05T19:07:12.118416: step 11584, loss 0.638105.
Train: 2018-08-05T19:07:12.321493: step 11585, loss 0.537306.
Train: 2018-08-05T19:07:12.540185: step 11586, loss 0.62081.
Train: 2018-08-05T19:07:12.743271: step 11587, loss 0.587334.
Train: 2018-08-05T19:07:12.946346: step 11588, loss 0.513136.
Train: 2018-08-05T19:07:13.149423: step 11589, loss 0.57076.
Train: 2018-08-05T19:07:13.352502: step 11590, loss 0.595345.
Test: 2018-08-05T19:07:14.414723: step 11590, loss 0.548971.
Train: 2018-08-05T19:07:14.617833: step 11591, loss 0.578935.
Train: 2018-08-05T19:07:14.820878: step 11592, loss 0.546381.
Train: 2018-08-05T19:07:15.023984: step 11593, loss 0.481437.
Train: 2018-08-05T19:07:15.227031: step 11594, loss 0.595255.
Train: 2018-08-05T19:07:15.430109: step 11595, loss 0.595315.
Train: 2018-08-05T19:07:15.633216: step 11596, loss 0.546193.
Train: 2018-08-05T19:07:15.836294: step 11597, loss 0.562549.
Train: 2018-08-05T19:07:16.039374: step 11598, loss 0.595458.
Train: 2018-08-05T19:07:16.258041: step 11599, loss 0.537795.
Train: 2018-08-05T19:07:16.461147: step 11600, loss 0.562492.
Test: 2018-08-05T19:07:17.507748: step 11600, loss 0.548624.
Train: 2018-08-05T19:07:18.491893: step 11601, loss 0.570758.
Train: 2018-08-05T19:07:18.694999: step 11602, loss 0.678808.
Train: 2018-08-05T19:07:18.898077: step 11603, loss 0.537669.
Train: 2018-08-05T19:07:19.101156: step 11604, loss 0.579009.
Train: 2018-08-05T19:07:19.319854: step 11605, loss 0.504904.
Train: 2018-08-05T19:07:19.522931: step 11606, loss 0.529519.
Train: 2018-08-05T19:07:19.725978: step 11607, loss 0.57904.
Train: 2018-08-05T19:07:19.944707: step 11608, loss 0.579071.
Train: 2018-08-05T19:07:20.147785: step 11609, loss 0.579093.
Train: 2018-08-05T19:07:20.366484: step 11610, loss 0.570766.
Test: 2018-08-05T19:07:21.413113: step 11610, loss 0.549247.
Train: 2018-08-05T19:07:21.616192: step 11611, loss 0.529028.
Train: 2018-08-05T19:07:21.834889: step 11612, loss 0.503769.
Train: 2018-08-05T19:07:22.037937: step 11613, loss 0.553934.
Train: 2018-08-05T19:07:22.241045: step 11614, loss 0.604819.
Train: 2018-08-05T19:07:22.444122: step 11615, loss 0.536763.
Train: 2018-08-05T19:07:22.647169: step 11616, loss 0.570904.
Train: 2018-08-05T19:07:22.850278: step 11617, loss 0.476334.
Train: 2018-08-05T19:07:23.053354: step 11618, loss 0.649114.
Train: 2018-08-05T19:07:23.272053: step 11619, loss 0.553659.
Train: 2018-08-05T19:07:23.475132: step 11620, loss 0.562362.
Test: 2018-08-05T19:07:24.537381: step 11620, loss 0.547988.
Train: 2018-08-05T19:07:24.740460: step 11621, loss 0.536175.
Train: 2018-08-05T19:07:24.943536: step 11622, loss 0.562381.
Train: 2018-08-05T19:07:25.146614: step 11623, loss 0.597498.
Train: 2018-08-05T19:07:25.365314: step 11624, loss 0.615021.
Train: 2018-08-05T19:07:25.568390: step 11625, loss 0.579843.
Train: 2018-08-05T19:07:25.771468: step 11626, loss 0.518893.
Train: 2018-08-05T19:07:25.943302: step 11627, loss 0.52534.
Train: 2018-08-05T19:07:26.162002: step 11628, loss 0.544998.
Train: 2018-08-05T19:07:26.365079: step 11629, loss 0.56235.
Train: 2018-08-05T19:07:26.568157: step 11630, loss 0.571042.
Test: 2018-08-05T19:07:27.630376: step 11630, loss 0.547243.
Train: 2018-08-05T19:07:27.849076: step 11631, loss 0.571039.
Train: 2018-08-05T19:07:28.052183: step 11632, loss 0.510273.
Train: 2018-08-05T19:07:28.255231: step 11633, loss 0.544959.
Train: 2018-08-05T19:07:28.458309: step 11634, loss 0.536199.
Train: 2018-08-05T19:07:28.661416: step 11635, loss 0.483584.
Train: 2018-08-05T19:07:28.864465: step 11636, loss 0.61537.
Train: 2018-08-05T19:07:29.083192: step 11637, loss 0.615573.
Train: 2018-08-05T19:07:29.286240: step 11638, loss 0.553594.
Train: 2018-08-05T19:07:29.489348: step 11639, loss 0.571281.
Train: 2018-08-05T19:07:29.692424: step 11640, loss 0.553598.
Test: 2018-08-05T19:07:30.754670: step 11640, loss 0.547288.
Train: 2018-08-05T19:07:30.957753: step 11641, loss 0.606503.
Train: 2018-08-05T19:07:31.160830: step 11642, loss 0.571172.
Train: 2018-08-05T19:07:31.379500: step 11643, loss 0.571106.
Train: 2018-08-05T19:07:31.582607: step 11644, loss 0.579733.
Train: 2018-08-05T19:07:31.785684: step 11645, loss 0.605527.
Train: 2018-08-05T19:07:32.004384: step 11646, loss 0.639427.
Train: 2018-08-05T19:07:32.207462: step 11647, loss 0.536979.
Train: 2018-08-05T19:07:32.410538: step 11648, loss 0.604298.
Train: 2018-08-05T19:07:32.629236: step 11649, loss 0.612198.
Train: 2018-08-05T19:07:32.832283: step 11650, loss 0.5544.
Test: 2018-08-05T19:07:33.878944: step 11650, loss 0.548989.
Train: 2018-08-05T19:07:34.097637: step 11651, loss 0.611288.
Train: 2018-08-05T19:07:34.300720: step 11652, loss 0.554858.
Train: 2018-08-05T19:07:34.503792: step 11653, loss 0.594714.
Train: 2018-08-05T19:07:34.722495: step 11654, loss 0.539613.
Train: 2018-08-05T19:07:34.925544: step 11655, loss 0.508635.
Train: 2018-08-05T19:07:35.144273: step 11656, loss 0.602303.
Train: 2018-08-05T19:07:35.347352: step 11657, loss 0.571106.
Train: 2018-08-05T19:07:35.566049: step 11658, loss 0.56333.
Train: 2018-08-05T19:07:35.769097: step 11659, loss 0.617843.
Train: 2018-08-05T19:07:35.972202: step 11660, loss 0.563358.
Test: 2018-08-05T19:07:37.034424: step 11660, loss 0.550032.
Train: 2018-08-05T19:07:37.237533: step 11661, loss 0.563367.
Train: 2018-08-05T19:07:37.440610: step 11662, loss 0.493356.
Train: 2018-08-05T19:07:37.643690: step 11663, loss 0.563217.
Train: 2018-08-05T19:07:37.862382: step 11664, loss 0.586753.
Train: 2018-08-05T19:07:38.081086: step 11665, loss 0.547107.
Train: 2018-08-05T19:07:38.299784: step 11666, loss 0.530868.
Train: 2018-08-05T19:07:38.502862: step 11667, loss 0.56267.
Train: 2018-08-05T19:07:38.752802: step 11668, loss 0.545465.
Train: 2018-08-05T19:07:38.971498: step 11669, loss 0.535108.
Train: 2018-08-05T19:07:39.190200: step 11670, loss 0.605573.
Test: 2018-08-05T19:07:40.236800: step 11670, loss 0.548356.
Train: 2018-08-05T19:07:40.455529: step 11671, loss 0.525363.
Train: 2018-08-05T19:07:40.658606: step 11672, loss 0.553626.
Train: 2018-08-05T19:07:40.861685: step 11673, loss 0.510506.
Train: 2018-08-05T19:07:41.080353: step 11674, loss 0.51989.
Train: 2018-08-05T19:07:41.283461: step 11675, loss 0.570824.
Train: 2018-08-05T19:07:41.486538: step 11676, loss 0.57083.
Train: 2018-08-05T19:07:41.689614: step 11677, loss 0.664459.
Train: 2018-08-05T19:07:41.908315: step 11678, loss 0.536886.
Train: 2018-08-05T19:07:42.111392: step 11679, loss 0.587798.
Train: 2018-08-05T19:07:42.330090: step 11680, loss 0.545426.
Test: 2018-08-05T19:07:43.376720: step 11680, loss 0.547841.
Train: 2018-08-05T19:07:43.579798: step 11681, loss 0.562358.
Train: 2018-08-05T19:07:43.798496: step 11682, loss 0.536993.
Train: 2018-08-05T19:07:44.001573: step 11683, loss 0.48615.
Train: 2018-08-05T19:07:44.204621: step 11684, loss 0.55382.
Train: 2018-08-05T19:07:44.423350: step 11685, loss 0.630956.
Train: 2018-08-05T19:07:44.626428: step 11686, loss 0.536587.
Train: 2018-08-05T19:07:44.829505: step 11687, loss 0.484891.
Train: 2018-08-05T19:07:45.032552: step 11688, loss 0.519015.
Train: 2018-08-05T19:07:45.235630: step 11689, loss 0.51866.
Train: 2018-08-05T19:07:45.454358: step 11690, loss 0.465267.
Test: 2018-08-05T19:07:46.500988: step 11690, loss 0.548205.
Train: 2018-08-05T19:07:46.704066: step 11691, loss 0.580483.
Train: 2018-08-05T19:07:46.922734: step 11692, loss 0.598933.
Train: 2018-08-05T19:07:47.125812: step 11693, loss 0.526392.
Train: 2018-08-05T19:07:47.344540: step 11694, loss 0.572085.
Train: 2018-08-05T19:07:47.547588: step 11695, loss 0.52603.
Train: 2018-08-05T19:07:47.750698: step 11696, loss 0.535212.
Train: 2018-08-05T19:07:47.969396: step 11697, loss 0.57258.
Train: 2018-08-05T19:07:48.172442: step 11698, loss 0.544535.
Train: 2018-08-05T19:07:48.375549: step 11699, loss 0.488116.
Train: 2018-08-05T19:07:48.594250: step 11700, loss 0.639106.
Test: 2018-08-05T19:07:49.640878: step 11700, loss 0.547245.
Train: 2018-08-05T19:07:50.593781: step 11701, loss 0.601221.
Train: 2018-08-05T19:07:50.796828: step 11702, loss 0.56334.
Train: 2018-08-05T19:07:50.999936: step 11703, loss 0.553869.
Train: 2018-08-05T19:07:51.218604: step 11704, loss 0.600266.
Train: 2018-08-05T19:07:51.421680: step 11705, loss 0.590572.
Train: 2018-08-05T19:07:51.624789: step 11706, loss 0.526303.
Train: 2018-08-05T19:07:51.843458: step 11707, loss 0.598813.
Train: 2018-08-05T19:07:52.046564: step 11708, loss 0.589371.
Train: 2018-08-05T19:07:52.265263: step 11709, loss 0.562436.
Train: 2018-08-05T19:07:52.468342: step 11710, loss 0.579854.
Test: 2018-08-05T19:07:53.530592: step 11710, loss 0.548906.
Train: 2018-08-05T19:07:53.733639: step 11711, loss 0.527783.
Train: 2018-08-05T19:07:53.936747: step 11712, loss 0.562335.
Train: 2018-08-05T19:07:54.155446: step 11713, loss 0.519845.
Train: 2018-08-05T19:07:54.358524: step 11714, loss 0.503097.
Train: 2018-08-05T19:07:54.561600: step 11715, loss 0.562353.
Train: 2018-08-05T19:07:54.780299: step 11716, loss 0.630279.
Train: 2018-08-05T19:07:54.983347: step 11717, loss 0.51173.
Train: 2018-08-05T19:07:55.186454: step 11718, loss 0.528611.
Train: 2018-08-05T19:07:55.389502: step 11719, loss 0.672322.
Train: 2018-08-05T19:07:55.608232: step 11720, loss 0.612881.
Test: 2018-08-05T19:07:56.654860: step 11720, loss 0.548464.
Train: 2018-08-05T19:07:56.857939: step 11721, loss 0.545707.
Train: 2018-08-05T19:07:57.061016: step 11722, loss 0.562451.
Train: 2018-08-05T19:07:57.279714: step 11723, loss 0.612119.
Train: 2018-08-05T19:07:57.482761: step 11724, loss 0.4886.
Train: 2018-08-05T19:07:57.701491: step 11725, loss 0.57076.
Train: 2018-08-05T19:07:57.920190: step 11726, loss 0.554349.
Train: 2018-08-05T19:07:58.123267: step 11727, loss 0.562549.
Train: 2018-08-05T19:07:58.341936: step 11728, loss 0.578979.
Train: 2018-08-05T19:07:58.545042: step 11729, loss 0.587206.
Train: 2018-08-05T19:07:58.763711: step 11730, loss 0.628286.
Test: 2018-08-05T19:07:59.810342: step 11730, loss 0.549674.
Train: 2018-08-05T19:08:00.029041: step 11731, loss 0.53804.
Train: 2018-08-05T19:08:00.232149: step 11732, loss 0.587104.
Train: 2018-08-05T19:08:00.435225: step 11733, loss 0.546338.
Train: 2018-08-05T19:08:00.638303: step 11734, loss 0.587062.
Train: 2018-08-05T19:08:00.856972: step 11735, loss 0.570783.
Train: 2018-08-05T19:08:01.075700: step 11736, loss 0.530185.
Train: 2018-08-05T19:08:01.278778: step 11737, loss 0.521968.
Train: 2018-08-05T19:08:01.497445: step 11738, loss 0.546237.
Train: 2018-08-05T19:08:01.700554: step 11739, loss 0.554303.
Train: 2018-08-05T19:08:01.903631: step 11740, loss 0.496215.
Test: 2018-08-05T19:08:02.965884: step 11740, loss 0.548038.
Train: 2018-08-05T19:08:03.184582: step 11741, loss 0.545661.
Train: 2018-08-05T19:08:03.387658: step 11742, loss 0.494653.
Train: 2018-08-05T19:08:03.606359: step 11743, loss 0.579502.
Train: 2018-08-05T19:08:03.809436: step 11744, loss 0.588399.
Train: 2018-08-05T19:08:04.012512: step 11745, loss 0.54487.
Train: 2018-08-05T19:08:04.215559: step 11746, loss 0.580085.
Train: 2018-08-05T19:08:04.434259: step 11747, loss 0.615729.
Train: 2018-08-05T19:08:04.652987: step 11748, loss 0.535959.
Train: 2018-08-05T19:08:04.871686: step 11749, loss 0.606812.
Train: 2018-08-05T19:08:05.090387: step 11750, loss 0.562353.
Test: 2018-08-05T19:08:06.136985: step 11750, loss 0.548958.
Train: 2018-08-05T19:08:06.355715: step 11751, loss 0.554001.
Train: 2018-08-05T19:08:06.574383: step 11752, loss 0.613008.
Train: 2018-08-05T19:08:06.777490: step 11753, loss 0.578875.
Train: 2018-08-05T19:08:06.980568: step 11754, loss 0.579724.
Train: 2018-08-05T19:08:07.199236: step 11755, loss 0.571967.
Train: 2018-08-05T19:08:07.402344: step 11756, loss 0.602987.
Train: 2018-08-05T19:08:07.605422: step 11757, loss 0.546486.
Train: 2018-08-05T19:08:07.824120: step 11758, loss 0.604405.
Train: 2018-08-05T19:08:08.042820: step 11759, loss 0.596022.
Train: 2018-08-05T19:08:08.245896: step 11760, loss 0.628593.
Test: 2018-08-05T19:08:09.308118: step 11760, loss 0.550498.
Train: 2018-08-05T19:08:09.511225: step 11761, loss 0.525021.
Train: 2018-08-05T19:08:09.714272: step 11762, loss 0.547427.
Train: 2018-08-05T19:08:09.932971: step 11763, loss 0.636301.
Train: 2018-08-05T19:08:10.136078: step 11764, loss 0.546263.
Train: 2018-08-05T19:08:10.354778: step 11765, loss 0.55426.
Train: 2018-08-05T19:08:10.557855: step 11766, loss 0.48721.
Train: 2018-08-05T19:08:10.776553: step 11767, loss 0.545191.
Train: 2018-08-05T19:08:10.979601: step 11768, loss 0.510117.
Train: 2018-08-05T19:08:11.182679: step 11769, loss 0.535343.
Train: 2018-08-05T19:08:11.401407: step 11770, loss 0.525657.
Test: 2018-08-05T19:08:12.448006: step 11770, loss 0.548847.
Train: 2018-08-05T19:08:12.713601: step 11771, loss 0.67791.
Train: 2018-08-05T19:08:12.932269: step 11772, loss 0.553737.
Train: 2018-08-05T19:08:13.135346: step 11773, loss 0.477933.
Train: 2018-08-05T19:08:13.354044: step 11774, loss 0.518745.
Train: 2018-08-05T19:08:13.572769: step 11775, loss 0.642057.
Train: 2018-08-05T19:08:13.775852: step 11776, loss 0.527196.
Train: 2018-08-05T19:08:13.994550: step 11777, loss 0.509594.
Train: 2018-08-05T19:08:14.166384: step 11778, loss 0.581255.
Train: 2018-08-05T19:08:14.385083: step 11779, loss 0.562431.
Train: 2018-08-05T19:08:14.588162: step 11780, loss 0.553597.
Test: 2018-08-05T19:08:15.650383: step 11780, loss 0.548081.
Train: 2018-08-05T19:08:15.853459: step 11781, loss 0.527083.
Train: 2018-08-05T19:08:16.056567: step 11782, loss 0.615575.
Train: 2018-08-05T19:08:16.275237: step 11783, loss 0.54476.
Train: 2018-08-05T19:08:16.478345: step 11784, loss 0.57125.
Train: 2018-08-05T19:08:16.681421: step 11785, loss 0.641673.
Train: 2018-08-05T19:08:16.900091: step 11786, loss 0.509912.
Train: 2018-08-05T19:08:17.103198: step 11787, loss 0.5188.
Train: 2018-08-05T19:08:17.306245: step 11788, loss 0.562358.
Train: 2018-08-05T19:08:17.524973: step 11789, loss 0.623226.
Train: 2018-08-05T19:08:17.728052: step 11790, loss 0.484466.
Test: 2018-08-05T19:08:18.790302: step 11790, loss 0.548686.
Train: 2018-08-05T19:08:18.993350: step 11791, loss 0.493105.
Train: 2018-08-05T19:08:19.196456: step 11792, loss 0.562353.
Train: 2018-08-05T19:08:19.415155: step 11793, loss 0.579812.
Train: 2018-08-05T19:08:19.618233: step 11794, loss 0.544893.
Train: 2018-08-05T19:08:19.836931: step 11795, loss 0.544861.
Train: 2018-08-05T19:08:20.055630: step 11796, loss 0.536039.
Train: 2018-08-05T19:08:20.258708: step 11797, loss 0.597701.
Train: 2018-08-05T19:08:20.461787: step 11798, loss 0.6507.
Train: 2018-08-05T19:08:20.680484: step 11799, loss 0.536048.
Train: 2018-08-05T19:08:20.899183: step 11800, loss 0.527368.
Test: 2018-08-05T19:08:21.945813: step 11800, loss 0.547573.
Train: 2018-08-05T19:08:22.914339: step 11801, loss 0.527406.
Train: 2018-08-05T19:08:23.133035: step 11802, loss 0.474898.
Train: 2018-08-05T19:08:23.336114: step 11803, loss 0.54481.
Train: 2018-08-05T19:08:23.539189: step 11804, loss 0.571293.
Train: 2018-08-05T19:08:23.742267: step 11805, loss 0.50917.
Train: 2018-08-05T19:08:23.960968: step 11806, loss 0.58041.
Train: 2018-08-05T19:08:24.164013: step 11807, loss 0.526667.
Train: 2018-08-05T19:08:24.382711: step 11808, loss 0.598708.
Train: 2018-08-05T19:08:24.585790: step 11809, loss 0.571679.
Train: 2018-08-05T19:08:24.788868: step 11810, loss 0.562646.
Test: 2018-08-05T19:08:25.851119: step 11810, loss 0.54839.
Train: 2018-08-05T19:08:26.054226: step 11811, loss 0.571665.
Train: 2018-08-05T19:08:26.257303: step 11812, loss 0.490532.
Train: 2018-08-05T19:08:26.460383: step 11813, loss 0.643864.
Train: 2018-08-05T19:08:26.679051: step 11814, loss 0.490674.
Train: 2018-08-05T19:08:26.882128: step 11815, loss 0.607526.
Train: 2018-08-05T19:08:27.085236: step 11816, loss 0.571505.
Train: 2018-08-05T19:08:27.288312: step 11817, loss 0.500082.
Train: 2018-08-05T19:08:27.491390: step 11818, loss 0.58032.
Train: 2018-08-05T19:08:27.710088: step 11819, loss 0.553589.
Train: 2018-08-05T19:08:27.913135: step 11820, loss 0.544721.
Test: 2018-08-05T19:08:28.959797: step 11820, loss 0.548065.
Train: 2018-08-05T19:08:29.178465: step 11821, loss 0.544736.
Train: 2018-08-05T19:08:29.381572: step 11822, loss 0.571295.
Train: 2018-08-05T19:08:29.584648: step 11823, loss 0.615444.
Train: 2018-08-05T19:08:29.787721: step 11824, loss 0.553613.
Train: 2018-08-05T19:08:30.006396: step 11825, loss 0.509952.
Train: 2018-08-05T19:08:30.209473: step 11826, loss 0.527418.
Train: 2018-08-05T19:08:30.428201: step 11827, loss 0.536141.
Train: 2018-08-05T19:08:30.631279: step 11828, loss 0.501055.
Train: 2018-08-05T19:08:30.834326: step 11829, loss 0.597634.
Train: 2018-08-05T19:08:31.037404: step 11830, loss 0.544776.
Test: 2018-08-05T19:08:32.084034: step 11830, loss 0.548686.
Train: 2018-08-05T19:08:32.287112: step 11831, loss 0.588974.
Train: 2018-08-05T19:08:32.490189: step 11832, loss 0.56244.
Train: 2018-08-05T19:08:32.693296: step 11833, loss 0.527071.
Train: 2018-08-05T19:08:32.911965: step 11834, loss 0.491626.
Train: 2018-08-05T19:08:33.130696: step 11835, loss 0.562485.
Train: 2018-08-05T19:08:33.333741: step 11836, loss 0.571449.
Train: 2018-08-05T19:08:33.536848: step 11837, loss 0.481988.
Train: 2018-08-05T19:08:33.755547: step 11838, loss 0.598631.
Train: 2018-08-05T19:08:33.958595: step 11839, loss 0.589728.
Train: 2018-08-05T19:08:34.161702: step 11840, loss 0.598757.
Test: 2018-08-05T19:08:35.223924: step 11840, loss 0.547361.
Train: 2018-08-05T19:08:35.442652: step 11841, loss 0.463569.
Train: 2018-08-05T19:08:35.645730: step 11842, loss 0.526531.
Train: 2018-08-05T19:08:35.848808: step 11843, loss 0.535504.
Train: 2018-08-05T19:08:36.067506: step 11844, loss 0.562732.
Train: 2018-08-05T19:08:36.270554: step 11845, loss 0.553652.
Train: 2018-08-05T19:08:36.473662: step 11846, loss 0.544527.
Train: 2018-08-05T19:08:36.676739: step 11847, loss 0.58114.
Train: 2018-08-05T19:08:36.895408: step 11848, loss 0.571977.
Train: 2018-08-05T19:08:37.098509: step 11849, loss 0.517123.
Train: 2018-08-05T19:08:37.301592: step 11850, loss 0.608476.
Test: 2018-08-05T19:08:38.363844: step 11850, loss 0.547334.
Train: 2018-08-05T19:08:38.566920: step 11851, loss 0.617363.
Train: 2018-08-05T19:08:38.769968: step 11852, loss 0.517466.
Train: 2018-08-05T19:08:38.988667: step 11853, loss 0.517625.
Train: 2018-08-05T19:08:39.191774: step 11854, loss 0.562566.
Train: 2018-08-05T19:08:39.394822: step 11855, loss 0.669899.
Train: 2018-08-05T19:08:39.597900: step 11856, loss 0.642204.
Train: 2018-08-05T19:08:39.800977: step 11857, loss 0.614795.
Train: 2018-08-05T19:08:40.019706: step 11858, loss 0.553764.
Train: 2018-08-05T19:08:40.222784: step 11859, loss 0.545424.
Train: 2018-08-05T19:08:40.425861: step 11860, loss 0.587649.
Test: 2018-08-05T19:08:41.488093: step 11860, loss 0.549039.
Train: 2018-08-05T19:08:41.691158: step 11861, loss 0.587309.
Train: 2018-08-05T19:08:41.894267: step 11862, loss 0.521704.
Train: 2018-08-05T19:08:42.097345: step 11863, loss 0.619479.
Train: 2018-08-05T19:08:42.300422: step 11864, loss 0.538697.
Train: 2018-08-05T19:08:42.519120: step 11865, loss 0.562895.
Train: 2018-08-05T19:08:42.722199: step 11866, loss 0.555029.
Train: 2018-08-05T19:08:42.925245: step 11867, loss 0.626379.
Train: 2018-08-05T19:08:43.143974: step 11868, loss 0.594614.
Train: 2018-08-05T19:08:43.347052: step 11869, loss 0.563234.
Train: 2018-08-05T19:08:43.565750: step 11870, loss 0.571108.
Test: 2018-08-05T19:08:44.612380: step 11870, loss 0.550242.
Train: 2018-08-05T19:08:44.815427: step 11871, loss 0.586674.
Train: 2018-08-05T19:08:45.018505: step 11872, loss 0.532502.
Train: 2018-08-05T19:08:45.221613: step 11873, loss 0.547958.
Train: 2018-08-05T19:08:45.440311: step 11874, loss 0.532327.
Train: 2018-08-05T19:08:45.643389: step 11875, loss 0.586698.
Train: 2018-08-05T19:08:45.846436: step 11876, loss 0.571022.
Train: 2018-08-05T19:08:46.049543: step 11877, loss 0.55519.
Train: 2018-08-05T19:08:46.252590: step 11878, loss 0.658263.
Train: 2018-08-05T19:08:46.455667: step 11879, loss 0.539148.
Train: 2018-08-05T19:08:46.658776: step 11880, loss 0.570895.
Test: 2018-08-05T19:08:47.720996: step 11880, loss 0.549913.
Train: 2018-08-05T19:08:47.924104: step 11881, loss 0.54691.
Train: 2018-08-05T19:08:48.127181: step 11882, loss 0.578869.
Train: 2018-08-05T19:08:48.330258: step 11883, loss 0.619163.
Train: 2018-08-05T19:08:48.548958: step 11884, loss 0.57888.
Train: 2018-08-05T19:08:48.752006: step 11885, loss 0.59501.
Train: 2018-08-05T19:08:48.955112: step 11886, loss 0.594989.
Train: 2018-08-05T19:08:49.158191: step 11887, loss 0.619058.
Train: 2018-08-05T19:08:49.376890: step 11888, loss 0.578863.
Train: 2018-08-05T19:08:49.579968: step 11889, loss 0.61866.
Train: 2018-08-05T19:08:49.783045: step 11890, loss 0.586766.
Test: 2018-08-05T19:08:50.845265: step 11890, loss 0.549155.
Train: 2018-08-05T19:08:51.063965: step 11891, loss 0.610276.
Train: 2018-08-05T19:08:51.267072: step 11892, loss 0.594472.
Train: 2018-08-05T19:08:51.470150: step 11893, loss 0.548049.
Train: 2018-08-05T19:08:51.673226: step 11894, loss 0.617385.
Train: 2018-08-05T19:08:51.876303: step 11895, loss 0.556124.
Train: 2018-08-05T19:08:52.079380: step 11896, loss 0.541051.
Train: 2018-08-05T19:08:52.282459: step 11897, loss 0.525881.
Train: 2018-08-05T19:08:52.501158: step 11898, loss 0.479894.
Train: 2018-08-05T19:08:52.704205: step 11899, loss 0.524991.
Train: 2018-08-05T19:08:52.907312: step 11900, loss 0.555441.
Test: 2018-08-05T19:08:53.969567: step 11900, loss 0.549036.
Train: 2018-08-05T19:08:54.922436: step 11901, loss 0.555078.
Train: 2018-08-05T19:08:55.125543: step 11902, loss 0.538564.
Train: 2018-08-05T19:08:55.344243: step 11903, loss 0.596197.
Train: 2018-08-05T19:08:55.547319: step 11904, loss 0.537367.
Train: 2018-08-05T19:08:55.750399: step 11905, loss 0.511643.
Train: 2018-08-05T19:08:55.953476: step 11906, loss 0.624073.
Train: 2018-08-05T19:08:56.156552: step 11907, loss 0.597107.
Train: 2018-08-05T19:08:56.359629: step 11908, loss 0.493587.
Train: 2018-08-05T19:08:56.562706: step 11909, loss 0.597002.
Train: 2018-08-05T19:08:56.765784: step 11910, loss 0.562361.
Test: 2018-08-05T19:08:57.828018: step 11910, loss 0.548163.
Train: 2018-08-05T19:08:58.031113: step 11911, loss 0.579887.
Train: 2018-08-05T19:08:58.234190: step 11912, loss 0.623843.
Train: 2018-08-05T19:08:58.437268: step 11913, loss 0.501034.
Train: 2018-08-05T19:08:58.655966: step 11914, loss 0.536051.
Train: 2018-08-05T19:08:58.859044: step 11915, loss 0.553604.
Train: 2018-08-05T19:08:59.062122: step 11916, loss 0.571262.
Train: 2018-08-05T19:08:59.265199: step 11917, loss 0.571285.
Train: 2018-08-05T19:08:59.468277: step 11918, loss 0.571289.
Train: 2018-08-05T19:08:59.671353: step 11919, loss 0.491711.
Train: 2018-08-05T19:08:59.874431: step 11920, loss 0.562459.
Test: 2018-08-05T19:09:00.936652: step 11920, loss 0.548042.
Train: 2018-08-05T19:09:01.124139: step 11921, loss 0.553589.
Train: 2018-08-05T19:09:01.327217: step 11922, loss 0.607023.
Train: 2018-08-05T19:09:01.530293: step 11923, loss 0.606945.
Train: 2018-08-05T19:09:01.748993: step 11924, loss 0.544742.
Train: 2018-08-05T19:09:01.952071: step 11925, loss 0.500678.
Train: 2018-08-05T19:09:02.155147: step 11926, loss 0.500674.
Train: 2018-08-05T19:09:02.358224: step 11927, loss 0.544743.
Train: 2018-08-05T19:09:02.561303: step 11928, loss 0.580237.
Train: 2018-08-05T19:09:02.733137: step 11929, loss 0.619424.
Train: 2018-08-05T19:09:02.936214: step 11930, loss 0.500348.
Test: 2018-08-05T19:09:03.998465: step 11930, loss 0.54682.
Train: 2018-08-05T19:09:04.201513: step 11931, loss 0.597998.
Train: 2018-08-05T19:09:04.404591: step 11932, loss 0.518132.
Train: 2018-08-05T19:09:04.592045: step 11933, loss 0.580197.
Train: 2018-08-05T19:09:04.810775: step 11934, loss 0.491587.
Train: 2018-08-05T19:09:05.013852: step 11935, loss 0.553589.
Train: 2018-08-05T19:09:05.216931: step 11936, loss 0.624811.
Train: 2018-08-05T19:09:05.420007: step 11937, loss 0.518056.
Train: 2018-08-05T19:09:05.623085: step 11938, loss 0.544705.
Train: 2018-08-05T19:09:05.841785: step 11939, loss 0.615821.
Train: 2018-08-05T19:09:06.029240: step 11940, loss 0.553592.
Test: 2018-08-05T19:09:07.091460: step 11940, loss 0.547472.
Train: 2018-08-05T19:09:07.294568: step 11941, loss 0.597778.
Train: 2018-08-05T19:09:07.513267: step 11942, loss 0.54482.
Train: 2018-08-05T19:09:07.716315: step 11943, loss 0.544871.
Train: 2018-08-05T19:09:07.919422: step 11944, loss 0.501264.
Train: 2018-08-05T19:09:08.122493: step 11945, loss 0.544899.
Train: 2018-08-05T19:09:08.325547: step 11946, loss 0.588624.
Train: 2018-08-05T19:09:08.528654: step 11947, loss 0.562374.
Train: 2018-08-05T19:09:08.747353: step 11948, loss 0.562371.
Train: 2018-08-05T19:09:08.950432: step 11949, loss 0.544914.
Train: 2018-08-05T19:09:09.153508: step 11950, loss 0.492574.
Test: 2018-08-05T19:09:10.200139: step 11950, loss 0.547354.
Train: 2018-08-05T19:09:10.403185: step 11951, loss 0.50984.
Train: 2018-08-05T19:09:10.606293: step 11952, loss 0.588852.
Train: 2018-08-05T19:09:10.809341: step 11953, loss 0.580124.
Train: 2018-08-05T19:09:11.012449: step 11954, loss 0.571308.
Train: 2018-08-05T19:09:11.215526: step 11955, loss 0.544731.
Train: 2018-08-05T19:09:11.434225: step 11956, loss 0.509244.
Train: 2018-08-05T19:09:11.637301: step 11957, loss 0.562489.
Train: 2018-08-05T19:09:11.840380: step 11958, loss 0.598205.
Train: 2018-08-05T19:09:12.043456: step 11959, loss 0.616023.
Train: 2018-08-05T19:09:12.246534: step 11960, loss 0.526945.
Test: 2018-08-05T19:09:13.308772: step 11960, loss 0.547246.
Train: 2018-08-05T19:09:13.511834: step 11961, loss 0.544731.
Train: 2018-08-05T19:09:13.699289: step 11962, loss 0.562443.
Train: 2018-08-05T19:09:13.902365: step 11963, loss 0.668416.
Train: 2018-08-05T19:09:14.105474: step 11964, loss 0.641192.
Train: 2018-08-05T19:09:14.308520: step 11965, loss 0.527764.
Train: 2018-08-05T19:09:14.511628: step 11966, loss 0.553775.
Train: 2018-08-05T19:09:14.714707: step 11967, loss 0.519899.
Train: 2018-08-05T19:09:14.917783: step 11968, loss 0.503213.
Train: 2018-08-05T19:09:15.136482: step 11969, loss 0.54547.
Train: 2018-08-05T19:09:15.323940: step 11970, loss 0.613087.
Test: 2018-08-05T19:09:16.386190: step 11970, loss 0.549251.
Train: 2018-08-05T19:09:16.589237: step 11971, loss 0.646723.
Train: 2018-08-05T19:09:16.792344: step 11972, loss 0.562402.
Train: 2018-08-05T19:09:16.995421: step 11973, loss 0.612376.
Train: 2018-08-05T19:09:17.198501: step 11974, loss 0.579009.
Train: 2018-08-05T19:09:17.401576: step 11975, loss 0.513472.
Train: 2018-08-05T19:09:17.604656: step 11976, loss 0.49738.
Train: 2018-08-05T19:09:17.807702: step 11977, loss 0.538097.
Train: 2018-08-05T19:09:18.010810: step 11978, loss 0.537969.
Train: 2018-08-05T19:09:18.213887: step 11979, loss 0.471838.
Train: 2018-08-05T19:09:18.416964: step 11980, loss 0.504072.
Test: 2018-08-05T19:09:19.479190: step 11980, loss 0.548791.
Train: 2018-08-05T19:09:19.666675: step 11981, loss 0.545411.
Train: 2018-08-05T19:09:19.885372: step 11982, loss 0.606024.
Train: 2018-08-05T19:09:20.088448: step 11983, loss 0.666081.
Train: 2018-08-05T19:09:20.291495: step 11984, loss 0.579642.
Train: 2018-08-05T19:09:20.494602: step 11985, loss 0.528404.
Train: 2018-08-05T19:09:20.697681: step 11986, loss 0.536274.
Train: 2018-08-05T19:09:20.900727: step 11987, loss 0.510017.
Train: 2018-08-05T19:09:21.103835: step 11988, loss 0.509722.
Train: 2018-08-05T19:09:21.306912: step 11989, loss 0.571297.
Train: 2018-08-05T19:09:21.509984: step 11990, loss 0.580315.
Test: 2018-08-05T19:09:22.572242: step 11990, loss 0.54739.
Train: 2018-08-05T19:09:22.775318: step 11991, loss 0.535699.
Train: 2018-08-05T19:09:22.978395: step 11992, loss 0.634481.
Train: 2018-08-05T19:09:23.181442: step 11993, loss 0.517676.
Train: 2018-08-05T19:09:23.384552: step 11994, loss 0.616533.
Train: 2018-08-05T19:09:23.587629: step 11995, loss 0.544627.
Train: 2018-08-05T19:09:23.790676: step 11996, loss 0.562536.
Train: 2018-08-05T19:09:23.993753: step 11997, loss 0.571433.
Train: 2018-08-05T19:09:24.196859: step 11998, loss 0.50024.
Train: 2018-08-05T19:09:24.399938: step 11999, loss 0.660286.
Train: 2018-08-05T19:09:24.603009: step 12000, loss 0.571266.
Test: 2018-08-05T19:09:25.665266: step 12000, loss 0.547333.
Train: 2018-08-05T19:09:26.649411: step 12001, loss 0.59749.
Train: 2018-08-05T19:09:26.852457: step 12002, loss 0.571065.
Train: 2018-08-05T19:09:27.039944: step 12003, loss 0.588258.
Train: 2018-08-05T19:09:27.242993: step 12004, loss 0.536639.
Train: 2018-08-05T19:09:27.446070: step 12005, loss 0.562343.
Train: 2018-08-05T19:09:27.649177: step 12006, loss 0.579282.
Train: 2018-08-05T19:09:27.852254: step 12007, loss 0.469847.
Train: 2018-08-05T19:09:28.039710: step 12008, loss 0.56238.
Train: 2018-08-05T19:09:28.242788: step 12009, loss 0.545546.
Train: 2018-08-05T19:09:28.445865: step 12010, loss 0.511644.
Test: 2018-08-05T19:09:29.508087: step 12010, loss 0.547916.
Train: 2018-08-05T19:09:29.695574: step 12011, loss 0.588796.
Train: 2018-08-05T19:09:29.898651: step 12012, loss 0.648528.
Train: 2018-08-05T19:09:30.101728: step 12013, loss 0.562362.
Train: 2018-08-05T19:09:30.304775: step 12014, loss 0.587656.
Train: 2018-08-05T19:09:30.492231: step 12015, loss 0.520406.
Train: 2018-08-05T19:09:30.695338: step 12016, loss 0.604343.
Train: 2018-08-05T19:09:30.898416: step 12017, loss 0.545675.
Train: 2018-08-05T19:09:31.085873: step 12018, loss 0.58748.
Train: 2018-08-05T19:09:31.288949: step 12019, loss 0.520754.
Train: 2018-08-05T19:09:31.491997: step 12020, loss 0.595785.
Test: 2018-08-05T19:09:32.538657: step 12020, loss 0.547946.
Train: 2018-08-05T19:09:32.741705: step 12021, loss 0.579094.
Train: 2018-08-05T19:09:32.944811: step 12022, loss 0.504228.
Train: 2018-08-05T19:09:33.132269: step 12023, loss 0.537425.
Train: 2018-08-05T19:09:33.335346: step 12024, loss 0.528944.
Train: 2018-08-05T19:09:33.522802: step 12025, loss 0.587616.
Train: 2018-08-05T19:09:33.725879: step 12026, loss 0.570808.
Train: 2018-08-05T19:09:33.913334: step 12027, loss 0.553887.
Train: 2018-08-05T19:09:34.116412: step 12028, loss 0.536858.
Train: 2018-08-05T19:09:34.303838: step 12029, loss 0.622077.
Train: 2018-08-05T19:09:34.506947: step 12030, loss 0.528179.
Test: 2018-08-05T19:09:35.553545: step 12030, loss 0.546856.
Train: 2018-08-05T19:09:35.756623: step 12031, loss 0.553775.
Train: 2018-08-05T19:09:35.959732: step 12032, loss 0.502246.
Train: 2018-08-05T19:09:36.162808: step 12033, loss 0.605505.
Train: 2018-08-05T19:09:36.350233: step 12034, loss 0.579655.
Train: 2018-08-05T19:09:36.553357: step 12035, loss 0.649008.
Train: 2018-08-05T19:09:36.740798: step 12036, loss 0.501911.
Train: 2018-08-05T19:09:36.943877: step 12037, loss 0.596859.
Train: 2018-08-05T19:09:37.131331: step 12038, loss 0.613999.
Train: 2018-08-05T19:09:37.334410: step 12039, loss 0.502366.
Train: 2018-08-05T19:09:37.521865: step 12040, loss 0.519554.
Test: 2018-08-05T19:09:38.584086: step 12040, loss 0.54783.
Train: 2018-08-05T19:09:38.771572: step 12041, loss 0.588037.
Train: 2018-08-05T19:09:39.005892: step 12042, loss 0.545207.
Train: 2018-08-05T19:09:39.193350: step 12043, loss 0.562335.
Train: 2018-08-05T19:09:39.380805: step 12044, loss 0.562335.
Train: 2018-08-05T19:09:39.568262: step 12045, loss 0.596644.
Train: 2018-08-05T19:09:39.771309: step 12046, loss 0.510954.
Train: 2018-08-05T19:09:39.958796: step 12047, loss 0.596637.
Train: 2018-08-05T19:09:40.146221: step 12048, loss 0.536627.
Train: 2018-08-05T19:09:40.333707: step 12049, loss 0.570911.
Train: 2018-08-05T19:09:40.536784: step 12050, loss 0.510871.
Test: 2018-08-05T19:09:41.583384: step 12050, loss 0.547176.
Train: 2018-08-05T19:09:41.770870: step 12051, loss 0.570938.
Train: 2018-08-05T19:09:41.973947: step 12052, loss 0.510613.
Train: 2018-08-05T19:09:42.161404: step 12053, loss 0.553683.
Train: 2018-08-05T19:09:42.348860: step 12054, loss 0.579755.
Train: 2018-08-05T19:09:42.536285: step 12055, loss 0.623434.
Train: 2018-08-05T19:09:42.723771: step 12056, loss 0.536218.
Train: 2018-08-05T19:09:42.926819: step 12057, loss 0.579795.
Train: 2018-08-05T19:09:43.114305: step 12058, loss 0.518822.
Train: 2018-08-05T19:09:43.301756: step 12059, loss 0.65826.
Train: 2018-08-05T19:09:43.489188: step 12060, loss 0.614426.
Test: 2018-08-05T19:09:44.551439: step 12060, loss 0.547547.
Train: 2018-08-05T19:09:44.738925: step 12061, loss 0.562337.
Train: 2018-08-05T19:09:44.926352: step 12062, loss 0.562336.
Train: 2018-08-05T19:09:45.113838: step 12063, loss 0.579367.
Train: 2018-08-05T19:09:45.301293: step 12064, loss 0.596194.
Train: 2018-08-05T19:09:45.488718: step 12065, loss 0.553993.
Train: 2018-08-05T19:09:45.676175: step 12066, loss 0.487311.
Train: 2018-08-05T19:09:45.863662: step 12067, loss 0.529072.
Train: 2018-08-05T19:09:46.051118: step 12068, loss 0.57912.
Train: 2018-08-05T19:09:46.238574: step 12069, loss 0.537345.
Train: 2018-08-05T19:09:46.426029: step 12070, loss 0.528899.
Test: 2018-08-05T19:09:47.488281: step 12070, loss 0.54814.
Train: 2018-08-05T19:09:47.675731: step 12071, loss 0.579202.
Train: 2018-08-05T19:09:47.863164: step 12072, loss 0.579241.
Train: 2018-08-05T19:09:48.050649: step 12073, loss 0.528552.
Train: 2018-08-05T19:09:48.238076: step 12074, loss 0.579319.
Train: 2018-08-05T19:09:48.425562: step 12075, loss 0.630391.
Train: 2018-08-05T19:09:48.613018: step 12076, loss 0.545362.
Train: 2018-08-05T19:09:48.784822: step 12077, loss 0.596308.
Train: 2018-08-05T19:09:48.972309: step 12078, loss 0.664016.
Train: 2018-08-05T19:09:49.159764: step 12079, loss 0.562382.
Train: 2018-08-05T19:09:49.315949: step 12080, loss 0.508964.
Test: 2018-08-05T19:09:50.362608: step 12080, loss 0.548331.
Train: 2018-08-05T19:09:50.550065: step 12081, loss 0.479135.
Train: 2018-08-05T19:09:50.721899: step 12082, loss 0.620884.
Train: 2018-08-05T19:09:50.924977: step 12083, loss 0.545727.
Train: 2018-08-05T19:09:51.112404: step 12084, loss 0.512301.
Train: 2018-08-05T19:09:51.299889: step 12085, loss 0.595932.
Train: 2018-08-05T19:09:51.487314: step 12086, loss 0.528795.
Train: 2018-08-05T19:09:51.659179: step 12087, loss 0.612944.
Train: 2018-08-05T19:09:51.846635: step 12088, loss 0.579236.
Train: 2018-08-05T19:09:52.034093: step 12089, loss 0.612957.
Train: 2018-08-05T19:09:52.205896: step 12090, loss 0.528758.
Test: 2018-08-05T19:09:53.268148: step 12090, loss 0.549128.
Train: 2018-08-05T19:09:53.455635: step 12091, loss 0.579189.
Train: 2018-08-05T19:09:53.643084: step 12092, loss 0.637911.
Train: 2018-08-05T19:09:53.830547: step 12093, loss 0.545726.
Train: 2018-08-05T19:09:54.017972: step 12094, loss 0.57908.
Train: 2018-08-05T19:09:54.205429: step 12095, loss 0.504457.
Train: 2018-08-05T19:09:54.377263: step 12096, loss 0.545886.
Train: 2018-08-05T19:09:54.564719: step 12097, loss 0.554142.
Train: 2018-08-05T19:09:54.736554: step 12098, loss 0.604225.
Train: 2018-08-05T19:09:54.924010: step 12099, loss 0.570762.
Train: 2018-08-05T19:09:55.095875: step 12100, loss 0.537478.
Test: 2018-08-05T19:09:56.154016: step 12100, loss 0.547169.
Train: 2018-08-05T19:09:57.138173: step 12101, loss 0.479091.
Train: 2018-08-05T19:09:57.325617: step 12102, loss 0.545617.
Train: 2018-08-05T19:09:57.497452: step 12103, loss 0.503234.
Train: 2018-08-05T19:09:57.669290: step 12104, loss 0.630578.
Train: 2018-08-05T19:09:57.856712: step 12105, loss 0.588051.
Train: 2018-08-05T19:09:58.044169: step 12106, loss 0.605322.
Train: 2018-08-05T19:09:58.216034: step 12107, loss 0.484944.
Train: 2018-08-05T19:09:58.403489: step 12108, loss 0.545066.
Train: 2018-08-05T19:09:58.575324: step 12109, loss 0.623093.
Train: 2018-08-05T19:09:58.762751: step 12110, loss 0.588403.
Test: 2018-08-05T19:09:59.809380: step 12110, loss 0.54766.
Train: 2018-08-05T19:09:59.996867: step 12111, loss 0.510296.
Train: 2018-08-05T19:10:00.168672: step 12112, loss 0.579734.
Train: 2018-08-05T19:10:00.340506: step 12113, loss 0.562354.
Train: 2018-08-05T19:10:00.512371: step 12114, loss 0.562354.
Train: 2018-08-05T19:10:00.699828: step 12115, loss 0.571049.
Train: 2018-08-05T19:10:00.871662: step 12116, loss 0.640547.
Train: 2018-08-05T19:10:01.043497: step 12117, loss 0.467267.
Train: 2018-08-05T19:10:01.230956: step 12118, loss 0.527755.
Train: 2018-08-05T19:10:01.402787: step 12119, loss 0.58834.
Train: 2018-08-05T19:10:01.590238: step 12120, loss 0.571013.
Test: 2018-08-05T19:10:02.636844: step 12120, loss 0.548075.
Train: 2018-08-05T19:10:02.808678: step 12121, loss 0.545017.
Train: 2018-08-05T19:10:02.996164: step 12122, loss 0.562346.
Train: 2018-08-05T19:10:03.167999: step 12123, loss 0.510333.
Train: 2018-08-05T19:10:03.339833: step 12124, loss 0.614515.
Train: 2018-08-05T19:10:03.511639: step 12125, loss 0.614486.
Train: 2018-08-05T19:10:03.699125: step 12126, loss 0.62295.
Train: 2018-08-05T19:10:03.870929: step 12127, loss 0.510735.
Train: 2018-08-05T19:10:04.042794: step 12128, loss 0.528048.
Train: 2018-08-05T19:10:04.214629: step 12129, loss 0.579461.
Train: 2018-08-05T19:10:04.386463: step 12130, loss 0.570883.
Test: 2018-08-05T19:10:05.448684: step 12130, loss 0.547512.
Train: 2018-08-05T19:10:05.620520: step 12131, loss 0.60497.
Train: 2018-08-05T19:10:05.792385: step 12132, loss 0.56235.
Train: 2018-08-05T19:10:05.979811: step 12133, loss 0.537027.
Train: 2018-08-05T19:10:06.151675: step 12134, loss 0.553927.
Train: 2018-08-05T19:10:06.323510: step 12135, loss 0.579231.
Train: 2018-08-05T19:10:06.510935: step 12136, loss 0.562379.
Train: 2018-08-05T19:10:06.698422: step 12137, loss 0.57919.
Train: 2018-08-05T19:10:06.870257: step 12138, loss 0.570781.
Train: 2018-08-05T19:10:07.042092: step 12139, loss 0.545671.
Train: 2018-08-05T19:10:07.213930: step 12140, loss 0.612585.
Test: 2018-08-05T19:10:08.260527: step 12140, loss 0.548508.
Train: 2018-08-05T19:10:08.448014: step 12141, loss 0.487408.
Train: 2018-08-05T19:10:08.619848: step 12142, loss 0.612519.
Train: 2018-08-05T19:10:08.791651: step 12143, loss 0.495695.
Train: 2018-08-05T19:10:08.963487: step 12144, loss 0.453624.
Train: 2018-08-05T19:10:09.135351: step 12145, loss 0.528581.
Train: 2018-08-05T19:10:09.307186: step 12146, loss 0.596473.
Train: 2018-08-05T19:10:09.479016: step 12147, loss 0.588126.
Train: 2018-08-05T19:10:09.635235: step 12148, loss 0.545058.
Train: 2018-08-05T19:10:09.807069: step 12149, loss 0.623166.
Train: 2018-08-05T19:10:09.978874: step 12150, loss 0.588453.
Test: 2018-08-05T19:10:11.041137: step 12150, loss 0.548434.
Train: 2018-08-05T19:10:11.212995: step 12151, loss 0.614523.
Train: 2018-08-05T19:10:11.384825: step 12152, loss 0.579673.
Train: 2018-08-05T19:10:11.556660: step 12153, loss 0.588218.
Train: 2018-08-05T19:10:11.728465: step 12154, loss 0.519435.
Train: 2018-08-05T19:10:11.900329: step 12155, loss 0.579451.
Train: 2018-08-05T19:10:12.072134: step 12156, loss 0.613516.
Train: 2018-08-05T19:10:12.243999: step 12157, loss 0.486033.
Train: 2018-08-05T19:10:12.415834: step 12158, loss 0.613188.
Train: 2018-08-05T19:10:12.572046: step 12159, loss 0.503269.
Train: 2018-08-05T19:10:12.743882: step 12160, loss 0.655257.
Test: 2018-08-05T19:10:13.806132: step 12160, loss 0.547962.
Train: 2018-08-05T19:10:14.009210: step 12161, loss 0.528768.
Train: 2018-08-05T19:10:14.181045: step 12162, loss 0.579165.
Train: 2018-08-05T19:10:14.337258: step 12163, loss 0.470428.
Train: 2018-08-05T19:10:14.509093: step 12164, loss 0.60433.
Train: 2018-08-05T19:10:14.680928: step 12165, loss 0.528824.
Train: 2018-08-05T19:10:14.852767: step 12166, loss 0.495069.
Train: 2018-08-05T19:10:15.024597: step 12167, loss 0.57929.
Train: 2018-08-05T19:10:15.196427: step 12168, loss 0.519803.
Train: 2018-08-05T19:10:15.368267: step 12169, loss 0.596598.
Train: 2018-08-05T19:10:15.540072: step 12170, loss 0.59674.
Test: 2018-08-05T19:10:16.586732: step 12170, loss 0.54874.
Train: 2018-08-05T19:10:16.758566: step 12171, loss 0.562337.
Train: 2018-08-05T19:10:16.914750: step 12172, loss 0.441539.
Train: 2018-08-05T19:10:17.086584: step 12173, loss 0.553659.
Train: 2018-08-05T19:10:17.258420: step 12174, loss 0.527346.
Train: 2018-08-05T19:10:17.430295: step 12175, loss 0.553598.
Train: 2018-08-05T19:10:17.586468: step 12176, loss 0.606956.
Train: 2018-08-05T19:10:17.758332: step 12177, loss 0.58037.
Train: 2018-08-05T19:10:17.930162: step 12178, loss 0.499931.
Train: 2018-08-05T19:10:18.086350: step 12179, loss 0.535631.
Train: 2018-08-05T19:10:18.258185: step 12180, loss 0.58068.
Test: 2018-08-05T19:10:19.320446: step 12180, loss 0.548594.
Train: 2018-08-05T19:10:19.476684: step 12181, loss 0.571711.
Train: 2018-08-05T19:10:19.648515: step 12182, loss 0.580792.
Train: 2018-08-05T19:10:19.820329: step 12183, loss 0.526464.
Train: 2018-08-05T19:10:19.992154: step 12184, loss 0.626068.
Train: 2018-08-05T19:10:20.148397: step 12185, loss 0.60775.
Train: 2018-08-05T19:10:20.320232: step 12186, loss 0.56256.
Train: 2018-08-05T19:10:20.507689: step 12187, loss 0.553588.
Train: 2018-08-05T19:10:20.679493: step 12188, loss 0.59789.
Train: 2018-08-05T19:10:20.835737: step 12189, loss 0.509645.
Train: 2018-08-05T19:10:21.007543: step 12190, loss 0.536115.
Test: 2018-08-05T19:10:22.069792: step 12190, loss 0.547185.
Train: 2018-08-05T19:10:22.226040: step 12191, loss 0.667134.
Train: 2018-08-05T19:10:22.397870: step 12192, loss 0.562343.
Train: 2018-08-05T19:10:22.554084: step 12193, loss 0.527978.
Train: 2018-08-05T19:10:22.725923: step 12194, loss 0.647779.
Train: 2018-08-05T19:10:22.897754: step 12195, loss 0.553893.
Train: 2018-08-05T19:10:23.069559: step 12196, loss 0.503615.
Train: 2018-08-05T19:10:23.225772: step 12197, loss 0.528946.
Train: 2018-08-05T19:10:23.397607: step 12198, loss 0.478859.
Train: 2018-08-05T19:10:23.569442: step 12199, loss 0.528839.
Train: 2018-08-05T19:10:23.741275: step 12200, loss 0.511752.
Test: 2018-08-05T19:10:24.787907: step 12200, loss 0.547358.
Train: 2018-08-05T19:10:25.709589: step 12201, loss 0.596362.
Train: 2018-08-05T19:10:25.881430: step 12202, loss 0.536687.
Train: 2018-08-05T19:10:26.037643: step 12203, loss 0.579542.
Train: 2018-08-05T19:10:26.209478: step 12204, loss 0.527769.
Train: 2018-08-05T19:10:26.381314: step 12205, loss 0.55366.
Train: 2018-08-05T19:10:26.553148: step 12206, loss 0.571115.
Train: 2018-08-05T19:10:26.724987: step 12207, loss 0.667727.
Train: 2018-08-05T19:10:26.896817: step 12208, loss 0.492318.
Train: 2018-08-05T19:10:27.053031: step 12209, loss 0.5887.
Train: 2018-08-05T19:10:27.224869: step 12210, loss 0.571153.
Test: 2018-08-05T19:10:28.287087: step 12210, loss 0.548161.
Train: 2018-08-05T19:10:28.443330: step 12211, loss 0.588648.
Train: 2018-08-05T19:10:28.615165: step 12212, loss 0.509983.
Train: 2018-08-05T19:10:28.787000: step 12213, loss 0.579831.
Train: 2018-08-05T19:10:28.958834: step 12214, loss 0.597246.
Train: 2018-08-05T19:10:29.115048: step 12215, loss 0.536276.
Train: 2018-08-05T19:10:29.286853: step 12216, loss 0.527639.
Train: 2018-08-05T19:10:29.443096: step 12217, loss 0.527634.
Train: 2018-08-05T19:10:29.614931: step 12218, loss 0.614521.
Train: 2018-08-05T19:10:29.771115: step 12219, loss 0.614443.
Train: 2018-08-05T19:10:29.942949: step 12220, loss 0.570986.
Test: 2018-08-05T19:10:30.989609: step 12220, loss 0.547369.
Train: 2018-08-05T19:10:31.161438: step 12221, loss 0.55373.
Train: 2018-08-05T19:10:31.333279: step 12222, loss 0.588057.
Train: 2018-08-05T19:10:31.489494: step 12223, loss 0.579405.
Train: 2018-08-05T19:10:31.661327: step 12224, loss 0.553861.
Train: 2018-08-05T19:10:31.817541: step 12225, loss 0.587718.
Train: 2018-08-05T19:10:31.989344: step 12226, loss 0.58761.
Train: 2018-08-05T19:10:32.161209: step 12227, loss 0.595854.
Train: 2018-08-05T19:10:32.317394: step 12228, loss 0.529242.
Train: 2018-08-05T19:10:32.489227: step 12229, loss 0.545945.
Train: 2018-08-05T19:10:32.645442: step 12230, loss 0.537748.
Test: 2018-08-05T19:10:33.707723: step 12230, loss 0.548917.
Train: 2018-08-05T19:10:33.832693: step 12231, loss 0.562506.
Train: 2018-08-05T19:10:34.004528: step 12232, loss 0.554251.
Train: 2018-08-05T19:10:34.160741: step 12233, loss 0.587281.
Train: 2018-08-05T19:10:34.332546: step 12234, loss 0.463343.
Train: 2018-08-05T19:10:34.488789: step 12235, loss 0.587387.
Train: 2018-08-05T19:10:34.660625: step 12236, loss 0.604167.
Train: 2018-08-05T19:10:34.816807: step 12237, loss 0.579138.
Train: 2018-08-05T19:10:34.973055: step 12238, loss 0.554029.
Train: 2018-08-05T19:10:35.144856: step 12239, loss 0.495299.
Train: 2018-08-05T19:10:35.316691: step 12240, loss 0.596101.
Test: 2018-08-05T19:10:36.363321: step 12240, loss 0.548036.
Train: 2018-08-05T19:10:36.535185: step 12241, loss 0.579277.
Train: 2018-08-05T19:10:36.691398: step 12242, loss 0.562352.
Train: 2018-08-05T19:10:36.863235: step 12243, loss 0.528377.
Train: 2018-08-05T19:10:37.019446: step 12244, loss 0.536768.
Train: 2018-08-05T19:10:37.191281: step 12245, loss 0.553771.
Train: 2018-08-05T19:10:37.363117: step 12246, loss 0.493495.
Train: 2018-08-05T19:10:37.519301: step 12247, loss 0.649082.
Train: 2018-08-05T19:10:37.691164: step 12248, loss 0.553661.
Train: 2018-08-05T19:10:37.847382: step 12249, loss 0.536226.
Train: 2018-08-05T19:10:38.019213: step 12250, loss 0.518677.
Test: 2018-08-05T19:10:39.065842: step 12250, loss 0.546718.
Train: 2018-08-05T19:10:39.237647: step 12251, loss 0.527267.
Train: 2018-08-05T19:10:39.393890: step 12252, loss 0.562432.
Train: 2018-08-05T19:10:39.550075: step 12253, loss 0.518076.
Train: 2018-08-05T19:10:39.721943: step 12254, loss 0.48211.
Train: 2018-08-05T19:10:39.878152: step 12255, loss 0.644186.
Train: 2018-08-05T19:10:40.049987: step 12256, loss 0.59886.
Train: 2018-08-05T19:10:40.206201: step 12257, loss 0.508356.
Train: 2018-08-05T19:10:40.378007: step 12258, loss 0.589927.
Train: 2018-08-05T19:10:40.534250: step 12259, loss 0.599003.
Train: 2018-08-05T19:10:40.706083: step 12260, loss 0.517412.
Test: 2018-08-05T19:10:41.752714: step 12260, loss 0.546097.
Train: 2018-08-05T19:10:41.924550: step 12261, loss 0.607882.
Train: 2018-08-05T19:10:42.080731: step 12262, loss 0.490528.
Train: 2018-08-05T19:10:42.236975: step 12263, loss 0.508561.
Train: 2018-08-05T19:10:42.408810: step 12264, loss 0.571659.
Train: 2018-08-05T19:10:42.580644: step 12265, loss 0.553609.
Train: 2018-08-05T19:10:42.736858: step 12266, loss 0.55361.
Train: 2018-08-05T19:10:42.908687: step 12267, loss 0.55361.
Train: 2018-08-05T19:10:43.064877: step 12268, loss 0.535547.
Train: 2018-08-05T19:10:43.221123: step 12269, loss 0.562647.
Train: 2018-08-05T19:10:43.392954: step 12270, loss 0.55361.
Test: 2018-08-05T19:10:44.439555: step 12270, loss 0.548182.
Train: 2018-08-05T19:10:44.611413: step 12271, loss 0.544579.
Train: 2018-08-05T19:10:44.767632: step 12272, loss 0.499438.
Train: 2018-08-05T19:10:44.939437: step 12273, loss 0.616973.
Train: 2018-08-05T19:10:45.095685: step 12274, loss 0.454188.
Train: 2018-08-05T19:10:45.267515: step 12275, loss 0.544554.
Train: 2018-08-05T19:10:45.423729: step 12276, loss 0.590063.
Train: 2018-08-05T19:10:45.595567: step 12277, loss 0.544535.
Train: 2018-08-05T19:10:45.751748: step 12278, loss 0.581023.
Train: 2018-08-05T19:10:45.923612: step 12279, loss 0.617453.
Train: 2018-08-05T19:10:46.079825: step 12280, loss 0.562702.
Test: 2018-08-05T19:10:47.142046: step 12280, loss 0.546519.
Train: 2018-08-05T19:10:47.298259: step 12281, loss 0.517487.
Train: 2018-08-05T19:10:47.470128: step 12282, loss 0.598635.
Train: 2018-08-05T19:10:47.626338: step 12283, loss 0.54463.
Train: 2018-08-05T19:10:47.798174: step 12284, loss 0.580363.
Train: 2018-08-05T19:10:47.970011: step 12285, loss 0.571347.
Train: 2018-08-05T19:10:48.141842: step 12286, loss 0.500631.
Train: 2018-08-05T19:10:48.298026: step 12287, loss 0.527179.
Train: 2018-08-05T19:10:48.469894: step 12288, loss 0.606429.
Train: 2018-08-05T19:10:48.641725: step 12289, loss 0.641374.
Train: 2018-08-05T19:10:48.797939: step 12290, loss 0.536229.
Test: 2018-08-05T19:10:49.844539: step 12290, loss 0.548477.
Train: 2018-08-05T19:10:50.016404: step 12291, loss 0.596991.
Train: 2018-08-05T19:10:50.188238: step 12292, loss 0.545135.
Train: 2018-08-05T19:10:50.344446: step 12293, loss 0.545232.
Train: 2018-08-05T19:10:50.516257: step 12294, loss 0.528272.
Train: 2018-08-05T19:10:50.672469: step 12295, loss 0.536838.
Train: 2018-08-05T19:10:50.859956: step 12296, loss 0.596351.
Train: 2018-08-05T19:10:51.031790: step 12297, loss 0.59629.
Train: 2018-08-05T19:10:51.187973: step 12298, loss 0.553905.
Train: 2018-08-05T19:10:51.359838: step 12299, loss 0.537071.
Train: 2018-08-05T19:10:51.531673: step 12300, loss 0.570799.
Test: 2018-08-05T19:10:52.578274: step 12300, loss 0.548706.
Train: 2018-08-05T19:10:53.546827: step 12301, loss 0.537123.
Train: 2018-08-05T19:10:53.718665: step 12302, loss 0.545526.
Train: 2018-08-05T19:10:53.890497: step 12303, loss 0.596122.
Train: 2018-08-05T19:10:54.062301: step 12304, loss 0.55393.
Train: 2018-08-05T19:10:54.218548: step 12305, loss 0.520158.
Train: 2018-08-05T19:10:54.390379: step 12306, loss 0.562356.
Train: 2018-08-05T19:10:54.562185: step 12307, loss 0.545369.
Train: 2018-08-05T19:10:54.734048: step 12308, loss 0.511222.
Train: 2018-08-05T19:10:54.905883: step 12309, loss 0.46805.
Train: 2018-08-05T19:10:55.077718: step 12310, loss 0.510369.
Test: 2018-08-05T19:10:56.139939: step 12310, loss 0.547743.
Train: 2018-08-05T19:10:56.311774: step 12311, loss 0.536082.
Train: 2018-08-05T19:10:56.467987: step 12312, loss 0.562468.
Train: 2018-08-05T19:10:56.655443: step 12313, loss 0.499773.
Train: 2018-08-05T19:10:56.827308: step 12314, loss 0.562811.
Train: 2018-08-05T19:10:56.999137: step 12315, loss 0.562943.
Train: 2018-08-05T19:10:57.170978: step 12316, loss 0.544509.
Train: 2018-08-05T19:10:57.342817: step 12317, loss 0.581697.
Train: 2018-08-05T19:10:57.514647: step 12318, loss 0.488509.
Train: 2018-08-05T19:10:57.686482: step 12319, loss 0.563331.
Train: 2018-08-05T19:10:57.858287: step 12320, loss 0.554.
Test: 2018-08-05T19:10:58.904947: step 12320, loss 0.549666.
Train: 2018-08-05T19:10:59.092372: step 12321, loss 0.639273.
Train: 2018-08-05T19:10:59.264207: step 12322, loss 0.544567.
Train: 2018-08-05T19:10:59.436074: step 12323, loss 0.563409.
Train: 2018-08-05T19:10:59.607878: step 12324, loss 0.647876.
Train: 2018-08-05T19:10:59.779742: step 12325, loss 0.544509.
Train: 2018-08-05T19:10:59.951577: step 12326, loss 0.526624.
Train: 2018-08-05T19:11:00.123411: step 12327, loss 0.608647.
Train: 2018-08-05T19:11:00.295217: step 12328, loss 0.598968.
Train: 2018-08-05T19:11:00.467085: step 12329, loss 0.650665.
Train: 2018-08-05T19:11:00.638916: step 12330, loss 0.619541.
Test: 2018-08-05T19:11:01.701166: step 12330, loss 0.556598.
Train: 2018-08-05T19:11:01.873002: step 12331, loss 0.540723.
Train: 2018-08-05T19:11:02.044838: step 12332, loss 0.552238.
Train: 2018-08-05T19:11:02.216641: step 12333, loss 0.640297.
Train: 2018-08-05T19:11:02.388506: step 12334, loss 0.672094.
Train: 2018-08-05T19:11:02.575932: step 12335, loss 0.556713.
Train: 2018-08-05T19:11:02.747778: step 12336, loss 0.557056.
Train: 2018-08-05T19:11:02.935253: step 12337, loss 0.58667.
Train: 2018-08-05T19:11:03.107088: step 12338, loss 0.548569.
Train: 2018-08-05T19:11:03.278894: step 12339, loss 0.610136.
Train: 2018-08-05T19:11:03.450728: step 12340, loss 0.531513.
Test: 2018-08-05T19:11:04.512979: step 12340, loss 0.550524.
Train: 2018-08-05T19:11:04.684844: step 12341, loss 0.538743.
Train: 2018-08-05T19:11:04.872299: step 12342, loss 0.530069.
Train: 2018-08-05T19:11:05.044134: step 12343, loss 0.505274.
Train: 2018-08-05T19:11:05.215973: step 12344, loss 0.537791.
Train: 2018-08-05T19:11:05.403425: step 12345, loss 0.59568.
Train: 2018-08-05T19:11:05.575260: step 12346, loss 0.570769.
Train: 2018-08-05T19:11:05.747095: step 12347, loss 0.52882.
Train: 2018-08-05T19:11:05.934551: step 12348, loss 0.587703.
Train: 2018-08-05T19:11:06.106385: step 12349, loss 0.536897.
Train: 2018-08-05T19:11:06.293841: step 12350, loss 0.553808.
Test: 2018-08-05T19:11:07.340442: step 12350, loss 0.54722.
Train: 2018-08-05T19:11:07.527898: step 12351, loss 0.57949.
Train: 2018-08-05T19:11:07.699762: step 12352, loss 0.60539.
Train: 2018-08-05T19:11:07.871597: step 12353, loss 0.614055.
Train: 2018-08-05T19:11:08.059054: step 12354, loss 0.519325.
Train: 2018-08-05T19:11:08.230858: step 12355, loss 0.527913.
Train: 2018-08-05T19:11:08.418344: step 12356, loss 0.579587.
Train: 2018-08-05T19:11:08.590183: step 12357, loss 0.596869.
Train: 2018-08-05T19:11:08.777629: step 12358, loss 0.596825.
Train: 2018-08-05T19:11:08.949440: step 12359, loss 0.519359.
Train: 2018-08-05T19:11:09.136926: step 12360, loss 0.545153.
Test: 2018-08-05T19:11:10.183526: step 12360, loss 0.547586.
Train: 2018-08-05T19:11:10.370982: step 12361, loss 0.665469.
Train: 2018-08-05T19:11:10.542846: step 12362, loss 0.587982.
Train: 2018-08-05T19:11:10.730303: step 12363, loss 0.587831.
Train: 2018-08-05T19:11:10.902137: step 12364, loss 0.562369.
Train: 2018-08-05T19:11:11.089564: step 12365, loss 0.579162.
Train: 2018-08-05T19:11:11.261422: step 12366, loss 0.537443.
Train: 2018-08-05T19:11:11.448884: step 12367, loss 0.587352.
Train: 2018-08-05T19:11:11.636340: step 12368, loss 0.521205.
Train: 2018-08-05T19:11:11.808176: step 12369, loss 0.636733.
Train: 2018-08-05T19:11:11.995631: step 12370, loss 0.562556.
Test: 2018-08-05T19:11:13.042257: step 12370, loss 0.548386.
Train: 2018-08-05T19:11:13.229718: step 12371, loss 0.505396.
Train: 2018-08-05T19:11:13.401552: step 12372, loss 0.619806.
Train: 2018-08-05T19:11:13.588978: step 12373, loss 0.611532.
Train: 2018-08-05T19:11:13.776435: step 12374, loss 0.513993.
Train: 2018-08-05T19:11:13.948299: step 12375, loss 0.578901.
Train: 2018-08-05T19:11:14.135755: step 12376, loss 0.554595.
Train: 2018-08-05T19:11:14.307590: step 12377, loss 0.578898.
Train: 2018-08-05T19:11:14.495016: step 12378, loss 0.562693.
Train: 2018-08-05T19:11:14.682502: step 12379, loss 0.538364.
Train: 2018-08-05T19:11:14.854306: step 12380, loss 0.668326.
Test: 2018-08-05T19:11:15.916588: step 12380, loss 0.548601.
Train: 2018-08-05T19:11:16.088427: step 12381, loss 0.578898.
Train: 2018-08-05T19:11:16.244632: step 12382, loss 0.579963.
Train: 2018-08-05T19:11:16.432093: step 12383, loss 0.562774.
Train: 2018-08-05T19:11:16.619548: step 12384, loss 0.538702.
Train: 2018-08-05T19:11:16.807005: step 12385, loss 0.546729.
Train: 2018-08-05T19:11:16.994461: step 12386, loss 0.586927.
Train: 2018-08-05T19:11:17.166295: step 12387, loss 0.578879.
Train: 2018-08-05T19:11:17.353751: step 12388, loss 0.522429.
Train: 2018-08-05T19:11:17.541208: step 12389, loss 0.506044.
Train: 2018-08-05T19:11:17.728666: step 12390, loss 0.587081.
Test: 2018-08-05T19:11:18.775294: step 12390, loss 0.549056.
Train: 2018-08-05T19:11:18.962753: step 12391, loss 0.529762.
Train: 2018-08-05T19:11:19.150206: step 12392, loss 0.579018.
Train: 2018-08-05T19:11:19.337666: step 12393, loss 0.65389.
Train: 2018-08-05T19:11:19.525118: step 12394, loss 0.537487.
Train: 2018-08-05T19:11:19.712578: step 12395, loss 0.587442.
Train: 2018-08-05T19:11:19.900030: step 12396, loss 0.562421.
Train: 2018-08-05T19:11:20.071865: step 12397, loss 0.495562.
Train: 2018-08-05T19:11:20.259321: step 12398, loss 0.604384.
Train: 2018-08-05T19:11:20.446777: step 12399, loss 0.528696.
Train: 2018-08-05T19:11:20.634203: step 12400, loss 0.56236.
Test: 2018-08-05T19:11:21.680834: step 12400, loss 0.548366.
Train: 2018-08-05T19:11:22.633766: step 12401, loss 0.587817.
Train: 2018-08-05T19:11:22.821221: step 12402, loss 0.545327.
Train: 2018-08-05T19:11:23.008648: step 12403, loss 0.553805.
Train: 2018-08-05T19:11:23.196134: step 12404, loss 0.579457.
Train: 2018-08-05T19:11:23.383589: step 12405, loss 0.545182.
Train: 2018-08-05T19:11:23.586671: step 12406, loss 0.579533.
Train: 2018-08-05T19:11:23.774123: step 12407, loss 0.545116.
Train: 2018-08-05T19:11:23.961583: step 12408, loss 0.527829.
Train: 2018-08-05T19:11:24.149035: step 12409, loss 0.571002.
Train: 2018-08-05T19:11:24.336495: step 12410, loss 0.623114.
Test: 2018-08-05T19:11:25.383121: step 12410, loss 0.547867.
Train: 2018-08-05T19:11:25.570547: step 12411, loss 0.527667.
Train: 2018-08-05T19:11:25.758004: step 12412, loss 0.553673.
Train: 2018-08-05T19:11:25.961082: step 12413, loss 0.56235.
Train: 2018-08-05T19:11:26.148567: step 12414, loss 0.510228.
Train: 2018-08-05T19:11:26.335994: step 12415, loss 0.59722.
Train: 2018-08-05T19:11:26.523449: step 12416, loss 0.579804.
Train: 2018-08-05T19:11:26.710905: step 12417, loss 0.536217.
Train: 2018-08-05T19:11:26.898392: step 12418, loss 0.527483.
Train: 2018-08-05T19:11:27.085852: step 12419, loss 0.588591.
Train: 2018-08-05T19:11:27.273274: step 12420, loss 0.579857.
Test: 2018-08-05T19:11:28.335525: step 12420, loss 0.547785.
Train: 2018-08-05T19:11:28.523012: step 12421, loss 0.562369.
Train: 2018-08-05T19:11:28.710471: step 12422, loss 0.597255.
Train: 2018-08-05T19:11:28.913546: step 12423, loss 0.614519.
Train: 2018-08-05T19:11:29.101001: step 12424, loss 0.501848.
Train: 2018-08-05T19:11:29.288457: step 12425, loss 0.579583.
Train: 2018-08-05T19:11:29.491534: step 12426, loss 0.588123.
Train: 2018-08-05T19:11:29.678960: step 12427, loss 0.579454.
Train: 2018-08-05T19:11:29.882037: step 12428, loss 0.621964.
Train: 2018-08-05T19:11:30.069494: step 12429, loss 0.570813.
Train: 2018-08-05T19:11:30.256981: step 12430, loss 0.545611.
Test: 2018-08-05T19:11:31.319231: step 12430, loss 0.548485.
Train: 2018-08-05T19:11:31.506687: step 12431, loss 0.545732.
Train: 2018-08-05T19:11:31.709769: step 12432, loss 0.529186.
Train: 2018-08-05T19:11:31.912843: step 12433, loss 0.520922.
Train: 2018-08-05T19:11:32.100299: step 12434, loss 0.529156.
Train: 2018-08-05T19:11:32.287754: step 12435, loss 0.579123.
Train: 2018-08-05T19:11:32.490827: step 12436, loss 0.545647.
Train: 2018-08-05T19:11:32.678288: step 12437, loss 0.469896.
Train: 2018-08-05T19:11:32.881335: step 12438, loss 0.545387.
Train: 2018-08-05T19:11:33.068822: step 12439, loss 0.579451.
Train: 2018-08-05T19:11:33.256247: step 12440, loss 0.553719.
Test: 2018-08-05T19:11:34.318529: step 12440, loss 0.548659.
Train: 2018-08-05T19:11:34.505985: step 12441, loss 0.518974.
Train: 2018-08-05T19:11:34.709066: step 12442, loss 0.492399.
Train: 2018-08-05T19:11:34.912110: step 12443, loss 0.553588.
Train: 2018-08-05T19:11:35.099590: step 12444, loss 0.52628.
Train: 2018-08-05T19:11:35.302643: step 12445, loss 0.545048.
Train: 2018-08-05T19:11:35.505751: step 12446, loss 0.496437.
Train: 2018-08-05T19:11:35.693178: step 12447, loss 0.617621.
Train: 2018-08-05T19:11:35.896255: step 12448, loss 0.564243.
Train: 2018-08-05T19:11:36.083744: step 12449, loss 0.563189.
Train: 2018-08-05T19:11:36.302435: step 12450, loss 0.562889.
Test: 2018-08-05T19:11:37.349069: step 12450, loss 0.547122.
Train: 2018-08-05T19:11:37.552146: step 12451, loss 0.581239.
Train: 2018-08-05T19:11:37.739597: step 12452, loss 0.590281.
Train: 2018-08-05T19:11:37.942680: step 12453, loss 0.599025.
Train: 2018-08-05T19:11:38.130136: step 12454, loss 0.571115.
Train: 2018-08-05T19:11:38.333207: step 12455, loss 0.531541.
Train: 2018-08-05T19:11:38.520670: step 12456, loss 0.587558.
Train: 2018-08-05T19:11:38.723718: step 12457, loss 0.66477.
Train: 2018-08-05T19:11:38.926797: step 12458, loss 0.587166.
Train: 2018-08-05T19:11:39.129902: step 12459, loss 0.555683.
Train: 2018-08-05T19:11:39.332983: step 12460, loss 0.48794.
Test: 2018-08-05T19:11:40.379609: step 12460, loss 0.54905.
Train: 2018-08-05T19:11:40.582691: step 12461, loss 0.54657.
Train: 2018-08-05T19:11:40.770143: step 12462, loss 0.562354.
Train: 2018-08-05T19:11:40.973190: step 12463, loss 0.527764.
Train: 2018-08-05T19:11:41.160676: step 12464, loss 0.579738.
Train: 2018-08-05T19:11:41.363754: step 12465, loss 0.597166.
Train: 2018-08-05T19:11:41.551206: step 12466, loss 0.562353.
Train: 2018-08-05T19:11:41.754288: step 12467, loss 0.518941.
Train: 2018-08-05T19:11:41.957334: step 12468, loss 0.553662.
Train: 2018-08-05T19:11:42.160442: step 12469, loss 0.57976.
Train: 2018-08-05T19:11:42.347898: step 12470, loss 0.614561.
Test: 2018-08-05T19:11:43.410149: step 12470, loss 0.547863.
Train: 2018-08-05T19:11:43.597606: step 12471, loss 0.605713.
Train: 2018-08-05T19:11:43.800683: step 12472, loss 0.47608.
Train: 2018-08-05T19:11:44.003731: step 12473, loss 0.562337.
Train: 2018-08-05T19:11:44.206842: step 12474, loss 0.545096.
Train: 2018-08-05T19:11:44.409886: step 12475, loss 0.510589.
Train: 2018-08-05T19:11:44.612994: step 12476, loss 0.562342.
Train: 2018-08-05T19:11:44.816071: step 12477, loss 0.501615.
Train: 2018-08-05T19:11:45.003497: step 12478, loss 0.571091.
Train: 2018-08-05T19:11:45.222225: step 12479, loss 0.588671.
Train: 2018-08-05T19:11:45.425303: step 12480, loss 0.562395.
Test: 2018-08-05T19:11:46.471903: step 12480, loss 0.547919.
Train: 2018-08-05T19:11:46.674981: step 12481, loss 0.518433.
Train: 2018-08-05T19:11:46.878058: step 12482, loss 0.544776.
Train: 2018-08-05T19:11:47.081169: step 12483, loss 0.58016.
Train: 2018-08-05T19:11:47.284213: step 12484, loss 0.500359.
Train: 2018-08-05T19:11:47.487320: step 12485, loss 0.464468.
Train: 2018-08-05T19:11:47.690397: step 12486, loss 0.517621.
Train: 2018-08-05T19:11:47.877854: step 12487, loss 0.553675.
Train: 2018-08-05T19:11:48.096552: step 12488, loss 0.590729.
Train: 2018-08-05T19:11:48.284009: step 12489, loss 0.61809.
Train: 2018-08-05T19:11:48.487080: step 12490, loss 0.590522.
Test: 2018-08-05T19:11:49.549307: step 12490, loss 0.546699.
Train: 2018-08-05T19:11:49.768036: step 12491, loss 0.608802.
Train: 2018-08-05T19:11:49.971114: step 12492, loss 0.617566.
Train: 2018-08-05T19:11:50.174191: step 12493, loss 0.57132.
Train: 2018-08-05T19:11:50.377269: step 12494, loss 0.554664.
Train: 2018-08-05T19:11:50.580350: step 12495, loss 0.554003.
Train: 2018-08-05T19:11:50.783394: step 12496, loss 0.571055.
Train: 2018-08-05T19:11:51.002093: step 12497, loss 0.483431.
Train: 2018-08-05T19:11:51.205203: step 12498, loss 0.535924.
Train: 2018-08-05T19:11:51.408277: step 12499, loss 0.624599.
Train: 2018-08-05T19:11:51.611355: step 12500, loss 0.650359.
Test: 2018-08-05T19:11:52.673576: step 12500, loss 0.546795.
Train: 2018-08-05T19:11:53.720235: step 12501, loss 0.605972.
Train: 2018-08-05T19:11:53.938905: step 12502, loss 0.545074.
Train: 2018-08-05T19:11:54.142012: step 12503, loss 0.622251.
Train: 2018-08-05T19:11:54.345089: step 12504, loss 0.553891.
Train: 2018-08-05T19:11:54.548167: step 12505, loss 0.503694.
Train: 2018-08-05T19:11:54.751214: step 12506, loss 0.587457.
Train: 2018-08-05T19:11:54.954325: step 12507, loss 0.56246.
Train: 2018-08-05T19:11:55.157398: step 12508, loss 0.587275.
Train: 2018-08-05T19:11:55.360476: step 12509, loss 0.587192.
Train: 2018-08-05T19:11:55.579179: step 12510, loss 0.505412.
Test: 2018-08-05T19:11:56.625776: step 12510, loss 0.548233.
Train: 2018-08-05T19:11:56.828886: step 12511, loss 0.554452.
Train: 2018-08-05T19:11:57.031959: step 12512, loss 0.570772.
Train: 2018-08-05T19:11:57.235037: step 12513, loss 0.595253.
Train: 2018-08-05T19:11:57.453740: step 12514, loss 0.611523.
Train: 2018-08-05T19:11:57.656814: step 12515, loss 0.603272.
Train: 2018-08-05T19:11:57.859891: step 12516, loss 0.554643.
Train: 2018-08-05T19:11:58.078560: step 12517, loss 0.546656.
Train: 2018-08-05T19:11:58.281668: step 12518, loss 0.643239.
Train: 2018-08-05T19:11:58.484745: step 12519, loss 0.554852.
Train: 2018-08-05T19:11:58.687822: step 12520, loss 0.570883.
Test: 2018-08-05T19:11:59.734423: step 12520, loss 0.54911.
Train: 2018-08-05T19:11:59.953154: step 12521, loss 0.547035.
Train: 2018-08-05T19:12:00.156229: step 12522, loss 0.515247.
Train: 2018-08-05T19:12:00.359306: step 12523, loss 0.610769.
Train: 2018-08-05T19:12:00.562383: step 12524, loss 0.554908.
Train: 2018-08-05T19:12:00.765431: step 12525, loss 0.506852.
Train: 2018-08-05T19:12:00.968509: step 12526, loss 0.562777.
Train: 2018-08-05T19:12:01.171586: step 12527, loss 0.497903.
Train: 2018-08-05T19:12:01.374693: step 12528, loss 0.62803.
Train: 2018-08-05T19:12:01.577774: step 12529, loss 0.562531.
Train: 2018-08-05T19:12:01.796440: step 12530, loss 0.587302.
Test: 2018-08-05T19:12:02.843069: step 12530, loss 0.548966.
Train: 2018-08-05T19:12:03.108633: step 12531, loss 0.58737.
Train: 2018-08-05T19:12:03.327364: step 12532, loss 0.587415.
Train: 2018-08-05T19:12:03.499195: step 12533, loss 0.580211.
Train: 2018-08-05T19:12:03.717895: step 12534, loss 0.52908.
Train: 2018-08-05T19:12:03.920972: step 12535, loss 0.612559.
Train: 2018-08-05T19:12:04.124020: step 12536, loss 0.637605.
Train: 2018-08-05T19:12:04.342745: step 12537, loss 0.53749.
Train: 2018-08-05T19:12:04.545825: step 12538, loss 0.520961.
Train: 2018-08-05T19:12:04.748903: step 12539, loss 0.637208.
Train: 2018-08-05T19:12:04.951984: step 12540, loss 0.54592.
Test: 2018-08-05T19:12:05.998580: step 12540, loss 0.548686.
Train: 2018-08-05T19:12:06.217309: step 12541, loss 0.529427.
Train: 2018-08-05T19:12:06.436007: step 12542, loss 0.570756.
Train: 2018-08-05T19:12:06.639086: step 12543, loss 0.521081.
Train: 2018-08-05T19:12:06.842163: step 12544, loss 0.562452.
Train: 2018-08-05T19:12:07.060861: step 12545, loss 0.554092.
Train: 2018-08-05T19:12:07.279562: step 12546, loss 0.60425.
Train: 2018-08-05T19:12:07.482638: step 12547, loss 0.579158.
Train: 2018-08-05T19:12:07.685715: step 12548, loss 0.520476.
Train: 2018-08-05T19:12:07.888792: step 12549, loss 0.553971.
Train: 2018-08-05T19:12:08.091870: step 12550, loss 0.537051.
Test: 2018-08-05T19:12:09.154092: step 12550, loss 0.547802.
Train: 2018-08-05T19:12:09.372789: step 12551, loss 0.57083.
Train: 2018-08-05T19:12:09.575901: step 12552, loss 0.596385.
Train: 2018-08-05T19:12:09.778945: step 12553, loss 0.536769.
Train: 2018-08-05T19:12:09.997673: step 12554, loss 0.613625.
Train: 2018-08-05T19:12:10.200755: step 12555, loss 0.587973.
Train: 2018-08-05T19:12:10.403829: step 12556, loss 0.519686.
Train: 2018-08-05T19:12:10.606906: step 12557, loss 0.511111.
Train: 2018-08-05T19:12:10.825609: step 12558, loss 0.579473.
Train: 2018-08-05T19:12:11.028682: step 12559, loss 0.467861.
Train: 2018-08-05T19:12:11.247381: step 12560, loss 0.510437.
Test: 2018-08-05T19:12:12.293982: step 12560, loss 0.547587.
Train: 2018-08-05T19:12:12.512680: step 12561, loss 0.632214.
Train: 2018-08-05T19:12:12.715757: step 12562, loss 0.57992.
Train: 2018-08-05T19:12:12.934486: step 12563, loss 0.588763.
Train: 2018-08-05T19:12:13.137563: step 12564, loss 0.527236.
Train: 2018-08-05T19:12:13.340610: step 12565, loss 0.57122.
Train: 2018-08-05T19:12:13.543718: step 12566, loss 0.527156.
Train: 2018-08-05T19:12:13.746796: step 12567, loss 0.64195.
Train: 2018-08-05T19:12:13.965494: step 12568, loss 0.588849.
Train: 2018-08-05T19:12:14.168572: step 12569, loss 0.518521.
Train: 2018-08-05T19:12:14.371650: step 12570, loss 0.544866.
Test: 2018-08-05T19:12:15.433902: step 12570, loss 0.547564.
Train: 2018-08-05T19:12:15.683842: step 12571, loss 0.553628.
Train: 2018-08-05T19:12:15.886919: step 12572, loss 0.597348.
Train: 2018-08-05T19:12:16.089997: step 12573, loss 0.510053.
Train: 2018-08-05T19:12:16.308696: step 12574, loss 0.605957.
Train: 2018-08-05T19:12:16.511743: step 12575, loss 0.623227.
Train: 2018-08-05T19:12:16.730442: step 12576, loss 0.588275.
Train: 2018-08-05T19:12:16.933551: step 12577, loss 0.605259.
Train: 2018-08-05T19:12:17.136628: step 12578, loss 0.536813.
Train: 2018-08-05T19:12:17.355326: step 12579, loss 0.570816.
Train: 2018-08-05T19:12:17.558403: step 12580, loss 0.612809.
Test: 2018-08-05T19:12:18.605035: step 12580, loss 0.548893.
Train: 2018-08-05T19:12:18.870593: step 12581, loss 0.604106.
Train: 2018-08-05T19:12:19.136155: step 12582, loss 0.570756.
Train: 2018-08-05T19:12:19.354858: step 12583, loss 0.603505.
Train: 2018-08-05T19:12:19.557935: step 12584, loss 0.522159.
Train: 2018-08-05T19:12:19.761013: step 12585, loss 0.643357.
Train: 2018-08-05T19:12:19.964090: step 12586, loss 0.506991.
Train: 2018-08-05T19:12:20.182759: step 12587, loss 0.578859.
Train: 2018-08-05T19:12:20.401482: step 12588, loss 0.51548.
Train: 2018-08-05T19:12:20.604536: step 12589, loss 0.531303.
Train: 2018-08-05T19:12:20.807613: step 12590, loss 0.562942.
Test: 2018-08-05T19:12:21.869893: step 12590, loss 0.549431.
Train: 2018-08-05T19:12:22.072975: step 12591, loss 0.514702.
Train: 2018-08-05T19:12:22.276048: step 12592, loss 0.646068.
Train: 2018-08-05T19:12:22.479126: step 12593, loss 0.53819.
Train: 2018-08-05T19:12:22.697795: step 12594, loss 0.52212.
Train: 2018-08-05T19:12:22.900872: step 12595, loss 0.554478.
Train: 2018-08-05T19:12:23.135224: step 12596, loss 0.595353.
Train: 2018-08-05T19:12:23.353921: step 12597, loss 0.595429.
Train: 2018-08-05T19:12:23.557003: step 12598, loss 0.546033.
Train: 2018-08-05T19:12:23.760075: step 12599, loss 0.595573.
Train: 2018-08-05T19:12:23.978775: step 12600, loss 0.56247.
Test: 2018-08-05T19:12:25.025404: step 12600, loss 0.548011.
Train: 2018-08-05T19:12:26.009519: step 12601, loss 0.570759.
Train: 2018-08-05T19:12:26.228249: step 12602, loss 0.520847.
Train: 2018-08-05T19:12:26.431325: step 12603, loss 0.528996.
Train: 2018-08-05T19:12:26.634404: step 12604, loss 0.511949.
Train: 2018-08-05T19:12:26.853072: step 12605, loss 0.519963.
Train: 2018-08-05T19:12:27.056179: step 12606, loss 0.545206.
Train: 2018-08-05T19:12:27.259256: step 12607, loss 0.510433.
Train: 2018-08-05T19:12:27.477955: step 12608, loss 0.553625.
Train: 2018-08-05T19:12:27.681033: step 12609, loss 0.580143.
Train: 2018-08-05T19:12:27.899731: step 12610, loss 0.571434.
Test: 2018-08-05T19:12:28.946331: step 12610, loss 0.548818.
Train: 2018-08-05T19:12:29.165060: step 12611, loss 0.571553.
Train: 2018-08-05T19:12:29.383730: step 12612, loss 0.598702.
Train: 2018-08-05T19:12:29.602458: step 12613, loss 0.535554.
Train: 2018-08-05T19:12:29.805535: step 12614, loss 0.598851.
Train: 2018-08-05T19:12:30.008583: step 12615, loss 0.544575.
Train: 2018-08-05T19:12:30.227281: step 12616, loss 0.589723.
Train: 2018-08-05T19:12:30.430359: step 12617, loss 0.589531.
Train: 2018-08-05T19:12:30.649088: step 12618, loss 0.597473.
Train: 2018-08-05T19:12:30.852165: step 12619, loss 0.603217.
Train: 2018-08-05T19:12:31.055213: step 12620, loss 0.540127.
Test: 2018-08-05T19:12:32.117494: step 12620, loss 0.551359.
Train: 2018-08-05T19:12:32.320575: step 12621, loss 0.609239.
Train: 2018-08-05T19:12:32.539239: step 12622, loss 0.539717.
Train: 2018-08-05T19:12:32.742347: step 12623, loss 0.529438.
Train: 2018-08-05T19:12:32.945429: step 12624, loss 0.501898.
Train: 2018-08-05T19:12:33.164123: step 12625, loss 0.535813.
Train: 2018-08-05T19:12:33.367202: step 12626, loss 0.580571.
Train: 2018-08-05T19:12:33.570248: step 12627, loss 0.598241.
Train: 2018-08-05T19:12:33.788977: step 12628, loss 0.598112.
Train: 2018-08-05T19:12:33.992056: step 12629, loss 0.606839.
Train: 2018-08-05T19:12:34.210724: step 12630, loss 0.562339.
Test: 2018-08-05T19:12:35.257354: step 12630, loss 0.550027.
Train: 2018-08-05T19:12:35.476052: step 12631, loss 0.570764.
Train: 2018-08-05T19:12:35.694785: step 12632, loss 0.618564.
Train: 2018-08-05T19:12:35.897858: step 12633, loss 0.545839.
Train: 2018-08-05T19:12:36.100937: step 12634, loss 0.511995.
Train: 2018-08-05T19:12:36.319605: step 12635, loss 0.595484.
Train: 2018-08-05T19:12:36.522712: step 12636, loss 0.570932.
Train: 2018-08-05T19:12:36.741381: step 12637, loss 0.62347.
Train: 2018-08-05T19:12:36.944492: step 12638, loss 0.562345.
Train: 2018-08-05T19:12:37.163187: step 12639, loss 0.613693.
Train: 2018-08-05T19:12:37.366235: step 12640, loss 0.546591.
Test: 2018-08-05T19:12:38.428486: step 12640, loss 0.547872.
Train: 2018-08-05T19:12:38.631593: step 12641, loss 0.488585.
Train: 2018-08-05T19:12:38.850293: step 12642, loss 0.562336.
Train: 2018-08-05T19:12:39.053370: step 12643, loss 0.510609.
Train: 2018-08-05T19:12:39.272069: step 12644, loss 0.484495.
Train: 2018-08-05T19:12:39.490761: step 12645, loss 0.483908.
Train: 2018-08-05T19:12:39.693814: step 12646, loss 0.580178.
Train: 2018-08-05T19:12:39.896922: step 12647, loss 0.553589.
Train: 2018-08-05T19:12:40.115620: step 12648, loss 0.517784.
Train: 2018-08-05T19:12:40.318702: step 12649, loss 0.544581.
Train: 2018-08-05T19:12:40.537396: step 12650, loss 0.644628.
Test: 2018-08-05T19:12:41.583998: step 12650, loss 0.546703.
Train: 2018-08-05T19:12:41.802696: step 12651, loss 0.55365.
Train: 2018-08-05T19:12:42.005803: step 12652, loss 0.526273.
Train: 2018-08-05T19:12:42.224502: step 12653, loss 0.498777.
Train: 2018-08-05T19:12:42.427579: step 12654, loss 0.60885.
Train: 2018-08-05T19:12:42.646248: step 12655, loss 0.572105.
Train: 2018-08-05T19:12:42.849326: step 12656, loss 0.581274.
Train: 2018-08-05T19:12:43.083676: step 12657, loss 0.507855.
Train: 2018-08-05T19:12:43.286724: step 12658, loss 0.54452.
Train: 2018-08-05T19:12:43.489830: step 12659, loss 0.572.
Train: 2018-08-05T19:12:43.708529: step 12660, loss 0.626826.
Test: 2018-08-05T19:12:44.755129: step 12660, loss 0.547754.
Train: 2018-08-05T19:12:44.973858: step 12661, loss 0.553636.
Train: 2018-08-05T19:12:45.192561: step 12662, loss 0.580735.
Train: 2018-08-05T19:12:45.411256: step 12663, loss 0.571553.
Train: 2018-08-05T19:12:45.614335: step 12664, loss 0.624894.
Train: 2018-08-05T19:12:45.833032: step 12665, loss 0.544785.
Train: 2018-08-05T19:12:46.036109: step 12666, loss 0.579841.
Train: 2018-08-05T19:12:46.239186: step 12667, loss 0.510444.
Train: 2018-08-05T19:12:46.457886: step 12668, loss 0.570935.
Train: 2018-08-05T19:12:46.660964: step 12669, loss 0.596533.
Train: 2018-08-05T19:12:46.879662: step 12670, loss 0.477519.
Test: 2018-08-05T19:12:47.926292: step 12670, loss 0.547612.
Train: 2018-08-05T19:12:48.144990: step 12671, loss 0.528449.
Train: 2018-08-05T19:12:48.348068: step 12672, loss 0.536884.
Train: 2018-08-05T19:12:48.566736: step 12673, loss 0.528288.
Train: 2018-08-05T19:12:48.769839: step 12674, loss 0.57089.
Train: 2018-08-05T19:12:48.988543: step 12675, loss 0.536581.
Train: 2018-08-05T19:12:49.191624: step 12676, loss 0.553714.
Train: 2018-08-05T19:12:49.410319: step 12677, loss 0.493048.
Train: 2018-08-05T19:12:49.613396: step 12678, loss 0.562378.
Train: 2018-08-05T19:12:49.816478: step 12679, loss 0.571244.
Train: 2018-08-05T19:12:50.035173: step 12680, loss 0.518311.
Test: 2018-08-05T19:12:51.081803: step 12680, loss 0.547641.
Train: 2018-08-05T19:12:51.284850: step 12681, loss 0.597979.
Train: 2018-08-05T19:12:51.503579: step 12682, loss 0.607013.
Train: 2018-08-05T19:12:51.706627: step 12683, loss 0.651503.
Train: 2018-08-05T19:12:51.894083: step 12684, loss 0.449164.
Train: 2018-08-05T19:12:52.097190: step 12685, loss 0.606725.
Train: 2018-08-05T19:12:52.300238: step 12686, loss 0.597766.
Train: 2018-08-05T19:12:52.518966: step 12687, loss 0.46569.
Train: 2018-08-05T19:12:52.722043: step 12688, loss 0.518404.
Train: 2018-08-05T19:12:52.940746: step 12689, loss 0.606564.
Train: 2018-08-05T19:12:53.143821: step 12690, loss 0.482998.
Test: 2018-08-05T19:12:54.206071: step 12690, loss 0.546636.
Train: 2018-08-05T19:12:54.409149: step 12691, loss 0.606752.
Train: 2018-08-05T19:12:54.627818: step 12692, loss 0.562456.
Train: 2018-08-05T19:12:54.830925: step 12693, loss 0.624504.
Train: 2018-08-05T19:12:55.034003: step 12694, loss 0.562425.
Train: 2018-08-05T19:12:55.252701: step 12695, loss 0.492123.
Train: 2018-08-05T19:12:55.455779: step 12696, loss 0.527236.
Train: 2018-08-05T19:12:55.658826: step 12697, loss 0.58883.
Train: 2018-08-05T19:12:55.877556: step 12698, loss 0.59762.
Train: 2018-08-05T19:12:56.091668: step 12699, loss 0.667725.
Train: 2018-08-05T19:12:56.294778: step 12700, loss 0.510171.
Test: 2018-08-05T19:12:57.341378: step 12700, loss 0.547895.
Train: 2018-08-05T19:12:58.294311: step 12701, loss 0.579643.
Train: 2018-08-05T19:12:58.497387: step 12702, loss 0.588127.
Train: 2018-08-05T19:12:58.716057: step 12703, loss 0.519667.
Train: 2018-08-05T19:12:58.919163: step 12704, loss 0.579349.
Train: 2018-08-05T19:12:59.122242: step 12705, loss 0.570821.
Train: 2018-08-05T19:12:59.325319: step 12706, loss 0.579225.
Train: 2018-08-05T19:12:59.528398: step 12707, loss 0.520474.
Train: 2018-08-05T19:12:59.747094: step 12708, loss 0.604273.
Train: 2018-08-05T19:12:59.950142: step 12709, loss 0.545738.
Train: 2018-08-05T19:13:00.153251: step 12710, loss 0.554107.
Test: 2018-08-05T19:13:01.215471: step 12710, loss 0.549113.
Train: 2018-08-05T19:13:01.418578: step 12711, loss 0.595733.
Train: 2018-08-05T19:13:01.637277: step 12712, loss 0.554153.
Train: 2018-08-05T19:13:01.840324: step 12713, loss 0.545878.
Train: 2018-08-05T19:13:02.043432: step 12714, loss 0.512669.
Train: 2018-08-05T19:13:02.246509: step 12715, loss 0.570765.
Train: 2018-08-05T19:13:02.465208: step 12716, loss 0.654431.
Train: 2018-08-05T19:13:02.668286: step 12717, loss 0.495655.
Train: 2018-08-05T19:13:02.886955: step 12718, loss 0.545674.
Train: 2018-08-05T19:13:03.090061: step 12719, loss 0.621162.
Train: 2018-08-05T19:13:03.308730: step 12720, loss 0.553996.
Test: 2018-08-05T19:13:04.355390: step 12720, loss 0.547201.
Train: 2018-08-05T19:13:04.558468: step 12721, loss 0.595981.
Train: 2018-08-05T19:13:04.761545: step 12722, loss 0.570781.
Train: 2018-08-05T19:13:04.964593: step 12723, loss 0.562402.
Train: 2018-08-05T19:13:05.167671: step 12724, loss 0.57914.
Train: 2018-08-05T19:13:05.370748: step 12725, loss 0.562417.
Train: 2018-08-05T19:13:05.589472: step 12726, loss 0.637501.
Train: 2018-08-05T19:13:05.792554: step 12727, loss 0.662033.
Train: 2018-08-05T19:13:05.995631: step 12728, loss 0.562549.
Train: 2018-08-05T19:13:06.198679: step 12729, loss 0.562642.
Train: 2018-08-05T19:13:06.401789: step 12730, loss 0.586963.
Test: 2018-08-05T19:13:07.464038: step 12730, loss 0.548157.
Train: 2018-08-05T19:13:07.667115: step 12731, loss 0.546804.
Train: 2018-08-05T19:13:07.870194: step 12732, loss 0.602789.
Train: 2018-08-05T19:13:08.073270: step 12733, loss 0.666058.
Train: 2018-08-05T19:13:08.276349: step 12734, loss 0.571042.
Train: 2018-08-05T19:13:08.479424: step 12735, loss 0.540125.
Train: 2018-08-05T19:13:08.698093: step 12736, loss 0.602083.
Train: 2018-08-05T19:13:08.901203: step 12737, loss 0.517694.
Train: 2018-08-05T19:13:09.104249: step 12738, loss 0.586648.
Train: 2018-08-05T19:13:09.307358: step 12739, loss 0.5025.
Train: 2018-08-05T19:13:09.510435: step 12740, loss 0.509741.
Test: 2018-08-05T19:13:10.572684: step 12740, loss 0.550045.
Train: 2018-08-05T19:13:10.760111: step 12741, loss 0.509003.
Train: 2018-08-05T19:13:10.963188: step 12742, loss 0.492288.
Train: 2018-08-05T19:13:11.166296: step 12743, loss 0.554854.
Train: 2018-08-05T19:13:11.369343: step 12744, loss 0.497616.
Train: 2018-08-05T19:13:11.588042: step 12745, loss 0.529376.
Train: 2018-08-05T19:13:11.791120: step 12746, loss 0.537098.
Train: 2018-08-05T19:13:11.994226: step 12747, loss 0.639482.
Train: 2018-08-05T19:13:12.197275: step 12748, loss 0.545016.
Train: 2018-08-05T19:13:12.400352: step 12749, loss 0.544872.
Train: 2018-08-05T19:13:12.587838: step 12750, loss 0.571275.
Test: 2018-08-05T19:13:13.650059: step 12750, loss 0.54762.
Train: 2018-08-05T19:13:13.853135: step 12751, loss 0.535774.
Train: 2018-08-05T19:13:14.040622: step 12752, loss 0.517687.
Train: 2018-08-05T19:13:14.243695: step 12753, loss 0.562679.
Train: 2018-08-05T19:13:14.446777: step 12754, loss 0.526284.
Train: 2018-08-05T19:13:14.649850: step 12755, loss 0.553709.
Train: 2018-08-05T19:13:14.852903: step 12756, loss 0.590785.
Train: 2018-08-05T19:13:15.040388: step 12757, loss 0.54451.
Train: 2018-08-05T19:13:15.243435: step 12758, loss 0.544513.
Train: 2018-08-05T19:13:15.462135: step 12759, loss 0.451311.
Train: 2018-08-05T19:13:15.665242: step 12760, loss 0.525753.
Test: 2018-08-05T19:13:16.711871: step 12760, loss 0.548792.
Train: 2018-08-05T19:13:16.914949: step 12761, loss 0.525644.
Train: 2018-08-05T19:13:17.118027: step 12762, loss 0.554175.
Train: 2018-08-05T19:13:17.305454: step 12763, loss 0.525493.
Train: 2018-08-05T19:13:17.508560: step 12764, loss 0.641256.
Train: 2018-08-05T19:13:17.711639: step 12765, loss 0.506175.
Train: 2018-08-05T19:13:17.899088: step 12766, loss 0.49652.
Train: 2018-08-05T19:13:18.102142: step 12767, loss 0.602771.
Train: 2018-08-05T19:13:18.305248: step 12768, loss 0.554393.
Train: 2018-08-05T19:13:18.492705: step 12769, loss 0.592816.
Train: 2018-08-05T19:13:18.695782: step 12770, loss 0.554211.
Test: 2018-08-05T19:13:19.758034: step 12770, loss 0.54729.
Train: 2018-08-05T19:13:19.945489: step 12771, loss 0.667982.
Train: 2018-08-05T19:13:20.148569: step 12772, loss 0.590554.
Train: 2018-08-05T19:13:20.336024: step 12773, loss 0.596031.
Train: 2018-08-05T19:13:20.539101: step 12774, loss 0.527813.
Train: 2018-08-05T19:13:20.726557: step 12775, loss 0.571242.
Train: 2018-08-05T19:13:20.929635: step 12776, loss 0.52913.
Train: 2018-08-05T19:13:21.132682: step 12777, loss 0.501022.
Train: 2018-08-05T19:13:21.320168: step 12778, loss 0.580412.
Train: 2018-08-05T19:13:21.523247: step 12779, loss 0.544641.
Train: 2018-08-05T19:13:21.710671: step 12780, loss 0.544631.
Test: 2018-08-05T19:13:22.772955: step 12780, loss 0.547581.
Train: 2018-08-05T19:13:22.960408: step 12781, loss 0.508725.
Train: 2018-08-05T19:13:23.163456: step 12782, loss 0.544592.
Train: 2018-08-05T19:13:23.350942: step 12783, loss 0.52648.
Train: 2018-08-05T19:13:23.569612: step 12784, loss 0.599071.
Train: 2018-08-05T19:13:23.772721: step 12785, loss 0.662802.
Train: 2018-08-05T19:13:23.960175: step 12786, loss 0.598833.
Train: 2018-08-05T19:13:24.163223: step 12787, loss 0.65226.
Train: 2018-08-05T19:13:24.366330: step 12788, loss 0.624393.
Train: 2018-08-05T19:13:24.569403: step 12789, loss 0.562357.
Train: 2018-08-05T19:13:24.756832: step 12790, loss 0.605177.
Test: 2018-08-05T19:13:25.819085: step 12790, loss 0.547342.
Train: 2018-08-05T19:13:26.006573: step 12791, loss 0.620816.
Train: 2018-08-05T19:13:26.209617: step 12792, loss 0.586665.
Train: 2018-08-05T19:13:26.397106: step 12793, loss 0.546885.
Train: 2018-08-05T19:13:26.600181: step 12794, loss 0.476604.
Train: 2018-08-05T19:13:26.787637: step 12795, loss 0.52342.
Train: 2018-08-05T19:13:26.975093: step 12796, loss 0.667632.
Train: 2018-08-05T19:13:27.178171: step 12797, loss 0.530617.
Train: 2018-08-05T19:13:27.365627: step 12798, loss 0.594964.
Train: 2018-08-05T19:13:27.553083: step 12799, loss 0.570836.
Train: 2018-08-05T19:13:27.756161: step 12800, loss 0.546754.
Test: 2018-08-05T19:13:28.802761: step 12800, loss 0.549555.
Train: 2018-08-05T19:13:29.802527: step 12801, loss 0.554757.
Train: 2018-08-05T19:13:30.005634: step 12802, loss 0.514418.
Train: 2018-08-05T19:13:30.193089: step 12803, loss 0.59511.
Train: 2018-08-05T19:13:30.396138: step 12804, loss 0.538234.
Train: 2018-08-05T19:13:30.583593: step 12805, loss 0.611674.
Train: 2018-08-05T19:13:30.771083: step 12806, loss 0.587166.
Train: 2018-08-05T19:13:30.974128: step 12807, loss 0.562548.
Train: 2018-08-05T19:13:31.161613: step 12808, loss 0.587209.
Train: 2018-08-05T19:13:31.349070: step 12809, loss 0.620131.
Train: 2018-08-05T19:13:31.536525: step 12810, loss 0.505095.
Test: 2018-08-05T19:13:32.598777: step 12810, loss 0.548618.
Train: 2018-08-05T19:13:32.801854: step 12811, loss 0.603643.
Train: 2018-08-05T19:13:33.004931: step 12812, loss 0.603627.
Train: 2018-08-05T19:13:33.192388: step 12813, loss 0.562565.
Train: 2018-08-05T19:13:33.379844: step 12814, loss 0.595317.
Train: 2018-08-05T19:13:33.582922: step 12815, loss 0.49734.
Train: 2018-08-05T19:13:33.770347: step 12816, loss 0.660699.
Train: 2018-08-05T19:13:33.957833: step 12817, loss 0.497455.
Train: 2018-08-05T19:13:34.145291: step 12818, loss 0.57893.
Train: 2018-08-05T19:13:34.348367: step 12819, loss 0.513638.
Train: 2018-08-05T19:13:34.535823: step 12820, loss 0.54618.
Test: 2018-08-05T19:13:35.582423: step 12820, loss 0.548389.
Train: 2018-08-05T19:13:35.785543: step 12821, loss 0.51311.
Train: 2018-08-05T19:13:35.957365: step 12822, loss 0.520963.
Train: 2018-08-05T19:13:36.160442: step 12823, loss 0.537261.
Train: 2018-08-05T19:13:36.347899: step 12824, loss 0.570822.
Train: 2018-08-05T19:13:36.535325: step 12825, loss 0.528176.
Train: 2018-08-05T19:13:36.722811: step 12826, loss 0.553713.
Train: 2018-08-05T19:13:36.925888: step 12827, loss 0.544951.
Train: 2018-08-05T19:13:37.097723: step 12828, loss 0.492148.
Train: 2018-08-05T19:13:37.285150: step 12829, loss 0.535801.
Train: 2018-08-05T19:13:37.488257: step 12830, loss 0.599181.
Test: 2018-08-05T19:13:38.534856: step 12830, loss 0.548387.
Train: 2018-08-05T19:13:38.722312: step 12831, loss 0.535524.
Train: 2018-08-05T19:13:38.909799: step 12832, loss 0.599199.
Train: 2018-08-05T19:13:39.097225: step 12833, loss 0.526237.
Train: 2018-08-05T19:13:39.347196: step 12834, loss 0.507779.
Train: 2018-08-05T19:13:39.503412: step 12835, loss 0.6024.
Train: 2018-08-05T19:13:39.690866: step 12836, loss 0.60006.
Train: 2018-08-05T19:13:39.878292: step 12837, loss 0.516759.
Train: 2018-08-05T19:13:40.081394: step 12838, loss 0.553764.
Train: 2018-08-05T19:13:40.268857: step 12839, loss 0.581525.
Train: 2018-08-05T19:13:40.471933: step 12840, loss 0.590685.
Test: 2018-08-05T19:13:41.534153: step 12840, loss 0.547335.
Train: 2018-08-05T19:13:41.721640: step 12841, loss 0.61808.
Train: 2018-08-05T19:13:41.909067: step 12842, loss 0.644858.
Train: 2018-08-05T19:13:42.096523: step 12843, loss 0.447874.
Train: 2018-08-05T19:13:42.284009: step 12844, loss 0.490893.
Train: 2018-08-05T19:13:42.471465: step 12845, loss 0.544633.
Train: 2018-08-05T19:13:42.658890: step 12846, loss 0.616238.
Train: 2018-08-05T19:13:42.846377: step 12847, loss 0.589235.
Train: 2018-08-05T19:13:43.033832: step 12848, loss 0.518151.
Train: 2018-08-05T19:13:43.221289: step 12849, loss 0.562429.
Train: 2018-08-05T19:13:43.408745: step 12850, loss 0.518403.
Test: 2018-08-05T19:13:44.455375: step 12850, loss 0.547922.
Train: 2018-08-05T19:13:44.642833: step 12851, loss 0.623941.
Train: 2018-08-05T19:13:44.830289: step 12852, loss 0.527375.
Train: 2018-08-05T19:13:45.017745: step 12853, loss 0.483813.
Train: 2018-08-05T19:13:45.205199: step 12854, loss 0.571116.
Train: 2018-08-05T19:13:45.377005: step 12855, loss 0.623615.
Train: 2018-08-05T19:13:45.564492: step 12856, loss 0.492584.
Train: 2018-08-05T19:13:45.751946: step 12857, loss 0.457611.
Train: 2018-08-05T19:13:45.939402: step 12858, loss 0.527265.
Train: 2018-08-05T19:13:46.126859: step 12859, loss 0.527064.
Train: 2018-08-05T19:13:46.314315: step 12860, loss 0.571412.
Test: 2018-08-05T19:13:47.360915: step 12860, loss 0.547585.
Train: 2018-08-05T19:13:47.548370: step 12861, loss 0.517735.
Train: 2018-08-05T19:13:47.735860: step 12862, loss 0.499445.
Train: 2018-08-05T19:13:47.923313: step 12863, loss 0.599182.
Train: 2018-08-05T19:13:48.095148: step 12864, loss 0.617764.
Train: 2018-08-05T19:13:48.282605: step 12865, loss 0.526188.
Train: 2018-08-05T19:13:48.470060: step 12866, loss 0.562881.
Train: 2018-08-05T19:13:48.657516: step 12867, loss 0.581278.
Train: 2018-08-05T19:13:48.844973: step 12868, loss 0.562878.
Train: 2018-08-05T19:13:49.016809: step 12869, loss 0.526191.
Train: 2018-08-05T19:13:49.204233: step 12870, loss 0.59031.
Test: 2018-08-05T19:13:50.266484: step 12870, loss 0.546701.
Train: 2018-08-05T19:13:50.438349: step 12871, loss 0.581055.
Train: 2018-08-05T19:13:50.625808: step 12872, loss 0.571819.
Train: 2018-08-05T19:13:50.813262: step 12873, loss 0.625939.
Train: 2018-08-05T19:13:51.000718: step 12874, loss 0.641626.
Train: 2018-08-05T19:13:51.172552: step 12875, loss 0.562733.
Train: 2018-08-05T19:13:51.359978: step 12876, loss 0.54066.
Train: 2018-08-05T19:13:51.547465: step 12877, loss 0.570838.
Train: 2018-08-05T19:13:51.719299: step 12878, loss 0.496262.
Train: 2018-08-05T19:13:51.906725: step 12879, loss 0.502254.
Train: 2018-08-05T19:13:52.078590: step 12880, loss 0.562351.
Test: 2018-08-05T19:13:53.140845: step 12880, loss 0.547671.
Train: 2018-08-05T19:13:53.312646: step 12881, loss 0.623018.
Train: 2018-08-05T19:13:53.500132: step 12882, loss 0.510446.
Train: 2018-08-05T19:13:53.687591: step 12883, loss 0.605621.
Train: 2018-08-05T19:13:53.859423: step 12884, loss 0.5537.
Train: 2018-08-05T19:13:54.046849: step 12885, loss 0.55371.
Train: 2018-08-05T19:13:54.234305: step 12886, loss 0.553716.
Train: 2018-08-05T19:13:54.406140: step 12887, loss 0.51063.
Train: 2018-08-05T19:13:54.593626: step 12888, loss 0.622802.
Train: 2018-08-05T19:13:54.796704: step 12889, loss 0.605469.
Train: 2018-08-05T19:13:54.968539: step 12890, loss 0.545146.
Test: 2018-08-05T19:13:56.030789: step 12890, loss 0.54762.
Train: 2018-08-05T19:13:56.202624: step 12891, loss 0.639508.
Train: 2018-08-05T19:13:56.390083: step 12892, loss 0.5879.
Train: 2018-08-05T19:13:56.577537: step 12893, loss 0.570818.
Train: 2018-08-05T19:13:56.749374: step 12894, loss 0.671611.
Train: 2018-08-05T19:13:56.952445: step 12895, loss 0.520946.
Train: 2018-08-05T19:13:57.124283: step 12896, loss 0.496643.
Train: 2018-08-05T19:13:57.296118: step 12897, loss 0.505093.
Train: 2018-08-05T19:13:57.483575: step 12898, loss 0.595405.
Train: 2018-08-05T19:13:57.655409: step 12899, loss 0.537921.
Train: 2018-08-05T19:13:57.842865: step 12900, loss 0.521437.
Test: 2018-08-05T19:13:58.889497: step 12900, loss 0.548721.
Train: 2018-08-05T19:13:59.826775: step 12901, loss 0.628524.
Train: 2018-08-05T19:14:00.014226: step 12902, loss 0.512975.
Train: 2018-08-05T19:14:00.186067: step 12903, loss 0.587322.
Train: 2018-08-05T19:14:00.357901: step 12904, loss 0.570758.
Train: 2018-08-05T19:14:00.545327: step 12905, loss 0.595701.
Train: 2018-08-05T19:14:00.732813: step 12906, loss 0.520878.
Train: 2018-08-05T19:14:00.904648: step 12907, loss 0.554093.
Train: 2018-08-05T19:14:01.092104: step 12908, loss 0.587497.
Train: 2018-08-05T19:14:01.279560: step 12909, loss 0.537272.
Train: 2018-08-05T19:14:01.451365: step 12910, loss 0.654814.
Test: 2018-08-05T19:14:02.513649: step 12910, loss 0.547812.
Train: 2018-08-05T19:14:02.685477: step 12911, loss 0.562395.
Train: 2018-08-05T19:14:02.872908: step 12912, loss 0.562404.
Train: 2018-08-05T19:14:03.044773: step 12913, loss 0.520602.
Train: 2018-08-05T19:14:03.232228: step 12914, loss 0.57915.
Train: 2018-08-05T19:14:03.404063: step 12915, loss 0.595916.
Train: 2018-08-05T19:14:03.575898: step 12916, loss 0.570775.
Train: 2018-08-05T19:14:03.763356: step 12917, loss 0.629297.
Train: 2018-08-05T19:14:03.935188: step 12918, loss 0.570762.
Train: 2018-08-05T19:14:04.107023: step 12919, loss 0.562469.
Train: 2018-08-05T19:14:04.294450: step 12920, loss 0.545975.
Test: 2018-08-05T19:14:05.341078: step 12920, loss 0.548929.
Train: 2018-08-05T19:14:05.528565: step 12921, loss 0.570757.
Train: 2018-08-05T19:14:05.700400: step 12922, loss 0.587222.
Train: 2018-08-05T19:14:05.872205: step 12923, loss 0.628249.
Train: 2018-08-05T19:14:06.044070: step 12924, loss 0.636118.
Train: 2018-08-05T19:14:06.231525: step 12925, loss 0.449305.
Train: 2018-08-05T19:14:06.403361: step 12926, loss 0.530325.
Train: 2018-08-05T19:14:06.590817: step 12927, loss 0.538347.
Train: 2018-08-05T19:14:06.778269: step 12928, loss 0.595204.
Train: 2018-08-05T19:14:06.950110: step 12929, loss 0.505501.
Train: 2018-08-05T19:14:07.153185: step 12930, loss 0.480502.
Test: 2018-08-05T19:14:08.199785: step 12930, loss 0.548435.
Train: 2018-08-05T19:14:08.387273: step 12931, loss 0.55418.
Train: 2018-08-05T19:14:08.559100: step 12932, loss 0.512174.
Train: 2018-08-05T19:14:08.746562: step 12933, loss 0.460615.
Train: 2018-08-05T19:14:08.934018: step 12934, loss 0.518439.
Train: 2018-08-05T19:14:09.105825: step 12935, loss 0.506151.
Train: 2018-08-05T19:14:09.293309: step 12936, loss 0.643103.
Train: 2018-08-05T19:14:09.465143: step 12937, loss 0.609706.
Train: 2018-08-05T19:14:09.652599: step 12938, loss 0.544653.
Train: 2018-08-05T19:14:09.824435: step 12939, loss 0.544535.
Train: 2018-08-05T19:14:10.011861: step 12940, loss 0.535643.
Test: 2018-08-05T19:14:11.058520: step 12940, loss 0.547574.
Train: 2018-08-05T19:14:11.230355: step 12941, loss 0.499668.
Train: 2018-08-05T19:14:11.417812: step 12942, loss 0.544584.
Train: 2018-08-05T19:14:11.605267: step 12943, loss 0.662293.
Train: 2018-08-05T19:14:11.777071: step 12944, loss 0.598782.
Train: 2018-08-05T19:14:11.948907: step 12945, loss 0.481661.
Train: 2018-08-05T19:14:12.136395: step 12946, loss 0.571567.
Train: 2018-08-05T19:14:12.323849: step 12947, loss 0.571533.
Train: 2018-08-05T19:14:12.495684: step 12948, loss 0.634108.
Train: 2018-08-05T19:14:12.667518: step 12949, loss 0.455823.
Train: 2018-08-05T19:14:12.854974: step 12950, loss 0.491433.
Test: 2018-08-05T19:14:13.901576: step 12950, loss 0.548239.
Train: 2018-08-05T19:14:14.104680: step 12951, loss 0.589189.
Train: 2018-08-05T19:14:14.323378: step 12952, loss 0.544686.
Train: 2018-08-05T19:14:14.542076: step 12953, loss 0.562496.
Train: 2018-08-05T19:14:14.760782: step 12954, loss 0.571404.
Train: 2018-08-05T19:14:14.963827: step 12955, loss 0.642581.
Train: 2018-08-05T19:14:15.182556: step 12956, loss 0.562408.
Train: 2018-08-05T19:14:15.354391: step 12957, loss 0.579467.
Train: 2018-08-05T19:14:15.541846: step 12958, loss 0.570982.
Train: 2018-08-05T19:14:15.713681: step 12959, loss 0.555096.
Train: 2018-08-05T19:14:15.885517: step 12960, loss 0.654796.
Test: 2018-08-05T19:14:16.932114: step 12960, loss 0.549583.
Train: 2018-08-05T19:14:17.135225: step 12961, loss 0.570776.
Train: 2018-08-05T19:14:17.307058: step 12962, loss 0.507811.
Train: 2018-08-05T19:14:17.494516: step 12963, loss 0.595351.
Train: 2018-08-05T19:14:17.666348: step 12964, loss 0.545855.
Train: 2018-08-05T19:14:17.838152: step 12965, loss 0.562383.
Train: 2018-08-05T19:14:18.025609: step 12966, loss 0.562355.
Train: 2018-08-05T19:14:18.197473: step 12967, loss 0.562347.
Train: 2018-08-05T19:14:18.369279: step 12968, loss 0.655702.
Train: 2018-08-05T19:14:18.556753: step 12969, loss 0.528613.
Train: 2018-08-05T19:14:18.728569: step 12970, loss 0.553968.
Test: 2018-08-05T19:14:19.775199: step 12970, loss 0.548373.
Train: 2018-08-05T19:14:19.962685: step 12971, loss 0.579177.
Train: 2018-08-05T19:14:20.134522: step 12972, loss 0.554033.
Train: 2018-08-05T19:14:20.306325: step 12973, loss 0.503918.
Train: 2018-08-05T19:14:20.493813: step 12974, loss 0.528914.
Train: 2018-08-05T19:14:20.665615: step 12975, loss 0.579192.
Train: 2018-08-05T19:14:20.837476: step 12976, loss 0.528668.
Train: 2018-08-05T19:14:21.024936: step 12977, loss 0.638524.
Train: 2018-08-05T19:14:21.196771: step 12978, loss 0.562357.
Train: 2018-08-05T19:14:21.368606: step 12979, loss 0.587746.
Train: 2018-08-05T19:14:21.556031: step 12980, loss 0.604625.
Test: 2018-08-05T19:14:22.602662: step 12980, loss 0.547914.
Train: 2018-08-05T19:14:22.774527: step 12981, loss 0.58765.
Train: 2018-08-05T19:14:22.961983: step 12982, loss 0.595955.
Train: 2018-08-05T19:14:23.133819: step 12983, loss 0.570768.
Train: 2018-08-05T19:14:23.321245: step 12984, loss 0.603978.
Train: 2018-08-05T19:14:23.493079: step 12985, loss 0.512998.
Train: 2018-08-05T19:14:23.633703: step 12986, loss 0.509862.
Train: 2018-08-05T19:14:23.821157: step 12987, loss 0.537808.
Train: 2018-08-05T19:14:23.992991: step 12988, loss 0.537676.
Train: 2018-08-05T19:14:24.164826: step 12989, loss 0.604657.
Train: 2018-08-05T19:14:24.352252: step 12990, loss 0.695622.
Test: 2018-08-05T19:14:25.414504: step 12990, loss 0.549084.
Train: 2018-08-05T19:14:25.586369: step 12991, loss 0.620311.
Train: 2018-08-05T19:14:25.758172: step 12992, loss 0.521595.
Train: 2018-08-05T19:14:25.930008: step 12993, loss 0.554454.
Train: 2018-08-05T19:14:26.117493: step 12994, loss 0.635866.
Train: 2018-08-05T19:14:26.289328: step 12995, loss 0.586976.
Train: 2018-08-05T19:14:26.461163: step 12996, loss 0.627085.
Train: 2018-08-05T19:14:26.633000: step 12997, loss 0.554958.
Train: 2018-08-05T19:14:26.820455: step 12998, loss 0.63427.
Train: 2018-08-05T19:14:26.992289: step 12999, loss 0.586719.
Train: 2018-08-05T19:14:27.164094: step 13000, loss 0.532235.
Test: 2018-08-05T19:14:28.226375: step 13000, loss 0.55016.
Train: 2018-08-05T19:14:29.148004: step 13001, loss 0.478296.
Train: 2018-08-05T19:14:29.319870: step 13002, loss 0.547897.
Train: 2018-08-05T19:14:29.507327: step 13003, loss 0.532202.
Train: 2018-08-05T19:14:29.679129: step 13004, loss 0.492737.
Train: 2018-08-05T19:14:29.850995: step 13005, loss 0.539288.
Train: 2018-08-05T19:14:30.038421: step 13006, loss 0.554842.
Train: 2018-08-05T19:14:30.210285: step 13007, loss 0.578898.
Train: 2018-08-05T19:14:30.397711: step 13008, loss 0.57895.
Train: 2018-08-05T19:14:30.585198: step 13009, loss 0.545988.
Train: 2018-08-05T19:14:30.757003: step 13010, loss 0.537437.
Test: 2018-08-05T19:14:31.803663: step 13010, loss 0.549102.
Train: 2018-08-05T19:14:32.006742: step 13011, loss 0.629691.
Train: 2018-08-05T19:14:32.178575: step 13012, loss 0.553899.
Train: 2018-08-05T19:14:32.366025: step 13013, loss 0.51982.
Train: 2018-08-05T19:14:32.537866: step 13014, loss 0.562335.
Train: 2018-08-05T19:14:32.725322: step 13015, loss 0.553716.
Train: 2018-08-05T19:14:32.897125: step 13016, loss 0.57102.
Train: 2018-08-05T19:14:33.084582: step 13017, loss 0.510072.
Train: 2018-08-05T19:14:33.256417: step 13018, loss 0.588714.
Train: 2018-08-05T19:14:33.443904: step 13019, loss 0.597668.
Train: 2018-08-05T19:14:33.615738: step 13020, loss 0.571256.
Test: 2018-08-05T19:14:34.677990: step 13020, loss 0.5491.
Train: 2018-08-05T19:14:34.865445: step 13021, loss 0.62428.
Train: 2018-08-05T19:14:35.052873: step 13022, loss 0.553604.
Train: 2018-08-05T19:14:35.224737: step 13023, loss 0.597547.
Train: 2018-08-05T19:14:35.412192: step 13024, loss 0.509888.
Train: 2018-08-05T19:14:35.584027: step 13025, loss 0.632244.
Train: 2018-08-05T19:14:35.771483: step 13026, loss 0.579727.
Train: 2018-08-05T19:14:35.958939: step 13027, loss 0.596885.
Train: 2018-08-05T19:14:36.146396: step 13028, loss 0.59663.
Train: 2018-08-05T19:14:36.318201: step 13029, loss 0.613353.
Train: 2018-08-05T19:14:36.505688: step 13030, loss 0.638105.
Test: 2018-08-05T19:14:37.567937: step 13030, loss 0.548398.
Train: 2018-08-05T19:14:37.739743: step 13031, loss 0.545849.
Train: 2018-08-05T19:14:37.927229: step 13032, loss 0.595403.
Train: 2018-08-05T19:14:38.099066: step 13033, loss 0.60328.
Train: 2018-08-05T19:14:38.286490: step 13034, loss 0.530696.
Train: 2018-08-05T19:14:38.473975: step 13035, loss 0.539036.
Train: 2018-08-05T19:14:38.645810: step 13036, loss 0.555087.
Train: 2018-08-05T19:14:38.833266: step 13037, loss 0.499879.
Train: 2018-08-05T19:14:39.005103: step 13038, loss 0.531394.
Train: 2018-08-05T19:14:39.192557: step 13039, loss 0.570914.
Train: 2018-08-05T19:14:39.379984: step 13040, loss 0.57886.
Test: 2018-08-05T19:14:40.426643: step 13040, loss 0.550029.
Train: 2018-08-05T19:14:40.614099: step 13041, loss 0.59488.
Train: 2018-08-05T19:14:40.801526: step 13042, loss 0.594914.
Train: 2018-08-05T19:14:40.973384: step 13043, loss 0.530689.
Train: 2018-08-05T19:14:41.160817: step 13044, loss 0.530537.
Train: 2018-08-05T19:14:41.348303: step 13045, loss 0.562695.
Train: 2018-08-05T19:14:41.535759: step 13046, loss 0.481166.
Train: 2018-08-05T19:14:41.723186: step 13047, loss 0.537843.
Train: 2018-08-05T19:14:41.910670: step 13048, loss 0.612352.
Train: 2018-08-05T19:14:42.098096: step 13049, loss 0.587536.
Train: 2018-08-05T19:14:42.285583: step 13050, loss 0.511829.
Test: 2018-08-05T19:14:43.332213: step 13050, loss 0.54778.
Train: 2018-08-05T19:14:43.519670: step 13051, loss 0.613285.
Train: 2018-08-05T19:14:43.707125: step 13052, loss 0.56234.
Train: 2018-08-05T19:14:43.894552: step 13053, loss 0.596571.
Train: 2018-08-05T19:14:44.066416: step 13054, loss 0.562335.
Train: 2018-08-05T19:14:44.253874: step 13055, loss 0.502239.
Train: 2018-08-05T19:14:44.441298: step 13056, loss 0.510599.
Train: 2018-08-05T19:14:44.628786: step 13057, loss 0.623107.
Train: 2018-08-05T19:14:44.816240: step 13058, loss 0.571058.
Train: 2018-08-05T19:14:45.003698: step 13059, loss 0.5275.
Train: 2018-08-05T19:14:45.191122: step 13060, loss 0.527407.
Test: 2018-08-05T19:14:46.237753: step 13060, loss 0.54753.
Train: 2018-08-05T19:14:46.425238: step 13061, loss 0.562394.
Train: 2018-08-05T19:14:46.612694: step 13062, loss 0.580032.
Train: 2018-08-05T19:14:46.800120: step 13063, loss 0.606552.
Train: 2018-08-05T19:14:46.987578: step 13064, loss 0.518336.
Train: 2018-08-05T19:14:47.175033: step 13065, loss 0.562425.
Train: 2018-08-05T19:14:47.362519: step 13066, loss 0.597746.
Train: 2018-08-05T19:14:47.549975: step 13067, loss 0.491906.
Train: 2018-08-05T19:14:47.737400: step 13068, loss 0.4918.
Train: 2018-08-05T19:14:47.924887: step 13069, loss 0.571331.
Train: 2018-08-05T19:14:48.112343: step 13070, loss 0.517989.
Test: 2018-08-05T19:14:49.174595: step 13070, loss 0.547186.
Train: 2018-08-05T19:14:49.362051: step 13071, loss 0.508877.
Train: 2018-08-05T19:14:49.549509: step 13072, loss 0.589602.
Train: 2018-08-05T19:14:49.736963: step 13073, loss 0.57168.
Train: 2018-08-05T19:14:49.924413: step 13074, loss 0.553619.
Train: 2018-08-05T19:14:50.127498: step 13075, loss 0.562698.
Train: 2018-08-05T19:14:50.314923: step 13076, loss 0.56271.
Train: 2018-08-05T19:14:50.518030: step 13077, loss 0.53547.
Train: 2018-08-05T19:14:50.705486: step 13078, loss 0.580892.
Train: 2018-08-05T19:14:50.892913: step 13079, loss 0.589935.
Train: 2018-08-05T19:14:51.096019: step 13080, loss 0.59886.
Test: 2018-08-05T19:14:52.142620: step 13080, loss 0.547361.
Train: 2018-08-05T19:14:52.330106: step 13081, loss 0.589604.
Train: 2018-08-05T19:14:52.533183: step 13082, loss 0.598289.
Train: 2018-08-05T19:14:52.736263: step 13083, loss 0.544728.
Train: 2018-08-05T19:14:52.923717: step 13084, loss 0.615207.
Train: 2018-08-05T19:14:53.111175: step 13085, loss 0.501353.
Train: 2018-08-05T19:14:53.314250: step 13086, loss 0.640335.
Train: 2018-08-05T19:14:53.501708: step 13087, loss 0.519416.
Train: 2018-08-05T19:14:53.704784: step 13088, loss 0.596457.
Train: 2018-08-05T19:14:53.892240: step 13089, loss 0.587752.
Train: 2018-08-05T19:14:54.079696: step 13090, loss 0.537197.
Test: 2018-08-05T19:14:55.141917: step 13090, loss 0.549436.
Train: 2018-08-05T19:14:55.329403: step 13091, loss 0.562419.
Train: 2018-08-05T19:14:55.516830: step 13092, loss 0.595685.
Train: 2018-08-05T19:14:55.719937: step 13093, loss 0.620313.
Train: 2018-08-05T19:14:55.907393: step 13094, loss 0.529801.
Train: 2018-08-05T19:14:56.094849: step 13095, loss 0.505563.
Train: 2018-08-05T19:14:56.282305: step 13096, loss 0.546342.
Train: 2018-08-05T19:14:56.485382: step 13097, loss 0.578926.
Train: 2018-08-05T19:14:56.672841: step 13098, loss 0.56262.
Train: 2018-08-05T19:14:56.860295: step 13099, loss 0.55445.
Train: 2018-08-05T19:14:57.063342: step 13100, loss 0.505365.
Test: 2018-08-05T19:14:58.109972: step 13100, loss 0.547301.
Train: 2018-08-05T19:14:59.109768: step 13101, loss 0.570759.
Train: 2018-08-05T19:14:59.297195: step 13102, loss 0.59553.
Train: 2018-08-05T19:14:59.484680: step 13103, loss 0.570757.
Train: 2018-08-05T19:14:59.687728: step 13104, loss 0.570759.
Train: 2018-08-05T19:14:59.875184: step 13105, loss 0.5874.
Train: 2018-08-05T19:15:00.093912: step 13106, loss 0.595742.
Train: 2018-08-05T19:15:00.296960: step 13107, loss 0.562444.
Train: 2018-08-05T19:15:00.484446: step 13108, loss 0.537503.
Train: 2018-08-05T19:15:00.687524: step 13109, loss 0.537451.
Train: 2018-08-05T19:15:00.874981: step 13110, loss 0.629236.
Test: 2018-08-05T19:15:01.937201: step 13110, loss 0.547907.
Train: 2018-08-05T19:15:02.124689: step 13111, loss 0.537385.
Train: 2018-08-05T19:15:02.327764: step 13112, loss 0.503941.
Train: 2018-08-05T19:15:02.515190: step 13113, loss 0.545613.
Train: 2018-08-05T19:15:02.718298: step 13114, loss 0.579234.
Train: 2018-08-05T19:15:02.905754: step 13115, loss 0.579283.
Train: 2018-08-05T19:15:03.093179: step 13116, loss 0.485986.
Train: 2018-08-05T19:15:03.280666: step 13117, loss 0.494018.
Train: 2018-08-05T19:15:03.483714: step 13118, loss 0.519231.
Train: 2018-08-05T19:15:03.686792: step 13119, loss 0.553647.
Train: 2018-08-05T19:15:03.874279: step 13120, loss 0.456794.
Test: 2018-08-05T19:15:04.920878: step 13120, loss 0.548399.
Train: 2018-08-05T19:15:05.123987: step 13121, loss 0.652563.
Train: 2018-08-05T19:15:05.311410: step 13122, loss 0.499696.
Train: 2018-08-05T19:15:05.514518: step 13123, loss 0.598954.
Train: 2018-08-05T19:15:05.701974: step 13124, loss 0.544534.
Train: 2018-08-05T19:15:05.905052: step 13125, loss 0.471221.
Train: 2018-08-05T19:15:06.092508: step 13126, loss 0.590699.
Train: 2018-08-05T19:15:06.295585: step 13127, loss 0.590936.
Train: 2018-08-05T19:15:06.483042: step 13128, loss 0.488683.
Train: 2018-08-05T19:15:06.686119: step 13129, loss 0.516481.
Train: 2018-08-05T19:15:06.889196: step 13130, loss 0.629112.
Test: 2018-08-05T19:15:07.935826: step 13130, loss 0.547644.
Train: 2018-08-05T19:15:08.138903: step 13131, loss 0.497542.
Train: 2018-08-05T19:15:08.341981: step 13132, loss 0.563397.
Train: 2018-08-05T19:15:08.529439: step 13133, loss 0.58228.
Train: 2018-08-05T19:15:08.732514: step 13134, loss 0.516297.
Train: 2018-08-05T19:15:08.919970: step 13135, loss 0.601054.
Train: 2018-08-05T19:15:09.123050: step 13136, loss 0.516374.
Train: 2018-08-05T19:15:09.294853: step 13137, loss 0.583269.
Train: 2018-08-05T19:15:09.482341: step 13138, loss 0.619225.
Train: 2018-08-05T19:15:09.685386: step 13139, loss 0.581597.
Train: 2018-08-05T19:15:09.888494: step 13140, loss 0.461759.
Test: 2018-08-05T19:15:10.935124: step 13140, loss 0.547543.
Train: 2018-08-05T19:15:11.138171: step 13141, loss 0.599501.
Train: 2018-08-05T19:15:11.341249: step 13142, loss 0.626536.
Train: 2018-08-05T19:15:11.528734: step 13143, loss 0.508469.
Train: 2018-08-05T19:15:11.731783: step 13144, loss 0.625368.
Train: 2018-08-05T19:15:11.934889: step 13145, loss 0.642468.
Train: 2018-08-05T19:15:12.137936: step 13146, loss 0.509755.
Train: 2018-08-05T19:15:12.325393: step 13147, loss 0.579733.
Train: 2018-08-05T19:15:12.528500: step 13148, loss 0.536515.
Train: 2018-08-05T19:15:12.731578: step 13149, loss 0.596509.
Train: 2018-08-05T19:15:12.934656: step 13150, loss 0.562355.
Test: 2018-08-05T19:15:13.981285: step 13150, loss 0.548925.
Train: 2018-08-05T19:15:14.184363: step 13151, loss 0.545572.
Train: 2018-08-05T19:15:14.387409: step 13152, loss 0.537342.
Train: 2018-08-05T19:15:14.590518: step 13153, loss 0.562436.
Train: 2018-08-05T19:15:14.793595: step 13154, loss 0.520943.
Train: 2018-08-05T19:15:14.996674: step 13155, loss 0.520945.
Train: 2018-08-05T19:15:15.199752: step 13156, loss 0.545791.
Train: 2018-08-05T19:15:15.402830: step 13157, loss 0.503946.
Train: 2018-08-05T19:15:15.605875: step 13158, loss 0.469891.
Train: 2018-08-05T19:15:15.808983: step 13159, loss 0.553843.
Train: 2018-08-05T19:15:15.996437: step 13160, loss 0.605289.
Test: 2018-08-05T19:15:17.058690: step 13160, loss 0.5465.
Train: 2018-08-05T19:15:17.261761: step 13161, loss 0.666155.
Train: 2018-08-05T19:15:17.464845: step 13162, loss 0.553686.
Train: 2018-08-05T19:15:17.667892: step 13163, loss 0.562345.
Train: 2018-08-05T19:15:17.855378: step 13164, loss 0.536335.
Train: 2018-08-05T19:15:18.074047: step 13165, loss 0.536294.
Train: 2018-08-05T19:15:18.277154: step 13166, loss 0.536231.
Train: 2018-08-05T19:15:18.464612: step 13167, loss 0.544891.
Train: 2018-08-05T19:15:18.667657: step 13168, loss 0.571166.
Train: 2018-08-05T19:15:18.870768: step 13169, loss 0.615197.
Train: 2018-08-05T19:15:19.073844: step 13170, loss 0.536025.
Test: 2018-08-05T19:15:20.120475: step 13170, loss 0.547309.
Train: 2018-08-05T19:15:20.323550: step 13171, loss 0.536016.
Train: 2018-08-05T19:15:20.526630: step 13172, loss 0.527179.
Train: 2018-08-05T19:15:20.729705: step 13173, loss 0.606591.
Train: 2018-08-05T19:15:20.932782: step 13174, loss 0.571258.
Train: 2018-08-05T19:15:21.135860: step 13175, loss 0.615344.
Train: 2018-08-05T19:15:21.323285: step 13176, loss 0.606319.
Train: 2018-08-05T19:15:21.526363: step 13177, loss 0.509993.
Train: 2018-08-05T19:15:21.729471: step 13178, loss 0.640665.
Train: 2018-08-05T19:15:21.932542: step 13179, loss 0.56234.
Train: 2018-08-05T19:15:22.151247: step 13180, loss 0.570916.
Test: 2018-08-05T19:15:23.197877: step 13180, loss 0.547317.
Train: 2018-08-05T19:15:23.400954: step 13181, loss 0.485613.
Train: 2018-08-05T19:15:23.619623: step 13182, loss 0.613404.
Train: 2018-08-05T19:15:23.822731: step 13183, loss 0.469142.
Train: 2018-08-05T19:15:24.025809: step 13184, loss 0.553866.
Train: 2018-08-05T19:15:24.228885: step 13185, loss 0.604836.
Train: 2018-08-05T19:15:24.431932: step 13186, loss 0.553856.
Train: 2018-08-05T19:15:24.635011: step 13187, loss 0.562348.
Train: 2018-08-05T19:15:24.838117: step 13188, loss 0.553858.
Train: 2018-08-05T19:15:25.041196: step 13189, loss 0.570841.
Train: 2018-08-05T19:15:25.244275: step 13190, loss 0.519874.
Test: 2018-08-05T19:15:26.290873: step 13190, loss 0.548122.
Train: 2018-08-05T19:15:26.493982: step 13191, loss 0.579372.
Train: 2018-08-05T19:15:26.697058: step 13192, loss 0.511187.
Train: 2018-08-05T19:15:26.900135: step 13193, loss 0.570894.
Train: 2018-08-05T19:15:27.103212: step 13194, loss 0.588088.
Train: 2018-08-05T19:15:27.306290: step 13195, loss 0.622504.
Train: 2018-08-05T19:15:27.509367: step 13196, loss 0.502293.
Train: 2018-08-05T19:15:27.712448: step 13197, loss 0.605275.
Train: 2018-08-05T19:15:27.915522: step 13198, loss 0.64811.
Train: 2018-08-05T19:15:28.134191: step 13199, loss 0.604983.
Train: 2018-08-05T19:15:28.337268: step 13200, loss 0.587754.
Test: 2018-08-05T19:15:29.399550: step 13200, loss 0.549134.
Train: 2018-08-05T19:15:30.336801: step 13201, loss 0.587584.
Train: 2018-08-05T19:15:30.555529: step 13202, loss 0.570764.
Train: 2018-08-05T19:15:30.758606: step 13203, loss 0.554226.
Train: 2018-08-05T19:15:30.977305: step 13204, loss 0.595404.
Train: 2018-08-05T19:15:31.180383: step 13205, loss 0.570773.
Train: 2018-08-05T19:15:31.383460: step 13206, loss 0.554575.
Train: 2018-08-05T19:15:31.602129: step 13207, loss 0.619249.
Train: 2018-08-05T19:15:31.805237: step 13208, loss 0.538767.
Train: 2018-08-05T19:15:32.023935: step 13209, loss 0.586851.
Train: 2018-08-05T19:15:32.226983: step 13210, loss 0.531112.
Test: 2018-08-05T19:15:33.289247: step 13210, loss 0.54968.
Train: 2018-08-05T19:15:33.492343: step 13211, loss 0.714012.
Train: 2018-08-05T19:15:33.695418: step 13212, loss 0.523691.
Train: 2018-08-05T19:15:33.914120: step 13213, loss 0.476878.
Train: 2018-08-05T19:15:34.117197: step 13214, loss 0.547429.
Train: 2018-08-05T19:15:34.320273: step 13215, loss 0.578864.
Train: 2018-08-05T19:15:34.523350: step 13216, loss 0.507645.
Train: 2018-08-05T19:15:34.742051: step 13217, loss 0.578859.
Train: 2018-08-05T19:15:34.945128: step 13218, loss 0.522753.
Train: 2018-08-05T19:15:35.148203: step 13219, loss 0.506093.
Train: 2018-08-05T19:15:35.351281: step 13220, loss 0.59568.
Test: 2018-08-05T19:15:36.397880: step 13220, loss 0.548431.
Train: 2018-08-05T19:15:36.616612: step 13221, loss 0.587338.
Train: 2018-08-05T19:15:36.819687: step 13222, loss 0.52924.
Train: 2018-08-05T19:15:37.022735: step 13223, loss 0.528914.
Train: 2018-08-05T19:15:37.225842: step 13224, loss 0.537007.
Train: 2018-08-05T19:15:37.428919: step 13225, loss 0.545267.
Train: 2018-08-05T19:15:37.647618: step 13226, loss 0.545098.
Train: 2018-08-05T19:15:37.850696: step 13227, loss 0.544952.
Train: 2018-08-05T19:15:38.053773: step 13228, loss 0.54483.
Train: 2018-08-05T19:15:38.272472: step 13229, loss 0.535869.
Train: 2018-08-05T19:15:38.475549: step 13230, loss 0.598296.
Test: 2018-08-05T19:15:39.537800: step 13230, loss 0.546745.
Train: 2018-08-05T19:15:39.740848: step 13231, loss 0.517635.
Train: 2018-08-05T19:15:40.022033: step 13232, loss 0.562668.
Train: 2018-08-05T19:15:40.225110: step 13233, loss 0.535443.
Train: 2018-08-05T19:15:40.443808: step 13234, loss 0.544524.
Train: 2018-08-05T19:15:40.646916: step 13235, loss 0.535321.
Train: 2018-08-05T19:15:40.849993: step 13236, loss 0.609165.
Train: 2018-08-05T19:15:41.068688: step 13237, loss 0.57225.
Train: 2018-08-05T19:15:41.271770: step 13238, loss 0.544509.
Train: 2018-08-05T19:15:41.490471: step 13239, loss 0.562988.
Train: 2018-08-05T19:15:41.693546: step 13240, loss 0.599872.
Test: 2018-08-05T19:15:42.755798: step 13240, loss 0.547123.
Train: 2018-08-05T19:15:42.958874: step 13241, loss 0.535323.
Train: 2018-08-05T19:15:43.177567: step 13242, loss 0.572006.
Train: 2018-08-05T19:15:43.380651: step 13243, loss 0.58103.
Train: 2018-08-05T19:15:43.599350: step 13244, loss 0.62623.
Train: 2018-08-05T19:15:43.802427: step 13245, loss 0.535611.
Train: 2018-08-05T19:15:44.021125: step 13246, loss 0.508941.
Train: 2018-08-05T19:15:44.224203: step 13247, loss 0.571373.
Train: 2018-08-05T19:15:44.427281: step 13248, loss 0.58899.
Train: 2018-08-05T19:15:44.645950: step 13249, loss 0.562403.
Train: 2018-08-05T19:15:44.849057: step 13250, loss 0.571116.
Test: 2018-08-05T19:15:45.911308: step 13250, loss 0.546243.
Train: 2018-08-05T19:15:46.114356: step 13251, loss 0.571039.
Train: 2018-08-05T19:15:46.317465: step 13252, loss 0.596837.
Train: 2018-08-05T19:15:46.536131: step 13253, loss 0.537129.
Train: 2018-08-05T19:15:46.754860: step 13254, loss 0.562345.
Train: 2018-08-05T19:15:46.973562: step 13255, loss 0.528403.
Train: 2018-08-05T19:15:47.176637: step 13256, loss 0.672426.
Train: 2018-08-05T19:15:47.395336: step 13257, loss 0.553986.
Train: 2018-08-05T19:15:47.598413: step 13258, loss 0.579113.
Train: 2018-08-05T19:15:47.817083: step 13259, loss 0.603925.
Train: 2018-08-05T19:15:48.020189: step 13260, loss 0.529621.
Test: 2018-08-05T19:15:49.082438: step 13260, loss 0.54909.
Train: 2018-08-05T19:15:49.285488: step 13261, loss 0.570764.
Train: 2018-08-05T19:15:49.504217: step 13262, loss 0.554467.
Train: 2018-08-05T19:15:49.707264: step 13263, loss 0.530132.
Train: 2018-08-05T19:15:49.925993: step 13264, loss 0.538267.
Train: 2018-08-05T19:15:50.129040: step 13265, loss 0.546346.
Train: 2018-08-05T19:15:50.347769: step 13266, loss 0.603446.
Train: 2018-08-05T19:15:50.550817: step 13267, loss 0.529879.
Train: 2018-08-05T19:15:50.769516: step 13268, loss 0.595375.
Train: 2018-08-05T19:15:50.972623: step 13269, loss 0.529671.
Train: 2018-08-05T19:15:51.191322: step 13270, loss 0.55426.
Test: 2018-08-05T19:15:52.237954: step 13270, loss 0.547306.
Train: 2018-08-05T19:15:52.456651: step 13271, loss 0.653588.
Train: 2018-08-05T19:15:52.659728: step 13272, loss 0.554205.
Train: 2018-08-05T19:15:52.878427: step 13273, loss 0.587309.
Train: 2018-08-05T19:15:53.081504: step 13274, loss 0.562488.
Train: 2018-08-05T19:15:53.300202: step 13275, loss 0.570756.
Train: 2018-08-05T19:15:53.503281: step 13276, loss 0.488148.
Train: 2018-08-05T19:15:53.721979: step 13277, loss 0.520992.
Train: 2018-08-05T19:15:53.940648: step 13278, loss 0.512341.
Train: 2018-08-05T19:15:54.159376: step 13279, loss 0.528695.
Train: 2018-08-05T19:15:54.362456: step 13280, loss 0.536724.
Test: 2018-08-05T19:15:55.424705: step 13280, loss 0.54637.
Train: 2018-08-05T19:15:55.627782: step 13281, loss 0.562521.
Train: 2018-08-05T19:15:55.846482: step 13282, loss 0.58955.
Train: 2018-08-05T19:15:56.065151: step 13283, loss 0.597412.
Train: 2018-08-05T19:15:56.283850: step 13284, loss 0.527434.
Train: 2018-08-05T19:15:56.486959: step 13285, loss 0.571159.
Train: 2018-08-05T19:15:56.713547: step 13286, loss 0.562405.
Train: 2018-08-05T19:15:56.916623: step 13287, loss 0.509519.
Train: 2018-08-05T19:15:57.104079: step 13288, loss 0.562449.
Train: 2018-08-05T19:15:57.322778: step 13289, loss 0.598024.
Train: 2018-08-05T19:15:57.525856: step 13290, loss 0.624741.
Test: 2018-08-05T19:15:58.588077: step 13290, loss 0.548495.
Train: 2018-08-05T19:15:58.791184: step 13291, loss 0.580086.
Train: 2018-08-05T19:15:59.009883: step 13292, loss 0.587828.
Train: 2018-08-05T19:15:59.228581: step 13293, loss 0.502543.
Train: 2018-08-05T19:15:59.447280: step 13294, loss 0.529451.
Train: 2018-08-05T19:15:59.665979: step 13295, loss 0.562354.
Train: 2018-08-05T19:15:59.869057: step 13296, loss 0.606591.
Train: 2018-08-05T19:16:00.087755: step 13297, loss 0.597713.
Train: 2018-08-05T19:16:00.306424: step 13298, loss 0.606381.
Train: 2018-08-05T19:16:00.509531: step 13299, loss 0.62361.
Train: 2018-08-05T19:16:00.712610: step 13300, loss 0.62307.
Test: 2018-08-05T19:16:01.774864: step 13300, loss 0.548006.
Train: 2018-08-05T19:16:02.790248: step 13301, loss 0.536595.
Train: 2018-08-05T19:16:03.008917: step 13302, loss 0.596369.
Train: 2018-08-05T19:16:03.212025: step 13303, loss 0.570799.
Train: 2018-08-05T19:16:03.430723: step 13304, loss 0.554068.
Train: 2018-08-05T19:16:03.633801: step 13305, loss 0.653646.
Train: 2018-08-05T19:16:03.852468: step 13306, loss 0.554372.
Train: 2018-08-05T19:16:04.071168: step 13307, loss 0.562668.
Train: 2018-08-05T19:16:04.274245: step 13308, loss 0.530538.
Train: 2018-08-05T19:16:04.492943: step 13309, loss 0.578868.
Train: 2018-08-05T19:16:04.696051: step 13310, loss 0.602824.
Test: 2018-08-05T19:16:05.758303: step 13310, loss 0.549707.
Train: 2018-08-05T19:16:05.961379: step 13311, loss 0.578858.
Train: 2018-08-05T19:16:06.195672: step 13312, loss 0.555148.
Train: 2018-08-05T19:16:06.398748: step 13313, loss 0.507931.
Train: 2018-08-05T19:16:06.617476: step 13314, loss 0.523613.
Train: 2018-08-05T19:16:06.820553: step 13315, loss 0.515404.
Train: 2018-08-05T19:16:07.039273: step 13316, loss 0.52207.
Train: 2018-08-05T19:16:07.242336: step 13317, loss 0.562388.
Train: 2018-08-05T19:16:07.461029: step 13318, loss 0.525733.
Train: 2018-08-05T19:16:07.679698: step 13319, loss 0.569481.
Train: 2018-08-05T19:16:07.882805: step 13320, loss 0.535184.
Test: 2018-08-05T19:16:08.945027: step 13320, loss 0.547131.
Train: 2018-08-05T19:16:09.148134: step 13321, loss 0.609174.
Train: 2018-08-05T19:16:09.366803: step 13322, loss 0.648181.
Train: 2018-08-05T19:16:09.585531: step 13323, loss 0.595404.
Train: 2018-08-05T19:16:09.788608: step 13324, loss 0.529963.
Train: 2018-08-05T19:16:10.007307: step 13325, loss 0.587052.
Train: 2018-08-05T19:16:10.226006: step 13326, loss 0.595112.
Train: 2018-08-05T19:16:10.444705: step 13327, loss 0.522394.
Train: 2018-08-05T19:16:10.647754: step 13328, loss 0.530514.
Train: 2018-08-05T19:16:10.866452: step 13329, loss 0.546592.
Train: 2018-08-05T19:16:11.069561: step 13330, loss 0.611283.
Test: 2018-08-05T19:16:12.131810: step 13330, loss 0.549538.
Train: 2018-08-05T19:16:12.334887: step 13331, loss 0.619405.
Train: 2018-08-05T19:16:12.553557: step 13332, loss 0.538467.
Train: 2018-08-05T19:16:12.772273: step 13333, loss 0.538461.
Train: 2018-08-05T19:16:12.975362: step 13334, loss 0.587002.
Train: 2018-08-05T19:16:13.194031: step 13335, loss 0.530226.
Train: 2018-08-05T19:16:13.397138: step 13336, loss 0.611485.
Train: 2018-08-05T19:16:13.615838: step 13337, loss 0.587074.
Train: 2018-08-05T19:16:13.818915: step 13338, loss 0.530026.
Train: 2018-08-05T19:16:14.037613: step 13339, loss 0.52175.
Train: 2018-08-05T19:16:14.240691: step 13340, loss 0.513284.
Test: 2018-08-05T19:16:15.302913: step 13340, loss 0.549044.
Train: 2018-08-05T19:16:15.506020: step 13341, loss 0.545934.
Train: 2018-08-05T19:16:15.724718: step 13342, loss 0.54574.
Train: 2018-08-05T19:16:15.927766: step 13343, loss 0.596034.
Train: 2018-08-05T19:16:16.146466: step 13344, loss 0.528498.
Train: 2018-08-05T19:16:16.349572: step 13345, loss 0.613501.
Train: 2018-08-05T19:16:16.568271: step 13346, loss 0.545216.
Train: 2018-08-05T19:16:16.771348: step 13347, loss 0.553738.
Train: 2018-08-05T19:16:16.990048: step 13348, loss 0.570973.
Train: 2018-08-05T19:16:17.193124: step 13349, loss 0.640306.
Train: 2018-08-05T19:16:17.396202: step 13350, loss 0.588294.
Test: 2018-08-05T19:16:18.458454: step 13350, loss 0.548923.
Train: 2018-08-05T19:16:18.724016: step 13351, loss 0.63998.
Train: 2018-08-05T19:16:18.927094: step 13352, loss 0.613749.
Train: 2018-08-05T19:16:19.145763: step 13353, loss 0.579331.
Train: 2018-08-05T19:16:19.348840: step 13354, loss 0.579192.
Train: 2018-08-05T19:16:19.551918: step 13355, loss 0.554262.
Train: 2018-08-05T19:16:19.770617: step 13356, loss 0.554211.
Train: 2018-08-05T19:16:19.973723: step 13357, loss 0.554274.
Train: 2018-08-05T19:16:20.176801: step 13358, loss 0.570761.
Train: 2018-08-05T19:16:20.395500: step 13359, loss 0.43992.
Train: 2018-08-05T19:16:20.598578: step 13360, loss 0.562549.
Test: 2018-08-05T19:16:21.660800: step 13360, loss 0.548551.
Train: 2018-08-05T19:16:21.863876: step 13361, loss 0.554266.
Train: 2018-08-05T19:16:22.082604: step 13362, loss 0.57904.
Train: 2018-08-05T19:16:22.285682: step 13363, loss 0.529197.
Train: 2018-08-05T19:16:22.488760: step 13364, loss 0.554056.
Train: 2018-08-05T19:16:22.691837: step 13365, loss 0.654824.
Train: 2018-08-05T19:16:22.910536: step 13366, loss 0.545575.
Train: 2018-08-05T19:16:23.113614: step 13367, loss 0.478229.
Train: 2018-08-05T19:16:23.332283: step 13368, loss 0.536964.
Train: 2018-08-05T19:16:23.535390: step 13369, loss 0.621989.
Train: 2018-08-05T19:16:23.738467: step 13370, loss 0.570883.
Test: 2018-08-05T19:16:24.785097: step 13370, loss 0.548625.
Train: 2018-08-05T19:16:25.003795: step 13371, loss 0.630844.
Train: 2018-08-05T19:16:25.206873: step 13372, loss 0.59653.
Train: 2018-08-05T19:16:25.409951: step 13373, loss 0.57086.
Train: 2018-08-05T19:16:25.613028: step 13374, loss 0.613282.
Train: 2018-08-05T19:16:25.816105: step 13375, loss 0.562367.
Train: 2018-08-05T19:16:26.034805: step 13376, loss 0.587577.
Train: 2018-08-05T19:16:26.237882: step 13377, loss 0.512336.
Train: 2018-08-05T19:16:26.440959: step 13378, loss 0.504133.
Train: 2018-08-05T19:16:26.644036: step 13379, loss 0.52906.
Train: 2018-08-05T19:16:26.847084: step 13380, loss 0.537294.
Test: 2018-08-05T19:16:27.909334: step 13380, loss 0.54853.
Train: 2018-08-05T19:16:28.128034: step 13381, loss 0.562381.
Train: 2018-08-05T19:16:28.331112: step 13382, loss 0.613044.
Train: 2018-08-05T19:16:28.549811: step 13383, loss 0.630025.
Train: 2018-08-05T19:16:28.752918: step 13384, loss 0.596122.
Train: 2018-08-05T19:16:28.955995: step 13385, loss 0.596011.
Train: 2018-08-05T19:16:29.159073: step 13386, loss 0.503855.
Train: 2018-08-05T19:16:29.362150: step 13387, loss 0.495569.
Train: 2018-08-05T19:16:29.565227: step 13388, loss 0.562398.
Train: 2018-08-05T19:16:29.768305: step 13389, loss 0.545576.
Train: 2018-08-05T19:16:29.971382: step 13390, loss 0.494888.
Test: 2018-08-05T19:16:31.017983: step 13390, loss 0.547182.
Train: 2018-08-05T19:16:31.221090: step 13391, loss 0.485891.
Train: 2018-08-05T19:16:31.424138: step 13392, loss 0.510827.
Train: 2018-08-05T19:16:31.627244: step 13393, loss 0.58842.
Train: 2018-08-05T19:16:31.830323: step 13394, loss 0.439616.
Train: 2018-08-05T19:16:32.033400: step 13395, loss 0.544693.
Train: 2018-08-05T19:16:32.252098: step 13396, loss 0.580644.
Train: 2018-08-05T19:16:32.439555: step 13397, loss 0.526326.
Train: 2018-08-05T19:16:32.642633: step 13398, loss 0.452515.
Train: 2018-08-05T19:16:32.845705: step 13399, loss 0.553844.
Train: 2018-08-05T19:16:33.048787: step 13400, loss 0.582301.
Test: 2018-08-05T19:16:34.111008: step 13400, loss 0.547751.
Train: 2018-08-05T19:16:35.079532: step 13401, loss 0.563639.
Train: 2018-08-05T19:16:35.282639: step 13402, loss 0.573378.
Train: 2018-08-05T19:16:35.485716: step 13403, loss 0.525483.
Train: 2018-08-05T19:16:35.673143: step 13404, loss 0.631511.
Train: 2018-08-05T19:16:35.876251: step 13405, loss 0.641002.
Train: 2018-08-05T19:16:36.094948: step 13406, loss 0.640326.
Train: 2018-08-05T19:16:36.297997: step 13407, loss 0.544577.
Train: 2018-08-05T19:16:36.485452: step 13408, loss 0.497685.
Train: 2018-08-05T19:16:36.704181: step 13409, loss 0.60033.
Train: 2018-08-05T19:16:36.891608: step 13410, loss 0.581378.
Test: 2018-08-05T19:16:37.953858: step 13410, loss 0.546072.
Train: 2018-08-05T19:16:38.156966: step 13411, loss 0.544532.
Train: 2018-08-05T19:16:38.360044: step 13412, loss 0.49937.
Train: 2018-08-05T19:16:38.563120: step 13413, loss 0.571574.
Train: 2018-08-05T19:16:38.766168: step 13414, loss 0.616116.
Train: 2018-08-05T19:16:38.969276: step 13415, loss 0.535887.
Train: 2018-08-05T19:16:39.172322: step 13416, loss 0.53603.
Train: 2018-08-05T19:16:39.375430: step 13417, loss 0.553631.
Train: 2018-08-05T19:16:39.562857: step 13418, loss 0.579759.
Train: 2018-08-05T19:16:39.765934: step 13419, loss 0.545034.
Train: 2018-08-05T19:16:39.953390: step 13420, loss 0.562337.
Test: 2018-08-05T19:16:41.015671: step 13420, loss 0.548588.
Train: 2018-08-05T19:16:41.203128: step 13421, loss 0.562335.
Train: 2018-08-05T19:16:41.406176: step 13422, loss 0.613672.
Train: 2018-08-05T19:16:41.609282: step 13423, loss 0.502806.
Train: 2018-08-05T19:16:41.796738: step 13424, loss 0.519911.
Train: 2018-08-05T19:16:41.999787: step 13425, loss 0.587822.
Train: 2018-08-05T19:16:42.202893: step 13426, loss 0.519931.
Train: 2018-08-05T19:16:42.405971: step 13427, loss 0.553849.
Train: 2018-08-05T19:16:42.593427: step 13428, loss 0.477209.
Train: 2018-08-05T19:16:42.796475: step 13429, loss 0.562335.
Train: 2018-08-05T19:16:42.999584: step 13430, loss 0.519261.
Test: 2018-08-05T19:16:44.046181: step 13430, loss 0.547058.
Train: 2018-08-05T19:16:44.249258: step 13431, loss 0.562349.
Train: 2018-08-05T19:16:44.436745: step 13432, loss 0.509978.
Train: 2018-08-05T19:16:44.639823: step 13433, loss 0.544801.
Train: 2018-08-05T19:16:44.827279: step 13434, loss 0.526938.
Train: 2018-08-05T19:16:45.014735: step 13435, loss 0.553642.
Train: 2018-08-05T19:16:45.217812: step 13436, loss 0.655144.
Train: 2018-08-05T19:16:45.420884: step 13437, loss 0.490445.
Train: 2018-08-05T19:16:45.608346: step 13438, loss 0.635082.
Train: 2018-08-05T19:16:45.780183: step 13439, loss 0.562657.
Train: 2018-08-05T19:16:45.983253: step 13440, loss 0.517493.
Test: 2018-08-05T19:16:47.029888: step 13440, loss 0.54735.
Train: 2018-08-05T19:16:47.232935: step 13441, loss 0.544579.
Train: 2018-08-05T19:16:47.420421: step 13442, loss 0.634919.
Train: 2018-08-05T19:16:47.623470: step 13443, loss 0.571597.
Train: 2018-08-05T19:16:47.810955: step 13444, loss 0.490892.
Train: 2018-08-05T19:16:48.014033: step 13445, loss 0.616219.
Train: 2018-08-05T19:16:48.201488: step 13446, loss 0.500136.
Train: 2018-08-05T19:16:48.404566: step 13447, loss 0.580282.
Train: 2018-08-05T19:16:48.592022: step 13448, loss 0.526963.
Train: 2018-08-05T19:16:48.779478: step 13449, loss 0.544723.
Train: 2018-08-05T19:16:48.982556: step 13450, loss 0.526997.
Test: 2018-08-05T19:16:50.044807: step 13450, loss 0.546418.
Train: 2018-08-05T19:16:50.232233: step 13451, loss 0.580213.
Train: 2018-08-05T19:16:50.435341: step 13452, loss 0.615685.
Train: 2018-08-05T19:16:50.622766: step 13453, loss 0.61547.
Train: 2018-08-05T19:16:50.810253: step 13454, loss 0.562396.
Train: 2018-08-05T19:16:51.013332: step 13455, loss 0.466337.
Train: 2018-08-05T19:16:51.200789: step 13456, loss 0.614707.
Train: 2018-08-05T19:16:51.388242: step 13457, loss 0.536277.
Train: 2018-08-05T19:16:51.591290: step 13458, loss 0.545001.
Train: 2018-08-05T19:16:51.778746: step 13459, loss 0.562345.
Train: 2018-08-05T19:16:51.981858: step 13460, loss 0.536385.
Test: 2018-08-05T19:16:53.028483: step 13460, loss 0.548688.
Train: 2018-08-05T19:16:53.231561: step 13461, loss 0.614261.
Train: 2018-08-05T19:16:53.419016: step 13462, loss 0.562338.
Train: 2018-08-05T19:16:53.606473: step 13463, loss 0.639789.
Train: 2018-08-05T19:16:53.809545: step 13464, loss 0.562337.
Train: 2018-08-05T19:16:53.997006: step 13465, loss 0.519851.
Train: 2018-08-05T19:16:54.184462: step 13466, loss 0.55388.
Train: 2018-08-05T19:16:54.387540: step 13467, loss 0.604635.
Train: 2018-08-05T19:16:54.590617: step 13468, loss 0.553956.
Train: 2018-08-05T19:16:54.778074: step 13469, loss 0.587573.
Train: 2018-08-05T19:16:54.965500: step 13470, loss 0.462076.
Test: 2018-08-05T19:16:56.027780: step 13470, loss 0.547634.
Train: 2018-08-05T19:16:56.230858: step 13471, loss 0.587536.
Train: 2018-08-05T19:16:56.418314: step 13472, loss 0.520466.
Train: 2018-08-05T19:16:56.605771: step 13473, loss 0.545554.
Train: 2018-08-05T19:16:56.793197: step 13474, loss 0.61304.
Train: 2018-08-05T19:16:56.996273: step 13475, loss 0.562361.
Train: 2018-08-05T19:16:57.183760: step 13476, loss 0.587745.
Train: 2018-08-05T19:16:57.371215: step 13477, loss 0.613116.
Train: 2018-08-05T19:16:57.558672: step 13478, loss 0.49489.
Train: 2018-08-05T19:16:57.746131: step 13479, loss 0.570809.
Train: 2018-08-05T19:16:57.933585: step 13480, loss 0.613065.
Test: 2018-08-05T19:16:58.995836: step 13480, loss 0.549837.
Train: 2018-08-05T19:16:59.183292: step 13481, loss 0.545501.
Train: 2018-08-05T19:16:59.370718: step 13482, loss 0.5708.
Train: 2018-08-05T19:16:59.558205: step 13483, loss 0.528695.
Train: 2018-08-05T19:16:59.761251: step 13484, loss 0.62138.
Train: 2018-08-05T19:16:59.948737: step 13485, loss 0.495074.
Train: 2018-08-05T19:17:00.136194: step 13486, loss 0.562371.
Train: 2018-08-05T19:17:00.323650: step 13487, loss 0.528577.
Train: 2018-08-05T19:17:00.526728: step 13488, loss 0.562352.
Train: 2018-08-05T19:17:00.714183: step 13489, loss 0.562344.
Train: 2018-08-05T19:17:00.901609: step 13490, loss 0.596474.
Test: 2018-08-05T19:17:01.963873: step 13490, loss 0.547678.
Train: 2018-08-05T19:17:02.151347: step 13491, loss 0.528167.
Train: 2018-08-05T19:17:02.338803: step 13492, loss 0.536638.
Train: 2018-08-05T19:17:02.526263: step 13493, loss 0.596728.
Train: 2018-08-05T19:17:02.713718: step 13494, loss 0.605391.
Train: 2018-08-05T19:17:02.901142: step 13495, loss 0.562335.
Train: 2018-08-05T19:17:03.088622: step 13496, loss 0.519368.
Train: 2018-08-05T19:17:03.291675: step 13497, loss 0.502103.
Train: 2018-08-05T19:17:03.463539: step 13498, loss 0.527775.
Train: 2018-08-05T19:17:03.650996: step 13499, loss 0.536284.
Train: 2018-08-05T19:17:03.838451: step 13500, loss 0.48365.
Test: 2018-08-05T19:17:04.900704: step 13500, loss 0.547808.
Train: 2018-08-05T19:17:05.822363: step 13501, loss 0.616128.
Train: 2018-08-05T19:17:06.009818: step 13502, loss 0.55359.
Train: 2018-08-05T19:17:06.197245: step 13503, loss 0.589218.
Train: 2018-08-05T19:17:06.384734: step 13504, loss 0.598223.
Train: 2018-08-05T19:17:06.572157: step 13505, loss 0.642821.
Train: 2018-08-05T19:17:06.759642: step 13506, loss 0.535832.
Train: 2018-08-05T19:17:06.962690: step 13507, loss 0.527054.
Train: 2018-08-05T19:17:07.150176: step 13508, loss 0.650734.
Train: 2018-08-05T19:17:07.353254: step 13509, loss 0.650113.
Train: 2018-08-05T19:17:07.540710: step 13510, loss 0.510282.
Test: 2018-08-05T19:17:08.587310: step 13510, loss 0.547354.
Train: 2018-08-05T19:17:08.774796: step 13511, loss 0.553721.
Train: 2018-08-05T19:17:08.962252: step 13512, loss 0.545212.
Train: 2018-08-05T19:17:09.149678: step 13513, loss 0.68164.
Train: 2018-08-05T19:17:09.352786: step 13514, loss 0.596097.
Train: 2018-08-05T19:17:09.540241: step 13515, loss 0.570765.
Train: 2018-08-05T19:17:09.727698: step 13516, loss 0.579008.
Train: 2018-08-05T19:17:09.915153: step 13517, loss 0.562597.
Train: 2018-08-05T19:17:10.102579: step 13518, loss 0.587001.
Train: 2018-08-05T19:17:10.290036: step 13519, loss 0.619052.
Train: 2018-08-05T19:17:10.477522: step 13520, loss 0.562946.
Test: 2018-08-05T19:17:11.539742: step 13520, loss 0.549174.
Train: 2018-08-05T19:17:11.711608: step 13521, loss 0.531514.
Train: 2018-08-05T19:17:11.914685: step 13522, loss 0.594581.
Train: 2018-08-05T19:17:12.102112: step 13523, loss 0.563254.
Train: 2018-08-05T19:17:12.273976: step 13524, loss 0.602262.
Train: 2018-08-05T19:17:12.461403: step 13525, loss 0.540146.
Train: 2018-08-05T19:17:12.648890: step 13526, loss 0.540209.
Train: 2018-08-05T19:17:12.836345: step 13527, loss 0.563413.
Train: 2018-08-05T19:17:13.023771: step 13528, loss 0.563369.
Train: 2018-08-05T19:17:13.211257: step 13529, loss 0.524341.
Train: 2018-08-05T19:17:13.398683: step 13530, loss 0.571035.
Test: 2018-08-05T19:17:14.445343: step 13530, loss 0.550458.
Train: 2018-08-05T19:17:14.632768: step 13531, loss 0.555199.
Train: 2018-08-05T19:17:14.820255: step 13532, loss 0.618568.
Train: 2018-08-05T19:17:15.007681: step 13533, loss 0.546972.
Train: 2018-08-05T19:17:15.195167: step 13534, loss 0.594891.
Train: 2018-08-05T19:17:15.382594: step 13535, loss 0.522587.
Train: 2018-08-05T19:17:15.570077: step 13536, loss 0.530351.
Train: 2018-08-05T19:17:15.741885: step 13537, loss 0.562619.
Train: 2018-08-05T19:17:15.929371: step 13538, loss 0.521453.
Train: 2018-08-05T19:17:16.116797: step 13539, loss 0.51268.
Train: 2018-08-05T19:17:16.288661: step 13540, loss 0.663118.
Test: 2018-08-05T19:17:17.350912: step 13540, loss 0.548082.
Train: 2018-08-05T19:17:17.538339: step 13541, loss 0.55393.
Train: 2018-08-05T19:17:17.725795: step 13542, loss 0.562351.
Train: 2018-08-05T19:17:17.897659: step 13543, loss 0.570864.
Train: 2018-08-05T19:17:18.085115: step 13544, loss 0.562336.
Train: 2018-08-05T19:17:18.272572: step 13545, loss 0.502231.
Train: 2018-08-05T19:17:18.444406: step 13546, loss 0.50186.
Train: 2018-08-05T19:17:18.647454: step 13547, loss 0.536217.
Train: 2018-08-05T19:17:18.834943: step 13548, loss 0.536025.
Train: 2018-08-05T19:17:19.006775: step 13549, loss 0.597947.
Train: 2018-08-05T19:17:19.194230: step 13550, loss 0.482209.
Test: 2018-08-05T19:17:20.240861: step 13550, loss 0.548398.
Train: 2018-08-05T19:17:20.428317: step 13551, loss 0.580601.
Train: 2018-08-05T19:17:20.615744: step 13552, loss 0.607962.
Train: 2018-08-05T19:17:20.803229: step 13553, loss 0.526388.
Train: 2018-08-05T19:17:20.990685: step 13554, loss 0.462523.
Train: 2018-08-05T19:17:21.178141: step 13555, loss 0.572054.
Train: 2018-08-05T19:17:21.365567: step 13556, loss 0.627557.
Train: 2018-08-05T19:17:21.553057: step 13557, loss 0.433715.
Train: 2018-08-05T19:17:21.724859: step 13558, loss 0.488786.
Train: 2018-08-05T19:17:21.912344: step 13559, loss 0.488339.
Train: 2018-08-05T19:17:22.099801: step 13560, loss 0.592555.
Test: 2018-08-05T19:17:23.146430: step 13560, loss 0.546869.
Train: 2018-08-05T19:17:23.333857: step 13561, loss 0.497068.
Train: 2018-08-05T19:17:23.521342: step 13562, loss 0.554238.
Train: 2018-08-05T19:17:23.708820: step 13563, loss 0.496586.
Train: 2018-08-05T19:17:23.896255: step 13564, loss 0.651378.
Train: 2018-08-05T19:17:24.083681: step 13565, loss 0.661103.
Train: 2018-08-05T19:17:24.271166: step 13566, loss 0.535092.
Train: 2018-08-05T19:17:24.458623: step 13567, loss 0.506336.
Train: 2018-08-05T19:17:24.646079: step 13568, loss 0.621047.
Train: 2018-08-05T19:17:24.817883: step 13569, loss 0.563552.
Train: 2018-08-05T19:17:25.005370: step 13570, loss 0.506915.
Test: 2018-08-05T19:17:26.052000: step 13570, loss 0.550197.
Train: 2018-08-05T19:17:26.239456: step 13571, loss 0.553882.
Train: 2018-08-05T19:17:26.426912: step 13572, loss 0.553816.
Train: 2018-08-05T19:17:26.598717: step 13573, loss 0.544509.
Train: 2018-08-05T19:17:26.786203: step 13574, loss 0.618133.
Train: 2018-08-05T19:17:26.973659: step 13575, loss 0.608417.
Train: 2018-08-05T19:17:27.161086: step 13576, loss 0.571671.
Train: 2018-08-05T19:17:27.348576: step 13577, loss 0.526784.
Train: 2018-08-05T19:17:27.520406: step 13578, loss 0.606754.
Train: 2018-08-05T19:17:27.707833: step 13579, loss 0.615006.
Train: 2018-08-05T19:17:27.895289: step 13580, loss 0.527696.
Test: 2018-08-05T19:17:28.941919: step 13580, loss 0.54682.
Train: 2018-08-05T19:17:29.129374: step 13581, loss 0.588075.
Train: 2018-08-05T19:17:29.316860: step 13582, loss 0.545358.
Train: 2018-08-05T19:17:29.488696: step 13583, loss 0.604504.
Train: 2018-08-05T19:17:29.676121: step 13584, loss 0.545726.
Train: 2018-08-05T19:17:29.863578: step 13585, loss 0.496181.
Train: 2018-08-05T19:17:30.035442: step 13586, loss 0.579021.
Train: 2018-08-05T19:17:30.222899: step 13587, loss 0.587238.
Train: 2018-08-05T19:17:30.410325: step 13588, loss 0.57076.
Train: 2018-08-05T19:17:30.597811: step 13589, loss 0.603505.
Train: 2018-08-05T19:17:30.738404: step 13590, loss 0.58001.
Test: 2018-08-05T19:17:31.800624: step 13590, loss 0.547831.
Train: 2018-08-05T19:17:31.988110: step 13591, loss 0.546462.
Train: 2018-08-05T19:17:32.175566: step 13592, loss 0.538448.
Train: 2018-08-05T19:17:32.347371: step 13593, loss 0.675917.
Train: 2018-08-05T19:17:32.534857: step 13594, loss 0.538686.
Train: 2018-08-05T19:17:32.706691: step 13595, loss 0.578865.
Train: 2018-08-05T19:17:32.894148: step 13596, loss 0.562891.
Train: 2018-08-05T19:17:33.081604: step 13597, loss 0.594796.
Train: 2018-08-05T19:17:33.253439: step 13598, loss 0.570913.
Train: 2018-08-05T19:17:33.440865: step 13599, loss 0.49165.
Train: 2018-08-05T19:17:33.628350: step 13600, loss 0.507296.
Test: 2018-08-05T19:17:34.674951: step 13600, loss 0.550407.
Train: 2018-08-05T19:17:35.596618: step 13601, loss 0.546845.
Train: 2018-08-05T19:17:35.784096: step 13602, loss 0.63544.
Train: 2018-08-05T19:17:35.971557: step 13603, loss 0.578893.
Train: 2018-08-05T19:17:36.159008: step 13604, loss 0.578905.
Train: 2018-08-05T19:17:36.346464: step 13605, loss 0.546383.
Train: 2018-08-05T19:17:36.518299: step 13606, loss 0.562611.
Train: 2018-08-05T19:17:36.705755: step 13607, loss 0.587146.
Train: 2018-08-05T19:17:36.893211: step 13608, loss 0.546128.
Train: 2018-08-05T19:17:37.080637: step 13609, loss 0.578997.
Train: 2018-08-05T19:17:37.252471: step 13610, loss 0.579018.
Test: 2018-08-05T19:17:38.314753: step 13610, loss 0.549035.
Train: 2018-08-05T19:17:38.502179: step 13611, loss 0.595591.
Train: 2018-08-05T19:17:38.674014: step 13612, loss 0.595598.
Train: 2018-08-05T19:17:38.861501: step 13613, loss 0.545944.
Train: 2018-08-05T19:17:39.048956: step 13614, loss 0.545939.
Train: 2018-08-05T19:17:39.220791: step 13615, loss 0.579042.
Train: 2018-08-05T19:17:39.423869: step 13616, loss 0.570758.
Train: 2018-08-05T19:17:39.595704: step 13617, loss 0.587351.
Train: 2018-08-05T19:17:39.783159: step 13618, loss 0.545881.
Train: 2018-08-05T19:17:40.017481: step 13619, loss 0.545861.
Train: 2018-08-05T19:17:40.204936: step 13620, loss 0.554129.
Test: 2018-08-05T19:17:41.267157: step 13620, loss 0.54812.
Train: 2018-08-05T19:17:41.454614: step 13621, loss 0.529079.
Train: 2018-08-05T19:17:41.642099: step 13622, loss 0.528906.
Train: 2018-08-05T19:17:41.829557: step 13623, loss 0.503403.
Train: 2018-08-05T19:17:42.001389: step 13624, loss 0.579344.
Train: 2018-08-05T19:17:42.188846: step 13625, loss 0.639359.
Train: 2018-08-05T19:17:42.376302: step 13626, loss 0.605214.
Train: 2018-08-05T19:17:42.548137: step 13627, loss 0.536618.
Train: 2018-08-05T19:17:42.735563: step 13628, loss 0.545173.
Train: 2018-08-05T19:17:42.923019: step 13629, loss 0.579526.
Train: 2018-08-05T19:17:43.110505: step 13630, loss 0.450528.
Test: 2018-08-05T19:17:44.157105: step 13630, loss 0.548685.
Train: 2018-08-05T19:17:44.344591: step 13631, loss 0.631582.
Train: 2018-08-05T19:17:44.532047: step 13632, loss 0.579689.
Train: 2018-08-05T19:17:44.719504: step 13633, loss 0.579701.
Train: 2018-08-05T19:17:44.891339: step 13634, loss 0.52766.
Train: 2018-08-05T19:17:45.078794: step 13635, loss 0.501578.
Train: 2018-08-05T19:17:45.266221: step 13636, loss 0.527497.
Train: 2018-08-05T19:17:45.453676: step 13637, loss 0.693805.
Train: 2018-08-05T19:17:45.641163: step 13638, loss 0.492453.
Train: 2018-08-05T19:17:45.828618: step 13639, loss 0.588627.
Train: 2018-08-05T19:17:46.016075: step 13640, loss 0.597352.
Test: 2018-08-05T19:17:47.078326: step 13640, loss 0.546394.
Train: 2018-08-05T19:17:47.250131: step 13641, loss 0.571085.
Train: 2018-08-05T19:17:47.437617: step 13642, loss 0.571048.
Train: 2018-08-05T19:17:47.625042: step 13643, loss 0.484368.
Train: 2018-08-05T19:17:47.828151: step 13644, loss 0.49298.
Train: 2018-08-05T19:17:48.015607: step 13645, loss 0.605892.
Train: 2018-08-05T19:17:48.203062: step 13646, loss 0.492624.
Train: 2018-08-05T19:17:48.390519: step 13647, loss 0.579894.
Train: 2018-08-05T19:17:48.577944: step 13648, loss 0.544833.
Train: 2018-08-05T19:17:48.765431: step 13649, loss 0.588837.
Train: 2018-08-05T19:17:48.952858: step 13650, loss 0.59769.
Test: 2018-08-05T19:17:49.999487: step 13650, loss 0.54892.
Train: 2018-08-05T19:17:50.202595: step 13651, loss 0.544798.
Train: 2018-08-05T19:17:50.374398: step 13652, loss 0.57121.
Train: 2018-08-05T19:17:50.577477: step 13653, loss 0.588771.
Train: 2018-08-05T19:17:50.764933: step 13654, loss 0.562386.
Train: 2018-08-05T19:17:50.952419: step 13655, loss 0.579851.
Train: 2018-08-05T19:17:51.139875: step 13656, loss 0.544946.
Train: 2018-08-05T19:17:51.327302: step 13657, loss 0.553668.
Train: 2018-08-05T19:17:51.530409: step 13658, loss 0.449731.
Train: 2018-08-05T19:17:51.717865: step 13659, loss 0.562354.
Train: 2018-08-05T19:17:51.905321: step 13660, loss 0.527478.
Test: 2018-08-05T19:17:52.967566: step 13660, loss 0.547753.
Train: 2018-08-05T19:17:53.139406: step 13661, loss 0.501065.
Train: 2018-08-05T19:17:53.326863: step 13662, loss 0.615335.
Train: 2018-08-05T19:17:53.514319: step 13663, loss 0.527059.
Train: 2018-08-05T19:17:53.701775: step 13664, loss 0.544708.
Train: 2018-08-05T19:17:53.889201: step 13665, loss 0.526835.
Train: 2018-08-05T19:17:54.076657: step 13666, loss 0.571516.
Train: 2018-08-05T19:17:54.264114: step 13667, loss 0.62554.
Train: 2018-08-05T19:17:54.451600: step 13668, loss 0.54461.
Train: 2018-08-05T19:17:54.639056: step 13669, loss 0.544612.
Train: 2018-08-05T19:17:54.826512: step 13670, loss 0.562579.
Test: 2018-08-05T19:17:55.888763: step 13670, loss 0.548819.
Train: 2018-08-05T19:17:56.076219: step 13671, loss 0.535639.
Train: 2018-08-05T19:17:56.263645: step 13672, loss 0.571554.
Train: 2018-08-05T19:17:56.451130: step 13673, loss 0.499764.
Train: 2018-08-05T19:17:56.638587: step 13674, loss 0.607523.
Train: 2018-08-05T19:17:56.841665: step 13675, loss 0.508713.
Train: 2018-08-05T19:17:57.029121: step 13676, loss 0.544611.
Train: 2018-08-05T19:17:57.216548: step 13677, loss 0.661546.
Train: 2018-08-05T19:17:57.404033: step 13678, loss 0.598365.
Train: 2018-08-05T19:17:57.591495: step 13679, loss 0.580279.
Train: 2018-08-05T19:17:57.794566: step 13680, loss 0.571263.
Test: 2018-08-05T19:17:58.841199: step 13680, loss 0.548754.
Train: 2018-08-05T19:17:59.075521: step 13681, loss 0.579922.
Train: 2018-08-05T19:17:59.309835: step 13682, loss 0.58845.
Train: 2018-08-05T19:17:59.528537: step 13683, loss 0.53647.
Train: 2018-08-05T19:17:59.762827: step 13684, loss 0.588032.
Train: 2018-08-05T19:17:59.997179: step 13685, loss 0.54534.
Train: 2018-08-05T19:18:00.215846: step 13686, loss 0.536999.
Train: 2018-08-05T19:18:00.418946: step 13687, loss 0.655024.
Train: 2018-08-05T19:18:00.606408: step 13688, loss 0.512289.
Train: 2018-08-05T19:18:00.809486: step 13689, loss 0.645624.
Train: 2018-08-05T19:18:00.981291: step 13690, loss 0.546004.
Test: 2018-08-05T19:18:02.043542: step 13690, loss 0.548487.
Train: 2018-08-05T19:18:02.230998: step 13691, loss 0.529757.
Train: 2018-08-05T19:18:02.434075: step 13692, loss 0.464493.
Train: 2018-08-05T19:18:02.621531: step 13693, loss 0.554362.
Train: 2018-08-05T19:18:02.809020: step 13694, loss 0.587218.
Train: 2018-08-05T19:18:02.996473: step 13695, loss 0.595506.
Train: 2018-08-05T19:18:03.183929: step 13696, loss 0.628551.
Train: 2018-08-05T19:18:03.371385: step 13697, loss 0.52958.
Train: 2018-08-05T19:18:03.574462: step 13698, loss 0.48017.
Train: 2018-08-05T19:18:03.761919: step 13699, loss 0.661787.
Train: 2018-08-05T19:18:03.949375: step 13700, loss 0.612108.
Test: 2018-08-05T19:18:05.011626: step 13700, loss 0.547789.
Train: 2018-08-05T19:18:06.011392: step 13701, loss 0.529525.
Train: 2018-08-05T19:18:06.214470: step 13702, loss 0.570757.
Train: 2018-08-05T19:18:06.401926: step 13703, loss 0.603716.
Train: 2018-08-05T19:18:06.589382: step 13704, loss 0.546093.
Train: 2018-08-05T19:18:06.776838: step 13705, loss 0.546108.
Train: 2018-08-05T19:18:06.979887: step 13706, loss 0.611876.
Train: 2018-08-05T19:18:07.182993: step 13707, loss 0.57076.
Train: 2018-08-05T19:18:07.370449: step 13708, loss 0.505165.
Train: 2018-08-05T19:18:07.573526: step 13709, loss 0.513224.
Train: 2018-08-05T19:18:07.776574: step 13710, loss 0.562493.
Test: 2018-08-05T19:18:08.823204: step 13710, loss 0.54801.
Train: 2018-08-05T19:18:09.026311: step 13711, loss 0.612286.
Train: 2018-08-05T19:18:09.213768: step 13712, loss 0.587411.
Train: 2018-08-05T19:18:09.401224: step 13713, loss 0.579096.
Train: 2018-08-05T19:18:09.604295: step 13714, loss 0.612437.
Train: 2018-08-05T19:18:09.791757: step 13715, loss 0.545812.
Train: 2018-08-05T19:18:09.994835: step 13716, loss 0.529209.
Train: 2018-08-05T19:18:10.182291: step 13717, loss 0.579085.
Train: 2018-08-05T19:18:10.385368: step 13718, loss 0.595748.
Train: 2018-08-05T19:18:10.572824: step 13719, loss 0.529156.
Train: 2018-08-05T19:18:10.775901: step 13720, loss 0.587429.
Test: 2018-08-05T19:18:11.822531: step 13720, loss 0.549281.
Train: 2018-08-05T19:18:12.025609: step 13721, loss 0.53743.
Train: 2018-08-05T19:18:12.213065: step 13722, loss 0.529027.
Train: 2018-08-05T19:18:12.416143: step 13723, loss 0.55402.
Train: 2018-08-05T19:18:12.603599: step 13724, loss 0.478257.
Train: 2018-08-05T19:18:12.806647: step 13725, loss 0.545387.
Train: 2018-08-05T19:18:12.994102: step 13726, loss 0.562336.
Train: 2018-08-05T19:18:13.212836: step 13727, loss 0.527867.
Train: 2018-08-05T19:18:13.400258: step 13728, loss 0.544972.
Train: 2018-08-05T19:18:13.603335: step 13729, loss 0.562384.
Train: 2018-08-05T19:18:13.790820: step 13730, loss 0.527125.
Test: 2018-08-05T19:18:14.837421: step 13730, loss 0.545991.
Train: 2018-08-05T19:18:15.040527: step 13731, loss 0.615844.
Train: 2018-08-05T19:18:15.243606: step 13732, loss 0.589298.
Train: 2018-08-05T19:18:15.431062: step 13733, loss 0.482063.
Train: 2018-08-05T19:18:15.634139: step 13734, loss 0.517658.
Train: 2018-08-05T19:18:15.821564: step 13735, loss 0.544573.
Train: 2018-08-05T19:18:16.024672: step 13736, loss 0.535456.
Train: 2018-08-05T19:18:16.227750: step 13737, loss 0.590233.
Train: 2018-08-05T19:18:16.430827: step 13738, loss 0.599527.
Train: 2018-08-05T19:18:16.618284: step 13739, loss 0.599524.
Train: 2018-08-05T19:18:16.821331: step 13740, loss 0.581096.
Test: 2018-08-05T19:18:17.867961: step 13740, loss 0.54901.
Train: 2018-08-05T19:18:18.039825: step 13741, loss 0.582171.
Train: 2018-08-05T19:18:18.242903: step 13742, loss 0.526455.
Train: 2018-08-05T19:18:18.430359: step 13743, loss 0.553605.
Train: 2018-08-05T19:18:18.633437: step 13744, loss 0.571568.
Train: 2018-08-05T19:18:18.836515: step 13745, loss 0.562536.
Train: 2018-08-05T19:18:19.023971: step 13746, loss 0.5714.
Train: 2018-08-05T19:18:19.227018: step 13747, loss 0.535869.
Train: 2018-08-05T19:18:19.430125: step 13748, loss 0.597744.
Train: 2018-08-05T19:18:19.617581: step 13749, loss 0.562395.
Train: 2018-08-05T19:18:19.836281: step 13750, loss 0.571104.
Test: 2018-08-05T19:18:20.882910: step 13750, loss 0.547045.
Train: 2018-08-05T19:18:21.132848: step 13751, loss 0.544979.
Train: 2018-08-05T19:18:21.320307: step 13752, loss 0.545043.
Train: 2018-08-05T19:18:21.523381: step 13753, loss 0.57096.
Train: 2018-08-05T19:18:21.726462: step 13754, loss 0.553741.
Train: 2018-08-05T19:18:21.913920: step 13755, loss 0.52805.
Train: 2018-08-05T19:18:22.116996: step 13756, loss 0.622298.
Train: 2018-08-05T19:18:22.320073: step 13757, loss 0.545271.
Train: 2018-08-05T19:18:22.523152: step 13758, loss 0.579368.
Train: 2018-08-05T19:18:22.726230: step 13759, loss 0.613268.
Train: 2018-08-05T19:18:22.913684: step 13760, loss 0.503284.
Test: 2018-08-05T19:18:23.975905: step 13760, loss 0.547717.
Train: 2018-08-05T19:18:24.179013: step 13761, loss 0.537091.
Train: 2018-08-05T19:18:24.382090: step 13762, loss 0.579228.
Train: 2018-08-05T19:18:24.585168: step 13763, loss 0.570797.
Train: 2018-08-05T19:18:24.788246: step 13764, loss 0.587623.
Train: 2018-08-05T19:18:24.991323: step 13765, loss 0.528795.
Train: 2018-08-05T19:18:25.194370: step 13766, loss 0.520386.
Train: 2018-08-05T19:18:25.381826: step 13767, loss 0.663446.
Train: 2018-08-05T19:18:25.600556: step 13768, loss 0.646372.
Train: 2018-08-05T19:18:25.803633: step 13769, loss 0.545745.
Train: 2018-08-05T19:18:26.006710: step 13770, loss 0.56246.
Test: 2018-08-05T19:18:27.053340: step 13770, loss 0.548884.
Train: 2018-08-05T19:18:27.256418: step 13771, loss 0.529441.
Train: 2018-08-05T19:18:27.459495: step 13772, loss 0.504759.
Train: 2018-08-05T19:18:27.662543: step 13773, loss 0.562489.
Train: 2018-08-05T19:18:27.865649: step 13774, loss 0.587331.
Train: 2018-08-05T19:18:28.068727: step 13775, loss 0.496099.
Train: 2018-08-05T19:18:28.271805: step 13776, loss 0.587441.
Train: 2018-08-05T19:18:28.474852: step 13777, loss 0.612586.
Train: 2018-08-05T19:18:28.677959: step 13778, loss 0.629099.
Train: 2018-08-05T19:18:28.881037: step 13779, loss 0.586891.
Train: 2018-08-05T19:18:29.084115: step 13780, loss 0.528792.
Test: 2018-08-05T19:18:30.130744: step 13780, loss 0.548478.
Train: 2018-08-05T19:18:30.333792: step 13781, loss 0.555355.
Train: 2018-08-05T19:18:30.552520: step 13782, loss 0.620105.
Train: 2018-08-05T19:18:30.755598: step 13783, loss 0.612385.
Train: 2018-08-05T19:18:30.958676: step 13784, loss 0.504231.
Train: 2018-08-05T19:18:31.177374: step 13785, loss 0.520825.
Train: 2018-08-05T19:18:31.380451: step 13786, loss 0.512317.
Train: 2018-08-05T19:18:31.583529: step 13787, loss 0.570787.
Train: 2018-08-05T19:18:31.786606: step 13788, loss 0.570808.
Train: 2018-08-05T19:18:31.989654: step 13789, loss 0.579309.
Train: 2018-08-05T19:18:32.192730: step 13790, loss 0.536832.
Test: 2018-08-05T19:18:33.255012: step 13790, loss 0.547094.
Train: 2018-08-05T19:18:33.458059: step 13791, loss 0.622113.
Train: 2018-08-05T19:18:33.661169: step 13792, loss 0.596512.
Train: 2018-08-05T19:18:33.864245: step 13793, loss 0.587935.
Train: 2018-08-05T19:18:34.067323: step 13794, loss 0.519793.
Train: 2018-08-05T19:18:34.270400: step 13795, loss 0.638933.
Train: 2018-08-05T19:18:34.473477: step 13796, loss 0.503024.
Train: 2018-08-05T19:18:34.676555: step 13797, loss 0.638605.
Train: 2018-08-05T19:18:34.895254: step 13798, loss 0.537061.
Train: 2018-08-05T19:18:35.098331: step 13799, loss 0.528716.
Train: 2018-08-05T19:18:35.301409: step 13800, loss 0.511894.
Test: 2018-08-05T19:18:36.348009: step 13800, loss 0.548082.
Train: 2018-08-05T19:18:37.300911: step 13801, loss 0.537056.
Train: 2018-08-05T19:18:37.504018: step 13802, loss 0.553883.
Train: 2018-08-05T19:18:37.707095: step 13803, loss 0.519814.
Train: 2018-08-05T19:18:37.925765: step 13804, loss 0.528107.
Train: 2018-08-05T19:18:38.128871: step 13805, loss 0.648529.
Train: 2018-08-05T19:18:38.347541: step 13806, loss 0.588243.
Train: 2018-08-05T19:18:38.550647: step 13807, loss 0.536432.
Train: 2018-08-05T19:18:38.753725: step 13808, loss 0.570989.
Train: 2018-08-05T19:18:38.956803: step 13809, loss 0.536384.
Train: 2018-08-05T19:18:39.159851: step 13810, loss 0.579682.
Test: 2018-08-05T19:18:40.222102: step 13810, loss 0.547864.
Train: 2018-08-05T19:18:40.425208: step 13811, loss 0.553675.
Train: 2018-08-05T19:18:40.628287: step 13812, loss 0.484241.
Train: 2018-08-05T19:18:40.831364: step 13813, loss 0.579799.
Train: 2018-08-05T19:18:41.050062: step 13814, loss 0.562374.
Train: 2018-08-05T19:18:41.268732: step 13815, loss 0.658793.
Train: 2018-08-05T19:18:41.471839: step 13816, loss 0.527426.
Train: 2018-08-05T19:18:41.674916: step 13817, loss 0.536192.
Train: 2018-08-05T19:18:41.893615: step 13818, loss 0.562365.
Train: 2018-08-05T19:18:42.112284: step 13819, loss 0.536202.
Train: 2018-08-05T19:18:42.315391: step 13820, loss 0.544911.
Test: 2018-08-05T19:18:43.362021: step 13820, loss 0.546972.
Train: 2018-08-05T19:18:43.580720: step 13821, loss 0.579852.
Train: 2018-08-05T19:18:43.783797: step 13822, loss 0.466229.
Train: 2018-08-05T19:18:43.986845: step 13823, loss 0.57996.
Train: 2018-08-05T19:18:44.205574: step 13824, loss 0.562413.
Train: 2018-08-05T19:18:44.424242: step 13825, loss 0.668384.
Train: 2018-08-05T19:18:44.627349: step 13826, loss 0.562405.
Train: 2018-08-05T19:18:44.830427: step 13827, loss 0.518565.
Train: 2018-08-05T19:18:45.049125: step 13828, loss 0.518619.
Train: 2018-08-05T19:18:45.252203: step 13829, loss 0.544865.
Train: 2018-08-05T19:18:45.470902: step 13830, loss 0.562388.
Test: 2018-08-05T19:18:46.517534: step 13830, loss 0.547129.
Train: 2018-08-05T19:18:46.767445: step 13831, loss 0.536063.
Train: 2018-08-05T19:18:47.017386: step 13832, loss 0.588778.
Train: 2018-08-05T19:18:47.282974: step 13833, loss 0.588774.
Train: 2018-08-05T19:18:47.486055: step 13834, loss 0.527292.
Train: 2018-08-05T19:18:47.704754: step 13835, loss 0.579938.
Train: 2018-08-05T19:18:47.907801: step 13836, loss 0.562384.
Train: 2018-08-05T19:18:48.110909: step 13837, loss 0.527384.
Train: 2018-08-05T19:18:48.329608: step 13838, loss 0.641124.
Train: 2018-08-05T19:18:48.532685: step 13839, loss 0.518799.
Train: 2018-08-05T19:18:48.735762: step 13840, loss 0.553658.
Test: 2018-08-05T19:18:49.797983: step 13840, loss 0.548047.
Train: 2018-08-05T19:18:50.001062: step 13841, loss 0.588405.
Train: 2018-08-05T19:18:50.219790: step 13842, loss 0.519042.
Train: 2018-08-05T19:18:50.422867: step 13843, loss 0.536373.
Train: 2018-08-05T19:18:50.625944: step 13844, loss 0.519026.
Train: 2018-08-05T19:18:50.829022: step 13845, loss 0.501532.
Train: 2018-08-05T19:18:51.047721: step 13846, loss 0.571108.
Train: 2018-08-05T19:18:51.250798: step 13847, loss 0.536069.
Train: 2018-08-05T19:18:51.469494: step 13848, loss 0.553602.
Train: 2018-08-05T19:18:51.672545: step 13849, loss 0.527025.
Train: 2018-08-05T19:18:51.875654: step 13850, loss 0.669334.
Test: 2018-08-05T19:18:52.937874: step 13850, loss 0.547834.
Train: 2018-08-05T19:18:53.140981: step 13851, loss 0.562482.
Train: 2018-08-05T19:18:53.344058: step 13852, loss 0.482552.
Train: 2018-08-05T19:18:53.562757: step 13853, loss 0.562488.
Train: 2018-08-05T19:18:53.765805: step 13854, loss 0.544675.
Train: 2018-08-05T19:18:53.984533: step 13855, loss 0.580376.
Train: 2018-08-05T19:18:54.187610: step 13856, loss 0.58038.
Train: 2018-08-05T19:18:54.406281: step 13857, loss 0.589263.
Train: 2018-08-05T19:18:54.640630: step 13858, loss 0.651391.
Train: 2018-08-05T19:18:54.843707: step 13859, loss 0.5536.
Train: 2018-08-05T19:18:55.062406: step 13860, loss 0.553623.
Test: 2018-08-05T19:18:56.109035: step 13860, loss 0.546816.
Train: 2018-08-05T19:18:56.312113: step 13861, loss 0.588477.
Train: 2018-08-05T19:18:56.530813: step 13862, loss 0.467258.
Train: 2018-08-05T19:18:56.749510: step 13863, loss 0.570968.
Train: 2018-08-05T19:18:56.952589: step 13864, loss 0.579561.
Train: 2018-08-05T19:18:57.167207: step 13865, loss 0.588099.
Train: 2018-08-05T19:18:57.370283: step 13866, loss 0.52812.
Train: 2018-08-05T19:18:57.588982: step 13867, loss 0.562338.
Train: 2018-08-05T19:18:57.807681: step 13868, loss 0.528241.
Train: 2018-08-05T19:18:58.010758: step 13869, loss 0.553811.
Train: 2018-08-05T19:18:58.213836: step 13870, loss 0.605015.
Test: 2018-08-05T19:18:59.276057: step 13870, loss 0.547519.
Train: 2018-08-05T19:18:59.479164: step 13871, loss 0.48564.
Train: 2018-08-05T19:18:59.697863: step 13872, loss 0.519608.
Train: 2018-08-05T19:18:59.916533: step 13873, loss 0.665371.
Train: 2018-08-05T19:19:00.119640: step 13874, loss 0.605212.
Train: 2018-08-05T19:19:00.322716: step 13875, loss 0.5367.
Train: 2018-08-05T19:19:00.541417: step 13876, loss 0.562339.
Train: 2018-08-05T19:19:00.760114: step 13877, loss 0.604928.
Train: 2018-08-05T19:19:00.963191: step 13878, loss 0.570835.
Train: 2018-08-05T19:19:01.181891: step 13879, loss 0.61309.
Train: 2018-08-05T19:19:01.384938: step 13880, loss 0.587599.
Test: 2018-08-05T19:19:02.447189: step 13880, loss 0.549049.
Train: 2018-08-05T19:19:02.665915: step 13881, loss 0.562418.
Train: 2018-08-05T19:19:02.868995: step 13882, loss 0.587366.
Train: 2018-08-05T19:19:03.087694: step 13883, loss 0.554251.
Train: 2018-08-05T19:19:03.290773: step 13884, loss 0.55433.
Train: 2018-08-05T19:19:03.509470: step 13885, loss 0.562577.
Train: 2018-08-05T19:19:03.728169: step 13886, loss 0.562602.
Train: 2018-08-05T19:19:03.946838: step 13887, loss 0.538159.
Train: 2018-08-05T19:19:04.149945: step 13888, loss 0.554459.
Train: 2018-08-05T19:19:04.368644: step 13889, loss 0.554433.
Train: 2018-08-05T19:19:04.571722: step 13890, loss 0.529833.
Test: 2018-08-05T19:19:05.633973: step 13890, loss 0.548427.
Train: 2018-08-05T19:19:05.837050: step 13891, loss 0.513206.
Train: 2018-08-05T19:19:06.008855: step 13892, loss 0.580141.
Train: 2018-08-05T19:19:06.227583: step 13893, loss 0.612401.
Train: 2018-08-05T19:19:06.430662: step 13894, loss 0.554068.
Train: 2018-08-05T19:19:06.649357: step 13895, loss 0.637791.
Train: 2018-08-05T19:19:06.868058: step 13896, loss 0.520566.
Train: 2018-08-05T19:19:07.071105: step 13897, loss 0.503727.
Train: 2018-08-05T19:19:07.289836: step 13898, loss 0.612908.
Train: 2018-08-05T19:19:07.492912: step 13899, loss 0.596117.
Train: 2018-08-05T19:19:07.727203: step 13900, loss 0.621425.
Test: 2018-08-05T19:19:08.773833: step 13900, loss 0.548141.
Train: 2018-08-05T19:19:09.742356: step 13901, loss 0.511917.
Train: 2018-08-05T19:19:09.961084: step 13902, loss 0.537145.
Train: 2018-08-05T19:19:10.164131: step 13903, loss 0.520245.
Train: 2018-08-05T19:19:10.382861: step 13904, loss 0.579276.
Train: 2018-08-05T19:19:10.585939: step 13905, loss 0.570833.
Train: 2018-08-05T19:19:10.804637: step 13906, loss 0.477348.
Train: 2018-08-05T19:19:11.007685: step 13907, loss 0.562336.
Train: 2018-08-05T19:19:11.226383: step 13908, loss 0.562336.
Train: 2018-08-05T19:19:11.445112: step 13909, loss 0.570991.
Train: 2018-08-05T19:19:11.663808: step 13910, loss 0.544983.
Test: 2018-08-05T19:19:12.710411: step 13910, loss 0.547197.
Train: 2018-08-05T19:19:12.913520: step 13911, loss 0.579807.
Train: 2018-08-05T19:19:13.132218: step 13912, loss 0.457438.
Train: 2018-08-05T19:19:13.366537: step 13913, loss 0.53598.
Train: 2018-08-05T19:19:13.569585: step 13914, loss 0.704562.
Train: 2018-08-05T19:19:13.788315: step 13915, loss 0.624582.
Train: 2018-08-05T19:19:13.991391: step 13916, loss 0.624282.
Train: 2018-08-05T19:19:14.210060: step 13917, loss 0.544849.
Train: 2018-08-05T19:19:14.413168: step 13918, loss 0.597224.
Train: 2018-08-05T19:19:14.616244: step 13919, loss 0.622886.
Train: 2018-08-05T19:19:14.834943: step 13920, loss 0.562336.
Test: 2018-08-05T19:19:15.881542: step 13920, loss 0.549747.
Train: 2018-08-05T19:19:16.100272: step 13921, loss 0.545387.
Train: 2018-08-05T19:19:16.303349: step 13922, loss 0.596053.
Train: 2018-08-05T19:19:16.522050: step 13923, loss 0.537372.
Train: 2018-08-05T19:19:16.725125: step 13924, loss 0.603959.
Train: 2018-08-05T19:19:16.943824: step 13925, loss 0.537797.
Train: 2018-08-05T19:19:17.162525: step 13926, loss 0.587163.
Train: 2018-08-05T19:19:17.381221: step 13927, loss 0.611564.
Train: 2018-08-05T19:19:17.599920: step 13928, loss 0.497873.
Train: 2018-08-05T19:19:17.802998: step 13929, loss 0.538457.
Train: 2018-08-05T19:19:18.006075: step 13930, loss 0.643603.
Test: 2018-08-05T19:19:19.068326: step 13930, loss 0.54893.
Train: 2018-08-05T19:19:19.271404: step 13931, loss 0.57082.
Train: 2018-08-05T19:19:19.490102: step 13932, loss 0.586906.
Train: 2018-08-05T19:19:19.693182: step 13933, loss 0.602887.
Train: 2018-08-05T19:19:19.911880: step 13934, loss 0.57886.
Train: 2018-08-05T19:19:20.114956: step 13935, loss 0.547111.
Train: 2018-08-05T19:19:20.333655: step 13936, loss 0.602621.
Train: 2018-08-05T19:19:20.536736: step 13937, loss 0.547282.
Train: 2018-08-05T19:19:20.739810: step 13938, loss 0.618297.
Train: 2018-08-05T19:19:20.958479: step 13939, loss 0.547425.
Train: 2018-08-05T19:19:21.161587: step 13940, loss 0.625991.
Test: 2018-08-05T19:19:22.208187: step 13940, loss 0.549262.
Train: 2018-08-05T19:19:22.426886: step 13941, loss 0.524105.
Train: 2018-08-05T19:19:22.629993: step 13942, loss 0.56323.
Train: 2018-08-05T19:19:22.833040: step 13943, loss 0.571044.
Train: 2018-08-05T19:19:23.051769: step 13944, loss 0.52395.
Train: 2018-08-05T19:19:23.254817: step 13945, loss 0.555216.
Train: 2018-08-05T19:19:23.457923: step 13946, loss 0.547152.
Train: 2018-08-05T19:19:23.676622: step 13947, loss 0.57088.
Train: 2018-08-05T19:19:23.879700: step 13948, loss 0.522646.
Train: 2018-08-05T19:19:24.082778: step 13949, loss 0.55459.
Train: 2018-08-05T19:19:24.301447: step 13950, loss 0.538066.
Test: 2018-08-05T19:19:25.363727: step 13950, loss 0.549661.
Train: 2018-08-05T19:19:25.566775: step 13951, loss 0.603782.
Train: 2018-08-05T19:19:25.769884: step 13952, loss 0.537503.
Train: 2018-08-05T19:19:25.972930: step 13953, loss 0.537256.
Train: 2018-08-05T19:19:26.191660: step 13954, loss 0.596177.
Train: 2018-08-05T19:19:26.394738: step 13955, loss 0.511308.
Train: 2018-08-05T19:19:26.597784: step 13956, loss 0.519452.
Train: 2018-08-05T19:19:26.816482: step 13957, loss 0.545025.
Train: 2018-08-05T19:19:27.019589: step 13958, loss 0.562373.
Train: 2018-08-05T19:19:27.222667: step 13959, loss 0.527165.
Train: 2018-08-05T19:19:27.441366: step 13960, loss 0.518028.
Test: 2018-08-05T19:19:28.487996: step 13960, loss 0.546547.
Train: 2018-08-05T19:19:28.691073: step 13961, loss 0.472809.
Train: 2018-08-05T19:19:28.909774: step 13962, loss 0.626353.
Train: 2018-08-05T19:19:29.112849: step 13963, loss 0.49874.
Train: 2018-08-05T19:19:29.315927: step 13964, loss 0.553747.
Train: 2018-08-05T19:19:29.534627: step 13965, loss 0.544514.
Train: 2018-08-05T19:19:29.737703: step 13966, loss 0.553899.
Train: 2018-08-05T19:19:29.940781: step 13967, loss 0.629309.
Train: 2018-08-05T19:19:30.143828: step 13968, loss 0.563393.
Train: 2018-08-05T19:19:30.362557: step 13969, loss 0.582193.
Train: 2018-08-05T19:19:30.565634: step 13970, loss 0.591452.
Test: 2018-08-05T19:19:31.627855: step 13970, loss 0.549745.
Train: 2018-08-05T19:19:31.830964: step 13971, loss 0.581842.
Train: 2018-08-05T19:19:32.034011: step 13972, loss 0.507734.
Train: 2018-08-05T19:19:32.237118: step 13973, loss 0.553742.
Train: 2018-08-05T19:19:32.440195: step 13974, loss 0.5629.
Train: 2018-08-05T19:19:32.658896: step 13975, loss 0.636022.
Train: 2018-08-05T19:19:32.861972: step 13976, loss 0.508278.
Train: 2018-08-05T19:19:33.065019: step 13977, loss 0.517541.
Train: 2018-08-05T19:19:33.268125: step 13978, loss 0.625452.
Train: 2018-08-05T19:19:33.471204: step 13979, loss 0.651683.
Train: 2018-08-05T19:19:33.674282: step 13980, loss 0.562417.
Test: 2018-08-05T19:19:34.736503: step 13980, loss 0.548403.
Train: 2018-08-05T19:19:34.939611: step 13981, loss 0.544924.
Train: 2018-08-05T19:19:35.142658: step 13982, loss 0.605541.
Train: 2018-08-05T19:19:35.345765: step 13983, loss 0.536692.
Train: 2018-08-05T19:19:35.548813: step 13984, loss 0.519966.
Train: 2018-08-05T19:19:35.751889: step 13985, loss 0.629839.
Train: 2018-08-05T19:19:35.954998: step 13986, loss 0.562408.
Train: 2018-08-05T19:19:36.158076: step 13987, loss 0.620596.
Train: 2018-08-05T19:19:36.361152: step 13988, loss 0.54607.
Train: 2018-08-05T19:19:36.564232: step 13989, loss 0.603455.
Train: 2018-08-05T19:19:36.767307: step 13990, loss 0.497844.
Test: 2018-08-05T19:19:37.829558: step 13990, loss 0.548859.
Train: 2018-08-05T19:19:38.032637: step 13991, loss 0.50616.
Train: 2018-08-05T19:19:38.235713: step 13992, loss 0.570802.
Train: 2018-08-05T19:19:38.438791: step 13993, loss 0.538401.
Train: 2018-08-05T19:19:38.641867: step 13994, loss 0.554539.
Train: 2018-08-05T19:19:38.844946: step 13995, loss 0.56262.
Train: 2018-08-05T19:19:39.048023: step 13996, loss 0.538019.
Train: 2018-08-05T19:19:39.251101: step 13997, loss 0.529604.
Train: 2018-08-05T19:19:39.454179: step 13998, loss 0.52931.
Train: 2018-08-05T19:19:39.657226: step 13999, loss 0.587493.
Train: 2018-08-05T19:19:39.860333: step 14000, loss 0.562378.
Test: 2018-08-05T19:19:40.969448: step 14000, loss 0.548411.
Train: 2018-08-05T19:19:41.969214: step 14001, loss 0.613156.
Train: 2018-08-05T19:19:42.156672: step 14002, loss 0.494423.
Train: 2018-08-05T19:19:42.359718: step 14003, loss 0.596512.
Train: 2018-08-05T19:19:42.562825: step 14004, loss 0.545183.
Train: 2018-08-05T19:19:42.765904: step 14005, loss 0.545111.
Train: 2018-08-05T19:19:42.968980: step 14006, loss 0.570995.
Train: 2018-08-05T19:19:43.172057: step 14007, loss 0.597084.
Train: 2018-08-05T19:19:43.359513: step 14008, loss 0.605821.
Train: 2018-08-05T19:19:43.562591: step 14009, loss 0.553669.
Train: 2018-08-05T19:19:43.765668: step 14010, loss 0.571019.
Test: 2018-08-05T19:19:44.812298: step 14010, loss 0.547485.
Train: 2018-08-05T19:19:45.015375: step 14011, loss 0.57966.
Train: 2018-08-05T19:19:45.218423: step 14012, loss 0.579612.
Train: 2018-08-05T19:19:45.421525: step 14013, loss 0.570944.
Train: 2018-08-05T19:19:45.624578: step 14014, loss 0.562335.
Train: 2018-08-05T19:19:45.812064: step 14015, loss 0.647819.
Train: 2018-08-05T19:19:46.015141: step 14016, loss 0.54568.
Train: 2018-08-05T19:19:46.218189: step 14017, loss 0.596049.
Train: 2018-08-05T19:19:46.421296: step 14018, loss 0.503856.
Train: 2018-08-05T19:19:46.608755: step 14019, loss 0.587491.
Train: 2018-08-05T19:19:46.811830: step 14020, loss 0.520779.
Test: 2018-08-05T19:19:47.874052: step 14020, loss 0.547956.
Train: 2018-08-05T19:19:48.077159: step 14021, loss 0.595742.
Train: 2018-08-05T19:19:48.264614: step 14022, loss 0.554141.
Train: 2018-08-05T19:19:48.467692: step 14023, loss 0.57906.
Train: 2018-08-05T19:19:48.670770: step 14024, loss 0.562469.
Train: 2018-08-05T19:19:48.873847: step 14025, loss 0.504516.
Train: 2018-08-05T19:19:49.076924: step 14026, loss 0.529248.
Train: 2018-08-05T19:19:49.280004: step 14027, loss 0.570766.
Train: 2018-08-05T19:19:49.483079: step 14028, loss 0.570776.
Train: 2018-08-05T19:19:49.670506: step 14029, loss 0.51198.
Train: 2018-08-05T19:19:49.873582: step 14030, loss 0.460952.
Test: 2018-08-05T19:19:50.935835: step 14030, loss 0.54964.
Train: 2018-08-05T19:19:51.138941: step 14031, loss 0.579426.
Train: 2018-08-05T19:19:51.342019: step 14032, loss 0.527871.
Train: 2018-08-05T19:19:51.529478: step 14033, loss 0.466626.
Train: 2018-08-05T19:19:51.732553: step 14034, loss 0.52714.
Train: 2018-08-05T19:19:51.920008: step 14035, loss 0.562529.
Train: 2018-08-05T19:19:52.123057: step 14036, loss 0.607869.
Train: 2018-08-05T19:19:52.326163: step 14037, loss 0.480777.
Train: 2018-08-05T19:19:52.513619: step 14038, loss 0.553711.
Train: 2018-08-05T19:19:52.716697: step 14039, loss 0.563062.
Train: 2018-08-05T19:19:52.919775: step 14040, loss 0.553857.
Test: 2018-08-05T19:19:53.966375: step 14040, loss 0.546121.
Train: 2018-08-05T19:19:54.169482: step 14041, loss 0.535151.
Train: 2018-08-05T19:19:54.372560: step 14042, loss 0.563421.
Train: 2018-08-05T19:19:54.528774: step 14043, loss 0.583684.
Train: 2018-08-05T19:19:54.731850: step 14044, loss 0.582449.
Train: 2018-08-05T19:19:54.934928: step 14045, loss 0.544569.
Train: 2018-08-05T19:19:55.138007: step 14046, loss 0.497385.
Train: 2018-08-05T19:19:55.341082: step 14047, loss 0.554.
Train: 2018-08-05T19:19:55.544163: step 14048, loss 0.525691.
Train: 2018-08-05T19:19:55.731616: step 14049, loss 0.54456.
Train: 2018-08-05T19:19:55.934694: step 14050, loss 0.563429.
Test: 2018-08-05T19:19:56.981324: step 14050, loss 0.548095.
Train: 2018-08-05T19:19:57.168779: step 14051, loss 0.572819.
Train: 2018-08-05T19:19:57.371857: step 14052, loss 0.535145.
Train: 2018-08-05T19:19:57.574935: step 14053, loss 0.525787.
Train: 2018-08-05T19:19:57.762390: step 14054, loss 0.581961.
Train: 2018-08-05T19:19:57.965468: step 14055, loss 0.563172.
Train: 2018-08-05T19:19:58.152925: step 14056, loss 0.600243.
Train: 2018-08-05T19:19:58.356002: step 14057, loss 0.553737.
Train: 2018-08-05T19:19:58.543457: step 14058, loss 0.562854.
Train: 2018-08-05T19:19:58.746535: step 14059, loss 0.553645.
Train: 2018-08-05T19:19:58.933991: step 14060, loss 0.59886.
Test: 2018-08-05T19:19:59.996212: step 14060, loss 0.549027.
Train: 2018-08-05T19:20:00.199320: step 14061, loss 0.499752.
Train: 2018-08-05T19:20:00.402399: step 14062, loss 0.553589.
Train: 2018-08-05T19:20:00.589853: step 14063, loss 0.52692.
Train: 2018-08-05T19:20:00.777311: step 14064, loss 0.482666.
Train: 2018-08-05T19:20:00.980388: step 14065, loss 0.526959.
Train: 2018-08-05T19:20:01.183464: step 14066, loss 0.517992.
Train: 2018-08-05T19:20:01.386512: step 14067, loss 0.544652.
Train: 2018-08-05T19:20:01.573997: step 14068, loss 0.652413.
Train: 2018-08-05T19:20:01.777046: step 14069, loss 0.634198.
Train: 2018-08-05T19:20:01.964531: step 14070, loss 0.598114.
Test: 2018-08-05T19:20:03.011132: step 14070, loss 0.547674.
Train: 2018-08-05T19:20:03.214208: step 14071, loss 0.474051.
Train: 2018-08-05T19:20:03.401697: step 14072, loss 0.50071.
Train: 2018-08-05T19:20:03.589151: step 14073, loss 0.588879.
Train: 2018-08-05T19:20:03.792229: step 14074, loss 0.527181.
Train: 2018-08-05T19:20:03.979684: step 14075, loss 0.571224.
Train: 2018-08-05T19:20:04.182763: step 14076, loss 0.500777.
Train: 2018-08-05T19:20:04.385846: step 14077, loss 0.5889.
Train: 2018-08-05T19:20:04.573295: step 14078, loss 0.580082.
Train: 2018-08-05T19:20:04.760722: step 14079, loss 0.571238.
Train: 2018-08-05T19:20:04.948177: step 14080, loss 0.483186.
Test: 2018-08-05T19:20:06.010461: step 14080, loss 0.54708.
Train: 2018-08-05T19:20:06.197915: step 14081, loss 0.544779.
Train: 2018-08-05T19:20:06.400994: step 14082, loss 0.544752.
Train: 2018-08-05T19:20:06.588448: step 14083, loss 0.61566.
Train: 2018-08-05T19:20:06.791526: step 14084, loss 0.580172.
Train: 2018-08-05T19:20:06.978982: step 14085, loss 0.633172.
Train: 2018-08-05T19:20:07.166408: step 14086, loss 0.553611.
Train: 2018-08-05T19:20:07.353864: step 14087, loss 0.562374.
Train: 2018-08-05T19:20:07.556942: step 14088, loss 0.553657.
Train: 2018-08-05T19:20:07.744398: step 14089, loss 0.622975.
Train: 2018-08-05T19:20:07.931855: step 14090, loss 0.613927.
Test: 2018-08-05T19:20:08.978514: step 14090, loss 0.547923.
Train: 2018-08-05T19:20:09.181591: step 14091, loss 0.579374.
Train: 2018-08-05T19:20:09.369047: step 14092, loss 0.562376.
Train: 2018-08-05T19:20:09.556504: step 14093, loss 0.587452.
Train: 2018-08-05T19:20:09.759583: step 14094, loss 0.505619.
Train: 2018-08-05T19:20:09.947008: step 14095, loss 0.612066.
Train: 2018-08-05T19:20:10.134495: step 14096, loss 0.537906.
Train: 2018-08-05T19:20:10.321949: step 14097, loss 0.529845.
Train: 2018-08-05T19:20:10.509405: step 14098, loss 0.587121.
Train: 2018-08-05T19:20:10.712483: step 14099, loss 0.54628.
Train: 2018-08-05T19:20:10.899940: step 14100, loss 0.570771.
Test: 2018-08-05T19:20:11.962190: step 14100, loss 0.54916.
Train: 2018-08-05T19:20:12.868228: step 14101, loss 0.529951.
Train: 2018-08-05T19:20:13.071305: step 14102, loss 0.554398.
Train: 2018-08-05T19:20:13.274385: step 14103, loss 0.57076.
Train: 2018-08-05T19:20:13.461839: step 14104, loss 0.562524.
Train: 2018-08-05T19:20:13.649266: step 14105, loss 0.529466.
Train: 2018-08-05T19:20:13.836721: step 14106, loss 0.545862.
Train: 2018-08-05T19:20:14.024177: step 14107, loss 0.579114.
Train: 2018-08-05T19:20:14.211663: step 14108, loss 0.57078.
Train: 2018-08-05T19:20:14.414711: step 14109, loss 0.528715.
Train: 2018-08-05T19:20:14.602198: step 14110, loss 0.579281.
Test: 2018-08-05T19:20:15.664448: step 14110, loss 0.54718.
Train: 2018-08-05T19:20:15.851905: step 14111, loss 0.587835.
Train: 2018-08-05T19:20:16.039360: step 14112, loss 0.553826.
Train: 2018-08-05T19:20:16.242408: step 14113, loss 0.605029.
Train: 2018-08-05T19:20:16.429895: step 14114, loss 0.622106.
Train: 2018-08-05T19:20:16.617321: step 14115, loss 0.553832.
Train: 2018-08-05T19:20:16.804809: step 14116, loss 0.553857.
Train: 2018-08-05T19:20:17.007883: step 14117, loss 0.570829.
Train: 2018-08-05T19:20:17.195339: step 14118, loss 0.579281.
Train: 2018-08-05T19:20:17.382798: step 14119, loss 0.570807.
Train: 2018-08-05T19:20:17.570253: step 14120, loss 0.612894.
Test: 2018-08-05T19:20:18.632503: step 14120, loss 0.547828.
Train: 2018-08-05T19:20:18.819959: step 14121, loss 0.654563.
Train: 2018-08-05T19:20:19.023008: step 14122, loss 0.570759.
Train: 2018-08-05T19:20:19.210493: step 14123, loss 0.578983.
Train: 2018-08-05T19:20:19.397943: step 14124, loss 0.481265.
Train: 2018-08-05T19:20:19.585405: step 14125, loss 0.668537.
Train: 2018-08-05T19:20:19.772863: step 14126, loss 0.538456.
Train: 2018-08-05T19:20:19.975944: step 14127, loss 0.506429.
Train: 2018-08-05T19:20:20.163395: step 14128, loss 0.56278.
Train: 2018-08-05T19:20:20.350851: step 14129, loss 0.675499.
Train: 2018-08-05T19:20:20.538307: step 14130, loss 0.554837.
Test: 2018-08-05T19:20:21.584907: step 14130, loss 0.547915.
Train: 2018-08-05T19:20:21.819258: step 14131, loss 0.522992.
Train: 2018-08-05T19:20:22.022304: step 14132, loss 0.522997.
Train: 2018-08-05T19:20:22.209790: step 14133, loss 0.570859.
Train: 2018-08-05T19:20:22.397248: step 14134, loss 0.546754.
Train: 2018-08-05T19:20:22.584703: step 14135, loss 0.530496.
Train: 2018-08-05T19:20:22.772161: step 14136, loss 0.497743.
Train: 2018-08-05T19:20:22.959584: step 14137, loss 0.496978.
Train: 2018-08-05T19:20:23.147073: step 14138, loss 0.545803.
Train: 2018-08-05T19:20:23.350121: step 14139, loss 0.545181.
Train: 2018-08-05T19:20:23.537575: step 14140, loss 0.581739.
Test: 2018-08-05T19:20:24.584234: step 14140, loss 0.548005.
Train: 2018-08-05T19:20:24.771692: step 14141, loss 0.55359.
Train: 2018-08-05T19:20:24.959146: step 14142, loss 0.51848.
Train: 2018-08-05T19:20:25.146604: step 14143, loss 0.527161.
Train: 2018-08-05T19:20:25.334061: step 14144, loss 0.606984.
Train: 2018-08-05T19:20:25.521516: step 14145, loss 0.580292.
Train: 2018-08-05T19:20:25.708972: step 14146, loss 0.562508.
Train: 2018-08-05T19:20:25.896430: step 14147, loss 0.571481.
Train: 2018-08-05T19:20:26.099505: step 14148, loss 0.544631.
Train: 2018-08-05T19:20:26.302584: step 14149, loss 0.553594.
Train: 2018-08-05T19:20:26.474416: step 14150, loss 0.598544.
Test: 2018-08-05T19:20:27.536668: step 14150, loss 0.547372.
Train: 2018-08-05T19:20:27.724126: step 14151, loss 0.59849.
Train: 2018-08-05T19:20:27.911580: step 14152, loss 0.66096.
Train: 2018-08-05T19:20:28.099007: step 14153, loss 0.526983.
Train: 2018-08-05T19:20:28.286463: step 14154, loss 0.580037.
Train: 2018-08-05T19:20:28.473919: step 14155, loss 0.562376.
Train: 2018-08-05T19:20:28.661405: step 14156, loss 0.588416.
Train: 2018-08-05T19:20:28.848861: step 14157, loss 0.553716.
Train: 2018-08-05T19:20:29.036317: step 14158, loss 0.510954.
Train: 2018-08-05T19:20:29.239364: step 14159, loss 0.579409.
Train: 2018-08-05T19:20:29.426851: step 14160, loss 0.511329.
Test: 2018-08-05T19:20:30.489079: step 14160, loss 0.548351.
Train: 2018-08-05T19:20:30.676558: step 14161, loss 0.519862.
Train: 2018-08-05T19:20:30.864014: step 14162, loss 0.570855.
Train: 2018-08-05T19:20:31.035848: step 14163, loss 0.579384.
Train: 2018-08-05T19:20:31.238897: step 14164, loss 0.579387.
Train: 2018-08-05T19:20:31.426382: step 14165, loss 0.545308.
Train: 2018-08-05T19:20:31.613838: step 14166, loss 0.553822.
Train: 2018-08-05T19:20:31.801294: step 14167, loss 0.639068.
Train: 2018-08-05T19:20:31.988751: step 14168, loss 0.536859.
Train: 2018-08-05T19:20:32.176208: step 14169, loss 0.570832.
Train: 2018-08-05T19:20:32.363633: step 14170, loss 0.579288.
Test: 2018-08-05T19:20:33.425915: step 14170, loss 0.547873.
Train: 2018-08-05T19:20:33.613370: step 14171, loss 0.511698.
Train: 2018-08-05T19:20:33.800797: step 14172, loss 0.562362.
Train: 2018-08-05T19:20:33.988282: step 14173, loss 0.545448.
Train: 2018-08-05T19:20:34.175739: step 14174, loss 0.494593.
Train: 2018-08-05T19:20:34.363164: step 14175, loss 0.562342.
Train: 2018-08-05T19:20:34.550650: step 14176, loss 0.630785.
Train: 2018-08-05T19:20:34.738107: step 14177, loss 0.528091.
Train: 2018-08-05T19:20:34.925563: step 14178, loss 0.579498.
Train: 2018-08-05T19:20:35.113019: step 14179, loss 0.545153.
Train: 2018-08-05T19:20:35.300474: step 14180, loss 0.700051.
Test: 2018-08-05T19:20:36.347104: step 14180, loss 0.54765.
Train: 2018-08-05T19:20:36.534561: step 14181, loss 0.579451.
Train: 2018-08-05T19:20:36.737639: step 14182, loss 0.53683.
Train: 2018-08-05T19:20:36.925094: step 14183, loss 0.655526.
Train: 2018-08-05T19:20:37.112550: step 14184, loss 0.537198.
Train: 2018-08-05T19:20:37.300007: step 14185, loss 0.570767.
Train: 2018-08-05T19:20:37.487434: step 14186, loss 0.529279.
Train: 2018-08-05T19:20:37.674890: step 14187, loss 0.570756.
Train: 2018-08-05T19:20:37.862371: step 14188, loss 0.513029.
Train: 2018-08-05T19:20:38.065452: step 14189, loss 0.570756.
Train: 2018-08-05T19:20:38.237287: step 14190, loss 0.579012.
Test: 2018-08-05T19:20:39.299509: step 14190, loss 0.549283.
Train: 2018-08-05T19:20:39.486995: step 14191, loss 0.545989.
Train: 2018-08-05T19:20:39.674451: step 14192, loss 0.545958.
Train: 2018-08-05T19:20:39.877522: step 14193, loss 0.537611.
Train: 2018-08-05T19:20:40.033741: step 14194, loss 0.544694.
Train: 2018-08-05T19:20:40.221198: step 14195, loss 0.587492.
Train: 2018-08-05T19:20:40.408624: step 14196, loss 0.579168.
Train: 2018-08-05T19:20:40.596110: step 14197, loss 0.537167.
Train: 2018-08-05T19:20:40.799189: step 14198, loss 0.65516.
Train: 2018-08-05T19:20:40.986643: step 14199, loss 0.553954.
Train: 2018-08-05T19:20:41.174101: step 14200, loss 0.553966.
Test: 2018-08-05T19:20:42.220700: step 14200, loss 0.547944.
Train: 2018-08-05T19:20:43.173632: step 14201, loss 0.537144.
Train: 2018-08-05T19:20:43.361087: step 14202, loss 0.545527.
Train: 2018-08-05T19:20:43.564164: step 14203, loss 0.613026.
Train: 2018-08-05T19:20:43.751620: step 14204, loss 0.553927.
Train: 2018-08-05T19:20:43.939078: step 14205, loss 0.63833.
Train: 2018-08-05T19:20:44.126533: step 14206, loss 0.537159.
Train: 2018-08-05T19:20:44.313959: step 14207, loss 0.646308.
Train: 2018-08-05T19:20:44.501445: step 14208, loss 0.554085.
Train: 2018-08-05T19:20:44.704493: step 14209, loss 0.579059.
Train: 2018-08-05T19:20:44.876357: step 14210, loss 0.554236.
Test: 2018-08-05T19:20:45.938579: step 14210, loss 0.548591.
Train: 2018-08-05T19:20:46.126067: step 14211, loss 0.546065.
Train: 2018-08-05T19:20:46.329142: step 14212, loss 0.537897.
Train: 2018-08-05T19:20:46.516568: step 14213, loss 0.570759.
Train: 2018-08-05T19:20:46.704055: step 14214, loss 0.587196.
Train: 2018-08-05T19:20:46.891512: step 14215, loss 0.529697.
Train: 2018-08-05T19:20:47.078967: step 14216, loss 0.529628.
Train: 2018-08-05T19:20:47.266424: step 14217, loss 0.570756.
Train: 2018-08-05T19:20:47.453879: step 14218, loss 0.570758.
Train: 2018-08-05T19:20:47.641336: step 14219, loss 0.537467.
Train: 2018-08-05T19:20:47.844382: step 14220, loss 0.511503.
Test: 2018-08-05T19:20:48.891011: step 14220, loss 0.546496.
Train: 2018-08-05T19:20:49.078499: step 14221, loss 0.508134.
Train: 2018-08-05T19:20:49.265956: step 14222, loss 0.515426.
Train: 2018-08-05T19:20:49.453411: step 14223, loss 0.564629.
Train: 2018-08-05T19:20:49.640837: step 14224, loss 0.494337.
Train: 2018-08-05T19:20:49.828323: step 14225, loss 0.606603.
Train: 2018-08-05T19:20:50.031400: step 14226, loss 0.599177.
Train: 2018-08-05T19:20:50.218856: step 14227, loss 0.53642.
Train: 2018-08-05T19:20:50.406282: step 14228, loss 0.511508.
Train: 2018-08-05T19:20:50.593739: step 14229, loss 0.477631.
Train: 2018-08-05T19:20:50.796846: step 14230, loss 0.545317.
Test: 2018-08-05T19:20:51.843476: step 14230, loss 0.548832.
Train: 2018-08-05T19:20:52.030932: step 14231, loss 0.476761.
Train: 2018-08-05T19:20:52.234010: step 14232, loss 0.527791.
Train: 2018-08-05T19:20:52.421436: step 14233, loss 0.640882.
Train: 2018-08-05T19:20:52.608892: step 14234, loss 0.553621.
Train: 2018-08-05T19:20:52.811999: step 14235, loss 0.562408.
Train: 2018-08-05T19:20:53.015078: step 14236, loss 0.527103.
Train: 2018-08-05T19:20:53.202535: step 14237, loss 0.535847.
Train: 2018-08-05T19:20:53.389991: step 14238, loss 0.491174.
Train: 2018-08-05T19:20:53.593066: step 14239, loss 0.598519.
Train: 2018-08-05T19:20:53.780523: step 14240, loss 0.544583.
Test: 2018-08-05T19:20:54.842775: step 14240, loss 0.547341.
Train: 2018-08-05T19:20:55.030230: step 14241, loss 0.553622.
Train: 2018-08-05T19:20:55.217686: step 14242, loss 0.553638.
Train: 2018-08-05T19:20:55.420734: step 14243, loss 0.498934.
Train: 2018-08-05T19:20:55.608219: step 14244, loss 0.553684.
Train: 2018-08-05T19:20:55.795676: step 14245, loss 0.627334.
Train: 2018-08-05T19:20:55.983102: step 14246, loss 0.581304.
Train: 2018-08-05T19:20:56.170588: step 14247, loss 0.627106.
Train: 2018-08-05T19:20:56.358014: step 14248, loss 0.562772.
Train: 2018-08-05T19:20:56.561122: step 14249, loss 0.571733.
Train: 2018-08-05T19:20:56.748577: step 14250, loss 0.580443.
Test: 2018-08-05T19:20:57.795207: step 14250, loss 0.548601.
Train: 2018-08-05T19:20:57.998284: step 14251, loss 0.562335.
Train: 2018-08-05T19:20:58.185741: step 14252, loss 0.562782.
Train: 2018-08-05T19:20:58.388820: step 14253, loss 0.6126.
Train: 2018-08-05T19:20:58.576275: step 14254, loss 0.555353.
Train: 2018-08-05T19:20:58.779322: step 14255, loss 0.602632.
Train: 2018-08-05T19:20:58.966807: step 14256, loss 0.548462.
Train: 2018-08-05T19:20:59.169880: step 14257, loss 0.507309.
Train: 2018-08-05T19:20:59.357311: step 14258, loss 0.553925.
Train: 2018-08-05T19:20:59.560389: step 14259, loss 0.510538.
Train: 2018-08-05T19:20:59.747874: step 14260, loss 0.640318.
Test: 2018-08-05T19:21:00.810096: step 14260, loss 0.548088.
Train: 2018-08-05T19:21:00.997582: step 14261, loss 0.51907.
Train: 2018-08-05T19:21:01.200631: step 14262, loss 0.501687.
Train: 2018-08-05T19:21:01.403737: step 14263, loss 0.605866.
Train: 2018-08-05T19:21:01.591193: step 14264, loss 0.579786.
Train: 2018-08-05T19:21:01.778619: step 14265, loss 0.527512.
Train: 2018-08-05T19:21:01.981728: step 14266, loss 0.588543.
Train: 2018-08-05T19:21:02.169152: step 14267, loss 0.59726.
Train: 2018-08-05T19:21:02.356639: step 14268, loss 0.536247.
Train: 2018-08-05T19:21:02.559716: step 14269, loss 0.631915.
Train: 2018-08-05T19:21:02.747172: step 14270, loss 0.545035.
Test: 2018-08-05T19:21:03.809424: step 14270, loss 0.547539.
Train: 2018-08-05T19:21:03.996882: step 14271, loss 0.614077.
Train: 2018-08-05T19:21:04.199958: step 14272, loss 0.570906.
Train: 2018-08-05T19:21:04.403035: step 14273, loss 0.485662.
Train: 2018-08-05T19:21:04.590491: step 14274, loss 0.519795.
Train: 2018-08-05T19:21:04.793539: step 14275, loss 0.604942.
Train: 2018-08-05T19:21:04.981025: step 14276, loss 0.579364.
Train: 2018-08-05T19:21:05.168480: step 14277, loss 0.562347.
Train: 2018-08-05T19:21:05.371558: step 14278, loss 0.647153.
Train: 2018-08-05T19:21:05.574635: step 14279, loss 0.545513.
Train: 2018-08-05T19:21:05.762093: step 14280, loss 0.612747.
Test: 2018-08-05T19:21:06.824343: step 14280, loss 0.547542.
Train: 2018-08-05T19:21:07.027420: step 14281, loss 0.495719.
Train: 2018-08-05T19:21:07.230498: step 14282, loss 0.570762.
Train: 2018-08-05T19:21:07.417954: step 14283, loss 0.595683.
Train: 2018-08-05T19:21:07.621000: step 14284, loss 0.545912.
Train: 2018-08-05T19:21:07.808487: step 14285, loss 0.537682.
Train: 2018-08-05T19:21:08.011563: step 14286, loss 0.603843.
Train: 2018-08-05T19:21:08.214643: step 14287, loss 0.603791.
Train: 2018-08-05T19:21:08.417691: step 14288, loss 0.562526.
Train: 2018-08-05T19:21:08.605176: step 14289, loss 0.488659.
Train: 2018-08-05T19:21:08.808253: step 14290, loss 0.54607.
Test: 2018-08-05T19:21:09.854883: step 14290, loss 0.549088.
Train: 2018-08-05T19:21:10.057961: step 14291, loss 0.562499.
Train: 2018-08-05T19:21:10.245411: step 14292, loss 0.570757.
Train: 2018-08-05T19:21:10.448494: step 14293, loss 0.5292.
Train: 2018-08-05T19:21:10.635952: step 14294, loss 0.562416.
Train: 2018-08-05T19:21:10.839027: step 14295, loss 0.587568.
Train: 2018-08-05T19:21:11.026478: step 14296, loss 0.562377.
Train: 2018-08-05T19:21:11.229562: step 14297, loss 0.528596.
Train: 2018-08-05T19:21:11.417017: step 14298, loss 0.596276.
Train: 2018-08-05T19:21:11.620096: step 14299, loss 0.58785.
Train: 2018-08-05T19:21:11.807551: step 14300, loss 0.545327.
Test: 2018-08-05T19:21:12.869772: step 14300, loss 0.547322.
Train: 2018-08-05T19:21:13.885191: step 14301, loss 0.502681.
Train: 2018-08-05T19:21:14.088261: step 14302, loss 0.613718.
Train: 2018-08-05T19:21:14.275724: step 14303, loss 0.536606.
Train: 2018-08-05T19:21:14.478800: step 14304, loss 0.493539.
Train: 2018-08-05T19:21:14.681878: step 14305, loss 0.562342.
Train: 2018-08-05T19:21:14.884957: step 14306, loss 0.510173.
Train: 2018-08-05T19:21:15.088032: step 14307, loss 0.562383.
Train: 2018-08-05T19:21:15.275490: step 14308, loss 0.544787.
Train: 2018-08-05T19:21:15.478566: step 14309, loss 0.518115.
Train: 2018-08-05T19:21:15.681614: step 14310, loss 0.499983.
Test: 2018-08-05T19:21:16.728275: step 14310, loss 0.54777.
Train: 2018-08-05T19:21:16.931352: step 14311, loss 0.53557.
Train: 2018-08-05T19:21:17.134430: step 14312, loss 0.608237.
Train: 2018-08-05T19:21:17.321885: step 14313, loss 0.617682.
Train: 2018-08-05T19:21:17.524962: step 14314, loss 0.526217.
Train: 2018-08-05T19:21:17.728039: step 14315, loss 0.617867.
Train: 2018-08-05T19:21:17.931118: step 14316, loss 0.60857.
Train: 2018-08-05T19:21:18.134196: step 14317, loss 0.590058.
Train: 2018-08-05T19:21:18.337272: step 14318, loss 0.553614.
Train: 2018-08-05T19:21:18.540350: step 14319, loss 0.571567.
Train: 2018-08-05T19:21:18.759050: step 14320, loss 0.51789.
Test: 2018-08-05T19:21:19.821299: step 14320, loss 0.545794.
Train: 2018-08-05T19:21:20.024376: step 14321, loss 0.589132.
Train: 2018-08-05T19:21:20.211833: step 14322, loss 0.597768.
Train: 2018-08-05T19:21:20.414911: step 14323, loss 0.614995.
Train: 2018-08-05T19:21:20.617990: step 14324, loss 0.544987.
Train: 2018-08-05T19:21:20.821065: step 14325, loss 0.545115.
Train: 2018-08-05T19:21:21.024146: step 14326, loss 0.587997.
Train: 2018-08-05T19:21:21.227220: step 14327, loss 0.519894.
Train: 2018-08-05T19:21:21.430292: step 14328, loss 0.503178.
Train: 2018-08-05T19:21:21.633377: step 14329, loss 0.545461.
Train: 2018-08-05T19:21:21.836452: step 14330, loss 0.596183.
Test: 2018-08-05T19:21:22.883053: step 14330, loss 0.548064.
Train: 2018-08-05T19:21:23.086160: step 14331, loss 0.579256.
Train: 2018-08-05T19:21:23.289237: step 14332, loss 0.638245.
Train: 2018-08-05T19:21:23.492285: step 14333, loss 0.537248.
Train: 2018-08-05T19:21:23.695363: step 14334, loss 0.537356.
Train: 2018-08-05T19:21:23.898470: step 14335, loss 0.554084.
Train: 2018-08-05T19:21:24.101550: step 14336, loss 0.562429.
Train: 2018-08-05T19:21:24.304595: step 14337, loss 0.570765.
Train: 2018-08-05T19:21:24.492080: step 14338, loss 0.562433.
Train: 2018-08-05T19:21:24.695158: step 14339, loss 0.562433.
Train: 2018-08-05T19:21:24.913827: step 14340, loss 0.56243.
Test: 2018-08-05T19:21:25.960488: step 14340, loss 0.549437.
Train: 2018-08-05T19:21:26.163564: step 14341, loss 0.554071.
Train: 2018-08-05T19:21:26.366641: step 14342, loss 0.596286.
Train: 2018-08-05T19:21:26.569719: step 14343, loss 0.562423.
Train: 2018-08-05T19:21:26.772797: step 14344, loss 0.587443.
Train: 2018-08-05T19:21:26.960252: step 14345, loss 0.70455.
Train: 2018-08-05T19:21:27.163330: step 14346, loss 0.62853.
Train: 2018-08-05T19:21:27.366407: step 14347, loss 0.603414.
Train: 2018-08-05T19:21:27.569485: step 14348, loss 0.619193.
Train: 2018-08-05T19:21:27.772562: step 14349, loss 0.578858.
Train: 2018-08-05T19:21:27.975610: step 14350, loss 0.524014.
Test: 2018-08-05T19:21:29.037861: step 14350, loss 0.550812.
Train: 2018-08-05T19:21:29.225347: step 14351, loss 0.485316.
Train: 2018-08-05T19:21:29.444046: step 14352, loss 0.539956.
Train: 2018-08-05T19:21:29.647123: step 14353, loss 0.563295.
Train: 2018-08-05T19:21:29.850201: step 14354, loss 0.555435.
Train: 2018-08-05T19:21:30.053278: step 14355, loss 0.641631.
Train: 2018-08-05T19:21:30.256357: step 14356, loss 0.563193.
Train: 2018-08-05T19:21:30.475054: step 14357, loss 0.492561.
Train: 2018-08-05T19:21:30.678132: step 14358, loss 0.563079.
Train: 2018-08-05T19:21:30.881179: step 14359, loss 0.475638.
Train: 2018-08-05T19:21:31.099879: step 14360, loss 0.586901.
Test: 2018-08-05T19:21:32.146539: step 14360, loss 0.548958.
Train: 2018-08-05T19:21:32.365238: step 14361, loss 0.587008.
Train: 2018-08-05T19:21:32.568315: step 14362, loss 0.505401.
Train: 2018-08-05T19:21:32.771361: step 14363, loss 0.587274.
Train: 2018-08-05T19:21:32.974440: step 14364, loss 0.487457.
Train: 2018-08-05T19:21:33.193168: step 14365, loss 0.545505.
Train: 2018-08-05T19:21:33.396216: step 14366, loss 0.605008.
Train: 2018-08-05T19:21:33.599323: step 14367, loss 0.588154.
Train: 2018-08-05T19:21:33.818021: step 14368, loss 0.553685.
Train: 2018-08-05T19:21:34.021098: step 14369, loss 0.640734.
Train: 2018-08-05T19:21:34.239798: step 14370, loss 0.614652.
Test: 2018-08-05T19:21:35.286398: step 14370, loss 0.548233.
Train: 2018-08-05T19:21:35.489505: step 14371, loss 0.553659.
Train: 2018-08-05T19:21:35.692583: step 14372, loss 0.571028.
Train: 2018-08-05T19:21:35.895660: step 14373, loss 0.614296.
Train: 2018-08-05T19:21:36.098737: step 14374, loss 0.596789.
Train: 2018-08-05T19:21:36.317407: step 14375, loss 0.485741.
Train: 2018-08-05T19:21:36.520484: step 14376, loss 0.630701.
Train: 2018-08-05T19:21:36.723591: step 14377, loss 0.502837.
Train: 2018-08-05T19:21:36.926669: step 14378, loss 0.596306.
Train: 2018-08-05T19:21:37.129748: step 14379, loss 0.562357.
Train: 2018-08-05T19:21:37.348414: step 14380, loss 0.553924.
Test: 2018-08-05T19:21:38.395075: step 14380, loss 0.548299.
Train: 2018-08-05T19:21:38.598123: step 14381, loss 0.537093.
Train: 2018-08-05T19:21:38.816852: step 14382, loss 0.570799.
Train: 2018-08-05T19:21:39.019929: step 14383, loss 0.537104.
Train: 2018-08-05T19:21:39.223006: step 14384, loss 0.553935.
Train: 2018-08-05T19:21:39.426083: step 14385, loss 0.613055.
Train: 2018-08-05T19:21:39.644782: step 14386, loss 0.596125.
Train: 2018-08-05T19:21:39.847859: step 14387, loss 0.469789.
Train: 2018-08-05T19:21:40.050937: step 14388, loss 0.587689.
Train: 2018-08-05T19:21:40.285228: step 14389, loss 0.638431.
Train: 2018-08-05T19:21:40.488334: step 14390, loss 0.553943.
Test: 2018-08-05T19:21:41.550586: step 14390, loss 0.548718.
Train: 2018-08-05T19:21:41.753665: step 14391, loss 0.579204.
Train: 2018-08-05T19:21:41.956711: step 14392, loss 0.587565.
Train: 2018-08-05T19:21:42.175440: step 14393, loss 0.554047.
Train: 2018-08-05T19:21:42.378487: step 14394, loss 0.529051.
Train: 2018-08-05T19:21:42.581595: step 14395, loss 0.545737.
Train: 2018-08-05T19:21:42.784672: step 14396, loss 0.604183.
Train: 2018-08-05T19:21:43.003372: step 14397, loss 0.562422.
Train: 2018-08-05T19:21:43.206450: step 14398, loss 0.529063.
Train: 2018-08-05T19:21:43.425147: step 14399, loss 0.562416.
Train: 2018-08-05T19:21:43.628226: step 14400, loss 0.604248.
Test: 2018-08-05T19:21:44.690456: step 14400, loss 0.54767.
Train: 2018-08-05T19:21:45.627758: step 14401, loss 0.562409.
Train: 2018-08-05T19:21:45.846455: step 14402, loss 0.537323.
Train: 2018-08-05T19:21:46.049532: step 14403, loss 0.537277.
Train: 2018-08-05T19:21:46.252611: step 14404, loss 0.503483.
Train: 2018-08-05T19:21:46.471279: step 14405, loss 0.562363.
Train: 2018-08-05T19:21:46.674387: step 14406, loss 0.580721.
Train: 2018-08-05T19:21:46.893086: step 14407, loss 0.553638.
Train: 2018-08-05T19:21:47.096162: step 14408, loss 0.545137.
Train: 2018-08-05T19:21:47.314861: step 14409, loss 0.59666.
Train: 2018-08-05T19:21:47.517939: step 14410, loss 0.630881.
Test: 2018-08-05T19:21:48.564568: step 14410, loss 0.546827.
Train: 2018-08-05T19:21:48.783267: step 14411, loss 0.47757.
Train: 2018-08-05T19:21:48.986344: step 14412, loss 0.502277.
Train: 2018-08-05T19:21:49.189422: step 14413, loss 0.553713.
Train: 2018-08-05T19:21:49.408091: step 14414, loss 0.562346.
Train: 2018-08-05T19:21:49.611200: step 14415, loss 0.632004.
Train: 2018-08-05T19:21:49.814275: step 14416, loss 0.579769.
Train: 2018-08-05T19:21:50.032974: step 14417, loss 0.518878.
Train: 2018-08-05T19:21:50.236052: step 14418, loss 0.571061.
Train: 2018-08-05T19:21:50.439130: step 14419, loss 0.57977.
Train: 2018-08-05T19:21:50.657828: step 14420, loss 0.518866.
Test: 2018-08-05T19:21:51.720051: step 14420, loss 0.549018.
Train: 2018-08-05T19:21:51.985646: step 14421, loss 0.518817.
Train: 2018-08-05T19:21:52.235583: step 14422, loss 0.579843.
Train: 2018-08-05T19:21:52.454282: step 14423, loss 0.527375.
Train: 2018-08-05T19:21:52.657331: step 14424, loss 0.474618.
Train: 2018-08-05T19:21:52.876029: step 14425, loss 0.50056.
Train: 2018-08-05T19:21:53.079136: step 14426, loss 0.562508.
Train: 2018-08-05T19:21:53.297835: step 14427, loss 0.571574.
Train: 2018-08-05T19:21:53.500914: step 14428, loss 0.580734.
Train: 2018-08-05T19:21:53.719611: step 14429, loss 0.526409.
Train: 2018-08-05T19:21:53.922659: step 14430, loss 0.517202.
Test: 2018-08-05T19:21:54.984909: step 14430, loss 0.547964.
Train: 2018-08-05T19:21:55.188018: step 14431, loss 0.608644.
Train: 2018-08-05T19:21:55.406716: step 14432, loss 0.489462.
Train: 2018-08-05T19:21:55.625417: step 14433, loss 0.553725.
Train: 2018-08-05T19:21:55.828493: step 14434, loss 0.581494.
Train: 2018-08-05T19:21:56.047192: step 14435, loss 0.581536.
Train: 2018-08-05T19:21:56.265890: step 14436, loss 0.544509.
Train: 2018-08-05T19:21:56.484559: step 14437, loss 0.59995.
Train: 2018-08-05T19:21:56.703258: step 14438, loss 0.618164.
Train: 2018-08-05T19:21:56.921986: step 14439, loss 0.626807.
Train: 2018-08-05T19:21:57.140682: step 14440, loss 0.544569.
Test: 2018-08-05T19:21:58.183232: step 14440, loss 0.546761.
Train: 2018-08-05T19:21:58.401960: step 14441, loss 0.490852.
Train: 2018-08-05T19:21:58.620661: step 14442, loss 0.562507.
Train: 2018-08-05T19:21:58.823737: step 14443, loss 0.589079.
Train: 2018-08-05T19:21:59.042435: step 14444, loss 0.6153.
Train: 2018-08-05T19:21:59.245515: step 14445, loss 0.492497.
Train: 2018-08-05T19:21:59.464211: step 14446, loss 0.536276.
Train: 2018-08-05T19:21:59.667289: step 14447, loss 0.579676.
Train: 2018-08-05T19:21:59.885987: step 14448, loss 0.57097.
Train: 2018-08-05T19:22:00.089065: step 14449, loss 0.588122.
Train: 2018-08-05T19:22:00.307764: step 14450, loss 0.587989.
Test: 2018-08-05T19:22:01.354364: step 14450, loss 0.548348.
Train: 2018-08-05T19:22:01.573093: step 14451, loss 0.562346.
Train: 2018-08-05T19:22:01.776141: step 14452, loss 0.604624.
Train: 2018-08-05T19:22:01.994870: step 14453, loss 0.512035.
Train: 2018-08-05T19:22:02.197941: step 14454, loss 0.512225.
Train: 2018-08-05T19:22:02.416645: step 14455, loss 0.562409.
Train: 2018-08-05T19:22:02.635314: step 14456, loss 0.47039.
Train: 2018-08-05T19:22:02.838422: step 14457, loss 0.629673.
Train: 2018-08-05T19:22:03.057091: step 14458, loss 0.612916.
Train: 2018-08-05T19:22:03.275819: step 14459, loss 0.59603.
Train: 2018-08-05T19:22:03.478896: step 14460, loss 0.537227.
Test: 2018-08-05T19:22:04.541147: step 14460, loss 0.547437.
Train: 2018-08-05T19:22:04.744227: step 14461, loss 0.537256.
Train: 2018-08-05T19:22:04.962924: step 14462, loss 0.48691.
Train: 2018-08-05T19:22:05.181623: step 14463, loss 0.537083.
Train: 2018-08-05T19:22:05.384700: step 14464, loss 0.511466.
Train: 2018-08-05T19:22:05.603370: step 14465, loss 0.52813.
Train: 2018-08-05T19:22:05.822097: step 14466, loss 0.501892.
Train: 2018-08-05T19:22:06.025175: step 14467, loss 0.580145.
Train: 2018-08-05T19:22:06.243875: step 14468, loss 0.571273.
Train: 2018-08-05T19:22:06.446952: step 14469, loss 0.686635.
Train: 2018-08-05T19:22:06.665650: step 14470, loss 0.535863.
Test: 2018-08-05T19:22:07.712280: step 14470, loss 0.546833.
Train: 2018-08-05T19:22:07.915357: step 14471, loss 0.526988.
Train: 2018-08-05T19:22:08.134028: step 14472, loss 0.580238.
Train: 2018-08-05T19:22:08.352755: step 14473, loss 0.615779.
Train: 2018-08-05T19:22:08.571454: step 14474, loss 0.571306.
Train: 2018-08-05T19:22:08.790152: step 14475, loss 0.615364.
Train: 2018-08-05T19:22:08.993232: step 14476, loss 0.553626.
Train: 2018-08-05T19:22:09.211929: step 14477, loss 0.597122.
Train: 2018-08-05T19:22:09.430627: step 14478, loss 0.629776.
Train: 2018-08-05T19:22:09.633705: step 14479, loss 0.578922.
Train: 2018-08-05T19:22:09.836783: step 14480, loss 0.579838.
Test: 2018-08-05T19:22:10.899003: step 14480, loss 0.553115.
Train: 2018-08-05T19:22:11.102083: step 14481, loss 0.542659.
Train: 2018-08-05T19:22:11.320811: step 14482, loss 0.538891.
Train: 2018-08-05T19:22:11.523857: step 14483, loss 0.587934.
Train: 2018-08-05T19:22:11.742588: step 14484, loss 0.553729.
Train: 2018-08-05T19:22:11.945635: step 14485, loss 0.570955.
Train: 2018-08-05T19:22:12.148741: step 14486, loss 0.477363.
Train: 2018-08-05T19:22:12.367410: step 14487, loss 0.509606.
Train: 2018-08-05T19:22:12.570517: step 14488, loss 0.506524.
Train: 2018-08-05T19:22:12.773565: step 14489, loss 0.601779.
Train: 2018-08-05T19:22:12.992296: step 14490, loss 0.618404.
Test: 2018-08-05T19:22:14.038924: step 14490, loss 0.548279.
Train: 2018-08-05T19:22:14.241971: step 14491, loss 0.591448.
Train: 2018-08-05T19:22:14.460703: step 14492, loss 0.570918.
Train: 2018-08-05T19:22:14.663777: step 14493, loss 0.562447.
Train: 2018-08-05T19:22:14.882476: step 14494, loss 0.603817.
Train: 2018-08-05T19:22:15.085556: step 14495, loss 0.562555.
Train: 2018-08-05T19:22:15.273009: step 14496, loss 0.562617.
Train: 2018-08-05T19:22:15.491710: step 14497, loss 0.562672.
Train: 2018-08-05T19:22:15.710410: step 14498, loss 0.554632.
Train: 2018-08-05T19:22:15.913485: step 14499, loss 0.514342.
Train: 2018-08-05T19:22:16.116531: step 14500, loss 0.530416.
Test: 2018-08-05T19:22:17.178784: step 14500, loss 0.548022.
Train: 2018-08-05T19:22:18.147336: step 14501, loss 0.554575.
Train: 2018-08-05T19:22:18.366037: step 14502, loss 0.570777.
Train: 2018-08-05T19:22:18.584734: step 14503, loss 0.611657.
Train: 2018-08-05T19:22:18.803403: step 14504, loss 0.562575.
Train: 2018-08-05T19:22:19.006510: step 14505, loss 0.595374.
Train: 2018-08-05T19:22:19.209587: step 14506, loss 0.546142.
Train: 2018-08-05T19:22:19.428286: step 14507, loss 0.587198.
Train: 2018-08-05T19:22:19.646985: step 14508, loss 0.504964.
Train: 2018-08-05T19:22:19.850065: step 14509, loss 0.537722.
Train: 2018-08-05T19:22:20.068762: step 14510, loss 0.579063.
Test: 2018-08-05T19:22:21.115362: step 14510, loss 0.549072.
Train: 2018-08-05T19:22:21.318439: step 14511, loss 0.537403.
Train: 2018-08-05T19:22:21.537168: step 14512, loss 0.478517.
Train: 2018-08-05T19:22:21.740247: step 14513, loss 0.587766.
Train: 2018-08-05T19:22:21.943292: step 14514, loss 0.494062.
Train: 2018-08-05T19:22:22.162021: step 14515, loss 0.58821.
Train: 2018-08-05T19:22:22.365100: step 14516, loss 0.501509.
Train: 2018-08-05T19:22:22.568176: step 14517, loss 0.615073.
Train: 2018-08-05T19:22:22.786846: step 14518, loss 0.553598.
Train: 2018-08-05T19:22:22.989954: step 14519, loss 0.606864.
Train: 2018-08-05T19:22:23.193030: step 14520, loss 0.571383.
Test: 2018-08-05T19:22:24.255252: step 14520, loss 0.547826.
Train: 2018-08-05T19:22:24.520844: step 14521, loss 0.544683.
Train: 2018-08-05T19:22:24.723924: step 14522, loss 0.642742.
Train: 2018-08-05T19:22:24.926999: step 14523, loss 0.553589.
Train: 2018-08-05T19:22:25.145698: step 14524, loss 0.580156.
Train: 2018-08-05T19:22:25.348745: step 14525, loss 0.650578.
Train: 2018-08-05T19:22:25.551824: step 14526, loss 0.518682.
Train: 2018-08-05T19:22:25.770552: step 14527, loss 0.536294.
Train: 2018-08-05T19:22:25.973629: step 14528, loss 0.579639.
Train: 2018-08-05T19:22:26.176706: step 14529, loss 0.579549.
Train: 2018-08-05T19:22:26.379783: step 14530, loss 0.536656.
Test: 2018-08-05T19:22:27.442005: step 14530, loss 0.548683.
Train: 2018-08-05T19:22:27.645112: step 14531, loss 0.596457.
Train: 2018-08-05T19:22:27.848161: step 14532, loss 0.553862.
Train: 2018-08-05T19:22:28.051267: step 14533, loss 0.638436.
Train: 2018-08-05T19:22:28.269936: step 14534, loss 0.52045.
Train: 2018-08-05T19:22:28.488666: step 14535, loss 0.554065.
Train: 2018-08-05T19:22:28.691742: step 14536, loss 0.604064.
Train: 2018-08-05T19:22:28.910441: step 14537, loss 0.570757.
Train: 2018-08-05T19:22:29.113520: step 14538, loss 0.513025.
Train: 2018-08-05T19:22:29.316566: step 14539, loss 0.595477.
Train: 2018-08-05T19:22:29.519674: step 14540, loss 0.587203.
Test: 2018-08-05T19:22:30.581895: step 14540, loss 0.548684.
Train: 2018-08-05T19:22:30.785002: step 14541, loss 0.619951.
Train: 2018-08-05T19:22:31.003701: step 14542, loss 0.611546.
Train: 2018-08-05T19:22:31.206778: step 14543, loss 0.497925.
Train: 2018-08-05T19:22:31.425477: step 14544, loss 0.578888.
Train: 2018-08-05T19:22:31.628525: step 14545, loss 0.506307.
Train: 2018-08-05T19:22:31.831633: step 14546, loss 0.611208.
Train: 2018-08-05T19:22:32.034709: step 14547, loss 0.627354.
Train: 2018-08-05T19:22:32.237757: step 14548, loss 0.619134.
Train: 2018-08-05T19:22:32.440864: step 14549, loss 0.586872.
Train: 2018-08-05T19:22:32.643942: step 14550, loss 0.570895.
Test: 2018-08-05T19:22:33.706193: step 14550, loss 0.550132.
Train: 2018-08-05T19:22:33.909271: step 14551, loss 0.602639.
Train: 2018-08-05T19:22:34.112350: step 14552, loss 0.5631.
Train: 2018-08-05T19:22:34.315425: step 14553, loss 0.618125.
Train: 2018-08-05T19:22:34.518502: step 14554, loss 0.500849.
Train: 2018-08-05T19:22:34.721549: step 14555, loss 0.508696.
Train: 2018-08-05T19:22:34.924658: step 14556, loss 0.531906.
Train: 2018-08-05T19:22:35.127706: step 14557, loss 0.56311.
Train: 2018-08-05T19:22:35.330813: step 14558, loss 0.53921.
Train: 2018-08-05T19:22:35.533890: step 14559, loss 0.514907.
Train: 2018-08-05T19:22:35.736969: step 14560, loss 0.586969.
Test: 2018-08-05T19:22:36.783568: step 14560, loss 0.549002.
Train: 2018-08-05T19:22:36.986676: step 14561, loss 0.554466.
Train: 2018-08-05T19:22:37.189722: step 14562, loss 0.529616.
Train: 2018-08-05T19:22:37.392829: step 14563, loss 0.570761.
Train: 2018-08-05T19:22:37.595877: step 14564, loss 0.528828.
Train: 2018-08-05T19:22:37.798955: step 14565, loss 0.477575.
Train: 2018-08-05T19:22:38.002062: step 14566, loss 0.579527.
Train: 2018-08-05T19:22:38.205110: step 14567, loss 0.640605.
Train: 2018-08-05T19:22:38.408211: step 14568, loss 0.55363.
Train: 2018-08-05T19:22:38.611296: step 14569, loss 0.439296.
Train: 2018-08-05T19:22:38.814372: step 14570, loss 0.651377.
Test: 2018-08-05T19:22:39.876593: step 14570, loss 0.548632.
Train: 2018-08-05T19:22:40.064079: step 14571, loss 0.491067.
Train: 2018-08-05T19:22:40.267156: step 14572, loss 0.607584.
Train: 2018-08-05T19:22:40.470203: step 14573, loss 0.598761.
Train: 2018-08-05T19:22:40.673282: step 14574, loss 0.562649.
Train: 2018-08-05T19:22:40.876359: step 14575, loss 0.526499.
Train: 2018-08-05T19:22:41.079469: step 14576, loss 0.553616.
Train: 2018-08-05T19:22:41.282544: step 14577, loss 0.64417.
Train: 2018-08-05T19:22:41.485592: step 14578, loss 0.535578.
Train: 2018-08-05T19:22:41.688698: step 14579, loss 0.58057.
Train: 2018-08-05T19:22:41.891777: step 14580, loss 0.51778.
Test: 2018-08-05T19:22:42.938406: step 14580, loss 0.546986.
Train: 2018-08-05T19:22:43.141454: step 14581, loss 0.508921.
Train: 2018-08-05T19:22:43.344560: step 14582, loss 0.580398.
Train: 2018-08-05T19:22:43.547633: step 14583, loss 0.535737.
Train: 2018-08-05T19:22:43.750715: step 14584, loss 0.464354.
Train: 2018-08-05T19:22:43.953793: step 14585, loss 0.661133.
Train: 2018-08-05T19:22:44.156870: step 14586, loss 0.580428.
Train: 2018-08-05T19:22:44.359952: step 14587, loss 0.571425.
Train: 2018-08-05T19:22:44.563026: step 14588, loss 0.491394.
Train: 2018-08-05T19:22:44.766073: step 14589, loss 0.606889.
Train: 2018-08-05T19:22:44.953559: step 14590, loss 0.589021.
Test: 2018-08-05T19:22:46.015810: step 14590, loss 0.547287.
Train: 2018-08-05T19:22:46.203266: step 14591, loss 0.491878.
Train: 2018-08-05T19:22:46.406344: step 14592, loss 0.580036.
Train: 2018-08-05T19:22:46.609423: step 14593, loss 0.571196.
Train: 2018-08-05T19:22:46.796878: step 14594, loss 0.527305.
Train: 2018-08-05T19:22:46.999955: step 14595, loss 0.685077.
Train: 2018-08-05T19:22:47.203033: step 14596, loss 0.501464.
Train: 2018-08-05T19:22:47.406105: step 14597, loss 0.657674.
Train: 2018-08-05T19:22:47.609189: step 14598, loss 0.596705.
Train: 2018-08-05T19:22:47.812265: step 14599, loss 0.562343.
Train: 2018-08-05T19:22:48.015342: step 14600, loss 0.520184.
Test: 2018-08-05T19:22:49.061975: step 14600, loss 0.548377.
Train: 2018-08-05T19:22:50.014873: step 14601, loss 0.528826.
Train: 2018-08-05T19:22:50.217951: step 14602, loss 0.54567.
Train: 2018-08-05T19:22:50.421029: step 14603, loss 0.579134.
Train: 2018-08-05T19:22:50.608455: step 14604, loss 0.520707.
Train: 2018-08-05T19:22:50.811563: step 14605, loss 0.579188.
Train: 2018-08-05T19:22:51.014639: step 14606, loss 0.554006.
Train: 2018-08-05T19:22:51.202096: step 14607, loss 0.545455.
Train: 2018-08-05T19:22:51.405142: step 14608, loss 0.509695.
Train: 2018-08-05T19:22:51.608221: step 14609, loss 0.60309.
Train: 2018-08-05T19:22:51.795676: step 14610, loss 0.526038.
Test: 2018-08-05T19:22:52.857960: step 14610, loss 0.546307.
Train: 2018-08-05T19:22:53.061036: step 14611, loss 0.571689.
Train: 2018-08-05T19:22:53.264083: step 14612, loss 0.579991.
Train: 2018-08-05T19:22:53.451569: step 14613, loss 0.570898.
Train: 2018-08-05T19:22:53.654647: step 14614, loss 0.638118.
Train: 2018-08-05T19:22:53.857726: step 14615, loss 0.520841.
Train: 2018-08-05T19:22:54.045150: step 14616, loss 0.570759.
Train: 2018-08-05T19:22:54.248227: step 14617, loss 0.512797.
Train: 2018-08-05T19:22:54.435714: step 14618, loss 0.512747.
Train: 2018-08-05T19:22:54.638791: step 14619, loss 0.520845.
Train: 2018-08-05T19:22:54.841868: step 14620, loss 0.487075.
Test: 2018-08-05T19:22:55.888469: step 14620, loss 0.547273.
Train: 2018-08-05T19:22:56.091546: step 14621, loss 0.553909.
Train: 2018-08-05T19:22:56.279003: step 14622, loss 0.553808.
Train: 2018-08-05T19:22:56.482080: step 14623, loss 0.545122.
Train: 2018-08-05T19:22:56.685188: step 14624, loss 0.536303.
Train: 2018-08-05T19:22:56.888264: step 14625, loss 0.544862.
Train: 2018-08-05T19:22:57.075720: step 14626, loss 0.677314.
Train: 2018-08-05T19:22:57.278768: step 14627, loss 0.535895.
Train: 2018-08-05T19:22:57.466254: step 14628, loss 0.589068.
Train: 2018-08-05T19:22:57.669302: step 14629, loss 0.562462.
Train: 2018-08-05T19:22:57.856757: step 14630, loss 0.606802.
Test: 2018-08-05T19:22:58.919008: step 14630, loss 0.547264.
Train: 2018-08-05T19:22:59.106465: step 14631, loss 0.544754.
Train: 2018-08-05T19:22:59.309572: step 14632, loss 0.527133.
Train: 2018-08-05T19:22:59.497028: step 14633, loss 0.518331.
Train: 2018-08-05T19:22:59.700105: step 14634, loss 0.544767.
Train: 2018-08-05T19:22:59.903183: step 14635, loss 0.527019.
Train: 2018-08-05T19:23:00.106262: step 14636, loss 0.562694.
Train: 2018-08-05T19:23:00.293687: step 14637, loss 0.580505.
Train: 2018-08-05T19:23:00.496796: step 14638, loss 0.491353.
Train: 2018-08-05T19:23:00.684220: step 14639, loss 0.526826.
Train: 2018-08-05T19:23:00.887328: step 14640, loss 0.490876.
Test: 2018-08-05T19:23:01.949579: step 14640, loss 0.547976.
Train: 2018-08-05T19:23:02.137035: step 14641, loss 0.607737.
Train: 2018-08-05T19:23:02.340083: step 14642, loss 0.481205.
Train: 2018-08-05T19:23:02.543190: step 14643, loss 0.644738.
Train: 2018-08-05T19:23:02.746268: step 14644, loss 0.626583.
Train: 2018-08-05T19:23:02.933724: step 14645, loss 0.671764.
Train: 2018-08-05T19:23:03.136802: step 14646, loss 0.589618.
Train: 2018-08-05T19:23:03.308639: step 14647, loss 0.657564.
Train: 2018-08-05T19:23:03.496092: step 14648, loss 0.606211.
Train: 2018-08-05T19:23:03.699140: step 14649, loss 0.579159.
Train: 2018-08-05T19:23:03.902247: step 14650, loss 0.511622.
Test: 2018-08-05T19:23:04.948846: step 14650, loss 0.548918.
Train: 2018-08-05T19:23:05.151955: step 14651, loss 0.546828.
Train: 2018-08-05T19:23:05.339410: step 14652, loss 0.604.
Train: 2018-08-05T19:23:05.542458: step 14653, loss 0.537501.
Train: 2018-08-05T19:23:05.729946: step 14654, loss 0.612096.
Train: 2018-08-05T19:23:05.933021: step 14655, loss 0.595383.
Train: 2018-08-05T19:23:06.120477: step 14656, loss 0.595205.
Train: 2018-08-05T19:23:06.323525: step 14657, loss 0.52237.
Train: 2018-08-05T19:23:06.526603: step 14658, loss 0.602978.
Train: 2018-08-05T19:23:06.714059: step 14659, loss 0.498972.
Train: 2018-08-05T19:23:06.901545: step 14660, loss 0.554914.
Test: 2018-08-05T19:23:07.963796: step 14660, loss 0.549184.
Train: 2018-08-05T19:23:08.151252: step 14661, loss 0.578861.
Train: 2018-08-05T19:23:08.369952: step 14662, loss 0.546907.
Train: 2018-08-05T19:23:08.573029: step 14663, loss 0.546847.
Train: 2018-08-05T19:23:08.776105: step 14664, loss 0.667221.
Train: 2018-08-05T19:23:08.963562: step 14665, loss 0.562836.
Train: 2018-08-05T19:23:09.151017: step 14666, loss 0.562852.
Train: 2018-08-05T19:23:09.354097: step 14667, loss 0.522833.
Train: 2018-08-05T19:23:09.541552: step 14668, loss 0.546762.
Train: 2018-08-05T19:23:09.729009: step 14669, loss 0.603059.
Train: 2018-08-05T19:23:09.932085: step 14670, loss 0.562732.
Test: 2018-08-05T19:23:10.978716: step 14670, loss 0.547498.
Train: 2018-08-05T19:23:11.181763: step 14671, loss 0.473621.
Train: 2018-08-05T19:23:11.369248: step 14672, loss 0.627906.
Train: 2018-08-05T19:23:11.556706: step 14673, loss 0.587152.
Train: 2018-08-05T19:23:11.759781: step 14674, loss 0.562542.
Train: 2018-08-05T19:23:11.947238: step 14675, loss 0.455382.
Train: 2018-08-05T19:23:12.150317: step 14676, loss 0.51256.
Train: 2018-08-05T19:23:12.337771: step 14677, loss 0.621223.
Train: 2018-08-05T19:23:12.540849: step 14678, loss 0.596211.
Train: 2018-08-05T19:23:12.728307: step 14679, loss 0.502837.
Train: 2018-08-05T19:23:12.931385: step 14680, loss 0.553773.
Test: 2018-08-05T19:23:13.978014: step 14680, loss 0.547938.
Train: 2018-08-05T19:23:14.181084: step 14681, loss 0.545094.
Train: 2018-08-05T19:23:14.368546: step 14682, loss 0.605759.
Train: 2018-08-05T19:23:14.556003: step 14683, loss 0.614651.
Train: 2018-08-05T19:23:14.759074: step 14684, loss 0.605958.
Train: 2018-08-05T19:23:14.946536: step 14685, loss 0.614562.
Train: 2018-08-05T19:23:15.133992: step 14686, loss 0.545025.
Train: 2018-08-05T19:23:15.337069: step 14687, loss 0.510568.
Train: 2018-08-05T19:23:15.524525: step 14688, loss 0.605451.
Train: 2018-08-05T19:23:15.711981: step 14689, loss 0.596725.
Train: 2018-08-05T19:23:15.915029: step 14690, loss 0.656479.
Test: 2018-08-05T19:23:16.961689: step 14690, loss 0.547601.
Train: 2018-08-05T19:23:17.164783: step 14691, loss 0.562351.
Train: 2018-08-05T19:23:17.367839: step 14692, loss 0.663331.
Train: 2018-08-05T19:23:17.555270: step 14693, loss 0.529221.
Train: 2018-08-05T19:23:17.758377: step 14694, loss 0.562526.
Train: 2018-08-05T19:23:17.961454: step 14695, loss 0.529938.
Train: 2018-08-05T19:23:18.148911: step 14696, loss 0.55453.
Train: 2018-08-05T19:23:18.351990: step 14697, loss 0.586997.
Train: 2018-08-05T19:23:18.539413: step 14698, loss 0.506252.
Train: 2018-08-05T19:23:18.726902: step 14699, loss 0.578885.
Train: 2018-08-05T19:23:18.929977: step 14700, loss 0.611185.
Test: 2018-08-05T19:23:19.976578: step 14700, loss 0.549856.
Train: 2018-08-05T19:23:20.960752: step 14701, loss 0.643357.
Train: 2018-08-05T19:23:21.148208: step 14702, loss 0.530774.
Train: 2018-08-05T19:23:21.335664: step 14703, loss 0.594854.
Train: 2018-08-05T19:23:21.523121: step 14704, loss 0.5151.
Train: 2018-08-05T19:23:21.726200: step 14705, loss 0.57886.
Train: 2018-08-05T19:23:21.913654: step 14706, loss 0.530992.
Train: 2018-08-05T19:23:22.101081: step 14707, loss 0.546857.
Train: 2018-08-05T19:23:22.304181: step 14708, loss 0.514582.
Train: 2018-08-05T19:23:22.491645: step 14709, loss 0.473661.
Train: 2018-08-05T19:23:22.679100: step 14710, loss 0.546185.
Test: 2018-08-05T19:23:23.741321: step 14710, loss 0.548233.
Train: 2018-08-05T19:23:23.928807: step 14711, loss 0.579051.
Train: 2018-08-05T19:23:24.131885: step 14712, loss 0.595915.
Train: 2018-08-05T19:23:24.319311: step 14713, loss 0.537034.
Train: 2018-08-05T19:23:24.522418: step 14714, loss 0.604916.
Train: 2018-08-05T19:23:24.709876: step 14715, loss 0.570896.
Train: 2018-08-05T19:23:24.897330: step 14716, loss 0.553738.
Train: 2018-08-05T19:23:25.100378: step 14717, loss 0.570972.
Train: 2018-08-05T19:23:25.287864: step 14718, loss 0.562344.
Train: 2018-08-05T19:23:25.475321: step 14719, loss 0.571036.
Train: 2018-08-05T19:23:25.662746: step 14720, loss 0.571057.
Test: 2018-08-05T19:23:26.725029: step 14720, loss 0.548216.
Train: 2018-08-05T19:23:26.912453: step 14721, loss 0.501397.
Train: 2018-08-05T19:23:27.115530: step 14722, loss 0.562374.
Train: 2018-08-05T19:23:27.302987: step 14723, loss 0.562388.
Train: 2018-08-05T19:23:27.490474: step 14724, loss 0.579984.
Train: 2018-08-05T19:23:27.693520: step 14725, loss 0.571207.
Train: 2018-08-05T19:23:27.881007: step 14726, loss 0.606411.
Train: 2018-08-05T19:23:28.084078: step 14727, loss 0.562393.
Train: 2018-08-05T19:23:28.271511: step 14728, loss 0.492338.
Train: 2018-08-05T19:23:28.458967: step 14729, loss 0.553621.
Train: 2018-08-05T19:23:28.662074: step 14730, loss 0.500982.
Test: 2018-08-05T19:23:29.724295: step 14730, loss 0.546691.
Train: 2018-08-05T19:23:29.911781: step 14731, loss 0.615243.
Train: 2018-08-05T19:23:30.099239: step 14732, loss 0.474332.
Train: 2018-08-05T19:23:30.302315: step 14733, loss 0.642064.
Train: 2018-08-05T19:23:30.489741: step 14734, loss 0.518226.
Train: 2018-08-05T19:23:30.692819: step 14735, loss 0.571302.
Train: 2018-08-05T19:23:30.880274: step 14736, loss 0.580165.
Train: 2018-08-05T19:23:31.067761: step 14737, loss 0.527049.
Train: 2018-08-05T19:23:31.255187: step 14738, loss 0.527036.
Train: 2018-08-05T19:23:31.458294: step 14739, loss 0.624538.
Train: 2018-08-05T19:23:31.645721: step 14740, loss 0.491639.
Test: 2018-08-05T19:23:32.692350: step 14740, loss 0.548469.
Train: 2018-08-05T19:23:32.895457: step 14741, loss 0.677673.
Train: 2018-08-05T19:23:33.082916: step 14742, loss 0.580056.
Train: 2018-08-05T19:23:33.285992: step 14743, loss 0.483485.
Train: 2018-08-05T19:23:33.473418: step 14744, loss 0.58864.
Train: 2018-08-05T19:23:33.692116: step 14745, loss 0.544913.
Train: 2018-08-05T19:23:33.879602: step 14746, loss 0.614613.
Train: 2018-08-05T19:23:34.082679: step 14747, loss 0.571013.
Train: 2018-08-05T19:23:34.270106: step 14748, loss 0.639958.
Train: 2018-08-05T19:23:34.473183: step 14749, loss 0.545236.
Train: 2018-08-05T19:23:34.660669: step 14750, loss 0.536873.
Test: 2018-08-05T19:23:35.707298: step 14750, loss 0.548054.
Train: 2018-08-05T19:23:35.910370: step 14751, loss 0.53701.
Train: 2018-08-05T19:23:36.097834: step 14752, loss 0.596079.
Train: 2018-08-05T19:23:36.300880: step 14753, loss 0.520437.
Train: 2018-08-05T19:23:36.488366: step 14754, loss 0.621055.
Train: 2018-08-05T19:23:36.691445: step 14755, loss 0.562422.
Train: 2018-08-05T19:23:36.894521: step 14756, loss 0.512549.
Train: 2018-08-05T19:23:37.097601: step 14757, loss 0.620651.
Train: 2018-08-05T19:23:37.285049: step 14758, loss 0.521013.
Train: 2018-08-05T19:23:37.488132: step 14759, loss 0.554177.
Train: 2018-08-05T19:23:37.691180: step 14760, loss 0.662022.
Test: 2018-08-05T19:23:38.737839: step 14760, loss 0.54889.
Train: 2018-08-05T19:23:38.940887: step 14761, loss 0.603799.
Train: 2018-08-05T19:23:39.128343: step 14762, loss 0.529695.
Train: 2018-08-05T19:23:39.315829: step 14763, loss 0.53801.
Train: 2018-08-05T19:23:39.518906: step 14764, loss 0.554402.
Train: 2018-08-05T19:23:39.706333: step 14765, loss 0.546216.
Train: 2018-08-05T19:23:39.909440: step 14766, loss 0.603548.
Train: 2018-08-05T19:23:40.096898: step 14767, loss 0.570763.
Train: 2018-08-05T19:23:40.299943: step 14768, loss 0.578955.
Train: 2018-08-05T19:23:40.503051: step 14769, loss 0.546208.
Train: 2018-08-05T19:23:40.690507: step 14770, loss 0.546186.
Test: 2018-08-05T19:23:41.752740: step 14770, loss 0.54884.
Train: 2018-08-05T19:23:41.955806: step 14771, loss 0.57076.
Train: 2018-08-05T19:23:42.158883: step 14772, loss 0.537857.
Train: 2018-08-05T19:23:42.346340: step 14773, loss 0.579011.
Train: 2018-08-05T19:23:42.533825: step 14774, loss 0.587309.
Train: 2018-08-05T19:23:42.736903: step 14775, loss 0.537606.
Train: 2018-08-05T19:23:42.924359: step 14776, loss 0.529197.
Train: 2018-08-05T19:23:43.111815: step 14777, loss 0.545712.
Train: 2018-08-05T19:23:43.314863: step 14778, loss 0.562387.
Train: 2018-08-05T19:23:43.517970: step 14779, loss 0.562366.
Train: 2018-08-05T19:23:43.705426: step 14780, loss 0.553869.
Test: 2018-08-05T19:23:44.767647: step 14780, loss 0.5485.
Train: 2018-08-05T19:23:44.970756: step 14781, loss 0.51973.
Train: 2018-08-05T19:23:45.158211: step 14782, loss 0.545177.
Train: 2018-08-05T19:23:45.361290: step 14783, loss 0.519126.
Train: 2018-08-05T19:23:45.564366: step 14784, loss 0.553593.
Train: 2018-08-05T19:23:45.751792: step 14785, loss 0.636796.
Train: 2018-08-05T19:23:45.939278: step 14786, loss 0.580248.
Train: 2018-08-05T19:23:46.142355: step 14787, loss 0.456897.
Train: 2018-08-05T19:23:46.329811: step 14788, loss 0.527077.
Train: 2018-08-05T19:23:46.532883: step 14789, loss 0.473531.
Train: 2018-08-05T19:23:46.720315: step 14790, loss 0.57156.
Test: 2018-08-05T19:23:47.782566: step 14790, loss 0.546927.
Train: 2018-08-05T19:23:47.970054: step 14791, loss 0.508362.
Train: 2018-08-05T19:23:48.173131: step 14792, loss 0.590189.
Train: 2018-08-05T19:23:48.360556: step 14793, loss 0.516959.
Train: 2018-08-05T19:23:48.563634: step 14794, loss 0.498277.
Train: 2018-08-05T19:23:48.751090: step 14795, loss 0.563163.
Train: 2018-08-05T19:23:48.954197: step 14796, loss 0.582064.
Train: 2018-08-05T19:23:49.141623: step 14797, loss 0.535134.
Train: 2018-08-05T19:23:49.313458: step 14798, loss 0.503002.
Train: 2018-08-05T19:23:49.500914: step 14799, loss 0.506628.
Train: 2018-08-05T19:23:49.704021: step 14800, loss 0.535092.
Test: 2018-08-05T19:23:50.750651: step 14800, loss 0.548958.
Train: 2018-08-05T19:23:51.656689: step 14801, loss 0.525491.
Train: 2018-08-05T19:23:51.859736: step 14802, loss 0.612275.
Train: 2018-08-05T19:23:52.047223: step 14803, loss 0.573694.
Train: 2018-08-05T19:23:52.250300: step 14804, loss 0.582849.
Train: 2018-08-05T19:23:52.453377: step 14805, loss 0.617092.
Train: 2018-08-05T19:23:52.640836: step 14806, loss 0.554677.
Train: 2018-08-05T19:23:52.843881: step 14807, loss 0.556337.
Train: 2018-08-05T19:23:53.046988: step 14808, loss 0.586938.
Train: 2018-08-05T19:23:53.234445: step 14809, loss 0.537813.
Train: 2018-08-05T19:23:53.437493: step 14810, loss 0.571067.
Test: 2018-08-05T19:23:54.484152: step 14810, loss 0.547601.
Train: 2018-08-05T19:23:54.687229: step 14811, loss 0.544653.
Train: 2018-08-05T19:23:54.890307: step 14812, loss 0.644918.
Train: 2018-08-05T19:23:55.093386: step 14813, loss 0.607525.
Train: 2018-08-05T19:23:55.280840: step 14814, loss 0.562347.
Train: 2018-08-05T19:23:55.483918: step 14815, loss 0.612204.
Train: 2018-08-05T19:23:55.671374: step 14816, loss 0.542058.
Train: 2018-08-05T19:23:55.874452: step 14817, loss 0.556333.
Train: 2018-08-05T19:23:56.061902: step 14818, loss 0.506225.
Train: 2018-08-05T19:23:56.264985: step 14819, loss 0.623747.
Train: 2018-08-05T19:23:56.468035: step 14820, loss 0.651948.
Test: 2018-08-05T19:23:57.514663: step 14820, loss 0.548065.
Train: 2018-08-05T19:23:57.717771: step 14821, loss 0.535877.
Train: 2018-08-05T19:23:57.905226: step 14822, loss 0.571289.
Train: 2018-08-05T19:23:58.108273: step 14823, loss 0.571213.
Train: 2018-08-05T19:23:58.311380: step 14824, loss 0.597392.
Train: 2018-08-05T19:23:58.498807: step 14825, loss 0.553667.
Train: 2018-08-05T19:23:58.701916: step 14826, loss 0.579592.
Train: 2018-08-05T19:23:58.904992: step 14827, loss 0.59661.
Train: 2018-08-05T19:23:59.108069: step 14828, loss 0.60485.
Train: 2018-08-05T19:23:59.311149: step 14829, loss 0.528709.
Train: 2018-08-05T19:23:59.498572: step 14830, loss 0.595888.
Test: 2018-08-05T19:24:00.560824: step 14830, loss 0.548952.
Train: 2018-08-05T19:24:00.748311: step 14831, loss 0.57076.
Train: 2018-08-05T19:24:00.951358: step 14832, loss 0.58727.
Train: 2018-08-05T19:24:01.138845: step 14833, loss 0.505183.
Train: 2018-08-05T19:24:01.341891: step 14834, loss 0.570764.
Train: 2018-08-05T19:24:01.544998: step 14835, loss 0.505262.
Train: 2018-08-05T19:24:01.748075: step 14836, loss 0.611971.
Train: 2018-08-05T19:24:01.951153: step 14837, loss 0.46341.
Train: 2018-08-05T19:24:02.138579: step 14838, loss 0.48685.
Train: 2018-08-05T19:24:02.341656: step 14839, loss 0.579703.
Train: 2018-08-05T19:24:02.544765: step 14840, loss 0.535745.
Test: 2018-08-05T19:24:03.607015: step 14840, loss 0.547345.
Train: 2018-08-05T19:24:03.810093: step 14841, loss 0.526027.
Train: 2018-08-05T19:24:04.013141: step 14842, loss 0.631798.
Train: 2018-08-05T19:24:04.216250: step 14843, loss 0.506681.
Train: 2018-08-05T19:24:04.403674: step 14844, loss 0.535105.
Train: 2018-08-05T19:24:04.606781: step 14845, loss 0.573132.
Train: 2018-08-05T19:24:04.809859: step 14846, loss 0.525761.
Train: 2018-08-05T19:24:05.012936: step 14847, loss 0.553859.
Train: 2018-08-05T19:24:05.216015: step 14848, loss 0.553761.
Train: 2018-08-05T19:24:05.419091: step 14849, loss 0.654317.
Train: 2018-08-05T19:24:05.606547: step 14850, loss 0.650611.
Test: 2018-08-05T19:24:06.668769: step 14850, loss 0.549004.
Train: 2018-08-05T19:24:06.871876: step 14851, loss 0.562357.
Train: 2018-08-05T19:24:07.074955: step 14852, loss 0.603806.
Train: 2018-08-05T19:24:07.278031: step 14853, loss 0.636134.
Train: 2018-08-05T19:24:07.481078: step 14854, loss 0.562765.
Train: 2018-08-05T19:24:07.668565: step 14855, loss 0.554978.
Train: 2018-08-05T19:24:07.871642: step 14856, loss 0.531549.
Train: 2018-08-05T19:24:08.074719: step 14857, loss 0.58672.
Train: 2018-08-05T19:24:08.277797: step 14858, loss 0.547667.
Train: 2018-08-05T19:24:08.480844: step 14859, loss 0.516586.
Train: 2018-08-05T19:24:08.683951: step 14860, loss 0.610125.
Test: 2018-08-05T19:24:09.730582: step 14860, loss 0.549856.
Train: 2018-08-05T19:24:09.933628: step 14861, loss 0.547638.
Train: 2018-08-05T19:24:10.136740: step 14862, loss 0.602375.
Train: 2018-08-05T19:24:10.339784: step 14863, loss 0.547521.
Train: 2018-08-05T19:24:10.542891: step 14864, loss 0.547424.
Train: 2018-08-05T19:24:10.730349: step 14865, loss 0.55517.
Train: 2018-08-05T19:24:10.933395: step 14866, loss 0.586799.
Train: 2018-08-05T19:24:11.152145: step 14867, loss 0.602787.
Train: 2018-08-05T19:24:11.339550: step 14868, loss 0.578862.
Train: 2018-08-05T19:24:11.542658: step 14869, loss 0.594884.
Train: 2018-08-05T19:24:11.745735: step 14870, loss 0.570852.
Test: 2018-08-05T19:24:12.792335: step 14870, loss 0.549432.
Train: 2018-08-05T19:24:12.995412: step 14871, loss 0.667091.
Train: 2018-08-05T19:24:13.214111: step 14872, loss 0.499038.
Train: 2018-08-05T19:24:13.417218: step 14873, loss 0.51498.
Train: 2018-08-05T19:24:13.604674: step 14874, loss 0.618951.
Train: 2018-08-05T19:24:13.807752: step 14875, loss 0.538744.
Train: 2018-08-05T19:24:14.010830: step 14876, loss 0.562776.
Train: 2018-08-05T19:24:14.213908: step 14877, loss 0.522336.
Train: 2018-08-05T19:24:14.416984: step 14878, loss 0.578913.
Train: 2018-08-05T19:24:14.620061: step 14879, loss 0.538072.
Train: 2018-08-05T19:24:14.823109: step 14880, loss 0.587219.
Test: 2018-08-05T19:24:15.885361: step 14880, loss 0.549806.
Train: 2018-08-05T19:24:16.088470: step 14881, loss 0.554208.
Train: 2018-08-05T19:24:16.291545: step 14882, loss 0.562441.
Train: 2018-08-05T19:24:16.494622: step 14883, loss 0.545672.
Train: 2018-08-05T19:24:16.697700: step 14884, loss 0.537113.
Train: 2018-08-05T19:24:16.900777: step 14885, loss 0.570835.
Train: 2018-08-05T19:24:17.103855: step 14886, loss 0.536716.
Train: 2018-08-05T19:24:17.322553: step 14887, loss 0.631184.
Train: 2018-08-05T19:24:17.510010: step 14888, loss 0.588204.
Train: 2018-08-05T19:24:17.728709: step 14889, loss 0.596817.
Train: 2018-08-05T19:24:17.931785: step 14890, loss 0.467773.
Test: 2018-08-05T19:24:18.978418: step 14890, loss 0.547727.
Train: 2018-08-05T19:24:19.228355: step 14891, loss 0.51919.
Train: 2018-08-05T19:24:19.431435: step 14892, loss 0.544983.
Train: 2018-08-05T19:24:19.634512: step 14893, loss 0.53615.
Train: 2018-08-05T19:24:19.837591: step 14894, loss 0.527185.
Train: 2018-08-05T19:24:20.056288: step 14895, loss 0.571359.
Train: 2018-08-05T19:24:20.259366: step 14896, loss 0.49098.
Train: 2018-08-05T19:24:20.462438: step 14897, loss 0.544572.
Train: 2018-08-05T19:24:20.665521: step 14898, loss 0.608461.
Train: 2018-08-05T19:24:20.884190: step 14899, loss 0.507839.
Train: 2018-08-05T19:24:21.087268: step 14900, loss 0.553741.
Test: 2018-08-05T19:24:22.149518: step 14900, loss 0.54629.
Train: 2018-08-05T19:24:23.118042: step 14901, loss 0.590912.
Train: 2018-08-05T19:24:23.321120: step 14902, loss 0.572367.
Train: 2018-08-05T19:24:23.524227: step 14903, loss 0.637175.
Train: 2018-08-05T19:24:23.742925: step 14904, loss 0.507785.
Train: 2018-08-05T19:24:23.946003: step 14905, loss 0.535392.
Train: 2018-08-05T19:24:24.149051: step 14906, loss 0.562743.
Train: 2018-08-05T19:24:24.352158: step 14907, loss 0.499251.
Train: 2018-08-05T19:24:24.555235: step 14908, loss 0.472102.
Train: 2018-08-05T19:24:24.773933: step 14909, loss 0.444434.
Train: 2018-08-05T19:24:24.977011: step 14910, loss 0.489293.
Test: 2018-08-05T19:24:26.023611: step 14910, loss 0.547813.
Train: 2018-08-05T19:24:26.304826: step 14911, loss 0.544519.
Train: 2018-08-05T19:24:26.507873: step 14912, loss 0.62018.
Train: 2018-08-05T19:24:26.726601: step 14913, loss 0.544602.
Train: 2018-08-05T19:24:26.929679: step 14914, loss 0.611406.
Train: 2018-08-05T19:24:27.132756: step 14915, loss 0.630302.
Train: 2018-08-05T19:24:27.351474: step 14916, loss 0.554006.
Train: 2018-08-05T19:24:27.554532: step 14917, loss 0.694348.
Train: 2018-08-05T19:24:27.757610: step 14918, loss 0.572135.
Train: 2018-08-05T19:24:27.960689: step 14919, loss 0.544561.
Train: 2018-08-05T19:24:28.179386: step 14920, loss 0.48211.
Test: 2018-08-05T19:24:29.226016: step 14920, loss 0.546838.
Train: 2018-08-05T19:24:29.429094: step 14921, loss 0.509283.
Train: 2018-08-05T19:24:29.632141: step 14922, loss 0.553601.
Train: 2018-08-05T19:24:29.850873: step 14923, loss 0.553612.
Train: 2018-08-05T19:24:30.053918: step 14924, loss 0.606157.
Train: 2018-08-05T19:24:30.272647: step 14925, loss 0.518823.
Train: 2018-08-05T19:24:30.475693: step 14926, loss 0.614447.
Train: 2018-08-05T19:24:30.678801: step 14927, loss 0.562339.
Train: 2018-08-05T19:24:30.881849: step 14928, loss 0.579524.
Train: 2018-08-05T19:24:31.084957: step 14929, loss 0.528141.
Train: 2018-08-05T19:24:31.303654: step 14930, loss 0.545293.
Test: 2018-08-05T19:24:32.350285: step 14930, loss 0.548524.
Train: 2018-08-05T19:24:32.568983: step 14931, loss 0.545325.
Train: 2018-08-05T19:24:32.772062: step 14932, loss 0.545335.
Train: 2018-08-05T19:24:32.975108: step 14933, loss 0.519797.
Train: 2018-08-05T19:24:33.193807: step 14934, loss 0.605061.
Train: 2018-08-05T19:24:33.396914: step 14935, loss 0.519667.
Train: 2018-08-05T19:24:33.599992: step 14936, loss 0.596559.
Train: 2018-08-05T19:24:33.818693: step 14937, loss 0.570895.
Train: 2018-08-05T19:24:34.021738: step 14938, loss 0.596571.
Train: 2018-08-05T19:24:34.224846: step 14939, loss 0.53671.
Train: 2018-08-05T19:24:34.443544: step 14940, loss 0.553798.
Test: 2018-08-05T19:24:35.490174: step 14940, loss 0.54827.
Train: 2018-08-05T19:24:35.708875: step 14941, loss 0.58796.
Train: 2018-08-05T19:24:35.911951: step 14942, loss 0.613521.
Train: 2018-08-05T19:24:36.146240: step 14943, loss 0.536854.
Train: 2018-08-05T19:24:36.349348: step 14944, loss 0.553871.
Train: 2018-08-05T19:24:36.552425: step 14945, loss 0.4946.
Train: 2018-08-05T19:24:36.771095: step 14946, loss 0.528382.
Train: 2018-08-05T19:24:36.989823: step 14947, loss 0.56234.
Train: 2018-08-05T19:24:37.192900: step 14948, loss 0.510969.
Train: 2018-08-05T19:24:37.364706: step 14949, loss 0.690982.
Train: 2018-08-05T19:24:37.583405: step 14950, loss 0.570938.
Test: 2018-08-05T19:24:38.630064: step 14950, loss 0.5476.
Train: 2018-08-05T19:24:38.848763: step 14951, loss 0.631025.
Train: 2018-08-05T19:24:39.051841: step 14952, loss 0.553798.
Train: 2018-08-05T19:24:39.254918: step 14953, loss 0.48583.
Train: 2018-08-05T19:24:39.473631: step 14954, loss 0.570848.
Train: 2018-08-05T19:24:39.676695: step 14955, loss 0.528339.
Train: 2018-08-05T19:24:39.895393: step 14956, loss 0.50273.
Train: 2018-08-05T19:24:40.098470: step 14957, loss 0.49388.
Train: 2018-08-05T19:24:40.317140: step 14958, loss 0.562338.
Train: 2018-08-05T19:24:40.520217: step 14959, loss 0.536291.
Train: 2018-08-05T19:24:40.738944: step 14960, loss 0.579884.
Test: 2018-08-05T19:24:41.785545: step 14960, loss 0.54852.
Train: 2018-08-05T19:24:42.004274: step 14961, loss 0.553607.
Train: 2018-08-05T19:24:42.222972: step 14962, loss 0.615515.
Train: 2018-08-05T19:24:42.441671: step 14963, loss 0.597874.
Train: 2018-08-05T19:24:42.644749: step 14964, loss 0.509472.
Train: 2018-08-05T19:24:42.879071: step 14965, loss 0.580168.
Train: 2018-08-05T19:24:43.097768: step 14966, loss 0.535879.
Train: 2018-08-05T19:24:43.316466: step 14967, loss 0.535864.
Train: 2018-08-05T19:24:43.519545: step 14968, loss 0.473684.
Train: 2018-08-05T19:24:43.738242: step 14969, loss 0.553589.
Train: 2018-08-05T19:24:43.941320: step 14970, loss 0.499744.
Test: 2018-08-05T19:24:45.003571: step 14970, loss 0.546931.
Train: 2018-08-05T19:24:45.206619: step 14971, loss 0.553613.
Train: 2018-08-05T19:24:45.425348: step 14972, loss 0.535444.
Train: 2018-08-05T19:24:45.628427: step 14973, loss 0.553677.
Train: 2018-08-05T19:24:45.831473: step 14974, loss 0.526103.
Train: 2018-08-05T19:24:46.050202: step 14975, loss 0.581537.
Train: 2018-08-05T19:24:46.253279: step 14976, loss 0.553796.
Train: 2018-08-05T19:24:46.471977: step 14977, loss 0.61896.
Train: 2018-08-05T19:24:46.675056: step 14978, loss 0.507368.
Train: 2018-08-05T19:24:46.893723: step 14979, loss 0.525941.
Train: 2018-08-05T19:24:47.096831: step 14980, loss 0.58168.
Test: 2018-08-05T19:24:48.159083: step 14980, loss 0.547572.
Train: 2018-08-05T19:24:48.362163: step 14981, loss 0.553789.
Train: 2018-08-05T19:24:48.580860: step 14982, loss 0.581557.
Train: 2018-08-05T19:24:48.783907: step 14983, loss 0.599876.
Train: 2018-08-05T19:24:49.002605: step 14984, loss 0.590367.
Train: 2018-08-05T19:24:49.205683: step 14985, loss 0.653704.
Train: 2018-08-05T19:24:49.424411: step 14986, loss 0.47276.
Train: 2018-08-05T19:24:49.627458: step 14987, loss 0.607083.
Train: 2018-08-05T19:24:49.846190: step 14988, loss 0.571261.
Train: 2018-08-05T19:24:50.049266: step 14989, loss 0.52738.
Train: 2018-08-05T19:24:50.267964: step 14990, loss 0.527595.
Test: 2018-08-05T19:24:51.314564: step 14990, loss 0.548493.
Train: 2018-08-05T19:24:51.517641: step 14991, loss 0.510443.
Train: 2018-08-05T19:24:51.736370: step 14992, loss 0.579613.
Train: 2018-08-05T19:24:51.939418: step 14993, loss 0.527873.
Train: 2018-08-05T19:24:52.158146: step 14994, loss 0.622622.
Train: 2018-08-05T19:24:52.361227: step 14995, loss 0.519431.
Train: 2018-08-05T19:24:52.579924: step 14996, loss 0.502335.
Train: 2018-08-05T19:24:52.782999: step 14997, loss 0.519386.
Train: 2018-08-05T19:24:53.001698: step 14998, loss 0.596838.
Train: 2018-08-05T19:24:53.204777: step 14999, loss 0.622823.
Train: 2018-08-05T19:24:53.423475: step 15000, loss 0.545086.
Test: 2018-08-05T19:24:54.470104: step 15000, loss 0.547347.
Train: 2018-08-05T19:24:55.423010: step 15001, loss 0.665771.
Train: 2018-08-05T19:24:55.626084: step 15002, loss 0.579469.
Train: 2018-08-05T19:24:55.844783: step 15003, loss 0.596388.
Train: 2018-08-05T19:24:56.047860: step 15004, loss 0.613043.
Train: 2018-08-05T19:24:56.266530: step 15005, loss 0.545672.
Train: 2018-08-05T19:24:56.469606: step 15006, loss 0.554148.
Train: 2018-08-05T19:24:56.688305: step 15007, loss 0.595527.
Train: 2018-08-05T19:24:56.891412: step 15008, loss 0.488746.
Train: 2018-08-05T19:24:57.110112: step 15009, loss 0.521631.
Train: 2018-08-05T19:24:57.328812: step 15010, loss 0.513364.
Test: 2018-08-05T19:24:58.386954: step 15010, loss 0.548957.
Train: 2018-08-05T19:24:58.605682: step 15011, loss 0.546051.
Train: 2018-08-05T19:24:58.824381: step 15012, loss 0.529364.
Train: 2018-08-05T19:24:59.027458: step 15013, loss 0.520753.
Train: 2018-08-05T19:24:59.230536: step 15014, loss 0.537159.
Train: 2018-08-05T19:24:59.449234: step 15015, loss 0.613266.
Train: 2018-08-05T19:24:59.652312: step 15016, loss 0.536734.
Train: 2018-08-05T19:24:59.870980: step 15017, loss 0.639648.
Train: 2018-08-05T19:25:00.074089: step 15018, loss 0.562336.
Train: 2018-08-05T19:25:00.292787: step 15019, loss 0.596807.
Train: 2018-08-05T19:25:00.511486: step 15020, loss 0.570949.
Test: 2018-08-05T19:25:01.558086: step 15020, loss 0.548561.
Train: 2018-08-05T19:25:01.761193: step 15021, loss 0.545129.
Train: 2018-08-05T19:25:01.979893: step 15022, loss 0.570938.
Train: 2018-08-05T19:25:02.182969: step 15023, loss 0.588124.
Train: 2018-08-05T19:25:02.401668: step 15024, loss 0.536599.
Train: 2018-08-05T19:25:02.604715: step 15025, loss 0.588058.
Train: 2018-08-05T19:25:02.807824: step 15026, loss 0.613684.
Train: 2018-08-05T19:25:03.026522: step 15027, loss 0.604915.
Train: 2018-08-05T19:25:03.229570: step 15028, loss 0.587457.
Train: 2018-08-05T19:25:03.448298: step 15029, loss 0.539976.
Train: 2018-08-05T19:25:03.651376: step 15030, loss 0.505663.
Test: 2018-08-05T19:25:04.713627: step 15030, loss 0.548013.
Train: 2018-08-05T19:25:04.916675: step 15031, loss 0.495342.
Train: 2018-08-05T19:25:05.135403: step 15032, loss 0.629722.
Train: 2018-08-05T19:25:05.338451: step 15033, loss 0.646578.
Train: 2018-08-05T19:25:05.557149: step 15034, loss 0.545623.
Train: 2018-08-05T19:25:05.760257: step 15035, loss 0.487127.
Train: 2018-08-05T19:25:05.963334: step 15036, loss 0.486968.
Train: 2018-08-05T19:25:06.182034: step 15037, loss 0.520213.
Train: 2018-08-05T19:25:06.400702: step 15038, loss 0.579341.
Train: 2018-08-05T19:25:06.619431: step 15039, loss 0.536688.
Train: 2018-08-05T19:25:06.822478: step 15040, loss 0.596768.
Test: 2018-08-05T19:25:07.884729: step 15040, loss 0.547505.
Train: 2018-08-05T19:25:08.087838: step 15041, loss 0.59692.
Train: 2018-08-05T19:25:08.290914: step 15042, loss 0.579666.
Train: 2018-08-05T19:25:08.509613: step 15043, loss 0.527679.
Train: 2018-08-05T19:25:08.728313: step 15044, loss 0.571037.
Train: 2018-08-05T19:25:08.931389: step 15045, loss 0.501474.
Train: 2018-08-05T19:25:09.150059: step 15046, loss 0.623501.
Train: 2018-08-05T19:25:09.353165: step 15047, loss 0.588578.
Train: 2018-08-05T19:25:09.556243: step 15048, loss 0.562365.
Train: 2018-08-05T19:25:09.774942: step 15049, loss 0.544938.
Train: 2018-08-05T19:25:09.978019: step 15050, loss 0.632003.
Test: 2018-08-05T19:25:11.040270: step 15050, loss 0.547871.
Train: 2018-08-05T19:25:11.243317: step 15051, loss 0.527677.
Train: 2018-08-05T19:25:11.446425: step 15052, loss 0.5364.
Train: 2018-08-05T19:25:11.649473: step 15053, loss 0.536423.
Train: 2018-08-05T19:25:11.868202: step 15054, loss 0.588267.
Train: 2018-08-05T19:25:12.071280: step 15055, loss 0.588236.
Train: 2018-08-05T19:25:12.274356: step 15056, loss 0.614002.
Train: 2018-08-05T19:25:12.493026: step 15057, loss 0.579471.
Train: 2018-08-05T19:25:12.696128: step 15058, loss 0.579382.
Train: 2018-08-05T19:25:12.899181: step 15059, loss 0.587768.
Train: 2018-08-05T19:25:13.117908: step 15060, loss 0.604459.
Test: 2018-08-05T19:25:14.164539: step 15060, loss 0.548474.
Train: 2018-08-05T19:25:14.383208: step 15061, loss 0.554069.
Train: 2018-08-05T19:25:14.586315: step 15062, loss 0.554167.
Train: 2018-08-05T19:25:14.804983: step 15063, loss 0.521232.
Train: 2018-08-05T19:25:15.008091: step 15064, loss 0.636663.
Train: 2018-08-05T19:25:15.211169: step 15065, loss 0.54618.
Train: 2018-08-05T19:25:15.414246: step 15066, loss 0.538105.
Train: 2018-08-05T19:25:15.617324: step 15067, loss 0.554461.
Train: 2018-08-05T19:25:15.835993: step 15068, loss 0.59524.
Train: 2018-08-05T19:25:16.039070: step 15069, loss 0.603355.
Train: 2018-08-05T19:25:16.242178: step 15070, loss 0.513942.
Test: 2018-08-05T19:25:17.288807: step 15070, loss 0.547778.
Train: 2018-08-05T19:25:17.491885: step 15071, loss 0.635793.
Train: 2018-08-05T19:25:17.710578: step 15072, loss 0.538384.
Train: 2018-08-05T19:25:17.913632: step 15073, loss 0.595091.
Train: 2018-08-05T19:25:18.116709: step 15074, loss 0.530387.
Train: 2018-08-05T19:25:18.335437: step 15075, loss 0.603165.
Train: 2018-08-05T19:25:18.538514: step 15076, loss 0.570805.
Train: 2018-08-05T19:25:18.757184: step 15077, loss 0.595047.
Train: 2018-08-05T19:25:18.960261: step 15078, loss 0.595015.
Train: 2018-08-05T19:25:19.178990: step 15079, loss 0.538648.
Train: 2018-08-05T19:25:19.382067: step 15080, loss 0.619088.
Test: 2018-08-05T19:25:20.428667: step 15080, loss 0.54943.
Train: 2018-08-05T19:25:20.631744: step 15081, loss 0.506679.
Train: 2018-08-05T19:25:20.850443: step 15082, loss 0.578871.
Train: 2018-08-05T19:25:21.053551: step 15083, loss 0.562791.
Train: 2018-08-05T19:25:21.256628: step 15084, loss 0.562768.
Train: 2018-08-05T19:25:21.475328: step 15085, loss 0.554667.
Train: 2018-08-05T19:25:21.678404: step 15086, loss 0.570798.
Train: 2018-08-05T19:25:21.881482: step 15087, loss 0.505815.
Train: 2018-08-05T19:25:22.084559: step 15088, loss 0.554421.
Train: 2018-08-05T19:25:22.287637: step 15089, loss 0.546073.
Train: 2018-08-05T19:25:22.490715: step 15090, loss 0.55418.
Test: 2018-08-05T19:25:23.552965: step 15090, loss 0.547511.
Train: 2018-08-05T19:25:23.771664: step 15091, loss 0.537367.
Train: 2018-08-05T19:25:23.974742: step 15092, loss 0.528694.
Train: 2018-08-05T19:25:24.193441: step 15093, loss 0.545303.
Train: 2018-08-05T19:25:24.396518: step 15094, loss 0.606101.
Train: 2018-08-05T19:25:24.599596: step 15095, loss 0.545061.
Train: 2018-08-05T19:25:24.802643: step 15096, loss 0.623107.
Train: 2018-08-05T19:25:25.005721: step 15097, loss 0.527566.
Train: 2018-08-05T19:25:25.224443: step 15098, loss 0.52746.
Train: 2018-08-05T19:25:25.427497: step 15099, loss 0.562387.
Train: 2018-08-05T19:25:25.614982: step 15100, loss 0.65628.
Test: 2018-08-05T19:25:26.661612: step 15100, loss 0.547121.
